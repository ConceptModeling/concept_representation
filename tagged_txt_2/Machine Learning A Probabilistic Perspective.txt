machine	B
learning	B
machine	B
learning	B
a	O
probabilistic	O
perspective	O
kevin	O
p	O
murphy	O
today	O
s	O
web-enabled	O
deluge	O
of	O
electronic	O
data	O
calls	O
for	O
automated	O
methods	O
of	O
data	O
analysis	O
machine	B
learning	B
provides	O
these	O
developing	O
methods	O
that	O
can	O
automatically	O
detect	O
patterns	O
in	O
data	O
and	O
use	O
the	O
uncovered	O
patterns	O
to	O
predict	O
future	O
data	O
this	O
textbook	O
offers	O
a	O
comprehensive	O
and	O
self-contained	O
introduction	O
to	O
the	O
field	O
of	O
machine	B
learning	B
a	O
unified	O
probabilistic	O
approach	O
the	O
coverage	O
combines	O
breadth	O
and	O
depth	O
offering	O
necessary	O
background	O
material	O
on	O
such	O
topics	O
as	O
probability	O
optimization	B
and	O
linear	O
algebra	O
as	O
well	O
as	O
discussion	O
of	O
recent	O
developments	O
in	O
the	O
field	O
including	O
conditional	O
random	O
fields	O
regularization	B
and	O
deep	B
learning	B
the	O
book	O
is	O
written	O
in	O
an	O
informal	O
accessible	O
style	O
complete	B
with	O
pseudo-code	O
for	O
the	O
most	O
important	O
algorithms	O
all	O
topics	O
are	O
copiously	O
illustrated	O
with	O
color	O
images	O
and	O
worked	O
examples	O
drawn	O
from	O
such	O
application	O
domains	O
as	O
biology	O
text	O
processing	O
computer	O
vision	O
and	O
robotics	O
rather	O
than	O
providing	O
a	O
cookbook	O
of	O
different	O
heuristic	O
methods	O
the	O
book	O
stresses	O
a	O
principled	O
model-based	O
approach	O
often	O
using	O
the	O
language	O
of	O
graphical	B
models	I
to	O
specify	O
models	O
in	O
a	O
concise	O
and	O
intuitive	O
way	O
almost	O
all	O
the	O
models	O
described	O
have	O
been	O
implemented	O
in	O
a	O
matlab	O
software	O
package	O
pmtk	O
modeling	O
toolkit	O
that	O
is	O
freely	O
available	O
online	O
the	O
book	O
is	O
suitable	O
for	O
upper-level	O
undergraduates	O
with	O
an	O
introductory-level	O
college	O
math	O
background	O
and	O
beginning	O
graduate	O
students	O
kevin	O
p	O
murphy	O
is	O
a	O
research	O
scientist	O
at	O
google	O
previously	O
he	O
was	O
associate	O
professor	O
of	O
computer	O
science	O
and	O
statistics	O
at	O
the	O
university	O
of	O
british	O
columbia	O
adaptive	O
computation	O
and	O
machine	B
learning	B
series	O
an	O
astonishing	O
machine	B
learning	B
book	O
intuitive	O
full	B
of	O
examples	O
fun	O
to	O
read	O
but	O
still	O
comprehensive	O
strong	O
and	O
deep	B
a	O
great	O
starting	O
point	O
for	O
any	O
university	O
student	O
and	O
a	O
must-have	O
for	O
anybody	O
in	O
the	O
field	O
jan	O
peters	O
darmstadt	O
university	O
of	O
technology	O
max-planck	O
institute	O
for	O
intelligent	O
systems	O
kevin	O
murphy	O
excels	O
at	O
unraveling	O
the	O
complexities	O
of	O
machine	B
learning	B
methods	O
while	O
motivating	O
the	O
reader	O
with	O
a	O
stream	O
of	O
illustrated	O
examples	O
and	O
real-world	O
case	O
studies	O
the	O
accompanying	O
software	O
package	O
includes	O
source	O
code	O
for	O
many	O
of	O
the	O
figures	O
making	O
it	O
both	O
easy	O
and	O
very	O
tempting	O
to	O
dive	O
in	O
and	O
explore	O
these	O
methods	O
for	O
yourself	O
a	O
must-buy	O
for	O
anyone	O
interested	O
in	O
machine	B
learning	B
or	O
curious	O
about	O
how	O
to	O
extract	O
useful	O
knowledge	O
from	O
big	B
data	I
john	O
winn	O
microsoft	B
research	O
this	O
is	O
a	O
wonderful	O
book	O
that	O
starts	O
with	O
basic	O
topics	O
in	O
statistical	O
modeling	O
culminating	O
in	O
the	O
most	O
advanced	O
topics	O
it	O
provides	O
both	O
the	O
theoretical	O
foundations	O
of	O
probabilistic	O
machine	B
learning	B
as	O
well	O
as	O
practical	O
tools	O
in	O
the	O
form	O
of	O
matlab	O
code	O
the	O
book	O
should	O
be	O
on	O
the	O
shelf	O
of	O
any	O
student	O
interested	O
in	O
the	O
topic	B
and	O
any	O
practitioner	O
working	O
in	O
the	O
field	O
yoram	O
singer	O
google	O
research	O
this	O
book	O
will	O
be	O
an	O
essential	O
reference	O
for	O
practitioners	O
of	O
modern	O
machine	B
learning	B
it	O
covers	O
the	O
basic	O
concepts	O
needed	O
to	O
understand	O
the	O
field	O
as	O
a	O
whole	O
and	O
the	O
powerful	O
modern	O
methods	O
that	O
build	O
on	O
those	O
concepts	O
in	O
machine	B
learning	B
the	O
language	O
of	O
probability	O
and	O
statistics	O
reveals	O
important	O
connections	O
between	O
seemingly	O
disparate	O
algorithms	O
and	O
strategies	O
thus	O
its	O
readers	O
will	O
become	O
articulate	O
in	O
a	O
holistic	O
view	O
of	O
the	O
state-of-the-art	O
and	O
poised	O
to	O
build	O
the	O
next	O
generation	O
of	O
machine	B
learning	B
algorithms	O
david	O
blei	O
princeton	O
university	O
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
cambridge	O
massachusetts	O
httpmitpress	O
mit	O
edu	O
the	O
cover	O
image	O
is	O
based	O
on	O
sequential	B
bayesian	B
updating	O
of	O
a	O
gaussian	B
distribution	O
see	O
figure	O
for	O
details	O
machine	B
learning	B
a	O
probabilistic	O
perspective	O
kevin	O
p	O
murphy	O
machine	B
learning	B
a	O
probabilistic	O
perspective	O
machine	B
learning	B
a	O
probabilistic	O
perspective	O
kevin	O
p	O
murphy	O
the	O
mit	O
press	O
cambridge	O
massachusetts	O
london	O
england	O
massachusetts	O
institute	O
of	O
technology	O
all	O
rights	O
reserved	O
no	O
part	O
of	O
this	O
book	O
may	O
be	O
reproduced	O
in	O
any	O
form	O
by	O
any	O
electronic	O
or	O
mechanical	O
means	O
photocopying	O
recording	O
or	O
information	B
storage	O
and	O
retrieval	O
without	O
permission	O
in	O
writing	O
from	O
the	O
publisher	O
for	O
information	B
about	O
special	O
quantity	O
discounts	O
please	O
email	O
special	O
salesmitpress	O
mit	O
edu	O
this	O
book	O
was	O
set	O
in	O
the	O
latex	O
programming	O
language	O
by	O
the	O
author	O
printed	O
and	O
bound	O
in	O
the	O
united	O
states	O
of	O
america	O
library	O
of	O
congress	O
cataloging-in-publication	O
information	B
murphy	O
kevin	O
p	O
machine	B
learning	B
a	O
probabilistic	O
perspective	O
kevin	O
p	O
murphy	O
p	O
cm	O
computation	O
and	O
machine	B
learning	B
series	O
includes	O
bibliographical	O
references	O
and	O
index	O
isbn	O
alk	O
paper	O
machine	B
learning	B
probabilities	O
i	O
title	O
this	O
book	O
is	O
dedicated	O
to	O
alessandro	O
michael	O
and	O
stefano	O
and	O
to	O
the	O
memory	O
of	O
gerard	O
joseph	O
murphy	O
contents	O
preface	O
xxvii	O
introduction	O
types	O
of	O
machine	B
learning	B
classification	O
regression	B
discovering	O
clusters	B
discovering	O
latent	B
factors	B
discovering	O
graph	B
structure	O
matrix	B
completion	I
machine	B
learning	B
what	O
and	O
why	O
supervised	B
learning	B
unsupervised	B
learning	B
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
parametric	O
vs	O
non-parametric	O
models	O
a	O
simple	O
non-parametric	O
classifier	O
k-nearest	O
neighbors	B
the	O
curse	B
of	I
dimensionality	I
parametric	O
models	O
for	O
classification	O
and	O
regression	B
linear	B
regression	B
logistic	B
regression	B
overfitting	O
model	B
selection	I
no	B
free	I
lunch	I
theorem	I
probability	O
introduction	O
a	O
brief	O
review	O
of	O
probability	B
theory	I
discrete	B
random	O
variables	O
fundamental	O
rules	B
bayes	B
rule	I
independence	O
and	O
conditional	B
independence	I
continuous	O
random	O
variables	O
viii	O
contents	O
the	O
binomial	B
and	O
bernoulli	B
distributions	O
the	O
multinomial	B
and	O
multinoulli	O
distributions	O
the	O
poisson	B
distribution	O
the	O
empirical	B
distribution	I
quantiles	O
mean	B
and	O
variance	B
some	O
common	O
discrete	B
distributions	O
some	O
common	O
continuous	O
distributions	O
gaussian	B
distribution	O
degenerate	B
pdf	B
the	O
laplace	B
distribution	I
the	O
gamma	B
distribution	I
the	O
beta	B
distribution	I
pareto	B
distribution	I
joint	O
probability	O
distributions	O
transformations	O
of	O
random	O
variables	O
monte	B
carlo	I
approximation	O
information	B
theory	I
covariance	B
and	O
correlation	O
the	O
multivariate	B
gaussian	B
multivariate	B
student	B
t	I
distribution	I
dirichlet	B
distribution	I
linear	O
transformations	O
general	O
transformations	O
central	B
limit	I
theorem	I
entropy	B
kl	B
divergence	I
mutual	B
information	B
example	O
change	B
of	I
variables	I
the	O
mc	O
way	O
example	O
estimating	O
by	O
monte	B
carlo	I
integration	I
accuracy	O
of	O
monte	B
carlo	I
approximation	O
generative	O
models	O
for	O
discrete	B
data	O
likelihood	B
prior	O
posterior	O
posterior	B
predictive	B
distribution	I
a	O
more	O
complex	O
prior	O
introduction	O
bayesian	B
concept	B
learning	B
the	O
beta-binomial	B
model	O
likelihood	B
prior	O
posterior	O
posterior	B
predictive	B
distribution	I
contents	O
ix	O
likelihood	B
prior	O
posterior	O
posterior	O
predictive	B
the	O
dirichlet-multinomial	O
model	O
naive	O
bayes	O
classifiers	O
model	O
fitting	O
using	O
the	O
model	O
for	O
prediction	O
the	O
log-sum-exp	B
trick	O
feature	B
selection	I
using	O
mutual	B
information	B
classifying	O
documents	O
using	O
bag	B
of	I
words	I
notation	O
basics	O
mle	B
for	O
an	O
mvn	B
maximum	B
entropy	B
derivation	O
of	O
the	O
gaussian	B
quadratic	B
discriminant	B
analysis	I
linear	B
discriminant	B
analysis	I
two-class	O
lda	B
mle	B
for	O
discriminant	B
analysis	I
strategies	O
for	O
preventing	O
overfitting	O
regularized	O
lda	B
diagonal	B
lda	B
nearest	B
shrunken	I
centroids	B
classifier	O
gaussian	B
models	O
introduction	O
gaussian	B
discriminant	B
analysis	I
inference	B
in	O
jointly	O
gaussian	B
distributions	O
linear	B
gaussian	B
systems	O
digression	O
the	O
wishart	B
distribution	O
inverse	B
wishart	B
distribution	O
visualizing	B
the	O
wishart	B
distribution	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
posterior	O
distribution	O
of	O
posterior	O
distribution	O
of	O
posterior	O
distribution	O
of	O
and	O
sensor	B
fusion	I
with	O
unknown	B
precisions	O
statement	O
of	O
the	O
result	O
examples	O
information	B
form	I
proof	O
of	O
the	O
result	O
statement	O
of	O
the	O
result	O
examples	O
proof	O
of	O
the	O
result	O
x	O
contents	O
map	O
estimation	O
credible	O
intervals	O
inference	B
for	O
a	O
difference	O
in	O
proportions	O
bayesian	B
occam	O
s	O
razor	O
computing	O
the	O
marginal	B
likelihood	B
bayes	O
factors	B
jeffreys-lindley	B
paradox	I
uninformative	B
priors	O
jeffreys	O
priors	O
robust	B
priors	I
mixtures	O
of	O
conjugate	B
priors	I
bayesian	B
statistics	I
introduction	O
summarizing	O
posterior	O
distributions	O
bayesian	B
model	B
selection	I
priors	O
hierarchical	O
bayes	O
empirical	B
bayes	I
bayesian	B
decision	B
theory	O
bayes	O
estimators	O
for	O
common	O
loss	B
functions	O
the	O
false	B
positive	I
vs	O
false	B
negative	I
tradeoff	O
other	O
topics	O
example	O
beta-binomial	B
model	O
example	O
gaussian-gaussian	O
model	O
example	O
modeling	O
related	O
cancer	O
rates	O
frequentist	B
statistics	I
bootstrap	B
large	O
sample	O
theory	O
for	O
the	O
mle	B
bayes	B
risk	B
minimax	O
risk	B
admissible	B
estimators	O
introduction	O
sampling	B
distribution	I
of	O
an	O
estimator	B
frequentist	B
decision	B
theory	O
desirable	O
properties	O
of	O
estimators	O
empirical	B
risk	B
minimization	I
consistent	B
estimators	I
unbiased	B
estimators	O
minimum	O
variance	B
estimators	O
the	O
bias-variance	B
tradeoff	I
regularized	B
risk	B
minimization	I
structural	B
risk	B
minimization	I
estimating	O
the	O
risk	B
using	O
cross	B
validation	I
upper	O
bounding	O
the	O
risk	B
using	O
statistical	B
learning	B
theory	I
contents	O
xi	O
surrogate	B
loss	B
functions	O
pathologies	B
of	O
frequentist	B
statistics	I
counter-intuitive	O
behavior	O
of	O
confidence	O
intervals	O
p-values	O
considered	O
harmful	O
the	O
likelihood	B
principle	I
why	O
isn	O
t	O
everyone	O
a	O
bayesian	B
derivation	O
of	O
the	O
mle	B
geometric	O
interpretation	O
convexity	O
linear	B
regression	B
introduction	O
model	O
specification	O
maximum	O
likelihood	B
estimation	O
squares	O
robust	B
linear	B
regression	B
ridge	B
regression	B
bayesian	B
linear	B
regression	B
basic	O
idea	O
numerically	O
stable	B
computation	O
connection	O
with	O
pca	B
regularization	B
effects	O
of	O
big	B
data	I
computing	O
the	O
posterior	O
computing	O
the	O
posterior	O
predictive	B
bayesian	B
inference	B
when	O
is	O
unknown	B
eb	B
for	O
linear	B
regression	B
procedure	O
logistic	B
regression	B
introduction	O
model	O
specification	O
model	O
fitting	O
mle	B
steepest	B
descent	I
newton	O
s	O
method	O
iteratively	B
reweighted	I
least	B
squares	I
quasi-newton	B
metric	B
methods	O
regularization	B
multi-class	B
logistic	B
regression	B
bayesian	B
logistic	B
regression	B
online	B
learning	B
and	O
stochastic	B
optimization	B
laplace	B
approximation	I
derivation	O
of	O
the	O
bic	B
gaussian	B
approximation	I
for	O
logistic	B
regression	B
approximating	O
the	O
posterior	O
predictive	B
residual	B
analysis	I
detection	O
online	B
learning	B
and	O
regret	B
minimization	O
xii	O
stochastic	B
optimization	B
and	O
risk	B
minimization	O
the	O
lms	B
algorithm	O
the	O
perceptron	B
algorithm	I
a	O
bayesian	B
view	O
generative	O
vs	O
discriminative	B
classifiers	O
pros	O
and	O
cons	O
of	O
each	O
approach	O
dealing	O
with	O
missing	B
data	I
fisher	O
s	O
linear	B
discriminant	B
analysis	I
contents	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
definition	O
examples	O
log	B
partition	B
function	I
mle	B
for	O
the	O
exponential	B
family	B
bayes	O
for	O
the	O
exponential	B
family	B
maximum	B
entropy	B
derivation	O
of	O
the	O
exponential	B
family	B
introduction	O
the	O
exponential	B
family	B
generalized	B
linear	I
models	I
probit	B
regression	B
basics	O
ml	O
and	O
map	O
estimation	O
bayesian	B
inference	B
generalized	O
linear	O
mixed	O
models	O
learning	B
to	I
rank	I
the	O
pointwise	B
approach	I
the	O
pairwise	O
approach	O
the	O
listwise	O
approach	O
loss	B
functions	O
for	O
ranking	B
directed	B
graphical	B
models	I
nets	O
introduction	O
chain	B
rule	I
conditional	B
independence	I
mlmap	O
estimation	O
using	O
gradient-based	O
optimization	B
latent	B
variable	O
interpretation	O
ordinal	B
probit	B
regression	B
multinomial	B
probit	B
models	O
multi-task	B
learning	B
hierarchical	O
bayes	O
for	O
multi-task	B
learning	B
application	O
to	O
personalized	O
email	O
spam	B
filtering	B
application	O
to	O
domain	B
adaptation	I
other	O
kinds	O
of	O
prior	O
example	O
semi-parametric	O
glmms	O
for	O
medical	O
data	O
computational	O
issues	O
contents	O
xiii	O
graphical	B
models	I
graph	B
terminology	O
directed	B
graphical	B
models	I
naive	O
bayes	O
classifiers	O
markov	B
and	O
hidden	B
markov	B
models	I
medical	O
diagnosis	O
genetic	B
linkage	I
analysis	I
directed	B
gaussian	B
graphical	B
models	I
examples	O
inference	B
learning	B
conditional	B
independence	I
properties	O
of	O
dgms	O
plate	O
notation	O
learning	B
from	O
complete	B
data	I
learning	B
with	O
missing	B
andor	O
latent	B
variables	O
d-separation	O
and	O
the	O
bayes	B
ball	I
algorithm	I
markov	B
properties	O
other	O
markov	B
properties	O
of	O
dgms	O
markov	B
blanket	I
and	O
full	B
conditionals	O
influence	O
diagrams	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
mixture	B
models	O
latent	B
variable	I
models	I
unidentifiability	O
computing	O
a	O
map	B
estimate	I
is	O
non-convex	O
mixtures	O
of	O
gaussians	O
mixture	B
of	O
multinoullis	O
using	O
mixture	B
models	O
for	O
clustering	B
mixtures	O
of	O
experts	O
parameter	B
estimation	O
for	O
mixture	B
models	O
the	O
em	B
algorithm	O
basic	O
idea	O
em	B
for	O
gmms	O
em	B
for	O
mixture	B
of	I
experts	I
em	B
for	O
dgms	O
with	O
hidden	B
variables	I
em	B
for	O
the	O
student	O
distribution	O
em	B
for	O
probit	B
regression	B
theoretical	O
basis	O
for	O
em	B
online	B
em	B
other	O
em	B
variants	O
model	B
selection	I
for	O
latent	B
variable	I
models	I
model	B
selection	I
for	O
probabilistic	O
models	O
model	B
selection	I
for	O
non-probabilistic	O
methods	O
fitting	O
models	O
with	O
missing	B
data	I
xiv	O
contents	O
em	B
for	O
the	O
mle	B
of	O
an	O
mvn	B
with	O
missing	B
data	I
fa	B
is	O
a	O
low	O
rank	O
parameterization	O
of	O
an	O
mvn	B
inference	B
of	O
the	O
latent	B
factors	B
unidentifiability	O
mixtures	O
of	O
factor	B
analysers	O
em	B
for	O
factor	B
analysis	I
models	O
fitting	O
fa	B
models	O
with	O
missing	B
data	I
classical	B
pca	B
statement	O
of	O
the	O
theorem	O
proof	O
singular	B
value	I
decomposition	I
probabilistic	B
pca	B
em	B
algorithm	O
for	O
pca	B
latent	B
linear	O
models	O
factor	B
analysis	I
principal	B
components	I
analysis	I
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
pca	B
for	O
categorical	B
data	O
pca	B
for	O
paired	O
and	O
multi-view	O
data	O
supervised	B
pca	B
factor	B
regression	B
partial	B
least	B
squares	I
canonical	B
correlation	I
analysis	I
independent	B
component	I
analysis	I
maximum	O
likelihood	B
estimation	O
the	O
fastica	O
algorithm	O
using	O
em	B
other	O
estimation	O
principles	O
model	B
selection	I
for	O
fappca	O
model	B
selection	I
for	O
pca	B
sparse	B
linear	O
models	O
introduction	O
bayesian	B
variable	O
selection	O
regularization	B
basics	O
regularization	B
algorithms	O
coordinate	O
descent	O
the	O
spike	B
and	I
slab	I
model	O
from	O
the	O
bernoulli-gaussian	B
model	O
to	O
regularization	B
algorithms	O
why	O
does	O
regularization	B
yield	O
sparse	B
solutions	O
optimality	O
conditions	O
for	O
lasso	B
comparison	O
of	O
least	B
squares	I
lasso	B
ridge	O
and	O
subset	O
selection	O
regularization	B
path	B
model	B
selection	I
bayesian	B
inference	B
for	O
linear	O
models	O
with	O
laplace	B
priors	O
contents	O
xv	O
lars	B
and	O
other	O
homotopy	B
methods	O
proximal	O
and	O
gradient	O
projection	B
methods	O
em	B
for	O
lasso	B
regularization	B
extensions	O
group	B
lasso	B
fused	B
lasso	B
elastic	B
net	I
and	O
lasso	B
combined	O
non-convex	O
regularizers	O
bridge	B
regression	B
hierarchical	B
adaptive	B
lasso	B
other	O
hierarchical	O
priors	O
automatic	B
relevance	I
determination	I
bayesian	B
learning	B
sparse	B
coding	I
learning	B
a	O
sparse	B
coding	I
dictionary	B
results	O
of	O
dictionary	B
learning	B
from	O
image	O
patches	O
compressed	B
sensing	I
image	B
inpainting	I
and	O
denoising	O
ard	B
for	O
linear	B
regression	B
whence	O
sparsity	B
connection	O
to	O
map	O
estimation	O
algorithms	O
for	O
ard	B
ard	B
for	O
logistic	B
regression	B
kernels	O
rbf	B
kernels	O
kernels	O
for	O
comparing	O
documents	O
mercer	O
definite	O
kernels	O
linear	O
kernels	O
matern	O
kernels	O
string	O
kernels	O
pyramid	O
match	O
kernels	O
kernels	O
derived	O
from	O
probabilistic	O
generative	O
models	O
introduction	O
kernel	B
functions	O
using	O
kernels	O
inside	O
glms	O
kernel	B
machines	O
rvms	O
and	O
other	O
sparse	B
vector	O
machines	O
the	O
kernel	B
trick	I
support	B
vector	I
machines	I
kernelized	O
nearest	B
neighbor	I
classification	O
kernelized	O
k-medoids	O
clustering	B
kernelized	O
ridge	B
regression	B
kernel	B
pca	B
svms	O
for	O
regression	B
svms	O
for	O
classification	O
xvi	O
choosing	O
c	O
summary	O
of	O
key	O
points	O
a	O
probabilistic	O
interpretation	O
of	O
svms	O
comparison	O
of	O
discriminative	B
kernel	B
methods	O
kernels	O
for	O
building	O
generative	O
models	O
smoothing	B
kernels	O
kernel	B
density	B
estimation	I
from	O
kde	B
to	O
knn	B
kernel	B
regression	B
locally	B
weighted	I
regression	B
predictions	O
using	O
noise-free	O
observations	O
predictions	O
using	O
noisy	O
observations	O
effect	O
of	O
the	O
kernel	B
parameters	O
estimating	O
the	O
kernel	B
parameters	O
computational	O
and	O
numerical	O
issues	O
semi-parametric	O
gps	B
gaussian	B
processes	I
introduction	O
gps	B
for	O
regression	B
gps	B
meet	O
glms	O
connection	O
with	O
other	O
methods	O
gp	O
latent	B
variable	O
model	O
approximation	O
methods	O
for	O
large	O
datasets	O
linear	O
models	O
compared	O
to	O
gps	B
linear	O
smoothers	O
compared	O
to	O
gps	B
svms	O
compared	O
to	O
gps	B
and	O
rvms	O
compared	O
to	O
gps	B
neural	B
networks	I
compared	O
to	O
gps	B
smoothing	B
splines	I
compared	O
to	O
gps	B
rkhs	B
methods	O
compared	O
to	O
gps	B
binary	O
classification	O
multi-class	O
classification	O
gps	B
for	O
poisson	B
regression	B
contents	O
adaptive	O
basis	O
function	O
models	O
introduction	O
classification	O
and	O
regression	B
trees	O
generalized	O
additive	O
models	O
basics	O
growing	O
a	O
tree	B
pruning	B
a	O
tree	B
pros	O
and	O
cons	O
of	O
trees	O
random	B
forests	I
cart	B
compared	O
to	O
hierarchical	B
mixture	B
of	I
experts	I
contents	O
xvii	O
a	O
bayesian	B
view	O
backfitting	O
computational	O
efficiency	O
multivariate	B
adaptive	I
regression	B
splines	I
forward	B
stagewise	I
additive	I
modeling	I
adaboost	O
logitboost	B
boosting	B
as	O
functional	B
gradient	B
descent	I
sparse	B
boosting	B
multivariate	O
adaptive	O
regression	B
trees	O
boosting	B
why	O
does	O
boosting	B
work	O
so	O
well	O
feedforward	O
neural	B
networks	I
perceptrons	O
ensemble	B
learning	B
stacking	B
error-correcting	B
output	I
codes	I
ensemble	B
learning	B
is	O
not	O
equivalent	O
to	O
bayes	B
model	I
averaging	I
experimental	O
comparison	O
interpreting	O
black-box	B
models	O
convolutional	O
neural	B
networks	I
other	O
kinds	O
of	O
neural	B
networks	I
a	O
brief	O
history	O
of	O
the	O
field	O
the	O
backpropagation	B
algorithm	I
identifiability	O
regularization	B
bayesian	B
inference	B
low-dimensional	O
features	B
high-dimensional	O
features	B
markov	B
and	O
hidden	B
markov	B
models	I
introduction	O
markov	B
models	I
transition	B
matrix	I
application	O
language	B
modeling	I
stationary	B
distribution	I
of	O
a	O
markov	B
chain	I
application	O
google	O
s	O
pagerank	B
algorithm	O
for	O
web	O
page	O
ranking	B
hidden	B
markov	B
models	I
applications	O
of	O
hmms	B
inference	B
in	O
hmms	B
types	O
of	O
inference	B
problems	O
for	O
temporal	O
models	O
the	O
forwards	O
algorithm	O
the	O
forwards-backwards	B
algorithm	I
the	O
viterbi	B
algorithm	O
forwards	O
filtering	B
backwards	O
sampling	O
xviii	O
learning	B
for	O
hmms	B
training	O
with	O
fully	O
observed	O
data	O
em	B
for	O
hmms	B
baum-welch	B
algorithm	O
bayesian	B
methods	O
for	O
fitting	O
hmms	B
discriminative	B
training	O
model	B
selection	I
generalizations	O
of	O
hmms	B
variable	O
duration	O
hmms	B
hierarchical	O
hmms	B
input-output	O
hmms	B
auto-regressive	O
and	O
buried	O
hmms	B
factorial	B
hmm	B
coupled	B
hmm	B
and	O
the	O
influence	O
model	O
dynamic	O
bayesian	B
networks	I
contents	O
ssms	O
for	O
object	O
tracking	B
robotic	O
slam	B
online	O
parameter	B
learning	B
using	O
recursive	B
least	B
squares	I
ssm	B
for	O
time	O
series	O
forecasting	O
state	B
space	I
models	O
introduction	O
applications	O
of	O
ssms	O
inference	B
in	O
lg-ssm	B
learning	B
for	O
lg-ssm	B
approximate	O
online	O
inference	B
for	O
non-linear	O
non-gaussian	O
ssms	O
identifiability	O
and	O
numerical	O
stability	O
training	O
with	O
fully	O
observed	O
data	O
em	B
for	O
lg-ssm	B
subspace	O
methods	O
bayesian	B
methods	O
for	O
fitting	O
lg-ssms	O
the	O
kalman	O
filtering	B
algorithm	O
the	O
kalman	B
smoothing	B
algorithm	O
extended	O
kalman	O
filter	O
unscented	O
kalman	O
filter	O
assumed	O
density	O
filtering	B
hybrid	O
discretecontinuous	O
ssms	O
inference	B
application	O
data	B
association	I
and	O
multi-target	B
tracking	B
application	O
fault	B
diagnosis	I
application	O
econometric	B
forecasting	I
undirected	B
graphical	B
models	I
random	O
fields	O
introduction	O
conditional	B
independence	I
properties	O
of	O
ugms	O
key	O
properties	O
contents	O
xix	O
an	O
undirected	B
alternative	O
to	O
d-separation	O
comparing	O
directed	B
and	O
undirected	B
graphical	B
models	I
the	O
hammersley-clifford	B
theorem	O
representing	O
potential	O
functions	O
ising	B
model	I
hopfield	O
networks	O
potts	B
model	I
gaussian	B
mrfs	O
markov	B
logic	O
networks	O
parameterization	O
of	O
mrfs	O
examples	O
of	O
mrfs	O
learning	B
conditional	O
random	O
fields	O
structural	O
svms	O
training	O
maxent	B
models	O
using	O
gradient	O
methods	O
training	O
partially	O
observed	O
maxent	B
models	O
approximate	O
methods	O
for	O
computing	O
the	O
mles	O
of	O
mrfs	O
pseudo	B
likelihood	B
stochastic	B
maximum	I
likelihood	B
feature	B
induction	B
for	O
maxent	B
models	O
iterative	O
proportional	O
fitting	O
chain-structured	O
crfs	O
memms	O
and	O
the	O
label-bias	O
problem	O
applications	O
of	O
crfs	O
crf	B
training	O
ssvms	B
a	O
probabilistic	O
view	O
ssvms	B
a	O
non-probabilistic	O
view	O
cutting	B
plane	I
methods	O
for	O
fitting	O
ssvms	B
online	O
algorithms	O
for	O
fitting	O
ssvms	B
latent	B
structural	O
svms	O
exact	O
inference	B
for	O
graphical	B
models	I
serial	O
protocol	O
parallel	O
protocol	O
gaussian	B
bp	B
other	O
bp	B
variants	O
introduction	O
belief	B
propagation	I
for	O
trees	O
the	O
variable	B
elimination	I
algorithm	O
the	O
junction	B
tree	B
algorithm	I
message	B
passing	I
on	O
a	O
junction	B
tree	B
the	O
generalized	O
distributive	B
law	I
computational	O
complexity	O
of	O
ve	O
a	O
weakness	O
of	O
ve	O
computational	O
complexity	O
of	O
jta	B
creating	O
a	O
junction	B
tree	B
xx	O
contents	O
jta	B
generalizations	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
approximate	B
inference	B
variational	B
inference	B
alternative	O
interpretations	O
of	O
the	O
variational	O
objective	O
forward	O
or	O
reverse	B
kl	I
derivation	O
of	O
the	O
mean	B
field	O
update	O
equations	O
example	O
mean	B
field	O
for	O
the	O
ising	B
model	I
example	O
factorial	B
hmm	B
introduction	O
variational	B
inference	B
the	O
mean	B
field	O
method	O
structured	O
mean	B
field	O
variational	B
bayes	I
variational	B
bayes	I
em	B
variational	B
message	B
passing	I
and	O
vibes	B
local	O
variational	O
bounds	O
example	O
vb	B
for	O
a	O
univariate	O
gaussian	B
example	O
vb	B
for	O
linear	B
regression	B
example	O
vbem	B
for	O
mixtures	O
of	O
gaussians	O
motivating	O
applications	O
bohning	O
s	O
quadratic	O
bound	O
to	O
the	O
log-sum-exp	B
function	O
bounds	O
for	O
the	O
sigmoid	B
function	O
other	O
bounds	O
and	O
approximations	O
to	O
the	O
log-sum-exp	B
function	O
variational	B
inference	B
based	O
on	O
upper	O
bounds	O
more	O
variational	B
inference	B
a	O
brief	O
history	O
lbp	B
on	O
pairwise	O
models	O
lbp	B
on	O
a	O
factor	B
graph	B
convergence	O
accuracy	O
of	O
lbp	B
other	O
speedup	O
tricks	O
for	O
lbp	B
introduction	O
loopy	B
belief	B
propagation	I
algorithmic	O
issues	O
loopy	B
belief	B
propagation	I
theoretical	O
issues	O
ugms	O
represented	O
in	O
exponential	B
family	B
form	O
the	O
marginal	B
polytope	I
exact	O
inference	B
as	O
a	O
variational	O
optimization	B
problem	O
mean	B
field	O
as	O
a	O
variational	O
optimization	B
problem	O
lbp	B
as	O
a	O
variational	O
optimization	B
problem	O
loopy	O
bp	B
vs	O
mean	B
field	O
extensions	O
of	O
belief	B
propagation	I
generalized	B
belief	B
propagation	I
contents	O
xxi	O
convex	B
belief	B
propagation	I
expectation	B
propagation	I
ep	B
as	O
a	O
variational	B
inference	B
problem	O
optimizing	O
the	O
ep	B
objective	O
using	O
moment	B
matching	I
ep	B
for	O
the	O
clutter	B
problem	I
lbp	B
is	O
a	O
special	O
case	O
of	O
ep	B
ranking	B
players	O
using	O
trueskill	B
other	O
applications	O
of	O
ep	B
map	O
state	B
estimation	I
linear	O
programming	O
relaxation	O
max-product	B
belief	B
propagation	I
graphcuts	B
experimental	O
comparison	O
of	O
graphcuts	B
and	O
bp	B
dual	B
decomposition	I
monte	B
carlo	I
inference	B
using	O
the	O
cdf	B
sampling	O
from	O
a	O
gaussian	B
method	O
introduction	O
sampling	O
from	O
standard	O
distributions	O
rejection	B
sampling	I
basic	O
idea	O
example	O
application	O
to	O
bayesian	B
statistics	I
adaptive	B
rejection	B
sampling	I
rejection	B
sampling	I
in	O
high	O
dimensions	O
importance	B
sampling	I
particle	O
filtering	B
rao-blackwellised	O
particle	O
filtering	B
sequential	B
importance	B
sampling	I
the	O
degeneracy	B
problem	I
the	O
resampling	O
step	O
the	O
proposal	B
distribution	I
application	O
robot	O
localization	O
application	O
visual	O
object	O
tracking	B
application	O
time	O
series	O
forecasting	O
rbpf	B
for	O
switching	O
lg-ssms	O
application	O
tracking	B
a	O
maneuvering	O
target	O
application	O
fast	O
slam	B
basic	O
idea	O
handling	O
unnormalized	O
distributions	O
importance	B
sampling	I
for	O
a	O
dgm	B
likelihood	B
weighting	I
sampling	B
importance	I
resampling	I
markov	B
chain	I
monte	B
carlo	I
inference	B
xxii	O
introduction	O
gibbs	B
sampling	I
basic	O
idea	O
example	O
gibbs	B
sampling	I
for	O
the	O
ising	B
model	I
example	O
gibbs	B
sampling	I
for	O
inferring	O
the	O
parameters	O
of	O
a	O
gmm	B
collapsed	B
gibbs	B
sampling	I
gibbs	B
sampling	I
for	O
hierarchical	O
glms	O
bugs	B
and	O
jags	B
the	O
imputation	B
posterior	I
algorithm	O
blocking	B
gibbs	B
sampling	I
metropolis	B
hastings	I
algorithm	O
contents	O
basic	O
idea	O
gibbs	B
sampling	I
is	O
a	O
special	O
case	O
of	O
mh	B
proposal	O
distributions	O
adaptive	B
mcmc	B
initialization	O
and	O
mode	B
hopping	O
reversible	B
jump	I
mcmc	B
why	O
mh	B
works	O
speed	O
and	O
accuracy	O
of	O
mcmc	B
auxiliary	O
variable	O
mcmc	B
the	O
burn-in	B
phase	B
mixing	O
rates	O
of	O
markov	B
chains	O
practical	O
convergence	O
diagnostics	O
accuracy	O
of	O
mcmc	B
how	O
many	O
chains	O
auxiliary	O
variable	O
sampling	O
for	O
logistic	B
regression	B
slice	B
sampling	I
swendsen	B
wang	I
hybridhamiltonian	O
mcmc	B
annealing	B
methods	O
simulated	B
annealing	B
annealed	B
importance	B
sampling	I
parallel	B
tempering	I
approximating	O
the	O
marginal	B
likelihood	B
the	O
candidate	B
method	I
harmonic	B
mean	B
estimate	O
annealed	B
importance	B
sampling	I
clustering	B
introduction	O
measuring	O
evaluating	O
the	O
output	O
of	O
clustering	B
methods	O
dirichlet	B
process	I
mixture	B
models	I
from	O
finite	O
to	O
infinite	O
mixture	B
models	O
the	O
dirichlet	B
process	I
contents	O
xxiii	O
applying	O
dirichlet	B
processes	O
to	O
mixture	B
modeling	O
fitting	O
a	O
dp	O
mixture	B
model	I
affinity	B
propagation	I
spectral	B
clustering	B
graph	B
laplacian	I
normalized	O
graph	B
laplacian	I
example	O
hierarchical	B
clustering	B
agglomerative	B
clustering	B
divisive	B
clustering	B
choosing	O
the	O
number	O
of	O
clusters	B
bayesian	B
hierarchical	B
clustering	B
clustering	B
datapoints	O
and	O
features	B
multi-view	O
clustering	B
biclustering	B
graphical	B
model	I
structure	B
learning	B
directed	B
or	O
undirected	B
tree	B
chow-liu	B
algorithm	I
for	O
finding	O
the	O
ml	O
tree	B
structure	O
finding	O
the	O
map	O
forest	B
approximating	O
the	O
marginal	B
likelihood	B
when	O
we	O
have	O
missing	B
data	I
structural	B
em	B
discovering	O
hidden	B
variables	I
case	O
study	O
google	O
s	O
rephil	B
structural	B
equation	I
models	I
relevance	O
networks	O
dependency	B
networks	I
markov	B
equivalence	I
exact	O
structural	O
inference	B
scaling	O
up	O
to	O
larger	O
graphs	O
introduction	O
structure	B
learning	B
for	O
knowledge	B
discovery	I
learning	B
tree	B
structures	O
mixtures	O
of	O
trees	O
learning	B
dag	B
structures	O
learning	B
dag	B
structure	O
with	O
latent	B
variables	O
learning	B
causal	O
dags	O
learning	B
undirected	B
gaussian	B
graphical	B
models	I
causal	O
interpretation	O
of	O
dags	O
using	O
causal	O
dags	O
to	O
resolve	O
simpson	O
s	O
paradox	O
learning	B
causal	O
dag	B
structures	O
mle	B
for	O
a	O
ggm	O
graphical	B
lasso	B
bayesian	B
inference	B
for	O
ggm	O
structure	O
handling	O
non-gaussian	O
data	O
using	O
copulas	O
xxiv	O
learning	B
undirected	B
discrete	B
graphical	B
models	I
graphical	B
lasso	B
for	O
mrfscrfs	O
thin	B
junction	B
trees	I
latent	B
variable	I
models	I
for	O
discrete	B
data	O
introduction	O
distributed	O
state	B
lvms	O
for	O
discrete	B
data	O
contents	O
mixture	B
models	O
exponential	B
family	B
pca	B
lda	B
and	O
mpca	B
gap	B
model	O
and	O
non-negative	B
matrix	B
factorization	I
basics	O
unsupervised	O
discovery	O
of	O
topics	O
quantitatively	O
evaluating	O
lda	B
as	O
a	O
language	B
model	I
fitting	O
using	O
gibbs	B
sampling	I
example	O
fitting	O
using	O
batch	B
variational	B
inference	B
fitting	O
using	O
online	O
variational	B
inference	B
determining	O
the	O
number	O
of	O
topics	O
latent	B
dirichlet	B
allocation	I
extensions	O
of	O
lda	B
lvms	O
for	O
graph-structured	O
data	O
stochastic	B
block	I
model	I
mixed	B
membership	I
stochastic	B
block	I
model	I
relational	B
topic	B
model	I
lvms	O
for	O
relational	O
data	O
infinite	O
relational	O
model	O
probabilistic	B
matrix	B
factorization	I
for	O
collaborative	O
filtering	B
restricted	O
boltzmann	O
machines	O
correlated	B
topic	B
model	I
dynamic	B
topic	B
model	I
lda-hmm	B
supervised	B
lda	B
varieties	O
of	O
rbms	O
learning	B
rbms	O
applications	O
of	O
rbms	O
deep	B
learning	B
introduction	O
deep	B
generative	O
models	O
deep	B
directed	B
networks	I
deep	B
boltzmann	O
machines	O
deep	B
belief	B
networks	I
greedy	O
layer-wise	O
learning	B
of	O
dbns	O
deep	B
neural	B
networks	I
contents	O
deep	B
multi-layer	O
perceptrons	O
deep	B
auto-encoders	B
stacked	O
denoising	O
auto-encoders	B
applications	O
of	O
deep	B
networks	I
handwritten	O
digit	O
classification	O
using	O
dbns	O
data	O
visualization	O
and	O
feature	O
discovery	O
using	O
deep	B
auto-encoders	B
information	B
retrieval	I
using	O
deep	B
auto-encoders	B
hashing	O
learning	B
audio	O
features	B
using	O
convolutional	B
dbns	I
learning	B
image	O
features	B
using	O
convolutional	B
dbns	I
xxv	O
discussion	O
notation	O
bibliography	O
indexes	O
index	O
to	O
code	O
index	O
to	O
keywords	O
preface	O
introduction	O
with	O
the	O
ever	O
increasing	O
amounts	O
of	O
data	O
in	O
electronic	O
form	O
the	O
need	O
for	O
automated	O
methods	O
for	O
data	O
analysis	O
continues	O
to	O
grow	O
the	O
goal	O
of	O
machine	B
learning	B
is	O
to	O
develop	O
methods	O
that	O
can	O
automatically	O
detect	O
patterns	O
in	O
data	O
and	O
then	O
to	O
use	O
the	O
uncovered	O
patterns	O
to	O
predict	O
future	O
data	O
or	O
other	O
outcomes	O
of	O
interest	O
machine	B
learning	B
is	O
thus	O
closely	O
related	O
to	O
the	O
fields	O
of	O
statistics	O
and	O
data	O
mining	O
but	O
differs	O
slightly	O
in	O
terms	O
of	O
its	O
emphasis	O
and	O
terminology	O
this	O
book	O
provides	O
a	O
detailed	O
introduction	O
to	O
the	O
field	O
and	O
includes	O
worked	O
examples	O
drawn	O
from	O
application	O
domains	O
such	O
as	O
molecular	O
biology	O
text	O
processing	O
computer	O
vision	O
and	O
robotics	O
target	O
audience	O
this	O
book	O
is	O
suitable	O
for	O
upper-level	O
undergraduate	O
students	O
and	O
beginning	O
graduate	O
students	O
in	O
computer	O
science	O
statistics	O
electrical	O
engineering	O
econometrics	O
or	O
any	O
one	O
else	O
who	O
has	O
the	O
appropriate	O
mathematical	O
background	O
specifically	O
the	O
reader	O
is	O
assumed	O
to	O
already	O
be	O
familiar	O
with	O
basic	O
multivariate	O
calculus	O
probability	O
linear	O
algebra	O
and	O
computer	O
programming	O
prior	O
exposure	O
to	O
statistics	O
is	O
helpful	O
but	O
not	O
necessary	O
a	O
probabilistic	O
approach	O
this	O
books	O
adopts	O
the	O
view	O
that	O
the	O
best	O
way	O
to	O
make	O
machines	O
that	O
can	O
learn	O
from	O
data	O
is	O
to	O
use	O
the	O
tools	O
of	O
probability	B
theory	I
which	O
has	O
been	O
the	O
mainstay	O
of	O
statistics	O
and	O
engineering	O
for	O
centuries	O
probability	B
theory	I
can	O
be	O
applied	O
to	O
any	O
problem	O
involving	O
uncertainty	B
in	O
machine	B
learning	B
uncertainty	B
comes	O
in	O
many	O
forms	O
what	O
is	O
the	O
best	O
prediction	O
decision	B
given	O
some	O
data	O
what	O
is	O
the	O
best	O
model	O
given	O
some	O
data	O
what	O
measurement	O
should	O
i	O
perform	O
next	O
etc	O
including	O
inferring	O
parameters	O
of	O
statistical	O
models	O
is	O
sometimes	O
called	O
a	O
bayesian	B
approach	O
however	O
this	O
term	O
tends	O
to	O
elicit	O
very	O
strong	O
reactions	O
positive	O
or	O
negative	O
depending	O
on	O
who	O
you	O
ask	O
so	O
we	O
prefer	O
the	O
more	O
neutral	O
term	O
probabilistic	O
approach	O
besides	O
we	O
will	O
often	O
use	O
techniques	O
such	O
as	O
maximum	O
likelihood	B
estimation	O
which	O
are	O
not	O
bayesian	B
methods	O
but	O
certainly	O
fall	O
within	O
the	O
probabilistic	O
paradigm	O
the	O
systematic	O
application	O
of	O
probabilistic	O
reasoning	O
to	O
all	O
inferential	O
problems	O
rather	O
than	O
describing	O
a	O
cookbook	O
of	O
different	O
heuristic	O
methods	O
this	O
book	O
stresses	O
a	O
principled	O
model-based	O
approach	O
to	O
machine	B
learning	B
for	O
any	O
given	O
model	O
a	O
variety	O
of	O
algorithms	O
xxviii	O
preface	O
can	O
often	O
be	O
applied	O
conversely	O
any	O
given	O
algorithm	O
can	O
often	O
be	O
applied	O
to	O
a	O
variety	O
of	O
models	O
this	O
kind	O
of	O
modularity	O
where	O
we	O
distinguish	O
model	O
from	O
algorithm	O
is	O
good	O
pedagogy	O
and	O
good	O
engineering	O
we	O
will	O
often	O
use	O
the	O
language	O
of	O
graphical	B
models	I
to	O
specify	O
our	O
models	O
in	O
a	O
concise	O
and	O
intuitive	O
way	O
in	O
addition	O
to	O
aiding	O
comprehension	O
the	O
graph	B
structure	O
aids	O
in	O
developing	O
efficient	O
algorithms	O
as	O
we	O
will	O
see	O
however	O
this	O
book	O
is	O
not	O
primarily	O
about	O
graphical	B
models	I
it	O
is	O
about	O
probabilistic	O
modeling	O
in	O
general	O
a	O
practical	O
approach	O
nearly	O
all	O
of	O
the	O
methods	O
described	O
in	O
this	O
book	O
have	O
been	O
implemented	O
in	O
a	O
matlab	O
software	O
package	O
called	O
pmtk	O
which	O
stands	O
for	O
probabilistic	O
modeling	O
toolkit	O
this	O
is	O
freely	O
available	O
from	O
digit	O
refers	O
to	O
the	O
third	O
edition	O
of	O
the	O
toolkit	O
which	O
is	O
the	O
one	O
used	O
in	O
this	O
version	O
of	O
the	O
book	O
there	O
are	O
also	O
a	O
variety	O
of	O
supporting	O
files	O
written	O
by	O
other	O
people	O
available	O
at	O
pmtksupport	O
googlecode	O
com	O
these	O
will	O
be	O
downloaded	O
automatically	O
if	O
you	O
follow	O
the	O
setup	O
instructions	O
described	O
on	O
the	O
pmtk	O
website	O
matlab	O
is	O
a	O
high-level	O
interactive	O
scripting	O
language	O
ideally	O
suited	O
to	O
numerical	O
computation	O
and	O
data	O
visualization	O
and	O
can	O
be	O
purchased	O
from	O
www	O
mathworks	O
com	O
some	O
of	O
the	O
code	O
requires	O
the	O
statistics	O
toolbox	O
which	O
needs	O
to	O
be	O
purchased	O
separately	O
there	O
is	O
also	O
a	O
free	O
version	O
of	O
matlab	O
called	O
octave	O
available	O
at	O
httpwww	O
gnu	O
orgsoftwareoctave	O
which	O
supports	O
most	O
of	O
the	O
functionality	O
of	O
matlab	O
some	O
not	O
all	O
of	O
the	O
code	O
in	O
this	O
book	O
also	O
works	O
in	O
octave	O
see	O
the	O
pmtk	O
website	O
for	O
details	O
pmtk	O
was	O
used	O
to	O
generate	O
many	O
of	O
the	O
figures	O
in	O
this	O
book	O
the	O
source	O
code	O
for	O
these	O
figures	O
is	O
included	O
on	O
the	O
pmtk	O
website	O
allowing	O
the	O
reader	O
to	O
easily	O
see	O
the	O
effects	O
of	O
changing	O
the	O
data	O
or	O
algorithm	O
or	O
parameter	B
settings	O
the	O
book	O
refers	O
to	O
files	O
by	O
name	O
e	O
g	O
naivebayesfit	O
in	O
order	O
to	O
find	O
the	O
corresponding	O
file	O
you	O
can	O
use	O
two	O
methods	O
within	O
matlab	O
you	O
can	O
type	O
which	O
naivebayesfit	O
and	O
it	O
will	O
return	O
the	O
full	B
path	B
to	O
the	O
file	O
or	O
if	O
you	O
do	O
not	O
have	O
matlab	O
but	O
want	O
to	O
read	O
the	O
source	O
code	O
anyway	O
you	O
can	O
use	O
your	O
favorite	O
search	O
engine	O
which	O
should	O
return	O
the	O
corresponding	O
file	O
from	O
the	O
website	O
details	O
on	O
how	O
to	O
use	O
pmtk	O
can	O
be	O
found	O
on	O
the	O
website	O
which	O
will	O
be	O
udpated	O
over	O
time	O
details	O
on	O
the	O
underlying	O
theory	O
behind	O
these	O
methods	O
can	O
be	O
found	O
in	O
this	O
book	O
acknowledgments	O
a	O
book	O
this	O
large	O
is	O
obviously	O
a	O
team	O
effort	O
i	O
would	O
especially	O
like	O
to	O
thank	O
the	O
following	O
people	O
my	O
wife	O
margaret	O
for	O
keeping	O
the	O
home	O
fires	O
burning	O
as	O
i	O
toiled	O
away	O
in	O
my	O
office	O
for	O
the	O
last	O
six	O
years	O
matt	O
dunham	O
who	O
created	O
many	O
of	O
the	O
figures	O
in	O
this	O
book	O
and	O
who	O
wrote	O
much	O
of	O
the	O
code	O
in	O
pmtk	O
baback	O
moghaddam	O
who	O
gave	O
extremely	O
detailed	O
feedback	O
on	O
every	O
page	O
of	O
an	O
earlier	O
draft	O
of	O
the	O
book	O
chris	O
williams	O
who	O
also	O
gave	O
very	O
detailed	O
feedback	O
cody	O
severinski	O
and	O
wei-lwun	O
lu	O
who	O
assisted	O
with	O
figures	O
generations	O
of	O
ubc	O
students	O
who	O
gave	O
helpful	O
comments	O
on	O
earlier	O
drafts	O
daphne	O
koller	O
nir	O
friedman	O
and	O
chris	O
manning	O
for	O
letting	O
me	O
use	O
their	O
latex	O
style	O
files	O
stanford	O
university	O
google	O
research	O
and	O
skyline	O
college	O
for	O
hosting	O
me	O
during	O
part	O
of	O
my	O
sabbatical	O
and	O
various	O
canadian	O
funding	O
agencies	O
crc	O
and	O
cifar	O
who	O
have	O
supported	O
me	O
financially	O
over	O
the	O
years	O
in	O
addition	O
i	O
would	O
like	O
to	O
thank	O
the	O
following	O
people	O
for	O
giving	O
me	O
helpful	O
feedback	O
on	O
preface	O
xxix	O
parts	O
of	O
the	O
book	O
andor	O
for	O
sharing	O
figures	O
code	O
exercises	O
or	O
even	O
some	O
cases	O
text	O
david	O
blei	O
hannes	O
bretschneider	O
greg	O
corrado	O
arnaud	O
doucet	O
mario	O
figueiredo	O
nando	O
de	O
freitas	O
mark	O
girolami	O
gabriel	O
goh	O
tom	O
griffiths	O
katherine	O
heller	O
geoff	O
hinton	O
aapo	O
hyvarinen	O
tommi	O
jaakkola	O
mike	O
jordan	O
charles	O
kemp	O
emtiyaz	O
khan	O
bonnie	O
kirkpatrick	O
daphne	O
koller	O
zico	O
kolter	O
honglak	O
lee	O
julien	O
mairal	O
andrew	O
mcpherson	O
tom	O
minka	O
ian	O
nabney	O
arthur	O
pope	O
carl	O
rassmussen	O
ryan	O
rifkin	O
ruslan	O
salakhutdinov	O
mark	O
schmidt	O
daniel	O
selsam	O
david	O
sontag	O
erik	O
sudderth	O
josh	O
tenenbaum	O
kai	O
yu	O
martin	O
wainwright	O
yair	O
weiss	O
kevin	O
patrick	O
murphy	O
palo	O
alto	O
california	O
june	O
introduction	O
machine	B
learning	B
what	O
and	O
why	O
we	O
are	O
drowning	O
in	O
information	B
and	O
starving	O
for	O
knowledge	O
john	O
naisbitt	O
we	O
are	O
entering	O
the	O
era	O
of	O
big	B
data	I
for	O
example	O
there	O
are	O
about	O
trillion	O
web	O
one	O
hour	O
of	O
video	O
is	O
uploaded	O
to	O
youtube	O
every	O
second	O
amounting	O
to	O
years	O
of	O
content	O
every	O
the	O
genomes	O
of	O
of	O
people	O
each	O
of	O
which	O
has	O
a	O
length	O
of	O
base	O
pairs	O
have	O
been	O
sequenced	O
by	O
various	O
labs	O
walmart	O
handles	O
more	O
than	O
transactions	O
per	O
hour	O
and	O
has	O
databases	O
containing	O
more	O
than	O
petabytes	O
of	O
information	B
and	O
so	O
on	O
this	O
deluge	O
of	O
data	O
calls	O
for	O
automated	O
methods	O
of	O
data	O
analysis	O
which	O
is	O
what	O
machine	B
learning	B
provides	O
in	O
particular	O
we	O
define	O
machine	B
learning	B
as	O
a	O
set	O
of	O
methods	O
that	O
can	O
automatically	O
detect	O
patterns	O
in	O
data	O
and	O
then	O
use	O
the	O
uncovered	O
patterns	O
to	O
predict	O
future	O
data	O
or	O
to	O
perform	O
other	O
kinds	O
of	O
decision	B
making	O
under	O
uncertainty	B
as	O
planning	O
how	O
to	O
collect	O
more	O
data	O
this	O
books	O
adopts	O
the	O
view	O
that	O
the	O
best	O
way	O
to	O
solve	O
such	O
problems	O
is	O
to	O
use	O
the	O
tools	O
of	O
probability	B
theory	I
probability	B
theory	I
can	O
be	O
applied	O
to	O
any	O
problem	O
involving	O
uncertainty	B
in	O
machine	B
learning	B
uncertainty	B
comes	O
in	O
many	O
forms	O
what	O
is	O
the	O
best	O
prediction	O
about	O
the	O
future	O
given	O
some	O
past	O
data	O
what	O
is	O
the	O
best	O
model	O
to	O
explain	O
some	O
data	O
what	O
measurement	O
should	O
i	O
perform	O
next	O
etc	O
the	O
probabilistic	O
approach	O
to	O
machine	B
learning	B
is	O
closely	O
related	O
to	O
the	O
field	O
of	O
statistics	O
but	O
differs	O
slightly	O
in	O
terms	O
of	O
its	O
emphasis	O
and	O
we	O
will	O
describe	O
a	O
wide	O
variety	O
of	O
probabilistic	O
models	O
suitable	O
for	O
a	O
wide	O
variety	O
of	O
data	O
and	O
tasks	O
we	O
will	O
also	O
describe	O
a	O
wide	O
variety	O
of	O
algorithms	O
for	O
learning	B
and	O
using	O
such	O
models	O
the	O
goal	O
is	O
not	O
to	O
develop	O
a	O
cook	O
book	O
of	O
ad	O
hoc	O
techiques	O
but	O
instead	O
to	O
present	O
a	O
unified	O
view	O
of	O
the	O
field	O
through	O
the	O
lens	O
of	O
probabilistic	O
modeling	O
and	O
inference	B
although	O
we	O
will	O
pay	O
attention	O
to	O
computational	O
efficiency	O
details	O
on	O
how	O
to	O
scale	O
these	O
methods	O
to	O
truly	O
massive	O
datasets	O
are	O
better	O
described	O
in	O
other	O
books	O
such	O
as	O
and	O
ullman	O
bekkerman	O
et	O
al	O
source	O
httpwww	O
youtube	O
comtpress	O
statistics	O
rob	O
tibshirani	O
a	O
statistician	O
at	O
stanford	O
university	O
has	O
created	O
an	O
amusing	O
comparison	O
between	O
machine	B
learning	B
and	O
statistics	O
available	O
at	O
chapter	O
introduction	O
it	O
should	O
be	O
noted	O
however	O
that	O
even	O
when	O
one	O
has	O
an	O
apparently	O
massive	O
data	O
set	O
the	O
effective	O
number	O
of	O
data	O
points	O
for	O
certain	O
cases	O
of	O
interest	O
might	O
be	O
quite	O
small	O
in	O
fact	O
data	O
across	O
a	O
variety	O
of	O
domains	O
exhibits	O
a	O
property	O
known	O
as	O
the	O
long	B
tail	I
which	O
means	O
that	O
a	O
few	O
things	O
words	O
are	O
very	O
common	O
but	O
most	O
things	O
are	O
quite	O
rare	O
section	O
for	O
details	O
for	O
example	O
of	O
google	O
searches	O
each	O
day	O
have	O
never	O
been	O
seen	O
this	O
means	O
that	O
the	O
core	O
statistical	O
issues	O
that	O
we	O
discuss	O
in	O
this	O
book	O
concerning	O
generalizing	O
from	O
relatively	O
small	O
samples	B
sizes	O
are	O
still	O
very	O
relevant	O
even	O
in	O
the	O
big	B
data	I
era	O
types	O
of	O
machine	B
learning	B
in	O
the	O
predictive	B
or	O
supervised	O
machine	B
learning	B
is	O
usually	O
divided	O
into	O
two	O
main	O
types	O
learning	B
approach	O
the	O
goal	O
is	O
to	O
learn	O
a	O
mapping	O
from	O
inputs	O
x	O
to	O
outputs	O
y	O
given	O
a	O
labeled	O
set	O
of	O
input-output	O
pairs	O
d	O
yin	O
here	O
d	O
is	O
called	O
the	O
training	B
set	I
and	O
n	O
is	O
the	O
number	O
of	O
training	O
examples	O
in	O
the	O
simplest	O
setting	O
each	O
training	O
input	O
xi	O
is	O
a	O
d-dimensional	O
vector	O
of	O
numbers	O
representing	O
say	O
the	O
height	O
and	O
weight	O
of	O
a	O
person	O
these	O
are	O
called	O
features	B
attributes	B
or	O
covariates	B
in	O
general	O
however	O
xi	O
could	O
be	O
a	O
complex	O
structured	O
object	O
such	O
as	O
an	O
image	O
a	O
sentence	O
an	O
email	O
message	O
a	O
time	O
series	O
a	O
molecular	O
shape	O
a	O
graph	B
etc	O
similarly	O
the	O
form	O
of	O
the	O
output	O
or	O
response	B
variable	I
can	O
in	O
principle	O
be	O
anything	O
but	O
most	O
methods	O
assume	O
that	O
yi	O
is	O
a	O
categorical	B
or	O
nominal	B
variable	O
from	O
some	O
finite	O
set	O
yi	O
c	O
as	O
male	O
or	O
female	O
or	O
that	O
yi	O
is	O
a	O
real-valued	O
scalar	O
as	O
income	O
level	O
when	O
yi	O
is	O
categorical	B
the	O
problem	O
is	O
known	O
as	O
classification	O
or	O
pattern	B
recognition	I
and	O
when	O
yi	O
is	O
real-valued	O
the	O
problem	O
is	O
known	O
as	O
regression	B
another	O
variant	O
known	O
as	O
ordinal	B
regression	B
occurs	O
where	O
label	B
space	O
y	O
has	O
some	O
natural	O
ordering	O
such	O
as	O
grades	O
a	O
f	O
the	O
second	O
main	O
type	O
of	O
machine	B
learning	B
is	O
the	O
descriptive	B
or	O
unsupervised	B
learning	B
approach	O
here	O
we	O
are	O
only	O
given	O
inputs	O
d	O
and	O
the	O
goal	O
is	O
to	O
find	O
interesting	O
patterns	O
in	O
the	O
data	O
this	O
is	O
sometimes	O
called	O
knowledge	B
discovery	I
this	O
is	O
a	O
much	O
less	O
well-defined	O
problem	O
since	O
we	O
are	O
not	O
told	O
what	O
kinds	O
of	O
patterns	O
to	O
look	O
for	O
and	O
there	O
is	O
no	O
obvious	O
error	O
metric	B
to	O
use	O
supervised	B
learning	B
where	O
we	O
can	O
compare	O
our	O
prediction	O
of	O
y	O
for	O
a	O
given	O
x	O
to	O
the	O
observed	O
value	O
there	O
is	O
a	O
third	O
type	O
of	O
machine	B
learning	B
known	O
as	O
reinforcement	B
learning	B
which	O
is	O
somewhat	O
less	O
commonly	O
used	O
this	O
is	O
useful	O
for	O
learning	B
how	O
to	O
act	O
or	O
behave	O
when	O
given	O
occasional	O
reward	B
or	O
punishment	O
signals	O
example	O
consider	O
how	O
a	O
baby	O
learns	O
to	O
walk	O
unfortunately	O
rl	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
although	O
we	O
do	O
discuss	O
decision	B
theory	O
in	O
section	O
which	O
is	O
the	O
basis	O
of	O
rl	O
see	O
e	O
g	O
et	O
al	O
sutton	O
and	O
barto	O
russell	O
and	O
norvig	O
szepesvari	O
wiering	O
and	O
van	O
otterlo	O
for	O
more	O
information	B
on	O
rl	O
httpcertifiedknowledge	O
orgblogare-search-queries-becoming-even-more-unique-statistic	O
s-from-google	O
supervised	B
learning	B
figure	O
left	O
some	O
labeled	O
training	O
examples	O
of	O
colored	O
shapes	O
along	O
with	O
unlabeled	O
test	O
cases	O
right	O
representing	O
the	O
training	O
data	O
as	O
an	O
n	O
d	O
design	B
matrix	I
row	O
i	O
represents	O
the	O
feature	O
vector	O
xi	O
the	O
last	O
column	O
is	O
the	O
label	B
yi	O
based	O
on	O
a	O
figure	O
by	O
leslie	O
kaelbling	O
supervised	B
learning	B
we	O
begin	O
our	O
investigation	O
of	O
machine	B
learning	B
by	O
discussing	O
supervised	B
learning	B
which	O
is	O
the	O
form	O
of	O
ml	O
most	O
widely	O
used	O
in	O
practice	O
classification	O
in	O
this	O
section	O
we	O
discuss	O
classification	O
here	O
the	O
goal	O
is	O
to	O
learn	O
a	O
mapping	O
from	O
inputs	O
x	O
to	O
outputs	O
y	O
where	O
y	O
c	O
with	O
c	O
being	O
the	O
number	O
of	O
classes	O
if	O
c	O
this	O
is	O
called	O
binary	O
classification	O
which	O
case	O
we	O
often	O
assume	O
y	O
if	O
c	O
this	O
is	O
called	O
multiclass	O
classification	O
if	O
the	O
class	O
labels	O
are	O
not	O
mutually	O
exclusive	O
somebody	O
may	O
be	O
classified	O
as	O
tall	O
and	O
strong	O
we	O
call	O
it	O
multi-label	O
classification	O
but	O
this	O
is	O
best	O
viewed	O
as	O
predicting	O
multiple	O
related	O
binary	O
class	O
labels	O
so-called	O
multiple	B
output	I
model	I
when	O
we	O
use	O
the	O
term	O
classification	O
we	O
will	O
mean	B
multiclass	O
classification	O
with	O
a	O
single	O
output	O
unless	O
we	O
state	B
otherwise	O
one	O
way	O
to	O
formalize	O
the	O
problem	O
is	O
as	O
function	B
approximation	I
we	O
assume	O
y	O
f	O
for	O
some	O
unknown	B
function	O
f	O
and	O
the	O
goal	O
of	O
learning	B
is	O
to	O
estimate	O
the	O
function	O
f	O
given	O
a	O
labeled	O
training	B
set	I
and	O
then	O
to	O
make	O
predictions	O
using	O
y	O
f	O
use	O
the	O
hat	O
symbol	O
to	O
denote	O
an	O
estimate	O
our	O
main	O
goal	O
is	O
to	O
make	O
predictions	O
on	O
novel	O
inputs	O
meaning	O
ones	O
that	O
we	O
have	O
not	O
seen	O
before	O
is	O
called	O
generalization	B
since	O
predicting	O
the	O
response	O
on	O
the	O
training	B
set	I
is	O
easy	O
can	O
just	O
look	O
up	O
the	O
answer	O
example	O
as	O
a	O
simple	O
toy	O
example	O
of	O
classification	O
consider	O
the	O
problem	O
illustrated	O
in	O
figure	O
we	O
have	O
two	O
classes	O
of	O
object	O
which	O
correspond	O
to	O
labels	O
and	O
the	O
inputs	O
are	O
colored	O
shapes	O
these	O
have	O
been	O
described	O
by	O
a	O
set	O
of	O
d	O
features	B
or	O
attributes	B
which	O
are	O
stored	O
in	O
an	O
n	O
d	O
design	B
matrix	I
x	O
shown	O
in	O
figure	O
the	O
input	O
features	B
x	O
can	O
be	O
discrete	B
continuous	O
or	O
a	O
combination	O
of	O
the	O
two	O
in	O
addition	O
to	O
the	O
inputs	O
we	O
have	O
a	O
vector	O
of	O
training	O
labels	O
y	O
in	O
figure	O
the	O
test	O
cases	O
are	O
a	O
blue	O
crescent	O
a	O
yellow	O
circle	O
and	O
a	O
blue	O
arrow	O
none	O
of	O
these	O
have	O
been	O
seen	O
before	O
thus	O
we	O
are	O
required	O
to	O
generalize	B
beyond	O
the	O
training	B
set	I
a	O
chapter	O
introduction	O
reasonable	O
guess	O
is	O
that	O
blue	O
crescent	O
should	O
be	O
y	O
since	O
all	O
blue	O
shapes	O
are	O
labeled	O
in	O
the	O
training	B
set	I
the	O
yellow	O
circle	O
is	O
harder	O
to	O
classify	O
since	O
some	O
yellow	O
things	O
are	O
labeled	O
y	O
and	O
some	O
are	O
labeled	O
y	O
and	O
some	O
circles	O
are	O
labeled	O
y	O
and	O
some	O
y	O
consequently	O
it	O
is	O
not	O
clear	O
what	O
the	O
right	O
label	B
should	O
be	O
in	O
the	O
case	O
of	O
the	O
yellow	O
circle	O
similarly	O
the	O
correct	O
label	B
for	O
the	O
blue	O
arrow	O
is	O
unclear	O
the	O
need	O
for	O
probabilistic	O
predictions	O
to	O
handle	O
ambiguous	O
cases	O
such	O
as	O
the	O
yellow	O
circle	O
above	O
it	O
is	O
desirable	O
to	O
return	O
a	O
probability	O
the	O
reader	O
is	O
assumed	O
to	O
already	O
have	O
some	O
familiarity	O
with	O
basic	O
concepts	O
in	O
probability	O
if	O
not	O
please	O
consult	O
chapter	O
for	O
a	O
refresher	O
if	O
necessary	O
we	O
will	O
denote	O
the	O
probability	O
distribution	O
over	O
possible	O
labels	O
given	O
the	O
input	O
vector	O
x	O
and	O
training	B
set	I
d	O
by	O
pyxd	O
in	O
general	O
this	O
represents	O
a	O
vector	O
of	O
length	O
c	O
there	O
are	O
just	O
two	O
classes	O
it	O
is	O
sufficient	O
to	O
return	O
the	O
single	O
number	O
py	O
since	O
py	O
py	O
in	O
our	O
notation	O
we	O
make	O
explicit	O
that	O
the	O
probability	O
is	O
conditional	O
on	O
the	O
test	O
input	O
x	O
as	O
well	O
as	O
the	O
training	B
set	I
d	O
by	O
putting	O
these	O
terms	O
on	O
the	O
right	O
hand	O
side	O
of	O
the	O
conditioning	B
bar	O
we	O
are	O
also	O
implicitly	O
conditioning	B
on	O
the	O
form	O
of	O
model	O
that	O
we	O
use	O
to	O
make	O
predictions	O
when	O
choosing	O
between	O
different	O
models	O
we	O
will	O
make	O
this	O
assumption	O
explicit	O
by	O
writing	O
pyxd	O
m	O
where	O
m	O
denotes	O
the	O
model	O
however	O
if	O
the	O
model	O
is	O
clear	O
from	O
context	O
we	O
will	O
drop	O
m	O
from	O
our	O
notation	O
for	O
brevity	O
given	O
a	O
probabilistic	O
output	O
we	O
can	O
always	O
compute	O
our	O
best	O
guess	O
as	O
to	O
the	O
true	O
label	B
using	O
y	O
f	O
c	O
argmax	O
py	O
cxd	O
this	O
corresponds	O
to	O
the	O
most	O
probable	O
class	O
label	B
and	O
is	O
called	O
the	O
mode	B
of	O
the	O
distribution	O
pyxd	O
it	O
is	O
also	O
known	O
as	O
a	O
map	B
estimate	I
stands	O
for	O
maximum	B
a	I
posteriori	I
using	O
the	O
most	O
probable	O
label	B
makes	O
intuitive	O
sense	O
but	O
we	O
will	O
give	O
a	O
more	O
formal	O
justification	O
for	O
this	O
procedure	O
in	O
section	O
now	O
consider	O
a	O
case	O
such	O
as	O
the	O
yellow	O
circle	O
where	O
p	O
yxd	O
is	O
far	O
from	O
in	O
such	O
a	O
case	O
we	O
are	O
not	O
very	O
confident	O
of	O
our	O
answer	O
so	O
it	O
might	O
be	O
better	O
to	O
say	O
i	O
don	O
t	O
know	O
instead	O
of	O
returning	O
an	O
answer	O
that	O
we	O
don	O
t	O
really	O
trust	O
this	O
is	O
particularly	O
important	O
in	O
domains	O
such	O
as	O
medicine	O
and	O
finance	O
where	O
we	O
may	O
be	O
risk	B
averse	I
as	O
we	O
explain	O
in	O
section	O
another	O
application	O
where	O
it	O
is	O
important	O
to	O
assess	O
risk	B
is	O
when	O
playing	O
tv	O
game	O
shows	O
such	O
as	O
jeopardy	B
in	O
this	O
game	O
contestants	O
have	O
to	O
solve	O
various	O
word	O
puzzles	O
and	O
answer	O
a	O
variety	O
of	O
trivia	O
questions	O
but	O
if	O
they	O
answer	O
incorrectly	O
they	O
lose	O
money	O
in	O
ibm	O
unveiled	O
a	O
computer	O
system	O
called	O
watson	B
which	O
beat	O
the	O
top	O
human	O
jeopardy	B
champion	O
watson	B
uses	O
a	O
variety	O
of	O
interesting	O
techniques	O
et	O
al	O
but	O
the	O
most	O
pertinent	O
one	O
for	O
our	O
present	O
purposes	O
is	O
that	O
it	O
contains	O
a	O
module	O
that	O
estimates	O
how	O
confident	O
it	O
is	O
of	O
its	O
answer	O
the	O
system	O
only	O
chooses	O
to	O
buzz	O
in	O
its	O
answer	O
if	O
sufficiently	O
confident	O
it	O
is	O
correct	O
similarly	O
google	O
has	O
a	O
system	O
known	O
as	O
smartass	B
selection	O
system	O
that	O
predicts	O
the	O
probability	O
you	O
will	O
click	O
on	O
an	O
ad	O
based	O
on	O
your	O
search	O
history	O
and	O
other	O
user	O
and	O
ad-specific	O
features	B
this	O
probability	O
is	O
known	O
as	O
the	O
click-through	B
rate	B
or	O
ctr	B
and	O
can	O
be	O
used	O
to	O
maximize	O
expected	O
profit	O
we	O
will	O
discuss	O
some	O
of	O
the	O
basic	O
principles	O
behind	O
systems	O
such	O
as	O
smartass	B
later	O
in	O
this	O
book	O
supervised	B
learning	B
s	O
t	O
n	O
e	O
m	O
u	O
c	O
o	O
d	O
words	O
figure	O
subset	O
of	O
size	O
x	O
of	O
the	O
data	O
we	O
only	O
show	O
rows	O
for	O
clarity	O
each	O
row	O
is	O
a	O
document	O
as	O
a	O
bag-of-words	B
bit	O
vector	O
each	O
column	O
is	O
a	O
word	O
the	O
red	O
lines	O
separate	O
the	O
classes	O
which	O
are	O
descending	O
order	O
comp	O
rec	O
sci	O
talk	O
are	O
the	O
titles	O
of	O
usenet	O
groups	O
we	O
can	O
see	O
that	O
there	O
are	O
subsets	O
of	O
words	O
whose	O
presence	O
or	O
absence	O
is	O
indicative	O
of	O
the	O
class	O
the	O
data	O
is	O
available	O
from	O
httpcs	O
nyu	O
eduroweisdata	O
html	O
figure	O
generated	O
by	O
newsgroupsvisualize	O
real-world	O
applications	O
classification	O
is	O
probably	O
the	O
most	O
widely	O
used	O
form	O
of	O
machine	B
learning	B
and	O
has	O
been	O
used	O
to	O
solve	O
many	O
interesting	O
and	O
often	O
difficult	O
real-world	O
problems	O
we	O
have	O
already	O
mentioned	O
some	O
important	O
applciations	O
we	O
give	O
a	O
few	O
more	O
examples	O
below	O
document	O
classification	O
and	O
email	O
spam	B
filtering	B
in	O
document	O
classification	O
the	O
goal	O
is	O
to	O
classify	O
a	O
document	O
such	O
as	O
a	O
web	O
page	O
or	O
email	O
message	O
into	O
one	O
of	O
c	O
classes	O
that	O
is	O
to	O
compute	O
py	O
cxd	O
where	O
x	O
is	O
some	O
representation	O
of	O
the	O
text	O
a	O
special	O
case	O
of	O
this	O
is	O
email	O
spam	B
filtering	B
where	O
the	O
classes	O
are	O
spam	B
y	O
or	O
ham	B
y	O
most	O
classifiers	O
assume	O
that	O
the	O
input	O
vector	O
x	O
has	O
a	O
fixed	O
size	O
a	O
common	O
way	O
to	O
represent	O
variable-length	O
documents	O
in	O
feature-vector	O
format	O
is	O
to	O
use	O
a	O
bag	B
of	I
words	I
representation	O
this	O
is	O
explained	O
in	O
detail	O
in	O
section	O
but	O
the	O
basic	O
idea	O
is	O
to	O
define	O
xij	O
iff	B
word	O
j	O
occurs	O
in	O
document	O
i	O
if	O
we	O
apply	O
this	O
transformation	O
to	O
every	O
document	O
in	O
our	O
data	O
set	O
we	O
get	O
a	O
binary	O
document	O
word	O
co-occurrence	B
matrix	I
see	O
figure	O
for	O
an	O
example	O
essentially	O
the	O
document	O
classification	O
problem	O
has	O
been	O
reduced	O
to	O
one	O
that	O
looks	O
for	O
subtle	O
changes	O
in	O
the	O
pattern	B
of	O
bits	B
for	O
example	O
we	O
may	O
notice	O
that	O
most	O
spam	B
messages	O
have	O
a	O
high	O
probability	O
of	O
containing	O
the	O
words	O
buy	O
cheap	O
viagra	O
etc	O
in	O
exercise	O
and	O
exercise	O
you	O
will	O
get	O
hands-on	O
experience	O
applying	O
various	O
classification	O
techniques	O
to	O
the	O
spam	B
filtering	B
problem	O
chapter	O
introduction	O
figure	O
three	O
types	O
of	O
iris	B
flowers	O
setosa	O
versicolor	O
and	O
virginica	O
source	O
httpwww	O
statlab	O
u	O
ni-heidelberg	O
dedatairis	O
used	O
with	O
kind	O
permission	O
of	O
dennis	O
kramb	O
and	O
signa	O
sepal	O
length	O
sepal	O
width	O
petal	O
length	O
petal	O
width	O
h	O
t	O
g	O
n	O
e	O
l	O
l	O
a	O
p	O
e	O
s	O
h	O
t	O
i	O
d	O
w	O
l	O
a	O
p	O
e	O
s	O
h	O
t	O
g	O
n	O
e	O
l	O
l	O
a	O
t	O
e	O
p	O
t	O
h	O
d	O
w	O
i	O
l	O
a	O
t	O
e	O
p	O
figure	O
visualization	O
of	O
the	O
iris	B
data	O
as	O
a	O
pairwise	O
scatter	B
plot	I
the	O
diagonal	B
plots	O
the	O
marginal	O
histograms	O
of	O
the	O
features	B
the	O
off	O
diagonals	O
contain	O
scatterplots	O
of	O
all	O
possible	O
pairs	O
of	O
features	B
red	O
circle	O
setosa	O
green	O
diamond	O
versicolor	O
blue	O
star	O
virginica	O
figure	O
generated	O
by	O
fisheririsdemo	O
classifying	O
flowers	O
figure	O
gives	O
another	O
example	O
of	O
classification	O
due	O
to	O
the	O
statistician	O
ronald	O
fisher	O
the	O
goal	O
is	O
to	O
learn	O
to	O
distinguish	O
three	O
different	O
kinds	O
of	O
iris	B
flower	O
called	O
setosa	O
versicolor	O
and	O
virginica	O
fortunately	O
rather	O
than	O
working	O
directly	O
with	O
images	O
a	O
botanist	O
has	O
already	O
extracted	O
useful	O
feature	O
features	B
or	O
characteristics	O
sepal	O
length	O
and	O
width	O
and	O
petal	O
length	O
and	O
width	O
extraction	O
is	O
an	O
important	O
but	O
difficult	O
task	O
most	O
machine	B
learning	B
methods	O
use	O
features	B
chosen	O
by	O
some	O
human	O
later	O
we	O
will	O
discuss	O
some	O
methods	O
that	O
can	O
learn	O
good	O
features	B
from	O
the	O
data	O
if	O
we	O
make	O
a	O
scatter	B
plot	I
of	O
the	O
iris	B
data	O
as	O
in	O
figure	O
we	O
see	O
that	O
it	O
is	O
easy	O
to	O
distinguish	O
setosas	O
circles	O
from	O
the	O
other	O
two	O
classes	O
by	O
just	O
checking	O
if	O
their	O
petal	O
length	O
supervised	B
learning	B
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
true	O
class	O
first	O
test	O
mnist	B
gray-scale	O
images	O
figure	O
same	O
as	O
but	O
with	O
the	O
features	B
permuted	O
randomly	O
classification	O
performance	O
is	O
identical	O
on	O
both	O
versions	O
of	O
the	O
data	O
the	O
training	O
data	O
is	O
permuted	O
in	O
an	O
identical	O
way	O
figure	O
generated	O
by	O
shuffleddigitsdemo	O
or	O
width	O
is	O
below	O
some	O
threshold	O
however	O
distinguishing	O
versicolor	O
from	O
virginica	O
is	O
slightly	O
harder	O
any	O
decision	B
will	O
need	O
to	O
be	O
based	O
on	O
at	O
least	O
two	O
features	B
is	O
always	O
a	O
good	O
idea	O
to	O
perform	O
exploratory	B
data	I
analysis	I
such	O
as	O
plotting	O
the	O
data	O
before	O
applying	O
a	O
machine	B
learning	B
method	O
image	O
classification	O
and	O
handwriting	B
recognition	I
now	O
consider	O
the	O
harder	O
problem	O
of	O
classifying	O
images	O
directly	O
where	O
a	O
human	O
has	O
not	O
preprocessed	O
the	O
data	O
we	O
might	O
want	O
to	O
classify	O
the	O
image	O
as	O
a	O
whole	O
e	O
g	O
is	O
it	O
an	O
indoors	O
or	O
outdoors	O
scene	O
is	O
it	O
a	O
horizontal	O
or	O
vertical	O
photo	O
does	O
it	O
contain	O
a	O
dog	O
or	O
not	O
this	O
is	O
called	O
image	O
classification	O
in	O
the	O
special	O
case	O
that	O
the	O
images	O
consist	O
of	O
isolated	O
handwritten	O
letters	O
and	O
digits	O
for	O
example	O
in	O
a	O
postal	O
or	O
zip	O
code	O
on	O
a	O
letter	O
we	O
can	O
use	O
classification	O
to	O
perform	O
handwriting	B
recognition	I
a	O
standard	O
dataset	O
used	O
in	O
this	O
area	O
is	O
known	O
as	O
mnist	B
which	O
stands	O
for	O
modified	O
national	O
institute	O
of	O
standards	O
term	O
modified	O
is	O
used	O
because	O
the	O
images	O
have	O
been	O
preprocessed	O
to	O
ensure	O
the	O
digits	O
are	O
mostly	O
in	O
the	O
center	O
of	O
the	O
image	O
this	O
dataset	O
contains	O
training	O
images	O
and	O
test	O
images	O
of	O
the	O
digits	O
to	O
as	O
written	O
by	O
various	O
people	O
the	O
images	O
are	O
size	O
and	O
have	O
grayscale	O
values	O
in	O
the	O
range	O
see	O
figure	O
for	O
some	O
example	O
images	O
many	O
generic	O
classification	O
methods	O
ignore	O
any	O
structure	O
in	O
the	O
input	O
features	B
such	O
as	O
spatial	O
layout	O
consequently	O
they	O
can	O
also	O
just	O
as	O
easily	O
handle	O
data	O
that	O
looks	O
like	O
figure	O
which	O
is	O
the	O
same	O
data	O
except	O
we	O
have	O
randomly	O
permuted	O
the	O
order	O
of	O
all	O
the	O
features	B
will	O
verify	O
this	O
in	O
exercise	O
this	O
flexibility	O
is	O
both	O
a	O
blessing	O
the	O
methods	O
are	O
general	O
purpose	O
and	O
a	O
curse	O
the	O
methods	O
ignore	O
an	O
obviously	O
useful	O
source	O
of	O
information	B
we	O
will	O
discuss	O
methods	O
for	O
exploiting	O
structure	O
in	O
the	O
input	O
features	B
later	O
in	O
the	O
book	O
available	O
from	O
httpyann	O
lecun	O
comexdbmnist	O
chapter	O
introduction	O
figure	O
example	O
of	O
face	B
detection	I
input	O
image	O
family	B
photo	O
taken	O
august	O
used	O
with	O
kind	O
permission	O
of	O
bernard	O
diedrich	O
of	O
sherwood	O
studios	O
output	O
of	O
classifier	O
which	O
detected	O
faces	O
at	O
different	O
poses	O
this	O
was	O
produced	O
using	O
the	O
online	O
demo	O
at	O
httpdemo	O
pittpatt	O
com	O
the	O
classifier	O
was	O
trained	O
on	O
of	O
manually	O
labeled	O
images	O
of	O
faces	O
and	O
non-faces	O
and	O
then	O
was	O
applied	O
to	O
a	O
dense	O
set	O
of	O
overlapping	O
patches	O
in	O
the	O
test	O
image	O
only	O
the	O
patches	O
whose	O
probability	O
of	O
containing	O
a	O
face	O
was	O
sufficiently	O
high	O
were	O
returned	O
used	O
with	O
kind	O
permission	O
of	O
pittpatt	O
com	O
face	B
detection	I
and	O
recognition	O
a	O
harder	O
problem	O
is	O
to	O
find	O
objects	O
within	O
an	O
image	O
this	O
is	O
called	O
object	B
detection	I
or	O
object	B
localization	I
an	O
important	O
special	O
case	O
of	O
this	O
is	O
face	B
detection	I
one	O
approach	O
to	O
this	O
problem	O
is	O
to	O
divide	O
the	O
image	O
into	O
many	O
small	O
overlapping	O
patches	O
at	O
different	O
locations	O
scales	O
and	O
orientations	O
and	O
to	O
classify	O
each	O
such	O
patch	O
based	O
on	O
whether	O
it	O
contains	O
face-like	O
texture	O
or	O
not	O
this	O
is	O
called	O
a	O
sliding	B
window	I
detector	I
the	O
system	O
then	O
returns	O
those	O
locations	O
where	O
the	O
probability	O
of	O
face	O
is	O
sufficiently	O
high	O
see	O
figure	O
for	O
an	O
example	O
such	O
face	B
detection	I
systems	O
are	O
built-in	O
to	O
most	O
modern	O
digital	B
cameras	I
the	O
locations	O
of	O
the	O
detected	O
faces	O
are	O
used	O
to	O
determine	O
the	O
center	O
of	O
the	O
auto-focus	O
another	O
application	O
is	O
automatically	O
blurring	O
out	O
faces	O
in	O
google	O
s	O
streetview	B
system	O
having	O
found	O
the	O
faces	O
one	O
can	O
then	O
proceed	O
to	O
perform	O
face	B
recognition	I
which	O
means	O
estimating	O
the	O
identity	O
of	O
the	O
person	O
figure	O
in	O
this	O
case	O
the	O
number	O
of	O
class	O
labels	O
might	O
be	O
very	O
large	O
also	O
the	O
features	B
one	O
should	O
use	O
are	O
likely	O
to	O
be	O
different	O
than	O
in	O
the	O
face	B
detection	I
problem	O
for	O
recognition	O
subtle	O
differences	O
between	O
faces	O
such	O
as	O
hairstyle	O
may	O
be	O
important	O
for	O
determining	O
identity	O
but	O
for	O
detection	O
it	O
is	O
important	O
to	O
be	O
invariant	B
to	O
such	O
details	O
and	O
to	O
just	O
focus	O
on	O
the	O
differences	O
between	O
faces	O
and	O
non-faces	O
for	O
more	O
information	B
about	O
visual	O
object	B
detection	I
see	O
e	O
g	O
regression	B
regression	B
is	O
just	O
like	O
classification	O
except	O
the	O
response	B
variable	I
is	O
continuous	O
figure	O
shows	O
a	O
simple	O
example	O
we	O
have	O
a	O
single	O
real-valued	O
input	O
xi	O
r	O
and	O
a	O
single	O
real-valued	O
response	O
yi	O
r	O
we	O
consider	O
fitting	O
two	O
models	O
to	O
the	O
data	O
a	O
straight	O
line	O
and	O
a	O
quadratic	O
function	O
explain	O
how	O
to	O
fit	O
such	O
models	O
below	O
various	O
extensions	O
of	O
this	O
basic	O
problem	O
can	O
arise	O
such	O
as	O
having	O
high-dimensional	O
inputs	O
outliers	B
non-smooth	B
responses	O
etc	O
we	O
will	O
discuss	O
ways	O
to	O
handle	O
such	O
problems	O
later	O
in	O
the	O
book	O
unsupervised	B
learning	B
degree	B
degree	B
figure	O
figure	O
generated	O
by	O
linregpolyvsdegree	O
linear	B
regression	B
on	O
some	O
data	O
same	O
data	O
with	O
polynomial	B
regression	B
here	O
are	O
some	O
examples	O
of	O
real-world	O
regression	B
problems	O
predict	O
tomorrow	O
s	O
stock	O
market	O
price	O
given	O
current	O
market	O
conditions	O
and	O
other	O
possible	O
side	B
information	B
predict	O
the	O
age	O
of	O
a	O
viewer	O
watching	O
a	O
given	O
video	O
on	O
youtube	O
predict	O
the	O
location	O
in	O
space	O
of	O
a	O
robot	O
arm	O
end	B
effector	I
given	O
control	O
signals	O
sent	O
to	O
its	O
various	O
motors	O
predict	O
the	O
amount	O
of	O
prostate	O
specific	O
antigen	O
in	O
the	O
body	O
as	O
a	O
function	O
of	O
a	O
number	O
of	O
different	O
clinical	O
measurements	O
predict	O
the	O
temperature	B
at	O
any	O
location	O
inside	O
a	O
building	O
using	O
weather	O
data	O
time	O
door	O
sensors	O
etc	O
unsupervised	B
learning	B
we	O
now	O
consider	O
unsupervised	B
learning	B
where	O
we	O
are	O
just	O
given	O
output	O
data	O
without	O
any	O
inputs	O
the	O
goal	O
is	O
to	O
discover	O
interesting	O
structure	O
in	O
the	O
data	O
this	O
is	O
sometimes	O
called	O
knowledge	B
discovery	I
unlike	O
supervised	B
learning	B
we	O
are	O
not	O
told	O
what	O
the	O
desired	O
output	O
is	O
instead	O
we	O
will	O
formalize	O
our	O
task	O
as	O
one	O
of	O
density	B
estimation	I
that	O
is	O
we	O
for	O
each	O
input	O
want	O
to	O
build	O
models	O
of	O
the	O
form	O
pxi	O
there	O
are	O
two	O
differences	O
from	O
the	O
supervised	O
case	O
first	O
we	O
have	O
written	O
pxi	O
instead	O
of	O
pyixi	O
that	O
is	O
supervised	B
learning	B
is	O
conditional	O
density	B
estimation	I
whereas	O
unsupervised	B
learning	B
is	O
unconditional	O
density	B
estimation	I
second	O
xi	O
is	O
a	O
vector	O
of	O
features	B
so	O
we	O
need	O
to	O
create	O
multivariate	O
probability	O
models	O
by	O
contrast	O
in	O
supervised	B
learning	B
yi	O
is	O
usually	O
just	O
a	O
single	O
variable	O
that	O
we	O
are	O
trying	O
to	O
predict	O
this	O
means	O
that	O
for	O
most	O
supervised	B
learning	B
problems	O
we	O
can	O
use	O
univariate	O
probability	O
models	O
input-dependent	O
parameters	O
which	O
significantly	O
simplifies	O
the	O
problem	O
will	O
discuss	O
multi-output	O
classification	O
in	O
chapter	O
where	O
we	O
will	O
see	O
that	O
it	O
also	O
involves	O
multivariate	O
probability	O
models	O
unsupervised	B
learning	B
is	O
arguably	O
more	O
typical	O
of	O
human	O
and	O
animal	O
learning	B
it	O
is	O
also	O
more	O
widely	O
applicable	O
than	O
supervised	B
learning	B
since	O
it	O
does	O
not	O
require	O
a	O
human	O
expert	O
to	O
chapter	O
introduction	O
t	O
i	O
h	O
g	O
e	O
w	O
height	O
t	O
i	O
h	O
g	O
e	O
w	O
height	O
figure	O
the	O
height	O
and	O
weight	O
of	O
some	O
people	O
figure	O
generated	O
by	O
kmeansheightweight	O
a	O
possible	O
clustering	B
using	O
k	O
clusters	B
manually	O
label	B
the	O
data	O
labeled	O
data	O
is	O
not	O
only	O
expensive	O
to	O
but	O
it	O
also	O
contains	O
relatively	O
little	O
information	B
certainly	O
not	O
enough	O
to	O
reliably	O
estimate	O
the	O
parameters	O
of	O
complex	O
models	O
geoff	O
hinton	O
who	O
is	O
a	O
famous	O
professor	O
of	O
ml	O
at	O
the	O
university	O
of	O
toronto	O
has	O
said	O
when	O
we	O
re	O
learning	B
to	O
see	O
nobody	O
s	O
telling	O
us	O
what	O
the	O
right	O
answers	O
are	O
we	O
just	O
look	O
every	O
so	O
often	O
your	O
mother	O
says	O
that	O
s	O
a	O
dog	O
but	O
that	O
s	O
very	O
little	O
information	B
you	O
d	O
be	O
lucky	O
if	O
you	O
got	O
a	O
few	O
bits	B
of	O
information	B
even	O
one	O
bit	O
per	O
second	O
that	O
way	O
the	O
brain	O
s	O
visual	O
system	O
has	O
neural	O
connections	O
and	O
you	O
only	O
live	O
for	O
seconds	O
so	O
it	O
s	O
no	O
use	O
learning	B
one	O
bit	O
per	O
second	O
you	O
need	O
more	O
like	O
bits	B
per	O
second	O
and	O
there	O
s	O
only	O
one	O
place	O
you	O
can	O
get	O
that	O
much	O
information	B
from	O
the	O
input	O
itself	O
geoffrey	O
hinton	O
in	O
below	O
we	O
describe	O
some	O
canonical	O
examples	O
of	O
unsupervised	B
learning	B
discovering	O
clusters	B
as	O
a	O
canonical	O
example	O
of	O
unsupervised	B
learning	B
consider	O
the	O
problem	O
of	O
clustering	B
data	O
into	O
groups	O
for	O
example	O
figure	O
plots	O
some	O
data	O
representing	O
the	O
height	O
and	O
weight	O
of	O
a	O
group	O
of	O
people	O
it	O
seems	O
that	O
there	O
might	O
be	O
various	O
clusters	B
or	O
subgroups	O
although	O
it	O
is	O
not	O
clear	O
how	O
many	O
let	O
k	O
denote	O
the	O
number	O
of	O
clusters	B
our	O
first	O
goal	O
is	O
to	O
estimate	O
the	O
distribution	O
over	O
the	O
number	O
of	O
clusters	B
pkd	O
this	O
tells	O
us	O
if	O
there	O
are	O
subpopulations	O
within	O
the	O
data	O
for	O
simplicity	O
we	O
often	O
approximate	O
the	O
distribution	O
pkd	O
by	O
its	O
mode	B
arg	O
maxk	O
pkd	O
in	O
the	O
supervised	O
case	O
we	O
were	O
told	O
that	O
there	O
are	O
two	O
classes	O
k	O
and	O
female	O
but	O
in	O
the	O
unsupervised	O
case	O
we	O
are	O
free	O
to	O
choose	O
as	O
many	O
or	O
few	O
clusters	B
as	O
we	O
like	O
picking	O
a	O
model	O
of	O
the	O
right	O
complexity	O
is	O
called	O
model	B
selection	I
and	O
will	O
be	O
discussed	O
in	O
detail	O
below	O
our	O
second	O
goal	O
is	O
to	O
estimate	O
which	O
cluster	O
each	O
point	O
belongs	O
to	O
let	O
zi	O
k	O
is	O
an	O
example	O
of	O
a	O
hidden	B
or	O
represent	O
the	O
cluster	O
to	O
which	O
data	O
point	O
i	O
is	O
assigned	O
the	O
advent	O
of	O
crowd	B
sourcing	I
web	O
sites	O
such	O
as	O
mechanical	B
turk	I
which	O
outsource	O
data	O
processing	O
tasks	O
to	O
humans	O
all	O
over	O
the	O
world	O
has	O
reduced	O
the	O
cost	O
of	O
labeling	O
data	O
nevertheless	O
the	O
amount	O
of	O
unlabeled	O
data	O
is	O
still	O
orders	O
of	O
magnitude	O
larger	O
than	O
the	O
amount	O
of	O
labeled	O
data	O
unsupervised	B
learning	B
figure	O
a	O
set	O
of	O
points	O
that	O
live	O
on	O
a	O
linear	O
subspace	O
embedded	O
in	O
the	O
solid	O
red	O
line	O
is	O
the	O
first	O
principal	B
component	I
direction	O
the	O
dotted	O
black	O
line	O
is	O
the	O
second	O
pc	O
direction	O
representation	O
of	O
the	O
data	O
figure	O
generated	O
by	O
latent	B
variable	O
since	O
it	O
is	O
never	O
observed	O
in	O
the	O
training	B
set	I
we	O
can	O
infer	O
which	O
cluster	O
each	O
i	O
argmaxk	O
pzi	O
kxid	O
this	O
is	O
illustrated	O
in	O
data	O
point	O
belongs	O
to	O
by	O
computing	O
z	O
figure	O
where	O
we	O
use	O
different	O
colors	O
to	O
indicate	O
the	O
assignments	O
assuming	O
k	O
in	O
this	O
book	O
we	O
focus	O
on	O
model	B
based	I
clustering	B
which	O
means	O
we	O
fit	O
a	O
probabilistic	O
model	O
to	O
the	O
data	O
rather	O
than	O
running	O
some	O
ad	O
hoc	O
algorithm	O
the	O
advantages	O
of	O
the	O
model-based	O
approach	O
are	O
that	O
one	O
can	O
compare	O
different	O
kinds	O
of	O
models	O
in	O
an	O
objective	O
way	O
terms	O
of	O
the	O
likelihood	B
they	O
assign	O
to	O
the	O
data	O
we	O
can	O
combine	O
them	O
together	O
into	O
larger	O
systems	O
etc	O
here	O
are	O
some	O
real	O
world	O
applications	O
of	O
clustering	B
in	O
astronomy	O
the	O
autoclass	B
system	O
et	O
al	O
discovered	O
a	O
new	O
type	O
of	O
star	O
based	O
on	O
clustering	B
astrophysical	O
measurements	O
in	O
e-commerce	B
it	O
is	O
common	O
to	O
cluster	O
users	O
into	O
groups	O
based	O
on	O
their	O
purchasing	O
or	O
web-surfing	O
behavior	O
and	O
then	O
to	O
send	O
customized	O
targeted	O
advertising	O
to	O
each	O
group	O
e	O
g	O
in	O
biology	O
it	O
is	O
common	O
to	O
cluster	O
flow-cytometry	O
data	O
into	O
groups	O
to	O
discover	O
different	O
sub-populations	O
of	O
cells	O
e	O
g	O
et	O
al	O
discovering	O
latent	B
factors	B
when	O
dealing	O
with	O
high	O
dimensional	O
data	O
it	O
is	O
often	O
useful	O
to	O
reduce	O
the	O
dimensionality	O
by	O
projecting	O
the	O
data	O
to	O
a	O
lower	O
dimensional	O
subspace	O
which	O
captures	O
the	O
essence	O
of	O
the	O
data	O
this	O
is	O
called	O
dimensionality	B
reduction	I
a	O
simple	O
example	O
is	O
shown	O
in	O
figure	O
where	O
we	O
project	O
some	O
data	O
down	O
to	O
a	O
plane	O
the	O
approximation	O
is	O
quite	O
good	O
since	O
most	O
points	O
lie	O
close	O
to	O
this	O
subspace	O
reducing	O
to	O
would	O
involve	O
projecting	O
points	O
onto	O
the	O
red	O
line	O
in	O
figure	O
this	O
would	O
be	O
a	O
rather	O
poor	O
approximation	O
will	O
make	O
this	O
notion	O
precise	O
in	O
chapter	O
the	O
motivation	O
behind	O
this	O
technique	O
is	O
that	O
although	O
the	O
data	O
may	O
appear	O
high	O
dimensional	O
there	O
may	O
only	O
be	O
a	O
small	O
number	O
of	O
degrees	O
of	O
variability	O
corresponding	O
to	O
latent	B
factors	B
for	O
example	O
when	O
modeling	O
the	O
appearance	O
of	O
face	O
images	O
there	O
may	O
only	O
be	O
a	O
few	O
underlying	O
latent	B
factors	B
which	O
describe	O
most	O
of	O
the	O
variability	O
such	O
as	O
lighting	O
pose	O
identity	O
etc	O
as	O
illustrated	O
in	O
figure	O
chapter	O
introduction	O
figure	O
a	O
randomly	O
chosen	O
pixel	O
images	O
from	O
the	O
olivetti	O
face	O
database	O
and	O
the	O
first	O
three	O
principal	B
component	I
basis	O
vectors	O
figure	O
generated	O
by	O
pcaimagedemo	O
the	O
mean	B
when	O
used	O
as	O
input	O
to	O
other	O
statistical	O
models	O
such	O
low	O
dimensional	O
representations	O
often	O
result	O
in	O
better	O
predictive	B
accuracy	O
because	O
they	O
focus	O
on	O
the	O
essence	O
of	O
the	O
object	O
filtering	B
out	O
inessential	O
features	B
also	O
low	O
dimensional	O
representations	O
are	O
useful	O
for	O
enabling	O
fast	O
nearest	B
neighbor	I
searches	O
and	O
two	O
dimensional	O
projections	O
are	O
very	O
useful	O
for	O
visualizing	B
high	O
dimensional	O
data	O
the	O
most	O
common	O
approach	O
to	O
dimensionality	B
reduction	I
is	O
called	O
principal	B
components	I
analysis	I
or	O
pca	B
this	O
can	O
be	O
thought	O
of	O
as	O
an	O
unsupervised	O
version	O
of	O
linear	B
regression	B
where	O
we	O
observe	O
the	O
high-dimensional	O
response	O
y	O
but	O
not	O
the	O
low-dimensional	O
cause	O
z	O
thus	O
the	O
model	O
has	O
the	O
form	O
z	O
y	O
we	O
have	O
to	O
invert	O
the	O
arrow	O
and	O
infer	O
the	O
latent	B
low-dimensional	O
z	O
from	O
the	O
observed	O
high-dimensional	O
y	O
see	O
section	O
for	O
details	O
dimensionality	B
reduction	I
and	O
pca	B
in	O
particular	O
has	O
been	O
applied	O
in	O
many	O
different	O
areas	O
some	O
examples	O
include	O
the	O
following	O
in	O
biology	O
it	O
is	O
common	O
to	O
use	O
pca	B
to	O
interpret	O
gene	O
microarray	O
data	O
to	O
account	O
for	O
the	O
fact	O
that	O
each	O
measurement	O
is	O
usually	O
the	O
result	O
of	O
many	O
genes	O
which	O
are	O
correlated	O
in	O
their	O
behavior	O
by	O
the	O
fact	O
that	O
they	O
belong	O
to	O
different	O
biological	O
pathways	O
in	O
natural	O
language	O
processing	O
it	O
is	O
common	O
to	O
use	O
a	O
variant	O
of	O
pca	B
called	O
latent	B
semantic	I
analysis	I
for	O
document	O
retrieval	O
section	O
in	O
signal	B
processing	I
of	O
acoustic	O
or	O
neural	O
signals	O
it	O
is	O
common	O
to	O
use	O
ica	B
is	O
a	O
variant	O
of	O
pca	B
to	O
separate	O
signals	O
into	O
their	O
different	O
sources	O
section	O
in	O
computer	O
graphics	O
it	O
is	O
common	O
to	O
project	O
motion	O
capture	O
data	O
to	O
a	O
low	O
dimensional	O
space	O
and	O
use	O
it	O
to	O
create	O
animations	O
see	O
section	O
for	O
one	O
way	O
to	O
tackle	O
such	O
problems	O
unsupervised	B
learning	B
figure	O
a	O
sparse	B
undirected	B
gaussian	B
graphical	B
model	I
learned	O
using	O
graphical	B
lasso	B
applied	O
to	O
some	O
flow	O
cytometry	O
data	O
et	O
al	O
which	O
measures	O
the	O
phosphorylation	O
status	O
of	O
proteins	O
figure	O
generated	O
by	O
ggmlassodemo	O
discovering	O
graph	B
structure	O
sometimes	O
we	O
measure	O
a	O
set	O
of	O
correlated	O
variables	O
and	O
we	O
would	O
like	O
to	O
discover	O
which	O
ones	O
are	O
most	O
correlated	O
with	O
which	O
others	O
this	O
can	O
be	O
represented	O
by	O
a	O
graph	B
g	O
in	O
which	O
nodes	B
represent	O
variables	O
and	O
edges	B
represent	O
direct	O
dependence	O
between	O
variables	O
will	O
make	O
this	O
precise	O
in	O
chapter	O
when	O
we	O
discuss	O
graphical	B
models	I
we	O
can	O
then	O
learn	O
this	O
graph	B
structure	O
from	O
data	O
i	O
e	O
we	O
compute	O
g	O
argmax	O
pgd	O
as	O
with	O
unsupervised	B
learning	B
in	O
general	O
there	O
are	O
two	O
main	O
applications	O
for	O
learning	B
sparse	B
graphs	O
to	O
discover	O
new	O
knowledge	O
and	O
to	O
get	O
better	O
joint	O
probability	O
density	O
estimators	O
we	O
now	O
give	O
somes	O
example	O
of	O
each	O
much	O
of	O
the	O
motivation	O
for	O
learning	B
sparse	B
graphical	B
models	I
comes	O
from	O
the	O
systems	B
biology	I
community	O
for	O
example	O
suppose	O
we	O
measure	O
the	O
phosphorylation	O
status	O
of	O
some	O
proteins	O
in	O
a	O
cell	O
et	O
al	O
figure	O
gives	O
an	O
example	O
of	O
a	O
graph	B
structure	O
that	O
was	O
learned	O
from	O
this	O
data	O
methods	O
discussed	O
in	O
section	O
as	O
another	O
example	O
smith	O
et	O
al	O
showed	O
that	O
one	O
can	O
recover	O
the	O
neural	O
wiring	O
diagram	O
of	O
a	O
certain	O
kind	O
of	O
bird	O
from	O
time-series	O
eeg	O
data	O
the	O
recovered	O
structure	O
closely	O
matched	O
the	O
known	O
functional	O
connectivity	O
of	O
this	O
part	O
of	O
the	O
bird	O
brain	O
in	O
some	O
cases	O
we	O
are	O
not	O
interested	O
in	O
interpreting	O
the	O
graph	B
structure	O
we	O
just	O
want	O
to	O
use	O
it	O
to	O
model	O
correlations	O
and	O
to	O
make	O
predictions	O
one	O
example	O
of	O
this	O
is	O
in	O
financial	O
portfolio	O
management	O
where	O
accurate	O
models	O
of	O
the	O
covariance	B
between	O
large	O
numbers	O
of	O
different	O
stocks	O
is	O
important	O
carvalho	O
and	O
west	O
show	O
that	O
by	O
learning	B
a	O
sparse	B
graph	B
and	O
then	O
using	O
this	O
as	O
the	O
basis	O
of	O
a	O
trading	O
strategy	O
it	O
is	O
possible	O
to	O
outperform	O
make	O
more	O
money	O
than	O
methods	O
that	O
do	O
not	O
exploit	O
sparse	B
graphs	O
another	O
example	O
is	O
predicting	O
traffic	O
jams	O
on	O
the	O
freeway	O
horvitz	O
et	O
al	O
describe	O
a	O
deployed	O
system	O
called	O
jambayes	B
for	O
predicting	O
traffic	O
flow	O
in	O
the	O
seattle	O
area	O
predictions	O
are	O
made	O
using	O
a	O
graphical	B
model	I
whose	O
structure	O
was	O
learned	O
from	O
data	O
chapter	O
introduction	O
figure	O
a	O
noisy	O
image	O
with	O
an	O
occluder	O
an	O
estimate	O
of	O
the	O
underlying	O
pixel	O
intensities	O
based	O
on	O
a	O
pairwise	B
mrf	B
model	O
source	O
figure	O
of	O
and	O
huttenlocher	O
used	O
with	O
kind	O
permission	O
of	O
pedro	O
felzenszwalb	O
matrix	B
completion	I
sometimes	O
we	O
have	O
missing	B
data	I
that	O
is	O
variables	O
whose	O
values	O
are	O
unknown	B
for	O
example	O
we	O
might	O
have	O
conducted	O
a	O
survey	O
and	O
some	O
people	O
might	O
not	O
have	O
answered	O
certain	O
questions	O
or	O
we	O
might	O
have	O
various	O
sensors	O
some	O
of	O
which	O
fail	O
the	O
corresponding	O
design	B
matrix	I
will	O
then	O
have	O
holes	O
in	O
it	O
these	O
missing	B
entries	O
are	O
often	O
represented	O
by	O
nan	B
which	O
stands	O
for	O
not	O
a	O
number	O
the	O
goal	O
of	O
imputation	B
is	O
to	O
infer	O
plausible	O
values	O
for	O
the	O
missing	B
entries	O
this	O
is	O
sometimes	O
called	O
matrix	B
completion	I
below	O
we	O
give	O
some	O
example	O
applications	O
image	B
inpainting	I
an	O
interesting	O
example	O
of	O
an	O
imputation-like	O
task	O
is	O
known	O
as	O
image	B
inpainting	I
the	O
goal	O
is	O
to	O
fill	O
in	O
holes	O
due	O
to	O
scratches	O
or	O
occlusions	O
in	O
an	O
image	O
with	O
realistic	O
texture	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
denoise	O
the	O
image	O
as	O
well	O
as	O
impute	O
the	O
pixels	O
hidden	B
behind	O
the	O
occlusion	O
this	O
can	O
be	O
tackled	O
by	O
building	O
a	O
joint	O
probability	O
model	O
of	O
the	O
pixels	O
given	O
a	O
set	O
of	O
clean	O
images	O
and	O
then	O
inferring	O
the	O
unknown	B
variables	O
given	O
the	O
known	O
variables	O
this	O
is	O
somewhat	O
like	O
masket	O
basket	O
analysis	O
except	O
the	O
data	O
is	O
real-valued	O
and	O
spatially	O
structured	O
so	O
the	O
kinds	O
of	O
probability	O
models	O
we	O
use	O
are	O
quite	O
different	O
see	O
sections	O
and	O
for	O
some	O
possible	O
choices	O
collaborative	O
filtering	B
another	O
interesting	O
example	O
of	O
an	O
imputation-like	O
task	O
is	O
known	O
as	O
collaborative	O
filtering	B
a	O
common	O
example	O
of	O
this	O
concerns	O
predicting	O
which	O
movies	O
people	O
will	O
want	O
to	O
watch	O
based	O
on	O
how	O
they	O
and	O
other	O
people	O
have	O
rated	O
movies	O
which	O
they	O
have	O
already	O
seen	O
the	O
key	O
idea	O
is	O
that	O
the	O
prediction	O
is	O
not	O
based	O
on	O
features	B
of	O
the	O
movie	O
or	O
user	O
it	O
could	O
be	O
but	O
merely	O
on	O
a	O
ratings	O
matrix	O
more	O
precisely	O
we	O
have	O
a	O
matrix	O
x	O
where	O
xm	O
u	O
is	O
the	O
rating	O
unsupervised	B
learning	B
figure	O
example	O
of	O
movie-rating	O
data	O
training	O
data	O
is	O
in	O
red	O
test	O
data	O
is	O
denoted	O
by	O
empty	O
cells	O
are	O
unknown	B
an	O
integer	O
between	O
and	O
where	O
is	O
dislike	O
and	O
is	O
like	O
by	O
user	O
u	O
of	O
movie	O
m	O
note	O
that	O
most	O
of	O
the	O
entries	O
in	O
x	O
will	O
be	O
missing	B
or	O
unknown	B
since	O
most	O
users	O
will	O
not	O
have	O
rated	O
most	O
movies	O
hence	O
we	O
only	O
observe	O
a	O
tiny	O
subset	O
of	O
the	O
x	O
matrix	O
and	O
we	O
want	O
to	O
predict	O
in	O
particular	O
for	O
any	O
given	O
user	O
u	O
we	O
might	O
want	O
to	O
predict	O
which	O
of	O
the	O
a	O
different	O
subset	O
unrated	O
movies	O
heshe	O
is	O
most	O
likely	O
to	O
want	O
to	O
watch	O
launched	O
in	O
with	O
a	O
usd	O
prize	O
httpnetflixprize	O
com	O
in	O
order	O
to	O
encourage	O
research	O
in	O
this	O
area	O
the	O
dvd	O
rental	O
company	O
netflix	O
created	O
a	O
competition	O
in	O
particular	O
they	O
provided	O
a	O
large	O
matrix	O
of	O
ratings	O
on	O
a	O
scale	O
of	O
to	O
for	O
movies	O
created	O
by	O
users	O
the	O
full	B
matrix	O
would	O
have	O
entries	O
but	O
only	O
about	O
of	O
the	O
entries	O
are	O
observed	O
so	O
the	O
matrix	O
is	O
extremely	O
sparse	B
a	O
subset	O
of	O
these	O
are	O
used	O
for	O
training	O
and	O
the	O
rest	O
for	O
testing	O
as	O
shown	O
in	O
figure	O
the	O
goal	O
of	O
the	O
competition	O
was	O
to	O
predict	O
more	O
accurately	O
than	O
netflix	O
s	O
existing	O
system	O
on	O
september	O
the	O
prize	O
was	O
awarded	O
to	O
a	O
team	O
of	O
researchers	O
known	O
as	O
bellkor	O
s	O
pragmatic	O
chaos	O
section	O
discusses	O
some	O
of	O
their	O
methodology	O
further	O
details	O
on	O
the	O
teams	O
and	O
their	O
methods	O
can	O
be	O
found	O
at	O
market	B
basket	I
analysis	I
in	O
commercial	O
data	O
mining	O
there	O
is	O
much	O
interest	O
in	O
a	O
task	O
called	O
market	B
basket	I
analysis	I
the	O
data	O
consists	O
of	O
a	O
very	O
large	O
but	O
sparse	B
binary	O
matrix	O
where	O
each	O
column	O
represents	O
an	O
item	O
or	O
product	O
and	O
each	O
row	O
represents	O
a	O
transaction	O
we	O
set	O
xij	O
if	O
item	O
j	O
was	O
purchased	O
on	O
the	O
i	O
th	O
transaction	O
many	O
items	O
are	O
purchased	O
together	O
bread	O
and	O
butter	O
so	O
there	O
will	O
be	O
correlations	O
amongst	O
the	O
bits	B
given	O
a	O
new	O
partially	O
observed	O
bit	O
vector	O
representing	O
a	O
subset	O
of	O
items	O
that	O
the	O
consumer	O
has	O
bought	O
the	O
goal	O
is	O
to	O
predict	O
which	O
other	O
bits	B
are	O
likely	O
to	O
turn	O
on	O
representing	O
other	O
items	O
the	O
consumer	O
might	O
be	O
likely	O
to	O
buy	O
collaborative	O
filtering	B
we	O
often	O
assume	O
there	O
is	O
no	O
missing	B
data	I
in	O
the	O
training	O
data	O
since	O
we	O
know	O
the	O
past	O
shopping	O
behavior	O
of	O
each	O
customer	O
this	O
task	O
arises	O
in	O
other	O
domains	O
besides	O
modeling	O
purchasing	O
patterns	O
for	O
example	O
similar	B
techniques	O
can	O
be	O
used	O
to	O
model	O
dependencies	O
between	O
files	O
in	O
complex	O
software	O
systems	O
in	O
this	O
case	O
the	O
task	O
is	O
to	O
predict	O
given	O
a	O
subset	O
of	O
files	O
that	O
have	O
been	O
changed	O
which	O
other	O
ones	O
need	O
to	O
be	O
updated	O
to	O
ensure	O
consistency	O
e	O
g	O
et	O
al	O
it	O
is	O
common	O
to	O
solve	O
such	O
tasks	O
using	O
frequent	B
itemset	I
mining	I
which	O
create	O
association	B
rules	B
e	O
g	O
et	O
al	O
sec	O
for	O
details	O
alternatively	O
we	O
can	O
adopt	O
a	O
probabilistic	O
approach	O
and	O
fit	O
a	O
joint	O
density	O
model	O
xd	O
to	O
the	O
bit	O
vectors	O
see	O
e	O
g	O
et	O
al	O
chapter	O
introduction	O
illustration	O
of	O
a	O
k-nearest	O
neighbors	B
classifier	O
in	O
for	O
k	O
the	O
nearest	O
neighbors	B
figure	O
of	O
test	O
point	O
have	O
labels	O
and	O
so	O
we	O
predict	O
py	O
k	O
the	O
nearest	O
neighbors	B
of	O
test	O
point	O
have	O
labels	O
and	O
so	O
we	O
predict	O
py	O
k	O
illustration	O
of	O
the	O
voronoi	O
tesselation	O
induced	O
by	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
knnvoronoi	O
such	O
models	O
often	O
have	O
better	O
predictive	B
acccuracy	O
than	O
association	B
rules	B
although	O
they	O
may	O
be	O
less	O
interpretible	O
this	O
is	O
typical	O
of	O
the	O
difference	O
between	O
data	O
mining	O
and	O
machine	B
learning	B
in	O
data	O
mining	O
there	O
is	O
more	O
emphasis	O
on	O
interpretable	O
models	O
whereas	O
in	O
machine	B
learning	B
there	O
is	O
more	O
emphasis	O
on	O
accurate	O
models	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
in	O
this	O
section	O
we	O
provide	O
an	O
introduction	O
to	O
some	O
key	O
ideas	O
in	O
machine	B
learning	B
we	O
will	O
expand	O
on	O
these	O
concepts	O
later	O
in	O
the	O
book	O
but	O
we	O
introduce	O
them	O
briefly	O
here	O
to	O
give	O
a	O
flavor	O
of	O
things	O
to	O
come	O
parametric	O
vs	O
non-parametric	O
models	O
in	O
this	O
book	O
we	O
will	O
be	O
focussing	O
on	O
probabilistic	O
models	O
of	O
the	O
form	O
pyx	O
or	O
px	O
depending	O
on	O
whether	O
we	O
are	O
interested	O
in	O
supervised	O
or	O
unsupervised	B
learning	B
respectively	O
there	O
are	O
many	O
ways	O
to	O
define	O
such	O
models	O
but	O
the	O
most	O
important	O
distinction	O
is	O
this	O
does	O
the	O
model	O
have	O
a	O
fixed	O
number	O
of	O
parameters	O
or	O
does	O
the	O
number	O
of	O
parameters	O
grow	O
with	O
the	O
amount	O
of	O
training	O
data	O
the	O
former	O
is	O
called	O
a	O
parametric	B
model	I
and	O
the	O
latter	O
is	O
called	O
a	O
nonparametric	O
model	O
parametric	O
models	O
have	O
the	O
advantage	O
of	O
often	O
being	O
faster	O
to	O
use	O
but	O
the	O
disadvantage	O
of	O
making	O
stronger	O
assumptions	O
about	O
the	O
nature	O
of	O
the	O
data	O
distributions	O
nonparametric	O
models	O
are	O
more	O
flexible	O
but	O
often	O
computationally	O
intractable	O
for	O
large	O
datasets	O
we	O
will	O
give	O
examples	O
of	O
both	O
kinds	O
of	O
models	O
in	O
the	O
sections	O
below	O
we	O
focus	O
on	O
supervised	B
learning	B
for	O
simplicity	O
although	O
much	O
of	O
our	O
discussion	O
also	O
applies	O
to	O
unsupervised	B
learning	B
a	O
simple	O
non-parametric	O
classifier	O
k-nearest	O
neighbors	B
a	O
simple	O
example	O
of	O
a	O
non-parametric	O
classifier	O
is	O
the	O
k	O
nearest	B
neighbor	I
classifier	O
this	O
simply	O
looks	O
at	O
the	O
k	O
points	O
in	O
the	O
training	B
set	I
that	O
are	O
nearest	O
to	O
the	O
test	O
input	O
x	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
train	O
predicted	O
label	B
figure	O
probability	O
of	O
class	O
map	B
estimate	I
of	O
class	O
label	B
figure	O
generated	O
by	O
knnclassifydemo	O
some	O
synthetic	O
training	O
data	O
in	O
probability	O
of	O
class	O
for	O
knn	B
with	O
k	O
counts	O
how	O
many	O
members	O
of	O
each	O
class	O
are	O
in	O
this	O
set	O
and	O
returns	O
that	O
empirical	O
fraction	O
as	O
the	O
estimate	O
as	O
illustrated	O
in	O
figure	O
more	O
formally	O
i	O
nk	O
py	O
cxd	O
k	O
k	O
iyi	O
c	O
where	O
nkxd	O
are	O
the	O
of	O
the	O
k	O
nearest	O
points	O
to	O
x	O
in	O
d	O
and	O
ie	O
is	O
the	O
indicator	B
function	I
defined	O
as	O
follows	O
if	O
e	O
is	O
true	O
if	O
e	O
is	O
false	O
ie	O
this	O
method	O
is	O
an	O
example	O
of	O
memory-based	B
learning	B
or	O
instance-based	B
learning	B
it	O
can	O
be	O
derived	O
from	O
a	O
probabilistic	O
framework	O
as	O
explained	O
in	O
section	O
the	O
most	O
common	O
chapter	O
introduction	O
s	O
e	O
b	O
u	O
c	O
f	O
o	O
h	O
t	O
g	O
n	O
e	O
l	O
e	O
g	O
d	O
e	O
fraction	O
of	O
data	O
in	O
neighborhood	O
figure	O
illustration	O
of	O
the	O
curse	B
of	I
dimensionality	I
we	O
embed	O
a	O
small	O
cube	O
of	O
side	O
s	O
inside	O
a	O
larger	O
unit	O
cube	O
we	O
plot	O
the	O
edge	O
length	O
of	O
a	O
cube	O
needed	O
to	O
cover	O
a	O
given	O
volume	O
of	O
the	O
unit	O
cube	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
dimensions	O
based	O
on	O
figure	O
from	O
et	O
al	O
figure	O
generated	O
by	O
cursedimensionality	O
distance	O
metric	B
to	O
use	O
is	O
euclidean	B
distance	I
limits	O
the	O
applicability	O
of	O
the	O
technique	O
to	O
data	O
which	O
is	O
real-valued	O
although	O
other	O
metrics	O
can	O
be	O
used	O
figure	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
where	O
the	O
input	O
is	O
two	O
dimensional	O
we	O
have	O
three	O
classes	O
and	O
k	O
discuss	O
the	O
effect	O
of	O
k	O
below	O
panel	O
plots	O
the	O
training	O
data	O
panel	O
plots	O
py	O
where	O
x	O
is	O
evaluated	O
on	O
a	O
grid	O
of	O
points	O
panel	O
plots	O
py	O
we	O
do	O
not	O
need	O
to	O
plot	O
py	O
since	O
probabilities	O
sum	O
to	O
one	O
panel	O
plots	O
the	O
map	B
estimate	I
yx	O
argmaxcy	O
cxd	O
a	O
knn	B
classifier	O
with	O
k	O
induces	O
a	O
voronoi	B
tessellation	I
of	O
the	O
points	O
figure	O
this	O
is	O
a	O
partition	O
of	O
space	O
which	O
associates	O
a	O
region	O
v	O
with	O
each	O
point	O
xi	O
in	O
such	O
a	O
way	O
that	O
all	O
points	O
in	O
v	O
are	O
closer	O
to	O
xi	O
than	O
to	O
any	O
other	O
point	O
within	O
each	O
cell	O
the	O
predicted	O
label	B
is	O
the	O
label	B
of	O
the	O
corresponding	O
training	O
point	O
the	O
curse	B
of	I
dimensionality	I
the	O
knn	B
classifier	O
is	O
simple	O
and	O
can	O
work	O
quite	O
well	O
provided	O
it	O
is	O
given	O
a	O
good	O
distance	O
metric	B
and	O
has	O
enough	O
labeled	O
training	O
data	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
the	O
knn	B
classifier	O
can	O
come	O
within	O
a	O
factor	B
of	O
of	O
the	O
best	O
possible	O
performance	O
if	O
n	O
and	O
hart	O
however	O
the	O
main	O
problem	O
with	O
knn	B
classifiers	O
is	O
that	O
they	O
do	O
not	O
work	O
well	O
with	O
high	O
dimensional	O
inputs	O
the	O
poor	O
performance	O
in	O
high	O
dimensional	O
settings	O
is	O
due	O
to	O
the	O
curse	B
of	I
dimensionality	I
to	O
explain	O
the	O
curse	O
we	O
give	O
some	O
examples	O
from	O
et	O
al	O
consider	O
applying	O
a	O
knn	B
classifier	O
to	O
data	O
where	O
the	O
inputs	O
are	O
uniformly	O
distributed	O
in	O
the	O
d-dimensional	O
unit	O
cube	O
suppose	O
we	O
estimate	O
the	O
density	O
of	O
class	O
labels	O
around	O
a	O
test	O
point	O
x	O
by	O
growing	O
a	O
hyper-cube	O
around	O
x	O
until	O
it	O
contains	O
a	O
desired	O
fraction	O
f	O
of	O
the	O
data	O
points	O
the	O
expected	O
edge	O
length	O
of	O
this	O
cube	O
will	O
be	O
edf	O
f	O
if	O
d	O
and	O
we	O
want	O
to	O
base	O
our	O
estimate	O
on	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
pdf	B
a	O
gaussian	B
pdf	B
with	O
mean	B
and	O
variance	B
figure	O
generated	O
by	O
gaussplotdemo	O
figure	O
visualization	O
of	O
the	O
conditional	O
density	O
model	O
pyx	O
n	O
the	O
density	O
falls	O
off	O
exponentially	O
fast	O
as	O
we	O
move	O
away	O
from	O
the	O
regression	B
line	O
figure	O
generated	O
by	O
of	O
the	O
data	O
we	O
have	O
so	O
we	O
need	O
to	O
extend	O
the	O
cube	O
along	O
each	O
dimension	O
around	O
x	O
even	O
if	O
we	O
only	O
use	O
of	O
the	O
data	O
we	O
find	O
see	O
figure	O
since	O
the	O
entire	O
range	O
of	O
the	O
data	O
is	O
only	O
along	O
each	O
dimension	O
we	O
see	O
that	O
the	O
method	O
is	O
no	O
longer	O
very	O
local	O
despite	O
the	O
name	O
nearest	B
neighbor	I
the	O
trouble	O
with	O
looking	O
at	O
neighbors	B
that	O
are	O
so	O
far	O
away	O
is	O
that	O
they	O
may	O
not	O
be	O
good	O
predictors	O
about	O
the	O
behavior	O
of	O
the	O
input-output	O
function	O
at	O
a	O
given	O
point	O
parametric	O
models	O
for	O
classification	O
and	O
regression	B
the	O
main	O
way	O
to	O
combat	O
the	O
curse	B
of	I
dimensionality	I
is	O
to	O
make	O
some	O
assumptions	O
about	O
the	O
nature	O
of	O
the	O
data	O
distribution	O
pyx	O
for	O
a	O
supervised	O
problem	O
or	O
px	O
for	O
an	O
unsupervised	O
problem	O
these	O
assumptions	O
known	O
as	O
inductive	B
bias	B
are	O
often	O
embodied	O
in	O
the	O
form	O
of	O
a	O
parametric	B
model	I
which	O
is	O
a	O
statistical	O
model	O
with	O
a	O
fixed	O
number	O
of	O
parameters	O
below	O
we	O
briefly	O
describe	O
two	O
widely	O
used	O
examples	O
we	O
will	O
revisit	O
these	O
and	O
other	O
models	O
in	O
much	O
greater	O
depth	O
later	O
in	O
the	O
book	O
linear	B
regression	B
one	O
of	O
the	O
most	O
widely	O
used	O
models	O
for	O
regression	B
is	O
known	O
as	O
linear	B
regression	B
this	O
asserts	O
that	O
the	O
response	O
is	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
this	O
can	O
be	O
written	O
as	O
follows	O
yx	O
wt	O
x	O
wjxj	O
where	O
wt	O
x	O
represents	O
the	O
inner	O
or	O
scalar	B
product	I
between	O
the	O
input	O
vector	O
x	O
and	O
the	O
model	O
s	O
weight	B
vector	I
and	O
is	O
the	O
residual	B
error	I
between	O
our	O
linear	O
predictions	O
and	O
the	O
true	O
response	O
in	O
statistics	O
it	O
is	O
more	O
common	O
to	O
denote	O
the	O
regression	B
weights	O
by	O
chapter	O
introduction	O
degree	B
degree	B
figure	O
polynomial	O
of	O
degrees	O
and	O
fit	O
by	O
least	B
squares	I
to	O
data	O
points	O
figure	O
generated	O
by	O
linregpolyvsdegree	O
we	O
often	O
assume	O
that	O
has	O
a	O
or	O
normal	B
distribution	O
we	O
denote	O
this	O
by	O
n	O
where	O
is	O
the	O
mean	B
and	O
is	O
the	O
variance	B
chapter	O
for	O
details	O
when	O
we	O
plot	O
this	O
distribution	O
we	O
get	O
the	O
well-known	O
bell	B
curve	I
shown	O
in	O
figure	O
to	O
make	O
the	O
connection	O
between	O
linear	B
regression	B
and	O
gaussians	O
more	O
explicit	O
we	O
can	O
rewrite	O
the	O
model	O
in	O
the	O
following	O
form	O
pyx	O
n	O
this	O
makes	O
it	O
clear	O
that	O
the	O
model	O
is	O
a	O
conditional	B
probability	I
density	O
in	O
the	O
simplest	O
case	O
we	O
assume	O
is	O
a	O
linear	O
function	O
of	O
x	O
so	O
wt	O
x	O
and	O
that	O
the	O
noise	O
is	O
fixed	O
in	O
this	O
case	O
are	O
the	O
parameters	O
of	O
the	O
model	O
for	O
example	O
suppose	O
the	O
input	O
is	O
dimensional	O
we	O
can	O
represent	O
the	O
expected	O
response	O
as	O
follows	O
wt	O
x	O
where	O
is	O
the	O
intercept	O
or	O
bias	B
term	I
is	O
the	O
slope	O
and	O
where	O
we	O
have	O
defined	O
the	O
vector	O
x	O
x	O
a	O
constant	O
term	O
to	O
an	O
input	O
vector	O
is	O
a	O
common	O
notational	O
trick	O
which	O
if	O
is	O
positive	O
allows	O
us	O
to	O
combine	O
the	O
intercept	O
term	O
with	O
the	O
other	O
terms	O
in	O
the	O
model	O
it	O
means	O
we	O
expect	O
the	O
output	O
to	O
increase	O
as	O
the	O
input	O
increases	O
this	O
is	O
illustrated	O
in	O
in	O
figure	O
a	O
more	O
conventional	O
plot	O
of	O
the	O
mean	B
response	O
vs	O
x	O
is	O
shown	O
in	O
figure	O
linear	B
regression	B
can	O
be	O
made	O
to	O
model	O
non-linear	O
relationships	O
by	O
replacing	O
x	O
with	O
some	O
non-linear	O
function	O
of	O
the	O
inputs	O
that	O
is	O
we	O
use	O
pyx	O
n	O
this	O
is	O
known	O
as	O
basis	B
function	I
expansion	I
for	O
example	O
figure	O
illustrates	O
the	O
case	O
where	O
x	O
xd	O
for	O
d	O
and	O
d	O
this	O
is	O
known	O
as	O
polynomial	B
regression	B
we	O
will	O
consider	O
other	O
kinds	O
of	O
basis	B
functions	I
later	O
in	O
the	O
book	O
in	O
fact	O
many	O
popular	O
machine	B
learning	B
methods	O
such	O
as	O
support	B
vector	I
machines	I
neural	B
networks	I
classification	O
and	O
regression	B
trees	O
etc	O
can	O
be	O
seen	O
as	O
just	O
different	O
ways	O
of	O
estimating	O
basis	B
functions	I
from	O
data	O
as	O
we	O
discuss	O
in	O
chapters	O
and	O
carl	O
friedrich	O
gauss	O
was	O
a	O
german	O
mathematician	O
and	O
physicist	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
figure	O
the	O
sigmoid	B
or	O
logistic	B
function	O
we	O
have	O
sigm	O
and	O
sigm	O
figure	O
generated	O
by	O
sigmoidplot	O
logistic	B
regression	B
for	O
sat	O
scores	B
solid	O
black	O
dots	O
are	O
the	O
data	O
the	O
open	O
red	O
circles	O
are	O
the	O
predicted	O
probabilities	O
the	O
green	O
crosses	O
denote	O
two	O
students	O
with	O
the	O
same	O
sat	O
score	O
of	O
hence	O
same	O
input	O
representation	O
x	O
but	O
with	O
different	O
training	O
labels	O
student	O
passed	O
y	O
the	O
other	O
failed	O
y	O
hence	O
this	O
data	O
is	O
not	O
perfectly	O
separable	O
using	O
just	O
the	O
sat	O
feature	O
figure	O
generated	O
by	O
logregsatdemo	O
logistic	B
regression	B
we	O
can	O
generalize	B
linear	B
regression	B
to	O
the	O
classification	O
setting	O
by	O
making	O
two	O
changes	O
first	O
we	O
replace	O
the	O
gaussian	B
distribution	O
for	O
y	O
with	O
a	O
bernoulli	B
is	O
more	O
appropriate	O
for	O
the	O
case	O
when	O
the	O
response	O
is	O
binary	O
y	O
that	O
is	O
we	O
use	O
pyx	O
w	O
bery	O
where	O
e	O
second	O
we	O
compute	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
as	O
before	O
but	O
then	O
we	O
pass	O
this	O
through	O
a	O
function	O
that	O
ensures	O
by	O
defining	O
sigmwt	O
x	O
where	O
sigm	O
refers	O
to	O
the	O
sigmoid	B
function	O
also	O
known	O
as	O
the	O
logistic	B
or	O
logit	B
function	O
this	O
is	O
defined	O
as	O
sigm	O
exp	O
e	O
e	O
the	O
term	O
sigmoid	B
means	O
s-shaped	O
see	O
figure	O
for	O
a	O
plot	O
it	O
is	O
also	O
known	O
as	O
a	O
squashing	B
function	I
since	O
it	O
maps	O
the	O
whole	O
real	O
line	O
to	O
which	O
is	O
necessary	O
for	O
the	O
output	O
to	O
be	O
interpreted	O
as	O
a	O
probability	O
putting	O
these	O
two	O
steps	O
together	O
we	O
get	O
pyx	O
w	O
berysigmwt	O
x	O
this	O
is	O
called	O
logistic	B
regression	B
due	O
to	O
its	O
similarity	O
to	O
linear	B
regression	B
it	O
is	O
a	O
form	O
of	O
classification	O
not	O
regression	B
daniel	O
bernoulli	B
was	O
a	O
dutch-swiss	O
mathematician	O
and	O
physicist	O
chapter	O
introduction	O
a	O
simple	O
example	O
of	O
logistic	B
regression	B
is	O
shown	O
in	O
figure	O
where	O
we	O
plot	O
pyi	O
w	O
where	O
xi	O
is	O
the	O
score	O
of	O
student	O
i	O
and	O
yi	O
is	O
whether	O
they	O
passed	O
or	O
failed	O
a	O
class	O
the	O
solid	O
black	O
dots	O
show	O
the	O
training	O
data	O
and	O
the	O
red	O
circles	O
plot	O
py	O
w	O
where	O
w	O
are	O
the	O
parameters	O
estimated	O
from	O
the	O
training	O
data	O
discuss	O
how	O
to	O
compute	O
these	O
estimates	O
in	O
section	O
if	O
we	O
threshold	O
the	O
output	O
probability	O
at	O
we	O
can	O
induce	O
a	O
decision	B
rule	I
of	O
the	O
form	O
yx	O
py	O
by	O
looking	O
at	O
figure	O
we	O
see	O
that	O
for	O
x	O
x	O
we	O
can	O
imagine	O
drawing	O
a	O
vertical	O
line	O
at	O
x	O
x	O
this	O
is	O
known	O
as	O
a	O
decision	B
boundary	I
everything	O
to	O
the	O
left	O
of	O
this	O
line	O
is	O
classified	O
as	O
a	O
and	O
everything	O
to	O
the	O
right	O
of	O
the	O
line	O
is	O
classified	O
as	O
a	O
we	O
notice	O
that	O
this	O
decision	B
rule	I
has	O
a	O
non-zero	O
error	O
rate	B
even	O
on	O
the	O
training	B
set	I
this	O
is	O
because	O
the	O
data	O
is	O
not	O
linearly	B
separable	I
i	O
e	O
there	O
is	O
no	O
straight	O
line	O
we	O
can	O
draw	O
to	O
separate	O
the	O
from	O
the	O
we	O
can	O
create	O
models	O
with	O
non-linear	O
decision	B
boundaries	O
using	O
basis	B
function	I
expansion	I
just	O
as	O
we	O
did	O
with	O
non-linear	O
regression	B
we	O
will	O
see	O
many	O
examples	O
of	O
this	O
later	O
in	O
the	O
book	O
overfitting	O
when	O
we	O
fit	O
highly	O
flexible	O
models	O
we	O
need	O
to	O
be	O
careful	O
that	O
we	O
do	O
not	O
overfit	O
the	O
data	O
that	O
is	O
we	O
should	O
avoid	O
trying	O
to	O
model	O
every	O
minor	O
variation	O
in	O
the	O
input	O
since	O
this	O
is	O
more	O
likely	O
to	O
be	O
noise	O
than	O
true	O
signal	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
see	O
that	O
using	O
a	O
high	O
degree	B
polynomial	O
results	O
in	O
a	O
curve	O
that	O
is	O
very	O
wiggly	O
it	O
is	O
unlikely	O
that	O
the	O
true	O
function	O
has	O
such	O
extreme	O
oscillations	O
thus	O
using	O
such	O
a	O
model	O
might	O
result	O
in	O
accurate	O
predictions	O
of	O
future	O
outputs	O
as	O
another	O
example	O
consider	O
the	O
knn	B
classifier	O
the	O
value	O
of	O
k	O
can	O
have	O
a	O
large	O
effect	O
on	O
the	O
behavior	O
of	O
this	O
model	O
when	O
k	O
the	O
method	O
makes	O
no	O
errors	O
on	O
the	O
training	B
set	I
we	O
just	O
return	O
the	O
labels	O
of	O
the	O
original	O
training	O
points	O
but	O
the	O
resulting	O
prediction	O
surface	O
is	O
very	O
wiggly	O
figure	O
therefore	O
the	O
method	O
may	O
not	O
work	O
well	O
at	O
predicting	O
future	O
in	O
figure	O
we	O
see	O
that	O
using	O
k	O
results	O
in	O
a	O
smoother	O
prediction	O
surface	O
data	O
because	O
we	O
are	O
averaging	O
over	O
a	O
larger	O
neighborhood	O
as	O
k	O
increases	O
the	O
predictions	O
becomes	O
smoother	O
until	O
in	O
the	O
limit	O
of	O
k	O
n	O
we	O
end	O
up	O
predicting	O
the	O
majority	O
label	B
of	O
the	O
whole	O
data	O
set	O
below	O
we	O
discuss	O
how	O
to	O
pick	O
the	O
right	O
value	O
of	O
k	O
model	B
selection	I
when	O
we	O
have	O
a	O
variety	O
of	O
models	O
of	O
different	O
complexity	O
linear	O
or	O
logistic	B
regression	B
models	O
with	O
different	O
degree	B
polynomials	O
or	O
knn	B
classifiers	O
with	O
different	O
values	O
of	O
k	O
how	O
should	O
we	O
pick	O
the	O
right	O
one	O
a	O
natural	O
approach	O
is	O
to	O
compute	O
the	O
misclassification	O
rate	B
on	O
sat	O
stands	O
for	O
scholastic	O
aptitude	O
test	O
this	O
is	O
a	O
standardized	B
test	O
for	O
college	O
admissions	O
used	O
in	O
the	O
united	O
states	O
data	O
in	O
this	O
example	O
is	O
from	O
and	O
albert	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
predicted	O
label	B
predicted	O
label	B
figure	O
prediction	O
surface	O
for	O
knn	B
on	O
the	O
data	O
in	O
figure	O
figure	O
generated	O
by	O
knnclassifydemo	O
the	O
training	B
set	I
for	O
each	O
method	O
this	O
is	O
defined	O
as	O
follows	O
errfd	O
n	O
if	O
yi	O
where	O
f	O
is	O
our	O
classifier	O
in	O
figure	O
we	O
plot	O
this	O
error	O
rate	B
vs	O
k	O
for	O
a	O
knn	B
classifier	O
blue	O
line	O
we	O
see	O
that	O
increasing	O
k	O
increases	O
our	O
error	O
rate	B
on	O
the	O
training	B
set	I
because	O
we	O
are	O
over-smoothing	O
as	O
we	O
said	O
above	O
we	O
can	O
get	O
minimal	B
error	O
on	O
the	O
training	B
set	I
by	O
using	O
k	O
since	O
this	O
model	O
is	O
just	O
memorizing	O
the	O
data	O
however	O
what	O
we	O
care	O
about	O
is	O
generalization	B
error	I
which	O
is	O
the	O
expected	B
value	I
of	O
the	O
misclassification	O
rate	B
when	O
averaged	O
over	O
future	O
data	O
section	O
for	O
details	O
this	O
can	O
be	O
approximated	O
by	O
computing	O
the	O
misclassification	O
rate	B
on	O
a	O
large	O
independent	O
test	O
set	O
not	O
used	O
during	O
model	O
training	O
we	O
plot	O
the	O
test	O
error	O
vs	O
k	O
in	O
figure	O
in	O
solid	O
red	O
curve	O
now	O
we	O
see	O
a	O
u-shaped	B
curve	I
for	O
complex	O
models	O
k	O
the	O
method	O
overfits	O
and	O
for	O
simple	O
models	O
k	O
the	O
method	O
underfits	O
therefore	O
an	O
obvious	O
way	O
to	O
pick	O
k	O
is	O
to	O
pick	O
the	O
value	O
with	O
the	O
minimum	O
error	O
on	O
the	O
test	O
set	O
this	O
example	O
any	O
value	O
between	O
and	O
should	O
be	O
fine	O
unfortunately	O
when	O
training	O
the	O
model	O
we	O
don	O
t	O
have	O
access	O
to	O
the	O
test	O
set	O
assumption	O
so	O
we	O
cannot	O
use	O
the	O
test	O
set	O
to	O
pick	O
the	O
model	O
of	O
the	O
right	O
however	O
we	O
can	O
create	O
a	O
test	O
set	O
by	O
partitioning	B
the	O
training	B
set	I
into	O
two	O
the	O
part	O
used	O
for	O
training	O
the	O
model	O
and	O
a	O
second	O
part	O
called	O
the	O
validation	B
set	I
used	O
for	O
selecting	O
the	O
model	O
complexity	O
we	O
then	O
fit	O
all	O
the	O
models	O
on	O
the	O
training	B
set	I
and	O
evaluate	O
their	O
performance	O
on	O
the	O
validation	B
set	I
and	O
pick	O
the	O
best	O
once	O
we	O
have	O
picked	O
the	O
best	O
we	O
can	O
refit	O
it	O
to	O
all	O
the	O
available	O
data	O
if	O
we	O
have	O
a	O
separate	O
test	O
set	O
we	O
can	O
evaluate	O
performance	O
on	O
this	O
in	O
order	O
to	O
estimate	O
the	O
accuracy	O
of	O
our	O
method	O
discuss	O
this	O
in	O
more	O
detail	O
in	O
section	O
often	O
we	O
use	O
about	O
of	O
the	O
data	O
for	O
the	O
training	B
set	I
and	O
for	O
the	O
validation	B
set	I
but	O
if	O
the	O
number	O
of	O
training	O
cases	O
is	O
small	O
this	O
technique	O
runs	O
into	O
problems	O
because	O
the	O
model	O
in	O
academic	O
settings	O
we	O
usually	O
do	O
have	O
access	O
to	O
the	O
test	O
set	O
but	O
we	O
should	O
not	O
use	O
it	O
for	O
model	O
fitting	O
or	O
model	B
selection	I
otherwise	O
we	O
will	O
get	O
an	O
unrealistically	O
optimistic	O
estimate	O
of	O
performance	O
of	O
our	O
method	O
this	O
is	O
one	O
of	O
the	O
golden	O
rules	B
of	O
machine	B
learning	B
research	O
e	O
t	O
a	O
r	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
s	O
m	O
i	O
l	O
chapter	O
introduction	O
train	O
test	O
k	O
misclassification	O
rate	B
vs	O
k	O
in	O
a	O
k-nearest	O
neighbor	O
classifier	O
on	O
the	O
left	O
where	O
k	O
is	O
figure	O
small	O
the	O
model	O
is	O
complex	O
and	O
hence	O
we	O
overfit	O
on	O
the	O
right	O
where	O
k	O
is	O
large	O
the	O
model	O
is	O
simple	O
and	O
we	O
underfit	O
dotted	O
blue	O
line	O
training	B
set	I
solid	O
red	O
line	O
test	O
set	O
schematic	O
of	O
cross	B
validation	I
figure	O
generated	O
by	O
knnclassifydemo	O
won	O
t	O
have	O
enough	O
data	O
to	O
train	O
on	O
and	O
we	O
won	O
t	O
have	O
enough	O
data	O
to	O
make	O
a	O
reliable	O
estimate	O
of	O
the	O
future	O
performance	O
a	O
simple	O
but	O
popular	O
solution	O
to	O
this	O
is	O
to	O
use	O
cross	B
validation	I
the	O
idea	O
is	O
simple	O
we	O
split	O
the	O
training	O
data	O
into	O
k	O
folds	B
then	O
for	O
each	O
fold	O
k	O
k	O
we	O
train	O
on	O
all	O
the	O
folds	B
but	O
the	O
k	O
th	O
and	O
test	O
on	O
the	O
k	O
th	O
in	O
a	O
round-robin	O
fashion	O
as	O
sketched	O
in	O
figure	O
we	O
then	O
compute	O
the	O
error	O
averaged	O
over	O
all	O
the	O
folds	B
and	O
use	O
this	O
as	O
a	O
proxy	O
for	O
the	O
test	O
error	O
that	O
each	O
point	O
gets	O
predicted	O
only	O
once	O
although	O
it	O
will	O
be	O
used	O
for	O
training	O
k	O
times	O
it	O
is	O
common	O
to	O
use	O
k	O
this	O
is	O
called	O
cv	B
if	O
we	O
set	O
k	O
n	O
then	O
we	O
get	O
a	O
method	O
called	O
leave-one	B
out	I
cross	B
validation	I
or	O
loocv	B
since	O
in	O
fold	O
i	O
we	O
train	O
on	O
all	O
the	O
data	O
cases	O
except	O
for	O
i	O
and	O
then	O
test	O
on	O
i	O
exercise	O
asks	O
you	O
to	O
compute	O
the	O
cv	B
estimate	O
of	O
the	O
test	O
error	O
vs	O
k	O
and	O
to	O
compare	O
it	O
to	O
the	O
empirical	O
test	O
error	O
in	O
figure	O
choosing	O
k	O
for	O
a	O
knn	B
classifier	O
is	O
a	O
special	O
case	O
of	O
a	O
more	O
general	O
problem	O
known	O
as	O
model	B
selection	I
where	O
we	O
have	O
to	O
choose	O
between	O
models	O
with	O
different	O
degrees	O
of	O
flexibility	O
crossvalidation	O
is	O
widely	O
used	O
for	O
solving	O
such	O
problems	O
although	O
we	O
will	O
discuss	O
other	O
approaches	O
later	O
in	O
the	O
book	O
no	B
free	I
lunch	I
theorem	I
all	O
models	O
are	O
wrong	O
but	O
some	O
models	O
are	O
useful	O
george	O
box	O
and	O
draper	O
much	O
of	O
machine	B
learning	B
is	O
concerned	O
with	O
devising	O
different	O
models	O
and	O
different	O
algorithms	O
to	O
fit	O
them	O
we	O
can	O
use	O
methods	O
such	O
as	O
cross	B
validation	I
to	O
empirically	O
choose	O
the	O
best	O
method	O
for	O
our	O
particular	O
problem	O
however	O
there	O
is	O
no	O
universally	O
best	O
model	O
this	O
is	O
sometimes	O
called	O
the	O
no	B
free	I
lunch	I
theorem	I
the	O
reason	O
for	O
this	O
is	O
that	O
a	O
set	O
of	O
assumptions	O
that	O
works	O
well	O
in	O
one	O
domain	O
may	O
work	O
poorly	O
in	O
another	O
george	O
box	O
is	O
a	O
retired	O
statistics	O
professor	O
at	O
the	O
university	O
of	O
wisconsin	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	B
as	O
a	O
consequence	O
of	O
the	O
no	B
free	I
lunch	I
theorem	I
we	O
need	O
to	O
develop	O
many	O
different	O
types	O
of	O
models	O
to	O
cover	O
the	O
wide	O
variety	O
of	O
data	O
that	O
occurs	O
in	O
the	O
real	O
world	O
and	O
for	O
each	O
model	O
there	O
may	O
be	O
many	O
different	O
algorithms	O
we	O
can	O
use	O
to	O
train	O
the	O
model	O
which	O
make	O
different	O
speed-accuracy-complexity	O
tradeoffs	O
it	O
is	O
this	O
combination	O
of	O
data	O
models	O
and	O
algorithms	O
that	O
we	O
will	O
be	O
studying	O
in	O
the	O
subsequent	O
chapters	O
exercises	O
exercise	O
knn	B
classifier	O
on	O
shuffled	O
mnist	B
data	O
run	O
and	O
verify	O
that	O
the	O
misclassification	O
rate	B
the	O
first	O
test	O
cases	O
of	O
mnist	B
of	O
a	O
classifier	O
is	O
you	O
run	O
it	O
all	O
on	O
all	O
test	O
cases	O
the	O
error	O
rate	B
is	O
modify	O
the	O
code	O
so	O
that	O
you	O
first	O
randomly	O
permute	O
the	O
features	B
of	O
the	O
training	O
and	O
test	O
design	O
matrices	O
as	O
in	O
shuffleddigitsdemo	O
and	O
then	O
apply	O
the	O
classifier	O
verify	O
that	O
the	O
error	O
rate	B
is	O
not	O
changed	O
exercise	O
approximate	O
knn	B
classifiers	O
use	O
the	O
matlabc	O
code	O
at	O
httppeople	O
cs	O
ubc	O
camariusmindex	O
phpflannflann	O
to	O
perform	O
approximate	O
nearest	B
neighbor	I
search	O
and	O
combine	O
it	O
with	O
to	O
classify	O
the	O
mnist	B
data	O
set	O
how	O
much	O
speedup	O
do	O
you	O
get	O
and	O
what	O
is	O
the	O
drop	O
any	O
in	O
accuracy	O
exercise	O
cv	B
for	O
knn	B
use	O
knnclassifydemo	O
to	O
plot	O
the	O
cv	B
estimate	O
of	O
the	O
misclassification	O
rate	B
on	O
the	O
test	O
set	O
compare	O
this	O
to	O
figure	O
discuss	O
the	O
similarities	O
and	O
differences	O
to	O
the	O
test	O
error	O
rate	B
probability	O
introduction	O
probability	B
theory	I
is	O
nothing	O
but	O
common	O
sense	O
reduced	O
to	O
calculation	O
pierre	O
laplace	B
in	O
the	O
previous	O
chapter	O
we	O
saw	O
how	O
probability	O
can	O
play	O
a	O
useful	O
role	O
in	O
machine	B
learning	B
in	O
this	O
chapter	O
we	O
discuss	O
probability	B
theory	I
in	O
more	O
detail	O
we	O
do	O
not	O
have	O
to	O
space	O
to	O
go	O
into	O
great	O
detail	O
for	O
that	O
you	O
are	O
better	O
off	O
consulting	O
some	O
of	O
the	O
excellent	O
textbooks	O
available	O
on	O
this	O
topic	B
such	O
as	O
bertsekas	O
and	O
tsitsiklis	O
wasserman	O
but	O
we	O
will	O
briefly	O
review	O
many	O
of	O
the	O
key	O
ideas	O
you	O
will	O
need	O
in	O
later	O
chapters	O
before	O
we	O
start	O
with	O
the	O
more	O
technical	O
material	O
let	O
us	O
pause	O
and	O
ask	O
what	O
is	O
probability	O
we	O
are	O
all	O
familiar	O
with	O
the	O
phrase	O
the	O
probability	O
that	O
a	O
coin	O
will	O
land	O
heads	O
is	O
but	O
what	O
does	O
this	O
mean	B
there	O
are	O
actually	O
at	O
least	O
two	O
different	O
interpretations	O
of	O
probability	O
one	O
is	O
called	O
the	O
frequentist	B
interpretation	O
in	O
this	O
view	O
probabilities	O
represent	O
long	O
run	O
frequencies	O
of	O
events	O
for	O
example	O
the	O
above	O
statement	O
means	O
that	O
if	O
we	O
flip	O
the	O
coin	O
many	O
times	O
we	O
expect	O
it	O
to	O
land	O
heads	O
about	O
half	O
the	O
the	O
other	O
interpretation	O
is	O
called	O
the	O
bayesian	B
interpretation	O
of	O
probability	O
in	O
this	O
view	O
probability	O
is	O
used	O
to	O
quantify	O
our	O
uncertainty	B
about	O
something	O
hence	O
it	O
is	O
fundamentally	O
related	O
to	O
information	B
rather	O
than	O
repeated	O
trials	O
in	O
the	O
bayesian	B
view	O
the	O
above	O
statement	O
means	O
we	O
believe	O
the	O
coin	O
is	O
equally	O
likely	O
to	O
land	O
heads	O
or	O
tails	O
on	O
the	O
next	O
toss	O
one	O
big	O
advantage	O
of	O
the	O
bayesian	B
interpretation	O
is	O
that	O
it	O
can	O
be	O
used	O
to	O
model	O
our	O
uncertainty	B
about	O
events	O
that	O
do	O
not	O
have	O
long	O
term	O
frequencies	O
for	O
example	O
we	O
might	O
want	O
to	O
compute	O
the	O
probability	O
that	O
the	O
polar	B
ice	O
cap	O
will	O
melt	O
by	O
ce	O
this	O
event	O
will	O
happen	O
zero	O
or	O
one	O
times	O
but	O
cannot	O
happen	O
repeatedly	O
nevertheless	O
we	O
ought	O
to	O
be	O
able	O
to	O
quantify	O
our	O
uncertainty	B
about	O
this	O
event	O
based	O
on	O
how	O
probable	O
we	O
think	O
this	O
event	O
is	O
we	O
will	O
take	O
appropriate	O
actions	B
section	O
for	O
a	O
discussion	O
of	O
optimal	O
decision	B
making	O
under	O
uncertainty	B
to	O
give	O
some	O
more	O
machine	B
learning	B
oriented	O
examples	O
we	O
might	O
have	O
received	O
a	O
specific	O
email	O
message	O
and	O
want	O
to	O
compute	O
the	O
probability	O
it	O
is	O
spam	B
or	O
we	O
might	O
have	O
observed	O
a	O
blip	O
on	O
our	O
radar	B
screen	O
and	O
want	O
to	O
compute	O
the	O
probability	O
distribution	O
over	O
the	O
location	O
of	O
the	O
corresponding	O
target	O
it	O
a	O
bird	O
plane	O
or	O
missile	O
in	O
all	O
these	O
cases	O
the	O
idea	O
of	O
repeated	O
trials	O
does	O
not	O
make	O
sense	O
but	O
the	O
bayesian	B
interpretation	O
is	O
valid	O
and	O
indeed	O
actually	O
the	O
stanford	O
statistician	O
former	O
professional	O
magician	O
persi	O
diaconis	O
has	O
shown	O
that	O
a	O
coin	O
is	O
about	O
likely	O
to	O
land	O
facing	O
the	O
same	O
way	O
up	O
as	O
it	O
started	O
due	O
to	O
the	O
physics	O
of	O
the	O
problem	O
et	O
al	O
chapter	O
probability	O
a	O
uniform	B
distribution	I
on	O
with	O
px	O
k	O
a	O
degenerate	B
distribution	O
figure	O
px	O
if	O
x	O
and	O
px	O
if	O
x	O
figure	O
generated	O
by	O
discreteprobdistfig	O
quite	O
natural	O
we	O
shall	O
therefore	O
adopt	O
the	O
bayesian	B
interpretation	O
in	O
this	O
book	O
fortunately	O
the	O
basic	O
rules	B
of	O
probability	B
theory	I
are	O
the	O
same	O
no	O
matter	O
which	O
interpretation	O
is	O
adopted	O
a	O
brief	O
review	O
of	O
probability	B
theory	I
this	O
section	O
is	O
a	O
very	O
brief	O
review	O
of	O
the	O
basics	O
of	O
probability	B
theory	I
and	O
is	O
merely	O
meant	O
as	O
a	O
refresher	O
for	O
readers	O
who	O
may	O
be	O
rusty	O
readers	O
who	O
are	O
already	O
familiar	O
with	O
these	O
basics	O
may	O
safely	O
skip	O
this	O
section	O
discrete	B
random	O
variables	O
the	O
expression	O
pa	O
denotes	O
the	O
probability	O
that	O
the	O
event	O
a	O
is	O
true	O
for	O
example	O
a	O
might	O
be	O
the	O
logical	O
expression	O
it	O
will	O
rain	O
tomorrow	O
we	O
require	O
that	O
pa	O
where	O
pa	O
means	O
the	O
event	O
definitely	O
will	O
not	O
happen	O
and	O
pa	O
means	O
the	O
event	O
definitely	O
will	O
happen	O
we	O
write	O
pa	O
to	O
denote	O
the	O
probability	O
of	O
the	O
event	O
not	O
a	O
this	O
is	O
defined	O
to	O
pa	O
pa	O
we	O
will	O
often	O
write	O
a	O
to	O
mean	B
the	O
event	O
a	O
is	O
true	O
and	O
a	O
to	O
mean	B
the	O
event	O
a	O
is	O
false	O
we	O
can	O
extend	O
the	O
notion	O
of	O
binary	O
events	O
by	O
defining	O
a	O
discrete	B
random	I
variable	I
x	O
which	O
can	O
take	O
on	O
any	O
value	O
from	O
a	O
finite	O
or	O
countably	O
infinite	O
set	O
x	O
we	O
denote	O
the	O
probability	O
of	O
the	O
event	O
that	O
x	O
x	O
by	O
px	O
x	O
or	O
just	O
px	O
for	O
short	O
here	O
p	O
is	O
called	O
a	O
probability	B
mass	I
function	I
or	O
pmf	B
this	O
satisfies	O
the	O
properties	O
px	O
and	O
x	O
x	O
px	O
figure	O
shows	O
two	O
pmf	B
s	O
defined	O
on	O
the	O
finite	O
state	B
space	I
x	O
on	O
the	O
left	O
we	O
have	O
a	O
uniform	B
distribution	I
px	O
and	O
on	O
the	O
right	O
we	O
have	O
a	O
degenerate	B
distribution	O
px	O
ix	O
where	O
i	O
is	O
the	O
binary	O
indicator	B
function	I
this	O
distribution	O
represents	O
the	O
fact	O
that	O
x	O
is	O
always	O
equal	O
to	O
the	O
value	O
in	O
other	O
words	O
it	O
is	O
a	O
constant	O
fundamental	O
rules	B
in	O
this	O
section	O
we	O
review	O
the	O
basic	O
rules	B
of	O
probability	O
a	O
brief	O
review	O
of	O
probability	B
theory	I
probability	O
of	O
a	O
union	O
of	O
two	O
events	O
given	O
two	O
events	O
a	O
and	O
b	O
we	O
define	O
the	O
probability	O
of	O
a	O
or	O
b	O
as	O
follows	O
pa	O
b	O
a	O
b	O
pa	O
b	O
pa	O
b	O
if	O
a	O
and	O
b	O
are	O
mutually	O
exclusive	O
joint	O
probabilities	O
we	O
define	O
the	O
probability	O
of	O
the	O
joint	O
event	O
a	O
and	O
b	O
as	O
follows	O
pa	O
b	O
pa	O
b	O
pabpb	O
this	O
is	O
sometimes	O
called	O
the	O
product	B
rule	I
given	O
a	O
joint	B
distribution	I
on	O
two	O
events	O
pa	O
b	O
we	O
define	O
the	O
marginal	B
distribution	I
as	O
follows	O
pab	O
bpb	O
b	O
pa	O
pa	O
b	O
b	O
b	O
where	O
we	O
are	O
summing	O
over	O
all	O
possible	O
states	O
of	O
b	O
we	O
can	O
define	O
pb	O
similarly	O
this	O
is	O
sometimes	O
called	O
the	O
sum	B
rule	I
or	O
the	O
rule	B
of	I
total	I
probability	I
the	O
product	B
rule	I
can	O
be	O
applied	O
multiple	O
times	O
to	O
yield	O
the	O
chain	B
rule	I
of	O
probability	O
where	O
we	O
introduce	O
the	O
matlab-like	O
notation	O
to	O
denote	O
the	O
set	O
d	O
conditional	B
probability	I
we	O
define	O
the	O
conditional	B
probability	I
of	O
event	O
a	O
given	O
that	O
event	O
b	O
is	O
true	O
as	O
follows	O
pab	O
pa	O
b	O
pb	O
if	O
pb	O
bayes	B
rule	I
combining	O
the	O
definition	O
of	O
conditional	B
probability	I
with	O
the	O
product	O
and	O
sum	O
rules	B
yields	O
bayes	B
rule	I
also	O
called	O
bayes	O
px	O
xpy	O
yx	O
x	O
px	O
yx	O
px	O
xy	O
y	O
px	O
x	O
y	O
y	O
py	O
y	O
example	O
medical	O
diagnosis	O
as	O
an	O
example	O
of	O
how	O
to	O
use	O
this	O
rule	O
consider	O
the	O
following	O
medical	O
diagonsis	O
problem	O
suppose	O
you	O
are	O
a	O
woman	O
in	O
your	O
and	O
you	O
decide	O
to	O
have	O
a	O
medical	O
test	O
for	O
breast	O
cancer	O
called	O
a	O
mammogram	B
if	O
the	O
test	O
is	O
positive	O
what	O
is	O
the	O
probability	O
you	O
have	O
cancer	O
that	O
obviously	O
depends	O
on	O
how	O
reliable	O
the	O
test	O
is	O
suppose	O
you	O
are	O
told	O
the	O
test	O
has	O
a	O
sensitivity	B
thomas	O
bayes	O
was	O
an	O
english	O
mathematician	O
and	O
presbyterian	O
minister	O
chapter	O
probability	O
of	O
which	O
means	O
if	O
you	O
have	O
cancer	O
the	O
test	O
will	O
be	O
positive	O
with	O
probability	O
in	O
other	O
words	O
px	O
where	O
x	O
is	O
the	O
event	O
the	O
mammogram	B
is	O
positive	O
and	O
y	O
is	O
the	O
event	O
you	O
have	O
breast	O
cancer	O
many	O
people	O
conclude	O
they	O
are	O
therefore	O
likely	O
to	O
have	O
cancer	O
but	O
this	O
is	O
false	O
it	O
ignores	O
the	O
prior	O
probability	O
of	O
having	O
breast	O
cancer	O
which	O
fortunately	O
is	O
quite	O
low	O
py	O
ignoring	O
this	O
prior	O
is	O
called	O
the	O
base	B
rate	B
fallacy	I
we	O
also	O
need	O
to	O
take	O
into	O
account	O
the	O
fact	O
that	O
the	O
test	O
may	O
be	O
a	O
false	B
positive	I
or	O
false	B
alarm	I
unfortunately	O
such	O
false	O
positives	O
are	O
quite	O
likely	O
current	O
screening	B
technology	O
px	O
combining	O
these	O
three	O
terms	O
using	O
bayes	B
rule	I
we	O
can	O
compute	O
the	O
correct	O
answer	O
as	O
follows	O
py	O
px	O
px	O
px	O
where	O
py	O
py	O
about	O
a	O
chance	O
of	O
actually	O
having	O
breast	O
in	O
other	O
words	O
if	O
you	O
test	O
positive	O
you	O
only	O
have	O
example	O
generative	O
classifiers	O
we	O
can	O
generalize	B
the	O
medical	O
diagonosis	O
example	O
to	O
classify	O
feature	O
vectors	O
x	O
of	O
arbitrary	O
type	O
as	O
follows	O
py	O
cx	O
py	O
c	O
c	O
py	O
this	O
is	O
called	O
a	O
generative	O
classifier	O
since	O
it	O
specifies	O
how	O
to	O
generate	O
the	O
data	O
using	O
the	O
classconditional	O
density	O
pxy	O
c	O
and	O
the	O
class	O
prior	O
py	O
c	O
we	O
discuss	O
such	O
models	O
in	O
detail	O
in	O
chapters	O
and	O
an	O
alternative	O
approach	O
is	O
to	O
directly	O
fit	O
the	O
class	O
posterior	O
py	O
cx	O
this	O
is	O
known	O
as	O
a	O
discriminative	B
classifier	O
we	O
discuss	O
the	O
pros	O
and	O
cons	O
of	O
the	O
two	O
approaches	O
in	O
section	O
independence	O
and	O
conditional	B
independence	I
we	O
say	O
x	O
and	O
y	O
are	O
unconditionally	B
independent	I
or	O
marginally	B
independent	I
denoted	O
x	O
y	O
if	O
we	O
can	O
represent	O
the	O
joint	O
as	O
the	O
product	O
of	O
the	O
two	O
marginals	O
figure	O
i	O
e	O
x	O
y	O
px	O
y	O
pxpy	O
a	O
brief	O
review	O
of	O
probability	B
theory	I
figure	O
computing	O
px	O
y	O
pxpy	O
where	O
x	O
y	O
here	O
x	O
and	O
y	O
are	O
discrete	B
random	O
variables	O
x	O
has	O
possible	O
states	O
and	O
y	O
has	O
possible	O
states	O
a	O
general	O
joint	B
distribution	I
on	O
two	O
such	O
variables	O
would	O
require	O
parameters	O
to	O
define	O
it	O
subtract	O
because	O
of	O
the	O
sum-to-one	O
constraint	O
by	O
assuming	O
independence	O
we	O
only	O
need	O
parameters	O
to	O
define	O
px	O
y	O
in	O
general	O
we	O
say	O
a	O
set	O
of	O
variables	O
is	O
mutually	B
independent	I
if	O
the	O
joint	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
marginals	O
unfortunately	O
unconditional	O
independence	O
is	O
rare	O
because	O
most	O
variables	O
can	O
influence	O
most	O
other	O
variables	O
however	O
usually	O
this	O
influence	O
is	O
mediated	O
via	O
other	O
variables	O
rather	O
than	O
being	O
direct	O
we	O
therefore	O
say	O
x	O
and	O
y	O
are	O
conditionally	B
independent	I
given	O
z	O
iff	B
the	O
conditional	O
joint	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
conditional	O
marginals	O
x	O
y	O
px	O
y	O
pxzpy	O
when	O
we	O
discuss	O
graphical	B
models	I
in	O
chapter	O
we	O
will	O
see	O
that	O
we	O
can	O
write	O
this	O
assumption	O
as	O
a	O
graph	B
x	O
z	O
y	O
which	O
captures	O
the	O
intuition	O
that	O
all	O
the	O
dependencies	O
between	O
x	O
and	O
y	O
are	O
mediated	O
via	O
z	O
for	O
example	O
the	O
probability	O
it	O
will	O
rain	O
tomorrow	O
x	O
is	O
independent	O
of	O
whether	O
the	O
ground	O
is	O
wet	O
today	O
y	O
given	O
knowledge	O
of	O
whether	O
it	O
is	O
raining	O
today	O
z	O
intuitively	O
this	O
is	O
because	O
z	O
causes	O
both	O
x	O
and	O
y	O
so	O
if	O
we	O
know	O
z	O
we	O
do	O
not	O
need	O
to	O
know	O
about	O
y	O
in	O
order	O
to	O
predict	O
x	O
or	O
vice	O
versa	O
we	O
shall	O
expand	O
on	O
this	O
concept	B
in	O
chapter	O
another	O
characterization	O
of	O
ci	B
is	O
this	O
theorem	O
x	O
y	O
iff	B
there	O
exist	O
function	O
g	O
and	O
h	O
such	O
that	O
px	O
yz	O
gx	O
zhy	O
z	O
for	O
all	O
x	O
y	O
z	O
such	O
that	O
pz	O
these	O
numbers	O
are	O
from	O
based	O
on	O
this	O
analysis	O
the	O
us	O
government	O
decided	O
not	O
to	O
recommend	O
annual	O
mammogram	B
screening	B
to	O
women	O
in	O
their	O
the	O
number	O
of	O
false	O
alarms	O
would	O
cause	O
needless	O
worry	O
and	O
stress	O
amongst	O
women	O
and	O
result	O
in	O
unnecesssary	O
expensive	O
and	O
potentially	O
harmful	O
followup	O
tests	O
see	O
section	O
for	O
the	O
optimal	O
way	O
to	O
trade	O
off	O
risk	B
reverse	O
reward	B
in	O
the	O
face	O
of	O
uncertainty	B
chapter	O
probability	O
see	O
exercise	O
for	O
the	O
proof	O
ci	B
assumptions	O
allow	O
us	O
to	O
build	O
large	O
probabilistic	O
models	O
from	O
small	O
pieces	O
we	O
will	O
see	O
many	O
examples	O
of	O
this	O
throughout	O
the	O
book	O
in	O
particular	O
in	O
section	O
we	O
discuss	O
naive	O
bayes	O
classifiers	O
in	O
section	O
we	O
discuss	O
markov	B
models	I
and	O
in	O
chapter	O
we	O
discuss	O
graphical	B
models	I
all	O
of	O
these	O
models	O
heavily	O
exploit	O
ci	B
properties	O
continuous	O
random	O
variables	O
so	O
far	O
we	O
have	O
only	O
considered	O
reasoning	O
about	O
uncertain	O
discrete	B
quantities	O
we	O
will	O
now	O
show	O
how	O
to	O
extend	O
probability	O
to	O
reason	O
about	O
uncertain	O
continuous	O
quantities	O
suppose	O
x	O
is	O
some	O
uncertain	O
continuous	O
quantity	O
the	O
probability	O
that	O
x	O
lies	O
in	O
any	O
interval	O
a	O
x	O
b	O
can	O
be	O
computed	O
as	O
follows	O
define	O
the	O
events	O
a	O
a	O
b	O
b	O
and	O
w	O
x	O
b	O
we	O
have	O
that	O
b	O
a	O
w	O
and	O
since	O
a	O
and	O
w	O
are	O
mutually	O
exclusive	O
the	O
sum	O
rules	B
gives	O
pb	O
pa	O
w	O
and	O
hence	O
pw	O
pb	O
pa	O
define	O
the	O
function	O
f	O
px	O
q	O
this	O
is	O
called	O
the	O
cumulative	B
distribution	I
function	I
or	O
cdf	B
of	O
x	O
this	O
is	O
obviously	O
a	O
monotonically	O
increasing	O
function	O
see	O
figure	O
for	O
an	O
example	O
using	O
this	O
notation	O
we	O
have	O
pa	O
x	O
b	O
f	O
f	O
now	O
define	O
f	O
d	O
dx	O
f	O
assume	O
this	O
derivative	O
exists	O
this	O
is	O
called	O
the	O
probability	B
density	I
function	I
or	O
pdf	B
see	O
figure	O
for	O
an	O
example	O
given	O
a	O
pdf	B
we	O
can	O
compute	O
the	O
probability	O
of	O
a	O
continuous	O
variable	O
being	O
in	O
a	O
finite	O
interval	O
as	O
follows	O
b	O
p	O
x	O
b	O
f	O
a	O
as	O
the	O
size	O
of	O
the	O
interval	O
gets	O
smaller	O
we	O
can	O
write	O
p	O
x	O
x	O
dx	O
pxdx	O
we	O
require	O
px	O
but	O
it	O
is	O
possible	O
for	O
px	O
for	O
any	O
given	O
x	O
so	O
long	O
as	O
the	O
density	O
integrates	O
to	O
as	O
an	O
example	O
consider	O
the	O
uniform	B
distribution	I
unifa	O
b	O
unifxa	O
b	O
b	O
a	O
if	O
we	O
set	O
a	O
and	O
b	O
ia	O
x	O
b	O
we	O
havep	O
x	O
for	O
any	O
x	O
a	O
brief	O
review	O
of	O
probability	B
theory	I
cdf	B
figure	O
plot	O
of	O
the	O
cdf	B
for	O
the	O
standard	B
normal	B
n	O
corresponding	O
pdf	B
the	O
shaded	O
regions	O
each	O
contain	O
of	O
the	O
probability	O
mass	O
therefore	O
the	O
nonshaded	O
region	O
contains	O
of	O
the	O
probability	O
mass	O
if	O
the	O
distribution	O
is	O
gaussian	B
n	O
then	O
the	O
leftmost	O
cutoff	O
point	O
is	O
where	O
if	O
is	O
the	O
cdf	B
of	O
the	O
gaussian	B
by	O
symmetry	O
the	O
rightost	O
cutoff	O
point	O
is	O
the	O
central	B
interval	I
is	O
and	O
the	O
left	O
cutoff	O
is	O
and	O
the	O
right	O
is	O
figure	O
generated	O
by	O
quantiledemo	O
quantiles	O
since	O
the	O
cdf	B
f	O
is	O
a	O
monotonically	O
increasing	O
function	O
it	O
has	O
an	O
inverse	O
let	O
us	O
denote	O
this	O
by	O
is	O
the	O
value	O
of	O
x	O
such	O
that	O
p	O
x	O
this	O
is	O
if	O
f	O
is	O
the	O
cdf	B
of	O
x	O
then	O
f	O
f	O
is	O
the	O
median	B
of	O
the	O
distribution	O
with	O
half	O
of	O
called	O
the	O
quantile	B
of	O
f	O
the	O
value	O
f	O
the	O
probability	O
mass	O
on	O
the	O
left	O
and	O
half	O
on	O
the	O
right	O
the	O
values	O
f	O
are	O
the	O
lower	O
and	O
upper	O
quartiles	B
the	O
cdf	B
of	O
the	O
gaussian	B
distribution	O
n	O
then	O
points	O
to	O
the	O
left	O
of	O
probability	O
mass	O
as	O
illustrated	O
in	O
figure	O
by	O
symmetry	O
points	O
to	O
the	O
right	O
of	O
also	O
contain	O
of	O
the	O
mass	O
hence	O
the	O
central	B
interval	I
of	O
the	O
mass	O
if	O
we	O
set	O
the	O
central	B
interval	I
is	O
covered	O
by	O
the	O
range	O
we	O
can	O
also	O
use	O
the	O
inverse	O
cdf	B
to	O
compute	O
tail	B
area	I
probabilities	I
for	O
example	O
if	O
is	O
contain	O
contains	O
and	O
f	O
if	O
the	O
distribution	O
is	O
n	O
then	O
the	O
interval	O
becomes	O
this	O
is	O
sometimes	O
approximated	O
by	O
writing	O
mean	B
and	O
variance	B
the	O
most	O
familiar	O
property	O
of	O
a	O
distribution	O
is	O
its	O
mean	B
orexpected	O
value	O
denoted	O
by	O
for	O
discrete	B
rv	O
s	O
it	O
is	O
defined	O
as	O
e	O
x	O
x	O
x	O
px	O
and	O
for	O
continuous	O
rv	O
s	O
it	O
is	O
defined	O
as	O
e	O
x	O
x	O
pxdx	O
if	O
this	O
integral	O
is	O
not	O
finite	O
the	O
mean	B
is	O
not	O
defined	O
will	O
see	O
some	O
examples	O
of	O
this	O
later	O
the	O
variance	B
is	O
a	O
measure	O
of	O
the	O
spread	O
of	O
a	O
distribution	O
denoted	O
by	O
this	O
is	O
defined	O
chapter	O
probability	O
pxdx	O
x	O
xpxdx	O
e	O
as	O
follows	O
var	B
e	O
from	O
which	O
we	O
derive	O
the	O
useful	O
result	O
e	O
x	O
the	O
standard	B
deviation	I
is	O
defined	O
as	O
std	O
var	B
this	O
is	O
useful	O
since	O
it	O
has	O
the	O
same	O
units	O
as	O
x	O
itself	O
some	O
common	O
discrete	B
distributions	O
in	O
this	O
section	O
we	O
review	O
some	O
commonly	O
used	O
parametric	O
distributions	O
defined	O
on	O
discrete	B
state	B
spaces	O
both	O
finite	O
and	O
countably	O
infinite	O
the	O
binomial	B
and	O
bernoulli	B
distributions	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
times	O
let	O
x	O
n	O
be	O
the	O
number	O
of	O
heads	O
if	O
the	O
probability	O
of	O
heads	O
is	O
then	O
we	O
say	O
x	O
has	O
a	O
binomial	B
distribution	I
written	O
as	O
x	O
binn	O
the	O
pmf	B
is	O
given	O
by	O
n	O
k	O
binkn	O
n	O
k	O
n	O
k	O
k	O
k	O
is	O
the	O
number	O
of	O
ways	O
to	O
choose	O
k	O
items	O
from	O
n	O
is	O
known	O
as	O
the	O
binomial	B
coefficient	I
and	O
is	O
pronounced	O
n	O
choose	O
k	O
see	O
figure	O
for	O
some	O
examples	O
of	O
the	O
binomial	B
distribution	I
this	O
distribution	O
has	O
the	O
following	O
mean	B
and	O
variance	B
mean	B
var	B
n	O
now	O
suppose	O
we	O
toss	O
a	O
coin	O
only	O
once	O
let	O
x	O
be	O
a	O
binary	O
random	O
variable	O
with	O
probability	O
of	O
success	O
or	O
heads	O
of	O
we	O
say	O
that	O
x	O
has	O
a	O
bernoulli	B
distribution	O
this	O
is	O
written	O
as	O
x	O
ber	O
where	O
the	O
pmf	B
is	O
defined	O
as	O
berx	O
in	O
other	O
words	O
berx	O
if	O
x	O
if	O
x	O
this	O
is	O
obviously	O
just	O
a	O
special	O
case	O
of	O
a	O
binomial	B
distribution	I
with	O
n	O
some	O
common	O
discrete	B
distributions	O
illustration	O
of	O
the	O
binomial	B
distribution	I
with	O
n	O
and	O
figure	O
generated	O
figure	O
by	O
binomdistplot	O
the	O
multinomial	B
and	O
multinoulli	O
distributions	O
the	O
binomial	B
distribution	I
can	O
be	O
used	O
to	O
model	O
the	O
outcomes	O
of	O
coin	O
tosses	O
to	O
model	O
the	O
outcomes	O
of	O
tossing	O
a	O
k-sided	O
die	O
we	O
can	O
use	O
the	O
multinomial	B
distribution	O
this	O
is	O
defined	O
as	O
let	O
x	O
xk	O
be	O
a	O
random	O
vector	O
where	O
xj	O
is	O
the	O
number	O
of	O
times	O
side	O
j	O
of	O
follows	O
the	O
die	O
occurs	O
then	O
x	O
has	O
the	O
following	O
pmf	B
xj	O
j	O
n	O
xk	O
muxn	O
n	O
xk	O
n	O
x	O
k	O
where	O
j	O
is	O
the	O
probability	O
that	O
side	O
j	O
shows	O
up	O
and	O
is	O
the	O
multinomial	B
coefficient	I
number	O
of	O
ways	O
to	O
divide	O
a	O
set	O
of	O
size	O
n	O
subsets	O
with	O
sizes	O
up	O
to	O
xk	O
xk	O
into	O
now	O
suppose	O
n	O
this	O
is	O
like	O
rolling	O
a	O
k-sided	O
dice	O
once	O
so	O
x	O
will	O
be	O
a	O
vector	O
of	O
and	O
bit	O
vector	O
in	O
which	O
only	O
one	O
bit	O
can	O
be	O
turned	O
on	O
specifically	O
if	O
the	O
dice	O
shows	O
up	O
as	O
face	O
k	O
then	O
the	O
k	O
th	O
bit	O
will	O
be	O
on	O
in	O
this	O
case	O
we	O
can	O
think	O
of	O
x	O
as	O
being	O
a	O
scalar	O
categorical	B
random	O
variable	O
with	O
k	O
states	O
and	O
x	O
is	O
its	O
dummy	B
encoding	I
that	O
is	O
x	O
ix	O
k	O
for	O
example	O
if	O
k	O
we	O
encode	O
the	O
states	O
and	O
as	O
and	O
this	O
is	O
also	O
called	O
a	O
one-hot	B
encoding	I
since	O
we	O
imagine	O
that	O
only	O
one	O
of	O
the	O
k	O
wires	O
is	O
hot	O
or	O
on	O
in	O
this	O
case	O
the	O
pmf	B
becomes	O
ixj	O
j	O
see	O
figure	O
for	O
an	O
example	O
this	O
very	O
common	O
special	O
case	O
is	O
known	O
as	O
a	O
categorical	B
or	O
discrete	B
distribution	O
lacerda	O
suggested	O
we	O
call	O
it	O
the	O
multinoulli	B
distribution	I
by	O
analogy	O
with	O
the	O
binomial	B
bernoulli	B
distinction	O
a	O
term	O
which	O
we	O
shall	O
adopt	O
in	O
this	O
book	O
we	O
chapter	O
probability	O
name	O
multinomial	B
multinoulli	O
binomial	B
bernoulli	B
n	O
k	O
x	O
x	O
nk	O
x	O
x	O
n	O
x	O
xk	O
n	O
xk	O
encoding	O
table	O
summary	O
of	O
the	O
multinomial	B
and	O
related	O
distributions	O
a	O
t	O
a	O
g	O
c	O
c	O
g	O
g	O
t	O
a	O
c	O
g	O
g	O
c	O
a	O
t	O
t	O
a	O
g	O
c	O
t	O
g	O
c	O
a	O
a	O
c	O
c	O
g	O
c	O
a	O
t	O
c	O
a	O
g	O
c	O
c	O
a	O
c	O
t	O
a	O
g	O
a	O
g	O
c	O
a	O
a	O
t	O
a	O
a	O
c	O
c	O
g	O
c	O
g	O
a	O
c	O
c	O
g	O
c	O
a	O
t	O
t	O
a	O
g	O
c	O
c	O
g	O
c	O
t	O
a	O
a	O
g	O
g	O
t	O
a	O
t	O
a	O
a	O
g	O
c	O
c	O
t	O
c	O
g	O
t	O
a	O
c	O
g	O
t	O
a	O
t	O
t	O
a	O
g	O
c	O
c	O
g	O
t	O
t	O
a	O
c	O
g	O
g	O
c	O
c	O
a	O
t	O
a	O
t	O
c	O
c	O
g	O
g	O
t	O
a	O
c	O
a	O
g	O
t	O
a	O
a	O
t	O
a	O
g	O
c	O
a	O
g	O
g	O
t	O
a	O
c	O
c	O
g	O
a	O
a	O
a	O
c	O
a	O
t	O
c	O
c	O
g	O
t	O
g	O
a	O
c	O
g	O
g	O
a	O
a	O
s	O
t	O
i	O
b	O
sequence	O
position	O
figure	O
seqlogodemo	O
some	O
aligned	O
dna	B
sequences	I
the	O
corresponding	O
sequence	B
logo	I
figure	O
generated	O
by	O
will	O
use	O
the	O
following	O
notation	O
for	O
this	O
case	O
catx	O
in	O
otherwords	O
if	O
x	O
cat	O
then	O
px	O
j	O
j	O
see	O
table	O
for	O
a	O
summary	O
application	O
dna	O
sequence	O
motifs	O
an	O
interesting	O
application	O
of	O
multinomial	B
models	O
arises	O
in	O
biosequence	B
analysis	I
suppose	O
we	O
have	O
a	O
set	O
of	O
dna	B
sequences	I
such	O
as	O
in	O
figure	O
where	O
there	O
are	O
rows	O
and	O
columns	O
along	O
the	O
genome	B
we	O
see	O
that	O
several	O
locations	O
are	O
conserved	O
by	O
evolution	O
because	O
they	O
are	O
part	O
of	O
a	O
gene	O
coding	O
region	O
since	O
the	O
corresponding	O
columns	O
tend	O
to	O
be	O
pure	B
for	O
example	O
column	O
is	O
all	O
g	O
s	O
one	O
way	O
to	O
visually	O
summarize	O
the	O
data	O
is	O
by	O
using	O
a	O
sequence	B
logo	I
see	O
figure	O
we	O
plot	O
the	O
letters	O
a	O
c	O
g	O
and	O
t	O
with	O
a	O
fontsize	O
proportional	O
to	O
their	O
empirical	O
probability	O
and	O
with	O
the	O
most	O
probable	O
letter	O
on	O
the	O
top	O
the	O
empirical	O
probability	O
distribution	O
at	O
location	O
t	O
t	O
is	O
gotten	O
by	O
normalizing	O
the	O
vector	O
of	O
counts	O
equation	O
nt	O
ixit	O
ixit	O
ixit	O
ixit	O
t	O
ntn	O
this	O
distribution	O
is	O
known	O
as	O
a	O
motif	B
we	O
can	O
also	O
compute	O
the	O
most	O
probable	O
letter	O
in	O
each	O
location	O
this	O
is	O
called	O
the	O
consensus	B
sequence	I
some	O
common	O
discrete	B
distributions	O
poi	O
poi	O
figure	O
illustration	O
of	O
some	O
poisson	B
distributions	O
for	O
we	O
have	O
truncated	O
the	O
x-axis	O
to	O
for	O
clarity	O
but	O
the	O
support	B
of	O
the	O
distribution	O
is	O
over	O
all	O
the	O
non-negative	O
integers	O
figure	O
generated	O
by	O
poissonplotdemo	O
the	O
poisson	B
distribution	O
we	O
say	O
that	O
x	O
has	O
a	O
poisson	B
distribution	O
with	O
parameter	B
written	O
x	O
poi	O
if	O
its	O
pmf	B
is	O
x	O
x	O
poix	O
the	O
first	O
term	O
is	O
just	O
the	O
normalization	O
constant	O
required	O
to	O
ensure	O
the	O
distribution	O
sums	O
to	O
the	O
poisson	B
distribution	O
is	O
often	O
used	O
as	O
a	O
model	O
for	O
counts	O
of	O
rare	O
events	O
like	O
radioactive	O
decay	O
and	O
traffic	O
accidents	O
see	O
figure	O
for	O
some	O
plots	O
the	O
empirical	B
distribution	I
given	O
a	O
set	O
of	O
data	O
d	O
xn	O
we	O
define	O
the	O
empirical	B
distribution	I
also	O
called	O
the	O
empirical	B
measure	I
as	O
follows	O
xi	O
pempa	O
n	O
xa	O
if	O
x	O
a	O
if	O
x	O
a	O
where	O
xa	O
is	O
the	O
dirac	B
measure	I
defined	O
by	O
in	O
general	O
we	O
can	O
associate	O
weights	O
with	O
each	O
sample	O
px	O
wi	O
xi	O
where	O
we	O
require	O
wi	O
and	O
wi	O
we	O
can	O
think	O
of	O
this	O
as	O
a	O
histogram	B
with	O
spikes	O
at	O
the	O
data	O
points	O
xi	O
where	O
wi	O
determines	O
the	O
height	O
of	O
spike	O
i	O
this	O
distribution	O
assigns	O
probability	O
to	O
any	O
point	O
not	O
in	O
the	O
data	O
set	O
chapter	O
probability	O
some	O
common	O
continuous	O
distributions	O
in	O
this	O
section	O
we	O
present	O
some	O
commonly	O
used	O
univariate	O
continuous	O
probability	O
distributions	O
gaussian	B
distribution	O
the	O
most	O
widely	O
used	O
distribution	O
in	O
statistics	O
and	O
machine	B
learning	B
is	O
the	O
gaussian	B
or	O
normal	B
distribution	O
its	O
pdf	B
is	O
given	O
by	O
n	O
e	O
here	O
e	O
is	O
the	O
mean	B
mode	B
and	O
var	B
is	O
the	O
variance	B
normalization	O
constant	O
needed	O
to	O
ensure	O
the	O
density	O
integrates	O
to	O
exercise	O
is	O
the	O
if	O
x	O
n	O
we	O
say	O
x	O
follows	O
a	O
standard	B
normal	B
distribution	O
see	O
figure	O
for	O
a	O
plot	O
of	O
this	O
pdf	B
this	O
is	O
sometimes	O
called	O
the	O
bell	B
curve	I
we	O
write	O
x	O
n	O
to	O
denote	O
that	O
px	O
x	O
we	O
will	O
often	O
talk	O
about	O
the	O
precision	B
of	O
a	O
gaussian	B
by	O
which	O
we	O
mean	B
the	O
inverse	O
variance	B
a	O
high	O
precision	B
means	O
a	O
narrow	O
distribution	O
variance	B
centered	O
on	O
density	O
at	O
its	O
center	O
x	O
we	O
have	O
n	O
px	O
note	O
that	O
since	O
this	O
is	O
a	O
pdf	B
we	O
can	O
have	O
px	O
to	O
see	O
this	O
consider	O
evaluating	O
the	O
we	O
have	O
so	O
if	O
x	O
the	O
cumulative	B
distribution	I
function	I
or	O
cdf	B
of	O
the	O
gaussian	B
is	O
defined	O
as	O
n	O
see	O
figure	O
for	O
a	O
plot	O
of	O
this	O
cdf	B
when	O
this	O
integral	O
has	O
no	O
closed	O
form	O
expression	O
but	O
is	O
built	O
in	O
to	O
most	O
software	O
packages	O
in	O
particular	O
we	O
can	O
compute	O
it	O
in	O
terms	O
of	O
the	O
error	B
function	I
erfz	O
where	O
z	O
and	O
x	O
erfx	O
e	O
dt	O
the	O
gaussian	B
distribution	O
is	O
the	O
most	O
widely	O
used	O
distribution	O
in	O
statistics	O
there	O
are	O
several	O
reasons	O
for	O
this	O
first	O
it	O
has	O
two	O
parameters	O
which	O
are	O
easy	O
to	O
interpret	O
and	O
which	O
capture	O
some	O
of	O
the	O
most	O
basic	O
properties	O
of	O
a	O
distribution	O
namely	O
its	O
mean	B
and	O
variance	B
second	O
the	O
central	B
limit	I
theorem	I
tells	O
us	O
that	O
sums	O
of	O
independent	O
random	O
variables	O
have	O
an	O
approximately	O
gaussian	B
distribution	O
making	O
it	O
a	O
good	O
choice	O
for	O
modeling	O
residual	B
errors	O
or	O
noise	O
third	O
the	O
gaussian	B
distribution	O
makes	O
the	O
least	O
number	O
of	O
assumptions	O
the	O
symbol	O
will	O
have	O
many	O
different	O
meanings	O
in	O
this	O
book	O
in	O
order	O
to	O
be	O
consistent	B
with	O
the	O
rest	O
of	O
the	O
literature	O
the	O
intended	O
meaning	O
should	O
be	O
clear	O
from	O
context	O
some	O
common	O
continuous	O
distributions	O
maximum	B
entropy	B
subject	O
to	O
the	O
constraint	O
of	O
having	O
a	O
specified	O
mean	B
and	O
variance	B
as	O
we	O
show	O
in	O
section	O
this	O
makes	O
it	O
a	O
good	O
default	O
choice	O
in	O
many	O
cases	O
finally	O
it	O
has	O
a	O
simple	O
mathematical	O
form	O
which	O
results	O
in	O
easy	O
to	O
implement	O
but	O
often	O
highly	O
effective	O
methods	O
as	O
we	O
will	O
see	O
see	O
ch	O
for	O
a	O
more	O
extensive	O
discussion	O
of	O
why	O
gaussians	O
are	O
so	O
widely	O
used	O
degenerate	B
pdf	B
in	O
the	O
limit	O
that	O
the	O
gaussian	B
becomes	O
an	O
infinitely	O
tall	O
and	O
infinitely	O
thin	O
spike	O
centered	O
at	O
where	O
is	O
called	O
a	O
dirac	B
delta	I
function	I
and	O
is	O
defined	O
as	O
n	O
lim	O
if	O
x	O
if	O
x	O
such	O
that	O
from	O
a	O
sum	O
or	O
integral	O
a	O
useful	O
property	O
of	O
delta	O
functions	O
is	O
the	O
sifting	B
property	I
which	O
selects	O
out	O
a	O
single	O
term	O
f	O
f	O
since	O
the	O
integrand	O
is	O
only	O
non-zero	O
if	O
x	O
one	O
problem	O
with	O
the	O
gaussian	B
distribution	O
is	O
that	O
it	O
is	O
sensitive	O
to	O
outliers	B
since	O
the	O
logprobability	O
only	O
decays	O
quadratically	O
with	O
distance	O
from	O
the	O
center	O
a	O
more	O
robust	B
distribution	O
is	O
the	O
student	B
t	I
its	O
pdf	B
is	O
as	O
follows	O
t	O
x	O
where	O
is	O
the	O
mean	B
is	O
the	O
scale	O
parameter	B
and	O
is	O
called	O
the	O
degrees	B
of	I
freedom	I
see	O
figure	O
for	O
some	O
plots	O
for	O
later	O
reference	O
we	O
note	O
that	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
mode	B
var	B
this	O
distribution	O
has	O
a	O
colourful	O
etymology	O
it	O
was	O
first	O
published	O
in	O
by	O
william	O
sealy	O
gosset	O
who	O
worked	O
at	O
the	O
guinness	O
brewery	O
in	O
dublin	O
since	O
his	O
employer	O
would	O
not	O
allow	O
him	O
to	O
use	O
his	O
own	O
name	O
he	O
called	O
it	O
the	O
student	O
distribution	O
the	O
origin	O
of	O
the	O
term	O
t	O
seems	O
to	O
have	O
arisen	O
in	O
the	O
context	O
of	O
tables	O
of	O
the	O
student	O
distribution	O
used	O
by	O
fisher	O
when	O
developing	O
the	O
basis	O
of	O
classical	B
statistical	O
inference	B
see	O
for	O
more	O
historical	O
details	O
gauss	O
student	O
laplace	B
chapter	O
probability	O
gauss	O
student	O
laplace	B
the	O
pdf	B
s	O
for	O
a	O
n	O
t	O
and	O
figure	O
the	O
mean	B
is	O
and	O
the	O
variance	B
is	O
for	O
both	O
the	O
gaussian	B
and	O
laplace	B
the	O
mean	B
and	O
variance	B
of	O
the	O
student	O
is	O
undefined	O
when	O
log	O
of	O
these	O
pdf	B
s	O
note	O
that	O
the	O
student	O
distribution	O
is	O
not	O
log-concave	O
for	O
any	O
parameter	B
value	O
unlike	O
the	O
laplace	B
distribution	I
which	O
is	O
always	O
log-concave	O
log-convex	O
nevertheless	O
both	O
are	O
unimodal	O
figure	O
generated	O
by	O
studentlaplacepdfplot	O
gaussian	B
student	B
t	I
laplace	B
gaussian	B
student	B
t	I
laplace	B
figure	O
illustration	O
of	O
the	O
effect	O
of	O
outliers	B
on	O
fitting	O
gaussian	B
student	O
and	O
laplace	B
distributions	O
no	O
outliers	B
gaussian	B
and	O
student	O
curves	O
are	O
on	O
top	O
of	O
each	O
other	O
with	O
outliers	B
we	O
see	O
that	O
the	O
gaussian	B
is	O
more	O
affected	O
by	O
outliers	B
than	O
the	O
student	O
and	O
laplace	B
distributions	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
robustdemo	O
the	O
variance	B
is	O
only	O
defined	O
if	O
the	O
mean	B
is	O
only	O
defined	O
if	O
as	O
an	O
illustration	O
of	O
the	O
robustness	B
of	O
the	O
student	O
distribution	O
consider	O
figure	O
on	O
the	O
left	O
we	O
show	O
a	O
gaussian	B
and	O
a	O
student	O
fit	O
to	O
some	O
data	O
with	O
no	O
outliers	B
on	O
the	O
right	O
we	O
add	O
some	O
outliers	B
we	O
see	O
that	O
the	O
gaussian	B
is	O
affected	O
a	O
lot	O
whereas	O
the	O
student	O
distribution	O
hardly	O
changes	O
this	O
is	O
because	O
the	O
student	O
has	O
heavier	O
tails	O
at	O
least	O
for	O
small	O
figure	O
if	O
this	O
distribution	O
is	O
known	O
as	O
the	O
cauchy	B
or	O
lorentz	B
distribution	O
this	O
is	O
notable	O
to	O
ensure	O
finite	O
variance	B
we	O
require	O
for	O
having	O
such	O
heavy	B
tails	I
that	O
the	O
integral	O
that	O
defines	O
the	O
mean	B
does	O
not	O
converge	B
it	O
is	O
common	O
to	O
use	O
which	O
gives	O
good	O
performance	O
in	O
a	O
range	O
of	O
problems	O
et	O
al	O
for	O
the	O
student	O
distribution	O
rapidly	O
approaches	O
a	O
gaussian	B
distribution	O
and	O
loses	O
its	O
robustness	B
properties	O
some	O
common	O
continuous	O
distributions	O
gamma	B
distributions	O
if	O
a	O
the	O
mode	B
is	O
at	O
otherwise	O
it	O
is	O
as	O
figure	O
some	O
gaa	O
b	O
distributions	O
we	O
increase	O
the	O
rate	B
b	O
we	O
reduce	O
the	O
horizontal	O
scale	O
thus	O
squeezing	O
everything	O
leftwards	O
and	O
upwards	O
figure	O
generated	O
by	O
gammaplotdemo	O
an	O
empirical	O
pdf	B
of	O
some	O
rainfall	O
data	O
with	O
a	O
fitted	O
gamma	B
distribution	I
superimposed	O
figure	O
generated	O
by	O
gammarainfalldemo	O
the	O
laplace	B
distribution	I
another	O
distribution	O
with	O
heavy	B
tails	I
is	O
the	O
laplace	B
also	O
known	O
as	O
the	O
double	B
sided	I
exponential	B
distribution	I
this	O
has	O
the	O
following	O
pdf	B
lapx	O
b	O
exp	O
b	O
here	O
is	O
a	O
location	O
parameter	B
and	O
b	O
is	O
a	O
scale	O
parameter	B
see	O
figure	O
for	O
a	O
plot	O
this	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
mode	B
var	B
its	O
robustness	B
to	O
outliers	B
is	O
illustrated	O
in	O
figure	O
it	O
also	O
put	O
mores	O
probability	O
density	O
at	O
than	O
the	O
gaussian	B
this	O
property	O
is	O
a	O
useful	O
way	O
to	O
encourage	O
sparsity	B
in	O
a	O
model	O
as	O
we	O
will	O
see	O
in	O
section	O
the	O
gamma	B
distribution	I
the	O
gamma	B
distribution	I
is	O
a	O
flexible	O
distribution	O
for	O
positive	O
real	O
valued	O
rv	O
s	O
x	O
defined	O
in	O
terms	O
of	O
two	O
parameters	O
called	O
the	O
shape	O
a	O
and	O
the	O
rate	B
b	O
it	O
is	O
gatshape	O
a	O
rate	B
b	O
ba	O
t	O
a	O
t	O
b	O
pierre-simon	O
laplace	B
was	O
a	O
french	O
mathematician	O
who	O
played	O
a	O
key	O
role	O
in	O
creating	O
the	O
field	O
of	O
bayesian	B
statistics	I
there	O
is	O
an	O
alternative	O
parameterization	O
where	O
we	O
use	O
the	O
scale	O
parameter	B
instead	O
of	O
the	O
rate	B
gasta	O
b	O
gata	O
this	O
version	O
is	O
the	O
one	O
used	O
by	O
matlab	O
s	O
gampdf	O
although	O
in	O
this	O
book	O
will	O
use	O
the	O
rate	B
parameterization	O
unless	O
otherwise	O
specified	O
chapter	O
probability	O
where	O
is	O
the	O
gamma	B
function	I
ux	O
udu	O
a	O
b	O
see	O
figure	O
for	O
some	O
plots	O
for	O
later	O
reference	O
we	O
note	O
that	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
a	O
b	O
mode	B
var	B
a	O
there	O
are	O
several	O
distributions	O
which	O
are	O
just	O
special	O
cases	O
of	O
the	O
gamma	B
which	O
we	O
discuss	O
erlang	B
distribution	I
this	O
is	O
the	O
same	O
as	O
the	O
gamma	B
distribution	I
where	O
a	O
is	O
an	O
integer	O
below	O
exponential	B
distribution	I
this	O
is	O
defined	O
by	O
exponx	O
where	O
is	O
the	O
rate	B
parameter	B
this	O
distribution	O
describes	O
the	O
times	O
between	O
events	O
in	O
a	O
poisson	B
process	O
i	O
e	O
a	O
process	O
in	O
which	O
events	O
occur	O
continuously	O
and	O
independently	O
at	O
a	O
constant	O
average	O
rate	B
it	O
is	O
common	O
to	O
fix	O
a	O
yielding	O
the	O
one-parameter	O
erlang	B
distribution	I
erlangx	O
where	O
is	O
the	O
rate	B
parameter	B
this	O
is	O
the	O
distribution	O
of	O
the	O
sum	O
of	O
squared	O
gaussian	B
random	O
variables	O
more	O
precisely	O
if	O
zi	O
n	O
and	O
s	O
chi-squared	B
distribution	I
this	O
is	O
defined	O
by	O
gax	O
i	O
then	O
s	O
z	O
that	O
another	O
useful	O
result	O
is	O
the	O
following	O
x	O
iga	O
b	O
whereig	O
is	O
the	O
inverse	B
gamma	B
distribution	I
defined	O
by	O
igxshape	O
a	O
scale	O
b	O
x	O
bx	O
ba	O
if	O
x	O
gaa	O
b	O
then	O
one	O
can	O
show	O
the	O
distribution	O
has	O
these	O
properties	O
mean	B
b	O
a	O
mode	B
b	O
a	O
var	B
the	O
mean	B
only	O
exists	O
if	O
a	O
the	O
variance	B
only	O
exists	O
if	O
a	O
we	O
will	O
see	O
applications	O
of	O
these	O
distributions	O
later	O
on	O
the	O
beta	B
distribution	I
the	O
beta	B
distribution	I
has	O
support	B
over	O
the	O
interval	O
and	O
is	O
defined	O
as	O
follows	O
betaxa	O
b	O
ba	O
b	O
xa	O
xb	O
here	O
bp	B
q	O
is	O
the	O
beta	B
function	I
ba	O
b	O
b	O
see	O
figure	O
for	O
plots	O
of	O
some	O
beta	O
distributions	O
we	O
require	O
a	O
b	O
to	O
ensure	O
the	O
distribution	O
if	O
is	O
integrable	O
to	O
ensure	O
ba	O
b	O
exists	O
if	O
a	O
b	O
we	O
get	O
the	O
uniform	O
distirbution	O
some	O
common	O
continuous	O
distributions	O
beta	O
distributions	O
figure	O
some	O
beta	O
distributions	O
figure	O
generated	O
by	O
betaplotdemo	O
a	O
and	O
b	O
are	O
both	O
less	O
than	O
we	O
get	O
a	O
bimodal	O
distribution	O
with	O
spikes	O
at	O
and	O
if	O
a	O
and	O
b	O
are	O
both	O
greater	O
than	O
the	O
distribution	O
is	O
unimodal	O
for	O
later	O
reference	O
we	O
note	O
that	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
a	O
a	O
b	O
mode	B
pareto	B
distribution	I
a	O
a	O
b	O
var	B
ab	O
b	O
the	O
pareto	B
distribution	I
is	O
used	O
to	O
model	O
the	O
distribution	O
of	O
quantities	O
that	O
exhibit	O
long	B
tails	I
also	O
called	O
heavy	B
tails	I
for	O
example	O
it	O
has	O
been	O
observed	O
that	O
the	O
most	O
frequent	O
word	O
in	O
english	O
the	O
occurs	O
approximately	O
twice	O
as	O
often	O
as	O
the	O
second	O
most	O
frequent	O
word	O
of	O
which	O
occurs	O
twice	O
as	O
often	O
as	O
the	O
fourth	O
most	O
frequent	O
word	O
etc	O
if	O
we	O
plot	O
the	O
frequency	O
of	O
words	O
vs	O
their	O
rank	O
we	O
will	O
get	O
a	O
power	B
law	I
this	O
is	O
known	O
as	O
zipf	O
s	O
law	O
wealth	O
has	O
a	O
similarly	O
skewed	O
distribution	O
especially	O
in	O
plutocracies	B
such	O
as	O
the	O
the	O
pareto	O
pdf	B
is	O
defined	O
as	O
follow	O
paretoxk	O
m	O
kmkx	O
ix	O
m	O
this	O
density	O
asserts	O
that	O
x	O
must	O
be	O
greater	O
than	O
some	O
constant	O
m	O
but	O
not	O
too	O
much	O
greater	O
where	O
k	O
controls	O
what	O
is	O
too	O
much	O
as	O
k	O
the	O
distribution	O
approaches	O
m	O
see	O
if	O
we	O
plot	O
the	O
distibution	O
on	O
a	O
log-log	O
scale	O
it	O
forms	O
a	O
straight	O
figure	O
for	O
some	O
plots	O
line	O
of	O
the	O
form	O
log	O
px	O
a	O
log	O
x	O
c	O
for	O
some	O
constants	O
a	O
and	O
c	O
see	O
figure	O
for	O
an	O
illustration	O
is	O
known	O
as	O
a	O
power	B
law	I
this	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
km	O
k	O
if	O
k	O
mode	B
m	O
var	B
if	O
k	O
americans	O
have	O
more	O
wealth	O
than	O
half	O
of	O
in	O
the	O
usa	O
see	O
and	O
pierson	O
for	O
a	O
political	O
analysis	O
of	O
how	O
such	O
an	O
extreme	O
distribution	O
of	O
income	O
has	O
arisen	O
in	O
a	O
democratic	O
country	O
all	O
americans	O
combined	O
chapter	O
probability	O
pareto	B
distribution	I
k	O
on	O
log	O
scale	O
the	O
pareto	B
distribution	I
paretoxm	O
k	O
for	O
m	O
the	O
pdf	B
on	O
a	O
log-log	O
scale	O
figure	O
figure	O
generated	O
by	O
paretoplot	O
joint	O
probability	O
distributions	O
so	O
far	O
we	O
have	O
been	O
mostly	O
focusing	O
on	O
modeling	O
univariate	O
probability	O
distributions	O
in	O
this	O
section	O
we	O
start	O
our	O
discussion	O
of	O
the	O
more	O
challenging	O
problem	O
of	O
building	O
joint	O
probability	O
distributions	O
on	O
multiple	O
related	O
random	O
variables	O
this	O
will	O
be	O
a	O
central	O
topic	B
in	O
this	O
book	O
a	O
joint	B
probability	I
distribution	I
has	O
the	O
form	O
xd	O
for	O
a	O
set	O
ofd	O
variables	O
and	O
models	O
the	O
relationships	O
between	O
the	O
variables	O
if	O
all	O
the	O
variables	O
are	O
discrete	B
we	O
can	O
represent	O
the	O
joint	B
distribution	I
as	O
a	O
big	O
multi-dimensional	O
array	O
with	O
one	O
variable	O
per	O
dimension	O
however	O
the	O
number	O
of	O
parameters	O
needed	O
to	O
define	O
such	O
a	O
model	O
is	O
ok	O
d	O
where	O
k	O
is	O
the	O
number	O
of	O
states	O
for	O
each	O
variable	O
we	O
can	O
define	O
high	O
dimensional	O
joint	O
distributions	O
using	O
fewer	O
parameters	O
by	O
making	O
conditional	B
independence	I
assumptions	O
as	O
we	O
explain	O
in	O
chapter	O
in	O
the	O
case	O
of	O
continuous	O
distributions	O
an	O
alternative	O
approach	O
is	O
to	O
restrict	O
the	O
form	O
of	O
the	O
pdf	B
to	O
certain	O
functional	O
forms	O
some	O
of	O
which	O
we	O
will	O
examine	O
below	O
covariance	B
and	O
correlation	O
the	O
covariance	B
between	O
two	O
rv	O
s	O
x	O
and	O
y	O
measures	O
the	O
degree	B
to	O
which	O
x	O
and	O
y	O
are	O
related	O
covariance	B
is	O
defined	O
as	O
cov	O
y	O
e	O
e	O
e	O
e	O
e	O
e	O
joint	O
probability	O
distributions	O
figure	O
several	O
sets	O
of	O
y	O
points	O
with	O
the	O
correlation	B
coefficient	I
of	O
x	O
and	O
y	O
for	O
each	O
set	O
note	O
that	O
the	O
correlation	O
reflects	O
the	O
noisiness	O
and	O
direction	O
of	O
a	O
linear	O
relationship	O
row	O
but	O
not	O
the	O
slope	O
of	O
that	O
relationship	O
nor	O
many	O
aspects	O
of	O
nonlinear	O
relationships	O
n	O
b	O
the	O
figure	O
in	O
the	O
center	O
has	O
a	O
slope	O
of	O
but	O
in	O
that	O
case	O
the	O
correlation	B
coefficient	I
is	O
undefined	O
because	O
the	O
variance	B
of	O
y	O
is	O
zero	O
source	O
httpen	O
wikipedia	O
orgwikifilecorrelation	O
examples	O
png	O
if	O
x	O
is	O
a	O
d-dimensional	O
random	O
vector	O
its	O
covariance	B
matrix	I
is	O
defined	O
to	O
be	O
the	O
following	O
symmetric	B
positive	O
definite	O
matrix	O
cov	O
e	O
e	O
e	O
var	B
t	O
cov	O
cov	O
var	B
cov	O
cov	O
cov	O
xd	O
cov	O
xd	O
var	B
covariances	O
can	O
be	O
between	O
and	O
infinity	O
sometimes	O
it	O
is	O
more	O
convenient	O
to	O
work	O
with	O
a	O
normalized	O
measure	O
with	O
a	O
finite	O
upper	O
bound	O
the	O
correlation	B
coefficient	I
between	O
x	O
and	O
y	O
is	O
defined	O
as	O
corr	O
y	O
cov	O
y	O
var	B
var	B
a	O
correlation	B
matrix	I
has	O
the	O
form	O
corr	O
r	O
corr	O
corr	O
xd	O
corr	O
xd	O
corr	O
corr	O
one	O
can	O
show	O
that	O
corr	O
y	O
hence	O
in	O
a	O
correlation	B
matrix	I
each	O
entry	O
on	O
the	O
diagonal	B
is	O
and	O
the	O
other	O
entries	O
are	O
between	O
and	O
one	O
can	O
also	O
show	O
that	O
corr	O
y	O
if	O
and	O
only	O
if	O
y	O
ax	O
b	O
for	O
some	O
parameters	O
a	O
and	O
b	O
i	O
e	O
if	O
there	O
is	O
a	O
linear	O
relationship	O
between	O
x	O
and	O
y	O
exercise	O
intuitively	O
one	O
chapter	O
probability	O
might	O
expect	O
the	O
correlation	B
coefficient	I
to	O
be	O
related	O
to	O
the	O
slope	O
of	O
the	O
regression	B
line	O
i	O
e	O
the	O
coefficient	O
a	O
in	O
the	O
expression	O
y	O
ax	O
b	O
however	O
as	O
we	O
show	O
in	O
equation	O
later	O
the	O
regression	B
coefficient	O
is	O
in	O
fact	O
given	O
by	O
a	O
cov	O
y	O
a	O
better	O
way	O
to	O
think	O
of	O
the	O
correlation	B
coefficient	I
is	O
as	O
a	O
degree	B
of	O
linearity	O
see	O
figure	O
if	O
x	O
and	O
y	O
are	O
independent	O
meaning	O
px	O
y	O
xpy	O
section	O
then	O
cov	O
y	O
and	O
hence	O
corr	O
y	O
so	O
they	O
are	O
uncorrelated	O
however	O
the	O
converse	O
is	O
not	O
true	O
uncorrelated	O
does	O
not	O
imply	O
independent	O
for	O
example	O
let	O
x	O
u	O
and	O
y	O
x	O
clearly	O
y	O
is	O
dependent	O
on	O
x	O
fact	O
y	O
is	O
uniquely	O
determined	O
by	O
x	O
yet	O
one	O
can	O
show	O
that	O
corr	O
y	O
some	O
striking	O
examples	O
of	O
this	O
fact	O
are	O
shown	O
in	O
figure	O
this	O
shows	O
several	O
data	O
sets	O
where	O
there	O
is	O
clear	O
dependendence	O
between	O
x	O
and	O
y	O
and	O
yet	O
the	O
correlation	B
coefficient	I
is	O
a	O
more	O
general	O
measure	O
of	O
dependence	O
between	O
random	O
variables	O
is	O
mutual	B
information	B
discussed	O
in	O
section	O
this	O
is	O
only	O
zero	O
if	O
the	O
variables	O
truly	O
are	O
independent	O
the	O
multivariate	B
gaussian	B
the	O
multivariate	B
gaussian	B
or	O
multivariate	B
normal	B
is	O
the	O
most	O
widely	O
used	O
joint	O
probability	B
density	I
function	I
for	O
continuous	O
variables	O
we	O
discuss	O
mvns	O
in	O
detail	O
in	O
chapter	O
here	O
we	O
just	O
give	O
some	O
definitions	O
and	O
plots	O
the	O
pdf	B
of	O
the	O
mvn	B
in	O
d	O
dimensions	O
is	O
defined	O
by	O
the	O
following	O
n	O
exp	O
where	O
e	O
r	O
d	O
is	O
the	O
mean	B
vector	O
and	O
cov	O
is	O
the	O
d	O
d	O
covariance	B
matrix	I
sometimes	O
we	O
will	O
work	O
in	O
terms	O
of	O
the	O
precision	B
matrix	I
or	O
concentration	B
matrix	I
instead	O
this	O
is	O
just	O
the	O
inverse	O
covariance	B
matrix	I
the	O
normalization	O
constant	O
just	O
ensures	O
that	O
the	O
pdf	B
integrates	O
to	O
exercise	O
figure	O
plots	O
some	O
mvn	B
densities	O
in	O
for	O
three	O
different	O
kinds	O
of	O
covariance	B
matrices	O
a	O
full	B
covariance	B
matrix	I
has	O
dd	O
parameters	O
divide	O
by	O
since	O
is	O
symmetric	B
a	O
diagonal	B
covariance	B
matrix	I
has	O
d	O
parameters	O
and	O
has	O
in	O
the	O
off-diagonal	O
terms	O
a	O
spherical	B
or	O
isotropic	B
covariance	B
has	O
one	O
free	O
parameter	B
multivariate	B
student	B
t	I
distribution	I
a	O
more	O
robust	B
alternative	O
to	O
the	O
mvn	B
is	O
the	O
multivariate	B
student	B
t	I
distribution	I
whose	O
pdf	B
is	O
given	O
by	O
t	O
v	O
v	O
where	O
is	O
called	O
the	O
scale	O
matrix	O
it	O
is	O
not	O
exactly	O
the	O
covariance	B
matrix	I
and	O
v	O
this	O
has	O
fatter	O
tails	O
than	O
a	O
gaussian	B
the	O
smaller	O
is	O
the	O
fatter	O
the	O
tails	O
as	O
the	O
joint	O
probability	O
distributions	O
full	B
spherical	B
diagonal	B
spherical	B
figure	O
we	O
show	O
the	O
level	B
sets	I
for	O
gaussians	O
a	O
full	B
covariance	B
matrix	I
has	O
elliptical	O
contours	O
a	O
diagonal	B
covariance	B
matrix	I
is	O
an	O
axis	B
aligned	I
ellipse	O
a	O
spherical	B
covariance	B
matrix	I
has	O
a	O
circular	O
shape	O
surface	O
plot	O
for	O
the	O
spherical	B
gaussian	B
in	O
figure	O
generated	O
by	O
distribution	O
tends	O
towards	O
a	O
gaussian	B
the	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
mode	B
cov	O
dirichlet	B
distribution	I
a	O
multivariate	O
generalization	B
of	O
the	O
beta	B
distribution	I
is	O
the	O
dirichlet	B
which	O
has	O
support	B
over	O
the	O
probability	B
simplex	I
defined	O
by	O
xk	O
x	O
k	O
k	O
ix	O
sk	O
sk	O
xk	O
the	O
pdf	B
is	O
defined	O
as	O
follows	O
dirx	O
b	O
johann	O
dirichlet	B
was	O
a	O
german	O
mathematician	O
chapter	O
probability	O
p	O
figure	O
the	O
dirichlet	B
distribution	I
when	O
k	O
defines	O
a	O
distribution	O
over	O
the	O
simplex	O
which	O
can	O
be	O
represented	O
by	O
the	O
triangular	O
surface	O
points	O
on	O
this	O
surface	O
satisfy	O
k	O
and	O
k	O
figure	O
generated	O
by	O
visdirichletgui	O
by	O
jonathan	O
huang	O
comb-like	O
structure	O
on	O
the	O
edges	B
is	O
a	O
plotting	O
artifact	O
figure	O
generated	O
by	O
plot	O
of	O
the	O
dirichlet	B
density	O
when	O
samples	B
from	O
dir	O
samples	B
from	O
dir	O
figure	O
this	O
results	O
in	O
very	O
sparse	B
distributions	O
with	O
many	O
results	O
in	O
more	O
uniform	O
dense	O
distributions	O
figure	O
generated	O
by	O
dirichlethistogramdemo	O
samples	B
from	O
a	O
symmetric	B
dirichlet	B
distribution	I
for	O
different	O
parameter	B
values	O
this	O
where	O
b	O
k	O
is	O
the	O
natural	O
generalization	B
of	O
the	O
beta	B
function	I
to	O
k	O
variables	O
transformations	O
of	O
random	O
variables	O
b	O
where	O
k	O
k	O
figure	O
shows	O
some	O
plots	O
of	O
the	O
dirichlet	B
when	O
k	O
and	O
figure	O
for	O
some	O
sampled	O
probability	O
vectors	O
we	O
see	O
that	O
k	O
controls	O
the	O
strength	O
of	O
the	O
distribution	O
peaked	O
it	O
is	O
and	O
the	O
k	O
control	O
where	O
the	O
peak	O
occurs	O
for	O
example	O
is	O
a	O
uniform	B
distribution	I
is	O
a	O
broad	O
distribution	O
centered	O
at	O
and	O
is	O
a	O
narrow	O
distribution	O
centered	O
at	O
if	O
k	O
for	O
all	O
k	O
we	O
get	O
spikes	O
at	O
the	O
corner	O
of	O
the	O
simplex	O
for	O
future	O
reference	O
the	O
distribution	O
has	O
these	O
properties	O
e	O
mode	B
var	B
k	O
k	O
k	O
k	O
where	O
case	O
the	O
mean	B
becomes	O
and	O
the	O
variance	B
becomes	O
var	B
k	O
increases	O
the	O
precision	B
the	O
variance	B
of	O
the	O
distribution	O
k	O
k	O
often	O
we	O
use	O
a	O
symmetric	B
dirichlet	B
prior	O
of	O
the	O
form	O
k	O
in	O
this	O
so	O
increasing	O
k	O
transformations	O
of	O
random	O
variables	O
if	O
x	O
p	O
is	O
some	O
random	O
variable	O
and	O
y	O
f	O
what	O
is	O
the	O
distribution	O
of	O
y	O
this	O
is	O
the	O
question	O
we	O
address	O
in	O
this	O
section	O
linear	O
transformations	O
suppose	O
f	O
is	O
a	O
linear	O
function	O
y	O
f	O
ax	O
b	O
in	O
this	O
case	O
we	O
can	O
easily	O
derive	O
the	O
mean	B
and	O
covariance	B
of	O
y	O
as	O
follows	O
first	O
for	O
the	O
mean	B
we	O
have	O
e	O
e	O
b	O
a	O
b	O
where	O
e	O
this	O
is	O
called	O
the	O
linearity	B
of	I
expectation	I
if	O
f	O
is	O
a	O
scalar-valued	O
function	O
f	O
at	O
x	O
b	O
the	O
corresponding	O
result	O
is	O
e	O
at	O
x	O
b	O
at	O
b	O
for	O
the	O
covariance	B
we	O
have	O
cov	O
cov	O
b	O
a	O
at	O
where	O
cov	O
we	O
leave	O
the	O
proof	O
of	O
this	O
as	O
an	O
exercise	O
if	O
f	O
is	O
scalar	O
valued	O
the	O
result	O
becomes	O
var	B
var	B
at	O
x	O
b	O
at	O
a	O
chapter	O
probability	O
we	O
will	O
use	O
both	O
of	O
these	O
results	O
extensively	O
in	O
later	O
chapters	O
note	O
however	O
that	O
the	O
mean	B
and	O
covariance	B
only	O
completely	O
define	O
the	O
distribution	O
of	O
y	O
if	O
x	O
is	O
gaussian	B
in	O
general	O
we	O
must	O
use	O
the	O
techniques	O
described	O
below	O
to	O
derive	O
the	O
full	B
distribution	O
of	O
y	O
as	O
opposed	O
to	O
just	O
its	O
first	O
two	O
moments	O
general	O
transformations	O
if	O
x	O
is	O
a	O
discrete	B
rv	O
we	O
can	O
derive	O
the	O
pmf	B
for	O
y	O
by	O
simply	O
summing	O
up	O
the	O
probability	O
mass	O
for	O
all	O
the	O
x	O
s	O
such	O
that	O
f	O
pyy	O
pxx	O
xf	O
for	O
example	O
if	O
f	O
if	O
x	O
is	O
even	O
and	O
f	O
otherwise	O
and	O
pxx	O
is	O
uniform	O
on	O
the	O
set	O
then	O
x	O
pxx	O
and	O
similarly	O
note	O
that	O
in	O
this	O
example	O
f	O
is	O
a	O
many-to-one	O
function	O
if	O
x	O
is	O
continuous	O
we	O
cannot	O
use	O
equation	O
since	O
pxx	O
is	O
a	O
density	O
not	O
a	O
pmf	B
and	O
we	O
cannot	O
sum	O
up	O
densities	O
instead	O
we	O
work	O
with	O
cdf	B
s	O
and	O
write	O
pyy	O
p	O
y	O
p	O
y	O
p	O
y	O
we	O
can	O
derive	O
the	O
pdf	B
of	O
y	O
by	O
differentiating	O
the	O
cdf	B
in	O
the	O
case	O
of	O
monotonic	O
and	O
hence	O
invertible	O
functions	O
we	O
can	O
write	O
pyy	O
p	O
y	O
p	O
f	O
pxf	O
taking	O
derivatives	O
we	O
get	O
pxf	O
d	O
dy	O
pyy	O
d	O
pyy	O
dy	O
we	O
can	O
think	O
of	O
dx	O
as	O
a	O
measure	O
of	O
volume	O
in	O
the	O
x-space	O
similarly	O
dy	O
dy	O
measures	O
the	O
change	O
in	O
volume	O
since	O
the	O
sign	O
of	O
this	O
where	O
x	O
f	O
measures	O
volume	O
in	O
y	O
space	O
thus	O
dx	O
change	O
is	O
not	O
important	O
we	O
take	O
the	O
absolute	O
value	O
to	O
get	O
the	O
general	O
expression	O
pxx	O
dx	O
dy	O
dx	O
dy	O
d	O
dx	O
pxx	O
dx	O
dy	O
pyy	O
pxx	O
this	O
is	O
called	O
change	B
of	I
variables	I
formula	O
we	O
can	O
understand	O
this	O
result	O
more	O
intuitively	O
as	O
follows	O
observations	O
falling	O
in	O
the	O
range	O
x	O
x	O
will	O
get	O
transformed	O
into	O
y	O
y	O
where	O
pxx	O
x	O
pyy	O
y	O
hence	O
pyy	O
pxx	O
x	O
y	O
for	O
example	O
suppose	O
x	O
u	O
and	O
y	O
x	O
then	O
pyy	O
y	O
see	O
also	O
exercise	O
multivariate	O
change	B
of	I
variables	I
we	O
can	O
extend	O
the	O
previous	O
results	O
to	O
multivariate	O
distributions	O
as	O
follows	O
let	O
f	O
be	O
a	O
function	O
that	O
maps	O
r	O
n	O
and	O
let	O
y	O
f	O
then	O
its	O
jacobian	B
matrix	I
j	O
is	O
given	O
by	O
n	O
to	O
r	O
jx	O
y	O
yn	O
xn	O
yn	O
xn	O
yn	O
xn	O
transformations	O
of	O
random	O
variables	O
det	O
j	O
measures	O
how	O
much	O
a	O
unit	O
cube	O
changes	O
in	O
volume	O
when	O
we	O
apply	O
f	O
jacobian	B
of	O
the	O
inverse	O
mapping	O
y	O
x	O
if	O
f	O
is	O
an	O
invertible	O
mapping	O
we	O
can	O
define	O
the	O
pdf	B
of	O
the	O
transformed	O
variables	O
using	O
the	O
det	O
x	O
y	O
pxx	O
det	O
jy	O
x	O
pyy	O
pxx	O
in	O
exercise	O
you	O
will	O
use	O
this	O
formula	O
to	O
derive	O
the	O
normalization	O
constant	O
for	O
a	O
multivariate	B
gaussian	B
as	O
a	O
simple	O
example	O
consider	O
transforming	O
a	O
density	O
from	O
cartesian	B
coordinates	O
x	O
to	O
polar	B
coordinates	O
y	O
wherex	O
r	O
cos	O
and	O
r	O
sin	O
then	O
cos	O
r	O
sin	O
r	O
cos	O
sin	O
jy	O
x	O
r	O
r	O
and	O
det	O
j	O
r	O
hence	O
pyy	O
xx	O
det	O
j	O
pr	O
cos	O
r	O
sin	O
to	O
see	O
this	O
geometrically	O
notice	O
that	O
the	O
area	O
of	O
the	O
shaded	O
patch	O
in	O
figure	O
is	O
given	O
by	O
p	O
r	O
r	O
dr	O
d	O
r	O
in	O
the	O
limit	O
this	O
is	O
equal	O
to	O
the	O
density	O
at	O
the	O
center	O
of	O
the	O
patch	O
pr	O
times	O
the	O
size	O
of	O
the	O
patch	O
r	O
dr	O
d	O
hence	O
pr	O
cos	O
r	O
sin	O
dr	O
d	O
central	B
limit	I
theorem	I
now	O
consider	O
n	O
random	O
variables	O
with	O
pdf	B
s	O
necessarily	O
gaussian	B
pxi	O
each	O
with	O
mean	B
and	O
variance	B
we	O
assume	O
each	O
variable	O
is	O
independent	B
and	I
identically	I
distributed	I
or	O
iid	B
for	O
short	O
let	O
sn	O
xi	O
be	O
the	O
sum	O
of	O
the	O
rv	O
s	O
this	O
is	O
a	O
simple	O
but	O
widely	O
used	O
transformation	O
of	O
rv	O
s	O
one	O
can	O
show	O
that	O
as	O
n	O
increases	O
the	O
distribution	O
of	O
this	O
sum	O
approaches	O
psn	O
s	O
n	O
exp	O
n	O
hence	O
the	O
distribution	O
of	O
the	O
quantity	O
zn	O
sn	O
n	O
n	O
x	O
n	O
converges	O
to	O
the	O
standard	B
normal	B
where	O
x	O
n	O
the	O
central	B
limit	I
theorem	I
see	O
e	O
g	O
or	O
for	O
a	O
proof	O
xi	O
is	O
the	O
sample	O
mean	B
this	O
is	O
called	O
in	O
figure	O
we	O
give	O
an	O
example	O
in	O
which	O
we	O
compute	O
the	O
mean	B
of	O
rv	O
s	O
drawn	O
from	O
a	O
beta	B
distribution	I
we	O
see	O
that	O
the	O
sampling	B
distribution	I
of	O
the	O
mean	B
value	O
rapidly	O
converges	O
to	O
a	O
gaussian	B
distribution	O
chapter	O
probability	O
figure	O
change	B
of	I
variables	I
from	O
polar	B
to	O
cartesian	B
the	O
area	O
of	O
the	O
shaded	O
patch	O
is	O
r	O
dr	O
d	O
based	O
on	O
figure	O
n	O
n	O
figure	O
the	O
central	B
limit	I
theorem	I
in	O
pictures	O
we	O
plot	O
a	O
histogram	B
of	O
for	O
j	O
as	O
n	O
the	O
distribution	O
tends	O
towards	O
a	O
gaussian	B
n	O
n	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
centrallimitdemo	O
xij	O
where	O
xij	O
n	O
monte	B
carlo	I
approximation	O
in	O
general	O
computing	O
the	O
distribution	O
of	O
a	O
function	O
of	O
an	O
rv	O
using	O
the	O
change	B
of	I
variables	I
formula	O
can	O
be	O
difficult	O
one	O
simple	O
but	O
powerful	O
alternative	O
is	O
as	O
follows	O
first	O
we	O
generate	O
s	O
samples	B
from	O
the	O
distribution	O
call	O
them	O
xs	O
are	O
many	O
ways	O
to	O
generate	O
such	O
samples	B
one	O
popular	O
method	O
for	O
high	O
dimensional	O
distributions	O
is	O
called	O
markov	B
chain	I
monte	B
carlo	I
or	O
mcmc	B
this	O
will	O
be	O
explained	O
in	O
chapter	O
given	O
the	O
samples	B
we	O
can	O
approximate	O
the	O
distribution	O
of	O
f	O
by	O
using	O
the	O
empirical	B
distribution	I
of	O
this	O
is	O
called	O
a	O
monte	B
carlo	I
approximation	O
named	O
after	O
a	O
city	O
in	O
europe	O
known	O
for	O
its	O
plush	O
gambling	O
casinos	O
monte	B
carlo	I
techniques	O
were	O
first	O
developed	O
in	O
the	O
area	O
of	O
statistical	O
physics	O
in	O
particular	O
during	O
development	O
of	O
the	O
atomic	B
bomb	I
but	O
are	O
now	O
widely	O
used	O
in	O
statistics	O
and	O
machine	B
learning	B
as	O
well	O
we	O
can	O
use	O
monte	B
carlo	I
to	O
approximate	O
the	O
expected	B
value	I
of	O
any	O
function	O
of	O
a	O
random	O
monte	B
carlo	I
approximation	O
figure	O
computing	O
the	O
distribution	O
of	O
y	O
where	O
px	O
is	O
uniform	O
the	O
analytic	O
result	O
is	O
shown	O
in	O
the	O
middle	O
and	O
the	O
monte	B
carlo	I
approximation	O
is	O
shown	O
on	O
the	O
right	O
figure	O
generated	O
by	O
variable	O
we	O
simply	O
draw	O
samples	B
and	O
then	O
compute	O
the	O
arithmetic	O
mean	B
of	O
the	O
function	O
applied	O
to	O
the	O
samples	B
this	O
can	O
be	O
written	O
as	O
follows	O
f	O
s	O
e	O
where	O
xs	O
px	O
this	O
is	O
called	O
monte	B
carlo	I
integration	I
and	O
has	O
the	O
advantage	O
over	O
numerical	O
integration	O
is	O
based	O
on	O
evaluating	O
the	O
function	O
at	O
a	O
fixed	O
grid	O
of	O
points	O
that	O
the	O
function	O
is	O
only	O
evaluated	O
in	O
places	O
where	O
there	O
is	O
non-negligible	O
probability	O
f	O
by	O
varying	O
the	O
function	O
f	O
we	O
can	O
approximate	O
many	O
quantities	O
of	O
interest	O
such	O
as	O
x	O
s	O
xs	O
e	O
var	B
s	O
c	O
p	O
c	O
s	O
xs	O
median	B
x	O
we	O
give	O
some	O
examples	O
below	O
and	O
will	O
see	O
many	O
more	O
in	O
later	O
chapters	O
example	O
change	B
of	I
variables	I
the	O
mc	O
way	O
in	O
section	O
we	O
discussed	O
how	O
to	O
analytically	O
compute	O
the	O
distribution	O
of	O
a	O
function	O
of	O
a	O
random	O
variable	O
y	O
f	O
a	O
much	O
simpler	O
approach	O
is	O
to	O
use	O
a	O
monte	B
carlo	I
approximation	O
for	O
example	O
suppose	O
x	O
unif	O
and	O
y	O
we	O
can	O
approximate	O
py	O
by	O
drawing	O
many	O
samples	B
from	O
px	O
squaring	O
them	O
and	O
computing	O
the	O
resulting	O
empirical	B
distribution	I
see	O
figure	O
for	O
an	O
illustration	O
we	O
will	O
use	O
this	O
technique	O
extensively	O
in	O
later	O
chapters	O
see	O
also	O
figure	O
chapter	O
probability	O
figure	O
estimating	O
by	O
monte	B
carlo	I
integration	I
blue	O
points	O
are	O
inside	O
the	O
circle	O
red	O
crosses	O
are	O
outside	O
figure	O
generated	O
by	O
mcestimatepi	O
example	O
estimating	O
by	O
monte	B
carlo	I
integration	I
mc	O
approximation	O
can	O
be	O
used	O
for	O
many	O
applications	O
not	O
just	O
statistical	O
ones	O
suppose	O
we	O
want	O
to	O
estimate	O
we	O
know	O
that	O
the	O
area	O
of	O
a	O
circle	O
with	O
radius	O
r	O
is	O
but	O
it	O
is	O
also	O
equal	O
to	O
the	O
following	O
definite	O
integral	O
r	O
r	O
i	O
r	O
r	O
hence	O
let	O
f	O
y	O
be	O
an	O
indicator	B
function	I
that	O
is	O
for	O
points	O
inside	O
the	O
circle	O
and	O
outside	O
and	O
let	O
px	O
and	O
py	O
be	O
uniform	O
distributions	O
on	O
r	O
r	O
sop	O
x	O
y	O
then	O
let	O
us	O
approximate	O
this	O
by	O
monte	B
carlo	I
integration	I
i	O
f	O
ypxpydxdy	O
s	O
f	O
ys	O
f	O
ypxpydxdy	O
we	O
find	O
with	O
standard	B
error	I
section	O
for	O
a	O
discussion	O
of	O
standard	B
errors	I
we	O
can	O
plot	O
the	O
points	O
that	O
are	O
accepted	O
rejected	O
as	O
in	O
figure	O
accuracy	O
of	O
monte	B
carlo	I
approximation	O
the	O
accuracy	O
of	O
an	O
mc	O
approximation	O
increases	O
with	O
sample	O
size	O
this	O
is	O
illustrated	O
in	O
figure	O
on	O
the	O
top	O
line	O
we	O
plot	O
a	O
histogram	B
of	O
samples	B
from	O
a	O
gaussian	B
distribution	O
on	O
the	O
bottom	O
line	O
we	O
plot	O
a	O
smoothed	O
version	O
of	O
these	O
samples	B
created	O
using	O
a	O
kernel	B
density	O
estimate	O
this	O
smoothed	O
distribution	O
is	O
then	O
evaluated	O
on	O
a	O
dense	O
grid	O
of	O
points	O
monte	B
carlo	I
approximation	O
samples	B
samples	B
samples	B
samples	B
figure	O
and	O
samples	B
from	O
a	O
gaussian	B
distribution	O
n	O
solid	O
red	O
line	O
is	O
true	O
pdf	B
top	O
line	O
histogram	B
of	O
samples	B
bottom	O
line	O
kernel	B
density	O
estimate	O
derived	O
from	O
samples	B
in	O
dotted	O
blue	O
solid	O
red	O
line	O
is	O
true	O
pdf	B
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
mcaccuracydemo	O
and	O
plotted	O
note	O
that	O
this	O
smoothing	B
is	O
just	O
for	O
the	O
purposes	O
of	O
plotting	O
it	O
is	O
not	O
used	O
for	O
the	O
monte	B
carlo	I
estimate	O
itself	O
if	O
we	O
denote	O
the	O
exact	O
mean	B
by	O
e	O
and	O
the	O
mc	O
approximation	O
by	O
one	O
can	O
show	O
that	O
with	O
independent	O
samples	B
n	O
s	O
where	O
var	B
e	O
e	O
f	O
this	O
is	O
a	O
consequence	O
of	O
the	O
central-limit	B
theorem	I
of	O
course	O
is	O
unknown	B
in	O
the	O
above	O
expression	O
but	O
it	O
can	O
also	O
be	O
estimated	O
by	O
mc	O
s	O
then	O
we	O
have	O
p	O
s	O
s	O
chapter	O
probability	O
s	O
is	O
called	O
the	O
or	O
empirical	O
standard	B
error	I
and	O
is	O
an	O
estimate	O
of	O
our	O
the	O
term	O
uncertainty	B
about	O
our	O
estimate	O
of	O
section	O
for	O
more	O
discussion	O
on	O
standard	B
errors	I
if	O
we	O
want	O
to	O
report	O
an	O
answer	O
which	O
is	O
accurate	O
to	O
within	O
with	O
probability	O
at	O
least	O
we	O
can	O
approximate	O
we	O
need	O
to	O
use	O
a	O
number	O
of	O
samples	B
s	O
which	O
satisfies	O
the	O
factor	B
by	O
yielding	O
s	O
information	B
theory	I
information	B
theory	I
is	O
concerned	O
with	O
representing	O
data	O
in	O
a	O
compact	O
fashion	O
task	O
known	O
as	O
data	B
compression	I
or	O
source	B
coding	I
as	O
well	O
as	O
with	O
transmitting	O
and	O
storing	O
it	O
in	O
a	O
way	O
that	O
is	O
robust	B
to	O
errors	O
task	O
known	O
as	O
error	B
correction	I
or	O
channel	B
coding	I
at	O
first	O
this	O
seems	O
far	O
removed	O
from	O
the	O
concerns	O
of	O
probability	B
theory	I
and	O
machine	B
learning	B
but	O
in	O
fact	O
there	O
is	O
an	O
intimate	O
connection	O
to	O
see	O
this	O
note	O
that	O
compactly	O
representing	O
data	O
requires	O
allocating	O
short	O
codewords	O
to	O
highly	O
probable	O
bit	O
strings	O
and	O
reserving	O
longer	O
codewords	O
to	O
less	O
probable	O
bit	O
strings	O
this	O
is	O
similar	B
to	O
the	O
situation	O
in	O
natural	O
language	O
where	O
common	O
words	O
as	O
a	O
the	O
and	O
are	O
generally	O
much	O
shorter	O
than	O
rare	O
words	O
also	O
decoding	B
messages	O
sent	O
over	O
noisy	O
channels	O
requires	O
having	O
a	O
good	O
probability	O
model	O
of	O
the	O
kinds	O
of	O
messages	O
that	O
people	O
tend	O
to	O
send	O
in	O
both	O
cases	O
we	O
need	O
a	O
model	O
that	O
can	O
predict	O
which	O
kinds	O
of	O
data	O
are	O
likely	O
and	O
which	O
unlikely	O
which	O
is	O
also	O
a	O
central	O
problem	O
in	O
machine	B
learning	B
for	O
more	O
details	O
on	O
the	O
connection	O
between	O
information	B
theory	I
and	O
machine	B
learning	B
obviously	O
we	O
cannot	O
go	O
into	O
the	O
details	O
of	O
information	B
theory	I
here	O
e	O
g	O
and	O
thomas	O
if	O
you	O
are	O
interested	O
to	O
learn	O
more	O
however	O
we	O
will	O
introduce	O
a	O
few	O
basic	O
concepts	O
that	O
we	O
will	O
need	O
later	O
in	O
the	O
book	O
entropy	B
the	O
entropy	B
of	O
a	O
random	O
variable	O
x	O
with	O
distribution	O
p	O
denoted	O
by	O
h	O
or	O
sometimes	O
h	O
is	O
a	O
measure	O
of	O
its	O
uncertainty	B
in	O
particular	O
for	O
a	O
discrete	B
variable	O
with	O
k	O
states	O
it	O
is	O
defined	O
by	O
h	O
px	O
k	O
px	O
k	O
usually	O
we	O
use	O
log	O
base	O
in	O
which	O
case	O
the	O
units	O
are	O
called	O
bits	B
for	O
binary	O
digits	O
if	O
we	O
use	O
log	O
base	O
e	O
the	O
units	O
are	O
called	O
nats	B
for	O
example	O
if	O
x	O
with	O
histogram	B
distribution	O
p	O
we	O
find	O
h	O
the	O
discrete	B
distribution	O
with	O
maximum	B
entropy	B
is	O
the	O
uniform	B
distribution	I
section	O
for	O
a	O
proof	O
hence	O
for	O
a	O
k-ary	O
random	O
variable	O
the	O
entropy	B
is	O
maximized	O
if	O
px	O
k	O
in	O
this	O
case	O
h	O
k	O
conversely	O
the	O
distribution	O
with	O
minimum	O
entropy	B
is	O
zero	O
is	O
any	O
delta-function	O
that	O
puts	O
all	O
its	O
mass	O
on	O
one	O
state	B
such	O
a	O
distribution	O
has	O
no	O
uncertainty	B
in	O
figure	O
where	O
we	O
plotted	O
a	O
dna	O
sequence	B
logo	I
the	O
height	O
of	O
each	O
bar	O
is	O
defined	O
to	O
be	O
h	O
where	O
h	O
is	O
the	O
entropy	B
of	O
that	O
distribution	O
and	O
is	O
the	O
maximum	O
possible	O
entropy	B
thus	O
a	O
bar	O
of	O
height	O
corresponds	O
to	O
a	O
uniform	B
distribution	I
whereas	O
a	O
bar	O
of	O
height	O
corresponds	O
to	O
a	O
deterministic	O
distribution	O
information	B
theory	I
x	O
h	O
px	O
figure	O
entropy	B
of	O
a	O
bernoulli	B
random	O
variable	O
as	O
a	O
function	O
of	O
the	O
maximum	B
entropy	B
is	O
figure	O
generated	O
by	O
bernoullientropyfig	O
for	O
the	O
special	O
case	O
of	O
binary	O
random	O
variables	O
x	O
we	O
can	O
write	O
px	O
and	O
px	O
hence	O
the	O
entropy	B
becomes	O
h	O
px	O
px	O
px	O
px	O
this	O
is	O
called	O
the	O
binary	B
entropy	B
function	I
and	O
is	O
also	O
written	O
h	O
we	O
plot	O
this	O
in	O
figure	O
we	O
see	O
that	O
the	O
maximum	O
value	O
of	O
occurs	O
when	O
the	O
distribution	O
is	O
uniform	O
kl	B
divergence	I
one	O
way	O
to	O
measure	O
the	O
dissimilarity	O
of	O
two	O
probability	O
distributions	O
p	O
and	O
q	O
is	O
known	O
as	O
the	O
kullback-leibler	B
divergence	I
divergence	O
orrelative	O
entropy	B
this	O
is	O
defined	O
as	O
follows	O
kl	O
pk	O
log	O
pk	O
qk	O
k	O
k	O
pk	O
log	O
pk	O
pk	O
log	O
qk	O
k	O
where	O
the	O
sum	O
gets	O
replaced	O
by	O
an	O
integral	O
for	O
we	O
can	O
rewrite	O
this	O
as	O
kl	O
pk	O
log	O
qk	O
h	O
q	O
where	O
h	O
q	O
is	O
called	O
the	O
cross	B
entropy	B
h	O
q	O
one	O
can	O
show	O
and	O
thomas	O
that	O
the	O
cross	B
entropy	B
is	O
the	O
average	O
number	O
of	O
bits	B
needed	O
to	O
encode	O
data	O
coming	O
from	O
a	O
source	O
with	O
distribution	O
p	O
when	O
we	O
use	O
model	O
q	O
to	O
the	O
kl	B
divergence	I
is	O
not	O
a	O
distance	O
since	O
it	O
is	O
asymmetric	O
one	O
symmetric	B
version	O
of	O
the	O
kl	B
divergence	I
is	O
the	O
jensen-shannon	B
divergence	I
defined	O
as	O
whereq	O
chapter	O
probability	O
define	O
our	O
codebook	B
hence	O
the	O
regular	B
entropy	B
h	O
h	O
p	O
defined	O
in	O
section	O
is	O
the	O
expected	O
number	O
of	O
bits	B
if	O
we	O
use	O
the	O
true	O
model	O
so	O
the	O
kl	B
divergence	I
is	O
the	O
difference	O
between	O
these	O
in	O
other	O
words	O
the	O
kl	B
divergence	I
is	O
the	O
average	O
number	O
of	O
extra	O
bits	B
needed	O
to	O
encode	O
the	O
data	O
due	O
to	O
the	O
fact	O
that	O
we	O
used	O
distribution	O
q	O
to	O
encode	O
the	O
data	O
instead	O
of	O
the	O
true	O
distribution	O
p	O
the	O
extra	O
number	O
of	O
bits	B
interpretation	O
should	O
make	O
it	O
clear	O
that	O
kl	O
and	O
that	O
the	O
kl	O
is	O
only	O
equal	O
to	O
zero	O
iff	B
q	O
p	O
we	O
now	O
give	O
a	O
proof	O
of	O
this	O
important	O
result	O
theorem	O
inequality	O
kl	O
with	O
equality	O
iff	B
p	O
q	O
proof	O
to	O
prove	O
the	O
theorem	O
we	O
need	O
to	O
use	O
jensen	O
s	O
inequality	O
this	O
states	O
that	O
for	O
any	O
convex	B
function	O
f	O
we	O
have	O
that	O
f	O
ixi	O
if	O
i	O
this	O
is	O
clearly	O
true	O
for	O
n	O
definition	O
of	O
convexity	O
and	O
let	O
us	O
now	O
prove	O
the	O
main	O
theorem	O
following	O
and	O
thomas	O
let	O
a	O
where	O
i	O
and	O
can	O
be	O
proved	O
by	O
induction	B
for	O
n	O
px	O
be	O
the	O
support	B
of	O
px	O
then	O
px	O
qx	O
kl	O
px	O
log	O
x	O
a	O
x	O
a	O
log	O
log	O
x	O
a	O
px	O
log	O
px	O
qx	O
px	O
log	O
qx	O
x	O
a	O
qx	O
log	O
x	O
x	O
qx	O
px	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
jensen	O
s	O
since	O
logx	O
is	O
a	O
strictly	O
concave	B
function	O
we	O
have	O
equality	O
in	O
equation	O
iff	B
px	O
for	O
some	O
c	O
we	O
have	O
equality	O
in	O
equation	O
x	O
x	O
qx	O
which	O
implies	O
c	O
hence	O
kl	O
iff	B
px	O
qx	O
iff	B
for	O
all	O
x	O
x	O
a	O
qx	O
one	O
important	O
consequence	O
of	O
this	O
result	O
is	O
that	O
the	O
discrete	B
distribution	O
with	O
the	O
maximum	B
entropy	B
is	O
the	O
uniform	B
distribution	I
more	O
precisely	O
h	O
log	O
where	O
is	O
the	O
number	O
of	O
states	O
for	O
x	O
with	O
equality	O
iff	B
px	O
is	O
uniform	O
to	O
see	O
this	O
let	O
ux	O
then	O
kl	O
x	O
px	O
log	O
px	O
ux	O
px	O
log	O
ux	O
h	O
log	O
x	O
px	O
log	O
px	O
x	O
this	O
is	O
a	O
formulation	O
of	O
laplace	B
s	O
principle	B
of	I
insufficient	I
reason	I
which	O
argues	O
in	O
favor	O
of	O
using	O
uniform	O
distributions	O
when	O
there	O
are	O
no	O
other	O
reasons	O
to	O
favor	O
one	O
distribution	O
over	O
another	O
see	O
section	O
for	O
a	O
discussion	O
of	O
how	O
to	O
create	O
distributions	O
that	O
satisfy	O
certain	O
constraints	O
but	O
otherwise	O
are	O
as	O
least-commital	O
as	O
possible	O
example	O
the	O
gaussian	B
satisfies	O
first	O
and	O
second	O
moment	O
constraints	O
but	O
otherwise	O
has	O
maximum	B
entropy	B
information	B
theory	I
mutual	B
information	B
consider	O
two	O
random	O
variables	O
x	O
and	O
y	O
suppose	O
we	O
want	O
to	O
know	O
how	O
much	O
knowing	O
one	O
variable	O
tells	O
us	O
about	O
the	O
other	O
we	O
could	O
compute	O
the	O
correlation	B
coefficient	I
but	O
this	O
is	O
only	O
defined	O
for	O
real-valued	O
random	O
variables	O
and	O
furthermore	O
this	O
is	O
a	O
very	O
limited	O
measure	O
of	O
dependence	O
as	O
we	O
saw	O
in	O
figure	O
a	O
more	O
general	O
approach	O
is	O
to	O
determine	O
how	O
similar	B
the	O
joint	B
distribution	I
px	O
y	O
is	O
to	O
the	O
factored	O
distribution	O
pxpy	O
this	O
is	O
called	O
the	O
mutual	B
information	B
or	O
mi	B
and	O
is	O
defined	O
as	O
follows	O
i	O
y	O
kl	O
y	O
px	O
y	O
log	O
px	O
y	O
pxpy	O
x	O
y	O
we	O
have	O
i	O
y	O
with	O
equality	O
iff	B
px	O
y	O
pxpy	O
that	O
is	O
the	O
mi	B
is	O
zero	O
iff	B
the	O
variables	O
are	O
independent	O
to	O
gain	O
insight	O
into	O
the	O
meaning	O
of	O
mi	B
it	O
helps	O
to	O
re-express	O
it	O
in	O
terms	O
of	O
joint	O
and	O
conditional	O
entropies	O
one	O
can	O
show	O
that	O
the	O
above	O
expression	O
is	O
equivalent	O
to	O
the	O
following	O
i	O
y	O
h	O
h	O
h	O
h	O
where	O
h	O
is	O
the	O
conditional	B
entropy	B
defined	O
as	O
h	O
x	O
pxh	O
x	O
thus	O
we	O
can	O
interpret	O
the	O
mi	B
between	O
x	O
and	O
y	O
as	O
the	O
reduction	O
in	O
uncertainty	B
about	O
x	O
after	O
observing	O
y	O
or	O
by	O
symmetry	O
the	O
reduction	O
in	O
uncertainty	B
about	O
y	O
after	O
observing	O
x	O
we	O
will	O
encounter	O
several	O
applications	O
of	O
mi	B
later	O
in	O
the	O
book	O
see	O
also	O
exercises	O
and	O
for	O
the	O
connection	O
between	O
mi	B
and	O
correlation	O
coefficients	O
a	O
quantity	O
which	O
is	O
closely	O
related	O
to	O
mi	B
is	O
the	O
pointwise	B
mutual	B
information	B
or	O
pmi	O
for	O
two	O
events	O
random	O
variables	O
x	O
and	O
y	O
this	O
is	O
defined	O
as	O
pmix	O
y	O
log	O
px	O
y	O
pxpy	O
log	O
pxy	O
px	O
pyx	O
py	O
log	O
this	O
measures	O
the	O
discrepancy	O
between	O
these	O
events	O
occuring	O
together	O
compared	O
to	O
what	O
would	O
be	O
expected	O
by	O
chance	O
clearly	O
the	O
mi	B
of	O
x	O
and	O
y	O
is	O
just	O
the	O
expected	B
value	I
of	O
the	O
pmi	O
interestingly	O
we	O
can	O
rewrite	O
the	O
pmi	O
as	O
follows	O
pxy	O
px	O
pyx	O
py	O
pmix	O
y	O
log	O
this	O
is	O
the	O
amount	O
we	O
learn	O
from	O
updating	O
the	O
prior	O
px	O
into	O
the	O
posterior	O
pxy	O
or	O
equivalently	O
updating	O
the	O
prior	O
py	O
into	O
the	O
posterior	O
pyx	O
log	O
mutual	B
information	B
for	O
continuous	O
random	O
variables	O
the	O
above	O
formula	O
for	O
mi	B
is	O
defined	O
for	O
discrete	B
random	O
variables	O
for	O
continuous	O
random	O
variables	O
it	O
is	O
common	O
to	O
first	O
discretize	B
or	O
quantize	B
them	O
by	O
dividing	O
the	O
ranges	O
of	O
each	O
variable	O
into	O
bins	O
and	O
computing	O
how	O
many	O
values	O
fall	O
in	O
each	O
histogram	B
bin	O
we	O
can	O
then	O
easily	O
compute	O
the	O
mi	B
using	O
the	O
formula	O
above	O
mutualinfoallpairsmixed	O
for	O
some	O
code	O
and	O
mimixeddemo	O
for	O
a	O
demo	O
unfortunately	O
the	O
number	O
of	O
bins	O
used	O
and	O
the	O
location	O
of	O
the	O
bin	O
boundaries	O
can	O
have	O
a	O
significant	O
effect	O
on	O
the	O
results	O
one	O
way	O
around	O
this	O
is	O
to	O
try	O
to	O
estimate	O
the	O
mi	B
directly	O
chapter	O
probability	O
figure	O
left	O
correlation	B
coefficient	I
vs	O
maximal	O
information	B
criterion	O
for	O
all	O
pairwise	O
relationships	O
in	O
the	O
who	O
data	O
right	O
scatter	O
plots	O
of	O
certain	O
pairs	O
of	O
variables	O
the	O
red	O
lines	O
are	O
non-parametric	O
smoothing	B
regressions	O
fit	O
separately	O
to	O
each	O
trend	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
david	O
reshef	O
and	O
the	O
american	O
association	O
for	O
the	O
advancement	O
of	O
science	O
without	O
first	O
performing	O
density	B
estimation	I
another	O
approach	O
is	O
to	O
try	O
many	O
different	O
bin	O
sizes	O
and	O
locations	O
and	O
to	O
compute	O
the	O
maximum	O
mi	B
achieved	O
this	O
statistic	O
appropriately	O
normalized	O
is	O
known	O
as	O
the	O
maximal	B
information	B
coefficient	I
et	O
al	O
more	O
precisely	O
define	O
maxg	O
gxy	O
i	O
y	O
mx	O
y	O
log	O
minx	O
y	O
where	O
gx	O
y	O
is	O
the	O
set	O
of	O
grids	O
of	O
size	O
x	O
y	O
and	O
xg	O
y	O
represents	O
a	O
discretization	O
of	O
the	O
variables	O
onto	O
this	O
grid	O
maximization	O
over	O
bin	O
locations	O
can	O
be	O
performed	O
efficiently	O
using	O
dynamic	B
programming	I
et	O
al	O
now	O
define	O
the	O
mic	O
as	O
mic	O
max	O
xyxyb	O
mx	O
y	O
where	O
b	O
is	O
some	O
sample-size	O
dependent	O
bound	O
on	O
the	O
number	O
of	O
bins	O
we	O
can	O
use	O
and	O
still	O
reliably	O
estimate	O
the	O
distribution	O
et	O
al	O
suggest	O
b	O
n	O
it	O
can	O
be	O
shown	O
that	O
the	O
mic	O
lies	O
in	O
the	O
range	O
where	O
represents	O
no	O
relationship	O
between	O
the	O
variables	O
and	O
represents	O
a	O
noise-free	O
relationship	O
of	O
any	O
form	O
not	O
just	O
linear	O
figure	O
gives	O
an	O
example	O
of	O
this	O
statistic	O
in	O
action	B
the	O
data	O
consists	O
of	O
variables	O
measuring	O
a	O
variety	O
of	O
social	O
economic	O
health	O
and	O
political	O
indicators	O
collected	O
by	O
the	O
world	B
health	I
organization	I
on	O
the	O
left	O
of	O
the	O
figure	O
we	O
see	O
the	O
correlation	B
coefficient	I
plotted	O
against	O
the	O
mic	O
for	O
all	O
variable	O
pairs	O
on	O
the	O
right	O
of	O
the	O
figure	O
we	O
see	O
scatter	O
plots	O
for	O
particular	O
pairs	O
of	O
variables	O
which	O
we	O
now	O
discuss	O
the	O
point	O
marked	O
c	O
has	O
a	O
low	O
cc	O
and	O
a	O
low	O
mic	O
the	O
corresponding	O
scatter	B
plot	I
makes	O
it	O
information	B
theory	I
clear	O
that	O
there	O
is	O
no	O
relationship	O
between	O
these	O
two	O
variables	O
of	O
lives	O
lost	O
to	O
injury	O
and	O
density	O
of	O
dentists	O
in	O
the	O
population	O
the	O
points	O
marked	O
d	O
and	O
h	O
have	O
high	O
cc	O
absolute	O
value	O
and	O
high	O
mic	O
because	O
they	O
represent	O
nearly	O
linear	O
relationships	O
the	O
points	O
marked	O
e	O
f	O
and	O
g	O
have	O
low	O
cc	O
but	O
high	O
mic	O
this	O
is	O
because	O
they	O
correspond	O
to	O
non-linear	O
sometimes	O
as	O
in	O
the	O
case	O
of	O
e	O
and	O
f	O
non-functional	O
i	O
e	O
one-to-many	O
relationships	O
between	O
the	O
variables	O
in	O
summary	O
we	O
see	O
that	O
statistics	O
as	O
mic	O
based	O
on	O
mutual	B
information	B
can	O
be	O
used	O
to	O
discover	O
interesting	O
relationships	O
between	O
variables	O
in	O
a	O
way	O
that	O
simpler	O
measures	O
such	O
as	O
correlation	O
coefficients	O
cannot	O
for	O
this	O
reason	O
the	O
mic	O
has	O
been	O
called	O
a	O
correlation	O
for	O
the	O
century	O
exercises	O
exercise	O
probabilities	O
are	O
sensitive	O
to	O
the	O
form	O
of	O
the	O
question	O
that	O
was	O
used	O
to	O
generate	O
the	O
answer	O
minka	O
my	O
neighbor	O
has	O
two	O
children	B
assuming	O
that	O
the	O
gender	O
of	O
a	O
child	O
is	O
like	O
a	O
coin	O
flip	O
it	O
is	O
most	O
likely	O
a	O
priori	O
that	O
my	O
neighbor	O
has	O
one	O
boy	O
and	O
one	O
girl	O
with	O
probability	O
the	O
other	O
possibilities	O
two	O
boys	O
or	O
two	O
girls	O
have	O
probabilities	O
and	O
a	O
suppose	O
i	O
ask	O
him	O
whether	O
he	O
has	O
any	O
boys	O
and	O
he	O
says	O
yes	O
what	O
is	O
the	O
probability	O
that	O
one	O
child	O
is	O
a	O
girl	O
b	O
suppose	O
instead	O
that	O
i	O
happen	O
to	O
see	O
one	O
of	O
his	O
children	B
run	O
by	O
and	O
it	O
is	O
a	O
boy	O
what	O
is	O
the	O
probability	O
that	O
the	O
other	O
child	O
is	O
a	O
girl	O
exercise	O
legal	O
reasoning	O
peter	O
lee	O
suppose	O
a	O
crime	O
has	O
been	O
committed	O
blood	O
is	O
found	O
at	O
the	O
scene	O
for	O
which	O
there	O
is	O
no	O
innocent	O
explanation	O
it	O
is	O
of	O
a	O
type	O
which	O
is	O
present	O
in	O
of	O
the	O
population	O
a	O
the	O
prosecutor	O
claims	O
there	O
is	O
a	O
chance	O
that	O
the	O
defendant	O
would	O
have	O
the	O
crime	O
blood	O
type	O
if	O
he	O
were	O
innocent	O
thus	O
there	O
is	O
a	O
chance	O
that	O
he	O
guilty	O
this	O
is	O
known	O
as	O
the	O
prosecutor	O
s	O
fallacy	O
what	O
is	O
wrong	O
with	O
this	O
argument	O
b	O
the	O
defender	O
claims	O
the	O
crime	O
occurred	O
in	O
a	O
city	O
of	O
people	O
the	O
blood	O
type	O
would	O
be	O
found	O
in	O
approximately	O
people	O
the	O
evidence	B
has	O
provided	O
a	O
probability	O
of	O
just	O
in	O
that	O
the	O
defendant	O
is	O
guilty	O
and	O
thus	O
has	O
no	O
relevance	O
this	O
is	O
known	O
as	O
the	O
defender	O
s	O
fallacy	O
what	O
is	O
wrong	O
with	O
this	O
argument	O
exercise	O
variance	B
of	O
a	O
sum	O
show	O
that	O
the	O
variance	B
of	O
a	O
sum	O
is	O
var	B
y	O
var	B
var	B
y	O
where	O
cov	O
y	O
is	O
the	O
covariance	B
between	O
x	O
and	O
y	O
exercise	O
bayes	B
rule	I
for	O
medical	O
diagnosis	O
koller	O
after	O
your	O
yearly	O
checkup	O
the	O
doctor	O
has	O
bad	O
news	O
and	O
good	O
news	O
the	O
bad	O
news	O
is	O
that	O
you	O
tested	O
positive	O
for	O
a	O
serious	O
disease	O
and	O
that	O
the	O
test	O
is	O
accurate	O
the	O
probability	O
of	O
testing	O
positive	O
given	O
that	O
you	O
have	O
the	O
disease	O
is	O
as	O
is	O
the	O
probability	O
of	O
tetsing	O
negative	O
given	O
that	O
you	O
don	O
t	O
have	O
the	O
disease	O
the	O
good	O
news	O
is	O
that	O
this	O
is	O
a	O
rare	O
disease	O
striking	O
only	O
one	O
in	O
people	O
what	O
are	O
the	O
chances	O
that	O
you	O
actually	O
have	O
the	O
disease	O
your	O
calculations	O
as	O
well	O
as	O
giving	O
the	O
final	O
result	O
chapter	O
probability	O
exercise	O
the	O
monty	O
hall	O
problem	O
mackay	O
on	O
a	O
game	O
show	O
a	O
contestant	O
is	O
told	O
the	O
rules	B
as	O
follows	O
there	O
are	O
three	O
doors	O
labelled	O
a	O
single	O
prize	O
has	O
been	O
hidden	B
behind	O
one	O
of	O
them	O
you	O
get	O
to	O
select	O
one	O
door	O
initially	O
your	O
chosen	O
door	O
will	O
not	O
be	O
opened	O
instead	O
the	O
gameshow	O
host	O
will	O
open	O
one	O
of	O
the	O
other	O
two	O
doors	O
and	O
he	O
will	O
do	O
so	O
in	O
such	O
a	O
way	O
as	O
not	O
to	O
reveal	O
the	O
prize	O
for	O
example	O
if	O
you	O
first	O
choose	O
door	O
he	O
will	O
then	O
open	O
one	O
of	O
doors	O
and	O
and	O
it	O
is	O
guaranteed	O
that	O
he	O
will	O
choose	O
which	O
one	O
to	O
open	O
so	O
that	O
the	O
prize	O
will	O
not	O
be	O
revealed	O
at	O
this	O
point	O
you	O
will	O
be	O
given	O
a	O
fresh	O
choice	O
of	O
door	O
you	O
can	O
either	O
stick	O
with	O
your	O
first	O
choice	O
or	O
you	O
can	O
switch	O
to	O
the	O
other	O
closed	O
door	O
all	O
the	O
doors	O
will	O
then	O
be	O
opened	O
and	O
you	O
will	O
receive	O
whatever	O
is	O
behind	O
your	O
final	O
choice	O
of	O
door	O
imagine	O
that	O
the	O
contestant	O
chooses	O
door	O
first	O
then	O
the	O
gameshow	O
host	O
opens	O
door	O
revealing	O
nothing	O
behind	O
the	O
door	O
as	O
promised	O
should	O
the	O
contestant	O
stick	O
with	O
door	O
or	O
switch	O
to	O
door	O
or	O
does	O
it	O
make	O
no	O
difference	O
you	O
may	O
assume	O
that	O
initially	O
the	O
prize	O
is	O
equally	O
likely	O
to	O
be	O
behind	O
any	O
of	O
the	O
doors	O
hint	O
use	O
bayes	B
rule	I
exercise	O
conditional	B
independence	I
koller	O
a	O
let	O
h	O
k	O
be	O
a	O
discrete	B
random	I
variable	I
and	O
let	O
and	O
be	O
the	O
observed	O
values	O
of	O
two	O
other	O
random	O
variables	O
and	O
suppose	O
we	O
wish	O
to	O
calculate	O
the	O
vector	O
p	O
which	O
of	O
the	O
following	O
sets	O
of	O
numbers	O
are	O
sufficient	O
for	O
the	O
calculation	O
i	O
p	O
p	O
p	O
p	O
ii	O
p	O
p	O
p	O
iii	O
p	O
p	O
p	O
b	O
now	O
suppose	O
we	O
now	O
assume	O
and	O
are	O
conditionally	B
independent	I
given	O
h	O
which	O
of	O
the	O
above	O
sets	O
are	O
sufficent	O
now	O
show	O
your	O
calculations	O
as	O
well	O
as	O
giving	O
the	O
final	O
result	O
hint	O
use	O
bayes	B
rule	I
exercise	O
pairwise	O
independence	O
does	O
not	O
imply	O
mutual	O
independence	O
we	O
say	O
that	O
two	O
random	O
variables	O
are	O
pairwise	B
independent	I
if	O
and	O
hence	O
we	O
say	O
that	O
n	O
random	O
variables	O
are	O
mutually	B
independent	I
if	O
pxixs	O
pxi	O
s	O
n	O
and	O
hence	O
pxi	O
show	O
that	O
pairwise	O
independence	O
between	O
all	B
pairs	I
of	O
variables	O
does	O
not	O
necessarily	O
imply	O
mutual	O
independence	O
it	O
suffices	O
to	O
give	O
a	O
counter	O
example	O
information	B
theory	I
exercise	O
conditional	B
independence	I
iff	B
joint	O
factorizes	O
in	O
the	O
text	O
we	O
said	O
x	O
y	O
iff	B
px	O
yz	O
pxzpyz	O
for	O
all	O
x	O
y	O
z	O
such	O
that	O
pz	O
now	O
prove	O
the	O
following	O
alternative	O
definition	O
x	O
y	O
iff	B
there	O
exist	O
function	O
g	O
and	O
h	O
such	O
that	O
px	O
yz	O
gx	O
zhy	O
z	O
for	O
all	O
x	O
y	O
z	O
such	O
that	O
pz	O
exercise	O
conditional	B
independence	I
koller	O
are	O
the	O
following	O
properties	O
true	O
prove	O
or	O
disprove	O
note	O
that	O
we	O
are	O
not	O
restricting	O
attention	O
to	O
distributions	O
that	O
can	O
be	O
represented	O
by	O
a	O
graphical	B
model	I
a	O
true	O
or	O
false	O
wz	O
y	O
y	O
y	O
wz	O
b	O
true	O
or	O
false	O
y	O
y	O
y	O
w	O
exercise	O
deriving	O
the	O
inverse	B
gamma	B
density	O
let	O
x	O
gaa	O
b	O
i	O
e	O
gaxa	O
b	O
ba	O
xa	O
xb	O
let	O
y	O
show	O
that	O
y	O
iga	O
b	O
i	O
e	O
igxshape	O
a	O
scale	O
b	O
ba	O
x	O
bx	O
hint	O
use	O
the	O
change	B
of	I
variables	I
formula	O
exercise	O
normalization	O
constant	O
for	O
a	O
gaussian	B
the	O
normalization	O
constant	O
for	O
a	O
zero-mean	O
gaussian	B
is	O
given	O
by	O
b	O
z	O
exp	O
a	O
dx	O
where	O
a	O
and	O
b	O
to	O
compute	O
this	O
consider	O
its	O
square	O
b	O
b	O
z	O
exp	O
a	O
a	O
dxdy	O
let	O
us	O
change	O
variables	O
from	O
cartesian	B
y	O
to	O
polar	B
using	O
x	O
r	O
cos	O
and	O
y	O
r	O
sin	O
since	O
dxdy	O
rdrd	O
and	O
we	O
have	O
z	O
r	O
exp	O
drd	O
evaluate	O
this	O
integral	O
and	O
hence	O
show	O
z	O
two	O
terms	O
the	O
first	O
of	O
which	O
d	O
dudr	O
re	O
hint	O
separate	O
the	O
integral	O
into	O
a	O
product	O
of	O
then	O
is	O
constant	O
so	O
is	O
easy	O
hint	O
if	O
u	O
e	O
so	O
the	O
second	O
integral	O
is	O
also	O
easy	O
u	O
ur	O
chapter	O
probability	O
exercise	O
expressing	O
mutual	B
information	B
in	O
terms	O
of	O
entropies	O
show	O
that	O
ix	O
y	O
hx	O
hxy	O
hy	O
hy	O
exercise	O
mutual	B
information	B
for	O
correlated	O
normals	O
and	O
thomas	O
find	O
the	O
mutual	B
information	B
where	O
x	O
has	O
a	O
bivariate	O
normal	B
distribution	O
n	O
evaluate	O
for	O
and	O
and	O
comment	O
hint	O
the	O
entropy	B
of	O
a	O
d-dimensional	O
gaussian	B
is	O
hx	O
ed	O
det	O
in	O
the	O
case	O
this	O
becomes	O
e	O
hx	O
hint	O
exercise	O
a	O
measure	O
of	O
correlation	O
mutual	B
information	B
and	O
thomas	O
let	O
x	O
and	O
y	O
be	O
discrete	B
random	O
variables	O
which	O
are	O
identically	O
distributed	O
hx	O
hy	O
but	O
not	O
necessarily	O
independent	O
define	O
r	O
hy	O
hx	O
hx	O
a	O
show	O
r	O
ixy	O
b	O
show	O
r	O
c	O
when	O
is	O
r	O
d	O
when	O
is	O
r	O
exercise	O
mle	B
minimizes	O
kl	B
divergence	I
to	O
the	O
empirical	B
distribution	I
let	O
pempx	O
be	O
the	O
empirical	B
distribution	I
and	O
let	O
qx	O
be	O
some	O
model	O
show	O
that	O
argminq	O
kl	O
is	O
obtained	O
by	O
qx	O
where	O
is	O
the	O
mle	B
hint	O
use	O
non-negativity	O
of	O
the	O
kl	B
divergence	I
exercise	O
mean	B
mode	B
variance	B
for	O
the	O
beta	B
distribution	I
suppose	O
betaa	O
b	O
derive	O
the	O
mean	B
mode	B
and	O
variance	B
exercise	O
expected	B
value	I
of	O
the	O
minimum	O
suppose	O
x	O
y	O
are	O
two	O
points	O
sampled	O
independently	O
and	O
uniformly	O
at	O
random	O
from	O
the	O
interval	O
what	O
is	O
the	O
expected	O
location	O
of	O
the	O
left	O
most	O
point	O
generative	O
models	O
for	O
discrete	B
data	O
introduction	O
in	O
section	O
we	O
discussed	O
how	O
to	O
classify	O
a	O
feature	O
vector	O
x	O
by	O
applying	O
bayes	B
rule	I
to	O
a	O
generative	O
classifier	O
of	O
the	O
form	O
py	O
cx	O
pxy	O
c	O
c	O
the	O
key	O
to	O
using	O
such	O
models	O
is	O
specifying	O
a	O
suitable	O
form	O
for	O
the	O
class-conditional	B
density	I
pxy	O
c	O
which	O
defines	O
what	O
kind	O
of	O
data	O
we	O
expect	O
to	O
see	O
in	O
each	O
class	O
in	O
this	O
chapter	O
we	O
focus	O
on	O
the	O
case	O
where	O
the	O
observed	O
data	O
are	O
discrete	B
symbols	O
we	O
also	O
discuss	O
how	O
to	O
infer	O
the	O
unknown	B
parameters	O
of	O
such	O
models	O
bayesian	B
concept	B
learning	B
consider	O
how	O
a	O
child	O
learns	O
to	O
understand	O
the	O
meaning	O
of	O
a	O
word	O
such	O
as	O
dog	O
presumably	O
the	O
child	O
s	O
parents	B
point	O
out	O
positive	B
examples	I
of	O
this	O
concept	B
saying	O
such	O
things	O
as	O
look	O
at	O
the	O
cute	O
dog	O
or	O
mind	O
the	O
doggy	O
etc	O
however	O
it	O
is	O
very	O
unlikely	O
that	O
they	O
provide	O
negative	B
examples	I
by	O
saying	O
look	O
at	O
that	O
non-dog	O
certainly	O
negative	B
examples	I
may	O
be	O
obtained	O
during	O
an	O
active	B
learning	B
process	O
the	O
child	O
says	O
look	O
at	O
the	O
dog	O
and	O
the	O
parent	O
says	O
that	O
s	O
a	O
cat	O
dear	O
not	O
a	O
dog	O
but	O
psychological	O
research	O
has	O
shown	O
that	O
people	O
can	O
learn	O
concepts	O
from	O
positive	B
examples	I
alone	O
and	O
tenenbaum	O
we	O
can	O
think	O
of	O
learning	B
the	O
meaning	O
of	O
a	O
word	O
as	O
equivalent	O
to	O
concept	B
learning	B
which	O
in	O
turn	O
is	O
equivalent	O
to	O
binary	O
classification	O
to	O
see	O
this	O
define	O
f	O
if	O
x	O
is	O
an	O
example	O
of	O
the	O
concept	B
c	O
and	O
f	O
otherwise	O
then	O
the	O
goal	O
is	O
to	O
learn	O
the	O
indicator	B
function	I
f	O
which	O
just	O
defines	O
which	O
elements	O
are	O
in	O
the	O
set	O
c	O
by	O
allowing	O
for	O
uncertainty	B
about	O
the	O
definition	O
of	O
f	O
or	O
equivalently	O
the	O
elements	O
of	O
c	O
we	O
can	O
emulate	O
fuzzy	B
set	I
theory	I
but	O
using	O
standard	O
probability	O
calculus	O
note	O
that	O
standard	O
binary	O
classification	O
techniques	O
require	O
positive	O
and	O
negative	B
examples	I
by	O
contrast	O
we	O
will	O
devise	O
a	O
way	O
to	O
learn	O
from	O
positive	B
examples	I
alone	O
for	O
pedagogical	O
purposes	O
we	O
will	O
consider	O
a	O
very	O
simple	O
example	O
of	O
concept	B
learning	B
called	O
the	O
number	B
game	I
based	O
on	O
part	O
of	O
josh	O
tenenbaum	O
s	O
phd	O
thesis	O
the	O
game	O
proceeds	O
as	O
follows	O
i	O
choose	O
some	O
simple	O
arithmetical	O
concept	B
c	O
such	O
as	O
prime	O
number	O
or	O
a	O
number	O
between	O
and	O
i	O
then	O
give	O
you	O
a	O
series	O
of	O
randomly	O
chosen	O
positive	B
examples	I
d	O
xn	O
drawn	O
from	O
c	O
and	O
ask	O
you	O
whether	O
some	O
new	O
test	O
case	O
x	O
belongs	O
to	O
c	O
i	O
e	O
i	O
ask	O
you	O
to	O
classify	O
x	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
examples	O
figure	O
empirical	O
predictive	B
distribution	O
averaged	O
over	O
humans	O
in	O
the	O
number	B
game	I
first	O
two	O
rows	O
after	O
seeing	O
d	O
and	O
d	O
this	O
illustrates	O
diffuse	O
similarity	O
third	O
row	O
after	O
seeing	O
d	O
this	O
illustrates	O
rule-like	O
behavior	O
of	O
bottom	O
row	O
after	O
seeing	O
d	O
this	O
illustrates	O
focussed	O
similarity	O
near	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
josh	O
tenenbaum	O
suppose	O
for	O
simplicity	O
that	O
all	O
numbers	O
are	O
integers	O
between	O
and	O
now	O
suppose	O
i	O
tell	O
you	O
is	O
a	O
positive	O
example	O
of	O
the	O
concept	B
what	O
other	O
numbers	O
do	O
you	O
think	O
are	O
positive	O
it	O
s	O
hard	O
to	O
tell	O
with	O
only	O
one	O
example	O
so	O
your	O
predictions	O
will	O
be	O
quite	O
vague	O
presumably	O
numbers	O
that	O
are	O
similar	B
in	O
some	O
sense	O
to	O
are	O
more	O
likely	O
but	O
similar	B
in	O
what	O
way	O
is	O
similar	B
because	O
it	O
is	O
close	O
by	O
is	O
similar	B
because	O
it	O
has	O
a	O
digit	O
in	O
common	O
is	O
similar	B
because	O
it	O
is	O
also	O
even	O
and	O
a	O
power	O
of	O
but	O
does	O
not	O
seem	O
similar	B
thus	O
some	O
numbers	O
are	O
more	O
likely	O
than	O
others	O
we	O
can	O
represent	O
this	O
as	O
a	O
probability	O
distribution	O
p	O
xd	O
which	O
is	O
the	O
probability	O
that	O
x	O
c	O
given	O
the	O
data	O
d	O
for	O
any	O
x	O
this	O
is	O
called	O
the	O
posterior	B
predictive	B
distribution	I
figure	O
shows	O
the	O
predictive	B
distribution	O
of	O
people	O
derived	O
from	O
a	O
lab	O
experiment	O
we	O
see	O
that	O
people	O
predict	O
numbers	O
that	O
are	O
similar	B
to	O
under	O
a	O
variety	O
of	O
kinds	O
of	O
similarity	O
now	O
suppose	O
i	O
tell	O
you	O
that	O
and	O
are	O
also	O
positive	B
examples	I
now	O
you	O
may	O
guess	O
that	O
the	O
hidden	B
concept	B
is	O
powers	O
of	O
two	O
this	O
is	O
an	O
example	O
of	O
induction	B
given	O
this	O
hypothesis	O
the	O
predictive	B
distribution	O
is	O
quite	O
specific	O
and	O
puts	O
most	O
of	O
its	O
mass	O
on	O
powers	O
of	O
as	O
shown	O
if	O
instead	O
i	O
tell	O
you	O
the	O
data	O
is	O
d	O
you	O
will	O
get	O
a	O
in	O
figure	O
row	O
different	O
kind	O
of	O
generalization	B
gradient	I
as	O
shown	O
in	O
figure	O
how	O
can	O
we	O
explain	O
this	O
behavior	O
and	O
emulate	O
it	O
in	O
a	O
machine	O
the	O
classic	O
approach	O
to	O
induction	B
is	O
to	O
suppose	O
we	O
have	O
a	O
hypothesis	B
space	I
of	O
concepts	O
h	O
such	O
as	O
odd	O
numbers	O
even	O
numbers	O
all	O
numbers	O
between	O
and	O
powers	O
of	O
two	O
all	O
numbers	O
ending	O
in	O
j	O
bayesian	B
concept	B
learning	B
j	O
etc	O
the	O
subset	O
of	O
h	O
that	O
is	O
consistent	B
with	O
the	O
data	O
d	O
is	O
called	O
the	O
version	B
space	I
as	O
we	O
see	O
more	O
examples	O
the	O
version	B
space	I
shrinks	O
and	O
we	O
become	O
increasingly	O
certain	O
about	O
the	O
concept	B
however	O
the	O
version	B
space	I
is	O
not	O
the	O
whole	O
story	O
after	O
seeing	O
d	O
there	O
are	O
many	O
consistent	B
rules	B
how	O
do	O
you	O
combine	O
them	O
to	O
predict	O
if	O
x	O
c	O
also	O
after	O
seeing	O
d	O
why	O
did	O
you	O
choose	O
the	O
rule	O
powers	O
of	O
two	O
and	O
not	O
say	O
all	O
even	O
numbers	O
or	O
powers	O
of	O
two	O
except	O
for	O
both	O
of	O
which	O
are	O
equally	O
consistent	B
with	O
the	O
evidence	B
we	O
will	O
now	O
provide	O
a	O
bayesian	B
explanation	O
for	O
this	O
likelihood	B
we	O
must	O
explain	O
why	O
we	O
chose	O
htwo	O
powers	O
of	O
two	O
and	O
not	O
say	O
heven	O
even	O
numbers	O
after	O
seeing	O
d	O
given	O
that	O
both	O
hypotheses	O
are	O
consistent	B
with	O
the	O
evidence	B
the	O
key	O
intuition	O
is	O
that	O
we	O
want	O
to	O
avoid	O
suspicious	B
coincidences	I
if	O
the	O
true	O
concept	B
was	O
even	O
numbers	O
how	O
come	O
we	O
only	O
saw	O
numbers	O
that	O
happened	O
to	O
be	O
powers	O
of	O
two	O
to	O
formalize	O
this	O
let	O
us	O
assume	O
that	O
examples	O
are	O
sampled	O
uniformly	O
at	O
random	O
from	O
the	O
extension	B
of	O
a	O
concept	B
extension	B
of	O
a	O
concept	B
is	O
just	O
the	O
set	O
of	O
numbers	O
that	O
belong	O
to	O
it	O
e	O
g	O
the	O
extension	B
of	O
heven	O
is	O
the	O
extension	B
of	O
numbers	O
ending	O
in	O
is	O
tenenbaum	O
calls	O
this	O
the	O
strong	B
sampling	I
assumption	I
given	O
this	O
assumption	O
the	O
probability	O
of	O
independently	O
sampling	O
n	O
items	O
replacement	O
from	O
h	O
is	O
given	O
by	O
pdh	O
sizeh	O
this	O
crucial	O
equation	O
embodies	O
what	O
tenenbaum	O
calls	O
the	O
size	B
principle	I
which	O
means	O
the	O
model	O
favors	O
the	O
simplest	O
hypothesis	O
consistent	B
with	O
the	O
data	O
this	O
is	O
more	O
commonly	O
known	O
as	O
occam	O
s	O
to	O
see	O
how	O
it	O
works	O
let	O
d	O
then	O
pdhtwo	O
since	O
there	O
are	O
only	O
powers	O
of	O
two	O
less	O
than	O
but	O
pdheven	O
since	O
there	O
are	O
even	O
numbers	O
so	O
the	O
likelihood	B
that	O
h	O
htwo	O
is	O
higher	O
than	O
if	O
h	O
heven	O
after	O
examples	O
the	O
likelihood	B
of	O
htwo	O
whereas	O
the	O
likelihood	B
of	O
heven	O
is	O
is	O
this	O
is	O
a	O
likelihood	B
ratio	I
of	O
almost	O
in	O
favor	O
of	O
htwo	O
this	O
quantifies	O
our	O
earlier	O
intuition	O
that	O
d	O
would	O
be	O
a	O
very	O
suspicious	B
coincidence	I
if	O
generated	O
by	O
heven	O
prior	O
suppose	O
d	O
given	O
this	O
data	O
the	O
concept	B
more	O
likely	O
than	O
h	O
powers	O
of	O
two	O
since	O
is	O
missing	B
from	O
the	O
set	O
of	O
examples	O
powers	O
of	O
two	O
except	O
is	O
does	O
not	O
need	O
to	O
explain	O
the	O
coincidence	O
that	O
however	O
the	O
hypothesis	O
powers	O
of	O
two	O
except	O
seems	O
conceptually	O
unnatural	O
we	O
can	O
capture	O
such	O
intution	O
by	O
assigning	O
low	O
prior	O
probability	O
to	O
unnatural	O
concepts	O
of	O
course	O
your	O
prior	O
might	O
be	O
different	O
than	O
mine	O
this	O
subjective	B
aspect	O
of	O
bayesian	B
reasoning	O
is	O
a	O
source	O
of	O
much	O
controversy	O
since	O
it	O
means	O
for	O
example	O
that	O
a	O
child	O
and	O
a	O
math	O
professor	O
william	O
of	O
occam	O
spelt	O
ockham	O
was	O
an	O
english	O
monk	O
and	O
philosopher	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
will	O
reach	O
different	O
answers	O
in	O
fact	O
they	O
presumably	O
not	O
only	O
have	O
different	O
priors	O
but	O
also	O
different	O
hypothesis	O
spaces	O
however	O
we	O
can	O
finesse	O
that	O
by	O
defining	O
the	O
hypothesis	B
space	I
of	O
the	O
child	O
and	O
the	O
math	O
professor	O
to	O
be	O
the	O
same	O
and	O
then	O
setting	O
the	O
child	O
s	O
prior	O
weight	O
to	O
be	O
zero	O
on	O
certain	O
advanced	O
concepts	O
thus	O
there	O
is	O
no	O
sharp	O
distinction	O
between	O
the	O
prior	O
and	O
the	O
hypothesis	B
space	I
although	O
the	O
subjectivity	O
of	O
the	O
prior	O
is	O
controversial	O
it	O
is	O
actually	O
quite	O
useful	O
if	O
you	O
are	O
told	O
the	O
numbers	O
are	O
from	O
some	O
arithmetic	O
rule	O
then	O
given	O
and	O
you	O
may	O
think	O
is	O
likely	O
but	O
is	O
unlikely	O
but	O
if	O
you	O
are	O
told	O
that	O
the	O
numbers	O
are	O
examples	O
of	O
healthy	O
cholesterol	O
levels	O
you	O
would	O
probably	O
think	O
is	O
unlikely	O
and	O
is	O
likely	O
thus	O
we	O
see	O
that	O
the	O
prior	O
is	O
the	O
mechanism	O
by	O
which	O
background	B
knowledge	I
can	O
be	O
brought	O
to	O
bear	O
on	O
a	O
problem	O
without	O
this	O
rapid	O
learning	B
from	O
small	O
samples	B
sizes	O
is	O
impossible	O
so	O
what	O
prior	O
should	O
we	O
use	O
for	O
illustration	O
purposes	O
let	O
us	O
use	O
a	O
simple	O
prior	O
which	O
puts	O
uniform	O
probability	O
on	O
simple	O
arithmetical	O
concepts	O
such	O
as	O
even	O
numbers	O
odd	O
numbers	O
prime	O
numbers	O
numbers	O
ending	O
in	O
etc	O
to	O
make	O
things	O
more	O
interesting	O
we	O
make	O
the	O
concepts	O
even	O
and	O
odd	O
more	O
likely	O
apriori	O
we	O
also	O
include	O
two	O
unnatural	O
concepts	O
namely	O
powers	O
of	O
plus	O
and	O
powers	O
of	O
except	O
but	O
give	O
them	O
low	O
prior	O
weight	O
see	O
figure	O
for	O
a	O
plot	O
of	O
this	O
prior	O
we	O
will	O
consider	O
a	O
slightly	O
more	O
sophisticated	O
prior	O
later	O
on	O
posterior	O
the	O
posterior	O
is	O
simply	O
the	O
likelihood	B
times	O
the	O
prior	O
normalized	O
in	O
this	O
context	O
we	O
have	O
phd	O
pdhph	O
h	O
pd	O
phid	O
hhn	O
h	O
where	O
id	O
h	O
is	O
iff	B
and	O
only	O
if	O
all	O
the	O
data	O
are	O
in	O
the	O
extension	B
of	O
the	O
hypothesis	O
h	O
figure	O
plots	O
the	O
prior	O
likelihood	B
and	O
posterior	O
after	O
seeing	O
d	O
we	O
see	O
that	O
the	O
posterior	O
is	O
a	O
combination	O
of	O
prior	O
and	O
likelihood	B
in	O
the	O
case	O
of	O
most	O
of	O
the	O
concepts	O
the	O
prior	O
is	O
uniform	O
so	O
the	O
posterior	O
is	O
proportional	O
to	O
the	O
likelihood	B
however	O
the	O
unnatural	O
concepts	O
of	O
powers	O
of	O
plus	O
and	O
powers	O
of	O
except	O
have	O
low	O
posterior	O
support	B
despite	O
having	O
high	O
likelihood	B
due	O
to	O
the	O
low	O
prior	O
conversely	O
the	O
concept	B
of	O
odd	O
numbers	O
has	O
low	O
posterior	O
support	B
despite	O
having	O
a	O
high	O
prior	O
due	O
to	O
the	O
low	O
likelihood	B
figure	O
plots	O
the	O
prior	O
likelihood	B
and	O
posterior	O
after	O
seeing	O
d	O
now	O
the	O
likelihood	B
is	O
much	O
more	O
peaked	O
on	O
the	O
powers	O
of	O
two	O
concept	B
so	O
this	O
dominates	B
the	O
posterior	O
essentially	O
the	O
learner	O
has	O
an	O
aha	B
moment	O
and	O
figures	O
out	O
the	O
true	O
concept	B
we	O
see	O
the	O
need	O
for	O
the	O
low	O
prior	O
on	O
the	O
unnatural	O
concepts	O
otherwise	O
we	O
would	O
have	O
overfit	O
the	O
data	O
and	O
picked	O
powers	O
of	O
except	O
for	O
in	O
general	O
when	O
we	O
have	O
enough	O
data	O
the	O
posterior	O
phd	O
becomes	O
peaked	O
on	O
a	O
single	O
concept	B
namely	O
the	O
map	B
estimate	I
i	O
e	O
phd	O
hm	O
ap	O
where	O
hm	O
ap	O
argmaxh	O
phd	O
is	O
the	O
posterior	B
mode	B
and	O
where	O
is	O
the	O
dirac	B
measure	I
defined	O
by	O
xa	O
if	O
x	O
a	O
if	O
x	O
a	O
bayesian	B
concept	B
learning	B
data	O
even	O
odd	O
squares	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
all	O
powers	O
of	O
powers	O
of	O
prior	O
lik	O
post	O
figure	O
prior	O
likelihood	B
and	O
posterior	O
for	O
d	O
based	O
on	O
figure	O
generated	O
by	O
numbersgame	O
note	O
that	O
the	O
map	B
estimate	I
can	O
be	O
written	O
as	O
pdhph	O
argmax	O
hm	O
ap	O
argmax	O
h	O
h	O
pdh	O
log	O
ph	O
since	O
the	O
likelihood	B
term	O
depends	O
exponentially	O
on	O
n	O
and	O
the	O
prior	O
stays	O
constant	O
as	O
we	O
get	O
more	O
and	O
more	O
data	O
the	O
map	B
estimate	I
converges	O
towards	O
the	O
maximum	B
likelihood	B
estimate	I
or	O
mle	B
hmle	O
argmax	O
h	O
pdh	O
argmax	O
log	O
pdh	O
h	O
in	O
other	O
words	O
if	O
we	O
have	O
enough	O
data	O
we	O
see	O
that	O
the	O
data	B
overwhelms	I
the	I
prior	I
in	O
this	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
data	O
even	O
odd	O
squares	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
mult	O
of	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
ends	O
in	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
powers	O
of	O
all	O
powers	O
of	O
powers	O
of	O
prior	O
lik	O
x	O
post	O
figure	O
prior	O
likelihood	B
and	O
posterior	O
for	O
d	O
based	O
on	O
figure	O
generated	O
by	O
numbersgame	O
case	O
the	O
map	B
estimate	I
converges	O
towards	O
the	O
mle	B
if	O
the	O
true	O
hypothesis	O
is	O
in	O
the	O
hypothesis	B
space	I
then	O
the	O
map	O
ml	O
estimate	O
will	O
converge	B
upon	O
this	O
hypothesis	O
thus	O
we	O
say	O
that	O
bayesian	B
inference	B
ml	O
estimation	O
are	O
consistent	B
estimators	I
section	O
for	O
details	O
we	O
also	O
say	O
that	O
the	O
hypothesis	B
space	I
is	O
identifiable	O
in	O
the	O
limit	O
meaning	O
we	O
can	O
recover	O
the	O
truth	O
in	O
the	O
limit	O
of	O
infinite	O
data	O
if	O
our	O
hypothesis	O
class	O
is	O
not	O
rich	O
enough	O
to	O
represent	O
the	O
truth	O
will	O
usually	O
be	O
the	O
case	O
we	O
will	O
converge	B
on	O
the	O
hypothesis	O
that	O
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
truth	O
however	O
formalizing	O
this	O
notion	O
of	O
closeness	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
bayesian	B
concept	B
learning	B
powers	O
of	O
powers	O
of	O
ends	O
in	O
squares	O
even	O
mult	O
of	O
mult	O
of	O
all	O
powers	O
of	O
powers	O
of	O
ph	O
figure	O
posterior	O
over	O
hypotheses	O
and	O
the	O
corresponding	O
predictive	B
distribution	O
after	O
seeing	O
one	O
example	O
d	O
a	O
dot	O
means	O
this	O
number	O
is	O
consistent	B
with	O
this	O
hypothesis	O
the	O
graph	B
phd	O
on	O
the	O
right	O
is	O
the	O
weight	O
given	O
to	O
hypothesis	O
h	O
by	O
taking	O
a	O
weighed	O
sum	O
of	O
dots	O
we	O
get	O
p	O
x	O
cd	B
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
numbersgame	O
posterior	B
predictive	B
distribution	I
the	O
posterior	O
is	O
our	O
internal	O
belief	B
state	B
about	O
the	O
world	O
the	O
way	O
to	O
test	O
if	O
our	O
beliefs	O
are	O
justified	O
is	O
to	O
use	O
them	O
to	O
predict	O
objectively	O
observable	O
quantities	O
is	O
the	O
basis	O
of	O
the	O
scientific	O
method	O
specifically	O
the	O
posterior	B
predictive	B
distribution	I
in	O
this	O
context	O
is	O
given	O
by	O
p	O
x	O
cd	B
py	O
x	O
hphd	O
h	O
this	O
is	O
just	O
a	O
weighted	B
average	I
of	O
the	O
predictions	O
of	O
each	O
individual	O
hypothesis	O
and	O
is	O
called	O
bayes	B
model	I
averaging	I
et	O
al	O
this	O
is	O
illustrated	O
in	O
figure	O
the	O
dots	O
at	O
the	O
bottom	O
show	O
the	O
predictions	O
from	O
each	O
hypothesis	O
the	O
vertical	O
curve	O
on	O
the	O
right	O
shows	O
the	O
weight	O
associated	O
with	O
each	O
hypothesis	O
if	O
we	O
multiply	O
each	O
row	O
by	O
its	O
weight	O
and	O
add	O
up	O
we	O
get	O
the	O
distribution	O
at	O
the	O
top	O
when	O
we	O
have	O
a	O
small	O
andor	O
ambiguous	O
dataset	O
the	O
posterior	O
phd	O
is	O
vague	O
which	O
induces	O
a	O
broad	O
predictive	B
distribution	O
however	O
once	O
we	O
have	O
figured	O
things	O
out	O
the	O
posterior	O
becomes	O
a	O
delta	O
function	O
centered	O
at	O
the	O
map	B
estimate	I
in	O
this	O
case	O
the	O
predictive	B
distribution	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
becomes	O
p	O
x	O
cd	B
h	O
p	O
xh	O
hh	O
p	O
x	O
h	O
this	O
is	O
called	O
a	O
plug-in	B
approximation	I
to	O
the	O
predictive	B
density	O
and	O
is	O
very	O
widely	O
used	O
due	O
to	O
its	O
simplicity	O
however	O
in	O
general	O
this	O
under-represents	O
our	O
uncertainty	B
and	O
our	O
predictions	O
will	O
not	O
be	O
as	O
smooth	O
as	O
when	O
using	O
bma	O
we	O
will	O
see	O
more	O
examples	O
of	O
this	O
later	O
in	O
the	O
book	O
although	O
map	O
learning	B
is	O
simple	O
it	O
cannot	O
explain	O
the	O
gradual	O
shift	O
from	O
similarity-based	O
for	O
reasoning	O
uncertain	O
posteriors	O
to	O
rule-based	O
reasoning	O
certain	O
posteriors	O
example	O
suppose	O
we	O
observe	O
d	O
if	O
we	O
use	O
the	O
simple	O
prior	O
above	O
the	O
minimal	B
consistent	B
hypothesis	O
is	O
all	O
powers	O
of	O
so	O
only	O
and	O
get	O
a	O
non-zero	O
probability	O
of	O
being	O
predicted	O
this	O
is	O
of	O
course	O
an	O
example	O
of	O
overfitting	O
given	O
d	O
the	O
map	O
hypothesis	O
is	O
all	O
powers	O
of	O
two	O
thus	O
the	O
plug-in	B
predictive	B
distribution	O
gets	O
broader	O
stays	O
the	O
same	O
as	O
we	O
see	O
more	O
data	O
it	O
starts	O
narrow	O
but	O
is	O
forced	O
to	O
broaden	O
as	O
it	O
seems	O
more	O
data	O
in	O
contrast	O
in	O
the	O
bayesian	B
approach	O
we	O
start	O
broad	O
and	O
then	O
narrow	O
down	O
as	O
we	O
learn	O
more	O
in	O
particular	O
given	O
d	O
there	O
are	O
many	O
hypotheses	O
which	O
makes	O
more	O
intuitive	O
sense	O
with	O
non-negligible	O
posterior	O
support	B
so	O
the	O
predictive	B
distribution	O
is	O
broad	O
however	O
when	O
we	O
see	O
d	O
the	O
posterior	O
concentrates	O
its	O
mass	O
on	O
one	O
hypothesis	O
so	O
the	O
predictive	B
distribution	O
becomes	O
narrower	O
so	O
the	O
predictions	O
made	O
by	O
a	O
plug-in	B
approach	O
and	O
a	O
bayesian	B
approach	O
are	O
quite	O
different	O
in	O
the	O
small	O
sample	O
regime	O
although	O
they	O
converge	B
to	O
the	O
same	O
answer	O
as	O
we	O
see	O
more	O
data	O
a	O
more	O
complex	O
prior	O
to	O
model	O
human	O
behavior	O
tenenbaum	O
used	O
a	O
slightly	O
more	O
sophisticated	O
prior	O
which	O
was	O
derived	O
by	O
analysing	O
some	O
experimental	O
data	O
of	O
how	O
people	O
measure	O
similarity	O
between	O
numbers	O
see	O
for	O
details	O
the	O
result	O
is	O
a	O
set	O
of	O
arithmetical	O
concepts	O
similar	B
to	O
those	O
mentioned	O
above	O
plus	O
all	O
intervals	O
between	O
n	O
and	O
m	O
for	O
n	O
m	O
that	O
these	O
hypotheses	O
are	O
not	O
mutually	O
exclusive	O
thus	O
the	O
prior	O
is	O
a	O
mixture	B
of	O
two	O
priors	O
one	O
over	O
arithmetical	O
rules	B
and	O
one	O
over	O
intervals	O
intervalh	O
rulesh	O
ph	O
the	O
only	O
free	O
parameter	B
in	O
the	O
model	O
is	O
the	O
relative	O
weight	O
given	O
to	O
these	O
two	O
parts	O
of	O
the	O
prior	O
the	O
results	O
are	O
not	O
very	O
sensitive	O
to	O
this	O
value	O
so	O
long	O
as	O
reflecting	O
the	O
fact	O
that	O
people	O
are	O
more	O
likely	O
to	O
think	O
of	O
concepts	O
defined	O
by	O
rules	B
the	O
predictive	B
distribution	O
of	O
the	O
model	O
using	O
this	O
larger	O
hypothesis	B
space	I
is	O
shown	O
in	O
figure	O
it	O
is	O
strikingly	O
similar	B
to	O
the	O
human	O
predictive	B
distribution	O
shown	O
in	O
figure	O
even	O
though	O
it	O
was	O
not	O
fit	O
to	O
human	O
data	O
the	O
choice	O
of	O
hypothesis	B
space	I
the	O
beta-binomial	B
model	O
the	O
number	B
game	I
involved	O
inferring	O
a	O
distribution	O
over	O
a	O
discrete	B
variable	O
drawn	O
from	O
a	O
finite	O
hypothesis	B
space	I
h	O
h	O
given	O
a	O
series	O
of	O
discrete	B
observations	O
this	O
made	O
the	O
computations	O
particularly	O
simple	O
we	O
just	O
needed	O
to	O
sum	O
multiply	O
and	O
divide	O
however	O
in	O
many	O
applications	O
k	O
where	O
the	O
unknown	B
parameters	O
are	O
continuous	O
so	O
the	O
hypothesis	B
space	I
is	O
subset	O
of	O
r	O
the	O
beta-binomial	B
model	O
examples	O
figure	O
predictive	B
distributions	O
for	O
the	O
model	O
using	O
the	O
full	B
hypothesis	B
space	I
compare	O
to	O
figure	O
the	O
predictions	O
of	O
the	O
bayesian	B
model	O
are	O
only	O
plotted	O
for	O
those	O
values	O
of	O
x	O
for	O
which	O
human	O
data	O
is	O
available	O
this	O
is	O
why	O
the	O
top	O
line	O
looks	O
sparser	O
than	O
figure	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
josh	O
tenenbaum	O
k	O
is	O
the	O
number	O
of	O
parameters	O
this	O
complicates	O
the	O
mathematics	O
since	O
we	O
have	O
to	O
replace	O
sums	O
with	O
integrals	O
however	O
the	O
basic	O
ideas	O
are	O
the	O
same	O
we	O
will	O
illustrate	O
this	O
by	O
considering	O
the	O
problem	O
of	O
inferring	O
the	O
probability	O
that	O
a	O
coin	O
shows	O
up	O
heads	O
given	O
a	O
series	O
of	O
observed	O
coin	O
tosses	O
although	O
this	O
might	O
seem	O
trivial	O
it	O
turns	O
out	O
that	O
this	O
model	O
forms	O
the	O
basis	O
of	O
many	O
of	O
the	O
methods	O
we	O
will	O
consider	O
later	O
in	O
this	O
book	O
including	O
naive	O
bayes	O
classifiers	O
markov	B
models	I
etc	O
it	O
is	O
historically	O
important	O
since	O
it	O
was	O
the	O
example	O
which	O
was	O
analyzed	O
in	O
bayes	O
original	O
paper	O
of	O
analysis	O
was	O
subsequently	O
generalized	O
by	O
pierre-simon	O
laplace	B
creating	O
what	O
we	O
now	O
call	O
bayes	B
rule	I
see	O
for	O
further	O
historical	O
details	O
we	O
will	O
follow	O
our	O
now-familiar	O
recipe	O
of	O
specifying	O
the	O
likelihood	B
and	O
prior	O
and	O
deriving	O
the	O
posterior	O
and	O
posterior	O
predictive	B
likelihood	B
suppose	O
xi	O
ber	O
where	O
xi	O
represents	O
heads	O
xi	O
represents	O
tails	O
and	O
is	O
the	O
rate	B
parameter	B
of	O
heads	O
if	O
the	O
data	O
are	O
iid	B
the	O
likelihood	B
has	O
the	O
form	O
pd	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
ixi	O
heads	O
and	O
where	O
we	O
haven	O
ixi	O
tails	O
these	O
two	O
counts	O
are	O
called	O
the	O
sufficient	B
statistics	I
of	O
the	O
data	O
since	O
this	O
is	O
all	O
we	O
need	O
to	O
know	O
about	O
d	O
to	O
infer	O
alternative	O
set	O
of	O
sufficient	B
statistics	I
are	O
and	O
n	O
more	O
formally	O
we	O
say	O
sd	O
is	O
a	O
sufficient	O
statistic	O
for	O
data	O
d	O
if	O
p	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
this	O
is	O
equivalent	O
to	O
saying	O
pd	O
psd	O
consequently	O
if	O
we	O
have	O
two	O
datasets	O
with	O
the	O
same	O
sufficient	B
statistics	I
we	O
will	O
infer	O
the	O
same	O
value	O
for	O
now	O
suppose	O
the	O
data	O
consists	O
of	O
the	O
count	O
of	O
the	O
number	O
of	O
heads	O
observed	O
in	O
a	O
fixed	O
number	O
n	O
of	O
trials	O
in	O
this	O
case	O
we	O
have	O
binn	O
where	O
bin	O
represents	O
the	O
binomial	B
distribution	I
which	O
has	O
the	O
following	O
pmf	B
binkn	O
n	O
k	O
n	O
k	O
k	O
is	O
a	O
constant	O
independent	O
of	O
the	O
likelihood	B
for	O
the	O
binomial	B
sampling	O
model	O
is	O
the	O
since	O
same	O
as	O
the	O
likelihood	B
for	O
the	O
bernoulli	B
model	O
so	O
any	O
inferences	O
we	O
make	O
about	O
will	O
be	O
the	O
same	O
whether	O
we	O
observe	O
the	O
counts	O
d	O
n	O
or	O
a	O
sequence	O
of	O
trials	O
d	O
xn	O
prior	O
we	O
need	O
a	O
prior	O
which	O
has	O
support	B
over	O
the	O
interval	O
to	O
make	O
the	O
math	O
easier	O
it	O
would	O
convenient	O
if	O
the	O
prior	O
had	O
the	O
same	O
form	O
as	O
the	O
likelihood	B
i	O
e	O
if	O
the	O
prior	O
looked	O
like	O
p	O
for	O
some	O
prior	O
parameters	O
and	O
posterior	O
by	O
simply	O
adding	O
up	O
the	O
exponents	O
if	O
this	O
were	O
the	O
case	O
then	O
we	O
could	O
easily	O
evaluate	O
the	O
p	O
pd	O
when	O
the	O
prior	O
and	O
the	O
posterior	O
have	O
the	O
same	O
form	O
we	O
say	O
that	O
the	O
prior	O
is	O
a	O
conjugate	B
prior	I
for	O
the	O
corresponding	O
likelihood	B
conjugate	B
priors	I
are	O
widely	O
used	O
because	O
they	O
simplify	O
computation	O
and	O
are	O
easy	O
to	O
interpret	O
as	O
we	O
see	O
below	O
in	O
the	O
case	O
of	O
the	O
bernoulli	B
the	O
conjugate	B
prior	I
is	O
the	O
beta	B
distribution	I
which	O
we	O
encountered	O
in	O
section	O
beta	O
b	O
a	O
the	O
parameters	O
of	O
the	O
prior	O
are	O
called	O
hyper-parameters	B
we	O
can	O
set	O
them	O
in	O
order	O
to	O
encode	O
our	O
prior	O
beliefs	O
for	O
example	O
to	O
encode	O
our	O
beliefs	O
that	O
has	O
mean	B
and	O
standard	B
deviation	I
we	O
set	O
a	O
and	O
b	O
or	O
to	O
encode	O
our	O
beliefs	O
that	O
has	O
mean	B
and	O
that	O
we	O
think	O
it	O
lives	O
in	O
the	O
interval	O
with	O
probability	O
then	O
we	O
find	O
a	O
and	O
b	O
if	O
we	O
know	O
nothing	O
about	O
except	O
that	O
it	O
lies	O
in	O
the	O
interval	O
we	O
can	O
use	O
a	O
uniform	O
prior	O
which	O
is	O
a	O
kind	O
of	O
uninformative	B
prior	O
section	O
for	O
details	O
the	O
uniform	B
distribution	I
can	O
be	O
represented	O
by	O
a	O
beta	B
distribution	I
with	O
a	O
b	O
the	O
beta-binomial	B
model	O
prior	O
lik	O
post	O
prior	O
lik	O
post	O
figure	O
updating	O
a	O
prior	O
with	O
a	O
binomial	B
likelihood	B
with	O
sufficient	B
statistics	I
to	O
yield	O
a	O
posterior	O
likelihood	B
with	O
sufficient	B
statistics	I
to	O
yield	O
a	O
posterior	O
figure	O
generated	O
by	O
binomialbetaposteriordemo	O
updating	O
a	O
prior	O
with	O
a	O
binomial	B
posterior	O
if	O
we	O
multiply	O
the	O
likelihood	B
by	O
the	O
beta	O
prior	O
we	O
get	O
the	O
following	O
posterior	O
equation	O
p	O
bbeta	O
a	O
b	O
in	O
particular	O
the	O
posterior	O
is	O
obtained	O
by	O
adding	O
the	O
prior	O
hyper-parameters	B
to	O
the	O
empirical	O
counts	O
for	O
this	O
reason	O
the	O
hyper-parameters	B
are	O
known	O
as	O
pseudo	B
counts	I
the	O
strength	O
of	O
the	O
prior	O
also	O
known	O
as	O
the	O
effective	B
sample	I
size	I
of	O
the	O
prior	O
is	O
the	O
sum	O
of	O
the	O
pseudo	B
counts	I
a	O
b	O
this	O
plays	O
a	O
role	O
analogous	O
to	O
the	O
data	O
set	O
size	O
n	O
figure	O
gives	O
an	O
example	O
where	O
we	O
update	O
a	O
weak	O
prior	O
with	O
a	O
peaked	O
likelihood	B
function	O
corresponding	O
to	O
a	O
large	O
sample	O
size	O
we	O
see	O
that	O
the	O
posterior	O
is	O
essentially	O
identical	O
to	O
the	O
likelihood	B
since	O
the	O
data	O
has	O
overwhelmed	O
the	O
prior	O
figure	O
gives	O
an	O
example	O
where	O
we	O
update	O
a	O
strong	O
prior	O
with	O
a	O
peaked	O
likelihood	B
function	O
now	O
we	O
see	O
that	O
the	O
posterior	O
is	O
a	O
compromise	O
between	O
the	O
prior	O
and	O
likelihood	B
to	O
see	O
this	O
suppose	O
we	O
have	O
two	O
data	O
sets	O
da	O
and	O
db	O
with	O
sufficient	B
statistics	I
n	O
a	O
n	O
b	O
n	O
b	O
datasets	O
in	O
batch	B
mode	B
we	O
have	O
note	O
that	O
updating	O
the	O
posterior	O
sequentially	O
is	O
equivalent	O
to	O
updating	O
in	O
a	O
single	O
batch	B
and	O
be	O
the	O
sufficient	B
statistics	I
of	O
the	O
combined	O
and	O
n	O
a	O
let	O
n	O
a	O
n	O
b	O
n	O
b	O
n	O
a	O
p	O
b	O
beta	O
a	O
b	O
in	O
sequential	B
mode	B
we	O
have	O
p	O
pdb	O
binn	O
b	O
n	O
b	O
beta	O
n	O
a	O
n	O
b	O
n	O
b	O
a	O
a	O
n	O
a	O
n	O
b	O
a	O
n	O
a	O
b	O
b	O
this	O
makes	O
bayesian	B
inference	B
particularly	O
well-suited	O
to	O
online	B
learning	B
as	O
we	O
will	O
see	O
later	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
posterior	B
mean	B
and	O
mode	B
from	O
equation	O
the	O
map	B
estimate	I
is	O
given	O
by	O
m	O
ap	O
a	O
a	O
b	O
n	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
then	O
the	O
map	B
estimate	I
reduces	O
to	O
the	O
mle	B
which	O
is	O
just	O
the	O
empirical	O
fraction	O
of	O
heads	O
n	O
m	O
le	O
this	O
makes	O
intuitive	O
sense	O
but	O
it	O
can	O
also	O
be	O
derived	O
by	O
applying	O
elementary	O
calculus	O
to	O
maximize	O
the	O
likelihood	B
function	O
in	O
equation	O
by	O
contrast	O
the	O
posterior	B
mean	B
is	O
given	O
by	O
a	O
a	O
b	O
n	O
this	O
difference	O
between	O
the	O
mode	B
and	O
the	O
mean	B
will	O
prove	O
important	O
later	O
we	O
will	O
now	O
show	O
that	O
the	O
posterior	B
mean	B
is	O
convex	B
combination	I
of	O
the	O
prior	O
mean	B
and	O
the	O
mle	B
which	O
captures	O
the	O
notion	O
that	O
the	O
posterior	O
is	O
a	O
compromise	O
between	O
what	O
we	O
previously	O
believed	O
and	O
what	O
the	O
data	O
is	O
telling	O
us	O
let	O
a	O
b	O
be	O
the	O
equivalent	B
sample	I
size	I
of	O
the	O
prior	O
which	O
controls	O
its	O
strength	O
and	O
let	O
the	O
prior	O
mean	B
be	O
a	O
then	O
the	O
posterior	B
mean	B
is	O
given	O
by	O
n	O
n	O
e	O
where	O
n	O
is	O
the	O
ratio	O
of	O
the	O
prior	O
to	O
posterior	O
equivalent	B
sample	I
size	I
so	O
the	O
weaker	O
the	O
prior	O
the	O
smaller	O
is	O
and	O
hence	O
the	O
closer	O
the	O
posterior	B
mean	B
is	O
to	O
the	O
mle	B
one	O
can	O
show	O
similarly	O
that	O
the	O
posterior	B
mode	B
is	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mode	B
and	O
the	O
mle	B
and	O
that	O
it	O
too	O
converges	O
to	O
the	O
mle	B
m	O
le	O
n	O
n	O
n	O
posterior	O
variance	B
the	O
mean	B
and	O
mode	B
are	O
point	O
estimates	O
but	O
it	O
is	O
useful	O
to	O
know	O
how	O
much	O
we	O
can	O
trust	O
them	O
the	O
variance	B
of	O
the	O
posterior	O
is	O
one	O
way	O
to	O
measure	O
this	O
the	O
variance	B
of	O
the	O
beta	O
posterior	O
is	O
given	O
by	O
var	B
b	O
b	O
we	O
can	O
simplify	O
this	O
formidable	O
expression	O
in	O
the	O
case	O
that	O
n	O
a	O
b	O
to	O
get	O
var	B
n	O
n	O
n	O
n	O
var	B
where	O
is	O
the	O
mle	B
hence	O
the	O
error	B
bar	I
in	O
our	O
estimate	O
the	O
posterior	O
standard	B
deviation	I
is	O
given	O
by	O
n	O
the	O
beta-binomial	B
model	O
we	O
see	O
that	O
the	O
uncertainty	B
goes	O
down	O
at	O
a	O
rate	B
of	O
n	O
note	O
however	O
that	O
the	O
uncertainty	B
is	O
maximized	O
when	O
and	O
is	O
minimized	O
when	O
is	O
close	O
to	O
or	O
this	O
means	O
it	O
is	O
easier	O
to	O
be	O
sure	O
that	O
a	O
coin	O
is	O
biased	O
than	O
to	O
be	O
sure	O
that	O
it	O
is	O
fair	O
posterior	B
predictive	B
distribution	I
so	O
far	O
we	O
have	O
been	O
focusing	O
on	O
inference	B
of	O
the	O
unknown	B
parameters	O
let	O
us	O
now	O
turn	O
our	O
attention	O
to	O
prediction	O
of	O
future	O
observable	O
data	O
consider	O
predicting	O
the	O
probability	O
of	O
heads	O
in	O
a	O
single	O
future	O
trial	O
under	O
a	O
betaa	O
b	O
poste	O
rior	O
we	O
have	O
p	O
x	O
px	O
beta	O
bd	O
e	O
a	O
a	O
b	O
thus	O
we	O
see	O
that	O
the	O
mean	B
of	O
the	O
posterior	B
predictive	B
distribution	I
is	O
equivalent	O
this	O
case	O
to	O
plugging	O
in	O
the	O
posterior	B
mean	B
parameters	O
p	O
xd	O
ber	O
xe	O
overfitting	O
and	O
the	O
black	B
swan	I
paradox	I
suppose	O
instead	O
that	O
we	O
plug-in	B
the	O
mle	B
i	O
e	O
we	O
use	O
p	O
xd	O
ber	O
x	O
m	O
le	O
unfortunately	O
this	O
approximation	O
can	O
perform	O
quite	O
poorly	O
when	O
the	O
sample	O
size	O
is	O
small	O
for	O
example	O
suppose	O
we	O
have	O
seen	O
n	O
tails	O
in	O
a	O
row	O
the	O
mle	B
is	O
since	O
this	O
makes	O
the	O
observed	O
data	O
as	O
probable	O
as	O
possible	O
however	O
using	O
this	O
estimate	O
we	O
predict	O
that	O
heads	O
are	O
impossible	O
this	O
is	O
called	O
the	O
zero	B
count	I
problem	I
or	O
the	O
sparse	B
data	I
problem	I
and	O
frequently	O
occurs	O
when	O
estimating	O
counts	O
from	O
small	O
amounts	O
of	O
data	O
one	O
might	O
think	O
that	O
in	O
the	O
era	O
of	O
big	B
data	I
such	O
concerns	O
are	O
irrelevant	O
but	O
note	O
that	O
once	O
we	O
partition	O
the	O
data	O
based	O
on	O
certain	O
criteria	O
such	O
as	O
the	O
number	O
of	O
times	O
a	O
specific	O
person	O
has	O
engaged	O
in	O
a	O
specific	O
activity	O
the	O
sample	O
sizes	O
can	O
become	O
much	O
smaller	O
this	O
problem	O
arises	O
for	O
example	O
when	O
trying	O
to	O
perform	O
personalized	B
recommendation	I
of	O
web	O
pages	O
thus	O
bayesian	B
methods	O
are	O
still	O
useful	O
even	O
in	O
the	O
big	B
data	I
regime	O
the	O
zero-count	O
problem	O
is	O
analogous	O
to	O
a	O
problem	O
in	O
philosophy	O
called	O
the	O
black	B
swan	I
paradox	I
this	O
is	O
based	O
on	O
the	O
ancient	O
western	O
conception	O
that	O
all	O
swans	O
were	O
white	O
in	O
that	O
context	O
a	O
black	O
swan	O
was	O
a	O
metaphor	O
for	O
something	O
that	O
could	O
not	O
exist	O
swans	O
were	O
discovered	O
in	O
australia	O
by	O
european	O
explorers	O
in	O
the	O
century	O
the	O
term	O
black	B
swan	I
paradox	I
was	O
first	O
coined	O
by	O
the	O
famous	O
philosopher	O
of	O
science	O
karl	B
popper	I
the	O
term	O
has	O
also	O
been	O
used	O
as	O
the	O
title	O
of	O
a	O
recent	O
popular	O
book	O
this	O
paradox	O
was	O
used	O
to	O
illustrate	O
the	O
problem	O
of	O
induction	B
which	O
is	O
the	O
problem	O
of	O
how	O
to	O
draw	O
general	O
conclusions	O
about	O
the	O
future	O
from	O
specific	O
observations	O
from	O
the	O
past	O
let	O
us	O
now	O
derive	O
a	O
simple	O
bayesian	B
solution	O
to	O
the	O
problem	O
we	O
will	O
use	O
a	O
uniform	O
prior	O
so	O
a	O
b	O
in	O
this	O
case	O
plugging	O
in	O
the	O
posterior	B
mean	B
gives	O
laplace	B
s	O
rule	O
of	O
succession	O
p	O
x	O
this	O
justifies	O
the	O
common	O
practice	O
of	O
adding	O
to	O
the	O
empirical	O
counts	O
normalizing	O
and	O
then	O
plugging	O
them	O
in	O
a	O
technique	O
known	O
as	O
add-one	B
smoothing	B
that	O
plugging	O
in	O
the	O
map	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
parameters	O
would	O
not	O
have	O
this	O
smoothing	B
effect	O
since	O
the	O
mode	B
has	O
the	O
form	O
n	O
which	O
becomes	O
the	O
mle	B
if	O
a	O
b	O
predicting	O
the	O
outcome	O
of	O
multiple	O
future	O
trials	O
suppose	O
now	O
we	O
were	O
interested	O
in	O
predicting	O
the	O
number	O
of	O
heads	O
x	O
in	O
m	O
future	O
trials	O
this	O
is	O
given	O
by	O
m	O
x	O
pxd	O
m	O
binx	O
m	O
bd	O
x	O
a	O
we	O
recognize	O
the	O
integral	O
as	O
the	O
normalization	O
constant	O
for	O
a	O
betaax	O
m	O
xb	O
distribution	O
ba	O
b	O
x	O
a	O
bx	O
a	O
m	O
x	O
b	O
thus	O
we	O
find	O
that	O
the	O
posterior	O
predictive	B
is	O
given	O
by	O
the	O
following	O
known	O
as	O
the	O
beta-binomial	B
distribution	O
bbxa	O
b	O
m	O
m	O
x	O
bx	O
a	O
m	O
x	O
b	O
ba	O
b	O
this	O
distribution	O
has	O
the	O
following	O
mean	B
and	O
variance	B
a	O
m	O
ab	O
b	O
m	O
e	O
var	B
a	O
b	O
if	O
m	O
and	O
hence	O
x	O
we	O
see	O
that	O
the	O
mean	B
becomes	O
e	O
px	O
a	O
ab	O
which	O
is	O
consistent	B
with	O
equation	O
a	O
b	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
we	O
start	O
with	O
a	O
prior	O
and	O
plot	O
the	O
posterior	B
predictive	B
density	I
after	O
seeing	O
heads	O
and	O
tails	O
figure	O
plots	O
a	O
plug-in	B
approximation	I
using	O
a	O
map	B
estimate	I
we	O
see	O
that	O
the	O
bayesian	B
prediction	O
has	O
longer	O
tails	O
spreading	O
its	O
probablity	O
mass	O
more	O
widely	O
and	O
is	O
therefore	O
less	O
prone	O
to	O
overfitting	O
and	O
blackswan	O
type	O
paradoxes	O
the	O
dirichlet-multinomial	O
model	O
in	O
the	O
previous	O
section	O
we	O
discussed	O
how	O
to	O
infer	O
the	O
probability	O
that	O
a	O
coin	O
comes	O
up	O
heads	O
in	O
this	O
section	O
we	O
generalize	B
these	O
results	O
to	O
infer	O
the	O
probability	O
that	O
a	O
dice	O
with	O
k	O
sides	O
comes	O
up	O
as	O
face	O
k	O
this	O
might	O
seem	O
like	O
another	O
toy	O
exercise	O
but	O
the	O
methods	O
we	O
will	O
study	O
are	O
widely	O
used	O
to	O
analyse	O
text	O
data	O
biosequence	O
data	O
etc	O
as	O
we	O
will	O
see	O
later	O
the	O
dirichlet-multinomial	O
model	O
posterior	O
predictive	B
plugin	O
predictive	B
figure	O
figure	O
generated	O
by	O
betabinompostpreddemo	O
posterior	O
predictive	B
distributions	O
after	O
seeing	O
plugin	O
approximation	O
likelihood	B
suppose	O
we	O
observe	O
n	O
dice	O
rolls	O
d	O
xn	O
where	O
xi	O
k	O
the	O
data	O
is	O
iid	B
the	O
likelihood	B
has	O
the	O
form	O
if	O
we	O
assume	O
pd	O
nk	O
k	O
iyi	O
k	O
is	O
the	O
number	O
of	O
times	O
event	O
k	O
occured	O
are	O
the	O
sufficient	O
where	O
nk	O
statistics	O
for	O
this	O
model	O
the	O
likelihood	B
for	O
the	O
multinomial	B
model	O
has	O
the	O
same	O
form	O
up	O
to	O
an	O
irrelevant	O
constant	O
factor	B
prior	O
since	O
the	O
parameter	B
vector	O
lives	O
in	O
the	O
k-dimensional	O
probability	B
simplex	I
we	O
need	O
a	O
prior	O
that	O
has	O
support	B
over	O
this	O
simplex	O
ideally	O
it	O
would	O
also	O
be	O
conjugate	O
fortunately	O
the	O
dirichlet	B
distribution	I
satisfies	O
both	O
criteria	O
so	O
we	O
will	O
use	O
the	O
following	O
prior	O
dir	O
b	O
posterior	O
k	O
k	O
ix	O
sk	O
multiplying	O
the	O
likelihood	B
by	O
the	O
prior	O
we	O
find	O
that	O
the	O
posterior	O
is	O
also	O
dirichlet	B
p	O
pd	O
k	O
k	O
nk	O
knk	O
dir	O
k	O
nk	O
k	O
k	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
we	O
see	O
that	O
the	O
posterior	O
is	O
obtained	O
by	O
adding	O
the	O
prior	O
hyper-parameters	B
k	O
to	O
the	O
empirical	O
counts	O
nk	O
we	O
can	O
derive	O
the	O
mode	B
of	O
this	O
posterior	O
the	O
map	B
estimate	I
by	O
using	O
calculus	O
however	O
k	O
k	O
we	O
can	O
do	O
this	O
by	O
using	O
a	O
lagrange	O
we	O
must	O
enforce	O
the	O
constraint	O
that	O
multiplier	O
the	O
constrained	O
objective	O
function	O
or	O
lagrangian	B
is	O
given	O
by	O
the	O
log	O
likelihood	B
plus	O
log	O
prior	O
plus	O
the	O
constraint	O
nk	O
log	O
k	O
k	O
log	O
k	O
k	O
k	O
k	O
k	O
k	O
nk	O
k	O
taking	O
derivatives	O
with	O
respect	O
to	O
yields	O
to	O
simplify	O
notation	O
we	O
define	O
the	O
original	O
constraint	O
k	O
k	O
taking	O
derivatives	O
with	O
respect	O
to	O
k	O
yields	O
k	O
k	O
k	O
k	O
k	O
k	O
n	O
k	O
k	O
where	O
given	O
by	O
k	O
nk	O
k	O
n	O
k	O
k	O
k	O
we	O
can	O
solve	O
for	O
using	O
the	O
sum-to-one	O
constraint	O
k	O
is	O
the	O
equivalent	B
sample	I
size	I
of	O
the	O
prior	O
thus	O
the	O
map	B
estimate	I
is	O
which	O
is	O
consistent	B
with	O
equation	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
k	O
we	O
recover	O
the	O
mle	B
k	O
nkn	O
this	O
is	O
just	O
the	O
empirical	O
fraction	O
of	O
times	O
face	O
k	O
shows	O
up	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
enforce	O
the	O
constraint	O
that	O
k	O
since	O
the	O
gradient	O
of	O
the	O
objective	O
has	O
the	O
form	O
nk	O
k	O
so	O
negative	O
values	O
would	O
reduce	O
the	O
objective	O
rather	O
than	O
maximize	O
it	O
course	O
this	O
does	O
not	O
preclude	O
setting	O
k	O
and	O
indeed	O
this	O
is	O
the	O
optimal	O
solution	O
if	O
nk	O
and	O
k	O
the	O
dirichlet-multinomial	O
model	O
posterior	O
predictive	B
the	O
posterior	B
predictive	B
distribution	I
for	O
a	O
single	O
multinoulli	O
trial	O
expression	O
is	O
given	O
by	O
the	O
following	O
px	O
jd	O
px	O
j	O
px	O
j	O
j	O
jp	O
jdd	O
j	O
e	O
jd	O
p	O
j	O
jdd	O
j	O
d	O
j	O
j	O
nj	O
k	O
k	O
nk	O
j	O
nj	O
n	O
where	O
j	O
are	O
all	O
the	O
components	O
of	O
except	O
j	O
see	O
also	O
exercise	O
the	O
above	O
expression	O
avoids	O
the	O
zero-count	O
problem	O
just	O
as	O
we	O
saw	O
in	O
section	O
in	O
fact	O
this	O
form	O
of	O
bayesian	B
smoothing	B
is	O
even	O
more	O
important	O
in	O
the	O
multinomial	B
case	O
than	O
the	O
binary	O
case	O
since	O
the	O
likelihood	B
of	O
data	O
sparsity	B
increases	O
once	O
we	O
start	O
partitioning	B
the	O
data	O
into	O
many	O
categories	O
worked	O
example	O
language	B
models	I
using	O
bag	B
of	I
words	I
one	O
application	O
of	O
bayesian	B
smoothing	B
using	O
the	O
dirichlet-multinomial	O
model	O
is	O
to	O
language	B
modeling	I
which	O
means	O
predicting	O
which	O
words	O
might	O
occur	O
next	O
in	O
a	O
sequence	O
here	O
we	O
will	O
take	O
a	O
very	O
simple-minded	O
approach	O
and	O
assume	O
that	O
the	O
i	O
th	O
word	O
xi	O
k	O
is	O
sampled	O
independently	O
from	O
all	O
the	O
other	O
words	O
using	O
a	O
cat	O
distribution	O
this	O
is	O
called	O
the	O
bag	B
of	I
words	I
model	O
given	O
a	O
past	O
sequence	O
of	O
words	O
how	O
can	O
we	O
predict	O
which	O
one	O
is	O
likely	O
to	O
come	O
next	O
for	O
example	O
suppose	O
we	O
observe	O
the	O
following	O
sequence	O
of	O
a	O
children	B
s	O
nursery	O
rhyme	O
mary	O
had	O
a	O
little	O
lamb	O
little	O
lamb	O
little	O
lamb	O
mary	O
had	O
a	O
little	O
lamb	O
its	O
fleece	O
as	O
white	O
as	O
snow	O
furthermore	O
suppose	O
our	O
vocabulary	O
consists	O
of	O
the	O
following	O
words	O
mary	O
lamb	O
little	O
big	O
fleece	O
white	O
black	O
snow	O
rain	O
unk	B
here	O
unk	B
stands	O
for	O
unknown	B
and	O
represents	O
all	O
other	O
words	O
that	O
do	O
not	O
appear	O
elsewhere	O
on	O
the	O
list	O
to	O
encode	O
each	O
line	O
of	O
the	O
nursery	O
rhyme	O
we	O
first	O
strip	O
off	O
punctuation	O
and	O
remove	O
any	O
stop	B
words	I
such	O
as	O
a	O
as	O
the	O
etc	O
we	O
can	O
also	O
perform	O
stemming	B
which	O
means	O
reducing	O
words	O
to	O
their	O
base	O
form	O
such	O
as	O
stripping	O
off	O
the	O
final	O
s	O
in	O
plural	O
words	O
or	O
the	O
ing	O
from	O
verbs	O
running	O
becomes	O
run	O
in	O
this	O
example	O
no	O
words	O
need	O
stemming	B
finally	O
we	O
replace	O
each	O
word	O
by	O
its	O
index	O
into	O
the	O
vocabulary	O
to	O
get	O
we	O
now	O
ignore	O
the	O
word	O
order	O
and	O
count	O
how	O
often	O
each	O
word	O
occurred	O
resulting	O
in	O
a	O
histogram	B
of	O
word	O
counts	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
token	O
word	O
mary	O
count	O
lamb	O
little	O
big	O
fleece	O
white	O
black	O
snow	O
rain	O
unk	B
just	O
denote	O
the	O
above	O
counts	O
by	O
nj	O
p	O
x	O
jd	O
e	O
jd	O
if	O
we	O
use	O
a	O
dir	O
prior	O
for	O
the	O
posterior	O
predictive	B
is	O
j	O
nj	O
nj	O
if	O
we	O
set	O
j	O
we	O
get	O
p	O
x	O
jd	O
the	O
modes	O
of	O
the	O
predictive	B
distribution	O
are	O
x	O
lamb	O
and	O
x	O
unk	B
note	O
that	O
the	O
words	O
big	O
black	O
and	O
rain	O
are	O
predicted	O
to	O
occur	O
with	O
non-zero	O
probability	O
in	O
the	O
future	O
even	O
though	O
they	O
have	O
never	O
been	O
seen	O
before	O
later	O
on	O
we	O
will	O
see	O
more	O
sophisticated	O
language	B
models	I
naive	O
bayes	O
classifiers	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
classify	O
vectors	O
of	O
discrete-valued	O
features	B
x	O
kd	O
where	O
k	O
is	O
the	O
number	O
of	O
values	O
for	O
each	O
feature	O
and	O
d	O
is	O
the	O
number	O
of	O
features	B
we	O
will	O
use	O
a	O
generative	B
approach	I
this	O
requires	O
us	O
to	O
specify	O
the	O
class	O
conditional	O
distribution	O
pxy	O
c	O
the	O
simplest	O
approach	O
is	O
to	O
assume	O
the	O
features	B
are	O
conditionally	B
independent	I
given	O
the	O
class	O
label	B
this	O
allows	O
us	O
to	O
write	O
the	O
class	O
conditional	O
density	O
as	O
a	O
product	O
of	O
one	O
dimensional	O
densities	O
pxy	O
c	O
pxjy	O
c	O
jc	O
the	O
resulting	O
model	O
is	O
called	O
a	O
naive	O
bayes	O
classifier	O
the	O
model	O
is	O
called	O
naive	O
since	O
we	O
do	O
not	O
expect	O
the	O
features	B
to	O
be	O
independent	O
even	O
conditional	O
on	O
the	O
class	O
label	B
however	O
even	O
if	O
the	O
naive	O
bayes	O
assumption	O
is	O
not	O
true	O
it	O
often	O
results	O
in	O
classifiers	O
that	O
work	O
well	O
and	O
pazzani	O
one	O
reason	O
for	O
this	O
is	O
that	O
the	O
model	O
is	O
quite	O
simple	O
only	O
has	O
ocd	O
parameters	O
for	O
c	O
classes	O
and	O
d	O
features	B
and	O
hence	O
it	O
is	O
relatively	O
immune	O
to	O
overfitting	O
the	O
form	O
of	O
the	O
class-conditional	B
density	I
depends	O
on	O
the	O
type	O
of	O
each	O
feature	O
we	O
give	O
some	O
possibilities	O
below	O
jc	O
where	O
jc	O
is	O
the	O
mean	B
of	O
feature	O
j	O
in	O
objects	O
of	O
class	O
c	O
and	O
in	O
the	O
case	O
of	O
real-valued	O
features	B
we	O
can	O
use	O
the	O
gaussian	B
distribution	O
pxy	O
c	O
n	O
jc	O
jc	O
is	O
its	O
variance	B
in	O
the	O
case	O
of	O
binary	O
features	B
xj	O
we	O
can	O
use	O
the	O
bernoulli	B
distribution	O
pxy	O
berxj	O
jc	O
where	O
jc	O
is	O
the	O
probability	O
that	O
feature	O
j	O
occurs	O
in	O
class	O
c	O
c	O
this	O
is	O
sometimes	O
called	O
the	O
multivariate	B
bernoulli	B
naive	I
bayes	I
model	O
we	O
will	O
see	O
an	O
application	O
of	O
this	O
below	O
naive	O
bayes	O
classifiers	O
in	O
the	O
case	O
of	O
categorical	B
features	B
xj	O
k	O
we	O
can	O
model	O
use	O
the	O
multinoulli	B
distribution	I
pxy	O
c	O
catxj	O
jc	O
where	O
jc	O
is	O
a	O
histogram	B
over	O
the	O
k	O
possible	O
values	O
for	O
xj	O
in	O
class	O
c	O
obviously	O
we	O
can	O
handle	O
other	O
kinds	O
of	O
features	B
or	O
use	O
different	O
distributional	O
assumptions	O
also	O
it	O
is	O
easy	O
to	O
mix	O
and	O
match	O
features	B
of	O
different	O
types	O
model	O
fitting	O
we	O
now	O
discuss	O
how	O
to	O
train	O
a	O
naive	O
bayes	O
classifier	O
this	O
usually	O
means	O
computing	O
the	O
mle	B
or	O
the	O
map	B
estimate	I
for	O
the	O
parameters	O
however	O
we	O
will	O
also	O
discuss	O
how	O
to	O
compute	O
the	O
full	B
posterior	O
p	O
mle	B
for	O
nbc	O
the	O
probability	O
for	O
a	O
single	O
data	O
case	O
is	O
given	O
by	O
pxi	O
yi	O
pyi	O
pxij	O
j	O
iyic	O
c	O
pxij	O
jciyic	O
j	O
c	O
j	O
c	O
hence	O
the	O
log-likelihood	O
is	O
given	O
by	O
log	O
pd	O
nc	O
log	O
c	O
log	O
pxij	O
jc	O
iyic	O
we	O
see	O
that	O
this	O
expression	O
decomposes	B
into	O
a	O
series	O
of	O
terms	O
one	O
concerning	O
and	O
dc	O
terms	O
containing	O
the	O
jc	O
s	O
hence	O
we	O
can	O
optimize	O
all	O
these	O
parameters	O
separately	O
from	O
equation	O
the	O
mle	B
for	O
the	O
class	O
prior	O
is	O
given	O
by	O
c	O
nc	O
n	O
i	O
iyi	O
c	O
is	O
the	O
number	O
of	O
examples	O
in	O
class	O
c	O
where	O
nc	O
the	O
mle	B
for	O
the	O
likelihood	B
depends	O
on	O
the	O
type	O
of	O
distribution	O
we	O
choose	O
to	O
use	O
for	O
each	O
feature	O
for	O
simplicity	O
let	O
us	O
suppose	O
all	O
features	B
are	O
binary	O
so	O
xjy	O
c	O
ber	O
jc	O
in	O
this	O
case	O
the	O
mle	B
becomes	O
jc	O
njc	O
nc	O
it	O
is	O
extremely	O
simple	O
to	O
implement	O
this	O
model	O
fitting	O
procedure	O
see	O
algorithm	O
for	O
some	O
pseudo-code	O
naivebayesfit	O
for	O
some	O
matlab	O
code	O
this	O
algorithm	O
obviously	O
takes	O
on	O
d	O
time	O
the	O
method	O
is	O
easily	O
generalized	O
to	O
handle	O
features	B
of	O
mixed	O
type	O
this	O
simplicity	O
is	O
one	O
reason	O
the	O
method	O
is	O
so	O
widely	O
used	O
figure	O
gives	O
an	O
example	O
where	O
we	O
have	O
classes	O
and	O
binary	O
features	B
representing	O
the	O
presence	O
or	O
absence	O
of	O
words	O
in	O
a	O
bag-of-words	B
model	O
the	O
plot	O
visualizes	O
the	O
c	O
vectors	O
for	O
the	O
two	O
classes	O
the	O
big	O
spike	O
at	O
index	O
corresponds	O
to	O
the	O
word	O
subject	O
which	O
occurs	O
in	O
both	O
classes	O
with	O
probability	O
section	O
we	O
discuss	O
how	O
to	O
filter	O
out	O
such	O
uninformative	B
features	B
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
algorithm	O
fitting	O
a	O
naive	O
bayes	O
classifier	O
to	O
binary	O
features	B
nc	O
njc	O
for	O
i	O
do	O
c	O
yi	O
class	O
label	B
of	O
i	O
th	O
example	O
nc	O
nc	O
for	O
j	O
do	O
if	O
xij	O
then	O
njc	O
njc	O
c	O
nc	O
n	O
jc	O
njc	O
n	O
figure	O
class	O
conditional	O
densities	O
pxj	O
c	O
for	O
two	O
document	O
classes	O
corresponding	O
to	O
x	O
windows	O
and	O
ms	O
windows	O
figure	O
generated	O
by	O
naivebayesbowdemo	O
bayesian	B
naive	O
bayes	O
the	O
trouble	O
with	O
maximum	O
likelihood	B
is	O
that	O
it	O
can	O
overfit	O
for	O
example	O
consider	O
the	O
example	O
in	O
figure	O
the	O
feature	O
corresponding	O
to	O
the	O
word	O
subject	O
it	O
feature	O
j	O
always	O
occurs	O
in	O
both	O
classes	O
so	O
we	O
estimate	O
jc	O
what	O
will	O
happen	O
if	O
we	O
encounter	O
a	O
new	O
email	O
which	O
does	O
not	O
have	O
this	O
word	O
in	O
it	O
our	O
algorithm	O
will	O
crash	O
and	O
burn	O
since	O
we	O
will	O
find	O
that	O
py	O
cx	O
for	O
both	O
classes	O
this	O
is	O
another	O
manifestation	O
of	O
the	O
black	B
swan	I
paradox	I
discussed	O
in	O
section	O
a	O
simple	O
solution	O
to	O
overfitting	O
is	O
to	O
be	O
bayesian	B
for	O
simplicity	O
we	O
will	O
use	O
a	O
factored	O
prior	O
p	O
p	O
jc	O
we	O
will	O
use	O
a	O
dir	O
prior	O
for	O
and	O
a	O
beta	O
prior	O
for	O
each	O
jc	O
often	O
we	O
just	O
take	O
and	O
corresponding	O
to	O
add-one	O
or	O
laplace	B
smoothing	B
naive	O
bayes	O
classifiers	O
combining	O
the	O
factored	O
likelihood	B
in	O
equation	O
with	O
the	O
factored	O
prior	O
above	O
gives	O
the	O
following	O
factored	O
posterior	O
p	O
p	O
nc	O
c	O
p	O
jcd	O
betanc	O
njc	O
njc	O
p	O
jcd	O
c	O
in	O
other	O
words	O
to	O
compute	O
the	O
posterior	O
we	O
just	O
update	O
the	O
prior	O
counts	O
with	O
the	O
empirical	O
counts	O
from	O
the	O
likelihood	B
it	O
is	O
straightforward	O
to	O
modify	O
algorithm	O
to	O
handle	O
this	O
version	O
of	O
model	O
fitting	O
using	O
the	O
model	O
for	O
prediction	O
at	O
test	O
time	O
the	O
goal	O
is	O
to	O
compute	O
py	O
cxd	O
py	O
cd	B
pxjy	O
cd	B
the	O
correct	O
bayesian	B
procedure	O
is	O
to	O
integrate	B
out	I
the	O
unknown	B
parameters	O
py	O
cxd	O
caty	O
c	O
berxjy	O
c	O
jcp	O
jcd	O
in	O
particular	O
from	O
equafortunately	O
this	O
is	O
easy	O
to	O
do	O
at	O
least	O
if	O
the	O
posterior	O
is	O
dirichlet	B
tion	O
we	O
know	O
the	O
posterior	B
predictive	B
density	I
can	O
be	O
obtained	O
by	O
simply	O
plugging	O
in	O
the	O
posterior	B
mean	B
parameters	O
hence	O
py	O
cxd	O
c	O
jcixj	O
jcixj	O
njc	O
nc	O
nc	O
c	O
n	O
jk	O
c	O
c	O
c	O
where	O
if	O
we	O
have	O
approximated	O
the	O
posterior	O
by	O
a	O
single	O
point	O
p	O
where	O
may	O
be	O
the	O
ml	O
or	O
map	B
estimate	I
then	O
the	O
posterior	B
predictive	B
density	I
is	O
obtained	O
by	O
simply	O
plugging	O
in	O
the	O
parameters	O
to	O
yield	O
a	O
virtually	O
identical	O
rule	O
py	O
cxd	O
c	O
jcixj	O
jcixj	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
the	O
only	O
difference	O
is	O
we	O
replaced	O
the	O
posterior	B
mean	B
with	O
the	O
posterior	B
mode	B
or	O
mle	B
however	O
this	O
small	O
difference	O
can	O
be	O
important	O
in	O
practice	O
since	O
the	O
posterior	B
mean	B
will	O
result	O
in	O
less	O
overfitting	O
section	O
the	O
log-sum-exp	B
trick	O
we	O
now	O
discuss	O
one	O
important	O
practical	O
detail	O
that	O
arises	O
when	O
using	O
generative	O
classifiers	O
of	O
any	O
kind	O
we	O
can	O
compute	O
the	O
posterior	O
over	O
class	O
labels	O
using	O
equation	O
using	O
the	O
appropriate	O
class-conditional	B
density	I
a	O
plug-in	B
approximation	I
unfortunately	O
a	O
naive	O
implementation	O
of	O
equation	O
can	O
fail	O
due	O
to	O
numerical	O
underflow	O
the	O
problem	O
is	O
that	O
pxy	O
c	O
is	O
often	O
a	O
very	O
small	O
number	O
especially	O
if	O
x	O
is	O
a	O
high-dimensional	O
vector	O
this	O
is	O
because	O
we	O
require	O
x	O
pxy	O
so	O
the	O
probability	O
of	O
observing	O
any	O
particular	O
high-dimensional	O
vector	O
is	O
that	O
small	O
the	O
obvious	O
solution	O
is	O
to	O
take	O
logs	O
when	O
applying	O
bayes	B
rule	I
as	O
follows	O
log	O
py	O
cx	O
c	O
log	O
bc	O
log	O
pxy	O
c	O
log	O
py	O
c	O
however	O
this	O
requires	O
evaluating	O
the	O
following	O
expression	O
log	O
py	O
x	O
log	O
px	O
log	O
and	O
we	O
can	O
t	O
add	O
up	O
in	O
the	O
log	O
domain	O
fortunately	O
we	O
can	O
factor	B
out	O
the	O
largest	O
term	O
and	O
just	O
represent	O
the	O
remaining	O
numbers	O
relative	O
to	O
that	O
for	O
example	O
loge	O
e	O
log	O
e	O
e	O
e	O
c	O
ebc	O
log	O
ebc	O
beb	O
log	O
in	O
general	O
we	O
have	O
log	O
c	O
ebc	O
b	O
b	O
c	O
where	O
b	O
maxc	O
bc	O
this	O
is	O
called	O
the	O
log-sum-exp	B
trick	O
and	O
is	O
widely	O
used	O
the	O
function	O
logsumexp	O
for	O
an	O
implementation	O
this	O
trick	O
is	O
used	O
in	O
algorithm	O
which	O
gives	O
pseudo-code	O
for	O
using	O
an	O
nbc	O
to	O
compute	O
pyixi	O
see	O
naivebayespredict	O
for	O
the	O
matlab	O
code	O
note	O
that	O
we	O
do	O
not	O
need	O
the	O
log-sum-exp	B
trick	O
if	O
we	O
only	O
want	O
to	O
compute	O
yi	O
since	O
we	O
can	O
just	O
maximize	O
the	O
unnormalized	O
quantity	O
log	O
pyi	O
c	O
log	O
pxiy	O
c	O
feature	B
selection	I
using	O
mutual	B
information	B
since	O
an	O
nbc	O
is	O
fitting	O
a	O
joint	B
distribution	I
over	O
potentially	O
many	O
features	B
it	O
can	O
suffer	O
from	O
overfitting	O
in	O
addition	O
the	O
run-time	O
cost	O
is	O
od	O
which	O
may	O
be	O
too	O
high	O
for	O
some	O
applications	O
one	O
common	O
approach	O
to	O
tackling	O
both	O
of	O
these	O
problems	O
is	O
to	O
perform	O
feature	B
selection	I
to	O
remove	O
irrelevant	O
features	B
that	O
do	O
not	O
help	O
much	O
with	O
the	O
classification	O
problem	O
the	O
simplest	O
approach	O
to	O
feature	B
selection	I
is	O
to	O
evaluate	O
the	O
relevance	O
of	O
each	O
feature	O
separately	O
and	O
then	O
naive	O
bayes	O
classifiers	O
algorithm	O
predicting	O
with	O
a	O
naive	O
bayes	O
classifier	O
for	O
binary	O
features	B
for	O
i	O
do	O
for	O
c	O
do	O
lic	O
log	O
c	O
for	O
j	O
do	O
pic	O
explic	O
logsumexpli	O
yi	O
argmaxc	O
pic	O
if	O
xij	O
then	O
lic	O
lic	O
log	O
jc	O
else	O
lic	O
lic	O
jc	O
take	O
the	O
top	O
k	O
where	O
k	O
is	O
chosen	O
based	O
on	O
some	O
tradeoff	O
between	O
accuracy	O
and	O
complexity	O
this	O
approach	O
is	O
known	O
as	O
variable	O
ranking	B
filtering	B
orscreening	O
one	O
way	O
to	O
measure	O
relevance	O
is	O
to	O
use	O
mutual	B
information	B
between	O
feature	O
xj	O
and	O
the	O
class	O
label	B
y	O
xj	O
y	O
ix	O
y	O
pxj	O
y	O
log	O
pxj	O
y	O
pxjpy	O
the	O
mutual	B
information	B
can	O
be	O
thought	O
of	O
as	O
the	O
reduction	O
in	O
entropy	B
on	O
the	O
label	B
distribution	O
once	O
we	O
observe	O
the	O
value	O
of	O
feature	O
j	O
if	O
the	O
features	B
are	O
binary	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
mi	B
can	O
be	O
computed	O
as	O
follows	O
c	O
ij	O
jc	O
c	O
log	O
jc	O
j	O
jc	O
c	O
log	O
jc	O
j	O
where	O
c	O
py	O
c	O
jc	O
pxj	O
c	O
and	O
j	O
pxj	O
quantities	O
can	O
be	O
computed	O
as	O
a	O
by-product	O
of	O
fitting	O
a	O
naive	O
bayes	O
classifier	O
c	O
c	O
jc	O
figure	O
illustrates	O
what	O
happens	O
if	O
we	O
apply	O
this	O
to	O
the	O
binary	O
bag	B
of	I
words	I
dataset	O
used	O
in	O
figure	O
we	O
see	O
that	O
the	O
words	O
with	O
highest	O
mutual	B
information	B
are	O
much	O
more	O
discriminative	B
than	O
the	O
words	O
which	O
are	O
most	O
probable	O
for	O
example	O
the	O
most	O
probable	O
word	O
in	O
both	O
classes	O
is	O
subject	O
which	O
always	O
occurs	O
because	O
this	O
is	O
newsgroup	O
data	O
which	O
always	O
has	O
a	O
subject	O
line	O
but	O
obviously	O
this	O
is	O
not	O
very	O
discriminative	B
the	O
words	O
with	O
highest	O
mi	B
with	O
the	O
class	O
label	B
are	O
decreasing	O
order	O
windows	O
microsoft	B
dos	O
and	O
motif	B
which	O
makes	O
sense	O
since	O
the	O
classes	O
correspond	O
to	O
microsoft	B
windows	O
and	O
x	O
windows	O
classifying	O
documents	O
using	O
bag	B
of	I
words	I
of	O
these	O
document	O
classification	O
is	O
the	O
problem	O
of	O
classifying	O
text	O
documents	O
into	O
different	O
categories	O
one	O
simple	O
approach	O
is	O
to	O
represent	O
each	O
document	O
as	O
a	O
binary	O
vector	O
which	O
records	O
whether	O
each	O
word	O
is	O
present	O
or	O
not	O
so	O
xij	O
iff	B
word	O
j	O
occurs	O
in	O
document	O
i	O
otherwise	O
xij	O
we	O
can	O
then	O
use	O
the	O
following	O
class	O
conditional	O
density	O
pxiyi	O
c	O
berxij	O
jc	O
ixij	O
jc	O
xij	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
class	O
subject	O
this	O
with	O
but	O
you	O
prob	O
class	O
subject	O
windows	O
this	O
with	O
but	O
prob	O
highest	O
mi	B
windows	O
microsoft	B
dos	O
motif	B
window	O
mi	B
table	O
we	O
list	O
the	O
most	O
likely	O
words	O
for	O
class	O
windows	O
and	O
class	O
windows	O
we	O
also	O
show	O
the	O
words	O
with	O
highest	O
mutual	B
information	B
with	O
class	O
label	B
produced	O
by	O
naivebayesbowdemo	O
this	O
is	O
called	O
the	O
bernoulli	B
product	I
model	I
or	O
thebinary	O
independence	O
model	O
however	O
ignoring	O
the	O
number	O
of	O
times	O
each	O
word	O
occurs	O
in	O
a	O
document	O
loses	O
some	O
information	B
and	O
nigam	O
a	O
more	O
accurate	O
representation	O
counts	O
the	O
number	O
let	O
xi	O
be	O
a	O
vector	O
of	O
counts	O
for	O
document	O
i	O
so	O
of	O
occurrences	O
of	O
each	O
word	O
specifically	O
xij	O
ni	O
where	O
ni	O
is	O
the	O
number	O
of	O
terms	O
in	O
document	O
i	O
xij	O
ni	O
for	O
the	O
class	O
conditional	O
densities	O
we	O
can	O
use	O
a	O
multinomial	B
distribution	O
xij	O
jc	O
pxiyi	O
c	O
muxini	O
c	O
xij	O
where	O
we	O
have	O
implicitly	O
assumed	O
that	O
the	O
document	O
length	O
ni	O
is	O
independent	O
of	O
the	O
class	O
here	O
jc	O
is	O
the	O
probability	O
of	O
generating	O
word	O
j	O
in	O
documents	O
of	O
class	O
c	O
these	O
parameters	O
satisfy	O
the	O
constraint	O
that	O
jc	O
for	O
each	O
class	O
although	O
the	O
multinomial	B
classifier	O
is	O
easy	O
to	O
train	O
and	O
easy	O
to	O
use	O
at	O
test	O
time	O
it	O
does	O
not	O
work	O
particularly	O
well	O
for	O
document	O
classification	O
one	O
reason	O
for	O
this	O
is	O
that	O
it	O
does	O
not	O
take	O
into	O
account	O
the	O
burstiness	B
of	O
word	O
usage	O
this	O
refers	O
to	O
the	O
phenomenon	O
that	O
most	O
words	O
never	O
appear	O
in	O
any	O
given	O
document	O
but	O
if	O
they	O
do	O
appear	O
once	O
they	O
are	O
likely	O
to	O
appear	O
more	O
than	O
once	O
i	O
e	O
words	O
occur	O
in	O
bursts	O
the	O
multinomial	B
model	O
cannot	O
capture	O
the	O
burstiness	B
phenomenon	O
to	O
see	O
why	O
note	O
that	O
jc	O
and	O
since	O
jc	O
for	O
rare	O
words	O
it	O
becomes	O
increasingly	O
equation	O
has	O
the	O
form	O
nij	O
unlikely	O
to	O
generate	O
many	O
of	O
them	O
for	O
more	O
frequent	O
words	O
the	O
decay	O
rate	B
is	O
not	O
as	O
fast	O
to	O
see	O
why	O
intuitively	O
note	O
that	O
the	O
most	O
frequent	O
words	O
are	O
function	O
words	O
which	O
are	O
not	O
specific	O
to	O
the	O
class	O
such	O
as	O
and	O
the	O
and	O
but	O
the	O
chance	O
of	O
the	O
word	O
and	O
occuring	O
is	O
pretty	O
much	O
the	O
same	O
no	O
matter	O
how	O
many	O
time	O
it	O
has	O
previously	O
occurred	O
document	O
length	O
so	O
the	O
independence	O
assumption	O
is	O
more	O
reasonable	O
for	O
common	O
words	O
however	O
since	O
rare	O
words	O
are	O
the	O
ones	O
that	O
matter	O
most	O
for	O
classification	O
purposes	O
these	O
are	O
the	O
ones	O
we	O
want	O
to	O
model	O
the	O
most	O
carefully	O
various	O
ad	O
hoc	O
heuristics	B
have	O
been	O
proposed	O
to	O
improve	O
the	O
performance	O
of	O
the	O
multinomial	B
document	O
classifier	O
et	O
al	O
we	O
now	O
present	O
an	O
alternative	O
class	O
conditional	O
density	O
that	O
performs	O
as	O
well	O
as	O
these	O
ad	O
hoc	O
methods	O
yet	O
is	O
probabilistically	O
sound	O
et	O
al	O
since	O
equation	O
models	O
each	O
word	O
independently	O
this	O
model	O
is	O
often	O
called	O
a	O
naive	O
bayes	O
classifier	O
although	O
technically	O
the	O
features	B
xij	O
are	O
not	O
independent	O
because	O
of	O
the	O
constraint	O
j	O
xij	O
ni	O
naive	O
bayes	O
classifiers	O
suppose	O
we	O
simply	O
replace	O
the	O
multinomial	B
class	O
conditional	O
density	O
with	O
the	O
dirichlet	B
compound	I
multinomial	B
or	O
dcm	B
density	O
defined	O
as	O
follows	O
muxini	O
cdir	O
c	O
cd	B
c	O
pxiyi	O
c	O
xij	O
bxi	O
c	O
b	O
c	O
equation	O
is	O
derived	O
in	O
equation	O
surprisingly	O
this	O
simple	O
change	O
is	O
all	O
that	O
is	O
needed	O
to	O
capture	O
the	O
burstiness	B
phenomenon	O
the	O
intuitive	O
reason	O
for	O
this	O
is	O
as	O
follows	O
after	O
seeing	O
one	O
occurence	O
of	O
a	O
word	O
say	O
word	O
j	O
the	O
posterior	O
counts	O
on	O
j	O
gets	O
updated	O
making	O
another	O
occurence	O
of	O
word	O
j	O
more	O
likely	O
by	O
contrast	O
if	O
j	O
is	O
fixed	O
then	O
the	O
occurences	O
of	O
each	O
word	O
are	O
independent	O
the	O
multinomial	B
model	O
corresponds	O
to	O
drawing	O
a	O
ball	O
from	O
an	O
urn	O
with	O
k	O
colors	O
of	O
ball	O
recording	O
its	O
color	O
and	O
then	O
replacing	O
it	O
by	O
contrast	O
the	O
dcm	B
model	O
corresponds	O
to	O
drawing	O
a	O
ball	O
recording	O
its	O
color	O
and	O
then	O
replacing	O
it	O
with	O
one	O
additional	O
copy	O
this	O
is	O
called	O
the	O
polya	B
urn	I
using	O
the	O
dcm	B
as	O
the	O
class	O
conditional	O
density	O
gives	O
much	O
better	O
results	O
than	O
using	O
the	O
multinomial	B
and	O
has	O
performance	O
comparable	O
to	O
state	B
of	O
the	O
art	O
methods	O
as	O
described	O
in	O
et	O
al	O
the	O
only	O
disadvantage	O
is	O
that	O
fitting	O
the	O
dcm	B
model	O
is	O
more	O
complex	O
see	O
elkan	O
for	O
the	O
details	O
exercises	O
exercise	O
mle	B
for	O
the	O
bernoulli	B
binomial	B
model	O
derive	O
equation	O
by	O
optimizing	O
the	O
log	O
of	O
the	O
likelihood	B
in	O
equation	O
exercise	O
marginal	B
likelihood	B
for	O
the	O
beta-bernoulli	O
model	O
in	O
equation	O
we	O
showed	O
that	O
the	O
marginal	B
likelihood	B
is	O
the	O
ratio	O
of	O
the	O
normalizing	O
constants	O
pd	O
z	O
z	O
n	O
we	O
will	O
now	O
derive	O
an	O
alternative	O
derivation	O
of	O
this	O
fact	O
by	O
the	O
chain	B
rule	I
of	O
probability	O
in	O
section	O
we	O
showed	O
that	O
the	O
posterior	B
predictive	B
distribution	I
is	O
px	O
nk	O
k	O
i	O
ni	O
i	O
nk	O
k	O
n	O
where	O
k	O
and	O
is	O
the	O
data	O
seen	O
so	O
far	O
now	O
suppose	O
d	O
h	O
t	O
t	O
h	O
h	O
or	O
d	O
then	O
pd	O
n	O
show	O
how	O
this	O
reduces	O
to	O
equation	O
by	O
using	O
the	O
fact	O
that	O
for	O
integers	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
exercise	O
posterior	O
predictive	B
for	O
beta-binomial	B
model	O
recall	B
from	O
equation	O
that	O
the	O
posterior	O
predictive	B
for	O
the	O
beta-binomial	B
is	O
given	O
by	O
pxn	O
d	O
bx	O
n	O
n	O
x	O
b	O
n	O
x	O
prove	O
that	O
this	O
reduces	O
to	O
p	O
x	O
when	O
n	O
hence	O
x	O
i	O
e	O
show	O
that	O
hint	O
use	O
the	O
fact	O
that	O
exercise	O
beta	O
updating	O
from	O
censored	O
likelihood	B
gelman	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
times	O
let	O
x	O
be	O
the	O
number	O
of	O
heads	O
we	O
observe	O
that	O
there	O
are	O
fewer	O
than	O
heads	O
but	O
we	O
don	O
t	O
know	O
exactly	O
how	O
many	O
let	O
the	O
prior	O
probability	O
of	O
heads	O
be	O
p	O
beta	O
compute	O
the	O
posterior	O
p	O
up	O
to	O
normalization	O
constants	O
i	O
e	O
derive	O
an	O
expression	O
proportional	O
to	O
p	O
x	O
hint	O
the	O
answer	O
is	O
a	O
mixture	B
distribution	O
exercise	O
uninformative	B
prior	O
for	O
log-odds	B
ratio	I
let	O
logit	B
log	O
show	O
that	O
if	O
p	O
then	O
p	O
beta	O
hint	O
use	O
the	O
change	B
of	I
variables	I
formula	O
exercise	O
mle	B
for	O
the	O
poisson	B
distribution	O
the	O
poisson	B
pmf	B
is	O
defined	O
as	O
poix	O
x	O
parameter	B
derive	O
the	O
mle	B
x	O
for	O
x	O
where	O
is	O
the	O
rate	B
exercise	O
bayesian	B
analysis	O
of	O
the	O
poisson	B
distribution	O
in	O
exercise	O
we	O
defined	O
the	O
poisson	B
distribution	O
with	O
rate	B
and	O
derived	O
its	O
mle	B
here	O
we	O
perform	O
a	O
conjugate	O
bayesian	B
analysis	O
a	O
derive	O
the	O
posterior	O
p	O
assuming	O
a	O
conjugate	B
prior	I
p	O
ga	O
b	O
a	O
b	O
hint	O
the	O
b	O
what	O
does	O
the	O
posterior	B
mean	B
tend	O
to	O
as	O
a	O
and	O
b	O
that	O
the	O
mean	B
of	O
a	O
gaa	O
b	O
posterior	O
is	O
also	O
a	O
gamma	B
distribution	I
distribution	O
is	O
ab	O
exercise	O
mle	B
for	O
the	O
uniform	B
distribution	I
kaelbling	O
consider	O
a	O
uniform	B
distribution	I
centered	O
on	O
with	O
width	O
the	O
density	O
function	O
is	O
given	O
by	O
px	O
ix	O
a	O
a	O
naive	O
bayes	O
classifiers	O
a	O
given	O
a	O
data	O
set	O
xn	O
what	O
is	O
the	O
maximum	B
likelihood	B
estimate	I
of	O
a	O
it	O
a	O
b	O
what	O
probability	O
would	O
the	O
model	O
assign	O
to	O
a	O
new	O
data	O
point	O
using	O
a	O
c	O
do	O
you	O
see	O
any	O
problem	O
with	O
the	O
above	O
approach	O
briefly	O
suggest	O
words	O
a	O
better	O
approach	O
exercise	O
bayesian	B
analysis	O
of	O
the	O
uniform	B
distribution	I
consider	O
the	O
uniform	B
distribution	I
the	O
maximum	B
likelihood	B
estimate	I
is	O
maxd	O
as	O
we	O
saw	O
in	O
exercise	O
but	O
this	O
is	O
unsuitable	O
for	O
predicting	O
future	O
data	O
since	O
it	O
puts	O
zero	O
probability	O
mass	O
outside	O
the	O
training	O
data	O
in	O
this	O
exercise	O
we	O
will	O
perform	O
a	O
bayesian	B
analysis	O
of	O
the	O
uniform	B
distribution	I
the	O
conjugate	B
prior	I
is	O
the	O
pareto	B
distribution	I
p	O
pareto	O
k	O
defined	O
in	O
section	O
given	O
a	O
pareto	O
prior	O
the	O
joint	B
distribution	I
of	O
and	O
d	O
xn	O
is	O
pd	O
kbk	O
i	O
maxd	O
n	O
let	O
m	O
maxd	O
the	O
evidence	B
probability	O
that	O
all	O
n	O
samples	B
came	O
from	O
the	O
same	O
uniform	B
distribution	I
is	O
pd	O
kbk	O
d	O
if	O
m	O
b	O
if	O
m	O
b	O
m	O
n	O
k	O
kbk	O
derive	O
the	O
posterior	O
p	O
and	O
show	O
that	O
if	O
can	O
be	O
expressed	O
as	O
a	O
pareto	B
distribution	I
exercise	O
taxicab	O
problem	O
suppose	O
you	O
arrive	O
in	O
a	O
new	O
city	O
and	O
see	O
a	O
taxi	O
numbered	O
how	O
many	O
taxis	O
are	O
there	O
in	O
this	O
city	O
let	O
us	O
assume	O
taxis	O
are	O
numbered	O
sequentially	O
as	O
integers	O
starting	O
from	O
up	O
to	O
some	O
unknown	B
upper	O
bound	O
number	O
taxis	O
from	O
for	O
simplicity	O
we	O
can	O
also	O
count	O
from	O
without	O
changing	O
the	O
analysis	O
hence	O
the	O
likelihood	B
function	O
is	O
px	O
the	O
uniform	B
distribution	I
the	O
goal	O
is	O
to	O
estimate	O
we	O
will	O
use	O
the	O
bayesian	B
analysis	O
from	O
exercise	O
a	O
suppose	O
we	O
see	O
one	O
taxi	O
numbered	O
so	O
d	O
m	O
n	O
using	O
an	O
non-informative	B
prior	O
on	O
of	O
the	O
form	O
p	O
p	O
a	O
what	O
is	O
the	O
posterior	O
p	O
b	O
compute	O
the	O
posterior	B
mean	B
mode	B
and	O
median	B
number	O
of	O
taxis	O
in	O
the	O
city	O
if	O
such	O
quantities	O
exist	O
c	O
rather	O
than	O
trying	O
to	O
compute	O
a	O
point	B
estimate	I
of	O
the	O
number	O
of	O
taxis	O
we	O
can	O
compute	O
the	O
predictive	B
density	O
over	O
the	O
next	O
taxicab	O
number	O
using	O
where	O
k	O
are	O
the	O
hyper-parameters	B
n	O
k	O
are	O
the	O
updated	O
hyper-parameters	B
now	O
consider	O
the	O
case	O
d	O
and	O
using	O
equation	O
write	O
down	O
an	O
expression	O
for	O
pxd	O
as	O
above	O
use	O
a	O
non-informative	B
prior	O
b	O
k	O
d	O
use	O
the	O
predictive	B
density	O
formula	O
to	O
compute	O
the	O
probability	O
that	O
the	O
next	O
taxi	O
you	O
will	O
see	O
the	O
next	O
day	O
has	O
number	O
or	O
i	O
e	O
compute	O
px	O
px	O
px	O
e	O
briefly	O
describe	O
sentences	O
some	O
ways	O
we	O
might	O
make	O
the	O
model	O
more	O
accurate	O
at	O
prediction	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
exercise	O
bayesian	B
analysis	O
of	O
the	O
exponential	B
distribution	I
a	O
lifetime	O
x	O
of	O
a	O
machine	O
is	O
modeled	O
by	O
an	O
exponential	B
distribution	I
with	O
unknown	B
parameter	B
the	O
likelihood	B
is	O
px	O
e	O
x	O
for	O
x	O
a	O
show	O
that	O
the	O
mle	B
is	O
where	O
x	O
n	O
b	O
suppose	O
we	O
observe	O
lifetimes	O
years	O
of	O
different	O
iid	B
machines	O
xi	O
what	O
is	O
the	O
mle	B
given	O
this	O
data	O
c	O
assume	O
that	O
an	O
expert	O
believes	O
should	O
have	O
a	O
prior	O
distribution	O
that	O
is	O
also	O
exponential	O
p	O
expon	O
choose	O
the	O
prior	O
parameter	B
call	O
it	O
such	O
that	O
e	O
hint	O
recall	B
that	O
the	O
gamma	B
distribution	I
has	O
the	O
form	O
ga	O
b	O
a	O
b	O
and	O
its	O
mean	B
is	O
ab	O
d	O
what	O
is	O
the	O
posterior	O
p	O
e	O
f	O
what	O
is	O
the	O
posterior	B
mean	B
e	O
is	O
the	O
exponential	O
prior	O
conjugate	O
to	O
the	O
exponential	O
likelihood	B
g	O
explain	O
why	O
the	O
mle	B
and	O
posterior	B
mean	B
differ	O
which	O
is	O
more	O
reasonable	O
in	O
this	O
example	O
exercise	O
map	O
estimation	O
for	O
the	O
bernoulli	B
with	O
non-conjugate	O
priors	O
prior	O
p	O
beta	O
we	O
know	O
that	O
with	O
this	O
prior	O
the	O
map	B
estimate	I
is	O
given	O
by	O
in	O
the	O
book	O
we	O
discussed	O
bayesian	B
inference	B
of	O
a	O
bernoulli	B
rate	B
parameter	B
with	O
the	O
jaakkola	O
n	O
p	O
where	O
is	O
the	O
number	O
of	O
heads	O
is	O
the	O
number	O
of	O
tails	O
and	O
n	O
is	O
the	O
total	O
number	O
of	O
trials	O
a	O
now	O
consider	O
the	O
following	O
prior	O
that	O
believes	O
the	O
coin	O
is	O
fair	O
or	O
is	O
slightly	O
biased	O
towards	O
tails	O
if	O
if	O
otherwise	O
derive	O
the	O
map	B
estimate	I
under	O
this	O
prior	O
as	O
a	O
function	O
of	O
and	O
n	O
b	O
suppose	O
the	O
true	O
parameter	B
is	O
which	O
prior	O
leads	O
to	O
a	O
better	O
estimate	O
when	O
n	O
is	O
small	O
which	O
prior	O
leads	O
to	O
a	O
better	O
estimate	O
when	O
n	O
is	O
large	O
exercise	O
posterior	B
predictive	B
distribution	I
for	O
a	O
batch	B
of	O
data	O
with	O
the	O
dirichlet-multinomial	O
model	O
in	O
equation	O
we	O
gave	O
the	O
the	O
posterior	B
predictive	B
distribution	I
for	O
a	O
single	O
multinomial	B
trial	O
using	O
a	O
dirichlet	B
prior	O
now	O
consider	O
predicting	O
a	O
batch	B
of	O
new	O
data	O
d	O
xm	O
consisting	O
of	O
m	O
single	O
multinomial	B
trials	O
of	O
predicting	O
the	O
next	O
m	O
words	O
in	O
a	O
sentence	O
assuming	O
they	O
are	O
drawn	O
iid	B
derive	O
an	O
expression	O
for	O
p	O
dd	O
naive	O
bayes	O
classifiers	O
your	O
answer	O
should	O
be	O
a	O
function	O
of	O
and	O
the	O
old	O
and	O
new	O
counts	O
statistics	O
defined	O
as	O
n	O
old	O
k	O
n	O
new	O
k	O
ixi	O
k	O
ixi	O
k	O
i	O
d	O
i	O
d	O
hint	O
recall	B
that	O
for	O
a	O
vector	O
of	O
counts	O
the	O
marginal	B
likelihood	B
is	O
given	O
by	O
pd	O
k	O
where	O
k	O
k	O
and	O
n	O
k	O
k	O
k	O
nk	O
exercise	O
posterior	O
predictive	B
for	O
dirichlet-multinomial	O
koller	O
a	O
suppose	O
we	O
compute	O
the	O
empirical	B
distribution	I
over	O
letters	O
of	O
the	O
roman	O
alphabet	O
plus	O
the	O
space	O
character	O
distribution	O
over	O
values	O
from	O
samples	B
suppose	O
we	O
see	O
the	O
letter	O
e	O
times	O
what	O
is	O
ed	O
if	O
we	O
assume	O
dir	O
where	O
k	O
for	O
all	O
k	O
in	O
the	O
samples	B
we	O
saw	O
e	O
times	O
a	O
times	O
and	O
p	O
times	O
what	O
is	O
p	O
ad	O
if	O
we	O
assume	O
dir	O
where	O
k	O
for	O
all	O
k	O
show	O
your	O
work	O
b	O
suppose	O
exercise	O
setting	O
the	O
beta	O
hyper-parameters	B
suppose	O
and	O
we	O
believe	O
that	O
e	O
m	O
and	O
var	B
v	O
using	O
equation	O
solve	O
for	O
and	O
in	O
terms	O
of	O
m	O
and	O
v	O
what	O
values	O
do	O
you	O
get	O
if	O
m	O
and	O
v	O
exercise	O
setting	O
the	O
beta	O
hyper-parameters	B
ii	O
draper	O
suppose	O
and	O
we	O
believe	O
that	O
e	O
and	O
u	O
write	O
a	O
program	O
that	O
can	O
solve	O
for	O
and	O
in	O
terms	O
of	O
m	O
and	O
u	O
hint	O
write	O
as	O
a	O
function	O
of	O
and	O
m	O
so	O
the	O
pdf	B
only	O
has	O
one	O
unknown	B
then	O
write	O
down	O
the	O
probability	O
mass	O
contained	O
in	O
the	O
interval	O
as	O
an	O
integral	O
and	O
minimize	O
its	O
squared	O
discrepancy	O
from	O
what	O
values	O
do	O
you	O
get	O
if	O
m	O
and	O
u	O
what	O
is	O
the	O
equivalent	B
sample	I
size	I
of	O
this	O
prior	O
exercise	O
marginal	B
likelihood	B
for	O
beta-binomial	B
under	O
uniform	O
prior	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
times	O
and	O
observe	O
heads	O
let	O
binn	O
and	O
show	O
that	O
the	O
marginal	B
likelihood	B
is	O
hint	O
x	O
if	O
x	O
is	O
an	O
integer	O
exercise	O
bayes	B
factor	B
for	O
coin	O
tossing	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
times	O
and	O
observe	O
heads	O
let	O
the	O
null	B
hypothesis	I
be	O
that	O
the	O
coin	O
is	O
fair	O
and	O
the	O
alternative	O
be	O
that	O
the	O
coin	O
can	O
have	O
any	O
bias	B
so	O
p	O
derive	O
the	O
bayes	B
factor	B
in	O
favor	O
of	O
the	O
biased	O
coin	O
hypothesis	O
what	O
if	O
n	O
and	O
hint	O
see	O
exercise	O
exercise	O
irrelevant	O
features	B
with	O
naive	O
bayes	O
jaakkola	O
let	O
xiw	O
if	O
word	O
w	O
occurs	O
in	O
document	O
i	O
and	O
xiw	O
otherwise	O
let	O
cw	O
be	O
the	O
estimated	O
probability	O
that	O
word	O
w	O
occurs	O
in	O
documents	O
of	O
class	O
c	O
then	O
the	O
log-likelihood	O
that	O
document	O
chapter	O
generative	O
models	O
for	O
discrete	B
data	O
x	O
belongs	O
to	O
class	O
c	O
is	O
log	O
pxic	O
log	O
cw	O
xiw	O
xiw	O
xiw	O
log	O
cw	O
xiw	O
cw	O
xiw	O
log	O
cw	O
cw	O
cw	O
w	O
where	O
w	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
vocabulary	O
we	O
can	O
write	O
this	O
more	O
succintly	O
as	O
log	O
pxic	O
c	O
where	O
xi	O
xiw	O
is	O
a	O
bit	O
vector	O
and	O
c	O
cw	O
cw	O
cwt	O
w	O
we	O
see	O
that	O
this	O
is	O
a	O
linear	O
classifier	O
since	O
the	O
class-conditional	B
density	I
is	O
a	O
linear	O
function	O
inner	O
product	O
of	O
the	O
parameters	O
c	O
a	O
assuming	O
pc	O
write	O
down	O
an	O
expression	O
for	O
the	O
log	O
posterior	O
odds	O
ratio	O
in	O
terms	O
of	O
the	O
features	B
and	O
the	O
parameters	O
and	O
b	O
intuitively	O
words	O
that	O
occur	O
in	O
both	O
classes	O
are	O
not	O
very	O
discriminative	B
and	O
therefore	O
should	O
not	O
affect	O
our	O
beliefs	O
about	O
the	O
class	O
label	B
consider	O
a	O
particular	O
word	O
w	O
state	B
the	O
conditions	O
on	O
and	O
equivalently	O
the	O
conditions	O
on	O
under	O
which	O
the	O
presence	O
or	O
absence	O
of	O
w	O
in	O
a	O
test	O
document	O
will	O
have	O
no	O
effect	O
on	O
the	O
class	O
posterior	O
a	O
word	O
will	O
be	O
ignored	O
by	O
the	O
classifier	O
hint	O
using	O
your	O
previous	O
result	O
figure	O
out	O
when	O
the	O
posterior	O
odds	O
ratio	O
is	O
c	O
the	O
posterior	B
mean	B
estimate	O
of	O
using	O
a	O
prior	O
is	O
given	O
by	O
cw	O
i	O
c	O
xiw	O
c	O
where	O
the	O
sum	O
is	O
over	O
the	O
nc	O
documents	O
in	O
class	O
c	O
consider	O
a	O
particular	O
word	O
w	O
and	O
suppose	O
it	O
always	O
occurs	O
in	O
every	O
document	O
of	O
class	O
let	O
there	O
be	O
documents	O
of	O
class	O
and	O
be	O
the	O
number	O
of	O
documents	O
in	O
class	O
where	O
e	O
g	O
we	O
get	O
much	O
more	O
non-spam	O
than	O
if	O
we	O
use	O
the	O
above	O
estimate	O
for	O
cw	O
will	O
word	O
w	O
be	O
spam	B
this	O
is	O
an	O
example	O
of	O
class	B
imbalance	I
ignored	O
by	O
our	O
classifier	O
explain	O
why	O
or	O
why	O
not	O
d	O
what	O
other	O
ways	O
can	O
you	O
think	O
of	O
which	O
encourage	O
irrelevant	O
words	O
to	O
be	O
ignored	O
exercise	O
class	O
conditional	O
densities	O
for	O
binary	O
data	O
consider	O
a	O
generative	O
classifier	O
for	O
c	O
classes	O
with	O
class	O
conditional	O
density	O
pxy	O
and	O
uniform	O
class	O
prior	O
py	O
suppose	O
all	O
the	O
d	O
features	B
are	O
binary	O
xj	O
if	O
we	O
assume	O
all	O
the	O
features	B
are	O
conditionally	B
independent	I
naive	O
bayes	O
assumption	O
we	O
can	O
write	O
pxy	O
c	O
berxj	O
jc	O
this	O
requires	O
dc	O
parameters	O
naive	O
bayes	O
classifiers	O
a	O
now	O
consider	O
a	O
different	O
model	O
which	O
we	O
will	O
call	O
the	O
full	B
model	O
in	O
which	O
all	O
the	O
features	B
are	O
fully	O
dependent	O
we	O
make	O
no	O
factorization	O
assumptions	O
how	O
might	O
we	O
represent	O
pxy	O
c	O
in	O
this	O
case	O
how	O
many	O
parameters	O
are	O
needed	O
to	O
represent	O
pxy	O
c	O
b	O
assume	O
the	O
number	O
of	O
features	B
d	O
is	O
fixed	O
let	O
there	O
be	O
n	O
training	O
cases	O
if	O
the	O
sample	O
size	O
n	O
is	O
very	O
small	O
which	O
model	O
bayes	O
or	O
full	B
is	O
likely	O
to	O
give	O
lower	O
test	O
set	O
error	O
and	O
why	O
if	O
the	O
sample	O
size	O
n	O
is	O
very	O
large	O
which	O
model	O
bayes	O
or	O
full	B
is	O
likely	O
to	O
give	O
lower	O
test	O
set	O
error	O
and	O
why	O
c	O
d	O
what	O
is	O
the	O
computational	O
complexity	O
of	O
fitting	O
the	O
full	B
and	O
naive	O
bayes	O
models	O
as	O
a	O
function	O
of	O
n	O
the	O
model	O
here	O
means	O
computing	O
the	O
mle	B
or	O
map	O
parameter	B
and	O
d	O
use	O
big-oh	O
notation	O
estimates	O
you	O
may	O
assume	O
you	O
can	O
convert	O
a	O
d-bit	O
vector	O
to	O
an	O
array	O
index	O
in	O
od	O
time	O
e	O
what	O
is	O
the	O
computational	O
complexity	O
of	O
applying	O
the	O
full	B
and	O
naive	O
bayes	O
models	O
at	O
test	O
time	O
to	O
a	O
single	O
test	O
case	O
f	O
suppose	O
the	O
test	O
case	O
has	O
missing	B
data	I
let	O
xv	O
be	O
the	O
visible	B
features	B
of	O
size	O
v	O
and	O
xh	O
be	O
the	O
hidden	B
features	B
of	O
size	O
h	O
where	O
v	O
h	O
d	O
what	O
is	O
the	O
computational	O
complexity	O
of	O
computing	O
pyxv	O
for	O
the	O
full	B
and	O
naive	O
bayes	O
models	O
as	O
a	O
function	O
of	O
v	O
and	O
h	O
exercise	O
mutual	B
information	B
for	O
naive	O
bayes	O
classifiers	O
with	O
binary	O
features	B
derive	O
equation	O
exercise	O
fitting	O
a	O
naive	O
bayes	O
spam	B
filter	O
by	O
hand	O
daphne	O
koller	O
consider	O
a	O
naive	O
bayes	O
model	O
bernoulli	B
version	O
for	O
spam	B
classification	O
with	O
the	O
vocabulary	O
vsecret	O
we	O
have	O
the	O
following	O
example	O
spam	B
messages	O
dollar	O
offer	O
offer	O
today	O
is	O
secret	O
and	O
normal	B
messages	O
price	O
for	O
valued	O
customer	O
secret	O
sports	O
today	O
is	O
healthy	O
price	O
pizza	O
give	O
the	O
mles	O
for	O
the	O
following	O
parameters	O
spam	B
secretspam	O
secretnon-spam	O
sportsnon-spam	O
dollarspam	O
gaussian	B
models	O
introduction	O
in	O
this	O
chapter	O
we	O
discuss	O
the	O
multivariate	B
gaussian	B
or	O
multivariate	B
normal	B
which	O
is	O
the	O
most	O
widely	O
used	O
joint	O
probability	B
density	I
function	I
for	O
continuous	O
variables	O
it	O
will	O
form	O
the	O
basis	O
for	O
many	O
of	O
the	O
models	O
we	O
will	O
encounter	O
in	O
later	O
chapters	O
unfortunately	O
the	O
level	O
of	O
mathematics	O
in	O
this	O
chapter	O
is	O
higher	O
than	O
in	O
many	O
other	O
chapters	O
in	O
particular	O
we	O
rely	O
heavily	O
on	O
linear	O
algebra	O
and	O
matrix	O
calculus	O
this	O
is	O
the	O
price	O
one	O
must	O
pay	O
in	O
order	O
to	O
deal	O
with	O
high-dimensional	O
data	O
beginners	O
may	O
choose	O
to	O
skip	O
sections	O
marked	O
with	O
a	O
in	O
addition	O
since	O
there	O
are	O
so	O
many	O
equations	O
in	O
this	O
chapter	O
we	O
have	O
put	O
a	O
box	O
around	O
those	O
that	O
are	O
particularly	O
important	O
notation	O
let	O
us	O
briefly	O
say	O
a	O
few	O
words	O
about	O
notation	O
we	O
denote	O
vectors	O
by	O
boldface	O
lower	O
case	O
letters	O
such	O
as	O
x	O
we	O
denote	O
matrices	O
by	O
boldface	O
upper	O
case	O
letters	O
such	O
as	O
x	O
we	O
denote	O
entries	O
in	O
a	O
matrix	O
by	O
non-bold	O
upper	O
case	O
letters	O
such	O
as	O
xij	O
all	O
vectors	O
are	O
assumed	O
to	O
be	O
column	O
vectors	O
unless	O
noted	O
otherwise	O
we	O
use	O
xd	O
to	O
denote	O
a	O
column	O
vector	O
created	O
by	O
stacking	B
d	O
scalars	O
similarly	O
if	O
we	O
write	O
x	O
xd	O
where	O
the	O
left	O
hand	O
side	O
is	O
a	O
tall	O
column	O
vector	O
we	O
mean	B
to	O
stack	O
the	O
xi	O
along	O
the	O
rows	O
this	O
is	O
usually	O
written	O
as	O
x	O
dt	O
but	O
that	O
is	O
rather	O
ugly	O
if	O
we	O
write	O
x	O
xd	O
where	O
the	O
left	O
hand	O
side	O
is	O
a	O
matrix	O
we	O
mean	B
to	O
stack	O
the	O
xi	O
along	O
the	O
columns	O
creating	O
a	O
matrix	O
xt	O
basics	O
recall	B
from	O
section	O
that	O
the	O
pdf	B
for	O
an	O
mvn	B
in	O
d	O
dimensions	O
is	O
defined	O
by	O
the	O
following	O
n	O
exp	O
chapter	O
gaussian	B
models	O
x	O
figure	O
visualization	O
of	O
a	O
dimensional	O
gaussian	B
density	O
the	O
major	O
and	O
minor	O
axes	O
of	O
the	O
ellipse	O
are	O
defined	O
by	O
the	O
first	O
two	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
namely	O
and	O
based	O
on	O
figure	O
of	O
the	O
expression	O
inside	O
the	O
exponent	O
is	O
the	O
mahalanobis	B
distance	I
between	O
a	O
data	O
vector	O
x	O
and	O
the	O
mean	B
vector	O
we	O
can	O
gain	O
a	O
better	O
understanding	O
of	O
this	O
quantity	O
by	O
performing	O
an	O
eigendecomposition	B
of	O
that	O
is	O
we	O
write	O
u	O
ut	O
whereu	O
is	O
an	O
orthonormal	O
matrix	O
of	O
eigenvectors	O
satsifying	O
ut	O
u	O
i	O
and	O
is	O
a	O
diagonal	B
matrix	O
of	O
eigenvalues	O
using	O
the	O
eigendecomposition	B
we	O
have	O
that	O
u	O
t	O
u	O
i	O
uiut	O
i	O
where	O
ui	O
is	O
the	O
i	O
th	O
column	O
of	O
u	O
containing	O
the	O
i	O
th	O
eigenvector	O
hence	O
we	O
can	O
rewrite	O
the	O
mahalanobis	B
distance	I
as	O
follows	O
i	O
uiut	O
i	O
i	O
uiut	O
i	O
i	O
i	O
where	O
yi	O
ut	O
i	O
recall	B
that	O
the	O
equation	O
for	O
an	O
ellipse	O
in	O
is	O
hence	O
we	O
see	O
that	O
the	O
contours	O
of	O
equal	O
probability	O
density	O
of	O
a	O
gaussian	B
lie	O
along	O
ellipses	O
this	O
is	O
illustrated	O
in	O
figure	O
the	O
eigenvectors	O
determine	O
the	O
orientation	O
of	O
the	O
ellipse	O
and	O
the	O
eigenvalues	O
determine	O
how	O
elogonated	O
it	O
is	O
in	O
general	O
we	O
see	O
that	O
the	O
mahalanobis	B
distance	I
corresponds	O
to	O
euclidean	B
distance	I
in	O
a	O
transformed	O
coordinate	O
system	O
where	O
we	O
shift	O
by	O
and	O
rotate	O
by	O
u	O
introduction	O
mle	B
for	O
an	O
mvn	B
we	O
now	O
describe	O
one	O
way	O
to	O
estimate	O
the	O
parameters	O
of	O
an	O
mvn	B
using	O
mle	B
in	O
later	O
sections	O
we	O
will	O
discuss	O
bayesian	B
inference	B
for	O
the	O
parameters	O
which	O
can	O
mitigate	O
overfitting	O
and	O
can	O
provide	O
a	O
measure	O
of	O
confidence	O
in	O
our	O
estimates	O
theorem	O
for	O
a	O
gaussian	B
if	O
we	O
have	O
n	O
iid	B
samples	B
xi	O
n	O
then	O
the	O
mle	B
for	O
the	O
parameters	O
is	O
given	O
by	O
mle	B
mle	B
n	O
n	O
xi	O
x	O
xxi	O
xt	O
n	O
i	O
x	O
xt	O
xixt	O
that	O
is	O
the	O
mle	B
is	O
just	O
the	O
empirical	O
mean	B
and	O
empirical	O
covariance	B
in	O
the	O
univariate	O
case	O
we	O
get	O
the	O
following	O
familiar	O
results	O
xi	O
x	O
i	O
n	O
i	O
i	O
i	O
n	O
n	O
proof	O
to	O
prove	O
this	O
result	O
we	O
will	O
need	O
several	O
results	O
from	O
matrix	O
algebra	O
which	O
we	O
summarize	O
in	O
the	O
equations	O
a	O
and	O
b	O
are	O
vectors	O
and	O
a	O
and	O
b	O
are	O
matrices	O
also	O
the	O
notation	O
below	O
tra	O
refers	O
to	O
the	O
trace	B
of	O
a	O
matrix	O
which	O
is	O
the	O
sum	O
of	O
its	O
diagonals	O
tra	O
i	O
aii	O
a	O
a	O
aa	O
a	O
b	O
at	O
a	O
a	O
trba	O
t	O
log	O
a	O
t	O
trabc	O
trcab	O
trbca	O
the	O
last	O
equation	O
is	O
called	O
the	O
cyclic	B
permutation	I
property	I
of	O
the	O
trace	B
operator	O
using	O
this	O
we	O
can	O
derive	O
the	O
widely	O
used	O
trace	B
trick	I
which	O
reorders	O
the	O
scalar	O
inner	O
product	O
xt	O
ax	O
as	O
follows	O
xt	O
ax	O
trxt	O
ax	O
trxxt	O
a	O
traxxt	O
chapter	O
gaussian	B
models	O
proof	O
we	O
can	O
now	O
begin	O
with	O
the	O
proof	O
the	O
log-likelihood	O
is	O
log	O
pd	O
n	O
log	O
where	O
is	O
the	O
precision	B
matrix	I
using	O
the	O
substitution	O
yi	O
xi	O
and	O
the	O
chain	B
rule	I
of	O
calculus	O
we	O
have	O
yi	O
i	O
yt	O
yi	O
t	O
so	O
the	O
mle	B
of	O
is	O
just	O
the	O
empirical	O
mean	B
now	O
we	O
can	O
use	O
the	O
trace-trick	O
to	O
rewrite	O
the	O
log-likelihood	O
for	O
as	O
follows	O
trxi	O
hence	O
n	O
xi	O
x	O
i	O
tr	O
n	O
n	O
log	O
log	O
where	O
s	O
is	O
the	O
scatter	O
matrix	O
centered	O
on	O
taking	O
derivatives	O
of	O
this	O
expression	O
with	O
respect	O
to	O
yields	O
n	O
t	O
t	O
st	O
n	O
s	O
so	O
n	O
if	O
we	O
plug-in	B
the	O
mle	B
x	O
which	O
is	O
just	O
the	O
empirical	O
covariance	B
matrix	I
centered	O
on	O
both	O
parameters	O
must	O
be	O
simultaneously	O
optimized	O
we	O
get	O
the	O
standard	O
equation	O
for	O
the	O
mle	B
of	O
a	O
covariance	B
matrix	I
gaussian	B
discriminant	B
analysis	I
maximum	B
entropy	B
derivation	O
of	O
the	O
gaussian	B
in	O
this	O
section	O
we	O
show	O
that	O
the	O
multivariate	B
gaussian	B
is	O
the	O
distribution	O
with	O
maximum	B
entropy	B
subject	O
to	O
having	O
a	O
specified	O
mean	B
and	O
covariance	B
also	O
section	O
this	O
is	O
one	O
reason	O
the	O
gaussian	B
is	O
so	O
widely	O
used	O
the	O
first	O
two	O
moments	O
are	O
usually	O
all	O
that	O
we	O
can	O
reliably	O
estimate	O
from	O
data	O
so	O
we	O
want	O
a	O
distribution	O
that	O
captures	O
these	O
properties	O
but	O
otherwise	O
makes	O
as	O
few	O
addtional	O
assumptions	O
as	O
possible	O
to	O
simplify	O
notation	O
we	O
will	O
assume	O
the	O
mean	B
is	O
zero	O
the	O
pdf	B
has	O
the	O
form	O
z	O
exp	O
xt	O
px	O
for	O
i	O
j	O
d	O
we	O
see	O
that	O
this	O
is	O
in	O
if	O
we	O
define	O
fijx	O
ixj	O
and	O
ij	O
the	O
same	O
form	O
as	O
equation	O
the	O
entropy	B
of	O
this	O
distribution	O
log	O
base	O
e	O
is	O
given	O
by	O
hn	O
ln	O
ed	O
we	O
now	O
show	O
the	O
mvn	B
has	O
maximum	B
entropy	B
amongst	O
all	O
distributions	O
with	O
a	O
specified	O
covariance	B
qxxixj	O
ij	O
let	O
p	O
n	O
then	O
theorem	O
let	O
qx	O
be	O
any	O
density	O
satisfying	O
hq	O
hp	O
proof	O
and	O
thomas	O
we	O
have	O
kl	O
hq	O
hq	O
hq	O
p	O
qx	O
log	O
qx	O
px	O
dx	O
qx	O
log	O
pxdx	O
px	O
log	O
pxdx	O
where	O
the	O
key	O
step	O
in	O
equation	O
with	O
a	O
follows	O
since	O
q	O
and	O
p	O
yield	O
the	O
same	O
moments	O
for	O
the	O
quadratic	O
form	O
encoded	O
by	O
log	O
px	O
gaussian	B
discriminant	B
analysis	I
one	O
important	O
application	O
of	O
mvns	O
is	O
to	O
define	O
the	O
the	O
class	O
conditional	O
densities	O
in	O
a	O
generative	O
classifier	O
i	O
e	O
pxy	O
c	O
n	O
c	O
c	O
the	O
resulting	O
technique	O
is	O
called	O
discriminant	B
analysis	I
or	O
gda	B
though	O
it	O
is	O
a	O
generative	O
not	O
discriminative	B
classifier	O
see	O
section	O
for	O
more	O
on	O
this	O
distinction	O
if	O
c	O
is	O
diagonal	B
this	O
is	O
equivalent	O
to	O
naive	O
bayes	O
chapter	O
gaussian	B
models	O
t	O
i	O
h	O
g	O
e	O
w	O
red	O
female	O
bluemale	O
height	O
t	O
i	O
h	O
g	O
e	O
w	O
red	O
female	O
bluemale	O
height	O
figure	O
heightweight	O
data	O
visualization	O
of	O
gaussians	O
fit	O
to	O
each	O
class	O
of	O
the	O
probability	O
mass	O
is	O
inside	O
the	O
ellipse	O
figure	O
generated	O
by	O
gaussheightweight	O
we	O
can	O
classify	O
a	O
feature	O
vector	O
using	O
the	O
following	O
decision	B
rule	I
derived	O
from	O
equation	O
yx	O
argmax	O
c	O
py	O
c	O
log	O
px	O
c	O
when	O
we	O
compute	O
the	O
probability	O
of	O
x	O
under	O
each	O
class	O
conditional	O
density	O
we	O
are	O
measuring	O
the	O
distance	O
from	O
x	O
to	O
the	O
center	O
of	O
each	O
class	O
c	O
using	O
mahalanobis	B
distance	I
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
nearest	O
centroids	B
classifier	O
as	O
an	O
example	O
figure	O
shows	O
two	O
gaussian	B
class-conditional	O
densities	O
in	O
representing	O
the	O
height	O
and	O
weight	O
of	O
men	O
and	O
women	O
we	O
can	O
see	O
that	O
the	O
features	B
are	O
correlated	O
as	O
is	O
to	O
be	O
expected	O
people	O
tend	O
to	O
weigh	O
more	O
the	O
ellipses	O
for	O
each	O
class	O
contain	O
of	O
the	O
probability	O
mass	O
if	O
we	O
have	O
a	O
uniform	O
prior	O
over	O
classes	O
we	O
can	O
classify	O
a	O
new	O
test	O
vector	O
as	O
follows	O
yx	O
argmin	O
c	O
ct	O
c	O
c	O
quadratic	B
discriminant	B
analysis	I
the	O
posterior	O
over	O
class	O
labels	O
is	O
given	O
by	O
equation	O
we	O
can	O
gain	O
further	O
insight	O
into	O
this	O
model	O
by	O
plugging	O
in	O
the	O
definition	O
of	O
the	O
gaussian	B
density	O
as	O
follows	O
py	O
cx	O
c	O
exp	O
exp	O
ct	O
c	O
c	O
thresholding	O
this	O
results	O
in	O
a	O
quadratic	O
function	O
of	O
x	O
the	O
result	O
is	O
known	O
as	O
quadratic	B
discriminant	B
analysis	I
figure	O
gives	O
some	O
examples	O
of	O
what	O
the	O
decision	B
boundaries	O
look	O
like	O
in	O
gaussian	B
discriminant	B
analysis	I
parabolic	O
boundary	O
some	O
linear	O
some	O
quadratic	O
figure	O
quadratic	O
decision	B
boundaries	O
in	O
for	O
the	O
and	O
class	O
case	O
discrimanalysisdboundariesdemo	O
figure	O
generated	O
by	O
softmax	B
distribution	O
s	O
where	O
at	O
different	O
temperatures	O
t	O
when	O
the	O
figure	O
temperature	B
is	O
high	O
the	O
distribution	O
is	O
uniform	O
whereas	O
when	O
the	O
temperature	B
is	O
low	O
the	O
distribution	O
is	O
spiky	O
with	O
all	O
its	O
mass	O
on	O
the	O
largest	O
element	O
figure	O
generated	O
by	O
linear	B
discriminant	B
analysis	I
py	O
cx	O
c	O
exp	O
we	O
now	O
consider	O
a	O
special	O
case	O
in	O
which	O
the	O
covariance	B
matrices	O
are	O
tied	B
or	O
shared	B
across	O
classes	O
c	O
in	O
this	O
case	O
we	O
can	O
simplify	O
equation	O
as	O
follows	O
c	O
c	O
t	O
exp	O
since	O
the	O
quadratic	O
term	O
xt	O
is	O
independent	O
of	O
c	O
it	O
will	O
cancel	O
out	O
in	O
the	O
numerator	O
and	O
denominator	O
if	O
we	O
define	O
xt	O
xt	O
c	O
t	O
c	O
c	O
log	O
c	O
t	O
c	O
t	O
exp	O
c	O
c	O
c	O
log	O
c	O
t	O
c	O
c	O
chapter	O
gaussian	B
models	O
c	O
x	O
c	O
e	O
t	O
e	O
t	O
x	O
s	O
cx	O
c	O
and	O
s	O
is	O
the	O
softmax	B
function	O
defined	O
as	O
follows	O
then	O
we	O
can	O
write	O
py	O
cx	O
where	O
t	O
s	O
x	O
t	O
e	O
e	O
the	O
softmax	B
function	O
is	O
so-called	O
since	O
it	O
acts	O
a	O
bit	O
like	O
the	O
max	O
function	O
to	O
see	O
this	O
let	O
us	O
divide	O
each	O
c	O
by	O
a	O
constant	O
t	O
called	O
the	O
temperature	B
then	O
as	O
t	O
we	O
find	O
s	O
if	O
c	O
otherwise	O
in	O
other	O
words	O
at	O
low	O
temperatures	O
the	O
distribution	O
spends	O
essentially	O
all	O
of	O
its	O
time	O
in	O
the	O
most	O
probable	O
state	B
whereas	O
at	O
high	O
temperatures	O
it	O
visits	O
all	O
states	O
uniformly	O
see	O
figure	O
for	O
an	O
illustration	O
note	O
that	O
this	O
terminology	O
comes	O
from	O
the	O
area	O
of	O
statistical	O
physics	O
where	O
it	O
is	O
common	O
to	O
use	O
the	O
boltzmann	B
distribution	I
which	O
has	O
the	O
same	O
form	O
as	O
the	O
softmax	B
function	O
py	O
cx	O
an	O
interesting	O
property	O
of	O
equation	O
is	O
that	O
if	O
we	O
take	O
logs	O
we	O
end	O
up	O
with	O
a	O
linear	O
reason	O
it	O
is	O
linear	O
is	O
because	O
the	O
xt	O
cancels	O
from	O
the	O
numerator	O
function	O
of	O
x	O
and	O
denominator	O
thus	O
the	O
decision	B
boundary	I
between	O
any	O
two	O
classes	O
say	O
c	O
and	O
will	O
be	O
a	O
straight	O
line	O
hence	O
this	O
technique	O
is	O
called	O
linear	B
discriminant	B
analysis	I
or	O
lda	B
we	O
can	O
derive	O
the	O
form	O
of	O
this	O
line	O
as	O
follows	O
y	O
x	O
xt	O
c	O
see	O
figure	O
for	O
some	O
examples	O
an	O
alternative	O
to	O
fitting	O
an	O
lda	B
model	O
and	O
then	O
deriving	O
the	O
class	O
posterior	O
is	O
to	O
directly	O
fit	O
pyx	O
w	O
catywx	O
for	O
some	O
c	O
d	O
weight	O
matrix	O
w	O
this	O
is	O
called	O
multi-class	B
logistic	B
regression	B
or	O
multinomial	B
logistic	B
we	O
will	O
discuss	O
this	O
model	O
in	O
detail	O
in	O
section	O
the	O
difference	O
between	O
the	O
two	O
approaches	O
is	O
explained	O
in	O
section	O
t	O
c	O
x	O
c	O
t	O
two-class	O
lda	B
to	O
gain	O
further	O
insight	O
into	O
the	O
meaning	O
of	O
these	O
equations	O
let	O
us	O
consider	O
the	O
binary	O
case	O
in	O
this	O
case	O
the	O
posterior	O
is	O
given	O
by	O
e	O
t	O
py	O
x	O
x	O
e	O
t	O
e	O
t	O
x	O
x	O
e	O
x	O
sigm	O
the	O
abbreviation	O
lda	B
could	O
either	O
stand	O
for	O
linear	B
discriminant	B
analysis	I
or	O
latent	B
dirichlet	B
allocation	I
we	O
hope	O
the	O
meaning	O
is	O
clear	O
from	O
text	O
in	O
the	O
language	B
modeling	I
community	O
this	O
model	O
is	O
called	O
a	O
maximum	B
entropy	B
model	O
for	O
reasons	O
explained	O
in	O
section	O
gaussian	B
discriminant	B
analysis	I
linear	O
boundary	O
all	O
linear	O
boundaries	O
figure	O
discrimanalysisdboundariesdemo	O
linear	O
decision	B
boundaries	O
in	O
for	O
the	O
and	O
class	O
case	O
figure	O
generated	O
by	O
figure	O
geometry	O
of	O
lda	B
in	O
the	O
class	O
case	O
where	O
i	O
where	O
sigm	O
refers	O
to	O
the	O
sigmoid	B
function	O
now	O
so	O
if	O
we	O
define	O
log	O
t	O
t	O
log	O
w	O
log	O
chapter	O
gaussian	B
models	O
then	O
we	O
have	O
wt	O
and	O
hence	O
py	O
sigmwt	O
which	O
is	O
half	O
way	O
between	O
the	O
means	O
is	O
closely	O
related	O
to	O
logistic	B
regression	B
which	O
we	O
will	O
discuss	O
in	O
section	O
so	O
the	O
final	O
decision	B
rule	I
is	O
as	O
follows	O
shift	O
x	O
by	O
project	O
onto	O
the	O
line	O
w	O
and	O
see	O
if	O
the	O
result	O
is	O
positive	O
or	O
negative	O
if	O
then	O
w	O
is	O
in	O
the	O
direction	O
of	O
so	O
we	O
classify	O
the	O
point	O
based	O
on	O
whether	O
its	O
projection	B
is	O
closer	O
to	O
or	O
this	O
is	O
illustrated	O
in	O
figure	O
furthemore	O
if	O
then	O
if	O
we	O
make	O
then	O
gets	O
closer	O
to	O
so	O
more	O
of	O
the	O
line	O
belongs	O
to	O
class	O
a	O
priori	O
conversely	O
if	O
the	O
boundary	O
shifts	O
right	O
thus	O
we	O
see	O
that	O
the	O
class	O
prior	O
c	O
just	O
changes	O
the	O
decision	B
threshold	O
and	O
not	O
the	O
overall	O
geometry	O
as	O
we	O
claimed	O
above	O
similar	B
argument	O
applies	O
in	O
the	O
multi-class	O
case	O
the	O
magnitude	O
of	O
w	O
determines	O
the	O
steepness	O
of	O
the	O
logistic	B
function	O
and	O
depends	O
on	O
how	O
well-separated	O
the	O
means	O
are	O
relative	O
to	O
the	O
variance	B
in	O
psychology	O
and	O
signal	B
detection	I
theory	I
it	O
is	O
common	O
to	O
define	O
the	O
discriminability	B
of	O
a	O
signal	O
from	O
the	O
background	O
noise	O
using	O
a	O
quantity	O
called	O
d-prime	B
where	O
is	O
the	O
mean	B
of	O
the	O
signal	O
and	O
is	O
the	O
mean	B
of	O
the	O
noise	O
and	O
is	O
the	O
standard	B
deviation	I
of	O
the	O
noise	O
if	O
is	O
large	O
the	O
signal	O
will	O
be	O
easier	O
to	O
discriminate	O
from	O
the	O
noise	O
mle	B
for	O
discriminant	B
analysis	I
we	O
now	O
discuss	O
how	O
to	O
fit	O
a	O
discriminant	B
analysis	I
model	O
the	O
simplest	O
way	O
is	O
to	O
use	O
maximum	O
likelihood	B
the	O
log-likelihood	O
function	O
is	O
as	O
follows	O
log	O
pd	O
iyi	O
c	O
log	O
c	O
iyic	O
log	O
n	O
c	O
c	O
we	O
see	O
that	O
this	O
factorizes	O
into	O
a	O
term	O
for	O
and	O
c	O
terms	O
for	O
each	O
c	O
and	O
c	O
hence	O
we	O
can	O
estimate	O
these	O
parameters	O
separately	O
for	O
the	O
class	O
prior	O
we	O
have	O
c	O
nc	O
n	O
as	O
with	O
naive	O
bayes	O
for	O
the	O
class-conditional	O
densities	O
we	O
just	O
partition	O
the	O
data	O
based	O
on	O
its	O
class	O
label	B
and	O
compute	O
the	O
mle	B
for	O
each	O
gaussian	B
cxi	O
ct	O
iyic	O
c	O
nc	O
xi	O
c	O
nc	O
iyic	O
see	O
discrimanalysisfit	O
for	O
a	O
matlab	O
implementation	O
once	O
the	O
model	O
has	O
been	O
fit	O
you	O
can	O
make	O
predictions	O
using	O
discrimanalysispredict	O
which	O
uses	O
a	O
plug-in	B
approximation	I
strategies	O
for	O
preventing	O
overfitting	O
the	O
speed	O
and	O
simplicity	O
of	O
the	O
mle	B
method	O
is	O
one	O
of	O
its	O
greatest	O
appeals	O
however	O
the	O
mle	B
can	O
badly	O
overfit	O
in	O
high	O
dimensions	O
in	O
particular	O
the	O
mle	B
for	O
a	O
full	B
covariance	B
matrix	I
is	O
singular	O
if	O
nc	O
d	O
and	O
even	O
when	O
nc	O
d	O
the	O
mle	B
can	O
be	O
ill-conditioned	B
meaning	O
it	O
is	O
close	O
to	O
singular	O
there	O
are	O
several	O
possible	O
solutions	O
to	O
this	O
problem	O
gaussian	B
discriminant	B
analysis	I
use	O
a	O
diagonal	B
covariance	B
matrix	I
for	O
each	O
class	O
which	O
assumes	O
the	O
features	B
are	O
conditionally	B
independent	I
this	O
is	O
equivalent	O
to	O
using	O
a	O
naive	O
bayes	O
classifier	O
use	O
a	O
full	B
covariance	B
matrix	I
but	O
force	O
it	O
to	O
be	O
the	O
same	O
for	O
all	O
classes	O
c	O
this	O
is	O
an	O
example	O
of	O
parameter	B
tying	I
or	O
parameter	B
sharing	I
and	O
is	O
equivalent	O
to	O
lda	B
use	O
a	O
diagonal	B
covariance	B
matrix	I
and	O
forced	O
it	O
to	O
be	O
shared	B
this	O
is	O
called	O
diagonal	B
covariance	B
lda	B
and	O
is	O
discussed	O
in	O
section	O
use	O
a	O
full	B
covariance	B
matrix	I
but	O
impose	O
a	O
prior	O
and	O
then	O
integrate	O
it	O
out	O
if	O
we	O
use	O
a	O
conjugate	B
prior	I
this	O
can	O
be	O
done	O
in	O
closed	O
form	O
using	O
the	O
results	O
from	O
section	O
this	O
is	O
analogous	O
to	O
the	O
bayesian	B
naive	O
bayes	O
method	O
in	O
section	O
see	O
for	O
details	O
fit	O
a	O
full	B
or	O
diagonal	B
covariance	B
matrix	I
by	O
map	O
estimation	O
we	O
discuss	O
two	O
different	O
kinds	O
of	O
prior	O
below	O
project	O
the	O
data	O
into	O
a	O
low	O
dimensional	O
subspace	O
and	O
fit	O
the	O
gaussians	O
there	O
see	O
sec	O
tion	O
for	O
a	O
way	O
to	O
find	O
the	O
best	O
discriminative	B
linear	O
projection	B
we	O
discuss	O
some	O
of	O
these	O
options	O
below	O
regularized	O
lda	B
suppose	O
we	O
tie	O
the	O
covariance	B
matrices	O
so	O
c	O
as	O
in	O
lda	B
and	O
furthermore	O
we	O
perform	O
map	O
estimation	O
of	O
using	O
an	O
inverse	B
wishart	B
prior	O
of	O
the	O
form	O
iwdiag	O
mle	B
section	O
then	O
we	O
have	O
diag	O
mle	B
mle	B
when	O
we	O
evaluate	O
the	O
class	O
conditional	O
densities	O
we	O
need	O
to	O
compute	O
where	O
controls	O
the	O
amount	O
of	O
regularization	B
which	O
is	O
related	O
to	O
the	O
strength	O
of	O
the	O
prior	O
section	O
for	O
details	O
this	O
technique	O
is	O
known	O
as	O
regularized	B
discriminant	B
analysis	I
or	O
rda	O
et	O
al	O
mle	B
which	O
is	O
impossible	O
to	O
compute	O
if	O
d	O
n	O
however	O
we	O
can	O
use	O
the	O
svd	B
of	O
x	O
to	O
get	O
around	O
this	O
as	O
we	O
show	O
below	O
that	O
this	O
trick	O
cannot	O
be	O
applied	O
to	O
qda	O
which	O
is	O
a	O
nonlinear	O
function	O
of	O
x	O
let	O
x	O
udvt	O
be	O
the	O
svd	B
of	O
the	O
design	B
matrix	I
where	O
v	O
is	O
d	O
n	O
u	O
is	O
an	O
n	O
n	O
orthogonal	O
matrix	O
and	O
d	O
is	O
a	O
diagonal	B
matrix	O
of	O
size	O
n	O
furthermore	O
define	O
the	O
n	O
n	O
matrix	O
z	O
ud	O
this	O
is	O
like	O
a	O
design	B
matrix	I
in	O
a	O
lower	O
dimensional	O
space	O
we	O
assume	O
n	O
d	O
also	O
define	O
z	O
vt	O
as	O
the	O
mean	B
of	O
the	O
data	O
in	O
this	O
reduced	O
space	O
we	O
can	O
recover	O
the	O
original	O
mean	B
using	O
v	O
z	O
since	O
vt	O
v	O
vvt	O
i	O
with	O
these	O
definitions	O
we	O
can	O
and	O
hence	O
chapter	O
gaussian	B
models	O
rewrite	O
the	O
mle	B
as	O
follows	O
mle	B
n	O
n	O
n	O
v	O
xt	O
x	O
t	O
zv	O
zt	O
vzt	O
zvt	O
v	O
z	O
t	O
n	O
zt	O
z	O
z	O
t	O
z	O
z	O
vt	O
v	O
zvt	O
where	O
z	O
is	O
the	O
empirical	O
covariance	B
of	O
z	O
hence	O
we	O
can	O
rewrite	O
the	O
map	B
estimate	I
as	O
map	O
v	O
zvt	O
z	O
diag	O
z	O
z	O
note	O
however	O
that	O
we	O
never	O
need	O
to	O
actually	O
compute	O
the	O
d	O
d	O
matrix	O
map	O
this	O
is	O
because	O
equation	O
tells	O
us	O
that	O
to	O
classify	O
using	O
lda	B
all	O
we	O
need	O
to	O
compute	O
is	O
py	O
cx	O
exp	O
c	O
where	O
c	O
xt	O
c	O
c	O
c	O
we	O
can	O
compute	O
the	O
crucial	O
c	O
term	O
for	O
rda	O
without	O
inverting	O
the	O
d	O
d	O
matrix	O
as	O
follows	O
c	O
c	O
log	O
c	O
c	O
c	O
t	O
c	O
map	O
c	O
zvt	O
c	O
v	O
z	O
vt	O
c	O
v	O
z	O
zc	O
where	O
zc	O
vt	O
c	O
is	O
the	O
mean	B
of	O
the	O
z	O
matrix	O
for	O
data	O
belonging	O
to	O
class	O
c	O
see	O
rdafit	O
for	O
the	O
code	O
diagonal	B
lda	B
a	O
simple	O
alternative	O
to	O
rda	O
is	O
to	O
tie	O
the	O
covariance	B
matrices	O
so	O
c	O
as	O
in	O
lda	B
and	O
then	O
to	O
use	O
a	O
diagonal	B
covariance	B
matrix	I
for	O
each	O
class	O
this	O
is	O
called	O
the	O
diagonal	B
lda	B
model	O
and	O
is	O
equivalent	O
to	O
rda	O
with	O
the	O
corresponding	O
discriminant	B
function	I
is	O
as	O
follows	O
to	O
equation	O
cx	O
log	O
px	O
y	O
c	O
typically	O
we	O
set	O
cj	O
xcj	O
and	O
across	O
classes	O
defined	O
by	O
j	O
log	O
c	O
j	O
j	O
which	O
is	O
the	O
pooled	B
empirical	I
variance	B
of	O
feature	O
j	O
j	O
iyicxij	O
n	O
c	O
in	O
high	O
dimensional	O
settings	O
this	O
model	O
can	O
work	O
much	O
better	O
than	O
lda	B
and	O
rda	O
and	O
levina	O
gaussian	B
discriminant	B
analysis	I
number	O
of	O
genes	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
s	O
m	O
i	O
l	O
test	O
train	O
cv	B
figure	O
error	O
versus	O
amount	O
of	O
shrinkage	B
for	O
nearest	O
shrunken	O
centroid	B
classifier	O
applied	O
to	O
the	O
srbct	O
gene	O
expression	O
data	O
figure	O
generated	O
by	O
shrunkencentroidssrbctdemo	O
based	O
on	O
figure	O
of	O
et	O
al	O
nearest	B
shrunken	I
centroids	B
classifier	O
one	O
drawback	O
of	O
diagonal	B
lda	B
is	O
that	O
it	O
depends	O
on	O
all	O
of	O
the	O
features	B
in	O
high	O
dimensional	O
problems	O
we	O
might	O
prefer	O
a	O
method	O
that	O
only	O
depends	O
on	O
a	O
subset	O
of	O
the	O
features	B
for	O
reasons	O
of	O
accuracy	O
and	O
interpretability	O
one	O
approach	O
is	O
to	O
use	O
a	O
screening	B
method	O
perhaps	O
based	O
on	O
mutual	B
information	B
as	O
in	O
section	O
we	O
now	O
discuss	O
another	O
approach	O
to	O
this	O
problem	O
known	O
as	O
the	O
nearest	B
shrunken	I
centroids	B
classifier	O
et	O
al	O
the	O
basic	O
idea	O
is	O
to	O
perform	O
map	O
estimation	O
for	O
diagonal	B
lda	B
with	O
a	O
sparsity-promoting	B
prior	I
section	O
more	O
precisely	O
define	O
the	O
class-specific	O
feature	O
mean	B
cj	O
in	O
terms	O
of	O
the	O
class-independent	O
feature	O
mean	B
mj	O
and	O
a	O
class-specific	O
offset	O
cj	O
thus	O
we	O
have	O
cj	O
mj	O
cj	O
we	O
will	O
then	O
put	O
a	O
prior	O
on	O
the	O
cj	O
terms	O
to	O
encourage	O
them	O
to	O
be	O
strictly	O
zero	O
and	O
compute	O
a	O
map	B
estimate	I
if	O
for	O
feature	O
j	O
we	O
find	O
that	O
cj	O
for	O
all	O
c	O
then	O
feature	O
j	O
will	O
play	O
no	O
role	O
in	O
the	O
classification	O
decision	B
cj	O
will	O
be	O
independent	O
of	O
c	O
thus	O
features	B
that	O
are	O
not	O
discriminative	B
are	O
automatically	O
ignored	O
the	O
details	O
can	O
be	O
found	O
in	O
et	O
al	O
and	O
and	O
park	O
see	O
shrunkencentroidsfit	O
for	O
some	O
code	O
let	O
us	O
give	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
based	O
on	O
et	O
al	O
consider	O
the	O
problem	O
of	O
classifying	O
a	O
gene	O
expression	O
dataset	O
which	O
genes	O
classes	O
training	O
samples	B
and	O
test	O
samples	B
using	O
a	O
diagonal	B
lda	B
classifier	O
produces	O
errors	O
on	O
the	O
test	O
set	O
using	O
the	O
nearest	B
shrunken	I
centroids	B
classifier	O
produced	O
errors	O
on	O
the	O
test	O
set	O
for	O
a	O
range	O
of	O
values	O
see	O
figure	O
more	O
importantly	O
the	O
model	O
is	O
sparse	B
and	O
hence	O
more	O
interpretable	O
figure	O
plots	O
an	O
unpenalized	O
estimate	O
of	O
the	O
difference	O
dcj	O
in	O
gray	O
as	O
well	O
as	O
the	O
shrunken	O
estimates	O
are	O
computed	O
using	O
the	O
value	O
of	O
estimated	O
by	O
cv	B
estimates	O
cj	O
in	O
blue	O
we	O
see	O
that	O
only	O
genes	O
are	O
used	O
out	O
of	O
the	O
original	O
now	O
consider	O
an	O
even	O
harder	O
problem	O
with	O
genes	O
a	O
training	B
set	I
of	O
patients	O
a	O
test	O
set	O
of	O
patients	O
and	O
different	O
types	O
of	O
cancer	O
et	O
al	O
hastie	O
et	O
al	O
et	O
al	O
report	O
that	O
nearest	B
shrunken	I
centroids	B
produced	O
errors	O
on	O
the	O
test	O
class	O
class	O
chapter	O
gaussian	B
models	O
class	O
class	O
figure	O
profile	O
of	O
ure	O
this	O
selects	O
genes	O
based	O
on	O
figure	O
of	O
et	O
al	O
shrunkencentroidssrbctdemo	O
the	O
shrunken	O
centroids	B
corresponding	O
to	O
optimal	O
in	O
figfigure	O
generated	O
by	O
set	O
using	O
genes	O
and	O
that	O
rda	O
produced	O
errors	O
on	O
the	O
test	O
set	O
using	O
all	O
genes	O
the	O
pmtk	O
function	O
cancerhighdimclassifdemo	O
can	O
be	O
used	O
to	O
reproduce	O
these	O
numbers	O
inference	B
in	O
jointly	O
gaussian	B
distributions	O
given	O
a	O
joint	B
distribution	I
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
compute	O
marginals	O
and	O
conditionals	O
we	O
discuss	O
how	O
to	O
do	O
this	O
below	O
and	O
then	O
give	O
some	O
applications	O
these	O
operations	O
take	O
time	O
in	O
the	O
worst	O
case	O
see	O
section	O
for	O
faster	O
methods	O
inference	B
in	O
jointly	O
gaussian	B
distributions	O
statement	O
of	O
the	O
result	O
theorem	O
and	O
conditionals	O
of	O
an	O
mvn	B
suppose	O
x	O
is	O
jointly	O
gaussian	B
with	O
parameters	O
then	O
the	O
marginals	O
are	O
given	O
by	O
and	O
the	O
posterior	O
conditional	O
is	O
given	O
by	O
n	O
equation	O
is	O
of	O
such	O
crucial	O
importance	O
in	O
this	O
book	O
that	O
we	O
have	O
put	O
a	O
box	O
around	O
it	O
so	O
you	O
can	O
easily	O
find	O
it	O
for	O
the	O
proof	O
see	O
section	O
we	O
see	O
that	O
both	O
the	O
marginal	O
and	O
conditional	O
distributions	O
are	O
themselves	O
gaussian	B
for	O
the	O
marginals	O
we	O
just	O
extract	O
the	O
rows	O
and	O
columns	O
corresponding	O
to	O
or	O
for	O
the	O
conditional	O
we	O
have	O
to	O
do	O
a	O
bit	O
more	O
work	O
however	O
it	O
is	O
not	O
that	O
complicated	O
the	O
conditional	O
mean	B
is	O
just	O
a	O
linear	O
function	O
of	O
and	O
the	O
conditional	O
covariance	B
is	O
just	O
a	O
constant	O
matrix	O
that	O
is	O
independent	O
of	O
we	O
give	O
three	O
different	O
equivalent	O
expressions	O
for	O
the	O
posterior	B
mean	B
and	O
two	O
different	O
equivalent	O
expressions	O
for	O
the	O
posterior	O
covariance	B
each	O
one	O
is	O
useful	O
in	O
different	O
circumstances	O
examples	O
below	O
we	O
give	O
some	O
examples	O
of	O
these	O
equations	O
in	O
action	B
which	O
will	O
make	O
them	O
seem	O
more	O
intuitive	O
marginals	O
and	O
conditionals	O
of	O
a	O
gaussian	B
let	O
us	O
consider	O
a	O
example	O
the	O
covariance	B
matrix	I
is	O
the	O
marginal	O
is	O
a	O
gaussian	B
obtained	O
by	O
projecting	O
the	O
joint	B
distribution	I
onto	O
the	O
line	O
n	O
chapter	O
gaussian	B
models	O
x	O
x	O
x	O
figure	O
a	O
joint	O
gaussian	B
distribution	O
with	O
a	O
correlation	B
coefficient	I
of	O
we	O
plot	O
the	O
contour	O
and	O
the	O
principal	O
axes	O
the	O
unconditional	O
marginal	O
the	O
conditional	O
n	O
obtained	O
by	O
slicing	O
at	O
height	O
figure	O
generated	O
by	O
if	O
we	O
get	O
suppose	O
we	O
observe	O
the	O
conditional	O
is	O
obtained	O
by	O
slicing	O
the	O
joint	B
distribution	I
through	O
the	O
line	O
figure	O
in	O
figure	O
we	O
show	O
an	O
example	O
where	O
and	O
we	O
see	O
that	O
e	O
which	O
makes	O
sense	O
since	O
means	O
that	O
we	O
believe	O
that	O
if	O
increases	O
by	O
its	O
mean	B
then	O
increases	O
by	O
we	O
also	O
see	O
var	B
this	O
also	O
makes	O
sense	O
our	O
uncertainty	B
about	O
has	O
gone	O
down	O
since	O
we	O
n	O
if	O
we	O
get	O
have	O
learned	O
something	O
about	O
by	O
observing	O
since	O
conveys	O
no	O
information	B
about	O
if	O
they	O
are	O
uncorrelated	O
hence	O
independent	O
interpolating	O
noise-free	O
data	O
suppose	O
we	O
want	O
to	O
estimate	O
a	O
function	O
defined	O
on	O
the	O
interval	O
t	O
such	O
that	O
yi	O
f	O
for	O
n	O
observed	O
points	O
ti	O
we	O
assume	O
for	O
now	O
that	O
the	O
data	O
is	O
noise-free	O
so	O
we	O
want	O
to	O
interpolate	B
it	O
that	O
is	O
fit	O
a	O
function	O
that	O
goes	O
exactly	O
through	O
the	O
data	O
section	O
for	O
the	O
noisy	O
data	O
case	O
the	O
question	O
is	O
how	O
does	O
the	O
function	O
behave	O
in	O
between	O
the	O
observed	O
data	O
points	O
it	O
is	O
often	O
reasonable	O
to	O
assume	O
that	O
the	O
unknown	B
function	O
is	O
smooth	O
in	O
chapter	O
we	O
shall	O
see	O
how	O
to	O
encode	O
priors	O
over	O
functions	O
and	O
how	O
to	O
update	O
such	O
a	O
prior	O
with	O
observed	O
values	O
to	O
get	O
a	O
posterior	O
over	O
functions	O
but	O
in	O
this	O
section	O
we	O
take	O
a	O
simpler	O
approach	O
which	O
is	O
adequate	O
for	O
map	O
estimation	O
of	O
functions	O
defined	O
on	O
inputs	O
we	O
follow	O
the	O
presentation	O
of	O
and	O
somersalo	O
we	O
start	O
by	O
discretizing	O
the	O
problem	O
first	O
we	O
divide	O
the	O
support	B
of	O
the	O
function	O
into	O
d	O
equal	O
subintervals	O
we	O
then	O
define	O
xj	O
f	O
sj	O
jh	O
h	O
j	O
d	O
t	O
d	O
inference	B
in	O
jointly	O
gaussian	B
distributions	O
figure	O
interpolating	O
noise-free	O
data	O
using	O
a	O
gaussian	B
with	O
prior	O
precision	B
see	O
also	O
figure	O
based	O
on	O
figure	O
of	O
and	O
somersalo	O
figure	O
generated	O
by	O
gaussinterpdemo	O
we	O
can	O
encode	O
our	O
smoothness	O
prior	O
by	O
assuming	O
that	O
xj	O
is	O
an	O
average	O
of	O
its	O
neighbors	B
xj	O
and	O
plus	O
some	O
gaussian	B
noise	O
j	O
j	O
d	O
xj	O
where	O
n	O
the	O
precision	B
term	O
controls	O
how	O
much	O
we	O
think	O
the	O
function	O
will	O
vary	O
a	O
large	O
corresponds	O
to	O
a	O
belief	O
that	O
the	O
function	O
is	O
very	O
smooth	O
a	O
small	O
corresponds	O
to	O
a	O
belief	O
that	O
the	O
function	O
is	O
quite	O
wiggly	O
in	O
vector	O
form	O
the	O
above	O
equation	O
can	O
be	O
written	O
as	O
follows	O
where	O
l	O
is	O
the	O
d	O
second	B
order	I
finite	O
difference	O
matrix	O
lx	O
l	O
the	O
corresponding	O
prior	O
has	O
the	O
form	O
exp	O
px	O
n	O
l	O
we	O
will	O
henceforth	O
assume	O
we	O
have	O
scaled	O
l	O
by	O
so	O
we	O
can	O
ignore	O
the	O
term	O
and	O
just	O
write	O
lt	O
l	O
for	O
the	O
precision	B
matrix	I
note	O
that	O
although	O
x	O
is	O
d-dimensional	O
the	O
precision	B
matrix	I
only	O
has	O
rank	O
d	O
thus	O
this	O
is	O
an	O
improper	B
prior	I
known	O
as	O
an	O
intrinsic	O
gaussian	B
random	O
field	O
section	O
for	O
chapter	O
gaussian	B
models	O
more	O
information	B
however	O
providing	O
we	O
observe	O
n	O
data	O
points	O
the	O
posterior	O
will	O
be	O
proper	O
now	O
let	O
be	O
the	O
n	O
noise-free	O
observations	O
of	O
the	O
function	O
and	O
be	O
the	O
d	O
n	O
unknown	B
function	O
values	O
without	O
loss	B
of	O
generality	O
assume	O
that	O
the	O
unknown	B
variables	O
are	O
ordered	O
first	O
then	O
the	O
known	O
variables	O
then	O
we	O
can	O
partition	O
the	O
l	O
matrix	O
as	O
follows	O
l	O
r	O
n	O
r	O
we	O
can	O
also	O
partition	O
the	O
precision	B
matrix	I
of	O
the	O
joint	B
distribution	I
lt	O
l	O
lt	O
lt	O
lt	O
lt	O
using	O
equation	O
we	O
can	O
write	O
the	O
conditional	O
distribution	O
as	O
follows	O
lt	O
note	O
that	O
we	O
can	O
compute	O
the	O
mean	B
by	O
solving	O
the	O
following	O
system	O
of	O
linear	O
equations	O
this	O
is	O
efficient	O
since	O
is	O
tridiagonal	B
figure	O
gives	O
an	O
illustration	O
of	O
these	O
equations	O
we	O
see	O
that	O
the	O
posterior	B
mean	B
equals	O
the	O
observed	O
data	O
at	O
the	O
specified	O
points	O
and	O
smoothly	O
interpolates	O
in	O
between	O
as	O
desired	O
it	O
is	O
also	O
interesting	O
to	O
plot	O
the	O
pointwise	B
marginal	I
credibility	I
intervals	I
j	O
shown	O
in	O
grey	O
we	O
see	O
that	O
the	O
variance	B
goes	O
up	O
as	O
we	O
move	O
away	O
from	O
the	O
data	O
we	O
also	O
see	O
that	O
the	O
variance	B
goes	O
up	O
as	O
we	O
decrease	O
the	O
precision	B
of	O
the	O
prior	O
interestingly	O
has	O
no	O
effect	O
on	O
the	O
posterior	B
mean	B
since	O
it	O
cancels	O
out	O
when	O
multiplying	O
and	O
by	O
contrast	O
when	O
we	O
consider	O
noisy	O
data	O
in	O
section	O
we	O
will	O
see	O
that	O
the	O
prior	O
precision	B
affects	O
the	O
smoothness	O
of	O
posterior	B
mean	B
estimate	O
the	O
marginal	O
credibility	O
intervals	O
do	O
not	O
capture	O
the	O
fact	O
that	O
neighboring	O
locations	O
are	O
correlated	O
we	O
can	O
represent	O
that	O
by	O
drawing	O
complete	B
functions	O
vectors	O
x	O
from	O
the	O
posterior	O
and	O
plotting	O
them	O
these	O
are	O
shown	O
by	O
the	O
thin	O
lines	O
in	O
figure	O
these	O
are	O
not	O
quite	O
as	O
smooth	O
as	O
the	O
posterior	B
mean	B
itself	O
this	O
is	O
because	O
the	O
prior	O
only	O
penalizes	O
first-order	O
differences	O
see	O
section	O
for	O
further	O
discussion	O
of	O
this	O
point	O
data	O
imputation	B
suppose	O
we	O
are	O
missing	B
some	O
entries	O
in	O
a	O
design	B
matrix	I
if	O
the	O
columns	O
are	O
correlated	O
we	O
can	O
use	O
the	O
observed	O
entries	O
to	O
predict	O
the	O
missing	B
entries	O
figure	O
shows	O
a	O
simple	O
example	O
we	O
sampled	O
some	O
data	O
from	O
a	O
dimensional	O
gaussian	B
and	O
then	O
deliberately	O
hid	O
of	O
the	O
data	O
in	O
each	O
row	O
we	O
then	O
inferred	O
the	O
missing	B
entries	O
given	O
the	O
observed	O
entries	O
using	O
the	O
true	O
model	O
more	O
precisely	O
for	O
each	O
row	O
i	O
we	O
compute	O
pxhi	O
where	O
hi	O
and	O
vi	O
are	O
the	O
indices	O
of	O
the	O
hidden	B
and	O
visible	B
entries	O
in	O
case	O
i	O
from	O
this	O
we	O
compute	O
the	O
marginal	O
we	O
then	O
plot	O
the	O
mean	B
of	O
this	O
distribution	O
distribution	O
of	O
each	O
missing	B
variable	O
pxhij	O
xij	O
e	O
this	O
represents	O
our	O
best	O
guess	O
about	O
the	O
true	O
value	O
of	O
that	O
entry	O
in	O
the	O
inference	B
in	O
jointly	O
gaussian	B
distributions	O
observed	O
imputed	O
truth	O
figure	O
illustration	O
of	O
data	O
imputation	B
left	O
column	O
visualization	O
of	O
three	O
rows	O
of	O
the	O
data	O
matrix	O
with	O
missing	B
entries	O
middle	O
column	O
mean	B
of	O
the	O
posterior	O
predictive	B
based	O
on	O
partially	O
observed	O
data	O
in	O
that	O
row	O
but	O
the	O
true	O
model	O
parameters	O
right	O
column	O
figure	O
generated	O
by	O
gaussimputationdemo	O
true	O
values	O
xhijxvi	O
sense	O
that	O
it	O
minimizes	O
our	O
expected	O
squared	B
error	I
section	O
for	O
details	O
figure	O
shows	O
that	O
the	O
estimates	O
are	O
quite	O
close	O
to	O
the	O
truth	O
course	O
if	O
j	O
vi	O
the	O
expected	B
value	I
is	O
equal	O
to	O
the	O
observed	O
value	O
xij	O
xij	O
we	O
can	O
use	O
var	B
as	O
a	O
measure	O
of	O
confidence	O
in	O
this	O
guess	O
although	O
this	O
is	O
not	O
this	O
is	O
called	O
multiple	O
shown	O
alternatively	O
we	O
could	O
draw	O
multiple	O
samples	B
from	O
pxhi	O
imputation	B
in	O
addition	O
to	O
imputing	O
the	O
missing	B
entries	O
we	O
may	O
be	O
interested	O
in	O
computing	O
the	O
likelihood	B
of	O
each	O
partially	O
observed	O
row	O
in	O
the	O
table	O
pxvi	O
which	O
can	O
be	O
computed	O
using	O
equation	O
this	O
is	O
useful	O
for	O
detecting	O
outliers	B
observations	O
information	B
form	I
suppose	O
x	O
n	O
one	O
can	O
show	O
that	O
e	O
is	O
the	O
mean	B
vector	O
and	O
cov	O
is	O
the	O
covariance	B
matrix	I
these	O
are	O
called	O
the	O
moment	B
parameters	I
of	O
the	O
distribution	O
however	O
it	O
is	O
sometimes	O
useful	O
to	O
use	O
the	O
canonical	B
parameters	I
or	O
natural	B
parameters	I
defined	O
as	O
we	O
can	O
convert	O
back	O
to	O
the	O
moment	B
parameters	I
using	O
using	O
the	O
canonical	B
parameters	I
we	O
can	O
write	O
the	O
mvn	B
in	O
information	B
form	I
in	O
exponential	B
family	B
form	O
defined	O
in	O
section	O
x	O
t	O
ncx	O
exp	O
where	O
we	O
use	O
the	O
notation	O
nc	O
to	O
distinguish	O
from	O
the	O
moment	O
parameterization	O
n	O
it	O
is	O
also	O
possible	O
to	O
derive	O
the	O
marginalization	O
and	O
conditioning	B
formulas	O
in	O
information	B
form	I
we	O
find	O
chapter	O
gaussian	B
models	O
thus	O
we	O
see	O
that	O
marginalization	O
is	O
easier	O
in	O
moment	O
form	O
and	O
conditioning	B
is	O
easier	O
in	O
information	B
form	I
another	O
operation	O
that	O
is	O
significantly	O
easier	O
in	O
information	B
form	I
is	O
multiplying	O
two	O
gaussians	O
one	O
can	O
show	O
that	O
nc	O
f	O
f	O
g	O
g	O
c	O
f	O
g	O
f	O
g	O
however	O
in	O
moment	O
form	O
things	O
are	O
much	O
messier	O
n	O
f	O
f	O
g	O
g	O
n	O
g	O
g	O
f	O
f	O
g	O
g	O
f	O
g	O
g	O
g	O
proof	O
of	O
the	O
result	O
we	O
now	O
prove	O
theorem	O
readers	O
who	O
are	O
intimidated	O
by	O
heavy	O
matrix	O
algebra	O
can	O
safely	O
skip	O
this	O
section	O
we	O
first	O
derive	O
some	O
results	O
that	O
we	O
will	O
need	O
here	O
and	O
elsewhere	O
in	O
the	O
book	O
we	O
will	O
return	O
to	O
the	O
proof	O
at	O
the	O
end	O
inverse	O
of	O
a	O
partitioned	O
matrix	O
using	O
schur	O
complements	O
the	O
key	O
tool	O
we	O
need	O
is	O
a	O
way	O
to	O
invert	O
a	O
partitioned	O
matrix	O
this	O
can	O
be	O
done	O
using	O
the	O
following	O
result	O
theorem	O
of	O
a	O
partitioned	O
matrix	O
consider	O
a	O
general	O
partitioned	O
matrix	O
m	O
e	O
f	O
g	O
h	O
where	O
we	O
assume	O
e	O
and	O
h	O
are	O
invertible	O
we	O
have	O
m	O
h	O
e	O
e	O
h	O
h	O
e	O
where	O
mh	B
e	O
fh	O
me	O
h	O
ge	O
we	O
say	O
that	O
mh	B
is	O
the	O
schur	B
complement	I
of	O
m	O
wrt	O
h	O
equation	O
is	O
called	O
the	O
partitioned	B
inverse	I
formula	I
proof	O
if	O
we	O
could	O
block	O
diagonalize	O
m	O
it	O
would	O
be	O
easier	O
to	O
invert	O
to	O
zero	O
out	O
the	O
top	O
right	O
block	O
of	O
m	O
we	O
can	O
pre-multiply	O
as	O
follows	O
i	O
fh	O
i	O
e	O
f	O
g	O
h	O
e	O
fh	O
h	O
g	O
similarly	O
to	O
zero	O
out	O
the	O
bottom	O
left	O
we	O
can	O
post-multiply	O
as	O
follows	O
inference	B
in	O
jointly	O
gaussian	B
distributions	O
e	O
fh	O
h	O
g	O
putting	O
it	O
all	O
together	O
we	O
get	O
i	O
fh	O
i	O
e	O
f	O
g	O
h	O
i	O
h	O
i	O
i	O
h	O
i	O
e	O
fh	O
h	O
e	O
fh	O
h	O
x	O
m	O
z	O
w	O
taking	O
the	O
inverse	O
of	O
both	O
sides	O
yields	O
z	O
w	O
and	O
hence	O
substituting	O
in	O
the	O
definitions	O
we	O
get	O
e	O
f	O
g	O
h	O
m	O
zw	O
h	O
h	O
h	O
i	O
i	O
h	O
i	O
fh	O
i	O
h	O
i	O
fh	O
i	O
alternatively	O
we	O
could	O
have	O
decomposed	O
the	O
matrix	O
m	O
in	O
terms	O
of	O
e	O
and	O
me	O
ge	O
yielding	O
h	O
h	O
e	O
e	O
e	O
e	O
f	O
g	O
h	O
e	O
f	O
g	O
h	O
the	O
matrix	B
inversion	I
lemma	I
we	O
now	O
derive	O
some	O
useful	O
corollaries	O
of	O
the	O
above	O
result	O
corollary	O
inversion	O
lemma	O
consider	O
a	O
general	O
partitioned	O
matrix	O
m	O
where	O
we	O
assume	O
e	O
and	O
h	O
are	O
invertible	O
we	O
have	O
fh	O
e	O
e	O
ge	O
fh	O
e	O
ge	O
fh	O
ge	O
chapter	O
gaussian	B
models	O
the	O
first	O
two	O
equations	O
are	O
s	O
known	O
as	O
the	O
matrix	B
inversion	I
lemma	I
or	O
the	O
shermanmorrison-woodbury	O
formula	O
the	O
third	O
equation	O
is	O
known	O
as	O
the	O
matrix	B
determinant	I
lemma	I
a	O
typical	O
application	O
in	O
machine	B
learning	B
statistics	O
is	O
the	O
following	O
let	O
e	O
be	O
a	O
n	O
n	O
diagonal	B
matrix	O
let	O
f	O
gt	O
x	O
of	O
size	O
n	O
d	O
where	O
n	O
d	O
and	O
let	O
h	O
i	O
then	O
we	O
have	O
xxt	O
xt	O
the	O
lhs	O
takes	O
on	O
time	O
to	O
compute	O
the	O
rhs	O
takes	O
time	O
to	O
compute	O
another	O
application	O
concerns	O
computing	O
a	O
rank	B
one	I
update	I
of	O
an	O
inverse	O
matrix	O
h	O
scalar	O
f	O
u	O
column	O
vector	O
and	O
g	O
vt	O
row	O
vector	O
then	O
we	O
have	O
let	O
uvt	O
e	O
e	O
vt	O
e	O
e	O
e	O
e	O
vt	O
e	O
e	O
this	O
is	O
useful	O
when	O
we	O
incrementally	O
add	O
a	O
data	O
vector	O
to	O
a	O
design	B
matrix	I
and	O
want	O
to	O
update	O
our	O
sufficient	B
statistics	I
can	O
derive	O
an	O
analogous	O
formula	O
for	O
removing	O
a	O
data	O
vector	O
proof	O
to	O
prove	O
equation	O
we	O
simply	O
equate	O
the	O
top	O
left	O
block	O
of	O
equation	O
and	O
equation	O
to	O
prove	O
equation	O
we	O
simple	O
equate	O
the	O
top	O
right	O
blocks	O
of	O
equations	O
and	O
the	O
proof	O
of	O
equation	O
is	O
left	O
as	O
an	O
exercise	O
proof	O
of	O
gaussian	B
conditioning	B
formulas	O
we	O
can	O
now	O
return	O
to	O
our	O
original	O
goal	O
which	O
is	O
to	O
derive	O
equation	O
let	O
us	O
factor	B
the	O
joint	O
as	O
as	O
follows	O
e	O
exp	O
e	O
exp	O
using	O
equation	O
the	O
above	O
exponent	O
becomes	O
i	O
i	O
i	O
exp	O
this	O
is	O
of	O
the	O
form	O
expquadratic	O
form	O
in	O
expquadratic	O
form	O
in	O
i	O
exp	O
linear	B
gaussian	B
systems	O
hence	O
we	O
have	O
successfully	O
factorized	O
the	O
joint	O
as	O
n	O
where	O
the	O
parameters	O
of	O
the	O
conditional	O
distribution	O
can	O
be	O
read	O
off	O
from	O
the	O
above	O
equations	O
using	O
we	O
can	O
also	O
use	O
the	O
fact	O
that	O
to	O
check	O
the	O
normalization	O
constants	O
are	O
correct	O
where	O
and	O
we	O
leave	O
the	O
proof	O
of	O
the	O
other	O
forms	O
of	O
the	O
result	O
in	O
equation	O
as	O
an	O
exercise	O
linear	B
gaussian	B
systems	O
suppose	O
we	O
have	O
two	O
variables	O
x	O
and	O
y	O
let	O
x	O
r	O
a	O
noisy	O
observation	B
of	O
x	O
let	O
us	O
assume	O
we	O
have	O
the	O
following	O
prior	O
and	O
likelihood	B
dx	O
be	O
a	O
hidden	B
variable	I
and	O
y	O
r	O
dy	O
be	O
px	O
n	O
x	O
x	O
pyx	O
n	O
b	O
y	O
where	O
a	O
is	O
a	O
matrix	O
of	O
size	O
dy	O
dx	O
this	O
is	O
an	O
example	O
of	O
a	O
linear	B
gaussian	B
system	I
we	O
can	O
represent	O
this	O
schematically	O
as	O
x	O
y	O
meaning	O
x	O
generates	O
y	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
invert	O
the	O
arrow	O
that	O
is	O
how	O
to	O
infer	O
x	O
from	O
y	O
we	O
state	B
the	O
result	O
below	O
then	O
give	O
several	O
examples	O
and	O
finally	O
we	O
derive	O
the	O
result	O
we	O
will	O
see	O
many	O
more	O
applications	O
of	O
these	O
results	O
in	O
later	O
chapters	O
statement	O
of	O
the	O
result	O
theorem	O
rule	O
for	O
linear	B
gaussian	B
systems	O
given	O
a	O
linear	B
gaussian	B
system	I
as	O
in	O
equation	O
the	O
posterior	O
pxy	O
is	O
given	O
by	O
the	O
following	O
pxy	O
n	O
xy	O
xy	O
x	O
at	O
xy	O
y	O
a	O
b	O
xy	O
xyat	O
y	O
x	O
x	O
chapter	O
gaussian	B
models	O
in	O
addition	O
the	O
normalization	O
constant	O
py	O
is	O
given	O
by	O
py	O
n	O
x	O
b	O
y	O
a	O
xat	O
for	O
the	O
proof	O
see	O
section	O
examples	O
in	O
this	O
section	O
we	O
give	O
some	O
example	O
applications	O
of	O
the	O
above	O
result	O
inferring	O
an	O
unknown	B
scalar	O
from	O
noisy	O
measurements	O
suppose	O
we	O
make	O
n	O
noisy	O
measurements	O
yi	O
of	O
some	O
underlying	O
quantity	O
x	O
let	O
us	O
assume	O
the	O
measurement	O
noise	O
has	O
fixed	O
precision	B
y	O
so	O
the	O
likelihood	B
is	O
pyix	O
y	O
now	O
let	O
us	O
use	O
a	O
gaussian	B
prior	O
for	O
the	O
value	O
of	O
the	O
unknown	B
source	O
px	O
we	O
want	O
to	O
compute	O
yn	O
we	O
can	O
convert	O
this	O
to	O
a	O
form	O
that	O
lets	O
us	O
apply	O
n	O
n	O
row	O
vector	O
of	O
s	O
bayes	B
rule	I
for	O
gaussians	O
by	O
defining	O
y	O
yn	O
a	O
and	O
y	O
diag	O
yi	O
then	O
we	O
get	O
pxy	O
n	O
n	O
n	O
n	O
y	O
n	O
n	O
yy	O
n	O
n	O
y	O
n	O
y	O
y	O
n	O
y	O
these	O
equations	O
are	O
quite	O
intuitive	O
the	O
posterior	O
precision	B
n	O
is	O
the	O
prior	O
precision	B
plus	O
n	O
units	O
of	O
measurement	O
precision	B
y	O
also	O
the	O
posterior	B
mean	B
n	O
is	O
a	O
convex	B
combination	I
of	O
the	O
mle	B
y	O
and	O
the	O
prior	O
mean	B
this	O
makes	O
it	O
clear	O
that	O
the	O
posterior	B
mean	B
is	O
a	O
compromise	O
if	O
the	O
prior	O
is	O
weak	O
relative	O
to	O
the	O
signal	O
strength	O
is	O
between	O
the	O
mle	B
and	O
the	O
prior	O
small	O
relative	O
to	O
y	O
we	O
put	O
more	O
weight	O
on	O
the	O
mle	B
if	O
the	O
prior	O
is	O
strong	O
relative	O
to	O
the	O
signal	O
strength	O
is	O
large	O
relative	O
to	O
y	O
we	O
put	O
more	O
weight	O
on	O
the	O
prior	O
this	O
is	O
illustrated	O
in	O
figure	O
which	O
is	O
very	O
similar	B
to	O
the	O
analogous	O
results	O
for	O
the	O
beta-binomial	B
model	O
in	O
figure	O
note	O
that	O
the	O
posterior	B
mean	B
is	O
written	O
in	O
terms	O
of	O
n	O
yy	O
so	O
having	O
n	O
measurements	O
each	O
of	O
precision	B
y	O
is	O
like	O
having	O
one	O
measurement	O
with	O
value	O
y	O
and	O
precision	B
n	O
y	O
we	O
can	O
rewrite	O
the	O
results	O
in	O
terms	O
of	O
the	O
posterior	O
variance	B
rather	O
than	O
posterior	O
precision	B
linear	B
gaussian	B
systems	O
prior	O
variance	B
prior	O
lik	O
post	O
prior	O
variance	B
prior	O
lik	O
post	O
figure	O
inference	B
about	O
x	O
given	O
a	O
noisy	O
observation	B
y	O
strong	O
prior	O
n	O
the	O
posterior	O
weak	O
prior	O
n	O
the	O
posterior	B
mean	B
is	O
mean	B
is	O
shrunk	O
towards	O
the	O
prior	O
mean	B
which	O
is	O
similar	B
to	O
the	O
mle	B
figure	O
generated	O
by	O
as	O
follows	O
pxd	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
y	O
n	O
n	O
n	O
is	O
the	O
posterior	O
variance	B
n	O
n	O
y	O
where	O
is	O
the	O
prior	O
variance	B
and	O
we	O
can	O
also	O
compute	O
the	O
posterior	O
sequentially	O
by	O
updating	O
after	O
each	O
observation	B
if	O
n	O
we	O
can	O
rewrite	O
the	O
posterior	O
after	O
seeing	O
a	O
single	O
observation	B
as	O
follows	O
we	O
define	O
y	O
to	O
be	O
the	O
variances	O
of	O
the	O
likelihood	B
prior	O
and	O
posterior	O
and	O
y	O
y	O
pxy	O
y	O
y	O
y	O
we	O
can	O
rewrite	O
the	O
posterior	B
mean	B
in	O
different	O
ways	O
y	O
y	O
y	O
y	O
y	O
y	O
y	O
y	O
chapter	O
gaussian	B
models	O
the	O
first	O
equation	O
is	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
and	O
the	O
data	O
the	O
second	O
equation	O
is	O
the	O
prior	O
mean	B
adjusted	O
towards	O
the	O
data	O
the	O
third	O
equation	O
is	O
the	O
data	O
adjusted	O
towards	O
the	O
prior	O
mean	B
this	O
is	O
called	O
shrinkage	B
these	O
are	O
all	O
equivalent	O
ways	O
of	O
expressing	O
the	O
tradeoff	O
between	O
likelihood	B
and	O
prior	O
if	O
is	O
small	O
relative	O
to	O
y	O
corresponding	O
to	O
a	O
strong	O
prior	O
the	O
amount	O
of	O
shrinkage	B
is	O
large	O
figure	O
whereas	O
if	O
is	O
large	O
relative	O
to	O
y	O
corresponding	O
to	O
a	O
weak	O
prior	O
the	O
amount	O
of	O
shrinkage	B
is	O
small	O
figure	O
another	O
way	O
to	O
quantify	O
the	O
amount	O
of	O
shrinkage	B
is	O
in	O
terms	O
of	O
the	O
signal-to-noise	B
ratio	I
which	O
is	O
defined	O
as	O
follows	O
snr	O
e	O
x	O
e	O
where	O
x	O
n	O
is	O
the	O
true	O
signal	O
y	O
x	O
is	O
the	O
observed	O
signal	O
and	O
n	O
y	O
is	O
the	O
noise	O
term	O
y	O
inferring	O
an	O
unknown	B
vector	O
from	O
noisy	O
measurements	O
now	O
consider	O
n	O
vector-valued	O
observations	O
yi	O
n	O
y	O
and	O
a	O
gaussian	B
prior	O
x	O
n	O
setting	O
a	O
i	O
b	O
and	O
using	O
y	O
for	O
the	O
effective	O
observation	B
with	O
precision	B
n	O
y	O
we	O
have	O
yn	O
n	O
n	O
n	O
n	O
n	O
n	O
y	O
y	O
y	O
see	O
figure	O
for	O
a	O
example	O
we	O
can	O
think	O
of	O
x	O
as	O
representing	O
the	O
true	O
but	O
unknown	B
location	O
of	O
an	O
object	O
in	O
space	O
such	O
as	O
a	O
missile	O
or	O
airplane	O
and	O
the	O
yi	O
as	O
being	O
noisy	O
observations	O
such	O
as	O
radar	B
blips	O
as	O
we	O
receive	O
more	O
blips	O
we	O
are	O
better	O
able	O
to	O
localize	O
the	O
source	O
in	O
section	O
we	O
will	O
see	O
how	O
to	O
extend	O
this	O
example	O
to	O
track	O
moving	O
objects	O
using	O
the	O
famous	O
kalman	O
filter	O
algorithm	O
now	O
suppose	O
we	O
have	O
multiple	O
measuring	O
devices	O
and	O
we	O
want	O
to	O
combine	O
them	O
together	O
this	O
is	O
known	O
as	O
sensor	B
fusion	I
if	O
we	O
have	O
multiple	O
observations	O
with	O
different	O
covariances	O
to	O
sensors	O
with	O
different	O
reliabilities	O
the	O
posterior	O
will	O
be	O
an	O
appropriate	O
weighted	B
average	I
of	O
the	O
data	O
consider	O
the	O
example	O
in	O
figure	O
we	O
use	O
an	O
uninformative	B
prior	O
on	O
x	O
namely	O
px	O
n	O
n	O
we	O
get	O
noisy	O
observations	O
n	O
and	O
n	O
we	O
then	O
compute	O
in	O
figure	O
we	O
set	O
so	O
both	O
sensors	O
are	O
equally	O
reliable	O
in	O
this	O
case	O
the	O
posterior	B
mean	B
is	O
half	O
way	O
between	O
the	O
two	O
observations	O
and	O
in	O
figure	O
we	O
set	O
and	O
so	O
sensor	O
is	O
more	O
reliable	O
than	O
sensor	O
in	O
this	O
case	O
the	O
posterior	B
mean	B
is	O
closer	O
to	O
in	O
figure	O
we	O
set	O
so	O
sensor	O
is	O
more	O
reliable	O
in	O
the	O
component	O
direction	O
and	O
sensor	O
is	O
more	O
in	O
this	O
case	O
the	O
posterior	B
mean	B
uses	O
s	O
reliable	O
in	O
the	O
component	O
direction	O
vertical	O
component	O
and	O
s	O
horizontal	O
component	O
linear	B
gaussian	B
systems	O
post	O
after	O
obs	O
data	O
prior	O
figure	O
illustration	O
of	O
bayesian	B
inference	B
for	O
the	O
mean	B
of	O
a	O
gaussian	B
the	O
data	O
is	O
generated	O
from	O
yi	O
n	O
y	O
where	O
x	O
and	O
y	O
we	O
assume	O
the	O
sensor	O
noise	O
covariance	B
y	O
is	O
known	O
but	O
x	O
is	O
unknown	B
the	O
black	O
cross	O
represents	O
x	O
the	O
prior	O
is	O
px	O
n	O
we	O
show	O
the	O
posterior	O
after	O
data	O
points	O
have	O
been	O
observed	O
figure	O
generated	O
by	O
figure	O
we	O
observe	O
cross	O
and	O
cross	O
and	O
infer	O
e	O
equally	O
reliable	O
sensors	O
so	O
the	O
posterior	B
mean	B
estimate	O
is	O
in	O
between	O
the	O
two	O
circles	O
cross	O
sensor	O
is	O
more	O
reliable	O
so	O
the	O
estimate	O
shifts	O
more	O
towards	O
the	O
green	O
circle	O
sensor	O
is	O
more	O
reliable	O
in	O
the	O
vertical	O
direction	O
sensor	O
is	O
more	O
reliable	O
in	O
the	O
horizontal	O
direction	O
the	O
estimate	O
is	O
an	O
appropriate	O
combination	O
of	O
the	O
two	O
measurements	O
figure	O
generated	O
by	O
note	O
that	O
this	O
technique	O
crucially	O
relies	O
on	O
modeling	O
our	O
uncertainty	B
of	O
each	O
sensor	O
computing	O
an	O
unweighted	O
average	O
would	O
give	O
the	O
wrong	O
result	O
however	O
we	O
have	O
assumed	O
the	O
sensor	O
precisions	O
are	O
known	O
when	O
they	O
are	O
not	O
we	O
should	O
model	O
out	O
uncertainty	B
about	O
and	O
as	O
well	O
see	O
section	O
for	O
details	O
interpolating	O
noisy	O
data	O
we	O
now	O
revisit	O
the	O
example	O
of	O
section	O
this	O
time	O
we	O
no	O
longer	O
assume	O
noise-free	O
let	O
us	O
assume	O
that	O
we	O
obtain	O
n	O
noisy	O
observations	O
yi	O
without	O
loss	B
observations	O
of	O
generality	O
assume	O
these	O
correspond	O
to	O
xn	O
we	O
can	O
model	O
this	O
setup	O
as	O
a	O
linear	O
instead	O
chapter	O
gaussian	B
models	O
gaussian	B
system	O
y	O
ax	O
where	O
n	O
y	O
y	O
is	O
the	O
observation	B
noise	O
and	O
a	O
is	O
a	O
n	O
d	O
projection	B
matrix	O
that	O
selects	O
out	O
the	O
observed	O
elements	O
for	O
example	O
if	O
n	O
and	O
d	O
we	O
have	O
a	O
we	O
can	O
easily	O
compute	O
the	O
posterior	O
using	O
the	O
same	O
improper	B
prior	I
as	O
before	O
x	O
l	O
mean	B
and	O
variance	B
in	O
figure	O
we	O
plot	O
the	O
posterior	B
mean	B
posterior	O
variance	B
and	O
some	O
posterior	O
samples	B
now	O
we	O
see	O
that	O
the	O
prior	O
precision	B
effects	O
the	O
posterior	B
mean	B
as	O
well	O
as	O
the	O
posterior	O
variance	B
in	O
particular	O
for	O
a	O
strong	O
prior	O
the	O
estimate	O
is	O
very	O
smooth	O
and	O
the	O
uncertainty	B
is	O
low	O
but	O
for	O
a	O
weak	O
prior	O
the	O
estimate	O
is	O
wiggly	O
and	O
the	O
uncertainty	B
from	O
the	O
data	O
is	O
high	O
the	O
posterior	B
mean	B
can	O
also	O
be	O
computed	O
by	O
solving	O
the	O
following	O
optimization	B
problem	O
min	O
x	O
xj	O
where	O
we	O
have	O
defined	O
and	O
xd	O
for	O
notational	O
simplicity	O
we	O
recognize	O
this	O
as	O
a	O
discrete	B
approximation	O
to	O
the	O
following	O
problem	O
f	O
min	O
where	O
is	O
the	O
first	O
derivative	O
of	O
f	O
the	O
first	O
term	O
measures	O
fit	O
to	O
the	O
data	O
and	O
the	O
second	O
term	O
penalizes	O
functions	O
that	O
are	O
too	O
wiggly	O
this	O
is	O
an	O
example	O
of	O
tikhonov	B
regularization	B
which	O
is	O
a	O
popular	O
approach	O
to	O
functional	B
data	I
analysis	I
see	O
chapter	O
for	O
more	O
sophisticated	O
approaches	O
which	O
enforce	O
higher	O
order	O
smoothness	O
the	O
resulting	O
samples	B
look	O
less	O
jagged	O
proof	O
of	O
the	O
result	O
we	O
now	O
derive	O
equation	O
the	O
basic	O
idea	O
is	O
to	O
derive	O
the	O
joint	B
distribution	I
px	O
y	O
pxpyx	O
and	O
then	O
to	O
use	O
the	O
results	O
from	O
section	O
for	O
computing	O
pxy	O
in	O
more	O
detail	O
we	O
proceed	O
as	O
follows	O
the	O
log	O
of	O
the	O
joint	B
distribution	I
is	O
as	O
follows	O
irrelevant	O
constants	O
log	O
px	O
y	O
xt	O
x	O
x	O
ax	O
bt	O
y	O
ax	O
b	O
this	O
is	O
clearly	O
a	O
joint	O
gaussian	B
distribution	O
since	O
it	O
is	O
the	O
exponential	O
of	O
a	O
quadratic	O
form	O
expanding	O
out	O
the	O
quadratic	O
terms	O
involving	O
x	O
and	O
y	O
and	O
ignoring	O
linear	O
and	O
constant	O
terms	O
we	O
have	O
q	O
y	O
y	O
y	O
a	O
at	O
y	O
y	O
t	O
y	O
ax	O
y	O
x	O
y	O
yt	O
x	O
x	O
xt	O
x	O
at	O
x	O
y	O
y	O
a	O
x	O
y	O
x	O
y	O
digression	O
the	O
wishart	B
distribution	O
interpolating	O
noisy	O
data	O
variance	B
using	O
a	O
gaussian	B
with	O
prior	O
precision	B
see	O
also	O
figure	O
based	O
on	O
figure	O
of	O
and	O
somersalo	O
figure	O
figure	O
generated	O
by	O
gaussinterpnoisydemo	O
see	O
also	O
splinebasisdemo	O
where	O
the	O
precision	B
matrix	I
of	O
the	O
joint	O
is	O
defined	O
as	O
x	O
at	O
y	O
a	O
at	O
y	O
y	O
a	O
y	O
xx	O
xy	O
yx	O
yy	O
from	O
equation	O
and	O
using	O
the	O
fact	O
that	O
y	O
a	O
x	O
b	O
we	O
have	O
pxy	O
xy	O
xy	O
xy	O
xx	O
xy	O
xy	O
xy	O
x	O
at	O
y	O
a	O
xx	O
x	O
xyy	O
y	O
y	O
b	O
x	O
at	O
digression	O
the	O
wishart	B
distribution	O
the	O
wishart	B
distribution	O
is	O
the	O
generalization	B
of	O
the	O
gamma	B
distribution	I
to	O
positive	O
definite	O
matrices	O
press	O
has	O
said	O
the	O
wishart	B
distribution	O
ranks	O
next	O
to	O
the	O
normal	B
distribution	O
in	O
order	O
of	O
importance	O
and	O
usefuleness	O
in	O
multivariate	O
statistics	O
we	O
will	O
mostly	O
use	O
it	O
to	O
model	O
our	O
uncertainty	B
in	O
covariance	B
matrices	O
or	O
their	O
inverses	O
the	O
pdf	B
of	O
the	O
wishart	B
is	O
defined	O
as	O
follows	O
wi	O
d	O
exp	O
tr	O
s	O
zwi	O
here	O
is	O
called	O
the	O
degrees	B
of	I
freedom	I
and	O
s	O
is	O
the	O
scale	O
matrix	O
shall	O
get	O
more	O
intuition	O
for	O
these	O
parameters	O
shortly	O
the	O
normalization	O
constant	O
for	O
this	O
distribution	O
chapter	O
gaussian	B
models	O
requires	O
integrating	O
over	O
all	O
symmetric	B
pd	O
matrices	O
is	O
the	O
following	O
formidable	O
expression	O
zwi	O
d	O
where	O
da	O
is	O
the	O
multivariate	B
gamma	B
function	I
dx	O
dd	O
hence	O
and	O
i	O
d	O
the	O
normalization	O
constant	O
only	O
exists	O
hence	O
the	O
pdf	B
is	O
only	O
well	O
defined	O
if	O
d	O
in	O
particular	O
let	O
xi	O
n	O
then	O
the	O
scatter	O
matrix	O
s	O
i	O
has	O
a	O
wishart	B
distribution	O
s	O
wi	O
hence	O
e	O
n	O
more	O
generally	O
one	O
can	O
show	O
that	O
the	O
mean	B
and	O
mode	B
of	O
wis	O
are	O
given	O
by	O
there	O
is	O
a	O
connection	O
between	O
the	O
wishart	B
distribution	O
and	O
the	O
gaussian	B
xixt	O
mean	B
s	O
mode	B
d	O
where	O
the	O
mode	B
only	O
exists	O
if	O
d	O
if	O
d	O
the	O
wishart	B
reduces	O
to	O
the	O
gamma	B
distribution	I
wi	O
ga	O
s	O
inverse	B
wishart	B
distribution	O
recall	B
that	O
we	O
showed	O
that	O
if	O
gaa	O
b	O
then	O
that	O
iga	O
b	O
similarly	O
if	O
wis	O
then	O
iws	O
d	O
where	O
iw	O
is	O
the	O
inverse	B
wishart	B
the	O
multidimensional	O
generalization	B
of	O
the	O
inverse	B
gamma	B
it	O
is	O
defined	O
as	O
follows	O
for	O
d	O
and	O
s	O
iw	O
exp	O
ziw	O
d	O
ziw	O
trs	O
one	O
can	O
show	O
that	O
the	O
distribution	O
has	O
these	O
properties	O
mean	B
s	O
d	O
mode	B
s	O
d	O
if	O
d	O
this	O
reduces	O
to	O
the	O
inverse	B
gamma	B
iw	O
ig	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
s	O
figure	O
visualization	O
of	O
the	O
wishart	B
distribution	O
left	O
some	O
samples	B
from	O
the	O
wishart	B
distribution	O
wis	O
where	O
s	O
and	O
right	O
plots	O
of	O
the	O
marginals	O
are	O
gamma	B
and	O
the	O
approximate	O
marginal	O
on	O
the	O
correlation	B
coefficient	I
if	O
there	O
is	O
a	O
lot	O
of	O
uncertainty	B
about	O
the	O
value	O
of	O
the	O
correlation	B
coefficient	I
the	O
almost	O
uniform	B
distribution	I
on	O
the	O
sampled	O
matrices	O
are	O
highly	O
variable	O
and	O
some	O
are	O
nearly	O
singular	O
as	O
increases	O
the	O
sampled	O
matrices	O
are	O
more	O
concentrated	O
on	O
the	O
prior	O
s	O
figure	O
generated	O
by	O
wiplotdemo	O
visualizing	B
the	O
wishart	B
distribution	O
since	O
the	O
wishart	B
is	O
a	O
distribution	O
over	O
matrices	O
it	O
is	O
hard	O
to	O
plot	O
as	O
a	O
density	O
function	O
however	O
we	O
can	O
easily	O
sample	O
from	O
it	O
and	O
in	O
the	O
case	O
we	O
can	O
use	O
the	O
eigenvectors	O
of	O
the	O
resulting	O
matrix	O
to	O
define	O
an	O
ellipse	O
as	O
explained	O
in	O
section	O
see	O
figure	O
for	O
some	O
examples	O
for	O
higher	O
dimensional	O
matrices	O
we	O
can	O
plot	O
marginals	O
of	O
the	O
distribution	O
the	O
diagonals	O
of	O
a	O
wishart	B
distributed	O
matrix	O
have	O
gamma	B
distributions	O
so	O
are	O
easy	O
to	O
plot	O
it	O
is	O
hard	O
in	O
general	O
to	O
work	O
out	O
the	O
distribution	O
of	O
the	O
off-diagonal	O
elements	O
but	O
we	O
can	O
sample	O
matrices	O
from	O
the	O
distribution	O
and	O
then	O
compute	O
the	O
distribution	O
empirically	O
in	O
particular	O
we	O
can	O
convert	O
each	O
sampled	O
matrix	O
to	O
a	O
correlation	B
matrix	I
and	O
thus	O
compute	O
a	O
monte	B
carlo	I
approximation	O
to	O
the	O
expected	O
correlation	O
coefficients	O
e	O
s	O
r	O
where	O
wi	O
and	O
r	O
converts	O
matrix	O
into	O
a	O
correlation	B
matrix	I
rij	O
ii	O
jj	O
we	O
can	O
then	O
use	O
kernel	B
density	B
estimation	I
to	O
produce	O
a	O
smooth	O
approximation	O
to	O
the	O
univariate	O
density	O
e	O
for	O
plotting	O
purposes	O
see	O
figure	O
for	O
some	O
examples	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
so	O
far	O
we	O
have	O
discussed	O
inference	B
in	O
a	O
gaussian	B
assuming	O
the	O
parameters	O
are	O
known	O
we	O
now	O
discuss	O
how	O
to	O
infer	O
the	O
parameters	O
themselves	O
we	O
will	O
assume	O
the	O
data	O
has	O
chapter	O
gaussian	B
models	O
the	O
form	O
xi	O
n	O
for	O
i	O
and	O
is	O
fully	O
observed	O
so	O
we	O
have	O
no	O
missing	B
data	I
section	O
for	O
how	O
to	O
estimate	O
parameters	O
of	O
an	O
mvn	B
in	O
the	O
presence	O
of	O
missing	B
values	O
to	O
simplify	O
the	O
presentation	O
we	O
derive	O
the	O
posterior	O
in	O
three	O
parts	O
first	O
we	O
compute	O
p	O
then	O
we	O
compute	O
p	O
finally	O
we	O
compute	O
the	O
joint	O
p	O
posterior	O
distribution	O
of	O
we	O
have	O
discussed	O
how	O
to	O
compute	O
the	O
mle	B
for	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
its	O
posterior	O
which	O
is	O
useful	O
for	O
modeling	O
our	O
uncertainty	B
about	O
its	O
value	O
the	O
likelihood	B
has	O
the	O
form	O
pd	O
n	O
n	O
for	O
simplicity	O
we	O
will	O
use	O
a	O
conjugate	B
prior	I
which	O
in	O
this	O
case	O
is	O
a	O
gaussian	B
in	O
particular	O
if	O
p	O
n	O
then	O
we	O
can	O
derive	O
a	O
gaussian	B
posterior	O
for	O
based	O
on	O
the	O
results	O
in	O
section	O
we	O
get	O
p	O
vn	O
n	O
v	O
n	O
v	O
mn	O
vn	O
x	O
this	O
is	O
exactly	O
the	O
same	O
process	O
as	O
inferring	O
the	O
location	O
of	O
an	O
object	O
based	O
on	O
noisy	O
radar	B
blips	O
except	O
now	O
we	O
are	O
inferring	O
the	O
mean	B
of	O
a	O
distribution	O
based	O
on	O
noisy	O
samples	B
a	O
bayesian	B
there	O
is	O
no	O
difference	O
between	O
uncertainty	B
about	O
parameters	O
and	O
uncertainty	B
about	O
anything	O
else	O
we	O
can	O
model	O
an	O
uninformative	B
prior	O
by	O
setting	O
i	O
in	O
this	O
case	O
we	O
have	O
p	O
n	O
n	O
so	O
the	O
posterior	B
mean	B
is	O
equal	O
to	O
the	O
mle	B
we	O
also	O
see	O
that	O
the	O
posterior	O
variance	B
goes	O
down	O
as	O
which	O
is	O
a	O
standard	O
result	O
from	O
frequentist	B
statistics	I
posterior	O
distribution	O
of	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
p	O
the	O
likelihood	B
has	O
the	O
form	O
pd	O
n	O
exp	O
trs	O
the	O
corresponding	O
conjugate	B
prior	I
is	O
known	O
as	O
the	O
inverse	B
wishart	B
distribution	O
recall	B
that	O
this	O
has	O
the	O
following	O
pdf	B
iw	O
exp	O
here	O
d	O
is	O
the	O
degrees	B
of	I
freedom	I
and	O
is	O
a	O
symmetric	B
pd	O
matrix	O
we	O
see	O
that	O
s	O
plays	O
the	O
role	O
of	O
the	O
prior	O
scatter	O
matrix	O
and	O
d	O
controls	O
the	O
strength	O
of	O
the	O
prior	O
and	O
hence	O
plays	O
a	O
role	O
analogous	O
to	O
the	O
sample	O
size	O
n	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
true	O
mle	B
k	O
map	O
true	O
mle	B
map	O
true	O
mle	B
map	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
figure	O
estimating	O
a	O
covariance	B
matrix	I
in	O
d	O
dimensions	O
using	O
n	O
samples	B
we	O
plot	O
the	O
eigenvalues	O
in	O
descending	O
order	O
for	O
the	O
true	O
covariance	B
matrix	I
black	O
the	O
mle	B
blue	O
and	O
the	O
map	B
estimate	I
red	O
using	O
equation	O
with	O
we	O
also	O
list	O
the	O
condition	O
number	O
of	O
each	O
matrix	O
in	O
the	O
legend	O
based	O
on	O
figure	O
of	O
and	O
strimmer	O
figure	O
generated	O
by	O
shrinkcovdemo	O
multiplying	O
the	O
likelihood	B
and	O
prior	O
we	O
find	O
that	O
the	O
posterior	O
is	O
also	O
inverse	B
wishart	B
p	O
n	O
tr	O
exp	O
tr	O
exp	O
n	O
iw	O
n	O
exp	O
tr	O
n	O
n	O
s	O
n	O
s	O
in	O
words	O
this	O
says	O
that	O
the	O
posterior	O
strength	O
n	O
is	O
the	O
prior	O
strength	O
plus	O
the	O
number	O
of	O
observations	O
n	O
and	O
the	O
posterior	O
scatter	O
matrix	O
sn	O
is	O
the	O
prior	O
scatter	O
matrix	O
plus	O
the	O
data	O
scatter	O
matrix	O
s	O
map	O
estimation	O
we	O
see	O
from	O
equation	O
that	O
mle	B
is	O
a	O
rank	O
minn	O
d	O
matrix	O
if	O
n	O
d	O
this	O
is	O
not	O
full	B
rank	O
and	O
hence	O
will	O
be	O
uninvertible	O
and	O
even	O
if	O
n	O
d	O
it	O
may	O
be	O
the	O
case	O
that	O
is	O
ill-conditioned	B
it	O
is	O
nearly	O
singular	O
to	O
solve	O
these	O
problems	O
we	O
can	O
use	O
the	O
posterior	B
mode	B
mean	B
one	O
can	O
show	O
techniques	O
analogous	O
to	O
the	O
derivation	O
of	O
the	O
mle	B
that	O
the	O
map	B
estimate	I
is	O
given	O
by	O
map	O
sn	O
n	O
d	O
s	O
n	O
if	O
we	O
use	O
an	O
improper	O
uniform	O
prior	O
corresponding	O
to	O
and	O
we	O
recover	O
the	O
mle	B
chapter	O
gaussian	B
models	O
let	O
us	O
now	O
consider	O
the	O
use	O
of	O
a	O
proper	O
informative	O
prior	O
which	O
is	O
necessary	O
whenever	O
dn	O
is	O
large	O
bigger	O
than	O
let	O
x	O
so	O
s	O
sx	O
then	O
we	O
can	O
rewrite	O
the	O
map	B
estimate	I
as	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mode	B
and	O
the	O
mle	B
to	O
see	O
this	O
let	O
be	O
the	O
prior	O
mode	B
then	O
the	O
posterior	B
mode	B
can	O
be	O
rewritten	O
as	O
map	O
sx	O
n	O
n	O
n	O
n	O
s	O
n	O
mle	B
where	O
controls	O
the	O
amount	O
of	O
shrinkage	B
towards	O
the	O
prior	O
this	O
begs	O
the	O
question	O
where	O
do	O
the	O
parameters	O
of	O
the	O
prior	O
come	O
from	O
it	O
is	O
common	O
to	O
set	O
by	O
cross	B
validation	I
alternatively	O
we	O
can	O
use	O
the	O
closed-form	O
formula	O
provided	O
in	O
and	O
wolf	O
schaefer	O
and	O
strimmer	O
which	O
is	O
the	O
optimal	O
frequentist	B
estimate	O
if	O
we	O
use	O
squared	B
loss	B
this	O
is	O
arguably	O
not	O
the	O
most	O
natural	O
loss	B
function	I
for	O
covariance	B
matrices	O
it	O
ignores	O
the	O
postive	O
definite	O
constraint	O
but	O
it	O
results	O
in	O
a	O
simple	O
estimator	B
which	O
is	O
implemented	O
in	O
the	O
pmtk	O
function	O
shrinkcov	O
we	O
discuss	O
bayesian	B
ways	O
of	O
estimating	O
later	O
as	O
for	O
the	O
prior	O
covariance	B
matrix	I
it	O
is	O
common	O
to	O
use	O
the	O
following	O
dependent	O
prior	O
diag	O
mle	B
in	O
this	O
case	O
the	O
map	B
estimate	I
is	O
given	O
by	O
mapi	O
j	O
mlei	O
j	O
mlei	O
j	O
if	O
i	O
j	O
otherwise	O
thus	O
we	O
see	O
that	O
the	O
diagonal	B
entries	O
are	O
equal	O
to	O
their	O
ml	O
estimates	O
and	O
the	O
off	O
diagonal	B
elements	O
are	O
shrunk	O
somewhat	O
towards	O
this	O
technique	O
is	O
therefore	O
called	O
shrinkage	B
estimation	I
orregularized	O
estimation	O
the	O
benefits	O
of	O
map	O
estimation	O
are	O
illustrated	O
in	O
figure	O
we	O
consider	O
fitting	O
a	O
dimensional	O
gaussian	B
to	O
n	O
n	O
and	O
n	O
data	O
points	O
we	O
see	O
that	O
the	O
map	B
estimate	I
is	O
always	O
well-conditioned	O
unlike	O
the	O
mle	B
in	O
particular	O
we	O
see	O
that	O
the	O
eigenvalue	B
spectrum	I
of	O
the	O
map	B
estimate	I
is	O
much	O
closer	O
to	O
that	O
of	O
the	O
true	O
matrix	O
than	O
the	O
mle	B
s	O
the	O
eigenvectors	O
however	O
are	O
unaffected	O
the	O
importance	O
of	O
regularizing	O
the	O
estimate	O
of	O
will	O
become	O
apparent	O
in	O
later	O
chapters	O
when	O
we	O
consider	O
fitting	O
covariance	B
matrices	O
to	O
high	O
dimensional	O
data	O
univariate	O
posterior	O
in	O
the	O
case	O
the	O
likelihood	B
has	O
the	O
form	O
pd	O
exp	O
the	O
standard	O
conjugate	B
prior	I
is	O
the	O
inverse	B
gamma	B
distribution	I
which	O
is	O
just	O
the	O
scalar	O
version	O
of	O
the	O
inverse	B
wishart	B
ig	O
exp	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
prior	O
iw	O
true	O
figure	O
sequential	B
updating	O
of	O
the	O
posterior	O
for	O
starting	O
from	O
an	O
uninformative	B
prior	O
the	O
data	O
was	O
generated	O
from	O
a	O
gaussian	B
with	O
known	O
mean	B
and	O
unknown	B
variance	B
figure	O
generated	O
by	O
multiplying	O
the	O
likelihood	B
and	O
the	O
prior	O
we	O
see	O
that	O
the	O
posterior	O
is	O
also	O
ig	O
p	O
ig	O
bn	O
an	O
bn	O
see	O
figure	O
for	O
an	O
illustration	O
this	O
arises	O
because	O
iw	O
ig	O
the	O
form	O
of	O
the	O
posterior	O
is	O
not	O
quite	O
as	O
pretty	O
as	O
the	O
multivariate	O
case	O
because	O
of	O
the	O
factors	B
of	O
another	O
problem	O
with	O
using	O
the	O
distribution	O
is	O
that	O
the	O
strength	O
of	O
the	O
prior	O
is	O
encoded	O
in	O
both	O
and	O
to	O
avoid	O
both	O
of	O
these	O
problems	O
it	O
is	O
common	O
the	O
statistics	O
literature	O
to	O
use	O
an	O
alternative	O
parameterization	O
of	O
the	O
ig	O
distribution	O
known	O
as	O
the	O
inverse	B
chi-squared	B
distribution	I
this	O
is	O
defined	O
as	O
follows	O
ig	O
exp	O
here	O
controls	O
the	O
strength	O
of	O
the	O
prior	O
and	O
prior	O
the	O
posterior	O
becomes	O
p	O
n	O
n	O
n	O
n	O
n	O
n	O
encodes	O
the	O
value	O
of	O
the	O
prior	O
with	O
this	O
we	O
see	O
that	O
the	O
posterior	O
dof	O
n	O
is	O
the	O
prior	O
dof	O
plus	O
n	O
and	O
the	O
posterior	O
sum	B
of	I
squares	I
n	O
n	O
is	O
the	O
prior	O
sum	B
of	I
squares	I
we	O
can	O
emulate	O
an	O
uninformative	B
prior	O
p	O
by	O
setting	O
which	O
makes	O
plus	O
the	O
data	O
sum	B
of	I
squares	I
intuitive	O
sense	O
it	O
corresponds	O
to	O
a	O
zero	O
virtual	O
sample	O
size	O
chapter	O
gaussian	B
models	O
posterior	O
distribution	O
of	O
and	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
p	O
these	O
results	O
are	O
a	O
bit	O
complex	O
but	O
will	O
prove	O
useful	O
later	O
on	O
in	O
this	O
book	O
feel	O
free	O
to	O
skip	O
this	O
section	O
on	O
a	O
first	O
reading	O
likelihood	B
the	O
likelihood	B
is	O
given	O
by	O
pd	O
n	O
n	O
exp	O
now	O
one	O
can	O
show	O
that	O
tr	O
hence	O
we	O
can	O
rewrite	O
the	O
likelihood	B
as	O
follows	O
pd	O
n	O
n	O
n	O
exp	O
tr	O
exp	O
xt	O
x	O
n	O
we	O
will	O
use	O
this	O
form	O
below	O
prior	O
the	O
obvious	O
prior	O
to	O
use	O
is	O
the	O
following	O
p	O
unfortunately	O
this	O
is	O
not	O
conjugate	O
to	O
the	O
likelihood	B
to	O
see	O
why	O
note	O
that	O
and	O
appear	O
together	O
in	O
a	O
non-factorized	O
way	O
in	O
the	O
likelihood	B
hence	O
they	O
will	O
also	O
be	O
coupled	O
together	O
in	O
the	O
posterior	O
the	O
above	O
prior	O
is	O
sometimes	O
called	O
semi-conjugate	B
or	O
conditionally	B
conjugate	I
since	O
both	O
conditionals	O
p	O
and	O
p	O
are	O
individually	O
conjugate	O
to	O
create	O
a	O
full	B
conjugate	B
prior	I
we	O
need	O
to	O
use	O
a	O
prior	O
where	O
and	O
are	O
dependent	O
on	O
each	O
other	O
we	O
will	O
use	O
a	O
joint	B
distribution	I
of	O
the	O
form	O
p	O
looking	O
at	O
the	O
form	O
of	O
the	O
likelihood	B
equation	O
equation	O
we	O
see	O
that	O
a	O
natural	O
conjugate	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
prior	O
has	O
the	O
form	O
of	O
a	O
normal-inverse-wishart	B
or	O
niw	B
distribution	O
defined	O
as	O
follows	O
niw	B
iw	O
tr	O
n	O
zn	O
iw	O
exp	O
exp	O
zn	O
iw	O
exp	O
zn	O
iw	O
d	O
tr	O
where	O
da	O
is	O
the	O
multivariate	B
gamma	B
function	I
the	O
parameters	O
of	O
the	O
niw	B
can	O
be	O
interpreted	O
as	O
follows	O
is	O
our	O
prior	O
mean	B
for	O
and	O
is	O
how	O
strongly	O
we	O
believe	O
this	O
prior	O
and	O
is	O
to	O
our	O
prior	O
mean	B
for	O
and	O
is	O
how	O
strongly	O
we	O
believe	O
this	O
one	O
can	O
show	O
that	O
the	O
uninformative	B
prior	O
has	O
the	O
form	O
lim	O
k	O
n	O
k	O
d	O
niw	B
in	O
practice	O
it	O
is	O
often	O
better	O
to	O
use	O
a	O
weakly	O
informative	O
data-dependent	O
prior	O
a	O
common	O
and	O
raftery	O
is	O
to	O
use	O
choice	O
e	O
g	O
diagsxn	O
and	O
d	O
to	O
ensure	O
e	O
and	O
to	O
set	O
x	O
and	O
to	O
some	O
small	O
number	O
such	O
as	O
et	O
al	O
although	O
this	O
prior	O
has	O
four	O
parameters	O
there	O
are	O
really	O
only	O
three	O
free	O
parameters	O
since	O
our	O
uncertainty	B
in	O
the	O
mean	B
is	O
proportional	O
to	O
the	O
variance	B
in	O
particular	O
if	O
we	O
believe	O
that	O
the	O
variance	B
is	O
large	O
then	O
our	O
uncertainty	B
in	O
must	O
be	O
large	O
too	O
this	O
makes	O
sense	O
intuitively	O
since	O
if	O
the	O
data	O
has	O
large	O
spread	O
it	O
may	O
be	O
hard	O
to	O
pin	O
down	O
its	O
mean	B
see	O
also	O
exercise	O
where	O
we	O
will	O
see	O
the	O
three	O
free	O
parameters	O
more	O
explicitly	O
if	O
we	O
want	O
separate	O
control	O
over	O
our	O
confidence	O
in	O
and	O
we	O
must	O
use	O
a	O
semi-conjugate	B
prior	O
posterior	O
chapter	O
gaussian	B
models	O
the	O
posterior	O
can	O
be	O
shown	O
to	O
be	O
niw	B
with	O
updated	O
parameters	O
p	O
niw	B
n	O
n	O
sn	O
n	O
x	O
mn	O
n	O
n	O
n	O
x	O
n	O
n	O
n	O
n	O
n	O
sn	O
sx	O
n	O
s	O
xixt	O
n	O
mn	O
mt	O
i	O
as	O
the	O
uncentered	O
sum-of-squares	O
matrix	O
is	O
easier	O
n	O
where	O
we	O
have	O
defined	O
s	O
to	O
update	O
incrementally	O
than	O
the	O
centered	O
version	O
this	O
result	O
is	O
actually	O
quite	O
intuitive	O
the	O
posterior	B
mean	B
is	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mean	B
and	O
the	O
mle	B
with	O
strength	O
n	O
and	O
the	O
posterior	O
scatter	O
matrix	O
sn	O
is	O
the	O
prior	O
scatter	O
matrix	O
plus	O
the	O
empirical	O
scatter	O
matrix	O
sx	O
plus	O
an	O
extra	O
term	O
due	O
to	O
the	O
uncertainty	B
in	O
the	O
mean	B
creates	O
its	O
own	O
virtual	O
scatter	O
matrix	O
posterior	B
mode	B
the	O
mode	B
of	O
the	O
joint	B
distribution	I
has	O
the	O
following	O
form	O
argmax	O
p	O
sn	O
n	O
d	O
if	O
we	O
set	O
this	O
reduces	O
to	O
argmax	O
p	O
sx	O
n	O
d	O
the	O
corresponding	O
estimate	O
is	O
almost	O
the	O
same	O
as	O
equation	O
but	O
differs	O
by	O
in	O
the	O
denominator	O
because	O
this	O
is	O
the	O
mode	B
of	O
the	O
joint	O
not	O
the	O
mode	B
of	O
the	O
marginal	O
posterior	O
marginals	O
the	O
posterior	O
marginal	O
for	O
is	O
simply	O
p	O
p	O
iw	O
n	O
the	O
mode	B
and	O
mean	B
of	O
this	O
marginal	O
are	O
given	O
by	O
n	O
d	O
n	O
d	O
e	O
map	O
sn	O
sn	O
one	O
can	O
show	O
that	O
the	O
posterior	O
marginal	O
for	O
has	O
a	O
multivariate	B
student	B
t	I
distribution	I
p	O
p	O
t	O
sn	O
n	O
d	O
n	O
n	O
d	O
this	O
follows	O
from	O
the	O
fact	O
that	O
the	O
student	O
distribution	O
can	O
be	O
represented	O
as	O
a	O
scaled	O
mixture	B
of	I
gaussians	I
equation	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
is	O
the	O
prior	O
variance	B
and	O
is	O
how	O
strongly	O
we	O
believe	O
this	O
figure	O
the	O
n	O
i	O
believe	O
this	O
notice	O
that	O
the	O
contour	O
plot	O
the	O
surface	O
is	O
shaped	O
like	O
a	O
squashed	O
egg	O
increase	O
the	O
strength	O
of	O
our	O
belief	O
in	O
the	O
mean	B
so	O
it	O
gets	O
narrower	O
we	O
increase	O
the	O
strength	O
of	O
our	O
belief	O
in	O
the	O
variance	B
so	O
it	O
gets	O
narrower	O
figure	O
generated	O
by	O
distribution	O
is	O
the	O
prior	O
mean	B
and	O
is	O
how	O
strongly	O
we	O
we	O
posterior	O
predictive	B
the	O
posterior	O
predictive	B
is	O
given	O
by	O
pxd	O
pxd	O
pd	O
so	O
it	O
can	O
be	O
easily	O
evaluated	O
in	O
terms	O
of	O
a	O
ratio	O
of	O
marginal	O
likelihoods	O
it	O
turns	O
out	O
that	O
this	O
ratio	O
has	O
the	O
form	O
of	O
a	O
multivariate	O
student-t	O
distribution	O
pxd	O
n	O
n	O
n	O
sn	O
d	O
t	O
n	O
n	O
n	O
d	O
sn	O
n	O
d	O
the	O
student-t	O
has	O
wider	O
tails	O
than	O
a	O
gaussian	B
which	O
takes	O
into	O
account	O
the	O
fact	O
that	O
is	O
unknown	B
however	O
this	O
rapidly	O
becomes	O
gaussian-like	O
posterior	O
for	O
scalar	O
data	O
we	O
now	O
specialise	O
the	O
above	O
results	O
to	O
the	O
case	O
where	O
xi	O
is	O
these	O
results	O
are	O
widely	O
used	O
in	O
the	O
statistics	O
literature	O
as	O
in	O
section	O
it	O
is	O
conventional	O
not	O
to	O
use	O
the	O
normal	B
inverse	O
chapter	O
gaussian	B
models	O
wishart	B
but	O
to	O
use	O
the	O
normal	B
inverse	I
chi-squared	I
or	O
nix	B
distribution	O
defined	O
by	O
n	O
i	O
n	O
exp	O
see	O
figure	O
for	O
some	O
plots	O
along	O
the	O
axis	O
the	O
distribution	O
is	O
shaped	O
like	O
a	O
gaussian	B
and	O
along	O
the	O
axis	O
the	O
distribution	O
is	O
shaped	O
like	O
a	O
the	O
contours	O
of	O
the	O
joint	O
density	O
have	O
interestingly	O
we	O
see	O
that	O
the	O
contours	O
for	O
are	O
more	O
peaked	O
a	O
squashed	O
egg	O
appearance	O
for	O
small	O
values	O
of	O
which	O
makes	O
sense	O
since	O
if	O
the	O
data	O
is	O
low	O
variance	B
we	O
will	O
be	O
able	O
to	O
estimate	O
its	O
mean	B
more	O
reliably	O
one	O
can	O
show	O
that	O
the	O
posterior	O
is	O
given	O
by	O
p	O
i	O
n	O
n	O
n	O
n	O
x	O
mn	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
let	O
us	O
see	O
how	O
these	O
results	O
look	O
if	O
we	O
use	O
the	O
following	O
uninformative	B
prior	O
p	O
p	O
n	O
i	O
with	O
this	O
prior	O
the	O
posterior	O
has	O
the	O
form	O
p	O
i	O
x	O
n	O
n	O
n	O
n	O
n	O
where	O
n	O
n	O
n	O
mle	B
is	O
the	O
the	O
sample	B
standard	B
deviation	I
estimate	O
of	O
the	O
variance	B
hence	O
the	O
marginal	O
posterior	O
for	O
the	O
mean	B
is	O
given	O
by	O
section	O
we	O
show	O
that	O
this	O
is	O
an	O
unbiased	B
p	O
t	O
n	O
n	O
the	O
posterior	O
marginal	O
for	O
is	O
just	O
p	O
with	O
the	O
posterior	B
mean	B
given	O
by	O
e	O
p	O
n	O
n	O
n	O
n	O
n	O
mixture	B
representation	O
of	O
the	O
student	O
p	O
p	O
t	O
with	O
the	O
posterior	B
mean	B
given	O
by	O
e	O
mn	O
n	O
n	O
n	O
the	O
posterior	O
marginal	O
for	O
has	O
a	O
student	B
t	I
distribution	I
which	O
follows	O
from	O
the	O
scale	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
and	O
the	O
posterior	O
variance	B
of	O
is	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
var	B
var	B
s	O
n	O
the	O
square	O
root	B
of	O
this	O
is	O
called	O
the	O
standard	B
error	I
of	I
the	I
mean	B
thus	O
an	O
approximate	O
posterior	O
credible	B
interval	I
for	O
the	O
mean	B
is	O
x	O
s	O
n	O
credible	O
intervals	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
they	O
are	O
contrasted	O
with	O
frequentist	B
confidence	O
intervals	O
in	O
section	O
bayesian	B
t-test	B
suppose	O
we	O
want	O
to	O
test	O
the	O
hypothesis	O
that	O
for	O
some	O
known	O
value	O
given	O
values	O
xi	O
n	O
this	O
is	O
called	O
a	O
two-sided	O
one-sample	O
t-test	B
a	O
simple	O
way	O
to	O
perform	O
such	O
a	O
test	O
is	O
just	O
to	O
check	O
if	O
if	O
it	O
is	O
not	O
then	O
we	O
can	O
be	O
sure	O
that	O
a	O
more	O
common	O
scenario	O
is	O
when	O
we	O
want	O
to	O
test	O
if	O
two	O
paired	O
samples	B
have	O
the	O
same	O
mean	B
more	O
precisely	O
suppose	O
yi	O
n	O
and	O
zi	O
n	O
we	O
want	O
to	O
determine	O
if	O
using	O
xi	O
yi	O
zi	O
as	O
our	O
data	O
we	O
can	O
evaluate	O
this	O
quantity	O
as	O
follows	O
p	O
p	O
this	O
is	O
called	O
a	O
one-sided	O
paired	B
t-test	B
the	O
difference	O
in	O
binomial	B
proportions	O
see	O
section	O
a	O
similar	B
approach	O
to	O
unpaired	O
tests	O
comparing	O
to	O
calculate	O
the	O
posterior	O
we	O
must	O
specify	O
a	O
prior	O
suppose	O
we	O
use	O
an	O
uninformative	B
prior	O
as	O
we	O
showed	O
above	O
we	O
find	O
that	O
the	O
posterior	O
marginal	O
on	O
has	O
the	O
form	O
p	O
t	O
n	O
n	O
now	O
let	O
us	O
define	O
the	O
following	O
t	B
statistic	I
t	O
x	O
s	O
n	O
where	O
the	O
denominator	O
is	O
the	O
standard	B
error	I
of	I
the	I
mean	B
we	O
see	O
that	O
p	O
fn	O
where	O
f	O
is	O
the	O
cdf	B
of	O
the	O
standard	O
student	B
t	I
distribution	I
t	O
a	O
more	O
complex	O
approach	O
is	O
to	O
perform	O
bayesian	B
model	O
comparison	O
that	O
is	O
we	O
compute	O
the	O
bayes	B
factor	B
in	O
section	O
where	O
is	O
the	O
point	O
null	B
hypothesis	I
that	O
and	O
is	O
the	O
alternative	B
hypothesis	I
that	O
see	O
et	O
al	O
rouder	O
et	O
al	O
for	O
details	O
chapter	O
gaussian	B
models	O
connection	O
with	O
frequentist	B
statistics	I
if	O
we	O
use	O
an	O
uninformative	B
prior	O
it	O
turns	O
out	O
that	O
the	O
above	O
bayesian	B
analysis	O
gives	O
the	O
same	O
result	O
as	O
derived	O
using	O
frequentist	B
methods	O
discuss	O
frequentist	B
statistics	I
in	O
chapter	O
specifically	O
from	O
the	O
above	O
results	O
we	O
see	O
that	O
sn	O
tn	O
this	O
has	O
the	O
same	O
form	O
as	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	B
sn	O
tn	O
the	O
reason	O
is	O
that	O
the	O
student	O
distribution	O
is	O
symmetric	B
in	O
its	O
first	O
two	O
arguments	O
so	O
t	O
t	O
hence	O
statements	O
about	O
the	O
posterior	O
for	O
have	O
the	O
same	O
form	O
as	O
statements	O
about	O
the	O
sampling	B
distribution	I
of	O
x	O
consequently	O
the	O
p-value	B
in	O
section	O
returned	O
by	O
a	O
frequentist	B
test	O
is	O
the	O
same	O
as	O
p	O
returned	O
by	O
the	O
bayesian	B
method	O
see	O
bayesttestdemo	O
for	O
an	O
example	O
despite	O
the	O
superficial	O
similarity	O
these	O
two	O
results	O
have	O
a	O
different	O
interpretation	O
in	O
the	O
bayesian	B
approach	O
is	O
unknown	B
and	O
x	O
is	O
fixed	O
whereas	O
in	O
the	O
frequentist	B
approach	O
x	O
is	O
unknown	B
and	O
is	O
fixed	O
more	O
equivalences	O
between	O
frequentist	B
and	O
bayesian	B
inference	B
in	O
simple	O
models	O
using	O
uninformative	B
priors	O
can	O
be	O
found	O
in	O
and	O
tiao	O
see	O
also	O
section	O
sensor	B
fusion	I
with	O
unknown	B
precisions	O
in	O
this	O
section	O
we	O
apply	O
the	O
results	O
in	O
section	O
to	O
the	O
problem	O
of	O
sensor	B
fusion	I
in	O
the	O
case	O
where	O
the	O
precision	B
of	O
each	O
measurement	O
device	O
is	O
unknown	B
this	O
generalizes	O
the	O
results	O
of	O
section	O
where	O
the	O
measurement	O
model	O
was	O
assumed	O
to	O
be	O
gaussian	B
with	O
known	O
precision	B
the	O
unknown	B
precision	B
case	O
turns	O
out	O
to	O
give	O
qualitatively	O
different	O
results	O
yielding	O
a	O
potentially	O
multi-modal	O
posterior	O
as	O
we	O
will	O
see	O
our	O
presentation	O
is	O
based	O
on	O
suppose	O
we	O
want	O
to	O
pool	O
data	O
from	O
multiple	O
sources	O
to	O
estimate	O
some	O
quantity	O
r	O
but	O
the	O
reliability	O
of	O
the	O
sources	O
is	O
unknown	B
specifically	O
suppose	O
we	O
have	O
two	O
different	O
measurement	O
devices	O
x	O
and	O
y	O
with	O
different	O
precisions	O
xi	O
n	O
y	O
we	O
make	O
two	O
independent	O
measurements	O
with	O
each	O
device	O
which	O
turn	O
out	O
to	O
be	O
x	O
and	O
yi	O
n	O
we	O
will	O
use	O
a	O
non-informative	B
prior	O
for	O
p	O
which	O
we	O
can	O
emulate	O
using	O
an	O
infinitely	O
broad	O
gaussian	B
p	O
if	O
the	O
x	O
and	O
y	O
terms	O
were	O
known	O
then	O
the	O
posterior	O
would	O
be	O
gaussian	B
p	O
x	O
y	O
n	O
n	O
nx	O
x	O
ny	O
y	O
mn	O
xnxx	O
ynyy	O
nx	O
x	O
ny	O
y	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
xi	O
and	O
y	O
ny	O
where	O
nx	O
is	O
the	O
number	O
of	O
x	O
measurements	O
ny	O
is	O
the	O
number	O
of	O
y	O
measurements	O
x	O
yi	O
this	O
result	O
follows	O
because	O
the	O
posterior	O
nx	O
precision	B
is	O
the	O
sum	O
of	O
the	O
measurement	O
precisions	O
and	O
the	O
posterior	B
mean	B
is	O
a	O
weighted	O
sum	O
of	O
the	O
prior	O
mean	B
is	O
and	O
the	O
data	O
means	O
however	O
the	O
measurement	O
precisions	O
are	O
not	O
known	O
initially	O
we	O
will	O
estimate	O
them	O
by	O
maximum	O
likelihood	B
the	O
log-likelihood	O
is	O
given	O
by	O
x	O
y	O
log	O
x	O
x	O
log	O
y	O
y	O
i	O
i	O
the	O
mle	B
is	O
obtained	O
by	O
solving	O
the	O
following	O
simultaneous	O
equations	O
x	O
y	O
xnxx	O
ynyy	O
x	O
nx	O
y	O
ny	O
this	O
gives	O
x	O
y	O
nx	O
xx	O
ny	O
yy	O
nx	O
x	O
ny	O
y	O
nx	O
i	O
ny	O
i	O
we	O
notice	O
that	O
the	O
mle	B
for	O
has	O
the	O
same	O
form	O
as	O
the	O
posterior	B
mean	B
mn	O
we	O
can	O
solve	O
these	O
equations	O
by	O
fixed	O
point	O
iteration	O
let	O
us	O
initialize	O
by	O
estimating	O
x	O
x	O
and	O
y	O
using	O
this	O
we	O
get	O
so	O
p	O
x	O
y	O
if	O
we	O
now	O
iterate	O
we	O
converge	B
to	O
x	O
y	O
p	O
x	O
y	O
n	O
and	O
y	O
where	O
x	O
nx	O
y	O
ny	O
the	O
plug-in	B
approximation	I
to	O
the	O
posterior	O
is	O
plotted	O
in	O
figure	O
this	O
weights	O
each	O
sensor	O
according	O
to	O
its	O
estimated	O
precision	B
since	O
sensor	O
y	O
was	O
estimated	O
to	O
be	O
much	O
less	O
reliable	O
than	O
sensor	O
x	O
we	O
havee	O
x	O
so	O
we	O
effectively	O
ignore	O
the	O
y	O
sensor	O
x	O
y	O
now	O
we	O
will	O
adopt	O
a	O
bayesian	B
approach	O
and	O
integrate	B
out	I
the	O
unknown	B
precisions	O
rather	O
than	O
trying	O
to	O
estimate	O
them	O
that	O
is	O
we	O
compute	O
p	O
p	O
pdx	O
xp	O
x	O
x	O
we	O
will	O
use	O
uninformative	B
jeffrey	O
s	O
priors	O
p	O
p	O
x	O
x	O
and	O
p	O
y	O
y	O
pdy	O
yp	O
y	O
y	O
chapter	O
gaussian	B
models	O
since	O
the	O
x	O
and	O
y	O
terms	O
are	O
symmetric	B
we	O
will	O
just	O
focus	O
on	O
one	O
of	O
them	O
the	O
key	O
integral	O
is	O
i	O
pdx	O
xp	O
x	O
x	O
x	O
nx	O
xx	O
nx	O
exp	O
x	O
x	O
d	O
x	O
exploiting	O
the	O
fact	O
that	O
nx	O
this	O
simplifies	O
to	O
i	O
x	O
x	O
exp	O
xx	O
xd	O
x	O
we	O
recognize	O
this	O
as	O
proportional	O
to	O
the	O
integral	O
of	O
an	O
unnormalized	O
gamma	B
density	O
ga	O
b	O
a	O
b	O
x	O
hence	O
the	O
integral	O
is	O
proportional	O
to	O
the	O
normalizing	O
where	O
a	O
and	O
b	O
constant	O
of	O
the	O
gamma	B
distribution	I
a	O
so	O
we	O
get	O
x	O
pdx	O
xp	O
x	O
x	O
i	O
x	O
and	O
the	O
posterior	O
becomes	O
p	O
x	O
y	O
the	O
exact	O
posterior	O
is	O
plotted	O
in	O
figure	O
we	O
see	O
that	O
it	O
has	O
two	O
modes	O
one	O
near	O
x	O
and	O
one	O
near	O
y	O
these	O
correspond	O
to	O
the	O
beliefs	O
that	O
the	O
x	O
sensor	O
is	O
more	O
reliable	O
than	O
the	O
y	O
one	O
and	O
vice	O
versa	O
the	O
weight	O
of	O
the	O
first	O
mode	B
is	O
larger	O
since	O
the	O
data	O
from	O
the	O
x	O
sensor	O
agree	O
more	O
with	O
each	O
other	O
so	O
it	O
seems	O
slightly	O
more	O
likely	O
that	O
the	O
x	O
sensor	O
is	O
the	O
reliable	O
one	O
obviously	O
cannot	O
both	O
be	O
reliable	O
since	O
they	O
disagree	O
on	O
the	O
values	O
that	O
they	O
are	O
reporting	O
however	O
the	O
bayesian	B
solution	O
keeps	O
open	O
the	O
possibility	O
that	O
the	O
y	O
sensor	O
is	O
the	O
more	O
reliable	O
one	O
from	O
two	O
measurements	O
we	O
cannot	O
tell	O
and	O
choosing	O
just	O
the	O
x	O
sensor	O
as	O
the	O
plug-in	B
approximation	I
does	O
results	O
in	O
over	O
confidence	O
posterior	O
that	O
is	O
too	O
narrow	O
exercises	O
exercise	O
uncorrelated	O
does	O
not	O
imply	O
independent	O
let	O
x	O
u	O
and	O
y	O
x	O
clearly	O
y	O
is	O
dependent	O
on	O
x	O
fact	O
y	O
is	O
uniquely	O
determined	O
if	O
x	O
u	O
b	O
then	O
ex	O
and	O
by	O
x	O
however	O
show	O
that	O
y	O
hint	O
var	B
exercise	O
uncorrelated	O
and	O
gaussian	B
does	O
not	O
imply	O
independent	O
unless	O
jointly	O
gaussian	B
let	O
x	O
n	O
and	O
y	O
w	O
x	O
where	O
pw	O
pw	O
it	O
is	O
clear	O
that	O
x	O
and	O
y	O
are	O
not	O
independent	O
since	O
y	O
is	O
a	O
function	O
of	O
x	O
a	O
show	O
y	O
n	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
figure	O
posterior	O
for	O
sensorfusionunknownprec	O
plug-in	B
approximation	I
exact	O
posterior	O
figure	O
generated	O
by	O
b	O
show	O
cov	O
y	O
thus	O
x	O
and	O
y	O
are	O
uncorrelated	O
but	O
dependent	O
even	O
though	O
they	O
are	O
gaussian	B
hint	O
use	O
the	O
definition	O
of	O
covariance	B
cov	O
y	O
e	O
e	O
e	O
and	O
the	O
rule	B
of	I
iterated	I
expectation	I
e	O
e	O
exercise	O
correlation	B
coefficient	I
is	O
between	O
and	O
prove	O
that	O
y	O
exercise	O
correlation	B
coefficient	I
for	O
linearly	O
related	O
variables	O
is	O
show	O
that	O
if	O
y	O
ax	O
b	O
for	O
some	O
parameters	O
a	O
and	O
b	O
then	O
y	O
similarly	O
show	O
that	O
if	O
a	O
then	O
y	O
exercise	O
normalization	O
constant	O
for	O
a	O
multidimensional	O
gaussian	B
prove	O
that	O
the	O
normalization	O
constant	O
for	O
a	O
d-dimensional	O
gaussian	B
is	O
given	O
by	O
exp	O
hint	O
diagonalize	O
and	O
use	O
the	O
fact	O
that	O
i	O
i	O
to	O
write	O
the	O
joint	O
pdf	B
as	O
a	O
product	O
of	O
d	O
onedimensional	O
gaussians	O
in	O
a	O
transformed	O
coordinate	O
system	O
will	O
need	O
the	O
change	B
of	I
variables	I
formula	O
finally	O
use	O
the	O
normalization	O
constant	O
for	O
univariate	O
gaussians	O
exercise	O
bivariate	O
gaussian	B
let	O
x	O
n	O
where	O
x	O
r	O
and	O
where	O
is	O
the	O
correlation	B
coefficient	I
show	O
that	O
the	O
pdf	B
is	O
given	O
by	O
exp	O
chapter	O
gaussian	B
models	O
raw	O
standarized	O
whitened	O
figure	O
heightweight	O
data	O
for	O
the	O
men	O
standardized	B
whitened	O
exercise	O
conditioning	B
a	O
bivariate	O
gaussian	B
consider	O
a	O
bivariate	O
gaussian	B
distribution	O
where	O
where	O
the	O
correlation	B
coefficient	I
is	O
given	O
by	O
a	O
what	O
is	O
p	O
simplify	O
your	O
answer	O
by	O
expressing	O
it	O
in	O
terms	O
of	O
and	O
b	O
assume	O
what	O
is	O
p	O
now	O
exercise	O
whitening	B
vs	O
standardizing	B
a	O
load	O
the	O
heightweight	O
data	O
using	O
rawdata	O
dlmread	O
heightweightdata	O
txt	O
the	O
first	O
column	O
is	O
the	O
class	O
label	B
the	O
second	O
column	O
is	O
height	O
the	O
third	O
weight	O
extract	O
the	O
heightweight	O
data	O
corresponding	O
to	O
the	O
males	O
fit	O
a	O
gaussian	B
to	O
the	O
male	O
data	O
using	O
the	O
empirical	O
mean	B
and	O
covariance	B
plot	O
your	O
gaussian	B
as	O
an	O
ellipse	O
superimposing	O
on	O
your	O
scatter	B
plot	I
it	O
should	O
look	O
like	O
figure	O
where	O
have	O
labeled	O
each	O
datapoint	O
by	O
its	O
index	O
turn	O
in	O
your	O
figure	O
and	O
code	O
b	O
standardizing	B
the	O
data	O
means	O
ensuring	O
the	O
empirical	O
variance	B
along	O
each	O
dimension	O
is	O
this	O
can	O
be	O
where	O
j	O
is	O
the	O
empirical	O
std	O
of	O
dimension	O
j	O
standardize	O
the	O
data	O
and	O
done	O
by	O
computing	O
replot	O
it	O
should	O
look	O
like	O
figure	O
axis	O
equal	O
turn	O
in	O
your	O
figure	O
and	O
code	O
xij	O
xj	O
j	O
c	O
whitening	B
or	O
sphereing	B
the	O
data	O
means	O
ensuring	O
its	O
empirical	O
covariance	B
matrix	I
is	O
proportional	O
to	O
i	O
so	O
the	O
data	O
is	O
uncorrelated	O
and	O
of	O
equal	O
variance	B
along	O
each	O
dimension	O
this	O
can	O
be	O
done	O
by	O
computing	O
ut	O
x	O
for	O
each	O
data	O
vector	O
x	O
where	O
u	O
are	O
the	O
eigenvectors	O
and	O
the	O
eigenvalues	O
of	O
x	O
whiten	O
the	O
data	O
and	O
replot	O
it	O
should	O
look	O
like	O
figure	O
note	O
that	O
whitening	B
rotates	O
the	O
data	O
so	O
people	O
move	O
to	O
counter-intuitive	O
locations	O
in	O
the	O
new	O
coordinate	O
system	O
e	O
g	O
person	O
who	O
moves	O
from	O
the	O
right	O
hand	O
side	O
to	O
the	O
left	O
exercise	O
sensor	B
fusion	I
with	O
known	O
variances	O
in	O
suppose	O
we	O
have	O
two	O
sensors	O
with	O
known	O
different	O
variances	O
and	O
but	O
unknown	B
the	O
same	O
i	O
n	O
from	O
the	O
first	O
sensor	O
and	O
observations	O
mean	B
suppose	O
we	O
observe	O
observations	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
i	O
n	O
from	O
the	O
second	O
sensor	O
example	O
suppose	O
is	O
the	O
true	O
temperature	B
outside	O
and	O
sensor	O
is	O
a	O
precise	O
variance	B
digital	O
thermosensing	O
device	O
and	O
sensor	O
is	O
an	O
imprecise	O
variance	B
mercury	O
thermometer	O
let	O
d	O
represent	O
all	O
the	O
data	O
from	O
both	O
sensors	O
what	O
is	O
the	O
posterior	O
p	O
assuming	O
a	O
non-informative	B
prior	O
for	O
we	O
can	O
simulate	O
using	O
a	O
gaussian	B
with	O
a	O
precision	B
of	O
give	O
an	O
explicit	O
expression	O
for	O
the	O
posterior	B
mean	B
and	O
variance	B
exercise	O
derivation	O
of	O
information	B
form	I
formulae	O
for	O
marginalizing	O
and	O
conditioning	B
derive	O
the	O
information	B
form	I
results	O
of	O
section	O
exercise	O
derivation	O
of	O
the	O
niw	B
posterior	O
derive	O
equation	O
hint	O
one	O
can	O
show	O
that	O
n	O
n	O
mn	O
mn	O
n	O
this	O
is	O
a	O
matrix	O
generalization	B
of	O
an	O
operation	O
called	O
completing	O
the	O
derive	O
the	O
corresponding	O
result	O
for	O
the	O
normal-wishart	O
model	O
exercise	O
bic	B
for	O
gaussians	O
jaakkola	O
the	O
bayesian	B
information	B
criterion	I
is	O
a	O
penalized	O
log-likelihood	O
function	O
that	O
can	O
be	O
used	O
for	O
model	B
selection	I
section	O
it	O
is	O
defined	O
as	O
bic	B
log	O
pd	O
m	O
l	O
d	O
logn	O
where	O
d	O
is	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
model	O
and	O
n	O
is	O
the	O
number	O
of	O
samples	B
in	O
this	O
question	O
we	O
will	O
see	O
how	O
to	O
use	O
this	O
to	O
choose	O
between	O
a	O
full	B
covariance	B
gaussian	B
and	O
a	O
gaussian	B
with	O
a	O
diagonal	B
covariance	B
obviously	O
a	O
full	B
covariance	B
gaussian	B
has	O
higher	O
likelihood	B
but	O
it	O
may	O
not	O
be	O
worth	O
the	O
extra	O
parameters	O
if	O
the	O
improvement	O
over	O
a	O
diagonal	B
covariance	B
matrix	I
is	O
too	O
small	O
so	O
we	O
use	O
the	O
bic	B
score	O
to	O
choose	O
the	O
model	O
following	O
section	O
we	O
can	O
write	O
log	O
pd	O
n	O
tr	O
s	O
n	O
s	O
n	O
log	O
xxi	O
xt	O
where	O
s	O
is	O
the	O
scatter	O
matrix	O
covariance	B
the	O
trace	B
of	O
a	O
matrix	O
is	O
the	O
sum	O
of	O
its	O
diagonals	O
and	O
we	O
have	O
used	O
the	O
trace	B
trick	I
a	O
derive	O
the	O
bic	B
score	O
for	O
a	O
gaussian	B
in	O
d	O
dimensions	O
with	O
full	B
covariance	B
matrix	I
simplify	O
your	O
answer	O
as	O
much	O
as	O
possible	O
exploiting	O
the	O
form	O
of	O
the	O
mle	B
be	O
sure	O
to	O
specify	O
the	O
number	O
of	O
free	O
parameters	O
d	O
b	O
derive	O
the	O
bic	B
score	O
for	O
a	O
gaussian	B
in	O
d	O
dimensions	O
with	O
a	O
diagonal	B
covariance	B
matrix	I
be	O
sure	O
to	O
specify	O
the	O
number	O
of	O
free	O
parameters	O
d	O
hint	O
for	O
the	O
digaonal	O
case	O
the	O
ml	O
estimate	O
of	O
is	O
the	O
same	O
as	O
m	O
l	O
except	O
the	O
off-diagonal	O
terms	O
are	O
zero	O
diag	O
diag	O
m	O
m	O
ld	O
d	O
in	O
the	O
scalar	O
case	O
completing	B
the	I
square	I
means	O
rewriting	O
as	O
ax	O
w	O
where	O
a	O
b	O
and	O
w	O
chapter	O
gaussian	B
models	O
exercise	O
gaussian	B
posterior	O
credible	B
interval	I
degroot	O
let	O
x	O
n	O
where	O
is	O
unknown	B
but	O
has	O
prior	O
n	O
seeing	O
n	O
samples	B
is	O
n	O
n	O
n	O
confidence	O
interval	O
how	O
big	O
does	O
n	O
have	O
to	O
be	O
to	O
ensure	O
the	O
posterior	O
after	O
is	O
called	O
a	O
credible	B
interval	I
and	O
is	O
the	O
bayesian	B
analog	O
of	O
a	O
n	O
ud	O
where	O
u	O
is	O
an	O
interval	O
on	O
n	O
of	O
width	O
and	O
d	O
is	O
the	O
data	O
hint	O
recall	B
that	O
of	O
the	O
probability	O
mass	O
of	O
a	O
gaussian	B
is	O
within	O
of	O
the	O
mean	B
exercise	O
map	O
estimation	O
for	O
gaussians	O
jaakkola	O
consider	O
samples	B
xn	O
from	O
a	O
gaussian	B
random	O
variable	O
with	O
known	O
variance	B
and	O
unknown	B
mean	B
we	O
further	O
assume	O
a	O
prior	O
distribution	O
gaussian	B
over	O
the	O
mean	B
n	O
with	O
fixed	O
mean	B
m	O
and	O
fixed	O
variance	B
thus	O
the	O
only	O
unknown	B
is	O
a	O
calculate	O
the	O
map	B
estimate	I
m	O
ap	O
you	O
can	O
state	B
the	O
result	O
without	O
proof	O
alternatively	O
with	O
a	O
lot	O
more	O
work	O
you	O
can	O
compute	O
derivatives	O
of	O
the	O
log	O
posterior	O
set	O
to	O
zero	O
and	O
solve	O
b	O
show	O
that	O
as	O
the	O
number	O
of	O
samples	B
n	O
increase	O
the	O
map	B
estimate	I
converges	O
to	O
the	O
maximum	B
likelihood	B
estimate	I
c	O
suppose	O
n	O
is	O
small	O
and	O
fixed	O
what	O
does	O
the	O
map	O
estimator	B
converge	B
to	O
if	O
we	O
increase	O
the	O
prior	O
variance	B
d	O
suppose	O
n	O
is	O
small	O
and	O
fixed	O
what	O
does	O
the	O
map	O
estimator	B
converge	B
to	O
if	O
we	O
decrease	O
the	O
prior	O
variance	B
exercise	O
sequential	B
updating	O
of	O
et	O
al	O
the	O
unbiased	B
estimates	O
for	O
the	O
covariance	B
of	O
a	O
d-dimensional	O
gaussian	B
based	O
on	O
n	O
samples	B
is	O
given	O
by	O
cn	O
n	O
mnxi	O
mnt	O
it	O
is	O
clear	O
that	O
it	O
takes	O
time	O
to	O
compute	O
cn	O
efficient	O
to	O
incrementally	O
update	O
these	O
estimates	O
than	O
to	O
recompute	O
from	O
scratch	O
if	O
the	O
data	O
points	O
arrive	O
one	O
at	O
a	O
time	O
it	O
is	O
more	O
a	O
show	O
that	O
the	O
covariance	B
can	O
be	O
sequentially	O
udpated	O
as	O
follows	O
mnt	O
cn	O
n	O
n	O
n	O
b	O
how	O
much	O
time	O
does	O
it	O
take	O
per	O
sequential	B
update	O
big-o	O
notation	O
c	O
show	O
that	O
we	O
can	O
sequentially	O
update	O
the	O
precision	B
matrix	I
using	O
c	O
n	O
n	O
n	O
c	O
c	O
n	O
mnt	O
c	O
n	O
mnt	O
c	O
n	O
mn	O
n	O
hint	O
notice	O
that	O
the	O
update	O
to	O
consists	O
of	O
adding	O
a	O
rank-one	O
matrix	O
namely	O
uut	O
where	O
u	O
mn	O
use	O
the	O
matrix	B
inversion	I
lemma	I
for	O
rank-one	O
updates	O
which	O
we	O
repeat	O
here	O
for	O
convenience	O
uvt	O
e	O
e	O
e	O
t	O
e	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
d	O
what	O
is	O
the	O
time	O
complexity	O
per	O
update	O
exercise	O
likelihood	B
ratio	I
for	O
gaussians	O
source	O
source	O
alpaydin	O
ex	O
consider	O
a	O
binary	O
classifier	O
where	O
the	O
k	O
class	O
conditional	O
densities	O
are	O
mvn	B
pxy	O
j	O
n	O
j	O
j	O
by	O
bayes	B
rule	I
we	O
have	O
py	O
py	O
log	O
log	O
pxy	O
pxy	O
log	O
py	O
py	O
in	O
other	O
words	O
the	O
log	O
posterior	O
ratio	O
is	O
the	O
log	O
likelihood	B
ratio	I
plus	O
the	O
log	O
prior	O
ratio	O
for	O
each	O
of	O
the	O
cases	O
in	O
the	O
table	O
below	O
derive	O
an	O
expression	O
for	O
the	O
log	O
likelihood	B
ratio	I
log	O
simplifying	O
as	O
much	O
as	O
possible	O
form	O
of	O
j	O
arbitrary	O
shared	B
shared	B
axis-aligned	O
j	O
with	O
ij	O
for	O
i	O
j	O
shared	B
spherical	B
cov	O
j	O
j	O
j	O
num	O
parameters	O
kdd	O
dd	O
d	O
exercise	O
ldaqda	O
on	O
heightweight	O
data	O
the	O
function	O
discrimanalysisheightweightdemo	O
fits	O
an	O
lda	B
and	O
qda	O
model	O
to	O
the	O
heightweight	O
data	O
compute	O
the	O
misclassification	O
rate	B
of	O
both	O
of	O
these	O
models	O
on	O
the	O
training	B
set	I
turn	O
in	O
your	O
numbers	O
and	O
code	O
exercise	O
naive	O
bayes	O
with	O
mixed	O
features	B
consider	O
a	O
class	O
naive	O
bayes	O
classifier	O
with	O
one	O
binary	O
feature	O
and	O
one	O
gaussian	B
feature	O
y	O
muy	O
c	O
c	O
c	O
n	O
c	O
c	O
let	O
the	O
parameter	B
vectors	O
be	O
as	O
follows	O
a	O
compute	O
result	O
should	O
be	O
a	O
vector	O
of	O
numbers	O
that	O
sums	O
to	O
b	O
compute	O
c	O
compute	O
d	O
explain	O
any	O
interesting	O
patterns	O
you	O
see	O
in	O
your	O
results	O
hint	O
look	O
at	O
the	O
parameter	B
vector	O
exercise	O
decision	B
boundary	I
for	O
lda	B
with	O
semi	O
tied	B
covariances	O
consider	O
a	O
generative	O
classifier	O
with	O
class	O
conditional	O
densities	O
of	O
the	O
form	O
n	O
c	O
c	O
in	O
lda	B
we	O
assume	O
c	O
and	O
in	O
qda	O
each	O
c	O
is	O
arbitrary	O
here	O
we	O
consider	O
the	O
class	O
case	O
in	O
which	O
k	O
for	O
k	O
that	O
is	O
the	O
gaussian	B
ellipsoids	O
have	O
the	O
same	O
shape	O
but	O
the	O
one	O
for	O
class	O
is	O
wider	O
derive	O
an	O
expression	O
for	O
py	O
simplifying	O
as	O
much	O
as	O
possible	O
give	O
a	O
geometric	O
interpretation	O
of	O
your	O
result	O
if	O
possible	O
exercise	O
logistic	B
regression	B
vs	O
ldaqda	O
jaakkola	O
suppose	O
we	O
train	O
the	O
following	O
binary	O
classifiers	O
via	O
maximum	O
likelihood	B
matrices	O
set	O
to	O
i	O
matrix	O
i	O
e	O
pxy	O
c	O
n	O
c	O
i	O
we	O
assume	O
py	O
is	O
uniform	O
a	O
gaussi	O
a	O
generative	O
classifier	O
where	O
the	O
class	O
conditional	O
densities	O
are	O
gaussian	B
with	O
both	O
covariance	B
b	O
gaussx	O
as	O
for	O
gaussi	O
but	O
the	O
covariance	B
matrices	O
are	O
unconstrained	O
i	O
e	O
pxy	O
c	O
n	O
c	O
c	O
chapter	O
gaussian	B
models	O
c	O
linlog	O
a	O
logistic	B
regression	B
model	O
with	O
linear	O
features	B
d	O
quadlog	O
a	O
logistic	B
regression	B
model	O
using	O
linear	O
and	O
quadratic	O
features	B
polynomial	O
basis	B
function	I
expansion	I
of	O
degree	B
after	O
training	O
we	O
compute	O
the	O
performance	O
of	O
each	O
model	O
m	O
on	O
the	O
training	B
set	I
as	O
follows	O
log	O
pyixi	O
m	O
lm	O
n	O
that	O
this	O
is	O
the	O
conditional	O
log-likelihood	O
pyx	O
and	O
not	O
the	O
joint	O
log-likelihood	O
py	O
x	O
we	O
now	O
want	O
to	O
compare	O
the	O
performance	O
of	O
each	O
model	O
we	O
will	O
write	O
lm	O
if	O
model	O
m	O
must	O
have	O
lower	O
equal	O
log	O
likelihood	B
the	O
training	B
set	I
than	O
for	O
any	O
training	B
set	I
other	O
words	O
m	O
is	O
worse	O
than	O
at	O
least	O
as	O
far	O
as	O
training	B
set	I
logprob	O
is	O
concerned	O
for	O
each	O
of	O
the	O
following	O
model	O
pairs	O
state	B
whether	O
lm	O
or	O
whether	O
no	O
such	O
statement	O
can	O
be	O
made	O
m	O
might	O
sometimes	O
be	O
better	O
than	O
and	O
sometimes	O
worse	O
also	O
for	O
each	O
question	O
briefly	O
sentences	O
explain	O
why	O
lm	O
a	O
gaussi	O
linlog	O
b	O
gaussx	O
quadlog	O
c	O
linlog	O
quadlog	O
d	O
gaussi	O
quadlog	O
e	O
now	O
suppose	O
we	O
measure	O
performance	O
in	O
terms	O
of	O
the	O
average	O
misclassification	O
rate	B
on	O
the	O
training	B
set	I
rm	O
n	O
iyi	O
yxi	O
is	O
it	O
true	O
in	O
general	O
that	O
lm	O
implies	O
that	O
rm	O
explain	O
why	O
or	O
why	O
not	O
exercise	O
gaussian	B
decision	B
boundaries	O
et	O
al	O
let	O
pxy	O
j	O
j	O
j	O
where	O
j	O
and	O
let	O
the	O
class	O
priors	O
be	O
equal	O
py	O
a	O
find	O
the	O
decision	B
region	O
px	O
px	O
sketch	O
the	O
result	O
hint	O
draw	O
the	O
curves	O
and	O
find	O
where	O
they	O
intersect	O
find	O
both	O
solutions	O
of	O
the	O
equation	O
px	O
px	O
hint	O
recall	B
that	O
to	O
solve	O
a	O
quadratic	O
equation	O
bx	O
c	O
we	O
use	O
b	O
x	O
b	O
now	O
suppose	O
all	O
other	O
parameters	O
remain	O
the	O
same	O
what	O
is	O
in	O
this	O
case	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	B
exercise	O
qda	O
with	O
classes	O
consider	O
a	O
three	O
category	O
classification	O
problem	O
let	O
the	O
prior	O
probabilites	O
p	O
the	O
class-conditional	O
densities	O
are	O
multivariate	B
normal	B
densities	O
with	O
parameters	O
classify	O
the	O
following	O
points	O
a	O
x	O
b	O
x	O
exercise	O
scalar	O
qda	O
you	O
can	O
solve	O
this	O
exercise	O
by	O
hand	O
or	O
using	O
a	O
computer	O
r	O
whatever	O
in	O
either	O
case	O
show	O
your	O
work	O
consider	O
the	O
following	O
training	B
set	I
of	O
heights	O
x	O
inches	O
and	O
gender	O
y	O
of	O
some	O
us	O
college	O
students	O
x	O
y	O
m	O
m	O
f	O
f	O
f	O
a	O
fit	O
a	O
bayes	O
classifier	O
to	O
this	O
data	O
using	O
maximum	O
likelihood	B
estimation	O
i	O
e	O
estimate	O
the	O
parameters	O
of	O
the	O
class	O
conditional	O
likelihoods	O
pxy	O
c	O
n	O
c	O
c	O
and	O
the	O
class	O
prior	O
py	O
c	O
c	O
what	O
are	O
your	O
values	O
of	O
c	O
c	O
c	O
for	O
c	O
m	O
f	O
show	O
your	O
work	O
you	O
can	O
get	O
partial	O
credit	O
if	O
you	O
make	O
an	O
arithmetic	O
error	O
b	O
compute	O
py	O
mx	O
where	O
x	O
and	O
are	O
the	O
mle	B
parameters	O
is	O
called	O
a	O
plug-in	B
prediction	O
c	O
what	O
would	O
be	O
a	O
simple	O
way	O
to	O
extend	O
this	O
technique	O
if	O
you	O
had	O
multiple	O
attributes	B
per	O
person	O
such	O
as	O
height	O
and	O
weight	O
write	O
down	O
your	O
proposed	O
model	O
as	O
an	O
equation	O
bayesian	B
statistics	I
introduction	O
we	O
have	O
now	O
seen	O
a	O
variety	O
of	O
different	O
probability	O
models	O
and	O
we	O
have	O
discussed	O
how	O
to	O
i	O
e	O
we	O
have	O
discussed	O
how	O
to	O
compute	O
map	O
parameter	B
estimates	O
fit	O
them	O
to	O
data	O
argmax	O
p	O
using	O
a	O
variety	O
of	O
different	O
priors	O
we	O
have	O
also	O
discussed	O
how	O
to	O
compute	O
the	O
full	B
posterior	O
p	O
as	O
well	O
as	O
the	O
posterior	B
predictive	B
density	I
pxd	O
for	O
certain	O
special	O
cases	O
in	O
later	O
chapters	O
we	O
will	O
discuss	O
algorithms	O
for	O
the	O
general	O
case	O
using	O
the	O
posterior	O
distribution	O
to	O
summarize	O
everything	O
we	O
know	O
about	O
a	O
set	O
of	O
unknown	B
variables	O
is	O
at	O
the	O
core	O
of	O
bayesian	B
statistics	I
in	O
this	O
chapter	O
we	O
discuss	O
this	O
approach	O
to	O
statistics	O
in	O
more	O
detail	O
in	O
chapter	O
we	O
discuss	O
an	O
alternative	O
approach	O
to	O
statistics	O
known	O
as	O
frequentist	B
or	O
classical	B
statistics	I
summarizing	O
posterior	O
distributions	O
the	O
posterior	O
p	O
summarizes	O
everything	O
we	O
know	O
about	O
the	O
unknown	B
quantities	O
in	O
this	O
section	O
we	O
discuss	O
some	O
simple	O
quantities	O
that	O
can	O
be	O
derived	O
from	O
a	O
probability	O
distribution	O
such	O
as	O
a	O
posterior	O
these	O
summary	O
statistics	O
are	O
often	O
easier	O
to	O
understand	O
and	O
visualize	O
than	O
the	O
full	B
joint	O
map	O
estimation	O
we	O
can	O
easily	O
compute	O
a	O
point	B
estimate	I
of	O
an	O
unknown	B
quantity	O
by	O
computing	O
the	O
posterior	B
mean	B
median	B
or	O
mode	B
in	O
section	O
we	O
discuss	O
how	O
to	O
use	O
decision	B
theory	O
to	O
choose	O
between	O
these	O
methods	O
typically	O
the	O
posterior	B
mean	B
or	O
median	B
is	O
the	O
most	O
appropriate	O
choice	O
for	O
a	O
realvalued	O
quantity	O
and	O
the	O
vector	O
of	O
posterior	O
marginals	O
is	O
the	O
best	O
choice	O
for	O
a	O
discrete	B
quantity	O
however	O
the	O
posterior	B
mode	B
aka	O
the	O
map	B
estimate	I
is	O
the	O
most	O
popular	O
choice	O
because	O
it	O
reduces	O
to	O
an	O
optimization	B
problem	O
for	O
which	O
efficient	O
algorithms	O
often	O
exist	O
futhermore	O
map	O
estimation	O
can	O
be	O
interpreted	O
in	O
non-bayesian	O
terms	O
by	O
thinking	O
of	O
the	O
log	O
prior	O
as	O
a	O
regularizer	O
section	O
for	O
more	O
details	O
although	O
this	O
approach	O
is	O
computationally	O
appealing	O
it	O
is	O
important	O
to	O
point	O
out	O
that	O
there	O
are	O
various	O
drawbacks	O
to	O
map	O
estimation	O
which	O
we	O
briefly	O
discuss	O
below	O
this	O
will	O
provide	O
motivation	O
for	O
the	O
more	O
thoroughly	O
bayesian	B
approach	O
which	O
we	O
will	O
study	O
later	O
in	O
this	O
chapter	O
elsewhere	O
in	O
this	O
book	O
chapter	O
bayesian	B
statistics	I
figure	O
a	O
bimodal	O
distribution	O
in	O
which	O
the	O
mode	B
is	O
very	O
untypical	O
of	O
the	O
distribution	O
the	O
thin	O
blue	O
vertical	O
line	O
is	O
the	O
mean	B
which	O
is	O
arguably	O
a	O
better	O
summary	O
of	O
the	O
distribution	O
since	O
it	O
is	O
near	O
the	O
majority	O
of	O
the	O
probability	O
mass	O
figure	O
generated	O
by	O
bimodaldemo	O
a	O
skewed	O
distribution	O
in	O
which	O
the	O
mode	B
is	O
quite	O
different	O
from	O
the	O
mean	B
figure	O
generated	O
by	O
gammaplotdemo	O
no	O
measure	O
of	O
uncertainty	B
the	O
most	O
obvious	O
drawback	O
of	O
map	O
estimation	O
and	O
indeed	O
of	O
any	O
other	O
point	B
estimate	I
such	O
as	O
the	O
posterior	B
mean	B
or	O
median	B
is	O
that	O
it	O
does	O
not	O
provide	O
any	O
measure	O
of	O
uncertainty	B
in	O
many	O
applications	O
it	O
is	O
important	O
to	O
know	O
how	O
much	O
one	O
can	O
trust	O
a	O
given	O
estimate	O
we	O
can	O
derive	O
such	O
confidence	O
measures	O
from	O
the	O
posterior	O
as	O
we	O
discuss	O
in	O
section	O
plugging	O
in	O
the	O
map	B
estimate	I
can	O
result	O
in	O
overfitting	O
in	O
machine	B
learning	B
we	O
often	O
care	O
more	O
about	O
predictive	B
accuracy	O
than	O
in	O
interpreting	O
the	O
parameters	O
of	O
our	O
models	O
however	O
if	O
we	O
don	O
t	O
model	O
the	O
uncertainty	B
in	O
our	O
parameters	O
then	O
our	O
predictive	B
distribution	O
will	O
be	O
overconfident	O
we	O
saw	O
several	O
examples	O
of	O
this	O
in	O
chapter	O
and	O
we	O
will	O
see	O
more	O
examples	O
later	O
overconfidence	O
in	O
predictions	O
is	O
particularly	O
problematic	O
in	O
situations	O
where	O
we	O
may	O
be	O
risk	B
averse	I
see	O
section	O
for	O
details	O
the	O
mode	B
is	O
an	O
untypical	O
point	O
choosing	O
the	O
mode	B
as	O
a	O
summary	O
of	O
a	O
posterior	O
distribution	O
is	O
often	O
a	O
very	O
poor	O
choice	O
since	O
the	O
mode	B
is	O
usually	O
quite	O
untypical	O
of	O
the	O
distribution	O
unlike	O
the	O
mean	B
or	O
median	B
this	O
is	O
illustrated	O
in	O
figure	O
for	O
a	O
continuous	O
space	O
the	O
basic	O
problem	O
is	O
that	O
the	O
mode	B
is	O
a	O
point	O
of	O
measure	O
zero	O
whereas	O
the	O
mean	B
and	O
median	B
take	O
the	O
volume	O
of	O
the	O
space	O
into	O
account	O
another	O
example	O
is	O
shown	O
in	O
figure	O
here	O
the	O
mode	B
is	O
but	O
the	O
mean	B
is	O
non-zero	O
such	O
skewed	O
distributions	O
often	O
arise	O
when	O
inferring	O
variance	B
parameters	O
especially	O
in	O
hierarchical	O
models	O
in	O
such	O
cases	O
the	O
map	B
estimate	I
hence	O
the	O
mle	B
is	O
obviously	O
a	O
very	O
bad	O
estimate	O
how	O
should	O
we	O
summarize	O
a	O
posterior	O
if	O
the	O
mode	B
is	O
not	O
a	O
good	O
choice	O
the	O
answer	O
is	O
to	O
use	O
decision	B
theory	O
which	O
we	O
discuss	O
in	O
section	O
the	O
basic	O
idea	O
is	O
to	O
specify	O
a	O
loss	B
function	I
where	O
l	O
is	O
the	O
loss	B
you	O
incur	O
if	O
the	O
truth	O
is	O
and	O
your	O
estimate	O
is	O
if	O
we	O
use	O
loss	B
l	O
i	O
then	O
the	O
optimal	O
estimate	O
is	O
the	O
posterior	B
mode	B
loss	B
means	O
you	O
only	O
get	O
points	O
if	O
you	O
make	O
no	O
errors	O
otherwise	O
you	O
get	O
nothing	O
there	O
is	O
no	O
partial	O
credit	O
under	O
summarizing	O
posterior	O
distributions	O
py	O
g	O
px	O
figure	O
example	O
of	O
the	O
transformation	O
of	O
a	O
density	O
under	O
a	O
nonlinear	O
transform	O
note	O
how	O
the	O
mode	B
of	O
the	O
transformed	O
distribution	O
is	O
not	O
the	O
transform	O
of	O
the	O
original	O
mode	B
based	O
on	O
exercise	O
of	O
figure	O
generated	O
by	O
bayeschangeofvar	O
this	O
loss	B
function	I
for	O
continuous-valued	O
quantities	O
we	O
often	O
prefer	O
to	O
use	O
squared	B
error	I
loss	B
l	O
the	O
corresponding	O
optimal	O
estimator	B
is	O
then	O
the	O
posterior	B
mean	B
as	O
we	O
show	O
in	O
section	O
or	O
we	O
can	O
use	O
a	O
more	O
robust	B
loss	B
function	I
l	O
which	O
gives	O
rise	O
to	O
the	O
posterior	B
median	B
map	O
estimation	O
is	O
not	O
invariant	B
to	O
reparameterization	O
a	O
more	O
subtle	O
problem	O
with	O
map	O
estimation	O
is	O
that	O
the	O
result	O
we	O
get	O
depends	O
on	O
how	O
we	O
parameterize	O
the	O
probability	O
distribution	O
changing	O
from	O
one	O
representation	O
to	O
another	O
equivalent	O
representation	O
changes	O
the	O
result	O
which	O
is	O
not	O
very	O
desirable	O
since	O
the	O
units	O
of	O
measurement	O
are	O
arbitrary	O
when	O
measuring	O
distance	O
we	O
can	O
use	O
centimetres	O
or	O
inches	O
to	O
understand	O
the	O
problem	O
suppose	O
we	O
compute	O
the	O
posterior	O
for	O
x	O
if	O
we	O
define	O
y	O
f	O
the	O
distribution	O
for	O
y	O
is	O
given	O
by	O
equation	O
which	O
we	O
repeat	O
here	O
for	O
convenience	O
dx	O
dy	O
pyy	O
pxx	O
the	O
dx	O
dy	O
term	O
is	O
called	O
the	O
jacobian	B
and	O
it	O
measures	O
the	O
change	O
in	O
size	O
of	O
a	O
unit	O
volume	O
passed	O
through	O
f	O
let	O
x	O
argmaxx	O
pxx	O
be	O
the	O
map	B
estimate	I
for	O
x	O
in	O
general	O
it	O
is	O
not	O
the	O
case	O
that	O
y	O
argmaxy	O
pyy	O
is	O
given	O
by	O
f	O
x	O
for	O
example	O
let	O
x	O
n	O
and	O
y	O
f	O
where	O
f	O
exp	O
x	O
we	O
can	O
derive	O
the	O
distribution	O
of	O
y	O
using	O
monte	B
carlo	I
simulation	O
section	O
the	O
result	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
original	O
gaussian	B
has	O
become	O
squashed	O
by	O
the	O
sigmoid	B
nonlinearity	O
in	O
particular	O
we	O
see	O
that	O
the	O
mode	B
of	O
the	O
transformed	O
distribution	O
is	O
not	O
equal	O
to	O
the	O
transform	O
of	O
the	O
original	O
mode	B
chapter	O
bayesian	B
statistics	I
to	O
see	O
how	O
this	O
problem	O
arises	O
in	O
the	O
context	O
of	O
map	O
estimation	O
consider	O
the	O
following	O
example	O
due	O
to	O
michael	O
jordan	O
the	O
bernoulli	B
distribution	O
is	O
typically	O
parameterized	O
by	O
its	O
mean	B
so	O
py	O
where	O
y	O
suppose	O
we	O
have	O
a	O
uniform	O
prior	O
on	O
the	O
unit	O
interval	O
p	O
if	O
there	O
is	O
no	O
data	O
the	O
map	B
estimate	I
is	O
just	O
the	O
mode	B
of	O
the	O
prior	O
which	O
can	O
be	O
anywhere	O
between	O
and	O
we	O
will	O
now	O
show	O
that	O
different	O
parameterizations	O
can	O
pick	O
different	O
points	O
in	O
this	O
interval	O
arbitrarily	O
first	O
let	O
so	O
the	O
new	O
prior	O
is	O
d	O
d	O
p	O
p	O
for	O
so	O
the	O
new	O
mode	B
is	O
m	O
ap	O
arg	O
max	O
now	O
let	O
the	O
new	O
prior	O
is	O
d	O
for	O
so	O
the	O
new	O
mode	B
is	O
p	O
p	O
m	O
ap	O
arg	O
max	O
d	O
thus	O
the	O
map	B
estimate	I
depends	O
on	O
the	O
parameterization	O
the	O
mle	B
does	O
not	O
suffer	O
from	O
this	O
since	O
the	O
likelihood	B
is	O
a	O
function	O
not	O
a	O
probability	O
density	O
bayesian	B
inference	B
does	O
not	O
suffer	O
from	O
this	O
problem	O
either	O
since	O
the	O
change	O
of	O
measure	O
is	O
taken	O
into	O
account	O
when	O
integrating	O
over	O
the	O
parameter	B
space	O
one	O
solution	O
to	O
the	O
problem	O
is	O
to	O
optimize	O
the	O
following	O
objective	O
function	O
pd	O
argmax	O
here	O
i	O
is	O
the	O
fisher	B
information	B
matrix	I
associated	O
with	O
px	O
section	O
this	O
estimate	O
is	O
parameterization	O
independent	O
for	O
reasons	O
explained	O
in	O
druilhet	O
and	O
marin	O
unfortunately	O
optimizing	O
equation	O
is	O
often	O
difficult	O
which	O
minimizes	O
the	O
appeal	O
of	O
the	O
whole	O
approach	O
credible	O
intervals	O
in	O
addition	O
to	O
point	O
estimates	O
we	O
often	O
want	O
a	O
measure	O
of	O
confidence	O
a	O
standard	O
measure	O
of	O
confidence	O
in	O
some	O
quantity	O
is	O
the	O
width	O
of	O
its	O
posterior	O
distribution	O
this	O
can	O
be	O
measured	O
using	O
a	O
credible	B
interval	I
which	O
is	O
a	O
region	O
c	O
u	O
for	O
lower	O
and	O
upper	O
which	O
contains	O
of	O
the	O
posterior	O
probability	O
mass	O
i	O
e	O
c	O
u	O
p	O
ud	O
there	O
may	O
be	O
many	O
such	O
intervals	O
so	O
we	O
choose	O
one	O
such	O
that	O
there	O
is	O
mass	O
in	O
each	O
tail	O
this	O
is	O
called	O
a	O
central	B
interval	I
summarizing	O
posterior	O
distributions	O
figure	O
central	B
interval	I
and	O
hpd	B
region	O
for	O
a	O
posterior	O
the	O
ci	B
is	O
and	O
the	O
hpd	B
is	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
betahpd	O
if	O
the	O
posterior	O
has	O
a	O
known	O
functional	O
form	O
we	O
can	O
compute	O
the	O
posterior	O
central	B
interval	I
using	O
f	O
and	O
u	O
f	O
where	O
f	O
is	O
the	O
cdf	B
of	O
the	O
posterior	O
for	O
example	O
if	O
the	O
posterior	O
is	O
gaussian	B
p	O
n	O
and	O
then	O
we	O
have	O
and	O
u	O
where	O
denotes	O
the	O
cdf	B
of	O
the	O
gaussian	B
this	O
is	O
illustrated	O
in	O
figure	O
this	O
justifies	O
the	O
common	O
practice	O
of	O
quoting	O
a	O
credible	B
interval	I
in	O
the	O
form	O
of	O
where	O
represents	O
the	O
posterior	B
mean	B
represents	O
the	O
posterior	O
standard	B
deviation	I
and	O
is	O
a	O
good	O
approximation	O
to	O
of	O
course	O
the	O
posterior	O
is	O
not	O
always	O
gaussian	B
for	O
example	O
in	O
our	O
coin	O
example	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
and	O
we	O
observe	O
heads	O
out	O
of	O
n	O
trials	O
then	O
the	O
posterior	O
is	O
a	O
beta	B
distribution	I
p	O
we	O
find	O
the	O
posterior	O
credible	B
interval	I
is	O
betacredibleint	O
for	O
the	O
one	O
line	O
of	O
matlab	O
code	O
we	O
used	O
to	O
compute	O
this	O
if	O
we	O
don	O
t	O
know	O
the	O
functional	O
form	O
but	O
we	O
can	O
draw	O
samples	B
from	O
the	O
posterior	O
then	O
we	O
can	O
use	O
a	O
monte	B
carlo	I
approximation	O
to	O
the	O
posterior	O
quantiles	O
we	O
simply	O
sort	O
the	O
s	O
samples	B
and	O
find	O
the	O
one	O
that	O
occurs	O
at	O
location	O
along	O
the	O
sorted	O
list	O
as	O
s	O
this	O
converges	O
to	O
the	O
true	O
quantile	B
see	O
mcquantiledemo	O
for	O
a	O
demo	O
people	O
often	O
confuse	O
bayesian	B
credible	O
intervals	O
with	O
frequentist	B
confidence	O
intervals	O
however	O
they	O
are	O
not	O
the	O
same	O
thing	O
as	O
we	O
discuss	O
in	O
section	O
in	O
general	O
credible	O
intervals	O
are	O
usually	O
what	O
people	O
want	O
to	O
compute	O
but	O
confidence	O
intervals	O
are	O
usually	O
what	O
they	O
actually	O
compute	O
because	O
most	O
people	O
are	O
taught	O
frequentist	B
statistics	I
but	O
not	O
bayesian	B
statistics	I
fortunately	O
the	O
mechanics	O
of	O
computing	O
a	O
credible	B
interval	I
is	O
just	O
as	O
easy	O
as	O
computing	O
a	O
confidence	O
interval	O
e	O
g	O
betacredibleint	O
for	O
how	O
to	O
do	O
it	O
in	O
matlab	O
highest	B
posterior	I
density	I
regions	O
a	O
problem	O
with	O
central	O
intervals	O
is	O
that	O
there	O
might	O
be	O
points	O
outside	O
the	O
ci	B
which	O
have	O
higher	O
probability	O
density	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
see	O
that	O
points	O
outside	O
the	O
left-most	O
ci	B
boundary	O
have	O
higher	O
density	O
than	O
those	O
just	O
inside	O
the	O
right-most	O
ci	B
boundary	O
this	O
motivates	O
an	O
alternative	O
quantity	O
known	O
as	O
the	O
highest	B
posterior	I
density	I
or	O
hpd	B
region	O
this	O
is	O
defined	O
as	O
the	O
of	O
most	O
probable	O
points	O
that	O
in	O
total	O
constitute	O
of	O
the	O
chapter	O
bayesian	B
statistics	I
pmin	O
figure	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
postdensityintervals	O
central	B
interval	I
and	O
hpd	B
region	O
for	O
a	O
hypothetical	O
multimodal	O
posterior	O
based	O
on	O
probability	O
mass	O
more	O
formally	O
we	O
find	O
the	O
threshold	O
p	O
p	O
and	O
then	O
define	O
the	O
hpd	B
as	O
c	O
p	O
p	O
on	O
the	O
pdf	B
such	O
that	O
in	O
the	O
hpd	B
region	O
is	O
sometimes	O
called	O
a	O
highest	B
density	I
interval	I
or	O
hdi	B
for	O
example	O
figure	O
shows	O
the	O
hdi	B
of	O
a	O
distribution	O
which	O
is	O
we	O
see	O
that	O
this	O
is	O
narrower	O
than	O
the	O
ci	B
even	O
though	O
it	O
still	O
contains	O
of	O
the	O
mass	O
furthermore	O
every	O
point	O
inside	O
of	O
it	O
has	O
higher	O
density	O
than	O
every	O
point	O
outside	O
of	O
it	O
for	O
a	O
unimodal	O
distribution	O
the	O
hdi	B
will	O
be	O
the	O
narrowest	O
interval	O
around	O
the	O
mode	B
containing	O
of	O
the	O
mass	O
to	O
see	O
this	O
imagine	O
water	O
filling	O
in	O
reverse	O
where	O
we	O
lower	O
the	O
level	O
until	O
of	O
the	O
mass	O
is	O
revealed	O
and	O
only	O
is	O
submerged	O
this	O
gives	O
a	O
simple	O
algorithm	O
for	O
computing	O
hdis	O
in	O
the	O
case	O
simply	O
search	O
over	O
points	O
such	O
that	O
the	O
interval	O
contains	O
of	O
the	O
mass	O
and	O
has	O
minimal	B
width	O
this	O
can	O
be	O
done	O
by	O
numerical	O
optimization	B
if	O
we	O
know	O
the	O
inverse	O
cdf	B
of	O
the	O
distribution	O
or	O
by	O
search	O
over	O
the	O
sorted	O
data	O
points	O
if	O
we	O
have	O
a	O
bag	O
of	O
samples	B
betahpd	O
for	O
a	O
demo	O
if	O
the	O
posterior	O
is	O
multimodal	O
the	O
hdi	B
may	O
not	O
even	O
be	O
a	O
connected	O
region	O
see	O
figure	O
for	O
an	O
example	O
however	O
summarizing	O
multimodal	O
posteriors	O
is	O
always	O
difficult	O
inference	B
for	O
a	O
difference	O
in	O
proportions	O
sometimes	O
we	O
have	O
multiple	O
parameters	O
and	O
we	O
are	O
interested	O
in	O
computing	O
the	O
posterior	O
distribution	O
of	O
some	O
function	O
of	O
these	O
parameters	O
for	O
example	O
suppose	O
you	O
are	O
about	O
to	O
buy	O
something	O
from	O
amazon	O
com	O
and	O
there	O
are	O
two	O
sellers	O
offering	O
it	O
for	O
the	O
same	O
price	O
seller	O
has	O
positive	O
reviews	O
and	O
negative	O
reviews	O
seller	O
has	O
positive	O
reviews	O
and	O
negative	O
reviews	O
who	O
should	O
you	O
buy	O
this	O
example	O
is	O
from	O
see	O
also	O
lingpipe-blog	O
c	O
bayesian	B
model	B
selection	I
p	O
p	O
f	O
d	O
p	O
exact	O
posteriors	O
p	O
idi	O
monte	B
carlo	I
approximation	O
to	O
p	O
we	O
use	O
kernel	B
density	O
figure	O
estimation	O
to	O
get	O
a	O
smooth	O
plot	O
the	O
vertical	O
lines	O
enclose	O
the	O
central	B
interval	I
figure	O
generated	O
by	O
amazonsellerdemo	O
on	O
the	O
face	O
of	O
it	O
you	O
should	O
pick	O
seller	O
but	O
we	O
cannot	O
be	O
very	O
confident	O
that	O
seller	O
is	O
better	O
since	O
it	O
has	O
had	O
so	O
few	O
reviews	O
in	O
this	O
section	O
we	O
sketch	O
a	O
bayesian	B
analysis	O
of	O
this	O
problem	O
similar	B
methodology	O
can	O
be	O
used	O
to	O
compare	O
rates	O
or	O
proportions	O
across	O
groups	O
for	O
a	O
variety	O
of	O
other	O
settings	O
let	O
and	O
be	O
the	O
unknown	B
reliabilities	O
of	O
the	O
two	O
sellers	O
since	O
we	O
don	O
t	O
know	O
much	O
about	O
them	O
we	O
ll	O
endow	O
them	O
both	O
with	O
uniform	O
priors	O
i	O
the	O
posteriors	O
are	O
p	O
and	O
p	O
let	O
us	O
define	O
as	O
the	O
difference	O
in	O
the	O
rates	O
we	O
might	O
want	O
to	O
work	O
in	O
terms	O
of	O
the	O
log-odds	B
ratio	I
we	O
can	O
compute	O
the	O
desired	O
quantity	O
using	O
numerical	O
integration	O
we	O
want	O
to	O
compute	O
p	O
for	O
convenience	O
p	O
i	O
beta	O
we	O
find	O
p	O
which	O
means	O
you	O
are	O
better	O
off	O
buying	O
from	O
seller	O
see	O
amazonsellerdemo	O
for	O
the	O
code	O
is	O
also	O
possible	O
to	O
solve	O
the	O
integral	O
analytically	O
a	O
simpler	O
way	O
to	O
solve	O
the	O
problem	O
is	O
to	O
approximate	O
the	O
posterior	O
p	O
by	O
monte	B
carlo	I
sampling	O
this	O
is	O
easy	O
since	O
and	O
are	O
independent	O
in	O
the	O
posterior	O
and	O
both	O
have	O
beta	O
distributions	O
which	O
can	O
be	O
sampled	O
from	O
using	O
standard	O
methods	O
the	O
distributions	O
p	O
idi	O
are	O
shown	O
in	O
figure	O
and	O
a	O
mc	O
approximation	O
to	O
p	O
together	O
with	O
a	O
hpd	B
is	O
shown	O
figure	O
an	O
mc	O
approximation	O
to	O
p	O
is	O
obtained	O
by	O
counting	O
the	O
fraction	O
of	O
samples	B
where	O
this	O
turns	O
out	O
to	O
be	O
which	O
is	O
very	O
close	O
to	O
the	O
exact	O
value	O
amazonsellerdemo	O
for	O
the	O
code	O
bayesian	B
model	B
selection	I
in	O
figure	O
we	O
saw	O
that	O
using	O
too	O
high	O
a	O
degree	B
polynomial	O
results	O
in	O
overfitting	O
and	O
using	O
too	O
low	O
a	O
degree	B
results	O
in	O
underfitting	O
similarly	O
in	O
figure	O
we	O
saw	O
that	O
using	O
too	O
small	O
chapter	O
bayesian	B
statistics	I
a	O
regularization	B
parameter	B
results	O
in	O
overfitting	O
and	O
too	O
large	O
a	O
value	O
results	O
in	O
underfitting	O
in	O
general	O
when	O
faced	O
with	O
a	O
set	O
of	O
models	O
families	O
of	O
parametric	O
distributions	O
of	O
different	O
complexity	O
how	O
should	O
we	O
choose	O
the	O
best	O
one	O
this	O
is	O
called	O
the	O
model	B
selection	I
problem	O
one	O
approach	O
is	O
to	O
use	O
cross-validation	O
to	O
estimate	O
the	O
generalization	B
error	I
of	O
all	O
the	O
candiate	O
models	O
and	O
then	O
to	O
pick	O
the	O
model	O
that	O
seems	O
the	O
best	O
however	O
this	O
requires	O
fitting	O
each	O
model	O
k	O
times	O
where	O
k	O
is	O
the	O
number	O
of	O
cv	B
folds	B
a	O
more	O
efficient	O
approach	O
is	O
to	O
compute	O
the	O
posterior	O
over	O
models	O
from	O
this	O
we	O
can	O
easily	O
compute	O
the	O
map	O
model	O
m	O
argmax	O
pmd	O
this	O
is	O
called	O
bayesian	B
model	B
selection	I
if	O
we	O
use	O
a	O
uniform	O
prior	O
over	O
models	O
pm	O
this	O
amounts	O
to	O
picking	O
the	O
model	O
which	O
pmd	O
pdmpm	O
m	O
m	O
pmd	O
maximizes	O
pdm	O
pd	O
this	O
quantity	O
is	O
called	O
the	O
marginal	B
likelihood	B
the	O
integrated	B
likelihood	B
or	O
the	O
evidence	B
for	O
model	O
m	O
the	O
details	O
on	O
how	O
to	O
perform	O
this	O
integral	O
will	O
be	O
discussed	O
in	O
section	O
but	O
first	O
we	O
give	O
an	O
intuitive	O
interpretation	O
of	O
what	O
this	O
quantity	O
means	O
bayesian	B
occam	O
s	O
razor	O
one	O
might	O
think	O
that	O
using	O
pdm	O
to	O
select	O
models	O
would	O
always	O
favor	O
the	O
model	O
with	O
the	O
most	O
parameters	O
this	O
is	O
true	O
if	O
we	O
use	O
pd	O
m	O
to	O
select	O
models	O
where	O
m	O
is	O
the	O
mle	B
or	O
map	B
estimate	I
of	O
the	O
parameters	O
for	O
model	O
m	O
because	O
models	O
with	O
more	O
parameters	O
will	O
fit	O
the	O
data	O
better	O
and	O
hence	O
achieve	O
higher	O
likelihood	B
however	O
if	O
we	O
integrate	B
out	I
the	O
parameters	O
rather	O
than	O
maximizing	O
them	O
we	O
are	O
automatically	O
protected	O
from	O
overfitting	O
models	O
with	O
more	O
parameters	O
do	O
not	O
necessarily	O
have	O
higher	O
marginal	B
likelihood	B
this	O
is	O
called	O
the	O
bayesian	B
occam	O
s	O
razor	O
effect	O
murray	O
and	O
ghahramani	O
named	O
after	O
the	O
principle	O
known	O
as	O
occam	O
s	O
razor	O
which	O
says	O
one	O
should	O
pick	O
the	O
simplest	O
model	O
that	O
adequately	O
explains	O
the	O
data	O
one	O
way	O
to	O
understand	O
the	O
bayesian	B
occam	O
s	O
razor	O
is	O
to	O
notice	O
that	O
the	O
marginal	B
likelihood	B
can	O
be	O
rewritten	O
as	O
follows	O
based	O
on	O
the	O
chain	B
rule	I
of	O
probability	O
pd	O
where	O
we	O
have	O
dropped	O
the	O
conditioning	B
on	O
x	O
for	O
brevity	O
this	O
is	O
similar	B
to	O
a	O
leave-one-out	O
cross-validation	O
estimate	O
of	O
the	O
likelihood	B
since	O
we	O
predict	O
each	O
future	O
point	O
given	O
all	O
the	O
previous	O
ones	O
course	O
the	O
order	O
of	O
the	O
data	O
does	O
not	O
matter	O
in	O
the	O
above	O
expression	O
if	O
a	O
model	O
is	O
too	O
complex	O
it	O
will	O
overfit	O
the	O
early	O
examples	O
and	O
will	O
then	O
predict	O
the	O
remaining	O
ones	O
poorly	O
another	O
way	O
to	O
understand	O
the	O
bayesian	B
occam	O
s	O
razor	O
effect	O
is	O
to	O
note	O
that	O
probabilities	O
must	O
where	O
the	O
sum	O
is	O
over	O
all	O
possible	O
data	O
sets	O
complex	O
sum	O
to	O
one	O
hence	O
models	O
which	O
can	O
predict	O
many	O
things	O
must	O
spread	O
their	O
probability	O
mass	O
thinly	O
and	O
hence	O
will	O
not	O
obtain	O
as	O
large	O
a	O
probability	O
for	O
any	O
given	O
data	O
set	O
as	O
simpler	O
models	O
this	O
is	O
sometimes	O
bayesian	B
model	B
selection	I
figure	O
a	O
schematic	O
illustration	O
of	O
the	O
bayesian	B
occam	O
s	O
razor	O
the	O
broad	O
curve	O
corresponds	O
to	O
a	O
complex	O
model	O
the	O
narrow	O
curve	O
to	O
a	O
simple	O
model	O
and	O
the	O
middle	O
curve	O
is	O
just	O
right	O
based	O
on	O
figure	O
of	O
see	O
also	O
and	O
ghahramani	O
figure	O
for	O
a	O
similar	B
plot	O
produced	O
on	O
real	O
data	O
called	O
the	O
conservation	B
of	I
probability	I
mass	I
principle	O
and	O
is	O
illustrated	O
in	O
figure	O
on	O
the	O
horizontal	O
axis	O
we	O
plot	O
all	O
possible	O
data	O
sets	O
in	O
order	O
of	O
increasing	O
complexity	O
in	O
some	O
abstract	O
sense	O
on	O
the	O
vertical	O
axis	O
we	O
plot	O
the	O
predictions	O
of	O
possible	O
models	O
a	O
simple	O
one	O
a	O
medium	O
one	O
and	O
a	O
complex	O
one	O
we	O
also	O
indicate	O
the	O
actually	O
observed	O
data	O
by	O
a	O
vertical	O
line	O
model	O
is	O
too	O
simple	O
and	O
assigns	O
low	O
probability	O
to	O
model	O
also	O
assigns	O
relatively	O
low	O
probability	O
because	O
it	O
can	O
predict	O
many	O
data	O
sets	O
and	O
hence	O
it	O
spreads	O
its	O
probability	O
quite	O
widely	O
and	O
thinly	O
model	O
is	O
just	O
right	O
it	O
predicts	O
the	O
observed	O
data	O
with	O
a	O
reasonable	O
degree	B
of	O
confidence	O
but	O
does	O
not	O
predict	O
too	O
many	O
other	O
things	O
hence	O
model	O
is	O
the	O
most	O
probable	O
model	O
as	O
a	O
concrete	O
example	O
of	O
the	O
bayesian	B
occam	O
s	O
razor	O
consider	O
the	O
data	O
in	O
figure	O
we	O
plot	O
polynomials	O
of	O
degrees	O
and	O
fit	O
to	O
n	O
data	O
points	O
it	O
also	O
shows	O
the	O
posterior	O
over	O
models	O
where	O
we	O
use	O
a	O
gaussian	B
prior	O
section	O
for	O
details	O
there	O
is	O
not	O
enough	O
data	O
to	O
justify	O
a	O
complex	O
model	O
so	O
the	O
map	O
model	O
is	O
d	O
figure	O
shows	O
what	O
happens	O
when	O
n	O
now	O
it	O
is	O
clear	O
that	O
d	O
is	O
the	O
right	O
model	O
data	O
was	O
in	O
fact	O
generated	O
from	O
a	O
quadratic	O
as	O
another	O
example	O
figure	O
plots	O
log	O
pd	O
vs	O
log	O
for	O
the	O
polynomial	O
ridge	B
regression	B
model	O
where	O
ranges	O
over	O
the	O
same	O
set	O
of	O
values	O
used	O
in	O
the	O
cv	B
experiment	O
we	O
see	O
that	O
the	O
maximum	O
evidence	B
occurs	O
at	O
roughly	O
the	O
same	O
point	O
as	O
the	O
minimum	O
of	O
the	O
test	O
mse	B
which	O
also	O
corresponds	O
to	O
the	O
point	O
chosen	O
by	O
cv	B
when	O
using	O
the	O
bayesian	B
approach	O
we	O
are	O
not	O
restricted	O
to	O
evaluating	O
the	O
evidence	B
at	O
a	O
argmax	O
pd	O
finite	O
grid	O
of	O
values	O
instead	O
we	O
can	O
use	O
numerical	O
optimization	B
to	O
find	O
this	O
technique	O
is	O
called	O
empirical	B
bayes	I
or	O
type	B
ii	I
maximum	I
likelihood	B
section	O
for	O
details	O
an	O
example	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
curve	O
has	O
a	O
similar	B
shape	O
to	O
the	O
cv	B
estimate	O
but	O
it	O
can	O
be	O
computed	O
more	O
efficiently	O
logev	O
eb	B
logev	O
eb	B
chapter	O
bayesian	B
statistics	I
d	O
m	O
p	O
methodeb	O
m	O
logev	O
eb	B
we	O
plot	O
polynomials	O
of	O
degrees	O
and	O
fit	O
to	O
n	O
data	O
points	O
using	O
empirical	O
figure	O
bayes	O
the	O
solid	O
green	O
curve	O
is	O
the	O
true	O
function	O
the	O
dashed	O
red	O
curve	O
is	O
the	O
prediction	O
blue	O
lines	O
represent	O
around	O
the	O
mean	B
we	O
plot	O
the	O
posterior	O
over	O
models	O
pdd	O
assuming	O
a	O
uniform	O
prior	O
pd	O
based	O
on	O
a	O
figure	O
by	O
zoubin	O
ghahramani	O
figure	O
generated	O
by	O
linregebmodelselvsn	O
computing	O
the	O
marginal	B
likelihood	B
when	O
discussing	O
parameter	B
inference	B
for	O
a	O
fixed	O
model	O
we	O
often	O
wrote	O
p	O
m	O
p	O
m	O
thus	O
ignoring	O
the	O
normalization	O
constant	O
pdm	O
this	O
is	O
valid	O
since	O
pdm	O
is	O
constant	O
wrt	O
however	O
when	O
comparing	O
models	O
we	O
need	O
to	O
know	O
how	O
to	O
compute	O
the	O
marginal	B
likelihood	B
pdm	O
in	O
general	O
this	O
can	O
be	O
quite	O
hard	O
since	O
we	O
have	O
to	O
integrate	O
over	O
all	O
possible	O
parameter	B
values	O
but	O
when	O
we	O
have	O
a	O
conjugate	B
prior	I
it	O
is	O
easy	O
to	O
compute	O
as	O
we	O
now	O
show	O
let	O
p	O
be	O
our	O
prior	O
where	O
q	O
is	O
an	O
unnormalized	O
distribution	O
and	O
is	O
the	O
normalization	O
constant	O
of	O
the	O
prior	O
let	O
pd	O
be	O
the	O
likelihood	B
where	O
contains	O
any	O
constant	O
factors	B
in	O
the	O
likelihood	B
finally	O
let	O
p	O
q	O
be	O
our	O
poste	O
bayesian	B
model	B
selection	I
logev	O
eb	B
logev	O
eb	B
d	O
m	O
p	O
logev	O
eb	B
methodeb	O
m	O
figure	O
same	O
as	O
figure	O
except	O
now	O
n	O
figure	O
generated	O
by	O
linregebmodelselvsn	O
rior	O
where	O
q	O
qd	O
is	O
the	O
unnormalized	O
posterior	O
and	O
zn	O
is	O
the	O
normalization	O
constant	O
of	O
the	O
posterior	O
we	O
have	O
p	O
q	O
zn	O
pd	O
pd	O
pd	O
qd	O
zn	O
so	O
assuming	O
the	O
relevant	O
normalization	O
constants	O
are	O
tractable	O
we	O
have	O
an	O
easy	O
way	O
to	O
compute	O
the	O
marginal	B
likelihood	B
we	O
give	O
some	O
examples	O
below	O
pd	O
pd	O
n	O
ba	O
b	O
pd	O
ba	O
b	O
a	O
n	O
n	O
n	O
pd	O
ba	O
b	O
ba	O
b	O
ba	O
b	O
so	O
ba	O
b	O
pd	O
n	O
chapter	O
bayesian	B
statistics	I
beta-binomial	B
model	O
let	O
us	O
apply	O
the	O
above	O
result	O
to	O
the	O
beta-binomial	B
model	O
since	O
we	O
know	O
p	O
beta	O
where	O
b	O
we	O
know	O
the	O
normalization	O
constant	O
of	O
the	O
posterior	O
is	O
a	O
and	O
hence	O
p	O
pd	O
the	O
marginal	B
likelihood	B
for	O
the	O
beta-bernoulli	O
model	O
is	O
the	O
same	O
as	O
above	O
except	O
it	O
is	O
missing	B
the	O
term	O
dirichlet-multinoulli	O
model	O
by	O
the	O
same	O
reasoning	O
as	O
the	O
beta-bernoulli	O
case	O
one	O
can	O
show	O
that	O
the	O
marginal	B
likelihood	B
for	O
the	O
dirichlet-multinoulli	O
model	O
is	O
given	O
by	O
pd	O
bn	O
b	O
where	O
b	O
k	O
k	O
k	O
hence	O
we	O
can	O
rewrite	O
the	O
above	O
result	O
in	O
the	O
following	O
form	O
which	O
is	O
what	O
is	O
usually	O
presented	O
in	O
the	O
literature	O
k	O
k	O
k	O
k	O
pd	O
k	O
k	O
k	O
we	O
will	O
see	O
many	O
applications	O
of	O
this	O
equation	O
later	O
gaussian-gaussian-wishart	O
model	O
consider	O
the	O
case	O
of	O
an	O
mvn	B
with	O
a	O
conjugate	O
niw	B
prior	O
let	O
be	O
the	O
normalizer	O
for	O
the	O
prior	O
zn	O
be	O
normalizer	O
for	O
the	O
posterior	O
and	O
let	O
zl	O
be	O
the	O
normalizer	O
for	O
the	O
bayesian	B
model	B
selection	I
likelihood	B
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
pd	O
zn	O
n	O
n	O
d	O
n	O
d	O
n	O
d	O
n	O
d	O
n	O
n	O
n	O
this	O
equation	O
will	O
prove	O
useful	O
later	O
bic	B
approximation	O
to	O
log	O
marginal	B
likelihood	B
in	O
general	O
computing	O
the	O
integral	O
in	O
equation	O
can	O
be	O
quite	O
difficult	O
one	O
simple	O
but	O
popular	O
approximation	O
is	O
known	O
as	O
the	O
bayesian	B
information	B
criterion	I
or	O
bic	B
which	O
has	O
the	O
following	O
form	O
bic	B
log	O
pd	O
dof	O
log	O
n	O
log	O
pd	O
where	O
dof	O
is	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
model	O
and	O
is	O
the	O
mle	B
for	O
the	O
we	O
see	O
that	O
this	O
has	O
the	O
form	O
of	O
a	O
penalized	B
log	I
likelihood	B
where	O
the	O
penalty	O
term	O
depends	O
on	O
the	O
model	O
s	O
complexity	O
see	O
section	O
for	O
the	O
derivation	O
of	O
the	O
bic	B
score	O
as	O
an	O
example	O
consider	O
linear	B
regression	B
as	O
we	O
show	O
in	O
section	O
the	O
mle	B
is	O
given	O
by	O
w	O
the	O
corresponding	O
y	O
and	O
rssn	O
where	O
rss	O
wt	O
x	O
log	O
likelihood	B
is	O
given	O
by	O
log	O
pd	O
n	O
n	O
hence	O
the	O
bic	B
score	O
is	O
as	O
follows	O
constant	O
terms	O
bic	B
n	O
log	O
d	O
logn	O
where	O
d	O
is	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	O
use	O
an	O
alternative	O
definition	O
of	O
bic	B
which	O
we	O
call	O
the	O
bic	B
cost	O
we	O
want	O
to	O
minimize	O
it	O
in	O
the	O
statistics	O
literature	O
it	O
is	O
common	O
to	O
bic-cost	O
log	O
pd	O
dof	O
log	O
n	O
log	O
pd	O
in	O
the	O
context	O
of	O
linear	B
regression	B
this	O
becomes	O
bic-cost	O
n	O
log	O
logn	O
traditionally	O
the	O
bic	B
score	O
is	O
defined	O
using	O
the	O
ml	O
estimate	O
so	O
it	O
is	O
independent	O
of	O
the	O
prior	O
however	O
for	O
models	O
such	O
as	O
mixtures	O
of	O
gaussians	O
the	O
ml	O
estimate	O
can	O
be	O
poorly	O
behaved	O
so	O
it	O
is	O
better	O
to	O
evaluate	O
the	O
bic	B
score	O
using	O
the	O
map	B
estimate	I
as	O
in	O
and	O
raftery	O
chapter	O
bayesian	B
statistics	I
the	O
bic	B
method	O
is	O
very	O
closely	O
related	O
to	O
the	O
minimum	B
description	I
length	I
or	O
mdl	B
principle	O
which	O
characterizes	O
the	O
score	O
for	O
a	O
model	O
in	O
terms	O
of	O
how	O
well	O
it	O
fits	O
the	O
data	O
minus	O
how	O
complex	O
the	O
model	O
is	O
to	O
define	O
see	O
and	O
yu	O
for	O
details	O
there	O
is	O
a	O
very	O
similar	B
expression	O
to	O
bic	B
mdl	B
called	O
the	O
akaike	B
information	B
criterion	I
or	O
aic	B
defined	O
as	O
aicmd	O
log	O
pd	O
m	O
le	O
dofm	O
this	O
is	O
derived	O
from	O
a	O
frequentist	B
framework	O
and	O
cannot	O
be	O
interpreted	O
as	O
an	O
approximation	O
to	O
the	O
marginal	B
likelihood	B
nevertheless	O
the	O
form	O
of	O
this	O
expression	O
is	O
very	O
similar	B
to	O
bic	B
we	O
see	O
that	O
the	O
penalty	O
for	O
aic	B
is	O
less	O
than	O
for	O
bic	B
this	O
causes	O
aic	B
to	O
pick	O
more	O
complex	O
models	O
however	O
this	O
can	O
result	O
in	O
better	O
predictive	B
accuracy	O
see	O
e	O
g	O
et	O
al	O
sec	O
for	O
further	O
discussion	O
on	O
such	O
information	B
criteria	O
effect	O
of	O
the	O
prior	O
sometimes	O
it	O
is	O
not	O
clear	O
how	O
to	O
set	O
the	O
prior	O
when	O
we	O
are	O
performing	O
posterior	O
inference	B
the	O
details	O
of	O
the	O
prior	O
may	O
not	O
matter	O
too	O
much	O
since	O
the	O
likelihood	B
often	O
overwhelms	O
the	O
prior	O
anyway	O
but	O
when	O
computing	O
the	O
marginal	B
likelihood	B
the	O
prior	O
plays	O
a	O
much	O
more	O
important	O
role	O
since	O
we	O
are	O
averaging	O
the	O
likelihood	B
over	O
all	O
possible	O
parameter	B
settings	O
as	O
weighted	O
by	O
the	O
prior	O
in	O
figures	O
and	O
where	O
we	O
demonstrated	O
model	B
selection	I
for	O
linear	B
regression	B
we	O
used	O
a	O
prior	O
of	O
the	O
form	O
pw	O
n	O
here	O
is	O
a	O
tuning	O
parameter	B
that	O
controls	O
how	O
strong	O
the	O
prior	O
is	O
this	O
parameter	B
can	O
have	O
a	O
large	O
effect	O
as	O
we	O
discuss	O
in	O
section	O
intuitively	O
if	O
is	O
large	O
the	O
weights	O
are	O
forced	O
to	O
be	O
small	O
so	O
we	O
need	O
to	O
use	O
a	O
complex	O
model	O
with	O
many	O
small	O
parameters	O
a	O
high	O
degree	B
polynomial	O
to	O
fit	O
the	O
data	O
conversely	O
if	O
is	O
small	O
we	O
will	O
favor	O
simpler	O
models	O
since	O
each	O
parameter	B
is	O
allowed	O
to	O
vary	O
in	O
magnitude	O
by	O
a	O
lot	O
if	O
the	O
prior	O
is	O
unknown	B
the	O
correct	O
bayesian	B
procedure	O
is	O
to	O
put	O
a	O
prior	O
on	O
the	O
prior	O
that	O
is	O
we	O
should	O
put	O
a	O
prior	O
on	O
the	O
hyper-parameter	O
as	O
well	O
as	O
the	O
parametrs	O
w	O
to	O
compute	O
the	O
marginal	B
likelihood	B
we	O
should	O
integrate	B
out	I
all	O
unknowns	O
i	O
e	O
we	O
should	O
compute	O
pdm	O
pdwpw	O
mp	O
of	O
course	O
this	O
requires	O
specifying	O
the	O
hyper-prior	O
fortunately	O
the	O
higher	O
up	O
we	O
go	O
in	O
the	O
bayesian	B
hierarchy	O
the	O
less	O
sensitive	O
are	O
the	O
results	O
to	O
the	O
prior	O
settings	O
so	O
we	O
can	O
usually	O
make	O
the	O
hyper-prior	O
uninformative	B
a	O
computational	O
shortcut	O
is	O
to	O
optimize	O
rather	O
than	O
integrating	O
it	O
out	O
that	O
is	O
we	O
use	O
pdm	O
pdwpw	O
mdw	O
pd	O
m	O
argmax	O
where	O
argmax	O
pdwpw	O
mdw	O
this	O
approach	O
is	O
called	O
empirical	B
bayes	I
and	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
this	O
is	O
the	O
method	O
used	O
in	O
figures	O
and	O
bayesian	B
model	B
selection	I
bayes	B
factor	B
bf	O
bf	O
bf	O
bf	O
bf	O
bf	O
bf	O
bf	O
bf	O
interpretation	O
decisive	O
evidence	B
for	O
strong	O
evidence	B
for	O
moderate	O
evidence	B
for	O
weak	O
evidence	B
for	O
weak	O
evidence	B
for	O
moderate	O
evidence	B
for	O
strong	O
evidence	B
for	O
decisive	O
evidence	B
for	O
table	O
jeffreys	O
scale	B
of	I
evidence	B
for	O
interpreting	O
bayes	O
factors	B
bayes	O
factors	B
suppose	O
our	O
prior	O
on	O
models	O
is	O
uniform	O
pm	O
then	O
model	B
selection	I
is	O
equivalent	O
to	O
picking	O
the	O
model	O
with	O
the	O
highest	O
marginal	B
likelihood	B
now	O
suppose	O
we	O
just	O
have	O
two	O
models	O
we	O
are	O
considering	O
call	O
them	O
the	O
null	B
hypothesis	I
and	O
the	O
alternative	B
hypothesis	I
define	O
the	O
bayes	B
factor	B
as	O
the	O
ratio	O
of	O
marginal	O
likelihoods	O
is	O
like	O
a	O
likelihood	B
ratio	I
except	O
we	O
integrate	B
out	I
the	O
parameters	O
which	O
allows	O
us	O
to	O
if	O
then	O
we	O
prefer	O
model	O
otherwise	O
we	O
compare	O
models	O
of	O
different	O
complexity	O
prefer	O
model	O
of	O
course	O
it	O
might	O
be	O
that	O
is	O
only	O
slightly	O
greater	O
than	O
in	O
that	O
case	O
we	O
are	O
not	O
very	O
confident	O
that	O
model	O
is	O
better	O
jeffreys	O
proposed	O
a	O
scale	B
of	I
evidence	B
for	O
interpreting	O
the	O
magnitude	O
of	O
a	O
bayes	B
factor	B
which	O
is	O
shown	O
in	O
table	O
this	O
is	O
a	O
bayesian	B
alternative	O
to	O
the	O
frequentist	B
concept	B
of	O
a	O
alternatively	O
we	O
can	O
just	O
convert	O
the	O
bayes	B
factor	B
to	O
a	O
posterior	O
over	O
models	O
if	O
we	O
have	O
example	O
testing	O
if	O
a	O
coin	O
is	O
fair	O
suppose	O
we	O
observe	O
some	O
coin	O
tosses	O
and	O
want	O
to	O
decide	O
if	O
the	O
data	O
was	O
generated	O
by	O
a	O
fair	O
coin	O
or	O
a	O
potentially	O
biased	O
coin	O
where	O
could	O
be	O
any	O
value	O
in	O
let	O
us	O
denote	O
the	O
first	O
model	O
by	O
and	O
the	O
second	O
model	O
by	O
the	O
marginal	B
likelihood	B
under	O
is	O
simply	O
a	O
p-value	B
is	O
defined	O
as	O
the	O
probability	O
the	O
null	B
hypothesis	I
of	O
observing	O
some	O
test	B
statistic	I
f	O
as	O
the	O
chi-squared	B
statistic	I
that	O
is	O
as	O
large	O
or	O
larger	O
than	O
that	O
actually	O
observed	O
i	O
e	O
pvalued	O
p	O
d	O
f	O
d	O
note	O
that	O
has	O
almost	O
nothing	O
to	O
do	O
with	O
what	O
we	O
really	O
want	O
to	O
know	O
which	O
is	O
chapter	O
bayesian	B
statistics	I
bic	B
approximation	O
to	O
figure	O
log	O
marginal	B
likelihood	B
for	O
the	O
coins	O
example	O
bic	B
approximation	O
figure	O
generated	O
by	O
coinsmodelseldemo	O
where	O
n	O
is	O
the	O
number	O
of	O
coin	O
tosses	O
the	O
marginal	B
likelihood	B
under	O
using	O
a	O
beta	O
prior	O
is	O
b	O
b	O
pd	O
we	O
plot	O
log	O
vs	O
the	O
number	O
of	O
heads	O
in	O
figure	O
assuming	O
n	O
and	O
shape	O
of	O
the	O
curve	O
is	O
not	O
very	O
sensitive	O
to	O
and	O
as	O
long	O
as	O
if	O
we	O
observe	O
or	O
heads	O
the	O
unbiased	B
coin	O
hypothesis	O
is	O
more	O
likely	O
than	O
since	O
is	O
a	O
simpler	O
model	O
has	O
no	O
free	O
parameters	O
it	O
would	O
be	O
a	O
suspicious	B
coincidence	I
if	O
the	O
coin	O
were	O
biased	O
but	O
happened	O
to	O
produce	O
almost	O
exactly	O
headstails	O
however	O
as	O
the	O
counts	O
become	O
more	O
extreme	O
we	O
favor	O
the	O
biased	O
coin	O
hypothesis	O
note	O
that	O
if	O
we	O
plot	O
the	O
log	O
bayes	B
factor	B
log	O
it	O
will	O
have	O
exactly	O
the	O
same	O
shape	O
since	O
log	O
is	O
a	O
constant	O
see	O
also	O
exercise	O
in	O
figure	O
shows	O
the	O
bic	B
approximation	O
to	O
log	O
for	O
our	O
biased	O
coin	O
example	O
from	O
section	O
we	O
see	O
that	O
the	O
curve	O
has	O
approximately	O
the	O
same	O
shape	O
as	O
the	O
exact	O
log	O
marginal	B
likelihood	B
which	O
is	O
all	O
that	O
matters	O
for	O
model	B
selection	I
purposes	O
since	O
the	O
absolute	O
scale	O
is	O
irrelevant	O
in	O
particular	O
it	O
favors	O
the	O
simpler	O
model	O
unless	O
the	O
data	O
is	O
overwhelmingly	O
in	O
support	B
of	O
the	O
more	O
complex	O
model	O
jeffreys-lindley	B
paradox	I
problems	O
can	O
arise	O
when	O
we	O
use	O
improper	O
priors	O
priors	O
that	O
do	O
not	O
integrate	O
to	O
for	O
model	B
selection	I
hypothesis	O
testing	O
even	O
though	O
such	O
priors	O
may	O
be	O
acceptable	O
for	O
other	O
purposes	O
for	O
example	O
consider	O
testing	O
the	O
hypotheses	O
vs	O
to	O
define	O
the	O
marginal	O
density	O
on	O
we	O
use	O
the	O
following	O
mixture	B
model	I
p	O
priors	O
this	O
is	O
only	O
meaningful	O
if	O
p	O
and	O
p	O
are	O
proper	O
density	O
functions	O
in	O
this	O
case	O
the	O
posterior	O
is	O
given	O
by	O
pd	O
pd	O
pd	O
now	O
suppose	O
we	O
use	O
improper	O
priors	O
p	O
and	O
p	O
then	O
pd	O
pd	O
pd	O
pd	O
is	O
the	O
integrated	O
or	O
marginal	B
likelihood	B
for	O
model	O
i	O
now	O
let	O
i	O
where	O
hence	O
thus	O
we	O
can	O
change	O
the	O
posterior	O
arbitrarily	O
by	O
choosing	O
and	O
as	O
we	O
please	O
note	O
that	O
using	O
proper	O
but	O
very	O
vague	O
priors	O
can	O
cause	O
similar	B
problems	O
in	O
particular	O
the	O
bayes	B
factor	B
will	O
always	O
favor	O
the	O
simpler	O
model	O
since	O
the	O
probability	O
of	O
the	O
observed	O
data	O
under	O
a	O
complex	O
model	O
with	O
a	O
very	O
diffuse	O
prior	O
will	O
be	O
very	O
small	O
this	O
is	O
called	O
the	O
jeffreys-lindley	B
paradox	I
thus	O
it	O
is	O
important	O
to	O
use	O
proper	O
priors	O
when	O
performing	O
model	B
selection	I
note	O
however	O
that	O
if	O
and	O
share	O
the	O
same	O
prior	O
over	O
a	O
subset	O
of	O
the	O
parameters	O
this	O
part	O
of	O
the	O
prior	O
can	O
be	O
improper	O
since	O
the	O
corresponding	O
normalization	O
constant	O
will	O
cancel	O
out	O
priors	O
the	O
most	O
controversial	O
aspect	O
of	O
bayesian	B
statistics	I
is	O
its	O
reliance	O
on	O
priors	O
bayesians	O
argue	O
this	O
is	O
unavoidable	O
since	O
nobody	O
is	O
a	O
tabula	B
rasa	I
or	O
blank	B
slate	I
all	O
inference	B
must	O
be	O
done	O
conditional	O
on	O
certain	O
assumptions	O
about	O
the	O
world	O
nevertheless	O
one	O
might	O
be	O
interested	O
in	O
minimizing	O
the	O
impact	O
of	O
one	O
s	O
prior	O
assumptions	O
we	O
briefly	O
discuss	O
some	O
ways	O
to	O
do	O
this	O
below	O
uninformative	B
priors	O
if	O
we	O
don	O
t	O
have	O
strong	O
beliefs	O
about	O
what	O
should	O
be	O
it	O
is	O
common	O
to	O
use	O
an	O
uninformative	B
or	O
non-informative	B
prior	O
and	O
to	O
let	O
the	O
data	O
speak	O
for	O
itself	O
the	O
issue	O
of	O
designing	O
uninformative	B
priors	O
is	O
actually	O
somewhat	O
tricky	O
as	O
an	O
example	O
of	O
the	O
difficulty	O
consider	O
a	O
bernoulli	B
parameter	B
one	O
might	O
think	O
that	O
the	O
most	O
uninformative	B
prior	O
would	O
be	O
the	O
uniform	B
distribution	I
but	O
the	O
posterior	B
mean	B
in	O
this	O
case	O
is	O
e	O
hence	O
one	O
could	O
argue	O
that	O
the	O
prior	O
wasn	O
t	O
completely	O
uninformative	B
after	O
all	O
whereas	O
the	O
mle	B
is	O
chapter	O
bayesian	B
statistics	I
clearly	O
by	O
decreasing	O
the	O
magnitude	O
of	O
the	O
pseudo	B
counts	I
we	O
can	O
lessen	O
the	O
impact	O
of	O
the	O
prior	O
by	O
the	O
above	O
argument	O
the	O
most	O
non-informative	B
prior	O
is	O
betac	O
c	O
lim	O
c	O
which	O
is	O
a	O
mixture	B
of	O
two	O
equal	O
point	O
masses	O
at	O
and	O
and	O
lu	O
this	O
is	O
also	O
called	O
the	O
haldane	B
prior	I
note	O
that	O
the	O
haldane	B
prior	I
is	O
an	O
improper	B
prior	I
meaning	O
it	O
does	O
not	O
integrate	O
to	O
however	O
as	O
long	O
as	O
we	O
see	O
at	O
least	O
one	O
head	O
and	O
at	O
least	O
one	O
tail	O
the	O
posterior	O
will	O
be	O
proper	O
in	O
section	O
we	O
will	O
argue	O
that	O
the	O
right	O
uninformative	B
prior	O
is	O
in	O
fact	O
beta	O
clearly	O
the	O
difference	O
in	O
practice	O
between	O
these	O
three	O
priors	O
is	O
very	O
likely	O
negligible	O
in	O
general	O
it	O
is	O
advisable	O
to	O
perform	O
some	O
kind	O
of	O
sensitivity	B
analysis	I
in	O
which	O
one	O
checks	O
how	O
much	O
one	O
s	O
conclusions	O
or	O
predictions	O
change	O
in	O
response	O
to	O
change	O
in	O
the	O
modeling	O
assumptions	O
which	O
includes	O
the	O
choice	O
of	O
prior	O
but	O
also	O
the	O
choice	O
of	O
likelihood	B
and	O
any	O
kind	O
of	O
data	O
preprocessing	O
if	O
the	O
conclusions	O
are	O
relatively	O
insensitive	O
to	O
the	O
modeling	O
assumptions	O
one	O
can	O
have	O
more	O
confidence	O
in	O
the	O
results	O
jeffreys	O
priors	O
harold	O
designed	O
a	O
general	O
purpose	O
technique	O
for	O
creating	O
non-informative	B
priors	O
the	O
result	O
is	O
known	O
as	O
the	O
jeffreys	B
prior	I
the	O
key	O
observation	B
is	O
that	O
if	O
p	O
is	O
non-informative	B
then	O
any	O
re-parameterization	O
of	O
the	O
prior	O
such	O
as	O
h	O
for	O
some	O
function	O
h	O
should	O
also	O
be	O
non-informative	B
now	O
by	O
the	O
change	B
of	I
variables	I
formula	O
p	O
p	O
d	O
d	O
so	O
the	O
prior	O
will	O
in	O
general	O
change	O
however	O
let	O
us	O
pick	O
p	O
i	O
e	O
where	O
i	O
is	O
the	O
fisher	B
information	B
d	O
log	O
px	O
d	O
this	O
is	O
a	O
measure	O
of	O
curvature	O
of	O
the	O
expected	O
negative	B
log	I
likelihood	B
and	O
hence	O
a	O
measure	O
of	O
stability	O
of	O
the	O
mle	B
section	O
now	O
d	O
log	O
px	O
d	O
d	O
log	O
px	O
d	O
d	O
d	O
squaring	O
and	O
taking	O
expectations	O
over	O
x	O
we	O
have	O
i	O
i	O
e	O
i	O
i	O
d	O
log	O
px	O
d	O
d	O
d	O
d	O
d	O
harold	O
jeffreys	O
was	O
an	O
english	O
mathematician	O
statistician	O
geophysicist	O
and	O
astronomer	O
priors	O
so	O
we	O
find	O
the	O
transformed	O
prior	O
is	O
p	O
d	O
d	O
d	O
d	O
i	O
so	O
p	O
and	O
p	O
are	O
the	O
same	O
some	O
examples	O
will	O
make	O
this	O
clearer	O
example	O
jeffreys	B
prior	I
for	O
the	O
bernoulli	B
and	O
multinoulli	O
suppose	O
x	O
ber	O
the	O
log	O
likelihood	B
for	O
a	O
single	O
sample	O
is	O
log	O
px	O
x	O
log	O
x	O
the	O
score	B
function	I
is	O
just	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
s	O
d	O
d	O
log	O
px	O
x	O
x	O
the	O
observed	B
information	B
is	O
the	O
second	O
derivative	O
of	O
the	O
log-likelihood	O
j	O
d	O
log	O
px	O
x	O
x	O
the	O
fisher	B
information	B
is	O
the	O
expected	O
information	B
i	O
hence	O
jeffreys	B
prior	I
is	O
p	O
beta	O
now	O
consider	O
a	O
multinoulli	O
random	O
variable	O
with	O
k	O
states	O
one	O
can	O
show	O
that	O
the	O
jeffreys	B
prior	I
is	O
given	O
by	O
p	O
dir	O
note	O
that	O
this	O
is	O
different	O
from	O
the	O
more	O
obvious	O
choices	O
of	O
dir	O
k	O
k	O
or	O
example	O
jeffreys	B
prior	I
for	O
location	O
and	O
scale	O
parameters	O
one	O
can	O
show	O
that	O
the	O
jeffreys	B
prior	I
for	O
a	O
location	O
parameter	B
such	O
as	O
the	O
gaussian	B
mean	B
is	O
p	O
thus	O
is	O
an	O
example	O
of	O
a	O
translation	B
invariant	B
prior	I
which	O
satisfies	O
the	O
property	O
that	O
the	O
probability	O
mass	O
assigned	O
to	O
any	O
interval	O
b	O
is	O
the	O
same	O
as	O
that	O
assigned	O
to	O
any	O
other	O
shifted	O
interval	O
of	O
the	O
same	O
width	O
such	O
as	O
c	O
b	O
c	O
that	O
is	O
b	O
c	O
b	O
p	O
c	O
c	O
b	O
p	O
a	O
a	O
c	O
chapter	O
bayesian	B
statistics	I
this	O
can	O
be	O
achieved	O
using	O
p	O
which	O
we	O
can	O
approximate	O
by	O
using	O
a	O
gaussian	B
with	O
infinite	O
variance	B
p	O
note	O
that	O
this	O
is	O
an	O
improper	B
prior	I
since	O
it	O
does	O
not	O
integrate	O
to	O
using	O
improper	O
priors	O
is	O
fine	O
as	O
long	O
as	O
the	O
posterior	O
is	O
proper	O
which	O
will	O
be	O
the	O
case	O
provided	O
we	O
have	O
seen	O
n	O
data	O
points	O
since	O
we	O
can	O
nail	O
down	O
the	O
location	O
as	O
soon	O
as	O
we	O
have	O
seen	O
a	O
single	O
data	O
point	O
similarly	O
one	O
can	O
show	O
that	O
the	O
jeffreys	B
prior	I
for	O
a	O
scale	O
parameter	B
such	O
as	O
the	O
gaussian	B
variance	B
is	O
p	O
this	O
is	O
an	O
example	O
of	O
a	O
scale	B
invariant	B
prior	I
which	O
satisfies	O
the	O
property	O
that	O
the	O
probability	O
mass	O
assigned	O
to	O
any	O
interval	O
b	O
is	O
the	O
same	O
as	O
that	O
assigned	O
to	O
any	O
other	O
interval	O
bc	O
which	O
is	O
scaled	O
in	O
size	O
by	O
some	O
constant	O
factor	B
c	O
example	O
if	O
we	O
change	O
units	O
from	O
meters	O
to	O
feet	O
we	O
do	O
not	O
want	O
that	O
to	O
affect	O
our	O
inferences	O
this	O
can	O
be	O
achieved	O
by	O
using	O
ps	O
bc	O
to	O
see	O
this	O
note	O
that	O
psds	O
s	O
ac	O
bc	O
ac	O
logbc	O
logac	O
b	O
logb	O
loga	O
psds	O
we	O
can	O
approximate	O
this	O
using	O
a	O
degenerate	B
gamma	B
distribution	I
ps	O
the	O
prior	O
ps	O
is	O
also	O
improper	O
but	O
the	O
posterior	O
is	O
proper	O
as	O
soon	O
as	O
we	O
have	O
seen	O
n	O
data	O
points	O
we	O
need	O
at	O
least	O
two	O
data	O
points	O
to	O
estimate	O
a	O
variance	B
a	O
robust	B
priors	I
in	O
many	O
cases	O
we	O
are	O
not	O
very	O
confident	O
in	O
our	O
prior	O
so	O
we	O
want	O
to	O
make	O
sure	O
it	O
does	O
not	O
have	O
an	O
undue	O
influence	O
on	O
the	O
result	O
this	O
can	O
be	O
done	O
by	O
using	O
robust	B
priors	I
and	O
ruggeri	O
which	O
typically	O
have	O
heavy	B
tails	I
which	O
avoids	O
forcing	O
things	O
to	O
be	O
too	O
close	O
to	O
the	O
prior	O
mean	B
let	O
us	O
consider	O
an	O
example	O
from	O
suppose	O
x	O
n	O
we	O
observe	O
that	O
x	O
and	O
we	O
want	O
to	O
estimate	O
the	O
mle	B
is	O
of	O
course	O
which	O
seems	O
reasonable	O
the	O
posterior	B
mean	B
under	O
a	O
uniform	O
prior	O
is	O
also	O
but	O
now	O
suppose	O
we	O
know	O
that	O
the	O
prior	O
median	B
is	O
and	O
the	O
prior	O
quantiles	O
are	O
at	O
and	O
so	O
p	O
p	O
let	O
us	O
also	O
assume	O
the	O
prior	O
is	O
smooth	O
and	O
unimodal	O
it	O
is	O
easy	O
to	O
show	O
that	O
a	O
gaussian	B
prior	O
of	O
the	O
form	O
n	O
satisfies	O
these	O
prior	O
constraints	O
but	O
in	O
this	O
case	O
the	O
posterior	B
mean	B
is	O
given	O
by	O
which	O
doesn	O
t	O
seem	O
very	O
satisfactory	O
now	O
suppose	O
we	O
use	O
as	O
a	O
cauchy	B
prior	O
t	O
this	O
also	O
satisfies	O
the	O
prior	O
constraints	O
of	O
our	O
example	O
but	O
this	O
time	O
we	O
find	O
numerical	O
method	O
integration	O
see	O
robustpriordemo	O
for	O
the	O
code	O
that	O
the	O
posterior	B
mean	B
is	O
about	O
which	O
seems	O
much	O
more	O
reasonable	O
mixtures	O
of	O
conjugate	B
priors	I
robust	B
priors	I
are	O
useful	O
but	O
can	O
be	O
computationally	O
expensive	O
to	O
use	O
conjugate	B
priors	I
simplify	O
the	O
computation	O
but	O
are	O
often	O
not	O
robust	B
and	O
not	O
flexible	O
enough	O
to	O
encode	O
our	O
prior	O
knowl	O
priors	O
edge	O
however	O
it	O
turns	O
out	O
that	O
a	O
mixture	B
of	I
conjugate	B
priors	I
is	O
also	O
conjugate	O
and	O
can	O
approximate	O
any	O
kind	O
of	O
prior	O
and	O
hall	O
diaconis	O
and	O
ylvisaker	O
thus	O
such	O
priors	O
provide	O
a	O
good	O
compromise	O
between	O
computational	O
convenience	O
and	O
flexibility	O
for	O
example	O
suppose	O
we	O
are	O
modeling	O
coin	O
tosses	O
and	O
we	O
think	O
the	O
coin	O
is	O
either	O
fair	O
or	O
is	O
biased	O
towards	O
heads	O
this	O
cannot	O
be	O
represented	O
by	O
a	O
beta	B
distribution	I
however	O
we	O
can	O
model	O
it	O
using	O
a	O
mixture	B
of	O
two	O
beta	O
distributions	O
for	O
example	O
we	O
might	O
use	O
p	O
beta	O
beta	O
if	O
comes	O
from	O
the	O
first	O
distribution	O
the	O
coin	O
is	O
fair	O
but	O
if	O
it	O
comes	O
from	O
the	O
second	O
it	O
is	O
biased	O
towards	O
heads	O
we	O
can	O
represent	O
a	O
mixture	B
by	O
introducing	O
a	O
latent	B
indicator	O
variable	O
z	O
where	O
z	O
k	O
means	O
that	O
comes	O
from	O
mixture	B
component	O
k	O
the	O
prior	O
has	O
the	O
form	O
p	O
pz	O
kp	O
k	O
k	O
where	O
each	O
p	O
k	O
is	O
conjugate	O
and	O
pz	O
k	O
are	O
called	O
the	O
mixing	B
weights	I
one	O
can	O
show	O
that	O
the	O
posterior	O
can	O
also	O
be	O
written	O
as	O
a	O
mixture	B
of	O
conjugate	O
distributions	O
as	O
follows	O
p	O
pz	O
kdp	O
z	O
k	O
k	O
where	O
pz	O
kd	O
are	O
the	O
posterior	O
mixing	B
weights	I
given	O
by	O
pz	O
kd	O
pz	O
kpdz	O
k	O
pz	O
here	O
the	O
quantity	O
pdz	O
k	O
is	O
the	O
marginal	B
likelihood	B
for	O
mixture	B
component	O
k	O
section	O
example	O
suppose	O
we	O
use	O
the	O
mixture	B
prior	O
p	O
where	O
and	O
and	O
we	O
observe	O
heads	O
and	O
tails	O
the	O
posterior	O
becomes	O
p	O
pz	O
z	O
if	O
heads	O
and	O
tails	O
then	O
using	O
equation	O
the	O
posterior	O
becomes	O
p	O
beta	O
beta	O
see	O
figure	O
for	O
an	O
illustration	O
chapter	O
bayesian	B
statistics	I
mixture	B
of	O
beta	O
distributions	O
prior	O
posterior	O
figure	O
a	O
mixture	B
of	O
two	O
beta	O
distributions	O
figure	O
generated	O
by	O
mixbetademo	O
application	O
finding	O
conserved	O
regions	O
in	O
dna	O
and	O
protein	O
sequences	O
we	O
mentioned	O
that	O
dirichlet-multinomial	O
models	O
are	O
widely	O
used	O
in	O
biosequence	B
analysis	I
let	O
us	O
give	O
a	O
simple	O
example	O
to	O
illustrate	O
some	O
of	O
the	O
machinery	O
that	O
has	O
developed	O
specifically	O
consider	O
the	O
sequence	B
logo	I
discussed	O
in	O
section	O
now	O
suppose	O
we	O
want	O
to	O
find	O
locations	O
which	O
represent	O
coding	O
regions	O
of	O
the	O
genome	B
such	O
locations	O
often	O
have	O
the	O
same	O
letter	O
across	O
all	O
sequences	O
because	O
of	O
evolutionary	O
pressure	O
so	O
we	O
need	O
to	O
find	O
columns	O
which	O
are	O
pure	B
or	O
nearly	O
so	O
in	O
the	O
sense	O
that	O
they	O
are	O
mostly	O
all	O
as	O
mostly	O
all	O
ts	O
mostly	O
all	O
cs	O
or	O
mostly	O
all	O
gs	O
one	O
approach	O
is	O
to	O
look	O
for	O
low-entropy	O
columns	O
these	O
will	O
be	O
ones	O
whose	O
distribution	O
is	O
nearly	O
deterministic	O
but	O
suppose	O
we	O
want	O
to	O
associate	O
a	O
confidence	O
measure	O
with	O
our	O
estimates	O
of	O
purity	B
this	O
in	O
this	O
case	O
we	O
can	O
let	O
can	O
be	O
useful	O
if	O
we	O
believe	O
adjacent	O
locations	O
are	O
conserved	O
together	O
if	O
location	O
t	O
is	O
conserved	O
and	O
let	O
zt	O
otherwise	O
we	O
can	O
then	O
add	O
a	O
dependence	O
between	O
adjacent	O
zt	O
variables	O
using	O
a	O
markov	B
chain	I
see	O
chapter	O
for	O
details	O
in	O
any	O
case	O
we	O
need	O
to	O
define	O
a	O
likelihood	B
model	O
pntzt	O
where	O
nt	O
is	O
the	O
vector	O
of	O
counts	O
for	O
column	O
t	O
it	O
is	O
natural	O
to	O
make	O
this	O
be	O
a	O
multinomial	B
distribution	O
with	O
parameter	B
t	O
since	O
each	O
column	O
has	O
a	O
different	O
distribution	O
we	O
will	O
want	O
to	O
integrate	B
out	I
t	O
and	O
thus	O
compute	O
the	O
marginal	B
likelihood	B
pnt	O
tp	O
tztd	O
t	O
but	O
what	O
prior	O
should	O
we	O
use	O
for	O
t	O
when	O
zt	O
we	O
can	O
use	O
a	O
uniform	O
prior	O
p	O
but	O
what	O
should	O
we	O
use	O
if	O
zt	O
after	O
all	O
if	O
the	O
column	O
is	O
conserved	O
it	O
could	O
be	O
a	O
pure	B
column	O
of	O
as	O
cs	O
gs	O
or	O
ts	O
a	O
natural	O
approach	O
is	O
to	O
use	O
a	O
mixture	B
of	O
dirichlet	B
priors	O
each	O
one	O
of	O
which	O
is	O
tilted	O
towards	O
the	O
appropriate	O
corner	O
of	O
the	O
simplex	O
e	O
g	O
pntzt	O
p	O
dir	O
dir	O
since	O
this	O
is	O
conjugate	O
we	O
can	O
easily	O
compute	O
pntzt	O
see	O
et	O
al	O
for	O
an	O
hierarchical	O
bayes	O
application	O
of	O
these	O
ideas	O
to	O
a	O
real	O
bio-sequence	O
problem	O
hierarchical	O
bayes	O
a	O
key	O
requirement	O
for	O
computing	O
the	O
posterior	O
p	O
is	O
the	O
specification	O
of	O
a	O
prior	O
p	O
where	O
are	O
the	O
hyper-parameters	B
what	O
if	O
we	O
don	O
t	O
know	O
how	O
to	O
set	O
in	O
some	O
cases	O
we	O
can	O
use	O
uninformative	B
priors	O
we	O
we	O
discussed	O
above	O
a	O
more	O
bayesian	B
approach	O
is	O
to	O
put	O
a	O
prior	O
on	O
our	O
priors	O
in	O
terms	O
of	O
graphical	B
models	I
we	O
can	O
represent	O
the	O
situation	O
as	O
follows	O
d	O
this	O
is	O
an	O
example	O
of	O
a	O
hierarchical	B
bayesian	B
model	I
also	O
called	O
a	O
multi-level	B
model	I
since	O
there	O
are	O
multiple	O
levels	O
of	O
unknown	B
quantities	O
we	O
give	O
a	O
simple	O
example	O
below	O
and	O
we	O
will	O
see	O
many	O
others	O
later	O
in	O
the	O
book	O
example	O
modeling	O
related	O
cancer	O
rates	O
consider	O
the	O
problem	O
of	O
predicting	O
cancer	O
rates	O
in	O
various	O
cities	O
example	O
is	O
from	O
and	O
albert	O
in	O
particular	O
suppose	O
we	O
measure	O
the	O
number	O
of	O
people	O
in	O
various	O
cities	O
ni	O
and	O
the	O
number	O
of	O
people	O
who	O
died	O
of	O
cancer	O
in	O
these	O
cities	O
xi	O
we	O
assume	O
xi	O
binni	O
i	O
and	O
we	O
want	O
to	O
estimate	O
the	O
cancer	O
rates	O
i	O
one	O
approach	O
is	O
to	O
estimate	O
them	O
all	O
separately	O
but	O
this	O
will	O
suffer	O
from	O
the	O
sparse	B
data	I
problem	I
of	O
the	O
rate	B
of	O
cancer	O
due	O
to	O
small	O
ni	O
another	O
approach	O
is	O
to	O
assume	O
all	O
the	O
i	O
are	O
the	O
same	O
this	O
is	O
called	O
parameter	B
tying	I
the	O
resulting	O
pooled	B
mle	B
is	O
just	O
i	O
ni	O
but	O
the	O
assumption	O
that	O
all	O
the	O
cities	O
have	O
the	O
same	O
rate	B
is	O
a	O
rather	O
strong	O
one	O
a	O
compromise	O
approach	O
is	O
to	O
assume	O
that	O
the	O
i	O
are	O
similar	B
but	O
that	O
there	O
may	O
be	O
city-specific	O
variations	O
this	O
can	O
be	O
modeled	O
by	O
assuming	O
the	O
i	O
are	O
drawn	O
from	O
some	O
common	O
distribution	O
say	O
i	O
betaa	O
b	O
the	O
full	B
joint	B
distribution	I
can	O
be	O
written	O
as	O
pd	O
binxini	O
ibeta	O
i	O
i	O
where	O
b	O
note	O
that	O
it	O
is	O
crucial	O
that	O
we	O
infer	O
b	O
from	O
the	O
data	O
if	O
we	O
just	O
clamp	O
it	O
to	O
a	O
constant	O
the	O
i	O
will	O
be	O
conditionally	B
independent	I
and	O
there	O
will	O
be	O
no	O
information	B
flow	O
between	O
them	O
by	O
contrast	O
by	O
treating	O
as	O
an	O
unknown	B
variable	O
we	O
allow	O
the	O
data-poor	O
cities	O
to	O
borrow	B
statistical	I
strength	I
from	O
data-rich	O
ones	O
suppose	O
we	O
compute	O
the	O
joint	O
posterior	O
p	O
from	O
this	O
we	O
can	O
get	O
the	O
posterior	O
marginals	O
p	O
id	O
in	O
figure	O
we	O
plot	O
the	O
posterior	O
means	O
e	O
id	O
as	O
blue	O
bars	O
as	O
well	O
as	O
the	O
population	O
level	O
mean	B
e	O
bd	O
shown	O
as	O
a	O
red	O
line	O
represents	O
the	O
average	O
of	O
the	O
i	O
s	O
we	O
see	O
that	O
the	O
posterior	B
mean	B
is	O
shrunk	O
towards	O
the	O
pooled	B
estimate	O
more	O
strongly	O
for	O
cities	O
with	O
small	O
sample	O
sizes	O
ni	O
for	O
example	O
city	O
and	O
city	O
both	O
have	O
a	O
observed	O
cancer	O
incidence	O
rate	B
but	O
city	O
has	O
a	O
smaller	O
population	O
so	O
its	O
rate	B
is	O
shrunk	O
more	O
towards	O
the	O
population-level	O
estimate	O
it	O
is	O
closer	O
to	O
the	O
horizontal	O
red	O
line	O
than	O
city	O
figure	O
shows	O
the	O
posterior	O
credible	O
intervals	O
for	O
i	O
we	O
see	O
that	O
city	O
which	O
has	O
a	O
very	O
large	O
population	O
people	O
has	O
small	O
posterior	O
uncertainty	B
consequently	O
this	O
city	O
chapter	O
bayesian	B
statistics	I
number	O
of	O
people	O
with	O
cancer	O
at	O
pop	O
of	O
city	O
at	O
linepooled	O
mle	B
posterior	O
linepop	O
mean	B
credible	B
interval	I
on	O
theta	O
x	O
figure	O
results	O
of	O
fitting	O
the	O
model	O
using	O
the	O
data	O
from	O
and	O
albert	O
first	O
row	O
number	O
of	O
cancer	O
incidents	O
xi	O
in	O
cities	O
in	O
missouri	O
second	O
row	O
population	O
size	O
ni	O
the	O
largest	O
city	O
has	O
a	O
population	O
of	O
and	O
incidents	O
but	O
we	O
truncate	O
the	O
vertical	O
axes	O
of	O
the	O
first	O
two	O
rows	O
so	O
that	O
the	O
differences	O
between	O
the	O
other	O
cities	O
are	O
visible	B
third	O
row	O
mle	B
i	O
the	O
red	O
line	O
is	O
the	O
pooled	B
mle	B
fourth	O
row	O
posterior	B
mean	B
e	O
id	O
the	O
red	O
line	O
is	O
e	O
bd	O
the	O
population-level	O
mean	B
posterior	O
credible	O
intervals	O
on	O
the	O
cancer	O
rates	O
figure	O
generated	O
by	O
cancerrateseb	O
has	O
the	O
largest	O
impact	O
on	O
the	O
posterior	O
estimate	O
of	O
which	O
in	O
turn	O
will	O
impact	O
the	O
estimate	O
of	O
the	O
cancer	O
rates	O
for	O
other	O
cities	O
cities	O
and	O
which	O
have	O
the	O
highest	O
mle	B
also	O
have	O
the	O
highest	O
posterior	O
uncertainty	B
reflecting	O
the	O
fact	O
that	O
such	O
a	O
high	O
estimate	O
is	O
in	O
conflict	O
with	O
the	O
prior	O
is	O
estimated	O
from	O
all	O
the	O
other	O
cities	O
in	O
the	O
above	O
example	O
we	O
have	O
one	O
parameter	B
per	O
city	O
modeling	O
the	O
probability	O
the	O
response	O
is	O
on	O
by	O
making	O
the	O
bernoulli	B
rate	B
parameter	B
be	O
a	O
function	O
of	O
covariates	B
i	O
sigmwt	O
i	O
x	O
we	O
can	O
model	O
multiple	O
correlated	O
logistic	B
regression	B
tasks	O
this	O
is	O
called	O
multi-task	B
learning	B
and	O
will	O
be	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
empirical	B
bayes	I
in	O
hierarchical	B
bayesian	B
models	I
we	O
need	O
to	O
compute	O
the	O
posterior	O
on	O
multiple	O
levels	O
of	O
latent	B
variables	O
for	O
example	O
in	O
a	O
two-level	O
model	O
we	O
need	O
to	O
compute	O
p	O
pd	O
in	O
some	O
cases	O
we	O
can	O
analytically	O
marginalize	O
out	O
this	O
leaves	B
is	O
with	O
the	O
simpler	O
problem	O
of	O
just	O
computing	O
p	O
as	O
a	O
computational	O
shortcut	O
we	O
can	O
approximate	O
the	O
posterior	O
on	O
the	O
hyper-parameters	B
with	O
a	O
point-estimate	O
p	O
where	O
argmax	O
p	O
since	O
is	O
typically	O
much	O
smaller	O
than	O
in	O
dimensionality	O
it	O
is	O
less	O
prone	O
to	O
overfitting	O
so	O
we	O
can	O
safely	O
use	O
a	O
uniform	O
prior	O
on	O
then	O
the	O
estimate	O
becomes	O
argmax	O
pd	O
argmax	O
pd	O
empirical	B
bayes	I
where	O
the	O
quantity	O
inside	O
the	O
brackets	O
is	O
the	O
marginal	O
or	O
integrated	B
likelihood	B
sometimes	O
called	O
the	O
evidence	B
this	O
overall	O
approach	O
is	O
called	O
empirical	B
bayes	I
ortype-ii	O
maximum	O
likelihood	B
in	O
machine	B
learning	B
it	O
is	O
sometimes	O
called	O
the	O
evidence	B
procedure	I
empirical	B
bayes	I
violates	O
the	O
principle	O
that	O
the	O
prior	O
should	O
be	O
chosen	O
independently	O
of	O
the	O
data	O
however	O
we	O
can	O
just	O
view	O
it	O
as	O
a	O
computationally	O
cheap	O
approximation	O
to	O
inference	B
in	O
a	O
hierarchical	B
bayesian	B
model	I
just	O
as	O
we	O
viewed	O
map	O
estimation	O
as	O
an	O
approximation	O
to	O
inference	B
in	O
the	O
one	O
level	O
model	O
d	O
in	O
fact	O
we	O
can	O
construct	O
a	O
hierarchy	O
in	O
which	O
the	O
more	O
integrals	O
one	O
performs	O
the	O
more	O
bayesian	B
one	O
becomes	O
method	O
maximum	O
likelihood	B
map	O
estimation	O
ml-ii	O
bayes	O
map-ii	O
full	B
bayes	O
definition	O
argmax	O
pd	O
argmax	O
pd	O
argmax	O
argmax	O
p	O
pd	O
pd	O
argmax	O
pd	O
pd	O
argmax	O
pd	O
note	O
that	O
eb	B
can	O
be	O
shown	O
to	O
have	O
good	O
frequentist	B
properties	O
e	O
g	O
and	O
louis	O
efron	O
so	O
it	O
is	O
widely	O
used	O
by	O
non-bayesians	O
for	O
example	O
the	O
popular	O
james-stein	B
estimator	B
discussed	O
in	O
section	O
can	O
be	O
derived	O
using	O
eb	B
example	O
beta-binomial	B
model	O
let	O
us	O
return	O
to	O
the	O
cancer	O
rates	O
model	O
we	O
can	O
analytically	O
integrate	B
out	I
the	O
i	O
s	O
and	O
write	O
down	O
the	O
marginal	B
likelihood	B
directly	O
as	O
follows	O
i	O
pda	O
b	O
binxini	O
ibeta	O
ia	O
bd	O
i	O
ba	O
xi	O
b	O
ni	O
xi	O
ba	O
b	O
i	O
various	O
ways	O
of	O
maximizing	O
this	O
wrt	O
a	O
and	O
b	O
are	O
discussed	O
in	O
having	O
estimated	O
a	O
and	O
b	O
we	O
can	O
plug	O
in	O
the	O
hyper-parameters	B
to	O
compute	O
the	O
posterior	O
p	O
i	O
a	O
bd	O
in	O
the	O
usual	O
way	O
using	O
conjugate	O
analysis	O
the	O
net	O
result	O
is	O
that	O
the	O
posterior	B
mean	B
of	O
each	O
i	O
is	O
a	O
weighted	B
average	I
of	O
its	O
local	O
mle	B
and	O
the	O
prior	O
means	O
which	O
depends	O
on	O
b	O
but	O
since	O
is	O
estimated	O
based	O
on	O
all	O
the	O
data	O
each	O
i	O
is	O
influenced	O
by	O
all	O
the	O
data	O
example	O
gaussian-gaussian	O
model	O
we	O
now	O
study	O
another	O
example	O
that	O
is	O
analogous	O
to	O
the	O
cancer	O
rates	O
example	O
except	O
the	O
data	O
is	O
real-valued	O
we	O
will	O
use	O
a	O
gaussian	B
likelihood	B
and	O
a	O
gaussian	B
prior	O
this	O
will	O
allow	O
us	O
to	O
write	O
down	O
the	O
solution	O
analytically	O
in	O
particular	O
suppose	O
we	O
have	O
data	O
from	O
multiple	O
related	O
groups	O
for	O
example	O
xij	O
could	O
be	O
the	O
test	O
score	O
for	O
student	O
i	O
in	O
school	O
j	O
for	O
j	O
and	O
i	O
j	O
we	O
want	O
to	O
estimate	O
the	O
mean	B
score	O
for	O
each	O
school	O
j	O
however	O
since	O
the	O
sample	O
size	O
nj	O
may	O
be	O
small	O
for	O
chapter	O
bayesian	B
statistics	I
some	O
schools	O
we	O
can	O
regularize	O
the	O
problem	O
by	O
using	O
a	O
hierarchical	B
bayesian	B
model	I
where	O
we	O
assume	O
j	O
come	O
from	O
a	O
common	O
prior	O
n	O
the	O
joint	B
distribution	I
has	O
the	O
following	O
form	O
p	O
n	O
j	O
n	O
j	O
where	O
we	O
assume	O
is	O
known	O
for	O
simplicity	O
relax	O
this	O
assumption	O
in	O
exercise	O
we	O
explain	O
how	O
to	O
estimate	O
below	O
once	O
we	O
have	O
estimated	O
we	O
can	O
compute	O
the	O
posteriors	O
over	O
the	O
j	O
s	O
to	O
do	O
that	O
it	O
simplifies	O
matters	O
to	O
rewrite	O
the	O
joint	B
distribution	I
in	O
the	O
following	O
form	O
exploiting	O
the	O
fact	O
that	O
nj	O
gaussian	B
measurements	O
with	O
values	O
xij	O
and	O
variance	B
j	O
are	O
equivalent	O
to	O
one	O
measurement	O
of	O
value	O
xj	O
this	O
yields	O
xij	O
with	O
variance	B
nj	O
p	O
n	O
j	O
j	O
j	O
from	O
this	O
it	O
follows	O
from	O
the	O
results	O
of	O
section	O
that	O
the	O
posteriors	O
are	O
given	O
by	O
p	O
jd	O
j	O
bj	O
bjxj	O
bj	O
j	O
bj	O
j	O
j	O
where	O
x	O
and	O
will	O
be	O
defined	O
below	O
the	O
quantity	O
bj	O
controls	O
the	O
degree	B
of	O
shrinkage	B
towards	O
the	O
overall	O
mean	B
if	O
the	O
data	O
is	O
reliable	O
for	O
group	O
j	O
because	O
the	O
sample	O
size	O
nj	O
is	O
large	O
then	O
j	O
will	O
be	O
small	O
relative	O
to	O
hence	O
bj	O
will	O
be	O
small	O
and	O
we	O
will	O
put	O
more	O
weight	O
on	O
xj	O
when	O
we	O
estimate	O
j	O
however	O
groups	O
with	O
small	O
sample	O
sizes	O
will	O
get	O
regularized	O
towards	O
the	O
overall	O
mean	B
more	O
heavily	O
we	O
will	O
see	O
an	O
example	O
of	O
this	O
below	O
if	O
j	O
for	O
all	O
groups	O
j	O
the	O
posterior	B
mean	B
becomes	O
j	O
bx	O
bxj	O
x	O
bxj	O
x	O
this	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
james	B
stein	I
estimator	B
discussed	O
in	O
section	O
example	O
predicting	O
baseball	O
scores	B
we	O
now	O
give	O
an	O
example	O
of	O
shrinkage	B
applied	O
to	O
baseball	O
batting	O
averages	O
from	O
and	O
morris	O
we	O
observe	O
the	O
number	O
of	O
hits	O
for	O
d	O
players	O
during	O
the	O
first	O
t	O
games	O
call	O
the	O
number	O
of	O
hits	O
bi	O
we	O
assume	O
bj	O
bint	O
j	O
where	O
j	O
is	O
the	O
true	O
batting	O
average	O
for	O
player	O
j	O
the	O
goal	O
is	O
to	O
estimate	O
the	O
j	O
the	O
mle	B
is	O
of	O
course	O
j	O
xj	O
where	O
xj	O
bjt	O
is	O
the	O
empirical	O
batting	O
average	O
however	O
we	O
can	O
use	O
an	O
eb	B
approach	O
to	O
do	O
better	O
gaussian	B
xj	O
n	O
j	O
for	O
known	O
to	O
apply	O
the	O
gaussian	B
shrinkage	B
approach	O
described	O
above	O
we	O
require	O
that	O
the	O
likelihood	B
be	O
drop	O
the	O
i	O
subscript	O
since	O
we	O
assume	O
nj	O
empirical	B
bayes	I
mle	B
and	O
shrinkage	B
estimates	O
mse	B
mle	B
mse	B
shrunk	O
e	O
s	O
m	O
player	O
number	O
true	O
shrunk	O
mle	B
figure	O
mle	B
parameters	O
and	O
corresponding	O
shrunken	O
estimates	O
we	O
plot	O
the	O
true	O
parameters	O
the	O
posterior	B
mean	B
estimate	O
and	O
the	O
mles	O
for	O
of	O
the	O
players	O
figure	O
generated	O
by	O
shrinkagedemobaseball	O
since	O
xj	O
already	O
represents	O
the	O
average	O
for	O
player	O
j	O
however	O
binomial	B
likelihood	B
while	O
this	O
has	O
the	O
right	O
mean	B
e	O
j	O
the	O
variance	B
is	O
not	O
constant	O
in	O
this	O
example	O
we	O
have	O
a	O
var	B
t	O
var	B
t	O
j	O
t	O
t	O
so	O
let	O
us	O
apply	O
a	O
variance	B
stabilizing	O
to	O
xj	O
to	O
better	O
match	O
the	O
gaussian	B
assumption	O
yj	O
f	O
now	O
we	O
have	O
approximately	O
yj	O
n	O
j	O
n	O
j	O
we	O
use	O
gaussian	B
shrinkage	B
to	O
estimate	O
the	O
j	O
using	O
equation	O
with	O
and	O
we	O
then	O
transform	O
back	O
to	O
get	O
t	O
j	O
j	O
the	O
results	O
are	O
shown	O
in	O
figure	O
in	O
we	O
plot	O
the	O
mle	B
j	O
and	O
the	O
posterior	B
mean	B
j	O
we	O
see	O
that	O
all	O
the	O
estimates	O
have	O
shrunk	O
towards	O
the	O
global	O
mean	B
in	O
we	O
plot	O
the	O
true	O
value	O
j	O
the	O
mle	B
j	O
and	O
the	O
posterior	B
mean	B
j	O
true	O
values	O
of	O
j	O
are	O
estimated	O
from	O
a	O
large	O
number	O
of	O
independent	O
games	O
we	O
see	O
that	O
on	O
average	O
the	O
shrunken	O
estimate	O
is	O
much	O
closer	O
to	O
the	O
true	O
parameters	O
than	O
the	O
mle	B
is	O
specifically	O
the	O
mean	B
squared	B
error	I
j	O
is	O
over	O
three	O
times	O
smaller	O
using	O
the	O
shrinkage	B
estimates	O
defined	O
by	O
mse	B
n	O
j	O
than	O
using	O
the	O
mles	O
j	O
estimating	O
the	O
hyper-parameters	B
in	O
this	O
section	O
we	O
give	O
an	O
algorithm	O
for	O
estimating	O
suppose	O
initially	O
that	O
j	O
is	O
the	O
same	O
for	O
all	O
groups	O
in	O
this	O
case	O
we	O
can	O
derive	O
the	O
eb	B
estimate	O
in	O
closed	O
form	O
as	O
we	O
now	O
show	O
from	O
equation	O
we	O
have	O
pxj	O
n	O
j	O
j	O
j	O
n	O
suppose	O
e	O
and	O
var	B
let	O
y	O
f	O
then	O
a	O
taylor	B
series	I
expansions	O
gives	O
y	O
f	O
a	O
variance	B
stabilizing	O
transformation	O
is	O
a	O
function	O
f	O
such	O
that	O
hence	O
var	B
is	O
independent	O
of	O
chapter	O
bayesian	B
statistics	I
hence	O
the	O
marginal	B
likelihood	B
is	O
pd	O
n	O
thus	O
we	O
can	O
estimate	O
the	O
hyper-parameters	B
using	O
the	O
usual	O
mles	O
for	O
a	O
gaussian	B
for	O
we	O
have	O
xj	O
x	O
d	O
d	O
which	O
is	O
the	O
overall	O
mean	B
for	O
the	O
variance	B
we	O
can	O
use	O
moment	B
matching	I
is	O
equivalent	O
to	O
the	O
mle	B
for	O
a	O
gaussian	B
we	O
simply	O
equate	O
the	O
model	O
variance	B
to	O
the	O
empirical	O
variance	B
so	O
since	O
we	O
know	O
must	O
be	O
positive	O
it	O
is	O
common	O
to	O
use	O
the	O
following	O
revised	O
estimate	O
hence	O
the	O
shrinkage	B
factor	B
is	O
b	O
in	O
the	O
case	O
where	O
the	O
j	O
s	O
are	O
different	O
we	O
can	O
no	O
longer	O
derive	O
a	O
solution	O
in	O
closed	O
form	O
exercise	O
discusses	O
how	O
to	O
use	O
the	O
em	B
algorithm	O
to	O
derive	O
an	O
eb	B
estimate	O
and	O
exercise	O
discusses	O
how	O
to	O
perform	O
full	B
bayesian	B
inference	B
in	O
this	O
hierarchical	O
model	O
bayesian	B
decision	B
theory	O
we	O
have	O
seen	O
how	O
probability	B
theory	I
can	O
be	O
used	O
to	O
represent	O
and	O
updates	O
our	O
beliefs	O
about	O
the	O
state	B
of	O
the	O
world	O
however	O
ultimately	O
our	O
goal	O
is	O
to	O
convert	O
our	O
beliefs	O
into	O
actions	B
in	O
this	O
section	O
we	O
discuss	O
the	O
optimal	O
way	O
to	O
do	O
this	O
we	O
can	O
formalize	O
any	O
given	O
statistical	O
decision	B
problem	I
as	O
a	O
game	B
against	I
nature	I
opposed	O
to	O
a	O
game	O
against	O
other	O
strategic	O
players	O
which	O
is	O
the	O
topic	B
of	O
game	B
theory	I
see	O
e	O
g	O
and	O
leyton-brown	O
for	O
details	O
in	O
this	O
game	O
nature	O
picks	O
a	O
state	B
or	O
parameter	B
or	O
label	B
y	O
y	O
unknown	B
to	O
us	O
and	O
then	O
generates	O
an	O
observation	B
x	O
x	O
which	O
we	O
get	O
to	O
see	O
we	O
then	O
have	O
to	O
make	O
a	O
decision	B
that	O
is	O
we	O
have	O
to	O
choose	O
an	O
action	B
a	O
from	O
some	O
action	B
space	I
a	O
finally	O
we	O
incur	O
some	O
loss	B
ly	O
a	O
which	O
measures	O
how	O
compatible	O
our	O
action	B
a	O
is	O
with	O
nature	O
s	O
hidden	B
state	B
y	O
for	O
example	O
we	O
might	O
use	O
misclassification	O
loss	B
ly	O
a	O
a	O
or	O
squared	B
loss	B
ly	O
a	O
we	O
will	O
see	O
some	O
other	O
examples	O
below	O
bayesian	B
decision	B
theory	O
our	O
goal	O
is	O
to	O
devise	O
a	O
decision	B
procedure	I
or	O
policy	B
x	O
a	O
which	O
specifies	O
the	O
optimal	B
action	B
for	O
each	O
possible	O
input	O
by	O
optimal	O
we	O
mean	B
the	O
action	B
that	O
minimizes	O
the	O
expected	O
loss	B
argmin	O
a	O
a	O
e	O
a	O
in	O
economics	O
u	O
a	O
ly	O
a	O
thus	O
the	O
above	O
rule	O
becomes	O
it	O
is	O
more	O
common	O
to	O
talk	O
of	O
a	O
utility	B
function	I
this	O
is	O
just	O
negative	O
loss	B
argmax	O
a	O
a	O
e	O
a	O
this	O
is	O
called	O
the	O
maximum	B
expected	I
utility	I
principle	I
and	O
is	O
the	O
essence	O
of	O
what	O
we	O
mean	B
by	O
rational	B
behavior	I
note	O
that	O
there	O
are	O
two	O
different	O
interpretations	O
of	O
what	O
we	O
mean	B
by	O
expected	O
in	O
the	O
bayesian	B
version	O
which	O
we	O
discuss	O
below	O
we	O
mean	B
the	O
expected	B
value	I
of	O
y	O
given	O
the	O
data	O
we	O
have	O
seen	O
so	O
far	O
in	O
the	O
frequentist	B
version	O
which	O
we	O
discuss	O
in	O
section	O
we	O
mean	B
the	O
expected	B
value	I
of	O
y	O
and	O
x	O
that	O
we	O
expect	O
to	O
see	O
in	O
the	O
future	O
in	O
the	O
bayesian	B
approach	O
to	O
decision	B
theory	O
the	O
optimal	B
action	B
having	O
observed	O
x	O
is	O
defined	O
as	O
the	O
action	B
a	O
that	O
minimizes	O
the	O
posterior	B
expected	I
loss	B
epyx	O
a	O
ly	O
apyx	O
y	O
y	O
is	O
continuous	O
when	O
we	O
want	O
to	O
estimate	O
a	O
parameter	B
vector	O
we	O
should	O
replace	O
the	O
sum	O
with	O
an	O
integral	O
hence	O
the	O
bayes	B
estimator	B
also	O
called	O
the	O
bayes	B
decision	B
rule	I
is	O
given	O
by	O
arg	O
min	O
a	O
a	O
bayes	O
estimators	O
for	O
common	O
loss	B
functions	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
construct	O
bayes	O
estimators	O
for	O
the	O
loss	B
functions	O
most	O
commonly	O
arising	O
in	O
machine	B
learning	B
map	B
estimate	I
minimizes	O
loss	B
the	O
loss	B
is	O
defined	O
by	O
ly	O
a	O
iy	O
a	O
if	O
a	O
y	O
if	O
a	O
y	O
this	O
is	O
commonly	O
used	O
in	O
classification	O
problems	O
where	O
y	O
is	O
the	O
true	O
class	O
label	B
and	O
a	O
y	O
is	O
the	O
estimate	O
for	O
example	O
in	O
the	O
two	O
class	O
case	O
we	O
can	O
write	O
the	O
loss	B
matrix	I
as	O
follows	O
y	O
y	O
y	O
y	O
chapter	O
bayesian	B
statistics	I
figure	O
for	O
some	O
regions	O
of	O
input	O
space	O
where	O
the	O
class	O
posteriors	O
are	O
uncertain	O
we	O
may	O
prefer	O
not	O
to	O
choose	O
class	O
or	O
instead	O
we	O
may	O
prefer	O
the	O
reject	O
option	O
based	O
on	O
figure	O
of	O
section	O
we	O
generalize	B
this	O
loss	B
function	I
so	O
it	O
penalizes	O
the	O
two	O
kinds	O
of	O
errors	O
on	O
the	O
off-diagonal	O
differently	O
the	O
posterior	B
expected	I
loss	B
is	O
pa	O
yx	O
pyx	O
hence	O
the	O
action	B
that	O
minimizes	O
the	O
expected	O
loss	B
is	O
the	O
posterior	B
mode	B
or	O
map	B
estimate	I
y	O
arg	O
max	O
y	O
y	O
pyx	O
reject	O
option	O
in	O
classification	O
problems	O
where	O
pyx	O
is	O
very	O
uncertain	O
we	O
may	O
prefer	O
to	O
choose	O
a	O
reject	B
action	B
in	O
which	O
we	O
refuse	O
to	O
classify	O
the	O
example	O
as	O
any	O
of	O
the	O
specified	O
classes	O
and	O
instead	O
say	O
don	O
t	O
know	O
such	O
ambiguous	O
cases	O
can	O
be	O
handled	O
by	O
e	O
g	O
a	O
human	O
expert	O
see	O
figure	O
for	O
an	O
illustration	O
this	O
is	O
useful	O
in	O
risk	B
averse	I
domains	O
such	O
as	O
medicine	O
and	O
finance	O
we	O
can	O
formalize	O
the	O
reject	O
option	O
as	O
follows	O
let	O
choosing	O
a	O
c	O
correspond	O
to	O
picking	O
the	O
reject	B
action	B
and	O
choosing	O
a	O
c	O
correspond	O
to	O
picking	O
one	O
of	O
the	O
classes	O
suppose	O
we	O
define	O
the	O
loss	B
function	I
as	O
if	O
i	O
j	O
and	O
i	O
j	O
c	O
if	O
i	O
c	O
otherwise	O
ly	O
j	O
a	O
i	O
r	O
s	O
where	O
r	O
is	O
the	O
cost	O
of	O
the	O
reject	B
action	B
and	O
s	O
is	O
the	O
cost	O
of	O
a	O
substitution	O
error	O
in	O
exercise	O
you	O
will	O
show	O
that	O
the	O
optimal	B
action	B
is	O
to	O
pick	O
the	O
reject	B
action	B
if	O
the	O
most	O
probable	O
class	O
has	O
a	O
probability	O
below	O
r	O
s	O
otherwise	O
you	O
should	O
just	O
pick	O
the	O
most	O
probable	O
class	O
bayesian	B
decision	B
theory	O
plots	O
of	O
the	O
ly	O
a	O
aq	O
vs	O
a	O
for	O
q	O
q	O
and	O
q	O
figure	O
figure	O
generated	O
by	O
lossfunctionfig	O
posterior	B
mean	B
minimizes	O
loss	B
for	O
continuous	O
parameters	O
a	O
more	O
appropriate	O
loss	B
function	I
is	O
squared	B
error	I
loss	B
or	O
quadratic	B
loss	B
defined	O
as	O
ly	O
a	O
e	O
the	O
posterior	B
expected	I
loss	B
is	O
given	O
by	O
hence	O
the	O
optimal	O
estimate	O
is	O
the	O
posterior	B
mean	B
y	O
e	O
ypyxdy	O
a	O
this	O
is	O
often	O
called	O
the	O
minimum	B
mean	B
squared	B
error	I
estimate	O
or	O
mmse	B
estimate	O
in	O
a	O
linear	B
regression	B
problem	O
we	O
have	O
pyx	O
n	O
w	O
in	O
this	O
case	O
the	O
optimal	O
estimate	O
given	O
some	O
training	O
data	O
d	O
is	O
given	O
by	O
e	O
xt	O
e	O
that	O
is	O
we	O
just	O
plug-in	B
the	O
posterior	B
mean	B
parameter	B
estimate	O
note	O
that	O
this	O
is	O
the	O
optimal	O
thing	O
to	O
do	O
no	O
matter	O
what	O
prior	O
we	O
use	O
for	O
w	O
posterior	B
median	B
minimizes	O
loss	B
the	O
loss	B
penalizes	O
deviations	O
from	O
the	O
truth	O
quadratically	O
and	O
thus	O
is	O
sensitive	O
to	O
outliers	B
a	O
more	O
robust	B
alternative	O
is	O
the	O
absolute	O
or	O
loss	B
ly	O
a	O
a	O
figure	O
the	O
optimal	O
estimate	O
is	O
the	O
posterior	B
median	B
i	O
e	O
a	O
value	O
a	O
such	O
that	O
p	O
ax	O
p	O
ax	O
see	O
exercise	O
for	O
a	O
proof	O
supervised	B
learning	B
consider	O
a	O
prediction	O
function	O
x	O
y	O
and	O
suppose	O
we	O
have	O
some	O
cost	O
function	O
which	O
gives	O
the	O
cost	O
of	O
predicting	O
when	O
the	O
truth	O
is	O
y	O
we	O
can	O
define	O
the	O
loss	B
incurred	O
by	O
chapter	O
bayesian	B
statistics	I
taking	O
action	B
using	O
this	O
predictor	O
when	O
the	O
unknown	B
state	B
of	O
nature	O
is	O
parameters	O
of	O
the	O
data	O
generating	O
mechanism	O
as	O
follows	O
l	O
exy	O
pxy	O
ly	O
y	O
x	O
y	O
this	O
is	O
known	O
as	O
the	O
generalization	B
error	I
our	O
goal	O
is	O
to	O
minimize	O
the	O
posterior	B
expected	I
loss	B
given	O
by	O
p	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
frequentist	B
risk	B
which	O
is	O
defined	O
in	O
equation	O
the	O
false	B
positive	I
vs	O
false	B
negative	I
tradeoff	O
in	O
this	O
section	O
we	O
focus	O
on	O
binary	O
decision	B
problems	O
such	O
as	O
hypothesis	O
testing	O
two-class	O
classification	O
object	O
event	O
detection	O
etc	O
there	O
are	O
two	O
types	O
of	O
error	O
we	O
can	O
make	O
a	O
false	B
positive	I
false	B
alarm	I
which	O
arises	O
when	O
we	O
estimate	O
y	O
but	O
the	O
truth	O
is	O
y	O
or	O
a	O
false	B
negative	I
missed	B
detection	I
which	O
arises	O
when	O
we	O
estimate	O
y	O
but	O
the	O
truth	O
is	O
y	O
the	O
loss	B
treats	O
these	O
two	O
kinds	O
of	O
errors	O
equivalently	O
however	O
we	O
can	O
consider	O
the	O
following	O
more	O
general	O
loss	B
matrix	I
y	O
y	O
y	O
lf	O
p	O
y	O
lf	O
n	O
where	O
lf	O
n	O
is	O
the	O
cost	O
of	O
a	O
false	B
negative	I
and	O
lf	O
p	O
is	O
the	O
cost	O
of	O
a	O
false	B
positive	I
the	O
posterior	B
expected	I
loss	B
for	O
the	O
two	O
possible	O
actions	B
is	O
given	O
by	O
y	O
f	O
n	O
py	O
y	O
f	O
p	O
py	O
hence	O
we	O
should	O
pick	O
class	O
y	O
iff	B
y	O
y	O
py	O
py	O
lf	O
p	O
lf	O
n	O
if	O
lf	O
n	O
clf	O
p	O
it	O
is	O
easy	O
to	O
show	O
that	O
we	O
should	O
pick	O
y	O
iff	B
py	O
where	O
c	O
also	O
et	O
al	O
for	O
example	O
if	O
a	O
false	B
negative	I
costs	O
twice	O
as	O
much	O
as	O
false	B
positive	I
so	O
c	O
then	O
we	O
use	O
a	O
decision	B
threshold	O
of	O
before	O
declaring	O
a	O
positive	O
below	O
we	O
discuss	O
roc	B
curves	O
which	O
provide	O
a	O
way	O
to	O
study	O
the	O
fp-fn	O
tradeoff	O
without	O
having	O
to	O
choose	O
a	O
specific	O
threshold	O
roc	B
curves	O
and	O
all	O
that	O
suppose	O
we	O
are	O
solving	O
a	O
binary	O
decision	B
problem	I
such	O
as	O
classification	O
hypothesis	O
testing	O
object	B
detection	I
etc	O
also	O
assume	O
we	O
have	O
a	O
labeled	O
data	O
set	O
d	O
yi	O
let	O
bayesian	B
decision	B
theory	O
truth	O
estimate	O
n	O
t	O
p	O
f	O
n	O
n	O
f	O
p	O
t	O
n	O
n	O
t	O
p	O
f	O
p	O
f	O
n	O
t	O
n	O
n	O
t	O
p	O
f	O
p	O
n	O
f	O
n	O
t	O
n	O
tp	O
fn	O
fp	O
tn	O
table	O
quantities	O
derivable	O
from	O
a	O
confusion	B
matrix	I
n	O
is	O
the	O
true	O
number	O
of	O
positives	O
n	O
is	O
the	O
called	O
number	O
of	O
positives	O
n	O
is	O
the	O
true	O
number	O
of	O
negatives	O
n	O
is	O
the	O
called	O
number	O
of	O
negatives	O
y	O
t	O
pntprsensitivityrecall	O
y	O
y	O
f	O
nnfnrmiss	O
ratetype	O
ii	O
y	O
f	O
pn	O
i	O
t	O
nn	O
table	O
estimating	O
p	O
yy	O
from	O
a	O
confusion	B
matrix	I
abbreviations	O
fnr	O
false	B
negative	I
rate	B
fpr	O
false	B
positive	I
rate	B
tnr	O
true	O
negative	O
rate	B
tpr	O
true	B
positive	I
rate	B
if	O
be	O
our	O
decision	B
rule	I
where	O
f	O
is	O
a	O
measure	O
of	O
confidence	O
that	O
y	O
should	O
be	O
monotonically	O
related	O
to	O
py	O
but	O
does	O
not	O
need	O
to	O
be	O
a	O
probability	O
and	O
is	O
some	O
threshold	O
parameter	B
for	O
each	O
given	O
value	O
of	O
we	O
can	O
apply	O
our	O
decision	B
rule	I
and	O
count	O
the	O
number	O
of	O
true	O
positives	O
false	O
positives	O
true	O
negatives	O
and	O
false	O
negatives	O
that	O
occur	O
as	O
shown	O
in	O
table	O
this	O
table	O
of	O
errors	O
is	O
called	O
a	O
confusion	B
matrix	I
from	O
this	O
table	O
we	O
can	O
compute	O
the	O
true	B
positive	I
rate	B
also	O
known	O
as	O
the	O
sensitivity	B
recall	B
or	O
hit	B
rate	B
by	O
using	O
t	O
p	O
r	O
t	O
pn	O
p	O
y	O
we	O
can	O
also	O
compute	O
the	O
false	B
positive	I
rate	B
also	O
called	O
the	O
false	B
alarm	I
rate	B
or	O
the	O
type	B
i	I
error	I
rate	B
by	O
using	O
f	O
p	O
r	O
f	O
pn	O
p	O
y	O
these	O
and	O
other	O
definitions	O
are	O
summarized	O
in	O
tables	O
and	O
we	O
can	O
combine	O
these	O
errors	O
in	O
any	O
way	O
we	O
choose	O
to	O
compute	O
a	O
loss	B
function	I
however	O
rather	O
than	O
than	O
computing	O
the	O
tpr	O
and	O
fpr	O
for	O
a	O
fixed	O
threshold	O
we	O
can	O
run	O
our	O
detector	O
for	O
a	O
set	O
of	O
thresholds	O
and	O
then	O
plot	O
the	O
tpr	O
vs	O
fpr	O
as	O
an	O
implicit	O
function	O
of	O
this	O
is	O
called	O
a	O
receiver	B
operating	I
characteristic	I
or	O
roc	B
curve	O
see	O
figure	O
for	O
an	O
example	O
any	O
system	O
can	O
achieve	O
the	O
point	O
on	O
the	O
bottom	O
left	O
p	O
r	O
t	O
p	O
r	O
by	O
setting	O
and	O
thus	O
classifying	O
everything	O
as	O
negative	O
similarly	O
any	O
system	O
can	O
achieve	O
the	O
point	O
on	O
the	O
top	O
right	O
p	O
r	O
t	O
p	O
r	O
by	O
setting	O
and	O
thus	O
classifying	O
everything	O
as	O
positive	O
if	O
a	O
system	O
is	O
performing	O
at	O
chance	O
level	O
then	O
we	O
can	O
achieve	O
any	O
point	O
on	O
the	O
diagonal	B
line	O
t	O
p	O
r	O
f	O
p	O
r	O
by	O
choosing	O
an	O
appropriate	O
threshold	O
a	O
system	O
that	O
perfectly	O
separates	O
the	O
positives	O
from	O
negatives	O
has	O
a	O
threshold	O
that	O
can	O
achieve	O
the	O
top	O
left	O
corner	O
p	O
r	O
t	O
p	O
r	O
by	O
varying	O
the	O
threshold	O
such	O
a	O
system	O
will	O
hug	O
the	O
left	O
axis	O
and	O
then	O
the	O
top	O
axis	O
as	O
shown	O
in	O
figure	O
the	O
quality	O
of	O
a	O
roc	B
curve	O
is	O
often	O
summarized	O
as	O
a	O
single	O
number	O
using	O
the	O
area	B
under	I
the	I
curve	I
or	O
auc	B
higher	O
auc	B
scores	B
are	O
better	O
the	O
maximum	O
is	O
obviously	O
another	O
summary	O
statistic	O
that	O
is	O
used	O
is	O
the	O
equal	B
error	I
rate	B
or	O
eer	B
also	O
called	O
the	O
cross	B
over	I
rate	B
defined	O
as	O
the	O
value	O
which	O
satisfies	O
f	O
p	O
r	O
f	O
n	O
r	O
since	O
f	O
n	O
r	O
t	O
p	O
r	O
we	O
can	O
compute	O
the	O
eer	B
by	O
drawing	O
a	O
line	O
from	O
the	O
top	O
left	O
to	O
the	O
bottom	O
right	O
and	O
seeing	O
where	O
it	O
intersects	O
the	O
roc	B
curve	O
points	O
a	O
and	O
b	O
in	O
figure	O
lower	O
eer	B
scores	B
are	O
better	O
the	O
minimum	O
is	O
obviously	O
chapter	O
bayesian	B
statistics	I
r	O
p	O
t	O
a	O
b	O
fpr	O
b	O
a	O
i	O
i	O
n	O
o	O
s	O
c	O
e	O
r	O
p	O
recall	B
figure	O
roc	B
curves	O
for	O
two	O
hypothetical	O
classification	O
systems	O
a	O
is	O
better	O
than	O
b	O
we	O
plot	O
the	O
true	B
positive	I
rate	B
vs	O
the	O
false	B
positive	I
rate	B
as	O
we	O
vary	O
the	O
threshold	O
we	O
also	O
indicate	O
the	O
equal	B
error	I
rate	B
with	O
the	O
red	O
and	O
blue	O
dots	O
and	O
the	O
area	B
under	I
the	I
curve	I
for	O
classifier	O
b	O
a	O
precision-recall	O
curve	O
for	O
two	O
hypothetical	O
classification	O
systems	O
a	O
is	O
better	O
than	O
b	O
figure	O
generated	O
by	O
prhand	O
y	O
y	O
y	O
y	O
t	O
p	O
nprecisionppv	O
f	O
p	O
nfdp	O
t	O
n	O
n	O
f	O
n	O
n	O
table	O
estimating	O
py	O
y	O
from	O
a	O
confusion	B
matrix	I
abbreviations	O
fdp	O
false	O
discovery	O
probability	O
npv	O
negative	O
predictive	B
value	O
ppv	O
positive	O
predictive	B
value	O
precision	B
recall	B
curves	O
when	O
trying	O
to	O
detect	O
a	O
rare	B
event	I
as	O
retrieving	O
a	O
relevant	O
document	O
or	O
finding	O
a	O
face	O
in	O
an	O
image	O
the	O
number	O
of	O
negatives	O
is	O
very	O
large	O
hence	O
comparing	O
t	O
p	O
r	O
t	O
pn	O
to	O
f	O
p	O
r	O
f	O
pn	O
is	O
not	O
very	O
informative	O
since	O
the	O
fpr	O
will	O
be	O
very	O
small	O
hence	O
all	O
the	O
action	B
in	O
the	O
roc	B
curve	O
will	O
occur	O
on	O
the	O
extreme	O
left	O
in	O
such	O
cases	O
it	O
is	O
common	O
to	O
plot	O
the	O
tpr	O
versus	O
the	O
number	O
of	O
false	O
positives	O
rather	O
than	O
vs	O
the	O
false	B
positive	I
rate	B
however	O
in	O
some	O
cases	O
the	O
very	O
notion	O
of	O
negative	O
is	O
not	O
well-defined	O
for	O
example	O
when	O
detecting	O
objects	O
in	O
images	O
section	O
if	O
the	O
detector	O
works	O
by	O
classifying	O
patches	O
then	O
the	O
number	O
of	O
patches	O
examined	O
and	O
hence	O
the	O
number	O
of	O
true	O
negatives	O
is	O
a	O
parameter	B
of	O
the	O
algorithm	O
not	O
part	O
of	O
the	O
problem	O
definition	O
so	O
we	O
would	O
like	O
to	O
use	O
a	O
measure	O
that	O
only	O
talks	O
about	O
positives	O
the	O
precision	B
is	O
defined	O
as	O
t	O
p	O
n	O
py	O
y	O
and	O
the	O
recall	B
is	O
defined	O
as	O
t	O
pn	O
p	O
y	O
precision	B
measures	O
what	O
fraction	O
of	O
our	O
detections	O
are	O
actually	O
positive	O
and	O
recall	B
measures	O
what	O
fraction	O
of	O
the	O
positives	O
we	O
actually	O
detected	O
if	O
yi	O
is	O
the	O
predicted	O
label	B
and	O
yi	O
is	O
the	O
true	O
label	B
we	O
can	O
estimate	O
precision	B
and	O
recall	B
using	O
i	O
yi	O
i	O
yi	O
i	O
yi	O
i	O
yi	O
p	O
r	O
a	O
precision	B
recall	B
curve	I
is	O
a	O
plot	O
of	O
precision	B
vs	O
recall	B
as	O
we	O
vary	O
the	O
threshold	O
see	O
figure	O
hugging	O
the	O
top	O
right	O
is	O
the	O
best	O
one	O
can	O
do	O
this	O
curve	O
can	O
be	O
summarized	O
as	O
a	O
single	O
number	O
using	O
the	O
mean	B
precision	B
over	O
bayesian	B
decision	B
theory	O
class	O
y	O
y	O
y	O
y	O
class	O
y	O
y	O
y	O
y	O
pooled	B
y	O
y	O
y	O
y	O
illustration	O
of	O
the	O
difference	O
between	O
macro-	O
and	O
micro-averaging	O
y	O
is	O
the	O
true	O
label	B
and	O
y	O
table	O
is	O
the	O
called	O
label	B
in	O
this	O
example	O
the	O
macro-averaged	O
precision	B
is	O
the	O
micro-averaged	O
precision	B
is	O
based	O
on	O
table	O
of	O
et	O
al	O
recall	B
values	O
which	O
approximates	O
the	O
area	B
under	I
the	I
curve	I
alternatively	O
one	O
can	O
quote	O
the	O
precision	B
for	O
a	O
fixed	O
recall	B
level	O
such	O
as	O
the	O
precision	B
of	O
the	O
first	O
k	O
entities	O
recalled	O
this	O
is	O
called	O
the	O
average	B
precision	B
at	I
k	I
score	O
this	O
measure	O
is	O
widely	O
used	O
when	O
evaluating	O
information	B
retrieval	I
systems	O
f-scores	O
for	O
a	O
fixed	O
threshold	O
one	O
can	O
compute	O
a	O
single	O
precision	B
and	O
recall	B
value	O
these	O
are	O
often	O
combined	O
into	O
a	O
single	O
statistic	O
called	O
the	O
f	B
score	I
or	O
score	O
which	O
is	O
the	O
harmonic	B
mean	B
of	O
precision	B
and	O
recall	B
r	O
r	O
p	O
using	O
equation	O
we	O
can	O
write	O
this	O
as	O
yi	O
yi	O
yi	O
yi	O
this	O
is	O
a	O
widely	O
used	O
measure	O
in	O
information	B
retrieval	I
systems	O
to	O
understand	O
why	O
we	O
use	O
the	O
harmonic	B
mean	B
instead	O
of	O
the	O
arithmetic	O
mean	B
consider	O
the	O
following	O
scenario	O
suppose	O
we	O
recall	B
all	O
entries	O
so	O
r	O
the	O
precision	B
will	O
be	O
the	O
given	O
by	O
the	O
prevalence	B
py	O
suppose	O
the	O
prevalence	B
is	O
low	O
say	O
py	O
by	O
contrast	O
the	O
arithmetic	O
mean	B
of	O
p	O
and	O
r	O
is	O
given	O
by	O
harmonic	B
mean	B
of	O
this	O
strategy	O
is	O
only	O
in	O
the	O
multi-class	O
case	O
for	O
document	O
classification	O
problems	O
there	O
are	O
two	O
ways	O
to	O
generalize	B
scores	B
the	O
first	O
is	O
called	O
macro-averaged	O
and	O
is	O
defined	O
as	O
where	O
is	O
the	O
score	O
obtained	O
on	O
the	O
task	O
of	O
distinguishing	O
class	O
c	O
from	O
all	O
the	O
others	O
the	O
other	O
is	O
called	O
micro-averaged	O
and	O
is	O
defined	O
as	O
the	O
score	O
where	O
we	O
pool	O
all	O
the	O
counts	O
from	O
each	O
class	O
s	O
contingency	B
table	I
table	O
gives	O
a	O
worked	O
example	O
that	O
illustrates	O
the	O
difference	O
we	O
see	O
that	O
the	O
precision	B
of	O
class	O
is	O
and	O
of	O
class	O
is	O
the	O
macro-averaged	O
precision	B
is	O
therefore	O
whereas	O
the	O
micro-averaged	O
precision	B
is	O
the	O
latter	O
is	O
much	O
closer	O
to	O
the	O
precision	B
of	O
class	O
than	O
to	O
the	O
precision	B
of	O
class	O
since	O
class	O
is	O
five	O
times	O
larger	O
than	O
class	O
to	O
give	O
equal	O
weight	O
to	O
each	O
class	O
use	O
macro-averaging	O
chapter	O
bayesian	B
statistics	I
false	O
discovery	O
rates	O
suppose	O
we	O
are	O
trying	O
to	O
discover	O
a	O
rare	O
phenomenon	O
using	O
some	O
kind	O
of	O
high	B
throughput	I
measurement	O
device	O
such	O
as	O
a	O
gene	O
expression	O
micro	O
array	O
or	O
a	O
radio	O
telescope	O
we	O
will	O
need	O
to	O
make	O
many	O
binary	O
decisions	O
of	O
the	O
form	O
pyi	O
whered	O
and	O
n	O
may	O
be	O
large	O
this	O
is	O
called	O
multiple	B
hypothesis	I
testing	I
note	O
that	O
the	O
difference	O
from	O
standard	O
binary	O
classification	O
is	O
that	O
we	O
are	O
classifying	O
yi	O
based	O
on	O
all	O
the	O
data	O
not	O
just	O
based	O
on	O
xi	O
so	O
this	O
is	O
a	O
simultaneous	O
classification	O
problem	O
where	O
we	O
might	O
hope	O
to	O
do	O
better	O
than	O
a	O
series	O
of	O
individual	O
classification	O
problems	O
how	O
should	O
we	O
set	O
the	O
threshold	O
a	O
natural	O
approach	O
is	O
to	O
try	O
to	O
minimize	O
the	O
expected	O
number	O
of	O
false	O
positives	O
in	O
the	O
bayesian	B
approach	O
this	O
can	O
be	O
computed	O
as	O
follows	O
f	O
d	O
i	O
pi	O
pr	O
error	O
ipi	O
discovery	O
where	O
pi	O
pyi	O
is	O
your	O
belief	O
that	O
this	O
object	O
exhibits	O
the	O
phenomenon	O
in	O
question	O
we	O
then	O
define	O
the	O
posterior	O
expected	O
false	B
discovery	I
rate	B
as	O
follows	O
f	O
dr	O
f	O
d	O
where	O
n	O
i	O
ipi	O
is	O
the	O
number	O
of	O
discovered	O
items	O
given	O
a	O
desired	O
fdr	O
tolerance	O
say	O
one	O
can	O
then	O
adapt	O
to	O
achieve	O
this	O
this	O
is	O
called	O
the	O
direct	B
posterior	I
probability	I
approach	I
to	O
controlling	O
the	O
fdr	O
et	O
al	O
muller	O
et	O
al	O
in	O
order	O
to	O
control	O
the	O
fdr	O
it	O
is	O
very	O
helpful	O
to	O
estimate	O
the	O
pi	O
s	O
jointly	O
using	O
a	O
hierarchical	B
bayesian	B
model	I
as	O
in	O
section	O
rather	O
than	O
independently	O
this	O
allows	O
the	O
pooling	O
of	O
statistical	O
strength	O
and	O
thus	O
lower	O
fdr	O
see	O
e	O
g	O
and	O
hochberg	O
for	O
more	O
information	B
other	O
topics	O
in	O
this	O
section	O
we	O
briefly	O
mention	O
a	O
few	O
other	O
topics	O
related	O
to	O
bayesian	B
decision	B
theory	O
we	O
do	O
not	O
have	O
space	O
to	O
go	O
into	O
detail	O
but	O
we	O
include	O
pointers	O
to	O
the	O
relevant	O
literature	O
contextual	O
bandits	O
a	O
one-armed	B
bandit	I
is	O
a	O
colloquial	O
term	O
for	O
a	O
slot	B
machine	I
found	O
in	O
casinos	O
around	O
the	O
world	O
the	O
game	O
is	O
this	O
you	O
insert	O
some	O
money	O
pull	O
an	O
arm	O
and	O
wait	O
for	O
the	O
machine	O
to	O
stop	O
if	O
you	O
re	O
lucky	O
you	O
win	O
some	O
money	O
now	O
imagine	O
there	O
is	O
a	O
bank	O
of	O
k	O
such	O
machines	O
to	O
choose	O
from	O
which	O
one	O
should	O
you	O
use	O
this	O
is	O
called	O
a	O
multi-armed	B
bandit	I
and	O
can	O
be	O
modeled	O
using	O
bayesian	B
decision	B
theory	O
there	O
are	O
k	O
possible	O
actions	B
and	O
each	O
action	B
has	O
an	O
unknown	B
reward	B
function	O
rk	O
by	O
maintaining	O
a	O
belief	B
state	B
k	O
prkd	O
one	O
can	O
devise	O
an	O
optimal	O
policy	B
this	O
can	O
be	O
compiled	O
into	O
a	O
series	O
of	O
gittins	B
indices	I
this	O
optimally	O
solves	O
the	O
exploration-exploitation	B
tradeoff	O
which	O
specifies	O
how	O
many	O
times	O
one	O
should	O
try	O
each	O
action	B
before	O
deciding	O
to	O
go	O
with	O
the	O
winner	O
now	O
consider	O
an	O
extension	B
where	O
each	O
arm	O
and	O
the	O
player	O
has	O
an	O
associated	O
feature	O
vector	O
call	O
all	O
these	O
features	B
x	O
this	O
is	O
called	O
a	O
contextual	B
bandit	I
e	O
g	O
scott	O
li	O
et	O
al	O
for	O
example	O
the	O
arms	O
could	O
represent	O
ads	O
or	O
news	O
articles	O
which	O
we	O
want	O
to	O
show	O
to	O
the	O
user	O
and	O
the	O
features	B
could	O
represent	O
properties	O
of	O
these	O
ads	O
or	O
articles	O
such	O
bayesian	B
decision	B
theory	O
as	O
a	O
bag	B
of	I
words	I
as	O
well	O
as	O
properties	O
of	O
the	O
user	O
such	O
as	O
demographics	O
if	O
we	O
assume	O
a	O
linear	O
model	O
for	O
reward	B
rk	O
t	O
k	O
x	O
we	O
can	O
maintain	O
a	O
distribution	O
over	O
the	O
parameters	O
of	O
each	O
arm	O
p	O
kd	O
where	O
d	O
is	O
a	O
series	O
of	O
tuples	B
of	O
the	O
form	O
x	O
r	O
which	O
specifies	O
which	O
arm	O
was	O
pulled	O
what	O
its	O
features	B
were	O
and	O
what	O
the	O
resulting	O
outcome	O
was	O
r	O
if	O
the	O
user	O
clicked	O
on	O
the	O
ad	O
and	O
r	O
otherwise	O
we	O
discuss	O
ways	O
to	O
compute	O
p	O
kd	O
from	O
linear	O
and	O
logistic	B
regression	B
models	O
in	O
later	O
chapters	O
given	O
the	O
posterior	O
we	O
must	O
decide	O
what	O
action	B
to	O
take	O
one	O
common	O
heuristic	O
known	O
as	O
ucb	B
stands	O
for	O
upper	O
confidence	O
bound	O
is	O
to	O
take	O
the	O
action	B
which	O
maximizes	O
k	O
k	O
argmax	O
k	O
k	O
k	O
var	B
and	O
is	O
a	O
tuning	O
parameter	B
that	O
trades	O
off	O
exploration	O
where	O
k	O
e	O
and	O
exploitation	O
the	O
intuition	O
is	O
that	O
we	O
should	O
pick	O
actions	B
about	O
which	O
we	O
believe	O
are	O
good	O
k	O
is	O
large	O
and	O
or	O
actions	B
about	O
which	O
we	O
are	O
uncertain	O
k	O
is	O
large	O
an	O
even	O
simpler	O
method	O
known	O
as	O
thompson	B
sampling	I
is	O
as	O
follows	O
at	O
each	O
step	O
we	O
pick	O
action	B
k	O
with	O
a	O
probability	O
that	O
is	O
equal	O
to	O
its	O
probability	O
of	O
being	O
the	O
optimal	B
action	B
ie	O
x	O
max	O
e	O
x	O
pk	O
we	O
can	O
approximate	O
this	O
by	O
drawing	O
a	O
single	O
sample	O
from	O
the	O
posterior	O
t	O
p	O
and	O
then	O
choosing	O
k	O
despite	O
its	O
simplicity	O
this	O
has	O
been	O
shown	O
to	O
work	O
quite	O
argmaxk	O
e	O
well	O
and	O
li	O
rx	O
k	O
t	O
utility	O
theory	O
suppose	O
we	O
are	O
a	O
doctor	O
trying	O
to	O
decide	O
whether	O
to	O
operate	O
on	O
a	O
patient	O
or	O
not	O
we	O
imagine	O
there	O
are	O
states	O
of	O
nature	O
the	O
patient	O
has	O
no	O
cancer	O
the	O
patient	O
has	O
lung	O
cancer	O
or	O
the	O
patient	O
has	O
breast	O
cancer	O
since	O
the	O
action	B
and	O
state	B
space	I
is	O
discrete	B
we	O
can	O
represent	O
the	O
loss	B
function	I
l	O
a	O
as	O
a	O
loss	B
matrix	I
such	O
as	O
the	O
following	O
surgery	O
no	O
surgery	O
no	O
cancer	O
lung	O
cancer	O
breast	O
cancer	O
these	O
numbers	O
reflects	O
the	O
fact	O
that	O
not	O
performing	O
surgery	O
when	O
the	O
patient	O
has	O
cancer	O
is	O
very	O
bad	O
of	O
or	O
depending	O
on	O
the	O
type	O
of	O
cancer	O
since	O
the	O
patient	O
might	O
die	O
not	O
performing	O
surgery	O
when	O
the	O
patient	O
does	O
not	O
have	O
cancer	O
incurs	O
no	O
loss	B
performing	O
surgery	O
when	O
the	O
patient	O
does	O
not	O
have	O
cancer	O
is	O
wasteful	O
of	O
and	O
performing	O
surgery	O
when	O
the	O
patient	O
does	O
have	O
cancer	O
is	O
painful	O
but	O
necessary	O
it	O
is	O
natural	O
to	O
ask	O
where	O
these	O
numbers	O
come	O
from	O
ultimately	O
they	O
represent	O
the	O
personal	O
preferences	B
or	O
values	O
of	O
a	O
fictitious	O
doctor	O
and	O
are	O
somewhat	O
arbitrary	O
just	O
as	O
some	O
people	O
prefer	O
chocolate	O
ice	O
cream	O
and	O
others	O
prefer	O
vanilla	O
there	O
is	O
no	O
such	O
thing	O
as	O
the	O
right	O
loss	B
utility	B
function	I
however	O
it	O
can	O
be	O
shown	O
e	O
g	O
that	O
any	O
set	O
of	O
consistent	B
preferences	B
can	O
be	O
converted	O
to	O
a	O
scalar	O
loss	B
utility	B
function	I
note	O
that	O
utility	O
can	O
be	O
measured	O
on	O
an	O
arbitrary	O
scale	O
such	O
as	O
dollars	O
since	O
it	O
is	O
only	O
relative	O
values	O
that	O
people	O
are	O
often	O
squeamish	O
about	O
talking	O
about	O
human	O
lives	O
in	O
monetary	O
terms	O
but	O
all	O
decision	B
making	O
requires	O
chapter	O
bayesian	B
statistics	I
sequential	B
decision	B
theory	O
so	O
far	O
we	O
have	O
concentrated	O
on	O
one-shot	O
decision	B
problems	O
where	O
we	O
only	O
have	O
to	O
make	O
one	O
decision	B
and	O
then	O
the	O
game	O
ends	O
in	O
setion	O
we	O
will	O
generalize	B
this	O
to	O
multi-stage	B
or	O
sequential	B
decision	B
problems	O
such	O
problems	O
frequently	O
arise	O
in	O
many	O
business	O
and	O
engineering	O
settings	O
this	O
is	O
closely	O
related	O
to	O
the	O
problem	O
of	O
reinforcement	B
learning	B
however	O
further	O
discussion	O
of	O
this	O
point	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
exercises	O
exercise	O
proof	O
that	O
a	O
mixture	B
of	I
conjugate	B
priors	I
is	O
indeed	O
conjugate	O
derive	O
equation	O
exercise	O
optimal	O
threshold	O
on	O
classification	O
probability	O
consider	O
a	O
case	O
where	O
we	O
have	O
learned	O
a	O
conditional	B
probability	I
distribution	I
p	O
suppose	O
there	O
are	O
only	O
two	O
classes	O
and	O
let	O
p	O
and	O
p	O
consider	O
the	O
loss	B
matrix	I
below	O
predicted	O
label	B
y	O
true	O
label	B
y	O
a	O
show	O
that	O
the	O
decision	B
y	O
that	O
minimizes	O
the	O
expected	O
loss	B
is	O
equivalent	O
to	O
setting	O
a	O
probability	O
threshold	O
and	O
predicting	O
y	O
if	O
and	O
y	O
if	O
what	O
is	O
as	O
a	O
function	O
of	O
and	O
your	O
work	O
b	O
show	O
a	O
loss	B
matrix	I
where	O
the	O
threshold	O
is	O
your	O
work	O
exercise	O
reject	O
option	O
in	O
classifiers	O
et	O
al	O
in	O
many	O
classification	O
problems	O
one	O
has	O
the	O
option	O
either	O
of	O
assigning	O
x	O
to	O
class	O
j	O
or	O
if	O
you	O
are	O
too	O
uncertain	O
of	O
choosing	O
the	O
reject	O
option	O
if	O
the	O
cost	O
for	O
rejects	O
is	O
less	O
than	O
the	O
cost	O
of	O
falsely	O
classifying	O
the	O
object	O
it	O
may	O
be	O
the	O
optimal	B
action	B
let	O
i	O
mean	B
you	O
choose	O
action	B
i	O
for	O
i	O
c	O
where	O
c	O
is	O
the	O
number	O
of	O
classes	O
and	O
c	O
is	O
the	O
reject	B
action	B
let	O
y	O
j	O
be	O
the	O
true	O
unknown	B
state	B
of	O
nature	O
define	O
the	O
loss	B
function	I
as	O
follows	O
iy	O
j	O
if	O
i	O
j	O
and	O
i	O
j	O
c	O
r	O
s	O
if	O
i	O
c	O
otherwise	O
in	O
otherwords	O
you	O
incur	O
loss	B
if	O
you	O
correctly	O
classify	O
you	O
incur	O
r	O
loss	B
if	O
you	O
choose	O
the	O
reject	O
option	O
and	O
you	O
incur	O
s	O
loss	B
if	O
you	O
make	O
a	O
substitution	O
error	O
tradeoffs	O
and	O
one	O
needs	O
to	O
use	O
some	O
kind	O
of	O
currency	O
to	O
compare	O
different	O
courses	O
of	O
action	B
insurance	O
companies	O
do	O
this	O
all	O
the	O
time	O
ross	O
schachter	O
a	O
decision	B
theorist	O
at	O
stanford	O
university	O
likes	O
to	O
tell	O
a	O
story	O
of	O
a	O
school	O
board	O
who	O
rejected	O
a	O
study	O
on	O
absestos	O
removal	O
from	O
schools	O
because	O
it	O
performed	O
a	O
cost-benefit	O
analysis	O
which	O
was	O
considered	O
inhumane	O
because	O
they	O
put	O
a	O
dollar	O
value	O
on	O
children	B
s	O
health	O
the	O
result	O
of	O
rejecting	O
the	O
report	O
was	O
that	O
the	O
absestos	O
was	O
not	O
removed	O
which	O
is	O
surely	O
more	O
inhumane	O
in	O
medical	O
domains	O
one	O
often	O
measures	O
utility	O
in	O
terms	O
of	O
qaly	B
or	O
quality-adjusted	O
life-years	O
instead	O
of	O
dollars	O
but	O
it	O
s	O
the	O
same	O
idea	O
of	O
course	O
even	O
if	O
you	O
do	O
not	O
explicitly	O
specify	O
how	O
much	O
you	O
value	O
different	O
people	O
s	O
lives	O
your	O
behavior	O
will	O
reveal	O
your	O
implicit	O
values	O
preferences	B
and	O
these	O
preferences	B
can	O
then	O
be	O
converted	O
to	O
a	O
real-valued	O
scale	O
such	O
as	O
dollars	O
or	O
qaly	B
inferring	O
a	O
utility	B
function	I
from	O
behavior	O
is	O
called	O
inverse	B
reinforcement	B
learning	B
bayesian	B
decision	B
theory	O
decision	B
y	O
predict	O
predict	O
reject	O
true	O
label	B
y	O
a	O
show	O
that	O
the	O
minimum	O
risk	B
is	O
obtained	O
if	O
we	O
decide	O
y	O
j	O
if	O
py	O
jx	O
py	O
kx	O
for	O
all	O
k	O
j	O
is	O
the	O
most	O
probable	O
class	O
and	O
if	O
py	O
jx	O
r	O
s	O
otherwise	O
we	O
decide	O
to	O
reject	O
b	O
describe	O
qualitatively	O
what	O
happens	O
as	O
r	O
s	O
is	O
increased	O
from	O
to	O
the	O
relative	O
cost	O
of	O
rejection	O
increases	O
exercise	O
more	O
reject	O
options	O
in	O
many	O
applications	O
the	O
classifier	O
is	O
allowed	O
to	O
reject	O
a	O
test	O
example	O
rather	O
than	O
classifying	O
it	O
into	O
one	O
of	O
the	O
classes	O
consider	O
for	O
example	O
a	O
case	O
in	O
which	O
the	O
cost	O
of	O
a	O
misclassification	O
is	O
but	O
the	O
cost	O
of	O
having	O
a	O
human	O
manually	O
make	O
the	O
decison	O
is	O
only	O
we	O
can	O
formulate	O
this	O
as	O
the	O
following	O
loss	B
matrix	I
a	O
suppose	O
p	O
is	O
predicted	O
to	O
be	O
which	O
decision	B
minimizes	O
the	O
expected	O
loss	B
b	O
now	O
suppose	O
p	O
now	O
which	O
decision	B
minimizes	O
the	O
expected	O
loss	B
c	O
show	O
that	O
in	O
general	O
for	O
this	O
loss	B
matrix	I
but	O
for	O
any	O
posterior	O
distribution	O
there	O
will	O
be	O
two	O
thresholds	O
and	O
such	O
that	O
the	O
optimal	O
decisionn	O
is	O
to	O
predict	O
if	O
reject	O
if	O
and	O
predict	O
if	O
py	O
what	O
are	O
these	O
thresholds	O
exercise	O
newsvendor	O
problem	O
consider	O
the	O
following	O
classic	O
problem	O
in	O
decision	B
theory	O
economics	O
suppose	O
you	O
are	O
trying	O
to	O
decide	O
how	O
much	O
quantity	O
q	O
of	O
some	O
product	O
newspapers	O
to	O
buy	O
to	O
maximize	O
your	O
profits	O
the	O
optimal	O
amount	O
will	O
depend	O
on	O
how	O
much	O
demand	O
d	O
you	O
think	O
there	O
is	O
for	O
your	O
product	O
as	O
well	O
as	O
its	O
cost	O
to	O
you	O
c	O
and	O
its	O
selling	O
price	O
p	O
suppose	O
d	O
is	O
unknown	B
but	O
has	O
pdf	B
f	O
and	O
cdf	B
f	O
we	O
can	O
evaluate	O
the	O
expected	O
profit	O
by	O
considering	O
two	O
cases	O
if	O
d	O
q	O
then	O
we	O
sell	O
all	O
q	O
items	O
and	O
make	O
profit	O
cq	O
but	O
if	O
d	O
q	O
we	O
only	O
sell	O
d	O
items	O
at	O
profit	O
cd	B
but	O
have	O
wasted	O
cq	O
d	O
on	O
the	O
unsold	O
items	O
so	O
the	O
expected	O
profit	O
if	O
we	O
buy	O
quantity	O
q	O
is	O
cdf	B
cqf	O
cq	O
df	O
q	O
q	O
e	O
q	O
simplify	O
this	O
expression	O
and	O
then	O
take	O
derivatives	O
wrt	O
q	O
to	O
show	O
that	O
the	O
optimal	O
quantity	O
q	O
maximizes	O
the	O
expected	O
profit	O
satisfies	O
f	O
p	O
c	O
p	O
exercise	O
bayes	O
factors	B
and	O
roc	B
curves	O
let	O
b	O
be	O
the	O
bayes	B
factor	B
in	O
favor	O
of	O
model	O
suppose	O
we	O
plot	O
two	O
roc	B
curves	O
one	O
computed	O
by	O
thresholding	O
b	O
and	O
the	O
other	O
computed	O
by	O
thresholding	O
will	O
they	O
be	O
the	O
same	O
or	O
different	O
explain	O
why	O
exercise	O
bayes	B
model	I
averaging	I
helps	O
predictive	B
accuracy	O
let	O
be	O
a	O
quantity	O
that	O
we	O
want	O
to	O
predict	O
let	O
d	O
be	O
the	O
observed	O
data	O
and	O
m	O
be	O
a	O
finite	O
set	O
of	O
models	O
suppose	O
our	O
action	B
is	O
to	O
provide	O
a	O
probabilistic	O
prediction	O
p	O
and	O
the	O
loss	B
function	I
is	O
l	O
p	O
chapter	O
bayesian	B
statistics	I
log	O
p	O
we	O
can	O
either	O
perform	O
bayes	B
model	I
averaging	I
and	O
predict	O
using	O
pbm	O
a	O
p	O
m	O
m	O
or	O
we	O
could	O
predict	O
using	O
any	O
single	O
model	O
plugin	O
approximation	O
show	O
that	O
for	O
all	O
models	O
m	O
m	O
the	O
posterior	B
expected	I
loss	B
using	O
bma	O
is	O
lower	O
i	O
e	O
pm	O
p	O
e	O
l	O
pbm	O
a	O
e	O
pm	O
where	O
the	O
expectation	O
over	O
is	O
with	O
respect	O
to	O
p	O
p	O
m	O
m	O
hint	O
use	O
the	O
non-negativity	O
of	O
the	O
kl	B
divergence	I
exercise	O
mle	B
and	O
model	B
selection	I
for	O
a	O
discrete	B
distribution	O
jaakkola	O
let	O
x	O
denote	O
the	O
result	O
of	O
a	O
coin	O
toss	O
for	O
tails	O
x	O
for	O
heads	O
the	O
coin	O
is	O
potentially	O
biased	O
so	O
that	O
heads	O
occurs	O
with	O
probability	O
suppose	O
that	O
someone	O
else	O
observes	O
the	O
coin	O
flip	O
and	O
reports	O
to	O
you	O
the	O
outcome	O
y	O
but	O
this	O
person	O
is	O
unreliable	O
and	O
only	O
reports	O
the	O
result	O
correctly	O
with	O
probability	O
i	O
e	O
pyx	O
is	O
given	O
by	O
x	O
x	O
y	O
y	O
assume	O
that	O
is	O
independent	O
of	O
x	O
and	O
a	O
write	O
down	O
the	O
joint	B
probability	I
distribution	I
px	O
y	O
as	O
a	O
table	O
in	O
terms	O
of	O
b	O
suppose	O
have	O
the	O
following	O
dataset	O
x	O
y	O
what	O
are	O
the	O
mles	O
for	O
and	O
justify	O
your	O
answer	O
hint	O
note	O
that	O
the	O
likelihood	B
function	O
factorizes	O
px	O
y	O
what	O
is	O
pd	O
where	O
denotes	O
this	O
model	O
fractional	O
form	O
if	O
you	O
wish	O
may	O
leave	O
your	O
answer	O
in	O
c	O
now	O
consider	O
a	O
model	O
with	O
parameters	O
representing	O
px	O
y	O
xy	O
of	O
these	O
parameters	O
are	O
free	O
to	O
vary	O
since	O
they	O
must	O
sum	O
to	O
one	O
what	O
is	O
the	O
mle	B
of	O
what	O
is	O
pd	O
where	O
denotes	O
this	O
model	O
d	O
suppose	O
we	O
are	O
not	O
sure	O
which	O
model	O
is	O
correct	O
we	O
compute	O
the	O
leave-one-out	O
cross	O
validated	O
log	O
likelihood	B
of	O
the	O
model	O
and	O
the	O
model	O
as	O
follows	O
lm	O
log	O
pxi	O
yim	O
i	O
and	O
i	O
denotes	O
the	O
mle	B
computed	O
on	O
d	O
excluding	O
row	O
i	O
which	O
model	O
will	O
cv	B
pick	O
and	O
why	O
hint	O
notice	O
how	O
the	O
table	O
of	O
counts	O
changes	O
when	O
you	O
omit	O
each	O
training	O
case	O
one	O
at	O
a	O
time	O
bayesian	B
decision	B
theory	O
e	O
recall	B
that	O
an	O
alternative	O
to	O
cv	B
is	O
to	O
use	O
the	O
bic	B
score	O
defined	O
as	O
bicmd	O
log	O
pd	O
m	O
le	O
dofm	O
log	O
n	O
where	O
dofm	O
is	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
model	O
compute	O
the	O
bic	B
scores	B
for	O
both	O
models	O
log	O
base	O
e	O
which	O
model	O
does	O
bic	B
prefer	O
exercise	O
posterior	B
median	B
is	O
optimal	O
estimate	O
under	O
loss	B
prove	O
that	O
the	O
posterior	B
median	B
is	O
optimal	O
estimate	O
under	O
loss	B
exercise	O
decision	B
rule	I
for	O
trading	O
off	O
fps	O
and	O
fns	O
if	O
lf	O
n	O
clf	O
p	O
show	O
that	O
we	O
should	O
pick	O
y	O
iff	B
py	O
where	O
c	O
frequentist	B
statistics	I
introduction	O
the	O
approach	O
to	O
statistical	O
inference	B
that	O
we	O
described	O
in	O
chapter	O
is	O
known	O
as	O
bayesian	B
statistics	I
perhaps	O
surprisingly	O
this	O
is	O
considered	O
controversial	O
by	O
some	O
people	O
whereas	O
the	O
application	O
of	O
bayes	B
rule	I
to	O
non-statistical	O
problems	O
such	O
as	O
medical	O
diagnosis	O
spam	B
filtering	B
or	O
airplane	O
tracking	B
is	O
not	O
controversial	O
the	O
reason	O
for	O
the	O
objection	O
has	O
to	O
do	O
with	O
a	O
misguided	O
distinction	O
between	O
parameters	O
of	O
a	O
statistical	O
model	O
and	O
other	O
kinds	O
of	O
unknown	B
attempts	O
have	O
been	O
made	O
to	O
devise	O
approaches	O
to	O
statistical	O
inference	B
that	O
avoid	O
treating	O
parameters	O
like	O
random	O
variables	O
and	O
which	O
thus	O
avoid	O
the	O
use	O
of	O
priors	O
and	O
bayes	B
rule	I
such	O
approaches	O
are	O
known	O
as	O
frequentist	B
statistics	I
classical	B
statistics	I
or	O
orthodox	B
statistics	I
instead	O
of	O
being	O
based	O
on	O
the	O
posterior	O
distribution	O
they	O
are	O
based	O
on	O
the	O
concept	B
of	O
a	O
sampling	B
distribution	I
this	O
is	O
the	O
distribution	O
that	O
an	O
estimator	B
has	O
when	O
applied	O
to	O
multiple	O
data	O
sets	O
sampled	O
from	O
the	O
true	O
but	O
unknown	B
distribution	O
see	O
section	O
for	O
details	O
it	O
is	O
this	O
notion	O
of	O
variation	O
across	O
repeated	O
trials	O
that	O
forms	O
the	O
basis	O
for	O
modeling	O
uncertainty	B
used	O
by	O
the	O
frequentist	B
approach	O
by	O
contrast	O
in	O
the	O
bayesian	B
approach	O
we	O
only	O
ever	O
condition	O
on	O
the	O
actually	O
observed	O
data	O
there	O
is	O
no	O
notion	O
of	O
repeated	O
trials	O
this	O
allows	O
the	O
bayesian	B
to	O
compute	O
the	O
probability	O
of	O
one-off	O
events	O
as	O
we	O
discussed	O
in	O
section	O
perhaps	O
more	O
importantly	O
the	O
bayesian	B
approach	O
avoids	O
certain	O
paradoxes	O
that	O
plague	O
the	O
frequentist	B
approach	O
section	O
nevertheless	O
it	O
is	O
important	O
to	O
be	O
familiar	O
with	O
frequentist	B
statistics	I
section	O
since	O
it	O
is	O
widely	O
used	O
in	O
machine	B
learning	B
sampling	B
distribution	I
of	O
an	O
estimator	B
in	O
frequentist	B
statistics	I
a	O
parameter	B
estimate	O
is	O
computed	O
by	O
applying	O
an	O
estimator	B
to	O
some	O
data	O
d	O
so	O
the	O
parameter	B
is	O
viewed	O
as	O
fixed	O
and	O
the	O
data	O
as	O
random	O
which	O
is	O
the	O
exact	O
opposite	O
of	O
the	O
bayesian	B
approach	O
the	O
uncertainty	B
in	O
the	O
parameter	B
estimate	O
can	O
be	O
measured	O
by	O
computing	O
the	O
sampling	B
distribution	I
of	O
the	O
estimator	B
to	O
understand	O
this	O
parameters	O
are	O
sometimes	O
considered	O
to	O
represent	O
true	O
unknown	B
physical	O
quantities	O
which	O
are	O
therefore	O
not	O
random	O
however	O
we	O
have	O
seen	O
that	O
it	O
is	O
perfectly	O
reasonable	O
to	O
use	O
a	O
probability	O
distribution	O
to	O
represent	O
one	O
s	O
uncertainty	B
about	O
an	O
unknown	B
constant	O
chapter	O
frequentist	B
statistics	I
boot	O
true	O
mle	B
se	O
boot	O
true	O
mle	B
se	O
figure	O
a	O
bootstrap	B
approximation	O
to	O
the	O
sampling	B
distribution	I
of	O
for	O
a	O
bernoulli	B
distribution	O
we	O
use	O
b	O
bootstrap	B
samples	B
the	O
n	O
datacases	O
were	O
generated	O
from	O
ber	O
mle	B
with	O
n	O
mle	B
with	O
n	O
figure	O
generated	O
by	O
bootstrapdemober	O
i	O
where	O
xs	O
i	O
p	O
concept	B
imagine	O
sampling	O
many	O
different	O
data	O
sets	O
ds	O
from	O
some	O
true	O
model	O
p	O
i	O
e	O
let	O
ds	O
is	O
the	O
true	O
parameter	B
here	O
s	O
indexes	O
the	O
sampled	O
data	O
set	O
and	O
n	O
is	O
the	O
size	O
of	O
each	O
such	O
dataset	O
now	O
apply	O
the	O
estimator	B
to	O
each	O
ds	O
to	O
get	O
a	O
set	O
of	O
estimates	O
as	O
we	O
let	O
s	O
the	O
distribution	O
induced	O
on	O
is	O
the	O
sampling	B
distribution	I
of	O
the	O
estimator	B
we	O
will	O
discuss	O
various	O
ways	O
to	O
use	O
the	O
sampling	B
distribution	I
in	O
later	O
sections	O
but	O
first	O
we	O
sketch	O
two	O
approaches	O
for	O
computing	O
the	O
sampling	B
distribution	I
itself	O
and	O
bootstrap	B
the	O
bootstrap	B
is	O
a	O
simple	O
monte	B
carlo	I
technique	O
to	O
approximate	O
the	O
sampling	B
distribution	I
this	O
is	O
particularly	O
useful	O
in	O
cases	O
where	O
the	O
estimator	B
is	O
a	O
complex	O
function	O
of	O
the	O
true	O
parameters	O
the	O
idea	O
is	O
simple	O
if	O
we	O
knew	O
the	O
true	O
parameters	O
we	O
could	O
generate	O
many	O
s	O
fake	O
i	O
p	O
datasets	O
each	O
of	O
size	O
n	O
from	O
the	O
true	O
distribution	O
xs	O
for	O
s	O
s	O
i	O
n	O
we	O
could	O
then	O
compute	O
our	O
estimator	B
from	O
each	O
sample	O
s	O
f	O
and	O
use	O
the	O
empirical	B
distribution	I
of	O
the	O
resulting	O
samples	B
as	O
our	O
estimate	O
of	O
the	O
sampling	B
distribution	I
since	O
is	O
unknown	B
the	O
idea	O
of	O
the	O
parametric	B
bootstrap	B
is	O
to	O
generate	O
the	O
samples	B
using	O
instead	O
an	O
alternative	O
called	O
the	O
non-parametric	B
bootstrap	B
is	O
to	O
sample	O
the	O
xs	O
i	O
replacement	O
from	O
the	O
original	O
data	O
d	O
and	O
then	O
compute	O
the	O
induced	O
distribution	O
as	O
before	O
some	O
methods	O
for	O
speeding	O
up	O
the	O
bootstrap	B
when	O
applied	O
to	O
massive	O
data	O
sets	O
are	O
discussed	O
in	O
et	O
al	O
figure	O
shows	O
an	O
example	O
where	O
we	O
compute	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	B
for	O
a	O
bernoulli	B
using	O
the	O
parametric	B
bootstrap	B
using	O
the	O
non-parametric	B
bootstrap	B
are	O
essentially	O
the	O
same	O
we	O
see	O
that	O
the	O
sampling	B
distribution	I
is	O
asymmetric	O
and	O
therefore	O
quite	O
far	O
from	O
gaussian	B
when	O
n	O
when	O
n	O
the	O
distribution	O
looks	O
more	O
gaussian	B
as	O
theory	O
suggests	O
below	O
computed	O
by	O
the	O
bootstrap	B
and	O
parameter	B
values	O
sampled	O
from	O
the	O
posterior	O
s	O
p	O
a	O
natural	O
question	O
is	O
what	O
is	O
the	O
connection	O
between	O
the	O
parameter	B
estimates	O
s	O
sampling	B
distribution	I
of	O
an	O
estimator	B
conceptually	O
they	O
are	O
quite	O
different	O
but	O
in	O
the	O
common	O
case	O
that	O
that	O
the	O
prior	O
is	O
not	O
very	O
strong	O
they	O
can	O
be	O
quite	O
similar	B
for	O
example	O
figure	O
shows	O
an	O
example	O
where	O
we	O
compute	O
the	O
posterior	O
using	O
a	O
uniform	O
prior	O
and	O
then	O
sample	O
from	O
it	O
we	O
see	O
that	O
the	O
posterior	O
and	O
the	O
sampling	B
distribution	I
are	O
quite	O
similar	B
so	O
one	O
can	O
think	O
of	O
the	O
bootstrap	B
distribution	O
as	O
a	O
poor	O
man	O
s	O
posterior	O
see	O
et	O
al	O
for	O
details	O
however	O
perhaps	O
surprisingly	O
bootstrap	B
can	O
be	O
slower	O
than	O
posterior	O
sampling	O
the	O
reason	O
is	O
that	O
the	O
bootstrap	B
has	O
to	O
fit	O
the	O
model	O
s	O
times	O
whereas	O
in	O
posterior	O
sampling	O
we	O
usually	O
only	O
fit	O
the	O
model	O
once	O
find	O
a	O
local	O
mode	B
and	O
then	O
perform	O
local	O
exploration	O
around	O
the	O
mode	B
such	O
local	O
exploration	O
is	O
usually	O
much	O
faster	O
than	O
fitting	O
a	O
model	O
from	O
scratch	O
large	O
sample	O
theory	O
for	O
the	O
mle	B
in	O
some	O
cases	O
the	O
sampling	B
distribution	I
for	O
some	O
estimators	O
can	O
be	O
computed	O
analytically	O
in	O
particular	O
it	O
can	O
be	O
shown	O
that	O
under	O
certain	O
conditions	O
as	O
the	O
sample	O
size	O
tends	O
to	O
infinity	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	B
becomes	O
gaussian	B
informally	O
the	O
requirement	O
for	O
this	O
result	O
to	O
hold	O
is	O
that	O
each	O
parameter	B
in	O
the	O
model	O
gets	O
to	O
see	O
an	O
infinite	O
amount	O
of	O
data	O
and	O
that	O
the	O
model	O
be	O
identifiable	O
unfortunately	O
this	O
excludes	O
many	O
of	O
the	O
models	O
of	O
interest	O
to	O
machine	B
learning	B
nevertheless	O
let	O
us	O
assume	O
we	O
are	O
in	O
a	O
simple	O
setting	O
where	O
the	O
theorem	O
holds	O
the	O
center	O
of	O
the	O
gaussian	B
will	O
be	O
the	O
mle	B
but	O
what	O
about	O
the	O
variance	B
of	O
this	O
gaussian	B
intuitively	O
the	O
variance	B
of	O
the	O
estimator	B
will	O
be	O
related	O
to	O
the	O
amount	O
of	O
curvature	O
of	O
the	O
likelihood	B
surface	O
at	O
its	O
peak	O
if	O
the	O
curvature	O
is	O
large	O
the	O
peak	O
will	O
be	O
sharp	O
and	O
the	O
variance	B
low	O
in	O
this	O
case	O
the	O
estimate	O
is	O
well	O
determined	O
by	O
contrast	O
if	O
the	O
curvature	O
is	O
small	O
the	O
peak	O
will	O
be	O
nearly	O
flat	O
so	O
the	O
variance	B
is	O
high	O
likelihood	B
evaluated	O
at	O
some	O
point	O
let	O
us	O
now	O
formalize	O
this	O
intuition	O
define	O
the	O
score	B
function	I
as	O
the	O
gradient	O
of	O
the	O
log	O
s	O
log	O
pd	O
define	O
the	O
observed	B
information	B
matrix	I
as	O
the	O
gradient	O
of	O
the	O
negative	O
score	B
function	I
or	O
equivalently	O
the	O
hessian	B
of	O
the	O
nll	B
j	O
s	O
log	O
pd	O
in	O
this	O
becomes	O
j	O
d	O
d	O
log	O
pd	O
this	O
is	O
just	O
a	O
measure	O
of	O
curvature	O
of	O
the	O
log-likelihood	O
function	O
at	O
since	O
we	O
are	O
studying	O
the	O
sampling	B
distribution	I
d	O
xn	O
is	O
a	O
set	O
of	O
random	O
variables	O
the	O
fisher	B
information	B
matrix	I
is	O
defined	O
to	O
be	O
the	O
expected	B
value	I
of	O
the	O
observed	B
information	B
e	O
in	O
j	O
this	O
is	O
not	O
the	O
usual	O
definition	O
but	O
is	O
equivalent	O
to	O
it	O
under	O
standard	O
assumptions	O
more	O
precisely	O
the	O
standard	O
definition	O
is	O
as	O
follows	O
just	O
give	O
the	O
scalar	O
case	O
to	O
simplify	O
notation	O
i	O
that	O
is	O
the	O
variance	B
of	O
the	O
score	B
function	I
if	O
is	O
the	O
mle	B
it	O
is	O
easy	O
to	O
see	O
that	O
e	O
d	O
log	O
px	O
var	B
d	O
d	O
log	O
px	O
d	O
chapter	O
frequentist	B
statistics	I
f	O
often	O
n	O
where	O
e	O
is	O
the	O
expected	B
value	I
of	O
the	O
function	O
f	O
when	O
applied	O
to	O
data	O
sampled	O
from	O
representing	O
the	O
true	O
parameter	B
that	O
generated	O
the	O
data	O
is	O
assumed	O
known	O
so	O
we	O
just	O
write	O
in	O
in	O
for	O
short	O
furthermore	O
it	O
is	O
easy	O
to	O
see	O
that	O
in	O
because	O
the	O
log-likelihood	O
for	O
a	O
sample	O
of	O
size	O
n	O
is	O
just	O
n	O
times	O
steeper	O
than	O
the	O
log-likelihood	O
for	O
a	O
sample	O
of	O
size	O
so	O
we	O
can	O
drop	O
the	O
subscript	O
and	O
just	O
write	O
i	O
this	O
is	O
the	O
notation	O
that	O
is	O
usually	O
used	O
now	O
let	O
mled	O
be	O
the	O
mle	B
where	O
d	O
n	O
in	O
it	O
can	O
be	O
shown	O
that	O
as	O
n	O
e	O
g	O
for	O
a	O
proof	O
we	O
say	O
that	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	B
is	O
asymptotically	B
normal	B
in	O
the	O
mle	B
unfortunately	O
distribution	O
however	O
we	O
can	O
approximate	O
the	O
sampling	B
distribution	I
by	O
replacing	O
consequently	O
the	O
approximate	O
standard	B
errors	I
of	O
k	O
are	O
given	O
by	O
what	O
about	O
the	O
variance	B
of	O
the	O
mle	B
which	O
can	O
be	O
used	O
as	O
some	O
measure	O
of	O
confidence	O
is	O
unknown	B
so	O
we	O
can	O
t	O
evaluate	O
the	O
variance	B
of	O
the	O
sampling	O
with	O
sek	O
in	O
kk	O
for	O
example	O
from	O
equation	O
we	O
know	O
that	O
the	O
fisher	B
information	B
for	O
a	O
binomial	B
sampling	O
model	O
is	O
i	O
so	O
the	O
approximate	O
standard	B
error	I
of	O
the	O
mle	B
is	O
se	O
n	O
i	O
n	O
in	O
where	O
n	O
under	O
a	O
uniform	O
prior	O
i	O
xi	O
compare	O
this	O
to	O
equation	O
which	O
is	O
the	O
posterior	O
standard	B
deviation	I
frequentist	B
decision	B
theory	O
in	O
frequentist	B
or	O
classical	B
decision	B
theory	O
there	O
is	O
a	O
loss	B
function	I
and	O
a	O
likelihood	B
but	O
there	O
is	O
no	O
prior	O
and	O
hence	O
no	O
posterior	O
or	O
posterior	B
expected	I
loss	B
thus	O
there	O
is	O
no	O
automatic	O
way	O
of	O
deriving	O
an	O
optimal	O
estimator	B
unlike	O
the	O
bayesian	B
case	O
instead	O
in	O
the	O
frequentist	B
approach	O
we	O
are	O
free	O
to	O
choose	O
any	O
estimator	B
or	O
decision	B
procedure	I
x	O
a	O
we	O
the	O
gradient	O
must	O
be	O
zero	O
at	O
a	O
maximum	O
so	O
the	O
variance	B
reduces	O
to	O
the	O
expected	O
square	O
of	O
the	O
score	B
function	I
d	O
log	O
px	O
i	O
d	O
d	O
log	O
px	O
e	O
is	O
a	O
much	O
more	O
intuitive	O
quantity	O
than	O
the	O
variance	B
of	O
the	O
score	O
in	O
practice	O
the	O
frequentist	B
approach	O
is	O
usually	O
only	O
applied	O
to	O
one-shot	O
statistical	O
decision	B
problems	O
such	O
as	O
classification	O
regression	B
and	O
parameter	B
estimation	O
since	O
its	O
non-constructive	O
nature	O
makes	O
it	O
difficult	O
to	O
apply	O
to	O
sequential	B
decision	B
problems	O
which	O
adapt	O
to	O
data	O
online	O
so	O
now	O
the	O
fisher	B
information	B
reduces	O
to	O
the	O
expected	O
second	O
derivative	O
of	O
the	O
nll	B
which	O
d	O
log	O
px	O
d	O
that	O
e	O
it	O
can	O
be	O
shown	O
frequentist	B
decision	B
theory	O
having	O
chosen	O
an	O
estimator	B
we	O
define	O
its	O
expected	O
loss	B
or	O
risk	B
as	O
follows	O
r	O
ep	B
d	O
l	O
dp	O
d	O
l	O
d	O
d	O
where	O
d	O
is	O
data	O
sampled	O
from	O
nature	O
s	O
distribution	O
which	O
is	O
represented	O
by	O
parameter	B
in	O
other	O
words	O
the	O
expectation	O
is	O
wrt	O
the	O
sampling	B
distribution	I
of	O
the	O
estimator	B
compare	O
this	O
to	O
the	O
bayesian	B
posterior	B
expected	I
loss	B
ep	B
a	O
we	O
see	O
that	O
the	O
bayesian	B
approach	O
averages	O
over	O
is	O
unknown	B
and	O
conditions	O
on	O
d	O
is	O
known	O
whereas	O
the	O
frequentist	B
approach	O
averages	O
over	O
d	O
ignoring	O
the	O
observed	O
data	O
and	O
conditions	O
on	O
not	O
only	O
is	O
the	O
frequentist	B
definition	O
unnatural	O
it	O
cannot	O
even	O
be	O
computed	O
because	O
is	O
unknown	B
consequently	O
we	O
cannot	O
compare	O
different	O
estimators	O
in	O
terms	O
of	O
their	O
frequentist	B
risk	B
we	O
discuss	O
various	O
solutions	O
to	O
this	O
below	O
l	O
ap	O
is	O
unknown	B
bayes	B
risk	B
how	O
do	O
we	O
choose	O
amongst	O
estimators	O
we	O
need	O
some	O
way	O
to	O
convert	O
r	O
into	O
a	O
single	O
measure	O
of	O
quality	O
r	O
which	O
does	O
not	O
depend	O
on	O
knowing	O
one	O
approach	O
is	O
to	O
put	O
a	O
prior	O
on	O
and	O
then	O
to	O
define	O
bayes	B
risk	B
or	O
integrated	B
risk	B
of	O
an	O
estimator	B
as	O
follows	O
rb	O
ep	B
r	O
a	O
bayes	B
estimator	B
or	O
bayes	B
decision	B
rule	I
is	O
one	O
which	O
minimizes	O
the	O
expected	O
risk	B
b	O
argmin	O
rb	O
note	O
that	O
the	O
integrated	B
risk	B
is	O
also	O
called	O
the	O
preposterior	B
risk	B
since	O
it	O
is	O
before	O
we	O
have	O
seen	O
the	O
data	O
minimizing	O
this	O
can	O
be	O
useful	O
for	O
experiment	O
design	O
we	O
will	O
now	O
prove	O
a	O
very	O
important	O
theorem	O
that	O
connects	O
the	O
bayesian	B
and	O
frequentist	B
approaches	O
to	O
decision	B
theory	O
theorem	O
a	O
bayes	B
estimator	B
can	O
be	O
obtained	O
by	O
minimizing	O
the	O
posterior	B
expected	I
loss	B
for	O
each	O
x	O
proof	O
by	O
switching	O
the	O
order	O
of	O
integration	O
we	O
have	O
y	O
x	O
x	O
y	O
y	O
x	O
x	O
rb	O
px	O
ly	O
y	O
p	O
ly	O
y	O
ly	O
px	O
chapter	O
frequentist	B
statistics	I
r	O
r	O
r	O
figure	O
risk	B
functions	O
for	O
two	O
decision	B
procedures	O
and	O
since	O
has	O
lower	O
worst	O
case	O
risk	B
it	O
is	O
the	O
minimax	O
estimator	B
even	O
though	O
has	O
lower	O
risk	B
for	O
most	O
values	O
of	O
thus	O
minimax	O
estimators	O
are	O
overly	O
conservative	O
to	O
minimize	O
the	O
overall	O
expectation	O
we	O
just	O
minimize	O
the	O
term	O
inside	O
for	O
each	O
x	O
so	O
our	O
decision	B
rule	I
is	O
to	O
pick	O
bx	O
argmin	O
a	O
a	O
hence	O
we	O
see	O
that	O
the	O
picking	O
the	O
optimal	B
action	B
on	O
a	O
case-by-case	O
basis	O
in	O
the	O
bayesian	B
approach	O
is	O
optimal	O
on	O
average	O
in	O
the	O
frequentist	B
approach	O
in	O
other	O
words	O
the	O
bayesian	B
approach	O
provides	O
a	O
good	O
way	O
of	O
achieving	O
frequentist	B
goals	O
in	O
fact	O
one	O
can	O
go	O
further	O
and	O
prove	O
the	O
following	O
theorem	O
every	O
admissable	O
decision	B
rule	I
is	O
a	O
bayes	B
decision	B
rule	I
with	O
respect	O
to	O
some	O
possibly	O
improper	B
prior	I
distribution	O
this	O
theorem	O
shows	O
that	O
the	O
best	O
way	O
to	O
minimize	O
frequentist	B
risk	B
is	O
to	O
be	O
bayesian	B
see	O
and	O
smith	O
for	O
further	O
discussion	O
of	O
this	O
point	O
minimax	O
risk	B
obviously	O
some	O
frequentists	O
dislike	O
using	O
bayes	B
risk	B
since	O
it	O
requires	O
the	O
choice	O
of	O
a	O
prior	O
this	O
is	O
only	O
in	O
the	O
evaluation	O
of	O
the	O
estimator	B
not	O
necessarily	O
as	O
part	O
of	O
its	O
construction	O
an	O
alternative	O
approach	O
is	O
as	O
follows	O
define	O
the	O
maximum	B
risk	B
of	O
an	O
estimator	B
as	O
rmax	O
max	O
r	O
a	O
minimax	B
rule	I
is	O
one	O
which	O
minimizes	O
the	O
maximum	B
risk	B
mm	B
argmin	O
rmax	O
frequentist	B
decision	B
theory	O
for	O
example	O
in	O
figure	O
we	O
see	O
that	O
has	O
lower	O
worst-case	O
risk	B
than	O
ranging	O
over	O
all	O
possible	O
values	O
of	O
so	O
it	O
is	O
the	O
minimax	O
estimator	B
section	O
for	O
an	O
explanation	O
of	O
how	O
to	O
compute	O
a	O
risk	B
function	O
for	O
an	O
actual	O
model	O
minimax	O
estimators	O
have	O
a	O
certain	O
appeal	O
however	O
computing	O
them	O
can	O
be	O
hard	O
and	O
furthermore	O
they	O
are	O
very	O
pessimistic	O
in	O
fact	O
one	O
can	O
show	O
that	O
all	O
minimax	O
estimators	O
are	O
equivalent	O
to	O
bayes	O
estimators	O
under	O
a	O
least	B
favorable	I
prior	I
in	O
most	O
statistical	O
situations	O
game	O
theoretic	O
ones	O
assuming	O
nature	O
is	O
an	O
adversary	O
is	O
not	O
a	O
reasonable	O
assumption	O
admissible	B
estimators	O
the	O
basic	O
problem	O
with	O
frequentist	B
decision	B
theory	O
is	O
that	O
it	O
relies	O
on	O
knowing	O
the	O
true	O
distribution	O
p	O
in	O
order	O
to	O
evaluate	O
the	O
risk	B
however	O
it	O
might	O
be	O
the	O
case	O
that	O
some	O
estimators	O
in	O
particular	O
if	O
r	O
r	O
for	O
all	O
are	O
worse	O
than	O
others	O
regardless	O
of	O
the	O
value	O
of	O
then	O
we	O
say	O
that	O
dominates	B
the	O
domination	O
is	O
said	O
to	O
be	O
strict	B
if	O
the	O
inequality	O
is	O
strict	B
for	O
some	O
an	O
estimator	B
is	O
said	O
to	O
be	O
admissible	B
if	O
it	O
is	O
not	O
strictly	O
dominated	O
by	O
any	O
other	O
estimator	B
example	O
let	O
us	O
give	O
an	O
example	O
based	O
on	O
and	O
smith	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
mean	B
of	O
a	O
gaussian	B
we	O
assume	O
the	O
data	O
is	O
sampled	O
from	O
xi	O
n	O
and	O
use	O
quadratic	B
loss	B
l	O
the	O
corresponding	O
risk	B
function	O
is	O
the	O
mse	B
some	O
possible	O
decision	B
rules	B
or	O
estimators	O
are	O
as	O
follows	O
x	O
the	O
sample	O
mean	B
x	O
the	O
sample	O
median	B
a	O
fixed	O
value	O
the	O
posterior	B
mean	B
under	O
a	O
n	O
prior	O
n	O
n	O
x	O
n	O
wx	O
w	O
for	O
we	O
consider	O
a	O
weak	O
prior	O
and	O
a	O
stronger	O
prior	O
the	O
prior	O
mean	B
is	O
some	O
fixed	O
value	O
we	O
assume	O
is	O
known	O
is	O
the	O
same	O
as	O
with	O
an	O
infinitely	O
strong	O
prior	O
we	O
know	O
the	O
true	O
parameter	B
into	O
squared	O
bias	B
plus	O
variance	B
can	O
do	O
this	O
since	O
in	O
this	O
toy	O
example	O
in	O
section	O
we	O
show	O
that	O
the	O
mse	B
can	O
be	O
decomposed	O
let	O
us	O
now	O
derive	O
the	O
risk	B
functions	O
analytically	O
m	O
se	O
var	B
the	O
sample	O
mean	B
is	O
unbiased	B
so	O
its	O
risk	B
is	O
m	O
se	O
var	B
n	O
chapter	O
frequentist	B
statistics	I
risk	B
functions	O
for	O
mle	B
median	B
fixed	O
risk	B
functions	O
for	O
mle	B
median	B
fixed	O
r	O
r	O
figure	O
risk	B
functions	O
for	O
estimating	O
the	O
mean	B
of	O
a	O
gaussian	B
using	O
data	O
sampled	O
n	O
the	O
solid	O
dark	O
blue	O
horizontal	O
line	O
is	O
the	O
mle	B
the	O
solid	O
light	O
blue	O
curved	O
line	O
is	O
the	O
posterior	B
mean	B
when	O
left	O
n	O
samples	B
right	O
n	O
samples	B
based	O
on	O
figure	O
of	O
and	O
smith	O
figure	O
generated	O
by	O
riskfngauss	O
the	O
sample	O
median	B
is	O
also	O
unbiased	B
one	O
can	O
show	O
that	O
the	O
variance	B
is	O
approximately	O
so	O
m	O
se	O
for	O
the	O
variance	B
is	O
zero	O
so	O
m	O
se	O
finally	O
for	O
the	O
posterior	B
mean	B
we	O
have	O
m	O
se	O
w	O
w	O
n	O
e	O
n	O
these	O
functions	O
are	O
plotted	O
in	O
figure	O
for	O
n	O
we	O
see	O
that	O
in	O
general	O
the	O
best	O
estimator	B
depends	O
on	O
the	O
value	O
of	O
is	O
very	O
close	O
to	O
then	O
which	O
is	O
unknown	B
if	O
is	O
within	O
some	O
reasonable	O
range	O
around	O
then	O
the	O
just	O
predicts	O
is	O
best	O
posterior	B
mean	B
which	O
combines	O
the	O
prior	O
guess	O
of	O
with	O
the	O
actual	O
data	O
is	O
best	O
if	O
is	O
far	O
from	O
the	O
mle	B
is	O
best	O
none	O
of	O
this	O
should	O
be	O
suprising	O
a	O
small	O
amount	O
of	O
shrinkage	B
the	O
posterior	B
mean	B
with	O
a	O
weak	O
prior	O
is	O
usually	O
desirable	O
assuming	O
our	O
prior	O
mean	B
is	O
sensible	O
what	O
is	O
more	O
surprising	O
is	O
that	O
the	O
risk	B
of	O
decision	B
rule	I
median	B
is	O
always	O
higher	O
than	O
that	O
of	O
mean	B
for	O
every	O
value	O
of	O
consequently	O
the	O
sample	O
median	B
is	O
an	O
if	O
frequentist	B
decision	B
theory	O
inadmissible	O
estimator	B
for	O
this	O
particular	O
problem	O
the	O
data	O
is	O
assumed	O
to	O
come	O
from	O
a	O
gaussian	B
in	O
practice	O
the	O
sample	O
median	B
is	O
often	O
better	O
than	O
the	O
sample	O
mean	B
because	O
it	O
is	O
more	O
robust	B
to	O
outliers	B
one	O
can	O
show	O
that	O
the	O
median	B
is	O
the	O
bayes	B
estimator	B
squared	B
loss	B
if	O
we	O
assume	O
the	O
data	O
comes	O
from	O
a	O
laplace	B
distribution	I
which	O
has	O
heavier	O
tails	O
than	O
a	O
gaussian	B
more	O
generally	O
we	O
can	O
construct	O
robust	B
estimators	O
by	O
using	O
flexible	O
models	O
of	O
our	O
data	O
such	O
as	O
mixture	B
models	O
or	O
non-parametric	O
density	O
estimators	O
and	O
then	O
computing	O
the	O
posterior	B
mean	B
or	O
median	B
stein	O
s	O
paradox	O
suppose	O
we	O
have	O
n	O
iid	B
random	O
variables	O
xi	O
n	O
i	O
and	O
we	O
want	O
to	O
estimate	O
the	O
i	O
the	O
obvious	O
estimator	B
is	O
the	O
mle	B
which	O
in	O
this	O
case	O
sets	O
i	O
xi	O
it	O
turns	O
out	O
that	O
this	O
is	O
an	O
inadmissible	O
estimator	B
under	O
quadratic	B
loss	B
when	O
n	O
to	O
show	O
this	O
it	O
suffices	O
to	O
construct	O
an	O
estimator	B
that	O
is	O
better	O
the	O
james-stein	B
estimator	B
is	O
one	O
such	O
estimator	B
and	O
is	O
defined	O
as	O
follows	O
i	O
bx	O
bxi	O
x	O
bxi	O
x	O
xi	O
and	O
b	O
is	O
some	O
tuning	O
constant	O
this	O
estimate	O
shrinks	O
the	O
derive	O
this	O
estimator	B
using	O
an	O
empirical	B
bayes	I
approach	O
in	O
where	O
x	O
n	O
i	O
towards	O
the	O
overall	O
mean	B
section	O
it	O
can	O
be	O
shown	O
that	O
this	O
shrinkage	B
estimator	B
has	O
lower	O
frequentist	B
risk	B
than	O
the	O
mle	B
mean	B
for	O
n	O
this	O
is	O
known	O
as	O
stein	O
s	O
paradox	O
the	O
reason	O
it	O
is	O
called	O
a	O
paradox	O
is	O
illustrated	O
by	O
the	O
following	O
example	O
suppose	O
i	O
is	O
the	O
true	O
iq	O
of	O
student	O
i	O
and	O
xi	O
is	O
his	O
test	O
score	O
why	O
should	O
my	O
estimate	O
of	O
i	O
depend	O
on	O
the	O
global	O
mean	B
x	O
and	O
hence	O
on	O
some	O
other	O
student	O
s	O
scores	B
one	O
can	O
create	O
even	O
more	O
paradoxical	O
examples	O
by	O
making	O
the	O
different	O
dimensions	O
be	O
qualitatively	O
different	O
e	O
g	O
is	O
my	O
iq	O
is	O
the	O
average	O
rainfall	O
in	O
vancouver	O
etc	O
the	O
solution	O
to	O
the	O
paradox	O
is	O
the	O
following	O
if	O
your	O
goal	O
is	O
to	O
estimate	O
just	O
i	O
you	O
cannot	O
do	O
better	O
than	O
using	O
xi	O
but	O
if	O
the	O
goal	O
is	O
to	O
estimate	O
the	O
whole	O
vector	O
and	O
you	O
use	O
squared	B
error	I
as	O
your	O
loss	B
function	I
then	O
shrinkage	B
helps	O
to	O
see	O
this	O
suppose	O
we	O
want	O
to	O
estimate	O
from	O
a	O
single	O
sample	O
x	O
n	O
i	O
a	O
simple	O
estimate	O
is	O
but	O
this	O
will	O
overestimate	O
the	O
result	O
since	O
e	O
e	O
i	O
i	O
n	O
i	O
consequently	O
we	O
can	O
reduce	O
our	O
risk	B
by	O
pooling	O
information	B
even	O
from	O
unrelated	O
sources	O
and	O
shrinking	O
towards	O
the	O
overall	O
mean	B
in	O
section	O
we	O
give	O
a	O
bayesian	B
explanation	O
for	O
this	O
see	O
also	O
and	O
morris	O
admissibility	O
is	O
not	O
enough	O
it	O
seems	O
clear	O
that	O
we	O
can	O
restrict	O
our	O
search	O
for	O
good	O
estimators	O
to	O
the	O
class	O
of	O
admissible	B
estimators	O
but	O
in	O
fact	O
it	O
is	O
easy	O
to	O
construct	O
admissible	B
estimators	O
as	O
we	O
show	O
in	O
the	O
following	O
example	O
chapter	O
frequentist	B
statistics	I
then	O
r	O
and	O
r	O
theorem	O
let	O
x	O
n	O
and	O
consider	O
estimating	O
under	O
squared	B
loss	B
let	O
a	O
constant	O
independent	O
of	O
the	O
data	O
this	O
is	O
an	O
admissible	B
estimator	B
proof	O
suppose	O
not	O
then	O
there	O
is	O
some	O
other	O
estimator	B
with	O
smaller	O
risk	B
so	O
r	O
r	O
where	O
the	O
inequality	O
must	O
be	O
strict	B
for	O
some	O
suppose	O
the	O
true	O
parameter	B
is	O
since	O
r	O
r	O
for	O
all	O
and	O
r	O
we	O
have	O
r	O
and	O
hence	O
thus	O
the	O
only	O
way	O
can	O
avoid	O
having	O
higher	O
risk	B
than	O
at	O
some	O
specific	O
point	O
is	O
by	O
being	O
equal	O
to	O
hence	O
there	O
is	O
no	O
other	O
estimator	B
with	O
strictly	O
lower	O
risk	B
so	O
is	O
admissible	B
desirable	O
properties	O
of	O
estimators	O
since	O
frequentist	B
decision	B
theory	O
does	O
not	O
provide	O
an	O
automatic	O
way	O
to	O
choose	O
the	O
best	O
estimator	B
we	O
need	O
to	O
come	O
up	O
with	O
other	O
heuristics	B
for	O
choosing	O
amongst	O
them	O
in	O
this	O
section	O
we	O
discuss	O
some	O
properties	O
we	O
would	O
like	O
estimators	O
to	O
have	O
unfortunately	O
we	O
will	O
see	O
that	O
we	O
cannot	O
achieve	O
all	O
of	O
these	O
properties	O
at	O
the	O
same	O
time	O
consistent	B
estimators	I
an	O
estimator	B
is	O
said	O
to	O
be	O
consistent	B
if	O
it	O
eventually	O
recovers	O
the	O
true	O
parameters	O
that	O
generated	O
as	O
the	O
arrow	O
the	O
data	O
as	O
the	O
sample	O
size	O
goes	O
to	O
infinity	O
i	O
e	O
denotes	O
convergence	O
in	O
probability	O
of	O
course	O
this	O
concept	B
only	O
makes	O
sense	O
if	O
the	O
data	O
actually	O
comes	O
from	O
the	O
specified	O
model	O
with	O
parameters	O
which	O
is	O
not	O
usually	O
the	O
case	O
with	O
real	O
data	O
nevertheless	O
it	O
can	O
be	O
a	O
useful	O
theoretical	O
property	O
p	O
it	O
can	O
be	O
shown	O
that	O
the	O
mle	B
is	O
a	O
consistent	B
estimator	B
the	O
intuitive	O
reason	O
is	O
that	O
maxi	O
is	O
the	O
true	O
where	O
p	O
mizing	O
likelihood	B
is	O
equivalent	O
to	O
minimizing	O
kl	O
distribution	O
and	O
p	O
is	O
our	O
estimate	O
we	O
can	O
achieve	O
kl	B
divergence	I
iff	B
unbiased	B
estimators	O
the	O
bias	B
of	O
an	O
estimator	B
is	O
defined	O
as	O
bias	B
epd	O
where	O
is	O
the	O
true	O
parameter	B
value	O
if	O
the	O
bias	B
is	O
zero	O
the	O
estimator	B
is	O
called	O
unbiased	B
this	O
means	O
the	O
sampling	B
distribution	I
is	O
centered	O
on	O
the	O
true	O
parameter	B
for	O
example	O
the	O
mle	B
for	O
a	O
gaussian	B
mean	B
is	O
unbiased	B
bias	B
e	O
e	O
n	O
xi	O
n	O
n	O
if	O
the	O
model	O
is	O
unidentifiable	O
the	O
mle	B
may	O
select	O
a	O
set	O
of	O
parameters	O
that	O
is	O
different	O
from	O
the	O
true	O
parameters	O
but	O
for	O
which	O
the	O
induced	O
distribution	O
p	O
is	O
the	O
same	O
as	O
the	O
exact	O
distribution	O
such	O
parameters	O
are	O
said	O
to	O
be	O
likelihood	B
equivalent	I
desirable	O
properties	O
of	O
estimators	O
however	O
the	O
mle	B
for	O
a	O
gaussian	B
variance	B
is	O
not	O
an	O
unbiased	B
estimator	B
of	O
in	O
fact	O
one	O
can	O
show	O
that	O
e	O
n	O
n	O
however	O
the	O
following	O
estimator	B
n	O
n	O
n	O
n	O
is	O
an	O
unbiased	B
estimator	B
which	O
we	O
can	O
easily	O
prove	O
as	O
follows	O
e	O
n	O
e	O
n	O
n	O
n	O
n	O
n	O
n	O
in	O
matlab	O
varx	O
returns	O
n	O
the	O
difference	O
will	O
be	O
negligible	O
n	O
whereas	O
returns	O
mle	B
for	O
large	O
enough	O
although	O
the	O
mle	B
may	O
sometimes	O
be	O
a	O
biased	O
estimator	B
one	O
can	O
show	O
that	O
asymptotically	O
it	O
is	O
always	O
unbiased	B
is	O
necessary	O
for	O
the	O
mle	B
to	O
be	O
a	O
consistent	B
estimator	B
although	O
being	O
unbiased	B
sounds	O
like	O
a	O
desirable	O
property	O
this	O
is	O
not	O
always	O
true	O
see	O
sec	O
tion	O
and	O
for	O
discussion	O
of	O
this	O
point	O
minimum	O
variance	B
estimators	O
it	O
seems	O
intuitively	O
reasonable	O
that	O
we	O
want	O
our	O
estimator	B
to	O
be	O
unbiased	B
we	O
shall	O
give	O
some	O
arguments	O
against	O
this	O
claim	O
below	O
however	O
being	O
unbiased	B
is	O
not	O
enough	O
for	O
example	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
mean	B
of	O
a	O
gaussian	B
from	O
d	O
xn	O
the	O
estimator	B
that	O
just	O
looks	O
at	O
the	O
first	O
data	O
point	O
is	O
an	O
unbiased	B
estimator	B
but	O
will	O
generally	O
be	O
further	O
from	O
than	O
the	O
empirical	O
mean	B
x	O
is	O
also	O
unbiased	B
so	O
the	O
variance	B
of	O
an	O
estimator	B
is	O
also	O
important	O
a	O
natural	O
question	O
is	O
how	O
long	O
can	O
the	O
variance	B
go	O
a	O
famous	O
result	O
called	O
the	O
cramerrao	O
lower	O
bound	O
provides	O
a	O
lower	O
bound	O
on	O
the	O
variance	B
of	O
any	O
unbiased	B
estimator	B
more	O
precisely	O
theorem	O
inequality	O
let	O
xn	O
px	O
and	O
xn	O
be	O
an	O
unbiased	B
estimator	B
of	O
then	O
under	O
various	O
smoothness	O
assumptions	O
on	O
px	O
we	O
have	O
var	B
ni	O
where	O
i	O
is	O
the	O
fisher	B
information	B
matrix	I
section	O
a	O
proof	O
can	O
be	O
found	O
e	O
g	O
in	O
it	O
can	O
be	O
shown	O
that	O
the	O
mle	B
achieves	O
the	O
cramer	O
rao	O
lower	O
bound	O
and	O
hence	O
has	O
the	O
smallest	O
asymptotic	O
variance	B
of	O
any	O
unbiased	B
estimator	B
thus	O
mle	B
is	O
said	O
to	O
be	O
asymptotically	B
optimal	I
chapter	O
frequentist	B
statistics	I
the	O
bias-variance	B
tradeoff	I
e	O
although	O
using	O
an	O
unbiased	B
estimator	B
seems	O
like	O
a	O
good	O
idea	O
this	O
is	O
not	O
always	O
the	O
case	O
to	O
see	O
why	O
suppose	O
we	O
use	O
quadratic	B
loss	B
as	O
we	O
showed	O
above	O
the	O
corresponding	O
risk	B
is	O
the	O
mse	B
we	O
now	O
derive	O
a	O
very	O
useful	O
decomposition	O
of	O
the	O
mse	B
expectations	O
and	O
variances	O
are	O
wrt	O
the	O
true	O
distribution	O
pd	O
but	O
we	O
drop	O
the	O
explicit	O
conditioning	B
for	O
notational	O
brevity	O
let	O
denote	O
the	O
estimate	O
and	O
e	O
denote	O
the	O
expected	B
value	I
of	O
the	O
estimate	O
we	O
vary	O
d	O
then	O
we	O
have	O
var	B
e	O
e	O
e	O
in	O
words	O
mse	B
variance	B
this	O
is	O
called	O
the	O
bias-variance	B
tradeoff	I
e	O
g	O
et	O
al	O
what	O
it	O
means	O
is	O
that	O
it	O
might	O
be	O
wise	O
to	O
use	O
a	O
biased	O
estimator	B
so	O
long	O
as	O
it	O
reduces	O
our	O
variance	B
assuming	O
our	O
goal	O
is	O
to	O
minimize	O
squared	B
error	I
example	O
estimating	O
a	O
gaussian	B
mean	B
let	O
us	O
give	O
an	O
example	O
based	O
on	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
mean	B
of	O
a	O
gaussian	B
from	O
x	O
xn	O
we	O
assume	O
the	O
data	O
is	O
sampled	O
from	O
xi	O
n	O
an	O
obvious	O
estimate	O
is	O
the	O
mle	B
this	O
has	O
a	O
bias	B
of	O
and	O
a	O
variance	B
of	O
var	B
n	O
but	O
we	O
could	O
also	O
use	O
a	O
map	B
estimate	I
in	O
section	O
we	O
show	O
that	O
the	O
map	B
estimate	I
under	O
a	O
gaussian	B
prior	O
of	O
the	O
form	O
n	O
is	O
given	O
by	O
x	O
n	O
wx	O
w	O
x	O
n	O
n	O
where	O
w	O
controls	O
how	O
much	O
we	O
trust	O
the	O
mle	B
compared	O
to	O
our	O
prior	O
is	O
also	O
the	O
posterior	B
mean	B
since	O
the	O
mean	B
and	O
mode	B
of	O
a	O
gaussian	B
are	O
the	O
same	O
the	O
bias	B
and	O
variance	B
are	O
given	O
by	O
e	O
x	O
w	O
w	O
w	O
var	B
x	O
n	O
desirable	O
properties	O
of	O
estimators	O
sampling	B
distribution	I
truth	O
prior	O
n	O
mse	B
of	O
postmean	O
mse	B
of	O
mle	B
sample	O
size	O
e	O
s	O
m	O
e	O
v	O
i	O
t	O
l	O
a	O
e	O
r	O
left	O
sampling	B
distribution	I
of	O
the	O
map	B
estimate	I
with	O
different	O
prior	O
strengths	O
figure	O
mle	B
corresponds	O
to	O
right	O
mse	B
relative	O
to	O
that	O
of	O
the	O
mle	B
versus	O
sample	O
size	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
samplingdistgaussshrinkage	O
let	O
us	O
assume	O
that	O
our	O
prior	O
is	O
slightly	O
misspecified	O
so	O
we	O
use	O
whereas	O
the	O
truth	O
is	O
in	O
figure	O
we	O
see	O
that	O
the	O
sampling	B
distribution	I
of	O
the	O
map	B
estimate	I
for	O
so	O
although	O
the	O
map	B
estimate	I
is	O
biased	O
w	O
it	O
has	O
lower	O
variance	B
is	O
biased	O
away	O
from	O
the	O
truth	O
but	O
has	O
lower	O
variance	B
narrower	O
than	O
that	O
of	O
the	O
mle	B
in	O
figure	O
we	O
plot	O
mse	B
xmsex	O
vs	O
n	O
we	O
see	O
that	O
the	O
map	B
estimate	I
has	O
lower	O
mse	B
than	O
the	O
mle	B
especially	O
for	O
small	O
sample	O
size	O
for	O
the	O
case	O
corresponds	O
to	O
the	O
mle	B
and	O
the	O
case	O
corresponds	O
to	O
a	O
strong	O
prior	O
which	O
hurts	O
performance	O
because	O
the	O
prior	O
mean	B
is	O
wrong	O
it	O
is	O
clearly	O
important	O
to	O
tune	O
the	O
strength	O
of	O
the	O
prior	O
a	O
topic	B
we	O
discuss	O
later	O
example	O
ridge	B
regression	B
another	O
important	O
example	O
of	O
the	O
bias	B
variance	B
tradeoff	O
arises	O
in	O
ridge	B
regression	B
which	O
we	O
discuss	O
in	O
section	O
in	O
brief	O
this	O
corresponds	O
to	O
map	O
estimation	O
for	O
linear	B
regression	B
under	O
a	O
gaussian	B
prior	O
pw	O
n	O
the	O
zero-mean	O
prior	O
encourages	O
the	O
weights	O
to	O
be	O
small	O
which	O
reduces	O
overfitting	O
the	O
precision	B
term	O
controls	O
the	O
strength	O
of	O
this	O
prior	O
setting	O
results	O
in	O
the	O
mle	B
using	O
results	O
in	O
a	O
biased	O
estimate	O
to	O
illustrate	O
the	O
effect	O
on	O
the	O
variance	B
consider	O
a	O
simple	O
example	O
figure	O
on	O
the	O
left	O
plots	O
each	O
individual	O
fitted	O
curve	O
and	O
on	O
the	O
right	O
plots	O
the	O
average	O
fitted	O
curve	O
we	O
see	O
that	O
as	O
we	O
increase	O
the	O
strength	O
of	O
the	O
regularizer	O
the	O
variance	B
decreases	O
but	O
the	O
bias	B
increases	O
bias-variance	B
tradeoff	I
for	O
classification	O
if	O
we	O
use	O
loss	B
instead	O
of	O
squared	B
error	I
the	O
above	O
analysis	O
breaks	O
down	O
since	O
the	O
frequentist	B
risk	B
is	O
no	O
longer	O
expressible	O
as	O
squared	O
bias	B
plus	O
variance	B
in	O
fact	O
one	O
can	O
show	O
of	O
et	O
al	O
that	O
the	O
bias	B
and	O
variance	B
combine	O
multiplicatively	O
if	O
the	O
estimate	O
is	O
on	O
chapter	O
frequentist	B
statistics	I
ln	O
ln	O
ln	O
ln	O
figure	O
illustration	O
of	O
bias-variance	B
tradeoff	I
for	O
ridge	B
regression	B
we	O
generate	O
data	O
sets	O
from	O
the	O
true	O
function	O
shown	O
in	O
solid	O
green	O
left	O
we	O
plot	O
the	O
regularized	O
fit	O
for	O
different	O
data	O
sets	O
we	O
use	O
linear	B
regression	B
with	O
a	O
gaussian	B
rbf	B
expansion	O
with	O
centers	O
evenly	O
spread	O
over	O
the	O
interval	O
right	O
we	O
plot	O
the	O
average	O
of	O
the	O
fits	O
averaged	O
over	O
all	O
datasets	O
top	O
row	O
strongly	O
regularized	O
we	O
see	O
that	O
the	O
individual	O
fits	O
are	O
similar	B
to	O
each	O
other	O
variance	B
but	O
the	O
average	O
is	O
far	O
from	O
the	O
truth	O
bias	B
bottom	O
row	O
lightly	O
regularized	O
we	O
see	O
that	O
the	O
individual	O
fits	O
are	O
quite	O
different	O
from	O
each	O
other	O
variance	B
but	O
the	O
average	O
is	O
close	O
to	O
the	O
truth	O
bias	B
based	O
on	O
figure	O
figure	O
generated	O
by	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
then	O
the	O
bias	B
is	O
negative	O
and	O
decreasing	O
the	O
variance	B
will	O
decrease	O
the	O
misclassification	O
rate	B
but	O
if	O
the	O
estimate	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
boundary	I
then	O
the	O
bias	B
is	O
positive	O
so	O
it	O
pays	O
to	O
increase	O
the	O
variance	B
this	O
little	O
known	O
fact	O
illustrates	O
that	O
the	O
bias-variance	B
tradeoff	I
is	O
not	O
very	O
useful	O
for	O
classification	O
it	O
is	O
better	O
to	O
focus	O
on	O
expected	O
loss	B
below	O
not	O
directly	O
on	O
bias	B
and	O
variance	B
we	O
can	O
approximate	O
the	O
expected	O
loss	B
using	O
cross	O
validatinon	O
as	O
we	O
discuss	O
in	O
section	O
empirical	B
risk	B
minimization	I
frequentist	B
decision	B
theory	O
suffers	O
from	O
the	O
fundamental	O
problem	O
that	O
one	O
cannot	O
actually	O
compute	O
the	O
risk	B
function	O
since	O
it	O
relies	O
on	O
knowing	O
the	O
true	O
data	O
distribution	O
contrast	O
the	O
bayesian	B
posterior	B
expected	I
loss	B
can	O
always	O
be	O
computed	O
since	O
it	O
conditions	O
on	O
the	O
the	O
data	O
rather	O
than	O
conditioning	B
on	O
however	O
there	O
is	O
one	O
setting	O
which	O
avoids	O
this	O
problem	O
and	O
that	O
is	O
where	O
the	O
task	O
is	O
to	O
predict	O
observable	O
quantities	O
as	O
opposed	O
to	O
estimating	O
hidden	B
variables	I
or	O
parameters	O
that	O
is	O
instead	O
of	O
looking	O
at	O
loss	B
functions	O
of	O
the	O
form	O
l	O
where	O
is	O
the	O
true	O
but	O
unknown	B
parameter	B
and	O
is	O
our	O
estimator	B
let	O
us	O
look	O
at	O
loss	B
empirical	B
risk	B
minimization	I
functions	O
of	O
the	O
form	O
ly	O
where	O
y	O
is	O
the	O
true	O
but	O
unknown	B
response	O
and	O
is	O
our	O
prediction	O
given	O
the	O
input	O
x	O
in	O
this	O
case	O
the	O
frequentist	B
risk	B
becomes	O
rp	O
exy	O
p	O
ly	O
y	O
x	O
y	O
where	O
p	O
represents	O
nature	O
s	O
distribution	O
of	O
course	O
this	O
distribution	O
is	O
unknown	B
but	O
a	O
simple	O
approach	O
is	O
to	O
use	O
the	O
empirical	B
distribution	I
derived	O
from	O
some	O
training	O
data	O
to	O
approximate	O
p	O
i	O
e	O
p	O
y	O
pempx	O
y	O
n	O
xi	O
yi	O
we	O
then	O
define	O
the	O
empirical	B
risk	B
as	O
follows	O
rempdd	O
rpemp	O
n	O
lyi	O
in	O
the	O
case	O
of	O
loss	B
ly	O
iy	O
this	O
becomes	O
the	O
misclassification	O
rate	B
in	O
the	O
case	O
of	O
squared	B
error	I
loss	B
ly	O
this	O
becomes	O
the	O
mean	B
squared	B
error	I
we	O
define	O
the	O
task	O
of	O
empirical	B
risk	B
minimization	I
or	O
erm	B
as	O
finding	O
a	O
decision	B
procedure	I
a	O
classification	O
rule	O
to	O
minimize	O
the	O
empirical	B
risk	B
erm	B
argmin	O
rempd	O
in	O
the	O
unsupervised	O
case	O
we	O
eliminate	O
all	O
references	O
to	O
y	O
and	O
replace	O
ly	O
with	O
lx	O
where	O
for	O
example	O
lx	O
which	O
measures	O
the	O
reconstruction	B
error	I
we	O
can	O
define	O
the	O
decision	B
rule	I
using	O
encodex	O
as	O
in	O
vector	B
quantization	I
or	O
pca	B
finally	O
we	O
define	O
the	O
empirical	B
risk	B
as	O
lxi	O
rempd	O
n	O
of	O
course	O
we	O
can	O
always	O
trivially	O
minimize	O
this	O
risk	B
by	O
setting	O
x	O
so	O
it	O
is	O
critical	O
that	O
the	O
encoder-decoder	O
go	O
via	O
some	O
kind	O
of	O
bottleneck	B
regularized	B
risk	B
minimization	I
note	O
that	O
the	O
empirical	B
risk	B
is	O
equal	O
to	O
the	O
bayes	B
risk	B
if	O
our	O
prior	O
about	O
nature	O
s	O
distribution	O
is	O
that	O
it	O
is	O
exactly	O
equal	O
to	O
the	O
empirical	B
distribution	I
e	O
pemp	O
rempd	O
therefore	O
minimizing	O
the	O
empirical	B
risk	B
will	O
typically	O
result	O
in	O
overfitting	O
necessary	O
to	O
add	O
a	O
complexity	O
penalty	O
to	O
the	O
objective	O
function	O
it	O
is	O
therefore	O
often	O
rempd	O
c	O
chapter	O
frequentist	B
statistics	I
where	O
c	O
measures	O
the	O
complexity	O
of	O
the	O
prediction	O
function	O
and	O
controls	O
the	O
strength	O
of	O
the	O
complexity	O
penalty	O
this	O
approach	O
is	O
known	O
as	O
regularized	B
risk	B
minimization	I
note	O
that	O
if	O
the	O
loss	B
function	I
is	O
negative	B
log	I
likelihood	B
and	O
the	O
regularizer	O
is	O
a	O
negative	O
log	O
prior	O
this	O
is	O
equivalent	O
to	O
map	O
estimation	O
the	O
two	O
key	O
issues	O
in	O
rrm	O
are	O
how	O
do	O
we	O
measure	O
complexity	O
and	O
how	O
do	O
we	O
pick	O
for	O
a	O
linear	O
model	O
we	O
can	O
define	O
the	O
complexity	O
of	O
in	O
terms	O
of	O
its	O
degrees	B
of	I
freedom	I
discussed	O
in	O
section	O
for	O
more	O
general	O
models	O
we	O
can	O
use	O
the	O
vc	B
dimension	I
discussed	O
in	O
section	O
to	O
pick	O
we	O
can	O
use	O
the	O
methods	O
discussed	O
in	O
section	O
structural	B
risk	B
minimization	I
the	O
regularized	B
risk	B
minimization	I
principle	O
says	O
that	O
we	O
should	O
fit	O
the	O
model	O
for	O
a	O
given	O
complexity	O
penalty	O
by	O
using	O
argmin	O
c	O
but	O
how	O
should	O
we	O
pick	O
we	O
cannot	O
using	O
the	O
training	B
set	I
since	O
this	O
will	O
underestimate	O
the	O
true	O
risk	B
a	O
problem	O
known	O
as	O
optimism	B
of	I
the	I
training	I
error	I
as	O
an	O
alternative	O
we	O
can	O
use	O
the	O
following	O
rule	O
known	O
as	O
the	O
structural	B
risk	B
minimization	I
principle	O
argmin	O
r	O
where	O
r	O
is	O
an	O
estimate	O
of	O
the	O
risk	B
there	O
are	O
two	O
widely	O
used	O
estimates	O
cross	B
validation	I
and	O
theoretical	O
upper	O
bounds	O
on	O
the	O
risk	B
we	O
discuss	O
both	O
of	O
these	O
below	O
estimating	O
the	O
risk	B
using	O
cross	B
validation	I
we	O
can	O
estimate	O
the	O
risk	B
of	O
some	O
estimator	B
using	O
a	O
validation	B
set	I
if	O
we	O
don	O
t	O
have	O
a	O
separate	O
validation	B
set	I
we	O
can	O
use	O
cross	B
validation	I
as	O
we	O
briefly	O
discussed	O
in	O
section	O
more	O
precisely	O
cv	B
is	O
defined	O
as	O
follows	O
let	O
there	O
be	O
n	O
data	O
cases	O
in	O
the	O
training	B
set	I
denote	O
the	O
data	O
in	O
the	O
k	O
th	O
test	O
fold	O
by	O
dk	O
and	O
all	O
the	O
other	O
data	O
by	O
d	O
k	O
stratified	O
cv	B
these	O
folds	B
are	O
chosen	O
so	O
the	O
class	O
proportions	O
discrete	B
labels	O
are	O
present	O
are	O
roughly	O
equal	O
in	O
each	O
fold	O
let	O
f	O
be	O
a	O
learning	B
algorithm	O
or	O
fitting	O
function	O
that	O
takes	O
a	O
dataset	O
and	O
a	O
model	O
index	O
m	O
could	O
a	O
discrete	B
index	O
such	O
as	O
the	O
degree	B
of	O
a	O
polynomial	O
or	O
a	O
continuous	O
index	O
such	O
as	O
the	O
strength	O
of	O
a	O
regularizer	O
and	O
returns	O
a	O
parameter	B
vector	O
m	O
fd	O
m	O
finally	O
let	O
p	O
be	O
a	O
prediction	O
function	O
that	O
takes	O
an	O
input	O
and	O
a	O
parameter	B
vector	O
and	O
returns	O
a	O
prediction	O
y	O
px	O
f	O
thus	O
the	O
combined	O
fit-predict	O
cycle	B
is	O
denoted	O
as	O
fmxd	O
pxfd	O
m	O
empirical	B
risk	B
minimization	I
i	O
dk	O
the	O
k-fold	O
cv	B
estimate	O
of	O
the	O
risk	B
of	O
fm	O
is	O
defined	O
by	O
rmd	O
k	O
n	O
l	O
k	O
m	O
mx	O
k	O
m	O
be	O
note	O
that	O
we	O
can	O
call	O
the	O
fitting	O
algorithm	O
once	O
per	O
fold	O
let	O
f	O
k	O
the	O
function	O
that	O
was	O
trained	O
on	O
all	O
the	O
data	O
except	O
for	O
the	O
test	O
data	O
in	O
fold	O
k	O
then	O
we	O
can	O
rewrite	O
the	O
cv	B
estimate	O
as	O
n	O
l	O
yi	O
f	O
k	O
mxi	O
l	O
yi	O
f	O
ki	O
m	O
rmd	O
k	O
i	O
dk	O
n	O
where	O
ki	O
is	O
the	O
fold	O
in	O
which	O
i	O
is	O
used	O
as	O
test	O
data	O
model	O
that	O
was	O
trained	O
on	O
data	O
that	O
does	O
not	O
contain	O
xi	O
in	O
other	O
words	O
we	O
predict	O
yi	O
using	O
a	O
of	O
k	O
n	O
the	O
method	O
is	O
known	O
as	O
leave	B
one	I
out	I
cross	B
validation	I
or	O
loocv	B
in	O
this	O
case	O
n	O
the	O
estimated	O
risk	B
becomes	O
rmd	O
n	O
n	O
yi	O
f	O
i	O
m	O
l	O
mx	O
pxfd	O
i	O
m	O
this	O
requires	O
fitting	O
the	O
model	O
n	O
times	O
where	O
for	O
f	O
i	O
where	O
f	O
i	O
m	O
we	O
omit	O
the	O
i	O
th	O
training	O
case	O
fortunately	O
for	O
some	O
model	O
classes	O
and	O
loss	B
functions	O
linear	O
models	O
and	O
quadratic	B
loss	B
we	O
can	O
fit	O
the	O
model	O
once	O
and	O
analytically	O
remove	O
the	O
effect	O
of	O
the	O
i	O
th	O
training	O
case	O
this	O
is	O
known	O
as	O
generalized	B
cross	B
validation	I
or	O
gcv	O
example	O
using	O
cv	B
to	O
pick	O
for	O
ridge	B
regression	B
i	O
dk	O
as	O
a	O
concrete	O
example	O
consider	O
picking	O
the	O
strength	O
of	O
the	O
regularizer	O
in	O
penalized	O
linear	B
regression	B
we	O
use	O
the	O
following	O
rule	O
r	O
k	O
arg	O
min	O
min	O
max	O
where	O
min	O
max	O
is	O
a	O
finite	O
range	O
of	O
values	O
that	O
we	O
search	O
over	O
and	O
r	O
k	O
is	O
the	O
k-fold	O
cv	B
estimate	O
of	O
the	O
risk	B
of	O
using	O
given	O
by	O
r	O
k	O
xt	O
w	O
k	O
is	O
the	O
prediction	O
function	O
trained	O
on	O
data	O
excluding	O
fold	O
k	O
and	O
is	O
the	O
map	B
estimate	I
figure	O
gives	O
an	O
example	O
where	O
f	O
k	O
w	O
arg	O
minw	O
n	O
llwd	O
of	O
a	O
cv	B
estimate	O
of	O
the	O
risk	B
vs	O
log	O
where	O
the	O
loss	B
function	I
is	O
squared	B
error	I
lyi	O
f	O
k	O
when	O
performing	O
classification	O
we	O
usually	O
use	O
loss	B
in	O
this	O
case	O
we	O
optimize	O
a	O
convex	B
upper	O
bound	O
on	O
the	O
empirical	B
risk	B
to	O
estimate	O
w	O
m	O
but	O
we	O
optimize	O
cv	B
estimate	O
of	O
the	O
risk	B
itself	O
to	O
estimate	O
we	O
can	O
handle	O
the	O
non-smooth	B
loss	B
function	I
when	O
estimating	O
because	O
we	O
are	O
using	O
brute-force	O
search	O
over	O
the	O
entire	O
space	O
when	O
we	O
have	O
more	O
than	O
one	O
or	O
two	O
tuning	O
parameters	O
this	O
approach	O
becomes	O
infeasible	O
in	O
such	O
cases	O
one	O
can	O
use	O
empirical	B
bayes	I
which	O
allows	O
one	O
to	O
optimize	O
large	O
numbers	O
of	O
hyper-parameters	B
using	O
gradient-based	O
optimizers	O
instead	O
of	O
brute-force	O
search	O
see	O
section	O
for	O
details	O
chapter	O
frequentist	B
statistics	I
fold	O
cross	B
validation	I
ntrain	O
mean	B
squared	B
error	I
train	O
mse	B
test	O
mse	B
e	O
s	O
m	O
log	O
lambda	O
log	O
lambda	O
figure	O
mean	B
squared	B
error	I
for	O
penalized	O
degree	B
polynomial	B
regression	B
vs	O
log	O
regularizer	O
same	O
as	O
in	O
figures	O
except	O
now	O
we	O
have	O
n	O
training	O
points	O
instead	O
of	O
the	O
stars	O
correspond	O
to	O
the	O
values	O
used	O
to	O
plot	O
the	O
functions	O
in	O
figure	O
cv	B
estimate	O
the	O
vertical	O
scale	O
is	O
truncated	O
for	O
clarity	O
the	O
blue	O
line	O
corresponds	O
to	O
the	O
value	O
chosen	O
by	O
the	O
one	O
standard	B
error	I
rule	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
the	O
one	O
standard	B
error	I
rule	O
the	O
above	O
procedure	O
estimates	O
the	O
risk	B
but	O
does	O
not	O
give	O
any	O
measure	O
of	O
uncertainty	B
a	O
standard	O
frequentist	B
measure	O
of	O
uncertainty	B
of	O
an	O
estimate	O
is	O
the	O
standard	B
error	I
of	I
the	I
mean	B
defined	O
by	O
n	O
n	O
se	O
where	O
is	O
an	O
estimate	O
of	O
the	O
variance	B
of	O
the	O
loss	B
li	O
lyi	O
f	O
ki	O
m	O
l	O
n	O
n	O
li	O
note	O
that	O
measures	O
the	O
intrinsic	O
variability	O
of	O
li	O
across	O
samples	B
whereas	O
se	O
measures	O
our	O
uncertainty	B
about	O
the	O
mean	B
l	O
suppose	O
we	O
apply	O
cv	B
to	O
a	O
set	O
of	O
models	O
and	O
compute	O
the	O
mean	B
and	O
se	O
of	O
their	O
estimated	O
risks	O
a	O
common	O
heuristic	O
for	O
picking	O
a	O
model	O
from	O
these	O
noisy	O
estimates	O
is	O
to	O
pick	O
the	O
value	O
which	O
corresponds	O
to	O
the	O
simplest	O
model	O
whose	O
risk	B
is	O
no	O
more	O
than	O
one	O
standard	B
error	I
above	O
the	O
risk	B
of	O
the	O
best	O
model	O
this	O
is	O
called	O
the	O
one-standard	B
error	I
rule	I
et	O
al	O
for	O
example	O
in	O
figure	O
we	O
see	O
that	O
this	O
heuristic	O
does	O
not	O
choose	O
the	O
lowest	O
point	O
on	O
the	O
curve	O
but	O
one	O
that	O
is	O
slightly	O
to	O
its	O
right	O
since	O
that	O
corresponds	O
to	O
a	O
more	O
heavily	O
regularized	O
model	O
with	O
essentially	O
the	O
same	O
empirical	O
performance	O
empirical	B
risk	B
minimization	I
cv	B
for	O
model	B
selection	I
in	O
non-probabilistic	O
unsupervised	B
learning	B
if	O
we	O
are	O
performing	O
unsupervised	B
learning	B
we	O
must	O
use	O
a	O
loss	B
function	I
such	O
as	O
lx	O
which	O
measures	O
reconstruction	B
error	I
here	O
is	O
some	O
encode-decode	O
scheme	O
however	O
as	O
we	O
discussed	O
in	O
section	O
we	O
cannot	O
use	O
cv	B
to	O
determine	O
the	O
complexity	O
of	O
since	O
we	O
will	O
always	O
get	O
lower	O
loss	B
with	O
a	O
more	O
complex	O
model	O
even	O
if	O
evaluated	O
on	O
the	O
test	O
set	O
this	O
is	O
because	O
more	O
complex	O
models	O
will	O
compress	O
the	O
data	O
less	O
and	O
induce	O
less	O
distortion	B
consequently	O
we	O
must	O
either	O
use	O
probabilistic	O
models	O
or	O
invent	O
other	O
heuristics	B
upper	O
bounding	O
the	O
risk	B
using	O
statistical	B
learning	B
theory	I
the	O
principle	O
problem	O
with	O
cross	B
validation	I
is	O
that	O
it	O
is	O
slow	O
since	O
we	O
have	O
to	O
fit	O
the	O
model	O
multiple	O
times	O
this	O
motivates	O
the	O
desire	O
to	O
compute	O
analytic	O
approximations	O
or	O
bounds	O
to	O
the	O
generalization	B
error	I
this	O
is	O
the	O
studied	O
in	O
the	O
field	O
of	O
statistical	B
learning	B
theory	I
more	O
precisely	O
slt	O
tries	O
to	O
bound	O
the	O
risk	B
rp	O
h	O
for	O
any	O
data	O
distribution	O
p	O
and	O
hypothesis	O
h	O
h	O
in	O
terms	O
of	O
the	O
empirical	B
risk	B
rempd	O
h	O
the	O
sample	O
size	O
n	O
and	O
the	O
size	O
of	O
the	O
hypothesis	B
space	I
h	O
let	O
us	O
initially	O
consider	O
the	O
case	O
where	O
the	O
hypothesis	B
space	I
is	O
finite	O
with	O
size	O
dimh	O
in	O
other	O
words	O
we	O
are	O
selecting	O
a	O
model	O
hypothesis	O
from	O
a	O
finite	O
list	O
rather	O
than	O
optimizing	O
real-valued	O
parameters	O
then	O
we	O
can	O
prove	O
the	O
following	O
theorem	O
for	O
any	O
data	O
distribution	O
p	O
and	O
any	O
dataset	O
d	O
of	O
size	O
n	O
drawn	O
from	O
p	O
the	O
probability	O
that	O
our	O
estimate	O
of	O
the	O
error	O
rate	B
will	O
be	O
more	O
than	O
wrong	O
in	O
the	O
worst	O
case	O
is	O
upper	O
bounded	O
as	O
follows	O
h	O
rp	O
h	O
dimhe	O
p	O
max	O
h	O
h	O
proof	O
to	O
prove	O
this	O
we	O
need	O
two	O
useful	O
results	O
first	O
hoeffding	O
s	O
inequality	O
which	O
states	O
that	O
if	O
xn	O
ber	O
then	O
for	O
any	O
p	O
where	O
x	O
n	O
events	O
then	O
p	O
d	O
xi	O
second	O
the	O
union	B
bound	I
which	O
says	O
that	O
if	O
ad	O
are	O
a	O
set	O
of	O
finally	O
for	O
notational	O
brevity	O
let	O
rh	O
rh	O
p	O
be	O
the	O
true	O
risk	B
and	O
rn	O
rempd	O
h	O
p	O
be	O
the	O
empirical	B
risk	B
using	O
these	O
results	O
we	O
have	O
p	O
max	O
h	O
h	O
rn	O
rh	O
p	O
h	O
h	O
h	O
h	O
h	O
h	O
p	O
rn	O
rh	O
rn	O
rh	O
dimhe	O
chapter	O
frequentist	B
statistics	I
ths	O
bound	O
tells	O
us	O
that	O
the	O
optimism	B
of	I
the	I
training	I
error	I
increases	O
with	O
dimh	O
but	O
decreases	O
with	O
n	O
as	O
is	O
to	O
be	O
expected	O
if	O
the	O
hypothesis	B
space	I
h	O
is	O
infinite	O
we	O
have	O
real-valued	O
parameters	O
we	O
cannot	O
use	O
dimh	O
instead	O
we	O
can	O
use	O
a	O
quantity	O
called	O
the	O
vapnik-chervonenkis	B
or	O
vc	B
dimension	I
of	O
the	O
hypothesis	O
class	O
see	O
for	O
details	O
stepping	O
back	O
from	O
all	O
the	O
theory	O
the	O
key	O
intuition	O
behind	O
statistical	B
learning	B
theory	I
is	O
quite	O
if	O
the	O
hypothesis	B
space	I
h	O
is	O
very	O
simple	O
suppose	O
we	O
find	O
a	O
model	O
with	O
low	O
empirical	B
risk	B
big	O
relative	O
to	O
the	O
data	O
size	O
then	O
it	O
is	O
quite	O
likely	O
that	O
we	O
just	O
got	O
lucky	O
and	O
were	O
given	O
a	O
data	O
set	O
that	O
is	O
well-modeled	O
by	O
our	O
chosen	O
function	O
by	O
chance	O
however	O
this	O
does	O
not	O
mean	B
that	O
such	O
a	O
function	O
will	O
have	O
low	O
generalization	B
error	I
but	O
if	O
the	O
hypothesis	O
class	O
is	O
sufficiently	O
constrained	O
in	O
size	O
andor	O
the	O
training	B
set	I
is	O
sufficiently	O
large	O
then	O
we	O
are	O
unlikely	O
to	O
get	O
lucky	O
in	O
this	O
way	O
so	O
a	O
low	O
empirical	B
risk	B
is	O
evidence	B
of	O
a	O
low	O
true	O
risk	B
note	O
that	O
optimism	B
of	I
the	I
training	I
error	I
does	O
not	O
necessarily	O
increase	O
with	O
model	O
complexity	O
but	O
it	O
does	O
increase	O
with	O
the	O
number	O
of	O
different	O
models	O
that	O
are	O
being	O
searched	O
over	O
the	O
advantage	O
of	O
statistical	B
learning	B
theory	I
compared	O
to	O
cv	B
is	O
that	O
the	O
bounds	O
on	O
the	O
risk	B
are	O
quicker	O
to	O
compute	O
than	O
using	O
cv	B
the	O
disadvantage	O
is	O
that	O
it	O
is	O
hard	O
to	O
compute	O
the	O
vc	B
dimension	I
for	O
many	O
interesting	O
models	O
and	O
the	O
upper	O
bounds	O
are	O
usually	O
very	O
loose	O
see	O
and	O
langford	O
one	O
can	O
extend	O
statistical	B
learning	B
theory	I
by	O
taking	O
computational	O
complexity	O
of	O
the	O
learner	O
into	O
account	O
this	O
field	O
is	O
called	O
computational	B
learning	B
theory	I
or	O
colt	B
most	O
of	O
this	O
work	O
focuses	O
on	O
the	O
case	O
where	O
h	O
is	O
a	O
binary	O
classifier	O
and	O
the	O
loss	B
function	I
is	O
loss	B
if	O
we	O
observe	O
a	O
low	O
empirical	B
risk	B
and	O
the	O
hypothesis	B
space	I
is	O
suitably	O
small	O
then	O
we	O
can	O
say	O
that	O
our	O
estimated	O
function	O
is	O
probably	B
approximately	I
correct	I
or	O
pac	B
a	O
hypothesis	B
space	I
is	O
said	O
to	O
be	O
efficiently	B
pac-learnable	I
if	O
there	O
is	O
a	O
polynomial	O
time	O
algorithm	O
that	O
can	O
identify	O
a	O
function	O
that	O
is	O
pac	B
see	O
and	O
vazirani	O
for	O
details	O
surrogate	B
loss	B
functions	O
minimizing	O
the	O
loss	B
in	O
the	O
erm	B
rrm	O
framework	O
is	O
not	O
always	O
easy	O
for	O
example	O
we	O
might	O
want	O
to	O
optimize	O
the	O
auc	B
or	O
scores	B
or	O
more	O
simply	O
we	O
might	O
just	O
want	O
to	O
minimize	O
the	O
loss	B
as	O
is	O
common	O
in	O
classification	O
unfortunately	O
the	O
risk	B
is	O
a	O
very	O
non-smooth	B
objective	O
and	O
hence	O
is	O
hard	O
to	O
optimize	O
one	O
alternative	O
is	O
to	O
use	O
maximum	O
likelihood	B
estimation	O
instead	O
since	O
log-likelihood	O
is	O
a	O
smooth	O
convex	B
upper	O
bound	O
on	O
the	O
risk	B
as	O
we	O
show	O
below	O
to	O
see	O
this	O
consider	O
binary	O
logistic	B
regression	B
and	O
let	O
yi	O
suppose	O
our	O
decision	B
function	O
computes	O
the	O
log-odds	B
ratio	I
py	O
w	O
py	O
w	O
f	O
log	O
wt	O
xi	O
i	O
then	O
the	O
corresponding	O
probability	O
distribution	O
on	O
the	O
output	O
label	B
is	O
pyixi	O
w	O
sigmyi	O
i	O
let	O
us	O
define	O
the	O
log-loss	B
as	O
as	O
l	O
nlly	O
log	O
pyx	O
w	O
e	O
y	O
pathologies	B
of	O
frequentist	B
statistics	I
s	O
s	O
o	O
l	O
hinge	O
logloss	O
figure	O
y	O
the	O
vertical	O
axis	O
is	O
the	O
loss	B
the	O
log	O
loss	B
uses	O
log	O
base	O
figure	O
generated	O
by	O
hingelossplot	O
illustration	O
of	O
various	O
loss	B
functions	O
for	O
binary	O
classification	O
the	O
horizontal	O
axis	O
is	O
the	O
margin	B
now	O
consider	O
computing	O
the	O
most	O
probable	O
label	B
which	O
is	O
equivalent	O
to	O
using	O
y	O
if	O
it	O
is	O
clear	O
that	O
minimizing	O
the	O
average	O
log-loss	B
is	O
equivalent	O
to	O
maximizing	O
the	O
likelihood	B
i	O
and	O
y	O
if	O
i	O
the	O
loss	B
of	O
our	O
function	O
becomes	O
iy	O
y	O
iy	O
figure	O
plots	O
these	O
two	O
loss	B
functions	O
we	O
see	O
that	O
the	O
nll	B
is	O
indeed	O
an	O
upper	O
bound	O
on	O
the	O
loss	B
log-loss	B
is	O
an	O
example	O
of	O
a	O
surrogate	B
loss	B
function	I
another	O
example	O
is	O
the	O
hinge	B
loss	B
l	O
hingey	O
y	O
see	O
figure	O
for	O
a	O
plot	O
we	O
see	O
that	O
the	O
function	O
looks	O
like	O
a	O
door	O
hinge	O
hence	O
its	O
name	O
this	O
loss	B
function	I
forms	O
the	O
basis	O
of	O
a	O
popular	O
classification	O
method	O
known	O
as	O
support	B
vector	I
machines	I
which	O
we	O
will	O
discuss	O
in	O
section	O
the	O
surrogate	O
is	O
usually	O
chosen	O
to	O
be	O
a	O
convex	B
upper	O
bound	O
since	O
convex	B
functions	O
are	O
easy	O
to	O
minimize	O
see	O
e	O
g	O
et	O
al	O
for	O
more	O
information	B
pathologies	B
of	O
frequentist	B
statistics	I
i	O
believe	O
that	O
it	O
would	O
be	O
very	O
difficult	O
to	O
persuade	O
an	O
intelligent	O
person	O
that	O
current	O
statistical	O
practice	O
was	O
sensible	O
but	O
that	O
there	O
would	O
be	O
much	O
less	O
difficulty	O
with	O
an	O
approach	O
via	O
likelihood	B
and	O
bayes	B
theorem	I
george	O
box	O
frequentist	B
statistics	I
exhibits	O
various	O
forms	O
of	O
weird	O
and	O
undesirable	O
behaviors	O
known	O
as	O
pathologies	B
we	O
give	O
a	O
few	O
examples	O
below	O
in	O
order	O
to	O
caution	O
the	O
reader	O
these	O
and	O
other	O
examples	O
are	O
explained	O
in	O
more	O
detail	O
in	O
lindley	O
and	O
phillips	O
lindley	O
berger	O
jaynes	O
minka	O
chapter	O
frequentist	B
statistics	I
counter-intuitive	O
behavior	O
of	O
confidence	O
intervals	O
a	O
confidence	O
interval	O
is	O
an	O
interval	O
derived	O
from	O
the	O
sampling	B
distribution	I
of	O
an	O
estimator	B
a	O
bayesian	B
credible	B
interval	I
is	O
derived	O
from	O
the	O
posterior	O
of	O
a	O
parameter	B
as	O
we	O
discussed	O
in	O
section	O
more	O
precisely	O
a	O
frequentist	B
confidence	O
interval	O
for	O
some	O
parameter	B
is	O
defined	O
by	O
the	O
following	O
un-natural	O
expression	O
u	O
d	O
u	O
d	O
d	O
that	O
is	O
if	O
we	O
sample	O
hypothetical	O
future	O
data	O
d	O
from	O
then	O
d	O
u	O
d	O
is	O
a	O
confidence	O
interval	O
if	O
the	O
parameter	B
lies	O
inside	O
this	O
interval	O
percent	O
of	O
the	O
time	O
in	O
bayesian	B
statistics	I
we	O
condition	O
on	O
what	O
is	O
known	O
namely	O
the	O
observed	O
data	O
d	O
and	O
average	O
over	O
what	O
is	O
not	O
known	O
namely	O
the	O
parameter	B
in	O
frequentist	B
statistics	I
we	O
do	O
exactly	O
the	O
opposite	O
we	O
condition	O
on	O
what	O
is	O
unknown	B
namely	O
the	O
true	O
parameter	B
value	O
and	O
average	O
over	O
hypothetical	O
future	O
data	O
sets	O
d	O
this	O
counter-intuitive	O
definition	O
of	O
confidence	O
intervals	O
can	O
lead	O
to	O
bizarre	O
results	O
consider	O
the	O
following	O
example	O
from	O
suppose	O
we	O
draw	O
two	O
integers	O
d	O
from	O
let	O
us	O
step	O
back	O
for	O
a	O
moment	O
and	O
think	O
about	O
what	O
is	O
going	O
on	O
px	O
if	O
x	O
if	O
x	O
otherwise	O
if	O
we	O
would	O
expect	O
the	O
following	O
outcomes	O
each	O
with	O
probability	O
let	O
m	O
and	O
define	O
the	O
following	O
confidence	O
interval	O
ud	O
m	O
for	O
the	O
above	O
samples	B
this	O
yields	O
hence	O
equation	O
is	O
clearly	O
a	O
ci	B
since	O
is	O
contained	O
in	O
of	O
these	O
intervals	O
however	O
if	O
d	O
then	O
p	O
so	O
we	O
know	O
that	O
must	O
be	O
yet	O
we	O
only	O
have	O
confidence	O
in	O
this	O
fact	O
another	O
less	O
contrived	O
example	O
is	O
as	O
follows	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
parameter	B
of	O
a	O
bernoulli	B
distribution	O
let	O
x	O
xi	O
be	O
the	O
sample	O
mean	B
the	O
mle	B
is	O
x	O
an	O
n	O
xn	O
is	O
approximate	O
confidence	O
interval	O
for	O
a	O
bernoulli	B
parameter	B
is	O
x	O
called	O
a	O
wald	B
interval	I
and	O
is	O
based	O
on	O
a	O
gaussian	B
approximation	I
to	O
the	O
binomial	B
distribution	I
compare	O
to	O
equation	O
now	O
consider	O
a	O
single	O
trial	O
where	O
n	O
and	O
the	O
mle	B
is	O
which	O
overfits	O
as	O
we	O
saw	O
in	O
section	O
but	O
our	O
confidence	O
interval	O
is	O
also	O
which	O
seems	O
even	O
worse	O
it	O
can	O
be	O
argued	O
that	O
the	O
above	O
flaw	O
is	O
because	O
we	O
approximated	O
the	O
true	O
sampling	B
distribution	I
with	O
a	O
gaussian	B
or	O
because	O
the	O
sample	O
size	O
was	O
to	O
small	O
or	O
the	O
parameter	B
too	O
extreme	O
however	O
the	O
wald	B
interval	I
can	O
behave	O
badly	O
even	O
for	O
large	O
n	O
and	O
non-extreme	O
parameters	O
et	O
al	O
pathologies	B
of	O
frequentist	B
statistics	I
p-values	O
considered	O
harmful	O
suppose	O
we	O
want	O
to	O
decide	O
whether	O
to	O
accept	B
or	O
reject	O
some	O
baseline	O
model	O
which	O
we	O
will	O
call	O
the	O
null	B
hypothesis	I
we	O
need	O
to	O
define	O
some	O
decision	B
rule	I
in	O
frequentist	B
statistics	I
it	O
is	O
standard	O
to	O
first	O
compute	O
a	O
quantity	O
called	O
the	O
p-value	B
which	O
is	O
defined	O
as	O
the	O
probability	O
the	O
null	O
of	O
observing	O
some	O
test	B
statistic	I
f	O
as	O
the	O
chi-squared	B
statistic	I
that	O
is	O
as	O
large	O
or	O
larger	O
than	O
that	O
actually	O
pvalued	O
p	O
d	O
f	O
d	O
this	O
quantity	O
relies	O
on	O
computing	O
a	O
tail	B
area	I
probability	I
of	O
the	O
sampling	B
distribution	I
we	O
give	O
an	O
example	O
of	O
how	O
to	O
do	O
this	O
below	O
given	O
the	O
p-value	B
we	O
define	O
our	O
decision	B
rule	I
as	O
follows	O
we	O
reject	O
the	O
null	B
hypothesis	I
iff	B
the	O
p-value	B
is	O
less	O
than	O
some	O
threshold	O
such	O
as	O
if	O
we	O
do	O
reject	O
it	O
we	O
say	O
the	O
difference	O
between	O
the	O
observed	O
test	B
statistic	I
and	O
the	O
expected	O
test	B
statistic	I
is	O
statistically	O
significant	O
at	O
level	O
this	O
approach	O
is	O
known	O
as	O
null	B
hypothesis	I
significance	O
testing	O
ornhst	O
this	O
procedure	O
guarantees	O
that	O
our	O
expected	O
type	B
i	I
positive	O
error	O
rate	B
is	O
at	O
most	O
this	O
is	O
sometimes	O
interpreted	O
as	O
saying	O
that	O
frequentist	B
hypothesis	O
testing	O
is	O
very	O
conservative	O
since	O
it	O
is	O
unlikely	O
to	O
accidently	O
reject	O
the	O
null	B
hypothesis	I
but	O
in	O
fact	O
the	O
opposite	O
is	O
the	O
case	O
because	O
this	O
method	O
only	O
worries	O
about	O
trying	O
to	O
reject	O
the	O
null	O
it	O
can	O
never	O
gather	O
evidence	B
in	O
favor	O
of	O
the	O
null	O
no	O
matter	O
how	O
large	O
the	O
sample	O
size	O
because	O
of	O
this	O
p-values	O
tend	O
to	O
overstate	O
the	O
evidence	B
against	O
the	O
null	O
and	O
are	O
thus	O
very	O
trigger	O
happy	O
in	O
general	O
there	O
can	O
be	O
huge	O
differences	O
between	O
p-values	O
and	O
the	O
quantity	O
that	O
we	O
really	O
care	O
about	O
which	O
is	O
the	O
posterior	O
probability	O
of	O
the	O
null	B
hypothesis	I
given	O
the	O
data	O
in	O
particular	O
sellke	O
et	O
al	O
show	O
that	O
even	O
if	O
the	O
p-value	B
is	O
as	O
slow	O
as	O
the	O
posterior	O
probability	O
of	O
is	O
at	O
least	O
and	O
often	O
much	O
higher	O
so	O
frequentists	O
often	O
claim	O
to	O
have	O
significant	O
evidence	B
of	O
an	O
effect	O
that	O
cannot	O
be	O
explained	O
by	O
the	O
null	B
hypothesis	I
whereas	O
bayesians	O
are	O
usually	O
more	O
conservative	O
in	O
their	O
claims	O
for	O
example	O
p-values	O
have	O
been	O
used	O
to	O
prove	O
that	O
esp	O
perception	O
is	O
real	O
et	O
al	O
even	O
though	O
esp	O
is	O
clearly	O
very	O
improbable	O
for	O
this	O
reason	O
p-values	O
have	O
been	O
banned	O
from	O
certain	O
medical	O
journals	O
another	O
problem	O
with	O
p-values	O
is	O
that	O
their	O
computation	O
depends	O
on	O
decisions	O
you	O
make	O
about	O
when	O
to	O
stop	O
collecting	O
data	O
even	O
if	O
these	O
decisions	O
don	O
t	O
change	O
the	O
data	O
you	O
actually	O
observed	O
for	O
example	O
suppose	O
i	O
toss	O
a	O
coin	O
n	O
times	O
and	O
observe	O
s	O
successes	O
and	O
f	O
failures	O
so	O
n	O
s	O
f	O
in	O
this	O
case	O
n	O
is	O
fixed	O
and	O
s	O
hence	O
f	O
is	O
random	O
the	O
relevant	O
sampling	O
model	O
is	O
the	O
binomial	B
binsn	O
s	O
n	O
s	O
let	O
the	O
null	B
hypothesis	I
be	O
that	O
the	O
coin	O
is	O
fair	O
where	O
is	O
the	O
probability	O
of	O
success	O
the	O
one-sided	O
p-value	B
using	O
test	B
statistic	I
ts	O
s	O
is	O
p	O
s	O
the	O
reason	O
we	O
cannot	O
just	O
compute	O
the	O
probability	O
of	O
the	O
observed	O
value	O
of	O
the	O
test	B
statistic	I
is	O
that	O
this	O
will	O
have	O
probability	O
zero	O
under	O
a	O
pdf	B
the	O
p-value	B
is	O
defined	O
in	O
terms	O
of	O
the	O
cdf	B
so	O
is	O
always	O
a	O
number	O
between	O
and	O
chapter	O
frequentist	B
statistics	I
the	O
two-sided	O
p-value	B
is	O
in	O
either	O
case	O
the	O
p-value	B
is	O
larger	O
than	O
the	O
magical	O
threshold	O
so	O
a	O
frequentist	B
would	O
not	O
reject	O
the	O
null	B
hypothesis	I
now	O
suppose	O
i	O
told	O
you	O
that	O
i	O
actually	O
kept	O
tossing	O
the	O
coin	O
until	O
i	O
observed	O
f	O
tails	O
in	O
this	O
case	O
f	O
is	O
fixed	O
and	O
n	O
hence	O
s	O
n	O
f	O
is	O
random	O
the	O
probability	O
model	O
becomes	O
the	O
negative	B
binomial	B
distribution	I
given	O
by	O
negbinomsf	O
s	O
f	O
f	O
where	O
f	O
n	O
s	O
note	O
that	O
the	O
term	O
which	O
depends	O
on	O
is	O
the	O
same	O
in	O
equations	O
and	O
so	O
the	O
posterior	O
over	O
would	O
be	O
the	O
same	O
in	O
both	O
cases	O
however	O
these	O
two	O
interpretations	O
of	O
the	O
same	O
data	O
give	O
different	O
p-values	O
in	O
particular	O
under	O
the	O
negative	B
binomial	B
model	O
we	O
get	O
p	O
s	O
so	O
the	O
p-value	B
is	O
and	O
suddenly	O
there	O
seems	O
to	O
be	O
significant	O
evidence	B
of	O
bias	B
in	O
the	O
coin	O
obviously	O
this	O
is	O
ridiculous	O
the	O
data	O
is	O
the	O
same	O
so	O
our	O
inferences	O
about	O
the	O
coin	O
should	O
be	O
the	O
same	O
after	O
all	O
i	O
could	O
have	O
chosen	O
the	O
experimental	O
protocol	O
at	O
random	O
it	O
is	O
the	O
outcome	O
of	O
the	O
experiment	O
that	O
matters	O
not	O
the	O
details	O
of	O
how	O
i	O
decided	O
which	O
one	O
to	O
run	O
although	O
this	O
might	O
seem	O
like	O
just	O
a	O
mathematical	O
curiosity	O
this	O
also	O
has	O
significant	O
practical	O
in	O
particular	O
the	O
fact	O
that	O
the	O
stopping	B
rule	I
affects	O
the	O
computation	O
of	O
the	O
pimplications	O
value	O
means	O
that	O
frequentists	O
often	O
do	O
not	O
terminate	O
experiments	O
early	O
even	O
when	O
it	O
is	O
obvious	O
what	O
the	O
conclusions	O
are	O
lest	O
it	O
adversely	O
affect	O
their	O
statistical	O
analysis	O
if	O
the	O
experiments	O
are	O
costly	O
or	O
harmful	O
to	O
people	O
this	O
is	O
obviously	O
a	O
bad	O
idea	O
perhaps	O
it	O
is	O
not	O
surprising	O
then	O
that	O
the	O
us	O
food	O
and	O
drug	O
administration	O
which	O
regulates	O
clinical	O
trials	O
of	O
new	O
drugs	O
has	O
recently	O
become	O
supportive	O
of	O
bayesian	B
since	O
bayesian	B
methods	O
are	O
not	O
affected	O
by	O
the	O
stopping	B
rule	I
the	O
likelihood	B
principle	I
the	O
fundamental	O
reason	O
for	O
many	O
of	O
these	O
pathologies	B
is	O
that	O
frequentist	B
inference	B
violates	O
the	O
likelihood	B
principle	I
which	O
says	O
that	O
inference	B
should	O
be	O
based	O
on	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
not	O
based	O
on	O
hypothetical	O
future	O
data	O
that	O
you	O
have	O
not	O
observed	O
bayes	O
obviously	O
satisfies	O
the	O
likelihood	B
principle	I
and	O
consequently	O
does	O
not	O
suffer	O
from	O
these	O
pathologies	B
a	O
compelling	O
argument	O
in	O
favor	O
of	O
the	O
likelihood	B
principle	I
was	O
presented	O
in	O
who	O
showed	O
that	O
it	O
followed	O
automatically	O
from	O
two	O
simpler	O
principles	O
the	O
first	O
of	O
these	O
is	O
the	O
sufficiency	B
principle	I
which	O
says	O
that	O
a	O
sufficient	O
statistic	O
contains	O
all	O
the	O
relevant	O
information	B
see	O
ian	O
pathologies	B
of	O
frequentist	B
statistics	I
about	O
an	O
unknown	B
parameter	B
this	O
is	O
true	O
by	O
definition	O
the	O
second	O
principle	O
is	O
known	O
as	O
weak	B
conditionality	I
which	O
says	O
that	O
inferences	O
should	O
be	O
based	O
on	O
the	O
events	O
that	O
happened	O
not	O
which	O
might	O
have	O
happened	O
to	O
motivate	O
this	O
consider	O
an	O
example	O
from	O
suppose	O
we	O
need	O
to	O
analyse	O
a	O
substance	O
and	O
can	O
send	O
it	O
either	O
to	O
a	O
laboratory	O
in	O
new	O
york	O
or	O
in	O
california	O
the	O
two	O
labs	O
seem	O
equally	O
good	O
so	O
a	O
fair	O
coin	O
is	O
used	O
to	O
decide	O
between	O
them	O
the	O
coin	O
comes	O
up	O
heads	O
so	O
the	O
california	O
lab	O
is	O
chosen	O
when	O
the	O
results	O
come	O
back	O
should	O
it	O
be	O
taken	O
into	O
account	O
that	O
the	O
coin	O
could	O
have	O
come	O
up	O
tails	O
and	O
thus	O
the	O
new	O
york	O
lab	O
could	O
have	O
been	O
used	O
most	O
people	O
would	O
argue	O
that	O
the	O
new	O
york	O
lab	O
is	O
irrelevant	O
since	O
the	O
tails	O
event	O
didn	O
t	O
happen	O
this	O
is	O
an	O
example	O
of	O
weak	B
conditionality	I
given	O
this	O
principle	O
one	O
can	O
show	O
that	O
all	O
inferences	O
should	O
only	O
be	O
based	O
on	O
what	O
was	O
observed	O
which	O
is	O
in	O
contrast	O
to	O
standard	O
frequentist	B
procedures	O
see	O
and	O
wolpert	O
for	O
further	O
details	O
on	O
the	O
likelihood	B
principle	I
why	O
isn	O
t	O
everyone	O
a	O
bayesian	B
given	O
these	O
fundamental	O
flaws	O
of	O
frequentist	B
statistics	I
and	O
the	O
fact	O
that	O
bayesian	B
methods	O
do	O
not	O
have	O
such	O
flaws	O
an	O
obvious	O
question	O
to	O
ask	O
is	O
why	O
isn	O
t	O
everyone	O
a	O
bayesian	B
the	O
statistician	O
bradley	O
efron	O
wrote	O
a	O
paper	O
with	O
exactly	O
this	O
title	O
his	O
short	O
paper	O
is	O
well	O
worth	O
reading	O
for	O
anyone	O
interested	O
in	O
this	O
topic	B
below	O
we	O
quote	O
his	O
opening	O
section	O
the	O
title	O
is	O
a	O
reasonable	O
question	O
to	O
ask	O
on	O
at	O
least	O
two	O
counts	O
first	O
of	O
all	O
everone	O
used	O
to	O
be	O
a	O
bayesian	B
laplace	B
wholeheatedly	O
endorsed	O
bayes	O
s	O
formulation	O
of	O
the	O
inference	B
problem	O
and	O
most	O
scientists	O
followed	O
suit	O
this	O
included	O
gauss	O
whose	O
statistical	O
work	O
is	O
usually	O
presented	O
in	O
frequentist	B
terms	O
a	O
second	O
and	O
more	O
important	O
point	O
is	O
the	O
cogency	O
of	O
the	O
bayesian	B
argument	O
modern	O
statisticians	O
following	O
the	O
lead	O
of	O
savage	O
and	O
de	O
finetti	O
have	O
advanced	O
powerful	O
theoretical	O
arguments	O
for	O
preferring	O
bayesian	B
inference	B
a	O
byproduct	O
of	O
this	O
work	O
is	O
a	O
disturbing	O
catalogue	O
of	O
inconsistencies	O
in	O
the	O
frequentist	B
point	O
of	O
view	O
nevertheless	O
everyone	O
is	O
not	O
a	O
bayesian	B
the	O
current	O
era	O
is	O
the	O
first	O
century	O
in	O
which	O
statistics	O
has	O
been	O
widely	O
used	O
for	O
scientific	O
reporting	O
and	O
in	O
fact	O
statistics	O
is	O
mainly	O
non-bayesian	O
however	O
lindley	O
predicts	O
a	O
change	O
for	O
the	O
century	O
time	O
will	O
tell	O
whether	O
lindley	O
was	O
right	O
exercises	O
exercise	O
pessimism	O
of	O
loocv	B
suppose	O
we	O
have	O
a	O
completely	O
random	O
labeled	O
dataset	O
the	O
features	B
x	O
tell	O
us	O
nothing	O
about	O
the	O
class	O
labels	O
y	O
with	O
examples	O
of	O
class	O
and	O
examples	O
of	O
class	O
where	O
what	O
is	O
the	O
best	O
misclassification	O
rate	B
any	O
method	O
can	O
achieve	O
what	O
is	O
the	O
estimated	O
misclassification	O
rate	B
of	O
the	O
same	O
method	O
using	O
loocv	B
exercise	O
james	B
stein	I
estimator	B
for	O
gaussian	B
means	O
consider	O
the	O
stage	O
model	O
yi	O
i	O
n	O
i	O
and	O
i	O
n	O
we	O
observe	O
the	O
following	O
data	O
points	O
i	O
suppose	O
is	O
known	O
and	O
chapter	O
frequentist	B
statistics	I
a	O
find	O
the	O
ml-ii	O
estimates	O
of	O
and	O
b	O
find	O
the	O
posterior	O
estimates	O
e	O
iyi	O
and	O
var	B
iyi	O
for	O
i	O
other	O
terms	O
c	O
give	O
a	O
credible	B
interval	I
for	O
p	O
iyi	O
for	O
i	O
do	O
you	O
trust	O
this	O
interval	O
the	O
i	O
are	O
computed	O
similarly	O
gaussian	B
assumption	O
is	O
reasonable	O
i	O
e	O
is	O
it	O
likely	O
to	O
be	O
too	O
large	O
or	O
too	O
small	O
or	O
just	O
right	O
d	O
what	O
do	O
you	O
expect	O
would	O
happen	O
to	O
your	O
estimates	O
if	O
were	O
much	O
smaller	O
you	O
do	O
not	O
need	O
to	O
compute	O
the	O
numerical	O
answer	O
just	O
briefly	O
explain	O
what	O
would	O
happen	O
qualitatively	O
and	O
why	O
exercise	O
show	O
that	O
m	O
le	O
n	O
m	O
le	O
is	O
biased	O
is	O
a	O
biased	O
estimator	B
of	O
i	O
e	O
show	O
n	O
n	O
hint	O
note	O
that	O
xn	O
are	O
independent	O
and	O
use	O
the	O
fact	O
that	O
the	O
expectation	O
of	O
a	O
product	O
of	O
independent	O
random	O
variables	O
is	O
the	O
product	O
of	O
the	O
expectations	O
exercise	O
estimation	O
of	O
when	O
is	O
known	O
suppose	O
we	O
sample	O
xn	O
n	O
where	O
is	O
a	O
known	O
constant	O
derive	O
an	O
expression	O
for	O
the	O
mle	B
for	O
in	O
this	O
case	O
is	O
it	O
unbiased	B
linear	B
regression	B
introduction	O
linear	B
regression	B
is	O
the	O
work	O
horse	O
of	O
statistics	O
and	O
machine	B
learning	B
when	O
augmented	O
with	O
kernels	O
or	O
other	O
forms	O
of	O
basis	B
function	I
expansion	I
it	O
can	O
model	O
also	O
nonlinear	O
relationships	O
and	O
when	O
the	O
gaussian	B
output	O
is	O
replaced	O
with	O
a	O
bernoulli	B
or	O
multinoulli	B
distribution	I
it	O
can	O
be	O
used	O
for	O
classification	O
as	O
we	O
will	O
see	O
below	O
so	O
it	O
pays	O
to	O
study	O
this	O
model	O
in	O
detail	O
model	O
specification	O
as	O
we	O
discussed	O
in	O
section	O
linear	B
regression	B
is	O
a	O
model	O
of	O
the	O
form	O
pyx	O
n	O
x	O
linear	B
regression	B
can	O
be	O
made	O
to	O
model	O
non-linear	O
relationships	O
by	O
replacing	O
x	O
with	O
some	O
pyx	O
n	O
non-linear	O
function	O
of	O
the	O
inputs	O
that	O
is	O
we	O
use	O
this	O
is	O
known	O
as	O
basis	B
function	I
expansion	I
that	O
the	O
model	O
is	O
still	O
linear	O
in	O
the	O
parameters	O
w	O
so	O
it	O
is	O
still	O
called	O
linear	B
regression	B
the	O
importance	O
of	O
this	O
will	O
become	O
clear	O
below	O
a	O
simple	O
example	O
are	O
polynomial	O
basis	B
functions	I
where	O
the	O
model	O
has	O
the	O
form	O
x	O
xd	O
figure	O
illustrates	O
the	O
effect	O
of	O
changing	O
d	O
increasingly	O
complex	O
functions	O
we	O
can	O
also	O
apply	O
linear	B
regression	B
to	O
more	O
than	O
input	O
for	O
example	O
consider	O
modeling	O
temperature	B
as	O
a	O
function	O
of	O
location	O
figure	O
plots	O
e	O
and	O
figure	O
plots	O
e	O
increasing	O
the	O
degree	B
d	O
allows	O
us	O
to	O
create	O
maximum	O
likelihood	B
estimation	O
squares	O
a	O
common	O
way	O
to	O
esitmate	O
the	O
parameters	O
of	O
a	O
statistical	O
model	O
is	O
to	O
compute	O
the	O
mle	B
which	O
is	O
defined	O
as	O
arg	O
max	O
log	O
pd	O
chapter	O
linear	B
regression	B
figure	O
linear	B
regression	B
applied	O
to	O
data	O
vertical	O
axis	O
is	O
temperature	B
horizontal	O
axes	O
are	O
location	O
within	O
a	O
room	O
data	O
was	O
collected	O
by	O
some	O
remote	O
sensing	O
motes	B
at	O
intel	O
s	O
lab	O
in	O
berkeley	O
ca	O
courtesy	O
of	O
romain	O
thibaux	O
temperature	B
data	O
is	O
fitted	O
with	O
a	O
quadratic	O
of	O
the	O
form	O
f	O
produced	O
by	O
surfacefitdemo	O
the	O
fitted	O
plane	O
has	O
the	O
form	O
f	O
it	O
is	O
common	O
to	O
assume	O
the	O
training	O
examples	O
are	O
independent	B
and	I
identically	I
distributed	I
commonly	O
abbreviated	O
to	O
iid	B
this	O
means	O
we	O
can	O
write	O
the	O
log-likelihood	O
as	O
follows	O
log	O
pd	O
log	O
pyixi	O
nll	B
instead	O
of	O
maximizing	O
the	O
log-likelihood	O
we	O
can	O
equivalently	O
minimize	O
the	O
negative	B
log	I
likelihood	B
or	O
nll	B
log	O
pyixi	O
the	O
nll	B
formulation	O
is	O
sometimes	O
more	O
convenient	O
since	O
many	O
optimization	B
software	O
packages	O
are	O
designed	O
to	O
find	O
the	O
minima	O
of	O
functions	O
rather	O
than	O
maxima	O
now	O
let	O
us	O
apply	O
the	O
method	O
of	O
mle	B
to	O
the	O
linear	B
regression	B
setting	O
inserting	O
the	O
definition	O
of	O
the	O
gaussian	B
into	O
the	O
above	O
we	O
find	O
that	O
the	O
log	O
likelihood	B
is	O
given	O
by	O
log	O
exp	O
wt	O
rssw	O
n	O
rss	O
stands	O
for	O
residual	B
sum	B
of	I
squares	I
and	O
is	O
defined	O
by	O
rssw	O
wt	O
the	O
rss	O
is	O
also	O
called	O
the	O
sum	B
of	I
squared	I
errors	I
or	O
sse	O
and	O
ssen	O
is	O
called	O
the	O
mean	B
squared	B
error	I
or	O
mse	B
it	O
can	O
also	O
be	O
written	O
as	O
the	O
square	O
of	O
the	O
norm	O
of	O
the	O
vector	O
of	O
maximum	O
likelihood	B
estimation	O
squares	O
sum	B
of	I
squares	I
error	O
contours	O
for	O
linear	B
regression	B
prediction	O
truth	O
w	O
figure	O
in	O
linear	O
least	B
squares	I
we	O
try	O
to	O
minimize	O
the	O
sum	O
of	O
squared	O
distances	O
from	O
each	O
training	O
point	O
by	O
a	O
red	O
circle	O
to	O
its	O
approximation	O
by	O
a	O
blue	O
cross	O
that	O
is	O
we	O
minimize	O
the	O
sum	O
of	O
the	O
lengths	O
of	O
the	O
little	O
vertical	O
blue	O
lines	O
the	O
red	O
diagonal	B
line	O
represents	O
yx	O
which	O
is	O
the	O
least	B
squares	I
regression	B
line	O
note	O
that	O
these	O
residual	B
lines	O
are	O
not	O
perpendicular	O
to	O
the	O
least	B
squares	I
line	O
in	O
contrast	O
to	O
figure	O
figure	O
generated	O
by	O
residualsdemo	O
contours	O
of	O
the	O
rss	O
error	O
surface	O
for	O
the	O
same	O
example	O
the	O
red	O
cross	O
represents	O
the	O
mle	B
w	O
figure	O
generated	O
by	O
contoursssedemo	O
residual	B
errors	O
rssw	O
i	O
where	O
wt	O
xi	O
we	O
see	O
that	O
the	O
mle	B
for	O
w	O
is	O
the	O
one	O
that	O
minimizes	O
the	O
rss	O
so	O
this	O
method	O
is	O
known	O
as	O
least	B
squares	I
this	O
method	O
is	O
illustrated	O
in	O
figure	O
the	O
training	O
data	O
yi	O
are	O
shown	O
as	O
red	O
circles	O
the	O
estimated	O
values	O
yi	O
are	O
shown	O
as	O
blue	O
crosses	O
and	O
the	O
residuals	O
yi	O
yi	O
are	O
shown	O
as	O
vertical	O
blue	O
lines	O
the	O
goal	O
is	O
to	O
find	O
the	O
setting	O
of	O
the	O
parameters	O
slope	O
and	O
intercept	O
such	O
that	O
the	O
resulting	O
red	O
line	O
minimizes	O
the	O
sum	O
of	O
squared	O
residuals	O
lengths	O
of	O
the	O
vertical	O
blue	O
lines	O
in	O
figure	O
we	O
plot	O
the	O
nll	B
surface	O
for	O
our	O
linear	B
regression	B
example	O
we	O
see	O
that	O
it	O
is	O
a	O
quadratic	O
bowl	O
with	O
a	O
unique	O
minimum	O
which	O
we	O
now	O
derive	O
this	O
is	O
true	O
even	O
if	O
we	O
use	O
basis	B
function	I
expansion	I
such	O
as	O
polynomials	O
because	O
the	O
nll	B
is	O
still	O
linear	O
in	O
the	O
parameters	O
w	O
even	O
if	O
it	O
is	O
not	O
linear	O
in	O
the	O
inputs	O
x	O
derivation	O
of	O
the	O
mle	B
first	O
we	O
rewrite	O
the	O
objective	O
in	O
a	O
form	O
that	O
is	O
more	O
amenable	O
to	O
differentiation	O
nllw	O
xwt	O
xw	O
wt	O
xw	O
wt	O
y	O
where	O
xt	O
x	O
xixt	O
i	O
id	O
is	O
the	O
sum	B
of	I
squares	I
matrix	O
and	O
xt	O
y	O
xiyi	O
using	O
results	O
from	O
equation	O
we	O
see	O
that	O
the	O
gradient	O
of	O
this	O
is	O
given	O
by	O
gw	O
xw	O
xt	O
y	O
xiwt	O
xi	O
yi	O
equating	O
to	O
zero	O
we	O
get	O
xt	O
xw	O
xt	O
y	O
chapter	O
linear	B
regression	B
this	O
is	O
known	O
as	O
the	O
normal	B
equation	I
the	O
corresponding	O
solution	O
w	O
to	O
this	O
linear	O
system	O
of	O
equations	O
is	O
called	O
the	O
ordinary	B
least	B
squares	I
or	O
ols	B
solution	O
which	O
is	O
given	O
by	O
wols	O
x	O
y	O
geometric	O
interpretation	O
this	O
equation	O
has	O
an	O
elegant	O
geometrical	O
intrepretation	O
as	O
we	O
now	O
explain	O
we	O
assume	O
n	O
d	O
so	O
we	O
have	O
more	O
examples	O
than	O
features	B
the	O
columns	O
of	O
x	O
define	O
a	O
linear	O
subspace	O
of	O
dimensionality	O
d	O
which	O
is	O
embedded	O
in	O
n	O
dimensions	O
let	O
the	O
j	O
th	O
column	O
be	O
xj	O
which	O
is	O
d	O
which	O
represents	O
the	O
i	O
th	O
data	O
a	O
vector	O
in	O
r	O
n	O
for	O
example	O
suppose	O
we	O
have	O
n	O
examples	O
in	O
d	O
case	O
similarly	O
y	O
is	O
a	O
vector	O
in	O
r	O
dimensions	O
should	O
not	O
be	O
confused	O
with	O
xi	O
r	O
y	O
x	O
n	O
these	O
vectors	O
are	O
illustrated	O
in	O
figure	O
we	O
seek	O
a	O
vector	O
y	O
r	O
n	O
that	O
lies	O
in	O
this	O
linear	O
subspace	O
and	O
is	O
as	O
close	O
as	O
possible	O
to	O
y	O
i	O
e	O
we	O
want	O
to	O
find	O
argmin	O
y	O
span	O
xd	O
since	O
y	O
spanx	O
there	O
exists	O
some	O
weight	B
vector	I
w	O
such	O
that	O
y	O
wd	O
xd	O
xw	O
maximum	O
likelihood	B
estimation	O
squares	O
x	O
y	O
y	O
figure	O
graphical	O
interpretation	O
of	O
least	B
squares	I
for	O
n	O
examples	O
and	O
d	O
features	B
and	O
but	O
does	O
not	O
lie	O
on	O
this	O
are	O
vectors	O
in	O
r	O
plane	O
the	O
orthogonal	B
projection	B
of	O
y	O
onto	O
this	O
plane	O
is	O
denoted	O
y	O
the	O
red	O
line	O
from	O
y	O
to	O
y	O
is	O
the	O
residual	B
whose	O
norm	O
we	O
want	O
to	O
minimize	O
for	O
visual	O
clarity	O
all	O
vectors	O
have	O
been	O
converted	O
to	O
unit	O
norm	O
figure	O
generated	O
by	O
leastsquaresprojection	O
together	O
they	O
define	O
a	O
plane	O
y	O
is	O
also	O
a	O
vector	O
in	O
r	O
to	O
minimize	O
the	O
norm	O
of	O
the	O
residual	B
y	O
y	O
we	O
want	O
the	O
residual	B
vector	O
to	O
be	O
orthogonal	O
to	O
every	O
column	O
of	O
x	O
so	O
xt	O
j	O
y	O
for	O
j	O
hence	O
j	O
y	O
xt	O
xw	O
w	O
x	O
xt	O
hence	O
our	O
projected	O
value	O
of	O
y	O
is	O
given	O
by	O
y	O
x	O
w	O
xxt	O
x	O
y	O
y	O
this	O
corresponds	O
to	O
an	O
orthogonal	B
projection	B
of	O
y	O
onto	O
the	O
column	O
space	O
of	O
x	O
the	O
projection	B
matrix	O
p	O
xxt	O
x	O
is	O
called	O
the	O
hat	B
matrix	I
since	O
it	O
puts	O
the	O
hat	O
on	O
y	O
convexity	O
when	O
discussing	O
least	B
squares	I
we	O
noted	O
that	O
the	O
nll	B
had	O
a	O
bowl	O
shape	O
with	O
a	O
unique	O
minimum	O
the	O
technical	O
term	O
for	O
functions	O
like	O
this	O
is	O
convex	B
convex	B
functions	O
play	O
a	O
very	O
important	O
role	O
in	O
machine	B
learning	B
let	O
us	O
define	O
this	O
concept	B
more	O
precisely	O
we	O
say	O
a	O
set	O
s	O
is	O
convex	B
if	O
for	O
any	O
s	O
we	O
have	O
s	O
chapter	O
linear	B
regression	B
figure	O
illustration	O
of	O
a	O
convex	B
set	O
illustration	O
of	O
a	O
nonconvex	O
set	O
x	O
y	O
a	O
b	O
illustration	O
of	O
a	O
convex	B
function	O
we	O
see	O
that	O
the	O
chord	O
joining	O
f	O
to	O
f	O
lies	O
figure	O
above	O
the	O
function	O
a	O
function	O
that	O
is	O
neither	O
convex	B
nor	O
concave	B
a	O
is	O
a	O
local	O
minimum	O
b	O
is	O
a	O
global	B
minimum	I
figure	O
generated	O
by	O
convexfnhand	O
that	O
is	O
if	O
we	O
draw	O
a	O
line	O
from	O
to	O
for	O
an	O
illustration	O
of	O
a	O
convex	B
set	O
and	O
figure	O
for	O
an	O
illustration	O
of	O
a	O
non-convex	O
set	O
all	O
points	O
on	O
the	O
line	O
lie	O
inside	O
the	O
set	O
see	O
figure	O
a	O
function	O
f	O
is	O
called	O
convex	B
if	O
its	O
epigraph	B
set	O
of	O
points	O
above	O
the	O
function	O
defines	O
a	O
convex	B
set	O
equivalently	O
a	O
function	O
f	O
is	O
called	O
convex	B
if	O
it	O
is	O
defined	O
on	O
a	O
convex	B
set	O
and	O
if	O
for	O
any	O
s	O
and	O
for	O
any	O
we	O
have	O
f	O
f	O
see	O
figure	O
for	O
a	O
example	O
a	O
function	O
is	O
called	O
strictly	B
convex	B
if	O
the	O
inequality	O
is	O
strict	B
a	O
function	O
f	O
is	O
concave	B
if	O
f	O
is	O
convex	B
examples	O
of	O
scalar	O
convex	B
functions	O
include	O
e	O
and	O
log	O
examples	O
of	O
scalar	O
concave	B
functions	O
include	O
log	O
and	O
intuitively	O
a	O
convex	B
function	O
has	O
a	O
bowl	O
shape	O
and	O
hence	O
has	O
a	O
unique	O
global	B
minimum	I
corresponding	O
to	O
the	O
bottom	O
of	O
the	O
bowl	O
hence	O
its	O
second	O
derivative	O
must	O
be	O
positive	O
everywhere	O
d	O
d	O
f	O
a	O
twice-continuously	O
differentiable	O
multivariate	O
function	O
f	O
is	O
convex	B
iff	B
its	O
hessian	B
is	O
positive	O
definite	O
for	O
all	O
in	O
the	O
machine	B
learning	B
context	O
the	O
function	O
f	O
often	O
corresponds	O
to	O
the	O
nll	B
recall	B
that	O
the	O
hessian	B
is	O
the	O
matrix	O
of	O
second	O
partial	O
derivatives	O
defined	O
by	O
hjk	O
matrix	O
h	O
is	O
positive	O
definite	O
iff	B
vt	O
hv	O
for	O
any	O
non-zero	O
vector	O
v	O
f	O
j	O
k	O
also	O
recall	B
that	O
a	O
huber	O
robust	B
linear	B
regression	B
linear	O
data	O
with	O
noise	O
and	O
outliers	B
least	B
squares	I
laplace	B
figure	O
illustration	O
of	O
robust	B
linear	B
regression	B
figure	O
generated	O
by	O
linregrobustdemocombined	O
illustration	O
of	O
and	O
huber	B
loss	B
functions	O
figure	O
generated	O
by	O
huberlossdemo	O
models	O
where	O
the	O
nll	B
is	O
convex	B
are	O
desirable	O
since	O
this	O
means	O
we	O
can	O
always	O
find	O
the	O
globally	O
optimal	O
mle	B
we	O
will	O
see	O
many	O
examples	O
of	O
this	O
later	O
in	O
the	O
book	O
however	O
many	O
models	O
of	O
interest	O
will	O
not	O
have	O
concave	B
likelihoods	O
in	O
such	O
cases	O
we	O
will	O
discuss	O
ways	O
to	O
derive	O
locally	O
optimal	O
parameter	B
estimates	O
robust	B
linear	B
regression	B
it	O
is	O
very	O
common	O
to	O
model	O
the	O
noise	O
in	O
regression	B
models	O
using	O
a	O
gaussian	B
distribution	O
with	O
zero	O
mean	B
and	O
constant	O
variance	B
n	O
where	O
yi	O
wt	O
xi	O
in	O
this	O
case	O
maximizing	O
likelihood	B
is	O
equivalent	O
to	O
minimizing	O
the	O
sum	O
of	O
squared	O
residuals	O
as	O
we	O
have	O
seen	O
however	O
if	O
we	O
have	O
outliers	B
in	O
our	O
data	O
this	O
can	O
result	O
in	O
a	O
poor	O
fit	O
as	O
illustrated	O
in	O
figure	O
outliers	B
are	O
the	O
points	O
on	O
the	O
bottom	O
of	O
the	O
figure	O
this	O
is	O
because	O
squared	B
error	I
penalizes	O
deviations	O
quadratically	O
so	O
points	O
far	O
from	O
the	O
line	O
have	O
more	O
affect	O
on	O
the	O
fit	O
than	O
points	O
near	O
to	O
the	O
line	O
one	O
way	O
to	O
achieve	O
robustness	B
to	O
outliers	B
is	O
to	O
replace	O
the	O
gaussian	B
distribution	O
for	O
the	O
response	B
variable	I
with	O
a	O
distribution	O
that	O
has	O
heavy	B
tails	I
such	O
a	O
distribution	O
will	O
assign	O
higher	O
likelihood	B
to	O
outliers	B
without	O
having	O
to	O
perturb	O
the	O
straight	O
line	O
to	O
explain	O
them	O
one	O
possibility	O
is	O
to	O
use	O
the	O
laplace	B
distribution	I
introduced	O
in	O
section	O
if	O
we	O
use	O
this	O
as	O
our	O
observation	B
model	I
for	O
regression	B
we	O
get	O
the	O
following	O
likelihood	B
pyx	O
w	O
b	O
lapywt	O
x	O
b	O
exp	O
b	O
the	O
robustness	B
arises	O
from	O
the	O
use	O
of	O
wt	O
x	O
instead	O
of	O
wt	O
for	O
simplicity	O
we	O
will	O
assume	O
b	O
is	O
fixed	O
let	O
ri	O
yi	O
wt	O
xi	O
be	O
the	O
i	O
th	O
residual	B
the	O
nll	B
has	O
the	O
form	O
wt	O
x	O
i	O
chapter	O
linear	B
regression	B
likelihood	B
gaussian	B
gaussian	B
gaussian	B
laplace	B
student	O
name	O
prior	O
uniform	O
least	B
squares	I
gaussian	B
laplace	B
uniform	O
robust	B
regression	B
uniform	O
robust	B
regression	B
ridge	O
lasso	B
section	O
exercise	O
table	O
summary	O
of	O
various	O
likelihoods	O
and	O
priors	O
used	O
for	O
linear	B
regression	B
the	O
likelihood	B
refers	O
to	O
the	O
distributional	O
form	O
of	O
pyx	O
w	O
and	O
the	O
prior	O
refers	O
to	O
the	O
distributional	O
form	O
of	O
pw	O
map	O
estimation	O
with	O
a	O
uniform	B
distribution	I
corresponds	O
to	O
mle	B
unfortunately	O
this	O
is	O
a	O
non-linear	O
objective	O
function	O
which	O
is	O
hard	O
to	O
optimize	O
fortunately	O
we	O
can	O
convert	O
the	O
nll	B
to	O
a	O
linear	O
objective	O
subject	O
to	O
linear	O
constraints	O
using	O
the	O
following	O
split	B
variable	I
trick	O
first	O
we	O
define	O
and	O
then	O
we	O
impose	O
the	O
linear	O
inequality	O
constraints	O
that	O
r	O
constrained	O
objective	O
becomes	O
i	O
r	O
i	O
s	O
t	O
i	O
r	O
r	O
i	O
wt	O
xi	O
r	O
i	O
r	O
i	O
yi	O
i	O
and	O
r	O
i	O
now	O
the	O
ri	O
r	O
i	O
r	O
i	O
min	O
wrr	O
i	O
this	O
is	O
an	O
example	O
of	O
a	O
linear	B
program	I
with	O
d	O
unknowns	O
and	O
constraints	O
since	O
this	O
is	O
a	O
convex	B
optimization	B
problem	O
it	O
has	O
a	O
unique	O
solution	O
to	O
solve	O
an	O
lp	O
we	O
must	O
first	O
write	O
it	O
in	O
standard	O
form	O
which	O
as	O
follows	O
min	O
f	O
t	O
s	O
t	O
a	O
b	O
aeq	O
beq	O
l	O
u	O
f	O
a	O
b	O
aeq	O
i	O
i	O
in	O
our	O
current	O
example	O
r	O
r	O
beq	O
y	O
l	O
u	O
this	O
can	O
be	O
solved	O
by	O
any	O
lp	O
solver	O
e	O
g	O
and	O
vandenberghe	O
see	O
figure	O
for	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
an	O
alternative	O
to	O
using	O
nll	B
under	O
a	O
laplace	B
likelihood	B
is	O
to	O
minimize	O
the	O
huber	B
loss	B
function	I
defined	O
as	O
follows	O
if	O
if	O
lh	O
this	O
is	O
equivalent	O
to	O
for	O
errors	O
that	O
are	O
smaller	O
than	O
and	O
is	O
equivalent	O
to	O
for	O
larger	O
errors	O
see	O
figure	O
the	O
advantage	O
of	O
this	O
loss	B
function	I
is	O
that	O
it	O
is	O
everywhere	O
differentiable	O
drr	O
signr	O
if	O
r	O
we	O
can	O
also	O
check	O
that	O
the	O
function	O
is	O
using	O
the	O
fact	O
that	O
d	O
continuous	O
since	O
the	O
gradients	O
of	O
the	O
two	O
parts	O
of	O
the	O
function	O
match	O
at	O
r	O
namely	O
dr	O
lh	O
consequently	O
optimizing	O
the	O
huber	B
loss	B
is	O
much	O
faster	O
than	O
using	O
the	O
laplace	B
likelihood	B
since	O
we	O
can	O
use	O
standard	O
smooth	O
optimization	B
methods	O
as	O
quasinewton	O
instead	O
of	O
linear	O
programming	O
d	O
figure	O
gives	O
an	O
illustration	O
of	O
the	O
huber	B
loss	B
function	I
the	O
results	O
are	O
qualitatively	O
fact	O
it	O
turns	O
out	O
that	O
the	O
huber	O
method	O
also	O
has	O
a	O
similiar	O
to	O
the	O
probabilistic	O
methods	O
probabilistic	O
interpretation	O
although	O
it	O
is	O
rather	O
unnatural	O
et	O
al	O
ridge	B
regression	B
ln	O
lambda	O
ln	O
lambda	O
figure	O
degree	B
polynomial	O
fit	O
to	O
n	O
data	O
points	O
with	O
increasing	O
amounts	O
of	O
regularization	B
data	O
was	O
generated	O
from	O
noise	O
with	O
variance	B
the	O
error	O
bars	O
representing	O
the	O
noise	O
variance	B
get	O
wider	O
as	O
the	O
fit	O
gets	O
smoother	O
since	O
we	O
are	O
ascribing	O
more	O
of	O
the	O
data	O
variation	O
to	O
the	O
noise	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
ridge	B
regression	B
one	O
problem	O
with	O
ml	O
estimation	O
is	O
that	O
it	O
can	O
result	O
in	O
overfitting	O
in	O
this	O
section	O
we	O
discuss	O
a	O
way	O
to	O
ameliorate	O
this	O
problem	O
by	O
using	O
map	O
estimation	O
with	O
a	O
gaussian	B
prior	O
for	O
simplicity	O
we	O
assume	O
a	O
gaussian	B
likelihood	B
rather	O
than	O
a	O
robust	B
likelihood	B
basic	O
idea	O
the	O
reason	O
that	O
the	O
mle	B
can	O
overfit	O
is	O
that	O
it	O
is	O
picking	O
the	O
parameter	B
values	O
that	O
are	O
the	O
best	O
for	O
modeling	O
the	O
training	O
data	O
but	O
if	O
the	O
data	O
is	O
noisy	O
such	O
parameters	O
often	O
result	O
in	O
complex	O
functions	O
as	O
a	O
simple	O
example	O
suppose	O
we	O
fit	O
a	O
degree	B
polynomial	O
to	O
n	O
data	O
points	O
using	O
least	B
squares	I
the	O
resulting	O
curve	O
is	O
very	O
wiggly	O
as	O
shown	O
in	O
figure	O
the	O
corresponding	O
least	B
squares	I
coefficients	O
are	O
as	O
follows	O
we	O
see	O
that	O
there	O
are	O
many	O
large	O
positive	O
and	O
negative	O
numbers	O
these	O
balance	O
out	O
exactly	O
to	O
make	O
the	O
curve	O
wiggle	O
in	O
just	O
the	O
right	O
way	O
so	O
that	O
it	O
almost	O
perfectly	O
interpolates	O
the	O
data	O
but	O
this	O
situation	O
is	O
unstable	B
if	O
we	O
changed	O
the	O
data	O
a	O
little	O
the	O
coefficients	O
would	O
change	O
a	O
lot	O
we	O
can	O
encourage	O
the	O
parameters	O
to	O
be	O
small	O
thus	O
resulting	O
in	O
a	O
smoother	O
curve	O
by	O
using	O
a	O
zero-mean	O
gaussian	B
prior	O
n	O
pw	O
j	O
argmax	O
w	O
where	O
controls	O
the	O
strength	O
of	O
the	O
prior	O
the	O
corresponding	O
map	O
estimation	O
problem	O
becomes	O
log	O
n	O
wt	O
xi	O
log	O
n	O
chapter	O
linear	B
regression	B
negative	O
log	O
marg	O
likelihood	B
cv	B
estimate	O
of	O
mse	B
mean	B
squared	B
error	I
train	O
mse	B
test	O
mse	B
log	O
lambda	O
log	O
lambda	O
figure	O
training	O
error	O
blue	O
and	O
test	O
error	O
red	O
for	O
a	O
degree	B
polynomial	O
fit	O
by	O
ridge	B
regression	B
plotted	O
vs	O
log	O
data	O
was	O
generated	O
from	O
noise	O
with	O
variance	B
set	O
has	O
size	O
n	O
note	O
models	O
are	O
ordered	O
from	O
complex	O
regularizer	O
on	O
the	O
left	O
to	O
simple	O
regularizer	O
on	O
the	O
right	O
the	O
stars	O
correspond	O
to	O
the	O
values	O
used	O
to	O
plot	O
the	O
functions	O
in	O
figure	O
estimate	O
of	O
performance	O
using	O
training	B
set	I
dotted	O
blue	O
cross-validation	O
estimate	O
of	O
future	O
mse	B
solid	O
black	O
negative	O
log	O
marginal	B
likelihood	B
log	O
pd	O
both	O
curves	O
have	O
been	O
vertically	O
rescaled	O
to	O
to	O
make	O
them	O
comparable	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
it	O
is	O
a	O
simple	O
exercise	O
to	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
minimizing	O
the	O
following	O
jw	O
n	O
wt	O
where	O
and	O
j	O
wt	O
w	O
is	O
the	O
squared	O
two-norm	O
here	O
the	O
first	O
term	O
is	O
the	O
mse	B
nll	B
as	O
usual	O
and	O
the	O
second	O
term	O
is	O
a	O
complexity	O
penalty	O
the	O
corresponding	O
solution	O
is	O
given	O
by	O
j	O
wridge	O
id	O
xt	O
x	O
y	O
this	O
technique	O
is	O
known	O
as	O
ridge	B
regression	B
or	O
penalized	B
least	B
squares	I
in	O
general	O
adding	O
a	O
gaussian	B
prior	O
to	O
the	O
parameters	O
of	O
a	O
model	O
to	O
encourage	O
them	O
to	O
be	O
small	O
is	O
called	O
regularization	B
or	O
weight	B
decay	I
note	O
that	O
the	O
offset	O
term	O
is	O
not	O
regularized	O
since	O
this	O
just	O
affects	O
the	O
height	O
of	O
the	O
function	O
not	O
its	O
complexity	O
by	O
penalizing	O
the	O
sum	O
of	O
the	O
magnitudes	O
of	O
the	O
weights	O
we	O
ensure	O
the	O
function	O
is	O
simple	O
w	O
corresponds	O
to	O
a	O
straight	O
line	O
which	O
is	O
the	O
simplest	O
possible	O
function	O
corresponding	O
to	O
a	O
constant	O
we	O
illustrate	O
this	O
idea	O
in	O
figure	O
where	O
we	O
see	O
that	O
increasing	O
results	O
in	O
smoother	O
we	O
functions	O
the	O
resulting	O
coefficients	O
also	O
become	O
smaller	O
for	O
example	O
using	O
have	O
ridge	B
regression	B
in	O
figure	O
we	O
plot	O
the	O
mse	B
on	O
the	O
training	O
and	O
test	O
sets	O
vs	O
log	O
we	O
see	O
that	O
as	O
we	O
increase	O
the	O
model	O
becomes	O
more	O
constrained	O
the	O
error	O
on	O
the	O
training	B
set	I
increases	O
for	O
the	O
test	O
set	O
we	O
see	O
the	O
characteristic	O
u-shaped	B
curve	I
where	O
the	O
model	O
overfits	O
and	O
then	O
underfits	O
in	O
section	O
we	O
will	O
discuss	O
a	O
more	O
probabilistic	O
approach	O
it	O
is	O
common	O
to	O
use	O
cross	B
validation	I
to	O
pick	O
as	O
shown	O
in	O
figure	O
we	O
will	O
consider	O
a	O
variety	O
of	O
different	O
priors	O
in	O
this	O
book	O
each	O
of	O
these	O
corresponds	O
to	O
a	O
different	O
form	O
of	O
regularization	B
this	O
technique	O
is	O
very	O
widely	O
used	O
to	O
prevent	O
overfitting	O
numerically	O
stable	B
computation	O
interestingly	O
ridge	B
regression	B
which	O
works	O
better	O
statistically	O
is	O
also	O
easier	O
to	O
fit	O
numerically	O
since	O
id	O
xt	O
x	O
is	O
much	O
better	O
conditioned	O
hence	O
more	O
likely	O
to	O
be	O
invertible	O
than	O
xt	O
x	O
at	O
least	O
for	O
suitable	O
largy	O
nevertheless	O
inverting	O
matrices	O
is	O
still	O
best	O
avoided	O
for	O
reasons	O
of	O
numerical	O
stability	O
if	O
you	O
write	O
winvx	O
xx	O
in	O
matlab	O
it	O
will	O
give	O
you	O
a	O
warning	O
we	O
now	O
describe	O
a	O
useful	O
trick	O
for	O
fitting	O
ridge	B
regression	B
models	O
hence	O
by	O
extension	B
computing	O
vanilla	O
ols	B
estimates	O
that	O
is	O
more	O
numerically	O
robust	B
we	O
assume	O
the	O
prior	O
has	O
the	O
form	O
pw	O
n	O
where	O
is	O
the	O
precision	B
matrix	I
in	O
the	O
case	O
of	O
ridge	B
regression	B
to	O
avoid	O
penalizing	O
the	O
term	O
we	O
should	O
center	O
the	O
data	O
first	O
as	O
explained	O
in	O
exercise	O
first	O
let	O
us	O
augment	O
the	O
original	O
data	O
with	O
some	O
virtual	O
data	O
coming	O
from	O
the	O
prior	O
x	O
x	O
y	O
y	O
where	O
where	O
the	O
extra	O
rows	O
represent	O
pseudo-data	O
from	O
the	O
prior	O
t	O
is	O
a	O
cholesky	B
decomposition	I
of	O
we	O
see	O
that	O
x	O
is	O
d	O
d	O
we	O
now	O
show	O
that	O
the	O
nll	B
on	O
this	O
expanded	O
data	O
is	O
equivalent	O
to	O
penalized	O
nll	B
on	O
the	O
original	O
data	O
f	O
y	O
xwt	O
y	O
xw	O
x	O
y	O
xw	O
w	O
w	O
xw	O
w	O
xwt	O
xw	O
wt	O
xwt	O
xw	O
t	O
w	O
w	O
x	O
w	O
y	O
hence	O
the	O
map	B
estimate	I
is	O
given	O
by	O
wridge	O
xt	O
x	O
xt	O
y	O
as	O
we	O
claimed	O
now	O
let	O
x	O
qr	O
chapter	O
linear	B
regression	B
be	O
the	O
qr	B
decomposition	I
of	O
x	O
where	O
q	O
is	O
orthonormal	O
qt	O
q	O
qqt	O
i	O
and	O
r	O
is	O
upper	O
triangular	O
then	O
xt	O
x	O
qt	O
qr	O
r	O
r	O
t	O
hence	O
wridge	O
r	O
t	O
rt	O
qt	O
y	O
r	O
y	O
note	O
that	O
r	O
is	O
easy	O
to	O
invert	O
since	O
it	O
is	O
upper	O
triangular	O
this	O
gives	O
us	O
a	O
way	O
to	O
compute	O
the	O
ridge	O
estimate	O
while	O
avoiding	O
having	O
to	O
invert	O
xt	O
x	O
we	O
can	O
use	O
this	O
technique	O
to	O
find	O
the	O
mle	B
by	O
simply	O
computing	O
the	O
qr	B
decomposition	I
of	O
the	O
unaugmented	O
matrix	O
x	O
and	O
using	O
the	O
original	O
y	O
this	O
is	O
the	O
method	O
of	O
choice	O
for	O
solving	O
least	B
squares	I
problems	O
fact	O
it	O
is	O
so	O
sommon	O
that	O
it	O
can	O
be	O
implemented	O
in	O
one	O
line	O
of	O
matlab	O
using	O
the	O
backslash	B
operator	I
wxy	O
note	O
that	O
computing	O
the	O
qr	B
decomposition	I
of	O
an	O
n	O
d	O
matrix	O
takes	O
on	O
time	O
and	O
is	O
numerically	O
very	O
stable	B
if	O
d	O
n	O
we	O
should	O
first	O
perform	O
an	O
svd	B
decomposition	O
in	O
particular	O
let	O
x	O
usvt	O
be	O
the	O
svd	B
of	O
x	O
where	O
vt	O
v	O
in	O
uut	O
ut	O
u	O
in	O
and	O
s	O
is	O
a	O
diagonal	B
n	O
n	O
matrix	O
now	O
let	O
z	O
ud	O
be	O
an	O
n	O
n	O
matrix	O
then	O
we	O
can	O
rewrite	O
the	O
ridge	O
estimate	O
thus	O
wridge	O
vzt	O
z	O
in	O
y	O
in	O
other	O
words	O
we	O
can	O
replace	O
the	O
d-dimensional	O
vectors	O
xi	O
with	O
the	O
n	O
vectors	O
zi	O
and	O
perform	O
our	O
penalized	O
fit	O
as	O
before	O
we	O
then	O
transform	O
the	O
n	O
solution	O
to	O
the	O
d-dimensional	O
solution	O
by	O
multiplying	O
by	O
v	O
geometrically	O
we	O
are	O
rotating	O
to	O
a	O
new	O
coordinate	O
system	O
in	O
which	O
all	O
but	O
the	O
first	O
n	O
coordinates	O
are	O
zero	O
this	O
does	O
not	O
affect	O
the	O
solution	O
since	O
the	O
spherical	B
gaussian	B
prior	O
is	O
rotationally	O
invariant	B
the	O
overall	O
time	O
is	O
now	O
odn	O
operations	O
connection	O
with	O
pca	B
in	O
this	O
section	O
we	O
discuss	O
an	O
interesting	O
connection	O
between	O
ridge	B
regression	B
and	O
pca	B
which	O
gives	O
further	O
insight	O
into	O
why	O
ridge	B
regression	B
works	O
well	O
our	O
discussion	O
is	O
based	O
on	O
et	O
al	O
let	O
x	O
usvt	O
be	O
the	O
svd	B
of	O
x	O
from	O
equation	O
we	O
have	O
wridge	O
i	O
y	O
hence	O
the	O
ridge	O
predictions	O
on	O
the	O
training	B
set	I
are	O
given	O
by	O
y	O
x	O
wridge	O
usvt	O
i	O
y	O
u	O
sut	O
y	O
uj	O
sjjut	O
j	O
y	O
ridge	B
regression	B
ml	O
estimate	O
map	B
estimate	I
prior	O
mean	B
figure	O
geometry	O
of	O
ridge	B
regression	B
the	O
likelihood	B
is	O
shown	O
as	O
an	O
ellipse	O
and	O
the	O
prior	O
is	O
shown	O
as	O
a	O
circle	O
centered	O
on	O
the	O
origin	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
geomridge	O
where	O
sjj	O
i	O
j	O
j	O
and	O
j	O
are	O
the	O
singular	B
values	I
of	O
x	O
hence	O
y	O
x	O
wridge	O
uj	O
j	O
j	O
ut	O
j	O
y	O
in	O
contrast	O
the	O
least	B
squares	I
prediction	O
is	O
y	O
x	O
wls	O
y	O
uut	O
y	O
ujut	O
j	O
y	O
j	O
is	O
small	O
compared	O
to	O
then	O
direction	O
uj	O
will	O
not	O
have	O
much	O
effect	O
on	O
the	O
prediction	O
in	O
if	O
view	O
of	O
this	O
we	O
define	O
the	O
effective	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
model	O
as	O
follows	O
dof	O
j	O
j	O
let	O
us	O
try	O
to	O
understand	O
why	O
this	O
behavior	O
is	O
desirable	O
when	O
dof	O
and	O
as	O
dof	O
in	O
section	O
we	O
show	O
that	O
cov	O
x	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
for	O
w	O
thus	O
the	O
directions	O
in	O
which	O
we	O
are	O
most	O
uncertain	O
about	O
w	O
are	O
determined	O
by	O
the	O
eigenvectors	O
of	O
this	O
matrix	O
with	O
the	O
smallest	O
eigenvalues	O
as	O
shown	O
in	O
figure	O
furthermore	O
in	O
section	O
we	O
show	O
that	O
the	O
squared	O
singular	B
values	I
j	O
are	O
equal	O
to	O
the	O
eigenvalues	O
of	O
xt	O
x	O
hence	O
small	O
singular	B
values	I
j	O
correspond	O
to	O
directions	O
with	O
high	O
posterior	O
variance	B
it	O
is	O
these	O
directions	O
which	O
ridge	O
shrinks	O
the	O
most	O
chapter	O
linear	B
regression	B
this	O
process	O
is	O
illustrated	O
in	O
figure	O
the	O
horizontal	O
parameter	B
is	O
not-well	O
determined	O
by	O
the	O
data	O
high	O
posterior	O
variance	B
but	O
the	O
vertical	O
parameter	B
is	O
well-determined	O
hence	O
wmap	O
is	O
shifted	O
strongly	O
towards	O
the	O
prior	O
mean	B
which	O
is	O
to	O
figure	O
which	O
illustrated	O
sensor	B
fusion	I
with	O
sensors	O
of	O
different	O
reliabilities	O
in	O
this	O
way	O
ill-determined	O
parameters	O
are	O
reduced	O
in	O
size	O
towards	O
this	O
is	O
called	O
shrinkage	B
is	O
close	O
to	O
wmle	O
but	O
wmap	O
there	O
is	O
a	O
related	O
but	O
different	O
technique	O
called	O
principal	B
components	I
regression	B
the	O
idea	O
is	O
this	O
first	O
use	O
pca	B
to	O
reduce	O
the	O
dimensionality	O
to	O
k	O
dimensions	O
and	O
then	O
use	O
these	O
low	O
dimensional	O
features	B
as	O
input	O
to	O
regression	B
however	O
this	O
technique	O
does	O
not	O
work	O
as	O
well	O
as	O
ridge	O
in	O
terms	O
of	O
predictive	B
accuracy	O
et	O
al	O
the	O
reason	O
is	O
that	O
in	O
pc	O
regression	B
only	O
the	O
first	O
k	O
dimensions	O
are	O
retained	O
and	O
the	O
remaining	O
d	O
k	O
dimensions	O
are	O
entirely	O
ignored	O
by	O
contrast	O
ridge	B
regression	B
uses	O
a	O
soft	O
weighting	O
of	O
all	O
the	O
dimensions	O
regularization	B
effects	O
of	O
big	B
data	I
regularization	B
is	O
the	O
most	O
common	O
way	O
to	O
avoid	O
overfitting	O
however	O
another	O
effective	O
approach	O
which	O
is	O
not	O
always	O
available	O
is	O
to	O
use	O
lots	O
of	O
data	O
it	O
should	O
be	O
intuitively	O
obvious	O
that	O
the	O
more	O
training	O
data	O
we	O
have	O
the	O
better	O
we	O
will	O
be	O
able	O
to	O
so	O
we	O
expect	O
the	O
test	O
set	O
error	O
to	O
decrease	O
to	O
some	O
plateau	O
as	O
n	O
increases	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
plot	O
the	O
mean	B
squared	B
error	I
incurred	O
on	O
the	O
test	O
set	O
achieved	O
by	O
polynomial	B
regression	B
models	O
of	O
different	O
degrees	O
vs	O
n	O
plot	O
of	O
error	O
vs	O
training	B
set	I
size	O
is	O
known	O
as	O
a	O
learning	B
curve	I
the	O
level	O
of	O
the	O
plateau	O
for	O
the	O
test	O
error	O
consists	O
of	O
two	O
terms	O
an	O
irreducible	B
component	O
that	O
all	O
models	O
incur	O
due	O
to	O
the	O
intrinsic	O
variability	O
of	O
the	O
generating	O
process	O
is	O
called	O
the	O
noise	O
floor	O
and	O
a	O
component	O
that	O
depends	O
on	O
the	O
discrepancy	O
between	O
the	O
generating	O
process	O
truth	O
and	O
the	O
model	O
this	O
is	O
called	O
structural	B
error	I
in	O
figure	O
the	O
truth	O
is	O
a	O
degree	B
polynomial	O
and	O
we	O
try	O
fitting	O
polynomials	O
of	O
degrees	O
and	O
to	O
this	O
data	O
call	O
the	O
models	O
and	O
we	O
see	O
that	O
the	O
structural	B
error	I
for	O
models	O
and	O
is	O
zero	O
since	O
both	O
are	O
able	O
to	O
capture	O
the	O
true	O
generating	O
process	O
however	O
the	O
structural	B
error	I
for	O
is	O
substantial	O
which	O
is	O
evident	O
from	O
the	O
fact	O
that	O
the	O
plateau	O
occurs	O
high	O
above	O
the	O
noise	O
floor	O
for	O
any	O
model	O
that	O
is	O
expressive	O
enough	O
to	O
capture	O
the	O
truth	O
one	O
with	O
small	O
structural	B
error	I
the	O
test	O
error	O
will	O
go	O
to	O
the	O
noise	O
floor	O
as	O
n	O
however	O
it	O
will	O
typically	O
go	O
to	O
zero	O
faster	O
for	O
simpler	O
models	O
since	O
there	O
are	O
fewer	O
parameters	O
to	O
estimate	O
in	O
particular	O
for	O
finite	O
training	O
sets	O
there	O
will	O
be	O
some	O
discrepancy	O
between	O
the	O
parameters	O
that	O
we	O
estimate	O
and	O
the	O
best	O
parameters	O
that	O
we	O
could	O
estimate	O
given	O
the	O
particular	O
model	O
class	O
this	O
is	O
called	O
approximation	B
error	I
and	O
goes	O
to	O
zero	O
as	O
n	O
but	O
it	O
goes	O
to	O
zero	O
faster	O
for	O
simpler	O
models	O
this	O
is	O
illustrated	O
in	O
figure	O
see	O
also	O
exercise	O
in	O
domains	O
with	O
lots	O
of	O
data	O
simple	O
methods	O
can	O
work	O
surprisingly	O
well	O
et	O
al	O
however	O
there	O
are	O
still	O
reasons	O
to	O
study	O
more	O
sophisticated	O
learning	B
methods	O
because	O
there	O
will	O
always	O
be	O
problems	O
for	O
which	O
we	O
have	O
little	O
data	O
for	O
example	O
even	O
in	O
such	O
a	O
data-rich	O
domain	O
as	O
web	O
search	O
as	O
soon	O
as	O
we	O
want	O
to	O
start	O
personalizing	O
the	O
results	O
the	O
amount	O
of	O
data	O
available	O
for	O
any	O
given	O
user	O
starts	O
to	O
look	O
small	O
again	O
to	O
the	O
complexity	O
of	O
the	O
problem	O
this	O
assumes	O
the	O
training	O
data	O
is	O
randomly	O
sampled	O
and	O
we	O
don	O
t	O
just	O
get	O
repetitions	O
of	O
the	O
same	O
examples	O
having	O
informatively	O
sampled	O
data	O
can	O
help	O
even	O
more	O
this	O
is	O
the	O
motivation	O
for	O
an	O
approach	O
known	O
as	O
active	B
learning	B
where	O
you	O
get	O
to	O
choose	O
your	O
training	O
data	O
truthdegree	O
model	O
degree	B
train	O
test	O
bayesian	B
linear	B
regression	B
e	O
s	O
m	O
truthdegree	O
model	O
degree	B
train	O
test	O
e	O
s	O
m	O
size	O
of	O
training	B
set	I
size	O
of	O
training	B
set	I
truthdegree	O
model	O
degree	B
train	O
test	O
truthdegree	O
model	O
degree	B
train	O
test	O
e	O
s	O
m	O
e	O
s	O
m	O
size	O
of	O
training	B
set	I
size	O
of	O
training	B
set	I
figure	O
mse	B
on	O
training	O
and	O
test	O
sets	O
vs	O
size	O
of	O
training	B
set	I
for	O
data	O
generated	O
from	O
a	O
degree	B
polynomial	O
with	O
gaussian	B
noise	O
of	O
variance	B
we	O
fit	O
polynomial	O
models	O
of	O
varying	O
degree	B
to	O
this	O
data	O
degree	B
degree	B
degree	B
degree	B
note	O
that	O
for	O
small	O
training	B
set	I
sizes	O
the	O
test	O
error	O
of	O
the	O
degree	B
polynomial	O
is	O
higher	O
than	O
that	O
of	O
the	O
degree	B
polynomial	O
due	O
to	O
overfitting	O
but	O
this	O
difference	O
vanishes	O
once	O
we	O
have	O
enough	O
data	O
note	O
also	O
that	O
the	O
degree	B
polynomial	O
is	O
too	O
simple	O
and	O
has	O
high	O
test	O
error	O
even	O
given	O
large	O
amounts	O
of	O
training	O
data	O
figure	O
generated	O
by	O
linregpolyvsn	O
in	O
such	O
cases	O
we	O
may	O
want	O
to	O
learn	O
multiple	O
related	O
models	O
at	O
the	O
same	O
time	O
which	O
is	O
known	O
as	O
multi-task	B
learning	B
this	O
will	O
allow	O
us	O
to	O
borrow	B
statistical	I
strength	I
from	O
tasks	O
with	O
lots	O
of	O
data	O
and	O
to	O
share	O
it	O
with	O
tasks	O
with	O
little	O
data	O
we	O
will	O
discuss	O
ways	O
to	O
do	O
later	O
in	O
the	O
book	O
bayesian	B
linear	B
regression	B
although	O
ridge	B
regression	B
is	O
a	O
useful	O
way	O
to	O
compute	O
a	O
point	B
estimate	I
sometimes	O
we	O
want	O
to	O
compute	O
the	O
full	B
posterior	O
over	O
w	O
and	O
for	O
simplicity	O
we	O
will	O
initially	O
assume	O
the	O
noise	O
variance	B
is	O
known	O
so	O
we	O
focus	O
on	O
computing	O
pwd	O
then	O
in	O
section	O
we	O
consider	O
chapter	O
linear	B
regression	B
the	O
general	O
case	O
where	O
we	O
compute	O
pw	O
we	O
assume	O
throughout	O
a	O
gaussian	B
likelihood	B
model	O
performing	O
bayesian	B
inference	B
with	O
a	O
robust	B
likelihood	B
is	O
also	O
possible	O
but	O
requires	O
more	O
advanced	O
techniques	O
exercise	O
computing	O
the	O
posterior	O
in	O
linear	B
regression	B
the	O
likelihood	B
is	O
given	O
by	O
pyx	O
w	O
xw	O
exp	O
xwt	O
xw	O
where	O
is	O
an	O
offset	O
term	O
if	O
the	O
inputs	O
are	O
centered	O
so	O
i	O
xij	O
for	O
each	O
j	O
the	O
mean	B
of	O
the	O
output	O
is	O
equally	O
likely	O
to	O
be	O
positive	O
or	O
negative	O
so	O
let	O
us	O
put	O
an	O
improper	B
prior	I
on	O
of	O
the	O
form	O
p	O
and	O
then	O
integrate	O
it	O
out	O
to	O
get	O
pyx	O
w	O
exp	O
yi	O
is	O
the	O
empirical	O
mean	B
of	O
the	O
output	O
for	O
notational	O
simplicity	O
we	O
shall	O
where	O
y	O
assume	O
the	O
output	O
has	O
been	O
centered	O
and	O
write	O
y	O
for	O
y	O
n	O
the	O
conjugate	B
prior	I
to	O
the	O
above	O
gaussian	B
likelihood	B
is	O
also	O
a	O
gaussian	B
which	O
we	O
will	O
denote	O
by	O
pw	O
n	O
using	O
bayes	B
rule	I
for	O
gaussians	O
equation	O
the	O
posterior	O
is	O
given	O
by	O
pwx	O
y	O
n	O
n	O
vn	O
vn	O
xt	O
y	O
wn	O
vn	O
v	O
n	O
v	O
v	O
vn	O
xt	O
x	O
xt	O
x	O
if	O
and	O
then	O
the	O
posterior	B
mean	B
reduces	O
to	O
the	O
ridge	O
estimate	O
if	O
we	O
define	O
this	O
is	O
because	O
the	O
mean	B
and	O
mode	B
of	O
a	O
gaussian	B
are	O
the	O
same	O
to	O
gain	O
insight	O
into	O
the	O
posterior	O
distribution	O
not	O
just	O
its	O
mode	B
let	O
us	O
consider	O
a	O
example	O
yx	O
w	O
where	O
the	O
true	O
parameters	O
are	O
and	O
in	O
figure	O
we	O
plot	O
the	O
prior	O
the	O
likelihood	B
the	O
posterior	O
and	O
some	O
samples	B
from	O
the	O
posterior	O
predictive	B
in	O
particular	O
the	O
right	O
hand	O
column	O
plots	O
the	O
function	O
yx	O
ws	O
where	O
x	O
ranges	O
over	O
and	O
ws	O
n	O
vn	O
is	O
a	O
sample	O
from	O
the	O
parameter	B
posterior	O
initially	O
when	O
we	O
sample	O
from	O
the	O
prior	O
row	O
our	O
predictions	O
are	O
all	O
over	O
the	O
place	O
since	O
our	O
prior	O
is	O
uniform	O
after	O
we	O
see	O
one	O
data	O
point	O
row	O
our	O
posterior	O
becomes	O
constrained	O
by	O
the	O
corresponding	O
likelihood	B
and	O
our	O
predictions	O
pass	O
close	O
to	O
the	O
observed	O
data	O
however	O
we	O
see	O
that	O
the	O
posterior	O
has	O
a	O
ridge-like	O
shape	O
reflecting	O
the	O
fact	O
that	O
there	O
are	O
many	O
possible	O
solutions	O
with	O
different	O
bayesian	B
linear	B
regression	B
likelihood	B
priorposterior	O
data	O
space	O
y	O
y	O
y	O
y	O
x	O
x	O
x	O
x	O
sequential	B
bayesian	B
updating	O
of	O
a	O
linear	B
regression	B
model	O
pyx	O
n	O
figure	O
row	O
represents	O
the	O
prior	O
row	O
represents	O
the	O
first	O
data	O
point	O
row	O
represents	O
the	O
second	O
data	O
point	O
row	O
represents	O
the	O
data	O
point	O
left	O
column	O
likelihood	B
function	O
for	O
current	O
data	O
point	O
middle	O
column	O
posterior	O
given	O
data	O
so	O
far	O
the	O
first	O
line	O
is	O
the	O
prior	O
right	O
column	O
samples	B
from	O
the	O
current	O
priorposterior	O
predictive	B
distribution	O
the	O
white	O
cross	O
in	O
columns	O
and	O
represents	O
the	O
true	O
parameter	B
value	O
we	O
see	O
that	O
the	O
mode	B
of	O
the	O
posterior	O
rapidly	O
samples	B
converges	O
to	O
this	O
point	O
the	O
blue	O
circles	O
in	O
column	O
are	O
the	O
observed	O
data	O
points	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
slopesintercepts	O
this	O
makes	O
sense	O
since	O
we	O
cannot	O
uniquely	O
infer	O
two	O
parameters	O
from	O
one	O
observation	B
after	O
we	O
see	O
two	O
data	O
points	O
row	O
the	O
posterior	O
becomes	O
much	O
narrower	O
and	O
our	O
predictions	O
all	O
have	O
similar	B
slopes	O
and	O
intercepts	O
after	O
we	O
observe	O
data	O
points	O
row	O
the	O
posterior	O
is	O
essentially	O
a	O
delta	O
function	O
centered	O
on	O
the	O
true	O
value	O
indicated	O
by	O
a	O
white	O
cross	O
estimate	O
converges	O
to	O
the	O
truth	O
since	O
the	O
data	O
was	O
generated	O
from	O
this	O
model	O
and	O
because	O
bayes	O
is	O
a	O
consistent	B
estimator	B
see	O
section	O
for	O
discussion	O
of	O
this	O
point	O
computing	O
the	O
posterior	O
predictive	B
it	O
s	O
tough	O
to	O
make	O
predictions	O
especially	O
about	O
the	O
future	O
yogi	O
berra	O
chapter	O
linear	B
regression	B
in	O
machine	B
learning	B
we	O
often	O
care	O
more	O
about	O
predictions	O
than	O
about	O
interpreting	O
the	O
parameters	O
using	O
equation	O
we	O
can	O
easily	O
show	O
that	O
the	O
posterior	B
predictive	B
distribution	I
at	O
a	O
test	O
point	O
x	O
is	O
also	O
gaussian	B
pyxd	O
n	O
w	O
vn	O
n	O
n	O
x	O
n	O
xt	O
vn	O
x	O
n	O
the	O
variance	B
in	O
this	O
prediction	O
n	O
depends	O
on	O
two	O
terms	O
the	O
variance	B
of	O
the	O
observation	B
noise	O
and	O
the	O
variance	B
in	O
the	O
parameters	O
vn	O
the	O
latter	O
translates	O
into	O
variance	B
about	O
observations	O
in	O
a	O
way	O
which	O
depends	O
on	O
how	O
close	O
x	O
is	O
to	O
the	O
training	O
data	O
d	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
see	O
that	O
the	O
error	O
bars	O
get	O
larger	O
as	O
we	O
move	O
away	O
from	O
the	O
training	O
points	O
representing	O
increased	O
uncertainty	B
this	O
is	O
important	O
for	O
applications	O
such	O
as	O
active	B
learning	B
where	O
we	O
want	O
to	O
model	O
what	O
we	O
don	O
t	O
know	O
as	O
well	O
as	O
what	O
we	O
do	O
by	O
contrast	O
the	O
plugin	O
approximation	O
has	O
constant	O
sized	O
error	O
bars	O
since	O
n	O
w	O
wwdw	O
pyx	O
w	O
pyxd	O
see	O
figure	O
bayesian	B
inference	B
when	O
is	O
unknown	B
in	O
this	O
section	O
we	O
apply	O
the	O
results	O
in	O
section	O
to	O
the	O
problem	O
of	O
computing	O
pw	O
for	O
a	O
linear	B
regression	B
model	O
this	O
generalizes	O
the	O
results	O
from	O
section	O
where	O
we	O
assumed	O
was	O
known	O
in	O
the	O
case	O
where	O
we	O
use	O
an	O
uninformative	B
prior	O
we	O
will	O
see	O
some	O
interesting	O
connections	O
to	O
frequentist	B
statistics	I
conjugate	B
prior	I
as	O
usual	O
the	O
likelihood	B
has	O
the	O
form	O
pyx	O
w	O
n	O
by	O
analogy	O
to	O
section	O
one	O
can	O
show	O
that	O
the	O
natural	O
conjugate	B
prior	I
has	O
the	O
following	O
form	O
pw	O
nigw	O
n	O
exp	O
v	O
posterior	O
predictive	B
variance	B
prediction	O
training	O
data	O
functions	O
sampled	O
from	O
posterior	O
bayesian	B
linear	B
regression	B
plugin	O
approximation	O
prediction	O
training	O
data	O
functions	O
sampled	O
from	O
plugin	O
approximation	O
to	O
posterior	O
figure	O
plug-in	B
approximation	I
to	O
predictive	B
density	O
plug	O
in	O
the	O
mle	B
of	O
the	O
parameters	O
posterior	B
predictive	B
density	I
obtained	O
by	O
integrating	O
out	O
the	O
parameters	O
black	O
curve	O
is	O
posterior	B
mean	B
error	O
bars	O
are	O
standard	O
deviations	O
of	O
the	O
posterior	B
predictive	B
density	I
samples	B
from	O
the	O
plugin	O
approximation	O
to	O
posterior	O
predictive	B
samples	B
from	O
the	O
posterior	O
predictive	B
figure	O
generated	O
by	O
linregpostpreddemo	O
with	O
this	O
prior	O
and	O
likelihood	B
one	O
can	O
show	O
that	O
the	O
posterior	O
has	O
the	O
following	O
form	O
pw	O
nigw	O
vn	O
an	O
bn	O
xt	O
y	O
xt	O
x	O
wn	O
vn	O
vn	O
an	O
bn	O
v	O
wt	O
yt	O
y	O
wt	O
n	O
v	O
n	O
wn	O
the	O
expressions	O
for	O
wn	O
and	O
vn	O
are	O
similar	B
to	O
the	O
case	O
where	O
is	O
known	O
the	O
expression	O
for	O
an	O
is	O
also	O
intuitive	O
since	O
it	O
just	O
updates	O
the	O
counts	O
the	O
expression	O
for	O
bn	O
can	O
be	O
interpreted	O
chapter	O
linear	B
regression	B
it	O
is	O
the	O
prior	O
sum	B
of	I
squares	I
plus	O
the	O
empirical	O
sum	B
of	I
squares	I
yt	O
y	O
plus	O
a	O
as	O
follows	O
term	O
due	O
to	O
the	O
error	O
in	O
the	O
prior	O
on	O
w	O
the	O
posterior	O
marginals	O
are	O
as	O
follows	O
p	O
igan	O
bn	O
pwd	O
bn	O
an	O
vn	O
we	O
give	O
a	O
worked	O
example	O
of	O
using	O
these	O
equations	O
in	O
section	O
by	O
analogy	O
to	O
section	O
the	O
posterior	B
predictive	B
distribution	I
is	O
a	O
student	B
t	I
distribution	I
in	O
particular	O
given	O
m	O
new	O
test	O
inputs	O
x	O
we	O
have	O
p	O
y	O
xd	O
y	O
xwn	O
bn	O
an	O
xvn	O
xt	O
the	O
predictive	B
variance	B
has	O
two	O
components	O
due	O
to	O
the	O
measurement	O
noise	O
and	O
xvn	O
xt	O
due	O
to	O
the	O
uncertainty	B
in	O
w	O
this	O
latter	O
terms	O
varies	O
depending	O
on	O
how	O
close	O
the	O
test	O
inputs	O
are	O
to	O
the	O
training	O
data	O
it	O
is	O
common	O
to	O
set	O
corresponding	O
to	O
an	O
uninformative	B
prior	O
for	O
and	O
to	O
set	O
for	O
any	O
positive	O
value	O
g	O
this	O
is	O
called	O
zellner	O
s	O
g-prior	B
and	O
gxt	O
x	O
here	O
g	O
plays	O
a	O
role	O
analogous	O
to	O
in	O
ridge	B
regression	B
however	O
the	O
prior	O
covariance	B
is	O
rather	O
than	O
i	O
this	O
ensures	O
that	O
the	O
posterior	O
is	O
invariant	B
to	O
scaling	O
proportional	O
to	O
x	O
of	O
the	O
inputs	O
see	O
also	O
exercise	O
we	O
will	O
see	O
below	O
that	O
if	O
we	O
use	O
an	O
uninformative	B
prior	O
the	O
posterior	O
precision	B
given	O
n	O
measurements	O
is	O
v	O
n	O
xt	O
x	O
the	O
unit	B
information	B
prior	I
is	O
defined	O
to	O
contain	O
as	O
much	O
information	B
as	O
one	O
sample	O
and	O
wasserman	O
to	O
create	O
a	O
unit	B
information	B
prior	I
for	O
linear	B
regression	B
we	O
need	O
to	O
use	O
v	O
n	O
xt	O
x	O
which	O
is	O
equivalent	O
to	O
the	O
g-prior	B
with	O
g	O
n	O
uninformative	B
prior	O
an	O
uninformative	B
prior	O
can	O
be	O
obtained	O
by	O
considering	O
the	O
uninformative	B
limit	O
of	O
the	O
conjugate	O
g-prior	B
which	O
corresponds	O
to	O
setting	O
g	O
this	O
is	O
equivalent	O
to	O
an	O
improper	O
nig	O
prior	O
with	O
i	O
and	O
which	O
gives	O
pw	O
alternatively	O
we	O
can	O
start	O
with	O
the	O
semi-conjugate	B
prior	O
pw	O
wp	O
and	O
take	O
each	O
term	O
to	O
its	O
uninformative	B
limit	O
individually	O
which	O
gives	O
pw	O
this	O
is	O
equivalent	O
to	O
an	O
improper	O
nig	O
prior	O
with	O
i	O
and	O
the	O
corresponding	O
posterior	O
is	O
given	O
by	O
pw	O
nigw	O
vn	O
an	O
bn	O
y	O
wn	O
wmle	O
x	O
vn	O
x	O
n	O
d	O
an	O
bn	O
x	O
wmlet	O
x	O
wmle	O
bayesian	B
linear	B
regression	B
wj	O
e	O
var	B
sig	O
ci	B
table	O
posterior	B
mean	B
standard	B
deviation	I
and	O
credible	O
intervals	O
for	O
a	O
linear	B
regression	B
model	O
with	O
an	O
uninformative	B
prior	O
fit	O
to	O
the	O
caterpillar	O
data	O
produced	O
by	O
linregbayescaterpillar	O
the	O
marginal	B
distribution	I
of	O
the	O
weights	O
is	O
given	O
by	O
pwd	O
t	O
w	O
c	O
n	O
d	O
where	O
c	O
x	O
n	O
d	O
and	O
w	O
is	O
the	O
mle	B
we	O
discuss	O
the	O
implications	O
of	O
these	O
equations	O
below	O
an	O
example	O
where	O
bayesian	B
and	O
frequentist	B
inference	B
coincide	O
the	O
use	O
of	O
a	O
uninformative	B
prior	O
is	O
interesting	O
because	O
the	O
resulting	O
posterior	O
turns	O
out	O
to	O
be	O
equivalent	O
to	O
the	O
results	O
from	O
frequentist	B
statistics	I
also	O
section	O
in	O
particular	O
from	O
equation	O
we	O
have	O
pwjd	O
t	O
wj	O
n	O
d	O
n	O
d	O
this	O
is	O
equivalent	O
to	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	B
which	O
is	O
given	O
by	O
the	O
following	O
e	O
g	O
and	O
berger	O
wj	O
wj	O
sj	O
where	O
sj	O
tn	O
d	O
n	O
d	O
is	O
the	O
standard	B
error	I
of	O
the	O
estimated	O
parameter	B
section	O
for	O
a	O
discussion	O
of	O
sampling	O
distributions	O
consequently	O
the	O
frequentist	B
confidence	O
interval	O
and	O
the	O
bayesian	B
marginal	O
credible	B
interval	I
for	O
the	O
parameters	O
are	O
the	O
same	O
in	O
this	O
case	O
as	O
a	O
worked	O
example	O
of	O
this	O
consider	O
the	O
caterpillar	O
dataset	O
from	O
and	O
robert	O
details	O
of	O
what	O
the	O
data	O
mean	B
don	O
t	O
matter	O
for	O
our	O
present	O
purposes	O
we	O
can	O
compute	O
chapter	O
linear	B
regression	B
the	O
posterior	B
mean	B
and	O
standard	B
deviation	I
and	O
the	O
credible	O
intervals	O
for	O
the	O
regression	B
coefficients	O
using	O
equation	O
the	O
results	O
are	O
shown	O
in	O
table	O
it	O
is	O
easy	O
to	O
check	O
that	O
these	O
credible	O
intervals	O
are	O
identical	O
to	O
the	O
confidence	O
intervals	O
computed	O
using	O
standard	O
frequentist	B
methods	O
linregbayescaterpillar	O
for	O
the	O
code	O
we	O
can	O
also	O
use	O
these	O
marginal	O
posteriors	O
to	O
compute	O
if	O
the	O
coefficients	O
are	O
significantly	O
different	O
from	O
an	O
informal	O
way	O
to	O
do	O
this	O
using	O
decision	B
theory	O
is	O
to	O
check	O
if	O
its	O
ci	B
excludes	O
from	O
table	O
we	O
see	O
that	O
the	O
cis	O
for	O
coefficients	O
are	O
all	O
significant	O
by	O
this	O
measure	O
so	O
we	O
put	O
a	O
little	O
star	O
by	O
them	O
it	O
is	O
easy	O
to	O
check	O
that	O
these	O
results	O
are	O
the	O
same	O
as	O
those	O
produced	O
by	O
standard	O
frequentist	B
software	O
packages	O
which	O
compute	O
p-values	O
at	O
the	O
level	O
although	O
the	O
correspondence	B
between	O
the	O
bayesian	B
and	O
frequentist	B
results	O
might	O
seem	O
appealing	O
to	O
some	O
readers	O
recall	B
from	O
section	O
that	O
frequentist	B
inference	B
is	O
riddled	O
with	O
pathologies	B
also	O
note	O
that	O
the	O
mle	B
does	O
not	O
even	O
exist	O
when	O
n	O
d	O
so	O
standard	O
frequentist	B
inference	B
theory	O
breaks	O
down	O
in	O
this	O
setting	O
bayesian	B
inference	B
theory	O
still	O
works	O
although	O
it	O
requires	O
the	O
use	O
of	O
proper	O
priors	O
and	O
george	O
for	O
one	O
extension	B
of	O
the	O
g-prior	B
to	O
the	O
case	O
where	O
d	O
n	O
eb	B
for	O
linear	B
regression	B
procedure	O
so	O
far	O
we	O
have	O
assumed	O
the	O
prior	O
is	O
known	O
in	O
this	O
section	O
we	O
describe	O
an	O
empirical	B
bayes	I
procedure	O
for	O
picking	O
the	O
hyper-parameters	B
more	O
precisely	O
we	O
choose	O
to	O
maximize	O
the	O
marignal	O
likelihood	B
where	O
be	O
the	O
precision	B
of	O
the	O
observation	B
noise	O
and	O
is	O
the	O
precision	B
of	O
the	O
prior	O
pw	O
this	O
is	O
known	O
as	O
the	O
evidence	B
procedure	I
see	O
section	O
for	O
the	O
algorithmic	O
details	O
the	O
evidence	B
procedure	I
provides	O
an	O
alternative	O
to	O
using	O
cross	B
validation	I
for	O
example	O
in	O
likelihood	B
for	O
different	O
values	O
of	O
as	O
well	O
as	O
the	O
figure	O
we	O
plot	O
the	O
log	O
marginal	O
maximum	O
value	O
found	O
by	O
the	O
optimizer	O
we	O
see	O
that	O
in	O
this	O
example	O
we	O
get	O
the	O
same	O
result	O
kept	O
fixed	O
in	O
both	O
methods	O
to	O
make	O
them	O
as	O
shown	O
in	O
figure	O
comparable	O
the	O
principle	O
practical	O
advantage	O
of	O
the	O
evidence	B
procedure	I
over	O
cv	B
will	O
become	O
apparent	O
in	O
section	O
where	O
we	O
generalize	B
the	O
prior	O
by	O
allowing	O
a	O
different	O
j	O
for	O
every	O
feature	O
this	O
can	O
be	O
used	O
to	O
perform	O
feature	B
selection	I
using	O
a	O
technique	O
known	O
as	O
automatic	B
relevancy	I
determination	I
or	O
ard	B
by	O
contrast	O
it	O
would	O
not	O
be	O
possible	O
to	O
use	O
cv	B
to	O
tune	O
d	O
different	O
hyper-parameters	B
the	O
evidence	B
procedure	I
is	O
also	O
useful	O
when	O
comparing	O
different	O
kinds	O
of	O
models	O
since	O
it	O
provides	O
a	O
good	O
approximation	O
to	O
the	O
evidence	B
pdm	O
max	O
pdw	O
mpwm	O
pdw	O
mpwm	O
it	O
is	O
important	O
to	O
least	O
approximately	O
integrate	O
over	O
rather	O
than	O
setting	O
it	O
arbitrarily	O
for	O
reasons	O
discussed	O
in	O
section	O
indeed	O
this	O
is	O
the	O
method	O
we	O
used	O
to	O
evaluate	O
the	O
marginal	O
alternatively	O
we	O
could	O
integrate	B
out	I
analytically	O
as	O
shown	O
in	O
section	O
and	O
just	O
optimize	O
and	O
weigend	O
however	O
it	O
turns	O
out	O
that	O
this	O
is	O
less	O
accurate	O
than	O
optimizing	O
both	O
and	O
bayesian	B
linear	B
regression	B
e	O
s	O
m	O
fold	O
cross	B
validation	I
ntrain	O
log	O
evidence	B
log	O
lambda	O
log	O
alpha	O
figure	O
estimate	O
of	O
test	O
mse	B
produced	O
by	O
cross-validation	O
vs	O
log	O
the	O
smallest	O
value	O
is	O
indicated	O
by	O
the	O
vertical	O
line	O
note	O
the	O
vertical	O
scale	O
is	O
in	O
log	O
units	O
log	O
marginal	B
likelihood	B
vs	O
log	O
the	O
largest	O
value	O
is	O
indicated	O
by	O
the	O
vertical	O
line	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
likelihood	B
for	O
the	O
polynomial	B
regression	B
models	O
in	O
figures	O
and	O
for	O
a	O
more	O
bayesian	B
approach	O
in	O
which	O
we	O
model	O
our	O
uncertainty	B
about	O
rather	O
than	O
computing	O
point	O
estimates	O
see	O
section	O
exercises	O
exercise	O
behavior	O
of	O
training	B
set	I
error	O
with	O
increasing	O
sample	O
size	O
the	O
error	O
on	O
the	O
test	O
will	O
always	O
decrease	O
as	O
we	O
get	O
more	O
training	O
data	O
since	O
the	O
model	O
will	O
be	O
better	O
estimated	O
however	O
as	O
shown	O
in	O
figure	O
for	O
sufficiently	O
complex	O
models	O
the	O
error	O
on	O
the	O
training	B
set	I
can	O
increase	O
we	O
we	O
get	O
more	O
training	O
data	O
until	O
we	O
reach	O
some	O
plateau	O
explain	O
why	O
exercise	O
multi-output	O
linear	B
regression	B
jaakkola	O
when	O
we	O
have	O
multiple	O
independent	O
outputs	O
in	O
linear	B
regression	B
the	O
model	O
becomes	O
pyx	O
w	O
n	O
j	O
xi	O
j	O
since	O
the	O
likelihood	B
factorizes	O
across	O
dimensions	O
so	O
does	O
the	O
mle	B
thus	O
w	O
wm	O
where	O
wj	O
x	O
in	O
this	O
exercise	O
we	O
apply	O
this	O
result	O
to	O
a	O
model	O
with	O
dimensional	O
response	O
vector	O
yi	O
r	O
have	O
some	O
binary	O
input	O
data	O
xi	O
the	O
training	O
data	O
is	O
as	O
follows	O
suppose	O
we	O
chapter	O
linear	B
regression	B
x	O
y	O
let	O
us	O
embed	O
each	O
xi	O
into	O
using	O
the	O
following	O
basis	O
function	O
the	O
model	O
becomes	O
y	O
wt	O
where	O
w	O
is	O
a	O
matrix	O
compute	O
the	O
mle	B
for	O
w	O
from	O
the	O
above	O
data	O
exercise	O
centering	O
and	O
ridge	B
regression	B
assume	O
that	O
x	O
so	O
the	O
input	O
data	O
has	O
been	O
centered	O
show	O
that	O
the	O
optimizer	O
of	O
jw	O
xw	O
xw	O
wt	O
w	O
is	O
y	O
w	O
x	O
i	O
y	O
exercise	O
mle	B
for	O
for	O
linear	B
regression	B
show	O
that	O
the	O
mle	B
for	O
the	O
error	O
variance	B
in	O
linear	B
regression	B
is	O
given	O
by	O
n	O
xt	O
i	O
this	O
is	O
just	O
the	O
empirical	O
variance	B
of	O
the	O
residual	B
errors	O
when	O
we	O
plug	O
in	O
our	O
estimate	O
of	O
w	O
exercise	O
mle	B
for	O
the	O
offset	O
term	O
in	O
linear	B
regression	B
linear	B
regression	B
has	O
the	O
form	O
e	O
wt	O
x	O
it	O
is	O
common	O
to	O
include	O
a	O
column	O
of	O
s	O
in	O
the	O
design	B
matrix	I
so	O
we	O
can	O
solve	O
for	O
the	O
offset	O
term	O
term	O
and	O
the	O
other	O
parameters	O
w	O
at	O
the	O
same	O
time	O
using	O
the	O
normal	B
equations	O
however	O
it	O
is	O
also	O
possible	O
to	O
solve	O
for	O
w	O
and	O
separately	O
show	O
that	O
n	O
yi	O
n	O
i	O
w	O
y	O
xt	O
w	O
xt	O
i	O
i	O
so	O
models	O
the	O
difference	O
in	O
the	O
average	O
output	O
from	O
the	O
average	O
predicted	O
output	O
also	O
show	O
that	O
w	O
c	O
xc	O
c	O
yc	O
xxi	O
xt	O
yxi	O
x	O
i	O
xi	O
x	O
along	O
its	O
rows	O
and	O
yc	O
y	O
y	O
is	O
where	O
xc	O
is	O
the	O
centered	O
input	O
matrix	O
containing	O
xc	O
the	O
centered	O
output	O
vector	O
thus	O
we	O
can	O
first	O
compute	O
w	O
on	O
centered	O
data	O
and	O
then	O
estimate	O
using	O
y	O
xt	O
w	O
bayesian	B
linear	B
regression	B
exercise	O
mle	B
for	O
simple	B
linear	B
regression	B
simple	B
linear	B
regression	B
refers	O
to	O
the	O
case	O
where	O
the	O
input	O
is	O
scalar	O
so	O
d	O
show	O
that	O
the	O
mle	B
in	O
this	O
case	O
is	O
given	O
by	O
the	O
following	O
equations	O
which	O
may	O
be	O
familiar	O
from	O
basic	O
statistics	O
classes	O
i	O
xiyi	O
n	O
x	O
y	O
i	O
y	O
x	O
e	O
ixi	O
xyi	O
y	O
ixi	O
i	O
n	O
cov	O
y	O
var	B
see	O
for	O
a	O
demo	O
exercise	O
sufficient	B
statistics	I
for	O
online	O
linear	B
regression	B
not	O
keep	O
the	O
original	O
data	O
xi	O
yi	O
but	O
we	O
do	O
have	O
the	O
following	O
functions	O
of	O
the	O
data	O
jaakkola	O
consider	O
fitting	O
the	O
model	O
y	O
using	O
least	B
squares	I
unfortunately	O
we	O
did	O
xn	O
c	O
xx	O
n	O
n	O
xi	O
yn	O
n	O
c	O
xy	O
n	O
xyi	O
y	O
c	O
yy	O
n	O
yi	O
a	O
what	O
are	O
the	O
minimal	B
set	O
of	O
statistics	O
that	O
we	O
need	O
to	O
estimate	O
see	O
equation	O
b	O
what	O
are	O
the	O
minimal	B
set	O
of	O
statistics	O
that	O
we	O
need	O
to	O
estimate	O
see	O
equation	O
c	O
suppose	O
a	O
new	O
data	O
point	O
arrives	O
and	O
we	O
want	O
to	O
update	O
our	O
sufficient	B
statistics	I
without	O
is	O
useful	O
for	O
online	B
learning	B
show	O
that	O
we	O
looking	O
at	O
the	O
old	O
data	O
which	O
we	O
have	O
not	O
stored	O
can	O
this	O
for	O
x	O
as	O
follows	O
this	O
has	O
the	O
form	O
new	O
estimate	O
is	O
old	O
estimate	O
plus	O
correction	O
we	O
see	O
that	O
the	O
size	O
of	O
the	O
correction	O
diminishes	O
over	O
time	O
as	O
we	O
get	O
more	O
samples	B
derive	O
a	O
similar	B
expression	O
to	O
update	O
y	O
d	O
show	O
that	O
one	O
can	O
update	O
c	O
xy	O
recursively	O
using	O
c	O
xy	O
n	O
nc	O
xy	O
nxnyn	O
derive	O
a	O
similar	B
expression	O
to	O
update	O
cxx	O
implement	O
the	O
online	B
learning	B
algorithm	O
i	O
e	O
write	O
a	O
function	O
of	O
the	O
form	O
linregupdatessss	O
x	O
y	O
where	O
x	O
and	O
y	O
are	O
scalars	O
and	O
ss	O
is	O
a	O
structure	O
containing	O
the	O
sufficient	B
statistics	I
e	O
f	O
plot	O
the	O
coefficients	O
over	O
time	O
using	O
the	O
dataset	O
in	O
use	O
polydatamake	O
sampling	O
thibaux	O
check	O
that	O
they	O
converge	B
to	O
the	O
solution	O
given	O
by	O
the	O
batch	B
learner	O
ordinary	B
least	B
squares	I
your	O
result	O
should	O
look	O
like	O
figure	O
turn	O
in	O
your	O
derivation	O
code	O
and	O
plot	O
exercise	O
bayesian	B
linear	B
regression	B
in	O
with	O
known	O
bolstad	O
consider	O
fitting	O
a	O
model	O
of	O
the	O
form	O
pyx	O
n	O
to	O
the	O
data	O
shown	O
below	O
n	O
xn	O
xi	O
n	O
nxn	O
xn	O
n	O
chapter	O
linear	B
regression	B
s	O
t	O
h	O
g	O
e	O
w	O
i	O
online	O
linear	B
regression	B
batch	B
batch	B
time	O
figure	O
regression	B
coefficients	O
over	O
time	O
produced	O
by	O
exercise	O
x	O
y	O
a	O
compute	O
an	O
unbiased	B
estimate	O
of	O
using	O
n	O
denominator	O
is	O
n	O
since	O
we	O
have	O
inputs	O
namely	O
the	O
offset	O
term	O
and	O
x	O
here	O
yi	O
and	O
w	O
is	O
the	O
mle	B
b	O
now	O
assume	O
the	O
following	O
prior	O
on	O
w	O
pw	O
use	O
an	O
uniform	O
prior	O
on	O
and	O
a	O
n	O
prior	O
on	O
show	O
that	O
this	O
can	O
be	O
written	O
as	O
a	O
gaussian	B
prior	O
of	O
the	O
form	O
pw	O
n	O
what	O
are	O
and	O
c	O
compute	O
the	O
marginal	O
posterior	O
of	O
the	O
slope	O
where	O
d	O
is	O
the	O
data	O
above	O
and	O
is	O
the	O
unbiased	B
estimate	O
computed	O
above	O
what	O
is	O
e	O
show	O
your	O
work	O
can	O
use	O
matlab	O
if	O
you	O
like	O
hint	O
the	O
posterior	O
variance	B
is	O
a	O
very	O
small	O
number	O
and	O
var	B
d	O
what	O
is	O
a	O
credible	B
interval	I
for	O
exercise	O
generative	O
model	O
for	O
linear	B
regression	B
linear	B
regression	B
is	O
the	O
problem	O
of	O
estimating	O
ey	O
using	O
a	O
linear	O
function	O
of	O
the	O
form	O
wt	O
x	O
typically	O
we	O
assume	O
that	O
the	O
conditional	O
distribution	O
of	O
y	O
given	O
x	O
is	O
gaussian	B
we	O
can	O
either	O
estimate	O
this	O
conditional	B
gaussian	B
directly	O
discriminative	B
approach	O
or	O
we	O
can	O
fit	O
a	O
gaussian	B
to	O
the	O
joint	B
distribution	I
of	O
x	O
y	O
and	O
then	O
derive	O
ey	O
x	O
in	O
exercise	O
we	O
showed	O
that	O
the	O
discriminative	B
approach	O
leads	O
to	O
these	O
equations	O
ey	O
wt	O
x	O
y	O
xt	O
w	O
w	O
c	O
xc	O
c	O
yc	O
bayesian	B
linear	B
regression	B
where	O
xc	O
x	O
x	O
is	O
the	O
centered	O
input	O
matrix	O
and	O
x	O
replicates	O
x	O
across	O
the	O
rows	O
similarly	O
yc	O
y	O
y	O
is	O
the	O
centered	O
output	O
vector	O
and	O
y	O
replicates	O
y	O
across	O
the	O
rows	O
a	O
by	O
finding	O
the	O
maximum	O
likelihood	B
estimates	O
of	O
xx	O
xy	O
x	O
and	O
y	O
derive	O
the	O
above	O
equations	O
by	O
fitting	O
a	O
joint	O
gaussian	B
to	O
x	O
y	O
and	O
using	O
the	O
formula	O
for	O
conditioning	B
a	O
gaussian	B
section	O
show	O
your	O
work	O
b	O
what	O
are	O
the	O
advantages	O
and	O
disadvantages	O
of	O
this	O
approach	O
compared	O
to	O
the	O
standard	O
discriminative	B
approach	O
exercise	O
bayesian	B
linear	B
regression	B
using	O
the	O
g-prior	B
show	O
that	O
when	O
we	O
use	O
the	O
g-prior	B
pw	O
nigw	O
gxt	O
x	O
following	O
form	O
the	O
posterior	O
has	O
the	O
pw	O
nigw	O
vn	O
an	O
bn	O
vn	O
g	O
g	O
g	O
x	O
wmle	O
wn	O
g	O
an	O
bn	O
wt	O
mlext	O
x	O
wmle	O
logistic	B
regression	B
introduction	O
one	O
way	O
to	O
build	O
a	O
probabilistic	O
classifier	O
is	O
to	O
create	O
a	O
joint	O
model	O
of	O
the	O
form	O
py	O
x	O
and	O
then	O
to	O
condition	O
on	O
x	O
thereby	O
deriving	O
pyx	O
this	O
is	O
called	O
the	O
generative	B
approach	I
an	O
alternative	O
approach	O
is	O
to	O
fit	O
a	O
model	O
of	O
the	O
form	O
pyx	O
directly	O
this	O
is	O
called	O
the	O
discriminative	B
approach	O
and	O
is	O
the	O
approach	O
we	O
adopt	O
in	O
this	O
chapter	O
in	O
particular	O
we	O
will	O
assume	O
discriminative	B
models	O
which	O
are	O
linear	O
in	O
the	O
parameters	O
this	O
will	O
turn	O
out	O
to	O
significantly	O
simplify	O
model	O
fitting	O
as	O
we	O
will	O
see	O
in	O
section	O
we	O
compare	O
the	O
generative	O
and	O
discriminative	B
approaches	O
and	O
in	O
later	O
chapters	O
we	O
will	O
consider	O
non-linear	O
and	O
non-parametric	O
discriminative	B
models	O
model	O
specification	O
as	O
we	O
discussed	O
in	O
section	O
logistic	B
regression	B
corresponds	O
to	O
the	O
following	O
binary	O
classification	O
model	O
pyx	O
w	O
berysigmwt	O
x	O
a	O
example	O
is	O
shown	O
in	O
figure	O
logistic	B
regression	B
can	O
easily	O
be	O
extended	O
to	O
higherdimensional	O
inputs	O
for	O
example	O
figure	O
shows	O
plots	O
of	O
py	O
w	O
sigmwt	O
x	O
for	O
input	O
and	O
different	O
weight	O
vectors	O
w	O
if	O
we	O
threshold	O
these	O
probabilities	O
at	O
we	O
induce	O
a	O
linear	O
decision	B
boundary	I
whose	O
normal	B
is	O
given	O
by	O
w	O
model	O
fitting	O
in	O
this	O
section	O
we	O
discuss	O
algorithms	O
for	O
estimating	O
the	O
parameters	O
of	O
a	O
logistic	B
regression	B
model	O
chapter	O
logistic	B
regression	B
w	O
w	O
w	O
x	O
x	O
w	O
x	O
x	O
w	O
x	O
x	O
x	O
x	O
w	O
w	O
x	O
x	O
w	O
x	O
x	O
w	O
x	O
x	O
x	O
x	O
x	O
x	O
w	O
x	O
x	O
plots	O
of	O
here	O
w	O
defines	O
the	O
normal	B
to	O
the	O
decision	B
figure	O
boundary	O
points	O
to	O
the	O
right	O
of	O
this	O
have	O
sigmwt	O
x	O
and	O
points	O
to	O
the	O
left	O
have	O
sigmwt	O
x	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
mle	B
the	O
negative	O
log-likelihood	O
for	O
logistic	B
regression	B
is	O
given	O
by	O
nllw	O
log	O
i	O
log	O
i	O
yi	O
i	O
this	O
is	O
also	O
called	O
the	O
cross-entropy	B
error	B
function	I
section	O
another	O
way	O
of	O
writing	O
this	O
is	O
as	O
follows	O
suppose	O
yi	O
instead	O
of	O
yi	O
we	O
x	O
hence	O
have	O
py	O
wt	O
x	O
and	O
py	O
exp	O
yiwt	O
xi	O
n	O
llw	O
unlike	O
linear	B
regression	B
we	O
can	O
no	O
longer	O
write	O
down	O
the	O
mle	B
in	O
closed	O
form	O
instead	O
we	O
need	O
to	O
use	O
an	O
optimization	B
algorithm	O
to	O
compute	O
it	O
for	O
this	O
we	O
need	O
to	O
derive	O
the	O
gradient	O
and	O
hessian	B
in	O
the	O
case	O
of	O
logistic	B
regression	B
one	O
can	O
show	O
that	O
the	O
gradient	O
and	O
hessian	B
model	O
fitting	O
figure	O
gradient	B
descent	I
on	O
a	O
simple	O
function	O
starting	O
from	O
for	O
steps	O
using	O
a	O
fixed	O
learning	B
rate	B
size	O
the	O
global	B
minimum	I
is	O
at	O
figure	O
generated	O
by	O
steepestdescentdemo	O
of	O
this	O
are	O
given	O
by	O
the	O
following	O
g	O
h	O
d	O
dw	O
d	O
dw	O
f	O
gwt	O
i	O
yixi	O
xt	O
y	O
i	O
w	O
ixt	O
i	O
i	O
i	O
ixixt	O
i	O
xt	O
sx	O
where	O
s	O
diag	O
i	O
one	O
can	O
also	O
show	O
that	O
h	O
is	O
positive	O
definite	O
hence	O
the	O
nll	B
is	O
convex	B
and	O
has	O
a	O
unique	O
global	B
minimum	I
below	O
we	O
discuss	O
some	O
methods	O
for	O
finding	O
this	O
minimum	O
steepest	B
descent	I
perhaps	O
the	O
simplest	O
algorithm	O
for	O
unconstrained	O
optimization	B
is	O
gradient	B
descent	I
also	O
known	O
as	O
steepest	B
descent	I
this	O
can	O
be	O
written	O
as	O
follows	O
k	O
kgk	O
where	O
k	O
is	O
the	O
step	B
size	I
or	O
learning	B
rate	B
the	O
main	O
issue	O
in	O
gradient	B
descent	I
is	O
how	O
should	O
we	O
set	O
the	O
step	B
size	I
this	O
turns	O
out	O
to	O
be	O
quite	O
tricky	O
if	O
we	O
use	O
a	O
constant	O
learning	B
rate	B
but	O
make	O
it	O
too	O
small	O
convergence	O
will	O
be	O
very	O
slow	O
but	O
if	O
we	O
make	O
it	O
too	O
large	O
the	O
method	O
can	O
fail	O
to	O
converge	B
at	O
all	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
plot	O
the	O
following	O
function	O
f	O
we	O
arbitrarily	O
decide	O
to	O
start	O
from	O
in	O
figure	O
we	O
use	O
a	O
fixed	O
step	B
size	I
of	O
we	O
see	O
that	O
it	O
moves	O
slowly	O
along	O
the	O
valley	O
in	O
figure	O
we	O
use	O
a	O
fixed	O
step	B
size	I
of	O
we	O
see	O
that	O
the	O
algorithm	O
starts	O
oscillating	O
up	O
and	O
down	O
the	O
sides	O
of	O
the	O
valley	O
and	O
never	O
converges	O
to	O
the	O
optimum	O
chapter	O
logistic	B
regression	B
exact	O
line	O
searching	O
figure	O
steepest	B
descent	I
on	O
the	O
same	O
function	O
as	O
figure	O
starting	O
from	O
using	O
line	B
search	I
figure	O
generated	O
by	O
steepestdescentdemo	O
illustration	O
of	O
the	O
fact	O
that	O
at	O
the	O
end	O
of	O
a	O
line	B
search	I
of	O
picture	O
the	O
local	O
gradient	O
of	O
the	O
function	O
will	O
be	O
perpendicular	O
to	O
the	O
search	O
direction	O
based	O
on	O
figure	O
of	O
et	O
al	O
let	O
us	O
develop	O
a	O
more	O
stable	B
method	O
for	O
picking	O
the	O
step	B
size	I
so	O
that	O
the	O
method	O
is	O
guaranthis	O
property	O
is	O
called	O
global	O
teed	O
to	O
converge	B
to	O
a	O
local	O
optimum	O
no	O
matter	O
where	O
we	O
start	O
convergence	O
which	O
should	O
not	O
be	O
confused	O
with	O
convergence	O
to	O
the	O
global	O
optimum	O
by	O
taylor	O
s	O
theorem	O
we	O
have	O
f	O
d	O
f	O
gt	O
d	O
where	O
d	O
is	O
our	O
descent	O
direction	O
so	O
if	O
is	O
chosen	O
small	O
enough	O
then	O
f	O
d	O
f	O
since	O
the	O
gradient	O
will	O
be	O
negative	O
but	O
we	O
don	O
t	O
want	O
to	O
choose	O
the	O
step	B
size	I
too	O
small	O
or	O
we	O
will	O
move	O
very	O
slowly	O
and	O
may	O
not	O
reach	O
the	O
minimum	O
so	O
let	O
us	O
pick	O
to	O
minimize	O
f	O
k	O
dk	O
this	O
is	O
called	O
line	B
minimization	I
or	O
line	B
search	I
there	O
are	O
various	O
methods	O
for	O
solving	O
this	O
optimization	B
problem	O
see	O
and	O
wright	O
for	O
details	O
figure	O
demonstrates	O
that	O
line	B
search	I
does	O
indeed	O
work	O
for	O
our	O
simple	O
problem	O
however	O
we	O
see	O
that	O
the	O
steepest	B
descent	I
path	B
with	O
exact	O
line	O
searches	O
exhibits	O
a	O
characteristic	O
zig-zag	B
behavior	O
to	O
see	O
why	O
note	O
that	O
an	O
exact	O
line	B
search	I
satisfies	O
k	O
arg	O
min	O
a	O
necessary	O
condition	O
for	O
the	O
optimum	O
is	O
t	O
g	O
where	O
g	O
d	O
is	O
the	O
gradient	O
at	O
the	O
end	O
of	O
the	O
step	O
so	O
we	O
either	O
have	O
g	O
which	O
means	O
we	O
have	O
found	O
a	O
stationary	B
point	O
or	O
g	O
d	O
which	O
means	O
that	O
exact	O
search	O
stops	O
at	O
a	O
point	O
where	O
the	O
local	O
gradient	O
is	O
perpendicular	O
to	O
the	O
search	O
direction	O
hence	O
consecutive	O
directions	O
will	O
be	O
orthogonal	O
figure	O
this	O
explains	O
the	O
zig-zag	B
behavior	O
one	O
simple	O
heuristic	O
to	O
reduce	O
the	O
effect	O
of	O
zig-zagging	O
is	O
to	O
add	O
a	O
momentum	B
term	O
k	O
by	O
the	O
chain	B
rule	I
k	O
as	O
follows	O
k	O
kgk	O
k	O
k	O
k	O
model	O
fitting	O
where	O
k	O
controls	O
the	O
importance	O
of	O
the	O
momentum	B
term	O
community	O
this	O
is	O
known	O
as	O
the	O
heavy	B
ball	I
method	I
e	O
g	O
in	O
the	O
optimization	B
an	O
alternative	O
way	O
to	O
minimize	O
zig-zagging	O
is	O
to	O
use	O
the	O
method	O
of	O
conjugate	B
gradients	I
e	O
g	O
and	O
wright	O
ch	O
or	O
and	O
van	O
loan	O
sec	O
this	O
is	O
the	O
method	O
of	O
choice	O
for	O
quadratic	O
objectives	O
of	O
the	O
form	O
f	O
t	O
a	O
which	O
arise	O
when	O
solving	O
linear	O
systems	O
however	O
non-linear	O
cg	O
is	O
less	O
popular	O
newton	O
s	O
method	O
algorithm	O
newton	O
s	O
method	O
for	O
minimizing	O
a	O
strictly	B
convex	B
function	O
initialize	O
for	O
k	O
until	O
convergence	O
do	O
evaluate	O
gk	O
f	O
k	O
evaluate	O
hk	O
k	O
solve	O
hkdk	O
gk	O
for	O
dk	O
use	O
line	B
search	I
to	O
find	O
stepsize	O
k	O
along	O
dk	O
k	O
kdk	O
one	O
can	O
derive	O
faster	O
optimization	B
methods	O
by	O
taking	O
the	O
curvature	O
of	O
the	O
space	O
the	O
into	O
account	O
these	O
are	O
called	O
second	B
order	I
optimization	B
metods	O
the	O
primary	O
hessian	B
example	O
is	O
newton	O
s	O
algorithm	O
this	O
is	O
an	O
iterative	O
algorithm	O
which	O
consists	O
of	O
updates	O
of	O
the	O
form	O
k	O
kh	O
k	O
gk	O
the	O
full	B
pseudo-code	O
is	O
given	O
in	O
algorithm	O
this	O
algorithm	O
can	O
be	O
derived	O
as	O
follows	O
consider	O
making	O
a	O
second-order	O
taylor	B
series	I
approximation	O
of	O
f	O
around	O
k	O
k	O
k	O
fquad	O
fk	O
gt	O
kt	O
hk	O
k	O
let	O
us	O
rewrite	O
this	O
as	O
fquad	O
t	O
a	O
bt	O
c	O
where	O
a	O
hk	O
b	O
gk	O
hk	O
k	O
c	O
fk	O
gt	O
k	O
k	O
t	O
k	O
hk	O
k	O
the	O
minimum	O
of	O
fquad	O
is	O
at	O
a	O
k	O
h	O
k	O
gk	O
thus	O
the	O
newton	O
step	O
dk	O
h	O
order	O
approximation	O
of	O
f	O
around	O
k	O
see	O
figure	O
for	O
an	O
illustration	O
k	O
gk	O
is	O
what	O
should	O
be	O
added	O
to	O
k	O
to	O
minimize	O
the	O
second	O
chapter	O
logistic	B
regression	B
fx	O
f	O
quad	O
fx	O
f	O
quad	O
x	O
k	O
x	O
k	O
k	O
x	O
k	O
x	O
k	O
k	O
illustration	O
of	O
newton	O
s	O
method	O
for	O
minimizing	O
a	O
function	O
figure	O
the	O
solid	O
curve	O
is	O
the	O
function	O
f	O
the	O
dotted	O
line	O
fquadx	O
is	O
its	O
second	B
order	I
approximation	O
at	O
xk	O
the	O
newton	O
step	O
dk	O
is	O
what	O
must	O
be	O
added	O
to	O
xk	O
to	O
get	O
to	O
the	O
minimum	O
of	O
fquadx	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
newtonsmethodminquad	O
illustration	O
of	O
newton	O
s	O
method	O
applied	O
to	O
a	O
nonconvex	O
function	O
we	O
fit	O
a	O
quadratic	O
around	O
the	O
current	O
point	O
xk	O
and	O
move	O
to	O
its	O
stationary	B
point	O
xk	O
dk	O
unfortunately	O
this	O
is	O
a	O
local	O
maximum	O
not	O
minimum	O
this	O
means	O
we	O
need	O
to	O
be	O
careful	O
about	O
the	O
extent	O
of	O
our	O
quadratic	O
approximation	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
newtonsmethodnonconvex	O
in	O
its	O
simplest	O
form	O
listed	O
newton	O
s	O
method	O
requires	O
that	O
hk	O
be	O
positive	O
definite	O
which	O
will	O
hold	O
if	O
the	O
function	O
is	O
strictly	B
convex	B
if	O
not	O
the	O
objective	O
function	O
is	O
not	O
convex	B
then	O
hk	O
may	O
not	O
be	O
positive	O
definite	O
so	O
dk	O
h	O
k	O
gk	O
may	O
not	O
be	O
a	O
descent	O
direction	O
figure	O
for	O
an	O
example	O
in	O
this	O
case	O
one	O
simple	O
strategy	O
is	O
to	O
revert	O
to	O
steepest	B
descent	I
dk	O
gk	O
the	O
levenberg	B
marquardt	I
algorithm	O
is	O
an	O
adaptive	O
way	O
to	O
blend	O
between	O
newton	O
steps	O
and	O
steepest	B
descent	I
steps	O
this	O
method	O
is	O
widely	O
used	O
when	O
solving	O
nonlinear	O
least	B
squares	I
problems	O
an	O
alternative	O
approach	O
is	O
this	O
rather	O
than	O
computing	O
dk	O
h	O
k	O
gk	O
directly	O
we	O
can	O
solve	O
the	O
linear	O
system	O
of	O
equations	O
hkdk	O
gk	O
for	O
dk	O
using	O
conjugate	O
gradient	O
if	O
hk	O
is	O
not	O
positive	O
definite	O
we	O
can	O
simply	O
truncate	O
the	O
cg	O
iterations	O
as	O
soon	O
as	O
negative	O
curvature	O
is	O
detected	O
this	O
is	O
called	O
truncated	B
newton	I
iteratively	B
reweighted	I
least	B
squares	I
let	O
us	O
now	O
apply	O
newton	O
s	O
algorithm	O
to	O
find	O
the	O
mle	B
for	O
binary	O
logistic	B
regression	B
the	O
newton	O
update	O
at	O
iteration	O
k	O
for	O
this	O
model	O
is	O
as	O
follows	O
k	O
since	O
the	O
hessian	B
is	O
exact	O
wk	O
h	O
wk	O
skx	O
skx	O
skx	O
skx	O
k	O
skxwk	O
xt	O
k	O
y	O
k	O
skzk	O
where	O
we	O
have	O
defined	O
the	O
working	B
response	I
as	O
zk	O
xwk	O
s	O
k	O
k	O
model	O
fitting	O
equation	O
is	O
an	O
example	O
of	O
a	O
weighted	B
least	B
squares	I
problem	I
which	O
is	O
a	O
minimizer	O
of	O
skizki	O
wt	O
since	O
sk	O
is	O
a	O
diagonal	B
matrix	O
we	O
can	O
rewrite	O
the	O
targets	O
in	O
component	O
form	O
each	O
case	O
i	O
as	O
zki	O
wt	O
k	O
xi	O
yi	O
ki	O
ki	O
this	O
algorithm	O
is	O
known	O
as	O
iteratively	B
reweighted	I
least	B
squares	I
or	O
irls	B
for	O
short	O
since	O
at	O
each	O
iteration	O
we	O
solve	O
a	O
weighted	B
least	B
squares	I
problem	I
where	O
the	O
weight	O
matrix	O
sk	O
changes	O
at	O
each	O
iteration	O
see	O
algorithm	O
for	O
some	O
pseudocode	O
algorithm	O
iteratively	B
reweighted	I
least	B
squares	I
w	O
y	O
repeat	O
i	O
wt	O
xi	O
i	O
sigm	O
i	O
si	O
i	O
zi	O
i	O
yi	O
i	O
s	O
w	O
sx	O
si	O
sz	O
until	O
converged	O
quasi-newton	B
metric	B
methods	O
the	O
mother	O
of	O
all	O
second-order	O
optimization	B
algorithm	O
is	O
newton	O
s	O
algorithm	O
which	O
we	O
discussed	O
in	O
section	O
unfortunately	O
it	O
may	O
be	O
too	O
expensive	O
to	O
compute	O
h	O
explicitly	O
quasinewton	O
methods	O
iteratively	O
build	O
up	O
an	O
approximation	O
to	O
the	O
hessian	B
using	O
information	B
gleaned	O
from	O
the	O
gradient	O
vector	O
at	O
each	O
step	O
the	O
most	O
common	O
method	O
is	O
called	O
bfgs	B
after	O
its	O
inventors	O
broyden	O
fletcher	O
goldfarb	O
and	O
shanno	O
which	O
updates	O
the	O
approximation	O
to	O
the	O
hessian	B
bk	O
hk	O
as	O
follows	O
ykyt	O
k	O
yt	O
k	O
sk	O
sk	O
k	O
k	O
yk	O
gk	O
gk	O
bk	O
st	O
k	O
bksk	O
this	O
is	O
a	O
rank-two	O
update	O
to	O
the	O
matrix	O
and	O
ensures	O
that	O
the	O
matrix	O
remains	O
positive	O
definite	O
certain	O
restrictions	O
on	O
the	O
step	B
size	I
we	O
typically	O
start	O
with	O
a	O
diagonal	B
approximation	O
i	O
thus	O
bfgs	B
can	O
be	O
thought	O
of	O
as	O
a	O
diagonal	B
plus	O
low-rank	O
approximation	O
to	O
the	O
hessian	B
chapter	O
logistic	B
regression	B
alternatively	O
bfgs	B
can	O
iteratively	O
update	O
an	O
approximation	O
to	O
the	O
inverse	O
hessian	B
ck	O
h	O
k	O
as	O
follows	O
i	O
skyt	O
k	O
yt	O
k	O
sk	O
ck	O
i	O
ykst	O
k	O
yt	O
k	O
sk	O
skst	O
k	O
yt	O
k	O
sk	O
since	O
storing	O
the	O
hessian	B
takes	O
space	O
for	O
very	O
large	O
problems	O
one	O
can	O
use	O
limited	B
memory	I
bfgs	B
orl-bfgs	O
where	O
hk	O
or	O
h	O
is	O
approximated	O
by	O
a	O
diagonal	B
plus	O
low	O
rank	O
matrix	O
in	O
particular	O
the	O
product	O
h	O
k	O
gk	O
can	O
be	O
obtained	O
by	O
performing	O
a	O
sequence	O
of	O
inner	O
products	O
with	O
sk	O
and	O
yk	O
using	O
only	O
the	O
m	O
most	O
recent	O
yk	O
pairs	O
and	O
ignoring	O
older	O
information	B
the	O
storage	O
requirements	O
are	O
therefore	O
omd	O
typically	O
m	O
suffices	O
for	O
good	O
performance	O
see	O
and	O
wright	O
for	O
more	O
information	B
l-bfgs	B
is	O
often	O
the	O
method	O
of	O
choice	O
for	O
most	O
unconstrained	O
smooth	O
optimization	B
problems	O
that	O
arise	O
in	O
machine	B
learning	B
see	O
section	O
k	O
regularization	B
just	O
as	O
we	O
prefer	O
ridge	B
regression	B
to	O
linear	B
regression	B
so	O
we	O
should	O
prefer	O
map	O
estimation	O
for	O
logistic	B
regression	B
to	O
computing	O
the	O
mle	B
in	O
fact	O
regularization	B
is	O
important	O
in	O
the	O
classification	O
setting	O
even	O
if	O
we	O
have	O
lots	O
of	O
data	O
to	O
see	O
why	O
suppose	O
the	O
data	O
is	O
linearly	B
separable	I
in	O
this	O
case	O
the	O
mle	B
is	O
obtained	O
when	O
corresponding	O
to	O
an	O
infinitely	O
steep	O
sigmoid	B
function	O
iwt	O
x	O
also	O
known	O
as	O
a	O
linear	B
threshold	I
unit	I
this	O
assigns	O
the	O
maximal	O
amount	O
of	O
probability	O
mass	O
to	O
the	O
training	O
data	O
however	O
such	O
a	O
solution	O
is	O
very	O
brittle	O
and	O
will	O
not	O
generalize	B
well	O
to	O
prevent	O
this	O
we	O
can	O
use	O
regularization	B
just	O
as	O
we	O
did	O
with	O
ridge	B
regression	B
we	O
note	O
that	O
the	O
new	O
objective	O
gradient	O
and	O
hessian	B
have	O
the	O
following	O
forms	O
nllw	O
w	O
t	O
w	O
w	O
w	O
i	O
it	O
is	O
a	O
simple	O
matter	O
to	O
pass	O
these	O
modified	O
equations	O
into	O
any	O
gradient-based	O
optimizer	O
multi-class	B
logistic	B
regression	B
now	O
we	O
consider	O
multinomial	B
logistic	B
regression	B
sometimes	O
called	O
a	O
maximum	B
entropy	B
classifier	O
this	O
is	O
a	O
model	O
of	O
the	O
form	O
py	O
cx	O
w	O
expwt	O
c	O
x	O
expwt	O
x	O
a	O
slight	O
variant	O
known	O
as	O
a	O
conditional	B
logit	B
model	I
normalizes	O
over	O
a	O
different	O
set	O
of	O
classes	O
for	O
each	O
data	O
case	O
this	O
can	O
be	O
useful	O
for	O
modeling	O
choices	O
that	O
users	O
make	O
between	O
different	O
sets	O
of	O
items	O
that	O
are	O
offered	O
to	O
them	O
let	O
us	O
now	O
introduce	O
some	O
notation	O
let	O
ic	O
pyi	O
cxi	O
w	O
s	O
ic	O
where	O
i	O
wt	O
xi	O
is	O
a	O
c	O
vector	O
also	O
let	O
yic	O
iyi	O
c	O
be	O
the	O
one-of-c	B
encoding	I
of	O
yi	O
thus	O
yi	O
is	O
a	O
bit	O
vector	O
in	O
which	O
the	O
c	O
th	O
bit	O
turns	O
on	O
iff	B
yi	O
c	O
following	O
et	O
al	O
let	O
us	O
model	O
fitting	O
set	O
wc	O
to	O
ensure	O
identifiability	O
and	O
define	O
w	O
vecw	O
to	O
be	O
a	O
d	O
column	O
vector	O
with	O
this	O
the	O
log-likelihood	O
can	O
be	O
written	O
as	O
yic	O
ic	O
yic	O
log	O
ic	O
log	O
yicwt	O
c	O
xi	O
expwt	O
xi	O
log	O
define	O
the	O
nll	B
as	O
f	O
we	O
now	O
proceed	O
to	O
compute	O
the	O
gradient	O
and	O
hessian	B
of	O
this	O
expression	O
since	O
w	O
is	O
blockit	O
helps	O
to	O
define	O
a	O
b	O
structured	O
the	O
notation	O
gets	O
a	O
bit	O
heavy	O
but	O
the	O
ideas	O
are	O
simple	O
be	O
the	O
kronecker	B
product	I
of	O
matrices	O
a	O
and	O
b	O
if	O
a	O
is	O
an	O
m	O
n	O
matrix	O
and	O
b	O
is	O
a	O
p	O
q	O
matrix	O
then	O
a	O
b	O
is	O
the	O
mp	O
nq	O
block	O
matrix	O
amnb	O
a	O
b	O
returning	O
to	O
the	O
task	O
at	O
hand	O
one	O
can	O
show	O
that	O
the	O
gradient	O
is	O
given	O
by	O
gw	O
f	O
i	O
yi	O
xi	O
where	O
yi	O
iyi	O
c	O
and	O
iw	O
w	O
pyi	O
c	O
w	O
are	O
column	O
vectors	O
of	O
length	O
c	O
for	O
example	O
if	O
we	O
have	O
d	O
feature	O
dimensions	O
and	O
c	O
classes	O
this	O
becomes	O
i	O
gw	O
wc	O
f	O
ic	O
yicxi	O
in	O
other	O
words	O
for	O
each	O
class	O
c	O
the	O
derivative	O
for	O
the	O
weights	O
in	O
the	O
c	O
th	O
column	O
is	O
i	O
this	O
has	O
the	O
same	O
form	O
as	O
in	O
the	O
binary	O
logistic	B
regression	B
case	O
namely	O
an	O
error	O
term	O
times	O
xi	O
turns	O
out	O
to	O
be	O
a	O
general	O
property	O
of	O
distributions	O
in	O
the	O
exponential	B
family	B
as	O
we	O
will	O
see	O
in	O
section	O
chapter	O
logistic	B
regression	B
one	O
can	O
also	O
show	O
that	O
the	O
hessian	B
is	O
the	O
following	O
block	O
structured	O
dc	O
dc	O
matrix	O
hw	O
for	O
example	O
if	O
we	O
have	O
features	B
and	O
classes	O
this	O
becomes	O
i	O
i	O
t	O
i	O
i	O
i	O
i	O
in	O
other	O
words	O
the	O
block	O
c	O
i	O
ic	O
i	O
submatrix	O
is	O
given	O
by	O
hw	O
where	O
xi	O
xixt	O
i	O
this	O
is	O
also	O
a	O
positive	O
definite	O
matrix	O
so	O
there	O
is	O
a	O
unique	O
mle	B
now	O
consider	O
minimizing	O
log	O
pdw	O
log	O
pw	O
where	O
pw	O
c	O
n	O
the	O
new	O
objective	O
its	O
gradient	O
and	O
hessian	B
are	O
given	O
by	O
w	O
c	O
v	O
c	O
c	O
wc	O
wcv	O
wc	O
this	O
can	O
be	O
passed	O
to	O
any	O
gradient-based	O
optimizer	O
to	O
find	O
the	O
map	B
estimate	I
note	O
however	O
that	O
the	O
hessian	B
has	O
size	O
o	O
which	O
is	O
c	O
times	O
more	O
row	O
and	O
columns	O
than	O
in	O
the	O
binary	O
case	O
so	O
limited	B
memory	I
bfgs	B
is	O
more	O
appropriate	O
than	O
newton	O
s	O
method	O
see	O
logregfit	O
for	O
some	O
matlab	O
code	O
bayesian	B
logistic	B
regression	B
it	O
is	O
natural	O
to	O
want	O
to	O
compute	O
the	O
full	B
posterior	O
over	O
the	O
parameters	O
pwd	O
for	O
logistic	B
regression	B
models	O
this	O
can	O
be	O
useful	O
for	O
any	O
situation	O
where	O
we	O
want	O
to	O
associate	O
confidence	O
intervals	O
with	O
our	O
predictions	O
this	O
is	O
necessary	O
when	O
solving	O
contextual	B
bandit	I
problems	O
discussed	O
in	O
section	O
unfortunately	O
unlike	O
the	O
linear	B
regression	B
case	O
this	O
cannot	O
be	O
done	O
exactly	O
since	O
there	O
is	O
no	O
convenient	O
conjugate	B
prior	I
for	O
logistic	B
regression	B
we	O
discuss	O
one	O
simple	O
approximation	O
below	O
some	O
other	O
approaches	O
include	O
mcmc	B
variational	B
inference	B
expectation	B
propagation	I
and	O
rasmussen	O
etc	O
for	O
notational	O
simplicity	O
we	O
stick	O
to	O
binary	O
logistic	B
regression	B
bayesian	B
logistic	B
regression	B
laplace	B
approximation	I
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
make	O
a	O
gaussian	B
approximation	I
to	O
a	O
posterior	O
distribution	O
the	O
approximation	O
works	O
as	O
follows	O
suppose	O
r	O
d	O
let	O
p	O
e	O
e	O
z	O
where	O
e	O
is	O
called	O
an	O
energy	B
function	I
and	O
is	O
equal	O
to	O
the	O
negative	O
log	O
of	O
the	O
unnormalized	O
log	O
posterior	O
e	O
log	O
p	O
with	O
z	O
pd	O
being	O
the	O
normalization	O
constant	O
performing	O
a	O
taylor	B
series	I
expansion	I
around	O
the	O
mode	B
the	O
lowest	O
energy	O
state	B
we	O
get	O
h	O
where	O
g	O
is	O
the	O
gradient	O
and	O
h	O
is	O
the	O
hessian	B
of	O
the	O
energy	B
function	I
evaluated	O
at	O
the	O
mode	B
g	O
e	O
e	O
h	O
t	O
g	O
e	O
since	O
is	O
the	O
mode	B
the	O
gradient	O
term	O
is	O
zero	O
hence	O
p	O
z	O
e	O
e	O
exp	O
n	O
h	O
h	O
z	O
pd	O
p	O
e	O
e	O
the	O
last	O
line	O
follows	O
from	O
normalization	O
constant	O
of	O
the	O
multivariate	B
gaussian	B
equation	O
is	O
known	O
as	O
the	O
laplace	B
approximation	I
to	O
the	O
marginal	B
likelihood	B
therefore	O
equation	O
is	O
sometimes	O
called	O
the	O
the	O
laplace	B
approximation	I
to	O
the	O
posterior	O
however	O
in	O
the	O
statistics	O
community	O
the	O
term	O
laplace	B
approximation	I
refers	O
to	O
a	O
more	O
sophisticated	O
method	O
e	O
g	O
it	O
may	O
therefore	O
be	O
better	O
to	O
use	O
the	O
term	O
gaussian	B
approximation	I
to	O
refer	O
to	O
equation	O
a	O
gaussian	B
approximation	I
is	O
often	O
a	O
reasonable	O
approximation	O
since	O
posteriors	O
often	O
become	O
more	O
gaussian-like	O
as	O
the	O
sample	O
size	O
increases	O
for	O
reasons	O
analogous	O
to	O
the	O
central	O
physics	O
there	O
is	O
an	O
analogous	O
technique	O
known	O
as	O
a	O
saddle	B
point	I
approximation	I
et	O
al	O
for	O
details	O
limit	O
theorem	O
derivation	O
of	O
the	O
bic	B
we	O
can	O
use	O
the	O
gaussian	B
approximation	I
to	O
write	O
the	O
log	O
marginal	B
likelihood	B
as	O
follows	O
dropping	O
irrelevant	O
constants	O
log	O
pd	O
log	O
pd	O
log	O
p	O
log	O
the	O
penalization	O
terms	O
which	O
are	O
added	O
to	O
the	O
log	O
pd	O
are	O
sometimes	O
called	O
the	O
occam	B
factor	B
and	O
are	O
a	O
measure	O
of	O
model	O
complexity	O
if	O
we	O
have	O
a	O
uniform	O
prior	O
p	O
we	O
can	O
drop	O
the	O
second	O
term	O
and	O
replace	O
with	O
the	O
mle	B
chapter	O
logistic	B
regression	B
we	O
now	O
focus	O
on	O
approximating	O
the	O
third	O
term	O
we	O
have	O
h	O
log	O
pdi	O
let	O
us	O
approximate	O
each	O
hi	O
by	O
a	O
fixed	O
matrix	O
h	O
then	O
we	O
have	O
hi	O
where	O
hi	O
log	O
log	O
h	O
logn	O
d	O
h	O
d	O
log	O
n	O
log	O
h	O
where	O
d	O
dim	O
and	O
we	O
have	O
assumed	O
h	O
is	O
full	B
rank	O
we	O
can	O
drop	O
the	O
log	O
h	O
term	O
since	O
it	O
is	O
independent	O
of	O
n	O
and	O
thus	O
will	O
get	O
overwhelmed	O
by	O
the	O
likelihood	B
putting	O
all	O
the	O
pieces	O
together	O
we	O
recover	O
the	O
bic	B
score	O
log	O
pd	O
log	O
pd	O
d	O
log	O
n	O
gaussian	B
approximation	I
for	O
logistic	B
regression	B
now	O
let	O
us	O
apply	O
the	O
gaussian	B
approximation	I
to	O
logistic	B
regression	B
we	O
will	O
use	O
a	O
a	O
gaussian	B
prior	O
of	O
the	O
form	O
pw	O
n	O
just	O
as	O
we	O
did	O
in	O
map	O
estimation	O
the	O
approximate	O
posterior	O
is	O
given	O
by	O
pwd	O
n	O
w	O
h	O
where	O
w	O
arg	O
minw	O
ew	O
ew	O
pdw	O
log	O
pw	O
and	O
h	O
w	O
as	O
an	O
example	O
consider	O
the	O
linearly	B
separable	I
data	O
in	O
figure	O
there	O
are	O
many	O
parameter	B
settings	O
that	O
correspond	O
to	O
lines	O
that	O
perfectly	O
separate	O
the	O
training	O
data	O
we	O
show	O
examples	O
the	O
likelihood	B
surface	O
is	O
shown	O
in	O
figure	O
where	O
we	O
see	O
that	O
the	O
likelihood	B
is	O
unbounded	O
as	O
we	O
move	O
up	O
and	O
to	O
the	O
right	O
in	O
parameter	B
space	O
along	O
a	O
ridge	O
where	O
is	O
indicated	O
by	O
the	O
diagonal	B
line	O
the	O
reasons	O
for	O
this	O
is	O
that	O
we	O
can	O
maximize	O
the	O
likelihood	B
by	O
driving	O
to	O
infinity	O
to	O
being	O
on	O
this	O
line	O
since	O
large	O
regression	B
weights	O
make	O
the	O
sigmoid	B
function	O
very	O
steep	O
turning	O
it	O
into	O
a	O
step	O
function	O
consequently	O
the	O
mle	B
is	O
not	O
well	O
defined	O
when	O
the	O
data	O
is	O
linearly	B
separable	I
to	O
regularize	O
the	O
problem	O
let	O
us	O
use	O
a	O
vague	O
spherical	B
prior	O
centered	O
at	O
the	O
origin	O
n	O
multiplying	O
this	O
spherical	B
prior	O
by	O
the	O
likelihood	B
surface	O
results	O
in	O
a	O
highly	O
skewed	O
posterior	O
shown	O
in	O
figure	O
posterior	O
is	O
skewed	O
because	O
the	O
likelihood	B
function	O
chops	O
off	O
regions	O
of	O
parameter	B
space	O
a	O
soft	O
fashion	O
which	O
disagree	O
with	O
the	O
data	O
the	O
map	B
estimate	I
is	O
shown	O
by	O
the	O
blue	O
dot	O
unlike	O
the	O
mle	B
this	O
is	O
not	O
at	O
infinity	O
the	O
gaussian	B
approximation	I
to	O
this	O
posterior	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
this	O
is	O
a	O
symmetric	B
distribution	O
and	O
therefore	O
not	O
a	O
great	O
approximation	O
of	O
course	O
it	O
gets	O
the	O
mode	B
correct	O
construction	O
and	O
it	O
at	O
least	O
represents	O
the	O
fact	O
that	O
there	O
is	O
more	O
uncertainty	B
along	O
the	O
southwest-northeast	O
direction	O
corresponds	O
to	O
uncertainty	B
about	O
the	O
orientation	O
of	O
separating	O
lines	O
than	O
perpendicular	O
to	O
this	O
although	O
a	O
crude	O
approximation	O
this	O
is	O
surely	O
better	O
than	O
approximating	O
the	O
posterior	O
by	O
a	O
delta	O
function	O
which	O
is	O
what	O
map	O
estimation	O
does	O
approximating	O
the	O
posterior	O
predictive	B
given	O
the	O
posterior	O
we	O
can	O
compute	O
credible	O
intervals	O
perform	O
hypothesis	O
tests	O
etc	O
just	O
as	O
we	O
did	O
in	O
section	O
in	O
the	O
case	O
of	O
linear	B
regression	B
but	O
in	O
machine	B
learning	B
interest	O
usually	O
focusses	O
on	O
prediction	O
the	O
posterior	B
predictive	B
distribution	I
has	O
the	O
form	O
pyxd	O
pyx	O
wpwddw	O
bayesian	B
logistic	B
regression	B
data	O
log	O
likelihood	B
log	O
unnormalised	O
posterior	O
laplace	B
approximation	I
to	O
posterior	O
figure	O
two-class	O
data	O
in	O
log-likelihood	O
for	O
a	O
logistic	B
regression	B
model	O
the	O
line	O
is	O
drawn	O
from	O
the	O
origin	O
in	O
the	O
direction	O
of	O
the	O
mle	B
is	O
at	O
infinity	O
the	O
numbers	O
correspond	O
to	O
points	O
in	O
parameter	B
space	O
corresponding	O
to	O
the	O
lines	O
in	O
unnormalized	O
log	O
posterior	O
vague	O
spherical	B
prior	O
laplace	B
approximation	I
to	O
posterior	O
based	O
on	O
a	O
figure	O
by	O
mark	O
girolami	O
figure	O
generated	O
by	O
logreglaplacegirolamidemo	O
unfortunately	O
this	O
integral	O
is	O
intractable	O
the	O
simplest	O
approximation	O
is	O
the	O
plug-in	B
approximation	I
which	O
in	O
the	O
binary	O
case	O
takes	O
the	O
form	O
py	O
py	O
e	O
where	O
e	O
is	O
the	O
posterior	B
mean	B
in	O
this	O
context	O
e	O
is	O
called	O
the	O
bayes	B
point	I
of	O
course	O
such	O
a	O
plug-in	B
estimate	O
underestimates	O
the	O
uncertainty	B
we	O
discuss	O
some	O
better	O
approximations	O
below	O
wmap	O
decision	B
boundary	I
for	O
sampled	O
w	O
chapter	O
logistic	B
regression	B
numerical	O
approx	O
of	O
mc	O
approx	O
of	O
figure	O
posterior	B
predictive	B
distribution	I
for	O
a	O
logistic	B
regression	B
model	O
in	O
top	O
left	O
contours	O
of	O
py	O
wmap	O
top	O
right	O
samples	B
from	O
the	O
posterior	B
predictive	B
distribution	I
bottom	O
left	O
averaging	O
over	O
these	O
samples	B
bottom	O
right	O
moderated	B
output	I
approximation	O
based	O
on	O
a	O
figure	O
by	O
mark	O
girolami	O
figure	O
generated	O
by	O
logreglaplacegirolamidemo	O
monte	B
carlo	I
approximation	O
a	O
better	O
approach	O
is	O
to	O
use	O
a	O
monte	B
carlo	I
approximation	O
as	O
follows	O
py	O
s	O
sigmwst	O
x	O
where	O
ws	O
pwd	O
are	O
samples	B
from	O
the	O
posterior	O
technique	O
can	O
be	O
trivially	O
extended	O
to	O
the	O
multi-class	O
case	O
if	O
we	O
have	O
approximated	O
the	O
posterior	O
using	O
monte	B
carlo	I
we	O
can	O
reuse	O
these	O
samples	B
for	O
prediction	O
if	O
we	O
made	O
a	O
gaussian	B
approximation	I
to	O
the	O
posterior	O
we	O
can	O
draw	O
independent	O
samples	B
from	O
the	O
gaussian	B
using	O
standard	O
methods	O
figure	O
shows	O
samples	B
from	O
the	O
posteiror	O
predictive	B
for	O
our	O
example	O
figure	O
bayesian	B
logistic	B
regression	B
sigmoid	B
probit	B
figure	O
posterior	B
predictive	B
density	I
for	O
sat	O
data	O
the	O
red	O
circle	O
denotes	O
the	O
posterior	B
mean	B
the	O
blue	O
cross	O
the	O
posterior	B
median	B
and	O
the	O
blue	O
lines	O
denote	O
the	O
and	O
percentiles	O
of	O
the	O
predictive	B
the	O
logistic	B
function	O
sigmx	O
in	O
distribution	O
figure	O
generated	O
by	O
logregsatdemobayes	O
solid	O
red	O
with	O
the	O
rescaled	O
probit	B
function	O
x	O
in	O
dotted	O
blue	O
superimposed	O
here	O
which	O
was	O
chosen	O
so	O
that	O
the	O
derivatives	O
of	O
the	O
two	O
curves	O
match	O
at	O
x	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
probitplot	O
figure	O
generated	O
by	O
probitregdemo	O
shows	O
the	O
average	O
of	O
these	O
samples	B
by	O
averaging	O
over	O
multiple	O
predictions	O
we	O
see	O
that	O
the	O
uncertainty	B
in	O
the	O
decision	B
boundary	I
splays	O
out	O
as	O
we	O
move	O
further	O
from	O
the	O
training	O
data	O
so	O
although	O
the	O
decision	B
boundary	I
is	O
linear	O
the	O
posterior	B
predictive	B
density	I
is	O
not	O
linear	O
note	O
also	O
that	O
the	O
posterior	B
mean	B
decision	B
boundary	I
is	O
roughly	O
equally	O
far	O
from	O
both	O
classes	O
this	O
is	O
the	O
bayesian	B
analog	O
of	O
the	O
large	B
margin	B
principle	I
discussed	O
in	O
section	O
figure	O
shows	O
an	O
example	O
in	O
the	O
red	O
dots	O
denote	O
the	O
mean	B
of	O
the	O
posterior	O
predictive	B
evaluated	O
at	O
the	O
training	O
data	O
the	O
vertical	O
blue	O
lines	O
denote	O
credible	O
intervals	O
for	O
the	O
posterior	O
predictive	B
the	O
small	O
blue	O
star	O
is	O
the	O
median	B
we	O
see	O
that	O
with	O
the	O
bayesian	B
approach	O
we	O
are	O
able	O
to	O
model	O
our	O
uncertainty	B
about	O
the	O
probability	O
a	O
student	O
will	O
pass	O
the	O
exam	O
based	O
on	O
his	O
sat	O
score	O
rather	O
than	O
just	O
getting	O
a	O
point	B
estimate	I
probit	B
approximation	O
output	O
if	O
we	O
have	O
a	O
gaussian	B
approximation	I
to	O
the	O
posterior	O
pwd	O
n	O
vn	O
we	O
can	O
also	O
compute	O
a	O
deterministic	O
approximation	O
to	O
the	O
posterior	B
predictive	B
distribution	I
at	O
least	O
in	O
the	O
binary	O
case	O
we	O
proceed	O
as	O
follows	O
py	O
sigmwt	O
xpwddw	O
sigman	O
a	O
ada	O
a	O
wt	O
x	O
a	O
e	O
mt	O
n	O
x	O
e	O
a	O
var	B
pwdwt	O
n	O
xt	O
vn	O
x	O
chapter	O
logistic	B
regression	B
thus	O
we	O
see	O
that	O
we	O
need	O
to	O
evaluate	O
the	O
expectation	O
of	O
a	O
sigmoid	B
with	O
respect	O
to	O
a	O
gaussian	B
this	O
can	O
be	O
approximated	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
sigmoid	B
function	O
is	O
similar	B
to	O
the	O
probit	B
function	O
which	O
is	O
given	O
by	O
the	O
cdf	B
of	O
the	O
standard	B
normal	B
n	O
a	O
figure	O
plots	O
the	O
sigmoid	B
and	O
probit	B
functions	O
we	O
have	O
rescaled	O
the	O
axes	O
so	O
that	O
sigma	O
has	O
the	O
same	O
slope	O
as	O
a	O
at	O
the	O
origin	O
where	O
the	O
advantage	O
of	O
using	O
the	O
probit	B
is	O
that	O
one	O
can	O
convolve	O
it	O
with	O
a	O
gaussian	B
analytically	O
an	O
a	O
we	O
now	O
plug	O
in	O
the	O
approximation	O
sigma	O
a	O
to	O
both	O
sides	O
of	O
this	O
equation	O
to	O
get	O
sigman	O
sigm	O
applying	O
this	O
to	O
the	O
logistic	B
regression	B
model	O
we	O
get	O
the	O
following	O
expression	O
suggested	O
in	O
and	O
lauritzen	O
py	O
sigm	O
a	O
a	O
figure	O
indicates	O
that	O
this	O
gives	O
very	O
similar	B
results	O
to	O
the	O
monte	B
carlo	I
approximation	O
the	O
plug-in	B
estimate	O
to	O
see	O
this	O
note	O
that	O
and	O
hence	O
using	O
equation	O
is	O
sometimes	O
called	O
a	O
moderated	B
output	I
since	O
it	O
is	O
less	O
extreme	O
than	O
sigm	O
sigm	O
py	O
w	O
where	O
the	O
inequality	O
is	O
strict	B
if	O
if	O
we	O
have	O
py	O
w	O
but	O
the	O
moderated	O
prediction	O
is	O
always	O
closer	O
to	O
so	O
it	O
is	O
less	O
confident	O
however	O
the	O
decision	B
boundary	I
occurs	O
whenever	O
py	O
sigm	O
which	O
implies	O
wt	O
x	O
hence	O
the	O
decision	B
boundary	I
for	O
the	O
moderated	O
approximation	O
is	O
the	O
same	O
as	O
for	O
the	O
plug-in	B
approximation	I
so	O
the	O
number	O
of	O
misclassifications	O
will	O
be	O
the	O
same	O
for	O
the	O
two	O
methods	O
but	O
the	O
log-likelihood	O
will	O
not	O
that	O
in	O
the	O
multiclass	O
case	O
taking	O
into	O
account	O
posterior	O
covariance	B
gives	O
different	O
answers	O
than	O
the	O
plug-in	B
approach	O
see	O
exercise	O
of	O
and	O
williams	O
residual	B
analysis	I
detection	O
it	O
is	O
sometimes	O
useful	O
to	O
detect	O
data	O
cases	O
which	O
are	O
outliers	B
this	O
is	O
called	O
residual	B
analysis	I
or	O
case	B
analysis	I
in	O
a	O
regression	B
setting	O
this	O
can	O
be	O
performed	O
by	O
computing	O
ri	O
yi	O
yi	O
where	O
yi	O
wt	O
xi	O
these	O
values	O
should	O
follow	O
a	O
n	O
distribution	O
if	O
the	O
modelling	O
assumptions	O
are	O
correct	O
this	O
can	O
be	O
assessed	O
by	O
creating	O
a	O
qq-plot	B
where	O
we	O
plot	O
the	O
n	O
theoretical	O
quantiles	O
of	O
a	O
gaussian	B
distribution	O
against	O
the	O
n	O
empirical	O
quantiles	O
of	O
the	O
ri	O
points	O
that	O
deviate	O
from	O
the	O
straightline	O
are	O
potential	O
outliers	B
online	B
learning	B
and	O
stochastic	B
optimization	B
classical	B
methods	O
based	O
on	O
residuals	O
do	O
not	O
work	O
well	O
for	O
binary	O
data	O
because	O
they	O
rely	O
on	O
asymptotic	O
normality	O
of	O
the	O
test	O
statistics	O
however	O
adopting	O
a	O
bayesian	B
approach	O
we	O
can	O
just	O
define	O
outliers	B
to	O
be	O
points	O
which	O
which	O
pyi	O
yi	O
is	O
small	O
where	O
we	O
typically	O
use	O
yi	O
sigm	O
wt	O
xi	O
note	O
that	O
w	O
was	O
estimated	O
from	O
all	O
the	O
data	O
a	O
better	O
method	O
is	O
to	O
exclude	O
yi	O
from	O
the	O
estimate	O
of	O
w	O
when	O
predicting	O
yi	O
that	O
is	O
we	O
define	O
outliers	B
to	O
be	O
points	O
which	O
have	O
low	O
probability	O
under	O
the	O
cross-validated	O
posterior	B
predictive	B
distribution	I
defined	O
by	O
pyixi	O
x	O
i	O
y	O
i	O
pyixi	O
w	O
wpwdw	O
this	O
can	O
be	O
efficiently	O
approximated	O
by	O
sampling	O
methods	O
for	O
further	O
discussion	O
of	O
residual	B
analysis	I
in	O
logistic	B
regression	B
models	O
see	O
e	O
g	O
johnson	O
and	O
albert	O
sec	O
online	B
learning	B
and	O
stochastic	B
optimization	B
traditionally	O
machine	B
learning	B
is	O
performed	O
offline	B
which	O
means	O
we	O
have	O
a	O
batch	B
of	O
data	O
and	O
we	O
optimize	O
an	O
equation	O
of	O
the	O
following	O
form	O
f	O
n	O
f	O
zi	O
where	O
zi	O
yi	O
in	O
the	O
supervised	O
case	O
or	O
just	O
xi	O
in	O
the	O
unsupervised	O
case	O
and	O
f	O
zi	O
is	O
some	O
kind	O
of	O
loss	B
function	I
for	O
example	O
we	O
might	O
use	O
f	O
zi	O
log	O
pyixi	O
in	O
which	O
case	O
we	O
are	O
trying	O
to	O
maximize	O
the	O
likelihood	B
alternatively	O
we	O
might	O
use	O
f	O
zi	O
lyi	O
hxi	O
where	O
hxi	O
is	O
a	O
prediction	O
function	O
and	O
ly	O
y	O
is	O
some	O
other	O
loss	B
function	I
such	O
as	O
squared	B
error	I
or	O
the	O
huber	B
loss	B
in	O
frequentist	B
decision	B
theory	O
the	O
average	O
loss	B
is	O
called	O
the	O
risk	B
section	O
so	O
this	O
overall	O
approach	O
is	O
called	O
empirical	B
risk	B
minimization	I
or	O
erm	B
section	O
for	O
details	O
however	O
if	O
we	O
have	O
streaming	B
data	I
we	O
need	O
to	O
perform	O
online	B
learning	B
so	O
we	O
can	O
update	O
our	O
estimates	O
as	O
each	O
new	O
data	O
point	O
arrives	O
rather	O
than	O
waiting	O
until	O
the	O
end	O
may	O
never	O
occur	O
and	O
even	O
if	O
we	O
have	O
a	O
batch	B
of	O
data	O
we	O
might	O
want	O
to	O
treat	O
it	O
like	O
a	O
stream	O
if	O
it	O
is	O
too	O
large	O
to	O
hold	O
in	O
main	O
memory	O
below	O
we	O
discuss	O
learning	B
methods	O
for	O
this	O
kind	O
of	O
a	O
simple	O
implementation	O
trick	O
can	O
be	O
used	O
to	O
speed	O
up	O
batch	B
learning	B
algorithms	O
when	O
applied	O
to	O
data	O
sets	O
that	O
are	O
too	O
large	O
to	O
hold	O
in	O
memory	O
first	O
note	O
that	O
the	O
naive	O
implementation	O
makes	O
a	O
pass	O
over	O
the	O
data	O
file	O
from	O
the	O
beginning	O
to	O
end	O
accumulating	O
the	O
sufficient	B
statistics	I
and	O
gradients	O
as	O
it	O
goes	O
then	O
an	O
update	O
is	O
performed	O
and	O
the	O
process	O
repeats	O
unfortunately	O
at	O
the	O
end	O
of	O
each	O
pass	O
the	O
data	O
from	O
the	O
beginning	O
of	O
the	O
file	O
will	O
have	O
been	O
evicted	O
from	O
the	O
cache	O
are	O
are	O
assuming	O
it	O
cannot	O
all	O
fit	O
into	O
memory	O
rather	O
than	O
going	O
back	O
to	O
the	O
beginning	O
of	O
the	O
file	O
and	O
reloading	O
it	O
we	O
can	O
simply	O
work	O
backwards	O
from	O
the	O
end	O
of	O
the	O
file	O
which	O
is	O
already	O
in	O
memory	O
we	O
then	O
repeat	O
this	O
forwards-backwards	B
pattern	B
over	O
the	O
data	O
this	O
simple	O
trick	O
is	O
known	O
as	O
rocking	B
chapter	O
logistic	B
regression	B
online	B
learning	B
and	O
regret	B
minimization	O
suppose	O
that	O
at	O
each	O
step	O
nature	O
presents	O
a	O
sample	O
zk	O
and	O
the	O
learner	O
must	O
respond	O
with	O
a	O
parameter	B
estimate	O
k	O
in	O
the	O
theoretical	O
machine	B
learning	B
community	O
the	O
objective	O
used	O
in	O
online	B
learning	B
is	O
the	O
regret	B
which	O
is	O
the	O
averaged	O
loss	B
incurred	O
relative	O
to	O
the	O
best	O
we	O
could	O
have	O
gotten	O
in	O
hindsight	B
using	O
a	O
single	O
fixed	O
parameter	B
value	O
regretk	O
k	O
f	O
t	O
zt	O
min	O
k	O
f	O
zt	O
for	O
example	O
imagine	O
we	O
are	O
investing	O
in	O
the	O
stock-market	O
let	O
j	O
be	O
the	O
amount	O
we	O
invest	O
in	O
stock	O
j	O
and	O
let	O
zj	O
be	O
the	O
return	O
on	O
this	O
stock	O
our	O
loss	B
function	I
is	O
f	O
z	O
t	O
z	O
the	O
regret	B
is	O
how	O
much	O
better	O
worse	O
we	O
did	O
by	O
trading	O
at	O
each	O
step	O
rather	O
than	O
adopting	O
a	O
buy	O
and	O
hold	O
strategy	O
using	O
an	O
oracle	O
to	O
choose	O
which	O
stocks	O
to	O
buy	O
is	O
as	O
follows	O
at	O
each	O
step	O
k	O
update	O
the	O
parameters	O
using	O
one	O
simple	O
algorithm	O
for	O
online	B
learning	B
is	O
online	B
gradient	B
descent	I
which	O
proj	O
k	O
kgk	O
where	O
projv	O
argminw	O
v	O
is	O
the	O
projection	B
of	O
vector	O
v	O
onto	O
space	O
v	O
gk	O
f	O
k	O
zk	O
is	O
the	O
gradient	O
and	O
k	O
is	O
the	O
step	B
size	I
projection	B
step	O
is	O
only	O
needed	O
if	O
d	O
see	O
section	O
for	O
the	O
parameter	B
must	O
be	O
constrained	O
to	O
live	O
in	O
a	O
certain	O
subset	O
of	O
r	O
details	O
below	O
we	O
will	O
see	O
how	O
this	O
approach	O
to	O
regret	B
minimization	O
relates	O
to	O
more	O
traditional	O
objectives	O
such	O
as	O
mle	B
there	O
are	O
a	O
variety	O
of	O
other	O
approaches	O
to	O
regret	B
minimization	O
which	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
e	O
g	O
cesa-bianchi	O
and	O
lugosi	O
for	O
details	O
stochastic	B
optimization	B
and	O
risk	B
minimization	O
now	O
suppose	O
that	O
instead	O
of	O
minimizing	O
regret	B
with	O
respect	O
to	O
the	O
past	O
we	O
want	O
to	O
minimize	O
expected	O
loss	B
in	O
the	O
future	O
as	O
is	O
more	O
common	O
in	O
statistical	B
learning	B
theory	I
that	O
is	O
we	O
want	O
to	O
minimize	O
f	O
e	O
z	O
where	O
the	O
expectation	O
is	O
taken	O
over	O
future	O
data	O
optimizing	O
functions	O
where	O
some	O
of	O
the	O
variables	O
in	O
the	O
objective	O
are	O
random	O
is	O
called	O
stochastic	O
suppose	O
we	O
receive	O
an	O
infinite	O
stream	O
of	O
samples	B
from	O
the	O
distribution	O
one	O
way	O
to	O
optimize	O
stochastic	O
objectives	O
such	O
as	O
equation	O
is	O
to	O
perform	O
the	O
update	O
in	O
equation	O
at	O
each	O
step	O
this	O
is	O
called	O
stochastic	B
gradient	B
descent	I
or	O
sgd	B
and	O
yudin	O
since	O
we	O
typically	O
want	O
a	O
single	O
parameter	B
estimate	O
we	O
can	O
use	O
a	O
running	O
average	O
t	O
k	O
k	O
note	O
that	O
in	O
stochastic	B
optimization	B
the	O
objective	O
is	O
stochastic	O
and	O
therefore	O
the	O
algorithms	O
will	O
be	O
too	O
however	O
it	O
is	O
also	O
possible	O
to	O
apply	O
stochastic	B
optimization	B
algorithms	O
to	O
deterministic	O
objectives	O
examples	O
include	O
simulated	B
annealing	B
and	O
stochastic	B
gradient	B
descent	I
applied	O
to	O
the	O
empirical	B
risk	B
minimization	I
problem	O
there	O
are	O
some	O
interesting	O
theoretical	O
connections	O
between	O
online	B
learning	B
and	O
stochastic	B
optimization	B
and	O
lugosi	O
but	O
this	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
online	B
learning	B
and	O
stochastic	B
optimization	B
this	O
is	O
called	O
polyak-ruppert	B
averaging	I
and	O
can	O
be	O
implemented	O
recursively	O
as	O
follows	O
k	O
k	O
k	O
k	O
k	O
see	O
e	O
g	O
kushner	O
and	O
yin	O
for	O
details	O
setting	O
the	O
step	B
size	I
we	O
now	O
discuss	O
some	O
sufficient	O
conditions	O
on	O
the	O
learning	B
rate	B
to	O
guarantee	O
convergence	O
of	O
sgd	B
these	O
are	O
known	O
as	O
the	O
robbins-monro	B
conditions	O
k	O
k	O
the	O
set	O
of	O
values	O
of	O
k	O
over	O
time	O
is	O
called	O
the	O
learning	B
rate	B
schedule	B
various	O
formulas	O
are	O
used	O
such	O
as	O
k	O
or	O
the	O
following	O
bach	O
and	O
moulines	O
k	O
k	O
where	O
slows	O
down	O
early	O
iterations	O
of	O
the	O
algorithm	O
and	O
controls	O
the	O
rate	B
at	O
which	O
old	O
values	O
of	O
are	O
forgotten	O
the	O
need	O
to	O
adjust	O
these	O
tuning	O
parameters	O
is	O
one	O
of	O
the	O
main	O
drawback	O
of	O
stochastic	B
optimization	B
one	O
simple	O
heuristic	O
is	O
as	O
follows	O
store	O
an	O
initial	O
subset	O
of	O
the	O
data	O
and	O
try	O
a	O
range	O
of	O
values	O
on	O
this	O
subset	O
then	O
choose	O
the	O
one	O
that	O
results	O
in	O
the	O
fastest	O
decrease	O
in	O
the	O
objective	O
and	O
apply	O
it	O
to	O
all	O
the	O
rest	O
of	O
the	O
data	O
note	O
that	O
this	O
may	O
not	O
result	O
in	O
convergence	O
but	O
the	O
algorithm	O
can	O
be	O
terminated	O
when	O
the	O
performance	O
improvement	O
on	O
a	O
hold-out	O
set	O
plateaus	O
is	O
called	O
early	B
stopping	I
per-parameter	O
step	O
sizes	O
one	O
drawback	O
of	O
sgd	B
is	O
that	O
it	O
uses	O
the	O
same	O
step	B
size	I
for	O
all	O
parameters	O
we	O
now	O
briefly	O
present	O
a	O
method	O
known	O
as	O
adagrad	B
for	O
adaptive	O
gradient	O
et	O
al	O
which	O
is	O
similar	B
in	O
spirit	O
to	O
a	O
diagonal	B
hessian	B
approximation	O
also	O
et	O
al	O
for	O
a	O
similar	B
in	O
particular	O
if	O
ik	O
is	O
parameter	B
i	O
at	O
time	O
k	O
and	O
gik	O
is	O
its	O
gradient	O
then	O
we	O
approach	O
make	O
an	O
update	O
as	O
follows	O
ik	O
ik	O
gik	O
sik	O
where	O
the	O
diagonal	B
step	B
size	I
vector	O
is	O
the	O
gradient	O
vector	O
squared	O
summed	O
over	O
all	O
time	O
steps	O
this	O
can	O
be	O
recursively	O
updated	O
as	O
follows	O
sik	O
sik	O
the	O
result	O
is	O
a	O
per-parameter	O
step	B
size	I
that	O
adapts	O
to	O
the	O
curvature	O
of	O
the	O
loss	B
function	I
this	O
method	O
was	O
original	O
derived	O
for	O
the	O
regret	B
minimization	O
case	O
but	O
it	O
can	O
be	O
applied	O
more	O
generally	O
chapter	O
logistic	B
regression	B
sgd	B
compared	O
to	O
batch	B
learning	B
if	O
we	O
don	O
t	O
have	O
an	O
infinite	O
data	O
stream	O
we	O
can	O
simulate	O
one	O
by	O
sampling	O
data	O
points	O
at	O
random	O
from	O
our	O
training	B
set	I
essentially	O
we	O
are	O
optimizing	O
equation	O
by	O
treating	O
it	O
as	O
an	O
expectation	O
with	O
respect	O
to	O
the	O
empirical	B
distribution	I
algorithm	O
stochastic	B
gradient	B
descent	I
initialize	O
repeat	O
randomly	O
permute	O
data	O
for	O
i	O
n	O
do	O
g	O
f	O
zi	O
proj	O
g	O
update	O
until	O
converged	O
in	O
theory	O
we	O
should	O
sample	O
with	O
replacement	O
although	O
in	O
practice	O
it	O
is	O
usually	O
better	O
to	O
randomly	O
permute	O
the	O
data	O
and	O
sample	O
without	O
replacement	O
and	O
then	O
to	O
repeat	O
a	O
single	O
such	O
pass	O
over	O
the	O
entire	O
data	O
set	O
is	O
called	O
an	O
epoch	B
see	O
algorithm	O
for	O
some	O
pseudocode	O
in	O
this	O
offline	B
case	O
it	O
is	O
often	O
better	O
to	O
compute	O
the	O
gradient	O
of	O
a	O
mini-batch	B
of	O
b	O
data	O
cases	O
if	O
b	O
this	O
is	O
standard	O
sgd	B
and	O
if	O
b	O
n	O
this	O
is	O
standard	O
steepest	B
descent	I
typically	O
b	O
is	O
used	O
although	O
a	O
simple	O
first-order	O
method	O
sgd	B
performs	O
surprisingly	O
well	O
on	O
some	O
problems	O
especially	O
ones	O
with	O
large	O
data	O
sets	O
the	O
intuitive	O
reason	O
for	O
this	O
is	O
that	O
one	O
can	O
get	O
a	O
fairly	O
good	O
estimate	O
of	O
the	O
gradient	O
by	O
looking	O
at	O
just	O
a	O
few	O
examples	O
carefully	O
evaluating	O
precise	O
gradients	O
using	O
large	O
datasets	O
is	O
often	O
a	O
waste	O
of	O
time	O
since	O
the	O
algorithm	O
will	O
have	O
to	O
recompute	O
the	O
gradient	O
again	O
anyway	O
at	O
the	O
next	O
step	O
it	O
is	O
often	O
a	O
better	O
use	O
of	O
computer	O
time	O
to	O
have	O
a	O
noisy	O
estimate	O
and	O
to	O
move	O
rapidly	O
through	O
parameter	B
space	O
as	O
an	O
extreme	O
example	O
suppose	O
we	O
double	O
the	O
training	B
set	I
by	O
duplicating	O
every	O
example	O
batch	B
methods	O
will	O
take	O
twice	O
as	O
long	O
but	O
online	O
methods	O
will	O
be	O
unaffected	O
since	O
the	O
direction	O
of	O
the	O
gradient	O
has	O
not	O
changed	O
the	O
size	O
of	O
the	O
data	O
changes	O
the	O
magnitude	O
of	O
the	O
gradient	O
but	O
that	O
is	O
irrelevant	O
since	O
the	O
gradient	O
is	O
being	O
scaled	O
by	O
the	O
step	B
size	I
anyway	O
in	O
addition	O
to	O
enhanced	O
speed	O
sgd	B
is	O
often	O
less	O
prone	O
to	O
getting	O
stuck	O
in	O
shallow	O
local	O
minima	O
because	O
it	O
adds	O
a	O
certain	O
amount	O
of	O
noise	O
consequently	O
it	O
is	O
quite	O
popular	O
in	O
the	O
machine	B
learning	B
community	O
for	O
fitting	O
models	O
with	O
non-convex	O
objectives	O
such	O
as	O
neural	B
networks	I
and	O
deep	B
belief	B
networks	I
the	O
lms	B
algorithm	O
as	O
an	O
example	O
of	O
sgd	B
let	O
us	O
consider	O
how	O
to	O
compute	O
the	O
mle	B
for	O
linear	B
regression	B
in	O
an	O
online	O
fashion	O
we	O
derived	O
the	O
batch	B
gradient	O
in	O
equation	O
the	O
online	O
gradient	O
at	O
iteration	O
k	O
is	O
given	O
by	O
gk	O
xi	O
t	O
k	O
xi	O
yi	O
online	B
learning	B
and	O
stochastic	B
optimization	B
w	O
black	O
line	O
lms	B
trajectory	O
towards	O
ls	O
soln	O
cross	O
rss	O
vs	O
iteration	O
figure	O
illustration	O
of	O
the	O
lms	B
algorithm	O
left	O
we	O
start	O
from	O
and	O
slowly	O
converging	O
to	O
the	O
least	B
squares	I
solution	O
of	O
cross	O
right	O
plot	O
of	O
objective	O
function	O
over	O
time	O
note	O
that	O
it	O
does	O
not	O
decrease	O
monotonically	O
figure	O
generated	O
by	O
lmsdemo	O
where	O
i	O
ik	O
is	O
the	O
training	O
example	O
to	O
use	O
at	O
iteration	O
k	O
if	O
the	O
data	O
set	O
is	O
streaming	O
we	O
use	O
ik	O
k	O
we	O
shall	O
assume	O
this	O
from	O
now	O
on	O
for	O
notational	O
simplicity	O
equation	O
is	O
easy	O
to	O
interpret	O
it	O
is	O
the	O
feature	O
vector	O
xk	O
weighted	O
by	O
the	O
difference	O
between	O
what	O
we	O
predicted	O
yk	O
t	O
k	O
xk	O
and	O
the	O
true	O
response	O
yk	O
hence	O
the	O
gradient	O
acts	O
like	O
an	O
error	B
signal	I
after	O
computing	O
the	O
gradient	O
we	O
take	O
a	O
step	O
along	O
it	O
as	O
follows	O
k	O
k	O
yk	O
ykxk	O
is	O
no	O
need	O
for	O
a	O
projection	B
step	O
since	O
this	O
is	O
an	O
unconstrained	O
optimization	B
problem	O
this	O
algorithm	O
is	O
called	O
the	O
least	B
mean	B
squares	I
or	O
lms	B
algorithm	O
and	O
is	O
also	O
known	O
as	O
the	O
delta	B
rule	I
or	O
thewidrow-hoff	O
rule	O
start	O
at	O
and	O
converge	B
the	O
sense	O
that	O
k	O
k	O
of	O
figure	O
shows	O
the	O
results	O
of	O
applying	O
this	O
algorithm	O
to	O
the	O
data	O
shown	O
in	O
figure	O
we	O
drops	O
below	O
a	O
threshold	O
in	O
about	O
iterations	O
note	O
that	O
lms	B
may	O
require	O
multiple	O
passes	O
through	O
the	O
data	O
to	O
find	O
the	O
optimum	O
by	O
contrast	O
the	O
recursive	B
least	B
squares	I
algorithm	O
which	O
is	O
based	O
on	O
the	O
kalman	O
filter	O
and	O
which	O
uses	O
second-order	O
information	B
finds	O
the	O
optimum	O
in	O
a	O
single	O
pass	O
section	O
see	O
also	O
exercise	O
the	O
perceptron	B
algorithm	I
now	O
let	O
us	O
consider	O
how	O
to	O
fit	O
a	O
binary	O
logistic	B
regression	B
model	O
in	O
an	O
online	O
manner	O
the	O
batch	B
gradient	O
was	O
given	O
in	O
equation	O
in	O
the	O
online	O
case	O
the	O
weight	O
update	O
has	O
the	O
simple	O
form	O
k	O
k	O
kgi	O
k	O
k	O
i	O
yixi	O
where	O
i	O
pyi	O
k	O
e	O
k	O
we	O
see	O
that	O
this	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
lms	B
algorithm	O
indeed	O
this	O
property	O
holds	O
for	O
all	O
generalized	B
linear	I
models	I
chapter	O
logistic	B
regression	B
we	O
now	O
consider	O
an	O
approximation	O
to	O
this	O
algorithm	O
specifically	O
let	O
y	O
pyxi	O
yi	O
arg	O
max	O
represent	O
the	O
most	O
probable	O
class	O
label	B
we	O
replace	O
i	O
py	O
sigm	O
t	O
xi	O
in	O
the	O
gradient	O
expression	O
with	O
yi	O
thus	O
the	O
approximate	O
gradient	O
becomes	O
gi	O
yi	O
yixi	O
it	O
will	O
make	O
the	O
algebra	O
simpler	O
if	O
we	O
assume	O
y	O
rather	O
than	O
y	O
case	O
our	O
prediction	O
becomes	O
yi	O
sign	O
t	O
xi	O
in	O
this	O
then	O
if	O
yiyi	O
we	O
have	O
made	O
an	O
error	O
but	O
if	O
yiyi	O
we	O
guessed	O
the	O
right	O
label	B
at	O
each	O
step	O
we	O
update	O
the	O
weight	B
vector	I
by	O
adding	O
on	O
the	O
gradient	O
the	O
key	O
observation	B
is	O
that	O
if	O
we	O
predicted	O
correctly	O
then	O
yi	O
yi	O
so	O
the	O
gradient	O
is	O
zero	O
and	O
we	O
do	O
not	O
change	O
the	O
weight	B
vector	I
but	O
if	O
xi	O
is	O
misclassified	O
we	O
update	O
the	O
weights	O
as	O
follows	O
if	O
yi	O
but	O
yi	O
then	O
the	O
negative	O
gradient	O
is	O
yi	O
yixi	O
and	O
if	O
yi	O
but	O
yi	O
then	O
the	O
negative	O
gradient	O
is	O
yi	O
yixi	O
we	O
can	O
absorb	O
the	O
factor	B
of	O
into	O
the	O
learning	B
rate	B
and	O
just	O
write	O
the	O
update	O
in	O
the	O
case	O
of	O
a	O
misclassification	O
as	O
k	O
k	O
kyixi	O
since	O
it	O
is	O
only	O
the	O
sign	O
of	O
the	O
weights	O
that	O
matter	O
not	O
the	O
magnitude	O
we	O
will	O
set	O
k	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
one	O
can	O
show	O
that	O
this	O
method	O
known	O
as	O
the	O
perceptron	B
algorithm	I
will	O
converge	B
provided	O
the	O
data	O
is	O
linearly	B
separable	I
i	O
e	O
that	O
there	O
exist	O
parameters	O
such	O
that	O
predicting	O
with	O
sign	O
t	O
x	O
achieves	O
error	O
on	O
the	O
training	B
set	I
however	O
if	O
the	O
data	O
is	O
not	O
linearly	B
separable	I
the	O
algorithm	O
will	O
not	O
converge	B
and	O
even	O
if	O
it	O
does	O
converge	B
it	O
may	O
take	O
a	O
long	O
time	O
there	O
are	O
much	O
better	O
ways	O
to	O
train	O
logistic	B
regression	B
models	O
as	O
using	O
proper	O
sgd	B
without	O
the	O
gradient	O
approximation	O
or	O
irls	B
discussed	O
in	O
section	O
however	O
the	O
perceptron	B
algorithm	I
is	O
historically	O
important	O
it	O
was	O
one	O
of	O
the	O
first	O
machine	B
learning	B
algorithms	O
ever	O
derived	O
frank	O
rosenblatt	B
in	O
and	O
was	O
even	O
implemented	O
in	O
analog	O
hardware	O
in	O
addition	O
the	O
algorithm	O
can	O
be	O
used	O
to	O
fit	O
models	O
where	O
computing	O
marginals	O
pyix	O
is	O
more	O
expensive	O
than	O
computing	O
the	O
map	O
output	O
arg	O
maxy	O
pyx	O
this	O
arises	O
in	O
some	O
structured-output	O
classification	O
problems	O
see	O
section	O
for	O
details	O
a	O
bayesian	B
view	O
another	O
approach	O
to	O
online	B
learning	B
is	O
to	O
adopt	O
a	O
bayesian	B
view	O
this	O
is	O
conceptually	O
quite	O
simple	O
we	O
just	O
apply	O
bayes	B
rule	I
recursively	O
p	O
pdk	O
this	O
has	O
the	O
obvious	O
advantage	O
of	O
returning	O
a	O
posterior	O
instead	O
of	O
just	O
a	O
point	B
estimate	I
it	O
also	O
allows	O
for	O
the	O
online	O
adaptation	O
of	O
hyper-parameters	B
which	O
is	O
important	O
since	O
cross-validation	O
cannot	O
be	O
used	O
in	O
an	O
online	O
setting	O
finally	O
it	O
has	O
the	O
obvious	O
advantage	O
that	O
it	O
can	O
be	O
generative	O
vs	O
discriminative	B
classifiers	O
d	O
yi	O
for	O
i	O
n	O
algorithm	O
perceptron	B
algorithm	I
input	O
linearly	B
separable	I
data	O
set	O
xi	O
r	O
initialize	O
k	O
repeat	O
k	O
k	O
i	O
k	O
mod	O
n	O
if	O
yi	O
yi	O
then	O
k	O
yixi	O
else	O
no-op	O
until	O
converged	O
quicker	O
than	O
sgd	B
to	O
see	O
why	O
note	O
that	O
by	O
modeling	O
the	O
posterior	O
variance	B
of	O
each	O
parameter	B
in	O
addition	O
to	O
its	O
mean	B
we	O
effectively	O
associate	O
a	O
different	O
learning	B
rate	B
for	O
each	O
parameter	B
freitas	O
et	O
al	O
which	O
is	O
a	O
simple	O
way	O
to	O
model	O
the	O
curvature	O
of	O
the	O
space	O
these	O
variances	O
can	O
then	O
be	O
adapted	O
using	O
the	O
usual	O
rules	B
of	O
probability	B
theory	I
by	O
contrast	O
getting	O
second-order	O
optimization	B
methods	O
to	O
work	O
online	O
is	O
more	O
tricky	O
e	O
g	O
et	O
al	O
sunehag	O
et	O
al	O
bordes	O
et	O
al	O
as	O
a	O
simple	O
example	O
in	O
section	O
we	O
show	O
how	O
to	O
use	O
the	O
kalman	O
filter	O
to	O
fit	O
a	O
linear	B
regression	B
model	O
online	O
unlike	O
the	O
lms	B
algorithm	O
this	O
converges	O
to	O
the	O
optimal	O
answer	O
in	O
a	O
single	O
pass	O
over	O
the	O
data	O
an	O
extension	B
which	O
can	O
learn	O
a	O
robust	B
non-linear	O
regression	B
model	O
in	O
an	O
online	O
fashion	O
is	O
described	O
in	O
et	O
al	O
for	O
the	O
glm	B
case	O
we	O
can	O
use	O
an	O
assumed	O
density	O
filter	O
where	O
we	O
approximate	O
the	O
posterior	O
by	O
a	O
gaussian	B
with	O
a	O
diagonal	B
covariance	B
the	O
variance	B
terms	O
serve	O
as	O
a	O
per-parameter	O
step-size	O
see	O
section	O
for	O
details	O
another	O
approach	O
is	O
to	O
use	O
particle	O
filtering	B
this	O
was	O
used	O
in	O
et	O
al	O
for	O
sequentially	O
learning	B
a	O
kernelized	O
linearlogistic	O
regression	B
model	O
generative	O
vs	O
discriminative	B
classifiers	O
in	O
section	O
we	O
showed	O
that	O
the	O
posterior	O
over	O
class	O
labels	O
induced	O
by	O
gaussian	B
discriminant	B
analysis	I
has	O
exactly	O
the	O
same	O
form	O
as	O
logistic	B
regression	B
namely	O
py	O
sigmwt	O
x	O
the	O
decision	B
boundary	I
is	O
therefore	O
a	O
linear	O
function	O
of	O
x	O
in	O
both	O
cases	O
note	O
however	O
that	O
many	O
generative	O
models	O
can	O
give	O
rise	O
to	O
a	O
logistic	B
regression	B
posterior	O
e	O
g	O
if	O
each	O
class-conditional	B
density	I
is	O
poisson	B
pxy	O
c	O
poix	O
c	O
so	O
the	O
assumptions	O
made	O
by	O
gda	B
are	O
much	O
stronger	O
than	O
the	O
assumptions	O
made	O
by	O
logistic	B
regression	B
a	O
further	O
difference	O
between	O
these	O
models	O
is	O
the	O
way	O
they	O
are	O
trained	O
when	O
fitting	O
a	O
log	O
pyixi	O
whereas	O
log	O
pyi	O
xi	O
inative	O
model	O
we	O
usually	O
maximize	O
the	O
conditional	O
log	O
likelihood	B
when	O
fitting	O
a	O
generative	O
model	O
we	O
usually	O
maximize	O
the	O
joint	O
log	O
likelihood	B
it	O
is	O
clear	O
that	O
these	O
can	O
in	O
general	O
give	O
different	O
results	O
exercise	O
when	O
the	O
gaussian	B
assumptions	O
made	O
by	O
gda	B
are	O
correct	O
the	O
model	O
will	O
need	O
less	O
training	O
data	O
than	O
logistic	B
regression	B
to	O
achieve	O
a	O
certain	O
level	O
of	O
performance	O
but	O
if	O
the	O
gaussian	B
chapter	O
logistic	B
regression	B
assumptions	O
are	O
incorrect	O
logistic	B
regression	B
will	O
do	O
better	O
and	O
jordan	O
this	O
is	O
because	O
discriminative	B
models	O
do	O
not	O
need	O
to	O
model	O
the	O
distribution	O
of	O
the	O
features	B
this	O
is	O
illustrated	O
in	O
figure	O
we	O
see	O
that	O
the	O
class	O
conditional	O
densities	O
are	O
rather	O
complex	O
in	O
particular	O
pxy	O
is	O
a	O
multimodal	O
distribution	O
which	O
might	O
be	O
hard	O
to	O
estimate	O
however	O
the	O
class	O
posterior	O
py	O
cx	O
is	O
a	O
simple	O
sigmoidal	O
function	O
centered	O
on	O
the	O
threshold	O
value	O
of	O
this	O
suggests	O
that	O
in	O
general	O
discriminative	B
methods	O
will	O
be	O
more	O
accurate	O
since	O
their	O
job	O
is	O
in	O
some	O
sense	O
easier	O
however	O
accuracy	O
is	O
not	O
the	O
only	O
important	O
factor	B
when	O
choosing	O
a	O
method	O
below	O
we	O
discuss	O
some	O
other	O
advantages	O
and	O
disadvantages	O
of	O
each	O
approach	O
pros	O
and	O
cons	O
of	O
each	O
approach	O
easy	O
to	O
fit	O
as	O
we	O
have	O
seen	O
it	O
is	O
usually	O
very	O
easy	O
to	O
fit	O
generative	O
classifiers	O
for	O
example	O
in	O
sections	O
and	O
we	O
show	O
that	O
we	O
can	O
fit	O
a	O
naive	O
bayes	O
model	O
and	O
an	O
lda	B
model	O
by	O
simple	O
counting	O
and	O
averaging	O
by	O
contrast	O
logistic	B
regression	B
requires	O
solving	O
a	O
convex	B
optimization	B
problem	O
section	O
for	O
the	O
details	O
which	O
is	O
much	O
slower	O
fit	O
classes	O
separately	O
in	O
a	O
generative	O
classifier	O
we	O
estimate	O
the	O
parameters	O
of	O
each	O
class	O
conditional	O
density	O
independently	O
so	O
we	O
do	O
not	O
have	O
to	O
retrain	O
the	O
model	O
when	O
we	O
add	O
more	O
classes	O
in	O
contrast	O
in	O
discriminative	B
models	O
all	O
the	O
parameters	O
interact	O
so	O
the	O
whole	O
model	O
must	O
be	O
retrained	O
if	O
we	O
add	O
a	O
new	O
class	O
is	O
also	O
the	O
case	O
if	O
we	O
train	O
a	O
generative	O
model	O
to	O
maximize	O
a	O
discriminative	B
objective	O
salojarvi	O
et	O
al	O
handle	O
missing	B
features	B
easily	O
sometimes	O
some	O
of	O
the	O
inputs	O
of	O
x	O
are	O
not	O
observed	O
in	O
a	O
generative	O
classifier	O
there	O
is	O
a	O
simple	O
method	O
for	O
dealing	O
with	O
this	O
as	O
we	O
discuss	O
in	O
section	O
however	O
in	O
a	O
discriminative	B
classifier	O
there	O
is	O
no	O
principled	O
solution	O
to	O
this	O
problem	O
since	O
the	O
model	O
assumes	O
that	O
x	O
is	O
always	O
available	O
to	O
be	O
conditioned	O
on	O
see	O
for	O
some	O
heuristic	O
approaches	O
can	O
handle	O
unlabeled	O
training	O
data	O
there	O
is	O
much	O
interest	O
in	O
semi-supervised	B
learning	B
which	O
uses	O
unlabeled	O
data	O
to	O
help	O
solve	O
a	O
supervised	O
task	O
this	O
is	O
fairly	O
easy	O
to	O
do	O
using	O
generative	O
models	O
e	O
g	O
et	O
al	O
liang	O
et	O
al	O
but	O
is	O
much	O
harder	O
to	O
do	O
with	O
discriminative	B
models	O
symmetric	B
in	O
inputs	O
and	O
outputs	O
we	O
can	O
run	O
a	O
generative	O
model	O
backwards	O
and	O
infer	O
probable	O
inputs	O
given	O
the	O
output	O
by	O
computing	O
pxy	O
this	O
is	O
not	O
possible	O
with	O
a	O
discriminative	B
model	O
the	O
reason	O
is	O
that	O
a	O
generative	O
model	O
defines	O
a	O
joint	B
distribution	I
on	O
x	O
and	O
y	O
and	O
hence	O
treats	O
both	O
inputs	O
and	O
outputs	O
symmetrically	O
can	O
handle	O
feature	O
preprocessing	O
a	O
big	O
advantage	O
of	O
discriminative	B
methods	O
is	O
that	O
they	O
allow	O
us	O
to	O
preprocess	O
the	O
input	O
in	O
arbitrary	O
ways	O
e	O
g	O
we	O
can	O
replace	O
x	O
with	O
which	O
could	O
be	O
some	O
basis	B
function	I
expansion	I
as	O
illustrated	O
in	O
figure	O
it	O
is	O
often	O
hard	O
to	O
define	O
a	O
generative	O
model	O
on	O
such	O
pre-processed	O
data	O
since	O
the	O
new	O
features	B
are	O
correlated	O
in	O
complex	O
ways	O
well-calibrated	O
probabilities	O
some	O
generative	O
models	O
such	O
as	O
naive	O
bayes	O
make	O
strong	O
independence	O
assumptions	O
which	O
are	O
often	O
not	O
valid	O
this	O
can	O
result	O
in	O
very	O
extreme	O
posterior	O
class	O
probabilities	O
near	O
or	O
discriminative	B
models	O
such	O
as	O
logistic	B
regression	B
are	O
usually	O
better	O
calibrated	O
in	O
terms	O
of	O
their	O
probability	O
estimates	O
we	O
see	O
that	O
there	O
are	O
arguments	O
for	O
and	O
against	O
both	O
kinds	O
of	O
models	O
it	O
is	O
therefore	O
useful	O
to	O
have	O
both	O
kinds	O
in	O
your	O
toolbox	O
see	O
table	O
for	O
a	O
summary	O
of	O
the	O
classification	O
and	O
generative	O
vs	O
discriminative	B
classifiers	O
linear	O
multinomial	B
logistic	B
regression	B
kernel	B
rbf	B
multinomial	B
logistic	B
regression	B
figure	O
multinomial	B
logistic	B
regression	B
for	O
classes	O
in	O
the	O
original	O
feature	O
space	O
after	O
basis	B
function	I
expansion	I
using	O
rbf	B
kernels	O
with	O
a	O
bandwidth	B
of	O
and	O
using	O
all	O
the	O
data	O
points	O
as	O
centers	O
figure	O
generated	O
by	O
logregmultinomkerneldemo	O
s	O
e	O
i	O
t	O
i	O
s	O
n	O
e	O
d	O
l	O
a	O
n	O
o	O
i	O
t	O
i	O
d	O
n	O
o	O
c	O
s	O
s	O
a	O
c	O
l	O
x	O
x	O
figure	O
the	O
class-conditional	O
densities	O
pxy	O
c	O
may	O
be	O
more	O
complex	O
than	O
the	O
class	O
posteriors	O
py	O
cx	O
figure	O
generated	O
by	O
generativevsdiscrim	O
based	O
on	O
figure	O
of	O
regression	B
techniques	O
we	O
cover	O
in	O
this	O
book	O
dealing	O
with	O
missing	B
data	I
sometimes	O
some	O
of	O
the	O
inputs	O
of	O
x	O
are	O
not	O
observed	O
this	O
could	O
be	O
due	O
to	O
a	O
sensor	O
failure	O
or	O
a	O
failure	O
to	O
complete	B
an	O
entry	O
in	O
a	O
survey	O
etc	O
this	O
is	O
called	O
the	O
missing	B
data	I
problem	I
and	O
rubin	O
the	O
ability	O
to	O
handle	O
missing	B
data	I
in	O
a	O
principled	O
way	O
is	O
one	O
of	O
the	O
biggest	O
advantages	O
of	O
generative	O
models	O
to	O
formalize	O
our	O
assumptions	O
we	O
can	O
associate	O
a	O
binary	O
response	B
variable	I
ri	O
that	O
specifies	O
whether	O
each	O
value	O
xi	O
the	O
joint	O
model	O
has	O
the	O
form	O
pxi	O
ri	O
prixi	O
where	O
are	O
the	O
parameters	O
controlling	O
whether	O
the	O
item	O
is	O
observed	O
or	O
not	O
chapter	O
logistic	B
regression	B
model	O
discriminant	B
analysis	I
naive	O
bayes	O
classifier	O
tree-augmented	O
naive	O
bayes	O
classifier	O
linear	B
regression	B
logistic	B
regression	B
sparse	B
linear	O
logistic	B
regression	B
mixture	B
of	I
experts	I
multilayer	O
perceptron	B
neural	B
network	I
conditional	O
random	O
field	O
k	O
nearest	B
neighbor	I
classifier	O
mixture	B
discriminant	B
analysis	I
classification	O
and	O
regression	B
trees	O
boosted	O
model	O
sparse	B
kernelized	O
linlogreg	O
relevance	B
vector	I
machine	I
support	B
vector	I
machine	I
gaussian	B
processes	I
smoothing	B
splines	I
classifregr	O
classif	O
classif	O
classif	O
regr	O
classif	O
both	O
both	O
both	O
classif	O
classif	O
classif	O
both	O
both	O
both	O
both	O
both	O
both	O
regr	O
gendiscr	O
gen	O
gen	O
gen	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
gen	O
gen	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
paramnon	O
param	O
param	O
param	O
param	O
param	O
param	O
param	O
param	O
param	O
non	O
non	O
non	O
non	O
non	O
non	O
non	O
non	O
non	O
section	O
sec	O
sec	O
sec	O
sec	O
sec	O
ch	O
sec	O
ch	O
sec	O
sec	O
sec	O
sec	O
sec	O
sec	O
sec	O
sec	O
ch	O
section	O
is	O
the	O
model	O
suitable	O
for	O
classification	O
regression	B
or	O
both	O
table	O
list	O
of	O
various	O
models	O
for	O
classification	O
and	O
regression	B
which	O
we	O
discuss	O
in	O
this	O
book	O
columns	O
are	O
as	O
follows	O
model	O
name	O
is	O
the	O
model	O
generative	O
or	O
discriminative	B
is	O
the	O
model	O
parametric	O
or	O
non-parametric	O
list	O
of	O
sections	O
in	O
book	O
which	O
discuss	O
the	O
model	O
see	O
also	O
tsupervised	O
html	O
for	O
the	O
pmtk	O
equivalents	O
of	O
these	O
models	O
any	O
generative	O
probabilistic	O
model	O
hmms	B
boltzmann	O
machines	O
bayesian	B
networks	I
etc	O
can	O
be	O
turned	O
into	O
a	O
classifier	O
by	O
using	O
it	O
as	O
a	O
class	O
conditional	O
density	O
is	O
observed	O
or	O
not	O
if	O
we	O
assume	O
prixi	O
pri	O
we	O
say	O
the	O
data	O
is	O
missing	B
completely	I
at	I
random	I
or	O
mcar	B
if	O
we	O
assume	O
prixi	O
prixo	O
i	O
is	O
the	O
observed	O
part	O
of	O
xi	O
we	O
say	O
the	O
data	O
is	O
missing	B
at	I
random	I
or	O
mar	B
if	O
neither	O
of	O
these	O
assumptions	O
hold	O
we	O
say	O
the	O
data	O
is	O
not	B
missing	B
at	I
random	I
or	O
nmar	B
in	O
this	O
case	O
we	O
have	O
to	O
model	O
the	O
missing	B
data	I
mechanism	O
since	O
the	O
pattern	B
of	O
missingness	O
is	O
informative	O
about	O
the	O
values	O
of	O
the	O
missing	B
data	I
and	O
the	O
corresponding	O
parameters	O
this	O
is	O
the	O
case	O
in	O
most	O
collaborative	O
filtering	B
problems	O
for	O
example	O
see	O
e	O
g	O
for	O
further	O
discussion	O
we	O
will	O
henceforth	O
assume	O
the	O
data	O
is	O
mar	B
i	O
where	O
xo	O
when	O
dealing	O
with	O
missing	B
data	I
it	O
is	O
helpful	O
to	O
distinguish	O
the	O
cases	O
when	O
there	O
is	O
missingness	O
only	O
at	O
test	O
time	O
the	O
training	O
data	O
is	O
complete	B
data	I
from	O
the	O
harder	O
case	O
when	O
there	O
is	O
missingness	O
also	O
at	O
training	O
time	O
we	O
will	O
discuss	O
these	O
two	O
cases	O
below	O
note	O
that	O
the	O
class	O
label	B
is	O
always	O
missing	B
at	O
test	O
time	O
by	O
definition	O
if	O
the	O
class	O
label	B
is	O
also	O
sometimes	O
missing	B
at	O
training	O
time	O
the	O
problem	O
is	O
called	O
semi-supervised	B
learning	B
generative	O
vs	O
discriminative	B
classifiers	O
missing	B
data	I
at	O
test	O
time	O
in	O
a	O
generative	O
classifier	O
we	O
can	O
handle	O
features	B
that	O
are	O
mar	B
by	O
marginalizing	O
them	O
out	O
for	O
example	O
if	O
we	O
are	O
missing	B
the	O
value	O
of	O
we	O
can	O
compute	O
py	O
py	O
c	O
c	O
py	O
c	O
c	O
if	O
we	O
make	O
the	O
naive	O
bayes	O
assumption	O
the	O
marginalization	O
can	O
be	O
performed	O
as	O
follows	O
c	O
pxj	O
jc	O
pxj	O
jc	O
c	O
hence	O
in	O
a	O
naive	O
bayes	O
classifier	O
we	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
can	O
simply	O
ignore	O
missing	B
features	B
at	O
test	O
time	O
similarly	O
in	O
discriminant	B
analysis	I
no	O
matter	O
what	O
regularization	B
method	O
was	O
used	O
to	O
estimate	O
the	O
parameters	O
we	O
can	O
always	O
analytically	O
marginalize	O
out	O
the	O
missing	B
variables	O
section	O
c	O
n	O
missing	B
data	I
at	O
training	O
time	O
missing	B
data	I
at	O
training	O
time	O
is	O
harder	O
to	O
deal	O
with	O
in	O
particular	O
computing	O
the	O
mle	B
or	O
map	B
estimate	I
is	O
no	O
longer	O
a	O
simple	O
optimization	B
problem	O
for	O
reasons	O
discussed	O
in	O
section	O
however	O
soon	O
we	O
will	O
study	O
are	O
a	O
variety	O
of	O
more	O
sophisticated	O
algorithms	O
as	O
em	B
algorithm	O
in	O
section	O
for	O
finding	O
approximate	O
ml	O
or	O
map	O
estimates	O
in	O
such	O
cases	O
fisher	O
s	O
linear	B
discriminant	B
analysis	I
discriminant	B
analysis	I
is	O
a	O
generative	B
approach	I
to	O
classification	O
which	O
requires	O
fitting	O
an	O
mvn	B
to	O
the	O
features	B
as	O
we	O
have	O
discussed	O
this	O
can	O
be	O
problematic	O
in	O
high	O
dimensions	O
an	O
alternative	O
approach	O
is	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
features	B
x	O
r	O
d	O
and	O
then	O
fit	O
an	O
mvn	B
to	O
the	O
resulting	O
low-dimensional	O
features	B
z	O
r	O
l	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
linear	O
projection	B
matrix	O
z	O
wx	O
where	O
w	O
is	O
a	O
l	O
d	O
matrix	O
one	O
approach	O
to	O
finding	O
w	O
would	O
be	O
to	O
use	O
pca	B
the	O
result	O
would	O
be	O
very	O
similar	B
to	O
rda	O
since	O
svd	B
and	O
pca	B
are	O
essentially	O
equivalent	O
however	O
pca	B
is	O
an	O
unsupervised	O
technique	O
that	O
does	O
not	O
take	O
class	O
labels	O
into	O
account	O
thus	O
the	O
resulting	O
low	O
dimensional	O
features	B
are	O
not	O
necessarily	O
optimal	O
for	O
classification	O
as	O
illustrated	O
in	O
figure	O
an	O
alternative	O
approach	O
is	O
to	O
find	O
the	O
matrix	O
w	O
such	O
that	O
the	O
low-dimensional	O
data	O
can	O
be	O
classified	O
as	O
well	O
as	O
possible	O
using	O
a	O
gaussian	B
class-conditional	B
density	I
model	O
the	O
assumption	O
of	O
gaussianity	O
is	O
reasonable	O
since	O
we	O
are	O
computing	O
linear	O
combinations	O
of	O
non-gaussian	O
features	B
this	O
approach	O
is	O
called	O
fisher	O
s	O
linear	B
discriminant	B
analysis	I
orflda	O
flda	B
is	O
an	O
interesting	O
hybrid	O
of	O
discriminative	B
and	O
generative	O
techniques	O
the	O
drawback	O
of	O
this	O
technique	O
is	O
that	O
it	O
is	O
restricted	O
to	O
using	O
l	O
c	O
dimensions	O
regardless	O
of	O
d	O
for	O
reasons	O
that	O
we	O
will	O
explain	O
below	O
in	O
the	O
two-class	O
case	O
this	O
means	O
we	O
are	O
seeking	O
a	O
single	O
vector	O
w	O
onto	O
which	O
we	O
can	O
project	O
the	O
data	O
below	O
we	O
derive	O
the	O
optimal	O
w	O
in	O
the	O
two-class	O
case	O
we	O
chapter	O
logistic	B
regression	B
means	O
fisher	O
pca	B
fisher	O
pca	B
figure	O
example	O
of	O
fisher	O
s	O
linear	O
discriminant	O
two	O
class	O
data	O
in	O
dashed	O
green	O
line	O
first	O
principal	O
basis	O
vector	O
dotted	O
red	O
line	O
fisher	O
s	O
linear	O
discriminant	O
vector	O
solid	O
black	O
line	O
joins	O
the	O
class-conditional	O
means	O
projection	B
of	O
points	O
onto	O
pca	B
vector	O
shows	O
poor	O
class	O
separation	O
figure	O
generated	O
by	O
fisherldademo	O
projection	B
of	O
points	O
onto	O
fisher	O
s	O
vector	O
shows	O
good	O
class	O
separation	O
generative	O
vs	O
discriminative	B
classifiers	O
then	O
generalize	B
to	O
the	O
multi-class	O
case	O
and	O
finally	O
we	O
give	O
a	O
probabilistic	O
interpretation	O
of	O
this	O
technique	O
derivation	O
of	O
the	O
optimal	O
projection	B
we	O
now	O
derive	O
this	O
optimal	O
direction	O
w	O
for	O
the	O
two-class	O
case	O
following	O
the	O
presentation	O
of	O
sec	O
define	O
the	O
class-conditional	O
means	O
as	O
xi	O
xi	O
let	O
mk	O
wt	O
k	O
be	O
the	O
projection	B
of	O
each	O
mean	B
onto	O
the	O
line	O
w	O
also	O
let	O
zi	O
wt	O
xi	O
be	O
the	O
projection	B
of	O
the	O
data	O
onto	O
the	O
line	O
the	O
variance	B
of	O
the	O
projected	O
points	O
is	O
proportional	O
to	O
k	O
iyik	O
the	O
goal	O
is	O
to	O
find	O
w	O
such	O
that	O
we	O
maximize	O
the	O
distance	O
between	O
the	O
means	O
while	O
also	O
ensuring	O
the	O
projected	O
clusters	B
are	O
tight	O
jw	O
we	O
can	O
rewrite	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
in	O
terms	O
of	O
w	O
as	O
follows	O
jw	O
wt	O
sbw	O
wt	O
sw	O
w	O
where	O
sb	O
is	O
the	O
between-class	O
scatter	O
matrix	O
given	O
by	O
sb	O
and	O
sw	O
is	O
the	O
within-class	O
scatter	O
matrix	O
given	O
by	O
sw	O
to	O
see	O
this	O
note	O
that	O
wt	O
sbw	O
wt	O
w	O
and	O
wt	O
sw	O
w	O
wt	O
w	O
wt	O
equation	O
is	O
a	O
ratio	O
of	O
two	O
scalars	O
we	O
can	O
take	O
its	O
derivative	O
with	O
respect	O
to	O
w	O
and	O
equate	O
to	O
zero	O
one	O
can	O
show	O
that	O
that	O
jw	O
is	O
maximized	O
when	O
sbw	O
sw	O
w	O
chapter	O
logistic	B
regression	B
figure	O
pca	B
projection	B
of	O
vowel	O
data	O
to	O
flda	B
projection	B
of	O
vowel	O
data	O
to	O
we	O
see	O
there	O
is	O
better	O
class	O
separation	O
in	O
the	O
flda	B
case	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
fisherdiscrimvoweldemo	O
by	O
hannes	O
bretschneider	O
where	O
wt	O
sbw	O
wt	O
sw	O
w	O
equation	O
is	O
called	O
a	O
generalized	B
eigenvalue	I
problem	O
if	O
sw	O
is	O
invertible	O
we	O
can	O
convert	O
it	O
to	O
a	O
regular	B
eigenvalue	O
problem	O
s	O
w	O
sbw	O
w	O
however	O
in	O
the	O
two	O
class	O
case	O
there	O
is	O
a	O
simpler	O
solution	O
in	O
particular	O
since	O
sbw	O
w	O
then	O
from	O
equation	O
we	O
have	O
w	O
s	O
w	O
s	O
w	O
w	O
since	O
we	O
only	O
care	O
about	O
the	O
directionality	O
and	O
not	O
the	O
scale	O
factor	B
we	O
can	O
just	O
set	O
w	O
s	O
w	O
this	O
is	O
the	O
optimal	O
solution	O
in	O
the	O
two-class	O
case	O
if	O
sw	O
i	O
meaning	O
the	O
pooled	B
covariance	B
matrix	I
is	O
isotropic	B
then	O
w	O
is	O
proportional	O
to	O
the	O
vector	O
that	O
joins	O
the	O
class	O
means	O
this	O
is	O
an	O
intuitively	O
reasonable	O
direction	O
to	O
project	O
onto	O
as	O
shown	O
in	O
figure	O
extension	B
to	O
higher	O
dimensions	O
and	O
multiple	O
classes	O
we	O
can	O
extend	O
the	O
above	O
idea	O
to	O
multiple	O
classes	O
and	O
to	O
higher	O
dimensional	O
subspaces	O
by	O
finding	O
a	O
projection	B
matrix	O
w	O
which	O
maps	O
from	O
d	O
to	O
l	O
so	O
as	O
to	O
maximize	O
jw	O
bwt	O
w	O
wt	O
generative	O
vs	O
discriminative	B
classifiers	O
where	O
b	O
w	O
c	O
c	O
c	O
nc	O
c	O
c	O
nc	O
n	O
c	O
nc	O
n	O
cxi	O
ct	O
iyic	O
the	O
solution	O
can	O
be	O
shown	O
to	O
be	O
w	O
w	O
u	O
w	O
assuming	O
w	O
is	O
non-singular	O
it	O
where	O
u	O
are	O
the	O
l	O
leading	O
eigenvectors	O
of	O
is	O
singular	O
we	O
can	O
first	O
perform	O
pca	B
on	O
all	O
the	O
data	O
w	O
b	O
figure	O
gives	O
an	O
example	O
of	O
this	O
method	O
applied	O
to	O
some	O
d	O
dimensional	O
speech	O
data	O
representing	O
c	O
different	O
vowel	O
sounds	O
we	O
see	O
that	O
flda	B
gives	O
better	O
class	O
separation	O
than	O
pca	B
note	O
that	O
flda	B
is	O
restricted	O
to	O
finding	O
at	O
most	O
a	O
l	O
c	O
dimensional	O
linear	O
subspace	O
no	O
matter	O
how	O
large	O
d	O
because	O
the	O
rank	O
of	O
the	O
between	O
class	O
covariance	B
matrix	I
b	O
is	O
c	O
term	O
arises	O
because	O
of	O
the	O
term	O
which	O
is	O
a	O
linear	O
function	O
of	O
the	O
c	O
this	O
is	O
a	O
rather	O
severe	O
restriction	O
which	O
limits	O
the	O
usefulness	O
of	O
flda	B
probabilistic	O
interpretation	O
of	O
flda	B
to	O
find	O
a	O
valid	O
probabilistic	O
interpretation	O
of	O
flda	B
we	O
follow	O
the	O
approach	O
of	O
and	O
andreo	O
zhou	O
et	O
al	O
they	O
proposed	O
a	O
model	O
known	O
as	O
heteroscedastic	B
lda	B
which	O
works	O
as	O
follows	O
let	O
w	O
be	O
a	O
d	O
d	O
invertible	O
matrix	O
and	O
let	O
zi	O
wxi	O
be	O
a	O
transformed	O
version	O
of	O
the	O
data	O
we	O
now	O
fit	O
full	B
covariance	B
gaussians	O
to	O
the	O
transformed	O
data	O
one	O
per	O
class	O
but	O
with	O
the	O
constraint	O
that	O
only	O
the	O
first	O
l	O
components	O
will	O
be	O
class-specific	O
the	O
remaining	O
h	O
d	O
l	O
components	O
will	O
be	O
shared	B
across	O
classes	O
and	O
will	O
thus	O
not	O
be	O
discriminative	B
that	O
is	O
we	O
use	O
where	O
is	O
the	O
shared	B
h	O
dimensional	O
mean	B
and	O
is	O
the	O
shared	B
h	O
h	O
covariace	O
the	O
pdf	B
of	O
the	O
original	O
data	O
is	O
given	O
by	O
pxiyi	O
c	O
w	O
n	O
c	O
c	O
n	O
sc	O
n	O
for	O
fixed	O
w	O
it	O
is	O
easy	O
to	O
derive	O
the	O
mle	B
for	O
one	O
can	O
then	O
optimize	O
pzi	O
yi	O
c	O
c	O
c	O
c	O
sc	O
c	O
where	O
w	O
w	O
using	O
gradient	O
methods	O
wl	O
wh	O
chapter	O
logistic	B
regression	B
in	O
the	O
special	O
case	O
that	O
the	O
c	O
are	O
diagonal	B
there	O
is	O
a	O
closed-form	O
solution	O
for	O
w	O
and	O
in	O
the	O
special	O
case	O
the	O
c	O
are	O
all	O
equal	O
we	O
recover	O
classical	B
lda	B
et	O
al	O
in	O
view	O
of	O
this	O
this	O
result	O
it	O
should	O
be	O
clear	O
that	O
hlda	O
will	O
outperform	O
lda	B
if	O
the	O
class	O
covariances	O
are	O
not	O
equal	O
within	O
the	O
discriminative	B
subspace	O
if	O
the	O
assumption	O
that	O
c	O
is	O
independent	O
of	O
c	O
is	O
a	O
poor	O
assumption	O
this	O
is	O
easy	O
to	O
demonstrate	O
on	O
synthetic	O
data	O
and	O
is	O
also	O
the	O
case	O
on	O
more	O
challenging	O
tasks	O
such	O
as	O
speech	B
recognition	I
and	O
andreo	O
furthermore	O
we	O
can	O
extend	O
the	O
model	O
by	O
allowing	O
each	O
class	O
to	O
use	O
its	O
own	O
projection	B
matrix	O
this	O
is	O
known	O
as	O
multiple	B
lda	B
exercises	O
exercise	O
spam	B
classification	O
using	O
logistic	B
regression	B
consider	O
the	O
email	O
spam	B
data	O
set	O
discussed	O
on	O
of	O
et	O
al	O
this	O
consists	O
of	O
email	O
messages	O
from	O
which	O
features	B
have	O
been	O
extracted	O
these	O
are	O
as	O
follows	O
features	B
in	O
giving	O
the	O
percentage	O
of	O
words	O
in	O
a	O
given	O
message	O
which	O
match	O
a	O
given	O
word	O
on	O
the	O
list	O
the	O
list	O
contains	O
words	O
such	O
as	O
business	O
free	O
george	O
etc	O
data	O
was	O
collected	O
by	O
george	O
forman	O
so	O
his	O
name	O
occurs	O
quite	O
a	O
lot	O
features	B
in	O
giving	O
the	O
percentage	O
of	O
characters	O
in	O
the	O
email	O
that	O
match	O
a	O
given	O
character	O
on	O
the	O
list	O
the	O
characters	O
are	O
feature	O
the	O
average	O
length	O
of	O
an	O
uninterrupted	O
sequence	O
of	O
capital	O
letters	O
is	O
mean	B
is	O
feature	O
the	O
length	O
of	O
the	O
longest	O
uninterrupted	O
sequence	O
of	O
capital	O
letters	O
is	O
mean	B
is	O
feature	O
the	O
sum	O
of	O
the	O
lengts	O
of	O
uninterrupted	O
sequence	O
of	O
capital	O
letters	O
is	O
mean	B
is	O
load	O
the	O
data	O
from	O
spamdata	O
mat	O
which	O
contains	O
a	O
training	B
set	I
size	O
and	O
a	O
test	O
set	O
size	O
one	O
can	O
imagine	O
performing	O
several	O
kinds	O
of	O
preprocessing	O
to	O
this	O
data	O
try	O
each	O
of	O
the	O
following	O
separately	O
a	O
standardize	O
the	O
columns	O
so	O
they	O
all	O
have	O
mean	B
and	O
unit	O
variance	B
b	O
transform	O
the	O
features	B
using	O
logxij	O
c	O
binarize	O
the	O
features	B
using	O
ixij	O
for	O
each	O
version	O
of	O
the	O
data	O
fit	O
a	O
logistic	B
regression	B
model	O
use	O
cross	B
validation	I
to	O
choose	O
the	O
strength	O
of	O
the	O
regularizer	O
report	O
the	O
mean	B
error	O
rate	B
on	O
the	O
training	O
and	O
test	O
sets	O
you	O
should	O
get	O
numbers	O
similar	B
to	O
this	O
method	O
stnd	O
log	O
binary	O
train	O
test	O
precise	O
values	O
will	O
depend	O
on	O
what	O
regularization	B
value	O
you	O
choose	O
turn	O
in	O
your	O
code	O
and	O
numerical	O
results	O
also	O
exercise	O
exercise	O
spam	B
classification	O
using	O
naive	O
bayes	O
we	O
will	O
re-examine	O
the	O
dataset	O
from	O
exercise	O
generative	O
vs	O
discriminative	B
classifiers	O
a	O
use	O
naivebayesfit	O
and	O
naivebayespredict	O
on	O
the	O
binarized	O
spam	B
data	O
what	O
is	O
the	O
training	O
and	O
can	O
try	O
different	O
settings	O
of	O
the	O
pseudocount	O
if	O
you	O
like	O
corresponds	O
to	O
the	O
test	O
error	O
beta	O
prior	O
each	O
jc	O
although	O
the	O
default	O
of	O
is	O
probably	O
fine	O
turn	O
in	O
your	O
error	O
rates	O
b	O
modify	O
the	O
code	O
so	O
it	O
can	O
handle	O
real-valued	O
features	B
use	O
a	O
gaussian	B
density	O
for	O
each	O
feature	O
fit	O
it	O
with	O
maximum	O
likelihood	B
what	O
are	O
the	O
training	O
and	O
test	O
error	O
rates	O
on	O
the	O
standardized	B
data	O
and	O
the	O
log	O
transformed	O
data	O
turn	O
in	O
your	O
error	O
rates	O
and	O
code	O
exercise	O
gradient	O
and	O
hessian	B
of	O
log-likelihood	O
for	O
logistic	B
regression	B
a	O
let	O
a	O
be	O
the	O
sigmoid	B
function	O
show	O
that	O
d	O
da	O
log	O
likelihood	B
b	O
using	O
the	O
previous	O
result	O
and	O
the	O
chain	B
rule	I
of	O
calculus	O
derive	O
an	O
expression	O
for	O
the	O
gradient	O
of	O
the	O
c	O
the	O
hessian	B
can	O
be	O
written	O
as	O
h	O
xt	O
sx	O
where	O
s	O
diag	O
n	O
show	O
may	O
assume	O
that	O
i	O
so	O
the	O
elements	O
of	O
s	O
will	O
be	O
strictly	O
that	O
h	O
is	O
positive	O
definite	O
positive	O
and	O
that	O
x	O
is	O
full	B
rank	O
exercise	O
gradient	O
and	O
hessian	B
of	O
log-likelihood	O
for	O
multinomial	B
logistic	B
regression	B
a	O
let	O
ik	O
s	O
ik	O
prove	O
that	O
the	O
jacobian	B
of	O
the	O
softmax	B
is	O
ik	O
ij	O
ik	O
kj	O
ij	O
where	O
kj	O
ik	O
j	O
b	O
hence	O
show	O
that	O
wc	O
icxi	O
hint	O
use	O
the	O
chain	B
rule	I
and	O
the	O
fact	O
that	O
c	O
yic	O
c	O
show	O
that	O
the	O
block	O
submatrix	O
of	O
the	O
hessian	B
for	O
classes	O
c	O
and	O
is	O
given	O
by	O
ic	O
i	O
i	O
i	O
exercise	O
symmetric	B
version	O
of	O
regularized	O
multinomial	B
logistic	B
regression	B
ex	O
of	O
et	O
al	O
multiclass	O
logistic	B
regression	B
has	O
the	O
form	O
py	O
cx	O
w	O
wt	O
c	O
x	O
wt	O
k	O
x	O
where	O
w	O
is	O
a	O
c	O
weight	O
matrix	O
we	O
can	O
arbitrarily	O
define	O
wc	O
for	O
one	O
of	O
the	O
classes	O
say	O
c	O
c	O
since	O
py	O
cx	O
w	O
py	O
cx	O
w	O
in	O
this	O
case	O
the	O
model	O
has	O
the	O
form	O
c	O
x	O
py	O
cx	O
w	O
wt	O
wt	O
k	O
x	O
chapter	O
logistic	B
regression	B
if	O
we	O
don	O
t	O
clamp	O
one	O
of	O
the	O
vectors	O
to	O
some	O
constant	O
value	O
the	O
parameters	O
will	O
be	O
unidentifiable	O
however	O
suppose	O
we	O
don	O
t	O
clamp	O
wc	O
so	O
we	O
are	O
using	O
equation	O
but	O
we	O
add	O
regularization	B
by	O
optimizing	O
log	O
pyixi	O
w	O
show	O
that	O
at	O
the	O
optimum	O
we	O
have	O
still	O
need	O
to	O
enforce	O
that	O
to	O
ensure	O
identifiability	O
of	O
the	O
offset	O
wcj	O
for	O
j	O
the	O
unregularized	O
terms	O
we	O
exercise	O
elementary	O
properties	O
of	O
regularized	O
logistic	B
regression	B
jaaakkola	O
consider	O
minimizing	O
jw	O
where	O
i	O
d	O
log	O
i	O
w	O
is	O
the	O
average	O
log-likelihood	O
on	O
data	O
set	O
d	O
for	O
yi	O
answer	O
the	O
following	O
true	O
false	O
questions	O
a	O
jw	O
has	O
multiple	O
locally	O
optimal	O
solutions	O
tf	O
b	O
let	O
w	O
arg	O
minw	O
jw	O
be	O
a	O
global	O
optimum	O
w	O
is	O
sparse	B
many	O
zero	O
entries	O
tf	O
c	O
d	O
wdtrain	O
always	O
increases	O
as	O
we	O
increase	O
tf	O
e	O
wdtest	O
always	O
increases	O
as	O
we	O
increase	O
tf	O
if	O
the	O
training	O
data	O
is	O
linearly	B
separable	I
then	O
some	O
weights	O
wj	O
might	O
become	O
infinite	O
if	O
tf	O
exercise	O
regularizing	O
separate	O
terms	O
in	O
logistic	B
regression	B
jaaakkola	O
a	O
consider	O
the	O
data	O
in	O
figure	O
where	O
we	O
fit	O
the	O
model	O
py	O
w	O
suppose	O
we	O
fit	O
the	O
model	O
by	O
maximum	O
likelihood	B
i	O
e	O
we	O
minimize	O
jw	O
where	O
is	O
the	O
log	O
likelihood	B
on	O
the	O
training	B
set	I
sketch	O
a	O
possible	O
decision	B
boundary	I
corresponding	O
to	O
w	O
answer	O
on	O
your	O
copy	O
since	O
you	O
will	O
need	O
multiple	O
versions	O
of	O
this	O
figure	O
boundary	O
unique	O
how	O
many	O
classification	O
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
the	O
figure	O
first	O
rough	O
sketch	O
is	O
enough	O
and	O
then	O
superimpose	O
your	O
is	O
your	O
answer	O
b	O
now	O
suppose	O
we	O
regularize	O
only	O
the	O
parameter	B
i	O
e	O
we	O
minimize	O
w	O
suppose	O
is	O
a	O
very	O
large	O
number	O
so	O
we	O
regularize	O
all	O
the	O
way	O
to	O
but	O
all	O
other	O
parameters	O
are	O
unregularized	O
sketch	O
a	O
possible	O
decision	B
boundary	I
how	O
many	O
classification	O
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
hint	O
consider	O
the	O
behavior	O
of	O
simple	B
linear	B
regression	B
when	O
c	O
now	O
suppose	O
we	O
heavily	O
regularize	O
only	O
the	O
parameter	B
i	O
e	O
we	O
minimize	O
w	O
sketch	O
a	O
possible	O
decision	B
boundary	I
how	O
many	O
classification	O
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
generative	O
vs	O
discriminative	B
classifiers	O
figure	O
data	O
for	O
logistic	B
regression	B
question	O
d	O
now	O
suppose	O
we	O
heavily	O
regularize	O
only	O
the	O
parameter	B
sketch	O
a	O
possible	O
decision	B
boundary	I
how	O
many	O
classification	O
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
introduction	O
we	O
have	O
now	O
encountered	O
a	O
wide	O
variety	O
of	O
probability	O
distributions	O
the	O
gaussian	B
the	O
bernoulli	B
the	O
student	B
t	I
the	O
uniform	O
the	O
gamma	B
etc	O
it	O
turns	O
out	O
that	O
most	O
of	O
these	O
are	O
members	O
of	O
a	O
broader	O
class	O
of	O
distributions	O
known	O
as	O
the	O
exponential	O
in	O
this	O
chapter	O
we	O
discuss	O
various	O
properties	O
of	O
this	O
family	B
this	O
allows	O
us	O
to	O
derive	O
theorems	O
and	O
algorithms	O
with	O
very	O
broad	O
applicability	O
we	O
will	O
see	O
how	O
we	O
can	O
easily	O
use	O
any	O
member	O
of	O
the	O
exponential	B
family	B
as	O
a	O
class-conditional	B
density	I
in	O
order	O
to	O
make	O
a	O
generative	O
classifier	O
in	O
addition	O
we	O
will	O
discuss	O
how	O
to	O
build	O
discriminative	B
models	O
where	O
the	O
response	B
variable	I
has	O
an	O
exponential	B
family	B
distribution	O
whose	O
mean	B
is	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
this	O
is	O
known	O
as	O
a	O
generalized	B
linear	I
model	I
and	O
generalizes	O
the	O
idea	O
of	O
logistic	B
regression	B
to	O
other	O
kinds	O
of	O
response	O
variables	O
the	O
exponential	B
family	B
before	O
defining	O
the	O
exponential	B
family	B
we	O
mention	O
several	O
reasons	O
why	O
it	O
is	O
important	O
it	O
can	O
be	O
shown	O
that	O
under	O
certain	O
regularity	O
conditions	O
the	O
exponential	B
family	B
is	O
the	O
only	O
family	B
of	O
distributions	O
with	O
finite-sized	O
sufficient	B
statistics	I
meaning	O
that	O
we	O
can	O
compress	O
the	O
data	O
into	O
a	O
fixed-sized	O
summary	O
without	O
loss	B
of	O
information	B
this	O
is	O
particularly	O
useful	O
for	O
online	B
learning	B
as	O
we	O
will	O
see	O
later	O
the	O
exponential	B
family	B
is	O
the	O
only	O
family	B
of	O
distributions	O
for	O
which	O
conjugate	B
priors	I
exist	O
which	O
simplifies	O
the	O
computation	O
of	O
the	O
posterior	O
section	O
the	O
exponential	B
family	B
can	O
be	O
shown	O
to	O
be	O
the	O
family	B
of	O
distributions	O
that	O
makes	O
the	O
least	O
set	O
of	O
assumptions	O
subject	O
to	O
some	O
user-chosen	O
constraints	O
section	O
the	O
exponential	B
family	B
is	O
at	O
the	O
core	O
of	O
generalized	B
linear	I
models	I
as	O
discussed	O
in	O
section	O
the	O
exponential	B
family	B
is	O
at	O
the	O
core	O
of	O
variational	B
inference	B
as	O
discussed	O
in	O
section	O
the	O
exceptions	O
are	O
the	O
student	B
t	I
which	O
does	O
not	O
have	O
the	O
right	O
form	O
and	O
the	O
uniform	B
distribution	I
which	O
does	O
not	O
have	O
fixed	O
support	B
independent	O
of	O
the	O
parameter	B
values	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
definition	O
a	O
pdf	B
or	O
pmf	B
px	O
for	O
x	O
xm	O
x	O
m	O
and	O
r	O
exponential	B
family	B
if	O
it	O
is	O
of	O
the	O
form	O
d	O
is	O
said	O
to	O
be	O
in	O
the	O
px	O
z	O
hx	O
exp	O
t	O
hx	O
exp	O
t	O
a	O
x	O
m	O
hx	O
exp	O
t	O
where	O
z	O
a	O
log	O
z	O
here	O
are	O
called	O
the	O
natural	B
parameters	I
or	O
canonical	B
parameters	I
r	O
d	O
is	O
called	O
a	O
vector	O
of	O
sufficient	B
statistics	I
z	O
is	O
called	O
the	O
partition	B
function	I
a	O
is	O
called	O
the	O
log	B
partition	B
function	I
or	O
cumulant	B
function	I
and	O
hx	O
is	O
the	O
a	O
scaling	O
constant	O
often	O
if	O
x	O
we	O
say	O
it	O
is	O
a	O
natural	B
exponential	B
family	B
equation	O
can	O
be	O
generalized	O
by	O
writing	O
px	O
hx	O
exp	O
a	O
where	O
is	O
a	O
function	O
that	O
maps	O
the	O
parameters	O
to	O
the	O
canonical	B
parameters	I
if	O
dim	O
dim	O
it	O
is	O
called	O
a	O
curved	B
exponential	B
family	B
which	O
means	O
we	O
have	O
more	O
if	O
the	O
model	O
is	O
said	O
to	O
be	O
in	O
canonical	B
form	I
sufficient	B
statistics	I
than	O
parameters	O
we	O
will	O
assume	O
models	O
are	O
in	O
canonical	B
form	I
unless	O
we	O
state	B
otherwise	O
examples	O
let	O
us	O
consider	O
some	O
examples	O
to	O
make	O
things	O
clearer	O
bernoulli	B
the	O
bernoulli	B
for	O
x	O
can	O
be	O
written	O
in	O
exponential	B
family	B
form	O
as	O
follows	O
berx	O
x	O
expx	O
log	O
x	O
exp	O
where	O
ix	O
and	O
however	O
this	O
representation	O
is	O
over-complete	B
since	O
there	O
is	O
a	O
linear	O
dependendence	O
between	O
the	O
features	B
ix	O
consequently	O
is	O
not	O
uniquely	O
identifiable	O
it	O
is	O
common	O
to	O
require	O
that	O
the	O
representation	O
be	O
minimal	B
which	O
means	O
there	O
is	O
a	O
unique	O
associated	O
with	O
the	O
distribution	O
in	O
this	O
case	O
we	O
can	O
just	O
define	O
berx	O
exp	O
x	O
log	O
catx	O
k	O
exp	O
xk	O
k	O
k	O
k	O
exp	O
exp	O
exp	O
xk	O
log	O
k	O
xk	O
log	O
xk	O
log	O
k	O
k	O
xk	O
log	O
k	O
k	O
k	O
j	O
k	O
k	O
k	O
k	O
xk	O
log	O
k	O
where	O
k	O
catx	O
exp	O
t	O
a	O
k	O
k	O
log	O
k	O
ix	O
k	O
k	O
we	O
can	O
write	O
this	O
in	O
exponential	B
family	B
form	O
as	O
follows	O
the	O
exponential	B
family	B
now	O
we	O
have	O
log	O
can	O
recover	O
the	O
mean	B
parameter	B
from	O
the	O
canonical	O
parameter	B
using	O
which	O
is	O
the	O
log-odds	B
ratio	I
and	O
z	O
we	O
sigm	O
e	O
multinoulli	O
we	O
can	O
represent	O
the	O
multinoulli	O
as	O
a	O
minimal	B
exponential	B
family	B
as	O
follows	O
xk	O
ix	O
k	O
we	O
can	O
recover	O
the	O
mean	B
parameters	O
from	O
the	O
canonical	B
parameters	I
using	O
k	O
e	O
k	O
e	O
j	O
from	O
this	O
we	O
find	O
k	O
e	O
j	O
and	O
hence	O
a	O
log	O
e	O
j	O
k	O
e	O
k	O
e	O
j	O
if	O
we	O
define	O
k	O
we	O
can	O
write	O
s	O
and	O
a	O
log	O
softmax	B
function	O
in	O
equation	O
e	O
k	O
where	O
s	O
is	O
the	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
univariate	O
gaussian	B
the	O
univariate	O
gaussian	B
can	O
be	O
written	O
in	O
exponential	B
family	B
form	O
as	O
follows	O
n	O
exp	O
exp	O
x	O
z	O
exp	O
t	O
x	O
exp	O
log	O
where	O
z	O
a	O
non-examples	O
not	O
all	O
distributions	O
of	O
interest	O
belong	O
to	O
the	O
exponential	B
family	B
for	O
example	O
the	O
uniform	B
distribution	I
x	O
unifa	O
b	O
does	O
not	O
since	O
the	O
support	B
of	O
the	O
distribution	O
depends	O
on	O
the	O
parameters	O
also	O
the	O
student	B
t	I
distribution	I
does	O
not	O
belong	O
since	O
it	O
does	O
not	O
have	O
the	O
required	O
form	O
log	B
partition	B
function	I
an	O
important	O
property	O
of	O
the	O
exponential	B
family	B
is	O
that	O
derivatives	O
of	O
the	O
log	B
partition	B
function	I
for	O
this	O
reason	O
a	O
is	O
can	O
be	O
used	O
to	O
generate	O
cumulants	B
of	O
the	O
sufficient	O
sometimes	O
called	O
a	O
cumulant	B
function	I
we	O
will	O
prove	O
this	O
for	O
a	O
distribution	O
this	O
can	O
be	O
generalized	O
to	O
a	O
k-parameter	O
distribution	O
in	O
a	O
straightforward	O
way	O
for	O
the	O
first	O
the	O
first	O
and	O
second	O
cumulants	B
of	O
a	O
distribution	O
are	O
its	O
mean	B
e	O
and	O
variance	B
var	B
whereas	O
the	O
first	O
and	O
second	O
moments	O
are	O
its	O
mean	B
e	O
and	O
e	O
x	O
the	O
exponential	B
family	B
derivative	O
we	O
have	O
log	O
exp	O
exp	O
exp	O
exp	O
expa	O
exp	O
a	O
e	O
da	O
d	O
d	O
d	O
d	O
d	O
for	O
the	O
second	O
derivative	O
we	O
have	O
d	O
exp	O
a	O
hx	O
e	O
var	B
e	O
where	O
we	O
used	O
the	O
fact	O
that	O
da	O
in	O
the	O
multivariate	O
case	O
we	O
have	O
that	O
d	O
e	O
i	O
j	O
e	O
ix	O
jx	O
e	O
ix	O
e	O
jx	O
and	O
hence	O
cov	O
since	O
the	O
covariance	B
is	O
positive	O
definite	O
we	O
see	O
that	O
a	O
is	O
a	O
convex	B
function	O
section	O
example	O
the	O
bernoulli	B
distribution	O
for	O
example	O
consider	O
the	O
bernoulli	B
distribution	O
we	O
have	O
a	O
e	O
so	O
the	O
mean	B
is	O
given	O
by	O
da	O
d	O
e	O
e	O
e	O
sigm	O
the	O
variance	B
is	O
given	O
by	O
d	O
d	O
d	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
mle	B
for	O
the	O
exponential	B
family	B
the	O
likelihood	B
of	O
an	O
exponential	B
family	B
model	O
has	O
the	O
form	O
hxi	O
g	O
exp	O
we	O
see	O
that	O
the	O
sufficient	B
statistics	I
are	O
n	O
and	O
kxi	O
pd	O
for	O
example	O
for	O
the	O
bernoulli	B
model	O
we	O
have	O
gaussian	B
we	O
have	O
i	O
xi	O
i	O
i	O
also	O
need	O
to	O
know	O
the	O
sample	O
size	O
n	O
i	O
ixi	O
and	O
for	O
the	O
univariate	O
the	O
pitman-koopman-darmois	B
theorem	I
states	O
that	O
under	O
certain	O
regularity	O
conditions	O
the	O
exponential	B
family	B
is	O
the	O
only	O
family	B
of	O
distributions	O
with	O
finite	O
sufficient	B
statistics	I
finite	O
means	O
of	O
a	O
size	O
independent	O
of	O
the	O
size	O
of	O
the	O
data	O
set	O
one	O
of	O
the	O
conditions	O
required	O
in	O
this	O
theorem	O
is	O
that	O
the	O
support	B
of	O
the	O
distribution	O
not	O
be	O
dependent	O
on	O
the	O
parameter	B
for	O
a	O
simple	O
example	O
of	O
such	O
a	O
distribution	O
consider	O
the	O
uniform	B
distribution	I
px	O
u	O
x	O
the	O
likelihood	B
is	O
given	O
by	O
pd	O
n	O
maxxi	O
so	O
the	O
sufficient	B
statistics	I
are	O
n	O
and	O
sd	O
maxi	O
xi	O
this	O
is	O
finite	O
in	O
size	O
but	O
the	O
uniform	B
distribution	I
is	O
not	O
in	O
the	O
exponential	B
family	B
because	O
its	O
support	B
set	O
x	O
depends	O
on	O
the	O
parameters	O
n	O
iid	B
data	O
points	O
d	O
xn	O
the	O
log-likelihood	O
is	O
we	O
now	O
descibe	O
how	O
to	O
compute	O
the	O
mle	B
for	O
a	O
canonical	O
exponential	B
family	B
model	O
given	O
log	O
pd	O
t	O
n	O
a	O
since	O
a	O
is	O
concave	B
in	O
and	O
t	O
is	O
linear	O
in	O
we	O
see	O
that	O
the	O
log	O
likelihood	B
is	O
concave	B
and	O
hence	O
has	O
a	O
unique	O
global	O
maximum	O
to	O
derive	O
this	O
maximum	O
we	O
use	O
the	O
fact	O
that	O
the	O
derivative	O
of	O
the	O
log	B
partition	B
function	I
yields	O
the	O
expected	B
value	I
of	O
the	O
sufficient	O
statistic	O
vector	O
log	O
pd	O
n	O
e	O
setting	O
this	O
gradient	O
to	O
zero	O
we	O
see	O
that	O
at	O
the	O
mle	B
the	O
empirical	O
average	O
of	O
the	O
sufficient	B
statistics	I
must	O
equal	O
the	O
model	O
s	O
theoretical	O
expected	B
sufficient	B
statistics	I
i	O
e	O
must	O
satisfy	O
e	O
n	O
the	O
exponential	B
family	B
this	O
is	O
called	O
moment	B
matching	I
for	O
example	O
in	O
the	O
bernoulli	B
distribution	O
we	O
have	O
ix	O
so	O
the	O
mle	B
satisfies	O
e	O
px	O
n	O
ixi	O
bayes	O
for	O
the	O
exponential	B
family	B
we	O
have	O
seen	O
that	O
exact	O
bayesian	B
analysis	O
is	O
considerably	O
simplified	O
if	O
the	O
prior	O
is	O
conjugate	O
to	O
the	O
likelihood	B
informally	O
this	O
means	O
that	O
the	O
prior	O
p	O
has	O
the	O
same	O
form	O
as	O
the	O
likelihood	B
pd	O
for	O
this	O
to	O
make	O
sense	O
we	O
require	O
that	O
the	O
likelihood	B
have	O
finite	O
sufficient	B
statistics	I
so	O
that	O
we	O
can	O
write	O
pd	O
psd	O
this	O
suggests	O
that	O
the	O
only	O
family	B
of	O
distributions	O
for	O
which	O
conjugate	B
priors	I
exist	O
is	O
the	O
exponential	B
family	B
we	O
will	O
derive	O
the	O
form	O
of	O
the	O
prior	O
and	O
posterior	O
below	O
likelihood	B
the	O
likelihood	B
of	O
the	O
exponential	B
family	B
is	O
given	O
by	O
pd	O
g	O
exp	O
sn	O
pd	O
expn	O
t	O
s	O
n	O
a	O
where	O
sn	O
sxi	O
in	O
terms	O
of	O
the	O
canonical	B
parameters	I
this	O
becomes	O
where	O
s	O
n	O
sn	O
prior	O
the	O
natural	O
conjugate	B
prior	I
has	O
the	O
form	O
p	O
g	O
exp	O
let	O
us	O
write	O
to	O
separate	O
out	O
the	O
size	O
of	O
the	O
prior	O
pseudo-data	O
from	O
the	O
mean	B
of	O
the	O
sufficient	B
statistics	I
on	O
this	O
pseudo-data	O
in	O
canonical	B
form	I
the	O
prior	O
becomes	O
p	O
exp	O
t	O
posterior	O
the	O
posterior	O
is	O
given	O
by	O
p	O
p	O
n	O
n	O
p	O
n	O
sn	O
so	O
we	O
see	O
that	O
we	O
just	O
update	O
the	O
hyper-parameters	B
by	O
adding	O
in	O
canonical	B
form	I
this	O
becomes	O
p	O
exp	O
t	O
n	O
s	O
n	O
p	O
n	O
n	O
s	O
n	O
so	O
we	O
see	O
that	O
the	O
posterior	O
hyper-parameters	B
are	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mean	B
hyper-parameters	B
and	O
the	O
average	O
of	O
the	O
sufficient	B
statistics	I
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
posterior	B
predictive	B
density	I
let	O
us	O
derive	O
a	O
generic	O
expression	O
for	O
the	O
predictive	B
density	O
for	O
future	O
observables	O
given	O
past	O
data	O
d	O
xn	O
as	O
follows	O
for	O
notational	O
brevity	O
we	O
will	O
combine	O
the	O
sufficient	B
statistics	I
with	O
the	O
size	O
of	O
the	O
data	O
as	O
follows	O
sd	O
sd	O
and	O
so	O
the	O
prior	O
becomes	O
p	O
z	O
g	O
exp	O
the	O
likelihood	B
and	O
posterior	O
have	O
a	O
similar	B
form	O
hence	O
h	O
xi	O
z	O
sd	O
z	O
sd	O
z	O
sd	O
k	O
k	O
k	O
h	O
xi	O
exp	O
g	O
d	O
d	O
sk	O
xi	O
skxi	O
if	O
n	O
this	O
becomes	O
the	O
marginal	B
likelihood	B
of	O
normalizer	O
of	O
the	O
posterior	O
divided	O
by	O
the	O
normalizer	O
of	O
the	O
prior	O
multiplied	O
by	O
a	O
constant	O
which	O
reduces	O
to	O
the	O
familiar	O
form	O
of	O
example	O
bernoulli	B
distribution	O
as	O
a	O
simple	O
example	O
let	O
us	O
revisit	O
the	O
beta-bernoulli	O
model	O
in	O
our	O
new	O
notation	O
the	O
likelihood	B
is	O
given	O
by	O
pd	O
exp	O
log	O
hence	O
the	O
conjugate	B
prior	I
is	O
given	O
by	O
p	O
exp	O
log	O
i	O
xi	O
if	O
we	O
define	O
and	O
we	O
see	O
that	O
this	O
is	O
a	O
beta	B
distribution	I
i	O
ixi	O
is	O
the	O
sufficient	O
statistic	O
we	O
can	O
derive	O
the	O
posterior	O
as	O
follows	O
where	O
s	O
p	O
s	O
n	O
n	O
n	O
we	O
can	O
derive	O
the	O
posterior	B
predictive	B
distribution	I
as	O
follows	O
assume	O
p	O
beta	O
and	O
let	O
s	O
sd	O
be	O
the	O
number	O
of	O
heads	O
in	O
the	O
past	O
data	O
we	O
can	O
predict	O
the	O
probability	O
of	O
a	O
the	O
exponential	B
family	B
given	O
sequence	O
of	O
future	O
heads	O
as	O
follows	O
n	O
nd	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
where	O
nm	O
nm	O
nm	O
nm	O
nm	O
nm	O
n	O
nm	O
n	O
s	O
s	O
maximum	B
entropy	B
derivation	O
of	O
the	O
exponential	B
family	B
xm	O
with	O
sufficient	O
statistic	O
i	O
xi	O
although	O
the	O
exponential	B
family	B
is	O
convenient	O
is	O
there	O
any	O
deeper	O
justification	O
for	O
its	O
use	O
it	O
turns	O
out	O
that	O
there	O
is	O
it	O
is	O
the	O
distribution	O
that	O
makes	O
the	O
least	O
number	O
of	O
assumptions	O
about	O
the	O
data	O
subject	O
to	O
a	O
specific	O
set	O
of	O
user-specified	O
constraints	O
as	O
we	O
explain	O
below	O
in	O
particular	O
suppose	O
all	O
we	O
know	O
is	O
the	O
expected	O
values	O
of	O
certain	O
features	B
or	O
functions	O
fkxpx	O
fk	O
x	O
where	O
fk	O
are	O
known	O
constants	O
and	O
fkx	O
is	O
an	O
arbitrary	O
function	O
the	O
principle	O
of	O
maximum	B
entropy	B
or	O
maxent	B
says	O
we	O
should	O
pick	O
the	O
distribution	O
with	O
maximum	B
entropy	B
to	O
uniform	O
subject	O
to	O
the	O
constraints	O
that	O
the	O
moments	O
of	O
the	O
distribution	O
match	O
the	O
empirical	O
moments	O
of	O
the	O
specified	O
functions	O
px	O
and	O
jp	O
to	O
maximize	O
entropy	B
subject	O
to	O
the	O
constraints	O
in	O
equation	O
and	O
the	O
constraints	O
that	O
x	O
px	O
we	O
need	O
to	O
use	O
lagrange	B
multipliers	I
the	O
lagrangian	B
is	O
given	O
by	O
px	O
log	O
px	O
kfk	O
px	O
pxfkx	O
x	O
x	O
k	O
x	O
we	O
can	O
use	O
the	O
calculus	B
of	I
variations	I
to	O
take	O
derivatives	O
wrt	O
the	O
function	O
p	O
but	O
we	O
will	O
adopt	O
a	O
simpler	O
approach	O
and	O
treat	O
p	O
as	O
a	O
fixed	O
length	O
vector	O
we	O
are	O
assuming	O
x	O
is	O
discrete	B
then	O
we	O
have	O
k	O
j	O
px	O
log	O
px	O
setting	O
j	O
px	O
yields	O
exp	O
z	O
k	O
px	O
kfkx	O
kfkx	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
w	O
xi	O
g	O
g	O
i	O
i	O
i	O
figure	O
a	O
visualization	O
of	O
the	O
various	O
features	B
of	O
a	O
glm	B
based	O
on	O
figure	O
of	O
where	O
z	O
using	O
the	O
sum	O
to	O
one	O
constraint	O
we	O
have	O
x	O
px	O
z	O
exp	O
x	O
k	O
kfkx	O
hence	O
the	O
normalization	O
constant	O
is	O
given	O
by	O
z	O
exp	O
kfkx	O
x	O
k	O
thus	O
the	O
maxent	B
distribution	O
px	O
has	O
the	O
form	O
of	O
the	O
exponential	B
family	B
also	O
known	O
as	O
the	O
gibbs	B
distribution	I
generalized	B
linear	I
models	I
linear	O
and	O
logistic	B
regression	B
are	O
examples	O
of	O
generalized	B
linear	I
models	I
or	O
glms	O
and	O
nelder	O
these	O
are	O
models	O
in	O
which	O
the	O
output	O
density	O
is	O
in	O
the	O
exponential	B
family	B
and	O
in	O
which	O
the	O
mean	B
parameters	O
are	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
passed	O
through	O
a	O
possibly	O
nonlinear	O
function	O
such	O
as	O
the	O
logistic	B
function	O
we	O
describe	O
glms	O
in	O
more	O
detail	O
below	O
we	O
focus	O
on	O
scalar	O
outputs	O
for	O
notational	O
simplicity	O
excludes	O
multinomial	B
logistic	B
regression	B
but	O
this	O
is	O
just	O
to	O
simplify	O
the	O
presentation	O
basics	O
to	O
understand	O
glms	O
let	O
us	O
first	O
consider	O
the	O
case	O
of	O
an	O
unconditional	O
dstribution	O
for	O
a	O
scalar	O
response	B
variable	I
pyi	O
exp	O
cyi	O
yi	O
a	O
where	O
is	O
the	O
dispersion	B
parameter	B
set	O
to	O
is	O
the	O
natural	O
parameter	B
a	O
is	O
the	O
partition	B
function	I
and	O
c	O
is	O
a	O
normalization	O
constant	O
for	O
example	O
in	O
the	O
case	O
of	O
logistic	B
regression	B
is	O
the	O
log-odds	B
ratio	I
log	O
where	O
e	O
y	O
is	O
the	O
mean	B
parameter	B
section	O
to	O
convert	O
from	O
the	O
mean	B
parameter	B
to	O
the	O
natural	O
parameter	B
generalized	B
linear	I
models	I
distrib	O
n	O
binn	O
poi	O
link	O
g	O
identity	O
logit	B
log	O
log	O
log	O
sigm	O
e	O
e	O
table	O
canonical	O
link	O
functions	O
and	O
their	O
inverses	O
for	O
some	O
common	O
glms	O
we	O
can	O
use	O
a	O
function	O
so	O
this	O
function	O
is	O
uniquely	O
determined	O
by	O
the	O
form	O
of	O
the	O
exponential	B
family	B
distribution	O
in	O
fact	O
this	O
is	O
an	O
invertible	O
mapping	O
so	O
we	O
have	O
furthermore	O
we	O
know	O
from	O
section	O
that	O
the	O
mean	B
is	O
given	O
by	O
the	O
derivative	O
of	O
the	O
partition	B
function	I
so	O
we	O
have	O
now	O
let	O
us	O
add	O
inputs	O
covariates	B
we	O
first	O
define	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
i	O
wt	O
xi	O
we	O
now	O
make	O
the	O
mean	B
of	O
the	O
distribution	O
be	O
some	O
invertible	O
monotonic	O
function	O
of	O
this	O
linear	O
combination	O
by	O
convention	O
this	O
function	O
known	O
as	O
the	O
mean	B
function	I
is	O
denoted	O
by	O
g	O
so	O
i	O
g	O
i	O
g	O
xi	O
see	O
figure	O
for	O
a	O
summary	O
of	O
the	O
basic	O
model	O
the	O
inverse	O
of	O
the	O
mean	B
function	I
namely	O
g	O
is	O
called	O
the	O
link	B
function	I
we	O
are	O
free	O
to	O
choose	O
almost	O
any	O
function	O
we	O
like	O
for	O
g	O
so	O
long	O
as	O
it	O
is	O
invertible	O
and	O
so	O
long	O
as	O
g	O
has	O
the	O
appropriate	O
range	O
for	O
example	O
in	O
logistic	B
regression	B
we	O
set	O
i	O
g	O
i	O
sigm	O
i	O
one	O
particularly	O
simple	O
form	O
of	O
link	B
function	I
is	O
to	O
use	O
g	O
this	O
is	O
called	O
the	O
canonical	B
link	B
function	I
in	O
this	O
case	O
i	O
i	O
wt	O
xi	O
so	O
the	O
model	O
becomes	O
pyixi	O
w	O
exp	O
cyi	O
yiwt	O
xi	O
awt	O
xi	O
in	O
table	O
we	O
list	O
some	O
distributions	O
and	O
their	O
canonical	O
link	O
functions	O
we	O
see	O
that	O
for	O
the	O
bernoulli	B
binomial	B
distribution	I
the	O
canonical	O
link	O
is	O
the	O
logit	B
function	O
g	O
log	O
whose	O
inverse	O
is	O
the	O
logistic	B
function	O
sigm	O
based	O
on	O
the	O
results	O
in	O
section	O
we	O
can	O
show	O
that	O
the	O
mean	B
and	O
variance	B
of	O
the	O
response	B
variable	I
are	O
as	O
follows	O
yxi	O
w	O
yxi	O
w	O
e	O
var	B
i	O
i	O
i	O
i	O
to	O
make	O
the	O
notation	O
clearer	O
let	O
us	O
consider	O
some	O
simple	O
examples	O
for	O
linear	B
regression	B
we	O
have	O
log	O
pyixi	O
w	O
yi	O
i	O
i	O
i	O
where	O
yi	O
r	O
and	O
i	O
i	O
wt	O
xi	O
here	O
a	O
soe	O
i	O
and	O
var	B
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
for	O
binomial	B
regression	B
we	O
have	O
log	O
pyixi	O
w	O
i	O
log	O
i	O
i	O
i	O
i	O
log	O
ni	O
yi	O
where	O
yi	O
ni	O
i	O
sigmwt	O
xi	O
i	O
log	O
i	O
wt	O
xi	O
and	O
here	O
a	O
ni	O
e	O
so	O
e	O
ni	O
i	O
i	O
var	B
ni	O
i	O
for	O
poisson	B
regression	B
we	O
have	O
log	O
pyixi	O
w	O
i	O
log	O
i	O
i	O
logyi	O
where	O
yi	O
i	O
expwt	O
xi	O
i	O
log	O
i	O
wt	O
xi	O
and	O
here	O
a	O
so	O
e	O
var	B
i	O
poisson	B
regression	B
is	O
widely	O
used	O
in	O
bio-statistical	O
applications	O
where	O
yi	O
might	O
represent	O
the	O
number	O
of	O
diseases	O
of	O
a	O
given	O
person	O
or	O
place	O
or	O
the	O
number	O
of	O
reads	O
at	O
a	O
genomic	O
location	O
in	O
a	O
high-throughput	O
sequencing	O
context	O
e	O
g	O
et	O
al	O
ml	O
and	O
map	O
estimation	O
one	O
of	O
the	O
appealing	O
properties	O
of	O
glms	O
is	O
that	O
they	O
can	O
be	O
fit	O
using	O
exactly	O
the	O
same	O
methods	O
that	O
we	O
used	O
to	O
fit	O
logistic	B
regression	B
in	O
particular	O
the	O
log-likelihood	O
has	O
the	O
following	O
form	O
log	O
pdw	O
iyi	O
a	O
i	O
we	O
can	O
compute	O
the	O
gradient	O
vector	O
using	O
the	O
chain	B
rule	I
as	O
follows	O
dwj	O
d	O
i	O
d	O
i	O
d	O
i	O
i	O
d	O
i	O
d	O
i	O
i	O
d	O
i	O
d	O
i	O
d	O
i	O
dwj	O
d	O
i	O
d	O
i	O
d	O
i	O
d	O
i	O
xij	O
d	O
i	O
d	O
i	O
xij	O
if	O
we	O
use	O
a	O
canonical	O
link	O
i	O
i	O
this	O
simplifies	O
to	O
ixi	O
which	O
is	O
a	O
sum	O
of	O
the	O
input	O
vectors	O
weighted	O
by	O
the	O
errors	O
this	O
can	O
be	O
used	O
inside	O
a	O
gradient	B
descent	I
procedure	O
discussed	O
in	O
section	O
however	O
for	O
improved	O
efficiency	O
we	O
should	O
use	O
a	O
second-order	O
method	O
if	O
we	O
use	O
a	O
canonical	O
link	O
the	O
hessian	B
is	O
given	O
by	O
h	O
d	O
i	O
d	O
i	O
xixt	O
i	O
xt	O
sx	O
probit	B
regression	B
name	O
logistic	B
probit	B
log-log	O
complementary	O
log-log	O
formula	O
g	O
sigm	O
e	O
g	O
g	O
exp	O
exp	O
g	O
exp	O
exp	O
table	O
summary	O
of	O
some	O
possible	O
mean	B
functions	O
for	O
binary	O
regression	B
where	O
s	O
diag	O
d	O
d	O
irls	B
algorithm	O
specifically	O
we	O
have	O
the	O
following	O
newton	O
update	O
is	O
a	O
diagonal	B
weighting	O
matrix	O
this	O
can	O
be	O
used	O
inside	O
the	O
d	O
n	O
d	O
n	O
stx	O
zt	O
t	O
s	O
stzt	O
t	O
t	O
where	O
t	O
xwt	O
and	O
t	O
g	O
t	O
if	O
we	O
extend	O
the	O
derivation	O
to	O
handle	O
non-canonical	O
links	O
we	O
find	O
that	O
the	O
hessian	B
has	O
another	O
term	O
however	O
it	O
turns	O
out	O
that	O
the	O
expected	O
hessian	B
is	O
the	O
same	O
as	O
in	O
equation	O
using	O
the	O
expected	O
hessian	B
as	O
the	O
fisher	B
information	B
matrix	I
instead	O
of	O
the	O
actual	O
hessian	B
is	O
known	O
as	O
the	O
fisher	B
scoring	I
method	I
it	O
is	O
straightforward	O
to	O
modify	O
the	O
above	O
procedure	O
to	O
perform	O
map	O
estimation	O
with	O
a	O
gaussian	B
prior	O
we	O
just	O
modify	O
the	O
objective	O
gradient	O
and	O
hessian	B
just	O
as	O
we	O
added	O
regularization	B
to	O
logistic	B
regression	B
in	O
section	O
bayesian	B
inference	B
bayesian	B
inference	B
for	O
glms	O
is	O
usually	O
conducted	O
using	O
mcmc	B
possible	O
methods	O
include	O
metropolis	B
hastings	I
with	O
an	O
irls-based	O
proposal	O
gibbs	B
sampling	I
using	O
adaptive	B
rejection	B
sampling	I
for	O
each	O
full-conditional	O
and	O
smith	O
etc	O
see	O
e	O
g	O
et	O
al	O
for	O
futher	O
information	B
it	O
is	O
also	O
possible	O
to	O
use	O
the	O
gaussian	B
approximation	I
or	O
variational	B
inference	B
probit	B
regression	B
in	O
logistic	B
regression	B
we	O
use	O
a	O
model	O
of	O
the	O
form	O
py	O
w	O
sigmwt	O
xi	O
in	O
general	O
we	O
can	O
write	O
py	O
w	O
g	O
xi	O
for	O
any	O
function	O
g	O
that	O
maps	O
to	O
several	O
possible	O
mean	B
functions	O
are	O
listed	O
in	O
table	O
in	O
this	O
section	O
we	O
focus	O
on	O
the	O
case	O
where	O
g	O
where	O
is	O
the	O
cdf	B
of	O
the	O
standard	B
normal	B
this	O
is	O
known	O
as	O
probit	B
regression	B
the	O
probit	B
function	O
is	O
very	O
similar	B
to	O
the	O
logistic	B
function	O
as	O
shown	O
in	O
figure	O
however	O
this	O
model	O
has	O
some	O
advantages	O
over	O
logistic	B
regression	B
as	O
we	O
will	O
see	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
mlmap	O
estimation	O
using	O
gradient-based	O
optimization	B
we	O
can	O
find	O
the	O
mle	B
for	O
probit	B
regression	B
using	O
standard	O
gradient	O
methods	O
let	O
i	O
wt	O
xi	O
and	O
let	O
yi	O
then	O
the	O
gradient	O
of	O
the	O
log-likelihod	O
for	O
a	O
specific	O
case	O
is	O
given	O
by	O
gi	O
d	O
dw	O
log	O
p	O
yiwt	O
xi	O
d	O
i	O
dw	O
d	O
d	O
i	O
log	O
p	O
yiwt	O
xi	O
xi	O
yi	O
i	O
yi	O
i	O
where	O
is	O
the	O
standard	B
normal	B
pdf	B
and	O
is	O
its	O
cdf	B
similarly	O
the	O
hessian	B
for	O
a	O
single	O
case	O
is	O
given	O
by	O
hi	O
d	O
log	O
p	O
yiwt	O
xi	O
xi	O
xt	O
i	O
yi	O
i	O
i	O
yi	O
i	O
yi	O
we	O
can	O
modify	O
these	O
expressions	O
to	O
compute	O
the	O
map	B
estimate	I
in	O
a	O
straightforward	O
manner	O
in	O
particular	O
if	O
we	O
use	O
the	O
prior	O
pw	O
the	O
gradient	O
and	O
hessian	B
of	O
the	O
penalized	O
these	O
expressions	O
can	O
be	O
log	O
likelihood	B
have	O
the	O
form	O
passed	O
to	O
any	O
gradient-based	O
optimizer	O
see	O
probitregdemo	O
for	O
a	O
demo	O
i	O
hi	O
i	O
gi	O
w	O
and	O
latent	B
variable	O
interpretation	O
we	O
can	O
interpret	O
the	O
probit	B
logistic	B
model	O
as	O
follows	O
first	O
let	O
us	O
associate	O
each	O
item	O
xi	O
with	O
two	O
latent	B
utilities	B
and	O
corresponding	O
to	O
the	O
possible	O
choices	O
of	O
yi	O
and	O
yi	O
we	O
then	O
assume	O
that	O
the	O
observed	O
choice	O
is	O
whichever	O
action	B
has	O
larger	O
utility	O
more	O
precisely	O
the	O
model	O
is	O
as	O
follows	O
wt	O
xi	O
wt	O
xi	O
yi	O
where	O
s	O
are	O
error	O
terms	O
representing	O
all	O
the	O
other	O
factors	B
that	O
might	O
be	O
relevant	O
in	O
decision	B
making	O
that	O
we	O
have	O
chosen	O
not	O
to	O
are	O
unable	O
to	O
model	O
this	O
is	O
called	O
a	O
random	B
utility	I
model	I
or	O
rum	B
train	O
since	O
it	O
is	O
only	O
the	O
difference	O
in	O
utilities	B
that	O
matters	O
let	O
us	O
define	O
zi	O
where	O
if	O
the	O
s	O
have	O
a	O
gaussian	B
distribution	O
then	O
so	O
does	O
thus	O
we	O
can	O
write	O
zi	O
wt	O
xi	O
n	O
yi	O
izi	O
following	O
and	O
fruhwirth	O
we	O
call	O
this	O
the	O
difference	O
rum	B
or	O
drum	B
model	O
when	O
we	O
marginalize	O
out	O
zi	O
we	O
recover	O
the	O
probit	B
model	O
pyi	O
w	O
izi	O
xi	O
pwt	O
xi	O
p	O
wt	O
xi	O
wt	O
xi	O
xi	O
probit	B
regression	B
where	O
we	O
used	O
the	O
symmetry	O
of	O
the	O
this	O
latent	B
variable	O
interpretation	O
provides	O
an	O
alternative	O
way	O
to	O
fit	O
the	O
model	O
as	O
discussed	O
in	O
section	O
interestingly	O
if	O
we	O
use	O
a	O
gumbel	B
distribution	O
for	O
the	O
s	O
we	O
induce	O
a	O
logistic	B
distibution	O
for	O
and	O
the	O
model	O
reduces	O
to	O
logistic	B
regression	B
see	O
section	O
for	O
further	O
details	O
ordinal	B
probit	B
regression	B
one	O
advantage	O
of	O
the	O
latent	B
variable	O
interpretation	O
of	O
probit	B
regression	B
is	O
that	O
it	O
is	O
easy	O
to	O
extend	O
to	O
the	O
case	O
where	O
the	O
response	B
variable	I
is	O
ordinal	B
that	O
is	O
it	O
can	O
take	O
on	O
c	O
discrete	B
values	O
which	O
can	O
be	O
ordered	O
in	O
some	O
way	O
such	O
as	O
low	O
medium	O
and	O
high	O
this	O
is	O
called	O
ordinal	B
regression	B
the	O
basic	O
idea	O
is	O
as	O
follows	O
we	O
introduce	O
c	O
thresholds	O
j	O
and	O
set	O
if	O
j	O
zi	O
j	O
yi	O
j	O
where	O
c	O
for	O
identifiability	O
reasons	O
we	O
set	O
and	O
c	O
for	O
example	O
if	O
c	O
this	O
reduces	O
to	O
the	O
standard	O
binary	O
probit	B
model	O
whereby	O
zi	O
produces	O
yi	O
and	O
zi	O
produces	O
yi	O
if	O
c	O
we	O
partition	O
the	O
real	O
line	O
into	O
intervals	O
we	O
can	O
vary	O
the	O
parameter	B
to	O
ensure	O
the	O
right	O
relative	O
amount	O
of	O
probability	O
mass	O
falls	O
in	O
each	O
interval	O
so	O
as	O
to	O
match	O
the	O
empirical	O
frequencies	O
of	O
each	O
class	O
label	B
finding	O
the	O
mles	O
for	O
this	O
model	O
is	O
a	O
bit	O
trickier	O
than	O
for	O
binary	O
probit	B
regression	B
since	O
we	O
need	O
to	O
optimize	O
for	O
w	O
and	O
and	O
the	O
latter	O
must	O
obey	O
an	O
ordering	O
constraint	O
see	O
e	O
g	O
and	O
largey	O
for	O
an	O
approach	O
based	O
on	O
em	B
it	O
is	O
also	O
possible	O
to	O
derive	O
a	O
simple	O
gibbs	B
sampling	I
algorithm	O
for	O
this	O
model	O
e	O
g	O
multinomial	B
probit	B
models	O
now	O
consider	O
the	O
case	O
where	O
the	O
response	B
variable	I
can	O
take	O
on	O
c	O
unordered	O
categorical	B
values	O
yi	O
c	O
the	O
multinomial	B
probit	B
model	O
is	O
defined	O
as	O
follows	O
zic	O
wt	O
xic	O
n	O
r	O
yi	O
arg	O
max	O
zic	O
c	O
and	O
endersby	O
scott	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
for	O
see	O
e	O
g	O
more	O
details	O
on	O
the	O
model	O
and	O
its	O
connection	O
to	O
multinomial	B
logistic	B
regression	B
defining	O
w	O
wc	O
and	O
xic	O
xi	O
we	O
can	O
recover	O
the	O
more	O
familiar	O
formulation	O
zic	O
xt	O
i	O
wc	O
since	O
only	O
relative	O
utilities	B
matter	O
we	O
constrain	O
r	O
to	O
be	O
a	O
correlation	B
matrix	I
if	O
instead	O
of	O
setting	O
yi	O
argmaxc	O
zic	O
we	O
use	O
yic	O
izic	O
we	O
get	O
a	O
model	O
known	O
as	O
multivariate	B
probit	B
which	O
is	O
one	O
way	O
to	O
model	O
c	O
correlated	O
binary	O
outcomes	O
e	O
g	O
et	O
al	O
note	O
that	O
the	O
assumption	O
that	O
the	O
gaussian	B
noise	O
term	O
is	O
zero	O
mean	B
and	O
unit	O
variance	B
is	O
made	O
without	O
loss	B
of	O
generality	O
to	O
see	O
why	O
suppose	O
we	O
used	O
some	O
other	O
mean	B
and	O
variance	B
then	O
we	O
could	O
easily	O
rescale	O
w	O
and	O
add	O
an	O
offset	O
term	O
without	O
changing	O
the	O
likelihood	B
since	O
p	O
wt	O
x	O
p	O
x	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
multi-task	B
learning	B
sometimes	O
we	O
want	O
to	O
fit	O
many	O
related	O
classification	O
or	O
regression	B
models	O
it	O
is	O
often	O
reasonable	O
to	O
assume	O
the	O
input-output	O
mapping	O
is	O
similar	B
across	O
these	O
different	O
models	O
so	O
we	O
can	O
get	O
better	O
performance	O
by	O
fitting	O
all	O
the	O
parameters	O
at	O
the	O
same	O
time	O
in	O
machine	B
learning	B
this	O
setup	O
is	O
often	O
called	O
multi-task	B
learning	B
transfer	B
learning	B
et	O
al	O
or	O
learning	B
to	I
learn	I
and	O
pratt	O
in	O
statistics	O
this	O
is	O
usually	O
tackled	O
using	O
hierarchical	B
bayesian	B
models	I
and	O
heskes	O
as	O
we	O
discuss	O
below	O
although	O
there	O
are	O
other	O
possible	O
methods	O
e	O
g	O
hierarchical	O
bayes	O
for	O
multi-task	B
learning	B
let	O
yij	O
be	O
the	O
response	O
of	O
the	O
i	O
th	O
item	O
in	O
groupj	O
for	O
i	O
j	O
and	O
j	O
for	O
example	O
j	O
might	O
index	O
schools	O
i	O
might	O
index	O
students	O
within	O
a	O
school	O
and	O
yij	O
might	O
be	O
the	O
test	O
score	O
as	O
in	O
section	O
or	O
j	O
might	O
index	O
people	O
and	O
i	O
might	O
index	O
purchaes	O
and	O
yij	O
might	O
be	O
the	O
identity	O
of	O
the	O
item	O
that	O
was	O
purchased	O
is	O
known	O
as	O
discrete	B
choice	I
modeling	I
let	O
xij	O
be	O
a	O
feature	O
vector	O
associated	O
with	O
yij	O
the	O
goal	O
is	O
to	O
fit	O
the	O
models	O
pyjxj	O
for	O
all	O
j	O
although	O
some	O
groups	O
may	O
have	O
lots	O
of	O
data	O
there	O
is	O
often	O
a	O
long	B
tail	I
where	O
the	O
majority	O
of	O
groups	O
have	O
little	O
data	O
thus	O
we	O
can	O
t	O
reliably	O
fit	O
each	O
model	O
separately	O
but	O
we	O
don	O
t	O
want	O
to	O
use	O
the	O
same	O
model	O
for	O
all	O
groups	O
as	O
a	O
compromise	O
we	O
can	O
fit	O
a	O
separate	O
model	O
for	O
each	O
group	O
but	O
encourage	O
the	O
model	O
parameters	O
to	O
be	O
similar	B
across	O
groups	O
more	O
precisely	O
suppose	O
e	O
gxt	O
ij	O
j	O
where	O
g	O
is	O
the	O
link	B
function	I
for	O
the	O
glm	B
furthermore	O
suppose	O
j	O
n	O
in	O
this	O
model	O
groups	O
with	O
small	O
sample	O
size	O
borrow	B
statistical	I
strength	I
from	O
the	O
groups	O
with	O
larger	O
sample	O
size	O
because	O
the	O
j	O
s	O
are	O
correlated	O
via	O
the	O
latent	B
common	O
parents	B
section	O
for	O
further	O
discussion	O
of	O
this	O
point	O
the	O
term	O
j	O
controls	O
how	O
much	O
group	O
j	O
depends	O
on	O
the	O
common	O
parents	B
and	O
the	O
term	O
controls	O
the	O
strength	O
of	O
the	O
overall	O
prior	O
j	O
i	O
and	O
that	O
n	O
i	O
suppose	O
for	O
simplicity	O
that	O
and	O
that	O
j	O
and	O
are	O
all	O
known	O
they	O
could	O
be	O
set	O
by	O
cross	B
validation	I
the	O
overall	O
log	O
probability	O
has	O
the	O
form	O
log	O
pd	O
log	O
p	O
log	O
pdj	O
j	O
j	O
j	O
j	O
we	O
can	O
perform	O
map	O
estimation	O
of	O
using	O
standard	O
gradient	O
methods	O
alternatively	O
we	O
can	O
perform	O
an	O
iterative	O
optimization	B
scheme	O
alternating	O
between	O
optimizing	O
the	O
j	O
and	O
the	O
since	O
the	O
likelihood	B
and	O
prior	O
are	O
convex	B
this	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
note	O
that	O
once	O
the	O
models	O
are	O
trained	O
we	O
can	O
discard	O
and	O
use	O
each	O
model	O
separately	O
application	O
to	O
personalized	O
email	O
spam	B
filtering	B
an	O
interesting	O
application	O
of	O
multi-task	B
learning	B
is	O
personalized	O
spam	B
filtering	B
suppose	O
we	O
want	O
to	O
fit	O
one	O
classifier	O
per	O
user	O
j	O
since	O
most	O
users	O
do	O
not	O
label	B
their	O
email	O
as	O
spam	B
or	O
not	O
it	O
will	O
be	O
hard	O
to	O
estimate	O
these	O
models	O
independently	O
so	O
we	O
will	O
let	O
the	O
j	O
have	O
a	O
common	O
prior	O
representing	O
the	O
parameters	O
of	O
a	O
generic	O
user	O
multi-task	B
learning	B
in	O
this	O
case	O
we	O
can	O
emulate	O
the	O
behavior	O
of	O
the	O
above	O
model	O
with	O
a	O
simple	O
trick	O
attenberg	O
et	O
al	O
weinberger	O
et	O
al	O
we	O
make	O
two	O
copies	O
of	O
each	O
feature	O
xi	O
one	O
concatenated	O
with	O
the	O
user	O
id	O
and	O
one	O
not	O
the	O
effect	O
will	O
be	O
to	O
learn	O
a	O
predictor	O
of	O
the	O
form	O
e	O
u	O
wj	O
iu	O
iu	O
jxi	O
where	O
u	O
is	O
the	O
user	O
id	O
in	O
other	O
words	O
e	O
u	O
j	O
t	O
wjt	O
xi	O
thus	O
will	O
be	O
estimated	O
from	O
everyone	O
s	O
email	O
whereas	O
wj	O
will	O
just	O
be	O
estimated	O
from	O
user	O
j	O
s	O
email	O
to	O
see	O
the	O
correspondence	B
with	O
the	O
above	O
hierarchical	B
bayesian	B
model	I
define	O
wj	O
j	O
then	O
the	O
log	O
probability	O
of	O
the	O
original	O
model	O
can	O
be	O
rewritten	O
as	O
log	O
pdj	O
wj	O
j	O
j	O
if	O
we	O
assume	O
j	O
the	O
effect	O
is	O
the	O
same	O
as	O
using	O
the	O
augmented	O
feature	O
trick	O
with	O
the	O
same	O
regularizer	O
strength	O
for	O
both	O
wj	O
and	O
however	O
one	O
typically	O
gets	O
better	O
performance	O
by	O
not	O
requiring	O
that	O
j	O
be	O
equal	O
to	O
and	O
manning	O
application	O
to	O
domain	B
adaptation	I
domain	B
adaptation	I
is	O
the	O
problem	O
of	O
training	O
a	O
set	O
of	O
classifiers	O
on	O
data	O
drawn	O
from	O
different	O
distributions	O
such	O
as	O
email	O
and	O
newswire	O
text	O
this	O
problem	O
is	O
obviously	O
a	O
special	O
case	O
of	O
multi-task	B
learning	B
where	O
the	O
tasks	O
are	O
the	O
same	O
and	O
manning	O
used	O
the	O
above	O
hierarchical	B
bayesian	B
model	I
to	O
perform	O
domain	B
adaptation	I
for	O
two	O
nlp	O
tasks	O
namely	O
named	O
entity	O
recognition	O
and	O
parsing	O
they	O
report	O
reasonably	O
large	O
improvements	O
over	O
fitting	O
separate	O
models	O
to	O
each	O
dataset	O
and	O
small	O
improvements	O
over	O
the	O
approach	O
of	O
pooling	O
all	O
the	O
data	O
and	O
fitting	O
a	O
single	O
model	O
other	O
kinds	O
of	O
prior	O
in	O
multi-task	B
learning	B
it	O
is	O
common	O
to	O
assume	O
that	O
the	O
prior	O
is	O
gaussian	B
however	O
sometimes	O
other	O
priors	O
are	O
more	O
suitable	O
for	O
example	O
consider	O
the	O
task	O
of	O
conjoint	B
analysis	I
which	O
requires	O
figuring	O
out	O
which	O
features	B
of	O
a	O
product	O
customers	O
like	O
best	O
this	O
can	O
be	O
modelled	O
using	O
the	O
same	O
hierarchical	O
bayesian	B
setup	O
as	O
above	O
but	O
where	O
we	O
use	O
a	O
sparsity-promoting	B
prior	I
on	O
j	O
rather	O
than	O
a	O
gaussian	B
prior	O
this	O
is	O
called	O
multi-task	B
feature	B
selection	I
see	O
e	O
g	O
et	O
al	O
argyriou	O
et	O
al	O
for	O
some	O
possible	O
approaches	O
it	O
is	O
not	O
always	O
reasonable	O
to	O
assume	O
that	O
all	O
tasks	O
are	O
all	O
equally	O
similar	B
if	O
we	O
pool	O
the	O
parameters	O
across	O
tasks	O
that	O
are	O
qualitatively	O
different	O
the	O
performance	O
will	O
be	O
worse	O
than	O
not	O
using	O
pooling	O
because	O
the	O
inductive	B
bias	B
of	O
our	O
prior	O
is	O
wrong	O
indeed	O
it	O
has	O
been	O
found	O
experimentally	O
that	O
sometimes	O
multi-task	B
learning	B
does	O
worse	O
than	O
solving	O
each	O
task	O
separately	O
is	O
called	O
negative	B
transfer	I
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
one	O
way	O
around	O
this	O
problem	O
is	O
to	O
use	O
a	O
more	O
flexible	O
prior	O
such	O
as	O
a	O
mixture	B
of	I
gaussians	I
such	O
flexible	O
priors	O
can	O
provide	O
robustness	B
against	O
prior	O
mis-specification	O
see	O
e	O
g	O
et	O
al	O
jacob	O
et	O
al	O
for	O
details	O
one	O
can	O
of	O
course	O
combine	O
mixtures	O
with	O
sparsity-promoting	O
priors	O
et	O
al	O
many	O
other	O
variants	O
are	O
possible	O
generalized	O
linear	O
mixed	O
models	O
suppose	O
we	O
generalize	B
the	O
multi-task	B
learning	B
scenario	O
to	O
allow	O
the	O
response	O
to	O
include	O
information	B
at	O
the	O
group	O
level	O
xj	O
as	O
well	O
as	O
at	O
the	O
item	O
level	O
xij	O
similarly	O
we	O
can	O
allow	O
the	O
parameters	O
to	O
vary	O
across	O
groups	O
j	O
or	O
to	O
be	O
tied	B
across	O
groups	O
this	O
gives	O
rise	O
to	O
the	O
following	O
model	O
j	O
e	O
xj	O
g	O
j	O
where	O
the	O
k	O
are	O
basis	B
functions	I
this	O
model	O
can	O
be	O
represented	O
pictorially	O
as	O
shown	O
in	O
figures	O
will	O
be	O
explained	O
in	O
chapter	O
note	O
that	O
the	O
number	O
of	O
j	O
figure	O
parameters	O
grows	O
with	O
the	O
number	O
of	O
groups	O
whereas	O
the	O
size	O
of	O
is	O
fixed	O
frequentists	O
call	O
the	O
terms	O
j	O
random	B
effects	I
since	O
they	O
vary	O
randomly	O
across	O
groups	O
but	O
they	O
call	O
a	O
fixed	O
effect	O
since	O
it	O
is	O
viewed	O
as	O
a	O
fixed	O
but	O
unknown	B
constant	O
a	O
model	O
with	O
both	O
fixed	O
and	O
random	B
effects	I
is	O
called	O
a	O
mixed	B
model	I
if	O
pyx	O
is	O
a	O
glm	B
the	O
overall	O
model	O
is	O
called	O
a	O
generalized	B
linear	I
mixed	I
effects	I
model	I
or	O
glmm	B
such	O
models	O
are	O
widely	O
used	O
in	O
statistics	O
example	O
semi-parametric	O
glmms	O
for	O
medical	O
data	O
consider	O
the	O
following	O
example	O
from	O
suppose	O
yij	O
is	O
the	O
amount	O
of	O
spinal	O
bone	O
mineral	O
density	O
for	O
person	O
j	O
at	O
measurement	O
i	O
let	O
xij	O
be	O
the	O
age	O
of	O
person	O
and	O
let	O
xj	O
be	O
their	O
ethnicity	O
which	O
can	O
be	O
one	O
of	O
white	O
asian	O
black	O
or	O
hispanic	O
the	O
primary	O
goal	O
is	O
to	O
determine	O
if	O
there	O
are	O
significant	O
differences	O
in	O
the	O
mean	B
sbmd	O
among	O
the	O
four	O
ethnic	O
groups	O
after	O
accounting	O
for	O
age	O
the	O
data	O
is	O
shown	O
in	O
the	O
light	O
gray	O
lines	O
in	O
figure	O
we	O
see	O
that	O
there	O
is	O
a	O
nonlinear	O
effect	O
of	O
sbmd	O
vs	O
age	O
so	O
we	O
will	O
use	O
a	O
semi-parametric	B
model	I
which	O
combines	O
linear	B
regression	B
with	O
non-parametric	O
regression	B
et	O
al	O
we	O
also	O
see	O
that	O
there	O
is	O
variation	O
across	O
individuals	O
within	O
each	O
group	O
so	O
we	O
will	O
use	O
a	O
mixed	O
effects	O
model	O
specifically	O
we	O
will	O
use	O
to	O
account	O
for	O
the	O
random	O
effect	O
of	O
each	O
person	O
since	O
no	O
other	O
coefficients	O
are	O
person-specific	O
where	O
bk	O
is	O
the	O
k	O
th	O
spline	B
basis	B
functions	I
section	O
to	O
account	O
for	O
the	O
nonlinear	O
effect	O
of	O
age	O
and	O
w	O
ixj	O
a	O
ixj	O
b	O
ixj	O
h	O
to	O
account	O
for	O
the	O
effect	O
of	O
the	O
different	O
ethnicities	O
furthermore	O
we	O
use	O
a	O
linear	O
link	B
function	I
the	O
overall	O
model	O
is	O
therefore	O
e	O
xj	O
j	O
t	O
bxij	O
ij	O
w	O
ixj	O
w	O
where	O
n	O
y	O
contains	O
the	O
non-parametric	O
part	O
of	O
the	O
model	O
related	O
to	O
age	O
contains	O
the	O
parametric	O
part	O
of	O
the	O
model	O
related	O
to	O
ethnicity	O
and	O
j	O
is	O
a	O
random	O
offset	O
for	O
person	O
j	O
we	O
endow	O
all	O
of	O
these	O
regression	B
coefficients	O
with	O
separate	O
gaussian	B
priors	O
we	O
can	O
then	O
perform	O
posterior	O
inference	B
to	O
compute	O
p	O
section	O
for	O
hixj	O
h	O
aixj	O
a	O
bixj	O
b	O
generalized	O
linear	O
mixed	O
models	O
y	O
j	O
y	O
ij	O
xij	O
nj	O
j	O
xj	O
hispanic	O
white	O
m	O
c	O
g	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
l	O
i	O
a	O
r	O
e	O
n	O
m	O
e	O
n	O
o	O
b	O
l	O
a	O
n	O
p	O
s	O
i	O
asian	O
black	O
age	O
in	O
years	O
figure	O
directed	B
graphical	B
model	I
for	O
generalized	B
linear	I
mixed	I
effects	I
model	I
with	O
j	O
groups	O
spinal	O
bone	O
mineral	O
density	O
vs	O
age	O
for	O
four	O
different	O
ethnic	O
groups	O
raw	O
data	O
is	O
shown	O
in	O
the	O
light	O
gray	O
lines	O
fitted	O
model	O
shown	O
in	O
black	O
is	O
the	O
posterior	O
predicted	O
mean	B
dotted	O
is	O
the	O
posterior	O
predictive	B
variance	B
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
matt	O
wand	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
computational	O
details	O
after	O
fitting	O
the	O
model	O
we	O
can	O
compute	O
the	O
prediction	O
for	O
each	O
group	O
see	O
figure	O
for	O
the	O
results	O
we	O
can	O
also	O
perform	O
significance	O
testing	O
by	O
computing	O
p	O
g	O
wd	O
for	O
each	O
ethnic	O
group	O
g	O
relative	O
to	O
some	O
baseline	O
white	O
as	O
we	O
did	O
in	O
section	O
computational	O
issues	O
the	O
principle	O
problem	O
with	O
glmms	O
is	O
that	O
they	O
can	O
be	O
difficult	O
to	O
fit	O
for	O
two	O
reasons	O
first	O
pyij	O
may	O
not	O
be	O
conjugate	O
to	O
the	O
prior	O
p	O
where	O
second	O
there	O
are	O
two	O
levels	O
of	O
unknowns	O
in	O
the	O
model	O
namely	O
the	O
regression	B
coefficients	O
and	O
the	O
means	O
and	O
variances	O
of	O
the	O
priors	O
one	O
approach	O
is	O
to	O
adopt	O
fully	O
bayesian	B
inference	B
methods	O
such	O
as	O
variational	B
bayes	I
et	O
al	O
or	O
mcmc	B
and	O
hill	O
we	O
discuss	O
vb	B
in	O
section	O
and	O
mcmc	B
in	O
section	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
empirical	B
bayes	I
which	O
we	O
discuss	O
in	O
general	O
terms	O
in	O
section	O
in	O
the	O
context	O
of	O
a	O
glmm	B
we	O
can	O
use	O
the	O
em	B
algorithm	O
where	O
in	O
the	O
e	B
step	I
we	O
compute	O
p	O
and	O
in	O
the	O
m	B
step	I
we	O
optimize	O
if	O
the	O
linear	B
regression	B
setting	O
the	O
e	B
step	I
can	O
be	O
performed	O
exactly	O
but	O
in	O
general	O
we	O
need	O
to	O
use	O
approximations	O
traditional	O
methods	O
use	O
numerical	O
quadrature	O
or	O
monte	B
carlo	I
e	O
g	O
and	O
clayton	O
a	O
faster	O
approach	O
is	O
to	O
use	O
variational	B
em	B
see	O
and	O
mcauliffe	O
for	O
an	O
application	O
of	O
variational	B
em	B
to	O
a	O
multi-level	O
discrete	B
choice	I
modeling	I
problem	O
in	O
frequentist	B
statistics	I
there	O
is	O
a	O
popular	O
method	O
for	O
fitting	O
glmms	O
called	O
generalized	B
estimating	I
equations	I
or	O
gee	B
and	O
hilbe	O
however	O
we	O
do	O
not	O
recommend	O
this	O
approach	O
since	O
it	O
is	O
not	O
as	O
statistically	O
efficient	O
as	O
likelihood-based	O
methods	O
section	O
in	O
addition	O
it	O
can	O
only	O
provide	O
estimates	O
of	O
the	O
population	O
parameters	O
but	O
not	O
the	O
random	B
effects	I
j	O
which	O
are	O
sometimes	O
of	O
interest	O
in	O
themselves	O
learning	B
to	I
rank	I
in	O
this	O
section	O
we	O
discuss	O
the	O
learning	B
to	I
rank	I
or	O
letor	B
problem	O
that	O
is	O
we	O
want	O
to	O
learn	O
a	O
function	O
that	O
can	O
rank	O
order	O
a	O
set	O
of	O
items	O
will	O
be	O
more	O
precise	O
below	O
the	O
most	O
common	O
application	O
is	O
to	O
information	B
retrieval	I
specifically	O
suppose	O
we	O
have	O
a	O
query	O
q	O
and	O
a	O
set	O
of	O
documents	O
dm	O
that	O
might	O
be	O
relevant	O
to	O
q	O
all	O
documents	O
that	O
contain	O
the	O
string	O
q	O
we	O
would	O
like	O
to	O
sort	O
these	O
documents	O
in	O
decreasing	O
order	O
of	O
relevance	O
and	O
show	O
the	O
top	O
k	O
to	O
the	O
user	O
similar	B
problems	O
arise	O
in	O
other	O
areas	O
such	O
as	O
collaborative	O
filtering	B
players	O
in	O
a	O
game	O
or	O
tournament	O
setting	O
is	O
a	O
slightly	O
different	O
kind	O
of	O
problem	O
see	O
section	O
below	O
we	O
summarize	O
some	O
methods	O
for	O
solving	O
this	O
problem	O
following	O
the	O
presentation	O
of	O
this	O
material	O
is	O
not	O
based	O
on	O
glms	O
but	O
we	O
include	O
it	O
in	O
this	O
chapter	O
anyway	O
for	O
lack	O
of	O
a	O
better	O
place	O
a	O
standard	O
way	O
to	O
measure	O
the	O
relevance	O
of	O
a	O
document	O
d	O
to	O
a	O
query	O
q	O
is	O
to	O
use	O
a	O
probabilistic	O
n	O
language	B
model	I
based	O
on	O
a	O
bag	B
of	I
words	I
model	O
that	O
is	O
we	O
define	O
simq	O
d	O
pqd	O
is	O
the	O
i	O
th	O
word	O
or	O
term	O
and	O
pqid	O
is	O
a	O
multinoulli	B
distribution	I
pqid	O
where	O
qi	O
estimated	O
from	O
document	O
d	O
in	O
practice	O
we	O
need	O
to	O
smooth	O
the	O
estimated	O
distribution	O
for	O
example	O
by	O
using	O
a	O
dirichlet	B
prior	O
representing	O
the	O
overall	O
frequency	O
of	O
each	O
word	O
this	O
can	O
be	O
learning	B
to	I
rank	I
estimated	O
from	O
all	O
documents	O
in	O
the	O
system	O
more	O
precisely	O
we	O
can	O
use	O
ptd	O
tft	O
d	O
lend	O
ptbackground	O
where	O
tft	O
d	O
is	O
the	O
frequency	O
of	O
term	O
t	O
in	O
document	O
d	O
lend	O
is	O
the	O
number	O
of	O
words	O
in	O
d	O
and	O
is	O
a	O
smoothing	B
parameter	B
e	O
g	O
zhai	O
and	O
lafferty	O
for	O
details	O
however	O
there	O
might	O
be	O
many	O
other	O
signals	O
that	O
we	O
can	O
use	O
to	O
measure	O
relevance	O
for	O
example	O
the	O
pagerank	B
of	O
a	O
web	O
document	O
is	O
a	O
measure	O
of	O
its	O
authoritativeness	O
derived	O
from	O
the	O
web	O
s	O
link	O
structure	O
section	O
for	O
details	O
we	O
can	O
also	O
compute	O
how	O
often	O
and	O
where	O
the	O
query	O
occurs	O
in	O
the	O
document	O
below	O
we	O
discuss	O
how	O
to	O
learn	O
how	O
to	O
combine	O
all	O
these	O
the	O
pointwise	B
approach	I
suppose	O
we	O
collect	O
some	O
training	O
data	O
representing	O
the	O
relevance	O
of	O
a	O
set	O
of	O
documents	O
for	O
each	O
query	O
specifically	O
for	O
each	O
query	O
q	O
suppose	O
that	O
we	O
retrieve	O
m	O
possibly	O
relevant	O
documents	O
dj	O
for	O
j	O
for	O
each	O
query	O
document	O
pair	O
we	O
define	O
a	O
feature	O
vector	O
xq	O
d	O
for	O
example	O
this	O
might	O
contain	O
the	O
query-document	O
similarity	O
score	O
and	O
the	O
page	O
rank	O
score	O
of	O
the	O
document	O
furthermore	O
suppose	O
we	O
have	O
a	O
set	O
of	O
labels	O
yj	O
representing	O
the	O
degree	B
of	O
relevance	O
of	O
document	O
dj	O
to	O
query	O
q	O
such	O
labels	O
might	O
be	O
binary	O
relevant	O
or	O
irrelevant	O
or	O
they	O
may	O
represent	O
a	O
degree	B
of	O
relevance	O
very	O
relevant	O
somewhat	O
relevant	O
irrelevant	O
such	O
labels	O
can	O
be	O
obtained	O
from	O
query	B
logs	I
by	O
thresholding	O
the	O
number	O
of	O
times	O
a	O
document	O
was	O
clicked	O
on	O
for	O
a	O
given	O
query	O
if	O
we	O
have	O
binary	O
relevance	O
labels	O
we	O
can	O
solve	O
the	O
problem	O
using	O
a	O
standard	O
binary	O
classification	O
scheme	O
to	O
estimate	O
py	O
d	O
if	O
we	O
have	O
ordered	O
relevancy	O
labels	O
we	O
can	O
use	O
ordinal	B
regression	B
to	O
predict	O
the	O
rating	O
py	O
rxq	O
d	O
in	O
either	O
case	O
we	O
can	O
then	O
sort	O
the	O
documents	O
by	O
this	O
scoring	O
metric	B
this	O
is	O
called	O
the	O
pointwise	B
approach	I
to	O
letor	B
and	O
is	O
widely	O
used	O
because	O
of	O
its	O
simplicity	O
however	O
this	O
method	O
does	O
not	O
take	O
into	O
account	O
the	O
location	O
of	O
each	O
document	O
in	O
the	O
list	O
thus	O
it	O
penalizes	O
errors	O
at	O
the	O
end	O
of	O
the	O
list	O
just	O
as	O
much	O
as	O
errors	O
at	O
the	O
beginning	O
which	O
is	O
often	O
not	O
the	O
desired	O
behavior	O
in	O
addition	O
each	O
decision	B
about	O
relevance	O
is	O
made	O
very	O
myopically	O
the	O
pairwise	O
approach	O
there	O
is	O
evidence	B
et	O
al	O
that	O
people	O
are	O
better	O
at	O
judging	O
the	O
relative	O
relevance	O
of	O
two	O
items	O
rather	O
than	O
absolute	O
relevance	O
consequently	O
the	O
data	O
might	O
tell	O
us	O
that	O
dj	O
is	O
more	O
relevant	O
than	O
dk	O
for	O
a	O
given	O
query	O
or	O
vice	O
versa	O
we	O
can	O
model	O
this	O
kind	O
of	O
data	O
using	O
a	O
binary	O
classifier	O
of	O
the	O
form	O
pyjkxq	O
dj	O
xq	O
dk	O
where	O
we	O
set	O
yjk	O
if	O
reldj	O
q	O
reldk	O
q	O
and	O
yjk	O
otherwise	O
one	O
way	O
to	O
model	O
such	O
a	O
function	O
is	O
as	O
follows	O
pyjk	O
xk	O
sigmf	O
f	O
rather	O
surprisingly	O
google	O
does	O
not	O
at	O
least	O
did	O
not	O
as	O
of	O
using	O
such	O
learning	B
methods	O
in	O
its	O
search	O
engine	O
source	O
peter	O
norvig	O
quoted	O
in	O
rone-to-catastrophic-errors-than-machine-learned-models	O
html	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
where	O
f	O
is	O
a	O
scoring	O
function	O
often	O
taken	O
to	O
be	O
linear	O
f	O
t	O
x	O
this	O
is	O
a	O
special	O
kind	O
of	O
neural	B
network	I
known	O
as	O
ranknet	B
et	O
al	O
section	O
for	O
a	O
general	O
discussion	O
of	O
neural	B
networks	I
we	O
can	O
find	O
the	O
mle	B
of	O
w	O
by	O
maximizing	O
the	O
log	O
likelihood	B
or	O
equivalently	O
by	O
minimizing	O
the	O
cross	B
entropy	B
loss	B
given	O
by	O
l	O
lijk	O
lijk	O
iyijk	O
log	O
pyijk	O
xik	O
w	O
log	O
pyijk	O
xik	O
w	O
this	O
can	O
be	O
optimized	O
using	O
gradient	B
descent	I
a	O
variant	O
of	O
ranknet	B
is	O
used	O
by	O
microsoft	B
s	O
bing	B
search	O
the	O
listwise	O
approach	O
the	O
pairwise	O
approach	O
suffers	O
from	O
the	O
problem	O
that	O
decisions	O
about	O
relevance	O
are	O
made	O
just	O
based	O
on	O
a	O
pair	O
of	O
items	O
rather	O
than	O
considering	O
the	O
full	B
context	O
we	O
now	O
consider	O
methods	O
that	O
look	O
at	O
the	O
entire	O
list	O
of	O
items	O
at	O
the	O
same	O
time	O
we	O
can	O
define	O
a	O
total	O
order	O
on	O
a	O
list	O
by	O
specifying	O
a	O
permutation	O
of	O
its	O
indices	O
to	O
model	O
our	O
uncertainty	B
about	O
we	O
can	O
use	O
the	O
plackett-luce	B
distribution	O
which	O
derives	O
its	O
name	O
from	O
independent	O
work	O
by	O
and	O
this	O
has	O
the	O
following	O
form	O
p	O
uj	O
su	O
where	O
sj	O
s	O
is	O
the	O
score	O
of	O
the	O
document	O
ranked	O
at	O
the	O
j	O
th	O
position	O
to	O
understand	O
equation	O
let	O
us	O
consider	O
a	O
simple	O
example	O
suppose	O
b	O
c	O
then	O
we	O
have	O
that	O
p	O
is	O
the	O
probability	O
of	O
a	O
being	O
ranked	O
first	O
times	O
the	O
probability	O
of	O
b	O
being	O
ranked	O
second	O
given	O
that	O
a	O
is	O
ranked	O
first	O
times	O
the	O
probabilty	O
of	O
c	O
being	O
ranked	O
third	O
given	O
that	O
a	O
and	O
b	O
are	O
ranked	O
first	O
and	O
second	O
in	O
other	O
words	O
sb	O
sa	O
sb	O
sc	O
p	O
to	O
incorporate	O
features	B
we	O
can	O
define	O
sd	O
d	O
where	O
we	O
often	O
take	O
f	O
to	O
be	O
a	O
linear	O
function	O
f	O
t	O
x	O
this	O
is	O
known	O
as	O
the	O
listnet	B
model	O
et	O
al	O
to	O
train	O
this	O
model	O
let	O
yi	O
be	O
the	O
relevance	O
scores	B
of	O
the	O
documents	O
for	O
query	O
i	O
we	O
then	O
minimize	O
the	O
cross	B
entropy	B
term	O
sc	O
sc	O
sb	O
sc	O
sa	O
of	O
course	O
as	O
stated	O
this	O
is	O
intractable	O
since	O
the	O
i	O
th	O
term	O
needs	O
to	O
sum	O
over	O
mi	B
permutations	O
to	O
make	O
this	O
tractable	O
we	O
can	O
consider	O
permutations	O
over	O
the	O
top	O
k	O
positions	O
only	O
i	O
p	O
log	O
p	O
p	O
su	O
source	O
eatures-and-the-science-behind-bing	O
aspx	O
learning	B
to	I
rank	I
there	O
are	O
only	O
m	O
m	O
k	O
such	O
permutations	O
entropy	B
term	O
its	O
derivative	O
in	O
om	O
time	O
if	O
we	O
set	O
k	O
we	O
can	O
evaluate	O
each	O
cross	O
in	O
the	O
special	O
case	O
where	O
only	O
one	O
document	O
from	O
the	O
presented	O
list	O
is	O
deemed	O
relevant	O
say	O
yi	O
c	O
we	O
can	O
instead	O
use	O
multinomial	B
logistic	B
regression	B
pyi	O
cx	O
expsc	O
this	O
often	O
performs	O
at	O
least	O
as	O
well	O
as	O
ranking	B
methods	O
at	O
least	O
in	O
the	O
context	O
of	O
collaborative	O
filtering	B
et	O
al	O
loss	B
functions	O
for	O
ranking	B
there	O
are	O
a	O
variety	O
of	O
ways	O
to	O
measure	O
the	O
performance	O
of	O
a	O
ranking	B
system	O
which	O
we	O
summarize	O
below	O
mean	B
reciprocal	I
rank	I
for	O
a	O
query	O
q	O
let	O
the	O
rank	O
position	O
of	O
its	O
first	O
relevant	O
document	O
be	O
denoted	O
by	O
rq	O
then	O
we	O
define	O
the	O
mean	B
reciprocal	I
rank	I
to	O
be	O
this	O
is	O
a	O
very	O
simple	O
performance	O
measure	O
mean	B
average	B
precision	B
in	O
the	O
case	O
of	O
binary	O
relevance	O
labels	O
we	O
can	O
define	O
the	O
precision	B
at	I
k	I
of	O
some	O
ordering	O
as	O
follows	O
pk	O
num	O
relevant	O
documents	O
in	O
the	O
top	O
k	O
positions	O
of	O
k	O
we	O
then	O
define	O
the	O
average	B
precision	B
as	O
follows	O
k	O
pk	O
ik	O
num	O
relevant	O
documents	O
ap	O
where	O
ik	O
is	O
iff	B
document	O
k	O
is	O
relevant	O
for	O
example	O
y	O
then	O
the	O
ap	O
is	O
precision	B
as	O
the	O
ap	O
averaged	O
over	O
all	O
queries	O
if	O
we	O
have	O
the	O
relevancy	O
labels	O
finally	O
we	O
define	O
the	O
mean	B
average	O
normalized	B
discounted	B
cumulative	I
gain	I
suppose	O
the	O
relevance	O
labels	O
have	O
multiple	O
levels	O
we	O
can	O
define	O
the	O
discounted	B
cumulative	I
gain	I
of	O
the	O
first	O
k	O
items	O
in	O
an	O
ordering	O
as	O
follows	O
dcgkr	O
ri	O
i	O
where	O
ri	O
is	O
the	O
relevance	O
of	O
item	O
i	O
and	O
the	O
term	O
is	O
used	O
to	O
discount	O
items	O
later	O
in	O
the	O
list	O
table	O
gives	O
a	O
simple	O
numerical	O
example	O
an	O
alternative	O
definition	O
that	O
places	O
stronger	O
emphasis	O
on	O
retrieving	O
relevant	O
documents	O
uses	O
dcgkr	O
i	O
the	O
trouble	O
with	O
dcg	O
is	O
that	O
it	O
varies	O
in	O
magnitude	O
just	O
because	O
the	O
length	O
of	O
a	O
returned	O
list	O
may	O
vary	O
it	O
is	O
therefore	O
common	O
to	O
normalize	O
this	O
measure	O
by	O
the	O
ideal	O
dcg	O
which	O
is	O
chapter	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	B
i	O
ri	O
i	O
i	O
ri	O
na	O
table	O
illustration	O
of	O
how	O
to	O
compute	O
ndcg	B
from	O
httpen	O
wikipedia	O
orgwikidiscounted	O
the	O
value	O
ri	O
is	O
the	O
relevance	O
score	O
of	O
the	O
item	O
in	O
position	O
i	O
from	O
this	O
we	O
see	O
that	O
the	O
maximum	O
dcg	O
is	O
obtained	O
using	O
the	O
ordering	O
with	O
scores	B
hence	O
the	O
ideal	O
dcg	O
is	O
and	O
so	O
the	O
normalized	O
dcg	O
is	O
the	O
dcg	O
obtained	O
by	O
using	O
the	O
optimal	O
ordering	O
idcgkr	O
argmax	O
dcgkr	O
this	O
can	O
be	O
easily	O
computed	O
by	O
sorting	O
and	O
then	O
computing	O
dcgk	O
finally	O
we	O
define	O
the	O
normalized	B
discounted	B
cumulative	I
gain	I
or	O
ndcg	B
as	O
dcgidcg	O
table	O
gives	O
a	O
simple	O
numerical	O
example	O
the	O
ndcg	B
can	O
be	O
averaged	O
over	O
queries	O
to	O
give	O
a	O
measure	O
of	O
performance	O
rank	B
correlation	I
we	O
can	O
measure	O
the	O
correlation	O
between	O
the	O
ranked	O
list	O
and	O
the	O
relevance	O
judegment	O
using	O
a	O
variety	O
of	O
methods	O
one	O
approach	O
known	O
as	O
the	O
kendall	O
s	O
statistics	O
is	O
defined	O
in	O
terms	O
of	O
the	O
weighted	O
pairwise	O
inconsistency	O
between	O
the	O
two	O
lists	O
uv	O
wuv	O
sgn	O
u	O
vsgn	O
u	O
v	O
uv	O
wuv	O
a	O
variety	O
of	O
other	O
measures	O
are	O
commonly	O
used	O
these	O
loss	B
functions	O
can	O
be	O
used	O
in	O
different	O
ways	O
in	O
the	O
bayesian	B
approach	O
we	O
first	O
fit	O
the	O
model	O
using	O
posterior	O
inference	B
this	O
depends	O
on	O
the	O
likelihood	B
and	O
prior	O
but	O
not	O
the	O
loss	B
we	O
then	O
choose	O
our	O
actions	B
at	O
test	O
time	O
to	O
minimize	O
the	O
expected	O
future	O
loss	B
one	O
way	O
to	O
do	O
this	O
is	O
to	O
sample	O
parameters	O
from	O
the	O
posterior	O
s	O
p	O
and	O
then	O
evaluate	O
say	O
the	O
precisionk	O
for	O
different	O
thresholds	O
averaging	O
over	O
s	O
see	O
et	O
al	O
for	O
an	O
example	O
of	O
such	O
an	O
approach	O
in	O
the	O
frequentist	B
approach	O
we	O
try	O
to	O
minimize	O
the	O
empirical	O
loss	B
on	O
the	O
training	B
set	I
the	O
problem	O
is	O
that	O
these	O
loss	B
functions	O
are	O
not	O
differentiable	O
functions	O
of	O
the	O
model	O
parameters	O
we	O
can	O
either	O
use	O
gradient-free	O
optimization	B
methods	O
or	O
we	O
can	O
minimize	O
a	O
surrogate	B
loss	B
function	I
instead	O
cross	B
entropy	B
loss	B
negative	B
log	I
likelihood	B
is	O
an	O
example	O
of	O
a	O
widely	O
used	O
surrogate	B
loss	B
function	I
another	O
loss	B
known	O
as	O
weighted	B
approximate-rank	I
pairwise	I
or	O
warp	B
loss	B
proposed	O
in	O
et	O
al	O
and	O
extended	O
in	O
et	O
al	O
provides	O
a	O
better	O
approximation	O
to	O
the	O
precisionk	O
loss	B
warp	B
is	O
defined	O
as	O
follows	O
warpf	O
y	O
lrankf	O
y	O
rankf	O
y	O
if	O
f	O
y	O
lk	O
j	O
with	O
learning	B
to	I
rank	I
here	O
f	O
f	O
is	O
the	O
vector	O
of	O
scores	B
for	O
each	O
possible	O
output	O
label	B
or	O
in	O
ir	O
terms	O
for	O
each	O
possible	O
document	O
corresponding	O
to	O
input	O
query	O
x	O
the	O
expression	O
rankf	O
y	O
measures	O
the	O
rank	O
of	O
the	O
true	O
label	B
y	O
assigned	O
by	O
this	O
scoring	O
function	O
finally	O
l	O
transforms	O
the	O
integer	O
rank	O
into	O
a	O
real-valued	O
penalty	O
using	O
and	O
would	O
optimize	O
the	O
proportion	O
of	O
top-ranked	O
correct	O
labels	O
setting	O
to	O
be	O
non-zero	O
values	O
would	O
optimize	O
the	O
top	O
k	O
in	O
the	O
ranked	O
list	O
which	O
will	O
induce	O
good	O
performance	O
as	O
measured	O
by	O
map	O
or	O
precisionk	O
as	O
it	O
stands	O
warp	B
loss	B
is	O
still	O
hard	O
to	O
optimize	O
but	O
it	O
can	O
be	O
further	O
approximated	O
by	O
monte	B
carlo	I
sampling	O
and	O
then	O
optimized	O
by	O
gradient	B
descent	I
as	O
described	O
in	O
et	O
al	O
exercises	O
exercise	O
conjugate	B
prior	I
for	O
univariate	O
gaussian	B
in	O
exponential	B
family	B
form	O
derive	O
the	O
conjugate	B
prior	I
for	O
and	O
for	O
a	O
univariate	O
gaussian	B
using	O
the	O
exponential	B
family	B
by	O
analogy	O
to	O
section	O
by	O
suitable	O
reparameterization	O
show	O
that	O
the	O
prior	O
has	O
the	O
form	O
p	O
n	O
and	O
thus	O
only	O
has	O
free	O
parameters	O
exercise	O
the	O
mvn	B
is	O
in	O
the	O
exponential	B
family	B
show	O
that	O
we	O
can	O
write	O
the	O
mvn	B
in	O
exponential	B
family	B
form	O
hint	O
use	O
the	O
information	B
form	I
defined	O
in	O
section	O
directed	B
graphical	B
models	I
nets	O
introduction	O
i	O
basically	O
know	O
of	O
two	O
principles	O
for	O
treating	O
complicated	O
systems	O
in	O
simple	O
ways	O
the	O
first	O
is	O
the	O
principle	O
of	O
modularity	O
and	O
the	O
second	O
is	O
the	O
principle	O
of	O
abstraction	O
i	O
am	O
an	O
apologist	O
for	O
computational	O
probability	O
in	O
machine	B
learning	B
because	O
i	O
believe	O
that	O
probability	B
theory	I
implements	O
these	O
two	O
principles	O
in	O
deep	B
and	O
intriguing	O
ways	O
namely	O
through	O
factorization	O
and	O
through	O
averaging	O
exploiting	O
these	O
two	O
mechanisms	O
as	O
fully	O
as	O
possible	O
seems	O
to	O
me	O
to	O
be	O
the	O
way	O
forward	O
in	O
machine	B
learning	B
michael	O
jordan	O
in	O
suppose	O
we	O
observe	O
multiple	O
correlated	O
variables	O
such	O
as	O
words	O
in	O
a	O
document	O
pixels	O
in	O
an	O
image	O
or	O
genes	O
in	O
a	O
microarray	O
how	O
can	O
we	O
compactly	O
represent	O
the	O
joint	B
distribution	I
px	O
how	O
can	O
we	O
use	O
this	O
distribution	O
to	O
infer	O
one	O
set	O
of	O
variables	O
given	O
another	O
in	O
a	O
reasonable	O
amount	O
of	O
computation	O
time	O
and	O
how	O
can	O
we	O
learn	O
the	O
parameters	O
of	O
this	O
distribution	O
with	O
a	O
reasonable	O
amount	O
of	O
data	O
these	O
questions	O
are	O
at	O
the	O
core	O
of	O
probabilistic	O
modeling	O
inference	B
and	O
learning	B
and	O
form	O
the	O
topic	B
of	O
this	O
chapter	O
chain	B
rule	I
by	O
the	O
chain	B
rule	I
of	O
probability	O
we	O
can	O
always	O
represent	O
a	O
joint	B
distribution	I
as	O
follows	O
using	O
any	O
ordering	O
of	O
the	O
variables	O
pxv	O
where	O
v	O
is	O
the	O
number	O
of	O
variables	O
the	O
matlab-like	O
notation	O
denotes	O
the	O
set	O
v	O
and	O
where	O
we	O
have	O
dropped	O
the	O
conditioning	B
on	O
the	O
fixed	O
parameters	O
for	O
brevity	O
the	O
problem	O
with	O
this	O
expression	O
is	O
that	O
it	O
becomes	O
more	O
and	O
more	O
complicated	O
to	O
represent	O
the	O
conditional	O
distributions	O
as	O
t	O
gets	O
large	O
for	O
example	O
suppose	O
all	O
the	O
variables	O
have	O
k	O
states	O
we	O
can	O
represent	O
as	O
a	O
table	O
of	O
ok	O
numbers	O
representing	O
a	O
discrete	B
distribution	O
are	O
actually	O
only	O
k	O
free	O
parameters	O
due	O
to	O
the	O
sum-to-one	O
constraint	O
but	O
we	O
write	O
ok	O
for	O
simplicity	O
similarly	O
we	O
can	O
represent	O
as	O
a	O
table	O
of	O
ok	O
numbers	O
by	O
writing	O
i	O
tij	O
we	O
j	O
tij	O
for	O
all	O
rows	O
i	O
say	O
that	O
t	O
is	O
a	O
stochastic	B
matrix	I
since	O
it	O
satisfies	O
the	O
constraint	O
and	O
tij	O
for	O
all	O
entries	O
similarly	O
we	O
can	O
represent	O
as	O
a	O
table	O
with	O
chapter	O
directed	B
graphical	B
models	I
nets	O
ok	O
numbers	O
these	O
are	O
called	O
conditional	B
probability	I
tables	I
or	O
cpts	B
we	O
see	O
that	O
there	O
are	O
ok	O
v	O
parameters	O
in	O
the	O
model	O
we	O
would	O
need	O
an	O
awful	O
lot	O
of	O
data	O
to	O
learn	O
so	O
many	O
parameters	O
one	O
solution	O
is	O
to	O
replace	O
each	O
cpt	O
with	O
a	O
more	O
parsimonius	O
conditional	B
probability	I
distribution	I
or	O
cpd	B
such	O
as	O
multinomial	B
logistic	B
regression	B
i	O
e	O
pxt	O
the	O
total	O
number	O
of	O
parameters	O
is	O
now	O
only	O
ok	O
making	O
this	O
a	O
compact	O
density	O
model	O
frey	O
this	O
is	O
adequate	O
if	O
all	O
we	O
want	O
to	O
do	O
is	O
evaluate	O
the	O
probability	O
of	O
a	O
fully	O
observed	O
vector	O
for	O
example	O
we	O
can	O
use	O
this	O
model	O
to	O
define	O
a	O
class-conditional	B
density	I
pxy	O
c	O
thus	O
making	O
a	O
generative	O
classifier	O
and	O
bengio	O
however	O
this	O
model	O
is	O
not	O
useful	O
for	O
other	O
kinds	O
of	O
prediction	O
tasks	O
since	O
each	O
variable	O
depends	O
on	O
all	O
the	O
previous	O
variables	O
so	O
we	O
need	O
another	O
approach	O
conditional	B
independence	I
the	O
key	O
to	O
efficiently	O
representing	O
large	O
joint	O
distributions	O
is	O
to	O
make	O
some	O
assumptions	O
about	O
conditional	B
independence	I
recall	B
from	O
section	O
that	O
x	O
and	O
y	O
are	O
conditionally	B
independent	I
given	O
z	O
denoted	O
x	O
y	O
if	O
and	O
only	O
if	O
the	O
conditional	O
joint	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
conditional	O
marginals	O
i	O
e	O
x	O
y	O
px	O
y	O
pxzpy	O
let	O
us	O
see	O
why	O
this	O
might	O
help	O
suppose	O
we	O
assume	O
that	O
or	O
in	O
words	O
the	O
future	O
is	O
independent	O
of	O
the	O
past	O
given	O
the	O
present	O
this	O
is	O
called	O
the	O
order	O
markov	B
assumption	I
using	O
this	O
assumption	O
plus	O
the	O
chain	B
rule	I
we	O
can	O
write	O
the	O
joint	B
distribution	I
as	O
follows	O
pxtxt	O
this	O
is	O
called	O
a	O
markov	B
chain	I
they	O
can	O
be	O
characterized	O
by	O
an	O
initial	O
distribution	O
over	O
states	O
i	O
plus	O
a	O
state	B
transition	B
matrix	I
pxt	O
jxt	O
i	O
see	O
section	O
for	O
more	O
information	B
graphical	B
models	I
although	O
the	O
first-order	O
markov	B
assumption	I
is	O
useful	O
for	O
defining	O
distributions	O
on	O
sequences	O
how	O
can	O
we	O
define	O
distributions	O
on	O
images	O
or	O
videos	O
or	O
in	O
general	O
arbitrary	O
collections	O
of	O
variables	O
as	O
genes	O
belonging	O
to	O
some	O
biological	O
pathway	O
this	O
is	O
where	O
graphical	B
models	I
come	O
in	O
a	O
graphical	B
model	I
is	O
a	O
way	O
to	O
represent	O
a	O
joint	B
distribution	I
by	O
making	O
ci	B
assumptions	O
in	O
particular	O
the	O
nodes	B
in	O
the	O
graph	B
represent	O
random	O
variables	O
and	O
the	O
of	O
edges	B
represent	O
ci	B
assumptions	O
better	O
name	O
for	O
these	O
models	O
would	O
in	O
fact	O
be	O
independence	O
diagrams	O
but	O
the	O
term	O
graphical	B
models	I
is	O
now	O
entrenched	O
there	O
are	O
several	O
kinds	O
of	O
graphical	B
model	I
depending	O
on	O
whether	O
the	O
graph	B
is	O
directed	B
undirected	B
or	O
some	O
combination	O
of	O
directed	B
and	O
undirected	B
in	O
this	O
chapter	O
we	O
just	O
study	O
directed	B
graphs	O
we	O
consider	O
undirected	B
graphs	O
in	O
chapter	O
introduction	O
figure	O
are	O
the	O
leaves	B
a	O
simple	O
dag	B
on	O
nodes	B
numbered	O
in	O
topological	O
order	O
node	O
is	O
the	O
root	B
nodes	B
and	O
a	O
simple	O
undirected	B
graph	B
with	O
the	O
following	O
maximal	O
cliques	B
graph	B
terminology	O
before	O
we	O
continue	O
we	O
must	O
define	O
a	O
few	O
basic	O
terms	O
most	O
of	O
which	O
are	O
very	O
intuitive	O
a	O
graph	B
g	O
consists	O
of	O
a	O
set	O
of	O
nodes	B
or	O
vertices	B
v	O
v	O
and	O
a	O
set	O
of	O
edges	B
e	O
t	O
s	O
t	O
v	O
we	O
can	O
represent	O
the	O
graph	B
by	O
its	O
adjacency	B
matrix	I
in	O
which	O
we	O
write	O
gs	O
t	O
to	O
denote	O
t	O
e	O
that	O
is	O
if	O
s	O
t	O
is	O
an	O
edge	O
in	O
the	O
graph	B
if	O
gs	O
t	O
iff	B
gt	O
s	O
we	O
say	O
the	O
graph	B
is	O
undirected	B
otherwise	O
it	O
is	O
directed	B
we	O
usually	O
assume	O
gs	O
s	O
which	O
means	O
there	O
are	O
no	O
self	B
loops	I
here	O
are	O
some	O
other	O
terms	O
we	O
will	O
commonly	O
use	O
parent	O
for	O
a	O
directed	B
graph	B
the	O
parents	B
of	O
a	O
node	O
is	O
the	O
set	O
of	O
all	O
nodes	B
that	O
feed	O
into	O
it	O
pas	O
gt	O
s	O
chs	O
gs	O
t	O
pas	O
child	O
for	O
a	O
directed	B
graph	B
the	O
children	B
of	O
a	O
node	O
is	O
the	O
set	O
of	O
all	O
nodes	B
that	O
feed	O
out	O
of	O
it	O
family	B
for	O
a	O
directed	B
graph	B
the	O
family	B
of	O
a	O
node	O
is	O
the	O
node	O
and	O
its	O
parents	B
fams	O
root	B
for	O
a	O
directed	B
graph	B
a	O
root	B
is	O
a	O
node	O
with	O
no	O
parents	B
leaf	B
for	O
a	O
directed	B
graph	B
a	O
leaf	B
is	O
a	O
node	O
with	O
no	O
children	B
ancestors	B
for	O
a	O
directed	B
graph	B
the	O
ancestors	B
are	O
the	O
parents	B
grand-parents	O
etc	O
of	O
a	O
node	O
that	O
is	O
the	O
ancestors	B
of	O
t	O
is	O
the	O
set	O
of	O
nodes	B
that	O
connect	O
to	O
t	O
via	O
a	O
trail	B
anct	O
s	O
t	O
descendants	B
for	O
a	O
directed	B
graph	B
the	O
descendants	B
are	O
the	O
children	B
grand-children	O
etc	O
of	O
a	O
node	O
that	O
is	O
the	O
descendants	B
of	O
s	O
is	O
the	O
set	O
of	O
nodes	B
that	O
can	O
be	O
reached	O
via	O
trails	O
from	O
s	O
descs	O
s	O
t	O
neighbors	B
for	O
any	O
graph	B
we	O
define	O
the	O
neighbors	B
of	O
a	O
node	O
as	O
the	O
set	O
of	O
all	O
immediately	O
connected	O
nodes	B
nbrs	O
gs	O
t	O
gt	O
s	O
for	O
an	O
undirected	B
graph	B
we	O
chapter	O
directed	B
graphical	B
models	I
nets	O
write	O
s	O
t	O
to	O
indicate	O
that	O
s	O
and	O
t	O
are	O
neighbors	B
t	O
e	O
is	O
an	O
edge	O
in	O
the	O
graph	B
degree	B
the	O
degree	B
of	O
a	O
node	O
is	O
the	O
number	O
of	O
neighbors	B
for	O
directed	B
graphs	O
we	O
speak	O
of	O
the	O
in-degree	B
and	O
out-degree	B
which	O
count	O
the	O
number	O
of	O
parents	B
and	O
children	B
cycle	B
or	O
loop	B
for	O
any	O
graph	B
we	O
define	O
a	O
cycle	B
or	O
loop	B
to	O
be	O
a	O
series	O
of	O
nodes	B
such	O
that	O
we	O
can	O
get	O
back	O
to	O
where	O
we	O
started	O
by	O
following	O
edges	B
sn	O
n	O
if	O
the	O
graph	B
is	O
directed	B
we	O
may	O
speak	O
of	O
a	O
directed	B
cycle	B
for	O
example	O
in	O
figure	O
there	O
are	O
no	O
directed	B
cycles	O
but	O
is	O
an	O
undirected	B
cycle	B
dag	B
a	O
directed	B
acyclic	I
graph	B
or	O
dag	B
is	O
a	O
directed	B
graph	B
with	O
no	O
directed	B
cycles	O
see	O
figure	O
for	O
an	O
example	O
topological	B
ordering	I
for	O
a	O
dag	B
a	O
topological	B
ordering	I
or	O
total	B
ordering	I
is	O
a	O
numbering	O
of	O
the	O
nodes	B
such	O
that	O
parents	B
have	O
lower	O
numbers	O
than	O
their	O
children	B
for	O
example	O
in	O
figure	O
we	O
can	O
use	O
etc	O
path	B
or	O
trail	B
a	O
path	B
or	O
trail	B
s	O
t	O
is	O
a	O
series	O
of	O
directed	B
edges	B
leading	O
from	O
s	O
to	O
t	O
tree	B
an	O
undirected	B
tree	B
is	O
an	O
undirectecd	O
graph	B
with	O
no	O
cycles	O
a	O
directed	B
tree	B
is	O
a	O
dag	B
in	O
which	O
there	O
are	O
no	O
directed	B
cycles	O
if	O
we	O
allow	O
a	O
node	O
to	O
have	O
multiple	O
parents	B
we	O
call	O
it	O
a	O
polytree	B
otherwise	O
we	O
call	O
it	O
a	O
moral	O
directed	B
tree	B
forest	B
a	O
forest	B
is	O
a	O
set	O
of	O
trees	O
subgraph	B
a	O
subgraph	B
ga	O
is	O
the	O
graph	B
created	O
by	O
using	O
the	O
nodes	B
in	O
a	O
and	O
their	O
corresponding	O
edges	B
ga	O
clique	B
for	O
an	O
undirected	B
graph	B
a	O
clique	B
is	O
a	O
set	O
of	O
nodes	B
that	O
are	O
all	O
neighbors	B
of	O
each	O
other	O
a	O
maximal	B
clique	B
is	O
a	O
clique	B
which	O
cannot	O
be	O
made	O
any	O
larger	O
without	O
losing	O
the	O
clique	B
property	O
for	O
example	O
in	O
figure	O
is	O
a	O
clique	B
but	O
it	O
is	O
not	O
maximal	O
since	O
in	O
fact	O
the	O
maximal	O
cliques	B
are	O
as	O
we	O
can	O
add	O
and	O
still	O
maintain	O
the	O
clique	B
property	O
follows	O
directed	B
graphical	B
models	I
a	O
directed	B
graphical	B
model	I
or	O
dgm	B
is	O
a	O
gm	B
whose	O
graph	B
is	O
a	O
dag	B
these	O
are	O
more	O
commonly	O
known	O
as	O
bayesian	B
networks	I
however	O
there	O
is	O
nothing	O
inherently	O
bayesian	B
about	O
bayesian	B
networks	I
they	O
are	O
just	O
a	O
way	O
of	O
defining	O
probability	O
distributions	O
these	O
models	O
are	O
also	O
called	O
belief	B
networks	I
the	O
term	O
belief	O
here	O
refers	O
to	O
subjective	B
probability	I
once	O
again	O
there	O
is	O
nothing	O
inherently	O
subjective	B
about	O
the	O
kinds	O
of	O
probability	O
distributions	O
represented	O
by	O
dgms	O
finally	O
these	O
models	O
are	O
sometimes	O
called	O
causal	B
networks	I
because	O
the	O
directed	B
arrows	O
are	O
sometimes	O
interpreted	O
as	O
representing	O
causal	O
relations	O
however	O
there	O
is	O
nothing	O
inherently	O
causal	O
about	O
dgms	O
section	O
for	O
a	O
discussion	O
of	O
causal	O
dgms	O
for	O
these	O
reasons	O
we	O
use	O
the	O
more	O
neutral	O
less	O
glamorous	O
term	O
dgm	B
the	O
key	O
property	O
of	O
dags	O
is	O
that	O
the	O
nodes	B
can	O
be	O
ordered	O
such	O
that	O
parents	B
come	O
before	O
children	B
this	O
is	O
called	O
a	O
topological	B
ordering	I
and	O
it	O
can	O
be	O
constructed	O
from	O
any	O
dag	B
given	O
such	O
an	O
order	O
we	O
define	O
the	O
ordered	B
markov	B
property	I
to	O
be	O
the	O
assumption	O
that	O
a	O
node	O
only	O
depends	O
on	O
its	O
immediate	O
parents	B
not	O
on	O
all	O
predecessors	O
in	O
the	O
ordering	O
i	O
e	O
xs	O
xpredspasxpas	O
where	O
pas	O
are	O
the	O
parents	B
of	O
node	O
s	O
and	O
preds	O
are	O
the	O
predecessors	O
of	O
node	O
s	O
in	O
the	O
ordering	O
this	O
is	O
a	O
natural	O
generalization	B
of	O
the	O
first-order	O
markov	B
property	O
to	O
from	O
chains	O
to	O
general	O
dags	O
examples	O
y	O
y	O
figure	O
a	O
naive	O
bayes	O
classifier	O
represented	O
as	O
a	O
dgm	B
we	O
assume	O
there	O
are	O
d	O
features	B
for	O
simplicity	O
shaded	O
nodes	B
are	O
observed	O
unshaded	O
nodes	B
are	O
hidden	B
tree-augmented	O
naive	O
bayes	O
classifier	O
for	O
d	O
features	B
in	O
general	O
the	O
tree	B
topology	O
can	O
change	O
depending	O
on	O
the	O
value	O
of	O
y	O
for	O
example	O
the	O
dag	B
in	O
figure	O
encodes	O
the	O
following	O
joint	B
distribution	I
in	O
general	O
we	O
have	O
pxtxpat	O
where	O
each	O
term	O
pxtxpat	O
is	O
a	O
cpd	B
we	O
have	O
written	O
the	O
distribution	O
as	O
pxg	O
to	O
emphasize	O
that	O
this	O
equation	O
only	O
holds	O
if	O
the	O
ci	B
assumptions	O
encoded	O
in	O
dag	B
g	O
are	O
correct	O
however	O
if	O
each	O
node	O
has	O
of	O
parents	B
and	O
we	O
will	O
usual	O
drop	O
this	O
explicit	O
conditioning	B
for	O
brevity	O
k	O
states	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	O
is	O
ov	O
k	O
f	O
which	O
is	O
much	O
less	O
than	O
the	O
ok	O
v	O
needed	O
by	O
a	O
model	O
which	O
makes	O
no	O
ci	B
assumptions	O
examples	O
in	O
this	O
section	O
we	O
show	O
a	O
wide	O
variety	O
of	O
commonly	O
used	O
probabilistic	O
models	O
can	O
be	O
conveniently	O
represented	O
as	O
dgms	O
naive	O
bayes	O
classifiers	O
in	O
section	O
we	O
introduced	O
the	O
naive	O
bayes	O
classifier	O
this	O
assumes	O
the	O
features	B
are	O
conditionally	B
independent	I
given	O
the	O
class	O
label	B
this	O
assumption	O
is	O
illustrated	O
in	O
figure	O
this	O
allows	O
us	O
to	O
write	O
the	O
joint	O
distirbution	O
as	O
follows	O
py	O
x	O
py	O
pxjy	O
the	O
naive	O
bayes	O
assumption	O
is	O
rather	O
naive	O
since	O
it	O
assumes	O
the	O
features	B
are	O
conditionally	B
independent	I
one	O
way	O
to	O
capture	O
correlation	O
between	O
the	O
features	B
is	O
to	O
use	O
a	O
graphical	B
model	I
in	O
particular	O
if	O
the	O
model	O
is	O
a	O
tree	B
the	O
method	O
is	O
known	O
as	O
a	O
tree-augmented	O
naive	O
bayes	O
chapter	O
directed	B
graphical	B
models	I
nets	O
figure	O
a	O
first	O
and	O
second	B
order	I
markov	B
chain	I
zt	O
xt	O
figure	O
a	O
first-order	O
hmm	B
classifier	O
or	O
tan	B
model	O
et	O
al	O
this	O
is	O
illustrated	O
in	O
figure	O
the	O
reason	O
to	O
use	O
a	O
tree	B
as	O
opposed	O
to	O
a	O
generic	O
graph	B
is	O
two-fold	O
first	O
it	O
is	O
easy	O
to	O
find	O
the	O
optimal	O
tree	B
structure	O
using	O
the	O
chow-liu	B
algorithm	I
as	O
explained	O
in	O
section	O
second	O
it	O
is	O
easy	O
to	O
handle	O
missing	B
features	B
in	O
a	O
tree-structured	O
model	O
as	O
we	O
explain	O
in	O
section	O
markov	B
and	O
hidden	B
markov	B
models	I
figure	O
illustrates	O
a	O
first-order	O
markov	B
chain	I
as	O
a	O
dag	B
of	O
course	O
the	O
assumption	O
that	O
the	O
immediate	O
past	O
xt	O
captures	O
everything	O
we	O
need	O
to	O
know	O
about	O
the	O
entire	O
history	O
is	O
a	O
bit	O
strong	O
we	O
can	O
relax	O
it	O
a	O
little	O
by	O
adding	O
a	O
dependence	O
from	O
xt	O
to	O
xt	O
as	O
well	O
this	O
is	O
called	O
a	O
second	B
order	I
markov	B
chain	I
and	O
is	O
illustrated	O
in	O
figure	O
the	O
corresponding	O
joint	O
has	O
the	O
following	O
form	O
pxtxt	O
xt	O
we	O
can	O
create	O
higher-order	O
markov	B
models	I
in	O
a	O
similar	B
way	O
see	O
section	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
markov	B
models	I
unfortunately	O
even	O
the	O
second-order	O
markov	B
assumption	I
may	O
be	O
inadequate	O
if	O
there	O
are	O
longrange	O
correlations	O
amongst	O
the	O
observations	O
we	O
can	O
t	O
keep	O
building	O
ever	O
higher	O
order	O
models	O
since	O
the	O
number	O
of	O
parameters	O
will	O
blow	O
up	O
an	O
alternative	O
approach	O
is	O
to	O
assume	O
that	O
there	O
is	O
an	O
underlying	O
hidden	B
process	O
that	O
can	O
be	O
modeled	O
by	O
a	O
first-order	O
markov	B
chain	I
but	O
that	O
the	O
data	O
is	O
a	O
noisy	O
observation	B
of	O
this	O
process	O
the	O
result	O
is	O
known	O
as	O
a	O
hidden	B
markov	B
model	I
or	O
hmm	B
and	O
is	O
illustrated	O
in	O
figure	O
here	O
zt	O
is	O
known	O
as	O
a	O
hidden	B
variable	I
at	O
time	O
t	O
and	O
xt	O
is	O
the	O
observed	O
variable	O
put	O
time	O
in	O
quotation	O
marks	O
since	O
these	O
models	O
can	O
be	O
applied	O
to	O
any	O
kind	O
of	O
sequence	O
data	O
such	O
as	O
genomics	O
or	O
language	O
where	O
t	O
represents	O
location	O
rather	O
than	O
time	O
the	O
cpd	B
pztzt	O
is	O
the	O
transition	B
model	I
and	O
the	O
cpd	B
pxtzt	O
is	O
the	O
observation	B
model	I
examples	O
p	O
p	O
table	O
noisy-or	B
cpd	B
for	O
parents	B
augmented	O
with	O
leak	B
node	I
we	O
have	O
omitted	O
the	O
t	O
subscript	O
for	O
brevity	O
the	O
hidden	B
variables	I
often	O
represent	O
quantities	O
of	O
interest	O
such	O
as	O
the	O
identity	O
of	O
the	O
word	O
that	O
someone	O
is	O
currently	O
speaking	O
the	O
observed	O
variables	O
are	O
what	O
we	O
measure	O
such	O
as	O
the	O
acoustic	O
waveform	O
what	O
we	O
would	O
like	O
to	O
do	O
is	O
estimate	O
the	O
hidden	B
state	B
given	O
the	O
data	O
i	O
e	O
to	O
compute	O
this	O
is	O
called	O
state	B
estimation	I
and	O
is	O
just	O
another	O
form	O
of	O
probabilistic	B
inference	B
see	O
chapter	O
for	O
further	O
details	O
on	O
hmms	B
medical	O
diagnosis	O
consider	O
modeling	O
the	O
relationship	O
between	O
various	O
variables	O
that	O
are	O
measured	O
in	O
an	O
intensive	B
care	I
unit	I
such	O
as	O
the	O
breathing	O
rate	B
of	O
a	O
patient	O
their	O
blood	O
pressure	O
etc	O
the	O
alarm	B
network	I
in	O
figure	O
is	O
one	O
way	O
to	O
represent	O
these	O
et	O
al	O
this	O
model	O
has	O
variables	O
and	O
parameters	O
since	O
this	O
model	O
was	O
created	O
by	O
hand	O
by	O
a	O
process	O
called	O
knowledge	B
engineering	I
it	O
is	O
known	O
as	O
a	O
probabilistic	B
expert	I
system	I
in	O
section	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
parameters	O
of	O
dgms	O
from	O
data	O
assuming	O
the	O
graph	B
structure	O
is	O
known	O
and	O
in	O
chapter	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
graph	B
structure	O
itself	O
a	O
different	O
kind	O
of	O
medical	O
diagnosis	O
network	O
known	O
as	O
the	O
quick	B
medical	I
reference	I
or	O
qmr	B
network	O
et	O
al	O
is	O
shown	O
in	O
figure	O
this	O
was	O
designed	O
to	O
model	O
infectious	O
diseases	O
the	O
qmr	B
model	O
is	O
a	O
bipartite	B
graph	B
structure	O
with	O
diseases	O
at	O
the	O
top	O
and	O
symptoms	O
or	O
findings	O
at	O
the	O
bottom	O
all	O
nodes	B
are	O
binary	O
we	O
can	O
write	O
the	O
distribution	O
as	O
follows	O
pv	O
h	O
phs	O
pvthpat	O
s	O
t	O
where	O
hs	O
represent	O
the	O
hidden	B
nodes	B
and	O
vt	O
represent	O
the	O
visible	B
nodes	B
the	O
cpd	B
for	O
the	O
root	B
nodes	B
are	O
just	O
bernoulli	B
distributions	O
representing	O
the	O
prior	O
probability	O
of	O
that	O
disease	O
representing	O
the	O
cpds	O
for	O
the	O
leaves	B
using	O
cpts	B
would	O
require	O
too	O
many	O
parameters	O
because	O
the	O
fan-in	B
of	O
parents	B
of	O
many	O
leaf	B
nodes	B
is	O
very	O
high	O
a	O
natural	O
alternative	O
is	O
to	O
use	O
logistic	B
regression	B
to	O
model	O
the	O
cpd	B
pvt	O
sigmwt	O
t	O
hpat	O
dgm	B
in	O
which	O
the	O
cpds	O
are	O
logistic	B
regression	B
distributions	O
is	O
known	O
as	O
a	O
sigmoid	B
belief	I
net	I
however	O
since	O
the	O
parameters	O
of	O
this	O
model	O
were	O
created	O
by	O
hand	O
an	O
alternative	O
cpd	B
known	O
as	O
the	O
noisy-or	B
model	O
was	O
used	O
the	O
noisy-or	B
model	O
assumes	O
that	O
if	O
a	O
parent	O
is	O
on	O
then	O
the	O
child	O
will	O
usually	O
also	O
be	O
on	O
it	O
is	O
an	O
or-gate	O
but	O
occasionally	O
the	O
links	O
from	O
parents	B
to	O
child	O
may	O
fail	O
independently	O
at	O
random	O
in	O
this	O
case	O
even	O
if	O
the	O
parent	O
is	O
on	O
the	O
child	O
may	O
be	O
off	O
to	O
model	O
this	O
more	O
precisely	O
let	O
st	O
qst	O
be	O
the	O
probability	O
that	O
the	O
s	O
t	O
link	O
fails	O
so	O
qst	O
st	O
pvt	O
chapter	O
directed	B
graphical	B
models	I
nets	O
minvolset	O
disconnect	O
ventmach	O
pulm	O
embolus	O
intubation	O
venttube	O
kinked	O
tube	B
pap	O
shunt	O
press	O
ventlung	O
hypo	O
volemia	O
anaphy	O
laxis	O
stroke	O
volume	O
lvfailure	O
tpr	O
co	O
minvol	O
ventalv	O
pvsat	O
insuff	O
anesth	O
catechol	O
history	O
cvp	O
lved	O
volume	O
pcwp	O
bp	B
errlow	O
output	O
hrbp	O
hr	O
errcauter	O
hrsat	O
hrekg	O
figure	O
network	O
the	O
alarm	B
network	I
figure	O
generated	O
by	O
visualizealarmnetwork	O
the	O
qmr	B
examples	O
gp	O
gm	B
px	O
a	O
a	O
a	O
a	O
b	O
b	O
b	O
o	O
o	O
o	O
a	O
b	O
o	O
a	O
b	O
o	O
a	O
b	O
o	O
px	O
b	O
px	O
o	O
px	O
ab	O
table	O
cpt	O
which	O
encodes	O
a	O
mapping	O
from	O
genotype	B
to	O
phenotype	O
this	O
is	O
a	O
deterministic	O
but	O
many-to-one	O
mapping	O
h	O
s	O
is	O
the	O
probability	O
that	O
s	O
can	O
activate	O
t	O
on	O
its	O
own	O
causal	O
power	O
the	O
only	O
way	O
for	O
the	O
child	O
to	O
be	O
off	O
is	O
if	O
all	O
the	O
links	O
from	O
all	O
parents	B
that	O
are	O
on	O
fail	O
independently	O
at	O
random	O
thus	O
pvt	O
st	O
s	O
pat	O
obviously	O
pvt	O
pvt	O
if	O
we	O
observe	O
that	O
vt	O
but	O
all	O
its	O
parents	B
are	O
off	O
then	O
this	O
contradicts	O
the	O
model	O
such	O
a	O
data	O
case	O
would	O
get	O
probability	O
zero	O
under	O
the	O
model	O
which	O
is	O
problematic	O
because	O
it	O
is	O
possible	O
that	O
someone	O
exhibits	O
a	O
symptom	O
but	O
does	O
not	O
have	O
any	O
of	O
the	O
specified	O
diseases	O
to	O
handle	O
this	O
we	O
add	O
a	O
dummy	O
leak	B
node	I
which	O
is	O
always	O
on	O
this	O
represents	O
all	O
other	O
causes	O
the	O
parameter	B
represents	O
the	O
probability	O
that	O
the	O
background	O
leak	O
can	O
cause	O
the	O
effect	O
on	O
its	O
own	O
the	O
modified	O
cpd	B
becomes	O
pvt	O
st	O
see	O
table	O
for	O
a	O
numerical	O
example	O
s	O
pat	O
hs	O
if	O
we	O
define	O
wst	O
log	O
st	O
we	O
can	O
rewrite	O
the	O
cpd	B
as	O
pvt	O
exp	O
hswst	O
s	O
we	O
see	O
that	O
this	O
is	O
similar	B
to	O
a	O
logistic	B
regression	B
model	O
bipartite	O
models	O
with	O
noisy-or	B
cpds	O
are	O
called	O
models	O
it	O
is	O
relatively	O
easy	O
to	O
set	O
the	O
st	O
parameters	O
by	O
hand	O
based	O
on	O
domain	O
expertise	O
however	O
it	O
is	O
also	O
possible	O
to	O
learn	O
them	O
from	O
data	O
e	O
g	O
meek	O
and	O
heckerman	O
noisy-or	B
cpds	O
have	O
also	O
proved	O
useful	O
in	O
modeling	O
human	O
causal	O
learning	B
and	O
tenenbaum	O
as	O
well	O
as	O
general	O
binary	O
classification	O
settings	O
and	O
zheng	O
genetic	B
linkage	I
analysis	I
another	O
important	O
historically	O
very	O
early	O
application	O
of	O
dgms	O
is	O
to	O
the	O
problem	O
of	O
genetic	B
linkage	I
analysis	I
we	O
start	O
with	O
a	O
pedigree	B
graph	B
which	O
is	O
a	O
dag	B
that	O
representing	O
the	O
relationship	O
between	O
parents	B
and	O
children	B
as	O
shown	O
in	O
figure	O
we	O
then	O
convert	O
this	O
to	O
a	O
dgm	B
as	O
we	O
explain	O
below	O
finally	O
we	O
perform	O
probabilistic	B
inference	B
in	O
the	O
resulting	O
model	O
chapter	O
directed	B
graphical	B
models	I
nets	O
family	B
tree	B
circles	O
are	O
females	O
squares	O
are	O
males	O
figure	O
left	O
individuals	O
with	O
the	O
disease	O
of	O
interest	O
are	O
highlighted	O
right	O
dgm	B
for	O
two	O
loci	O
blue	O
nodes	B
xij	O
is	O
the	O
observed	O
phenotype	O
for	O
individual	O
i	O
at	O
locus	O
j	O
all	O
other	O
nodes	B
are	O
hidden	B
orange	O
nodes	B
gpm	O
is	O
the	O
paternal	O
maternal	O
allele	O
small	O
red	O
nodes	B
zpm	O
are	O
the	O
paternal	O
maternal	O
selection	O
switching	O
variables	O
these	O
are	O
linked	O
across	O
loci	O
ij	O
zp	O
ij	O
zm	O
zm	O
the	O
founder	O
nodes	B
do	O
not	O
have	O
any	O
parents	B
and	O
hence	O
do	O
no	O
need	O
switching	O
variables	O
based	O
on	O
figure	O
from	O
et	O
al	O
ij	O
ijl	O
and	O
zp	O
examples	O
in	O
more	O
detail	O
for	O
each	O
person	O
animal	O
i	O
and	O
location	O
or	O
locus	O
j	O
along	O
the	O
genome	B
we	O
the	O
observed	O
marker	B
xij	O
can	O
be	O
a	O
property	O
such	O
as	O
blood	O
type	O
create	O
three	O
nodes	B
or	O
just	O
a	O
fragment	O
of	O
dna	O
that	O
can	O
be	O
measured	O
and	O
two	O
hidden	B
alleles	B
gm	B
ij	O
one	O
inherited	O
from	O
i	O
s	O
mother	O
allele	O
and	O
the	O
other	O
from	O
i	O
s	O
father	O
allele	O
together	O
the	O
ordered	O
pair	O
gij	O
ij	O
constitutes	O
i	O
s	O
hidden	B
genotype	B
at	O
locus	O
j	O
ij	O
xij	O
arcs	O
representing	O
the	O
fact	O
that	O
genotypes	O
obviously	O
we	O
must	O
add	O
gm	B
cause	O
phenotypes	B
manifestations	O
of	O
genotypes	O
the	O
cpd	B
pxijgm	O
ij	O
is	O
called	O
the	O
penetrance	B
model	I
as	O
a	O
very	O
simple	O
example	O
suppose	O
xij	O
b	O
o	O
ab	O
represents	O
ij	O
b	O
o	O
is	O
their	O
genotype	B
we	O
can	O
repreperson	O
i	O
s	O
observed	O
bloodtype	B
and	O
gm	B
sent	O
the	O
penetrance	B
model	I
using	O
the	O
deterministic	O
cpd	B
shown	O
in	O
table	O
for	O
example	O
a	O
dominates	B
o	O
so	O
if	O
a	O
person	O
has	O
genotype	B
ao	O
or	O
oa	O
their	O
phenotype	O
will	O
be	O
a	O
ij	O
gp	O
ij	O
xij	O
and	O
gp	O
ij	O
and	O
gp	O
ij	O
gp	O
ij	O
gp	O
in	O
addition	O
we	O
add	O
arcs	O
from	O
i	O
s	O
mother	O
and	O
father	O
into	O
gij	O
reflecting	O
the	O
mendelian	B
inheritance	I
of	O
genetic	O
material	O
from	O
one	O
s	O
parents	B
more	O
precisely	O
let	O
mi	B
k	O
be	O
i	O
s	O
mother	O
then	O
gm	B
kj	O
that	O
is	O
i	O
s	O
maternal	O
allele	O
is	O
a	O
copy	O
of	O
one	O
of	O
its	O
mother	O
s	O
two	O
alleles	B
let	O
z	O
m	O
ij	O
be	O
a	O
hidden	B
variable	I
than	O
specifies	O
the	O
choice	O
we	O
can	O
model	O
this	O
using	O
the	O
following	O
cpd	B
known	O
as	O
the	O
inheritance	B
model	I
ij	O
could	O
either	O
be	O
equal	O
to	O
gm	B
kj	O
or	O
gp	O
igm	O
igm	O
ij	O
gm	B
kj	O
ij	O
gp	O
kj	O
if	O
z	O
m	O
if	O
z	O
m	O
ij	O
m	O
ij	O
p	O
pgm	O
ijgm	O
kj	O
gp	O
kj	O
z	O
m	O
ij	O
ijgm	O
kj	O
z	O
p	O
kj	O
gp	O
we	O
can	O
define	O
pgp	O
are	O
said	O
to	O
specify	O
the	O
phase	B
of	O
the	O
genotype	B
the	O
values	O
of	O
gp	O
the	O
haplotype	B
of	O
person	O
i	O
at	O
locus	O
ij	O
similarly	O
where	O
k	O
pi	O
is	O
i	O
s	O
father	O
the	O
values	O
of	O
the	O
zij	O
ij	O
constitute	O
ij	O
and	O
z	O
m	O
ij	O
gm	B
ij	O
z	O
p	O
next	O
we	O
need	O
to	O
specify	O
the	O
prior	O
for	O
the	O
root	B
nodes	B
pgm	O
ij	O
this	O
is	O
called	O
the	O
founder	B
model	I
and	O
represents	O
the	O
overall	O
prevalence	B
of	O
difference	O
kinds	O
of	O
alleles	B
in	O
the	O
population	O
we	O
usually	O
assume	O
independence	O
between	O
the	O
loci	O
for	O
these	O
founder	O
alleles	B
ij	O
and	O
pgp	O
finally	O
we	O
need	O
to	O
specify	O
priors	O
for	O
the	O
switch	O
variables	O
that	O
control	O
the	O
inheritance	O
process	O
these	O
variables	O
are	O
spatially	O
correlated	O
since	O
adjacent	O
sites	O
on	O
the	O
genome	B
are	O
typically	O
inherited	O
together	O
events	O
are	O
rare	O
we	O
can	O
model	O
this	O
by	O
imposing	O
a	O
two-state	O
markov	B
chain	I
on	O
the	O
z	O
s	O
where	O
the	O
probability	O
of	O
switching	O
state	B
at	O
locus	O
j	O
is	O
given	O
by	O
j	O
e	O
where	O
dj	O
is	O
the	O
distance	O
between	O
loci	O
j	O
and	O
j	O
this	O
is	O
called	O
the	O
recombination	B
model	I
the	O
resulting	O
dgm	B
is	O
shown	O
in	O
figure	O
it	O
is	O
a	O
series	O
of	O
replicated	O
pedigree	O
dags	O
augmented	O
with	O
switching	O
z	O
variables	O
which	O
are	O
linked	O
using	O
markov	B
chains	O
is	O
a	O
related	O
model	O
known	O
as	O
phylogenetic	B
hmm	B
and	O
haussler	O
which	O
is	O
used	O
to	O
model	O
evolution	O
amongst	O
phylogenies	O
as	O
a	O
simplified	O
example	O
of	O
how	O
this	O
model	O
can	O
be	O
used	O
suppose	O
we	O
only	O
have	O
one	O
locus	O
corresponding	O
to	O
blood	O
type	O
for	O
brevity	O
we	O
will	O
drop	O
the	O
j	O
index	O
suppose	O
we	O
observe	O
xi	O
a	O
then	O
there	O
are	O
possible	O
genotypes	O
gi	O
is	O
a	O
o	O
or	O
a	O
there	O
is	O
ambiguity	O
because	O
the	O
genotype	B
to	O
phenotype	O
mapping	O
is	O
many-to-one	O
we	O
want	O
to	O
reverse	O
this	O
mapping	O
this	O
is	O
known	O
as	O
an	O
inverse	B
problem	I
fortunately	O
we	O
can	O
use	O
the	O
blood	O
types	O
of	O
relatives	O
to	O
help	O
disambiguate	O
the	O
evidence	B
information	B
will	O
flow	O
from	O
the	O
other	O
s	O
up	O
to	O
their	O
s	O
then	O
across	O
to	O
i	O
s	O
gi	O
via	O
the	O
pedigree	O
dag	B
thus	O
we	O
can	O
combine	O
our	O
local	B
evidence	B
pxigi	O
sometimes	O
the	O
observed	O
marker	B
is	O
equal	O
to	O
the	O
unphased	O
genotype	B
which	O
is	O
the	O
unordered	O
set	O
ij	O
however	O
the	O
phased	O
or	O
hidden	B
genotype	B
is	O
not	O
directly	O
measurable	O
ij	O
gm	B
chapter	O
directed	B
graphical	B
models	I
nets	O
with	O
an	O
informative	O
prior	O
pgix	O
i	O
conditioned	O
on	O
the	O
other	O
data	O
to	O
get	O
a	O
less	O
entropic	O
local	O
posterior	O
pgix	O
pxigipgix	O
i	O
in	O
practice	O
the	O
model	O
is	O
used	O
to	O
try	O
to	O
determine	O
where	O
along	O
the	O
genome	B
a	O
given	O
diseasecausing	O
gene	O
is	O
assumed	O
to	O
lie	O
this	O
is	O
the	O
genetic	B
linkage	I
analysis	I
task	O
the	O
method	O
works	O
as	O
follows	O
first	O
suppose	O
all	O
the	O
parameters	O
of	O
the	O
model	O
including	O
the	O
distance	O
between	O
all	O
the	O
marker	B
loci	O
are	O
known	O
the	O
only	O
unknown	B
is	O
the	O
location	O
of	O
the	O
disease-causing	O
gene	O
if	O
there	O
are	O
l	O
marker	B
loci	O
we	O
construct	O
l	O
models	O
in	O
model	O
we	O
postulate	O
that	O
the	O
disease	O
gene	O
comes	O
after	O
marker	B
for	O
l	O
we	O
can	O
estimate	O
the	O
markov	B
switching	O
parameter	B
and	O
hence	O
the	O
distance	O
between	O
the	O
disease	O
gene	O
and	O
its	O
nearest	O
known	O
locus	O
we	O
measure	O
the	O
quality	O
of	O
that	O
model	O
using	O
its	O
likelihood	B
pd	O
we	O
then	O
can	O
then	O
pick	O
the	O
model	O
with	O
highest	O
likelihood	B
is	O
equivalent	O
to	O
the	O
map	O
model	O
under	O
a	O
uniform	O
prior	O
note	O
however	O
that	O
computing	O
the	O
likelihood	B
requires	O
marginalizing	B
out	I
all	O
the	O
hidden	B
z	O
and	O
g	O
variables	O
see	O
and	O
geiger	O
and	O
the	O
references	O
therein	O
for	O
some	O
exact	O
methods	O
for	O
this	O
task	O
these	O
are	O
based	O
on	O
the	O
variable	B
elimination	I
algorithm	O
which	O
we	O
discuss	O
in	O
section	O
unfortunately	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
exact	O
methods	O
can	O
be	O
computationally	O
intractable	O
if	O
the	O
number	O
of	O
individuals	O
andor	O
loci	O
is	O
large	O
see	O
et	O
al	O
for	O
an	O
approximate	O
method	O
for	O
computing	O
the	O
likelihood	B
this	O
is	O
based	O
on	O
a	O
form	O
of	O
variational	B
inference	B
which	O
we	O
will	O
discuss	O
in	O
section	O
directed	B
gaussian	B
graphical	B
models	I
consider	O
a	O
dgm	B
where	O
all	O
the	O
variables	O
are	O
real-valued	O
and	O
all	O
the	O
cpds	O
have	O
the	O
following	O
form	O
pxtxpat	O
n	O
t	O
wt	O
t	O
xpat	O
t	O
this	O
is	O
called	O
a	O
linear	B
gaussian	B
cpd	B
as	O
we	O
show	O
below	O
multiplying	O
all	O
these	O
cpds	O
together	O
results	O
in	O
a	O
large	O
joint	O
gaussian	B
distribution	O
of	O
the	O
form	O
px	O
this	O
is	O
called	O
a	O
directed	B
ggm	O
or	O
a	O
gaussian	B
bayes	I
net	I
we	O
now	O
explain	O
how	O
to	O
derive	O
and	O
from	O
the	O
cpd	B
parameters	O
following	O
and	O
kenley	O
app	O
b	O
for	O
convenience	O
we	O
will	O
rewrite	O
the	O
cpds	O
in	O
the	O
following	O
form	O
s	O
pat	O
xt	O
t	O
wtsxs	O
s	O
tzt	O
where	O
zt	O
n	O
t	O
is	O
the	O
conditional	O
standard	B
deviation	I
of	O
xt	O
given	O
its	O
parents	B
wts	O
is	O
the	O
strength	O
of	O
the	O
s	O
t	O
edge	O
and	O
t	O
is	O
the	O
local	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
global	O
mean	B
is	O
just	O
the	O
concatenation	O
of	O
the	O
local	O
means	O
d	O
we	O
now	O
derive	O
the	O
global	O
covariance	B
let	O
s	O
diag	O
be	O
a	O
diagonal	B
matrix	O
containing	O
the	O
standard	O
deviations	O
we	O
can	O
rewrite	O
equation	O
in	O
matrix-vector	O
form	O
as	O
follows	O
wx	O
if	O
we	O
do	O
not	O
subtract	O
off	O
the	O
parent	O
s	O
mean	B
if	O
we	O
use	O
xt	O
t	O
is	O
much	O
messier	O
as	O
can	O
be	O
seen	O
by	O
looking	O
at	O
s	O
pat	O
wtsxs	O
tzt	O
the	O
derivation	O
of	O
inference	B
now	O
let	O
e	O
be	O
a	O
vector	O
of	O
noise	O
terms	O
e	O
sz	O
we	O
can	O
rearrange	O
this	O
to	O
get	O
e	O
wx	O
since	O
w	O
is	O
lower	O
triangular	O
wts	O
if	O
t	O
s	O
in	O
the	O
topological	B
ordering	I
we	O
have	O
that	O
i	O
w	O
is	O
lower	O
triangular	O
with	O
on	O
the	O
diagonal	B
hence	O
xd	O
d	O
ed	O
wdd	O
since	O
i	O
w	O
is	O
always	O
invertible	O
we	O
can	O
write	O
x	O
w	O
ue	O
usz	O
where	O
we	O
defined	O
u	O
w	O
decomposition	O
of	O
as	O
we	O
now	O
show	O
cov	O
cov	O
cov	O
us	O
cov	O
sut	O
inference	B
thus	O
the	O
regression	B
weights	O
correspond	O
to	O
a	O
cholesky	O
we	O
have	O
seen	O
that	O
graphical	B
models	I
provide	O
a	O
compact	O
way	O
to	O
define	O
joint	O
probability	O
distributions	O
given	O
such	O
a	O
joint	B
distribution	I
what	O
can	O
we	O
do	O
with	O
it	O
the	O
main	O
use	O
for	O
such	O
a	O
joint	B
distribution	I
is	O
to	O
perform	O
probabilistic	B
inference	B
this	O
refers	O
to	O
the	O
task	O
of	O
estimating	O
unknown	B
quantities	O
from	O
known	O
quantities	O
for	O
example	O
in	O
section	O
we	O
introduced	O
hmms	B
and	O
said	O
that	O
one	O
of	O
the	O
goals	O
is	O
to	O
estimate	O
the	O
hidden	B
states	O
words	O
from	O
the	O
observations	O
speech	O
signal	O
and	O
in	O
section	O
we	O
discussed	O
genetic	B
linkage	I
analysis	I
and	O
said	O
that	O
one	O
of	O
the	O
goals	O
is	O
to	O
estimate	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
various	O
dags	O
corresponding	O
to	O
different	O
hypotheses	O
about	O
the	O
location	O
of	O
the	O
disease-causing	O
gene	O
in	O
general	O
we	O
can	O
pose	O
the	O
inference	B
problem	O
as	O
follows	O
suppose	O
we	O
have	O
a	O
set	O
of	O
correlated	O
random	O
variables	O
with	O
joint	B
distribution	I
this	O
section	O
we	O
are	O
assuming	O
the	O
parameters	O
of	O
the	O
model	O
are	O
known	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
parameters	O
in	O
section	O
let	O
us	O
partition	O
this	O
vector	O
into	O
the	O
visible	B
variables	I
xv	O
which	O
are	O
observed	O
and	O
the	O
hidden	B
variables	I
xh	O
which	O
are	O
unobserved	O
inference	B
refers	O
to	O
computing	O
the	O
posterior	O
distribution	O
of	O
the	O
unknowns	O
given	O
the	O
knowns	O
pxh	O
xv	O
pxh	O
xv	O
pxv	O
pxhxv	O
h	O
xv	O
h	O
essentially	O
we	O
are	O
conditioning	B
on	O
the	O
data	O
by	O
clamping	B
the	O
visible	B
variables	I
to	O
their	O
observed	O
values	O
xv	O
and	O
then	O
normalizing	O
to	O
go	O
from	O
pxh	O
xv	O
to	O
pxhxv	O
the	O
normalization	O
constant	O
pxv	O
is	O
the	O
likelihood	B
of	O
the	O
data	O
also	O
called	O
the	O
probability	B
of	I
the	I
evidence	B
chapter	O
directed	B
graphical	B
models	I
nets	O
sometimes	O
only	O
some	O
of	O
the	O
hidden	B
variables	I
are	O
of	O
interest	O
to	O
us	O
so	O
let	O
us	O
partition	O
the	O
hidden	B
variables	I
into	O
query	B
variables	I
xq	O
whose	O
value	O
we	O
wish	O
to	O
know	O
and	O
the	O
remaining	O
nuisance	B
variables	I
xn	O
which	O
we	O
are	O
not	O
interested	O
in	O
we	O
can	O
compute	O
what	O
we	O
are	O
interested	O
in	O
by	O
marginalizing	B
out	I
the	O
nuisance	B
variables	I
pxqxv	O
pxq	O
xnxv	O
xn	O
in	O
section	O
we	O
saw	O
how	O
to	O
perform	O
all	O
these	O
operations	O
for	O
a	O
multivariate	B
gaussian	B
in	O
ov	O
time	O
where	O
v	O
is	O
the	O
number	O
of	O
variables	O
what	O
if	O
we	O
have	O
discrete	B
random	O
variables	O
with	O
say	O
k	O
states	O
each	O
if	O
the	O
joint	B
distribution	I
is	O
represented	O
as	O
a	O
multi-dimensional	O
table	O
we	O
can	O
always	O
perform	O
these	O
operations	O
exactly	O
but	O
this	O
will	O
take	O
ok	O
v	O
time	O
in	O
chapter	O
we	O
explain	O
how	O
to	O
exploit	O
the	O
factorization	O
encoded	O
by	O
the	O
gm	B
to	O
perform	O
these	O
operations	O
in	O
ov	O
k	O
time	O
where	O
w	O
is	O
a	O
quantity	O
known	O
as	O
the	O
treewidth	B
of	O
the	O
graph	B
this	O
measures	O
if	O
the	O
graph	B
is	O
a	O
tree	B
a	O
chain	O
we	O
have	O
w	O
so	O
for	O
these	O
how	O
tree-like	O
the	O
graph	B
is	O
models	O
inference	B
takes	O
time	O
linear	O
in	O
the	O
number	O
of	O
nodes	B
unfortunately	O
for	O
more	O
general	O
graphs	O
exact	O
inference	B
can	O
take	O
time	O
exponential	O
in	O
the	O
number	O
of	O
nodes	B
as	O
we	O
explain	O
in	O
section	O
we	O
will	O
therefore	O
examine	O
various	O
approximate	B
inference	B
schemes	O
later	O
in	O
the	O
book	O
learning	B
in	O
the	O
graphical	B
models	I
literature	O
it	O
is	O
common	O
to	O
distinguish	O
between	O
inference	B
and	O
learning	B
inference	B
means	O
computing	O
of	O
pxhxv	O
where	O
v	O
are	O
the	O
visible	B
nodes	B
h	O
are	O
the	O
hidden	B
nodes	B
and	O
are	O
the	O
parameters	O
of	O
the	O
model	O
assumed	O
to	O
be	O
known	O
learning	B
usually	O
means	O
computing	O
a	O
map	B
estimate	I
of	O
the	O
parameters	O
given	O
data	O
log	O
pxiv	O
log	O
p	O
argmax	O
where	O
xiv	O
are	O
the	O
visible	B
variables	I
in	O
case	O
i	O
if	O
we	O
have	O
a	O
uniform	O
prior	O
p	O
this	O
reduces	O
to	O
the	O
mle	B
as	O
usual	O
if	O
we	O
adopt	O
a	O
bayesian	B
view	O
the	O
parameters	O
are	O
unknown	B
variables	O
and	O
should	O
also	O
be	O
inferred	O
thus	O
to	O
a	O
bayesian	B
there	O
is	O
no	O
distinction	O
between	O
inference	B
and	O
learning	B
in	O
fact	O
we	O
can	O
just	O
add	O
the	O
parameters	O
as	O
nodes	B
to	O
the	O
graph	B
condition	O
on	O
d	O
and	O
then	O
infer	O
the	O
values	O
of	O
all	O
the	O
nodes	B
discuss	O
this	O
in	O
more	O
detail	O
below	O
in	O
this	O
view	O
the	O
main	O
difference	O
between	O
hidden	B
variables	I
and	O
parameters	O
is	O
that	O
the	O
number	O
of	O
hidden	B
variables	I
grows	O
with	O
the	O
amount	O
of	O
training	O
data	O
there	O
is	O
usually	O
a	O
set	O
of	O
hidden	B
variables	I
for	O
each	O
observed	O
data	O
case	O
whereas	O
the	O
number	O
of	O
parameters	O
in	O
usually	O
fixed	O
least	O
in	O
a	O
parametric	B
model	I
this	O
means	O
that	O
we	O
must	O
integrate	B
out	I
the	O
hidden	B
variables	I
to	O
avoid	O
overfitting	O
but	O
we	O
may	O
be	O
able	O
to	O
get	O
away	O
with	O
point	O
estimation	O
techniques	O
for	O
parameters	O
which	O
are	O
fewer	O
in	O
number	O
plate	O
notation	O
when	O
inferring	O
parameters	O
from	O
data	O
we	O
often	O
assume	O
the	O
data	O
is	O
iid	B
we	O
can	O
represent	O
this	O
assumption	O
explicitly	O
using	O
a	O
graphical	B
model	I
as	O
shown	O
in	O
figure	O
this	O
illustrates	O
the	O
learning	B
xn	O
xi	O
n	O
left	O
data	O
points	O
xi	O
are	O
conditionally	B
independent	I
given	O
right	O
plate	O
notation	O
this	O
figure	O
represents	O
the	O
same	O
model	O
as	O
the	O
one	O
on	O
the	O
left	O
except	O
the	O
repeated	O
xi	O
nodes	B
are	O
inside	O
a	O
box	O
known	O
as	O
a	O
plate	O
the	O
number	O
in	O
the	O
lower	O
right	O
hand	O
corner	O
n	O
specifies	O
the	O
number	O
of	O
repetitions	O
of	O
the	O
xi	O
node	O
assumption	O
that	O
each	O
data	O
case	O
was	O
generated	O
independently	O
but	O
from	O
the	O
same	O
distribution	O
notice	O
that	O
the	O
data	O
cases	O
are	O
only	O
independent	O
conditional	O
on	O
the	O
parameters	O
marginally	O
the	O
data	O
cases	O
are	O
dependent	O
nevertheless	O
we	O
can	O
see	O
that	O
in	O
this	O
example	O
the	O
order	O
in	O
which	O
the	O
data	O
cases	O
arrive	O
makes	O
no	O
difference	O
to	O
our	O
beliefs	O
about	O
since	O
all	O
orderings	O
will	O
have	O
the	O
same	O
sufficient	B
statistics	I
hence	O
we	O
say	O
the	O
data	O
is	O
exchangeable	B
to	O
avoid	O
visual	O
clutter	O
it	O
is	O
common	O
to	O
use	O
a	O
form	O
of	O
syntactic	B
sugar	I
called	O
plates	B
we	O
simply	O
draw	O
a	O
little	O
box	O
around	O
the	O
repeated	O
variables	O
with	O
the	O
convention	O
that	O
nodes	B
within	O
the	O
box	O
will	O
get	O
repeated	O
when	O
the	O
model	O
is	O
unrolled	B
we	O
often	O
write	O
the	O
number	O
of	O
copies	O
or	O
repetitions	O
in	O
the	O
bottom	O
right	O
corner	O
of	O
the	O
box	O
see	O
figure	O
for	O
a	O
simple	O
example	O
the	O
corresponding	O
joint	B
distribution	I
has	O
the	O
form	O
p	O
p	O
pxi	O
this	O
dgm	B
represents	O
the	O
ci	B
assumptions	O
behind	O
the	O
models	O
we	O
considered	O
in	O
chapter	O
a	O
slightly	O
more	O
complex	O
example	O
is	O
shown	O
in	O
figure	O
on	O
the	O
left	O
we	O
show	O
a	O
naive	O
bayes	O
classifier	O
that	O
has	O
been	O
unrolled	B
for	O
d	O
features	B
but	O
uses	O
a	O
plate	O
to	O
represent	O
repetition	O
over	O
cases	O
i	O
the	O
version	O
on	O
the	O
right	O
shows	O
the	O
same	O
model	O
using	O
nested	B
plate	I
notation	O
when	O
a	O
variable	O
is	O
inside	O
two	O
plates	B
it	O
will	O
have	O
two	O
sub-indices	O
for	O
example	O
we	O
write	O
jc	O
to	O
represent	O
the	O
parameter	B
for	O
feature	O
j	O
in	O
class-conditional	B
density	I
c	O
note	O
that	O
plates	B
can	O
be	O
nested	O
or	O
crossing	O
notational	O
devices	O
for	O
modeling	O
more	O
complex	O
parameter	B
tying	I
patterns	O
can	O
be	O
devised	O
et	O
al	O
but	O
these	O
are	O
not	O
widely	O
used	O
what	O
is	O
not	O
clear	O
from	O
the	O
figure	O
is	O
that	O
jc	O
is	O
used	O
to	O
generate	O
xij	O
iff	B
yi	O
c	O
otherwise	O
it	O
is	O
ignored	O
this	O
is	O
an	O
example	O
of	O
context	O
specific	O
independence	O
since	O
the	O
ci	B
relationship	O
xij	O
jc	O
only	O
holds	O
if	O
yi	O
c	O
chapter	O
directed	B
graphical	B
models	I
nets	O
yi	O
xid	O
n	O
cd	B
c	O
yi	O
xij	O
n	O
jc	O
c	O
d	O
figure	O
naive	O
bayes	O
classifier	O
as	O
a	O
dgm	B
with	O
single	O
plates	B
with	O
nested	O
plates	B
learning	B
from	O
complete	B
data	I
if	O
all	O
the	O
variables	O
are	O
fully	O
observed	O
in	O
each	O
case	O
so	O
there	O
is	O
no	O
missing	B
data	I
and	O
there	O
are	O
no	O
hidden	B
variables	I
we	O
say	O
the	O
data	O
is	O
complete	B
for	O
a	O
dgm	B
with	O
complete	B
data	I
the	O
likelihood	B
is	O
given	O
by	O
pd	O
pxi	O
pxitxipat	O
t	O
pdt	O
t	O
where	O
dt	O
is	O
the	O
data	O
associated	O
with	O
node	O
t	O
and	O
its	O
parents	B
i	O
e	O
the	O
t	O
th	O
family	B
this	O
is	O
a	O
product	O
of	O
terms	O
one	O
per	O
cpd	B
we	O
say	O
that	O
the	O
likelihood	B
decomposes	B
according	O
to	O
the	O
graph	B
structure	O
now	O
suppose	O
that	O
the	O
prior	O
factorizes	O
as	O
well	O
p	O
p	O
t	O
then	O
clearly	O
the	O
posterior	O
also	O
factorizes	O
p	O
pd	O
pdt	O
tp	O
t	O
this	O
means	O
we	O
can	O
compute	O
the	O
posterior	O
of	O
each	O
cpd	B
independently	O
in	O
other	O
words	O
factored	O
prior	O
plus	O
factored	O
likelihood	B
implies	O
factored	O
posterior	O
let	O
us	O
consider	O
an	O
example	O
where	O
all	O
cpds	O
are	O
tabular	O
thus	O
extending	O
the	O
earlier	O
results	O
of	O
secion	O
where	O
discussed	O
bayesian	B
naive	O
bayes	O
we	O
have	O
a	O
separate	O
row	O
a	O
separate	O
multinoulli	B
distribution	I
for	O
each	O
conditioning	B
case	I
i	O
e	O
for	O
each	O
combination	O
of	O
parent	O
values	O
as	O
in	O
table	O
formally	O
we	O
can	O
write	O
the	O
t	O
th	O
cpt	O
as	O
xtxpat	O
c	O
cat	O
tc	O
where	O
tck	O
pxt	O
kxpat	O
c	O
for	O
k	O
kt	O
c	O
ct	O
and	O
t	O
t	O
here	O
kt	O
is	O
the	O
number	O
learning	B
of	O
states	O
for	O
node	O
t	O
ct	O
number	O
of	O
nodes	B
obviously	O
s	O
pat	O
ks	O
is	O
the	O
number	O
of	O
parent	O
combinations	O
and	O
t	O
is	O
the	O
k	O
tck	O
for	O
each	O
row	O
of	O
each	O
cpt	O
let	O
us	O
put	O
a	O
separate	O
dirichlet	B
prior	O
on	O
each	O
row	O
of	O
each	O
cpt	O
i	O
e	O
tc	O
dir	O
tc	O
then	O
we	O
can	O
compute	O
the	O
posterior	O
by	O
simply	O
adding	O
the	O
pseudo	B
counts	I
to	O
the	O
empirical	O
counts	O
to	O
get	O
tcd	O
dirntc	O
tc	O
wheren	O
tck	O
is	O
the	O
number	O
of	O
times	O
that	O
node	O
t	O
is	O
in	O
state	B
k	O
while	O
its	O
parents	B
are	O
in	O
state	B
c	O
ntck	O
ixit	O
k	O
xipat	O
c	O
from	O
equation	O
the	O
mean	B
of	O
this	O
distribution	O
is	O
given	O
by	O
the	O
following	O
tck	O
ntck	O
tck	O
for	O
example	O
consider	O
the	O
dgm	B
in	O
figure	O
suppose	O
the	O
training	O
data	O
consists	O
of	O
the	O
following	O
cases	O
below	O
we	O
list	O
all	O
the	O
sufficient	B
statistics	I
ntck	O
and	O
the	O
posterior	B
mean	B
parameters	O
ick	O
under	O
a	O
dirichlet	B
prior	O
with	O
ick	O
to	O
add-one	B
smoothing	B
for	O
the	O
t	O
node	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
mle	B
has	O
the	O
same	O
form	O
as	O
equation	O
except	O
without	O
the	O
tck	O
terms	O
i	O
e	O
tck	O
of	O
course	O
the	O
mle	B
suffers	O
from	O
the	O
zero-count	O
problem	O
discussed	O
in	O
section	O
so	O
it	O
is	O
important	O
to	O
use	O
a	O
prior	O
to	O
regularize	O
the	O
estimation	O
problem	O
learning	B
with	O
missing	B
andor	O
latent	B
variables	O
if	O
we	O
have	O
missing	B
data	I
andor	O
hidden	B
variables	I
the	O
likelihood	B
no	O
longer	O
factorizes	O
and	O
indeed	O
it	O
is	O
no	O
longer	O
convex	B
as	O
we	O
explain	O
in	O
detail	O
in	O
section	O
this	O
means	O
we	O
will	O
usually	O
can	O
only	O
compute	O
a	O
locally	O
optimal	O
ml	O
or	O
map	B
estimate	I
bayesian	B
inference	B
of	O
the	O
parameters	O
is	O
even	O
harder	O
we	O
discuss	O
suitable	O
approximate	B
inference	B
techniques	O
in	O
later	O
chapters	O
chapter	O
directed	B
graphical	B
models	I
nets	O
conditional	B
independence	I
properties	O
of	O
dgms	O
we	O
say	O
that	O
g	O
is	O
an	O
i-map	B
map	O
for	O
p	O
or	O
that	O
p	O
is	O
markov	B
wrt	O
g	O
at	O
the	O
heart	O
of	O
any	O
graphical	B
model	I
is	O
a	O
set	O
of	O
conditional	O
indepence	O
assumptions	O
we	O
write	O
xa	O
g	O
xbxc	O
if	O
a	O
is	O
independent	O
of	O
b	O
given	O
c	O
in	O
the	O
graph	B
g	O
using	O
the	O
semantics	O
to	O
be	O
defined	O
below	O
let	O
ig	O
be	O
the	O
set	O
of	O
all	O
such	O
ci	B
statements	O
encoded	O
by	O
the	O
graph	B
iff	B
ig	O
ip	B
where	O
ip	B
is	O
the	O
set	O
of	O
all	O
ci	B
statements	O
that	O
hold	O
for	O
distribution	O
p	O
in	O
other	O
words	O
the	O
graph	B
is	O
an	O
i-map	B
if	O
it	O
does	O
not	O
make	O
any	O
assertions	O
of	O
ci	B
that	O
are	O
not	O
true	O
of	O
the	O
distribution	O
this	O
allows	O
us	O
to	O
use	O
the	O
graph	B
as	O
a	O
safe	O
proxy	O
for	O
p	O
when	O
reasoning	O
about	O
p	O
s	O
ci	B
properties	O
this	O
is	O
helpful	O
for	O
designing	O
algorithms	O
that	O
work	O
for	O
large	O
classes	O
of	O
distributions	O
regardless	O
of	O
their	O
specific	O
numerical	O
parameters	O
note	O
that	O
the	O
fully	O
connected	O
graph	B
is	O
an	O
i-map	B
of	O
all	O
distributions	O
since	O
it	O
makes	O
no	O
ci	B
assertions	O
at	O
all	O
it	O
is	O
not	O
missing	B
any	O
edges	B
we	O
therefore	O
say	O
g	O
is	O
a	O
minimal	B
i-map	B
of	O
p	O
if	O
g	O
is	O
an	O
i-map	B
of	O
p	O
and	O
if	O
there	O
is	O
no	O
g	O
which	O
is	O
an	O
i-map	B
of	O
p	O
it	O
remains	O
to	O
specify	O
how	O
to	O
determine	O
if	O
xa	O
g	O
xbxc	O
deriving	O
these	O
independencies	O
for	O
undirected	B
graphs	O
is	O
easy	O
section	O
but	O
the	O
dag	B
situation	O
is	O
somewhat	O
complicated	O
because	O
of	O
the	O
need	O
to	O
respect	O
the	O
orientation	O
of	O
the	O
directed	B
edges	B
we	O
give	O
the	O
details	O
below	O
d-separation	O
and	O
the	O
bayes	B
ball	I
algorithm	I
markov	B
properties	O
first	O
we	O
introduce	O
some	O
definitions	O
we	O
say	O
an	O
undirected	B
path	B
p	O
is	O
d-separated	B
by	O
a	O
set	O
of	O
nodes	B
e	O
the	O
evidence	B
iff	B
at	O
least	O
one	O
of	O
the	O
following	O
conditions	O
hold	O
p	O
contains	O
a	O
chain	O
s	O
m	O
t	O
or	O
s	O
m	O
t	O
wherem	O
e	O
p	O
contains	O
a	O
tent	O
or	O
fork	O
s	O
t	O
wherem	O
e	O
p	O
contains	O
a	O
collider	B
or	O
v-structure	B
s	O
t	O
where	O
m	O
is	O
not	O
in	O
e	O
and	O
nor	O
is	O
any	O
descendant	O
of	O
m	O
next	O
we	O
say	O
that	O
a	O
set	O
of	O
nodes	B
a	O
is	O
d-separated	B
from	O
a	O
different	O
set	O
of	O
nodes	B
b	O
given	O
a	O
third	O
observed	O
set	O
e	O
iff	B
each	O
undirected	B
path	B
from	O
every	O
node	O
a	O
a	O
to	O
every	O
node	O
b	O
b	O
is	O
d-separated	B
by	O
e	O
finally	O
we	O
define	O
the	O
ci	B
properties	O
of	O
a	O
dag	B
as	O
follows	O
xa	O
g	O
xbxe	O
a	O
is	O
d-separated	B
from	O
b	O
given	O
e	O
the	O
bayes	B
ball	I
algorithm	I
is	O
a	O
simple	O
way	O
to	O
see	O
if	O
a	O
is	O
d-separated	B
from	O
b	O
given	O
e	O
based	O
on	O
the	O
above	O
definition	O
the	O
idea	O
is	O
this	O
we	O
shade	O
all	O
nodes	B
in	O
e	O
indicating	O
that	O
they	O
are	O
observed	O
we	O
then	O
place	O
balls	O
at	O
each	O
node	O
in	O
a	O
let	O
them	O
bounce	O
around	O
according	O
to	O
some	O
rules	B
and	O
then	O
ask	O
if	O
any	O
of	O
the	O
balls	O
reach	O
any	O
of	O
the	O
nodes	B
in	O
b	O
the	O
three	O
main	O
rules	B
are	O
shown	O
in	O
figure	O
notice	O
that	O
balls	O
can	O
travel	O
opposite	O
to	O
edge	O
directions	O
we	O
see	O
that	O
a	O
ball	O
can	O
pass	O
through	O
a	O
chain	O
but	O
not	O
if	O
it	O
is	O
shaded	O
in	O
the	O
middle	O
similarly	O
a	O
ball	O
can	O
pass	O
through	O
a	O
fork	O
but	O
not	O
if	O
it	O
is	O
shaded	O
in	O
the	O
middle	O
however	O
a	O
ball	O
cannot	O
pass	O
through	O
a	O
v-structure	B
unless	O
it	O
is	O
shaded	O
in	O
the	O
middle	O
we	O
can	O
justify	O
the	O
rules	B
of	O
bayes	O
ball	O
as	O
follows	O
first	O
consider	O
a	O
chain	O
structure	O
x	O
y	O
z	O
which	O
encodes	O
px	O
y	O
z	O
pxpyxpzy	O
conditional	B
independence	I
properties	O
of	O
dgms	O
y	O
x	O
z	O
z	O
x	O
y	O
z	O
x	O
z	O
y	O
x	O
y	O
x	O
y	O
x	O
z	O
z	O
y	O
figure	O
bayes	O
ball	O
rules	B
a	O
shaded	O
node	O
is	O
one	O
we	O
condition	O
on	O
if	O
there	O
is	O
an	O
arrow	O
hitting	O
a	O
bar	O
it	O
means	O
the	O
ball	O
cannot	O
pass	O
through	O
otherwise	O
the	O
ball	O
can	O
pass	O
through	O
based	O
on	O
when	O
we	O
condition	O
on	O
y	O
arex	O
and	O
z	O
independent	O
we	O
have	O
px	O
zy	O
pxpyxpzy	O
px	O
ypzy	O
and	O
therefore	O
x	O
zy	O
so	O
observing	O
the	O
middle	O
node	O
of	O
chain	O
breaks	O
it	O
in	O
two	O
in	O
a	O
markov	B
chain	I
py	O
py	O
pxypzy	O
now	O
consider	O
the	O
tent	O
structure	O
x	O
y	O
z	O
the	O
joint	O
is	O
px	O
y	O
z	O
pypxypzy	O
chapter	O
directed	B
graphical	B
models	I
nets	O
x	O
y	O
x	O
y	O
x	O
z	O
y	O
figure	O
bayes	O
ball	O
boundary	O
conditions	O
example	O
of	O
why	O
we	O
need	O
boundary	O
conditions	O
is	O
an	O
observed	O
child	O
of	O
y	O
rendering	O
y	O
effectively	O
observed	O
so	O
the	O
ball	O
bounces	O
back	O
up	O
on	O
its	O
way	O
from	O
x	O
to	O
z	O
when	O
we	O
condition	O
on	O
y	O
arex	O
and	O
z	O
independent	O
we	O
have	O
px	O
zy	O
px	O
y	O
z	O
pypxypzy	O
pxypzy	O
and	O
therefore	O
x	O
zy	O
so	O
observing	O
a	O
root	B
node	O
separates	O
its	O
children	B
in	O
a	O
naive	O
bayes	O
classifier	O
see	O
section	O
py	O
py	O
finally	O
consider	O
a	O
v-structure	B
x	O
y	O
z	O
the	O
joint	O
is	O
px	O
y	O
z	O
pxpzpyx	O
z	O
when	O
we	O
condition	O
on	O
y	O
arex	O
and	O
z	O
independent	O
we	O
have	O
px	O
zy	O
pxpzpyx	O
z	O
py	O
px	O
z	O
pxpz	O
so	O
x	O
zy	O
however	O
in	O
the	O
unconditional	O
distribution	O
we	O
have	O
so	O
we	O
see	O
that	O
x	O
and	O
z	O
are	O
marginally	B
independent	I
so	O
we	O
see	O
that	O
conditioning	B
on	O
a	O
common	O
child	O
at	O
the	O
bottom	O
of	O
a	O
v-structure	B
makes	O
its	O
parents	B
become	O
dependent	O
this	O
important	O
effect	O
is	O
called	O
explaining	B
away	I
inter-causal	B
reasoning	I
or	O
berkson	O
s	O
paradox	O
as	O
an	O
example	O
of	O
explaining	B
away	I
suppose	O
we	O
toss	O
two	O
coins	O
representing	O
the	O
binary	O
numbers	O
and	O
and	O
we	O
observe	O
the	O
sum	O
of	O
their	O
values	O
a	O
priori	O
the	O
coins	O
are	O
independent	O
but	O
once	O
we	O
observe	O
their	O
sum	O
they	O
become	O
coupled	O
if	O
the	O
sum	O
is	O
and	O
the	O
first	O
coin	O
is	O
then	O
we	O
know	O
the	O
second	O
coin	O
is	O
finally	O
bayes	O
ball	O
also	O
needs	O
the	O
boundary	O
conditions	O
shown	O
in	O
figure	O
to	O
understand	O
where	O
these	O
rules	B
come	O
from	O
consider	O
figure	O
suppose	O
y	O
is	O
a	O
noise-free	O
copy	O
of	O
y	O
then	O
if	O
we	O
observe	O
y	O
we	O
effectively	O
observe	O
y	O
as	O
well	O
so	O
the	O
parents	B
x	O
and	O
z	O
have	O
to	O
compete	O
to	O
explain	O
this	O
so	O
if	O
we	O
send	O
a	O
ball	O
down	O
x	O
y	O
y	O
it	O
should	O
bounce	O
back	O
up	O
along	O
y	O
y	O
z	O
however	O
if	O
y	O
and	O
all	O
its	O
children	B
are	O
hidden	B
the	O
ball	O
does	O
not	O
bounce	O
back	O
conditional	B
independence	I
properties	O
of	O
dgms	O
figure	O
a	O
dgm	B
for	O
example	O
in	O
figure	O
we	O
see	O
that	O
since	O
the	O
path	B
is	O
blocked	O
by	O
is	O
observed	O
the	O
path	B
is	O
blocked	O
by	O
is	O
hidden	B
and	O
the	O
path	B
is	O
blocked	O
by	O
is	O
hidden	B
however	O
we	O
also	O
see	O
that	O
since	O
now	O
the	O
path	B
is	O
no	O
longer	O
blocked	O
by	O
is	O
observed	O
exercise	O
gives	O
you	O
some	O
more	O
practice	O
in	O
determining	O
ci	B
relationships	O
for	O
dgms	O
other	O
markov	B
properties	O
of	O
dgms	O
from	O
the	O
d-separation	O
criterion	O
one	O
can	O
conclude	O
that	O
t	O
ndt	O
patpat	O
where	O
the	O
non-descendants	B
of	O
a	O
node	O
ndt	O
are	O
all	O
the	O
nodes	B
except	O
for	O
its	O
descendants	B
ndt	O
t	O
desct	O
equation	O
is	O
called	O
the	O
directed	B
local	I
markov	B
property	I
for	O
example	O
in	O
figure	O
we	O
have	O
and	O
a	O
special	O
case	O
of	O
this	O
property	O
is	O
when	O
we	O
only	O
look	O
at	O
predecessors	O
of	O
a	O
node	O
according	O
to	O
some	O
topological	B
ordering	I
we	O
have	O
t	O
predt	O
patpat	O
which	O
follows	O
since	O
predt	O
ndt	O
this	O
is	O
called	O
the	O
ordered	B
markov	B
property	I
which	O
justifies	O
equation	O
for	O
example	O
in	O
figure	O
if	O
we	O
use	O
the	O
ordering	O
we	O
find	O
and	O
we	O
have	O
now	O
described	O
three	O
markov	B
properties	O
for	O
dags	O
the	O
directed	B
global	B
markov	B
property	I
g	O
in	O
equation	O
the	O
ordered	B
markov	B
property	I
o	O
in	O
equation	O
and	O
the	O
directed	B
local	O
it	O
is	O
obvious	O
that	O
g	O
l	O
o	O
what	O
is	O
less	O
markov	B
property	O
l	O
in	O
equation	O
obvious	O
but	O
nevertheless	O
true	O
is	O
that	O
o	O
l	O
g	O
e	O
g	O
and	O
friedman	O
for	O
the	O
proof	O
hence	O
all	O
these	O
properties	O
are	O
equivalent	O
furthermore	O
any	O
distribution	O
p	O
that	O
is	O
markov	B
wrt	O
g	O
can	O
be	O
factorized	O
as	O
in	O
equation	O
this	O
is	O
called	O
the	O
factorization	O
property	O
f	O
it	O
is	O
obvious	O
that	O
o	O
f	O
but	O
one	O
can	O
show	O
that	O
the	O
converse	O
also	O
holds	O
e	O
g	O
and	O
friedman	O
for	O
the	O
proof	O
markov	B
blanket	I
and	O
full	B
conditionals	O
the	O
set	O
of	O
nodes	B
that	O
renders	O
a	O
node	O
t	O
conditionally	B
independent	I
of	O
all	O
the	O
other	O
nodes	B
in	O
the	O
graph	B
is	O
called	O
t	O
s	O
markov	B
blanket	I
we	O
will	O
denote	O
this	O
by	O
mbt	O
one	O
can	O
show	O
that	O
the	O
markov	B
blanket	I
of	O
a	O
node	O
in	O
a	O
dgm	B
is	O
equal	O
to	O
the	O
parents	B
the	O
children	B
and	O
the	O
co-parents	B
chapter	O
directed	B
graphical	B
models	I
nets	O
i	O
e	O
other	O
nodes	B
who	O
are	O
also	O
parents	B
of	O
its	O
children	B
mbt	O
cht	O
pat	O
copat	O
for	O
example	O
in	O
figure	O
we	O
have	O
where	O
is	O
a	O
co-parent	O
of	O
because	O
they	O
share	O
a	O
common	O
child	O
namely	O
to	O
see	O
why	O
the	O
co-parents	B
are	O
in	O
the	O
markov	B
blanket	I
note	O
that	O
when	O
we	O
derive	O
pxtx	O
t	O
pxt	O
x	O
tpx	O
t	O
all	O
the	O
terms	O
that	O
do	O
not	O
involve	O
xt	O
will	O
cancel	O
out	O
between	O
numerator	O
and	O
denominator	O
so	O
we	O
are	O
left	O
with	O
a	O
product	O
of	O
cpds	O
which	O
contain	O
xt	O
in	O
their	O
scope	B
hence	O
pxtx	O
t	O
pxtxpat	O
pxsxpas	O
s	O
cht	O
for	O
example	O
in	O
figure	O
we	O
have	O
the	O
resulting	O
expression	O
is	O
called	O
t	O
s	O
full	B
conditional	I
and	O
will	O
prove	O
to	O
be	O
important	O
when	O
we	O
study	O
gibbs	B
sampling	I
influence	O
diagrams	O
we	O
can	O
represent	O
multi-stage	B
decision	B
problems	O
by	O
using	O
a	O
graphical	O
notation	O
known	O
as	O
a	O
decision	B
diagram	I
or	O
an	O
influence	O
diagram	O
and	O
matheson	O
kjaerulff	O
and	O
madsen	O
this	O
extends	O
directed	B
graphical	B
models	I
by	O
adding	O
decision	B
nodes	B
called	O
action	B
nodes	B
represented	O
by	O
rectangles	O
and	O
utility	B
nodes	B
called	O
value	B
nodes	B
represented	O
by	O
diamonds	O
the	O
original	O
random	O
variables	O
are	O
called	O
chance	B
nodes	B
and	O
are	O
represented	O
by	O
ovals	O
as	O
usual	O
figure	O
gives	O
a	O
simple	O
example	O
illustrating	O
the	O
famous	O
oil	B
wild-catter	I
in	O
this	O
problem	O
you	O
have	O
to	O
decide	O
whether	O
to	O
drill	O
an	O
oil	O
well	O
or	O
not	O
you	O
have	O
two	O
possible	O
actions	B
d	O
means	O
drill	O
d	O
means	O
don	O
t	O
drill	O
you	O
assume	O
there	O
are	O
states	O
of	O
nature	O
o	O
means	O
the	O
well	O
is	O
dry	O
o	O
means	O
it	O
is	O
wet	O
some	O
oil	O
and	O
o	O
means	O
it	O
is	O
soaking	O
a	O
lot	O
of	O
oil	O
suppose	O
your	O
prior	O
beliefs	O
are	O
po	O
finally	O
you	O
must	O
specify	O
the	O
utility	B
function	I
u	O
o	O
since	O
the	O
states	O
and	O
actions	B
are	O
discrete	B
we	O
can	O
represent	O
it	O
as	O
a	O
table	O
to	O
a	O
cpt	O
in	O
a	O
dgm	B
suppose	O
we	O
use	O
the	O
following	O
numbers	O
in	O
dollars	O
o	O
o	O
o	O
d	O
d	O
we	O
see	O
that	O
if	O
you	O
don	O
t	O
drill	O
you	O
incur	O
no	O
costs	O
but	O
also	O
make	O
no	O
money	O
if	O
you	O
drill	O
a	O
dry	O
well	O
you	O
lose	O
if	O
you	O
drill	O
a	O
wet	O
well	O
you	O
gain	O
and	O
if	O
you	O
drill	O
a	O
soaking	O
well	O
you	O
gain	O
your	O
prior	O
expected	O
utility	O
if	O
you	O
drill	O
is	O
given	O
by	O
eu	O
pou	O
o	O
this	O
example	O
is	O
originally	O
from	O
our	O
presentation	O
is	O
based	O
on	O
some	O
notes	O
by	O
daphne	O
koller	O
influence	O
diagrams	O
oil	O
oil	O
sound	O
drill	O
drill	O
utility	O
test	O
utility	O
oil	O
sound	O
cost	O
drill	O
utility	O
figure	O
influence	O
diagram	O
for	O
basic	O
oil	O
wild	O
catter	O
problem	O
an	O
extension	B
in	O
which	O
we	O
have	O
an	O
information	B
arc	I
from	O
the	O
sound	O
chance	O
node	O
to	O
the	O
drill	O
decision	B
node	O
an	O
extension	B
in	O
which	O
we	O
get	O
to	O
decide	O
whether	O
to	O
perform	O
the	O
test	O
or	O
not	O
your	O
expected	O
utility	O
if	O
you	O
don	O
t	O
drill	O
is	O
so	O
your	O
maximum	O
expected	O
utility	O
is	O
m	O
eu	O
maxeu	O
eu	O
and	O
therefore	O
the	O
optimal	B
action	B
is	O
to	O
drill	O
d	O
arg	O
maxeu	O
eu	O
now	O
let	O
us	O
consider	O
a	O
slight	O
extension	B
to	O
the	O
model	O
suppose	O
you	O
perform	O
a	O
sounding	O
to	O
estimate	O
the	O
state	B
of	O
the	O
well	O
the	O
sounding	O
observation	B
can	O
be	O
in	O
one	O
of	O
states	O
s	O
is	O
a	O
diffuse	O
reflection	O
pattern	B
suggesting	O
no	O
oil	O
s	O
is	O
an	O
open	O
reflection	O
pattern	B
suggesting	O
some	O
oil	O
and	O
s	O
is	O
a	O
closed	O
reflection	O
pattern	B
indicating	O
lots	O
of	O
oil	O
since	O
s	O
is	O
caused	O
by	O
o	O
we	O
add	O
an	O
o	O
s	O
arc	O
to	O
our	O
model	O
in	O
addition	O
we	O
assume	O
that	O
the	O
outcome	O
of	O
the	O
sounding	O
test	O
will	O
be	O
available	O
before	O
we	O
decide	O
whether	O
to	O
drill	O
or	O
not	O
hence	O
we	O
add	O
an	O
information	B
arc	I
from	O
s	O
to	O
d	O
this	O
is	O
illustrated	O
in	O
figure	O
pso	O
let	O
us	O
model	O
the	O
reliability	O
of	O
our	O
sensor	O
using	O
the	O
following	O
conditional	O
distribution	O
for	O
chapter	O
directed	B
graphical	B
models	I
nets	O
s	O
s	O
s	O
o	O
o	O
o	O
suppose	O
we	O
do	O
the	O
sounding	O
test	O
and	O
we	O
observe	O
s	O
the	O
posterior	O
over	O
the	O
oil	O
state	B
is	O
pos	O
now	O
your	O
posterior	O
expected	O
utility	O
of	O
performing	O
action	B
d	O
is	O
eu	O
pos	O
d	O
if	O
d	O
this	O
gives	O
s	O
eu	O
however	O
if	O
d	O
then	O
eu	O
since	O
not	O
drilling	O
incurs	O
no	O
cost	O
so	O
if	O
we	O
observe	O
s	O
we	O
are	O
better	O
off	O
not	O
drilling	O
which	O
makes	O
sense	O
now	O
suppose	O
we	O
do	O
the	O
sounding	O
test	O
and	O
we	O
observe	O
s	O
by	O
similar	B
reasoning	O
one	O
can	O
show	O
that	O
eu	O
which	O
is	O
higher	O
than	O
eu	O
similarly	O
if	O
we	O
observe	O
s	O
we	O
have	O
eu	O
which	O
is	O
much	O
higher	O
than	O
eu	O
hence	O
the	O
optimal	O
policy	B
d	O
if	O
s	O
choose	O
d	O
and	O
get	O
and	O
get	O
and	O
if	O
s	O
choose	O
d	O
and	O
get	O
if	O
s	O
choose	O
d	O
is	O
as	O
follows	O
you	O
can	O
compute	O
your	O
expected	O
profit	O
or	O
maximum	O
expected	O
utility	O
as	O
follows	O
m	O
eu	O
pseu	O
this	O
is	O
the	O
expected	O
utility	O
given	O
possible	O
outcomes	O
of	O
the	O
sounding	O
test	O
assuming	O
you	O
act	O
optimally	O
given	O
the	O
outcome	O
the	O
prior	O
marginal	O
on	O
the	O
outcome	O
of	O
the	O
test	O
is	O
ps	O
popso	O
o	O
hence	O
your	O
maximum	O
expected	O
utility	O
is	O
m	O
eu	O
now	O
suppose	O
you	O
can	O
choose	O
whether	O
to	O
do	O
the	O
test	O
or	O
not	O
this	O
can	O
be	O
modelled	O
as	O
shown	O
in	O
figure	O
where	O
we	O
add	O
a	O
new	O
test	O
node	O
t	O
if	O
t	O
we	O
do	O
the	O
test	O
and	O
s	O
can	O
enter	O
of	O
states	O
determined	O
by	O
o	O
exactly	O
as	O
above	O
if	O
t	O
we	O
don	O
t	O
do	O
the	O
test	O
and	O
s	O
enters	O
a	O
special	O
unknown	B
state	B
there	O
is	O
also	O
some	O
cost	O
associated	O
with	O
performing	O
the	O
test	O
is	O
it	O
worth	O
doing	O
the	O
test	O
this	O
depends	O
on	O
how	O
much	O
our	O
meu	O
changes	O
if	O
we	O
know	O
the	O
if	O
you	O
don	O
t	O
do	O
the	O
test	O
we	O
have	O
m	O
eu	O
outcome	O
of	O
the	O
test	O
the	O
state	B
of	O
s	O
if	O
you	O
do	O
the	O
test	O
you	O
have	O
m	O
eu	O
from	O
equation	O
so	O
the	O
from	O
equation	O
improvement	O
in	O
utility	O
if	O
you	O
do	O
the	O
test	O
act	O
optimally	O
on	O
its	O
outcome	O
is	O
this	O
is	O
influence	O
diagrams	O
xt	O
zt	O
at	O
at	O
xt	O
rt	O
rt	O
figure	O
a	O
pomdp	B
shown	O
as	O
an	O
influence	O
diagram	O
zt	O
are	O
hidden	B
world	O
states	O
we	O
implicitly	O
make	O
the	O
no	B
forgetting	I
assumption	O
which	O
effectively	O
means	O
that	O
at	O
has	O
arrows	O
coming	O
into	O
it	O
from	O
all	O
previous	O
observations	O
an	O
mdp	B
shown	O
as	O
an	O
influence	O
diagram	O
called	O
the	O
value	B
of	I
perfect	I
information	B
so	O
we	O
should	O
do	O
the	O
test	O
as	O
long	O
as	O
it	O
costs	O
less	O
than	O
in	O
terms	O
of	O
graphical	B
models	I
the	O
vpi	O
of	O
a	O
variable	O
t	O
can	O
be	O
determined	O
by	O
computing	O
the	O
meu	O
for	O
the	O
base	O
influence	O
diagram	O
i	O
and	O
then	O
computing	O
the	O
meu	O
for	O
the	O
same	O
influence	O
diagram	O
where	O
we	O
add	O
information	B
arcs	O
from	O
t	O
to	O
the	O
action	B
nodes	B
and	O
then	O
computing	O
the	O
difference	O
in	O
other	O
words	O
vpi	O
meui	O
t	O
d	O
meui	O
where	O
d	O
is	O
the	O
decision	B
node	O
and	O
t	O
is	O
the	O
variable	O
we	O
are	O
measuring	O
it	O
is	O
possible	O
to	O
modify	O
the	O
variable	B
elimination	I
algorithm	O
so	O
that	O
it	O
computes	O
the	O
optimal	O
policy	B
given	O
an	O
influence	O
diagram	O
these	O
methods	O
essentially	O
work	O
backwards	O
from	O
the	O
final	O
time-step	O
computing	O
the	O
optimal	O
decision	B
at	O
each	O
step	O
assuming	O
all	O
following	O
actions	B
are	O
chosen	O
optimally	O
see	O
e	O
g	O
and	O
nilsson	O
kjaerulff	O
and	O
madsen	O
for	O
details	O
we	O
could	O
continue	O
to	O
extend	O
the	O
model	O
in	O
various	O
ways	O
for	O
example	O
we	O
could	O
imagine	O
a	O
dynamical	O
system	O
in	O
which	O
we	O
test	O
observe	O
outcomes	O
perform	O
actions	B
move	O
on	O
to	O
the	O
next	O
oil	O
well	O
and	O
continue	O
drilling	O
polluting	O
in	O
this	O
way	O
in	O
fact	O
many	O
problems	O
in	O
robotics	O
business	O
medicine	O
public	O
policy	B
etc	O
can	O
be	O
usefully	O
formulated	O
as	O
influence	O
diagrams	O
unrolled	B
over	O
time	O
lauritzen	O
and	O
nilsson	O
kjaerulff	O
and	O
madsen	O
a	O
generic	O
model	O
of	O
this	O
form	O
is	O
shown	O
in	O
figure	O
this	O
is	O
known	O
as	O
a	O
partially	B
observed	I
markov	B
decision	B
process	I
or	O
pomdp	B
pom-d-p	O
this	O
is	O
basically	O
a	O
hidden	B
markov	B
model	I
augmented	O
with	O
action	B
and	O
reward	B
nodes	B
this	O
can	O
be	O
used	O
to	O
model	O
the	O
perception-action	B
cycle	B
that	O
all	O
intelligent	O
agents	O
use	O
e	O
g	O
et	O
al	O
for	O
details	O
a	O
special	O
case	O
of	O
a	O
pomdp	B
in	O
which	O
the	O
states	O
are	O
fully	O
observed	O
is	O
called	O
a	O
markov	B
decision	B
process	I
or	O
mdp	B
shown	O
in	O
figure	O
this	O
is	O
much	O
easier	O
to	O
solve	O
since	O
we	O
only	O
have	O
to	O
compute	O
a	O
mapping	O
from	O
observed	O
states	O
to	O
actions	B
this	O
can	O
be	O
solved	O
using	O
dynamic	B
programming	I
e	O
g	O
and	O
barto	O
for	O
details	O
in	O
the	O
pomdp	B
case	O
the	O
information	B
arc	I
from	O
xt	O
to	O
at	O
is	O
not	O
sufficient	O
to	O
uniquely	O
determine	O
chapter	O
directed	B
graphical	B
models	I
nets	O
g	O
d	O
b	O
h	O
f	O
i	O
e	O
c	O
a	O
b	O
a	O
d	O
e	O
c	O
f	O
h	O
i	O
g	O
j	O
figure	O
some	O
dgms	O
the	O
best	O
action	B
since	O
the	O
state	B
is	O
not	O
fully	O
observed	O
instead	O
we	O
need	O
to	O
choose	O
actions	B
based	O
on	O
our	O
belief	B
state	B
since	O
the	O
belief	B
updating	I
process	O
is	O
deterministic	O
section	O
we	O
can	O
compute	O
a	O
belief	B
state	B
mdp	B
for	O
details	O
on	O
to	O
compute	O
the	O
policies	O
for	O
such	O
models	O
see	O
e	O
g	O
et	O
al	O
spaan	O
and	O
vlassis	O
exercises	O
exercise	O
marginalizing	O
a	O
node	O
in	O
a	O
dgm	B
koller	O
consider	O
the	O
dag	B
g	O
in	O
figure	O
assume	O
it	O
is	O
a	O
minimal	B
i-map	B
for	O
pa	O
b	O
c	O
d	O
e	O
f	O
x	O
now	O
consider	O
marginalizing	B
out	I
x	O
construct	O
a	O
new	O
dag	B
which	O
is	O
a	O
minimal	B
i-map	B
for	O
pa	O
b	O
c	O
d	O
e	O
f	O
specify	O
justify	O
which	O
extra	O
edges	B
need	O
to	O
be	O
added	O
exercise	O
bayes	O
ball	O
jordan	O
here	O
we	O
compute	O
some	O
global	O
independence	O
statements	O
from	O
some	O
directed	B
graphical	B
models	I
you	O
can	O
use	O
the	O
bayes	B
ball	I
algorithm	I
the	O
d-separation	O
criterion	O
or	O
the	O
method	O
of	O
converting	O
to	O
an	O
undirected	B
graph	B
should	O
give	O
the	O
same	O
results	O
a	O
consider	O
the	O
dag	B
in	O
figure	O
list	O
all	O
variables	O
that	O
are	O
independent	O
of	O
a	O
given	O
evidence	B
on	O
b	O
b	O
consider	O
the	O
dag	B
in	O
figure	O
list	O
all	O
variables	O
that	O
are	O
independent	O
of	O
a	O
given	O
evidence	B
on	O
j	O
exercise	O
markov	B
blanket	I
for	O
a	O
dgm	B
prove	O
that	O
the	O
full	B
conditional	I
for	O
node	O
i	O
in	O
a	O
dgm	B
is	O
given	O
by	O
pyjp	O
ayj	O
pxix	O
i	O
pxip	O
axi	O
yj	O
chxi	O
where	O
chxi	O
are	O
the	O
children	B
of	O
xi	O
and	O
p	O
ayj	O
are	O
the	O
parents	B
of	O
yj	O
exercise	O
hidden	B
variables	I
in	O
dgms	O
consider	O
the	O
dgms	O
in	O
figure	O
which	O
both	O
define	O
where	O
we	O
number	O
empty	O
nodes	B
left	O
to	O
right	O
top	O
to	O
bottom	O
the	O
graph	B
on	O
the	O
left	O
defines	O
the	O
joint	O
as	O
h	O
h	O
influence	O
diagrams	O
figure	O
weather	O
bn	O
fishing	O
bn	O
where	O
we	O
have	O
marginalized	O
over	O
the	O
hidden	B
variable	I
h	O
the	O
graph	B
on	O
the	O
right	O
defines	O
the	O
joint	O
as	O
a	O
b	O
c	O
points	O
assuming	O
all	O
nodes	B
h	O
are	O
binary	O
and	O
all	O
cpds	O
are	O
tabular	O
prove	O
that	O
the	O
model	O
on	O
the	O
left	O
has	O
free	O
parameters	O
points	O
assuming	O
all	O
nodes	B
are	O
binary	O
and	O
all	O
cpds	O
are	O
tabular	O
prove	O
that	O
the	O
model	O
on	O
the	O
right	O
has	O
free	O
parameters	O
points	O
suppose	O
we	O
have	O
a	O
data	O
set	O
d	O
x	O
n	O
for	O
n	O
n	O
where	O
we	O
observe	O
the	O
xs	O
but	O
not	O
h	O
and	O
we	O
want	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
cpds	O
using	O
maximum	O
likelihood	B
for	O
which	O
model	O
is	O
this	O
easier	O
explain	O
your	O
answer	O
exercise	O
bayes	O
nets	O
for	O
a	O
rainy	O
day	O
nando	O
de	O
freitas	O
in	O
this	O
question	O
you	O
must	O
model	O
a	O
problem	O
with	O
binary	O
variables	O
g	O
gray	O
v	O
vancouver	O
r	O
rain	O
and	O
s	O
sad	O
consider	O
the	O
directed	B
graphical	B
model	I
describing	O
the	O
relationship	O
between	O
these	O
variables	O
shown	O
in	O
figure	O
a	O
write	O
down	O
an	O
expression	O
for	O
p	O
in	O
terms	O
of	O
b	O
write	O
down	O
an	O
expression	O
for	O
p	O
is	O
this	O
the	O
same	O
or	O
different	O
to	O
p	O
explain	O
why	O
c	O
find	O
maximum	O
likelihood	B
estimates	O
of	O
using	O
the	O
following	O
data	O
set	O
where	O
each	O
row	O
is	O
a	O
training	O
case	O
may	O
state	B
your	O
answers	O
without	O
proof	O
v	O
g	O
r	O
s	O
exercise	O
fishing	O
nets	O
the	O
following	O
variables	O
et	O
al	O
consider	O
the	O
bayes	O
net	O
shown	O
in	O
figure	O
here	O
the	O
nodes	B
represent	O
spring	O
summer	O
autumn	O
sea	O
bass	O
medium	O
dark	O
thin	O
chapter	O
directed	B
graphical	B
models	I
nets	O
figure	O
a	O
qmr-style	O
network	O
with	O
some	O
hidden	B
leaves	B
removing	O
the	O
barren	O
nodes	B
the	O
corresponding	O
conditional	B
probability	I
tables	I
are	O
note	O
that	O
in	O
the	O
rows	O
represent	O
and	O
the	O
columns	O
each	O
row	O
sums	O
to	O
one	O
and	O
represents	O
the	O
child	O
of	O
the	O
cpd	B
thus	O
sea	O
bass	O
salmon	O
etc	O
answer	O
the	O
following	O
queries	O
you	O
may	O
use	O
matlab	O
or	O
do	O
it	O
by	O
hand	O
in	O
either	O
case	O
show	O
your	O
work	O
a	O
suppose	O
the	O
fish	O
was	O
caught	O
on	O
december	O
the	O
end	O
of	O
autumn	O
and	O
the	O
beginning	O
of	O
winter	O
and	O
thus	O
let	O
instead	O
of	O
the	O
above	O
prior	O
is	O
called	O
soft	O
evidence	B
since	O
we	O
do	O
not	O
know	O
the	O
exact	O
value	O
of	O
but	O
we	O
have	O
a	O
distribution	O
over	O
it	O
suppose	O
the	O
lightness	O
has	O
not	O
been	O
measured	O
but	O
it	O
is	O
known	O
that	O
the	O
fish	O
is	O
thin	O
classify	O
the	O
fish	O
as	O
salmon	O
or	O
sea	O
bass	O
b	O
suppose	O
all	O
we	O
know	O
is	O
that	O
the	O
fish	O
is	O
thin	O
and	O
medium	O
lightness	O
what	O
season	O
is	O
it	O
now	O
most	O
likely	O
use	O
exercise	O
removing	O
leaves	B
in	O
networks	O
a	O
consider	O
the	O
qmr	B
network	O
where	O
only	O
some	O
of	O
the	O
symtpoms	O
are	O
observed	O
for	O
example	O
in	O
figure	O
and	O
are	O
hidden	B
show	O
that	O
we	O
can	O
safely	O
remove	O
all	O
the	O
hidden	B
leaf	B
nodes	B
without	O
affecting	O
the	O
posterior	O
over	O
the	O
disease	O
nodes	B
i	O
e	O
prove	O
that	O
we	O
can	O
compute	O
using	O
the	O
network	O
in	O
figure	O
this	O
is	O
called	O
barren	B
node	I
removal	I
and	O
can	O
be	O
applied	O
to	O
any	O
dgm	B
b	O
now	O
suppose	O
we	O
partition	O
the	O
leaves	B
into	O
three	O
groups	O
on	O
off	O
and	O
unknown	B
clearly	O
we	O
can	O
remove	O
the	O
unknown	B
leaves	B
since	O
they	O
are	O
hidden	B
and	O
do	O
not	O
affect	O
their	O
parents	B
show	O
that	O
we	O
can	O
analytically	O
remove	O
the	O
leaves	B
that	O
are	O
in	O
the	O
off	O
state	B
by	O
absorbing	O
their	O
effect	O
into	O
the	O
prior	O
of	O
the	O
parents	B
trick	O
only	O
works	O
for	O
noisy-or	B
cpds	O
exercise	O
handling	O
negative	O
findings	O
in	O
the	O
qmr	B
network	O
consider	O
the	O
qmr	B
network	O
let	O
d	O
be	O
the	O
hidden	B
diseases	O
f	O
be	O
the	O
negative	O
findings	O
nodes	B
that	O
are	O
be	O
the	O
positive	O
findings	O
nodes	B
that	O
are	O
on	O
we	O
can	O
compute	O
the	O
posterior	O
pdf	B
in	O
off	O
and	O
f	O
pdpf	O
then	O
absorb	O
the	O
positive	O
findings	O
two	O
steps	O
first	O
absorb	O
the	O
negative	O
findings	O
pdf	B
pdf	B
f	O
pdf	B
show	O
that	O
the	O
first	O
step	O
can	O
be	O
done	O
in	O
odf	O
time	O
where	O
is	O
the	O
number	O
of	O
dieases	O
and	O
is	O
the	O
number	O
of	O
negative	O
findings	O
for	O
simplicity	O
you	O
can	O
ignore	O
leak	O
nodes	B
the	O
reason	O
for	O
this	O
is	O
that	O
there	O
is	O
no	O
correlation	O
induced	O
amongst	O
the	O
parents	B
when	O
the	O
finding	O
is	O
off	O
since	O
there	O
is	O
no	O
explaining	B
away	I
influence	O
diagrams	O
exercise	O
moralization	B
does	O
not	O
introduce	O
new	O
independence	O
statements	O
recall	B
that	O
the	O
process	O
of	O
moralizing	O
a	O
dag	B
means	O
connecting	O
together	O
all	O
unmarried	O
parents	B
that	O
share	O
a	O
common	O
child	O
and	O
then	O
dropping	O
all	O
the	O
arrows	O
let	O
m	O
be	O
the	O
moralization	B
of	O
dag	B
g	O
show	O
that	O
cim	O
cig	O
where	O
ci	B
are	O
the	O
set	O
of	O
conditional	B
independence	I
statements	O
implied	O
by	O
the	O
model	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
latent	B
variable	I
models	I
in	O
chapter	O
we	O
showed	O
how	O
graphical	B
models	I
can	O
be	O
used	O
to	O
define	O
high-dimensional	O
joint	O
probability	O
distributions	O
the	O
basic	O
idea	O
is	O
to	O
model	O
dependence	O
between	O
two	O
variables	O
by	O
adding	O
an	O
edge	O
between	O
them	O
in	O
the	O
graph	B
the	O
graph	B
represents	O
conditional	B
independence	I
but	O
you	O
get	O
the	O
point	O
an	O
alternative	O
approach	O
is	O
to	O
assume	O
that	O
the	O
observed	O
variables	O
are	O
correlated	O
because	O
they	O
arise	O
from	O
a	O
hidden	B
common	O
cause	O
model	O
with	O
hidden	B
variables	I
are	O
also	O
known	O
as	O
latent	B
variable	I
models	I
or	O
lvms	O
as	O
we	O
will	O
see	O
in	O
this	O
chapter	O
such	O
models	O
are	O
harder	O
to	O
fit	O
than	O
models	O
with	O
no	O
latent	B
variables	O
however	O
they	O
can	O
have	O
significant	O
advantages	O
for	O
two	O
main	O
reasons	O
first	O
lvms	O
often	O
have	O
fewer	O
parameters	O
than	O
models	O
that	O
directly	O
represent	O
correlation	O
in	O
the	O
visible	B
space	O
this	O
is	O
illustrated	O
in	O
figure	O
if	O
all	O
nodes	B
h	O
are	O
binary	O
and	O
all	O
cpds	O
are	O
tabular	O
the	O
model	O
on	O
the	O
left	O
has	O
free	O
parameters	O
whereas	O
the	O
model	O
on	O
the	O
right	O
has	O
free	O
parameters	O
second	O
the	O
hidden	B
variables	I
in	O
an	O
lvm	B
can	O
serve	O
as	O
a	O
bottleneck	B
which	O
computes	O
a	O
compressed	O
representation	O
of	O
the	O
data	O
this	O
forms	O
the	O
basis	O
of	O
unsupervised	B
learning	B
as	O
we	O
will	O
see	O
figure	O
illustrates	O
some	O
generic	O
lvm	B
structures	O
that	O
can	O
be	O
used	O
for	O
this	O
purpose	O
in	O
general	O
there	O
are	O
l	O
latent	B
variables	O
zil	O
and	O
d	O
visible	B
variables	I
xid	O
where	O
usually	O
d	O
l	O
if	O
we	O
have	O
l	O
there	O
are	O
many	O
latent	B
factors	B
contributing	O
to	O
each	O
if	O
l	O
we	O
we	O
only	O
have	O
a	O
single	O
latent	B
observation	B
so	O
we	O
have	O
a	O
many-to-many	O
mapping	O
in	O
this	O
case	O
zi	O
is	O
usually	O
discrete	B
and	O
we	O
have	O
a	O
one-to-many	O
mapping	O
we	O
can	O
variable	O
also	O
have	O
a	O
many-to-one	O
mapping	O
representing	O
different	O
competing	O
factors	B
or	O
causes	O
for	O
each	O
observed	O
variable	O
such	O
models	O
form	O
the	O
basis	O
of	O
probabilistic	B
matrix	B
factorization	I
discussed	O
in	O
section	O
finally	O
we	O
can	O
have	O
a	O
one-to-one	O
mapping	O
which	O
can	O
be	O
represented	O
as	O
zi	O
xi	O
by	O
allowing	O
zi	O
andor	O
xi	O
to	O
be	O
vector-valued	O
this	O
representation	O
can	O
subsume	O
all	O
the	O
others	O
depending	O
on	O
the	O
form	O
of	O
the	O
likelihood	B
pxizi	O
and	O
the	O
prior	O
pzi	O
we	O
can	O
generate	O
a	O
variety	O
of	O
different	O
models	O
as	O
summarized	O
in	O
table	O
mixture	B
models	O
the	O
simplest	O
form	O
of	O
lvm	B
is	O
when	O
zi	O
k	O
representing	O
a	O
discrete	B
latent	B
state	B
we	O
will	O
use	O
a	O
discrete	B
prior	O
for	O
this	O
pzi	O
cat	O
for	O
the	O
likelihood	B
we	O
use	O
pxizi	O
k	O
pkxi	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
figure	O
a	O
dgm	B
with	O
and	O
without	O
hidden	B
variables	I
the	O
leaves	B
represent	O
medical	O
symptoms	O
the	O
roots	O
represent	O
primary	O
causes	O
such	O
as	O
smoking	O
diet	O
and	O
exercise	O
the	O
hidden	B
variable	I
can	O
represent	O
mediating	O
factors	B
such	O
as	O
heart	O
disease	O
which	O
might	O
not	O
be	O
directly	O
visible	B
zi	O
zil	O
xid	O
xid	O
zil	O
xi	O
zi	O
xi	O
figure	O
a	O
latent	B
variable	O
model	O
represented	O
as	O
a	O
dgm	B
many-to-one	O
one-to-one	O
many-to-many	O
one-to-many	O
where	O
pk	O
is	O
the	O
k	O
th	O
base	B
distribution	I
for	O
the	O
observations	O
this	O
can	O
be	O
of	O
any	O
type	O
the	O
overall	O
model	O
is	O
known	O
as	O
a	O
mixture	B
model	I
since	O
we	O
are	O
mixing	O
together	O
the	O
k	O
base	O
distributions	O
as	O
follows	O
pxi	O
kpkxi	O
this	O
is	O
a	O
convex	B
combination	I
of	O
the	O
pk	O
s	O
since	O
we	O
are	O
taking	O
a	O
weighted	O
sum	O
where	O
the	O
mixing	B
weights	I
k	O
satisfy	O
k	O
and	O
k	O
we	O
give	O
some	O
examples	O
below	O
mixture	B
models	O
pxizi	O
mvn	B
prod	O
discrete	B
prod	O
gaussian	B
prod	O
gaussian	B
prod	O
discrete	B
prod	O
discrete	B
prod	O
noisy-or	B
prod	O
bernoulli	B
name	O
mixture	B
of	I
gaussians	I
mixture	B
of	O
multinomials	O
factor	B
analysis	I
probabilistic	B
pca	B
probabilistic	O
ica	B
sparse	B
coding	I
pzi	O
discrete	B
discrete	B
prod	O
gaussian	B
prod	O
laplace	B
prod	O
gaussian	B
multinomial	B
pca	B
dirichlet	B
prod	O
bernoulli	B
prod	O
bernoulli	B
latent	B
dirichlet	B
allocation	I
qmr	B
sigmoid	B
belief	I
net	I
section	O
table	O
prod	O
discrete	B
in	O
the	O
likelihood	B
means	O
a	O
factored	O
distribution	O
of	O
the	O
form	O
gaussian	B
means	O
a	O
factored	O
distribution	O
of	O
the	O
form	O
analysis	O
ica	B
stands	O
for	O
indepedendent	O
components	O
analysis	O
summary	O
of	O
some	O
popular	O
directed	B
latent	B
variable	I
models	I
here	O
prod	O
means	O
product	O
so	O
j	O
catxijzi	O
and	O
prod	O
j	O
n	O
pca	B
stands	O
for	O
principal	B
components	I
figure	O
a	O
mixture	B
of	I
gaussians	I
in	O
we	O
show	O
the	O
contours	O
of	O
constant	O
probability	O
for	O
each	O
component	O
in	O
the	O
mixture	B
a	O
surface	O
plot	O
of	O
the	O
overall	O
density	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
mixgaussplotdemo	O
mixtures	O
of	O
gaussians	O
the	O
most	O
widely	O
used	O
mixture	B
model	I
is	O
the	O
mixture	B
of	I
gaussians	I
also	O
called	O
a	O
gaussian	B
mixture	B
model	I
or	O
gmm	B
in	O
this	O
model	O
each	O
base	B
distribution	I
in	O
the	O
mixture	B
is	O
a	O
multivariate	B
gaussian	B
with	O
mean	B
k	O
and	O
covariance	B
matrix	I
k	O
thus	O
the	O
model	O
has	O
the	O
form	O
pxi	O
kn	O
k	O
k	O
figure	O
shows	O
a	O
mixture	B
of	I
gaussians	I
in	O
each	O
mixture	B
component	O
is	O
represented	O
by	O
a	O
different	O
set	O
of	O
eliptical	O
contours	O
given	O
a	O
sufficiently	O
large	O
number	O
of	O
mixture	B
components	O
a	O
gmm	B
can	O
be	O
used	O
to	O
approximate	O
any	O
density	O
defined	O
on	O
r	O
d	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
mixture	B
of	O
multinoullis	O
we	O
can	O
use	O
mixture	B
models	O
to	O
define	O
density	O
models	O
on	O
many	O
kinds	O
of	O
data	O
for	O
example	O
suppose	O
our	O
data	O
consist	O
of	O
d-dimensional	O
bit	O
vectors	O
in	O
this	O
case	O
an	O
appropriate	O
classconditional	O
density	O
is	O
a	O
product	O
of	O
bernoullis	O
pxizi	O
k	O
berxij	O
jk	O
jk	O
xij	O
xij	O
where	O
jk	O
is	O
the	O
probability	O
that	O
bit	O
j	O
turns	O
on	O
in	O
cluster	O
k	O
the	O
latent	B
variables	O
do	O
not	O
have	O
to	O
any	O
meaning	O
we	O
might	O
simply	O
introduce	O
latent	B
variables	O
in	O
order	O
to	O
make	O
the	O
model	O
more	O
powerful	O
for	O
example	O
one	O
can	O
show	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
mixture	B
distribution	O
are	O
given	O
by	O
k	O
e	O
cov	O
k	O
k	O
k	O
k	O
k	O
t	O
k	O
e	O
e	O
t	O
k	O
where	O
k	O
diag	O
jk	O
so	O
although	O
the	O
component	O
distributions	O
are	O
factorized	O
the	O
joint	B
distribution	I
is	O
not	O
thus	O
the	O
mixture	B
distribution	O
can	O
capture	O
correlations	O
between	O
variables	O
unlike	O
a	O
single	O
product-of-bernoullis	O
model	O
using	O
mixture	B
models	O
for	O
clustering	B
there	O
are	O
two	O
main	O
applications	O
of	O
mixture	B
models	O
the	O
first	O
is	O
to	O
use	O
them	O
as	O
a	O
black-box	B
density	O
model	O
pxi	O
this	O
can	O
be	O
useful	O
for	O
a	O
variety	O
of	O
tasks	O
such	O
as	O
data	B
compression	I
outlier	O
detection	O
and	O
creating	O
generative	O
classifiers	O
where	O
we	O
model	O
each	O
class-conditional	B
density	I
pxy	O
c	O
by	O
a	O
mixture	B
distribution	O
section	O
the	O
second	O
and	O
more	O
common	O
application	O
of	O
mixture	B
models	O
is	O
to	O
use	O
them	O
for	O
clustering	B
we	O
discuss	O
this	O
topic	B
in	O
detail	O
in	O
chapter	O
but	O
the	O
basic	O
idea	O
is	O
simple	O
we	O
first	O
fit	O
the	O
mixture	B
model	I
and	O
then	O
compute	O
pzi	O
kxi	O
which	O
represents	O
the	O
posterior	O
probability	O
that	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
this	O
is	O
known	O
as	O
the	O
responsibility	B
of	O
cluster	O
k	O
for	O
point	O
i	O
and	O
can	O
be	O
computed	O
using	O
bayes	B
rule	I
as	O
follows	O
rik	O
pzi	O
kxi	O
pzi	O
k	O
k	O
pzi	O
this	O
procedure	O
is	O
called	O
soft	B
clustering	B
and	O
is	O
identical	O
to	O
the	O
computations	O
performed	O
when	O
using	O
a	O
generative	O
classifier	O
the	O
difference	O
between	O
the	O
two	O
models	O
only	O
arises	O
at	O
training	O
time	O
in	O
the	O
mixture	B
case	O
we	O
never	O
observe	O
zi	O
whereas	O
with	O
a	O
generative	O
classifier	O
we	O
do	O
observe	O
yi	O
plays	O
the	O
role	O
of	O
zi	O
we	O
can	O
represent	O
the	O
amount	O
of	O
uncertainty	B
in	O
the	O
cluster	O
assignment	O
by	O
using	O
maxk	O
rik	O
assuming	O
this	O
is	O
small	O
it	O
may	O
be	O
reasonable	O
to	O
compute	O
a	O
hard	B
clustering	B
using	O
the	O
map	B
estimate	I
given	O
by	O
z	O
i	O
arg	O
max	O
k	O
rik	O
arg	O
max	O
k	O
log	O
pxizi	O
k	O
log	O
pzi	O
k	O
mixture	B
models	O
s	O
e	O
n	O
e	O
g	O
yeast	O
microarray	O
data	O
k	O
means	O
centroids	B
time	O
figure	O
centers	O
produced	O
by	O
k-means	O
figure	O
generated	O
by	O
kmeansyeastdemo	O
some	O
yeast	O
gene	O
expression	O
data	O
plotted	O
as	O
a	O
time	O
series	O
visualizing	B
the	O
cluster	O
figure	O
we	O
fit	O
a	O
mixture	B
of	O
bernoullis	O
to	O
the	O
binarized	O
mnist	B
digit	O
data	O
we	O
show	O
the	O
mle	B
for	O
the	O
corresponding	O
cluster	O
means	O
k	O
the	O
numbers	O
on	O
top	O
of	O
each	O
image	O
represent	O
the	O
mixing	B
weights	I
k	O
no	O
labels	O
were	O
used	O
when	O
training	O
the	O
model	O
figure	O
generated	O
by	O
mixbermnistem	O
hard	B
clustering	B
using	O
a	O
gmm	B
is	O
illustrated	O
in	O
figure	O
where	O
we	O
cluster	O
some	O
data	O
representing	O
the	O
height	O
and	O
weight	O
of	O
people	O
the	O
colors	O
represent	O
the	O
hard	O
assignments	O
note	O
that	O
the	O
identity	O
of	O
the	O
labels	O
used	O
is	O
immaterial	O
we	O
are	O
free	O
to	O
rename	O
all	O
the	O
clusters	B
without	O
affecting	O
the	O
partitioning	B
of	O
the	O
data	O
this	O
is	O
called	O
label	B
switching	I
another	O
example	O
is	O
shown	O
in	O
figure	O
here	O
the	O
data	O
vectors	O
xi	O
r	O
represent	O
the	O
expression	O
levels	O
of	O
different	O
genes	O
at	O
different	O
time	O
points	O
we	O
clustered	O
them	O
using	O
a	O
gmm	B
we	O
see	O
that	O
there	O
are	O
several	O
kinds	O
of	O
genes	O
such	O
as	O
those	O
whose	O
expression	O
level	O
goes	O
up	O
monotonically	O
over	O
time	O
response	O
to	O
a	O
given	O
stimulus	O
those	O
whose	O
expression	O
level	O
goes	O
down	O
monotonically	O
and	O
those	O
with	O
more	O
complex	O
response	O
patterns	O
we	O
have	O
clustered	O
the	O
series	O
into	O
k	O
groups	O
section	O
for	O
details	O
on	O
how	O
to	O
choose	O
k	O
for	O
example	O
we	O
can	O
represent	O
each	O
cluster	O
by	O
a	O
prototype	B
or	O
centroid	B
this	O
is	O
shown	O
in	O
figure	O
as	O
an	O
example	O
of	O
clustering	B
binary	O
data	O
consider	O
a	O
binarized	O
version	O
of	O
the	O
mnist	B
handwritten	O
digit	O
dataset	O
figure	O
where	O
we	O
ignore	O
the	O
class	O
labels	O
we	O
can	O
fit	O
a	O
mixture	B
of	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
bernoullis	O
to	O
this	O
using	O
k	O
and	O
then	O
visualize	O
the	O
resulting	O
centroids	B
k	O
as	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
method	O
correctly	O
discovered	O
some	O
of	O
the	O
digit	O
classes	O
but	O
overall	O
the	O
results	O
aren	O
t	O
great	O
it	O
has	O
created	O
multiple	O
clusters	B
for	O
some	O
digits	O
and	O
no	O
clusters	B
for	O
others	O
there	O
are	O
several	O
possible	O
reasons	O
for	O
these	O
errors	O
the	O
model	O
is	O
very	O
simple	O
and	O
does	O
not	O
capture	O
the	O
relevant	O
visual	O
characteristics	O
of	O
a	O
digit	O
for	O
example	O
each	O
pixel	O
is	O
treated	O
independently	O
and	O
there	O
is	O
no	O
notion	O
of	O
shape	O
or	O
a	O
stroke	O
although	O
we	O
think	O
there	O
should	O
be	O
clusters	B
some	O
of	O
the	O
digits	O
actually	O
exhibit	O
a	O
fair	O
degree	B
of	O
visual	O
variety	O
for	O
example	O
there	O
are	O
two	O
ways	O
of	O
writing	O
s	O
and	O
without	O
the	O
cross	O
bar	O
figure	O
illustrates	O
some	O
of	O
the	O
range	O
in	O
writing	O
styles	O
thus	O
we	O
need	O
k	O
clusters	B
to	O
adequately	O
model	O
this	O
data	O
however	O
if	O
we	O
set	O
k	O
to	O
be	O
large	O
there	O
is	O
nothing	O
in	O
the	O
model	O
or	O
algorithm	O
preventing	O
the	O
extra	O
clusters	B
from	O
being	O
used	O
to	O
create	O
multiple	O
versions	O
of	O
the	O
same	O
digit	O
and	O
indeed	O
this	O
is	O
what	O
happens	O
we	O
can	O
use	O
model	B
selection	I
to	O
prevent	O
too	O
many	O
clusters	B
from	O
being	O
chosen	O
but	O
what	O
looks	O
visually	O
appealing	O
and	O
what	O
makes	O
a	O
good	O
density	O
estimator	B
may	O
be	O
quite	O
different	O
the	O
likelihood	B
function	O
is	O
not	O
convex	B
so	O
we	O
may	O
be	O
stuck	O
in	O
a	O
local	O
optimum	O
as	O
we	O
explain	O
in	O
section	O
this	O
example	O
is	O
typical	O
of	O
mixture	B
modeling	O
and	O
goes	O
to	O
show	O
one	O
must	O
be	O
very	O
cautious	O
a	O
little	O
bit	O
of	O
trying	O
to	O
interpret	O
any	O
clusters	B
that	O
are	O
discovered	O
by	O
the	O
method	O
supervision	O
or	O
using	O
informative	O
priors	O
can	O
help	O
a	O
lot	O
mixtures	O
of	O
experts	O
section	O
described	O
how	O
to	O
use	O
mixture	B
models	O
in	O
the	O
context	O
of	O
generative	O
classifiers	O
we	O
can	O
also	O
use	O
them	O
to	O
create	O
discriminative	B
models	O
for	O
classification	O
and	O
regression	B
for	O
example	O
consider	O
the	O
data	O
in	O
figure	O
it	O
seems	O
like	O
a	O
good	O
model	O
would	O
be	O
three	O
different	O
linear	B
regression	B
functions	O
each	O
applying	O
to	O
a	O
different	O
part	O
of	O
the	O
input	O
space	O
we	O
can	O
model	O
this	O
by	O
allowing	O
the	O
mixing	B
weights	I
and	O
the	O
mixture	B
densities	O
to	O
be	O
input-dependent	O
pyixi	O
zi	O
k	O
k	O
xi	O
k	O
pzixi	O
catzisvt	O
xi	O
see	O
figure	O
for	O
the	O
dgm	B
this	O
model	O
is	O
called	O
a	O
mixture	B
of	I
experts	I
or	O
moe	B
and	O
jacobs	O
the	O
idea	O
is	O
that	O
each	O
submodel	O
is	O
considered	O
to	O
be	O
an	O
expert	O
in	O
a	O
certain	O
region	O
of	O
input	O
space	O
the	O
function	O
pzi	O
kxi	O
is	O
called	O
a	O
gating	B
function	I
and	O
decides	O
which	O
expert	O
to	O
use	O
depending	O
on	O
the	O
input	O
values	O
for	O
example	O
figure	O
shows	O
how	O
the	O
three	O
experts	O
have	O
carved	O
up	O
the	O
input	O
space	O
figure	O
shows	O
the	O
predictions	O
of	O
each	O
expert	O
individually	O
this	O
case	O
the	O
experts	O
are	O
just	O
linear	B
regression	B
models	O
and	O
figure	O
shows	O
the	O
overall	O
prediction	O
of	O
the	O
model	O
obtained	O
using	O
pyixi	O
pzi	O
kxi	O
zi	O
k	O
k	O
we	O
discuss	O
how	O
to	O
fit	O
this	O
model	O
in	O
section	O
mixture	B
models	O
expert	O
predictions	O
fixed	O
mixing	O
gating	O
functions	O
fixed	O
mixing	O
predicted	O
mean	B
and	O
var	B
fixed	O
mixing	O
figure	O
some	O
data	O
fit	O
with	O
three	O
separate	O
regression	B
lines	O
gating	O
functions	O
for	O
three	O
different	O
experts	O
the	O
conditionally	O
weighted	B
average	I
of	O
the	O
three	O
expert	O
predictions	O
figure	O
generated	O
by	O
mixexpdemo	O
xi	O
zi	O
yi	O
xi	O
i	O
i	O
yi	O
figure	O
a	O
mixture	B
of	I
experts	I
a	O
hierarchical	B
mixture	B
of	I
experts	I
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
forwards	O
problem	O
expert	O
predictions	O
prediction	O
mean	B
mode	B
figure	O
some	O
data	O
from	O
a	O
simple	O
forwards	B
model	I
some	O
data	O
from	O
the	O
inverse	O
model	O
fit	O
the	O
with	O
a	O
mixture	B
of	O
linear	O
regressions	O
training	O
points	O
are	O
color	O
coded	O
by	O
their	O
responsibilities	O
predictive	B
mean	B
cross	O
and	O
mode	B
square	O
based	O
on	O
figures	O
and	O
of	O
figure	O
generated	O
by	O
mixexpdemoonetomany	O
it	O
should	O
be	O
clear	O
that	O
we	O
can	O
plug	O
in	O
any	O
model	O
for	O
the	O
expert	O
for	O
example	O
we	O
can	O
use	O
neural	B
networks	I
to	O
represent	O
both	O
the	O
gating	O
functions	O
and	O
the	O
experts	O
the	O
result	O
is	O
known	O
as	O
a	O
mixture	B
density	I
network	I
such	O
models	O
are	O
slower	O
to	O
train	O
but	O
can	O
be	O
more	O
flexible	O
than	O
mixtures	O
of	O
experts	O
see	O
for	O
details	O
it	O
is	O
also	O
possible	O
to	O
make	O
each	O
expert	O
be	O
itself	O
a	O
mixture	B
of	I
experts	I
this	O
gives	O
rise	O
to	O
a	O
model	O
known	O
as	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
see	O
figure	O
for	O
the	O
dgm	B
and	O
section	O
for	O
further	O
details	O
application	O
to	O
inverse	B
problems	I
mixtures	O
of	O
experts	O
are	O
useful	O
in	O
solving	O
inverse	B
problems	I
these	O
are	O
problems	O
where	O
we	O
have	O
to	O
invert	O
a	O
many-to-one	O
mapping	O
a	O
typical	O
example	O
is	O
in	O
robotics	O
where	O
the	O
location	O
of	O
the	O
end	B
effector	I
y	O
is	O
uniquely	O
determined	O
by	O
the	O
joint	O
angles	O
of	O
the	O
motors	O
x	O
however	O
for	O
any	O
given	O
location	O
y	O
there	O
are	O
many	O
settings	O
of	O
the	O
joints	O
x	O
that	O
can	O
produce	O
it	O
thus	O
the	O
inverse	O
mapping	O
x	O
f	O
is	O
not	O
unique	O
another	O
example	O
is	O
kinematic	B
tracking	B
of	O
people	O
from	O
video	O
et	O
al	O
where	O
the	O
mapping	O
from	O
image	O
appearance	O
to	O
pose	O
is	O
not	O
unique	O
due	O
to	O
self	O
occlusion	O
etc	O
parameter	B
estimation	O
for	O
mixture	B
models	O
z	O
zn	O
xn	O
x	O
z	O
zi	O
xi	O
x	O
n	O
figure	O
a	O
lvm	B
represented	O
as	O
a	O
dgm	B
left	O
model	O
is	O
unrolled	B
for	O
n	O
examples	O
right	O
same	O
model	O
using	O
plate	O
notation	O
a	O
simpler	O
example	O
for	O
illustration	O
purposes	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
this	O
defines	O
a	O
function	O
y	O
f	O
since	O
for	O
every	O
value	O
x	O
along	O
the	O
horizontal	O
axis	O
there	O
is	O
a	O
unique	O
response	O
y	O
this	O
is	O
sometimes	O
called	O
the	O
forwards	B
model	I
now	O
consider	O
the	O
problem	O
of	O
computing	O
x	O
f	O
the	O
corresponding	O
inverse	O
model	O
is	O
shown	O
in	O
figure	O
this	O
is	O
obtained	O
by	O
simply	O
interchanging	O
the	O
x	O
and	O
y	O
axes	O
now	O
we	O
see	O
that	O
for	O
some	O
values	O
along	O
the	O
horizontal	O
axis	O
there	O
are	O
multiple	O
possible	O
outputs	O
so	O
the	O
inverse	O
is	O
not	O
uniquely	O
defined	O
for	O
example	O
if	O
y	O
then	O
x	O
could	O
be	O
or	O
consequently	O
the	O
predictive	B
distribution	O
pxy	O
is	O
multimodal	O
we	O
can	O
fit	O
a	O
mixture	B
of	O
linear	O
experts	O
to	O
this	O
data	O
figure	O
shows	O
the	O
prediction	O
of	O
each	O
expert	O
and	O
figure	O
shows	O
plugin	O
approximation	O
to	O
the	O
posterior	O
predictive	B
mode	B
and	O
mean	B
note	O
that	O
the	O
posterior	B
mean	B
does	O
not	O
yield	O
good	O
predictions	O
in	O
fact	O
any	O
model	O
which	O
is	O
trained	O
to	O
minimize	O
mean	B
squared	B
error	I
even	O
if	O
the	O
model	O
is	O
a	O
flexible	O
nonlinear	O
model	O
such	O
as	O
neural	B
network	I
will	O
work	O
poorly	O
on	O
inverse	B
problems	I
such	O
as	O
this	O
however	O
the	O
posterior	B
mode	B
where	O
the	O
mode	B
is	O
input	O
dependent	O
provides	O
a	O
reasonable	O
approximation	O
parameter	B
estimation	O
for	O
mixture	B
models	O
we	O
have	O
seen	O
how	O
to	O
compute	O
the	O
posterior	O
over	O
the	O
hidden	B
variables	I
given	O
the	O
observed	O
variables	O
assuming	O
the	O
parameters	O
are	O
known	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
parameters	O
in	O
section	O
we	O
showed	O
that	O
when	O
we	O
have	O
complete	B
data	I
and	O
a	O
factored	O
prior	O
the	O
posterior	O
over	O
the	O
parameters	O
also	O
factorizes	O
making	O
computation	O
very	O
simple	O
unfortunately	O
this	O
is	O
no	O
longer	O
true	O
if	O
we	O
have	O
hidden	B
variables	I
andor	O
missing	B
data	I
the	O
reason	O
is	O
apparent	O
from	O
looking	O
at	O
figure	O
if	O
the	O
zi	O
were	O
observed	O
then	O
by	O
d-separation	O
we	O
see	O
that	O
z	O
xd	O
and	O
hence	O
the	O
posterior	O
will	O
factorize	O
but	O
since	O
in	O
an	O
lvm	B
the	O
zi	O
are	O
hidden	B
the	O
parameters	O
are	O
no	O
longer	O
independent	O
and	O
the	O
posterior	O
does	O
not	O
factorize	O
making	O
it	O
much	O
harder	O
to	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
figure	O
left	O
n	O
data	O
points	O
sampled	O
from	O
a	O
mixture	B
of	I
gaussians	I
in	O
with	O
k	O
k	O
and	O
right	O
likelihood	B
surface	O
pd	O
with	O
all	O
other	O
parameters	O
set	O
to	O
their	O
true	O
values	O
we	O
see	O
the	O
two	O
symmetric	B
modes	O
reflecting	O
the	O
unidentifiability	O
of	O
the	O
parameters	O
figure	O
generated	O
by	O
mixgaussliksurfacedemo	O
compute	O
this	O
also	O
complicates	O
the	O
computation	O
of	O
map	O
and	O
ml	O
estimates	O
as	O
we	O
discus	O
below	O
unidentifiability	O
the	O
main	O
problem	O
with	O
computing	O
p	O
for	O
an	O
lvm	B
is	O
that	O
the	O
posterior	O
may	O
have	O
multiple	O
modes	O
to	O
see	O
why	O
consider	O
a	O
gmm	B
if	O
the	O
zi	O
were	O
all	O
observed	O
we	O
would	O
have	O
a	O
unimodal	O
posterior	O
for	O
the	O
parameters	O
p	O
dir	O
niw	B
k	O
kd	O
consequently	O
we	O
can	O
easily	O
find	O
the	O
globally	O
optimal	O
map	B
estimate	I
hence	O
globally	O
optimal	O
mle	B
but	O
now	O
suppose	O
the	O
zi	O
s	O
are	O
hidden	B
in	O
this	O
case	O
for	O
each	O
of	O
the	O
possible	O
ways	O
of	O
filling	O
in	O
the	O
zi	O
s	O
we	O
get	O
a	O
different	O
unimodal	O
likelihood	B
thus	O
when	O
we	O
marginalize	O
out	O
over	O
the	O
zi	O
s	O
we	O
get	O
a	O
multi-modal	O
posterior	O
for	O
p	O
these	O
modes	O
correspond	O
to	O
different	O
labelings	O
of	O
the	O
clusters	B
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
plot	O
the	O
likelihood	B
function	O
pd	O
for	O
a	O
gmm	B
with	O
k	O
for	O
the	O
data	O
is	O
shown	O
in	O
figure	O
we	O
see	O
two	O
peaks	O
one	O
corresponding	O
to	O
the	O
case	O
where	O
and	O
the	O
other	O
to	O
the	O
case	O
where	O
we	O
say	O
the	O
parameters	O
are	O
not	O
identifiable	O
since	O
there	O
is	O
not	O
a	O
unique	O
mle	B
therefore	O
there	O
cannot	O
be	O
a	O
unique	O
map	B
estimate	I
the	O
prior	O
does	O
not	O
rule	O
out	O
certain	O
labelings	O
and	O
hence	O
the	O
posterior	O
must	O
be	O
multimodal	O
the	O
question	O
of	O
how	O
many	O
modes	O
there	O
do	O
not	O
confuse	O
multimodality	O
of	O
the	O
parameter	B
posterior	O
p	O
with	O
the	O
multimodality	O
defined	O
by	O
the	O
model	O
px	O
in	O
the	O
latter	O
case	O
if	O
we	O
have	O
k	O
clusters	B
we	O
would	O
expect	O
to	O
only	O
get	O
k	O
peaks	O
although	O
it	O
is	O
theoretically	O
possible	O
to	O
get	O
more	O
than	O
k	O
at	O
least	O
if	O
d	O
and	O
williams	O
parameter	B
estimation	O
for	O
mixture	B
models	O
unidentifiability	O
can	O
cause	O
a	O
problem	O
for	O
bayesian	B
inference	B
are	O
in	O
the	O
parameter	B
posterior	O
is	O
hard	O
to	O
answer	O
there	O
are	O
k	O
possible	O
labelings	O
but	O
some	O
of	O
the	O
peaks	O
might	O
get	O
merged	O
nevertheless	O
there	O
can	O
be	O
an	O
exponential	O
number	O
since	O
finding	O
the	O
optimal	O
mle	B
for	O
a	O
gmm	B
is	O
np-hard	B
et	O
al	O
drineas	O
et	O
al	O
for	O
example	O
suppose	O
we	O
draw	O
some	O
samples	B
from	O
the	O
posterior	O
p	O
and	O
then	O
average	O
them	O
to	O
try	O
to	O
approximate	O
the	O
posterior	B
mean	B
kind	O
of	O
monte	B
carlo	I
approach	O
is	O
s	O
explained	O
in	O
more	O
detail	O
in	O
chapter	O
if	O
the	O
samples	B
come	O
from	O
different	O
modes	O
the	O
average	O
will	O
be	O
meaningless	O
note	O
however	O
that	O
it	O
is	O
reasonable	O
to	O
average	O
the	O
posterior	O
predictive	B
distributions	O
px	O
px	O
since	O
the	O
likelihood	B
function	O
is	O
invariant	B
to	O
which	O
mode	B
the	O
parameters	O
came	O
from	O
s	O
a	O
variety	O
of	O
solutions	O
have	O
been	O
proposed	O
to	O
the	O
unidentifiability	O
problem	O
these	O
solutions	O
depend	O
on	O
the	O
details	O
of	O
the	O
model	O
and	O
the	O
inference	B
algorithm	O
that	O
is	O
used	O
for	O
example	O
see	O
for	O
an	O
approach	O
to	O
handling	O
unidentifiability	O
in	O
mixture	B
models	O
using	O
mcmc	B
the	O
approach	O
we	O
will	O
adopt	O
in	O
this	O
chapter	O
is	O
much	O
simpler	O
we	O
just	O
compute	O
a	O
single	O
say	O
approximate	O
since	O
finding	O
local	O
mode	B
i	O
e	O
we	O
perform	O
approximate	O
map	O
estimation	O
the	O
globally	O
optimal	O
mle	B
and	O
hence	O
map	B
estimate	I
is	O
np-hard	B
at	O
least	O
for	O
mixture	B
models	O
et	O
al	O
this	O
is	O
by	O
far	O
the	O
most	O
common	O
approach	O
because	O
of	O
its	O
simplicity	O
it	O
is	O
also	O
a	O
reasonable	O
approximation	O
at	O
least	O
if	O
the	O
sample	O
size	O
is	O
sufficiently	O
large	O
to	O
see	O
why	O
consider	O
figure	O
we	O
see	O
that	O
there	O
are	O
n	O
latent	B
variables	O
each	O
of	O
which	O
gets	O
to	O
see	O
one	O
data	O
point	O
each	O
however	O
there	O
are	O
only	O
two	O
latent	B
parameters	O
each	O
of	O
which	O
gets	O
to	O
see	O
n	O
data	O
points	O
so	O
the	O
posterior	O
uncertainty	B
about	O
the	O
parameters	O
is	O
typically	O
much	O
less	O
than	O
the	O
posterior	O
uncertainty	B
about	O
the	O
latent	B
variables	O
this	O
justifies	O
the	O
common	O
strategy	O
of	O
computing	O
pzixi	O
but	O
not	O
bothering	O
to	O
compute	O
p	O
in	O
section	O
we	O
will	O
study	O
hierarchical	B
bayesian	B
models	I
which	O
essentially	O
put	O
structure	O
on	O
top	O
of	O
the	O
parameters	O
in	O
such	O
models	O
it	O
is	O
important	O
to	O
model	O
p	O
so	O
that	O
the	O
parameters	O
can	O
send	O
information	B
between	O
themselves	O
if	O
we	O
used	O
a	O
point	B
estimate	I
this	O
would	O
not	O
be	O
possible	O
computing	O
a	O
map	B
estimate	I
is	O
non-convex	O
in	O
the	O
previous	O
sections	O
we	O
have	O
argued	O
rather	O
heuristically	O
that	O
the	O
likelihood	B
function	O
has	O
multiple	O
modes	O
and	O
hence	O
that	O
finding	O
an	O
map	O
or	O
ml	O
estimate	O
will	O
be	O
hard	O
in	O
this	O
section	O
we	O
show	O
this	O
result	O
by	O
more	O
algebraic	O
means	O
which	O
sheds	O
some	O
additional	O
insight	O
into	O
the	O
problem	O
our	O
presentation	O
is	O
based	O
in	O
part	O
on	O
consider	O
the	O
log-likelihood	O
for	O
an	O
lvm	B
pxi	O
zi	O
log	O
pd	O
log	O
i	O
zi	O
unfortunately	O
this	O
objective	O
is	O
hard	O
to	O
maximize	O
since	O
we	O
cannot	O
push	O
the	O
log	O
inside	O
the	O
sum	O
this	O
precludes	O
certain	O
algebraic	O
simplications	O
but	O
does	O
not	O
prove	O
the	O
problem	O
is	O
hard	O
now	O
suppose	O
the	O
joint	B
probability	I
distribution	I
pzi	O
xi	O
is	O
in	O
the	O
exponential	B
family	B
which	O
means	O
it	O
can	O
be	O
written	O
as	O
follows	O
px	O
z	O
z	O
exp	O
t	O
z	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
where	O
z	O
are	O
the	O
sufficient	B
statistics	I
and	O
z	O
is	O
the	O
normalization	O
constant	O
section	O
for	O
more	O
details	O
it	O
can	O
be	O
shown	O
that	O
the	O
mvn	B
is	O
in	O
the	O
exponential	B
family	B
as	O
are	O
nearly	O
all	O
of	O
the	O
distributions	O
we	O
have	O
encountered	O
so	O
far	O
including	O
dirichlet	B
multinomial	B
gamma	B
wishart	B
etc	O
student	O
distribution	O
is	O
a	O
notable	O
exception	O
furthermore	O
mixtures	O
of	O
exponential	O
families	O
are	O
also	O
in	O
the	O
exponential	B
family	B
providing	O
the	O
mixing	O
indicator	O
variables	O
are	O
observed	O
with	O
this	O
assumption	O
the	O
complete	B
data	I
log	I
likelihood	B
can	O
be	O
written	O
as	O
follows	O
log	O
pxi	O
zi	O
t	O
zi	O
n	O
z	O
i	O
i	O
the	O
first	O
term	O
is	O
clearly	O
linear	O
in	O
one	O
can	O
show	O
that	O
z	O
is	O
a	O
convex	B
function	O
and	O
vandenberghe	O
so	O
the	O
overall	O
objective	O
is	O
concave	B
to	O
the	O
minus	O
sign	O
and	O
hence	O
has	O
a	O
unique	O
maximum	O
now	O
consider	O
what	O
happens	O
when	O
we	O
have	O
missing	B
data	I
the	O
observed	B
data	I
log	I
likelihood	B
is	O
given	O
by	O
log	O
i	O
zi	O
pxi	O
zi	O
log	O
i	O
zi	O
e	O
t	O
n	O
log	O
z	O
one	O
can	O
show	O
that	O
the	O
log-sum-exp	B
function	O
is	O
convex	B
and	O
vandenberghe	O
and	O
we	O
know	O
that	O
z	O
is	O
convex	B
however	O
the	O
difference	O
of	O
two	O
convex	B
functions	O
is	O
not	O
in	O
general	O
convex	B
so	O
the	O
objective	O
is	O
neither	O
convex	B
nor	O
concave	B
and	O
has	O
local	O
optima	O
the	O
disadvantage	O
of	O
non-convex	O
functions	O
is	O
that	O
it	O
is	O
usually	O
hard	O
to	O
find	O
their	O
global	O
optimum	O
most	O
optimization	B
algorithms	O
will	O
only	O
find	O
a	O
local	O
optimum	O
which	O
one	O
they	O
find	O
depends	O
on	O
where	O
they	O
start	O
there	O
are	O
some	O
algorithms	O
such	O
as	O
simulated	B
annealing	B
or	O
genetic	B
algorithms	I
that	O
claim	O
to	O
always	O
find	O
the	O
global	O
optimum	O
but	O
this	O
is	O
only	O
under	O
unrealistic	O
assumptions	O
if	O
they	O
are	O
allowed	O
to	O
be	O
cooled	O
infinitely	O
slowly	O
or	O
allowed	O
to	O
run	O
infinitely	O
long	O
in	O
practice	O
we	O
will	O
run	O
a	O
local	O
optimizer	O
perhaps	O
using	O
multiple	B
random	I
restarts	I
to	O
increase	O
out	O
chance	O
of	O
finding	O
a	O
good	O
local	O
optimum	O
of	O
course	O
careful	O
initialization	O
can	O
help	O
a	O
lot	O
too	O
we	O
give	O
examples	O
of	O
how	O
to	O
do	O
this	O
on	O
a	O
case-by-case	O
basis	O
note	O
that	O
a	O
convex	B
method	O
for	O
fitting	O
mixtures	O
of	O
gaussians	O
has	O
been	O
proposed	O
the	O
idea	O
is	O
to	O
assign	O
one	O
cluster	O
per	O
data	O
point	O
and	O
select	O
from	O
amongst	O
them	O
using	O
a	O
convex	B
penalty	O
rather	O
than	O
trying	O
to	O
optimize	O
the	O
locations	O
of	O
the	O
cluster	O
centers	O
see	O
and	O
golland	O
for	O
details	O
this	O
is	O
essentially	O
an	O
unsupervised	O
version	O
of	O
the	O
approach	O
used	O
in	O
sparse	B
kernel	B
logistic	B
regression	B
which	O
we	O
will	O
discuss	O
in	O
section	O
note	O
however	O
that	O
the	O
penalty	O
although	O
convex	B
is	O
not	O
necessarily	O
a	O
good	O
way	O
to	O
promote	O
sparsity	B
as	O
discussed	O
in	O
chapter	O
in	O
fact	O
as	O
we	O
will	O
see	O
in	O
that	O
chapter	O
some	O
of	O
the	O
best	O
sparsity-promoting	O
methods	O
use	O
non-convex	O
penalties	O
and	O
use	O
em	B
to	O
optimie	O
them	O
the	O
moral	O
of	O
the	O
story	O
is	O
do	O
not	O
be	O
afraid	O
of	O
non-convexity	O
the	O
em	B
algorithm	O
for	O
many	O
models	O
in	O
machine	B
learning	B
and	O
statistics	O
computing	O
the	O
ml	O
or	O
map	O
parameter	B
estimate	O
is	O
easy	O
provided	O
we	O
observe	O
all	O
the	O
values	O
of	O
all	O
the	O
relevant	O
random	O
variables	O
i	O
e	O
if	O
the	O
em	B
algorithm	O
model	O
mix	O
gaussians	O
mix	O
experts	O
factor	B
analysis	I
student	B
t	I
probit	B
regression	B
dgm	B
with	O
hidden	B
variables	I
mvn	B
with	O
missing	B
data	I
hmms	B
shrinkage	B
estimates	O
of	O
gaussian	B
means	O
section	O
exercise	O
table	O
some	O
models	O
discussed	O
in	O
this	O
book	O
for	O
which	O
em	B
can	O
be	O
easily	O
applied	O
to	O
find	O
the	O
ml	O
map	O
parameter	B
estimate	O
we	O
have	O
complete	B
data	I
however	O
if	O
we	O
have	O
missing	B
data	I
andor	O
latent	B
variables	O
then	O
computing	O
the	O
mlmap	O
estimate	O
becomes	O
hard	O
one	O
approach	O
is	O
to	O
use	O
a	O
generic	O
gradient-based	O
optimizer	O
to	O
find	O
a	O
local	O
minimum	O
of	O
the	O
negative	B
log	I
likelihood	B
or	O
nll	B
given	O
by	O
nll	B
n	O
log	O
pd	O
however	O
we	O
often	O
have	O
to	O
enforce	O
constraints	O
such	O
as	O
the	O
fact	O
that	O
covariance	B
matrices	O
must	O
be	O
positive	O
definite	O
mixing	B
weights	I
must	O
sum	O
to	O
one	O
etc	O
which	O
can	O
be	O
tricky	O
exercise	O
in	O
such	O
cases	O
it	O
is	O
often	O
much	O
simpler	O
not	O
always	O
faster	O
to	O
use	O
an	O
algorithm	O
called	O
expectation	B
maximization	I
or	O
em	B
for	O
short	O
et	O
al	O
meng	O
and	O
van	O
dyk	O
mclachlan	O
and	O
krishnan	O
this	O
is	O
a	O
simple	O
iterative	O
algorithm	O
often	O
with	O
closed-form	O
updates	O
at	O
each	O
step	O
furthermore	O
the	O
algorithm	O
automatically	O
enforce	O
the	O
required	O
constraints	O
em	B
exploits	O
the	O
fact	O
that	O
if	O
the	O
data	O
were	O
fully	O
observed	O
then	O
the	O
ml	O
map	B
estimate	I
would	O
be	O
easy	O
to	O
compute	O
in	O
particular	O
em	B
is	O
an	O
iterative	O
algorithm	O
which	O
alternates	O
between	O
inferring	O
the	O
missing	B
values	O
given	O
the	O
parameters	O
step	O
and	O
then	O
optimizing	O
the	O
parameters	O
given	O
the	O
filled	O
in	O
data	O
step	O
we	O
give	O
the	O
details	O
below	O
followed	O
by	O
several	O
examples	O
we	O
end	O
with	O
a	O
more	O
theoretical	O
discussion	O
where	O
we	O
put	O
the	O
algorithm	O
in	O
a	O
larger	O
context	O
see	O
table	O
for	O
a	O
summary	O
of	O
the	O
applications	O
of	O
em	B
in	O
this	O
book	O
basic	O
idea	O
let	O
xi	O
be	O
the	O
visible	B
or	O
observed	O
variables	O
in	O
case	O
i	O
and	O
let	O
zi	O
be	O
the	O
hidden	B
or	O
missing	B
variables	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
log	O
likelihood	B
of	O
the	O
observed	O
data	O
log	O
pxi	O
pxi	O
zi	O
log	O
zi	O
unfortunately	O
this	O
is	O
hard	O
to	O
optimize	O
since	O
the	O
log	O
cannot	O
be	O
pushed	O
inside	O
the	O
sum	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
em	B
gets	O
around	O
this	O
problem	O
as	O
follows	O
define	O
the	O
complete	B
data	I
log	I
likelihood	B
to	O
be	O
log	O
pxi	O
zi	O
this	O
cannot	O
be	O
computed	O
since	O
zi	O
is	O
unknown	B
so	O
let	O
us	O
define	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
as	O
follows	O
q	O
t	O
e	O
t	O
where	O
t	O
is	O
the	O
current	O
iteration	O
number	O
q	O
is	O
called	O
the	O
auxiliary	B
function	I
the	O
expectation	O
is	O
taken	O
wrt	O
the	O
old	O
parameters	O
t	O
and	O
the	O
observed	O
data	O
d	O
the	O
goal	O
of	O
the	O
e	O
stepis	O
to	O
compute	O
q	O
t	O
or	O
rather	O
the	O
terms	O
inside	O
of	O
it	O
which	O
the	O
mle	B
depends	O
on	O
these	O
are	O
known	O
as	O
the	O
expected	B
sufficient	B
statistics	I
or	O
ess	B
in	O
the	O
m	B
step	I
we	O
optimize	O
the	O
q	O
function	O
wrt	O
t	O
arg	O
max	O
q	O
t	O
to	O
perform	O
map	O
estimation	O
we	O
modify	O
the	O
m	B
step	I
as	O
follows	O
t	O
argmax	O
q	O
t	O
log	O
p	O
the	O
e	B
step	I
remains	O
unchanged	O
in	O
section	O
we	O
show	O
that	O
the	O
em	B
algorithm	O
monotonically	O
increases	O
the	O
log	O
likelihood	B
of	O
the	O
observed	O
data	O
the	O
log	O
prior	O
if	O
doing	O
map	O
estimation	O
or	O
it	O
stays	O
the	O
same	O
so	O
if	O
the	O
objective	O
ever	O
goes	O
down	O
there	O
must	O
be	O
a	O
bug	O
in	O
our	O
math	O
or	O
our	O
code	O
is	O
a	O
surprisingly	O
useful	O
debugging	O
tool	O
below	O
we	O
explain	O
how	O
to	O
perform	O
the	O
e	O
and	O
m	O
steps	O
for	O
several	O
simple	O
models	O
that	O
should	O
make	O
things	O
clearer	O
em	B
for	O
gmms	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
fit	O
a	O
mixture	B
of	I
gaussians	I
using	O
em	B
fitting	O
other	O
kinds	O
of	O
mixture	B
models	O
requires	O
a	O
straightforward	O
modification	O
see	O
exercise	O
we	O
assume	O
the	O
number	O
of	O
mixture	B
components	O
k	O
is	O
known	O
section	O
for	O
discussion	O
of	O
this	O
point	O
the	O
em	B
algorithm	O
auxiliary	B
function	I
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
is	O
given	O
by	O
i	O
q	O
e	O
e	O
k	O
k	O
i	O
i	O
i	O
log	O
pxi	O
zi	O
kpxi	O
kizik	O
log	O
e	O
k	O
log	O
kpxi	O
k	O
pzi	O
kxi	O
t	O
log	O
kpxi	O
k	O
rik	O
log	O
pxi	O
k	O
rik	O
log	O
k	O
where	O
rik	O
pzi	O
kxi	O
is	O
the	O
responsibility	B
that	O
cluster	O
k	O
takes	O
for	O
data	O
point	O
i	O
this	O
is	O
computed	O
in	O
the	O
e	B
step	I
described	O
below	O
k	O
k	O
i	O
i	O
e	B
step	I
the	O
e	B
step	I
has	O
the	O
following	O
simple	O
form	O
which	O
is	O
the	O
same	O
for	O
any	O
mixture	B
model	I
rik	O
m	B
step	I
kpxi	O
pxi	O
k	O
in	O
the	O
m	B
step	I
we	O
optimize	O
q	O
wrt	O
and	O
the	O
k	O
for	O
we	O
obviously	O
have	O
i	O
n	O
rik	O
rk	O
n	O
k	O
where	O
rk	O
i	O
rik	O
is	O
the	O
weighted	O
number	O
of	O
points	O
assigned	O
to	O
cluster	O
k	O
to	O
derive	O
the	O
m	B
step	I
for	O
the	O
k	O
and	O
k	O
terms	O
we	O
look	O
at	O
the	O
parts	O
of	O
q	O
that	O
depend	O
on	O
k	O
and	O
k	O
we	O
see	O
that	O
the	O
result	O
is	O
k	O
k	O
rik	O
log	O
pxi	O
k	O
i	O
i	O
k	O
rik	O
log	O
k	O
kt	O
k	O
k	O
this	O
is	O
just	O
a	O
weighted	O
version	O
of	O
the	O
standard	O
problem	O
of	O
computing	O
the	O
mles	O
of	O
an	O
mvn	B
section	O
one	O
can	O
show	O
that	O
the	O
new	O
parameter	B
estimates	O
are	O
given	O
by	O
k	O
k	O
i	O
rikxi	O
rk	O
i	O
rikxi	O
kxi	O
kt	O
rk	O
i	O
rikxixt	O
i	O
rk	O
k	O
t	O
k	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
these	O
equations	O
make	O
intuitive	O
sense	O
the	O
mean	B
of	O
cluster	O
k	O
is	O
just	O
the	O
weighted	B
average	I
of	O
all	O
points	O
assigned	O
to	O
cluster	O
k	O
and	O
the	O
covariance	B
is	O
proportional	O
to	O
the	O
weighted	O
empirical	O
scatter	O
matrix	O
after	O
computing	O
the	O
new	O
estimates	O
we	O
set	O
t	O
k	O
k	O
k	O
for	O
k	O
and	O
go	O
to	O
the	O
next	O
e	B
step	I
example	O
an	O
example	O
of	O
the	O
algorithm	O
in	O
action	B
is	O
shown	O
in	O
figure	O
we	O
start	O
with	O
i	O
i	O
we	O
color	O
code	O
points	O
such	O
that	O
blue	O
points	O
come	O
from	O
cluster	O
and	O
red	O
points	O
from	O
cluster	O
more	O
precisely	O
we	O
set	O
the	O
color	O
to	O
colori	O
so	O
ambiguous	O
points	O
appear	O
purple	O
after	O
iterations	O
the	O
algorithm	O
has	O
converged	O
on	O
a	O
good	O
clustering	B
data	O
was	O
standardized	B
by	O
removing	O
the	O
mean	B
and	O
dividing	O
by	O
the	O
standard	B
deviation	I
before	O
processing	O
this	O
often	O
helps	O
convergence	O
k-means	B
algorithm	I
there	O
is	O
a	O
popular	O
variant	O
of	O
the	O
em	B
algorithm	O
for	O
gmms	O
known	O
as	O
the	O
k-means	B
algorithm	I
which	O
we	O
now	O
discuss	O
consider	O
a	O
gmm	B
in	O
which	O
we	O
make	O
the	O
following	O
assumptions	O
k	O
is	O
fixed	O
and	O
k	O
is	O
fixed	O
so	O
only	O
the	O
cluster	O
centers	O
k	O
r	O
d	O
have	O
to	O
be	O
estimated	O
now	O
consider	O
the	O
following	O
delta-function	O
approximation	O
to	O
the	O
posterior	O
computed	O
during	O
the	O
e	B
step	I
pzi	O
kxi	O
ik	O
z	O
i	O
where	O
zi	O
argmaxk	O
pzi	O
kxi	O
this	O
is	O
sometimes	O
called	O
hard	B
em	B
since	O
we	O
are	O
making	O
a	O
hard	O
assignment	O
of	O
points	O
to	O
clusters	B
since	O
we	O
assumed	O
an	O
equal	O
spherical	B
covariance	B
matrix	I
for	O
each	O
cluster	O
the	O
most	O
probable	O
cluster	O
for	O
xi	O
can	O
be	O
computed	O
by	O
finding	O
the	O
nearest	O
prototype	B
z	O
i	O
arg	O
min	O
k	O
hence	O
in	O
each	O
e	B
step	I
we	O
must	O
find	O
the	O
euclidean	B
distance	I
between	O
n	O
data	O
points	O
and	O
k	O
cluster	O
centers	O
which	O
takes	O
on	O
kd	O
time	O
however	O
this	O
can	O
be	O
sped	O
up	O
using	O
various	O
techniques	O
such	O
as	O
applying	O
the	O
triangle	B
inequality	I
to	O
avoid	O
some	O
redundant	O
computations	O
given	O
the	O
hard	O
cluster	O
assignments	O
the	O
m	B
step	I
updates	O
each	O
cluster	O
center	O
by	O
computing	O
the	O
mean	B
of	O
all	O
points	O
assigned	O
to	O
it	O
k	O
nk	O
xi	O
izik	O
see	O
algorithm	O
for	O
the	O
pseudo-code	O
the	O
em	B
algorithm	O
figure	O
illustration	O
of	O
the	O
em	B
for	O
a	O
gmm	B
applied	O
to	O
the	O
old	O
faithful	B
data	O
initial	O
values	O
of	O
the	O
parameters	O
posterior	O
responsibility	B
of	O
each	O
point	O
computed	O
in	O
the	O
first	O
e	B
step	I
the	O
degree	B
of	O
redness	O
indicates	O
the	O
degree	B
to	O
which	O
the	O
point	O
belongs	O
to	O
the	O
red	O
cluster	O
and	O
similarly	O
for	O
blue	O
this	O
purple	O
points	O
have	O
a	O
roughly	O
uniform	O
posterior	O
over	O
clusters	B
we	O
show	O
the	O
updated	O
parameters	O
after	O
the	O
first	O
m	B
step	I
after	O
iterations	O
after	O
iterations	O
after	O
iterations	O
based	O
on	O
figure	O
figure	O
generated	O
by	O
mixgaussdemofaithful	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
algorithm	O
k-means	B
algorithm	I
initialize	O
mk	O
repeat	O
assign	O
each	O
data	O
point	O
to	O
its	O
closest	O
cluster	O
center	O
zi	O
arg	O
mink	O
update	O
each	O
cluster	O
center	O
by	O
computing	O
the	O
mean	B
of	O
all	O
points	O
assigned	O
to	O
it	O
k	O
nk	O
izik	O
xi	O
until	O
converged	O
figure	O
an	O
image	O
compressed	O
using	O
vector	B
quantization	I
with	O
a	O
codebook	B
of	O
size	O
k	O
k	O
k	O
figure	O
generated	O
by	O
vqdemo	O
vector	B
quantization	I
since	O
k-means	O
is	O
not	O
a	O
proper	O
em	B
algorithm	O
it	O
is	O
not	O
maximizing	O
likelihood	B
instead	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
greedy	O
algorithm	O
for	O
approximately	O
minimizing	O
a	O
loss	B
function	I
related	O
to	O
data	B
compression	I
as	O
we	O
now	O
explain	O
suppose	O
we	O
want	O
to	O
perform	O
lossy	B
compression	I
of	O
some	O
real-valued	O
vectors	O
xi	O
r	O
d	O
a	O
very	O
simple	O
approach	O
to	O
this	O
is	O
to	O
use	O
vector	B
quantization	I
or	O
vq	B
the	O
basic	O
idea	O
is	O
to	O
replace	O
each	O
real-valued	O
vector	O
xi	O
r	O
d	O
with	O
a	O
discrete	B
symbol	O
zi	O
k	O
which	O
is	O
an	O
index	O
into	O
a	O
codebook	B
of	O
k	O
prototypes	O
k	O
r	O
d	O
each	O
data	O
vector	O
is	O
encoded	O
by	O
using	O
the	O
index	O
of	O
the	O
most	O
similar	B
prototype	B
where	O
similarity	O
is	O
measured	O
in	O
terms	O
of	O
euclidean	B
distance	I
encodexi	O
arg	O
min	O
k	O
we	O
can	O
define	O
a	O
cost	O
function	O
that	O
measures	O
the	O
quality	O
of	O
a	O
codebook	B
by	O
computing	O
the	O
reconstruction	B
error	I
or	O
distortion	B
it	O
induces	O
j	O
zk	O
x	O
n	O
n	O
zi	O
where	O
decodek	O
k	O
the	O
k-means	B
algorithm	I
can	O
be	O
thought	O
of	O
as	O
a	O
simple	O
iterative	O
scheme	O
for	O
minimizing	O
this	O
objective	O
of	O
course	O
we	O
can	O
achieve	O
zero	O
distortion	B
if	O
we	O
assign	O
one	O
prototype	B
to	O
every	O
data	O
vector	O
but	O
that	O
takes	O
on	O
dc	O
space	O
where	O
n	O
is	O
the	O
number	O
of	O
real-valued	O
data	O
vectors	O
each	O
of	O
the	O
em	B
algorithm	O
length	O
d	O
and	O
c	O
is	O
the	O
number	O
of	O
bits	B
needed	O
to	O
represent	O
a	O
real-valued	O
scalar	O
quantization	O
accuracy	O
however	O
in	O
many	O
data	O
sets	O
we	O
see	O
similar	B
vectors	O
repeatedly	O
so	O
rather	O
than	O
storing	O
them	O
many	O
times	O
we	O
can	O
store	O
them	O
once	O
and	O
then	O
create	O
pointers	O
to	O
them	O
hence	O
we	O
can	O
reduce	O
the	O
space	O
requirement	O
to	O
on	O
k	O
kdc	O
the	O
on	O
k	O
term	O
arises	O
because	O
each	O
of	O
the	O
n	O
data	O
vectors	O
needs	O
to	O
specify	O
which	O
of	O
the	O
k	O
codewords	O
it	O
is	O
using	O
pointers	O
and	O
the	O
okdc	O
term	O
arises	O
because	O
we	O
have	O
to	O
store	O
each	O
codebook	B
entry	O
each	O
of	O
which	O
is	O
a	O
d-dimensional	O
vector	O
typically	O
the	O
first	O
term	O
dominates	B
the	O
second	O
so	O
we	O
can	O
approximate	O
the	O
rate	B
of	O
the	O
encoding	O
scheme	O
of	O
bits	B
needed	O
per	O
object	O
as	O
k	O
which	O
is	O
typically	O
much	O
less	O
than	O
odc	O
one	O
application	O
of	O
vq	B
is	O
to	O
image	B
compression	I
consider	O
the	O
n	O
pixel	O
image	O
in	O
figure	O
this	O
is	O
gray-scale	O
so	O
d	O
if	O
we	O
use	O
one	O
byte	O
to	O
represent	O
each	O
pixel	O
gray-scale	O
intensity	O
of	O
to	O
then	O
c	O
so	O
we	O
need	O
n	O
c	O
bits	B
to	O
represent	O
the	O
image	O
for	O
the	O
compressed	O
image	O
we	O
need	O
n	O
k	O
kc	O
bits	B
for	O
k	O
this	O
is	O
about	O
a	O
factor	B
of	O
compression	O
for	O
k	O
this	O
is	O
about	O
a	O
factor	B
of	O
compression	O
at	O
negligible	O
perceptual	O
loss	B
figure	O
greater	O
compression	O
could	O
be	O
achieved	O
if	O
we	O
modelled	O
spatial	O
correlation	O
between	O
the	O
pixels	O
e	O
g	O
if	O
we	O
encoded	O
blocks	O
used	O
by	O
jpeg	O
this	O
is	O
because	O
the	O
residual	B
errors	O
from	O
the	O
model	O
s	O
predictions	O
would	O
be	O
smaller	O
and	O
would	O
take	O
fewer	O
bits	B
to	O
encode	O
initialization	O
and	O
avoiding	O
local	O
minima	O
both	O
k-means	O
and	O
em	B
need	O
to	O
be	O
initialized	O
it	O
is	O
common	O
to	O
pick	O
k	O
data	O
points	O
at	O
random	O
and	O
to	O
make	O
these	O
be	O
the	O
initial	O
cluster	O
centers	O
or	O
we	O
can	O
pick	O
the	O
centers	O
sequentially	O
so	O
as	O
to	O
try	O
to	O
cover	O
the	O
data	O
that	O
is	O
we	O
pick	O
the	O
initial	O
point	O
uniformly	O
at	O
random	O
then	O
each	O
subsequent	O
point	O
is	O
picked	O
from	O
the	O
remaining	O
points	O
with	O
probability	O
proportional	O
to	O
its	O
squared	O
distance	O
to	O
the	O
points	O
s	O
closest	O
cluster	O
center	O
this	O
is	O
known	O
as	O
farthest	B
point	I
clustering	B
or	O
k-means	O
and	O
vassilvitskii	O
bahmani	O
et	O
al	O
surprisingly	O
this	O
simple	O
trick	O
can	O
be	O
shown	O
to	O
guarantee	O
that	O
the	O
distortion	B
is	O
never	O
more	O
than	O
olog	O
k	O
worse	O
than	O
optimal	O
and	O
vassilvitskii	O
an	O
heuristic	O
that	O
is	O
commonly	O
used	O
in	O
the	O
speech	B
recognition	I
community	O
is	O
to	O
incrementally	O
grow	O
gmms	O
we	O
initially	O
give	O
each	O
cluster	O
a	O
score	O
based	O
on	O
its	O
mixture	B
weight	O
after	O
each	O
round	O
of	O
training	O
we	O
consider	O
splitting	O
the	O
cluster	O
with	O
the	O
highest	O
score	O
into	O
two	O
with	O
the	O
new	O
centroids	B
being	O
random	O
perturbations	O
of	O
the	O
original	O
centroid	B
and	O
the	O
new	O
scores	B
being	O
half	O
of	O
the	O
old	O
scores	B
if	O
a	O
new	O
cluster	O
has	O
too	O
small	O
a	O
score	O
or	O
too	O
narrow	O
a	O
variance	B
it	O
is	O
removed	O
we	O
continue	O
in	O
this	O
way	O
until	O
the	O
desired	O
number	O
of	O
clusters	B
is	O
reached	O
see	O
and	O
jain	O
for	O
a	O
similar	B
incremental	O
approach	O
map	O
estimation	O
as	O
usual	O
the	O
mle	B
may	O
overfit	O
the	O
overfitting	O
problem	O
is	O
particularly	O
severe	O
in	O
the	O
case	O
of	O
gmms	O
to	O
understand	O
the	O
problem	O
suppose	O
for	O
simplicity	O
that	O
k	O
ki	O
and	O
that	O
k	O
it	O
is	O
possible	O
to	O
get	O
an	O
infinite	O
likelihood	B
by	O
assigning	O
one	O
of	O
the	O
centers	O
say	O
to	O
a	O
single	O
data	O
point	O
say	O
since	O
then	O
the	O
term	O
makes	O
the	O
following	O
contribution	O
to	O
the	O
likelihood	B
n	O
x	O
p	O
x	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
s	O
l	O
i	O
a	O
f	O
m	O
m	O
g	O
r	O
o	O
f	O
m	O
e	O
s	O
e	O
m	O
i	O
t	O
f	O
o	O
n	O
o	O
i	O
t	O
c	O
a	O
r	O
f	O
mle	B
map	O
dimensionality	O
figure	O
illustration	O
of	O
how	O
singularities	O
can	O
arise	O
in	O
the	O
likelihood	B
function	O
of	O
gmms	O
based	O
on	O
figure	O
figure	O
generated	O
by	O
mixgausssingularity	O
illustration	O
of	O
the	O
benefit	O
of	O
map	O
estimation	O
vs	O
ml	O
estimation	O
when	O
fitting	O
a	O
gaussian	B
mixture	B
model	I
we	O
plot	O
the	O
fraction	O
of	O
times	O
of	O
random	O
trials	O
each	O
method	O
encounters	O
numerical	O
problems	O
vs	O
the	O
dimensionality	O
of	O
the	O
problem	O
for	O
n	O
samples	B
solid	O
red	O
curve	O
mle	B
dotted	O
black	O
curve	O
map	O
figure	O
generated	O
by	O
mixgaussmlvsmap	O
hence	O
we	O
can	O
drive	O
this	O
term	O
to	O
infinity	O
by	O
letting	O
as	O
shown	O
in	O
figure	O
we	O
will	O
call	O
this	O
the	O
collapsing	O
variance	B
problem	O
an	O
easy	O
solution	O
to	O
this	O
is	O
to	O
perform	O
map	O
estimation	O
the	O
new	O
auxiliary	B
function	I
is	O
the	O
expected	O
complete	B
data	I
log-likelihood	O
plus	O
the	O
log	O
prior	O
old	O
rik	O
log	O
ik	O
i	O
k	O
i	O
k	O
rik	O
log	O
pxi	O
k	O
log	O
p	O
k	O
log	O
p	O
note	O
that	O
the	O
e	B
step	I
remains	O
unchanged	O
but	O
the	O
m	B
step	I
needs	O
to	O
be	O
modified	O
as	O
we	O
now	O
explain	O
for	O
the	O
prior	O
on	O
the	O
mixture	B
weights	O
it	O
is	O
natural	O
to	O
use	O
a	O
dirichlet	B
prior	O
dir	O
since	O
this	O
is	O
conjugate	O
to	O
the	O
categorical	B
distribution	O
the	O
map	B
estimate	I
is	O
given	O
by	O
k	O
rk	O
k	O
k	O
k	O
k	O
n	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
k	O
this	O
reduces	O
to	O
equation	O
the	O
prior	O
on	O
the	O
parameters	O
of	O
the	O
class	O
conditional	O
densities	O
p	O
k	O
depends	O
on	O
the	O
form	O
of	O
the	O
class	O
conditional	O
densities	O
we	O
discuss	O
the	O
case	O
of	O
gmms	O
below	O
and	O
leave	O
map	O
estimation	O
for	O
mixtures	O
of	O
bernoullis	O
to	O
exercise	O
for	O
simplicity	O
let	O
us	O
consider	O
a	O
conjugate	B
prior	I
of	O
the	O
form	O
p	O
k	O
k	O
niw	B
k	O
the	O
em	B
algorithm	O
from	O
section	O
the	O
map	B
estimate	I
is	O
given	O
by	O
k	O
xk	O
k	O
sk	O
rkxk	O
rk	O
i	O
rikxi	O
rk	O
sk	O
rk	O
d	O
rikxi	O
xkxi	O
xkt	O
i	O
we	O
now	O
illustrate	O
the	O
benefits	O
of	O
using	O
map	O
estimation	O
instead	O
of	O
ml	O
estimation	O
in	O
the	O
context	O
of	O
gmms	O
we	O
apply	O
em	B
to	O
some	O
synthetic	O
data	O
in	O
d	O
dimensions	O
using	O
either	O
ml	O
or	O
map	O
estimation	O
we	O
count	O
the	O
trial	O
as	O
a	O
failure	O
if	O
there	O
are	O
numerical	O
issues	O
involving	O
singular	O
matrices	O
for	O
each	O
dimensionality	O
we	O
conduct	O
random	O
trials	O
the	O
results	O
are	O
illustrated	O
in	O
figure	O
using	O
n	O
we	O
see	O
that	O
as	O
soon	O
as	O
d	O
becomes	O
even	O
moderately	O
large	O
ml	O
estimation	O
crashes	O
and	O
burns	O
whereas	O
map	O
estimation	O
never	O
encounters	O
numerical	O
problems	O
when	O
using	O
map	O
estimation	O
we	O
need	O
to	O
specify	O
the	O
hyper-parameters	B
here	O
we	O
mention	O
some	O
simple	O
heuristics	B
for	O
setting	O
them	O
and	O
raftery	O
we	O
can	O
set	O
so	O
that	O
the	O
k	O
are	O
unregularized	O
since	O
the	O
numerical	O
problems	O
only	O
arise	O
from	O
k	O
in	O
this	O
case	O
the	O
map	O
estimates	O
simplify	O
to	O
k	O
xk	O
and	O
k	O
which	O
is	O
not	O
quite	O
so	O
scary-looking	O
now	O
we	O
discuss	O
how	O
to	O
set	O
one	O
possibility	O
is	O
to	O
use	O
d	O
k	O
is	O
the	O
pooled	B
variance	B
for	O
dimension	O
j	O
where	O
sj	O
reason	O
term	O
is	O
that	O
the	O
resulting	O
volume	O
of	O
each	O
ellipsoid	O
is	O
then	O
given	O
by	O
for	O
the	O
d	O
the	O
parameter	B
controls	O
how	O
strongly	O
we	O
believe	O
this	O
prior	O
the	O
weakest	O
prior	O
we	O
can	O
use	O
while	O
still	O
being	O
proper	O
is	O
to	O
set	O
d	O
so	O
this	O
is	O
a	O
common	O
choice	O
em	B
for	O
mixture	B
of	I
experts	I
we	O
can	O
fit	O
a	O
mixture	B
of	I
experts	I
model	O
using	O
em	B
in	O
a	O
straightforward	O
manner	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
is	O
given	O
by	O
q	O
old	O
rik	O
log	O
ikn	O
k	O
xi	O
k	O
ik	O
svt	O
xik	O
rik	O
old	O
ik	O
n	O
i	O
wold	O
k	O
old	O
k	O
so	O
the	O
e	B
step	I
is	O
the	O
same	O
as	O
in	O
a	O
standard	O
mixture	B
model	I
except	O
we	O
have	O
to	O
replace	O
k	O
with	O
ik	O
when	O
computing	O
rik	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
in	O
the	O
m	B
step	I
we	O
need	O
to	O
maximize	O
q	O
old	O
wrt	O
wk	O
k	O
and	O
v	O
for	O
the	O
regression	B
parameters	O
for	O
model	O
k	O
the	O
objective	O
has	O
the	O
form	O
q	O
k	O
old	O
rik	O
wt	O
k	O
xi	O
k	O
if	O
rik	O
is	O
we	O
recognize	O
this	O
as	O
a	O
weighted	B
least	B
squares	I
problem	I
which	O
makes	O
intuitive	O
sense	O
small	O
then	O
data	O
point	O
i	O
will	O
be	O
downweighted	O
when	O
estimating	O
model	O
k	O
s	O
parameters	O
from	O
section	O
we	O
can	O
immediately	O
write	O
down	O
the	O
mle	B
as	O
wk	O
rkx	O
rky	O
where	O
rk	O
diagrk	O
the	O
mle	B
for	O
the	O
variance	B
is	O
given	O
by	O
rikyi	O
wt	O
rik	O
k	O
k	O
we	O
replace	O
the	O
estimate	O
of	O
the	O
unconditional	O
mixing	B
weights	I
with	O
the	O
estimate	O
of	O
the	O
gating	O
parameters	O
v	O
the	O
objective	O
has	O
the	O
form	O
rik	O
log	O
ik	O
i	O
k	O
we	O
recognize	O
this	O
as	O
equivalent	O
to	O
the	O
log-likelihood	O
for	O
multinomial	B
logistic	B
regression	B
in	O
equation	O
except	O
we	O
replace	O
the	O
hard	O
encoding	O
yi	O
with	O
the	O
soft	O
encoding	O
ri	O
thus	O
we	O
can	O
estimate	O
v	O
by	O
fitting	O
a	O
logistic	B
regression	B
model	O
to	O
soft	O
target	O
labels	O
em	B
for	O
dgms	O
with	O
hidden	B
variables	I
we	O
can	O
generalize	B
the	O
ideas	O
behind	O
em	B
for	O
mixtures	O
of	O
experts	O
to	O
compute	O
the	O
mle	B
or	O
map	B
estimate	I
for	O
an	O
arbitrary	O
dgm	B
we	O
could	O
use	O
gradient-based	O
methods	O
et	O
al	O
but	O
it	O
is	O
much	O
simpler	O
to	O
use	O
em	B
in	O
the	O
e	B
step	I
we	O
just	O
estimate	O
the	O
hidden	B
variables	I
and	O
in	O
the	O
m	B
step	I
we	O
will	O
compute	O
the	O
mle	B
using	O
these	O
filled-in	O
values	O
we	O
give	O
the	O
details	O
below	O
for	O
simplicity	O
of	O
presentation	O
we	O
will	O
assume	O
all	O
cpds	O
are	O
tabular	O
based	O
on	O
section	O
let	O
us	O
write	O
each	O
cpt	O
as	O
follows	O
pxitxipat	O
t	O
ixitixipatc	O
tck	O
the	O
log-likelihood	O
of	O
the	O
complete	B
data	I
is	O
given	O
by	O
log	O
pd	O
ntck	O
log	O
tck	O
where	O
ntck	O
complete	B
data	I
log-likelihood	O
has	O
the	O
form	O
e	O
pd	O
t	O
c	O
k	O
ixit	O
i	O
xipat	O
c	O
are	O
the	O
empirical	O
counts	O
hence	O
the	O
expected	O
n	O
tck	O
log	O
tck	O
the	O
em	B
algorithm	O
where	O
pxit	O
k	O
xipat	O
cdi	O
e	O
n	O
tck	O
ixit	O
i	O
xipat	O
c	O
where	O
di	O
are	O
all	O
the	O
visible	B
variables	I
in	O
case	O
i	O
i	O
the	O
quantity	O
pxit	O
xipatdi	O
is	O
known	O
as	O
a	O
family	B
marginal	I
and	O
can	O
be	O
computed	O
using	O
any	O
gm	B
inference	B
algorithm	O
the	O
n	O
tjk	O
are	O
the	O
expected	B
sufficient	B
statistics	I
and	O
constitute	O
the	O
output	O
of	O
the	O
e	B
step	I
given	O
these	O
ess	B
the	O
m	B
step	I
has	O
the	O
simple	O
form	O
tck	O
n	O
n	O
k	O
tjk	O
this	O
can	O
be	O
proved	O
by	O
adding	O
lagrange	B
multipliers	I
enforce	O
the	O
constraint	O
to	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
and	O
then	O
optimizing	O
each	O
parameter	B
vector	O
tc	O
separately	O
we	O
can	O
modify	O
this	O
to	O
perform	O
map	O
estimation	O
with	O
a	O
dirichlet	B
prior	O
by	O
simply	O
adding	O
pseudo	B
counts	I
to	O
the	O
expected	O
counts	O
em	B
for	O
the	O
student	O
distribution	O
one	O
problem	O
with	O
the	O
gaussian	B
distribution	O
is	O
that	O
it	O
is	O
sensitive	O
to	O
outliers	B
since	O
the	O
logprobability	O
only	O
decays	O
quadratically	O
with	O
distance	O
from	O
the	O
center	O
a	O
more	O
robust	B
alternative	O
is	O
the	O
student	B
t	I
distribution	I
as	O
discussed	O
in	O
section	O
unlike	O
the	O
case	O
of	O
a	O
gaussian	B
there	O
is	O
no	O
closed	O
form	O
formula	O
for	O
the	O
mle	B
of	O
a	O
student	O
even	O
if	O
we	O
have	O
no	O
missing	B
data	I
so	O
we	O
must	O
resort	O
to	O
iterative	O
optimization	B
methods	O
the	O
easiest	O
one	O
to	O
use	O
is	O
em	B
since	O
it	O
automatically	O
enforces	O
the	O
constraints	O
that	O
is	O
positive	O
and	O
that	O
is	O
symmetric	B
positive	O
definite	O
in	O
addition	O
the	O
resulting	O
algorithm	O
turns	O
out	O
to	O
have	O
a	O
simple	O
intuitive	O
form	O
as	O
we	O
see	O
below	O
at	O
first	O
blush	O
it	O
might	O
not	O
be	O
apparent	O
why	O
em	B
can	O
be	O
used	O
since	O
there	O
is	O
no	O
missing	B
data	I
the	O
key	O
idea	O
is	O
to	O
introduce	O
an	O
artificial	O
hidden	B
or	O
auxiliary	O
variable	O
in	O
order	O
to	O
simplify	O
the	O
algorithm	O
in	O
particular	O
we	O
will	O
exploit	O
the	O
fact	O
that	O
a	O
student	O
distribution	O
can	O
be	O
written	O
as	O
a	O
gaussian	B
scale	I
mixture	B
t	O
n	O
exercise	O
for	O
a	O
proof	O
of	O
this	O
in	O
the	O
case	O
this	O
can	O
be	O
thought	O
of	O
as	O
an	O
infinite	O
mixture	B
of	I
gaussians	I
each	O
one	O
with	O
a	O
slightly	O
different	O
covariance	B
matrix	I
treating	O
the	O
zi	O
as	O
missing	B
data	I
we	O
can	O
write	O
the	O
complete	B
data	I
log	I
likelihood	B
as	O
n	O
log	O
gazi	O
d	O
zi	O
zi	O
d	O
log	O
zi	O
log	O
zi	O
i	O
log	O
log	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
where	O
we	O
have	O
defined	O
the	O
mahalanobis	B
distance	I
to	O
be	O
i	O
we	O
can	O
partition	O
this	O
into	O
two	O
terms	O
one	O
involving	O
and	O
and	O
the	O
other	O
involving	O
we	O
have	O
dropping	O
irrelevant	O
constants	O
ln	O
n	O
g	O
n	O
log	O
lg	O
n	O
log	O
zi	O
i	O
n	O
log	O
zi	O
zi	O
em	B
with	O
known	O
let	O
us	O
first	O
derive	O
the	O
algorithm	O
with	O
assumed	O
known	O
for	O
simplicity	O
in	O
this	O
case	O
we	O
can	O
ignore	O
the	O
lg	O
term	O
so	O
we	O
only	O
need	O
to	O
figure	O
out	O
how	O
to	O
compute	O
e	O
wrt	O
the	O
old	O
parameters	O
now	O
if	O
zi	O
gaa	O
b	O
then	O
e	O
ab	O
hence	O
the	O
e	B
step	I
at	O
iteration	O
t	O
is	O
from	O
section	O
we	O
have	O
pzixi	O
gazi	O
d	O
i	O
i	O
e	O
zt	O
zixi	O
d	O
i	O
the	O
m	B
step	I
is	O
obtained	O
by	O
maximizing	O
e	O
to	O
yield	O
i	O
zt	O
i	O
xi	O
i	O
zt	O
n	O
i	O
n	O
i	O
i	O
zt	O
zt	O
i	O
xixt	O
i	O
i	O
zt	O
i	O
these	O
results	O
are	O
quite	O
intuitive	O
the	O
quantity	O
zi	O
is	O
the	O
precision	B
of	O
measurement	O
i	O
so	O
if	O
it	O
is	O
small	O
the	O
corresponding	O
data	O
point	O
is	O
down-weighted	O
when	O
estimating	O
the	O
mean	B
and	O
covariance	B
this	O
is	O
how	O
the	O
student	O
achieves	O
robustness	B
to	O
outliers	B
em	B
with	O
unknown	B
to	O
compute	O
the	O
mle	B
for	O
the	O
degrees	B
of	I
freedom	I
we	O
first	O
need	O
to	O
compute	O
the	O
expectation	O
of	O
lg	O
which	O
involves	O
zi	O
and	O
log	O
zi	O
now	O
if	O
zi	O
gaa	O
b	O
then	O
one	O
can	O
show	O
that	O
i	O
e	O
log	O
zi	O
log	O
b	O
the	O
em	B
algorithm	O
errors	O
using	O
gauss	O
bankrupt	O
solvent	O
errors	O
using	O
student	O
bankrupt	O
solvent	O
figure	O
mixture	B
modeling	O
on	O
the	O
bankruptcy	O
data	O
set	O
left	O
gaussian	B
class	O
conditional	O
densities	O
right	O
student	O
class	O
conditional	O
densities	O
points	O
that	O
belong	O
to	O
class	O
are	O
shown	O
as	O
triangles	O
points	O
that	O
belong	O
to	O
class	O
are	O
shown	O
as	O
circles	O
the	O
estimated	O
labels	O
based	O
on	O
the	O
posterior	O
probability	O
of	O
belonging	O
to	O
each	O
mixture	B
component	O
are	O
computed	O
if	O
these	O
are	O
incorrect	O
the	O
point	O
is	O
colored	O
red	O
otherwise	O
it	O
is	O
colored	O
blue	O
data	O
is	O
in	O
black	O
figure	O
generated	O
by	O
mixstudentbankruptcydemo	O
where	O
d	O
dx	O
log	O
is	O
the	O
digamma	B
function	O
hence	O
from	O
equation	O
we	O
have	O
i	O
d	O
log	O
i	O
logzt	O
i	O
d	O
log	O
d	O
substituting	O
into	O
equation	O
we	O
have	O
e	O
n	O
log	O
n	O
log	O
i	O
i	O
zt	O
i	O
the	O
gradient	O
of	O
this	O
expression	O
is	O
equal	O
to	O
n	O
e	O
n	O
d	O
d	O
this	O
has	O
a	O
unique	O
solution	O
in	O
the	O
interval	O
which	O
can	O
be	O
found	O
using	O
a	O
constrained	O
i	O
zt	O
i	O
log	O
n	O
i	O
optimizer	O
performing	O
a	O
gradient-based	O
optimization	B
in	O
the	O
m	B
step	I
rather	O
than	O
a	O
closed-form	O
update	O
is	O
an	O
example	O
of	O
what	O
is	O
known	O
as	O
the	O
generalized	B
em	B
algorithm	O
one	O
can	O
show	O
that	O
em	B
will	O
still	O
converge	B
to	O
a	O
local	O
optimum	O
even	O
if	O
we	O
only	O
perform	O
a	O
partial	O
improvement	O
to	O
the	O
parameters	O
in	O
the	O
m	B
step	I
mixtures	O
of	O
student	O
distributions	O
it	O
is	O
easy	O
to	O
extend	O
the	O
above	O
methods	O
to	O
fit	O
a	O
mixture	B
of	O
student	O
distributions	O
see	O
exercise	O
for	O
the	O
details	O
let	O
us	O
consider	O
a	O
small	O
example	O
from	O
we	O
have	O
a	O
n	O
d	O
data	O
set	O
regarding	O
the	O
bankrupty	O
patterns	O
of	O
certain	O
companies	O
the	O
first	O
feature	O
specifies	O
the	O
ratio	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
of	O
retained	O
earnings	O
to	O
total	O
assets	O
and	O
the	O
second	O
feature	O
specifies	O
the	O
ratio	O
of	O
earnings	O
before	O
interests	O
and	O
taxes	O
to	O
total	O
assets	O
we	O
fit	O
two	O
models	O
to	O
this	O
data	O
ignoring	O
the	O
class	O
labels	O
a	O
mixture	B
of	I
gaussians	I
and	O
a	O
mixture	B
of	O
students	O
we	O
then	O
use	O
each	O
fitted	O
model	O
to	O
classify	O
the	O
data	O
we	O
compute	O
the	O
most	O
probable	O
cluster	O
membership	O
and	O
treat	O
this	O
as	O
yi	O
we	O
then	O
compare	O
yi	O
to	O
the	O
true	O
labels	O
yi	O
and	O
compute	O
an	O
error	O
rate	B
if	O
this	O
is	O
more	O
than	O
we	O
permute	O
the	O
latent	B
labels	O
we	O
consider	O
cluster	O
to	O
represent	O
class	O
and	O
vice	O
versa	O
and	O
then	O
recompute	O
the	O
error	O
rate	B
points	O
which	O
are	O
misclassified	O
are	O
then	O
shown	O
in	O
red	O
the	O
result	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
student	O
model	O
made	O
errors	O
the	O
gaussian	B
model	O
made	O
this	O
is	O
because	O
the	O
class-conditional	O
densities	O
contain	O
some	O
extreme	O
values	O
causing	O
the	O
gaussian	B
to	O
be	O
a	O
poor	O
choice	O
em	B
for	O
probit	B
regression	B
in	O
section	O
we	O
described	O
the	O
latent	B
variable	O
interpretation	O
of	O
probit	B
regression	B
recall	B
that	O
this	O
has	O
the	O
form	O
pyi	O
zi	O
where	O
zi	O
n	O
xi	O
is	O
latent	B
we	O
now	O
show	O
how	O
to	O
fit	O
this	O
model	O
using	O
em	B
it	O
is	O
possible	O
to	O
fit	O
probit	B
regression	B
models	O
using	O
gradient	O
based	O
methods	O
as	O
shown	O
in	O
section	O
this	O
em-based	O
approach	O
has	O
the	O
advantage	O
that	O
it	O
generalized	O
to	O
many	O
other	O
kinds	O
of	O
models	O
as	O
we	O
will	O
see	O
later	O
on	O
the	O
complete	B
data	I
log	I
likelihood	B
has	O
the	O
following	O
form	O
assuming	O
a	O
n	O
prior	O
on	O
w	O
log	O
pyz	O
log	O
n	O
i	O
log	O
n	O
xwt	O
xw	O
log	O
pyizi	O
w	O
const	O
wt	O
v	O
i	O
the	O
posterior	O
in	O
the	O
e	B
step	I
is	O
a	O
truncated	B
gaussian	B
pziyi	O
xi	O
w	O
n	O
xi	O
n	O
xi	O
if	O
yi	O
if	O
yi	O
in	O
equation	O
we	O
see	O
that	O
w	O
only	O
depends	O
linearly	O
on	O
z	O
so	O
we	O
just	O
need	O
to	O
compute	O
e	O
xi	O
w	O
exercise	O
asks	O
you	O
to	O
show	O
that	O
the	O
posterior	B
mean	B
is	O
given	O
by	O
e	O
xi	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
if	O
yi	O
if	O
yi	O
where	O
i	O
wt	O
xi	O
in	O
the	O
m	B
step	I
we	O
estimate	O
w	O
using	O
ridge	B
regression	B
where	O
e	O
is	O
the	O
output	O
we	O
are	O
trying	O
to	O
predict	O
specifically	O
we	O
have	O
w	O
xt	O
x	O
the	O
em	B
algorithm	O
is	O
simple	O
but	O
can	O
be	O
much	O
slower	O
than	O
direct	O
gradient	O
methods	O
as	O
illustrated	O
in	O
figure	O
this	O
is	O
because	O
the	O
posterior	O
entropy	B
in	O
the	O
e	B
step	I
is	O
quite	O
high	O
since	O
we	O
only	O
observe	O
that	O
z	O
is	O
positive	O
or	O
negative	O
but	O
are	O
given	O
no	O
information	B
from	O
the	O
likelihood	B
about	O
its	O
magnitude	O
using	O
a	O
stronger	O
regularizer	O
can	O
help	O
speed	O
convergence	O
because	O
it	O
constrains	O
the	O
range	O
of	O
plausible	O
z	O
values	O
in	O
addition	O
one	O
can	O
use	O
various	O
speedup	O
tricks	O
such	O
as	O
data	B
augmentation	I
dyk	O
and	O
meng	O
but	O
we	O
do	O
not	O
discuss	O
that	O
here	O
the	O
em	B
algorithm	O
probit	B
regression	B
with	O
regularizer	O
of	O
em	B
minfunc	O
l	O
l	O
n	O
d	O
e	O
z	O
i	O
l	O
a	O
n	O
e	O
p	O
iter	O
figure	O
by	O
probitregdemo	O
fitting	O
a	O
probit	B
regression	B
model	O
in	O
using	O
a	O
quasi-newton	B
method	O
or	O
em	B
figure	O
generated	O
theoretical	O
basis	O
for	O
em	B
in	O
this	O
section	O
we	O
show	O
that	O
em	B
monotonically	O
increases	O
the	O
observed	B
data	I
log	I
likelihood	B
until	O
it	O
reaches	O
a	O
local	O
maximum	O
saddle	O
point	O
although	O
such	O
points	O
are	O
usually	O
unstable	B
our	O
derivation	O
will	O
also	O
serve	O
as	O
the	O
basis	O
for	O
various	O
generalizations	O
of	O
em	B
that	O
we	O
will	O
discuss	O
later	O
expected	B
complete	B
data	I
log	I
likelihood	B
is	O
a	O
lower	O
bound	O
consider	O
an	O
arbitrary	O
distribution	O
qzi	O
over	O
the	O
hidden	B
variables	I
the	O
observed	B
data	I
log	I
likelihood	B
can	O
be	O
written	O
as	O
follows	O
pxi	O
zi	O
pxi	O
zi	O
log	O
qzi	O
log	O
zi	O
zi	O
qzi	O
now	O
logu	O
is	O
a	O
concave	B
function	O
so	O
from	O
jensen	O
s	O
inequality	O
we	O
have	O
the	O
following	O
lower	O
bound	O
i	O
zi	O
qizi	O
log	O
pxi	O
zi	O
qizi	O
let	O
us	O
denote	O
this	O
lower	O
bound	O
as	O
follows	O
q	O
q	O
eqi	O
pxi	O
zi	O
h	O
i	O
where	O
h	O
is	O
the	O
entropy	B
of	O
qi	O
the	O
above	O
argument	O
holds	O
for	O
any	O
positive	O
distribution	O
q	O
which	O
one	O
should	O
we	O
choose	O
intuitively	O
we	O
should	O
pick	O
the	O
q	O
that	O
yields	O
the	O
tightest	O
lower	O
bound	O
the	O
lower	O
bound	O
is	O
a	O
sum	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
over	O
i	O
of	O
terms	O
of	O
the	O
following	O
form	O
l	O
qi	O
qizi	O
log	O
qizi	O
log	O
zi	O
zi	O
pxi	O
zi	O
qizi	O
pzixi	O
qizi	O
pzixi	O
qizi	O
log	O
kl	O
log	O
pxi	O
qizi	O
zi	O
zi	O
qizi	O
log	O
pxi	O
the	O
pxi	O
term	O
is	O
independent	O
of	O
qi	O
so	O
we	O
can	O
maximize	O
the	O
lower	O
bound	O
by	O
setting	O
qizi	O
pzixi	O
of	O
course	O
is	O
unknown	B
so	O
instead	O
we	O
use	O
qt	O
i	O
pzixi	O
t	O
where	O
t	O
is	O
our	O
estimate	O
of	O
the	O
parameters	O
at	O
iteration	O
t	O
this	O
is	O
the	O
output	O
of	O
the	O
e	B
step	I
plugging	O
this	O
in	O
to	O
the	O
lower	O
bound	O
we	O
get	O
q	O
qt	O
pxi	O
zi	O
h	O
qt	O
i	O
eqt	O
i	O
i	O
we	O
recognize	O
the	O
first	O
term	O
as	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
the	O
second	O
term	O
is	O
a	O
constant	O
wrt	O
so	O
the	O
m	B
step	I
becomes	O
arg	O
max	O
q	O
t	O
arg	O
max	O
pxi	O
zi	O
eqt	O
i	O
i	O
as	O
usual	O
zero	O
so	O
l	O
t	O
qi	O
log	O
pxi	O
t	O
and	O
hence	O
now	O
comes	O
the	O
punchline	O
since	O
we	O
used	O
qt	O
i	O
pzixi	O
t	O
the	O
kl	B
divergence	I
becomes	O
q	O
t	O
t	O
log	O
pxi	O
t	O
t	O
i	O
we	O
see	O
that	O
the	O
lower	O
bound	O
is	O
tight	O
after	O
the	O
e	B
step	I
since	O
the	O
lower	O
bound	O
touches	O
the	O
function	O
maximizing	O
the	O
lower	O
bound	O
will	O
also	O
push	O
up	O
on	O
the	O
function	O
itself	O
that	O
is	O
the	O
m	B
step	I
is	O
guaranteed	O
to	O
modify	O
the	O
parameters	O
so	O
as	O
to	O
increase	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
it	O
is	O
already	O
at	O
a	O
local	O
maximum	O
this	O
process	O
is	O
sketched	O
in	O
figure	O
the	O
dashed	O
red	O
curve	O
is	O
the	O
original	O
function	O
observed	O
data	O
log-likelihood	O
the	O
solid	O
blue	O
curve	O
is	O
the	O
lower	O
bound	O
evaluated	O
at	O
t	O
this	O
touches	O
the	O
objective	O
function	O
at	O
t	O
we	O
then	O
set	O
to	O
the	O
maximum	O
of	O
the	O
lower	O
bound	O
curve	O
and	O
fit	O
a	O
new	O
bound	O
at	O
that	O
point	O
green	O
curve	O
the	O
maximum	O
of	O
this	O
new	O
bound	O
becomes	O
etc	O
this	O
to	O
newton	O
s	O
method	O
in	O
figure	O
which	O
repeatedly	O
fits	O
and	O
then	O
optimizes	O
a	O
quadratic	O
approximation	O
em	B
monotonically	O
increases	O
the	O
observed	B
data	I
log	I
likelihood	B
we	O
now	O
prove	O
that	O
em	B
monotonically	O
increases	O
the	O
observed	B
data	I
log	I
likelihood	B
until	O
it	O
reaches	O
a	O
local	O
optimum	O
we	O
have	O
q	O
t	O
q	O
t	O
t	O
t	O
the	O
em	B
algorithm	O
q	O
t	O
q	O
l	O
t	O
figure	O
illustration	O
of	O
em	B
as	O
a	O
bound	B
optimization	B
algorithm	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
emloglikelihoodmax	O
where	O
the	O
first	O
inequality	O
follows	O
since	O
q	O
is	O
a	O
lower	O
bound	O
on	O
the	O
second	O
inequality	O
follows	O
since	O
by	O
definition	O
q	O
t	O
max	O
q	O
t	O
q	O
t	O
t	O
and	O
the	O
final	O
equality	O
follows	O
equation	O
as	O
a	O
consequence	O
of	O
this	O
result	O
if	O
you	O
do	O
not	O
observe	O
monotonic	O
increase	O
of	O
the	O
observed	O
you	O
are	O
performing	O
data	O
log	O
likelihood	B
you	O
must	O
have	O
an	O
error	O
in	O
your	O
math	O
andor	O
code	O
map	O
estimation	O
you	O
must	O
add	O
on	O
the	O
log	O
prior	O
term	O
to	O
the	O
objective	O
this	O
is	O
a	O
surprisingly	O
powerful	O
debugging	O
tool	O
online	B
em	B
when	O
dealing	O
with	O
large	O
or	O
streaming	O
datasets	O
it	O
is	O
important	O
to	O
be	O
able	O
to	O
learn	O
online	O
as	O
we	O
discussed	O
in	O
section	O
there	O
are	O
two	O
main	O
approaches	O
to	O
online	B
em	B
in	O
the	O
literature	O
the	O
first	O
approach	O
known	O
as	O
incremental	B
em	B
and	O
hinton	O
optimizes	O
the	O
lower	O
bound	O
q	O
qn	O
one	O
qi	O
at	O
a	O
time	O
however	O
this	O
requires	O
storing	O
the	O
expected	B
sufficient	B
statistics	I
for	O
each	O
data	O
case	O
the	O
second	O
approach	O
known	O
as	O
stepwise	B
em	B
and	O
ishii	O
cappe	O
and	O
mouline	O
cappe	O
is	O
based	O
on	O
stochastic	B
approximation	I
theory	O
and	O
only	O
requires	O
constant	O
memory	O
use	O
we	O
explain	O
both	O
approaches	O
in	O
more	O
detail	O
below	O
following	O
the	O
presentation	O
of	O
and	O
klein	O
liang	O
and	O
klein	O
batch	B
em	B
review	O
before	O
explaining	O
online	B
em	B
we	O
review	O
batch	B
em	B
in	O
a	O
more	O
abstract	O
setting	O
let	O
z	O
be	O
a	O
vector	O
of	O
sufficient	B
statistics	I
for	O
a	O
single	O
data	O
case	O
example	O
for	O
a	O
mixture	B
of	O
multinoullis	O
this	O
would	O
be	O
the	O
count	O
vector	O
aj	O
which	O
is	O
the	O
number	O
of	O
cluster	O
j	O
was	O
used	O
in	O
z	O
plus	O
the	O
matrix	O
bj	O
v	O
which	O
is	O
of	O
the	O
number	O
of	O
times	O
the	O
hidden	B
state	B
was	O
j	O
and	O
the	O
observed	O
letter	O
z	O
pzxi	O
z	O
be	O
the	O
expected	B
sufficient	B
statistics	I
for	O
case	O
i	O
and	O
was	O
v	O
let	O
si	O
si	O
be	O
the	O
sum	O
of	O
the	O
ess	B
given	O
we	O
can	O
derive	O
an	O
ml	O
or	O
map	B
estimate	I
of	O
the	O
parameters	O
in	O
the	O
m	B
step	I
we	O
will	O
denote	O
this	O
operation	O
by	O
example	O
in	O
the	O
case	O
of	O
mixtures	O
of	O
multinoullis	O
we	O
just	O
need	O
to	O
normalize	O
a	O
and	O
each	O
row	O
of	O
b	O
with	O
this	O
notation	O
under	O
our	O
belt	O
the	O
pseudo	O
code	O
for	O
batch	B
em	B
is	O
as	O
shown	O
in	O
algorithm	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
algorithm	O
batch	B
em	B
algorithm	O
initialize	O
repeat	O
new	O
for	O
each	O
example	O
i	O
do	O
z	O
pzxi	O
z	O
si	O
new	O
new	O
si	O
new	O
until	O
converged	O
incremental	B
em	B
in	O
incremental	B
em	B
and	O
hinton	O
we	O
keep	O
track	O
of	O
as	O
well	O
as	O
the	O
si	O
when	O
we	O
come	O
to	O
a	O
data	O
case	O
we	O
swap	O
out	O
the	O
old	O
si	O
and	O
replace	O
it	O
with	O
the	O
new	O
snew	O
as	O
shown	O
in	O
the	O
code	O
in	O
algorithm	O
note	O
that	O
we	O
can	O
exploit	O
the	O
sparsity	B
of	O
snew	O
to	O
speedup	O
the	O
computation	O
of	O
since	O
most	O
components	O
of	O
wil	O
not	O
have	O
changed	O
i	O
i	O
i	O
si	O
algorithm	O
incremental	B
em	B
algorithm	O
initialize	O
si	O
for	O
i	O
n	O
repeat	O
for	O
each	O
example	O
i	O
in	O
a	O
random	O
order	O
do	O
z	O
pzxi	O
z	O
i	O
si	O
snew	O
i	O
snew	O
si	O
snew	O
i	O
until	O
converged	O
this	O
can	O
be	O
viewed	O
as	O
maximizing	O
the	O
lower	O
bound	O
q	O
qn	O
by	O
optimizing	O
then	O
then	O
then	O
etc	O
as	O
such	O
this	O
method	O
is	O
guaranteed	O
to	O
monotonically	O
converge	B
to	O
a	O
local	O
maximum	O
of	O
the	O
lower	O
bound	O
and	O
to	O
the	O
log	O
likelihood	B
itself	O
stepwise	B
em	B
in	O
stepwise	B
em	B
whenever	O
we	O
compute	O
a	O
new	O
si	O
we	O
move	O
towards	O
it	O
as	O
shown	O
in	O
algorithm	O
at	O
iteration	O
k	O
the	O
stepsize	O
has	O
value	O
k	O
which	O
must	O
satisfy	O
the	O
robbins-monro	B
conditions	O
in	O
for	O
equation	O
we	O
can	O
get	O
somewhat	O
better	O
behavior	O
by	O
using	O
a	O
minibatch	O
of	O
size	O
m	O
before	O
it	O
is	O
possible	O
to	O
optimize	O
m	O
and	O
to	O
maximize	O
the	O
training	B
set	I
likelihood	B
by	O
each	O
update	O
and	O
klein	O
liang	O
and	O
klein	O
use	O
k	O
k	O
for	O
example	O
a	O
detail	O
as	O
written	O
the	O
update	O
for	O
does	O
not	O
exploit	O
the	O
sparsity	B
of	O
si	O
we	O
can	O
fix	O
this	O
by	O
storing	O
m	O
instead	O
of	O
and	O
then	O
using	O
the	O
sparse	B
update	O
m	O
m	O
since	O
scaling	O
the	O
counts	O
by	O
a	O
global	O
constant	O
has	O
no	O
effect	O
j	O
j	O
si	O
this	O
will	O
not	O
affect	O
the	O
results	O
k	O
the	O
em	B
algorithm	O
figure	O
uated	O
optimization	B
illustration	O
of	O
deterministic	B
annealing	B
based	O
on	O
httpen	O
wikipedia	O
orgwikigrad	O
trying	O
different	O
values	O
in	O
parallel	O
for	O
an	O
initial	O
trial	O
period	B
this	O
can	O
significantly	O
speed	O
up	O
the	O
algorithm	O
algorithm	O
stepwise	B
em	B
algorithm	O
initialize	O
k	O
repeat	O
for	O
each	O
example	O
i	O
in	O
a	O
random	O
order	O
do	O
z	O
pzxi	O
z	O
si	O
k	O
ksi	O
k	O
k	O
until	O
converged	O
and	O
klein	O
liang	O
and	O
klein	O
compare	O
batch	B
em	B
incremental	B
em	B
and	O
stepwise	B
em	B
on	O
four	O
different	O
unsupervised	O
language	B
modeling	I
tasks	O
they	O
found	O
that	O
stepwise	B
em	B
and	O
m	O
was	O
faster	O
than	O
incremental	B
em	B
and	O
both	O
were	O
much	O
faster	O
than	O
batch	B
em	B
in	O
terms	O
of	O
accuracy	O
stepwise	B
em	B
was	O
usually	O
as	O
good	O
or	O
sometimes	O
even	O
better	O
than	O
batch	B
em	B
incremental	B
em	B
was	O
often	O
worse	O
than	O
either	O
of	O
the	O
other	O
methods	O
other	O
em	B
variants	O
em	B
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
algorithms	O
in	O
statistics	O
and	O
machine	B
learning	B
not	O
surprisingly	O
many	O
variations	O
have	O
been	O
proposed	O
we	O
briefly	O
mention	O
a	O
few	O
below	O
some	O
of	O
which	O
we	O
will	O
use	O
in	O
later	O
chapters	O
see	O
and	O
krishnan	O
for	O
more	O
information	B
annealed	O
em	B
in	O
general	O
em	B
will	O
only	O
converge	B
to	O
a	O
local	O
maximum	O
to	O
increase	O
the	O
chance	O
of	O
finding	O
the	O
global	O
maximum	O
we	O
can	O
use	O
a	O
variety	O
of	O
methods	O
one	O
approach	O
is	O
to	O
use	O
a	O
method	O
known	O
as	O
deterministic	B
annealing	B
the	O
basic	O
idea	O
is	O
to	O
smooth	O
the	O
posterior	O
landscape	O
by	O
raising	O
it	O
to	O
a	O
temperature	B
and	O
then	O
gradually	O
cooling	O
it	O
all	O
the	O
while	O
slowly	O
tracking	B
the	O
global	O
maximum	O
see	O
figure	O
for	O
a	O
sketch	O
stochastic	O
version	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
true	O
log	O
likelihood	B
lower	O
bound	O
true	O
log	O
likelihood	B
lower	O
bound	O
training	O
time	O
training	O
time	O
figure	O
illustration	O
of	O
possible	O
behaviors	O
of	O
variational	B
em	B
the	O
lower	O
bound	O
increases	O
at	O
each	O
iteration	O
and	O
so	O
does	O
the	O
likelihood	B
in	O
this	O
case	O
the	O
algorithm	O
is	O
closing	O
the	O
gap	B
between	O
the	O
approximate	O
and	O
true	O
posterior	O
this	O
can	O
have	O
a	O
regularizing	O
effect	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
varembound	O
the	O
lower	O
bound	O
increases	O
but	O
the	O
likelihood	B
decreases	O
of	O
this	O
algorithm	O
is	O
described	O
in	O
section	O
an	O
annealed	O
version	O
of	O
em	B
is	O
described	O
in	O
and	O
nakano	O
variational	B
em	B
in	O
section	O
we	O
showed	O
that	O
the	O
optimal	O
thing	O
to	O
do	O
in	O
the	O
e	B
step	I
is	O
to	O
i	O
zixi	O
t	O
in	O
this	O
case	O
make	O
qi	O
be	O
the	O
exact	O
posterior	O
over	O
the	O
latent	B
variables	O
qt	O
the	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	B
will	O
be	O
tight	O
so	O
the	O
m	B
step	I
will	O
push	O
up	O
on	O
the	O
log-likelihood	O
itself	O
however	O
sometimes	O
it	O
is	O
computationally	O
intractable	O
to	O
perform	O
exact	O
inference	B
in	O
the	O
e	B
step	I
but	O
we	O
may	O
be	O
able	O
to	O
perform	O
approximate	B
inference	B
if	O
we	O
can	O
ensure	O
that	O
the	O
e	B
step	I
is	O
performing	O
inference	B
based	O
on	O
a	O
a	O
lowerbound	O
to	O
the	O
likelihood	B
then	O
the	O
m	B
step	I
can	O
be	O
seen	O
as	O
monotonically	O
increasing	O
this	O
lower	O
bound	O
figure	O
this	O
is	O
called	O
variational	B
em	B
and	O
hinton	O
see	O
chapter	O
for	O
some	O
variational	B
inference	B
methods	O
that	O
can	O
be	O
used	O
in	O
the	O
e	B
step	I
monte	B
carlo	I
em	B
another	O
approach	O
to	O
handling	O
an	O
intractable	O
e	B
step	I
is	O
to	O
use	O
a	O
monte	B
carlo	I
approximation	O
to	O
the	O
expected	B
sufficient	B
statistics	I
that	O
is	O
we	O
draw	O
samples	B
from	O
the	O
i	O
pzixi	O
t	O
and	O
then	O
compute	O
the	O
sufficient	B
statistics	I
for	O
each	O
completed	O
posterior	O
zs	O
vector	O
zs	O
i	O
and	O
then	O
average	O
the	O
results	O
this	O
is	O
called	O
monte	B
carlo	I
em	B
or	O
mcem	B
we	O
only	O
draw	O
a	O
single	O
sample	O
it	O
is	O
called	O
stochastic	B
em	B
and	O
and	O
tanner	O
diebolt	O
one	O
way	O
to	O
draw	O
samples	B
is	O
to	O
use	O
mcmc	B
chapter	O
however	O
if	O
we	O
have	O
to	O
wait	O
for	O
mcmc	B
to	O
converge	B
inside	O
each	O
e	B
step	I
the	O
method	O
becomes	O
very	O
slow	O
an	O
alternative	O
is	O
to	O
use	O
stochastic	B
approximation	I
and	O
only	O
perform	O
brief	O
sampling	O
in	O
the	O
e	B
step	I
followed	O
by	O
a	O
partial	O
parameter	B
update	O
this	O
is	O
called	O
stochastic	B
approximation	I
em	B
et	O
al	O
and	O
tends	O
to	O
work	O
better	O
than	O
mcem	B
another	O
alternative	O
is	O
to	O
apply	O
mcmc	B
to	O
infer	O
the	O
parameters	O
as	O
well	O
as	O
the	O
latent	B
variables	O
fully	O
bayesian	B
approach	O
thus	O
eliminating	O
the	O
distinction	O
between	O
e	O
and	O
m	O
steps	O
see	O
chapter	O
for	O
details	O
generalized	B
em	B
sometimes	O
we	O
can	O
perform	O
the	O
e	B
step	I
exactly	O
but	O
we	O
cannot	O
perform	O
the	O
m	B
step	I
exactly	O
however	O
we	O
can	O
still	O
monotonically	O
increase	O
the	O
log	O
likelihood	B
by	O
performing	O
a	O
partial	O
m	B
step	I
in	O
which	O
we	O
merely	O
increase	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
rather	O
than	O
maximizing	O
it	O
for	O
example	O
we	O
might	O
follow	O
a	O
few	O
gradient	O
steps	O
this	O
is	O
called	O
the	O
em	B
algorithm	O
k	O
i	O
l	O
g	O
o	O
l	O
em	B
iterations	O
k	O
i	O
l	O
g	O
o	O
l	O
em	B
iterations	O
figure	O
illustration	O
of	O
adaptive	O
over-relaxed	O
em	B
applied	O
to	O
a	O
mixture	B
of	I
gaussians	I
in	O
dimensions	O
we	O
show	O
the	O
algorithm	O
applied	O
to	O
two	O
different	O
datasets	O
randomly	O
sampled	O
from	O
a	O
mixture	B
of	I
gaussians	I
we	O
plot	O
the	O
convergence	O
for	O
different	O
update	O
rates	O
using	O
gives	O
the	O
same	O
results	O
as	O
regular	B
em	B
the	O
actual	O
running	O
time	O
is	O
printed	O
in	O
the	O
legend	O
figure	O
generated	O
by	O
mixgaussoverrelaxedemdemo	O
the	O
generalized	B
em	B
or	O
gem	B
algorithm	O
ways	O
to	O
generalize	B
em	B
is	O
an	O
unfortunate	O
term	O
since	O
there	O
are	O
many	O
ecme	B
algorithm	O
the	O
ecm	B
algorithm	O
stands	O
for	O
expectation	O
conditional	O
maximization	O
and	O
refers	O
to	O
optimizing	O
the	O
parameters	O
in	O
the	O
m	B
step	I
sequentially	O
if	O
they	O
turn	O
out	O
to	O
be	O
dependent	O
the	O
ecme	B
algorithm	O
which	O
stands	O
for	O
ecm	B
either	O
and	O
rubin	O
is	O
a	O
variant	O
of	O
ecm	B
in	O
which	O
we	O
maximize	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
q	O
function	O
as	O
usual	O
or	O
the	O
observed	B
data	I
log	I
likelihood	B
during	O
one	O
or	O
more	O
of	O
the	O
conditional	O
maximization	O
steps	O
the	O
latter	O
can	O
be	O
much	O
faster	O
since	O
it	O
ignores	O
the	O
results	O
of	O
the	O
e	B
step	I
and	O
directly	O
optimizes	O
the	O
objective	O
of	O
interest	O
a	O
standard	O
example	O
of	O
this	O
is	O
when	O
fitting	O
the	O
student	B
t	I
distribution	I
for	O
fixed	O
we	O
can	O
update	O
as	O
usual	O
but	O
then	O
to	O
update	O
we	O
replace	O
the	O
standard	O
update	O
of	O
the	O
form	O
arg	O
max	O
q	O
t	O
with	O
arg	O
max	O
log	O
pd	O
see	O
and	O
krishnan	O
for	O
more	O
information	B
over-relaxed	O
em	B
vanilla	O
em	B
can	O
be	O
quite	O
slow	O
especially	O
if	O
there	O
is	O
lots	O
of	O
missing	B
data	I
the	O
adaptive	O
overrelaxed	B
em	B
algorithm	I
and	O
roweis	O
performs	O
an	O
update	O
of	O
the	O
form	O
t	O
t	O
t	O
where	O
is	O
a	O
step-size	O
parameter	B
and	O
m	O
t	O
is	O
the	O
usual	O
update	O
computed	O
during	O
the	O
m	B
step	I
obviously	O
this	O
reduces	O
to	O
standard	O
em	B
if	O
but	O
using	O
larger	O
values	O
of	O
can	O
result	O
in	O
faster	O
convergence	O
see	O
figure	O
for	O
an	O
illustration	O
unfortunately	O
using	O
too	O
large	O
a	O
value	O
of	O
can	O
cause	O
the	O
algorithm	O
to	O
fail	O
to	O
converge	B
finally	O
note	O
that	O
em	B
is	O
in	O
fact	O
just	O
a	O
special	O
case	O
of	O
a	O
larger	O
class	O
of	O
algorithms	O
known	O
as	O
bound	B
optimization	B
or	O
mm	B
algorithms	O
stands	O
for	O
minorize-maximize	B
see	O
and	O
lange	O
for	O
further	O
discussion	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
model	B
selection	I
for	O
latent	B
variable	I
models	I
when	O
using	O
lvms	O
we	O
must	O
specify	O
the	O
number	O
of	O
latent	B
variables	O
which	O
controls	O
the	O
model	O
in	O
the	O
case	O
of	O
mixture	B
models	O
we	O
must	O
specify	O
k	O
the	O
number	O
complexity	O
of	O
clusters	B
choosing	O
these	O
parameters	O
is	O
an	O
example	O
of	O
model	B
selection	I
we	O
discuss	O
some	O
approaches	O
below	O
in	O
particuarl	O
model	B
selection	I
for	O
probabilistic	O
models	O
the	O
optimal	O
bayesian	B
approach	O
discussed	O
in	O
section	O
is	O
to	O
pick	O
the	O
model	O
with	O
the	O
largest	O
marginal	B
likelihood	B
k	O
argmaxk	O
pdk	O
there	O
are	O
two	O
problems	O
with	O
this	O
likelihood	B
for	O
lvms	O
is	O
quite	O
difficult	O
in	O
practice	O
simple	O
approximations	O
such	O
as	O
bic	B
can	O
be	O
used	O
e	O
g	O
and	O
raftery	O
alternatively	O
we	O
can	O
use	O
the	O
cross-validated	O
likelihood	B
as	O
a	O
performance	O
measure	O
although	O
this	O
can	O
be	O
slow	O
since	O
it	O
requires	O
fitting	O
each	O
model	O
f	O
times	O
where	O
f	O
is	O
the	O
number	O
of	O
cv	B
folds	B
first	O
evaluating	O
the	O
marginal	O
the	O
second	O
issue	O
is	O
the	O
need	O
to	O
search	O
over	O
a	O
potentially	O
large	O
number	O
of	O
models	O
the	O
usual	O
approach	O
is	O
to	O
perform	O
exhaustive	O
search	O
over	O
all	O
candidate	O
values	O
of	O
k	O
however	O
sometimes	O
we	O
can	O
set	O
the	O
model	O
to	O
its	O
maximal	O
size	O
and	O
then	O
rely	O
on	O
the	O
power	O
of	O
the	O
bayesian	B
occam	O
s	O
razor	O
to	O
kill	O
off	O
unwanted	O
components	O
an	O
example	O
of	O
this	O
will	O
be	O
shown	O
in	O
section	O
when	O
we	O
discuss	O
variational	B
bayes	I
an	O
alternative	O
approach	O
is	O
to	O
perform	O
stochastic	O
sampling	O
in	O
the	O
space	O
of	O
models	O
traditional	O
approaches	O
such	O
as	O
lunn	O
et	O
al	O
are	O
based	O
on	O
reversible	B
jump	I
mcmc	B
and	O
use	O
birth	B
moves	I
to	O
propose	B
new	O
centers	O
and	O
death	B
moves	I
to	O
kill	O
off	O
old	O
centers	O
however	O
this	O
can	O
be	O
slow	O
and	O
difficult	O
to	O
implement	O
a	O
simpler	O
approach	O
is	O
to	O
use	O
a	O
dirichlet	B
process	I
mixture	B
model	I
which	O
can	O
be	O
fit	O
using	O
gibbs	B
sampling	I
but	O
still	O
allows	O
for	O
an	O
unbounded	O
number	O
of	O
mixture	B
components	O
see	O
section	O
for	O
details	O
perhaps	O
surprisingly	O
these	O
sampling-based	O
methods	O
can	O
be	O
faster	O
than	O
the	O
simple	O
approach	O
of	O
evaluating	O
the	O
quality	O
of	O
each	O
k	O
separately	O
the	O
reason	O
is	O
that	O
fitting	O
the	O
model	O
for	O
each	O
k	O
is	O
often	O
slow	O
by	O
contrast	O
the	O
sampling	O
methods	O
can	O
often	O
quickly	O
determine	O
that	O
a	O
certain	O
value	O
of	O
k	O
is	O
poor	O
and	O
thus	O
they	O
need	O
not	O
waste	O
time	O
in	O
that	O
part	O
of	O
the	O
posterior	O
model	B
selection	I
for	O
non-probabilistic	O
methods	O
ed	O
k	O
i	O
d	O
what	O
if	O
we	O
are	O
not	O
using	O
a	O
probabilistic	O
model	O
for	O
example	O
how	O
do	O
we	O
choose	O
k	O
for	O
the	O
kmeans	O
algorithm	O
since	O
this	O
does	O
not	O
correspond	O
to	O
a	O
probability	O
model	O
there	O
is	O
no	O
likelihood	B
so	O
none	O
of	O
the	O
methods	O
described	O
above	O
can	O
be	O
used	O
struction	O
error	O
of	O
a	O
data	O
set	O
d	O
using	O
model	O
complexity	O
k	O
as	O
follows	O
an	O
obvious	O
proxy	O
for	O
the	O
likelihood	B
is	O
the	O
reconstruction	B
error	I
define	O
the	O
squared	O
recon	O
in	O
the	O
case	O
of	O
k-means	O
the	O
reconstruction	O
is	O
given	O
by	O
xi	O
zi	O
where	O
zi	O
argmink	O
as	O
explained	O
in	O
section	O
figure	O
plots	O
the	O
reconstruction	B
error	I
on	O
the	O
test	O
set	O
for	O
k-means	O
we	O
notice	O
that	O
the	O
error	O
decreases	O
with	O
increasing	O
model	O
complexity	O
the	O
reason	O
for	O
this	O
behavior	O
is	O
as	O
follows	O
model	B
selection	I
for	O
latent	B
variable	I
models	I
mse	B
on	O
test	O
vs	O
k	O
for	O
k	O
means	O
nll	B
on	O
test	O
set	O
vs	O
k	O
for	O
gmm	B
figure	O
test	O
set	O
performance	O
vs	O
k	O
for	O
data	O
generated	O
from	O
a	O
mixture	B
of	I
gaussians	I
in	O
is	O
shown	O
in	O
figure	O
mse	B
on	O
test	O
set	O
for	O
k-means	O
negative	B
log	I
likelihood	B
on	O
test	O
set	O
for	O
gmm	B
figure	O
generated	O
by	O
xtrain	O
figure	O
data	O
looks	O
essentially	O
the	O
same	O
gmm	B
density	O
model	O
estimated	O
by	O
em	B
for	O
for	O
the	O
same	O
values	O
of	O
k	O
synthetic	O
data	O
generated	O
from	O
a	O
mixture	B
of	I
gaussians	I
in	O
histogram	B
of	O
training	O
data	O
centroids	B
estimated	O
by	O
k-means	O
for	O
k	O
figure	O
generated	O
by	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
when	O
we	O
add	O
more	O
and	O
more	O
centroids	B
to	O
k-means	O
we	O
can	O
tile	O
the	O
space	O
more	O
densely	O
as	O
shown	O
in	O
figure	O
hence	O
any	O
given	O
test	O
vector	O
is	O
more	O
likely	O
to	O
find	O
a	O
close	O
prototype	B
to	O
accurately	O
represent	O
it	O
as	O
k	O
increases	O
thus	O
decreasing	O
reconstruction	B
error	I
however	O
if	O
we	O
use	O
a	O
probabilistic	O
model	O
such	O
as	O
the	O
gmm	B
and	O
plot	O
the	O
negative	O
log-likelihood	O
we	O
get	O
the	O
usual	O
u-shaped	B
curve	I
on	O
the	O
test	O
set	O
as	O
shown	O
in	O
figure	O
in	O
supervised	B
learning	B
we	O
can	O
always	O
use	O
cross	B
validation	I
to	O
select	O
between	O
non-probabilistic	O
models	O
of	O
different	O
complexity	O
but	O
this	O
is	O
not	O
the	O
case	O
with	O
unsupervised	B
learning	B
although	O
this	O
is	O
not	O
a	O
novel	O
observation	B
it	O
is	O
mentioned	O
in	O
passing	O
in	O
et	O
al	O
one	O
of	O
the	O
standard	O
references	O
in	O
this	O
field	O
it	O
is	O
perhaps	O
not	O
as	O
widely	O
appreciated	O
as	O
it	O
should	O
be	O
in	O
fact	O
it	O
is	O
one	O
of	O
the	O
more	O
compelling	O
arguments	O
in	O
favor	O
of	O
probabilistic	O
models	O
given	O
that	O
cross	B
validation	I
doesn	O
t	O
work	O
and	O
supposing	O
one	O
is	O
unwilling	O
to	O
use	O
probabilistic	O
models	O
some	O
bizarre	O
reason	O
how	O
can	O
one	O
choose	O
k	O
the	O
most	O
common	O
approach	O
is	O
to	O
plot	O
the	O
reconstruction	B
error	I
on	O
the	O
training	B
set	I
versus	O
k	O
and	O
to	O
try	O
to	O
identify	O
a	O
knee	B
or	O
kink	B
in	O
the	O
curve	O
the	O
idea	O
is	O
that	O
for	O
k	O
k	O
is	O
the	O
true	O
number	O
of	O
clusters	B
the	O
rate	B
of	O
decrease	O
in	O
the	O
error	B
function	I
will	O
be	O
high	O
since	O
we	O
are	O
splitting	O
apart	O
things	O
that	O
should	O
not	O
be	O
grouped	O
together	O
however	O
for	O
k	O
k	O
we	O
are	O
splitting	O
apart	O
natural	O
clusters	B
which	O
does	O
not	O
reduce	O
the	O
error	O
by	O
as	O
much	O
where	O
k	O
this	O
kink-finding	O
process	O
can	O
be	O
automated	O
by	O
use	O
of	O
the	O
gap	B
statistic	I
et	O
al	O
nevertheless	O
identifying	O
such	O
kinks	O
can	O
be	O
hard	O
as	O
shown	O
in	O
figure	O
since	O
the	O
loss	B
function	I
usually	O
drops	O
off	O
gradually	O
a	O
different	O
approach	O
to	O
kink	B
finding	O
is	O
described	O
in	O
section	O
fitting	O
models	O
with	O
missing	B
data	I
suppose	O
we	O
want	O
to	O
fit	O
a	O
joint	O
density	O
model	O
by	O
maximum	O
likelihood	B
but	O
we	O
have	O
holes	O
in	O
our	O
data	O
matrix	O
due	O
to	O
missing	B
data	I
represented	O
by	O
nans	O
more	O
formally	O
let	O
oij	O
if	O
component	O
j	O
of	O
data	O
case	O
i	O
is	O
observed	O
and	O
let	O
oij	O
otherwise	O
let	O
xv	O
oij	O
be	O
the	O
visible	B
data	O
and	O
xh	O
oij	O
be	O
the	O
missing	B
or	O
hidden	B
data	O
our	O
goal	O
is	O
to	O
compute	O
argmax	O
pxv	O
o	O
under	O
the	O
missing	B
at	I
random	I
assumption	O
section	O
we	O
have	O
pxv	O
o	O
pxiv	O
where	O
xiv	O
is	O
a	O
vector	O
created	O
from	O
row	O
i	O
and	O
the	O
columns	O
indexed	O
by	O
the	O
set	O
oij	O
hence	O
the	O
log-likelihood	O
has	O
the	O
form	O
log	O
pxv	O
log	O
pxiv	O
where	O
pxiv	O
xih	O
i	O
pxiv	O
xih	O
fitting	O
models	O
with	O
missing	B
data	I
and	O
xih	O
is	O
the	O
vector	O
of	O
hidden	B
variables	I
for	O
case	O
i	O
discrete	B
for	O
notational	O
simplicity	O
substituting	O
in	O
we	O
get	O
log	O
pxv	O
pxiv	O
xih	O
log	O
i	O
xih	O
unfortunately	O
this	O
objective	O
is	O
hard	O
to	O
maximize	O
since	O
we	O
cannot	O
push	O
the	O
log	O
inside	O
the	O
sum	O
however	O
we	O
can	O
use	O
the	O
em	B
algorithm	O
to	O
compute	O
a	O
local	O
optimum	O
we	O
give	O
an	O
example	O
of	O
this	O
below	O
em	B
for	O
the	O
mle	B
of	O
an	O
mvn	B
with	O
missing	B
data	I
suppose	O
we	O
want	O
to	O
fit	O
an	O
mvn	B
by	O
maximum	O
likelihood	B
but	O
we	O
have	O
missing	B
data	I
we	O
can	O
use	O
em	B
to	O
find	O
a	O
local	O
maximum	O
of	O
the	O
objective	O
as	O
we	O
explain	O
below	O
getting	O
started	O
to	O
get	O
the	O
algorithm	O
started	O
we	O
can	O
compute	O
the	O
mle	B
based	O
on	O
those	O
rows	O
of	O
the	O
data	O
matrix	O
that	O
are	O
fully	O
observed	O
if	O
there	O
are	O
no	O
such	O
rows	O
we	O
can	O
use	O
some	O
ad-hoc	O
imputation	B
procedures	O
and	O
then	O
compute	O
an	O
initial	O
mle	B
e	B
step	I
once	O
we	O
have	O
t	O
we	O
can	O
compute	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
at	O
iteration	O
t	O
as	O
follows	O
q	O
t	O
log	O
n	O
t	O
e	O
i	O
tr	O
e	O
i	O
tr	O
e	O
n	O
n	O
n	O
log	O
log	O
log	O
n	O
d	O
t	O
where	O
e	O
e	O
drop	O
the	O
conditioning	B
of	O
the	O
expectation	O
on	O
d	O
and	O
t	O
for	O
brevity	O
we	O
see	O
that	O
we	O
need	O
to	O
compute	O
these	O
are	O
the	O
expected	B
sufficient	B
statistics	I
t	O
e	O
xixt	O
i	O
i	O
e	O
and	O
i	O
e	O
xixt	O
i	O
i	O
i	O
where	O
components	O
v	O
are	O
observed	O
and	O
components	O
h	O
are	O
unobserved	O
we	O
have	O
to	O
compute	O
these	O
quantities	O
we	O
use	O
the	O
results	O
from	O
section	O
specifically	O
consider	O
case	O
xihxiv	O
n	O
vi	O
mi	B
h	O
hv	O
vi	O
hh	O
hv	O
vv	O
v	O
vv	O
vh	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
hence	O
the	O
expected	B
sufficient	B
statistics	I
are	O
e	O
xiv	O
xiv	O
where	O
we	O
have	O
assumed	O
loss	B
of	O
generality	O
that	O
the	O
unobserved	O
variables	O
come	O
before	O
the	O
observed	O
variables	O
in	O
the	O
node	O
ordering	O
we	O
use	O
the	O
result	O
that	O
cov	O
e	O
xihxt	O
e	O
ih	O
xiv	O
e	O
xt	O
ih	O
xt	O
iv	O
xih	O
xiv	O
t	O
e	O
e	O
xt	O
xxt	O
e	O
xt	O
iv	O
xivxt	O
iv	O
to	O
compute	O
e	O
xixt	O
i	O
e	O
xixt	O
i	O
e	O
hence	O
e	O
xihxt	O
ih	O
e	O
e	O
t	O
vi	O
m	B
step	I
by	O
solving	O
q	O
we	O
can	O
show	O
that	O
the	O
m	B
step	I
is	O
equivalent	O
to	O
plugging	O
these	O
ess	B
into	O
the	O
usual	O
mle	B
equations	O
to	O
get	O
t	O
t	O
n	O
n	O
i	O
e	O
xixt	O
i	O
e	O
i	O
t	O
tt	O
thus	O
we	O
see	O
that	O
em	B
is	O
not	O
equivalent	O
to	O
simply	O
replacing	O
variables	O
by	O
their	O
expectations	O
and	O
applying	O
the	O
standard	O
mle	B
formula	O
that	O
would	O
ignore	O
the	O
posterior	O
variance	B
and	O
would	O
result	O
in	O
an	O
incorrect	O
estimate	O
instead	O
we	O
must	O
compute	O
the	O
expectation	O
of	O
the	O
sufficient	B
statistics	I
and	O
plug	O
that	O
into	O
the	O
usual	O
equation	O
for	O
the	O
mle	B
we	O
can	O
easily	O
modify	O
the	O
algorithm	O
to	O
perform	O
map	O
estimation	O
by	O
plugging	O
in	O
the	O
ess	B
into	O
the	O
equation	O
for	O
the	O
map	B
estimate	I
for	O
an	O
implementation	O
see	O
gaussmissingfitem	O
example	O
as	O
an	O
example	O
of	O
this	O
procedure	O
in	O
action	B
let	O
us	O
reconsider	O
the	O
imputation	B
problem	O
from	O
section	O
which	O
had	O
n	O
data	O
cases	O
with	O
missing	B
data	I
let	O
us	O
fit	O
the	O
parameters	O
using	O
em	B
call	O
the	O
resulting	O
parameters	O
we	O
can	O
use	O
our	O
model	O
for	O
predictions	O
by	O
computing	O
e	O
figure	O
indicates	O
that	O
the	O
results	O
obtained	O
using	O
the	O
learned	O
parameters	O
are	O
almost	O
as	O
good	O
as	O
with	O
the	O
true	O
parameters	O
not	O
surprisingly	O
performance	O
improves	O
with	O
more	O
data	O
or	O
as	O
the	O
fraction	O
of	O
missing	B
data	I
is	O
reduced	O
xihxiv	O
extension	B
to	O
the	O
gmm	B
case	O
it	O
is	O
straightforward	O
to	O
fit	O
a	O
mixture	B
of	I
gaussians	I
in	O
the	O
presence	O
of	O
partially	O
observed	O
data	O
vectors	O
xi	O
we	O
leave	O
the	O
details	O
as	O
an	O
exercise	O
exercises	O
exercise	O
student	B
t	I
as	O
infinite	O
mixture	B
of	I
gaussians	I
derive	O
equation	O
for	O
simplicity	O
assume	O
a	O
one-dimensional	O
distribution	O
fitting	O
models	O
with	O
missing	B
data	I
imputation	B
with	O
true	O
params	O
imputation	B
with	O
em	B
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
truth	O
truth	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
truth	O
truth	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
truth	O
truth	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
truth	O
truth	O
figure	O
illustration	O
of	O
data	O
imputation	B
ing	O
true	O
parameters	O
gaussimputationdemo	O
scatter	B
plot	I
of	O
true	O
values	O
vs	O
imputed	O
values	O
usb	O
same	O
as	O
but	O
using	O
parameters	O
estimated	O
with	O
em	B
figure	O
generated	O
by	O
exercise	O
em	B
for	O
mixtures	O
of	O
gaussians	O
show	O
that	O
the	O
m	B
step	I
for	O
ml	O
estimation	O
of	O
a	O
mixture	B
of	I
gaussians	I
is	O
given	O
by	O
k	O
k	O
i	O
rikxi	O
rk	O
i	O
rikxi	O
kxi	O
kt	O
rk	O
i	O
rk	O
k	O
t	O
i	O
rikxixt	O
rk	O
k	O
exercise	O
em	B
for	O
mixture	B
of	O
student	O
distributions	O
derive	O
the	O
em	B
algorithm	O
for	O
ml	O
estimation	O
of	O
a	O
mixture	B
of	O
multivariate	B
student	B
t	I
distributions	O
exercise	O
gradient	B
descent	I
for	O
fitting	O
gmm	B
consider	O
the	O
gaussian	B
mixture	B
model	I
kn	O
k	O
k	O
px	O
k	O
define	O
the	O
log	O
likelihood	B
as	O
log	O
pxn	O
exercise	O
em	B
for	O
mixtures	O
of	O
bernoullis	O
show	O
that	O
the	O
m	B
step	I
for	O
ml	O
estimation	O
of	O
a	O
mixture	B
of	O
bernoullis	O
is	O
given	O
by	O
i	O
rik	O
i	O
i	O
rikxij	O
i	O
rik	O
kj	O
kj	O
show	O
that	O
the	O
m	B
step	I
for	O
map	O
estimation	O
of	O
a	O
mixture	B
of	O
bernoullis	O
with	O
a	O
prior	O
is	O
given	O
by	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
p	O
q	O
jn	O
kn	O
xn	O
j	O
m	O
n	O
k	O
l	O
figure	O
a	O
mixture	B
of	I
gaussians	I
with	O
two	O
discrete	B
latent	B
indicators	O
jn	O
specifies	O
which	O
mean	B
to	O
use	O
and	O
kn	O
specifies	O
which	O
variance	B
to	O
use	O
define	O
the	O
posterior	O
responsibility	B
that	O
cluster	O
k	O
has	O
for	O
datapoint	O
n	O
as	O
follows	O
rnk	O
pzn	O
kxn	O
kn	O
k	O
k	O
a	O
show	O
that	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
k	O
is	O
n	O
d	O
d	O
k	O
rnk	O
k	O
k	O
k	O
d	O
dwk	O
n	O
rnk	O
k	O
b	O
derive	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
k	O
now	O
ignore	O
any	O
constraints	O
on	O
k	O
c	O
one	O
way	O
to	O
handle	O
the	O
constraint	O
that	O
k	O
is	O
to	O
reparameterize	O
using	O
the	O
softmax	B
function	O
here	O
wk	O
r	O
are	O
unconstrained	O
parameters	O
show	O
that	O
may	O
be	O
a	O
constant	O
factor	B
missing	B
in	O
the	O
above	O
expression	O
hint	O
use	O
the	O
chain	B
rule	I
and	O
the	O
fact	O
that	O
d	O
j	O
dwk	O
j	O
j	O
k	O
if	O
j	O
k	O
if	O
j	O
k	O
which	O
follows	O
from	O
exercise	O
d	O
derive	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
k	O
now	O
ignore	O
any	O
constraints	O
on	O
k	O
e	O
one	O
way	O
to	O
handle	O
the	O
constraint	O
that	O
k	O
be	O
a	O
symmetric	B
positive	O
definite	O
matrix	O
is	O
to	O
reparamek	O
r	O
where	O
r	O
is	O
an	O
upper-triangular	O
but	O
otherwise	O
terize	O
using	O
a	O
cholesky	B
decomposition	I
k	O
rt	O
unconstrained	O
matrix	O
derive	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
rk	O
exercise	O
em	B
for	O
a	O
finite	O
scale	O
mixture	B
of	I
gaussians	I
jaakkola	O
consider	O
the	O
graphical	B
model	I
in	O
figure	O
which	O
defines	O
the	O
following	O
pxn	O
pj	O
qkn	O
j	O
k	O
fitting	O
models	O
with	O
missing	B
data	I
where	O
pm	O
m	O
ql	O
l	O
are	O
all	O
the	O
parameters	O
here	O
pj	O
p	O
j	O
and	O
qk	O
p	O
k	O
are	O
the	O
equivalent	O
of	O
mixture	B
weights	O
we	O
can	O
think	O
of	O
this	O
as	O
a	O
mixture	B
of	O
m	O
non-gaussian	O
components	O
where	O
each	O
component	O
distribution	O
is	O
a	O
scale	O
mixture	B
pxj	O
k	O
combining	O
gaussians	O
with	O
different	O
variances	O
qkn	O
j	O
we	O
will	O
now	O
derive	O
a	O
generalized	B
em	B
algorithm	O
for	O
this	O
model	O
partial	O
update	O
in	O
the	O
m	B
step	I
rather	O
than	O
finding	O
the	O
exact	O
maximum	O
a	O
derive	O
an	O
expression	O
for	O
the	O
responsibilities	O
p	O
j	O
kn	O
kxn	O
needed	O
for	O
the	O
e	B
step	I
b	O
write	O
out	O
a	O
full	B
expression	O
for	O
the	O
expected	O
complete	B
log-likelihood	O
that	O
in	O
generalized	B
em	B
we	O
do	O
a	O
q	O
new	O
old	O
e	O
old	O
log	O
p	O
kn	O
xn	O
new	O
c	O
solving	O
the	O
m-step	O
would	O
require	O
us	O
to	O
jointly	O
optimize	O
the	O
means	O
m	O
and	O
the	O
variances	O
l	O
it	O
will	O
turn	O
out	O
to	O
be	O
simpler	O
to	O
first	O
solve	O
for	O
the	O
j	O
s	O
given	O
fixed	O
j	O
s	O
and	O
subsequently	O
j	O
s	O
given	O
the	O
new	O
values	O
of	O
j	O
s	O
for	O
brevity	O
we	O
will	O
just	O
do	O
the	O
first	O
part	O
derive	O
an	O
solve	O
for	O
expression	O
for	O
the	O
maximizing	O
j	O
s	O
given	O
fixed	O
i	O
e	O
solve	O
q	O
new	O
exercise	O
manual	O
calculation	O
of	O
the	O
m	B
step	I
for	O
a	O
gmm	B
de	O
freitas	O
in	O
this	O
question	O
we	O
consider	O
clustering	B
data	O
with	O
a	O
mixture	B
of	I
gaussians	I
using	O
the	O
em	B
algorithm	O
you	O
are	O
given	O
the	O
data	O
points	O
x	O
suppose	O
the	O
output	O
of	O
the	O
e	B
step	I
is	O
the	O
following	O
matrix	O
r	O
where	O
entry	O
ric	O
is	O
the	O
probability	O
of	O
obervation	O
xi	O
belonging	O
to	O
cluster	O
c	O
responsibility	B
of	O
cluster	O
c	O
for	O
data	O
point	O
i	O
you	O
just	O
have	O
to	O
compute	O
the	O
m	B
step	I
you	O
may	O
state	B
the	O
equations	O
for	O
maximum	O
likelihood	B
estimates	O
of	O
these	O
quantities	O
you	O
should	O
know	O
without	O
proof	O
you	O
just	O
have	O
to	O
apply	O
the	O
equations	O
to	O
this	O
data	O
set	O
you	O
may	O
leave	O
your	O
answer	O
in	O
fractional	O
form	O
show	O
your	O
work	O
a	O
write	O
down	O
the	O
likelihood	B
function	O
you	O
are	O
trying	O
to	O
optimize	O
b	O
after	O
performing	O
the	O
m	B
step	I
for	O
the	O
mixing	B
weights	I
what	O
are	O
the	O
new	O
values	O
c	O
after	O
performing	O
the	O
m	B
step	I
for	O
the	O
means	O
and	O
what	O
are	O
the	O
new	O
values	O
exercise	O
moments	O
of	O
a	O
mixture	B
of	I
gaussians	I
consider	O
a	O
mixture	B
of	O
k	O
gaussians	O
px	O
kn	O
k	O
k	O
a	O
show	O
that	O
e	O
k	O
k	O
k	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
figure	O
some	O
data	O
points	O
in	O
circles	O
represent	O
the	O
initial	O
guesses	O
for	O
and	O
b	O
show	O
that	O
cov	O
k	O
k	O
k	O
k	O
t	O
k	O
e	O
e	O
e	O
e	O
xxt	O
hint	O
use	O
the	O
fact	O
that	O
cov	O
e	O
exercise	O
k-means	O
clustering	B
by	O
hand	O
jaakkola	O
in	O
figure	O
we	O
show	O
some	O
data	O
points	O
which	O
lie	O
on	O
the	O
integer	O
grid	O
that	O
the	O
x-axis	O
has	O
been	O
compressed	O
distances	O
should	O
be	O
measured	O
using	O
the	O
actual	O
grid	O
coordinates	O
suppose	O
we	O
apply	O
the	O
kmeans	O
algorithm	O
to	O
this	O
data	O
using	O
k	O
and	O
with	O
the	O
centers	O
initialized	O
at	O
the	O
two	O
circled	O
data	O
points	O
draw	O
the	O
final	O
clusters	B
obtained	O
after	O
k-means	O
converges	O
the	O
approximate	O
location	O
of	O
the	O
new	O
centers	O
and	O
group	O
together	O
all	O
the	O
points	O
assigned	O
to	O
each	O
center	O
hint	O
think	O
about	O
shortest	O
euclidean	B
distance	I
exercise	O
deriving	O
the	O
k-means	O
cost	O
function	O
show	O
that	O
izik	O
nk	O
izik	O
jw	O
hint	O
note	O
that	O
for	O
any	O
i	O
i	O
i	O
where	O
n	O
i	O
x	O
x	O
x	O
nx	O
since	O
i	O
i	O
i	O
x	O
x	O
x	O
xi	O
nx	O
xnx	O
nx	O
exercise	O
visible	B
mixtures	O
of	O
gaussians	O
are	O
in	O
the	O
exponential	B
family	B
show	O
that	O
the	O
joint	B
distribution	I
px	O
z	O
for	O
a	O
gmm	B
can	O
be	O
represented	O
in	O
exponential	B
family	B
form	O
fitting	O
models	O
with	O
missing	B
data	I
e	O
m	O
i	O
t	O
l	O
a	O
v	O
v	O
r	O
u	O
s	O
i	O
regression	B
with	O
censored	O
data	O
red	O
x	O
censored	O
green	O
predicted	O
em	B
ols	B
inverse	O
temperature	B
figure	O
example	O
of	O
censored	O
linear	B
regression	B
black	O
circles	O
are	O
observed	O
training	O
points	O
red	O
crosses	O
are	O
observed	O
but	O
censored	O
training	O
points	O
green	O
stars	O
are	O
predicted	O
values	O
of	O
the	O
censored	O
training	O
points	O
we	O
also	O
show	O
the	O
lines	O
fit	O
by	O
least	B
squares	I
censoring	O
and	O
by	O
em	B
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
linregcensoredschmeehahndemo	O
written	O
by	O
hannes	O
bretschneider	O
exercise	O
em	B
for	O
robust	B
linear	B
regression	B
with	O
a	O
student	B
t	I
likelihood	B
consider	O
a	O
model	O
of	O
the	O
form	O
pyixi	O
w	O
t	O
xi	O
derive	O
an	O
em	B
algorithm	O
to	O
compute	O
the	O
mle	B
for	O
w	O
you	O
may	O
assume	O
and	O
are	O
fixed	O
for	O
simplicity	O
hint	O
see	O
section	O
exercise	O
em	B
for	O
eb	B
estimation	O
of	O
gaussian	B
shrinkage	B
model	O
extend	O
the	O
results	O
of	O
section	O
to	O
the	O
case	O
where	O
the	O
j	O
are	O
not	O
equal	O
are	O
known	O
hint	O
treat	O
the	O
j	O
as	O
hidden	B
variables	I
and	O
then	O
to	O
integrate	O
them	O
out	O
in	O
the	O
e	B
step	I
and	O
maximize	O
in	O
the	O
m	B
step	I
exercise	O
em	B
for	O
censored	O
linear	B
regression	B
censored	B
regression	B
refers	O
to	O
the	O
case	O
where	O
one	O
knows	O
the	O
outcome	O
is	O
at	O
least	O
at	O
most	O
a	O
certain	O
value	O
but	O
the	O
precise	O
value	O
is	O
unknown	B
this	O
arises	O
in	O
many	O
different	O
settings	O
for	O
example	O
suppose	O
one	O
is	O
trying	O
to	O
learn	O
a	O
model	O
that	O
can	O
predict	O
how	O
long	O
a	O
program	O
will	O
take	O
to	O
run	O
for	O
different	O
settings	O
of	O
its	O
parameters	O
one	O
may	O
abort	O
certain	O
runs	O
if	O
they	O
seem	O
to	O
be	O
taking	O
too	O
long	O
the	O
resulting	O
run	O
times	O
are	O
said	O
to	O
be	O
right	B
censored	I
for	O
such	O
runs	O
all	O
we	O
know	O
is	O
that	O
yi	O
ci	B
where	O
ci	B
is	O
the	O
censoring	O
time	O
that	O
is	O
yi	O
minzi	O
ci	B
where	O
zi	O
is	O
the	O
true	O
running	O
time	O
and	O
yi	O
is	O
the	O
observed	O
running	O
time	O
we	O
can	O
also	O
define	O
left	B
censored	I
and	O
interval	B
censored	I
derive	O
an	O
em	B
algorithm	O
for	O
fitting	O
a	O
linear	B
regression	B
model	O
to	O
right-censored	O
data	O
hint	O
use	O
the	O
results	O
from	O
exercise	O
see	O
figure	O
for	O
an	O
example	O
based	O
on	O
the	O
data	O
from	O
and	O
hahn	O
we	O
notice	O
that	O
the	O
em	B
line	O
is	O
tilted	O
upwards	O
more	O
since	O
the	O
model	O
takes	O
into	O
account	O
the	O
fact	O
that	O
the	O
truncated	O
values	O
are	O
actually	O
higher	O
than	O
the	O
observed	O
values	O
there	O
is	O
a	O
closely	O
related	O
model	O
in	O
econometrics	O
called	O
the	O
tobit	B
model	I
in	O
which	O
yi	O
maxzi	O
so	O
we	O
only	O
get	O
to	O
observe	O
positive	O
outcomes	O
an	O
example	O
of	O
this	O
is	O
when	O
zi	O
represents	O
desired	O
investment	O
and	O
yi	O
is	O
actual	O
investment	O
probit	B
regression	B
is	O
another	O
example	O
chapter	O
mixture	B
models	O
and	O
the	O
em	B
algorithm	O
exercise	O
posterior	B
mean	B
and	O
variance	B
of	O
a	O
truncated	B
gaussian	B
let	O
zi	O
i	O
where	O
n	O
sometimes	O
such	O
as	O
in	O
probit	B
regression	B
or	O
censored	B
regression	B
we	O
do	O
not	O
observe	O
zi	O
but	O
we	O
observe	O
the	O
fact	O
that	O
it	O
is	O
above	O
some	O
threshold	O
namely	O
we	O
observe	O
the	O
event	O
e	O
izi	O
ci	B
ci	B
i	O
exercise	O
for	O
details	O
on	O
censored	B
regression	B
and	O
section	O
for	O
probit	B
regression	B
show	O
that	O
e	O
ci	B
i	O
h	O
ci	B
i	O
and	O
ci	B
i	O
i	O
ci	B
e	O
i	O
ih	O
where	O
we	O
have	O
defined	O
hu	O
and	O
where	O
is	O
the	O
pdf	B
of	O
a	O
standard	O
gaussian	B
and	O
is	O
its	O
cdf	B
hint	O
we	O
have	O
pie	O
pie	O
pe	O
wheree	O
is	O
some	O
event	O
of	O
interest	O
hint	O
it	O
can	O
be	O
shown	O
that	O
n	O
wn	O
and	O
hence	O
d	O
dw	O
c	O
b	O
wn	O
n	O
n	O
latent	B
linear	O
models	O
factor	B
analysis	I
one	O
problem	O
with	O
mixture	B
models	O
is	O
that	O
they	O
only	O
use	O
a	O
single	O
latent	B
variable	O
to	O
generate	O
the	O
observations	O
in	O
particular	O
each	O
observation	B
can	O
only	O
come	O
from	O
one	O
of	O
k	O
prototypes	O
one	O
can	O
think	O
of	O
a	O
mixture	B
model	I
as	O
using	O
k	O
hidden	B
binary	O
variables	O
representing	O
a	O
one-hot	B
encoding	I
of	O
the	O
cluster	O
identity	O
but	O
because	O
these	O
variables	O
are	O
mutually	O
exclusive	O
the	O
model	O
is	O
still	O
limited	O
in	O
its	O
representational	O
power	O
to	O
use	O
is	O
a	O
gaussian	B
will	O
consider	O
other	O
choices	O
later	O
an	O
alternative	O
is	O
to	O
use	O
a	O
vector	O
of	O
real-valued	O
latent	B
variables	O
zi	O
r	O
pzi	O
n	O
if	O
the	O
observations	O
are	O
also	O
continuous	O
so	O
xi	O
r	O
d	O
we	O
may	O
use	O
a	O
gaussian	B
for	O
the	O
likelihood	B
just	O
as	O
in	O
linear	B
regression	B
we	O
will	O
assume	O
the	O
mean	B
is	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
thus	O
yielding	O
l	O
the	O
simplest	O
prior	O
pxizi	O
n	O
where	O
w	O
is	O
a	O
d	O
l	O
matrix	O
known	O
as	O
the	O
factor	B
loading	I
matrix	I
and	O
is	O
a	O
d	O
d	O
covariance	B
matrix	I
we	O
take	O
to	O
be	O
diagonal	B
since	O
the	O
whole	O
point	O
of	O
the	O
model	O
is	O
to	O
force	O
zi	O
to	O
explain	O
the	O
correlation	O
rather	O
than	O
baking	O
it	O
in	O
to	O
the	O
observation	B
s	O
covariance	B
this	O
overall	O
model	O
is	O
called	O
factor	B
analysis	I
or	O
fa	B
the	O
special	O
case	O
in	O
which	O
is	O
called	O
probabilistic	B
principal	B
components	I
analysis	I
or	O
ppca	B
the	O
reason	O
for	O
this	O
name	O
will	O
become	O
apparent	O
later	O
the	O
generative	O
process	O
where	O
l	O
d	O
and	O
is	O
diagonal	B
is	O
illustrated	O
in	O
figure	O
we	O
take	O
an	O
isotropic	B
gaussian	B
spray	O
can	O
and	O
slide	O
it	O
along	O
the	O
line	O
defined	O
by	O
wzi	O
this	O
induces	O
an	O
ellongated	O
hence	O
correlated	O
gaussian	B
in	O
fa	B
is	O
a	O
low	O
rank	O
parameterization	O
of	O
an	O
mvn	B
fa	B
can	O
be	O
thought	O
of	O
as	O
a	O
way	O
of	O
specifying	O
a	O
joint	O
density	O
model	O
on	O
x	O
using	O
a	O
small	O
number	O
of	O
parameters	O
to	O
see	O
this	O
note	O
that	O
from	O
equation	O
the	O
induced	O
marginal	B
distribution	I
pxi	O
is	O
a	O
gaussian	B
pxi	O
n	O
n	O
w	O
chapter	O
latent	B
linear	O
models	O
figure	O
d	O
observed	O
dimensions	O
based	O
on	O
figure	O
of	O
illustration	O
of	O
the	O
ppca	B
generative	O
process	O
where	O
we	O
have	O
l	O
latent	B
dimension	O
generating	O
from	O
this	O
we	O
see	O
that	O
we	O
can	O
set	O
without	O
loss	B
of	O
generality	O
since	O
we	O
can	O
always	O
absorb	O
w	O
into	O
similarly	O
we	O
can	O
set	O
i	O
without	O
loss	B
of	O
generality	O
because	O
we	O
can	O
always	O
emulate	O
a	O
correlated	O
prior	O
by	O
using	O
defining	O
a	O
new	O
weight	O
matrix	O
w	O
w	O
then	O
we	O
find	O
cov	O
wt	O
e	O
we	O
thus	O
see	O
that	O
fa	B
approximates	O
the	O
covariance	B
matrix	I
of	O
the	O
visible	B
vector	O
using	O
a	O
low-rank	O
wwt	O
decomposition	O
c	O
cov	O
wwt	O
this	O
only	O
uses	O
old	O
parameters	O
which	O
allows	O
a	O
flexible	O
compromise	O
between	O
a	O
full	B
covariance	B
gaussian	B
with	O
parameters	O
and	O
a	O
diagonal	B
covariance	B
with	O
od	O
parameters	O
note	O
that	O
if	O
we	O
did	O
not	O
restrict	O
to	O
be	O
diagonal	B
we	O
could	O
trivially	O
set	O
to	O
a	O
full	B
covariance	B
matrix	I
then	O
we	O
could	O
set	O
w	O
in	O
which	O
case	O
the	O
latent	B
factors	B
would	O
not	O
be	O
required	O
inference	B
of	O
the	O
latent	B
factors	B
although	O
fa	B
can	O
be	O
thought	O
of	O
as	O
just	O
a	O
way	O
to	O
define	O
a	O
density	O
on	O
x	O
it	O
is	O
often	O
used	O
because	O
we	O
hope	O
that	O
the	O
latent	B
factors	B
z	O
will	O
reveal	O
something	O
interesting	O
about	O
the	O
data	O
to	O
do	O
this	O
we	O
need	O
to	O
compute	O
the	O
posterior	O
over	O
the	O
latent	B
factors	B
we	O
can	O
use	O
bayes	B
rule	I
for	O
gaussians	O
to	O
give	O
pzixi	O
n	O
i	O
wt	O
i	O
mi	B
iwt	O
note	O
that	O
in	O
the	O
fa	B
model	O
i	O
is	O
actually	O
independent	O
of	O
i	O
so	O
we	O
can	O
denote	O
it	O
by	O
computing	O
this	O
matrix	O
takes	O
time	O
and	O
computing	O
each	O
mi	B
e	O
takes	O
ld	O
time	O
the	O
mi	B
are	O
sometimes	O
called	O
the	O
latent	B
scores	B
or	O
latent	B
factors	B
factor	B
analysis	I
rotationnone	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
width	O
wheelbase	O
length	O
weight	O
engine	O
cylinders	O
gmc	O
yukon	O
xl	O
slt	O
nissan	O
pathfinder	O
armada	O
se	O
horsepower	O
kia	O
sorento	O
lx	O
mercedes	O
benz	O
saturn	O
honda	O
insight	O
citympg	O
highwaympg	O
retail	O
dealer	O
mercedes	O
benz	O
porsche	O
component	O
figure	O
projection	B
of	O
cars	O
data	O
based	O
on	O
factor	B
analysis	I
the	O
blue	O
text	O
are	O
the	O
names	O
of	O
cars	O
corresponding	O
to	O
certain	O
chosen	O
points	O
figure	O
generated	O
by	O
fabiplotdemo	O
let	O
us	O
give	O
a	O
simple	O
example	O
based	O
we	O
consider	O
a	O
dataset	O
of	O
d	O
variables	O
and	O
n	O
cases	O
describing	O
various	O
aspects	O
of	O
cars	O
such	O
as	O
the	O
engine	O
size	O
the	O
number	O
of	O
cylinders	O
the	O
miles	O
per	O
gallon	O
the	O
price	O
etc	O
we	O
first	O
fit	O
a	O
l	O
dimensional	O
model	O
we	O
can	O
plot	O
the	O
mi	B
scores	B
as	O
points	O
in	O
r	O
to	O
visualize	O
the	O
data	O
as	O
shown	O
in	O
figure	O
to	O
get	O
a	O
better	O
understanding	O
of	O
the	O
meaning	O
of	O
the	O
latent	B
factors	B
we	O
can	O
project	O
unit	O
vectors	O
corresponding	O
to	O
each	O
of	O
the	O
feature	O
dimensions	O
etc	O
into	O
the	O
low	O
dimensional	O
space	O
these	O
are	O
shown	O
as	O
blue	O
lines	O
in	O
figure	O
this	O
is	O
known	O
as	O
a	O
biplot	B
we	O
see	O
that	O
the	O
horizontal	O
axis	O
represents	O
price	O
corresponding	O
to	O
the	O
features	B
labeled	O
dealer	O
and	O
retail	O
with	O
expensive	O
cars	O
on	O
the	O
right	O
the	O
vertical	O
axis	O
represents	O
fuel	O
efficiency	O
in	O
terms	O
of	O
mpg	O
versus	O
size	O
heavy	O
vehicles	O
are	O
less	O
efficient	O
and	O
are	O
higher	O
up	O
whereas	O
light	O
vehicles	O
are	O
more	O
efficient	O
and	O
are	O
lower	O
down	O
we	O
can	O
verify	O
this	O
interpretation	O
by	O
clicking	O
on	O
some	O
points	O
and	O
finding	O
the	O
closest	O
exemplars	O
in	O
the	O
training	B
set	I
and	O
printing	O
their	O
names	O
as	O
in	O
figure	O
however	O
in	O
general	O
interpreting	O
latent	B
variable	I
models	I
is	O
fraught	O
with	O
difficulties	O
as	O
we	O
discuss	O
in	O
section	O
unidentifiability	O
just	O
like	O
with	O
mixture	B
models	O
fa	B
is	O
also	O
unidentifiable	O
to	O
see	O
this	O
suppose	O
r	O
is	O
an	O
arbitrary	O
orthogonal	O
rotation	O
matrix	O
satisfying	O
rrt	O
i	O
let	O
us	O
define	O
w	O
wr	O
then	O
the	O
likelihood	B
chapter	O
latent	B
linear	O
models	O
function	O
of	O
this	O
modified	O
matrix	O
is	O
the	O
same	O
as	O
for	O
the	O
unmodified	O
matrix	O
since	O
cov	O
we	O
zzt	O
wt	O
e	O
wrrt	O
wt	O
wwt	O
geometrically	O
multiplying	O
w	O
by	O
an	O
orthogonal	O
matrix	O
is	O
like	O
rotating	O
z	O
before	O
generating	O
x	O
but	O
since	O
z	O
is	O
drawn	O
from	O
an	O
isotropic	B
gaussian	B
this	O
makes	O
no	O
difference	O
to	O
the	O
likelihood	B
consequently	O
we	O
cannot	O
unique	O
identify	O
w	O
and	O
therefore	O
cannot	O
uniquely	O
identify	O
the	O
latent	B
factors	B
either	O
to	O
ensure	O
a	O
unique	O
solution	O
we	O
need	O
to	O
remove	O
ll	O
degrees	B
of	I
freedom	I
since	O
that	O
in	O
total	O
the	O
fa	B
model	O
has	O
d	O
ld	O
is	O
the	O
number	O
of	O
orthonormal	O
matrices	O
of	O
size	O
l	O
ll	O
free	O
parameters	O
the	O
mean	B
where	O
the	O
first	O
term	O
arises	O
from	O
obviously	O
we	O
require	O
this	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
dd	O
which	O
is	O
the	O
number	O
of	O
parameters	O
in	O
an	O
unconstrained	O
symmetric	B
covariance	B
matrix	I
this	O
gives	O
us	O
an	O
upper	O
bound	O
on	O
l	O
as	O
follows	O
lmax	O
for	O
example	O
d	O
implies	O
l	O
but	O
we	O
usually	O
never	O
choose	O
this	O
upper	O
bound	O
since	O
it	O
would	O
result	O
in	O
overfitting	O
discussion	O
in	O
section	O
on	O
how	O
to	O
choose	O
l	O
unfortunately	O
even	O
if	O
we	O
set	O
l	O
lmax	O
we	O
still	O
cannot	O
uniquely	O
identify	O
the	O
parameters	O
since	O
the	O
rotational	O
ambiguity	O
still	O
exists	O
non-identifiability	O
does	O
not	O
affect	O
the	O
predictive	B
performance	O
of	O
the	O
model	O
however	O
it	O
does	O
affect	O
the	O
loading	O
matrix	O
and	O
hence	O
the	O
interpretation	O
of	O
the	O
latent	B
factors	B
since	O
factor	B
analysis	I
is	O
often	O
used	O
to	O
uncover	O
structure	O
in	O
the	O
data	O
this	O
problem	O
needs	O
to	O
be	O
addressed	O
here	O
are	O
some	O
commonly	O
used	O
solutions	O
forcing	O
w	O
to	O
be	O
orthonormal	O
perhaps	O
the	O
cleanest	O
solution	O
to	O
the	O
identifiability	O
problem	O
is	O
to	O
force	O
w	O
to	O
be	O
orthonormal	O
and	O
to	O
order	O
the	O
columns	O
by	O
decreasing	O
variance	B
of	O
the	O
corresponding	O
latent	B
factors	B
this	O
is	O
the	O
approach	O
adopted	O
by	O
pca	B
which	O
we	O
will	O
discuss	O
in	O
section	O
the	O
result	O
is	O
not	O
necessarily	O
more	O
interpretable	O
but	O
at	O
least	O
it	O
is	O
unique	O
forcing	O
w	O
to	O
be	O
lower	O
triangular	O
one	O
way	O
to	O
achieve	O
identifiability	O
which	O
is	O
popular	O
in	O
the	O
bayesian	B
community	O
and	O
west	O
is	O
to	O
ensure	O
that	O
the	O
first	O
visible	B
feature	O
is	O
only	O
generated	O
by	O
the	O
first	O
latent	B
factor	B
the	O
second	O
visible	B
feature	O
is	O
only	O
generated	O
by	O
the	O
first	O
two	O
latent	B
factors	B
and	O
so	O
on	O
for	O
example	O
if	O
l	O
and	O
d	O
the	O
correspond	O
factor	B
loading	I
matrix	I
is	O
given	O
by	O
w	O
we	O
also	O
require	O
that	O
wjj	O
for	O
j	O
the	O
total	O
number	O
of	O
parameters	O
in	O
this	O
constrained	O
matrix	O
is	O
d	O
dl	O
ll	O
which	O
is	O
equal	O
to	O
the	O
number	O
of	O
uniquely	O
identifiable	O
parameters	O
the	O
disadvantage	O
of	O
this	O
method	O
is	O
that	O
the	O
first	O
l	O
visible	B
variables	I
to	O
see	O
this	O
note	O
that	O
there	O
are	O
l	O
free	O
parameters	O
in	O
r	O
in	O
the	O
first	O
column	O
the	O
column	O
vector	O
must	O
be	O
normalized	O
to	O
unit	O
length	O
there	O
are	O
l	O
free	O
parameters	O
in	O
the	O
second	O
column	O
must	O
be	O
orthogonal	O
to	O
the	O
first	O
and	O
so	O
on	O
factor	B
analysis	I
qi	O
zi	O
xi	O
n	O
k	O
w	O
k	O
k	O
figure	O
mixture	B
of	I
factor	B
analysers	I
as	O
a	O
dgm	B
known	O
as	O
the	O
founder	B
variables	I
affect	O
the	O
interpretation	O
of	O
the	O
latent	B
factors	B
and	O
so	O
must	O
be	O
chosen	O
carefully	O
sparsity	B
promoting	O
priors	O
on	O
the	O
weights	O
instead	O
of	O
pre-specifying	O
which	O
entries	O
in	O
w	O
are	O
zero	O
we	O
can	O
encourage	O
the	O
entries	O
to	O
be	O
zero	O
using	O
regularization	B
et	O
al	O
ard	B
archambeau	O
and	O
bach	O
or	O
spike-and-slab	O
priors	O
et	O
al	O
this	O
is	O
called	O
sparse	B
factor	B
analysis	I
this	O
does	O
not	O
necessarily	O
ensure	O
a	O
unique	O
map	B
estimate	I
but	O
it	O
does	O
encourage	O
interpretable	O
solutions	O
see	O
section	O
choosing	O
an	O
informative	O
rotation	O
matrix	O
there	O
are	O
a	O
variety	O
of	O
heuristic	O
methods	O
that	O
try	O
to	O
find	O
rotation	O
matrices	O
r	O
which	O
can	O
be	O
used	O
to	O
modify	O
w	O
hence	O
the	O
latent	B
factors	B
so	O
as	O
to	O
try	O
to	O
increase	O
the	O
interpretability	O
typically	O
by	O
encouraging	O
them	O
to	O
be	O
sparse	B
one	O
popular	O
method	O
is	O
known	O
as	O
varimax	B
use	O
of	O
non-gaussian	O
priors	O
for	O
the	O
latent	B
factors	B
in	O
section	O
we	O
will	O
dicuss	O
how	O
replacing	O
pzi	O
with	O
a	O
non-gaussian	O
distribution	O
can	O
enable	O
us	O
to	O
sometimes	O
uniquely	O
identify	O
w	O
as	O
well	O
as	O
the	O
latent	B
factors	B
this	O
technique	O
is	O
known	O
as	O
ica	B
mixtures	O
of	O
factor	B
analysers	O
the	O
fa	B
model	O
assumes	O
that	O
the	O
data	O
lives	O
on	O
a	O
low	O
dimensional	O
linear	O
manifold	O
in	O
reality	O
most	O
data	O
is	O
better	O
modeled	O
by	O
some	O
form	O
of	O
low	O
dimensional	O
curved	O
manifold	O
we	O
can	O
approximate	O
a	O
curved	O
manifold	O
by	O
a	O
piecewise	O
linear	O
manifold	O
this	O
suggests	O
the	O
following	O
model	O
let	O
the	O
k	O
th	O
linear	O
subspace	O
of	O
dimensionality	O
lk	O
be	O
represented	O
by	O
wk	O
for	O
k	O
suppose	O
we	O
have	O
a	O
latent	B
indicator	O
qi	O
k	O
specifying	O
which	O
subspace	O
we	O
should	O
use	O
to	O
generate	O
the	O
data	O
we	O
then	O
sample	O
zi	O
from	O
a	O
gaussian	B
prior	O
and	O
pass	O
it	O
through	O
the	O
wk	O
matrix	O
k	O
qi	O
and	O
add	O
noise	O
more	O
precisely	O
the	O
model	O
is	O
as	O
follows	O
pxizi	O
qi	O
k	O
k	O
wkzi	O
pzi	O
i	O
pqi	O
catqi	O
chapter	O
latent	B
linear	O
models	O
figure	O
mixture	B
of	O
mixppcademonetlab	O
ppcas	O
fit	O
to	O
a	O
dataset	O
for	O
k	O
figure	O
generated	O
by	O
this	O
is	O
called	O
a	O
mixture	B
of	I
factor	B
analysers	I
et	O
al	O
the	O
ci	B
assumptions	O
are	O
represented	O
in	O
figure	O
another	O
way	O
to	O
think	O
about	O
this	O
model	O
is	O
as	O
a	O
low-rank	O
version	O
of	O
a	O
mixture	B
of	I
gaussians	I
in	O
particular	O
this	O
model	O
needs	O
okld	O
parameters	O
instead	O
of	O
the	O
parameters	O
needed	O
for	O
a	O
mixture	B
of	O
full	B
covariance	B
gaussians	O
this	O
can	O
reduce	O
overfitting	O
in	O
fact	O
mfa	O
is	O
a	O
good	O
generic	O
density	O
model	O
for	O
high-dimensional	O
real-valued	O
data	O
em	B
for	O
factor	B
analysis	I
models	O
using	O
the	O
results	O
from	O
chapter	O
it	O
is	O
straightforward	O
to	O
derive	O
an	O
em	B
algorithm	O
to	O
fit	O
an	O
fa	B
model	O
with	O
just	O
a	O
little	O
more	O
work	O
we	O
can	O
fit	O
a	O
mixture	B
of	O
fas	O
below	O
we	O
state	B
the	O
results	O
without	O
proof	O
the	O
derivation	O
can	O
be	O
found	O
in	O
and	O
hinton	O
however	O
deriving	O
these	O
equations	O
yourself	O
is	O
a	O
useful	O
exercise	O
if	O
you	O
want	O
to	O
become	O
proficient	O
at	O
the	O
math	O
to	O
obtain	O
the	O
results	O
for	O
a	O
single	O
factor	B
analyser	O
just	O
set	O
ric	O
and	O
c	O
in	O
the	O
equations	O
below	O
in	O
section	O
we	O
will	O
see	O
a	O
further	O
simplification	O
of	O
these	O
equations	O
that	O
arises	O
when	O
fitting	O
a	O
ppca	B
model	O
where	O
the	O
results	O
will	O
turn	O
out	O
to	O
have	O
a	O
particularly	O
simple	O
and	O
elegant	O
intepretation	O
in	O
the	O
e	B
step	I
we	O
compute	O
the	O
posterior	O
responsibility	B
of	O
cluster	O
c	O
for	O
data	O
point	O
i	O
using	O
ric	O
pqi	O
cxi	O
cn	O
c	O
wcwt	O
c	O
the	O
conditional	O
posterior	O
for	O
zi	O
is	O
given	O
by	O
pzixi	O
qi	O
c	O
ic	O
c	O
c	O
wc	O
c	O
c	O
c	O
ic	O
wt	O
mic	O
icwt	O
in	O
the	O
m	B
step	I
it	O
is	O
easiest	O
to	O
estimate	O
c	O
and	O
wc	O
at	O
the	O
same	O
time	O
by	O
defining	O
wc	O
principal	B
components	I
analysis	I
c	O
z	O
also	O
define	O
bic	B
e	O
zxi	O
qi	O
c	O
zztxi	O
qi	O
c	O
z	O
ztxi	O
qi	O
c	O
cic	O
e	O
e	O
qi	O
c	O
then	O
the	O
m	B
step	I
is	O
as	O
follows	O
wc	O
ricxibt	O
ic	O
riccic	O
e	O
t	O
i	O
xi	O
wcbic	O
ric	O
xt	O
i	O
i	O
n	O
c	O
n	O
diag	O
ic	O
ric	O
e	O
qi	O
c	O
note	O
that	O
these	O
updates	O
are	O
for	O
vanilla	O
em	B
a	O
much	O
faster	O
version	O
of	O
this	O
algorithm	O
based	O
on	O
ecm	B
is	O
described	O
in	O
and	O
yu	O
fitting	O
fa	B
models	O
with	O
missing	B
data	I
in	O
many	O
applications	O
such	O
as	O
collaborative	O
filtering	B
we	O
have	O
missing	B
data	I
one	O
virtue	O
of	O
the	O
em	B
approach	O
to	O
fitting	O
an	O
fappca	O
model	O
is	O
that	O
it	O
is	O
easy	O
to	O
extend	O
to	O
this	O
case	O
however	O
overfitting	O
can	O
be	O
a	O
problem	O
if	O
there	O
is	O
a	O
lot	O
of	O
missing	B
data	I
consequently	O
it	O
is	O
important	O
to	O
perform	O
map	O
estimation	O
or	O
to	O
use	O
bayesian	B
inference	B
see	O
e	O
g	O
and	O
raiko	O
for	O
details	O
principal	B
components	I
analysis	I
consider	O
the	O
fa	B
model	O
where	O
we	O
constrain	O
and	O
w	O
to	O
be	O
orthonormal	O
it	O
can	O
be	O
shown	O
and	O
bishop	O
that	O
as	O
this	O
model	O
reduces	O
to	O
classical	B
principal	B
components	I
analysis	I
pca	B
also	O
known	O
as	O
the	O
karhunen	B
loeve	I
transform	O
the	O
version	O
where	O
is	O
known	O
as	O
probabilistic	B
pca	B
and	O
bishop	O
or	O
sensible	B
pca	B
equivalent	O
result	O
was	O
derived	O
independently	O
from	O
a	O
different	O
perspective	O
in	O
and	O
pentland	O
to	O
make	O
sense	O
of	O
this	O
result	O
we	O
first	O
have	O
to	O
learn	O
about	O
classical	B
pca	B
we	O
then	O
connect	O
pca	B
to	O
the	O
svd	B
and	O
finally	O
we	O
return	O
to	O
discuss	O
ppca	B
classical	B
pca	B
statement	O
of	O
the	O
theorem	O
the	O
synthesis	B
view	I
of	O
classical	B
pca	B
is	O
summarized	O
in	O
the	O
forllowing	O
theorem	O
theorem	O
suppose	O
we	O
want	O
to	O
find	O
an	O
orthogonal	O
set	O
of	O
l	O
linear	O
basis	O
vectors	O
wj	O
r	O
d	O
and	O
the	O
corresponding	O
scores	B
zi	O
r	O
l	O
such	O
that	O
we	O
minimize	O
the	O
average	O
reconstruction	B
error	I
jw	O
z	O
n	O
chapter	O
latent	B
linear	O
models	O
figure	O
an	O
illustration	O
of	O
pca	B
and	O
ppca	B
where	O
d	O
and	O
l	O
circles	O
are	O
the	O
original	O
data	O
points	O
crosses	O
are	O
the	O
reconstructions	O
the	O
red	O
star	O
is	O
the	O
data	O
mean	B
pca	B
the	O
points	O
are	O
orthogonally	O
projected	O
onto	O
the	O
line	O
figure	O
generated	O
by	O
ppca	B
the	O
projection	B
is	O
no	O
longer	O
orthogonal	O
the	O
reconstructions	O
are	O
shrunk	O
towards	O
the	O
data	O
mean	B
star	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
where	O
xi	O
wzi	O
subject	O
to	O
the	O
constraint	O
that	O
w	O
is	O
orthonormal	O
equivalently	O
we	O
can	O
write	O
this	O
objective	O
as	O
follows	O
jw	O
z	O
where	O
z	O
is	O
an	O
n	O
l	O
matrix	O
with	O
the	O
zi	O
in	O
its	O
rows	O
and	O
is	O
the	O
frobenius	B
norm	I
of	O
matrix	O
a	O
defined	O
by	O
f	O
trat	O
a	O
ij	O
the	O
optimal	O
solution	O
is	O
obtained	O
by	O
setting	O
w	O
vl	O
where	O
vl	O
contains	O
the	O
l	O
eigenvectors	O
with	O
largest	O
eigenvalues	O
of	O
the	O
empirical	O
covariance	B
matrix	I
i	O
assume	O
the	O
n	O
xi	O
have	O
zero	O
mean	B
for	O
notational	O
simplicity	O
furthermore	O
the	O
optimal	O
low-dimensional	O
encoding	O
of	O
the	O
data	O
is	O
given	O
by	O
zi	O
wt	O
xi	O
which	O
is	O
an	O
orthogonal	B
projection	B
of	O
the	O
data	O
onto	O
the	O
column	O
space	O
spanned	O
by	O
the	O
eigenvectors	O
xixt	O
an	O
example	O
of	O
this	O
is	O
shown	O
in	O
figure	O
for	O
d	O
and	O
l	O
the	O
diagonal	B
line	O
is	O
the	O
vector	O
this	O
is	O
called	O
the	O
first	O
principal	B
component	I
or	O
principal	O
direction	O
the	O
data	O
points	O
xi	O
r	O
are	O
orthogonally	O
projected	O
onto	O
this	O
line	O
to	O
get	O
zi	O
r	O
this	O
is	O
the	O
best	O
approximation	O
to	O
the	O
data	O
will	O
discuss	O
figure	O
later	O
in	O
general	O
it	O
is	O
hard	O
to	O
visualize	O
higher	O
dimensional	O
data	O
but	O
if	O
the	O
data	O
happens	O
to	O
be	O
a	O
set	O
of	O
images	O
it	O
is	O
easy	O
to	O
do	O
so	O
figure	O
shows	O
the	O
first	O
three	O
principal	O
vectors	O
reshaped	O
as	O
images	O
as	O
well	O
as	O
the	O
reconstruction	O
of	O
a	O
specific	O
image	O
using	O
a	O
varying	O
number	O
of	O
basis	O
vectors	O
discuss	O
how	O
to	O
choose	O
l	O
in	O
section	O
below	O
we	O
will	O
show	O
that	O
the	O
principal	O
directions	O
are	O
the	O
ones	O
along	O
which	O
the	O
data	O
shows	O
maximal	O
variance	B
this	O
means	O
that	O
pca	B
can	O
be	O
misled	O
by	O
directions	O
in	O
which	O
the	O
variance	B
is	O
high	O
merely	O
because	O
of	O
the	O
measurement	O
scale	O
figure	O
shows	O
an	O
example	O
where	O
the	O
vertical	O
axis	O
uses	O
a	O
large	O
range	O
than	O
the	O
horizontal	O
axis	O
resulting	O
in	O
a	O
line	O
that	O
it	O
is	O
therefore	O
standard	O
practice	O
to	O
standardize	O
the	O
data	O
first	O
or	O
looks	O
somewhat	O
unnatural	O
principal	B
components	I
analysis	I
mean	B
principal	O
basis	O
reconstructed	O
with	O
bases	O
reconstructed	O
with	O
bases	O
principal	O
basis	O
principal	O
basis	O
reconstructed	O
with	O
bases	O
reconstructed	O
with	O
bases	O
figure	O
the	O
mean	B
and	O
the	O
first	O
three	O
pc	O
basis	O
vectors	O
based	O
on	O
images	O
of	O
the	O
digit	O
the	O
mnist	B
dataset	O
reconstruction	O
of	O
an	O
image	O
based	O
on	O
and	O
all	O
the	O
basis	O
vectors	O
figure	O
generated	O
by	O
pcaimagedemo	O
t	O
i	O
h	O
g	O
e	O
w	O
height	O
t	O
i	O
h	O
g	O
e	O
w	O
height	O
figure	O
effect	O
of	O
standardization	O
on	O
pca	B
applied	O
to	O
the	O
height	O
weight	O
dataset	O
left	O
pca	B
of	O
raw	O
data	O
right	O
pca	B
of	O
standardized	B
data	O
figure	O
generated	O
by	O
pcademoheightweight	O
equivalently	O
to	O
work	O
with	O
correlation	O
matrices	O
instead	O
of	O
covariance	B
matrices	O
the	O
benefits	O
of	O
this	O
are	O
apparent	O
from	O
figure	O
proof	O
proof	O
we	O
use	O
wj	O
r	O
high-dimensional	O
observation	B
zi	O
r	O
zj	O
r	O
vectors	O
points	O
r	O
d	O
to	O
denote	O
the	O
j	O
th	O
principal	O
direction	O
xi	O
r	O
d	O
to	O
denote	O
the	O
i	O
th	O
l	O
to	O
denote	O
the	O
i	O
th	O
low-dimensional	O
representation	O
and	O
n	O
to	O
denote	O
the	O
zn	O
j	O
which	O
is	O
the	O
j	O
th	O
component	O
of	O
all	O
the	O
low-dimensional	O
let	O
us	O
start	O
by	O
estimating	O
the	O
best	O
solution	O
r	O
d	O
and	O
the	O
corresponding	O
projected	O
n	O
we	O
will	O
find	O
the	O
remaining	O
bases	O
etc	O
later	O
the	O
reconstruction	B
error	I
is	O
given	O
by	O
n	O
n	O
n	O
chapter	O
latent	B
linear	O
models	O
n	O
i	O
xi	O
xi	O
i	O
xi	O
xi	O
the	O
orthonormality	O
assumption	O
taking	O
derivatives	O
wrt	O
and	O
equating	O
since	O
wt	O
to	O
zero	O
gives	O
n	O
xi	O
wt	O
xi	O
so	O
the	O
optimal	O
reconstruction	O
weights	O
are	O
obtained	O
by	O
orthogonally	O
projecting	O
the	O
data	O
onto	O
the	O
first	O
principal	O
direction	O
figure	O
plugging	O
back	O
in	O
gives	O
n	O
i	O
xi	O
const	O
n	O
now	O
the	O
variance	B
of	O
the	O
projected	O
coordinates	O
is	O
given	O
by	O
n	O
xt	O
i	O
e	O
t	O
var	B
e	O
since	O
e	O
e	O
because	O
the	O
data	O
has	O
been	O
centered	O
from	O
this	O
we	O
see	O
that	O
minimizing	O
the	O
reconstruction	B
error	I
is	O
equivalent	O
to	O
maximizing	O
the	O
variance	B
of	O
the	O
projected	O
data	O
i	O
e	O
arg	O
min	O
arg	O
max	O
var	B
this	O
is	O
why	O
it	O
is	O
often	O
said	O
that	O
pca	B
finds	O
the	O
directions	O
of	O
maximal	O
variance	B
this	O
is	O
called	O
the	O
analysis	B
view	I
of	O
pca	B
the	O
variance	B
of	O
the	O
projected	O
data	O
can	O
be	O
written	O
as	O
wt	O
xixt	O
i	O
wt	O
i	O
xixt	O
i	O
is	O
the	O
empirical	O
covariance	B
matrix	I
correlation	B
matrix	I
if	O
the	O
n	O
n	O
where	O
n	O
data	O
is	O
standardized	B
principal	B
components	I
analysis	I
we	O
can	O
trivially	O
maximize	O
the	O
variance	B
of	O
the	O
projection	B
hence	O
minimize	O
the	O
reconstruction	B
error	I
by	O
letting	O
so	O
we	O
impose	O
the	O
constraint	O
and	O
instead	O
maximize	O
wt	O
where	O
is	O
the	O
lagrange	B
multiplier	I
taking	O
derivatives	O
and	O
equating	O
to	O
zero	O
we	O
have	O
hence	O
the	O
direction	O
that	O
maximizes	O
the	O
variance	B
is	O
an	O
eigenvector	O
of	O
the	O
covariance	B
matrix	I
left	O
multiplying	O
by	O
using	O
wt	O
we	O
find	O
that	O
the	O
variance	B
of	O
the	O
projected	O
data	O
is	O
wt	O
since	O
we	O
want	O
to	O
maximize	O
the	O
variance	B
we	O
pick	O
the	O
eigenvector	O
which	O
corresponds	O
to	O
the	O
largest	O
eigenvalue	O
now	O
let	O
us	O
find	O
another	O
direction	O
to	O
further	O
minimize	O
the	O
reconstruction	B
error	I
subject	O
to	O
and	O
wt	O
the	O
error	O
is	O
wt	O
n	O
optimizing	O
wrt	O
and	O
gives	O
the	O
same	O
solution	O
as	O
before	O
exercise	O
asks	O
you	O
to	O
show	O
that	O
j	O
in	O
other	O
words	O
the	O
second	O
principal	O
encoding	O
is	O
gotten	O
by	O
projecting	O
onto	O
the	O
second	O
principal	O
direction	O
substituting	O
in	O
yields	O
yields	O
wt	O
xi	O
n	O
i	O
xi	O
wt	O
xixt	O
i	O
wt	O
xixt	O
i	O
const	O
wt	O
dropping	O
the	O
constant	O
term	O
and	O
adding	O
the	O
constraints	O
yields	O
wt	O
exercise	O
asks	O
you	O
to	O
show	O
that	O
the	O
solution	O
is	O
given	O
by	O
the	O
eigenvector	O
with	O
the	O
second	O
largest	O
eigenvalue	O
the	O
proof	O
continues	O
in	O
this	O
way	O
one	O
can	O
use	O
induction	B
chapter	O
latent	B
linear	O
models	O
singular	B
value	I
decomposition	I
we	O
have	O
defined	O
the	O
solution	O
to	O
pca	B
in	O
terms	O
of	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
however	O
there	O
is	O
another	O
way	O
to	O
obtain	O
the	O
solution	O
based	O
on	O
the	O
singular	B
value	I
decomposition	I
or	O
svd	B
this	O
basically	O
generalizes	O
the	O
notion	O
of	O
eigenvectors	O
from	O
square	O
matrices	O
to	O
any	O
kind	O
of	O
matrix	O
in	O
particular	O
any	O
n	O
d	O
matrix	O
x	O
can	O
be	O
decomposed	O
as	O
follows	O
n	O
d	O
n	O
n	O
n	O
d	O
d	O
d	O
where	O
u	O
is	O
an	O
n	O
n	O
matrix	O
whose	O
columns	O
are	O
orthornormal	O
ut	O
u	O
in	O
v	O
is	O
d	O
d	O
matrix	O
whose	O
rows	O
and	O
columns	O
are	O
orthonormal	O
vt	O
v	O
vvt	O
id	O
and	O
s	O
is	O
a	O
n	O
d	O
matrix	O
containing	O
the	O
r	O
minn	O
d	O
singular	B
values	I
i	O
on	O
the	O
main	O
diagonal	B
with	O
filling	O
the	O
rest	O
of	O
the	O
matrix	O
the	O
columns	O
of	O
u	O
are	O
the	O
left	O
singular	O
vectors	O
and	O
the	O
columns	O
of	O
v	O
are	O
the	O
right	O
singular	O
vectors	O
see	O
figure	O
for	O
an	O
example	O
since	O
there	O
are	O
at	O
most	O
d	O
singular	B
values	I
n	O
d	O
the	O
last	O
n	O
d	O
columns	O
of	O
u	O
are	O
irrelevant	O
since	O
they	O
will	O
be	O
multiplied	O
by	O
the	O
economy	B
sized	I
svd	B
orthin	O
svd	B
avoids	O
computing	O
these	O
unnecessary	O
elements	O
let	O
us	O
denote	O
this	O
decomposition	O
by	O
u	O
s	O
v	O
if	O
n	O
d	O
we	O
have	O
n	O
d	O
n	O
d	O
d	O
d	O
d	O
d	O
as	O
in	O
figure	O
if	O
n	O
d	O
we	O
have	O
n	O
d	O
n	O
n	O
n	O
n	O
n	O
d	O
computing	O
the	O
economy-sized	O
svd	B
takes	O
on	O
d	O
minn	O
d	O
time	O
and	O
van	O
loan	O
the	O
connection	O
between	O
eigenvectors	O
and	O
singular	O
vectors	O
is	O
the	O
following	O
for	O
an	O
arbitrary	O
real	O
matrix	O
x	O
ifx	O
usvt	O
we	O
have	O
xt	O
x	O
vst	O
ut	O
usvt	O
vst	O
svt	O
vdvt	O
where	O
d	O
is	O
a	O
diagonal	B
matrix	O
containing	O
the	O
squares	O
singular	B
values	I
hence	O
xv	O
vd	O
so	O
the	O
eigenvectors	O
of	O
xt	O
x	O
are	O
equal	O
to	O
v	O
the	O
right	O
singular	O
vectors	O
of	O
x	O
and	O
the	O
eigenvalues	O
of	O
xt	O
x	O
are	O
equal	O
to	O
d	O
the	O
squared	O
singular	B
values	I
similarly	O
xxt	O
usvt	O
vst	O
ut	O
usst	O
usst	O
ud	O
so	O
the	O
eigenvectors	O
of	O
xxt	O
are	O
equal	O
to	O
u	O
the	O
left	O
singular	O
vectors	O
of	O
x	O
also	O
the	O
eigenvalues	O
of	O
xxt	O
are	O
equal	O
to	O
the	O
squared	O
singular	B
values	I
we	O
can	O
summarize	O
all	O
this	O
as	O
follows	O
u	O
evecxxt	O
v	O
evecxt	O
x	O
evalxxt	O
evalxt	O
x	O
principal	B
components	I
analysis	I
d	O
d	O
n	O
d	O
n	O
x	O
u	O
d	O
d	O
s	O
d	O
d	O
v	O
t	O
n	O
d	O
x	O
l	O
l	O
l	O
d	O
l	O
u	O
l	O
sl	O
v	O
t	O
l	O
figure	O
svd	B
decomposition	O
of	O
non-square	O
matrices	O
x	O
usvt	O
the	O
shaded	O
parts	O
of	O
s	O
and	O
all	O
the	O
off-diagonal	O
terms	O
are	O
zero	O
the	O
shaded	O
entries	O
in	O
u	O
and	O
s	O
are	O
not	O
computed	O
in	O
the	O
economy-sized	O
version	O
since	O
they	O
are	O
not	O
needed	O
truncated	B
svd	B
approximation	O
of	O
rank	O
l	O
since	O
the	O
eigenvectors	O
are	O
unaffected	O
by	O
linear	O
scaling	O
of	O
a	O
matrix	O
we	O
see	O
that	O
the	O
right	O
singular	O
vectors	O
of	O
x	O
are	O
equal	O
to	O
the	O
eigenvectors	O
of	O
the	O
empirical	O
covariance	B
furthermore	O
the	O
eigenvalues	O
of	O
are	O
a	O
scaled	O
version	O
of	O
the	O
squared	O
singular	B
values	I
this	O
means	O
we	O
can	O
perform	O
pca	B
using	O
just	O
a	O
few	O
lines	O
of	O
code	O
pcapmtk	O
however	O
the	O
connection	O
between	O
pca	B
and	O
svd	B
goes	O
deeper	O
from	O
equation	O
we	O
can	O
represent	O
a	O
rank	O
r	O
matrix	O
as	O
follows	O
vt	O
x	O
vt	O
r	O
ur	O
r	O
if	O
the	O
singular	B
values	I
die	O
off	O
quickly	O
as	O
in	O
figure	O
we	O
can	O
produce	O
a	O
rank	O
l	O
approximation	O
to	O
the	O
matrix	O
as	O
follows	O
x	O
vt	O
this	O
is	O
called	O
a	O
truncated	B
svd	B
figure	O
the	O
total	O
number	O
of	O
parameters	O
needed	O
to	O
represent	O
an	O
n	O
d	O
matrix	O
using	O
a	O
rank	O
l	O
approximation	O
is	O
n	O
l	O
ld	O
l	O
ln	O
d	O
chapter	O
latent	B
linear	O
models	O
rank	O
rank	O
rank	O
rank	O
figure	O
low	O
rank	O
approximations	O
to	O
an	O
image	O
top	O
left	O
the	O
original	O
image	O
is	O
of	O
size	O
so	O
has	O
rank	O
subsequent	O
images	O
have	O
ranks	O
and	O
figure	O
generated	O
by	O
svdimagedemo	O
original	O
randomized	O
i	O
g	O
o	O
l	O
i	O
figure	O
first	O
log	O
singular	B
values	I
for	O
the	O
clown	O
image	O
red	O
line	O
and	O
for	O
a	O
data	O
matrix	O
obtained	O
by	O
randomly	O
shuffling	O
the	O
pixels	O
green	O
line	O
figure	O
generated	O
by	O
svdimagedemo	O
principal	B
components	I
analysis	I
as	O
an	O
example	O
consider	O
the	O
pixel	O
image	O
in	O
figure	O
left	O
this	O
has	O
numbers	O
in	O
it	O
we	O
see	O
that	O
a	O
rank	O
approximation	O
with	O
only	O
numbers	O
is	O
a	O
very	O
good	O
approximation	O
one	O
can	O
show	O
that	O
the	O
error	O
in	O
this	O
approximation	O
is	O
given	O
by	O
xlf	O
furthermore	O
one	O
can	O
show	O
that	O
the	O
svd	B
offers	O
the	O
best	O
rank	O
l	O
approximation	O
to	O
a	O
matrix	O
in	O
the	O
sense	O
of	O
minimizing	O
the	O
above	O
frobenius	B
norm	I
let	O
us	O
connect	O
this	O
back	O
to	O
pca	B
let	O
x	O
usvt	O
be	O
a	O
truncated	B
svd	B
of	O
x	O
we	O
know	O
that	O
w	O
v	O
and	O
that	O
z	O
x	O
w	O
so	O
z	O
usvt	O
v	O
us	O
furthermore	O
the	O
optimal	O
reconstruction	O
is	O
given	O
by	O
x	O
z	O
wt	O
so	O
we	O
find	O
x	O
usvt	O
this	O
is	O
precisely	O
the	O
same	O
as	O
a	O
truncated	B
svd	B
approximation	O
this	O
is	O
another	O
illustration	O
of	O
the	O
fact	O
that	O
pca	B
is	O
the	O
best	O
low	O
rank	O
approximation	O
to	O
the	O
data	O
probabilistic	B
pca	B
we	O
are	O
now	O
ready	O
to	O
revisit	O
ppca	B
one	O
can	O
show	O
the	O
following	O
remarkable	O
result	O
theorem	O
and	O
bishop	O
consider	O
a	O
factor	B
analysis	I
model	O
in	O
which	O
and	O
w	O
is	O
orthogonal	O
the	O
observed	B
data	I
log	I
likelihood	B
is	O
given	O
by	O
i	O
c	O
n	O
xt	O
log	O
pxw	O
n	O
lnc	O
trc	O
where	O
c	O
wwt	O
and	O
s	O
n	O
data	O
for	O
notational	O
simplicity	O
the	O
maxima	O
of	O
the	O
log-likelihood	O
are	O
given	O
by	O
i	O
x	O
are	O
assuming	O
centered	O
lnc	O
xixt	O
w	O
v	O
r	O
where	O
r	O
is	O
an	O
arbitrary	O
l	O
l	O
orthogonal	O
matrix	O
v	O
is	O
the	O
d	O
l	O
matrix	O
whose	O
columns	O
are	O
the	O
first	O
l	O
eigenvectors	O
of	O
s	O
and	O
is	O
the	O
corresponding	O
diagonal	B
matrix	O
of	O
eigenvalues	O
without	O
loss	B
of	O
generality	O
we	O
can	O
set	O
r	O
i	O
furthermore	O
the	O
mle	B
of	O
the	O
noise	O
variance	B
is	O
given	O
by	O
j	O
d	O
l	O
which	O
is	O
the	O
average	O
variance	B
associated	O
with	O
the	O
discarded	O
dimensions	O
thus	O
as	O
we	O
have	O
w	O
v	O
as	O
in	O
classical	B
pca	B
what	O
about	O
z	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
posterior	O
over	O
the	O
latent	B
factors	B
is	O
given	O
by	O
pzixi	O
f	O
wt	O
xi	O
f	O
f	O
wt	O
w	O
chapter	O
latent	B
linear	O
models	O
not	O
confuse	O
f	O
wt	O
w	O
with	O
c	O
wwt	O
hence	O
as	O
we	O
find	O
w	O
v	O
f	O
i	O
and	O
zi	O
vt	O
xi	O
thus	O
the	O
posterior	B
mean	B
is	O
obtained	O
by	O
an	O
orthogonal	B
projection	B
of	O
the	O
data	O
onto	O
the	O
column	O
space	O
of	O
v	O
as	O
in	O
classical	B
pca	B
note	O
however	O
that	O
if	O
the	O
posterior	B
mean	B
is	O
not	O
an	O
orthogonal	B
projection	B
since	O
it	O
is	O
shrunk	O
somewhat	O
towards	O
the	O
prior	O
mean	B
as	O
illustrated	O
in	O
figure	O
this	O
sounds	O
like	O
an	O
undesirable	O
property	O
but	O
it	O
means	O
that	O
the	O
reconstructions	O
will	O
be	O
closer	O
to	O
the	O
overall	O
data	O
mean	B
x	O
em	B
algorithm	O
for	O
pca	B
although	O
the	O
usual	O
way	O
to	O
fit	O
a	O
pca	B
model	O
uses	O
eigenvector	O
methods	O
or	O
the	O
svd	B
we	O
can	O
also	O
use	O
em	B
which	O
will	O
turn	O
out	O
to	O
have	O
some	O
advantages	O
that	O
we	O
discuss	O
below	O
em	B
for	O
pca	B
relies	O
on	O
the	O
probabilistic	O
formulation	O
of	O
pca	B
however	O
the	O
algorithm	O
continues	O
to	O
work	O
in	O
the	O
zero	O
noise	O
limit	O
as	O
shown	O
by	O
let	O
z	O
be	O
a	O
l	O
n	O
matrix	O
storing	O
the	O
posterior	O
means	O
representations	O
let	O
x	O
xt	O
store	O
the	O
original	O
data	O
along	O
its	O
columns	O
from	O
along	O
its	O
columns	O
similarly	O
equation	O
when	O
we	O
have	O
z	O
w	O
x	O
this	O
constitutes	O
the	O
e	B
step	I
notice	O
that	O
this	O
is	O
just	O
an	O
orthogonal	B
projection	B
of	O
the	O
data	O
from	O
equation	O
the	O
m	B
step	I
is	O
given	O
by	O
w	O
xie	O
t	O
e	O
e	O
t	O
i	O
i	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
cov	O
when	O
it	O
is	O
worth	O
comparing	O
this	O
expression	O
to	O
the	O
mle	B
for	O
multi-output	O
linear	B
regression	B
which	O
has	O
the	O
form	O
thus	O
we	O
see	O
that	O
the	O
m	B
step	I
is	O
like	O
linear	B
regression	B
where	O
we	O
w	O
replace	O
the	O
observed	O
inputs	O
by	O
the	O
expected	O
values	O
of	O
the	O
latent	B
variables	O
i	O
xixt	O
i	O
i	O
yixt	O
i	O
in	O
summary	O
here	O
is	O
the	O
entire	O
algorithm	O
e	B
step	I
z	O
w	O
m	O
stepw	O
x	O
zt	O
z	O
zt	O
x	O
and	O
bishop	O
showed	O
that	O
the	O
only	O
stable	B
fixed	O
point	O
of	O
the	O
em	B
algorithm	O
is	O
the	O
globally	O
optimal	O
solution	O
that	O
is	O
the	O
em	B
algorithm	O
converges	O
to	O
a	O
solution	O
where	O
w	O
spans	O
the	O
same	O
linear	O
subspace	O
as	O
that	O
defined	O
by	O
the	O
first	O
l	O
eigenvectors	O
however	O
if	O
we	O
want	O
w	O
to	O
be	O
orthogonal	O
and	O
to	O
contain	O
the	O
eigenvectors	O
in	O
descending	O
order	O
of	O
eigenvalue	O
we	O
have	O
to	O
orthogonalize	O
the	O
resulting	O
matrix	O
can	O
be	O
done	O
quite	O
cheaply	O
alternatively	O
we	O
can	O
modify	O
em	B
to	O
give	O
the	O
principal	O
basis	O
directly	O
and	O
oh	O
this	O
algorithm	O
has	O
a	O
simple	O
physical	O
analogy	O
in	O
the	O
case	O
d	O
and	O
l	O
attached	O
by	O
springs	O
to	O
a	O
rigid	O
rod	O
whose	O
orientation	O
is	O
defined	O
by	O
a	O
consider	O
some	O
points	O
in	O
r	O
vector	O
w	O
let	O
zi	O
be	O
the	O
location	O
where	O
the	O
i	O
th	O
spring	O
attaches	O
to	O
the	O
rod	O
in	O
the	O
e	B
step	I
we	O
hold	O
the	O
rod	O
fixed	O
and	O
let	O
the	O
attachment	O
points	O
slide	O
around	O
so	O
as	O
to	O
minimize	O
the	O
spring	O
energy	O
is	O
proportional	O
to	O
the	O
sum	O
of	O
squared	O
residuals	O
in	O
the	O
m	B
step	I
we	O
hold	O
the	O
attachment	O
principal	B
components	I
analysis	I
e	B
step	I
m	B
step	I
e	B
step	I
m	B
step	I
illustration	O
of	O
em	B
for	O
pca	B
when	O
d	O
and	O
l	O
green	O
stars	O
are	O
the	O
original	O
data	O
points	O
figure	O
black	O
circles	O
are	O
their	O
reconstructions	O
the	O
weight	B
vector	I
w	O
is	O
represented	O
by	O
blue	O
line	O
we	O
start	O
with	O
a	O
random	O
initial	O
guess	O
of	O
w	O
the	O
e	B
step	I
is	O
represented	O
by	O
the	O
orthogonal	O
projections	O
we	O
update	O
the	O
rod	O
w	O
in	O
the	O
m	B
step	I
keeping	O
the	O
projections	O
onto	O
the	O
rod	O
circles	O
fixed	O
another	O
e	B
step	I
the	O
black	O
circles	O
can	O
slide	O
along	O
the	O
rod	O
but	O
the	O
rod	O
stays	O
fixed	O
another	O
m	B
step	I
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
pcaemstepbystep	O
points	O
fixed	O
and	O
let	O
the	O
rod	O
rotate	O
so	O
as	O
to	O
minimize	O
the	O
spring	O
energy	O
see	O
figure	O
for	O
an	O
illustration	O
apart	O
from	O
this	O
pleasing	O
intuitive	O
interpretation	O
em	B
for	O
pca	B
has	O
the	O
following	O
advantages	O
over	O
eigenvector	O
methods	O
em	B
can	O
be	O
faster	O
in	O
particular	O
assuming	O
n	O
d	O
l	O
the	O
dominant	O
cost	O
of	O
em	B
is	O
the	O
projection	B
operation	O
in	O
the	O
e	B
step	I
so	O
the	O
overall	O
time	O
is	O
ot	O
ln	O
d	O
where	O
t	O
is	O
the	O
number	O
of	O
chapter	O
latent	B
linear	O
models	O
figure	O
illustration	O
of	O
estimating	O
the	O
effective	O
dimensionalities	O
in	O
a	O
mixture	B
of	I
factor	B
analysers	I
using	O
vbem	B
the	O
blank	O
columns	O
have	O
been	O
forced	O
to	O
via	O
the	O
ard	B
mechanism	O
the	O
data	O
was	O
generated	O
from	O
clusters	B
with	O
intrinsic	O
dimensionalities	O
of	O
which	O
the	O
method	O
has	O
successfully	O
estimated	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
matt	O
beal	O
iterations	O
showed	O
experimentally	O
that	O
the	O
number	O
of	O
iterations	O
is	O
usually	O
very	O
small	O
mean	B
was	O
regardless	O
of	O
n	O
or	O
d	O
results	O
depends	O
on	O
the	O
ratio	O
of	O
eigenvalues	O
of	O
the	O
empirical	O
covariance	B
matrix	I
this	O
is	O
much	O
faster	O
than	O
the	O
ominn	O
dn	O
time	O
required	O
by	O
straightforward	O
eigenvector	O
methods	O
although	O
more	O
sophisticated	O
eigenvector	O
methods	O
such	O
as	O
the	O
lanczos	B
algorithm	I
have	O
running	O
times	O
comparable	O
to	O
em	B
em	B
can	O
be	O
implemented	O
in	O
an	O
online	O
fashion	O
i	O
e	O
we	O
can	O
update	O
our	O
estimate	O
of	O
w	O
as	O
the	O
data	O
streams	O
in	O
em	B
can	O
handle	O
missing	B
data	I
in	O
a	O
simple	O
way	O
section	O
em	B
can	O
be	O
extended	O
to	O
handle	O
mixtures	O
of	O
ppca	B
fa	B
models	O
em	B
can	O
be	O
modified	O
to	O
variational	B
em	B
or	O
to	O
variational	B
bayes	I
em	B
to	O
fit	O
more	O
complex	O
models	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
in	O
section	O
we	O
discussed	O
how	O
to	O
choose	O
the	O
number	O
of	O
components	O
k	O
in	O
a	O
mixture	B
model	I
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
choose	O
the	O
number	O
of	O
latent	B
dimensions	O
l	O
in	O
a	O
fapca	O
model	O
model	B
selection	I
for	O
fappca	O
argmaxl	O
pld	O
however	O
if	O
we	O
use	O
a	O
probabilistic	O
model	O
we	O
can	O
in	O
principle	O
compute	O
l	O
there	O
are	O
two	O
problems	O
with	O
this	O
first	O
evaluating	O
the	O
marginal	B
likelihood	B
for	O
lvms	O
is	O
quite	O
difficult	O
lower	O
bounds	O
section	O
can	O
be	O
used	O
also	O
alternatively	O
we	O
can	O
use	O
the	O
cross-validated	O
likelihood	B
as	O
a	O
performance	O
measure	O
although	O
this	O
can	O
be	O
slow	O
since	O
it	O
requires	O
fitting	O
each	O
model	O
f	O
times	O
where	O
f	O
is	O
the	O
number	O
of	O
cv	B
folds	B
in	O
practice	O
simple	O
approximations	O
such	O
as	O
bic	B
or	O
variational	O
the	O
second	O
issue	O
is	O
the	O
need	O
to	O
search	O
over	O
a	O
potentially	O
large	O
number	O
of	O
models	O
the	O
usual	O
approach	O
is	O
to	O
perform	O
exhaustive	O
search	O
over	O
all	O
candidate	O
values	O
of	O
l	O
however	O
sometimes	O
we	O
can	O
set	O
the	O
model	O
to	O
its	O
maximal	O
size	O
and	O
then	O
use	O
a	O
technique	O
called	O
automatic	B
relevancy	I
determination	I
combined	O
with	O
em	B
to	O
automatically	O
prune	O
out	O
irrelevant	O
weights	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
number	O
of	O
points	O
per	O
cluster	O
intrinsic	O
dimensionalities	O
figure	O
we	O
show	O
the	O
estimated	O
number	O
of	O
clusters	B
and	O
their	O
estimated	O
dimensionalities	O
as	O
a	O
function	O
of	O
sample	O
size	O
the	O
vbem	B
algorithm	O
found	O
two	O
different	O
solutions	O
when	O
n	O
note	O
that	O
more	O
clusters	B
with	O
larger	O
effective	O
dimensionalities	O
are	O
discovered	O
as	O
the	O
sample	O
sizes	O
increases	O
source	O
table	O
of	O
used	O
with	O
kind	O
permission	O
of	O
matt	O
beal	O
this	O
technique	O
will	O
be	O
described	O
in	O
a	O
supervised	O
context	O
in	O
chapter	O
but	O
can	O
be	O
adapted	O
to	O
the	O
context	O
as	O
shown	O
in	O
ghahramani	O
and	O
beal	O
figure	O
illustrates	O
this	O
approach	O
applied	O
to	O
a	O
mixture	B
of	O
fas	O
fit	O
to	O
a	O
small	O
synthetic	O
dataset	O
the	O
figures	O
visualize	O
the	O
weight	O
matrices	O
for	O
each	O
cluster	O
using	O
hinton	B
diagrams	I
where	O
where	O
the	O
size	O
of	O
the	O
square	O
is	O
proportional	O
to	O
the	O
value	O
of	O
the	O
entry	O
in	O
the	O
we	O
see	O
that	O
many	O
of	O
them	O
are	O
sparse	B
figure	O
shows	O
that	O
the	O
degree	B
of	O
sparsity	B
depends	O
on	O
the	O
amount	O
of	O
training	O
data	O
in	O
accord	O
with	O
the	O
bayesian	B
occam	O
s	O
razor	O
in	O
particular	O
when	O
the	O
sample	O
size	O
is	O
small	O
the	O
method	O
automatically	O
prefers	O
simpler	O
models	O
but	O
as	O
the	O
sample	O
size	O
gets	O
sufficiently	O
large	O
the	O
method	O
converges	O
on	O
the	O
correct	O
solution	O
which	O
is	O
one	O
with	O
subspaces	O
of	O
dimensionality	O
and	O
although	O
the	O
ard	B
em	B
method	O
is	O
elegant	O
it	O
still	O
needs	O
to	O
perform	O
search	O
over	O
k	O
this	O
is	O
done	O
using	O
birth	O
and	O
death	B
moves	I
and	O
beal	O
an	O
alternative	O
approach	O
is	O
to	O
perform	O
stochastic	O
sampling	O
in	O
the	O
space	O
of	O
models	O
traditional	O
approaches	O
such	O
as	O
and	O
west	O
are	O
based	O
on	O
reversible	B
jump	I
mcmc	B
and	O
also	O
use	O
birth	O
and	O
death	B
moves	I
however	O
this	O
can	O
be	O
slow	O
and	O
difficult	O
to	O
implement	O
more	O
recent	O
approaches	O
use	O
non-parametric	O
priors	O
combined	O
with	O
gibbs	B
sampling	I
see	O
e	O
g	O
and	O
carin	O
model	B
selection	I
for	O
pca	B
since	O
pca	B
is	O
not	O
a	O
probabilistic	O
model	O
we	O
cannot	O
use	O
any	O
of	O
the	O
methods	O
described	O
above	O
an	O
obvious	O
proxy	O
for	O
the	O
likelihood	B
is	O
the	O
reconstruction	B
error	I
ed	O
l	O
i	O
d	O
in	O
the	O
case	O
of	O
pca	B
the	O
reconstruction	O
is	O
given	O
by	O
by	O
xi	O
wzi	O
where	O
zi	O
wt	O
and	O
w	O
and	O
are	O
estimated	O
from	O
dtrain	O
geoff	O
hinton	O
is	O
an	O
english	O
professor	O
of	O
computer	O
science	O
at	O
the	O
university	O
of	O
toronto	O
chapter	O
latent	B
linear	O
models	O
train	O
set	O
reconstruction	B
error	I
test	O
set	O
reconstruction	B
error	I
e	O
s	O
m	O
r	O
num	O
pcs	O
e	O
s	O
m	O
r	O
num	O
pcs	O
figure	O
reconstruction	B
error	I
on	O
mnist	B
vs	O
number	O
of	O
latent	B
dimensions	O
used	O
by	O
pca	B
training	B
set	I
test	O
set	O
figure	O
generated	O
by	O
pcaoverfitdemo	O
figure	O
plots	O
edtrain	O
l	O
vs	O
l	O
on	O
the	O
mnist	B
training	O
data	O
in	O
figure	O
we	O
see	O
that	O
it	O
drops	O
off	O
quite	O
quickly	O
indicating	O
that	O
we	O
can	O
capture	O
most	O
of	O
the	O
empirical	O
correlation	O
of	O
the	O
pixels	O
with	O
a	O
small	O
number	O
of	O
factors	B
as	O
illustrated	O
qualitatively	O
in	O
figure	O
exercise	O
asks	O
you	O
to	O
prove	O
that	O
the	O
residual	B
error	I
from	O
only	O
using	O
l	O
terms	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
discarded	O
eigenvalues	O
j	O
therefore	O
an	O
alternative	O
to	O
plotting	O
the	O
error	O
is	O
to	O
plot	O
the	O
retained	O
eigenvalues	O
in	O
decreasing	O
order	O
this	O
is	O
called	O
a	O
scree	B
plot	I
because	O
the	O
plot	O
looks	O
like	O
the	O
side	O
of	O
a	O
mountain	O
and	O
scree	O
refers	O
to	O
the	O
debris	O
fallen	O
from	O
a	O
mountain	O
and	O
lying	O
at	O
its	O
base	O
this	O
will	O
have	O
the	O
same	O
shape	O
as	O
the	O
residual	B
error	I
plot	O
a	O
related	O
quantity	O
is	O
the	O
fraction	B
of	I
variance	B
explained	I
defined	O
as	O
edtrain	O
l	O
f	O
l	O
j	O
this	O
captures	O
the	O
same	O
information	B
as	O
the	O
scree	B
plot	I
of	O
course	O
if	O
we	O
use	O
l	O
rankx	O
we	O
get	O
zero	O
reconstruction	B
error	I
on	O
the	O
training	B
set	I
to	O
avoid	O
overfitting	O
it	O
is	O
natural	O
to	O
plot	O
reconstruction	B
error	I
on	O
the	O
test	O
set	O
this	O
is	O
shown	O
in	O
figure	O
here	O
we	O
see	O
that	O
the	O
error	O
continues	O
to	O
go	O
down	O
even	O
as	O
the	O
model	O
becomes	O
more	O
complex	O
thus	O
we	O
do	O
not	O
get	O
the	O
usual	O
u-shaped	B
curve	I
that	O
we	O
typically	O
expect	O
to	O
see	O
what	O
is	O
going	O
on	O
the	O
problem	O
is	O
that	O
pca	B
is	O
not	O
a	O
proper	O
generative	O
model	O
of	O
the	O
data	O
if	O
you	O
give	O
it	O
more	O
latent	B
dimensions	O
it	O
will	O
be	O
able	O
to	O
it	O
is	O
merely	O
a	O
compression	O
technique	O
approximate	O
the	O
test	O
data	O
more	O
accurately	O
by	O
contrast	O
a	O
probabilistic	O
model	O
enjoys	O
a	O
bayesian	B
occam	O
s	O
razor	O
effect	O
in	O
that	O
it	O
gets	O
punished	O
if	O
it	O
wastes	O
probability	O
mass	O
on	O
parts	O
of	O
the	O
space	O
where	O
there	O
is	O
little	O
data	O
this	O
is	O
illustrated	O
in	O
figure	O
which	O
plots	O
the	O
quotation	O
from	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
x	O
train	O
set	O
negative	O
loglik	O
k	O
i	O
l	O
l	O
g	O
o	O
g	O
e	O
n	O
num	O
pcs	O
x	O
test	O
set	O
negative	O
loglik	O
k	O
i	O
l	O
l	O
g	O
o	O
g	O
e	O
n	O
num	O
pcs	O
figure	O
negative	B
log	I
likelihood	B
on	O
mnist	B
vs	O
number	O
of	O
latent	B
dimensions	O
used	O
by	O
ppca	B
training	B
set	I
test	O
set	O
figure	O
generated	O
by	O
pcaoverfitdemo	O
negative	B
log	I
likelihood	B
computed	O
using	O
ppca	B
vs	O
l	O
here	O
on	O
the	O
test	O
set	O
we	O
see	O
the	O
usual	O
u-shaped	B
curve	I
these	O
results	O
are	O
analogous	O
to	O
those	O
in	O
section	O
where	O
we	O
discussed	O
the	O
issue	O
of	O
choosing	O
k	O
in	O
the	O
k-means	B
algorithm	I
vs	O
using	O
a	O
gmm	B
profile	O
likelihood	B
although	O
there	O
is	O
no	O
u-shape	O
there	O
is	O
sometimes	O
a	O
regime	O
change	O
in	O
the	O
plots	O
from	O
relatively	O
large	O
errors	O
to	O
relatively	O
small	O
one	O
way	O
to	O
automate	O
the	O
detection	O
of	O
this	O
is	O
described	O
in	O
and	O
ghodsi	O
the	O
idea	O
is	O
this	O
let	O
k	O
be	O
some	O
measure	O
of	O
the	O
error	O
incurred	O
by	O
a	O
model	O
of	O
size	O
k	O
such	O
that	O
lmax	O
in	O
pca	B
these	O
are	O
the	O
eigenvalues	O
but	O
the	O
method	O
can	O
also	O
be	O
applied	O
to	O
k-means	O
now	O
consider	O
partitioning	B
these	O
values	O
into	O
two	O
groups	O
depending	O
on	O
whether	O
k	O
l	O
or	O
k	O
l	O
where	O
l	O
is	O
some	O
threshold	O
which	O
we	O
will	O
determine	O
to	O
measure	O
the	O
quality	O
of	O
l	O
we	O
will	O
use	O
a	O
simple	O
change-point	O
model	O
where	O
k	O
n	O
if	O
k	O
l	O
and	O
k	O
n	O
if	O
k	O
l	O
is	O
important	O
that	O
be	O
the	O
same	O
in	O
both	O
models	O
to	O
prevent	O
overfitting	O
in	O
the	O
case	O
where	O
one	O
regime	O
has	O
less	O
data	O
than	O
the	O
other	O
within	O
each	O
of	O
the	O
two	O
regimes	O
we	O
assume	O
the	O
k	O
are	O
iid	B
which	O
is	O
obviously	O
incorrect	O
but	O
is	O
adequate	O
for	O
our	O
present	O
purposes	O
we	O
can	O
fit	O
this	O
model	O
for	O
each	O
l	O
lmax	O
by	O
partitioning	B
the	O
data	O
and	O
computing	O
the	O
mles	O
using	O
a	O
pooled	B
estimate	O
of	O
the	O
variance	B
k	O
l	O
k	O
l	O
k	O
l	O
k	O
kl	O
k	O
n	O
l	O
n	O
kl	O
k	O
we	O
can	O
then	O
evaluate	O
the	O
profile	O
log	O
likelihood	B
log	O
n	O
k	O
log	O
n	O
k	O
finally	O
we	O
choose	O
l	O
arg	O
max	O
this	O
is	O
illustrated	O
in	O
figure	O
on	O
the	O
left	O
we	O
plot	O
the	O
scree	B
plot	I
which	O
has	O
the	O
same	O
shape	O
as	O
in	O
figure	O
on	O
the	O
right	O
we	O
plot	O
the	O
profile	O
chapter	O
latent	B
linear	O
models	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
x	O
scree	B
plot	I
num	O
pcs	O
d	O
o	O
o	O
h	O
i	O
l	O
e	O
k	O
i	O
l	O
g	O
o	O
l	O
e	O
l	O
i	O
f	O
o	O
r	O
p	O
num	O
pcs	O
figure	O
scree	B
plot	I
for	O
training	B
set	I
corresponding	O
to	O
figure	O
generated	O
by	O
pcaoverfitdemo	O
profile	O
likelihood	B
figure	O
likelihood	B
rather	O
miraculously	O
we	O
see	O
a	O
fairly	O
well-determined	O
peak	O
pca	B
for	O
categorical	B
data	O
in	O
this	O
section	O
we	O
consider	O
extending	O
the	O
factor	B
analysis	I
model	O
to	O
the	O
case	O
where	O
the	O
observed	O
data	O
is	O
categorical	B
rather	O
than	O
real-valued	O
that	O
is	O
the	O
data	O
has	O
the	O
form	O
yij	O
c	O
where	O
j	O
is	O
the	O
number	O
of	O
observed	O
response	O
variables	O
we	O
assume	O
each	O
yij	O
is	O
generated	O
from	O
a	O
latent	B
variable	O
zi	O
r	O
l	O
with	O
a	O
gaussian	B
prior	O
which	O
is	O
passed	O
through	O
the	O
softmax	B
function	O
as	O
follows	O
pzi	O
i	O
pyizi	O
catyirswt	O
r	O
zi	O
l	O
m	O
is	O
the	O
factor	B
loading	I
matrix	I
for	O
response	O
j	O
and	O
r	O
where	O
wr	O
r	O
m	O
is	O
the	O
offset	O
term	O
for	O
response	O
r	O
and	O
need	O
an	O
explicit	O
offset	O
term	O
since	O
clamping	B
one	O
element	O
of	O
zi	O
to	O
can	O
cause	O
problems	O
when	O
computing	O
the	O
posterior	O
covariance	B
as	O
in	O
factor	B
analysis	I
we	O
have	O
defined	O
the	O
prior	O
mean	B
to	O
be	O
and	O
the	O
prior	O
covariance	B
i	O
since	O
we	O
can	O
capture	O
non-zero	O
mean	B
by	O
changing	O
and	O
non-identity	O
covariance	B
by	O
changing	O
wr	O
we	O
will	O
call	O
this	O
categorical	B
pca	B
see	O
chapter	O
for	O
a	O
discussion	O
of	O
related	O
models	O
it	O
is	O
interesting	O
to	O
study	O
what	O
kinds	O
of	O
distributions	O
we	O
can	O
induce	O
on	O
the	O
observed	O
variables	O
by	O
varying	O
the	O
parameters	O
for	O
simplicity	O
we	O
assume	O
there	O
is	O
a	O
single	O
ternary	O
response	B
variable	I
so	O
yi	O
lives	O
in	O
the	O
probability	B
simplex	I
figure	O
shows	O
what	O
happens	O
when	O
we	O
vary	O
the	O
parameters	O
of	O
the	O
prior	O
and	O
which	O
is	O
equivalent	O
to	O
varying	O
the	O
parameters	O
of	O
the	O
likelihood	B
and	O
we	O
see	O
that	O
this	O
can	O
define	O
fairly	O
complex	O
distributions	O
over	O
the	O
simplex	O
this	O
induced	O
distribution	O
is	O
known	O
as	O
the	O
logistic	B
normal	B
distribution	O
we	O
can	O
fit	O
this	O
model	O
to	O
data	O
using	O
a	O
modified	O
version	O
of	O
em	B
the	O
basic	O
idea	O
is	O
to	O
infer	O
a	O
gaussian	B
approximation	I
to	O
the	O
posterior	O
pziyi	O
in	O
the	O
e	B
step	I
and	O
then	O
to	O
maximize	O
in	O
the	O
m	B
step	I
the	O
details	O
for	O
the	O
multiclass	O
case	O
can	O
be	O
found	O
in	O
et	O
al	O
pca	B
for	O
categorical	B
data	O
some	O
examples	O
of	O
the	O
logistic	B
normal	B
distribution	O
defined	O
on	O
the	O
simplex	O
figure	O
diagonal	B
covariance	B
and	O
non-zero	O
mean	B
positive	O
correlation	O
between	O
states	O
and	O
source	O
figure	O
of	O
and	O
lafferty	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
negative	O
correlation	O
between	O
states	O
and	O
figure	O
left	O
synthetic	O
dimensional	O
bit	O
vectors	O
right	O
the	O
embedding	B
learned	O
by	O
binary	O
pca	B
using	O
variational	B
em	B
we	O
have	O
color	O
coded	O
points	O
by	O
the	O
identity	O
of	O
the	O
true	O
prototype	B
that	O
generated	O
them	O
figure	O
generated	O
by	O
binaryfademotipping	O
also	O
section	O
the	O
details	O
for	O
the	O
binary	O
case	O
for	O
the	O
the	O
sigmoid	B
link	O
can	O
be	O
found	O
in	O
exercise	O
and	O
for	O
the	O
probit	B
link	O
in	O
exercise	O
one	O
application	O
of	O
such	O
a	O
model	O
is	O
to	O
visualize	O
high	O
dimensional	O
categorical	B
data	O
figure	O
shows	O
a	O
simple	O
example	O
where	O
we	O
have	O
bit	O
vectors	O
it	O
is	O
clear	O
that	O
each	O
sample	O
is	O
just	O
a	O
noisy	O
copy	O
of	O
one	O
of	O
three	O
binary	O
prototypes	O
we	O
fit	O
a	O
catfa	O
to	O
this	O
model	O
yielding	O
approximate	O
mles	O
in	O
figure	O
we	O
plot	O
e	O
we	O
see	O
that	O
there	O
are	O
three	O
distinct	O
clusters	B
as	O
is	O
to	O
be	O
expected	O
zixi	O
in	O
et	O
al	O
we	O
show	O
that	O
this	O
model	O
outperforms	O
finite	O
mixture	B
models	O
on	O
the	O
task	O
of	O
imputing	O
missing	B
entries	O
in	O
design	O
matrices	O
consisting	O
of	O
real	O
and	O
categorical	B
data	O
this	O
is	O
useful	O
for	O
analysing	O
social	O
science	O
survey	O
data	O
which	O
often	O
has	O
missing	B
data	I
and	O
variables	O
of	O
mixed	O
type	O
chapter	O
latent	B
linear	O
models	O
wy	O
w	O
x	O
w	O
x	O
w	O
y	O
zi	O
zx	O
i	O
zs	O
i	O
yi	O
xi	O
n	O
bx	O
yi	O
n	O
xi	O
w	O
x	O
w	O
y	O
zx	O
i	O
zs	O
i	O
zy	O
i	O
bx	O
xi	O
yi	O
n	O
by	O
figure	O
gaussian	B
latent	B
factor	B
models	O
for	O
paired	O
data	O
canonical	B
correlation	I
analysis	I
supervised	B
pca	B
partial	B
least	B
squares	I
pca	B
for	O
paired	O
and	O
multi-view	O
data	O
it	O
is	O
common	O
to	O
have	O
a	O
pair	O
of	O
related	O
datasets	O
e	O
g	O
gene	O
expression	O
and	O
gene	O
copy	O
number	O
or	O
movie	O
ratings	O
by	O
users	O
and	O
movie	O
reviews	O
it	O
is	O
natural	O
to	O
want	O
to	O
combine	O
these	O
together	O
into	O
a	O
low-dimensional	O
embedding	B
this	O
is	O
an	O
example	O
of	O
data	B
fusion	I
in	O
some	O
cases	O
we	O
might	O
want	O
to	O
predict	O
one	O
element	O
of	O
the	O
pair	O
say	O
from	O
the	O
other	O
one	O
via	O
the	O
low-dimensional	O
bottleneck	B
below	O
we	O
discuss	O
various	O
latent	B
gaussian	B
models	O
for	O
these	O
tasks	O
following	O
the	O
presentation	O
of	O
the	O
models	O
easily	O
generalize	B
from	O
pairs	O
to	O
sets	O
of	O
data	O
xim	O
for	O
m	O
we	O
focus	O
on	O
the	O
case	O
where	O
xim	O
r	O
in	O
this	O
case	O
the	O
joint	B
distribution	I
is	O
multivariate	B
gaussian	B
so	O
we	O
can	O
easily	O
fit	O
the	O
models	O
using	O
em	B
or	O
gibbs	B
sampling	I
dm	O
we	O
can	O
generalize	B
the	O
models	O
to	O
handle	O
discrete	B
and	O
count	O
data	O
by	O
using	O
the	O
exponential	B
family	B
as	O
a	O
response	O
distribution	O
instead	O
of	O
the	O
gaussian	B
as	O
we	O
explain	O
in	O
section	O
however	O
this	O
will	O
require	O
the	O
use	O
of	O
approximate	B
inference	B
in	O
the	O
e	B
step	I
an	O
analogous	O
modification	O
to	O
mcmc	B
pca	B
for	O
paired	O
and	O
multi-view	O
data	O
supervised	B
pca	B
factor	B
regression	B
consider	O
the	O
following	O
model	O
illustrated	O
in	O
figure	O
pzi	O
il	O
pyizi	O
y	O
zi	O
y	O
y	O
pxizi	O
x	O
xid	O
in	O
et	O
al	O
this	O
is	O
called	O
supervised	B
pca	B
in	O
this	O
is	O
called	O
bayesian	B
factor	B
regression	B
this	O
model	O
is	O
like	O
pca	B
except	O
that	O
the	O
target	O
variable	O
yi	O
is	O
taken	O
into	O
account	O
when	O
learning	B
the	O
low	O
dimensional	O
embedding	B
since	O
the	O
model	O
is	O
jointly	O
gaussian	B
we	O
have	O
yixi	O
n	O
i	O
w	O
y	O
wt	O
y	O
cwy	O
x	O
so	O
although	O
this	O
is	O
a	O
g	O
j	O
contains	O
where	O
w	O
joint	O
density	O
model	O
of	O
xi	O
we	O
can	O
infer	O
the	O
implied	O
conditional	O
distribution	O
xid	O
and	O
c	O
i	O
wt	O
we	O
now	O
show	O
an	O
interesting	O
connection	O
to	O
zellner	O
s	O
g-prior	B
suppose	O
pwy	O
n	O
and	O
let	O
x	O
rvt	O
be	O
the	O
svd	B
of	O
x	O
where	O
vt	O
v	O
i	O
and	O
rt	O
r	O
diag	O
the	O
squared	O
singular	B
values	I
then	O
one	O
can	O
show	O
that	O
pw	O
n	O
gv	O
t	O
n	O
gxt	O
x	O
so	O
the	O
dependence	O
of	O
the	O
prior	O
for	O
w	O
on	O
x	O
arises	O
from	O
the	O
fact	O
that	O
w	O
is	O
derived	O
indirectly	O
by	O
a	O
joint	O
model	O
of	O
x	O
and	O
y	O
the	O
above	O
discussion	O
focussed	O
on	O
regression	B
generalizes	O
cca	B
to	O
the	O
exponential	B
family	B
which	O
is	O
more	O
appropriate	O
if	O
xi	O
andor	O
yi	O
are	O
discrete	B
although	O
we	O
can	O
no	O
longer	O
compute	O
the	O
conditional	O
pyixi	O
in	O
closed	O
form	O
the	O
model	O
has	O
a	O
similar	B
interpretation	O
to	O
the	O
regression	B
case	O
namely	O
that	O
we	O
are	O
predicting	O
the	O
response	O
via	O
a	O
latent	B
bottleneck	B
in	O
particular	O
we	O
might	O
want	O
to	O
find	O
an	O
encoding	O
distribution	O
pzx	O
such	O
that	O
we	O
minimize	O
the	O
basic	O
idea	O
of	O
compressing	O
xi	O
to	O
predict	O
yi	O
can	O
be	O
formulated	O
using	O
information	B
theory	I
i	O
z	O
i	O
y	O
where	O
is	O
some	O
parameter	B
controlling	O
the	O
tradeoff	O
between	O
compression	O
and	O
predictive	B
accuracy	O
this	O
is	O
known	O
as	O
the	O
information	B
bottleneck	B
et	O
al	O
often	O
z	O
is	O
taken	O
to	O
be	O
discrete	B
as	O
in	O
clustering	B
however	O
in	O
the	O
gaussian	B
case	O
ib	O
is	O
closely	O
related	O
to	O
cca	B
et	O
al	O
we	O
can	O
easily	O
generalize	B
cca	B
to	O
the	O
case	O
where	O
yi	O
is	O
a	O
vector	O
of	O
responses	O
to	O
be	O
predicted	O
as	O
in	O
multi-label	O
classification	O
et	O
al	O
williamson	O
and	O
ghahramani	O
used	O
this	O
model	O
to	O
perform	O
collaborative	O
filtering	B
where	O
the	O
goal	O
is	O
to	O
predict	O
yij	O
the	O
rating	O
person	O
i	O
gives	O
to	O
movie	O
j	O
where	O
the	O
side	B
information	B
xi	O
takes	O
the	O
form	O
of	O
a	O
list	O
of	O
i	O
s	O
friends	O
the	O
intuition	O
behind	O
this	O
approach	O
is	O
that	O
knowledge	O
of	O
who	O
your	O
friends	O
are	O
as	O
well	O
as	O
the	O
ratings	O
of	O
all	O
other	O
users	O
should	O
help	O
predict	O
which	O
movies	O
you	O
will	O
like	O
in	O
general	O
any	O
setting	O
where	O
the	O
tasks	O
are	O
correlated	O
could	O
benefit	O
from	O
cca	B
once	O
we	O
adopt	O
a	O
probabilistic	O
view	O
various	O
extensions	O
are	O
straightforward	O
for	O
example	O
we	O
can	O
easily	O
generalize	B
to	O
the	O
semi-supervised	B
case	O
where	O
we	O
do	O
not	O
observe	O
yi	O
for	O
all	O
i	O
et	O
al	O
chapter	O
latent	B
linear	O
models	O
discriminative	B
supervised	B
pca	B
one	O
problem	O
with	O
this	O
model	O
is	O
that	O
it	O
puts	O
as	O
much	O
weight	O
on	O
predicting	O
the	O
inputs	O
xi	O
as	O
the	O
outputs	O
yi	O
this	O
can	O
be	O
partially	O
alleviated	O
by	O
using	O
a	O
weighted	O
objective	O
of	O
the	O
following	O
form	O
et	O
al	O
pyi	O
iy	O
y	O
pxi	O
ix	O
x	O
where	O
the	O
m	O
control	O
the	O
relative	O
importance	O
of	O
the	O
data	O
sources	O
and	O
im	O
wmzi	O
for	O
gaussian	B
data	O
we	O
can	O
see	O
that	O
m	O
just	O
controls	O
the	O
noise	O
variance	B
i	O
xxt	O
yyt	O
i	O
exp	O
exp	O
i	O
i	O
this	O
interpretation	O
holds	O
more	O
generally	O
for	O
the	O
exponential	B
family	B
note	O
however	O
that	O
it	O
is	O
hard	O
to	O
estimate	O
the	O
m	O
parameters	O
because	O
changing	O
them	O
changes	O
the	O
normalization	O
constant	O
of	O
the	O
likelihood	B
we	O
give	O
an	O
alternative	O
approach	O
to	O
weighting	O
y	O
more	O
heavily	O
below	O
partial	B
least	B
squares	I
the	O
technique	O
of	O
partial	B
least	B
squares	I
sun	O
et	O
al	O
is	O
an	O
asymmetric	O
or	O
more	O
discriminative	B
form	O
of	O
supervised	B
pca	B
the	O
key	O
idea	O
is	O
to	O
allow	O
some	O
of	O
the	O
in	O
the	O
input	O
features	B
to	O
be	O
explained	O
by	O
its	O
own	O
subspace	O
zx	O
i	O
and	O
to	O
let	O
the	O
rest	O
of	O
the	O
subspace	O
zs	O
i	O
be	O
shared	B
between	O
input	O
and	O
output	O
the	O
model	O
has	O
the	O
form	O
pzi	O
ils	O
i	O
ilx	O
pyizi	O
pxizi	O
i	O
y	O
i	O
bxzx	O
i	O
x	O
see	O
figure	O
the	O
corresponding	O
induced	O
distribution	O
on	O
the	O
visible	B
variables	I
has	O
the	O
form	O
pvi	O
n	O
idzi	O
n	O
wwt	O
where	O
vi	O
yi	O
y	O
x	O
and	O
wy	O
wx	O
bx	O
wywt	O
y	O
wxwt	O
x	O
wxwt	O
wxwt	O
x	O
x	O
bxbt	O
x	O
w	O
wwt	O
we	O
should	O
choose	O
l	O
large	O
enough	O
so	O
that	O
the	O
shared	B
subspace	O
does	O
not	O
capture	O
covariatespecific	O
variation	O
this	O
model	O
can	O
be	O
easily	O
generalized	O
to	O
discrete	B
data	O
using	O
the	O
exponential	B
family	B
independent	B
component	I
analysis	I
canonical	B
correlation	I
analysis	I
canonical	B
correlation	I
analysis	I
or	O
cca	B
is	O
like	O
a	O
symmetric	B
unsupervised	O
version	O
of	O
pls	B
it	O
allows	O
each	O
view	O
to	O
have	O
its	O
own	O
private	O
subspace	O
but	O
there	O
is	O
also	O
a	O
shared	B
subspace	O
if	O
we	O
have	O
two	O
observed	O
variables	O
xi	O
and	O
yi	O
then	O
we	O
have	O
three	O
latent	B
variables	O
zs	O
which	O
is	O
i	O
r	O
shared	B
zx	O
ly	O
which	O
are	O
private	O
we	O
can	O
write	O
the	O
model	O
as	O
follows	O
and	O
jordan	O
lx	O
and	O
zy	O
i	O
r	O
i	O
r	O
pzi	O
pxizi	O
pyizi	O
ils	O
i	O
wxzs	O
i	O
wyzs	O
i	O
ilx	O
i	O
ily	O
i	O
x	O
i	O
y	O
see	O
figure	O
the	O
corresponding	O
observed	O
joint	B
distribution	I
has	O
the	O
form	O
pvi	O
where	O
w	O
wwt	O
n	O
idzi	O
n	O
wwt	O
wx	O
bx	O
wy	O
wxwt	O
by	O
x	O
bxbt	O
x	O
wxwt	O
y	O
wywt	O
y	O
wywt	O
y	O
bybt	O
y	O
one	O
can	O
compute	O
the	O
mle	B
for	O
this	O
model	O
using	O
em	B
and	O
jordan	O
show	O
that	O
the	O
resulting	O
mle	B
is	O
equivalent	O
to	O
rotation	O
and	O
scaling	O
to	O
the	O
classical	B
non-probabilistic	O
view	O
however	O
the	O
advantages	O
of	O
the	O
probabilistic	O
view	O
are	O
many	O
we	O
can	O
trivially	O
generalize	B
to	O
m	O
observed	O
variables	O
we	O
can	O
create	O
mixtures	O
of	O
cca	B
et	O
al	O
we	O
can	O
create	O
sparse	B
versions	O
of	O
cca	B
using	O
ard	B
and	O
bach	O
we	O
can	O
generalize	B
to	O
the	O
exponential	B
family	B
et	O
al	O
we	O
can	O
perform	O
bayesian	B
inference	B
of	O
the	O
parameters	O
klami	O
and	O
kaski	O
we	O
can	O
handle	O
non-parametric	O
sparsity-promoting	O
priors	O
for	O
w	O
and	O
b	O
and	O
daume	O
and	O
so	O
on	O
independent	B
component	I
analysis	I
consider	O
the	O
following	O
situation	O
you	O
are	O
in	O
a	O
crowded	O
room	O
and	O
many	O
people	O
are	O
speaking	O
your	O
ears	O
essentially	O
act	O
as	O
two	O
microphones	O
which	O
are	O
listening	O
to	O
a	O
linear	O
combination	O
of	O
the	O
different	O
speech	O
signals	O
in	O
the	O
room	O
your	O
goal	O
is	O
to	O
deconvolve	O
the	O
mixed	O
signals	O
into	O
their	O
constituent	O
parts	O
this	O
is	O
known	O
as	O
the	O
cocktail	B
party	I
problem	I
and	O
is	O
an	O
example	O
of	O
blind	B
signal	I
separation	I
or	O
blind	B
source	I
separation	I
where	O
blind	O
means	O
we	O
know	O
nothing	O
about	O
the	O
source	O
of	O
the	O
signals	O
besides	O
the	O
obvious	O
applications	O
to	O
acoustic	O
signal	B
processing	I
this	O
problem	O
also	O
arises	O
when	O
analysing	O
eeg	O
and	O
meg	O
signals	O
financial	O
data	O
and	O
any	O
other	O
dataset	O
necessarily	O
temporal	O
where	O
latent	B
sources	O
or	O
factors	B
get	O
mixed	O
together	O
in	O
a	O
linear	O
way	O
at	O
time	O
t	O
and	O
zt	O
r	O
we	O
can	O
formalize	O
the	O
problem	O
as	O
follows	O
let	O
xt	O
r	O
l	O
be	O
the	O
vector	O
of	O
source	O
signals	O
we	O
assume	O
that	O
d	O
be	O
the	O
observed	O
signal	O
at	O
the	O
sensors	O
xt	O
wzt	O
chapter	O
latent	B
linear	O
models	O
truth	O
observed	O
signals	O
pca	B
estimate	O
ica	B
estimate	O
figure	O
illustration	O
of	O
ica	B
applied	O
to	O
iid	B
samples	B
of	O
a	O
source	O
signal	O
observations	O
pca	B
estimate	O
ica	B
estimate	O
figure	O
generated	O
by	O
icademo	O
written	O
by	O
aapo	O
hyvarinen	O
latent	B
signals	O
where	O
w	O
is	O
an	O
d	O
l	O
matrix	O
and	O
n	O
in	O
this	O
section	O
we	O
treat	O
each	O
time	O
point	O
as	O
an	O
independent	O
observation	B
i	O
e	O
we	O
do	O
not	O
model	O
temporal	O
correlation	O
we	O
could	O
replace	O
the	O
t	O
index	O
with	O
i	O
but	O
we	O
stick	O
with	O
t	O
to	O
be	O
consistent	B
with	O
much	O
of	O
the	O
ica	B
literature	O
the	O
goal	O
is	O
to	O
infer	O
the	O
source	O
signals	O
pztxt	O
as	O
illustrated	O
in	O
figure	O
in	O
this	O
context	O
w	O
is	O
called	O
the	O
mixing	B
matrix	I
if	O
l	O
d	O
of	O
sources	O
number	O
of	O
sensors	O
it	O
will	O
be	O
a	O
square	O
matrix	O
often	O
we	O
will	O
assume	O
the	O
noise	O
level	O
is	O
zero	O
for	O
simplicity	O
so	O
far	O
the	O
model	O
is	O
identical	O
to	O
factor	B
analysis	I
pca	B
if	O
there	O
is	O
no	O
noise	O
except	O
we	O
don	O
t	O
in	O
general	O
require	O
orthogonality	O
of	O
w	O
however	O
we	O
will	O
use	O
a	O
different	O
prior	O
for	O
pzt	O
in	O
pca	B
we	O
assume	O
each	O
source	O
is	O
independent	O
and	O
has	O
a	O
gaussian	B
distribution	O
pzt	O
n	O
we	O
will	O
now	O
relax	O
this	O
gaussian	B
assumption	O
and	O
let	O
the	O
source	O
distributions	O
be	O
any	O
non-gaussian	O
independent	B
component	I
analysis	I
uniform	O
data	O
uniform	O
data	O
after	O
linear	O
mixing	O
pca	B
applied	O
to	O
mixed	O
data	O
from	O
uniform	O
source	O
ica	B
applied	O
to	O
mixed	O
data	O
from	O
uniform	O
source	O
figure	O
distribution	O
icademouniform	O
written	O
by	O
aapo	O
hyvarinen	O
latent	B
signals	O
observations	O
illustration	O
of	O
ica	B
and	O
pca	B
applied	O
to	O
iid	B
samples	B
of	O
a	O
source	O
signal	O
with	O
a	O
uniform	O
ica	B
estimate	O
figure	O
generated	O
by	O
pca	B
estimate	O
distribution	O
pzt	O
pjztj	O
without	O
loss	B
of	O
generality	O
we	O
can	O
constrain	O
the	O
variance	B
of	O
the	O
source	O
distributions	O
to	O
be	O
because	O
any	O
other	O
variance	B
can	O
be	O
modelled	O
by	O
scaling	O
the	O
rows	O
of	O
w	O
appropriately	O
the	O
resulting	O
model	O
is	O
known	O
as	O
independent	B
component	I
analysis	I
or	O
ica	B
the	O
reason	O
the	O
gaussian	B
distribution	O
is	O
disallowed	O
as	O
a	O
source	O
prior	O
in	O
ica	B
is	O
that	O
it	O
does	O
not	O
permit	O
unique	O
recovery	O
of	O
the	O
sources	O
as	O
illustrated	O
in	O
figure	O
this	O
is	O
because	O
the	O
pca	B
likelihood	B
is	O
invariant	B
to	O
any	O
orthogonal	O
transformation	O
of	O
the	O
sources	O
zt	O
and	O
mixing	B
matrix	I
w	O
pca	B
can	O
recover	O
the	O
best	O
linear	O
subspace	O
in	O
which	O
the	O
signals	O
lie	O
but	O
cannot	O
uniquely	O
recover	O
the	O
signals	O
themselves	O
chapter	O
latent	B
linear	O
models	O
to	O
illustrate	O
this	O
suppose	O
we	O
have	O
two	O
independent	O
sources	O
with	O
uniform	O
distributions	O
as	O
shown	O
in	O
figure	O
now	O
suppose	O
we	O
have	O
the	O
following	O
mixing	B
matrix	I
w	O
then	O
we	O
observe	O
the	O
data	O
shown	O
in	O
figure	O
no	O
noise	O
if	O
we	O
apply	O
pca	B
followed	O
by	O
scaling	O
to	O
this	O
we	O
get	O
the	O
result	O
in	O
figure	O
this	O
corresponds	O
to	O
a	O
whitening	B
of	O
the	O
data	O
to	O
uniquely	O
recover	O
the	O
sources	O
we	O
need	O
to	O
perform	O
an	O
additional	O
rotation	O
the	O
trouble	O
is	O
there	O
is	O
no	O
information	B
in	O
the	O
symmetric	B
gaussian	B
posterior	O
to	O
tell	O
us	O
which	O
angle	O
to	O
rotate	O
by	O
in	O
a	O
sense	O
pca	B
solves	O
half	O
of	O
the	O
problem	O
since	O
it	O
identifies	O
the	O
linear	O
subspace	O
all	O
that	O
ica	B
has	O
to	O
do	O
is	O
then	O
to	O
identify	O
the	O
appropriate	O
rotation	O
we	O
see	O
that	O
ica	B
is	O
not	O
that	O
different	O
from	O
methods	O
such	O
as	O
varimax	B
which	O
seek	O
good	O
rotations	O
of	O
the	O
latent	B
factors	B
to	O
enhance	O
interpretability	O
ica	B
requires	O
that	O
w	O
is	O
square	O
and	O
hence	O
invertible	O
figure	O
shows	O
that	O
ica	B
can	O
recover	O
the	O
source	O
up	O
to	O
a	O
permutation	O
of	O
the	O
indices	O
and	O
possible	O
sign	O
change	O
in	O
the	O
non-square	O
case	O
where	O
we	O
have	O
more	O
sources	O
than	O
sensors	O
we	O
cannot	O
uniquely	O
recover	O
the	O
true	O
signal	O
but	O
we	O
can	O
compute	O
the	O
posterior	O
pztxt	O
w	O
which	O
represents	O
our	O
beliefs	O
about	O
the	O
source	O
in	O
both	O
cases	O
we	O
need	O
to	O
estimate	O
w	O
as	O
well	O
as	O
the	O
source	O
distributions	O
pj	O
we	O
discuss	O
how	O
to	O
do	O
this	O
below	O
maximum	O
likelihood	B
estimation	O
in	O
this	O
section	O
we	O
discuss	O
ways	O
to	O
estimate	O
square	O
mixing	O
matrices	O
w	O
for	O
the	O
noise-free	O
ica	B
model	O
as	O
usual	O
we	O
will	O
assume	O
that	O
the	O
observations	O
have	O
been	O
centered	O
hence	O
we	O
can	O
also	O
assume	O
z	O
is	O
zero-mean	O
in	O
addition	O
we	O
assume	O
the	O
observations	O
have	O
been	O
whitened	O
which	O
can	O
be	O
done	O
with	O
pca	B
if	O
the	O
data	O
is	O
centered	O
and	O
whitened	O
we	O
have	O
e	O
xxt	O
i	O
but	O
in	O
the	O
noise	O
free	O
case	O
we	O
also	O
have	O
cov	O
e	O
xxt	O
we	O
zzt	O
wt	O
wwt	O
hence	O
we	O
see	O
that	O
w	O
must	O
be	O
orthogonal	O
this	O
reduces	O
the	O
number	O
of	O
parameters	O
we	O
have	O
to	O
estimate	O
from	O
to	O
dd	O
it	O
will	O
also	O
simplify	O
the	O
math	O
and	O
the	O
algorithms	O
let	O
v	O
w	O
these	O
are	O
often	O
called	O
the	O
recognition	B
weights	I
as	O
opposed	O
to	O
w	O
which	O
are	O
the	O
generative	O
since	O
x	O
wz	O
we	O
have	O
from	O
equation	O
pxwzt	O
pzzt	O
detw	O
pzvxt	O
detv	O
hence	O
we	O
can	O
write	O
the	O
log-likelihood	O
assuming	O
t	O
iid	B
samples	B
as	O
follows	O
t	O
log	O
pdv	O
log	O
detv	O
t	O
log	O
pjvt	O
j	O
xt	O
in	O
the	O
literature	O
it	O
is	O
common	O
to	O
denote	O
the	O
generative	B
weights	I
by	O
a	O
and	O
the	O
recognition	B
weights	I
by	O
w	O
but	O
we	O
are	O
trying	O
to	O
be	O
consistent	B
with	O
the	O
notation	O
used	O
earlier	O
in	O
this	O
chapter	O
independent	B
component	I
analysis	I
where	O
vj	O
is	O
the	O
j	O
th	O
row	O
of	O
v	O
since	O
we	O
are	O
constraining	O
v	O
to	O
be	O
orthogonal	O
the	O
first	O
term	O
is	O
a	O
constant	O
so	O
we	O
can	O
drop	O
it	O
we	O
can	O
also	O
replace	O
the	O
average	O
over	O
the	O
data	O
with	O
an	O
expectation	O
operator	O
to	O
get	O
the	O
following	O
objective	O
nllv	O
e	O
j	O
x	O
and	O
gjz	O
log	O
pjz	O
we	O
want	O
to	O
minimize	O
this	O
subject	O
to	O
the	O
constraint	O
where	O
zj	O
vt	O
that	O
the	O
rows	O
of	O
v	O
are	O
orthogonal	O
we	O
also	O
want	O
them	O
to	O
be	O
unit	O
norm	O
since	O
this	O
ensures	O
which	O
is	O
that	O
the	O
variance	B
of	O
the	O
factors	B
is	O
unity	O
with	O
whitened	O
data	O
e	O
necessary	O
to	O
fix	O
the	O
scale	O
of	O
the	O
weights	O
in	O
otherwords	O
v	O
should	O
be	O
an	O
orthonormal	O
matrix	O
it	O
is	O
straightforward	O
to	O
derive	O
a	O
gradient	B
descent	I
algorithm	O
to	O
fit	O
this	O
model	O
however	O
it	O
is	O
rather	O
slow	O
one	O
can	O
also	O
derive	O
a	O
faster	O
algorithm	O
that	O
follows	O
the	O
natural	B
gradient	I
see	O
e	O
g	O
ch	O
for	O
details	O
a	O
popular	O
alternative	O
is	O
to	O
use	O
an	O
approximate	O
newton	O
method	O
which	O
we	O
discuss	O
in	O
section	O
another	O
approach	O
is	O
to	O
use	O
em	B
which	O
we	O
discuss	O
in	O
section	O
vt	O
j	O
x	O
the	O
fastica	O
algorithm	O
we	O
now	O
describe	O
the	O
fast	B
ica	B
algorithm	O
based	O
on	O
and	O
oja	O
which	O
we	O
will	O
show	O
is	O
an	O
approximate	O
newton	O
method	O
for	O
fitting	O
ica	B
models	O
for	O
simplicity	O
of	O
presentation	O
we	O
initially	O
assume	O
there	O
is	O
only	O
one	O
latent	B
factor	B
in	O
addition	O
we	O
initially	O
assume	O
all	O
source	O
distributions	O
are	O
known	O
and	O
are	O
the	O
same	O
so	O
we	O
can	O
just	O
write	O
gz	O
log	O
pz	O
let	O
gz	O
d	O
dz	O
gz	O
the	O
constrained	O
objective	O
and	O
its	O
gradient	O
and	O
hessian	B
are	O
given	O
by	O
vt	O
v	O
v	O
i	O
where	O
is	O
a	O
lagrange	B
multiplier	I
let	O
us	O
make	O
the	O
approximation	O
f	O
f	O
hv	O
e	O
x	O
e	O
x	O
this	O
makes	O
the	O
hessian	B
very	O
easy	O
to	O
invert	O
giving	O
rise	O
to	O
the	O
following	O
newton	O
update	O
gvt	O
x	O
xgvt	O
x	O
xxt	O
x	O
e	O
x	O
v	O
xgvt	O
x	O
e	O
x	O
xxt	O
xxt	O
e	O
v	O
v	O
e	O
one	O
can	O
rewrite	O
this	O
in	O
the	O
following	O
way	O
v	O
e	O
xgvt	O
x	O
x	O
v	O
e	O
practice	O
the	O
expectations	O
can	O
be	O
replaced	O
by	O
monte	B
carlo	I
estimates	O
from	O
the	O
training	B
set	I
which	O
gives	O
an	O
efficient	O
online	B
learning	B
algorithm	O
after	O
performing	O
this	O
update	O
one	O
should	O
project	O
back	O
onto	O
the	O
constraint	O
surface	O
using	O
vnew	O
v	O
chapter	O
latent	B
linear	O
models	O
gaussian	B
laplace	B
uniform	O
figure	O
illustration	O
of	O
gaussian	B
sub-gaussian	B
and	O
super-gaussian	B
distributions	O
in	O
and	O
figure	O
generated	O
by	O
subsupergaussplot	O
written	O
by	O
kevin	O
swersky	O
to	O
the	O
sign	O
ambiguity	O
of	O
v	O
the	O
values	O
of	O
v	O
one	O
iterates	O
this	O
algorithm	O
until	O
convergence	O
may	O
not	O
converge	B
but	O
the	O
direction	O
defined	O
by	O
this	O
vector	O
should	O
converge	B
so	O
one	O
can	O
assess	O
convergence	O
by	O
monitoring	O
vnew	O
which	O
should	O
approach	O
since	O
the	O
objective	O
is	O
not	O
convex	B
there	O
are	O
multiple	O
local	O
optima	O
we	O
can	O
use	O
this	O
fact	O
to	O
learn	O
multiple	O
different	O
weight	O
vectors	O
or	O
features	B
we	O
can	O
either	O
learn	O
the	O
features	B
sequentially	O
and	O
then	O
project	O
out	O
the	O
part	O
of	O
vj	O
that	O
lies	O
in	O
the	O
subspace	O
defined	O
by	O
earlier	O
features	B
or	O
we	O
can	O
learn	O
them	O
in	O
parallel	O
and	O
orthogonalize	O
v	O
in	O
parallel	O
this	O
latter	O
approach	O
is	O
usually	O
preferred	O
since	O
unlike	O
pca	B
the	O
features	B
are	O
not	O
ordered	O
in	O
any	O
way	O
so	O
the	O
first	O
feature	O
is	O
not	O
more	O
important	O
than	O
the	O
second	O
and	O
hence	O
it	O
is	O
better	O
to	O
treat	O
them	O
symmetrically	O
independent	B
component	I
analysis	I
modeling	O
the	O
source	O
densities	O
so	O
far	O
we	O
have	O
assumed	O
that	O
gz	O
log	O
pz	O
is	O
known	O
what	O
kinds	O
of	O
models	O
might	O
be	O
reasonable	O
as	O
signal	O
priors	O
we	O
know	O
that	O
using	O
gaussians	O
correspond	O
to	O
quadratic	O
functions	O
for	O
g	O
won	O
t	O
work	O
so	O
we	O
want	O
some	O
kind	O
of	O
non-gaussian	O
distribution	O
in	O
general	O
there	O
are	O
several	O
kinds	O
of	O
non-gaussian	O
distributions	O
such	O
as	O
the	O
following	O
super-gaussian	B
distributions	O
these	O
are	O
distributions	O
which	O
have	O
a	O
big	O
spike	O
at	O
the	O
mean	B
and	O
hence	O
order	O
to	O
ensure	O
unit	O
variance	B
have	O
heavy	B
tails	I
the	O
laplace	B
distribution	I
is	O
a	O
classic	O
example	O
see	O
figure	O
formally	O
we	O
say	O
a	O
distribution	O
is	O
super-gaussian	B
or	O
leptokurtic	B
lepto	O
coming	O
from	O
the	O
greek	O
for	O
thin	O
if	O
kurtz	O
where	O
kurtz	O
is	O
the	O
kurtosis	B
of	O
the	O
distribution	O
defined	O
by	O
kurtz	O
where	O
is	O
the	O
standard	B
deviation	I
and	O
k	O
is	O
the	O
k	O
th	O
central	B
moment	I
or	O
moment	O
about	O
the	O
mean	B
k	O
e	O
is	O
the	O
mean	B
and	O
is	O
the	O
variance	B
it	O
is	O
conventional	O
to	O
subtract	O
in	O
the	O
definition	O
of	O
kurtosis	B
to	O
make	O
the	O
kurtosis	B
of	O
a	O
gaussian	B
variable	O
equal	O
to	O
zero	O
e	O
sub-gaussian	B
distributions	O
a	O
sub-gaussian	B
or	O
platykurtic	B
platy	O
coming	O
from	O
the	O
greek	O
for	O
broad	O
distribution	O
has	O
negative	O
kurtosis	B
these	O
are	O
distributions	O
which	O
are	O
much	O
flatter	O
than	O
a	O
gaussian	B
the	O
uniform	B
distribution	I
is	O
a	O
classic	O
example	O
see	O
figure	O
skewed	O
distributions	O
another	O
way	O
to	O
be	O
non-gaussian	O
is	O
to	O
be	O
asymmetric	O
one	O
measure	O
of	O
this	O
is	O
skewness	B
defined	O
by	O
skewz	O
an	O
example	O
of	O
a	O
skewed	O
distribution	O
is	O
the	O
gamma	B
distribution	I
figure	O
when	O
one	O
looks	O
at	O
the	O
empirical	B
distribution	I
of	O
many	O
natural	O
signals	O
such	O
as	O
images	O
and	O
speech	O
when	O
passed	O
through	O
certain	O
linear	O
filters	O
they	O
tend	O
to	O
be	O
very	O
super-gaussian	B
this	O
result	O
holds	O
both	O
for	O
the	O
kind	O
of	O
linear	O
filters	O
found	O
in	O
certain	O
parts	O
of	O
the	O
brain	O
such	O
as	O
the	O
simple	B
cells	I
found	O
in	O
the	O
primary	O
visual	O
cortex	O
as	O
well	O
as	O
for	O
the	O
kinds	O
of	O
linear	O
filters	O
used	O
in	O
signal	B
processing	I
such	O
as	O
wavelet	B
transforms	I
one	O
obvious	O
choice	O
for	O
modeling	O
natural	O
signals	O
with	O
ica	B
is	O
therefore	O
the	O
laplace	B
distribution	I
for	O
mean	B
zero	O
and	O
variance	B
this	O
has	O
a	O
log	O
pdf	B
given	O
by	O
log	O
pz	O
log	O
since	O
the	O
laplace	B
prior	O
is	O
not	O
differentiable	O
at	O
the	O
origin	O
it	O
is	O
more	O
common	O
to	O
use	O
other	O
smoother	O
super-gaussian	B
distributions	O
one	O
example	O
is	O
the	O
logistic	B
distribution	I
the	O
corresponding	O
log	O
pdf	B
for	O
the	O
case	O
where	O
the	O
mean	B
is	O
zero	O
and	O
the	O
variance	B
is	O
and	O
s	O
is	O
given	O
by	O
the	O
following	O
log	O
pz	O
log	O
cosh	O
z	O
log	O
chapter	O
latent	B
linear	O
models	O
dk	O
dk	O
qtd	O
ztd	O
xtd	O
t	O
w	O
figure	O
modeling	O
the	O
source	O
distributions	O
using	O
a	O
mixture	B
of	O
univariate	O
gaussians	O
independent	O
factor	B
analysis	I
model	O
of	O
et	O
al	O
attias	O
various	O
ways	O
of	O
estimating	O
gz	O
log	O
pz	O
are	O
discussed	O
in	O
the	O
seminal	O
paper	O
and	O
garrat	O
however	O
when	O
fitting	O
ica	B
by	O
maximum	O
likelihood	B
it	O
is	O
not	O
critical	O
that	O
the	O
exact	O
shape	O
of	O
the	O
source	O
distribution	O
be	O
known	O
it	O
is	O
important	O
to	O
know	O
whether	O
it	O
is	O
sub	O
z	O
or	O
gz	O
log	O
coshz	O
or	O
super	O
gaussian	B
consequently	O
it	O
is	O
common	O
to	O
just	O
use	O
gz	O
instead	O
of	O
the	O
more	O
complex	O
expressions	O
above	O
using	O
em	B
an	O
alternative	O
to	O
assuming	O
a	O
particular	O
form	O
for	O
gz	O
or	O
equivalently	O
for	O
pz	O
is	O
to	O
use	O
a	O
flexible	O
non-parametric	O
density	O
estimator	B
such	O
as	O
a	O
mixture	B
of	I
gaussians	I
pqj	O
k	O
k	O
pzjqj	O
k	O
jk	O
jk	O
pxz	O
this	O
approach	O
was	O
proposed	O
in	O
et	O
al	O
attias	O
and	O
the	O
corresponding	O
graphical	B
model	I
is	O
shown	O
in	O
figure	O
it	O
is	O
possible	O
to	O
derive	O
an	O
exact	O
em	B
algorithm	O
for	O
this	O
model	O
the	O
key	O
observation	B
is	O
that	O
it	O
is	O
possible	O
to	O
compute	O
e	O
exactly	O
by	O
summing	O
over	O
all	O
k	O
l	O
combinations	O
of	O
the	O
qt	O
variables	O
where	O
k	O
is	O
the	O
number	O
of	O
mixture	B
components	O
per	O
source	O
this	O
is	O
too	O
expensive	O
one	O
can	O
use	O
a	O
variational	O
mean	B
field	O
approximation	O
we	O
can	O
then	O
estimate	O
all	O
the	O
source	O
distributions	O
in	O
parallel	O
by	O
fitting	O
a	O
standard	O
gmm	B
to	O
e	O
when	O
the	O
source	O
gmms	O
are	O
independent	B
component	I
analysis	I
known	O
we	O
can	O
compute	O
the	O
marginals	O
pjzj	O
very	O
easily	O
using	O
pjzj	O
jkn	O
jk	O
jk	O
given	O
the	O
pj	O
s	O
we	O
can	O
then	O
use	O
an	O
ica	B
algorithm	O
to	O
estimate	O
w	O
of	O
course	O
these	O
steps	O
should	O
be	O
interleaved	O
the	O
details	O
can	O
be	O
found	O
in	O
other	O
estimation	O
principles	O
it	O
is	O
quite	O
common	O
to	O
estimate	O
the	O
parameters	O
of	O
ica	B
models	O
using	O
methods	O
that	O
seem	O
different	O
to	O
maximum	O
likelihood	B
we	O
will	O
review	O
some	O
of	O
these	O
methods	O
below	O
because	O
they	O
give	O
additional	O
insight	O
into	O
ica	B
however	O
we	O
will	O
also	O
see	O
that	O
these	O
methods	O
in	O
fact	O
are	O
equivalent	O
to	O
maximum	O
likelihood	B
after	O
all	O
our	O
presentation	O
is	O
based	O
on	O
and	O
oja	O
maximizing	O
non-gaussianity	O
an	O
early	O
approach	O
to	O
ica	B
was	O
to	O
find	O
a	O
matrix	O
v	O
such	O
that	O
the	O
distribution	O
z	O
vx	O
is	O
as	O
far	O
from	O
gaussian	B
as	O
possible	O
is	O
a	O
related	O
approach	O
in	O
statistics	O
called	O
projection	B
pursuit	I
one	O
measure	O
of	O
non-gaussianity	O
is	O
kurtosis	B
but	O
this	O
can	O
be	O
sensitive	O
to	O
outliers	B
another	O
measure	O
is	O
the	O
negentropy	B
defined	O
as	O
negentropyz	O
h	O
h	O
where	O
e	O
and	O
var	B
since	O
the	O
gaussian	B
is	O
the	O
maximum	B
entropy	B
distribution	O
this	O
measure	O
is	O
always	O
non-negative	O
and	O
becomes	O
large	O
for	O
distributions	O
that	O
are	O
highly	O
nongaussian	O
we	O
can	O
define	O
our	O
objective	O
as	O
maximizing	O
jv	O
negentropyzj	O
h	O
j	O
j	O
j	O
h	O
j	O
where	O
z	O
vx	O
will	O
be	O
i	O
independently	O
of	O
v	O
so	O
the	O
first	O
term	O
is	O
a	O
constant	O
hence	O
if	O
we	O
fix	O
v	O
to	O
be	O
orthogonal	O
and	O
if	O
we	O
whiten	O
the	O
data	O
the	O
covariance	B
of	O
z	O
jv	O
h	O
const	O
e	O
pzj	O
const	O
j	O
j	O
which	O
we	O
see	O
is	O
equal	O
to	O
a	O
sign	O
change	O
and	O
irrelevant	O
constants	O
to	O
the	O
log-likelihood	O
in	O
equation	O
minimizing	O
mutual	B
information	B
one	O
measure	O
of	O
dependence	O
of	O
a	O
set	O
of	O
random	O
variables	O
is	O
the	O
multi-information	B
pz	O
pzj	O
j	O
j	O
iz	O
kl	O
hzj	O
hz	O
chapter	O
latent	B
linear	O
models	O
we	O
would	O
like	O
to	O
minimize	O
this	O
since	O
we	O
are	O
trying	O
to	O
find	O
independent	O
components	O
put	O
another	O
way	O
we	O
want	O
the	O
best	O
possible	O
factored	O
approximation	O
to	O
the	O
joint	B
distribution	I
now	O
since	O
z	O
vx	O
we	O
have	O
hzj	O
hvx	O
iz	O
j	O
if	O
we	O
constrain	O
v	O
to	O
be	O
orthogonal	O
we	O
can	O
drop	O
the	O
last	O
term	O
since	O
then	O
hvx	O
hx	O
multiplying	O
by	O
v	O
does	O
not	O
change	O
the	O
shape	O
of	O
the	O
distribution	O
and	O
hx	O
is	O
a	O
constant	O
which	O
is	O
is	O
solely	O
determined	O
by	O
the	O
empirical	B
distribution	I
hence	O
we	O
have	O
iz	O
j	O
hzj	O
minimizing	O
this	O
is	O
equivalent	O
to	O
maximizing	O
the	O
negentropy	B
which	O
is	O
equivalent	O
to	O
maximum	O
likelihood	B
maximizing	O
mutual	B
information	B
instead	O
of	O
trying	O
to	O
minimize	O
the	O
mutual	B
information	B
between	O
the	O
components	O
of	O
z	O
let	O
us	O
imagine	O
a	O
neural	B
network	I
where	O
x	O
is	O
the	O
input	O
and	O
yj	O
j	O
x	O
is	O
the	O
noisy	O
output	O
where	O
is	O
some	O
nonlinear	O
scalar	O
function	O
and	O
n	O
it	O
seems	O
reasonable	O
to	O
try	O
to	O
maximize	O
the	O
information	B
flow	O
through	O
this	O
system	O
a	O
principle	O
known	O
as	O
infomax	B
and	O
sejnowski	O
that	O
is	O
we	O
want	O
to	O
maximize	O
the	O
mutual	B
information	B
between	O
y	O
internal	O
neural	O
representation	O
and	O
x	O
observed	O
input	O
signal	O
we	O
have	O
ix	O
y	O
hy	O
hyx	O
where	O
the	O
latter	O
term	O
is	O
constant	O
if	O
we	O
assume	O
the	O
noise	O
has	O
constant	O
variance	B
one	O
can	O
show	O
that	O
we	O
can	O
approximate	O
the	O
former	O
term	O
as	O
follows	O
log	O
j	O
x	O
log	O
detv	O
e	O
hy	O
where	O
as	O
usual	O
we	O
can	O
drop	O
the	O
last	O
term	O
if	O
v	O
is	O
orthogonal	O
if	O
we	O
define	O
to	O
be	O
a	O
cdf	B
then	O
is	O
its	O
pdf	B
and	O
the	O
above	O
expression	O
is	O
equivalent	O
to	O
the	O
log	O
likelihood	B
in	O
particular	O
if	O
we	O
use	O
a	O
logistic	B
nonlinearity	O
sigmz	O
then	O
the	O
corresponding	O
pdf	B
is	O
the	O
logistic	B
distribution	I
and	O
log	O
log	O
coshz	O
irrelevant	O
constants	O
thus	O
we	O
see	O
that	O
infomax	B
is	O
equivalent	O
to	O
maximum	O
likelihood	B
exercises	O
exercise	O
m	B
step	I
for	O
fa	B
for	O
the	O
fa	B
model	O
show	O
that	O
the	O
mle	B
in	O
the	O
m	B
step	I
for	O
w	O
is	O
given	O
by	O
equation	O
exercise	O
map	O
estimation	O
for	O
the	O
fa	B
model	O
derive	O
the	O
m	B
step	I
for	O
the	O
fa	B
model	O
using	O
conjugate	B
priors	I
for	O
the	O
parameters	O
let	O
the	O
empirical	O
covariance	B
matrix	I
have	O
eigenvalues	O
i	O
is	O
a	O
good	O
measure	O
of	O
whether	O
exercise	O
heuristic	O
for	O
assessing	O
applicability	O
of	O
pca	B
d	O
explain	O
why	O
the	O
variance	B
of	O
the	O
evalues	O
d	O
or	O
not	O
pca	B
would	O
be	O
useful	O
for	O
analysing	O
the	O
data	O
higher	O
the	O
value	O
of	O
the	O
more	O
useful	O
pca	B
independent	B
component	I
analysis	I
exercise	O
deriving	O
the	O
second	O
principal	B
component	I
a	O
let	O
n	O
show	O
that	O
j	O
yields	O
vt	O
xi	O
b	O
show	O
that	O
the	O
value	O
of	O
that	O
minimizes	O
vt	O
is	O
given	O
by	O
the	O
eigenvector	O
of	O
c	O
with	O
the	O
second	O
largest	O
eigenvalue	O
hint	O
recall	B
that	O
and	O
xt	O
ax	O
x	O
at	O
exercise	O
deriving	O
the	O
residual	B
error	I
for	O
pca	B
a	O
prove	O
that	O
i	O
xi	O
xt	O
vt	O
j	O
xixt	O
i	O
vj	O
j	O
vj	O
and	O
vt	O
j	O
vk	O
for	O
k	O
j	O
also	O
hint	O
first	O
consider	O
the	O
case	O
k	O
use	O
the	O
fact	O
that	O
vt	O
recall	B
zij	O
xt	O
b	O
now	O
show	O
that	O
i	O
vj	O
jk	O
n	O
vt	O
j	O
xixt	O
i	O
vj	O
n	O
i	O
xi	O
xt	O
i	O
xi	O
xt	O
j	O
j	O
cvj	O
jvt	O
hint	O
recall	B
vt	O
if	O
k	O
d	O
there	O
is	O
no	O
truncation	O
so	O
jd	O
use	O
this	O
to	O
show	O
that	O
the	O
error	O
from	O
only	O
using	O
k	O
d	O
terms	O
is	O
given	O
by	O
j	O
vj	O
j	O
c	O
jk	O
j	O
hint	O
partition	O
the	O
sum	O
j	O
into	O
j	O
and	O
j	O
exercise	O
derivation	O
of	O
fisher	O
s	O
linear	O
discriminant	O
show	O
that	O
the	O
maximum	O
of	O
jw	O
wt	O
sb	O
w	O
where	O
wt	O
sb	O
w	O
wt	O
sw	O
w	O
where	O
f	O
and	O
gx	O
also	O
recall	B
that	O
d	O
dx	O
d	O
dx	O
d	O
dx	O
wt	O
sw	O
w	O
is	O
given	O
by	O
sbw	O
sw	O
w	O
hint	O
recall	B
that	O
the	O
derivative	O
of	O
a	O
ratio	O
of	O
two	O
scalars	O
is	O
given	O
by	O
d	O
dx	O
xt	O
ax	O
at	O
f	O
gx	O
f	O
exercise	O
pca	B
via	O
successive	O
deflation	O
let	O
k	O
be	O
the	O
first	O
k	O
eigenvectors	O
with	O
largest	O
eigenvalues	O
of	O
c	O
n	O
basis	O
vectors	O
these	O
satisfy	O
if	O
j	O
k	O
if	O
j	O
k	O
j	O
vk	O
vt	O
xt	O
x	O
i	O
e	O
the	O
principal	O
we	O
will	O
construct	O
a	O
method	O
for	O
finding	O
the	O
vj	O
sequentially	O
chapter	O
latent	B
linear	O
models	O
as	O
we	O
showed	O
in	O
class	O
is	O
the	O
first	O
principal	O
eigenvector	O
of	O
c	O
and	O
satisfies	O
now	O
define	O
xi	O
as	O
the	O
orthogonal	B
projection	B
of	O
xi	O
onto	O
the	O
space	O
orthogonal	O
to	O
xi	O
p	O
xi	O
define	O
x	O
xn	O
as	O
the	O
deflated	O
matrix	O
of	O
rank	O
d	O
which	O
is	O
obtained	O
by	O
removing	O
from	O
the	O
d	O
dimensional	O
data	O
the	O
component	O
that	O
lies	O
in	O
the	O
direction	O
of	O
the	O
first	O
principal	O
direction	O
x	O
x	O
a	O
using	O
the	O
facts	O
that	O
xt	O
n	O
hence	O
vt	O
xt	O
x	O
n	O
and	O
vt	O
show	O
that	O
the	O
covariance	B
of	O
the	O
deflated	O
matrix	O
is	O
given	O
by	O
c	O
n	O
xt	O
x	O
n	O
xt	O
x	O
b	O
let	O
u	O
be	O
the	O
principal	O
eigenvector	O
of	O
c	O
explain	O
why	O
u	O
may	O
assume	O
u	O
is	O
unit	O
norm	O
c	O
suppose	O
we	O
have	O
a	O
simple	O
method	O
for	O
finding	O
the	O
leading	O
eigenvector	O
and	O
eigenvalue	O
of	O
a	O
pd	O
matrix	O
denoted	O
by	O
u	O
f	O
write	O
some	O
pseudo	O
code	O
for	O
finding	O
the	O
first	O
k	O
principal	O
basis	O
vectors	O
of	O
x	O
that	O
only	O
uses	O
the	O
special	O
f	O
function	O
and	O
simple	O
vector	O
arithmetic	O
i	O
e	O
your	O
code	O
should	O
not	O
use	O
svd	B
or	O
theeig	O
function	O
hint	O
this	O
should	O
be	O
a	O
simple	O
iterative	O
routine	O
that	O
takes	O
lines	O
to	O
write	O
the	O
input	O
is	O
c	O
k	O
and	O
the	O
function	O
f	O
the	O
output	O
should	O
be	O
vj	O
and	O
j	O
for	O
j	O
k	O
do	O
not	O
worry	O
about	O
being	O
syntactically	O
correct	O
exercise	O
latent	B
semantic	I
indexing	I
de	O
freitas	O
in	O
this	O
exercise	O
we	O
study	O
a	O
technique	O
called	O
latent	B
semantic	I
indexing	I
which	O
applies	O
svd	B
to	O
a	O
document	O
by	O
term	O
matrix	O
to	O
create	O
a	O
low-dimensional	O
embedding	B
of	O
the	O
data	O
that	O
is	O
designed	O
to	O
capture	O
semantic	O
similarity	O
of	O
words	O
the	O
file	O
lsidocuments	O
pdf	B
contains	O
documents	O
on	O
various	O
topics	O
a	O
list	O
of	O
all	O
the	O
unique	O
wordsterms	O
that	O
occur	O
in	O
these	O
documents	O
is	O
in	O
lsiwords	O
txt	O
a	O
document	O
by	O
term	O
matrix	O
is	O
in	O
lsimatrix	O
txt	O
a	O
let	O
x	O
be	O
the	O
transpose	O
of	O
lsimatrix	O
so	O
each	O
column	O
represents	O
a	O
document	O
compute	O
the	O
svd	B
of	O
x	O
and	O
make	O
an	O
approximation	O
to	O
it	O
x	O
using	O
the	O
first	O
singular	B
values	I
vectors	O
plot	O
the	O
low	O
dimensional	O
representation	O
of	O
the	O
documents	O
in	O
you	O
should	O
get	O
something	O
like	O
figure	O
b	O
consider	O
finding	O
documents	O
that	O
are	O
about	O
alien	O
abductions	O
if	O
if	O
you	O
look	O
at	O
lsiwords	O
txt	O
there	O
are	O
versions	O
of	O
this	O
word	O
term	O
abducted	O
term	O
abduction	O
and	O
term	O
abductions	O
suppose	O
we	O
want	O
to	O
find	O
documents	O
containing	O
the	O
word	O
abducted	O
documents	O
and	O
contain	O
it	O
but	O
document	O
does	O
not	O
however	O
document	O
is	O
clearly	O
related	O
to	O
this	O
topic	B
thus	O
lsi	B
should	O
also	O
find	O
document	O
create	O
a	O
test	O
document	O
q	O
containing	O
the	O
one	O
word	O
abducted	O
and	O
project	O
it	O
into	O
the	O
subspace	O
to	O
make	O
q	O
now	O
compute	O
the	O
cosine	B
similarity	I
between	O
q	O
and	O
the	O
low	O
dimensional	O
representation	O
of	O
all	O
the	O
documents	O
what	O
are	O
the	O
top	O
closest	O
matches	O
exercise	O
imputation	B
in	O
a	O
fa	B
model	O
derive	O
an	O
expression	O
for	O
pxhxv	O
for	O
a	O
fa	B
model	O
exercise	O
efficiently	O
evaluating	O
the	O
ppca	B
density	O
derive	O
an	O
expression	O
for	O
px	O
w	O
for	O
the	O
ppca	B
model	O
based	O
on	O
plugging	O
in	O
the	O
mles	O
and	O
using	O
the	O
matrix	B
inversion	I
lemma	I
independent	B
component	I
analysis	I
figure	O
projection	B
of	O
documents	O
into	O
dimensions	O
figure	O
generated	O
by	O
lsicode	O
exercise	O
ppca	B
vs	O
fa	B
exercise	O
of	O
et	O
al	O
due	O
to	O
hinton	O
generate	O
observations	O
from	O
the	O
following	O
model	O
where	O
zi	O
n	O
i	O
fit	O
a	O
fa	B
and	O
pca	B
model	O
with	O
latent	B
factor	B
hence	O
show	O
that	O
the	O
corresponding	O
weight	B
vector	I
w	O
aligns	O
with	O
the	O
maximal	O
variance	B
direction	O
in	O
the	O
pca	B
case	O
but	O
with	O
the	O
maximal	O
correlation	O
direction	O
in	O
the	O
case	O
of	O
fa	B
sparse	B
linear	O
models	O
introduction	O
we	O
introduced	O
the	O
topic	B
of	O
feature	B
selection	I
in	O
section	O
where	O
we	O
discussed	O
methods	O
for	O
finding	O
input	O
variables	O
which	O
had	O
high	O
mutual	B
information	B
with	O
the	O
output	O
the	O
trouble	O
with	O
this	O
approach	O
is	O
that	O
it	O
is	O
based	O
on	O
a	O
myopic	O
strategy	O
that	O
only	O
looks	O
at	O
one	O
variable	O
at	O
a	O
time	O
this	O
can	O
fail	O
if	O
there	O
are	O
interaction	B
effects	I
for	O
example	O
if	O
y	O
then	O
neither	O
nor	O
on	O
its	O
own	O
can	O
predict	O
the	O
response	O
but	O
together	O
they	O
perfectly	O
predict	O
the	O
response	O
for	O
a	O
real-world	O
example	O
of	O
this	O
consider	O
genetic	O
association	O
studies	O
sometimes	O
two	O
genes	O
on	O
their	O
own	O
may	O
be	O
harmless	O
but	O
when	O
present	O
together	O
they	O
cause	O
a	O
recessive	O
disease	O
in	O
this	O
chapter	O
we	O
focus	O
on	O
selecting	O
sets	O
of	O
variables	O
at	O
a	O
time	O
using	O
a	O
model-based	O
approach	O
if	O
the	O
model	O
is	O
a	O
generalized	B
linear	I
model	I
of	O
the	O
form	O
pyx	O
yf	O
x	O
for	O
some	O
link	B
function	I
f	O
then	O
we	O
can	O
perform	O
feature	B
selection	I
by	O
encouraging	O
the	O
weight	B
vector	I
w	O
to	O
be	O
sparse	B
i	O
e	O
to	O
have	O
lots	O
of	O
zeros	O
this	O
approach	O
turns	O
out	O
to	O
offer	O
significant	O
computational	O
advantages	O
as	O
we	O
will	O
see	O
below	O
here	O
are	O
some	O
applications	O
where	O
feature	B
selection	I
sparsity	B
is	O
useful	O
in	O
many	O
problems	O
we	O
have	O
many	O
more	O
dimensions	O
d	O
than	O
training	O
cases	O
n	O
the	O
corresponding	O
design	B
matrix	I
is	O
short	O
and	O
fat	O
rather	O
than	O
tall	O
and	O
skinny	O
this	O
is	O
called	O
the	O
small	B
n	I
large	O
d	O
problem	O
this	O
is	O
becoming	O
increasingly	O
prevalent	O
as	O
we	O
develop	O
more	O
high	B
throughput	I
measurement	O
devices	O
for	O
example	O
with	O
gene	B
microarrays	I
it	O
is	O
common	O
to	O
measure	O
the	O
expression	O
levels	O
of	O
d	O
genes	O
but	O
to	O
only	O
get	O
n	O
such	O
examples	O
is	O
perhaps	O
a	O
sign	O
of	O
the	O
times	O
that	O
even	O
our	O
data	O
seems	O
to	O
be	O
getting	O
fatter	O
we	O
may	O
want	O
to	O
find	O
the	O
smallest	O
set	O
of	O
features	B
that	O
can	O
accurately	O
predict	O
the	O
response	O
growth	O
rate	B
of	O
the	O
cell	O
in	O
order	O
to	O
prevent	O
overfitting	O
to	O
reduce	O
the	O
cost	O
of	O
building	O
a	O
diagnostic	O
device	O
or	O
to	O
help	O
with	O
scientific	O
insight	O
into	O
the	O
problem	O
in	O
chapter	O
we	O
will	O
use	O
basis	B
functions	I
centered	O
on	O
the	O
training	O
examples	O
so	O
xn	O
where	O
is	O
a	O
kernel	B
function	I
the	O
resulting	O
design	B
matrix	I
has	O
size	O
n	O
n	O
feature	B
selection	I
in	O
this	O
context	O
is	O
equivalent	O
to	O
selecting	O
a	O
subset	O
of	O
the	O
training	O
examples	O
which	O
can	O
help	O
reduce	O
overfitting	O
and	O
computational	O
cost	O
this	O
is	O
known	O
as	O
a	O
sparse	B
kernel	B
machine	I
in	O
terms	O
of	O
in	O
signal	B
processing	I
it	O
is	O
common	O
to	O
represent	O
signals	O
speech	O
etc	O
wavelet	B
basis	B
functions	I
to	O
save	O
time	O
and	O
space	O
it	O
is	O
useful	O
to	O
find	O
a	O
sparse	B
representation	I
chapter	O
sparse	B
linear	O
models	O
of	O
the	O
signals	O
in	O
terms	O
of	O
a	O
small	O
number	O
of	O
such	O
basis	B
functions	I
this	O
allows	O
us	O
to	O
estimate	O
signals	O
from	O
a	O
small	O
number	O
of	O
measurements	O
as	O
well	O
as	O
to	O
compress	O
the	O
signal	O
see	O
section	O
for	O
more	O
information	B
note	O
that	O
the	O
topic	B
of	O
feature	B
selection	I
and	O
sparsity	B
is	O
currently	O
one	O
of	O
the	O
most	O
active	O
areas	O
in	O
this	O
chapter	O
we	O
only	O
have	O
space	O
to	O
give	O
an	O
overview	O
of	O
the	O
of	O
machine	B
learning	B
statistics	O
main	O
results	O
bayesian	B
variable	O
selection	O
a	O
natural	O
way	O
to	O
pose	O
the	O
variable	O
selection	O
problem	O
is	O
as	O
follows	O
let	O
j	O
if	O
feature	O
j	O
is	O
relevant	O
and	O
let	O
j	O
otherwise	O
our	O
goal	O
is	O
to	O
compute	O
the	O
posterior	O
over	O
models	O
p	O
e	O
f	O
e	O
f	O
where	O
f	O
is	O
the	O
cost	O
function	O
f	O
pd	O
log	O
p	O
for	O
example	O
suppose	O
we	O
generate	O
n	O
samples	B
from	O
a	O
d	O
dimensional	O
linear	B
regression	B
model	O
yi	O
n	O
xi	O
in	O
particular	O
we	O
use	O
w	O
and	O
we	O
enumerate	O
all	O
models	O
and	O
compute	O
p	O
for	O
each	O
one	O
give	O
the	O
equations	O
for	O
this	O
below	O
we	O
order	O
the	O
models	O
in	O
gray	B
code	I
order	O
which	O
ensures	O
consecutive	O
vectors	O
differ	O
by	O
exactly	O
bit	O
reasons	O
for	O
this	O
are	O
computational	O
and	O
are	O
discussed	O
in	O
section	O
in	O
which	O
k	O
elements	O
of	O
w	O
are	O
non-zero	O
the	O
resulting	O
set	O
of	O
bit	O
patterns	O
is	O
shown	O
in	O
figure	O
the	O
cost	O
of	O
each	O
model	O
f	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
this	O
objective	O
function	O
is	O
extremely	O
bumpy	O
the	O
results	O
are	O
easier	O
to	O
interpret	O
if	O
we	O
compute	O
the	O
posterior	O
distribution	O
over	O
models	O
p	O
this	O
is	O
shown	O
in	O
figure	O
the	O
top	O
models	O
are	O
listed	O
below	O
model	O
prob	O
members	O
the	O
true	O
model	O
is	O
however	O
the	O
coefficients	O
associated	O
with	O
features	B
and	O
are	O
very	O
small	O
to	O
so	O
these	O
variables	O
are	O
harder	O
to	O
detect	O
given	O
enough	O
data	O
the	O
method	O
will	O
converge	B
on	O
the	O
true	O
model	O
the	O
data	O
is	O
generated	O
from	O
a	O
linear	O
model	O
but	O
for	O
finite	O
data	O
sets	O
there	O
will	O
usually	O
be	O
considerable	O
posterior	O
uncertainty	B
interpreting	O
the	O
posterior	O
over	O
a	O
large	O
number	O
of	O
models	O
is	O
quite	O
difficult	O
so	O
we	O
will	O
seek	O
various	O
summary	O
statistics	O
a	O
natural	O
one	O
is	O
the	O
posterior	B
mode	B
or	O
map	B
estimate	I
argmax	O
p	O
argmin	O
f	O
bayesian	B
variable	O
selection	O
pmodeldata	O
log	O
pmodel	O
data	O
pgammajdata	O
figure	O
all	O
possible	O
models	O
marginal	O
inclusion	B
probabilities	I
figure	O
generated	O
by	O
linregallsubsetsgraycodedemo	O
all	O
possible	O
bit	O
vectors	O
of	O
length	O
enumerated	O
in	O
gray	B
code	I
order	O
score	B
function	I
for	O
posterior	O
over	O
all	O
models	O
vertical	O
scale	O
has	O
been	O
truncated	O
at	O
for	O
clarity	O
however	O
the	O
mode	B
is	O
often	O
not	O
representative	O
of	O
the	O
full	B
posterior	O
mass	O
section	O
a	O
better	O
summary	O
is	O
the	O
median	B
model	I
and	O
berger	O
carvahlo	O
and	O
lawrence	O
computed	O
using	O
p	O
j	O
this	O
requires	O
computing	O
the	O
posterior	O
marginal	O
inclusion	B
probabilities	I
p	O
j	O
these	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
model	O
is	O
confident	O
that	O
variables	O
and	O
are	O
included	O
if	O
we	O
lower	O
the	O
decision	B
threshold	O
to	O
we	O
would	O
add	O
and	O
as	O
well	O
however	O
if	O
we	O
wanted	O
to	O
capture	O
variable	O
we	O
would	O
incur	O
two	O
false	O
positives	O
and	O
this	O
tradeoff	O
between	O
false	O
positives	O
and	O
false	O
negatives	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
the	O
above	O
example	O
illustrates	O
the	O
gold	O
standard	O
for	O
variable	O
selection	O
the	O
problem	O
was	O
sufficiently	O
small	O
variables	O
that	O
we	O
were	O
able	O
to	O
compute	O
the	O
full	B
posterior	O
exactly	O
of	O
course	O
variable	O
selection	O
is	O
most	O
useful	O
in	O
the	O
cases	O
where	O
the	O
number	O
of	O
dimensions	O
is	O
large	O
since	O
there	O
are	O
possible	O
models	O
vectors	O
it	O
will	O
be	O
impossible	O
to	O
compute	O
the	O
full	B
posterior	O
in	O
general	O
and	O
even	O
finding	O
summaries	O
such	O
as	O
the	O
map	B
estimate	I
or	O
marginal	O
chapter	O
sparse	B
linear	O
models	O
inclusion	B
probabilities	I
will	O
be	O
intractable	O
we	O
will	O
therefore	O
spend	O
most	O
of	O
this	O
chapter	O
focussing	O
on	O
algorithmic	O
speedups	O
but	O
before	O
we	O
do	O
that	O
we	O
will	O
explain	O
how	O
we	O
computed	O
p	O
in	O
the	O
above	O
example	O
the	O
spike	B
and	I
slab	I
model	O
the	O
posterior	O
is	O
given	O
by	O
p	O
p	O
we	O
first	O
consider	O
the	O
prior	O
then	O
the	O
likelihood	B
it	O
is	O
common	O
to	O
use	O
the	O
following	O
prior	O
on	O
the	O
bit	O
vector	O
p	O
ber	O
j	O
where	O
is	O
the	O
probability	O
a	O
feature	O
is	O
relevant	O
and	O
j	O
is	O
the	O
pseudo-norm	O
that	O
is	O
the	O
number	O
of	O
non-zero	O
elements	O
of	O
the	O
vector	O
for	O
comparison	O
with	O
later	O
models	O
it	O
is	O
useful	O
to	O
write	O
the	O
log	O
prior	O
as	O
follows	O
log	O
p	O
log	O
const	O
const	O
where	O
log	O
we	O
can	O
write	O
the	O
likelihood	B
as	O
follows	O
pd	O
pyx	O
controls	O
the	O
sparsity	B
of	O
the	O
model	O
pyx	O
w	O
for	O
notational	O
simplicity	O
we	O
have	O
assumed	O
the	O
response	O
is	O
centered	O
y	O
so	O
we	O
can	O
ignore	O
any	O
offset	O
term	O
we	O
now	O
discuss	O
the	O
prior	O
pw	O
if	O
j	O
feature	O
j	O
is	O
irrelevant	O
so	O
we	O
expect	O
wj	O
if	O
j	O
we	O
expect	O
wj	O
to	O
be	O
non-zero	O
if	O
we	O
standardize	O
the	O
inputs	O
a	O
reasonable	O
prior	O
is	O
n	O
w	O
controls	O
how	O
big	O
we	O
expect	O
the	O
coefficients	O
associated	O
with	O
the	O
relevant	O
variables	O
to	O
be	O
is	O
scaled	O
by	O
the	O
overall	O
noise	O
level	O
we	O
can	O
summarize	O
this	O
prior	O
as	O
follows	O
w	O
where	O
n	O
w	O
if	O
j	O
if	O
j	O
w	O
the	O
distribution	O
pwj	O
j	O
approaches	O
the	O
first	O
term	O
is	O
a	O
spike	O
at	O
the	O
origin	O
as	O
a	O
uniform	B
distribution	I
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
slab	O
of	O
constant	O
height	O
hence	O
this	O
is	O
called	O
the	O
spike	B
and	I
slab	I
model	O
and	O
beauchamp	O
we	O
can	O
drop	O
the	O
coefficients	O
wj	O
for	O
which	O
wj	O
from	O
the	O
model	O
since	O
they	O
are	O
clamped	O
to	O
zero	O
under	O
the	O
prior	O
hence	O
equation	O
becomes	O
the	O
following	O
a	O
gaussian	B
likelihood	B
pwj	O
j	O
pd	O
n	O
w	O
wid	O
d	O
bayesian	B
variable	O
selection	O
where	O
d	O
is	O
the	O
number	O
of	O
non-zero	O
elements	O
in	O
in	O
what	O
follows	O
we	O
will	O
generalize	B
this	O
slightly	O
by	O
defining	O
a	O
prior	O
of	O
the	O
form	O
pw	O
for	O
any	O
positive	O
definite	O
matrix	O
given	O
these	O
priors	O
we	O
can	O
now	O
compute	O
the	O
marginal	B
likelihood	B
if	O
the	O
noise	O
variance	B
is	O
known	O
we	O
can	O
write	O
down	O
the	O
marginal	B
likelihood	B
equation	O
as	O
follows	O
pd	O
n	O
w	O
n	O
c	O
c	O
xt	O
if	O
the	O
noise	O
is	O
unknown	B
we	O
can	O
put	O
a	O
prior	O
on	O
it	O
and	O
integrate	O
it	O
out	O
it	O
is	O
common	O
to	O
use	O
p	O
ig	O
b	O
some	O
guidelines	O
on	O
setting	O
a	O
b	O
can	O
be	O
found	O
in	O
et	O
al	O
if	O
we	O
use	O
a	O
b	O
we	O
recover	O
the	O
jeffrey	O
s	O
prior	O
p	O
when	O
we	O
integrate	B
out	I
the	O
noise	O
we	O
get	O
the	O
following	O
more	O
complicated	O
expression	O
for	O
the	O
marginal	B
likelihood	B
et	O
al	O
pd	O
py	O
w	O
d	O
where	O
s	O
is	O
the	O
rss	O
x	O
s	O
s	O
yt	O
y	O
yt	O
x	O
x	O
y	O
see	O
also	O
exercise	O
when	O
the	O
marginal	B
likelihood	B
cannot	O
be	O
computed	O
in	O
closed	O
form	O
if	O
we	O
are	O
using	O
logistic	B
regression	B
or	O
a	O
nonlinear	O
model	O
we	O
can	O
approximate	O
it	O
using	O
bic	B
which	O
has	O
the	O
form	O
log	O
pd	O
log	O
pyx	O
w	O
where	O
w	O
is	O
the	O
ml	O
or	O
map	B
estimate	I
based	O
on	O
x	O
and	O
is	O
the	O
degrees	B
of	I
freedom	I
of	O
the	O
model	O
et	O
al	O
adding	O
the	O
log	O
prior	O
the	O
overall	O
objective	O
becomes	O
log	O
n	O
log	O
p	O
log	O
pyx	O
w	O
log	O
n	O
const	O
we	O
see	O
that	O
there	O
are	O
two	O
complexity	O
penalties	O
one	O
arising	O
from	O
the	O
bic	B
approximation	O
to	O
the	O
marginal	B
likelihood	B
and	O
the	O
other	O
arising	O
from	O
the	O
prior	O
on	O
p	O
obviously	O
these	O
can	O
be	O
combined	O
into	O
one	O
overall	O
complexity	O
parameter	B
which	O
we	O
will	O
denote	O
by	O
from	O
the	O
bernoulli-gaussian	B
model	O
to	O
regularization	B
another	O
model	O
that	O
is	O
sometimes	O
used	O
and	O
mallick	O
zhou	O
et	O
al	O
soussen	O
et	O
al	O
is	O
the	O
following	O
yixi	O
w	O
n	O
jwjxij	O
j	O
j	O
ber	O
wj	O
n	O
w	O
for	O
reasons	O
explained	O
in	O
section	O
also	O
it	O
is	O
common	O
to	O
use	O
a	O
g-prior	B
of	O
the	O
form	O
gxt	O
exercise	O
various	O
approaches	O
have	O
been	O
proposed	O
for	O
setting	O
g	O
including	O
cross	B
validation	I
empirical	B
bayes	I
george	O
and	O
foster	O
hierarchical	O
bayes	O
et	O
al	O
etc	O
x	O
chapter	O
sparse	B
linear	O
models	O
et	O
al	O
this	O
is	O
called	O
the	O
bernoulliin	O
the	O
signal	B
processing	I
literature	O
gaussian	B
model	O
although	O
we	O
could	O
also	O
call	O
it	O
the	O
binary	B
mask	I
model	O
since	O
we	O
can	O
think	O
of	O
the	O
j	O
variables	O
as	O
masking	O
out	O
the	O
weights	O
wj	O
unlike	O
the	O
spike	B
and	I
slab	I
model	O
we	O
do	O
not	O
integrate	B
out	I
the	O
irrelevant	O
coefficients	O
they	O
always	O
exist	O
in	O
addition	O
the	O
binary	B
mask	I
model	O
has	O
the	O
form	O
j	O
y	O
wj	O
whereas	O
the	O
spike	B
and	I
slab	I
model	O
has	O
the	O
form	O
j	O
wj	O
y	O
in	O
the	O
binary	B
mask	I
model	O
only	O
the	O
product	O
jwj	O
can	O
be	O
identified	O
from	O
the	O
likelihood	B
one	O
interesting	O
aspect	O
of	O
this	O
model	O
is	O
that	O
it	O
can	O
be	O
used	O
to	O
derive	O
an	O
objective	O
function	O
that	O
is	O
widely	O
used	O
in	O
the	O
subset	O
selection	O
literature	O
first	O
note	O
that	O
the	O
joint	O
prior	O
has	O
the	O
form	O
p	O
w	O
n	O
wi	O
hence	O
the	O
scaled	O
unnormalized	O
negative	O
log	O
posterior	O
has	O
the	O
form	O
f	O
w	O
log	O
p	O
w	O
yx	O
x	O
const	O
w	O
where	O
log	O
let	O
us	O
split	O
w	O
into	O
two	O
subvectors	O
w	O
and	O
w	O
indexed	O
by	O
the	O
zero	O
and	O
non-zero	O
entries	O
of	O
respectively	O
since	O
x	O
w	O
x	O
w	O
we	O
can	O
just	O
set	O
w	O
w	O
so	O
we	O
do	O
not	O
regularize	O
the	O
non-zero	O
weights	O
there	O
is	O
no	O
complexity	O
penalty	O
coming	O
from	O
the	O
marginal	B
likelihood	B
or	O
its	O
bic	B
approximation	O
in	O
this	O
case	O
the	O
objective	O
becomes	O
now	O
consider	O
the	O
case	O
where	O
f	O
w	O
x	O
w	O
this	O
is	O
similar	B
to	O
the	O
bic	B
objective	O
above	O
instead	O
of	O
keeping	O
track	O
of	O
the	O
bit	O
vector	O
we	O
can	O
define	O
the	O
set	O
of	O
relevant	O
variables	O
to	O
be	O
the	O
support	B
or	O
set	O
of	O
non-zero	O
entries	O
of	O
w	O
then	O
we	O
can	O
rewrite	O
the	O
above	O
equation	O
as	O
follows	O
f	O
this	O
is	O
called	O
regularization	B
we	O
have	O
converted	O
the	O
discrete	B
optimization	B
problem	O
into	O
a	O
continuous	O
one	O
w	O
r	O
d	O
however	O
the	O
pseudo-norm	O
makes	O
the	O
objective	O
very	O
non	O
smooth	O
so	O
this	O
is	O
still	O
hard	O
to	O
optimize	O
we	O
will	O
discuss	O
different	O
solutions	O
to	O
this	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
algorithms	O
since	O
there	O
are	O
models	O
we	O
cannot	O
explore	O
the	O
full	B
posterior	O
or	O
find	O
the	O
globally	O
optimal	O
model	O
instead	O
we	O
will	O
have	O
to	O
resort	O
to	O
heuristics	B
of	O
one	O
form	O
or	O
another	O
all	O
of	O
the	O
methods	O
we	O
will	O
discuss	O
involve	O
searching	O
through	O
the	O
space	O
of	O
models	O
and	O
evaluating	O
the	O
cost	O
f	O
at	O
bayesian	B
variable	O
selection	O
all	O
subsets	O
on	O
prostate	O
cancer	O
r	O
o	O
r	O
r	O
e	O
t	O
e	O
s	O
i	O
g	O
n	O
n	O
a	O
r	O
t	O
i	O
subset	O
size	O
figure	O
a	O
lattice	B
of	O
subsets	O
of	O
residual	B
sum	B
of	I
squares	I
versus	O
subset	O
size	O
on	O
the	O
prostate	O
cancer	O
data	O
set	O
the	O
lower	O
envelope	O
is	O
the	O
best	O
rss	O
achievable	O
for	O
any	O
set	O
of	O
a	O
given	O
size	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
prostatesubsets	O
each	O
point	O
this	O
requires	O
fitting	O
the	O
model	O
computing	O
argmax	O
pdw	O
or	O
evaluating	O
its	O
pdwpwdw	O
at	O
each	O
step	O
this	O
is	O
sometimes	O
called	O
marginal	B
likelihood	B
computing	O
the	O
wrapper	B
method	I
since	O
we	O
wrap	O
our	O
search	O
for	O
the	O
best	O
model	O
set	O
of	O
good	O
models	O
around	O
a	O
generic	O
model-fitting	O
procedure	O
in	O
order	O
to	O
make	O
wrapper	O
methods	O
efficient	O
it	O
is	O
important	O
that	O
we	O
can	O
quickly	O
evaluate	O
the	O
score	B
function	I
for	O
some	O
new	O
model	O
given	O
the	O
score	O
of	O
a	O
previous	O
model	O
this	O
can	O
be	O
done	O
provided	O
we	O
can	O
efficiently	O
update	O
the	O
sufficient	B
statistics	I
needed	O
to	O
compute	O
f	O
this	O
is	O
possible	O
provided	O
only	O
differs	O
from	O
in	O
one	O
bit	O
to	O
adding	O
or	O
removing	O
a	O
single	O
variable	O
and	O
provided	O
f	O
only	O
depends	O
on	O
the	O
data	O
via	O
x	O
in	O
this	O
case	O
we	O
can	O
use	O
rank-one	O
matrix	O
updates	O
downdates	O
to	O
efficiently	O
compute	O
xt	O
x	O
these	O
updates	O
are	O
usually	O
applied	O
to	O
the	O
qr	B
decomposition	I
of	O
x	O
see	O
e	O
g	O
schniter	O
et	O
al	O
for	O
details	O
x	O
from	O
xt	O
greedy	O
search	O
suppose	O
we	O
want	O
to	O
find	O
the	O
map	O
model	O
if	O
we	O
use	O
the	O
objective	O
in	O
equation	O
we	O
can	O
exploit	O
properties	O
of	O
least	B
squares	I
to	O
derive	O
various	O
efficient	O
greedy	O
forwards	O
search	O
methods	O
some	O
of	O
which	O
we	O
summarize	O
below	O
for	O
further	O
details	O
see	O
soussen	O
et	O
al	O
single	B
best	I
replacement	I
the	O
simplest	O
method	O
is	O
to	O
use	O
greedy	O
hill	B
climbing	I
where	O
at	O
each	O
step	O
we	O
define	O
the	O
neighborhood	O
of	O
the	O
current	O
model	O
to	O
be	O
all	O
models	O
than	O
can	O
be	O
reached	O
by	O
flipping	O
a	O
single	O
bit	O
of	O
i	O
e	O
for	O
each	O
variable	O
if	O
it	O
is	O
currently	O
out	O
of	O
the	O
model	O
we	O
consider	O
adding	O
it	O
and	O
if	O
it	O
is	O
currently	O
in	O
the	O
model	O
we	O
consider	O
removing	O
it	O
in	O
et	O
al	O
they	O
call	O
this	O
the	O
single	B
best	I
replacement	I
since	O
we	O
are	O
expecting	O
a	O
sparse	B
solution	O
we	O
can	O
start	O
with	O
the	O
empty	O
set	O
we	O
are	O
essentially	O
moving	O
through	O
the	O
lattice	B
of	O
subsets	O
shown	O
in	O
figure	O
we	O
continue	O
adding	O
or	O
removing	O
until	O
no	O
improvement	O
is	O
possible	O
orthogonal	B
least	B
squares	I
if	O
we	O
set	O
in	O
equation	O
so	O
there	O
is	O
no	O
complexity	O
penalty	O
there	O
will	O
be	O
no	O
reason	O
to	O
perform	O
deletion	O
steps	O
in	O
this	O
case	O
the	O
sbr	O
algorithm	O
is	O
equivalent	O
to	O
orthogonal	B
least	B
squares	I
and	O
wigger	O
which	O
in	O
turn	O
is	O
equivalent	O
chapter	O
sparse	B
linear	O
models	O
to	O
greedy	O
forwards	B
selection	I
in	O
this	O
algorithm	O
we	O
start	O
with	O
the	O
empty	O
set	O
and	O
add	O
the	O
best	O
feature	O
at	O
each	O
step	O
the	O
error	O
will	O
go	O
down	O
monotonically	O
with	O
as	O
shown	O
in	O
figure	O
we	O
can	O
pick	O
the	O
next	O
best	O
feature	O
j	O
to	O
add	O
to	O
the	O
current	O
set	O
t	O
by	O
solving	O
j	O
t	O
min	O
w	O
arg	O
min	O
t	O
we	O
then	O
update	O
the	O
active	B
set	I
by	O
setting	O
to	O
choose	O
the	O
next	O
feature	O
to	O
add	O
at	O
step	O
t	O
we	O
need	O
to	O
solve	O
d	O
dt	O
least	B
squares	I
problems	O
at	O
step	O
t	O
where	O
dt	O
t	O
is	O
the	O
cardinality	O
of	O
the	O
current	O
active	B
set	I
having	O
chosen	O
the	O
best	O
feature	O
to	O
add	O
we	O
need	O
to	O
solve	O
an	O
additional	O
least	B
squares	I
problem	O
to	O
compute	O
orthogonal	B
matching	B
pursuits	I
orthogonal	B
least	B
squares	I
is	O
somewhat	O
expensive	O
a	O
simplification	O
is	O
to	O
freeze	O
the	O
current	O
weights	O
at	O
their	O
current	O
value	O
and	O
then	O
to	O
pick	O
the	O
next	O
feature	O
to	O
add	O
by	O
solving	O
j	O
arg	O
min	O
t	O
min	O
xwt	O
where	O
rt	O
this	O
inner	O
optimization	B
is	O
easy	O
to	O
solve	O
we	O
simply	O
set	O
xt	O
y	O
xwt	O
is	O
the	O
current	O
residual	B
vector	O
if	O
the	O
columns	O
are	O
unit	O
norm	O
we	O
have	O
j	O
arg	O
max	O
xt	O
so	O
we	O
are	O
just	O
looking	O
for	O
the	O
column	O
that	O
is	O
most	O
correlated	O
with	O
the	O
current	O
residual	B
we	O
then	O
update	O
the	O
active	B
set	I
and	O
compute	O
the	O
new	O
least	B
squares	I
estimate	O
using	O
x	O
this	O
method	O
is	O
called	O
orthogonal	B
matching	B
pursuits	I
or	O
omp	B
et	O
al	O
this	O
only	O
requires	O
one	O
least	B
squares	I
calculation	O
per	O
iteration	O
and	O
so	O
is	O
faster	O
than	O
orthogonal	B
least	B
squares	I
but	O
is	O
not	O
quite	O
as	O
accurate	O
and	O
davies	O
matching	B
pursuits	I
an	O
even	O
more	O
aggressive	O
approximation	O
is	O
to	O
just	O
greedily	O
add	O
the	O
feature	O
that	O
is	O
most	O
correlated	O
with	O
the	O
current	O
residual	B
this	O
is	O
called	O
matching	B
pursuits	I
and	O
zhang	O
this	O
is	O
also	O
equivalent	O
to	O
a	O
method	O
known	O
as	O
least	B
squares	I
boosting	B
backwards	B
selection	I
backwards	B
selection	I
starts	O
with	O
all	O
variables	O
in	O
the	O
model	O
socalled	O
saturated	B
model	I
and	O
then	O
deletes	O
the	O
worst	O
one	O
at	O
each	O
step	O
this	O
is	O
equivalent	O
to	O
performing	O
a	O
greedy	O
search	O
from	O
the	O
top	O
of	O
the	O
lattice	B
downwards	O
this	O
can	O
give	O
better	O
results	O
than	O
a	O
bottom-up	O
search	O
since	O
the	O
decision	B
about	O
whether	O
to	O
keep	O
a	O
variable	O
or	O
not	O
is	O
made	O
in	O
the	O
context	O
of	O
all	O
the	O
other	O
variables	O
that	O
might	O
depende	O
on	O
it	O
however	O
this	O
method	O
is	O
typically	O
infeasible	O
for	O
large	O
problems	O
since	O
the	O
saturated	B
model	I
will	O
be	O
too	O
expensive	O
to	O
fit	O
foba	O
the	O
forwards-backwards	B
algorithm	I
of	O
is	O
similar	B
to	O
the	O
single	B
best	I
replacement	I
algorithm	O
presented	O
above	O
except	O
it	O
uses	O
an	O
omp-like	O
approximation	O
when	O
choosing	O
the	O
next	O
move	O
to	O
make	O
a	O
similar	B
dual-pass	O
algorithm	O
was	O
described	O
in	O
et	O
al	O
bayesian	B
matching	B
pursuit	I
the	O
algorithm	O
of	O
et	O
al	O
is	O
similiar	O
to	O
omp	B
except	O
it	O
uses	O
a	O
bayesian	B
marginal	B
likelihood	B
scoring	O
criterion	O
a	O
spike	B
and	I
slab	I
model	O
instead	O
of	O
a	O
least	B
squares	I
objective	O
in	O
addition	O
it	O
uses	O
a	O
form	O
of	O
beam	B
search	I
to	O
explore	O
multiple	O
paths	O
through	O
the	O
lattice	B
at	O
once	O
regularization	B
basics	O
stochastic	B
search	I
if	O
we	O
want	O
to	O
approximate	O
the	O
posterior	O
rather	O
than	O
just	O
computing	O
a	O
mode	B
because	O
we	O
want	O
to	O
compute	O
marginal	O
inclusion	B
probabilities	I
one	O
option	O
is	O
to	O
use	O
mcmc	B
the	O
standard	O
approach	O
is	O
to	O
use	O
metropolis	B
hastings	I
where	O
the	O
proposal	B
distribution	I
just	O
flips	O
single	O
bits	B
this	O
enables	O
us	O
to	O
efficiently	O
compute	O
p	O
given	O
p	O
the	O
probability	O
of	O
a	O
state	B
configuration	O
is	O
estimated	O
by	O
counting	O
how	O
many	O
times	O
the	O
random	O
walk	O
visits	O
this	O
state	B
see	O
hara	O
and	O
sillanpaa	O
for	O
a	O
review	O
of	O
such	O
methods	O
and	O
and	O
richardson	O
for	O
a	O
very	O
recent	O
method	O
based	O
on	O
evolutionary	B
mcmc	B
however	O
in	O
a	O
discrete	B
state	B
space	I
mcmc	B
is	O
needlessly	O
inefficient	O
since	O
we	O
can	O
compute	O
the	O
probability	O
of	O
a	O
state	B
directly	O
using	O
p	O
exp	O
f	O
thus	O
there	O
is	O
no	O
need	O
to	O
ever	O
revisit	O
a	O
state	B
a	O
much	O
more	O
efficient	O
alternative	O
is	O
to	O
use	O
some	O
kind	O
of	O
stochastic	B
search	I
algorithm	O
to	O
generate	O
a	O
set	O
s	O
of	O
high	O
scoring	O
models	O
and	O
then	O
to	O
make	O
the	O
following	O
approximation	O
p	O
e	O
f	O
s	O
e	O
f	O
see	O
and	O
scott	O
for	O
a	O
review	O
of	O
recent	O
methods	O
of	O
this	O
kind	O
em	B
and	O
variational	B
inference	B
it	O
is	O
tempting	O
to	O
apply	O
em	B
to	O
the	O
spike	B
and	I
slab	I
model	O
which	O
has	O
the	O
form	O
j	O
wj	O
y	O
we	O
can	O
compute	O
p	O
j	O
in	O
the	O
e	B
step	I
and	O
optimize	O
w	O
in	O
the	O
m	B
step	I
however	O
this	O
will	O
not	O
work	O
because	O
when	O
we	O
compute	O
p	O
j	O
we	O
are	O
comparing	O
a	O
delta-function	O
with	O
a	O
gaussian	B
pdf	B
n	O
w	O
we	O
can	O
replace	O
the	O
delta	O
function	O
with	O
a	O
narrow	O
gaussian	B
and	O
then	O
the	O
e	B
step	I
amounts	O
to	O
classifying	O
wj	O
under	O
the	O
two	O
possible	O
gaussian	B
models	O
however	O
this	O
is	O
likely	O
to	O
suffer	O
from	O
severe	O
local	O
minima	O
an	O
alternative	O
is	O
to	O
apply	O
em	B
to	O
the	O
bernoulli-gaussian	B
model	O
which	O
has	O
the	O
form	O
j	O
y	O
wj	O
in	O
this	O
case	O
the	O
posterior	O
p	O
w	O
is	O
intractable	O
to	O
compute	O
because	O
all	O
the	O
bits	B
become	O
correlated	O
due	O
to	O
explaining	B
away	I
however	O
it	O
is	O
possible	O
to	O
derive	O
a	O
mean	B
field	O
approximation	O
of	O
the	O
form	O
j	O
q	O
jqwj	O
et	O
al	O
rattray	O
et	O
al	O
regularization	B
basics	O
when	O
we	O
have	O
many	O
variables	O
it	O
is	O
computationally	O
difficult	O
to	O
find	O
the	O
posterior	B
mode	B
of	O
p	O
and	O
although	O
greedy	O
algorithms	O
often	O
work	O
well	O
e	O
g	O
for	O
a	O
theoretical	O
analysis	O
they	O
can	O
of	O
course	O
get	O
stuck	O
in	O
local	O
optima	O
part	O
of	O
the	O
problem	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
j	O
variables	O
are	O
discrete	B
j	O
in	O
the	O
optimization	B
community	O
it	O
is	O
common	O
to	O
relax	O
hard	O
constraints	O
of	O
this	O
form	O
by	O
replacing	O
discrete	B
variables	O
with	O
continuous	O
variables	O
we	O
can	O
do	O
this	O
by	O
replacing	O
the	O
spike-and-slab	O
style	O
prior	O
that	O
assigns	O
finite	O
probability	O
mass	O
to	O
the	O
event	O
that	O
wj	O
to	O
continuous	O
priors	O
that	O
encourage	O
wj	O
by	O
putting	O
a	O
lot	O
of	O
probability	O
density	O
near	O
the	O
origin	O
such	O
as	O
a	O
zero-mean	O
laplace	B
distribution	I
this	O
was	O
first	O
introduced	O
in	O
section	O
in	O
the	O
context	O
of	O
robust	B
linear	B
regression	B
there	O
we	O
exploited	O
the	O
fact	O
that	O
the	O
laplace	B
has	O
heavy	B
tails	I
here	O
we	O
exploit	O
the	O
fact	O
chapter	O
sparse	B
linear	O
models	O
figure	O
illustration	O
of	O
vs	O
regularization	B
of	O
a	O
least	B
squares	I
problem	O
based	O
on	O
figure	O
of	O
et	O
al	O
that	O
it	O
has	O
a	O
spike	O
near	O
more	O
precisely	O
consider	O
a	O
prior	O
of	O
the	O
form	O
pw	O
e	O
we	O
will	O
use	O
a	O
uniform	O
prior	O
on	O
the	O
offset	O
term	O
let	O
us	O
perform	O
map	O
estimation	O
with	O
this	O
prior	O
the	O
penalized	O
negative	B
log	I
likelihood	B
has	O
the	O
form	O
f	O
log	O
pdw	O
log	O
pw	O
nllw	O
is	O
the	O
norm	O
of	O
w	O
for	O
suitably	O
large	O
the	O
estimate	O
w	O
will	O
be	O
where	O
sparse	B
for	O
reasons	O
we	O
explain	O
below	O
indeed	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
convex	B
approximation	O
to	O
the	O
non-convex	O
objective	O
nllw	O
argmin	O
w	O
in	O
the	O
case	O
of	O
linear	B
regression	B
the	O
objective	O
becomes	O
f	O
wt	O
rssw	O
where	O
this	O
method	O
is	O
known	O
as	O
basis	B
pursuit	I
denoising	I
or	O
bpdn	B
et	O
al	O
the	O
reason	O
for	O
this	O
term	O
will	O
become	O
clear	O
later	O
in	O
general	O
the	O
technique	O
of	O
putting	O
a	O
zero-mean	O
laplace	B
prior	O
on	O
the	O
parameters	O
and	O
performing	O
map	O
estimation	O
is	O
called	O
regularization	B
it	O
can	O
be	O
combined	O
with	O
any	O
convex	B
or	O
non-convex	O
nll	B
term	O
many	O
different	O
algorithms	O
have	O
been	O
devised	O
for	O
solving	O
such	O
problems	O
some	O
of	O
which	O
we	O
review	O
in	O
section	O
why	O
does	O
regularization	B
yield	O
sparse	B
solutions	O
we	O
now	O
explain	O
why	O
regularization	B
results	O
in	O
sparse	B
solutions	O
whereas	O
regularization	B
does	O
not	O
we	O
focus	O
on	O
the	O
case	O
of	O
linear	B
regression	B
although	O
similar	B
arguments	O
hold	O
for	O
logistic	B
regression	B
and	O
other	O
glms	O
regularization	B
basics	O
the	O
objective	O
is	O
the	O
following	O
non-smooth	B
objective	O
function	O
rssw	O
min	O
w	O
we	O
can	O
rewrite	O
this	O
as	O
a	O
constrained	O
but	O
smooth	O
objective	O
quadratic	O
function	O
with	O
linear	O
constraints	O
min	O
w	O
rssw	O
s	O
t	O
b	O
where	O
b	O
is	O
an	O
upper	O
bound	O
on	O
the	O
of	O
the	O
weights	O
a	O
small	O
bound	O
b	O
corresponds	O
to	O
a	O
large	O
penalty	O
and	O
vice	O
equation	O
is	O
known	O
as	O
lasso	B
which	O
stands	O
for	O
least	O
absolute	O
shrinkage	B
and	O
selection	O
operator	O
we	O
will	O
see	O
why	O
it	O
has	O
this	O
name	O
later	O
similarly	O
we	O
can	O
write	O
ridge	B
regression	B
rssw	O
min	O
w	O
or	O
as	O
a	O
bound	O
constrained	O
form	O
rssw	O
min	O
s	O
t	O
b	O
w	O
in	O
figure	O
we	O
plot	O
the	O
contours	O
of	O
the	O
rss	O
objective	O
function	O
as	O
well	O
as	O
the	O
contours	O
of	O
the	O
and	O
constraint	O
surfaces	O
from	O
the	O
theory	O
of	O
constrained	O
optimization	B
we	O
know	O
that	O
the	O
optimal	O
solution	O
occurs	O
at	O
the	O
point	O
where	O
the	O
lowest	O
level	O
set	O
of	O
the	O
objective	O
function	O
intersects	O
the	O
constraint	O
surface	O
the	O
constraint	O
is	O
active	O
it	O
should	O
be	O
geometrically	O
clear	O
that	O
as	O
we	O
relax	O
the	O
constraint	O
b	O
we	O
grow	O
ball	O
until	O
it	O
meets	O
the	O
objective	O
the	O
corners	O
of	O
the	O
ball	O
are	O
more	O
likely	O
to	O
intersect	O
the	O
ellipse	O
than	O
one	O
of	O
the	O
sides	O
especially	O
in	O
high	O
dimensions	O
because	O
the	O
corners	O
stick	O
out	O
more	O
the	O
corners	O
correspond	O
to	O
sparse	B
solutions	O
which	O
lie	O
on	O
the	O
coordinate	O
axes	O
by	O
contrast	O
when	O
we	O
grow	O
the	O
ball	O
it	O
can	O
intersect	O
the	O
objective	O
at	O
any	O
point	O
there	O
are	O
no	O
corners	O
so	O
there	O
is	O
no	O
preference	O
for	O
sparsity	B
to	O
see	O
this	O
another	O
away	O
notice	O
that	O
with	O
ridge	B
regression	B
the	O
prior	O
cost	O
of	O
a	O
sparse	B
solution	O
such	O
as	O
w	O
is	O
the	O
same	O
as	O
the	O
cost	O
of	O
a	O
dense	O
solution	O
such	O
as	O
w	O
as	O
long	O
as	O
they	O
have	O
the	O
same	O
norm	O
since	O
however	O
for	O
lasso	B
setting	O
w	O
is	O
cheaper	O
than	O
setting	O
w	O
the	O
most	O
rigorous	O
way	O
to	O
see	O
that	O
regularization	B
results	O
in	O
sparse	B
solutions	O
is	O
to	O
examine	O
conditions	O
that	O
hold	O
at	O
the	O
optimum	O
we	O
do	O
this	O
in	O
section	O
optimality	O
conditions	O
for	O
lasso	B
the	O
lasso	B
objective	O
has	O
the	O
form	O
f	O
rss	O
equation	O
is	O
an	O
example	O
of	O
a	O
quadratic	B
program	I
or	O
qp	B
since	O
we	O
have	O
a	O
quadratic	O
objective	O
subject	O
to	O
linear	O
inequality	O
constraints	O
its	O
lagrangian	B
is	O
given	O
by	O
equation	O
chapter	O
sparse	B
linear	O
models	O
c	O
c	O
fx	O
cx	O
x	O
figure	O
figure	O
generated	O
by	O
subgradientplot	O
illustration	O
of	O
some	O
sub-derivatives	O
of	O
a	O
function	O
at	O
point	O
based	O
on	O
a	O
figure	O
at	O
http	O
unfortunately	O
the	O
term	O
is	O
not	O
differentiable	O
whenever	O
wj	O
this	O
is	O
an	O
example	O
of	O
a	O
non-smooth	B
optimization	B
problem	O
to	O
handle	O
non-smooth	B
functions	O
we	O
need	O
to	O
extend	O
the	O
notion	O
of	O
a	O
derivative	O
we	O
define	O
a	O
subderivative	B
or	O
subgradient	B
of	O
a	O
function	O
f	O
i	O
r	O
at	O
a	O
point	O
to	O
be	O
a	O
scalar	O
g	O
such	O
that	O
f	O
f	O
g	O
i	O
where	O
i	O
is	O
some	O
interval	O
containing	O
see	O
figure	O
for	O
an	O
we	O
define	O
the	O
set	O
of	O
subderivatives	O
as	O
the	O
interval	O
b	O
where	O
a	O
and	O
b	O
are	O
the	O
one-sided	O
limits	O
f	O
f	O
a	O
lim	O
b	O
lim	O
f	O
f	O
the	O
set	O
b	O
of	O
all	O
subderivatives	O
is	O
called	O
the	O
subdifferential	B
of	O
the	O
function	O
f	O
at	O
and	O
is	O
denoted	O
f	O
for	O
example	O
in	O
the	O
case	O
of	O
the	O
absolute	O
value	O
function	O
f	O
the	O
subderivative	B
is	O
given	O
by	O
if	O
if	O
if	O
f	O
if	O
the	O
function	O
is	O
everywhere	O
differentiable	O
then	O
f	O
df	O
calculus	O
result	O
one	O
can	O
show	O
that	O
the	O
point	O
is	O
a	O
local	O
minimum	O
of	O
f	O
iff	B
f	O
d	O
by	O
analogy	O
to	O
the	O
standard	O
in	O
general	O
for	O
a	O
vector	O
valued	O
function	O
we	O
say	O
that	O
g	O
is	O
a	O
subgradient	B
of	O
f	O
at	O
if	O
for	O
all	O
vectors	O
f	O
f	O
g	O
so	O
g	O
is	O
a	O
linear	O
lower	O
bound	O
to	O
the	O
function	O
at	O
regularization	B
basics	O
figure	O
left	O
soft	B
thresholding	I
the	O
flat	O
region	O
is	O
the	O
interval	O
right	O
hard	B
thresholding	I
let	O
us	O
apply	O
these	O
concepts	O
to	O
the	O
lasso	B
problem	O
let	O
us	O
initially	O
ignore	O
the	O
non-smooth	B
penalty	O
term	O
one	O
can	O
show	O
that	O
rssw	O
jwj	O
cj	O
wj	O
aj	O
cj	O
ij	O
xijyi	O
wt	O
jxi	O
j	O
where	O
w	O
j	O
is	O
w	O
without	O
component	O
j	O
and	O
similarly	O
for	O
xi	O
j	O
we	O
see	O
that	O
cj	O
is	O
to	O
the	O
correlation	O
between	O
the	O
j	O
th	O
feature	O
xj	O
and	O
the	O
residual	B
due	O
to	O
the	O
other	O
features	B
r	O
j	O
y	O
x	O
jw	O
j	O
hence	O
the	O
magnitude	O
of	O
cj	O
is	O
an	O
indication	O
of	O
how	O
relevant	O
feature	O
j	O
is	O
for	O
predicting	O
y	O
to	O
the	O
other	O
features	B
and	O
the	O
current	O
parameters	O
adding	O
in	O
the	O
penalty	O
term	O
we	O
find	O
that	O
the	O
subderivative	B
is	O
given	O
by	O
wj	O
f	O
cj	O
wj	O
cj	O
cj	O
cj	O
cj	O
if	O
wj	O
if	O
wj	O
if	O
wj	O
we	O
can	O
write	O
this	O
in	O
a	O
more	O
compact	O
fashion	O
as	O
follows	O
xt	O
yj	O
if	O
wj	O
if	O
wj	O
if	O
wj	O
depending	O
on	O
the	O
value	O
of	O
cj	O
the	O
solution	O
to	O
wj	O
of	O
wj	O
as	O
follows	O
f	O
can	O
occur	O
at	O
different	O
values	O
chapter	O
sparse	B
linear	O
models	O
if	O
cj	O
so	O
the	O
feature	O
is	O
strongly	O
negatively	O
correlated	O
with	O
the	O
residual	B
then	O
the	O
subgradient	B
is	O
zero	O
at	O
wj	O
cj	O
aj	O
if	O
cj	O
so	O
the	O
feature	O
is	O
only	O
weakly	O
correlated	O
with	O
the	O
residual	B
then	O
the	O
subgradient	B
is	O
zero	O
at	O
wj	O
if	O
cj	O
so	O
the	O
feature	O
is	O
strongly	O
positively	O
correlated	O
with	O
the	O
residual	B
then	O
the	O
subgra	O
dient	O
is	O
zero	O
at	O
wj	O
cj	O
aj	O
in	O
summary	O
we	O
have	O
we	O
can	O
write	O
this	O
as	O
follows	O
wjcj	O
if	O
cj	O
if	O
cj	O
if	O
cj	O
wj	O
soft	O
cj	O
aj	O
aj	O
where	O
soft	O
signa	O
and	O
x	O
maxx	O
is	O
the	O
positive	O
part	O
of	O
x	O
this	O
is	O
called	O
soft	B
thresholding	I
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
plot	O
wj	O
vs	O
cj	O
the	O
dotted	O
line	O
is	O
the	O
line	O
wj	O
cjaj	O
corresponding	O
to	O
the	O
least	B
squares	I
fit	O
the	O
solid	O
line	O
which	O
represents	O
the	O
regularized	O
estimate	O
wjcj	O
shifts	O
the	O
dotted	O
line	O
down	O
up	O
by	O
except	O
when	O
cj	O
in	O
which	O
case	O
it	O
sets	O
wj	O
by	O
contrast	O
in	O
figure	O
we	O
illustrate	O
hard	B
thresholding	I
this	O
sets	O
values	O
of	O
wj	O
to	O
if	O
cj	O
but	O
it	O
does	O
not	O
shrink	O
the	O
values	O
of	O
wj	O
outside	O
of	O
this	O
interval	O
the	O
slope	O
of	O
the	O
soft	B
thresholding	I
line	O
does	O
not	O
coincide	O
with	O
the	O
diagonal	B
which	O
means	O
that	O
even	O
large	O
coefficients	O
are	O
shrunk	O
towards	O
zero	O
consequently	O
lasso	B
is	O
a	O
biased	O
estimator	B
this	O
is	O
undesirable	O
since	O
if	O
the	O
likelihood	B
indicates	O
cj	O
that	O
the	O
coefficient	O
wj	O
should	O
be	O
large	O
we	O
do	O
not	O
want	O
to	O
shrink	O
it	O
we	O
will	O
discuss	O
this	O
issue	O
in	O
more	O
detail	O
in	O
section	O
now	O
we	O
finally	O
can	O
understand	O
why	O
tibshirani	O
invented	O
the	O
term	O
lasso	B
in	O
it	O
stands	O
for	O
least	O
absolute	O
selection	O
and	O
shrinkage	B
operator	O
since	O
it	O
selects	O
a	O
subset	O
of	O
the	O
variables	O
and	O
shrinks	O
all	O
the	O
coefficients	O
by	O
penalizing	O
the	O
absolute	O
values	O
if	O
we	O
get	O
the	O
ols	B
solution	O
minimal	B
norm	O
if	O
max	O
we	O
get	O
w	O
where	O
max	O
y	O
max	O
xj	O
this	O
value	O
is	O
computed	O
using	O
the	O
fact	O
that	O
is	O
optimal	O
if	O
yj	O
for	O
all	O
j	O
in	O
general	O
the	O
maximum	O
penalty	O
for	O
an	O
regularized	O
objective	O
is	O
j	O
max	O
max	O
j	O
jn	O
regularization	B
basics	O
comparison	O
of	O
least	B
squares	I
lasso	B
ridge	O
and	O
subset	O
selection	O
we	O
can	O
gain	O
further	O
insight	O
into	O
regularization	B
by	O
comparing	O
it	O
to	O
least	B
squares	I
and	O
and	O
regularized	O
least	B
squares	I
for	O
simplicity	O
assume	O
all	O
the	O
features	B
of	O
x	O
are	O
orthonormal	O
so	O
xt	O
x	O
i	O
in	O
this	O
case	O
the	O
rss	O
is	O
given	O
by	O
rssw	O
yt	O
y	O
wt	O
xt	O
xw	O
xt	O
y	O
const	O
k	O
wkxikyi	O
k	O
k	O
i	O
so	O
we	O
see	O
this	O
factorizes	O
into	O
a	O
sum	O
of	O
terms	O
one	O
per	O
dimension	O
hence	O
we	O
can	O
write	O
down	O
the	O
map	O
and	O
ml	O
estimates	O
analytically	O
as	O
follows	O
mle	B
the	O
ols	B
solution	O
is	O
given	O
by	O
wols	O
xt	O
k	O
where	O
xk	O
is	O
the	O
k	O
th	O
column	O
of	O
x	O
this	O
follows	O
trivially	O
from	O
equation	O
we	O
see	O
that	O
wols	O
is	O
just	O
the	O
orthogonal	B
projection	B
of	O
feature	O
k	O
onto	O
the	O
response	O
vector	O
section	O
k	O
ridge	O
one	O
can	O
show	O
that	O
the	O
ridge	O
estimate	O
is	O
given	O
by	O
lasso	B
from	O
equation	O
and	O
using	O
the	O
fact	O
that	O
ak	O
and	O
wols	O
k	O
wridge	O
k	O
k	O
wols	O
wlasso	O
k	O
sign	O
wols	O
k	O
wols	O
k	O
we	O
have	O
this	O
corresponds	O
to	O
soft	B
thresholding	I
shown	O
in	O
figure	O
subset	O
selection	O
if	O
we	O
pick	O
the	O
best	O
k	O
features	B
using	O
subset	O
selection	O
the	O
parameter	B
estimate	O
is	O
as	O
follows	O
wss	O
k	O
wols	O
k	O
if	O
rankwols	O
otherwise	O
k	O
k	O
where	O
rank	O
refers	O
to	O
the	O
location	O
in	O
the	O
sorted	O
list	O
of	O
weight	O
magnitudes	O
this	O
corresponds	O
to	O
hard	B
thresholding	I
shown	O
in	O
figure	O
figure	O
plots	O
the	O
mse	B
vs	O
for	O
lasso	B
for	O
a	O
degree	B
polynomial	O
and	O
figure	O
plots	O
the	O
mse	B
vs	O
polynomial	O
order	O
we	O
see	O
that	O
lasso	B
gives	O
similar	B
results	O
to	O
the	O
subset	O
selection	O
method	O
as	O
another	O
example	O
consider	O
a	O
data	O
set	O
concerning	O
prostate	O
cancer	O
we	O
have	O
d	O
features	B
and	O
n	O
training	O
cases	O
the	O
goal	O
is	O
to	O
predict	O
the	O
log	O
prostate-specific	O
antigen	O
levels	O
et	O
al	O
for	O
more	O
biological	O
details	O
table	O
shows	O
that	O
lasso	B
gives	O
better	O
prediction	O
accuracy	O
least	O
on	O
this	O
particular	O
data	O
set	O
than	O
least	B
squares	I
ridge	O
and	O
best	O
subset	O
regression	B
each	O
case	O
the	O
strength	O
of	O
the	O
regularizer	O
was	O
chosen	O
by	O
cross	B
validation	I
lasso	B
also	O
gives	O
rise	O
to	O
a	O
sparse	B
solution	O
of	O
course	O
for	O
other	O
problems	O
ridge	O
may	O
give	O
better	O
predictive	B
accuracy	O
in	O
practice	O
a	O
combination	O
of	O
lasso	B
and	O
ridge	O
known	O
as	O
the	O
elastic	B
net	I
often	O
performs	O
best	O
since	O
it	O
provides	O
a	O
good	O
combination	O
of	O
sparsity	B
and	O
regularization	B
section	O
chapter	O
sparse	B
linear	O
models	O
train	O
test	O
e	O
s	O
m	O
lambda	O
e	O
s	O
m	O
performance	O
of	O
mle	B
train	O
test	O
degree	B
figure	O
mse	B
vs	O
for	O
lasso	B
for	O
a	O
degree	B
polynomial	O
note	O
that	O
decreases	O
as	O
we	O
move	O
to	O
the	O
right	O
figure	O
generated	O
by	O
linregpolylassodemo	O
mse	B
versus	O
polynomial	O
degree	B
note	O
that	O
the	O
model	O
order	O
increases	O
as	O
we	O
move	O
to	O
the	O
right	O
see	O
figure	O
for	O
a	O
plot	O
of	O
some	O
of	O
these	O
polynomial	B
regression	B
models	O
figure	O
generated	O
by	O
linregpolyvsdegree	O
term	O
intercept	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
test	O
error	O
ls	O
best	O
subset	O
ridge	O
lasso	B
table	O
results	O
of	O
different	O
methods	O
on	O
the	O
prostate	O
cancer	O
data	O
which	O
has	O
features	B
and	O
training	O
cases	O
methods	O
are	O
ls	O
least	B
squares	I
subset	O
best	O
subset	O
regression	B
ridge	O
lasso	B
rows	O
represent	O
the	O
coefficients	O
we	O
see	O
that	O
subset	O
regression	B
and	O
lasso	B
give	O
sparse	B
solutions	O
bottom	O
row	O
is	O
the	O
mean	B
squared	B
error	I
on	O
the	O
test	O
set	O
cases	O
based	O
on	O
table	O
of	O
et	O
al	O
figure	O
generated	O
by	O
prostatecomparison	O
regularization	B
path	B
as	O
we	O
increase	O
the	O
solution	O
vector	O
w	O
will	O
tend	O
to	O
get	O
sparser	O
although	O
not	O
necessarily	O
monotonically	O
we	O
can	O
plot	O
the	O
values	O
wj	O
vs	O
for	O
each	O
feature	O
j	O
this	O
is	O
known	O
as	O
the	O
regularization	B
path	B
this	O
is	O
illustrated	O
for	O
ridge	B
regression	B
in	O
figure	O
where	O
we	O
plot	O
wj	O
as	O
the	O
regularizer	O
decreases	O
we	O
see	O
that	O
when	O
all	O
the	O
coefficients	O
are	O
zero	O
but	O
for	O
any	O
finite	O
value	O
of	O
all	O
coefficients	O
are	O
non-zero	O
furthermore	O
they	O
increase	O
in	O
magnitude	O
as	O
is	O
decreased	O
in	O
figure	O
we	O
plot	O
the	O
analogous	O
result	O
for	O
lasso	B
as	O
we	O
move	O
to	O
the	O
right	O
the	O
upper	O
bound	O
on	O
the	O
penalty	O
b	O
increases	O
when	O
b	O
all	O
the	O
coefficients	O
are	O
zero	O
as	O
we	O
increase	O
regularization	B
basics	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
profiles	O
of	O
ridge	O
coefficients	O
for	O
the	O
prostate	O
cancer	O
example	O
vs	O
bound	O
on	O
norm	O
of	O
w	O
figure	O
so	O
small	O
t	O
is	O
on	O
the	O
left	O
the	O
vertical	O
line	O
is	O
the	O
value	O
chosen	O
by	O
cv	B
using	O
the	O
rule	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
ridgepathprostate	O
profiles	O
of	O
lasso	B
coefficients	O
for	O
the	O
prostate	O
cancer	O
example	O
vs	O
bound	O
on	O
norm	O
of	O
w	O
so	O
small	O
t	O
is	O
on	O
the	O
left	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
lassopathprostate	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
lars	B
step	O
figure	O
illustration	O
of	O
piecewise	O
linearity	O
of	O
regularization	B
path	B
for	O
lasso	B
on	O
the	O
prostate	O
cancer	O
example	O
we	O
plot	O
wjb	O
vs	O
b	O
for	O
the	O
critical	O
values	O
of	O
b	O
we	O
plot	O
vs	O
steps	O
of	O
the	O
lars	B
algorithm	O
figure	O
generated	O
by	O
lassopathprostate	O
b	O
the	O
coefficients	O
gradually	O
turn	O
on	O
but	O
for	O
any	O
value	O
between	O
and	O
bmax	O
the	O
solution	O
is	O
remarkably	O
it	O
can	O
be	O
shown	O
that	O
the	O
solution	O
path	B
is	O
a	O
piecewise	O
linear	O
function	O
of	O
b	O
et	O
al	O
that	O
is	O
there	O
are	O
a	O
set	O
of	O
critical	O
values	O
of	O
b	O
where	O
the	O
active	B
set	I
of	O
non-zero	O
coefficients	O
changes	O
for	O
values	O
of	O
b	O
between	O
these	O
critical	O
values	O
each	O
non-zero	O
coefficient	O
increases	O
or	O
decreases	O
in	O
a	O
linear	O
fashion	O
this	O
is	O
illustrated	O
in	O
figure	O
furthermore	O
one	O
can	O
solve	O
for	O
these	O
critical	O
values	O
analytically	O
this	O
is	O
the	O
basis	O
of	O
the	O
lars	B
algorithm	O
et	O
al	O
which	O
stands	O
for	O
least	O
angle	O
regression	B
and	O
shrinkage	B
section	O
for	O
details	O
remarkably	O
lars	B
can	O
compute	O
the	O
entire	O
regularization	B
path	B
for	O
roughly	O
the	O
same	O
it	O
is	O
common	O
to	O
plot	O
the	O
solution	O
versus	O
the	O
shrinkage	B
factor	B
defined	O
as	O
sb	O
max	O
rather	O
than	O
against	O
b	O
this	O
merely	O
affects	O
the	O
scale	O
of	O
the	O
horizontal	O
axis	O
not	O
the	O
shape	O
of	O
the	O
curves	O
chapter	O
sparse	B
linear	O
models	O
original	O
number	O
of	O
nonzeros	O
reconstruction	O
lambda	O
mse	B
debiased	O
minimum	O
norm	O
solution	O
figure	O
example	O
of	O
recovering	O
a	O
sparse	B
signal	O
using	O
lasso	B
see	O
text	O
for	O
details	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
sparsesensingdemo	O
written	O
by	O
mario	O
figueiredo	O
computational	O
cost	O
as	O
a	O
single	O
least	B
squares	I
fit	O
ominn	O
dn	O
in	O
figure	O
we	O
plot	O
the	O
coefficients	O
computed	O
at	O
each	O
critical	B
value	I
of	O
b	O
now	O
the	O
piecewise	O
linearity	O
is	O
more	O
evident	O
below	O
we	O
display	O
the	O
actual	O
coefficient	O
values	O
at	O
each	O
step	O
along	O
the	O
regularization	B
path	B
last	O
line	O
is	O
the	O
least	B
squares	I
solution	O
listing	O
output	O
of	O
lassopathprostate	O
by	O
changing	O
b	O
from	O
to	O
bmax	O
we	O
can	O
go	O
from	O
a	O
solution	O
in	O
which	O
all	O
the	O
weights	O
are	O
zero	O
to	O
a	O
solution	O
in	O
which	O
all	O
weights	O
are	O
non-zero	O
unfortunately	O
not	O
all	O
subset	O
sizes	O
are	O
achievable	O
using	O
lasso	B
one	O
can	O
show	O
that	O
if	O
d	O
n	O
the	O
optimal	O
solution	O
can	O
have	O
at	O
most	O
n	O
variables	O
in	O
it	O
before	O
reaching	O
the	O
complete	B
set	O
corresponding	O
to	O
the	O
ols	B
solution	O
of	O
minimal	B
norm	O
in	O
section	O
we	O
will	O
see	O
that	O
by	O
using	O
an	O
regularizer	O
as	O
well	O
as	O
an	O
regularizer	O
method	O
known	O
as	O
the	O
elastic	B
net	I
we	O
can	O
achieve	O
sparse	B
solutions	O
which	O
contain	O
more	O
variables	O
than	O
training	O
cases	O
this	O
lets	O
us	O
explore	O
model	O
sizes	O
between	O
n	O
and	O
d	O
regularization	B
basics	O
model	B
selection	I
it	O
is	O
tempting	O
to	O
use	O
regularization	B
to	O
estimate	O
the	O
set	O
of	O
relevant	O
variables	O
in	O
some	O
cases	O
we	O
can	O
recover	O
the	O
true	O
sparsity	B
pattern	B
of	O
w	O
the	O
parameter	B
vector	O
that	O
generated	O
the	O
data	O
a	O
method	O
that	O
can	O
recover	O
the	O
true	O
model	O
in	O
the	O
n	O
limit	O
is	O
called	O
model	B
selection	I
consistent	B
the	O
details	O
on	O
which	O
methods	O
enjoy	O
this	O
property	O
and	O
when	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
see	O
e	O
g	O
and	O
van	O
de	O
geer	O
for	O
details	O
instead	O
of	O
going	O
into	O
a	O
theoretical	O
discussion	O
we	O
will	O
just	O
show	O
a	O
small	O
example	O
we	O
first	O
of	O
size	O
d	O
consisting	O
of	O
randomly	O
placed	O
spikes	O
generate	O
a	O
sparse	B
signal	O
w	O
next	O
we	O
generate	O
a	O
random	O
design	B
matrix	I
x	O
of	O
size	O
n	O
d	O
where	O
n	O
finally	O
we	O
where	O
n	O
we	O
then	O
estimate	O
w	O
from	O
generate	O
a	O
noisy	O
observation	B
y	O
xw	O
y	O
and	O
x	O
is	O
shown	O
in	O
the	O
first	O
row	O
of	O
figure	O
the	O
second	O
row	O
is	O
the	O
estimate	O
using	O
max	O
we	O
see	O
that	O
this	O
has	O
spikes	O
in	O
the	O
right	O
places	O
but	O
they	O
are	O
too	O
small	O
the	O
third	O
row	O
is	O
the	O
least	B
squares	I
estimate	O
of	O
the	O
coefficients	O
which	O
are	O
estimated	O
to	O
be	O
non-zero	O
based	O
on	O
supp	O
this	O
is	O
called	O
debiasing	B
and	O
is	O
necessary	O
because	O
lasso	B
shrinks	O
the	O
relevant	O
coefficients	O
as	O
well	O
as	O
the	O
irrelevant	O
ones	O
the	O
last	O
row	O
is	O
the	O
least	B
squares	I
estimate	O
for	O
all	O
the	O
coefficients	O
jointly	O
ignoring	O
sparsity	B
we	O
see	O
that	O
the	O
sparse	B
estimate	O
is	O
an	O
excellent	O
estimate	O
of	O
the	O
original	O
signal	O
by	O
contrast	O
least	B
squares	I
without	O
the	O
sparsity	B
assumption	O
performs	O
very	O
poorly	O
the	O
original	O
w	O
of	O
course	O
to	O
perform	O
model	B
selection	I
we	O
have	O
to	O
pick	O
it	O
is	O
common	O
to	O
use	O
cross	B
validation	I
however	O
it	O
is	O
important	O
to	O
note	O
that	O
cross	B
validation	I
is	O
picking	O
a	O
value	O
of	O
that	O
results	O
in	O
good	O
predictive	B
accuracy	O
this	O
is	O
not	O
usually	O
the	O
same	O
value	O
as	O
the	O
one	O
that	O
is	O
likely	O
to	O
recover	O
the	O
true	O
model	O
to	O
see	O
why	O
recall	B
that	O
regularization	B
performs	O
selection	O
and	O
shrinkage	B
that	O
is	O
the	O
chosen	O
coefficients	O
are	O
brought	O
closer	O
to	O
in	O
order	O
to	O
prevent	O
relevant	O
coefficients	O
from	O
being	O
shrunk	O
in	O
this	O
way	O
cross	B
validation	I
will	O
tend	O
to	O
pick	O
a	O
value	O
of	O
that	O
is	O
not	O
too	O
large	O
of	O
course	O
this	O
will	O
result	O
in	O
a	O
less	O
sparse	B
model	O
which	O
contains	O
irrelevant	O
variables	O
positives	O
indeed	O
it	O
was	O
proved	O
in	O
and	O
buhlmann	O
that	O
the	O
prediction-optimal	O
value	O
of	O
does	O
not	O
result	O
in	O
model	B
selection	I
consistency	O
in	O
section	O
we	O
will	O
discuss	O
some	O
adaptive	O
mechanisms	O
for	O
automatically	O
tuning	O
on	O
a	O
per-dimension	O
basis	O
that	O
does	O
result	O
in	O
model	B
selection	I
consistency	O
a	O
downside	O
of	O
using	O
regularization	B
to	O
select	O
variables	O
is	O
that	O
it	O
can	O
give	O
quite	O
different	O
results	O
if	O
the	O
data	O
is	O
perturbed	O
slightly	O
the	O
bayesian	B
approach	O
which	O
estimates	O
posterior	O
marginal	O
inclusion	B
probabilities	I
p	O
j	O
is	O
much	O
more	O
robust	B
a	O
frequentist	B
solution	O
to	O
this	O
is	O
to	O
use	O
bootstrap	B
resampling	I
section	O
and	O
to	O
rerun	O
the	O
estimator	B
on	O
different	O
versions	O
of	O
the	O
data	O
by	O
computing	O
how	O
often	O
each	O
variable	O
is	O
selected	O
across	O
different	O
trials	O
we	O
can	O
approximate	O
the	O
posterior	O
inclusion	B
probabilities	I
this	O
method	O
is	O
known	O
as	O
stability	B
selection	I
and	O
b	O
ijhlmann	O
we	O
can	O
threshold	O
the	O
stability	B
selection	I
inclusion	B
probabilities	I
at	O
some	O
level	O
say	O
and	O
thus	O
derive	O
a	O
sparse	B
estimator	B
this	O
is	O
known	O
as	O
bootstrap	B
lasso	B
or	O
bolasso	B
it	O
will	O
include	O
a	O
variable	O
if	O
it	O
occurs	O
in	O
at	O
least	O
of	O
sets	O
returned	O
by	O
lasso	B
a	O
fixed	O
this	O
process	O
of	O
intersecting	O
the	O
sets	O
is	O
a	O
way	O
of	O
eliminating	O
the	O
false	O
positives	O
that	O
vanilla	O
lasso	B
produces	O
the	O
theoretical	O
results	O
in	O
prove	O
that	O
bolasso	B
is	O
model	B
selection	I
consistent	B
under	O
a	O
wider	O
range	O
of	O
conditions	O
than	O
vanilla	O
lasso	B
as	O
an	O
illustration	O
we	O
reproduced	O
the	O
experiments	O
in	O
in	O
particular	O
we	O
created	O
chapter	O
sparse	B
linear	O
models	O
x	O
e	O
d	O
n	O
i	O
l	O
e	O
b	O
a	O
i	O
r	O
a	O
v	O
lasso	B
on	O
sign	O
inconsistent	O
data	O
log	O
x	O
e	O
d	O
n	O
i	O
l	O
e	O
b	O
a	O
i	O
r	O
a	O
v	O
bolasso	B
on	O
sign	O
inconsistent	O
data	O
bootstraps	O
lasso	B
vs	O
bolasso	B
on	O
sign	O
inconsistent	O
data	O
nbootstraps	O
t	O
r	O
o	O
p	O
p	O
u	O
s	O
t	O
c	O
e	O
r	O
r	O
o	O
c	O
p	O
lasso	B
bolasso	B
log	O
log	O
figure	O
probability	O
of	O
selection	O
of	O
each	O
variable	O
large	O
probabilities	O
black	O
small	O
probabilities	O
vs	O
regularization	B
parameter	B
for	O
lasso	B
as	O
we	O
move	O
from	O
left	O
to	O
right	O
we	O
decrease	O
the	O
amount	O
of	O
regularization	B
and	O
therefore	O
select	O
more	O
variables	O
same	O
as	O
but	O
for	O
bolasso	B
probability	O
of	O
correct	O
sign	O
estimation	O
vs	O
regularization	B
parameter	B
bolasso	B
dashed	O
and	O
lasso	B
plain	O
the	O
number	O
of	O
bootstrap	B
replications	O
is	O
in	O
based	O
on	O
figures	O
of	O
figure	O
generated	O
by	O
bolassodemo	O
datasets	O
of	O
size	O
n	O
with	O
d	O
variables	O
of	O
which	O
are	O
relevant	O
see	O
for	O
more	O
detail	O
on	O
the	O
experimental	O
setup	O
for	O
dataset	O
n	O
variable	O
j	O
and	O
sparsity	B
level	O
k	O
define	O
sj	O
k	O
n	O
i	O
wj	O
kdn	O
now	O
definep	O
k	O
be	O
the	O
average	O
of	O
sj	O
k	O
n	O
over	O
the	O
in	O
figure	O
we	O
plot	O
p	O
vs	O
log	O
for	O
lasso	B
and	O
bolasso	B
we	O
see	O
that	O
for	O
datasets	O
bolasso	B
there	O
is	O
a	O
large	O
range	O
of	O
where	O
the	O
true	O
variables	O
are	O
selected	O
but	O
this	O
is	O
not	O
the	O
case	O
for	O
lasso	B
this	O
is	O
emphasized	O
in	O
figure	O
where	O
we	O
plot	O
the	O
empirical	O
probability	O
that	O
the	O
correct	O
set	O
of	O
variables	O
is	O
recovered	O
for	O
lasso	B
and	O
for	O
bolasso	B
with	O
an	O
increasing	O
number	O
of	O
bootstrap	B
samples	B
of	O
course	O
using	O
more	O
samples	B
takes	O
longer	O
in	O
practice	O
bootstraps	O
seems	O
to	O
be	O
a	O
good	O
compromise	O
between	O
speed	O
and	O
accuracy	O
with	O
bolasso	B
there	O
is	O
the	O
usual	O
issue	O
of	O
picking	O
obviously	O
we	O
could	O
use	O
cross	B
validation	I
but	O
plots	O
such	O
as	O
figure	O
suggest	O
another	O
heuristic	O
shuffle	O
the	O
rows	O
to	O
create	O
a	O
large	O
black	O
block	O
and	O
then	O
pick	O
to	O
be	O
in	O
the	O
middle	O
of	O
this	O
region	O
of	O
course	O
operationalizing	O
this	O
intuition	O
may	O
be	O
tricky	O
and	O
will	O
require	O
various	O
ad-hoc	O
thresholds	O
is	O
reminiscent	O
of	O
the	O
find	O
the	O
knee	B
in	O
the	O
curve	O
heuristic	O
discussed	O
in	O
section	O
when	O
discussing	O
how	O
to	O
pick	O
k	O
for	O
mixture	B
models	O
a	O
bayesian	B
approach	O
provides	O
a	O
more	O
principled	O
method	O
for	O
selecting	O
bayesian	B
inference	B
for	O
linear	O
models	O
with	O
laplace	B
priors	O
we	O
have	O
been	O
focusing	O
on	O
map	O
estimation	O
in	O
sparse	B
linear	O
models	O
it	O
is	O
also	O
possible	O
to	O
perform	O
bayesian	B
inference	B
e	O
g	O
and	O
casella	O
seeger	O
however	O
the	O
posterior	B
mean	B
and	O
median	B
as	O
well	O
as	O
samples	B
from	O
the	O
posterior	O
are	O
not	O
sparse	B
only	O
the	O
mode	B
is	O
sparse	B
this	O
is	O
another	O
example	O
of	O
the	O
phenomenon	O
discussed	O
in	O
section	O
where	O
we	O
said	O
that	O
the	O
map	B
estimate	I
is	O
often	O
untypical	O
of	O
the	O
bulk	O
of	O
the	O
posterior	O
another	O
argument	O
in	O
favor	O
of	O
using	O
the	O
posterior	B
mean	B
comes	O
from	O
equation	O
which	O
showed	O
that	O
that	O
plugging	O
in	O
the	O
posterior	B
mean	B
rather	O
than	O
the	O
posterior	B
mode	B
is	O
the	O
optimal	O
thing	O
to	O
do	O
if	O
we	O
want	O
to	O
minimize	O
squared	O
prediction	O
error	O
et	O
al	O
shows	O
experimentally	O
and	O
and	O
yavnch	O
shows	O
theoretically	O
that	O
using	O
the	O
posterior	B
mean	B
with	O
a	O
spike-and-slab	O
prior	O
results	O
in	O
better	O
prediction	O
accuracy	O
than	O
using	O
the	O
posterior	B
mode	B
with	O
a	O
laplace	B
prior	O
albeit	O
at	O
slightly	O
higher	O
computational	O
cost	O
regularization	B
algorithms	O
regularization	B
algorithms	O
in	O
this	O
section	O
we	O
give	O
a	O
brief	O
review	O
of	O
some	O
algorithms	O
that	O
can	O
be	O
used	O
to	O
solve	O
regularized	B
estimation	I
problems	O
we	O
focus	O
on	O
the	O
lasso	B
case	O
where	O
we	O
have	O
a	O
quadratic	B
loss	B
however	O
most	O
of	O
the	O
algorithms	O
can	O
be	O
extended	O
to	O
more	O
general	O
settings	O
such	O
as	O
logistic	B
regression	B
et	O
al	O
for	O
a	O
comprehensive	O
review	O
of	O
regularized	O
logistic	B
regression	B
note	O
that	O
this	O
area	O
of	O
machine	B
learning	B
is	O
advancing	O
very	O
rapidly	O
so	O
the	O
methods	O
below	O
may	O
not	O
be	O
state	B
of	O
the	O
art	O
by	O
the	O
time	O
you	O
read	O
this	O
chapter	O
et	O
al	O
yaun	O
et	O
al	O
yang	O
et	O
al	O
for	O
some	O
recent	O
surveys	O
coordinate	O
descent	O
sometimes	O
it	O
is	O
hard	O
to	O
optimize	O
all	O
the	O
variables	O
simultaneously	O
but	O
it	O
easy	O
to	O
optimize	O
them	O
one	O
by	O
one	O
in	O
particular	O
we	O
can	O
solve	O
for	O
the	O
j	O
th	O
coefficient	O
with	O
all	O
the	O
others	O
held	O
fixed	O
w	O
j	O
argmin	O
z	O
f	O
zej	O
f	O
where	O
ej	O
is	O
the	O
j	O
th	O
unit	O
vector	O
we	O
can	O
either	O
cycle	B
through	O
the	O
coordinates	O
in	O
a	O
deterministic	O
fashion	O
or	O
we	O
can	O
sample	O
them	O
at	O
random	O
or	O
we	O
can	O
choose	O
to	O
update	O
the	O
coordinate	O
for	O
which	O
the	O
gradient	O
is	O
steepest	O
the	O
coordinate	O
descent	O
method	O
is	O
particularly	O
appealing	O
if	O
each	O
one-dimensional	O
optimization	B
problem	O
can	O
be	O
solved	O
analytically	O
for	O
example	O
the	O
shooting	B
algorithm	O
wu	O
and	O
lange	O
for	O
lasso	B
uses	O
equation	O
to	O
compute	O
the	O
optimal	O
value	O
of	O
wj	O
given	O
all	O
the	O
other	O
coefficients	O
see	O
algorithm	O
for	O
the	O
pseudo	O
code	O
lassoshooting	O
for	O
some	O
matlab	O
code	O
see	O
et	O
al	O
for	O
some	O
extensions	O
of	O
this	O
method	O
to	O
the	O
logistic	B
regression	B
case	O
the	O
resulting	O
algorithm	O
was	O
the	O
fastest	O
method	O
in	O
their	O
experimental	O
comparison	O
which	O
concerned	O
document	O
classification	O
with	O
large	O
sparse	B
feature	O
vectors	O
bags	O
of	O
words	O
other	O
types	O
of	O
data	O
dense	O
features	B
andor	O
regression	B
problems	O
might	O
call	O
for	O
different	O
algorithms	O
y	O
algorithm	O
coordinate	O
descent	O
for	O
lasso	B
shooting	B
algorithm	O
initialize	O
w	O
x	O
i	O
repeat	O
for	O
j	O
d	O
do	O
ij	O
xijyi	O
wt	O
xi	O
wjxij	O
cj	O
aj	O
aj	O
aj	O
cj	O
wj	O
soft	O
until	O
converged	O
lars	B
and	O
other	O
homotopy	B
methods	O
the	O
problem	O
with	O
coordinate	O
descent	O
is	O
that	O
it	O
only	O
updates	O
one	O
variable	O
at	O
a	O
time	O
so	O
can	O
be	O
slow	O
to	O
converge	B
active	B
set	I
methods	O
update	O
many	O
variables	O
at	O
a	O
time	O
unfortunately	O
they	O
are	O
chapter	O
sparse	B
linear	O
models	O
more	O
complicated	O
because	O
of	O
the	O
need	O
to	O
identify	O
which	O
variables	O
are	O
constrained	O
to	O
be	O
zero	O
and	O
which	O
are	O
free	O
to	O
be	O
updated	O
active	B
set	I
methods	O
typically	O
only	O
add	O
or	O
remove	O
a	O
few	O
variables	O
at	O
a	O
time	O
so	O
they	O
can	O
take	O
a	O
long	O
if	O
they	O
are	O
started	O
far	O
from	O
the	O
solution	O
but	O
they	O
are	O
ideally	O
suited	O
for	O
generating	O
a	O
set	O
of	O
solutions	O
for	O
different	O
values	O
of	O
starting	O
with	O
the	O
empty	O
set	O
i	O
e	O
for	O
generating	O
regularization	B
path	B
these	O
algorithms	O
exploit	O
the	O
fact	O
that	O
one	O
can	O
quickly	O
compute	O
w	O
k	O
from	O
w	O
k	O
if	O
k	O
k	O
this	O
is	O
known	O
as	O
warm	B
starting	I
in	O
fact	O
even	O
if	O
we	O
only	O
want	O
the	O
solution	O
for	O
a	O
single	O
value	O
of	O
call	O
it	O
it	O
can	O
sometimes	O
be	O
computationally	O
more	O
efficient	O
to	O
compute	O
a	O
set	O
of	O
solutions	O
from	O
max	O
down	O
to	O
using	O
warm-starting	O
this	O
is	O
called	O
a	O
continuation	B
method	I
or	O
homotopy	B
method	O
this	O
is	O
often	O
much	O
faster	O
than	O
directly	O
cold-starting	O
at	O
this	O
is	O
particularly	O
true	O
if	O
is	O
small	O
perhaps	O
the	O
most	O
well-known	O
example	O
of	O
a	O
homotopy	B
method	O
in	O
machine	B
learning	B
is	O
the	O
lars	B
algorithm	O
which	O
stands	O
for	O
least	O
angle	O
regression	B
and	O
shrinkage	B
et	O
al	O
similar	B
algorithm	O
was	O
independently	O
invented	O
in	O
et	O
al	O
this	O
can	O
compute	O
w	O
for	O
all	O
possible	O
values	O
of	O
in	O
an	O
efficient	O
manner	O
lars	B
works	O
as	O
follows	O
it	O
starts	O
with	O
a	O
large	O
value	O
of	O
such	O
that	O
only	O
the	O
variable	O
that	O
is	O
most	O
correlated	O
with	O
the	O
response	O
vector	O
y	O
is	O
chosen	O
then	O
is	O
decreased	O
until	O
a	O
second	O
variable	O
is	O
found	O
which	O
has	O
the	O
same	O
correlation	O
terms	O
of	O
magnitude	O
with	O
the	O
current	O
residual	B
as	O
the	O
first	O
variable	O
where	O
the	O
residual	B
at	O
step	O
k	O
is	O
defined	O
as	O
rk	O
y	O
xfk	O
wk	O
where	O
fk	O
is	O
the	O
current	O
active	B
set	I
equation	O
remarkably	O
one	O
can	O
solve	O
for	O
this	O
new	O
value	O
of	O
analytically	O
by	O
using	O
a	O
geometric	O
argument	O
the	O
term	O
least	O
angle	O
this	O
allows	O
the	O
algorithm	O
to	O
quickly	O
jump	O
to	O
the	O
next	O
point	O
on	O
the	O
regularization	B
path	B
where	O
the	O
active	B
set	I
changes	O
this	O
repeats	O
until	O
all	O
the	O
variables	O
are	O
added	O
it	O
is	O
necessary	O
to	O
allow	O
variables	O
to	O
be	O
removed	O
from	O
the	O
active	B
set	I
if	O
we	O
want	O
the	O
sequence	O
of	O
solutions	O
to	O
correspond	O
to	O
the	O
regularization	B
path	B
of	O
lasso	B
if	O
we	O
disallow	O
variable	O
removal	O
we	O
get	O
a	O
slightly	O
different	O
algorithm	O
called	O
lar	B
which	O
tends	O
to	O
be	O
faster	O
in	O
particular	O
lar	B
costs	O
the	O
same	O
as	O
a	O
single	O
ordinary	B
least	B
squares	I
fit	O
namely	O
on	O
d	O
minn	O
d	O
which	O
is	O
on	O
if	O
n	O
d	O
and	O
on	O
if	O
d	O
n	O
lar	B
is	O
very	O
similar	B
to	O
greedy	O
forward	O
selection	O
and	O
a	O
method	O
known	O
as	O
least	B
squares	I
boosting	B
section	O
there	O
have	O
been	O
many	O
attempts	O
to	O
extend	O
the	O
lars	B
algorithm	O
to	O
compute	O
the	O
full	B
regularization	B
path	B
for	O
regularized	O
glms	O
such	O
as	O
logistic	B
regression	B
in	O
general	O
one	O
cannot	O
analytically	O
solve	O
for	O
the	O
critical	O
values	O
of	O
instead	O
the	O
standard	O
approach	O
is	O
to	O
start	O
at	O
max	O
and	O
then	O
slowly	O
decrease	O
tracking	B
the	O
solution	O
as	O
we	O
go	O
this	O
is	O
called	O
a	O
continuation	B
method	I
or	O
homotopy	B
method	O
these	O
methods	O
exploit	O
the	O
fact	O
that	O
we	O
can	O
quickly	O
compute	O
w	O
k	O
from	O
w	O
k	O
if	O
k	O
k	O
this	O
is	O
known	O
as	O
warm	B
starting	I
even	O
if	O
we	O
don	O
t	O
want	O
the	O
full	B
path	B
this	O
method	O
is	O
often	O
much	O
faster	O
than	O
directly	O
cold-starting	O
at	O
the	O
desired	O
value	O
of	O
is	O
particularly	O
true	O
if	O
is	O
small	O
the	O
method	O
described	O
in	O
et	O
al	O
combines	O
coordinate	O
descent	O
with	O
this	O
warmstarting	O
strategy	O
and	O
computes	O
the	O
full	B
regularization	B
path	B
for	O
any	O
regularized	O
glm	B
this	O
has	O
been	O
implemented	O
in	O
the	O
glmnet	B
package	O
which	O
is	O
bundled	O
with	O
pmtk	O
proximal	O
and	O
gradient	O
projection	B
methods	O
in	O
this	O
section	O
we	O
consider	O
some	O
methods	O
that	O
are	O
suitable	O
for	O
very	O
large	O
scale	O
problems	O
where	O
homotopy	B
methods	O
made	O
be	O
too	O
slow	O
these	O
methods	O
will	O
also	O
be	O
easy	O
to	O
extend	O
to	O
other	O
kinds	O
regularization	B
algorithms	O
of	O
regularizers	O
beyond	O
as	O
we	O
will	O
see	O
later	O
our	O
presentation	O
in	O
this	O
section	O
is	O
based	O
on	O
yang	O
et	O
al	O
consider	O
a	O
convex	B
objective	O
of	O
the	O
form	O
f	O
l	O
where	O
l	O
the	O
loss	B
is	O
convex	B
and	O
differentiable	O
and	O
r	O
the	O
regularizer	O
is	O
convex	B
but	O
not	O
necessarily	O
differentiable	O
for	O
example	O
l	O
rss	O
and	O
r	O
corresponds	O
to	O
the	O
bpdn	B
problem	O
as	O
another	O
example	O
the	O
lasso	B
problem	O
can	O
be	O
formulated	O
as	O
follows	O
l	O
rss	O
and	O
r	O
c	O
where	O
c	O
b	O
and	O
ic	O
is	O
the	O
indicator	B
function	I
of	O
a	O
convex	B
set	O
c	O
defined	O
as	O
ic	O
c	O
otherwise	O
in	O
some	O
cases	O
it	O
is	O
easy	O
to	O
optimize	O
functions	O
of	O
the	O
form	O
in	O
equation	O
for	O
example	O
suppose	O
l	O
rss	O
and	O
the	O
design	B
matrix	I
is	O
simply	O
x	O
i	O
then	O
the	O
obective	O
becomes	O
f	O
r	O
the	O
minimizer	O
of	O
this	O
is	O
given	O
by	O
proxry	O
which	O
is	O
the	O
proximal	B
operator	I
for	O
the	O
convex	B
function	O
r	O
defined	O
by	O
proxry	O
argmin	O
rz	O
intuitively	O
we	O
are	O
returning	O
a	O
point	O
that	O
minimizes	O
r	O
but	O
which	O
is	O
also	O
close	O
to	O
y	O
in	O
general	O
we	O
will	O
use	O
this	O
operator	O
inside	O
an	O
iterative	O
optimizer	O
in	O
which	O
case	O
we	O
want	O
to	O
stay	O
close	O
to	O
the	O
previous	O
iterate	O
in	O
this	O
case	O
we	O
use	O
proxr	O
k	O
argmin	O
rz	O
z	O
z	O
the	O
key	O
issues	O
are	O
how	O
do	O
we	O
efficiently	O
compute	O
the	O
proximal	B
operator	I
for	O
different	O
regularizers	O
r	O
and	O
how	O
do	O
we	O
extend	O
this	O
technique	O
to	O
more	O
general	O
loss	B
functions	O
l	O
we	O
discuss	O
these	O
issues	O
below	O
proximal	O
operators	O
if	O
r	O
the	O
proximal	B
operator	I
is	O
given	O
by	O
componentwise	O
soft-thresholding	O
proxr	O
soft	O
as	O
we	O
showed	O
in	O
section	O
if	O
r	O
the	O
proximal	B
operator	I
is	O
given	O
by	O
componentwise	O
hard-thresholding	O
where	O
hardu	O
a	O
uiu	O
a	O
proxr	O
hard	O
if	O
r	O
ic	O
the	O
proximal	B
operator	I
is	O
given	O
by	O
the	O
projection	B
onto	O
the	O
set	O
c	O
proxr	O
argmin	O
z	O
c	O
projc	O
chapter	O
sparse	B
linear	O
models	O
illustration	O
of	O
projected	B
gradient	B
descent	I
the	O
step	O
along	O
the	O
negative	O
gradient	O
to	O
k	O
gk	O
figure	O
if	O
we	O
project	O
that	O
point	O
onto	O
the	O
closest	O
point	O
in	O
the	O
set	O
we	O
get	O
takes	O
us	O
outside	O
the	O
feasible	O
set	O
proj	O
k	O
gk	O
we	O
can	O
then	O
derive	O
the	O
implicit	O
update	O
direction	O
using	O
dk	O
k	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
for	O
some	O
convex	B
sets	O
it	O
is	O
easy	O
to	O
compute	O
the	O
projection	B
operator	O
for	O
example	O
to	O
project	O
onto	O
the	O
rectangular	O
set	O
defined	O
by	O
the	O
box	B
constraints	I
c	O
j	O
uj	O
we	O
can	O
use	O
j	O
uj	O
j	O
j	O
uj	O
j	O
uj	O
projc	O
projc	O
to	O
project	O
onto	O
the	O
euclidean	O
ball	O
c	O
we	O
can	O
use	O
to	O
project	O
onto	O
the	O
ball	O
c	O
we	O
can	O
use	O
projc	O
soft	O
where	O
if	O
and	O
otherwise	O
is	O
the	O
solution	O
to	O
the	O
equation	O
max	O
j	O
we	O
can	O
implement	O
the	O
whole	O
procedure	O
in	O
od	O
time	O
as	O
explained	O
in	O
et	O
al	O
we	O
will	O
see	O
an	O
application	O
of	O
these	O
different	O
projection	B
methods	O
in	O
section	O
proximal	O
gradient	O
method	O
we	O
now	O
discuss	O
how	O
to	O
use	O
the	O
proximal	B
operator	I
inside	O
of	O
a	O
gradient	B
descent	I
routine	O
the	O
basic	O
idea	O
is	O
to	O
minimize	O
a	O
simple	O
quadratic	O
approximation	O
to	O
the	O
loss	B
function	I
centered	O
on	O
the	O
regularization	B
algorithms	O
k	O
z	O
rz	O
k	O
t	O
argmin	O
where	O
gk	O
l	O
k	O
is	O
the	O
gradient	O
of	O
the	O
loss	B
tk	O
is	O
a	O
constant	O
discussed	O
below	O
and	O
the	O
last	O
term	O
arises	O
from	O
a	O
simple	O
approximation	O
to	O
the	O
hessian	B
of	O
the	O
loss	B
of	O
the	O
form	O
k	O
i	O
dropping	O
terms	O
that	O
are	O
independent	O
of	O
z	O
and	O
multiplying	O
by	O
tk	O
we	O
can	O
rewrite	O
the	O
above	O
tk	O
k	O
k	O
expression	O
in	O
terms	O
of	O
a	O
proximal	B
operator	I
as	O
follows	O
argmin	O
tkrz	O
proxtkruk	O
z	O
uk	O
k	O
tkgk	O
gk	O
l	O
k	O
if	O
r	O
this	O
is	O
equivalent	O
to	O
gradient	B
descent	I
if	O
r	O
c	O
the	O
method	O
is	O
equivalent	O
if	O
r	O
the	O
method	O
is	O
to	O
projected	B
gradient	B
descent	I
sketched	O
in	O
figure	O
known	O
as	O
iterative	B
soft	B
thresholding	I
mation	O
to	O
the	O
hessian	B
we	O
require	O
that	O
there	O
are	O
several	O
ways	O
to	O
pick	O
tk	O
or	O
equivalently	O
k	O
given	O
that	O
ki	O
is	O
an	O
approxi	O
k	O
k	O
k	O
gk	O
gk	O
in	O
the	O
least	B
squares	I
sense	O
hence	O
k	O
argmin	O
k	O
k	O
gk	O
k	O
k	O
gk	O
k	O
k	O
k	O
k	O
this	O
is	O
known	O
as	O
the	O
barzilai-borwein	B
or	O
spectral	B
stepsize	O
and	O
borwein	O
fletcher	O
raydan	O
this	O
stepsize	O
can	O
be	O
used	O
with	O
any	O
gradient	O
method	O
whether	O
proximal	O
or	O
not	O
it	O
does	O
not	O
lead	O
to	O
monotonic	O
decrease	O
of	O
the	O
objective	O
but	O
it	O
is	O
much	O
faster	O
than	O
standard	O
line	B
search	I
techniques	O
ensure	O
convergence	O
we	O
require	O
that	O
the	O
objective	O
decrease	O
on	O
average	O
where	O
the	O
average	O
is	O
computed	O
over	O
a	O
sliding	O
window	O
of	O
size	O
m	O
when	O
we	O
combine	O
the	O
bb	O
stepsize	O
with	O
the	O
iterative	B
soft	B
thresholding	I
technique	O
r	O
plus	O
a	O
continuation	B
method	I
that	O
gradually	O
reduces	O
we	O
get	O
a	O
fast	O
method	O
for	O
the	O
bpdn	B
problem	O
known	O
as	O
the	O
sparsa	B
algorithm	O
which	O
stands	O
for	O
sparse	B
reconstruction	O
by	O
separable	O
approximation	O
et	O
al	O
however	O
we	O
will	O
call	O
it	O
the	O
iterative	B
shrinkage	B
and	I
thresholding	I
algorithm	I
see	O
algorithm	O
for	O
some	O
pseudocode	O
and	O
sparsa	B
for	O
some	O
matlab	O
code	O
see	O
also	O
exercise	O
for	O
a	O
related	O
approach	O
based	O
on	O
projected	B
gradient	B
descent	I
nesterov	O
s	O
method	O
a	O
faster	O
version	O
of	O
proximal	O
gradient	B
descent	I
can	O
be	O
obtained	O
by	O
epxanding	O
the	O
quadratic	O
approximation	O
around	O
a	O
point	O
other	O
than	O
the	O
most	O
recent	O
parameter	B
value	O
in	O
particular	O
consider	O
performing	O
updates	O
of	O
the	O
form	O
proxtkr	O
k	O
tkgk	O
gk	O
l	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
chapter	O
sparse	B
linear	O
models	O
n	O
d	O
y	O
r	O
n	O
parameters	O
m	O
s	O
algorithm	O
iterative	O
shrinkage-thresholding	O
algorithm	O
input	O
x	O
r	O
initialize	O
r	O
y	O
repeat	O
t	O
maxsxt	O
r	O
adapt	O
the	O
regularizer	O
repeat	O
g	O
l	O
u	O
g	O
softu	O
t	O
update	O
using	O
bb	O
stepsize	O
in	O
equation	O
until	O
f	O
increased	O
too	O
much	O
within	O
the	O
past	O
m	O
steps	O
r	O
y	O
x	O
update	O
residual	B
until	O
t	O
j	O
wj	O
yi	O
xi	O
d	O
n	O
figure	O
representing	O
lasso	B
using	O
a	O
gaussian	B
scale	I
mixture	B
prior	O
this	O
is	O
known	O
as	O
nesterov	O
s	O
method	O
tseng	O
as	O
before	O
there	O
are	O
a	O
variety	O
of	O
ways	O
of	O
setting	O
tk	O
typically	O
one	O
uses	O
line	B
search	I
when	O
this	O
method	O
is	O
combined	O
with	O
the	O
iterative	B
soft	B
thresholding	I
technique	O
r	O
plus	O
a	O
continuation	B
method	I
that	O
gradually	O
reduces	O
we	O
get	O
a	O
fast	O
method	O
for	O
the	O
bpdn	B
problem	O
known	O
as	O
the	O
fast	B
iterative	I
shrinkage	B
thesholding	I
algorithm	I
or	O
fista	B
and	O
teboulle	O
regularization	B
algorithms	O
em	B
for	O
lasso	B
in	O
this	O
section	O
we	O
show	O
how	O
to	O
solve	O
the	O
lasso	B
problem	O
using	O
lasso	B
at	O
first	O
sight	O
this	O
might	O
seem	O
odd	O
since	O
there	O
are	O
no	O
hidden	B
variables	I
the	O
key	O
insight	O
is	O
that	O
we	O
can	O
represent	O
the	O
laplace	B
distribution	I
as	O
a	O
gaussian	B
scale	I
mixture	B
and	O
mallows	O
west	O
as	O
follows	O
e	O
n	O
j	O
j	O
j	O
j	O
ga	O
thus	O
the	O
laplace	B
is	O
a	O
gsm	O
where	O
the	O
mixing	O
distibution	O
on	O
the	O
variances	O
is	O
the	O
exponential	O
j	O
distribution	O
expon	O
using	O
this	O
decomposition	O
we	O
can	O
represent	O
the	O
lasso	B
model	O
as	O
shown	O
in	O
figure	O
the	O
corresponding	O
joint	B
distribution	I
has	O
the	O
form	O
py	O
w	O
n	O
d	O
ig	O
b	O
ga	O
j	O
j	O
where	O
d	O
diag	O
j	O
and	O
where	O
we	O
have	O
assumed	O
for	O
notational	O
simplicity	O
that	O
x	O
is	O
standardized	B
and	O
that	O
y	O
is	O
centered	O
we	O
can	O
ignore	O
the	O
offset	O
term	O
expanding	O
out	O
we	O
get	O
py	O
w	O
exp	O
wt	O
d	O
w	O
exp	O
exp	O
b	O
exp	O
j	O
j	O
below	O
we	O
describe	O
how	O
to	O
apply	O
the	O
em	B
algorithm	O
to	O
the	O
model	O
in	O
figure	O
in	O
brief	O
in	O
the	O
e	B
step	I
we	O
infer	O
j	O
and	O
and	O
in	O
the	O
m	B
step	I
we	O
estimate	O
w	O
the	O
resulting	O
estimate	O
w	O
is	O
the	O
same	O
as	O
the	O
lasso	B
estimator	B
this	O
approach	O
was	O
first	O
proposed	O
in	O
also	O
and	O
brown	O
caron	O
and	O
doucet	O
ding	O
and	O
harrison	O
for	O
some	O
extensions	O
why	O
em	B
before	O
going	O
into	O
the	O
details	O
of	O
em	B
it	O
is	O
worthwhile	O
asking	O
why	O
we	O
are	O
presenting	O
this	O
approach	O
at	O
all	O
given	O
that	O
there	O
are	O
a	O
variety	O
of	O
other	O
much	O
faster	O
algorithms	O
that	O
directly	O
solve	O
the	O
map	O
estimation	O
problem	O
for	O
an	O
empirical	O
comparison	O
the	O
reason	O
is	O
that	O
the	O
latent	B
variable	O
perspective	O
brings	O
several	O
advantages	O
such	O
as	O
the	O
following	O
it	O
provides	O
an	O
easy	O
way	O
to	O
derive	O
an	O
algorithm	O
to	O
find	O
parameter	B
estimates	O
for	O
a	O
variety	O
of	O
other	O
models	O
such	O
as	O
robust	B
linear	B
regression	B
or	O
probit	B
regression	B
to	O
ensure	O
the	O
posterior	O
is	O
unimodal	O
one	O
can	O
follow	O
and	O
casella	O
and	O
slightly	O
modify	O
the	O
model	O
by	O
making	O
the	O
prior	O
variance	B
for	O
the	O
weights	O
depend	O
on	O
the	O
observation	B
noise	O
pwj	O
j	O
the	O
em	B
algorithm	O
is	O
easy	O
to	O
modify	O
j	O
chapter	O
sparse	B
linear	O
models	O
it	O
suggests	O
trying	O
other	O
priors	O
on	O
the	O
variances	O
besides	O
ga	O
various	O
extensions	O
below	O
it	O
makes	O
it	O
clear	O
how	O
we	O
can	O
compute	O
the	O
full	B
posterior	O
pwd	O
rather	O
than	O
just	O
a	O
map	B
estimate	I
this	O
technique	O
is	O
known	O
as	O
the	O
bayesian	B
lasso	B
and	O
casella	O
hans	O
j	O
we	O
will	O
consider	O
the	O
objective	O
function	O
from	O
equation	O
the	O
complete	B
data	I
penalized	B
log	I
likelihood	B
is	O
as	O
follows	O
terms	O
that	O
do	O
not	O
depend	O
on	O
w	O
wt	O
w	O
const	O
where	O
diag	O
j	O
is	O
the	O
precision	B
matrix	I
for	O
w	O
the	O
e	B
step	I
j	O
the	O
key	O
is	O
to	O
compute	O
e	O
we	O
can	O
derive	O
the	O
full	B
posterior	O
which	O
is	O
given	O
by	O
the	O
following	O
and	O
casella	O
we	O
can	O
derive	O
this	O
directly	O
exercise	O
alternatively	O
j	O
j	O
that	O
the	O
inverse	B
gaussian	B
distribution	O
is	O
also	O
known	O
as	O
the	O
wald	B
distribution	O
hence	O
e	O
j	O
let	O
diage	O
d	O
e	O
denote	O
the	O
result	O
of	O
this	O
e	B
step	I
we	O
also	O
need	O
to	O
infer	O
it	O
is	O
easy	O
to	O
show	O
that	O
that	O
the	O
posterior	O
is	O
p	O
w	O
iga	O
b	O
hence	O
x	O
wt	O
x	O
w	O
igan	O
bn	O
e	O
an	O
bn	O
the	O
m	B
step	I
the	O
m	B
step	I
consists	O
of	O
computing	O
w	O
argmax	O
w	O
wt	O
w	O
this	O
is	O
just	O
map	O
estimation	O
under	O
a	O
gaussian	B
prior	O
w	O
xt	O
x	O
y	O
regularization	B
extensions	O
however	O
since	O
we	O
expect	O
many	O
wj	O
we	O
will	O
have	O
j	O
for	O
many	O
j	O
making	O
inverting	O
numerically	O
unstable	B
fortunately	O
we	O
can	O
use	O
the	O
svd	B
of	O
x	O
given	O
by	O
x	O
udvt	O
as	O
follows	O
w	O
vvt	O
v	O
d	O
y	O
where	O
diag	O
j	O
e	O
diag	O
caveat	O
since	O
the	O
lasso	B
objective	O
is	O
convex	B
this	O
method	O
should	O
always	O
find	O
the	O
global	O
optimum	O
unfortunately	O
this	O
sometimes	O
does	O
not	O
happen	O
for	O
numerical	O
reasons	O
in	O
particular	O
suppose	O
that	O
in	O
j	O
further	O
suppose	O
that	O
we	O
set	O
wj	O
in	O
an	O
m	B
step	I
in	O
the	O
following	O
e	O
the	O
true	O
solution	O
w	O
step	O
we	O
infer	O
that	O
j	O
so	O
then	O
we	O
set	O
wj	O
again	O
thus	O
we	O
can	O
never	O
undo	O
our	O
mistake	O
fortunately	O
in	O
practice	O
this	O
situation	O
seems	O
to	O
be	O
rare	O
see	O
and	O
li	O
for	O
further	O
discussion	O
regularization	B
extensions	O
in	O
this	O
section	O
we	O
discuss	O
various	O
extensions	O
of	O
vanilla	O
regularization	B
group	B
lasso	B
in	O
standard	O
regularization	B
we	O
assume	O
that	O
there	O
is	O
a	O
correspondence	B
between	O
parameters	O
and	O
variables	O
so	O
that	O
if	O
wj	O
we	O
interpret	O
this	O
to	O
mean	B
that	O
variable	O
j	O
is	O
excluded	O
but	O
in	O
more	O
complex	O
models	O
there	O
may	O
be	O
many	O
parameters	O
associated	O
with	O
a	O
given	O
variable	O
in	O
particular	O
we	O
may	O
have	O
a	O
vector	O
of	O
weights	O
for	O
each	O
input	O
wj	O
here	O
are	O
some	O
examples	O
multinomial	B
logistic	B
regression	B
each	O
feature	O
is	O
associated	O
with	O
c	O
different	O
weights	O
one	O
per	O
class	O
linear	B
regression	B
with	O
categorical	B
inputs	O
each	O
scalar	O
input	O
is	O
one-hot	O
encoded	O
into	O
a	O
vector	O
of	O
length	O
c	O
multi-task	B
learning	B
in	O
multi-task	B
learning	B
we	O
have	O
multiple	O
related	O
prediction	O
problems	O
for	O
example	O
we	O
might	O
have	O
c	O
separate	O
regression	B
or	O
binary	O
classification	O
problems	O
thus	O
each	O
feature	O
is	O
associated	O
with	O
c	O
different	O
weights	O
we	O
may	O
want	O
to	O
use	O
a	O
feature	O
for	O
all	O
of	O
the	O
tasks	O
or	O
none	O
of	O
the	O
tasks	O
and	O
thus	O
select	O
weights	O
at	O
the	O
group	O
level	O
et	O
al	O
if	O
we	O
use	O
an	O
regularizer	O
of	O
the	O
form	O
c	O
we	O
may	O
end	O
up	O
with	O
with	O
some	O
elements	O
of	O
wj	O
being	O
zero	O
and	O
some	O
not	O
to	O
prevent	O
this	O
kind	O
of	O
situation	O
we	O
partition	O
the	O
parameter	B
vector	O
into	O
g	O
groups	O
we	O
now	O
minimize	O
the	O
following	O
objective	O
j	O
jw	O
nllw	O
where	O
j	O
g	O
j	O
chapter	O
sparse	B
linear	O
models	O
is	O
the	O
of	O
the	O
group	O
weight	B
vector	I
group	B
lasso	B
and	O
lin	O
if	O
the	O
nll	B
is	O
least	B
squares	I
this	O
method	O
is	O
called	O
we	O
often	O
use	O
a	O
larger	O
penalty	O
for	O
larger	O
groups	O
by	O
setting	O
g	O
number	O
of	O
elements	O
in	O
group	O
g	O
for	O
example	O
objective	O
becomes	O
dg	O
where	O
dg	O
is	O
the	O
if	O
we	O
have	O
groups	O
and	O
the	O
jw	O
nllw	O
note	O
that	O
if	O
we	O
had	O
used	O
the	O
square	O
of	O
the	O
the	O
model	O
would	O
become	O
equivalent	O
to	O
ridge	B
regression	B
since	O
j	O
g	O
j	O
g	O
by	O
using	O
the	O
square	O
root	B
we	O
are	O
penalizing	O
the	O
radius	O
of	O
a	O
ball	O
containing	O
the	O
group	O
s	O
weight	B
vector	I
the	O
only	O
way	O
for	O
the	O
radius	O
to	O
be	O
small	O
is	O
if	O
all	O
elements	O
are	O
small	O
thus	O
the	O
square	O
root	B
results	O
in	O
group	O
sparsity	B
a	O
variant	O
of	O
this	O
technique	O
replaces	O
the	O
with	O
the	O
infinity-norm	O
et	O
al	O
zhao	O
et	O
al	O
max	O
j	O
g	O
it	O
is	O
clear	O
that	O
this	O
will	O
also	O
result	O
in	O
group	O
sparsity	B
an	O
illustration	O
of	O
the	O
difference	O
is	O
shown	O
in	O
figures	O
and	O
in	O
both	O
cases	O
we	O
have	O
a	O
true	O
signal	O
w	O
of	O
size	O
d	O
divided	O
into	O
groups	O
each	O
of	O
size	O
we	O
randomly	O
choose	O
groups	O
of	O
w	O
and	O
assign	O
them	O
non-zero	O
values	O
in	O
the	O
first	O
example	O
the	O
values	O
are	O
drawn	O
from	O
a	O
n	O
in	O
the	O
second	O
example	O
the	O
values	O
are	O
all	O
set	O
to	O
we	O
then	O
pick	O
a	O
random	O
design	B
matrix	I
x	O
of	O
size	O
n	O
d	O
where	O
n	O
finally	O
we	O
generate	O
y	O
xw	O
where	O
n	O
given	O
this	O
data	O
we	O
estimate	O
the	O
support	B
of	O
w	O
using	O
or	O
group	O
and	O
then	O
estimate	O
the	O
non-zero	O
values	O
using	O
least	B
squares	I
we	O
see	O
that	O
group	B
lasso	B
does	O
a	O
much	O
better	O
job	O
than	O
vanilla	O
lasso	B
since	O
it	O
respects	O
the	O
known	O
group	O
we	O
also	O
see	O
that	O
the	O
norm	O
has	O
a	O
tendency	O
to	O
make	O
all	O
the	O
elements	O
within	O
a	O
block	O
to	O
have	O
similar	B
magnitude	O
this	O
is	O
appropriate	O
in	O
the	O
second	O
example	O
but	O
not	O
the	O
first	O
value	O
of	O
was	O
the	O
same	O
in	O
all	O
examples	O
and	O
was	O
chosen	O
by	O
hand	O
gsm	O
interpretation	O
of	O
group	B
lasso	B
group	B
lasso	B
is	O
equivalent	O
to	O
map	O
estimation	O
using	O
the	O
following	O
prior	O
pw	O
exp	O
the	O
slight	O
non-zero	O
noise	O
in	O
the	O
group	B
lasso	B
results	O
is	O
presumably	O
due	O
to	O
numerical	O
errors	O
regularization	B
extensions	O
original	O
number	O
groups	O
active	O
groups	O
standard	O
tau	O
mse	B
block	O
tau	O
mse	B
block	O
linf	O
tau	O
mse	B
figure	O
illustration	O
of	O
group	B
lasso	B
where	O
the	O
original	O
signal	O
is	O
piecewise	O
gaussian	B
top	O
left	O
original	O
signal	O
bottom	O
left	O
vanilla	O
lasso	B
estimate	O
top	O
right	O
group	B
lasso	B
estimate	O
using	O
a	O
norm	O
on	O
the	O
blocks	O
bottom	O
right	O
group	B
lasso	B
estimate	O
using	O
an	O
norm	O
on	O
the	O
blocks	O
based	O
on	O
figures	O
of	O
et	O
al	O
figure	O
generated	O
by	O
grouplassodemo	O
based	O
on	O
code	O
by	O
mario	O
figueiredo	O
now	O
one	O
can	O
show	O
that	O
this	O
prior	O
can	O
be	O
written	O
as	O
a	O
gsm	O
as	O
follows	O
wg	O
g	O
n	O
g	O
ga	O
dg	O
g	O
idg	O
where	O
dg	O
is	O
the	O
size	O
of	O
group	O
g	O
so	O
we	O
see	O
that	O
there	O
is	O
one	O
variance	B
term	O
per	O
group	O
each	O
of	O
which	O
comes	O
from	O
a	O
gamma	B
prior	O
whose	O
shape	O
parameter	B
depends	O
on	O
the	O
group	O
size	O
and	O
whose	O
rate	B
parameter	B
is	O
controlled	O
by	O
figure	O
gives	O
an	O
example	O
where	O
we	O
have	O
groups	O
one	O
of	O
size	O
and	O
one	O
of	O
size	O
this	O
picture	O
also	O
makes	O
it	O
clearer	O
why	O
there	O
should	O
be	O
a	O
grouping	B
effect	I
suppose	O
is	O
will	O
be	O
estimated	O
to	O
be	O
small	O
which	O
will	O
force	O
to	O
be	O
small	O
converseley	O
will	O
be	O
estimated	O
to	O
be	O
large	O
which	O
will	O
allow	O
to	O
be	O
become	O
small	O
then	O
suppose	O
is	O
large	O
then	O
large	O
as	O
well	O
chapter	O
sparse	B
linear	O
models	O
original	O
number	O
groups	O
active	O
groups	O
standard	O
tau	O
mse	B
block	O
tau	O
mse	B
block	O
linf	O
tau	O
mse	B
figure	O
same	O
as	O
figure	O
except	O
the	O
original	O
signal	O
is	O
piecewise	O
constant	O
yi	O
xi	O
figure	O
graphical	B
model	I
for	O
group	B
lasso	B
with	O
groups	O
the	O
first	O
has	O
size	O
the	O
second	O
has	O
size	O
regularization	B
extensions	O
algorithms	O
for	O
group	B
lasso	B
there	O
are	O
a	O
variety	O
of	O
algorithms	O
for	O
group	B
lasso	B
here	O
we	O
briefly	O
mention	O
two	O
the	O
first	O
approach	O
is	O
based	O
on	O
proximal	O
gradient	B
descent	I
discussed	O
in	O
section	O
since	O
the	O
regularizer	O
g	O
the	O
proximal	B
operator	I
decomposes	B
into	O
g	O
separate	O
operators	O
is	O
separable	O
rw	O
of	O
the	O
form	O
where	O
b	O
kg	O
tkgkg	O
if	O
p	O
one	O
can	O
show	O
and	O
wajs	O
that	O
this	O
can	O
be	O
implemented	O
as	O
follows	O
proxrb	O
argmin	O
z	O
r	O
dg	O
where	O
c	O
is	O
the	O
ball	O
using	O
equation	O
if	O
we	O
have	O
proxrb	O
b	O
proj	O
cb	O
proxrb	O
b	O
b	O
otherwise	O
we	O
have	O
proxrb	O
b	O
b	O
b	O
we	O
can	O
combine	O
these	O
into	O
a	O
vectorial	O
soft-threshold	O
function	O
as	O
follows	O
et	O
al	O
proxrb	O
b	O
if	O
p	O
we	O
use	O
c	O
which	O
is	O
the	O
ball	O
we	O
can	O
project	O
onto	O
this	O
in	O
odg	O
time	O
using	O
an	O
algorithm	O
described	O
in	O
et	O
al	O
another	O
approach	O
is	O
to	O
modify	O
the	O
em	B
algorithm	O
the	O
method	O
is	O
almost	O
the	O
same	O
as	O
for	O
vanilla	O
lasso	B
if	O
we	O
define	O
gj	O
where	O
gj	O
is	O
the	O
group	O
to	O
which	O
dimension	O
j	O
belongs	O
we	O
can	O
use	O
the	O
same	O
full	B
conditionals	O
for	O
and	O
w	O
as	O
before	O
the	O
only	O
changes	O
are	O
as	O
follows	O
j	O
we	O
must	O
modify	O
the	O
full	B
conditional	I
for	O
the	O
weight	O
precisions	O
which	O
are	O
estimated	O
based	O
on	O
w	O
y	O
x	O
inversegaussian	O
j	O
g	O
jg	O
for	O
the	O
e	B
step	I
we	O
can	O
use	O
a	O
shared	B
set	O
of	O
weights	O
g	O
e	O
where	O
g	O
we	O
must	O
modify	O
the	O
full	B
conditional	I
for	O
the	O
tuning	O
parameter	B
which	O
is	O
now	O
only	O
estimated	O
based	O
on	O
g	O
values	O
of	O
g	O
p	O
gaa	O
b	O
g	O
g	O
chapter	O
sparse	B
linear	O
models	O
h	O
g	O
c	O
index	O
figure	O
example	O
of	O
the	O
fused	B
lasso	B
the	O
vertical	O
axis	O
represents	O
array	B
cgh	I
genome	B
source	O
figure	O
of	O
hybridization	O
intensity	O
and	O
the	O
horizontal	O
axis	O
represents	O
location	O
along	O
a	O
genome	B
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
holger	O
hoefling	O
fused	B
lasso	B
estimate	O
using	O
lattice	B
prior	O
noisy	O
image	O
fused	B
lasso	B
in	O
some	O
problem	O
settings	O
functional	B
data	I
analysis	I
we	O
want	O
neighboring	O
coefficients	O
to	O
be	O
similar	B
to	O
each	O
other	O
in	O
addition	O
to	O
being	O
sparse	B
an	O
example	O
is	O
given	O
in	O
figure	O
where	O
we	O
want	O
to	O
fit	O
a	O
signal	O
that	O
is	O
mostly	O
off	O
but	O
in	O
addition	O
has	O
the	O
property	O
that	O
neighboring	O
locations	O
are	O
typically	O
similar	B
in	O
value	O
we	O
can	O
model	O
this	O
by	O
using	O
a	O
prior	O
of	O
the	O
form	O
d	O
pw	O
exp	O
wj	O
this	O
is	O
known	O
as	O
the	O
fused	B
lasso	B
penalty	O
in	O
the	O
context	O
of	O
functional	B
data	I
analysis	I
we	O
often	O
use	O
x	O
i	O
so	O
there	O
is	O
one	O
coefficient	O
for	O
each	O
location	O
in	O
the	O
signal	O
section	O
in	O
this	O
case	O
the	O
overall	O
objective	O
has	O
the	O
form	O
jw	O
wi	O
n	O
s	O
v	O
this	O
is	O
a	O
sparse	B
version	O
of	O
equation	O
it	O
is	O
possible	O
to	O
generalize	B
this	O
idea	O
beyond	O
chains	O
and	O
to	O
consider	O
other	O
graph	B
structures	O
using	O
a	O
penalty	O
of	O
the	O
form	O
jw	O
wt	O
s	O
v	O
e	O
this	O
is	O
called	O
graph-guided	B
fused	B
lasso	B
e	O
g	O
et	O
al	O
the	O
graph	B
might	O
come	O
from	O
some	O
prior	O
knowledge	O
e	O
g	O
from	O
a	O
database	O
of	O
known	O
biological	O
pathways	O
another	O
example	O
is	O
shown	O
in	O
figure	O
where	O
the	O
graph	B
structure	O
is	O
a	O
lattice	B
regularization	B
extensions	O
gsm	O
interpretation	O
of	O
fused	B
lasso	B
one	O
can	O
show	O
et	O
al	O
that	O
the	O
fused	B
lasso	B
model	O
is	O
equivalent	O
to	O
the	O
following	O
hierarchical	O
model	O
w	O
n	O
j	O
expon	O
j	O
expon	O
j	O
j	O
where	O
and	O
is	O
a	O
tridiagonal	B
precision	B
matrix	I
with	O
main	O
diagonal	B
j	O
off	O
diagonal	B
j	O
j	O
j	O
where	O
we	O
have	O
defined	O
d	O
this	O
is	O
very	O
similar	B
to	O
the	O
model	O
in	O
section	O
where	O
we	O
used	O
a	O
chain-structured	O
gaussian	B
markov	B
random	O
field	O
as	O
the	O
prior	O
with	O
fixed	O
variance	B
here	O
we	O
just	O
let	O
the	O
variance	B
be	O
random	O
in	O
the	O
case	O
of	O
graph-guided	O
lasso	B
the	O
structure	O
of	O
the	O
graph	B
is	O
reflected	O
in	O
the	O
zero	O
pattern	B
of	O
the	O
gaussian	B
precision	B
matrix	I
section	O
algorithms	O
for	O
fused	B
lasso	B
it	O
is	O
possible	O
to	O
generalize	B
the	O
em	B
algorithm	O
to	O
fit	O
the	O
fused	B
lasso	B
model	O
by	O
exploiting	O
the	O
markov	B
structure	O
of	O
the	O
gaussian	B
prior	O
for	O
efficiency	O
direct	O
solvers	O
don	O
t	O
use	O
the	O
latent	B
variable	O
trick	O
can	O
also	O
be	O
derived	O
e	O
g	O
however	O
this	O
model	O
is	O
undeniably	O
more	O
expensive	O
to	O
fit	O
than	O
the	O
other	O
variants	O
we	O
have	O
considered	O
elastic	B
net	I
and	O
lasso	B
combined	O
although	O
lasso	B
has	O
proved	O
to	O
be	O
effective	O
as	O
a	O
variable	O
selection	O
technique	O
problems	O
and	O
hastie	O
such	O
as	O
the	O
following	O
it	O
has	O
several	O
if	O
there	O
is	O
a	O
group	O
of	O
variables	O
that	O
are	O
highly	O
correlated	O
genes	O
that	O
are	O
in	O
the	O
same	O
pathway	O
then	O
the	O
lasso	B
tends	O
to	O
select	O
only	O
one	O
of	O
them	O
chosen	O
rather	O
arbitrarily	O
is	O
evident	O
from	O
the	O
lars	B
algorithm	O
once	O
one	O
member	O
of	O
the	O
group	O
has	O
been	O
chosen	O
the	O
remaining	O
members	O
of	O
the	O
group	O
will	O
not	O
be	O
very	O
correlated	O
with	O
the	O
new	O
residual	B
and	O
hence	O
will	O
not	O
be	O
chosen	O
it	O
is	O
usually	O
better	O
to	O
select	O
all	O
the	O
relevant	O
variables	O
in	O
a	O
group	O
if	O
we	O
know	O
the	O
grouping	O
structure	O
we	O
can	O
use	O
group	B
lasso	B
but	O
often	O
we	O
don	O
t	O
know	O
the	O
grouping	O
structure	O
in	O
the	O
d	O
n	O
case	O
lasso	B
can	O
select	O
at	O
most	O
n	O
variables	O
before	O
it	O
saturates	O
if	O
n	O
d	O
but	O
the	O
variables	O
are	O
correlated	O
prediction	O
performance	O
of	O
ridge	O
is	O
better	O
than	O
that	O
of	O
lasso	B
it	O
has	O
been	O
empirically	O
observed	O
that	O
the	O
chapter	O
sparse	B
linear	O
models	O
zou	O
and	O
hastie	O
and	O
hastie	O
proposed	O
an	O
approach	O
called	O
the	O
elastic	B
net	I
which	O
is	O
a	O
hybrid	O
between	O
lasso	B
and	O
ridge	B
regression	B
which	O
solves	O
all	O
of	O
these	O
problems	O
it	O
is	O
apparently	O
called	O
the	O
elastic	B
net	I
because	O
it	O
is	O
like	O
a	O
stretchable	O
fishing	O
net	O
that	O
retains	O
all	O
the	O
big	O
fish	O
and	O
hastie	O
vanilla	O
version	O
the	O
vanilla	O
version	O
of	O
the	O
model	O
defines	O
the	O
following	O
objective	O
function	O
jw	O
notice	O
that	O
this	O
penalty	O
function	O
is	O
strictly	B
convex	B
so	O
there	O
is	O
a	O
unique	O
global	B
minimum	I
even	O
if	O
x	O
is	O
not	O
full	B
rank	O
it	O
can	O
be	O
shown	O
and	O
hastie	O
that	O
any	O
strictly	B
convex	B
penalty	O
on	O
w	O
will	O
exhibit	O
a	O
grouping	B
effect	I
which	O
means	O
that	O
the	O
regression	B
coefficients	O
of	O
highly	O
correlated	O
variables	O
tend	O
to	O
be	O
equal	O
to	O
a	O
change	O
of	O
sign	O
if	O
they	O
are	O
negatively	O
correlated	O
for	O
example	O
if	O
two	O
features	B
are	O
equal	O
so	O
xj	O
xk	O
one	O
can	O
show	O
that	O
their	O
estimates	O
are	O
also	O
equal	O
wj	O
wk	O
by	O
contrast	O
with	O
lasso	B
we	O
may	O
have	O
that	O
wj	O
and	O
wk	O
or	O
vice	O
versa	O
algorithms	O
for	O
vanilla	O
elastic	B
net	I
it	O
is	O
simple	O
to	O
show	O
that	O
the	O
elastic	B
net	I
problem	O
can	O
be	O
reduced	O
to	O
a	O
lasso	B
problem	O
on	O
modified	O
data	O
in	O
particular	O
define	O
x	O
c	O
x	O
y	O
y	O
where	O
c	O
then	O
we	O
solve	O
w	O
arg	O
min	O
w	O
y	O
x	O
c	O
and	O
set	O
w	O
c	O
w	O
we	O
can	O
use	O
lars	B
to	O
solve	O
this	O
subproblem	O
this	O
is	O
known	O
as	O
the	O
lars-en	O
algorithm	O
if	O
we	O
stop	O
the	O
algorithm	O
after	O
m	O
variables	O
have	O
been	O
included	O
the	O
cost	O
is	O
note	O
that	O
we	O
can	O
use	O
m	O
d	O
if	O
we	O
wish	O
since	O
x	O
has	O
rank	O
d	O
this	O
is	O
in	O
contrast	O
to	O
lasso	B
which	O
cannot	O
select	O
more	O
than	O
n	O
variables	O
jumping	O
to	O
the	O
ols	B
solution	O
if	O
n	O
d	O
when	O
using	O
lars-en	O
other	O
solvers	O
one	O
typically	O
uses	O
cross-validation	O
to	O
select	O
and	O
improved	O
version	O
unfortunately	O
it	O
turns	O
out	O
that	O
the	O
vanilla	O
elastic	B
net	I
does	O
not	O
produce	O
functions	O
that	O
predict	O
very	O
accurately	O
unless	O
it	O
is	O
very	O
close	O
to	O
either	O
pure	B
ridge	O
or	O
pure	B
lasso	B
intuitively	O
the	O
reason	O
is	O
that	O
it	O
performs	O
shrinkage	B
twice	O
once	O
due	O
to	O
the	O
penalty	O
and	O
again	O
due	O
to	O
the	O
penalty	O
the	O
solution	O
is	O
simple	O
undo	O
the	O
shrinkage	B
by	O
scaling	O
up	O
the	O
estimates	O
from	O
the	O
vanilla	O
version	O
in	O
other	O
words	O
if	O
w	O
is	O
the	O
solution	O
of	O
equation	O
then	O
a	O
better	O
estimate	O
is	O
w	O
w	O
non-convex	O
regularizers	O
we	O
will	O
call	O
this	O
a	O
corrected	O
estimate	O
one	O
can	O
show	O
that	O
the	O
corrected	O
estimates	O
are	O
given	O
by	O
xt	O
x	O
w	O
xw	O
w	O
arg	O
min	O
w	O
wt	O
now	O
xt	O
x	O
i	O
where	O
so	O
the	O
the	O
elastic	B
net	I
is	O
like	O
lasso	B
but	O
where	O
we	O
use	O
a	O
version	O
of	O
that	O
is	O
shrunk	O
towards	O
i	O
section	O
for	O
more	O
discussion	O
of	O
regularized	O
estimates	O
of	O
covariance	B
matrices	O
gsm	O
interpretation	O
of	O
elastic	B
net	I
the	O
implicit	O
prior	O
being	O
used	O
by	O
the	O
elastic	B
net	I
obviously	O
has	O
the	O
form	O
pw	O
exp	O
j	O
which	O
is	O
just	O
a	O
product	O
of	O
gaussian	B
and	O
laplace	B
distributions	O
this	O
can	O
be	O
written	O
as	O
a	O
hierarchical	O
prior	O
as	O
follows	O
et	O
al	O
chen	O
et	O
al	O
wj	O
j	O
n	O
j	O
j	O
expon	O
clearly	O
if	O
this	O
reduces	O
to	O
the	O
regular	B
lasso	B
it	O
is	O
possible	O
to	O
perform	O
map	O
estimation	O
in	O
this	O
model	O
using	O
em	B
or	O
bayesian	B
inference	B
using	O
mcmc	B
et	O
al	O
or	O
variational	B
bayes	I
et	O
al	O
non-convex	O
regularizers	O
although	O
the	O
laplace	B
prior	O
results	O
in	O
a	O
convex	B
optimization	B
problem	O
from	O
a	O
statistical	O
point	O
of	O
view	O
this	O
prior	O
is	O
not	O
ideal	O
there	O
are	O
two	O
main	O
problems	O
with	O
it	O
first	O
it	O
does	O
not	O
put	O
enough	O
probability	O
mass	O
near	O
so	O
it	O
does	O
not	O
sufficiently	O
suppress	O
noise	O
second	O
it	O
does	O
not	O
put	O
enough	O
probability	O
mass	O
on	O
large	O
values	O
so	O
it	O
causes	O
shrinkage	B
of	O
relevant	O
coefficients	O
corresponding	O
to	O
signal	O
can	O
be	O
seen	O
in	O
figure	O
we	O
see	O
that	O
estimates	O
of	O
large	O
coefficients	O
are	O
significantly	O
smaller	O
than	O
their	O
ml	O
estimates	O
a	O
phenomenon	O
known	O
as	O
bias	B
both	O
problems	O
can	O
be	O
solved	O
by	O
going	O
to	O
more	O
flexible	O
kinds	O
of	O
priors	O
which	O
have	O
a	O
larger	O
spike	O
at	O
and	O
heavier	O
tails	O
even	O
though	O
we	O
cannot	O
find	O
the	O
global	O
optimum	O
anymore	O
these	O
non-convex	O
methods	O
often	O
outperform	O
regularization	B
both	O
in	O
terms	O
of	O
predictive	B
accuracy	O
and	O
in	O
detecting	O
relevant	O
variables	O
and	O
li	O
schniter	O
et	O
al	O
we	O
give	O
some	O
examples	O
below	O
chapter	O
sparse	B
linear	O
models	O
bridge	B
regression	B
a	O
natural	O
generalization	B
of	O
regularization	B
known	O
as	O
bridge	B
regression	B
and	O
friedman	O
has	O
the	O
form	O
w	O
nllw	O
j	O
for	O
b	O
this	O
corresponds	O
to	O
map	O
estimation	O
using	O
a	O
exponential	B
power	I
distribution	I
given	O
by	O
exppowerw	O
a	O
b	O
b	O
exp	O
b	O
a	O
if	O
b	O
we	O
get	O
the	O
gaussian	B
distribution	O
a	O
corresonding	O
to	O
ridge	B
regression	B
if	O
we	O
set	O
b	O
we	O
get	O
the	O
laplace	B
distribution	I
corresponding	O
to	O
lasso	B
if	O
we	O
set	O
b	O
we	O
get	O
regression	B
which	O
is	O
equivalent	O
to	O
best	O
subset	O
selection	O
unfortunately	O
the	O
objective	O
is	O
not	O
convex	B
for	O
b	O
and	O
is	O
not	O
sparsity	B
promoting	O
for	O
b	O
so	O
the	O
norm	O
is	O
the	O
tightest	O
convex	B
approximation	O
to	O
the	O
norm	O
the	O
effect	O
of	O
changing	O
b	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
plot	O
the	O
prior	O
for	O
b	O
b	O
and	O
b	O
we	O
assume	O
pw	O
we	O
also	O
plot	O
the	O
posterior	O
after	O
seeing	O
a	O
single	O
observation	B
y	O
which	O
imposes	O
a	O
single	O
linear	O
constraint	O
of	O
the	O
form	O
y	O
wt	O
x	O
with	O
a	O
certain	O
tolerance	O
controlled	O
by	O
the	O
observation	B
noise	O
to	O
figure	O
we	O
see	O
see	O
that	O
the	O
mode	B
of	O
the	O
laplace	B
is	O
on	O
the	O
vertical	O
axis	O
corresponding	O
to	O
by	O
contrast	O
there	O
are	O
two	O
modes	O
when	O
using	O
b	O
corresponding	O
to	O
two	O
different	O
sparse	B
solutions	O
when	O
using	O
the	O
gaussian	B
the	O
map	B
estimate	I
is	O
not	O
sparse	B
mode	B
does	O
not	O
lie	O
on	O
either	O
of	O
the	O
coordinate	O
axes	O
hierarchical	B
adaptive	B
lasso	B
recall	B
that	O
one	O
of	O
the	O
principal	O
problems	O
with	O
lasso	B
is	O
that	O
it	O
results	O
in	O
biased	O
estimates	O
this	O
is	O
because	O
it	O
needs	O
to	O
use	O
a	O
large	O
value	O
of	O
to	O
squash	O
the	O
irrelevant	O
parameters	O
but	O
this	O
then	O
over-penalizes	O
the	O
relevant	O
parameters	O
it	O
would	O
be	O
better	O
if	O
we	O
could	O
associate	O
a	O
different	O
penalty	O
parameter	B
with	O
each	O
parameter	B
of	O
course	O
it	O
is	O
completely	O
infeasible	O
to	O
tune	O
d	O
parameters	O
by	O
cross	B
validation	I
but	O
this	O
poses	O
no	O
problem	O
to	O
the	O
bayesian	B
we	O
simply	O
make	O
each	O
j	O
have	O
its	O
own	O
private	O
tuning	O
parameter	B
j	O
which	O
are	O
now	O
treated	O
as	O
random	O
variables	O
coming	O
from	O
the	O
conjugate	B
prior	I
j	O
iga	O
b	O
the	O
full	B
model	O
is	O
as	O
follows	O
j	O
iga	O
b	O
j	O
j	O
j	O
wj	O
j	O
n	O
j	O
see	O
figure	O
this	O
has	O
been	O
called	O
the	O
hierarchical	B
adaptive	B
lasso	B
et	O
al	O
also	O
et	O
al	O
cevher	O
armagan	O
et	O
al	O
we	O
can	O
integrate	B
out	I
j	O
which	O
induces	O
a	O
j	O
distribution	O
on	O
wj	O
as	O
before	O
the	O
result	O
is	O
that	O
pwj	O
is	O
now	O
a	O
it	O
turns	O
out	O
that	O
we	O
can	O
fit	O
this	O
model	O
compute	O
a	O
local	O
scaled	O
mixture	B
of	O
laplacians	O
posterior	B
mode	B
using	O
em	B
as	O
we	O
explain	O
below	O
the	O
resulting	O
estimate	O
whal	O
often	O
works	O
non-convex	O
regularizers	O
figure	O
top	O
plot	O
of	O
log	O
prior	O
for	O
three	O
different	O
distributions	O
with	O
unit	O
variance	B
gaussian	B
laplace	B
and	O
exponential	O
power	O
bottom	O
plot	O
of	O
log	O
posterior	O
after	O
observing	O
a	O
single	O
observation	B
corresponding	O
to	O
a	O
single	O
linear	O
constraint	O
the	O
precision	B
of	O
this	O
observation	B
is	O
shown	O
by	O
the	O
diagonal	B
lines	O
in	O
the	O
top	O
figure	O
in	O
the	O
case	O
of	O
the	O
laplace	B
prior	O
the	O
posterior	O
is	O
unimodal	O
and	O
asymmetric	O
in	O
the	O
case	O
of	O
the	O
exponential	O
prior	O
the	O
posterior	O
is	O
bimodal	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
sparsepostplot	O
written	O
by	O
florian	O
steinke	O
in	O
the	O
case	O
of	O
the	O
gaussian	B
prior	O
the	O
posterior	O
is	O
unimodal	O
and	O
symmetric	B
much	O
better	O
than	O
the	O
estimate	O
returned	O
by	O
lasso	B
in	O
the	O
sense	O
that	O
it	O
is	O
more	O
likely	O
to	O
contain	O
zeros	O
in	O
the	O
right	O
places	O
selection	O
consistency	O
and	O
more	O
likely	O
to	O
result	O
in	O
good	O
predictions	O
consistency	O
et	O
al	O
we	O
give	O
an	O
explanation	O
for	O
this	O
behavior	O
in	O
section	O
em	B
for	O
hal	O
since	O
the	O
inverse	B
gamma	B
is	O
conjugate	O
to	O
the	O
laplace	B
we	O
find	O
that	O
the	O
e	B
step	I
for	O
j	O
is	O
given	O
by	O
p	O
jwj	O
iga	O
b	O
the	O
e	B
step	I
for	O
is	O
the	O
same	O
as	O
for	O
vanilla	O
lasso	B
the	O
prior	O
for	O
w	O
has	O
the	O
following	O
form	O
pw	O
exp	O
j	O
j	O
j	O
hence	O
the	O
m	B
step	I
must	O
optimize	O
argmax	O
w	O
log	O
n	O
j	O
j	O
chapter	O
sparse	B
linear	O
models	O
a	O
b	O
hal	O
j	O
j	O
wj	O
d	O
yi	O
xi	O
n	O
a	O
b	O
figure	O
dgm	B
for	O
hierarchical	B
adaptive	B
lasso	B
contours	O
of	O
hierarchical	O
adpative	O
laplace	B
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
normalgammapenaltyplotdemo	O
the	O
expectation	O
is	O
given	O
by	O
e	O
j	O
a	O
b	O
j	O
st	O
j	O
thus	O
the	O
m	B
step	I
becomes	O
a	O
weighted	O
lasso	B
problem	O
argmin	O
w	O
j	O
st	O
j	O
j	O
this	O
is	O
easily	O
solved	O
using	O
standard	O
methods	O
lars	B
note	O
that	O
if	O
the	O
coefficient	O
was	O
estimated	O
to	O
be	O
large	O
in	O
the	O
previous	O
iteration	O
wt	O
j	O
will	O
be	O
small	O
so	O
large	O
coefficients	O
are	O
not	O
penalized	O
heavily	O
conversely	O
small	O
coefficients	O
do	O
get	O
penalized	O
heavily	O
this	O
is	O
the	O
way	O
that	O
the	O
algorithm	O
adapts	O
the	O
penalization	O
strength	O
of	O
each	O
coefficient	O
the	O
result	O
is	O
an	O
estimate	O
that	O
is	O
often	O
much	O
sparser	O
than	O
returned	O
by	O
lasso	B
but	O
also	O
less	O
biased	O
is	O
large	O
then	O
the	O
scaling	O
factor	B
st	O
note	O
that	O
if	O
we	O
seta	O
b	O
and	O
we	O
only	O
perform	O
iteration	O
of	O
em	B
we	O
get	O
a	O
method	O
that	O
is	O
closely	O
related	O
to	O
the	O
adaptive	B
lasso	B
of	O
zou	O
and	O
li	O
this	O
em	B
algorithm	O
is	O
also	O
closely	O
related	O
to	O
some	O
iteratively	O
reweighted	O
methods	O
proposed	O
in	O
the	O
signal	B
processing	I
community	O
and	O
yin	O
candes	O
et	O
al	O
understanding	O
the	O
behavior	O
of	O
hal	O
we	O
can	O
get	O
a	O
better	O
understanding	O
of	O
hal	O
by	O
integrating	O
out	O
j	O
to	O
get	O
the	O
following	O
marginal	B
distribution	I
pwja	O
b	O
a	O
b	O
non-convex	O
regularizers	O
lasso	B
hal	O
p	O
a	O
m	O
w	O
p	O
a	O
m	O
w	O
wmle	O
b	O
b	O
b	O
wmle	O
figure	O
thresholding	O
behavior	O
of	O
hierarchical	O
adaptive	O
laplace	B
normalgammathresholdplotdemo	O
two	O
penalty	O
functions	O
log	O
priors	O
based	O
on	O
figure	O
of	O
et	O
al	O
laplace	B
figure	O
generated	O
by	O
this	O
is	O
an	O
instance	O
of	O
the	O
generalized	B
t	I
distribution	I
and	O
newey	O
armagan	O
et	O
al	O
this	O
is	O
called	O
the	O
double	B
pareto	B
distribution	I
defined	O
as	O
gtw	O
a	O
c	O
q	O
q	O
a	O
acq	O
where	O
c	O
is	O
the	O
scale	O
parameter	B
controls	O
the	O
degree	B
of	O
sparsity	B
and	O
a	O
is	O
related	O
to	O
the	O
degrees	B
of	I
freedom	I
when	O
q	O
and	O
c	O
we	O
recover	O
the	O
standard	O
t	O
distribution	O
when	O
a	O
we	O
recover	O
the	O
exponential	B
power	I
distribution	I
and	O
when	O
q	O
and	O
a	O
we	O
in	O
the	O
context	O
of	O
the	O
current	O
model	O
we	O
see	O
that	O
pwja	O
b	O
get	O
the	O
laplace	B
distribution	I
a	O
ba	O
the	O
resulting	O
penalty	O
term	O
has	O
the	O
form	O
log	O
pwj	O
b	O
const	O
where	O
b	O
are	O
the	O
tuning	O
parameters	O
we	O
plot	O
this	O
penalty	O
in	O
we	O
plot	O
in	O
figure	O
for	O
various	O
values	O
of	O
b	O
compared	O
to	O
the	O
diamond-shaped	O
laplace	B
penalty	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
hal	O
penalty	O
looks	O
more	O
like	O
a	O
star	O
fish	O
it	O
puts	O
much	O
more	O
density	O
along	O
the	O
spines	O
thus	O
enforcing	O
sparsity	B
more	O
aggressively	O
note	O
that	O
this	O
penalty	O
is	O
clearly	O
not	O
convex	B
we	O
can	O
gain	O
further	O
understanding	O
into	O
the	O
behavior	O
of	O
this	O
penalty	O
function	O
by	O
considering	O
applying	O
it	O
to	O
the	O
problem	O
of	O
linear	B
regression	B
with	O
an	O
orthogonal	O
design	B
matrix	I
in	O
this	O
case	O
p	O
j	O
ga	O
j	O
ga	O
ig	O
c	O
pwj	O
p	O
j	O
fixed	O
iga	O
b	O
a	O
ba	O
gaa	O
b	O
fixed	O
fixed	O
c	O
b	O
nega	O
b	O
ng	O
njwj	O
t	O
horseshoeb	O
chapter	O
sparse	B
linear	O
models	O
ref	O
and	O
mallows	O
west	O
et	O
al	O
cevher	O
armagan	O
et	O
al	O
and	O
brown	O
chen	O
et	O
al	O
and	O
brown	O
and	O
mallows	O
west	O
et	O
al	O
table	O
some	O
scale	O
mixtures	O
of	O
gaussians	O
abbreviations	O
c	O
half-rectified	O
cauchy	B
ga	O
gamma	B
and	O
rate	B
parameterization	O
gt	O
generalized	O
t	O
ig	O
inverse	B
gamma	B
neg	O
normal-exponentialgamma	O
ng	O
normal-gamma	O
nj	O
normal-jeffreys	O
the	O
horseshoe	O
distribution	O
is	O
the	O
name	O
we	O
give	O
to	O
the	O
distribution	O
induced	O
on	O
wj	O
by	O
the	O
prior	O
described	O
in	O
et	O
al	O
this	O
has	O
no	O
simple	O
analytic	O
form	O
the	O
definitions	O
of	O
the	O
neg	O
and	O
ng	O
densities	O
are	O
a	O
bit	O
complicated	O
but	O
can	O
be	O
found	O
in	O
the	O
references	O
the	O
other	O
distributions	O
are	O
defined	O
in	O
the	O
text	O
one	O
can	O
show	O
that	O
the	O
objective	O
becomes	O
jw	O
j	O
wmle	O
where	O
wmle	O
xt	O
y	O
is	O
the	O
mle	B
and	O
y	O
x	O
wmle	O
thus	O
we	O
can	O
compute	O
the	O
map	B
estimate	I
one	O
dimension	O
at	O
a	O
time	O
by	O
solving	O
the	O
following	O
optimization	B
problem	O
wj	O
argmin	O
wj	O
j	O
wmle	O
in	O
figure	O
we	O
plot	O
the	O
lasso	B
estimate	O
vs	O
the	O
ml	O
estimate	O
wmle	O
we	O
see	O
that	O
the	O
estimator	B
has	O
the	O
usual	O
soft-thresholding	O
behavior	O
seen	O
earlier	O
in	O
figure	O
however	O
this	O
behavior	O
is	O
undesirable	O
since	O
the	O
large	O
magnitude	O
coefficients	O
are	O
also	O
shrunk	O
towards	O
whereas	O
we	O
would	O
like	O
them	O
to	O
be	O
equal	O
to	O
their	O
unshrunken	O
ml	O
estimates	O
in	O
figure	O
we	O
plot	O
the	O
hal	O
estimate	O
whal	O
vs	O
the	O
ml	O
estimate	O
wmle	O
we	O
see	O
that	O
this	O
approximates	O
the	O
more	O
desirable	O
hard	B
thresholding	I
behavior	O
seen	O
earlier	O
in	O
figure	O
much	O
more	O
closely	O
other	O
hierarchical	O
priors	O
many	O
other	O
hierarchical	O
sparsity-promoting	O
priors	O
have	O
been	O
proposed	O
see	O
table	O
for	O
a	O
brief	O
in	O
some	O
cases	O
we	O
can	O
analytically	O
derive	O
the	O
form	O
of	O
the	O
marginal	O
prior	O
for	O
wj	O
summary	O
generally	O
speaking	O
this	O
prior	O
is	O
not	O
concave	B
a	O
particularly	O
interesting	O
prior	O
is	O
the	O
improper	O
normal-jeffreys	O
prior	O
which	O
has	O
been	O
used	O
j	O
in	O
this	O
puts	O
a	O
non-informative	B
jeffreys	B
prior	I
on	O
the	O
variance	B
ga	O
automatic	B
relevance	I
determination	I
bayesian	B
learning	B
j	O
the	O
resulting	O
marginal	O
has	O
the	O
form	O
pwj	O
wj	O
this	O
gives	O
rise	O
to	O
a	O
thresholding	O
rule	O
that	O
looks	O
very	O
similar	B
to	O
hal	O
in	O
figure	O
which	O
in	O
turn	O
is	O
very	O
similar	B
to	O
hard	B
thresholding	I
however	O
this	O
prior	O
has	O
no	O
free	O
parameters	O
which	O
is	O
both	O
a	O
good	O
thing	O
to	O
tune	O
and	O
a	O
bad	O
thing	O
ability	O
to	O
adapt	O
the	O
level	O
of	O
sparsity	B
automatic	B
relevance	I
determination	I
bayesian	B
learning	B
all	O
the	O
methods	O
we	O
have	O
considered	O
so	O
far	O
for	O
the	O
spike-and-slab	O
methods	O
in	O
section	O
have	O
used	O
a	O
factorial	B
prior	I
of	O
the	O
form	O
pw	O
j	O
pwj	O
we	O
have	O
seen	O
how	O
these	O
priors	O
can	O
be	O
represented	O
in	O
terms	O
of	O
gaussian	B
scale	O
mixtures	O
of	O
the	O
form	O
wj	O
n	O
j	O
where	O
j	O
has	O
one	O
of	O
the	O
priors	O
listed	O
in	O
table	O
using	O
these	O
latent	B
variances	O
we	O
can	O
represent	O
the	O
j	O
wj	O
y	O
x	O
we	O
can	O
then	O
use	O
em	B
to	O
perform	O
map	O
estimation	O
model	O
in	O
the	O
form	O
j	O
and	O
in	O
the	O
m	B
step	I
we	O
estimate	O
w	O
from	O
y	O
x	O
and	O
where	O
in	O
the	O
e	B
step	I
we	O
inferp	O
this	O
m	B
step	I
either	O
involves	O
a	O
closed-form	O
weighted	O
optimization	B
the	O
case	O
of	O
gaussian	B
scale	O
mixtures	O
or	O
a	O
weighted	O
optimization	B
the	O
case	O
of	O
laplacian	O
scale	O
mixtures	O
we	O
also	O
discussed	O
how	O
to	O
perform	O
bayesian	B
inference	B
in	O
such	O
models	O
rather	O
than	O
just	O
computing	O
map	O
estimates	O
in	O
this	O
section	O
we	O
discuss	O
an	O
alternative	O
approach	O
based	O
on	O
type	O
ii	O
ml	O
estimation	O
bayes	O
whereby	O
we	O
integrate	B
out	I
w	O
and	O
maximize	O
the	O
marginal	B
likelihood	B
wrt	O
this	O
eb	B
procedure	O
can	O
be	O
implemented	O
via	O
em	B
or	O
via	O
a	O
reweighted	O
scheme	O
as	O
we	O
will	O
explain	O
below	O
having	O
estimated	O
the	O
variances	O
we	O
plug	O
them	O
in	O
to	O
compute	O
the	O
posterior	B
mean	B
of	O
the	O
weights	O
e	O
rather	O
surprisingly	O
view	O
of	O
the	O
gaussian	B
prior	O
the	O
result	O
is	O
an	O
sparse	B
estimate	O
for	O
reasons	O
we	O
explain	O
below	O
in	O
the	O
context	O
of	O
neural	B
networks	I
this	O
this	O
method	O
is	O
called	O
called	O
automatic	B
relevance	I
determination	I
or	O
ard	B
neal	O
see	O
section	O
in	O
the	O
context	O
of	O
the	O
linear	O
models	O
we	O
are	O
considering	O
in	O
this	O
chapter	O
this	O
method	O
is	O
called	O
sparse	B
bayesian	B
learning	B
or	O
sbl	B
combining	O
ardsbl	O
with	O
basis	B
function	I
expansion	I
in	O
a	O
linear	O
model	O
gives	O
rise	O
to	O
a	O
technique	O
called	O
the	O
relevance	B
vector	I
machine	I
which	O
we	O
will	O
discuss	O
in	O
section	O
ard	B
for	O
linear	B
regression	B
we	O
will	O
explain	O
the	O
procedure	O
in	O
the	O
context	O
of	O
linear	B
regression	B
ard	B
for	O
glms	O
requires	O
the	O
use	O
of	O
the	O
laplace	B
some	O
other	O
approximation	O
case	O
can	O
be	O
it	O
is	O
conventional	O
when	O
discussing	O
ard	B
sbl	B
to	O
denote	O
the	O
weight	O
precisions	O
by	O
j	O
j	O
and	O
the	O
measurement	O
precision	B
by	O
not	O
confuse	O
this	O
with	O
the	O
use	O
of	O
in	O
statistics	O
to	O
represent	O
the	O
regression	B
coefficients	O
in	O
particular	O
we	O
will	O
assume	O
the	O
following	O
model	O
pyx	O
w	O
x	O
pw	O
a	O
chapter	O
sparse	B
linear	O
models	O
where	O
a	O
diag	O
the	O
marginal	B
likelihood	B
can	O
be	O
computed	O
analytically	O
as	O
follows	O
pyx	O
n	O
in	O
adw	O
n	O
in	O
xa	O
exp	O
yt	O
c	O
y	O
where	O
c	O
xa	O
compare	O
this	O
to	O
the	O
marginal	B
likelihood	B
in	O
equation	O
in	O
the	O
spike	B
and	I
slab	I
model	O
modulo	O
the	O
factor	B
missing	B
from	O
the	O
second	O
term	O
the	O
equations	O
are	O
the	O
same	O
except	O
we	O
have	O
replaced	O
the	O
binary	O
j	O
with	O
continuous	O
j	O
r	O
in	O
log	O
form	O
the	O
objective	O
becomes	O
log	O
pyx	O
log	O
yt	O
c	O
y	O
to	O
regularize	O
the	O
problem	O
we	O
may	O
put	O
a	O
conjugate	B
prior	I
on	O
each	O
precision	B
j	O
gaa	O
b	O
and	O
gac	O
d	O
the	O
modified	O
objective	O
becomes	O
log	O
pyx	O
log	O
yt	O
c	O
y	O
log	O
ga	O
ja	O
b	O
log	O
ga	O
d	O
log	O
j	O
b	O
j	O
log	O
d	O
j	O
j	O
this	O
is	O
useful	O
when	O
performing	O
bayesian	B
inference	B
for	O
and	O
and	O
tipping	O
however	O
when	O
performing	O
ii	O
point	O
estimation	O
we	O
will	O
use	O
the	O
improper	B
prior	I
a	O
b	O
c	O
d	O
which	O
results	O
in	O
maximal	O
sparsity	B
below	O
we	O
describe	O
how	O
to	O
optimize	O
wrt	O
the	O
precision	B
terms	O
and	O
this	O
is	O
a	O
proxy	O
for	O
finding	O
the	O
most	O
probable	O
model	O
setting	O
of	O
in	O
the	O
spike	B
and	I
slab	I
model	O
which	O
in	O
turn	O
is	O
closely	O
related	O
to	O
regularization	B
in	O
particular	O
it	O
can	O
be	O
shown	O
et	O
al	O
that	O
the	O
objective	O
in	O
equation	O
has	O
many	O
fewer	O
local	O
optima	O
than	O
the	O
objective	O
and	O
hence	O
is	O
much	O
easier	O
to	O
optimize	O
once	O
we	O
have	O
estimated	O
and	O
we	O
can	O
compute	O
the	O
posterior	O
over	O
the	O
parameters	O
using	O
pwd	O
n	O
xt	O
x	O
a	O
xt	O
y	O
the	O
fact	O
that	O
we	O
compute	O
a	O
posterior	O
over	O
w	O
while	O
simultaneously	O
encouraging	O
sparsity	B
is	O
why	O
the	O
method	O
is	O
called	O
sparse	B
bayesian	B
learning	B
nevertheless	O
since	O
there	O
are	O
many	O
ways	O
to	O
be	O
sparse	B
and	O
bayesian	B
we	O
will	O
use	O
the	O
ard	B
term	O
instead	O
even	O
in	O
the	O
linear	O
model	O
context	O
addition	O
sbl	B
is	O
only	O
being	O
bayesian	B
about	O
the	O
values	O
of	O
the	O
coefficients	O
rather	O
than	O
reflecting	O
uncertainty	B
about	O
the	O
set	O
of	O
relevant	O
variables	O
which	O
is	O
typically	O
of	O
more	O
interest	O
an	O
alternative	O
approach	O
to	O
optimizing	O
is	O
to	O
put	O
a	O
gamma	B
prior	O
on	O
and	O
to	O
integrate	O
it	O
out	O
to	O
get	O
a	O
student	O
posterior	O
for	O
w	O
and	O
weigend	O
however	O
it	O
turns	O
out	O
that	O
this	O
results	O
in	O
a	O
less	O
accurate	O
estimate	O
for	O
in	O
addition	O
working	O
with	O
gaussians	O
is	O
easier	O
than	O
working	O
with	O
the	O
student	O
distribution	O
and	O
the	O
gaussian	B
case	O
generalizes	O
more	O
easily	O
to	O
other	O
cases	O
such	O
as	O
logistic	B
regression	B
automatic	B
relevance	I
determination	I
bayesian	B
learning	B
x	O
y	O
c	O
y	O
figure	O
illustration	O
of	O
why	O
ard	B
results	O
in	O
sparsity	B
the	O
vector	O
of	O
inputs	O
x	O
does	O
not	O
point	O
towards	O
the	O
vector	O
of	O
outputs	O
y	O
so	O
the	O
feature	O
should	O
be	O
removed	O
for	O
finite	O
the	O
probability	O
density	O
is	O
spread	O
in	O
directions	O
away	O
from	O
y	O
when	O
the	O
probability	O
density	O
at	O
y	O
is	O
maximized	O
based	O
on	O
figure	O
of	O
whence	O
sparsity	B
j	O
if	O
j	O
we	O
find	O
wj	O
wmle	O
since	O
the	O
gaussian	B
prior	O
shrinking	O
wj	O
towards	O
has	O
zero	O
precision	B
however	O
if	O
we	O
find	O
that	O
j	O
then	O
the	O
prior	O
is	O
very	O
confident	O
that	O
wj	O
and	O
hence	O
that	O
feature	O
j	O
is	O
irrelevant	O
hence	O
the	O
posterior	B
mean	B
will	O
have	O
wj	O
thus	O
irrelevant	O
features	B
automatically	O
have	O
their	O
weights	O
turned	O
off	O
or	O
pruned	O
out	O
we	O
now	O
give	O
an	O
intuitive	O
argument	O
based	O
on	O
about	O
why	O
ml-ii	O
should	O
encourage	O
j	O
for	O
irrelevant	O
features	B
consider	O
a	O
linear	B
regression	B
with	O
training	O
examples	O
so	O
x	O
x	O
and	O
y	O
we	O
can	O
plot	O
x	O
and	O
y	O
as	O
vectors	O
in	O
the	O
plane	O
as	O
shown	O
in	O
figure	O
suppose	O
the	O
feature	O
is	O
irrelevant	O
for	O
predicting	O
the	O
response	O
so	O
x	O
points	O
in	O
a	O
nearly	O
orthogonal	O
direction	O
to	O
y	O
let	O
us	O
see	O
what	O
happens	O
to	O
the	O
marginal	B
likelihood	B
as	O
we	O
change	O
the	O
marginal	B
likelihood	B
is	O
given	O
by	O
pyx	O
n	O
c	O
where	O
c	O
i	O
xxt	O
if	O
is	O
finite	O
the	O
posterior	O
will	O
be	O
elongated	O
along	O
the	O
direction	O
of	O
x	O
as	O
in	O
figure	O
however	O
if	O
we	O
find	O
c	O
if	O
is	O
held	O
constant	O
the	O
latter	O
assigns	O
higher	O
probability	O
density	O
to	O
the	O
observed	O
response	O
vector	O
y	O
so	O
this	O
is	O
the	O
preferred	O
solution	O
in	O
other	O
words	O
the	O
marginal	B
likelihood	B
punishes	O
solutions	O
where	O
j	O
is	O
small	O
but	O
xj	O
is	O
irrelevant	O
since	O
these	O
waste	O
probability	O
mass	O
it	O
is	O
more	O
parsimonious	O
the	O
point	O
of	O
view	O
of	O
bayesian	B
occam	O
s	O
razor	O
to	O
eliminate	O
redundant	O
dimensions	O
i	O
so	O
c	O
is	O
spherical	B
as	O
in	O
figure	O
connection	O
to	O
map	O
estimation	O
ard	B
seems	O
quite	O
different	O
from	O
the	O
map	O
estimation	O
methods	O
we	O
have	O
been	O
considering	O
earlier	O
in	O
this	O
chapter	O
in	O
particular	O
in	O
ard	B
we	O
are	O
not	O
integrating	O
out	O
and	O
optimizing	O
w	O
but	O
vice	O
chapter	O
sparse	B
linear	O
models	O
versa	O
because	O
the	O
parameters	O
wj	O
become	O
correlated	O
in	O
the	O
posterior	O
to	O
explaining	B
away	I
when	O
we	O
estimate	O
j	O
we	O
are	O
borrowing	O
information	B
from	O
all	O
the	O
features	B
not	O
just	O
feature	O
j	O
consequently	O
the	O
effective	O
prior	O
pw	O
is	O
non-factorial	B
and	O
furthermore	O
it	O
depends	O
on	O
the	O
data	O
d	O
however	O
in	O
and	O
nagarajan	O
it	O
was	O
shown	O
that	O
ard	B
can	O
be	O
viewed	O
as	O
the	O
following	O
map	O
estimation	O
problem	O
gardw	O
j	O
log	O
ward	O
arg	O
min	O
w	O
gardw	O
min	O
j	O
the	O
proof	O
which	O
is	O
based	O
on	O
convex	B
analysis	O
is	O
a	O
little	O
complicated	O
and	O
hence	O
is	O
omitted	O
furthermore	O
and	O
nagarajan	O
wipf	O
et	O
al	O
prove	O
that	O
map	O
estimation	O
with	O
non-factorial	B
priors	O
is	O
strictly	O
better	O
than	O
map	O
estimation	O
with	O
any	O
possible	O
factorial	B
prior	I
in	O
the	O
following	O
sense	O
the	O
non-factorial	B
objective	O
always	O
has	O
fewer	O
local	O
minima	O
than	O
factorial	O
objectives	O
while	O
still	O
satisfying	O
the	O
property	O
that	O
the	O
global	O
optimum	O
of	O
the	O
non-factorial	B
objective	O
corresponds	O
to	O
the	O
global	O
optimum	O
of	O
the	O
objective	O
a	O
property	O
that	O
regularization	B
which	O
has	O
no	O
local	O
minima	O
does	O
not	O
enjoy	O
algorithms	O
for	O
ard	B
in	O
this	O
section	O
we	O
review	O
several	O
different	O
algorithms	O
for	O
implementing	O
ard	B
em	B
algorithm	O
the	O
easiest	O
way	O
to	O
implement	O
sblard	O
is	O
to	O
use	O
em	B
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
is	O
given	O
by	O
log	O
n	O
log	O
n	O
a	O
n	O
log	O
e	O
log	O
j	O
trawwt	O
const	O
q	O
x	O
trxt	O
x	O
j	O
n	O
log	O
log	O
j	O
j	O
tra	O
t	O
const	O
where	O
and	O
are	O
computed	O
in	O
the	O
e	B
step	I
using	O
equation	O
suppose	O
we	O
put	O
a	O
gaa	O
b	O
prior	O
on	O
j	O
and	O
a	O
gac	O
d	O
prior	O
on	O
the	O
penalized	O
objective	O
becomes	O
q	O
log	O
j	O
b	O
j	O
log	O
d	O
j	O
setting	O
d	O
j	O
j	O
e	O
we	O
get	O
the	O
following	O
m	B
step	I
j	O
j	O
jj	O
automatic	B
relevance	I
determination	I
bayesian	B
learning	B
if	O
j	O
and	O
a	O
b	O
the	O
update	O
becomes	O
d	O
e	O
w	O
d	O
t	O
tr	O
the	O
update	O
for	O
is	O
given	O
by	O
x	O
j	O
jj	O
new	O
n	O
this	O
is	O
exercise	O
fixed-point	O
algorithm	O
a	O
faster	O
and	O
more	O
direct	O
approach	O
is	O
to	O
directly	O
optimize	O
the	O
objective	O
in	O
equation	O
one	O
can	O
show	O
that	O
the	O
equations	O
d	O
lead	O
to	O
the	O
following	O
fixed	O
d	O
j	O
point	O
updates	O
and	O
j	O
j	O
j	O
n	O
x	O
j	O
j	O
j	O
j	O
jj	O
the	O
quantity	O
j	O
is	O
a	O
measure	O
of	O
how	O
well-determined	O
wj	O
is	O
by	O
the	O
data	O
hence	O
j	O
j	O
is	O
the	O
effective	O
degrees	B
of	I
freedom	I
of	O
the	O
model	O
see	O
section	O
for	O
further	O
discussion	O
since	O
and	O
both	O
depend	O
on	O
and	O
can	O
be	O
computed	O
using	O
equation	O
or	O
the	O
laplace	B
approximation	I
we	O
need	O
to	O
re-estimate	O
these	O
equations	O
until	O
convergence	O
properties	O
of	O
this	O
algorithm	O
have	O
been	O
studied	O
in	O
and	O
nagarajan	O
at	O
convergence	O
the	O
results	O
are	O
formally	O
identical	O
to	O
those	O
obtained	O
by	O
em	B
but	O
since	O
the	O
objective	O
is	O
non-convex	O
the	O
results	O
can	O
depend	O
on	O
the	O
initial	O
values	O
iteratively	O
reweighted	O
algorithm	O
another	O
approach	O
to	O
solving	O
the	O
ard	B
problem	O
is	O
based	O
on	O
the	O
view	O
that	O
it	O
is	O
a	O
map	O
estimation	O
problem	O
although	O
the	O
log	O
prior	O
gw	O
is	O
rather	O
complex	O
in	O
form	O
it	O
can	O
be	O
shown	O
to	O
be	O
a	O
non-decreasing	O
concave	B
function	O
of	O
this	O
means	O
that	O
it	O
can	O
be	O
solved	O
by	O
an	O
iteratively	O
reweighted	O
problem	O
of	O
the	O
form	O
arg	O
min	O
w	O
nllw	O
j	O
j	O
in	O
and	O
nagarajan	O
the	O
following	O
procedure	O
for	O
setting	O
the	O
penalty	O
terms	O
is	O
suggested	O
on	O
a	O
convex	B
bound	O
to	O
the	O
penalty	O
function	O
we	O
initialize	O
with	O
j	O
and	O
then	O
at	O
chapter	O
sparse	B
linear	O
models	O
iteration	O
t	O
compute	O
j	O
by	O
iterating	O
the	O
following	O
equation	O
a	O
few	O
j	O
xj	O
j	O
xt	O
d	O
n	O
we	O
see	O
that	O
the	O
new	O
penalty	O
j	O
depends	O
on	O
all	O
the	O
old	O
weights	O
this	O
is	O
quite	O
different	O
from	O
the	O
adaptive	B
lasso	B
method	O
of	O
section	O
to	O
understand	O
this	O
difference	O
consider	O
the	O
noiseless	O
case	O
where	O
and	O
assume	O
d	O
n	O
solutions	O
which	O
perfectly	O
reconstruct	O
the	O
data	O
xw	O
y	O
and	O
which	O
in	O
this	O
case	O
there	O
are	O
have	O
sparsity	B
n	O
these	O
are	O
called	O
basic	O
feasible	O
solutions	O
or	O
bfs	O
what	O
we	O
want	O
are	O
solutions	O
that	O
satsify	O
xw	O
y	O
but	O
which	O
are	O
much	O
sparser	O
than	O
this	O
suppose	O
the	O
method	O
has	O
found	O
a	O
bfs	O
we	O
do	O
not	O
want	O
to	O
increase	O
the	O
penalty	O
on	O
a	O
weight	O
just	O
because	O
it	O
is	O
small	O
in	O
adaptive	B
lasso	B
since	O
that	O
will	O
just	O
reinforce	O
our	O
current	O
local	O
optimum	O
instead	O
we	O
want	O
to	O
increase	O
the	O
penalty	O
on	O
a	O
weight	O
if	O
it	O
is	O
small	O
and	O
if	O
we	O
have	O
n	O
the	O
covariance	B
term	O
has	O
this	O
effect	O
if	O
w	O
is	O
a	O
bfs	O
this	O
matrix	O
will	O
be	O
full	B
rank	O
so	O
the	O
penalty	O
will	O
not	O
increase	O
much	O
but	O
if	O
w	O
is	O
sparser	O
than	O
n	O
the	O
matrix	O
will	O
not	O
be	O
full	B
rank	O
so	O
the	O
penalties	O
associated	O
with	O
zero-valued	O
coefficients	O
will	O
increase	O
thus	O
reinforcing	O
this	O
solution	O
and	O
nagarajan	O
j	O
ard	B
for	O
logistic	B
regression	B
now	O
consider	O
binary	O
logistic	B
regression	B
pyx	O
w	O
berysigmwt	O
x	O
using	O
the	O
same	O
gaussian	B
prior	O
pw	O
a	O
we	O
can	O
no	O
longer	O
use	O
em	B
to	O
estimate	O
since	O
the	O
gaussian	B
prior	O
is	O
not	O
conjugate	O
to	O
the	O
logistic	B
likelihood	B
so	O
the	O
e	B
step	I
cannot	O
be	O
done	O
exactly	O
one	O
approach	O
is	O
to	O
use	O
a	O
variational	O
approximation	O
to	O
the	O
e	B
step	I
as	O
discussed	O
in	O
section	O
a	O
simpler	O
approach	O
is	O
to	O
use	O
a	O
laplace	B
approximation	I
section	O
in	O
the	O
e	B
step	I
we	O
can	O
then	O
use	O
this	O
approximation	O
inside	O
the	O
same	O
em	B
procedure	O
as	O
before	O
except	O
we	O
no	O
longer	O
need	O
to	O
update	O
note	O
however	O
that	O
this	O
is	O
not	O
guaranteed	O
to	O
converge	B
an	O
alternative	O
is	O
to	O
use	O
the	O
techniques	O
from	O
section	O
in	O
this	O
case	O
we	O
can	O
use	O
exact	O
methods	O
to	O
compute	O
the	O
inner	O
weighted	O
regularized	O
logistic	B
regression	B
problem	O
and	O
no	O
approximations	O
are	O
required	O
sparse	B
coding	I
so	O
far	O
we	O
have	O
been	O
concentrating	O
on	O
sparse	B
priors	O
for	O
supervised	B
learning	B
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
use	O
them	O
for	O
unsupervised	B
learning	B
in	O
section	O
we	O
discussed	O
ica	B
which	O
is	O
like	O
pca	B
except	O
it	O
uses	O
a	O
non-gaussian	O
prior	O
for	O
the	O
latent	B
factors	B
zi	O
if	O
we	O
make	O
the	O
non-gaussian	O
prior	O
be	O
sparsity	B
promoting	O
such	O
as	O
a	O
laplace	B
distribution	I
we	O
will	O
be	O
approximating	O
each	O
observed	O
vector	O
xi	O
as	O
a	O
sparse	B
combination	O
of	O
basis	O
vectors	O
of	O
w	O
note	O
that	O
the	O
sparsity	B
pattern	B
by	O
zi	O
changes	O
from	O
data	O
case	O
to	O
data	O
case	O
if	O
we	O
relax	O
the	O
constraint	O
that	O
w	O
is	O
orthogonal	O
we	O
get	O
a	O
method	O
called	O
the	O
algorithm	O
in	O
and	O
nagarajan	O
is	O
equivalent	O
to	O
a	O
single	O
iteration	O
of	O
equation	O
however	O
since	O
the	O
equation	O
is	O
cheap	O
to	O
compute	O
on	O
time	O
it	O
is	O
worth	O
iterating	O
a	O
few	O
times	O
before	O
solving	O
the	O
more	O
expensive	O
problem	O
sparse	B
coding	I
method	O
pca	B
fa	B
ica	B
sparse	B
coding	I
sparse	B
pca	B
sparse	B
mf	O
pzi	O
gauss	O
gauss	O
non-gauss	O
laplace	B
gauss	O
laplace	B
pw	O
w	O
orthogonal	O
laplace	B
maybe	O
laplace	B
yes	O
no	O
yes	O
no	O
no	O
table	O
summary	O
of	O
various	O
latent	B
factor	B
models	O
a	O
dash	O
in	O
the	O
pw	O
column	O
means	O
we	O
are	O
performing	O
ml	O
parameter	B
estimation	O
rather	O
than	O
map	O
parameter	B
estimation	O
summary	O
of	O
abbreviations	O
pca	B
principal	B
components	I
analysis	I
fa	B
factor	B
analysis	I
ica	B
independent	O
components	O
analysis	O
mf	O
matrix	B
factorization	I
sparse	B
coding	I
in	O
this	O
context	O
we	O
call	O
the	O
factor	B
loading	I
matrix	I
w	O
a	O
dictionary	B
each	O
column	O
is	O
referred	O
to	O
as	O
an	O
in	O
view	O
of	O
the	O
sparse	B
representation	I
it	O
is	O
common	O
for	O
l	O
d	O
in	O
which	O
case	O
we	O
call	O
the	O
representation	O
overcomplete	B
in	O
sparse	B
coding	I
the	O
dictionary	B
can	O
be	O
fixed	O
or	O
learned	O
if	O
it	O
is	O
fixed	O
it	O
is	O
common	O
to	O
use	O
a	O
wavelet	B
or	O
dct	B
basis	O
since	O
many	O
natural	O
signals	O
can	O
be	O
well	O
approximated	O
by	O
a	O
small	O
number	O
of	O
such	O
basis	B
functions	I
however	O
it	O
is	O
also	O
possible	O
to	O
learn	O
the	O
dictionary	B
by	O
maximizing	O
the	O
likelihood	B
log	O
pdw	O
n	O
log	O
zi	O
we	O
discuss	O
ways	O
to	O
optimize	O
this	O
below	O
and	O
then	O
we	O
present	O
several	O
interesting	O
applications	O
do	O
not	O
confuse	O
sparse	B
coding	I
with	O
sparse	B
pca	B
e	O
g	O
et	O
al	O
journee	O
et	O
al	O
this	O
puts	O
a	O
sparsity	B
promoting	O
prior	O
on	O
the	O
regression	B
weights	O
w	O
whereas	O
in	O
sparse	B
coding	I
we	O
put	O
a	O
sparsity	B
promoting	O
prior	O
on	O
the	O
latent	B
factors	B
zi	O
of	O
course	O
the	O
two	O
techniques	O
can	O
be	O
combined	O
we	O
call	O
the	O
result	O
sparse	B
matrix	B
factorization	I
although	O
this	O
term	O
is	O
nonstandard	O
see	O
table	O
for	O
a	O
summary	O
of	O
our	O
terminology	O
learning	B
a	O
sparse	B
coding	I
dictionary	B
since	O
equation	O
is	O
a	O
hard	O
objective	O
to	O
maximize	O
approximation	O
it	O
is	O
common	O
to	O
make	O
the	O
following	O
log	O
pdw	O
max	O
zi	O
log	O
n	O
log	O
pzi	O
if	O
pzi	O
is	O
laplace	B
we	O
can	O
rewrite	O
the	O
nll	B
as	O
nllw	O
z	O
it	O
is	O
common	O
to	O
denote	O
the	O
dictionary	B
by	O
d	O
and	O
to	O
denote	O
the	O
latent	B
factors	B
by	O
i	O
however	O
we	O
will	O
stick	O
with	O
the	O
w	O
and	O
zi	O
notation	O
chapter	O
sparse	B
linear	O
models	O
to	O
prevent	O
w	O
from	O
becoming	O
arbitrarily	O
large	O
it	O
is	O
common	O
to	O
constrain	O
the	O
norm	O
of	O
its	O
columns	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
let	O
us	O
denote	O
this	O
constraint	O
set	O
by	O
c	O
r	O
d	O
l	O
s	O
t	O
wt	O
j	O
wj	O
then	O
we	O
want	O
to	O
solve	O
minw	O
cz	O
rn	O
l	O
nllw	O
z	O
for	O
a	O
fixed	O
zi	O
the	O
optimization	B
over	O
w	O
is	O
a	O
simple	O
least	B
squares	I
problem	O
and	O
for	O
a	O
fixed	O
dictionary	B
w	O
the	O
optimization	B
problem	O
over	O
z	O
is	O
identical	O
to	O
the	O
lasso	B
problem	O
for	O
which	O
many	O
fast	O
algorithms	O
exist	O
this	O
suggests	O
an	O
obvious	O
iterative	O
optimization	B
scheme	O
in	O
which	O
we	O
alternate	O
between	O
optimizing	O
w	O
and	O
z	O
called	O
this	O
kind	O
of	O
approach	O
an	O
analysis-synthesis	B
loop	B
where	O
estimating	O
the	O
basis	O
w	O
is	O
the	O
analysis	O
phase	B
and	O
estimating	O
the	O
coefficients	O
z	O
is	O
the	O
synthesis	O
phase	B
in	O
cases	O
where	O
this	O
is	O
too	O
slow	O
more	O
sophisticated	O
algorithms	O
can	O
be	O
used	O
see	O
e	O
g	O
et	O
al	O
a	O
variety	O
of	O
other	O
models	O
result	O
in	O
an	O
optimization	B
problem	O
that	O
is	O
similar	B
to	O
equation	O
for	O
example	O
non-negative	B
matrix	B
factorization	I
or	O
nmf	B
and	O
tapper	O
lee	O
and	O
seung	O
requires	O
solving	O
an	O
objective	O
of	O
the	O
form	O
s	O
t	O
w	O
zi	O
min	O
w	O
cz	O
rl	O
n	O
that	O
this	O
has	O
no	O
hyper-parameters	B
to	O
tune	O
the	O
intuition	O
behind	O
this	O
constraint	O
is	O
that	O
the	O
learned	O
dictionary	B
may	O
be	O
more	O
interpretable	O
if	O
it	O
is	O
a	O
positive	O
sum	O
of	O
positive	O
parts	O
rather	O
than	O
a	O
sparse	B
sum	O
of	O
atoms	O
that	O
may	O
be	O
positive	O
or	O
negative	O
of	O
course	O
we	O
can	O
combine	O
nmf	B
with	O
a	O
sparsity	B
promoting	O
prior	O
on	O
the	O
latent	B
factors	B
this	O
is	O
called	O
non-negative	B
sparse	B
coding	I
alternatively	O
we	O
can	O
drop	O
the	O
positivity	O
constraint	O
but	O
impose	O
a	O
sparsity	B
constraint	O
on	O
both	O
the	O
factors	B
zi	O
and	O
the	O
dictionary	B
w	O
we	O
call	O
this	O
sparse	B
matrix	B
factorization	I
to	O
ensure	O
strict	B
convexity	O
we	O
can	O
use	O
an	O
elastic	B
net	I
type	O
penalty	O
on	O
the	O
weights	O
et	O
al	O
resulting	O
in	O
s	O
t	O
min	O
wz	O
there	O
are	O
several	O
related	O
objectives	O
one	O
can	O
write	O
down	O
for	O
example	O
we	O
can	O
replace	O
the	O
lasso	B
nll	B
with	O
group	B
lasso	B
or	O
fused	B
lasso	B
et	O
al	O
we	O
can	O
also	O
use	O
other	O
sparsity-promoting	O
priors	O
besides	O
the	O
laplace	B
for	O
example	O
et	O
al	O
propose	B
a	O
model	O
in	O
which	O
the	O
latent	B
factors	B
zi	O
are	O
made	O
sparse	B
using	O
the	O
binary	B
mask	I
model	O
of	O
section	O
each	O
bit	O
of	O
the	O
mask	O
can	O
be	O
generated	O
from	O
a	O
bernoulli	B
distribution	O
with	O
parameter	B
which	O
can	O
be	O
drawn	O
from	O
a	O
beta	B
distribution	I
alternatively	O
we	O
can	O
use	O
a	O
non-parametric	B
prior	I
such	O
as	O
the	O
beta	B
process	I
this	O
allows	O
the	O
model	O
to	O
use	O
dictionaries	O
of	O
unbounded	O
size	O
rather	O
than	O
having	O
to	O
specify	O
l	O
in	O
advance	O
one	O
can	O
perform	O
bayesian	B
inference	B
in	O
this	O
model	O
using	O
e	O
g	O
gibbs	B
sampling	I
or	O
variational	B
bayes	I
one	O
finds	O
that	O
the	O
effective	O
size	O
of	O
the	O
dictionary	B
goes	O
down	O
as	O
the	O
noise	O
level	O
goes	O
up	O
due	O
to	O
the	O
bayesian	B
occam	O
s	O
razor	O
this	O
can	O
prevent	O
overfitting	O
see	O
et	O
al	O
for	O
details	O
results	O
of	O
dictionary	B
learning	B
from	O
image	O
patches	O
one	O
reason	O
that	O
sparse	B
coding	I
has	O
generated	O
so	O
much	O
interest	O
recently	O
is	O
because	O
it	O
explains	O
an	O
interesting	O
phenomenon	O
in	O
neuroscience	O
in	O
particular	O
the	O
dictionary	B
that	O
is	O
learned	O
by	O
applying	O
sparse	B
coding	I
figure	O
illustration	O
of	O
the	O
filters	O
learned	O
by	O
various	O
methods	O
when	O
applied	O
to	O
natural	O
image	O
patches	O
ica	B
figure	O
generated	O
by	O
icabasisdemo	O
patch	O
is	O
first	O
centered	O
and	O
normalized	O
to	O
unit	O
norm	O
kindly	O
provided	O
by	O
aapo	O
hyvarinen	O
sparse	B
pca	B
with	O
low	O
sparsity	B
on	O
weight	O
matrix	O
sparse	B
pca	B
with	O
high	O
sparsity	B
on	O
weight	O
matrix	O
figure	O
generated	O
by	O
sparsedictdemo	O
written	O
by	O
julien	O
mairal	O
pca	B
non-negative	B
matrix	B
factorization	I
sparse	B
coding	I
chapter	O
sparse	B
linear	O
models	O
sparse	B
coding	I
to	O
patches	O
of	O
natural	O
images	O
consists	O
of	O
basis	O
vectors	O
that	O
look	O
like	O
the	O
filters	O
that	O
are	O
found	O
in	O
simple	B
cells	I
in	O
the	O
primary	O
visual	O
cortex	O
of	O
the	O
mammalian	O
brain	O
and	O
field	O
in	O
particular	O
the	O
filters	O
look	O
like	O
bar	O
and	O
edge	O
detectors	O
as	O
shown	O
in	O
figure	O
this	O
example	O
the	O
parameter	B
was	O
chosen	O
so	O
that	O
the	O
number	O
of	O
active	O
basis	B
functions	I
components	O
of	O
zi	O
is	O
about	O
interestingly	O
using	O
ica	B
gives	O
visually	O
similar	B
results	O
as	O
shown	O
in	O
figure	O
by	O
contrast	O
applying	O
pca	B
to	O
the	O
same	O
data	O
results	O
in	O
sinusoidal	O
gratings	O
as	O
shown	O
in	O
figure	O
these	O
do	O
not	O
look	O
like	O
cortical	O
cell	O
response	O
it	O
has	O
therefore	O
been	O
conjectured	O
that	O
parts	O
of	O
the	O
cortex	O
may	O
be	O
performing	O
sparse	B
coding	I
of	O
the	O
sensory	O
input	O
the	O
resulting	O
latent	B
representation	O
is	O
then	O
further	O
processed	O
by	O
higher	O
levels	O
of	O
the	O
brain	O
figure	O
shows	O
the	O
result	O
of	O
using	O
nmf	B
and	O
figure	O
show	O
the	O
results	O
of	O
sparse	B
pca	B
as	O
we	O
increase	O
the	O
sparsity	B
of	O
the	O
basis	O
vectors	O
compressed	B
sensing	I
imagine	O
that	O
instead	O
of	O
observing	O
the	O
data	O
x	O
r	O
although	O
it	O
is	O
interesting	O
to	O
look	O
at	O
the	O
dictionaries	O
learned	O
by	O
sparse	B
coding	I
it	O
is	O
not	O
necessarily	O
very	O
useful	O
however	O
there	O
are	O
some	O
practical	O
applications	O
of	O
sparse	B
coding	I
which	O
we	O
discuss	O
below	O
d	O
we	O
observe	O
a	O
low-dimensional	O
projection	B
m	O
r	O
is	O
a	O
m	O
d	O
matrix	O
m	O
d	O
and	O
is	O
a	O
noise	O
term	O
of	O
it	O
y	O
rx	O
where	O
y	O
r	O
gaussian	B
we	O
assume	O
r	O
is	O
a	O
known	O
sensing	O
matrix	O
corresponding	O
to	O
different	O
linear	O
projections	O
of	O
x	O
for	O
example	O
consider	O
an	O
mri	O
scanner	O
each	O
beam	O
direction	O
corresponds	O
to	O
a	O
vector	O
encoded	O
as	O
a	O
row	O
in	O
r	O
figure	O
illustrates	O
the	O
modeling	O
assumptions	O
our	O
goal	O
is	O
to	O
infer	O
pxy	O
r	O
how	O
can	O
we	O
hope	O
to	O
recover	O
all	O
of	O
x	O
if	O
we	O
do	O
not	O
measure	O
all	O
of	O
x	O
the	O
answer	O
is	O
we	O
can	O
use	O
bayesian	B
inference	B
with	O
an	O
appropriate	O
prior	O
that	O
exploits	O
the	O
fact	O
that	O
natural	O
signals	O
can	O
be	O
expressed	O
as	O
a	O
weighted	O
combination	O
of	O
a	O
small	O
number	O
of	O
suitably	O
chosen	O
basis	B
functions	I
that	O
is	O
we	O
assume	O
x	O
wz	O
where	O
z	O
has	O
a	O
sparse	B
prior	O
and	O
w	O
is	O
suitable	O
dictionary	B
this	O
is	O
called	O
compressed	B
sensing	I
or	O
compressive	B
sensing	I
et	O
al	O
baruniak	O
candes	O
and	O
wakin	O
bruckstein	O
et	O
al	O
for	O
cs	O
to	O
work	O
it	O
is	O
important	O
to	O
represent	O
the	O
signal	O
in	O
the	O
right	O
basis	O
otherwise	O
it	O
will	O
not	O
be	O
sparse	B
in	O
traditional	O
cs	O
applications	O
the	O
dictionary	B
is	O
fixed	O
to	O
be	O
a	O
standard	O
form	O
such	O
as	O
wavelets	O
however	O
one	O
can	O
get	O
much	O
better	O
performance	O
by	O
learning	B
a	O
domain-specific	O
dictionary	B
using	O
sparse	B
coding	I
et	O
al	O
as	O
for	O
the	O
sensing	O
matrix	O
r	O
it	O
is	O
often	O
chosen	O
to	O
be	O
a	O
random	O
matrix	O
for	O
reasons	O
explained	O
in	O
and	O
wakin	O
however	O
one	O
can	O
get	O
better	O
performance	O
by	O
adapting	O
the	O
projection	B
matrix	O
to	O
the	O
dictionary	B
and	O
nickish	O
chang	O
et	O
al	O
image	B
inpainting	I
and	O
denoising	O
suppose	O
we	O
have	O
an	O
image	O
which	O
is	O
corrupted	O
in	O
some	O
way	O
e	O
g	O
by	O
having	O
text	O
or	O
scratches	O
sparsely	O
superimposed	O
on	O
top	O
of	O
it	O
as	O
in	O
figure	O
we	O
might	O
want	O
to	O
estimate	O
the	O
underlying	O
the	O
reason	O
pca	B
discovers	O
sinusoidal	O
grating	O
patterns	O
is	O
because	O
it	O
is	O
trying	O
to	O
model	O
the	O
covariance	B
of	O
the	O
data	O
which	O
in	O
the	O
case	O
of	O
image	O
patches	O
is	O
translation	B
invariant	B
this	O
means	O
cov	O
y	O
for	O
some	O
function	O
f	O
where	O
ix	O
y	O
is	O
the	O
image	O
intensity	O
at	O
location	O
y	O
one	O
can	O
show	O
et	O
al	O
that	O
the	O
eigenvectors	O
of	O
a	O
matrix	O
of	O
this	O
kind	O
are	O
always	O
sinusoids	O
of	O
different	O
phases	O
i	O
e	O
pca	B
discovers	O
a	O
fourier	B
basis	I
f	O
sparse	B
coding	I
z	O
x	O
y	O
w	O
r	O
figure	O
schematic	O
dgm	B
for	O
compressed	B
sensing	I
we	O
observe	O
a	O
low	O
dimensional	O
measurement	O
y	O
generated	O
by	O
passing	O
x	O
through	O
a	O
measurement	O
matrix	O
r	O
and	O
possibly	O
subject	O
to	O
observation	B
noise	O
with	O
variance	B
we	O
assume	O
that	O
x	O
has	O
a	O
sparse	B
decomposition	O
in	O
terms	O
of	O
the	O
dictionary	B
w	O
and	O
the	O
latent	B
variables	O
z	O
the	O
parameter	B
controlls	O
the	O
sparsity	B
level	O
figure	O
an	O
example	O
of	O
image	B
inpainting	I
using	O
sparse	B
coding	I
left	O
original	O
image	O
right	O
reconstruction	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
julien	O
mairal	O
clean	O
image	O
this	O
is	O
called	O
image	B
inpainting	I
one	O
can	O
use	O
similar	B
techniques	O
for	O
image	B
denoising	I
we	O
can	O
model	O
this	O
as	O
a	O
special	O
kind	O
of	O
compressed	B
sensing	I
problem	O
the	O
basic	O
idea	O
is	O
as	O
follows	O
we	O
partition	O
the	O
image	O
into	O
overlapping	O
patches	O
yi	O
and	O
concatenate	O
them	O
to	O
form	O
y	O
we	O
define	O
r	O
so	O
that	O
the	O
i	O
th	O
row	O
selects	O
out	O
patch	O
i	O
now	O
define	O
v	O
to	O
be	O
the	O
visible	B
components	O
of	O
y	O
and	O
h	O
to	O
be	O
the	O
hidden	B
components	O
to	O
perform	O
image	B
inpainting	I
we	O
just	O
compute	O
pyhyv	O
where	O
are	O
the	O
model	O
parameters	O
which	O
specify	O
the	O
dictionary	B
w	O
and	O
the	O
sparsity	B
level	O
of	O
z	O
we	O
can	O
either	O
learn	O
a	O
dictionary	B
offline	B
from	O
a	O
database	O
of	O
images	O
or	O
we	O
can	O
learn	O
a	O
dictionary	B
just	O
for	O
this	O
image	O
based	O
on	O
the	O
non-corrupted	O
patches	O
from	O
undamaged	O
color	O
patches	O
in	O
the	O
mega-pixel	O
image	O
figure	O
shows	O
this	O
technique	O
in	O
action	B
the	O
dictionary	B
size	O
atoms	O
was	O
learned	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
a	O
graphical	B
model	I
the	O
fields	O
of	O
experts	O
model	O
chapter	O
sparse	B
linear	O
models	O
and	O
black	O
which	O
directly	O
encodes	O
correlations	O
between	O
neighboring	O
image	O
patches	O
rather	O
than	O
using	O
a	O
latent	B
variable	O
model	O
unfortunately	O
such	O
models	O
tend	O
to	O
be	O
computationally	O
more	O
expensive	O
exercises	O
exercise	O
partial	O
derivative	O
of	O
the	O
rss	O
define	O
rssw	O
a	O
show	O
that	O
wk	O
rssw	O
ak	O
kwk	O
ck	O
ik	O
xikyi	O
wt	O
kxi	O
k	O
ck	O
where	O
w	O
k	O
w	O
without	O
component	O
k	O
xi	O
k	O
is	O
xi	O
without	O
component	O
k	O
and	O
rk	O
y	O
wt	O
kx	O
k	O
is	O
the	O
residual	B
due	O
to	O
using	O
all	O
the	O
features	B
except	O
feature	O
k	O
hint	O
partition	O
the	O
weights	O
into	O
those	O
involving	O
k	O
and	O
those	O
not	O
involving	O
k	O
rssw	O
then	O
b	O
show	O
that	O
if	O
wk	O
wk	O
xt	O
hence	O
when	O
we	O
sequentially	O
add	O
features	B
the	O
optimal	O
weight	O
for	O
feature	O
k	O
is	O
computed	O
by	O
computing	O
orthogonally	O
projecting	O
xk	O
onto	O
the	O
current	O
residual	B
exercise	O
derivation	O
of	O
m	B
step	I
for	O
eb	B
for	O
linear	B
regression	B
derive	O
equations	O
and	O
hint	O
the	O
following	O
identity	O
should	O
be	O
useful	O
xt	O
x	O
xt	O
x	O
a	O
a	O
x	O
a	O
a	O
xt	O
x	O
a	O
x	O
a	O
a	O
exercise	O
derivation	O
of	O
fixed	O
point	O
updates	O
for	O
eb	B
for	O
linear	B
regression	B
derive	O
equations	O
and	O
hint	O
the	O
easiest	O
way	O
to	O
derive	O
this	O
result	O
is	O
to	O
rewrite	O
log	O
pd	O
as	O
in	O
equation	O
this	O
is	O
exactly	O
equivalent	O
since	O
in	O
the	O
case	O
of	O
a	O
gaussian	B
prior	O
and	O
likelihood	B
the	O
posterior	O
is	O
also	O
gaussian	B
so	O
the	O
laplace	B
approximation	I
is	O
exact	O
in	O
this	O
case	O
we	O
get	O
log	O
pd	O
n	O
log	O
j	O
log	O
j	O
mt	O
am	O
log	O
d	O
the	O
rest	O
is	O
straightforward	O
algebra	O
sparse	B
coding	I
exercise	O
marginal	B
likelihood	B
for	O
linear	B
regression	B
suppose	O
we	O
use	O
a	O
g-prior	B
of	O
the	O
form	O
gxt	O
x	O
show	O
that	O
equation	O
simplifies	O
to	O
pd	O
g	O
s	O
t	O
y	O
g	O
d	O
s	O
x	O
yt	O
x	O
y	O
exercise	O
reducing	O
elastic	B
net	I
to	O
lasso	B
define	O
and	O
y	O
x	O
c	O
where	O
c	O
and	O
y	O
x	O
y	O
x	O
c	O
show	O
arg	O
min	O
carg	O
min	O
i	O
e	O
and	O
hence	O
that	O
one	O
can	O
solve	O
an	O
elastic	B
net	I
problem	O
using	O
a	O
lasso	B
solver	O
on	O
modified	O
data	O
exercise	O
shrinkage	B
in	O
linear	B
regression	B
jaakkola	O
consider	O
performing	O
linear	B
regression	B
with	O
an	O
orthonormal	O
design	B
matrix	I
so	O
for	O
each	O
column	O
k	O
and	O
xt	O
figure	O
plots	O
wk	O
vs	O
ck	O
xk	O
the	O
correlation	O
of	O
feature	O
k	O
with	O
the	O
response	O
for	O
different	O
esimation	O
methods	O
ordinary	B
least	B
squares	I
ridge	B
regression	B
with	O
parameter	B
and	O
lasso	B
with	O
parameter	B
so	O
we	O
can	O
estimate	O
each	O
parameter	B
wk	O
separately	O
a	O
unfortunately	O
we	O
forgot	O
to	O
label	B
the	O
plots	O
which	O
method	O
does	O
the	O
solid	O
dotted	O
and	O
dashed	O
line	O
correspond	O
to	O
hint	O
see	O
section	O
b	O
what	O
is	O
the	O
value	O
of	O
c	O
what	O
is	O
the	O
value	O
of	O
exercise	O
prior	O
for	O
the	O
bernoulli	B
rate	B
parameter	B
in	O
the	O
spike	B
and	I
slab	I
model	O
consider	O
the	O
model	O
in	O
section	O
suppose	O
we	O
put	O
a	O
prior	O
on	O
the	O
sparsity	B
rates	O
j	O
beta	O
derive	O
an	O
expression	O
for	O
p	O
after	O
integrating	O
out	O
the	O
j	O
s	O
discuss	O
some	O
advantages	O
and	O
disadvantages	O
of	O
this	O
approach	O
compared	O
to	O
assuming	O
j	O
for	O
fixed	O
chapter	O
sparse	B
linear	O
models	O
k	O
w	O
c	O
k	O
figure	O
plot	O
of	O
wk	O
vs	O
amount	O
of	O
correlation	O
ck	O
for	O
three	O
different	O
estimators	O
exercise	O
deriving	O
e	B
step	I
for	O
gsm	O
prior	O
show	O
that	O
e	O
j	O
where	O
log	O
pwj	O
and	O
pwj	O
intn	O
j	O
j	O
j	O
hint	O
j	O
n	O
exp	O
j	O
j	O
j	O
j	O
exp	O
j	O
j	O
j	O
dwjn	O
d	O
j	O
hint	O
d	O
dwj	O
pwj	O
pwj	O
d	O
dwj	O
log	O
pwj	O
exercise	O
em	B
for	O
sparse	B
probit	B
regression	B
with	O
laplace	B
prior	O
derive	O
an	O
em	B
algorithm	O
for	O
fitting	O
a	O
binary	O
probit	B
classifier	O
using	O
a	O
laplace	B
prior	O
on	O
the	O
weights	O
you	O
get	O
stuck	O
see	O
ding	O
and	O
harrison	O
exercise	O
gsm	O
representation	O
of	O
group	B
lasso	B
j	O
ga	O
ignoring	O
the	O
grouping	O
issue	O
for	O
now	O
the	O
marginal	B
distribution	I
consider	O
the	O
prior	O
induced	O
on	O
the	O
weights	O
by	O
a	O
gamma	B
mixing	O
distribution	O
is	O
called	O
the	O
normal	B
gamma	B
distribution	I
and	O
is	O
sparse	B
coding	I
given	O
by	O
ngwj	O
z	O
j	O
n	O
k	O
j	O
j	O
where	O
k	O
is	O
the	O
modified	O
bessel	B
function	I
of	O
the	O
second	O
kind	O
besselk	O
function	O
in	O
matlab	O
now	O
suppose	O
we	O
have	O
the	O
following	O
prior	O
on	O
the	O
variances	O
j	O
g	O
p	O
p	O
p	O
ga	O
j	O
g	O
the	O
corresponding	O
marginal	O
for	O
each	O
group	O
of	O
weights	O
has	O
the	O
form	O
pwg	O
g	O
dg	O
k	O
g	O
dg	O
ug	O
where	O
ug	O
j	O
g	O
gj	O
now	O
suppose	O
g	O
so	O
g	O
that	O
the	O
resulting	O
map	B
estimate	I
is	O
equivalent	O
to	O
group	B
lasso	B
conveniently	O
we	O
have	O
k	O
exp	O
z	O
show	O
exercise	O
projected	B
gradient	B
descent	I
for	O
regularized	O
least	B
squares	I
consider	O
the	O
bpdn	B
problem	O
argmin	O
rss	O
by	O
using	O
the	O
split	B
variable	I
trick	O
introducted	O
in	O
section	O
by	O
defining	O
rewrite	O
this	O
as	O
a	O
quadratic	B
program	I
with	O
a	O
simple	O
bound	O
constraint	O
then	O
sketch	O
how	O
to	O
use	O
projected	B
gradient	B
descent	I
to	O
solve	O
this	O
problem	O
you	O
get	O
stuck	O
consult	O
et	O
al	O
exercise	O
subderivative	B
of	O
the	O
hinge	B
loss	B
function	I
let	O
f	O
x	O
be	O
the	O
hinge	B
loss	B
function	I
where	O
z	O
what	O
are	O
f	O
f	O
and	O
f	O
exercise	O
lower	O
bounds	O
to	O
convex	B
functions	O
let	O
f	O
be	O
a	O
convex	B
function	O
explain	O
how	O
to	O
find	O
a	O
global	O
affine	O
lower	O
bound	O
to	O
f	O
at	O
an	O
arbitrary	O
point	O
x	O
domf	O
kernels	O
introduction	O
so	O
far	O
in	O
this	O
book	O
we	O
have	O
been	O
assuming	O
that	O
each	O
object	O
that	O
we	O
wish	O
to	O
classify	O
or	O
cluster	O
or	O
process	O
in	O
anyway	O
can	O
be	O
represented	O
as	O
a	O
fixed-size	O
feature	O
vector	O
typically	O
of	O
the	O
form	O
xi	O
r	O
d	O
however	O
for	O
certain	O
kinds	O
of	O
objects	O
it	O
is	O
not	O
clear	O
how	O
to	O
best	O
represent	O
them	O
as	O
fixed-sized	O
feature	O
vectors	O
for	O
example	O
how	O
do	O
we	O
represent	O
a	O
text	O
document	O
or	O
protein	O
sequence	O
which	O
can	O
be	O
of	O
variable	O
length	O
or	O
a	O
molecular	O
structure	O
which	O
has	O
complex	O
geometry	O
or	O
an	O
evolutionary	O
tree	B
which	O
has	O
variable	O
size	O
and	O
shape	O
one	O
approach	O
to	O
such	O
problems	O
is	O
to	O
define	O
a	O
generative	O
model	O
for	O
the	O
data	O
and	O
use	O
the	O
inferred	O
latent	B
representation	O
andor	O
the	O
parameters	O
of	O
the	O
model	O
as	O
features	B
and	O
then	O
to	O
plug	O
these	O
features	B
in	O
to	O
standard	O
methods	O
for	O
example	O
in	O
chapter	O
we	O
discuss	O
deep	B
learning	B
which	O
is	O
essentially	O
an	O
unsupervised	O
way	O
to	O
learn	O
good	O
feature	O
representations	O
another	O
approach	O
is	O
to	O
assume	O
that	O
we	O
have	O
some	O
way	O
of	O
measuring	O
the	O
similarity	O
between	O
objects	O
that	O
doesn	O
t	O
require	O
preprocessing	O
them	O
into	O
feature	O
vector	O
format	O
for	O
example	O
when	O
be	O
some	O
comparing	O
strings	O
we	O
can	O
compute	O
the	O
edit	B
distance	I
between	O
them	O
let	O
measure	O
of	O
similarity	O
between	O
objects	O
x	O
x	O
where	O
x	O
is	O
some	O
abstract	O
space	O
we	O
will	O
call	O
a	O
kernel	B
function	I
note	O
that	O
the	O
word	O
kernel	B
has	O
several	O
meanings	O
we	O
will	O
discuss	O
a	O
different	O
interpretation	O
in	O
section	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
several	O
kinds	O
of	O
kernel	B
functions	O
we	O
then	O
describe	O
some	O
algorithms	O
that	O
can	O
be	O
written	O
purely	O
in	O
terms	O
of	O
kernel	B
function	I
computations	O
such	O
methods	O
can	O
be	O
used	O
when	O
we	O
don	O
t	O
have	O
access	O
to	O
choose	O
not	O
to	O
look	O
at	O
the	O
inside	O
of	O
the	O
objects	O
x	O
that	O
we	O
are	O
processing	O
kernel	B
functions	O
we	O
define	O
a	O
kernel	B
function	I
to	O
be	O
a	O
real-valued	O
function	O
of	O
two	O
arguments	O
x	O
x	O
typically	O
the	O
function	O
is	O
symmetric	B
give	O
several	O
examples	O
below	O
r	O
for	O
x	O
and	O
non-negative	O
so	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
measure	O
of	O
similarity	O
but	O
this	O
is	O
not	O
required	O
we	O
rbf	B
kernels	O
chapter	O
kernels	O
the	O
squared	B
exponential	I
kernel	B
kernel	B
or	O
gaussian	B
kernel	B
is	O
defined	O
by	O
exp	O
if	O
is	O
diagonal	B
this	O
can	O
be	O
written	O
as	O
j	O
exp	O
we	O
can	O
interpret	O
the	O
j	O
as	O
defining	O
the	O
characteristic	B
length	I
scale	I
of	O
dimension	O
j	O
if	O
j	O
if	O
is	O
the	O
corresponding	O
dimension	O
is	O
ignored	O
hence	O
this	O
is	O
known	O
as	O
the	O
ard	B
kernel	B
spherical	B
we	O
get	O
the	O
isotropic	B
kernel	B
exp	O
here	O
is	O
known	O
as	O
the	O
bandwidth	B
equation	O
is	O
an	O
example	O
of	O
a	O
a	O
radial	B
basis	I
function	I
or	O
rbf	B
kernel	B
since	O
it	O
is	O
only	O
a	O
function	O
of	O
kernels	O
for	O
comparing	O
documents	O
when	O
performing	O
document	O
classification	O
or	O
retrieval	O
it	O
is	O
useful	O
to	O
have	O
a	O
way	O
of	O
comparing	O
two	O
documents	O
xi	O
and	O
if	O
we	O
use	O
a	O
bag	B
of	I
words	I
representation	O
where	O
xij	O
is	O
the	O
number	O
of	O
times	O
words	O
j	O
occurs	O
in	O
document	O
i	O
we	O
can	O
use	O
the	O
cosine	B
similarity	I
which	O
is	O
defined	O
by	O
xt	O
i	O
this	O
quantity	O
measures	O
the	O
cosine	O
of	O
the	O
angle	O
between	O
xi	O
and	O
when	O
interpreted	O
as	O
vectors	O
since	O
xi	O
is	O
a	O
count	O
vector	O
hence	O
non-negative	O
the	O
cosine	B
similarity	I
is	O
between	O
and	O
where	O
means	O
the	O
vectors	O
are	O
orthogonal	O
and	O
therefore	O
have	O
no	O
words	O
in	O
common	O
unfortunately	O
this	O
simple	O
method	O
does	O
not	O
work	O
very	O
well	O
for	O
two	O
main	O
reasons	O
first	O
if	O
xi	O
has	O
any	O
word	O
in	O
common	O
with	O
it	O
is	O
deemed	O
similar	B
even	O
though	O
some	O
popular	O
words	O
such	O
as	O
the	O
or	O
and	O
occur	O
in	O
many	O
documents	O
and	O
are	O
therefore	O
not	O
discriminative	B
are	O
known	O
as	O
stop	B
words	I
second	O
if	O
a	O
discriminative	B
word	O
occurs	O
many	O
times	O
in	O
a	O
document	O
the	O
similarity	O
is	O
artificially	O
boosted	O
even	O
though	O
word	O
usage	O
tends	O
to	O
be	O
bursty	B
meaning	O
that	O
once	O
a	O
word	O
is	O
used	O
in	O
a	O
document	O
it	O
is	O
very	O
likely	O
to	O
be	O
used	O
again	O
section	O
fortunately	O
we	O
can	O
significantly	O
improve	O
performance	O
using	O
some	O
simple	O
preprocessing	O
the	O
idea	O
is	O
to	O
replace	O
the	O
word	O
count	O
vector	O
with	O
a	O
new	O
feature	O
vector	O
called	O
the	O
tf-idf	B
representation	O
which	O
stands	O
for	O
term	O
frequency	O
inverse	O
document	O
frequency	O
we	O
define	O
this	O
as	O
follows	O
first	O
the	O
term	O
frequency	O
is	O
defined	O
as	O
a	O
log-transform	O
of	O
the	O
count	O
this	O
reduces	O
the	O
impact	O
of	O
words	O
that	O
occur	O
many	O
times	O
within	O
one	O
document	O
second	O
the	O
inverse	O
document	O
frequency	O
is	O
defined	O
as	O
tfxij	O
xij	O
idfj	O
log	O
n	O
ixij	O
kernel	B
functions	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
documents	O
and	O
the	O
denominator	O
counts	O
how	O
many	O
documents	O
contain	O
term	O
j	O
finally	O
we	O
define	O
tf-idfxi	O
idfjv	O
are	O
several	O
other	O
ways	O
to	O
define	O
the	O
tf	O
and	O
idf	O
terms	O
see	O
et	O
al	O
for	O
details	O
we	O
then	O
use	O
this	O
inside	O
the	O
cosine	B
similarity	I
measure	O
that	O
is	O
our	O
new	O
kernel	B
has	O
the	O
form	O
where	O
tf-idfx	O
this	O
gives	O
good	O
results	O
for	O
information	B
retrieval	I
et	O
al	O
a	O
probabilistic	O
interpretation	O
of	O
the	O
tf-idf	B
kernel	B
is	O
given	O
in	O
mercer	O
definite	O
kernels	O
some	O
methods	O
that	O
we	O
will	O
study	O
require	O
that	O
the	O
kernel	B
function	I
satisfy	O
the	O
requirement	O
that	O
the	O
gram	B
matrix	I
defined	O
by	O
k	O
xn	O
xn	O
be	O
positive	O
definite	O
for	O
any	O
set	O
of	O
inputs	O
we	O
call	O
such	O
a	O
kernel	B
a	O
mercer	B
kernel	B
or	O
positive	O
definite	O
kernel	B
it	O
can	O
be	O
shown	O
and	O
smola	O
that	O
the	O
gaussian	B
kernel	B
is	O
a	O
mercer	B
kernel	B
as	O
is	O
the	O
cosine	B
similarity	I
kernel	B
and	O
heilman	O
the	O
importance	O
of	O
mercer	O
kernels	O
is	O
the	O
following	O
result	O
known	O
as	O
mercer	O
s	O
theorem	O
if	O
the	O
gram	B
matrix	I
is	O
positive	O
definite	O
we	O
can	O
compute	O
an	O
eigenvector	O
decomposition	O
of	O
it	O
as	O
follows	O
k	O
ut	O
u	O
where	O
is	O
a	O
diagonal	B
matrix	O
of	O
eigenvalues	O
i	O
now	O
consider	O
an	O
element	O
of	O
k	O
kij	O
uit	O
let	O
us	O
define	O
uj	O
ui	O
then	O
we	O
can	O
write	O
kij	O
thus	O
we	O
see	O
that	O
the	O
entries	O
in	O
the	O
kernel	B
matrix	O
can	O
be	O
computed	O
by	O
performing	O
an	O
inner	O
product	O
of	O
some	O
feature	O
vectors	O
that	O
are	O
implicitly	O
defined	O
by	O
the	O
eigenvectors	O
u	O
in	O
general	O
if	O
the	O
kernel	B
is	O
mercer	O
then	O
there	O
exists	O
a	O
function	O
mapping	O
x	O
x	O
to	O
r	O
d	O
such	O
that	O
where	O
depends	O
on	O
the	O
eigen	O
functions	O
of	O
d	O
is	O
a	O
potentially	O
infinite	O
dimensional	O
space	O
rm	O
where	O
r	O
one	O
can	O
show	O
that	O
the	O
corresponding	O
feature	O
vector	O
will	O
contain	O
all	O
terms	O
up	O
to	O
degree	B
m	O
for	O
example	O
if	O
m	O
r	O
and	O
x	O
r	O
for	O
example	O
consider	O
the	O
polynomial	B
kernel	B
xt	O
we	O
have	O
xt	O
chapter	O
kernels	O
this	O
can	O
be	O
written	O
as	O
where	O
so	O
using	O
this	O
kernel	B
is	O
equivalent	O
to	O
working	O
in	O
a	O
dimensional	O
feature	O
space	O
in	O
the	O
case	O
of	O
a	O
gaussian	B
kernel	B
the	O
feature	O
map	O
lives	O
in	O
an	O
infinite	O
dimensional	O
space	O
in	O
such	O
a	O
case	O
it	O
is	O
clearly	O
infeasible	O
to	O
explicitly	O
represent	O
the	O
feature	O
vectors	O
an	O
example	O
of	O
a	O
kernel	B
that	O
is	O
not	O
a	O
mercer	B
kernel	B
is	O
the	O
so-called	O
sigmoid	B
kernel	B
defined	O
by	O
tanh	O
xt	O
r	O
that	O
this	O
uses	O
the	O
tanh	O
function	O
even	O
though	O
it	O
is	O
called	O
a	O
sigmoid	B
kernel	B
this	O
kernel	B
was	O
inspired	O
by	O
the	O
multi-layer	B
perceptron	B
section	O
but	O
there	O
is	O
no	O
real	O
reason	O
to	O
use	O
it	O
a	O
true	O
neural	O
net	O
kernel	B
which	O
is	O
positive	O
definite	O
see	O
section	O
in	O
general	O
establishing	O
that	O
a	O
kernel	B
is	O
a	O
mercer	B
kernel	B
is	O
difficult	O
and	O
requires	O
techniques	O
from	O
functional	O
analysis	O
however	O
one	O
can	O
show	O
that	O
it	O
is	O
possible	O
to	O
build	O
up	O
new	O
mercer	O
kernels	O
from	O
simpler	O
ones	O
using	O
a	O
set	O
of	O
standard	O
rules	B
for	O
example	O
if	O
and	O
are	O
both	O
mercer	O
so	O
is	O
see	O
e	O
g	O
and	O
smola	O
for	O
details	O
linear	O
kernels	O
deriving	O
the	O
feature	O
vector	O
implied	O
by	O
a	O
kernel	B
is	O
in	O
general	O
quite	O
difficult	O
and	O
only	O
possible	O
if	O
the	O
kernel	B
is	O
mercer	O
however	O
deriving	O
a	O
kernel	B
from	O
a	O
feature	O
vector	O
is	O
easy	O
we	O
just	O
use	O
if	O
x	O
we	O
get	O
thelinear	O
kernel	B
defined	O
by	O
xt	O
this	O
is	O
useful	O
if	O
the	O
original	O
data	O
is	O
already	O
high	O
dimensional	O
and	O
if	O
the	O
original	O
features	B
are	O
individually	O
informative	O
e	O
g	O
a	O
bag	B
of	I
words	I
representation	O
where	O
the	O
vocabulary	O
size	O
is	O
large	O
or	O
the	O
expression	O
level	O
of	O
many	O
genes	O
in	O
such	O
a	O
case	O
the	O
decision	B
boundary	I
is	O
likely	O
to	O
be	O
representable	O
as	O
a	O
linear	O
combination	O
of	O
the	O
original	O
features	B
so	O
it	O
is	O
not	O
necessary	O
to	O
work	O
in	O
some	O
other	O
feature	O
space	O
of	O
course	O
not	O
all	O
high	O
dimensional	O
problems	O
are	O
linearly	B
separable	I
for	O
example	O
images	O
are	O
high	O
dimensional	O
but	O
individual	O
pixels	O
are	O
not	O
very	O
informative	O
so	O
image	O
classification	O
typically	O
requires	O
non-linear	O
kernels	O
e	O
g	O
section	O
matern	O
kernels	O
the	O
matern	B
kernel	B
which	O
is	O
commonly	O
used	O
in	O
gaussian	B
process	I
regression	B
section	O
has	O
the	O
following	O
form	O
r	O
k	O
r	O
kernel	B
functions	O
where	O
r	O
and	O
k	O
is	O
a	O
modified	O
bessel	B
function	I
as	O
this	O
approaches	O
the	O
se	O
kernel	B
if	O
the	O
kernel	B
simplifies	O
to	O
exp	O
if	O
d	O
and	O
we	O
use	O
this	O
kernel	B
to	O
define	O
a	O
gaussian	B
process	I
chapter	O
we	O
get	O
the	O
ornstein-uhlenbeck	B
process	I
which	O
describes	O
the	O
velocity	O
of	O
a	O
particle	O
undergoing	O
brownian	B
motion	I
corresponding	O
function	O
is	O
continuous	O
but	O
not	O
differentiable	O
and	O
hence	O
is	O
very	O
jagged	O
string	O
kernels	O
consider	O
two	O
strings	O
x	O
and	O
the	O
real	O
power	O
of	O
kernels	O
arises	O
when	O
the	O
inputs	O
are	O
structured	O
objects	O
as	O
an	O
example	O
we	O
now	O
describe	O
one	O
way	O
of	O
comparing	O
two	O
variable	O
length	O
strings	O
using	O
a	O
string	B
kernel	B
we	O
follow	O
the	O
presentation	O
of	O
and	O
williams	O
and	O
et	O
al	O
of	O
lengths	O
d	O
d	O
each	O
defined	O
over	O
the	O
alphabet	O
a	O
for	O
example	O
consider	O
two	O
amino	O
acid	O
sequences	O
defined	O
over	O
the	O
letter	O
alphabet	O
a	O
r	O
n	O
d	O
c	O
e	O
q	O
g	O
h	O
i	O
l	O
k	O
m	O
f	O
p	O
s	O
t	O
w	O
y	O
v	O
let	O
x	O
be	O
the	O
following	O
sequence	O
of	O
length	O
iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv	O
erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi	O
and	O
let	O
be	O
the	O
following	O
sequence	O
of	O
length	O
phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla	O
rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk	O
lwglkvlqelsqwtvrsihdlrfisshqtgip	O
these	O
strings	O
have	O
the	O
substring	O
lqe	O
in	O
common	O
we	O
can	O
define	O
the	O
similarity	O
of	O
two	O
strings	O
to	O
be	O
the	O
number	O
of	O
substrings	O
they	O
have	O
in	O
common	O
more	O
formally	O
and	O
more	O
generally	O
let	O
us	O
say	O
that	O
s	O
is	O
a	O
substring	O
of	O
x	O
if	O
we	O
can	O
write	O
x	O
usv	O
for	O
some	O
empty	O
strings	O
u	O
s	O
and	O
v	O
now	O
let	O
sx	O
denote	O
the	O
number	O
of	O
times	O
that	O
substring	O
s	O
appears	O
in	O
string	O
x	O
we	O
define	O
the	O
kernel	B
between	O
two	O
strings	O
x	O
and	O
as	O
s	O
a	O
ws	O
sx	O
where	O
ws	O
and	O
a	O
is	O
the	O
set	O
of	O
all	O
strings	O
any	O
length	O
from	O
the	O
alphabet	O
a	O
is	O
known	O
as	O
the	O
kleene	B
star	I
operator	O
this	O
is	O
a	O
mercer	B
kernel	B
and	O
be	O
computed	O
in	O
ox	O
time	O
certain	O
settings	O
of	O
the	O
weights	O
using	O
suffix	B
trees	I
et	O
al	O
vishwanathan	O
and	O
smola	O
shawe-taylor	O
and	O
cristianini	O
there	O
are	O
various	O
cases	O
of	O
interest	O
if	O
we	O
set	O
ws	O
for	O
we	O
get	O
a	O
bag-of-characters	B
kernel	B
this	O
defines	O
to	O
be	O
the	O
number	O
of	O
times	O
each	O
character	O
in	O
a	O
occurs	O
in	O
x	O
if	O
we	O
require	O
s	O
to	O
be	O
bordered	O
by	O
white-space	O
we	O
get	O
a	O
bag-of-words	B
kernel	B
where	O
counts	O
how	O
many	O
times	O
each	O
possible	O
word	O
occurs	O
note	O
that	O
this	O
is	O
a	O
very	O
sparse	B
vector	O
since	O
most	O
words	O
chapter	O
kernels	O
optimal	O
partial	O
matching	O
matching	O
figure	O
of	O
kristen	O
grauman	O
illustration	O
of	O
a	O
pyramid	B
match	I
kernel	B
computed	O
from	O
two	O
images	O
used	O
with	O
kind	O
permission	O
if	O
we	O
only	O
consider	O
strings	O
of	O
a	O
fixed	O
length	O
k	O
we	O
get	O
the	O
k-spectrum	O
will	O
not	O
be	O
present	O
kernel	B
this	O
has	O
been	O
used	O
to	O
classify	O
proteins	O
into	O
scop	O
superfamilies	O
et	O
al	O
for	O
example	O
if	O
k	O
we	O
have	O
lqex	O
and	O
for	O
the	O
two	O
strings	O
above	O
various	O
extensions	O
are	O
possible	O
for	O
example	O
we	O
can	O
allow	O
character	O
mismatches	O
et	O
al	O
and	O
we	O
can	O
generalize	B
string	O
kernels	O
to	O
compare	O
trees	O
as	O
described	O
in	O
and	O
duffy	O
this	O
is	O
useful	O
for	O
classifying	O
ranking	B
parse	O
trees	O
evolutionary	O
trees	O
etc	O
pyramid	O
match	O
kernels	O
in	O
computer	O
vision	O
it	O
is	O
common	O
to	O
create	O
a	O
bag-of-words	B
representation	O
of	O
an	O
image	O
by	O
computing	O
a	O
feature	O
vector	O
using	O
sift	B
from	O
a	O
variety	O
of	O
points	O
in	O
the	O
image	O
commonly	O
chosen	O
by	O
an	O
interest	B
point	I
detector	I
the	O
feature	O
vectors	O
at	O
the	O
chosen	O
places	O
are	O
then	O
vector-quantized	O
to	O
create	O
a	O
bag	O
of	O
discrete	B
symbols	O
one	O
way	O
to	O
compare	O
two	O
variable-sized	O
bags	O
of	O
this	O
kind	O
is	O
to	O
use	O
a	O
pyramid	B
match	I
kernel	B
and	O
darrell	O
the	O
basic	O
idea	O
is	O
illustrated	O
in	O
figure	O
each	O
feature	O
set	O
is	O
mapped	O
to	O
a	O
multi-resolution	O
histogram	B
these	O
are	O
then	O
compared	O
using	O
weighted	O
histogram	B
intersection	O
it	O
turns	O
out	O
that	O
this	O
provides	O
a	O
good	O
approximation	O
to	O
the	O
similarity	O
measure	O
one	O
would	O
obtain	O
by	O
performing	O
an	O
optimal	O
bipartite	O
match	O
at	O
the	O
finest	O
spatial	O
resolution	O
and	O
then	O
summing	O
up	O
pairwise	O
similarities	O
between	O
matched	O
points	O
however	O
the	O
histogram	B
method	O
is	O
faster	O
and	O
is	O
more	O
robust	B
to	O
missing	B
and	O
unequal	O
numbers	O
of	O
points	O
this	O
is	O
a	O
mercer	B
kernel	B
kernel	B
functions	O
kernels	O
derived	O
from	O
probabilistic	O
generative	O
models	O
suppose	O
we	O
have	O
a	O
probabilistic	O
generative	O
model	O
of	O
feature	O
vectors	O
px	O
then	O
there	O
are	O
several	O
ways	O
we	O
can	O
use	O
this	O
model	O
to	O
define	O
kernel	B
functions	O
and	O
thereby	O
make	O
the	O
model	O
suitable	O
for	O
discriminative	B
tasks	O
we	O
sketch	O
two	O
approaches	O
below	O
probability	O
product	O
kernels	O
one	O
approach	O
is	O
to	O
define	O
a	O
kernel	B
as	O
follows	O
pxxi	O
pxxj	O
dx	O
xj	O
where	O
and	O
pxxi	O
is	O
often	O
approximated	O
by	O
px	O
where	O
is	O
a	O
parameter	B
estimate	O
computed	O
using	O
a	O
single	O
data	O
vector	O
this	O
is	O
called	O
a	O
probability	B
product	I
kernel	B
et	O
al	O
although	O
it	O
seems	O
strange	O
to	O
fit	O
a	O
model	O
to	O
a	O
single	O
data	O
point	O
it	O
is	O
important	O
to	O
bear	O
in	O
mind	O
that	O
the	O
fitted	O
model	O
is	O
only	O
being	O
used	O
to	O
see	O
how	O
similar	B
two	O
objects	O
are	O
in	O
particular	O
if	O
we	O
fit	O
the	O
model	O
to	O
xi	O
and	O
then	O
the	O
model	O
thinks	O
xj	O
is	O
likely	O
this	O
means	O
that	O
xi	O
and	O
xj	O
are	O
similar	B
for	O
example	O
suppose	O
px	O
where	O
is	O
fixed	O
if	O
and	O
we	O
use	O
xi	O
and	O
xj	O
we	O
find	O
et	O
al	O
that	O
xj	O
exp	O
which	O
is	O
to	O
a	O
constant	O
factor	B
the	O
rbf	B
kernel	B
it	O
turns	O
out	O
that	O
one	O
can	O
compute	O
equation	O
for	O
a	O
variety	O
of	O
generative	O
models	O
including	O
ones	O
with	O
latent	B
variables	O
such	O
as	O
hmms	B
this	O
provides	O
one	O
way	O
to	O
define	O
kernels	O
on	O
variable	O
length	O
sequences	O
furthermore	O
this	O
technique	O
works	O
even	O
if	O
the	O
sequences	O
are	O
of	O
real-valued	O
vectors	O
unlike	O
the	O
string	B
kernel	B
in	O
section	O
see	O
et	O
al	O
for	O
further	O
details	O
fisher	O
kernels	O
a	O
more	O
efficient	O
way	O
to	O
use	O
generative	O
models	O
to	O
define	O
kernels	O
is	O
to	O
use	O
a	O
fisher	B
kernel	B
and	O
haussler	O
which	O
is	O
defined	O
as	O
follows	O
gxt	O
f	O
gx	O
log	O
px	O
f	O
log	O
px	O
where	O
g	O
is	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
or	O
score	B
vector	I
evaluated	O
at	O
the	O
mle	B
and	O
f	O
is	O
the	O
fisher	B
information	B
matrix	I
which	O
is	O
essentially	O
the	O
hessian	B
note	O
that	O
is	O
a	O
function	O
of	O
all	O
the	O
data	O
so	O
the	O
similarity	O
of	O
x	O
and	O
of	O
all	O
the	O
data	O
as	O
well	O
also	O
note	O
that	O
we	O
only	O
have	O
to	O
fit	O
one	O
model	O
is	O
computed	O
in	O
the	O
context	O
the	O
intuition	O
behind	O
the	O
fisher	B
kernel	B
is	O
the	O
following	O
let	O
gx	O
be	O
the	O
direction	O
parameter	B
space	O
in	O
which	O
x	O
would	O
like	O
the	O
parameters	O
to	O
move	O
so	O
as	O
to	O
maximize	O
its	O
own	O
chapter	O
kernels	O
rbf	B
prototypes	O
figure	O
xor	B
truth	O
table	O
fitting	O
a	O
linear	O
logistic	B
regression	B
classifier	O
using	O
degree	B
polynomial	O
expansion	O
same	O
model	O
but	O
using	O
an	O
rbf	B
kernel	B
with	O
centroids	B
specified	O
by	O
the	O
black	O
crosses	O
figure	O
generated	O
by	O
logregxordemo	O
likelihood	B
call	O
this	O
the	O
directional	O
gradient	O
then	O
we	O
say	O
that	O
two	O
vectors	O
x	O
and	O
are	O
similar	B
if	O
their	O
directional	O
gradients	O
are	O
similar	B
wrt	O
the	O
the	O
geometry	O
encoded	O
by	O
the	O
curvature	O
of	O
the	O
likelihood	B
function	O
section	O
interestingly	O
it	O
was	O
shown	O
in	O
et	O
al	O
that	O
the	O
string	B
kernel	B
of	O
section	O
is	O
equivalent	O
to	O
the	O
fisher	B
kernel	B
derived	O
from	O
an	O
l	O
th	O
order	O
markov	B
chain	I
section	O
also	O
it	O
was	O
shown	O
in	O
that	O
a	O
kernel	B
defined	O
by	O
the	O
inner	O
product	O
of	O
tf-idf	B
vectors	O
is	O
approximately	O
equal	O
to	O
the	O
fisher	B
kernel	B
for	O
a	O
certain	O
generative	O
model	O
of	O
text	O
based	O
on	O
the	O
compound	O
dirichlet	B
multinomial	B
model	O
using	O
kernels	O
inside	O
glms	O
in	O
this	O
section	O
we	O
discuss	O
one	O
simple	O
way	O
to	O
use	O
kernels	O
for	O
classification	O
and	O
regression	B
we	O
will	O
see	O
other	O
approaches	O
later	O
kernel	B
machines	O
we	O
define	O
a	O
kernel	B
machine	I
to	O
be	O
a	O
glm	B
where	O
the	O
input	O
feature	O
vector	O
has	O
the	O
form	O
k	O
where	O
k	O
x	O
are	O
a	O
set	O
of	O
k	O
centroids	B
if	O
is	O
an	O
rbf	B
kernel	B
this	O
is	O
called	O
an	O
rbf	B
network	I
we	O
discuss	O
ways	O
to	O
choose	O
the	O
k	O
parameters	O
below	O
we	O
will	O
call	O
equation	O
a	O
kernelised	B
feature	I
vector	I
note	O
that	O
in	O
this	O
approach	O
the	O
kernel	B
need	O
not	O
be	O
a	O
mercer	B
kernel	B
we	O
can	O
use	O
the	O
kernelized	O
feature	O
vector	O
for	O
logistic	B
regression	B
by	O
defining	O
pyx	O
berwt	O
this	O
provides	O
a	O
simple	O
way	O
to	O
define	O
a	O
non-linear	O
decision	B
boundary	I
as	O
an	O
example	O
consider	O
the	O
data	O
coming	O
from	O
the	O
exclusive	B
or	I
or	O
xor	B
function	O
this	O
is	O
a	O
binaryvalued	O
function	O
of	O
two	O
binary	O
inputs	O
its	O
truth	O
table	O
is	O
shown	O
in	O
figure	O
in	O
figure	O
we	O
have	O
show	O
some	O
data	O
labeled	O
by	O
the	O
xor	B
function	O
but	O
we	O
have	O
jittered	B
the	O
points	O
to	O
make	O
the	O
picture	O
we	O
see	O
we	O
cannot	O
separate	O
the	O
data	O
even	O
using	O
a	O
degree	B
polynomial	O
jittering	O
is	O
a	O
common	O
visualization	O
trick	O
in	O
statistics	O
wherein	O
points	O
in	O
a	O
plotdisplay	O
that	O
would	O
otherwise	O
land	O
on	O
top	O
of	O
each	O
other	O
are	O
dispersed	O
with	O
uniform	O
additive	O
noise	O
using	O
kernels	O
inside	O
glms	O
x	O
figure	O
rbf	B
basis	O
in	O
left	O
column	O
fitted	O
function	O
middle	O
column	O
basis	B
functions	I
evaluated	O
on	O
a	O
grid	O
right	O
column	O
design	B
matrix	I
top	O
to	O
bottom	O
we	O
show	O
different	O
bandwidths	O
figure	O
generated	O
by	O
linregrbfdemo	O
however	O
using	O
an	O
rbf	B
kernel	B
and	O
just	O
prototypes	O
easily	O
solves	O
the	O
problem	O
as	O
shown	O
in	O
figure	O
we	O
can	O
also	O
use	O
the	O
kernelized	O
feature	O
vector	O
inside	O
a	O
linear	B
regression	B
model	O
by	O
defining	O
pyx	O
n	O
for	O
example	O
figure	O
shows	O
a	O
data	O
set	O
fit	O
with	O
k	O
uniformly	O
spaced	O
rbf	B
prototypes	O
but	O
with	O
the	O
bandwidth	B
ranging	O
from	O
small	O
to	O
large	O
small	O
values	O
lead	O
to	O
very	O
wiggly	O
functions	O
since	O
the	O
predicted	O
function	O
value	O
will	O
only	O
be	O
non-zero	O
for	O
points	O
x	O
that	O
are	O
close	O
to	O
one	O
of	O
the	O
prototypes	O
k	O
if	O
the	O
bandwidth	B
is	O
very	O
large	O
the	O
design	B
matrix	I
reduces	O
to	O
a	O
constant	O
matrix	O
of	O
s	O
since	O
each	O
point	O
is	O
equally	O
close	O
to	O
every	O
prototype	B
hence	O
the	O
corresponding	O
function	O
is	O
just	O
a	O
straight	O
line	O
rvms	O
and	O
other	O
sparse	B
vector	O
machines	O
the	O
main	O
issue	O
with	O
kernel	B
machines	O
is	O
how	O
do	O
we	O
choose	O
the	O
centroids	B
k	O
if	O
the	O
input	O
is	O
low-dimensional	O
euclidean	O
space	O
we	O
can	O
uniformly	O
tile	O
the	O
space	O
occupied	O
by	O
the	O
data	O
with	O
prototypes	O
as	O
we	O
did	O
in	O
figure	O
however	O
this	O
approach	O
breaks	O
down	O
in	O
higher	O
numbers	O
if	O
k	O
r	O
d	O
we	O
can	O
try	O
to	O
perform	O
of	O
dimensions	O
because	O
of	O
the	O
curse	B
of	I
dimensionality	I
numerical	O
optimization	B
of	O
these	O
parameters	O
e	O
g	O
or	O
we	O
can	O
use	O
mcmc	B
inference	B
e	O
g	O
et	O
al	O
kohn	O
et	O
al	O
but	O
the	O
resulting	O
objective	O
function	O
posterior	O
is	O
highly	O
multimodal	O
furthermore	O
these	O
techniques	O
is	O
hard	O
to	O
extend	O
to	O
structured	O
input	O
spaces	O
where	O
kernels	O
are	O
most	O
useful	O
another	O
approach	O
is	O
to	O
find	O
clusters	B
in	O
the	O
data	O
and	O
then	O
to	O
assign	O
one	O
prototype	B
per	O
cluster	O
chapter	O
kernels	O
center	O
clustering	B
algorithms	O
just	O
need	O
a	O
similarity	O
metric	B
as	O
input	O
however	O
the	O
regions	O
of	O
space	O
that	O
have	O
high	O
density	O
are	O
not	O
necessarily	O
the	O
ones	O
where	O
the	O
prototypes	O
are	O
most	O
useful	O
for	O
representing	O
the	O
output	O
that	O
is	O
clustering	B
is	O
an	O
unsupervised	O
task	O
that	O
may	O
not	O
yield	O
a	O
representation	O
that	O
is	O
useful	O
for	O
prediction	O
furthermore	O
there	O
is	O
the	O
need	O
to	O
pick	O
the	O
number	O
of	O
clusters	B
a	O
simpler	O
approach	O
is	O
to	O
make	O
each	O
example	O
xi	O
be	O
a	O
prototype	B
so	O
we	O
get	O
xn	O
now	O
we	O
see	O
d	O
n	O
so	O
we	O
have	O
as	O
many	O
parameters	O
as	O
data	O
points	O
however	O
we	O
can	O
use	O
any	O
of	O
the	O
sparsity-promoting	O
priors	O
for	O
w	O
discussed	O
in	O
chapter	O
to	O
efficiently	O
select	O
a	O
subset	O
of	O
the	O
training	O
exemplars	O
we	O
call	O
this	O
a	O
sparse	B
vector	I
machine	I
the	O
most	O
natural	O
choice	O
is	O
to	O
use	O
regularization	B
et	O
al	O
that	O
in	O
the	O
multi-class	O
case	O
it	O
is	O
necessary	O
to	O
use	O
group	B
lasso	B
since	O
each	O
exemplar	O
is	O
associated	O
with	O
c	O
weights	O
one	O
per	O
class	O
we	O
call	O
this	O
which	O
stands	O
for	O
vector	O
machine	O
by	O
analogy	O
we	O
define	O
the	O
use	O
of	O
an	O
regularizer	O
to	O
be	O
a	O
or	O
vector	O
machine	O
this	O
of	O
course	O
will	O
not	O
be	O
sparse	B
we	O
can	O
get	O
even	O
greater	O
sparsity	B
by	O
using	O
ardsbl	O
resulting	O
in	O
a	O
method	O
called	O
the	O
relevance	B
vector	I
machine	I
or	O
rvm	B
one	O
can	O
fit	O
this	O
model	O
using	O
generic	O
ardsbl	O
algorithms	O
although	O
in	O
practice	O
the	O
most	O
common	O
method	O
is	O
the	O
greedy	O
algorithm	O
in	O
and	O
faul	O
is	O
the	O
algorithm	O
implemented	O
in	O
mike	O
tipping	O
s	O
code	O
which	O
is	O
bundled	O
with	O
pmtk	O
another	O
very	O
popular	O
approach	O
to	O
creating	O
a	O
sparse	B
kernel	B
machine	I
is	O
to	O
use	O
a	O
support	B
vector	I
machine	I
or	O
svm	B
this	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
section	O
rather	O
than	O
using	O
a	O
sparsity-promoting	B
prior	I
it	O
essentially	O
modifies	O
the	O
likelihood	B
term	O
which	O
is	O
rather	O
unnatural	O
from	O
a	O
bayesian	B
point	O
of	O
view	O
nevertheless	O
the	O
effect	O
is	O
similar	B
as	O
we	O
will	O
see	O
in	O
figure	O
we	O
compare	O
rvm	B
and	O
an	O
svm	B
using	O
the	O
same	O
rbf	B
kernel	B
on	O
a	O
binary	O
classification	O
problem	O
in	O
for	O
simplicity	O
was	O
chosen	O
by	O
hand	O
for	O
and	O
for	O
rvms	O
the	O
parameters	O
are	O
estimated	O
using	O
empirical	B
bayes	I
and	O
for	O
the	O
svm	B
we	O
use	O
cv	B
to	O
pick	O
c	O
since	O
svm	B
performance	O
is	O
very	O
sensitive	O
to	O
this	O
parameter	B
section	O
we	O
see	O
that	O
all	O
the	O
methods	O
give	O
similar	B
performance	O
however	O
rvm	B
is	O
the	O
sparsest	O
hence	O
fastest	O
at	O
test	O
time	O
then	O
and	O
then	O
svm	B
rvm	B
is	O
also	O
the	O
fastest	O
to	O
train	O
since	O
cv	B
for	O
an	O
svm	B
is	O
slow	O
is	O
despite	O
the	O
fact	O
that	O
the	O
rvm	B
code	O
is	O
in	O
matlab	O
and	O
the	O
svm	B
code	O
is	O
in	O
c	O
this	O
result	O
is	O
fairly	O
typical	O
in	O
figure	O
we	O
compare	O
rvm	B
and	O
an	O
svm	B
using	O
an	O
rbf	B
kernel	B
on	O
a	O
regression	B
problem	O
again	O
we	O
see	O
that	O
predictions	O
are	O
quite	O
similar	B
but	O
rvm	B
is	O
the	O
sparsest	O
then	O
then	O
svm	B
this	O
is	O
further	O
illustrated	O
in	O
figure	O
the	O
kernel	B
trick	I
rather	O
than	O
defining	O
our	O
feature	O
vector	O
in	O
terms	O
of	O
kernels	O
xn	O
we	O
can	O
instead	O
work	O
with	O
the	O
original	O
feature	O
vectors	O
x	O
but	O
modify	O
the	O
algorithm	O
so	O
that	O
it	O
replaces	O
all	O
inner	O
products	O
of	O
the	O
form	O
with	O
a	O
call	O
to	O
the	O
kernel	B
function	I
this	O
is	O
called	O
the	O
kernel	B
trick	I
it	O
turns	O
out	O
that	O
many	O
algorithms	O
can	O
be	O
kernelized	O
in	O
this	O
way	O
we	O
give	O
some	O
examples	O
below	O
note	O
that	O
we	O
require	O
that	O
the	O
kernel	B
be	O
a	O
mercer	B
kernel	B
for	O
this	O
trick	O
to	O
work	O
the	O
kernel	B
trick	I
rvm	B
svm	B
figure	O
example	O
of	O
non-linear	O
binary	O
classification	O
using	O
an	O
rbf	B
kernel	B
with	O
bandwidth	B
with	O
rvm	B
svm	B
with	O
c	O
chosen	O
by	O
cross	B
validation	I
black	O
circles	O
denote	O
the	O
support	B
vectors	I
figure	O
generated	O
by	O
kernelbinaryclassifdemo	O
with	O
kernelized	O
nearest	B
neighbor	I
classification	O
recall	B
that	O
in	O
a	O
classifier	O
we	O
just	O
need	O
to	O
compute	O
the	O
euclidean	B
distance	I
of	O
a	O
test	O
vector	O
to	O
all	O
the	O
training	O
points	O
find	O
the	O
closest	O
one	O
and	O
look	O
up	O
its	O
label	B
this	O
can	O
be	O
kernelized	O
by	O
observing	O
that	O
xi	O
this	O
allows	O
us	O
to	O
apply	O
the	O
nearest	B
neighbor	I
classifier	O
to	O
structured	O
data	O
objects	O
kernelized	O
k-medoids	O
clustering	B
k-means	O
clustering	B
uses	O
euclidean	B
distance	I
to	O
measure	O
dissimilarity	O
which	O
is	O
not	O
always	O
appropriate	O
for	O
structured	O
objects	O
we	O
now	O
describe	O
how	O
to	O
develop	O
a	O
kernelized	O
chapter	O
kernels	O
svm	B
rvm	B
figure	O
example	O
of	O
kernel	B
based	O
regression	B
on	O
the	O
noisy	O
sinc	O
function	O
using	O
an	O
rbf	B
kernel	B
with	O
bandwidth	B
rvm	B
svm	B
regression	B
with	O
c	O
chosen	O
by	O
cross	B
validation	I
and	O
default	O
for	O
svmlight	O
red	O
circles	O
denote	O
the	O
retained	O
training	O
exemplars	O
figure	O
generated	O
by	O
kernelregrdemo	O
with	O
with	O
version	O
of	O
the	O
algorithm	O
the	O
first	O
step	O
is	O
to	O
replace	O
the	O
k-means	B
algorithm	I
with	O
the	O
k-medoids	B
algorothm	I
this	O
is	O
similar	B
to	O
k-means	O
but	O
instead	O
of	O
representing	O
each	O
cluster	O
s	O
centroid	B
by	O
the	O
mean	B
of	O
all	O
data	O
vectors	O
assigned	O
to	O
this	O
cluster	O
we	O
make	O
each	O
centroid	B
be	O
one	O
of	O
the	O
data	O
vectors	O
themselves	O
thus	O
we	O
always	O
deal	O
with	O
integer	O
indexes	O
rather	O
than	O
data	O
objects	O
we	O
assign	O
objects	O
to	O
their	O
closest	O
centroids	B
as	O
before	O
when	O
we	O
update	O
the	O
centroids	B
we	O
look	O
at	O
each	O
object	O
that	O
belongs	O
to	O
the	O
cluster	O
and	O
measure	O
the	O
sum	O
of	O
its	O
distances	O
to	O
all	O
the	O
others	O
in	O
the	O
same	O
cluster	O
we	O
then	O
pick	O
the	O
one	O
which	O
has	O
the	O
smallest	O
such	O
sum	O
mk	O
argmin	O
izik	O
di	O
the	O
kernel	B
trick	I
weights	O
for	O
weights	O
for	O
weights	O
for	O
rvm	B
weights	O
for	O
svm	B
figure	O
coefficient	O
vectors	O
of	O
length	O
n	O
for	O
the	O
models	O
in	O
figure	O
figure	O
generated	O
by	O
kernelregrdemo	O
where	O
di	O
this	O
takes	O
k	O
work	O
per	O
cluster	O
whereas	O
k-means	O
takes	O
onkd	O
to	O
update	O
each	O
cluster	O
the	O
pseudo-code	O
is	O
given	O
in	O
algorithm	O
this	O
method	O
can	O
be	O
modified	O
to	O
derive	O
a	O
classifier	O
by	O
computing	O
the	O
nearest	O
medoid	O
for	O
each	O
class	O
this	O
is	O
known	O
as	O
nearest	O
medoid	O
classification	O
et	O
al	O
di	O
this	O
algorithm	O
can	O
be	O
kernelized	O
by	O
using	O
equation	O
to	O
replace	O
the	O
distance	O
computation	O
chapter	O
kernels	O
algorithm	O
k-medoids	O
algorithm	O
initialize	O
as	O
a	O
random	O
subset	O
of	O
size	O
k	O
from	O
n	O
repeat	O
zi	O
argmink	O
di	O
mk	O
for	O
i	O
mk	O
argminizik	O
di	O
for	O
k	O
k	O
until	O
converged	O
kernelized	O
ridge	B
regression	B
applying	O
the	O
kernel	B
trick	I
to	O
distance-based	O
methods	O
was	O
straightforward	O
it	O
is	O
not	O
so	O
obvious	O
how	O
to	O
apply	O
it	O
to	O
parametric	O
models	O
such	O
as	O
ridge	B
regression	B
however	O
it	O
can	O
be	O
done	O
as	O
we	O
now	O
explain	O
this	O
will	O
serve	O
as	O
a	O
good	O
warm	O
up	O
for	O
studying	O
svms	O
the	O
primal	O
problem	O
let	O
x	O
r	O
want	O
to	O
minimize	O
d	O
be	O
some	O
feature	O
vector	O
and	O
x	O
be	O
the	O
corresponding	O
n	O
d	O
design	B
matrix	I
we	O
jw	O
xwt	O
xw	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
w	O
x	O
id	O
y	O
i	O
xixt	O
i	O
id	O
y	O
the	O
dual	O
problem	O
equation	O
is	O
not	O
yet	O
in	O
the	O
form	O
of	O
inner	O
products	O
however	O
using	O
the	O
matrix	B
inversion	I
lemma	I
we	O
rewrite	O
the	O
ridge	O
estimate	O
as	O
follows	O
w	O
xt	O
in	O
which	O
takes	O
on	O
n	O
time	O
to	O
compute	O
this	O
can	O
be	O
advantageous	O
if	O
d	O
is	O
large	O
furthermore	O
we	O
see	O
that	O
we	O
can	O
partially	O
kernelize	O
this	O
by	O
replacing	O
xxt	O
with	O
the	O
gram	B
matrix	I
k	O
but	O
what	O
about	O
the	O
leading	O
xt	O
term	O
let	O
us	O
define	O
the	O
following	O
dual	B
variables	I
in	O
then	O
we	O
can	O
rewrite	O
the	O
primal	B
variables	I
as	O
follows	O
w	O
xt	O
ixi	O
this	O
tells	O
us	O
that	O
the	O
solution	O
vector	O
is	O
just	O
a	O
linear	O
sum	O
of	O
the	O
n	O
training	O
vectors	O
when	O
we	O
plug	O
this	O
in	O
at	O
test	O
time	O
to	O
compute	O
the	O
predictive	B
mean	B
we	O
get	O
f	O
wt	O
x	O
ixt	O
i	O
x	O
i	O
xi	O
the	O
kernel	B
trick	I
figure	O
visualization	O
of	O
the	O
first	O
kernel	B
principal	B
component	I
basis	B
functions	I
derived	O
from	O
some	O
data	O
we	O
use	O
an	O
rbf	B
kernel	B
with	O
figure	O
generated	O
by	O
kpcascholkopf	O
written	O
by	O
bernhard	O
scholkopf	O
so	O
we	O
have	O
succesfully	O
kernelized	O
ridge	B
regression	B
by	O
changing	O
from	O
primal	O
to	O
dual	B
variables	I
this	O
technique	O
can	O
be	O
applied	O
to	O
many	O
other	O
linear	O
models	O
such	O
as	O
logistic	B
regression	B
computational	O
cost	O
the	O
cost	O
of	O
computing	O
the	O
dual	B
variables	I
is	O
on	O
whereas	O
the	O
cost	O
of	O
computing	O
the	O
primal	B
variables	I
w	O
is	O
hence	O
the	O
kernel	B
method	O
can	O
be	O
useful	O
in	O
high	O
dimensional	O
settings	O
even	O
if	O
we	O
only	O
use	O
a	O
linear	B
kernel	B
the	O
svd	B
trick	O
in	O
equation	O
however	O
prediction	O
using	O
the	O
dual	B
variables	I
takes	O
on	O
d	O
time	O
while	O
prediction	O
using	O
the	O
primal	B
variables	I
only	O
takes	O
od	O
time	O
we	O
can	O
speedup	O
prediction	O
by	O
making	O
sparse	B
as	O
we	O
discuss	O
in	O
section	O
kernel	B
pca	B
in	O
section	O
we	O
saw	O
how	O
we	O
could	O
compute	O
a	O
low-dimensional	O
linear	O
embedding	B
of	O
some	O
data	O
using	O
pca	B
this	O
required	O
finding	O
the	O
eigenvectors	O
of	O
the	O
sample	O
covariance	B
matrix	I
s	O
chapter	O
kernels	O
xixt	O
i	O
x	O
however	O
we	O
can	O
also	O
compute	O
pca	B
by	O
finding	O
the	O
eigenvectors	O
n	O
of	O
the	O
inner	O
product	O
matrix	O
xxt	O
as	O
we	O
show	O
below	O
this	O
will	O
allow	O
us	O
to	O
produce	O
a	O
nonlinear	O
embedding	B
using	O
the	O
kernel	B
trick	I
a	O
method	O
known	O
as	O
kernel	B
pca	B
et	O
al	O
first	O
let	O
u	O
be	O
an	O
orthogonal	O
matrix	O
containing	O
the	O
eigenvectors	O
of	O
xxt	O
with	O
corresponding	O
eigenvalues	O
in	O
by	O
definition	O
we	O
have	O
u	O
pre-multiplying	O
by	O
xt	O
gives	O
xxt	O
u	O
u	O
from	O
which	O
we	O
see	O
that	O
the	O
eigenvectors	O
of	O
xt	O
x	O
hence	O
of	O
s	O
are	O
v	O
xt	O
u	O
with	O
eigenvalues	O
given	O
by	O
as	O
before	O
however	O
these	O
eigenvectors	O
are	O
not	O
normalized	O
since	O
j	O
uj	O
j	O
so	O
the	O
normalized	O
eigenvectors	O
are	O
given	O
by	O
vpca	O
xt	O
u	O
ut	O
j	O
xxt	O
uj	O
jut	O
this	O
is	O
a	O
useful	O
trick	O
for	O
regular	B
pca	B
if	O
d	O
n	O
since	O
xt	O
x	O
has	O
size	O
d	O
d	O
whereas	O
xxt	O
has	O
size	O
n	O
n	O
it	O
will	O
also	O
allow	O
us	O
to	O
use	O
the	O
kernel	B
trick	I
as	O
we	O
now	O
show	O
now	O
let	O
k	O
xxt	O
be	O
the	O
gram	B
matrix	I
recall	B
from	O
mercer	O
s	O
theorem	O
that	O
the	O
use	O
of	O
a	O
kernel	B
implies	O
some	O
underlying	O
feature	O
space	O
so	O
we	O
are	O
implicitly	O
replacing	O
xi	O
with	O
i	O
let	O
be	O
the	O
corresponding	O
design	B
matrix	I
and	O
s	O
i	O
be	O
the	O
corresponding	O
n	O
covariance	B
matrix	I
in	O
feature	O
space	O
the	O
eigenvectors	O
are	O
given	O
by	O
vkpca	O
t	O
u	O
where	O
u	O
and	O
contain	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
k	O
of	O
course	O
we	O
can	O
t	O
actually	O
compute	O
vkpca	O
since	O
i	O
is	O
potentially	O
infinite	O
dimensional	O
however	O
we	O
can	O
compute	O
the	O
projection	B
of	O
a	O
test	O
vector	O
x	O
onto	O
the	O
feature	O
space	O
as	O
follows	O
i	O
i	O
t	O
t	O
vkpca	O
t	O
u	O
kt	O
u	O
where	O
k	O
xn	O
there	O
is	O
one	O
final	O
detail	O
to	O
worry	O
about	O
so	O
far	O
we	O
have	O
assumed	O
the	O
projected	O
data	O
has	O
zero	O
mean	B
which	O
is	O
not	O
the	O
case	O
in	O
general	O
we	O
cannot	O
simply	O
subtract	O
off	O
the	O
mean	B
in	O
feature	O
space	O
however	O
there	O
is	O
a	O
trick	O
we	O
can	O
use	O
define	O
the	O
centered	O
feature	O
vector	O
as	O
i	O
the	O
gram	B
matrix	I
of	O
the	O
centered	O
feature	O
vectors	O
is	O
given	O
by	O
n	O
kij	O
t	O
i	O
j	O
i	O
j	O
n	O
t	O
i	O
k	O
t	O
n	O
t	O
j	O
k	O
n	O
xl	O
t	O
k	O
l	O
xj	O
n	O
xk	O
n	O
xk	O
n	O
this	O
can	O
be	O
expressed	O
in	O
matrix	O
notation	O
as	O
follows	O
n	O
n	O
k	O
hkh	O
where	O
h	O
i	O
is	O
the	O
centering	B
matrix	I
we	O
can	O
convert	O
all	O
this	O
algebra	O
into	O
the	O
pseudocode	O
shown	O
in	O
algorithm	O
whereas	O
linear	O
pca	B
is	O
limited	O
to	O
using	O
l	O
d	O
components	O
in	O
kpca	O
we	O
can	O
use	O
up	O
to	O
n	O
components	O
since	O
the	O
rank	O
of	O
is	O
n	O
d	O
is	O
the	O
infinite	O
dimensionality	O
of	O
embedded	O
feature	O
vectors	O
figure	O
gives	O
an	O
example	O
of	O
the	O
method	O
applied	O
to	O
some	O
d	O
dimensional	O
data	O
using	O
an	O
rbf	B
kernel	B
we	O
project	O
points	O
in	O
the	O
unit	O
grid	O
onto	O
the	O
first	O
where	O
d	O
the	O
kernel	B
trick	I
n	O
algorithm	O
kernel	B
pca	B
input	O
k	O
of	O
size	O
n	O
n	O
k	O
of	O
size	O
n	O
n	O
num	O
latent	B
dimensions	O
l	O
o	O
k	O
k	O
ok	O
ko	O
oko	O
k	O
for	O
i	O
do	O
i	O
vi	O
ui	O
o	O
n	O
k	O
k	O
o	O
k	O
k	O
o	O
o	O
k	O
o	O
z	O
k	O
v	O
pca	B
kpca	O
figure	O
visualization	O
of	O
some	O
data	O
generated	O
by	O
based	O
on	O
code	O
by	O
l	O
j	O
p	O
van	O
der	O
maaten	O
pca	B
projection	B
kernel	B
pca	B
projection	B
figure	O
components	O
and	O
visualize	O
the	O
corresponding	O
surfaces	O
using	O
a	O
contour	O
plot	O
we	O
see	O
that	O
the	O
first	O
two	O
component	O
separate	O
the	O
three	O
clusters	B
and	O
following	O
components	O
split	O
the	O
clusters	B
although	O
the	O
features	B
learned	O
by	O
kpca	O
can	O
be	O
useful	O
for	O
classification	O
et	O
al	O
they	O
are	O
not	O
necessarily	O
so	O
useful	O
for	O
data	O
visualization	O
for	O
example	O
figure	O
shows	O
the	O
projection	B
of	O
the	O
data	O
from	O
figure	O
onto	O
the	O
first	O
principal	O
bases	O
computed	O
using	O
pca	B
and	O
kpca	O
obviously	O
pca	B
perfectly	O
represents	O
the	O
data	O
kpca	O
represents	O
each	O
cluster	O
by	O
a	O
different	O
line	O
of	O
course	O
there	O
is	O
no	O
need	O
to	O
project	O
data	O
back	O
into	O
so	O
let	O
us	O
consider	O
a	O
different	O
data	O
set	O
we	O
will	O
use	O
a	O
dimensional	O
data	O
set	O
representing	O
the	O
three	O
known	O
phases	O
of	O
flow	O
in	O
an	O
oil	O
pipeline	B
data	O
which	O
is	O
widely	O
used	O
to	O
compare	O
data	O
visualization	O
methods	O
is	O
synthetic	O
and	O
comes	O
from	O
and	O
james	O
we	O
project	O
this	O
into	O
using	O
pca	B
and	O
kpca	O
an	O
rbf	B
kernel	B
the	O
results	O
are	O
shown	O
in	O
figure	O
if	O
we	O
perform	O
nearest	B
neighbor	I
classification	O
in	O
the	O
low-dimensional	O
space	O
kpca	O
makes	O
errors	O
and	O
pca	B
makes	O
chapter	O
kernels	O
figure	O
representation	O
of	O
dimensional	O
oil	O
flow	O
data	O
the	O
different	O
colorssymbols	O
represent	O
the	O
phases	O
of	O
oil	O
flow	O
pca	B
kernel	B
pca	B
with	O
gaussian	B
kernel	B
compare	O
to	O
figure	O
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
neil	O
lawrence	O
nevertheless	O
the	O
kpca	O
projection	B
is	O
rather	O
unnatural	O
how	O
to	O
make	O
kernelized	O
versions	O
of	O
probabilistic	B
pca	B
in	O
section	O
we	O
will	O
discuss	O
note	O
that	O
there	O
is	O
a	O
close	O
connection	O
between	O
kernel	B
pca	B
and	O
a	O
technique	O
known	O
as	O
multidimensional	B
scaling	I
or	O
mds	B
this	O
methods	O
finds	O
a	O
low-dimensional	O
embedding	B
such	O
that	O
euclidean	B
distance	I
in	O
the	O
embedding	B
space	O
approximates	O
the	O
original	O
dissimilarity	B
matrix	I
see	O
e	O
g	O
for	O
details	O
support	B
vector	I
machines	I
in	O
section	O
we	O
saw	O
one	O
way	O
to	O
derive	O
a	O
sparse	B
kernel	B
machine	I
namely	O
by	O
using	O
a	O
glm	B
with	O
kernel	B
basis	B
functions	I
plus	O
a	O
sparsity-promoting	B
prior	I
such	O
as	O
or	O
ard	B
an	O
alternative	O
approach	O
is	O
to	O
change	O
the	O
objective	O
function	O
from	O
negative	B
log	I
likelihood	B
to	O
some	O
other	O
loss	B
in	O
particular	O
consider	O
the	O
regularized	O
empirical	O
function	O
as	O
we	O
discussed	O
in	O
section	O
risk	B
function	O
jw	O
lyi	O
yi	O
where	O
yi	O
wt	O
xi	O
moment	O
defined	O
in	O
equation	O
this	O
is	O
equivalent	O
to	O
logistic	B
regression	B
far	O
this	O
is	O
in	O
the	O
original	O
feature	O
space	O
we	O
introduce	O
kernels	O
in	O
a	O
if	O
l	O
is	O
quadratic	B
loss	B
this	O
is	O
equivalent	O
to	O
ridge	B
regression	B
and	O
if	O
l	O
is	O
the	O
log-loss	B
in	O
the	O
ridge	B
regression	B
case	O
we	O
know	O
that	O
the	O
solution	O
to	O
this	O
has	O
the	O
form	O
w	O
x	O
y	O
and	O
plug-in	B
predictions	O
take	O
the	O
form	O
wt	O
x	O
as	O
we	O
saw	O
in	O
section	O
i	O
we	O
can	O
rewrite	O
these	O
equations	O
in	O
a	O
way	O
that	O
only	O
involves	O
inner	O
products	O
of	O
the	O
form	O
xt	O
which	O
we	O
can	O
replace	O
by	O
calls	O
to	O
a	O
kernel	B
function	I
this	O
is	O
kernelized	O
but	O
not	O
sparse	B
however	O
if	O
we	O
replace	O
the	O
quadratic	O
log-loss	B
with	O
some	O
other	O
loss	B
function	I
to	O
be	O
explained	O
below	O
we	O
can	O
ensure	O
that	O
the	O
solution	O
is	O
sparse	B
so	O
that	O
predictions	O
only	O
depend	O
on	O
a	O
subset	O
of	O
the	O
training	O
data	O
known	O
as	O
support	B
vectors	I
this	O
combination	O
of	O
the	O
kernel	B
trick	I
plus	O
a	O
modified	O
loss	B
function	I
is	O
known	O
as	O
a	O
support	B
vector	I
machine	I
or	O
svm	B
this	O
technique	O
was	O
support	B
vector	I
machines	I
insensitive	O
huber	O
yx	O
y	O
y	O
y	O
x	O
figure	O
illustration	O
of	O
huber	O
and	O
loss	B
functions	O
where	O
figure	O
generated	O
illustration	O
of	O
the	O
used	O
in	O
svm	B
regression	B
points	O
above	O
the	O
tube	B
have	O
by	O
huberlossdemo	O
i	O
and	O
i	O
points	O
inside	O
the	O
tube	B
have	O
i	O
i	O
points	O
below	O
the	O
tube	B
have	O
i	O
and	O
i	O
based	O
on	O
figure	O
of	O
originally	O
designed	O
for	O
binary	O
classification	O
but	O
can	O
be	O
extended	O
to	O
regression	B
and	O
multi-class	O
classification	O
as	O
we	O
explain	O
below	O
note	O
that	O
svms	O
are	O
very	O
unnatural	O
from	O
a	O
probabilistic	O
point	O
of	O
view	O
first	O
they	O
encode	O
sparsity	B
in	O
the	O
loss	B
function	I
rather	O
than	O
the	O
prior	O
second	O
they	O
encode	O
kernels	O
by	O
using	O
an	O
algorithmic	O
trick	O
rather	O
than	O
being	O
an	O
explicit	O
part	O
of	O
the	O
model	O
finally	O
svms	O
do	O
not	O
result	O
in	O
probabilistic	O
outputs	O
which	O
causes	O
various	O
difficulties	O
especially	O
in	O
the	O
multi-class	O
classification	O
setting	O
section	O
for	O
details	O
it	O
is	O
possible	O
to	O
obtain	O
sparse	B
probabilistic	O
multi-class	O
kernel-based	O
classifiers	O
which	O
work	O
as	O
well	O
or	O
better	O
than	O
svms	O
using	O
techniques	O
such	O
as	O
the	O
or	O
rvm	B
discussed	O
in	O
section	O
however	O
we	O
include	O
a	O
discussion	O
of	O
svms	O
despite	O
their	O
non-probabilistic	O
nature	O
for	O
two	O
main	O
reasons	O
first	O
they	O
are	O
very	O
popular	O
and	O
widely	O
used	O
so	O
all	O
students	O
of	O
machine	B
learning	B
should	O
know	O
about	O
them	O
second	O
they	O
have	O
some	O
computational	O
advantages	O
over	O
probabilistic	O
methods	O
in	O
the	O
structured	B
output	I
case	O
see	O
section	O
svms	O
for	O
regression	B
the	O
problem	O
with	O
kernelized	O
ridge	B
regression	B
is	O
that	O
the	O
solution	O
vector	O
w	O
depends	O
on	O
all	O
the	O
training	O
inputs	O
we	O
now	O
seek	O
a	O
method	O
to	O
produce	O
a	O
sparse	B
estimate	O
vapnik	O
et	O
al	O
proposed	O
a	O
variant	O
of	O
the	O
huber	B
loss	B
function	I
called	O
the	O
epsilon	B
insensitive	I
loss	B
function	I
defined	O
by	O
ly	O
y	O
y	O
if	O
y	O
otherwise	O
this	O
means	O
that	O
any	O
point	O
lying	O
inside	O
an	O
around	O
the	O
prediction	O
is	O
not	O
penalized	O
as	O
in	O
figure	O
the	O
corresponding	O
objective	O
function	O
is	O
usually	O
written	O
in	O
the	O
following	O
form	O
j	O
c	O
lyi	O
yi	O
chapter	O
kernels	O
where	O
yi	O
f	O
t	O
xi	O
and	O
c	O
is	O
a	O
regularization	B
constant	O
this	O
objective	O
is	O
convex	B
and	O
unconstrained	O
but	O
not	O
differentiable	O
because	O
of	O
the	O
absolute	O
value	O
function	O
in	O
the	O
loss	B
term	O
as	O
in	O
section	O
where	O
we	O
discussed	O
the	O
lasso	B
problem	O
there	O
are	O
several	O
possible	O
algorithms	O
we	O
could	O
use	O
one	O
popular	O
approach	O
is	O
to	O
formulate	O
the	O
problem	O
as	O
a	O
constrained	O
in	O
particular	O
we	O
introduce	O
slack	B
variables	I
to	O
represent	O
the	O
degree	B
to	O
optimization	B
problem	O
which	O
each	O
point	O
lies	O
outside	O
the	O
tube	B
yi	O
f	O
yi	O
f	O
i	O
i	O
given	O
this	O
we	O
can	O
rewrite	O
the	O
objective	O
as	O
follows	O
j	O
c	O
i	O
i	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
w	O
and	O
must	O
be	O
minimized	O
subject	O
to	O
the	O
linear	O
constraints	O
i	O
this	O
is	O
a	O
in	O
equations	O
as	O
well	O
as	O
the	O
positivity	O
constraints	O
standard	O
quadratic	B
program	I
in	O
d	O
variables	O
i	O
and	O
one	O
can	O
show	O
e	O
g	O
and	O
smola	O
that	O
the	O
optimal	O
solution	O
has	O
the	O
form	O
w	O
ixi	O
i	O
where	O
i	O
furthermore	O
it	O
turns	O
out	O
that	O
the	O
vector	O
is	O
sparse	B
because	O
we	O
don	O
t	O
care	O
about	O
errors	O
which	O
are	O
smaller	O
than	O
the	O
xi	O
for	O
which	O
i	O
are	O
called	O
the	O
support	B
vectors	I
thse	O
are	O
points	O
for	O
which	O
the	O
errors	O
lie	O
on	O
or	O
outside	O
the	O
tube	B
once	O
the	O
model	O
is	O
trained	O
we	O
can	O
then	O
make	O
predictions	O
using	O
yx	O
wt	O
x	O
plugging	O
in	O
the	O
definition	O
of	O
w	O
we	O
get	O
yx	O
ixt	O
i	O
x	O
finally	O
we	O
can	O
replace	O
xt	O
i	O
x	O
with	O
x	O
to	O
get	O
a	O
kernelized	O
solution	O
i	O
yx	O
i	O
x	O
i	O
svms	O
for	O
classification	O
we	O
now	O
discuss	O
how	O
to	O
apply	O
svms	O
to	O
classification	O
we	O
first	O
focus	O
on	O
the	O
binary	O
case	O
and	O
then	O
discuss	O
the	O
multi-class	O
case	O
in	O
section	O
hinge	B
loss	B
in	O
section	O
we	O
showed	O
that	O
the	O
negative	B
log	I
likelihood	B
of	O
a	O
logistic	B
regression	B
model	O
l	O
nlly	O
log	O
pyx	O
w	O
e	O
y	O
support	B
vector	I
machines	I
was	O
a	O
convex	B
upper	O
bound	O
on	O
the	O
risk	B
of	O
a	O
binary	O
classifier	O
where	O
f	O
t	O
x	O
is	O
the	O
log	O
odds	O
ratio	O
and	O
we	O
have	O
assumed	O
the	O
labels	O
are	O
y	O
rather	O
than	O
in	O
this	O
section	O
we	O
replace	O
the	O
nll	B
loss	B
with	O
the	O
hinge	B
loss	B
defined	O
as	O
lhingey	O
y	O
y	O
here	O
f	O
is	O
our	O
confidence	O
in	O
choosing	O
label	B
y	O
however	O
it	O
need	O
not	O
have	O
any	O
probabilistic	O
semantics	O
see	O
figure	O
for	O
a	O
plot	O
we	O
see	O
that	O
the	O
function	O
looks	O
like	O
a	O
door	O
hinge	O
hence	O
its	O
name	O
the	O
overall	O
objective	O
has	O
the	O
form	O
min	O
c	O
yif	O
once	O
again	O
this	O
is	O
non-differentiable	O
because	O
of	O
the	O
max	O
term	O
however	O
by	O
introducing	O
slack	B
variables	I
i	O
one	O
can	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
solving	O
min	O
c	O
i	O
s	O
t	O
i	O
yixt	O
i	O
w	O
i	O
i	O
this	O
is	O
a	O
quadratic	B
program	I
in	O
n	O
d	O
variables	O
subjet	O
to	O
on	O
constraints	O
we	O
can	O
eliminate	O
the	O
primal	B
variables	I
w	O
and	O
i	O
and	O
just	O
solve	O
the	O
n	O
dual	B
variables	I
which	O
correspond	O
to	O
the	O
lagrange	B
multipliers	I
for	O
the	O
constraints	O
standard	O
solvers	O
take	O
on	O
time	O
however	O
specialized	O
algorithms	O
which	O
avoid	O
the	O
use	O
of	O
generic	O
qp	B
solvers	O
have	O
been	O
developed	O
for	O
this	O
problem	O
such	O
as	O
the	O
sequential	B
minimal	B
optimization	B
or	O
smo	B
algorithm	O
in	O
practice	O
this	O
can	O
take	O
on	O
however	O
even	O
this	O
can	O
be	O
too	O
slow	O
if	O
n	O
is	O
large	O
in	O
such	O
settings	O
it	O
is	O
common	O
to	O
use	O
linear	O
svms	O
which	O
take	O
on	O
time	O
to	O
train	O
bottou	O
et	O
al	O
one	O
can	O
show	O
that	O
the	O
solution	O
has	O
the	O
form	O
w	O
ixi	O
i	O
where	O
i	O
iyi	O
and	O
where	O
is	O
sparse	B
of	O
the	O
hinge	B
loss	B
the	O
xi	O
for	O
which	O
i	O
are	O
called	O
support	B
vectors	I
these	O
are	O
points	O
which	O
are	O
either	O
incorrectly	O
classified	O
or	O
are	O
classified	O
correctly	O
but	O
are	O
on	O
or	O
inside	O
the	O
margin	B
disuss	O
margins	O
below	O
see	O
figure	O
for	O
an	O
illustration	O
at	O
test	O
time	O
prediction	O
is	O
done	O
using	O
yx	O
sgnf	O
sgn	O
wt	O
x	O
yx	O
sgn	O
i	O
x	O
using	O
equation	O
and	O
the	O
kernel	B
trick	I
we	O
have	O
this	O
takes	O
osd	O
time	O
to	O
compute	O
where	O
s	O
n	O
is	O
the	O
number	O
of	O
support	B
vectors	I
this	O
depends	O
on	O
the	O
sparsity	B
level	O
and	O
hence	O
on	O
the	O
regularizer	O
c	O
chapter	O
kernels	O
figure	O
right	O
a	O
separating	O
hyper-plane	O
with	O
small	O
margin	B
illustration	O
of	O
the	O
large	B
margin	B
principle	I
left	O
a	O
separating	O
hyper-plane	O
with	O
large	O
margin	B
y	O
y	O
y	O
w	O
x	O
r	O
f	O
x	O
y	O
y	O
y	O
figure	O
illustration	O
of	O
the	O
geometry	O
of	O
a	O
linear	O
decision	B
boundary	I
in	O
a	O
point	O
x	O
is	O
classified	O
as	O
belonging	O
in	O
decision	B
region	O
if	O
f	O
otherwise	O
it	O
belongs	O
in	O
decision	B
region	O
here	O
f	O
is	O
known	O
as	O
a	O
discriminant	B
function	I
the	O
decision	B
boundary	I
is	O
the	O
set	O
of	O
points	O
such	O
that	O
f	O
w	O
is	O
a	O
vector	O
which	O
is	O
perpendicular	O
to	O
the	O
decision	B
boundary	I
the	O
term	O
controls	O
the	O
distance	O
of	O
the	O
decision	B
boundary	I
from	O
the	O
origin	O
the	O
signed	O
distance	O
of	O
x	O
from	O
its	O
orthogonal	B
projection	B
onto	O
the	O
decision	B
boundary	I
x	O
is	O
given	O
by	O
f	O
based	O
on	O
figure	O
of	O
illustration	O
of	O
the	O
soft	O
margin	B
principle	O
points	O
with	O
circles	O
around	O
them	O
are	O
support	B
vectors	I
we	O
also	O
indicate	O
the	O
value	O
of	O
the	O
corresponding	O
slack	B
variables	I
based	O
on	O
figure	O
of	O
support	B
vector	I
machines	I
the	O
large	B
margin	B
principle	I
in	O
this	O
section	O
we	O
derive	O
equation	O
form	O
a	O
completely	O
different	O
perspective	O
recall	B
that	O
our	O
goal	O
is	O
to	O
derive	O
a	O
discriminant	B
function	I
f	O
which	O
will	O
be	O
linear	O
in	O
the	O
feature	O
space	O
implied	O
by	O
the	O
choice	O
of	O
kernel	B
consider	O
a	O
point	O
x	O
in	O
this	O
induced	O
space	O
referring	O
to	O
figure	O
we	O
see	O
that	O
x	O
x	O
r	O
w	O
where	O
r	O
is	O
the	O
distance	O
of	O
x	O
from	O
the	O
decision	B
boundary	I
whose	O
normal	B
vector	O
is	O
w	O
and	O
x	O
is	O
the	O
orthogonal	B
projection	B
of	O
x	O
onto	O
this	O
boundary	O
hence	O
f	O
t	O
x	O
x	O
wt	O
w	O
now	O
f	O
so	O
t	O
x	O
hence	O
f	O
r	O
wt	O
w	O
we	O
would	O
like	O
to	O
make	O
this	O
distance	O
r	O
f	O
as	O
large	O
as	O
possible	O
for	O
reasons	O
illustrated	O
in	O
figure	O
in	O
particular	O
there	O
might	O
be	O
many	O
lines	O
that	O
perfectly	O
separate	O
the	O
training	O
data	O
if	O
we	O
work	O
in	O
a	O
high	O
dimensional	O
feature	O
space	O
but	O
intuitively	O
the	O
best	O
one	O
to	O
pick	O
is	O
the	O
one	O
that	O
maximizes	O
the	O
margin	B
i	O
e	O
the	O
perpendicular	O
distance	O
to	O
the	O
closest	O
point	O
in	O
addition	O
we	O
want	O
to	O
ensure	O
each	O
point	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
boundary	O
hence	O
we	O
want	O
f	O
so	O
our	O
objective	O
becomes	O
wt	O
w	O
and	O
r	O
f	O
yiwt	O
xi	O
n	O
min	O
max	O
note	O
that	O
by	O
rescaling	O
the	O
parameters	O
using	O
w	O
kw	O
and	O
we	O
do	O
not	O
change	O
the	O
distance	O
of	O
any	O
point	O
to	O
the	O
boundary	O
since	O
the	O
k	O
factor	B
cancels	O
out	O
when	O
we	O
divide	O
by	O
therefore	O
let	O
us	O
define	O
the	O
scale	O
factor	B
such	O
that	O
yifi	O
for	O
the	O
point	O
that	O
is	O
closest	O
to	O
the	O
decision	B
boundary	I
we	O
therefore	O
want	O
to	O
optimize	O
min	O
s	O
t	O
yiwt	O
xi	O
i	O
fact	O
of	O
is	O
added	O
for	O
convenience	O
and	O
doesn	O
t	O
affect	O
the	O
optimal	O
parameters	O
the	O
constraint	O
says	O
that	O
we	O
want	O
all	O
points	O
to	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
with	O
a	O
margin	B
of	O
at	O
least	O
for	O
this	O
reason	O
we	O
say	O
that	O
an	O
svm	B
is	O
an	O
example	O
of	O
a	O
large	O
margin	B
classifier	O
if	O
the	O
data	O
is	O
not	O
linearly	B
separable	I
after	O
using	O
the	O
kernel	B
trick	I
there	O
will	O
be	O
no	O
feasible	O
solution	O
in	O
which	O
yifi	O
for	O
all	O
i	O
we	O
therefore	O
introduce	O
slack	B
variables	I
i	O
such	O
that	O
i	O
if	O
the	O
point	O
is	O
on	O
or	O
inside	O
the	O
correct	O
margin	B
boundary	O
and	O
i	O
fi	O
otherwise	O
if	O
i	O
the	O
point	O
lies	O
inside	O
the	O
margin	B
but	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
if	O
i	O
the	O
point	O
lies	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
boundary	I
see	O
figure	O
we	O
replace	O
the	O
hard	O
constraints	O
that	O
yifi	O
with	O
the	O
soft	B
margin	B
constraints	I
that	O
yifi	O
i	O
the	O
new	O
objective	O
becomes	O
i	O
s	O
t	O
i	O
yixt	O
i	O
w	O
i	O
min	O
c	O
chapter	O
kernels	O
correct	O
log	O
odds	O
rvm	B
yx	O
svm	B
yx	O
figure	O
log-odds	O
vs	O
x	O
for	O
different	O
methods	O
based	O
on	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
mike	O
tipping	O
which	O
is	O
the	O
same	O
as	O
equation	O
since	O
i	O
means	O
point	O
i	O
is	O
misclassified	O
we	O
can	O
interpret	O
i	O
i	O
as	O
an	O
upper	O
bound	O
on	O
the	O
number	O
of	O
misclassified	O
points	O
the	O
parameter	B
c	O
is	O
a	O
regularization	B
parameter	B
that	O
controls	O
the	O
number	O
of	O
errors	O
we	O
are	O
it	O
is	O
common	O
to	O
define	O
this	O
using	O
c	O
n	O
where	O
willing	O
to	O
tolerate	O
on	O
the	O
training	B
set	I
controls	O
the	O
fraction	O
of	O
misclassified	O
points	O
that	O
we	O
allow	O
during	O
the	O
training	O
phase	B
this	O
is	O
called	O
a	O
classifier	O
this	O
is	O
usually	O
set	O
using	O
cross-validation	O
section	O
probabilistic	O
output	O
an	O
svm	B
classifier	O
produces	O
a	O
hard-labeling	O
yx	O
signf	O
however	O
we	O
often	O
want	O
a	O
measure	O
of	O
confidence	O
in	O
our	O
prediction	O
one	O
heuristic	O
approach	O
is	O
to	O
interpret	O
f	O
as	O
the	O
log-odds	B
ratio	I
log	O
we	O
can	O
then	O
convert	O
the	O
output	O
of	O
an	O
svm	B
to	O
a	O
probability	O
using	O
py	O
where	O
a	O
b	O
can	O
be	O
estimated	O
by	O
maximum	O
likelihood	B
on	O
a	O
separate	O
validation	B
set	I
the	O
training	B
set	I
to	O
estimate	O
a	O
and	O
b	O
leads	O
to	O
severe	O
overfitting	O
this	O
technique	O
was	O
first	O
proposed	O
in	O
however	O
the	O
resulting	O
probabilities	O
are	O
not	O
particularly	O
well	O
calibrated	O
since	O
there	O
is	O
nothing	O
in	O
the	O
svm	B
training	O
procedure	O
that	O
justifies	O
interpreting	O
f	O
as	O
a	O
log-odds	B
ratio	I
to	O
illustrate	O
this	O
consider	O
an	O
example	O
from	O
suppose	O
we	O
have	O
data	O
where	O
pxy	O
and	O
pxy	O
since	O
the	O
class-conditional	O
distributions	O
overlap	O
in	O
the	O
middle	O
the	O
log-odds	O
of	O
class	O
over	O
class	O
should	O
be	O
zero	O
in	O
and	O
infinite	O
outside	O
this	O
region	O
we	O
sampled	O
points	O
from	O
the	O
model	O
and	O
then	O
fit	O
an	O
rvm	B
and	O
an	O
svm	B
with	O
a	O
gaussian	B
kenel	O
of	O
width	O
both	O
models	O
can	O
perfectly	O
capture	O
the	O
decision	B
boundary	I
and	O
achieve	O
a	O
generalizaton	O
error	O
of	O
which	O
is	O
bayes	O
optimal	O
in	O
this	O
problem	O
the	O
probabilistic	O
output	O
from	O
the	O
rvm	B
is	O
a	O
good	O
approximation	O
to	O
the	O
true	O
log-odds	O
but	O
this	O
is	O
not	O
the	O
case	O
for	O
the	O
svm	B
as	O
shown	O
in	O
figure	O
support	B
vector	I
machines	I
not	O
not	O
the	O
one-versus-rest	O
approach	O
the	O
green	O
region	O
is	O
predicted	O
to	O
be	O
both	O
class	O
and	O
class	O
the	O
one-versus-one	B
approach	O
the	O
label	B
of	O
the	O
green	O
region	O
is	O
ambiguous	O
based	O
on	O
figure	O
of	O
figure	O
svms	O
for	O
multi-class	O
classification	O
in	O
section	O
we	O
saw	O
how	O
we	O
could	O
upgrade	O
a	O
binary	O
logistic	B
regression	B
model	O
to	O
the	O
multiclass	O
case	O
by	O
replacing	O
the	O
sigmoid	B
function	O
with	O
the	O
softmax	B
and	O
the	O
bernoulli	B
distribution	O
with	O
the	O
multinomial	B
upgrading	O
an	O
svm	B
to	O
the	O
multi-class	O
case	O
is	O
not	O
so	O
easy	O
since	O
the	O
outputs	O
are	O
not	O
on	O
a	O
calibrated	O
scale	O
and	O
hence	O
are	O
hard	O
to	O
compare	O
to	O
each	O
other	O
the	O
obvious	O
approach	O
is	O
to	O
use	O
a	O
one-versus-the-rest	B
approach	O
called	O
one-vs-all	B
in	O
which	O
we	O
train	O
c	O
binary	O
classifiers	O
fcx	O
where	O
the	O
data	O
from	O
class	O
c	O
is	O
treated	O
as	O
positive	O
and	O
the	O
data	O
from	O
all	O
the	O
other	O
classes	O
is	O
treated	O
as	O
negative	O
however	O
this	O
can	O
result	O
in	O
regions	O
of	O
input	O
space	O
which	O
are	O
ambiguously	O
labeled	O
as	O
shown	O
in	O
figure	O
a	O
common	O
alternative	O
is	O
to	O
pick	O
yx	O
arg	O
maxc	O
fcx	O
however	O
this	O
technique	O
may	O
not	O
work	O
either	O
since	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
different	O
fc	O
functions	O
have	O
comparable	O
magnitudes	O
in	O
addition	O
each	O
binary	O
subproblem	O
is	O
likely	O
to	O
suffer	O
from	O
the	O
class	B
imbalance	I
problem	O
to	O
see	O
this	O
suppose	O
we	O
have	O
equally	O
represented	O
classes	O
when	O
training	O
we	O
will	O
have	O
positive	B
examples	I
and	O
negative	B
examples	I
which	O
can	O
hurt	O
performance	O
it	O
is	O
possible	O
to	O
devise	O
ways	O
to	O
train	O
all	O
c	O
classifiers	O
simultaneously	O
and	O
watkins	O
but	O
the	O
resulting	O
method	O
takes	O
oc	O
time	O
instead	O
of	O
the	O
usual	O
ocn	O
time	O
another	O
approach	O
is	O
to	O
use	O
the	O
one-versus-one	B
or	O
ovo	O
approach	O
also	O
called	O
all	B
pairs	I
in	O
which	O
we	O
train	O
cc	O
classifiers	O
to	O
discriminate	O
all	B
pairs	I
we	O
then	O
classify	O
a	O
point	O
into	O
the	O
class	O
which	O
has	O
the	O
highest	O
number	O
of	O
votes	O
however	O
this	O
can	O
also	O
result	O
in	O
ambiguities	O
as	O
shown	O
in	O
figure	O
also	O
it	O
takes	O
oc	O
time	O
to	O
train	O
and	O
oc	O
to	O
test	O
each	O
data	O
point	O
where	O
nsv	O
is	O
the	O
number	O
of	O
support	B
see	O
also	O
et	O
al	O
for	O
an	O
approach	O
based	O
on	O
error-correcting	B
output	I
codes	I
it	O
is	O
worth	O
remembering	O
that	O
all	O
of	O
these	O
difficulties	O
and	O
the	O
plethora	O
of	O
heuristics	B
that	O
have	O
been	O
proposed	O
to	O
fix	O
them	O
fundamentally	O
arise	O
because	O
svms	O
do	O
not	O
model	O
uncertainty	B
using	O
probabilities	O
so	O
their	O
output	O
scores	B
are	O
not	O
comparable	O
across	O
classes	O
we	O
can	O
reduce	O
the	O
test	O
time	O
by	O
structuring	O
the	O
classes	O
into	O
a	O
dag	B
acyclic	O
graph	B
and	O
performing	O
oc	O
pairwise	O
comparisons	O
et	O
al	O
however	O
the	O
factor	B
in	O
the	O
training	O
time	O
is	O
unavoidable	O
r	O
o	O
r	O
r	O
e	O
v	O
c	O
chapter	O
kernels	O
r	O
o	O
r	O
r	O
e	O
v	O
c	O
c	O
c	O
figure	O
a	O
cross	B
validation	I
estimate	O
of	O
the	O
error	O
for	O
an	O
svm	B
classifier	O
with	O
rbf	B
kernel	B
with	O
different	O
precisions	O
and	O
different	O
regularizer	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
drawn	O
from	O
a	O
mixture	B
of	I
gaussians	I
a	O
slice	O
through	O
this	O
surface	O
for	O
the	O
red	O
dotted	O
line	O
is	O
the	O
bayes	O
optimal	O
error	O
computed	O
using	O
bayes	B
rule	I
applied	O
to	O
the	O
model	O
used	O
to	O
generate	O
the	O
data	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
svmcgammademo	O
choosing	O
c	O
svms	O
for	O
both	O
classification	O
and	O
regression	B
require	O
that	O
you	O
specify	O
the	O
kernel	B
function	I
and	O
the	O
parameter	B
c	O
typically	O
c	O
is	O
chosen	O
by	O
cross-validation	O
note	O
however	O
that	O
c	O
interacts	O
quite	O
strongly	O
with	O
the	O
kernel	B
parameters	O
for	O
example	O
suppose	O
we	O
are	O
using	O
an	O
rbf	B
kernel	B
with	O
precision	B
if	O
corresponding	O
to	O
narrow	O
kernels	O
we	O
need	O
heavy	O
regularization	B
and	O
hence	O
small	O
c	O
is	O
big	O
if	O
a	O
larger	O
value	O
of	O
c	O
should	O
be	O
used	O
so	O
we	O
see	O
that	O
and	O
c	O
are	O
tightly	O
coupled	O
this	O
is	O
illustrated	O
in	O
figure	O
which	O
shows	O
the	O
cv	B
estimate	O
of	O
the	O
risk	B
as	O
a	O
function	O
of	O
c	O
and	O
the	O
authors	O
of	O
libsvm	O
recommend	O
et	O
al	O
using	O
cv	B
over	O
a	O
grid	O
with	O
values	O
c	O
in	O
addition	O
it	O
is	O
important	O
to	O
standardize	O
the	O
data	O
first	O
for	O
a	O
spherical	B
gaussian	B
kernel	B
to	O
make	O
sense	O
to	O
choose	O
c	O
efficiently	O
one	O
can	O
develop	O
a	O
path	B
following	O
algorithm	O
in	O
the	O
spirit	O
of	O
lars	B
the	O
basic	O
idea	O
is	O
to	O
start	O
with	O
large	O
so	O
that	O
the	O
margin	B
is	O
wide	O
and	O
hence	O
all	O
points	O
are	O
inside	O
of	O
it	O
and	O
have	O
i	O
by	O
slowly	O
decreasing	O
a	O
small	O
set	O
of	O
points	O
will	O
move	O
from	O
inside	O
the	O
margin	B
to	O
outside	O
and	O
their	O
i	O
values	O
will	O
change	O
from	O
to	O
as	O
they	O
cease	O
to	O
be	O
support	B
vectors	I
when	O
is	O
maximal	O
the	O
function	O
is	O
completely	O
smoothed	O
and	O
no	O
support	B
vectors	I
remain	O
see	O
et	O
al	O
for	O
the	O
details	O
and	O
summary	O
of	O
key	O
points	O
summarizing	O
the	O
above	O
discussion	O
we	O
recognize	O
that	O
svm	B
classifiers	O
involve	O
three	O
key	O
ingredients	O
the	O
kernel	B
trick	I
sparsity	B
and	O
the	O
large	B
margin	B
principle	I
the	O
kernel	B
trick	I
is	O
necessary	O
to	O
prevent	O
underfitting	O
i	O
e	O
to	O
ensure	O
that	O
the	O
feature	O
vector	O
is	O
sufficiently	O
rich	O
that	O
a	O
linear	O
classifier	O
can	O
separate	O
the	O
data	O
from	O
section	O
that	O
any	O
mercer	B
kernel	B
can	O
be	O
viewed	O
as	O
implicitly	O
defining	O
a	O
potentially	O
high	O
dimensional	O
feature	O
vector	O
if	O
the	O
original	O
features	B
are	O
already	O
high	O
dimensional	O
in	O
many	O
gene	O
expression	O
and	O
text	O
classification	O
problems	O
it	O
suffices	O
to	O
use	O
a	O
linear	B
kernel	B
which	O
is	O
equivalent	O
to	O
working	O
with	O
the	O
original	O
features	B
t	O
comparison	O
of	O
discriminative	B
kernel	B
methods	O
method	O
opt	O
w	O
convex	B
convex	B
not	O
convex	B
rvm	B
convex	B
svm	B
gp	O
na	O
opt	O
kernel	B
eb	B
cv	B
eb	B
cv	B
eb	B
sparse	B
no	O
yes	O
yes	O
yes	O
no	O
prob	O
multiclass	O
non-mercer	O
yes	O
yes	O
yes	O
no	O
yes	O
yes	O
yes	O
yes	O
indirectly	O
yes	O
yes	O
yes	O
yes	O
no	O
no	O
section	O
table	O
comparison	O
of	O
various	O
kernel	B
based	O
classifiers	O
eb	B
empirical	B
bayes	I
cv	B
cross	B
validation	I
see	O
text	O
for	O
details	O
the	O
sparsity	B
and	O
large	O
margin	B
principles	O
are	O
necessary	O
to	O
prevent	O
overfitting	O
i	O
e	O
to	O
ensure	O
that	O
we	O
do	O
not	O
use	O
all	O
the	O
basis	B
functions	I
these	O
two	O
ideas	O
are	O
closely	O
related	O
to	O
each	O
other	O
and	O
both	O
arise	O
this	O
case	O
from	O
the	O
use	O
of	O
the	O
hinge	B
loss	B
function	I
however	O
there	O
are	O
other	O
methods	O
of	O
achieving	O
sparsity	B
as	O
and	O
also	O
other	O
methods	O
of	O
maximizing	O
the	O
margin	B
as	O
boosting	B
a	O
deeper	O
discussion	O
of	O
this	O
point	O
takes	O
us	O
outside	O
of	O
the	O
scope	B
of	O
this	O
book	O
see	O
e	O
g	O
et	O
al	O
for	O
more	O
information	B
a	O
probabilistic	O
interpretation	O
of	O
svms	O
in	O
section	O
we	O
saw	O
how	O
to	O
use	O
kernels	O
inside	O
glms	O
to	O
derive	O
probabilistic	O
classifiers	O
such	O
as	O
the	O
and	O
rvm	B
and	O
in	O
section	O
we	O
will	O
discuss	O
gaussian	B
process	I
classifiers	O
which	O
also	O
use	O
kernels	O
however	O
all	O
of	O
these	O
approaches	O
use	O
a	O
logistic	B
or	O
probit	B
likelihood	B
as	O
opposed	O
to	O
the	O
hinge	B
loss	B
used	O
by	O
svms	O
it	O
is	O
natural	O
to	O
wonder	O
if	O
one	O
can	O
interpret	O
the	O
svm	B
more	O
directly	O
as	O
a	O
probabilistic	O
model	O
to	O
do	O
so	O
we	O
must	O
interpret	O
cgm	O
as	O
a	O
negative	B
log	I
likelihood	B
where	O
gm	B
m	O
where	O
m	O
yf	O
is	O
the	O
margin	B
hence	O
py	O
exp	O
cgf	O
and	O
py	O
exp	O
cg	O
f	O
by	O
summing	O
over	O
both	O
values	O
of	O
y	O
we	O
require	O
that	O
exp	O
cgf	O
exp	O
cg	O
f	O
be	O
a	O
constant	O
independent	O
of	O
f	O
but	O
it	O
turns	O
out	O
this	O
is	O
not	O
possible	O
for	O
any	O
c	O
however	O
if	O
we	O
are	O
willing	O
to	O
relax	O
the	O
sum-to-one	O
condition	O
and	O
work	O
with	O
a	O
pseudolikelihood	O
we	O
can	O
derive	O
a	O
probabilistic	O
interpretation	O
of	O
the	O
hinge	B
loss	B
and	O
scott	O
in	O
particular	O
one	O
can	O
show	O
that	O
i	O
exp	O
i	O
yixt	O
i	O
i	O
exp	O
yixt	O
i	O
w	O
d	O
i	O
thus	O
the	O
exponential	O
of	O
the	O
negative	O
hinge	B
loss	B
can	O
be	O
represented	O
as	O
a	O
gaussian	B
scale	I
mixture	B
this	O
allows	O
one	O
to	O
fit	O
an	O
svm	B
using	O
em	B
or	O
gibbs	B
sampling	I
where	O
i	O
are	O
the	O
latent	B
variables	O
this	O
in	O
turn	O
opens	O
the	O
door	O
to	O
bayesian	B
methods	O
for	O
setting	O
the	O
hyper-parameters	B
for	O
the	O
prior	O
on	O
w	O
see	O
and	O
scott	O
for	O
details	O
also	O
et	O
al	O
for	O
a	O
different	O
probabilistic	O
interpretation	O
of	O
svms	O
comparison	O
of	O
discriminative	B
kernel	B
methods	O
we	O
have	O
mentioned	O
several	O
different	O
methods	O
for	O
classification	O
and	O
regression	B
based	O
on	O
kernels	O
which	O
we	O
summarize	O
in	O
table	O
stands	O
for	O
gaussian	B
process	I
which	O
we	O
discuss	O
in	O
chapter	O
the	O
columns	O
have	O
the	O
following	O
meaning	O
chapter	O
kernels	O
optimize	O
w	O
a	O
key	O
question	O
is	O
whether	O
the	O
objective	O
jw	O
log	O
pdw	O
log	O
pw	O
is	O
convex	B
or	O
not	O
and	O
svms	O
have	O
convex	B
objectives	O
rvms	O
do	O
not	O
gps	B
are	O
bayesian	B
methods	O
that	O
do	O
not	O
perform	O
parameter	B
estimation	O
optimize	O
kernel	B
all	O
the	O
methods	O
require	O
that	O
one	O
tune	O
the	O
kernel	B
parameters	O
such	O
as	O
the	O
bandwidth	B
of	O
the	O
rbf	B
kernel	B
as	O
well	O
as	O
the	O
level	O
of	O
regularization	B
for	O
methods	O
based	O
on	O
gaussians	O
including	O
rvms	O
and	O
gps	B
we	O
can	O
use	O
efficient	O
gradient	O
based	O
optimizers	O
to	O
maximize	O
the	O
marginal	B
likelihood	B
for	O
svms	O
and	O
we	O
must	O
use	O
cross	B
validation	I
which	O
is	O
slower	O
section	O
sparse	B
rvms	O
and	O
svms	O
are	O
sparse	B
kernel	B
methods	O
in	O
that	O
they	O
only	O
use	O
a	O
subset	O
of	O
the	O
training	O
examples	O
gps	B
and	O
are	O
not	O
sparse	B
they	O
use	O
all	O
the	O
training	O
examples	O
the	O
principle	O
advantage	O
of	O
sparsity	B
is	O
that	O
prediction	O
at	O
test	O
time	O
is	O
usually	O
faster	O
in	O
addition	O
one	O
can	O
sometimes	O
get	O
improved	O
accuracy	O
probabilistic	O
all	O
the	O
methods	O
except	O
for	O
svms	O
produce	O
probabilistic	O
output	O
of	O
the	O
form	O
pyx	O
svms	O
produce	O
a	O
confidence	O
value	O
that	O
can	O
be	O
converted	O
to	O
a	O
probability	O
but	O
such	O
probabilities	O
are	O
usually	O
very	O
poorly	O
calibrated	O
section	O
multiclass	O
all	O
the	O
methods	O
except	O
for	O
svms	O
naturally	O
work	O
in	O
the	O
multiclass	O
setting	O
by	O
using	O
a	O
multinoulli	O
output	O
instead	O
of	O
bernoulli	B
the	O
svm	B
can	O
be	O
made	O
into	O
a	O
multiclass	O
classifier	O
but	O
there	O
are	O
various	O
difficulties	O
with	O
this	O
approach	O
as	O
discussed	O
in	O
section	O
mercer	B
kernel	B
svms	O
and	O
gps	B
require	O
that	O
the	O
kernel	B
is	O
positive	O
definite	O
the	O
other	O
techniques	O
do	O
not	O
apart	O
from	O
these	O
differences	O
there	O
is	O
the	O
natural	O
question	O
which	O
method	O
works	O
best	O
in	O
a	O
small	O
we	O
found	O
that	O
all	O
of	O
these	O
methods	O
had	O
similar	B
accuracy	O
when	O
averaged	O
over	O
a	O
range	O
of	O
problems	O
provided	O
they	O
have	O
the	O
same	O
kernel	B
and	O
provided	O
the	O
regularization	B
constants	O
are	O
chosen	O
appropriately	O
given	O
that	O
the	O
statistical	O
performance	O
is	O
roughly	O
the	O
same	O
what	O
about	O
the	O
computational	O
performance	O
gps	B
and	O
are	O
generally	O
the	O
slowest	O
taking	O
on	O
time	O
since	O
they	O
don	O
t	O
exploit	O
sparsity	B
various	O
speedups	O
are	O
possible	O
see	O
section	O
svms	O
also	O
take	O
on	O
time	O
to	O
train	O
we	O
use	O
a	O
linear	B
kernel	B
in	O
which	O
case	O
we	O
only	O
need	O
on	O
time	O
however	O
the	O
need	O
to	O
use	O
cross	B
validation	I
can	O
make	O
svms	O
slower	O
than	O
rvms	O
should	O
be	O
faster	O
than	O
an	O
rvm	B
since	O
an	O
rvm	B
requires	O
multiple	O
rounds	O
of	O
minimization	O
section	O
however	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
a	O
greedy	O
method	O
to	O
train	O
rvms	O
which	O
is	O
faster	O
than	O
minimization	O
this	O
is	O
reflected	O
in	O
our	O
empirical	O
results	O
the	O
conclusion	O
of	O
all	O
this	O
is	O
as	O
follows	O
if	O
speed	O
matters	O
use	O
an	O
rvm	B
but	O
if	O
well-calibrated	O
probabilistic	O
output	O
matters	O
for	O
active	B
learning	B
or	O
control	O
problems	O
use	O
a	O
gp	O
the	O
only	O
circumstances	O
under	O
which	O
using	O
an	O
svm	B
seems	O
sensible	O
is	O
the	O
structured	B
output	I
case	O
where	O
likelihood-based	O
methods	O
can	O
be	O
slow	O
attribute	O
the	O
enormous	O
popularity	O
of	O
svms	O
not	O
to	O
their	O
superiority	O
but	O
to	O
ignorance	O
of	O
the	O
alternatives	O
and	O
also	O
to	O
the	O
lack	O
of	O
high	O
quality	O
software	O
implementing	O
the	O
alternatives	O
section	O
gives	O
a	O
more	O
extensive	O
experimental	O
comparison	O
of	O
supervised	B
learning	B
methods	O
including	O
svms	O
and	O
various	O
non	O
kernel	B
methods	O
see	O
kernels	O
for	O
building	O
generative	O
models	O
boxcar	O
epanechnikov	O
tricube	O
gaussian	B
figure	O
a	O
comparison	O
of	O
some	O
popular	O
smoothing	B
kernels	O
the	O
boxcar	B
kernel	B
has	O
compact	O
support	B
but	O
is	O
not	O
smooth	O
the	O
epanechnikov	B
kernel	B
has	O
compact	O
support	B
but	O
is	O
not	O
differentiable	O
at	O
its	O
boundary	O
the	O
tri-cube	O
has	O
compact	O
support	B
and	O
two	O
continuous	O
derivatives	O
at	O
the	O
boundary	O
of	O
its	O
support	B
the	O
gaussian	B
is	O
differentiable	O
but	O
does	O
not	O
have	O
compact	O
support	B
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
smoothingkernelplot	O
kernels	O
for	O
building	O
generative	O
models	O
there	O
is	O
a	O
different	O
kind	O
of	O
kernel	B
known	O
as	O
a	O
smoothing	B
kernel	B
which	O
can	O
be	O
used	O
to	O
create	O
non-parametric	O
density	O
estimates	O
this	O
can	O
be	O
used	O
for	O
unsupervised	O
density	B
estimation	I
px	O
as	O
well	O
as	O
for	O
creating	O
generative	O
models	O
for	O
classification	O
and	O
regression	B
by	O
making	O
models	O
of	O
the	O
form	O
py	O
x	O
smoothing	B
kernels	O
a	O
smoothing	B
kernel	B
is	O
a	O
function	O
of	O
one	O
argument	O
which	O
satisfies	O
the	O
following	O
properties	O
x	O
a	O
simple	O
example	O
is	O
the	O
gaussian	B
kernel	B
e	O
we	O
can	O
control	O
the	O
width	O
of	O
the	O
kernel	B
by	O
introducing	O
a	O
bandwidth	B
parameter	B
h	O
hx	O
h	O
x	O
h	O
we	O
can	O
generalize	B
to	O
vector	O
valued	O
inputs	O
by	O
defining	O
an	O
rbf	B
kernel	B
hx	O
hx	O
in	O
the	O
case	O
of	O
the	O
gaussian	B
kernel	B
this	O
becomes	O
hx	O
exp	O
j	O
chapter	O
kernels	O
although	O
gaussian	B
kernels	O
are	O
popular	O
they	O
have	O
unbounded	O
support	B
an	O
alternative	O
kernel	B
with	O
compact	O
support	B
is	O
the	O
epanechnikov	B
kernel	B
defined	O
by	O
this	O
is	O
plotted	O
in	O
figure	O
compact	O
support	B
can	O
be	O
useful	O
for	O
efficiency	O
reasons	O
since	O
one	O
can	O
use	O
fast	O
nearest	B
neighbor	I
methods	O
to	O
evaluate	O
the	O
density	O
unfortunately	O
the	O
epanechnikov	B
kernel	B
is	O
not	O
differentiable	O
at	O
the	O
boundary	O
of	O
its	O
support	B
an	O
alterative	O
is	O
the	O
tri-cube	B
kernel	B
defined	O
as	O
follows	O
ix	O
this	O
has	O
compact	O
support	B
and	O
has	O
two	O
continuous	O
derivatives	O
at	O
the	O
boundary	O
of	O
its	O
support	B
see	O
figure	O
the	O
boxcar	B
kernel	B
is	O
simply	O
the	O
uniform	B
distribution	I
ix	O
we	O
will	O
use	O
this	O
kernel	B
below	O
kernel	B
density	B
estimation	I
recall	B
the	O
gaussian	B
mixture	B
model	I
from	O
section	O
this	O
is	O
a	O
parametric	O
density	O
estimator	B
for	O
d	O
however	O
it	O
requires	O
specifying	O
the	O
number	O
k	O
and	O
locations	O
k	O
of	O
the	O
clusters	B
an	O
data	O
in	O
r	O
alternative	O
to	O
estimating	O
the	O
k	O
is	O
to	O
allocate	O
one	O
cluster	O
center	O
per	O
data	O
point	O
so	O
i	O
xi	O
in	O
this	O
case	O
the	O
model	O
becomes	O
pxd	O
n	O
n	O
we	O
can	O
generalize	B
the	O
approach	O
by	O
writing	O
px	O
n	O
h	O
xi	O
this	O
is	O
called	O
a	O
parzen	B
window	I
density	I
estimator	B
or	O
kernel	B
density	I
estimator	B
and	O
is	O
a	O
simple	O
non-parametric	O
density	O
model	O
the	O
advantage	O
over	O
a	O
parametric	B
model	I
is	O
that	O
no	O
model	O
fitting	O
is	O
required	O
for	O
tuning	O
the	O
bandwidth	B
usually	O
done	O
by	O
cross-validation	O
and	O
there	O
is	O
no	O
need	O
to	O
pick	O
k	O
the	O
disadvantage	O
is	O
that	O
the	O
model	O
takes	O
a	O
lot	O
of	O
memory	O
to	O
store	O
and	O
a	O
lot	O
of	O
time	O
to	O
evaluate	O
it	O
is	O
also	O
of	O
no	O
use	O
for	O
clustering	B
tasks	O
figure	O
illustrates	O
kde	B
in	O
for	O
two	O
kinds	O
of	O
kernel	B
on	O
the	O
top	O
we	O
use	O
a	O
boxcar	B
kernel	B
z	O
the	O
result	O
is	O
equivalent	O
to	O
a	O
histogram	B
estimate	O
of	O
the	O
density	O
since	O
we	O
just	O
count	O
how	O
many	O
data	O
points	O
land	O
within	O
an	O
interval	O
of	O
size	O
h	O
around	O
xi	O
on	O
the	O
bottom	O
we	O
use	O
a	O
gaussian	B
kernel	B
which	O
results	O
in	O
a	O
smoother	O
fit	O
the	O
usual	O
way	O
to	O
pick	O
h	O
is	O
to	O
minimize	O
an	O
estimate	O
as	O
cross	B
validation	I
of	O
the	O
frequentist	B
risk	B
e	O
g	O
and	O
azzalini	O
in	O
section	O
we	O
discuss	O
a	O
bayesian	B
approach	O
to	O
non-parametric	O
density	B
estimation	I
based	O
on	O
dirichlet	B
process	I
mixture	B
models	I
which	O
allows	O
us	O
kernels	O
for	O
building	O
generative	O
models	O
unif	O
unif	O
gauss	O
gauss	O
figure	O
a	O
nonparametric	O
density	O
estimator	B
in	O
estimated	O
from	O
data	O
points	O
denoted	O
by	O
x	O
top	O
row	O
uniform	O
kernel	B
bottom	O
row	O
gaussian	B
kernel	B
rows	O
represent	O
increasingly	O
large	O
bandwidth	B
parameters	O
based	O
on	O
httpen	O
wikipedia	O
orgwikikernel	O
density	B
estimation	I
figure	O
generated	O
by	O
to	O
infer	O
h	O
dp	O
mixtures	O
can	O
also	O
be	O
more	O
efficient	O
than	O
kde	B
since	O
they	O
do	O
not	O
need	O
to	O
store	O
all	O
the	O
data	O
see	O
also	O
section	O
where	O
we	O
discuss	O
an	O
empirical	B
bayes	I
approach	O
to	O
estimating	O
kernel	B
parameters	O
in	O
a	O
gaussian	B
process	I
model	O
for	O
classification	O
regression	B
from	O
kde	B
to	O
knn	B
we	O
can	O
use	O
kde	B
to	O
define	O
the	O
class	O
conditional	O
densities	O
in	O
a	O
generative	O
classifier	O
this	O
turns	O
out	O
to	O
provide	O
an	O
alternative	O
derivation	O
of	O
the	O
nearest	O
neighbors	B
classifier	O
which	O
we	O
introduced	O
in	O
section	O
to	O
show	O
this	O
we	O
follow	O
the	O
presentation	O
of	O
in	O
kde	B
with	O
a	O
boxcar	B
kernel	B
we	O
fixed	O
the	O
bandwidth	B
and	O
count	O
how	O
many	O
data	O
points	O
fall	O
within	O
the	O
hyper-cube	O
centered	O
on	O
a	O
datapoint	O
suppose	O
that	O
instead	O
of	O
fixing	O
the	O
bandwidth	B
h	O
we	O
instead	O
chapter	O
kernels	O
gaussian	B
kernel	B
regression	B
true	O
data	O
estimate	O
figure	O
an	O
example	O
of	O
kernel	B
regression	B
in	O
using	O
a	O
gaussian	B
kernel	B
kernelregressiondemo	O
based	O
on	O
code	O
by	O
yi	O
cao	O
figure	O
generated	O
by	O
allow	O
the	O
bandwidth	B
or	O
volume	O
to	O
be	O
different	O
for	O
each	O
data	O
point	O
specifically	O
we	O
will	O
grow	O
a	O
volume	O
around	O
x	O
until	O
we	O
encounter	O
k	O
data	O
points	O
regardless	O
of	O
their	O
class	O
label	B
let	O
the	O
resulting	O
volume	O
have	O
size	O
v	O
was	O
previously	O
hd	O
and	O
let	O
there	O
be	O
ncx	O
examples	O
from	O
class	O
c	O
in	O
this	O
volume	O
then	O
we	O
can	O
estimate	O
the	O
class	O
conditional	O
density	O
as	O
follows	O
pxy	O
cd	B
ncx	O
ncv	O
where	O
nc	O
is	O
the	O
total	O
number	O
of	O
examples	O
in	O
class	O
c	O
in	O
the	O
whole	O
data	O
set	O
the	O
class	O
prior	O
can	O
be	O
estimated	O
by	O
py	O
cd	B
nc	O
n	O
hence	O
the	O
class	O
posterior	O
is	O
given	O
by	O
py	O
cxd	O
ncx	O
ncv	O
nc	O
n	O
v	O
ncx	O
ncx	O
k	O
n	O
where	O
we	O
used	O
the	O
fact	O
that	O
class	O
around	O
every	O
point	O
this	O
is	O
equivalent	O
to	O
equation	O
since	O
ncx	O
c	O
c	O
ncx	O
k	O
since	O
we	O
choose	O
a	O
total	O
of	O
k	O
points	O
of	O
i	O
nk	O
iyi	O
kernel	B
regression	B
in	O
section	O
we	O
discussed	O
the	O
use	O
of	O
kernel	B
density	B
estimation	I
or	O
kde	B
for	O
unsupervised	B
learning	B
we	O
can	O
also	O
use	O
kde	B
for	O
regression	B
the	O
goal	O
is	O
to	O
compute	O
the	O
conditional	O
expectation	O
f	O
e	O
y	O
pyxdy	O
y	O
px	O
ydy	O
px	O
ydy	O
kernels	O
for	O
building	O
generative	O
models	O
we	O
can	O
use	O
kde	B
to	O
approximate	O
the	O
joint	O
density	O
px	O
y	O
as	O
follows	O
px	O
y	O
n	O
hx	O
xi	O
hy	O
yi	O
n	O
n	O
hx	O
xi	O
hx	O
xi	O
hx	O
xiyi	O
hx	O
xi	O
y	O
hy	O
yidy	O
hy	O
yidy	O
hence	O
f	O
to	O
derive	O
this	O
result	O
we	O
used	O
two	O
properties	O
of	O
smoothing	B
kernels	O
first	O
that	O
they	O
integrate	O
to	O
y	O
hy	O
yidy	O
yi	O
this	O
follows	O
by	O
one	O
i	O
e	O
defining	O
x	O
y	O
yi	O
and	O
using	O
the	O
zero	O
mean	B
property	O
of	O
smoothing	B
kernels	O
hy	O
yidy	O
and	O
second	O
the	O
fact	O
that	O
yi	O
hxdx	O
x	O
hxdx	O
yi	O
hxdx	O
yi	O
yi	O
we	O
can	O
rewrite	O
the	O
above	O
result	O
as	O
follows	O
f	O
wix	O
wixyi	O
hx	O
xi	O
hx	O
we	O
see	O
that	O
the	O
prediction	O
is	O
just	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
at	O
the	O
training	O
points	O
where	O
the	O
weights	O
depend	O
on	O
how	O
similar	B
x	O
is	O
to	O
the	O
stored	O
training	O
points	O
this	O
method	O
is	O
called	O
kernel	B
regression	B
kernel	B
smoothing	B
or	O
the	O
nadaraya-watson	B
model	O
see	O
figure	O
for	O
an	O
example	O
where	O
we	O
use	O
a	O
gaussian	B
kernel	B
note	O
that	O
this	O
method	O
only	O
has	O
one	O
free	O
parameter	B
namely	O
h	O
one	O
can	O
show	O
and	O
azzalini	O
that	O
for	O
data	O
if	O
the	O
true	O
density	O
is	O
gaussian	B
and	O
we	O
are	O
using	O
gaussian	B
kernels	O
the	O
optimal	O
bandwidth	B
h	O
is	O
given	O
by	O
we	O
can	O
compute	O
a	O
robust	B
approximation	O
to	O
the	O
standard	B
deviation	I
by	O
first	O
computing	O
the	O
mean	B
absolute	I
deviation	I
mad	O
medianx	O
medianx	O
and	O
then	O
using	O
mad	O
mad	O
the	O
code	O
used	O
to	O
produce	O
figure	O
estimated	O
hx	O
and	O
hy	O
separately	O
and	O
then	O
set	O
h	O
hxhy	O
h	O
chapter	O
kernels	O
although	O
these	O
heuristics	B
seem	O
to	O
work	O
well	O
their	O
derivation	O
rests	O
on	O
some	O
rather	O
dubious	O
assumptions	O
as	O
gaussianity	O
of	O
the	O
true	O
density	O
furthermore	O
these	O
heuristics	B
are	O
limited	O
to	O
tuning	O
just	O
a	O
single	O
parameter	B
in	O
section	O
we	O
discuss	O
an	O
empirical	B
bayes	I
approach	O
to	O
estimating	O
multiple	O
kernel	B
parameters	O
in	O
a	O
gaussian	B
process	I
model	O
for	O
classification	O
regression	B
which	O
can	O
handle	O
many	O
tuning	O
parameters	O
and	O
which	O
is	O
based	O
on	O
much	O
more	O
transparent	O
principles	O
the	O
marginal	B
likelihood	B
locally	B
weighted	I
regression	B
if	O
we	O
define	O
hx	O
xi	O
xi	O
we	O
can	O
rewrite	O
the	O
prediction	O
made	O
by	O
kernel	B
regression	B
as	O
follows	O
f	O
yi	O
xi	O
note	O
that	O
xi	O
need	O
not	O
be	O
a	O
smoothing	B
kernel	B
normalization	O
term	O
so	O
we	O
can	O
just	O
write	O
if	O
it	O
is	O
not	O
we	O
no	O
longer	O
need	O
the	O
f	O
yi	O
xi	O
this	O
model	O
is	O
essentially	O
fitting	O
a	O
constant	O
function	O
locally	O
we	O
can	O
improve	O
on	O
this	O
by	O
fitting	O
a	O
linear	B
regression	B
model	O
for	O
each	O
point	O
x	O
by	O
solving	O
xiyi	O
min	O
where	O
x	O
this	O
is	O
called	O
locally	B
weighted	I
regression	B
an	O
example	O
of	O
such	O
a	O
method	O
is	O
loess	B
akalowess	O
which	O
stands	O
for	O
locally-weighted	O
scatterplot	O
smoothing	B
and	O
devlin	O
see	O
also	O
et	O
al	O
for	O
a	O
bayesian	B
version	O
of	O
this	O
model	O
we	O
can	O
compute	O
the	O
paramters	O
for	O
each	O
test	O
case	O
by	O
solving	O
the	O
following	O
weighted	B
least	B
squares	I
problem	I
t	O
dx	O
where	O
is	O
an	O
n	O
design	B
matrix	I
and	O
d	O
diag	O
xi	O
the	O
corresponding	O
prediction	O
has	O
the	O
form	O
t	O
dx	O
f	O
t	O
dx	O
t	O
dx	O
wix	O
the	O
term	O
wix	O
which	O
combines	O
the	O
local	O
smoothing	B
kernel	B
with	O
the	O
effect	O
of	O
linear	B
regression	B
is	O
called	O
the	O
equivalent	B
kernel	B
see	O
also	O
section	O
exercises	O
exercise	O
fitting	O
an	O
svm	B
classifier	O
by	O
hand	O
jaakkola	O
consider	O
a	O
dataset	O
with	O
points	O
in	O
and	O
consider	O
mapping	O
each	O
point	O
to	O
using	O
the	O
feature	O
vector	O
is	O
equivalent	O
to	O
kernels	O
for	O
building	O
generative	O
models	O
using	O
a	O
second	B
order	I
polynomial	B
kernel	B
the	O
max	O
margin	B
classifier	O
has	O
the	O
form	O
s	O
t	O
a	O
write	O
down	O
a	O
vector	O
that	O
is	O
parallel	O
to	O
the	O
optimal	O
vector	O
w	O
hint	O
recall	B
from	O
figure	O
version	O
that	O
w	O
is	O
perpendicular	O
to	O
the	O
decision	B
boundary	I
between	O
the	O
two	O
points	O
in	O
the	O
feature	O
space	O
b	O
what	O
is	O
the	O
value	O
of	O
the	O
margin	B
that	O
is	O
achieved	O
by	O
this	O
w	O
hint	O
recall	B
that	O
the	O
margin	B
is	O
the	O
distance	O
from	O
each	O
support	B
vector	O
to	O
the	O
decision	B
boundary	I
hint	O
think	O
about	O
the	O
geometry	O
of	O
points	O
in	O
space	O
with	O
a	O
line	O
separating	O
one	O
from	O
the	O
other	O
c	O
solve	O
for	O
w	O
using	O
the	O
fact	O
the	O
margin	B
is	O
equal	O
to	O
d	O
solve	O
for	O
using	O
your	O
value	O
for	O
w	O
and	O
equations	O
to	O
hint	O
decision	B
boundary	I
so	O
the	O
inequalities	O
will	O
be	O
tight	O
the	O
points	O
will	O
be	O
on	O
the	O
e	O
write	O
down	O
the	O
form	O
of	O
the	O
discriminant	B
function	I
f	O
wt	O
as	O
an	O
explicit	O
function	O
of	O
x	O
exercise	O
linear	O
separability	O
koller	O
consider	O
fitting	O
an	O
svm	B
with	O
c	O
to	O
a	O
dataset	O
that	O
is	O
linearly	B
separable	I
is	O
the	O
resulting	O
decision	B
boundary	I
guaranteed	O
to	O
separate	O
the	O
classes	O
gaussian	B
processes	I
introduction	O
in	O
supervised	B
learning	B
we	O
observe	O
some	O
inputs	O
xi	O
and	O
some	O
outputs	O
yi	O
we	O
assume	O
that	O
yi	O
f	O
for	O
some	O
unknown	B
function	O
f	O
possibly	O
corrupted	O
by	O
noise	O
the	O
optimal	O
approach	O
is	O
to	O
infer	O
a	O
distribution	O
over	O
functions	O
given	O
the	O
data	O
pfx	O
y	O
and	O
then	O
to	O
use	O
this	O
to	O
make	O
predictions	O
given	O
new	O
inputs	O
i	O
e	O
to	O
compute	O
py	O
x	O
y	O
py	O
x	O
ydf	O
up	O
until	O
now	O
we	O
have	O
focussed	O
on	O
parametric	O
representations	O
for	O
the	O
function	O
f	O
so	O
that	O
in	O
this	O
chapter	O
we	O
discuss	O
a	O
way	O
to	O
perform	O
instead	O
of	O
inferring	O
pfd	O
we	O
infer	O
p	O
bayesian	B
inference	B
over	O
functions	O
themselves	O
our	O
approach	O
will	O
be	O
based	O
on	O
gaussian	B
processes	I
or	O
gps	B
a	O
gp	O
defines	O
a	O
prior	O
over	O
functions	O
which	O
can	O
be	O
converted	O
into	O
a	O
posterior	O
over	O
functions	O
once	O
we	O
have	O
seen	O
some	O
data	O
although	O
it	O
might	O
seem	O
difficult	O
to	O
represent	O
a	O
distribution	O
over	O
a	O
function	O
it	O
turns	O
out	O
that	O
we	O
only	O
need	O
to	O
be	O
able	O
to	O
define	O
a	O
distribution	O
over	O
the	O
function	O
s	O
values	O
at	O
a	O
finite	O
but	O
arbitrary	O
set	O
of	O
points	O
say	O
xn	O
a	O
gp	O
assumes	O
that	O
pf	O
f	O
is	O
jointly	O
gaussian	B
with	O
some	O
mean	B
and	O
covariance	B
given	O
by	O
ij	O
xj	O
where	O
is	O
a	O
positive	O
definite	O
kernel	B
function	I
section	O
information	B
on	O
kernels	O
the	O
key	O
idea	O
is	O
that	O
if	O
xi	O
and	O
xj	O
are	O
deemed	O
by	O
the	O
kernel	B
to	O
be	O
similar	B
then	O
we	O
expect	O
the	O
output	O
of	O
the	O
function	O
at	O
those	O
points	O
to	O
be	O
similar	B
too	O
see	O
figure	O
for	O
an	O
illustration	O
it	O
turns	O
out	O
that	O
in	O
the	O
regression	B
setting	O
all	O
these	O
computations	O
can	O
be	O
done	O
in	O
closed	O
form	O
in	O
on	O
time	O
discuss	O
faster	O
approximations	O
in	O
section	O
in	O
the	O
classification	O
setting	O
we	O
must	O
use	O
approximations	O
such	O
as	O
the	O
gaussian	B
approximation	I
since	O
the	O
posterior	O
is	O
no	O
longer	O
exactly	O
gaussian	B
gps	B
can	O
be	O
thought	O
of	O
as	O
a	O
bayesian	B
alternative	O
to	O
the	O
kernel	B
methods	O
we	O
discussed	O
in	O
chapter	O
including	O
rvm	B
and	O
svm	B
although	O
those	O
methods	O
are	O
sparser	O
and	O
therefore	O
faster	O
they	O
do	O
not	O
give	O
well-calibrated	O
probabilistic	O
outputs	O
section	O
for	O
further	O
discussion	O
having	O
properly	O
tuned	O
probabilistic	O
output	O
is	O
important	O
in	O
certain	O
applications	O
such	O
as	O
online	O
tracking	B
for	O
vision	O
and	O
robotics	O
and	O
fox	O
reinforcement	B
learning	B
and	O
optimal	O
control	O
et	O
al	O
deisenroth	O
et	O
al	O
global	O
optimization	B
of	O
non-convex	O
functions	O
et	O
al	O
lizotte	O
brochu	O
et	O
al	O
experiment	O
design	O
et	O
al	O
etc	O
chapter	O
gaussian	B
processes	I
f	O
f	O
f	O
figure	O
a	O
gaussian	B
process	I
for	O
training	O
points	O
and	O
testing	O
point	O
represented	O
as	O
a	O
mixed	O
directed	B
and	O
undirected	B
graphical	B
model	I
representing	O
py	O
fx	O
kx	O
i	O
pyifi	O
the	O
hidden	B
nodes	B
fi	O
f	O
represent	O
the	O
value	O
of	O
the	O
function	O
at	O
each	O
of	O
the	O
data	O
points	O
these	O
hidden	B
nodes	B
are	O
fully	O
interconnected	O
by	O
undirected	B
edges	B
forming	O
a	O
gaussian	B
graphical	B
model	I
the	O
edge	O
strengths	O
represent	O
the	O
covariance	B
terms	O
ij	O
xj	O
if	O
the	O
test	O
point	O
x	O
is	O
similar	B
to	O
the	O
training	O
points	O
and	O
then	O
the	O
predicted	O
output	O
y	O
will	O
be	O
similar	B
to	O
and	O
our	O
presentation	O
is	O
closely	O
based	O
on	O
and	O
williams	O
which	O
should	O
be	O
consulted	O
for	O
futher	O
details	O
see	O
also	O
and	O
ribeiro	O
which	O
discusses	O
the	O
related	O
approach	O
known	O
as	O
kriging	B
which	O
is	O
widely	O
used	O
in	O
the	O
spatial	O
statistics	O
literature	O
gps	B
for	O
regression	B
in	O
this	O
section	O
we	O
discuss	O
gps	B
for	O
regression	B
let	O
the	O
prior	O
on	O
the	O
regression	B
function	O
be	O
a	O
gp	O
denoted	O
by	O
f	O
gp	O
where	O
mx	O
is	O
the	O
mean	B
function	I
and	O
is	O
the	O
kernel	B
or	O
covariance	B
function	O
i	O
e	O
mx	O
mxf	O
we	O
obviously	O
require	O
that	O
be	O
a	O
positive	O
definite	O
kernel	B
for	O
any	O
finite	O
set	O
of	O
points	O
this	O
process	O
defines	O
a	O
joint	O
gaussian	B
pfx	O
n	O
k	O
where	O
kij	O
xj	O
and	O
mxn	O
note	O
that	O
it	O
is	O
common	O
to	O
use	O
a	O
mean	B
function	I
of	O
mx	O
since	O
the	O
gp	O
is	O
flexible	O
enough	O
to	O
model	O
the	O
mean	B
arbitrarily	O
well	O
as	O
we	O
will	O
see	O
below	O
however	O
in	O
section	O
we	O
will	O
consider	O
parametric	O
models	O
for	O
the	O
mean	B
function	I
so	O
the	O
gp	O
just	O
has	O
to	O
model	O
the	O
residual	B
errors	O
this	O
semi-parametric	O
approach	O
combines	O
the	O
interpretability	O
of	O
parametric	O
models	O
with	O
the	O
accuracy	O
of	O
non-parametric	O
models	O
gps	B
for	O
regression	B
figure	O
left	O
some	O
functions	O
sampled	O
from	O
a	O
gp	O
prior	O
with	O
se	O
kernel	B
right	O
some	O
samples	B
from	O
a	O
gp	O
posterior	O
after	O
conditioning	B
on	O
noise-free	O
observations	O
the	O
shaded	O
area	O
represents	O
e	O
based	O
on	O
figure	O
of	O
and	O
williams	O
figure	O
generated	O
by	O
gprdemonoisefree	O
predictions	O
using	O
noise-free	O
observations	O
suppose	O
we	O
observe	O
a	O
training	B
set	I
d	O
fi	O
i	O
n	O
where	O
fi	O
f	O
is	O
the	O
noise-free	O
observation	B
of	O
the	O
function	O
evaluated	O
at	O
xi	O
given	O
a	O
test	O
set	O
x	O
of	O
size	O
n	O
d	O
we	O
want	O
to	O
predict	O
the	O
function	O
outputs	O
f	O
if	O
we	O
ask	O
the	O
gp	O
to	O
predict	O
f	O
for	O
a	O
value	O
of	O
x	O
that	O
it	O
has	O
already	O
seen	O
we	O
want	O
the	O
gp	O
to	O
return	O
the	O
answer	O
f	O
with	O
no	O
uncertainty	B
in	O
other	O
words	O
it	O
should	O
act	O
as	O
an	O
interpolator	B
of	O
the	O
training	O
data	O
this	O
will	O
only	O
happen	O
if	O
we	O
assume	O
the	O
observations	O
are	O
noiseless	O
we	O
will	O
consider	O
the	O
case	O
of	O
noisy	O
observations	O
below	O
now	O
we	O
return	O
to	O
the	O
prediction	O
problem	O
by	O
definition	O
of	O
the	O
gp	O
the	O
joint	B
distribution	I
has	O
the	O
following	O
form	O
f	O
f	O
n	O
k	O
k	O
kt	O
k	O
where	O
k	O
x	O
is	O
n	O
n	O
k	O
x	O
is	O
n	O
n	O
and	O
k	O
x	O
is	O
n	O
n	O
by	O
the	O
standard	O
rules	B
for	O
conditioning	B
gaussians	O
the	O
posterior	O
has	O
the	O
following	O
form	O
pf	O
x	O
f	O
t	O
k	O
k	O
kt	O
k	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
on	O
the	O
left	O
we	O
show	O
sample	O
samples	B
from	O
the	O
prior	O
in	O
pfx	O
where	O
we	O
use	O
a	O
squared	B
exponential	I
kernel	B
aka	O
gaussian	B
kernel	B
or	O
rbf	B
kernel	B
this	O
is	O
given	O
by	O
f	O
exp	O
here	O
controls	O
the	O
horizontal	O
length	O
scale	O
over	O
which	O
the	O
function	O
varies	O
and	O
f	O
controls	O
the	O
vertical	O
variation	O
discuss	O
how	O
to	O
estimate	O
such	O
kernel	B
parameters	O
below	O
on	O
the	O
right	O
we	O
chapter	O
gaussian	B
processes	I
show	O
samples	B
from	O
the	O
posterior	O
pf	O
x	O
f	O
we	O
see	O
that	O
the	O
model	O
perfectly	O
interpolates	O
the	O
training	O
data	O
and	O
that	O
the	O
predictive	B
uncertainty	B
increases	O
as	O
we	O
move	O
further	O
away	O
from	O
the	O
observed	O
data	O
one	O
application	O
of	O
noise-free	O
gp	O
regression	B
is	O
as	O
a	O
computationally	O
cheap	O
proxy	O
for	O
the	O
behavior	O
of	O
a	O
complex	O
simulator	O
such	O
as	O
a	O
weather	O
forecasting	O
program	O
the	O
simulator	O
is	O
stochastic	O
we	O
can	O
define	O
f	O
to	O
be	O
its	O
mean	B
output	O
note	O
that	O
there	O
is	O
still	O
no	O
observation	B
noise	O
one	O
can	O
then	O
estimate	O
the	O
effect	O
of	O
changing	O
simulator	O
parameters	O
by	O
examining	O
their	O
effect	O
on	O
the	O
gp	O
s	O
predictions	O
rather	O
than	O
having	O
to	O
run	O
the	O
simulator	O
many	O
times	O
which	O
may	O
be	O
prohibitively	O
slow	O
this	O
strategy	O
is	O
known	O
as	O
dace	B
which	O
stands	O
for	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
et	O
al	O
predictions	O
using	O
noisy	O
observations	O
now	O
let	O
us	O
consider	O
the	O
case	O
where	O
what	O
we	O
observe	O
is	O
a	O
noisy	O
version	O
of	O
the	O
underlying	O
function	O
y	O
f	O
where	O
n	O
y	O
in	O
this	O
case	O
the	O
model	O
is	O
not	O
required	O
to	O
interpolate	B
the	O
data	O
but	O
it	O
must	O
come	O
close	O
to	O
the	O
observed	O
data	O
the	O
covariance	B
of	O
the	O
observed	O
noisy	O
responses	O
is	O
cov	O
yq	O
xq	O
y	O
pq	O
where	O
pq	O
ip	B
q	O
in	O
other	O
words	O
cov	O
k	O
yin	O
ky	O
the	O
second	O
matrix	O
is	O
diagonal	B
because	O
we	O
assumed	O
the	O
noise	O
terms	O
were	O
independently	O
added	O
to	O
each	O
observation	B
the	O
joint	O
density	O
of	O
the	O
observed	O
data	O
and	O
the	O
latent	B
noise-free	O
function	O
on	O
the	O
test	O
points	O
is	O
given	O
by	O
n	O
y	O
f	O
ky	O
k	O
kt	O
k	O
pf	O
x	O
y	O
kt	O
k	O
y	O
y	O
k	O
kt	O
k	O
y	O
k	O
where	O
we	O
are	O
assuming	O
the	O
mean	B
is	O
zero	O
for	O
notational	O
simplicity	O
hence	O
the	O
posterior	B
predictive	B
density	I
is	O
in	O
the	O
case	O
of	O
a	O
single	O
test	O
input	O
this	O
simplifies	O
as	O
follows	O
y	O
y	O
k	O
kt	O
k	O
pf	O
x	O
y	O
k	O
y	O
k	O
where	O
k	O
xn	O
and	O
k	O
x	O
another	O
way	O
to	O
write	O
the	O
posterior	B
mean	B
is	O
as	O
follows	O
f	O
kt	O
k	O
y	O
y	O
i	O
x	O
where	O
k	O
y	O
y	O
we	O
will	O
revisit	O
this	O
expression	O
later	O
gps	B
for	O
regression	B
figure	O
some	O
gps	B
with	O
se	O
kernels	O
but	O
different	O
hyper-parameters	B
fit	O
to	O
noisy	O
observations	O
the	O
kernel	B
has	O
the	O
form	O
in	O
equation	O
the	O
hyper-parameters	B
f	O
y	O
are	O
as	O
follows	O
based	O
on	O
figure	O
of	O
and	O
williams	O
figure	O
generated	O
by	O
gprdemochangehparams	O
written	O
by	O
carl	O
rasmussen	O
effect	O
of	O
the	O
kernel	B
parameters	O
the	O
predictive	B
performance	O
of	O
gps	B
depends	O
exclusively	O
on	O
the	O
suitability	O
of	O
the	O
chosen	O
kernel	B
suppose	O
we	O
choose	O
the	O
following	O
squared-exponential	O
kernel	B
for	O
the	O
noisy	O
observations	O
yxp	O
xq	O
y	O
pq	O
f	O
exp	O
here	O
is	O
the	O
horizontal	O
scale	O
over	O
which	O
the	O
function	O
changes	O
f	O
controls	O
the	O
vertical	O
scale	O
of	O
the	O
function	O
and	O
y	O
is	O
the	O
noise	O
variance	B
figure	O
illustrates	O
the	O
effects	O
of	O
changing	O
these	O
parameters	O
we	O
sampled	O
noisy	O
data	O
points	O
from	O
the	O
se	O
kernel	B
using	O
f	O
y	O
and	O
then	O
made	O
predictions	O
various	O
parameters	O
conditional	O
on	O
the	O
data	O
in	O
figure	O
we	O
use	O
f	O
y	O
and	O
the	O
result	O
is	O
a	O
good	O
fit	O
in	O
figure	O
we	O
reduce	O
the	O
length	O
scale	O
to	O
other	O
parameters	O
were	O
optimized	O
by	O
maximum	O
likelihood	B
a	O
technique	O
we	O
discuss	O
below	O
now	O
the	O
function	O
looks	O
more	O
wiggly	O
also	O
the	O
uncertainty	B
goes	O
up	O
faster	O
since	O
the	O
effective	O
distance	O
from	O
the	O
training	O
points	O
increases	O
more	O
rapidly	O
in	O
figure	O
we	O
increase	O
the	O
length	O
scale	O
to	O
now	O
the	O
function	O
looks	O
smoother	O
chapter	O
gaussian	B
processes	I
y	O
t	O
u	O
p	O
t	O
u	O
o	O
y	O
t	O
u	O
p	O
t	O
u	O
o	O
input	O
input	O
input	O
input	O
y	O
t	O
u	O
p	O
u	O
o	O
t	O
input	O
input	O
figure	O
kernel	B
has	O
the	O
form	O
in	O
equation	O
where	O
m	O
i	O
m	O
written	O
by	O
carl	O
rasmussen	O
some	O
functions	O
sampled	O
from	O
a	O
gp	O
with	O
an	O
se	O
kernel	B
but	O
different	O
hyper-parameters	B
the	O
m	O
based	O
on	O
figure	O
of	O
and	O
williams	O
figure	O
generated	O
by	O
gprdemoard	O
we	O
can	O
extend	O
the	O
se	O
kernel	B
to	O
multiple	O
dimensions	O
as	O
follows	O
yxp	O
xq	O
f	O
exp	O
xqt	O
mxp	O
xq	O
y	O
pq	O
we	O
can	O
define	O
the	O
matrix	O
m	O
in	O
several	O
ways	O
the	O
simplest	O
is	O
to	O
use	O
an	O
isotropic	B
matrix	O
see	O
figure	O
for	O
an	O
example	O
we	O
can	O
also	O
endow	O
each	O
dimension	O
with	O
its	O
own	O
characteristic	B
length	I
scale	I
if	O
any	O
of	O
these	O
length	O
scales	O
become	O
large	O
the	O
corresponding	O
feature	O
dimension	O
is	O
deemed	O
irrelevant	O
just	O
as	O
in	O
ard	B
in	O
figure	O
we	O
use	O
m	O
with	O
so	O
the	O
function	O
changes	O
faster	O
along	O
the	O
direction	O
than	O
the	O
direction	O
where	O
is	O
a	O
d	O
k	O
matrix	O
where	O
k	O
d	O
and	O
williams	O
calls	O
this	O
the	O
factor	B
analysis	I
distance	I
function	O
by	O
analogy	O
to	O
the	O
fact	O
that	O
factor	B
analysis	I
approximates	O
a	O
covariance	B
matrix	I
as	O
a	O
low	O
rank	O
matrix	O
plus	O
a	O
diagonal	B
matrix	O
the	O
columns	O
of	O
correspond	O
to	O
relevant	O
directions	O
in	O
input	O
space	O
in	O
figure	O
we	O
use	O
and	O
so	O
the	O
function	O
changes	O
mostly	O
rapidly	O
in	O
the	O
direction	O
which	O
is	O
perpendicular	O
to	O
we	O
can	O
also	O
create	O
a	O
matrix	O
of	O
the	O
form	O
t	O
gps	B
for	O
regression	B
estimating	O
the	O
kernel	B
parameters	O
to	O
estimate	O
the	O
kernel	B
parameters	O
we	O
could	O
use	O
exhaustive	O
search	O
over	O
a	O
discrete	B
grid	O
of	O
values	O
with	O
validation	O
loss	B
as	O
an	O
objective	O
but	O
this	O
can	O
be	O
quite	O
slow	O
is	O
the	O
approach	O
used	O
to	O
tune	O
kernels	O
used	O
by	O
svms	O
here	O
we	O
consider	O
an	O
empirical	B
bayes	I
approach	O
which	O
will	O
allow	O
us	O
to	O
use	O
continuous	O
optimization	B
methods	O
which	O
are	O
much	O
faster	O
in	O
particular	O
we	O
will	O
maximize	O
the	O
marginal	O
pyx	O
pyf	O
xpfxdf	O
since	O
pfx	O
n	O
k	O
and	O
pyf	O
log	O
pyx	O
log	O
n	O
ky	O
i	O
n	O
y	O
y	O
yk	O
log	O
n	O
y	O
the	O
marginal	B
likelihood	B
is	O
given	O
by	O
the	O
first	O
term	O
is	O
a	O
data	O
fit	O
term	O
the	O
second	O
term	O
is	O
a	O
model	O
complexity	O
term	O
and	O
the	O
third	O
term	O
is	O
just	O
a	O
constant	O
to	O
understand	O
the	O
tradeoff	O
between	O
the	O
first	O
two	O
terms	O
consider	O
a	O
se	O
kernel	B
y	O
fixed	O
let	O
log	O
pyx	O
for	O
short	O
in	O
as	O
we	O
vary	O
the	O
length	O
scale	O
and	O
hold	O
length	O
scales	O
the	O
fit	O
will	O
be	O
good	O
so	O
yt	O
k	O
y	O
y	O
will	O
be	O
small	O
however	O
the	O
model	O
complexity	O
will	O
be	O
high	O
k	O
will	O
be	O
almost	O
diagonal	B
in	O
figure	O
top	O
right	O
since	O
most	O
points	O
will	O
not	O
be	O
considered	O
near	O
any	O
others	O
so	O
the	O
log	O
will	O
be	O
large	O
for	O
long	O
length	O
scales	O
the	O
fit	O
will	O
be	O
poor	O
but	O
the	O
model	O
complexity	O
will	O
be	O
low	O
k	O
will	O
be	O
almost	O
all	O
s	O
in	O
figure	O
bottom	O
right	O
so	O
log	O
will	O
be	O
small	O
we	O
now	O
discuss	O
how	O
to	O
maximize	O
the	O
marginal	O
likelhiood	O
let	O
the	O
kernel	B
parameters	O
called	O
hyper-parameters	B
be	O
denoted	O
by	O
one	O
can	O
show	O
that	O
trk	O
log	O
pyx	O
yt	O
k	O
ky	O
j	O
j	O
y	O
ky	O
j	O
y	O
y	O
y	O
k	O
ky	O
j	O
t	O
k	O
y	O
tr	O
it	O
takes	O
on	O
time	O
to	O
compute	O
k	O
where	O
k	O
y	O
y	O
parameter	B
to	O
compute	O
the	O
gradient	O
y	O
and	O
then	O
on	O
time	O
per	O
hyper	O
the	O
form	O
of	O
ky	O
j	O
depends	O
on	O
the	O
form	O
of	O
the	O
kernel	B
and	O
which	O
parameter	B
we	O
are	O
taking	O
y	O
derivatives	O
with	O
respect	O
to	O
often	O
we	O
have	O
constraints	O
on	O
the	O
hyper-parameters	B
such	O
as	O
in	O
this	O
case	O
we	O
can	O
define	O
log	O
y	O
and	O
then	O
use	O
the	O
chain	B
rule	I
given	O
an	O
expression	O
for	O
the	O
log	O
marginal	B
likelihood	B
and	O
its	O
derivative	O
we	O
can	O
estimate	O
the	O
kernel	B
parameters	O
using	O
any	O
standard	O
gradient-based	O
optimizer	O
however	O
since	O
the	O
objective	O
is	O
not	O
convex	B
local	O
minima	O
can	O
be	O
a	O
problem	O
as	O
we	O
illustrate	O
below	O
example	O
consider	O
figure	O
we	O
use	O
the	O
se	O
kernel	B
in	O
equation	O
with	O
x	O
and	O
y	O
are	O
the	O
data	O
points	O
shown	O
in	O
panels	O
b	O
and	O
c	O
as	O
we	O
vary	O
and	O
f	O
and	O
plot	O
log	O
pyx	O
y	O
y	O
the	O
two	O
the	O
reason	O
it	O
is	O
called	O
the	O
marginal	B
likelihood	B
rather	O
than	O
just	O
likelihood	B
is	O
because	O
we	O
have	O
marginalized	O
out	O
the	O
latent	B
gaussian	B
vector	O
f	O
this	O
moves	O
us	O
up	O
one	O
level	O
of	O
the	O
bayesian	B
hierarchy	O
and	O
reduces	O
the	O
chances	O
of	O
overfitting	O
number	O
of	O
kernel	B
parameters	O
is	O
usually	O
fairly	O
small	O
compared	O
to	O
a	O
standard	O
parametric	B
model	I
n	O
o	O
i	O
t	O
i	O
a	O
v	O
e	O
d	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
e	O
s	O
o	O
n	O
i	O
chapter	O
gaussian	B
processes	I
y	O
t	O
u	O
p	O
t	O
u	O
o	O
input	O
x	O
input	O
x	O
characteristic	O
lengthscale	O
y	O
t	O
u	O
p	O
t	O
u	O
o	O
illustration	O
of	O
local	O
minima	O
in	O
the	O
marginal	B
likelihood	B
surface	O
y	O
and	O
for	O
fixed	O
figure	O
likelihood	B
vs	O
corresponding	O
to	O
the	O
lower	O
left	O
local	O
minimum	O
noise	O
smooth	O
and	O
has	O
high	O
noise	O
the	O
data	O
was	O
generated	O
using	O
and	O
williams	O
figure	O
generated	O
by	O
gprdemomarglik	O
written	O
by	O
carl	O
rasmussen	O
we	O
plot	O
the	O
log	O
marginal	O
f	O
using	O
the	O
data	O
points	O
shown	O
in	O
panels	O
b	O
and	O
c	O
the	O
function	O
n	O
this	O
is	O
quite	O
wiggly	O
and	O
has	O
low	O
n	O
this	O
is	O
quite	O
source	O
figure	O
of	O
the	O
function	O
corresponding	O
to	O
the	O
top	O
right	O
local	O
minimum	O
n	O
local	O
optima	O
are	O
indicated	O
by	O
the	O
bottom	O
left	O
optimum	O
corresponds	O
to	O
a	O
low-noise	O
shortlength	O
scale	O
solution	O
in	O
panel	O
b	O
the	O
top	O
right	O
optimum	O
corresponds	O
to	O
a	O
high-noise	O
long-length	O
scale	O
solution	O
in	O
panel	O
c	O
with	O
only	O
data	O
points	O
there	O
is	O
not	O
enough	O
evidence	B
to	O
confidently	O
decide	O
which	O
is	O
more	O
reasonable	O
although	O
the	O
more	O
complex	O
model	O
b	O
has	O
a	O
marginal	B
likelihood	B
that	O
is	O
about	O
higher	O
than	O
the	O
simpler	O
model	O
c	O
with	O
more	O
data	O
the	O
map	B
estimate	I
should	O
come	O
to	O
dominate	O
y	O
of	O
panel	O
a	O
corresponds	O
to	O
the	O
case	O
where	O
the	O
noise	O
is	O
very	O
high	O
in	O
this	O
regime	O
the	O
marginal	B
likelihood	B
is	O
insensitive	O
to	O
the	O
length	O
scale	O
by	O
the	O
horizontal	O
contours	O
since	O
all	O
the	O
data	O
is	O
explained	O
as	O
noise	O
the	O
region	O
where	O
hand	O
side	O
of	O
panel	O
a	O
corresponds	O
to	O
the	O
case	O
where	O
the	O
length	O
scale	O
is	O
very	O
short	O
in	O
this	O
regime	O
the	O
marginal	B
likelihood	B
is	O
insensitive	O
to	O
the	O
noise	O
level	O
since	O
the	O
data	O
is	O
perfectly	O
interpolated	O
neither	O
of	O
these	O
regions	O
would	O
be	O
chosen	O
by	O
a	O
good	O
optimizer	O
figure	O
illustrates	O
some	O
other	O
interesting	O
typical	O
features	B
the	O
region	O
where	O
gps	B
for	O
regression	B
e	O
d	O
u	O
t	O
i	O
n	O
g	O
a	O
m	O
g	O
o	O
l	O
z	O
z	O
e	O
d	O
u	O
t	O
i	O
n	O
g	O
a	O
m	O
g	O
o	O
l	O
loglength	O
scale	O
loglength	O
scale	O
e	O
d	O
u	O
t	O
i	O
n	O
g	O
a	O
m	O
g	O
o	O
l	O
loglength	O
scale	O
figure	O
three	O
different	O
approximations	O
to	O
the	O
posterior	O
over	O
hyper-parameters	B
grid-based	O
monte	B
carlo	I
and	O
central	B
composite	I
design	I
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
jarno	O
vanhatalo	O
bayesian	B
inference	B
for	O
the	O
hyper-parameters	B
an	O
alternative	O
to	O
computing	O
a	O
point	B
estimate	I
of	O
the	O
hyper-parameters	B
is	O
to	O
compute	O
their	O
posterior	O
let	O
represent	O
all	O
the	O
kernel	B
parameters	O
as	O
well	O
as	O
y	O
if	O
the	O
dimensionality	O
of	O
is	O
small	O
we	O
can	O
compute	O
a	O
discrete	B
grid	O
of	O
possible	O
values	O
centered	O
on	O
the	O
map	B
estimate	I
as	O
above	O
we	O
can	O
then	O
approximate	O
the	O
posterior	O
over	O
the	O
latent	B
variables	O
using	O
pfd	O
pfd	O
sp	O
sd	O
s	O
where	O
s	O
denotes	O
the	O
weight	O
for	O
grid	O
point	O
s	O
in	O
higher	O
dimensions	O
a	O
regular	B
grid	O
suffers	O
from	O
the	O
curse	B
of	I
dimensionality	I
an	O
obvious	O
alternative	O
is	O
monte	B
carlo	I
but	O
this	O
can	O
be	O
slow	O
another	O
approach	O
is	O
to	O
use	O
a	O
form	O
of	O
quasimonte	O
carlo	O
whereby	O
we	O
place	O
grid	O
points	O
at	O
the	O
mode	B
and	O
at	O
a	O
distance	O
from	O
the	O
mode	B
along	O
each	O
dimension	O
for	O
a	O
total	O
of	O
points	O
this	O
is	O
called	O
a	O
central	B
composite	I
design	I
et	O
al	O
is	O
also	O
used	O
in	O
the	O
unscented	O
kalman	O
filter	O
see	O
section	O
to	O
make	O
this	O
gaussian-like	O
approximation	O
more	O
reasonable	O
we	O
often	O
log-transform	O
the	O
hyper-parameters	B
see	O
figure	O
for	O
an	O
illustration	O
chapter	O
gaussian	B
processes	I
multiple	B
kernel	B
learning	B
a	O
quite	O
different	O
approach	O
to	O
optimizing	O
kernel	B
parameters	O
known	O
as	O
multiple	B
kernel	B
learning	B
the	O
idea	O
is	O
to	O
define	O
the	O
kernel	B
as	O
a	O
weighted	O
sum	O
of	O
base	O
kernels	O
j	O
wj	O
jx	O
and	O
then	O
to	O
optimize	O
the	O
weights	O
wj	O
instead	O
of	O
the	O
kernel	B
parameters	O
themselves	O
this	O
is	O
particularly	O
useful	O
if	O
we	O
have	O
different	O
kinds	O
of	O
data	O
which	O
we	O
wish	O
to	O
fuse	O
together	O
see	O
e	O
g	O
et	O
al	O
for	O
an	O
approach	O
based	O
on	O
risk-minimization	O
and	O
convex	B
optimization	B
and	O
and	O
rogers	O
for	O
an	O
approach	O
based	O
on	O
variational	B
bayes	I
computational	O
and	O
numerical	O
issues	O
the	O
predictive	B
mean	B
is	O
given	O
by	O
f	O
kt	O
k	O
y	O
y	O
for	O
reasons	O
of	O
numerical	O
stability	O
it	O
is	O
unwise	O
to	O
directly	O
invert	O
ky	O
a	O
more	O
robust	B
alternative	O
is	O
to	O
compute	O
a	O
cholesky	B
decomposition	I
ky	O
llt	O
we	O
can	O
then	O
compute	O
the	O
predictive	B
mean	B
and	O
variance	B
and	O
the	O
log	O
marginal	B
likelihood	B
as	O
shown	O
in	O
the	O
pseudo-code	O
in	O
algorithm	O
on	O
and	O
williams	O
it	O
takes	O
on	O
time	O
to	O
compute	O
the	O
cholesky	B
decomposition	I
and	O
on	O
time	O
to	O
solve	O
for	O
k	O
y	O
y	O
l	O
t	O
l	O
we	O
can	O
then	O
compute	O
the	O
mean	B
using	O
kt	O
in	O
on	O
time	O
and	O
the	O
variance	B
using	O
k	O
kt	O
l	O
t	O
l	O
in	O
on	O
time	O
for	O
each	O
test	O
case	O
an	O
alternative	O
to	O
cholesky	B
decomposition	I
is	O
to	O
solve	O
the	O
linear	O
system	O
ky	O
y	O
using	O
conjugate	B
gradients	I
if	O
we	O
terminate	O
this	O
algorithm	O
after	O
k	O
iterations	O
it	O
takes	O
okn	O
time	O
it	O
gives	O
the	O
exact	O
solution	O
in	O
on	O
time	O
another	O
approach	O
is	O
to	O
if	O
we	O
run	O
for	O
k	O
n	O
approximate	O
the	O
matrix-vector	O
multiplies	O
needed	O
by	O
cg	O
using	O
the	O
fast	B
gauss	I
transform	I
et	O
al	O
however	O
this	O
doesn	O
t	O
scale	O
to	O
high-dimensional	O
inputs	O
see	O
also	O
section	O
for	O
a	O
discussion	O
of	O
other	O
speedup	O
techniques	O
yi	O
algorithm	O
gp	O
regression	B
l	O
choleskyk	O
lt	O
y	O
e	O
kt	O
v	O
l	O
k	O
yt	O
var	B
x	O
vt	O
v	O
log	O
pyx	O
i	O
log	O
lii	O
n	O
semi-parametric	O
gps	B
sometimes	O
it	O
is	O
useful	O
to	O
use	O
a	O
linear	O
model	O
for	O
the	O
mean	B
of	O
the	O
process	O
as	O
follows	O
f	O
t	O
where	O
rx	O
parametric	B
model	I
and	O
is	O
known	O
as	O
a	O
semi-parametric	B
model	I
models	O
the	O
residuals	O
this	O
combines	O
a	O
parametric	O
and	O
a	O
nonif	O
we	O
assume	O
n	O
b	O
we	O
can	O
integrate	O
these	O
parameters	O
out	O
to	O
get	O
a	O
new	O
gp	O
hagan	O
f	O
gp	O
b	O
xt	O
b	O
gps	B
meet	O
glms	O
log	O
pyifi	O
log	O
sigmyifi	O
log	O
log	O
pyifi	O
fi	O
ti	O
i	O
yi	O
f	O
i	O
log	O
pyifi	O
i	O
yifi	O
i	O
likelihood	B
gradient	O
and	O
hessian	B
for	O
binary	O
logistic	B
probit	B
gp	O
regression	B
we	O
assume	O
yi	O
table	O
and	O
define	O
ti	O
and	O
i	O
sigmfi	O
for	O
logistic	B
regression	B
and	O
i	O
for	O
probit	B
regression	B
also	O
and	O
are	O
the	O
pdf	B
and	O
cdf	B
of	O
n	O
from	O
and	O
williams	O
integrating	O
out	O
the	O
corresponding	O
predictive	B
distribution	O
for	O
test	O
inputs	O
x	O
has	O
the	O
following	O
form	O
and	O
williams	O
pf	O
x	O
y	O
cov	O
f	O
t	O
kt	O
k	O
t	O
k	O
y	O
y	O
b	O
k	O
y	O
y	O
b	O
y	O
k	O
rt	O
k	O
y	O
t	O
cov	O
kt	O
k	O
y	O
r	O
k	O
the	O
predictive	B
mean	B
is	O
the	O
output	O
of	O
the	O
linear	O
model	O
plus	O
a	O
correction	O
term	O
due	O
to	O
the	O
gp	O
and	O
the	O
predictive	B
covariance	B
is	O
the	O
usual	O
gp	O
covariance	B
plus	O
an	O
extra	O
term	O
due	O
to	O
the	O
uncertainty	B
in	O
gps	B
meet	O
glms	O
in	O
this	O
section	O
we	O
extend	O
gps	B
to	O
the	O
glm	B
setting	O
focussing	O
on	O
the	O
classification	O
case	O
as	O
with	O
bayesian	B
logistic	B
regression	B
the	O
main	O
difficulty	O
is	O
that	O
the	O
gaussian	B
prior	O
is	O
not	O
conjugate	O
to	O
the	O
bernoulli	B
multinoulli	O
likelihood	B
there	O
are	O
several	O
approximations	O
one	O
can	O
adopt	O
gaussian	B
approximation	I
expectation	B
propagation	I
and	O
rasmussen	O
nickisch	O
and	O
rasmussen	O
variational	O
and	O
rogers	O
opper	O
and	O
archambeau	O
mcmc	B
christensen	O
et	O
al	O
etc	O
here	O
we	O
focus	O
on	O
the	O
gaussian	B
approximation	I
since	O
it	O
is	O
the	O
simplest	O
and	O
fastest	O
binary	O
classification	O
in	O
the	O
binary	O
case	O
we	O
define	O
the	O
model	O
as	O
pyixi	O
where	O
following	O
and	O
williams	O
we	O
assume	O
yi	O
and	O
we	O
let	O
sigmz	O
regression	B
or	O
regression	B
as	O
for	O
gp	O
regression	B
we	O
assume	O
f	O
computing	O
the	O
posterior	O
define	O
the	O
log	O
of	O
the	O
unnormalized	O
posterior	O
as	O
follows	O
log	O
pyf	O
log	O
pfx	O
log	O
pyf	O
f	O
t	O
k	O
log	O
n	O
log	O
chapter	O
gaussian	B
processes	I
let	O
jf	O
be	O
the	O
function	O
we	O
want	O
to	O
minimize	O
the	O
gradient	O
and	O
hessian	B
of	O
this	O
are	O
given	O
by	O
g	O
log	O
pyf	O
h	O
log	O
pyf	O
w	O
k	O
note	O
that	O
w	O
log	O
pyf	O
is	O
a	O
diagonal	B
matrix	O
because	O
the	O
data	O
are	O
iid	B
on	O
f	O
expressions	O
for	O
the	O
gradient	O
and	O
hessian	B
of	O
the	O
log	O
likelihood	B
for	O
the	O
logit	B
and	O
probit	B
case	O
are	O
given	O
in	O
sections	O
and	O
and	O
summarized	O
in	O
table	O
we	O
can	O
use	O
irls	B
to	O
find	O
the	O
map	B
estimate	I
the	O
update	O
has	O
the	O
form	O
f	O
new	O
f	O
h	O
f	O
w	O
log	O
pyf	O
k	O
w	O
log	O
pyf	O
at	O
convergence	O
the	O
gaussian	B
approximation	I
of	O
the	O
posterior	O
takes	O
the	O
following	O
form	O
pfx	O
y	O
n	O
f	O
w	O
computing	O
the	O
posterior	O
predictive	B
we	O
now	O
compute	O
the	O
posterior	O
predictive	B
first	O
we	O
predict	O
the	O
latent	B
function	O
at	O
the	O
test	O
case	O
x	O
for	O
the	O
mean	B
we	O
have	O
e	O
x	O
y	O
e	O
x	O
x	O
y	O
pfx	O
ydf	O
kt	O
k	O
pfx	O
ydf	O
kt	O
k	O
e	O
y	O
kt	O
k	O
f	O
where	O
we	O
used	O
equation	O
to	O
get	O
the	O
mean	B
of	O
f	O
given	O
noise-free	O
f	O
to	O
compute	O
the	O
predictive	B
variance	B
we	O
use	O
the	O
rule	O
of	O
iterated	O
variance	B
var	B
var	B
where	O
all	O
probabilities	O
are	O
conditioned	O
on	O
x	O
x	O
y	O
from	O
equation	O
we	O
have	O
e	O
e	O
from	O
equation	O
we	O
have	O
var	B
var	B
combining	O
these	O
we	O
get	O
k	O
kt	O
k	O
k	O
k	O
k	O
kt	O
k	O
kt	O
k	O
k	O
var	B
kt	O
k	O
k	O
from	O
equation	O
we	O
have	O
cov	O
w	O
get	O
var	B
k	O
kt	O
k	O
kt	O
k	O
w	O
k	O
kt	O
w	O
using	O
the	O
matrix	B
inversion	I
lemma	I
we	O
gps	B
meet	O
glms	O
so	O
in	O
summary	O
we	O
have	O
pf	O
x	O
y	O
n	O
var	B
to	O
convert	O
this	O
in	O
to	O
a	O
predictive	B
distribution	O
for	O
binary	O
responses	O
we	O
use	O
py	O
x	O
y	O
x	O
ydf	O
this	O
can	O
be	O
approximated	O
using	O
any	O
of	O
the	O
methods	O
discussed	O
in	O
section	O
where	O
we	O
discussed	O
bayesian	B
logistic	B
regression	B
for	O
example	O
using	O
the	O
probit	B
approximation	O
of	O
section	O
we	O
have	O
sigm	O
wherev	O
var	B
and	O
computing	O
the	O
marginal	B
likelihood	B
we	O
need	O
the	O
marginal	B
likelihood	B
in	O
order	O
to	O
optimize	O
the	O
kernel	B
parameters	O
using	O
the	O
laplace	B
approximation	I
in	O
equation	O
we	O
have	O
log	O
pyx	O
f	O
log	O
const	O
hence	O
log	O
pyx	O
log	O
py	O
f	O
computing	O
the	O
derivatives	O
log	O
pyx	O
f	O
t	O
k	O
f	O
log	O
log	O
w	O
is	O
more	O
complex	O
than	O
in	O
the	O
regression	B
case	O
since	O
f	O
and	O
w	O
as	O
well	O
as	O
k	O
depend	O
on	O
details	O
can	O
be	O
found	O
in	O
and	O
williams	O
j	O
numerically	O
stable	B
computation	O
to	O
implement	O
the	O
above	O
equations	O
in	O
a	O
numerically	O
stable	B
way	O
it	O
is	O
best	O
to	O
avoid	O
inverting	O
k	O
or	O
w	O
and	O
williams	O
suggest	O
defining	O
b	O
in	O
w	O
kw	O
which	O
has	O
eigenvalues	O
bounded	O
below	O
by	O
of	O
the	O
i	O
and	O
above	O
by	O
n	O
wii	O
and	O
hence	O
can	O
be	O
safely	O
inverted	O
maxij	O
kij	O
one	O
can	O
use	O
the	O
matrix	B
inversion	I
lemma	I
to	O
show	O
w	O
k	O
kw	O
hence	O
the	O
irls	B
update	O
becomes	O
b	O
k	O
f	O
new	O
w	O
log	O
pyf	O
b	O
ki	O
w	O
k	O
w	O
kb	O
b	O
lt	O
kb	O
a	O
chapter	O
gaussian	B
processes	I
where	O
b	O
llt	O
is	O
a	O
cholesky	B
decomposition	I
of	O
b	O
the	O
fitting	O
algorithm	O
takes	O
in	O
ot	O
n	O
time	O
and	O
on	O
space	O
where	O
t	O
is	O
the	O
number	O
of	O
newton	O
iterations	O
at	O
convergence	O
we	O
have	O
a	O
k	O
f	O
so	O
we	O
can	O
evaluate	O
the	O
log	O
marginal	B
likelihood	B
tion	O
using	O
log	O
pyx	O
log	O
py	O
f	O
at	O
f	O
i	O
log	O
lii	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
w	O
w	O
we	O
now	O
compute	O
the	O
predictive	B
distribution	O
rather	O
than	O
using	O
e	O
kt	O
k	O
f	O
we	O
exploit	O
the	O
fact	O
that	O
at	O
the	O
mode	B
so	O
f	O
k	O
log	O
py	O
f	O
hence	O
we	O
can	O
rewrite	O
the	O
predictive	B
mean	B
as	O
kw	O
e	O
kt	O
log	O
py	O
f	O
to	O
compute	O
the	O
predictive	B
variance	B
we	O
exploit	O
the	O
fact	O
that	O
w	O
w	O
w	O
w	O
w	O
w	O
b	O
to	O
get	O
var	B
k	O
kt	O
w	O
k	O
k	O
vt	O
v	O
where	O
v	O
l	O
k	O
we	O
can	O
then	O
compute	O
the	O
whole	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
based	O
on	O
and	O
williams	O
fitting	O
takes	O
on	O
time	O
and	O
prediction	O
takes	O
on	O
time	O
where	O
n	O
is	O
the	O
number	O
of	O
test	O
cases	O
example	O
in	O
figure	O
we	O
show	O
a	O
synthetic	O
binary	O
classification	O
problem	O
in	O
we	O
use	O
an	O
se	O
kernel	B
on	O
the	O
left	O
we	O
show	O
predictions	O
using	O
hyper-parameters	B
set	O
by	O
hand	O
we	O
use	O
a	O
short	O
length	O
scale	O
hence	O
the	O
very	O
sharp	O
turns	O
in	O
the	O
decision	B
boundary	I
on	O
the	O
right	O
we	O
show	O
the	O
predictions	O
using	O
the	O
learned	O
hyper-parameters	B
the	O
model	O
favors	O
a	O
more	O
parsimonious	O
explanation	O
of	O
the	O
data	O
multi-class	O
classification	O
in	O
this	O
section	O
we	O
consider	O
a	O
model	O
of	O
the	O
form	O
pyixi	O
catyisfi	O
where	O
fi	O
fic	O
and	O
we	O
assume	O
f	O
c	O
c	O
thus	O
we	O
have	O
one	O
latent	B
function	O
per	O
class	O
which	O
are	O
a	O
priori	O
independent	O
and	O
which	O
may	O
use	O
different	O
kernels	O
as	O
before	O
we	O
will	O
use	O
a	O
gaussian	B
approximation	I
to	O
the	O
posterior	O
similar	B
model	O
but	O
using	O
the	O
multinomial	B
probit	B
function	O
instead	O
of	O
the	O
multinomial	B
logit	B
is	O
described	O
in	O
and	O
rogers	O
we	O
see	O
that	O
training	O
points	O
that	O
are	O
well-predicted	O
by	O
the	O
model	O
for	O
which	O
i	O
log	O
pyifi	O
do	O
not	O
contribute	O
strongly	O
to	O
the	O
prediction	O
at	O
test	O
points	O
this	O
is	O
similar	B
to	O
the	O
behavior	O
of	O
support	B
vectors	I
in	O
an	O
svm	B
section	O
gps	B
meet	O
glms	O
algorithm	O
gp	O
binary	O
classification	O
using	O
gaussian	B
approximation	I
first	O
compute	O
map	B
estimate	I
using	O
irls	B
f	O
repeat	O
w	O
log	O
pyf	O
b	O
in	O
w	O
kw	O
l	O
choleskyb	O
b	O
wf	O
log	O
pyf	O
a	O
b	O
w	O
f	O
ka	O
lt	O
kb	O
at	O
f	O
until	O
converged	O
log	O
pyx	O
log	O
pyf	O
now	O
perform	O
prediction	O
e	O
kt	O
log	O
pyf	O
v	O
l	O
k	O
var	B
k	O
vt	O
v	O
sigmzn	O
var	B
py	O
i	O
log	O
lii	O
se	O
kernel	B
se	O
kernel	B
figure	O
contours	O
of	O
the	O
posterior	O
predictive	B
probability	O
for	O
the	O
red	O
circle	O
class	O
generated	O
by	O
a	O
gp	O
with	O
an	O
se	O
kernel	B
thick	O
black	O
line	O
is	O
the	O
decision	B
boundary	I
if	O
we	O
threshold	O
at	O
a	O
probability	O
of	O
manual	O
parameters	O
short	O
length	O
scale	O
learned	O
parameters	O
long	O
length	O
scale	O
figure	O
generated	O
by	O
based	O
on	O
code	O
by	O
carl	O
rasmussen	O
chapter	O
gaussian	B
processes	I
computing	O
the	O
posterior	O
the	O
unnormalized	O
log	O
posterior	O
is	O
given	O
by	O
f	O
t	O
k	O
yt	O
f	O
log	O
where	O
exp	O
fic	O
log	O
cn	O
log	O
f	O
fn	O
fn	O
fn	O
ct	O
and	O
y	O
is	O
a	O
dummy	B
encoding	I
of	O
the	O
yi	O
s	O
which	O
has	O
the	O
same	O
layout	O
as	O
f	O
also	O
k	O
is	O
a	O
block	O
diagonal	B
matrix	O
containing	O
kc	O
where	O
kc	O
cxi	O
xj	O
models	O
the	O
correlation	O
of	O
the	O
c	O
th	O
latent	B
function	O
the	O
gradient	O
and	O
hessian	B
are	O
given	O
by	O
k	O
y	O
k	O
w	O
where	O
w	O
diag	O
t	O
where	O
is	O
a	O
cn	O
n	O
matrix	O
obtained	O
by	O
stacking	B
diag	O
vertically	O
these	O
expressions	O
to	O
standard	O
logistic	B
regression	B
in	O
section	O
we	O
can	O
use	O
irls	B
to	O
compute	O
the	O
mode	B
the	O
newton	O
step	O
has	O
the	O
form	O
f	O
new	O
w	O
y	O
naively	O
implementing	O
this	O
would	O
take	O
oc	O
time	O
however	O
we	O
can	O
reduce	O
this	O
to	O
ocn	O
as	O
shown	O
in	O
and	O
williams	O
computing	O
the	O
posterior	O
predictive	B
we	O
can	O
compute	O
the	O
posterior	O
predictive	B
in	O
a	O
manner	O
analogous	O
to	O
section	O
for	O
the	O
mean	B
of	O
the	O
latent	B
response	O
we	O
have	O
e	O
c	O
kcx	O
k	O
c	O
fc	O
kcx	O
c	O
we	O
can	O
put	O
this	O
in	O
vector	O
form	O
by	O
writing	O
e	O
q	O
t	O
where	O
q	O
kcx	O
using	O
a	O
similar	B
argument	O
to	O
equation	O
we	O
can	O
show	O
that	O
the	O
covariance	B
of	O
the	O
latent	B
response	O
is	O
given	O
by	O
cov	O
qt	O
k	O
w	O
diagkx	O
x	O
qt	O
w	O
gps	B
meet	O
glms	O
where	O
is	O
a	O
c	O
c	O
diagonal	B
matrix	O
with	O
cc	O
cx	O
x	O
kt	O
kx	O
x	O
cx	O
x	O
to	O
compute	O
the	O
posterior	O
predictive	B
for	O
the	O
visible	B
response	O
we	O
need	O
to	O
use	O
pyx	O
x	O
y	O
catysf	O
cov	O
c	O
c	O
kcx	O
and	O
we	O
can	O
use	O
any	O
of	O
deterministic	O
approximations	O
to	O
the	O
softmax	B
function	O
discussed	O
in	O
section	O
to	O
compute	O
this	O
alternatively	O
we	O
can	O
just	O
use	O
monte	B
carlo	I
computing	O
the	O
marginal	B
likelihood	B
using	O
arguments	O
similar	B
to	O
the	O
binary	O
case	O
we	O
can	O
show	O
that	O
f	O
t	O
k	O
f	O
yt	O
f	O
log	O
exp	O
fic	O
log	O
pyx	O
log	O
w	O
kw	O
this	O
can	O
be	O
optimized	O
numerically	O
in	O
the	O
usual	O
way	O
numerical	O
and	O
computational	O
issues	O
one	O
can	O
implement	O
model	O
fitting	O
in	O
ot	O
cn	O
time	O
and	O
ocn	O
space	O
where	O
t	O
is	O
the	O
number	O
of	O
newton	O
iterations	O
using	O
the	O
techniques	O
described	O
in	O
and	O
williams	O
prediction	O
takes	O
ocn	O
cn	O
time	O
where	O
n	O
is	O
the	O
number	O
of	O
test	O
cases	O
gps	B
for	O
poisson	B
regression	B
in	O
this	O
section	O
we	O
illustrate	O
gps	B
for	O
poisson	B
regression	B
an	O
interesting	O
application	O
of	O
this	O
is	O
to	O
spatial	O
disease	B
mapping	I
for	O
example	O
et	O
al	O
discuss	O
the	O
problem	O
of	O
modeling	O
the	O
relative	B
risk	B
of	O
heart	O
attack	O
in	O
different	O
regions	O
in	O
finland	O
the	O
data	O
consists	O
of	O
the	O
heart	O
attacks	O
in	O
finland	O
from	O
aggregated	O
into	O
x	O
lattice	B
cells	O
the	O
model	O
has	O
the	O
following	O
form	O
yi	O
poieiri	O
where	O
ei	O
is	O
the	O
known	O
expected	O
number	O
of	O
deaths	O
to	O
the	O
population	O
of	O
cell	O
i	O
and	O
the	O
overall	O
death	O
rate	B
and	O
ri	O
is	O
the	O
relative	B
risk	B
of	O
cell	O
i	O
which	O
we	O
want	O
to	O
infer	O
since	O
the	O
data	O
counts	O
are	O
small	O
we	O
regularize	O
the	O
problem	O
by	O
sharing	O
information	B
with	O
spatial	O
neighbors	B
hence	O
we	O
assume	O
f	O
logr	O
where	O
we	O
use	O
a	O
matern	B
kernel	B
with	O
and	O
a	O
length	O
scale	O
and	O
magnitude	O
that	O
are	O
estimated	O
from	O
data	O
figure	O
gives	O
an	O
example	O
of	O
the	O
kind	O
of	O
output	O
one	O
can	O
obtain	O
from	O
this	O
method	O
based	O
on	O
data	O
from	O
locations	O
on	O
the	O
left	O
we	O
plot	O
the	O
posterior	B
mean	B
relative	B
risk	B
and	O
on	O
the	O
right	O
the	O
posterior	O
variance	B
we	O
see	O
that	O
the	O
rr	O
is	O
higher	O
in	O
eastern	O
finland	O
which	O
is	O
consistent	B
with	O
other	O
studies	O
we	O
also	O
see	O
that	O
the	O
variance	B
in	O
the	O
north	O
is	O
higher	O
since	O
there	O
are	O
fewer	O
people	O
living	O
there	O
chapter	O
gaussian	B
processes	I
posterior	B
mean	B
of	O
the	O
relative	B
risk	B
fic	O
posterior	O
variance	B
of	O
the	O
relative	B
risk	B
fic	O
figure	O
we	O
show	O
the	O
relative	B
risk	B
of	O
heart	O
disease	O
in	O
finland	O
using	O
a	O
poisson	B
gp	O
left	O
posterior	B
mean	B
right	O
posterior	O
variance	B
figure	O
generated	O
by	O
gpspatialdemolaplace	O
written	O
by	O
jarno	O
vanhatalo	O
connection	O
with	O
other	O
methods	O
there	O
are	O
variety	O
of	O
other	O
methods	O
in	O
statistics	O
and	O
machine	B
learning	B
that	O
are	O
closely	O
related	O
to	O
gp	O
regression	B
classification	O
we	O
give	O
a	O
brief	O
review	O
of	O
some	O
of	O
these	O
below	O
linear	O
models	O
compared	O
to	O
gps	B
consider	O
bayesian	B
linear	B
regression	B
for	O
d-dimensional	O
features	B
where	O
the	O
prior	O
on	O
the	O
weights	O
is	O
pw	O
n	O
the	O
posterior	B
predictive	B
distribution	I
is	O
given	O
by	O
the	O
following	O
pf	O
x	O
y	O
n	O
xt	O
a	O
y	O
y	O
xt	O
a	O
y	O
xt	O
x	O
one	O
can	O
show	O
that	O
we	O
can	O
rewrite	O
the	O
above	O
distribution	O
as	O
where	O
a	O
follows	O
xt	O
xt	O
xt	O
x	O
xt	O
xt	O
yi	O
x	O
xt	O
where	O
we	O
have	O
defined	O
k	O
x	O
xt	O
which	O
is	O
of	O
size	O
n	O
n	O
since	O
the	O
features	B
only	O
ever	O
appear	O
in	O
the	O
form	O
x	O
xt	O
xt	O
xt	O
or	O
xt	O
x	O
we	O
can	O
kernelize	O
the	O
above	O
expression	O
by	O
defining	O
thus	O
we	O
see	O
that	O
bayesian	B
linear	B
regression	B
is	O
equivalent	O
to	O
a	O
gp	O
with	O
covariance	B
function	O
note	O
however	O
that	O
this	O
is	O
a	O
degenerate	B
covariance	B
function	O
since	O
it	O
has	O
at	O
most	O
d	O
non-zero	O
eigenvalues	O
intuitively	O
this	O
reflects	O
the	O
fact	O
that	O
the	O
model	O
can	O
only	O
represent	O
a	O
limited	O
number	O
of	O
functions	O
this	O
can	O
result	O
in	O
underfitting	O
since	O
the	O
model	O
is	O
not	O
flexible	O
enough	O
to	O
capture	O
the	O
data	O
what	O
is	O
perhaps	O
worse	O
it	O
can	O
result	O
in	O
overconfidence	O
since	O
the	O
xt	O
connection	O
with	O
other	O
methods	O
model	O
s	O
prior	O
is	O
so	O
impoverished	O
that	O
its	O
posterior	O
will	O
become	O
too	O
concentrated	O
so	O
not	O
only	O
is	O
the	O
model	O
wrong	O
it	O
think	O
it	O
s	O
right	O
linear	O
smoothers	O
compared	O
to	O
gps	B
a	O
linear	B
smoother	I
is	O
a	O
regression	B
function	O
which	O
is	O
a	O
linear	O
function	O
of	O
the	O
training	O
outputs	O
f	O
wix	O
yi	O
i	O
where	O
wix	O
is	O
called	O
the	O
weight	B
function	I
not	O
confuse	O
this	O
with	O
a	O
linear	O
model	O
where	O
the	O
output	O
is	O
a	O
linear	O
function	O
of	O
the	O
input	O
vector	O
there	O
are	O
a	O
variety	O
of	O
linear	O
smoothers	O
such	O
as	O
kernel	B
regression	B
locally	B
weighted	I
regression	B
smoothing	B
splines	I
and	O
gp	O
regression	B
to	O
see	O
that	O
gp	O
regession	O
is	O
a	O
linear	B
smoother	I
note	O
that	O
the	O
mean	B
of	O
the	O
posterior	B
predictive	B
distribution	I
of	O
a	O
gp	O
is	O
given	O
by	O
f	O
kt	O
yin	O
yiwix	O
where	O
wix	O
yin	O
in	O
kernel	B
regression	B
we	O
derive	O
the	O
weight	B
function	I
from	O
a	O
smoothing	B
kernel	B
rather	O
than	O
a	O
mercer	B
kernel	B
so	O
it	O
is	O
clear	O
that	O
the	O
weight	B
function	I
will	O
then	O
have	O
local	O
support	B
in	O
the	O
case	O
of	O
a	O
gp	O
things	O
are	O
not	O
as	O
clear	O
since	O
the	O
weight	B
function	I
depends	O
on	O
the	O
inverse	O
of	O
k	O
for	O
certain	O
gp	O
kernel	B
functions	O
we	O
can	O
analytically	O
derive	O
the	O
form	O
of	O
wix	O
this	O
is	O
known	O
as	O
the	O
wix	O
although	O
we	O
may	O
equivalent	B
kernel	B
one	O
can	O
show	O
that	O
have	O
wix	O
so	O
we	O
are	O
computing	O
a	O
linear	O
combination	O
but	O
not	O
a	O
convex	B
combination	I
of	O
the	O
yi	O
s	O
more	O
interestingly	O
wix	O
is	O
a	O
local	O
function	O
even	O
if	O
the	O
original	O
kernel	B
used	O
by	O
the	O
gp	O
is	O
not	O
local	O
futhermore	O
the	O
effective	O
bandwidth	B
of	O
the	O
equivalent	B
kernel	B
of	O
a	O
gp	O
automatically	O
decreases	O
as	O
the	O
sample	O
size	O
n	O
increases	O
whereas	O
in	O
kernel	B
smoothing	B
the	O
bandwidth	B
h	O
needs	O
to	O
be	O
set	O
by	O
hand	O
to	O
adapt	O
to	O
n	O
see	O
e	O
g	O
and	O
williams	O
sec	O
for	O
details	O
degrees	B
of	I
freedom	I
of	O
linear	O
smoothers	O
it	O
is	O
clear	O
why	O
this	O
method	O
is	O
called	O
linear	O
but	O
why	O
is	O
it	O
called	O
a	O
smoother	O
this	O
is	O
best	O
explained	O
in	O
terms	O
of	O
gps	B
consider	O
the	O
prediction	O
on	O
the	O
training	B
set	I
f	O
kk	O
y	O
iuiut	O
i	O
since	O
k	O
is	O
real	O
and	O
symmetric	B
now	O
let	O
k	O
have	O
the	O
eigendecomposition	B
k	O
positive	O
definite	O
the	O
eigenvalues	O
i	O
are	O
real	O
and	O
non-negative	O
and	O
the	O
eigenvectors	O
ui	O
are	O
orthonormal	O
now	O
let	O
y	O
i	O
y	O
then	O
we	O
can	O
rewrite	O
the	O
above	O
equation	O
as	O
follows	O
iui	O
where	O
i	O
ut	O
i	O
i	O
i	O
y	O
ui	O
f	O
chapter	O
gaussian	B
processes	I
this	O
is	O
the	O
same	O
as	O
equation	O
except	O
we	O
are	O
working	O
with	O
the	O
eigenvectors	O
of	O
the	O
gram	B
matrix	I
k	O
instead	O
of	O
the	O
data	O
matrix	O
x	O
in	O
any	O
case	O
the	O
interpretation	O
is	O
similar	B
if	O
then	O
the	O
corresponding	O
basis	O
function	O
ui	O
will	O
not	O
have	O
much	O
influence	O
consequently	O
the	O
highfrequency	O
components	O
in	O
y	O
are	O
smoothed	O
out	O
the	O
effective	O
degrees	B
of	I
freedom	I
of	O
the	O
linear	B
smoother	I
is	O
defined	O
as	O
dof	O
trkk	O
i	O
y	O
i	O
i	O
yi	O
i	O
y	O
this	O
specifies	O
how	O
wiggly	O
the	O
curve	O
is	O
svms	O
compared	O
to	O
gps	B
we	O
saw	O
in	O
section	O
that	O
the	O
svm	B
objective	O
for	O
binary	O
classification	O
is	O
given	O
by	O
equation	O
jw	O
yifi	O
c	O
ij	O
i	O
jxt	O
we	O
also	O
know	O
from	O
equation	O
that	O
the	O
optimal	O
solution	O
has	O
the	O
form	O
w	O
i	O
ixi	O
i	O
xj	O
kernelizing	O
we	O
get	O
k	O
from	O
equation	O
and	O
so	O
absorbing	O
the	O
term	O
into	O
one	O
of	O
the	O
kernels	O
we	O
have	O
f	O
k	O
so	O
f	O
t	O
k	O
hence	O
the	O
svm	B
objective	O
can	O
be	O
rewritten	O
as	O
yifi	O
f	O
t	O
f	O
c	O
jf	O
compare	O
this	O
to	O
map	O
estimation	O
for	O
gp	O
classifier	O
f	O
t	O
f	O
jf	O
log	O
pyifi	O
it	O
is	O
tempting	O
to	O
think	O
that	O
we	O
can	O
convert	O
an	O
svm	B
into	O
a	O
gp	O
by	O
figuring	O
out	O
what	O
likelihood	B
would	O
be	O
equivalent	O
to	O
the	O
hinge	B
loss	B
however	O
it	O
turns	O
out	O
there	O
is	O
no	O
such	O
likelihood	B
although	O
there	O
is	O
a	O
pseudo-likelihood	B
that	O
matches	O
the	O
svm	B
section	O
from	O
figure	O
we	O
saw	O
that	O
the	O
hinge	B
loss	B
and	O
the	O
logistic	B
loss	B
well	O
as	O
the	O
probit	B
loss	B
are	O
quite	O
similar	B
to	O
each	O
other	O
the	O
main	O
difference	O
is	O
that	O
the	O
hinge	B
loss	B
is	O
strictly	O
for	O
errors	O
larger	O
than	O
this	O
gives	O
rise	O
to	O
a	O
sparse	B
solution	O
in	O
section	O
we	O
discussed	O
other	O
ways	O
to	O
derive	O
sparse	B
kernel	B
machines	O
we	O
discuss	O
the	O
connection	O
between	O
these	O
methods	O
and	O
gps	B
below	O
and	O
rvms	O
compared	O
to	O
gps	B
sparse	B
kernel	B
machines	O
are	O
just	O
linear	O
models	O
with	O
basis	B
function	I
expansion	I
of	O
the	O
form	O
xn	O
from	O
section	O
we	O
know	O
that	O
this	O
is	O
equivalent	O
to	O
a	O
gp	O
with	O
the	O
following	O
kernel	B
jx	O
j	O
connection	O
with	O
other	O
methods	O
where	O
pw	O
n	O
diag	O
j	O
this	O
kernel	B
function	I
has	O
two	O
interesting	O
properties	O
first	O
it	O
is	O
degenerate	B
meaning	O
it	O
has	O
at	O
most	O
n	O
non-zero	O
eigenvalues	O
so	O
the	O
joint	B
distribution	I
pf	O
f	O
will	O
be	O
highly	O
constrained	O
second	O
the	O
kernel	B
depends	O
on	O
the	O
training	O
data	O
this	O
can	O
cause	O
the	O
model	O
to	O
be	O
overconfident	O
when	O
extrapolating	O
beyond	O
the	O
training	O
data	O
to	O
see	O
this	O
consider	O
a	O
point	O
x	O
far	O
outside	O
the	O
convex	B
hull	I
of	O
the	O
data	O
all	O
the	O
basis	B
functions	I
will	O
have	O
values	O
close	O
to	O
so	O
the	O
prediction	O
will	O
back	O
off	O
to	O
the	O
mean	B
of	O
the	O
gp	O
more	O
worryingly	O
the	O
variance	B
will	O
back	O
off	O
to	O
the	O
noise	O
variance	B
by	O
contrast	O
when	O
using	O
a	O
non-degenerate	O
kernel	B
function	I
the	O
predictive	B
variance	B
increases	O
as	O
we	O
move	O
away	O
from	O
the	O
training	O
data	O
as	O
desired	O
see	O
and	O
qui	O
onero-candela	O
for	O
further	O
discussion	O
neural	B
networks	I
compared	O
to	O
gps	B
in	O
section	O
we	O
will	O
discuss	O
neural	B
networks	I
which	O
are	O
a	O
nonlinear	O
generalization	B
of	O
glms	O
in	O
the	O
binary	O
classification	O
case	O
a	O
neural	B
network	I
is	O
defined	O
by	O
a	O
logistic	B
regression	B
model	O
applied	O
to	O
a	O
logistic	B
regression	B
model	O
pyx	O
ber	O
ysigm	O
wt	O
sigmvx	O
it	O
turns	O
out	O
there	O
is	O
an	O
interesting	O
connection	O
between	O
neural	B
networks	I
and	O
gaussian	B
processes	I
as	O
first	O
pointed	O
out	O
by	O
to	O
explain	O
the	O
connection	O
we	O
follow	O
the	O
presentation	O
of	O
and	O
williams	O
consider	O
a	O
neural	B
network	I
for	O
regression	B
with	O
one	O
hidden	B
layer	I
this	O
has	O
the	O
form	O
pyx	O
n	O
where	O
f	O
b	O
vjgx	O
uj	O
where	O
b	O
is	O
the	O
offset	O
of	O
bias	B
term	I
vj	O
is	O
the	O
output	O
weight	O
from	O
hidden	B
unit	O
j	O
to	O
the	O
response	O
y	O
uj	O
are	O
the	O
inputs	O
weights	O
to	O
unit	O
j	O
from	O
the	O
input	O
x	O
and	O
g	O
is	O
the	O
hidden	B
unit	O
activation	B
function	O
this	O
is	O
typically	O
the	O
sigmoid	B
or	O
tanh	O
function	O
but	O
can	O
be	O
any	O
smooth	O
function	O
j	O
n	O
w	O
u	O
let	O
us	O
use	O
the	O
following	O
priors	O
on	O
the	O
weights	O
where	O
b	O
n	O
j	O
puj	O
for	O
some	O
unspecified	O
puj	O
denoting	O
all	O
the	O
weights	O
by	O
we	O
have	O
b	O
v	O
e	O
e	O
b	O
j	O
b	O
h	O
v	O
ev	O
v	O
eu	O
u	O
uj	O
v	O
scale	O
as	O
where	O
the	O
last	O
equality	O
follows	O
since	O
the	O
h	O
hidden	B
units	I
are	O
iid	B
more	O
hidden	B
units	I
will	O
increase	O
the	O
input	O
to	O
the	O
final	O
node	O
so	O
we	O
should	O
scale	O
down	O
the	O
magnitude	O
of	O
the	O
weights	O
then	O
the	O
last	O
term	O
becomes	O
u	O
this	O
is	O
a	O
sum	O
over	O
h	O
iid	B
random	O
variables	O
assuming	O
that	O
g	O
is	O
bounded	O
we	O
can	O
apply	O
the	O
central	B
limit	I
theorem	I
the	O
result	O
is	O
that	O
as	O
h	O
we	O
get	O
a	O
gaussian	B
process	I
if	O
we	O
let	O
eu	O
chapter	O
gaussian	B
processes	I
x	O
t	O
u	O
p	O
n	O
i	O
input	O
x	O
x	O
f	O
t	O
u	O
p	O
t	O
u	O
o	O
input	O
x	O
figure	O
covariance	B
function	O
n	O
n	O
this	O
kernel	B
using	O
various	O
values	O
of	O
figure	O
generated	O
by	O
gpnndemo	O
written	O
by	O
chris	O
williams	O
for	O
samples	B
from	O
from	O
a	O
gp	O
with	O
z	O
ujxj	O
where	O
erfz	O
e	O
dt	O
and	O
we	O
choose	O
u	O
n	O
then	O
showed	O
that	O
the	O
covariance	B
if	O
we	O
use	O
as	O
activation	B
transfer	B
function	I
gx	O
u	O
kernel	B
has	O
the	O
form	O
n	O
n	O
sin	O
xt	O
xt	O
where	O
x	O
xd	O
this	O
is	O
a	O
true	O
neural	B
network	I
kernel	B
unlike	O
the	O
sigmoid	B
kernel	B
tanha	O
bxt	O
which	O
is	O
not	O
positive	O
definite	O
figure	O
illustrates	O
this	O
kernel	B
when	O
d	O
and	O
diag	O
figure	O
shows	O
some	O
functions	O
sampled	O
from	O
the	O
corresponding	O
gp	O
these	O
are	O
equivalent	O
to	O
functions	O
which	O
are	O
superpositions	O
of	O
ux	O
where	O
and	O
u	O
are	O
random	O
as	O
increases	O
the	O
variance	B
of	O
u	O
increases	O
so	O
the	O
function	O
varies	O
more	O
quickly	O
unlike	O
the	O
rbf	B
kernel	B
functions	O
sampled	O
from	O
this	O
kernel	B
do	O
not	O
tend	O
to	O
away	O
from	O
the	O
data	O
but	O
rather	O
they	O
tend	O
to	O
remain	O
at	O
the	O
same	O
value	O
they	O
had	O
at	O
the	O
edge	O
of	O
the	O
data	O
of	O
the	O
form	O
gx	O
u	O
exp	O
coresponding	O
kernel	B
is	O
equivalent	O
to	O
the	O
rbf	B
or	O
se	O
kernel	B
now	O
suppose	O
we	O
use	O
an	O
rbf	B
network	I
which	O
is	O
equivalent	O
to	O
a	O
hidden	B
unit	O
activation	B
function	O
ui	O
one	O
can	O
show	O
that	O
the	O
if	O
u	O
n	O
g	O
smoothing	B
splines	I
compared	O
to	O
gps	B
smoothing	B
splines	I
are	O
a	O
widely	O
used	O
non-parametric	O
method	O
for	O
smoothly	O
interpolating	O
data	O
and	O
silverman	O
they	O
are	O
are	O
a	O
special	O
case	O
of	O
gps	B
as	O
we	O
will	O
see	O
they	O
are	O
usually	O
used	O
when	O
the	O
input	O
is	O
or	O
dimensional	O
univariate	O
splines	O
the	O
basic	O
idea	O
is	O
to	O
fit	O
a	O
function	O
f	O
by	O
minimizing	O
the	O
discrepancy	O
to	O
the	O
data	O
plus	O
a	O
smoothing	B
if	O
we	O
penalize	O
the	O
m	O
th	O
derivative	O
of	O
the	O
term	O
that	O
penalizes	O
functions	O
that	O
are	O
too	O
wiggly	O
connection	O
with	O
other	O
methods	O
function	O
the	O
objective	O
becomes	O
jf	O
dm	O
dxm	O
f	O
one	O
can	O
show	O
and	O
silverman	O
that	O
the	O
solution	O
is	O
a	O
piecewise	B
polynomial	I
where	O
the	O
polynomials	O
have	O
order	O
in	O
the	O
interior	O
bins	O
xi	O
i	O
and	O
order	O
m	O
in	O
the	O
two	O
outermost	O
intervals	O
and	O
f	O
jxj	O
ix	O
i	O
ix	O
ix	O
i	O
ix	O
xim	O
m	O
for	O
example	O
if	O
m	O
we	O
get	O
the	O
cubic	B
spline	B
f	O
ix	O
i	O
ix	O
ix	O
i	O
ix	O
xi	O
which	O
is	O
a	O
series	O
of	O
truncated	O
cubic	O
polynomials	O
whose	O
left	O
hand	O
sides	O
are	O
located	O
at	O
each	O
of	O
the	O
n	O
training	O
points	O
fact	O
that	O
the	O
model	O
is	O
linear	O
on	O
the	O
edges	B
prevents	O
it	O
from	O
extrapolating	O
too	O
wildly	O
beyond	O
the	O
range	O
of	O
the	O
data	O
if	O
we	O
drop	O
this	O
requirement	O
we	O
get	O
an	O
unrestricted	O
spline	B
t	O
y	O
where	O
the	O
for	O
i	O
and	O
xi	O
for	O
i	O
or	O
i	O
n	O
columns	O
of	O
are	O
xi	O
and	O
however	O
we	O
can	O
also	O
derive	O
an	O
on	O
time	O
method	O
and	O
silverman	O
sec	O
we	O
can	O
clearly	O
fit	O
this	O
model	O
using	O
ridge	B
regression	B
w	O
t	O
in	O
regression	B
splines	O
in	O
general	O
we	O
can	O
place	O
the	O
polynomials	O
at	O
a	O
fixed	O
set	O
of	O
k	O
locations	O
known	O
as	O
knots	B
denoted	O
k	O
the	O
result	O
is	O
called	O
a	O
regression	B
spline	B
this	O
is	O
a	O
parametric	B
model	I
which	O
uses	O
basis	B
function	I
expansion	I
of	O
the	O
following	O
form	O
we	O
drop	O
the	O
interior	O
exterior	O
distinction	O
for	O
simplicity	O
f	O
jx	O
choosing	O
the	O
number	O
and	O
locations	O
of	O
the	O
knots	B
is	O
just	O
like	O
choosing	O
the	O
number	O
and	O
values	O
of	O
the	O
support	B
vectors	I
in	O
section	O
if	O
we	O
impose	O
an	O
regularizer	O
on	O
the	O
regression	B
coefficients	O
j	O
the	O
method	O
is	O
known	O
as	O
penalized	B
splines	I
see	O
section	O
for	O
a	O
practical	O
example	O
of	O
penalized	B
splines	I
the	O
connection	O
with	O
gps	B
one	O
can	O
show	O
and	O
williams	O
that	O
the	O
cubic	B
spline	B
is	O
the	O
map	B
estimate	I
of	O
the	O
following	O
function	O
f	O
rx	O
chapter	O
gaussian	B
processes	I
where	O
p	O
j	O
that	O
we	O
don	O
t	O
penalize	O
the	O
zero	O
th	O
and	O
first	O
derivatives	O
of	O
f	O
and	O
rx	O
f	O
spx	O
spx	O
where	O
udu	O
note	O
that	O
the	O
kernel	B
in	O
equation	O
is	O
rather	O
unnatural	O
and	O
indeed	O
posterior	O
samples	B
from	O
the	O
resulting	O
gp	O
are	O
rather	O
unsmooth	O
however	O
the	O
posterior	O
modemean	O
is	O
smooth	O
this	O
shows	O
that	O
regularizers	O
don	O
t	O
always	O
make	O
good	O
priors	O
input	O
splines	O
one	O
can	O
generalize	B
cubic	O
splines	O
to	O
input	O
by	O
defining	O
a	O
regularizer	O
of	O
the	O
following	O
form	O
one	O
can	O
show	O
that	O
the	O
solution	O
has	O
the	O
form	O
i	O
ix	O
f	O
t	O
x	O
where	O
ix	O
xi	O
and	O
log	O
this	O
is	O
known	O
as	O
a	O
thin	B
plate	I
spline	B
this	O
is	O
equivalent	O
to	O
map	O
estimation	O
with	O
a	O
gp	O
whose	O
kernel	B
is	O
defined	O
in	O
and	O
fitzgibbon	O
higher-dimensional	O
inputs	O
it	O
is	O
hard	O
to	O
analytically	O
solve	O
for	O
the	O
form	O
of	O
the	O
optimal	O
solution	O
when	O
using	O
higher-order	O
inputs	O
however	O
in	O
the	O
parametric	O
regression	B
spline	B
setting	O
where	O
we	O
forego	O
the	O
regularizer	O
on	O
f	O
we	O
have	O
more	O
freedom	O
in	O
defining	O
our	O
basis	B
functions	I
one	O
way	O
to	O
handle	O
multiple	O
inputs	O
is	O
to	O
use	O
a	O
tensor	B
product	I
basis	I
defined	O
as	O
the	O
cross	O
product	O
of	O
basis	B
functions	I
for	O
example	O
for	O
input	O
we	O
can	O
define	O
f	O
m	O
m	O
m	O
it	O
is	O
clear	O
that	O
for	O
high-dimensional	O
data	O
we	O
cannot	O
allow	O
higher-order	O
interactions	O
because	O
there	O
will	O
be	O
too	O
many	O
parameters	O
to	O
fit	O
one	O
approach	O
to	O
this	O
problem	O
is	O
to	O
use	O
a	O
search	O
procedure	O
to	O
look	O
for	O
useful	O
interaction	O
terms	O
this	O
is	O
known	O
as	O
mars	B
which	O
stands	O
for	O
multivariate	B
adaptive	I
regression	B
splines	I
see	O
section	O
for	O
details	O
rkhs	B
methods	O
compared	O
to	O
gps	B
we	O
can	O
generalize	B
the	O
idea	O
of	O
penalizing	O
derivatives	O
of	O
functions	O
as	O
used	O
in	O
smoothing	B
splines	I
to	O
fit	O
functions	O
with	O
a	O
more	O
general	O
notion	O
of	O
smoothness	O
recall	B
from	O
section	O
that	O
connection	O
with	O
other	O
methods	O
mercer	O
s	O
theorem	O
says	O
that	O
any	O
positive	O
definite	O
kernel	B
function	I
can	O
be	O
represented	O
in	O
terms	O
of	O
eigenfunctions	O
i	O
ix	O
the	O
i	O
form	O
an	O
orthormal	O
basis	O
for	O
a	O
function	O
space	O
hk	O
f	O
i	O
i	O
f	O
fi	O
ix	O
now	O
define	O
the	O
inner	O
product	O
between	O
two	O
functions	O
f	O
gi	O
ix	O
in	O
this	O
space	O
as	O
follows	O
figi	O
i	O
fi	O
ix	O
and	O
gx	O
in	O
exercise	O
we	O
show	O
that	O
this	O
definition	O
implies	O
that	O
this	O
is	O
called	O
the	O
reproducing	B
property	I
and	O
the	O
space	O
of	O
functions	O
hk	O
is	O
called	O
a	O
reproducing	B
kernel	B
hilbert	I
space	I
or	O
rkhs	B
now	O
consider	O
an	O
optimization	B
problem	O
of	O
the	O
form	O
jf	O
y	O
f	O
where	O
is	O
the	O
norm	B
of	I
a	I
function	I
h	O
f	O
i	O
i	O
the	O
intuition	O
is	O
that	O
functions	O
that	O
are	O
complex	O
wrt	O
the	O
kernel	B
will	O
have	O
large	O
norms	O
because	O
they	O
will	O
need	O
many	O
eigenfunctions	O
to	O
represent	O
them	O
we	O
want	O
to	O
pick	O
a	O
simple	O
function	O
that	O
provides	O
a	O
good	O
fit	O
to	O
the	O
data	O
one	O
can	O
show	O
e	O
g	O
and	O
smola	O
that	O
the	O
solution	O
must	O
have	O
the	O
form	O
f	O
i	O
xi	O
this	O
is	O
known	O
as	O
the	O
representer	B
theorem	I
and	O
holds	O
for	O
other	O
convex	B
loss	B
functions	O
besides	O
squared	B
error	I
i	O
xi	O
and	O
using	O
the	O
reproducing	O
we	O
can	O
solve	O
for	O
the	O
by	O
substituting	O
in	O
f	O
property	O
to	O
get	O
j	O
k	O
y	O
t	O
k	O
minimizing	O
wrt	O
we	O
find	O
yi	O
and	O
hence	O
f	O
i	O
xi	O
kt	O
yi	O
chapter	O
gaussian	B
processes	I
i	O
this	O
is	O
identical	O
to	O
equation	O
the	O
posterior	B
mean	B
of	O
a	O
gp	O
predictive	B
distribution	O
indeed	O
since	O
the	O
mean	B
and	O
mode	B
of	O
a	O
gaussian	B
are	O
the	O
same	O
we	O
can	O
see	O
that	O
linear	O
regresson	O
with	O
an	O
rkhs	B
regularizer	O
is	O
equivalent	O
to	O
map	O
estimation	O
with	O
a	O
gp	O
an	O
analogous	O
statement	O
holds	O
for	O
the	O
gp	O
logistic	B
regression	B
case	O
which	O
also	O
uses	O
a	O
convex	B
likelihood	B
loss	B
function	I
gp	O
latent	B
variable	O
model	O
in	O
section	O
we	O
discussed	O
kernel	B
pca	B
which	O
applies	O
the	O
kernel	B
trick	I
to	O
regular	B
pca	B
in	O
this	O
section	O
we	O
discuss	O
a	O
different	O
way	O
to	O
combine	O
kernels	O
with	O
probabilistic	B
pca	B
the	O
resulting	O
method	O
is	O
known	O
as	O
the	O
gp-lvm	B
which	O
stands	O
for	O
gaussian	B
process	I
latent	B
variable	O
model	O
to	O
explain	O
the	O
method	O
we	O
start	O
with	O
ppca	B
recall	B
from	O
section	O
that	O
the	O
ppca	B
model	O
is	O
as	O
follows	O
pzi	O
i	O
pyizi	O
we	O
can	O
fit	O
this	O
model	O
by	O
maximum	O
likelihood	B
by	O
integrating	O
out	O
the	O
zi	O
and	O
maximizing	O
w	O
the	O
objective	O
is	O
given	O
by	O
pyw	O
exp	O
trc	O
y	O
where	O
c	O
wwt	O
as	O
we	O
showed	O
in	O
theorem	O
the	O
mle	B
for	O
this	O
can	O
be	O
computed	O
in	O
terms	O
of	O
the	O
eigenvectors	O
of	O
yt	O
y	O
now	O
we	O
consider	O
the	O
dual	O
problem	O
whereby	O
we	O
maximize	O
z	O
and	O
integrate	B
out	I
w	O
we	O
will	O
j	O
n	O
i	O
the	O
corresponding	O
likelihood	B
becomes	O
use	O
a	O
prior	O
of	O
the	O
form	O
pw	O
pyz	O
n	O
zzt	O
exp	O
trk	O
z	O
yyt	O
where	O
kz	O
zzt	O
based	O
on	O
our	O
discussion	O
of	O
the	O
connection	O
between	O
the	O
eigenvalues	O
of	O
yyt	O
and	O
of	O
yt	O
y	O
in	O
section	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
we	O
can	O
also	O
solve	O
the	O
dual	O
problem	O
using	O
eigenvalue	O
methods	O
for	O
the	O
details	O
if	O
we	O
use	O
a	O
linear	B
kernel	B
we	O
recover	O
pca	B
but	O
we	O
can	O
also	O
use	O
a	O
more	O
general	O
kernel	B
kz	O
k	O
where	O
k	O
is	O
the	O
gram	B
matrix	I
for	O
z	O
the	O
mle	B
for	O
z	O
will	O
no	O
longer	O
be	O
available	O
gp	O
latent	B
variable	O
model	O
figure	O
representation	O
of	O
dimensional	O
oil	O
flow	O
data	O
the	O
different	O
colorssymbols	O
represent	O
the	O
phases	O
of	O
oil	O
flow	O
gp-lvm	B
with	O
gaussian	B
kernel	B
the	O
shading	O
represents	O
the	O
precision	B
of	O
the	O
posterior	O
where	O
lighter	O
pixels	O
have	O
higher	O
precision	B
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
neil	O
lawrence	O
kernel	B
pca	B
with	O
gaussian	B
kernel	B
via	O
eigenvalue	O
methods	O
instead	O
we	O
must	O
use	O
gradient-based	O
optimization	B
the	O
objective	O
is	O
given	O
by	O
d	O
log	O
trk	O
z	O
yyt	O
and	O
the	O
gradient	O
is	O
given	O
by	O
zij	O
kz	O
kz	O
zij	O
where	O
kz	O
k	O
z	O
yyt	O
k	O
z	O
dk	O
z	O
the	O
form	O
of	O
kz	O
where	O
kz	O
zzt	O
we	O
have	O
kz	O
optimizer	O
such	O
as	O
conjugate	O
gradient	B
descent	I
zij	O
will	O
of	O
course	O
depend	O
on	O
the	O
kernel	B
used	O
example	O
with	O
a	O
linear	B
kernel	B
z	O
z	O
we	O
can	O
then	O
pass	O
this	O
gradient	O
to	O
any	O
standard	O
let	O
us	O
now	O
compare	O
gp-lvm	B
to	O
kernel	B
pca	B
in	O
kpca	O
we	O
learn	O
a	O
kernelized	O
mapping	O
from	O
the	O
observed	O
space	O
to	O
the	O
latent	B
space	O
whereas	O
in	O
gp-lvm	B
we	O
learn	O
a	O
kernelized	O
mapping	O
from	O
the	O
latent	B
space	O
to	O
the	O
observed	O
space	O
figure	O
illustrates	O
the	O
results	O
of	O
applying	O
kpca	O
and	O
gp-lvm	B
to	O
visualize	O
the	O
dimensional	O
oil	O
flow	O
data	O
shown	O
in	O
in	O
figure	O
we	O
see	O
that	O
the	O
embedding	B
produced	O
by	O
gp-lvm	B
is	O
far	O
better	O
if	O
we	O
perform	O
nearest	B
neighbor	I
classification	O
in	O
the	O
latent	B
space	O
gp-lvm	B
makes	O
errors	O
while	O
kernel	B
pca	B
the	O
same	O
kernel	B
but	O
separately	O
optimized	O
hyper-parameters	B
makes	O
errors	O
and	O
regular	B
pca	B
makes	O
errors	O
gp-lvm	B
inherits	O
the	O
usual	O
advantages	O
of	O
probabilistic	O
generative	O
models	O
such	O
as	O
the	O
ability	O
to	O
handle	O
missing	B
data	I
and	O
data	O
of	O
different	O
types	O
the	O
ability	O
to	O
use	O
gradient-based	O
methods	O
of	O
grid	O
search	O
to	O
tune	O
the	O
kernel	B
parameters	O
the	O
ability	O
to	O
handle	O
prior	O
information	B
chapter	O
gaussian	B
processes	I
etc	O
for	O
a	O
discussion	O
of	O
some	O
other	O
probabilistic	O
methods	O
for	O
dimensionality	B
reduction	I
see	O
approximation	O
methods	O
for	O
large	O
datasets	O
the	O
principal	O
drawback	O
of	O
gps	B
is	O
that	O
they	O
take	O
on	O
time	O
to	O
use	O
this	O
is	O
because	O
of	O
the	O
need	O
to	O
invert	O
compute	O
the	O
cholesky	B
decomposition	I
of	O
the	O
n	O
n	O
kernel	B
matrix	O
k	O
a	O
variety	O
of	O
approximation	O
methods	O
have	O
been	O
devised	O
which	O
take	O
om	O
time	O
where	O
m	O
is	O
a	O
user-specifiable	O
parameter	B
for	O
details	O
see	O
et	O
al	O
exercises	O
exercise	O
reproducing	B
property	I
prove	O
equation	O
adaptive	O
basis	O
function	O
models	O
introduction	O
in	O
chapters	O
and	O
we	O
discussed	O
kernel	B
methods	O
which	O
provide	O
a	O
powerful	O
way	O
to	O
create	O
nonlinear	O
models	O
for	O
regression	B
and	O
classification	O
the	O
prediction	O
takes	O
the	O
form	O
f	O
t	O
where	O
we	O
define	O
n	O
and	O
where	O
k	O
are	O
either	O
all	O
the	O
training	O
data	O
or	O
some	O
subset	O
models	O
of	O
this	O
form	O
essentially	O
perform	O
a	O
form	O
of	O
template	B
matching	I
whereby	O
they	O
compare	O
the	O
input	O
x	O
to	O
the	O
stored	O
prototypes	O
k	O
although	O
this	O
can	O
work	O
well	O
it	O
relies	O
on	O
having	O
a	O
good	O
kernel	B
function	I
to	O
measure	O
the	O
similarity	O
between	O
data	O
vectors	O
often	O
coming	O
up	O
with	O
a	O
good	O
kernel	B
function	I
is	O
quite	O
difficult	O
for	O
example	O
how	O
do	O
we	O
define	O
the	O
similarity	O
between	O
two	O
images	O
pixel-wise	O
comparison	O
of	O
intensities	O
is	O
what	O
a	O
gaussian	B
kernel	B
corresponds	O
to	O
does	O
not	O
work	O
well	O
although	O
it	O
is	O
possible	O
indeed	O
common	O
to	O
hand-engineer	O
kernels	O
for	O
specific	O
tasks	O
e	O
g	O
the	O
pyramid	B
match	I
kernel	B
in	O
section	O
it	O
would	O
be	O
more	O
interesting	O
if	O
we	O
could	O
learn	O
the	O
kernel	B
in	O
section	O
we	O
discussed	O
a	O
way	O
to	O
learn	O
the	O
parameters	O
of	O
a	O
kernel	B
function	I
by	O
maxi	O
mizing	O
the	O
marginal	B
likelihood	B
for	O
example	O
if	O
we	O
use	O
the	O
ard	B
kernel	B
exp	O
jxj	O
we	O
can	O
can	O
estimate	O
the	O
j	O
and	O
thus	O
perform	O
a	O
form	O
of	O
nonlinear	O
feature	B
selection	I
however	O
such	O
methods	O
can	O
be	O
computationally	O
expensive	O
another	O
approach	O
known	O
as	O
multiple	O
kernel	B
et	O
al	O
uses	O
a	O
convex	B
combination	I
of	O
base	O
kernels	O
learning	B
e	O
g	O
and	O
then	O
estimates	O
the	O
mixing	B
weights	I
wj	O
but	O
this	O
relies	O
on	O
having	O
good	O
base	O
kernels	O
is	O
also	O
computationally	O
expensive	O
j	O
wj	O
jx	O
an	O
alternative	O
approach	O
is	O
to	O
dispense	O
with	O
kernels	O
altogether	O
and	O
try	O
to	O
learn	O
useful	O
features	B
directly	O
from	O
the	O
input	O
data	O
that	O
is	O
we	O
will	O
create	O
what	O
we	O
call	O
an	O
adaptive	O
basisfunction	O
model	O
which	O
is	O
a	O
model	O
of	O
the	O
form	O
f	O
wm	O
mx	O
chapter	O
adaptive	O
basis	O
function	O
models	O
where	O
mx	O
is	O
the	O
m	O
th	O
basis	O
function	O
which	O
is	O
learned	O
from	O
data	O
this	O
framework	O
covers	O
all	O
of	O
the	O
models	O
we	O
will	O
discuss	O
in	O
this	O
chapter	O
typically	O
the	O
basis	B
functions	I
are	O
parametric	O
so	O
we	O
can	O
write	O
mx	O
vm	O
where	O
vm	O
are	O
the	O
parameters	O
of	O
the	O
basis	O
function	O
itself	O
we	O
will	O
use	O
to	O
denote	O
the	O
entire	O
parameter	B
set	O
the	O
resulting	O
model	O
is	O
not	O
linear-in-the-parameters	O
anymore	O
so	O
we	O
will	O
only	O
be	O
able	O
to	O
compute	O
a	O
locally	O
optimal	O
mle	B
or	O
map	B
estimate	I
of	O
nevertheless	O
such	O
models	O
often	O
significantly	O
outperform	O
linear	O
models	O
as	O
we	O
will	O
see	O
classification	O
and	O
regression	B
trees	O
classification	O
and	O
regression	B
trees	O
or	O
cart	B
models	O
also	O
called	O
decision	B
trees	I
to	O
be	O
confused	O
with	O
the	O
decision	B
trees	I
used	O
in	O
decision	B
theory	O
are	O
defined	O
by	O
recursively	O
partitioning	B
the	O
input	O
space	O
and	O
defining	O
a	O
local	O
model	O
in	O
each	O
resulting	O
region	O
of	O
input	O
space	O
this	O
can	O
be	O
represented	O
by	O
a	O
tree	B
with	O
one	O
leaf	B
per	O
region	O
as	O
we	O
explain	O
below	O
basics	O
to	O
explain	O
the	O
cart	B
approach	O
consider	O
the	O
tree	B
in	O
figure	O
the	O
first	O
node	O
asks	O
if	O
is	O
less	O
than	O
some	O
threshold	O
if	O
yes	O
we	O
then	O
ask	O
if	O
is	O
less	O
than	O
some	O
other	O
threshold	O
if	O
yes	O
we	O
are	O
in	O
the	O
bottom	O
left	O
quadrant	O
of	O
space	O
if	O
no	O
we	O
ask	O
if	O
is	O
less	O
than	O
and	O
so	O
on	O
the	O
result	O
of	O
these	O
axis	B
parallel	I
splits	I
is	O
to	O
partition	O
space	O
into	O
regions	O
as	O
shown	O
in	O
figure	O
we	O
can	O
now	O
associate	O
a	O
mean	B
response	O
with	O
each	O
of	O
these	O
regions	O
resulting	O
in	O
the	O
piecewise	O
constant	O
surface	O
shown	O
in	O
figure	O
we	O
can	O
write	O
the	O
model	O
in	O
the	O
following	O
form	O
f	O
e	O
wmix	O
rm	O
wm	O
vm	O
where	O
rm	O
is	O
the	O
m	O
th	O
region	O
wm	O
is	O
the	O
mean	B
response	O
in	O
this	O
region	O
and	O
vm	O
encodes	O
the	O
choice	O
of	O
variable	O
to	O
split	O
on	O
and	O
the	O
threshold	O
value	O
on	O
the	O
path	B
from	O
the	O
root	B
to	O
the	O
m	O
th	O
leaf	B
this	O
makes	O
it	O
clear	O
that	O
a	O
cart	B
model	O
is	O
just	O
a	O
an	O
adaptive	B
basis-function	I
model	I
where	O
the	O
basis	B
functions	I
define	O
the	O
regions	O
and	O
the	O
weights	O
specify	O
the	O
response	O
value	O
in	O
each	O
region	O
we	O
discuss	O
how	O
to	O
find	O
these	O
basis	B
functions	I
below	O
we	O
can	O
generalize	B
this	O
to	O
the	O
classification	O
setting	O
by	O
storing	O
the	O
distribution	O
over	O
class	O
labels	O
in	O
each	O
leaf	B
instead	O
of	O
the	O
mean	B
response	O
this	O
is	O
illustrated	O
in	O
figure	O
this	O
model	O
can	O
be	O
used	O
to	O
classify	O
the	O
data	O
in	O
figure	O
for	O
example	O
we	O
first	O
check	O
the	O
color	O
of	O
the	O
object	O
if	O
it	O
is	O
blue	O
we	O
follow	O
the	O
left	O
branch	O
and	O
end	O
up	O
in	O
a	O
leaf	B
labeled	O
which	O
means	O
we	O
have	O
positive	B
examples	I
and	O
negative	B
examples	I
which	O
match	O
this	O
criterion	O
hence	O
we	O
predict	O
py	O
if	O
x	O
is	O
blue	O
if	O
it	O
is	O
an	O
ellipse	O
we	O
end	O
up	O
in	O
a	O
leaf	B
labeled	O
so	O
we	O
predict	O
py	O
if	O
it	O
is	O
red	O
but	O
not	O
an	O
ellipse	O
we	O
predict	O
py	O
if	O
it	O
is	O
some	O
other	O
colour	O
we	O
check	O
the	O
size	O
if	O
less	O
than	O
we	O
predict	O
py	O
otherwise	O
py	O
these	O
probabilities	O
are	O
just	O
the	O
empirical	O
fraction	O
of	O
positive	B
examples	I
that	O
satisfy	O
each	O
conjunction	O
of	O
feature	O
values	O
which	O
defines	O
a	O
path	B
from	O
the	O
root	B
to	O
a	O
leaf	B
if	O
it	O
is	O
red	O
we	O
then	O
check	O
the	O
shape	O
classification	O
and	O
regression	B
trees	O
figure	O
a	O
simple	O
regression	B
tree	B
on	O
two	O
inputs	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
regtreesurfacedemo	O
blue	O
color	O
red	O
other	O
shape	O
ellipse	O
other	O
size	O
yes	O
no	O
figure	O
a	O
simple	O
decision	B
tree	B
for	O
the	O
data	O
in	O
figure	O
a	O
leaf	B
labeled	O
as	O
means	O
that	O
there	O
are	O
positive	B
examples	I
that	O
match	O
this	O
path	B
and	O
negative	B
examples	I
in	O
this	O
tree	B
most	O
of	O
the	O
leaves	B
are	O
pure	B
meaning	O
they	O
only	O
have	O
examples	O
of	O
one	O
class	O
or	O
the	O
other	O
the	O
only	O
exception	O
is	O
leaf	B
representing	O
red	O
ellipses	O
which	O
has	O
a	O
label	B
distribution	O
of	O
we	O
could	O
distinguish	O
positive	O
from	O
negative	O
red	O
ellipses	O
by	O
adding	O
a	O
further	O
test	O
based	O
on	O
size	O
however	O
it	O
is	O
not	O
always	O
desirable	O
to	O
construct	O
trees	O
that	O
perfectly	O
model	O
the	O
training	O
data	O
due	O
to	O
overfitting	O
growing	O
a	O
tree	B
finding	O
the	O
optimal	O
partitioning	B
of	O
the	O
data	O
is	O
np-complete	B
and	O
rivest	O
so	O
it	O
is	O
common	O
to	O
use	O
the	O
greedy	O
procedure	O
shown	O
in	O
algorithm	O
to	O
compute	O
a	O
locally	O
optimal	O
mle	B
this	O
method	O
is	O
used	O
by	O
cart	B
et	O
al	O
and	O
dtfit	O
for	O
a	O
simple	O
matlab	O
which	O
are	O
three	O
popular	O
implementations	O
of	O
the	O
method	O
implementation	O
the	O
split	O
function	O
chooses	O
the	O
best	O
feature	O
and	O
the	O
best	O
value	O
for	O
that	O
feature	O
as	O
follows	O
t	O
costxi	O
yi	O
xij	O
t	O
yi	O
xij	O
t	O
arg	O
min	O
j	O
min	O
t	O
tj	O
chapter	O
adaptive	O
basis	O
function	O
models	O
algorithm	O
recursive	B
procedure	O
to	O
grow	O
a	O
classification	O
regression	B
tree	B
function	O
fittreenode	O
d	O
depth	O
node	O
prediction	O
meanyi	O
i	O
d	O
or	O
class	O
label	B
distribution	O
t	O
splitd	O
if	O
not	O
worthsplittingdepth	O
cost	O
dl	O
dr	O
then	O
return	O
node	O
else	O
node	O
test	O
x	O
xj	O
t	O
node	O
left	O
fittreenode	O
dl	O
node	O
right	O
fittreenode	O
dr	O
return	O
node	O
anonymous	O
function	O
where	O
the	O
cost	O
function	O
for	O
a	O
given	O
dataset	O
will	O
be	O
defined	O
below	O
for	O
notational	O
simplicity	O
we	O
have	O
assumed	O
all	O
inputs	O
are	O
real-valued	O
or	O
ordinal	B
so	O
it	O
makes	O
sense	O
to	O
compare	O
a	O
feature	O
xij	O
to	O
a	O
numeric	O
value	O
t	O
the	O
set	O
of	O
possible	O
thresholds	O
tj	O
for	O
feature	O
j	O
can	O
be	O
obtained	O
by	O
sorting	O
the	O
unique	O
values	O
of	O
xij	O
for	O
example	O
if	O
feature	O
has	O
the	O
values	O
then	O
we	O
set	O
in	O
the	O
case	O
of	O
categorical	B
inputs	O
the	O
most	O
common	O
approach	O
is	O
to	O
consider	O
splits	O
of	O
the	O
form	O
xij	O
ck	O
and	O
xij	O
ck	O
for	O
each	O
possible	O
class	O
label	B
ck	O
although	O
we	O
could	O
allow	O
for	O
multi-way	O
splits	O
in	O
non-binary	O
trees	O
this	O
would	O
result	O
in	O
data	B
fragmentation	I
meaning	O
too	O
little	O
data	O
might	O
fall	O
into	O
each	O
subtree	O
resulting	O
in	O
overfitting	O
the	O
function	O
that	O
checks	O
if	O
a	O
node	O
is	O
worth	O
splitting	O
can	O
use	O
several	O
stopping	O
heuristics	B
such	O
as	O
the	O
following	O
is	O
the	O
reduction	O
in	O
cost	O
too	O
small	O
typically	O
we	O
define	O
the	O
gain	O
of	O
using	O
a	O
feature	O
to	O
be	O
a	O
normalized	O
measure	O
of	O
the	O
reduction	O
in	O
cost	O
costdr	O
costdl	O
costd	O
has	O
the	O
tree	B
exceeded	O
the	O
maximum	O
desired	O
depth	O
is	O
the	O
distribution	O
of	O
the	O
response	O
in	O
either	O
dl	O
or	O
dr	O
sufficiently	O
homogeneous	B
all	O
labels	O
are	O
the	O
same	O
so	O
the	O
distribution	O
is	O
pure	B
is	O
the	O
number	O
of	O
examples	O
in	O
either	O
dl	O
or	O
dr	O
too	O
small	O
all	O
that	O
remains	O
is	O
to	O
specify	O
the	O
cost	O
measure	O
used	O
to	O
evaluate	O
the	O
quality	O
of	O
a	O
proposed	O
split	O
this	O
depends	O
on	O
whether	O
our	O
goal	O
is	O
regression	B
or	O
classification	O
we	O
discuss	O
both	O
cases	O
below	O
regression	B
cost	O
i	O
d	O
in	O
the	O
regression	B
setting	O
we	O
define	O
the	O
cost	O
as	O
follows	O
costd	O
classification	O
and	O
regression	B
trees	O
where	O
y	O
i	O
d	O
yi	O
is	O
the	O
mean	B
of	O
the	O
response	B
variable	I
in	O
the	O
specified	O
set	O
of	O
data	O
alternatively	O
we	O
can	O
fit	O
a	O
linear	B
regression	B
model	O
for	O
each	O
leaf	B
using	O
as	O
inputs	O
the	O
features	B
that	O
were	O
chosen	O
on	O
the	O
path	B
from	O
the	O
root	B
and	O
then	O
measure	O
the	O
residual	B
error	I
classification	O
cost	O
i	O
d	O
c	O
in	O
the	O
classification	O
setting	O
there	O
are	O
several	O
ways	O
to	O
measure	O
the	O
quality	O
of	O
a	O
split	O
first	O
we	O
fit	O
a	O
multinoulli	O
model	O
to	O
the	O
data	O
in	O
the	O
leaf	B
satisfying	O
the	O
test	O
xj	O
t	O
by	O
estimating	O
the	O
class-conditional	O
probabilities	O
as	O
follows	O
iyi	O
c	O
where	O
d	O
is	O
the	O
data	O
in	O
the	O
leaf	B
given	O
this	O
there	O
are	O
several	O
common	O
error	O
measures	O
for	O
evaluating	O
a	O
proposed	O
partition	O
misclassification	O
rate	B
we	O
define	O
the	O
most	O
probable	O
class	O
label	B
as	O
yc	O
argmaxc	O
c	O
the	O
corresponding	O
error	O
rate	B
is	O
then	O
iyi	O
y	O
y	O
i	O
d	O
entropy	B
ordeviance	O
h	O
c	O
log	O
c	O
note	O
that	O
minimizing	O
the	O
entropy	B
is	O
equivalent	O
to	O
maximizing	O
the	O
information	B
gain	I
between	O
test	O
xj	O
t	O
and	O
the	O
class	O
label	B
y	O
defined	O
by	O
infogainxj	O
t	O
y	O
h	O
h	O
t	O
c	O
c	O
py	O
c	O
log	O
py	O
c	O
py	O
cxj	O
t	O
log	O
pcxj	O
t	O
since	O
c	O
is	O
an	O
mle	B
for	O
the	O
distribution	O
pcxj	O
if	O
xj	O
is	O
categorical	B
and	O
we	O
use	O
tests	O
of	O
the	O
form	O
xj	O
k	O
then	O
taking	O
expectations	O
over	O
values	O
of	O
xj	O
gives	O
k	O
pxj	O
kinfogainxj	O
k	O
y	O
the	O
mutual	O
h	O
h	O
i	O
xj	O
information	B
between	O
xj	O
and	O
y	O
e	O
y	O
chapter	O
adaptive	O
basis	O
function	O
models	O
error	O
rate	B
gini	O
entropy	B
figure	O
node	O
impurity	O
measures	O
for	O
binary	O
classification	O
the	O
horizontal	O
axis	O
corresponds	O
to	O
p	O
the	O
probability	O
of	O
class	O
the	O
entropy	B
measure	O
has	O
been	O
rescaled	O
to	O
pass	O
through	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
ginidemo	O
gini	B
index	I
c	O
c	O
c	O
c	O
c	O
c	O
c	O
this	O
is	O
the	O
expected	O
error	O
rate	B
to	O
see	O
this	O
note	O
that	O
c	O
is	O
the	O
probability	O
a	O
random	O
entry	O
in	O
the	O
leaf	B
belongs	O
to	O
class	O
c	O
and	O
c	O
is	O
the	O
probability	O
it	O
would	O
be	O
misclassified	O
in	O
the	O
two-class	O
case	O
where	O
p	O
the	O
misclassification	O
rate	B
is	O
maxp	O
p	O
the	O
entropy	B
is	O
and	O
the	O
gini	B
index	I
is	O
p	O
these	O
are	O
plotted	O
in	O
figure	O
we	O
see	O
that	O
the	O
cross-entropy	B
and	O
gini	O
measures	O
are	O
very	O
similar	B
and	O
are	O
more	O
sensitive	O
to	O
changes	O
in	O
class	O
probability	O
than	O
is	O
the	O
misclassification	O
rate	B
for	O
example	O
consider	O
a	O
two-class	O
problem	O
with	O
cases	O
in	O
each	O
class	O
suppose	O
one	O
split	O
created	O
the	O
nodes	B
and	O
while	O
the	O
other	O
created	O
the	O
nodes	B
and	O
both	O
splits	O
produce	O
a	O
misclassification	O
rate	B
of	O
however	O
the	O
latter	O
seems	O
preferable	O
since	O
one	O
of	O
the	O
nodes	B
is	O
pure	B
i	O
e	O
it	O
only	O
contains	O
one	O
class	O
the	O
cross-entropy	B
and	O
gini	O
measures	O
will	O
favor	O
this	O
latter	O
choice	O
example	O
as	O
an	O
example	O
consider	O
two	O
of	O
the	O
four	O
features	B
from	O
the	O
iris	B
dataset	O
shown	O
in	O
figure	O
the	O
resulting	O
tree	B
is	O
shown	O
in	O
figure	O
and	O
the	O
decision	B
boundaries	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
tree	B
is	O
quite	O
complex	O
as	O
are	O
the	O
resulting	O
decision	B
boundaries	O
in	O
figure	O
we	O
show	O
that	O
the	O
cv	B
estimate	O
of	O
the	O
error	O
is	O
much	O
higher	O
than	O
the	O
training	B
set	I
error	O
indicating	O
overfitting	O
below	O
we	O
discuss	O
how	O
to	O
perform	O
a	O
tree-pruning	O
stage	O
to	O
simplify	O
the	O
tree	B
t	O
h	O
d	O
w	O
i	O
l	O
a	O
p	O
e	O
s	O
setosa	O
versicolor	O
virginica	O
y	O
classification	O
and	O
regression	B
trees	O
unpruned	O
decision	B
tree	B
versicolor	O
setosa	O
virginica	O
sepal	O
length	O
x	O
figure	O
petal	O
length	O
and	O
petal	O
width	O
decision	B
boundaries	O
induced	O
by	O
the	O
decision	B
tree	B
in	O
figure	O
iris	B
data	O
we	O
only	O
show	O
the	O
first	O
two	O
features	B
sepal	O
length	O
and	O
sepal	O
width	O
and	O
ignore	O
sl	O
sl	O
w	O
sw	O
sl	O
sl	O
versicolorsetosa	O
sw	O
sw	O
sl	O
sl	O
sl	O
sl	O
setosa	O
sw	O
sw	O
virginica	O
sw	O
sw	O
versicolor	O
versicolor	O
sw	O
sw	O
versicolor	O
sl	O
sl	O
sw	O
sw	O
versicolor	O
versicolorvirginica	O
sl	O
sl	O
virginica	O
sw	O
sw	O
sl	O
sl	O
sl	O
sl	O
virginicaversicolor	O
sw	O
sw	O
sw	O
sw	O
versicolor	O
sw	O
virginica	O
sw	O
virginicaversicolor	O
versicolorvirginica	O
cross	B
validation	I
training	B
set	I
min	O
std	O
err	O
best	O
choice	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
s	O
m	O
i	O
l	O
t	O
s	O
o	O
c	O
number	O
of	O
terminal	O
nodes	B
figure	O
tree	B
figure	O
generated	O
by	O
dtreedemoiris	O
unpruned	O
decision	B
tree	B
for	O
iris	B
data	O
plot	O
of	O
misclassification	O
error	O
rate	B
vs	O
depth	O
of	O
pruning	B
a	O
tree	B
to	O
prevent	O
overfitting	O
we	O
can	O
stop	O
growing	O
the	O
tree	B
if	O
the	O
decrease	O
in	O
the	O
error	O
is	O
not	O
sufficient	O
to	O
justify	O
the	O
extra	O
complexity	O
of	O
adding	O
an	O
extra	O
subtree	O
however	O
this	O
tends	O
to	O
be	O
too	O
myopic	O
for	O
example	O
on	O
the	O
xor	B
data	O
in	O
figure	O
it	O
would	O
might	O
never	O
make	O
any	O
splits	O
since	O
each	O
feature	O
on	O
its	O
own	O
has	O
little	O
predictive	B
power	O
the	O
standard	O
approach	O
is	O
therefore	O
to	O
grow	O
a	O
full	B
tree	B
and	O
then	O
to	O
perform	O
pruning	B
this	O
can	O
be	O
done	O
using	O
a	O
scheme	O
that	O
prunes	O
the	O
branches	O
giving	O
the	O
least	O
increase	O
in	O
the	O
error	O
see	O
et	O
al	O
for	O
details	O
to	O
determine	O
how	O
far	O
to	O
prune	O
back	O
we	O
can	O
evaluate	O
the	O
cross-validated	O
error	O
on	O
each	O
such	O
subtree	O
and	O
then	O
pick	O
the	O
tree	B
whose	O
cv	B
error	O
is	O
within	O
standard	B
error	I
of	O
the	O
minimum	O
this	O
is	O
illustrated	O
in	O
figure	O
the	O
point	O
with	O
the	O
minimum	O
cv	B
error	O
corresponds	O
to	O
the	O
simple	O
tree	B
in	O
figure	O
chapter	O
adaptive	O
basis	O
function	O
models	O
sl	O
sl	O
sw	O
sw	O
sl	O
sl	O
versicolor	O
setosa	O
sw	O
sw	O
virginica	O
y	O
pruned	O
decision	B
tree	B
versicolor	O
setosa	O
virginica	O
versicolor	O
setosa	O
x	O
figure	O
pruned	O
decision	B
tree	B
for	O
iris	B
data	O
figure	O
generated	O
by	O
dtreedemoiris	O
pros	O
and	O
cons	O
of	O
trees	O
cart	B
models	O
are	O
popular	O
for	O
several	O
reasons	O
they	O
are	O
easy	O
to	O
they	O
can	O
easily	O
handle	O
mixed	O
discrete	B
and	O
continuous	O
inputs	O
they	O
are	O
insensitive	O
to	O
monotone	O
transformations	O
of	O
the	O
inputs	O
the	O
split	O
points	O
are	O
based	O
on	O
ranking	B
the	O
data	O
points	O
they	O
perform	O
automatic	O
variable	O
selection	O
they	O
are	O
relatively	O
robust	B
to	O
outliers	B
they	O
scale	O
well	O
to	O
large	O
data	O
sets	O
and	O
they	O
can	O
be	O
modified	O
to	O
handle	O
missing	B
however	O
cart	B
models	O
also	O
have	O
some	O
disadvantages	O
the	O
primary	O
one	O
is	O
that	O
they	O
do	O
not	O
predict	O
very	O
accurately	O
compared	O
to	O
other	O
kinds	O
of	O
model	O
this	O
is	O
in	O
part	O
due	O
to	O
the	O
greedy	O
nature	O
of	O
the	O
tree	B
construction	O
algorithm	O
a	O
related	O
problem	O
is	O
that	O
trees	O
are	O
unstable	B
small	O
changes	O
to	O
the	O
input	O
data	O
can	O
have	O
large	O
effects	O
on	O
the	O
structure	O
of	O
the	O
tree	B
due	O
to	O
the	O
hierarchical	O
nature	O
of	O
the	O
tree-growing	O
process	O
causing	O
errors	O
at	O
the	O
top	O
to	O
affect	O
the	O
rest	O
of	O
the	O
tree	B
in	O
frequentist	B
terminology	O
we	O
say	O
that	O
trees	O
are	O
high	B
variance	B
estimators	I
we	O
discuss	O
a	O
solution	O
to	O
this	O
below	O
random	B
forests	I
one	O
way	O
to	O
reduce	O
the	O
variance	B
of	O
an	O
estimate	O
is	O
to	O
average	O
together	O
many	O
estimates	O
for	O
example	O
we	O
can	O
train	O
m	O
different	O
trees	O
on	O
different	O
subsets	O
of	O
the	O
data	O
chosen	O
randomly	O
with	O
we	O
can	O
postprocess	O
the	O
tree	B
to	O
derive	O
a	O
series	O
of	O
logical	O
rules	B
such	O
as	O
if	O
then	O
the	O
standard	O
heuristic	O
for	O
handling	O
missing	B
inputs	O
in	O
decision	B
trees	I
is	O
to	O
look	O
for	O
a	O
series	O
of	O
backup	O
variables	O
which	O
can	O
induce	O
a	O
similar	B
partition	O
to	O
the	O
chosen	O
variable	O
at	O
any	O
given	O
split	O
these	O
can	O
be	O
used	O
in	O
case	O
the	O
chosen	O
variable	O
is	O
unobserved	O
at	O
test	O
time	O
these	O
are	O
called	O
surrogate	B
splits	I
this	O
method	O
finds	O
highly	O
correlated	O
features	B
and	O
can	O
be	O
thought	O
of	O
as	O
learning	B
a	O
local	O
joint	O
model	O
of	O
the	O
input	O
this	O
has	O
the	O
advantage	O
over	O
a	O
generative	O
model	O
of	O
not	O
modeling	O
the	O
entire	O
joint	B
distribution	I
of	O
inputs	O
but	O
it	O
has	O
the	O
disadvantage	O
of	O
being	O
entirely	O
ad	O
hoc	O
a	O
simpler	O
approach	O
applicable	O
to	O
categorical	B
variables	I
is	O
to	O
code	O
missing	B
as	O
a	O
new	O
value	O
and	O
then	O
to	O
treat	O
the	O
data	O
as	O
fully	O
observed	O
classification	O
and	O
regression	B
trees	O
replacement	O
and	O
then	O
compute	O
the	O
ensemble	B
f	O
m	O
fmx	O
where	O
fm	O
is	O
the	O
m	O
th	O
tree	B
this	O
technique	O
is	O
called	O
bagging	B
which	O
stands	O
for	O
bootstrap	B
aggregating	O
unfortunately	O
simply	O
re-running	O
the	O
same	O
learning	B
algorithm	O
on	O
different	O
subsets	O
of	O
the	O
data	O
can	O
result	O
in	O
highly	O
correlated	O
predictors	O
which	O
limits	O
the	O
amount	O
of	O
variance	B
reduction	O
that	O
is	O
possible	O
the	O
technique	O
known	O
as	O
random	B
forests	I
tries	O
to	O
decorrelate	O
the	O
base	O
learners	O
by	O
learning	B
trees	O
based	O
on	O
a	O
randomly	O
chosen	O
subset	O
of	O
input	O
variables	O
as	O
well	O
as	O
a	O
randomly	O
chosen	O
subset	O
of	O
data	O
cases	O
such	O
models	O
often	O
have	O
very	O
good	O
predictive	B
accuracy	O
and	O
niculescu-mizil	O
and	O
have	O
been	O
widely	O
used	O
in	O
many	O
applications	O
for	O
body	O
pose	O
recognition	O
using	O
microsoft	B
s	O
popular	O
kinect	B
sensor	O
et	O
al	O
bagging	B
is	O
a	O
frequentist	B
concept	B
it	O
is	O
also	O
possible	O
to	O
adopt	O
a	O
bayesian	B
approach	O
to	O
learning	B
trees	O
in	O
particular	O
et	O
al	O
denison	O
et	O
al	O
wu	O
et	O
al	O
perform	O
approximate	B
inference	B
over	O
the	O
space	O
of	O
trees	O
and	O
parameters	O
using	O
mcmc	B
this	O
reduces	O
the	O
variance	B
of	O
the	O
predictions	O
we	O
can	O
also	O
perform	O
bayesian	B
inference	B
over	O
the	O
space	O
of	O
ensembles	O
of	O
trees	O
which	O
tends	O
to	O
work	O
much	O
better	O
this	O
is	O
known	O
as	O
bayesian	B
adaptive	I
regression	B
trees	I
or	O
bart	B
et	O
al	O
note	O
that	O
the	O
cost	O
of	O
these	O
sampling-based	O
bayesian	B
methods	O
is	O
comparable	O
to	O
the	O
sampling-based	O
random	O
forest	B
method	O
that	O
is	O
both	O
approaches	O
are	O
farily	O
slow	O
to	O
train	O
but	O
produce	O
high	O
quality	O
classifiers	O
unfortunately	O
methods	O
that	O
use	O
multiple	O
trees	O
derived	O
from	O
a	O
bayesian	B
or	O
frequentist	B
standpoint	O
lose	O
their	O
nice	O
interpretability	O
properties	O
fortunately	O
various	O
post-processing	O
measures	O
can	O
be	O
applied	O
as	O
discussed	O
in	O
section	O
cart	B
compared	O
to	O
hierarchical	B
mixture	B
of	I
experts	I
an	O
interesting	O
alternative	O
to	O
a	O
decision	B
tree	B
is	O
known	O
as	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
figure	O
gives	O
an	O
illustration	O
where	O
we	O
have	O
two	O
levels	O
of	O
experts	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
probabilistic	B
decision	B
tree	B
of	O
depth	O
since	O
we	O
recursively	O
partition	O
the	O
space	O
and	O
apply	O
a	O
different	O
expert	O
to	O
each	O
partition	O
hastie	O
et	O
al	O
et	O
al	O
write	O
that	O
the	O
hme	O
approach	O
is	O
a	O
promising	O
competitor	O
to	O
cart	B
trees	O
some	O
of	O
the	O
advantages	O
include	O
the	O
following	O
the	O
model	O
can	O
partition	O
the	O
input	O
space	O
using	O
any	O
set	O
of	O
nested	O
linear	O
decision	B
boundaries	O
by	O
contrast	O
standard	O
decision	B
trees	I
are	O
constrained	O
to	O
use	O
axis-parallel	O
splits	O
the	O
model	O
makes	O
predictions	O
by	O
averaging	O
over	O
all	O
experts	O
by	O
contrast	O
in	O
a	O
standard	O
decision	B
tree	B
predictions	O
are	O
made	O
only	O
based	O
on	O
the	O
model	O
in	O
the	O
corresponding	O
leaf	B
since	O
leaves	B
often	O
contain	O
few	O
training	O
examples	O
this	O
can	O
result	O
in	O
overfitting	O
fitting	O
an	O
hme	O
involves	O
solving	O
a	O
smooth	O
continuous	O
optimization	B
problem	O
using	O
em	B
which	O
is	O
likely	O
to	O
be	O
less	O
prone	O
to	O
local	O
optima	O
than	O
the	O
standard	O
greedy	O
discrete	B
optimization	B
methods	O
used	O
to	O
fit	O
decision	B
trees	I
for	O
similar	B
reasons	O
it	O
is	O
computationally	O
easier	O
to	O
be	O
bayesian	B
about	O
the	O
parameters	O
of	O
an	O
hme	O
e	O
g	O
et	O
al	O
bishop	O
chapter	O
adaptive	O
basis	O
function	O
models	O
and	O
svens	O
n	O
than	O
about	O
the	O
structure	O
and	O
parameters	O
of	O
a	O
decision	B
tree	B
e	O
g	O
et	O
al	O
generalized	O
additive	O
models	O
a	O
simple	O
way	O
to	O
create	O
a	O
nonlinear	O
model	O
with	O
multiple	O
inputs	O
is	O
to	O
use	O
a	O
generalized	B
additive	I
model	I
and	O
tibshirani	O
which	O
is	O
a	O
model	O
of	O
the	O
form	O
f	O
fdxd	O
here	O
each	O
fj	O
can	O
be	O
modeled	O
by	O
some	O
scatterplot	O
smoother	O
and	O
f	O
can	O
be	O
mapped	O
to	O
pyx	O
using	O
a	O
link	B
function	I
as	O
in	O
a	O
glm	B
the	O
term	O
generalized	B
additive	I
model	I
if	O
we	O
use	O
regression	B
splines	O
some	O
other	O
fixed	O
basis	B
function	I
expansion	I
approach	O
for	O
the	O
fj	O
then	O
each	O
fjxj	O
can	O
be	O
written	O
as	O
t	O
j	O
jxj	O
so	O
the	O
whole	O
model	O
can	O
be	O
written	O
as	O
f	O
t	O
where	O
dxd	O
however	O
it	O
is	O
more	O
common	O
to	O
use	O
smoothing	B
splines	I
for	O
the	O
fj	O
in	O
this	O
case	O
the	O
objective	O
the	O
regression	B
setting	O
becomes	O
yi	O
j	O
j	O
j	O
fd	O
fjxij	O
where	O
j	O
is	O
the	O
strength	O
of	O
the	O
regularizer	O
for	O
fj	O
backfitting	O
we	O
now	O
discuss	O
how	O
to	O
fit	O
the	O
model	O
using	O
mle	B
the	O
constant	O
is	O
not	O
uniquely	O
identifiable	O
since	O
we	O
can	O
always	O
add	O
or	O
subtract	O
constants	O
to	O
any	O
of	O
the	O
fj	O
functions	O
the	O
convention	O
is	O
to	O
assume	O
fjxij	O
for	O
all	O
j	O
in	O
this	O
case	O
the	O
mle	B
for	O
is	O
just	O
n	O
to	O
fit	O
the	O
rest	O
of	O
the	O
model	O
we	O
can	O
center	O
the	O
responses	O
subtracting	O
and	O
then	O
iteratively	O
update	O
each	O
fj	O
in	O
turn	O
using	O
as	O
a	O
target	O
vector	O
the	O
residuals	O
obtained	O
by	O
omitting	O
term	O
fj	O
yi	O
fkxikn	O
fj	O
smootheryi	O
fj	O
fj	O
n	O
fjxij	O
we	O
should	O
then	O
ensure	O
the	O
output	O
is	O
zero	O
mean	B
using	O
this	O
is	O
called	O
the	O
backfitting	O
algorithm	O
and	O
tibshirani	O
if	O
x	O
has	O
full	B
column	O
rank	O
then	O
the	O
above	O
objective	O
is	O
convex	B
each	O
smoothing	B
spline	B
is	O
a	O
linear	O
operator	O
as	O
shown	O
in	O
section	O
so	O
this	O
procedure	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
in	O
the	O
glm	B
case	O
we	O
need	O
to	O
modify	O
the	O
method	O
somewhat	O
the	O
basic	O
idea	O
is	O
to	O
replace	O
the	O
weighted	B
least	B
squares	I
step	O
of	O
irls	B
section	O
with	O
a	O
weighted	O
backfitting	O
algorithm	O
in	O
the	O
logistic	B
regression	B
case	O
each	O
response	O
has	O
weight	O
si	O
i	O
associated	O
with	O
it	O
where	O
i	O
sigm	O
fjxij	O
generalized	O
additive	O
models	O
computational	O
efficiency	O
each	O
call	O
to	O
the	O
smoother	O
takes	O
on	O
time	O
so	O
the	O
total	O
cost	O
is	O
on	O
dt	O
where	O
t	O
is	O
the	O
number	O
of	O
iterations	O
if	O
we	O
have	O
high-dimensional	O
inputs	O
fitting	O
a	O
gam	O
is	O
expensive	O
one	O
approach	O
is	O
to	O
combine	O
it	O
with	O
a	O
sparsity	B
penalty	O
see	O
e	O
g	O
the	O
spam	B
additive	O
model	O
approach	O
of	O
et	O
al	O
alternatively	O
we	O
can	O
use	O
a	O
greedy	O
approach	O
such	O
as	O
boosting	B
section	O
multivariate	B
adaptive	I
regression	B
splines	I
in	O
general	O
we	O
can	O
create	O
an	O
anova	B
we	O
can	O
extend	O
gams	O
by	O
allowing	O
for	O
interaction	B
effects	I
decomposition	O
f	O
fjxj	O
fjkxj	O
xk	O
fjklxj	O
xk	O
xl	O
jk	O
jkl	O
of	O
course	O
we	O
cannot	O
allow	O
for	O
too	O
many	O
higher-order	O
interactions	O
because	O
there	O
will	O
be	O
too	O
many	O
parameters	O
to	O
fit	O
it	O
is	O
common	O
to	O
use	O
greedy	O
search	O
to	O
decide	O
which	O
variables	O
to	O
add	O
the	O
multivariate	B
adaptive	I
regression	B
splines	I
or	O
mars	B
algorithm	O
is	O
one	O
example	O
of	O
this	O
et	O
al	O
it	O
fits	O
models	O
of	O
the	O
form	O
in	O
equation	O
where	O
it	O
uses	O
a	O
tensor	B
product	I
basis	I
of	O
regression	B
splines	O
to	O
represent	O
the	O
multidimensional	O
regression	B
functions	O
for	O
example	O
for	O
input	O
we	O
might	O
use	O
f	O
m	O
m	O
m	O
to	O
create	O
such	O
a	O
function	O
we	O
start	O
with	O
a	O
set	O
of	O
candidate	O
basis	B
functions	I
of	O
the	O
form	O
c	O
t	O
xj	O
t	O
xn	O
j	O
j	O
d	O
these	O
are	O
linear	O
splines	O
where	O
the	O
knots	B
are	O
at	O
all	O
the	O
observed	O
values	O
for	O
that	O
variable	O
we	O
consider	O
splines	O
sloping	O
up	O
in	O
both	O
directions	O
this	O
is	O
called	O
a	O
reflecting	O
pair	O
see	O
figure	O
let	O
m	O
represent	O
the	O
current	O
set	O
of	O
basis	B
functions	I
we	O
initialize	O
by	O
using	O
m	O
we	O
consider	O
creating	O
a	O
new	O
basis	O
function	O
pair	O
by	O
multplying	O
an	O
hm	O
m	O
with	O
one	O
of	O
the	O
reflecting	O
pairs	O
in	O
c	O
for	O
example	O
we	O
might	O
initially	O
get	O
f	O
obtained	O
by	O
multiplying	O
with	O
a	O
reflecting	O
pair	O
involving	O
with	O
knot	O
t	O
this	O
pair	O
is	O
added	O
to	O
m	O
see	O
figure	O
at	O
the	O
next	O
step	O
we	O
might	O
create	O
a	O
model	O
such	O
as	O
f	O
obtained	O
by	O
multiplying	O
from	O
m	O
by	O
the	O
new	O
reflecting	O
pair	O
and	O
this	O
new	O
function	O
is	O
shown	O
in	O
figure	O
chapter	O
adaptive	O
basis	O
function	O
models	O
linear	O
spline	B
function	O
with	O
a	O
knot	O
at	O
solid	O
blue	O
dotted	O
red	O
x	O
a	O
figure	O
mars	B
model	O
in	O
given	O
by	O
equation	O
a	O
simple	O
mars	B
model	O
in	O
given	O
by	O
equation	O
figure	O
generated	O
by	O
marsdemo	O
we	O
proceed	O
in	O
this	O
way	O
until	O
the	O
model	O
becomes	O
very	O
large	O
may	O
impose	O
an	O
upper	O
bound	O
on	O
the	O
order	O
of	O
interactions	O
then	O
we	O
prune	O
backwards	O
at	O
each	O
step	O
eliminating	O
the	O
basis	O
function	O
that	O
causes	O
the	O
smallest	O
increase	O
in	O
the	O
residual	B
error	I
until	O
the	O
cv	B
error	O
stops	O
improving	O
the	O
whole	O
procedure	O
is	O
closely	O
related	O
to	O
cart	B
to	O
see	O
this	O
suppose	O
we	O
replace	O
the	O
piecewise	O
linear	O
basis	B
functions	I
by	O
step	O
functions	O
ixj	O
t	O
and	O
ixj	O
t	O
multiplying	O
by	O
a	O
pair	O
of	O
reflected	O
step	O
functions	O
is	O
equivalent	O
to	O
splitting	O
a	O
node	O
now	O
suppose	O
we	O
impose	O
the	O
constraint	O
that	O
once	O
a	O
variable	O
is	O
involved	O
in	O
a	O
multiplication	O
by	O
a	O
candidate	O
term	O
that	O
variable	O
gets	O
replaced	O
by	O
the	O
interaction	O
so	O
the	O
original	O
variable	O
is	O
no	O
longer	O
available	O
this	O
ensures	O
that	O
a	O
variable	O
can	O
not	O
be	O
split	O
more	O
than	O
once	O
thus	O
guaranteeing	O
that	O
the	O
resulting	O
model	O
can	O
be	O
represented	O
as	O
a	O
tree	B
in	O
this	O
case	O
the	O
mars	B
growing	O
strategy	O
is	O
the	O
same	O
as	O
the	O
cart	B
growing	O
strategy	O
boosting	B
boosting	B
and	O
freund	O
is	O
a	O
greedy	O
algorithm	O
for	O
fitting	O
adaptive	O
basis-function	O
models	O
of	O
the	O
form	O
in	O
equation	O
where	O
the	O
m	O
are	O
generated	O
by	O
an	O
algorithm	O
called	O
a	O
weak	B
learner	I
or	O
a	O
base	B
learner	I
the	O
algorithm	O
works	O
by	O
applying	O
the	O
weak	B
learner	I
sequentially	O
to	O
weighted	O
versions	O
of	O
the	O
data	O
where	O
more	O
weight	O
is	O
given	O
to	O
examples	O
that	O
were	O
misclassified	O
by	O
earlier	O
rounds	O
this	O
weak	B
learner	I
can	O
be	O
any	O
classification	O
or	O
regression	B
algorithm	O
but	O
it	O
is	O
common	O
to	O
use	O
a	O
cart	B
model	O
in	O
the	O
late	O
leo	O
breiman	O
called	O
boosting	B
where	O
the	O
weak	B
learner	I
is	O
a	O
shallow	O
decision	B
tree	B
the	O
best	O
off-the-shelf	O
classifier	O
in	O
the	O
world	O
et	O
al	O
this	O
is	O
supported	O
by	O
an	O
extensive	O
empirical	O
comparison	O
of	O
different	O
classifiers	O
in	O
and	O
niculescu-mizil	O
who	O
showed	O
that	O
boosted	O
decision	B
trees	I
were	O
the	O
best	O
both	O
in	O
terms	O
of	O
misclassification	O
error	O
and	O
in	O
terms	O
of	O
producing	O
well-calibrated	O
probabilities	O
as	O
judged	O
by	O
roc	B
curves	O
second	O
best	O
method	O
was	O
random	B
forests	I
invented	O
by	O
breiman	O
see	O
section	O
by	O
contrast	O
single	O
decision	B
trees	I
performed	O
very	O
poorly	O
boosting	B
was	O
originally	O
derived	O
in	O
the	O
computational	B
learning	B
theory	I
literature	O
freund	O
and	O
schapire	O
where	O
the	O
focus	O
is	O
binary	O
classification	O
in	O
these	O
papers	O
it	O
was	O
proved	O
that	O
one	O
could	O
boost	O
the	O
performance	O
the	O
training	B
set	I
of	O
any	O
weak	B
learner	I
arbitrarily	O
boosting	B
train	O
test	O
figure	O
performance	O
of	O
adaboost	O
using	O
a	O
decision	B
stump	O
as	O
a	O
weak	B
learner	I
on	O
the	O
data	O
in	O
figure	O
training	O
blue	O
and	O
test	O
red	O
error	O
vs	O
number	O
of	O
iterations	O
figure	O
generated	O
by	O
boostingdemo	O
written	O
by	O
richard	O
stapenhurst	O
high	O
provided	O
the	O
weak	B
learner	I
could	O
always	O
perform	O
slightly	O
better	O
than	O
chance	O
for	O
example	O
in	O
figure	O
we	O
plot	O
the	O
training	O
and	O
test	O
error	O
for	O
boosted	O
decision	B
stumps	O
on	O
a	O
dataset	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
training	B
set	I
error	O
rapidly	O
goes	O
to	O
near	O
zero	O
what	O
is	O
more	O
surprising	O
is	O
that	O
the	O
test	O
set	O
error	O
continues	O
to	O
decline	O
even	O
after	O
the	O
training	B
set	I
error	O
has	O
reached	O
zero	O
the	O
test	O
set	O
error	O
will	O
eventually	O
go	O
up	O
thus	O
boosting	B
is	O
very	O
resistant	O
to	O
overfitting	O
decision	B
stumps	O
form	O
the	O
basis	O
of	O
a	O
very	O
successful	O
face	B
detector	I
and	O
jones	O
which	O
was	O
used	O
to	O
generate	O
the	O
results	O
in	O
figure	O
and	O
which	O
is	O
used	O
in	O
many	O
digital	B
cameras	I
in	O
view	O
of	O
its	O
stunning	O
empirical	O
success	O
statisticians	O
started	O
to	O
become	O
interested	O
in	O
this	O
method	O
breiman	O
showed	O
that	O
boosting	B
can	O
be	O
interpreted	O
as	O
a	O
form	O
of	O
gradient	B
descent	I
in	O
function	O
space	O
this	O
view	O
was	O
then	O
extended	O
in	O
et	O
al	O
who	O
showed	O
how	O
boosting	B
could	O
be	O
extended	O
to	O
handle	O
a	O
variety	O
of	O
loss	B
functions	O
including	O
for	O
regression	B
robust	B
regression	B
poisson	B
regression	B
etc	O
in	O
this	O
section	O
we	O
shall	O
present	O
this	O
statistical	O
interpretation	O
of	O
boosting	B
drawing	O
on	O
the	O
reviews	O
in	O
and	O
hothorn	O
and	O
et	O
al	O
which	O
should	O
be	O
consulted	O
for	O
further	O
details	O
forward	B
stagewise	I
additive	I
modeling	I
the	O
goal	O
of	O
boosting	B
is	O
to	O
solve	O
the	O
following	O
optimization	B
problem	O
lyi	O
f	O
min	O
f	O
and	O
ly	O
y	O
is	O
some	O
loss	B
function	I
and	O
f	O
is	O
assumed	O
to	O
be	O
an	O
abm	O
model	O
as	O
in	O
equation	O
common	O
choices	O
for	O
the	O
loss	B
function	I
are	O
listed	O
in	O
table	O
if	O
we	O
use	O
squared	B
error	I
loss	B
the	O
optimal	O
estimate	O
is	O
given	O
by	O
f	O
f	O
e	O
argmin	O
eyx	O
f	O
chapter	O
adaptive	O
basis	O
function	O
models	O
name	O
squared	B
error	I
absolute	O
error	O
exponential	B
loss	B
logloss	O
loss	B
f	O
f	O
exp	O
yif	O
yi	O
exp	O
yif	O
e	O
yifi	O
derivative	O
yi	O
f	O
sgnyi	O
f	O
yi	O
i	O
algorithm	O
f	O
e	O
medianyxi	O
gradient	B
boosting	B
log	O
i	O
i	O
log	O
i	O
i	O
adaboost	O
logitboost	B
some	O
commonly	O
used	O
loss	B
functions	O
their	O
gradients	O
their	O
population	O
minimizers	O
f	O
table	O
and	O
some	O
algorithms	O
to	O
minimize	O
the	O
loss	B
for	O
binary	O
classification	O
problems	O
we	O
assume	O
yi	O
yi	O
and	O
i	O
for	O
regression	B
problems	O
we	O
assume	O
yi	O
r	O
adapted	O
from	O
et	O
al	O
and	O
and	O
hothorn	O
logloss	O
exp	O
s	O
s	O
o	O
l	O
y	O
f	O
figure	O
illustration	O
of	O
various	O
loss	B
functions	O
for	O
binary	O
classification	O
the	O
horizontal	O
axis	O
is	O
the	O
margin	B
y	O
the	O
vertical	O
axis	O
is	O
the	O
loss	B
the	O
log	O
loss	B
uses	O
log	O
base	O
figure	O
generated	O
by	O
hingelossplot	O
as	O
we	O
showed	O
in	O
section	O
of	O
course	O
this	O
cannot	O
be	O
computed	O
in	O
practice	O
since	O
it	O
requires	O
knowing	O
the	O
true	O
conditional	O
distribution	O
pyx	O
hence	O
this	O
is	O
sometimes	O
called	O
the	O
population	B
minimizer	I
where	O
the	O
expectation	O
is	O
interpreted	O
in	O
a	O
frequentist	B
sense	O
below	O
we	O
will	O
see	O
that	O
boosting	B
will	O
try	O
to	O
approximate	O
this	O
conditional	O
expectation	O
for	O
binary	O
classification	O
the	O
obvious	O
loss	B
is	O
loss	B
but	O
this	O
is	O
not	O
differentiable	O
instead	O
it	O
is	O
common	O
to	O
use	O
logloss	O
which	O
is	O
a	O
convex	B
upper	O
bound	O
on	O
loss	B
as	O
we	O
showed	O
in	O
section	O
in	O
this	O
case	O
one	O
can	O
show	O
that	O
the	O
optimal	O
estimate	O
is	O
given	O
by	O
log	O
where	O
y	O
one	O
can	O
generalize	B
this	O
framework	O
to	O
the	O
multiclass	O
case	O
but	O
we	O
will	O
not	O
discuss	O
that	O
here	O
an	O
alternative	O
convex	B
upper	O
bound	O
is	O
exponential	B
loss	B
defined	O
by	O
l	O
y	O
f	O
exp	O
yf	O
see	O
figure	O
for	O
a	O
plot	O
this	O
will	O
have	O
some	O
computational	O
advantages	O
over	O
the	O
logloss	O
to	O
be	O
discussed	O
below	O
it	O
turns	O
out	O
that	O
the	O
optimal	O
estimate	O
for	O
this	O
loss	B
is	O
also	O
f	O
f	O
p	O
y	O
p	O
y	O
boosting	B
p	O
p	O
y	O
to	O
see	O
this	O
we	O
can	O
just	O
set	O
the	O
derivative	O
of	O
the	O
expected	O
loss	B
each	O
x	O
to	O
log	O
zero	O
f	O
e	O
e	O
yf	O
y	O
f	O
p	O
y	O
f	O
p	O
y	O
f	O
p	O
y	O
p	O
y	O
p	O
y	O
so	O
in	O
both	O
cases	O
we	O
can	O
see	O
that	O
boosting	B
should	O
try	O
to	O
approximate	O
the	O
log-odds	B
ratio	I
since	O
finding	O
the	O
optimal	O
f	O
is	O
hard	O
we	O
shall	O
tackle	O
it	O
sequentially	O
we	O
initialise	O
by	O
defining	O
arg	O
min	O
lyi	O
f	O
for	O
example	O
if	O
we	O
use	O
squared	B
error	I
we	O
can	O
set	O
y	O
and	O
if	O
we	O
use	O
log-loss	B
or	O
exponential	B
loss	B
we	O
can	O
set	O
iyi	O
we	O
could	O
also	O
use	O
a	O
more	O
powerful	O
model	O
for	O
our	O
baseline	O
such	O
as	O
a	O
glm	B
where	O
log	O
n	O
then	O
at	O
iteration	O
m	O
we	O
compute	O
lyi	O
fm	O
m	O
m	O
argmin	O
and	O
then	O
we	O
set	O
fmx	O
fm	O
m	O
m	O
the	O
key	O
point	O
is	O
that	O
we	O
do	O
not	O
go	O
back	O
and	O
adjust	O
earlier	O
parameters	O
this	O
is	O
why	O
the	O
method	O
is	O
called	O
forward	B
stagewise	I
additive	I
modeling	I
we	O
continue	O
this	O
for	O
a	O
fixed	O
number	O
of	O
iterations	O
m	O
in	O
fact	O
m	O
is	O
the	O
main	O
tuning	O
parameter	B
of	O
the	O
method	O
often	O
we	O
pick	O
it	O
by	O
monitoring	O
the	O
performance	O
on	O
a	O
separate	O
validation	B
set	I
and	O
then	O
stopping	O
once	O
performance	O
starts	O
to	O
decrease	O
this	O
is	O
called	O
early	B
stopping	I
alternatively	O
we	O
can	O
use	O
model	B
selection	I
criteria	O
such	O
as	O
aic	B
or	O
bic	B
e	O
g	O
and	O
hothorn	O
for	O
details	O
in	O
practice	O
better	O
set	O
performance	O
can	O
be	O
obtained	O
by	O
performing	O
partial	O
updates	O
of	O
the	O
form	O
fmx	O
fm	O
m	O
m	O
here	O
is	O
a	O
step-size	O
parameter	B
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
a	O
small	O
value	O
such	O
as	O
this	O
is	O
called	O
shrinkage	B
below	O
we	O
discuss	O
how	O
to	O
solve	O
the	O
suproblem	O
in	O
equation	O
this	O
will	O
depend	O
on	O
the	O
form	O
of	O
loss	B
function	I
however	O
it	O
is	O
independent	O
of	O
the	O
form	O
of	O
weak	B
learner	I
suppose	O
we	O
used	O
squared	B
error	I
loss	B
then	O
at	O
step	O
m	O
the	O
loss	B
has	O
the	O
form	O
lyi	O
fm	O
chapter	O
adaptive	O
basis	O
function	O
models	O
figure	O
example	O
of	O
adaboost	O
using	O
a	O
decision	B
stump	O
as	O
a	O
weak	B
learner	I
the	O
degree	B
of	O
blackness	O
represents	O
the	O
confidence	O
in	O
the	O
red	O
class	O
the	O
degree	B
of	O
whiteness	O
represents	O
the	O
confidence	O
in	O
the	O
blue	O
class	O
the	O
size	O
of	O
the	O
datapoints	O
represents	O
their	O
weight	O
decision	B
boundary	I
is	O
in	O
yellow	O
after	O
after	O
rounds	O
figure	O
generated	O
by	O
boostingdemo	O
written	O
by	O
richard	O
round	O
stapenhurst	O
after	O
rounds	O
where	O
rim	O
yi	O
fm	O
is	O
the	O
current	O
residual	B
and	O
we	O
have	O
set	O
without	O
loss	B
of	O
generality	O
hence	O
we	O
can	O
find	O
the	O
new	O
basis	O
function	O
by	O
using	O
the	O
weak	B
learner	I
to	O
predict	O
rm	O
this	O
is	O
called	O
or	O
least	B
squares	I
boosting	B
and	O
yu	O
in	O
section	O
we	O
will	O
see	O
that	O
this	O
method	O
with	O
a	O
suitable	O
choice	O
of	O
weak	B
learner	I
can	O
be	O
made	O
to	O
give	O
the	O
same	O
results	O
as	O
lars	B
which	O
can	O
be	O
used	O
to	O
perform	O
variable	O
selection	O
section	O
adaboost	O
consider	O
a	O
binary	O
classification	O
problem	O
with	O
exponential	B
loss	B
at	O
step	O
m	O
we	O
have	O
to	O
minimize	O
lm	O
where	O
wim	O
exp	O
yifm	O
is	O
a	O
weight	O
applied	O
to	O
datacase	O
i	O
and	O
yi	O
we	O
can	O
rewrite	O
this	O
objective	O
as	O
follows	O
wim	O
e	O
exp	O
yifm	O
wim	O
exp	O
yi	O
lm	O
e	O
m	O
log	O
errm	O
errm	O
yi	O
wim	O
e	O
wimi	O
yi	O
e	O
consequently	O
the	O
optimal	O
function	O
to	O
add	O
is	O
m	O
argmin	O
wimi	O
yi	O
wim	O
this	O
can	O
be	O
found	O
by	O
applying	O
the	O
weak	B
learner	I
to	O
a	O
weighted	O
version	O
of	O
the	O
dataset	O
with	O
weights	O
wim	O
subsituting	O
m	O
into	O
lm	O
and	O
solving	O
for	O
we	O
find	O
boosting	B
where	O
errm	O
wii	O
yi	O
mxi	O
wim	O
the	O
overall	O
update	O
becomes	O
fmx	O
fm	O
m	O
mx	O
with	O
this	O
the	O
weights	O
at	O
the	O
next	O
iteration	O
become	O
wime	O
m	O
yi	O
mxi	O
wime	O
mxi	O
m	O
i	O
mxie	O
m	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
yi	O
mxi	O
if	O
yi	O
mxi	O
and	O
yi	O
mxi	O
otherwise	O
since	O
e	O
m	O
will	O
cancel	O
out	O
in	O
the	O
normalization	O
step	O
we	O
can	O
drop	O
it	O
the	O
result	O
is	O
the	O
algorithm	O
shown	O
in	O
algorithm	O
known	O
an	O
example	O
of	O
this	O
algorithm	O
in	O
action	B
using	O
decision	B
stumps	O
as	O
the	O
weak	B
learner	I
is	O
given	O
in	O
figure	O
we	O
see	O
that	O
after	O
many	O
iterations	O
we	O
can	O
carve	O
out	O
a	O
complex	O
decision	B
boundary	I
what	O
is	O
rather	O
surprising	O
is	O
that	O
adaboost	O
is	O
very	O
slow	O
to	O
overfit	O
as	O
is	O
apparent	O
in	O
figure	O
see	O
section	O
for	O
a	O
discussion	O
of	O
this	O
point	O
algorithm	O
for	O
binary	O
classification	O
with	O
exponential	B
loss	B
wi	O
for	O
m	O
do	O
fit	O
a	O
classifier	O
mx	O
to	O
the	O
training	B
set	I
using	O
weights	O
w	O
wim	O
i	O
mxi	O
compute	O
errm	O
compute	O
m	O
errmerrm	O
set	O
wi	O
wi	O
exp	O
mi	B
yi	O
mxi	O
m	O
mx	O
wim	O
return	O
f	O
sgn	O
logitboost	B
the	O
trouble	O
with	O
exponential	B
loss	B
is	O
that	O
it	O
puts	O
a	O
lot	O
of	O
weight	O
on	O
misclassified	O
examples	O
as	O
is	O
apparent	O
from	O
the	O
exponential	O
blowup	O
on	O
the	O
left	O
hand	O
side	O
of	O
figure	O
this	O
makes	O
the	O
method	O
very	O
sensitive	O
to	O
outliers	B
examples	O
in	O
addition	O
e	O
yf	O
is	O
not	O
the	O
logarithm	O
of	O
any	O
pmf	B
for	O
binary	O
variables	O
y	O
consequently	O
we	O
cannot	O
recover	O
probability	O
estimates	O
from	O
f	O
in	O
et	O
al	O
this	O
is	O
called	O
discrete	B
adaboost	I
since	O
it	O
assumes	O
that	O
the	O
base	O
classifier	O
m	O
returns	O
a	O
binary	O
class	O
label	B
if	O
m	O
returns	O
a	O
probability	O
instead	O
a	O
modified	O
algorithm	O
known	O
as	O
real	B
adaboost	I
can	O
be	O
used	O
see	O
et	O
al	O
for	O
details	O
chapter	O
adaptive	O
basis	O
function	O
models	O
a	O
natural	O
alternative	O
is	O
to	O
use	O
logloss	O
instead	O
this	O
only	O
punishes	O
mistakes	O
linearly	O
as	O
is	O
clear	O
from	O
figure	O
furthermore	O
it	O
means	O
that	O
we	O
will	O
be	O
able	O
to	O
extract	O
probabilities	O
from	O
the	O
final	O
learned	O
function	O
using	O
py	O
e	O
ef	O
e	O
f	O
ef	O
the	O
goal	O
is	O
to	O
minimze	O
the	O
expected	O
log-loss	B
given	O
by	O
lm	O
log	O
exp	O
yifm	O
by	O
performing	O
a	O
newton	O
upate	O
on	O
this	O
objective	O
to	O
irls	B
one	O
can	O
derive	O
the	O
algorithm	O
shown	O
in	O
algorithm	O
this	O
is	O
known	O
as	O
logitboost	B
et	O
al	O
it	O
can	O
be	O
generalized	O
to	O
the	O
multi-class	O
setting	O
as	O
explained	O
in	O
et	O
al	O
algorithm	O
logitboost	B
for	O
binary	O
classification	O
with	O
log-loss	B
wi	O
i	O
for	O
m	O
do	O
i	O
i	O
y	O
compute	O
the	O
working	B
response	I
zi	O
i	O
compute	O
the	O
weights	O
wi	O
i	O
wizi	O
m	O
argmin	O
update	O
f	O
f	O
compute	O
i	O
exp	O
mx	O
return	O
f	O
sgn	O
mx	O
boosting	B
as	O
functional	B
gradient	B
descent	I
rather	O
than	O
deriving	O
new	O
versions	O
of	O
boosting	B
for	O
every	O
different	O
loss	B
function	I
it	O
is	O
possible	O
to	O
derive	O
a	O
generic	O
version	O
known	O
as	O
gradient	B
boosting	B
mason	O
et	O
al	O
to	O
explain	O
this	O
imagine	O
minimizing	O
f	O
argmin	O
f	O
lf	O
where	O
f	O
f	O
are	O
the	O
parameters	O
we	O
will	O
solve	O
this	O
stagewise	O
using	O
gradient	B
descent	I
at	O
step	O
m	O
letg	O
m	O
be	O
the	O
gradient	O
of	O
lf	O
evaluated	O
at	O
f	O
fm	O
gim	O
lyi	O
f	O
f	O
f	O
gradients	O
of	O
some	O
common	O
loss	B
functions	O
are	O
given	O
in	O
table	O
we	O
then	O
make	O
the	O
update	O
fm	O
fm	O
mgm	O
boosting	B
where	O
m	O
is	O
the	O
step	O
length	O
chosen	O
by	O
m	O
argmin	O
lfm	O
gm	B
this	O
is	O
called	O
functional	B
gradient	B
descent	I
in	O
its	O
current	O
form	O
this	O
is	O
not	O
much	O
use	O
since	O
it	O
only	O
optimizes	O
f	O
at	O
a	O
fixed	O
set	O
of	O
n	O
points	O
so	O
we	O
do	O
not	O
learn	O
a	O
function	O
that	O
can	O
generalize	B
however	O
we	O
can	O
modify	O
the	O
algorithm	O
by	O
fitting	O
a	O
weak	B
learner	I
to	O
approximate	O
the	O
negative	O
gradient	O
signal	O
that	O
is	O
we	O
use	O
this	O
update	O
gim	O
m	O
argmin	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
which	O
is	O
not	O
strictly	O
necessary	O
as	O
argued	O
in	O
and	O
hothorn	O
have	O
omitted	O
the	O
line	B
search	I
step	O
algorithm	O
gradient	B
boosting	B
initialize	O
argmin	O
for	O
m	O
do	O
lyi	O
compute	O
the	O
gradient	O
residual	B
using	O
rim	O
f	O
use	O
the	O
weak	B
learner	I
to	O
compute	O
m	O
which	O
minimizes	O
update	O
fmx	O
fm	O
m	O
lyif	O
f	O
return	O
f	O
fm	O
if	O
we	O
apply	O
this	O
algorithm	O
using	O
squared	B
loss	B
we	O
recover	O
if	O
we	O
apply	O
this	O
algorithm	O
to	O
log-loss	B
we	O
get	O
an	O
algorithm	O
known	O
as	O
binomialboost	B
and	O
hothorn	O
the	O
advantage	O
of	O
this	O
over	O
logitboost	B
is	O
that	O
it	O
does	O
not	O
need	O
to	O
be	O
able	O
to	O
do	O
weighted	O
fitting	O
it	O
just	O
applies	O
any	O
black-box	B
regression	B
model	O
to	O
the	O
gradient	O
vector	O
also	O
it	O
is	O
relatively	O
easy	O
to	O
extend	O
to	O
the	O
multi-class	O
case	O
et	O
al	O
we	O
can	O
also	O
apply	O
this	O
algorithm	O
to	O
other	O
loss	B
functions	O
such	O
as	O
the	O
huber	B
loss	B
which	O
is	O
more	O
robust	B
to	O
outliers	B
than	O
squared	B
error	I
loss	B
sparse	B
boosting	B
suppose	O
we	O
use	O
as	O
our	O
weak	B
learner	I
the	O
following	O
algorithm	O
search	O
over	O
all	O
possible	O
variables	O
j	O
and	O
pick	O
the	O
one	O
jm	O
that	O
best	O
predicts	O
the	O
residual	B
vector	O
jm	O
argmin	O
j	O
xijrim	O
ij	O
jm	O
mx	O
jmm	O
xjm	O
chapter	O
adaptive	O
basis	O
function	O
models	O
this	O
method	O
which	O
is	O
known	O
as	O
sparse	B
boosting	B
and	O
yu	O
is	O
identical	O
to	O
the	O
matching	B
pursuit	I
algorithm	O
discussed	O
in	O
section	O
it	O
is	O
clear	O
that	O
this	O
will	O
result	O
in	O
a	O
sparse	B
estimate	O
at	O
least	O
if	O
m	O
is	O
small	O
to	O
see	O
this	O
let	O
us	O
rewrite	O
the	O
update	O
as	O
follows	O
m	O
m	O
jmm	O
where	O
the	O
non-zero	O
entry	O
occurs	O
in	O
location	O
jm	O
this	O
is	O
known	O
as	O
forward	B
stagewise	I
linear	B
regression	B
et	O
al	O
which	O
becomes	O
equivalent	O
to	O
the	O
lar	B
algorithm	O
discussed	O
in	O
section	O
as	O
increasing	O
the	O
number	O
of	O
steps	O
m	O
in	O
boosting	B
is	O
analogous	O
to	O
decreasing	O
the	O
regularization	B
penalty	O
if	O
we	O
modify	O
boosting	B
to	O
allow	O
some	O
variable	O
deletion	O
steps	O
and	O
yu	O
we	O
can	O
make	O
it	O
equivalent	O
to	O
the	O
lars	B
algorithm	O
which	O
computes	O
the	O
full	B
regularization	B
path	B
for	O
the	O
lasso	B
problem	O
the	O
same	O
algorithm	O
can	O
be	O
used	O
for	O
sparse	B
logistic	B
regression	B
by	O
simply	O
modifying	O
the	O
residual	B
to	O
be	O
the	O
appropriate	O
negative	O
gradient	O
now	O
consider	O
a	O
weak	B
learner	I
that	O
is	O
similar	B
to	O
the	O
above	O
except	O
it	O
uses	O
a	O
smoothing	B
spline	B
instead	O
of	O
linear	B
regression	B
when	O
mapping	O
from	O
xj	O
to	O
the	O
residual	B
the	O
result	O
is	O
a	O
sparse	B
generalized	B
additive	I
model	I
section	O
it	O
can	O
obviously	O
be	O
extended	O
to	O
pick	O
pairs	O
of	O
variables	O
at	O
a	O
time	O
the	O
resulting	O
method	O
often	O
works	O
much	O
better	O
than	O
mars	B
and	O
yu	O
multivariate	O
adaptive	O
regression	B
trees	O
it	O
is	O
quite	O
common	O
to	O
use	O
cart	B
models	O
as	O
weak	O
learners	O
it	O
is	O
usually	O
advisable	O
to	O
use	O
a	O
shallow	O
tree	B
so	O
that	O
the	O
variance	B
is	O
low	O
even	O
though	O
the	O
bias	B
will	O
be	O
high	O
a	O
shallow	O
tree	B
is	O
likely	O
to	O
be	O
far	O
from	O
the	O
truth	O
this	O
will	O
compensated	O
for	O
in	O
subsequent	O
rounds	O
of	O
boosting	B
the	O
height	O
of	O
the	O
tree	B
is	O
an	O
additional	O
tuning	O
parameter	B
addition	O
to	O
m	O
the	O
number	O
of	O
rounds	O
of	O
boosting	B
and	O
the	O
shrinkage	B
factor	B
suppose	O
we	O
restrict	O
to	O
trees	O
with	O
j	O
leaves	B
if	O
j	O
we	O
get	O
a	O
stump	O
which	O
can	O
only	O
split	O
on	O
a	O
single	O
variable	O
if	O
j	O
we	O
allow	O
for	O
in	O
general	O
it	O
is	O
recommended	O
in	O
et	O
al	O
two-variable	O
interactions	O
etc	O
and	O
and	O
niculescu-mizil	O
to	O
use	O
j	O
if	O
we	O
combine	O
the	O
gradient	B
boosting	B
algorithm	O
with	O
regression	B
trees	O
we	O
get	O
a	O
model	O
known	O
as	O
mart	B
which	O
stands	O
for	O
multivariate	O
adaptive	O
regression	B
trees	O
this	O
actually	O
includes	O
a	O
slight	O
refinement	O
to	O
the	O
basic	O
gradient	B
boosting	B
algorithm	O
after	O
fitting	O
a	O
regression	B
tree	B
to	O
the	O
residual	B
gradient	O
we	O
re-estimate	O
the	O
parameters	O
at	O
the	O
leaves	B
of	O
the	O
tree	B
to	O
minimize	O
the	O
loss	B
jm	O
argmin	O
xi	O
rjm	O
lyi	O
fm	O
where	O
rjm	O
is	O
the	O
region	O
for	O
leaf	B
j	O
in	O
the	O
m	O
th	O
tree	B
and	O
jm	O
is	O
the	O
corresponding	O
parameter	B
mean	B
response	O
of	O
y	O
for	O
regression	B
problems	O
or	O
the	O
most	O
probable	O
class	O
label	B
for	O
classification	O
problems	O
why	O
does	O
boosting	B
work	O
so	O
well	O
we	O
have	O
seen	O
that	O
boosting	B
works	O
very	O
well	O
especially	O
for	O
classifiers	O
there	O
are	O
two	O
main	O
reasons	O
for	O
this	O
first	O
it	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
regularization	B
which	O
is	O
known	O
to	O
help	O
feedforward	O
neural	B
networks	I
perceptrons	O
prevent	O
overfitting	O
by	O
eliminating	O
irrelevant	O
features	B
to	O
see	O
this	O
imagine	O
pre-computing	O
all	O
possible	O
weak-learners	O
and	O
defining	O
a	O
feature	O
vector	O
of	O
the	O
form	O
kx	O
we	O
could	O
use	O
regularization	B
to	O
select	O
a	O
subset	O
of	O
these	O
alternatively	O
we	O
can	O
use	O
boosting	B
where	O
at	O
each	O
step	O
the	O
weak	B
learner	I
creates	O
a	O
new	O
k	O
on	O
the	O
fly	O
it	O
is	O
possible	O
to	O
combine	O
boosting	B
and	O
regularization	B
to	O
get	O
an	O
algorithm	O
known	O
as	O
and	O
singer	O
essentially	O
this	O
method	O
greedily	O
adds	O
the	O
best	O
features	B
learners	O
using	O
boosting	B
and	O
then	O
prunes	O
off	O
irrelevant	O
ones	O
using	O
regularization	B
another	O
explanation	O
has	O
to	O
do	O
with	O
the	O
concept	B
of	O
margin	B
which	O
we	O
introduced	O
in	O
section	O
et	O
al	O
ratsch	O
et	O
al	O
proved	O
that	O
adaboost	O
maximizes	O
the	O
margin	B
on	O
the	O
training	O
data	O
et	O
al	O
generalized	O
this	O
to	O
other	O
loss	B
functions	O
such	O
as	O
log-loss	B
a	O
bayesian	B
view	O
so	O
far	O
our	O
presentation	O
of	O
boosting	B
has	O
been	O
very	O
frequentist	B
since	O
it	O
has	O
focussed	O
on	O
greedily	O
minimizing	O
loss	B
functions	O
a	O
likelihood	B
interpretation	O
of	O
the	O
algorithm	O
was	O
given	O
in	O
and	O
mackay	O
meek	O
et	O
al	O
the	O
idea	O
is	O
to	O
consider	O
a	O
mixture	B
of	I
experts	I
model	O
of	O
the	O
form	O
pyx	O
mpyx	O
m	O
where	O
each	O
expert	O
pyx	O
m	O
is	O
like	O
a	O
weak	B
learner	I
we	O
usually	O
fit	O
all	O
m	O
experts	O
at	O
once	O
using	O
em	B
but	O
we	O
can	O
imagine	O
a	O
sequential	B
scheme	O
whereby	O
we	O
only	O
update	O
the	O
parameters	O
in	O
the	O
e	B
step	I
the	O
posterior	O
responsibilities	O
will	O
reflect	O
how	O
well	O
the	O
for	O
one	O
expert	O
at	O
a	O
time	O
existing	O
experts	O
explain	O
a	O
given	O
data	O
point	O
if	O
this	O
is	O
a	O
poor	O
fit	O
these	O
data	O
points	O
will	O
have	O
more	O
influence	O
on	O
the	O
next	O
expert	O
that	O
is	O
fitted	O
view	O
naturally	O
suggest	O
a	O
way	O
to	O
use	O
a	O
boosting-like	O
algorithm	O
for	O
unsupervised	B
learning	B
we	O
simply	O
sequentially	O
fit	O
mixture	B
models	O
instead	O
of	O
mixtures	O
of	O
experts	O
notice	O
that	O
this	O
is	O
a	O
rather	O
broken	O
mle	B
procedure	O
since	O
it	O
never	O
goes	O
back	O
to	O
update	O
the	O
parameters	O
of	O
an	O
old	O
expert	O
similarly	O
if	O
boosting	B
ever	O
wants	O
to	O
change	O
the	O
weight	O
assigned	O
to	O
a	O
weak	B
learner	I
the	O
only	O
way	O
to	O
do	O
this	O
is	O
to	O
add	O
the	O
weak	B
learner	I
again	O
with	O
a	O
new	O
weight	O
this	O
can	O
result	O
in	O
unnecessarily	O
large	O
models	O
by	O
contrast	O
the	O
bart	B
model	O
et	O
al	O
uses	O
a	O
bayesian	B
version	O
of	O
backfitting	O
to	O
fit	O
a	O
small	O
sum	O
of	O
weak	O
learners	O
trees	O
feedforward	O
neural	B
networks	I
perceptrons	O
a	O
feedforward	B
neural	B
network	I
aka	O
multi-layer	B
perceptron	B
is	O
a	O
series	O
of	O
logistic	B
regression	B
models	O
stacked	O
on	O
top	O
of	O
each	O
other	O
with	O
the	O
final	O
layer	O
being	O
either	O
another	O
logistic	B
regression	B
or	O
a	O
linear	B
regression	B
model	O
depending	O
on	O
whether	O
we	O
are	O
solving	O
a	O
classification	O
or	O
regression	B
problem	O
for	O
example	O
if	O
we	O
have	O
two	O
layers	O
and	O
we	O
are	O
solving	O
a	O
regression	B
problem	O
the	O
model	O
has	O
the	O
form	O
pyx	O
zx	O
zx	O
where	O
g	O
is	O
a	O
non-linear	O
activation	B
or	O
transfer	B
function	I
the	O
logistic	B
function	O
zx	O
x	O
v	O
is	O
called	O
the	O
hidden	B
layer	I
deterministic	O
function	O
of	O
the	O
input	O
h	O
is	O
the	O
x	O
gvt	O
h	O
x	O
chapter	O
adaptive	O
basis	O
function	O
models	O
xnd	O
xni	O
vij	O
znh	O
znj	O
wjk	O
ync	O
ynk	O
figure	O
a	O
neural	B
network	I
with	O
one	O
hidden	B
layer	I
number	O
of	O
hidden	B
units	I
v	O
is	O
the	O
weight	O
matrix	O
from	O
the	O
inputs	O
to	O
the	O
hidden	B
nodes	B
and	O
w	O
is	O
the	O
weight	B
vector	I
from	O
the	O
hidden	B
nodes	B
to	O
the	O
output	O
it	O
is	O
important	O
that	O
g	O
be	O
nonlinear	O
otherwise	O
the	O
whole	O
model	O
collapses	O
into	O
a	O
large	O
linear	B
regression	B
model	O
of	O
the	O
form	O
y	O
wt	O
one	O
can	O
show	O
that	O
an	O
mlp	B
is	O
a	O
universal	B
approximator	I
meaning	O
it	O
can	O
model	O
any	O
suitably	O
smooth	O
function	O
given	O
enough	O
hidden	B
units	I
to	O
any	O
desired	O
level	O
of	O
accuracy	O
to	O
handle	O
binary	O
classification	O
we	O
pass	O
the	O
output	O
through	O
a	O
sigmoid	B
as	O
in	O
a	O
glm	B
pyx	O
berysigmwt	O
zx	O
we	O
can	O
easily	O
extend	O
the	O
mlp	B
to	O
predict	O
multiple	O
outputs	O
for	O
example	O
in	O
the	O
regression	B
case	O
we	O
have	O
pyx	O
n	O
v	O
see	O
figure	O
for	O
an	O
illustration	O
if	O
we	O
add	O
mutual	B
inhibition	I
arcs	O
between	O
the	O
output	O
units	O
ensuring	O
that	O
only	O
one	O
of	O
them	O
turns	O
on	O
we	O
can	O
enforce	O
a	O
sum-to-one	O
constraint	O
which	O
can	O
be	O
used	O
for	O
multi-class	O
classification	O
the	O
resulting	O
model	O
has	O
the	O
form	O
pyx	O
catyswzx	O
convolutional	O
neural	B
networks	I
the	O
purpose	O
of	O
the	O
hidden	B
units	I
is	O
to	O
learn	O
non-linear	O
combinations	O
of	O
the	O
original	O
inputs	O
this	O
is	O
called	O
feature	B
extraction	I
or	O
feature	B
construction	I
these	O
hidden	B
features	B
are	O
then	O
passed	O
as	O
input	O
to	O
the	O
final	O
glm	B
this	O
approach	O
is	O
particularly	O
useful	O
for	O
problems	O
where	O
the	O
original	O
input	O
features	B
are	O
not	O
very	O
individually	O
informative	O
for	O
example	O
each	O
pixel	O
in	O
an	O
image	O
is	O
not	O
very	O
informative	O
it	O
is	O
the	O
combination	O
of	O
pixels	O
that	O
tells	O
us	O
what	O
objects	O
are	O
present	O
conversely	O
for	O
a	O
task	O
such	O
as	O
document	O
classification	O
using	O
a	O
bag	B
of	I
words	I
representation	O
each	O
feature	O
count	O
is	O
informative	O
on	O
its	O
own	O
so	O
extracting	O
higher	O
order	O
features	B
is	O
less	O
important	O
not	O
suprisingly	O
then	O
much	O
of	O
the	O
work	O
in	O
neural	B
networks	I
has	O
been	O
motivated	O
by	O
visual	O
pattern	B
feedforward	O
neural	B
networks	I
perceptrons	O
source	O
httpwww	O
codep	O
figure	O
the	O
convolutional	B
neural	B
network	I
from	O
et	O
al	O
roject	O
comkblibraryneuralnetrecognition	O
aspx	O
used	O
with	O
kind	O
permission	O
of	O
mike	O
o	O
neill	O
recognition	O
et	O
al	O
although	O
they	O
have	O
also	O
been	O
applied	O
to	O
other	O
types	O
of	O
data	O
including	O
text	O
and	O
weston	O
a	O
form	O
of	O
mlp	B
which	O
is	O
particularly	O
well	O
suited	O
to	O
signals	O
like	O
speech	O
or	O
text	O
or	O
signals	O
like	O
images	O
is	O
the	O
convolutional	B
neural	B
network	I
this	O
is	O
an	O
mlp	B
in	O
which	O
the	O
hidden	B
units	I
have	O
local	O
receptive	O
fields	O
in	O
the	O
primary	O
visual	O
cortex	O
and	O
in	O
which	O
the	O
weights	O
are	O
tied	B
or	O
shared	B
across	O
the	O
image	O
in	O
order	O
to	O
reduce	O
the	O
number	O
of	O
parameters	O
intuitively	O
the	O
effect	O
of	O
such	O
spatial	O
parameter	B
tying	I
is	O
that	O
any	O
useful	O
features	B
that	O
are	O
discovered	O
in	O
some	O
portion	O
of	O
the	O
image	O
can	O
be	O
re-used	O
everywhere	O
else	O
without	O
having	O
to	O
be	O
independently	O
learned	O
the	O
resulting	O
network	O
then	O
exhibits	O
translation	B
invariance	I
meaning	O
it	O
can	O
classify	O
patterns	O
no	O
matter	O
where	O
they	O
occur	O
inside	O
the	O
input	O
image	O
figure	O
gives	O
an	O
example	O
of	O
a	O
convolutional	O
network	O
designed	O
by	O
simard	O
and	O
colleagues	O
et	O
al	O
with	O
layers	O
layers	O
of	O
adjustable	O
parameters	O
designed	O
to	O
classify	O
gray-scale	O
images	O
of	O
handwritten	O
digits	O
from	O
the	O
mnist	B
dataset	O
section	O
in	O
layer	O
we	O
have	O
feature	B
maps	I
each	O
of	O
which	O
has	O
size	O
each	O
hidden	B
node	O
in	O
one	O
of	O
these	O
feature	B
maps	I
is	O
computed	O
by	O
convolving	O
the	O
image	O
with	O
a	O
weight	O
matrix	O
called	O
a	O
kernel	B
adding	O
a	O
bias	B
and	O
then	O
passing	O
the	O
result	O
through	O
some	O
form	O
of	O
nonlinearity	O
there	O
are	O
therefore	O
neurons	O
in	O
layer	O
and	O
weights	O
is	O
for	O
the	O
bias	B
if	O
we	O
did	O
not	O
share	O
these	O
parameters	O
there	O
would	O
be	O
weights	O
at	O
the	O
first	O
layer	O
in	O
layer	O
we	O
have	O
feature	B
maps	I
each	O
of	O
which	O
is	O
obtained	O
by	O
convolving	O
each	O
feature	O
map	O
in	O
layer	O
with	O
a	O
weight	O
matrix	O
adding	O
them	O
up	O
adding	O
a	O
bias	B
and	O
passing	O
through	O
a	O
nonlinearity	O
there	O
are	O
therefore	O
neurons	O
in	O
layer	O
adjustable	O
weights	O
kernel	B
for	O
each	O
pair	O
of	O
feature	O
chapter	O
adaptive	O
basis	O
function	O
models	O
maps	O
in	O
layers	O
and	O
and	O
connections	O
layer	O
is	O
fully	O
connected	O
to	O
layer	O
and	O
has	O
neurons	O
and	O
weights	O
finally	O
layer	O
is	O
also	O
fully	O
connected	O
and	O
has	O
neurons	O
and	O
weights	O
adding	O
the	O
above	O
numbers	O
there	O
are	O
a	O
total	O
of	O
neurons	O
adjustable	O
weights	O
and	O
connections	O
this	O
model	O
is	O
usually	O
trained	O
using	O
stochastic	B
gradient	B
descent	I
section	O
for	O
details	O
a	O
single	O
pass	O
over	O
the	O
data	O
set	O
is	O
called	O
an	O
epoch	B
when	O
mike	O
o	O
neill	O
did	O
these	O
experiments	O
in	O
he	O
found	O
that	O
a	O
single	O
epoch	B
took	O
about	O
minutes	O
that	O
there	O
are	O
training	O
examples	O
in	O
mnist	B
since	O
it	O
took	O
about	O
epochs	O
for	O
the	O
error	O
rate	B
to	O
converge	B
the	O
total	O
training	O
time	O
was	O
about	O
using	O
this	O
technique	O
he	O
obtained	O
a	O
misclassification	O
rate	B
on	O
the	O
test	O
cases	O
of	O
about	O
to	O
further	O
reduce	O
the	O
error	O
rate	B
a	O
standard	O
trick	O
is	O
to	O
expand	O
the	O
training	B
set	I
by	O
including	O
distorted	B
versions	O
of	O
the	O
original	O
data	O
to	O
encourage	O
the	O
network	O
to	O
be	O
invariant	B
to	O
small	O
changes	O
that	O
don	O
t	O
affect	O
the	O
identity	O
of	O
the	O
digit	O
these	O
can	O
be	O
created	O
by	O
applying	O
a	O
random	O
flow	O
field	O
to	O
shift	O
pixels	O
around	O
see	O
figure	O
for	O
some	O
examples	O
we	O
use	O
online	O
training	O
such	O
as	O
stochastic	B
gradient	B
descent	I
we	O
can	O
create	O
these	O
distortions	O
on	O
the	O
fly	O
rather	O
than	O
having	O
to	O
store	O
them	O
using	O
this	O
technique	O
mike	O
o	O
neill	O
obtained	O
a	O
misclassification	O
rate	B
on	O
the	O
test	O
cases	O
of	O
about	O
which	O
is	O
close	O
to	O
the	O
current	O
state	B
of	O
the	O
yann	O
le	O
cun	O
and	O
colleagues	O
et	O
al	O
obtained	O
similar	B
performance	O
using	O
a	O
slightly	O
more	O
complicated	O
architecture	O
shown	O
in	O
figure	O
this	O
model	O
is	O
known	O
as	O
and	O
historically	O
it	O
came	O
before	O
the	O
model	O
in	O
figure	O
there	O
are	O
two	O
main	O
differences	O
first	O
has	O
a	O
subsampling	B
layer	O
between	O
each	O
convolutional	O
layer	O
which	O
either	O
averages	O
or	O
computes	O
the	O
max	O
over	O
each	O
small	O
window	O
in	O
the	O
previous	O
layer	O
in	O
order	O
to	O
reduce	O
the	O
size	O
and	O
to	O
obtain	O
a	O
small	O
amount	O
of	O
shift	O
invariance	O
the	O
convolution	O
and	O
sub-sampling	O
combination	O
was	O
inspired	O
by	O
hubel	O
and	O
wiesel	O
s	O
model	O
of	O
simple	O
and	O
complex	O
cells	O
in	O
the	O
visual	O
cortex	O
and	O
wiesel	O
and	O
it	O
continues	O
to	O
be	O
popular	O
in	O
neurally-inspired	O
models	O
of	O
visual	O
object	O
recognition	O
and	O
poggio	O
a	O
similar	B
idea	O
first	O
appeared	O
in	O
fukushima	O
s	O
neocognitron	B
though	O
no	O
globally	O
supervised	O
training	O
algorithm	O
was	O
available	O
at	O
that	O
time	O
the	O
second	O
difference	O
between	O
and	O
the	O
simard	O
architecture	O
is	O
that	O
the	O
final	O
layer	O
is	O
actually	O
an	O
rbf	B
network	I
rather	O
than	O
a	O
more	O
standard	O
sigmoidal	O
or	O
softmax	B
layer	O
this	O
model	O
gets	O
a	O
test	O
error	O
rate	B
of	O
about	O
when	O
trained	O
with	O
no	O
distortions	O
and	O
when	O
trained	O
with	O
distortions	O
figure	O
shows	O
all	O
errors	O
made	O
by	O
the	O
system	O
some	O
are	O
genuinely	O
ambiguous	O
but	O
several	O
are	O
errors	O
that	O
a	O
person	O
would	O
never	O
make	O
a	O
web-based	O
demo	O
of	O
the	O
can	O
be	O
found	O
at	O
httpyann	O
lecun	O
comexdblenetindex	O
html	O
of	O
course	O
classifying	O
isolated	O
digits	O
is	O
of	O
limited	O
applicability	O
in	O
the	O
real	O
world	O
people	O
usually	O
write	O
strings	O
of	O
digits	O
or	O
other	O
letters	O
this	O
requires	O
both	O
segmentation	O
and	O
classification	O
le	O
cun	O
and	O
colleagues	O
devised	O
a	O
way	O
to	O
combine	O
convolutional	O
neural	B
networks	I
with	O
a	O
model	O
similar	B
to	O
a	O
conditional	O
random	O
field	O
in	O
section	O
to	O
solve	O
this	O
problem	O
the	O
system	O
was	O
eventually	O
deployed	O
by	O
the	O
us	O
postal	O
service	O
et	O
al	O
for	O
a	O
more	O
detailed	O
account	O
of	O
the	O
system	O
which	O
remains	O
one	O
of	O
the	O
best	O
performing	O
systems	O
for	O
this	O
task	O
implementation	O
details	O
mike	O
used	O
c	O
code	O
and	O
a	O
variety	O
of	O
speedup	O
tricks	O
he	O
was	O
using	O
standard	O
era	O
hardware	O
intel	O
pentium	O
hyperthreaded	O
processor	O
running	O
at	O
see	O
httpwww	O
codeproject	O
comkb	O
libraryneuralnetrecognition	O
aspx	O
for	O
details	O
a	O
list	O
of	O
various	O
methods	O
along	O
with	O
their	O
misclassification	O
rates	O
on	O
the	O
mnist	B
test	O
set	O
is	O
available	O
from	O
http	O
error	O
rates	O
within	O
of	O
each	O
other	O
are	O
not	O
statistically	O
significantly	O
different	O
feedforward	O
neural	B
networks	I
perceptrons	O
figure	O
several	O
synthetic	O
warpings	O
of	O
a	O
handwritten	O
digit	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
elasticdistortionsdemo	O
written	O
by	O
kevin	O
swersky	O
input	O
feature	B
maps	I
f	O
maps	O
f	O
maps	O
f	O
maps	O
layer	O
layer	O
output	O
convolutions	O
subsampling	B
convolutions	O
subsampling	B
full	B
connection	O
full	B
connection	O
gaussian	B
connections	O
figure	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yann	O
lecun	O
a	O
convolutional	O
neural	O
net	O
for	O
classifying	O
handwritten	O
digits	O
source	O
figure	O
from	O
chapter	O
adaptive	O
basis	O
function	O
models	O
figure	O
these	O
are	O
the	O
errors	O
made	O
by	O
on	O
the	O
test	O
cases	O
of	O
mnist	B
below	O
each	O
image	O
is	O
a	O
label	B
of	O
the	O
form	O
correct-label	O
estimated-label	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yann	O
lecun	O
to	O
figure	O
which	O
shows	O
the	O
results	O
of	O
a	O
deep	B
generative	O
model	O
other	O
kinds	O
of	O
neural	B
networks	I
other	O
network	O
topologies	O
are	O
possible	O
besides	O
the	O
ones	O
discussed	O
above	O
for	O
example	O
we	O
can	O
have	O
skip	B
arcs	I
that	O
go	O
directly	O
from	O
the	O
input	O
to	O
the	O
output	O
skipping	O
the	O
hidden	B
layer	I
we	O
can	O
have	O
sparse	B
connections	O
between	O
the	O
layers	O
etc	O
however	O
the	O
mlp	B
always	O
requires	O
that	O
the	O
weights	O
form	O
a	O
directed	B
acyclic	I
graph	B
if	O
we	O
allow	O
feedback	O
connections	O
the	O
model	O
is	O
known	O
as	O
a	O
recurrent	B
neural	B
network	I
this	O
defines	O
a	O
nonlinear	O
dynamical	O
system	O
but	O
does	O
not	O
have	O
a	O
simple	O
probabilistic	O
interpretation	O
such	O
rnn	O
models	O
are	O
currently	O
the	O
best	O
approach	O
for	O
language	B
modeling	I
performing	O
word	O
prediction	O
in	O
natural	O
language	O
et	O
al	O
significantly	O
outperforming	O
the	O
standard	O
n-gram-based	O
methods	O
discussed	O
in	O
section	O
if	O
we	O
allow	O
symmetric	B
connections	O
between	O
the	O
hidden	B
units	I
the	O
model	O
is	O
known	O
as	O
a	O
hopfield	O
network	O
or	O
associative	B
memory	I
its	O
probabilistic	O
counterpart	O
is	O
known	O
as	O
a	O
boltzmann	B
machine	I
section	O
and	O
can	O
be	O
used	O
for	O
unsupervised	B
learning	B
a	O
brief	O
history	O
of	O
the	O
field	O
neural	B
networks	I
have	O
been	O
the	O
subject	O
of	O
great	O
interest	O
for	O
many	O
decades	O
due	O
to	O
the	O
desire	O
to	O
understand	O
the	O
brain	O
and	O
to	O
build	O
learning	B
machines	O
it	O
is	O
not	O
possible	O
to	O
review	O
the	O
entire	O
history	O
here	O
instead	O
we	O
just	O
give	O
a	O
few	O
edited	O
highlights	O
the	O
field	O
is	O
generally	O
viewed	O
as	O
starting	O
with	O
mcculloch	O
and	O
pitts	O
and	O
pitts	O
who	O
devised	O
a	O
simple	O
mathematical	O
model	O
of	O
the	O
neuron	O
in	O
in	O
which	O
they	O
approximated	O
the	O
feedforward	O
neural	B
networks	I
perceptrons	O
output	O
as	O
a	O
weighted	O
sum	O
of	O
inputs	O
passed	O
through	O
a	O
threshold	O
function	O
y	O
i	O
i	O
wixi	O
for	O
some	O
threshold	O
this	O
is	O
similar	B
to	O
a	O
sigmoidal	O
activation	B
function	O
frank	O
rosenblatt	B
invented	O
the	O
perceptron	B
learning	B
algorithm	O
in	O
which	O
is	O
a	O
way	O
to	O
estimate	O
the	O
parameters	O
of	O
a	O
mcculloch-pitts	O
neuron	O
section	O
for	O
details	O
a	O
very	O
similar	B
model	O
called	O
the	O
adaline	B
adaptive	O
linear	O
element	O
was	O
invented	O
in	O
by	O
widrow	O
and	O
hoff	O
in	O
minsky	O
and	O
papert	O
and	O
papert	O
published	O
a	O
famous	O
book	O
called	O
perceptrons	O
in	O
which	O
they	O
showed	O
that	O
such	O
linear	O
models	O
with	O
no	O
hidden	B
layers	O
were	O
very	O
limited	O
in	O
their	O
power	O
since	O
they	O
cannot	O
classify	O
data	O
that	O
is	O
not	O
linearly	B
separable	I
this	O
considerably	O
reduced	O
interest	O
in	O
the	O
field	O
in	O
rumelhart	O
hinton	O
and	O
williams	O
et	O
al	O
discovered	O
the	O
backpropathe	O
gation	O
algorithm	O
section	O
which	O
allows	O
one	O
to	O
fit	O
models	O
with	O
hidden	B
layers	O
backpropagation	B
algorithm	I
was	O
originally	O
discovered	O
in	O
and	O
ho	O
and	O
independently	O
in	O
however	O
it	O
was	O
et	O
al	O
that	O
brought	O
the	O
algorithm	O
to	O
people	O
s	O
attention	O
this	O
spawned	O
a	O
decade	O
of	O
intense	O
interest	O
in	O
these	O
models	O
in	O
sejnowski	O
and	O
rosenberg	O
and	O
rosenberg	O
created	O
the	O
famous	O
nettalk	B
system	O
that	O
learned	O
a	O
mapping	O
from	O
english	O
words	O
to	O
phonetic	O
symbols	O
which	O
could	O
be	O
fed	O
into	O
a	O
speech	O
synthesizer	O
an	O
audio	O
demo	O
of	O
the	O
system	O
as	O
it	O
learns	O
over	O
time	O
can	O
be	O
found	O
at	O
the	O
systems	O
starts	O
by	O
babbling	O
and	O
then	O
gradually	O
learns	O
to	O
pronounce	O
english	O
words	O
nettalk	B
learned	O
a	O
distributed	B
representation	I
its	O
hidden	B
layer	I
of	O
various	O
sounds	O
and	O
its	O
success	O
spawned	O
a	O
big	O
debate	O
in	O
psychology	O
between	O
connectionism	B
based	O
on	O
neural	B
networks	I
and	O
computationalism	B
based	O
on	O
syntactic	O
rules	B
this	O
debate	O
lives	O
on	O
to	O
some	O
extent	O
in	O
the	O
machine	B
learning	B
community	O
where	O
there	O
are	O
still	O
arguments	O
about	O
whether	O
learning	B
is	O
best	O
performed	O
using	O
low-level	O
neurallike	O
representations	O
or	O
using	O
more	O
structured	O
models	O
in	O
yann	O
le	O
cun	O
and	O
others	O
et	O
al	O
created	O
the	O
famous	O
lenet	O
system	O
described	O
in	O
section	O
in	O
the	O
support	B
vector	I
machine	I
section	O
was	O
invented	O
et	O
al	O
svms	O
provide	O
similar	B
prediction	O
accuracy	O
to	O
neural	B
networks	I
while	O
being	O
considerably	O
easier	O
to	O
train	O
they	O
use	O
a	O
convex	B
objective	O
function	O
this	O
spawned	O
a	O
decade	O
of	O
interest	O
in	O
kernel	B
methods	O
in	O
note	O
however	O
that	O
svms	O
do	O
not	O
use	O
adaptive	O
basis	B
functions	I
so	O
they	O
require	O
a	O
fair	O
amount	O
of	O
human	O
expertise	O
to	O
design	O
the	O
right	O
kernel	B
function	I
in	O
geoff	O
hinton	O
invented	O
the	O
contrastive	B
divergence	I
training	O
procedure	O
which	O
provided	O
a	O
way	O
for	O
the	O
first	O
time	O
to	O
learn	O
deep	B
networks	I
by	O
training	O
one	O
layer	O
at	O
a	O
time	O
in	O
an	O
unsupervised	O
fashion	O
section	O
for	O
details	O
this	O
in	O
turn	O
has	O
spawned	O
renewed	O
interest	O
in	O
neural	B
networks	I
over	O
the	O
last	O
few	O
years	O
chapter	O
the	O
backpropagation	B
algorithm	I
unlike	O
a	O
glm	B
the	O
nll	B
of	O
an	O
mlp	B
is	O
a	O
non-convex	O
function	O
of	O
its	O
parameters	O
nevertheless	O
we	O
can	O
find	O
a	O
locally	O
optimal	O
ml	O
or	O
map	B
estimate	I
using	O
standard	O
gradient-based	O
optimization	B
methods	O
since	O
mlps	O
have	O
lots	O
of	O
parameters	O
they	O
are	O
often	O
trained	O
on	O
very	O
large	O
data	O
sets	O
it	O
became	O
part	O
of	O
the	O
folklore	O
during	O
the	O
that	O
to	O
get	O
published	O
in	O
the	O
top	O
machine	B
learning	B
conference	O
known	O
as	O
nips	O
which	O
stands	O
for	O
neural	O
information	B
processing	O
systems	O
it	O
was	O
important	O
to	O
ensure	O
your	O
paper	O
did	O
not	O
contain	O
the	O
word	O
neural	B
network	I
chapter	O
adaptive	O
basis	O
function	O
models	O
tanh	O
sigmoid	B
figure	O
two	O
possible	O
activation	B
functions	O
tanh	O
maps	O
r	O
to	O
and	O
is	O
the	O
preferred	O
nonlinearity	O
for	O
the	O
hidden	B
nodes	B
sigm	O
maps	O
r	O
to	O
and	O
is	O
the	O
preferred	O
nonlinearity	O
for	O
binary	O
nodes	B
at	O
the	O
output	O
layer	O
figure	O
generated	O
by	O
tanhplot	O
consequently	O
it	O
is	O
common	O
to	O
use	O
first-order	O
online	O
methods	O
such	O
as	O
stochastic	B
gradient	B
descent	I
whereas	O
glms	O
are	O
usually	O
fit	O
with	O
irls	B
which	O
is	O
a	O
second-order	O
offline	B
method	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
the	O
gradient	O
vector	O
of	O
the	O
nll	B
by	O
applying	O
the	O
chain	B
rule	I
of	O
calculus	O
the	O
resulting	O
algorithm	O
is	O
known	O
as	O
backpropagation	B
for	O
reasons	O
that	O
will	O
become	O
apparent	O
for	O
notational	O
simplicity	O
we	O
shall	O
assume	O
a	O
model	O
with	O
just	O
one	O
hidden	B
layer	I
it	O
is	O
helpful	O
to	O
distinguish	O
the	O
pre-	O
and	O
post-synaptic	O
values	O
of	O
a	O
neuron	O
that	O
is	O
before	O
and	O
after	O
we	O
apply	O
the	O
nonlinearity	O
let	O
xn	O
be	O
the	O
n	O
th	O
input	O
an	O
vxn	O
be	O
the	O
pre-synaptic	O
hidden	B
layer	I
and	O
zn	O
gan	O
be	O
the	O
post-synaptic	O
hidden	B
layer	I
where	O
g	O
is	O
some	O
transfer	B
function	I
we	O
typically	O
use	O
ga	O
sigma	O
but	O
we	O
may	O
also	O
use	O
ga	O
tanha	O
see	O
figure	O
for	O
a	O
comparison	O
the	O
input	O
to	O
sigm	O
or	O
tanh	O
is	O
a	O
vector	O
we	O
assume	O
it	O
is	O
applied	O
component-wise	O
we	O
now	O
convert	O
this	O
hidden	B
layer	I
to	O
the	O
output	O
layer	O
as	O
follows	O
let	O
bn	O
wzn	O
be	O
the	O
pre-synaptic	O
output	O
layer	O
and	O
yn	O
hbn	O
be	O
the	O
post-synaptic	O
output	O
layer	O
where	O
h	O
is	O
another	O
nonlinearity	O
corresponding	O
to	O
the	O
canonical	O
link	O
for	O
the	O
glm	B
reserve	O
the	O
notation	O
yn	O
without	O
the	O
hat	O
for	O
the	O
output	O
corresponding	O
to	O
the	O
n	O
th	O
training	O
case	O
for	O
a	O
regression	B
model	O
we	O
use	O
hb	O
b	O
for	O
binary	O
classifcation	O
we	O
use	O
hb	O
sigmbc	O
for	O
multi-class	O
classification	O
we	O
use	O
hb	O
sb	O
we	O
can	O
write	O
the	O
overall	O
model	O
as	O
follows	O
v	O
an	O
g	O
zn	O
w	O
bn	O
h	O
yn	O
xn	O
the	O
parameters	O
of	O
the	O
model	O
are	O
w	O
the	O
first	O
and	O
second	O
layer	O
weight	O
matrices	O
offset	O
or	O
bias	B
terms	O
can	O
be	O
accomodated	O
by	O
clamping	B
an	O
element	O
of	O
xn	O
and	O
zn	O
to	O
in	O
the	O
regression	B
setting	O
we	O
can	O
easily	O
estimate	O
the	O
variance	B
of	O
the	O
output	O
noise	O
using	O
the	O
empirical	O
variance	B
of	O
the	O
y	O
after	O
training	O
is	O
complete	B
there	O
will	O
be	O
one	O
value	O
of	O
for	O
each	O
output	O
node	O
residual	B
errors	O
n	O
if	O
we	O
are	O
performing	O
multi-target	O
regression	B
as	O
we	O
usually	O
assume	O
feedforward	O
neural	B
networks	I
perceptrons	O
n	O
k	O
in	O
the	O
regression	B
case	O
with	O
k	O
outputs	O
the	O
nll	B
is	O
given	O
by	O
the	O
squared	B
error	I
j	O
ynk	O
in	O
the	O
classification	O
case	O
with	O
k	O
classes	O
the	O
nll	B
is	O
given	O
by	O
the	O
cross	B
entropy	B
j	O
ynk	O
log	O
ynk	O
our	O
task	O
is	O
to	O
compute	O
j	O
we	O
will	O
derive	O
this	O
for	O
each	O
n	O
separately	O
the	O
overall	O
gradient	O
is	O
obtained	O
by	O
summing	O
over	O
n	O
although	O
often	O
we	O
just	O
use	O
a	O
mini-batch	B
section	O
n	O
k	O
let	O
us	O
start	O
by	O
considering	O
the	O
output	O
layer	O
weights	O
we	O
have	O
wk	O
wk	O
bnk	O
jn	O
zn	O
jn	O
bnk	O
jn	O
bnk	O
since	O
bnk	O
wt	O
equation	O
tells	O
us	O
that	O
k	O
zn	O
assuming	O
h	O
is	O
the	O
canonical	B
link	B
function	I
for	O
the	O
output	O
glm	B
then	O
jn	O
bnk	O
w	O
nk	O
ynk	O
ynk	O
which	O
is	O
the	O
error	B
signal	I
so	O
the	O
overall	O
gradient	O
is	O
wk	O
jn	O
w	O
nkzn	O
which	O
is	O
the	O
pre-synaptic	O
input	O
to	O
the	O
output	O
layer	O
namely	O
zn	O
times	O
the	O
error	B
signal	I
namely	O
w	O
nk	O
for	O
the	O
input	O
layer	O
weights	O
we	O
have	O
vj	O
jn	O
vj	O
anj	O
v	O
njxn	O
jn	O
anj	O
j	O
xn	O
all	O
that	O
remains	O
is	O
to	O
compute	O
the	O
first	O
level	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
anj	O
vt	O
error	B
signal	I
v	O
nj	O
we	O
have	O
v	O
nj	O
jn	O
anj	O
jn	O
bnk	O
bnk	O
anj	O
j	O
now	O
bnk	O
so	O
wkjganj	O
bnk	O
anj	O
da	O
ga	O
for	O
tanh	O
units	O
where	O
for	O
sigmoid	B
units	O
d	O
d	O
v	O
nj	O
w	O
w	O
nk	O
bnk	O
anj	O
da	O
tanha	O
and	O
d	O
da	O
hence	O
chapter	O
adaptive	O
basis	O
function	O
models	O
thus	O
the	O
layer	O
errors	O
can	O
be	O
computed	O
by	O
passing	O
the	O
layer	O
errors	O
back	O
through	O
the	O
w	O
matrix	O
hence	O
the	O
term	O
backpropagation	B
the	O
key	O
property	O
is	O
that	O
we	O
can	O
compute	O
the	O
gradients	O
locally	O
each	O
node	O
only	O
needs	O
to	O
know	O
about	O
its	O
immediate	O
neighbors	B
this	O
is	O
supposed	O
to	O
make	O
the	O
algorithm	O
neurally	O
plausible	O
although	O
this	O
interpretation	O
is	O
somewhat	O
controversial	O
putting	O
it	O
all	O
together	O
we	O
can	O
compute	O
all	O
the	O
gradients	O
as	O
follows	O
we	O
first	O
perform	O
a	O
forwards	O
pass	O
to	O
compute	O
an	O
zn	O
bn	O
and	O
yn	O
we	O
then	O
compute	O
the	O
error	O
for	O
the	O
output	O
layer	O
n	O
yn	O
yn	O
which	O
we	O
pass	O
backwards	O
through	O
w	O
using	O
equation	O
to	O
compute	O
the	O
error	O
for	O
the	O
hidden	B
layer	I
n	O
we	O
then	O
compute	O
the	O
overall	O
gradient	O
as	O
follows	O
j	O
ij	O
jk	O
v	O
nxn	O
w	O
n	O
zn	O
n	O
identifiability	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
parameters	O
of	O
a	O
neural	B
network	I
are	O
not	O
identifiable	O
for	O
example	O
we	O
can	O
change	O
the	O
sign	O
of	O
the	O
weights	O
going	O
into	O
one	O
of	O
the	O
hidden	B
units	I
so	O
long	O
as	O
we	O
change	O
the	O
sign	O
of	O
all	O
the	O
weights	O
going	O
out	O
of	O
it	O
these	O
effects	O
cancel	O
since	O
tanh	O
is	O
an	O
odd	O
function	O
so	O
tanh	O
a	O
tanha	O
there	O
will	O
be	O
h	O
such	O
sign	O
flip	O
symmetries	O
leading	O
to	O
equivalent	O
settings	O
of	O
the	O
parameters	O
similarly	O
we	O
can	O
change	O
the	O
identity	O
of	O
the	O
hidden	B
units	I
without	O
affecting	O
the	O
likelihood	B
there	O
are	O
h	O
such	O
permutations	O
the	O
total	O
number	O
of	O
equivalent	O
parameter	B
settings	O
the	O
same	O
likelihood	B
is	O
therefore	O
in	O
addition	O
there	O
may	O
be	O
local	O
minima	O
due	O
to	O
the	O
non-convexity	O
of	O
the	O
nll	B
this	O
can	O
be	O
a	O
more	O
serious	O
problem	O
although	O
with	O
enough	O
data	O
these	O
local	O
optima	O
are	O
often	O
quite	O
shallow	O
and	O
simple	O
stochastic	B
optimization	B
methods	O
can	O
avoid	O
them	O
in	O
addition	O
it	O
is	O
common	O
to	O
perform	O
multiple	B
restarts	I
and	O
to	O
pick	O
the	O
best	O
solution	O
or	O
to	O
average	O
over	O
the	O
resulting	O
predictions	O
does	O
not	O
make	O
sense	O
to	O
average	O
the	O
parameters	O
themselves	O
since	O
they	O
are	O
not	O
identifiable	O
regularization	B
as	O
usual	O
the	O
mle	B
can	O
overfit	O
especially	O
if	O
the	O
number	O
of	O
nodes	B
is	O
large	O
a	O
simple	O
way	O
to	O
prevent	O
this	O
is	O
called	O
early	B
stopping	I
which	O
means	O
stopping	O
the	O
training	O
procedure	O
when	O
the	O
error	O
on	O
the	O
validation	B
set	I
first	O
starts	O
to	O
increase	O
this	O
method	O
works	O
because	O
we	O
usually	O
initialize	O
from	O
small	O
random	O
weights	O
so	O
the	O
model	O
is	O
initially	O
simple	O
the	O
tanh	O
and	O
sigm	O
functions	O
are	O
nearly	O
linear	O
near	O
the	O
origin	O
as	O
training	O
progresses	O
the	O
weights	O
become	O
larger	O
and	O
the	O
model	O
becomes	O
nonlinear	O
eventually	O
it	O
will	O
overfit	O
another	O
way	O
to	O
prevent	O
overfitting	O
that	O
is	O
more	O
in	O
keeping	O
with	O
the	O
approaches	O
used	O
elsewhere	O
in	O
this	O
book	O
is	O
to	O
impose	O
a	O
prior	O
on	O
the	O
parameters	O
and	O
then	O
use	O
map	O
estimation	O
it	O
is	O
standard	O
to	O
use	O
a	O
n	O
prior	O
to	O
regularization	B
where	O
is	O
the	O
precision	B
of	O
the	O
prior	O
in	O
the	O
neural	B
networks	I
literature	O
this	O
is	O
called	O
weight	B
decay	I
since	O
it	O
encourages	O
small	O
weights	O
and	O
hence	O
simpler	O
models	O
the	O
penalized	O
nll	B
objective	O
becomes	O
j	O
log	O
pynxn	O
ij	O
jk	O
feedforward	O
neural	B
networks	I
perceptrons	O
j	O
that	O
we	O
don	O
t	O
penalize	O
the	O
bias	B
terms	O
the	O
gradient	O
of	O
the	O
modified	O
objective	O
becomes	O
v	O
nxn	O
v	O
w	O
n	O
zn	O
w	O
n	O
n	O
as	O
in	O
section	O
if	O
the	O
regularization	B
is	O
sufficiently	O
strong	O
it	O
does	O
not	O
matter	O
if	O
we	O
have	O
too	O
many	O
hidden	B
units	I
from	O
wasted	O
computation	O
hence	O
it	O
is	O
advisable	O
to	O
set	O
h	O
to	O
be	O
as	O
large	O
as	O
you	O
can	O
afford	O
and	O
then	O
to	O
choose	O
an	O
appropriate	O
regularizer	O
we	O
can	O
set	O
the	O
parameter	B
by	O
cross	B
validation	I
or	O
empirical	B
bayes	I
section	O
as	O
with	O
ridge	B
regression	B
it	O
is	O
good	O
practice	O
to	O
standardize	O
the	O
inputs	O
to	O
zero	O
mean	B
and	O
unit	O
variance	B
so	O
that	O
the	O
spherical	B
gaussian	B
prior	O
makes	O
sense	O
consistent	B
gaussian	B
priors	O
one	O
can	O
show	O
that	O
using	O
the	O
same	O
regularization	B
parameter	B
for	O
both	O
the	O
first	O
and	O
second	O
layer	O
weights	O
results	O
in	O
the	O
lack	O
of	O
a	O
certain	O
desirable	O
invariance	O
property	O
in	O
particular	O
suppose	O
we	O
linearly	O
scale	O
and	O
shift	O
the	O
inputs	O
andor	O
outputs	O
to	O
a	O
neural	B
network	I
regression	B
model	O
then	O
we	O
would	O
like	O
the	O
model	O
to	O
learn	O
to	O
predict	O
the	O
same	O
function	O
by	O
suitably	O
scaling	O
its	O
internal	O
weights	O
and	O
bias	B
terms	O
however	O
the	O
amount	O
of	O
scaling	O
needed	O
by	O
the	O
first	O
and	O
second	O
layer	O
weights	O
to	O
compensate	O
for	O
a	O
change	O
in	O
the	O
inputs	O
andor	O
outputs	O
is	O
not	O
the	O
same	O
therefore	O
we	O
need	O
to	O
use	O
a	O
different	O
regularization	B
strength	O
for	O
the	O
first	O
and	O
second	O
layer	O
fortunately	O
this	O
is	O
easy	O
to	O
do	O
we	O
just	O
use	O
the	O
following	O
prior	O
in	O
c	O
p	O
n	O
w	O
in	O
v	O
in	O
b	O
i	O
where	O
b	O
and	O
c	O
are	O
the	O
bias	B
to	O
get	O
a	O
feeling	O
for	O
the	O
effect	O
of	O
these	O
hyper-parameters	B
we	O
can	O
sample	O
mlp	B
parameters	O
from	O
this	O
prior	O
and	O
plot	O
the	O
resulting	O
random	O
functions	O
figure	O
shows	O
some	O
examples	O
decreasing	O
v	O
allows	O
the	O
first	O
layer	O
weights	O
to	O
get	O
bigger	O
making	O
the	O
sigmoid-like	O
shape	O
of	O
the	O
functions	O
steeper	O
decreasing	O
b	O
allows	O
the	O
first	O
layer	O
biases	O
to	O
get	O
bigger	O
which	O
allows	O
the	O
center	O
of	O
the	O
sigmoid	B
to	O
shift	O
left	O
and	O
right	O
more	O
decreasing	O
w	O
allows	O
the	O
second	O
layer	O
weights	O
to	O
get	O
bigger	O
making	O
the	O
functions	O
more	O
wiggly	O
sensitivity	B
to	O
change	O
in	O
the	O
input	O
and	O
hence	O
larger	O
dynamic	O
range	O
and	O
decreasing	O
c	O
allows	O
the	O
second	O
layer	O
biases	O
to	O
get	O
bigger	O
allowing	O
the	O
mean	B
level	O
of	O
the	O
function	O
to	O
move	O
up	O
and	O
down	O
more	O
chapter	O
we	O
will	O
see	O
an	O
easier	O
way	O
to	O
define	O
priors	O
over	O
functions	O
weight	O
pruning	B
since	O
there	O
are	O
many	O
weights	O
in	O
a	O
neural	B
network	I
it	O
is	O
often	O
helpful	O
to	O
encourage	O
sparsity	B
various	O
ad-hoc	O
methods	O
for	O
doing	O
this	O
with	O
names	O
such	O
as	O
optimal	O
brain	O
damage	O
were	O
devised	O
in	O
the	O
see	O
e	O
g	O
for	O
details	O
since	O
we	O
are	O
regularizing	O
the	O
output	O
bias	B
terms	O
it	O
is	O
helpful	O
in	O
the	O
case	O
of	O
regression	B
to	O
normalize	O
the	O
target	O
responses	O
in	O
the	O
training	B
set	I
to	O
zero	O
mean	B
to	O
be	O
consistent	B
with	O
the	O
fact	O
that	O
the	O
prior	O
on	O
the	O
output	O
bias	B
has	O
zero	O
mean	B
chapter	O
adaptive	O
basis	O
function	O
models	O
figure	O
the	O
effects	O
of	O
changing	O
the	O
hyper-parameters	B
on	O
an	O
mlp	B
v	O
b	O
w	O
c	O
factor	B
of	O
mlppriorsdemo	O
default	O
parameter	B
values	O
decreasing	O
b	O
by	O
decreasing	O
c	O
by	O
factor	B
of	O
figure	O
generated	O
by	O
decreasing	O
w	O
by	O
factor	B
of	O
decreasing	O
v	O
by	O
factor	B
of	O
feedforward	O
neural	B
networks	I
perceptrons	O
neural	B
network	I
y	O
data	O
deep	B
neural	O
net	O
figure	O
a	O
deep	B
but	O
sparse	B
neural	B
network	I
the	O
connections	O
are	O
pruned	O
using	O
regularization	B
at	O
each	O
level	O
nodes	B
numbered	O
are	O
clamped	O
to	O
so	O
their	O
outgoing	O
weights	O
correspond	O
to	O
the	O
offsetbias	O
predictions	O
made	O
by	O
the	O
model	O
on	O
the	O
training	B
set	I
figure	O
generated	O
by	O
sparsennetdemo	O
terms	O
written	O
by	O
mark	O
schmidt	O
however	O
we	O
can	O
also	O
use	O
the	O
more	O
principled	O
sparsity-promoting	O
techniques	O
we	O
discussed	O
in	O
chapter	O
one	O
approach	O
is	O
to	O
use	O
an	O
regularizer	O
see	O
figure	O
for	O
an	O
example	O
another	O
approach	O
is	O
to	O
use	O
ard	B
this	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
soft	B
weight	I
sharing	I
another	O
way	O
to	O
regularize	O
the	O
parameters	O
is	O
to	O
encourage	O
similar	B
weights	O
to	O
share	O
statistical	O
strength	O
but	O
how	O
do	O
we	O
know	O
which	O
parameters	O
to	O
group	O
together	O
we	O
can	O
learn	O
this	O
by	O
using	O
a	O
mixture	B
model	I
that	O
is	O
we	O
model	O
p	O
as	O
a	O
mixture	B
of	I
gaussians	I
parameters	O
that	O
are	O
assigned	O
to	O
the	O
same	O
cluster	O
will	O
share	O
the	O
same	O
mean	B
and	O
variance	B
and	O
thus	O
will	O
have	O
similar	B
values	O
the	O
variance	B
for	O
that	O
cluster	O
is	O
low	O
this	O
is	O
called	O
soft	B
weight	I
sharing	I
and	O
hinton	O
in	O
practice	O
this	O
technique	O
is	O
not	O
widely	O
used	O
see	O
e	O
g	O
if	O
you	O
want	O
to	O
know	O
the	O
details	O
semi-supervised	B
embedding	B
an	O
interesting	O
way	O
to	O
regularize	O
deep	B
feedforward	O
neural	B
networks	I
is	O
to	O
encourage	O
the	O
hidden	B
layers	O
to	O
assign	O
similar	B
objects	O
to	O
similar	B
representations	O
this	O
is	O
useful	O
because	O
it	O
is	O
often	O
easy	O
to	O
obtain	O
side	B
information	B
consisting	O
of	O
sets	O
of	O
pairs	O
of	O
similar	B
and	O
dissimilar	O
objects	O
for	O
example	O
in	O
a	O
video	O
classification	O
task	O
neighboring	O
frames	O
can	O
be	O
deemed	O
similar	B
but	O
frames	O
that	O
are	O
distant	O
in	O
time	O
can	O
be	O
deemed	O
dis-similar	O
et	O
al	O
note	O
that	O
this	O
can	O
be	O
done	O
without	O
collecting	O
any	O
labels	O
let	O
sij	O
if	O
examples	O
i	O
and	O
j	O
are	O
similar	B
and	O
sij	O
otherwise	O
let	O
f	O
be	O
some	O
embedding	B
of	O
item	O
xi	O
e	O
g	O
f	O
zxi	O
where	O
z	O
is	O
the	O
hidden	B
layer	I
of	O
a	O
neural	B
network	I
now	O
define	O
a	O
loss	B
function	I
lf	O
f	O
sij	O
that	O
depends	O
on	O
the	O
embedding	B
of	O
two	O
objects	O
chapter	O
adaptive	O
basis	O
function	O
models	O
and	O
the	O
observed	O
similarity	O
measure	O
for	O
example	O
we	O
might	O
want	O
to	O
force	O
similar	B
objects	O
to	O
have	O
similar	B
embeddings	O
and	O
to	O
force	O
the	O
embeddings	O
of	O
dissimilar	O
objects	O
to	O
be	O
a	O
minimal	B
distance	O
apart	O
lfi	O
fj	O
sij	O
m	O
if	O
sij	O
if	O
sij	O
ij	O
u	O
where	O
m	O
is	O
some	O
minimal	B
margin	B
we	O
can	O
now	O
define	O
an	O
augmented	O
loss	B
function	I
for	O
training	O
the	O
neural	B
network	I
nllf	O
yi	O
lf	O
f	O
sij	O
i	O
l	O
where	O
l	O
is	O
the	O
labeled	O
training	B
set	I
u	O
is	O
the	O
unlabeled	O
training	B
set	I
and	O
is	O
some	O
tradeoff	O
parameter	B
this	O
is	O
called	O
semi-supervised	B
embedding	B
et	O
al	O
such	O
an	O
objective	O
can	O
be	O
easily	O
optimized	O
by	O
stochastic	B
gradient	B
descent	I
at	O
each	O
iteration	O
pick	O
a	O
random	O
labeled	O
training	O
example	O
yn	O
and	O
take	O
a	O
gradient	O
step	O
to	O
optimize	O
nllf	O
yi	O
then	O
pick	O
a	O
random	O
pair	O
of	O
similar	B
unlabeled	O
examples	O
xi	O
xj	O
can	O
sometimes	O
be	O
generated	O
on	O
the	O
fly	O
rather	O
than	O
stored	O
in	O
advance	O
and	O
make	O
a	O
gradient	O
step	O
to	O
optimize	O
lf	O
f	O
finally	O
pick	O
a	O
random	O
unlabeled	O
example	O
xk	O
which	O
with	O
high	O
probability	O
is	O
dissimilar	O
to	O
xi	O
and	O
make	O
a	O
gradient	O
step	O
to	O
optimize	O
lf	O
f	O
note	O
that	O
this	O
technique	O
is	O
effective	O
because	O
it	O
can	O
leverage	O
massive	O
amounts	O
of	O
data	O
in	O
a	O
related	O
approach	O
and	O
weston	O
trained	O
a	O
neural	B
network	I
to	O
distinguish	O
valid	O
english	O
sentences	O
from	O
invalid	O
ones	O
this	O
was	O
done	O
by	O
taking	O
all	O
million	O
words	O
from	O
english	O
wikipedia	O
and	O
then	O
creating	O
windows	O
of	O
length	O
containing	O
neighboring	O
words	O
this	O
constitutes	O
the	O
positive	B
examples	I
to	O
create	O
negative	B
examples	I
the	O
middle	O
word	O
of	O
each	O
window	O
was	O
replaced	O
by	O
a	O
random	O
english	O
word	O
is	O
likely	O
to	O
be	O
an	O
invalid	O
sentence	O
either	O
grammatically	O
andor	O
semantically	O
with	O
high	O
probability	O
this	O
neural	B
network	I
was	O
then	O
trained	O
over	O
the	O
course	O
of	O
week	O
and	O
its	O
latent	B
representation	O
was	O
then	O
used	O
as	O
the	O
input	O
to	O
a	O
supervised	O
semantic	B
role	I
labeling	I
task	O
for	O
which	O
very	O
little	O
labeled	O
training	O
data	O
is	O
available	O
also	O
and	O
zhang	O
for	O
related	O
work	O
bayesian	B
inference	B
although	O
map	O
estimation	O
is	O
a	O
succesful	O
way	O
to	O
reduce	O
overfitting	O
there	O
are	O
still	O
some	O
good	O
reasons	O
to	O
want	O
to	O
adopt	O
a	O
fully	O
bayesian	B
approach	O
to	O
fitting	O
neural	B
networks	I
integrating	O
out	O
the	O
parameters	O
instead	O
of	O
optimizing	O
them	O
is	O
a	O
much	O
stronger	O
form	O
of	O
regularization	B
than	O
map	O
estimation	O
we	O
can	O
use	O
bayesian	B
model	B
selection	I
to	O
determine	O
things	O
like	O
the	O
hyper-parameter	O
settings	O
and	O
the	O
number	O
of	O
hidden	B
units	I
this	O
is	O
likely	O
to	O
be	O
much	O
faster	O
than	O
cross	B
validation	I
especially	O
if	O
we	O
have	O
many	O
hyper-parameters	B
as	O
in	O
ard	B
modelling	O
uncertainty	B
in	O
the	O
parameters	O
will	O
induce	O
uncertainty	B
in	O
our	O
predictive	B
distributions	O
which	O
is	O
important	O
for	O
certain	O
problems	O
such	O
as	O
active	B
learning	B
and	O
risk-averse	O
decision	B
making	O
feedforward	O
neural	B
networks	I
perceptrons	O
we	O
can	O
use	O
online	O
inference	B
methods	O
such	O
as	O
the	O
extended	O
kalman	O
filter	O
to	O
do	O
online	B
learning	B
one	O
can	O
adopt	O
a	O
variety	O
of	O
approximate	O
bayesian	B
inference	B
techniques	O
in	O
this	O
context	O
in	O
this	O
section	O
we	O
discuss	O
the	O
laplace	B
approximation	I
first	O
suggested	O
in	O
one	O
can	O
also	O
use	O
hybrid	B
monte	B
carlo	I
or	O
variational	B
bayes	I
and	O
camp	O
barber	O
and	O
bishop	O
parameter	B
posterior	O
for	O
regression	B
we	O
start	O
by	O
considering	O
regression	B
following	O
the	O
presentation	O
of	O
sec	O
which	O
summarizes	O
the	O
work	O
of	O
we	O
will	O
use	O
a	O
prior	O
of	O
the	O
form	O
pw	O
n	O
where	O
w	O
represents	O
all	O
the	O
weights	O
combined	O
we	O
will	O
denote	O
the	O
precision	B
of	O
the	O
noise	O
by	O
the	O
posterior	O
can	O
be	O
approximated	O
as	O
follows	O
pwd	O
exp	O
ew	O
ew	O
edw	O
e	O
w	O
f	O
edw	O
ew	O
wt	O
w	O
where	O
ed	O
is	O
the	O
data	O
error	O
ew	O
is	O
the	O
prior	O
error	O
and	O
e	O
is	O
the	O
overall	O
error	O
log	O
prior	O
plus	O
log	O
likelihood	B
now	O
let	O
us	O
make	O
a	O
second-order	O
taylor	B
series	I
approximation	O
of	O
ew	O
around	O
its	O
minimum	O
map	B
estimate	I
ew	O
ewm	O
p	O
wm	O
p	O
aw	O
wm	O
p	O
where	O
a	O
is	O
the	O
hessian	B
of	O
e	O
a	O
ewm	O
p	O
h	O
i	O
where	O
h	O
edwm	O
p	O
is	O
the	O
hessian	B
of	O
the	O
data	O
error	O
this	O
can	O
be	O
computed	O
exactly	O
in	O
time	O
where	O
d	O
is	O
the	O
number	O
of	O
parameters	O
using	O
a	O
variant	O
of	O
backpropagation	B
if	O
we	O
use	O
a	O
quasi-newton	B
method	O
to	O
find	O
sec	O
for	O
details	O
alternatively	O
the	O
mode	B
we	O
can	O
use	O
its	O
internally	O
computed	O
approximation	O
to	O
h	O
that	O
diagonal	B
approximations	O
of	O
h	O
are	O
usually	O
very	O
inaccurate	O
in	O
either	O
case	O
using	O
this	O
quadratic	O
approximation	O
the	O
posterior	O
becomes	O
gaussian	B
pw	O
n	O
p	O
a	O
chapter	O
adaptive	O
basis	O
function	O
models	O
parameter	B
posterior	O
for	O
classification	O
the	O
classification	O
case	O
is	O
the	O
same	O
as	O
the	O
regression	B
case	O
except	O
and	O
ed	O
is	O
a	O
crossentropy	O
error	O
of	O
the	O
form	O
edw	O
ln	O
f	O
w	O
yn	O
ln	O
f	O
w	O
predictive	B
posterior	O
for	O
regression	B
the	O
posterior	B
predictive	B
density	I
is	O
given	O
by	O
pyxd	O
n	O
w	O
p	O
a	O
this	O
is	O
not	O
analytically	O
tractable	O
because	O
of	O
the	O
nonlinearity	O
of	O
f	O
w	O
let	O
us	O
therefore	O
construct	O
a	O
first-order	O
taylor	B
series	I
approximation	O
around	O
the	O
mode	B
f	O
w	O
f	O
wm	O
p	O
t	O
wm	O
p	O
where	O
g	O
wf	O
wwwm	O
p	O
we	O
now	O
have	O
a	O
linear-gaussian	O
model	O
with	O
a	O
gaussian	B
prior	O
on	O
the	O
weights	O
from	O
equation	O
we	O
have	O
pyxd	O
n	O
wm	O
p	O
where	O
the	O
predictive	B
variance	B
depends	O
on	O
the	O
input	O
x	O
as	O
follows	O
gt	O
a	O
the	O
error	O
bars	O
will	O
be	O
larger	O
in	O
regions	O
of	O
input	O
space	O
where	O
we	O
have	O
little	O
training	O
data	O
see	O
figure	O
for	O
an	O
example	O
predictive	B
posterior	O
for	O
classification	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
approximate	O
pyxd	O
in	O
the	O
case	O
of	O
binary	O
classification	O
the	O
situation	O
is	O
similar	B
to	O
the	O
case	O
of	O
logistic	B
regression	B
discussed	O
in	O
section	O
except	O
in	O
addition	O
the	O
posterior	O
predictive	B
mean	B
is	O
a	O
non-linear	O
function	O
of	O
w	O
specifically	O
we	O
have	O
e	O
w	O
sigmax	O
w	O
where	O
ax	O
w	O
is	O
the	O
pre-synaptic	O
output	O
of	O
the	O
final	O
layer	O
let	O
us	O
make	O
a	O
linear	O
approximation	O
to	O
this	O
ax	O
w	O
am	O
p	O
t	O
wm	O
p	O
where	O
am	O
p	O
x	O
wm	O
p	O
and	O
g	O
xax	O
wm	O
p	O
can	O
be	O
found	O
by	O
a	O
modified	O
version	O
of	O
backpropagation	B
clearly	O
paxd	O
n	O
wm	O
p	O
gxt	O
a	O
feedforward	O
neural	B
networks	I
perceptrons	O
t	O
e	O
g	O
r	O
a	O
t	O
data	O
function	O
network	O
error	O
bars	O
input	O
data	O
function	O
prediction	O
samples	B
figure	O
the	O
posterior	B
predictive	B
density	I
for	O
an	O
mlp	B
with	O
hidden	B
nodes	B
trained	O
on	O
data	O
points	O
the	O
dashed	O
green	O
line	O
is	O
the	O
true	O
function	O
result	O
of	O
using	O
a	O
laplace	B
approximation	I
after	O
performing	O
empirical	B
bayes	I
to	O
optimize	O
the	O
hyperparameters	O
the	O
solid	O
red	O
line	O
is	O
the	O
posterior	B
mean	B
prediction	O
and	O
the	O
dotted	O
blue	O
lines	O
are	O
standard	B
deviation	I
above	O
and	O
below	O
the	O
mean	B
figure	O
generated	O
by	O
mlpregevidencedemo	O
result	O
of	O
using	O
hybrid	B
monte	B
carlo	I
using	O
the	O
same	O
trained	O
hyperparameters	O
as	O
in	O
the	O
solid	O
red	O
line	O
is	O
the	O
posterior	B
mean	B
prediction	O
and	O
the	O
dotted	O
blue	O
lines	O
are	O
samples	B
from	O
the	O
posterior	O
predictive	B
figure	O
generated	O
by	O
mlpreghmcdemo	O
written	O
by	O
ian	O
nabney	O
hence	O
the	O
posterior	O
predictive	B
for	O
the	O
output	O
is	O
py	O
sigmapaxdda	O
sigm	O
abt	O
wm	O
p	O
where	O
is	O
defined	O
by	O
equation	O
which	O
we	O
repeat	O
here	O
for	O
convenience	O
of	O
course	O
a	O
simpler	O
potentially	O
more	O
accurate	O
alternative	O
to	O
this	O
is	O
to	O
draw	O
a	O
few	O
samples	B
from	O
the	O
gaussian	B
posterior	O
and	O
to	O
approximate	O
the	O
posterior	O
predictive	B
using	O
monte	B
carlo	I
in	O
either	O
case	O
the	O
effect	O
of	O
taking	O
uncertainty	B
of	O
the	O
parameters	O
into	O
account	O
as	O
in	O
section	O
is	O
to	O
moderate	O
the	O
confidence	O
of	O
the	O
output	O
the	O
decision	B
boundary	I
itself	O
is	O
unaffected	O
however	O
ard	B
for	O
neural	B
networks	I
once	O
we	O
have	O
made	O
the	O
laplace	B
approximation	I
to	O
the	O
posterior	O
we	O
can	O
optimize	O
the	O
marginal	B
likelihood	B
wrt	O
the	O
hyper-parameters	B
using	O
the	O
same	O
fixed-point	O
equations	O
as	O
in	O
section	O
typically	O
we	O
use	O
one	O
hyper-parameter	O
for	O
the	O
weight	B
vector	I
leaving	O
each	O
node	O
to	O
achieve	O
an	O
effect	O
similar	B
to	O
group	B
lasso	B
that	O
is	O
the	O
prior	O
has	O
the	O
form	O
n	O
vi	O
i	O
n	O
wj	O
i	O
if	O
we	O
find	O
vi	O
then	O
input	O
feature	O
i	O
is	O
irrelevant	O
and	O
its	O
weight	B
vector	I
vi	O
is	O
pruned	O
out	O
similarly	O
if	O
we	O
find	O
wj	O
then	O
hidden	B
feature	O
j	O
is	O
irrelevant	O
this	O
is	O
known	O
as	O
automatic	O
p	O
chapter	O
adaptive	O
basis	O
function	O
models	O
relevancy	O
determination	O
or	O
ard	B
which	O
was	O
discussed	O
in	O
detail	O
in	O
section	O
applying	O
this	O
to	O
neural	B
networks	I
gives	O
us	O
an	O
efficient	O
means	O
of	O
variable	O
selection	O
in	O
non-linear	O
models	O
the	O
software	O
package	O
netlab	O
contains	O
a	O
simple	O
example	O
of	O
ard	B
applied	O
to	O
a	O
neural	B
network	I
called	O
demard	O
this	O
demo	O
creates	O
some	O
data	O
according	O
to	O
a	O
nonlinear	O
regression	B
function	O
f	O
where	O
is	O
a	O
noisy	O
copy	O
of	O
we	O
see	O
that	O
and	O
are	O
irrelevant	O
for	O
predicting	O
the	O
target	O
however	O
is	O
correlated	O
with	O
which	O
is	O
relevant	O
using	O
ard	B
the	O
final	O
hyper-parameters	B
are	O
as	O
follows	O
this	O
clearly	O
indicates	O
that	O
feature	O
is	O
irrelevant	O
feature	O
is	O
only	O
weakly	O
relevant	O
and	O
feature	O
is	O
very	O
relevant	O
ensemble	B
learning	B
m	O
m	O
ensemble	B
learning	B
refers	O
to	O
learning	B
a	O
weighted	O
combination	O
of	O
base	O
models	O
of	O
the	O
form	O
f	O
wmfmyx	O
where	O
the	O
wm	O
are	O
tunable	O
parameters	O
ensemble	B
learning	B
is	O
sometimes	O
called	O
a	O
committee	B
method	I
since	O
each	O
base	O
model	O
fm	O
gets	O
a	O
weighted	O
vote	O
clearly	O
ensemble	B
learning	B
is	O
closely	O
related	O
to	O
learning	B
adaptive-basis	O
function	O
models	O
in	O
fact	O
one	O
can	O
argue	O
that	O
a	O
neural	O
net	O
is	O
an	O
ensemble	B
method	O
where	O
fm	O
represents	O
the	O
m	O
th	O
hidden	B
unit	O
and	O
wm	O
are	O
the	O
output	O
layer	O
weights	O
also	O
we	O
can	O
think	O
of	O
boosting	B
as	O
kind	O
of	O
ensemble	B
learning	B
where	O
the	O
weights	O
on	O
the	O
base	O
models	O
are	O
determined	O
sequentially	O
below	O
we	O
describe	O
some	O
other	O
forms	O
of	O
ensemble	B
learning	B
stacking	B
an	O
obvious	O
way	O
to	O
estimate	O
the	O
weights	O
in	O
equation	O
is	O
to	O
use	O
w	O
argmin	O
w	O
lyi	O
wmfmx	O
however	O
this	O
will	O
result	O
in	O
overfitting	O
with	O
wm	O
being	O
large	O
for	O
the	O
most	O
complex	O
model	O
a	O
simple	O
solution	O
to	O
this	O
is	O
to	O
use	O
cross-validation	O
in	O
particular	O
we	O
can	O
use	O
the	O
loocv	B
estimate	O
w	O
argmin	O
w	O
lyi	O
wm	O
f	O
i	O
m	O
where	O
f	O
i	O
m	O
is	O
the	O
predictor	O
obtained	O
by	O
training	O
on	O
data	O
excluding	O
yi	O
this	O
is	O
known	O
as	O
stacking	B
which	O
stands	O
for	O
stacked	O
generalization	B
this	O
technique	O
is	O
more	O
robust	B
to	O
the	O
case	O
where	O
the	O
true	O
model	O
is	O
not	O
in	O
the	O
model	O
class	O
than	O
standard	O
bma	O
this	O
approach	O
was	O
used	O
by	O
the	O
netflix	O
team	O
known	O
as	O
the	O
ensemble	B
which	O
tied	B
the	O
submission	O
of	O
the	O
winning	O
team	O
s	O
pragmatic	O
chaos	O
in	O
terms	O
of	O
accuracy	O
et	O
al	O
stacking	B
has	O
also	O
been	O
used	O
for	O
problems	O
such	O
as	O
image	B
segmentation	I
and	O
labeling	O
ensemble	B
learning	B
class	O
table	O
part	O
of	O
a	O
error-correcting	O
output	O
code	O
for	O
a	O
problem	O
each	O
row	O
defines	O
a	O
two-class	O
problem	O
based	O
on	O
table	O
of	O
et	O
al	O
error-correcting	B
output	I
codes	I
an	O
interesting	O
form	O
of	O
ensemble	B
learning	B
is	O
known	O
as	O
error-correcting	B
output	I
codes	I
or	O
ecoc	B
and	O
bakiri	O
which	O
can	O
be	O
used	O
in	O
the	O
context	O
of	O
multi-class	O
classification	O
the	O
idea	O
is	O
that	O
we	O
are	O
trying	O
to	O
decode	O
a	O
symbol	O
the	O
class	O
label	B
which	O
has	O
c	O
possible	O
states	O
we	O
could	O
use	O
a	O
bit	O
vector	O
of	O
length	O
b	O
to	O
encode	O
the	O
class	O
label	B
and	O
train	O
b	O
separate	O
binary	O
classifiers	O
to	O
predict	O
each	O
bit	O
however	O
by	O
using	O
more	O
bits	B
and	O
by	O
designing	O
the	O
codewords	O
to	O
have	O
maximal	O
hamming	B
distance	I
from	O
each	O
other	O
we	O
get	O
a	O
method	O
that	O
is	O
more	O
resistant	O
to	O
individual	O
bit-flipping	O
errors	O
for	O
example	O
in	O
table	O
we	O
use	O
b	O
bits	B
to	O
encode	O
a	O
c	O
class	O
problem	O
the	O
minimum	O
hamming	B
distance	I
between	O
any	O
pair	O
of	O
rows	O
is	O
the	O
decoding	B
rule	O
is	O
pbx	O
cx	O
min	O
c	O
where	O
ccb	O
is	O
the	O
b	O
th	O
bit	O
of	O
the	O
codeword	O
for	O
class	O
c	O
and	O
hastie	O
showed	O
that	O
a	O
random	O
code	O
worked	O
just	O
as	O
well	O
as	O
the	O
optimal	O
code	O
both	O
methods	O
work	O
by	O
averaging	O
the	O
results	O
of	O
multiple	O
classifiers	O
thereby	O
reducing	O
variance	B
ensemble	B
learning	B
is	O
not	O
equivalent	O
to	O
bayes	B
model	I
averaging	I
in	O
section	O
we	O
discussed	O
bayesian	B
model	B
selection	I
an	O
alternative	O
to	O
picking	O
the	O
best	O
model	O
and	O
then	O
using	O
this	O
to	O
make	O
predictions	O
is	O
to	O
make	O
a	O
weighted	B
average	I
of	O
the	O
predictions	O
made	O
by	O
each	O
model	O
i	O
e	O
we	O
compute	O
pyxd	O
pyx	O
md	O
m	O
m	O
this	O
is	O
called	O
bayes	B
model	I
averaging	I
and	O
can	O
sometimes	O
give	O
better	O
performance	O
than	O
using	O
any	O
single	O
model	O
et	O
al	O
of	O
course	O
averaging	O
over	O
all	O
models	O
is	O
typically	O
computationally	O
infeasible	O
integration	O
is	O
obviously	O
not	O
possible	O
in	O
a	O
discrete	B
space	O
although	O
one	O
can	O
sometimes	O
use	O
dynamic	B
programming	I
to	O
perform	O
the	O
computation	O
exactly	O
e	O
g	O
and	O
jaakkola	O
a	O
simple	O
approximation	O
is	O
to	O
sample	O
a	O
few	O
models	O
from	O
the	O
posterior	O
an	O
even	O
simpler	O
approximation	O
the	O
one	O
most	O
widely	O
used	O
in	O
practice	O
is	O
to	O
just	O
use	O
the	O
map	O
model	O
it	O
is	O
important	O
to	O
note	O
that	O
bma	O
is	O
not	O
equivalent	O
to	O
ensemble	B
learning	B
this	O
latter	O
technique	O
corresponds	O
to	O
enlarging	O
the	O
model	O
space	O
by	O
defining	O
a	O
single	O
new	O
model	O
chapter	O
adaptive	O
basis	O
function	O
models	O
model	O
bst-dt	O
rf	O
bag-dt	O
svm	B
ann	O
knn	B
bst-stmp	O
dt	O
logreg	O
nb	O
table	O
fraction	O
of	O
time	O
each	O
method	O
achieved	O
a	O
specified	O
rank	O
when	O
sorting	O
by	O
mean	B
performance	O
across	O
datasets	O
and	O
metrics	O
based	O
on	O
table	O
of	O
and	O
niculescu-mizil	O
used	O
with	O
kind	O
permission	O
of	O
alexandru	O
niculescu-mizil	O
which	O
is	O
a	O
convex	B
combination	I
of	O
base	O
models	O
as	O
follows	O
pyx	O
mpyx	O
m	O
m	O
m	O
in	O
principle	O
we	O
can	O
now	O
perform	O
bayesian	B
inference	B
to	O
compute	O
p	O
we	O
then	O
make	O
predictions	O
using	O
pyxd	O
pyx	O
however	O
it	O
is	O
much	O
more	O
common	O
to	O
use	O
point	O
estimation	O
methods	O
for	O
as	O
we	O
saw	O
above	O
experimental	O
comparison	O
we	O
have	O
described	O
many	O
different	O
methods	O
for	O
classification	O
and	O
regression	B
which	O
one	O
should	O
you	O
use	O
that	O
depends	O
on	O
which	O
inductive	B
bias	B
you	O
think	O
is	O
most	O
appropriate	O
for	O
your	O
domain	O
usually	O
this	O
is	O
hard	O
to	O
assess	O
so	O
it	O
is	O
common	O
to	O
just	O
try	O
several	O
different	O
methods	O
and	O
see	O
how	O
they	O
perform	O
empirically	O
below	O
we	O
summarize	O
two	O
such	O
comparisons	O
that	O
were	O
carefully	O
conducted	O
the	O
data	O
sets	O
that	O
were	O
used	O
are	O
relatively	O
small	O
see	O
the	O
website	O
mlcomp	O
org	O
for	O
a	O
distributed	O
way	O
to	O
perform	O
large	O
scale	O
comparisons	O
of	O
this	O
kind	O
of	O
course	O
we	O
must	O
always	O
remember	O
the	O
no	B
free	I
lunch	I
theorem	I
which	O
tells	O
us	O
that	O
there	O
is	O
no	O
universally	O
best	O
learning	B
method	O
low-dimensional	O
features	B
in	O
rich	O
caruana	O
and	O
alex	O
niculescu-mizil	O
and	O
niculescu-mizil	O
conducted	O
a	O
very	O
extensive	O
experimental	O
comparison	O
of	O
different	O
binary	O
classification	O
methods	O
on	O
different	O
data	O
sets	O
the	O
data	O
sets	O
all	O
had	O
training	O
cases	O
and	O
had	O
test	O
sets	O
containing	O
examples	O
on	O
average	O
the	O
number	O
of	O
features	B
ranged	O
from	O
to	O
so	O
this	O
is	O
much	O
lower	O
dimensional	O
than	O
the	O
nips	O
feature	B
selection	I
challenge	O
cross	B
validation	I
was	O
used	O
to	O
assess	O
average	O
test	O
error	O
is	O
separate	O
from	O
any	O
internal	O
cv	B
a	O
method	O
may	O
need	O
to	O
use	O
for	O
model	B
selection	I
experimental	O
comparison	O
the	O
methods	O
they	O
compared	O
are	O
as	O
follows	O
in	O
roughly	O
decreasing	O
order	O
of	O
performance	O
as	O
assessed	O
by	O
table	O
bst-dt	O
boosted	O
decision	B
trees	I
rf	O
random	O
forest	B
bag-dt	O
bagged	O
decision	B
trees	I
svm	B
support	B
vector	I
machine	I
ann	O
artificial	O
neural	B
network	I
knn	B
k-nearest	O
neighbors	B
bst-stmp	O
boosted	O
stumps	O
dt	O
decision	B
tree	B
logreg	O
logistic	B
regression	B
nb	O
naive	O
bayes	O
they	O
used	O
different	O
performance	O
measures	O
which	O
can	O
be	O
divided	O
into	O
three	O
groups	O
threshold	O
metrics	O
just	O
require	O
a	O
point	B
estimate	I
as	O
output	O
these	O
include	O
accuracy	O
f-score	O
etc	O
ordering	O
ranking	B
metrics	O
measure	O
how	O
well	O
positive	O
cases	O
are	O
ordered	O
before	O
the	O
negative	O
cases	O
these	O
include	O
area	O
under	O
the	O
roc	B
curve	O
average	B
precision	B
and	O
the	O
precisionrecall	O
break	O
even	O
point	O
finally	O
the	O
probability	O
metrics	O
included	O
cross-entropy	B
and	O
squared	B
error	I
methods	O
such	O
as	O
svms	O
that	O
do	O
not	O
produce	O
calibrated	O
probabilities	O
were	O
post-processed	O
using	O
platt	O
s	O
logistic	B
regression	B
trick	O
or	O
using	O
isotonic	O
regression	B
performance	O
measures	O
were	O
standardized	B
to	O
a	O
scale	O
so	O
they	O
could	O
be	O
compared	O
obviously	O
the	O
results	O
vary	O
by	O
dataset	O
and	O
by	O
metric	B
therefore	O
just	O
averaging	O
the	O
performance	O
does	O
not	O
necessarily	O
give	O
reliable	O
conclusions	O
however	O
one	O
can	O
perform	O
a	O
bootstrap	B
analysis	O
which	O
shows	O
how	O
robust	B
the	O
conclusions	O
are	O
to	O
such	O
changes	O
the	O
results	O
are	O
shown	O
in	O
table	O
we	O
see	O
that	O
most	O
of	O
the	O
time	O
boosted	O
decision	B
trees	I
are	O
the	O
best	O
method	O
followed	O
by	O
random	B
forests	I
bagged	O
decision	B
trees	I
svms	O
and	O
neural	B
networks	I
however	O
the	O
following	O
methods	O
all	O
did	O
relatively	O
poorly	O
knn	B
stumps	O
single	O
decision	B
trees	I
logistic	B
regression	B
and	O
naive	O
bayes	O
these	O
results	O
are	O
generally	O
consistent	B
with	O
conventional	O
wisdom	O
of	O
practioners	O
in	O
the	O
field	O
of	O
course	O
the	O
conclusions	O
may	O
change	O
if	O
there	O
the	O
features	B
are	O
high	O
dimensional	O
and	O
or	O
there	O
are	O
lots	O
of	O
irrelevant	O
features	B
in	O
section	O
or	O
if	O
there	O
is	O
lots	O
of	O
noise	O
etc	O
high-dimensional	O
features	B
in	O
the	O
nips	O
conference	O
ran	O
a	O
competition	O
where	O
the	O
goal	O
was	O
to	O
solve	O
binary	O
classification	O
problems	O
with	O
large	O
numbers	O
of	O
irrelevant	O
features	B
given	O
small	O
training	O
sets	O
was	O
called	O
a	O
feature	B
selection	I
challenge	O
but	O
performance	O
was	O
measured	O
in	O
terms	O
of	O
predictive	B
accuracy	O
not	O
in	O
terms	O
of	O
the	O
ability	O
to	O
select	O
features	B
the	O
five	O
datasets	O
that	O
were	O
used	O
are	O
summarized	O
in	O
table	O
the	O
term	O
probe	B
refers	O
to	O
artifical	O
variables	O
that	O
were	O
added	O
to	O
the	O
problem	O
to	O
make	O
it	O
harder	O
these	O
have	O
no	O
predictive	B
power	O
but	O
are	O
correlated	O
with	O
the	O
original	O
features	B
results	O
of	O
the	O
competition	O
are	O
discussed	O
in	O
et	O
al	O
the	O
overall	O
winner	O
was	O
an	O
in	O
a	O
follow-up	O
study	O
approach	O
based	O
on	O
bayesian	B
neural	B
networks	I
and	O
zhang	O
chapter	O
adaptive	O
basis	O
function	O
models	O
type	O
dataset	O
aracene	O
dexter	O
dorothea	O
drug	O
discovery	O
gisette	O
madelon	O
domain	O
mass	O
spectrometry	O
dense	O
text	O
classification	O
sparse	B
sparse	B
dense	O
dense	O
digit	O
recognition	O
artificial	O
d	O
probes	O
ntrain	O
nval	O
ntest	O
table	O
datasets	O
the	O
features	B
are	O
binary	O
for	O
the	O
others	O
the	O
features	B
are	O
real-valued	O
summary	O
of	O
the	O
data	O
used	O
in	O
the	O
nips	O
feature	B
selection	I
challenge	O
for	O
the	O
dorothea	O
method	O
hmc	O
mlp	B
boosted	O
mlp	B
bagged	O
mlp	B
boosted	O
trees	O
random	B
forests	I
screened	O
features	B
ard	B
avg	O
rank	O
avg	O
time	O
avg	O
rank	O
avg	O
time	O
performance	O
of	O
different	O
methods	O
on	O
the	O
nips	O
feature	B
selection	I
challenge	O
table	O
stands	O
for	O
hybrid	B
monte	B
carlo	I
see	O
section	O
we	O
report	O
the	O
average	O
rank	O
is	O
better	O
across	O
the	O
datasets	O
we	O
also	O
report	O
the	O
average	O
training	O
time	O
in	O
minutes	O
error	O
in	O
brackets	O
the	O
mcmc	B
and	O
bagged	O
mlps	O
use	O
two	O
hidden	B
layers	O
of	O
and	O
units	O
the	O
boosted	O
mlps	O
use	O
one	O
hidden	B
layer	I
with	O
or	O
hidden	B
units	I
the	O
boosted	O
trees	O
used	O
depths	O
between	O
and	O
and	O
shrinkage	B
between	O
and	O
each	O
tree	B
was	O
trained	O
on	O
of	O
the	O
data	O
chosen	O
at	O
random	O
at	O
each	O
step	O
stochastic	B
gradient	B
boosting	B
from	O
table	O
of	O
et	O
al	O
bayesian	B
neural	O
nets	O
with	O
hidden	B
layers	O
were	O
compared	O
to	O
several	O
other	O
methods	O
based	O
on	O
bagging	B
and	O
boosting	B
note	O
that	O
all	O
of	O
these	O
methods	O
are	O
quite	O
similar	B
in	O
each	O
case	O
the	O
prediction	O
has	O
the	O
form	O
f	O
wme	O
m	O
the	O
bayesian	B
mlp	B
was	O
fit	O
by	O
mcmc	B
monte	B
carlo	I
so	O
we	O
set	O
wm	O
and	O
set	O
m	O
in	O
bagging	B
we	O
set	O
wm	O
and	O
m	O
is	O
estimated	O
by	O
fitting	O
to	O
a	O
draw	O
from	O
the	O
posterior	O
in	O
boosting	B
we	O
set	O
wm	O
and	O
the	O
m	O
are	O
the	O
model	O
to	O
a	O
bootstrap	B
sample	O
from	O
the	O
data	O
estimated	O
sequentially	O
to	O
improve	O
computational	O
and	O
statistical	O
performance	O
some	O
feature	B
selection	I
was	O
performed	O
two	O
methods	O
were	O
considered	O
simple	O
uni-variate	O
screening	B
using	O
t-tests	O
and	O
a	O
method	O
based	O
on	O
mlpard	O
results	O
of	O
this	O
follow-up	O
study	O
are	O
shown	O
in	O
table	O
we	O
see	O
that	O
bayesian	B
mlps	O
are	O
again	O
the	O
winner	O
in	O
second	O
place	O
are	O
either	O
random	B
forests	I
or	O
boosted	O
mlps	O
depending	O
on	O
the	O
preprocessing	O
however	O
it	O
is	O
not	O
clear	O
how	O
statistically	O
significant	O
these	O
differences	O
are	O
since	O
the	O
test	O
sets	O
are	O
relatively	O
small	O
in	O
terms	O
of	O
training	O
time	O
we	O
see	O
that	O
mcmc	B
is	O
much	O
slower	O
than	O
the	O
other	O
methods	O
it	O
would	O
be	O
interesting	O
to	O
see	O
how	O
well	O
deterministic	O
bayesian	B
inference	B
laplace	B
approximation	I
would	O
perform	O
it	O
will	O
be	O
much	O
faster	O
but	O
the	O
question	O
is	O
how	O
much	O
would	O
one	O
lose	O
interpreting	O
black-box	B
models	O
e	O
c	O
n	O
e	O
d	O
n	O
e	O
p	O
e	O
d	O
l	O
a	O
i	O
t	O
r	O
a	O
p	O
e	O
c	O
n	O
e	O
d	O
n	O
e	O
p	O
e	O
d	O
l	O
a	O
i	O
t	O
r	O
a	O
p	O
figure	O
partial	O
dependence	O
plots	O
for	O
the	O
predictors	O
in	O
friedman	O
s	O
synthetic	O
regression	B
problem	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
hugh	O
chipman	O
in	O
statistical	O
performance	O
interpreting	O
black-box	B
models	O
linear	O
models	O
are	O
popular	O
in	O
part	O
because	O
they	O
are	O
easy	O
to	O
interpet	O
however	O
they	O
often	O
are	O
poor	O
predictors	O
which	O
makes	O
them	O
a	O
poor	O
proxy	O
for	O
nature	O
s	O
mechanism	O
thus	O
any	O
conclusions	O
about	O
the	O
importance	O
of	O
particular	O
variables	O
should	O
only	O
be	O
based	O
on	O
models	O
that	O
have	O
good	O
predictive	B
accuracy	O
many	O
standard	O
statistical	O
tests	O
of	O
goodness	O
of	O
fit	O
do	O
not	O
test	O
the	O
predictive	B
accuracy	O
of	O
a	O
model	O
in	O
this	O
chapter	O
we	O
studied	O
black-box	B
models	O
which	O
do	O
have	O
good	O
predictive	B
accuracy	O
unfortunately	O
they	O
are	O
hard	O
to	O
interpret	O
directly	O
fortunately	O
there	O
are	O
various	O
heuristics	B
we	O
can	O
use	O
to	O
probe	B
such	O
models	O
in	O
order	O
to	O
assess	O
which	O
input	O
variables	O
are	O
the	O
most	O
important	O
as	O
a	O
simple	O
example	O
consider	O
the	O
following	O
non-linear	O
function	O
first	O
proposed	O
to	O
illustrate	O
the	O
power	O
of	O
mars	B
f	O
sin	O
where	O
n	O
we	O
see	O
that	O
the	O
output	O
is	O
a	O
complex	O
function	O
of	O
the	O
inputs	O
by	O
augmenting	O
the	O
x	O
vector	O
with	O
additional	O
irrelevant	O
random	O
variables	O
all	O
drawn	O
uniform	O
on	O
we	O
can	O
create	O
a	O
challenging	O
feature	B
selection	I
problem	O
in	O
the	O
experiments	O
below	O
we	O
add	O
extra	O
dummy	O
variables	O
chapter	O
adaptive	O
basis	O
function	O
models	O
e	O
g	O
a	O
s	O
u	O
figure	O
average	O
usage	O
of	O
each	O
variable	O
in	O
a	O
bart	B
model	O
fit	O
to	O
data	O
where	O
only	O
the	O
first	O
features	B
are	O
relevant	O
the	O
different	O
coloured	O
lines	O
correspond	O
to	O
different	O
numbers	O
of	O
trees	O
in	O
the	O
ensemble	B
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
hugh	O
chipman	O
one	O
useful	O
way	O
to	O
measure	O
the	O
effect	O
of	O
a	O
set	O
s	O
of	O
variables	O
on	O
the	O
output	O
is	O
to	O
compute	O
a	O
partial	B
dependence	I
plot	I
this	O
is	O
a	O
plot	O
of	O
f	O
vs	O
xs	O
where	O
f	O
is	O
defined	O
as	O
the	O
response	O
to	O
xs	O
with	O
the	O
other	O
predictors	O
averaged	O
out	O
f	O
n	O
f	O
xi	O
s	O
figure	O
shows	O
an	O
example	O
where	O
we	O
use	O
sets	O
corresponding	O
to	O
each	O
single	O
variable	O
the	O
data	O
was	O
generated	O
from	O
equation	O
with	O
irrelevant	O
variables	O
added	O
we	O
then	O
fit	O
a	O
bart	B
model	O
and	O
computed	O
the	O
partial	O
dependence	O
plots	O
we	O
see	O
that	O
the	O
predicted	O
response	O
is	O
invariant	B
for	O
s	O
indicating	O
that	O
these	O
variables	O
are	O
irrelevant	O
the	O
response	O
is	O
roughly	O
linear	O
in	O
and	O
and	O
roughly	O
quadratic	O
in	O
error	O
bars	O
are	O
obtained	O
by	O
computing	O
empirical	O
quantiles	O
of	O
f	O
based	O
on	O
posterior	O
samples	B
of	O
alternatively	O
we	O
can	O
use	O
bootstrap	B
another	O
very	O
useful	O
summary	O
computes	O
the	O
relative	B
importance	I
of	I
predictor	I
variables	I
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
nonlinear	O
or	O
even	O
model	O
free	O
way	O
of	O
performing	O
variable	O
selection	O
although	O
the	O
technique	O
is	O
restricted	O
to	O
ensembles	O
of	O
trees	O
the	O
basic	O
idea	O
originally	O
proposed	O
in	O
et	O
al	O
is	O
to	O
count	O
how	O
often	O
variable	O
j	O
is	O
used	O
as	O
a	O
node	O
in	O
any	O
of	O
the	O
trees	O
ij	O
tm	O
be	O
the	O
proportion	O
of	O
all	O
splitting	O
rules	B
that	O
use	O
xj	O
in	O
particular	O
let	O
vj	O
where	O
tm	O
is	O
the	O
m	O
th	O
tree	B
if	O
we	O
can	O
sample	O
the	O
posterior	O
of	O
trees	O
we	O
can	O
easily	O
m	O
compute	O
the	O
posterior	O
for	O
vj	O
alternatively	O
we	O
can	O
use	O
bootstrap	B
figure	O
gives	O
an	O
example	O
using	O
bart	B
we	O
see	O
that	O
the	O
five	O
relevant	O
variables	O
are	O
chosen	O
much	O
more	O
than	O
the	O
five	O
irrelevant	O
variables	O
as	O
we	O
increase	O
the	O
number	O
m	O
of	O
trees	O
all	O
the	O
variables	O
are	O
more	O
likely	O
to	O
be	O
chosen	O
reducing	O
the	O
sensitivity	B
of	O
this	O
method	O
but	O
for	O
small	O
m	O
the	O
method	O
is	O
farily	O
diagnostic	O
interpreting	O
black-box	B
models	O
exercises	O
exercise	O
nonlinear	O
regression	B
for	O
inverse	O
dynamics	O
in	O
this	O
question	O
we	O
fit	O
a	O
model	O
which	O
can	O
predict	O
what	O
torques	O
a	O
robot	O
needs	O
to	O
apply	O
in	O
order	O
to	O
make	O
its	O
arm	O
reach	O
a	O
desired	O
point	O
in	O
space	O
the	O
data	O
was	O
collected	O
from	O
a	O
sarcos	O
robot	O
arm	O
with	O
degrees	B
of	I
freedom	I
the	O
input	O
vector	O
x	O
r	O
encodes	O
the	O
desired	O
position	O
velocity	O
and	O
accelaration	O
of	O
the	O
joints	O
the	O
output	O
vector	O
y	O
r	O
encodes	O
the	O
torques	O
that	O
should	O
be	O
applied	O
to	O
the	O
joints	O
to	O
reach	O
that	O
point	O
the	O
mapping	O
from	O
x	O
to	O
y	O
is	O
highly	O
nonlinear	O
we	O
have	O
n	O
training	O
points	O
and	O
ntest	O
testing	O
points	O
for	O
simplicity	O
we	O
following	O
standard	O
practice	O
and	O
focus	O
on	O
just	O
predicting	O
a	O
scalar	O
output	O
namely	O
the	O
torque	O
for	O
the	O
first	O
joint	O
download	O
the	O
data	O
from	O
httpwww	O
gaussianprocess	O
orggpml	O
standardize	O
the	O
inputs	O
so	O
they	O
have	O
zero	O
mean	B
and	O
unit	O
variance	B
on	O
the	O
training	B
set	I
and	O
center	O
the	O
outputs	O
so	O
they	O
have	O
zero	O
mean	B
on	O
the	O
training	B
set	I
apply	O
the	O
corresponding	O
transformations	O
to	O
the	O
test	O
data	O
below	O
we	O
will	O
describe	O
various	O
models	O
which	O
you	O
should	O
fit	O
to	O
this	O
transformed	O
data	O
then	O
make	O
predictions	O
and	O
compute	O
the	O
standardized	B
mean	B
squared	B
error	I
on	O
the	O
test	O
set	O
as	O
follows	O
sm	O
se	O
ntest	O
train	O
where	O
ntrain	O
is	O
the	O
variance	B
of	O
the	O
output	O
computed	O
on	O
the	O
training	B
set	I
a	O
the	O
first	O
method	O
you	O
should	O
try	O
is	O
standard	O
linear	B
regression	B
turn	O
in	O
your	O
numbers	O
and	O
code	O
to	O
and	O
williams	O
you	O
should	O
be	O
able	O
to	O
achieve	O
a	O
smse	O
of	O
using	O
this	O
method	O
b	O
now	O
try	O
running	O
k-means	O
clustering	B
cross	B
validation	I
to	O
pick	O
k	O
then	O
fit	O
an	O
rbf	B
network	I
to	O
the	O
data	O
using	O
the	O
k	O
estimated	O
by	O
k-means	O
use	O
cv	B
to	O
estimate	O
the	O
rbf	B
bandwidth	B
what	O
smse	O
do	O
you	O
get	O
turn	O
in	O
your	O
numbers	O
and	O
code	O
to	O
and	O
williams	O
gaussian	B
process	I
regression	B
can	O
get	O
an	O
smse	O
of	O
so	O
the	O
goal	O
is	O
to	O
get	O
close	O
to	O
that	O
c	O
now	O
try	O
fitting	O
a	O
feedforward	B
neural	B
network	I
use	O
cv	B
to	O
pick	O
the	O
number	O
of	O
hidden	B
units	I
and	O
the	O
strength	O
of	O
the	O
regularizer	O
what	O
smse	O
do	O
you	O
get	O
turn	O
in	O
your	O
numbers	O
and	O
code	O
markov	B
and	O
hidden	B
markov	B
models	I
introduction	O
in	O
this	O
chapter	O
we	O
discuss	O
probabilistic	O
models	O
for	O
sequences	O
of	O
observations	O
xt	O
of	O
arbitrary	O
length	O
t	O
such	O
models	O
have	O
applications	O
in	O
computational	O
biology	O
natural	O
language	O
processing	O
time	O
series	O
forecasting	O
etc	O
we	O
focus	O
on	O
the	O
case	O
where	O
we	O
the	O
observations	O
occur	O
at	O
discrete	B
time	O
steps	O
although	O
time	O
may	O
also	O
refer	O
to	O
locations	O
within	O
a	O
sequence	O
markov	B
models	I
recall	B
from	O
section	O
that	O
the	O
basic	O
idea	O
behind	O
a	O
markov	B
chain	I
is	O
to	O
assume	O
that	O
xt	O
captures	O
all	O
the	O
relevant	O
information	B
for	O
predicting	O
the	O
future	O
we	O
assume	O
it	O
is	O
a	O
sufficient	O
statistic	O
if	O
we	O
assume	O
discrete	B
time	O
steps	O
we	O
can	O
write	O
the	O
joint	B
distribution	I
as	O
follows	O
pxtxt	O
this	O
is	O
called	O
a	O
markov	B
chain	I
or	O
markov	B
model	I
if	O
we	O
assume	O
the	O
transition	O
function	O
pxtxt	O
is	O
independent	O
of	O
time	O
then	O
the	O
chain	O
is	O
called	O
homogeneous	B
stationary	B
or	O
time-invariant	B
this	O
is	O
an	O
example	O
of	O
parameter	B
tying	I
since	O
the	O
same	O
parameter	B
is	O
shared	B
by	O
multiple	O
variables	O
this	O
assumption	O
allows	O
us	O
to	O
model	O
an	O
arbitrary	O
number	O
of	O
variables	O
using	O
a	O
fixed	O
number	O
of	O
parameters	O
such	O
models	O
are	O
called	O
stochastic	B
processes	I
if	O
we	O
assume	O
that	O
the	O
observed	O
variables	O
are	O
discrete	B
so	O
xt	O
k	O
this	O
is	O
called	O
a	O
discrete-state	O
or	O
finite-state	O
markov	B
chain	I
we	O
will	O
make	O
this	O
assumption	O
throughout	O
the	O
rest	O
of	O
this	O
section	O
transition	B
matrix	I
when	O
xt	O
is	O
discrete	B
so	O
xt	O
k	O
the	O
conditional	O
distribution	O
pxtxt	O
can	O
be	O
written	O
as	O
a	O
k	O
k	O
matrix	O
known	O
as	O
the	O
transition	B
matrix	I
a	O
where	O
aij	O
pxt	O
jxt	O
i	O
is	O
the	O
probability	O
of	O
going	O
from	O
state	B
i	O
to	O
state	B
j	O
each	O
row	O
of	O
the	O
matrix	O
sums	O
to	O
one	O
j	O
aij	O
so	O
this	O
is	O
called	O
a	O
stochastic	B
matrix	I
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
figure	O
left-to-right	B
chain	O
state	B
transition	O
diagrams	O
for	O
some	O
simple	O
markov	B
chains	O
left	O
a	O
chain	O
right	O
a	O
a	O
stationary	B
finite-state	O
markov	B
chain	I
is	O
equivalent	O
to	O
a	O
stochastic	B
automaton	I
it	O
is	O
common	O
to	O
visualize	O
such	O
automata	O
by	O
drawing	O
a	O
directed	B
graph	B
where	O
nodes	B
represent	O
states	O
and	O
arrows	O
represent	O
legal	O
transitions	O
i	O
e	O
non-zero	O
elements	O
of	O
a	O
this	O
is	O
known	O
as	O
a	O
state	B
transition	I
diagram	I
the	O
weights	O
associated	O
with	O
the	O
arcs	O
are	O
the	O
probabilities	O
for	O
example	O
the	O
following	O
chain	O
a	O
a	O
is	O
illustrated	O
in	O
figure	O
the	O
following	O
chain	O
is	O
illustrated	O
in	O
figure	O
this	O
is	O
called	O
a	O
left-to-right	B
transition	B
matrix	I
and	O
is	O
commonly	O
used	O
in	O
speech	B
recognition	I
the	O
aij	O
element	O
of	O
the	O
transition	B
matrix	I
specifies	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
j	O
in	O
one	O
step	O
the	O
n-step	O
transition	B
matrix	I
an	O
is	O
defined	O
as	O
aijn	O
pxtn	O
jxt	O
i	O
which	O
is	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
j	O
in	O
exactly	O
n	O
steps	O
obviously	O
a	O
the	O
chapman-kolmogorov	B
equations	O
state	B
that	O
aijm	O
n	O
aikmakjn	O
in	O
words	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
j	O
in	O
m	O
n	O
steps	O
is	O
just	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
k	O
in	O
m	O
steps	O
and	O
then	O
from	O
k	O
to	O
j	O
in	O
n	O
steps	O
summed	O
up	O
over	O
all	O
k	O
we	O
can	O
write	O
the	O
above	O
as	O
a	O
matrix	O
multiplication	O
am	O
n	O
aman	O
hence	O
an	O
a	O
an	O
a	O
a	O
an	O
an	O
thus	O
we	O
can	O
simulate	O
multiple	O
steps	O
of	O
a	O
markov	B
chain	I
by	O
powering	O
up	O
the	O
transition	B
matrix	I
markov	B
models	I
says	O
it	O
s	O
not	O
in	O
the	O
cards	O
legendary	O
reconnaissance	O
by	O
rollie	O
democracies	O
unsustainable	O
could	O
strike	O
redlining	O
visits	O
to	O
profit	O
booking	O
wait	O
here	O
at	O
madison	O
square	O
garden	O
county	O
courthouse	O
where	O
he	O
had	O
been	O
done	O
in	O
three	O
already	O
in	O
any	O
way	O
in	O
which	O
a	O
teacher	O
table	O
example	O
output	O
from	O
an	O
word	O
model	O
trained	O
using	O
backoff	B
smoothing	B
on	O
the	O
broadcast	O
news	O
corpus	B
the	O
first	O
words	O
are	O
specified	O
by	O
hand	O
the	O
model	O
generates	O
the	O
word	O
and	O
then	O
the	O
results	O
are	O
fed	O
back	O
into	O
the	O
model	O
source	O
m	O
txt	O
application	O
language	B
modeling	I
one	O
important	O
application	O
of	O
markov	B
models	I
is	O
to	O
make	O
statistical	O
language	B
models	I
which	O
are	O
probability	O
distributions	O
over	O
sequences	O
of	O
words	O
we	O
define	O
the	O
state	B
space	I
to	O
be	O
all	O
the	O
words	O
in	O
english	O
some	O
other	O
language	O
the	O
marginal	O
probabilities	O
pxt	O
k	O
are	O
called	O
unigram	B
statistics	I
if	O
we	O
use	O
a	O
first-order	O
markov	B
model	I
then	O
pxt	O
kxt	O
j	O
is	O
called	O
a	O
bigram	O
if	O
we	O
use	O
a	O
second-order	B
markov	B
model	I
then	O
pxt	O
kxt	O
j	O
xt	O
i	O
is	O
model	O
called	O
a	O
trigram	B
model	I
and	O
so	O
on	O
in	O
general	O
these	O
are	O
called	O
n-gram	B
models	I
for	O
example	O
figure	O
shows	O
and	O
counts	O
for	O
the	O
letters	O
z	O
represents	O
space	O
estimated	O
from	O
darwin	O
s	O
on	O
the	O
origin	O
of	O
species	O
language	B
models	I
can	O
be	O
used	O
for	O
several	O
things	O
such	O
as	O
the	O
following	O
sentence	O
completion	O
a	O
language	B
model	I
can	O
predict	O
the	O
next	O
word	O
given	O
the	O
previous	O
words	O
in	O
a	O
sentence	O
this	O
can	O
be	O
used	O
to	O
reduce	O
the	O
amount	O
of	O
typing	O
required	O
which	O
is	O
particularly	O
important	O
for	O
disabled	O
users	O
e	O
g	O
david	O
mackay	O
s	O
dasher	B
or	O
uses	O
of	O
mobile	O
devices	O
data	B
compression	I
any	O
density	O
model	O
can	O
be	O
used	O
to	O
define	O
an	O
encoding	O
scheme	O
by	O
assigning	O
short	O
codewords	O
to	O
more	O
probable	O
strings	O
the	O
more	O
accurate	O
the	O
predictive	B
model	O
the	O
fewer	O
the	O
number	O
of	O
bits	B
it	O
requires	O
to	O
store	O
the	O
data	O
text	O
classification	O
any	O
density	O
model	O
can	O
be	O
used	O
as	O
a	O
class-conditional	B
density	I
and	O
hence	O
turned	O
into	O
a	O
classifier	O
note	O
that	O
using	O
a	O
class-conditional	B
density	I
only	O
unigram	B
statistics	I
would	O
be	O
equivalent	O
to	O
a	O
naive	O
bayes	O
classifier	O
section	O
automatic	O
essay	O
writing	O
one	O
can	O
sample	O
from	O
to	O
generate	O
artificial	O
text	O
this	O
is	O
in	O
table	O
we	O
give	O
an	O
example	O
of	O
text	O
one	O
way	O
of	O
assessing	O
the	O
quality	O
of	O
the	O
model	O
generated	O
from	O
a	O
model	O
trained	O
on	O
a	O
corpus	B
with	O
million	O
words	O
et	O
al	O
describes	O
a	O
much	O
better	O
language	B
model	I
based	O
on	O
recurrent	B
neural	B
networks	I
which	O
generates	O
much	O
more	O
semantically	O
plausible	O
text	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukdasher	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
unigrams	B
bigrams	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
figure	O
unigram	O
and	O
bigram	O
counts	O
from	O
darwin	O
s	O
on	O
the	O
origin	O
of	O
species	O
the	O
picture	O
on	O
the	O
right	O
is	O
a	O
hinton	B
diagram	I
of	O
the	O
joint	B
distribution	I
the	O
size	O
of	O
the	O
white	O
squares	O
is	O
proportional	O
to	O
the	O
value	O
of	O
the	O
entry	O
in	O
the	O
corresponding	O
vector	O
matrix	O
based	O
on	O
figure	O
generated	O
by	O
ngramplot	O
mle	B
for	O
markov	B
language	B
models	I
we	O
now	O
discuss	O
a	O
simple	O
way	O
to	O
estimate	O
the	O
transition	B
matrix	I
from	O
training	O
data	O
the	O
probability	O
of	O
any	O
particular	O
sequence	O
of	O
length	O
t	O
is	O
given	O
by	O
axt	O
xt	O
hence	O
the	O
log-likelihood	O
of	O
a	O
set	O
of	O
sequences	O
d	O
xn	O
where	O
xi	O
xiti	O
is	O
a	O
sequence	O
of	O
length	O
ti	O
is	O
given	O
by	O
log	O
pd	O
log	O
pxi	O
ti	O
j	O
where	O
we	O
define	O
the	O
following	O
counts	O
j	O
n	O
j	O
njk	O
n	O
j	O
log	O
j	O
njk	O
log	O
ajk	O
j	O
k	O
ixit	O
j	O
k	O
markov	B
models	I
hence	O
we	O
can	O
write	O
the	O
mle	B
as	O
the	O
normalized	O
counts	O
j	O
ajk	O
n	O
j	O
n	O
j	O
k	O
njk	O
these	O
results	O
can	O
be	O
extended	O
in	O
a	O
straightforward	O
way	O
to	O
higher	O
order	O
markov	B
models	I
however	O
the	O
problem	O
of	O
zero-counts	O
becomes	O
very	O
acute	O
whenever	O
the	O
number	O
of	O
states	O
k	O
andor	O
the	O
order	O
of	O
the	O
chain	O
n	O
is	O
large	O
an	O
n-gram	B
models	I
has	O
ok	O
n	O
parameters	O
if	O
we	O
have	O
k	O
words	O
in	O
our	O
vocabulary	O
then	O
a	O
bi-gram	O
model	O
will	O
have	O
about	O
billion	O
free	O
parameters	O
corresponding	O
to	O
all	O
possible	O
word	O
pairs	O
it	O
is	O
very	O
unlikely	O
we	O
will	O
see	O
all	O
of	O
these	O
in	O
our	O
training	O
data	O
however	O
we	O
do	O
not	O
want	O
to	O
predict	O
that	O
a	O
particular	O
word	O
string	O
is	O
totally	O
impossible	O
just	O
because	O
we	O
happen	O
not	O
to	O
have	O
seen	O
it	O
in	O
our	O
training	O
text	O
that	O
would	O
be	O
a	O
severe	O
form	O
of	O
a	O
simple	O
solution	O
to	O
this	O
is	O
to	O
use	O
add-one	B
smoothing	B
where	O
we	O
simply	O
add	O
one	O
to	O
all	O
the	O
empirical	O
counts	O
before	O
normalizing	O
the	O
bayesian	B
justification	O
for	O
this	O
is	O
given	O
in	O
section	O
however	O
add-one	B
smoothing	B
assumes	O
all	O
n-grams	O
are	O
equally	O
likely	O
which	O
is	O
not	O
very	O
realistic	O
a	O
more	O
sophisticated	O
bayesian	B
approach	O
is	O
discussed	O
in	O
section	O
an	O
alternative	O
to	O
using	O
smart	O
priors	O
is	O
to	O
gather	O
lots	O
and	O
lots	O
of	O
data	O
for	O
example	O
google	O
has	O
fit	O
n-gram	B
models	I
n	O
based	O
on	O
one	O
trillion	O
words	O
extracted	O
from	O
the	O
web	O
their	O
data	O
which	O
is	O
over	O
when	O
uncompressed	O
is	O
publically	O
an	O
example	O
of	O
their	O
data	O
for	O
a	O
set	O
of	O
is	O
shown	O
below	O
serve	O
as	O
the	O
incoming	O
serve	O
as	O
the	O
incubator	O
serve	O
as	O
the	O
independent	O
serve	O
as	O
the	O
index	O
serve	O
as	O
the	O
indication	O
serve	O
as	O
the	O
indicator	O
serve	O
as	O
the	O
indicators	O
serve	O
as	O
the	O
indispensable	O
serve	O
as	O
the	O
indispensible	O
serve	O
as	O
the	O
individual	O
although	O
such	O
an	O
approach	O
based	O
on	O
brute	O
force	O
and	O
ignorance	O
can	O
be	O
successful	O
it	O
is	O
rather	O
unsatisfying	O
since	O
it	O
is	O
clear	O
that	O
this	O
is	O
not	O
how	O
humans	O
learn	O
e	O
g	O
and	O
xu	O
a	O
more	O
refined	O
bayesian	B
approach	O
that	O
needs	O
much	O
less	O
data	O
is	O
described	O
in	O
section	O
empirical	B
bayes	I
version	O
of	O
deleted	B
interpolation	I
a	O
common	O
heuristic	O
used	O
to	O
fix	O
the	O
sparse	B
data	I
problem	I
is	O
called	O
deleted	B
interpolation	I
and	O
goodman	O
this	O
defines	O
the	O
transition	B
matrix	I
as	O
a	O
convex	B
combination	I
of	O
the	O
bigram	O
a	O
famous	O
example	O
of	O
an	O
improbable	O
but	O
syntactically	O
valid	O
english	O
word	O
string	O
due	O
to	O
noam	O
chomsky	O
is	O
colourless	O
green	O
ideas	O
sleep	O
furiously	O
we	O
would	O
not	O
want	O
our	O
model	O
to	O
predict	O
that	O
this	O
string	O
is	O
impossible	O
even	O
ungrammatical	O
constructs	O
should	O
be	O
allowed	O
by	O
our	O
model	O
with	O
a	O
certain	O
probability	O
since	O
people	O
frequently	O
violate	O
grammatical	O
rules	B
especially	O
in	O
spoken	O
language	O
see	O
for	O
details	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
frequencies	O
fjk	O
njknj	O
and	O
the	O
unigram	O
frequencies	O
fk	O
nkn	O
ajk	O
fk	O
the	O
term	O
is	O
usually	O
set	O
by	O
cross	B
validation	I
there	O
is	O
also	O
a	O
closely	O
related	O
technique	O
called	O
backoff	B
smoothing	B
the	O
idea	O
is	O
that	O
if	O
fjk	O
is	O
too	O
small	O
we	O
back	O
off	O
to	O
a	O
more	O
reliable	O
estimate	O
namely	O
fk	O
we	O
will	O
now	O
show	O
that	O
the	O
deleted	B
interpolation	I
heuristic	O
is	O
an	O
approximation	O
to	O
the	O
predictions	O
made	O
by	O
a	O
simple	O
hierarchical	B
bayesian	B
model	I
our	O
presentation	O
follows	O
and	O
peto	O
first	O
let	O
us	O
use	O
an	O
independent	O
dirichlet	B
prior	O
on	O
each	O
row	O
of	O
the	O
transition	B
matrix	I
aj	O
dir	O
dir	O
dir	O
where	O
aj	O
is	O
row	O
j	O
of	O
the	O
transition	B
matrix	I
m	O
is	O
the	O
prior	O
mean	B
is	O
the	O
prior	O
strength	O
we	O
will	O
use	O
the	O
same	O
prior	O
for	O
each	O
row	O
see	O
figure	O
k	O
mk	O
and	O
the	O
posterior	O
is	O
given	O
by	O
aj	O
dir	O
nj	O
where	O
nj	O
njk	O
is	O
the	O
vector	O
that	O
records	O
the	O
number	O
of	O
times	O
we	O
have	O
transitioned	O
out	O
of	O
state	B
j	O
to	O
each	O
of	O
the	O
other	O
states	O
from	O
equation	O
the	O
posterior	B
predictive	B
density	I
is	O
kxt	O
jd	O
ajk	O
where	O
ajk	O
e	O
and	O
jfjk	O
nj	O
njk	O
mk	O
fjknj	O
mk	O
nj	O
j	O
nj	O
this	O
is	O
very	O
similar	B
to	O
equation	O
but	O
not	O
identical	O
the	O
main	O
difference	O
is	O
that	O
the	O
bayesian	B
model	O
uses	O
a	O
context-dependent	O
weight	O
j	O
to	O
combine	O
mk	O
with	O
the	O
empirical	O
frequency	O
fjk	O
rather	O
than	O
a	O
fixed	O
weight	O
this	O
is	O
like	O
adaptive	O
deleted	B
interpolation	I
furthermore	O
rather	O
than	O
backing	O
off	O
to	O
the	O
empirical	O
marginal	O
frequencies	O
fk	O
we	O
back	O
off	O
to	O
the	O
model	O
parameter	B
mk	O
the	O
only	O
remaining	O
question	O
is	O
what	O
values	O
should	O
we	O
use	O
for	O
and	O
m	O
let	O
s	O
use	O
empirical	B
bayes	I
since	O
we	O
assume	O
each	O
row	O
of	O
the	O
transition	B
matrix	I
is	O
a	O
priori	O
independent	O
given	O
the	O
marginal	B
likelihood	B
for	O
our	O
markov	B
model	I
is	O
found	O
by	O
applying	O
equation	O
to	O
each	O
row	O
pd	O
bnj	O
b	O
j	O
where	O
nj	O
njk	O
are	O
the	O
counts	O
for	O
leaving	O
state	B
j	O
and	O
b	O
is	O
the	O
generalized	O
beta	B
function	I
we	O
can	O
fit	O
this	O
using	O
the	O
methods	O
discussed	O
in	O
however	O
we	O
can	O
also	O
use	O
the	O
following	O
approximation	O
and	O
peto	O
mk	O
njk	O
this	O
says	O
that	O
the	O
prior	O
probability	O
of	O
word	O
k	O
is	O
given	O
by	O
the	O
number	O
of	O
different	O
contexts	O
in	O
which	O
it	O
occurs	O
rather	O
than	O
the	O
number	O
of	O
times	O
it	O
occurs	O
to	O
justify	O
the	O
reasonableness	O
of	O
this	O
result	O
mackay	O
and	O
peto	O
and	O
peto	O
give	O
the	O
following	O
example	O
markov	B
models	I
figure	O
a	O
markov	B
chain	I
in	O
which	O
we	O
put	O
a	O
different	O
dirichlet	B
prior	O
on	O
every	O
row	O
of	O
the	O
transition	B
matrix	I
a	O
but	O
the	O
hyperparameters	O
of	O
the	O
dirichlet	B
are	O
shared	B
imagine	O
you	O
see	O
that	O
the	O
language	O
you	O
see	O
has	O
you	O
see	O
a	O
frequently	O
occuring	O
couplet	O
you	O
see	O
you	O
see	O
in	O
which	O
the	O
second	O
word	O
of	O
the	O
couplet	O
see	O
follows	O
the	O
first	O
word	O
you	O
with	O
very	O
high	O
probability	O
you	O
see	O
then	O
the	O
marginal	O
statistics	O
you	O
see	O
are	O
going	O
to	O
become	O
hugely	O
dominated	O
you	O
see	O
by	O
the	O
words	O
you	O
and	O
see	O
with	O
equal	O
frequency	O
you	O
see	O
if	O
we	O
use	O
the	O
standard	O
smoothing	B
formula	O
equation	O
then	O
pyounovel	O
and	O
pseenovel	O
for	O
some	O
novel	O
context	O
word	O
not	O
seen	O
before	O
would	O
turn	O
out	O
to	O
be	O
the	O
same	O
since	O
the	O
marginal	O
frequencies	O
of	O
you	O
and	O
see	O
are	O
the	O
same	O
times	O
each	O
however	O
this	O
seems	O
unreasonable	O
you	O
appears	O
in	O
many	O
contexts	O
so	O
pyounovel	O
should	O
be	O
high	O
but	O
see	O
only	O
follows	O
you	O
so	O
pseenovel	O
should	O
be	O
low	O
if	O
we	O
use	O
the	O
bayesian	B
formula	O
equation	O
we	O
will	O
get	O
this	O
effect	O
for	O
free	O
since	O
we	O
back	O
off	O
to	O
mk	O
not	O
fk	O
and	O
mk	O
will	O
be	O
large	O
for	O
you	O
and	O
small	O
for	O
see	O
by	O
equation	O
unfortunately	O
although	O
elegant	O
this	O
bayesian	B
model	O
does	O
not	O
beat	O
the	O
state-of-the-art	O
language	B
model	I
known	O
as	O
interpolated	B
kneser-ney	I
and	O
ney	O
chen	O
and	O
goodman	O
however	O
in	O
it	O
was	O
shown	O
how	O
one	O
can	O
build	O
a	O
non-parametric	O
bayesian	B
model	O
which	O
outperforms	O
interpolated	B
kneser-ney	I
by	O
using	O
variable-length	O
contexts	O
in	O
et	O
al	O
this	O
method	O
was	O
extended	O
to	O
create	O
the	O
sequence	O
memoizer	O
which	O
is	O
currently	O
the	O
best-performing	O
language	O
handling	O
out-of-vocabulary	O
words	O
while	O
the	O
above	O
smoothing	B
methods	O
handle	O
the	O
case	O
where	O
the	O
counts	O
are	O
small	O
or	O
even	O
zero	O
none	O
of	O
them	O
deal	O
with	O
the	O
case	O
where	O
the	O
test	O
set	O
may	O
contain	O
a	O
completely	O
novel	O
word	O
in	O
particular	O
they	O
all	O
assume	O
that	O
the	O
words	O
in	O
the	O
vocabulary	O
the	O
state	B
space	I
of	O
xt	O
is	O
fixed	O
and	O
known	O
it	O
is	O
the	O
set	O
of	O
unique	O
words	O
in	O
the	O
training	O
data	O
or	O
in	O
some	O
dictionary	B
interestingly	O
these	O
non-parametric	O
methods	O
are	O
based	O
on	O
posterior	O
inference	B
using	O
mcmc	B
andor	O
particle	O
filtering	B
rather	O
than	O
optimization	B
methods	O
such	O
as	O
eb	B
despite	O
this	O
they	O
are	O
quite	O
efficient	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
figure	O
some	O
markov	B
chains	O
a	O
aperiodic	B
chain	O
a	O
reducible	O
chain	O
even	O
if	O
all	O
ajk	O
s	O
are	O
non-zero	O
none	O
of	O
these	O
models	O
will	O
predict	O
a	O
novel	O
word	O
outside	O
of	O
this	O
set	O
and	O
hence	O
will	O
assign	O
zero	O
probability	O
to	O
a	O
test	O
sentence	O
with	O
an	O
unfamiliar	O
word	O
words	O
are	O
bound	O
to	O
occur	O
because	O
the	O
set	O
of	O
words	O
is	O
an	O
open	B
class	I
for	O
example	O
the	O
set	O
of	O
proper	O
nouns	O
of	O
people	O
and	O
places	O
is	O
unbounded	O
a	O
standard	O
heuristic	O
to	O
solve	O
this	O
problem	O
is	O
to	O
replace	O
all	O
novel	O
words	O
with	O
the	O
special	O
symbol	O
unk	B
which	O
stands	O
for	O
unknown	B
a	O
certain	O
amount	O
of	O
probability	O
mass	O
is	O
held	O
aside	O
for	O
this	O
event	O
a	O
more	O
principled	O
solution	O
would	O
be	O
to	O
use	O
a	O
dirichlet	B
process	I
which	O
can	O
generate	O
a	O
countably	O
infinite	O
state	B
space	I
as	O
the	O
amount	O
of	O
data	O
increases	O
section	O
if	O
all	O
novel	O
words	O
are	O
accepted	O
as	O
genuine	O
words	O
then	O
the	O
system	O
has	O
no	O
predictive	B
power	O
since	O
any	O
misspelling	O
will	O
be	O
considered	O
a	O
new	O
word	O
so	O
the	O
novel	O
word	O
has	O
to	O
be	O
seen	O
frequently	O
enough	O
to	O
warrant	O
being	O
added	O
to	O
the	O
vocabulary	O
see	O
e	O
g	O
and	O
singer	O
griffiths	O
and	O
tenenbaum	O
for	O
details	O
stationary	B
distribution	I
of	O
a	O
markov	B
chain	I
we	O
have	O
been	O
focussing	O
on	O
markov	B
models	I
as	O
a	O
way	O
of	O
defining	O
joint	O
probability	O
distributions	O
over	O
sequences	O
however	O
we	O
can	O
also	O
interpret	O
them	O
as	O
stochastic	O
dynamical	O
systems	O
where	O
we	O
hop	O
from	O
one	O
state	B
to	O
another	O
at	O
each	O
time	O
step	O
in	O
this	O
case	O
we	O
are	O
often	O
interested	O
in	O
the	O
long	O
term	O
distribution	O
over	O
states	O
which	O
is	O
known	O
as	O
the	O
stationary	B
distribution	I
of	O
the	O
chain	O
in	O
this	O
section	O
we	O
discuss	O
some	O
of	O
the	O
relevant	O
theory	O
later	O
we	O
will	O
consider	O
two	O
important	O
applications	O
google	O
s	O
pagerank	B
algorithm	O
for	O
ranking	B
web	O
pages	O
and	O
the	O
mcmc	B
algorithm	O
for	O
generating	O
samples	B
from	O
hard-to-normalize	O
probability	O
distributions	O
what	O
is	O
a	O
stationary	B
distribution	I
let	O
aij	O
pxt	O
jxt	O
i	O
be	O
the	O
one-step	O
transition	B
matrix	I
and	O
let	O
tj	O
xt	O
j	O
be	O
the	O
probability	O
of	O
being	O
in	O
state	B
j	O
at	O
time	O
t	O
it	O
is	O
conventional	O
in	O
this	O
context	O
to	O
assume	O
that	O
is	O
a	O
row	O
vector	O
if	O
we	O
have	O
an	O
initial	O
distribution	O
over	O
states	O
of	O
then	O
at	O
time	O
we	O
have	O
i	O
or	O
in	O
matrix	O
notation	O
markov	B
models	I
we	O
can	O
imagine	O
iterating	O
these	O
equations	O
if	O
we	O
ever	O
reach	O
a	O
stage	O
where	O
a	O
then	O
we	O
say	O
we	O
have	O
reached	O
the	O
stationary	B
distribution	I
called	O
the	O
invariant	B
distribution	I
or	O
equilibrium	B
distribution	I
once	O
we	O
enter	O
the	O
stationary	B
distribution	I
we	O
will	O
never	O
leave	O
for	O
example	O
consider	O
the	O
chain	O
in	O
figure	O
to	O
find	O
its	O
stationary	B
distribution	I
we	O
write	O
so	O
or	O
in	O
general	O
we	O
have	O
i	O
aij	O
jaji	O
in	O
other	O
words	O
the	O
probability	O
of	O
being	O
in	O
state	B
i	O
times	O
the	O
net	O
flow	O
out	O
of	O
state	B
i	O
must	O
equal	O
the	O
probability	O
of	O
being	O
in	O
each	O
other	O
state	B
j	O
times	O
the	O
net	O
flow	O
from	O
that	O
state	B
into	O
i	O
these	O
are	O
called	O
the	O
global	B
balance	I
equations	I
we	O
can	O
then	O
solve	O
these	O
equations	O
subject	O
to	O
the	O
constraint	O
that	O
j	O
j	O
computing	O
the	O
stationary	B
distribution	I
to	O
find	O
the	O
stationary	B
distribution	I
we	O
can	O
just	O
solve	O
the	O
eigenvector	O
equation	O
at	O
v	O
v	O
and	O
then	O
to	O
set	O
vt	O
where	O
v	O
is	O
an	O
eigenvector	O
with	O
eigenvalue	O
can	O
be	O
sure	O
such	O
an	O
eigenvector	O
exists	O
since	O
a	O
is	O
a	O
row-stochastic	O
matrix	O
so	O
also	O
recall	B
that	O
the	O
eigenvalues	O
of	O
a	O
and	O
at	O
are	O
the	O
same	O
of	O
course	O
since	O
eigenvectors	O
are	O
unique	O
only	O
up	O
to	O
constants	O
of	O
proportionality	O
we	O
must	O
normalize	O
v	O
at	O
the	O
end	O
to	O
ensure	O
it	O
sums	O
to	O
one	O
note	O
however	O
that	O
the	O
eigenvectors	O
are	O
only	O
guaranteed	O
to	O
be	O
real-valued	O
if	O
the	O
matrix	O
is	O
positive	O
aij	O
hence	O
aij	O
due	O
to	O
the	O
sum-to-one	O
constraint	O
a	O
more	O
general	O
approach	O
which	O
can	O
handle	O
chains	O
where	O
some	O
transition	O
probabilities	O
are	O
or	O
as	O
figure	O
is	O
as	O
follows	O
we	O
have	O
k	O
constraints	O
from	O
a	O
and	O
constraint	O
from	O
since	O
we	O
only	O
have	O
k	O
unknowns	O
this	O
is	O
overconstrained	O
so	O
let	O
us	O
replace	O
any	O
column	O
the	O
last	O
of	O
i	O
a	O
with	O
to	O
get	O
a	O
new	O
matrix	O
call	O
it	O
m	O
next	O
we	O
define	O
r	O
where	O
the	O
in	O
the	O
last	O
position	O
corresponds	O
to	O
the	O
column	O
of	O
all	O
in	O
m	O
we	O
then	O
solve	O
m	O
r	O
for	O
example	O
for	O
a	O
state	B
chain	O
we	O
have	O
to	O
solve	O
this	O
linear	O
system	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
for	O
the	O
chain	O
in	O
figure	O
we	O
find	O
we	O
can	O
easily	O
verify	O
this	O
is	O
correct	O
since	O
a	O
see	O
mcstatdist	O
for	O
some	O
matlab	O
code	O
unfortunately	O
not	O
all	O
chains	O
have	O
a	O
stationary	B
distribution	I
as	O
we	O
explain	O
below	O
when	O
does	O
a	O
stationary	B
distribution	I
exist	O
consider	O
the	O
chain	O
in	O
figure	O
if	O
we	O
start	O
in	O
state	B
we	O
will	O
stay	O
there	O
forever	O
since	O
is	O
anabsorbing	O
state	B
thus	O
is	O
one	O
possible	O
stationary	B
distribution	I
however	O
if	O
we	O
start	O
in	O
or	O
we	O
will	O
oscillate	O
between	O
those	O
two	O
states	O
for	O
ever	O
so	O
is	O
another	O
possible	O
stationary	B
distribution	I
if	O
we	O
start	O
in	O
state	B
we	O
could	O
end	O
up	O
in	O
either	O
of	O
the	O
above	O
stationary	B
distributions	O
we	O
see	O
from	O
this	O
example	O
that	O
a	O
necessary	O
condition	O
to	O
have	O
a	O
unique	O
stationary	B
distribution	I
is	O
that	O
the	O
state	B
transition	I
diagram	I
be	O
a	O
singly	O
connected	O
component	O
i	O
e	O
we	O
can	O
get	O
from	O
any	O
state	B
to	O
any	O
other	O
state	B
such	O
chains	O
are	O
called	O
irreducible	B
now	O
consider	O
the	O
chain	O
in	O
figure	O
this	O
is	O
irreducible	B
provided	O
suppose	O
it	O
is	O
clear	O
by	O
symmetry	O
that	O
this	O
chain	O
will	O
spend	O
of	O
its	O
time	O
in	O
each	O
state	B
thus	O
but	O
now	O
suppose	O
in	O
this	O
case	O
the	O
chain	O
will	O
oscillate	O
between	O
the	O
two	O
states	O
but	O
the	O
long-term	O
distribution	O
on	O
states	O
depends	O
on	O
where	O
you	O
start	O
from	O
if	O
we	O
start	O
in	O
state	B
then	O
on	O
every	O
odd	O
time	O
step	O
we	O
will	O
be	O
in	O
state	B
but	O
if	O
we	O
start	O
in	O
state	B
then	O
on	O
every	O
odd	O
time	O
step	O
we	O
will	O
be	O
in	O
state	B
this	O
example	O
motivates	O
the	O
following	O
definition	O
let	O
us	O
say	O
that	O
a	O
chain	O
has	O
a	O
limiting	O
if	O
this	O
holds	O
then	O
distribution	O
if	O
j	O
limn	O
an	O
ij	O
exists	O
and	O
is	O
independent	O
of	O
i	O
for	O
all	O
j	O
the	O
long-run	O
distribution	O
over	O
states	O
will	O
be	O
independent	O
of	O
the	O
starting	O
state	B
p	O
j	O
p	O
iaijt	O
j	O
as	O
t	O
i	O
let	O
us	O
now	O
characterize	O
when	O
a	O
limiting	B
distribution	I
exists	O
define	O
the	O
period	B
of	O
state	B
i	O
to	O
be	O
di	O
aiit	O
where	O
gcd	O
stands	O
for	O
greatest	B
common	I
divisor	I
i	O
e	O
the	O
largest	O
integer	O
that	O
divides	O
all	O
the	O
members	O
of	O
the	O
set	O
for	O
example	O
in	O
figure	O
we	O
have	O
and	O
we	O
say	O
a	O
state	B
i	O
is	O
aperiodic	B
if	O
di	O
sufficient	O
condition	O
to	O
ensure	O
this	O
is	O
if	O
state	B
i	O
has	O
a	O
self-loop	O
but	O
this	O
is	O
not	O
a	O
necessary	O
condition	O
we	O
say	O
a	O
chain	O
is	O
aperiodic	B
if	O
all	O
its	O
states	O
are	O
aperiodic	B
one	O
can	O
show	O
the	O
following	O
important	O
result	O
theorem	O
every	O
irreducible	B
connected	O
aperiodic	B
finite	O
state	B
markov	B
chain	I
has	O
a	O
limiting	B
distribution	I
which	O
is	O
equal	O
to	O
its	O
unique	O
stationary	B
distribution	I
a	O
special	O
case	O
of	O
this	O
result	O
says	O
that	O
every	O
regular	B
finite	O
state	B
chain	O
has	O
a	O
unique	O
stationary	B
distribution	I
where	O
a	O
regular	B
chain	O
is	O
one	O
whose	O
transition	B
matrix	I
satisfies	O
an	O
ij	O
for	O
some	O
integer	O
n	O
and	O
all	O
i	O
j	O
i	O
e	O
it	O
is	O
possible	O
to	O
get	O
from	O
any	O
state	B
to	O
any	O
other	O
state	B
in	O
n	O
steps	O
consequently	O
after	O
n	O
steps	O
the	O
chain	O
could	O
be	O
in	O
any	O
state	B
no	O
matter	O
where	O
it	O
started	O
one	O
can	O
show	O
that	O
sufficient	O
conditions	O
to	O
ensure	O
regularity	O
are	O
that	O
the	O
chain	O
be	O
irreducible	B
connected	O
and	O
that	O
every	O
state	B
have	O
a	O
self-transition	O
to	O
handle	O
the	O
case	O
of	O
markov	B
chains	O
whose	O
state-space	O
is	O
not	O
finite	O
the	O
countable	O
set	O
of	O
all	O
integers	O
or	O
all	O
the	O
uncountable	O
set	O
of	O
all	O
reals	O
we	O
need	O
to	O
generalize	B
some	O
of	O
the	O
earlier	O
markov	B
models	I
definitions	O
since	O
the	O
details	O
are	O
rather	O
technical	O
we	O
just	O
briefly	O
state	B
the	O
main	O
results	O
without	O
proof	O
see	O
e	O
g	O
and	O
stirzaker	O
for	O
details	O
for	O
a	O
stationary	B
distribution	I
to	O
exist	O
we	O
require	O
irreducibility	O
connected	O
and	O
aperiodicity	O
as	O
before	O
but	O
we	O
also	O
require	O
that	O
each	O
state	B
is	O
recurrent	B
chain	O
in	O
which	O
all	O
states	O
are	O
recurrent	B
is	O
called	O
a	O
recurrent	B
chain	O
recurrent	B
means	O
that	O
you	O
will	O
return	O
to	O
that	O
state	B
with	O
probability	O
as	O
a	O
simple	O
example	O
of	O
a	O
non-recurrent	O
state	B
a	O
transient	B
state	B
consider	O
figure	O
states	O
is	O
transient	B
because	O
one	O
immediately	O
leaves	B
it	O
and	O
either	O
spins	O
around	O
state	B
forever	O
or	O
oscillates	O
between	O
states	O
and	O
forever	O
there	O
is	O
no	O
way	O
to	O
return	O
to	O
state	B
it	O
is	O
clear	O
that	O
any	O
finite-state	O
irreducible	B
chain	O
is	O
recurrent	B
since	O
you	O
can	O
always	O
get	O
back	O
to	O
where	O
you	O
started	O
from	O
but	O
now	O
consider	O
an	O
example	O
with	O
an	O
infinite	O
state	B
space	I
suppose	O
we	O
perform	O
a	O
random	B
walk	I
on	I
the	I
integers	I
x	O
let	O
p	O
be	O
the	O
probability	O
of	O
moving	O
right	O
and	O
aii	O
p	O
be	O
the	O
probability	O
of	O
moving	O
left	O
suppose	O
we	O
start	O
at	O
if	O
p	O
we	O
will	O
shoot	O
off	O
to	O
we	O
are	O
not	O
guaranteed	O
to	O
return	O
similarly	O
if	O
p	O
we	O
will	O
shoot	O
off	O
to	O
so	O
in	O
both	O
cases	O
the	O
chain	O
is	O
not	O
recurrent	B
even	O
though	O
it	O
is	O
irreducible	B
it	O
should	O
be	O
intuitively	O
obvious	O
that	O
we	O
require	O
all	O
states	O
to	O
be	O
recurrent	B
for	O
a	O
stationary	B
distribution	I
to	O
exist	O
however	O
this	O
is	O
not	O
sufficient	O
to	O
see	O
this	O
consider	O
the	O
random	B
walk	I
on	I
the	I
integers	I
again	O
and	O
suppose	O
p	O
in	O
this	O
case	O
we	O
can	O
return	O
to	O
the	O
origin	O
an	O
infinite	O
number	O
of	O
times	O
so	O
the	O
chain	O
is	O
recurrent	B
however	O
it	O
takes	O
infinitely	O
long	O
to	O
do	O
so	O
this	O
prohibits	O
it	O
from	O
having	O
a	O
stationary	B
distribution	I
the	O
intuitive	O
reason	O
is	O
that	O
the	O
distribution	O
keeps	O
spreading	O
out	O
over	O
a	O
larger	O
and	O
larger	O
set	O
of	O
the	O
integers	O
and	O
never	O
converges	O
to	O
a	O
stationary	B
distribution	I
more	O
formally	O
we	O
define	O
a	O
state	B
to	O
be	O
non-null	B
recurrent	B
if	O
the	O
expected	O
time	O
to	O
return	O
to	O
this	O
state	B
is	O
finite	O
a	O
chain	O
in	O
which	O
all	O
states	O
are	O
non-null	O
is	O
called	O
a	O
non-null	O
chain	O
for	O
brevity	O
we	O
we	O
say	O
that	O
a	O
state	B
is	O
ergodic	B
if	O
it	O
is	O
aperiodic	B
recurrent	B
and	O
non-null	O
and	O
we	O
say	O
a	O
chain	O
is	O
ergodic	B
if	O
all	O
its	O
states	O
are	O
ergodic	B
we	O
can	O
now	O
state	B
our	O
main	O
theorem	O
theorem	O
every	O
irreducible	B
connected	O
ergodic	B
markov	B
chain	I
has	O
a	O
limiting	B
distribution	I
which	O
is	O
equal	O
to	O
its	O
unique	O
stationary	B
distribution	I
this	O
generalizes	O
theorem	O
since	O
for	O
irreducible	B
finite-state	O
chains	O
all	O
states	O
are	O
recurrent	B
and	O
non-null	O
detailed	B
balance	I
establishing	O
ergodicity	O
can	O
be	O
difficult	O
we	O
now	O
give	O
an	O
alternative	O
condition	O
that	O
is	O
easier	O
to	O
verify	O
we	O
say	O
that	O
a	O
markov	B
chain	I
a	O
is	O
time	B
reversible	I
if	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
iaij	O
jaji	O
these	O
are	O
called	O
the	O
detailed	B
balance	I
equations	I
this	O
says	O
that	O
the	O
flow	O
from	O
i	O
to	O
j	O
must	O
equal	O
the	O
flow	O
from	O
j	O
to	O
i	O
weighted	O
by	O
the	O
appropriate	O
source	O
probabilities	O
we	O
have	O
the	O
following	O
important	O
result	O
theorem	O
balance	O
wrt	O
distribution	O
then	O
is	O
a	O
stationary	B
distribution	I
of	O
the	O
chain	O
if	O
a	O
markov	B
chain	I
with	O
transition	B
matrix	I
a	O
is	O
regular	B
and	O
satisfies	O
detailed	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
figure	O
a	O
very	O
small	O
world	O
wide	O
web	O
figure	O
generated	O
by	O
pagerankdemo	O
written	O
by	O
tim	O
davis	O
proof	O
to	O
see	O
this	O
note	O
that	O
iaij	O
jaji	O
j	O
i	O
i	O
and	O
hence	O
a	O
i	O
aji	O
j	O
note	O
that	O
this	O
condition	O
is	O
sufficient	O
but	O
not	O
necessary	O
figure	O
for	O
an	O
example	O
of	O
a	O
chain	O
with	O
a	O
stationary	B
distribution	I
which	O
does	O
not	O
satisfy	O
detailed	B
balance	I
in	O
section	O
we	O
will	O
discuss	O
markov	B
chain	I
monte	B
carlo	I
or	O
mcmc	B
methods	O
these	O
take	O
as	O
input	O
a	O
desired	O
distribution	O
and	O
construct	O
a	O
transition	B
matrix	I
in	O
general	O
a	O
transition	O
kernel	B
a	O
which	O
satisfies	O
detailed	B
balance	I
wrt	O
thus	O
by	O
sampling	O
states	O
from	O
such	O
a	O
chain	O
we	O
will	O
eventually	O
enter	O
the	O
stationary	B
distribution	I
and	O
will	O
visit	O
states	O
with	O
probabilities	O
given	O
by	O
application	O
google	O
s	O
pagerank	B
algorithm	O
for	O
web	O
page	O
ranking	B
the	O
results	O
in	O
section	O
form	O
the	O
theoretical	O
underpinnings	O
to	O
google	O
s	O
pagerank	B
algorithm	O
which	O
is	O
used	O
for	O
information	B
retrieval	I
on	O
the	O
world-wide	O
web	O
we	O
sketch	O
the	O
basic	O
idea	O
below	O
see	O
and	O
leise	O
for	O
a	O
more	O
detailed	O
explanation	O
we	O
will	O
treat	O
the	O
web	O
as	O
a	O
giant	O
directed	B
graph	B
where	O
nodes	B
represent	O
web	O
pages	O
and	O
edges	B
represent	O
we	O
then	O
perform	O
a	O
process	O
called	O
web	B
crawling	I
we	O
start	O
at	O
a	O
few	O
designated	O
root	B
nodes	B
such	O
as	O
dmoz	O
org	O
the	O
home	O
of	O
the	O
open	B
directory	I
project	I
and	O
then	O
follows	O
the	O
links	O
storing	O
all	O
the	O
pages	O
that	O
we	O
encounter	O
until	O
we	O
run	O
out	O
of	O
time	O
next	O
all	O
of	O
the	O
words	O
in	O
each	O
web	O
page	O
are	O
entered	O
into	O
a	O
data	O
structure	O
called	O
an	O
inverted	B
index	I
that	O
is	O
for	O
each	O
word	O
we	O
store	O
a	O
list	O
of	O
the	O
documents	O
where	O
this	O
word	O
occurs	O
practice	O
we	O
store	O
a	O
list	O
of	O
hash	O
codes	O
representing	O
the	O
urls	O
at	O
test	O
time	O
when	O
a	O
user	O
enters	O
in	O
google	O
said	O
it	O
had	O
indexed	O
trillion	O
unique	O
urls	O
if	O
we	O
assume	O
there	O
are	O
about	O
urls	O
per	O
page	O
average	O
this	O
means	O
there	O
were	O
about	O
billion	O
unique	O
web	O
pages	O
estimates	O
for	O
are	O
about	O
billion	O
unique	O
web	O
pages	O
source	O
markov	B
models	I
a	O
query	O
we	O
can	O
just	O
look	O
up	O
all	O
the	O
documents	O
containing	O
each	O
word	O
and	O
intersect	O
these	O
lists	O
queries	O
are	O
defined	O
by	O
a	O
conjunction	O
of	O
search	O
terms	O
we	O
can	O
get	O
a	O
refined	O
search	O
by	O
storing	O
the	O
location	O
of	O
each	O
word	O
in	O
each	O
document	O
we	O
can	O
then	O
test	O
if	O
the	O
words	O
in	O
a	O
document	O
occur	O
in	O
the	O
same	O
order	O
as	O
in	O
the	O
query	O
let	O
us	O
give	O
an	O
example	O
from	O
httpen	O
wikipedia	O
orgwikiinverted	O
index	O
we	O
have	O
documents	O
it	O
is	O
what	O
it	O
is	O
what	O
is	O
it	O
and	O
it	O
is	O
a	O
banana	O
then	O
we	O
can	O
create	O
the	O
following	O
inverted	B
index	I
where	O
each	O
pair	O
represents	O
a	O
document	O
and	O
word	O
location	O
for	O
example	O
we	O
see	O
that	O
the	O
word	O
what	O
occurs	O
at	O
location	O
from	O
in	O
document	O
and	O
location	O
in	O
document	O
suppose	O
we	O
search	O
for	O
what	O
is	O
it	O
if	O
we	O
ignore	O
word	O
order	O
we	O
retrieve	O
the	O
following	O
documents	O
if	O
we	O
require	O
that	O
the	O
word	O
order	O
matches	O
only	O
document	O
would	O
be	O
returned	O
more	O
generally	O
we	O
can	O
allow	O
out-of-order	O
matches	O
but	O
can	O
give	O
bonus	O
points	O
to	O
documents	O
whose	O
word	O
order	O
matches	O
the	O
query	O
s	O
word	O
order	O
or	O
to	O
other	O
features	B
such	O
as	O
if	O
the	O
words	O
occur	O
in	O
the	O
title	O
of	O
a	O
document	O
we	O
can	O
then	O
return	O
the	O
matching	O
documents	O
in	O
decreasing	O
order	O
of	O
their	O
score	O
relevance	O
this	O
is	O
called	O
document	O
ranking	B
so	O
far	O
we	O
have	O
described	O
the	O
standard	O
process	O
of	O
information	B
retrieval	I
but	O
the	O
link	O
structure	O
of	O
the	O
web	O
provides	O
an	O
additional	O
source	O
of	O
information	B
the	O
basic	O
idea	O
is	O
that	O
some	O
web	O
pages	O
are	O
more	O
authoritative	O
than	O
others	O
so	O
these	O
should	O
be	O
ranked	O
higher	O
they	O
match	O
the	O
query	O
a	O
web	O
page	O
is	O
an	O
authority	O
if	O
it	O
is	O
linked	O
to	O
by	O
many	O
other	O
pages	O
but	O
to	O
protect	O
against	O
the	O
effect	O
of	O
so-called	O
link	B
farms	I
which	O
are	O
dummy	O
pages	O
which	O
just	O
link	O
to	O
a	O
given	O
site	O
to	O
boost	O
its	O
apparent	O
relevance	O
we	O
will	O
weight	O
each	O
incoming	O
link	O
by	O
the	O
source	O
s	O
authority	O
thus	O
we	O
get	O
the	O
following	O
recursive	B
definition	O
for	O
the	O
authoritativeness	O
of	O
page	O
j	O
also	O
called	O
its	O
pagerank	B
j	O
aij	O
i	O
i	O
where	O
aij	O
is	O
the	O
probability	O
of	O
following	O
a	O
link	O
from	O
i	O
to	O
j	O
we	O
recognize	O
equation	O
as	O
the	O
stationary	B
distribution	I
of	O
a	O
markov	B
chain	I
in	O
the	O
simplest	O
setting	O
we	O
define	O
ai	B
as	O
a	O
uniform	B
distribution	I
over	O
all	O
states	O
that	O
i	O
is	O
connected	O
to	O
however	O
to	O
ensure	O
the	O
distribution	O
is	O
unique	O
we	O
need	O
to	O
make	O
the	O
chain	O
into	O
a	O
regular	B
chain	O
this	O
can	O
be	O
done	O
by	O
allowing	O
each	O
state	B
i	O
to	O
jump	O
to	O
any	O
other	O
state	B
itself	O
with	O
some	O
small	O
probability	O
this	O
effectively	O
makes	O
the	O
transition	B
matrix	I
aperiodic	B
and	O
fully	O
connected	O
the	O
adjacency	B
matrix	I
gij	O
of	O
the	O
web	O
itself	O
is	O
highly	O
sparse	B
we	O
discuss	O
efficient	O
methods	O
for	O
computing	O
the	O
leading	O
eigenvector	O
of	O
this	O
giant	O
matrix	O
below	O
but	O
first	O
let	O
us	O
give	O
an	O
example	O
of	O
the	O
pagerank	B
algorithm	O
consider	O
the	O
small	O
web	O
in	O
figure	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
nz	O
figure	O
web	O
graph	B
of	O
sites	O
rooted	O
at	O
www	O
harvard	O
edu	O
corresponding	O
page	O
rank	O
vector	O
figure	O
generated	O
by	O
pagerankdemopmtk	O
based	O
on	O
code	O
by	O
cleve	O
moler	O
we	O
find	O
that	O
the	O
stationary	B
distribution	I
is	O
so	O
a	O
random	O
surfer	O
will	O
visit	O
site	O
about	O
of	O
the	O
time	O
we	O
see	O
that	O
node	O
has	O
a	O
higher	O
pagerank	B
than	O
nodes	B
or	O
even	O
though	O
they	O
all	O
have	O
the	O
same	O
number	O
of	O
in-links	O
this	O
is	O
because	O
being	O
linked	O
to	O
from	O
an	O
influential	O
nodehelps	O
increase	O
your	O
pagerank	B
score	O
more	O
than	O
being	O
linked	O
to	O
by	O
a	O
less	O
influential	O
node	O
as	O
a	O
slightly	O
larger	O
example	O
figure	O
shows	O
a	O
web	O
graph	B
derived	O
from	O
the	O
root	B
of	O
harvard	O
edu	O
figure	O
shows	O
the	O
corresponding	O
pagerank	B
vector	O
efficiently	O
computing	O
the	O
pagerank	B
vector	O
let	O
gij	O
iff	B
there	O
is	O
a	O
link	O
from	O
j	O
to	O
i	O
now	O
imagine	O
performing	O
a	O
random	O
walk	O
on	O
this	O
graph	B
where	O
at	O
every	O
time	O
step	O
with	O
probability	O
p	O
you	O
follow	O
one	O
of	O
the	O
outlinks	O
uniformly	O
at	O
random	O
and	O
with	O
probability	O
p	O
you	O
jump	O
to	O
a	O
random	O
node	O
again	O
chosen	O
uniformly	O
at	O
random	O
if	O
there	O
are	O
no	O
outlinks	O
you	O
just	O
jump	O
to	O
a	O
random	O
page	O
random	O
jumps	O
including	O
self-transitions	O
ensure	O
the	O
chain	O
is	O
irreducible	B
connected	O
and	O
regular	B
hence	O
we	O
can	O
solve	O
for	O
its	O
unique	O
stationary	B
distribution	I
using	O
eigenvector	O
methods	O
this	O
defines	O
the	O
following	O
transition	B
matrix	I
if	O
cj	O
if	O
cj	O
where	O
n	O
is	O
the	O
number	O
of	O
nodes	B
pn	O
is	O
the	O
probability	O
of	O
jumping	O
from	O
one	O
page	O
i	O
gij	O
represents	O
the	O
out-degree	B
of	O
page	O
j	O
to	O
another	O
without	O
following	O
a	O
link	O
and	O
cj	O
n	O
and	O
p	O
then	O
here	O
m	O
is	O
a	O
stochastic	B
matrix	I
in	O
which	O
columns	O
sum	O
to	O
one	O
note	O
that	O
m	O
at	O
in	O
our	O
earlier	O
notation	O
mij	O
pgijcj	O
we	O
can	O
represent	O
the	O
transition	B
matrix	I
compactly	O
as	O
follows	O
define	O
the	O
diagonal	B
matrix	O
d	O
with	O
entries	O
djj	O
if	O
cj	O
if	O
cj	O
hidden	B
markov	B
models	I
define	O
the	O
vector	O
z	O
with	O
components	O
zj	O
if	O
cj	O
if	O
cj	O
then	O
we	O
can	O
rewrite	O
equation	O
as	O
follows	O
m	O
pgd	O
the	O
matrix	O
m	O
is	O
not	O
sparse	B
but	O
it	O
is	O
a	O
rank	O
one	O
modification	O
of	O
a	O
sparse	B
matrix	O
most	O
of	O
the	O
elements	O
of	O
m	O
are	O
equal	O
to	O
the	O
small	O
constant	O
obviously	O
these	O
do	O
not	O
need	O
to	O
be	O
stored	O
explicitly	O
our	O
goal	O
is	O
to	O
solve	O
v	O
mv	O
where	O
v	O
t	O
one	O
efficient	O
method	O
to	O
find	O
the	O
leading	O
eigenvector	O
of	O
a	O
large	O
matrix	O
is	O
known	O
as	O
the	O
power	B
method	I
this	O
simply	O
consists	O
of	O
repeated	O
matrix-vector	O
multiplication	O
followed	O
by	O
normalization	O
v	O
mv	O
pgdv	O
v	O
it	O
is	O
possible	O
to	O
implement	O
the	O
power	B
method	I
without	O
using	O
any	O
matrix	O
multiplications	O
by	O
simply	O
sampling	O
from	O
the	O
transition	B
matrix	I
and	O
counting	O
how	O
often	O
you	O
visit	O
each	O
state	B
this	O
is	O
essentially	O
a	O
monte	B
carlo	I
approximation	O
to	O
the	O
sum	O
implied	O
by	O
v	O
mv	O
applying	O
this	O
to	O
the	O
data	O
in	O
figure	O
yields	O
the	O
stationary	B
distribution	I
in	O
figure	O
this	O
took	O
iterations	O
to	O
also	O
the	O
function	O
pagerankdemo	O
by	O
tim	O
converge	B
starting	O
from	O
a	O
uniform	B
distribution	I
davis	O
for	O
an	O
animation	O
of	O
the	O
algorithm	O
in	O
action	B
applied	O
to	O
the	O
small	O
web	O
example	O
to	O
handle	O
changing	O
web	O
structure	O
we	O
can	O
re-run	O
this	O
algorithm	O
every	O
day	O
or	O
every	O
week	O
starting	O
v	O
off	O
at	O
the	O
old	O
distribution	O
and	O
meyer	O
for	O
details	O
on	O
how	O
to	O
perform	O
this	O
monte	B
carlo	I
power	B
method	I
in	O
a	O
parallel	O
distributed	O
computing	O
environment	O
see	O
e	O
g	O
and	O
ullman	O
web	B
spam	B
pagerank	B
is	O
not	O
foolproof	O
for	O
example	O
consider	O
the	O
strategy	O
adopted	O
by	O
jc	B
penney	I
a	O
department	O
store	O
in	O
the	O
usa	O
during	O
the	O
christmas	O
season	O
of	O
it	O
planted	O
many	O
links	O
to	O
its	O
home	O
page	O
on	O
of	O
irrelevant	O
web	O
pages	O
thus	O
increasing	O
its	O
ranking	B
on	O
google	O
s	O
search	O
engine	O
even	O
though	O
each	O
of	O
these	O
source	O
pages	O
has	O
low	O
pagerank	B
there	O
were	O
so	O
many	O
of	O
them	O
that	O
their	O
effect	O
added	O
up	O
businesses	O
call	O
this	O
search	B
engine	I
optimization	B
google	O
calls	O
it	O
web	B
spam	B
when	O
google	O
was	O
notified	O
of	O
this	O
scam	O
the	O
new	O
york	O
times	O
it	O
manually	O
downweighted	O
jc	B
penney	I
since	O
such	O
behavior	O
violates	O
google	O
s	O
code	O
of	O
conduct	O
the	O
result	O
was	O
that	O
jc	B
penney	I
dropped	O
from	O
rank	O
to	O
rank	O
essentially	O
making	O
it	O
disappear	O
from	O
view	O
automatically	O
detecting	O
such	O
scams	O
relies	O
on	O
various	O
techniques	O
which	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
hidden	B
markov	B
models	I
as	O
we	O
mentioned	O
in	O
section	O
a	O
hidden	B
markov	B
model	I
or	O
hmm	B
consists	O
of	O
a	O
discrete-time	O
discrete-state	O
markov	B
chain	I
with	O
hidden	B
states	O
zt	O
k	O
plus	O
an	O
observation	B
model	I
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
figure	O
hidden	B
state	B
sequence	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
hmmlillypaddemo	O
some	O
data	O
sampled	O
from	O
a	O
state	B
hmm	B
each	O
state	B
emits	O
from	O
a	O
gaussian	B
the	O
pxtzt	O
the	O
corresponding	O
joint	B
distribution	I
has	O
the	O
form	O
pztzt	O
pxtzt	O
the	O
observations	O
in	O
an	O
hmm	B
can	O
be	O
discrete	B
or	O
continuous	O
if	O
they	O
are	O
discrete	B
it	O
is	O
common	O
for	O
the	O
observation	B
model	I
to	O
be	O
an	O
observation	B
matrix	O
pxt	O
lzt	O
k	O
bk	O
l	O
if	O
the	O
observations	O
are	O
continuous	O
it	O
is	O
common	O
for	O
the	O
observation	B
model	I
to	O
be	O
a	O
conditional	B
gaussian	B
pxtzt	O
k	O
n	O
k	O
k	O
figure	O
shows	O
an	O
example	O
where	O
we	O
have	O
states	O
each	O
of	O
which	O
emits	O
a	O
different	O
gaussian	B
the	O
resulting	O
model	O
is	O
similar	B
to	O
a	O
gaussian	B
mixture	B
model	I
except	O
the	O
cluster	O
membership	O
hmms	B
are	O
sometimes	O
called	O
markov	B
switching	I
models	I
has	O
markovian	O
dynamics	O
we	O
see	O
that	O
we	O
tend	O
to	O
get	O
multiple	O
observations	O
in	O
the	O
same	O
location	O
and	O
then	O
a	O
sudden	O
jump	O
to	O
a	O
new	O
cluster	O
applications	O
of	O
hmms	B
hmms	B
can	O
be	O
used	O
as	O
black-box	B
density	O
models	O
on	O
sequences	O
they	O
have	O
the	O
advantage	O
over	O
markov	B
models	I
in	O
that	O
they	O
can	O
represent	O
long-range	O
dependencies	O
between	O
observations	O
mediated	O
via	O
the	O
latent	B
variables	O
in	O
particular	O
note	O
that	O
they	O
do	O
not	O
assume	O
the	O
markov	B
property	O
holds	O
for	O
the	O
observations	O
themselves	O
such	O
black-box	B
models	O
are	O
useful	O
for	O
timeseries	O
prediction	O
they	O
can	O
also	O
be	O
used	O
to	O
define	O
class-conditional	O
densities	O
inside	O
a	O
generative	O
classifier	O
however	O
it	O
is	O
more	O
common	O
to	O
imbue	O
the	O
hidden	B
states	O
with	O
some	O
desired	O
meaning	O
and	O
to	O
then	O
try	O
to	O
estimate	O
the	O
hidden	B
states	O
from	O
the	O
observations	O
i	O
e	O
to	O
compute	O
if	O
we	O
are	O
hidden	B
markov	B
models	I
bat	O
rat	O
cat	O
gnat	O
goat	O
d	O
i	O
m	O
m	O
i	O
begin	O
x	O
x	O
x	O
a	O
g	O
c	O
a	O
a	O
g	O
c	O
a	O
g	O
a	O
a	O
a	O
a	O
a	O
c	O
c	O
a	O
g	O
d	O
i	O
m	O
m	O
d	O
i	O
m	O
end	O
figure	O
some	O
dna	B
sequences	I
state	B
transition	I
diagram	I
for	O
a	O
profile	O
hmm	B
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
richard	O
durbin	O
in	O
an	O
online	O
scenario	O
or	O
if	O
we	O
are	O
in	O
an	O
offline	B
scenario	O
section	O
for	O
further	O
discussion	O
of	O
the	O
differences	O
between	O
these	O
two	O
approaches	O
below	O
we	O
give	O
some	O
examples	O
of	O
applications	O
which	O
use	O
hmms	B
in	O
this	O
way	O
automatic	B
speech	B
recognition	I
here	O
xt	O
represents	O
features	B
extracted	O
from	O
the	O
speech	O
signal	O
and	O
zt	O
represents	O
the	O
word	O
that	O
is	O
being	O
spoken	O
the	O
transition	B
model	I
pztzt	O
represents	O
the	O
language	B
model	I
and	O
the	O
observation	B
model	I
pxtzt	O
represents	O
the	O
acoustic	O
model	O
see	O
e	O
g	O
jurafsky	O
and	O
martin	O
for	O
details	O
activity	B
recognition	I
here	O
xt	O
represents	O
features	B
extracted	O
from	O
a	O
video	O
frame	O
and	O
zt	O
is	O
the	O
class	O
of	O
activity	O
the	O
person	O
is	O
engaged	O
in	O
running	O
walking	O
sitting	O
etc	O
see	O
e	O
g	O
for	O
details	O
part	B
of	I
speech	I
tagging	I
here	O
xt	O
represents	O
a	O
word	O
and	O
zt	O
represents	O
its	O
part	B
of	I
speech	I
verb	O
adjective	O
etc	O
see	O
section	O
for	O
more	O
information	B
on	O
pos	O
tagging	O
and	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
related	O
tasks	O
gene	O
finding	O
here	O
xt	O
represents	O
the	O
dna	O
nucleotides	O
and	O
zt	O
represents	O
whether	O
we	O
are	O
inside	O
a	O
gene-coding	O
region	O
or	O
not	O
see	O
e	O
g	O
et	O
al	O
for	O
details	O
protein	B
sequence	I
alignment	B
here	O
xt	O
represents	O
an	O
amino	O
acid	O
and	O
zt	O
represents	O
whether	O
this	O
matches	O
the	O
latent	B
consensus	B
sequence	I
at	O
this	O
location	O
this	O
model	O
is	O
called	O
a	O
profile	O
hmm	B
and	O
is	O
illustrated	O
in	O
figure	O
the	O
hmm	B
has	O
states	O
called	O
match	O
insert	O
and	O
delete	O
if	O
zt	O
is	O
a	O
match	O
state	B
then	O
xt	O
is	O
equal	O
to	O
the	O
t	O
th	O
value	O
of	O
the	O
consensus	O
if	O
zt	O
is	O
an	O
insert	O
state	B
then	O
xt	O
is	O
generated	O
from	O
a	O
uniform	B
distribution	I
that	O
is	O
unrelated	O
to	O
the	O
consensus	B
sequence	I
if	O
zt	O
is	O
a	O
delete	O
state	B
then	O
xt	O
in	O
this	O
way	O
we	O
can	O
generate	O
noisy	O
copies	O
of	O
the	O
consensus	B
sequence	I
of	O
different	O
lengths	O
in	O
figure	O
the	O
consensus	O
is	O
agc	O
and	O
we	O
see	O
various	O
versions	O
of	O
this	O
below	O
a	O
path	B
through	O
the	O
state	B
transition	I
diagram	I
shown	O
in	O
figure	O
specifies	O
how	O
to	O
align	O
a	O
sequence	O
to	O
the	O
consensus	O
e	O
g	O
for	O
the	O
gnat	O
the	O
most	O
probable	O
path	B
is	O
d	O
d	O
i	O
i	O
i	O
m	O
this	O
means	O
we	O
delete	O
the	O
a	O
and	O
g	O
parts	O
of	O
the	O
consensus	B
sequence	I
we	O
insert	O
a	O
s	O
and	O
then	O
we	O
match	O
the	O
final	O
c	O
we	O
can	O
estimate	O
the	O
model	O
parameters	O
by	O
counting	O
the	O
number	O
of	O
such	O
transitions	O
and	O
the	O
number	O
of	O
emissions	O
from	O
each	O
kind	O
of	O
state	B
as	O
shown	O
in	O
figure	O
see	O
section	O
for	O
more	O
information	B
on	O
training	O
an	O
hmm	B
and	O
et	O
al	O
for	O
details	O
on	O
profile	O
hmms	B
note	O
that	O
for	O
some	O
of	O
these	O
tasks	O
conditional	O
random	O
fields	O
which	O
are	O
essentially	O
discrimi	O
native	O
versions	O
of	O
hmms	B
may	O
be	O
more	O
suitable	O
see	O
chapter	O
for	O
details	O
inference	B
in	O
hmms	B
we	O
now	O
discuss	O
how	O
to	O
infer	O
the	O
hidden	B
state	B
sequence	O
of	O
an	O
hmm	B
assuming	O
the	O
parameters	O
are	O
known	O
exactly	O
the	O
same	O
algorithms	O
apply	O
to	O
other	O
chain-structured	O
graphical	B
models	I
such	O
as	O
chain	O
crfs	O
section	O
in	O
chapter	O
we	O
generalize	B
these	O
methods	O
to	O
arbitrary	O
graphs	O
and	O
in	O
section	O
we	O
show	O
how	O
we	O
can	O
use	O
the	O
output	O
of	O
inference	B
in	O
the	O
context	O
of	O
parameter	B
estimation	O
types	O
of	O
inference	B
problems	O
for	O
temporal	O
models	O
there	O
are	O
several	O
different	O
kinds	O
of	O
inferential	O
tasks	O
for	O
an	O
hmm	B
ssm	B
in	O
general	O
to	O
illustrate	O
the	O
differences	O
we	O
will	O
consider	O
an	O
example	O
called	O
the	O
occasionally	B
dishonest	I
casino	I
in	O
this	O
model	O
xt	O
represents	O
which	O
dice	O
face	O
shows	O
from	O
et	O
al	O
up	O
and	O
zt	O
represents	O
the	O
identity	O
of	O
the	O
dice	O
that	O
is	O
being	O
used	O
most	O
of	O
the	O
time	O
the	O
casino	O
uses	O
a	O
fair	O
dice	O
z	O
but	O
occasionally	O
it	O
switches	O
to	O
a	O
loaded	O
dice	O
z	O
for	O
a	O
short	O
period	B
if	O
z	O
the	O
observation	B
distribution	O
is	O
a	O
uniform	O
multinoulli	O
over	O
the	O
symbols	O
if	O
z	O
the	O
observation	B
distribution	O
is	O
skewed	O
towards	O
face	O
figure	O
if	O
we	O
sample	O
from	O
this	O
model	O
we	O
may	O
observe	O
data	O
such	O
as	O
the	O
following	O
rolls	O
die	O
listing	O
example	O
output	O
of	O
casinodemo	O
llllllllllllllffffffllllllllllllllffffffffffffffffffllllllll	O
here	O
rolls	O
refers	O
to	O
the	O
observed	O
symbol	O
and	O
die	O
refers	O
to	O
the	O
hidden	B
state	B
is	O
loaded	O
and	O
f	O
is	O
fair	O
thus	O
we	O
see	O
that	O
the	O
model	O
generates	O
a	O
sequence	O
of	O
symbols	O
but	O
the	O
statistics	O
of	O
the	O
inference	B
in	O
hmms	B
figure	O
an	O
hmm	B
for	O
the	O
occasionally	B
dishonest	I
casino	I
the	O
blue	O
arrows	O
visualize	O
the	O
state	B
transition	I
diagram	I
a	O
based	O
on	O
et	O
al	O
filtered	O
smoothed	O
viterbi	B
d	O
e	O
d	O
a	O
o	O
l	O
p	O
d	O
e	O
d	O
a	O
o	O
l	O
p	O
roll	O
number	O
roll	O
number	O
d	O
e	O
d	O
a	O
o	O
l	O
f	O
r	O
i	O
a	O
e	O
t	O
a	O
t	O
s	O
p	O
a	O
m	O
roll	O
number	O
figure	O
inference	B
in	O
the	O
dishonest	O
casino	O
vertical	O
gray	O
bars	O
denote	O
the	O
samples	B
that	O
we	O
generated	O
using	O
a	O
loaded	O
die	O
map	O
trajectory	O
figure	O
generated	O
by	O
casinodemo	O
filtered	O
estimate	O
of	O
probability	O
of	O
using	O
a	O
loaded	O
dice	O
smoothed	O
estimates	O
distribution	O
changes	O
abruptly	O
every	O
now	O
and	O
then	O
in	O
a	O
typical	O
application	O
we	O
just	O
see	O
the	O
rolls	O
and	O
want	O
to	O
infer	O
which	O
dice	O
is	O
being	O
used	O
but	O
there	O
are	O
different	O
kinds	O
of	O
inference	B
which	O
we	O
summarize	O
below	O
filtering	B
means	O
to	O
compute	O
the	O
belief	B
state	B
online	O
or	O
recursively	O
as	O
the	O
data	O
streams	O
in	O
this	O
is	O
called	O
filtering	B
because	O
it	O
reduces	O
the	O
noise	O
more	O
than	O
simply	O
estimating	O
the	O
hidden	B
state	B
using	O
just	O
the	O
current	O
estimate	O
pztxt	O
we	O
will	O
see	O
below	O
that	O
we	O
can	O
perform	O
filtering	B
by	O
simply	O
applying	O
bayes	B
rule	I
in	O
a	O
sequential	B
fashion	O
see	O
figure	O
for	O
an	O
example	O
smoothing	B
means	O
to	O
compute	O
offline	B
given	O
all	O
the	O
evidence	B
see	O
figure	O
for	O
an	O
example	O
by	O
conditioning	B
on	O
past	O
and	O
future	O
data	O
our	O
uncertainty	B
will	O
be	O
significantly	O
reduced	O
to	O
understand	O
this	O
intuitively	O
consider	O
a	O
detective	O
trying	O
to	O
figure	O
out	O
who	O
committed	O
a	O
crime	O
as	O
he	O
moves	O
through	O
the	O
crime	O
scene	O
his	O
uncertainty	B
is	O
high	O
until	O
he	O
finds	O
the	O
key	O
clue	O
then	O
he	O
has	O
an	O
aha	B
moment	O
his	O
uncertainty	B
is	O
reduced	O
and	O
all	O
the	O
previously	O
confusing	O
observations	O
are	O
in	O
hindsight	B
easy	O
to	O
explain	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
figure	O
the	O
main	O
kinds	O
of	O
inference	B
for	O
state-space	O
models	O
the	O
shaded	O
region	O
is	O
the	O
interval	O
for	O
which	O
we	O
have	O
data	O
the	O
arrow	O
represents	O
the	O
time	O
step	O
at	O
which	O
we	O
want	O
to	O
perform	O
inference	B
t	O
is	O
the	O
current	O
time	O
t	O
is	O
the	O
sequence	O
length	O
is	O
the	O
lag	B
and	O
h	O
is	O
the	O
prediction	O
horizon	B
see	O
text	O
for	O
details	O
fixed	B
lag	B
smoothing	B
is	O
an	O
interesting	O
compromise	O
between	O
online	O
and	O
offline	B
estimation	O
it	O
involves	O
computing	O
pzt	O
where	O
is	O
called	O
the	O
lag	B
this	O
gives	O
better	O
performance	O
than	O
filtering	B
but	O
incurs	O
a	O
slight	O
delay	O
by	O
changing	O
the	O
size	O
of	O
the	O
lag	B
one	O
can	O
trade	O
off	O
accuracy	O
vs	O
delay	O
prediction	O
instead	O
of	O
predicting	O
the	O
past	O
given	O
the	O
future	O
as	O
in	O
fixed	B
lag	B
smoothing	B
we	O
might	O
want	O
to	O
predict	O
the	O
future	O
given	O
the	O
past	O
i	O
e	O
to	O
compute	O
where	O
h	O
is	O
called	O
the	O
prediction	O
horizon	B
for	O
example	O
suppose	O
h	O
then	O
we	O
have	O
zt	O
it	O
is	O
straightforward	O
to	O
perform	O
this	O
computation	O
we	O
just	O
power	O
up	O
the	O
transition	B
matrix	I
and	O
apply	O
it	O
to	O
the	O
current	O
belief	B
state	B
the	O
quantity	O
is	O
a	O
prediction	O
about	O
future	O
hidden	B
states	O
it	O
can	O
be	O
converted	O
into	O
a	O
prediction	O
about	O
future	O
observations	O
using	O
zth	O
this	O
is	O
the	O
posterior	B
predictive	B
density	I
and	O
can	O
be	O
used	O
for	O
time-series	B
forecasting	I
for	O
details	O
see	O
figure	O
for	O
a	O
sketch	O
of	O
the	O
relationship	O
between	O
filtering	B
smoothing	B
and	O
prediction	O
which	O
is	O
a	O
most	O
probin	O
the	O
context	O
of	O
hmms	B
this	O
is	O
known	O
as	O
viterbi	B
decoding	B
map	O
estimation	O
this	O
means	O
computing	O
arg	O
able	O
state	B
sequence	O
inference	B
in	O
hmms	B
section	O
figure	O
illustrates	O
the	O
difference	O
between	O
filtering	B
smoothing	B
and	O
map	O
decoding	B
for	O
the	O
occasionally	B
dishonest	I
casino	I
hmm	B
we	O
see	O
that	O
the	O
smoothed	O
estimate	O
is	O
indeed	O
smoother	O
than	O
the	O
filtered	O
estimate	O
if	O
we	O
threshold	O
the	O
estimates	O
at	O
and	O
compare	O
to	O
the	O
true	O
sequence	O
we	O
find	O
that	O
the	O
filtered	O
method	O
makes	O
errors	O
out	O
of	O
and	O
the	O
smoothed	O
method	O
makes	O
the	O
map	O
path	B
makes	O
errors	O
it	O
is	O
not	O
surprising	O
that	O
smoothing	B
makes	O
fewer	O
errors	O
than	O
viterbi	B
since	O
the	O
optimal	O
way	O
to	O
minimize	O
bit-error	O
rate	B
is	O
to	O
threshold	O
the	O
posterior	O
marginals	O
section	O
nevertheless	O
for	O
some	O
applications	O
we	O
may	O
prefer	O
the	O
viterbi	B
decoding	B
as	O
we	O
discuss	O
in	O
section	O
posterior	O
samples	B
if	O
there	O
is	O
more	O
than	O
one	O
plausible	O
interpretation	O
of	O
the	O
data	O
it	O
can	O
be	O
useful	O
to	O
sample	O
from	O
the	O
posterior	O
these	O
sample	O
paths	O
contain	O
much	O
more	O
information	B
than	O
the	O
sequence	O
of	O
marginals	O
computed	O
by	O
smoothing	B
probability	B
of	I
the	I
evidence	B
we	O
can	O
compute	O
the	O
probability	B
of	I
the	I
evidence	B
this	O
can	O
be	O
used	O
to	O
by	O
summing	O
up	O
over	O
all	O
hidden	B
paths	O
classify	O
sequences	O
if	O
the	O
hmm	B
is	O
used	O
as	O
a	O
class	O
conditional	O
density	O
for	O
model-based	B
clustering	B
for	O
anomaly	O
detection	O
etc	O
the	O
forwards	O
algorithm	O
we	O
now	O
describe	O
how	O
to	O
recursively	O
compute	O
the	O
filtered	O
marginals	O
in	O
an	O
hmm	B
the	O
algorithm	O
has	O
two	O
steps	O
first	O
comes	O
the	O
prediction	O
step	O
in	O
which	O
we	O
compute	O
the	O
one-step-ahead	B
predictive	B
density	I
this	O
acts	O
as	O
the	O
new	O
prior	O
for	O
time	O
t	O
pzt	O
pzt	O
jzt	O
ipzt	O
i	O
next	O
comes	O
the	O
update	O
step	O
in	O
which	O
we	O
absorb	O
the	O
observed	O
data	O
from	O
time	O
t	O
using	O
bayes	B
rule	I
tj	O
pzt	O
pzt	O
jxt	O
pxtzt	O
zt	O
where	O
the	O
normalization	O
constant	O
is	O
given	O
by	O
zt	O
pzt	O
j	O
j	O
this	O
process	O
is	O
known	O
as	O
the	O
predict-update	B
cycle	B
the	O
distribution	O
is	O
called	O
the	O
belief	B
state	B
at	O
time	O
t	O
and	O
is	O
a	O
vector	O
of	O
k	O
numbers	O
often	O
denoted	O
by	O
t	O
in	O
matrixvector	O
notation	O
we	O
can	O
write	O
the	O
update	O
in	O
the	O
following	O
simple	O
form	O
t	O
t	O
t	O
t	O
where	O
tj	O
xtzt	O
j	O
is	O
the	O
local	B
evidence	B
at	O
time	O
t	O
j	O
zt	O
jzt	O
i	O
is	O
the	O
transition	B
matrix	I
and	O
u	O
v	O
is	O
the	O
hadamard	B
product	I
representing	O
elementwise	O
vector	O
multiplication	O
see	O
algorithm	O
for	O
the	O
pseudo-code	O
and	O
hmmfilter	O
for	O
some	O
matlab	O
code	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
in	O
addition	O
to	O
computing	O
the	O
hidden	B
states	O
we	O
can	O
use	O
this	O
algorithm	O
to	O
compute	O
the	O
log	O
probability	B
of	I
the	I
evidence	B
log	O
log	O
log	O
zt	O
need	O
to	O
work	O
in	O
the	O
log	O
domain	O
to	O
avoid	O
numerical	O
underflow	O
algorithm	O
forwards	O
algorithm	O
input	O
transition	O
matrices	O
j	O
pzt	O
jzt	O
i	O
local	B
evidence	B
vectors	O
tj	O
pxtzt	O
j	O
initial	O
state	B
distribution	O
j	O
normalize	O
for	O
t	O
do	O
return	O
and	O
log	O
t	O
zt	O
normalize	O
t	O
t	O
t	O
t	O
log	O
zt	O
subroutine	O
z	O
normalizeu	O
z	O
j	O
uj	O
vj	O
ujz	O
the	O
forwards-backwards	B
algorithm	I
in	O
section	O
we	O
explained	O
how	O
to	O
compute	O
the	O
filtered	O
marginals	O
pzt	O
using	O
online	O
inference	B
we	O
now	O
discuss	O
how	O
to	O
compute	O
the	O
smoothed	O
marginals	O
pzt	O
using	O
offline	B
inference	B
basic	O
idea	O
the	O
key	O
decomposition	O
relies	O
on	O
the	O
fact	O
that	O
we	O
can	O
break	O
the	O
chain	O
into	O
two	O
parts	O
the	O
past	O
and	O
the	O
future	O
by	O
conditioning	B
on	O
zt	O
pzt	O
pzt	O
j	O
pzt	O
let	O
tj	O
pzt	O
be	O
the	O
filtered	O
belief	B
state	B
as	O
before	O
also	O
define	O
tj	O
j	O
likelihood	B
of	O
future	O
evidence	B
given	O
that	O
the	O
hidden	B
state	B
at	O
time	O
t	O
is	O
j	O
as	O
the	O
conditional	O
that	O
this	O
is	O
not	O
a	O
probability	O
distribution	O
over	O
states	O
since	O
it	O
does	O
not	O
need	O
to	O
satisfy	O
j	O
tj	O
finally	O
define	O
tj	O
pzt	O
as	O
the	O
desired	O
smoothed	O
posterior	O
marginal	O
from	O
equation	O
we	O
have	O
tj	O
tj	O
tj	O
inference	B
in	O
hmms	B
we	O
have	O
already	O
described	O
how	O
to	O
recursively	O
compute	O
the	O
s	O
in	O
a	O
left-to-right	B
fashion	O
in	O
section	O
we	O
now	O
describe	O
how	O
to	O
recursively	O
compute	O
the	O
s	O
in	O
a	O
right-to-left	O
fashion	O
if	O
we	O
have	O
already	O
computed	O
t	O
we	O
can	O
compute	O
t	O
as	O
follows	O
t	O
xttzt	O
i	O
pzt	O
j	O
xt	O
i	O
j	O
jpxtzt	O
j	O
zt	O
ixtpzt	O
j	O
xtzt	O
i	O
zt	O
ipzt	O
jzt	O
i	O
j	O
j	O
j	O
tj	O
tj	O
j	O
j	O
we	O
can	O
write	O
the	O
resulting	O
equation	O
in	O
matrix-vector	O
form	O
as	O
t	O
t	O
t	O
the	O
base	O
case	O
is	O
t	O
pxt	O
i	O
i	O
which	O
is	O
the	O
probability	O
of	O
a	O
non-event	O
having	O
computed	O
the	O
forwards	O
and	O
backwards	O
messages	O
we	O
can	O
combine	O
them	O
to	O
compute	O
tj	O
tj	O
tj	O
the	O
overall	O
algorithm	O
is	O
known	O
as	O
the	O
forwards-backwards	B
algorithm	I
the	O
pseudo	O
code	O
is	O
very	O
similar	B
to	O
the	O
forwards	O
case	O
see	O
hmmfwdback	O
for	O
an	O
implementation	O
we	O
can	O
think	O
of	O
this	O
algorithm	O
as	O
passing	O
messages	O
from	O
left	O
to	O
right	O
and	O
then	O
from	O
right	O
to	O
left	O
and	O
then	O
combining	O
them	O
at	O
each	O
node	O
we	O
will	O
generalize	B
this	O
intuition	O
in	O
section	O
when	O
we	O
discuss	O
belief	B
propagation	I
two-slice	O
smoothed	O
marginals	O
t	O
when	O
we	O
estimate	O
the	O
parameters	O
of	O
the	O
transition	B
matrix	I
using	O
em	B
section	O
we	O
will	O
need	O
to	O
compute	O
the	O
expected	O
number	O
of	O
transitions	O
from	O
state	B
i	O
to	O
state	B
j	O
pzt	O
i	O
the	O
term	O
pzt	O
i	O
is	O
called	O
a	O
two-slice	B
marginal	I
and	O
can	O
be	O
computed	O
as	O
follows	O
e	O
i	O
t	O
nij	O
j	O
pzt	O
i	O
ti	O
j	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
in	O
matrix-vector	O
form	O
we	O
have	O
t	O
for	O
another	O
interpretation	O
of	O
these	O
equations	O
see	O
section	O
time	O
and	O
space	O
complexity	O
it	O
is	O
clear	O
that	O
a	O
straightforward	O
implementation	O
of	O
fb	O
takes	O
ok	O
time	O
since	O
we	O
must	O
perform	O
a	O
k	O
k	O
matrix	O
multiplication	O
at	O
each	O
step	O
for	O
some	O
applications	O
such	O
as	O
speech	B
recognition	I
k	O
is	O
very	O
large	O
so	O
the	O
ok	O
term	O
becomes	O
prohibitive	O
fortunately	O
if	O
the	O
in	O
a	O
left-to-right	B
transition	B
matrix	I
is	O
sparse	B
we	O
can	O
reduce	O
this	O
substantially	O
for	O
example	O
transition	B
matrix	I
the	O
algorithm	O
takes	O
ot	O
k	O
time	O
in	O
some	O
cases	O
we	O
can	O
exploit	O
special	O
properties	O
of	O
the	O
state	B
space	I
even	O
if	O
the	O
transition	B
matrix	I
is	O
not	O
sparse	B
in	O
particular	O
suppose	O
the	O
states	O
represent	O
a	O
discretization	O
of	O
an	O
underlying	O
continuous	O
state-space	O
and	O
the	O
transition	B
matrix	I
has	O
the	O
form	O
j	O
exp	O
zj	O
where	O
zi	O
is	O
the	O
continuous	O
vector	O
represented	O
by	O
state	B
i	O
then	O
one	O
can	O
implement	O
the	O
forwardsbackwards	O
algorithm	O
in	O
ot	O
k	O
log	O
k	O
time	O
this	O
is	O
very	O
useful	O
for	O
models	O
with	O
large	O
state	B
spaces	O
see	O
section	O
for	O
details	O
in	O
some	O
cases	O
the	O
bottleneck	B
is	O
memory	O
not	O
time	O
the	O
expected	B
sufficient	B
statistics	I
needed	O
t	O
t	O
j	O
this	O
takes	O
constant	O
space	O
of	O
t	O
however	O
to	O
compute	O
by	O
em	B
are	O
them	O
we	O
need	O
okt	O
working	O
space	O
since	O
we	O
must	O
store	O
t	O
for	O
t	O
t	O
until	O
we	O
do	O
the	O
backwards	O
pass	O
it	O
is	O
possible	O
to	O
devise	O
a	O
simple	O
divide-and-conquer	O
algorithm	O
that	O
reduces	O
the	O
space	O
complexity	O
from	O
okt	O
to	O
ok	O
log	O
t	O
at	O
the	O
cost	O
of	O
increasing	O
the	O
running	O
time	O
from	O
to	O
ok	O
log	O
t	O
see	O
et	O
al	O
zweig	O
and	O
padmanabhan	O
for	O
details	O
the	O
viterbi	B
algorithm	O
the	O
viterbi	B
algorithm	O
can	O
be	O
used	O
to	O
compute	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
in	O
a	O
chain-structured	O
graphical	B
model	I
i	O
e	O
it	O
can	O
compute	O
z	O
arg	O
max	O
this	O
is	O
equivalent	O
to	O
computing	O
a	O
shortest	O
path	B
through	O
the	O
trellis	B
diagram	I
in	O
figure	O
where	O
the	O
nodes	B
are	O
possible	O
states	O
at	O
each	O
time	O
step	O
and	O
the	O
node	O
and	O
edge	O
weights	O
are	O
log	O
probabilities	O
that	O
is	O
the	O
weight	O
of	O
a	O
path	B
zt	O
is	O
given	O
by	O
log	O
log	O
zt	O
log	O
tzt	O
map	O
vs	O
mpe	B
before	O
discussing	O
how	O
the	O
algorithm	O
works	O
let	O
us	O
make	O
one	O
important	O
remark	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
is	O
not	O
necessarily	O
the	O
same	O
as	O
the	O
sequence	O
of	O
most	O
probable	O
states	O
the	O
former	O
is	O
given	O
by	O
equation	O
and	O
is	O
what	O
viterbi	B
computes	O
whereas	O
the	O
latter	O
is	O
given	O
by	O
the	O
maximizer	B
of	I
the	I
posterior	I
marginals	I
or	O
mpm	B
z	O
max	O
arg	O
max	O
zt	O
inference	B
in	O
hmms	B
figure	O
the	O
trellis	B
of	O
states	O
vs	O
time	O
for	O
a	O
markov	B
chain	I
based	O
on	O
as	O
a	O
simple	O
example	O
of	O
the	O
difference	O
consider	O
a	O
chain	O
with	O
two	O
time	O
steps	O
defining	O
the	O
following	O
joint	O
the	O
joint	O
map	B
estimate	I
is	O
whereas	O
the	O
sequence	O
of	O
marginal	O
mpms	O
is	O
the	O
advantage	O
of	O
the	O
joint	O
map	B
estimate	I
is	O
that	O
is	O
is	O
always	O
globally	O
consistent	B
for	O
example	O
suppose	O
we	O
are	O
performing	O
speech	B
recognition	I
and	O
someones	O
says	O
recognize	O
speech	O
this	O
could	O
be	O
mis-heard	O
as	O
wreck	O
a	O
nice	O
beach	O
locally	O
it	O
may	O
appear	O
that	O
beach	O
is	O
the	O
most	O
probable	O
interpretation	O
of	O
that	O
particular	O
window	O
of	O
sound	O
but	O
when	O
we	O
add	O
the	O
requirement	O
that	O
the	O
data	O
be	O
explained	O
by	O
a	O
single	O
linguistically	O
plausible	O
path	B
this	O
interpretation	O
becomes	O
less	O
likely	O
on	O
the	O
other	O
hand	O
the	O
mpm	B
estimates	O
can	O
be	O
more	O
robust	B
et	O
al	O
to	O
see	O
why	O
note	O
that	O
in	O
viterbi	B
when	O
we	O
estimate	O
zt	O
we	O
max	O
out	O
the	O
other	O
variables	O
z	O
t	O
arg	O
max	O
zt	O
zt	O
max	O
whereas	O
we	O
when	O
we	O
use	O
forwards-backwards	B
we	O
sum	O
out	O
the	O
other	O
variables	O
zt	O
this	O
makes	O
the	O
mpm	B
in	O
equation	O
more	O
robust	B
since	O
we	O
estimate	O
each	O
node	O
averaging	O
over	O
its	O
neighbors	B
rather	O
than	O
conditioning	B
on	O
a	O
specific	O
value	O
of	O
its	O
in	O
general	O
we	O
may	O
want	O
to	O
mix	O
max	O
and	O
sum	O
for	O
example	O
consider	O
a	O
joint	B
distribution	I
where	O
we	O
observe	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
details	O
of	O
the	O
algorithm	O
it	O
is	O
tempting	O
to	O
think	O
that	O
we	O
can	O
implement	O
viterbi	B
by	O
just	O
replacing	O
the	O
sum-operator	O
in	O
forwards-backwards	B
with	O
a	O
max-operator	O
the	O
former	O
is	O
called	O
the	O
sum-product	B
and	O
the	O
latter	O
the	O
max-product	B
algorithm	O
if	O
there	O
is	O
a	O
unique	O
mode	B
running	O
max-product	B
and	O
then	O
computing	O
using	O
equation	O
will	O
give	O
the	O
same	O
result	O
as	O
using	O
equation	O
and	O
freeman	O
but	O
in	O
general	O
it	O
can	O
lead	O
to	O
incorrect	O
results	O
if	O
there	O
are	O
multiple	O
equally	O
probably	O
joint	O
assignments	O
the	O
reasons	O
is	O
that	O
each	O
node	O
breaks	O
ties	O
independently	O
and	O
hence	O
may	O
do	O
so	O
in	O
a	O
manner	O
that	O
is	O
inconsistent	O
with	O
its	O
neighbors	B
the	O
viterbi	B
algorithm	O
is	O
therefore	O
not	O
quite	O
as	O
simple	O
as	O
replacing	O
sum	O
with	O
max	O
in	O
particular	O
the	O
forwards	O
pass	O
does	O
use	O
maxproduct	O
but	O
the	O
backwards	O
pass	O
uses	O
a	O
traceback	B
procedure	O
to	O
recover	O
the	O
most	O
probable	O
path	B
through	O
the	O
trellis	B
of	O
states	O
essentially	O
once	O
zt	O
picks	O
its	O
most	O
probable	O
state	B
the	O
previous	O
nodes	B
condition	O
on	O
this	O
event	O
and	O
therefore	O
they	O
will	O
break	O
ties	O
consistently	O
in	O
more	O
detail	O
define	O
tj	O
max	O
zt	O
this	O
is	O
the	O
probability	O
of	O
ending	O
up	O
in	O
state	B
j	O
at	O
time	O
t	O
given	O
that	O
we	O
take	O
the	O
most	O
probable	O
path	B
the	O
key	O
insight	O
is	O
that	O
the	O
most	O
probable	O
path	B
to	O
state	B
j	O
at	O
time	O
t	O
must	O
consist	O
of	O
the	O
most	O
probable	O
path	B
to	O
some	O
other	O
state	B
i	O
at	O
time	O
t	O
followed	O
by	O
a	O
transition	O
from	O
i	O
to	O
j	O
hence	O
tj	O
max	O
i	O
t	O
j	O
tj	O
we	O
also	O
keep	O
track	O
of	O
the	O
most	O
likely	O
previous	O
state	B
for	O
each	O
possible	O
state	B
that	O
we	O
end	O
up	O
in	O
atj	O
argmax	O
i	O
t	O
j	O
tj	O
that	O
is	O
atj	O
tells	O
us	O
the	O
most	O
likely	O
previous	O
state	B
on	O
the	O
most	O
probable	O
path	B
to	O
zt	O
j	O
we	O
initialize	O
by	O
setting	O
j	O
and	O
we	O
terminate	O
by	O
computing	O
the	O
most	O
probable	O
final	O
state	B
z	O
t	O
z	O
t	O
arg	O
max	O
t	O
i	O
we	O
can	O
then	O
compute	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
using	O
traceback	B
z	O
t	O
as	O
usual	O
we	O
have	O
to	O
worry	O
about	O
numerical	O
underflow	O
we	O
are	O
free	O
to	O
normalize	O
the	O
t	O
terms	O
at	O
each	O
step	O
this	O
will	O
not	O
affect	O
the	O
maximum	O
however	O
unlike	O
the	O
forwards-backwards	B
case	O
xn	O
let	O
n	O
be	O
the	O
remaining	O
nuisance	B
variables	I
we	O
define	O
the	O
map	B
estimate	I
as	O
x	O
v	O
and	O
we	O
want	O
to	O
query	O
q	O
q	O
pxq	O
xnxv	O
where	O
we	O
max	O
over	O
xq	O
and	O
sum	O
over	O
xn	O
by	O
contrast	O
we	O
define	O
the	O
mpe	B
or	O
arg	O
maxxq	O
n	O
arg	O
maxxq	O
pxq	O
xnxv	O
where	O
we	O
max	O
over	O
both	O
xq	O
and	O
xn	O
this	O
most	O
probable	O
explanation	O
as	O
terminology	O
is	O
due	O
to	O
although	O
it	O
is	O
not	O
widely	O
used	O
outside	O
the	O
bayes	O
net	O
literatire	O
obviously	O
mapmpe	O
if	O
n	O
however	O
if	O
n	O
then	O
summing	O
out	O
the	O
nuisance	B
variables	I
can	O
give	O
different	O
results	O
than	O
maxing	O
them	O
out	O
summing	O
out	O
nuisance	B
variables	I
is	O
more	O
sensible	O
but	O
computationally	O
harder	O
because	O
of	O
the	O
need	O
to	O
combine	O
max	O
and	O
sum	O
operations	O
and	O
parr	O
q	O
x	O
inference	B
in	O
hmms	B
figure	O
illustration	O
of	O
viterbi	B
decoding	B
in	O
a	O
simple	O
hmm	B
for	O
speech	B
recognition	I
a	O
hmm	B
for	O
a	O
single	O
phone	B
we	O
are	O
visualizing	B
the	O
state	B
transition	I
diagram	I
we	O
assume	O
the	O
observations	O
have	O
been	O
vector	O
quantized	O
into	O
possible	O
symbols	O
each	O
state	B
has	O
a	O
different	O
distribution	O
over	O
these	O
symbols	O
based	O
on	O
figure	O
of	O
and	O
norvig	O
illustration	O
of	O
the	O
viterbi	B
algorithm	O
applied	O
to	O
this	O
model	O
with	O
data	O
sequence	O
the	O
columns	O
represent	O
time	O
and	O
the	O
rows	O
represent	O
states	O
an	O
arrow	O
from	O
state	B
i	O
at	O
t	O
to	O
state	B
j	O
at	O
t	O
is	O
annotated	O
with	O
two	O
numbers	O
the	O
first	O
is	O
the	O
probability	O
of	O
the	O
i	O
j	O
transition	O
and	O
the	O
second	O
is	O
the	O
probability	O
of	O
generating	O
observation	B
xt	O
from	O
state	B
j	O
the	O
bold	O
lines	O
circles	O
represent	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
based	O
on	O
figure	O
of	O
and	O
norvig	O
we	O
can	O
also	O
easily	O
work	O
in	O
the	O
log	O
domain	O
the	O
key	O
difference	O
is	O
that	O
log	O
max	O
max	O
log	O
whereas	O
log	O
log	O
hence	O
we	O
can	O
use	O
log	O
tj	O
max	O
max	O
i	O
log	O
zt	O
log	O
t	O
log	O
j	O
log	O
tj	O
in	O
the	O
case	O
of	O
gaussian	B
observation	B
models	O
this	O
can	O
result	O
in	O
a	O
significant	O
factor	B
speedup	O
since	O
computing	O
log	O
pxtzt	O
can	O
be	O
much	O
faster	O
than	O
computing	O
pxtzt	O
for	O
a	O
highdimensional	O
gaussian	B
this	O
is	O
one	O
reason	O
why	O
the	O
viterbi	B
algorithm	O
is	O
widely	O
used	O
in	O
the	O
e	B
step	I
of	O
em	B
when	O
training	O
large	O
speech	B
recognition	I
systems	O
based	O
on	O
hmms	B
example	O
figure	O
gives	O
a	O
worked	O
example	O
of	O
the	O
viterbi	B
algorithm	O
based	O
on	O
et	O
al	O
suppose	O
we	O
observe	O
the	O
discrete	B
sequence	O
of	O
observations	O
representing	O
codebook	B
entries	O
in	O
a	O
vector-quantized	O
version	O
of	O
a	O
speech	O
signal	O
the	O
model	O
starts	O
in	O
state	B
the	O
probability	O
of	O
generating	O
in	O
is	O
so	O
we	O
have	O
and	O
for	O
all	O
other	O
states	O
next	O
we	O
can	O
self-transition	O
to	O
with	O
probability	O
or	O
transition	O
to	O
with	O
proabability	O
if	O
we	O
end	O
up	O
in	O
the	O
probability	O
of	O
generating	O
is	O
if	O
we	O
end	O
up	O
in	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
the	O
probability	O
of	O
generating	O
is	O
hence	O
we	O
have	O
thus	O
state	B
is	O
more	O
probable	O
at	O
t	O
see	O
the	O
second	O
column	O
of	O
figure	O
in	O
time	O
step	O
we	O
see	O
that	O
there	O
are	O
two	O
paths	O
into	O
from	O
and	O
from	O
the	O
bold	O
arrow	O
indicates	O
that	O
the	O
latter	O
is	O
more	O
probable	O
hence	O
this	O
is	O
the	O
only	O
one	O
we	O
have	O
to	O
remember	O
the	O
algorithm	O
continues	O
in	O
this	O
way	O
until	O
we	O
have	O
reached	O
the	O
end	O
of	O
the	O
sequence	O
one	O
we	O
have	O
reached	O
the	O
end	O
we	O
can	O
follow	O
the	O
black	O
arrows	O
back	O
to	O
recover	O
the	O
map	O
path	B
is	O
time	O
and	O
space	O
complexity	O
is	O
clearly	O
ok	O
in	O
general	O
and	O
the	O
space	O
complexity	O
the	O
time	O
complexity	O
of	O
viterbi	B
is	O
okt	O
both	O
the	O
same	O
as	O
forwards-backwards	B
if	O
the	O
transition	B
matrix	I
has	O
the	O
form	O
j	O
exp	O
where	O
zi	O
is	O
the	O
continuous	O
vector	O
represented	O
by	O
state	B
i	O
we	O
can	O
implement	O
viterbi	B
in	O
ot	O
k	O
time	O
instead	O
of	O
ot	O
k	O
log	O
k	O
needed	O
by	O
forwards-backwards	B
see	O
section	O
for	O
details	O
n-best	B
list	I
the	O
viterbi	B
algorithm	O
returns	O
one	O
of	O
the	O
most	O
probable	O
paths	O
it	O
can	O
be	O
extended	O
to	O
return	O
the	O
top	O
n	O
paths	O
and	O
chow	O
nilsson	O
and	O
goldberger	O
this	O
is	O
called	O
the	O
n-best	B
list	I
once	O
can	O
then	O
use	O
a	O
discriminative	B
method	O
to	O
rerank	B
the	O
paths	O
based	O
on	O
global	O
features	B
derived	O
from	O
the	O
fully	O
observed	O
state	B
sequence	O
well	O
as	O
the	O
visible	B
features	B
this	O
technique	O
is	O
widely	O
used	O
in	O
speech	B
recognition	I
for	O
example	O
consider	O
the	O
sentence	O
recognize	O
speech	O
it	O
is	O
possible	O
that	O
the	O
most	O
probable	O
interpretation	O
by	O
the	O
system	O
of	O
this	O
acoustic	O
signal	O
is	O
wreck	O
a	O
nice	O
speech	O
or	O
maybe	O
wreck	O
a	O
nice	O
beach	O
maybe	O
the	O
correct	O
interpretation	O
is	O
much	O
lower	O
down	O
on	O
the	O
list	O
however	O
by	O
using	O
a	O
re-ranking	O
system	O
we	O
may	O
be	O
able	O
to	O
improve	O
the	O
score	O
of	O
the	O
correct	O
interpretation	O
based	O
on	O
a	O
more	O
global	O
context	O
one	O
problem	O
with	O
the	O
n	O
list	O
is	O
that	O
often	O
the	O
top	O
n	O
paths	O
are	O
very	O
similar	B
to	O
each	O
other	O
rather	O
than	O
representing	O
qualitatively	O
different	O
interpretations	O
of	O
the	O
data	O
instead	O
we	O
might	O
want	O
to	O
generate	O
a	O
more	O
diverse	O
set	O
of	O
paths	O
to	O
more	O
accurately	O
represent	O
posterior	O
uncertainty	B
one	O
way	O
to	O
do	O
this	O
is	O
to	O
sample	O
paths	O
from	O
the	O
posterior	O
as	O
we	O
discuss	O
below	O
for	O
some	O
other	O
ways	O
to	O
generate	O
diverse	O
map	O
estimates	O
see	O
e	O
g	O
et	O
al	O
kulesza	O
and	O
taskar	O
forwards	O
filtering	B
backwards	O
sampling	O
it	O
is	O
often	O
useful	O
to	O
sample	O
paths	O
from	O
the	O
posterior	O
zs	O
we	O
can	O
do	O
this	O
is	O
as	O
follow	O
run	O
forwards	O
backwards	O
to	O
compute	O
the	O
two-slice	O
smoothed	O
posteriors	O
pzt	O
next	O
compute	O
the	O
conditionals	O
pztzt	O
by	O
normalizing	O
sample	O
from	O
the	O
initial	O
pair	O
of	O
states	O
z	O
t	O
note	O
that	O
the	O
above	O
solution	O
requires	O
a	O
forwards-backwards	B
pass	O
and	O
then	O
an	O
additional	O
forwards	O
sampling	O
pass	O
an	O
alternative	O
is	O
to	O
do	O
the	O
forwards	O
pass	O
and	O
then	O
perform	O
sampling	O
finally	O
recursively	O
sample	O
z	O
t	O
pztz	O
learning	B
for	O
hmms	B
in	O
the	O
backwards	O
pass	O
the	O
key	O
insight	O
into	O
how	O
to	O
do	O
this	O
is	O
that	O
we	O
can	O
write	O
the	O
joint	O
from	O
right	O
to	O
left	O
using	O
tt	O
we	O
can	O
then	O
sample	O
zt	O
given	O
future	O
sampled	O
states	O
using	O
t	O
pztzs	O
zs	O
the	O
sampling	B
distribution	I
is	O
given	O
by	O
pzt	O
j	O
j	O
ti	O
the	O
base	O
case	O
is	O
t	O
pzt	O
t	O
zs	O
this	O
algorithm	O
forms	O
the	O
basis	O
of	O
blocked-gibbs	O
sampling	O
methods	O
for	O
parameter	B
inference	B
as	O
we	O
will	O
see	O
below	O
learning	B
for	O
hmms	B
we	O
now	O
discuss	O
how	O
to	O
estimate	O
the	O
parameters	O
a	O
b	O
where	O
i	O
is	O
the	O
initial	O
state	B
distribution	O
ai	B
j	O
zt	O
jzt	O
i	O
is	O
the	O
transition	B
matrix	I
and	O
b	O
are	O
the	O
parameters	O
of	O
the	O
class-conditional	O
densities	O
pxtzt	O
j	O
we	O
first	O
consider	O
the	O
case	O
where	O
is	O
observed	O
in	O
the	O
training	B
set	I
and	O
then	O
the	O
harder	O
case	O
where	O
is	O
hidden	B
training	O
with	O
fully	O
observed	O
data	O
if	O
we	O
observe	O
the	O
hidden	B
state	B
sequences	O
we	O
can	O
compute	O
the	O
mles	O
for	O
a	O
and	O
exactly	O
as	O
in	O
section	O
if	O
we	O
use	O
a	O
conjugate	B
prior	I
we	O
can	O
also	O
easily	O
compute	O
the	O
posterior	O
the	O
details	O
on	O
how	O
to	O
estimate	O
b	O
depend	O
on	O
the	O
form	O
of	O
the	O
observation	B
model	I
the	O
situation	O
is	O
identical	O
to	O
fitting	O
a	O
generative	O
classifier	O
for	O
example	O
if	O
each	O
state	B
has	O
a	O
multinoulli	B
distribution	I
associated	O
with	O
it	O
with	O
parameters	O
bjl	O
pxt	O
lzt	O
j	O
where	O
l	O
l	O
represents	O
the	O
observed	O
symbol	O
the	O
mle	B
is	O
given	O
by	O
izit	O
j	O
xit	O
l	O
bjl	O
n	O
x	O
jl	O
nj	O
n	O
x	O
jl	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
this	O
result	O
is	O
quite	O
intuitive	O
we	O
simply	O
add	O
up	O
the	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j	O
and	O
we	O
see	O
a	O
symbol	O
l	O
and	O
divide	O
by	O
the	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j	O
similarly	O
if	O
each	O
state	B
has	O
a	O
gaussian	B
distribution	O
associated	O
with	O
it	O
we	O
have	O
sec	O
tion	O
the	O
following	O
mles	O
k	O
xk	O
nk	O
k	O
k	O
nk	O
k	O
t	O
k	O
nk	O
where	O
the	O
sufficient	B
statistics	I
are	O
given	O
by	O
xk	O
k	O
izit	O
kxit	O
izit	O
kxitxt	O
it	O
analogous	O
results	O
can	O
be	O
derived	O
for	O
other	O
kinds	O
of	O
distributions	O
one	O
can	O
also	O
easily	O
extend	O
all	O
of	O
these	O
results	O
to	O
compute	O
map	O
estimates	O
or	O
even	O
full	B
posteriors	O
over	O
the	O
parameters	O
em	B
for	O
hmms	B
baum-welch	B
algorithm	O
if	O
the	O
zt	O
variables	O
are	O
not	O
observed	O
we	O
are	O
in	O
a	O
situation	O
analogous	O
to	O
fitting	O
a	O
mixture	B
model	I
the	O
most	O
common	O
approach	O
is	O
to	O
use	O
the	O
em	B
algorithm	O
to	O
find	O
the	O
mle	B
or	O
map	O
parameters	O
although	O
of	O
course	O
one	O
could	O
use	O
other	O
gradient-based	O
methods	O
e	O
g	O
and	O
chauvin	O
in	O
this	O
section	O
we	O
derive	O
the	O
em	B
algorithm	O
when	O
applied	O
to	O
hmms	B
this	O
is	O
also	O
known	O
as	O
the	O
baum-welch	B
algorithm	O
et	O
al	O
e	B
step	I
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
expected	B
complete	B
data	I
log	I
likelihood	B
is	O
given	O
by	O
q	O
old	O
e	O
n	O
k	O
log	O
k	O
e	O
log	O
ajk	O
pzt	O
kxi	O
old	O
log	O
pxit	O
k	O
e	O
n	O
k	O
where	O
the	O
expected	O
counts	O
are	O
given	O
by	O
kxi	O
old	O
e	O
e	O
pzit	O
jxi	O
old	O
pzit	O
j	O
zit	O
kxi	O
old	O
learning	B
for	O
hmms	B
these	O
expected	B
sufficient	B
statistics	I
can	O
be	O
computed	O
by	O
running	O
the	O
forwards-backwards	B
algorithm	I
on	O
each	O
sequence	O
in	O
particular	O
this	O
algorithm	O
computes	O
the	O
following	O
smoothed	O
node	O
and	O
edge	O
marginals	O
itj	O
pzt	O
itj	O
k	O
pzt	O
j	O
zt	O
m	B
step	I
based	O
on	O
section	O
we	O
have	O
that	O
the	O
m	B
step	I
for	O
a	O
and	O
is	O
to	O
just	O
normalize	O
the	O
expected	O
counts	O
ajk	O
e	O
e	O
e	O
k	O
n	O
k	O
n	O
this	O
result	O
is	O
quite	O
intuitive	O
we	O
simply	O
add	O
up	O
the	O
expected	O
number	O
of	O
transitions	O
from	O
j	O
to	O
k	O
and	O
divide	O
by	O
the	O
expected	O
number	O
of	O
times	O
we	O
transition	O
from	O
j	O
to	O
anything	O
else	O
for	O
a	O
multinoulli	O
observation	B
model	I
the	O
expected	B
sufficient	B
statistics	I
are	O
e	O
itjixit	O
l	O
txitl	O
the	O
m	B
step	I
has	O
the	O
form	O
bjl	O
e	O
e	O
itj	O
this	O
result	O
is	O
quite	O
intuitive	O
we	O
simply	O
add	O
up	O
the	O
expected	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j	O
and	O
we	O
see	O
a	O
symbol	O
l	O
and	O
divide	O
by	O
the	O
expected	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j	O
for	O
a	O
gaussian	B
observation	B
model	I
the	O
expected	B
sufficient	B
statistics	I
are	O
given	O
by	O
e	O
itkxit	O
e	O
k	O
itkxitxt	O
it	O
the	O
m	B
step	I
becomes	O
k	O
e	O
e	O
e	O
k	O
e	O
k	O
t	O
k	O
e	O
k	O
this	O
can	O
should	O
be	O
regularized	O
in	O
the	O
same	O
way	O
we	O
regularize	O
gmms	O
initialization	O
as	O
usual	O
with	O
em	B
we	O
must	O
take	O
care	O
to	O
ensure	O
that	O
we	O
initialize	O
the	O
parameters	O
carefully	O
to	O
minimize	O
the	O
chance	O
of	O
getting	O
stuck	O
in	O
poor	O
local	O
optima	O
there	O
are	O
several	O
ways	O
to	O
do	O
this	O
such	O
as	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
use	O
some	O
fully	O
labeled	O
data	O
to	O
initialize	O
the	O
parameters	O
initially	O
ignore	O
the	O
markov	B
dependencies	O
and	O
estimate	O
the	O
observation	B
parameters	O
using	O
the	O
standard	O
mixture	B
model	I
estimation	O
methods	O
such	O
as	O
k-means	O
or	O
em	B
randomly	O
initialize	O
the	O
parameters	O
use	O
multiple	B
restarts	I
and	O
pick	O
the	O
best	O
solution	O
techniques	O
such	O
as	O
deterministic	B
annealing	B
and	O
nakano	O
rao	O
and	O
rose	O
can	O
help	O
mitigate	O
the	O
effect	O
of	O
local	O
minima	O
also	O
just	O
as	O
k-means	O
is	O
often	O
used	O
to	O
initialize	O
em	B
for	O
gmms	O
so	O
it	O
is	O
common	O
to	O
initialize	O
em	B
for	O
hmms	B
using	O
viterbi	B
training	I
which	O
means	O
approximating	O
the	O
posterior	O
over	O
paths	O
with	O
the	O
single	O
most	O
probable	O
path	B
is	O
not	O
necessarily	O
a	O
good	O
idea	O
since	O
initially	O
the	O
parameters	O
are	O
often	O
poorly	O
estimated	O
so	O
the	O
viterbi	B
path	B
will	O
be	O
fairly	O
arbitrary	O
a	O
safer	O
option	O
is	O
to	O
start	O
training	O
using	O
forwards-backwards	B
and	O
to	O
switch	O
to	O
viterbi	B
near	O
convergence	O
bayesian	B
methods	O
for	O
fitting	O
hmms	B
em	B
returns	O
a	O
map	B
estimate	I
of	O
the	O
parameters	O
in	O
this	O
section	O
we	O
briefly	O
discuss	O
some	O
methods	O
for	O
bayesian	B
parameter	B
estimation	O
in	O
hmms	B
methods	O
rely	O
on	O
material	O
that	O
we	O
will	O
cover	O
later	O
in	O
the	O
book	O
one	O
approach	O
is	O
to	O
use	O
variational	B
bayes	I
em	B
which	O
we	O
discuss	O
in	O
general	O
terms	O
in	O
section	O
the	O
details	O
for	O
the	O
hmm	B
case	O
can	O
be	O
found	O
in	O
beal	O
but	O
the	O
basic	O
idea	O
is	O
this	O
the	O
e	B
step	I
uses	O
forwards-backwards	B
but	O
where	O
speaking	O
we	O
plug	O
in	O
the	O
posterior	B
mean	B
parameters	O
instead	O
of	O
the	O
map	O
estimates	O
the	O
m	B
step	I
updates	O
the	O
parameters	O
of	O
the	O
conjugate	O
posteriors	O
instead	O
of	O
updating	O
the	O
parameters	O
themselves	O
an	O
alternative	O
to	O
vbem	B
is	O
to	O
use	O
mcmc	B
a	O
particularly	O
appealing	O
algorithm	O
is	O
block	O
gibbs	B
sampling	I
which	O
we	O
discuss	O
in	O
general	O
terms	O
in	O
section	O
the	O
details	O
for	O
the	O
hmm	B
case	O
can	O
be	O
found	O
in	O
but	O
the	O
basic	O
idea	O
is	O
this	O
we	O
sample	O
given	O
the	O
data	O
and	O
parameters	O
using	O
forwards-filtering	O
backwards-sampling	O
and	O
we	O
then	O
sample	O
the	O
parameters	O
from	O
their	O
posteriors	O
conditional	O
on	O
the	O
sampled	O
latent	B
paths	O
this	O
is	O
simple	O
to	O
implement	O
but	O
one	O
does	O
need	O
to	O
take	O
care	O
of	O
unidentifiability	O
switching	O
just	O
as	O
with	O
mixture	B
models	O
section	O
discriminative	B
training	O
sometimes	O
hmms	B
are	O
used	O
as	O
the	O
class	O
conditional	O
density	O
inside	O
a	O
generative	O
classifier	O
in	O
this	O
case	O
pxy	O
c	O
can	O
be	O
computed	O
using	O
the	O
forwards	O
algorithm	O
we	O
can	O
easily	O
maximize	O
the	O
pxi	O
yi	O
by	O
using	O
em	B
some	O
other	O
method	O
to	O
fit	O
the	O
hmm	B
for	O
each	O
joint	O
likelihood	B
class-conditional	B
density	I
separately	O
however	O
we	O
might	O
like	O
to	O
find	O
the	O
parameters	O
that	O
maximize	O
the	O
conditional	B
likelihood	B
pyixi	O
i	O
pyi	O
c	O
pyi	O
c	O
this	O
is	O
more	O
expensive	O
than	O
maximizing	O
the	O
joint	O
likelihood	B
since	O
the	O
denominator	O
couples	O
all	O
c	O
class-conditional	O
hmms	B
together	O
furthermore	O
em	B
can	O
no	O
longer	O
be	O
used	O
and	O
one	O
must	O
resort	O
generalizations	O
of	O
hmms	B
to	O
generic	O
gradient	O
based	O
methods	O
nevertheless	O
discriminative	B
training	O
can	O
result	O
in	O
improved	O
accuracies	O
the	O
standard	O
practice	O
in	O
speech	B
recognition	I
is	O
to	O
initially	O
train	O
the	O
generative	O
models	O
separately	O
using	O
em	B
and	O
then	O
to	O
fine	O
tune	O
them	O
discriminatively	O
model	B
selection	I
in	O
hmms	B
the	O
two	O
main	O
model	B
selection	I
issues	O
are	O
how	O
many	O
states	O
and	O
what	O
topology	O
to	O
use	O
for	O
the	O
state	B
transition	I
diagram	I
we	O
discuss	O
both	O
of	O
these	O
issues	O
below	O
choosing	O
the	O
number	O
of	O
hidden	B
states	O
choosing	O
the	O
number	O
of	O
hidden	B
states	O
k	O
in	O
an	O
hmm	B
is	O
analogous	O
to	O
the	O
problem	O
of	O
choosing	O
the	O
number	O
of	O
mixture	B
components	O
here	O
are	O
some	O
possible	O
solutions	O
use	O
grid-search	O
over	O
a	O
range	O
of	O
k	O
s	O
using	O
as	O
an	O
objective	O
function	O
cross-validated	O
likelihood	B
the	O
bic	B
score	O
or	O
a	O
variational	O
lower	O
bound	O
to	O
the	O
log-marginal	O
likelihood	B
use	O
reversible	B
jump	I
mcmc	B
see	O
for	O
details	O
note	O
that	O
this	O
is	O
very	O
slow	O
and	O
is	O
not	O
widely	O
used	O
use	O
variational	B
bayes	I
to	O
extinguish	O
unwanted	O
components	O
by	O
analogy	O
to	O
the	O
gmm	B
case	O
discussed	O
in	O
section	O
see	O
beal	O
for	O
details	O
use	O
an	O
infinite	O
hmm	B
which	O
is	O
based	O
on	O
the	O
hierarchical	B
dirichlet	B
process	I
see	O
e	O
g	O
et	O
al	O
teh	O
et	O
al	O
for	O
details	O
structure	B
learning	B
the	O
term	O
structure	B
learning	B
in	O
the	O
context	O
of	O
hmms	B
refers	O
to	O
learning	B
a	O
sparse	B
transition	B
matrix	I
that	O
is	O
we	O
want	O
to	O
learn	O
the	O
structure	O
of	O
the	O
state	B
transition	I
diagram	I
not	O
the	O
structure	O
of	O
the	O
graphical	B
model	I
is	O
fixed	O
a	O
large	O
number	O
of	O
heuristic	O
methods	O
have	O
been	O
proposed	O
most	O
alternate	O
between	O
parameter	B
estimation	O
and	O
some	O
kind	O
of	O
heuristic	O
split	B
merge	I
method	O
e	O
g	O
and	O
omohundro	O
alternatively	O
one	O
can	O
pose	O
the	O
problem	O
as	O
map	O
estimation	O
using	O
a	O
minimum	B
entropy	B
prior	I
of	O
the	O
form	O
pai	O
exp	O
h	O
this	O
prior	O
prefers	O
states	O
whose	O
outgoing	O
distribution	O
is	O
nearly	O
deterministic	O
and	O
hence	O
has	O
low	O
entropy	B
the	O
corresponding	O
m	B
step	I
cannot	O
be	O
solved	O
in	O
closed	O
form	O
but	O
numerical	O
methods	O
can	O
be	O
used	O
the	O
trouble	O
with	O
this	O
is	O
that	O
we	O
might	O
prune	O
out	O
all	O
incoming	O
transitions	O
to	O
a	O
state	B
creating	O
isolated	O
islands	O
in	O
state-space	O
the	O
infinite	O
hmm	B
presents	O
an	O
interesting	O
alternative	O
to	O
these	O
methods	O
see	O
e	O
g	O
et	O
al	O
teh	O
et	O
al	O
for	O
details	O
generalizations	O
of	O
hmms	B
many	O
variants	O
of	O
the	O
basic	O
hmm	B
model	O
have	O
been	O
proposed	O
we	O
briefly	O
discuss	O
some	O
of	O
them	O
below	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
dt	O
dt	O
qt	O
qt	O
qt	O
xt	O
xt	O
figure	O
encoding	O
a	O
hidden	B
semi-markov	B
model	I
as	O
a	O
dgm	B
dt	O
are	O
deterministic	O
duration	O
counters	O
variable	O
duration	O
hmms	B
in	O
a	O
standard	O
hmm	B
the	O
probability	O
we	O
remain	O
in	O
state	B
i	O
for	O
exactly	O
d	O
steps	O
is	O
pti	O
d	O
aiiad	O
ii	O
expd	O
log	O
aii	O
where	O
aii	O
is	O
the	O
self-loop	O
probability	O
this	O
is	O
called	O
the	O
geometric	B
distribution	I
however	O
this	O
kind	O
of	O
exponentially	O
decaying	O
function	O
of	O
d	O
is	O
sometimes	O
unrealistic	O
to	O
allow	O
for	O
more	O
general	O
durations	O
one	O
can	O
use	O
a	O
semi-markov	B
model	I
it	O
is	O
called	O
semimarkov	O
because	O
to	O
predict	O
the	O
next	O
state	B
it	O
is	O
not	O
sufficient	O
to	O
condition	O
on	O
the	O
past	O
state	B
we	O
also	O
need	O
to	O
know	O
how	O
long	O
we	O
ve	O
been	O
in	O
that	O
state	B
when	O
the	O
state	B
space	I
is	O
not	O
observed	O
directly	O
the	O
result	O
is	O
called	O
a	O
hidden	B
semi-markov	B
model	I
a	O
variable	B
duration	I
hmm	B
or	O
an	O
explicit	B
duration	I
hmm	B
hsmms	O
are	O
widely	O
used	O
in	O
many	O
gene	O
finding	O
programs	O
since	O
the	O
length	O
distribution	O
of	O
exons	O
and	O
introns	O
is	O
not	O
geometric	O
e	O
g	O
et	O
al	O
and	O
in	O
some	O
chip-seq	B
data	O
analysis	O
programs	O
e	O
g	O
et	O
al	O
hsmms	O
are	O
useful	O
not	O
only	O
because	O
they	O
can	O
model	O
the	O
waiting	O
time	O
of	O
each	O
state	B
more	O
accurately	O
but	O
also	O
because	O
they	O
can	O
model	O
the	O
distribution	O
of	O
a	O
whole	O
batch	B
of	O
observations	O
at	O
once	O
instead	O
of	O
assuming	O
all	O
observations	O
are	O
conditionally	O
iid	B
that	O
is	O
they	O
can	O
use	O
likelihood	B
models	O
of	O
the	O
form	O
pxttlzt	O
k	O
dt	O
l	O
which	O
generate	O
l	O
correlated	O
observations	O
if	O
the	O
duration	O
in	O
state	B
k	O
is	O
for	O
l	O
time	O
steps	O
this	O
is	O
useful	O
for	O
modeling	O
data	O
that	O
is	O
piecewise	O
linear	O
or	O
shows	O
other	O
local	O
trends	O
et	O
al	O
hsmm	B
as	O
augmented	O
hmms	B
one	O
way	O
to	O
represent	O
a	O
hsmm	B
is	O
to	O
use	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
this	O
figure	O
we	O
have	O
assumed	O
the	O
observations	O
are	O
iid	B
within	O
each	O
state	B
but	O
this	O
is	O
not	O
required	O
as	O
mentioned	O
above	O
the	O
dt	O
d	O
node	O
is	O
a	O
state	B
duration	O
counter	O
where	O
d	O
is	O
the	O
maximum	O
duration	O
of	O
any	O
state	B
when	O
we	O
first	O
enter	O
state	B
j	O
we	O
sample	O
dt	O
from	O
the	O
duration	O
distribution	O
for	O
that	O
state	B
dt	O
pj	O
thereafer	O
dt	O
deterministically	O
counts	O
down	O
generalizations	O
of	O
hmms	B
p	O
p	O
p	O
p	O
p	O
p	O
p	O
figure	O
over	O
sequence	O
lengths	O
for	O
p	O
and	O
various	O
n	O
figure	O
generated	O
by	O
hmmselfloopdist	O
a	O
markov	B
chain	I
with	O
n	O
repeated	O
states	O
and	O
self	B
loops	I
the	O
resulting	O
distribution	O
until	O
dt	O
while	O
dt	O
the	O
state	B
zt	O
is	O
not	O
allowed	O
to	O
change	O
when	O
dt	O
we	O
make	O
a	O
stochastic	O
transition	O
to	O
a	O
new	O
state	B
more	O
precisely	O
we	O
define	O
the	O
cpds	O
as	O
follows	O
ajk	O
pdt	O
d	O
zt	O
j	O
pzt	O
kzt	O
j	O
dt	O
d	O
d	O
and	O
d	O
if	O
d	O
if	O
otherwise	O
if	O
d	O
and	O
j	O
k	O
if	O
d	O
otherwise	O
note	O
that	O
pjd	O
could	O
be	O
represented	O
as	O
a	O
table	O
non-parametric	O
approach	O
or	O
as	O
some	O
kind	O
of	O
parametric	O
distribution	O
such	O
as	O
a	O
gamma	B
distribution	I
if	O
pjd	O
is	O
a	O
geometric	B
distribution	I
this	O
emulates	O
a	O
standard	O
hmm	B
one	O
can	O
perform	O
inference	B
in	O
this	O
model	O
by	O
defining	O
a	O
mega-variable	O
yt	O
zt	O
however	O
this	O
is	O
rather	O
inefficient	O
since	O
dt	O
is	O
deterministic	O
it	O
is	O
possible	O
to	O
marginalize	O
dt	O
out	O
and	O
derive	O
special	O
purpose	O
inference	B
procedures	O
see	O
yu	O
and	O
kobayashi	O
for	O
details	O
unfortunately	O
all	O
these	O
methods	O
take	O
ot	O
k	O
time	O
where	O
t	O
is	O
the	O
sequence	O
length	O
k	O
is	O
the	O
number	O
of	O
states	O
and	O
d	O
is	O
the	O
maximum	O
duration	O
of	O
any	O
state	B
approximations	O
to	O
semi-markov	O
models	O
a	O
more	O
efficient	O
but	O
less	O
flexible	O
way	O
to	O
model	O
non-geometric	O
waiting	O
times	O
is	O
to	O
replace	O
each	O
state	B
with	O
n	O
new	O
states	O
each	O
with	O
the	O
same	O
emission	O
probabilities	O
as	O
the	O
original	O
state	B
for	O
example	O
consider	O
the	O
model	O
in	O
figure	O
obviously	O
the	O
smallest	O
sequence	O
this	O
can	O
generate	O
is	O
of	O
length	O
n	O
any	O
path	B
of	O
length	O
d	O
through	O
the	O
model	O
has	O
probability	O
pd	O
pn	O
multiplying	O
by	O
the	O
number	O
of	O
possible	O
paths	O
we	O
find	O
that	O
the	O
total	O
probability	O
of	O
a	O
path	B
of	O
length	O
d	O
is	O
pd	O
pd	O
pn	O
d	O
n	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
words	O
on	O
need	O
phones	O
aa	O
n	O
end	O
n	O
iy	O
d	O
end	O
the	O
dh	O
n	O
ax	O
iy	O
end	O
subphones	O
end	O
end	O
figure	O
an	O
example	O
of	O
an	O
hhmm	O
for	O
an	O
asr	O
system	O
which	O
can	O
recognize	O
words	O
the	O
top	O
level	O
represents	O
bigram	O
word	O
probabilities	O
the	O
middle	O
level	O
represents	O
the	O
phonetic	O
spelling	O
of	O
each	O
word	O
the	O
bottom	O
level	O
represents	O
the	O
subphones	O
of	O
each	O
phone	B
is	O
traditional	O
to	O
represent	O
a	O
phone	B
as	O
a	O
state	B
hmm	B
representing	O
the	O
beginning	O
middle	O
and	O
end	O
based	O
on	O
figure	O
of	O
and	O
martin	O
this	O
is	O
equivalent	O
to	O
the	O
negative	B
binomial	B
distribution	I
by	O
adjusting	O
n	O
and	O
the	O
self-loop	O
probabilities	O
p	O
of	O
each	O
state	B
we	O
can	O
model	O
a	O
wide	O
range	O
of	O
waiting	O
times	O
see	O
figure	O
let	O
e	O
be	O
the	O
number	O
of	O
expansions	O
of	O
each	O
state	B
needed	O
to	O
approximate	O
pjd	O
forwardsbackwards	O
on	O
this	O
model	O
takes	O
ot	O
time	O
where	O
fin	O
is	O
the	O
average	O
number	O
of	O
predecessor	O
states	O
compared	O
to	O
ot	O
kfin	O
for	O
the	O
hsmm	B
for	O
typical	O
speech	B
recognition	I
applications	O
fin	O
d	O
k	O
t	O
figures	O
apply	O
to	O
problems	O
such	O
as	O
gene	O
finding	O
which	O
also	O
often	O
uses	O
hsmms	O
since	O
fin	O
d	O
efin	O
the	O
expanded	O
state	B
method	O
is	O
much	O
faster	O
than	O
an	O
hsmm	B
see	O
for	O
details	O
hierarchical	O
hmms	B
a	O
hierarchical	B
hmm	B
et	O
al	O
is	O
an	O
extension	B
of	O
the	O
hmm	B
that	O
is	O
designed	O
to	O
model	O
domains	O
with	O
hierarchical	O
structure	O
figure	O
gives	O
an	O
example	O
of	O
an	O
hhmm	O
used	O
in	O
automatic	B
speech	B
recognition	I
the	O
phone	B
and	O
subphone	O
models	O
can	O
be	O
called	O
from	O
different	O
higher	O
level	O
contexts	O
we	O
can	O
always	O
flatten	O
an	O
hhmm	O
to	O
a	O
regular	B
hmm	B
but	O
a	O
factored	O
representation	O
is	O
often	O
easier	O
to	O
interpret	O
and	O
allows	O
for	O
more	O
efficient	O
inference	B
and	O
model	O
fitting	O
hhmms	O
have	O
been	O
used	O
in	O
many	O
application	O
domains	O
e	O
g	O
speech	B
recognition	I
gene	O
finding	O
et	O
al	O
plan	O
recognition	O
et	O
al	O
monitoring	O
transportation	O
patterns	O
et	O
al	O
indoor	O
robot	O
localization	O
et	O
al	O
etc	O
hhmms	O
are	O
less	O
expressive	O
than	O
stochastic	B
context	I
free	I
grammars	B
since	O
they	O
only	O
allow	O
hierarchies	O
of	O
bounded	O
depth	O
but	O
they	O
support	B
more	O
efficient	O
inference	B
in	O
particular	O
inference	B
in	O
scfgs	B
the	O
inside	B
outside	I
algorithm	O
and	O
martin	O
takes	O
ot	O
whereas	O
inference	B
in	O
an	O
hhmm	O
takes	O
ot	O
time	O
and	O
paskin	O
we	O
can	O
represent	O
an	O
hhmm	O
as	O
a	O
directed	B
graphical	B
model	I
as	O
shown	O
in	O
figure	O
t	O
represents	O
the	O
state	B
at	O
time	O
t	O
and	O
level	O
a	O
state	B
transition	O
at	O
level	O
is	O
only	O
allowed	O
if	O
the	O
generalizations	O
of	O
hmms	B
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
figure	O
an	O
hhmm	O
represented	O
as	O
a	O
dgm	B
level	O
has	O
finished	O
its	O
exit	O
state	B
otherwise	O
f	O
nodes	B
are	O
hidden	B
we	O
may	O
optionally	O
clamp	O
f	O
to	O
ensure	O
all	O
models	O
have	O
finished	O
by	O
the	O
end	O
of	O
the	O
sequence	O
t	O
is	O
the	O
state	B
at	O
time	O
t	O
level	O
f	O
t	O
if	O
the	O
hmm	B
at	O
t	O
shaded	O
nodes	B
are	O
observed	O
the	O
remaining	O
t	O
where	O
t	O
is	O
the	O
length	O
of	O
the	O
observation	B
sequence	O
source	O
figure	O
of	O
and	O
paskin	O
chain	O
at	O
the	O
level	O
below	O
has	O
finished	O
as	O
determined	O
by	O
the	O
f	O
chain	O
below	O
finishes	O
when	O
it	O
chooses	O
to	O
enter	O
its	O
end	O
state	B
this	O
mechanism	O
ensures	O
that	O
higher	O
level	O
chains	O
evolve	O
more	O
slowly	O
than	O
lower	O
level	O
chains	O
i	O
e	O
lower	O
levels	O
are	O
nested	O
within	O
higher	O
levels	O
node	O
t	O
a	O
variable	B
duration	I
hmm	B
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
case	O
of	O
an	O
hhmm	O
where	O
the	O
top	O
level	O
is	O
a	O
deterministic	O
counter	O
and	O
the	O
bottom	O
level	O
is	O
a	O
regular	B
hmm	B
which	O
can	O
only	O
change	O
states	O
once	O
the	O
counter	O
has	O
timed	O
out	O
see	O
and	O
paskin	O
for	O
further	O
details	O
input-output	O
hmms	B
it	O
is	O
straightforward	O
to	O
extend	O
an	O
hmm	B
to	O
handle	O
inputs	O
as	O
shown	O
in	O
figure	O
this	O
defines	O
a	O
conditional	O
density	O
model	O
for	O
sequences	O
of	O
the	O
form	O
where	O
ut	O
is	O
the	O
input	O
at	O
time	O
t	O
this	O
is	O
sometimes	O
called	O
a	O
control	B
signal	I
outputs	O
are	O
continuous	O
a	O
typical	O
parameterization	O
would	O
be	O
pztxt	O
zt	O
i	O
catztswiut	O
pytxt	O
zt	O
j	O
j	O
if	O
the	O
inputs	O
and	O
thus	O
the	O
transition	B
matrix	I
is	O
a	O
logistic	B
regression	B
model	O
whose	O
parameters	O
depend	O
on	O
the	O
previous	O
state	B
the	O
observation	B
model	I
is	O
a	O
gaussian	B
whose	O
parameters	O
depend	O
on	O
the	O
current	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
ut	O
ut	O
zt	O
zt	O
yt	O
yt	O
zt	O
xt	O
figure	O
input-output	O
hmm	B
first-order	O
auto-regressive	B
hmm	B
a	O
second-order	O
buried	O
markov	B
model	I
depending	O
on	O
the	O
value	O
of	O
the	O
hidden	B
variables	I
the	O
effective	O
graph	B
structure	O
between	O
the	O
components	O
of	O
the	O
observed	O
variables	O
the	O
non-zero	O
elements	O
of	O
the	O
regression	B
matrix	O
and	O
the	O
precision	B
matrix	I
can	O
change	O
although	O
this	O
is	O
not	O
shown	O
state	B
the	O
whole	O
model	O
can	O
be	O
thought	O
of	O
as	O
a	O
hidden	B
version	O
of	O
a	O
maximum	B
entropy	B
markov	B
model	I
conditional	O
on	O
the	O
inputs	O
and	O
the	O
parameters	O
one	O
can	O
apply	O
the	O
standard	O
forwardsit	O
is	O
also	O
straightforward	O
to	O
derive	O
an	O
em	B
backwards	O
algorithm	O
to	O
estimate	O
the	O
hidden	B
states	O
algorithm	O
to	O
estimate	O
the	O
parameters	O
and	O
frasconi	O
for	O
details	O
auto-regressive	O
and	O
buried	O
hmms	B
the	O
standard	O
hmm	B
assumes	O
the	O
observations	O
are	O
conditionally	B
independent	I
given	O
the	O
hidden	B
state	B
in	O
practice	O
this	O
is	O
often	O
not	O
the	O
case	O
however	O
it	O
is	O
straightforward	O
to	O
have	O
direct	O
arcs	O
from	O
xt	O
to	O
xt	O
as	O
well	O
as	O
from	O
zt	O
to	O
xt	O
as	O
in	O
figure	O
this	O
is	O
known	O
as	O
an	O
auto-regressive	B
hmm	B
or	O
aregime	O
switching	O
markov	B
model	I
for	O
continuous	O
data	O
the	O
observation	B
model	I
becomes	O
pxtxt	O
zt	O
j	O
n	O
j	O
j	O
this	O
is	O
a	O
linear	B
regression	B
model	O
where	O
the	O
parameters	O
are	O
chosen	O
according	O
to	O
the	O
current	O
hidden	B
state	B
we	O
can	O
also	O
consider	O
higher-order	O
extensions	O
where	O
we	O
condition	O
on	O
the	O
last	O
l	O
observations	O
pxtxt	O
lt	O
zt	O
j	O
n	O
j	O
j	O
such	O
models	O
are	O
widely	O
used	O
in	O
econometrics	O
similar	B
models	O
can	O
be	O
defined	O
for	O
discrete	B
observations	O
the	O
ar-hmm	O
essentially	O
combines	O
two	O
markov	B
chains	O
one	O
on	O
the	O
hidden	B
variables	I
to	O
capture	O
long	O
range	O
dependencies	O
and	O
one	O
on	O
the	O
observed	O
variables	O
to	O
capture	O
short	O
range	O
dependencies	O
since	O
the	O
x	O
nodes	B
are	O
observed	O
the	O
connections	O
between	O
them	O
only	O
generalizations	O
of	O
hmms	B
figure	O
a	O
factorial	B
hmm	B
with	O
chains	O
a	O
coupled	B
hmm	B
with	O
chains	O
change	O
the	O
computation	O
of	O
the	O
local	B
evidence	B
inference	B
can	O
still	O
be	O
performed	O
using	O
the	O
standard	O
forwards-backwards	B
algorithm	I
parameter	B
estimation	O
using	O
em	B
is	O
also	O
straightforward	O
the	O
e	B
step	I
is	O
unchanged	O
as	O
is	O
the	O
m	B
step	I
for	O
the	O
transition	B
matrix	I
if	O
we	O
assume	O
scalar	O
observations	O
for	O
notational	O
simplicty	O
the	O
m	B
step	I
involves	O
minimizing	O
t	O
lt	O
log	O
e	O
t	O
yt	O
tj	O
j	O
t	O
focussing	O
on	O
the	O
w	O
terms	O
we	O
see	O
that	O
this	O
requires	O
solving	O
k	O
weighted	B
least	B
squares	I
problems	O
yt	O
t	O
lt	O
where	O
tj	O
zt	O
is	O
the	O
smoothed	O
posterior	O
marginal	O
this	O
is	O
a	O
weighted	O
linear	B
regression	B
problem	O
where	O
the	O
design	B
matrix	I
has	O
a	O
toeplitz	B
form	O
this	O
subproblem	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
levinson-durbin	B
method	O
and	O
koopman	O
buried	B
markov	B
models	I
generalize	B
ar-hmms	O
by	O
allowing	O
the	O
dependency	O
structure	O
between	O
the	O
observable	O
nodes	B
to	O
change	O
based	O
on	O
the	O
hidden	B
state	B
as	O
in	O
figure	O
such	O
a	O
model	O
is	O
called	O
a	O
dynamic	O
bayesian	B
multi	B
net	I
since	O
it	O
is	O
a	O
mixture	B
of	O
different	O
networks	O
in	O
the	O
linear-gaussian	O
setting	O
we	O
can	O
change	O
the	O
structure	O
of	O
the	O
of	O
xt	O
xt	O
arcs	O
by	O
using	O
sparse	B
regression	B
matrices	O
wj	O
and	O
we	O
can	O
change	O
the	O
structure	O
of	O
the	O
connections	O
within	O
the	O
components	O
of	O
xt	O
by	O
using	O
sparse	B
gaussian	B
graphical	B
models	I
either	O
directed	B
or	O
undirected	B
see	O
for	O
details	O
factorial	B
hmm	B
an	O
hmm	B
represents	O
the	O
hidden	B
state	B
using	O
a	O
single	O
discrete	B
random	I
variable	I
zt	O
k	O
to	O
represent	O
bits	B
of	O
information	B
would	O
require	O
k	O
states	O
by	O
contrast	O
consider	O
a	O
distributed	B
representation	I
of	O
the	O
hidden	B
state	B
where	O
each	O
zct	O
represents	O
the	O
c	O
th	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
bit	O
of	O
the	O
t	O
th	O
hidden	B
state	B
now	O
we	O
can	O
represent	O
bits	B
using	O
just	O
binary	O
variables	O
as	O
illustrated	O
in	O
figure	O
this	O
model	O
is	O
called	O
a	O
factorial	B
hmm	B
and	O
jordan	O
the	O
hope	O
is	O
that	O
this	O
kind	O
of	O
model	O
could	O
capture	O
different	O
aspects	O
of	O
a	O
signal	O
e	O
g	O
one	O
chain	O
would	O
represent	O
speaking	O
style	O
another	O
the	O
words	O
that	O
are	O
being	O
spoken	O
unfortunately	O
conditioned	O
on	O
xt	O
all	O
the	O
hidden	B
variables	I
are	O
correlated	O
to	O
explaining	B
away	I
the	O
common	O
observed	O
child	O
xt	O
this	O
make	O
exact	O
state	B
estimation	I
intractable	O
however	O
we	O
can	O
derive	O
efficient	O
approximate	B
inference	B
algorithms	O
as	O
we	O
discuss	O
in	O
section	O
coupled	B
hmm	B
and	O
the	O
influence	O
model	O
if	O
we	O
have	O
multiple	O
related	O
data	O
streams	O
we	O
can	O
use	O
a	O
coupled	B
hmm	B
as	O
illustrated	O
in	O
figure	O
this	O
is	O
a	O
series	O
of	O
hmms	B
where	O
the	O
state	B
transitions	O
depend	O
on	O
the	O
states	O
of	O
neighboring	O
chains	O
that	O
is	O
we	O
represent	O
the	O
joint	O
conditional	O
distribution	O
as	O
pzctzt	O
zctzct	O
zc	O
c	O
pztzt	O
pzctzt	O
this	O
has	O
been	O
used	O
for	O
various	O
tasks	O
such	O
as	O
audio-visual	B
speech	B
recognition	I
et	O
al	O
and	O
modeling	O
freeway	O
traffic	O
flows	O
and	O
murphy	O
the	O
trouble	O
with	O
the	O
above	O
model	O
is	O
that	O
it	O
requires	O
ock	O
parameters	O
to	O
specify	O
if	O
there	O
are	O
c	O
chains	O
with	O
k	O
states	O
per	O
chain	O
because	O
each	O
state	B
depends	O
on	O
its	O
own	O
past	O
plus	O
the	O
past	O
of	O
its	O
two	O
neighbors	B
there	O
is	O
a	O
closely	O
related	O
model	O
known	O
as	O
the	O
influence	O
model	O
which	O
uses	O
fewer	O
parameters	O
it	O
models	O
the	O
joint	O
conditional	O
distribution	O
as	O
pzctzt	O
for	O
each	O
c	O
that	O
is	O
we	O
use	O
a	O
convex	B
combination	I
of	O
pairwise	O
transition	O
where	O
matrices	O
the	O
parameter	B
specifies	O
how	O
much	O
influence	O
chain	O
c	O
has	O
on	O
chain	O
this	O
model	O
only	O
takes	O
oc	O
ck	O
parameters	O
to	O
specify	O
furthermore	O
it	O
allows	O
each	O
chain	O
to	O
be	O
influenced	O
by	O
all	O
the	O
other	O
chains	O
not	O
just	O
its	O
nearest	O
neighbors	B
the	O
corresponding	O
graphical	B
model	I
is	O
similar	B
to	O
figure	O
except	O
that	O
each	O
node	O
has	O
incoming	O
edges	B
from	O
all	O
the	O
previous	O
nodes	B
this	O
has	O
been	O
used	O
for	O
various	O
tasks	O
such	O
as	O
modeling	O
conversational	O
interactions	O
between	O
people	O
et	O
al	O
unfortunately	O
inference	B
in	O
both	O
of	O
these	O
models	O
takes	O
ot	O
time	O
since	O
all	O
the	O
chains	O
become	O
fully	O
correlated	O
even	O
if	O
the	O
interaction	O
graph	B
is	O
sparse	B
various	O
approximate	B
inference	B
methods	O
can	O
be	O
applied	O
as	O
we	O
discuss	O
later	O
dynamic	O
bayesian	B
networks	I
a	O
dynamic	B
bayesian	B
network	I
is	O
just	O
a	O
way	O
to	O
represent	O
a	O
stochastic	B
process	I
using	O
a	O
directed	B
graphical	O
note	O
that	O
the	O
network	O
is	O
not	O
dynamic	O
structure	O
and	O
parameters	O
are	O
fixed	O
the	O
acronym	O
dbn	B
can	O
stand	O
for	O
either	O
dynamic	B
bayesian	B
network	I
or	O
deep	B
belief	I
network	I
depending	O
on	O
the	O
context	O
geoff	O
hinton	O
invented	O
the	O
term	O
deep	B
belief	I
network	I
has	O
suggested	O
the	O
acronyms	O
dybn	B
and	O
deebn	B
to	O
avoid	O
this	O
ambiguity	O
generalizations	O
of	O
hmms	B
slice	O
t	O
slice	O
evidence	B
figure	O
the	O
batnet	O
dbn	B
the	O
transient	B
nodes	B
are	O
only	O
shown	O
for	O
the	O
second	O
slice	O
to	O
minimize	O
clutter	O
the	O
dotted	O
lines	O
can	O
be	O
ignored	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
rather	O
it	O
is	O
a	O
network	O
representation	O
of	O
a	O
dynamical	O
system	O
all	O
of	O
the	O
hmm	B
variants	O
we	O
have	O
seen	O
above	O
could	O
be	O
considered	O
to	O
be	O
dbns	O
however	O
we	O
prefer	O
to	O
reserve	O
the	O
term	O
dbn	B
for	O
graph	B
structures	O
that	O
are	O
more	O
irregular	O
and	O
problem-specific	O
an	O
example	O
is	O
shown	O
in	O
figure	O
which	O
is	O
a	O
dbn	B
designed	O
to	O
monitor	O
the	O
state	B
of	O
a	O
simulated	O
autonomous	O
car	O
known	O
as	O
the	O
bayesian	B
automated	O
taxi	O
or	O
batmobile	O
et	O
al	O
defining	O
dbns	O
is	O
straightforward	O
you	O
just	O
need	O
to	O
specify	O
the	O
structure	O
of	O
the	O
first	O
time-slice	O
the	O
structure	O
between	O
two	O
time-slices	O
and	O
the	O
form	O
of	O
the	O
cpds	O
learning	B
is	O
also	O
easy	O
the	O
main	O
problem	O
is	O
that	O
exact	O
inference	B
can	O
be	O
computationally	O
expensive	O
because	O
all	O
the	O
hidden	B
variables	I
become	O
correlated	O
over	O
time	O
is	O
known	O
as	O
entanglement	B
see	O
e	O
g	O
and	O
for	O
details	O
thus	O
a	O
sparse	B
graph	B
does	O
not	O
necessarily	O
result	O
in	O
friedman	O
sec	O
tractable	O
exact	O
inference	B
however	O
later	O
we	O
will	O
see	O
algorithms	O
that	O
can	O
exploit	O
the	O
graph	B
structure	O
for	O
efficient	O
approximate	B
inference	B
exercises	O
exercise	O
derivation	O
of	O
q	O
function	O
for	O
hmm	B
derive	O
equation	O
exercise	O
two	O
filter	O
approach	O
to	O
smoothing	B
in	O
hmms	B
assuming	O
that	O
ti	O
i	O
for	O
all	O
i	O
and	O
t	O
derive	O
a	O
recursive	B
algorithm	O
for	O
updating	O
rti	O
pst	O
hint	O
it	O
should	O
be	O
very	O
similar	B
to	O
the	O
standard	O
forwards	O
algorithm	O
but	O
using	O
a	O
timereversed	O
transition	B
matrix	I
then	O
show	O
how	O
to	O
compute	O
the	O
posterior	O
marginals	O
ti	O
chapter	O
markov	B
and	O
hidden	B
markov	B
models	I
from	O
the	O
backwards	O
filtered	O
messages	O
rti	O
the	O
forwards	O
filtered	O
messages	O
ti	O
and	O
the	O
stationary	B
distribution	I
ti	O
exercise	O
em	B
for	O
for	O
hmms	B
with	O
mixture	B
of	O
gaussian	B
observations	O
consider	O
an	O
hmm	B
where	O
the	O
observation	B
model	I
has	O
the	O
form	O
wjkn	O
jk	O
jk	O
k	O
pxtzt	O
j	O
draw	O
the	O
dgm	B
derive	O
the	O
e	B
step	I
derive	O
the	O
m	B
step	I
exercise	O
em	B
for	O
for	O
hmms	B
with	O
tied	B
mixtures	O
in	O
many	O
applications	O
it	O
is	O
common	O
that	O
the	O
observations	O
are	O
high-dimensional	O
vectors	O
in	O
speech	B
recognition	I
xt	O
is	O
often	O
a	O
vector	O
of	O
cepstral	O
coefficients	O
and	O
their	O
derivatives	O
so	O
xt	O
r	O
so	O
estimating	O
a	O
full	B
covariance	B
matrix	I
for	O
km	O
values	O
m	O
is	O
the	O
number	O
of	O
mixture	B
components	O
per	O
hidden	B
state	B
as	O
in	O
exercise	O
requires	O
a	O
lot	O
of	O
data	O
an	O
alternative	O
is	O
to	O
use	O
just	O
m	O
gaussians	O
rather	O
than	O
m	O
k	O
gaussians	O
and	O
to	O
let	O
the	O
state	B
influence	O
the	O
mixing	B
weights	I
but	O
not	O
the	O
means	O
and	O
covariances	O
this	O
is	O
called	O
a	O
semi-continuous	B
hmm	B
or	O
tied-mixture	B
hmm	B
draw	O
the	O
corresponding	O
graphical	B
model	I
derive	O
the	O
e	B
step	I
derive	O
the	O
m	B
step	I
state	B
space	I
models	O
introduction	O
a	O
state	B
space	I
model	I
or	O
ssm	B
is	O
just	O
like	O
an	O
hmm	B
except	O
the	O
hidden	B
states	O
are	O
continuous	O
the	O
model	O
can	O
be	O
written	O
in	O
the	O
following	O
generic	O
form	O
zt	O
gut	O
zt	O
yt	O
hzt	O
ut	O
t	O
where	O
zt	O
is	O
the	O
hidden	B
state	B
ut	O
is	O
an	O
optional	O
input	O
or	O
control	B
signal	I
yt	O
is	O
the	O
observation	B
g	O
is	O
the	O
transition	B
model	I
h	O
is	O
the	O
observation	B
model	I
is	O
the	O
system	O
noise	O
at	O
time	O
t	O
and	O
t	O
is	O
the	O
observation	B
noise	O
at	O
time	O
t	O
we	O
assume	O
that	O
all	O
parameters	O
of	O
the	O
model	O
are	O
known	O
if	O
not	O
they	O
can	O
be	O
included	O
into	O
the	O
hidden	B
state	B
as	O
we	O
discuss	O
below	O
one	O
of	O
the	O
primary	O
goals	O
in	O
using	O
ssms	O
is	O
to	O
recursively	O
estimate	O
the	O
belief	B
state	B
we	O
will	O
often	O
drop	O
the	O
conditioning	B
on	O
u	O
and	O
for	O
brevity	O
we	O
will	O
discuss	O
algorithms	O
for	O
this	O
later	O
in	O
this	O
chapter	O
we	O
will	O
also	O
discuss	O
how	O
to	O
convert	O
our	O
beliefs	O
about	O
the	O
hidden	B
state	B
into	O
predictions	O
about	O
future	O
observables	O
by	O
computing	O
the	O
posterior	O
predictive	B
in	O
other	O
an	O
important	O
special	O
case	O
of	O
an	O
ssm	B
is	O
where	O
all	O
the	O
cpds	O
are	O
linear-gaussian	O
words	O
we	O
assume	O
the	O
transition	B
model	I
is	O
a	O
linear	O
function	O
zt	O
atzt	O
btut	O
the	O
observation	B
model	I
is	O
a	O
linear	O
function	O
yt	O
ctzt	O
dtut	O
t	O
the	O
system	O
noise	O
is	O
gaussian	B
n	O
qt	O
the	O
observation	B
noise	O
is	O
gaussian	B
t	O
n	O
rt	O
this	O
model	O
is	O
called	O
a	O
linear-gaussian	B
ssm	B
or	O
alinear	O
dynamical	O
system	O
if	O
the	O
parameters	O
t	O
bt	O
ct	O
dt	O
qt	O
rt	O
are	O
independent	O
of	O
time	O
the	O
model	O
is	O
called	O
stationary	B
observed	O
truth	O
chapter	O
state	B
space	I
models	O
observed	O
smoothed	O
observed	O
filtered	O
illustration	O
of	O
kalman	O
filtering	B
and	O
smoothing	B
figure	O
observations	O
cirles	O
are	O
generated	O
filtered	O
estimated	O
is	O
shown	O
by	O
an	O
object	O
moving	O
to	O
the	O
right	O
location	O
denoted	O
by	O
black	O
squares	O
by	O
dotted	O
red	O
line	O
red	O
cross	O
is	O
the	O
posterior	B
mean	B
blue	O
circles	O
are	O
confidence	O
ellipses	O
derived	O
from	O
the	O
posterior	O
covariance	B
for	O
clarity	O
we	O
only	O
plot	O
the	O
ellipses	O
every	O
other	O
time	O
step	O
same	O
as	O
but	O
using	O
offline	B
kalman	B
smoothing	B
figure	O
generated	O
by	O
kalmantrackingdemo	O
the	O
lg-ssm	B
is	O
important	O
because	O
it	O
supports	O
exact	O
inference	B
as	O
we	O
will	O
see	O
in	O
particular	O
if	O
the	O
initial	O
belief	B
state	B
is	O
gaussian	B
then	O
all	O
subsequent	O
belief	O
states	O
will	O
also	O
be	O
gaussian	B
we	O
will	O
denote	O
them	O
by	O
n	O
tt	O
tt	O
notation	O
t	O
denotes	O
e	O
and	O
similarly	O
for	O
tt	O
thus	O
denotes	O
the	O
prior	O
for	O
before	O
we	O
have	O
seen	O
any	O
data	O
for	O
brevity	O
we	O
will	O
denote	O
the	O
posterior	O
belief	O
states	O
using	O
tt	O
t	O
and	O
tt	O
t	O
we	O
can	O
compute	O
these	O
quantities	O
efficiently	O
using	O
the	O
celebrated	O
kalman	O
filter	O
as	O
we	O
show	O
in	O
section	O
but	O
before	O
discussing	O
algorithms	O
we	O
discuss	O
some	O
important	O
applications	O
applications	O
of	O
ssms	O
ssms	O
have	O
many	O
applications	O
some	O
of	O
which	O
we	O
discuss	O
in	O
the	O
sections	O
below	O
we	O
mostly	O
focus	O
on	O
lg-ssms	O
for	O
simplicity	O
although	O
non-linear	O
andor	O
non-gaussian	O
ssms	O
are	O
even	O
more	O
widely	O
used	O
ssms	O
for	O
object	O
tracking	B
one	O
of	O
the	O
earliest	O
applications	O
of	O
kalman	O
filtering	B
was	O
for	O
tracking	B
objects	O
such	O
as	O
airplanes	O
and	O
missiles	O
from	O
noisy	O
measurements	O
such	O
as	O
radar	B
here	O
we	O
give	O
a	O
simplified	O
example	O
to	O
illustrate	O
the	O
key	O
ideas	O
consider	O
an	O
object	O
moving	O
in	O
a	O
plane	O
let	O
and	O
be	O
the	O
horizontal	O
and	O
vertical	O
locations	O
of	O
the	O
object	O
and	O
and	O
be	O
the	O
corresponding	O
velocity	O
we	O
can	O
represent	O
this	O
as	O
a	O
state	B
vector	O
zt	O
r	O
as	O
follows	O
zt	O
t	O
applications	O
of	O
ssms	O
let	O
us	O
assume	O
that	O
the	O
object	O
is	O
moving	O
at	O
constant	O
velocity	O
but	O
is	O
perturbed	O
by	O
random	O
gaussian	B
noise	O
due	O
to	O
the	O
wind	O
thus	O
we	O
can	O
model	O
the	O
system	O
dynamics	O
as	O
follows	O
zt	O
atzt	O
where	O
n	O
q	O
is	O
the	O
system	O
noise	O
and	O
is	O
the	O
sampling	B
period	B
this	O
says	O
that	O
the	O
new	O
location	O
zjt	O
is	O
the	O
old	O
location	O
zjt	O
plus	O
times	O
the	O
old	O
velocity	O
zjt	O
plus	O
random	O
noise	O
for	O
j	O
also	O
the	O
new	O
velocity	O
zjt	O
is	O
the	O
old	O
velocity	O
zjt	O
plus	O
random	O
noise	O
for	O
j	O
this	O
is	O
called	O
a	O
random	B
accelerations	I
model	I
since	O
the	O
object	O
moves	O
according	O
to	O
newton	O
s	O
laws	O
but	O
is	O
subject	O
to	O
random	O
changes	O
in	O
velocity	O
now	O
suppose	O
that	O
we	O
can	O
observe	O
the	O
location	O
of	O
the	O
object	O
but	O
not	O
its	O
velocity	O
let	O
yt	O
r	O
represent	O
our	O
observation	B
which	O
we	O
assume	O
is	O
subject	O
to	O
gaussian	B
noise	O
we	O
can	O
model	O
this	O
as	O
follows	O
yt	O
ctzt	O
t	O
where	O
t	O
n	O
r	O
is	O
the	O
measurement	O
noise	O
finally	O
we	O
need	O
to	O
specify	O
our	O
initial	O
beliefs	O
about	O
the	O
state	B
of	O
the	O
object	O
we	O
will	O
assume	O
this	O
is	O
a	O
gaussian	B
n	O
we	O
can	O
represent	O
prior	O
ignorance	O
by	O
making	O
suitably	O
broad	O
e	O
g	O
i	O
we	O
have	O
now	O
fully	O
specified	O
the	O
model	O
and	O
can	O
perform	O
sequential	B
bayesian	B
updating	O
to	O
compute	O
using	O
an	O
algorithm	O
known	O
as	O
the	O
kalman	O
filter	O
to	O
be	O
described	O
in	O
section	O
figure	O
gives	O
an	O
example	O
the	O
object	O
moves	O
to	O
the	O
right	O
and	O
generates	O
an	O
observation	B
at	O
each	O
time	O
step	O
of	O
blips	O
on	O
a	O
radar	B
screen	O
we	O
observe	O
these	O
blips	O
and	O
filter	O
out	O
the	O
noise	O
by	O
using	O
the	O
kalman	O
filter	O
at	O
every	O
step	O
we	O
have	O
from	O
which	O
we	O
can	O
compute	O
by	O
marginalizing	B
out	I
the	O
dimensions	O
corresponding	O
to	O
the	O
velocities	O
is	O
easy	O
to	O
do	O
since	O
the	O
posterior	O
is	O
gaussian	B
our	O
best	O
guess	O
about	O
the	O
location	O
of	O
the	O
object	O
is	O
the	O
posterior	B
mean	B
denoted	O
as	O
a	O
red	O
cross	O
in	O
figure	O
our	O
uncertainty	B
associated	O
with	O
this	O
is	O
represented	O
as	O
an	O
ellipse	O
which	O
contains	O
of	O
the	O
probability	O
mass	O
we	O
see	O
that	O
our	O
uncertainty	B
goes	O
down	O
over	O
time	O
as	O
the	O
effects	O
of	O
the	O
initial	O
uncertainty	B
get	O
washed	O
out	O
we	O
also	O
see	O
that	O
the	O
estimated	O
trajectory	O
has	O
filtered	O
out	O
some	O
of	O
the	O
noise	O
to	O
obtain	O
the	O
much	O
smoother	O
plot	O
in	O
figure	O
we	O
need	O
to	O
use	O
the	O
kalman	B
smoother	I
which	O
computes	O
this	O
depends	O
on	O
future	O
as	O
well	O
as	O
past	O
data	O
as	O
discussed	O
in	O
section	O
robotic	O
slam	B
consider	O
a	O
robot	O
moving	O
around	O
an	O
unknown	B
world	O
it	O
needs	O
to	O
learn	O
a	O
map	O
and	O
keep	O
track	O
of	O
its	O
location	O
within	O
that	O
map	O
this	O
problem	O
is	O
known	O
as	O
simultaneous	O
localization	O
and	O
chapter	O
state	B
space	I
models	O
xt	O
yt	O
figure	O
illustration	O
of	O
graphical	B
model	I
underlying	O
slam	B
li	O
is	O
the	O
fixed	O
location	O
of	O
landmark	O
i	O
xt	O
is	O
the	O
location	O
of	O
the	O
robot	O
and	O
yt	O
is	O
the	O
observation	B
in	O
this	O
trace	B
the	O
robot	O
sees	O
landmarks	O
and	O
at	O
time	O
step	O
then	O
just	O
landmark	O
then	O
just	O
landmark	O
etc	O
based	O
on	O
figure	O
of	O
and	O
friedman	O
robot	O
pose	O
figure	O
illustration	O
of	O
the	O
slam	B
problem	O
a	O
robot	O
starts	O
at	O
the	O
top	O
left	O
and	O
moves	O
clockwise	O
in	O
a	O
circle	O
back	O
to	O
where	O
it	O
started	O
we	O
see	O
how	O
the	O
posterior	O
uncertainty	B
about	O
the	O
robot	O
s	O
location	O
increases	O
and	O
then	O
decreases	O
as	O
it	O
returns	O
to	O
a	O
familar	O
location	O
closing	B
the	I
loop	B
if	O
we	O
performed	O
smoothing	B
this	O
new	O
information	B
would	O
propagate	O
backwards	O
in	O
time	O
to	O
disambiguate	O
the	O
entire	O
trajectory	O
we	O
show	O
the	O
precision	B
matrix	I
representing	O
sparse	B
correlations	O
between	O
the	O
landmarks	O
and	O
between	O
the	O
landmarks	O
and	O
the	O
robot	O
s	O
position	O
this	O
sparse	B
precision	B
matrix	I
can	O
be	O
visualized	O
as	O
a	O
gaussian	B
graphical	B
model	I
as	O
shown	O
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
applications	O
of	O
ssms	O
mapping	O
or	O
slam	B
for	O
short	O
and	O
is	O
widely	O
used	O
in	O
mobile	O
robotics	O
as	O
well	O
as	O
other	O
applications	O
such	O
as	O
indoor	O
navigation	O
using	O
cellphones	O
gps	B
does	O
not	O
work	O
inside	O
buildings	O
let	O
us	O
assume	O
we	O
can	O
represent	O
the	O
map	O
as	O
the	O
locations	O
of	O
a	O
fixed	O
set	O
of	O
k	O
landmarks	O
denote	O
them	O
by	O
lk	O
is	O
a	O
vector	O
in	O
r	O
for	O
simplicity	O
we	O
will	O
assume	O
these	O
are	O
uniquely	O
identifiable	O
let	O
xt	O
represent	O
the	O
unknown	B
location	O
of	O
the	O
robot	O
at	O
time	O
t	O
we	O
define	O
the	O
state	B
space	I
to	O
be	O
zt	O
we	O
assume	O
the	O
landmarks	O
are	O
static	O
so	O
their	O
motion	O
if	O
yt	O
measures	O
the	O
distance	O
from	O
xt	O
to	O
model	O
is	O
a	O
constant	O
and	O
they	O
have	O
no	O
system	O
noise	O
the	O
set	O
of	O
closest	O
landmarks	O
then	O
the	O
robot	O
can	O
update	O
its	O
estimate	O
of	O
the	O
landmark	O
locations	O
based	O
on	O
what	O
it	O
sees	O
figure	O
shows	O
the	O
corresponding	O
graphical	B
model	I
for	O
the	O
case	O
where	O
k	O
and	O
where	O
on	O
the	O
first	O
step	O
it	O
sees	O
landmarks	O
and	O
then	O
just	O
landmark	O
then	O
just	O
landmark	O
etc	O
if	O
we	O
assume	O
the	O
observation	B
model	I
pytzt	O
l	O
is	O
linear-gaussian	O
and	O
we	O
use	O
a	O
gaussian	B
motion	O
model	O
for	O
pxtxt	O
ut	O
we	O
can	O
use	O
a	O
kalman	O
filter	O
to	O
maintain	O
our	O
belief	B
state	B
about	O
the	O
location	O
of	O
the	O
robot	O
and	O
the	O
location	O
of	O
the	O
landmarks	O
and	O
cheeseman	O
choset	O
and	O
nagatani	O
over	O
time	O
the	O
uncertainty	B
in	O
the	O
robot	O
s	O
location	O
will	O
increase	O
due	O
to	O
wheel	O
slippage	B
etc	O
but	O
when	O
the	O
robot	O
returns	O
to	O
a	O
familiar	O
location	O
its	O
uncertainty	B
will	O
decrease	O
again	O
this	O
is	O
called	O
closing	B
the	I
loop	B
and	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
see	O
the	O
uncertainty	B
ellipses	O
representing	O
cov	O
grow	O
and	O
then	O
shrink	O
that	O
in	O
this	O
section	O
we	O
assume	O
that	O
a	O
human	O
is	O
joysticking	O
the	O
robot	O
through	O
the	O
environment	O
so	O
is	O
given	O
as	O
input	O
i	O
e	O
we	O
do	O
not	O
address	O
the	O
decision-theoretic	O
issue	O
of	O
choosing	O
where	O
to	O
explore	O
since	O
the	O
belief	B
state	B
is	O
gaussian	B
we	O
can	O
visualize	O
the	O
posterior	O
covariance	B
matrix	I
t	O
actually	O
it	O
is	O
more	O
interesting	O
to	O
visualize	O
the	O
posterior	O
precision	B
matrix	I
t	O
since	O
that	O
is	O
fairly	O
sparse	B
as	O
shown	O
in	O
figure	O
the	O
reason	O
for	O
this	O
is	O
that	O
zeros	O
in	O
the	O
precision	B
matrix	I
correspond	O
to	O
absent	O
edges	B
in	O
the	O
corresponding	O
undirected	B
gaussian	B
graphical	B
model	I
section	O
initially	O
all	O
the	O
landmarks	O
are	O
uncorrelated	O
we	O
have	O
a	O
diagonal	B
prior	O
on	O
l	O
so	O
the	O
ggm	O
is	O
a	O
disconnected	O
graph	B
and	O
t	O
is	O
diagonal	B
however	O
as	O
the	O
robot	O
moves	O
about	O
it	O
will	O
induce	O
correlation	O
between	O
nearby	O
landmarks	O
intuitively	O
this	O
is	O
because	O
the	O
robot	O
is	O
estimating	O
its	O
position	O
based	O
on	O
distance	O
to	O
the	O
landmarks	O
but	O
the	O
landmarks	O
locations	O
are	O
being	O
estimated	O
based	O
on	O
the	O
robot	O
s	O
position	O
so	O
they	O
all	O
become	O
inter-dependent	O
this	O
can	O
be	O
seen	O
more	O
clearly	O
from	O
the	O
graphical	B
model	I
in	O
figure	O
it	O
is	O
clear	O
that	O
and	O
are	O
not	O
d-separated	B
by	O
because	O
there	O
is	O
a	O
path	B
between	O
them	O
via	O
the	O
unknown	B
sequence	O
of	O
nodes	B
as	O
a	O
consequence	O
of	O
the	O
precision	B
matrix	I
becoming	O
denser	O
exact	O
inference	B
takes	O
ok	O
time	O
is	O
an	O
example	O
of	O
the	O
entanglement	B
problem	I
for	O
inference	B
in	O
dbns	O
this	O
prevents	O
the	O
method	O
from	O
being	O
applied	O
to	O
large	O
maps	O
t	O
there	O
are	O
two	O
main	O
solutions	O
to	O
this	O
problem	O
the	O
first	O
is	O
to	O
notice	O
that	O
the	O
correlation	O
pattern	B
moves	O
along	O
with	O
the	O
location	O
of	O
the	O
robot	O
figure	O
the	O
remaining	O
correlations	O
become	O
weaker	O
over	O
time	O
consequently	O
we	O
can	O
dynamically	O
prune	O
out	O
weak	O
edges	B
from	O
the	O
ggm	O
using	O
a	O
technique	O
called	O
the	O
thin	O
junction	B
tree	B
filter	O
trees	O
are	O
explained	O
in	O
section	O
a	O
second	O
approach	O
is	O
to	O
notice	O
that	O
conditional	O
on	O
knowing	O
the	O
robot	O
s	O
path	B
the	O
landmark	O
locations	O
are	O
independent	O
that	O
is	O
this	O
forms	O
the	O
basis	O
of	O
a	O
method	O
known	O
as	O
fastslam	B
which	O
combines	O
kalman	O
filtering	B
and	O
particle	O
filtering	B
as	O
discussed	O
in	O
section	O
et	O
al	O
provides	O
a	O
more	O
detailed	O
account	O
of	O
slam	B
and	O
mobile	O
robotics	O
chapter	O
state	B
space	I
models	O
t	O
t	O
yt	O
yt	O
xt	O
xt	O
s	O
t	O
h	O
g	O
e	O
w	O
i	O
online	O
linear	B
regression	B
batch	B
batch	B
time	O
figure	O
a	O
dynamic	O
generalization	B
of	O
linear	B
regression	B
illustration	O
of	O
the	O
recursive	B
least	B
squares	I
algorithm	O
applied	O
to	O
the	O
model	O
pyx	O
we	O
plot	O
the	O
marginal	O
posterior	O
of	O
var	B
after	O
seeing	O
all	O
and	O
vs	O
number	O
of	O
data	O
points	O
the	O
data	O
we	O
converge	B
to	O
the	O
offline	B
ml	O
squares	O
solution	O
represented	O
by	O
the	O
horizontal	O
lines	O
figure	O
generated	O
by	O
linregonlinedemokalman	O
bars	O
represent	O
e	O
online	O
parameter	B
learning	B
using	O
recursive	B
least	B
squares	I
we	O
can	O
perform	O
online	O
bayesian	B
inference	B
for	O
the	O
parameters	O
of	O
various	O
statistical	O
models	O
using	O
ssms	O
in	O
section	O
we	O
discuss	O
logistic	B
regression	B
in	O
this	O
section	O
we	O
focus	O
on	O
linear	B
regression	B
the	O
basic	O
idea	O
is	O
to	O
let	O
the	O
hidden	B
state	B
represent	O
the	O
regression	B
parameters	O
and	O
to	O
let	O
the	O
observation	B
model	I
represent	O
the	O
current	O
data	O
vector	O
in	O
more	O
detail	O
define	O
the	O
prior	O
to	O
be	O
p	O
we	O
want	O
to	O
do	O
online	O
ml	O
estimation	O
we	O
can	O
just	O
set	O
i	O
let	O
the	O
hidden	B
state	B
be	O
zt	O
if	O
we	O
assume	O
the	O
regression	B
parameters	O
do	O
not	O
change	O
we	O
can	O
set	O
at	O
i	O
and	O
qt	O
so	O
p	O
t	O
t	O
n	O
t	O
t	O
t	O
t	O
we	O
do	O
let	O
the	O
parameters	O
change	O
over	O
time	O
we	O
get	O
a	O
so-called	O
dynamic	B
linear	I
model	I
west	O
and	O
harrison	O
petris	O
et	O
al	O
let	O
ct	O
xt	O
t	O
and	O
rt	O
so	O
the	O
observation	B
model	I
has	O
the	O
form	O
n	O
rt	O
n	O
t	O
t	O
applying	O
the	O
kalman	O
filter	O
to	O
this	O
model	O
provides	O
a	O
way	O
to	O
update	O
our	O
posterior	O
beliefs	O
about	O
the	O
parameters	O
as	O
the	O
data	O
streams	O
in	O
this	O
is	O
known	O
as	O
the	O
recursive	B
least	B
squares	I
or	O
rls	B
algorithm	O
we	O
can	O
derive	O
an	O
explicit	O
form	O
for	O
the	O
updates	O
as	O
follows	O
in	O
section	O
we	O
show	O
that	O
the	O
kalman	O
update	O
for	O
the	O
posterior	B
mean	B
has	O
the	O
form	O
t	O
at	O
t	O
ktyt	O
ctat	O
t	O
applications	O
of	O
ssms	O
where	O
kt	O
is	O
known	O
as	O
the	O
kalman	B
gain	I
matrix	I
based	O
on	O
equation	O
one	O
can	O
show	O
that	O
t	O
r	O
in	O
this	O
context	O
we	O
have	O
kt	O
txt	O
hence	O
the	O
update	O
for	O
the	O
kt	O
tct	O
parameters	O
becomes	O
t	O
t	O
t	O
ttyt	O
xt	O
t	O
t	O
if	O
we	O
approximate	O
tt	O
with	O
ti	O
we	O
recover	O
the	O
least	B
mean	B
squares	I
or	O
lms	B
algorithm	O
discussed	O
in	O
section	O
in	O
lms	B
we	O
need	O
to	O
specify	O
how	O
to	O
adapt	O
the	O
update	O
parameter	B
t	O
to	O
ensure	O
convergence	O
to	O
the	O
mle	B
furthermore	O
the	O
algorithm	O
may	O
take	O
multiple	O
passes	O
through	O
the	O
data	O
by	O
contrast	O
the	O
rls	B
algorithm	O
automatically	O
performs	O
step-size	O
adaptation	O
and	O
converges	O
to	O
the	O
optimal	O
posterior	O
in	O
one	O
pass	O
over	O
the	O
data	O
see	O
figure	O
for	O
an	O
example	O
ssm	B
for	O
time	O
series	O
forecasting	O
ssms	O
are	O
very	O
well	O
suited	O
for	O
time-series	B
forecasting	I
as	O
we	O
explain	O
below	O
we	O
focus	O
on	O
the	O
case	O
of	O
scalar	O
dimensional	O
time	O
series	O
for	O
simplicity	O
our	O
presentation	O
is	O
based	O
on	O
see	O
also	O
harvey	O
west	O
and	O
harrison	O
durbin	O
and	O
koopman	O
petris	O
et	O
al	O
prado	O
and	O
west	O
for	O
good	O
books	O
on	O
this	O
topic	B
at	O
first	O
sight	O
it	O
might	O
not	O
be	O
apparent	O
why	O
ssms	O
are	O
useful	O
since	O
the	O
goal	O
in	O
forecasting	O
is	O
to	O
predict	O
future	O
visible	B
variables	I
not	O
to	O
estimate	O
hidden	B
states	O
of	O
some	O
system	O
indeed	O
most	O
classical	B
methods	O
for	O
time	O
series	O
forecasting	O
are	O
just	O
functions	O
of	O
the	O
form	O
f	O
where	O
hidden	B
variables	I
play	O
no	O
role	O
section	O
the	O
idea	O
in	O
the	O
state-space	O
approach	O
to	O
time	O
series	O
is	O
to	O
create	O
a	O
generative	O
model	O
of	O
the	O
data	O
in	O
terms	O
of	O
latent	B
processes	O
which	O
capture	O
different	O
aspects	O
of	O
the	O
signal	O
we	O
can	O
then	O
integrate	B
out	I
the	O
hidden	B
variables	I
to	O
compute	O
the	O
posterior	O
predictive	B
of	O
the	O
visibles	O
since	O
the	O
model	O
is	O
linear-gaussian	O
we	O
can	O
just	O
add	O
these	O
processes	O
together	O
to	O
explain	O
the	O
observed	O
data	O
this	O
is	O
called	O
a	O
structural	B
time	I
series	I
model	O
below	O
we	O
explain	O
some	O
of	O
the	O
basic	O
building	O
blocks	O
local	B
level	I
model	I
the	O
simplest	O
latent	B
process	O
is	O
known	O
as	O
the	O
local	B
level	I
model	I
which	O
has	O
the	O
form	O
t	O
n	O
r	O
yt	O
at	O
t	O
t	O
n	O
q	O
at	O
at	O
t	O
where	O
the	O
hidden	B
state	B
is	O
just	O
zt	O
at	O
this	O
model	O
asserts	O
that	O
the	O
observed	O
data	O
yt	O
r	O
is	O
equal	O
to	O
some	O
unknown	B
level	O
term	O
at	O
r	O
plus	O
observation	B
noise	O
with	O
variance	B
r	O
in	O
addition	O
the	O
level	O
at	O
evolves	O
over	O
time	O
subject	O
to	O
system	O
noise	O
with	O
variance	B
q	O
see	O
figure	O
for	O
some	O
examples	O
at	O
at	O
yt	O
chapter	O
state	B
space	I
models	O
local	O
level	O
local	B
level	I
model	I
sample	O
output	O
for	O
black	O
solid	O
line	O
q	O
r	O
figure	O
system	O
noisy	O
observations	O
red	O
dotted	O
line	O
q	O
r	O
system	O
deterministic	O
observation	B
blue	O
dot-dash	O
line	O
q	O
r	O
system	O
and	O
observations	O
figure	O
generated	O
by	O
ssmtimeseriessimple	O
at	O
bt	O
at	O
bt	O
yt	O
local	O
trend	O
figure	O
local	O
trend	O
sample	O
output	O
for	O
color	O
code	O
as	O
in	O
figure	O
figure	O
generated	O
by	O
ssmtimeseriessimple	O
local	O
linear	B
trend	I
many	O
time	O
series	O
exhibit	O
linear	O
trends	O
upwards	O
or	O
downwards	O
at	O
least	O
locally	O
we	O
can	O
model	O
this	O
by	O
letting	O
the	O
level	O
at	O
change	O
by	O
an	O
amount	O
bt	O
at	O
each	O
step	O
as	O
follows	O
see	O
figure	O
we	O
can	O
write	O
this	O
in	O
standard	O
form	O
by	O
defining	O
zt	O
bt	O
and	O
when	O
qb	O
we	O
have	O
bt	O
which	O
is	O
some	O
constant	O
defining	O
the	O
slope	O
of	O
the	O
line	O
if	O
in	O
addition	O
we	O
have	O
qa	O
we	O
havea	O
t	O
at	O
unrolling	O
this	O
we	O
have	O
at	O
and	O
yt	O
at	O
t	O
at	O
at	O
bt	O
t	O
bt	O
bt	O
t	O
t	O
n	O
r	O
t	O
n	O
qa	O
a	O
c	O
t	O
n	O
qb	O
q	O
qa	O
qb	O
applications	O
of	O
ssms	O
at	O
bt	O
t	O
t	O
t	O
at	O
bt	O
t	O
t	O
t	O
yt	O
seasonal	O
model	O
figure	O
color	O
code	O
as	O
in	O
figure	O
figure	O
generated	O
by	O
ssmtimeseriessimple	O
seasonal	O
model	O
sample	O
output	O
for	O
with	O
a	O
period	B
of	O
hence	O
e	O
this	O
is	O
thus	O
a	O
generalization	B
of	O
the	O
classic	O
constant	O
linear	B
trend	I
model	O
an	O
example	O
of	O
which	O
is	O
shown	O
in	O
the	O
black	O
line	O
of	O
figure	O
seasonality	O
many	O
time	O
series	O
fluctuate	O
periodically	O
as	O
illustrated	O
in	O
figure	O
this	O
can	O
be	O
modeled	O
by	O
adding	O
a	O
latent	B
process	O
consisting	O
of	O
a	O
series	O
offset	O
terms	O
ct	O
which	O
sum	O
to	O
zero	O
average	O
over	O
a	O
complete	B
cycle	B
of	O
s	O
steps	O
ct	O
s	O
t	O
t	O
n	O
qc	O
see	O
figure	O
for	O
the	O
graphical	B
model	I
for	O
the	O
case	O
s	O
only	O
need	O
seasonal	O
variable	O
because	O
of	O
the	O
sum-to-zero	O
constraint	O
writing	O
this	O
in	O
standard	O
lg-ssm	B
form	O
is	O
left	O
to	O
exercise	O
arma	B
models	O
the	O
classical	B
approach	O
to	O
time-series	B
forecasting	I
is	O
based	O
on	O
arma	B
models	O
arma	B
stands	O
for	O
auto-regressive	O
moving-average	O
and	O
refers	O
to	O
a	O
model	O
of	O
the	O
form	O
ct	O
s	O
xt	O
ixt	O
i	O
jwt	O
j	O
vt	O
where	O
vt	O
wt	O
n	O
are	O
independent	O
gaussian	B
noise	O
terms	O
if	O
q	O
we	O
have	O
a	O
pure	B
ar	O
model	O
where	O
xt	O
xixt	O
p	O
for	O
i	O
t	O
p	O
for	O
example	O
if	O
p	O
we	O
have	O
the	O
model	O
chapter	O
state	B
space	I
models	O
figure	O
an	O
model	O
an	O
model	O
represented	O
as	O
a	O
bi-directed	B
graph	B
an	O
model	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
myung	O
choi	O
vt	O
nodes	B
are	O
implicit	O
in	O
the	O
gaussian	B
cpd	B
for	O
xt	O
this	O
is	O
just	O
a	O
shown	O
in	O
figure	O
if	O
p	O
we	O
have	O
a	O
pure	B
ma	O
model	O
where	O
xt	O
xi	O
for	O
i	O
t	O
q	O
first-order	O
markov	B
chain	I
for	O
example	O
if	O
q	O
we	O
have	O
the	O
model	O
shown	O
in	O
figure	O
here	O
the	O
wt	O
nodes	B
are	O
hidden	B
common	O
causes	O
which	O
induces	O
dependencies	O
between	O
adjacent	O
time	O
steps	O
this	O
models	O
short-range	O
correlation	O
if	O
p	O
q	O
we	O
get	O
the	O
model	O
shown	O
in	O
figure	O
which	O
captures	O
correlation	O
at	O
short	O
and	O
long	O
time	O
scales	O
it	O
turns	O
out	O
that	O
arma	B
models	O
can	O
be	O
represented	O
as	O
ssms	O
as	O
explained	O
in	O
harvey	O
west	O
and	O
harrison	O
durbin	O
and	O
koopman	O
petris	O
et	O
al	O
prado	O
and	O
west	O
however	O
the	O
structural	O
approach	O
to	O
time	O
series	O
is	O
often	O
easier	O
to	O
understand	O
than	O
the	O
arma	B
approach	O
in	O
addition	O
it	O
allows	O
the	O
parameters	O
to	O
evolve	O
over	O
time	O
which	O
makes	O
the	O
models	O
more	O
adaptive	O
to	O
non-stationarity	O
inference	B
in	O
lg-ssm	B
in	O
this	O
section	O
we	O
discuss	O
exact	O
inference	B
in	O
lg-ssm	B
models	O
we	O
first	O
consider	O
the	O
online	O
case	O
which	O
is	O
analogous	O
to	O
the	O
forwards	O
algorithm	O
for	O
hmms	B
we	O
then	O
consider	O
the	O
offline	B
case	O
which	O
is	O
analogous	O
to	O
the	O
forwards-backwards	B
algorithm	I
for	O
hmms	B
the	O
kalman	O
filtering	B
algorithm	O
the	O
kalman	O
filter	O
is	O
an	O
algorithm	O
for	O
exact	O
bayesian	B
filtering	B
for	O
linear-gaussian	O
state	B
space	I
models	O
we	O
will	O
represent	O
the	O
marginal	O
posterior	O
at	O
time	O
t	O
by	O
n	O
t	O
t	O
since	O
everything	O
is	O
gaussian	B
we	O
can	O
perform	O
the	O
prediction	O
and	O
update	O
steps	O
in	O
closed	O
form	O
as	O
we	O
explain	O
below	O
the	O
resulting	O
algorithm	O
is	O
the	O
gaussian	B
analog	O
of	O
the	O
hmm	B
filter	O
in	O
section	O
inference	B
in	O
lg-ssm	B
prediction	O
step	O
the	O
prediction	O
step	O
is	O
straightforward	O
to	O
derive	O
n	O
btut	O
qtn	O
t	O
t	O
n	O
tt	O
tt	O
tt	O
at	O
t	O
btut	O
tt	O
at	O
t	O
t	O
qt	O
measurement	O
step	O
the	O
measurement	O
step	O
can	O
be	O
computed	O
using	O
bayes	B
rule	I
as	O
follows	O
pztyt	O
pytzt	O
in	O
section	O
we	O
show	O
that	O
this	O
is	O
given	O
by	O
ut	O
t	O
t	O
t	O
tt	O
ktrt	O
t	O
ktct	O
tt	O
where	O
rt	O
is	O
the	O
residual	B
or	O
innovation	B
given	O
by	O
the	O
difference	O
between	O
our	O
predicted	O
observation	B
and	O
the	O
actual	O
observation	B
rt	O
yt	O
yt	O
yt	O
e	O
ct	O
tt	O
dtut	O
and	O
kt	O
is	O
the	O
kalman	B
gain	I
matrix	I
given	O
by	O
kt	O
tt	O
t	O
s	O
t	O
where	O
st	O
cov	O
e	O
ct	O
tt	O
t	O
ytctzt	O
t	O
t	O
rt	O
where	O
t	O
n	O
rt	O
is	O
an	O
observation	B
noise	O
term	O
which	O
is	O
independent	O
of	O
all	O
other	O
noise	O
sources	O
note	O
that	O
by	O
using	O
the	O
matrix	B
inversion	I
lemma	I
the	O
kalman	B
gain	I
matrix	I
can	O
also	O
be	O
written	O
as	O
kt	O
tt	O
tt	O
r	O
tt	O
ct	O
rc	O
r	O
we	O
now	O
have	O
all	O
the	O
quantities	O
we	O
need	O
to	O
implement	O
the	O
algorithm	O
see	O
kalmanfilter	O
for	O
some	O
matlab	O
code	O
let	O
us	O
try	O
to	O
make	O
sense	O
of	O
these	O
equations	O
in	O
particular	O
consider	O
the	O
equation	O
for	O
the	O
mean	B
update	O
t	O
tt	O
ktrt	O
this	O
says	O
that	O
the	O
new	O
mean	B
is	O
the	O
old	O
mean	B
plus	O
a	O
chapter	O
state	B
space	I
models	O
correction	O
factor	B
which	O
is	O
kt	O
times	O
the	O
error	B
signal	I
rt	O
the	O
amount	O
of	O
weight	O
placed	O
on	O
the	O
error	B
signal	I
depends	O
on	O
the	O
kalman	B
gain	I
matrix	I
which	O
is	O
the	O
ratio	O
between	O
the	O
covariance	B
of	O
the	O
prior	O
the	O
dynamic	O
model	O
and	O
the	O
covariance	B
if	O
we	O
have	O
a	O
strong	O
prior	O
andor	O
very	O
noisy	O
sensors	O
will	O
be	O
of	O
the	O
measurement	O
error	O
small	O
and	O
we	O
will	O
place	O
little	O
weight	O
on	O
the	O
correction	O
term	O
conversely	O
if	O
we	O
have	O
a	O
weak	O
prior	O
andor	O
high	O
precision	B
sensors	O
then	O
will	O
be	O
large	O
and	O
we	O
will	O
place	O
a	O
lot	O
of	O
weight	O
on	O
the	O
correction	O
term	O
if	O
ct	O
i	O
then	O
kt	O
tt	O
t	O
marginal	B
likelihood	B
as	O
a	O
byproduct	O
of	O
the	O
algorithm	O
we	O
can	O
also	O
compute	O
the	O
log-likelihood	O
of	O
the	O
sequence	O
using	O
log	O
log	O
t	O
where	O
n	O
tt	O
st	O
posterior	O
predictive	B
the	O
one-step-ahead	O
posterior	B
predictive	B
density	I
for	O
the	O
observations	O
can	O
be	O
computed	O
as	O
follows	O
n	O
rn	O
tt	O
tt	O
n	O
tt	O
c	O
tt	O
r	O
this	O
is	O
useful	O
for	O
time	O
series	O
forecasting	O
computational	O
issues	O
there	O
are	O
two	O
dominant	O
costs	O
in	O
the	O
kalman	O
filter	O
the	O
matrix	O
inversion	O
to	O
compute	O
the	O
kalman	B
gain	I
matrix	I
kt	O
which	O
takes	O
time	O
and	O
the	O
matrix-matrix	O
multiply	O
to	O
compute	O
t	O
which	O
takes	O
time	O
in	O
some	O
applications	O
robotic	O
mapping	O
we	O
have	O
so	O
the	O
latter	O
cost	O
dominates	B
however	O
in	O
such	O
cases	O
we	O
can	O
sometimes	O
use	O
sparse	B
approximations	O
et	O
al	O
in	O
cases	O
where	O
we	O
can	O
precompute	O
kt	O
since	O
suprisingly	O
it	O
does	O
not	O
depend	O
on	O
the	O
actual	O
observations	O
unusual	O
property	O
that	O
is	O
specific	O
to	O
linear	B
gaussian	B
systems	O
the	O
iterative	O
equations	O
for	O
updating	O
t	O
are	O
called	O
the	O
ricatti	B
equations	I
and	O
for	O
time	O
invariant	B
systems	O
where	O
t	O
they	O
converge	B
to	O
a	O
fixed	O
point	O
this	O
steady	O
state	B
solution	O
can	O
then	O
be	O
used	O
instead	O
of	O
using	O
a	O
time-specific	O
gain	O
matrix	O
in	O
practice	O
more	O
sophisticated	O
implementations	O
of	O
the	O
kalman	O
filter	O
should	O
be	O
used	O
for	O
reasons	O
of	O
numerical	O
stability	O
one	O
approach	O
is	O
the	O
information	B
filter	O
which	O
recursively	O
updates	O
the	O
canonical	B
parameters	I
of	O
the	O
gaussian	B
t	O
and	O
t	O
t	O
t	O
instead	O
of	O
the	O
moment	B
parameters	I
another	O
approach	O
is	O
the	O
square	O
root	B
filter	O
which	O
works	O
with	O
the	O
cholesky	B
decomposition	I
or	O
the	O
utdtut	O
decomposition	O
of	O
t	O
this	O
is	O
much	O
more	O
numerically	O
stable	B
than	O
directly	O
updating	O
t	O
further	O
details	O
can	O
be	O
found	O
at	O
httpwww	O
cs	O
unc	O
eduwelchkal	O
man	O
and	O
in	O
various	O
books	O
such	O
as	O
t	O
inference	B
in	O
lg-ssm	B
derivation	O
we	O
now	O
derive	O
the	O
kalman	O
filter	O
equations	O
for	O
notational	O
simplicity	O
we	O
will	O
ignore	O
the	O
input	O
terms	O
from	O
bayes	B
rule	I
for	O
gaussians	O
we	O
have	O
that	O
the	O
posterior	O
precision	B
is	O
given	O
by	O
t	O
tt	O
ct	O
t	O
r	O
t	O
ct	O
from	O
the	O
matrix	B
inversion	I
lemma	I
we	O
can	O
rewrite	O
this	O
as	O
t	O
tt	O
tt	O
ktct	O
tt	O
t	O
ct	O
tt	O
t	O
tt	O
from	O
bayes	B
rule	I
for	O
gaussians	O
the	O
posterior	B
mean	B
is	O
given	O
by	O
t	O
tctr	O
t	O
yt	O
t	O
tt	O
tt	O
we	O
will	O
now	O
massage	O
this	O
into	O
the	O
form	O
stated	O
earlier	O
applying	O
the	O
second	O
matrix	B
inversion	I
lemma	I
to	O
the	O
first	O
term	O
of	O
equation	O
we	O
have	O
tctr	O
t	O
yt	O
tt	O
ct	O
t	O
ct	O
t	O
r	O
t	O
ct	O
tt	O
t	O
t	O
yt	O
ktyt	O
tt	O
now	O
applying	O
the	O
matrix	B
inversion	I
lemma	I
to	O
the	O
second	O
term	O
of	O
equation	O
we	O
have	O
t	O
putting	O
the	O
two	O
together	O
we	O
get	O
t	O
tt	O
ktyt	O
ct	O
tt	O
the	O
kalman	B
smoothing	B
algorithm	O
in	O
section	O
we	O
described	O
the	O
kalman	O
filter	O
which	O
sequentially	O
computes	O
for	O
each	O
t	O
this	O
is	O
useful	O
for	O
online	O
inference	B
problems	O
such	O
as	O
tracking	B
however	O
in	O
an	O
offline	B
setting	O
we	O
can	O
wait	O
until	O
all	O
the	O
data	O
has	O
arrived	O
and	O
then	O
compute	O
by	O
conditioning	B
on	O
past	O
and	O
future	O
data	O
our	O
uncertainty	B
will	O
be	O
significantly	O
reduced	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
see	O
that	O
the	O
posterior	O
covariance	B
ellipsoids	O
are	O
smaller	O
for	O
the	O
smoothed	O
trajectory	O
than	O
for	O
the	O
filtered	O
trajectory	O
ellipsoids	O
are	O
larger	O
at	O
the	O
beginning	O
and	O
end	O
of	O
the	O
trajectory	O
since	O
states	O
near	O
the	O
boundary	O
do	O
not	O
have	O
as	O
many	O
useful	O
neighbors	B
from	O
which	O
to	O
borrow	O
information	B
t	O
ct	O
t	O
r	O
tt	O
tt	O
tt	O
ktct	O
tt	O
ktct	O
tt	O
ct	O
tt	O
tt	O
ct	O
t	O
tt	O
t	O
tt	O
tt	O
tt	O
tt	O
tt	O
t	O
tt	O
t	O
tt	O
tt	O
tt	O
chapter	O
state	B
space	I
models	O
we	O
now	O
explain	O
how	O
to	O
compute	O
the	O
smoothed	O
estimates	O
using	O
an	O
algorithm	O
called	O
the	O
rts	B
smoother	I
named	O
after	O
its	O
inventors	O
rauch	O
tung	O
and	O
striebel	O
et	O
al	O
it	O
is	O
also	O
known	O
as	O
the	O
kalman	B
smoothing	B
algorithm	O
the	O
algorithm	O
is	O
analogous	O
to	O
the	O
forwardsbackwards	O
algorithm	O
for	O
hmms	B
although	O
there	O
are	O
some	O
small	O
differences	O
which	O
we	O
discuss	O
below	O
algorithm	O
kalman	O
filtering	B
can	O
be	O
regarded	O
as	O
message	B
passing	I
on	O
a	O
graph	B
from	O
left	O
to	O
right	O
when	O
the	O
messages	O
have	O
reached	O
the	O
end	O
of	O
the	O
graph	B
we	O
have	O
successfully	O
computed	O
now	O
we	O
work	O
backwards	O
from	O
right	O
to	O
left	O
sending	O
information	B
from	O
the	O
future	O
back	O
to	O
the	O
past	O
and	O
them	O
combining	O
the	O
two	O
information	B
sources	O
the	O
question	O
is	O
how	O
do	O
we	O
compute	O
these	O
backwards	O
equations	O
we	O
first	O
give	O
the	O
equations	O
then	O
the	O
derivation	O
we	O
have	O
tt	O
tt	O
tt	O
tt	O
jt	O
tt	O
tt	O
jt	O
t	O
jt	O
ttat	O
where	O
jt	O
is	O
the	O
backwards	O
kalman	B
gain	I
matrix	I
the	O
algorithm	O
can	O
be	O
initialized	O
from	O
tt	O
and	O
tt	O
from	O
the	O
kalman	O
filter	O
note	O
that	O
this	O
backwards	O
pass	O
does	O
not	O
need	O
access	O
to	O
the	O
data	O
that	O
is	O
it	O
does	O
not	O
need	O
this	O
allows	O
us	O
to	O
throw	O
away	O
potentially	O
high	O
dimensional	O
observation	B
vectors	O
and	O
just	O
keep	O
the	O
filtered	O
belief	O
states	O
which	O
usually	O
requires	O
less	O
memory	O
derivation	O
we	O
now	O
derive	O
the	O
kalman	B
smoother	I
following	O
the	O
presentation	O
of	O
sec	O
the	O
key	O
idea	O
is	O
to	O
leverage	O
the	O
markov	B
property	O
which	O
says	O
that	O
zt	O
is	O
independent	O
of	O
future	O
data	O
as	O
long	O
as	O
is	O
known	O
of	O
course	O
is	O
not	O
known	O
but	O
we	O
have	O
a	O
distribution	O
over	O
it	O
so	O
we	O
condition	O
on	O
and	O
then	O
integrate	O
it	O
out	O
as	O
follows	O
by	O
induction	B
assume	O
we	O
have	O
already	O
computed	O
the	O
smoothed	O
distribution	O
for	O
t	O
the	O
question	O
is	O
how	O
do	O
we	O
perform	O
the	O
integration	O
n	O
first	O
we	O
compute	O
the	O
filtered	O
two-slice	O
distribution	O
pzt	O
as	O
follows	O
pzt	O
n	O
tt	O
tt	O
ttat	O
zt	O
tt	O
inference	B
in	O
lg-ssm	B
now	O
we	O
use	O
gaussian	B
conditioning	B
to	O
compute	O
as	O
follows	O
tt	O
tt	O
jt	O
t	O
we	O
can	O
compute	O
the	O
smoothed	O
distribution	O
for	O
t	O
using	O
the	O
rules	B
of	O
iterated	O
expectation	O
and	O
tt	O
e	O
e	O
iterated	O
covariance	B
first	O
the	O
mean	B
e	O
e	O
tt	O
e	O
tt	O
jt	O
e	O
e	O
cov	O
tt	O
cov	O
now	O
the	O
covariance	B
tt	O
cov	O
jtcov	O
t	O
tt	O
jt	O
jt	O
jt	O
tt	O
jt	O
t	O
cov	O
cov	O
tt	O
jt	O
t	O
e	O
t	O
tt	O
jt	O
jt	O
t	O
the	O
algorithm	O
can	O
be	O
initialized	O
from	O
tt	O
and	O
tt	O
from	O
the	O
last	O
step	O
of	O
the	O
filtering	B
algorithm	O
t	O
comparison	O
to	O
the	O
forwards-backwards	B
algorithm	I
for	O
hmms	B
note	O
that	O
in	O
both	O
the	O
forwards	O
and	O
backwards	O
passes	O
for	O
lds	B
we	O
always	O
worked	O
with	O
normalized	O
distributions	O
either	O
conditioned	O
on	O
the	O
past	O
data	O
or	O
conditioned	O
on	O
all	O
the	O
data	O
furthermore	O
the	O
backwards	O
pass	O
depends	O
on	O
the	O
results	O
of	O
the	O
forwards	O
pass	O
this	O
is	O
different	O
from	O
the	O
usual	O
presentation	O
of	O
forwards-backwards	B
for	O
hmms	B
where	O
the	O
backwards	O
pass	O
can	O
be	O
computed	O
independently	O
of	O
the	O
forwards	O
pass	O
section	O
it	O
turns	O
out	O
that	O
we	O
can	O
rewrite	O
the	O
kalman	B
smoother	I
in	O
a	O
modified	O
form	O
which	O
makes	O
it	O
more	O
similar	B
to	O
forwards-backwards	B
for	O
hmms	B
in	O
particular	O
we	O
have	O
pzt	O
now	O
so	O
chapter	O
state	B
space	I
models	O
which	O
is	O
the	O
conditional	B
likelihood	B
of	O
the	O
future	O
data	O
this	O
backwards	O
message	O
can	O
be	O
computed	O
independently	O
of	O
the	O
forwards	O
message	O
however	O
this	O
approach	O
has	O
several	O
disadvantages	O
it	O
needs	O
access	O
to	O
the	O
original	O
observation	B
sequence	O
the	O
backwards	O
message	O
is	O
a	O
likelihood	B
not	O
a	O
posterior	O
so	O
it	O
need	O
not	O
to	O
integrate	O
to	O
over	O
zt	O
in	O
fact	O
it	O
may	O
not	O
always	O
be	O
possible	O
to	O
represent	O
as	O
a	O
gaussian	B
with	O
positive	O
definite	O
covariance	B
problem	O
does	O
not	O
arise	O
in	O
discrete	B
state-spaces	O
as	O
used	O
in	O
hmms	B
when	O
exact	O
inference	B
is	O
not	O
possible	O
it	O
makes	O
more	O
sense	O
to	O
try	O
to	O
approximate	O
the	O
smoothed	O
distribution	O
rather	O
than	O
the	O
backwards	O
likelihood	B
term	O
section	O
there	O
is	O
yet	O
another	O
variant	O
known	O
as	O
two-filter	O
smoothing	B
whereby	O
we	O
compute	O
in	O
the	O
forwards	O
pass	O
as	O
usual	O
and	O
the	O
filtered	O
posterior	O
in	O
the	O
backwards	O
pass	O
these	O
can	O
then	O
be	O
easily	O
combined	O
to	O
compute	O
see	O
briers	O
et	O
al	O
for	O
details	O
learning	B
for	O
lg-ssm	B
in	O
this	O
section	O
we	O
briefly	O
discuss	O
how	O
to	O
estimate	O
the	O
parameters	O
of	O
an	O
lg-ssm	B
in	O
the	O
control	O
theory	O
community	O
this	O
is	O
known	O
as	O
systems	O
identification	O
when	O
using	O
ssms	O
for	O
time	O
series	O
forecasting	O
and	O
also	O
in	O
some	O
physical	O
state	B
estimation	I
problems	O
the	O
observation	B
matrix	O
c	O
and	O
the	O
transition	B
matrix	I
a	O
are	O
both	O
known	O
and	O
fixed	O
by	O
definition	O
of	O
the	O
model	O
in	O
such	O
cases	O
all	O
that	O
needs	O
to	O
be	O
learned	O
are	O
the	O
noise	O
covariances	O
q	O
and	O
r	O
initial	O
state	B
estimate	O
is	O
often	O
less	O
important	O
since	O
it	O
will	O
get	O
washed	O
away	O
by	O
the	O
data	O
after	O
a	O
few	O
time	O
steps	O
this	O
can	O
be	O
encouraged	O
by	O
setting	O
the	O
initial	O
state	B
covariance	B
to	O
be	O
large	O
representing	O
a	O
weak	O
prior	O
although	O
we	O
can	O
estimate	O
q	O
and	O
r	O
offline	B
using	O
the	O
methods	O
described	O
below	O
it	O
is	O
also	O
possible	O
to	O
derive	O
a	O
recursive	B
procedure	O
to	O
exactly	O
compute	O
the	O
posterior	O
pzt	O
r	O
which	O
has	O
the	O
form	O
of	O
a	O
normal-inverse-wishart	B
see	O
and	O
harrison	O
prado	O
and	O
west	O
for	O
details	O
identifiability	O
and	O
numerical	O
stability	O
in	O
the	O
more	O
general	O
setting	O
where	O
the	O
hidden	B
states	O
have	O
no	O
pre-specified	O
meaning	O
we	O
need	O
to	O
learn	O
a	O
and	O
c	O
however	O
in	O
this	O
case	O
we	O
can	O
set	O
q	O
i	O
without	O
loss	B
of	O
generality	O
since	O
an	O
arbitrary	O
noise	O
covariance	B
can	O
be	O
modeled	O
by	O
appropriately	O
modifying	O
a	O
also	O
by	O
analogy	O
with	O
factor	B
analysis	I
we	O
can	O
require	O
r	O
to	O
be	O
diagonal	B
without	O
loss	B
of	O
generality	O
doing	O
this	O
reduces	O
the	O
number	O
of	O
free	O
parameters	O
and	O
improves	O
numerical	O
stability	O
another	O
constraint	O
that	O
is	O
useful	O
to	O
impose	O
is	O
on	O
the	O
eigenvalues	O
of	O
the	O
dynamics	O
matrix	O
a	O
in	O
this	O
case	O
the	O
hidden	B
to	O
see	O
why	O
this	O
is	O
important	O
consider	O
the	O
case	O
of	O
no	O
system	O
noise	O
state	B
at	O
time	O
t	O
is	O
given	O
by	O
zt	O
u	O
tu	O
where	O
u	O
is	O
the	O
matrix	O
of	O
eigenvectors	O
for	O
a	O
and	O
diag	O
i	O
contains	O
the	O
eigenvalues	O
if	O
any	O
i	O
then	O
for	O
large	O
t	O
zt	O
will	O
blow	O
up	O
in	O
magnitude	O
consequently	O
to	O
ensure	O
stability	O
it	O
is	O
useful	O
to	O
require	O
that	O
all	O
the	O
eigenvalues	O
are	O
less	O
than	O
et	O
al	O
of	O
course	O
if	O
all	O
the	O
eigenvalues	O
are	O
less	O
than	O
then	O
e	O
for	O
large	O
t	O
so	O
the	O
state	B
will	O
return	O
to	O
the	O
origin	O
fortunately	O
when	O
we	O
add	O
noise	O
the	O
state	B
become	O
non-zero	O
so	O
the	O
model	O
does	O
not	O
degenerate	B
approximate	O
online	O
inference	B
for	O
non-linear	O
non-gaussian	O
ssms	O
below	O
we	O
discuss	O
how	O
to	O
estimate	O
the	O
parameters	O
however	O
for	O
simplicity	O
of	O
presentation	O
we	O
do	O
not	O
impose	O
any	O
of	O
the	O
constraints	O
mentioned	O
above	O
training	O
with	O
fully	O
observed	O
data	O
if	O
we	O
observe	O
the	O
hidden	B
state	B
sequences	O
we	O
can	O
fit	O
the	O
model	O
by	O
computing	O
the	O
mles	O
even	O
the	O
full	B
posteriors	O
for	O
the	O
parameters	O
by	O
solving	O
a	O
multivariate	O
linear	B
regression	B
problem	O
for	O
zt	O
zt	O
and	O
for	O
zt	O
yt	O
that	O
is	O
we	O
can	O
estimate	O
a	O
by	O
solving	O
the	O
least	B
squares	I
problem	O
azt	O
and	O
similarly	O
for	O
c	O
we	O
can	O
estimate	O
the	O
system	O
noise	O
covariance	B
ja	O
q	O
from	O
the	O
residuals	O
in	O
predicting	O
zt	O
from	O
zt	O
and	O
estimate	O
the	O
observation	B
noise	O
covariance	B
r	O
from	O
the	O
residuals	O
in	O
predicting	O
yt	O
from	O
zt	O
em	B
for	O
lg-ssm	B
if	O
we	O
only	O
observe	O
the	O
output	O
sequence	O
we	O
can	O
compute	O
ml	O
or	O
map	O
estimates	O
of	O
the	O
parameters	O
using	O
em	B
the	O
method	O
is	O
conceptually	O
quite	O
similar	B
to	O
the	O
baum-welch	B
algorithm	O
for	O
hmms	B
except	O
we	O
use	O
kalman	B
smoothing	B
instead	O
of	O
forwards-backwards	B
in	O
the	O
e	B
step	I
and	O
use	O
different	O
calculations	O
in	O
the	O
m	B
step	I
we	O
leave	O
the	O
details	O
to	O
exercise	O
subspace	O
methods	O
em	B
does	O
not	O
always	O
give	O
satisfactory	O
results	O
because	O
it	O
is	O
sensitive	O
to	O
the	O
initial	O
parameter	B
estimates	O
one	O
way	O
to	O
avoid	O
this	O
is	O
to	O
use	O
a	O
different	O
approach	O
known	O
as	O
a	O
subspace	B
method	I
and	O
moor	O
katayama	O
to	O
understand	O
this	O
approach	O
let	O
us	O
initially	O
assume	O
there	O
is	O
no	O
observation	B
noise	O
and	O
no	O
in	O
this	O
case	O
we	O
have	O
zt	O
azt	O
and	O
yt	O
czt	O
and	O
hence	O
yt	O
cat	O
system	O
noise	O
consequently	O
all	O
the	O
observations	O
must	O
be	O
generated	O
from	O
a	O
dimzt-dimensional	O
linear	O
manifold	O
or	O
subspace	O
we	O
can	O
identify	O
this	O
subspace	O
using	O
pca	B
the	O
above	O
references	O
for	O
details	O
once	O
we	O
have	O
an	O
estimate	O
of	O
the	O
zt	O
s	O
we	O
can	O
fit	O
the	O
model	O
as	O
if	O
it	O
were	O
fully	O
observed	O
we	O
can	O
either	O
use	O
these	O
estimates	O
in	O
their	O
own	O
right	O
or	O
use	O
them	O
to	O
initialize	O
em	B
bayesian	B
methods	O
for	O
fitting	O
lg-ssms	O
there	O
are	O
various	O
offline	B
bayesian	B
alternatives	O
to	O
the	O
em	B
algorithm	O
including	O
variational	B
bayes	I
em	B
barber	O
and	O
chiappa	O
and	O
blocked	B
gibbs	B
sampling	I
and	O
kohn	O
cappe	O
et	O
al	O
fruhwirth-schnatter	O
the	O
bayesian	B
approach	O
can	O
also	O
be	O
used	O
to	O
perform	O
online	B
learning	B
as	O
we	O
discussed	O
in	O
section	O
unfortunately	O
once	O
we	O
add	O
the	O
ssm	B
parameters	O
to	O
the	O
state	B
space	I
the	O
model	O
is	O
generally	O
no	O
longer	O
linear	B
gaussian	B
consequently	O
we	O
must	O
use	O
some	O
of	O
the	O
approximate	O
online	O
inference	B
methods	O
to	O
be	O
discussed	O
below	O
approximate	O
online	O
inference	B
for	O
non-linear	O
non-gaussian	O
ssms	O
in	O
section	O
we	O
discussed	O
how	O
to	O
perform	O
exact	O
online	O
inference	B
for	O
lg-ssms	O
however	O
many	O
models	O
are	O
non	O
linear	O
for	O
example	O
most	O
moving	O
objects	O
do	O
not	O
move	O
in	O
straight	O
lines	O
and	O
even	O
if	O
they	O
did	O
if	O
we	O
assume	O
the	O
parameters	O
of	O
the	O
model	O
are	O
unknown	B
and	O
add	O
them	O
chapter	O
state	B
space	I
models	O
to	O
the	O
state	B
space	I
the	O
model	O
becomes	O
nonlinear	O
furthermore	O
non-gaussian	O
noise	O
is	O
also	O
very	O
common	O
e	O
g	O
due	O
to	O
outliers	B
or	O
when	O
inferring	O
parameters	O
for	O
glms	O
instead	O
of	O
just	O
linear	B
regression	B
for	O
these	O
more	O
general	O
models	O
we	O
need	O
to	O
use	O
approximate	B
inference	B
the	O
approximate	B
inference	B
algorithms	O
we	O
discuss	O
below	O
approximate	O
the	O
posterior	O
by	O
a	O
gausin	O
general	O
if	O
y	O
f	O
where	O
x	O
has	O
a	O
gaussian	B
distribution	O
and	O
f	O
is	O
a	O
non-linear	O
sian	O
function	O
there	O
are	O
two	O
main	O
ways	O
to	O
approximate	O
py	O
by	O
a	O
gaussian	B
the	O
first	O
is	O
to	O
use	O
a	O
first-order	O
approximation	O
of	O
f	O
the	O
second	O
is	O
to	O
use	O
the	O
exact	O
f	O
but	O
to	O
project	O
f	O
onto	O
the	O
space	O
of	O
gaussians	O
by	O
moment	B
matching	I
we	O
discuss	O
each	O
of	O
these	O
methods	O
in	O
turn	O
also	O
section	O
where	O
we	O
discuss	O
particle	O
filtering	B
which	O
is	O
a	O
stochastic	B
algorithm	I
for	O
approximate	O
online	O
inference	B
which	O
uses	O
a	O
non-parametric	O
approximation	O
to	O
the	O
posterior	O
which	O
is	O
often	O
more	O
accurate	O
but	O
slower	O
to	O
compute	O
extended	O
kalman	O
filter	O
in	O
this	O
section	O
we	O
focus	O
on	O
non-linear	O
models	O
but	O
we	O
assume	O
the	O
noise	O
is	O
gaussian	B
that	O
is	O
we	O
consider	O
models	O
of	O
the	O
form	O
zt	O
gut	O
zt	O
qt	O
yt	O
hzt	O
rt	O
where	O
the	O
transition	B
model	I
g	O
and	O
the	O
observation	B
model	I
h	O
are	O
nonlinear	O
but	O
differentiable	O
functions	O
furthermore	O
we	O
focus	O
on	O
the	O
case	O
where	O
we	O
approximate	O
the	O
posterior	O
by	O
a	O
single	O
gaussian	B
simplest	O
way	O
to	O
handle	O
more	O
general	O
posteriors	O
multi-modal	O
discrete	B
etc	O
is	O
to	O
use	O
particle	O
filtering	B
which	O
we	O
discuss	O
in	O
section	O
the	O
extended	O
kalman	O
filter	O
or	O
ekf	B
can	O
be	O
applied	O
to	O
nonlinear	O
gaussian	B
dynamical	O
systems	O
of	O
this	O
form	O
the	O
basic	O
idea	O
is	O
to	O
linearize	O
g	O
and	O
h	O
about	O
the	O
previous	O
state	B
estimate	O
using	O
a	O
first	O
order	O
taylor	B
series	I
expansion	I
and	O
then	O
to	O
apply	O
the	O
standard	O
kalman	O
filter	O
equations	O
noise	O
variance	B
in	O
the	O
equations	O
and	O
r	O
is	O
not	O
changed	O
i	O
e	O
the	O
additional	O
error	O
due	O
to	O
linearization	O
is	O
not	O
modeled	O
thus	O
we	O
approximate	O
the	O
stationary	B
non-linear	O
dynamical	O
system	O
with	O
a	O
non-stationary	O
linear	B
dynamical	I
system	I
the	O
intuition	O
behind	O
the	O
approach	O
is	O
shown	O
in	O
figure	O
which	O
shows	O
what	O
happens	O
when	O
we	O
pass	O
a	O
gaussian	B
distribution	O
px	O
shown	O
on	O
the	O
bottom	O
right	O
through	O
a	O
nonlinear	O
function	O
y	O
gx	O
shown	O
on	O
the	O
top	O
right	O
the	O
resulting	O
distribution	O
by	O
monte	B
carlo	I
is	O
shown	O
in	O
the	O
shaded	O
gray	O
area	O
in	O
the	O
top	O
left	O
corner	O
the	O
best	O
gaussian	B
approximation	I
to	O
this	O
computed	O
from	O
e	O
and	O
var	B
by	O
monte	B
carlo	I
is	O
shown	O
by	O
the	O
solid	O
black	O
line	O
the	O
ekf	B
approximates	O
this	O
gaussian	B
as	O
follows	O
it	O
linearizes	O
the	O
g	O
function	O
at	O
the	O
current	O
mode	B
and	O
then	O
passes	O
the	O
gaussian	B
distribution	O
px	O
through	O
this	O
linearized	O
function	O
in	O
this	O
example	O
the	O
result	O
is	O
quite	O
a	O
good	O
approximation	O
to	O
the	O
first	O
and	O
second	O
moments	O
of	O
py	O
for	O
much	O
less	O
cost	O
than	O
an	O
mc	O
approximation	O
in	O
more	O
detail	O
the	O
method	O
works	O
as	O
follows	O
we	O
approximate	O
the	O
measurement	O
model	O
using	O
pytzt	O
n	O
tt	O
tyt	O
tt	O
rt	O
where	O
ht	O
is	O
the	O
jacobian	B
matrix	I
of	O
h	O
evaluated	O
at	O
the	O
prior	O
mode	B
hij	O
hiz	O
zj	O
ht	O
hz	O
tt	O
approximate	O
online	O
inference	B
for	O
non-linear	O
non-gaussian	O
ssms	O
py	O
gaussian	B
of	O
py	O
mean	B
of	O
py	O
ekf	B
gaussian	B
mean	B
of	O
ekf	B
y	O
py	O
x	O
g	O
y	O
x	O
p	O
function	O
gx	O
taylor	O
approx	O
mean	B
g	O
px	O
mean	B
x	O
x	O
figure	O
nonlinear	O
transformation	O
of	O
a	O
gaussian	B
random	O
variable	O
the	O
prior	O
px	O
is	O
shown	O
on	O
the	O
bottom	O
right	O
the	O
function	O
y	O
gx	O
is	O
shown	O
on	O
the	O
top	O
right	O
the	O
transformed	O
distribution	O
py	O
is	O
shown	O
in	O
the	O
top	O
left	O
a	O
linear	O
function	O
induces	O
a	O
gaussian	B
distribution	O
but	O
a	O
non-linear	O
function	O
induces	O
a	O
complex	O
distribution	O
the	O
solid	O
line	O
is	O
the	O
best	O
gaussian	B
approximation	I
to	O
this	O
the	O
dotted	O
line	O
is	O
the	O
ekf	B
approximation	O
to	O
this	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
sebastian	O
thrun	O
similarly	O
we	O
approximate	O
the	O
system	O
model	O
using	O
pztzt	O
ut	O
n	O
t	O
tzt	O
t	O
qt	O
where	O
giju	O
giu	O
z	O
zj	O
gt	O
gutz	O
t	O
so	O
g	O
is	O
the	O
jacobian	B
matrix	I
of	O
g	O
evaluated	O
at	O
the	O
prior	O
mode	B
given	O
this	O
we	O
can	O
then	O
apply	O
the	O
kalman	O
filter	O
to	O
compute	O
the	O
posterior	O
as	O
follows	O
tt	O
gut	O
t	O
vtt	O
gtvt	O
t	O
qt	O
t	O
kt	O
vtt	O
t	O
tt	O
ktyt	O
h	O
tt	O
vt	O
kthtvtt	O
t	O
rt	O
chapter	O
state	B
space	I
models	O
actual	O
sigma-point	O
linearized	O
covariance	B
sigma	B
points	I
mean	B
f	O
x	O
f	O
i	O
i	O
y	O
true	O
mean	B
s-p	O
mean	B
f	O
x	O
y	O
a	O
p	O
p	O
at	O
x	O
true	O
covariance	B
s-p	O
covariance	B
transformed	O
sigma	B
points	I
a	O
xp	O
at	O
f	O
x	O
figure	O
an	O
example	O
of	O
the	O
unscented	B
transform	I
in	O
two	O
dimensions	O
source	O
and	O
der	O
merwe	O
used	O
with	O
kind	O
permission	O
of	O
eric	O
wan	O
we	O
see	O
that	O
the	O
only	O
difference	O
from	O
the	O
regular	B
kalman	O
filter	O
is	O
that	O
when	O
we	O
compute	O
the	O
state	B
prediction	O
we	O
use	O
gut	O
t	O
instead	O
of	O
at	O
t	O
btut	O
and	O
when	O
we	O
compute	O
the	O
measurement	O
update	O
we	O
use	O
h	O
tt	O
instead	O
of	O
ct	O
tt	O
it	O
is	O
possible	O
to	O
improve	O
performance	O
by	O
repeatedly	O
re-linearizing	O
the	O
equations	O
around	O
t	O
instead	O
of	O
tt	O
this	O
is	O
called	O
the	O
iterated	B
ekf	B
and	O
yields	O
better	O
results	O
although	O
it	O
is	O
of	O
course	O
slower	O
there	O
are	O
two	O
cases	O
when	O
the	O
ekf	B
works	O
poorly	O
the	O
first	O
is	O
when	O
the	O
prior	O
covariance	B
is	O
large	O
in	O
this	O
case	O
the	O
prior	O
distribution	O
is	O
broad	O
so	O
we	O
end	O
up	O
sending	O
a	O
lot	O
of	O
probability	O
mass	O
through	O
different	O
parts	O
of	O
the	O
function	O
that	O
are	O
far	O
from	O
the	O
mean	B
where	O
the	O
function	O
has	O
been	O
linearized	O
the	O
other	O
setting	O
where	O
the	O
ekf	B
works	O
poorly	O
is	O
when	O
the	O
function	O
is	O
highly	O
nonlinear	O
near	O
the	O
current	O
mean	B
in	O
section	O
we	O
will	O
discuss	O
an	O
algorithm	O
called	O
the	O
ukf	B
which	O
works	O
better	O
than	O
the	O
ekf	B
in	O
both	O
of	O
these	O
settings	O
unscented	O
kalman	O
filter	O
the	O
unscented	O
kalman	O
filter	O
is	O
a	O
better	O
version	O
of	O
the	O
ekf	B
and	O
uhlmann	O
it	O
is	O
so-called	O
because	O
it	O
doesn	O
t	O
stink	O
the	O
key	O
intuition	O
is	O
this	O
it	O
is	O
easier	O
to	O
approximate	O
a	O
gaussian	B
than	O
to	O
approximate	O
a	O
function	O
so	O
instead	O
of	O
performing	O
a	O
linear	O
approximation	O
to	O
the	O
function	O
and	O
passing	O
a	O
gaussian	B
through	O
it	O
instead	O
pass	O
a	O
deterministically	O
chosen	O
set	O
of	O
points	O
known	O
as	O
sigma	B
points	I
through	O
the	O
function	O
and	O
fit	O
a	O
gaussian	B
to	O
the	O
resulting	O
transformed	O
points	O
this	O
is	O
known	O
as	O
the	O
unscented	B
transform	I
and	O
is	O
sketched	O
in	O
figure	O
explain	O
this	O
figure	O
in	O
detail	O
below	O
approximate	O
online	O
inference	B
for	O
non-linear	O
non-gaussian	O
ssms	O
the	O
ukf	B
basically	O
uses	O
the	O
unscented	B
transform	I
twice	O
once	O
to	O
approximate	O
passing	O
through	O
the	O
system	O
model	O
g	O
and	O
once	O
to	O
approximate	O
passing	O
through	O
the	O
measurement	O
model	O
h	O
we	O
give	O
the	O
details	O
below	O
note	O
that	O
the	O
ukf	B
and	O
ekf	B
both	O
perform	O
operations	O
per	O
time	O
step	O
where	O
d	O
is	O
the	O
size	O
of	O
the	O
latent	B
state-space	O
however	O
the	O
ukf	B
is	O
accurate	O
to	O
at	O
least	O
second	B
order	I
whereas	O
the	O
ekf	B
is	O
only	O
a	O
first	O
order	O
approximation	O
both	O
the	O
ekf	B
and	O
ukf	B
can	O
be	O
extended	O
to	O
capture	O
higher	O
order	O
terms	O
furthermore	O
the	O
unscented	B
transform	I
does	O
not	O
require	O
the	O
analytic	O
evaluation	O
of	O
any	O
derivatives	O
or	O
jacobians	O
so-called	O
derivative	O
free	O
filter	O
making	O
it	O
simpler	O
to	O
implement	O
and	O
more	O
widely	O
applicable	O
the	O
unscented	B
transform	I
before	O
explaining	O
the	O
ukf	B
we	O
first	O
explain	O
the	O
unscented	B
transform	I
assume	O
px	O
n	O
and	O
consider	O
estimating	O
py	O
where	O
y	O
f	O
for	O
some	O
nonlinear	O
function	O
f	O
the	O
unscented	B
transform	I
does	O
this	O
as	O
follows	O
first	O
we	O
create	O
a	O
set	O
of	O
sigma	B
points	I
xi	O
given	O
by	O
x	O
where	O
d	O
is	O
a	O
scaling	O
parameter	B
to	O
be	O
specified	O
below	O
and	O
the	O
notation	O
mi	B
means	O
the	O
i	O
th	O
column	O
of	O
matrix	O
m	O
these	O
sigma	B
points	I
are	O
propagated	O
through	O
the	O
nonlinear	O
function	O
to	O
yield	O
yi	O
f	O
and	O
the	O
mean	B
and	O
covariance	B
for	O
y	O
is	O
computed	O
as	O
follows	O
y	O
y	O
wi	O
myi	O
cyi	O
yyi	O
yt	O
wi	O
where	O
the	O
w	O
s	O
are	O
weighting	O
terms	O
given	O
by	O
wi	O
m	O
wi	O
c	O
d	O
d	O
wi	O
m	O
wi	O
c	O
see	O
figure	O
for	O
an	O
illustration	O
in	O
general	O
the	O
optimal	O
values	O
of	O
and	O
are	O
problem	O
dependent	O
but	O
when	O
d	O
they	O
are	O
thus	O
in	O
the	O
case	O
so	O
the	O
sigma	B
points	I
are	O
and	O
the	O
ukf	B
algorithm	O
the	O
ukf	B
algorithm	O
is	O
simply	O
two	O
applications	O
of	O
the	O
unscented	O
tranform	O
one	O
to	O
compute	O
and	O
the	O
other	O
to	O
compute	O
we	O
give	O
the	O
details	O
below	O
chapter	O
state	B
space	I
models	O
the	O
first	O
step	O
is	O
to	O
approximate	O
the	O
predictive	B
density	O
n	O
t	O
t	O
by	O
passing	O
the	O
old	O
belief	B
state	B
n	O
t	O
t	O
through	O
the	O
system	O
model	O
g	O
as	O
follows	O
t	O
t	O
t	O
t	O
z	O
i	O
t	O
gut	O
t	O
mz	O
i	O
wi	O
t	O
t	O
t	O
t	O
cz	O
i	O
wi	O
t	O
tz	O
i	O
t	O
t	O
t	O
t	O
d	O
t	O
y	O
i	O
t	O
t	O
my	O
i	O
wi	O
yt	O
t	O
t	O
t	O
zy	O
t	O
t	O
ty	O
i	O
t	O
ytt	O
cz	O
i	O
wi	O
t	O
s	O
t	O
kt	O
zy	O
t	O
t	O
ktyt	O
yt	O
t	O
t	O
ktstkt	O
t	O
the	O
second	O
step	O
is	O
to	O
approximate	O
the	O
likelihood	B
pytzt	O
n	O
yt	O
st	O
by	O
passing	O
the	O
where	O
prior	O
n	O
t	O
t	O
through	O
the	O
observation	B
model	I
h	O
tid	O
t	O
tid	O
cy	O
i	O
wi	O
t	O
yty	O
i	O
t	O
ytt	O
rt	O
st	O
finally	O
we	O
use	O
bayes	B
rule	I
for	O
gaussians	O
to	O
get	O
the	O
posterior	O
n	O
t	O
t	O
assumed	O
density	O
filtering	B
in	O
this	O
section	O
we	O
discuss	O
inference	B
where	O
we	O
perform	O
an	O
exact	O
update	O
step	O
but	O
then	O
approximate	O
the	O
posterior	O
by	O
a	O
distribution	O
of	O
a	O
certain	O
convenient	O
form	O
such	O
as	O
a	O
gaussian	B
more	O
precisely	O
let	O
the	O
unknowns	O
that	O
we	O
want	O
to	O
infer	O
be	O
denoted	O
by	O
t	O
suppose	O
that	O
q	O
is	O
a	O
set	O
of	O
tractable	O
distributions	O
e	O
g	O
gaussians	O
with	O
a	O
diagonal	B
covariance	B
matrix	I
or	O
a	O
product	O
of	O
discrete	B
distributions	O
suppose	O
that	O
we	O
have	O
an	O
approximate	O
prior	O
qt	O
t	O
p	O
t	O
where	O
qt	O
q	O
we	O
can	O
update	O
this	O
with	O
the	O
new	O
measurement	O
to	O
get	O
the	O
approximate	O
posterior	O
p	O
t	O
zt	O
pyt	O
tqtt	O
t	O
approximate	O
online	O
inference	B
for	O
non-linear	O
non-gaussian	O
ssms	O
u	O
pdate	O
qtt	O
predict	O
qt	O
u	O
pdate	O
predict	O
p	O
r	O
o	O
j	O
e	O
c	O
t	O
pt	O
p	O
r	O
o	O
j	O
e	O
c	O
t	O
qt	O
t	O
t	O
st	O
st	O
yt	O
yt	O
xt	O
xt	O
figure	O
ical	O
logistic	B
regression	B
model	O
compare	O
to	O
figure	O
illustration	O
of	O
the	O
predict-update-project	B
cycle	B
of	O
assumed	O
density	O
filtering	B
a	O
dynam	O
where	O
zt	O
pyt	O
tqtt	O
td	O
t	O
is	O
the	O
normalization	O
constant	O
and	O
qtt	O
t	O
p	O
t	O
t	O
t	O
t	O
is	O
the	O
one	O
step	O
ahead	O
predictive	B
distribution	O
if	O
the	O
prior	O
is	O
from	O
a	O
suitably	O
restricted	O
family	B
this	O
one-step	O
update	O
process	O
is	O
usually	O
tractable	O
however	O
we	O
often	O
find	O
that	O
the	O
resulting	O
posterior	O
is	O
no	O
longer	O
in	O
our	O
tractable	O
family	B
p	O
t	O
q	O
so	O
after	O
updating	O
we	O
seek	O
the	O
best	O
tractable	O
approximation	O
by	O
computing	O
q	O
t	O
argmin	O
q	O
q	O
kl	O
p	O
tq	O
t	O
this	O
minimizes	O
the	O
the	O
kullback-leibler	B
divergence	I
from	O
the	O
approximation	O
q	O
t	O
to	O
the	O
exact	O
posterior	O
p	O
t	O
and	O
can	O
be	O
thought	O
of	O
as	O
projecting	O
p	O
onto	O
the	O
space	O
of	O
tractable	O
distributions	O
the	O
whole	O
algorithm	O
consists	O
of	O
predict-update-project	B
cycles	O
this	O
is	O
known	O
as	O
assumed	O
density	O
filtering	B
or	O
adf	B
see	O
figure	O
for	O
a	O
sketch	O
if	O
q	O
is	O
in	O
the	O
exponential	B
family	B
one	O
can	O
show	O
that	O
this	O
kl	O
minimization	O
can	O
be	O
done	O
by	O
moment	B
matching	I
we	O
give	O
some	O
examples	O
of	O
this	O
below	O
boyen-koller	B
algorithm	O
for	O
online	O
inference	B
in	O
dbns	O
if	O
we	O
are	O
performing	O
inference	B
in	O
a	O
discrete-state	O
dynamic	B
bayes	I
net	I
where	O
tj	O
is	O
the	O
j	O
th	O
hidden	B
variable	I
at	O
time	O
t	O
then	O
the	O
exact	O
posterior	O
p	O
t	O
becomes	O
intractable	O
to	O
compute	O
because	O
of	O
the	O
entanglement	B
problem	I
suppose	O
we	O
use	O
a	O
fully	O
factored	O
approximation	O
cat	O
tj	O
tj	O
where	O
tjk	O
q	O
tj	O
k	O
is	O
the	O
probability	O
variable	O
of	O
the	O
form	O
q	O
t	O
j	O
is	O
in	O
state	B
k	O
and	O
d	O
is	O
the	O
number	O
of	O
variables	O
in	O
this	O
case	O
the	O
moment	B
matching	I
operation	O
becomes	O
tjk	O
p	O
tj	O
k	O
chapter	O
state	B
space	I
models	O
this	O
can	O
be	O
computed	O
by	O
performing	O
a	O
predict-update	O
step	O
using	O
the	O
factored	O
prior	O
and	O
then	O
computing	O
the	O
posterior	O
marginals	O
this	O
is	O
known	O
as	O
the	O
boyen-koller	B
algorithm	O
named	O
after	O
the	O
authors	O
of	O
and	O
koller	O
who	O
demonstrated	O
that	O
the	O
error	O
incurred	O
by	O
this	O
series	O
of	O
repeated	O
approximations	O
remains	O
bounded	O
certain	O
assumptions	O
about	O
the	O
stochasticity	O
of	O
the	O
system	O
gaussian	B
approximation	I
for	O
online	O
inference	B
in	O
glms	O
n	O
tj	O
tj	O
tj	O
where	O
tj	O
is	O
the	O
variance	B
then	O
the	O
optimal	O
now	O
suppose	O
q	O
t	O
parameters	O
of	O
the	O
tractable	O
approximation	O
to	O
the	O
posterior	O
are	O
tj	O
e	O
p	O
tj	O
tj	O
var	B
p	O
tj	O
this	O
method	O
can	O
be	O
used	O
to	O
do	O
online	O
inference	B
for	O
the	O
parameters	O
of	O
many	O
statistical	O
models	O
for	O
example	O
thetrueskill	O
system	O
used	O
in	O
microsoft	B
s	O
xbox	B
to	O
rank	O
players	O
over	O
time	O
uses	O
this	O
form	O
of	O
approximation	O
et	O
al	O
we	O
can	O
also	O
apply	O
this	O
method	O
to	O
simpler	O
models	O
such	O
as	O
glm	B
which	O
have	O
the	O
advantage	O
that	O
the	O
posterior	O
is	O
log-concave	O
below	O
we	O
explain	O
how	O
to	O
do	O
this	O
for	O
binary	O
logistic	B
regression	B
following	O
the	O
presentation	O
of	O
the	O
model	O
has	O
the	O
form	O
pytxt	O
t	O
berytsigmxt	O
p	O
t	O
t	O
t	O
t	O
t	O
t	O
where	O
is	O
some	O
process	O
noise	O
which	O
allows	O
the	O
parameters	O
to	O
change	O
slowly	O
over	O
time	O
can	O
be	O
set	O
to	O
as	O
in	O
the	O
recursive	B
least	B
squares	I
method	O
if	O
desired	O
we	O
will	O
j	O
n	O
t	O
t	O
t	O
is	O
the	O
tractable	O
prior	O
we	O
can	O
compute	O
the	O
assume	O
qt	O
t	O
one-step-ahead	B
predictive	B
density	I
qtt	O
t	O
using	O
the	O
standard	O
linear-gaussian	O
update	O
so	O
now	O
we	O
concentrate	O
on	O
the	O
measurement	O
update	O
step	O
define	O
the	O
deterministic	O
quantity	O
st	O
t	O
if	O
qtt	O
t	O
j	O
n	O
tj	O
tt	O
tt	O
then	O
we	O
can	O
compute	O
the	O
predictive	B
distribution	O
for	O
st	O
as	O
follows	O
qtt	O
vtt	O
t	O
xt	O
as	O
shown	O
in	O
figure	O
j	O
j	O
mtt	O
vtt	O
xtj	O
tt	O
tj	O
tt	O
the	O
posterior	O
for	O
st	O
is	O
given	O
by	O
qtst	O
vt	O
mt	O
vt	O
zt	O
st	O
zt	O
zt	O
pytstqtt	O
pytstqtt	O
t	O
t	O
pytstqtt	O
hybrid	O
discretecontinuous	O
ssms	O
where	O
pytst	O
berytst	O
these	O
integrals	O
are	O
one	O
dimensional	O
and	O
so	O
can	O
be	O
computed	O
using	O
gaussian	B
quadrature	O
for	O
details	O
this	O
is	O
the	O
same	O
as	O
one	O
step	O
of	O
the	O
ukf	B
algorithm	O
having	O
inferred	O
qst	O
we	O
need	O
to	O
compute	O
q	O
this	O
can	O
be	O
done	O
as	O
follows	O
define	O
m	O
as	O
the	O
change	O
in	O
the	O
mean	B
of	O
st	O
and	O
v	O
as	O
the	O
change	O
in	O
the	O
variance	B
mt	O
mtt	O
m	O
vt	O
vtt	O
v	O
then	O
one	O
can	O
show	O
that	O
the	O
new	O
factored	O
posterior	O
over	O
the	O
model	O
parameters	O
is	O
given	O
by	O
q	O
tj	O
tj	O
tj	O
tj	O
tj	O
tt	O
aj	O
m	O
tj	O
tt	O
j	O
v	O
xtj	O
tt	O
aj	O
thus	O
we	O
see	O
that	O
the	O
parameters	O
which	O
correspond	O
to	O
inputs	O
with	O
larger	O
magnitude	O
or	O
larger	O
uncertainty	B
tt	O
get	O
updated	O
most	O
which	O
makes	O
intuitive	O
sense	O
tt	O
in	O
a	O
version	O
of	O
this	O
algorithm	O
is	O
derived	O
using	O
a	O
probit	B
likelihood	B
section	O
in	O
this	O
case	O
the	O
measurement	O
update	O
can	O
be	O
done	O
in	O
closed	O
form	O
without	O
the	O
need	O
for	O
numerical	O
in	O
either	O
case	O
the	O
algorithm	O
only	O
takes	O
od	O
operations	O
per	O
time	O
step	O
so	O
it	O
can	O
integration	O
be	O
applied	O
to	O
models	O
with	O
large	O
numbers	O
of	O
parameters	O
and	O
since	O
it	O
is	O
an	O
online	O
algorithm	O
it	O
can	O
also	O
handle	O
massive	O
datasets	O
for	O
example	O
et	O
al	O
use	O
a	O
version	O
of	O
this	O
algorithm	O
to	O
fit	O
a	O
multi-class	O
classifier	O
online	O
to	O
very	O
large	O
datasets	O
they	O
beat	O
alternative	O
bayesian	B
online	B
learning	B
algorithms	O
and	O
sometimes	O
even	O
outperform	O
state	B
of	O
the	O
art	O
batch	B
learning	B
methods	O
such	O
as	O
svms	O
in	O
section	O
hybrid	O
discretecontinuous	O
ssms	O
many	O
systems	O
contain	O
both	O
discrete	B
and	O
continuous	O
hidden	B
variables	I
these	O
are	O
known	O
as	O
hybrid	B
systems	I
for	O
example	O
the	O
discrete	B
variables	O
may	O
indicate	O
whether	O
a	O
measurement	O
sensor	O
is	O
faulty	O
or	O
not	O
or	O
which	O
regime	O
the	O
system	O
is	O
in	O
we	O
will	O
see	O
some	O
other	O
examples	O
below	O
a	O
special	O
case	O
of	O
a	O
hybrid	O
system	O
is	O
when	O
we	O
combine	O
an	O
hmm	B
and	O
an	O
lg-ssm	B
this	O
is	O
called	O
a	O
switching	B
linear	B
dynamical	I
system	I
a	O
jump	B
markov	B
linear	I
system	I
or	O
a	O
switching	B
state	B
space	I
model	I
more	O
precisely	O
we	O
have	O
a	O
discrete	B
latent	B
variable	O
qt	O
k	O
a	O
continuous	O
latent	B
variable	O
zt	O
r	O
l	O
an	O
continuous	O
observed	O
response	O
yt	O
r	O
u	O
we	O
then	O
assume	O
that	O
the	O
continuous	O
variables	O
have	O
linear	B
gaussian	B
cpds	O
conditional	O
on	O
the	O
discrete	B
states	O
d	O
and	O
an	O
optional	O
continuous	O
observed	O
input	O
or	O
control	O
ut	O
r	O
pqt	O
kqt	O
j	O
ij	O
pztzt	O
qt	O
k	O
ut	O
bkut	O
qk	O
pytzt	O
qt	O
k	O
ut	O
dkut	O
rk	O
see	O
figure	O
for	O
the	O
dgm	B
representation	O
chapter	O
state	B
space	I
models	O
ut	O
qt	O
ut	O
qt	O
zt	O
zt	O
yt	O
yt	O
figure	O
a	O
switching	B
linear	B
dynamical	I
system	I
squares	O
represent	O
discrete	B
nodes	B
circles	O
represent	O
continuous	O
nodes	B
illustration	O
of	O
how	O
the	O
number	O
of	O
modes	O
in	O
the	O
belief	B
state	B
grows	O
exponentially	O
over	O
time	O
we	O
assume	O
there	O
are	O
two	O
binary	O
states	O
inference	B
unfortunately	O
inference	B
state	B
estimation	I
in	O
hybrid	O
models	O
including	O
the	O
switching	O
lgis	O
intractable	O
to	O
see	O
why	O
suppose	O
qt	O
is	O
binary	O
but	O
that	O
only	O
the	O
dynamics	O
ssm	B
model	O
a	O
depend	O
on	O
qt	O
not	O
the	O
observation	B
matrix	O
our	O
initial	O
belief	B
state	B
will	O
be	O
a	O
mixture	B
of	I
gaussians	I
corresponding	O
to	O
and	O
the	O
one-step-ahead	B
predictive	B
density	I
will	O
be	O
a	O
mixture	B
of	I
gaussians	I
and	O
obtained	O
by	O
passing	O
each	O
of	O
the	O
prior	O
modes	O
through	O
the	O
possible	O
transition	O
models	O
the	O
belief	B
state	B
at	O
step	O
will	O
also	O
be	O
a	O
mixture	B
of	I
gaussians	I
obtained	O
by	O
updating	O
each	O
of	O
the	O
above	O
distributions	O
with	O
at	O
step	O
the	O
belief	B
state	B
will	O
be	O
a	O
mixture	B
of	I
gaussians	I
and	O
so	O
on	O
so	O
we	O
see	O
there	O
is	O
an	O
exponential	O
explosion	O
in	O
the	O
number	O
of	O
modes	O
figure	O
various	O
approximate	B
inference	B
methods	O
have	O
been	O
proposed	O
for	O
this	O
model	O
such	O
as	O
the	O
following	O
prune	O
off	O
low	O
probability	O
trajectories	O
in	O
the	O
discrete	B
tree	B
this	O
is	O
the	O
basis	O
of	O
multiple	B
hypothesis	I
tracking	B
and	O
fortmann	O
bar-shalom	O
and	O
li	O
use	O
monte	B
carlo	I
essentially	O
we	O
just	O
sample	O
discrete	B
trajectories	O
and	O
apply	O
an	O
analytical	O
filter	O
to	O
the	O
continuous	O
variables	O
conditional	O
on	O
a	O
trajectory	O
see	O
section	O
for	O
details	O
use	O
adf	B
where	O
we	O
approximate	O
the	O
exponentially	O
large	O
mixture	B
of	I
gaussians	I
with	O
a	O
smaller	O
mixture	B
of	I
gaussians	I
see	O
section	O
for	O
details	O
a	O
gaussian	B
sum	O
filter	O
for	O
switching	O
ssms	O
a	O
gaussian	B
sum	O
filter	O
and	O
alspach	O
approximates	O
the	O
belief	B
state	B
at	O
each	O
step	O
by	O
a	O
mixture	B
of	O
k	O
gaussians	O
this	O
can	O
be	O
implemented	O
by	O
running	O
k	O
kalman	O
filters	O
in	O
hybrid	O
discretecontinuous	O
ssms	O
filter	O
t	O
merge	O
t	O
b	O
b	O
bbn	O
merge	O
t	O
t	O
t	O
b	O
b	O
filter	O
t	O
b	O
filter	O
t	O
filter	O
t	O
t	O
t	O
merge	O
t	O
filter	O
t	O
t	O
filter	O
t	O
figure	O
adf	B
for	O
a	O
switching	B
linear	B
dynamical	I
system	I
for	O
details	O
method	O
imm	B
method	O
see	O
text	O
parallel	O
this	O
is	O
particularly	O
well	O
suited	O
to	O
switching	O
ssms	O
we	O
now	O
describe	O
one	O
version	O
of	O
this	O
algorithm	O
known	O
as	O
the	O
second	B
order	I
generalized	O
pseudo	O
bayes	O
filter	O
and	O
fortmann	O
we	O
assume	O
that	O
the	O
prior	O
belief	B
state	B
bt	O
is	O
a	O
mixture	B
of	O
k	O
gaussians	O
one	O
per	O
discrete	B
state	B
t	O
pzt	O
qt	O
t	O
t	O
t	O
bi	O
we	O
then	O
pass	O
this	O
through	O
the	O
k	O
different	O
linear	O
models	O
to	O
get	O
t	O
pzt	O
qt	O
i	O
qt	O
tijn	O
tij	O
tij	O
bij	O
where	O
tij	O
t	O
jqt	O
i	O
finally	O
for	O
each	O
value	O
of	O
j	O
we	O
collapse	O
the	O
k	O
gaussian	B
mixtures	O
down	O
to	O
a	O
single	O
mixture	B
to	O
give	O
t	O
pzt	O
qt	O
tjn	O
tj	O
tj	O
bj	O
chapter	O
state	B
space	I
models	O
see	O
figure	O
for	O
a	O
sketch	O
q	O
arg	O
minq	O
kl	O
where	O
pz	O
be	O
solved	O
by	O
moment	B
matching	I
that	O
is	O
the	O
optimal	O
way	O
to	O
approximate	O
a	O
mixture	B
of	I
gaussians	I
with	O
a	O
single	O
gaussian	B
is	O
given	O
by	O
k	O
kn	O
k	O
k	O
and	O
qz	O
n	O
this	O
can	O
e	O
k	O
k	O
k	O
cov	O
k	O
k	O
k	O
k	O
k	O
in	O
the	O
graphical	B
model	I
literature	O
this	O
is	O
called	O
weak	B
marginalization	I
since	O
it	O
preserves	O
the	O
first	O
two	O
moments	O
applying	O
these	O
equations	O
to	O
our	O
model	O
we	O
can	O
go	O
from	O
bij	O
to	O
t	O
bj	O
t	O
as	O
follows	O
we	O
drop	O
the	O
t	O
subscript	O
for	O
brevity	O
ij	O
i	O
i	O
ji	O
ij	O
j	O
ji	O
j	O
j	O
ji	O
ij	O
ij	O
j	O
ij	O
jt	O
i	O
this	O
algorithm	O
requires	O
running	O
k	O
filters	O
at	O
each	O
step	O
a	O
cheaper	O
alternative	O
is	O
to	O
represent	O
the	O
belief	B
state	B
by	O
a	O
single	O
gaussian	B
marginalizing	O
over	O
the	O
discrete	B
switch	O
at	O
each	O
step	O
this	O
is	O
a	O
straightforward	O
application	O
of	O
adf	B
an	O
offline	B
extension	B
to	O
this	O
method	O
called	O
expectation	B
correction	I
is	O
described	O
in	O
mesot	O
and	O
barber	O
another	O
heuristic	O
approach	O
known	O
as	O
interactive	B
multiple	I
models	I
or	O
imm	B
and	O
fortmann	O
can	O
be	O
obtained	O
by	O
first	O
collapsing	O
the	O
prior	O
to	O
a	O
single	O
gaussian	B
moment	B
matching	I
and	O
then	O
updating	O
it	O
using	O
k	O
different	O
kalman	O
filters	O
one	O
per	O
value	O
of	O
qt	O
see	O
figure	O
for	O
a	O
sketch	O
application	O
data	B
association	I
and	O
multi-target	B
tracking	B
suppose	O
we	O
are	O
tracking	B
k	O
objects	O
such	O
as	O
airplanes	O
and	O
at	O
time	O
t	O
we	O
observe	O
detection	O
events	O
e	O
g	O
blips	O
on	O
a	O
radar	B
screen	O
we	O
can	O
have	O
k	O
due	O
to	O
occlusion	O
or	O
missed	O
detections	O
we	O
can	O
have	O
k	O
due	O
to	O
clutter	O
or	O
false	O
alarms	O
or	O
we	O
can	O
have	O
k	O
in	O
any	O
case	O
we	O
need	O
to	O
figure	O
out	O
the	O
correspondence	B
between	O
the	O
detections	O
ytk	O
and	O
the	O
k	O
objects	O
ztj	O
this	O
is	O
called	O
the	O
problem	O
of	O
data	B
association	I
and	O
it	O
arises	O
in	O
many	O
application	O
domains	O
figure	O
gives	O
an	O
example	O
in	O
which	O
we	O
are	O
tracking	B
k	O
objects	O
at	O
each	O
time	O
step	O
qt	O
is	O
the	O
unknown	B
mapping	O
which	O
specifies	O
which	O
objects	O
caused	O
which	O
observations	O
it	O
specifies	O
the	O
wiring	O
diagram	O
for	O
time	O
slice	O
t	O
the	O
standard	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
compute	O
a	O
weight	O
which	O
measures	O
the	O
compatibility	O
between	O
object	O
j	O
and	O
measurement	O
k	O
typically	O
based	O
on	O
how	O
close	O
k	O
is	O
to	O
where	O
the	O
model	O
thinks	O
j	O
should	O
be	O
so-called	O
nearest	B
neighbor	I
data	B
association	I
heuristic	O
this	O
gives	O
us	O
a	O
k	O
weight	O
matrix	O
we	O
can	O
make	O
this	O
into	O
a	O
hybrid	O
discretecontinuous	O
ssms	O
zt	O
zt	O
yt	O
yt	O
yt	O
qt	O
qt	O
figure	O
a	O
model	O
for	O
tracking	B
two	O
objects	O
in	O
the	O
presence	O
of	O
data-assocation	O
ambiguity	O
we	O
observe	O
and	O
detections	O
in	O
the	O
first	O
three	O
time	O
steps	O
square	O
matrix	O
of	O
size	O
n	O
n	O
wheren	O
maxk	O
by	O
adding	O
dummy	O
background	O
objects	O
which	O
can	O
explain	O
all	O
the	O
false	O
alarms	O
and	O
adding	O
dummy	O
observations	O
which	O
can	O
explain	O
all	O
the	O
missed	O
detections	O
we	O
can	O
then	O
compute	O
the	O
maximal	B
weight	I
bipartite	I
matching	I
using	O
the	O
hungarian	B
algorithm	I
which	O
takes	O
on	O
time	O
e	O
g	O
et	O
al	O
conditional	O
on	O
this	O
we	O
can	O
perform	O
a	O
kalman	O
filter	O
update	O
where	O
objects	O
that	O
are	O
assigned	O
to	O
dummy	O
observations	O
do	O
not	O
perform	O
a	O
measurement	O
update	O
an	O
extension	B
of	O
this	O
method	O
to	O
handle	O
a	O
variable	O
andor	O
unknown	B
number	O
of	O
objects	O
is	O
known	O
as	O
multi-target	B
tracking	B
this	O
requires	O
dealing	O
with	O
a	O
variable-sized	O
state	B
space	I
there	O
are	O
many	O
ways	O
to	O
do	O
this	O
but	O
perhaps	O
the	O
simplest	O
and	O
most	O
robust	B
methods	O
are	O
based	O
on	O
sequential	B
monte	B
carlo	I
et	O
al	O
or	O
mcmc	B
et	O
al	O
oh	O
et	O
al	O
application	O
fault	B
diagnosis	I
consider	O
the	O
model	O
in	O
figure	O
this	O
represents	O
an	O
industrial	O
plant	O
consisting	O
of	O
various	O
tanks	O
of	O
liquid	O
interconnected	O
by	O
pipes	O
in	O
this	O
example	O
we	O
just	O
have	O
two	O
tanks	O
for	O
simplicity	O
we	O
want	O
to	O
estimate	O
the	O
pressure	O
inside	O
each	O
tank	O
based	O
on	O
a	O
noisy	O
measurement	O
of	O
the	O
flow	O
into	O
and	O
out	O
of	O
each	O
tank	O
however	O
the	O
measurement	O
devices	O
can	O
sometimes	O
fail	O
furthermore	O
pipes	O
can	O
burst	O
or	O
get	O
blocked	O
we	O
call	O
this	O
a	O
resistance	O
failure	O
this	O
model	O
is	O
widely	O
used	O
as	O
a	O
benchmark	O
in	O
the	O
fault	B
diagnosis	I
community	O
and	O
biswas	O
we	O
can	O
create	O
a	O
probabilistic	O
model	O
of	O
the	O
system	O
as	O
shown	O
in	O
figure	O
the	O
square	O
nodes	B
represent	O
discrete	B
variables	O
such	O
as	O
measurement	O
failures	O
and	O
resistance	O
failures	O
the	O
remaining	O
variables	O
are	O
continuous	O
a	O
variety	O
of	O
approximate	B
inference	B
algorithms	O
can	O
be	O
applied	O
to	O
this	O
model	O
see	O
and	O
lerner	O
for	O
one	O
approach	O
based	O
on	O
rao-blackwellized	O
particle	O
filtering	B
is	O
explained	O
in	O
section	O
chapter	O
state	B
space	I
models	O
rf	O
rf	O
m	O
f	O
m	O
f	O
f	O
m	O
f	O
m	O
p	O
p	O
m	O
f	O
m	O
f	O
f	O
m	O
f	O
m	O
rf	O
p	O
rf	O
rf	O
p	O
f	O
m	O
f	O
m	O
m	O
f	O
m	O
f	O
rf	O
figure	O
the	O
two-tank	O
system	O
the	O
goal	O
is	O
to	O
infer	O
when	O
pipes	O
are	O
blocked	O
or	O
have	O
burst	O
or	O
sensors	O
have	O
broken	O
from	O
observations	O
of	O
the	O
flow	O
out	O
of	O
tank	O
f	O
out	O
of	O
tank	O
f	O
or	O
between	O
tanks	O
and	O
f	O
is	O
a	O
hidden	B
variable	I
representing	O
the	O
resistance	O
of	O
the	O
pipe	O
out	O
of	O
tank	O
p	O
is	O
a	O
hidden	B
variable	I
representing	O
the	O
pressure	O
in	O
tank	O
etc	O
source	O
figure	O
of	O
and	O
lerner	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
dynamic	B
bayes	I
net	I
representation	O
of	O
the	O
two-tank	O
system	O
discrete	B
nodes	B
are	O
squares	O
continuous	O
nodes	B
are	O
circles	O
abbreviations	O
r	O
resistance	O
p	O
pressure	O
f	O
flow	O
m	O
measurement	O
rf	O
resistance	O
failure	O
mf	O
measurement	O
failure	O
based	O
on	O
figure	O
of	O
and	O
lerner	O
application	O
econometric	B
forecasting	I
the	O
switching	O
lg-ssm	B
model	O
is	O
widely	O
used	O
in	O
econometric	B
forecasting	I
where	O
it	O
is	O
called	O
a	O
regime	B
switching	I
model	O
for	O
example	O
we	O
can	O
combine	O
two	O
linear	B
trend	I
models	O
section	O
one	O
in	O
which	O
bt	O
reflects	O
a	O
growing	O
economy	O
and	O
one	O
in	O
which	O
bt	O
reflects	O
a	O
shrinking	O
economy	O
see	O
and	O
harrison	O
for	O
further	O
details	O
exercises	O
exercise	O
derivation	O
of	O
em	B
for	O
lg-ssm	B
derive	O
the	O
e	O
and	O
m	O
steps	O
for	O
computing	O
a	O
optimal	O
mle	B
for	O
an	O
lg-ssm	B
model	O
hint	O
the	O
results	O
are	O
in	O
and	O
hinton	O
your	O
task	O
is	O
to	O
derive	O
these	O
results	O
exercise	O
seasonal	O
lg-ssm	B
model	O
in	O
standard	O
form	O
write	O
the	O
seasonal	O
model	O
in	O
figure	O
as	O
an	O
lg-ssm	B
define	O
the	O
matrices	O
a	O
c	O
q	O
and	O
r	O
undirected	B
graphical	B
models	I
random	O
fields	O
introduction	O
in	O
chapter	O
we	O
discussed	O
directed	B
graphical	B
models	I
commonly	O
known	O
as	O
bayes	O
nets	O
however	O
for	O
some	O
domains	O
being	O
forced	O
to	O
choose	O
a	O
direction	O
for	O
the	O
edges	B
as	O
required	O
by	O
a	O
dgm	B
is	O
rather	O
awkward	O
for	O
example	O
consider	O
modeling	O
an	O
image	O
we	O
might	O
suppose	O
that	O
the	O
intensity	O
values	O
of	O
neighboring	O
pixels	O
are	O
correlated	O
we	O
can	O
create	O
a	O
dag	B
model	O
with	O
a	O
lattice	B
topology	O
as	O
shown	O
in	O
figure	O
this	O
is	O
known	O
as	O
a	O
causal	B
mrf	B
or	O
a	O
markov	B
mesh	I
et	O
al	O
however	O
its	O
conditional	B
independence	I
properties	O
are	O
rather	O
unnatural	O
in	O
particular	O
the	O
markov	B
blanket	I
in	O
section	O
of	O
the	O
node	O
in	O
the	O
middle	O
is	O
the	O
other	O
colored	O
nodes	B
and	O
rather	O
than	O
just	O
its	O
nearest	O
neighbors	B
as	O
one	O
might	O
expect	O
an	O
alternative	O
is	O
to	O
use	O
an	O
undirected	B
graphical	B
model	I
also	O
called	O
a	O
markov	B
random	O
field	O
or	O
markov	B
network	I
these	O
do	O
not	O
require	O
us	O
to	O
specify	O
edge	O
orientations	O
and	O
are	O
much	O
more	O
natural	O
for	O
some	O
problems	O
such	O
as	O
image	O
analysis	O
and	O
spatial	O
statistics	O
for	O
example	O
an	O
undirected	B
lattice	B
is	O
shown	O
in	O
figure	O
now	O
the	O
markov	B
blanket	I
of	O
each	O
node	O
is	O
just	O
its	O
nearest	O
neighbors	B
as	O
we	O
show	O
in	O
section	O
roughly	O
speaking	O
the	O
main	O
advantages	O
of	O
ugms	O
over	O
dgms	O
are	O
they	O
are	O
symmetric	B
and	O
therefore	O
more	O
natural	O
for	O
certain	O
domains	O
such	O
as	O
spatial	O
or	O
relational	O
data	O
and	O
discriminativel	O
ugms	O
conditional	O
random	O
fields	O
or	O
crfs	O
which	O
define	O
conditional	O
densities	O
of	O
the	O
form	O
pyx	O
work	O
better	O
than	O
discriminative	B
dgms	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
the	O
main	O
disadvantages	O
of	O
ugms	O
compared	O
to	O
dgms	O
are	O
the	O
parameters	O
are	O
less	O
interpretable	O
and	O
less	O
modular	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
and	O
parameter	B
estimation	O
is	O
computationally	O
more	O
expensive	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
see	O
et	O
al	O
for	O
an	O
empirical	O
comparison	O
of	O
the	O
two	O
approaches	O
for	O
an	O
image	O
processing	O
task	O
conditional	B
independence	I
properties	O
of	O
ugms	O
key	O
properties	O
ugms	O
define	O
ci	B
relationships	O
via	O
simple	O
graph	B
separation	O
as	O
follows	O
for	O
sets	O
of	O
nodes	B
a	O
b	O
and	O
c	O
we	O
say	O
xa	O
g	O
xbxc	O
iff	B
c	O
separates	O
a	O
from	O
b	O
in	O
the	O
graph	B
g	O
this	O
means	O
that	O
when	O
we	O
remove	O
all	O
the	O
nodes	B
in	O
c	O
if	O
there	O
are	O
no	O
paths	O
connecting	O
any	O
node	O
in	O
a	O
to	O
any	O
node	O
in	O
b	O
then	O
the	O
ci	B
property	O
holds	O
this	O
is	O
called	O
the	O
global	B
markov	B
property	I
for	O
ugms	O
for	O
example	O
in	O
figure	O
we	O
have	O
that	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
a	O
lattice	B
represented	O
as	O
a	O
dag	B
the	O
dotted	O
red	O
node	O
is	O
independent	O
of	O
all	O
other	O
figure	O
nodes	B
given	O
its	O
markov	B
blanket	I
which	O
include	O
its	O
parents	B
children	B
and	O
co-parents	B
the	O
same	O
model	O
represented	O
as	O
a	O
ugm	B
the	O
red	O
node	O
is	O
independent	O
of	O
the	O
other	O
black	O
nodes	B
given	O
its	O
neighbors	B
nodes	B
figure	O
a	O
dgm	B
its	O
moralized	O
version	O
represented	O
as	O
a	O
ugm	B
the	O
set	O
of	O
nodes	B
that	O
renders	O
a	O
node	O
t	O
conditionally	B
independent	I
of	O
all	O
the	O
other	O
nodes	B
in	O
the	O
graph	B
is	O
called	O
t	O
s	O
markov	B
blanket	I
we	O
will	O
denote	O
this	O
by	O
mbt	O
formally	O
the	O
markov	B
blanket	I
satisfies	O
the	O
following	O
property	O
t	O
v	O
cltmbt	O
where	O
clt	O
mbt	O
is	O
the	O
closure	B
of	O
node	O
t	O
one	O
can	O
show	O
that	O
in	O
a	O
ugm	B
a	O
node	O
s	O
markov	B
blanket	I
is	O
its	O
set	O
of	O
immediate	O
neighbors	B
this	O
is	O
called	O
the	O
undirected	B
local	I
markov	B
property	I
for	O
example	O
in	O
figure	O
we	O
have	O
from	O
the	O
local	O
markov	B
property	O
we	O
can	O
easily	O
see	O
that	O
two	O
nodes	B
are	O
conditionally	B
independent	I
given	O
the	O
rest	O
if	O
there	O
is	O
no	O
direct	O
edge	O
between	O
them	O
this	O
is	O
called	O
the	O
pairwise	B
markov	B
property	I
in	O
symbols	O
this	O
is	O
written	O
as	O
s	O
tv	O
t	O
g	O
st	O
using	O
the	O
three	O
markov	B
properties	O
we	O
have	O
discussed	O
we	O
can	O
derive	O
the	O
following	O
ci	B
properties	O
others	O
from	O
the	O
ugm	B
in	O
figure	O
pairwise	O
local	O
conditional	B
independence	I
properties	O
of	O
ugms	O
g	O
l	O
p	O
px	O
figure	O
relationship	O
between	O
markov	B
properties	O
of	O
ugms	O
the	O
ancestral	B
graph	B
induced	O
by	O
the	O
dag	B
in	O
figure	O
wrt	O
u	O
figure	O
moralized	O
version	O
of	O
the	O
global	O
it	O
is	O
obvious	O
that	O
global	O
markov	B
implies	O
local	O
markov	B
which	O
implies	O
pairwise	O
markov	B
what	O
is	O
less	O
obvious	O
but	O
nevertheless	O
true	O
px	O
for	O
all	O
x	O
i	O
e	O
that	O
p	O
is	O
a	O
positive	O
density	O
is	O
that	O
pairwise	O
implies	O
global	O
and	O
hence	O
that	O
all	O
these	O
markov	B
properties	O
are	O
the	O
same	O
as	O
illustrated	O
in	O
figure	O
e	O
g	O
and	O
friedman	O
for	O
a	O
the	O
importance	O
of	O
this	O
result	O
is	O
that	O
it	O
is	O
usually	O
easier	O
to	O
empirically	O
assess	O
pairwise	O
conditional	B
independence	I
such	O
pairwise	O
ci	B
statements	O
can	O
be	O
used	O
to	O
construct	O
a	O
graph	B
from	O
which	O
global	O
ci	B
statements	O
can	O
be	O
extracted	O
an	O
undirected	B
alternative	O
to	O
d-separation	O
we	O
have	O
seen	O
that	O
determinining	O
ci	B
relationships	O
in	O
ugms	O
is	O
much	O
easier	O
than	O
in	O
dgms	O
because	O
we	O
do	O
not	O
have	O
to	O
worry	O
about	O
the	O
directionality	O
of	O
the	O
edges	B
in	O
this	O
section	O
we	O
show	O
how	O
to	O
determine	O
ci	B
relationships	O
for	O
a	O
dgm	B
using	O
a	O
ugm	B
it	O
is	O
tempting	O
to	O
simply	O
convert	O
the	O
dgm	B
to	O
a	O
ugm	B
by	O
dropping	O
the	O
orientation	O
of	O
the	O
edges	B
but	O
this	O
is	O
clearly	O
incorrect	O
since	O
a	O
v-structure	B
a	O
b	O
c	O
has	O
quite	O
different	O
ci	B
properties	O
than	O
the	O
corresponding	O
undirected	B
chain	O
a	O
b	O
c	O
the	O
latter	O
graph	B
incorrectly	O
states	O
that	O
a	O
cb	O
to	O
avoid	O
such	O
incorrect	O
ci	B
statements	O
we	O
can	O
add	O
edges	B
between	O
the	O
unmarried	O
parents	B
a	O
and	O
c	O
and	O
then	O
drop	O
the	O
arrows	O
from	O
the	O
edges	B
forming	O
this	O
case	O
a	O
fully	O
connected	O
undirected	B
graph	B
this	O
process	O
is	O
called	O
moralization	B
figure	O
gives	O
a	O
larger	O
the	O
restriction	O
to	O
positive	O
densities	O
arises	O
because	O
deterministic	O
constraints	O
can	O
result	O
in	O
independencies	O
present	O
in	O
the	O
distribution	O
that	O
are	O
not	O
explicitly	O
represented	O
in	O
the	O
graph	B
see	O
e	O
g	O
and	O
friedman	O
for	O
some	O
examples	O
distributions	O
with	O
non-graphical	O
ci	B
properties	O
are	O
said	O
to	O
be	O
unfaithful	B
to	O
the	O
graph	B
so	O
ip	B
ig	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
figure	O
dgms	O
and	O
ugms	O
can	O
perfectly	O
represent	O
different	O
sets	O
of	O
distributions	O
some	O
distributions	O
can	O
be	O
perfectly	O
represented	O
by	O
either	O
dgms	O
or	O
ugms	O
the	O
corresponding	O
graph	B
must	O
be	O
chordal	B
example	O
of	O
moralization	B
we	O
interconnect	O
and	O
since	O
they	O
have	O
a	O
common	O
child	O
and	O
we	O
interconnect	O
and	O
since	O
they	O
have	O
a	O
common	O
child	O
unfortunately	O
moralization	B
loses	O
some	O
ci	B
information	B
and	O
therefore	O
we	O
cannot	O
use	O
the	O
moralized	O
ugm	B
to	O
determine	O
ci	B
properties	O
of	O
the	O
dgm	B
for	O
example	O
in	O
figure	O
using	O
d-separation	O
we	O
see	O
that	O
adding	O
a	O
moralization	B
arc	O
would	O
lose	O
this	O
fact	O
figure	O
however	O
notice	O
that	O
the	O
moralization	B
edge	O
due	O
to	O
the	O
common	O
child	O
is	O
not	O
needed	O
if	O
we	O
do	O
not	O
observe	O
or	O
any	O
of	O
its	O
descendants	B
this	O
suggests	O
the	O
following	O
approach	O
to	O
determining	O
if	O
a	O
bc	O
first	O
we	O
form	O
the	O
ancestral	B
graph	B
of	O
dag	B
g	O
with	O
respect	O
to	O
u	O
a	O
b	O
c	O
this	O
means	O
we	O
remove	O
all	O
nodes	B
from	O
g	O
that	O
are	O
not	O
in	O
u	O
or	O
are	O
not	O
ancestors	B
of	O
u	O
we	O
then	O
moralize	O
this	O
ancestral	B
graph	B
and	O
apply	O
the	O
simple	O
graph	B
separation	O
rules	B
for	O
ugms	O
for	O
example	O
in	O
figure	O
we	O
show	O
the	O
ancestral	B
graph	B
for	O
figure	O
using	O
u	O
in	O
figure	O
we	O
show	O
the	O
moralized	O
version	O
of	O
this	O
graph	B
it	O
is	O
clear	O
that	O
we	O
now	O
correctly	O
conclude	O
that	O
comparing	O
directed	B
and	O
undirected	B
graphical	B
models	I
which	O
model	O
has	O
more	O
expressive	O
power	O
a	O
dgm	B
or	O
a	O
ugm	B
to	O
formalize	O
this	O
question	O
recall	B
that	O
we	O
say	O
that	O
g	O
is	O
an	O
i-map	B
of	O
a	O
distribution	O
p	O
if	O
ig	O
ip	B
now	O
define	O
g	O
to	O
be	O
perfect	B
map	I
of	O
p	O
if	O
ig	O
in	O
other	O
words	O
the	O
graph	B
can	O
represent	O
all	O
only	O
the	O
ci	B
properties	O
of	O
the	O
distribution	O
it	O
turns	O
out	O
that	O
dgms	O
and	O
ugms	O
are	O
perfect	O
maps	O
for	O
different	O
sets	O
of	O
distributions	O
figure	O
in	O
this	O
sense	O
neither	O
is	O
more	O
powerful	O
than	O
the	O
other	O
as	O
a	O
representation	O
language	O
as	O
an	O
example	O
of	O
some	O
ci	B
relationships	O
that	O
can	O
be	O
perfectly	O
modeled	O
by	O
a	O
dgm	B
but	O
not	O
a	O
ugm	B
consider	O
a	O
v-structure	B
a	O
c	O
b	O
this	O
asserts	O
that	O
a	O
b	O
and	O
a	O
bc	O
if	O
we	O
drop	O
the	O
arrows	O
we	O
get	O
a	O
c	O
b	O
which	O
asserts	O
a	O
bc	O
and	O
a	O
b	O
which	O
is	O
incorrect	O
in	O
fact	O
there	O
is	O
no	O
ugm	B
that	O
can	O
precisely	O
represent	O
all	O
and	O
only	O
the	O
two	O
ci	B
statements	O
encoded	O
by	O
a	O
vstructure	O
in	O
general	O
ci	B
properties	O
in	O
ugms	O
are	O
monotonic	O
in	O
the	O
following	O
sense	O
if	O
a	O
bc	O
then	O
a	O
bc	O
d	O
but	O
in	O
dgms	O
ci	B
properties	O
can	O
be	O
non-monotonic	O
since	O
conditioning	B
parameterization	O
of	O
mrfs	O
a	O
a	O
d	O
b	O
d	O
b	O
c	O
c	O
b	O
a	O
d	O
c	O
figure	O
a	O
ugm	B
and	O
two	O
failed	O
attempts	O
to	O
represent	O
it	O
as	O
a	O
dgm	B
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
on	O
extra	O
variables	O
can	O
eliminate	O
conditional	O
independencies	O
due	O
to	O
explaining	B
away	I
as	O
an	O
example	O
of	O
some	O
ci	B
relationships	O
that	O
can	O
be	O
perfectly	O
modeled	O
by	O
a	O
ugm	B
but	O
not	O
a	O
dgm	B
consider	O
the	O
shown	O
in	O
figure	O
one	O
attempt	O
to	O
model	O
this	O
with	O
a	O
dgm	B
is	O
shown	O
in	O
figure	O
this	O
correctly	O
asserts	O
that	O
a	O
cb	O
d	O
however	O
it	O
incorrectly	O
asserts	O
that	O
b	O
da	O
figure	O
is	O
another	O
incorrect	O
dgm	B
it	O
correctly	O
encodes	O
a	O
cb	O
d	O
but	O
incorrectly	O
encodes	O
b	O
d	O
in	O
fact	O
there	O
is	O
no	O
dgm	B
that	O
can	O
precisely	O
represent	O
all	O
and	O
only	O
the	O
ci	B
statements	O
encoded	O
by	O
this	O
ugm	B
some	O
distributions	O
can	O
be	O
perfectly	O
modeled	O
by	O
either	O
a	O
dgm	B
or	O
a	O
ugm	B
the	O
resulting	O
graphs	O
are	O
called	O
decomposable	B
or	O
chordal	B
roughly	O
speaking	O
this	O
means	O
the	O
following	O
if	O
we	O
collapse	O
together	O
all	O
the	O
variables	O
in	O
each	O
maximal	B
clique	B
to	O
make	O
mega-variables	O
the	O
resulting	O
graph	B
will	O
be	O
a	O
tree	B
of	O
course	O
if	O
the	O
graph	B
is	O
already	O
a	O
tree	B
includes	O
chains	O
as	O
a	O
special	O
case	O
it	O
will	O
be	O
chordal	B
see	O
section	O
for	O
further	O
details	O
parameterization	O
of	O
mrfs	O
although	O
the	O
ci	B
properties	O
of	O
ugm	B
are	O
simpler	O
and	O
more	O
natural	O
than	O
for	O
dgms	O
representing	O
the	O
joint	B
distribution	I
for	O
a	O
ugm	B
is	O
less	O
natural	O
than	O
for	O
a	O
dgm	B
as	O
we	O
see	O
below	O
the	O
hammersley-clifford	B
theorem	O
since	O
there	O
is	O
no	O
topological	B
ordering	I
associated	O
with	O
an	O
undirected	B
graph	B
we	O
can	O
t	O
use	O
the	O
chain	B
rule	I
to	O
represent	O
py	O
so	O
instead	O
of	O
associating	O
cpds	O
with	O
each	O
node	O
we	O
associate	O
potential	O
functions	O
orfactors	O
with	O
each	O
maximal	B
clique	B
in	O
the	O
graph	B
we	O
will	O
denote	O
the	O
potential	B
function	I
for	O
clique	B
c	O
by	O
cyc	O
c	O
a	O
potential	B
function	I
can	O
be	O
any	O
non-negative	O
function	O
of	O
its	O
arguments	O
the	O
joint	B
distribution	I
is	O
then	O
defined	O
to	O
be	O
proportional	O
to	O
the	O
product	O
of	O
clique	B
potentials	O
rather	O
surprisingly	O
one	O
can	O
show	O
that	O
any	O
positive	O
distribution	O
whose	O
ci	B
properties	O
can	O
be	O
represented	O
by	O
a	O
ugm	B
can	O
be	O
represented	O
in	O
this	O
way	O
we	O
state	B
this	O
result	O
more	O
formally	O
below	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
theorem	O
a	O
positive	O
distribution	O
py	O
satisfies	O
the	O
ci	B
properties	O
of	O
an	O
undirected	B
graph	B
g	O
iff	B
p	O
can	O
be	O
represented	O
as	O
a	O
product	O
of	O
factors	B
one	O
per	O
maximal	B
clique	B
i	O
e	O
cyc	O
c	O
where	O
c	O
is	O
the	O
set	O
of	O
all	O
the	O
cliques	B
of	O
g	O
and	O
z	O
is	O
the	O
partition	B
function	I
given	O
by	O
cyc	O
c	O
c	O
c	O
py	O
z	O
z	O
c	O
c	O
x	O
note	O
that	O
the	O
partition	B
function	I
is	O
what	O
ensures	O
the	O
overall	O
distribution	O
sums	O
to	O
the	O
proof	O
was	O
never	O
published	O
but	O
can	O
be	O
found	O
in	O
e	O
g	O
and	O
friedman	O
for	O
example	O
consider	O
the	O
mrf	B
in	O
figure	O
if	O
p	O
satisfies	O
the	O
ci	B
properties	O
of	O
this	O
graph	B
then	O
we	O
can	O
write	O
p	O
as	O
follows	O
py	O
z	O
y	O
where	O
z	O
in	O
particular	O
there	O
is	O
a	O
there	O
is	O
a	O
deep	B
connection	O
between	O
ugms	O
and	O
statistical	O
physics	O
model	O
known	O
as	O
the	O
gibbs	B
distribution	I
which	O
can	O
be	O
written	O
as	O
follows	O
py	O
exp	O
z	O
eyc	O
c	O
c	O
where	O
eyc	O
is	O
the	O
energy	O
associated	O
with	O
the	O
variables	O
in	O
clique	B
c	O
we	O
can	O
convert	O
this	O
to	O
a	O
ugm	B
by	O
defining	O
cyc	O
c	O
exp	O
eyc	O
c	O
we	O
see	O
that	O
high	O
probability	O
states	O
correspond	O
to	O
low	O
energy	O
configurations	O
models	O
of	O
this	O
form	O
are	O
known	O
as	O
energy	B
based	I
models	I
and	O
are	O
commonly	O
used	O
in	O
physics	O
and	O
biochemistry	O
as	O
well	O
as	O
some	O
branches	O
of	O
machine	B
learning	B
et	O
al	O
note	O
that	O
we	O
are	O
free	O
to	O
restrict	O
the	O
parameterization	O
to	O
the	O
edges	B
of	O
the	O
graph	B
rather	O
than	O
the	O
maximal	O
cliques	B
this	O
is	O
called	O
a	O
pairwise	B
mrf	B
in	O
figure	O
we	O
get	O
py	O
s	O
t	O
stys	O
yt	O
this	O
form	O
is	O
widely	O
used	O
due	O
to	O
its	O
simplicity	O
although	O
it	O
is	O
not	O
as	O
general	O
the	O
partition	B
function	I
is	O
denoted	O
by	O
z	O
because	O
of	O
the	O
german	O
word	O
zustandssumme	O
which	O
means	O
sum	O
over	O
states	O
this	O
reflects	O
the	O
fact	O
that	O
a	O
lot	O
of	O
pioneering	O
working	O
in	O
statistical	O
physics	O
was	O
done	O
by	O
germans	O
parameterization	O
of	O
mrfs	O
representing	O
potential	O
functions	O
if	O
the	O
variables	O
are	O
discrete	B
we	O
can	O
represent	O
the	O
potential	O
or	O
energy	O
functions	O
as	O
tables	O
of	O
numbers	O
just	O
as	O
we	O
did	O
with	O
cpts	B
however	O
the	O
potentials	O
are	O
not	O
probabilities	O
rather	O
they	O
represent	O
the	O
relative	O
compatibility	O
between	O
the	O
different	O
assignments	O
to	O
the	O
potential	O
we	O
will	O
see	O
some	O
examples	O
of	O
this	O
below	O
a	O
more	O
general	O
approach	O
is	O
to	O
define	O
the	O
log	O
potentials	O
as	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
log	O
cyc	O
cyct	O
c	O
where	O
cxc	O
is	O
a	O
feature	O
vector	O
derived	O
from	O
the	O
values	O
of	O
the	O
variables	O
yc	O
the	O
resulting	O
log	O
probability	O
has	O
the	O
form	O
log	O
py	O
cyct	O
c	O
z	O
c	O
this	O
is	O
also	O
known	O
as	O
a	O
maximum	B
entropy	B
or	O
a	O
log-linear	B
model	O
for	O
example	O
consider	O
a	O
pairwise	B
mrf	B
where	O
for	O
each	O
edge	O
we	O
associate	O
a	O
feature	O
vector	O
of	O
length	O
k	O
as	O
follows	O
stys	O
yt	O
j	O
yt	O
k	O
if	O
we	O
have	O
a	O
weight	O
for	O
each	O
feature	O
we	O
can	O
convert	O
this	O
into	O
a	O
k	O
k	O
potential	B
function	I
as	O
follows	O
stys	O
j	O
yt	O
k	O
exp	O
t	O
st	O
stjk	O
exp	O
stj	O
k	O
so	O
we	O
see	O
that	O
we	O
can	O
easily	O
represent	O
tabular	O
potentials	O
using	O
a	O
log-linear	B
form	O
but	O
the	O
log-linear	B
form	O
is	O
more	O
general	O
to	O
see	O
why	O
this	O
is	O
useful	O
suppose	O
we	O
are	O
interested	O
in	O
making	O
a	O
probabilistic	O
model	O
of	O
english	O
spelling	O
since	O
certain	O
letter	O
combinations	O
occur	O
together	O
quite	O
frequently	O
ing	O
we	O
will	O
need	O
higher	O
order	O
factors	B
to	O
capture	O
this	O
suppose	O
we	O
limit	O
ourselves	O
to	O
letter	O
trigrams	O
a	O
tabular	O
potential	O
still	O
has	O
parameters	O
in	O
it	O
however	O
most	O
of	O
these	O
triples	O
will	O
never	O
occur	O
an	O
alternative	O
approach	O
is	O
to	O
define	O
indicator	O
functions	O
that	O
look	O
for	O
certain	O
special	O
triples	O
such	O
as	O
ing	O
qu-	O
etc	O
then	O
we	O
can	O
define	O
the	O
potential	O
on	O
each	O
trigram	O
as	O
follows	O
yt	O
exp	O
k	O
kyt	O
yt	O
k	O
py	O
exp	O
where	O
k	O
indexes	O
the	O
different	O
features	B
corresponding	O
to	O
ing	O
qu-	O
etc	O
and	O
k	O
is	O
the	O
corresponding	O
binary	O
feature	B
function	I
by	O
tying	O
the	O
parameters	O
across	O
locations	O
we	O
can	O
define	O
the	O
probability	O
of	O
a	O
word	O
of	O
any	O
length	O
using	O
k	O
kyt	O
yt	O
t	O
k	O
this	O
raises	O
the	O
question	O
of	O
where	O
these	O
feature	O
functions	O
come	O
from	O
in	O
many	O
applications	O
they	O
are	O
created	O
by	O
hand	O
to	O
reflect	O
domain	O
knowledge	O
will	O
see	O
examples	O
later	O
but	O
it	O
is	O
also	O
possible	O
to	O
learn	O
them	O
from	O
data	O
as	O
we	O
discuss	O
in	O
section	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
examples	O
of	O
mrfs	O
in	O
this	O
section	O
we	O
show	O
how	O
several	O
popular	O
probability	O
models	O
can	O
be	O
conveniently	O
expressed	O
as	O
ugms	O
ising	B
model	I
the	O
ising	B
model	I
is	O
an	O
example	O
of	O
an	O
mrf	B
that	O
arose	O
from	O
statistical	O
it	O
was	O
originally	O
used	O
for	O
modeling	O
the	O
behavior	O
of	O
magnets	O
in	O
particular	O
let	O
ys	O
represent	O
the	O
spin	B
in	O
some	O
magnets	O
called	O
ferro-magnets	B
of	O
an	O
atom	B
which	O
can	O
either	O
be	O
spin	B
down	O
or	O
up	O
neighboring	O
spins	O
tend	O
to	O
line	O
up	O
in	O
the	O
same	O
direction	O
whereas	O
in	O
other	O
kinds	O
of	O
magnets	O
called	O
anti-ferromagnets	B
the	O
spins	O
want	O
to	O
be	O
different	O
from	O
their	O
neighbors	B
we	O
can	O
model	O
this	O
as	O
an	O
mrf	B
as	O
follows	O
we	O
create	O
a	O
graph	B
in	O
the	O
form	O
of	O
a	O
or	O
lattice	B
and	O
connect	O
neighboring	O
variables	O
as	O
in	O
figure	O
we	O
then	O
define	O
the	O
following	O
pairwise	O
clique	B
potential	O
stys	O
yt	O
ewst	O
e	O
wst	O
e	O
wst	O
ewst	O
here	O
wst	O
is	O
the	O
coupling	O
strength	O
between	O
nodes	B
s	O
and	O
t	O
if	O
two	O
nodes	B
are	O
not	O
connected	O
in	O
the	O
graph	B
we	O
set	O
wst	O
we	O
assume	O
that	O
the	O
weight	O
matrix	O
w	O
is	O
symmetric	B
so	O
wst	O
wts	O
often	O
we	O
assume	O
all	O
edges	B
have	O
the	O
same	O
strength	O
so	O
wst	O
j	O
wst	O
if	O
all	O
the	O
weights	O
are	O
positive	O
j	O
then	O
neighboring	O
spins	O
are	O
likely	O
to	O
be	O
in	O
the	O
same	O
state	B
this	O
can	O
be	O
used	O
to	O
model	O
ferromagnets	O
and	O
is	O
an	O
example	O
of	O
an	O
associative	B
markov	B
network	I
if	O
the	O
weights	O
are	O
sufficiently	O
strong	O
the	O
corresponding	O
probability	O
distribution	O
will	O
have	O
two	O
modes	O
corresponding	O
to	O
the	O
all	O
s	O
state	B
and	O
the	O
all	O
s	O
state	B
these	O
are	O
called	O
the	O
ground	B
states	I
of	O
the	O
system	O
if	O
all	O
of	O
the	O
weights	O
are	O
negative	O
j	O
then	O
the	O
spins	O
want	O
to	O
be	O
different	O
from	O
their	O
neighbors	B
this	O
can	O
be	O
used	O
to	O
model	O
an	O
anti-ferromagnet	O
and	O
results	O
in	O
a	O
frustrated	B
system	I
in	O
which	O
not	O
all	O
the	O
constraints	O
can	O
be	O
satisfied	O
at	O
the	O
same	O
time	O
the	O
corresponding	O
probability	O
distribution	O
will	O
have	O
multiple	O
modes	O
interestingly	O
computing	O
the	O
partition	B
function	I
zj	O
can	O
be	O
done	O
in	O
polynomial	O
time	O
for	O
associative	B
markov	B
networks	O
but	O
is	O
np-hard	B
in	O
general	O
there	O
is	O
an	O
interesting	O
analogy	O
between	O
ising	O
models	O
and	O
gaussian	B
graphical	B
models	I
first	O
assuming	O
yt	O
we	O
can	O
write	O
the	O
unnormalized	O
log	O
probability	O
of	O
an	O
ising	B
model	I
as	O
follows	O
log	O
py	O
yswstyt	O
yt	O
wy	O
s	O
t	O
arises	O
because	O
we	O
sum	O
each	O
edge	O
twice	O
if	O
wst	O
j	O
we	O
get	O
a	O
low	O
energy	O
factor	B
of	O
hence	O
high	O
probability	O
if	O
neighboring	O
states	O
agree	O
sometimes	O
there	O
is	O
an	O
external	O
field	O
which	O
is	O
an	O
energy	O
term	O
which	O
is	O
added	O
to	O
each	O
spin	B
this	O
can	O
be	O
modelled	O
using	O
a	O
local	O
energy	O
term	O
of	O
the	O
form	O
bt	O
y	O
where	O
b	O
is	O
sometimes	O
called	O
ernst	O
ising	O
was	O
a	O
german-american	O
physicist	O
examples	O
of	O
mrfs	O
a	O
bias	B
term	I
the	O
modified	O
distribution	O
is	O
given	O
by	O
log	O
py	O
wstysyt	O
bsys	O
yt	O
wy	O
bt	O
y	O
s	O
t	O
s	O
where	O
b	O
if	O
we	O
define	O
that	O
looks	O
similar	B
to	O
a	O
gaussian	B
w	O
and	O
c	O
t	O
we	O
can	O
rewrite	O
this	O
in	O
a	O
form	O
py	O
exp	O
one	O
very	O
important	O
difference	O
is	O
that	O
in	O
the	O
case	O
of	O
gaussians	O
the	O
normalization	O
constant	O
z	O
requires	O
the	O
computation	O
of	O
a	O
matrix	O
determinant	O
which	O
can	O
be	O
computed	O
in	O
time	O
whereas	O
in	O
the	O
case	O
of	O
the	O
ising	B
model	I
the	O
normalization	O
constant	O
requires	O
summing	O
over	O
all	O
bit	O
vectors	O
this	O
is	O
equivalent	O
to	O
computing	O
the	O
matrix	B
permanent	B
which	O
is	O
np-hard	B
in	O
general	O
et	O
al	O
hopfield	O
networks	O
a	O
hopfield	O
network	O
is	O
a	O
fully	O
connected	O
ising	B
model	I
with	O
a	O
symmetric	B
weight	O
matrix	O
w	O
wt	O
these	O
weights	O
plus	O
the	O
bias	B
terms	O
b	O
can	O
be	O
learned	O
from	O
training	O
data	O
using	O
maximum	O
likelihood	B
as	O
described	O
in	O
section	O
the	O
main	O
application	O
of	O
hopfield	O
networks	O
is	O
as	O
an	O
associative	B
memory	I
or	O
content	B
addressable	I
memory	I
the	O
idea	O
is	O
this	O
suppose	O
we	O
train	O
on	O
a	O
set	O
of	O
fully	O
observed	O
bit	O
vectors	O
corresponding	O
to	O
patterns	O
we	O
want	O
to	O
memorize	O
then	O
at	O
test	O
time	O
we	O
present	O
a	O
partial	O
pattern	B
to	O
the	O
network	O
we	O
would	O
like	O
to	O
estimate	O
the	O
missing	B
variables	O
this	O
is	O
called	O
pattern	B
completion	I
see	O
figure	O
for	O
an	O
example	O
this	O
can	O
be	O
thought	O
of	O
as	O
retrieving	O
an	O
example	O
from	O
memory	O
based	O
on	O
a	O
piece	O
of	O
the	O
example	O
itself	O
hence	O
the	O
term	O
associative	B
memory	I
since	O
exact	O
inference	B
is	O
intractable	O
in	O
this	O
model	O
it	O
is	O
standard	O
to	O
use	O
a	O
coordinate	O
descent	O
algorithm	O
known	O
as	O
iterative	B
conditional	I
modes	I
which	O
just	O
sets	O
each	O
node	O
to	O
its	O
most	O
likely	O
energy	O
state	B
given	O
all	O
its	O
neighbors	B
the	O
full	B
conditional	I
can	O
be	O
shown	O
to	O
be	O
pys	O
s	O
sigmwt	O
sy	O
s	O
bs	O
picking	O
the	O
most	O
probable	O
state	B
amounts	O
to	O
using	O
the	O
rule	O
y	O
y	O
s	O
otherwise	O
better	O
inference	B
algorithms	O
will	O
be	O
discussed	O
later	O
in	O
this	O
book	O
since	O
inference	B
is	O
deterministic	O
it	O
is	O
also	O
possible	O
to	O
interpret	O
this	O
model	O
as	O
a	O
recurrent	B
neural	B
network	I
is	O
quite	O
different	O
from	O
the	O
feedforward	O
neural	O
nets	O
studied	O
in	O
section	O
they	O
are	O
univariate	O
conditional	O
density	O
models	O
of	O
the	O
form	O
pyx	O
which	O
can	O
only	O
be	O
used	O
for	O
supervised	B
learning	B
see	O
hertz	O
et	O
al	O
for	O
further	O
details	O
on	O
hopfield	O
networks	O
t	O
wstyt	O
bs	O
and	O
using	O
s	O
if	O
a	O
boltzmann	B
machine	I
generalizes	O
the	O
hopfield	O
ising	B
model	I
by	O
including	O
some	O
hidden	B
inference	B
in	O
such	O
models	O
nodes	B
which	O
makes	O
the	O
model	O
representationally	O
more	O
powerful	O
often	O
uses	O
gibbs	B
sampling	I
which	O
is	O
a	O
stochastic	O
version	O
of	O
icm	O
section	O
for	O
details	O
ml	O
estimation	O
works	O
much	O
better	O
than	O
the	O
outer	O
product	B
rule	I
proposed	O
in	O
in	O
because	O
it	O
not	O
only	O
lowers	O
the	O
energy	O
of	O
the	O
observed	O
patterns	O
but	O
it	O
also	O
raises	O
the	O
energy	O
of	O
the	O
non-observed	O
patterns	O
in	O
order	O
to	O
make	O
the	O
distribution	O
sum	O
to	O
one	O
et	O
al	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
figure	O
examples	O
of	O
how	O
an	O
associative	B
memory	I
can	O
reconstruct	O
images	O
these	O
are	O
binary	O
images	O
of	O
size	O
pixels	O
top	O
training	O
images	O
row	O
partially	O
visible	B
test	O
images	O
row	O
estimate	O
after	O
iterations	O
bottom	O
final	O
state	B
estimate	O
based	O
on	O
figure	O
of	O
hertz	O
et	O
al	O
figure	O
generated	O
by	O
hopfielddemo	O
figure	O
visualizing	B
a	O
sample	O
from	O
a	O
potts	B
model	I
of	O
size	O
for	O
different	O
association	O
j	O
j	O
j	O
the	O
regions	O
are	O
labeled	O
according	O
to	O
size	O
blue	O
is	O
strengths	O
largest	O
red	O
is	O
smallest	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
see	O
gibbsdemoising	O
for	O
matlab	O
code	O
to	O
produce	O
a	O
similar	B
plot	O
for	O
the	O
ising	B
model	I
however	O
we	O
could	O
equally	O
well	O
apply	O
gibbs	O
to	O
a	O
hopfield	O
net	O
and	O
icm	O
to	O
a	O
boltzmann	B
machine	I
the	O
inference	B
algorithm	O
is	O
not	O
part	O
of	O
the	O
model	O
definition	O
see	O
section	O
for	O
further	O
details	O
on	O
boltzmann	O
machines	O
examples	O
of	O
mrfs	O
ys	O
xs	O
yt	O
xt	O
figure	O
a	O
grid-structured	O
mrf	B
with	O
local	B
evidence	B
nodes	B
potts	B
model	I
it	O
is	O
easy	O
to	O
generalize	B
the	O
ising	B
model	I
to	O
multiple	O
discrete	B
states	O
yt	O
k	O
common	O
to	O
use	O
a	O
potential	B
function	I
of	O
the	O
following	O
form	O
it	O
is	O
ej	O
ej	O
ej	O
stys	O
yt	O
this	O
is	O
called	O
the	O
potts	O
if	O
j	O
then	O
neighboring	O
nodes	B
are	O
encouraged	O
to	O
have	O
the	O
same	O
label	B
some	O
samples	B
from	O
this	O
model	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
for	O
j	O
large	O
clusters	B
occur	O
for	O
j	O
many	O
small	O
clusters	B
occur	O
and	O
at	O
the	O
critical	B
value	I
of	O
k	O
there	O
is	O
a	O
mix	O
of	O
small	O
and	O
large	O
clusters	B
this	O
rapid	O
change	O
in	O
behavior	O
as	O
we	O
vary	O
a	O
parameter	B
of	O
the	O
system	O
is	O
called	O
a	O
phase	B
transition	I
and	O
has	O
been	O
widely	O
studied	O
in	O
the	O
physics	O
community	O
an	O
analogous	O
phenomenon	O
occurs	O
in	O
the	O
ising	B
model	I
see	O
ch	O
for	O
details	O
the	O
potts	B
model	I
can	O
be	O
used	O
as	O
a	O
prior	O
for	O
image	B
segmentation	I
since	O
it	O
says	O
that	O
neighboring	O
pixels	O
are	O
likely	O
to	O
have	O
the	O
same	O
discrete	B
label	B
and	O
hence	O
belong	O
to	O
the	O
same	O
segment	O
we	O
can	O
combine	O
this	O
prior	O
with	O
a	O
likelihood	B
term	O
as	O
follows	O
py	O
x	O
pyj	O
pxtyt	O
t	O
where	O
pxtyt	O
k	O
is	O
the	O
probability	O
of	O
observing	O
pixel	O
xt	O
given	O
that	O
the	O
corresponding	O
segment	O
belongs	O
to	O
class	O
k	O
this	O
observation	B
model	I
can	O
be	O
modeled	O
using	O
a	O
gaussian	B
or	O
a	O
non-parametric	O
density	O
that	O
we	O
label	B
the	O
hidden	B
nodes	B
yt	O
and	O
the	O
observed	O
nodes	B
xt	O
to	O
be	O
compatible	O
with	O
section	O
zj	O
yt	O
j	O
t	O
pxtyt	O
the	O
corresponding	O
graphical	B
model	I
is	O
a	O
mix	O
of	O
undirected	B
and	O
directed	B
edges	B
as	O
shown	O
in	O
figure	O
the	O
undirected	B
lattice	B
represents	O
the	O
prior	O
py	O
in	O
addition	O
there	O
are	O
directed	B
edge	O
from	O
each	O
yt	O
to	O
its	O
corresponding	O
xt	O
representing	O
the	O
local	B
evidence	B
technically	O
speaking	O
this	O
combination	O
of	O
an	O
undirected	B
and	O
directed	B
graph	B
is	O
called	O
a	O
chain	B
graph	B
however	O
renfrey	O
potts	O
was	O
an	O
australian	O
mathematician	O
s	O
t	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
since	O
the	O
xt	O
nodes	B
are	O
observed	O
they	O
can	O
be	O
absorbed	O
into	O
the	O
model	O
thus	O
leaving	O
behind	O
an	O
undirected	B
backbone	O
this	O
model	O
is	O
a	O
analog	O
of	O
an	O
hmm	B
and	O
could	O
be	O
called	O
a	O
partially	B
observed	I
mrf	B
as	O
in	O
an	O
hmm	B
the	O
goal	O
is	O
to	O
perform	O
posterior	O
inference	B
i	O
e	O
to	O
compute	O
function	O
of	O
pyx	O
unfortunately	O
the	O
case	O
is	O
provably	O
much	O
harder	O
than	O
the	O
case	O
and	O
we	O
must	O
resort	O
to	O
approximate	O
methods	O
as	O
we	O
discuss	O
in	O
later	O
chapters	O
although	O
the	O
potts	O
prior	O
is	O
adequate	O
for	O
regularizing	O
supervised	B
learning	B
problems	O
it	O
is	O
not	O
sufficiently	O
accurate	O
to	O
perform	O
image	B
segmentation	I
in	O
an	O
unsupervised	O
way	O
since	O
the	O
segments	O
produced	O
by	O
this	O
model	O
do	O
not	O
accurately	O
represent	O
the	O
kinds	O
of	O
segments	O
one	O
sees	O
in	O
natural	O
images	O
et	O
al	O
for	O
the	O
unsupervised	O
case	O
one	O
needs	O
to	O
use	O
more	O
sophisticated	O
priors	O
such	O
as	O
the	O
truncated	B
gaussian	B
process	I
prior	O
of	O
and	O
jordan	O
gaussian	B
mrfs	O
an	O
undirected	B
ggm	O
also	O
called	O
a	O
gaussian	B
mrf	B
e	O
g	O
and	O
held	O
is	O
a	O
pairwise	B
mrf	B
of	O
the	O
following	O
form	O
s	O
t	O
py	O
stys	O
yt	O
tyt	O
stys	O
yt	O
exp	O
tyt	O
exp	O
t	O
ys	O
styt	O
t	O
tyt	O
that	O
we	O
could	O
easily	O
absorb	O
the	O
node	O
potentials	O
t	O
into	O
the	O
edge	O
potentials	O
but	O
we	O
have	O
kept	O
them	O
separate	O
for	O
clarity	O
the	O
joint	B
distribution	I
can	O
be	O
written	O
as	O
follows	O
py	O
exp	O
t	O
y	O
yt	O
y	O
we	O
recognize	O
this	O
as	O
a	O
multivariate	B
gaussian	B
written	O
in	O
information	B
form	I
where	O
and	O
if	O
st	O
then	O
there	O
is	O
no	O
pairwise	O
term	O
connecting	O
s	O
and	O
t	O
so	O
by	O
the	O
factorization	O
theorem	O
we	O
conclude	O
that	O
ys	O
yty	O
st	O
the	O
zero	O
entries	O
in	O
are	O
called	O
structural	B
zeros	I
since	O
they	O
represent	O
the	O
absent	O
edges	B
in	O
the	O
graph	B
thus	O
undirected	B
ggms	O
correspond	O
to	O
sparse	B
precision	B
matrices	O
a	O
fact	O
which	O
we	O
exploit	O
in	O
section	O
to	O
efficiently	O
learn	O
the	O
structure	O
of	O
the	O
graph	B
comparing	O
gaussian	B
dgms	O
and	O
ugms	O
in	O
section	O
we	O
saw	O
that	O
directed	B
ggms	O
correspond	O
to	O
sparse	B
regression	B
matrices	O
and	O
hence	O
sparse	B
cholesky	O
factorizations	O
of	O
covariance	B
matrices	O
whereas	O
undirected	B
ggms	O
correspond	O
to	O
an	O
influential	O
paper	O
and	O
geman	O
which	O
introduced	O
the	O
idea	O
of	O
a	O
gibbs	B
sampler	I
proposed	O
using	O
the	O
potts	B
model	I
as	O
a	O
prior	O
for	O
image	B
segmentation	I
but	O
the	O
results	O
in	O
their	O
paper	O
are	O
misleading	O
because	O
they	O
did	O
not	O
run	O
their	O
gibbs	B
sampler	I
for	O
long	O
enough	O
see	O
figure	O
for	O
a	O
vivid	O
illustration	O
of	O
this	O
point	O
examples	O
of	O
mrfs	O
figure	O
a	O
process	O
represented	O
as	O
a	O
dynamic	O
chain	B
graph	B
source	O
used	O
with	O
kind	O
permission	O
of	O
rainer	O
dahlhaus	O
and	O
oxford	O
university	O
press	O
and	O
eichler	O
sparse	B
precision	B
matrices	O
the	O
advantage	O
of	O
the	O
dag	B
formulation	O
is	O
that	O
we	O
can	O
make	O
the	O
regression	B
weights	O
w	O
and	O
hence	O
be	O
conditional	O
on	O
covariate	O
information	B
without	O
worrying	O
about	O
positive	O
definite	O
constraints	O
the	O
disadavantage	O
of	O
the	O
dag	B
formulation	O
is	O
its	O
dependence	O
on	O
the	O
order	O
although	O
in	O
certain	O
domains	O
such	O
as	O
time	O
series	O
there	O
is	O
already	O
a	O
natural	O
ordering	O
of	O
the	O
variables	O
it	O
is	O
actually	O
possible	O
to	O
combine	O
both	O
representations	O
resulting	O
in	O
a	O
gaussian	B
chain	B
graph	B
for	O
example	O
consider	O
a	O
a	O
discrete-time	O
second-order	O
markov	B
chain	I
in	O
which	O
the	O
states	O
are	O
continuous	O
yt	O
r	O
d	O
the	O
transition	O
function	O
can	O
be	O
represented	O
as	O
a	O
lineargaussian	O
cpd	B
pytyt	O
yt	O
n	O
this	O
is	O
called	O
vector	B
auto-regressive	I
or	O
var	B
process	O
of	O
order	O
such	O
models	O
are	O
widely	O
used	O
in	O
econometrics	O
for	O
time-series	B
forecasting	I
the	O
time	O
series	O
aspect	O
is	O
most	O
naturally	O
modeled	O
using	O
a	O
dgm	B
however	O
if	O
is	O
sparse	B
then	O
the	O
correlation	O
amongst	O
the	O
components	O
within	O
a	O
time	O
slice	O
is	O
most	O
naturally	O
modeled	O
using	O
a	O
ugm	B
for	O
example	O
suppose	O
we	O
have	O
and	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
figure	O
based	O
on	O
figures	O
of	O
used	O
with	O
kind	O
permission	O
of	O
myung	O
choi	O
a	O
bi-directed	B
graph	B
the	O
equivalent	O
dag	B
here	O
the	O
w	O
nodes	B
are	O
latent	B
confounders	B
the	O
resulting	O
graphical	B
model	I
is	O
illustrated	O
in	O
figure	O
zeros	O
in	O
the	O
transition	O
matrices	O
and	O
correspond	O
to	O
absent	O
directed	B
arcs	O
from	O
yt	O
and	O
yt	O
into	O
yt	O
zeros	O
in	O
the	O
precision	B
matrix	I
correspond	O
to	O
absent	O
undirected	B
arcs	O
between	O
nodes	B
in	O
yt	O
sometimes	O
we	O
have	O
a	O
sparse	B
covariance	B
matrix	I
rather	O
than	O
a	O
sparse	B
precision	B
matrix	I
this	O
can	O
be	O
represented	O
using	O
a	O
bi-directed	B
graph	B
where	O
each	O
edge	O
has	O
arrows	O
in	O
both	O
directions	O
as	O
in	O
figure	O
here	O
nodes	B
that	O
are	O
not	O
connected	O
are	O
unconditionally	B
independent	I
for	O
example	O
in	O
figure	O
we	O
see	O
that	O
in	O
the	O
gaussian	B
case	O
this	O
means	O
graph	B
representing	O
a	O
sparse	B
covariance	B
matrix	I
is	O
called	O
a	O
covariance	B
graph	B
by	O
contrast	O
if	O
this	O
were	O
an	O
undirected	B
model	O
we	O
would	O
have	O
that	O
and	O
where	O
a	O
bidirected	O
graph	B
can	O
be	O
converted	O
to	O
a	O
dag	B
with	O
latent	B
variables	O
where	O
each	O
bidirected	O
edge	O
is	O
replaced	O
with	O
a	O
hidden	B
variable	I
representing	O
a	O
hidden	B
common	O
cause	O
or	O
confounder	B
as	O
illustrated	O
in	O
figure	O
the	O
relevant	O
ci	B
properties	O
can	O
then	O
be	O
determined	O
using	O
dseparation	O
we	O
can	O
combine	O
bidirected	O
and	O
directed	B
edges	B
to	O
get	O
a	O
directed	B
mixed	I
graphical	B
model	I
this	O
is	O
useful	O
for	O
representing	O
a	O
variety	O
of	O
models	O
such	O
as	O
arma	B
models	O
structural	B
equation	I
models	I
etc	O
markov	B
logic	O
networks	O
in	O
section	O
we	O
saw	O
how	O
we	O
could	O
unroll	O
markov	B
models	I
and	O
hmms	B
for	O
an	O
arbitrary	O
number	O
of	O
time	O
steps	O
in	O
order	O
to	O
model	O
variable-length	O
sequences	O
similarly	O
in	O
section	O
we	O
saw	O
how	O
we	O
could	O
expand	O
a	O
lattice	B
ugm	B
to	O
model	O
images	O
of	O
any	O
size	O
what	O
about	O
more	O
complex	O
domains	O
where	O
we	O
have	O
a	O
variable	O
number	O
of	O
objects	O
and	O
relationships	O
between	O
them	O
creating	O
models	O
for	O
such	O
scenarios	O
is	O
often	O
done	O
using	O
first-order	O
logic	O
e	O
g	O
and	O
norvig	O
for	O
example	O
consider	O
the	O
sentences	O
smoking	O
causes	O
cancer	O
and	O
if	O
two	O
people	O
are	O
friends	O
and	O
one	O
smokes	O
then	O
so	O
does	O
the	O
other	O
we	O
can	O
write	O
these	O
sentences	O
in	O
first-order	O
examples	O
of	O
mrfs	O
friendsab	O
friendsaa	O
smokesa	O
smokesb	O
friendsbb	O
cancera	O
cancerb	O
friendsba	O
figure	O
an	O
example	O
of	O
a	O
ground	O
markov	B
logic	I
network	I
represented	O
as	O
a	O
pairwise	B
mrf	B
for	O
people	O
based	O
on	O
figure	O
from	O
and	O
lowd	O
used	O
with	O
kind	O
permission	O
of	O
pedro	O
domingos	O
logic	O
as	O
follows	O
x	O
smx	O
cax	O
x	O
y	O
f	O
rx	O
y	O
smx	O
smy	O
where	O
sm	O
and	O
ca	O
are	O
predicates	O
and	O
f	O
r	O
is	O
a	O
of	O
course	O
such	O
rules	B
are	O
not	O
always	O
true	O
indeed	O
this	O
brittleness	O
is	O
the	O
main	O
reason	O
why	O
logical	O
approaches	O
to	O
ai	B
are	O
no	O
longer	O
widely	O
used	O
at	O
least	O
not	O
in	O
their	O
pure	B
form	O
there	O
have	O
been	O
a	O
variety	O
of	O
attempts	O
to	O
combine	O
first	O
order	O
logic	O
with	O
probability	B
theory	I
an	O
area	O
known	O
as	O
statistical	B
relational	I
ai	B
or	O
probabilistic	B
relational	I
modeling	I
et	O
al	O
one	O
simple	O
approach	O
is	O
to	O
take	O
logical	O
rules	B
and	O
attach	O
weights	O
as	O
certainty	B
factors	B
to	O
them	O
and	O
then	O
to	O
interpret	O
them	O
as	O
conditional	B
probability	I
distributions	O
for	O
example	O
we	O
might	O
say	O
pcax	O
unfortunately	O
the	O
rule	O
does	O
not	O
say	O
what	O
to	O
predict	O
if	O
smx	O
furthermore	O
combining	O
cpds	O
in	O
this	O
way	O
is	O
not	O
guaranteed	O
to	O
define	O
a	O
consistent	B
joint	B
distribution	I
because	O
the	O
resulting	O
graph	B
may	O
not	O
be	O
a	O
dag	B
an	O
alternative	O
approach	O
is	O
to	O
treat	O
these	O
rules	B
as	O
a	O
way	O
of	O
defining	O
potential	O
functions	O
in	O
an	O
unrolled	B
ugm	B
the	O
result	O
is	O
known	O
as	O
a	O
markov	B
logic	I
network	I
and	O
lowd	O
to	O
specify	O
the	O
network	O
we	O
first	O
rewrite	O
all	O
the	O
rules	B
in	O
conjunctive	B
normal	B
form	I
also	O
known	O
as	O
clausal	B
form	I
in	O
this	O
case	O
we	O
get	O
smx	O
cax	O
f	O
rx	O
y	O
smx	O
smy	O
the	O
first	O
clause	B
can	O
be	O
read	O
as	O
either	O
x	O
does	O
not	O
smoke	O
or	O
he	O
has	O
cancer	O
which	O
is	O
logically	O
equivalent	O
to	O
equation	O
that	O
in	O
a	O
clause	B
any	O
unbound	O
variable	O
such	O
as	O
x	O
is	O
assumed	O
to	O
be	O
universally	O
quantified	O
a	O
predicate	O
is	O
just	O
a	O
function	O
of	O
one	O
argument	O
known	O
as	O
an	O
object	O
that	O
evaluates	O
to	O
true	O
or	O
false	O
depending	O
on	O
whether	O
the	O
property	O
holds	O
or	O
not	O
of	O
that	O
object	O
a	O
relation	B
is	O
just	O
a	O
function	O
of	O
two	O
or	O
more	O
arguments	O
that	O
evaluates	O
to	O
true	O
or	O
false	O
depending	O
on	O
whether	O
the	O
relationship	O
holds	O
between	O
that	O
set	O
of	O
objects	O
or	O
not	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
inference	B
in	O
first-order	O
logic	O
is	O
only	O
semi-decidable	O
so	O
it	O
is	O
common	O
to	O
use	O
a	O
restricted	O
subset	O
a	O
common	O
approach	O
used	O
in	O
prolog	B
is	O
to	O
restrict	O
the	O
language	O
to	O
horn	B
clauses	I
which	O
are	O
clauses	O
that	O
contain	O
at	O
most	O
one	O
positive	O
literal	O
essentially	O
this	O
means	O
the	O
model	O
is	O
a	O
series	O
of	O
if-then	O
rules	B
where	O
the	O
right	O
hand	O
side	O
of	O
the	O
rules	B
then	O
part	O
or	O
consequence	O
has	O
only	O
a	O
single	O
term	O
once	O
we	O
have	O
encoded	O
our	O
knowledge	B
base	I
as	O
a	O
set	O
of	O
clauses	O
we	O
can	O
attach	O
weights	O
to	O
each	O
one	O
these	O
weights	O
are	O
the	O
parameter	B
of	O
the	O
model	O
and	O
they	O
define	O
the	O
clique	B
potentials	O
as	O
follows	O
cxc	O
expwc	O
cxc	O
where	O
cxc	O
is	O
a	O
logical	O
expression	O
which	O
evaluates	O
clause	B
c	O
applied	O
to	O
the	O
variables	O
xc	O
and	O
wc	O
is	O
the	O
weight	O
we	O
attach	O
to	O
this	O
clause	B
roughly	O
speaking	O
the	O
weight	O
of	O
a	O
clause	B
specifies	O
the	O
probability	O
of	O
a	O
world	O
in	O
which	O
this	O
clause	B
is	O
satsified	O
relative	O
to	O
a	O
world	O
in	O
which	O
it	O
is	O
not	O
satisfied	O
now	O
suppose	O
there	O
are	O
two	O
objects	O
in	O
the	O
world	O
anna	O
and	O
bob	O
which	O
we	O
will	O
denote	O
by	O
constant	B
symbols	I
a	O
and	O
b	O
we	O
can	O
make	O
a	O
ground	B
network	I
from	O
the	O
above	O
clauses	O
by	O
creating	O
binary	O
random	O
variables	O
sx	O
cx	O
and	O
fxy	O
for	O
x	O
y	O
b	O
and	O
then	O
wiring	O
these	O
up	O
according	O
to	O
the	O
clauses	O
above	O
the	O
result	O
is	O
the	O
ugm	B
in	O
figure	O
with	O
binary	O
nodes	B
note	O
that	O
we	O
have	O
not	O
encoded	O
the	O
fact	O
that	O
f	O
r	O
is	O
a	O
symmetric	B
relation	B
so	O
f	O
ra	O
b	O
and	O
f	O
rb	O
a	O
might	O
have	O
different	O
values	O
similarly	O
we	O
have	O
the	O
degenerate	B
nodes	B
f	O
ra	O
a	O
and	O
f	O
rb	O
b	O
since	O
we	O
did	O
not	O
enforce	O
x	O
y	O
in	O
equation	O
we	O
add	O
such	O
constraints	O
then	O
the	O
model	O
compiler	O
which	O
generates	O
the	O
ground	B
network	I
could	O
avoid	O
creating	O
redundant	O
nodes	B
in	O
summary	O
we	O
can	O
think	O
of	O
mlns	O
as	O
a	O
convenient	O
way	O
of	O
specifying	O
a	O
ugm	B
template	B
that	O
can	O
get	O
unrolled	B
to	O
handle	O
data	O
of	O
arbitrary	O
size	O
there	O
are	O
several	O
other	O
ways	O
to	O
define	O
relational	B
probabilistic	I
models	I
see	O
e	O
g	O
and	O
friedman	O
kersting	O
et	O
al	O
for	O
details	O
in	O
some	O
cases	O
there	O
is	O
uncertainty	B
about	O
the	O
number	O
or	O
existence	O
of	O
objects	O
or	O
relations	O
so-called	O
open	B
universe	I
problem	O
section	O
gives	O
a	O
concrete	O
example	O
in	O
the	O
context	O
of	O
multi-object	O
tracking	B
see	O
e	O
g	O
and	O
norvig	O
kersting	O
et	O
al	O
and	O
references	O
therein	O
for	O
further	O
details	O
learning	B
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
perform	O
ml	O
and	O
map	O
parameter	B
estimation	O
for	O
mrfs	O
we	O
will	O
see	O
that	O
this	O
is	O
quite	O
computationally	O
expensive	O
for	O
this	O
reason	O
it	O
is	O
rare	O
to	O
perform	O
bayesian	B
inference	B
for	O
the	O
parameters	O
of	O
mrfs	O
see	O
et	O
al	O
training	O
maxent	B
models	O
using	O
gradient	O
methods	O
consider	O
an	O
mrf	B
in	O
log-linear	B
form	O
py	O
z	O
exp	O
t	O
c	O
cy	O
c	O
learning	B
where	O
c	O
indexes	O
the	O
cliques	B
the	O
scaled	O
log-likelihood	O
is	O
given	O
by	O
c	O
cyi	O
log	O
z	O
t	O
log	O
pyi	O
n	O
n	O
i	O
i	O
c	O
since	O
mrfs	O
are	O
in	O
the	O
exponential	B
family	B
we	O
know	O
that	O
this	O
function	O
is	O
convex	B
in	O
section	O
so	O
it	O
has	O
a	O
unique	O
global	O
maximum	O
which	O
we	O
can	O
find	O
using	O
gradient-based	O
optimizers	O
in	O
particular	O
the	O
derivative	O
for	O
the	O
weights	O
of	O
a	O
particular	O
clique	B
c	O
is	O
given	O
by	O
c	O
n	O
cyi	O
c	O
log	O
z	O
i	O
exercise	O
asks	O
you	O
to	O
show	O
that	O
the	O
derivative	O
of	O
the	O
log	B
partition	B
function	I
wrt	O
c	O
is	O
the	O
expectation	O
of	O
the	O
c	O
th	O
feature	O
under	O
the	O
model	O
i	O
e	O
cypy	O
log	O
z	O
c	O
e	O
cy	O
y	O
hence	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
is	O
e	O
cy	O
cyi	O
c	O
n	O
i	O
in	O
the	O
first	O
term	O
we	O
fix	O
y	O
to	O
its	O
observed	O
values	O
this	O
is	O
sometimes	O
called	O
the	O
clamped	B
term	I
in	O
the	O
second	O
term	O
y	O
is	O
free	O
this	O
is	O
sometimes	O
called	O
the	O
unclamped	B
term	I
or	O
contrastive	B
term	I
note	O
that	O
computing	O
the	O
unclamped	B
term	I
requires	O
inference	B
in	O
the	O
model	O
and	O
this	O
must	O
be	O
done	O
once	O
per	O
gradient	O
step	O
this	O
makes	O
ugm	B
training	O
much	O
slower	O
than	O
dgm	B
training	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
can	O
be	O
rewritten	O
as	O
the	O
expected	O
feature	O
vector	O
according	O
to	O
the	O
empirical	B
distribution	I
minus	O
the	O
model	O
s	O
expectation	O
of	O
the	O
feature	O
vector	O
c	O
epemp	O
cy	O
ep	B
cy	O
at	O
the	O
optimum	O
the	O
gradient	O
will	O
be	O
zero	O
so	O
the	O
empirical	B
distribution	I
of	O
the	O
features	B
will	O
match	O
the	O
model	O
s	O
predictions	O
epemp	O
cy	O
ep	B
cy	O
this	O
is	O
called	O
moment	B
matching	I
this	O
observation	B
motivates	O
a	O
different	O
optimization	B
algorithm	O
which	O
we	O
discuss	O
in	O
section	O
training	O
partially	O
observed	O
maxent	B
models	O
suppose	O
we	O
have	O
missing	B
data	I
andor	O
hidden	B
variables	I
in	O
our	O
model	O
represent	O
such	O
models	O
as	O
follows	O
in	O
general	O
we	O
can	O
t	O
c	O
ch	O
y	O
py	O
h	O
z	O
exp	O
c	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
the	O
log	O
likelihood	B
has	O
the	O
form	O
n	O
where	O
hi	O
i	O
n	O
log	O
z	O
hi	O
pyi	O
hi	O
pyi	O
hi	O
log	O
i	O
py	O
h	O
exp	O
t	O
c	O
ch	O
y	O
hi	O
pyi	O
hi	O
is	O
the	O
same	O
as	O
the	O
partition	B
function	I
is	O
the	O
unnormalized	O
distribution	O
the	O
term	O
for	O
the	O
whole	O
model	O
except	O
that	O
y	O
is	O
fixed	O
at	O
yi	O
hence	O
the	O
gradient	O
is	O
just	O
the	O
expected	O
features	B
where	O
we	O
clamp	O
yi	O
but	O
average	O
over	O
h	O
c	O
hi	O
c	O
log	O
pyi	O
hi	O
so	O
the	O
overall	O
gradient	O
is	O
given	O
by	O
e	O
ch	O
yi	O
c	O
n	O
i	O
ch	O
yi	O
e	O
ch	O
y	O
the	O
first	O
set	O
of	O
expectations	O
are	O
computed	O
by	O
clamping	B
the	O
visible	B
nodes	B
to	O
their	O
observed	O
values	O
and	O
the	O
second	O
set	O
are	O
computed	O
by	O
letting	O
the	O
visible	B
nodes	B
be	O
free	O
in	O
both	O
cases	O
we	O
marginalize	O
over	O
hi	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
generalized	B
em	B
where	O
we	O
use	O
gradient	O
methods	O
in	O
the	O
m	B
step	I
see	O
and	O
friedman	O
for	O
details	O
approximate	O
methods	O
for	O
computing	O
the	O
mles	O
of	O
mrfs	O
when	O
fitting	O
a	O
ugm	B
there	O
is	O
general	O
no	O
closed	O
form	O
solution	O
for	O
the	O
ml	O
or	O
the	O
map	B
estimate	I
of	O
the	O
parameters	O
so	O
we	O
need	O
to	O
use	O
gradient-based	O
optimizers	O
this	O
gradient	O
requires	O
inference	B
in	O
models	O
where	O
inference	B
is	O
intractable	O
learning	B
also	O
becomes	O
intractable	O
this	O
has	O
motivated	O
various	O
computationally	O
faster	O
alternatives	O
to	O
mlmap	O
estimation	O
which	O
we	O
list	O
in	O
table	O
we	O
dicsuss	O
some	O
of	O
these	O
alternatives	O
below	O
and	O
defer	O
others	O
to	O
later	O
sections	O
pseudo	B
likelihood	B
one	O
alternative	O
to	O
mle	B
is	O
to	O
maximize	O
the	O
pseudo	B
likelihood	B
defined	O
as	O
follows	O
l	O
pempy	O
log	O
pydy	O
d	O
n	O
log	O
pyidyi	O
d	O
that	O
is	O
we	O
optimize	O
the	O
product	O
of	O
the	O
full	B
conditionals	O
also	O
known	O
as	O
the	O
composite	B
likelihood	B
compare	O
this	O
to	O
the	O
objective	O
for	O
maximum	O
likelihood	B
l	O
pempy	O
log	O
py	O
log	O
pyi	O
yx	O
y	O
learning	B
method	O
closed	O
form	O
ipf	B
gradient-based	O
optimization	B
max-margin	O
training	O
pseudo-likelihood	B
stochastic	O
ml	O
contrastive	B
divergence	I
minimum	O
probability	O
flow	O
restriction	O
only	O
chordal	B
mrf	B
only	O
tabular	O
gaussian	B
mrf	B
low	O
tree	B
width	O
only	O
crfs	O
no	O
hidden	B
variables	I
can	O
integrate	B
out	I
the	O
hiddens	O
exact	O
mle	B
exact	O
exact	O
exact	O
na	O
approximate	O
exact	O
to	O
mc	O
error	O
approximate	O
approximate	O
section	O
section	O
section	O
section	O
section	O
section	O
section	O
section	O
sohl-dickstein	O
et	O
al	O
table	O
some	O
methods	O
that	O
can	O
be	O
used	O
to	O
compute	O
approximate	O
ml	O
map	O
parameter	B
estimates	O
for	O
mrfs	O
crfs	O
low	O
tree-width	O
means	O
that	O
in	O
order	O
for	O
the	O
method	O
to	O
be	O
efficient	O
the	O
graph	B
must	O
tree-like	O
see	O
section	O
for	O
details	O
figure	O
a	O
small	O
lattice	B
observed	O
neighbors	B
based	O
on	O
figure	O
of	O
the	O
representation	O
used	O
by	O
pseudo	B
likelihood	B
solid	O
nodes	B
are	O
in	O
the	O
case	O
of	O
gaussian	B
mrfs	O
pl	O
is	O
equivalent	O
to	O
ml	O
but	O
this	O
is	O
not	O
true	O
in	O
general	O
and	O
jordan	O
the	O
pl	O
approach	O
is	O
illustrated	O
in	O
figure	O
for	O
a	O
grid	O
we	O
learn	O
to	O
predict	O
each	O
node	O
given	O
all	O
of	O
its	O
neighbors	B
this	O
objective	O
is	O
generally	O
fast	O
to	O
compute	O
since	O
each	O
full	B
conditional	I
pyidyi	O
d	O
only	O
requires	O
summing	O
over	O
the	O
states	O
of	O
a	O
single	O
node	O
yid	O
in	O
order	O
to	O
compute	O
the	O
local	O
normalization	O
constant	O
the	O
pl	O
approach	O
is	O
similar	B
to	O
fitting	O
each	O
full	B
conditional	I
separately	O
is	O
the	O
method	O
used	O
to	O
train	O
dependency	B
networks	I
discussed	O
in	O
section	O
except	O
that	O
the	O
parameters	O
are	O
tied	B
between	O
adjacent	O
nodes	B
one	O
problem	O
with	O
pl	O
is	O
that	O
it	O
is	O
hard	O
to	O
apply	O
to	O
models	O
with	O
hidden	B
variables	I
and	O
welling	O
another	O
more	O
subtle	O
problem	O
is	O
that	O
each	O
node	O
assumes	O
that	O
its	O
neighbors	B
have	O
if	O
node	O
t	O
nbrs	O
is	O
a	O
perfect	O
predictor	O
for	O
node	O
s	O
then	O
s	O
will	O
learn	O
to	O
rely	O
known	O
values	O
completely	O
on	O
node	O
t	O
even	O
at	O
the	O
expense	O
of	O
ignoring	O
other	O
potentially	O
useful	O
information	B
such	O
as	O
its	O
local	B
evidence	B
however	O
experiments	O
in	O
and	O
welling	O
hoefling	O
and	O
tibshirani	O
suggest	O
that	O
pl	O
works	O
as	O
well	O
as	O
exact	O
ml	O
for	O
fully	O
observed	O
ising	O
models	O
and	O
of	O
course	O
pl	O
is	O
much	O
faster	O
stochastic	B
maximum	I
likelihood	B
recall	B
that	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
for	O
a	O
fully	O
observed	O
mrf	B
is	O
given	O
by	O
i	O
n	O
e	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
the	O
gradient	O
for	O
a	O
partially	B
observed	I
mrf	B
is	O
similar	B
in	O
both	O
cases	O
we	O
can	O
approximate	O
the	O
model	O
expectations	O
using	O
monte	B
carlo	I
sampling	O
we	O
can	O
combine	O
this	O
with	O
stochastic	B
gradient	B
descent	I
which	O
takes	O
samples	B
from	O
the	O
empirical	B
distribution	I
pseudocode	O
for	O
the	O
resulting	O
method	O
is	O
shown	O
in	O
algorithm	O
algorithm	O
stochastic	B
maximum	I
likelihood	B
for	O
fitting	O
an	O
mrf	B
initialize	O
weights	O
randomly	O
k	O
for	O
each	O
epoch	B
do	O
for	O
each	O
minibatch	O
of	O
size	O
b	O
do	O
for	O
each	O
sample	O
s	O
s	O
do	O
sample	O
ysk	O
py	O
k	O
gik	O
e	O
e	O
s	O
for	O
each	O
training	O
case	O
i	O
in	O
minibatch	O
do	O
gk	O
i	O
b	O
gik	O
k	O
gk	O
b	O
k	O
k	O
decrease	O
step	B
size	I
typically	O
we	O
use	O
mcmc	B
to	O
generate	O
the	O
samples	B
of	O
course	O
running	O
mcmc	B
to	O
convergence	O
at	O
each	O
step	O
of	O
the	O
inner	O
loop	B
would	O
be	O
extremely	O
slow	O
fortunately	O
it	O
was	O
shown	O
in	O
that	O
we	O
can	O
start	O
the	O
mcmc	B
chain	O
at	O
its	O
previous	O
value	O
and	O
just	O
take	O
a	O
few	O
steps	O
in	O
otherwords	O
we	O
sample	O
ysk	O
by	O
initializing	O
the	O
mcmc	B
chain	O
at	O
ysk	O
and	O
then	O
run	O
for	O
a	O
few	O
iterations	O
this	O
is	O
valid	O
since	O
py	O
k	O
is	O
likely	O
to	O
be	O
close	O
to	O
py	O
k	O
since	O
we	O
only	O
changed	O
the	O
parameters	O
a	O
small	O
amount	O
we	O
call	O
this	O
algorithm	O
stochastic	B
maximum	I
likelihood	B
or	O
sml	B
is	O
a	O
closely	O
related	O
algorithm	O
called	O
persistent	B
contrastive	B
divergence	I
which	O
we	O
discuss	O
in	O
section	O
feature	B
induction	B
for	O
maxent	B
models	O
mrfs	O
require	O
a	O
good	O
set	O
of	O
features	B
one	O
unsupervised	O
way	O
to	O
learn	O
such	O
features	B
known	O
as	O
feature	B
induction	B
is	O
to	O
start	O
with	O
a	O
base	O
set	O
of	O
features	B
and	O
then	O
to	O
continually	O
create	O
new	O
feature	O
combinations	O
out	O
of	O
old	O
ones	O
greedily	O
adding	O
the	O
best	O
ones	O
to	O
the	O
model	O
this	O
approach	O
was	O
first	O
proposed	O
in	O
et	O
al	O
zhu	O
et	O
al	O
and	O
was	O
later	O
extended	O
to	O
the	O
crf	B
case	O
in	O
to	O
illustrate	O
the	O
basic	O
idea	O
we	O
present	O
an	O
example	O
from	O
et	O
al	O
which	O
described	O
how	O
to	O
build	O
unconditional	O
probabilistic	O
models	O
to	O
represent	O
english	O
spelling	O
initially	O
the	O
model	O
has	O
no	O
features	B
which	O
represents	O
the	O
uniform	B
distribution	I
the	O
algorithm	O
starts	O
by	O
choosing	O
to	O
add	O
the	O
feature	O
iyt	O
z	O
t	O
learning	B
which	O
checks	O
if	O
any	O
letter	O
is	O
lower	O
case	O
or	O
not	O
after	O
the	O
feature	O
is	O
added	O
the	O
parameters	O
are	O
by	O
maximum	O
likelihood	B
for	O
this	O
feature	O
it	O
turns	O
out	O
that	O
which	O
means	O
that	O
a	O
word	O
with	O
a	O
lowercase	O
letter	O
in	O
any	O
position	O
is	O
about	O
times	O
more	O
likely	O
than	O
the	O
same	O
word	O
without	O
a	O
lowercase	O
letter	O
in	O
that	O
position	O
some	O
samples	B
from	O
this	O
model	O
generated	O
using	O
gibbs	B
sampling	I
are	O
shown	O
m	O
r	O
xevo	O
ijjiir	O
b	O
to	O
jz	O
gsr	O
wq	O
vf	O
x	O
ga	O
msmgh	O
pcp	O
d	O
ozivlal	O
hzagh	O
yzop	O
io	O
advzmxnv	O
ijv	O
bolft	O
x	O
emx	O
kayerf	O
mlj	O
rawzyb	O
jp	O
ag	O
ctdnnnbg	O
wgdw	O
t	O
kguv	O
cy	O
spxcq	O
uzflbbf	O
dxtkkn	O
cxwx	O
jpd	O
ztzh	O
lv	O
zhpkvnu	O
l	O
r	O
qee	O
nynrx	O
ik	O
se	O
w	O
lrh	O
hp	O
yrqyka	O
h	O
zcngotcnx	O
igcump	O
zjcjs	O
lqpwiqu	O
cefmfhc	O
o	O
lb	O
fdcy	O
tzby	O
yopxmvk	O
by	O
fz	O
t	O
govyccm	O
ijyiduwfzo	O
duh	O
ejv	O
pk	O
pjw	O
l	O
fl	O
w	O
the	O
second	O
feature	O
added	O
by	O
the	O
algorithm	O
checks	O
if	O
two	O
adjacent	O
characters	O
are	O
lower	O
case	O
iys	O
z	O
yt	O
z	O
s	O
t	O
now	O
the	O
model	O
has	O
the	O
form	O
py	O
z	O
exp	O
continuing	O
in	O
this	O
way	O
the	O
algorithm	O
adds	O
features	B
for	O
the	O
strings	O
s	O
and	O
ing	O
where	O
represents	O
the	O
end	O
of	O
word	O
and	O
for	O
various	O
regular	B
expressions	O
such	O
as	O
etc	O
some	O
samples	B
from	O
the	O
model	O
with	O
features	B
generated	O
using	O
gibbs	B
sampling	I
are	O
shown	O
below	O
was	O
reaser	O
in	O
there	O
to	O
will	O
was	O
by	O
homes	O
thing	O
be	O
reloverated	O
ther	O
which	O
conists	O
at	O
fores	O
anditing	O
with	O
mr	O
proveral	O
the	O
on	O
t	O
prolling	O
prothere	O
mento	O
at	O
yaou	O
chestraing	O
for	O
have	O
to	O
intrally	O
of	O
qut	O
best	O
compers	O
cluseliment	O
uster	O
of	O
is	O
deveral	O
this	O
thise	O
of	O
offect	O
inatever	O
thifer	O
constranded	O
stater	O
vill	O
in	O
thase	O
in	O
youse	O
menttering	O
and	O
of	O
in	O
verate	O
of	O
to	O
this	O
approach	O
of	O
feature	O
learning	B
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
graphical	B
model	I
structure	B
learning	B
except	O
it	O
is	O
more	O
fine-grained	O
we	O
add	O
features	B
that	O
are	O
useful	O
regardless	O
of	O
the	O
resulting	O
graph	B
structure	O
however	O
the	O
resulting	O
graphs	O
can	O
become	O
densely	O
connected	O
which	O
makes	O
inference	B
hence	O
parameter	B
estimation	O
intractable	O
iterative	O
proportional	O
fitting	O
consider	O
a	O
pairwise	B
mrf	B
where	O
the	O
potentials	O
are	O
represented	O
as	O
tables	O
with	O
one	O
parameter	B
per	O
variable	O
setting	O
we	O
can	O
represent	O
this	O
in	O
log-linear	B
form	O
using	O
stys	O
yt	O
exp	O
t	O
stiys	O
yt	O
iys	O
k	O
yt	O
k	O
and	O
similarly	O
for	O
tyt	O
thus	O
the	O
feature	O
vectors	O
are	O
just	O
indicator	O
functions	O
we	O
thank	O
john	O
lafferty	O
for	O
sharing	O
this	O
example	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
from	O
equation	O
we	O
have	O
that	O
at	O
the	O
maximum	O
of	O
the	O
likelihood	B
the	O
empirical	O
expectation	O
of	O
the	O
features	B
equals	O
the	O
model	O
s	O
expectation	O
epemp	O
j	O
yt	O
k	O
ep	B
j	O
yt	O
k	O
pempys	O
j	O
yt	O
k	O
where	O
pemp	O
is	O
the	O
empirical	O
probability	O
ys	O
j	O
yt	O
k	O
pempys	O
j	O
yt	O
k	O
nstjk	O
n	O
iyns	O
j	O
ynt	O
k	O
n	O
for	O
a	O
general	O
graph	B
the	O
condition	O
that	O
must	O
hold	O
at	O
the	O
optimum	O
is	O
pempyc	O
pyc	O
for	O
a	O
special	O
family	B
of	O
graphs	O
known	O
as	O
decomposable	B
graphs	I
in	O
section	O
one	O
can	O
show	O
that	O
pyc	O
cyc	O
however	O
even	O
if	O
the	O
graph	B
is	O
not	O
decomposable	B
we	O
can	O
imagine	O
trying	O
to	O
enforce	O
this	O
condition	O
this	O
suggests	O
an	O
iterative	O
coordinate	O
ascent	O
scheme	O
where	O
at	O
each	O
step	O
we	O
compute	O
c	O
t	O
cyc	O
pempyc	O
pyc	O
t	O
where	O
the	O
multiplication	O
is	O
elementwise	O
this	O
is	O
known	O
as	O
iterative	O
proportional	O
fitting	O
or	O
ipf	B
bishop	O
et	O
al	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
algorithm	O
iterative	O
proportional	O
fitting	O
algorithm	O
for	O
tabular	O
mrfs	O
initialize	O
c	O
for	O
c	O
repeat	O
for	O
c	O
do	O
pc	O
pyc	O
pc	O
pempyc	O
c	O
c	O
pc	O
pc	O
until	O
converged	O
example	O
let	O
us	O
consider	O
a	O
simple	O
example	O
from	O
httpen	O
wikipedia	O
orgwikiiterative	O
propo	O
rtional	O
fitting	O
we	O
have	O
two	O
binary	O
variables	O
and	O
where	O
if	O
man	O
n	O
is	O
left	O
handed	O
and	O
otherwise	O
similarly	O
if	O
woman	O
n	O
is	O
left	O
handed	O
and	O
otherwise	O
we	O
can	O
summarize	O
the	O
data	O
using	O
the	O
following	O
contingency	B
table	I
male	O
female	O
total	O
right-handed	O
left-handed	O
total	O
learning	B
suppose	O
we	O
want	O
to	O
fit	O
a	O
disconnected	O
graphical	B
model	I
containing	O
nodes	B
and	O
but	O
with	O
c	O
no	O
edge	O
between	O
them	O
that	O
is	O
we	O
want	O
to	O
find	O
vectors	O
and	O
such	O
that	O
m	O
t	O
where	O
m	O
are	O
the	O
model	O
s	O
expected	O
counts	O
and	O
c	O
are	O
the	O
empirical	O
counts	O
by	O
moment	B
matching	I
we	O
find	O
that	O
the	O
row	O
and	O
column	O
sums	O
of	O
the	O
model	O
must	O
exactly	O
match	O
the	O
row	O
and	O
column	O
sums	O
of	O
the	O
data	O
one	O
possible	O
solution	O
is	O
to	O
use	O
and	O
below	O
we	O
show	O
the	O
model	O
s	O
predictions	O
m	O
t	O
total	O
right-handed	O
left-handed	O
male	O
female	O
total	O
it	O
is	O
easy	O
to	O
see	O
that	O
this	O
matches	O
the	O
required	O
constraints	O
see	O
for	O
some	O
matlab	O
code	O
that	O
computes	O
these	O
numbers	O
this	O
method	O
is	O
easily	O
to	O
generalized	O
to	O
arbitrary	O
graphs	O
speed	O
of	O
ipf	B
ipf	B
is	O
a	O
fixed	O
point	O
algorithm	O
for	O
enforcing	O
the	O
moment	B
matching	I
constraints	O
and	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
et	O
al	O
the	O
number	O
of	O
iterations	O
depends	O
on	O
the	O
form	O
of	O
the	O
model	O
if	O
the	O
graph	B
is	O
decomposable	B
then	O
ipf	B
converges	O
in	O
a	O
single	O
iteration	O
but	O
in	O
general	O
ipf	B
may	O
require	O
many	O
iterations	O
it	O
is	O
clear	O
that	O
the	O
dominant	O
cost	O
of	O
ipf	B
is	O
computing	O
the	O
required	O
marginals	O
under	O
the	O
model	O
efficient	O
methods	O
such	O
as	O
the	O
junction	B
tree	B
algorithm	I
can	O
be	O
used	O
resulting	O
in	O
something	O
called	O
efficient	B
ipf	B
and	O
preucil	O
nevertheless	O
coordinate	O
descent	O
can	O
be	O
slow	O
an	O
alternative	O
method	O
is	O
to	O
update	O
all	O
the	O
parameters	O
at	O
once	O
by	O
simply	O
following	O
the	O
gradient	O
of	O
the	O
likelihood	B
this	O
gradient	O
approach	O
has	O
the	O
further	O
significant	O
advantage	O
that	O
it	O
works	O
for	O
models	O
in	O
which	O
the	O
clique	B
potentials	O
may	O
not	O
be	O
fully	O
parameterized	O
i	O
e	O
the	O
features	B
may	O
not	O
consist	O
of	O
all	O
possible	O
indicators	O
for	O
each	O
clique	B
but	O
instead	O
can	O
be	O
arbitrary	O
although	O
it	O
is	O
possible	O
to	O
adapt	O
ipf	B
to	O
this	O
setting	O
of	O
general	O
features	B
resulting	O
in	O
a	O
method	O
known	O
as	O
iterative	B
scaling	I
in	O
practice	O
the	O
gradient	O
method	O
is	O
much	O
faster	O
minka	O
generalizations	O
of	O
ipf	B
we	O
can	O
use	O
ipf	B
to	O
fit	O
gaussian	B
graphical	B
models	I
instead	O
of	O
working	O
with	O
empirical	O
counts	O
we	O
work	O
with	O
empirical	O
means	O
and	O
covariances	O
and	O
kiiveri	O
it	O
is	O
also	O
possible	O
to	O
create	O
a	O
bayesian	B
ipf	B
algorithm	O
for	O
sampling	O
from	O
the	O
posterior	O
of	O
the	O
model	O
s	O
parameters	O
e	O
g	O
and	O
massam	O
ipf	B
for	O
decomposable	B
graphical	B
models	I
there	O
is	O
a	O
special	O
family	B
of	O
undirected	B
graphical	B
models	I
known	O
as	O
decomposable	B
graphical	B
models	I
this	O
is	O
formally	O
defined	O
in	O
section	O
but	O
the	O
basic	O
idea	O
is	O
that	O
it	O
contains	O
graphs	O
which	O
are	O
tree-like	O
such	O
graphs	O
can	O
be	O
represented	O
by	O
ugms	O
or	O
dgms	O
without	O
any	O
loss	B
of	O
information	B
in	O
the	O
case	O
of	O
decomposable	B
graphical	B
models	I
ipf	B
converges	O
in	O
one	O
iteration	O
in	O
fact	O
the	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
mle	B
has	O
a	O
closed	O
form	O
solution	O
in	O
particular	O
for	O
tabular	O
potentials	O
we	O
have	O
cyc	O
k	O
iyic	O
k	O
n	O
and	O
for	O
gaussian	B
potentials	O
we	O
have	O
c	O
yic	O
n	O
c	O
iyic	O
cxic	O
ct	O
n	O
by	O
using	O
conjugate	B
priors	I
we	O
can	O
also	O
easily	O
compute	O
the	O
full	B
posterior	O
over	O
the	O
model	O
parameters	O
in	O
the	O
decomposable	B
case	O
just	O
as	O
we	O
did	O
in	O
the	O
dgm	B
case	O
see	O
for	O
details	O
conditional	O
random	O
fields	O
a	O
conditional	O
random	O
field	O
or	O
crf	B
et	O
al	O
sometimes	O
a	O
discriminative	B
random	O
field	O
and	O
hebert	O
is	O
just	O
a	O
version	O
of	O
an	O
mrf	B
where	O
all	O
the	O
clique	B
potentials	O
are	O
conditioned	O
on	O
input	O
features	B
pyx	O
w	O
cycx	O
w	O
zx	O
w	O
c	O
a	O
crf	B
can	O
be	O
thought	O
of	O
as	O
a	O
structured	B
output	I
extension	B
of	O
logistic	B
regression	B
we	O
will	O
usually	O
assume	O
a	O
log-linear	B
representation	O
of	O
the	O
potentials	O
cycx	O
w	O
expwt	O
c	O
yc	O
where	O
yc	O
is	O
a	O
feature	O
vector	O
derived	O
from	O
the	O
global	O
inputs	O
x	O
and	O
the	O
local	O
set	O
of	O
labels	O
yc	O
we	O
will	O
give	O
some	O
examples	O
below	O
which	O
will	O
make	O
this	O
notation	O
clearer	O
the	O
advantage	O
of	O
a	O
crf	B
over	O
an	O
mrf	B
is	O
analogous	O
to	O
the	O
advantage	O
of	O
a	O
discriminative	B
classifier	O
over	O
a	O
generative	O
classifier	O
section	O
namely	O
we	O
don	O
t	O
need	O
to	O
waste	O
resources	O
modeling	O
things	O
that	O
we	O
always	O
observe	O
instead	O
we	O
can	O
focus	O
our	O
attention	O
on	O
modeling	O
what	O
we	O
care	O
about	O
namely	O
the	O
distribution	O
of	O
labels	O
given	O
the	O
data	O
another	O
important	O
advantage	O
of	O
crfs	O
is	O
that	O
we	O
can	O
make	O
the	O
potentials	O
factors	B
of	O
the	O
model	O
be	O
data-dependent	O
for	O
example	O
in	O
image	O
processing	O
applications	O
we	O
may	O
turn	O
off	O
the	O
label	B
smoothing	B
between	O
two	O
neighboring	O
nodes	B
s	O
and	O
t	O
if	O
there	O
is	O
an	O
observed	O
discontinuity	O
in	O
the	O
image	O
intensity	O
between	O
pixels	O
s	O
and	O
t	O
similarly	O
in	O
natural	O
language	O
processing	O
problems	O
we	O
can	O
make	O
the	O
latent	B
labels	O
depend	O
on	O
global	O
properties	O
of	O
the	O
sentence	O
such	O
as	O
which	O
language	O
it	O
is	O
written	O
in	O
it	O
is	O
hard	O
to	O
incorporate	O
global	O
features	B
into	O
generative	O
models	O
the	O
disadvantage	O
of	O
crfs	O
over	O
mrfs	O
is	O
that	O
they	O
require	O
labeled	O
training	O
data	O
and	O
they	O
are	O
slower	O
to	O
train	O
as	O
we	O
explain	O
in	O
section	O
this	O
is	O
analogous	O
to	O
the	O
strengths	O
and	O
weaknesses	O
of	O
logistic	B
regression	B
vs	O
naive	O
bayes	O
discussed	O
in	O
section	O
chain-structured	O
crfs	O
memms	O
and	O
the	O
label-bias	O
problem	O
the	O
most	O
widely	O
used	O
kind	O
of	O
crf	B
uses	O
a	O
chain-structured	O
graph	B
to	O
model	O
correlation	O
amongst	O
neighboring	O
labels	O
such	O
models	O
are	O
useful	O
for	O
a	O
variety	O
of	O
sequence	O
labeling	O
tasks	O
section	O
conditional	O
random	O
fields	O
yt	O
yt	O
yt	O
xt	O
xt	O
xt	O
xg	O
yt	O
xt	O
xg	O
yt	O
xt	O
yt	O
xt	O
figure	O
various	O
models	O
for	O
sequential	B
data	O
directed	B
memm	B
a	O
discriminative	B
undirected	B
crf	B
a	O
generative	O
directed	B
hmm	B
a	O
discriminative	B
traditionally	O
hmms	B
in	O
detail	O
in	O
chapter	O
have	O
been	O
used	O
for	O
such	O
tasks	O
these	O
are	O
joint	O
density	O
models	O
of	O
the	O
form	O
px	O
yw	O
pytyt	O
wpxtyt	O
w	O
where	O
we	O
have	O
dropped	O
the	O
initial	O
term	O
for	O
simplicity	O
see	O
figure	O
if	O
we	O
observe	O
both	O
xt	O
and	O
yt	O
for	O
all	O
t	O
it	O
is	O
very	O
easy	O
to	O
train	O
such	O
models	O
using	O
techniques	O
described	O
in	O
section	O
an	O
hmm	B
requires	O
specifying	O
a	O
generative	O
observation	B
model	I
pxtyt	O
w	O
which	O
can	O
be	O
difficult	O
furthemore	O
each	O
xt	O
is	O
required	O
to	O
be	O
local	O
since	O
it	O
is	O
hard	O
to	O
define	O
a	O
generative	O
model	O
for	O
the	O
whole	O
stream	O
of	O
observations	O
x	O
an	O
obvious	O
way	O
to	O
make	O
a	O
discriminative	B
version	O
of	O
an	O
hmm	B
is	O
to	O
reverse	O
the	O
arrows	O
from	O
yt	O
to	O
xt	O
as	O
in	O
figure	O
this	O
defines	O
a	O
directed	B
discriminative	B
model	O
of	O
the	O
form	O
pyx	O
w	O
pytyt	O
x	O
w	O
t	O
where	O
x	O
xg	O
xg	O
are	O
global	O
features	B
and	O
xt	O
are	O
features	B
specific	O
to	O
node	O
t	O
partition	O
into	O
local	O
and	O
global	O
is	O
not	O
necessary	O
but	O
helps	O
when	O
comparing	O
to	O
hmms	B
this	O
is	O
called	O
a	O
maximum	B
entropy	B
markov	B
model	I
or	O
memm	B
et	O
al	O
kakade	O
et	O
al	O
an	O
memm	B
is	O
simply	O
a	O
markov	B
chain	I
in	O
which	O
the	O
state	B
transition	O
probabilities	O
are	O
conditioned	O
is	O
therefore	O
a	O
special	O
case	O
of	O
an	O
input-output	O
hmm	B
discussed	O
in	O
on	O
the	O
input	O
features	B
section	O
this	O
seems	O
like	O
the	O
natural	O
generalization	B
of	O
logistic	B
regression	B
to	O
the	O
structuredoutput	O
setting	O
but	O
it	O
suffers	O
from	O
a	O
subtle	O
problem	O
known	O
obscurely	O
as	O
the	O
label	B
bias	B
problem	O
et	O
al	O
the	O
problem	O
is	O
that	O
local	O
features	B
at	O
time	O
t	O
do	O
not	O
influence	O
states	O
prior	O
to	O
time	O
t	O
this	O
follows	O
by	O
examining	O
the	O
dag	B
which	O
shows	O
that	O
xt	O
is	O
d-separated	B
from	O
yt	O
all	O
earlier	O
time	O
points	O
by	O
the	O
v-structure	B
at	O
yt	O
which	O
is	O
a	O
hidden	B
child	O
thus	O
blocking	O
the	O
information	B
flow	O
to	O
understand	O
what	O
this	O
means	O
in	O
practice	O
consider	O
the	O
part	B
of	I
speech	I
tagging	I
task	O
suppose	O
we	O
see	O
the	O
word	O
banks	O
this	O
could	O
be	O
a	O
verb	O
in	O
he	O
banks	O
at	O
boa	O
or	O
a	O
noun	O
in	O
the	O
river	O
banks	O
were	O
overflowing	O
locally	O
the	O
pos	O
tag	O
for	O
the	O
word	O
is	O
ambiguous	O
however	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
figure	O
example	O
of	O
handwritten	O
letter	O
recognition	O
in	O
the	O
word	O
brace	O
the	O
r	O
and	O
the	O
c	O
look	O
very	O
similar	B
but	O
can	O
be	O
disambiguated	O
using	O
context	O
source	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
ben	O
taskar	O
suppose	O
that	O
later	O
in	O
the	O
sentence	O
we	O
see	O
the	O
word	O
fishing	O
this	O
gives	O
us	O
enough	O
context	O
to	O
infer	O
that	O
the	O
sense	O
of	O
banks	O
is	O
river	O
banks	O
however	O
in	O
an	O
memm	B
in	O
an	O
hmm	B
and	O
crf	B
the	O
fishing	O
evidence	B
will	O
not	O
flow	O
backwards	O
so	O
we	O
will	O
not	O
be	O
able	O
to	O
disambiguate	O
banks	O
now	O
consider	O
a	O
chain-structured	O
crf	B
this	O
model	O
has	O
the	O
form	O
pyx	O
w	O
w	O
w	O
t	O
zx	O
w	O
from	O
the	O
graph	B
in	O
figure	O
we	O
see	O
that	O
the	O
label	B
bias	B
problem	O
no	O
longer	O
exists	O
since	O
yt	O
does	O
not	O
block	O
the	O
information	B
from	O
xt	O
from	O
reaching	O
other	O
nodes	B
the	O
label	B
bias	B
problem	O
in	O
memms	O
occurs	O
because	O
directed	B
models	O
are	O
locally	B
normalized	I
meaning	O
each	O
cpd	B
sums	O
to	O
by	O
contrast	O
mrfs	O
and	O
crfs	O
are	O
globally	B
normalized	I
which	O
means	O
that	O
local	O
factors	B
do	O
not	O
need	O
to	O
sum	O
to	O
since	O
the	O
partition	B
function	I
z	O
which	O
sums	O
over	O
all	O
joint	O
configurations	O
will	O
ensure	O
the	O
model	O
defines	O
a	O
valid	O
distribution	O
however	O
this	O
solution	O
comes	O
at	O
a	O
price	O
we	O
do	O
not	O
get	O
a	O
valid	O
probability	O
distribution	O
over	O
y	O
until	O
we	O
have	O
seen	O
the	O
whole	O
sentence	O
since	O
only	O
then	O
can	O
we	O
normalize	O
over	O
all	O
configurations	O
consequently	O
crfs	O
are	O
not	O
as	O
useful	O
as	O
dgms	O
discriminative	B
or	O
generative	O
for	O
online	O
or	O
real-time	O
inference	B
furthermore	O
the	O
fact	O
that	O
z	O
depends	O
on	O
all	O
the	O
nodes	B
and	O
hence	O
all	O
their	O
parameters	O
makes	O
crfs	O
much	O
slower	O
to	O
train	O
than	O
dgms	O
as	O
we	O
will	O
see	O
in	O
section	O
applications	O
of	O
crfs	O
crfs	O
have	O
been	O
applied	O
to	O
many	O
interesting	O
problems	O
we	O
give	O
a	O
representative	O
sample	O
below	O
these	O
applications	O
illustrate	O
several	O
useful	O
modeling	O
tricks	O
and	O
will	O
also	O
provide	O
motivation	O
for	O
some	O
of	O
the	O
inference	B
techniques	O
we	O
will	O
discuss	O
in	O
chapter	O
handwriting	B
recognition	I
a	O
natural	O
application	O
of	O
crfs	O
is	O
to	O
classify	O
hand-written	O
digit	O
strings	O
as	O
illustrated	O
in	O
figure	O
the	O
key	O
observation	B
is	O
that	O
locally	O
a	O
letter	O
may	O
be	O
ambiguous	O
but	O
by	O
depending	O
on	O
the	O
labels	O
of	O
one	O
s	O
neighbors	B
it	O
is	O
possible	O
to	O
use	O
context	O
to	O
reduce	O
the	O
error	O
rate	B
note	O
that	O
the	O
node	O
potential	O
tytxt	O
is	O
often	O
taken	O
to	O
be	O
a	O
probabilistic	O
discriminative	B
classifier	O
conditional	O
random	O
fields	O
b	O
adj	O
i	O
n	O
o	O
v	O
o	O
in	O
o	O
v	O
b	O
prp	O
i	O
n	O
o	O
b	O
in	O
dt	O
i	O
n	O
i	O
np	O
n	O
pos	O
british	O
airways	O
rose	O
after	O
announcing	O
its	O
withdrawal	O
from	O
the	O
ual	O
deal	O
key	O
b	O
i	O
o	O
n	O
adj	O
begin	O
noun	O
phrase	O
within	O
noun	O
phrase	O
not	O
a	O
noun	O
phrase	O
noun	O
adjective	O
v	O
in	O
prp	O
dt	O
verb	O
preposition	O
possesive	O
pronoun	O
determiner	O
a	O
an	O
the	O
figure	O
a	O
crf	B
for	O
joint	O
pos	O
tagging	O
and	O
np	O
segmentation	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
source	O
figure	O
of	O
and	O
such	O
as	O
a	O
neural	B
network	I
or	O
rvm	B
that	O
is	O
trained	O
on	O
isolated	O
letters	O
and	O
the	O
edge	O
potentials	O
stys	O
yt	O
are	O
often	O
taken	O
to	O
be	O
a	O
language	O
bigram	B
model	I
later	O
we	O
will	O
discuss	O
how	O
to	O
train	O
all	O
the	O
potentials	O
jointly	O
noun	B
phrase	I
chunking	I
one	O
common	O
nlp	O
task	O
is	O
noun	B
phrase	I
chunking	I
which	O
refers	O
to	O
the	O
task	O
of	O
segmenting	O
a	O
sentence	O
into	O
its	O
distinct	O
noun	O
phrases	O
this	O
is	O
a	O
simple	O
example	O
of	O
a	O
technique	O
known	O
as	O
shallow	B
parsing	I
in	O
more	O
detail	O
we	O
tag	O
each	O
word	O
in	O
the	O
sentence	O
with	O
b	O
beginning	O
of	O
a	O
new	O
np	O
i	O
inside	O
a	O
np	O
or	O
o	O
outside	O
an	O
np	O
this	O
is	O
called	O
bio	B
notation	O
for	O
example	O
in	O
the	O
following	O
sentence	O
the	O
nps	O
are	O
marked	O
with	O
brackets	O
b	O
i	O
o	O
o	O
o	O
b	O
i	O
o	O
b	O
i	O
i	O
airways	O
rose	O
after	O
announcing	O
withdrawl	O
from	O
uai	O
deal	O
need	O
the	O
b	O
symbol	O
so	O
that	O
we	O
can	O
distinguish	O
i	O
i	O
meaning	O
two	O
words	O
within	O
a	O
single	O
np	O
from	O
b	O
b	O
meaning	O
two	O
separate	O
nps	O
a	O
standard	O
approach	O
to	O
this	O
problem	O
would	O
first	O
convert	O
the	O
string	O
of	O
words	O
into	O
a	O
string	O
of	O
pos	O
tags	O
and	O
then	O
convert	O
the	O
pos	O
tags	O
to	O
a	O
string	O
of	O
bios	O
however	O
such	O
a	O
pipeline	B
method	O
can	O
propagate	O
errors	O
a	O
more	O
robust	B
approach	O
is	O
to	O
build	O
a	O
joint	O
probabilistic	O
model	O
of	O
the	O
form	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
use	O
the	O
crf	B
in	O
figure	O
the	O
connections	O
between	O
adjacent	O
labels	O
encode	O
the	O
probability	O
of	O
transitioning	O
between	O
the	O
b	O
i	O
and	O
o	O
states	O
and	O
can	O
enforce	O
constraints	O
such	O
as	O
the	O
fact	O
that	O
b	O
must	O
preceed	O
i	O
the	O
features	B
are	O
usually	O
hand	O
engineered	O
and	O
include	O
things	O
like	O
does	O
this	O
word	O
begin	O
with	O
a	O
capital	O
letter	O
is	O
this	O
word	O
followed	O
by	O
a	O
full	B
stop	O
is	O
this	O
word	O
a	O
noun	O
etc	O
typically	O
there	O
are	O
features	B
per	O
node	O
the	O
number	O
of	O
features	B
has	O
minimal	B
impact	O
on	O
the	O
inference	B
time	O
since	O
the	O
features	B
are	O
increase	O
in	O
the	O
cost	O
of	O
observed	O
and	O
do	O
not	O
need	O
to	O
be	O
summed	O
over	O
is	O
a	O
small	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
b-per	O
i-per	O
oth	O
oth	O
oth	O
b-loc	O
i-loc	O
b-per	O
oth	O
oth	O
oth	O
oth	O
mrs	O
green	O
spoke	O
today	O
in	O
new	O
york	O
green	O
chairs	O
the	O
finance	O
committee	O
key	O
b-per	O
i-per	O
b-loc	O
begin	O
person	O
name	O
within	O
person	O
name	O
begin	O
location	O
name	O
i-loc	O
oth	O
within	O
location	O
name	O
not	O
an	O
entitiy	O
figure	O
a	O
skip-chain	B
crf	B
for	O
named	O
entity	O
recognition	O
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
evaluating	O
potential	O
functions	O
with	O
many	O
features	B
but	O
this	O
is	O
usually	O
negligible	O
if	O
not	O
one	O
can	O
use	O
regularization	B
to	O
prune	O
out	O
irrelevant	O
features	B
however	O
the	O
graph	B
structure	O
can	O
have	O
a	O
dramatic	O
effect	O
on	O
inference	B
time	O
the	O
model	O
in	O
figure	O
is	O
tractable	O
since	O
it	O
is	O
essentially	O
a	O
fat	O
chain	O
so	O
we	O
can	O
use	O
the	O
forwards-backwards	B
algorithm	I
for	O
exact	O
inference	B
in	O
time	O
where	O
is	O
the	O
number	O
of	O
pos	O
tags	O
and	O
is	O
the	O
number	O
of	O
np	O
tags	O
however	O
the	O
seemingly	O
similar	B
graph	B
in	O
figure	O
to	O
be	O
explained	O
below	O
is	O
computationally	O
intractable	O
named	O
entity	O
recognition	O
a	O
task	O
that	O
is	O
related	O
to	O
np	O
chunking	O
is	O
named	B
entity	I
extraction	I
instead	O
of	O
just	O
segmenting	O
out	O
noun	O
phrases	O
we	O
can	O
segment	O
out	O
phrases	O
to	O
do	O
with	O
people	O
and	O
locations	O
similar	B
techniques	O
are	O
used	O
to	O
automatically	O
populate	O
your	O
calendar	O
from	O
your	O
email	O
messages	O
this	O
is	O
called	O
information	B
extraction	I
a	O
simple	O
approach	O
to	O
this	O
is	O
to	O
use	O
a	O
chain-structured	O
crf	B
but	O
to	O
expand	O
the	O
state	B
space	I
from	O
bio	B
to	O
b-per	O
i-per	O
b-loc	O
i-loc	O
and	O
other	O
however	O
sometimes	O
it	O
is	O
ambiguous	O
whether	O
a	O
word	O
is	O
a	O
person	O
location	O
or	O
something	O
else	O
nouns	O
are	O
particularly	O
difficult	O
to	O
deal	O
with	O
because	O
they	O
belong	O
to	O
an	O
open	B
class	I
that	O
is	O
there	O
is	O
an	O
unbounded	O
number	O
of	O
possible	O
names	O
unlike	O
the	O
set	O
of	O
nouns	O
and	O
verbs	O
which	O
is	O
large	O
but	O
essentially	O
fixed	O
we	O
can	O
get	O
better	O
performance	O
by	O
considering	O
long-range	O
correlations	O
between	O
words	O
for	O
example	O
we	O
might	O
add	O
a	O
link	O
between	O
all	O
occurrences	O
of	O
the	O
same	O
word	O
and	O
force	O
the	O
word	O
to	O
have	O
the	O
same	O
tag	O
in	O
each	O
occurence	O
same	O
technique	O
can	O
also	O
be	O
helpful	O
for	O
resolving	O
the	O
identity	O
of	O
pronouns	O
this	O
is	O
known	O
as	O
a	O
skip-chain	B
crf	B
see	O
figure	O
for	O
an	O
illustration	O
we	O
see	O
that	O
the	O
graph	B
structure	O
itself	O
changes	O
depending	O
on	O
the	O
input	O
which	O
is	O
an	O
additional	O
advantage	O
of	O
crfs	O
over	O
generative	O
models	O
unfortunately	O
inference	B
in	O
this	O
model	O
is	O
generally	O
more	O
expensive	O
than	O
in	O
a	O
simple	O
chain	O
with	O
local	O
connections	O
for	O
reasons	O
explained	O
in	O
section	O
conditional	O
random	O
fields	O
figure	O
illustration	O
of	O
a	O
simple	O
parse	O
tree	B
based	O
on	O
a	O
context	B
free	I
grammar	I
in	O
chomsky	B
normal	B
form	I
the	O
feature	O
vector	O
y	O
y	O
counts	O
the	O
number	O
of	O
times	O
each	O
production	O
rule	O
was	O
used	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yasemin	O
altun	O
natural	O
language	O
parsing	O
a	O
generalization	B
of	O
chain-structured	O
models	O
for	O
language	O
is	O
to	O
use	O
probabilistic	O
grammars	B
in	O
particular	O
a	O
probabilistic	O
context	B
free	I
grammar	I
or	O
pcfg	B
is	O
a	O
set	O
of	O
re-write	O
or	O
production	B
rules	B
of	O
the	O
form	O
or	O
x	O
where	O
are	O
non-terminals	B
to	O
parts	O
of	O
speech	O
and	O
x	O
x	O
are	O
terminals	B
i	O
e	O
words	O
see	O
figure	O
for	O
an	O
example	O
each	O
such	O
rule	O
has	O
an	O
associated	O
probability	O
the	O
resulting	O
model	O
defines	O
a	O
probability	O
distribution	O
over	O
sequences	O
of	O
words	O
we	O
can	O
compute	O
the	O
probability	O
of	O
observing	O
a	O
particular	O
sequence	O
x	O
xt	O
by	O
summing	O
over	O
all	O
trees	O
that	O
generate	O
it	O
this	O
can	O
be	O
done	O
in	O
ot	O
time	O
using	O
the	O
inside-outside	B
algorithm	I
see	O
e	O
g	O
and	O
martin	O
manning	O
and	O
schuetze	O
for	O
details	O
pcfgs	O
are	O
generative	O
models	O
it	O
is	O
possible	O
to	O
make	O
discriminative	B
versions	O
which	O
encode	O
the	O
probability	O
of	O
a	O
labeled	O
tree	B
y	O
given	O
a	O
sequence	O
of	O
words	O
x	O
by	O
using	O
a	O
crf	B
of	O
the	O
form	O
pyx	O
expwt	O
y	O
for	O
example	O
we	O
might	O
define	O
y	O
to	O
count	O
the	O
number	O
of	O
times	O
each	O
production	O
rule	O
was	O
used	O
is	O
analogous	O
to	O
the	O
number	O
of	O
state	B
transitions	O
in	O
a	O
chain-structured	O
model	O
see	O
e	O
g	O
et	O
al	O
for	O
details	O
hierarchical	O
classification	O
suppose	O
we	O
are	O
performing	O
multi-class	O
classification	O
where	O
we	O
have	O
a	O
label	B
taxonomy	I
which	O
groups	O
the	O
classes	O
into	O
a	O
hierarchy	O
we	O
can	O
encode	O
the	O
position	O
of	O
y	O
within	O
this	O
hierarchy	O
by	O
defining	O
a	O
binary	O
vector	O
where	O
we	O
turn	O
on	O
the	O
bit	O
for	O
component	O
y	O
and	O
for	O
all	O
its	O
children	B
this	O
can	O
be	O
combined	O
with	O
input	O
features	B
using	O
a	O
tensor	B
product	I
y	O
see	O
figure	O
for	O
an	O
example	O
this	O
method	O
is	O
widely	O
used	O
for	O
text	O
classification	O
where	O
manually	O
constructed	O
taxnomies	O
as	O
the	O
open	B
directory	I
project	I
at	O
www	O
dmoz	O
org	O
are	O
quite	O
common	O
the	O
benefit	O
is	O
that	O
information	B
can	O
be	O
shared	B
between	O
the	O
parameters	O
for	O
nearby	O
categories	O
enabling	O
generalization	B
across	O
classes	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
figure	O
illustration	O
of	O
a	O
simple	O
label	B
taxonomy	I
and	O
how	O
it	O
can	O
be	O
used	O
to	O
compute	O
a	O
distributed	B
representation	I
for	O
the	O
label	B
for	O
class	O
in	O
this	O
figure	O
x	O
y	O
is	O
denoted	O
by	O
and	O
wt	O
y	O
is	O
denoted	O
by	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yasemin	O
altun	O
protein	O
side-chain	O
prediction	O
an	O
interesting	O
analog	O
to	O
the	O
skip-chain	O
model	O
arises	O
in	O
the	O
problem	O
of	O
predicting	O
the	O
structure	O
of	O
protein	O
side	B
chains	I
each	O
residue	O
in	O
the	O
side	O
chain	O
has	O
dihedral	O
angles	O
which	O
are	O
usually	O
discretized	O
into	O
values	O
called	O
rotamers	B
the	O
goal	O
is	O
to	O
predict	O
this	O
discrete	B
sequence	O
of	O
angles	O
y	O
from	O
the	O
discrete	B
sequence	O
of	O
amino	O
acids	O
x	O
we	O
can	O
define	O
an	O
energy	B
function	I
ex	O
y	O
where	O
we	O
include	O
various	O
pairwise	O
interaction	O
terms	O
between	O
nearby	O
residues	O
of	O
the	O
y	O
vector	O
this	O
energy	O
is	O
usually	O
defined	O
as	O
a	O
weighted	O
sum	O
of	O
individual	O
energy	O
terms	O
ex	O
yw	O
jejx	O
y	O
where	O
the	O
ej	O
are	O
energy	O
contribution	O
due	O
to	O
various	O
electrostatic	O
charges	O
hydrogen	O
bonding	O
potentials	O
etc	O
and	O
w	O
are	O
the	O
parameters	O
of	O
the	O
model	O
see	O
et	O
al	O
for	O
details	O
argmin	O
ex	O
yw	O
in	O
general	O
this	O
problem	O
is	O
np-hard	B
depending	O
on	O
the	O
nature	O
of	O
the	O
graph	B
induced	O
by	O
the	O
ej	O
terms	O
due	O
to	O
long-range	O
connections	O
between	O
the	O
variables	O
nevertheless	O
some	O
special	O
cases	O
can	O
be	O
efficiently	O
handled	O
using	O
methods	O
discussed	O
in	O
section	O
given	O
the	O
model	O
we	O
can	O
compute	O
the	O
most	O
probable	O
side	O
chain	O
configuration	O
using	O
y	O
stereo	O
vision	O
low-level	B
vision	I
problems	O
are	O
problems	O
where	O
the	O
input	O
is	O
an	O
image	O
set	O
of	O
images	O
and	O
the	O
output	O
is	O
a	O
processed	O
version	O
of	O
the	O
image	O
in	O
such	O
cases	O
it	O
is	O
common	O
to	O
use	O
latticestructured	O
models	O
the	O
models	O
are	O
similar	B
to	O
figure	O
except	O
that	O
the	O
features	B
can	O
be	O
global	O
and	O
are	O
not	O
generated	O
by	O
the	O
model	O
we	O
will	O
assume	O
a	O
pairwise	O
crf	B
a	O
classic	O
low-level	B
vision	I
problem	O
is	O
dense	B
stereo	I
reconstruction	I
where	O
the	O
goal	O
is	O
to	O
estimate	O
the	O
depth	O
of	O
every	O
pixel	O
given	O
two	O
images	O
taken	O
from	O
slightly	O
different	O
angles	O
in	O
this	O
section	O
on	O
and	O
freeman	O
we	O
give	O
a	O
sketch	O
of	O
how	O
a	O
simple	O
crf	B
can	O
be	O
used	O
to	O
solve	O
this	O
task	O
see	O
e	O
g	O
et	O
al	O
for	O
a	O
more	O
sophisticated	O
model	O
by	O
using	O
some	O
standard	O
preprocessing	O
techniques	O
one	O
can	O
convert	O
depth	O
estimation	O
into	O
a	O
conditional	O
random	O
fields	O
problem	O
of	O
estimating	O
the	O
disparity	B
ys	O
between	O
the	O
pixel	O
at	O
location	O
js	O
in	O
the	O
left	O
image	O
and	O
the	O
corresponding	O
pixel	O
at	O
location	O
ys	O
js	O
in	O
the	O
right	O
image	O
we	O
typically	O
assume	O
that	O
corresponding	O
pixels	O
have	O
similar	B
intensity	O
so	O
we	O
define	O
a	O
local	O
node	O
potential	O
of	O
the	O
form	O
sysx	O
exp	O
js	O
xris	O
ys	O
js	O
where	O
xl	O
is	O
the	O
left	O
image	O
and	O
xr	O
is	O
the	O
right	O
image	O
this	O
equation	O
can	O
be	O
generalized	O
to	O
model	O
the	O
intensity	O
of	O
small	O
windows	O
around	O
each	O
location	O
in	O
highly	O
textured	O
regions	O
it	O
is	O
usually	O
possible	O
to	O
find	O
the	O
corresponding	O
patch	O
using	O
cross	O
correlation	O
but	O
in	O
regions	O
of	O
low	O
texture	O
there	O
will	O
be	O
considerable	O
ambiguity	O
about	O
the	O
correct	O
value	O
of	O
ys	O
we	O
can	O
easily	O
add	O
a	O
gaussian	B
prior	O
on	O
the	O
edges	B
of	O
the	O
mrf	B
that	O
encodes	O
the	O
assumption	O
that	O
neighboring	O
disparities	O
ys	O
yt	O
should	O
be	O
similar	B
as	O
follows	O
stys	O
yt	O
exp	O
the	O
resulting	O
model	O
is	O
a	O
gaussian	B
crf	B
however	O
using	O
gaussian	B
edge-potentials	O
will	O
oversmooth	O
the	O
estimate	O
since	O
this	O
prior	O
fails	O
to	O
account	O
for	O
the	O
occasional	O
large	O
changes	O
in	O
disparity	B
that	O
occur	O
between	O
neighboring	O
pixels	O
which	O
are	O
on	O
different	O
sides	O
of	O
an	O
occlusion	O
boundary	O
one	O
gets	O
much	O
better	O
results	O
using	O
a	O
truncated	B
gaussian	B
potential	I
of	O
the	O
form	O
stys	O
yt	O
exp	O
min	O
where	O
encodes	O
the	O
expected	O
smoothness	O
and	O
encodes	O
the	O
maximum	O
penalty	O
that	O
will	O
be	O
imposed	O
if	O
disparities	O
are	O
significantly	O
different	O
this	O
is	O
called	O
a	O
discontinuity	B
preserving	I
potential	O
note	O
that	O
such	O
penalties	O
are	O
not	O
convex	B
the	O
local	B
evidence	B
potential	O
can	O
be	O
made	O
robust	B
in	O
a	O
similar	B
way	O
in	O
order	O
to	O
handle	O
outliers	B
due	O
to	O
specularities	O
occlusions	O
etc	O
figure	O
illustrates	O
the	O
difference	O
between	O
these	O
two	O
forms	O
of	O
prior	O
on	O
the	O
top	O
left	O
is	O
an	O
image	O
from	O
the	O
standard	O
middlebury	O
stereo	O
benchmark	O
dataset	O
and	O
szeliski	O
on	O
the	O
bottom	O
left	O
is	O
the	O
corresponding	O
true	O
disparity	B
values	O
the	O
remaining	O
columns	O
represent	O
the	O
estimated	O
disparity	B
after	O
and	O
an	O
infinite	O
number	O
of	O
rounds	O
of	O
loopy	B
belief	B
propagation	I
section	O
where	O
by	O
infinite	O
we	O
mean	B
the	O
results	O
at	O
convergence	O
the	O
top	O
row	O
shows	O
the	O
results	O
using	O
a	O
gaussian	B
edge	O
potential	O
and	O
the	O
bottom	O
row	O
shows	O
the	O
results	O
using	O
the	O
truncated	O
potential	O
the	O
latter	O
is	O
clearly	O
better	O
unfortunately	O
performing	O
inference	B
with	O
real-valued	O
variables	O
is	O
computationally	O
difficult	O
unless	O
the	O
model	O
is	O
jointly	O
gaussian	B
consequently	O
it	O
is	O
common	O
to	O
discretize	B
the	O
variables	O
example	O
figure	O
used	O
states	O
the	O
edge	O
potentials	O
still	O
have	O
the	O
form	O
given	O
in	O
equation	O
the	O
resulting	O
model	O
is	O
called	O
a	O
metric	B
crf	B
since	O
the	O
potentials	O
form	O
a	O
metric	B
inference	B
in	O
metric	B
crfs	O
is	O
more	O
efficient	O
than	O
in	O
crfs	O
where	O
the	O
discrete	B
labels	O
have	O
no	O
natural	O
ordering	O
as	O
we	O
explain	O
in	O
section	O
see	O
section	O
for	O
a	O
comparison	O
of	O
various	O
approximate	B
inference	B
methods	O
applied	O
to	O
low-level	O
crfs	O
and	O
see	O
et	O
al	O
prince	O
for	O
more	O
details	O
on	O
probabilistic	O
models	O
for	O
computer	O
vision	O
a	O
function	O
f	O
is	O
said	O
to	O
be	O
a	O
metric	B
if	O
it	O
satisfies	O
the	O
following	O
three	O
properties	O
reflexivity	O
f	O
b	O
iff	B
a	O
b	O
symmetry	O
f	O
b	O
a	O
and	O
triangle	B
inequality	I
f	O
b	O
f	O
c	O
f	O
c	O
if	O
f	O
satisfies	O
only	O
the	O
first	O
two	O
properties	O
it	O
is	O
called	O
a	O
semi-metric	B
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
figure	O
illustration	O
of	O
belief	B
propagation	I
for	O
stereo	O
depth	O
estimation	O
left	O
column	O
image	O
and	O
true	O
disparities	O
remaining	O
columns	O
initial	O
estimate	O
estimate	O
after	O
iteration	O
and	O
estimate	O
at	O
convergence	O
top	O
row	O
gaussian	B
edge	O
potentials	O
bottom	O
row	O
robust	B
edge	O
potentials	O
source	O
figure	O
of	O
and	O
freeman	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
crf	B
training	O
we	O
can	O
modify	O
the	O
gradient	O
based	O
optimization	B
of	O
mrfs	O
described	O
in	O
section	O
to	O
the	O
crf	B
case	O
in	O
a	O
straightforward	O
way	O
in	O
particular	O
the	O
scaled	O
log-likelihood	O
becomes	O
and	O
the	O
gradient	O
becomes	O
i	O
n	O
wc	O
n	O
n	O
i	O
i	O
c	O
n	O
log	O
pyixi	O
w	O
i	O
cyi	O
xi	O
wc	O
cyi	O
xi	O
e	O
cy	O
xi	O
c	O
cyi	O
xi	O
log	O
zw	O
xi	O
wt	O
log	O
zw	O
xi	O
note	O
that	O
we	O
now	O
have	O
to	O
perform	O
inference	B
for	O
every	O
single	O
training	O
case	O
inside	O
each	O
gradient	O
step	O
which	O
is	O
on	O
times	O
slower	O
than	O
the	O
mrf	B
case	O
this	O
is	O
because	O
the	O
partition	B
function	I
depends	O
on	O
the	O
inputs	O
xi	O
in	O
most	O
applications	O
of	O
crfs	O
some	O
applications	O
of	O
mrfs	O
the	O
size	O
of	O
the	O
graph	B
structure	O
can	O
vary	O
hence	O
we	O
need	O
to	O
use	O
parameter	B
tying	I
to	O
ensure	O
we	O
can	O
define	O
a	O
distribution	O
of	O
arbitrary	O
size	O
in	O
the	O
pairwise	O
case	O
we	O
can	O
write	O
the	O
model	O
as	O
follows	O
pyx	O
w	O
zw	O
x	O
exp	O
wt	O
x	O
structural	O
svms	O
where	O
w	O
we	O
are	O
the	O
node	O
and	O
edge	O
parameters	O
and	O
x	O
tyt	O
x	O
stys	O
yt	O
x	O
s	O
t	O
are	O
the	O
summed	O
node	O
and	O
edge	O
features	B
are	O
the	O
sufficient	B
statistics	I
the	O
gradient	O
expression	O
is	O
easily	O
modified	O
to	O
handle	O
this	O
case	O
in	O
practice	O
it	O
is	O
important	O
to	O
use	O
a	O
prior	O
regularization	B
to	O
prevent	O
overfitting	O
if	O
we	O
use	O
a	O
t	O
i	O
i	O
gaussian	B
prior	O
the	O
new	O
objective	O
becomes	O
n	O
log	O
pyixi	O
w	O
it	O
is	O
simple	O
to	O
modify	O
the	O
gradient	O
expression	O
alternatively	O
we	O
can	O
use	O
regularization	B
for	O
example	O
we	O
could	O
use	O
for	O
the	O
edge	O
weights	O
we	O
to	O
learn	O
a	O
sparse	B
graph	B
structure	O
and	O
for	O
the	O
node	O
weights	O
wn	O
as	O
in	O
et	O
al	O
in	O
other	O
words	O
the	O
objective	O
becomes	O
n	O
log	O
pyixi	O
w	O
unfortunately	O
the	O
optimization	B
algorithms	O
are	O
more	O
complicated	O
when	O
we	O
use	O
section	O
although	O
the	O
problem	O
is	O
still	O
convex	B
to	O
handle	O
large	O
datasets	O
we	O
can	O
use	O
stochastic	B
gradient	B
descent	I
as	O
described	O
in	O
section	O
it	O
is	O
possible	O
useful	O
to	O
define	O
crfs	O
with	O
hidden	B
variables	I
for	O
example	O
to	O
allow	O
for	O
an	O
unknown	B
alignment	B
between	O
the	O
visible	B
features	B
and	O
the	O
hidden	B
labels	O
e	O
g	O
et	O
al	O
in	O
this	O
case	O
the	O
objective	O
function	O
is	O
no	O
longer	O
convex	B
nevertheless	O
we	O
can	O
find	O
a	O
locally	O
optimal	O
ml	O
or	O
map	O
parameter	B
estimate	O
using	O
em	B
and	O
or	O
gradient	O
methods	O
structural	O
svms	O
we	O
have	O
seen	O
that	O
training	O
a	O
crf	B
requires	O
inference	B
in	O
order	O
to	O
compute	O
the	O
expected	B
sufficient	B
statistics	I
needed	O
to	O
evaluate	O
the	O
gradient	O
for	O
certain	O
models	O
computing	O
a	O
joint	O
map	B
estimate	I
of	O
the	O
states	O
is	O
provably	O
simpler	O
than	O
computing	O
marginals	O
as	O
we	O
discuss	O
in	O
section	O
in	O
this	O
section	O
we	O
discuss	O
a	O
way	O
to	O
train	O
structured	B
output	I
classifiers	O
that	O
that	O
leverages	O
the	O
existence	O
of	O
fast	O
map	O
solvers	O
avoid	O
confusion	O
with	O
map	O
estimation	O
of	O
parameters	O
we	O
will	O
often	O
refer	O
to	O
map	O
estimation	O
of	O
states	O
as	O
decoding	B
these	O
methods	O
are	O
known	O
as	O
structural	B
support	B
vector	I
machines	I
or	O
ssvms	B
et	O
al	O
is	O
also	O
a	O
very	O
similar	B
class	O
of	O
methods	O
known	O
as	O
max	B
margin	B
markov	B
networks	I
or	O
et	O
al	O
see	O
section	O
for	O
a	O
discussion	O
of	O
the	O
differences	O
ssvms	B
a	O
probabilistic	O
view	O
in	O
this	O
book	O
we	O
have	O
mostly	O
concentrated	O
on	O
fitting	O
models	O
using	O
map	O
parameter	B
estimation	O
i	O
e	O
by	O
minimizing	O
functions	O
of	O
the	O
form	O
log	O
pyixi	O
w	O
rm	O
ap	O
log	O
pw	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
however	O
at	O
test	O
time	O
we	O
pick	O
the	O
label	B
so	O
as	O
to	O
minimize	O
the	O
posterior	B
expected	I
loss	B
in	O
section	O
yxw	O
argmin	O
l	O
y	O
ypyx	O
w	O
y	O
y	O
where	O
ly	O
y	O
is	O
the	O
loss	B
we	O
incur	O
when	O
we	O
estimate	O
y	O
but	O
the	O
truth	O
is	O
y	O
it	O
therefore	O
seems	O
reasonable	O
to	O
take	O
the	O
loss	B
function	I
into	O
account	O
when	O
performing	O
parameter	B
so	O
following	O
and	O
he	O
let	O
us	O
instead	O
minimized	O
the	O
posterior	B
expected	I
loss	B
on	O
the	O
training	B
set	I
relw	O
log	O
pw	O
log	O
lyi	O
ypyxi	O
w	O
in	O
the	O
special	O
case	O
of	O
loss	B
lyi	O
y	O
yyi	O
this	O
reduces	O
to	O
rm	O
ap	O
y	O
we	O
will	O
assume	O
that	O
we	O
can	O
write	O
our	O
model	O
in	O
the	O
following	O
form	O
pyx	O
w	O
expwt	O
y	O
zx	O
w	O
exp	O
ew	O
z	O
pw	O
where	O
zx	O
w	O
this	O
we	O
can	O
rewrite	O
our	O
objective	O
as	O
follows	O
y	O
expwt	O
y	O
also	O
let	O
us	O
define	O
lyi	O
y	O
exp	O
lyi	O
y	O
with	O
relw	O
log	O
pw	O
ew	O
log	O
i	O
y	O
exp	O
lyi	O
y	O
expwt	O
y	O
zx	O
w	O
lvyi	O
y	O
t	O
y	O
i	O
log	O
zxi	O
w	O
log	O
exp	O
y	O
y	O
y	O
we	O
will	O
now	O
consider	O
various	O
bounds	O
in	O
order	O
to	O
simplify	O
this	O
objective	O
first	O
note	O
that	O
for	O
any	O
function	O
f	O
we	O
have	O
y	O
y	O
f	O
log	O
max	O
expf	O
log	O
exp	O
max	O
y	O
f	O
log	O
max	O
y	O
f	O
for	O
example	O
suppose	O
y	O
and	O
f	O
y	O
then	O
we	O
have	O
we	O
can	O
ignore	O
the	O
log	O
term	O
which	O
is	O
independent	O
of	O
y	O
and	O
treat	O
maxy	O
y	O
f	O
as	O
both	O
a	O
lower	O
and	O
upper	O
bound	O
hence	O
we	O
see	O
that	O
relw	O
ew	O
lyi	O
y	O
t	O
y	O
wt	O
y	O
max	O
y	O
max	O
y	O
note	O
that	O
this	O
violates	O
the	O
fundamental	O
bayesian	B
distinction	O
between	O
inference	B
and	O
decision	B
making	O
however	O
performing	O
these	O
tasks	O
separately	O
will	O
only	O
result	O
in	O
an	O
optimal	O
decision	B
if	O
we	O
can	O
compute	O
the	O
exact	O
posterior	O
in	O
most	O
cases	O
this	O
is	O
intractable	O
so	O
we	O
need	O
to	O
perform	O
loss-calibrated	B
inference	B
et	O
al	O
in	O
this	O
section	O
we	O
just	O
perform	O
loss-calibrated	O
map	O
parameter	B
estimation	O
which	O
is	O
computationally	O
simpler	O
also	O
et	O
al	O
structural	O
svms	O
where	O
x	O
y	O
means	O
x	O
y	O
for	O
some	O
constants	O
unfortunately	O
this	O
objective	O
is	O
not	O
convex	B
in	O
w	O
however	O
we	O
can	O
devise	O
a	O
convex	B
upper	O
bound	O
by	O
exploiting	O
the	O
following	O
looser	O
lower	O
bound	O
on	O
the	O
log-sum-exp	B
function	O
expf	O
f	O
log	O
y	O
for	O
any	O
y	O
applying	O
this	O
equation	O
to	O
our	O
earlier	O
example	O
for	O
f	O
y	O
and	O
and	O
applying	O
this	O
bound	O
to	O
rel	O
we	O
get	O
we	O
get	O
lyi	O
y	O
t	O
y	O
wt	O
yi	O
relw	O
ew	O
max	O
y	O
if	O
we	O
set	O
ew	O
to	O
a	O
spherical	B
gaussian	B
prior	O
we	O
get	O
rssv	O
m	O
c	O
lyi	O
y	O
t	O
y	O
wt	O
yi	O
max	O
y	O
this	O
is	O
the	O
same	O
objective	O
as	O
used	O
in	O
the	O
ssvm	O
approach	O
of	O
et	O
al	O
in	O
the	O
special	O
case	O
that	O
y	O
ly	O
y	O
yy	O
and	O
y	O
criterion	O
reduces	O
to	O
the	O
following	O
considering	O
the	O
two	O
cases	O
that	O
y	O
yi	O
and	O
y	O
yi	O
yx	O
this	O
rsv	O
m	O
c	O
yiwt	O
which	O
is	O
the	O
standard	O
binary	O
svm	B
objective	O
equation	O
so	O
we	O
see	O
that	O
the	O
ssvm	O
criterion	O
can	O
be	O
seen	O
as	O
optimizing	O
an	O
upper	O
bound	O
on	O
the	O
bayesian	B
objective	O
a	O
result	O
first	O
shown	O
in	O
and	O
he	O
this	O
bound	O
will	O
be	O
tight	O
hence	O
the	O
approximation	O
will	O
be	O
a	O
good	O
one	O
when	O
is	O
large	O
since	O
in	O
that	O
case	O
pyx	O
w	O
will	O
concentrate	O
its	O
mass	O
on	O
argmaxy	O
pyx	O
w	O
unfortunately	O
a	O
large	O
corresponds	O
to	O
a	O
model	O
that	O
is	O
likely	O
to	O
overfit	O
so	O
it	O
is	O
unlikely	O
that	O
we	O
will	O
be	O
working	O
in	O
this	O
regime	O
we	O
will	O
tune	O
the	O
strength	O
of	O
the	O
regularizer	O
to	O
avoid	O
this	O
situation	O
an	O
alternative	O
justification	O
for	O
the	O
svm	B
criterion	O
is	O
that	O
it	O
focusses	O
effort	O
on	O
fitting	O
parameters	O
that	O
affect	O
the	O
decision	B
boundary	I
this	O
is	O
a	O
better	O
use	O
of	O
computational	O
resources	O
than	O
fitting	O
the	O
full	B
distribution	O
especially	O
when	O
the	O
model	O
is	O
wrong	O
ssvms	B
a	O
non-probabilistic	O
view	O
we	O
now	O
present	O
ssvms	B
in	O
a	O
more	O
traditional	O
way	O
following	O
et	O
al	O
the	O
resulting	O
objective	O
will	O
be	O
the	O
same	O
as	O
the	O
one	O
above	O
however	O
this	O
derivation	O
will	O
set	O
the	O
stage	O
for	O
the	O
algorithms	O
we	O
discuss	O
below	O
let	O
f	O
w	O
argmaxy	O
y	O
wt	O
y	O
be	O
the	O
prediction	O
function	O
we	O
can	O
obtain	O
zero	O
loss	B
on	O
the	O
training	B
set	I
using	O
this	O
predictor	O
if	O
wt	O
y	O
wt	O
yi	O
i	O
max	O
y	O
yyi	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
each	O
one	O
of	O
these	O
nonlinear	O
inequalities	O
can	O
be	O
equivalently	O
replaced	O
by	O
linear	O
inequalities	O
resulting	O
in	O
a	O
total	O
of	O
ny	O
n	O
linear	O
constraints	O
of	O
the	O
following	O
form	O
i	O
y	O
y	O
yi	O
wt	O
yi	O
wt	O
y	O
for	O
brevity	O
we	O
introduce	O
the	O
notation	O
iy	O
yi	O
y	O
so	O
we	O
can	O
rewrite	O
these	O
constraints	O
as	O
wt	O
iy	O
if	O
we	O
can	O
achieve	O
zero	O
loss	B
there	O
will	O
typically	O
be	O
multiple	O
solution	O
vectors	O
w	O
we	O
pick	O
the	O
one	O
that	O
maximizes	O
the	O
margin	B
defined	O
as	O
f	O
f	O
yi	O
w	O
max	O
yy	O
min	O
i	O
w	O
since	O
the	O
margin	B
can	O
be	O
made	O
arbitrarily	O
large	O
by	O
rescaling	O
w	O
we	O
fix	O
its	O
norm	O
to	O
be	O
resulting	O
in	O
the	O
optimization	B
problem	O
max	O
s	O
t	O
i	O
y	O
y	O
yi	O
wt	O
iy	O
equivalently	O
we	O
can	O
write	O
min	O
w	O
s	O
t	O
i	O
y	O
y	O
yi	O
wt	O
iy	O
to	O
allow	O
for	O
the	O
case	O
where	O
zero	O
loss	B
cannot	O
be	O
achieved	O
to	O
the	O
data	O
being	O
inseparable	O
in	O
the	O
case	O
of	O
binary	O
classification	O
we	O
relax	O
the	O
constraints	O
by	O
introducing	O
slack	O
terms	O
i	O
one	O
per	O
data	O
case	O
this	O
yields	O
c	O
min	O
w	O
s	O
t	O
i	O
y	O
y	O
yi	O
wt	O
iy	O
i	O
i	O
i	O
in	O
the	O
case	O
of	O
structured	O
outputs	O
we	O
don	O
t	O
want	O
to	O
treat	O
all	O
constraint	O
violations	O
equally	O
for	O
example	O
in	O
a	O
segmentation	O
problem	O
getting	O
one	O
position	O
wrong	O
should	O
be	O
punished	O
less	O
than	O
getting	O
many	O
positions	O
wrong	O
one	O
way	O
to	O
achieve	O
this	O
is	O
to	O
divide	O
the	O
slack	O
variable	O
by	O
the	O
size	O
of	O
the	O
loss	B
is	O
called	O
slack	B
re-scaling	I
this	O
yields	O
c	O
min	O
w	O
s	O
t	O
i	O
y	O
y	O
yi	O
wt	O
iy	O
i	O
i	O
lyi	O
y	O
i	O
alternatively	O
we	O
can	O
define	O
the	O
margin	B
to	O
be	O
proportional	O
to	O
the	O
loss	B
is	O
called	O
margin	B
re-rescaling	O
this	O
yields	O
c	O
min	O
w	O
i	O
s	O
t	O
i	O
y	O
y	O
yi	O
wt	O
iy	O
lyi	O
y	O
i	O
i	O
fact	O
we	O
can	O
write	O
y	O
y	O
instead	O
of	O
y	O
y	O
yi	O
since	O
if	O
y	O
yi	O
then	O
wt	O
iy	O
and	O
i	O
by	O
using	O
the	O
simpler	O
notation	O
which	O
doesn	O
t	O
exclude	O
yi	O
we	O
add	O
an	O
extra	O
but	O
redundant	O
constraint	O
this	O
latter	O
approach	O
is	O
used	O
in	O
structural	O
svms	O
for	O
future	O
reference	O
note	O
that	O
we	O
can	O
solve	O
for	O
the	O
i	O
max	O
y	O
wt	O
i	O
max	O
y	O
y	O
lyi	O
y	O
t	O
y	O
c	O
min	O
w	O
max	O
y	O
i	O
i	O
terms	O
as	O
follows	O
y	O
wt	O
i	O
wt	O
yi	O
substituting	O
in	O
and	O
dropping	O
the	O
constraints	O
we	O
get	O
the	O
following	O
equivalent	O
problem	O
empirical	B
risk	B
minimization	I
let	O
us	O
pause	O
and	O
consider	O
whether	O
the	O
above	O
objective	O
is	O
reasonable	O
recall	B
that	O
in	O
the	O
frequentist	B
approach	O
to	O
machine	B
learning	B
the	O
goal	O
is	O
to	O
minimize	O
the	O
regularized	O
empirical	B
risk	B
defined	O
by	O
rw	O
c	O
n	O
lyi	O
f	O
w	O
where	O
rw	O
is	O
the	O
regularizer	O
and	O
f	O
w	O
argmaxy	O
wt	O
y	O
yi	O
is	O
the	O
prediction	O
since	O
this	O
objective	O
is	O
hard	O
to	O
optimize	O
because	O
the	O
loss	B
is	O
not	O
differentiable	O
we	O
will	O
construct	O
a	O
convex	B
upper	O
bound	O
instead	O
i	O
we	O
can	O
show	O
that	O
rw	O
c	O
n	O
max	O
y	O
y	O
wt	O
i	O
is	O
such	O
a	O
convex	B
upper	O
bound	O
to	O
see	O
this	O
note	O
that	O
lyi	O
f	O
w	O
lyi	O
f	O
w	O
wt	O
yi	O
t	O
yi	O
max	O
using	O
this	O
bound	O
and	O
rw	O
lyi	O
y	O
wt	O
yi	O
t	O
y	O
yields	O
equation	O
y	O
computational	O
issues	O
although	O
the	O
above	O
objectives	O
are	O
simple	O
quadratic	O
programs	O
they	O
have	O
ony	O
constraints	O
this	O
is	O
intractable	O
since	O
y	O
is	O
usually	O
exponentially	O
large	O
in	O
the	O
case	O
of	O
the	O
margin	B
rescaling	O
formulation	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
exponential	O
number	O
of	O
constraints	O
to	O
a	O
polynomial	O
number	O
provided	O
the	O
loss	B
function	I
and	O
the	O
feature	O
vector	O
decompose	O
according	O
to	O
a	O
graphical	B
model	I
this	O
is	O
the	O
approach	O
used	O
in	O
et	O
al	O
an	O
alternative	O
approach	O
is	O
to	O
work	O
directly	O
with	O
the	O
exponentially	O
sized	O
qp	B
this	O
allows	O
for	O
the	O
use	O
of	O
more	O
general	O
loss	B
functions	O
there	O
are	O
several	O
possible	O
methods	O
to	O
make	O
this	O
feasible	O
one	O
is	O
to	O
use	O
cutting	B
plane	I
methods	O
another	O
is	O
to	O
use	O
stochastic	O
subgradient	B
methods	O
we	O
discuss	O
both	O
of	O
these	O
below	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
fi	O
l	O
l	O
c	O
i	O
illustration	O
of	O
the	O
cutting	B
plane	I
algorithm	O
in	O
we	O
start	O
with	O
the	O
estimate	O
w	O
figure	O
we	O
add	O
the	O
first	O
constraint	O
the	O
shaded	O
region	O
is	O
the	O
new	O
feasible	O
set	O
the	O
new	O
minimum	O
norm	O
solution	O
is	O
we	O
add	O
a	O
third	O
constraint	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yasemin	O
altun	O
we	O
add	O
another	O
constraint	O
the	O
dark	O
shaded	O
region	O
is	O
the	O
new	O
feasible	O
set	O
f	O
h	O
h	O
s	O
l	O
h	O
l	O
cutting	B
plane	I
methods	O
for	O
fitting	O
ssvms	B
in	O
this	O
section	O
we	O
discuss	O
an	O
efficient	O
algorithm	O
for	O
fitting	O
ssvms	B
due	O
to	O
et	O
al	O
this	O
method	O
can	O
handle	O
general	O
loss	B
functions	O
and	O
is	O
implemented	O
in	O
the	O
popular	O
svmstruct	B
the	O
method	O
is	O
based	O
on	O
the	O
cutting	B
plane	I
method	O
from	O
convex	B
optimization	B
the	O
basic	O
idea	O
is	O
as	O
follows	O
we	O
start	O
with	O
an	O
initial	O
guess	O
w	O
and	O
no	O
constraints	O
at	O
each	O
iteration	O
we	O
then	O
do	O
the	O
following	O
for	O
each	O
example	O
i	O
we	O
find	O
the	O
most	O
violated	O
constraint	O
involving	O
xi	O
and	O
yi	O
if	O
the	O
loss-augmented	O
margin	B
violation	O
exceeds	O
the	O
current	O
value	O
of	O
i	O
by	O
more	O
than	O
we	O
add	O
yi	O
to	O
the	O
working	O
set	O
of	O
constraints	O
for	O
this	O
training	O
case	O
wi	O
and	O
then	O
solve	O
the	O
resulting	O
new	O
qp	B
to	O
find	O
the	O
new	O
w	O
see	O
figure	O
for	O
a	O
sketch	O
and	O
algorithm	O
for	O
the	O
pseudo	O
code	O
at	O
each	O
step	O
we	O
only	O
add	O
one	O
new	O
constraint	O
we	O
can	O
warm-start	O
the	O
qp	B
solver	O
we	O
can	O
can	O
easily	O
modify	O
the	O
algorithm	O
to	O
optimize	O
the	O
slack	O
rescaling	O
version	O
by	O
replacing	O
the	O
expression	O
lyi	O
y	O
wt	O
i	O
yi	O
with	O
lyi	O
wt	O
i	O
yi	O
the	O
key	O
to	O
the	O
efficiency	O
of	O
this	O
method	O
is	O
that	O
only	O
polynomially	O
many	O
constraints	O
need	O
to	O
be	O
added	O
and	O
as	O
soon	O
as	O
they	O
are	O
the	O
exponential	O
number	O
of	O
other	O
constraints	O
are	O
guaranteed	O
to	O
also	O
be	O
satisfied	O
to	O
within	O
a	O
tolerance	O
of	O
et	O
al	O
for	O
the	O
proof	O
loss-augmented	B
decoding	B
the	O
other	O
key	O
to	O
efficiency	O
is	O
the	O
ability	O
to	O
find	O
the	O
most	O
violated	O
constraint	O
in	O
line	O
of	O
the	O
algorithm	O
i	O
e	O
to	O
compute	O
argmax	O
y	O
y	O
lyi	O
y	O
wt	O
iy	O
argmax	O
y	O
y	O
lyi	O
y	O
wt	O
y	O
httpsvmlight	O
joachims	O
orgsvm	O
struct	O
html	O
structural	O
svms	O
algorithm	O
cutting	B
plane	I
algorithm	O
for	O
ssvms	B
rescaling	O
n	O
version	O
input	O
d	O
yn	O
c	O
wi	O
i	O
for	O
i	O
n	O
repeat	O
for	O
i	O
n	O
do	O
yi	O
argmax	O
yi	O
y	O
lyi	O
y	O
wt	O
i	O
yi	O
if	O
lyi	O
y	O
wt	O
i	O
yi	O
i	O
then	O
wi	O
wi	O
yi	O
argminw	O
c	O
i	O
s	O
t	O
i	O
wi	O
wt	O
i	O
yi	O
lyi	O
i	O
until	O
no	O
wi	O
has	O
changed	O
return	O
we	O
call	O
this	O
process	O
loss-augmented	B
decoding	B
et	O
al	O
this	O
procedure	O
is	O
called	O
the	O
separation	B
oracle	I
if	O
the	O
loss	B
function	I
has	O
an	O
additive	O
decomposition	O
of	O
the	O
same	O
form	O
as	O
the	O
features	B
then	O
we	O
can	O
fold	O
the	O
loss	B
into	O
the	O
weight	B
vector	I
i	O
e	O
we	O
can	O
find	O
a	O
new	O
set	O
of	O
parameters	O
iy	O
wt	O
iy	O
we	O
can	O
then	O
use	O
a	O
standard	O
decoding	B
algorithm	O
such	O
as	O
viterbi	B
on	O
the	O
model	O
pyx	O
with	O
a	O
value	O
of	O
of	O
wt	O
i	O
y	O
or	O
it	O
will	O
be	O
the	O
second	O
best	O
solution	O
i	O
e	O
in	O
the	O
special	O
case	O
of	O
loss	B
the	O
optimum	O
will	O
either	O
be	O
the	O
best	O
solution	O
argmaxy	O
wt	O
y	O
such	O
that	O
wt	O
y	O
y	O
argmax	O
y	O
which	O
achieves	O
an	O
overall	O
value	O
of	O
wt	O
i	O
y	O
for	O
chain	O
structured	O
crfs	O
we	O
can	O
use	O
the	O
viterbi	B
algorithm	O
to	O
do	O
decoding	B
the	O
second	O
best	O
path	B
will	O
differ	O
from	O
the	O
best	O
path	B
in	O
a	O
single	O
position	O
which	O
can	O
be	O
obtained	O
by	O
changing	O
the	O
variable	O
whose	O
max	O
marginal	O
is	O
closest	O
to	O
its	O
decision	B
boundary	I
to	O
its	O
second	O
best	O
value	O
we	O
can	O
generalize	B
this	O
a	O
bit	O
more	O
work	O
to	O
find	O
the	O
n	O
list	O
and	O
chow	O
nilsson	O
and	O
goldberger	O
t	O
yt	O
and	O
for	O
the	O
score	O
in	O
section	O
we	O
can	O
devise	O
a	O
dynamic	B
programming	I
algorithm	O
to	O
compute	O
equation	O
see	O
et	O
al	O
for	O
details	O
other	O
models	O
and	O
loss	B
function	I
combinations	O
will	O
require	O
different	O
methods	O
for	O
hamming	O
loss	B
ly	O
y	O
t	O
iy	O
a	O
linear	O
time	O
algorithm	O
although	O
the	O
above	O
algorithm	O
takes	O
polynomial	O
time	O
we	O
can	O
do	O
better	O
and	O
devise	O
an	O
algorithm	O
that	O
runs	O
in	O
linear	O
time	O
assuming	O
we	O
use	O
a	O
linear	B
kernel	B
we	O
work	O
with	O
the	O
original	O
features	B
y	O
and	O
do	O
not	O
apply	O
the	O
kernel	B
trick	I
the	O
basic	O
idea	O
as	O
explained	O
in	O
et	O
al	O
is	O
to	O
have	O
a	O
single	O
slack	O
variable	O
instead	O
of	O
n	O
but	O
to	O
use	O
constraints	O
instead	O
of	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
just	O
ny	O
specifically	O
we	O
optimize	O
the	O
following	O
the	O
margin	B
rescaling	O
formulation	O
min	O
w	O
c	O
s	O
t	O
yn	O
y	O
n	O
n	O
wt	O
iyi	O
n	O
lyi	O
yi	O
compare	O
this	O
to	O
the	O
original	O
version	O
which	O
was	O
min	O
w	O
c	O
n	O
s	O
t	O
i	O
y	O
y	O
wt	O
iy	O
lyi	O
yi	O
i	O
one	O
can	O
show	O
that	O
any	O
solution	O
w	O
vice	O
versa	O
with	O
n	O
i	O
of	O
equation	O
is	O
also	O
a	O
solution	O
of	O
equation	O
and	O
argminw	O
algorithm	O
cutting	B
plane	I
algorithm	O
for	O
ssvms	B
rescaling	O
version	O
input	O
d	O
yn	O
c	O
w	O
repeat	O
c	O
s	O
t	O
yn	O
w	O
for	O
i	O
n	O
do	O
w	O
w	O
yn	O
i	O
yi	O
lyi	O
yi	O
n	O
wt	O
iyi	O
lyi	O
yi	O
yi	O
argmax	O
yi	O
y	O
lyi	O
yi	O
t	O
yi	O
n	O
wt	O
n	O
until	O
return	O
n	O
we	O
can	O
optimize	O
equation	O
using	O
the	O
cutting	B
plane	I
algorithm	O
in	O
algorithm	O
is	O
what	O
is	O
implemented	O
in	O
svmstruct	B
the	O
inner	O
qp	B
in	O
line	O
can	O
be	O
solved	O
in	O
on	O
time	O
using	O
the	O
method	O
of	O
in	O
line	O
we	O
make	O
n	O
calls	O
to	O
the	O
loss-augmented	O
decoder	O
finally	O
it	O
can	O
be	O
shown	O
that	O
the	O
number	O
of	O
iterations	O
is	O
a	O
constant	O
independent	O
on	O
n	O
thus	O
the	O
overall	O
running	O
time	O
is	O
linear	O
online	O
algorithms	O
for	O
fitting	O
ssvms	B
although	O
the	O
cutting	B
plane	I
algorithm	O
can	O
be	O
made	O
to	O
run	O
in	O
time	O
linear	O
in	O
the	O
number	O
of	O
data	O
points	O
that	O
can	O
still	O
be	O
slow	O
if	O
we	O
have	O
a	O
large	O
dataset	O
in	O
such	O
cases	O
it	O
is	O
preferable	O
to	O
use	O
online	B
learning	B
we	O
briefly	O
mention	O
a	O
few	O
possible	O
algorithms	O
below	O
the	O
structured	B
perceptron	B
algorithm	I
a	O
very	O
simple	O
algorithm	O
for	O
fitting	O
ssvms	B
is	O
the	O
structured	B
perceptron	B
algorithm	I
this	O
method	O
is	O
an	O
extension	B
of	O
the	O
regular	B
perceptron	B
algorithm	I
of	O
section	O
at	O
each	O
structural	O
svms	O
step	O
we	O
compute	O
y	O
argmax	O
pyx	O
using	O
the	O
viterbi	B
algorithm	O
for	O
the	O
current	O
training	O
sample	O
x	O
if	O
y	O
y	O
we	O
do	O
nothing	O
otherwise	O
we	O
update	O
the	O
weight	B
vector	I
using	O
wk	O
x	O
y	O
x	O
to	O
get	O
good	O
performance	O
it	O
is	O
necessary	O
to	O
average	O
the	O
parameters	O
over	O
the	O
last	O
few	O
updates	O
section	O
for	O
details	O
rather	O
than	O
using	O
the	O
most	O
recent	O
value	O
stochastic	O
subgradient	B
descent	O
the	O
disadvantage	O
of	O
the	O
structured	B
perceptron	B
algorithm	I
is	O
that	O
it	O
implicitly	O
assumes	O
loss	B
and	O
it	O
does	O
not	O
enforce	O
any	O
kind	O
of	O
margin	B
an	O
alternative	O
approach	O
is	O
to	O
perform	O
stochastic	O
subgradient	B
descent	O
a	O
specific	O
instance	O
of	O
this	O
the	O
pegasos	B
algorithm	O
et	O
al	O
which	O
stands	O
for	O
primal	O
estimated	O
sub-gradient	O
solver	O
for	O
svm	B
pegasos	B
was	O
designed	O
for	O
binary	O
svms	O
but	O
can	O
be	O
extended	O
to	O
ssvms	B
let	O
us	O
start	O
by	O
considering	O
the	O
objective	O
function	O
lyi	O
yi	O
t	O
yi	O
wt	O
yi	O
f	O
max	O
yi	O
letting	O
yi	O
be	O
the	O
argmax	O
of	O
this	O
max	O
then	O
the	O
subgradient	B
of	O
this	O
objective	O
function	O
is	O
gw	O
yi	O
yi	O
w	O
in	O
stochastic	O
subgradient	B
descent	O
we	O
approximate	O
this	O
gradient	O
with	O
a	O
single	O
term	O
i	O
and	O
then	O
perform	O
an	O
update	O
wk	O
kgiwk	O
wk	O
k	O
yi	O
yi	O
w	O
where	O
k	O
is	O
the	O
step	B
size	I
parameter	B
which	O
should	O
satisfy	O
the	O
robbins-monro	B
conditions	O
that	O
the	O
perceptron	B
algorithm	I
is	O
just	O
a	O
special	O
case	O
where	O
and	O
tion	O
k	O
to	O
ensure	O
that	O
w	O
has	O
unit	O
norm	O
we	O
can	O
project	O
it	O
onto	O
the	O
ball	O
after	O
each	O
update	O
latent	B
structural	O
svms	O
in	O
many	O
applications	O
of	O
interest	O
we	O
have	O
latent	B
or	O
hidden	B
variables	I
h	O
for	O
example	O
in	O
object	O
detections	O
problems	O
we	O
may	O
be	O
told	O
that	O
the	O
image	O
contains	O
an	O
object	O
so	O
y	O
but	O
we	O
may	O
not	O
know	O
where	O
it	O
is	O
the	O
location	O
of	O
the	O
object	O
or	O
its	O
pose	O
can	O
be	O
considered	O
a	O
hidden	B
variable	I
or	O
in	O
machine	O
translation	O
we	O
may	O
know	O
the	O
source	O
text	O
x	O
english	O
and	O
the	O
target	O
text	O
y	O
french	O
but	O
we	O
typically	O
do	O
not	O
know	O
the	O
alignment	B
between	O
the	O
words	O
we	O
will	O
extend	O
our	O
model	O
as	O
follows	O
to	O
get	O
a	O
latent	B
crf	B
py	O
hx	O
w	O
expwt	O
y	O
h	O
zx	O
w	O
zx	O
w	O
expwt	O
y	O
h	O
yh	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
in	O
addition	O
we	O
introduce	O
the	O
loss	B
function	I
ly	O
y	O
h	O
this	O
measures	O
the	O
loss	B
when	O
the	O
action	B
that	O
we	O
take	O
is	O
to	O
predict	O
y	O
using	O
latent	B
variables	O
h	O
we	O
could	O
just	O
use	O
ly	O
y	O
as	O
before	O
since	O
h	O
is	O
usually	O
a	O
nuisance	O
variable	O
and	O
not	O
of	O
direct	O
interest	O
however	O
h	O
can	O
sometimes	O
play	O
a	O
useful	O
role	O
in	O
defining	O
a	O
loss	B
given	O
the	O
loss	B
function	I
we	O
define	O
our	O
objective	O
as	O
relw	O
log	O
pw	O
exp	O
lyi	O
y	O
h	O
expwt	O
y	O
h	O
zx	O
w	O
using	O
the	O
same	O
loose	O
lower	O
bound	O
as	O
before	O
we	O
get	O
relw	O
ew	O
lyi	O
y	O
h	O
t	O
y	O
h	O
i	O
yh	O
log	O
max	O
yh	O
wt	O
yi	O
h	O
max	O
h	O
if	O
we	O
set	O
ew	O
and	O
joachims	O
we	O
get	O
the	O
same	O
objective	O
as	O
is	O
optimized	O
in	O
latent	B
svms	I
unfortunately	O
this	O
objective	O
is	O
no	O
longer	O
convex	B
however	O
it	O
is	O
a	O
difference	O
of	O
convex	B
functions	O
and	O
hence	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
cccp	B
or	O
concave-convex	B
procedure	I
and	O
rangarajan	O
this	O
is	O
a	O
method	O
for	O
minimizing	O
functions	O
of	O
the	O
form	O
f	O
gw	O
where	O
f	O
and	O
g	O
are	O
convex	B
the	O
method	O
alternates	O
between	O
finding	O
a	O
linear	O
upper	O
bound	O
u	O
on	O
g	O
and	O
then	O
minimizing	O
the	O
convex	B
function	O
f	O
w	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
cccp	B
is	O
guaranteed	O
to	O
decrease	O
the	O
objective	O
at	O
every	O
iteration	O
and	O
to	O
converge	B
to	O
a	O
local	O
minimum	O
or	O
a	O
saddle	O
point	O
algorithm	O
concave-convex	B
procedure	I
set	O
t	O
and	O
initialize	O
repeat	O
find	O
hyperplane	O
vt	O
such	O
that	O
gw	O
gwt	O
wtt	O
vt	O
for	O
all	O
w	O
solve	O
argminw	O
f	O
t	O
vt	O
set	O
t	O
t	O
until	O
converged	O
when	O
applied	O
to	O
latent	B
ssvms	B
cccp	B
is	O
very	O
similar	B
to	O
em	B
in	O
the	O
e	B
step	I
we	O
compute	O
for	O
example	O
consider	O
the	O
problem	O
of	O
learning	B
to	O
classify	O
a	O
set	O
of	O
documents	O
as	O
relevant	O
or	O
not	O
to	O
a	O
query	O
that	O
is	O
given	O
n	O
documents	O
n	O
for	O
a	O
single	O
query	O
q	O
we	O
want	O
to	O
produce	O
a	O
labeling	O
yj	O
representing	O
whether	O
document	O
j	O
is	O
relevant	O
to	O
q	O
or	O
not	O
suppose	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
precision	B
at	I
k	I
which	O
is	O
a	O
metric	B
widely	O
used	O
in	O
ranking	B
section	O
we	O
will	O
introduce	O
a	O
latent	B
variable	O
for	O
each	O
document	O
hj	O
representing	O
its	O
degree	B
of	O
relevance	O
this	O
corresponds	O
to	O
a	O
latent	B
total	B
ordering	I
that	O
has	O
to	O
be	O
consistent	B
with	O
the	O
observed	O
partial	O
ordering	O
y	O
given	O
this	O
we	O
can	O
define	O
the	O
following	O
loss	B
function	I
ly	O
y	O
h	O
ny	O
iyhj	O
where	O
ny	O
is	O
the	O
total	O
number	O
of	O
relevant	O
documents	O
this	O
loss	B
is	O
essentially	O
just	O
minus	O
the	O
precisionk	O
except	O
we	O
replace	O
with	O
nyk	O
so	O
that	O
the	O
loss	B
will	O
have	O
a	O
minimum	O
of	O
zero	O
see	O
and	O
joachims	O
for	O
details	O
k	O
k	O
structural	O
svms	O
the	O
linear	O
upper	O
bound	O
by	O
setting	O
vt	O
c	O
hi	O
argmax	O
h	O
wt	O
t	O
yi	O
h	O
yi	O
h	O
i	O
where	O
in	O
the	O
m	B
step	I
we	O
estimate	O
w	O
using	O
techniques	O
for	O
solving	O
fully	O
visible	B
ssvms	B
specifically	O
we	O
minimize	O
lyi	O
y	O
h	O
t	O
y	O
h	O
wt	O
yi	O
h	O
i	O
max	O
yh	O
c	O
exercises	O
c	O
exercise	O
derivative	O
of	O
the	O
log	B
partition	B
function	I
derive	O
equation	O
exercise	O
ci	B
properties	O
of	O
gaussian	B
graphical	B
models	I
jordan	O
in	O
this	O
question	O
we	O
study	O
the	O
relationship	O
between	O
sparse	B
matrices	O
and	O
sparse	B
graphs	O
for	O
gaussian	B
graphical	B
models	I
consider	O
a	O
multivariate	B
gaussian	B
n	O
in	O
dimensions	O
suppose	O
throughout	O
recall	B
that	O
for	O
jointly	O
gaussian	B
random	O
variables	O
we	O
know	O
that	O
xi	O
and	O
xj	O
are	O
independent	O
iff	B
they	O
are	O
uncorrelated	O
ie	O
ij	O
is	O
not	O
true	O
in	O
general	O
or	O
even	O
if	O
xi	O
and	O
xj	O
are	O
gaussian	B
but	O
not	O
jointly	O
gaussian	B
also	O
xi	O
is	O
conditionally	B
independent	I
of	O
xj	O
given	O
all	O
the	O
other	O
variables	O
iff	B
ij	O
a	O
suppose	O
are	O
there	O
any	O
marginal	O
independencies	O
amongst	O
and	O
what	O
about	O
conditional	O
indepen	O
which	O
pairwise	O
terms	O
xixj	O
are	O
missing	B
draw	O
dencies	O
hint	O
compute	O
an	O
undirected	B
graphical	B
model	I
that	O
captures	O
as	O
many	O
of	O
these	O
independence	O
statements	O
and	O
conditional	O
as	O
possible	O
but	O
does	O
not	O
make	O
any	O
false	O
independence	O
assertions	O
and	O
expand	O
out	O
xt	O
b	O
suppose	O
are	O
there	O
any	O
marginal	O
independencies	O
amongst	O
and	O
are	O
there	O
any	O
conditional	O
independencies	O
amongst	O
and	O
draw	O
an	O
undirected	B
graphical	B
model	I
that	O
captures	O
as	O
many	O
of	O
these	O
independence	O
statements	O
and	O
conditional	O
as	O
possible	O
but	O
does	O
not	O
make	O
any	O
false	O
independence	O
assertions	O
c	O
now	O
suppose	O
the	O
distribution	O
on	O
x	O
can	O
be	O
represented	O
by	O
the	O
following	O
dag	B
let	O
the	O
cpds	O
be	O
as	O
follows	O
p	O
n	O
p	O
n	O
p	O
n	O
multiply	O
these	O
cpds	O
together	O
and	O
complete	B
the	O
square	O
to	O
find	O
the	O
corresponding	O
joint	B
distribution	I
n	O
may	O
find	O
it	O
easier	O
to	O
solve	O
for	O
rather	O
than	O
chapter	O
undirected	B
graphical	B
models	I
random	O
fields	O
d	O
for	O
the	O
dag	B
model	O
in	O
the	O
previous	O
question	O
are	O
there	O
any	O
marginal	O
independencies	O
amongst	O
and	O
what	O
about	O
conditional	O
independencies	O
draw	O
an	O
undirected	B
graphical	B
model	I
that	O
captures	O
as	O
many	O
of	O
these	O
independence	O
statements	O
as	O
possible	O
but	O
does	O
not	O
make	O
any	O
false	O
independence	O
assertions	O
marginal	O
or	O
conditional	O
exercise	O
independencies	O
in	O
gaussian	B
graphical	B
models	I
mackay	O
a	O
consider	O
the	O
dag	B
assume	O
that	O
all	O
the	O
cpds	O
are	O
linear-gaussian	O
which	O
of	O
the	O
following	O
matrices	O
could	O
be	O
the	O
covariance	B
matrix	I
d	O
b	O
c	O
a	O
b	O
which	O
of	O
the	O
above	O
matrices	O
could	O
be	O
inverse	O
covariance	B
matrix	I
c	O
consider	O
the	O
dag	B
assume	O
that	O
all	O
the	O
cpds	O
are	O
linear-gaussian	O
which	O
of	O
the	O
above	O
matrices	O
could	O
be	O
the	O
covariance	B
matrix	I
d	O
which	O
of	O
the	O
above	O
matrices	O
could	O
be	O
the	O
inverse	O
covariance	B
matrix	I
e	O
let	O
three	O
variables	O
have	O
covariance	B
matrix	I
and	O
precision	B
matrix	I
as	O
follows	O
now	O
focus	O
on	O
and	O
which	O
of	O
the	O
following	O
statements	O
about	O
their	O
covariance	B
matrix	I
and	O
precision	B
matrix	I
are	O
true	O
a	O
b	O
exercise	O
cost	O
of	O
training	O
mrfs	O
and	O
crfs	O
koller	O
consider	O
the	O
process	O
of	O
gradient-ascent	O
training	O
for	O
a	O
log-linear	B
model	O
with	O
k	O
features	B
given	O
a	O
data	O
set	O
with	O
n	O
training	O
instances	O
assume	O
for	O
simplicity	O
that	O
the	O
cost	O
of	O
computing	O
a	O
single	O
feature	O
over	O
a	O
single	O
instance	O
in	O
our	O
data	O
set	O
is	O
constant	O
as	O
is	O
the	O
cost	O
of	O
computing	O
the	O
expected	B
value	I
of	O
each	O
feature	O
once	O
we	O
compute	O
a	O
marginal	O
over	O
the	O
variables	O
in	O
its	O
scope	B
assume	O
that	O
it	O
takes	O
c	O
time	O
to	O
compute	O
all	O
the	O
marginals	O
for	O
each	O
data	O
case	O
also	O
assume	O
that	O
we	O
need	O
r	O
iterations	O
for	O
the	O
gradient	O
process	O
to	O
converge	B
using	O
this	O
notation	O
what	O
is	O
the	O
time	O
required	O
to	O
train	O
an	O
mrf	B
in	O
big-o	O
notation	O
using	O
this	O
notation	O
what	O
is	O
the	O
time	O
required	O
to	O
train	O
a	O
crf	B
in	O
big-o	O
notation	O
exercise	O
full	B
conditional	I
in	O
an	O
ising	B
model	I
consider	O
an	O
ising	B
model	I
xn	O
expjijxixj	O
z	O
exphixi	O
where	O
ij	O
denotes	O
all	O
unique	O
pairs	O
all	O
edges	B
jij	O
r	O
is	O
the	O
coupling	O
strength	O
on	O
edge	O
i	O
j	O
hi	O
r	O
is	O
the	O
local	B
evidence	B
term	O
and	O
h	O
are	O
all	O
the	O
parameters	O
structural	O
svms	O
if	O
xi	O
derive	O
an	O
expression	O
for	O
the	O
full	B
conditional	I
pxi	O
i	O
pxi	O
where	O
x	O
i	O
are	O
all	O
nodes	B
except	O
i	O
and	O
nbi	O
are	O
the	O
neighbors	B
of	O
i	O
in	O
the	O
graph	B
hint	O
you	O
answer	O
should	O
use	O
the	O
sigmoid	B
logistic	B
function	O
e	O
z	O
now	O
suppose	O
xi	O
derive	O
a	O
related	O
expression	O
for	O
pxix	O
i	O
in	O
this	O
case	O
result	O
can	O
be	O
used	O
when	O
applying	O
gibbs	B
sampling	I
to	O
the	O
model	O
exact	O
inference	B
for	O
graphical	B
models	I
introduction	O
in	O
section	O
we	O
discussed	O
the	O
forwards-backwards	B
algorithm	I
which	O
can	O
exactly	O
compute	O
the	O
posterior	O
marginals	O
pxtv	O
in	O
any	O
chain-structured	O
graphical	B
model	I
where	O
x	O
are	O
the	O
hidden	B
variables	I
discrete	B
and	O
v	O
are	O
the	O
visible	B
variables	I
this	O
algorithm	O
can	O
be	O
modified	O
to	O
compute	O
the	O
posterior	B
mode	B
and	O
posterior	O
samples	B
a	O
similar	B
algorithm	O
for	O
linear-gaussian	O
chains	O
known	O
as	O
the	O
kalman	B
smoother	I
was	O
discussed	O
in	O
section	O
our	O
goal	O
in	O
this	O
chapter	O
is	O
to	O
generalize	B
these	O
exact	O
inference	B
algorithms	O
to	O
arbitrary	O
graphs	O
the	O
resulting	O
methods	O
apply	O
to	O
both	O
directed	B
and	O
undirected	B
graphical	B
models	I
we	O
will	O
describe	O
a	O
variety	O
of	O
algorithms	O
but	O
we	O
omit	O
their	O
derivations	O
for	O
brevity	O
see	O
e	O
g	O
koller	O
and	O
friedman	O
for	O
a	O
detailed	O
exposition	O
of	O
exact	O
inference	B
techniques	O
for	O
discrete	B
directed	B
graphical	B
models	I
belief	B
propagation	I
for	O
trees	O
in	O
this	O
section	O
we	O
generalize	B
the	O
forwards-backwards	B
algorithm	I
from	O
chains	O
to	O
trees	O
the	O
resulting	O
algorithm	O
is	O
known	O
as	O
belief	B
propagation	I
or	O
the	O
sum-product	B
algorithm	I
serial	O
protocol	O
zv	O
s	O
v	O
e	O
we	O
initially	O
assume	O
notational	O
simplicity	O
that	O
the	O
model	O
is	O
a	O
pairwise	B
mrf	B
crf	B
i	O
e	O
pxv	O
sxs	O
stxs	O
xt	O
where	O
s	O
is	O
the	O
local	B
evidence	B
for	O
node	O
s	O
and	O
st	O
is	O
the	O
potential	O
for	O
edge	O
s	O
t	O
we	O
will	O
consider	O
the	O
case	O
of	O
models	O
with	O
higher	O
order	O
cliques	B
as	O
directed	B
trees	O
later	O
on	O
one	O
way	O
to	O
implement	O
bp	B
for	O
undirected	B
trees	O
is	O
as	O
follows	O
pick	O
an	O
arbitrary	O
node	O
and	O
call	O
it	O
the	O
root	B
r	O
now	O
orient	O
all	O
edges	B
away	O
from	O
r	O
we	O
can	O
imagine	O
picking	O
up	O
the	O
graph	B
at	O
node	O
r	O
and	O
letting	O
all	O
the	O
edges	B
dangle	O
down	O
this	O
gives	O
us	O
a	O
well-defined	O
notion	O
of	O
parent	O
and	O
child	O
now	O
we	O
send	O
messages	O
up	O
from	O
the	O
leaves	B
to	O
the	O
root	B
collect	B
evidence	B
phase	B
and	O
then	O
back	O
down	O
from	O
the	O
root	B
distribute	B
evidence	B
phase	B
in	O
a	O
manner	O
analogous	O
to	O
forwards-backwards	B
on	O
chains	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
root	B
v	O
st	O
t	O
v	O
st	O
t	O
root	B
s	O
u	O
s	O
u	O
figure	O
message	B
passing	I
on	O
a	O
tree	B
collect-to-root	B
phase	B
distribute-from-root	B
phase	B
to	O
explain	O
the	O
process	O
in	O
more	O
detail	O
consider	O
the	O
example	O
in	O
figure	O
suppose	O
we	O
want	O
to	O
compute	O
the	O
belief	B
state	B
at	O
node	O
t	O
we	O
will	O
initially	O
condition	O
the	O
belief	O
only	O
on	O
evidence	B
that	O
is	O
at	O
or	O
below	O
t	O
in	O
the	O
graph	B
i	O
e	O
we	O
want	O
to	O
compute	O
bel	O
t	O
we	O
will	O
call	O
this	O
a	O
bottom-up	O
belief	B
state	B
suppose	O
by	O
induction	B
that	O
we	O
have	O
computed	O
messages	O
from	O
t	O
s	O
two	O
children	B
summarizing	O
what	O
they	O
think	O
t	O
should	O
know	O
about	O
the	O
evidence	B
in	O
their	O
subtrees	O
i	O
e	O
we	O
have	O
computed	O
m	O
st	O
is	O
all	O
the	O
evidence	B
on	O
the	O
downstream	O
side	O
of	O
the	O
s	O
t	O
edge	O
figure	O
and	O
similarly	O
we	O
have	O
computed	O
mu	O
txt	O
then	O
we	O
can	O
compute	O
the	O
bottom-up	O
belief	B
state	B
at	O
t	O
as	O
follows	O
s	O
txt	O
xtv	O
st	O
where	O
v	O
t	O
pxtv	O
t	O
pxtv	O
t	O
bel	O
zt	O
txt	O
c	O
cht	O
m	O
c	O
txt	O
where	O
txt	O
pxtvt	O
is	O
the	O
local	B
evidence	B
for	O
node	O
t	O
and	O
zt	O
is	O
the	O
local	O
normalization	O
in	O
words	O
we	O
multiply	O
all	O
the	O
incoming	O
messages	O
from	O
our	O
children	B
as	O
well	O
as	O
the	O
constant	O
incoming	O
message	O
from	O
our	O
local	B
evidence	B
and	O
then	O
normalize	O
how	O
do	O
we	O
compute	O
the	O
messages	O
themselves	O
consider	O
computing	O
m	O
of	O
t	O
s	O
children	B
assume	O
by	O
recursion	O
that	O
we	O
have	O
computed	O
bel	O
can	O
compute	O
the	O
message	O
as	O
follows	O
s	O
we	O
have	O
explained	O
how	O
to	O
compute	O
the	O
bottom-up	O
belief	O
states	O
from	O
the	O
bottom-up	O
messages	O
s	O
txt	O
where	O
s	O
is	O
one	O
s	O
then	O
we	O
s	O
pxsv	O
stxs	O
xtbel	O
s	O
txt	O
m	O
xs	O
essentially	O
we	O
convert	O
beliefs	O
about	O
xs	O
into	O
beliefs	O
about	O
xt	O
by	O
using	O
the	O
edge	O
potential	O
st	O
we	O
continue	O
in	O
this	O
way	O
up	O
the	O
tree	B
until	O
we	O
reach	O
the	O
root	B
once	O
at	O
the	O
root	B
we	O
have	O
seen	O
all	O
the	O
evidence	B
in	O
the	O
tree	B
so	O
we	O
can	O
compute	O
our	O
local	O
belief	B
state	B
at	O
the	O
root	B
using	O
belrxr	O
pxrv	O
pxtv	O
r	O
rxr	O
m	O
c	O
rxr	O
c	O
chr	O
this	O
completes	O
the	O
end	O
of	O
the	O
upwards	O
pass	O
which	O
is	O
analogous	O
to	O
the	O
forwards	O
pass	O
in	O
an	O
hmm	B
as	O
a	O
side	O
effect	O
we	O
can	O
compute	O
the	O
probability	B
of	I
the	I
evidence	B
by	O
collecting	O
the	O
belief	B
propagation	I
for	O
trees	O
normalization	O
constants	O
pv	O
zt	O
t	O
we	O
can	O
now	O
pass	O
messages	O
down	O
from	O
the	O
root	B
for	O
example	O
consider	O
node	O
s	O
with	O
parent	O
t	O
as	O
shown	O
in	O
figure	O
to	O
compute	O
the	O
belief	B
state	B
for	O
s	O
we	O
need	O
to	O
combine	O
the	O
bottom-up	O
belief	O
for	O
s	O
together	O
with	O
a	O
top-down	O
message	O
from	O
t	O
which	O
summarizes	O
all	O
the	O
information	B
in	O
the	O
rest	O
of	O
the	O
graph	B
m	O
st	O
is	O
all	O
the	O
evidence	B
on	O
the	O
upstream	O
side	O
of	O
the	O
s	O
t	O
edge	O
as	O
shown	O
in	O
figure	O
we	O
then	O
have	O
st	O
where	O
v	O
t	O
sxs	O
pxtv	O
belsxs	O
pxsv	O
bel	O
s	O
m	O
t	O
sxt	O
t	O
pas	O
how	O
do	O
we	O
compute	O
these	O
downward	O
messages	O
for	O
example	O
consider	O
the	O
message	O
from	O
t	O
to	O
s	O
suppose	O
t	O
s	O
parent	O
is	O
r	O
and	O
t	O
s	O
children	B
are	O
s	O
and	O
u	O
as	O
shown	O
in	O
figure	O
we	O
want	O
to	O
include	O
in	O
m	O
t	O
s	O
all	O
the	O
information	B
that	O
t	O
has	O
received	O
except	O
for	O
the	O
information	B
that	O
s	O
sent	O
it	O
xt	O
xt	O
t	O
sxs	O
pxsv	O
m	O
st	O
stxs	O
xt	O
beltxt	O
m	O
s	O
txt	O
rather	O
than	O
dividing	O
out	O
the	O
message	O
sent	O
up	O
to	O
t	O
we	O
can	O
plug	O
in	O
the	O
equation	O
of	O
belt	O
to	O
get	O
m	O
t	O
sxs	O
stxs	O
xt	O
txt	O
m	O
p	O
txt	O
m	O
c	O
txt	O
c	O
p	O
pat	O
in	O
other	O
words	O
we	O
multiply	O
together	O
all	O
the	O
messages	O
coming	O
into	O
t	O
from	O
all	O
nodes	B
except	O
for	O
the	O
recipient	O
s	O
combine	O
together	O
and	O
then	O
pass	O
through	O
the	O
edge	O
potential	O
st	O
in	O
the	O
case	O
of	O
a	O
chain	O
t	O
only	O
has	O
one	O
child	O
s	O
and	O
one	O
parent	O
p	O
so	O
the	O
above	O
simplifies	O
to	O
m	O
t	O
sxs	O
stxs	O
xt	O
txtm	O
p	O
txt	O
xt	O
the	O
version	O
of	O
bp	B
in	O
which	O
we	O
use	O
division	O
is	O
called	O
belief	B
updating	I
and	O
the	O
version	O
in	O
which	O
we	O
multiply	O
all-but-one	O
of	O
the	O
messages	O
is	O
called	O
sum-product	B
the	O
belief	B
updating	I
version	O
is	O
analogous	O
to	O
how	O
we	O
formulated	O
the	O
kalman	B
smoother	I
in	O
section	O
the	O
topdown	O
messages	O
depend	O
on	O
the	O
bottom-up	O
messages	O
this	O
means	O
they	O
can	O
be	O
interpreted	O
as	O
conditional	O
posterior	O
probabilities	O
the	O
sum-product	B
version	O
is	O
analogous	O
to	O
how	O
we	O
formulated	O
the	O
backwards	O
algorithm	O
in	O
section	O
the	O
top-down	O
messages	O
are	O
completely	O
independent	O
of	O
the	O
bottom-up	O
messages	O
which	O
means	O
they	O
can	O
only	O
be	O
interpreted	O
as	O
conditional	O
likelihoods	O
see	O
section	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
this	O
subtle	O
difference	O
parallel	O
protocol	O
so	O
far	O
we	O
have	O
presented	O
a	O
serial	O
version	O
of	O
the	O
algorithm	O
in	O
which	O
we	O
send	O
messages	O
up	O
to	O
the	O
root	B
and	O
back	O
this	O
is	O
the	O
optimal	O
approach	O
for	O
a	O
tree	B
and	O
is	O
a	O
natural	O
extension	B
of	O
forwards-backwards	B
on	O
chains	O
however	O
as	O
a	O
prelude	O
to	O
handling	O
general	O
graphs	O
with	O
loops	O
we	O
now	O
consider	O
a	O
parallel	O
version	O
of	O
bp	B
this	O
gives	O
equivalent	O
results	O
to	O
the	O
serial	O
version	O
but	O
is	O
less	O
efficient	O
when	O
implemented	O
on	O
a	O
serial	O
machine	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
the	O
basic	O
idea	O
is	O
that	O
all	O
nodes	B
receive	O
messages	O
from	O
their	O
neighbors	B
in	O
parallel	O
they	O
then	O
updates	O
their	O
belief	O
states	O
and	O
finally	O
they	O
send	O
new	O
messages	O
back	O
out	O
to	O
their	O
neighbors	B
this	O
process	O
repeats	O
until	O
convergence	O
this	O
kind	O
of	O
computing	O
architecture	O
is	O
called	O
a	O
systolic	B
array	I
due	O
to	O
its	O
resemblance	O
to	O
a	O
beating	O
heart	O
more	O
precisely	O
we	O
initialize	O
all	O
messages	O
to	O
the	O
all	O
s	O
vector	O
then	O
in	O
parallel	O
each	O
node	O
absorbs	O
messages	O
from	O
all	O
its	O
neighbors	B
using	O
t	O
nbrs	O
mt	O
sxs	O
sxs	O
stxs	O
xt	O
belsxs	O
sxs	O
ms	O
txt	O
xs	O
then	O
in	O
parallel	O
each	O
node	O
sends	O
messages	O
to	O
each	O
of	O
its	O
neighbors	B
mu	O
sxs	O
u	O
nbrst	O
the	O
ms	O
t	O
message	O
is	O
computed	O
by	O
multiplying	O
together	O
all	O
incoming	O
messages	O
except	O
the	O
one	O
sent	O
by	O
the	O
recipient	O
and	O
then	O
passing	O
through	O
the	O
st	O
potential	O
at	O
iteration	O
t	O
of	O
the	O
algorithm	O
belsxs	O
represents	O
the	O
posterior	O
belief	O
of	O
xs	O
conditioned	O
on	O
the	O
evidence	B
that	O
is	O
t	O
steps	O
away	O
in	O
the	O
graph	B
after	O
dg	O
steps	O
where	O
dg	O
is	O
the	O
diameter	B
of	O
the	O
graph	B
largest	O
distance	O
between	O
any	O
two	O
pairs	O
of	O
nodes	B
every	O
node	O
has	O
obtained	O
information	B
from	O
all	O
the	O
other	O
nodes	B
its	O
local	O
belief	B
state	B
is	O
then	O
the	O
correct	O
posterior	O
marginal	O
since	O
the	O
diameter	B
of	O
a	O
tree	B
is	O
at	O
most	O
the	O
algorithm	O
converges	O
in	O
a	O
linear	O
number	O
of	O
steps	O
we	O
can	O
actually	O
derive	O
the	O
up-down	B
version	O
of	O
the	O
algorithm	O
by	O
imposing	O
the	O
condition	O
that	O
a	O
node	O
can	O
only	O
send	O
a	O
message	O
once	O
it	O
has	O
received	O
messages	O
from	O
all	O
its	O
other	O
neighbors	B
this	O
means	O
we	O
must	O
start	O
with	O
the	O
leaf	B
nodes	B
which	O
only	O
have	O
one	O
neighbor	O
the	O
messages	O
then	O
propagate	O
up	O
to	O
the	O
root	B
and	O
back	O
we	O
can	O
also	O
update	O
the	O
nodes	B
in	O
a	O
random	O
order	O
the	O
only	O
requirement	O
is	O
that	O
each	O
node	O
get	O
updated	O
dg	O
times	O
this	O
is	O
just	O
enough	O
time	O
for	O
information	B
to	O
spread	O
throughout	O
the	O
whole	O
tree	B
similar	B
parallel	O
distributed	O
algorithms	O
for	O
solving	O
linear	O
systems	O
of	O
equations	O
are	O
discussed	O
in	O
in	O
particular	O
the	O
gauss-seidel	B
algorithm	O
is	O
analogous	O
to	O
the	O
serial	O
up-down	B
version	O
of	O
bp	B
and	O
the	O
jacobi	B
algorithm	O
is	O
analogous	O
to	O
the	O
parallel	O
version	O
of	O
bp	B
gaussian	B
bp	B
now	O
consider	O
the	O
case	O
where	O
pxv	O
is	O
jointly	O
gaussian	B
so	O
it	O
can	O
be	O
represented	O
as	O
a	O
gaussian	B
pairwise	B
mrf	B
as	O
in	O
section	O
we	O
now	O
present	O
the	O
belief	B
propagation	I
algorithm	O
for	O
this	O
class	O
of	O
models	O
follow	O
the	O
presentation	O
of	O
also	O
et	O
al	O
we	O
will	O
assume	O
the	O
following	O
node	O
and	O
edge	O
potentials	O
txt	O
exp	O
stxs	O
xt	O
exp	O
t	O
btxt	O
xsastxt	O
so	O
the	O
overall	O
model	O
has	O
the	O
form	O
pxv	O
exp	O
xt	O
ax	O
bt	O
x	O
belief	B
propagation	I
for	O
trees	O
this	O
is	O
the	O
information	B
form	I
of	O
the	O
mvn	B
exercise	O
where	O
a	O
is	O
the	O
precision	B
matrix	I
note	O
that	O
by	O
completing	B
the	I
square	I
the	O
local	B
evidence	B
can	O
be	O
rewritten	O
as	O
a	O
gaussian	B
tt	O
n	O
t	O
txt	O
n	O
a	O
below	O
we	O
describe	O
how	O
to	O
use	O
bp	B
to	O
compute	O
the	O
posterior	O
node	O
marginals	O
pxtv	O
n	O
t	O
t	O
if	O
the	O
graph	B
is	O
a	O
tree	B
the	O
method	O
is	O
exact	O
if	O
the	O
graph	B
is	O
loopy	O
the	O
posterior	O
means	O
may	O
still	O
be	O
exact	O
but	O
the	O
posterior	O
variances	O
are	O
often	O
too	O
small	O
and	O
freeman	O
although	O
the	O
precision	B
matrix	I
a	O
is	O
often	O
sparse	B
computing	O
the	O
posterior	B
mean	B
requires	O
inverting	O
it	O
since	O
a	O
bp	B
provides	O
a	O
way	O
to	O
exploit	O
graph	B
structure	O
to	O
perform	O
this	O
computation	O
in	O
od	O
time	O
instead	O
of	O
this	O
is	O
related	O
to	O
various	O
methods	O
from	O
linear	O
algebra	O
as	O
discussed	O
in	O
since	O
the	O
model	O
is	O
jointly	O
gaussian	B
all	O
marginals	O
and	O
all	O
messages	O
will	O
be	O
gaussian	B
the	O
key	O
operations	O
we	O
need	O
are	O
to	O
multiply	O
together	O
two	O
gaussian	B
factors	B
and	O
to	O
marginalize	O
out	O
a	O
variable	O
from	O
a	O
joint	O
gaussian	B
factor	B
for	O
multiplication	O
we	O
can	O
use	O
the	O
fact	O
that	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
gaussian	B
n	O
n	O
n	O
which	O
follows	O
from	O
the	O
normalization	O
constant	O
of	O
a	O
gaussian	B
we	O
now	O
have	O
all	O
the	O
pieces	O
we	O
need	O
in	O
particular	O
let	O
the	O
message	O
ms	O
txt	O
be	O
a	O
gaussian	B
with	O
mean	B
st	O
and	O
precision	B
st	O
from	O
equation	O
the	O
belief	O
at	O
node	O
s	O
is	O
given	O
by	O
the	O
product	O
of	O
incoming	O
messages	O
times	O
the	O
local	B
evidence	B
and	O
hence	O
belsxs	O
sxs	O
mtsxs	O
n	O
s	O
s	O
t	O
nbrs	O
t	O
nbrs	O
s	O
ts	O
s	O
s	O
ts	O
ts	O
t	O
nbrs	O
where	O
c	O
exp	O
see	O
exercise	O
for	O
the	O
proof	O
for	O
marginalization	O
we	O
have	O
the	O
following	O
result	O
exp	O
bxdx	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
to	O
compute	O
the	O
messages	O
themselves	O
we	O
use	O
equation	O
which	O
is	O
given	O
by	O
stxs	O
xt	O
sxs	O
stxs	O
xtfstxsdxs	O
xs	O
xs	O
mu	O
sxs	O
u	O
nbrst	O
dxs	O
ms	O
txt	O
where	O
fstxs	O
is	O
the	O
product	O
of	O
the	O
local	B
evidence	B
and	O
all	O
incoming	O
messages	O
excluding	O
the	O
message	O
from	O
t	O
fstxs	O
sxs	O
mu	O
sxs	O
u	O
nbrst	O
n	O
st	O
st	O
st	O
u	O
nbrst	O
us	O
us	O
us	O
u	O
nbrst	O
returning	O
to	O
equation	O
we	O
have	O
st	O
st	O
xs	O
ms	O
txt	O
xs	O
exp	O
n	O
st	O
st	O
st	O
st	O
ast	O
st	O
st	O
st	O
st	O
stxsxt	O
exp	O
xsastxt	O
exp	O
st	O
st	O
astxtxs	O
exp	O
st	O
st	O
st	O
fstxs	O
dxs	O
dxs	O
const	O
one	O
can	O
generalize	B
these	O
equations	O
to	O
the	O
case	O
where	O
each	O
node	O
is	O
a	O
vector	O
and	O
the	O
messages	O
become	O
small	O
mvns	O
instead	O
of	O
scalar	O
gaussians	O
and	O
agogino	O
if	O
we	O
apply	O
the	O
resulting	O
algorithm	O
to	O
a	O
linear	B
dynamical	I
system	I
we	O
recover	O
the	O
kalman	B
smoothing	B
algorithm	O
of	O
section	O
to	O
perform	O
message	B
passing	I
in	O
models	O
with	O
non-gaussian	O
potentials	O
one	O
can	O
use	O
sampling	O
methods	O
to	O
approximate	O
the	O
relevant	O
integrals	O
this	O
is	O
called	O
non-parametric	B
bp	B
et	O
al	O
isard	O
sudderth	O
et	O
al	O
other	O
bp	B
variants	O
in	O
this	O
section	O
we	O
briefly	O
discuss	O
several	O
variants	O
of	O
the	O
main	O
algorithm	O
belief	B
propagation	I
for	O
trees	O
ft	O
t	O
xt	O
vt	O
figure	O
illustration	O
of	O
how	O
to	O
compute	O
the	O
two-slice	O
distribution	O
for	O
an	O
hmm	B
the	O
t	O
and	O
terms	O
are	O
the	O
local	B
evidence	B
messages	O
from	O
the	O
visible	B
nodes	B
vt	O
to	O
the	O
hidde	O
nodes	B
xt	O
respectively	O
ft	O
is	O
the	O
forwards	O
message	O
from	O
xt	O
and	O
is	O
the	O
backwards	O
message	O
from	O
max-product	B
algorithm	O
it	O
is	O
possible	O
to	O
devise	O
a	O
max-product	B
version	O
of	O
the	O
bp	B
algorithm	O
by	O
replacing	O
the	O
operator	O
with	O
the	O
max	O
operator	O
we	O
can	O
then	O
compute	O
the	O
local	O
map	O
marginal	O
of	O
each	O
node	O
however	O
if	O
there	O
are	O
ties	O
this	O
might	O
not	O
be	O
globally	O
consistent	B
as	O
discussed	O
in	O
section	O
fortunately	O
we	O
can	O
generalize	B
the	O
viterbi	B
algorithm	O
to	O
trees	O
where	O
we	O
use	O
max	O
and	O
argmax	O
in	O
the	O
collectto-root	O
phase	B
and	O
perform	O
traceback	B
in	O
the	O
distribute-from-root	B
phase	B
see	O
for	O
details	O
sampling	O
from	O
a	O
tree	B
it	O
is	O
possible	O
to	O
draw	O
samples	B
from	O
a	O
tree	B
structured	O
model	O
by	O
generalizing	O
the	O
forwards	O
filtering	B
backwards	O
sampling	O
algorithm	O
discussed	O
in	O
section	O
see	O
for	O
details	O
computing	O
posteriors	O
on	O
sets	O
of	O
variables	O
in	O
section	O
we	O
explained	O
how	O
to	O
compute	O
the	O
two-slice	O
distribution	O
j	O
pxt	O
i	O
jv	O
in	O
an	O
hmm	B
namely	O
by	O
using	O
j	O
ti	O
j	O
since	O
ti	O
tifti	O
where	O
ft	O
is	O
the	O
forwards	O
message	O
we	O
can	O
think	O
of	O
this	O
as	O
sending	O
messages	O
ft	O
and	O
t	O
into	O
xt	O
and	O
into	O
and	O
then	O
combining	O
them	O
with	O
the	O
matrix	O
as	O
shown	O
in	O
figure	O
this	O
is	O
like	O
treating	O
xt	O
and	O
as	O
a	O
single	O
mega	O
node	O
and	O
then	O
multiplying	O
all	O
the	O
incoming	O
messages	O
as	O
well	O
as	O
all	O
the	O
local	O
factors	B
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
coherence	O
coherence	O
difficulty	O
intelligence	O
difficulty	O
intelligence	O
grade	O
sat	O
grade	O
sat	O
letter	O
job	O
letter	O
job	O
happy	O
happy	O
figure	O
left	O
the	O
student	O
dgm	B
right	O
the	O
equivalent	O
ugm	B
we	O
add	O
moralization	B
arcs	O
d-i	O
g-j	O
and	O
l-s	O
based	O
on	O
figure	O
of	O
and	O
friedman	O
the	O
variable	B
elimination	I
algorithm	O
we	O
have	O
seen	O
how	O
to	O
use	O
bp	B
to	O
compute	O
exact	O
marginals	O
on	O
chains	O
and	O
trees	O
in	O
this	O
section	O
we	O
discuss	O
an	O
algorithm	O
to	O
compute	O
pxqxv	O
for	O
any	O
kind	O
of	O
graph	B
we	O
will	O
explain	O
the	O
algorithm	O
by	O
example	O
consider	O
the	O
dgm	B
in	O
figure	O
this	O
model	O
from	O
and	O
friedman	O
is	O
a	O
hypothetical	O
model	O
relating	O
various	O
variables	O
pertaining	O
to	O
a	O
typical	O
student	O
the	O
corresponding	O
joint	O
has	O
the	O
following	O
form	O
p	O
d	O
i	O
g	O
s	O
l	O
j	O
h	O
p	O
dp	O
sp	O
j	O
note	O
that	O
the	O
forms	O
of	O
the	O
cpds	O
do	O
not	O
matter	O
since	O
all	O
our	O
calculations	O
will	O
be	O
symbolic	O
however	O
for	O
illustration	O
purposes	O
we	O
will	O
assume	O
all	O
variables	O
are	O
binary	O
before	O
proceeding	O
we	O
convert	O
our	O
model	O
to	O
undirected	B
form	O
this	O
is	O
not	O
required	O
but	O
it	O
makes	O
for	O
a	O
more	O
unified	O
presentation	O
since	O
the	O
resulting	O
method	O
can	O
then	O
be	O
applied	O
to	O
both	O
dgms	O
and	O
ugms	O
as	O
we	O
will	O
see	O
in	O
section	O
to	O
a	O
variety	O
of	O
other	O
problems	O
that	O
have	O
nothing	O
to	O
do	O
with	O
graphical	B
models	I
since	O
the	O
computational	O
complexity	O
of	O
inference	B
in	O
dgms	O
and	O
ugms	O
is	O
generally	O
speaking	O
the	O
same	O
nothing	O
is	O
lost	O
in	O
this	O
transformation	O
from	O
a	O
computational	O
point	O
of	O
to	O
convert	O
the	O
dgm	B
to	O
a	O
ugm	B
we	O
simply	O
define	O
a	O
potential	O
or	O
factor	B
for	O
every	O
cpd	B
yielding	O
pc	O
d	O
i	O
g	O
s	O
l	O
j	O
h	O
cc	O
dd	O
c	O
i	O
gg	O
i	O
d	O
ss	O
i	O
ll	O
g	O
j	O
l	O
s	O
h	O
g	O
there	O
are	O
a	O
few	O
tricks	O
one	O
can	O
exploit	O
in	O
the	O
directed	B
case	O
that	O
cannot	O
easily	O
be	O
exploited	O
in	O
the	O
undirected	B
case	O
one	O
important	O
example	O
is	O
barren	B
node	I
removal	I
to	O
explain	O
this	O
consider	O
a	O
naive	O
bayes	O
classifier	O
as	O
in	O
figure	O
suppose	O
we	O
want	O
to	O
infer	O
y	O
and	O
we	O
observe	O
and	O
but	O
not	O
and	O
it	O
is	O
clear	O
that	O
we	O
can	O
safely	O
remove	O
and	O
since	O
in	O
general	O
once	O
we	O
have	O
removed	O
hidden	B
leaves	B
we	O
can	O
apply	O
this	O
process	O
recursively	O
since	O
potential	O
functions	O
do	O
not	O
necessary	O
sum	O
to	O
one	O
we	O
cannot	O
use	O
this	O
trick	O
in	O
the	O
undirected	B
case	O
see	O
and	O
friedman	O
for	O
a	O
variety	O
of	O
other	O
speedup	O
tricks	O
and	O
similarly	O
for	O
the	O
variable	B
elimination	I
algorithm	O
since	O
all	O
the	O
potentials	O
are	O
locally	B
normalized	I
since	O
they	O
are	O
cpds	O
there	O
is	O
no	O
need	O
for	O
a	O
global	O
normalization	O
constant	O
so	O
z	O
the	O
corresponding	O
undirected	B
graph	B
is	O
shown	O
in	O
figure	O
note	O
that	O
it	O
has	O
more	O
edges	B
than	O
the	O
dag	B
in	O
particular	O
any	O
unmarried	O
nodes	B
that	O
share	O
a	O
child	O
must	O
get	O
married	O
by	O
adding	O
an	O
edge	O
between	O
them	O
this	O
process	O
is	O
known	O
as	O
moralization	B
only	O
then	O
can	O
the	O
arrows	O
be	O
dropped	O
in	O
this	O
example	O
we	O
added	O
d-i	O
g-j	O
and	O
l-s	O
moralization	B
arcs	O
the	O
reason	O
this	O
operation	O
is	O
required	O
is	O
to	O
ensure	O
that	O
the	O
ci	B
properties	O
of	O
the	O
ugm	B
match	O
those	O
of	O
the	O
dgm	B
as	O
explained	O
in	O
section	O
it	O
also	O
ensures	O
there	O
is	O
a	O
clique	B
that	O
can	O
store	O
the	O
cpds	O
of	O
each	O
family	B
now	O
suppose	O
we	O
want	O
to	O
compute	O
pj	O
the	O
marginal	O
probability	O
that	O
a	O
person	O
will	O
get	O
a	O
job	O
since	O
we	O
have	O
binary	O
variables	O
we	O
could	O
simply	O
enumerate	O
over	O
all	O
possible	O
assignments	O
to	O
all	O
the	O
variables	O
for	O
j	O
adding	O
up	O
the	O
probability	O
of	O
each	O
joint	O
instantiation	O
pj	O
pc	O
d	O
i	O
g	O
s	O
l	O
j	O
h	O
l	O
s	O
g	O
h	O
i	O
d	O
c	O
however	O
this	O
would	O
take	O
time	O
we	O
can	O
be	O
smarter	O
by	O
pushing	B
sums	I
inside	I
products	I
this	O
is	O
the	O
key	O
idea	O
behind	O
the	O
variable	B
elimination	I
algorithm	O
and	O
poole	O
also	O
called	O
bucket	B
elimination	I
or	O
in	O
the	O
context	O
of	O
genetic	O
pedigree	O
trees	O
the	O
peeling	B
algorithm	I
et	O
al	O
in	O
our	O
example	O
we	O
get	O
we	O
now	O
evaluate	O
this	O
expression	O
working	O
right	O
to	O
left	O
as	O
shown	O
in	O
table	O
first	O
we	O
multiply	O
together	O
all	O
the	O
terms	O
in	O
the	O
scope	B
of	O
the	O
c	O
operator	O
to	O
create	O
the	O
temporary	O
factor	B
then	O
we	O
marginalize	O
out	O
c	O
to	O
get	O
the	O
new	O
factor	B
d	O
cc	O
dd	O
c	O
c	O
d	O
next	O
we	O
multiply	O
together	O
all	O
the	O
terms	O
in	O
the	O
scope	B
of	O
the	O
out	O
to	O
create	O
i	O
d	O
gg	O
i	O
d	O
i	O
i	O
d	O
d	O
d	O
operator	O
and	O
then	O
marginalize	O
pj	O
pc	O
d	O
i	O
g	O
s	O
l	O
j	O
h	O
lsghidc	O
cc	O
dd	O
c	O
i	O
gg	O
i	O
d	O
ss	O
i	O
ll	O
g	O
lsghidc	O
j	O
l	O
s	O
h	O
g	O
j	O
j	O
l	O
s	O
ls	O
g	O
d	O
c	O
gg	O
i	O
d	O
cc	O
dd	O
c	O
ll	O
g	O
h	O
g	O
j	O
ss	O
i	O
i	O
h	O
i	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
l	O
s	O
j	O
l	O
s	O
g	O
ll	O
g	O
h	O
h	O
g	O
j	O
s	O
i	O
i	O
d	O
gg	O
i	O
d	O
i	O
h	O
g	O
j	O
i	O
s	O
i	O
i	O
l	O
s	O
j	O
l	O
s	O
g	O
ll	O
g	O
l	O
s	O
j	O
l	O
s	O
ll	O
g	O
h	O
h	O
g	O
j	O
h	O
g	O
l	O
s	O
j	O
l	O
s	O
g	O
ll	O
g	O
l	O
s	O
j	O
l	O
s	O
c	O
d	O
c	O
dd	O
c	O
gg	O
i	O
d	O
s	O
i	O
i	O
i	O
h	O
g	O
j	O
s	O
i	O
h	O
ll	O
g	O
j	O
s	O
j	O
l	O
s	O
l	O
s	O
l	O
l	O
g	O
l	O
s	O
table	O
eliminating	O
variables	O
from	O
figure	O
in	O
the	O
order	O
c	O
d	O
i	O
h	O
g	O
s	O
l	O
to	O
compute	O
p	O
next	O
we	O
multiply	O
together	O
all	O
the	O
terms	O
in	O
the	O
scope	B
of	O
the	O
out	O
to	O
create	O
i	O
s	O
ss	O
i	O
i	O
i	O
s	O
i	O
s	O
i	O
i	O
operator	O
and	O
then	O
marginalize	O
and	O
so	O
on	O
the	O
above	O
technique	O
can	O
be	O
used	O
to	O
compute	O
any	O
marginal	O
of	O
interest	O
such	O
as	O
pj	O
or	O
pj	O
h	O
to	O
compute	O
a	O
conditional	O
we	O
can	O
take	O
a	O
ratio	O
of	O
two	O
marginals	O
where	O
the	O
visible	B
variables	I
have	O
been	O
clamped	O
to	O
their	O
known	O
values	O
hence	O
don	O
t	O
need	O
to	O
be	O
summed	O
over	O
for	O
example	O
pj	O
ji	O
h	O
in	O
general	O
we	O
can	O
write	O
pxqxv	O
pxq	O
xv	O
pxv	O
pj	O
j	O
i	O
h	O
pj	O
i	O
h	O
pxh	O
xq	O
xv	O
pxh	O
q	O
xv	O
xh	O
xh	O
q	O
the	O
variable	B
elimination	I
algorithm	O
the	O
normalization	O
constant	O
in	O
the	O
denominator	O
pxv	O
is	O
called	O
the	O
probability	B
of	I
the	I
evidence	B
see	O
variableelimination	O
for	O
a	O
simple	O
matlab	O
implementation	O
of	O
this	O
algorithm	O
which	O
works	O
for	O
arbitrary	O
graphs	O
and	O
arbitrary	O
discrete	B
factors	B
but	O
before	O
you	O
go	O
too	O
crazy	O
please	O
read	O
section	O
which	O
points	O
out	O
that	O
ve	O
can	O
be	O
exponentially	O
slow	O
in	O
the	O
worst	O
case	O
the	O
generalized	O
distributive	B
law	I
pxqxv	O
abstractly	O
ve	O
can	O
be	O
thought	O
of	O
as	O
computing	O
the	O
following	O
expression	O
cxc	O
x	O
c	O
it	O
is	O
understood	O
that	O
the	O
visible	B
variables	I
xv	O
are	O
clamped	O
and	O
not	O
summed	O
over	O
ve	O
uses	O
non-serial	B
dynamic	B
programming	I
and	O
brioschi	O
caching	O
intermediate	O
results	O
to	O
avoid	O
redundant	O
computation	O
however	O
there	O
are	O
other	O
tasks	O
we	O
might	O
like	O
to	O
solve	O
for	O
any	O
given	O
graphical	B
model	I
for	O
example	O
we	O
might	O
want	O
the	O
map	B
estimate	I
x	O
argmax	O
x	O
c	O
cxc	O
fortunately	O
essentially	O
the	O
same	O
algorithm	O
can	O
also	O
be	O
used	O
to	O
solve	O
this	O
task	O
we	O
just	O
replace	O
sum	O
with	O
max	O
also	O
need	O
a	O
traceback	B
step	O
which	O
actually	O
recovers	O
the	O
argmax	O
as	O
opposed	O
to	O
just	O
the	O
value	O
of	O
max	O
these	O
details	O
are	O
explained	O
in	O
section	O
two	O
binary	O
operations	O
called	O
and	O
which	O
satisfy	O
the	O
following	O
three	O
axioms	O
the	O
operation	O
is	O
associative	B
and	O
commutative	O
and	O
there	O
is	O
an	O
additive	O
identity	O
element	O
in	O
general	O
ve	O
can	O
be	O
applied	O
to	O
any	O
commutative	B
semi-ring	I
this	O
is	O
a	O
set	O
k	O
together	O
with	O
called	O
such	O
that	O
k	O
k	O
for	O
all	O
k	O
k	O
the	O
operation	O
is	O
associative	B
and	O
commutative	O
and	O
there	O
is	O
a	O
multiplicative	O
identity	O
element	O
called	O
such	O
that	O
k	O
for	O
all	O
k	O
k	O
the	O
distributive	B
law	I
holds	O
i	O
e	O
b	O
c	O
a	O
c	O
for	O
all	O
triples	O
b	O
c	O
from	O
k	O
this	O
framework	O
covers	O
an	O
extremely	O
wide	O
range	O
of	O
important	O
applications	O
including	O
constraint	B
satisfaction	I
problems	I
et	O
al	O
dechter	O
the	O
fast	B
fourier	I
transform	I
and	O
mceliece	O
etc	O
see	O
table	O
for	O
some	O
examples	O
computational	O
complexity	O
of	O
ve	O
the	O
running	O
time	O
of	O
ve	O
is	O
clearly	O
exponential	O
in	O
the	O
size	O
of	O
the	O
largest	O
factor	B
since	O
we	O
have	O
sum	O
over	O
all	O
of	O
the	O
corresponding	O
variables	O
some	O
of	O
the	O
factors	B
come	O
from	O
the	O
original	O
model	O
are	O
thus	O
unavoidable	O
but	O
new	O
factors	B
are	O
created	O
in	O
the	O
process	O
of	O
summing	O
out	O
for	O
example	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
domain	O
f	O
f	O
name	O
sum-product	B
max-product	B
min-sum	O
t	O
boolean	O
satisfiability	O
dd	O
c	O
table	O
some	O
commutative	O
semirings	O
j	O
l	O
s	O
i	O
s	O
i	O
d	O
c	O
h	O
l	O
s	O
i	O
dd	O
c	O
d	O
c	O
h	O
l	O
s	O
j	O
l	O
s	O
d	O
c	O
dd	O
c	O
h	O
l	O
g	O
i	O
gg	O
i	O
d	O
ll	O
h	O
g	O
j	O
i	O
s	O
i	O
d	O
l	O
j	O
h	O
j	O
l	O
s	O
l	O
s	O
j	O
h	O
s	O
dd	O
c	O
d	O
c	O
h	O
d	O
c	O
dd	O
c	O
l	O
l	O
j	O
h	O
j	O
h	O
h	O
dd	O
c	O
j	O
j	O
d	O
d	O
c	O
table	O
eliminating	O
variables	O
from	O
figure	O
in	O
the	O
order	O
g	O
i	O
s	O
l	O
h	O
c	O
d	O
in	O
equation	O
we	O
created	O
a	O
factor	B
involving	O
g	O
i	O
and	O
s	O
but	O
these	O
nodes	B
were	O
not	O
originally	O
present	O
together	O
in	O
any	O
factor	B
the	O
order	O
in	O
which	O
we	O
perform	O
the	O
summation	O
is	O
known	O
as	O
the	O
elimination	B
order	I
this	O
can	O
have	O
a	O
large	O
impact	O
on	O
the	O
size	O
of	O
the	O
intermediate	O
factors	B
that	O
are	O
created	O
for	O
example	O
consider	O
the	O
ordering	O
in	O
table	O
the	O
largest	O
created	O
factor	B
the	O
original	O
ones	O
in	O
the	O
model	O
has	O
size	O
corresponding	O
to	O
l	O
s	O
now	O
consider	O
the	O
ordering	O
in	O
table	O
now	O
the	O
largest	O
factors	B
are	O
d	O
l	O
j	O
h	O
and	O
l	O
s	O
j	O
h	O
which	O
are	O
much	O
bigger	O
we	O
can	O
determine	O
the	O
size	O
of	O
the	O
largest	O
factor	B
graphically	O
without	O
worrying	O
about	O
the	O
actual	O
numerical	O
values	O
of	O
the	O
factors	B
when	O
we	O
eliminate	O
a	O
variable	O
xt	O
we	O
connect	O
it	O
to	O
all	O
variables	O
the	O
variable	B
elimination	I
algorithm	O
coherence	O
coherence	O
coherence	O
difficulty	O
intelligence	O
difficulty	O
intelligence	O
difficulty	O
intelligence	O
grade	O
sat	O
grade	O
sat	O
letter	O
job	O
letter	O
job	O
happy	O
happy	O
grade	O
sat	O
letter	O
job	O
happy	O
figure	O
example	O
of	O
the	O
elimination	O
process	O
in	O
the	O
order	O
c	O
d	O
i	O
etc	O
when	O
we	O
eliminate	O
i	O
c	O
we	O
add	O
a	O
fill-in	O
edge	O
between	O
g	O
and	O
s	O
since	O
they	O
are	O
not	O
connected	O
based	O
on	O
figure	O
of	O
and	O
friedman	O
that	O
share	O
a	O
factor	B
with	O
xt	O
reflect	O
the	O
new	O
temporary	O
factor	B
t	O
the	O
edges	B
created	O
by	O
this	O
process	O
are	O
called	O
fill-in	O
edges	B
for	O
example	O
figure	O
shows	O
the	O
fill-in	O
edges	B
introduced	O
when	O
we	O
eliminate	O
in	O
the	O
order	O
c	O
d	O
i	O
the	O
first	O
two	O
steps	O
do	O
not	O
introduce	O
any	O
fill-ins	O
but	O
when	O
we	O
eliminate	O
i	O
we	O
connect	O
g	O
and	O
s	O
since	O
they	O
co-occur	O
in	O
equation	O
let	O
g	O
be	O
the	O
graph	B
induced	O
by	O
applying	O
variable	B
elimination	I
to	O
g	O
using	O
elimination	O
ordering	O
the	O
temporary	O
factors	B
generated	O
by	O
ve	O
correspond	O
to	O
maximal	O
cliques	B
in	O
the	O
graph	B
g	O
for	O
example	O
with	O
ordering	O
d	O
i	O
h	O
g	O
s	O
l	O
the	O
maximal	O
cliques	B
are	O
as	O
follows	O
dd	O
i	O
gg	O
l	O
s	O
jg	O
j	O
hg	O
i	O
s	O
it	O
is	O
clear	O
that	O
the	O
time	O
complexity	O
of	O
ve	O
is	O
kc	O
c	O
cg	O
where	O
c	O
are	O
the	O
cliques	B
that	O
are	O
created	O
is	O
the	O
size	O
of	O
the	O
clique	B
c	O
and	O
we	O
assume	O
for	O
notational	O
simplicity	O
that	O
all	O
the	O
variables	O
have	O
k	O
states	O
each	O
let	O
us	O
define	O
the	O
induced	B
width	I
of	O
a	O
graph	B
given	O
elimination	O
ordering	O
denoted	O
w	O
as	O
the	O
size	O
of	O
the	O
largest	O
factor	B
the	O
largest	O
clique	B
in	O
the	O
induced	O
graph	B
minus	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
complexity	O
of	O
ve	O
with	O
ordering	O
is	O
okw	O
obviously	O
we	O
would	O
like	O
to	O
minimize	O
the	O
running	O
time	O
and	O
hence	O
the	O
induced	B
width	I
let	O
us	O
define	O
the	O
treewidth	B
of	O
a	O
graph	B
as	O
the	O
minimal	B
induced	B
width	I
w	O
min	O
max	O
c	O
g	O
then	O
clearly	O
the	O
best	O
possible	O
running	O
time	O
for	O
ve	O
is	O
odk	O
unfortunately	O
one	O
can	O
show	O
that	O
for	O
arbitrary	O
graphs	O
finding	O
an	O
elimination	O
ordering	O
that	O
minimizes	O
w	O
is	O
np-hard	B
et	O
al	O
in	O
practice	O
greedy	O
search	O
techniques	O
are	O
used	O
to	O
find	O
reasonable	O
orderings	O
although	O
people	O
have	O
tried	O
other	O
heuristic	O
methods	O
for	O
discrete	B
optimization	B
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
such	O
as	O
genetic	B
algorithms	I
et	O
al	O
algorithms	O
with	O
provable	O
performance	O
guarantees	O
it	O
is	O
also	O
possible	O
to	O
derive	O
approximate	O
in	O
some	O
cases	O
the	O
optimal	O
elimination	O
ordering	O
is	O
clear	O
for	O
example	O
for	O
chains	O
we	O
should	O
work	O
forwards	O
or	O
backwards	O
in	O
time	O
for	O
trees	O
we	O
should	O
work	O
from	O
the	O
leaves	B
to	O
the	O
root	B
these	O
orderings	O
do	O
not	O
introduce	O
any	O
fill-in	O
edges	B
so	O
w	O
consequently	O
inference	B
in	O
chains	O
and	O
trees	O
takes	O
ov	O
k	O
time	O
this	O
is	O
one	O
reason	O
why	O
markov	B
chains	O
and	O
markov	B
trees	O
are	O
so	O
widely	O
used	O
unfortunately	O
for	O
other	O
graphs	O
the	O
treewidth	B
is	O
large	O
for	O
example	O
for	O
an	O
m	O
n	O
lattice	B
the	O
treewidth	B
is	O
ominm	O
n	O
and	O
tarjan	O
so	O
ve	O
on	O
a	O
ising	B
model	I
would	O
take	O
time	O
of	O
course	O
just	O
because	O
ve	O
is	O
slow	O
doesn	O
t	O
mean	B
that	O
there	O
isn	O
t	O
some	O
smarter	O
algorithm	O
out	O
there	O
we	O
discuss	O
this	O
issue	O
in	O
section	O
a	O
weakness	O
of	O
ve	O
the	O
main	O
disadvantage	O
of	O
the	O
variable	B
elimination	I
algorithm	O
from	O
its	O
exponential	O
dependence	O
on	O
treewidth	B
is	O
that	O
it	O
is	O
inefficient	O
if	O
we	O
want	O
to	O
compute	O
multiple	O
queries	O
conditioned	O
on	O
the	O
same	O
evidence	B
for	O
example	O
consider	O
computing	O
all	O
the	O
marginals	O
in	O
a	O
chain-structured	O
graphical	B
model	I
such	O
as	O
an	O
hmm	B
we	O
can	O
easily	O
compute	O
the	O
final	O
marginal	O
pxtv	O
by	O
eliminating	O
all	O
the	O
nodes	B
to	O
xt	O
in	O
order	O
this	O
is	O
equivalent	O
to	O
the	O
forwards	O
algorithm	O
and	O
takes	O
ok	O
time	O
but	O
now	O
suppose	O
we	O
want	O
to	O
compute	O
pxt	O
we	O
have	O
to	O
run	O
ve	O
again	O
at	O
a	O
cost	O
of	O
ok	O
time	O
so	O
the	O
total	O
cost	O
to	O
compute	O
all	O
the	O
marginals	O
is	O
ok	O
however	O
we	O
know	O
that	O
we	O
can	O
solve	O
this	O
problem	O
in	O
ok	O
using	O
forwards-backwards	B
the	O
difference	O
is	O
that	O
fb	O
caches	O
the	O
messages	O
computed	O
on	O
the	O
forwards	O
pass	O
so	O
it	O
can	O
reuse	O
them	O
later	O
the	O
same	O
argument	O
holds	O
for	O
bp	B
on	O
trees	O
for	O
example	O
consider	O
the	O
tree	B
in	O
figure	O
we	O
can	O
compute	O
by	O
eliminating	O
this	O
is	O
equivalent	O
to	O
sending	O
messages	O
up	O
to	O
messages	O
correspond	O
to	O
the	O
factors	B
created	O
by	O
ve	O
similarly	O
we	O
can	O
compute	O
and	O
then	O
we	O
see	O
that	O
some	O
of	O
the	O
messages	O
used	O
to	O
compute	O
the	O
marginal	O
on	O
one	O
node	O
can	O
be	O
re-used	O
to	O
compute	O
the	O
marginals	O
on	O
the	O
other	O
nodes	B
by	O
storing	O
the	O
messages	O
for	O
later	O
re-use	O
we	O
can	O
compute	O
all	O
the	O
marginals	O
in	O
odk	O
time	O
this	O
is	O
what	O
the	O
up-down	B
algorithm	O
on	O
trees	O
does	O
the	O
question	O
is	O
how	O
can	O
we	O
combine	O
the	O
efficiency	O
of	O
bp	B
on	O
trees	O
with	O
the	O
generality	O
of	O
ve	O
the	O
answer	O
is	O
given	O
in	O
section	O
the	O
junction	B
tree	B
algorithm	I
the	O
junction	B
tree	B
algorithm	I
or	O
jta	B
generalizes	O
bp	B
from	O
trees	O
to	O
arbitrary	O
graphs	O
we	O
sketch	O
the	O
basic	O
idea	O
below	O
for	O
details	O
see	O
e	O
g	O
and	O
friedman	O
creating	O
a	O
junction	B
tree	B
the	O
basic	O
idea	O
behind	O
the	O
jta	B
is	O
this	O
we	O
first	O
run	O
the	O
ve	O
algorithm	O
symbolically	O
adding	O
fill-in	O
edges	B
as	O
we	O
go	O
according	O
to	O
a	O
given	O
elimination	O
ordering	O
the	O
resulting	O
graph	B
will	O
be	O
a	O
chordal	B
graph	B
which	O
means	O
that	O
every	O
undirected	B
cycle	B
x	O
k	O
of	O
length	O
k	O
has	O
a	O
the	O
junction	B
tree	B
algorithm	I
figure	O
of	O
the	O
messages	O
needed	O
to	O
compute	O
all	O
singleton	O
marginals	O
based	O
on	O
figure	O
of	O
sending	O
multiple	O
messages	O
along	O
a	O
tree	B
is	O
root	B
is	O
root	B
is	O
root	B
all	O
figure	O
left	O
this	O
graph	B
is	O
not	O
triangulated	B
despite	O
appearances	O
since	O
it	O
contains	O
a	O
chordless	O
right	O
one	O
possible	O
triangulation	O
by	O
adding	O
the	O
and	O
fill-in	O
edges	B
based	O
on	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
chord	O
i	O
e	O
an	O
edge	O
connects	O
xi	O
xj	O
for	O
all	O
non-adjacent	O
nodes	B
ij	O
in	O
the	O
having	O
created	O
a	O
chordal	B
graph	B
we	O
can	O
extract	O
its	O
maximal	O
cliques	B
in	O
general	O
finding	O
max	O
cliques	B
is	O
computationally	O
hard	O
but	O
it	O
turns	O
out	O
that	O
it	O
can	O
be	O
done	O
efficiently	O
from	O
this	O
special	O
kind	O
of	O
graph	B
figure	O
gives	O
an	O
example	O
where	O
the	O
max	O
cliques	B
are	O
as	O
follows	O
dg	O
i	O
dg	O
s	O
ig	O
j	O
s	O
lh	O
g	O
j	O
note	O
that	O
if	O
the	O
original	O
graphical	B
model	I
was	O
already	O
chordal	B
the	O
elimination	O
process	O
would	O
not	O
add	O
any	O
extra	O
fill-in	O
edges	B
the	O
optimal	O
elimination	O
ordering	O
was	O
used	O
we	O
call	O
such	O
models	O
decomposable	B
since	O
they	O
break	O
into	O
little	O
pieces	O
defined	O
by	O
the	O
cliques	B
it	O
turns	O
out	O
that	O
the	O
cliques	B
of	O
a	O
chordal	B
graph	B
can	O
be	O
arranged	O
into	O
a	O
special	O
kind	O
of	O
tree	B
known	O
as	O
a	O
junction	B
tree	B
this	O
enjoys	O
the	O
running	B
intersection	I
property	I
which	O
means	O
that	O
any	O
subset	O
of	O
nodes	B
containing	O
a	O
given	O
variable	O
forms	O
a	O
connected	O
component	O
figure	O
gives	O
an	O
example	O
of	O
such	O
a	O
tree	B
we	O
see	O
that	O
the	O
node	O
i	O
occurs	O
in	O
two	O
adjacent	O
tree	B
nodes	B
so	O
they	O
can	O
share	O
information	B
about	O
this	O
variable	O
a	O
similar	B
situation	O
holds	O
for	O
all	O
the	O
other	O
variables	O
one	O
can	O
show	O
that	O
if	O
a	O
tree	B
that	O
satisfies	O
the	O
running	B
intersection	I
property	I
then	O
applying	O
bp	B
to	O
this	O
tree	B
we	O
explain	O
below	O
will	O
return	O
the	O
exact	O
values	O
of	O
pxcv	O
for	O
each	O
node	O
c	O
in	O
the	O
tree	B
clique	B
in	O
the	O
induced	O
graph	B
from	O
this	O
we	O
can	O
easily	O
extract	O
the	O
node	O
and	O
edge	O
marginals	O
pxtv	O
and	O
pxs	O
xtv	O
from	O
the	O
original	O
model	O
by	O
marginalizing	O
the	O
clique	B
message	B
passing	I
on	O
a	O
junction	B
tree	B
having	O
constructed	O
a	O
junction	B
tree	B
we	O
can	O
use	O
it	O
for	O
inference	B
the	O
process	O
is	O
very	O
similar	B
to	O
belief	B
propagation	I
on	O
a	O
tree	B
as	O
in	O
section	O
there	O
are	O
two	O
versions	O
the	O
sum-product	B
form	O
also	O
known	O
as	O
the	O
shafer-shenoy	B
algorithm	O
named	O
after	O
and	O
shenoy	O
and	O
the	O
belief	B
updating	I
form	O
involves	O
division	O
also	O
known	O
as	O
the	O
hugin	B
after	O
a	O
company	O
or	O
the	O
lauritzen-spiegelhalter	B
algorithm	O
after	O
and	O
spiegelhalter	O
see	O
and	O
shenoy	O
for	O
a	O
detailed	O
comparison	O
of	O
these	O
methods	O
below	O
we	O
sketch	O
how	O
the	O
hugin	B
algorithm	O
works	O
we	O
assume	O
the	O
original	O
model	O
has	O
the	O
following	O
form	O
cxc	O
px	O
c	O
cg	O
where	O
cg	O
are	O
the	O
cliques	B
of	O
the	O
original	O
graph	B
on	O
the	O
other	O
hand	O
the	O
tree	B
defines	O
a	O
distribution	O
of	O
the	O
following	O
form	O
z	O
px	O
c	O
ct	O
cxc	O
s	O
st	O
sxs	O
the	O
largest	O
loop	B
in	O
a	O
chordal	B
graph	B
is	O
length	O
consequently	O
chordal	B
graphs	O
are	O
sometimes	O
called	O
triangulated	B
however	O
it	O
is	O
not	O
enough	O
for	O
the	O
graph	B
to	O
look	O
like	O
it	O
is	O
made	O
of	O
little	O
triangles	O
for	O
example	O
figure	O
is	O
not	O
chordal	B
even	O
though	O
it	O
is	O
made	O
of	O
little	O
triangles	O
since	O
it	O
contains	O
the	O
chordless	O
if	O
we	O
want	O
the	O
joint	B
distribution	I
of	O
some	O
variables	O
that	O
are	O
not	O
in	O
the	O
same	O
clique	B
a	O
so-called	O
out-of-clique	B
query	I
we	O
can	O
adapt	O
the	O
technique	O
described	O
in	O
section	O
as	O
follows	O
create	O
a	O
mega	O
node	O
containing	O
the	O
query	B
variables	I
and	O
any	O
other	O
nuisance	B
variables	I
that	O
lie	O
on	O
the	O
path	B
between	O
them	O
multiply	O
in	O
messages	O
onto	O
the	O
boundary	O
of	O
the	O
mega	O
node	O
and	O
then	O
marginalize	O
out	O
the	O
internal	O
nuisance	B
variables	I
this	O
internal	O
marginalization	O
may	O
require	O
the	O
use	O
of	O
bp	B
or	O
ve	O
see	O
and	O
friedman	O
for	O
details	O
the	O
junction	B
tree	B
algorithm	I
coherence	O
coherence	O
difficulty	O
intelligence	O
difficulty	O
intelligence	O
grade	O
sat	O
grade	O
sat	O
letter	O
letter	O
job	O
job	O
happy	O
happy	O
cd	B
gid	O
gsi	O
gjsl	O
hgj	O
d	O
gi	O
gs	O
gj	O
the	O
student	O
graph	B
with	O
fill-in	O
edges	B
added	O
figure	O
the	O
junction	B
tree	B
an	O
edge	O
between	O
nodes	B
s	O
and	O
t	O
is	O
labeled	O
by	O
the	O
intersection	O
of	O
the	O
sets	O
on	O
nodes	B
s	O
and	O
t	O
this	O
is	O
called	O
the	O
separating	B
set	I
from	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
the	O
maximal	O
cliques	B
where	O
ct	O
are	O
the	O
nodes	B
of	O
the	O
junction	B
tree	B
are	O
the	O
cliques	B
of	O
the	O
chordal	B
graph	B
and	O
st	O
are	O
the	O
separators	O
of	O
the	O
tree	B
to	O
make	O
these	O
equal	O
we	O
initialize	O
by	O
defining	O
s	O
for	O
all	O
separators	O
and	O
c	O
for	O
all	O
cliques	B
then	O
for	O
each	O
clique	B
in	O
the	O
original	O
model	O
c	O
cg	O
we	O
find	O
a	O
clique	B
in	O
the	O
tree	B
ct	O
which	O
contains	O
it	O
c	O
we	O
then	O
multiply	O
c	O
onto	O
by	O
computing	O
c	O
after	O
doing	O
this	O
for	O
all	O
the	O
cliques	B
in	O
the	O
original	O
graph	B
we	O
have	O
cxc	O
cxc	O
c	O
ct	O
c	O
cg	O
as	O
in	O
section	O
we	O
now	O
send	O
messages	O
from	O
the	O
leaves	B
to	O
the	O
root	B
and	O
back	O
as	O
sketched	O
in	O
figure	O
in	O
the	O
upwards	O
pass	O
also	O
known	O
as	O
the	O
collect-to-root	B
phase	B
node	O
i	O
sends	O
to	O
its	O
parent	O
j	O
the	O
following	O
message	O
mi	B
jsij	O
ici	O
cisij	O
j	O
chi	O
that	O
is	O
we	O
marginalize	O
out	O
the	O
variables	O
that	O
node	O
i	O
knows	O
about	O
which	O
are	O
irrelevant	O
to	O
j	O
and	O
then	O
we	O
send	O
what	O
is	O
left	O
over	O
once	O
a	O
node	O
has	O
received	O
messages	O
from	O
all	O
its	O
children	B
it	O
updates	O
its	O
belief	B
state	B
using	O
ici	O
ici	O
mj	O
isij	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
which	O
is	O
the	O
posterior	O
over	O
the	O
nodes	B
in	O
clique	B
at	O
the	O
root	B
rcr	O
represents	O
pxcr	O
cr	O
conditioned	O
on	O
all	O
the	O
evidence	B
its	O
normalization	O
constant	O
is	O
where	O
is	O
the	O
normalization	O
constant	O
for	O
the	O
unconditional	O
prior	O
px	O
have	O
if	O
the	O
original	O
model	O
was	O
a	O
dgm	B
in	O
the	O
downwards	O
pass	O
also	O
known	O
as	O
the	O
distribute-from-root	B
phase	B
node	O
i	O
sends	O
to	O
its	O
children	B
j	O
the	O
following	O
message	O
ici	O
mi	B
jsij	O
cisij	O
mj	O
isij	O
we	O
divide	O
out	O
by	O
what	O
j	O
sent	O
to	O
i	O
to	O
avoid	O
double	O
counting	O
the	O
evidence	B
this	O
requires	O
that	O
we	O
store	O
the	O
messages	O
from	O
the	O
upwards	O
pass	O
once	O
a	O
node	O
has	O
received	O
a	O
top-down	O
message	O
from	O
its	O
parent	O
it	O
can	O
compute	O
its	O
final	O
belief	B
state	B
using	O
jcj	O
jcjmi	O
jsij	O
an	O
equivalent	O
way	O
to	O
present	O
this	O
algorithm	O
is	O
based	O
on	O
storing	O
the	O
messages	O
inside	O
the	O
separator	O
potentials	O
so	O
on	O
the	O
way	O
up	O
sending	O
from	O
i	O
to	O
j	O
we	O
compute	O
the	O
separator	O
potential	O
and	O
then	O
update	O
the	O
recipient	O
potential	O
ijsij	O
ici	O
cisij	O
j	O
jcj	O
ij	O
cisij	O
ijsij	O
ijsij	O
i	O
and	O
then	O
update	O
the	O
recipient	O
potential	O
j	O
j	O
ij	O
ijsij	O
that	O
we	O
initialize	O
ijsij	O
this	O
is	O
sometimes	O
called	O
passing	O
a	O
flow	O
from	O
i	O
to	O
j	O
on	O
the	O
way	O
down	O
from	O
i	O
to	O
j	O
we	O
compute	O
the	O
separator	O
potential	O
this	O
process	O
is	O
known	O
as	O
junction	B
tree	B
calibration	B
see	O
figure	O
for	O
an	O
illustration	O
its	O
correctness	O
follows	O
from	O
the	O
fact	O
that	O
each	O
edge	O
partitions	O
the	O
evidence	B
into	O
two	O
distinct	O
groups	O
plus	O
the	O
fact	O
that	O
the	O
tree	B
satisfies	O
rip	O
which	O
ensures	O
that	O
no	O
information	B
is	O
lost	O
by	O
only	O
performing	O
local	O
computations	O
example	O
jtree	O
algorithm	O
on	O
a	O
chain	O
it	O
is	O
interesting	O
to	O
see	O
what	O
happens	O
if	O
we	O
apply	O
this	O
process	O
to	O
a	O
chain	O
structured	O
graph	B
such	O
as	O
an	O
hmm	B
a	O
detailed	O
discussion	O
can	O
be	O
found	O
in	O
et	O
al	O
but	O
the	O
basic	O
idea	O
is	O
this	O
the	O
cliques	B
are	O
the	O
edges	B
and	O
the	O
separators	O
are	O
the	O
nodes	B
as	O
shown	O
in	O
figure	O
we	O
initialize	O
the	O
potentials	O
as	O
follows	O
we	O
set	O
s	O
for	O
all	O
the	O
separators	O
we	O
set	O
cxt	O
xt	O
pxtxt	O
for	O
clique	B
c	O
xt	O
and	O
we	O
set	O
cxt	O
yt	O
pytxt	O
for	O
clique	B
c	O
yt	O
the	O
junction	B
tree	B
algorithm	I
figure	O
the	O
junction	B
tree	B
derived	O
from	O
an	O
hmmssm	O
of	O
length	O
t	O
next	O
we	O
send	O
messages	O
from	O
left	O
to	O
right	O
consider	O
clique	B
xt	O
with	O
potential	O
pxtxt	O
it	O
receives	O
a	O
message	O
from	O
clique	B
xt	O
via	O
separator	O
xt	O
of	O
the	O
form	O
pxt	O
xt	O
xt	O
when	O
combined	O
with	O
the	O
clique	B
potential	O
xt	O
this	O
becomes	O
the	O
two-slice	O
predictive	B
density	O
pxtxt	O
pxt	O
the	O
clique	B
xt	O
also	O
receives	O
a	O
message	O
from	O
yt	O
via	O
separator	O
xt	O
of	O
the	O
form	O
pytxt	O
which	O
corresponds	O
to	O
its	O
local	B
evidence	B
when	O
combined	O
with	O
the	O
updated	O
clique	B
potential	O
this	O
becomes	O
the	O
two-slice	O
filtered	O
posterior	O
pxt	O
pxt	O
thus	O
the	O
messages	O
in	O
the	O
forwards	O
pass	O
are	O
the	O
filtered	O
belief	O
states	O
t	O
and	O
the	O
clique	B
potentials	O
are	O
the	O
two-slice	O
distributions	O
in	O
the	O
backwards	O
pass	O
the	O
messages	O
are	O
the	O
update	O
factors	B
t	O
t	O
where	O
tk	O
xt	O
and	O
tk	O
xt	O
by	O
multiplying	O
by	O
this	O
message	O
we	O
swap	O
out	O
the	O
old	O
t	O
message	O
and	O
swap	O
in	O
the	O
new	O
t	O
message	O
we	O
see	O
that	O
the	O
backwards	O
pass	O
involves	O
working	O
with	O
posterior	O
beliefs	O
not	O
conditional	O
likelihoods	O
see	O
section	O
for	O
further	O
discussion	O
of	O
this	O
difference	O
computational	O
complexity	O
of	O
jta	B
if	O
all	O
nodes	B
are	O
discrete	B
with	O
k	O
states	O
each	O
it	O
is	O
clear	O
that	O
the	O
jta	B
takes	O
ock	O
time	O
and	O
space	O
where	O
is	O
the	O
number	O
of	O
cliques	B
and	O
w	O
is	O
the	O
treewidth	B
of	O
the	O
graph	B
i	O
e	O
the	O
size	O
of	O
the	O
largest	O
clique	B
minus	O
unfortunately	O
choosing	O
a	O
triangulation	O
so	O
as	O
to	O
minimize	O
the	O
treewidth	B
is	O
np-hard	B
as	O
explained	O
in	O
section	O
the	O
jta	B
can	O
be	O
modified	O
to	O
handle	O
the	O
case	O
of	O
gaussian	B
graphical	B
models	I
the	O
graph-theoretic	O
steps	O
remain	O
unchanged	O
only	O
the	O
message	O
computation	O
differs	O
we	O
just	O
need	O
to	O
define	O
how	O
to	O
multiply	O
divide	O
and	O
marginalize	O
gaussian	B
potential	O
functions	O
this	O
is	O
most	O
easily	O
done	O
in	O
information	B
form	I
see	O
e	O
g	O
murphy	O
cemgil	O
for	O
the	O
details	O
the	O
algorithm	O
takes	O
time	O
and	O
space	O
when	O
applied	O
to	O
a	O
chain	O
structured	O
graph	B
the	O
algorithm	O
is	O
equivalent	O
to	O
the	O
kalman	B
smoother	I
in	O
section	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
qn	O
cm	O
am	O
cm	O
x	O
figure	O
encoding	O
a	O
problem	O
on	O
n	O
variables	O
and	O
m	O
clauses	O
as	O
a	O
dgm	B
the	O
qs	O
variables	O
are	O
binary	O
random	O
variables	O
the	O
ct	O
variables	O
are	O
deterministic	O
functions	O
of	O
the	O
qs	O
s	O
and	O
compute	O
the	O
truth	O
value	O
of	O
each	O
clause	B
the	O
at	O
nodes	B
are	O
a	O
chain	O
of	O
and	O
gates	O
to	O
ensure	O
that	O
the	O
cpt	O
for	O
the	O
final	O
x	O
node	O
has	O
bounded	O
size	O
the	O
double	O
rings	O
denote	O
nodes	B
with	O
deterministic	O
cpds	O
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
jta	B
generalizations	O
we	O
have	O
seen	O
how	O
to	O
use	O
the	O
jta	B
algorithm	O
to	O
compute	O
posterior	O
marginals	O
in	O
a	O
graphical	B
model	I
there	O
are	O
several	O
possible	O
generalizations	O
of	O
this	O
algorithm	O
some	O
of	O
which	O
we	O
mention	O
below	O
all	O
of	O
these	O
exploit	O
graph	B
decomposition	O
in	O
some	O
form	O
or	O
other	O
they	O
only	O
differ	O
in	O
terms	O
of	O
how	O
they	O
define	O
compute	O
messages	O
and	O
beliefs	O
the	O
key	O
requirement	O
is	O
that	O
the	O
operators	O
which	O
compute	O
messages	O
form	O
a	O
commutative	B
semiring	I
section	O
computing	O
the	O
map	B
estimate	I
we	O
just	O
replace	O
the	O
sum-product	B
with	O
max-product	B
in	O
the	O
collect	O
phase	B
and	O
use	O
traceback	B
in	O
the	O
distribute	O
phase	B
as	O
in	O
the	O
viterbi	B
algorithm	O
see	O
for	O
details	O
computing	O
the	O
n-most	O
probable	O
configurations	O
computing	O
posterior	O
samples	B
the	O
collect	O
pass	O
is	O
the	O
same	O
as	O
usual	O
but	O
in	O
the	O
distribute	O
pass	O
we	O
sample	O
variables	O
given	O
the	O
values	O
higher	O
up	O
in	O
the	O
tree	B
thus	O
generalizing	O
forwardsfiltering	O
backwards-sampling	O
for	O
hmms	B
described	O
in	O
section	O
see	O
for	O
details	O
solving	O
constraint	B
satisfaction	I
problems	I
solving	O
logical	B
reasoning	I
problems	I
and	O
mcilraith	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
as	O
we	O
saw	O
in	O
sections	O
and	O
ve	O
and	O
jta	B
take	O
time	O
that	O
is	O
exponential	O
in	O
the	O
treewidth	B
of	O
a	O
graph	B
since	O
the	O
treewidth	B
can	O
be	O
onumber	O
of	O
nodes	B
in	O
the	O
worst	O
case	O
this	O
means	O
these	O
algorithms	O
can	O
be	O
exponential	O
in	O
the	O
problem	O
size	O
of	O
course	O
just	O
because	O
ve	O
and	O
jta	B
are	O
slow	O
doesn	O
t	O
mean	B
that	O
there	O
isn	O
t	O
some	O
smarter	O
algorithm	O
out	O
there	O
unfortunately	O
this	O
seems	O
unlikely	O
since	O
it	O
is	O
easy	O
to	O
show	O
that	O
exact	O
inference	B
is	O
np-hard	B
and	O
luby	O
the	O
proof	O
is	O
a	O
simple	O
reduction	O
from	O
the	O
satisfiability	O
prob	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
method	O
forwards-backwards	B
belief	B
propagation	I
variable	B
elimination	I
junction	B
tree	B
algorithm	I
loopy	B
belief	B
propagation	I
convex	B
belief	B
propagation	I
mean	B
field	O
gibbs	B
sampling	I
restriction	O
chains	O
d	O
or	O
lg	O
trees	O
d	O
or	O
lg	O
low	O
treewidth	B
d	O
or	O
lg	O
single	O
query	O
low	O
treewidth	B
d	O
or	O
lg	O
approximate	O
d	O
or	O
lg	O
approximate	O
d	O
or	O
lg	O
approximate	O
c-e	O
approximate	O
section	O
section	O
section	O
section	O
section	O
section	O
section	O
section	O
section	O
table	O
summary	O
of	O
some	O
methods	O
that	O
can	O
be	O
used	O
for	O
inference	B
in	O
graphical	B
models	I
d	O
means	O
that	O
all	O
the	O
hidden	B
variables	I
must	O
be	O
discrete	B
l-g	O
means	O
that	O
all	O
the	O
factors	B
must	O
be	O
linear-gaussian	O
the	O
term	O
single	O
query	O
refers	O
to	O
the	O
restriction	O
that	O
ve	O
only	O
computes	O
one	O
marginal	O
pxqxv	O
at	O
a	O
time	O
see	O
section	O
for	O
a	O
discussion	O
of	O
this	O
point	O
c-e	O
stands	O
for	O
conjugate	O
exponential	O
this	O
means	O
that	O
variational	O
mean	B
field	O
only	O
applies	O
to	O
models	O
where	O
the	O
likelihood	B
is	O
in	O
the	O
exponential	B
family	B
and	O
the	O
prior	O
is	O
conjugate	O
this	O
includes	O
the	O
d	O
and	O
lg	O
case	O
but	O
many	O
others	O
as	O
well	O
as	O
we	O
will	O
see	O
in	O
section	O
in	O
particular	O
note	O
that	O
we	O
can	O
encode	O
any	O
as	O
a	O
dgm	B
with	O
deterministic	O
lem	O
links	O
as	O
shown	O
in	O
figure	O
we	O
clamp	O
the	O
final	O
node	O
x	O
to	O
be	O
on	O
and	O
we	O
arrange	O
the	O
cpts	B
so	O
that	O
px	O
iff	B
there	O
a	O
satisfying	B
assignment	I
computing	O
any	O
posterior	O
marginal	O
requires	O
evaluating	O
the	O
normalization	O
constant	O
px	O
which	O
represents	O
the	O
probability	B
of	I
the	I
evidence	B
so	O
inference	B
in	O
this	O
model	O
implicitly	O
solves	O
the	O
sat	O
problem	O
in	O
fact	O
exact	O
inference	B
is	O
which	O
is	O
even	O
harder	O
than	O
np-hard	B
e	O
g	O
and	O
barak	O
for	O
definitions	O
of	O
these	O
terms	O
the	O
intuitive	O
reason	O
for	O
this	O
is	O
that	O
to	O
compute	O
the	O
normalizing	O
constant	O
z	O
we	O
have	O
tocount	O
how	O
many	O
satisfying	O
assignments	O
there	O
are	O
by	O
contrast	O
map	O
estimation	O
is	O
provably	O
easier	O
for	O
some	O
model	O
classes	O
et	O
al	O
since	O
intuitively	O
speaking	O
it	O
only	O
requires	O
finding	O
one	O
satisfying	B
assignment	I
not	O
counting	O
all	O
of	O
them	O
approximate	B
inference	B
many	O
popular	O
probabilistic	O
models	O
support	B
efficient	O
exact	O
inference	B
since	O
they	O
are	O
based	O
on	O
chains	O
trees	O
or	O
low	O
treewidth	B
graphs	O
but	O
there	O
are	O
many	O
other	O
models	O
for	O
which	O
exact	O
in	O
fact	O
even	O
simple	O
two	O
node	O
models	O
of	O
the	O
form	O
x	O
may	O
not	O
inference	B
is	O
intractable	O
support	B
exact	O
inference	B
if	O
the	O
prior	O
on	O
is	O
not	O
conjugate	O
to	O
the	O
likelihood	B
px	O
therefore	O
we	O
will	O
need	O
to	O
turn	O
to	O
approximate	B
inference	B
methods	O
see	O
table	O
for	O
a	O
summary	O
of	O
coming	O
attractions	O
for	O
the	O
most	O
part	O
these	O
methods	O
do	O
not	O
come	O
with	O
any	O
guarantee	O
as	O
to	O
their	O
accuracy	O
or	O
running	O
time	O
theoretical	O
computer	O
scientists	O
would	O
therefore	O
describe	O
them	O
as	O
heuristics	B
rather	O
than	O
approximation	O
algorithms	O
in	O
fact	O
one	O
can	O
prove	O
that	O
a	O
problem	O
is	O
a	O
logical	O
expression	O
of	O
the	O
form	O
where	O
the	O
qi	O
are	O
binary	O
variables	O
and	O
each	O
clause	B
consists	O
of	O
the	O
conjunction	O
of	O
three	O
variables	O
their	O
negation	O
the	O
goal	O
is	O
to	O
find	O
a	O
satisfying	B
assignment	I
which	O
is	O
a	O
set	O
of	O
values	O
for	O
the	O
qi	O
variables	O
such	O
that	O
the	O
expression	O
evaluates	O
to	O
true	O
for	O
discrete	B
random	O
variables	O
conjugacy	O
is	O
not	O
a	O
concern	O
since	O
discrete	B
distributions	O
are	O
always	O
closed	O
under	O
conditioning	B
and	O
marginalization	O
consequently	O
graph-theoretic	O
considerations	O
are	O
of	O
more	O
importance	O
when	O
discussing	O
inference	B
in	O
models	O
with	O
discrete	B
hidden	B
states	O
chapter	O
exact	O
inference	B
for	O
graphical	B
models	I
it	O
is	O
not	O
possible	O
to	O
construct	O
polynomial	B
time	I
approximation	I
schemes	I
for	O
inference	B
in	O
general	O
discrete	B
gms	O
and	O
luby	O
roth	O
fortunately	O
we	O
will	O
see	O
that	O
for	O
many	O
of	O
these	O
heuristic	O
methods	O
often	O
perform	O
well	O
in	O
practice	O
exercises	O
exercise	O
variable	B
elimination	I
consider	O
the	O
mrf	B
in	O
figure	O
a	O
suppose	O
we	O
want	O
to	O
compute	O
the	O
partition	B
function	I
using	O
the	O
elimination	O
ordering	O
if	O
we	O
use	O
the	O
variable	B
elimination	I
algorithm	O
we	O
will	O
create	O
new	O
intermediate	O
factors	B
what	O
is	O
the	O
largest	O
intermediate	O
factor	B
b	O
add	O
an	O
edge	O
to	O
the	O
original	O
mrf	B
between	O
every	O
pair	O
of	O
variables	O
that	O
end	O
up	O
in	O
the	O
same	O
factor	B
are	O
called	O
fill	O
in	O
edges	B
draw	O
the	O
resulting	O
mrf	B
what	O
is	O
the	O
size	O
of	O
the	O
largest	O
maximal	B
clique	B
in	O
this	O
graph	B
c	O
now	O
consider	O
elimination	O
ordering	O
i	O
e	O
if	O
we	O
use	O
the	O
variable	B
elimination	I
algorithm	O
we	O
will	O
create	O
new	O
intermediate	O
factors	B
what	O
is	O
the	O
largest	O
intermediate	O
factor	B
d	O
add	O
an	O
edge	O
to	O
the	O
original	O
mrf	B
between	O
every	O
pair	O
of	O
variables	O
that	O
end	O
up	O
in	O
the	O
same	O
factor	B
are	O
called	O
fill	O
in	O
edges	B
draw	O
the	O
resulting	O
mrf	B
what	O
is	O
the	O
size	O
of	O
the	O
largest	O
maximal	B
clique	B
in	O
this	O
graph	B
exercise	O
gaussian	B
times	O
gaussian	B
is	O
gaussian	B
prove	O
equation	O
hint	O
use	O
completing	B
the	I
square	I
exercise	O
message	B
passing	I
on	O
a	O
tree	B
consider	O
the	O
dgm	B
in	O
figure	O
which	O
represents	O
the	O
following	O
fictitious	O
biological	O
model	O
each	O
gi	O
represents	O
the	O
genotype	B
of	O
a	O
person	O
gi	O
if	O
they	O
have	O
a	O
healthy	O
gene	O
and	O
gi	O
if	O
they	O
have	O
an	O
unhealthy	O
gene	O
and	O
may	O
inherit	O
the	O
unhealthy	O
gene	O
from	O
their	O
parent	O
xi	O
r	O
is	O
a	O
continuous	O
measure	O
of	O
blood	O
pressure	O
which	O
is	O
low	O
if	O
you	O
are	O
healthy	O
and	O
high	O
if	O
you	O
are	O
unhealthy	O
we	O
define	O
the	O
cpds	O
as	O
follows	O
pxigi	O
n	O
pxigi	O
n	O
the	O
meaning	O
of	O
the	O
matrix	O
for	O
is	O
that	O
etc	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
figure	O
a	O
simple	O
dag	B
representing	O
inherited	O
diseases	O
a	O
suppose	O
you	O
observe	O
and	O
is	O
unobserved	O
what	O
is	O
the	O
posterior	O
belief	O
on	O
i	O
e	O
b	O
now	O
suppose	O
you	O
observe	O
amd	O
what	O
is	O
explain	O
your	O
answer	O
c	O
now	O
suppose	O
what	O
is	O
explain	O
your	O
answer	O
intuitively	O
d	O
now	O
suppose	O
what	O
is	O
explain	O
your	O
answer	O
intuitively	O
intuitively	O
exercise	O
inference	B
in	O
lattice	B
mrfs	O
consider	O
an	O
mrf	B
with	O
a	O
m	O
n	O
lattice	B
graph	B
structure	O
so	O
each	O
hidden	B
node	O
xij	O
is	O
connected	O
to	O
its	O
nearest	O
neighbors	B
as	O
in	O
an	O
ising	B
model	I
in	O
addition	O
each	O
hidden	B
node	O
has	O
its	O
own	O
local	B
evidence	B
yij	O
assume	O
all	O
hidden	B
nodes	B
have	O
k	O
states	O
in	O
general	O
exact	O
inference	B
in	O
such	O
models	O
is	O
intractable	O
because	O
the	O
maximum	O
cliques	B
of	O
the	O
corresponding	O
triangulated	B
graph	B
have	O
size	O
omaxm	O
n	O
suppose	O
m	O
n	O
i	O
e	O
the	O
lattice	B
is	O
short	O
and	O
fat	O
a	O
how	O
can	O
one	O
efficiently	O
perform	O
exact	O
inference	B
a	O
deterministic	O
algorithm	O
in	O
such	O
models	O
exact	O
inference	B
i	O
mean	B
computing	O
marginal	O
probabilities	O
p	O
exactly	O
where	O
is	O
all	O
the	O
evidence	B
give	O
a	O
brief	O
description	O
of	O
your	O
method	O
b	O
what	O
is	O
the	O
asymptotic	O
complexity	O
time	O
of	O
your	O
algorithm	O
c	O
now	O
suppose	O
the	O
lattice	B
is	O
large	O
and	O
square	O
so	O
m	O
n	O
but	O
all	O
hidden	B
states	O
are	O
binary	O
k	O
in	O
this	O
case	O
how	O
can	O
one	O
efficiently	O
exactly	O
compute	O
a	O
deterministic	O
algorithm	O
the	O
map	B
estimate	I
arg	O
maxx	O
p	O
wherex	O
is	O
the	O
joint	O
assignment	O
to	O
all	O
hidden	B
nodes	B
variational	B
inference	B
introduction	O
we	O
have	O
now	O
seen	O
several	O
algorithms	O
for	O
computing	O
of	O
a	O
posterior	O
distribution	O
for	O
discrete	B
graphical	B
models	I
we	O
can	O
use	O
the	O
junction	B
tree	B
algorithm	I
to	O
perform	O
exact	O
inference	B
as	O
explained	O
in	O
section	O
however	O
this	O
takes	O
time	O
exponential	O
in	O
the	O
treewidth	B
of	O
the	O
graph	B
rendering	O
exact	O
inference	B
often	O
impractical	O
for	O
the	O
case	O
of	O
gaussian	B
graphical	B
models	I
exact	O
inference	B
is	O
cubic	O
in	O
the	O
treewidth	B
however	O
even	O
this	O
can	O
be	O
too	O
slow	O
if	O
we	O
have	O
many	O
variables	O
in	O
addition	O
the	O
jta	B
does	O
not	O
work	O
for	O
continuous	O
random	O
variables	O
outside	O
of	O
the	O
gaussian	B
case	O
nor	O
for	O
mixed	O
discrete-continuous	O
variables	O
outside	O
of	O
the	O
conditionally	O
gaussian	B
case	O
for	O
some	O
simple	O
two	O
node	O
graphical	B
models	I
of	O
the	O
form	O
x	O
d	O
we	O
can	O
compute	O
the	O
exact	O
posterior	O
pxd	O
in	O
closed	O
form	O
provided	O
the	O
prior	O
px	O
is	O
conjugate	O
to	O
the	O
likelihood	B
pdx	O
means	O
the	O
likelihood	B
must	O
be	O
in	O
the	O
exponential	B
family	B
see	O
chapter	O
for	O
some	O
that	O
in	O
this	O
chapter	O
x	O
represent	O
the	O
unknown	B
variables	O
whereas	O
in	O
examples	O
of	O
this	O
chapter	O
we	O
used	O
to	O
represent	O
the	O
unknowns	O
in	O
more	O
general	O
settings	O
we	O
must	O
use	O
approximate	B
inference	B
methods	O
in	O
section	O
we	O
discussed	O
the	O
gaussian	B
approximation	I
which	O
is	O
useful	O
for	O
inference	B
in	O
two	O
node	O
models	O
of	O
the	O
form	O
x	O
d	O
where	O
the	O
prior	O
is	O
not	O
conjugate	O
example	O
section	O
applied	O
the	O
method	O
to	O
logistic	B
regression	B
the	O
gaussian	B
approximation	I
is	O
simple	O
however	O
some	O
posteriors	O
are	O
not	O
naturally	O
modelled	O
using	O
gaussians	O
for	O
example	O
when	O
inferring	O
multinomial	B
parameters	O
a	O
dirichlet	B
distribution	I
is	O
a	O
better	O
choice	O
and	O
when	O
inferring	O
states	O
in	O
a	O
discrete	B
graphical	B
model	I
a	O
categorical	B
distribution	O
is	O
a	O
better	O
choice	O
in	O
this	O
chapter	O
we	O
will	O
study	O
a	O
more	O
general	O
class	O
of	O
deterministic	O
approximate	B
inference	B
algorithms	O
based	O
on	O
variational	B
inference	B
et	O
al	O
jaakkola	O
and	O
jordan	O
jaakkola	O
wainwright	O
and	O
jordan	O
the	O
basic	O
idea	O
is	O
to	O
pick	O
an	O
approximation	O
qx	O
to	O
the	O
distribution	O
from	O
some	O
tractable	O
family	B
and	O
then	O
to	O
try	O
to	O
make	O
this	O
approximation	O
as	O
close	O
pxd	O
this	O
reduces	O
inference	B
to	O
an	O
optimization	B
as	O
possible	O
to	O
the	O
true	O
posterior	O
p	O
problem	O
by	O
relaxing	O
the	O
constraints	O
andor	O
approximating	O
the	O
objective	O
we	O
can	O
trade	O
accuracy	O
for	O
speed	O
the	O
bottom	O
line	O
is	O
that	O
variational	B
inference	B
often	O
gives	O
us	O
the	O
speed	O
benefits	O
of	O
map	O
estimation	O
but	O
the	O
statistical	O
benefits	O
of	O
the	O
bayesian	B
approach	O
chapter	O
variational	B
inference	B
variational	B
inference	B
suppose	O
p	O
is	O
our	O
true	O
but	O
intractable	O
distribution	O
and	O
qx	O
is	O
some	O
approximation	O
chosen	O
from	O
some	O
tractable	O
family	B
such	O
as	O
a	O
multivariate	B
gaussian	B
or	O
a	O
factored	O
distribution	O
we	O
assume	O
q	O
has	O
some	O
free	O
parameters	O
which	O
we	O
want	O
to	O
optimize	O
so	O
as	O
to	O
make	O
q	O
similar	B
to	O
p	O
an	O
obvious	O
cost	O
function	O
to	O
try	O
to	O
minimize	O
is	O
the	O
kl	B
divergence	I
kl	O
log	O
p	O
p	O
qx	O
however	O
this	O
is	O
hard	O
to	O
compute	O
since	O
taking	O
expectations	O
wrt	O
p	O
a	O
natural	O
alternative	O
is	O
the	O
reverse	B
kl	B
divergence	I
x	O
kl	O
qx	O
log	O
qx	O
p	O
x	O
is	O
assumed	O
to	O
be	O
intractable	O
the	O
main	O
advantage	O
of	O
this	O
objective	O
is	O
that	O
computing	O
expectations	O
wrt	O
q	O
is	O
tractable	O
choosing	O
a	O
suitable	O
form	O
for	O
q	O
we	O
discuss	O
the	O
statistical	O
differences	O
between	O
these	O
two	O
objectives	O
in	O
section	O
unfortunately	O
equation	O
is	O
still	O
not	O
tractable	O
as	O
written	O
since	O
even	O
evaluating	O
p	O
pxd	O
pointwise	O
is	O
hard	O
since	O
it	O
requires	O
evaluating	O
the	O
intractable	O
normalization	O
constant	O
z	O
pd	O
however	O
usually	O
the	O
unnormalized	O
distribution	O
px	O
pxd	O
is	O
tractable	O
to	O
compute	O
we	O
therefore	O
define	O
our	O
new	O
objective	O
function	O
as	O
follows	O
jq	O
kl	O
p	O
where	O
we	O
are	O
slightly	O
abusing	O
notation	O
since	O
p	O
is	O
not	O
a	O
normalized	O
distribution	O
plugging	O
in	O
the	O
definition	O
of	O
kl	O
we	O
get	O
jq	O
x	O
x	O
qx	O
log	O
qx	O
px	O
qx	O
log	O
qx	O
zp	O
qx	O
p	O
log	O
z	O
qx	O
log	O
kl	O
x	O
log	O
z	O
since	O
z	O
is	O
a	O
constant	O
by	O
minimizing	O
jq	O
we	O
will	O
force	O
q	O
to	O
become	O
close	O
to	O
p	O
log	O
likelihood	B
since	O
kl	B
divergence	I
is	O
always	O
non-negative	O
we	O
see	O
that	O
jq	O
is	O
an	O
upper	O
bound	O
on	O
the	O
nll	B
jq	O
kl	O
log	O
z	O
log	O
z	O
log	O
pd	O
alternatively	O
we	O
can	O
try	O
to	O
maximize	O
the	O
following	O
quantity	O
and	O
friedman	O
this	O
is	O
referred	O
to	O
as	O
the	O
energy	B
functional	I
which	O
is	O
a	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	B
of	O
the	O
data	O
lq	O
jq	O
kl	O
log	O
z	O
log	O
z	O
log	O
pd	O
we	O
see	O
that	O
variational	B
inference	B
is	O
closely	O
related	O
to	O
em	B
since	O
this	O
bound	O
is	O
tight	O
when	O
q	O
p	O
section	O
variational	B
inference	B
alternative	O
interpretations	O
of	O
the	O
variational	O
objective	O
there	O
are	O
several	O
equivalent	O
ways	O
of	O
writing	O
this	O
objective	O
that	O
provide	O
different	O
insights	O
one	O
formulation	O
is	O
as	O
follows	O
jq	O
eq	O
qx	O
eq	O
log	O
px	O
h	O
q	O
which	O
is	O
the	O
expected	O
energy	O
ex	O
log	O
px	O
minus	O
the	O
entropy	B
of	O
the	O
system	O
statistical	O
physics	O
jq	O
is	O
called	O
the	O
variational	B
free	B
energy	I
or	O
the	O
helmholtz	O
free	O
in	O
another	O
formulation	O
of	O
the	O
objective	O
is	O
as	O
follows	O
jq	O
q	O
qx	O
log	O
pxpdx	O
eq	O
qx	O
log	O
px	O
log	O
pdx	O
eq	O
log	O
pdx	O
kl	O
this	O
is	O
the	O
expected	O
nll	B
plus	O
a	O
penalty	O
term	O
that	O
measures	O
how	O
far	O
the	O
approximate	O
posterior	O
is	O
from	O
the	O
exact	O
prior	O
we	O
can	O
also	O
interpret	O
the	O
variational	O
objective	O
from	O
the	O
point	O
of	O
view	O
of	O
information	B
theory	I
so-called	O
bits-back	B
argument	O
see	O
and	O
camp	O
honkela	O
and	O
valpola	O
for	O
details	O
forward	O
or	O
reverse	B
kl	I
since	O
the	O
kl	B
divergence	I
is	O
not	O
symmetric	B
in	O
its	O
arguments	O
minimizing	O
kl	O
wrt	O
q	O
will	O
give	O
different	O
behavior	O
than	O
minimizing	O
kl	O
below	O
we	O
discuss	O
these	O
two	O
different	O
methods	O
first	O
consider	O
the	O
reverse	B
kl	I
kl	O
also	O
known	O
as	O
an	O
i-projection	B
or	O
information	B
projection	B
by	O
definition	O
we	O
have	O
kl	O
qx	O
ln	O
qx	O
px	O
x	O
x	O
this	O
is	O
infinite	O
if	O
px	O
and	O
qx	O
thus	O
if	O
px	O
we	O
must	O
ensure	O
qx	O
we	O
say	O
that	O
the	O
reverse	B
kl	I
is	O
zero	B
forcing	I
for	O
q	O
hence	O
q	O
will	O
typically	O
under-estimate	O
the	O
support	B
of	O
p	O
now	O
consider	O
the	O
forwards	B
kl	I
also	O
known	O
as	O
an	O
m-projection	B
or	O
moment	B
projection	B
kl	O
px	O
ln	O
px	O
qx	O
this	O
is	O
infinite	O
if	O
qx	O
and	O
px	O
so	O
if	O
px	O
we	O
must	O
ensure	O
qx	O
we	O
say	O
that	O
the	O
forwards	B
kl	I
is	O
zero	B
avoiding	I
for	O
q	O
hence	O
q	O
will	O
typically	O
over-estimate	O
the	O
support	B
of	O
p	O
the	O
difference	O
between	O
these	O
methods	O
is	O
illustrated	O
in	O
figure	O
we	O
see	O
that	O
when	O
the	O
true	O
distribution	O
is	O
multimodal	O
using	O
the	O
forwards	B
kl	I
is	O
a	O
bad	O
idea	O
q	O
is	O
constrained	O
to	O
be	O
unimodal	O
since	O
the	O
resulting	O
posterior	O
modemean	O
will	O
be	O
in	O
a	O
region	O
of	O
low	O
density	O
right	O
between	O
the	O
two	O
peaks	O
in	O
such	O
contexts	O
the	O
reverse	B
kl	I
is	O
not	O
only	O
more	O
tractable	O
to	O
compute	O
but	O
also	O
more	O
sensible	O
statistically	O
chapter	O
variational	B
inference	B
figure	O
illustrating	O
forwards	O
vs	O
reverse	B
kl	I
on	O
a	O
bimodal	O
distribution	O
the	O
blue	O
curves	O
are	O
the	O
contours	O
of	O
the	O
true	O
distribution	O
p	O
the	O
red	O
curves	O
are	O
the	O
contours	O
of	O
the	O
unimodal	O
approximation	O
q	O
minimizing	O
forwards	B
kl	I
q	O
tends	O
to	O
cover	O
p	O
minimizing	O
reverse	B
kl	I
q	O
locks	O
on	O
to	O
one	O
of	O
the	O
two	O
modes	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
klfwdreversemixgauss	O
figure	O
illustrating	O
forwards	O
vs	O
reverse	B
kl	I
on	O
a	O
symmetric	B
gaussian	B
the	O
blue	O
curves	O
are	O
the	O
contours	O
of	O
the	O
true	O
distribution	O
p	O
the	O
red	O
curves	O
are	O
the	O
contours	O
of	O
a	O
factorized	O
approximation	O
q	O
minimizing	O
kl	O
minimizing	O
kl	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
klpqgauss	O
another	O
example	O
of	O
the	O
difference	O
is	O
shown	O
in	O
figure	O
where	O
the	O
target	O
distribution	O
is	O
an	O
elongated	O
gaussian	B
and	O
the	O
approximating	O
distribution	O
is	O
a	O
product	O
of	O
two	O
gaussians	O
that	O
is	O
px	O
n	O
where	O
in	O
figure	O
we	O
show	O
the	O
result	O
of	O
minimizing	O
kl	O
in	O
this	O
simple	O
example	O
one	O
can	O
show	O
that	O
the	O
solution	O
has	O
the	O
form	O
qx	O
it	O
is	O
called	O
free	O
because	O
the	O
variables	O
x	O
are	O
free	O
to	O
vary	O
rather	O
than	O
being	O
fixed	O
the	O
variational	B
free	B
energy	I
is	O
a	O
function	O
of	O
the	O
distribution	O
q	O
whereas	O
the	O
regular	B
energy	O
is	O
a	O
function	O
of	O
the	O
state	B
vector	O
x	O
the	O
mean	B
field	O
method	O
its	O
variance	B
is	O
controlled	O
by	O
the	O
direction	O
of	O
smallest	O
variance	B
of	O
p	O
figure	O
shows	O
that	O
we	O
have	O
correctly	O
captured	O
the	O
mean	B
but	O
the	O
approximation	O
is	O
too	O
compact	O
in	O
fact	O
it	O
is	O
often	O
the	O
case	O
not	O
always	O
et	O
al	O
that	O
minimizing	O
kl	O
where	O
q	O
is	O
factorized	O
results	O
in	O
an	O
approximation	O
that	O
is	O
overconfident	O
in	O
figure	O
we	O
show	O
the	O
result	O
of	O
minimizing	O
kl	O
as	O
we	O
show	O
in	O
exercise	O
the	O
optimal	O
solution	O
when	O
minimizing	O
the	O
forward	O
kl	O
wrt	O
a	O
factored	O
approximation	O
is	O
to	O
set	O
q	O
to	O
be	O
the	O
product	O
of	O
marginals	O
thus	O
the	O
solution	O
has	O
the	O
form	O
qx	O
n	O
figure	O
shows	O
that	O
this	O
is	O
too	O
broad	O
since	O
it	O
is	O
an	O
over-estimate	O
of	O
the	O
support	B
of	O
p	O
for	O
the	O
rest	O
of	O
this	O
chapter	O
and	O
for	O
most	O
of	O
the	O
next	O
we	O
will	O
focus	O
on	O
minimizing	O
kl	O
in	O
section	O
when	O
we	O
discuss	O
expectation	B
proagation	I
we	O
will	O
discuss	O
ways	O
to	O
locally	O
optimize	O
kl	O
one	O
can	O
create	O
a	O
family	B
of	O
divergence	O
measures	O
indexed	O
by	O
a	O
parameter	B
r	O
by	O
defining	O
the	O
alpha	B
divergence	I
as	O
follows	O
d	O
this	O
measure	O
satisfies	O
d	O
iff	B
p	O
q	O
but	O
is	O
obviously	O
not	O
symmetric	B
and	O
hence	O
is	O
not	O
a	O
metric	B
kl	O
corresponds	O
to	O
the	O
limit	O
whereas	O
kl	O
corresponds	O
to	O
the	O
limit	O
when	O
we	O
get	O
a	O
symmetric	B
divergence	O
measure	O
that	O
is	O
linearly	O
related	O
to	O
the	O
hellinger	B
distance	I
defined	O
by	O
qx	O
px	O
dx	O
dh	O
is	O
a	O
valid	O
distance	O
metric	B
that	O
is	O
note	O
that	O
satisfies	O
the	O
triangle	B
inequality	I
see	O
for	O
details	O
it	O
is	O
symmetric	B
non-negative	O
and	O
dh	O
the	O
mean	B
field	O
method	O
one	O
of	O
the	O
most	O
popular	O
forms	O
of	O
variational	B
inference	B
is	O
called	O
the	O
mean	B
field	O
approximation	O
and	O
saad	O
in	O
this	O
approach	O
we	O
assume	O
the	O
posterior	O
is	O
a	O
fully	O
factorized	O
approximation	O
of	O
the	O
form	O
qx	O
qixi	O
i	O
our	O
goal	O
is	O
to	O
solve	O
this	O
optimization	B
problem	O
kl	O
min	O
where	O
we	O
optimize	O
over	O
the	O
parameters	O
of	O
each	O
marginal	B
distribution	I
qi	O
derive	O
a	O
coordinate	O
descent	O
method	O
where	O
at	O
each	O
step	O
we	O
make	O
the	O
following	O
update	O
in	O
section	O
we	O
log	O
qjxj	O
e	O
qj	O
px	O
const	O
chapter	O
variational	B
inference	B
model	O
ising	B
model	I
factorial	B
hmm	B
univariate	O
gaussian	B
linear	B
regression	B
logistic	B
regression	B
mixtures	O
of	O
gaussians	O
latent	B
dirichlet	B
allocation	I
section	O
section	O
section	O
section	O
section	O
section	O
section	O
section	O
table	O
algorithm	O
some	O
models	O
in	O
this	O
book	O
for	O
which	O
we	O
provide	O
detailed	O
derivations	O
of	O
the	O
mean	B
field	O
inference	B
where	O
px	O
xd	O
is	O
the	O
unnormalized	O
posterior	O
and	O
the	O
notation	O
e	O
qj	O
means	O
to	O
take	O
the	O
expectation	O
over	O
f	O
with	O
respect	O
to	O
all	O
the	O
variables	O
except	O
for	O
xj	O
for	O
example	O
if	O
we	O
have	O
three	O
variables	O
then	O
e	O
where	O
sums	O
get	O
replaced	O
by	O
integrals	O
where	O
necessary	O
when	O
updating	O
qj	O
we	O
only	O
need	O
to	O
reason	O
about	O
the	O
variables	O
which	O
share	O
a	O
factor	B
with	O
xj	O
i	O
e	O
the	O
terms	O
in	O
j	O
s	O
markov	B
blanket	I
section	O
the	O
other	O
terms	O
get	O
absorbed	O
into	O
the	O
constant	O
term	O
since	O
we	O
are	O
replacing	O
the	O
neighboring	O
values	O
by	O
their	O
mean	B
value	O
the	O
method	O
is	O
known	O
as	O
mean	B
field	O
this	O
is	O
very	O
similar	B
to	O
gibbs	B
sampling	I
except	O
instead	O
of	O
sending	O
sampled	O
values	O
between	O
neighboring	O
nodes	B
we	O
send	O
mean	B
values	O
between	O
nodes	B
this	O
tends	O
to	O
be	O
more	O
efficient	O
since	O
the	O
mean	B
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
a	O
large	O
number	O
of	O
samples	B
the	O
other	O
hand	O
mean	B
field	O
messages	O
are	O
dense	O
whereas	O
samples	B
are	O
sparse	B
this	O
can	O
make	O
sampling	O
more	O
scalable	O
to	O
very	O
large	O
models	O
of	O
course	O
updating	O
one	O
distribution	O
at	O
a	O
time	O
can	O
be	O
slow	O
since	O
it	O
is	O
a	O
form	O
of	O
coordinate	O
descent	O
several	O
methods	O
have	O
been	O
proposed	O
to	O
speed	O
up	O
this	O
basic	O
approach	O
including	O
using	O
pattern	B
search	I
et	O
al	O
and	O
techniques	O
based	O
on	O
parameter	B
expansion	I
and	O
jaakkola	O
however	O
we	O
will	O
not	O
consider	O
these	O
methods	O
in	O
this	O
chapter	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
mean	B
field	O
method	O
can	O
be	O
used	O
to	O
infer	O
discrete	B
or	O
continuous	O
latent	B
quantities	O
using	O
a	O
variety	O
of	O
parametric	O
forms	O
for	O
qi	O
as	O
we	O
will	O
see	O
below	O
this	O
is	O
in	O
contrast	O
to	O
some	O
of	O
the	O
other	O
variational	O
methods	O
we	O
will	O
encounter	O
later	O
which	O
are	O
more	O
restricted	O
in	O
their	O
applicability	O
table	O
lists	O
some	O
of	O
the	O
examples	O
of	O
mean	B
field	O
that	O
we	O
cover	O
in	O
this	O
book	O
derivation	O
of	O
the	O
mean	B
field	O
update	O
equations	O
recall	B
that	O
the	O
goal	O
of	O
variational	B
inference	B
is	O
to	O
minimize	O
the	O
upper	O
bound	O
jq	O
log	O
pd	O
equivalently	O
we	O
can	O
try	O
to	O
maximize	O
the	O
lower	O
bound	O
lq	O
jq	O
qx	O
log	O
px	O
qx	O
log	O
pd	O
x	O
we	O
will	O
do	O
this	O
one	O
term	O
at	O
a	O
time	O
the	O
mean	B
field	O
method	O
if	O
we	O
write	O
the	O
objective	O
singling	O
out	O
the	O
terms	O
that	O
involve	O
qj	O
and	O
regarding	O
all	O
the	O
other	O
terms	O
as	O
constants	O
we	O
get	O
lqj	O
i	O
x	O
xj	O
xj	O
xj	O
x	O
j	O
log	O
qkxk	O
k	O
k	O
qixi	O
qjxj	O
log	O
px	O
qixi	O
x	O
j	O
log	O
px	O
qjxj	O
qixi	O
log	O
px	O
log	O
qkxk	O
qjxj	O
qixi	O
log	O
qkxk	O
jxj	O
qjxj	O
log	O
qjxj	O
const	O
xj	O
where	O
log	O
fjxj	O
x	O
j	O
qjxj	O
log	O
fjxj	O
x	O
j	O
lqj	O
kl	O
qjxj	O
zj	O
exp	O
e	O
qj	O
px	O
xj	O
qixi	O
log	O
px	O
e	O
qj	O
px	O
so	O
we	O
average	O
out	O
all	O
the	O
hidden	B
variables	I
except	O
for	O
xj	O
thus	O
we	O
can	O
rewrite	O
lqj	O
as	O
follows	O
we	O
can	O
maximize	O
l	O
by	O
minimizing	O
this	O
kl	O
which	O
we	O
can	O
do	O
by	O
setting	O
qj	O
fj	O
as	O
follows	O
we	O
can	O
usually	O
ignore	O
the	O
local	O
normalization	O
constant	O
zj	O
since	O
we	O
know	O
qj	O
must	O
be	O
a	O
normalized	O
distribution	O
hence	O
we	O
usually	O
work	O
with	O
the	O
form	O
log	O
qjxj	O
e	O
qj	O
px	O
const	O
the	O
functional	O
form	O
of	O
the	O
qj	O
distributions	O
will	O
be	O
determined	O
by	O
the	O
type	O
of	O
variables	O
xj	O
as	O
if	O
xj	O
is	O
a	O
well	O
as	O
the	O
form	O
of	O
the	O
model	O
discrete	B
random	I
variable	I
then	O
qj	O
will	O
be	O
a	O
discrete	B
distribution	O
if	O
xj	O
is	O
a	O
continuous	O
random	O
variable	O
then	O
qj	O
will	O
be	O
some	O
kind	O
of	O
pdf	B
we	O
will	O
see	O
examples	O
of	O
this	O
below	O
is	O
sometimes	O
called	O
free-form	B
optimization	B
example	O
mean	B
field	O
for	O
the	O
ising	B
model	I
consider	O
the	O
image	B
denoising	I
example	O
from	O
section	O
where	O
xi	O
are	O
the	O
hidden	B
pixel	O
values	O
of	O
the	O
clean	O
image	O
we	O
have	O
a	O
joint	O
model	O
of	O
the	O
form	O
px	O
y	O
xpyx	O
chapter	O
variational	B
inference	B
where	O
the	O
prior	O
has	O
the	O
form	O
px	O
exp	O
wijxixj	O
j	O
nbri	O
and	O
the	O
likelihood	B
has	O
the	O
form	O
pyx	O
pyixi	O
exp	O
lixi	O
i	O
i	O
therefore	O
the	O
posterior	O
has	O
the	O
form	O
pxy	O
ex	O
z	O
exp	O
ex	O
lixi	O
i	O
i	O
qx	O
qxi	O
i	O
we	O
will	O
now	O
approximate	O
this	O
by	O
a	O
fully	O
factored	O
approximation	O
where	O
i	O
is	O
the	O
mean	B
value	O
of	O
node	O
i	O
to	O
derive	O
the	O
update	O
for	O
the	O
variational	O
parameter	B
i	O
we	O
first	O
write	O
out	O
log	O
px	O
ex	O
dropping	O
terms	O
that	O
do	O
not	O
involve	O
xi	O
log	O
px	O
xi	O
wijxj	O
lixi	O
const	O
this	O
only	O
depends	O
on	O
the	O
states	O
of	O
the	O
neighboring	O
nodes	B
now	O
we	O
take	O
expectations	O
of	O
this	O
wrt	O
j	O
nbri	O
xi	O
qjxj	O
to	O
get	O
qixi	O
exp	O
mi	B
j	O
nbri	O
wij	O
j	O
j	O
nbri	O
wij	O
j	O
lixi	O
thus	O
we	O
replace	O
the	O
states	O
of	O
the	O
neighbors	B
by	O
their	O
average	O
values	O
let	O
be	O
the	O
mean	B
field	O
influence	O
on	O
node	O
i	O
also	O
approximate	O
marginal	O
posterior	O
is	O
given	O
by	O
let	O
l	O
i	O
and	O
l	O
i	O
li	O
the	O
qixi	O
emil	O
ai	B
mi	B
i	O
emil	O
i	O
e	O
mil	O
i	O
l	O
i	O
i	O
e	O
i	O
l	O
i	O
structured	O
mean	B
field	O
sample	O
meanfieldh	O
sample	O
meanfieldh	O
mean	B
after	O
sweeps	O
of	O
meanfieldh	O
figure	O
example	O
of	O
image	B
denoising	I
using	O
mean	B
field	O
parallel	O
updates	O
and	O
a	O
damping	B
factor	B
of	O
we	O
use	O
an	O
ising	O
prior	O
with	O
wij	O
and	O
a	O
gaussian	B
noise	O
model	O
with	O
we	O
show	O
the	O
results	O
after	O
and	O
iterations	O
across	O
the	O
image	O
compare	O
to	O
figure	O
figure	O
generated	O
by	O
isingimagedenoisedemo	O
similarly	O
we	O
have	O
qixi	O
sigm	O
from	O
this	O
we	O
can	O
compute	O
the	O
new	O
mean	B
for	O
site	O
i	O
i	O
eqi	O
qixi	O
qixi	O
hence	O
the	O
update	O
equation	O
becomes	O
i	O
tanh	O
wij	O
j	O
eai	O
eai	O
e	O
ai	B
i	O
l	O
i	O
e	O
ai	B
e	O
ai	B
eai	O
tanhai	O
e	O
j	O
nbri	O
j	O
nbri	O
see	O
also	O
exercise	O
for	O
an	O
alternative	O
derivation	O
of	O
these	O
equations	O
we	O
can	O
turn	O
the	O
above	O
equations	O
in	O
to	O
a	O
fixed	O
point	O
algorithm	O
by	O
writing	O
t	O
i	O
tanh	O
wij	O
t	O
j	O
i	O
l	O
i	O
it	O
is	O
usually	O
better	O
to	O
use	O
damped	B
updates	I
of	O
the	O
form	O
i	O
t	O
t	O
i	O
tanh	O
wij	O
t	O
j	O
i	O
l	O
i	O
j	O
nbri	O
for	O
we	O
can	O
update	O
all	O
the	O
nodes	B
in	O
parallel	O
or	O
update	O
them	O
asychronously	O
figure	O
shows	O
the	O
method	O
in	O
action	B
applied	O
to	O
a	O
ising	B
model	I
with	O
homogeneous	B
attractive	O
potentials	O
wij	O
we	O
use	O
parallel	O
updates	O
with	O
a	O
damping	B
factor	B
of	O
we	O
don	O
t	O
use	O
damping	B
we	O
tend	O
to	O
get	O
checkerboard	O
artefacts	O
structured	O
mean	B
field	O
assuming	O
that	O
all	O
the	O
variables	O
are	O
independent	O
in	O
the	O
posterior	O
is	O
a	O
very	O
strong	O
assumption	O
that	O
can	O
lead	O
to	O
poor	O
results	O
sometimes	O
we	O
can	O
exploit	O
tractable	B
substructure	I
in	O
our	O
problem	O
so	O
chapter	O
variational	B
inference	B
figure	O
chains	O
approximation	O
based	O
on	O
figure	O
of	O
and	O
jordan	O
a	O
factorial	B
hmm	B
with	O
chains	O
a	O
fully	O
factorized	O
approximation	O
a	O
product-of	O
that	O
we	O
can	O
efficiently	O
handle	O
some	O
kinds	O
of	O
dependencies	O
this	O
is	O
called	O
the	O
structured	O
mean	B
field	O
approach	O
and	O
jordan	O
the	O
approach	O
is	O
the	O
same	O
as	O
before	O
except	O
we	O
group	O
sets	O
of	O
variables	O
together	O
and	O
we	O
update	O
them	O
simultaneously	O
follows	O
by	O
simply	O
treating	O
all	O
the	O
variables	O
in	O
the	O
i	O
th	O
group	O
as	O
a	O
single	O
mega-variable	O
and	O
then	O
repeating	O
the	O
derivation	O
in	O
section	O
as	O
long	O
as	O
we	O
can	O
perform	O
efficient	O
inference	B
in	O
each	O
qi	O
the	O
method	O
is	O
tractable	O
overall	O
we	O
give	O
an	O
example	O
below	O
see	O
and	O
jordan	O
for	O
some	O
more	O
recent	O
work	O
in	O
this	O
area	O
example	O
factorial	B
hmm	B
consider	O
the	O
factorial	B
hmm	B
model	O
and	O
jordan	O
introduced	O
in	O
section	O
suppose	O
there	O
are	O
m	O
chains	O
each	O
of	O
length	O
t	O
and	O
suppose	O
each	O
hidden	B
node	O
has	O
k	O
states	O
the	O
model	O
is	O
defined	O
as	O
follows	O
pxtmxt	O
px	O
y	O
where	O
pxtm	O
kxt	O
j	O
mjk	O
is	O
an	O
entry	O
in	O
the	O
transition	B
matrix	I
for	O
chain	O
m	O
k	O
mk	O
is	O
the	O
initial	O
state	B
distribution	O
for	O
chain	O
m	O
and	O
m	O
t	O
yt	O
pytxt	O
n	O
wmxtm	O
is	O
the	O
observation	B
model	I
where	O
xtm	O
is	O
a	O
encoding	O
of	O
xtm	O
and	O
wm	O
is	O
a	O
d	O
k	O
matrix	O
yt	O
r	O
d	O
figure	O
illustrates	O
the	O
model	O
for	O
the	O
case	O
where	O
m	O
even	O
though	O
each	O
chain	O
is	O
a	O
priori	O
independent	O
they	O
become	O
coupled	O
in	O
the	O
posterior	O
due	O
to	O
having	O
an	O
observed	O
common	O
child	O
yt	O
the	O
junction	B
tree	B
algorithm	I
applied	O
to	O
this	O
graph	B
takes	O
ot	O
m	O
km	O
time	O
below	O
we	O
will	O
derive	O
a	O
structured	O
mean	B
field	O
algorithm	O
that	O
takes	O
ot	O
m	O
k	O
time	O
where	O
i	O
is	O
the	O
number	O
of	O
mean	B
field	O
iterations	O
i	O
suffices	O
for	O
good	O
performance	O
structured	O
mean	B
field	O
we	O
can	O
write	O
the	O
exact	O
posterior	O
in	O
the	O
following	O
form	O
pxy	O
z	O
exp	O
ex	O
y	O
m	O
yt	O
xt	O
m	O
wmxtm	O
yt	O
xt	O
tm	O
amxt	O
ex	O
y	O
wmxtm	O
m	O
m	O
m	O
where	O
am	O
log	O
am	O
and	O
m	O
log	O
m	O
interpreted	O
elementwise	O
we	O
can	O
approximate	O
the	O
posterior	O
as	O
a	O
product	O
of	O
marginals	O
as	O
in	O
figure	O
but	O
a	O
better	O
approximation	O
is	O
to	O
use	O
a	O
product	O
of	O
chains	O
as	O
in	O
figure	O
each	O
chain	O
can	O
be	O
tractably	O
updated	O
individually	O
using	O
the	O
forwards-backwards	B
algorithm	I
more	O
precisely	O
we	O
assume	O
qxtmxt	O
tm	O
qxy	O
qxtmxt	O
tm	O
zq	O
tmk	O
xtmk	O
we	O
see	O
that	O
the	O
tmk	O
parameters	O
play	O
the	O
role	O
of	O
an	O
approximate	O
local	B
evidence	B
averaging	O
out	O
the	O
effects	O
of	O
the	O
other	O
chains	O
this	O
is	O
contrast	O
to	O
the	O
exact	O
local	B
evidence	B
which	O
couples	O
all	O
the	O
chains	O
together	O
we	O
can	O
rewrite	O
the	O
approximate	O
posterior	O
as	O
qx	O
zq	O
tm	O
tm	O
xt	O
m	O
xt	O
eqx	O
exp	O
eqx	O
where	O
xt	O
tm	O
amxt	O
where	O
tm	O
log	O
tm	O
we	O
see	O
that	O
this	O
has	O
the	O
same	O
temporal	O
factors	B
as	O
the	O
exact	O
posterior	O
but	O
the	O
local	B
evidence	B
term	O
is	O
different	O
the	O
objective	O
function	O
is	O
given	O
by	O
where	O
the	O
expectations	O
are	O
taken	O
wrt	O
q	O
one	O
can	O
show	O
that	O
the	O
update	O
has	O
the	O
form	O
kl	O
e	O
log	O
zq	O
log	O
z	O
ytm	O
yt	O
wt	O
tm	O
exp	O
m	O
diagwt	O
m	O
ytm	O
m	O
m	O
chapter	O
variational	B
inference	B
the	O
tm	O
parameter	B
plays	O
the	O
role	O
of	O
the	O
local	B
evidence	B
averaging	O
over	O
the	O
neighboring	O
chains	O
having	O
computed	O
this	O
for	O
each	O
chain	O
we	O
can	O
perform	O
forwards-backwards	B
in	O
parallel	O
using	O
these	O
approximate	O
local	B
evidence	B
terms	O
to	O
compute	O
for	O
each	O
m	O
and	O
t	O
the	O
update	O
cost	O
is	O
ot	O
m	O
k	O
for	O
a	O
full	B
sweep	O
over	O
all	O
the	O
variational	O
parameters	O
since	O
we	O
have	O
to	O
run	O
forwards-backwards	B
m	O
times	O
for	O
each	O
chain	O
independently	O
this	O
is	O
the	O
same	O
cost	O
as	O
a	O
fully	O
factorized	O
approximation	O
but	O
is	O
much	O
more	O
accurate	O
variational	B
bayes	I
make	O
a	O
fully	O
factorized	O
mean	B
field	O
approximation	O
p	O
so	O
far	O
we	O
have	O
been	O
concentrating	O
on	O
inferring	O
latent	B
variables	O
zi	O
assuming	O
the	O
parameters	O
of	O
the	O
model	O
are	O
known	O
now	O
suppose	O
we	O
want	O
to	O
infer	O
the	O
parameters	O
themselves	O
if	O
we	O
k	O
q	O
k	O
we	O
get	O
a	O
method	O
known	O
as	O
variational	B
bayes	I
or	O
vb	B
and	O
camp	O
mackay	O
attias	O
beal	O
and	O
ghahramani	O
smidl	O
and	O
quinn	O
we	O
give	O
some	O
examples	O
of	O
vb	B
below	O
assuming	O
that	O
there	O
are	O
no	O
latent	B
variables	O
if	O
we	O
want	O
to	O
infer	O
both	O
latent	B
variables	O
and	O
parameters	O
and	O
we	O
make	O
an	O
approximation	O
of	O
the	O
form	O
p	O
q	O
i	O
qizi	O
we	O
get	O
a	O
method	O
known	O
as	O
variational	B
bayes	I
em	B
which	O
we	O
described	O
in	O
section	O
example	O
vb	B
for	O
a	O
univariate	O
gaussian	B
following	O
let	O
us	O
consider	O
how	O
to	O
apply	O
vb	B
to	O
infer	O
the	O
posterior	O
over	O
the	O
parameters	O
for	O
a	O
gaussian	B
p	O
where	O
is	O
the	O
precision	B
for	O
convenience	O
we	O
will	O
use	O
a	O
conjugate	B
prior	I
of	O
the	O
form	O
p	O
n	O
however	O
we	O
will	O
use	O
an	O
approximate	O
factored	O
posterior	O
of	O
the	O
form	O
q	O
q	O
we	O
do	O
not	O
need	O
to	O
specify	O
the	O
forms	O
for	O
the	O
distributions	O
q	O
and	O
q	O
the	O
optimal	O
forms	O
will	O
fall	O
out	O
automatically	O
during	O
the	O
derivation	O
conveniently	O
they	O
turn	O
out	O
to	O
be	O
gaussian	B
and	O
gamma	B
respectively	O
you	O
might	O
wonder	O
why	O
we	O
would	O
want	O
to	O
do	O
this	O
since	O
we	O
know	O
how	O
to	O
compute	O
the	O
exact	O
posterior	O
for	O
this	O
model	O
there	O
are	O
two	O
reasons	O
first	O
it	O
is	O
a	O
useful	O
pedagogical	O
exercise	O
since	O
we	O
can	O
compare	O
the	O
quality	O
of	O
our	O
approximation	O
to	O
the	O
exact	O
posterior	O
second	O
it	O
is	O
simple	O
to	O
modify	O
the	O
method	O
to	O
handle	O
a	O
semi-conjugate	B
prior	O
of	O
the	O
form	O
p	O
n	O
for	O
which	O
exact	O
inference	B
is	O
no	O
longer	O
possible	O
this	O
method	O
was	O
originally	O
called	O
ensemble	B
learning	B
since	O
we	O
are	O
using	O
an	O
ensemble	B
of	O
parameters	O
distribution	O
instead	O
of	O
a	O
point	B
estimate	I
however	O
the	O
term	O
ensemble	B
learning	B
is	O
also	O
used	O
to	O
describe	O
methods	O
such	O
as	O
boosting	B
so	O
we	O
prefer	O
the	O
term	O
vb	B
variational	B
bayes	I
target	O
distribution	O
the	O
unnormalized	O
log	O
posterior	O
has	O
the	O
form	O
log	O
p	O
log	O
p	O
log	O
pd	O
log	O
p	O
log	O
p	O
n	O
log	O
log	O
log	O
const	O
updating	O
q	O
the	O
optimal	O
form	O
for	O
q	O
is	O
obtained	O
by	O
averaging	O
over	O
log	O
q	O
q	O
pd	O
log	O
p	O
const	O
eq	O
const	O
by	O
completing	B
the	I
square	I
one	O
can	O
show	O
that	O
q	O
n	O
n	O
n	O
where	O
n	O
n	O
x	O
n	O
n	O
n	O
at	O
this	O
stage	O
we	O
don	O
t	O
know	O
what	O
q	O
is	O
and	O
hence	O
we	O
cannot	O
compute	O
e	O
but	O
we	O
will	O
derive	O
this	O
below	O
updating	O
q	O
the	O
optimal	O
form	O
for	O
q	O
is	O
given	O
by	O
log	O
q	O
q	O
pd	O
log	O
p	O
log	O
p	O
const	O
log	O
eq	O
log	O
log	O
n	O
const	O
we	O
recognize	O
this	O
as	O
the	O
log	O
of	O
a	O
gamma	B
distribution	I
hence	O
q	O
ga	O
bn	O
where	O
an	O
bn	O
n	O
eq	O
chapter	O
variational	B
inference	B
computing	O
the	O
expectations	O
n	O
we	O
have	O
to	O
implement	O
the	O
updates	O
we	O
have	O
to	O
specify	O
how	O
to	O
compute	O
the	O
various	O
expectations	O
since	O
q	O
n	O
n	O
eq	O
n	O
n	O
n	O
eq	O
since	O
q	O
ga	O
bn	O
we	O
have	O
eq	O
an	O
bn	O
we	O
can	O
now	O
give	O
explicit	O
forms	O
for	O
the	O
update	O
equations	O
for	O
q	O
we	O
have	O
n	O
x	O
n	O
n	O
n	O
n	O
an	O
bn	O
and	O
for	O
q	O
we	O
have	O
an	O
n	O
bn	O
xi	O
i	O
e	O
we	O
see	O
that	O
n	O
and	O
an	O
are	O
in	O
fact	O
fixed	O
constants	O
and	O
only	O
n	O
and	O
bn	O
need	O
to	O
be	O
updated	O
fact	O
one	O
can	O
solve	O
for	O
the	O
fixed	O
points	O
of	O
n	O
and	O
bn	O
analytically	O
but	O
we	O
don	O
t	O
iteratively	O
do	O
this	O
here	O
in	O
order	O
to	O
illustrate	O
the	O
iterative	O
updating	O
scheme	O
illustration	O
figure	O
gives	O
an	O
example	O
of	O
this	O
method	O
in	O
action	B
the	O
green	O
contours	O
represent	O
the	O
exact	O
posterior	O
which	O
is	O
gaussian-gamma	O
the	O
dotted	O
red	O
contours	O
represent	O
the	O
variational	O
approximation	O
over	O
several	O
iterations	O
we	O
see	O
that	O
the	O
final	O
approximation	O
is	O
reasonably	O
close	O
to	O
the	O
exact	O
solution	O
however	O
it	O
is	O
more	O
compact	O
than	O
the	O
true	O
distribution	O
it	O
is	O
often	O
the	O
case	O
that	O
mean	B
field	O
inference	B
underestimates	O
the	O
posterior	O
uncertainty	B
see	O
section	O
for	O
more	O
discussion	O
of	O
this	O
point	O
lower	O
bound	O
in	O
vb	B
we	O
are	O
maximizing	O
lq	O
which	O
is	O
a	O
lower	O
bound	O
on	O
the	O
log	O
marginal	B
likelihood	B
lq	O
log	O
pd	O
log	O
pd	O
d	O
it	O
is	O
very	O
useful	O
to	O
compute	O
the	O
lower	O
bound	O
itself	O
for	O
three	O
reasons	O
first	O
it	O
can	O
be	O
used	O
to	O
assess	O
convergence	O
of	O
the	O
algorithm	O
second	O
it	O
can	O
be	O
used	O
to	O
assess	O
the	O
correctness	O
of	O
one	O
s	O
variational	B
bayes	I
exact	O
vb	B
exact	O
vb	B
exact	O
vb	B
exact	O
vb	B
factored	O
variational	O
approximation	O
to	O
the	O
gaussian-gamma	O
distribution	O
figure	O
initial	O
guess	O
after	O
updating	O
q	O
after	O
updating	O
q	O
at	O
convergence	O
iterations	O
based	O
on	O
of	O
figure	O
generated	O
by	O
unigaussvbdemo	O
code	O
as	O
with	O
em	B
if	O
the	O
bound	O
does	O
not	O
increase	O
monotonically	O
there	O
must	O
be	O
a	O
bug	O
third	O
the	O
bound	O
can	O
be	O
used	O
as	O
an	O
approximation	O
to	O
the	O
marginal	B
likelihood	B
which	O
can	O
be	O
used	O
for	O
bayesian	B
model	B
selection	I
unfortunately	O
computing	O
this	O
lower	O
bound	O
involves	O
a	O
fair	O
amount	O
of	O
tedious	O
algebra	O
we	O
work	O
out	O
the	O
details	O
for	O
this	O
example	O
but	O
for	O
other	O
models	O
we	O
will	O
just	O
state	B
the	O
results	O
without	O
proof	O
or	O
even	O
omit	O
discussion	O
of	O
the	O
bound	O
altogether	O
for	O
brevity	O
for	O
this	O
model	O
lq	O
can	O
be	O
computed	O
as	O
follows	O
lq	O
q	O
log	O
pd	O
q	O
d	O
d	O
e	O
pd	O
e	O
p	O
e	O
p	O
e	O
q	O
e	O
q	O
where	O
all	O
expectations	O
are	O
wrt	O
q	O
we	O
recognize	O
the	O
last	O
two	O
terms	O
as	O
the	O
entropy	B
of	O
a	O
gaussian	B
and	O
the	O
entropy	B
of	O
a	O
gamma	B
distribution	I
which	O
are	O
given	O
by	O
n	O
n	O
h	O
log	O
n	O
h	O
bn	O
log	O
logbn	O
n	O
chapter	O
variational	B
inference	B
where	O
is	O
the	O
digamma	B
function	O
to	O
compute	O
the	O
other	O
terms	O
we	O
need	O
the	O
following	O
facts	O
e	O
xx	O
gaa	O
b	O
logb	O
e	O
gaa	O
b	O
xx	O
n	O
n	O
a	O
b	O
e	O
e	O
for	O
the	O
expected	O
log	O
likelihood	B
one	O
can	O
show	O
that	O
eq	O
pd	O
n	O
n	O
n	O
n	O
eq	O
e	O
log	O
bn	O
n	O
an	O
n	O
x	O
n	O
n	O
eq	O
where	O
x	O
and	O
are	O
the	O
empirical	O
mean	B
and	O
variance	B
for	O
the	O
expected	O
log	O
prior	O
of	O
we	O
have	O
eq	O
p	O
log	O
log	O
log	O
bn	O
an	O
bn	O
log	O
log	O
eq	O
for	O
the	O
expected	O
log	O
prior	O
of	O
one	O
can	O
show	O
that	O
eq	O
p	O
log	O
log	O
an	O
bn	O
n	O
e	O
q	O
log	O
bn	O
n	O
putting	O
it	O
altogether	O
one	O
can	O
show	O
that	O
lq	O
log	O
n	O
log	O
an	O
log	O
bn	O
const	O
this	O
quantity	O
monotonically	O
increases	O
after	O
each	O
vb	B
update	O
example	O
vb	B
for	O
linear	B
regression	B
in	O
section	O
we	O
discussed	O
an	O
empirical	B
bayes	I
approach	O
to	O
setting	O
the	O
hyper-parameters	B
for	O
ridge	B
regression	B
known	O
as	O
the	O
evidence	B
procedure	I
in	O
particular	O
we	O
assumed	O
a	O
likelihood	B
of	O
the	O
form	O
pyx	O
and	O
a	O
prior	O
of	O
the	O
form	O
pw	O
we	O
then	O
variational	B
bayes	I
computed	O
a	O
type	O
ii	O
estimate	O
of	O
and	O
the	O
same	O
approach	O
was	O
extended	O
in	O
section	O
to	O
handle	O
a	O
prior	O
of	O
the	O
form	O
n	O
diag	O
which	O
allows	O
one	O
hyper-parameter	O
per	O
feature	O
a	O
technique	O
known	O
as	O
automatic	B
relevancy	I
determination	I
in	O
this	O
section	O
we	O
derive	O
a	O
vb	B
algorithm	O
for	O
this	O
model	O
we	O
follow	O
the	O
presentation	O
of	O
initially	O
we	O
will	O
use	O
the	O
following	O
prior	O
pw	O
b	O
b	O
we	O
choose	O
to	O
use	O
the	O
following	O
factorized	O
approximation	O
to	O
the	O
posterior	O
qw	O
qw	O
given	O
these	O
assumptions	O
one	O
can	O
show	O
that	O
the	O
optimal	O
form	O
for	O
the	O
posterior	O
is	O
qw	O
n	O
n	O
b	O
n	O
b	O
n	O
where	O
v	O
n	O
a	O
xx	O
wn	O
vn	O
xt	O
y	O
n	O
a	O
n	O
a	O
b	O
n	O
b	O
a	O
n	O
a	O
n	O
b	O
b	O
a	O
a	O
n	O
b	O
n	O
a	O
n	O
b	O
n	O
i	O
wt	O
d	O
n	O
awn	O
wt	O
n	O
wn	O
trvn	O
this	O
method	O
can	O
be	O
extended	O
to	O
the	O
ard	B
case	O
in	O
a	O
straightforward	O
way	O
by	O
using	O
the	O
following	O
priors	O
pw	O
diag	O
ga	O
ja	O
p	O
b	O
the	O
posterior	O
for	O
w	O
and	O
is	O
computed	O
as	O
before	O
except	O
we	O
use	O
a	O
diaga	O
n	O
nj	O
instead	O
of	O
note	O
that	O
drugowitsch	O
uses	O
as	O
the	O
hyper-parameters	B
for	O
p	O
and	O
as	O
the	O
hyper-parameters	B
for	O
p	O
whereas	O
sec	O
uses	O
as	O
the	O
hyper-parameters	B
for	O
p	O
and	O
treats	O
as	O
fixed	O
to	O
avoid	O
confusion	O
i	O
use	O
a	O
as	O
the	O
hyper-parameters	B
for	O
p	O
and	O
a	O
as	O
the	O
hyper-parameters	B
for	O
p	O
b	O
b	O
chapter	O
variational	B
inference	B
a	O
n	O
n	O
i	O
the	O
posterior	O
for	O
has	O
the	O
form	O
q	O
n	O
b	O
nj	O
a	O
n	O
a	O
b	O
nj	O
b	O
nj	O
j	O
ga	O
ja	O
a	O
n	O
b	O
n	O
the	O
algorithm	O
alternates	O
between	O
updating	O
qw	O
and	O
q	O
once	O
w	O
and	O
have	O
been	O
inferred	O
the	O
posterior	O
predictive	B
is	O
a	O
student	O
distribution	O
as	O
shown	O
in	O
equation	O
specifically	O
for	O
a	O
single	O
data	O
case	O
we	O
have	O
xt	O
vn	O
x	O
n	O
x	O
n	O
pyxd	O
b	O
n	O
a	O
n	O
the	O
exact	O
marginal	B
likelihood	B
which	O
can	O
be	O
used	O
for	O
model	B
selection	I
is	O
given	O
by	O
pd	O
pyx	O
w	O
d	O
we	O
can	O
compute	O
a	O
lower	O
bound	O
on	O
log	O
pd	O
as	O
follows	O
lq	O
n	O
wt	O
n	O
xt	O
i	O
vn	O
xi	O
a	O
n	O
b	O
n	O
log	O
d	O
log	O
log	O
log	O
b	O
log	O
b	O
a	O
n	O
b	O
n	O
b	O
log	O
log	O
n	O
a	O
n	O
a	O
n	O
log	O
b	O
n	O
n	O
log	O
b	O
n	O
a	O
n	O
in	O
the	O
ard	B
case	O
the	O
last	O
line	O
becomes	O
log	O
log	O
b	O
log	O
n	O
a	O
n	O
log	O
b	O
nj	O
figure	O
compare	O
vb	B
and	O
eb	B
on	O
a	O
model	B
selection	I
problem	O
for	O
polynomial	B
regression	B
we	O
see	O
that	O
vb	B
gives	O
similar	B
results	O
to	O
eb	B
but	O
the	O
precise	O
behavior	O
depends	O
on	O
the	O
sample	O
size	O
when	O
n	O
vb	B
s	O
estimate	O
of	O
the	O
posterior	O
over	O
models	O
is	O
more	O
diffuse	O
than	O
eb	B
s	O
since	O
vb	B
models	O
uncertainty	B
in	O
the	O
hyper-parameters	B
when	O
n	O
the	O
posterior	O
estimate	O
of	O
the	O
hyperindeed	O
if	O
we	O
compute	O
e	O
when	O
we	O
have	O
an	O
parameters	O
becomes	O
more	O
well-determined	O
uninformative	B
prior	O
a	O
we	O
get	O
b	O
a	O
n	O
b	O
n	O
a	O
n	O
b	O
n	O
wt	O
n	O
wn	O
trvn	O
variational	B
bayes	I
em	B
methodvb	O
methodeb	O
d	O
m	O
p	O
d	O
m	O
p	O
d	O
m	O
p	O
m	O
methodvb	O
d	O
m	O
p	O
m	O
m	O
methodeb	O
m	O
figure	O
we	O
plot	O
the	O
posterior	O
over	O
models	O
of	O
degree	B
and	O
assuming	O
a	O
uniform	O
prior	O
pm	O
we	O
approximate	O
the	O
marginal	B
likelihood	B
using	O
vb	B
and	O
eb	B
in	O
we	O
use	O
n	O
data	O
points	O
in	O
figure	O
in	O
we	O
use	O
n	O
data	O
points	O
in	O
figure	O
figure	O
generated	O
by	O
linregebmodelselvsn	O
compare	O
this	O
to	O
equation	O
for	O
eb	B
d	O
e	O
w	O
d	O
wt	O
n	O
wn	O
trvn	O
n	O
and	O
b	O
modulo	O
the	O
a	O
in	O
hindsight	B
this	O
is	O
perhaps	O
not	O
that	O
surprising	O
since	O
eb	B
is	O
trying	O
to	O
maximize	O
log	O
pd	O
and	O
vb	B
is	O
trying	O
to	O
maximize	O
a	O
lower	O
bound	O
on	O
log	O
pd	O
n	O
terms	O
these	O
are	O
the	O
same	O
variational	B
bayes	I
em	B
now	O
consider	O
latent	B
variable	I
models	I
of	O
the	O
form	O
zi	O
xi	O
this	O
includes	O
mixtures	O
models	O
pca	B
hmms	B
etc	O
there	O
are	O
now	O
two	O
kinds	O
of	O
unknowns	O
parameters	O
and	O
latent	B
variables	O
zi	O
as	O
we	O
saw	O
in	O
section	O
it	O
is	O
common	O
to	O
fit	O
such	O
models	O
using	O
em	B
where	O
in	O
the	O
e	B
step	I
we	O
infer	O
the	O
posterior	O
over	O
the	O
latent	B
variables	O
pzixi	O
and	O
in	O
the	O
m	B
step	I
we	O
compute	O
a	O
point	B
estimate	I
of	O
the	O
parameters	O
the	O
justification	O
for	O
this	O
is	O
two-fold	O
first	O
it	O
results	O
in	O
simple	O
algorithms	O
second	O
the	O
posterior	O
uncertainty	B
in	O
is	O
usually	O
less	O
than	O
in	O
zi	O
since	O
the	O
are	O
informed	O
by	O
all	O
n	O
data	O
cases	O
whereas	O
zi	O
is	O
only	O
informed	O
by	O
xi	O
this	O
makes	O
a	O
map	B
estimate	I
of	O
chapter	O
variational	B
inference	B
more	O
reasonable	O
than	O
a	O
map	B
estimate	I
of	O
zi	O
however	O
vb	B
provides	O
a	O
way	O
to	O
be	O
more	O
bayesian	B
by	O
modeling	O
uncertainty	B
in	O
the	O
parameters	O
as	O
well	O
in	O
the	O
latent	B
variables	O
zi	O
at	O
a	O
computational	O
cost	O
that	O
is	O
essentially	O
the	O
same	O
as	O
em	B
this	O
method	O
is	O
known	O
as	O
variational	B
bayes	I
em	B
or	O
vbem	B
the	O
basic	O
idea	O
is	O
to	O
use	O
mean	B
field	O
where	O
the	O
approximate	O
posterior	O
has	O
the	O
form	O
p	O
q	O
q	O
qzi	O
i	O
the	O
first	O
factorization	O
between	O
and	O
z	O
is	O
a	O
crucial	O
assumption	O
to	O
make	O
the	O
algorithm	O
tractable	O
the	O
second	O
factorization	O
follows	O
from	O
the	O
model	O
since	O
the	O
latent	B
variables	O
are	O
iid	B
conditional	O
on	O
in	O
vbem	B
we	O
alternate	O
between	O
updating	O
qzid	O
variational	O
e	B
step	I
and	O
updating	O
q	O
variational	O
m	B
step	I
we	O
can	O
recover	O
standard	O
em	B
from	O
vbem	B
by	O
approximating	O
the	O
parameter	B
posterior	O
using	O
a	O
delta	O
function	O
q	O
the	O
variational	O
e	B
step	I
is	O
similar	B
to	O
a	O
standard	O
e	B
step	I
except	O
instead	O
of	O
plugging	O
in	O
a	O
map	B
estimate	I
of	O
the	O
parameters	O
and	O
computing	O
pzid	O
we	O
need	O
to	O
average	O
over	O
the	O
parameters	O
roughly	O
speaking	O
this	O
can	O
be	O
computed	O
by	O
plugging	O
in	O
the	O
posterior	B
mean	B
of	O
the	O
parameters	O
instead	O
of	O
the	O
map	B
estimate	I
and	O
then	O
computing	O
pzid	O
using	O
standard	O
algorithms	O
such	O
as	O
forwards-backwards	B
unfortunately	O
things	O
are	O
not	O
quite	O
this	O
simple	O
but	O
this	O
is	O
the	O
basic	O
idea	O
the	O
details	O
depend	O
on	O
the	O
form	O
of	O
the	O
model	O
we	O
give	O
some	O
examples	O
below	O
the	O
variational	O
m	B
step	I
is	O
similar	B
to	O
a	O
standard	O
m	B
step	I
except	O
instead	O
of	O
computing	O
a	O
point	B
estimate	I
of	O
the	O
parameters	O
we	O
update	O
the	O
hyper-parameters	B
using	O
the	O
expected	B
sufficient	B
statistics	I
this	O
process	O
is	O
usually	O
very	O
similar	B
to	O
map	O
estimation	O
in	O
regular	B
em	B
again	O
the	O
details	O
on	O
how	O
to	O
do	O
this	O
depend	O
on	O
the	O
form	O
of	O
the	O
model	O
the	O
principle	O
advantage	O
of	O
vbem	B
over	O
regular	B
em	B
is	O
that	O
by	O
marginalizing	B
out	I
the	O
parameters	O
we	O
can	O
compute	O
a	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	B
which	O
can	O
be	O
used	O
for	O
model	B
selection	I
we	O
will	O
see	O
an	O
example	O
of	O
this	O
in	O
section	O
vbem	B
is	O
also	O
egalitarian	O
since	O
it	O
treats	O
parameters	O
as	O
first	O
class	O
citizens	O
just	O
like	O
any	O
other	O
unknown	B
quantity	O
whereas	O
em	B
makes	O
an	O
artificial	O
distinction	O
between	O
parameters	O
and	O
latent	B
variables	O
example	O
vbem	B
for	O
mixtures	O
of	O
gaussians	O
let	O
us	O
consider	O
how	O
to	O
fit	O
a	O
mixture	B
of	I
gaussians	I
using	O
vbem	B
use	O
scare	O
quotes	O
since	O
we	O
are	O
not	O
estimating	O
the	O
model	O
parameters	O
but	O
inferring	O
a	O
posterior	O
over	O
them	O
we	O
will	O
follow	O
the	O
presentation	O
of	O
sec	O
unfortunately	O
the	O
details	O
are	O
rather	O
complicated	O
fortunately	O
as	O
with	O
em	B
one	O
gets	O
used	O
to	O
it	O
after	O
a	O
bit	O
of	O
practice	O
usual	O
with	O
math	O
simply	O
reading	O
the	O
equations	O
won	O
t	O
help	O
much	O
you	O
should	O
really	O
try	O
deriving	O
these	O
results	O
yourself	O
try	O
some	O
of	O
the	O
exercises	O
if	O
you	O
want	O
to	O
learn	O
this	O
stuff	O
in	O
depth	O
the	O
variational	O
posterior	O
the	O
likelihood	B
function	O
is	O
the	O
usual	O
one	O
for	O
gaussian	B
mixture	B
models	O
pz	O
x	O
k	O
n	O
k	O
zik	O
k	O
i	O
k	O
where	O
zik	O
if	O
data	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
and	O
zik	O
otherwise	O
variational	B
bayes	I
em	B
we	O
will	O
assume	O
the	O
following	O
factored	O
conjugate	B
prior	I
p	O
dir	O
n	O
k	O
k	O
where	O
k	O
is	O
the	O
precision	B
matrix	I
for	O
cluster	O
k	O
the	O
subscript	O
means	O
these	O
are	O
parameters	O
of	O
the	O
prior	O
we	O
assume	O
all	O
the	O
prior	O
parameters	O
are	O
the	O
same	O
for	O
all	O
clusters	B
for	O
the	O
mixing	B
weights	I
we	O
usually	O
use	O
a	O
symmetric	B
prior	O
the	O
exact	O
posterior	O
pz	O
is	O
a	O
mixture	B
of	O
k	O
n	O
distributions	O
corresponding	O
to	O
all	O
possible	O
labelings	O
z	O
we	O
will	O
try	O
to	O
approximate	O
the	O
volume	O
around	O
one	O
of	O
these	O
modes	O
we	O
will	O
use	O
the	O
standard	O
vb	B
approximation	O
to	O
the	O
posterior	O
p	O
q	O
qzi	O
i	O
at	O
this	O
stage	O
we	O
have	O
not	O
specified	O
the	O
forms	O
of	O
the	O
q	O
functions	O
these	O
will	O
be	O
determined	O
by	O
the	O
form	O
of	O
the	O
likelihood	B
and	O
prior	O
below	O
we	O
will	O
show	O
that	O
the	O
optimal	O
form	O
is	O
as	O
follows	O
qz	O
catziri	O
i	O
dir	O
n	O
kmk	O
k	O
k	O
klk	O
k	O
k	O
lack	O
of	O
subscript	O
means	O
these	O
are	O
parameters	O
of	O
the	O
posterior	O
not	O
the	O
prior	O
below	O
we	O
will	O
derive	O
the	O
update	O
equations	O
for	O
these	O
variational	O
parameters	O
derivation	O
of	O
qz	O
e	B
step	I
the	O
form	O
for	O
qz	O
can	O
be	O
obtained	O
by	O
looking	O
at	O
the	O
complete	B
data	I
log	O
joint	O
ignoring	O
terms	O
that	O
do	O
not	O
involve	O
z	O
and	O
taking	O
expectations	O
of	O
what	O
s	O
left	O
over	O
wrt	O
all	O
the	O
hidden	B
variables	I
except	O
for	O
z	O
we	O
have	O
log	O
qz	O
q	O
px	O
z	O
const	O
zik	O
log	O
ik	O
const	O
i	O
i	O
where	O
we	O
define	O
log	O
ik	O
eq	O
k	O
eq	O
k	O
d	O
kt	O
kxi	O
k	O
eq	O
using	O
the	O
fact	O
that	O
q	O
dir	O
we	O
have	O
log	O
k	O
e	O
k	O
k	O
chapter	O
variational	B
inference	B
where	O
is	O
the	O
digamma	B
function	O
exercise	O
for	O
the	O
detailed	O
derivation	O
next	O
we	O
use	O
the	O
fact	O
that	O
q	O
k	O
k	O
n	O
kmk	O
k	O
k	O
to	O
get	O
log	O
k	O
e	O
k	O
klk	O
k	O
d	O
log	O
log	O
k	O
k	O
j	O
finally	O
for	O
the	O
expected	B
value	I
of	O
the	O
quadratic	O
form	O
we	O
get	O
kt	O
kxi	O
k	O
d	O
k	O
kxi	O
mkt	O
kxi	O
mk	O
e	O
putting	O
it	O
altogether	O
we	O
get	O
that	O
the	O
posterior	O
responsibility	B
of	O
cluster	O
k	O
for	O
datapoint	O
i	O
is	O
rik	O
k	O
k	O
exp	O
k	O
mkt	O
kxi	O
mk	O
compare	O
this	O
to	O
the	O
expression	O
used	O
in	O
regular	B
em	B
rem	O
ik	O
k	O
k	O
exp	O
kt	O
kxi	O
k	O
d	O
k	O
k	O
i	O
k	O
the	O
significance	O
of	O
this	O
difference	O
is	O
discussed	O
further	O
in	O
section	O
derivation	O
of	O
q	O
m	B
step	I
using	O
the	O
mean	B
field	O
recipe	O
we	O
have	O
log	O
q	O
log	O
p	O
log	O
p	O
k	O
k	O
eqz	O
pzi	O
eqz	O
log	O
n	O
k	O
k	O
const	O
i	O
we	O
see	O
this	O
factorizes	O
into	O
the	O
form	O
q	O
q	O
k	O
k	O
k	O
for	O
the	O
term	O
we	O
have	O
log	O
q	O
k	O
k	O
i	O
log	O
k	O
rik	O
log	O
k	O
const	O
exponentiating	O
we	O
recognize	O
this	O
as	O
a	O
dirichlet	B
distribution	I
q	O
dir	O
k	O
nk	O
nk	O
rik	O
i	O
variational	B
bayes	I
em	B
variational	B
bayes	I
objective	O
for	O
gmm	B
on	O
old	O
faithful	B
data	O
d	O
o	O
o	O
h	O
i	O
l	O
e	O
k	O
i	O
i	O
l	O
l	O
a	O
n	O
g	O
r	O
a	O
m	O
g	O
o	O
l	O
n	O
o	O
d	O
n	O
u	O
o	O
b	O
r	O
e	O
w	O
o	O
l	O
iter	O
figure	O
lower	O
bound	O
vs	O
iterations	O
for	O
the	O
vb	B
algorithm	O
in	O
figure	O
the	O
steep	O
parts	O
of	O
the	O
curve	O
correspond	O
to	O
places	O
where	O
the	O
algorithm	O
figures	O
out	O
that	O
it	O
can	O
increase	O
the	O
bound	O
by	O
killing	O
off	O
unnecessary	O
mixture	B
components	O
as	O
described	O
in	O
section	O
the	O
plateaus	O
correspond	O
to	O
slowly	O
moving	O
the	O
clusters	B
around	O
figure	O
generated	O
by	O
mixgaussvbdemofaithful	O
for	O
the	O
k	O
and	O
k	O
terms	O
we	O
have	O
q	O
k	O
k	O
kmk	O
k	O
k	O
klk	O
k	O
nk	O
k	O
nk	O
mk	O
nkxk	O
k	O
l	O
nksk	O
k	O
k	O
nk	O
l	O
nk	O
i	O
nk	O
i	O
xk	O
sk	O
rikxi	O
rikxi	O
xkxi	O
xkt	O
this	O
is	O
very	O
similar	B
to	O
the	O
m	B
step	I
for	O
map	O
estimation	O
discussed	O
in	O
section	O
except	O
here	O
we	O
are	O
computing	O
the	O
parameters	O
of	O
the	O
posterior	O
over	O
rather	O
than	O
map	O
estimates	O
of	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	B
the	O
algorithm	O
is	O
trying	O
to	O
maximize	O
the	O
following	O
lower	O
bound	O
z	O
l	O
qz	O
log	O
px	O
z	O
qz	O
d	O
log	O
pd	O
this	O
quantity	O
should	O
increase	O
monotonically	O
with	O
each	O
iteration	O
as	O
shown	O
in	O
figure	O
unfortunately	O
deriving	O
the	O
bound	O
is	O
a	O
bit	O
messy	O
because	O
we	O
need	O
to	O
compute	O
expectations	O
of	O
the	O
unnormalized	O
log	O
posterior	O
as	O
well	O
as	O
entropies	O
of	O
the	O
q	O
distribution	O
we	O
leave	O
the	O
details	O
are	O
similar	B
to	O
section	O
to	O
exercise	O
chapter	O
variational	B
inference	B
posterior	B
predictive	B
distribution	I
k	O
we	O
showed	O
that	O
the	O
approximate	O
posterior	O
has	O
the	O
form	O
q	O
dir	O
n	O
kmk	O
k	O
k	O
klk	O
k	O
consequently	O
the	O
posterior	B
predictive	B
density	I
can	O
be	O
approximated	O
as	O
follows	O
using	O
the	O
results	O
from	O
section	O
pxz	O
kn	O
k	O
k	O
d	O
k	O
z	O
k	O
k	O
lk	O
k	O
k	O
t	O
mk	O
k	O
d	O
pxd	O
mk	O
this	O
is	O
just	O
a	O
weighted	O
sum	O
of	O
student	O
distributions	O
if	O
instead	O
we	O
used	O
a	O
plug-in	B
approximation	I
we	O
would	O
get	O
a	O
weighted	O
sum	O
of	O
gaussian	B
distributions	O
model	B
selection	I
using	O
vbem	B
the	O
simplest	O
way	O
to	O
select	O
k	O
when	O
using	O
vb	B
is	O
to	O
fit	O
several	O
models	O
and	O
then	O
to	O
use	O
the	O
variational	O
lower	O
bound	O
to	O
the	O
log	O
marginal	B
likelihood	B
lk	O
log	O
pdk	O
to	O
approximate	O
pkd	O
elk	O
pkd	O
however	O
the	O
lower	O
bound	O
needs	O
to	O
be	O
modified	O
somewhat	O
to	O
take	O
into	O
account	O
the	O
lack	O
of	O
identifiability	O
of	O
the	O
parameters	O
in	O
particular	O
although	O
vb	B
will	O
approximate	O
the	O
volume	O
occupied	O
by	O
the	O
parameter	B
posterior	O
it	O
will	O
only	O
do	O
so	O
around	O
one	O
of	O
the	O
local	O
modes	O
with	O
k	O
components	O
there	O
are	O
k	O
equivalent	O
modes	O
which	O
differ	O
merely	O
by	O
permuting	O
the	O
labels	O
therefore	O
we	O
should	O
use	O
log	O
pdk	O
lk	O
logk	O
automatic	O
sparsity	B
inducing	O
effects	O
of	O
vbem	B
although	O
vb	B
provides	O
a	O
reasonable	O
approximation	O
to	O
the	O
marginal	B
likelihood	B
than	O
bic	B
and	O
ghahramani	O
this	O
method	O
still	O
requires	O
fitting	O
multiple	O
models	O
one	O
for	O
each	O
value	O
of	O
k	O
being	O
considered	O
a	O
faster	O
alternative	O
is	O
to	O
fit	O
a	O
single	O
model	O
where	O
k	O
is	O
set	O
large	O
but	O
where	O
is	O
set	O
very	O
small	O
from	O
figure	O
we	O
see	O
that	O
the	O
resulting	O
prior	O
for	O
the	O
mixing	B
weights	I
has	O
spikes	O
near	O
the	O
corners	O
of	O
the	O
simplex	O
encouraging	O
a	O
sparse	B
mixing	O
weight	B
vector	I
in	O
regular	B
em	B
the	O
map	B
estimate	I
of	O
the	O
mixing	B
weights	I
will	O
have	O
the	O
form	O
k	O
k	O
where	O
k	O
nk	O
unforuntately	O
this	O
can	O
be	O
negative	O
if	O
and	O
nk	O
variational	B
bayes	I
em	B
iter	O
iter	O
figure	O
we	O
visualize	O
the	O
posterior	B
mean	B
parameters	O
at	O
various	O
stages	O
of	O
the	O
vbem	B
algorithm	O
applied	O
to	O
a	O
mixture	B
of	I
gaussians	I
model	O
on	O
the	O
old	O
faithful	B
data	O
shading	O
intensity	O
is	O
proportional	O
to	O
the	O
mixing	O
weight	O
we	O
initialize	O
with	O
k-means	O
and	O
use	O
as	O
the	O
dirichlet	B
hyper-parameter	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
mixgaussvbdemofaithful	O
based	O
on	O
code	O
by	O
emtiyaz	O
khan	O
iter	O
iter	O
figure	O
we	O
visualize	O
the	O
posterior	O
values	O
of	O
k	O
for	O
the	O
model	O
in	O
figure	O
we	O
see	O
that	O
unnecessary	O
components	O
get	O
killed	O
off	O
figure	O
generated	O
by	O
mixgaussvbdemofaithful	O
and	O
jain	O
however	O
in	O
vbem	B
we	O
use	O
exp	O
k	O
exp	O
k	O
now	O
exp	O
x	O
for	O
x	O
so	O
if	O
k	O
when	O
we	O
compute	O
k	O
it	O
s	O
like	O
we	O
substract	O
from	O
the	O
posterior	O
counts	O
this	O
will	O
hurt	O
small	O
clusters	B
more	O
than	O
large	O
clusters	B
a	O
regressive	O
the	O
effect	O
is	O
that	O
clusters	B
which	O
have	O
very	O
few	O
members	O
become	O
more	O
and	O
more	O
empty	O
over	O
successive	O
iterations	O
whereas	O
the	O
popular	O
clusters	B
get	O
more	O
and	O
more	O
members	O
this	O
is	O
called	O
the	O
rich	B
get	I
richer	I
phenomenon	O
we	O
will	O
encounter	O
it	O
again	O
in	O
section	O
when	O
we	O
discuss	O
dirichlet	B
process	I
mixture	B
models	I
this	O
automatic	O
pruning	B
method	O
is	O
demonstrated	O
in	O
figure	O
we	O
fit	O
a	O
mixture	B
of	I
gaussians	I
to	O
the	O
old	O
faithful	B
dataset	O
but	O
the	O
data	O
only	O
really	O
needs	O
clusters	B
so	O
the	O
rest	O
get	O
killed	O
off	O
for	O
more	O
details	O
see	O
et	O
al	O
chapter	O
variational	B
inference	B
in	O
this	O
example	O
we	O
used	O
if	O
we	O
use	O
a	O
larger	O
we	O
do	O
not	O
get	O
a	O
sparsity	B
effect	O
in	O
figure	O
we	O
plot	O
q	O
at	O
various	O
iterations	O
we	O
see	O
that	O
the	O
unwanted	O
components	O
get	O
extinguished	O
this	O
provides	O
an	O
efficient	O
alternative	O
to	O
performing	O
a	O
discrete	B
search	O
over	O
the	O
number	O
of	O
clusters	B
variational	B
message	B
passing	I
and	O
vibes	B
we	O
have	O
seen	O
that	O
mean	B
field	O
methods	O
at	O
least	O
of	O
the	O
fully-factorized	O
variety	O
are	O
all	O
very	O
similar	B
just	O
compute	O
each	O
node	O
s	O
full	B
conditional	I
and	O
average	O
out	O
the	O
neighbors	B
this	O
is	O
very	O
similar	B
to	O
gibbs	B
sampling	I
except	O
the	O
derivation	O
of	O
the	O
equations	O
is	O
usually	O
a	O
bit	O
more	O
work	O
fortunately	O
it	O
is	O
possible	O
to	O
derive	O
a	O
general	O
purpose	O
set	O
of	O
update	O
equations	O
that	O
work	O
for	O
any	O
dgm	B
for	O
which	O
all	O
cpds	O
are	O
in	O
the	O
exponential	B
family	B
and	O
for	O
which	O
all	O
parent	O
nodes	B
have	O
conjugate	O
distributions	O
and	O
beal	O
et	O
al	O
for	O
a	O
recent	O
extension	B
to	O
handle	O
non-conjugate	O
priors	O
one	O
can	O
then	O
sweep	O
over	O
the	O
graph	B
updating	O
nodes	B
one	O
at	O
a	O
time	O
in	O
a	O
manner	O
similar	B
to	O
gibbs	B
sampling	I
this	O
is	O
known	O
as	O
variational	B
message	B
passing	I
or	O
vmp	B
and	O
bishop	O
and	O
has	O
been	O
implemented	O
in	O
the	O
open-source	O
program	O
this	O
is	O
a	O
vb	B
analog	O
to	O
bugs	B
which	O
is	O
a	O
popular	O
generic	O
program	O
for	O
gibbs	B
sampling	I
discussed	O
in	O
section	O
vmp	B
mean	B
field	O
is	O
best-suited	O
to	O
inference	B
where	O
one	O
or	O
more	O
of	O
the	O
hidden	B
nodes	B
are	O
continuous	O
when	O
performing	O
bayesian	B
learning	B
for	O
models	O
where	O
all	O
the	O
hidden	B
nodes	B
are	O
discrete	B
more	O
accurate	O
approximate	B
inference	B
algorithms	O
can	O
be	O
used	O
as	O
we	O
discuss	O
in	O
chapter	O
local	O
variational	O
bounds	O
so	O
far	O
we	O
have	O
been	O
focusing	O
on	O
mean	B
field	O
inference	B
which	O
is	O
a	O
form	O
of	O
variational	B
inference	B
based	O
on	O
minimizing	O
kl	O
p	O
where	O
q	O
is	O
the	O
approximate	O
posterior	O
assumed	O
to	O
be	O
factorized	O
and	O
p	O
is	O
the	O
exact	O
unnormalized	O
posterior	O
however	O
there	O
is	O
another	O
kind	O
of	O
variational	B
inference	B
where	O
we	O
replace	O
a	O
specific	O
term	O
in	O
the	O
joint	B
distribution	I
with	O
a	O
simpler	O
function	O
to	O
simplify	O
computation	O
of	O
the	O
posterior	O
such	O
an	O
approach	O
is	O
sometimes	O
called	O
a	O
local	B
variational	I
approximation	I
since	O
we	O
are	O
only	O
modifying	O
one	O
piece	O
of	O
the	O
model	O
unlike	O
mean	B
field	O
which	O
is	O
a	O
global	O
approximation	O
in	O
this	O
section	O
we	O
study	O
several	O
examples	O
of	O
this	O
method	O
motivating	O
applications	O
before	O
we	O
explain	O
how	O
to	O
derive	O
local	O
variational	O
bounds	O
we	O
give	O
some	O
examples	O
of	O
where	O
this	O
is	O
useful	O
variational	O
logistic	B
regression	B
consider	O
the	O
problem	O
of	O
how	O
to	O
approximate	O
the	O
parameter	B
posterior	O
for	O
multiclass	O
logistic	B
regression	B
model	O
under	O
a	O
gaussian	B
prior	O
one	O
approach	O
is	O
to	O
use	O
a	O
gaussian	B
approximation	I
as	O
discussed	O
in	O
section	O
however	O
a	O
variational	O
approach	O
can	O
produce	O
a	O
more	O
available	O
at	O
httpvibes	O
sourceforge	O
net	O
local	O
variational	O
bounds	O
accurate	O
approximation	O
to	O
the	O
posterior	O
since	O
it	O
has	O
tunable	O
parameters	O
another	O
advantage	O
is	O
that	O
the	O
variational	O
approach	O
monotonically	O
optimizes	O
a	O
lower	O
bound	O
on	O
the	O
likelihood	B
of	O
the	O
data	O
as	O
we	O
will	O
see	O
to	O
see	O
why	O
we	O
need	O
a	O
bound	O
note	O
that	O
the	O
likelihood	B
can	O
be	O
written	O
as	O
follows	O
pyx	O
w	O
i	O
i	O
lse	B
i	O
yt	O
exp	O
where	O
i	O
xiwi	O
identifiability	O
and	O
where	O
we	O
define	O
the	O
log-sum-exp	B
or	O
lse	B
function	O
as	O
follows	O
i	O
xt	O
i	O
wm	O
where	O
m	O
c	O
we	O
set	O
wc	O
for	O
lse	B
i	O
log	O
e	O
im	O
the	O
main	O
problem	O
is	O
that	O
this	O
likelihood	B
is	O
not	O
conjugate	O
to	O
the	O
gaussian	B
prior	O
below	O
we	O
discuss	O
how	O
to	O
compute	O
gaussian-like	O
lower	O
bounds	O
to	O
this	O
likelihood	B
which	O
give	O
rise	O
to	O
approximate	O
gaussian	B
posteriors	O
multi-task	B
learning	B
one	O
important	O
application	O
of	O
bayesian	B
inference	B
for	O
logistic	B
regression	B
is	O
where	O
we	O
have	O
multiple	O
related	O
classifiers	O
we	O
want	O
to	O
fit	O
in	O
this	O
case	O
we	O
want	O
to	O
share	O
information	B
between	O
the	O
parameters	O
for	O
each	O
classifier	O
this	O
requires	O
that	O
we	O
maintain	O
a	O
posterior	O
distibution	O
over	O
the	O
parameters	O
so	O
we	O
have	O
a	O
measure	O
of	O
confidence	O
as	O
well	O
as	O
an	O
estimate	O
of	O
the	O
value	O
we	O
can	O
embed	O
the	O
above	O
variational	O
method	O
inside	O
of	O
a	O
larger	O
hierarchical	O
model	O
in	O
order	O
to	O
perform	O
such	O
multi-task	B
learning	B
as	O
described	O
in	O
e	O
g	O
and	O
mcauliffe	O
discrete	B
factor	B
analysis	I
another	O
situation	O
where	O
variational	O
bounds	O
are	O
useful	O
arises	O
when	O
we	O
fit	O
a	O
factor	B
analysis	I
model	O
to	O
discrete	B
data	O
this	O
model	O
is	O
just	O
like	O
multinomial	B
logistic	B
regression	B
except	O
the	O
input	O
variables	O
are	O
hidden	B
factors	B
we	O
need	O
to	O
perform	O
inference	B
on	O
the	O
hidden	B
variables	I
as	O
well	O
as	O
the	O
regression	B
weights	O
for	O
simplicity	O
we	O
might	O
perform	O
point	O
estimation	O
of	O
the	O
weights	O
and	O
just	O
integrate	B
out	I
the	O
hidden	B
variables	I
we	O
can	O
do	O
this	O
using	O
variational	B
em	B
where	O
we	O
use	O
the	O
variational	O
bound	O
in	O
the	O
e	B
step	I
see	O
section	O
for	O
details	O
correlated	B
topic	B
model	I
a	O
topic	B
model	I
is	O
a	O
latent	B
variable	O
model	O
for	O
text	O
documents	O
and	O
other	O
forms	O
of	O
discrete	B
data	O
see	O
section	O
for	O
details	O
often	O
we	O
assume	O
the	O
distribution	O
over	O
topics	O
has	O
a	O
dirichlet	B
prior	O
but	O
a	O
more	O
powerful	O
model	O
known	O
as	O
the	O
correlated	B
topic	B
model	I
uses	O
a	O
gaussian	B
prior	O
which	O
can	O
model	O
correlations	O
more	O
easily	O
section	O
for	O
details	O
unfortunately	O
this	O
also	O
involves	O
the	O
lse	B
function	O
however	O
we	O
can	O
use	O
our	O
variational	O
bounds	O
in	O
the	O
context	O
of	O
a	O
variational	B
em	B
algorithm	O
as	O
we	O
will	O
see	O
later	O
chapter	O
variational	B
inference	B
bohning	O
s	O
quadratic	O
bound	O
to	O
the	O
log-sum-exp	B
function	O
all	O
of	O
the	O
above	O
examples	O
require	O
dealing	O
with	O
multiplying	O
a	O
gaussian	B
prior	O
by	O
a	O
multinomial	B
likelihood	B
this	O
is	O
difficult	O
because	O
of	O
the	O
log-sum-exp	B
term	O
in	O
this	O
section	O
we	O
derive	O
a	O
way	O
to	O
derive	O
a	O
gaussian-like	O
lower	O
bound	O
on	O
this	O
likelihood	B
consider	O
a	O
taylor	B
series	I
expansion	I
of	O
the	O
lse	B
function	O
around	O
i	O
r	O
lse	B
i	O
lse	B
i	O
i	O
it	O
g	O
i	O
g	O
i	O
exp	O
i	O
lse	B
i	O
s	O
i	O
h	O
i	O
diagg	O
i	O
g	O
ig	O
it	O
i	O
it	O
h	O
i	O
i	O
i	O
m	O
where	O
g	O
and	O
h	O
are	O
the	O
gradient	O
and	O
hessian	B
of	O
lse	B
and	O
i	O
r	O
m	O
is	O
chosen	O
such	O
that	O
equality	O
holds	O
an	O
upper	O
bound	O
to	O
lse	B
can	O
be	O
found	O
by	O
replacing	O
the	O
hessian	B
matrix	O
h	O
i	O
with	O
a	O
matrix	O
ai	B
such	O
that	O
ai	B
h	O
i	O
showed	O
that	O
this	O
can	O
be	O
achieved	O
if	O
we	O
use	O
the	O
matrix	O
ai	B
that	O
m	O
c	O
is	O
the	O
number	O
of	O
classes	O
note	O
that	O
ai	B
is	O
independent	O
of	O
i	O
however	O
we	O
still	O
write	O
it	O
as	O
ai	B
than	O
dropping	O
the	O
i	O
subscript	O
since	O
other	O
bounds	O
that	O
we	O
consider	O
below	O
will	O
have	O
a	O
data-dependent	O
curvature	O
term	O
the	O
upper	O
bound	O
on	O
lse	B
therefore	O
becomes	O
im	O
m	O
m	O
i	O
i	O
ci	B
m	O
lse	B
i	O
i	O
ai	B
i	O
bt	O
t	O
im	O
ai	B
m	O
bi	O
ai	B
i	O
g	O
i	O
ci	B
where	O
i	O
r	O
i	O
ai	B
i	O
g	O
it	O
i	O
lse	B
i	O
t	O
m	O
is	O
a	O
vector	O
of	O
variational	O
parameters	O
we	O
can	O
use	O
the	O
above	O
result	O
to	O
get	O
the	O
following	O
lower	O
bound	O
on	O
the	O
softmax	B
likelihood	B
log	O
pyi	O
cxi	O
w	O
wt	O
xiaixiw	O
bt	O
i	O
xiw	O
ci	B
i	O
xiw	O
yt	O
c	O
to	O
simplify	O
notation	O
define	O
the	O
pseudo-measurement	O
yi	O
a	O
i	O
yi	O
then	O
we	O
can	O
get	O
a	O
gaussianized	O
version	O
of	O
the	O
observation	B
model	I
pyixi	O
w	O
f	O
i	O
n	O
yixiw	O
a	O
i	O
where	O
f	O
i	O
is	O
some	O
function	O
that	O
does	O
not	O
depend	O
on	O
w	O
given	O
this	O
it	O
is	O
easy	O
to	O
compute	O
the	O
posterior	O
qw	O
n	O
vn	O
using	O
bayes	B
rule	I
for	O
gaussians	O
below	O
we	O
will	O
explain	O
how	O
to	O
update	O
the	O
variational	O
parameters	O
i	O
local	O
variational	O
bounds	O
applying	O
bohning	O
s	O
bound	O
to	O
multinomial	B
logistic	B
regression	B
let	O
us	O
see	O
how	O
to	O
apply	O
this	O
bound	O
to	O
multinomial	B
logistic	B
regression	B
from	O
equation	O
we	O
can	O
define	O
the	O
goal	O
of	O
variational	B
inference	B
as	O
maximizing	O
log	O
pyixi	O
w	O
lq	O
kl	O
eq	O
kl	O
eq	O
i	O
eq	O
i	O
kl	O
vn	O
v	O
kl	O
i	O
i	O
lse	B
i	O
yt	O
yt	O
eq	O
i	O
where	O
qw	O
vn	O
is	O
the	O
approximate	O
posterior	O
the	O
first	O
term	O
is	O
just	O
the	O
kl	B
divergence	I
between	O
two	O
gaussians	O
which	O
is	O
given	O
by	O
trvn	O
v	O
log	O
v	O
dm	O
where	O
dm	O
is	O
the	O
dimensionality	O
of	O
the	O
gaussian	B
and	O
we	O
assume	O
a	O
prior	O
of	O
the	O
form	O
pw	O
n	O
where	O
typically	O
and	O
is	O
block	O
diagonal	B
the	O
second	O
term	O
is	O
simply	O
yt	O
i	O
eq	O
i	O
yt	O
i	O
mi	B
where	O
mi	B
ximn	O
the	O
final	O
term	O
can	O
be	O
lower	O
bounded	O
by	O
taking	O
expectations	O
of	O
our	O
quadratic	O
upper	O
bound	O
on	O
lse	B
as	O
follows	O
eq	O
i	O
trai	O
vi	O
miai	O
mi	B
bt	O
i	O
mi	B
ci	B
where	O
vi	O
xivn	O
xt	O
lqj	O
i	O
putting	O
it	O
altogether	O
we	O
have	O
log	O
v	O
trvn	O
v	O
dm	O
i	O
mi	B
yt	O
trai	O
vi	O
v	O
i	O
mi	B
ci	B
miai	O
mi	B
bt	O
this	O
lower	O
bound	O
combines	O
jensen	O
s	O
inequality	O
in	O
mean	B
field	O
inference	B
plus	O
the	O
quadratic	O
lower	O
bound	O
due	O
to	O
the	O
lse	B
term	O
so	O
we	O
write	O
it	O
as	O
lqj	O
we	O
will	O
use	O
coordinate	O
ascent	O
to	O
optimize	O
this	O
lower	O
bound	O
that	O
is	O
we	O
update	O
the	O
variational	O
posterior	O
parameters	O
vn	O
and	O
mn	O
and	O
then	O
the	O
variational	O
likelihood	B
parameters	O
i	O
we	O
leave	O
chapter	O
variational	B
inference	B
xt	O
i	O
aixi	O
the	O
detailed	O
derivation	O
as	O
an	O
exercise	O
and	O
just	O
state	B
the	O
results	O
we	O
have	O
vn	O
mn	O
vn	O
v	O
xt	O
i	O
bi	O
i	O
mi	B
ximn	O
we	O
can	O
exploit	O
the	O
fact	O
that	O
ai	B
is	O
a	O
constant	O
matrix	O
plus	O
the	O
fact	O
that	O
xi	O
has	O
block	O
structure	O
to	O
simplify	O
the	O
first	O
two	O
terms	O
as	O
follows	O
vn	O
a	O
v	O
xixt	O
i	O
mn	O
vn	O
where	O
denotes	O
the	O
kronecker	B
product	I
see	O
algorithm	O
for	O
some	O
pseudocode	O
and	O
http	O
for	O
some	O
matlab	O
code	O
bi	O
xi	O
inference	B
for	O
multi-class	B
logistic	B
regression	B
using	O
bohning	O
s	O
d	O
i	O
n	O
prior	O
algorithm	O
variational	O
bound	O
input	O
yi	O
c	O
xi	O
r	O
define	O
m	O
c	O
dummy	O
encode	O
yi	O
define	O
xi	O
blockdiagxt	O
i	O
m	O
define	O
y	O
yn	O
x	O
xn	O
and	O
a	O
vn	O
initialize	O
mn	O
repeat	O
im	O
xt	O
i	O
axi	O
v	O
m	O
xmn	O
reshapem	O
m	O
n	O
g	O
exp	O
lse	B
b	O
a	O
g	O
b	O
v	O
mn	O
vn	O
compute	O
the	O
lower	O
bound	O
lqj	O
using	O
equation	O
xt	O
b	O
until	O
converged	O
return	O
mn	O
and	O
vn	O
bounds	O
for	O
the	O
sigmoid	B
function	O
in	O
many	O
models	O
we	O
just	O
have	O
binary	O
data	O
i	O
wt	O
xi	O
where	O
w	O
r	O
d	O
is	O
a	O
weight	B
vector	I
matrix	O
in	O
this	O
case	O
we	O
have	O
yi	O
m	O
and	O
in	O
this	O
case	O
the	O
bohning	O
bound	O
local	O
variational	O
bounds	O
bohning	O
bound	O
jj	B
bound	I
figure	O
quadratic	O
lower	O
bounds	O
on	O
the	O
sigmoid	B
function	O
in	O
solid	O
red	O
we	O
plot	O
sigmx	O
vs	O
x	O
in	O
dotted	O
blue	O
we	O
plot	O
the	O
lower	O
bound	O
lx	O
vs	O
x	O
for	O
bohning	O
bound	O
this	O
is	O
tight	O
at	O
jj	B
bound	I
this	O
is	O
tight	O
at	O
figure	O
generated	O
by	O
sigmoidlowerbounds	O
becomes	O
a	O
b	O
c	O
e	O
a	O
b	O
a	O
e	O
c	O
a	O
e	O
e	O
it	O
is	O
possible	O
to	O
derive	O
an	O
alternative	O
quadratic	O
bound	O
for	O
this	O
case	O
as	O
shown	O
in	O
and	O
jordan	O
this	O
has	O
the	O
following	O
form	O
e	O
tanh	O
sigm	O
we	O
shall	O
refer	O
to	O
this	O
as	O
the	O
jj	B
bound	I
after	O
its	O
inventors	O
and	O
jordan	O
to	O
facilitate	O
comparison	O
with	O
bohning	O
s	O
bound	O
let	O
us	O
rewrite	O
the	O
jj	B
bound	I
as	O
a	O
quadratic	O
form	O
as	O
follows	O
e	O
a	O
b	O
c	O
a	O
b	O
c	O
e	O
the	O
jj	B
bound	I
has	O
an	O
adaptive	O
curvature	O
term	O
since	O
a	O
depends	O
on	O
in	O
addition	O
it	O
is	O
tight	O
at	O
two	O
points	O
as	O
is	O
evident	O
from	O
figure	O
by	O
contrast	O
the	O
bohning	O
bound	O
is	O
a	O
constant	O
curvature	O
bound	O
and	O
is	O
only	O
tight	O
at	O
one	O
point	O
as	O
is	O
evident	O
from	O
figure	O
chapter	O
variational	B
inference	B
if	O
we	O
wish	O
to	O
use	O
the	O
jj	B
bound	I
for	O
binary	O
logistic	B
regression	B
we	O
can	O
make	O
some	O
small	O
modifications	O
to	O
algorithm	O
first	O
we	O
use	O
the	O
new	O
definitions	O
for	O
ai	B
bi	O
and	O
ci	B
the	O
fact	O
that	O
ai	B
is	O
not	O
constant	O
when	O
using	O
the	O
jj	B
bound	I
unlike	O
when	O
using	O
the	O
bohning	O
bound	O
means	O
we	O
cannot	O
compute	O
vn	O
outside	O
of	O
the	O
main	O
loop	B
making	O
the	O
method	O
a	O
constant	O
factor	B
slower	O
next	O
we	O
note	O
that	O
xi	O
xt	O
i	O
so	O
the	O
updates	O
for	O
the	O
posterior	O
become	O
n	O
v	O
v	O
mn	O
vn	O
v	O
ixixt	O
i	O
finally	O
to	O
compute	O
the	O
update	O
for	O
i	O
we	O
isolate	O
the	O
terms	O
in	O
lqj	O
that	O
depend	O
on	O
i	O
to	O
get	O
i	O
eq	O
wwt	O
xi	O
i	O
const	O
l	O
optimizing	O
this	O
wrt	O
i	O
gives	O
the	O
equation	O
ln	O
sigm	O
i	O
ixt	O
ixt	O
i	O
eq	O
wwt	O
i	O
is	O
monotonic	O
for	O
i	O
and	O
we	O
do	O
not	O
need	O
to	O
consider	O
negative	O
values	O
of	O
i	O
by	O
now	O
symmetry	O
of	O
the	O
bound	O
around	O
i	O
figure	O
hence	O
the	O
only	O
way	O
to	O
make	O
the	O
above	O
expression	O
is	O
if	O
we	O
have	O
i	O
hence	O
the	O
update	O
becomes	O
xi	O
wwt	O
i	O
e	O
xi	O
i	O
new	O
i	O
xt	O
i	O
mn	O
mt	O
n	O
although	O
the	O
jj	B
bound	I
is	O
tighter	O
than	O
the	O
bohning	O
bound	O
sometimes	O
it	O
is	O
not	O
tight	O
enough	O
in	O
order	O
to	O
estimate	O
the	O
posterior	O
covariance	B
accurately	O
a	O
more	O
accurate	O
approach	O
which	O
uses	O
a	O
piecewise	O
quadratic	O
upper	O
bound	O
to	O
lse	B
is	O
described	O
in	O
et	O
al	O
by	O
increasing	O
the	O
number	O
of	O
pieces	O
the	O
bound	O
can	O
be	O
made	O
arbitrarily	O
tight	O
other	O
bounds	O
and	O
approximations	O
to	O
the	O
log-sum-exp	B
function	O
there	O
are	O
several	O
other	O
bounds	O
and	O
approximations	O
to	O
the	O
multiclass	O
lse	B
function	O
which	O
we	O
can	O
use	O
which	O
we	O
briefly	O
summarize	O
below	O
note	O
however	O
that	O
all	O
of	O
these	O
require	O
numerical	O
optimization	B
methods	O
to	O
compute	O
mn	O
and	O
vn	O
making	O
them	O
more	O
complicated	O
to	O
implement	O
product	O
of	O
sigmoids	O
the	O
approach	O
in	O
exploits	O
the	O
fact	O
that	O
log	O
e	O
k	O
e	O
k	O
it	O
then	O
applies	O
the	O
jj	B
bound	I
to	O
the	O
term	O
on	O
the	O
right	O
local	O
variational	O
bounds	O
jensen	O
s	O
inequality	O
the	O
approach	O
in	O
and	O
lafferty	O
uses	O
jensen	O
s	O
inequality	O
as	O
follows	O
eq	O
i	O
eq	O
log	O
expxt	O
i	O
wc	O
log	O
log	O
eq	O
expxt	O
i	O
wc	O
expxt	O
i	O
mnc	O
xt	O
i	O
vnccxi	O
where	O
the	O
last	O
term	O
follows	O
from	O
the	O
mean	B
of	O
a	O
log-normal	O
distribution	O
which	O
is	O
e	O
multivariate	B
delta	I
method	I
the	O
approach	O
in	O
and	O
xing	O
braun	O
and	O
mcauliffe	O
uses	O
the	O
multivariate	B
delta	I
method	I
which	O
is	O
a	O
way	O
to	O
approximate	O
moments	O
of	O
a	O
function	O
using	O
a	O
taylor	B
series	I
expansion	I
in	O
more	O
detail	O
let	O
f	O
be	O
the	O
function	O
of	O
interest	O
using	O
a	O
second-order	O
approximation	O
around	O
m	O
we	O
have	O
f	O
f	O
mt	O
gw	O
m	O
where	O
g	O
and	O
h	O
are	O
the	O
gradient	O
and	O
hessian	B
evaluated	O
at	O
m	O
if	O
qw	O
n	O
v	O
we	O
have	O
mt	O
hw	O
m	O
eq	O
f	O
trhv	O
if	O
we	O
use	O
f	O
lsexiw	O
we	O
get	O
eq	O
lsexim	O
trxihxt	O
i	O
v	O
where	O
g	O
and	O
h	O
for	O
the	O
lse	B
function	O
are	O
defined	O
in	O
equations	O
and	O
variational	B
inference	B
based	O
on	O
upper	O
bounds	O
so	O
far	O
we	O
have	O
been	O
concentrating	O
on	O
lower	O
bounds	O
however	O
sometimes	O
we	O
need	O
to	O
use	O
an	O
upper	O
bound	O
for	O
example	O
et	O
al	O
derives	O
a	O
mean	B
field	O
algorithm	O
for	O
sigmoid	B
belief	I
nets	I
which	O
are	O
dgms	O
in	O
which	O
each	O
cpd	B
is	O
a	O
logistic	B
regression	B
function	O
unlike	O
the	O
case	O
of	O
ising	O
models	O
the	O
resulting	O
mrf	B
is	O
not	O
pairwise	O
but	O
contains	O
higher	O
order	O
interactions	O
this	O
makes	O
the	O
standard	O
mean	B
field	O
updates	O
intractable	O
in	O
particular	O
they	O
turn	O
out	O
to	O
involve	O
computing	O
an	O
expression	O
which	O
requires	O
evaluating	O
e	O
j	O
pai	O
wij	O
xj	O
e	O
e	O
log	O
sigmwt	O
i	O
xpai	O
the	O
minus	O
sign	O
in	O
front	O
et	O
al	O
show	O
how	O
to	O
derive	O
an	O
upper	O
bound	O
on	O
the	O
sigmoid	B
function	O
so	O
as	O
to	O
make	O
this	O
update	O
tractable	O
resulting	O
in	O
a	O
monotonically	O
convergent	O
inference	B
procedure	O
chapter	O
variational	B
inference	B
exercises	O
exercise	O
laplace	B
approximation	I
to	O
p	O
log	O
for	O
a	O
univariate	O
gaussian	B
compute	O
a	O
laplace	B
approximation	I
of	O
p	O
log	O
for	O
a	O
gaussian	B
using	O
an	O
uninformative	B
prior	O
p	O
log	O
exercise	O
laplace	B
approximation	I
to	O
normal-gamma	O
consider	O
estimating	O
and	O
log	O
for	O
a	O
gaussian	B
using	O
an	O
uniformative	O
normal-gamma	O
prior	O
the	O
log	O
posterior	O
is	O
log	O
p	O
n	O
log	O
ny	O
a	O
show	O
that	O
the	O
first	O
derivatives	O
are	O
ny	O
log	O
p	O
log	O
p	O
n	O
ny	O
b	O
show	O
that	O
the	O
hessian	B
matrix	O
is	O
given	O
by	O
log	O
p	O
log	O
p	O
n	O
log	O
p	O
log	O
p	O
y	O
ny	O
y	O
h	O
c	O
use	O
this	O
to	O
derive	O
a	O
laplace	B
approximation	I
to	O
the	O
posterior	O
p	O
exercise	O
variational	O
lower	O
bound	O
for	O
vb	B
for	O
univariate	O
gaussian	B
fill	O
in	O
the	O
details	O
of	O
the	O
derivation	O
in	O
section	O
exercise	O
variational	O
lower	O
bound	O
for	O
vb	B
for	O
gmms	O
consider	O
vbem	B
for	O
gmms	O
as	O
in	O
section	O
show	O
that	O
the	O
lower	O
bound	O
has	O
the	O
following	O
form	O
l	O
e	O
pxz	O
e	O
pz	O
e	O
p	O
e	O
p	O
e	O
qz	O
e	O
q	O
e	O
q	O
local	O
variational	O
bounds	O
where	O
e	O
pxz	O
k	O
nk	O
ln	O
k	O
d	O
k	O
ktrsklk	O
kxk	O
mkt	O
lkxk	O
mk	O
d	O
e	O
pz	O
e	O
p	O
ln	O
cdir	O
rik	O
ln	O
k	O
k	O
i	O
k	O
ln	O
k	O
e	O
p	O
e	O
qz	O
e	O
q	O
e	O
q	O
k	O
d	O
ln	O
cw	O
d	O
ln	O
ln	O
k	O
d	O
k	O
kmk	O
lkmk	O
k	O
ln	O
k	O
ln	O
cdir	O
rik	O
ln	O
rik	O
k	O
k	O
i	O
ln	O
k	O
d	O
ln	O
k	O
d	O
k	O
ln	O
k	O
h	O
k	O
ktrl	O
lk	O
where	O
the	O
normalization	O
constant	O
for	O
the	O
dirichlet	B
and	O
wishart	B
is	O
given	O
by	O
cdir	O
k	O
k	O
k	O
k	O
cw	O
il	O
d	O
d	O
dd	O
where	O
d	O
is	O
the	O
multivariate	B
gamma	B
function	I
finally	O
the	O
entropy	B
of	O
the	O
wishart	B
is	O
given	O
by	O
h	O
ln	O
cw	O
il	O
d	O
e	O
where	O
e	O
is	O
given	O
in	O
equation	O
exercise	O
derivation	O
of	O
e	O
k	O
under	O
a	O
dirichlet	B
distribution	I
show	O
that	O
d	O
exp	O
k	O
exp	O
expe	O
k	O
where	O
dir	O
exercise	O
alternative	O
derivation	O
of	O
the	O
mean	B
field	O
updates	O
for	O
the	O
ising	B
model	I
derive	O
equation	O
by	O
directly	O
optimizing	O
the	O
variational	B
free	B
energy	I
one	O
term	O
at	O
a	O
time	O
chapter	O
variational	B
inference	B
exercise	O
forwards	O
vs	O
reverse	B
kl	B
divergence	I
exercise	O
of	O
consider	O
a	O
factored	O
approximation	O
qx	O
y	O
qxqy	O
to	O
a	O
joint	B
distribution	I
px	O
y	O
show	O
that	O
to	O
minimize	O
the	O
forwards	B
kl	I
kl	O
we	O
should	O
set	O
qx	O
px	O
and	O
qy	O
py	O
i	O
e	O
the	O
optimal	O
approximation	O
is	O
a	O
product	O
of	O
marginals	O
now	O
consider	O
the	O
following	O
joint	B
distribution	I
where	O
the	O
rows	O
represent	O
y	O
and	O
the	O
columns	O
x	O
x	O
show	O
that	O
the	O
reverse	B
kl	I
kl	O
for	O
this	O
p	O
has	O
three	O
distinct	O
minima	O
evaluate	O
kl	O
at	O
each	O
of	O
them	O
what	O
is	O
the	O
value	O
of	O
kl	O
if	O
we	O
set	O
qx	O
y	O
pxpy	O
exercise	O
derivation	O
of	O
the	O
structured	O
mean	B
field	O
updates	O
for	O
fhmm	O
derive	O
the	O
updates	O
in	O
section	O
identify	O
those	O
minima	O
and	O
exercise	O
variational	B
em	B
for	O
binary	O
fa	B
with	O
sigmoid	B
link	O
consider	O
the	O
binary	O
fa	B
model	O
pxizi	O
berxijsigmwt	O
j	O
zi	O
j	O
i	O
w	O
zi	O
zi	O
w	O
pzi	O
i	O
berxijsigm	O
ij	O
derive	O
an	O
em	B
algorithm	O
to	O
fit	O
this	O
model	O
using	O
the	O
jaakkola-jordan	O
bound	O
hint	O
the	O
answer	O
is	O
in	O
but	O
the	O
exercise	O
asks	O
you	O
to	O
derive	O
these	O
equations	O
exercise	O
vb	B
for	O
binary	O
fa	B
with	O
probit	B
link	O
in	O
section	O
we	O
showed	O
how	O
to	O
use	O
em	B
to	O
fit	O
probit	B
regression	B
using	O
a	O
model	O
of	O
the	O
form	O
pyi	O
where	O
zi	O
n	O
xi	O
is	O
latent	B
now	O
consider	O
the	O
case	O
where	O
the	O
inputs	O
xi	O
are	O
also	O
unknown	B
as	O
in	O
binary	O
factor	B
analysis	I
show	O
how	O
to	O
fit	O
this	O
model	O
using	O
variational	B
bayes	I
making	O
an	O
qwl	O
hint	O
qxi	O
and	O
qxiqzi	O
approximation	O
to	O
the	O
posterior	O
of	O
the	O
form	O
qx	O
z	O
w	O
qwi	O
will	O
be	O
gaussian	B
and	O
qzi	O
will	O
be	O
a	O
truncated	O
univariate	O
gaussian	B
more	O
variational	B
inference	B
introduction	O
in	O
chapter	O
we	O
discussed	O
mean	B
field	O
inference	B
which	O
approximates	O
the	O
posterior	O
by	O
a	O
product	O
of	O
marginal	O
distributions	O
this	O
allows	O
us	O
to	O
use	O
different	O
parametric	O
forms	O
for	O
each	O
variable	O
which	O
is	O
particularly	O
useful	O
when	O
performing	O
bayesian	B
inference	B
for	O
the	O
parameters	O
of	O
statistical	O
models	O
as	O
the	O
mean	B
and	O
variance	B
of	O
a	O
gaussian	B
or	O
gmm	B
or	O
the	O
regression	B
weights	O
in	O
a	O
glm	B
as	O
we	O
saw	O
when	O
we	O
discussed	O
variational	B
bayes	I
and	O
vb-em	O
in	O
this	O
chapter	O
we	O
discuss	O
a	O
slightly	O
different	O
kind	O
of	O
variational	B
inference	B
the	O
basic	O
idea	O
is	O
to	O
minimize	O
jq	O
kl	O
p	O
where	O
p	O
is	O
the	O
exact	O
but	O
unnormalized	O
posterior	O
as	O
before	O
but	O
where	O
we	O
no	O
longer	O
require	O
q	O
to	O
be	O
factorized	O
in	O
fact	O
we	O
do	O
not	O
even	O
require	O
q	O
to	O
be	O
a	O
globally	O
instead	O
we	O
only	O
require	O
that	O
q	O
is	O
locally	O
consistent	B
meaning	O
that	O
the	O
valid	O
joint	B
distribution	I
joint	B
distribution	I
of	O
two	O
adjacent	O
nodes	B
agrees	O
with	O
the	O
corresponding	O
marginals	O
will	O
define	O
this	O
more	O
precisely	O
below	O
in	O
addition	O
to	O
this	O
new	O
kind	O
of	O
inference	B
we	O
will	O
discuss	O
approximate	O
methods	O
for	O
map	O
state	B
estimation	I
in	O
discrete	B
graphical	B
models	I
it	O
turns	O
out	O
that	O
algorithms	O
for	O
solving	O
the	O
map	O
problem	O
are	O
very	O
similar	B
to	O
some	O
approximate	O
methods	O
for	O
computing	O
marginals	O
as	O
we	O
will	O
see	O
loopy	B
belief	B
propagation	I
algorithmic	O
issues	O
there	O
is	O
a	O
very	O
simple	O
approximate	B
inference	B
algorithm	O
for	O
discrete	B
gaussian	B
graphical	B
models	I
known	O
as	O
loopy	B
belief	B
propagation	I
or	O
lbp	B
the	O
basic	O
idea	O
is	O
extremely	O
simple	O
we	O
apply	O
the	O
belief	B
propagation	I
algorithm	O
of	O
section	O
to	O
the	O
graph	B
even	O
if	O
it	O
has	O
loops	O
even	O
if	O
it	O
is	O
not	O
a	O
tree	B
this	O
method	O
is	O
simple	O
and	O
efficient	O
and	O
often	O
works	O
well	O
in	O
practice	O
outperforming	O
mean	B
field	O
in	O
this	O
section	O
we	O
discuss	O
the	O
algorithm	O
in	O
more	O
detail	O
in	O
the	O
next	O
section	O
we	O
analyse	O
this	O
algorithm	O
in	O
terms	O
of	O
variational	B
inference	B
a	O
brief	O
history	O
when	O
applied	O
to	O
loopy	O
graphs	O
bp	B
is	O
not	O
guaranteed	O
to	O
give	O
correct	O
results	O
and	O
may	O
not	O
even	O
converge	B
indeed	O
judea	O
pearl	O
who	O
invented	O
belief	B
propagation	I
for	O
trees	O
wrote	O
the	O
following	O
about	O
loopy	O
bp	B
in	O
when	O
loops	O
are	O
present	O
the	O
network	O
is	O
no	O
longer	O
singly	O
connected	O
and	O
local	O
propagation	O
chapter	O
more	O
variational	B
inference	B
schemes	O
will	O
invariably	O
run	O
into	O
trouble	O
if	O
we	O
ignore	O
the	O
existence	O
of	O
loops	O
and	O
permit	O
the	O
nodes	B
to	O
continue	O
communicating	O
with	O
each	O
other	O
as	O
if	O
the	O
network	O
were	O
singly	O
connected	O
messages	O
may	O
circulate	O
indefinitely	O
around	O
the	O
loops	O
and	O
the	O
process	O
may	O
not	O
converge	B
to	O
a	O
stable	B
equilibrium	O
such	O
oscillations	O
do	O
not	O
normally	O
occur	O
in	O
probabilistic	O
networks	O
which	O
tend	O
to	O
bring	O
all	O
messages	O
to	O
some	O
stable	B
equilibrium	O
as	O
time	O
goes	O
on	O
however	O
this	O
asymptotic	O
equilibrium	O
is	O
not	O
coherent	O
in	O
the	O
sense	O
that	O
it	O
does	O
not	O
represent	O
the	O
posterior	O
probabilities	O
of	O
all	O
nodes	B
of	O
the	O
network	O
despite	O
these	O
reservations	O
pearl	O
advocated	O
the	O
use	O
of	O
belief	B
propagation	I
in	O
loopy	O
networks	O
as	O
an	O
approximation	O
scheme	O
pearl	O
personal	O
communication	O
and	O
exercise	O
in	O
investigates	O
the	O
quality	O
of	O
the	O
approximation	O
when	O
it	O
is	O
applied	O
to	O
a	O
particular	O
loopy	O
belief	O
network	O
however	O
the	O
main	O
impetus	O
behind	O
the	O
interest	O
in	O
bp	B
arose	O
when	O
mceliece	O
et	O
al	O
showed	O
that	O
a	O
popular	O
algorithm	O
for	O
error	B
correcting	I
codes	I
known	O
as	O
turbo	B
codes	I
et	O
al	O
could	O
be	O
viewed	O
as	O
an	O
instance	O
of	O
bp	B
applied	O
to	O
a	O
certain	O
kind	O
of	O
graph	B
this	O
was	O
an	O
important	O
observation	B
since	O
turbo	B
codes	I
have	O
gotten	O
very	O
close	O
to	O
the	O
theoretical	O
lower	O
bound	O
on	O
coding	O
efficiency	O
proved	O
by	O
shannon	O
approach	O
known	O
as	O
low	B
density	I
parity	I
check	I
or	O
ldpc	B
codes	O
has	O
achieved	O
comparable	O
performance	O
it	O
also	O
uses	O
lbp	B
for	O
decoding	B
see	O
figure	O
for	O
an	O
example	O
in	O
et	O
al	O
lbp	B
was	O
experimentally	O
shown	O
to	O
also	O
work	O
well	O
for	O
inference	B
in	O
other	O
kinds	O
of	O
graphical	B
models	I
beyond	O
the	O
error-correcting	O
code	O
context	O
and	O
since	O
then	O
the	O
method	O
has	O
been	O
widely	O
used	O
in	O
many	O
different	O
applications	O
lbp	B
on	O
pairwise	O
models	O
we	O
now	O
discuss	O
how	O
to	O
apply	O
lbp	B
to	O
an	O
undirected	B
graphical	B
model	I
with	O
pairwise	O
factors	B
discuss	O
the	O
directed	B
case	O
which	O
can	O
involve	O
higher	O
order	O
factors	B
in	O
the	O
next	O
section	O
the	O
method	O
is	O
simple	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
and	O
beliefpropagation	O
for	O
some	O
matlab	O
code	O
we	O
will	O
discuss	O
issues	O
such	O
as	O
convergence	O
and	O
accuracy	O
of	O
this	O
method	O
shortly	O
just	O
continually	O
apply	O
equations	O
and	O
until	O
convergence	O
algorithm	O
loopy	B
belief	B
propagation	I
for	O
a	O
pairwise	B
mrf	B
input	O
node	O
potentials	O
sxs	O
edge	O
potentials	O
stxs	O
xt	O
initialize	O
messages	O
ms	O
txt	O
for	O
all	O
edges	B
s	O
t	O
initialize	O
beliefs	O
belsxs	O
for	O
all	O
nodes	B
s	O
repeat	O
send	O
message	O
on	O
each	O
edge	O
ms	O
txt	O
update	O
belief	O
of	O
each	O
node	O
belsxs	O
sxs	O
sxs	O
stxs	O
xt	O
xs	O
until	O
beliefs	O
don	O
t	O
change	O
significantly	O
return	O
marginal	O
beliefs	O
belsxs	O
u	O
nbrst	O
mu	O
sxs	O
t	O
nbrs	O
mt	O
sxs	O
loopy	B
belief	B
propagation	I
algorithmic	O
issues	O
figure	O
a	O
simple	O
factor	B
graph	B
representation	O
of	O
a	O
low-density	O
parity	O
check	O
code	O
graphs	O
are	O
defined	O
in	O
section	O
each	O
message	O
bit	O
round	O
circle	O
is	O
connected	O
to	O
two	O
parity	O
factors	B
black	O
squares	O
and	O
each	O
parity	O
factor	B
is	O
connected	O
to	O
three	O
bits	B
each	O
parity	O
factor	B
has	O
the	O
form	O
stuxs	O
xt	O
xu	O
xt	O
xu	O
where	O
is	O
the	O
xor	B
operator	O
the	O
local	B
evidence	B
factors	B
for	O
each	O
hidden	B
node	O
are	O
not	O
shown	O
a	O
larger	O
example	O
of	O
a	O
random	O
ldpc	B
code	O
we	O
see	O
that	O
this	O
graph	B
is	O
locally	O
tree-like	O
meaning	O
there	O
are	O
no	O
short	O
cycles	O
rather	O
each	O
cycle	B
has	O
length	O
log	O
m	O
where	O
m	O
is	O
the	O
number	O
of	O
nodes	B
this	O
gives	O
us	O
a	O
hint	O
as	O
to	O
why	O
loopy	O
bp	B
works	O
so	O
well	O
on	O
such	O
graphs	O
however	O
that	O
some	O
error	O
correcting	O
code	O
graphs	O
have	O
short	O
loops	O
so	O
this	O
is	O
not	O
the	O
full	B
explanation	O
source	O
figure	O
from	O
and	O
jordan	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
lbp	B
on	O
a	O
factor	B
graph	B
to	O
handle	O
models	O
with	O
higher-order	O
clique	B
potentials	O
includes	O
directed	B
models	O
where	O
some	O
nodes	B
have	O
more	O
than	O
one	O
parent	O
it	O
is	O
useful	O
to	O
use	O
a	O
representation	O
known	O
as	O
a	O
factor	B
graph	B
we	O
explain	O
this	O
representation	O
below	O
and	O
then	O
describe	O
how	O
to	O
apply	O
lbp	B
to	O
such	O
models	O
factor	B
graphs	O
a	O
factor	B
graph	B
et	O
al	O
frey	O
is	O
a	O
graphical	O
representation	O
that	O
unifies	O
directed	B
and	O
undirected	B
models	O
and	O
which	O
simplifies	O
certain	O
message	B
passing	I
algorithms	O
more	O
precisely	O
a	O
factor	B
graph	B
is	O
an	O
undirected	B
bipartite	B
graph	B
with	O
two	O
kinds	O
of	O
nodes	B
round	O
nodes	B
represent	O
variables	O
square	O
nodes	B
represent	O
factors	B
and	O
there	O
is	O
an	O
edge	O
from	O
each	O
variable	O
to	O
every	O
factor	B
that	O
mentions	O
it	O
for	O
example	O
consider	O
the	O
mrf	B
in	O
figure	O
if	O
we	O
assume	O
one	O
potential	O
per	O
maximal	B
clique	B
we	O
get	O
the	O
factor	B
graph	B
in	O
figure	O
which	O
represents	O
the	O
function	O
f	O
if	O
we	O
assume	O
one	O
potential	O
per	O
edge	O
we	O
get	O
the	O
factor	B
graph	B
in	O
figure	O
which	O
represents	O
the	O
function	O
f	O
chapter	O
more	O
variational	B
inference	B
figure	O
a	O
simple	O
ugm	B
a	O
factor	B
graph	B
representation	O
assuming	O
one	O
potential	O
per	O
maximal	B
clique	B
a	O
factor	B
graph	B
representation	O
assuming	O
one	O
potential	O
per	O
edge	O
figure	O
a	O
simple	O
dgm	B
its	O
corresponding	O
factor	B
graph	B
based	O
on	O
figure	O
of	O
et	O
al	O
we	O
can	O
also	O
convert	O
a	O
dgm	B
to	O
a	O
factor	B
graph	B
just	O
create	O
one	O
factor	B
per	O
cpd	B
and	O
connect	O
that	O
factor	B
to	O
all	O
the	O
variables	O
that	O
use	O
that	O
cpd	B
for	O
example	O
figure	O
represents	O
the	O
following	O
factorization	O
f	O
where	O
we	O
define	O
etc	O
if	O
each	O
node	O
has	O
at	O
most	O
one	O
parent	O
hence	O
the	O
graph	B
is	O
a	O
chain	O
or	O
simple	O
tree	B
then	O
there	O
will	O
be	O
one	O
factor	B
per	O
edge	O
nodes	B
can	O
have	O
their	O
prior	O
cpds	O
absorvbed	O
into	O
their	O
children	B
s	O
factors	B
such	O
models	O
are	O
equivalent	O
to	O
pairwise	O
mrfs	O
loopy	B
belief	B
propagation	I
algorithmic	O
issues	O
figure	O
message	B
passing	I
on	O
a	O
bipartite	O
factor	B
graph	B
square	O
nodes	B
represent	O
factors	B
and	O
circles	O
represent	O
variables	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
brendan	O
frey	O
bp	B
on	O
a	O
factor	B
graph	B
we	O
now	O
derive	O
a	O
version	O
of	O
bp	B
that	O
sends	O
messages	O
on	O
a	O
factor	B
graph	B
as	O
proposed	O
in	O
et	O
al	O
specifically	O
we	O
now	O
have	O
two	O
kinds	O
of	O
messages	O
variables	O
to	O
factors	B
mx	O
f	O
h	O
nbrxf	O
and	O
factors	B
to	O
variables	O
mf	O
xx	O
f	O
y	O
mh	B
xx	O
y	O
nbrf	O
my	O
f	O
here	O
nbrx	O
are	O
all	O
the	O
factors	B
that	O
are	O
connected	O
to	O
variable	O
x	O
and	O
nbrf	O
are	O
all	O
the	O
variables	O
that	O
are	O
connected	O
to	O
factor	B
f	O
these	O
messages	O
are	O
illustrated	O
in	O
figure	O
at	O
convergence	O
we	O
can	O
compute	O
the	O
final	O
beliefs	O
as	O
a	O
product	O
of	O
incoming	O
messages	O
y	O
belx	O
f	O
nbrx	O
mf	O
xx	O
in	O
the	O
following	O
sections	O
we	O
will	O
focus	O
on	O
lbp	B
for	O
pairwise	O
models	O
rather	O
than	O
for	O
factor	B
graphs	O
but	O
this	O
is	O
just	O
for	O
notational	O
simplicity	O
convergence	O
lbp	B
does	O
not	O
always	O
converge	B
and	O
even	O
when	O
it	O
does	O
it	O
may	O
converge	B
to	O
the	O
wrong	O
answers	O
this	O
raises	O
several	O
questions	O
how	O
can	O
we	O
predict	O
when	O
convergence	O
will	O
occur	O
what	O
can	O
we	O
do	O
to	O
increase	O
the	O
probability	O
of	O
convergence	O
what	O
can	O
we	O
do	O
to	O
increase	O
the	O
rate	B
of	O
convergence	O
we	O
briefly	O
discuss	O
these	O
issues	O
below	O
we	O
then	O
discuss	O
the	O
issue	O
of	O
accuracy	O
of	O
the	O
results	O
at	O
convergence	O
chapter	O
more	O
variational	B
inference	B
d	O
e	O
g	O
r	O
e	O
v	O
n	O
o	O
c	O
s	O
e	O
g	O
a	O
s	O
s	O
e	O
m	O
f	O
o	O
x	O
p	O
x	O
p	O
x	O
p	O
time	O
time	O
x	O
p	O
x	O
p	O
time	O
time	O
time	O
time	O
synchronous	O
asynchronous	O
no	O
smoothing	B
true	O
illustration	O
of	O
the	O
behavior	O
of	O
loopy	B
belief	B
propagation	I
on	O
an	O
ising	O
grid	O
with	O
figure	O
random	O
potentials	O
wij	O
unif	O
c	O
c	O
where	O
c	O
for	O
larger	O
c	O
inference	B
becomes	O
harder	O
percentage	O
of	O
messasges	O
that	O
have	O
converged	O
vs	O
time	O
for	O
different	O
update	O
schedules	O
dotted	O
damped	O
sychronous	O
nodes	B
converge	B
dashed	O
undamped	O
asychnronous	O
the	O
nodes	B
converge	B
solid	O
marginal	O
beliefs	O
of	O
certain	O
nodes	B
vs	O
time	O
solid	O
straight	O
damped	O
asychnronous	O
nodes	B
converge	B
line	O
truth	O
dashed	O
sychronous	O
solid	O
damped	O
asychronous	O
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
when	O
will	O
lbp	B
converge	B
the	O
details	O
of	O
the	O
analysis	O
of	O
when	O
lbp	B
will	O
converge	B
are	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
but	O
we	O
briefly	O
sketch	O
the	O
basic	O
idea	O
the	O
key	O
analysis	O
tool	O
is	O
the	O
computation	B
tree	B
which	O
visualizes	O
the	O
messages	O
that	O
are	O
passed	O
as	O
the	O
algorithm	O
proceeds	O
figure	O
gives	O
a	O
simple	O
example	O
in	O
the	O
first	O
iteration	O
node	O
receives	O
messages	O
from	O
nodes	B
and	O
in	O
the	O
second	O
iteration	O
it	O
receives	O
one	O
message	O
from	O
node	O
node	O
one	O
from	O
node	O
node	O
and	O
two	O
messages	O
from	O
node	O
nodes	B
and	O
and	O
so	O
on	O
the	O
key	O
insight	O
is	O
that	O
t	O
iterations	O
of	O
lbp	B
is	O
equivalent	O
to	O
exact	O
computation	O
in	O
a	O
computation	B
tree	B
of	O
height	O
t	O
if	O
the	O
strengths	O
of	O
the	O
connections	O
on	O
the	O
edges	B
is	O
sufficiently	O
weak	O
then	O
the	O
influence	O
of	O
the	O
leaves	B
on	O
the	O
root	B
will	O
diminish	O
over	O
time	O
and	O
convergence	O
will	O
occur	O
see	O
and	O
jordan	O
and	O
references	O
therein	O
for	O
more	O
information	B
loopy	B
belief	B
propagation	I
algorithmic	O
issues	O
figure	O
a	O
simple	O
loopy	O
graph	B
the	O
computation	B
tree	B
rooted	O
at	O
node	O
after	O
rounds	O
of	O
message	B
passing	I
nodes	B
and	O
occur	O
more	O
often	O
in	O
the	O
tree	B
because	O
they	O
have	O
higher	O
degree	B
than	O
nodes	B
and	O
source	O
figure	O
of	O
and	O
jordan	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
making	O
lbp	B
converge	B
although	O
the	O
theoretical	O
convergence	O
analysis	O
is	O
very	O
interesting	O
in	O
practice	O
when	O
faced	O
with	O
a	O
model	O
where	O
lbp	B
is	O
not	O
converging	O
what	O
should	O
we	O
do	O
one	O
simple	O
way	O
to	O
reduce	O
the	O
chance	O
of	O
oscillation	O
is	O
to	O
use	O
damping	B
that	O
is	O
instead	O
of	O
sending	O
the	O
message	O
m	O
k	O
ts	O
we	O
send	O
a	O
damped	O
message	O
of	O
the	O
form	O
tsxs	O
mtsxs	O
m	O
k	O
m	O
k	O
ts	O
where	O
is	O
the	O
damping	B
factor	B
clearly	O
if	O
this	O
reduces	O
to	O
the	O
standard	O
scheme	O
but	O
for	O
this	O
partial	O
updating	O
scheme	O
can	O
help	O
improve	O
convergence	O
using	O
a	O
value	O
such	O
as	O
is	O
standard	O
practice	O
the	O
benefits	O
of	O
this	O
approach	O
are	O
shown	O
in	O
figure	O
where	O
we	O
see	O
that	O
damped	O
updating	O
results	O
in	O
convergence	O
much	O
more	O
often	O
than	O
undamped	O
updating	O
it	O
is	O
possible	O
to	O
devise	O
methods	O
known	O
as	O
double	B
loop	B
algorithms	I
which	O
are	O
guaranteed	O
to	O
converge	B
to	O
a	O
local	O
minimum	O
of	O
the	O
same	O
objective	O
that	O
lbp	B
is	O
minimizing	O
welling	O
and	O
teh	O
unfortunately	O
these	O
methods	O
are	O
rather	O
slow	O
and	O
complicated	O
and	O
the	O
accuracy	O
of	O
the	O
resulting	O
marginals	O
is	O
usually	O
not	O
much	O
greater	O
than	O
with	O
standard	O
lbp	B
oscillating	O
marginals	O
is	O
sometimes	O
a	O
sign	O
that	O
the	O
lbp	B
approximation	O
itself	O
is	O
a	O
poor	O
one	O
consequently	O
these	O
techniques	O
are	O
not	O
very	O
widely	O
used	O
in	O
section	O
we	O
will	O
see	O
a	O
different	O
convergent	O
version	O
of	O
bp	B
that	O
is	O
widely	O
used	O
increasing	O
the	O
convergence	O
rate	B
message	O
scheduling	O
even	O
if	O
lbp	B
converges	O
it	O
may	O
take	O
a	O
long	O
time	O
the	O
standard	O
approach	O
when	O
implementing	O
lbp	B
is	O
to	O
perform	O
synchronous	B
updates	I
where	O
all	O
nodes	B
absorb	O
messages	O
in	O
parallel	O
and	O
then	O
send	O
out	O
messages	O
in	O
parallel	O
that	O
is	O
the	O
new	O
messages	O
at	O
iteration	O
k	O
are	O
computed	O
in	O
parallel	O
using	O
femk	O
where	O
e	O
is	O
the	O
number	O
of	O
edges	B
and	O
fstm	O
is	O
the	O
function	O
that	O
computes	O
the	O
message	O
for	O
edge	O
s	O
t	O
given	O
all	O
the	O
old	O
messages	O
this	O
is	O
analogous	O
to	O
the	O
jacobi	B
method	O
for	O
solving	O
linear	O
chapter	O
more	O
variational	B
inference	B
systems	O
of	O
equations	O
it	O
is	O
well	O
known	O
that	O
the	O
gauss-seidel	B
method	O
which	O
performs	O
asynchronous	B
updates	I
in	O
a	O
fixed	O
round-robin	O
fashion	O
converges	O
faster	O
when	O
solving	O
linear	O
systems	O
of	O
equations	O
we	O
can	O
apply	O
the	O
same	O
idea	O
to	O
lbp	B
using	O
updates	O
of	O
the	O
form	O
j	O
j	O
j	O
i	O
fi	O
j	O
imk	O
where	O
the	O
message	O
for	O
edge	O
i	O
is	O
computed	O
using	O
new	O
messages	O
k	O
from	O
edges	B
earlier	O
in	O
the	O
ordering	O
and	O
using	O
old	O
messages	O
k	O
from	O
edges	B
later	O
in	O
the	O
ordering	O
this	O
raises	O
the	O
question	O
of	O
what	O
order	O
to	O
update	O
the	O
messages	O
in	O
one	O
simple	O
idea	O
is	O
to	O
use	O
a	O
fixed	O
or	O
random	O
order	O
the	O
benefits	O
of	O
this	O
approach	O
are	O
shown	O
in	O
figure	O
where	O
we	O
see	O
that	O
asynchronous	O
updating	O
results	O
in	O
convergence	O
much	O
more	O
often	O
than	O
synchronous	O
updating	O
a	O
smarter	O
approach	O
is	O
to	O
pick	O
a	O
set	O
of	O
spanning	O
trees	O
and	O
then	O
to	O
perform	O
an	O
up-down	B
sweep	O
on	O
one	O
tree	B
at	O
a	O
time	O
keeping	O
all	O
the	O
other	O
messages	O
fixed	O
this	O
is	O
known	O
as	O
tree	B
reparameterization	I
et	O
al	O
which	O
should	O
not	O
be	O
confused	O
with	O
the	O
more	O
sophisticated	O
tree-reweighted	O
bp	B
abbreviated	O
to	O
trw	B
to	O
be	O
discussed	O
in	O
section	O
however	O
we	O
can	O
do	O
even	O
better	O
by	O
using	O
an	O
adaptive	O
ordering	O
the	O
intuition	O
is	O
that	O
we	O
should	O
focus	O
our	O
computational	O
efforts	O
on	O
those	O
variables	O
that	O
are	O
most	O
uncertain	O
et	O
al	O
proposed	O
a	O
technique	O
known	O
as	O
residual	B
belief	B
propagation	I
in	O
which	O
messages	O
are	O
scheduled	O
to	O
be	O
sent	O
according	O
to	O
the	O
norm	O
of	O
the	O
difference	O
from	O
their	O
previous	O
value	O
that	O
is	O
we	O
define	O
the	O
residual	B
of	O
new	O
message	O
mst	O
at	O
iteration	O
k	O
to	O
be	O
log	O
rs	O
t	O
k	O
log	O
mst	O
log	O
mk	O
st	O
max	O
msti	O
mk	O
sti	O
i	O
we	O
can	O
store	O
messages	O
in	O
a	O
priority	O
queue	O
and	O
always	O
send	O
the	O
one	O
with	O
highest	O
residual	B
when	O
a	O
message	O
is	O
sent	O
from	O
s	O
to	O
t	O
all	O
of	O
the	O
other	O
messages	O
that	O
depend	O
on	O
mst	O
messages	O
of	O
the	O
form	O
mtu	O
where	O
u	O
nbrt	O
s	O
need	O
to	O
be	O
recomputed	O
their	O
residual	B
is	O
recomputed	O
and	O
they	O
are	O
added	O
back	O
to	O
the	O
queue	O
in	O
et	O
al	O
they	O
showed	O
that	O
this	O
method	O
converges	O
more	O
often	O
and	O
much	O
faster	O
than	O
using	O
sychronous	O
updating	O
asynchronous	O
updating	O
with	O
a	O
fixed	O
order	O
and	O
the	O
trp	O
approach	O
a	O
refinement	O
of	O
residual	B
bp	B
was	O
presented	O
in	O
and	O
mccallum	O
in	O
this	O
paper	O
they	O
use	O
an	O
upper	O
bound	O
on	O
the	O
residual	B
of	O
a	O
message	O
instead	O
of	O
the	O
actual	O
residual	B
this	O
means	O
that	O
messages	O
are	O
only	O
computed	O
if	O
they	O
are	O
going	O
to	O
be	O
sent	O
they	O
are	O
not	O
just	O
computed	O
for	O
the	O
purposes	O
of	O
evaluating	O
the	O
residual	B
this	O
was	O
observed	O
to	O
be	O
about	O
five	O
times	O
faster	O
than	O
residual	B
bp	B
although	O
the	O
quality	O
of	O
the	O
final	O
results	O
is	O
similar	B
accuracy	O
of	O
lbp	B
for	O
a	O
graph	B
with	O
a	O
single	O
loop	B
one	O
can	O
show	O
that	O
the	O
max-product	B
version	O
of	O
lbp	B
will	O
find	O
the	O
correct	O
map	B
estimate	I
if	O
it	O
converges	O
for	O
more	O
general	O
graphs	O
one	O
can	O
bound	O
the	O
error	O
in	O
the	O
approximate	O
marginals	O
computed	O
by	O
lbp	B
as	O
shown	O
in	O
et	O
al	O
vinyals	O
et	O
al	O
much	O
stronger	O
results	O
are	O
available	O
in	O
the	O
case	O
of	O
gaussian	B
models	O
and	O
freeman	O
johnson	O
et	O
al	O
bickson	O
in	O
particular	O
in	O
the	O
gaussian	B
case	O
if	O
the	O
method	O
converges	O
the	O
means	O
are	O
exact	O
although	O
the	O
variances	O
are	O
not	O
the	O
beliefs	O
are	O
over	O
confident	O
loopy	B
belief	B
propagation	I
algorithmic	O
issues	O
other	O
speedup	O
tricks	O
for	O
lbp	B
there	O
are	O
several	O
tricks	O
one	O
can	O
use	O
to	O
make	O
bp	B
run	O
faster	O
we	O
discuss	O
some	O
of	O
them	O
below	O
fast	O
message	O
computation	O
for	O
large	O
state	B
spaces	O
the	O
cost	O
of	O
computing	O
each	O
message	O
in	O
bp	B
in	O
a	O
tree	B
or	O
a	O
loopy	O
graph	B
is	O
ok	O
f	O
where	O
k	O
is	O
the	O
number	O
of	O
states	O
and	O
f	O
is	O
the	O
size	O
of	O
the	O
largest	O
factor	B
for	O
pairwise	O
in	O
many	O
vision	O
problems	O
image	B
denoising	I
k	O
is	O
quite	O
large	O
because	O
ugms	O
it	O
represents	O
the	O
discretization	O
of	O
some	O
underlying	O
continuous	O
space	O
so	O
ok	O
per	O
message	O
is	O
too	O
expensive	O
fortunately	O
for	O
certain	O
kinds	O
of	O
pairwise	O
potential	O
functions	O
of	O
the	O
form	O
stxs	O
xt	O
xt	O
one	O
can	O
compute	O
the	O
sum-product	B
messages	O
in	O
ok	O
log	O
k	O
time	O
using	O
the	O
fast	B
fourier	I
transform	I
or	O
fft	B
as	O
explained	O
in	O
and	O
huttenlocher	O
the	O
key	O
insight	O
is	O
that	O
message	O
computation	O
is	O
just	O
convolution	O
xs	O
m	O
k	O
stxt	O
xthxs	O
v	O
nbrst	O
m	O
k	O
vs	O
where	O
hxs	O
sxs	O
if	O
the	O
potential	B
function	I
is	O
a	O
gaussian-like	O
potential	O
we	O
can	O
compute	O
the	O
convolution	O
in	O
ok	O
time	O
by	O
sequentially	O
convolving	O
with	O
a	O
small	O
number	O
of	O
box	O
filters	O
and	O
huttenlocher	O
for	O
the	O
max-product	B
case	O
a	O
technique	O
called	O
the	O
distance	B
transform	I
can	O
be	O
used	O
to	O
compute	O
messages	O
in	O
ok	O
time	O
however	O
this	O
only	O
works	O
if	O
exp	O
ez	O
and	O
where	O
ez	O
has	O
one	O
the	O
following	O
forms	O
quadratic	O
ez	O
truncated	O
linear	O
ez	O
or	O
potts	B
model	I
ez	O
c	O
iz	O
see	O
and	O
huttenlocher	O
for	O
details	O
multi-scale	O
methods	O
a	O
method	O
which	O
is	O
specific	O
to	O
lattice	B
structures	O
which	O
commonly	O
arise	O
in	O
computer	O
vision	O
is	O
based	O
on	O
multi-grid	B
techniques	I
such	O
methods	O
are	O
widely	O
used	O
in	O
numerical	O
linear	O
algebra	O
where	O
one	O
of	O
the	O
core	O
problems	O
is	O
the	O
fast	O
solution	O
of	O
linear	O
systems	O
of	O
equations	O
this	O
is	O
equivalent	O
to	O
map	O
estimation	O
in	O
a	O
gaussian	B
mrf	B
in	O
the	O
computer	O
vision	O
context	O
and	O
huttenlocher	O
suggested	O
using	O
the	O
following	O
heuristic	O
to	O
significantly	O
speedup	O
bp	B
construct	O
a	O
coarse-to-fine	O
grid	O
compute	O
messages	O
at	O
the	O
coarse	O
level	O
and	O
use	O
this	O
to	O
initialize	O
messages	O
at	O
the	O
level	O
below	O
when	O
we	O
reach	O
the	O
bottom	O
level	O
just	O
a	O
few	O
iterations	O
of	O
standard	O
bp	B
are	O
required	O
since	O
long-range	O
communication	O
has	O
already	O
been	O
achieved	O
via	O
the	O
initialization	O
process	O
the	O
beliefs	O
at	O
the	O
coarse	O
level	O
are	O
computed	O
over	O
a	O
small	O
number	O
of	O
large	O
blocks	O
the	O
local	B
evidence	B
is	O
computed	O
from	O
the	O
average	O
log-probability	O
each	O
possible	O
block	O
label	B
assigns	O
to	O
all	O
the	O
pixels	O
in	O
the	O
block	O
the	O
pairwise	O
potential	O
is	O
based	O
on	O
the	O
discrepancy	O
between	O
labels	O
of	O
neighboring	O
blocks	O
taking	O
into	O
account	O
their	O
size	O
we	O
can	O
then	O
run	O
lbp	B
at	O
the	O
coarse	O
level	O
and	O
then	O
use	O
this	O
to	O
initialize	O
the	O
messages	O
one	O
level	O
down	O
note	O
that	O
the	O
model	O
is	O
still	O
a	O
flat	O
grid	O
however	O
the	O
initialization	O
process	O
exploits	O
the	O
multi-scale	O
nature	O
of	O
the	O
problem	O
see	O
and	O
huttenlocher	O
for	O
details	O
chapter	O
more	O
variational	B
inference	B
cascades	O
another	O
trick	O
for	O
handling	O
high-dimensional	O
state-spaces	O
that	O
can	O
also	O
be	O
used	O
with	O
exact	O
inference	B
for	O
chain-structured	O
crfs	O
is	O
to	O
prune	O
out	O
improbable	O
states	O
based	O
on	O
a	O
comin	O
fact	O
one	O
can	O
create	O
a	O
hierarchy	O
of	O
models	O
which	O
tradeoff	O
putationally	O
cheap	O
filtering	B
step	O
speed	O
and	O
accuracy	O
this	O
is	O
called	O
a	O
computational	O
cascade	B
in	O
the	O
case	O
of	O
chains	O
one	O
can	O
guarantee	O
that	O
the	O
cascade	B
will	O
never	O
filter	O
out	O
the	O
true	O
map	O
solution	O
et	O
al	O
loopy	B
belief	B
propagation	I
theoretical	O
issues	O
we	O
now	O
attempt	O
to	O
understand	O
the	O
lbp	B
algorithm	O
from	O
a	O
variational	O
point	O
of	O
view	O
our	O
presentation	O
is	O
closely	O
based	O
on	O
an	O
excellent	O
review	O
article	O
and	O
jordan	O
this	O
paper	O
is	O
sometimes	O
called	O
the	O
monster	O
its	O
own	O
authors	O
in	O
view	O
of	O
its	O
length	O
and	O
technical	O
difficulty	O
this	O
section	O
just	O
sketches	O
some	O
of	O
the	O
main	O
results	O
to	O
simplify	O
the	O
presentation	O
we	O
focus	O
on	O
the	O
special	O
case	O
of	O
pairwise	O
ugms	O
with	O
discrete	B
variables	O
and	O
tabular	O
potentials	O
many	O
of	O
the	O
results	O
generalize	B
to	O
ugms	O
with	O
higher-order	O
clique	B
potentials	O
includes	O
dgms	O
but	O
this	O
makes	O
the	O
notation	O
more	O
complex	O
and	O
friedman	O
for	O
details	O
of	O
the	O
general	O
case	O
ugms	O
represented	O
in	O
exponential	B
family	B
form	O
we	O
assume	O
the	O
distribution	O
has	O
the	O
following	O
form	O
s	O
v	O
e	O
px	O
g	O
z	O
exp	O
sxs	O
stxs	O
xt	O
px	O
ex	O
t	O
z	O
where	O
graph	B
g	O
has	O
nodes	B
v	O
and	O
edges	B
e	O
we	O
will	O
drop	O
the	O
explicit	O
conditioning	B
on	O
and	O
g	O
for	O
brevity	O
since	O
we	O
assume	O
both	O
are	O
known	O
and	O
fixed	O
we	O
can	O
rewrite	O
this	O
in	O
exponential	B
family	B
form	O
as	O
follows	O
exp	O
ex	O
where	O
sj	O
stjk	O
are	O
all	O
the	O
node	O
and	O
edge	O
parameters	O
canonical	B
parameters	I
and	O
jixs	O
j	O
xt	O
k	O
are	O
all	O
the	O
node	O
and	O
edge	O
indicator	O
functions	O
sufficient	B
statistics	I
note	O
we	O
use	O
s	O
t	O
v	O
to	O
index	O
nodes	B
and	O
j	O
k	O
x	O
to	O
index	O
states	O
the	O
mean	B
of	O
the	O
sufficient	B
statistics	I
are	O
known	O
as	O
the	O
mean	B
parameters	O
of	O
the	O
model	O
and	O
are	O
given	O
by	O
e	O
jspxs	O
j	O
xt	O
sjs	O
this	O
is	O
a	O
vector	O
of	O
length	O
d	O
containing	O
the	O
node	O
and	O
edge	O
marginals	O
it	O
completely	O
characterizes	O
the	O
distribution	O
px	O
so	O
we	O
sometimes	O
treat	O
as	O
a	O
distribution	O
itself	O
equation	O
is	O
called	O
the	O
standard	B
overcomplete	B
representation	I
it	O
is	O
called	O
overcomplete	B
because	O
it	O
ignores	O
the	O
sum-to-one	O
constraints	O
in	O
some	O
cases	O
it	O
is	O
convenient	O
to	O
remove	O
loopy	B
belief	B
propagation	I
theoretical	O
issues	O
this	O
redundancy	O
for	O
example	O
consider	O
an	O
ising	B
model	I
where	O
xs	O
the	O
model	O
can	O
be	O
written	O
as	O
s	O
v	O
e	O
px	O
z	O
exp	O
sxs	O
stxsxt	O
hence	O
we	O
can	O
use	O
the	O
following	O
minimal	B
parameterization	O
s	O
v	O
xsxt	O
t	O
e	O
r	O
d	O
where	O
d	O
the	O
corresponding	O
mean	B
parameters	O
are	O
s	O
pxs	O
and	O
st	O
pxs	O
xt	O
the	O
marginal	B
polytope	I
x	O
the	O
space	O
of	O
allowable	O
vectors	O
is	O
called	O
the	O
marginal	B
polytope	I
and	O
is	O
denoted	O
mg	O
where	O
g	O
is	O
the	O
structure	O
of	O
the	O
graph	B
defining	O
the	O
ugm	B
this	O
is	O
defined	O
to	O
be	O
the	O
set	O
of	O
all	O
mean	B
parameters	O
for	O
the	O
given	O
model	O
that	O
can	O
be	O
generated	O
from	O
a	O
valid	O
probability	O
distribution	O
d	O
p	O
s	O
t	O
mg	O
r	O
for	O
example	O
consider	O
an	O
ising	B
model	I
px	O
for	O
some	O
px	O
if	O
we	O
have	O
just	O
two	O
nodes	B
connected	O
as	O
one	O
can	O
show	O
that	O
we	O
have	O
the	O
following	O
minimal	B
set	O
of	O
constraints	O
and	O
we	O
can	O
write	O
these	O
in	O
matrix-vector	O
form	O
as	O
x	O
these	O
four	O
constraints	O
define	O
a	O
series	O
of	O
half-planes	O
whose	O
intersection	O
defines	O
a	O
polytope	O
as	O
shown	O
in	O
figure	O
since	O
mg	O
is	O
obtained	O
by	O
taking	O
a	O
convex	B
combination	I
of	O
the	O
vectors	O
it	O
can	O
also	O
be	O
written	O
as	O
the	O
convex	B
hull	I
of	O
the	O
feature	O
set	O
mg	O
conv	O
dx	O
for	O
example	O
for	O
a	O
node	O
mrf	B
with	O
binary	O
states	O
we	O
have	O
mg	O
these	O
are	O
the	O
four	O
black	O
dots	O
in	O
figure	O
we	O
see	O
that	O
the	O
convex	B
hull	I
defines	O
the	O
same	O
volume	O
as	O
the	O
intersection	O
of	O
half-spaces	O
the	O
marginal	B
polytope	I
will	O
play	O
a	O
crucial	O
role	O
in	O
the	O
approximate	B
inference	B
algorithms	O
we	O
discuss	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
chapter	O
more	O
variational	B
inference	B
illustration	O
of	O
the	O
marginal	B
polytope	I
for	O
an	O
ising	B
model	I
with	O
two	O
variables	O
figure	O
cartoon	O
illustration	O
of	O
the	O
set	O
mf	O
which	O
is	O
a	O
nonconvex	O
inner	O
bound	O
on	O
the	O
marginal	B
polytope	I
mg	O
mf	O
is	O
used	O
by	O
mean	B
field	O
cartoon	O
illustration	O
of	O
the	O
relationship	O
between	O
mg	O
and	O
lg	O
which	O
is	O
used	O
by	O
loopy	O
bp	B
the	O
set	O
lg	O
is	O
always	O
an	O
outer	O
bound	O
on	O
mg	O
and	O
the	O
inclusion	O
mg	O
lg	O
is	O
strict	B
whenever	O
g	O
has	O
loops	O
both	O
sets	O
are	O
polytopes	O
which	O
can	O
be	O
defined	O
as	O
an	O
intersection	O
of	O
half-planes	O
by	O
facets	O
or	O
as	O
the	O
convex	B
hull	I
of	O
the	O
vertices	B
lg	O
actually	O
has	O
fewer	O
facets	O
than	O
mg	O
despite	O
the	O
picture	O
in	O
fact	O
lg	O
has	O
oxv	O
facets	O
where	O
is	O
the	O
number	O
of	O
states	O
per	O
variable	O
is	O
the	O
number	O
of	O
variables	O
and	O
is	O
the	O
number	O
of	O
edges	B
by	O
contrast	O
mg	O
has	O
oxv	O
facets	O
on	O
the	O
other	O
hand	O
lg	O
has	O
more	O
vertices	B
than	O
mg	O
despite	O
the	O
picture	O
since	O
lg	O
contains	O
all	O
the	O
binary	O
vector	O
extreme	O
points	O
mg	O
plus	O
additional	O
fractional	O
extreme	O
points	O
source	O
figures	O
and	O
of	O
and	O
jordan	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
exact	O
inference	B
as	O
a	O
variational	O
optimization	B
problem	O
recall	B
from	O
section	O
that	O
the	O
goal	O
of	O
variational	B
inference	B
is	O
to	O
find	O
the	O
distribution	O
q	O
that	O
maximizes	O
the	O
energy	B
functional	I
lq	O
kl	O
log	O
z	O
eq	O
px	O
h	O
log	O
z	O
where	O
px	O
is	O
the	O
unnormalized	O
posterior	O
let	O
q	O
p	O
then	O
the	O
exact	O
energy	B
functional	I
becomes	O
max	O
mg	O
t	O
h	O
if	O
we	O
write	O
log	O
px	O
t	O
and	O
we	O
where	O
ep	B
is	O
a	O
joint	B
distribution	I
over	O
all	O
state	B
configurations	O
x	O
it	O
is	O
valid	O
to	O
write	O
h	O
since	O
the	O
kl	B
divergence	I
is	O
zero	O
when	O
p	O
q	O
we	O
know	O
that	O
max	O
mg	O
t	O
h	O
log	O
z	O
this	O
is	O
a	O
way	O
to	O
cast	O
exact	O
inference	B
as	O
a	O
variational	O
optimization	B
problem	O
equation	O
seems	O
easy	O
to	O
optimize	O
the	O
objective	O
is	O
concave	B
since	O
it	O
is	O
the	O
sum	O
of	O
a	O
linear	O
function	O
and	O
a	O
concave	B
function	O
figure	O
to	O
see	O
why	O
entropy	B
is	O
concave	B
furthermore	O
we	O
are	O
maximizing	O
this	O
over	O
a	O
convex	B
set	O
however	O
the	O
marginal	B
polytope	I
mg	O
has	O
exponentially	O
many	O
facets	O
in	O
some	O
cases	O
there	O
is	O
structure	O
to	O
this	O
polytope	O
that	O
can	O
be	O
exploited	O
by	O
dynamic	B
programming	I
we	O
saw	O
in	O
chapter	O
but	O
in	O
general	O
exact	O
inference	B
takes	O
exponential	O
time	O
most	O
of	O
the	O
existing	O
deterministic	O
approximate	B
inference	B
schemes	O
that	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
can	O
be	O
seen	O
as	O
different	O
approximations	O
to	O
the	O
marginal	B
polytope	I
as	O
we	O
explain	O
below	O
loopy	B
belief	B
propagation	I
theoretical	O
issues	O
mean	B
field	O
as	O
a	O
variational	O
optimization	B
problem	O
we	O
discussed	O
mean	B
field	O
at	O
length	O
in	O
chapter	O
let	O
us	O
re-interpret	O
mean	B
field	O
inference	B
in	O
our	O
new	O
more	O
abstract	O
framework	O
this	O
will	O
help	O
us	O
compare	O
it	O
to	O
other	O
approximate	O
methods	O
which	O
we	O
discuss	O
below	O
first	O
let	O
f	O
be	O
an	O
edge	O
subgraph	B
of	O
the	O
original	O
graph	B
g	O
and	O
let	O
if	O
i	O
be	O
the	O
subset	O
of	O
sufficient	B
statistics	I
associated	O
with	O
the	O
cliques	B
of	O
f	O
let	O
be	O
the	O
set	O
of	O
canonical	B
parameters	I
for	O
the	O
full	B
model	O
and	O
define	O
the	O
canonical	O
parameter	B
space	O
for	O
the	O
submodel	O
as	O
follows	O
i	O
if	O
in	O
other	O
words	O
we	O
require	O
that	O
the	O
natural	B
parameters	I
associated	O
with	O
the	O
sufficient	B
statistics	I
outside	O
of	O
our	O
chosen	O
class	O
to	O
be	O
zero	O
in	O
the	O
case	O
of	O
a	O
fully	O
factorized	O
approximation	O
we	O
remove	O
all	O
edges	B
from	O
the	O
graph	B
giving	O
for	O
example	O
st	O
t	O
e	O
in	O
the	O
case	O
of	O
structured	O
mean	B
field	O
we	O
set	O
st	O
for	O
edges	B
which	O
are	O
not	O
in	O
our	O
tractable	O
subgraph	B
next	O
we	O
define	O
the	O
mean	B
parameter	B
space	O
of	O
the	O
restricted	O
model	O
as	O
follows	O
mf	O
r	O
this	O
is	O
called	O
an	O
inner	B
approximation	I
to	O
the	O
marginal	B
polytope	I
since	O
mf	O
mg	O
see	O
figure	O
for	O
a	O
sketch	O
note	O
that	O
mf	O
is	O
a	O
non-convex	O
polytope	O
which	O
results	O
in	O
multiple	O
local	O
optima	O
by	O
contrast	O
some	O
of	O
the	O
approximations	O
we	O
will	O
consider	O
later	O
will	O
be	O
convex	B
for	O
some	O
d	O
e	O
we	O
define	O
the	O
entropy	B
of	O
our	O
approximation	O
h	O
as	O
the	O
entropy	B
of	O
the	O
distribution	O
defined	O
on	O
submodel	O
f	O
then	O
we	O
define	O
the	O
mean	B
field	O
energy	B
functional	I
optimization	B
problem	O
as	O
follows	O
max	O
mf	O
t	O
h	O
log	O
z	O
sxs	O
sxs	O
max	O
p	O
d	O
s	O
v	O
xs	O
e	O
xsxt	O
in	O
the	O
case	O
of	O
the	O
fully	O
factorized	O
mean	B
field	O
approximation	O
for	O
pairwise	O
ugms	O
we	O
can	O
write	O
this	O
objective	O
as	O
follows	O
stxs	O
xt	O
sxs	O
txt	O
h	O
s	O
s	O
v	O
where	O
s	O
p	O
and	O
p	O
is	O
the	O
probability	B
simplex	I
over	O
x	O
mean	B
field	O
involves	O
a	O
concave	B
objective	O
being	O
maximized	O
over	O
a	O
non-convex	O
set	O
it	O
is	O
typically	O
optimized	O
using	O
coordinate	O
ascent	O
since	O
it	O
is	O
easy	O
to	O
optimize	O
a	O
scalar	O
concave	B
function	O
over	O
p	O
for	O
each	O
s	O
for	O
example	O
for	O
a	O
pairwise	O
ugm	B
we	O
get	O
sxs	O
exp	O
sxs	O
exp	O
txt	O
stxs	O
xt	O
t	O
nbrs	O
xt	O
lbp	B
as	O
a	O
variational	O
optimization	B
problem	O
in	O
this	O
section	O
we	O
explain	O
how	O
lbp	B
can	O
be	O
viewed	O
as	O
a	O
variational	B
inference	B
problem	O
chapter	O
more	O
variational	B
inference	B
figure	O
illustration	O
of	O
pairwise	O
ugm	B
on	O
binary	O
nodes	B
together	O
with	O
a	O
set	O
of	O
pseudo	B
marginals	I
that	O
are	O
not	O
globally	O
consistent	B
a	O
slice	O
of	O
the	O
marginal	B
polytope	I
illustrating	O
the	O
set	O
of	O
feasible	O
edge	O
marginals	O
assuming	O
the	O
node	O
marginals	O
are	O
clamped	O
at	O
source	O
figure	O
of	O
and	O
jordan	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
an	O
outer	B
approximation	I
to	O
the	O
marginal	B
polytope	I
if	O
we	O
want	O
to	O
consider	O
all	O
possible	O
probability	O
distributions	O
which	O
are	O
markov	B
wrt	O
our	O
model	O
we	O
need	O
to	O
consider	O
all	O
vectors	O
mg	O
since	O
the	O
set	O
mg	O
is	O
exponentially	O
large	O
it	O
is	O
usually	O
infeasible	O
to	O
optimize	O
over	O
a	O
standard	O
strategy	O
in	O
combinatorial	O
optimization	B
is	O
to	O
relax	O
the	O
constraints	O
in	O
this	O
case	O
instead	O
of	O
requiring	O
probability	O
vector	O
to	O
live	O
in	O
mg	O
we	O
consider	O
a	O
vector	O
that	O
only	O
satisfies	O
the	O
following	O
local	B
consistency	I
constraints	O
xt	O
sxs	O
xs	O
stxs	O
xt	O
sxs	O
the	O
first	O
constraint	O
is	O
called	O
the	O
normalization	O
constraint	O
and	O
the	O
second	O
is	O
called	O
the	O
marginalization	O
constraint	O
we	O
then	O
define	O
the	O
set	O
lg	O
holds	O
s	O
v	O
and	O
holds	O
t	O
e	O
the	O
set	O
lg	O
is	O
also	O
a	O
polytope	O
but	O
it	O
only	O
has	O
ov	O
constraints	O
it	O
is	O
a	O
convex	B
outer	B
approximation	I
on	O
mg	O
as	O
shown	O
in	O
figure	O
we	O
call	O
the	O
terms	O
s	O
st	O
lg	O
pseudo	B
marginals	I
since	O
they	O
may	O
not	O
correspond	O
to	O
marginals	O
of	O
any	O
valid	O
probability	O
distribution	O
as	O
an	O
example	O
of	O
this	O
consider	O
figure	O
the	O
picture	O
shows	O
a	O
set	O
of	O
pseudo	O
node	O
and	O
edge	O
marginals	O
which	O
satisfy	O
the	O
local	B
consistency	I
requirements	O
however	O
they	O
are	O
not	O
globally	O
consistent	B
to	O
see	O
why	O
note	O
that	O
implies	O
implies	O
but	O
implies	O
which	O
is	O
not	O
possible	O
and	O
jordan	O
for	O
a	O
formal	O
proof	O
indeed	O
figure	O
shows	O
that	O
lg	O
contains	O
points	O
that	O
are	O
not	O
in	O
mg	O
we	O
claim	O
that	O
mg	O
lg	O
with	O
equality	O
iff	B
g	O
is	O
a	O
tree	B
to	O
see	O
this	O
first	O
consider	O
loopy	B
belief	B
propagation	I
theoretical	O
issues	O
an	O
element	O
mg	O
any	O
such	O
vector	O
must	O
satisfy	O
the	O
normalization	O
and	O
marginalization	O
constraints	O
hence	O
mg	O
lg	O
now	O
consider	O
the	O
converse	O
suppose	O
t	O
is	O
a	O
tree	B
and	O
let	O
lt	O
by	O
definition	O
this	O
satisfies	O
the	O
normalization	O
and	O
marginalization	O
constraints	O
however	O
any	O
tree	B
can	O
be	O
represented	O
in	O
the	O
form	O
p	O
sxs	O
stxs	O
xt	O
sxs	O
txt	O
s	O
v	O
e	O
hence	O
satsifying	O
normalization	O
and	O
local	B
consistency	I
is	O
enough	O
to	O
define	O
a	O
valid	O
distribution	O
for	O
any	O
tree	B
hence	O
mt	O
as	O
well	O
in	O
contrast	O
if	O
the	O
graph	B
has	O
loops	O
we	O
have	O
that	O
mg	O
lg	O
see	O
figure	O
for	O
an	O
example	O
of	O
this	O
fact	O
the	O
entropy	B
approximation	O
from	O
equation	O
we	O
can	O
write	O
the	O
exact	O
entropy	B
of	O
any	O
tree	B
structured	O
distribution	O
mt	O
as	O
follows	O
e	O
ist	O
st	O
sxs	O
log	O
sxs	O
hs	O
s	O
xs	O
xs	O
s	O
v	O
xs	O
xt	O
h	O
hs	O
s	O
ist	O
st	O
stxs	O
xt	O
log	O
stxs	O
xt	O
sxs	O
txt	O
note	O
that	O
we	O
can	O
rewrite	O
the	O
mutual	B
information	B
term	O
in	O
the	O
form	O
ist	O
st	O
hs	O
sht	O
t	O
hst	O
st	O
and	O
hence	O
we	O
get	O
the	O
following	O
alternative	O
but	O
equivalent	O
expression	O
h	O
s	O
hst	O
st	O
e	O
where	O
ds	O
is	O
the	O
degree	B
of	O
neighbors	B
for	O
node	O
s	O
the	O
approximation	O
to	O
the	O
entropy	B
is	O
simply	O
the	O
use	O
of	O
equation	O
even	O
when	O
we	O
s	O
v	O
s	O
v	O
don	O
t	O
have	O
a	O
tree	B
hbethe	O
hs	O
s	O
we	O
define	O
the	O
bethe	B
free	B
energy	I
as	O
fbethe	O
t	O
hbethe	O
e	O
ist	O
st	O
we	O
define	O
the	O
bethe	B
energy	B
functional	I
as	O
the	O
negative	O
of	O
the	O
bethe	B
free	B
energy	I
hans	O
bethe	B
was	O
a	O
german-american	O
physicist	O
chapter	O
more	O
variational	B
inference	B
the	O
lbp	B
objective	O
combining	O
the	O
outer	B
approximation	I
lg	O
with	O
the	O
bethe	B
approximation	O
to	O
the	O
entropy	B
we	O
get	O
the	O
following	O
bethe	B
variational	O
problem	O
min	O
lg	O
fbethe	O
max	O
lg	O
t	O
hbethe	O
the	O
space	O
we	O
are	O
optimizing	O
over	O
is	O
a	O
convex	B
set	O
but	O
the	O
objective	O
itself	O
is	O
not	O
concave	B
hbethe	O
is	O
not	O
concave	B
thus	O
there	O
can	O
be	O
multiple	O
local	O
optima	O
of	O
the	O
bvp	O
the	O
value	O
obtained	O
by	O
the	O
bvp	O
is	O
an	O
approximation	O
to	O
log	O
z	O
in	O
the	O
case	O
of	O
trees	O
the	O
approximation	O
is	O
exact	O
and	O
in	O
the	O
case	O
of	O
models	O
with	O
attractive	O
potentials	O
the	O
approximation	O
turns	O
out	O
to	O
be	O
an	O
upper	O
bound	O
et	O
al	O
message	B
passing	I
and	O
lagrange	B
multipliers	I
xs	O
sxs	O
and	O
the	O
marginalization	O
constraint	O
as	O
ctsxs	O
sxs	O
in	O
this	O
subsection	O
we	O
will	O
show	O
that	O
any	O
fixed	O
point	O
of	O
the	O
lbp	B
algorithm	O
defines	O
a	O
stationary	B
point	O
of	O
the	O
above	O
constrained	O
objective	O
let	O
us	O
define	O
the	O
normalization	O
constraint	O
at	O
css	O
stxs	O
xt	O
for	O
each	O
edge	O
t	O
s	O
we	O
can	O
now	O
write	O
the	O
lagrangian	B
as	O
sscss	O
l	O
t	O
hbethe	O
xt	O
s	O
constraint	O
that	O
is	O
not	O
explicitly	O
enforced	O
but	O
one	O
can	O
show	O
that	O
it	O
will	O
hold	O
at	O
the	O
optimum	O
since	O
some	O
simple	O
algebra	O
then	O
shows	O
that	O
l	O
yields	O
tsxsctsxs	O
stxtcstxt	O
st	O
xs	O
xt	O
xt	O
using	O
the	O
fact	O
that	O
the	O
marginalization	O
con	O
t	O
nbrs	O
log	O
sxs	O
ss	O
sxs	O
tsxs	O
log	O
stxs	O
xt	O
sxs	O
txt	O
stxs	O
xt	O
tsxs	O
stxt	O
where	O
we	O
have	O
defined	O
sxs	O
xt	O
straint	O
implies	O
sxs	O
sxs	O
we	O
get	O
log	O
stxs	O
xt	O
ss	O
tt	O
stxs	O
xt	O
sxs	O
txt	O
utxt	O
usxs	O
u	O
nbrst	O
u	O
nbrts	O
sxs	O
exp	O
sxs	O
stxs	O
xt	O
exp	O
stxs	O
xt	O
sxs	O
txt	O
t	O
nbrs	O
mtsxs	O
musxs	O
mutxt	O
u	O
nbrst	O
u	O
nbrts	O
to	O
make	O
the	O
connection	O
to	O
message	B
passing	I
define	O
mtsxs	O
exp	O
tsxs	O
with	O
this	O
notation	O
we	O
can	O
rewrite	O
the	O
above	O
equations	O
taking	O
exponents	O
of	O
both	O
sides	O
as	O
follows	O
extensions	O
of	O
belief	B
propagation	I
where	O
the	O
terms	O
are	O
absorbed	O
into	O
the	O
constant	O
of	O
proportionality	O
we	O
see	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
usual	O
expression	O
for	O
the	O
node	O
and	O
edge	O
marginals	O
in	O
lbp	B
to	O
derive	O
an	O
equation	O
for	O
the	O
messages	O
in	O
terms	O
of	O
other	O
messages	O
than	O
in	O
terms	O
of	O
stxs	O
xt	O
sxs	O
then	O
one	O
can	O
show	O
ts	O
we	O
enforce	O
the	O
marginalization	O
condition	O
that	O
exp	O
stxs	O
xt	O
txt	O
xt	O
mtsxs	O
xt	O
mutxt	O
u	O
nbrts	O
we	O
see	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
usual	O
expression	O
for	O
the	O
messages	O
in	O
lbp	B
loopy	O
bp	B
vs	O
mean	B
field	O
it	O
is	O
interesting	O
to	O
compare	O
the	O
naive	O
mean	B
field	O
and	O
lbp	B
approximations	O
there	O
are	O
several	O
obvious	O
differences	O
first	O
lbp	B
is	O
exact	O
for	O
trees	O
whereas	O
mf	O
is	O
not	O
suggesting	O
lbp	B
will	O
in	O
general	O
be	O
more	O
accurate	O
et	O
al	O
for	O
an	O
analysis	O
second	O
lbp	B
optimizes	O
over	O
node	O
and	O
edge	O
marginals	O
whereas	O
mf	O
only	O
optimizes	O
over	O
node	O
marginals	O
again	O
suggesting	O
lbp	B
will	O
be	O
more	O
accurate	O
third	O
in	O
the	O
case	O
that	O
the	O
true	O
edge	O
marginals	O
factorize	O
so	O
st	O
s	O
t	O
the	O
free	B
energy	I
approximations	O
will	O
be	O
the	O
same	O
in	O
both	O
cases	O
what	O
is	O
less	O
obvious	O
but	O
which	O
nevertheless	O
seems	O
to	O
be	O
true	O
is	O
that	O
the	O
mf	O
objective	O
has	O
many	O
more	O
local	O
optima	O
than	O
the	O
lbp	B
objective	O
so	O
optimizing	O
the	O
mf	O
objective	O
seems	O
to	O
be	O
harder	O
in	O
particular	O
shows	O
empirically	O
that	O
optimizing	O
mf	O
starting	O
from	O
uniform	O
or	O
random	O
initial	O
conditions	O
often	O
leads	O
to	O
poor	O
results	O
whereas	O
optimizing	O
bp	B
from	O
uniform	O
initial	O
messages	O
often	O
leads	O
to	O
good	O
results	O
furthermore	O
initializing	O
mf	O
with	O
the	O
bp	B
marginals	O
also	O
leads	O
to	O
good	O
results	O
mf	O
tends	O
to	O
be	O
more	O
overconfident	O
than	O
bp	B
indicating	O
that	O
the	O
problem	O
is	O
caused	O
not	O
by	O
the	O
inaccuracy	O
of	O
the	O
mf	O
approximation	O
but	O
rather	O
by	O
the	O
severe	O
non-convexity	O
of	O
the	O
mf	O
objective	O
and	O
by	O
the	O
weakness	O
of	O
the	O
standard	O
coordinate	O
descent	O
optimization	B
method	O
used	O
by	O
however	O
the	O
advantage	O
of	O
mf	O
is	O
that	O
it	O
gives	O
a	O
lower	O
bound	O
on	O
the	O
partition	B
function	I
unlike	O
bp	B
which	O
is	O
useful	O
when	O
using	O
it	O
as	O
a	O
subroutine	O
inside	O
a	O
learning	B
algorithm	O
also	O
mf	O
is	O
easier	O
to	O
extend	O
to	O
other	O
distributions	O
besides	O
discrete	B
and	O
gaussian	B
as	O
we	O
saw	O
in	O
chapter	O
intuitively	O
this	O
is	O
because	O
mf	O
only	O
works	O
with	O
marginal	O
distributions	O
which	O
have	O
a	O
single	O
type	O
rather	O
than	O
needing	O
to	O
define	O
pairwise	O
distributions	O
which	O
may	O
need	O
to	O
have	O
two	O
different	O
types	O
extensions	O
of	O
belief	B
propagation	I
in	O
this	O
section	O
we	O
discuss	O
various	O
extensions	O
of	O
lbp	B
generalized	B
belief	B
propagation	I
we	O
can	O
improve	O
the	O
accuracy	O
of	O
loopy	O
bp	B
by	O
clustering	B
together	O
nodes	B
that	O
form	O
a	O
tight	O
loop	B
this	O
is	O
known	O
as	O
the	O
cluster	B
variational	I
method	I
the	O
result	O
is	O
a	O
hyper-graph	O
which	O
is	O
a	O
graph	B
et	O
al	O
discusses	O
the	O
use	O
of	O
the	O
pattern	B
search	I
algorithm	O
to	O
speedup	O
mean	B
field	O
inference	B
in	O
the	O
case	O
of	O
continuous	O
random	O
variables	O
it	O
is	O
possible	O
that	O
similar	B
ideas	O
could	O
be	O
adapted	O
to	O
the	O
discrete	B
case	O
although	O
there	O
may	O
be	O
no	O
reason	O
to	O
do	O
this	O
given	O
that	O
lbp	B
already	O
works	O
well	O
in	O
the	O
discrete	B
case	O
chapter	O
more	O
variational	B
inference	B
figure	O
kikuchi	O
clusters	B
superimposed	O
on	O
a	O
lattice	B
graph	B
source	O
figure	O
of	O
and	O
jordan	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
corresponding	O
hyper-graph	O
where	O
there	O
are	O
hyper-edges	O
between	O
sets	O
of	O
vertices	B
instead	O
of	O
between	O
single	O
vertices	B
note	O
that	O
a	O
junction	B
tree	B
is	O
a	O
kind	O
of	O
hyper-graph	O
we	O
can	O
represent	O
hyper-graph	O
using	O
a	O
poset	O
ordered	O
set	O
diagram	O
where	O
each	O
node	O
represents	O
a	O
hyper-edge	O
and	O
there	O
is	O
an	O
arrow	O
if	O
see	O
figure	O
for	O
an	O
example	O
let	O
t	O
be	O
the	O
size	O
of	O
the	O
largest	O
hyper-edge	O
in	O
the	O
hyper-graph	O
if	O
we	O
allow	O
t	O
to	O
be	O
as	O
large	O
as	O
the	O
treewidth	B
of	O
the	O
graph	B
then	O
we	O
can	O
represent	O
the	O
hyper-graph	O
as	O
a	O
tree	B
and	O
the	O
method	O
will	O
be	O
exact	O
just	O
as	O
lbp	B
is	O
exact	O
on	O
regular	B
trees	O
treewidth	B
in	O
this	O
way	O
we	O
can	O
define	O
a	O
continuum	O
of	O
approximations	O
from	O
lbp	B
all	O
the	O
way	O
to	O
exact	O
inference	B
define	O
ltg	O
to	O
be	O
the	O
set	O
of	O
all	O
pseudo-marginals	O
such	O
that	O
normalization	O
and	O
marginalization	O
constraints	O
hold	O
on	O
a	O
hyper-graph	O
whose	O
largest	O
hyper-edge	O
is	O
of	O
size	O
t	O
for	O
example	O
in	O
figure	O
we	O
impose	O
constraints	O
of	O
the	O
form	O
furthermore	O
we	O
approximate	O
the	O
entropy	B
as	O
follows	O
hkikuchi	O
cghg	O
g	O
where	O
hg	O
g	O
is	O
the	O
entropy	B
of	O
the	O
joint	B
distribution	I
on	O
the	O
vertices	B
in	O
set	O
g	O
and	O
cg	O
is	O
called	O
the	O
overcounting	B
number	I
of	O
set	O
g	O
these	O
are	O
related	O
to	O
mobious	B
numbers	I
in	O
set	O
theory	O
rather	O
than	O
giving	O
a	O
precise	O
definition	O
we	O
just	O
give	O
a	O
simple	O
example	O
for	O
the	O
graph	B
in	O
figure	O
we	O
have	O
g	O
e	O
putting	O
these	O
two	O
approximations	O
together	O
we	O
can	O
define	O
the	O
kikuchi	O
free	O
as	O
follows	O
hkikuchi	O
fkikuchi	O
t	O
hkikuchi	O
ryoichi	O
kikuchi	O
is	O
a	O
japanese	O
physicist	O
extensions	O
of	O
belief	B
propagation	I
our	O
variational	O
problem	O
becomes	O
fkikuchi	O
max	O
ltg	O
min	O
t	O
hkikuchi	O
ltg	O
just	O
as	O
with	O
the	O
bethe	B
free	B
energy	I
this	O
is	O
not	O
a	O
concave	B
objective	O
there	O
are	O
several	O
possible	O
algorithms	O
for	O
finding	O
a	O
local	O
optimum	O
of	O
this	O
objective	O
including	O
a	O
message	B
passing	I
algorithm	O
known	O
as	O
generalized	B
belief	B
propagation	I
however	O
the	O
details	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
see	O
e	O
g	O
and	O
jordan	O
sec	O
or	O
and	O
friedman	O
sec	O
for	O
more	O
information	B
suffice	O
it	O
to	O
say	O
that	O
the	O
method	O
gives	O
more	O
accurate	O
results	O
than	O
lbp	B
but	O
at	O
increased	O
computational	O
cost	O
of	O
the	O
need	O
to	O
handle	O
clusters	B
of	O
nodes	B
this	O
cost	O
plus	O
the	O
complexity	O
of	O
the	O
approach	O
have	O
precluded	O
it	O
from	O
widespread	O
use	O
convex	B
belief	B
propagation	I
the	O
mean	B
field	O
energy	B
functional	I
is	O
concave	B
but	O
it	O
is	O
maximized	O
over	O
a	O
non-convex	O
inner	B
approximation	I
to	O
the	O
marginal	B
polytope	I
the	O
bethe	B
and	O
kikuchi	O
energy	O
functionals	O
are	O
not	O
concave	B
but	O
they	O
are	O
maximized	O
over	O
a	O
convex	B
outer	B
approximation	I
to	O
the	O
marginal	B
polytope	I
consequently	O
for	O
both	O
mf	O
and	O
lbp	B
the	O
optimization	B
problem	O
has	O
multiple	O
optima	O
so	O
the	O
methods	O
are	O
sensitive	O
to	O
the	O
initial	O
conditions	O
given	O
that	O
the	O
exact	O
formulation	O
a	O
concave	B
objective	O
maximized	O
over	O
a	O
convex	B
set	O
it	O
is	O
natural	O
to	O
try	O
to	O
come	O
up	O
with	O
an	O
appproximation	O
which	O
involves	O
a	O
concave	B
objective	O
being	O
maximized	O
over	O
a	O
convex	B
set	O
we	O
now	O
describe	O
one	O
method	O
known	O
as	O
convex	B
belief	B
propagation	I
this	O
involves	O
working	O
with	O
a	O
set	O
of	O
tractable	O
submodels	O
f	O
such	O
as	O
trees	O
or	O
planar	O
graphs	O
for	O
each	O
model	O
f	O
g	O
the	O
entropy	B
is	O
higher	O
h	O
h	O
since	O
f	O
has	O
fewer	O
constraints	O
consequently	O
any	O
convex	B
combination	I
of	O
such	O
subgraphs	O
will	O
have	O
higher	O
entropy	B
too	O
h	O
h	O
f	O
f	O
where	O
and	O
define	O
the	O
convex	B
free	B
energy	I
as	O
fconvex	O
t	O
h	O
f	O
furthermore	O
h	O
is	O
a	O
concave	B
function	O
of	O
we	O
now	O
we	O
define	O
the	O
concave	B
energy	B
functional	I
as	O
the	O
negative	O
of	O
the	O
convex	B
free	B
energy	I
we	O
discuss	O
how	O
to	O
optimize	O
below	O
having	O
defined	O
an	O
upper	O
bound	O
on	O
the	O
entropy	B
we	O
now	O
consider	O
a	O
convex	B
outerbound	O
on	O
the	O
marginal	B
polytope	I
of	O
mean	B
parameters	O
we	O
want	O
to	O
ensure	O
we	O
can	O
evaluate	O
the	O
entropy	B
of	O
any	O
vector	O
in	O
this	O
set	O
so	O
we	O
restrict	O
it	O
so	O
that	O
the	O
projection	B
of	O
onto	O
the	O
subgraph	B
g	O
lives	O
in	O
the	O
projection	B
of	O
m	O
onto	O
f	O
lgf	O
r	O
d	O
mf	O
f	O
f	O
this	O
is	O
a	O
convex	B
set	O
since	O
each	O
mf	O
is	O
a	O
projection	B
of	O
a	O
convex	B
set	O
hence	O
we	O
define	O
our	O
problem	O
as	O
min	O
lgf	O
fconvex	O
max	O
lgf	O
t	O
h	O
this	O
is	O
a	O
concave	B
objective	O
being	O
maximized	O
over	O
a	O
convex	B
set	O
and	O
hence	O
has	O
a	O
unique	O
maximum	O
we	O
give	O
a	O
specific	O
example	O
below	O
chapter	O
more	O
variational	B
inference	B
f	O
e	O
b	O
f	O
e	O
b	O
f	O
e	O
b	O
f	O
e	O
b	O
figure	O
a	O
graph	B
some	O
of	O
its	O
spanning	O
trees	O
source	O
figure	O
of	O
and	O
jordan	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
tree-reweighted	O
belief	B
propagation	I
consider	O
the	O
specific	O
case	O
where	O
f	O
is	O
all	O
spanning	O
trees	O
of	O
a	O
graph	B
for	O
any	O
given	O
tree	B
the	O
entropy	B
is	O
given	O
by	O
equation	O
to	O
compute	O
the	O
upper	O
bound	O
obtained	O
by	O
averaging	O
over	O
f	O
for	O
single	O
nodes	B
will	O
just	O
be	O
hs	O
since	O
node	O
s	O
all	O
trees	O
note	O
that	O
the	O
terms	O
f	O
but	O
the	O
mutual	B
information	B
term	O
ist	O
receives	O
weight	O
appears	O
in	O
every	O
tree	B
and	O
st	O
e	O
t	O
et	O
known	O
as	O
the	O
edge	B
appearance	I
probability	I
hence	O
we	O
have	O
the	O
following	O
upper	O
bound	O
on	O
the	O
entropy	B
h	O
hs	O
s	O
stist	O
st	O
e	O
s	O
v	O
the	O
edge	O
appearance	O
probabilities	O
live	O
in	O
a	O
space	O
called	O
the	O
spanning	B
tree	B
polytope	I
this	O
is	O
because	O
they	O
are	O
constrained	O
to	O
arise	O
from	O
a	O
distribution	O
over	O
trees	O
figure	O
gives	O
an	O
example	O
of	O
a	O
graph	B
and	O
three	O
of	O
its	O
spanning	O
trees	O
suppose	O
each	O
tree	B
has	O
equal	O
weight	O
under	O
the	O
edge	O
f	O
occurs	O
in	O
of	O
the	O
trees	O
so	O
f	O
the	O
edge	O
e	O
occurs	O
in	O
of	O
the	O
trees	O
so	O
e	O
the	O
edge	O
b	O
appears	O
in	O
all	O
of	O
the	O
trees	O
so	O
b	O
and	O
so	O
on	O
ideally	O
we	O
can	O
find	O
a	O
distribution	O
or	O
equivalently	O
edge	O
probabilities	O
in	O
the	O
spanning	B
tree	B
polytope	I
that	O
make	O
the	O
above	O
bound	O
as	O
tight	O
as	O
possible	O
an	O
algorithm	O
to	O
do	O
this	O
is	O
described	O
in	O
et	O
al	O
simpler	O
approach	O
is	O
to	O
generate	O
spanning	O
trees	O
of	O
g	O
at	O
random	O
until	O
all	O
edges	B
are	O
covered	O
or	O
use	O
all	O
single	O
edges	B
with	O
weight	O
e	O
what	O
about	O
the	O
set	O
we	O
are	O
optimizing	O
over	O
we	O
require	O
mt	O
for	O
each	O
tree	B
t	O
which	O
means	O
enforcing	O
normalization	O
and	O
local	B
consistency	I
since	O
we	O
have	O
to	O
do	O
this	O
for	O
every	O
tree	B
we	O
are	O
enforcing	O
normalization	O
and	O
local	B
consistency	I
on	O
every	O
edge	O
hence	O
lgf	O
lg	O
so	O
our	O
final	O
optimization	B
problem	O
is	O
as	O
follows	O
t	O
s	O
v	O
max	O
lg	O
hs	O
s	O
stist	O
st	O
eg	O
which	O
is	O
the	O
same	O
as	O
the	O
lbp	B
objective	O
except	O
for	O
the	O
crucial	O
st	O
weights	O
so	O
long	O
as	O
st	O
for	O
all	O
edges	B
t	O
this	O
problem	O
is	O
strictly	O
concave	B
with	O
a	O
unique	O
maximum	O
how	O
can	O
we	O
find	O
this	O
global	O
optimum	O
as	O
for	O
lbp	B
there	O
are	O
several	O
algorithms	O
but	O
perhaps	O
the	O
simplest	O
is	O
a	O
modification	O
of	O
belief	B
propagation	I
known	O
as	O
tree	B
reweighted	I
belief	B
propagation	I
expectation	B
propagation	I
also	O
called	O
trw	B
or	O
trbp	B
for	O
short	O
the	O
message	O
from	O
t	O
to	O
s	O
is	O
now	O
a	O
function	O
of	O
all	O
messages	O
sent	O
from	O
other	O
neighbors	B
v	O
to	O
t	O
as	O
before	O
but	O
now	O
it	O
is	O
also	O
a	O
function	O
of	O
the	O
message	O
sent	O
from	O
s	O
to	O
t	O
specifically	O
exp	O
xt	O
mtsxs	O
st	O
stxs	O
xt	O
txt	O
v	O
nbrtsmvtxt	O
vt	O
ts	O
at	O
convergence	O
the	O
node	O
and	O
edge	O
pseudo	B
marginals	I
are	O
given	O
by	O
sxs	O
exp	O
sxs	O
stxs	O
xt	O
stxs	O
xt	O
vs	O
v	O
nbrs	O
v	O
nbrstmvsxs	O
vs	O
st	O
v	O
nbrtsmvtxt	O
vt	O
ts	O
stxs	O
xt	O
exp	O
stxs	O
xt	O
sxs	O
txt	O
st	O
this	O
algorithm	O
can	O
be	O
derived	O
using	O
a	O
method	O
similar	B
to	O
that	O
described	O
in	O
section	O
if	O
st	O
for	O
all	O
edges	B
t	O
e	O
the	O
algorithm	O
reduces	O
to	O
the	O
standard	O
lbp	B
algorithm	O
however	O
the	O
condition	O
st	O
implies	O
every	O
edge	O
is	O
present	O
in	O
every	O
spanning	O
tree	B
with	O
probability	O
which	O
is	O
only	O
possible	O
if	O
the	O
original	O
graph	B
is	O
a	O
tree	B
hence	O
the	O
method	O
is	O
only	O
equivalent	O
to	O
standard	O
lbp	B
on	O
trees	O
when	O
the	O
method	O
is	O
of	O
course	O
exact	O
in	O
general	O
this	O
message	B
passing	I
scheme	O
is	O
not	O
guaranteed	O
to	O
converge	B
to	O
the	O
unique	O
global	O
optimum	O
one	O
can	O
devise	O
double-loop	O
methods	O
that	O
are	O
guaranteed	O
to	O
converge	B
and	O
shashua	O
but	O
in	O
practice	O
using	O
damped	B
updates	I
as	O
in	O
equation	O
is	O
often	O
sufficient	O
to	O
ensure	O
convergence	O
it	O
is	O
also	O
possible	O
to	O
produce	O
a	O
convex	B
version	O
of	O
the	O
kikuchi	B
free	B
energy	I
which	O
one	O
can	O
optimize	O
with	O
a	O
modified	O
version	O
of	O
generalized	B
belief	B
propagation	I
see	O
and	O
jordan	O
sec	O
for	O
details	O
from	O
equation	O
and	O
using	O
the	O
fact	O
that	O
the	O
trbp	B
entropy	B
approximation	O
is	O
an	O
upper	O
bound	O
on	O
the	O
true	O
entropy	B
wee	O
see	O
that	O
the	O
trbp	B
objective	O
is	O
an	O
upper	O
bound	O
on	O
log	O
z	O
using	O
the	O
fact	O
that	O
ist	O
hs	O
ht	O
hst	O
we	O
can	O
rewrite	O
the	O
upper	O
bound	O
as	O
follows	O
cshs	O
s	O
log	O
z	O
log	O
z	O
t	O
where	O
cs	O
sthst	O
st	O
st	O
t	O
st	O
s	O
expectation	B
propagation	I
expectation	B
propagation	I
is	O
a	O
form	O
of	O
belief	B
propagation	I
where	O
the	O
messages	O
are	O
approximated	O
it	O
is	O
a	O
generalization	B
of	O
the	O
assumed	O
density	O
filtering	B
algorithm	O
discussed	O
in	O
section	O
in	O
that	O
method	O
we	O
approximated	O
the	O
posterior	O
at	O
each	O
step	O
using	O
an	O
assumed	O
functional	O
form	O
such	O
as	O
a	O
gaussian	B
this	O
posterior	O
can	O
be	O
computed	O
using	O
moment	B
matching	I
which	O
locally	O
optimizes	O
kl	O
for	O
a	O
single	O
term	O
from	O
this	O
we	O
derived	O
the	O
message	O
to	O
send	O
to	O
the	O
next	O
time	O
step	O
chapter	O
more	O
variational	B
inference	B
adf	B
works	O
well	O
for	O
sequential	B
bayesian	B
updating	O
but	O
the	O
answer	O
it	O
gives	O
depends	O
on	O
the	O
order	O
in	O
which	O
the	O
data	O
is	O
seen	O
ep	B
essentially	O
corrects	O
this	O
flaw	O
by	O
making	O
multiple	O
passes	O
over	O
the	O
data	O
ep	B
is	O
an	O
offline	B
or	O
batch	B
inference	B
algorithm	O
ep	B
as	O
a	O
variational	B
inference	B
problem	O
we	O
now	O
explain	O
how	O
to	O
view	O
ep	B
in	O
terms	O
of	O
variational	B
inference	B
we	O
follow	O
the	O
presentation	O
of	O
and	O
jordan	O
sec	O
which	O
should	O
be	O
consulted	O
for	O
further	O
details	O
suppose	O
the	O
joint	B
distribution	I
can	O
be	O
written	O
in	O
exponential	B
family	B
form	O
as	O
follows	O
px	O
exp	O
t	O
t	O
i	O
ix	O
exp	O
where	O
we	O
have	O
partitioned	O
the	O
parameters	O
and	O
the	O
sufficient	B
statistics	I
into	O
a	O
tractable	O
term	O
of	O
size	O
dt	O
and	O
di	O
intractable	O
terms	O
i	O
each	O
of	O
size	O
b	O
for	O
example	O
consider	O
the	O
problem	O
of	O
inferring	O
an	O
unknown	B
vector	O
x	O
when	O
the	O
observation	B
model	I
is	O
a	O
mixture	B
of	O
two	O
gaussians	O
one	O
centered	O
at	O
x	O
and	O
one	O
centered	O
at	O
can	O
be	O
used	O
to	O
represent	O
outliers	B
for	O
example	O
minka	O
invented	O
ep	B
calls	O
this	O
the	O
clutter	B
problem	I
more	O
formally	O
we	O
assume	O
an	O
observation	B
model	I
of	O
the	O
form	O
pyx	O
wn	O
i	O
n	O
ai	B
where	O
w	O
is	O
the	O
known	O
mixing	O
weight	O
of	O
outliers	B
and	O
a	O
is	O
the	O
variance	B
of	O
the	O
background	O
distribution	O
assuming	O
a	O
fixed	O
prior	O
of	O
the	O
form	O
px	O
we	O
can	O
write	O
our	O
model	O
in	O
the	O
required	O
form	O
as	O
follows	O
n	O
pyix	O
exp	O
xt	O
exp	O
log	O
pyix	O
xt	O
this	O
matches	O
our	O
canonical	B
form	I
where	O
exp	O
t	O
corresponds	O
to	O
exp	O
using	O
xxt	O
and	O
we	O
set	O
ix	O
log	O
pyix	O
i	O
and	O
di	O
n	O
the	O
exact	O
inference	B
problem	O
corresponds	O
to	O
max	O
t	O
t	O
h	O
m	O
where	O
m	O
is	O
the	O
set	O
of	O
mean	B
parameters	O
realizable	O
by	O
any	O
probability	O
distribution	O
as	O
seen	O
through	O
the	O
eyes	O
of	O
the	O
sufficient	B
statistics	I
dt	O
r	O
m	O
r	O
as	O
it	O
stands	O
it	O
is	O
intractable	O
to	O
perform	O
inference	B
in	O
this	O
distribution	O
for	O
example	O
in	O
our	O
clutter	O
example	O
the	O
posterior	O
contains	O
modes	O
but	O
suppose	O
we	O
incorporate	O
just	O
one	O
of	O
the	O
intractable	O
terms	O
say	O
the	O
i	O
th	O
one	O
we	O
will	O
call	O
this	O
the	O
i-augmented	O
distribution	O
di	O
b	O
e	O
di	O
px	O
i	O
exp	O
t	O
exp	O
t	O
i	O
ix	O
expectation	B
propagation	I
in	O
our	O
clutter	O
example	O
this	O
becomes	O
px	O
i	O
exp	O
xt	O
ai	B
wn	O
i	O
this	O
is	O
tractable	O
to	O
compute	O
since	O
it	O
is	O
just	O
a	O
mixture	B
of	I
gaussians	I
fashion	O
first	O
we	O
approximate	O
the	O
convex	B
set	O
m	O
with	O
another	O
larger	O
convex	B
set	O
the	O
key	O
idea	O
behind	O
ep	B
is	O
to	O
work	O
with	O
these	O
the	O
i-augmented	O
distributions	O
in	O
an	O
iterative	O
l	O
m	O
i	O
m	O
i	O
dt	O
e	O
and	O
m	O
i	O
i	O
r	O
where	O
m	O
r	O
b	O
i	O
ix	O
next	O
we	O
approximate	O
the	O
entropy	B
by	O
the	O
following	O
term-by-term	O
approximation	O
dt	O
r	O
hep	O
h	O
i	O
h	O
then	O
the	O
ep	B
problem	O
becomes	O
max	O
l	O
t	O
t	O
hep	O
optimizing	O
the	O
ep	B
objective	O
using	O
moment	B
matching	I
we	O
now	O
discuss	O
how	O
to	O
maximize	O
the	O
ep	B
objective	O
in	O
equation	O
let	O
us	O
duplicate	O
di	O
times	O
to	O
yield	O
i	O
the	O
augmented	O
set	O
of	O
parameters	O
we	O
need	O
to	O
optimize	O
is	O
now	O
i	O
idi	O
r	O
dt	O
dt	O
r	O
bdi	O
subject	O
to	O
the	O
constraints	O
that	O
i	O
and	O
i	O
i	O
m	O
i	O
let	O
us	O
associate	O
a	O
vector	O
of	O
lagrange	B
multipliers	I
i	O
r	O
dt	O
with	O
the	O
first	O
set	O
of	O
constraints	O
then	O
the	O
partial	O
lagrangian	B
becomes	O
l	O
t	O
h	O
i	O
i	O
h	O
i	O
i	O
h	O
i	O
t	O
t	O
i	O
i	O
by	O
solving	O
l	O
we	O
can	O
show	O
that	O
the	O
corresponding	O
distribution	O
in	O
m	O
has	O
the	O
form	O
qx	O
exp	O
it	O
the	O
t	O
i	O
terms	O
represents	O
an	O
approximation	O
to	O
the	O
i	O
th	O
intractable	O
term	O
using	O
the	O
sufficient	B
statistics	I
from	O
the	O
base	B
distribution	I
as	O
we	O
will	O
see	O
below	O
similarly	O
by	O
solving	O
i	O
il	O
we	O
find	O
that	O
the	O
corresponding	O
distribution	O
in	O
m	O
i	O
has	O
the	O
form	O
qix	O
i	O
exp	O
jt	O
t	O
i	O
ix	O
chapter	O
more	O
variational	B
inference	B
this	O
corresponds	O
to	O
removing	O
the	O
approximation	O
to	O
the	O
i	O
th	O
term	O
i	O
from	O
the	O
base	B
distribution	I
and	O
adding	O
in	O
the	O
correct	O
i	O
th	O
term	O
i	O
finally	O
l	O
just	O
enforces	O
the	O
constraints	O
that	O
eq	O
and	O
i	O
eqi	O
are	O
equal	O
in	O
other	O
words	O
we	O
get	O
the	O
following	O
moment	B
matching	I
constraints	O
qx	O
qix	O
i	O
thus	O
the	O
overall	O
algorithm	O
is	O
as	O
follows	O
first	O
we	O
initialize	O
the	O
i	O
then	O
we	O
iterate	O
the	O
following	O
to	O
convergence	O
pick	O
a	O
term	O
i	O
compute	O
qi	O
to	O
removing	O
the	O
old	O
approximation	O
to	O
i	O
and	O
adding	O
in	O
the	O
new	O
one	O
then	O
update	O
the	O
i	O
term	O
in	O
q	O
by	O
solving	O
the	O
moment	B
matching	I
equation	O
eqi	O
eq	O
that	O
this	O
particular	O
optimization	B
scheme	O
is	O
not	O
guaranteed	O
to	O
converge	B
to	O
a	O
fixed	O
point	O
an	O
equivalent	O
way	O
of	O
stating	O
the	O
algorithm	O
is	O
as	O
follows	O
let	O
us	O
assume	O
the	O
true	O
distribution	O
is	O
given	O
by	O
pxd	O
z	O
i	O
fix	O
i	O
qx	O
z	O
fix	O
we	O
approximate	O
each	O
fi	O
by	O
fi	O
and	O
set	O
now	O
we	O
repeat	O
the	O
following	O
until	O
convergence	O
choose	O
a	O
factor	B
fi	O
to	O
refine	O
remove	O
fi	O
from	O
the	O
posterior	O
by	O
dividing	O
it	O
out	O
q	O
ix	O
qx	O
fix	O
zi	O
this	O
can	O
be	O
implemented	O
by	O
substracting	O
off	O
the	O
natural	B
parameters	I
of	O
fi	O
from	O
q	O
compute	O
the	O
new	O
posterior	O
qnewx	O
by	O
solving	O
fixq	O
ixqnewx	O
kl	O
min	O
qnewx	O
this	O
can	O
be	O
done	O
by	O
equating	O
the	O
moments	O
of	O
qnewx	O
with	O
those	O
of	O
qix	O
q	O
ixfix	O
the	O
corresponding	O
normalization	O
constant	O
has	O
the	O
form	O
zi	O
q	O
ixfixdx	O
compute	O
the	O
new	O
factor	B
that	O
was	O
implicitly	O
used	O
it	O
can	O
be	O
later	O
removed	O
fix	O
zi	O
qnewx	O
q	O
ix	O
expectation	B
propagation	I
after	O
convergence	O
we	O
can	O
approximate	O
the	O
marginal	B
likelihood	B
using	O
pd	O
fixdx	O
i	O
we	O
will	O
give	O
some	O
examples	O
of	O
this	O
below	O
which	O
will	O
make	O
things	O
clearer	O
ep	B
for	O
the	O
clutter	B
problem	I
let	O
us	O
return	O
to	O
considering	O
the	O
clutter	B
problem	I
our	O
presentation	O
is	O
based	O
on	O
for	O
simplicity	O
we	O
will	O
assume	O
that	O
the	O
prior	O
is	O
a	O
spherical	B
gaussian	B
px	O
n	O
bi	O
also	O
we	O
choose	O
to	O
approximate	O
the	O
posterior	O
by	O
a	O
spherical	B
gaussian	B
qx	O
n	O
vi	O
we	O
set	O
to	O
be	O
the	O
prior	O
this	O
can	O
be	O
held	O
fixed	O
the	O
factor	B
approximations	O
will	O
be	O
gaussian	B
like	O
terms	O
of	O
the	O
form	O
fix	O
sin	O
vii	O
note	O
however	O
that	O
in	O
the	O
ep	B
updates	O
the	O
variances	O
may	O
be	O
negative	O
thus	O
these	O
terms	O
should	O
the	O
variance	B
is	O
be	O
interpreted	O
as	O
functions	O
but	O
not	O
necessarily	O
probability	O
distributions	O
negative	O
it	O
means	O
the	O
that	O
fi	O
curves	O
upwards	O
instead	O
of	O
downwards	O
first	O
we	O
remove	O
fix	O
from	O
qx	O
by	O
division	O
which	O
yields	O
q	O
ix	O
n	O
i	O
v	O
ii	O
where	O
v	O
i	O
v	O
v	O
m	O
i	O
m	O
v	O
iv	O
i	O
mi	B
the	O
normalization	O
constant	O
is	O
given	O
by	O
i	O
zi	O
wn	O
i	O
i	O
n	O
ai	B
next	O
we	O
compute	O
qnewx	O
by	O
computing	O
the	O
mean	B
and	O
variance	B
of	O
q	O
ixfix	O
as	O
follows	O
m	O
m	O
i	O
i	O
v	O
v	O
i	O
i	O
i	O
w	O
zi	O
m	O
i	O
i	O
iyi	O
dv	O
i	O
v	O
i	O
n	O
ai	B
v	O
i	O
i	O
v	O
i	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
and	O
i	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
that	O
yi	O
is	O
not	O
clutter	O
i	O
v	O
v	O
i	O
finally	O
we	O
compute	O
the	O
new	O
factor	B
fi	O
whose	O
parameters	O
are	O
given	O
by	O
v	O
mi	B
m	O
i	O
v	O
iv	O
i	O
m	O
i	O
i	O
v	O
ii	O
si	O
zi	O
for	O
a	O
handy	O
crib	O
sheet	O
containing	O
many	O
of	O
the	O
standard	O
equations	O
needed	O
for	O
deriving	O
gaussian	B
ep	B
algorithms	O
see	O
httpresearch	O
microsoft	B
comen-usumpeopleminkapapersepminka-ep-quickref	O
pdf	B
chapter	O
more	O
variational	B
inference	B
at	O
convergence	O
we	O
can	O
approximate	O
the	O
marginal	B
likelihood	B
as	O
follows	O
pd	O
c	O
mt	O
m	O
v	O
mt	O
i	O
mi	B
vi	O
vi	O
in	O
it	O
is	O
shown	O
that	O
at	O
least	O
on	O
this	O
example	O
ep	B
gives	O
better	O
accuracy	O
per	O
unit	O
of	O
cpu	O
time	O
than	O
vb	B
and	O
mcmc	B
lbp	B
is	O
a	O
special	O
case	O
of	O
ep	B
we	O
now	O
show	O
that	O
loopy	B
belief	B
propagation	I
is	O
a	O
special	O
case	O
of	O
ep	B
where	O
the	O
base	B
distribution	I
contains	O
the	O
node	O
marginals	O
and	O
the	O
intractable	O
terms	O
correspond	O
to	O
the	O
edge	O
potentials	O
we	O
if	O
there	O
are	O
m	O
nodes	B
the	O
assume	O
the	O
model	O
has	O
the	O
pairwise	O
form	O
shown	O
in	O
equation	O
base	B
distribution	I
takes	O
the	O
form	O
px	O
m	O
exp	O
sxs	O
the	O
entropy	B
of	O
this	O
distribution	O
is	O
simply	O
h	O
h	O
s	O
s	O
if	O
we	O
add	O
in	O
the	O
u	O
v	O
edge	O
the	O
uv	O
augmented	O
distribution	O
has	O
the	O
form	O
px	O
uv	O
exp	O
sxs	O
exp	O
uvxu	O
xv	O
s	O
v	O
s	O
v	O
since	O
this	O
graph	B
is	O
a	O
tree	B
the	O
exact	O
entropy	B
of	O
this	O
distribution	O
is	O
given	O
by	O
h	O
s	O
i	O
uv	O
h	O
uv	O
where	O
i	O
uv	O
u	O
v	O
h	O
uv	O
is	O
the	O
mutual	B
information	B
thus	O
the	O
ep	B
approximation	O
to	O
the	O
entropy	B
of	O
the	O
full	B
distribution	O
is	O
given	O
by	O
s	O
hep	O
s	O
s	O
h	O
s	O
h	O
s	O
uv	O
h	O
s	O
h	O
s	O
i	O
uv	O
e	O
i	O
uv	O
e	O
h	O
s	O
s	O
e	O
which	O
is	O
precisely	O
the	O
bethe	B
approximation	O
to	O
the	O
entropy	B
expectation	B
propagation	I
we	O
now	O
show	O
that	O
the	O
convex	B
set	O
that	O
ep	B
is	O
optimizing	O
over	O
l	O
given	O
by	O
equation	O
is	O
the	O
same	O
as	O
the	O
one	O
that	O
lbp	B
is	O
optimizing	O
over	O
lg	O
given	O
in	O
equation	O
first	O
let	O
us	O
consider	O
the	O
set	O
m	O
this	O
consists	O
of	O
all	O
marginal	O
distributions	O
s	O
s	O
v	O
realizable	O
by	O
a	O
factored	O
distribution	O
this	O
is	O
therefore	O
equivalent	O
to	O
the	O
set	O
of	O
all	O
distributions	O
which	O
satisfy	O
non-negativity	O
sxs	O
and	O
the	O
local	O
normalization	O
constraint	O
now	O
consider	O
the	O
set	O
m	O
uv	O
for	O
a	O
single	O
u	O
v	O
edge	O
this	O
is	O
equivalent	O
to	O
the	O
marginal	B
polytope	I
mguv	O
where	O
guv	O
is	O
the	O
graph	B
with	O
the	O
single	O
u	O
v	O
edge	O
added	O
since	O
this	O
graph	B
corresponds	O
to	O
a	O
tree	B
this	O
set	O
also	O
satisfies	O
the	O
marginalization	O
conditions	O
xs	O
uvxu	O
xv	O
uxu	O
uvxu	O
xv	O
vxv	O
xv	O
xu	O
since	O
l	O
is	O
the	O
union	O
of	O
such	O
sets	O
as	O
we	O
sweep	O
over	O
all	O
edges	B
in	O
the	O
graph	B
we	O
recover	O
the	O
same	O
set	O
as	O
lg	O
we	O
have	O
shown	O
that	O
the	O
bethe	B
approximation	O
is	O
equivalent	O
to	O
the	O
ep	B
approximation	O
we	O
now	O
show	O
how	O
the	O
ep	B
algorithm	O
reduces	O
to	O
lbp	B
associated	O
with	O
each	O
intractable	O
term	O
i	O
v	O
will	O
be	O
a	O
pair	O
of	O
lagrange	B
multipliers	I
uvxv	O
vuxu	O
recalling	O
that	O
t	O
sxss	O
the	O
base	B
distribution	I
in	O
equation	O
has	O
the	O
form	O
qx	O
exp	O
sxs	O
exp	O
uvxv	O
vuxu	O
s	O
sxs	O
e	O
exp	O
s	O
tsxs	O
t	O
n	O
similarly	O
the	O
augmented	O
distribution	O
in	O
equation	O
has	O
the	O
form	O
quvx	O
qx	O
exp	O
uvxu	O
xv	O
uvxv	O
vuxu	O
we	O
now	O
need	O
to	O
update	O
uxu	O
and	O
vxv	O
to	O
enforce	O
the	O
moment	B
matching	I
constraints	O
eq	O
equv	O
it	O
can	O
be	O
shown	O
that	O
this	O
can	O
be	O
done	O
by	O
performing	O
the	O
usual	O
sum-product	B
message	B
passing	I
step	O
along	O
the	O
u	O
v	O
edge	O
both	O
directions	O
where	O
the	O
messages	O
are	O
given	O
by	O
muvxv	O
exp	O
uvxv	O
and	O
mvuxu	O
exp	O
vuxu	O
once	O
we	O
have	O
updated	O
q	O
we	O
can	O
derive	O
the	O
corresponding	O
messages	O
uv	O
and	O
vu	O
the	O
above	O
analysis	O
suggests	O
a	O
natural	O
extension	B
where	O
we	O
make	O
the	O
base	B
distribution	I
be	O
a	O
tree	B
structure	O
instead	O
of	O
a	O
fully	O
factored	O
distribution	O
we	O
then	O
add	O
in	O
one	O
edge	O
at	O
a	O
time	O
absorb	O
its	O
effect	O
and	O
approximate	O
the	O
resulting	O
distribution	O
by	O
a	O
new	O
tree	B
this	O
is	O
known	O
as	O
tree	B
ep	B
and	O
qi	O
and	O
is	O
more	O
accurate	O
than	O
lbp	B
and	O
sometimes	O
faster	O
by	O
considering	O
other	O
kinds	O
of	O
structured	O
base	O
distributions	O
we	O
can	O
derive	O
algorothms	O
that	O
outperform	O
generalization	B
belief	B
propagation	I
et	O
al	O
ranking	B
players	O
using	O
trueskill	B
we	O
now	O
present	O
an	O
interesting	O
application	O
of	O
ep	B
to	O
the	O
problem	O
of	O
ranking	B
players	O
who	O
compete	O
in	O
games	O
microsoft	B
uses	O
this	O
method	O
known	O
as	O
trueskill	B
et	O
al	O
to	O
rank	O
chapter	O
more	O
variational	B
inference	B
figure	O
a	O
dgm	B
representing	O
the	O
trueskill	B
model	O
for	O
players	O
and	O
teams	O
where	O
team	O
is	O
player	O
team	O
is	O
players	O
and	O
and	O
team	O
is	O
player	O
we	O
assume	O
there	O
are	O
two	O
games	O
team	O
vs	O
team	O
and	O
team	O
vs	O
team	O
nodes	B
with	O
double	O
circles	O
are	O
deterministic	O
a	O
factor	B
graph	B
representation	O
of	O
the	O
model	O
where	O
we	O
assume	O
there	O
are	O
players	O
no	O
teams	O
there	O
are	O
games	O
player	O
vs	O
player	O
and	O
player	O
vs	O
player	O
the	O
numbers	O
inside	O
circles	O
represent	O
steps	O
in	O
the	O
message	B
passing	I
algorithm	O
expectation	B
propagation	I
ist	O
i	O
players	O
who	O
use	O
the	O
xbox	B
live	O
online	O
gaming	O
system	O
this	O
system	O
process	O
over	O
games	O
per	O
day	O
making	O
this	O
one	O
of	O
the	O
largest	O
application	O
of	O
bayesian	B
statistics	I
to	O
the	O
same	O
method	O
can	O
also	O
be	O
applied	O
to	O
other	O
games	O
such	O
as	O
tennis	O
or	O
the	O
basic	O
idea	O
is	O
shown	O
in	O
figure	O
we	O
assume	O
each	O
player	O
i	O
has	O
a	O
latent	B
or	O
true	O
underlying	O
skill	O
level	O
si	O
r	O
these	O
skill	O
levels	O
can	O
evolve	O
over	O
time	O
according	O
to	O
a	O
simple	O
in	O
any	O
given	O
game	O
we	O
define	O
the	O
performance	O
dynamical	O
model	O
pst	O
of	O
player	O
i	O
to	O
be	O
pi	O
which	O
has	O
the	O
conditional	O
distribution	O
ppisi	O
we	O
then	O
define	O
the	O
performance	O
of	O
a	O
team	O
to	O
be	O
the	O
sum	O
of	O
the	O
performance	O
of	O
its	O
constituent	O
players	O
for	O
example	O
in	O
figure	O
we	O
assume	O
team	O
is	O
composed	O
of	O
players	O
and	O
so	O
we	O
define	O
finally	O
we	O
assume	O
that	O
the	O
outcome	O
of	O
a	O
game	O
depends	O
on	O
the	O
difference	O
in	O
performance	O
levels	O
of	O
the	O
two	O
teams	O
for	O
example	O
in	O
figure	O
we	O
assume	O
where	O
and	O
where	O
means	O
team	O
won	O
and	O
means	O
team	O
won	O
thus	O
the	O
prior	O
probability	O
that	O
team	O
wins	O
is	O
n	O
ist	O
i	O
where	O
n	O
and	O
n	O
to	O
simplify	O
the	O
presentation	O
of	O
the	O
algorithm	O
we	O
will	O
ignore	O
the	O
dynamical	O
model	O
and	O
assume	O
a	O
common	O
static	O
factored	O
gaussian	B
prior	O
n	O
on	O
the	O
skills	O
also	O
we	O
will	O
assume	O
that	O
each	O
team	O
consists	O
of	O
player	O
so	O
ti	O
pi	O
and	O
that	O
there	O
can	O
be	O
no	O
ties	O
finally	O
we	O
will	O
integrate	B
out	I
the	O
performance	O
variables	O
pi	O
and	O
assume	O
leading	O
to	O
a	O
final	O
model	O
of	O
the	O
form	O
ps	O
i	O
n	O
sjg	O
signdg	O
pdgs	O
pygdg	O
where	O
ig	O
is	O
the	O
first	O
player	O
of	O
game	O
g	O
and	O
jg	O
is	O
the	O
second	O
player	O
this	O
is	O
represented	O
in	O
factor	B
graph	B
form	O
in	O
in	O
figure	O
we	O
have	O
kinds	O
of	O
factors	B
the	O
prior	O
factor	B
fisi	O
n	O
the	O
game	O
factor	B
hgsig	O
sjg	O
dg	O
sjg	O
and	O
the	O
outcome	O
factor	B
kgdg	O
yg	O
iyg	O
signdg	O
since	O
the	O
likelihood	B
term	O
is	O
not	O
conjugate	O
to	O
the	O
gaussian	B
priors	O
we	O
will	O
have	O
to	O
perform	O
approximate	B
inference	B
thus	O
even	O
when	O
the	O
graph	B
is	O
a	O
tree	B
we	O
will	O
need	O
to	O
iterate	O
there	O
were	O
an	O
additional	O
game	O
say	O
between	O
player	O
and	O
player	O
then	O
the	O
graph	B
would	O
no	O
longer	O
be	O
a	O
tree	B
we	O
will	O
represent	O
all	O
messages	O
and	O
marginal	O
beliefs	O
by	O
gaussians	O
we	O
will	O
use	O
the	O
notation	O
and	O
v	O
for	O
the	O
mean	B
and	O
variance	B
moment	B
parameters	I
and	O
and	O
for	O
the	O
precision	B
and	O
precision-adjusted	O
mean	B
natural	B
parameters	I
naive	O
bayes	O
classifiers	O
which	O
are	O
widely	O
used	O
in	O
spam	B
filters	O
are	O
often	O
described	O
as	O
the	O
most	O
common	O
application	O
of	O
bayesian	B
methods	O
however	O
the	O
parameters	O
of	O
such	O
models	O
are	O
usually	O
fit	O
using	O
non-bayesian	O
methods	O
such	O
as	O
penalized	O
maximum	O
likelihood	B
our	O
presentation	O
of	O
this	O
algorithm	O
is	O
based	O
in	O
part	O
on	O
lecture	O
notes	O
by	O
carl	O
rasmussen	O
joaquin	O
quinonero-candela	O
available	O
at	O
note	O
that	O
this	O
is	O
very	O
similar	B
to	O
probit	B
regression	B
discussed	O
in	O
section	O
except	O
the	O
inputs	O
are	O
differences	O
of	O
latent	B
dimensional	O
factors	B
if	O
we	O
assume	O
a	O
logistic	B
noise	O
model	O
instead	O
of	O
a	O
gaussian	B
noise	O
model	O
we	O
recover	O
the	O
bradley	B
terry	I
model	O
of	O
ranking	B
chapter	O
more	O
variational	B
inference	B
we	O
initialize	O
by	O
assuming	O
that	O
at	O
iteration	O
the	O
initial	O
upward	O
messages	O
from	O
factors	B
hg	O
to	O
variables	O
si	O
are	O
uniform	O
i	O
e	O
hg	O
sig	O
hg	O
sig	O
hg	O
sig	O
and	O
similarly	O
as	O
illustrated	O
in	O
figure	O
we	O
give	O
the	O
details	O
of	O
these	O
steps	O
below	O
the	O
messages	O
passing	O
algorithm	O
consists	O
of	O
steps	O
per	O
game	O
hg	O
sjg	O
compute	O
the	O
posterior	O
over	O
the	O
skills	O
variables	O
qtsi	O
t	O
i	O
hg	O
si	O
ncsi	O
t	O
mt	O
t	O
hg	O
si	O
i	O
t	O
i	O
t	O
i	O
t	O
hg	O
si	O
g	O
g	O
g	O
compute	O
the	O
message	O
from	O
the	O
skills	O
variables	O
down	O
to	O
the	O
game	O
factor	B
hg	O
mt	O
sig	O
hg	O
qtsig	O
hg	O
sig	O
mt	O
mt	O
sjg	O
hg	O
qtsjg	O
hg	O
sjg	O
mt	O
where	O
the	O
division	O
is	O
implemented	O
by	O
subtracting	O
the	O
natural	B
parameters	I
as	O
follows	O
t	O
sig	O
hg	O
t	O
sig	O
t	O
hg	O
sig	O
t	O
sig	O
hg	O
t	O
sig	O
t	O
hg	O
sig	O
and	O
similarly	O
for	O
sjg	O
compute	O
the	O
message	O
from	O
the	O
game	O
factor	B
hg	O
down	O
to	O
the	O
difference	O
variable	O
dg	O
mt	O
hg	O
dg	O
sig	O
hg	O
sjg	O
hg	O
sig	O
hg	O
vt	O
sig	O
hg	O
sjg	O
hgdg	O
sig	O
n	O
sjg	O
t	O
sjg	O
hg	O
dsjg	O
hg	O
dg	O
sjg	O
hg	O
vt	O
sig	O
hg	O
vt	O
sjg	O
hg	O
hg	O
dg	O
vt	O
n	O
t	O
n	O
t	O
vt	O
hg	O
dg	O
vt	O
t	O
hg	O
dg	O
t	O
sig	O
hg	O
t	O
sjg	O
hg	O
dsjg	O
compute	O
the	O
posterior	O
over	O
the	O
difference	O
variables	O
qtdg	O
mt	O
hg	O
dg	O
dg	O
n	O
t	O
n	O
t	O
hg	O
dg	O
g	O
vt	O
g	O
vt	O
hg	O
dg	O
signdg	O
expectation	B
propagation	I
function	O
function	O
figure	O
function	O
function	O
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
trueskillplot	O
that	O
the	O
upward	O
message	O
from	O
the	O
kg	O
factor	B
is	O
constant	O
we	O
can	O
find	O
these	O
parameters	O
by	O
moment	B
matching	I
as	O
follows	O
yg	O
t	O
hg	O
dg	O
t	O
hg	O
dg	O
yg	O
t	O
hg	O
dg	O
t	O
hg	O
dg	O
g	O
yg	O
t	O
t	O
hg	O
dg	O
t	O
hg	O
dg	O
vt	O
g	O
vt	O
hg	O
dg	O
n	O
derivation	O
of	O
these	O
equations	O
is	O
left	O
as	O
a	O
modification	O
to	O
exercise	O
these	O
functions	O
are	O
plotted	O
in	O
figure	O
let	O
us	O
try	O
to	O
understand	O
these	O
equations	O
suppose	O
t	O
hg	O
dg	O
is	O
a	O
large	O
positive	O
number	O
that	O
means	O
we	O
expect	O
based	O
on	O
the	O
current	O
estimate	O
of	O
the	O
skills	O
that	O
dg	O
will	O
be	O
large	O
and	O
positive	O
consequently	O
if	O
we	O
observe	O
yg	O
we	O
will	O
not	O
be	O
surprised	O
that	O
ig	O
is	O
the	O
winner	O
which	O
is	O
reflected	O
in	O
the	O
fact	O
that	O
the	O
update	O
factor	B
for	O
the	O
hg	O
dg	O
similarly	O
the	O
update	O
factor	B
for	O
the	O
variance	B
is	O
small	O
mean	B
is	O
small	O
t	O
hg	O
dg	O
however	O
if	O
we	O
observe	O
yg	O
then	O
the	O
update	O
factor	B
for	O
the	O
mean	B
t	O
and	O
variance	B
becomes	O
quite	O
large	O
compute	O
the	O
upward	O
message	O
from	O
the	O
difference	O
variable	O
to	O
the	O
game	O
factor	B
hg	O
mt	O
dg	O
hg	O
qtdg	O
mt	O
dg	O
hg	O
g	O
t	O
t	O
dg	O
hh	O
t	O
hg	O
dg	O
t	O
dg	O
hh	O
t	O
g	O
t	O
hg	O
dg	O
compute	O
the	O
upward	O
messages	O
from	O
the	O
game	O
factor	B
to	O
the	O
skill	O
variables	O
let	O
us	O
assume	O
chapter	O
more	O
variational	B
inference	B
figure	O
a	O
dag	B
representing	O
a	O
partial	O
ordering	O
of	O
players	O
posterior	B
mean	B
plusminus	O
standard	B
deviation	I
for	O
the	O
latent	B
skills	O
of	O
each	O
player	O
based	O
on	O
games	O
figure	O
generated	O
by	O
trueskilldemo	O
that	O
ig	O
is	O
the	O
winner	O
and	O
jg	O
is	O
the	O
loser	O
then	O
we	O
have	O
mt	O
hg	O
sig	O
sjg	O
dg	O
hg	O
sjg	O
hg	O
vt	O
hg	O
sig	O
t	O
hg	O
sig	O
and	O
similarly	O
hgdg	O
sig	O
t	O
hg	O
sig	O
dg	O
hg	O
vt	O
vt	O
hg	O
sig	O
sjg	O
hg	O
dg	O
hg	O
t	O
sjg	O
hg	O
n	O
vt	O
t	O
mt	O
hg	O
sjg	O
sjg	O
dg	O
hg	O
sig	O
hg	O
n	O
vt	O
t	O
dg	O
hg	O
hgdg	O
sig	O
t	O
hg	O
sjg	O
dg	O
hg	O
vt	O
t	O
sig	O
hg	O
vt	O
hg	O
sjg	O
sig	O
hg	O
vt	O
hg	O
sjg	O
t	O
hg	O
sjg	O
when	O
we	O
compute	O
at	O
the	O
next	O
iteration	O
by	O
combining	O
mt	O
with	O
the	O
prior	O
factor	B
we	O
will	O
see	O
that	O
the	O
posterior	B
mean	B
of	O
sig	O
goes	O
up	O
similarly	O
the	O
posterior	B
mean	B
of	O
sjg	O
goes	O
down	O
hg	O
sig	O
it	O
is	O
straightforward	O
to	O
combine	O
ep	B
with	O
adf	B
to	O
perform	O
online	O
inference	B
which	O
is	O
necessary	O
for	O
most	O
practical	O
applications	O
let	O
us	O
consider	O
a	O
simple	O
example	O
of	O
this	O
method	O
we	O
create	O
a	O
partial	O
ordering	O
of	O
players	O
as	O
shown	O
in	O
figure	O
we	O
then	O
sample	O
some	O
game	O
outcomes	O
from	O
this	O
graph	B
where	O
a	O
map	O
state	B
estimation	I
parent	O
always	O
beats	O
a	O
child	O
we	O
pass	O
this	O
data	O
into	O
iterations	O
of	O
the	O
ep	B
algorithm	O
and	O
infer	O
the	O
posterior	B
mean	B
and	O
variance	B
for	O
each	O
player	O
s	O
skill	O
level	O
the	O
results	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
method	O
has	O
correctly	O
inferred	O
the	O
rank	O
ordering	O
of	O
the	O
players	O
other	O
applications	O
of	O
ep	B
the	O
trueskill	B
model	O
was	O
developed	O
by	O
researchers	O
at	O
microsoft	B
they	O
and	O
others	O
have	O
extended	O
the	O
model	O
to	O
a	O
variety	O
of	O
other	O
interesting	O
applications	O
including	O
personalized	O
ad	O
recommendation	O
et	O
al	O
predicting	O
click-through-rate	O
on	O
ads	O
in	O
the	O
bing	B
search	O
engine	O
et	O
al	O
etc	O
they	O
have	O
also	O
developed	O
a	O
general	O
purpose	O
bayesian	B
inference	B
toolbox	O
based	O
on	O
ep	B
called	O
infer	O
net	O
et	O
al	O
ep	B
has	O
also	O
been	O
used	O
for	O
a	O
variety	O
of	O
other	O
models	O
such	O
as	O
gaussian	B
process	I
classification	O
and	O
rasmussen	O
see	O
httpresearch	O
microsoft	B
comen-usumpeople	O
minkapaperseproadmap	O
html	O
for	O
a	O
list	O
of	O
other	O
ep	B
applications	O
map	O
state	B
estimation	I
in	O
this	O
section	O
we	O
consider	O
the	O
problem	O
of	O
finding	O
the	O
most	O
probable	O
configuration	O
of	O
variables	O
in	O
a	O
discrete-state	O
graphical	B
model	I
i	O
e	O
our	O
goal	O
is	O
to	O
find	O
a	O
map	O
assignment	O
of	O
the	O
following	O
form	O
x	O
ixi	O
px	O
arg	O
max	O
x	O
x	O
m	O
f	O
arg	O
max	O
x	O
x	O
m	O
t	O
arg	O
max	O
x	O
x	O
m	O
i	O
v	O
f	O
f	O
where	O
i	O
are	O
the	O
singleton	O
node	O
potentials	O
and	O
f	O
are	O
the	O
factor	B
potentials	O
this	O
section	O
we	O
follow	O
the	O
notation	O
of	O
et	O
al	O
which	O
considers	O
the	O
case	O
of	O
general	O
potentials	O
not	O
just	O
pairwise	O
ones	O
note	O
that	O
the	O
partition	B
function	I
z	O
plays	O
no	O
role	O
in	O
map	O
estimation	O
if	O
the	O
treewidth	B
is	O
low	O
we	O
can	O
solve	O
this	O
problem	O
with	O
the	O
junction	B
tree	B
algorithm	I
but	O
in	O
general	O
this	O
problem	O
is	O
intractable	O
in	O
this	O
section	O
we	O
discuss	O
various	O
approximations	O
building	O
on	O
the	O
material	O
from	O
section	O
linear	O
programming	O
relaxation	O
we	O
can	O
rewrite	O
the	O
objective	O
in	O
terms	O
of	O
the	O
variational	O
parameters	O
as	O
follows	O
t	O
arg	O
max	O
x	O
x	O
m	O
t	O
arg	O
max	O
mg	O
where	O
jixf	O
k	O
and	O
is	O
a	O
probability	O
vector	O
in	O
the	O
marginal	B
polytope	I
to	O
see	O
why	O
this	O
equation	O
is	O
true	O
note	O
that	O
we	O
can	O
just	O
set	O
to	O
be	O
a	O
degenerate	B
distribution	O
with	O
ixs	O
x	O
s	O
is	O
the	O
optimal	O
assigment	O
of	O
node	O
s	O
so	O
instead	O
of	O
optimizing	O
over	O
discrete	B
assignments	O
we	O
now	O
optimize	O
over	O
probability	O
distributions	O
s	O
where	O
x	O
it	O
seems	O
like	O
we	O
have	O
an	O
easy	O
problem	O
to	O
solve	O
since	O
the	O
objective	O
in	O
equation	O
is	O
linear	O
in	O
and	O
the	O
constraint	O
set	O
mg	O
is	O
convex	B
the	O
trouble	O
is	O
mg	O
in	O
general	O
has	O
a	O
number	O
of	O
facets	O
that	O
is	O
exponential	O
in	O
the	O
number	O
of	O
nodes	B
a	O
standard	O
strategy	O
in	O
combinatorial	O
optimization	B
is	O
to	O
relax	O
the	O
constraints	O
in	O
this	O
case	O
instead	O
of	O
requiring	O
probability	O
vector	O
to	O
live	O
in	O
the	O
marginal	B
polytope	I
mg	O
we	O
allow	O
it	O
to	O
chapter	O
more	O
variational	B
inference	B
live	O
inside	O
a	O
convex	B
outer	O
bound	O
lg	O
having	O
defined	O
this	O
relaxed	O
constraint	O
set	O
we	O
have	O
max	O
x	O
x	O
m	O
t	O
max	O
mg	O
t	O
max	O
lg	O
t	O
if	O
the	O
solution	O
is	O
integral	O
it	O
is	O
exact	O
if	O
it	O
is	O
fractional	O
it	O
is	O
an	O
approximation	O
this	O
is	O
called	O
a	O
order	O
linear	B
programming	I
relaxtion	I
the	O
reason	O
it	O
is	O
called	O
first-order	O
is	O
that	O
the	O
constraints	O
that	O
are	O
enforced	O
are	O
those	O
that	O
correspond	O
to	O
consistency	O
on	O
a	O
tree	B
which	O
is	O
a	O
graph	B
of	O
treewidth	B
it	O
is	O
possible	O
to	O
enforce	O
higher-order	O
consistency	O
using	O
graphs	O
with	O
larger	O
treewidth	B
and	O
jordan	O
sec	O
for	O
details	O
how	O
should	O
we	O
actually	O
perform	O
the	O
optimization	B
we	O
can	O
use	O
a	O
generic	O
linear	O
programming	O
package	O
but	O
this	O
is	O
often	O
very	O
slow	O
fortunately	O
in	O
the	O
case	O
of	O
graphical	B
models	I
it	O
is	O
possible	O
to	O
devise	O
specialised	O
distributed	O
message	B
passing	I
algorithms	O
for	O
solving	O
this	O
optimization	B
problem	O
as	O
we	O
explain	O
below	O
max-product	B
belief	B
propagation	I
the	O
map	O
objective	O
in	O
equation	O
max	O
mg	O
t	O
is	O
almost	O
identical	O
to	O
the	O
inference	B
objective	O
in	O
equation	O
max	O
mg	O
t	O
h	O
apart	O
from	O
the	O
entropy	B
term	O
one	O
heuristic	O
way	O
to	O
proceed	O
would	O
be	O
to	O
consider	O
the	O
zero	B
temperature	B
limit	I
of	O
the	O
probability	O
distribution	O
where	O
the	O
probability	O
distribution	O
has	O
all	O
its	O
mass	O
centered	O
on	O
its	O
mode	B
section	O
in	O
such	O
a	O
setting	O
the	O
entropy	B
term	O
becomes	O
zero	O
we	O
can	O
then	O
modify	O
the	O
message	B
passing	I
methods	O
used	O
to	O
solve	O
the	O
inference	B
problem	O
so	O
that	O
they	O
solve	O
the	O
map	O
estimation	O
problem	O
instead	O
in	O
the	O
zero	B
temperature	B
limit	I
the	O
sum	O
operator	O
becomes	O
the	O
max	O
operator	O
which	O
results	O
in	O
a	O
method	O
called	O
max-product	B
belief	B
propagation	I
in	O
particular	O
in	O
more	O
detail	O
let	O
a	O
max	O
mg	O
t	O
h	O
max	O
mg	O
now	O
consider	O
an	O
inverse	O
temperature	B
going	O
to	O
infinity	O
we	O
have	O
lim	O
a	O
lim	O
h	O
max	O
mg	O
t	O
lim	O
h	O
max	O
mg	O
t	O
it	O
is	O
the	O
concavity	O
of	O
the	O
objective	O
function	O
that	O
allows	O
us	O
to	O
interchange	O
the	O
lim	O
and	O
max	O
operators	O
and	O
jordan	O
for	O
details	O
now	O
consider	O
the	O
bethe	B
approximation	O
which	O
has	O
the	O
form	O
max	O
lg	O
t	O
hbethe	O
we	O
showed	O
that	O
loopy	O
bp	B
finds	O
a	O
local	O
optimum	O
of	O
this	O
objective	O
in	O
the	O
zero	B
temperature	B
limit	I
this	O
objective	O
is	O
equivalent	O
to	O
the	O
lp	O
relaxation	O
of	O
the	O
map	O
problem	O
unfortunately	O
max-product	B
loopy	O
bp	B
does	O
not	O
solve	O
this	O
lp	O
relaxation	O
unless	O
the	O
graph	B
is	O
a	O
tree	B
and	O
jordan	O
the	O
reason	O
is	O
that	O
bethe	B
energy	B
functional	I
is	O
not	O
concave	B
on	O
trees	O
so	O
we	O
are	O
not	O
licensed	O
to	O
swap	O
the	O
limit	O
and	O
max	O
operators	O
in	O
the	O
above	O
zero-temperature	O
derivation	O
however	O
if	O
we	O
use	O
tree-reweighted	O
bp	B
or	O
trbp	B
trw	B
we	O
have	O
a	O
concave	B
objective	O
in	O
this	O
case	O
map	O
state	B
estimation	I
one	O
can	O
show	O
and	O
wainwright	O
that	O
the	O
max-product	B
version	O
of	O
trbp	B
does	O
solve	O
the	O
above	O
lp	O
relaxation	O
a	O
certain	O
scheduling	O
of	O
this	O
algorithm	O
known	O
as	O
sequential	B
trbp	B
trbp-s	B
ortrw-s	O
can	O
be	O
shown	O
to	O
always	O
converge	B
and	O
furthermore	O
it	O
typically	O
does	O
so	O
faster	O
than	O
the	O
standard	O
parallel	O
updates	O
the	O
idea	O
is	O
to	O
pick	O
an	O
arbitrary	O
node	O
ordering	O
xn	O
we	O
then	O
consider	O
a	O
set	O
of	O
trees	O
which	O
is	O
a	O
subsequence	O
of	O
this	O
ordering	O
at	O
each	O
iteration	O
we	O
perform	O
max-product	B
bp	B
from	O
towards	O
xn	O
and	O
back	O
along	O
one	O
of	O
these	O
trees	O
it	O
can	O
be	O
shown	O
that	O
this	O
monotonically	O
minimizes	O
a	O
lower	O
bound	O
on	O
the	O
energy	O
and	O
thus	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
of	O
the	O
lp	O
relaxation	O
graphcuts	B
in	O
this	O
section	O
we	O
show	O
how	O
to	O
find	O
map	O
state	B
estimates	O
or	O
equivalently	O
minimum	O
energy	O
configurations	O
by	O
using	O
the	O
max	O
flowmin	O
cut	O
algorithm	O
for	O
this	O
class	O
of	O
methods	O
is	O
known	O
as	O
graphcuts	B
and	O
is	O
very	O
widely	O
used	O
especially	O
in	O
computer	O
vision	O
applications	O
we	O
will	O
start	O
by	O
considering	O
the	O
case	O
of	O
mrfs	O
with	O
binary	O
nodes	B
and	O
a	O
restricted	O
class	O
of	O
potentials	O
in	O
this	O
case	O
graphcuts	B
will	O
find	O
the	O
exact	O
global	O
optimum	O
we	O
then	O
consider	O
the	O
case	O
of	O
multiple	O
states	O
per	O
node	O
which	O
are	O
assumed	O
to	O
have	O
some	O
underlying	O
ordering	O
we	O
can	O
approximately	O
solve	O
this	O
case	O
by	O
solving	O
a	O
series	O
of	O
binary	O
subproblems	O
as	O
we	O
will	O
see	O
graphcuts	B
for	O
the	O
generalized	O
ising	B
model	I
st	O
if	O
xu	O
xv	O
if	O
xu	O
xv	O
let	O
us	O
start	O
by	O
considering	O
a	O
binary	O
mrf	B
where	O
the	O
edge	O
energies	O
have	O
the	O
following	O
form	O
euvxu	O
xv	O
where	O
st	O
is	O
the	O
edge	O
cost	O
this	O
encourages	O
neighboring	O
nodes	B
to	O
have	O
the	O
same	O
value	O
we	O
are	O
trying	O
to	O
minimize	O
energy	O
since	O
we	O
are	O
free	O
to	O
add	O
any	O
constant	O
we	O
like	O
to	O
the	O
overall	O
energy	O
without	O
affecting	O
the	O
map	O
state	B
estimate	O
let	O
us	O
rescale	O
the	O
local	O
energy	O
terms	O
such	O
that	O
either	O
or	O
now	O
let	O
us	O
construct	O
a	O
graph	B
which	O
has	O
the	O
same	O
set	O
of	O
nodes	B
as	O
the	O
mrf	B
plus	O
two	O
distinguished	O
nodes	B
the	O
source	O
s	O
and	O
the	O
sinkt	O
if	O
we	O
add	O
the	O
edge	O
xu	O
t	O
with	O
cost	O
ensures	O
that	O
if	O
u	O
is	O
not	O
in	O
partition	O
xt	O
meaning	O
u	O
is	O
assigned	O
to	O
state	B
we	O
will	O
pay	O
a	O
cost	O
of	O
in	O
the	O
cut	O
similarly	O
if	O
we	O
add	O
the	O
edge	O
xu	O
s	O
with	O
cost	O
finally	O
for	O
every	O
pair	O
of	O
variables	O
that	O
are	O
connected	O
in	O
the	O
mrf	B
we	O
add	O
edges	B
xu	O
xv	O
and	O
xv	O
xu	O
both	O
with	O
cost	O
uv	O
figure	O
illustrates	O
this	O
construction	O
for	O
an	O
mrf	B
with	O
nodes	B
and	O
with	O
the	O
following	O
non-zero	O
energy	O
values	O
having	O
constructed	O
the	O
graph	B
we	O
compute	O
a	O
minimal	B
s	O
t	O
cut	O
this	O
is	O
a	O
partition	O
of	O
the	O
nodes	B
into	O
two	O
sets	O
xs	O
which	O
are	O
nodes	B
connected	O
to	O
s	O
and	O
xt	O
which	O
are	O
nodes	B
connected	O
to	O
t	O
we	O
there	O
are	O
a	O
variety	O
of	O
ways	O
to	O
implement	O
this	O
algorithm	O
see	O
e	O
g	O
oev	O
log	O
v	O
or	O
ov	O
time	O
where	O
e	O
is	O
the	O
number	O
of	O
edges	B
and	O
v	O
is	O
the	O
number	O
of	O
nodes	B
and	O
wayne	O
the	O
best	O
take	O
chapter	O
more	O
variational	B
inference	B
t	O
s	O
figure	O
illustration	O
of	O
graphcuts	B
applied	O
to	O
an	O
mrf	B
with	O
nodes	B
dashed	O
lines	O
are	O
ones	O
which	O
contribute	O
to	O
the	O
cost	O
of	O
the	O
cut	O
bidirected	O
edges	B
we	O
only	O
count	O
one	O
of	O
the	O
costs	O
here	O
the	O
min	O
cut	O
has	O
cost	O
source	O
figure	O
from	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
pick	O
the	O
partition	O
which	O
minimizes	O
the	O
sum	O
of	O
the	O
cost	O
of	O
the	O
edges	B
between	O
nodes	B
on	O
different	O
sides	O
of	O
the	O
partition	O
costxsxt	O
xu	O
xsxv	O
xt	O
costxu	O
sv	O
in	O
figure	O
we	O
see	O
that	O
the	O
min-cut	O
has	O
cost	O
minimizing	O
the	O
cost	O
in	O
this	O
graph	B
is	O
equivalent	O
to	O
minimizing	O
the	O
energy	O
in	O
the	O
mrf	B
hence	O
nodes	B
that	O
are	O
assigned	O
to	O
s	O
have	O
an	O
optimal	O
state	B
of	O
and	O
the	O
nodes	B
that	O
are	O
assigned	O
to	O
t	O
have	O
an	O
optimal	O
state	B
of	O
in	O
figure	O
we	O
see	O
that	O
the	O
optimal	O
map	B
estimate	I
is	O
graphcuts	B
for	O
binary	O
mrfs	O
with	O
submodular	B
potentials	O
we	O
now	O
discuss	O
how	O
to	O
extend	O
the	O
graphcuts	B
construction	O
to	O
binary	O
mrfs	O
with	O
more	O
general	O
kinds	O
of	O
potential	O
functions	O
in	O
particular	O
suppose	O
each	O
pairwise	O
energy	O
satisfies	O
the	O
following	O
condition	O
in	O
other	O
words	O
the	O
sum	O
of	O
the	O
diagonal	B
energies	O
is	O
less	O
than	O
the	O
sum	O
of	O
the	O
off-diagonal	O
energies	O
in	O
this	O
case	O
we	O
say	O
the	O
energies	O
are	O
submodular	B
and	O
zabin	O
an	O
example	O
of	O
a	O
submodular	B
energy	O
is	O
an	O
ising	B
model	I
where	O
uv	O
this	O
is	O
also	O
known	O
as	O
an	O
attractive	B
mrf	B
or	O
associative	B
mrf	B
since	O
the	O
model	O
wants	O
neighboring	O
states	O
to	O
be	O
the	O
same	O
submodularity	O
is	O
the	O
discrete	B
analog	O
of	O
convexity	O
intuitively	O
it	O
corresponds	O
to	O
the	O
law	O
of	O
diminishing	O
returns	O
that	O
is	O
the	O
extra	O
value	O
of	O
adding	O
one	O
more	O
element	O
to	O
a	O
set	O
is	O
reduced	O
if	O
the	O
set	O
is	O
already	O
large	O
more	O
formally	O
we	O
say	O
that	O
f	O
r	O
is	O
submodular	B
if	O
for	O
any	O
a	O
b	O
s	O
and	O
x	O
s	O
we	O
havef	O
f	O
f	O
f	O
if	O
f	O
is	O
submodular	B
then	O
f	O
is	O
supermodular	B
map	O
state	B
estimation	I
to	O
apply	O
graphcuts	B
to	O
a	O
binary	O
mrf	B
with	O
submodular	B
potentials	O
we	O
construct	O
the	O
pairwise	O
edge	O
weights	O
as	O
follows	O
this	O
is	O
guaranteed	O
to	O
be	O
non-negative	O
by	O
virtue	O
of	O
the	O
submodularity	O
assumption	O
in	O
addition	O
we	O
construct	O
new	O
local	O
edge	O
weights	O
as	O
follows	O
first	O
we	O
initialize	O
eu	O
and	O
then	O
for	O
each	O
edge	O
pair	O
v	O
we	O
update	O
these	O
values	O
as	O
follows	O
we	O
now	O
construct	O
a	O
graph	B
in	O
a	O
similar	B
way	O
to	O
before	O
specifically	O
if	O
add	O
the	O
edge	O
u	O
s	O
with	O
cost	O
xu	O
xv	O
with	O
cost	O
finally	O
for	O
every	O
mrf	B
edge	O
for	O
which	O
don	O
t	O
need	O
to	O
add	O
the	O
edge	O
in	O
both	O
directions	O
we	O
otherwise	O
we	O
add	O
the	O
edge	O
u	O
t	O
with	O
cost	O
we	O
add	O
a	O
graphcuts	B
edge	O
one	O
can	O
show	O
that	O
the	O
min	O
cut	O
in	O
this	O
graph	B
is	O
the	O
same	O
as	O
the	O
minimum	O
energy	O
configuration	O
thus	O
we	O
can	O
use	O
max	O
flowmin	O
cut	O
to	O
find	O
the	O
globally	O
optimal	O
map	B
estimate	I
et	O
al	O
graphcuts	B
for	O
nonbinary	O
metric	B
mrfs	O
we	O
now	O
discuss	O
how	O
to	O
use	O
graphcuts	B
for	O
approximate	O
map	O
estimation	O
in	O
mrfs	O
where	O
each	O
node	O
can	O
have	O
multiple	O
states	O
et	O
al	O
however	O
we	O
require	O
that	O
the	O
pairwise	O
energies	O
form	O
a	O
metric	B
we	O
call	O
such	O
a	O
model	O
a	O
metric	B
mrf	B
for	O
example	O
suppose	O
the	O
states	O
have	O
a	O
natural	O
ordering	O
as	O
commonly	O
arises	O
if	O
they	O
are	O
a	O
discretization	O
of	O
an	O
underlying	O
continuous	O
in	O
this	O
case	O
we	O
can	O
define	O
a	O
metric	B
of	O
the	O
form	O
exs	O
xt	O
min	O
xt	O
or	O
a	O
space	O
semi-metric	B
of	O
the	O
form	O
exs	O
xt	O
min	O
for	O
some	O
constant	O
this	O
energy	O
encourages	O
neighbors	B
to	O
have	O
similar	B
labels	O
but	O
never	O
punishes	O
them	O
by	O
more	O
than	O
this	O
term	O
prevents	O
over-smoothing	O
which	O
we	O
illustrate	O
in	O
figure	O
one	O
version	O
of	O
graphcuts	B
is	O
the	O
alpha	B
expansion	I
at	O
each	O
step	O
it	O
picks	O
one	O
of	O
the	O
available	O
labels	O
or	O
states	O
and	O
calls	O
it	O
then	O
it	O
solves	O
a	O
binary	O
subproblem	O
where	O
each	O
variable	O
can	O
choose	O
to	O
remain	O
in	O
its	O
current	O
state	B
or	O
to	O
become	O
state	B
figure	O
for	O
an	O
illustration	O
more	O
precisely	O
we	O
define	O
a	O
new	O
mrf	B
on	O
binary	O
nodes	B
and	O
we	O
define	O
the	O
energies	O
of	O
this	O
new	O
model	O
relative	O
to	O
the	O
current	O
assignment	O
x	O
as	O
follows	O
euvxu	O
euxu	O
eu	O
euv	O
xv	O
euvxu	O
xv	O
euv	O
to	O
optimize	O
using	O
graph	B
cuts	I
thus	O
figure	O
out	O
the	O
optimal	O
alpha	B
expansion	I
move	O
we	O
require	O
that	O
the	O
energies	O
be	O
submodular	B
plugging	O
in	O
the	O
definition	O
we	O
get	O
the	O
following	O
constraint	O
euvxu	O
xv	O
uv	O
euvxu	O
uv	O
xv	O
for	O
any	O
distance	O
function	O
euv	O
and	O
the	O
remaining	O
inequality	O
follows	O
from	O
the	O
triangle	B
inequality	I
thus	O
we	O
can	O
apply	O
the	O
alpha	B
expansion	I
move	O
to	O
any	O
metric	B
mrf	B
chapter	O
more	O
variational	B
inference	B
initial	O
labeling	O
standard	O
move	O
an	O
image	O
with	O
labels	O
figure	O
just	O
flips	O
the	O
label	B
of	O
one	O
pixel	O
be	O
relabeled	O
as	O
if	O
this	O
decreases	O
the	O
energy	O
labeled	O
as	O
to	O
be	O
relabeled	O
as	O
if	O
this	O
decreases	O
the	O
energy	O
used	O
with	O
kind	O
permission	O
of	O
ramin	O
zabih	O
a	O
standard	O
local	O
move	O
by	O
iterative	B
conditional	I
modes	I
an	O
swap	O
allows	O
all	O
nodes	B
that	O
are	O
currently	O
labeled	O
as	O
to	O
an	O
expansion	O
allows	O
all	O
nodes	B
that	O
are	O
not	O
currently	O
source	O
figure	O
of	O
et	O
al	O
at	O
each	O
step	O
of	O
alpha	B
expansion	I
we	O
find	O
the	O
optimal	O
move	O
from	O
amongst	O
an	O
exponentially	O
large	O
set	O
thus	O
we	O
reach	O
a	O
strong	B
local	I
optimum	I
of	O
much	O
lower	O
energy	O
than	O
the	O
local	O
optima	O
found	O
by	O
standard	O
greedy	O
label	B
flipping	O
methods	O
such	O
as	O
iterative	B
conditional	I
modes	I
in	O
fact	O
one	O
can	O
show	O
that	O
once	O
the	O
algorithm	O
has	O
converged	O
the	O
energy	O
of	O
the	O
resulting	O
solution	O
is	O
at	O
most	O
times	O
the	O
optimal	O
energy	O
where	O
c	O
max	O
e	O
max	O
euv	O
min	O
euv	O
see	O
exercise	O
for	O
the	O
proof	O
approximation	O
in	O
the	O
case	O
of	O
the	O
potts	B
model	I
c	O
so	O
we	O
have	O
a	O
another	O
version	O
of	O
graphcuts	B
is	O
the	O
alpha-beta	B
swap	I
at	O
each	O
step	O
two	O
labels	O
are	O
chosen	O
call	O
them	O
and	O
all	O
the	O
nodes	B
currently	O
labeled	O
can	O
change	O
to	O
vice	O
versa	O
if	O
this	O
reduces	O
the	O
energy	O
figure	O
for	O
an	O
illustration	O
the	O
resulting	O
binary	O
subproblem	O
can	O
be	O
solved	O
exactly	O
even	O
if	O
the	O
energies	O
are	O
only	O
semi-metric	B
is	O
the	O
triangle	B
inequality	I
need	O
not	O
hold	O
see	O
exercise	O
although	O
the	O
swap	O
version	O
can	O
be	O
applied	O
to	O
a	O
broader	O
class	O
of	O
models	O
than	O
the	O
version	O
it	O
is	O
theoretically	O
not	O
as	O
powerful	O
indeed	O
in	O
various	O
low-level	B
vision	I
problems	O
et	O
al	O
show	O
empirically	O
that	O
the	O
expansion	O
version	O
is	O
usually	O
better	O
than	O
the	O
swap	O
version	O
section	O
experimental	O
comparison	O
of	O
graphcuts	B
and	O
bp	B
in	O
section	O
we	O
described	O
lattice-structured	O
crfs	O
for	O
various	O
low-level	B
vision	I
problems	O
et	O
al	O
performed	O
an	O
extensive	O
comparison	O
of	O
different	O
approximate	O
optimization	B
techniques	O
for	O
this	O
class	O
of	O
problems	O
some	O
of	O
the	O
results	O
for	O
the	O
problem	O
of	O
stereo	O
depth	O
estimation	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
graphcut	O
and	O
tree-reweighted	O
maxproduct	O
bp	B
give	O
the	O
best	O
results	O
with	O
regular	B
max-product	B
bp	B
being	O
much	O
worse	O
in	O
terms	O
of	O
speed	O
graphcuts	B
is	O
the	O
fastest	O
with	O
trw	B
a	O
close	O
second	O
other	O
algorithms	O
such	O
as	O
icm	O
simulated	B
annealing	B
or	O
a	O
standard	O
domain-specific	O
heuristic	O
known	O
as	O
normalize	O
correlation	O
are	O
map	O
state	B
estimation	I
y	O
g	O
r	O
e	O
n	O
e	O
max-product	B
bp	B
a-expansion	O
a-b	O
swap	O
trw	B
y	O
g	O
r	O
e	O
n	O
e	O
max-product	B
bp	B
a-expansion	O
a-b	O
swap	O
trw	B
running	O
time	O
running	O
time	O
figure	O
energy	O
minimization	O
on	O
a	O
crf	B
for	O
stereo	O
depth	O
estimation	O
top	O
row	O
two	O
input	O
images	O
along	O
with	O
the	O
ground	O
truth	O
depth	O
values	O
bottom	O
row	O
energy	O
vs	O
time	O
for	O
different	O
optimization	B
algorithms	O
bottom	O
left	O
results	O
are	O
for	O
the	O
teddy	O
image	O
in	O
top	O
row	O
bottom	O
right	O
results	O
are	O
for	O
the	O
tsukuba	O
image	O
in	O
figure	O
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
even	O
worse	O
as	O
shown	O
qualitatively	O
in	O
figure	O
since	O
trw	B
is	O
optimizing	O
the	O
dual	O
of	O
the	O
relaxed	O
lp	O
problem	O
we	O
can	O
use	O
its	O
value	O
at	O
convergence	O
to	O
evaluate	O
the	O
optimal	O
energy	O
it	O
turns	O
out	O
that	O
for	O
many	O
of	O
the	O
images	O
in	O
the	O
stereo	O
benchmark	O
dataset	O
the	O
ground	O
truth	O
has	O
higher	O
energy	O
probability	O
than	O
the	O
globally	O
optimal	O
estimate	O
et	O
al	O
this	O
indicates	O
that	O
we	O
are	O
optimizing	O
the	O
wrong	O
model	O
this	O
is	O
not	O
surprising	O
since	O
the	O
pairwise	O
crf	B
ignores	O
known	O
long-range	O
constraints	O
unfortunately	O
if	O
we	O
add	O
these	O
constraints	O
to	O
the	O
model	O
the	O
graph	B
either	O
becomes	O
too	O
dense	O
bp	B
slow	O
andor	O
the	O
potentials	O
become	O
non-submodular	O
graphcuts	B
inapplicable	O
one	O
way	O
around	O
this	O
is	O
to	O
generate	O
a	O
diverse	O
set	O
of	O
local	O
modes	O
using	O
repeated	O
applications	O
of	O
graph	B
cuts	I
as	O
described	O
in	O
et	O
al	O
we	O
can	O
then	O
apply	O
a	O
more	O
sophisticated	O
model	O
which	O
uses	O
global	O
features	B
to	O
rerank	B
the	O
solutions	O
chapter	O
more	O
variational	B
inference	B
left	O
image	O
labels	O
ground	O
truth	O
swap	O
algorithm	O
expansion	O
algorithm	O
normalized	O
correlation	O
simulated	B
annealing	B
figure	O
an	O
example	O
of	O
stereo	O
depth	O
estimation	O
using	O
an	O
mrf	B
left	O
image	O
of	O
size	O
pixels	O
from	O
the	O
university	O
of	O
tsukuba	O
map	O
estimates	O
using	O
different	O
methods	O
ground	O
truth	O
depth	O
map	O
quantized	O
to	O
levels	O
swap	O
expansion	O
normalized	O
cross	O
correlation	O
simulated	B
annealing	B
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
ramin	O
zabih	O
corresponding	O
right	O
image	O
is	O
similar	B
but	O
not	O
shown	O
dual	B
decomposition	I
we	O
are	O
interested	O
in	O
computing	O
f	O
f	O
p	O
max	O
x	O
x	O
m	O
i	O
v	O
ixi	O
f	O
where	O
f	O
represents	O
a	O
set	O
of	O
factors	B
we	O
will	O
assume	O
that	O
we	O
can	O
tractably	O
optimize	O
each	O
local	O
factor	B
but	O
the	O
combination	O
of	O
all	O
of	O
these	O
factors	B
makes	O
the	O
problem	O
intractable	O
one	O
way	O
to	O
proceed	O
is	O
to	O
optimize	O
each	O
term	O
independently	O
but	O
then	O
to	O
introduce	O
constraints	O
that	O
force	O
all	O
the	O
local	O
estimates	O
of	O
the	O
variables	O
values	O
to	O
agree	O
with	O
each	O
other	O
we	O
explain	O
this	O
in	O
more	O
detail	O
below	O
following	O
the	O
presentation	O
of	O
et	O
al	O
map	O
state	B
estimation	I
fxf	O
xf	O
xf	O
xg	O
xf	O
xh	O
hxh	O
xh	O
gxg	O
xg	O
xg	O
xh	O
xk	O
xk	O
xk	O
kxk	O
figure	O
a	O
pairwise	B
mrf	B
with	O
different	O
edge	O
factors	B
we	O
have	O
separate	O
variables	O
plus	O
a	O
copy	O
of	O
each	O
variable	O
for	O
each	O
factor	B
it	O
participates	O
in	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
david	O
sontag	O
basic	O
idea	O
let	O
us	O
duplicate	O
the	O
variables	O
xi	O
once	O
for	O
each	O
factor	B
and	O
then	O
force	O
them	O
to	O
be	O
equal	O
i	O
f	O
be	O
the	O
set	O
of	O
variables	O
used	O
by	O
factor	B
f	O
this	O
construction	O
is	O
specifically	O
let	O
xf	O
illustrated	O
in	O
figure	O
we	O
can	O
reformulate	O
the	O
objective	O
as	O
follows	O
i	O
xi	O
f	O
i	O
f	O
f	O
s	O
t	O
xf	O
ixi	O
f	O
f	O
p	O
max	O
xxf	O
i	O
v	O
f	O
f	O
let	O
us	O
now	O
introduce	O
lagrange	B
multipliers	I
or	O
dual	B
variables	I
f	O
ik	O
to	O
enforce	O
these	O
constraints	O
the	O
lagrangian	B
becomes	O
i	O
v	O
ixi	O
f	O
f	O
f	O
f	O
i	O
f	O
xi	O
l	O
x	O
xf	O
f	O
f	O
ixi	O
xi	O
ixf	O
f	O
i	O
xi	O
i	O
xi	O
this	O
is	O
equivalent	O
to	O
our	O
original	O
problem	O
in	O
the	O
following	O
sense	O
for	O
any	O
value	O
of	O
we	O
have	O
p	O
max	O
xxf	O
l	O
x	O
xf	O
s	O
t	O
xf	O
i	O
xi	O
f	O
i	O
f	O
since	O
if	O
the	O
constraints	O
hold	O
the	O
last	O
term	O
is	O
zero	O
we	O
can	O
get	O
an	O
upper	O
bound	O
by	O
dropping	O
the	O
consistency	O
constraints	O
and	O
just	O
optimizing	O
the	O
following	O
upper	O
bound	O
l	O
max	O
xxf	O
max	O
xi	O
i	O
l	O
x	O
xf	O
ixi	O
f	O
f	O
f	O
ixi	O
f	O
max	O
xf	O
f	O
i	O
f	O
f	O
ixi	O
see	O
figure	O
for	O
an	O
illustration	O
chapter	O
more	O
variational	B
inference	B
f	O
f	O
f	O
f	O
figure	O
illustration	O
of	O
dual	B
decomposition	I
kind	O
permission	O
of	O
david	O
sontag	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
this	O
objective	O
is	O
tractable	O
to	O
optimize	O
since	O
each	O
xf	O
term	O
is	O
decoupled	O
furthermore	O
we	O
see	O
since	O
by	O
relaxing	O
the	O
consistency	O
constraints	O
we	O
are	O
optimizing	O
over	O
a	O
larger	O
that	O
l	O
p	O
space	O
furthermore	O
we	O
have	O
the	O
property	O
that	O
l	O
p	O
min	O
so	O
the	O
upper	O
bound	O
is	O
tight	O
at	O
the	O
optimal	O
value	O
of	O
which	O
enforces	O
the	O
original	O
constraints	O
minimizing	O
this	O
upper	O
bound	O
is	O
known	O
as	O
dual	B
decomposition	I
or	O
lagrangian	B
relaxation	I
et	O
al	O
sontag	O
et	O
al	O
rush	O
and	O
collins	O
furthemore	O
it	O
can	O
be	O
shown	O
that	O
l	O
is	O
the	O
dual	O
to	O
the	O
same	O
lp	O
relaxation	O
we	O
saw	O
before	O
we	O
will	O
discuss	O
several	O
possible	O
optimization	B
algorithms	O
below	O
the	O
main	O
advantage	O
of	O
dual	B
decomposition	I
from	O
a	O
practical	O
point	O
of	O
view	O
is	O
that	O
it	O
allows	O
one	O
to	O
mix	O
and	O
match	O
different	O
kinds	O
of	O
optimization	B
algorithms	O
in	O
a	O
convenient	O
way	O
for	O
example	O
we	O
can	O
combine	O
a	O
grid	O
structured	O
graph	B
with	O
local	O
submodular	B
factors	B
to	O
perform	O
image	B
segmentation	I
together	O
with	O
a	O
tree	B
structured	O
model	O
to	O
perform	O
pose	O
estimation	O
exercise	O
analogous	O
methods	O
can	O
be	O
used	O
in	O
natural	O
language	O
processing	O
where	O
we	O
often	O
have	O
a	O
mix	O
of	O
local	O
and	O
global	O
constraints	O
e	O
g	O
et	O
al	O
rush	O
and	O
collins	O
theoretical	O
guarantees	O
what	O
can	O
we	O
say	O
about	O
the	O
quality	O
of	O
the	O
solutions	O
obtained	O
in	O
this	O
way	O
to	O
understand	O
this	O
let	O
us	O
first	O
introduce	O
some	O
more	O
notation	O
f	O
ixi	O
f	O
ixi	O
f	O
f	O
i	O
ixi	O
f	O
f	O
i	O
f	O
this	O
represents	O
a	O
reparameterization	O
of	O
the	O
original	O
problem	O
in	O
the	O
sense	O
that	O
map	O
state	B
estimation	I
ixi	O
i	O
and	O
hence	O
l	O
f	O
max	O
xi	O
i	O
f	O
i	O
f	O
f	O
max	O
xf	O
f	O
i	O
f	O
i	O
and	O
an	O
assignment	O
x	O
f	O
in	O
this	O
case	O
we	O
have	O
now	O
suppose	O
there	O
is	O
a	O
set	O
of	O
dual	B
variables	I
f	O
argmaxxf	O
ix	O
such	O
that	O
the	O
maximizing	O
assignments	O
to	O
the	O
singleton	O
terms	O
agrees	O
with	O
the	O
assignments	O
to	O
the	O
factor	B
terms	O
i	O
e	O
so	O
that	O
x	O
l	O
now	O
ix	O
i	O
argmaxxi	O
i	O
i	O
and	O
x	O
f	O
f	O
p	O
l	O
f	O
f	O
f	O
f	O
i	O
i	O
i	O
i	O
f	O
i	O
f	O
i	O
f	O
we	O
conclude	O
that	O
l	O
p	O
sox	O
is	O
the	O
map	O
assignment	O
so	O
if	O
we	O
can	O
find	O
a	O
solution	O
where	O
all	O
the	O
subproblems	O
agree	O
we	O
can	O
be	O
assured	O
that	O
it	O
is	O
the	O
global	O
optimum	O
this	O
happens	O
surprisingly	O
often	O
in	O
practical	O
problems	O
subgradient	B
descent	O
l	O
is	O
a	O
convex	B
and	O
continuous	O
objective	O
but	O
it	O
is	O
non-differentiable	O
at	O
points	O
where	O
or	O
the	O
elements	O
of	O
at	O
the	O
same	O
time	O
as	O
follows	O
i	O
f	O
have	O
multiple	O
optima	O
one	O
approach	O
is	O
to	O
use	O
subgradient	B
descent	O
this	O
updates	O
all	O
f	O
i	O
t	O
f	O
ixi	O
tgt	O
f	O
ixi	O
one	O
can	O
show	O
that	O
the	O
gradient	O
is	O
given	O
by	O
the	O
following	O
sparse	B
vector	O
first	O
let	O
xs	O
where	O
gt	O
the	O
subgradient	B
of	O
l	O
at	O
t	O
tion	O
this	O
method	O
is	O
guaranteed	O
to	O
converge	B
to	O
a	O
global	O
optimum	O
of	O
the	O
dual	O
et	O
al	O
for	O
details	O
if	O
the	O
step	O
sizes	O
t	O
are	O
set	O
appropriately	O
secsee	O
i	O
t	O
f	O
next	O
let	O
gf	O
ixi	O
for	O
all	O
elements	O
finally	O
argmaxxi	O
i	O
xs	O
if	O
xf	O
i	O
and	O
gf	O
ixf	O
i	O
bringing	O
them	O
closer	O
to	O
agreement	O
similarly	O
the	O
subgradient	B
update	O
will	O
decrease	O
the	O
value	O
of	O
t	O
f	O
to	O
compute	O
the	O
gradient	O
we	O
need	O
to	O
be	O
able	O
to	O
solve	O
subproblems	O
of	O
the	O
following	O
form	O
t	O
i	O
and	O
xf	O
i	O
factor	B
f	O
disagrees	O
with	O
the	O
local	O
term	O
on	O
how	O
to	O
set	O
variable	O
i	O
we	O
set	O
gf	O
ixs	O
i	O
and	O
increasing	O
t	O
i	O
this	O
has	O
the	O
effect	O
of	O
decreasing	O
t	O
f	O
argmaxxf	O
i	O
xfi	O
i	O
f	O
i	O
i	O
xfi	O
and	O
increasing	O
the	O
value	O
of	O
t	O
f	O
t	O
f	O
argmax	O
xf	O
argmax	O
xf	O
t	O
f	O
ixi	O
i	O
f	O
chapter	O
more	O
variational	B
inference	B
et	O
al	O
these	O
subproblems	O
are	O
called	O
slaves	B
whereas	O
l	O
is	O
called	O
the	O
master	B
obviously	O
if	O
the	O
scope	B
of	O
factor	B
f	O
is	O
small	O
this	O
is	O
simple	O
for	O
example	O
if	O
each	O
factor	B
is	O
pairwise	O
and	O
each	O
variable	O
has	O
k	O
states	O
the	O
cost	O
is	O
just	O
k	O
however	O
there	O
are	O
some	O
kinds	O
of	O
global	O
factors	B
that	O
also	O
support	B
exact	O
and	O
efficient	O
maximization	O
including	O
the	O
following	O
graphical	B
models	I
with	O
low	O
tree	B
width	O
factors	B
that	O
correspond	O
to	O
bipartite	B
graph	B
matchings	O
e	O
g	O
et	O
al	O
this	O
is	O
useful	O
for	O
data	B
association	I
problems	O
where	O
we	O
must	O
match	O
up	O
a	O
sensor	O
reading	O
with	O
an	O
unknown	B
source	O
we	O
can	O
find	O
the	O
maximal	O
matching	O
using	O
the	O
so-called	O
hungarian	B
algorithm	I
in	O
time	O
e	O
g	O
and	O
steiglitz	O
supermodular	B
functions	O
we	O
discuss	O
this	O
case	O
in	O
more	O
detail	O
in	O
section	O
cardinality	B
constraints	I
for	O
example	O
we	O
might	O
have	O
a	O
factor	B
over	O
a	O
large	O
set	O
of	O
binary	O
variables	O
that	O
enforces	O
that	O
a	O
certain	O
number	O
of	O
bits	B
are	O
turned	O
on	O
this	O
can	O
be	O
useful	O
in	O
problems	O
such	O
as	O
image	B
segmentation	I
in	O
particular	O
suppose	O
f	O
if	O
i	O
f	O
xi	O
l	O
and	O
f	O
otherwise	O
we	O
can	O
find	O
the	O
maximizing	O
assignment	O
in	O
of	O
log	O
time	O
as	O
follows	O
first	O
define	O
ei	O
f	O
f	O
now	O
sort	O
the	O
ei	O
finally	O
set	O
xi	O
for	O
the	O
first	O
l	O
values	O
and	O
xi	O
for	O
the	O
rest	O
et	O
al	O
factors	B
which	O
are	O
constant	O
for	O
all	O
but	O
a	O
small	O
set	O
s	O
of	O
distinguished	O
values	O
of	O
xf	O
then	O
we	O
can	O
optimize	O
over	O
the	O
factor	B
in	O
os	O
time	O
et	O
al	O
coordinate	O
descent	O
an	O
alternative	O
to	O
updating	O
the	O
entire	O
vector	O
at	O
once	O
sparsely	O
is	O
to	O
update	O
it	O
using	O
block	O
coordinate	O
descent	O
by	O
choosing	O
the	O
size	O
of	O
the	O
blocks	O
we	O
can	O
trade	O
off	O
convergence	O
speed	O
with	O
ease	O
of	O
the	O
local	O
optimization	B
problem	O
one	O
approach	O
which	O
optimizes	O
f	O
ixi	O
for	O
all	O
i	O
f	O
and	O
all	O
xi	O
at	O
the	O
same	O
time	O
a	O
fixed	O
factor	B
f	O
is	O
known	O
as	O
max	B
product	I
linear	I
programming	I
and	O
jaakkola	O
algorithmically	O
this	O
is	O
similar	B
to	O
belief	B
propagation	I
on	O
a	O
factor	B
graph	B
in	O
particular	O
we	O
define	O
f	O
i	O
as	O
messages	O
sent	O
from	O
factor	B
f	O
to	O
variable	O
i	O
and	O
we	O
define	O
i	O
f	O
as	O
messages	O
sent	O
from	O
variable	O
i	O
to	O
factor	B
f	O
these	O
messages	O
can	O
be	O
computed	O
as	O
follows	O
and	O
jaakkola	O
for	O
the	O
i	O
f	O
ixi	O
g	O
ixi	O
f	O
ixi	O
i	O
f	O
max	O
xfi	O
f	O
j	O
f	O
j	O
f	O
we	O
then	O
set	O
the	O
dual	B
variables	I
f	O
ixi	O
to	O
be	O
the	O
messages	O
f	O
ixi	O
for	O
example	O
consider	O
a	O
grid	O
mrf	B
with	O
the	O
following	O
pairwise	O
factors	B
f	O
and	O
the	O
outgoing	O
message	O
from	O
factor	B
f	O
to	O
variable	O
is	O
a	O
note	O
that	O
we	O
denote	O
their	O
f	O
by	O
i	O
f	O
i	O
map	O
state	B
estimation	I
function	O
of	O
all	O
messages	O
coming	O
into	O
f	O
and	O
f	O
s	O
local	O
factor	B
f	O
f	O
max	O
f	O
f	O
f	O
similarly	O
the	O
outgoing	O
message	O
from	O
variable	O
to	O
factor	B
f	O
is	O
a	O
function	O
of	O
all	O
the	O
messages	O
sent	O
into	O
variable	O
from	O
other	O
connected	O
factors	B
this	O
example	O
just	O
factor	B
h	O
and	O
the	O
local	O
potential	O
f	O
the	O
key	O
computational	O
bottleneck	B
is	O
computing	O
the	O
max	O
marginals	O
of	O
each	O
factor	B
where	O
we	O
max	O
out	O
all	O
the	O
variables	O
from	O
xf	O
except	O
for	O
xi	O
i	O
e	O
we	O
need	O
to	O
be	O
able	O
to	O
compute	O
the	O
following	O
max	O
marginals	O
efficiently	O
j	O
f	O
hxfi	O
xi	O
hxfi	O
xi	O
f	O
max	O
xfi	O
jf	O
the	O
difference	O
from	O
equation	O
is	O
that	O
we	O
are	O
maxing	O
over	O
all	O
but	O
one	O
of	O
the	O
variables	O
we	O
can	O
solve	O
this	O
efficiently	O
for	O
low	O
treewidth	B
graphical	B
models	I
using	O
message	B
passing	I
we	O
can	O
also	O
solve	O
this	O
efficiently	O
for	O
factors	B
corresponding	O
to	O
bipartite	O
matchings	O
et	O
al	O
or	O
to	O
cardinality	B
constraints	I
et	O
al	O
however	O
there	O
are	O
cases	O
where	O
maximizing	O
over	O
all	O
the	O
variables	O
in	O
a	O
factor	B
s	O
scope	B
is	O
computationally	O
easier	O
than	O
maximizing	O
over	O
all-but-one	O
et	O
al	O
sec	O
for	O
an	O
example	O
in	O
such	O
cases	O
we	O
may	O
prefer	O
to	O
use	O
a	O
subgradient	B
method	O
coordinate	O
descent	O
is	O
a	O
simple	O
algorithm	O
that	O
is	O
often	O
much	O
faster	O
at	O
minimizing	O
the	O
dual	O
than	O
gradient	B
descent	I
especially	O
in	O
the	O
early	O
iterations	O
it	O
also	O
reduces	O
the	O
objective	O
monotonically	O
and	O
does	O
not	O
need	O
any	O
step	B
size	I
parameters	O
unfortunately	O
it	O
is	O
not	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
since	O
l	O
is	O
convex	B
but	O
not	O
strictly	B
convex	B
implies	O
there	O
may	O
be	O
more	O
than	O
one	O
globally	O
optimizing	O
value	O
one	O
way	O
to	O
ensure	O
convergence	O
is	O
to	O
replace	O
the	O
max	O
function	O
in	O
the	O
definition	O
of	O
l	O
with	O
the	O
soft-max	O
function	O
which	O
makes	O
the	O
objective	O
strictly	B
convex	B
e	O
g	O
and	O
shashua	O
for	O
details	O
in	O
general	O
computing	O
x	O
from	O
recovering	O
the	O
map	O
assignment	O
so	O
far	O
we	O
have	O
been	O
focussing	O
on	O
finding	O
the	O
optimal	O
value	O
of	O
but	O
what	O
we	O
really	O
want	O
is	O
the	O
optimal	O
value	O
of	O
x	O
is	O
np-hard	B
even	O
if	O
the	O
lp	O
relaxation	O
is	O
tight	O
and	O
the	O
map	O
assignment	O
is	O
unique	O
et	O
al	O
theorem	O
troublesome	O
cases	O
arise	O
when	O
there	O
are	O
fractional	O
assignments	O
with	O
the	O
same	O
optimal	O
value	O
as	O
the	O
map	B
estimate	I
however	O
suppose	O
that	O
each	O
is	O
locally	B
decodable	I
to	O
x	O
one	O
can	O
show	O
than	O
in	O
this	O
case	O
the	O
lp	O
relaxation	O
is	O
unique	O
and	O
its	O
solution	O
is	O
indeed	O
x	O
if	O
many	O
but	O
not	O
all	O
of	O
the	O
nodes	B
are	O
uniquely	O
decodable	O
we	O
can	O
clamp	O
the	O
uniquely	O
decodable	O
ones	O
to	O
their	O
map	O
value	O
and	O
then	O
use	O
exact	O
inference	B
algorithms	O
to	O
figure	O
out	O
the	O
optimal	O
assignment	O
to	O
the	O
remaining	O
variables	O
using	O
this	O
method	O
et	O
al	O
was	O
able	O
to	O
optimally	O
solve	O
various	O
stereo	O
vision	O
crf	B
estimation	O
problems	O
and	O
et	O
al	O
was	O
able	O
to	O
optimally	O
solve	O
various	O
protein	O
side-chain	O
structure	O
predicition	O
problems	O
another	O
approach	O
is	O
to	O
use	O
the	O
upper	O
bound	O
provided	O
by	O
the	O
dual	O
in	O
a	O
branch	B
and	I
bound	I
i	O
in	O
this	O
case	O
we	O
say	O
that	O
i	O
has	O
a	O
unique	O
maximum	O
x	O
search	O
procedure	O
exercises	O
chapter	O
more	O
variational	B
inference	B
exercise	O
graphcuts	B
for	O
map	O
estimation	O
in	O
binary	O
submodular	B
mrfs	O
ex	O
of	O
and	O
friedman	O
show	O
that	O
using	O
the	O
graph	B
construction	O
described	O
in	O
section	O
the	O
cost	O
of	O
the	O
cut	O
is	O
equal	O
to	O
the	O
energy	O
of	O
the	O
corresponding	O
assignment	O
up	O
to	O
an	O
irrelevant	O
constant	O
this	O
exercise	O
involves	O
a	O
lot	O
of	O
algebraic	O
book-keeping	O
ex	O
const	O
i	O
ti	O
if	O
i	O
and	O
is	O
submodular	B
if	O
e	O
is	O
a	O
semimetric	O
i	O
xi	O
is	O
unchanged	O
f	O
xi	O
and	O
xi	O
exercise	O
graphcuts	B
for	O
alpha-beta	B
swap	I
ex	O
of	O
and	O
friedman	O
show	O
how	O
the	O
optimal	O
alpha-beta	B
swap	I
can	O
be	O
found	O
by	O
running	O
min-cut	O
on	O
an	O
appropriately	O
constructed	O
graph	B
more	O
precisely	O
a	O
define	O
a	O
set	O
of	O
binary	O
variables	O
tn	O
such	O
that	O
ti	O
means	O
b	O
define	O
an	O
energy	B
function	I
over	O
the	O
new	O
variables	O
such	O
that	O
c	O
show	O
that	O
exercise	O
constant	O
factor	B
optimality	O
for	O
alpha-expansion	O
daphne	O
koller	O
let	O
x	O
be	O
a	O
pairwise	O
metric	B
markov	B
random	O
field	O
over	O
a	O
graph	B
g	O
e	O
suppose	O
that	O
the	O
variables	O
are	O
nonbinary	O
and	O
that	O
the	O
node	O
potentials	O
are	O
nonnegative	O
let	O
a	O
denote	O
the	O
set	O
of	O
labels	O
for	O
each	O
x	O
x	O
though	O
it	O
is	O
not	O
possible	O
to	O
find	O
the	O
globally	O
optimal	O
assignment	O
in	O
general	O
the	O
algorithm	O
provides	O
a	O
method	O
for	O
finding	O
assignments	O
x	O
that	O
are	O
locally	O
optimal	O
with	O
respect	O
to	O
a	O
large	O
set	O
of	O
transformations	O
i	O
e	O
the	O
possible	O
moves	O
despite	O
the	O
fact	O
that	O
only	O
produces	O
a	O
locally	O
optimal	O
map	O
assignment	O
it	O
is	O
possible	O
to	O
prove	O
that	O
the	O
energy	O
of	O
this	O
assignment	O
is	O
within	O
a	O
known	O
factor	B
of	O
the	O
energy	O
of	O
the	O
globally	O
optimal	O
solution	O
in	O
fact	O
this	O
is	O
a	O
special	O
case	O
of	O
a	O
more	O
general	O
principle	O
that	O
applies	O
to	O
a	O
wide	O
variety	O
of	O
algorithms	O
including	O
max-product	B
belief	B
propagation	I
and	O
more	O
general	O
move-making	O
algorithms	O
if	O
one	O
can	O
prove	O
that	O
the	O
solutions	O
obtained	O
by	O
the	O
algorithm	O
are	O
strong	O
local	O
minima	O
i	O
e	O
local	O
minima	O
with	O
respect	O
to	O
a	O
large	O
set	O
of	O
potential	O
moves	O
then	O
it	O
is	O
possible	O
to	O
derive	O
bounds	O
on	O
the	O
suboptimality	O
of	O
these	O
solutions	O
and	O
the	O
quality	O
of	O
the	O
bounds	O
will	O
depend	O
on	O
the	O
nature	O
of	O
the	O
moves	O
considered	O
is	O
a	O
precise	O
definition	O
of	O
large	O
set	O
of	O
moves	O
consider	O
the	O
following	O
approach	O
to	O
proving	O
the	O
suboptimality	O
bound	O
for	O
a	O
let	O
x	O
be	O
a	O
local	O
minimum	O
with	O
respect	O
to	O
expansion	O
moves	O
for	O
each	O
a	O
let	O
v	O
v	O
s	O
i	O
e	O
the	O
set	O
of	O
nodes	B
labelled	O
in	O
the	O
global	B
minimum	I
let	O
be	O
an	O
assignment	O
that	O
is	O
equal	O
to	O
on	O
v	O
and	O
equal	O
to	O
x	O
elsewhere	O
this	O
is	O
an	O
of	O
x	O
verify	O
that	O
e	O
x	O
b	O
building	O
on	O
the	O
previous	O
part	O
show	O
that	O
e	O
x	O
where	O
c	O
maxst	O
e	O
and	O
e	O
denotes	O
the	O
energy	O
of	O
an	O
assignment	O
hint	O
think	O
about	O
where	O
agrees	O
with	O
x	O
and	O
where	O
it	O
agrees	O
with	O
max	O
st	O
min	O
st	O
exercise	O
dual	B
decomposition	I
for	O
pose	O
segmentation	O
daphne	O
koller	O
two	O
important	O
problems	O
in	O
computer	O
vision	O
are	O
that	O
of	O
parsing	O
articulated	O
objects	O
the	O
human	O
body	O
called	O
pose	O
estimation	O
and	O
segmenting	O
the	O
foreground	O
and	O
the	O
background	O
called	O
segmentation	O
intuitively	O
these	O
two	O
problems	O
are	O
linked	O
in	O
that	O
solving	O
either	O
one	O
would	O
be	O
easier	O
if	O
the	O
solution	O
to	O
the	O
other	O
were	O
available	O
we	O
consider	O
solving	O
these	O
problems	O
simultaneously	O
using	O
a	O
joint	O
model	O
over	O
human	O
poses	O
and	O
foregroundbackground	O
labels	O
and	O
then	O
using	O
dual	B
decomposition	I
for	O
map	O
inference	B
in	O
this	O
model	O
we	O
construct	O
a	O
two-level	O
model	O
where	O
the	O
high	O
level	O
handles	O
pose	O
estimation	O
and	O
the	O
low	O
level	O
handles	O
pixel-level	O
background	O
segmentation	O
let	O
g	O
be	O
an	O
undirected	B
grid	O
over	O
the	O
pixels	O
each	O
node	O
i	O
v	O
represents	O
a	O
pixel	O
suppose	O
we	O
have	O
one	O
binary	O
variable	O
xi	O
for	O
each	O
pixel	O
where	O
xi	O
means	O
that	O
pixel	O
i	O
is	O
in	O
the	O
foreground	O
denote	O
the	O
full	B
set	O
of	O
these	O
variables	O
by	O
x	O
map	O
state	B
estimation	I
in	O
addition	O
suppose	O
we	O
have	O
an	O
undirected	B
tree	B
structure	O
t	O
on	O
the	O
parts	O
for	O
each	O
body	O
part	O
we	O
have	O
a	O
discrete	B
set	O
of	O
candidate	O
poses	O
that	O
the	O
part	O
can	O
be	O
in	O
where	O
each	O
pose	O
is	O
characterized	O
by	O
parameters	O
specifying	O
its	O
position	O
and	O
orientation	O
candidates	O
are	O
generated	O
by	O
a	O
procedure	O
external	O
to	O
the	O
algorithm	O
described	O
here	O
define	O
yjk	O
to	O
be	O
a	O
binary	O
variable	O
indicating	O
whether	O
body	O
part	O
is	O
in	O
configuration	O
k	O
then	O
the	O
full	B
set	O
of	O
part	O
variables	O
is	O
given	O
by	O
y	O
with	O
j	O
j	O
and	O
k	O
k	O
where	O
j	O
is	O
the	O
total	O
number	O
of	O
body	O
parts	O
and	O
k	O
is	O
the	O
number	O
of	O
candidate	O
poses	O
for	O
each	O
part	O
note	O
that	O
in	O
order	O
to	O
describe	O
a	O
valid	O
configuration	O
y	O
must	O
satisfy	O
the	O
constraint	O
that	O
yjk	O
for	O
each	O
j	O
suppose	O
we	O
have	O
the	O
following	O
energy	B
function	I
on	O
pixels	O
xj	O
ij	O
i	O
e	O
i	O
v	O
p	O
assume	O
that	O
the	O
ij	O
arises	O
from	O
a	O
metric	B
based	O
on	O
differences	O
in	O
pixel	O
intensities	O
so	O
this	O
can	O
be	O
viewed	O
as	O
the	O
energy	O
for	O
a	O
pairwise	O
metric	B
mrf	B
with	O
respect	O
to	O
g	O
we	O
then	O
have	O
the	O
following	O
energy	B
function	I
for	O
parts	O
pyp	O
pqyp	O
yq	O
since	O
each	O
part	O
candidate	O
yjk	O
is	O
assumed	O
to	O
come	O
with	O
a	O
position	O
and	O
orientation	O
we	O
can	O
compute	O
a	O
jki	O
v	O
where	O
binary	B
mask	I
in	O
the	O
image	O
plane	O
the	O
mask	O
assigns	O
a	O
value	O
to	O
each	O
pixel	O
denoted	O
by	O
wi	O
jk	O
if	O
pixel	O
i	O
lies	O
on	O
the	O
skeleton	O
and	O
decreases	O
as	O
we	O
move	O
away	O
we	O
can	O
use	O
this	O
to	O
define	O
an	O
energy	B
function	I
relating	O
the	O
parts	O
and	O
the	O
pixels	O
y	O
yjk	O
wi	O
jk	O
i	O
v	O
j	O
in	O
other	O
words	O
this	O
energy	O
term	O
only	O
penalizes	O
the	O
case	O
where	O
a	O
part	O
candidate	O
is	O
active	O
but	O
the	O
pixel	O
underneath	O
is	O
labeled	O
as	O
background	O
formulate	O
the	O
minimization	O
of	O
as	O
an	O
integer	O
program	O
and	O
show	O
how	O
you	O
can	O
use	O
dual	B
decomposition	I
to	O
solve	O
the	O
dual	O
of	O
this	O
integer	O
program	O
your	O
solution	O
should	O
describe	O
the	O
decomposition	O
into	O
slaves	B
the	O
method	O
for	O
solving	O
each	O
one	O
and	O
the	O
update	O
rules	B
for	O
the	O
overall	O
algorithm	O
briefly	O
justify	O
your	O
design	O
choices	O
particularly	O
your	O
choice	O
of	O
inference	B
algorithms	O
for	O
the	O
slaves	B
monte	B
carlo	I
inference	B
introduction	O
so	O
far	O
we	O
discussed	O
various	O
deterministic	O
algorithms	O
for	O
posterior	O
inference	B
these	O
methods	O
enjoy	O
many	O
of	O
the	O
benefits	O
of	O
the	O
bayesian	B
approach	O
while	O
still	O
being	O
about	O
as	O
fast	O
as	O
optimization-based	O
point-estimation	O
methods	O
the	O
trouble	O
with	O
these	O
methods	O
is	O
that	O
they	O
can	O
be	O
rather	O
complicated	O
to	O
derive	O
and	O
they	O
are	O
somewhat	O
limited	O
in	O
their	O
domain	O
of	O
applicability	O
they	O
usually	O
assume	O
conjugate	B
priors	I
and	O
exponential	B
family	B
likelihoods	O
although	O
see	O
et	O
al	O
for	O
some	O
recent	O
extensions	O
of	O
mean	B
field	O
to	O
more	O
complex	O
distributions	O
furthermore	O
although	O
they	O
are	O
fast	O
their	O
accuracy	O
is	O
often	O
limited	O
by	O
the	O
form	O
of	O
the	O
approximation	O
which	O
we	O
choose	O
s	O
in	O
this	O
chapter	O
we	O
discuss	O
an	O
alternative	O
class	O
of	O
algorithms	O
based	O
on	O
the	O
idea	O
of	O
monte	B
carlo	I
approximation	O
which	O
we	O
first	O
introduced	O
in	O
section	O
the	O
idea	O
is	O
very	O
simple	O
generate	O
some	O
samples	B
from	O
the	O
posterior	O
xs	O
pxd	O
and	O
then	O
use	O
these	O
to	O
compute	O
any	O
quantity	O
of	O
interest	O
such	O
as	O
a	O
posterior	O
marginal	O
or	O
the	O
posterior	O
of	O
the	O
difference	O
of	O
two	O
quantities	O
or	O
the	O
posterior	O
predictive	B
pyd	O
etc	O
all	O
of	O
these	O
quantities	O
can	O
be	O
approximated	O
by	O
e	O
f	O
for	O
some	O
suitable	O
function	O
f	O
by	O
generating	O
enough	O
samples	B
we	O
can	O
achieve	O
any	O
desired	O
level	O
of	O
accuracy	O
we	O
like	O
the	O
main	O
issue	O
is	O
how	O
do	O
we	O
efficiently	O
generate	O
samples	B
from	O
a	O
probability	O
distribution	O
particularly	O
in	O
high	O
dimensions	O
in	O
this	O
chapter	O
we	O
discuss	O
non-iterative	O
methods	O
for	O
generating	O
independent	O
samples	B
in	O
the	O
next	O
chapter	O
we	O
discuss	O
an	O
iterative	O
method	O
known	O
as	O
markov	B
chain	I
monte	B
carlo	I
or	O
mcmc	B
for	O
short	O
which	O
produces	O
dependent	O
samples	B
but	O
which	O
works	O
well	O
in	O
high	O
dimensions	O
note	O
that	O
sampling	O
is	O
a	O
large	O
topic	B
the	O
reader	O
should	O
consult	O
other	O
books	O
such	O
as	O
robert	O
and	O
casella	O
for	O
more	O
information	B
sampling	O
from	O
standard	O
distributions	O
we	O
briefly	O
discuss	O
some	O
ways	O
to	O
sample	O
from	O
or	O
dimensional	O
distributions	O
of	O
standard	O
form	O
these	O
methods	O
are	O
often	O
used	O
as	O
subroutines	O
by	O
more	O
complex	O
methods	O
using	O
the	O
cdf	B
the	O
simplest	O
method	O
for	O
sampling	O
from	O
a	O
univariate	O
distribution	O
is	O
based	O
on	O
the	O
inverse	B
probability	I
transform	I
let	O
f	O
be	O
a	O
cdf	B
of	O
some	O
distribution	O
we	O
want	O
to	O
sample	O
from	O
and	O
let	O
f	O
chapter	O
monte	B
carlo	I
inference	B
f	O
u	O
x	O
figure	O
sampling	O
using	O
an	O
inverse	O
cdf	B
figure	O
generated	O
by	O
samplecdf	O
be	O
its	O
inverse	O
then	O
we	O
have	O
the	O
following	O
result	O
theorem	O
if	O
u	O
u	O
is	O
a	O
uniform	O
rv	O
then	O
f	O
f	O
proof	O
prf	O
x	O
pru	O
f	O
f	O
to	O
both	O
sides	O
f	O
pru	O
y	O
y	O
where	O
the	O
first	O
line	O
follows	O
since	O
f	O
is	O
a	O
monotonic	O
function	O
and	O
the	O
second	O
line	O
follows	O
since	O
u	O
is	O
uniform	O
on	O
the	O
unit	O
interval	O
hence	O
we	O
can	O
sample	O
from	O
any	O
univariate	O
distribution	O
for	O
which	O
we	O
can	O
evaluate	O
its	O
inverse	O
cdf	B
as	O
follows	O
generate	O
a	O
random	O
number	O
u	O
u	O
using	O
a	O
pseudo	B
random	I
number	I
generator	I
e	O
g	O
et	O
al	O
for	O
details	O
let	O
u	O
represent	O
the	O
height	O
up	O
the	O
y	O
axis	O
then	O
slide	O
along	O
the	O
x	O
axis	O
until	O
you	O
intersect	O
the	O
f	O
curve	O
and	O
then	O
drop	O
down	O
and	O
return	O
the	O
corresponding	O
x	O
value	O
this	O
corresponds	O
to	O
computing	O
x	O
f	O
see	O
figure	O
for	O
an	O
illustration	O
for	O
example	O
consider	O
the	O
exponential	B
distribution	I
exponx	O
e	O
x	O
ix	O
the	O
cdf	B
is	O
f	O
e	O
x	O
ix	O
whose	O
inverse	O
is	O
the	O
quantile	B
function	O
f	O
p	O
by	O
the	O
above	O
theorem	O
if	O
u	O
we	O
know	O
that	O
f	O
expon	O
furthermore	O
since	O
u	O
as	O
well	O
we	O
can	O
sample	O
from	O
the	O
exponential	B
distribution	I
by	O
first	O
sampling	O
from	O
the	O
uniform	O
and	O
then	O
transforming	O
the	O
results	O
using	O
lnu	O
rejection	B
sampling	I
sampling	O
from	O
a	O
gaussian	B
method	O
we	O
now	O
describe	O
a	O
method	O
to	O
sample	O
from	O
a	O
gaussian	B
the	O
idea	O
is	O
we	O
sample	O
uniformly	O
from	O
a	O
unit	O
radius	O
circle	O
and	O
then	O
use	O
the	O
change	B
of	I
variables	I
formula	O
to	O
derive	O
samples	B
from	O
a	O
spherical	B
gaussian	B
this	O
can	O
be	O
thought	O
of	O
as	O
two	O
samples	B
from	O
a	O
gaussian	B
in	O
more	O
detail	O
sample	O
uniformly	O
and	O
then	O
discard	O
pairs	O
that	O
do	O
not	O
satisfy	O
the	O
result	O
will	O
be	O
points	O
uniformly	O
distributed	O
inside	O
the	O
unit	O
circle	O
so	O
pz	O
iz	O
inside	O
circle	O
now	O
define	O
lnr	O
xi	O
zi	O
for	O
i	O
wherer	O
using	O
the	O
multivariate	O
change	B
of	I
variables	I
formula	O
we	O
have	O
exp	O
exp	O
hence	O
and	O
are	O
two	O
independent	O
samples	B
from	O
a	O
univariate	O
gaussian	B
this	O
is	O
known	O
as	O
the	O
box-muller	B
method	O
to	O
sample	O
from	O
a	O
multivariate	B
gaussian	B
we	O
first	O
compute	O
the	O
cholesky	B
decomposition	I
of	O
its	O
covariance	B
matrix	I
llt	O
where	O
l	O
is	O
lower	O
triangular	O
next	O
we	O
sample	O
x	O
n	O
i	O
using	O
the	O
box-muller	B
method	O
finally	O
we	O
set	O
y	O
lx	O
this	O
is	O
valid	O
since	O
cov	O
lcov	O
lt	O
l	O
i	O
lt	O
rejection	B
sampling	I
when	O
the	O
inverse	O
cdf	B
method	O
cannot	O
be	O
used	O
one	O
simple	O
alternative	O
is	O
to	O
use	O
rejection	B
sampling	I
which	O
we	O
now	O
explain	O
basic	O
idea	O
in	O
rejection	B
sampling	I
we	O
create	O
a	O
proposal	B
distribution	I
qx	O
which	O
satisifes	O
m	O
qx	O
px	O
for	O
some	O
constant	O
m	O
where	O
px	O
is	O
an	O
unnormalized	O
version	O
of	O
px	O
px	O
pxzp	O
for	O
some	O
possibly	O
unknown	B
constant	O
zp	O
the	O
function	O
m	O
qx	O
provides	O
an	O
upper	O
envelope	O
for	O
p	O
we	O
then	O
sample	O
x	O
qx	O
which	O
corresponds	O
to	O
picking	O
a	O
random	O
x	O
location	O
and	O
then	O
we	O
sample	O
u	O
u	O
which	O
corresponds	O
to	O
picking	O
a	O
random	O
height	O
location	O
under	O
the	O
envelope	O
if	O
u	O
px	O
m	O
qx	O
we	O
reject	O
the	O
sample	O
otherwise	O
we	O
accept	B
it	O
see	O
figure	O
where	O
the	O
acceptance	O
region	O
is	O
shown	O
shaded	O
and	O
the	O
rejection	O
region	O
is	O
the	O
white	O
region	O
between	O
the	O
shaded	O
zone	O
and	O
the	O
upper	O
envelope	O
we	O
now	O
prove	O
that	O
this	O
procedure	O
is	O
correct	O
let	O
s	O
u	O
pxm	O
qx	O
u	O
u	O
pxm	O
qx	O
chapter	O
monte	B
carlo	I
inference	B
mqx	O
accept	B
region	O
umqx	O
px	O
reject	O
region	O
target	O
px	O
comparison	O
function	O
mqx	O
x	O
qx	O
x	O
figure	O
schematic	O
illustration	O
of	O
rejection	B
sampling	I
source	O
figure	O
of	O
et	O
al	O
rejection	B
sampling	I
from	O
a	O
ga	O
used	O
with	O
kind	O
permission	O
of	O
nando	O
de	O
freitas	O
distribution	O
blue	O
using	O
a	O
proposal	O
of	O
the	O
form	O
m	O
gak	O
red	O
where	O
k	O
the	O
curves	O
touch	O
at	O
k	O
figure	O
generated	O
by	O
rejectionsamplingdemo	O
then	O
the	O
cdf	B
of	O
the	O
accepted	O
points	O
is	O
given	O
by	O
p	O
accepted	O
p	O
x	O
accepted	O
p	O
accepted	O
ix	O
u	O
ix	O
u	O
sqxdudx	O
pxdx	O
pxdx	O
which	O
is	O
the	O
cdf	B
of	O
px	O
as	O
desired	O
how	O
efficient	O
is	O
this	O
method	O
since	O
we	O
generate	O
with	O
probability	O
qx	O
and	O
accept	B
with	O
probability	O
px	O
m	O
qx	O
the	O
probability	O
of	O
acceptance	O
is	O
paccept	O
px	O
m	O
qx	O
qxdx	O
m	O
hence	O
we	O
want	O
to	O
choose	O
m	O
as	O
small	O
as	O
possible	O
while	O
still	O
satisfying	O
m	O
qx	O
px	O
pxdx	O
example	O
for	O
example	O
suppose	O
we	O
want	O
to	O
sample	O
from	O
a	O
gamma	B
gax	O
x	O
exp	O
x	O
iid	B
expon	O
and	O
y	O
xk	O
then	O
y	O
gak	O
for	O
one	O
can	O
show	O
that	O
if	O
xi	O
non-integer	O
shape	O
parameters	O
we	O
cannot	O
use	O
this	O
trick	O
however	O
we	O
can	O
use	O
rejection	B
sampling	I
this	O
section	O
is	O
based	O
on	O
notes	O
by	O
ioana	O
a	O
cosma	O
available	O
at	O
rejection	B
sampling	I
fx	O
half	O
gaussian	B
samples	B
from	O
fx	O
ars	O
figure	O
idea	O
behind	O
adaptive	B
rejection	B
sampling	I
we	O
place	O
piecewise	O
linear	O
upper	O
lower	O
bounds	O
on	O
the	O
log-concave	O
density	O
based	O
on	O
figure	O
of	O
and	O
wild	O
figure	O
generated	O
by	O
arsenvelope	O
using	O
ars	O
to	O
sample	O
from	O
a	O
half-gaussian	O
figure	O
generated	O
by	O
arsdemo	O
written	O
by	O
daniel	O
eaton	O
using	O
a	O
gak	O
distribution	O
as	O
a	O
proposal	O
where	O
k	O
the	O
ratio	O
has	O
the	O
form	O
px	O
qx	O
gax	O
gaxk	O
x	O
k	O
exp	O
x	O
x	O
exp	O
x	O
xk	O
exp	O
this	O
ratio	O
attains	O
its	O
maximum	O
when	O
x	O
k	O
hence	O
m	O
ga	O
k	O
ga	O
kk	O
see	O
figure	O
for	O
a	O
plot	O
based	O
on	O
the	O
cauchy	B
distribution	O
asks	O
you	O
to	O
devise	O
a	O
better	O
proposal	B
distribution	I
application	O
to	O
bayesian	B
statistics	I
suppose	O
we	O
want	O
to	O
draw	O
samples	B
from	O
the	O
posterior	O
p	O
pd	O
we	O
can	O
use	O
rejection	B
sampling	I
with	O
p	O
pd	O
as	O
the	O
target	O
distribution	O
q	O
p	O
as	O
our	O
proposal	O
and	O
m	O
pd	O
where	O
arg	O
max	O
pd	O
is	O
the	O
mle	B
this	O
was	O
first	O
suggested	O
in	O
and	O
gelfand	O
we	O
accept	B
points	O
with	O
probability	O
p	O
m	O
q	O
pd	O
pd	O
thus	O
samples	B
from	O
the	O
prior	O
that	O
have	O
high	O
likelihood	B
are	O
more	O
likely	O
to	O
be	O
retained	O
in	O
the	O
posterior	O
of	O
course	O
if	O
there	O
is	O
a	O
big	O
mismatch	O
between	O
prior	O
and	O
posterior	O
will	O
be	O
the	O
case	O
if	O
the	O
prior	O
is	O
vague	O
and	O
the	O
likelihood	B
is	O
informative	O
this	O
procedure	O
is	O
very	O
inefficient	O
we	O
discuss	O
better	O
algorithms	O
later	O
adaptive	B
rejection	B
sampling	I
we	O
now	O
describe	O
a	O
method	O
that	O
can	O
automatically	O
come	O
up	O
with	O
a	O
tight	O
upper	O
envelope	O
qx	O
to	O
any	O
log	O
concave	B
density	O
px	O
the	O
idea	O
is	O
to	O
upper	O
bound	O
the	O
log	O
density	O
with	O
a	O
piecewise	O
chapter	O
monte	B
carlo	I
inference	B
linear	O
function	O
as	O
illustrated	O
in	O
figure	O
we	O
choose	O
the	O
initial	O
locations	O
for	O
the	O
pieces	O
based	O
on	O
a	O
fixed	O
grid	O
over	O
the	O
support	B
of	O
the	O
distribution	O
we	O
then	O
evaluate	O
the	O
gradient	O
of	O
the	O
log	O
density	O
at	O
these	O
locations	O
and	O
make	O
the	O
lines	O
be	O
tangent	O
at	O
these	O
points	O
since	O
the	O
log	O
of	O
the	O
envelope	O
is	O
piecewise	O
linear	O
the	O
envelope	O
itself	O
is	O
piecewise	O
exponential	O
qx	O
i	O
i	O
exp	O
ix	O
xi	O
xi	O
x	O
xi	O
where	O
xi	O
are	O
the	O
grid	O
points	O
it	O
is	O
relatively	O
straightforward	O
to	O
sample	O
from	O
this	O
distribution	O
if	O
the	O
sample	O
x	O
is	O
rejected	O
we	O
create	O
a	O
new	O
grid	O
point	O
at	O
x	O
and	O
thereby	O
refine	O
the	O
envelope	O
as	O
the	O
number	O
of	O
grid	O
points	O
is	O
increased	O
the	O
tightness	O
of	O
the	O
envelope	O
improves	O
and	O
the	O
rejection	O
rate	B
goes	O
down	O
this	O
is	O
known	O
as	O
adaptive	B
rejection	B
sampling	I
and	O
wild	O
figure	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
as	O
with	O
standard	O
rejection	B
sampling	I
it	O
can	O
be	O
applied	O
to	O
unnormalized	O
distributions	O
rejection	B
sampling	I
in	O
high	O
dimensions	O
it	O
is	O
clear	O
that	O
we	O
want	O
to	O
make	O
our	O
proposal	O
qx	O
as	O
close	O
as	O
possible	O
to	O
the	O
target	O
distribution	O
px	O
while	O
still	O
being	O
an	O
upper	O
bound	O
but	O
this	O
is	O
quite	O
hard	O
to	O
achieve	O
especially	O
in	O
high	O
dimensions	O
to	O
see	O
this	O
consider	O
sampling	O
from	O
px	O
pi	O
using	O
as	O
a	O
proposal	O
qx	O
n	O
in	O
d	O
dimensions	O
the	O
optimum	O
value	O
is	O
given	O
by	O
m	O
q	O
pd	O
the	O
acceptance	O
rate	B
is	O
both	O
p	O
and	O
q	O
are	O
normalized	O
which	O
decreases	O
exponentially	O
fast	O
with	O
dimension	O
for	O
example	O
if	O
q	O
exceeds	O
p	O
by	O
just	O
then	O
in	O
dimensions	O
the	O
acceptance	O
ratio	O
will	O
be	O
about	O
this	O
is	O
a	O
fundamental	O
weakness	O
of	O
rejection	B
sampling	I
q	O
i	O
obviously	O
we	O
must	O
have	O
q	O
p	O
in	O
order	O
to	O
be	O
an	O
upper	O
bound	O
in	O
chapter	O
we	O
will	O
describe	O
mcmc	B
sampling	O
which	O
is	O
a	O
more	O
efficient	O
way	O
to	O
sample	O
from	O
high	O
dimensional	O
distributions	O
sometimes	O
this	O
uses	O
rejection	B
sampling	I
as	O
a	O
subroutine	O
which	O
is	O
known	O
as	O
adaptive	B
rejection	I
metropolis	I
sampling	I
et	O
al	O
importance	B
sampling	I
we	O
now	O
describe	O
a	O
monte	B
carlo	I
method	O
known	O
as	O
importance	B
sampling	I
for	O
approximating	O
integrals	O
of	O
the	O
form	O
i	O
e	O
f	O
basic	O
idea	O
the	O
idea	O
is	O
to	O
draw	O
samples	B
x	O
in	O
regions	O
which	O
have	O
high	O
probability	O
px	O
but	O
also	O
where	O
is	O
large	O
the	O
result	O
can	O
be	O
super	B
efficient	I
meaning	O
it	O
needs	O
less	O
samples	B
than	O
if	O
we	O
were	O
to	O
sample	O
from	O
the	O
exact	O
distribution	O
px	O
the	O
reason	O
is	O
that	O
the	O
samples	B
are	O
focussed	O
on	O
the	O
important	O
parts	O
of	O
space	O
for	O
example	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
probability	O
of	O
a	O
rare	B
event	I
define	O
f	O
e	O
for	O
some	O
set	O
e	O
then	O
it	O
is	O
better	O
to	O
sample	O
from	O
a	O
proposal	O
of	O
the	O
form	O
qx	O
f	O
than	O
to	O
sample	O
from	O
px	O
itself	O
importance	B
sampling	I
samples	B
from	O
any	O
proposal	O
qx	O
it	O
then	O
uses	O
these	O
samples	B
to	O
estimate	O
importance	B
sampling	I
the	O
integral	O
as	O
follows	O
e	O
f	O
px	O
qx	O
qxdx	O
s	O
wsf	O
i	O
qxs	O
are	O
the	O
importance	B
weights	I
note	O
that	O
unlike	O
rejection	B
sampling	I
we	O
use	O
all	O
how	O
should	O
we	O
choose	O
the	O
proposal	O
a	O
natural	O
criterion	O
is	O
to	O
minimize	O
the	O
variance	B
of	O
the	O
where	O
ws	O
pxs	O
the	O
samples	B
estimate	O
i	O
s	O
wsf	O
now	O
varqx	O
eqx	O
eqx	O
f	O
f	O
i	O
since	O
the	O
last	O
term	O
is	O
independent	O
of	O
q	O
we	O
can	O
ignore	O
it	O
by	O
jensen	O
s	O
inequality	O
we	O
have	O
the	O
following	O
lower	O
bound	O
the	O
lower	O
bound	O
is	O
obtained	O
when	O
we	O
use	O
the	O
optimal	O
importance	O
distribution	O
q	O
when	O
we	O
don	O
t	O
have	O
a	O
particular	O
target	O
function	O
f	O
in	O
mind	O
we	O
often	O
just	O
try	O
to	O
make	O
qx	O
as	O
close	O
as	O
possible	O
to	O
px	O
in	O
general	O
this	O
is	O
difficult	O
especially	O
in	O
high	O
dimensions	O
but	O
it	O
is	O
possible	O
to	O
adapt	O
the	O
proposal	B
distribution	I
to	O
improve	O
the	O
approximation	O
this	O
is	O
known	O
as	O
adaptive	B
importance	B
sampling	I
and	O
berger	O
handling	O
unnormalized	O
distributions	O
it	O
is	O
frequently	O
the	O
case	O
that	O
we	O
can	O
evaluate	O
the	O
unnormalized	O
target	O
distribution	O
px	O
but	O
not	O
its	O
normalization	O
constant	O
zp	O
we	O
may	O
also	O
want	O
to	O
use	O
an	O
unnormalized	O
proposal	O
qx	O
with	O
possibly	O
unknown	B
normlization	O
constant	O
zq	O
we	O
can	O
do	O
this	O
as	O
follows	O
first	O
we	O
evaluate	O
e	O
zq	O
zp	O
f	O
px	O
qx	O
qxdx	O
zq	O
zp	O
s	O
wsf	O
qxs	O
is	O
the	O
unnormalized	O
importance	O
weight	O
we	O
can	O
use	O
the	O
same	O
set	O
of	O
samples	B
where	O
ws	O
pxs	O
to	O
evaluate	O
the	O
ratio	O
zpzq	O
as	O
follows	O
pxdx	O
px	O
qx	O
qxdx	O
s	O
zp	O
zq	O
hence	O
s	O
i	O
zq	O
s	O
s	O
wsf	O
s	O
ws	O
wsf	O
ws	O
where	O
ws	O
chapter	O
monte	B
carlo	I
inference	B
are	O
the	O
normalized	O
importance	B
weights	I
the	O
resulting	O
estimate	O
is	O
a	O
ratio	O
of	O
two	O
estimates	O
and	O
hence	O
is	O
biased	O
however	O
as	O
s	O
we	O
have	O
that	O
i	O
i	O
under	O
weak	O
assumptions	O
e	O
g	O
and	O
casella	O
for	O
details	O
importance	B
sampling	I
for	O
a	O
dgm	B
likelihood	B
weighting	I
we	O
now	O
describe	O
a	O
way	O
to	O
use	O
importance	B
sampling	I
to	O
generate	O
samples	B
from	O
a	O
distribution	O
which	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graphical	B
model	I
if	O
we	O
have	O
no	O
evidence	B
we	O
can	O
sample	O
from	O
the	O
unconditional	O
joint	B
distribution	I
of	O
a	O
dgm	B
px	O
as	O
follows	O
first	O
sample	O
the	O
root	B
nodes	B
then	O
sample	O
their	O
children	B
then	O
sample	O
their	O
children	B
etc	O
this	O
is	O
known	O
as	O
ancestral	B
sampling	I
it	O
works	O
because	O
in	O
a	O
dag	B
we	O
can	O
always	O
topologically	O
order	O
the	O
nodes	B
so	O
that	O
parents	B
preceed	O
children	B
that	O
there	O
is	O
no	O
equivalent	O
easy	O
method	O
for	O
sampling	O
from	O
an	O
unconditional	O
undirected	B
graphical	B
model	I
now	O
suppose	O
we	O
have	O
some	O
evidence	B
so	O
some	O
nodes	B
are	O
clamped	O
to	O
observed	O
values	O
and	O
we	O
want	O
to	O
sample	O
from	O
the	O
posterior	O
pxd	O
if	O
all	O
the	O
variables	O
are	O
discrete	B
we	O
can	O
use	O
the	O
following	O
simple	O
procedure	O
perform	O
ancestral	B
sampling	I
but	O
as	O
soon	O
as	O
we	O
sample	O
a	O
value	O
that	O
is	O
inconsistent	O
with	O
an	O
observed	O
value	O
reject	O
the	O
whole	O
sample	O
and	O
start	O
again	O
this	O
is	O
known	O
as	O
logic	B
sampling	I
needless	O
to	O
say	O
logic	B
sampling	I
is	O
very	O
inefficient	O
and	O
it	O
cannot	O
be	O
applied	O
when	O
we	O
have	O
real-valued	O
evidence	B
however	O
it	O
can	O
be	O
modified	O
as	O
follows	O
sample	O
unobserved	O
variables	O
as	O
before	O
conditional	O
on	O
their	O
parents	B
but	O
don	O
t	O
sample	O
observed	O
variables	O
instead	O
we	O
just	O
use	O
their	O
observed	O
values	O
this	O
is	O
equivalent	O
to	O
using	O
a	O
proposal	O
of	O
the	O
form	O
qx	O
pxtxpat	O
x	O
t	O
t	O
e	O
e	O
where	O
e	O
is	O
the	O
set	O
of	O
observed	O
nodes	B
and	O
x	O
therefore	O
give	O
the	O
overall	O
sample	O
an	O
importance	O
weight	O
as	O
follows	O
t	O
is	O
the	O
observed	O
value	O
for	O
node	O
t	O
we	O
should	O
wx	O
px	O
qx	O
pxtxpat	O
pxtxpat	O
pxtxpat	O
t	O
e	O
e	O
t	O
e	O
pxtxpat	O
this	O
technique	O
is	O
known	O
as	O
likelihood	B
weighting	I
and	O
chang	O
shachter	O
and	O
peot	O
sampling	B
importance	I
resampling	I
we	O
can	O
draw	O
unweighted	O
samples	B
from	O
px	O
by	O
first	O
using	O
importance	B
sampling	I
proposal	O
q	O
to	O
generate	O
a	O
distribution	O
of	O
the	O
form	O
ws	O
xs	O
s	O
px	O
particle	O
filtering	B
where	O
ws	O
are	O
the	O
normalized	O
importance	B
weights	I
we	O
then	O
sample	O
with	O
replacement	O
from	O
equation	O
where	O
the	O
probability	O
that	O
we	O
pick	O
xs	O
is	O
ws	O
let	O
this	O
procedure	O
induce	O
a	O
distribution	O
denoted	O
by	O
p	O
to	O
see	O
that	O
this	O
is	O
valid	O
note	O
that	O
s	O
ixs	O
pxsqxs	O
s	O
pxsqxs	O
s	O
px	O
qx	O
qxdx	O
ixs	O
px	O
ix	O
qx	O
qxdx	O
ix	O
pxdx	O
pxdx	O
ix	O
px	O
px	O
px	O
this	O
is	O
known	O
as	O
sampling	B
importance	I
resampling	I
the	O
result	O
is	O
an	O
unweighted	O
approximation	O
of	O
the	O
form	O
xs	O
note	O
that	O
we	O
typically	O
take	O
s	O
this	O
algorithm	O
can	O
be	O
used	O
to	O
perform	O
bayesian	B
inference	B
in	O
low-dimensional	O
settings	O
and	O
gelfand	O
that	O
is	O
suppose	O
we	O
want	O
to	O
draw	O
samples	B
from	O
the	O
posterior	O
p	O
pd	O
we	O
can	O
use	O
importance	B
sampling	I
with	O
p	O
pd	O
as	O
the	O
unnormalized	O
posterior	O
and	O
q	O
p	O
as	O
our	O
proposal	O
the	O
normalized	O
weights	O
have	O
the	O
form	O
ws	O
pd	O
s	O
pd	O
we	O
can	O
then	O
use	O
sir	B
to	O
sample	O
from	O
p	O
p	O
sq	O
s	O
p	O
of	O
course	O
if	O
there	O
is	O
a	O
big	O
discrepancy	O
between	O
our	O
proposal	O
prior	O
and	O
the	O
target	O
posterior	O
we	O
will	O
need	O
a	O
huge	O
number	O
of	O
importance	O
samples	B
for	O
this	O
technique	O
to	O
work	O
reliably	O
since	O
otherwise	O
the	O
variance	B
of	O
the	O
importance	B
weights	I
will	O
be	O
very	O
large	O
implying	O
that	O
most	O
samples	B
carry	O
no	O
useful	O
information	B
issue	O
will	O
come	O
up	O
again	O
in	O
section	O
when	O
we	O
discuss	O
particle	O
filtering	B
particle	O
filtering	B
particle	O
filtering	B
is	O
a	O
monte	B
carlo	I
or	O
simulation	B
based	I
algorithm	O
for	O
recursive	B
bayesian	B
inference	B
that	O
is	O
it	O
approximates	O
the	O
predict-update	B
cycle	B
described	O
in	O
section	O
it	O
is	O
very	O
widely	O
used	O
in	O
many	O
areas	O
including	O
tracking	B
time-series	B
forecasting	I
online	O
parameter	B
learning	B
etc	O
we	O
explain	O
the	O
basic	O
algorithm	O
below	O
for	O
a	O
book-length	O
treatment	O
see	O
et	O
al	O
for	O
a	O
good	O
tutorial	O
see	O
et	O
al	O
or	O
just	O
read	O
on	O
chapter	O
monte	B
carlo	I
inference	B
sequential	B
importance	B
sampling	I
the	O
basic	O
idea	O
is	O
to	O
appproximate	O
the	O
belief	B
state	B
the	O
entire	O
state	B
trajectory	O
using	O
a	O
weighted	O
set	O
of	O
particles	O
ws	O
t	O
zs	O
where	O
ws	O
t	O
is	O
the	O
normalized	O
weight	O
of	O
sample	O
s	O
at	O
time	O
t	O
from	O
this	O
representation	O
we	O
can	O
easily	O
compute	O
the	O
marginal	B
distribution	I
over	O
the	O
most	O
recent	O
state	B
by	O
simply	O
ignoring	O
the	O
previous	O
parts	O
of	O
the	O
trajectory	O
fact	O
that	O
pf	O
samples	B
in	O
the	O
space	O
of	O
entire	O
trajectories	O
has	O
various	O
implications	O
which	O
we	O
will	O
discuss	O
later	O
if	O
the	O
proposal	O
has	O
the	O
form	O
qzs	O
we	O
update	O
this	O
belief	B
state	B
using	O
importance	B
sampling	I
then	O
the	O
importance	B
weights	I
are	O
given	O
by	O
t	O
pzs	O
ws	O
qzs	O
ws	O
which	O
can	O
be	O
normalized	O
as	O
follows	O
t	O
ws	O
t	O
we	O
can	O
rewrite	O
the	O
numerator	O
recursively	O
as	O
follows	O
pytztpztzt	O
where	O
we	O
have	O
made	O
the	O
usual	O
markov	B
assumptions	O
we	O
will	O
restrict	O
attention	O
to	O
proposal	O
densities	O
of	O
the	O
following	O
form	O
so	O
that	O
we	O
can	O
grow	O
the	O
trajectory	O
by	O
adding	O
the	O
new	O
state	B
zt	O
to	O
the	O
end	O
importance	B
weights	I
simplify	O
to	O
tzs	O
ws	O
t	O
pytzs	O
t	O
t	O
tzs	O
pytzs	O
tzs	O
t	O
t	O
tzs	O
qzs	O
ws	O
qzs	O
t	O
if	O
we	O
further	O
assume	O
that	O
yt	O
then	O
we	O
only	O
need	O
to	O
keep	O
the	O
most	O
recent	O
part	O
of	O
the	O
trajectory	O
and	O
observation	B
sequence	O
rather	O
than	O
the	O
whole	O
history	O
in	O
order	O
to	O
compute	O
the	O
new	O
sample	O
in	O
this	O
case	O
the	O
weight	O
becomes	O
in	O
this	O
case	O
the	O
t	O
ws	O
ws	O
t	O
pytzs	O
qzs	O
tzs	O
t	O
yt	O
t	O
tzs	O
t	O
particle	O
filtering	B
hence	O
we	O
can	O
approximate	O
the	O
posterior	O
filtered	O
density	O
using	O
ws	O
t	O
zs	O
t	O
as	O
s	O
one	O
can	O
show	O
that	O
this	O
approaches	O
the	O
true	O
posterior	O
et	O
al	O
the	O
basic	O
algorithm	O
is	O
now	O
very	O
simple	O
for	O
each	O
old	O
sample	O
s	O
propose	B
an	O
extension	B
using	O
t	O
qztzs	O
zs	O
t	O
using	O
equation	O
unfortunately	O
this	O
basic	O
algorithm	O
does	O
not	O
work	O
very	O
well	O
as	O
we	O
discuss	O
below	O
t	O
yt	O
and	O
give	O
this	O
new	O
particle	O
weight	O
ws	O
the	O
degeneracy	B
problem	I
the	O
basic	O
sequential	B
importance	B
sampling	I
algorithm	O
fails	O
after	O
a	O
few	O
steps	O
because	O
most	O
of	O
the	O
particles	O
will	O
have	O
negligible	O
weight	O
this	O
is	O
called	O
the	O
degeneracy	B
problem	I
and	O
occurs	O
because	O
we	O
are	O
sampling	O
in	O
a	O
high-dimensional	O
space	O
fact	O
the	O
space	O
is	O
growing	O
in	O
size	O
over	O
time	O
using	O
a	O
myopic	O
proposal	B
distribution	I
we	O
can	O
quantify	O
the	O
degree	B
of	O
degeneracy	O
using	O
the	O
effective	B
sample	I
size	I
defined	O
by	O
where	O
w	O
s	O
t	O
yt	O
is	O
the	O
true	O
weight	O
of	O
particle	O
s	O
this	O
quantity	O
cannot	O
be	O
computed	O
exactly	O
since	O
we	O
don	O
t	O
know	O
the	O
true	O
posterior	O
but	O
we	O
can	O
approximate	O
it	O
using	O
t	O
pzs	O
tzs	O
s	O
eff	O
s	O
var	B
s	O
t	O
t	O
s	O
eff	O
if	O
the	O
variance	B
of	O
the	O
weights	O
is	O
large	O
then	O
we	O
are	O
wasting	O
our	O
resources	O
updating	O
particles	O
with	O
low	O
weight	O
which	O
do	O
not	O
contribute	O
much	O
to	O
our	O
posterior	O
estimate	O
there	O
are	O
two	O
main	O
solutions	O
to	O
the	O
degeneracy	B
problem	I
adding	O
a	O
resampling	O
step	O
and	O
using	O
a	O
good	O
proposal	B
distribution	I
we	O
discuss	O
both	O
of	O
these	O
in	O
turn	O
the	O
resampling	O
step	O
the	O
main	O
improvement	O
to	O
the	O
basic	O
sis	O
algorithm	O
is	O
to	O
monitor	O
the	O
effective	O
sampling	O
size	O
and	O
whenever	O
it	O
drops	O
below	O
a	O
threshold	O
to	O
eliminate	O
particles	O
with	O
low	O
weight	O
and	O
then	O
pf	O
is	O
sometimes	O
called	O
survival	O
of	O
the	O
to	O
create	O
replicates	O
of	O
the	O
surviving	O
particles	O
fittest	O
et	O
al	O
in	O
particular	O
we	O
generate	O
a	O
new	O
set	O
by	O
sampling	O
with	O
replacement	O
s	O
times	O
from	O
the	O
weighted	O
distribution	O
t	O
ws	O
t	O
zs	O
t	O
where	O
the	O
probability	O
of	O
choosing	O
particle	O
j	O
for	O
replication	O
is	O
wj	O
is	O
sometimes	O
called	O
t	O
rejuvenation	B
the	O
result	O
is	O
an	O
iid	B
unweighted	O
sample	O
from	O
the	O
discrete	B
density	O
equation	O
so	O
we	O
set	O
the	O
new	O
weights	O
to	O
ws	O
t	O
this	O
scheme	O
is	O
illustrated	O
in	O
figure	O
chapter	O
monte	B
carlo	I
inference	B
figure	O
illustration	O
of	O
particle	O
filtering	B
there	O
are	O
a	O
variety	O
of	O
algorithms	O
for	O
peforming	O
the	O
resampling	O
step	O
the	O
simplest	O
is	O
multi	O
nomial	O
resampling	O
which	O
computes	O
ks	O
mus	O
we	O
then	O
make	O
ks	O
copies	O
of	O
zs	O
t	O
various	O
improvements	O
exist	O
such	O
as	O
systematic	B
resampling	I
residual	B
resampling	I
and	O
stratified	O
sampling	O
which	O
can	O
reduce	O
the	O
variance	B
of	O
the	O
weights	O
all	O
these	O
methods	O
take	O
os	O
time	O
see	O
et	O
al	O
for	O
details	O
t	O
ws	O
t	O
the	O
overall	O
particle	O
filtering	B
algorithm	O
is	O
summarized	O
in	O
algorithm	O
that	O
if	O
an	O
estimate	O
of	O
the	O
state	B
is	O
required	O
it	O
should	O
be	O
computed	O
before	O
the	O
resampling	O
step	O
since	O
this	O
will	O
result	O
in	O
lower	O
variance	B
algorithm	O
one	O
step	O
of	O
a	O
generic	O
particle	O
filter	O
for	O
s	O
s	O
do	O
t	O
qztzs	O
draw	O
zs	O
compute	O
weight	O
ws	O
pytzs	O
qzs	O
tzs	O
t	O
t	O
tzs	O
t	O
t	O
yt	O
t	O
ws	O
t	O
ws	O
t	O
t	O
t	O
normalize	O
weights	O
ws	O
compute	O
seff	O
if	O
s	O
smin	O
then	O
eff	O
resample	O
s	O
indices	O
wt	O
t	O
z	O
z	O
t	O
ws	O
t	O
although	O
the	O
resampling	O
step	O
helps	O
with	O
the	O
degeneracy	B
problem	I
it	O
introduces	O
problems	O
of	O
its	O
own	O
in	O
particular	O
since	O
the	O
particles	O
with	O
high	O
weight	O
will	O
be	O
selected	O
many	O
times	O
there	O
is	O
a	O
loss	B
of	O
diversity	O
amongst	O
the	O
population	O
this	O
is	O
known	O
as	O
sample	B
impoverishment	I
in	O
the	O
particle	O
filtering	B
extreme	O
case	O
of	O
no	O
process	O
noise	O
if	O
we	O
have	O
static	O
but	O
unknown	B
parameters	O
as	O
part	O
of	O
the	O
state	B
space	I
then	O
all	O
the	O
particles	O
will	O
collapse	O
to	O
a	O
single	O
point	O
within	O
a	O
few	O
iterations	O
to	O
mitigate	O
this	O
problem	O
several	O
solutions	O
have	O
been	O
proposed	O
only	O
resample	O
when	O
original	O
bootstrap	B
filter	O
resampled	O
at	O
necessary	O
not	O
at	O
every	O
time	O
step	O
every	O
step	O
but	O
this	O
is	O
suboptimal	O
after	O
replicating	O
old	O
particles	O
sample	O
new	O
values	O
using	O
an	O
mcmc	B
step	O
which	O
leaves	B
the	O
posterior	O
distribution	O
invariant	B
e	O
g	O
the	O
resample-move	B
algorithm	O
in	O
and	O
berzuini	O
create	O
a	O
kernel	B
density	O
estimate	O
on	O
top	O
of	O
the	O
particles	O
t	O
zs	O
ws	O
t	O
where	O
is	O
some	O
smoothing	B
kernel	B
we	O
then	O
sample	O
from	O
this	O
smoothed	O
distribution	O
this	O
is	O
known	O
as	O
a	O
regularized	O
particle	O
filter	O
et	O
al	O
when	O
performing	O
inference	B
on	O
static	O
parameters	O
add	O
some	O
artificial	O
process	O
noise	O
this	O
is	O
undesirable	O
other	O
algorithms	O
must	O
be	O
used	O
for	O
online	O
parameter	B
estimation	O
e	O
g	O
et	O
al	O
the	O
proposal	B
distribution	I
the	O
simplest	O
and	O
most	O
widely	O
used	O
proposal	B
distribution	I
is	O
to	O
sample	O
from	O
the	O
prior	O
qztzs	O
t	O
yt	O
pztzs	O
t	O
in	O
this	O
case	O
the	O
weight	O
update	O
simplifies	O
to	O
t	O
ws	O
ws	O
t	O
t	O
this	O
can	O
be	O
thought	O
of	O
a	O
generate	B
and	I
test	I
approach	O
we	O
sample	O
values	O
from	O
the	O
dynamic	O
model	O
and	O
then	O
evaluate	O
how	O
good	O
they	O
are	O
after	O
we	O
see	O
the	O
data	O
figure	O
this	O
is	O
the	O
approach	O
used	O
in	O
the	O
condensation	B
algorithm	O
stands	O
for	O
conditional	O
density	O
propagation	O
used	O
for	O
visual	O
tracking	B
and	O
blake	O
however	O
if	O
the	O
likelihood	B
is	O
narrower	O
than	O
the	O
dynamical	O
prior	O
the	O
sensor	O
is	O
more	O
informative	O
than	O
the	O
motion	O
model	O
which	O
is	O
often	O
the	O
case	O
this	O
is	O
a	O
very	O
inefficient	O
approach	O
since	O
most	O
particles	O
will	O
be	O
assigned	O
very	O
low	O
weight	O
it	O
is	O
much	O
better	O
to	O
actually	O
look	O
at	O
the	O
data	O
yt	O
when	O
generating	O
a	O
proposal	O
qztzs	O
t	O
yt	O
pztzs	O
optimal	O
proposal	B
distribution	I
has	O
the	O
following	O
form	O
pytztpztzs	O
t	O
pytzs	O
if	O
we	O
use	O
this	O
proposal	O
the	O
new	O
weight	O
is	O
given	O
by	O
tzs	O
t	O
t	O
ws	O
ws	O
t	O
ws	O
t	O
t	O
yt	O
t	O
t	O
t	O
in	O
fact	O
the	O
this	O
proposal	O
is	O
optimal	O
since	O
for	O
any	O
given	O
zs	O
regardless	O
of	O
the	O
value	O
drawn	O
for	O
zs	O
true	O
weights	O
var	B
s	O
t	O
is	O
zero	O
t	O
the	O
new	O
weight	O
ws	O
t	O
hence	O
conditional	O
on	O
the	O
old	O
values	O
z	O
t	O
takes	O
the	O
same	O
value	O
t	O
the	O
variance	B
of	O
chapter	O
monte	B
carlo	I
inference	B
in	O
general	O
it	O
is	O
intractable	O
to	O
sample	O
from	O
pztzs	O
t	O
yt	O
and	O
to	O
evaluate	O
the	O
integral	O
needed	O
to	O
compute	O
the	O
predictive	B
density	O
pytzs	O
t	O
however	O
there	O
are	O
two	O
cases	O
when	O
the	O
optimal	O
proposal	B
distribution	I
can	O
be	O
used	O
the	O
first	O
setting	O
is	O
when	O
zt	O
is	O
discrete	B
so	O
the	O
integral	O
becomes	O
a	O
sum	O
of	O
course	O
if	O
the	O
entire	O
state	B
space	I
is	O
discrete	B
we	O
can	O
use	O
an	O
hmm	B
filter	O
instead	O
but	O
in	O
some	O
cases	O
some	O
parts	O
of	O
the	O
state	B
are	O
discrete	B
and	O
some	O
continuous	O
the	O
second	O
setting	O
is	O
when	O
pztzs	O
t	O
yt	O
is	O
gaussian	B
this	O
occurs	O
when	O
the	O
dynamics	O
are	O
nonlinear	O
but	O
the	O
observations	O
are	O
linear	O
see	O
exercise	O
for	O
the	O
details	O
in	O
cases	O
where	O
the	O
model	O
is	O
not	O
linear-gaussian	O
we	O
may	O
still	O
compute	O
a	O
gaussian	B
approximation	I
to	O
pztzs	O
t	O
yt	O
using	O
the	O
unscented	B
transform	I
and	O
use	O
this	O
as	O
a	O
proposal	O
this	O
is	O
known	O
as	O
the	O
unscented	O
particle	O
filter	O
der	O
merwe	O
et	O
al	O
in	O
more	O
general	O
settings	O
we	O
can	O
use	O
other	O
kinds	O
of	O
data-driven	B
proposals	I
perhaps	O
based	O
on	O
discriminative	B
models	O
unlike	O
mcmc	B
we	O
do	O
not	O
need	O
to	O
worry	O
about	O
the	O
proposals	O
being	O
reversible	O
application	O
robot	O
localization	O
consider	O
a	O
mobile	O
robot	O
wandering	O
around	O
an	O
office	O
environment	O
we	O
will	O
assume	O
that	O
it	O
already	O
has	O
a	O
map	O
of	O
the	O
world	O
represented	O
in	O
the	O
form	O
of	O
an	O
occupancy	B
grid	I
which	O
just	O
specifies	O
whether	O
each	O
grid	O
cell	O
is	O
empty	O
space	O
or	O
occupied	O
by	O
an	O
something	O
solid	O
like	O
a	O
wall	O
the	O
goal	O
is	O
for	O
the	O
robot	O
to	O
estimate	O
its	O
location	O
this	O
can	O
be	O
solved	O
optimally	O
using	O
an	O
hmm	B
filter	O
since	O
we	O
are	O
assuming	O
the	O
state	B
space	I
is	O
discrete	B
however	O
since	O
the	O
number	O
of	O
states	O
k	O
is	O
often	O
very	O
large	O
the	O
ok	O
time	O
complexity	O
per	O
update	O
is	O
prohibitive	O
we	O
can	O
use	O
a	O
particle	O
filter	O
as	O
a	O
sparse	B
approximation	O
to	O
the	O
belief	B
state	B
this	O
is	O
known	O
as	O
monte	B
carlo	I
localization	I
and	O
is	O
described	O
in	O
detail	O
in	O
et	O
al	O
figure	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
the	O
robot	O
uses	O
a	O
sonar	O
range	O
finder	O
so	O
it	O
can	O
only	O
sense	O
distance	O
to	O
obstacles	O
it	O
starts	O
out	O
with	O
a	O
uniform	O
prior	O
reflecting	O
the	O
fact	O
that	O
the	O
owner	O
of	O
the	O
robot	O
may	O
have	O
turned	O
it	O
on	O
in	O
an	O
arbitrary	O
location	O
out	O
where	O
you	O
are	O
starting	O
from	O
a	O
uniform	O
prior	O
is	O
called	O
global	B
localization	I
after	O
the	O
first	O
scan	O
which	O
indicates	O
two	O
walls	O
on	O
either	O
side	O
the	O
belief	B
state	B
is	O
shown	O
in	O
the	O
posterior	O
is	O
still	O
fairly	O
broad	O
since	O
the	O
robot	O
could	O
be	O
in	O
any	O
location	O
where	O
the	O
walls	O
are	O
fairly	O
close	O
by	O
such	O
as	O
a	O
corridor	O
or	O
any	O
of	O
the	O
narrow	O
rooms	O
after	O
moving	O
to	O
location	O
the	O
robot	O
is	O
pretty	O
sure	O
it	O
must	O
be	O
in	O
the	O
corridor	O
as	O
shown	O
in	O
after	O
moving	O
to	O
location	O
the	O
sensor	O
is	O
able	O
to	O
detect	O
the	O
end	O
of	O
the	O
corridor	O
however	O
due	O
to	O
symmetry	O
it	O
is	O
not	O
sure	O
if	O
it	O
is	O
in	O
location	O
i	O
true	O
location	O
or	O
location	O
ii	O
is	O
an	O
example	O
of	O
perceptual	B
aliasing	I
which	O
refers	O
to	O
the	O
fact	O
that	O
different	O
things	O
may	O
look	O
the	O
same	O
after	O
moving	O
to	O
locations	O
and	O
it	O
is	O
finally	O
able	O
to	O
figure	O
out	O
precisely	O
where	O
it	O
is	O
the	O
whole	O
process	O
is	O
analogous	O
to	O
someone	O
getting	O
lost	O
in	O
an	O
office	O
building	O
and	O
wandering	O
the	O
corridors	O
until	O
they	O
see	O
a	O
sign	O
they	O
recognize	O
in	O
section	O
we	O
discuss	O
how	O
to	O
estimate	O
location	O
and	O
the	O
map	O
at	O
the	O
same	O
time	O
application	O
visual	O
object	O
tracking	B
our	O
next	O
example	O
is	O
concerned	O
with	O
tracking	B
an	O
object	O
this	O
case	O
a	O
remote-controlled	O
helicopter	O
in	O
a	O
video	O
sequence	O
the	O
method	O
uses	O
a	O
simple	O
linear	O
motion	O
model	O
for	O
the	O
centroid	B
of	O
the	O
object	O
and	O
a	O
color	O
histogram	B
for	O
the	O
likelihood	B
model	O
using	O
bhattacharya	B
distance	I
to	O
compare	O
histograms	O
the	O
proposal	B
distribution	I
is	O
obtained	O
by	O
sampling	O
from	O
the	O
likelihood	B
see	O
et	O
al	O
for	O
further	O
details	O
particle	O
filtering	B
path	B
and	O
reference	O
poses	O
belief	O
at	O
reference	O
pose	O
room	O
a	O
start	O
room	O
b	O
room	O
c	O
belief	O
at	O
reference	O
pose	O
belief	O
at	O
reference	O
pose	O
ii	O
i	O
belief	O
at	O
reference	O
pose	O
belief	O
at	O
reference	O
pose	O
i	O
ii	O
figure	O
with	O
kind	O
permission	O
of	O
sebastian	O
thrun	O
illustration	O
of	O
monte	B
carlo	I
localization	I
source	O
figure	O
of	O
et	O
al	O
used	O
figure	O
shows	O
some	O
example	O
frames	O
the	O
system	O
uses	O
s	O
particles	O
with	O
an	O
effective	B
sample	I
size	I
of	O
s	O
eff	O
shows	O
the	O
belief	B
state	B
at	O
frame	O
the	O
system	O
has	O
had	O
to	O
resample	O
times	O
to	O
keep	O
the	O
effective	B
sample	I
size	I
above	O
the	O
threshold	O
of	O
shows	O
the	O
belief	B
state	B
at	O
frame	O
the	O
red	O
lines	O
show	O
the	O
estimated	O
location	O
of	O
the	O
center	O
of	O
the	O
object	O
over	O
the	O
last	O
frames	O
shows	O
that	O
the	O
system	O
can	O
handle	O
visual	O
clutter	O
as	O
long	O
as	O
it	O
does	O
not	O
have	O
the	O
same	O
color	O
as	O
the	O
target	O
object	O
shows	O
that	O
the	O
system	O
is	O
confused	O
between	O
the	O
grey	O
of	O
the	O
helicopter	O
and	O
the	O
grey	O
of	O
the	O
building	O
the	O
posterior	O
is	O
bimodal	O
the	O
green	O
ellipse	O
representing	O
the	O
posterior	B
mean	B
and	O
covariance	B
is	O
in	O
between	O
the	O
two	O
modes	O
shows	O
that	O
the	O
probability	O
mass	O
has	O
shifted	O
to	O
the	O
wrong	O
mode	B
the	O
system	O
has	O
lost	O
track	O
shows	O
the	O
particles	O
spread	O
out	O
over	O
the	O
gray	O
building	O
recovery	O
of	O
the	O
object	O
is	O
very	O
unlikely	O
from	O
this	O
state	B
using	O
this	O
chapter	O
monte	B
carlo	I
inference	B
figure	O
example	O
of	O
particle	O
filtering	B
applied	O
to	O
visual	O
object	O
tracking	B
based	O
on	O
color	O
histograms	O
succesful	O
tracking	B
green	O
ellipse	O
is	O
on	O
top	O
of	O
the	O
helicopter	O
tracker	O
gets	O
distracted	O
by	O
gray	O
clutter	O
in	O
the	O
background	O
see	O
text	O
for	O
details	O
figure	O
generated	O
by	O
pfcolortrackerdemo	O
written	O
by	O
sebastien	O
paris	O
proposal	O
we	O
see	O
that	O
the	O
method	O
is	O
able	O
to	O
keep	O
track	O
for	O
a	O
fairly	O
long	O
time	O
despite	O
the	O
presence	O
of	O
clutter	O
however	O
eventually	O
it	O
loses	O
track	O
of	O
the	O
object	O
note	O
that	O
since	O
the	O
algorithm	O
is	O
stochastic	O
simply	O
re-running	O
the	O
demo	O
may	O
fix	O
the	O
problem	O
but	O
in	O
the	O
real	O
world	O
this	O
is	O
not	O
an	O
option	O
the	O
simplest	O
way	O
to	O
improve	O
performance	O
is	O
to	O
use	O
more	O
particles	O
an	O
alternative	O
is	O
to	O
perform	O
tracking	B
by	I
detection	I
by	O
running	O
an	O
object	O
detector	O
over	O
the	O
image	O
every	O
few	O
frames	O
see	O
and	O
ponce	O
szeliski	O
prince	O
for	O
details	O
rao-blackwellised	O
particle	O
filtering	B
application	O
time	O
series	O
forecasting	O
in	O
section	O
we	O
discussed	O
how	O
to	O
use	O
the	O
kalman	O
filter	O
to	O
perform	O
time	O
series	O
forecasting	O
this	O
assumes	O
that	O
the	O
model	O
is	O
a	O
linear-gaussian	O
state-space	O
model	O
there	O
are	O
many	O
models	O
which	O
are	O
either	O
non-linear	O
andor	O
non-gaussian	O
for	O
example	O
stochastic	B
volatility	I
models	O
which	O
are	O
widely	O
used	O
in	O
finance	O
assume	O
that	O
the	O
variance	B
of	O
the	O
system	O
andor	O
observation	B
noise	O
changes	O
over	O
time	O
particle	O
filtering	B
is	O
widely	O
used	O
in	O
such	O
settings	O
see	O
e	O
g	O
et	O
al	O
and	O
references	O
therein	O
for	O
details	O
rao-blackwellised	O
particle	O
filtering	B
in	O
some	O
models	O
we	O
can	O
partition	O
the	O
hidden	B
variables	I
into	O
two	O
kinds	O
qt	O
and	O
zt	O
such	O
that	O
we	O
can	O
analytically	O
integrate	B
out	I
zt	O
provided	O
we	O
know	O
the	O
values	O
of	O
this	O
means	O
we	O
only	O
have	O
sample	O
and	O
can	O
represent	O
parametrically	O
thus	O
each	O
particle	O
s	O
represents	O
a	O
value	O
for	O
qs	O
these	O
hybrid	O
particles	O
are	O
are	O
sometimes	O
called	O
distributional	B
particles	I
or	O
collapsed	B
particles	I
and	O
friedman	O
sec	O
and	O
a	O
distribution	O
of	O
the	O
form	O
qs	O
the	O
advantage	O
of	O
this	O
approach	O
is	O
that	O
we	O
reduce	O
the	O
dimensionality	O
of	O
the	O
space	O
in	O
which	O
we	O
are	O
sampling	O
which	O
reduces	O
the	O
variance	B
of	O
our	O
estimate	O
hence	O
this	O
technique	O
is	O
known	O
as	O
rao-blackwellised	O
particle	O
filtering	B
or	O
rbpf	B
for	O
short	O
named	O
after	O
theorem	O
the	O
method	O
is	O
best	O
explained	O
using	O
a	O
specific	O
example	O
rbpf	B
for	O
switching	O
lg-ssms	O
a	O
canonical	O
example	O
for	O
which	O
rbpf	B
can	O
be	O
applied	O
is	O
the	O
switching	B
linear	B
dynamical	I
system	I
model	O
discussed	O
in	O
section	O
and	O
liu	O
doucet	O
et	O
al	O
we	O
can	O
represent	O
qs	O
using	O
a	O
mean	B
and	O
covariance	B
matrix	I
for	O
each	O
particle	O
s	O
where	O
qt	O
k	O
t	O
k	O
qs	O
if	O
we	O
propose	B
from	O
the	O
prior	O
qqt	O
kqs	O
t	O
ws	O
ws	O
t	O
the	O
weight	O
update	O
becomes	O
ws	O
t	O
tk	O
where	O
pytqt	O
k	O
tk	O
is	O
the	O
predictive	B
density	O
for	O
the	O
new	O
observation	B
yt	O
conditioned	O
on	O
qt	O
k	O
and	O
in	O
the	O
case	O
of	O
slds	O
models	O
this	O
can	O
be	O
computed	O
using	O
the	O
normalization	O
k	O
the	O
quantity	O
ls	O
the	O
history	O
qs	O
constant	O
of	O
the	O
kalman	O
filter	O
equation	O
ls	O
tk	O
we	O
give	O
some	O
pseudo-code	O
in	O
algorithm	O
step	O
marked	O
kfupdate	O
refers	O
to	O
the	O
kalman	O
filter	O
update	O
equations	O
in	O
section	O
this	O
is	O
known	O
as	O
a	O
mixture	B
of	O
kalman	O
filters	O
if	O
k	O
is	O
small	O
we	O
can	O
compute	O
the	O
optimal	O
proposal	B
distribution	I
which	O
is	O
pqt	O
qs	O
t	O
kyt	O
ps	O
t	O
k	O
ps	O
ps	O
ps	O
t	O
tkpqt	O
kqs	O
ls	O
t	O
pqt	O
ls	O
t	O
t	O
k	O
chapter	O
monte	B
carlo	I
inference	B
algorithm	O
one	O
step	O
of	O
rbpf	B
for	O
slds	O
using	O
prior	O
as	O
proposal	O
for	O
s	O
s	O
do	O
k	O
pqtqs	O
qs	O
t	O
k	O
t	O
s	O
s	O
ws	O
t	O
ws	O
tk	O
kfupdate	O
s	O
t	O
ls	O
t	O
ts	O
t	O
yt	O
k	O
t	O
s	O
t	O
normalize	O
weights	O
ws	O
compute	O
s	O
if	O
s	O
smin	O
then	O
eff	O
t	O
ws	O
t	O
t	O
eff	O
resample	O
s	O
indices	O
wt	O
t	O
t	O
q	O
q	O
t	O
ws	O
t	O
t	O
t	O
t	O
where	O
we	O
use	O
the	O
following	O
shorthand	O
t	O
p	O
qs	O
ps	O
we	O
then	O
sample	O
from	O
pqtqs	O
t	O
ws	O
ws	O
t	O
and	O
give	O
the	O
resulting	O
particle	O
weight	O
ws	O
t	O
tkpqt	O
kqs	O
ls	O
t	O
k	O
since	O
the	O
weights	O
of	O
the	O
particles	O
in	O
equation	O
are	O
independent	O
of	O
the	O
new	O
value	O
that	O
is	O
actually	O
sampled	O
for	O
qt	O
we	O
can	O
compute	O
these	O
weights	O
first	O
and	O
use	O
them	O
to	O
decide	O
which	O
particles	O
to	O
propagate	O
that	O
is	O
we	O
choose	O
the	O
fittest	O
particles	O
at	O
time	O
t	O
using	O
information	B
from	O
time	O
t	O
this	O
is	O
called	O
look-ahead	B
rbpf	B
freitas	O
et	O
al	O
in	O
more	O
detail	O
the	O
idea	O
is	O
this	O
we	O
pass	O
each	O
sample	O
in	O
the	O
prior	O
through	O
all	O
k	O
models	O
to	O
get	O
k	O
posteriors	O
one	O
per	O
sample	O
the	O
normalization	O
constants	O
of	O
this	O
process	O
allow	O
us	O
to	O
compute	O
the	O
optimal	O
weights	O
in	O
equation	O
we	O
then	O
resample	O
s	O
indices	O
finally	O
for	O
each	O
old	O
particle	O
s	O
that	O
is	O
chosen	O
we	O
sample	O
one	O
new	O
state	B
qs	O
t	O
k	O
and	O
use	O
the	O
corresponding	O
posterior	O
from	O
the	O
k	O
possible	O
alternative	O
that	O
we	O
have	O
already	O
computed	O
the	O
pseudo-code	O
is	O
shown	O
in	O
algorithm	O
this	O
method	O
needs	O
oks	O
storage	O
but	O
has	O
the	O
advantage	O
that	O
each	O
particle	O
is	O
chosen	O
using	O
the	O
latest	O
information	B
yt	O
a	O
further	O
improvement	O
can	O
be	O
obtained	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
state	B
space	I
is	O
discrete	B
hence	O
we	O
can	O
use	O
the	O
resampling	O
method	O
of	O
which	O
avoids	O
duplicating	O
particles	O
application	O
tracking	B
a	O
maneuvering	O
target	O
one	O
application	O
of	O
slds	O
is	O
to	O
track	O
moving	O
objects	O
that	O
have	O
piecewise	O
linear	O
dynamics	O
for	O
example	O
suppose	O
we	O
want	O
to	O
track	O
an	O
airplane	O
or	O
missile	O
qt	O
can	O
specify	O
if	O
the	O
object	O
is	O
flying	O
normally	O
or	O
is	O
taking	O
evasive	O
action	B
this	O
is	O
called	O
maneuvering	B
target	I
tracking	B
figure	O
gives	O
an	O
example	O
of	O
an	O
object	O
moving	O
in	O
the	O
setup	O
is	O
essentially	O
the	O
same	O
as	O
in	O
section	O
except	O
that	O
we	O
add	O
a	O
three-state	O
discrete	B
markov	B
chain	I
which	O
controls	O
the	O
rao-blackwellised	O
particle	O
filtering	B
algorithm	O
one	O
step	O
of	O
look-ahead	B
rbpf	B
for	O
slds	O
using	O
optimal	O
proposal	O
for	O
s	O
s	O
do	O
t	O
s	O
t	O
yt	O
k	O
tkpqtkqs	O
ls	O
ls	O
t	O
tkpqtkqs	O
t	O
ws	O
tk	O
s	O
t	O
s	O
t	O
ws	O
for	O
k	O
k	O
do	O
tk	O
lk	O
k	O
lk	O
normalize	O
weights	O
ws	O
resample	O
s	O
indices	O
wt	O
for	O
s	O
do	O
ts	O
kfupdate	O
s	O
tspqt	O
kqs	O
t	O
ws	O
t	O
t	O
compute	O
optimal	O
proposal	O
pkqs	O
sample	O
k	O
pkqs	O
t	O
k	O
s	O
qs	O
t	O
s	O
ws	O
t	O
t	O
s	O
tk	O
tk	O
s	O
method	O
misclassification	O
rate	B
mse	B
pf	O
rbpf	B
time	O
table	O
comparison	O
of	O
pf	O
an	O
rbpf	B
on	O
the	O
maneuvering	O
target	O
problem	O
in	O
figure	O
input	O
to	O
the	O
system	O
we	O
define	O
ut	O
and	O
set	O
so	O
the	O
system	O
will	O
turn	O
in	O
different	O
directions	O
depending	O
on	O
the	O
discrete	B
state	B
figure	O
shows	O
the	O
true	O
state	B
of	O
the	O
system	O
from	O
a	O
sample	O
run	O
starting	O
at	O
the	O
colored	O
symbols	O
denote	O
the	O
discrete	B
state	B
and	O
the	O
location	O
of	O
the	O
symbol	O
denotes	O
the	O
y	O
location	O
the	O
small	O
dots	O
represent	O
noisy	O
observations	O
figure	O
shows	O
the	O
estimate	O
of	O
the	O
state	B
computed	O
using	O
particle	O
filtering	B
with	O
particles	O
where	O
the	O
proposal	O
is	O
to	O
sample	O
from	O
the	O
prior	O
the	O
colored	O
symbols	O
denote	O
the	O
map	B
estimate	I
of	O
the	O
state	B
and	O
the	O
location	O
of	O
the	O
symbol	O
denotes	O
the	O
mmse	B
mean	B
square	O
error	O
estimate	O
of	O
the	O
location	O
which	O
is	O
given	O
by	O
the	O
posterior	B
mean	B
figure	O
shows	O
the	O
estimate	O
computing	O
using	O
rbpf	B
with	O
particles	O
using	O
the	O
optimal	O
proposal	B
distribution	I
a	O
more	O
quantitative	O
comparison	O
is	O
shown	O
in	O
table	O
we	O
see	O
that	O
rbpf	B
has	O
slightly	O
better	O
performance	O
although	O
it	O
is	O
also	O
slightly	O
slower	O
figure	O
visualizes	O
the	O
belief	B
state	B
of	O
the	O
system	O
in	O
we	O
show	O
the	O
distribution	O
over	O
the	O
discrete	B
states	O
we	O
see	O
that	O
the	O
particle	O
filter	O
estimate	O
of	O
the	O
belief	B
state	B
column	O
is	O
not	O
as	O
accurate	O
as	O
the	O
rbpf	B
estimate	O
column	O
in	O
the	O
beginning	O
although	O
after	O
the	O
first	O
few	O
observations	O
performance	O
is	O
similar	B
for	O
both	O
methods	O
in	O
we	O
plot	O
the	O
posterior	O
over	O
the	O
x	O
locations	O
for	O
simplicity	O
we	O
use	O
the	O
pf	O
estimate	O
which	O
is	O
a	O
set	O
of	O
weighted	O
samples	B
but	O
we	O
could	O
also	O
have	O
used	O
the	O
rbpf	B
estimate	O
which	O
is	O
a	O
set	O
of	O
weighted	O
gaussians	O
chapter	O
monte	B
carlo	I
inference	B
data	O
pf	O
mse	B
rbpf	B
mse	B
figure	O
particle	O
filter	O
estimate	O
nando	O
de	O
freitas	O
a	O
maneuvering	O
target	O
the	O
colored	O
symbols	O
represent	O
the	O
hidden	B
discrete	B
state	B
rbpf	B
estimate	O
figure	O
generated	O
by	O
rbpfmaneuverdemo	O
based	O
on	O
code	O
by	O
application	O
fast	O
slam	B
in	O
section	O
we	O
introduced	O
the	O
problem	O
of	O
simultaneous	B
localization	I
and	I
mapping	I
or	O
slam	B
for	O
mobile	O
robotics	O
the	O
main	O
problem	O
with	O
the	O
kalman	O
filter	O
implementation	O
is	O
that	O
it	O
is	O
cubic	O
in	O
the	O
number	O
of	O
landmarks	O
however	O
by	O
looking	O
at	O
the	O
dgm	B
in	O
figure	O
we	O
see	O
that	O
conditional	O
on	O
knowing	O
the	O
robot	O
s	O
path	B
where	O
qt	O
r	O
the	O
landmark	O
locations	O
z	O
r	O
assume	O
the	O
landmarks	O
don	O
t	O
move	O
so	O
we	O
drop	O
the	O
t	O
subscript	O
that	O
is	O
are	O
independent	O
consequently	O
we	O
can	O
use	O
rbpf	B
where	O
we	O
sample	O
the	O
robot	O
s	O
trajectory	O
and	O
we	O
run	O
l	O
independent	O
kalman	O
filters	O
inside	O
each	O
particle	O
this	O
takes	O
ol	O
time	O
per	O
particle	O
fortunately	O
the	O
number	O
of	O
particles	O
needed	O
for	O
good	O
performance	O
is	O
quite	O
small	O
partly	O
depends	O
on	O
the	O
control	O
exploration	O
policy	B
so	O
the	O
algorithm	O
is	O
essentially	O
linear	O
in	O
the	O
number	O
of	O
particles	O
this	O
technique	O
has	O
the	O
additional	O
advantage	O
that	O
rao-blackwellised	O
particle	O
filtering	B
truth	O
pf	O
error	O
rate	B
rbpf	B
error	O
rate	B
pf	O
t	O
y	O
t	O
x	O
p	O
t	O
figure	O
belief	O
states	O
corresponding	O
to	O
figure	O
discrete	B
state	B
the	O
system	O
starts	O
in	O
state	B
x	O
in	O
figure	O
then	O
moves	O
to	O
state	B
in	O
figure	O
returns	O
briefly	O
to	O
state	B
then	O
switches	O
to	O
state	B
circle	O
in	O
figure	O
etc	O
horizontal	O
location	O
estimate	O
figure	O
generated	O
by	O
rbpfmaneuverdemo	O
based	O
on	O
code	O
by	O
nando	O
de	O
freitas	O
it	O
is	O
easy	O
to	O
use	O
sampling	O
to	O
handle	O
the	O
data	B
association	I
ambiguity	O
and	O
that	O
it	O
allows	O
for	O
other	O
representations	O
of	O
the	O
map	O
such	O
as	O
occupancy	O
grids	O
this	O
idea	O
was	O
first	O
suggested	O
in	O
and	O
was	O
subsequently	O
extended	O
and	O
made	O
practical	O
in	O
et	O
al	O
who	O
christened	O
the	O
technique	O
fastslam	B
see	O
rbpfslamdemo	O
for	O
a	O
simple	O
demo	O
in	O
a	O
discrete	B
grid	O
world	O
exercises	O
exercise	O
sampling	O
from	O
a	O
cauchy	B
show	O
how	O
to	O
use	O
inverse	B
probability	I
transform	I
to	O
sample	O
from	O
a	O
standard	O
cauchy	B
t	O
exercise	O
rejection	B
sampling	I
from	O
a	O
gamma	B
using	O
a	O
cauchy	B
proposal	O
show	O
how	O
to	O
use	O
a	O
cauchy	B
proposal	O
to	O
perform	O
rejection	B
sampling	I
from	O
a	O
gamma	B
distribution	I
derive	O
the	O
optimal	O
constant	O
m	O
and	O
plot	O
the	O
density	O
and	O
its	O
upper	O
envelope	O
exercise	O
optimal	O
proposal	O
for	O
particle	O
filtering	B
with	O
linear-gaussian	O
measurement	O
model	O
consider	O
a	O
state-space	O
model	O
of	O
the	O
following	O
form	O
zt	O
ftzt	O
n	O
qt	O
yt	O
htzt	O
n	O
rt	O
derive	O
expressions	O
for	O
pztzt	O
yt	O
and	O
pytzt	O
which	O
are	O
needed	O
to	O
compute	O
the	O
optimal	O
variance	B
proposal	B
distribution	I
hint	O
use	O
bayes	B
rule	I
for	O
gaussians	O
markov	B
chain	I
monte	B
carlo	I
inference	B
introduction	O
in	O
chapter	O
we	O
introduced	O
some	O
simple	O
monte	B
carlo	I
methods	O
including	O
rejection	B
sampling	I
and	O
importance	B
sampling	I
the	O
trouble	O
with	O
these	O
methods	O
is	O
that	O
they	O
do	O
not	O
work	O
well	O
in	O
high	O
dimensional	O
spaces	O
the	O
most	O
popular	O
method	O
for	O
sampling	O
from	O
high-dimensional	O
distributions	O
is	O
markov	B
chain	I
monte	B
carlo	I
or	O
mcmc	B
in	O
a	O
survey	O
bysiam	O
news	O
mcmc	B
was	O
placed	O
in	O
the	O
top	O
most	O
important	O
algorithms	O
of	O
the	O
century	O
the	O
basic	O
idea	O
behind	O
mcmc	B
is	O
to	O
construct	O
a	O
markov	B
chain	I
on	O
the	O
state	B
space	I
x	O
whose	O
stationary	B
distribution	I
is	O
the	O
target	O
density	O
p	O
of	O
interest	O
may	O
be	O
a	O
prior	O
or	O
a	O
posterior	O
that	O
is	O
we	O
perform	O
a	O
random	O
walk	O
on	O
the	O
state	B
space	I
in	O
such	O
a	O
way	O
that	O
the	O
fraction	O
of	O
time	O
we	O
spend	O
in	O
each	O
state	B
x	O
is	O
proportional	O
to	O
p	O
by	O
drawing	O
samples	B
from	O
the	O
chain	O
we	O
can	O
perform	O
monte	B
carlo	I
integration	I
wrt	O
p	O
we	O
give	O
the	O
details	O
below	O
the	O
mcmc	B
algorithm	O
has	O
an	O
interesting	O
history	O
it	O
was	O
discovered	O
by	O
physicists	O
working	O
on	O
the	O
atomic	B
bomb	I
at	O
los	O
alamos	O
during	O
world	O
war	O
ii	O
and	O
was	O
first	O
published	O
in	O
the	O
open	O
literature	O
in	O
et	O
al	O
in	O
a	O
chemistry	O
journal	O
an	O
extension	B
was	O
published	O
in	O
the	O
statistics	O
literature	O
in	O
but	O
was	O
largely	O
unnoticed	O
a	O
special	O
case	O
sampling	O
section	O
was	O
independently	O
invented	O
in	O
in	O
the	O
context	O
of	O
ising	O
models	O
and	O
was	O
published	O
in	O
and	O
geman	O
but	O
it	O
was	O
not	O
until	O
and	O
smith	O
that	O
the	O
algorithm	O
became	O
well-known	O
to	O
the	O
wider	O
statistical	O
community	O
since	O
then	O
it	O
has	O
become	O
wildly	O
popular	O
in	O
bayesian	B
statistics	I
and	O
is	O
becoming	O
increasingly	O
popular	O
in	O
machine	B
learning	B
it	O
is	O
worth	O
briefly	O
comparing	O
mcmc	B
to	O
variational	B
inference	B
the	O
advantages	O
of	O
variational	O
it	O
is	O
deterministic	O
is	O
it	O
easy	O
to	O
determine	O
when	O
to	O
stop	O
it	O
often	O
provides	O
a	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	B
the	O
advantages	O
of	O
sampling	O
are	O
it	O
is	O
often	O
easier	O
to	O
implement	O
it	O
is	O
applicable	O
to	O
a	O
broader	O
range	O
of	O
models	O
such	O
as	O
models	O
whose	O
size	O
or	O
structure	O
changes	O
depending	O
on	O
the	O
values	O
of	O
certain	O
variables	O
as	O
happens	O
in	O
matching	O
problems	O
or	O
models	O
without	O
nice	O
conjugate	B
priors	I
sampling	O
can	O
be	O
faster	O
than	O
variational	O
methods	O
when	O
applied	O
to	O
really	O
huge	O
models	O
or	O
inference	B
are	O
for	O
small	O
to	O
medium	O
problems	O
it	O
is	O
usually	O
faster	O
source	O
the	O
reason	O
is	O
that	O
sampling	O
passes	O
specific	O
values	O
of	O
variables	O
sets	O
of	O
variables	O
whereas	O
in	O
variational	B
inference	B
we	O
pass	O
around	O
distributions	O
thus	O
sampling	O
passes	O
sparse	B
messages	O
whereas	O
variational	B
inference	B
passes	O
dense	O
messages	O
for	O
comparisons	O
of	O
the	O
two	O
approaches	O
see	O
e	O
g	O
and	O
west	O
and	O
articles	O
in	O
et	O
al	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
gibbs	B
sampling	I
in	O
this	O
section	O
we	O
present	O
one	O
of	O
the	O
most	O
popular	O
mcmc	B
algorithms	O
known	O
as	O
gibbs	O
physics	O
this	O
method	O
is	O
known	O
as	O
glauber	B
dynamics	I
or	O
the	O
heat	B
bath	I
method	O
this	O
is	O
the	O
mcmc	B
analog	O
of	O
coordinate	O
descent	O
basic	O
idea	O
the	O
idea	O
behind	O
gibbs	B
sampling	I
is	O
that	O
we	O
sample	O
each	O
variable	O
in	O
turn	O
conditioned	O
on	O
the	O
values	O
of	O
all	O
the	O
other	O
variables	O
in	O
the	O
distribution	O
that	O
is	O
given	O
a	O
joint	O
sample	O
xs	O
of	O
all	O
the	O
variables	O
we	O
generate	O
a	O
new	O
sample	O
by	O
sampling	O
each	O
component	O
in	O
turn	O
based	O
on	O
the	O
most	O
recent	O
values	O
of	O
the	O
other	O
variables	O
for	O
example	O
if	O
we	O
have	O
d	O
variables	O
we	O
use	O
xs	O
xs	O
this	O
readily	O
generalizes	O
to	O
d	O
variables	O
if	O
xi	O
is	O
a	O
visible	B
variable	O
we	O
do	O
not	O
sample	O
it	O
since	O
its	O
value	O
is	O
already	O
known	O
the	O
expression	O
pxix	O
i	O
is	O
called	O
the	O
full	B
conditional	I
for	O
variable	O
i	O
in	O
general	O
xi	O
may	O
only	O
depend	O
on	O
some	O
of	O
the	O
other	O
variables	O
if	O
we	O
represent	O
px	O
as	O
a	O
graphical	B
model	I
we	O
can	O
infer	O
the	O
dependencies	O
by	O
looking	O
at	O
i	O
s	O
markov	B
blanket	I
which	O
are	O
its	O
neighbors	B
in	O
the	O
graph	B
thus	O
to	O
sample	O
xi	O
we	O
only	O
need	O
to	O
know	O
the	O
values	O
of	O
i	O
s	O
neighbors	B
in	O
this	O
sense	O
gibbs	B
sampling	I
is	O
a	O
distributed	O
algorithm	O
however	O
it	O
is	O
not	O
a	O
parallel	O
algorithm	O
since	O
the	O
samples	B
must	O
be	O
generated	O
sequentially	O
for	O
reasons	O
that	O
we	O
will	O
explain	O
in	O
section	O
it	O
is	O
necessary	O
to	O
discard	O
some	O
of	O
the	O
initial	O
samples	B
until	O
the	O
markov	B
chain	I
has	O
burned	B
in	I
or	O
entered	O
its	O
stationary	B
distribution	I
we	O
discuss	O
how	O
to	O
estimate	O
when	O
burnin	O
has	O
occured	O
in	O
section	O
in	O
the	O
examples	O
below	O
we	O
just	O
discard	O
the	O
initial	O
of	O
the	O
samples	B
for	O
simplicity	O
example	O
gibbs	B
sampling	I
for	O
the	O
ising	B
model	I
in	O
section	O
we	O
applied	O
mean	B
field	O
to	O
an	O
ising	B
model	I
here	O
we	O
apply	O
gibbs	B
sampling	I
gibbs	B
sampling	I
in	O
pairwise	O
mrfcrf	O
takes	O
the	O
form	O
pxtx	O
t	O
stxs	O
xt	O
s	O
nbrt	O
in	O
the	O
case	O
of	O
an	O
ising	B
model	I
with	O
edge	O
potentials	O
xt	O
expjxsxt	O
where	O
xt	O
josiah	O
willard	O
gibbs	O
was	O
an	O
american	O
physicist	O
gibbs	B
sampling	I
sample	O
gibbs	O
sample	O
gibbs	O
mean	B
after	O
sweeps	O
of	O
gibbs	O
figure	O
example	O
of	O
image	B
denoising	I
we	O
use	O
an	O
ising	O
prior	O
with	O
wij	O
j	O
and	O
a	O
gaussian	B
noise	O
model	O
with	O
we	O
use	O
gibbs	B
sampling	I
to	O
perform	O
approximate	B
inference	B
sample	O
from	O
the	O
posterior	O
after	O
one	O
sweep	O
over	O
the	O
image	O
sample	O
after	O
sweeps	O
posterior	B
mean	B
computed	O
by	O
averaging	O
over	O
sweeps	O
compare	O
to	O
figure	O
which	O
shows	O
the	O
results	O
of	O
using	O
mean	B
field	O
inference	B
figure	O
generated	O
by	O
isingimagedenoisedemo	O
the	O
full	B
conditional	I
becomes	O
pxt	O
t	O
s	O
nbrt	O
stxt	O
xs	O
s	O
nbrt	O
xs	O
s	O
nbrt	O
xs	O
s	O
nbrt	O
xs	O
expj	O
expj	O
s	O
nbrt	O
xs	O
s	O
nbrt	O
xs	O
exp	O
j	O
expj	O
t	O
expj	O
t	O
exp	O
j	O
t	O
t	O
s	O
nbrt	O
xt	O
and	O
sigmu	O
e	O
u	O
is	O
the	O
sigmoid	B
where	O
j	O
is	O
the	O
coupling	O
strength	O
t	O
function	O
it	O
is	O
easy	O
to	O
see	O
that	O
t	O
xtat	O
dt	O
where	O
at	O
is	O
the	O
number	O
of	O
neighbors	B
that	O
agree	O
with	O
the	O
same	O
sign	O
as	O
t	O
and	O
dt	O
is	O
the	O
number	O
of	O
neighbors	B
who	O
disagree	O
if	O
this	O
number	O
is	O
equal	O
the	O
forces	O
on	O
xt	O
cancel	O
out	O
so	O
the	O
full	B
conditional	I
is	O
uniform	O
observation	B
model	I
we	O
have	O
txt	O
n	O
the	O
full	B
conditional	I
becomes	O
we	O
can	O
combine	O
an	O
ising	O
prior	O
with	O
a	O
local	B
evidence	B
term	O
t	O
for	O
example	O
with	O
a	O
gaussian	B
pxt	O
t	O
y	O
expj	O
t	O
expj	O
t	O
exp	O
j	O
t	O
t	O
t	O
log	O
t	O
sigm	O
now	O
the	O
probability	O
of	O
xt	O
entering	O
each	O
state	B
is	O
determined	O
both	O
by	O
compatibility	O
with	O
its	O
neighbors	B
ising	O
prior	O
and	O
compatibility	O
with	O
the	O
data	O
local	O
likelihood	B
term	O
see	O
figure	O
for	O
an	O
example	O
of	O
this	O
algorithm	O
applied	O
to	O
a	O
simple	O
image	B
denoising	I
problem	O
the	O
results	O
are	O
similar	B
to	O
mean	B
field	O
except	O
that	O
the	O
final	O
estimate	O
on	O
averaging	O
the	O
samples	B
is	O
somewhat	O
blurrier	O
due	O
to	O
the	O
fact	O
that	O
mean	B
field	O
tends	O
to	O
be	O
over-confident	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
example	O
gibbs	B
sampling	I
for	O
inferring	O
the	O
parameters	O
of	O
a	O
gmm	B
it	O
is	O
straightforward	O
to	O
derive	O
a	O
gibbs	B
sampling	I
algorithm	O
to	O
fit	O
a	O
mixture	B
model	I
especially	O
if	O
we	O
use	O
conjugate	B
priors	I
we	O
will	O
focus	O
on	O
the	O
case	O
of	O
mixture	B
of	I
gaussians	I
although	O
the	O
results	O
are	O
easily	O
extended	O
to	O
other	O
kinds	O
of	O
mixture	B
models	O
derivation	O
which	O
follows	O
from	O
the	O
results	O
of	O
section	O
is	O
much	O
easier	O
than	O
the	O
corresponding	O
variational	B
bayes	I
algorithm	O
in	O
section	O
suppose	O
we	O
use	O
a	O
semi-conjugate	B
prior	O
then	O
the	O
full	B
joint	B
distribution	I
is	O
given	O
by	O
px	O
z	O
xz	O
p	O
kp	O
k	O
kn	O
k	O
kizik	O
dir	O
n	O
we	O
use	O
the	O
same	O
prior	O
for	O
each	O
mixture	B
component	O
the	O
full	B
conditionals	O
are	O
as	O
follows	O
for	O
the	O
discrete	B
indicators	O
we	O
have	O
pzi	O
kxi	O
kn	O
k	O
k	O
for	O
the	O
mixing	B
weights	I
we	O
have	O
results	O
from	O
section	O
p	O
dir	O
k	O
izi	O
kk	O
for	O
the	O
means	O
we	O
have	O
results	O
from	O
section	O
k	O
p	O
k	O
k	O
z	O
x	O
kmk	O
vk	O
nk	O
v	O
v	O
mk	O
vk	O
nk	O
izi	O
k	O
xk	O
izi	O
kxi	O
k	O
k	O
nkxk	O
v	O
nk	O
for	O
the	O
covariances	O
we	O
have	O
results	O
from	O
section	O
p	O
k	O
k	O
z	O
x	O
iw	O
ksk	O
k	O
izi	O
kxi	O
kxi	O
kt	O
sk	O
k	O
nk	O
see	O
gaussmissingfitgibbs	O
for	O
some	O
matlab	O
code	O
values	O
for	O
x	O
if	O
necessary	O
code	O
can	O
also	O
sample	O
missing	B
gibbs	B
sampling	I
label	B
switching	I
although	O
it	O
is	O
simple	O
to	O
implement	O
gibbs	B
sampling	I
for	O
mixture	B
models	O
has	O
a	O
fundamental	O
weakness	O
the	O
problem	O
is	O
that	O
the	O
parameters	O
of	O
the	O
model	O
and	O
the	O
indicator	O
functions	O
z	O
are	O
unidentifiable	O
since	O
we	O
can	O
arbitrarily	O
permute	O
the	O
hidden	B
labels	O
without	O
affecting	O
the	O
likelihood	B
section	O
consequently	O
we	O
cannot	O
just	O
take	O
a	O
monte	B
carlo	I
average	O
of	O
the	O
samples	B
to	O
compute	O
posterior	O
means	O
since	O
what	O
one	O
sample	O
considers	O
the	O
parameters	O
for	O
cluster	O
may	O
be	O
what	O
another	O
sample	O
considers	O
the	O
parameters	O
for	O
cluster	O
indeed	O
if	O
we	O
could	O
average	O
over	O
all	O
modes	O
we	O
would	O
find	O
e	O
kd	O
is	O
the	O
same	O
for	O
all	O
k	O
a	O
symmetric	B
prior	O
this	O
is	O
called	O
the	O
label	B
switching	I
problem	O
this	O
problem	O
does	O
not	O
arise	O
in	O
em	B
or	O
vbem	B
which	O
just	O
lock	O
on	O
to	O
a	O
single	O
mode	B
however	O
it	O
arises	O
in	O
any	O
method	O
that	O
visits	O
multiple	O
modes	O
in	O
problems	O
one	O
can	O
try	O
to	O
prevent	O
this	O
problem	O
by	O
introducing	O
constraints	O
on	O
the	O
parameters	O
to	O
ensure	O
identifiability	O
e	O
g	O
and	O
green	O
however	O
this	O
does	O
not	O
always	O
work	O
since	O
the	O
likelihood	B
might	O
overwhelm	O
the	O
prior	O
and	O
cause	O
label	B
switching	I
anyway	O
furthermore	O
this	O
technique	O
does	O
not	O
scale	O
to	O
higher	O
dimensions	O
another	O
approach	O
is	O
to	O
post-process	O
the	O
samples	B
by	O
searching	O
for	O
a	O
global	O
label	B
permutation	O
to	O
apply	O
to	O
each	O
sample	O
that	O
minimizes	O
some	O
loss	B
function	I
however	O
this	O
can	O
be	O
slow	O
perhaps	O
the	O
best	O
solution	O
is	O
simply	O
to	O
not	O
ask	O
questions	O
that	O
cannot	O
be	O
uniquely	O
identified	O
for	O
example	O
instead	O
of	O
asking	O
for	O
the	O
probability	O
that	O
data	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
ask	O
for	O
the	O
probability	O
that	O
data	O
points	O
i	O
and	O
j	O
belong	O
to	O
the	O
same	O
cluster	O
the	O
latter	O
question	O
is	O
invariant	B
to	O
the	O
labeling	O
furthermore	O
it	O
only	O
refers	O
to	O
observable	O
quantities	O
i	O
and	O
j	O
grouped	O
together	O
or	O
not	O
rather	O
than	O
referring	O
to	O
unobservable	O
quantities	O
such	O
as	O
latent	B
clusters	B
this	O
approach	O
has	O
the	O
further	O
advantage	O
that	O
it	O
extends	O
to	O
infinite	O
mixture	B
models	O
discussed	O
in	O
section	O
where	O
k	O
is	O
unbounded	O
in	O
such	O
models	O
the	O
notion	O
of	O
a	O
hidden	B
cluster	O
is	O
not	O
well	O
defined	O
but	O
the	O
notion	O
of	O
a	O
partitioning	B
of	O
the	O
data	O
is	O
well	O
defined	O
collapsed	B
gibbs	B
sampling	I
in	O
some	O
cases	O
we	O
can	O
analytically	O
integrate	B
out	I
some	O
of	O
the	O
unknown	B
quantities	O
and	O
just	O
sample	O
the	O
rest	O
this	O
is	O
called	O
a	O
collapsed	B
gibbs	B
sampler	I
and	O
it	O
tends	O
to	O
be	O
much	O
more	O
efficient	O
since	O
it	O
is	O
sampling	O
in	O
a	O
lower	O
dimensional	O
space	O
more	O
precisely	O
suppose	O
we	O
sample	O
z	O
and	O
integrate	B
out	I
thus	O
the	O
parameters	O
do	O
not	O
participate	O
in	O
the	O
markov	B
chain	I
consequently	O
we	O
can	O
draw	O
conditionally	B
independent	I
samples	B
s	O
p	O
which	O
will	O
have	O
much	O
lower	O
variance	B
than	O
samples	B
drawn	O
from	O
the	O
joint	O
state	B
space	I
et	O
al	O
this	O
process	O
is	O
called	O
rao-blackwellisation	B
named	O
after	O
the	O
following	O
theorem	O
theorem	O
let	O
z	O
and	O
be	O
dependent	O
random	O
variables	O
and	O
f	O
be	O
some	O
scalar	O
function	O
then	O
varz	O
varz	O
this	O
theorem	O
guarantees	O
that	O
the	O
variance	B
of	O
the	O
estimate	O
created	O
by	O
analytically	O
integrating	O
out	O
will	O
always	O
be	O
lower	O
rather	O
will	O
never	O
be	O
higher	O
than	O
the	O
variance	B
of	O
a	O
direct	O
mc	O
in	O
collapsed	O
gibbs	O
we	O
sample	O
z	O
with	O
integrated	O
out	O
the	O
above	O
rao-blackwell	B
estimate	O
theorem	O
still	O
applies	O
in	O
this	O
case	O
et	O
al	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
zn	O
xn	O
zi	O
xi	O
k	O
zn	O
xn	O
zi	O
xi	O
figure	O
a	O
mixture	B
model	I
after	O
integrating	O
out	O
the	O
parameters	O
we	O
will	O
encounter	O
rao-blackwellisation	B
again	O
in	O
section	O
although	O
it	O
can	O
reduce	O
statistical	O
variance	B
it	O
is	O
only	O
worth	O
doing	O
if	O
the	O
integrating	O
out	O
can	O
be	O
done	O
quickly	O
otherwise	O
we	O
will	O
not	O
be	O
able	O
to	O
produce	O
as	O
many	O
samples	B
per	O
second	O
as	O
the	O
naive	O
method	O
we	O
give	O
an	O
example	O
of	O
this	O
below	O
example	O
collapsed	O
gibbs	O
for	O
fitting	O
a	O
gmm	B
consider	O
a	O
gmm	B
with	O
a	O
fully	O
conjugate	B
prior	I
in	O
this	O
case	O
we	O
can	O
analytically	O
integrate	B
out	I
the	O
model	O
parameters	O
k	O
k	O
and	O
and	O
just	O
sample	O
the	O
indicators	O
z	O
once	O
we	O
integrate	B
out	I
all	O
the	O
zi	O
nodes	B
become	O
inter-dependent	O
similarly	O
once	O
we	O
integrate	B
out	I
k	O
all	O
the	O
xi	O
nodes	B
become	O
inter-dependent	O
as	O
shown	O
in	O
figure	O
nevertheless	O
we	O
can	O
easily	O
compute	O
the	O
full	B
conditionals	O
as	O
follows	O
pzi	O
kz	O
i	O
x	O
pzi	O
kz	O
i	O
k	O
z	O
i	O
pzi	O
kz	O
i	O
i	O
zi	O
k	O
z	O
i	O
px	O
izi	O
k	O
z	O
i	O
pzi	O
kz	O
i	O
i	O
zi	O
k	O
z	O
i	O
where	O
are	O
the	O
hyper-parameters	B
for	O
the	O
class-conditional	O
densities	O
the	O
first	O
term	O
can	O
be	O
obtained	O
by	O
integrating	O
out	O
suppose	O
we	O
use	O
a	O
symmetric	B
prior	O
of	O
the	O
form	O
dir	O
where	O
k	O
from	O
equation	O
we	O
have	O
zn	O
gibbs	B
sampling	I
hence	O
pzi	O
kz	O
i	O
where	O
nk	O
i	O
x	O
pz	O
i	O
i	O
i	O
izn	O
k	O
k	O
and	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
i	O
nk	O
i	O
n	O
to	O
obtain	O
the	O
second	O
term	O
in	O
equation	O
which	O
is	O
the	O
posterior	B
predictive	B
distribution	I
for	O
xi	O
given	O
all	O
the	O
other	O
data	O
and	O
all	O
the	O
assignments	O
we	O
use	O
the	O
fact	O
that	O
pxix	O
i	O
z	O
i	O
zi	O
k	O
pxid	O
ik	O
where	O
d	O
ik	O
zj	O
k	O
j	O
i	O
is	O
all	O
the	O
data	O
assigned	O
to	O
cluster	O
k	O
except	O
for	O
xi	O
if	O
we	O
use	O
a	O
conjugate	B
prior	I
for	O
k	O
we	O
can	O
compute	O
pxid	O
ik	O
in	O
closed	O
form	O
furthermore	O
we	O
can	O
efficiently	O
update	O
these	O
predictive	B
likelihoods	O
by	O
caching	O
the	O
sufficient	B
statistics	I
for	O
each	O
cluster	O
to	O
compute	O
the	O
above	O
expression	O
we	O
remove	O
xi	O
s	O
statistics	O
from	O
its	O
current	O
cluster	O
zi	O
and	O
then	O
evaluate	O
xi	O
under	O
each	O
cluster	O
s	O
posterior	O
predictive	B
once	O
we	O
have	O
picked	O
a	O
new	O
cluster	O
we	O
add	O
xi	O
s	O
statistics	O
to	O
this	O
new	O
cluster	O
some	O
pseudo-code	O
for	O
one	O
step	O
of	O
the	O
algorithm	O
is	O
shown	O
in	O
algorithm	O
based	O
on	O
update	O
the	O
nodes	B
in	O
random	O
order	O
to	O
improve	O
the	O
mixing	B
time	I
as	O
suggested	O
in	O
and	O
sahu	O
we	O
can	O
initialize	O
the	O
sample	O
by	O
sequentially	O
sampling	O
from	O
fmgibbs	O
for	O
some	O
matlab	O
code	O
by	O
yee-whye	O
teh	O
in	O
the	O
case	O
of	O
gmms	O
both	O
the	O
naive	O
sampler	O
and	O
collapsed	O
sampler	O
take	O
on	O
kd	O
time	O
per	O
step	O
algorithm	O
collapsed	B
gibbs	B
sampler	I
for	O
a	O
mixture	B
model	I
for	O
each	O
i	O
in	O
random	O
order	O
do	O
remove	O
xi	O
s	O
sufficient	B
statistics	I
from	O
old	O
cluster	O
zi	O
for	O
each	O
k	O
do	O
compute	O
pkxi	O
pxixj	O
zj	O
k	O
j	O
i	O
compute	O
pzi	O
kz	O
id	O
i	O
sample	O
zi	O
pzi	O
add	O
xi	O
s	O
sufficient	B
statistics	I
to	O
new	O
cluster	O
zi	O
a	O
comparison	O
of	O
this	O
method	O
with	O
the	O
standard	O
gibbs	B
sampler	I
is	O
shown	O
in	O
figure	O
the	O
vertical	O
axis	O
is	O
the	O
data	O
log	O
probability	O
at	O
each	O
iteration	O
computed	O
using	O
log	O
pdz	O
log	O
zi	O
pxi	O
zi	O
to	O
compute	O
this	O
quantity	O
using	O
the	O
collapsed	O
sampler	O
we	O
have	O
to	O
sample	O
given	O
the	O
data	O
and	O
the	O
current	O
assignment	O
z	O
in	O
figure	O
we	O
see	O
that	O
the	O
collapsed	O
sampler	O
does	O
indeed	O
generally	O
work	O
better	O
than	O
the	O
vanilla	O
sampler	O
occasionally	O
however	O
both	O
methods	O
can	O
get	O
stuck	O
in	O
poor	O
local	O
modes	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
x	O
p	O
g	O
o	O
l	O
standard	O
gibbs	B
sampler	I
rao	O
blackwellized	O
sampler	O
iteration	O
x	O
p	O
g	O
o	O
l	O
standard	O
gibbs	B
sampler	I
rao	O
blackwellized	O
sampler	O
iteration	O
figure	O
comparison	O
of	O
collapsed	O
and	O
vanilla	O
gibbs	B
sampling	I
for	O
a	O
mixture	B
of	O
k	O
twodimensional	O
gaussians	O
applied	O
to	O
n	O
data	O
points	O
in	O
figure	O
we	O
plot	O
log	O
probability	O
of	O
the	O
data	O
vs	O
iteration	O
different	O
random	O
initializations	O
logprob	O
averaged	O
over	O
different	O
random	O
initializations	O
solid	O
line	O
is	O
the	O
median	B
thick	O
dashed	O
in	O
the	O
and	O
quantiles	O
and	O
thin	O
dashed	O
are	O
the	O
and	O
quintiles	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
e	O
r	O
o	O
c	O
s	O
h	O
t	O
a	O
m	O
e	O
p	O
o	O
s	O
l	O
ses	O
sample	O
size	O
e	O
r	O
o	O
c	O
s	O
h	O
t	O
a	O
m	O
ses	O
figure	O
least	B
squares	I
regression	B
lines	O
for	O
math	O
scores	B
vs	O
socio-economic	O
status	O
for	O
schools	O
plot	O
of	O
slope	O
vs	O
nj	O
size	O
for	O
the	O
population	O
mean	B
estimate	O
is	O
in	O
bold	O
schools	O
the	O
extreme	O
slopes	O
tend	O
to	O
correspond	O
to	O
schools	O
with	O
smaller	O
sample	O
sizes	O
predictions	O
from	O
the	O
hierarchical	O
model	O
population	O
mean	B
is	O
in	O
bold	O
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
multilevellinregdemo	O
written	O
by	O
emtiyaz	O
khan	O
that	O
the	O
error	O
bars	O
in	O
figure	O
are	O
averaged	O
over	O
starting	O
values	O
whereas	O
the	O
theorem	O
refers	O
to	O
mc	O
samples	B
in	O
a	O
single	O
run	O
gibbs	B
sampling	I
for	O
hierarchical	O
glms	O
often	O
we	O
have	O
data	O
from	O
multiple	O
related	O
sources	O
if	O
some	O
sources	O
are	O
more	O
reliable	O
andor	O
data-rich	O
than	O
others	O
it	O
makes	O
sense	O
to	O
model	O
all	O
the	O
data	O
simultaneously	O
so	O
as	O
to	O
enable	O
the	O
borrowing	O
of	O
statistical	O
strength	O
one	O
of	O
the	O
most	O
natural	O
way	O
to	O
solve	O
such	O
problems	O
is	O
to	O
use	O
hierarchical	O
bayesian	B
modeling	O
also	O
called	O
multi-level	B
modeling	I
in	O
section	O
we	O
discussed	O
a	O
way	O
to	O
perform	O
approximate	B
inference	B
in	O
such	O
models	O
using	O
variational	O
methods	O
here	O
we	O
discuss	O
how	O
to	O
use	O
gibbs	B
sampling	I
to	O
explain	O
the	O
method	O
consider	O
the	O
following	O
example	O
suppose	O
we	O
have	O
data	O
on	O
students	O
gibbs	B
sampling	I
w	O
w	O
wj	O
yij	O
xij	O
nj	O
j	O
figure	O
multi-level	B
model	I
for	O
linear	B
regression	B
in	O
different	O
schools	O
such	O
data	O
is	O
naturally	O
modeled	O
in	O
a	O
two-level	O
hierarchy	O
we	O
let	O
yij	O
be	O
the	O
response	B
variable	I
we	O
want	O
to	O
predict	O
for	O
student	O
i	O
in	O
school	O
j	O
this	O
prediction	O
can	O
be	O
based	O
on	O
school	O
and	O
student	O
specific	O
covariates	B
xij	O
since	O
the	O
quality	O
of	O
schools	O
varies	O
we	O
want	O
to	O
use	O
a	O
separate	O
parameter	B
for	O
each	O
school	O
so	O
our	O
model	O
becomes	O
yij	O
xt	O
ijwj	O
we	O
will	O
illustrate	O
this	O
model	O
below	O
using	O
a	O
dataset	O
from	O
where	O
xij	O
is	O
the	O
socio-economic	O
status	O
of	O
student	O
i	O
in	O
school	O
y	O
and	O
yij	O
is	O
their	O
math	O
score	O
we	O
could	O
fit	O
each	O
wj	O
separately	O
but	O
this	O
can	O
give	O
poor	O
results	O
if	O
the	O
sample	O
size	O
of	O
a	O
given	O
school	O
is	O
small	O
this	O
is	O
illustrated	O
in	O
figure	O
which	O
plots	O
the	O
least	B
squares	I
regression	B
line	O
estimated	O
separately	O
for	O
each	O
of	O
the	O
j	O
schools	O
we	O
see	O
that	O
most	O
of	O
the	O
slopes	O
are	O
positive	O
but	O
there	O
are	O
a	O
few	O
errant	O
cases	O
where	O
the	O
slope	O
is	O
negative	O
it	O
turns	O
out	O
that	O
the	O
lines	O
with	O
extreme	O
slopes	O
tend	O
to	O
be	O
in	O
schools	O
with	O
small	O
sample	O
size	O
as	O
shown	O
in	O
figure	O
thus	O
we	O
may	O
not	O
necessarily	O
trust	O
these	O
fits	O
we	O
can	O
get	O
better	O
results	O
if	O
we	O
construct	O
a	O
hierarchical	B
bayesian	B
model	I
in	O
which	O
the	O
wj	O
are	O
assumed	O
to	O
come	O
from	O
a	O
common	O
prior	O
wj	O
n	O
w	O
w	O
this	O
is	O
illustrated	O
in	O
figure	O
in	O
this	O
model	O
the	O
schools	O
with	O
small	O
sample	O
size	O
borrow	B
statistical	I
strength	I
from	O
the	O
schools	O
with	O
larger	O
sample	O
size	O
because	O
the	O
wj	O
s	O
are	O
correlated	O
via	O
the	O
latent	B
common	O
parents	B
w	O
w	O
is	O
crucial	O
that	O
these	O
hyper-parameters	B
be	O
inferrred	O
from	O
data	O
if	O
they	O
were	O
fixed	O
constants	O
the	O
wj	O
would	O
be	O
conditionally	B
independent	I
and	O
there	O
would	O
be	O
no	O
information	B
sharing	O
between	O
them	O
to	O
complete	B
the	O
model	O
specification	O
we	O
must	O
specify	O
priors	O
for	O
the	O
shared	B
parameters	O
fol	O
lowing	O
we	O
will	O
use	O
the	O
following	O
semi-conjugate	B
forms	O
for	O
convenience	O
w	O
n	O
w	O
iw	O
s	O
ig	O
given	O
this	O
it	O
is	O
simple	O
to	O
show	O
that	O
the	O
full	B
conditionals	O
needed	O
for	O
gibbs	B
sampling	I
have	O
the	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
following	O
forms	O
for	O
the	O
group-specific	O
weights	O
pwjdj	O
j	O
j	O
j	O
xt	O
j	O
xj	O
j	O
j	O
xt	O
j	O
yj	O
for	O
the	O
overall	O
mean	B
p	O
w	O
n	O
n	O
j	O
n	O
v	O
n	O
n	O
j	O
wj	O
for	O
the	O
overall	O
covariance	B
j	O
where	O
w	O
j	O
p	O
w	O
w	O
s	O
j	O
wwj	O
wt	O
s	O
for	O
the	O
noise	O
variance	B
j	O
p	O
ig	O
n	O
wt	O
j	O
applying	O
gibbs	B
sampling	I
to	O
our	O
hierarchical	O
model	O
we	O
get	O
the	O
results	O
shown	O
in	O
figure	O
the	O
light	O
gray	O
lines	O
plot	O
the	O
mean	B
of	O
the	O
posterior	B
predictive	B
distribution	I
for	O
each	O
school	O
e	O
xt	O
ij	O
wj	O
where	O
wj	O
e	O
s	O
ws	O
j	O
the	O
dark	O
gray	O
line	O
in	O
the	O
middle	O
plots	O
the	O
prediction	O
using	O
the	O
overall	O
mean	B
parameters	O
xt	O
ij	O
w	O
we	O
see	O
that	O
the	O
method	O
has	O
regularized	O
the	O
fits	O
quite	O
nicely	O
without	O
enforcing	O
too	O
much	O
amount	O
of	O
shrinkage	B
is	O
controlled	O
by	O
w	O
which	O
in	O
turns	O
depends	O
on	O
the	O
uniformity	O
hyper-parameters	B
in	O
this	O
example	O
we	O
used	O
vague	O
values	O
bugs	B
and	O
jags	B
one	O
reason	O
gibbs	B
sampling	I
is	O
so	O
popular	O
is	O
that	O
it	O
is	O
possible	O
to	O
design	O
general	O
purpose	O
software	O
that	O
will	O
work	O
for	O
almost	O
any	O
model	O
this	O
software	O
just	O
needs	O
a	O
model	O
specification	O
usually	O
in	O
the	O
form	O
a	O
directed	B
graphical	B
model	I
in	O
a	O
file	O
or	O
created	O
with	O
a	O
graphical	O
user	O
interface	O
and	O
a	O
library	O
of	O
methods	O
for	O
sampling	O
from	O
different	O
kinds	O
of	O
full	B
conditionals	O
can	O
often	O
be	O
done	O
using	O
adaptive	B
rejection	B
sampling	I
described	O
in	O
section	O
an	O
example	O
gibbs	B
sampling	I
of	O
such	O
a	O
package	O
is	O
bugs	B
et	O
al	O
which	O
stands	O
for	O
bayesian	B
updating	O
using	O
gibbs	B
sampling	I
bugs	B
is	O
very	O
widely	O
used	O
in	O
biostatistics	O
and	O
social	O
science	O
another	O
more	O
recent	O
but	O
very	O
similar	B
package	O
is	O
jags	B
which	O
stands	O
for	O
just	O
another	O
gibbs	B
sampler	I
this	O
uses	O
a	O
similar	B
model	O
specification	O
language	O
to	O
bugs	B
for	O
example	O
we	O
can	O
describe	O
the	O
model	O
in	O
figure	O
as	O
follows	O
model	O
for	O
in	O
for	O
in	O
yij	O
dnormy	O
hatij	O
tau	O
y	O
y	O
hatij	O
inprodwj	O
xi	O
j	O
tau	O
y	O
powsigma	O
y	O
sigma	O
y	O
for	O
in	O
wj	O
dmnormmu	O
sigmainv	O
sigmainv	O
mu	O
we	O
can	O
then	O
just	O
pass	O
this	O
model	O
to	O
bugs	B
or	O
jags	B
which	O
will	O
generate	O
samples	B
for	O
us	O
see	O
the	O
webpages	O
for	O
details	O
although	O
this	O
approach	O
is	O
appealing	O
unfortunately	O
it	O
can	O
be	O
much	O
slower	O
than	O
using	O
handwritten	O
code	O
especially	O
for	O
complex	O
models	O
there	O
has	O
been	O
some	O
work	O
on	O
automatically	O
deriving	O
model-specific	O
optimized	O
inference	B
code	O
and	O
schumann	O
but	O
fast	O
code	O
still	O
typically	O
requires	O
human	O
expertise	O
the	O
imputation	B
posterior	I
algorithm	O
the	O
imputation	B
posterior	I
or	O
ip	B
algorithm	O
and	O
wong	O
is	O
a	O
special	O
case	O
of	O
gibbs	B
sampling	I
in	O
which	O
we	O
group	O
the	O
variables	O
into	O
two	O
classes	O
hidden	B
variables	I
z	O
and	O
parameters	O
this	O
should	O
sound	O
familiar	O
it	O
is	O
basically	O
an	O
mcmc	B
version	O
of	O
em	B
where	O
the	O
e	B
step	I
gets	O
replaced	O
by	O
the	O
i	O
step	O
and	O
the	O
m	B
step	I
gets	O
replaced	O
the	O
p	O
step	O
this	O
is	O
an	O
example	O
of	O
a	O
more	O
general	O
strategy	O
called	O
data	B
augmentation	I
whereby	O
we	O
introduce	O
auxiliary	B
variables	I
in	O
order	O
to	O
simplify	O
the	O
posterior	O
computations	O
the	O
computation	O
of	O
p	O
see	O
van	O
dyk	O
and	O
meng	O
for	O
more	O
information	B
blocking	B
gibbs	B
sampling	I
gibbs	B
sampling	I
can	O
be	O
quite	O
slow	O
since	O
it	O
only	O
updates	O
one	O
variable	O
at	O
a	O
time	O
single	B
site	I
updating	I
if	O
the	O
variables	O
are	O
highly	O
correlated	O
it	O
will	O
take	O
a	O
long	O
time	O
to	O
move	O
away	O
from	O
the	O
current	O
state	B
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
illustrate	O
sampling	O
from	O
a	O
gaussian	B
exercise	O
for	O
the	O
details	O
if	O
the	O
variables	O
are	O
highly	O
correlated	O
the	O
algorithm	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
figure	O
illustration	O
of	O
potentially	O
slow	O
sampling	O
when	O
using	O
gibbs	B
sampling	I
for	O
a	O
skewed	O
gaussian	B
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
gibbsgaussdemo	O
will	O
move	O
very	O
slowly	O
through	O
the	O
state	B
space	I
in	O
particular	O
the	O
size	O
of	O
the	O
moves	O
is	O
controlled	O
by	O
the	O
variance	B
of	O
the	O
conditional	O
distributions	O
if	O
this	O
is	O
in	O
the	O
direction	O
and	O
the	O
support	B
of	O
the	O
distribution	O
is	O
l	O
along	O
this	O
dimension	O
then	O
we	O
need	O
steps	O
to	O
obtain	O
an	O
independent	O
sample	O
in	O
some	O
cases	O
we	O
can	O
efficiently	O
sample	O
groups	O
of	O
variables	O
at	O
a	O
time	O
this	O
is	O
called	O
blocking	B
gibbs	B
sampling	I
or	O
blocked	B
gibbs	B
sampling	I
et	O
al	O
wilkinson	O
and	O
yeung	O
and	O
can	O
make	O
much	O
bigger	O
moves	O
through	O
the	O
state	B
space	I
metropolis	B
hastings	I
algorithm	O
although	O
gibbs	B
sampling	I
is	O
simple	O
it	O
is	O
somewhat	O
restricted	O
in	O
the	O
set	O
of	O
models	O
to	O
which	O
it	O
can	O
be	O
applied	O
for	O
example	O
it	O
is	O
not	O
much	O
help	O
in	O
computing	O
pwd	O
for	O
a	O
logistic	B
regression	B
model	O
since	O
the	O
corresponding	O
graphical	B
model	I
has	O
no	O
useful	O
markov	B
structure	O
in	O
addition	O
gibbs	B
sampling	I
can	O
be	O
quite	O
slow	O
as	O
we	O
mentioned	O
above	O
fortunately	O
there	O
is	O
a	O
more	O
general	O
algorithm	O
that	O
can	O
be	O
used	O
known	O
as	O
the	O
metropolis	B
hastings	I
or	O
mh	B
algorithm	O
which	O
we	O
describe	O
below	O
basic	O
idea	O
the	O
basic	O
idea	O
in	O
mh	B
is	O
that	O
at	O
each	O
step	O
we	O
propose	B
to	O
move	O
from	O
the	O
current	O
state	B
x	O
to	O
a	O
with	O
probability	O
where	O
q	O
is	O
called	O
the	O
proposal	B
distribution	I
called	O
new	O
state	B
the	O
kernel	B
the	O
user	O
is	O
free	O
to	O
use	O
any	O
kind	O
of	O
proposal	O
they	O
want	O
subject	O
to	O
some	O
conditions	O
which	O
we	O
explain	O
below	O
this	O
makes	O
mh	B
quite	O
a	O
flexible	O
method	O
a	O
commonly	O
used	O
proposal	O
is	O
a	O
symmetric	B
gaussian	B
distribution	O
centered	O
on	O
the	O
current	O
state	B
this	O
is	O
called	O
a	O
random	B
walk	I
metropolis	I
algorithm	I
we	O
discuss	O
how	O
to	O
choose	O
in	O
section	O
if	O
we	O
use	O
a	O
proposal	O
of	O
the	O
form	O
where	O
the	O
new	O
state	B
is	O
independent	O
of	O
the	O
old	O
state	B
we	O
get	O
a	O
method	O
known	O
as	O
the	O
independence	B
sampler	I
which	O
is	O
similar	B
to	O
importance	B
sampling	I
having	O
proposed	O
a	O
move	O
to	O
we	O
then	O
decide	O
whether	O
to	O
accept	B
this	O
proposal	O
or	O
not	O
according	O
to	O
some	O
formula	O
which	O
ensures	O
that	O
the	O
fraction	O
of	O
time	O
spent	O
in	O
each	O
state	B
is	O
proportional	O
to	O
p	O
otherwise	O
the	O
new	O
state	B
if	O
the	O
proposal	O
is	O
accepted	O
the	O
new	O
state	B
is	O
metropolis	B
hastings	I
algorithm	O
is	O
the	O
same	O
as	O
the	O
current	O
state	B
x	O
we	O
repeat	O
the	O
sample	O
if	O
the	O
proposal	O
is	O
symmetric	B
so	O
following	O
formula	O
p	O
p	O
r	O
the	O
acceptance	O
probability	O
is	O
given	O
by	O
the	O
is	O
more	O
probable	O
than	O
x	O
we	O
definitely	O
move	O
there	O
p	O
we	O
see	O
that	O
if	O
p	O
but	O
if	O
is	O
less	O
probable	O
we	O
may	O
still	O
move	O
there	O
anyway	O
depending	O
on	O
the	O
relative	O
probabilities	O
so	O
instead	O
of	O
greedily	O
moving	O
to	O
only	O
more	O
probable	O
states	O
we	O
occasionally	O
allow	O
downhill	O
moves	O
to	O
less	O
probable	O
states	O
in	O
section	O
we	O
prove	O
that	O
this	O
procedure	O
ensures	O
that	O
the	O
fraction	O
of	O
time	O
we	O
spend	O
in	O
each	O
state	B
x	O
is	O
proportional	O
to	O
p	O
if	O
the	O
proposal	O
is	O
asymmetric	O
so	O
we	O
need	O
the	O
hastings	B
correction	I
given	O
by	O
the	O
following	O
r	O
p	O
p	O
p	O
p	O
this	O
correction	O
is	O
needed	O
to	O
compensate	O
for	O
the	O
fact	O
that	O
the	O
proposal	B
distribution	I
itself	O
than	O
just	O
the	O
target	O
distribution	O
might	O
favor	O
certain	O
states	O
know	O
the	O
target	O
density	O
up	O
to	O
a	O
normalization	O
constant	O
in	O
particular	O
suppose	O
p	O
where	O
px	O
is	O
an	O
unnormalized	O
distribution	O
and	O
z	O
is	O
the	O
normalization	O
constant	O
then	O
an	O
important	O
reason	O
why	O
mh	B
is	O
a	O
useful	O
algorithm	O
is	O
that	O
when	O
evaluating	O
we	O
only	O
need	O
to	O
z	O
px	O
pxz	O
so	O
the	O
z	O
s	O
cancel	O
hence	O
we	O
can	O
sample	O
from	O
p	O
have	O
to	O
do	O
is	O
evaluate	O
p	O
pointwise	O
where	O
px	O
p	O
even	O
if	O
z	O
is	O
unknown	B
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
in	O
particular	O
all	O
we	O
gibbs	B
sampling	I
is	O
a	O
special	O
case	O
of	O
mh	B
it	O
turns	O
out	O
that	O
gibbs	B
sampling	I
which	O
we	O
discussed	O
in	O
section	O
is	O
a	O
special	O
case	O
of	O
mh	B
in	O
particular	O
it	O
is	O
equivalent	O
to	O
using	O
mh	B
with	O
a	O
sequence	O
of	O
proposals	O
of	O
the	O
form	O
ix	O
i	O
x	O
i	O
that	O
is	O
we	O
move	O
to	O
a	O
new	O
state	B
where	O
xi	O
is	O
sampled	O
from	O
its	O
full	B
conditional	I
but	O
x	O
i	O
is	O
left	O
unchanged	O
we	O
now	O
prove	O
that	O
the	O
acceptance	O
rate	B
of	O
each	O
such	O
proposal	O
is	O
so	O
the	O
overall	O
algorithm	O
also	O
has	O
an	O
acceptance	O
rate	B
of	O
we	O
have	O
pxix	O
ipx	O
i	O
ix	O
i	O
ix	O
ipx	O
ipxix	O
i	O
ix	O
i	O
pxix	O
ipx	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
algorithm	O
metropolis	B
hastings	I
algorithm	O
initialize	O
for	O
s	O
do	O
define	O
x	O
xs	O
sample	O
compute	O
acceptance	O
probability	O
compute	O
r	O
sample	O
u	O
u	O
set	O
new	O
sample	O
to	O
xs	O
if	O
u	O
r	O
if	O
u	O
r	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
i	O
x	O
i	O
and	O
that	O
ix	O
i	O
the	O
fact	O
that	O
the	O
acceptance	O
rate	B
is	O
does	O
not	O
necessarily	O
mean	B
that	O
gibbs	O
will	O
converge	B
rapidly	O
since	O
it	O
only	O
updates	O
one	O
coordinate	O
at	O
a	O
time	O
section	O
fortunately	O
there	O
are	O
many	O
other	O
kinds	O
of	O
proposals	O
we	O
can	O
use	O
as	O
we	O
discuss	O
below	O
proposal	O
distributions	O
for	O
a	O
given	O
target	O
distribution	O
p	O
a	O
proposal	B
distribution	I
q	O
is	O
valid	O
or	O
admissible	B
if	O
it	O
gives	O
a	O
non-zero	O
probability	O
of	O
moving	O
to	O
the	O
states	O
that	O
have	O
non-zero	O
probability	O
in	O
the	O
target	O
formally	O
we	O
can	O
write	O
this	O
as	O
xsuppq	O
suppp	O
for	O
example	O
a	O
gaussian	B
random	B
walk	I
proposal	I
has	O
non-zero	O
probability	O
density	O
on	O
the	O
entire	O
state	B
space	I
and	O
hence	O
is	O
a	O
valid	O
proposal	O
for	O
any	O
continuous	O
state	B
space	I
of	O
course	O
in	O
practice	O
it	O
is	O
important	O
that	O
the	O
proposal	O
spread	O
its	O
probability	O
mass	O
in	O
just	O
the	O
right	O
way	O
figure	O
shows	O
an	O
example	O
where	O
we	O
use	O
mh	B
to	O
sample	O
from	O
a	O
mixture	B
of	O
two	O
gaussians	O
using	O
a	O
random	B
walk	I
proposal	I
v	O
this	O
is	O
a	O
somewhat	O
tricky	O
target	O
distribution	O
since	O
it	O
consists	O
of	O
two	O
well	O
separated	O
modes	O
it	O
is	O
very	O
important	O
to	O
set	O
the	O
variance	B
of	O
the	O
proposal	O
v	O
correctly	O
if	O
the	O
variance	B
is	O
too	O
low	O
the	O
chain	O
will	O
only	O
explore	O
one	O
of	O
the	O
modes	O
as	O
shown	O
in	O
figure	O
but	O
if	O
the	O
variance	B
is	O
too	O
large	O
most	O
of	O
the	O
moves	O
will	O
be	O
rejected	O
and	O
the	O
chain	O
will	O
be	O
very	O
sticky	B
i	O
e	O
it	O
will	O
stay	O
in	O
the	O
same	O
state	B
for	O
a	O
long	O
time	O
this	O
is	O
evident	O
from	O
the	O
long	O
stretches	O
of	O
repeated	O
values	O
in	O
figure	O
if	O
we	O
set	O
the	O
proposal	O
s	O
variance	B
just	O
right	O
we	O
get	O
the	O
trace	B
in	O
figure	O
where	O
the	O
samples	B
clearly	O
explore	O
the	O
support	B
of	O
the	O
target	O
distribution	O
we	O
discuss	O
how	O
to	O
tune	O
the	O
proposal	O
below	O
one	O
big	O
advantage	O
of	O
gibbs	B
sampling	I
is	O
that	O
one	O
does	O
not	O
need	O
to	O
choose	O
the	O
proposal	O
metropolis	B
hastings	I
algorithm	O
mh	B
with	O
proposal	O
mh	B
with	O
proposal	O
iterations	O
samples	B
iterations	O
samples	B
mh	B
with	O
proposal	O
iterations	O
samples	B
figure	O
an	O
example	O
of	O
the	O
metropolis	B
hastings	I
algorithm	O
for	O
sampling	O
from	O
a	O
mixture	B
of	O
two	O
gaussians	O
using	O
a	O
gaussian	B
proposal	O
with	O
variances	O
of	O
v	O
when	O
v	O
the	O
chain	O
gets	O
trapped	O
near	O
the	O
starting	O
state	B
and	O
fails	O
to	O
sample	O
from	O
the	O
mode	B
at	O
when	O
v	O
the	O
chain	O
is	O
very	O
sticky	B
so	O
its	O
effective	B
sample	I
size	I
is	O
low	O
reflected	O
by	O
the	O
rough	O
histogram	B
approximation	O
at	O
the	O
end	O
using	O
a	O
variance	B
of	O
v	O
is	O
just	O
right	O
and	O
leads	O
to	O
a	O
good	O
approximation	O
of	O
the	O
true	O
distribution	O
in	O
red	O
figure	O
generated	O
by	O
mcmcgmmdemo	O
based	O
on	O
code	O
by	O
christophe	O
andrieu	O
and	O
nando	O
de	O
freitas	O
distribution	O
and	O
furthermore	O
the	O
acceptance	O
rate	B
is	O
of	O
course	O
a	O
acceptance	O
can	O
trivially	O
be	O
achieved	O
by	O
using	O
a	O
proposal	O
with	O
variance	B
we	O
start	O
at	O
a	O
mode	B
but	O
this	O
is	O
obviously	O
not	O
exploring	O
the	O
posterior	O
so	O
having	O
a	O
high	O
acceptance	O
is	O
not	O
the	O
ultimate	O
goal	O
we	O
can	O
increase	O
the	O
amount	O
of	O
exploration	O
by	O
increasing	O
the	O
variance	B
of	O
the	O
gaussian	B
kernel	B
often	O
one	O
experiments	O
with	O
different	O
parameters	O
until	O
the	O
acceptance	O
rate	B
is	O
between	O
and	O
which	O
theory	O
suggests	O
is	O
optimal	O
at	O
least	O
for	O
gaussian	B
target	O
distributions	O
these	O
short	O
initial	O
runs	O
used	O
to	O
tune	O
the	O
proposal	O
are	O
called	O
pilot	B
runs	I
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
w	O
intercept	O
slope	O
figure	O
joint	O
posterior	O
of	O
the	O
parameters	O
for	O
logistic	B
regression	B
when	O
applied	O
to	O
some	O
sat	O
data	O
marginal	O
for	O
the	O
offset	O
marginal	O
for	O
the	O
slope	O
we	O
see	O
that	O
the	O
marginals	O
do	O
not	O
capture	O
the	O
fact	O
that	O
the	O
parameters	O
are	O
highly	O
correlated	O
figure	O
generated	O
by	O
logregsatmhdemo	O
gaussian	B
proposals	O
if	O
we	O
have	O
a	O
continuous	O
state	B
space	I
the	O
hessian	B
h	O
at	O
a	O
local	O
mode	B
w	O
can	O
be	O
used	O
to	O
define	O
the	O
covariance	B
of	O
a	O
gaussian	B
proposal	B
distribution	I
this	O
approach	O
has	O
the	O
advantage	O
that	O
the	O
hessian	B
models	O
the	O
local	O
curvature	O
and	O
length	O
scales	O
of	O
each	O
dimension	O
this	O
approach	O
therefore	O
avoids	O
some	O
of	O
the	O
slow	O
mixing	O
behavior	O
of	O
gibbs	B
sampling	I
shown	O
in	O
figure	O
there	O
are	O
two	O
obvious	O
approaches	O
an	O
independence	O
proposal	O
n	O
w	O
h	O
or	O
a	O
random	B
walk	I
proposal	I
n	O
where	O
is	O
a	O
scale	O
factor	B
chosen	O
to	O
facilitate	O
rapid	O
mixing	O
and	O
rosenthal	O
prove	O
that	O
if	O
the	O
posterior	O
is	O
gaussian	B
the	O
asymptotically	B
optimal	I
value	O
is	O
to	O
use	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
w	O
this	O
results	O
in	O
an	O
acceptance	O
rate	B
of	O
for	O
example	O
consider	O
mh	B
for	O
binary	O
logistic	B
regression	B
from	O
equation	O
we	O
have	O
that	O
the	O
hessian	B
of	O
the	O
log-likelihood	O
is	O
hl	O
xt	O
dx	O
where	O
d	O
diag	O
i	O
and	O
i	O
sigm	O
wt	O
xi	O
if	O
we	O
assume	O
a	O
gaussian	B
prior	O
pw	O
n	O
we	O
have	O
h	O
v	O
hl	O
so	O
the	O
asymptotically	B
optimal	I
gaussian	B
proposal	O
has	O
the	O
form	O
n	O
w	O
d	O
v	O
xt	O
dx	O
see	O
rossi	O
et	O
al	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
for	O
further	O
details	O
the	O
approach	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
sample	O
parameters	O
from	O
a	O
logistic	B
regression	B
model	O
fit	O
to	O
some	O
sat	O
data	O
we	O
initialize	O
the	O
chain	O
at	O
the	O
mode	B
computed	O
using	O
irls	B
and	O
then	O
use	O
the	O
above	O
random	O
walk	O
metropolis	O
sampler	O
if	O
you	O
cannot	O
afford	O
to	O
compute	O
the	O
mode	B
or	O
its	O
hessian	B
xdx	O
an	O
alternative	O
approach	O
suggested	O
in	O
is	O
to	O
approximate	O
the	O
above	O
proposal	O
as	O
follows	O
n	O
w	O
v	O
xt	O
x	O
metropolis	B
hastings	I
algorithm	O
mixture	B
proposals	O
if	O
one	O
doesn	O
t	O
know	O
what	O
kind	O
of	O
proposal	O
to	O
use	O
one	O
can	O
try	O
a	O
mixture	B
proposal	I
which	O
is	O
a	O
convex	B
combination	I
of	O
base	O
proposals	O
where	O
wk	O
are	O
the	O
mixing	B
weights	I
as	O
long	O
as	O
each	O
qk	O
is	O
individually	O
valid	O
the	O
overall	O
proposal	O
will	O
also	O
be	O
valid	O
data-driven	B
mcmc	B
the	O
most	O
efficient	O
proposals	O
depend	O
not	O
just	O
on	O
the	O
previous	O
hidden	B
state	B
but	O
also	O
the	O
visible	B
data	O
i	O
e	O
they	O
have	O
the	O
form	O
this	O
is	O
called	O
data-driven	B
mcmc	B
e	O
g	O
and	O
zhu	O
to	O
create	O
such	O
proposals	O
one	O
can	O
sample	O
pairs	O
from	O
the	O
forwards	B
model	I
and	O
then	O
train	O
a	O
discriminative	B
classifier	O
to	O
predict	O
pxf	O
where	O
f	O
are	O
some	O
features	B
extracted	O
from	O
the	O
visible	B
data	O
typically	O
x	O
is	O
a	O
high-dimensional	O
vector	O
position	O
and	O
orientation	O
of	O
all	O
the	O
limbs	O
of	O
a	O
person	O
in	O
a	O
visual	O
object	O
detector	O
so	O
it	O
is	O
hard	O
to	O
predict	O
the	O
entire	O
state	B
vector	O
pxf	O
instead	O
we	O
might	O
train	O
a	O
discriminative	B
detector	O
to	O
predict	O
parts	O
of	O
the	O
state-space	O
pxkfkd	O
such	O
as	O
the	O
location	O
of	O
just	O
the	O
face	O
of	O
a	O
person	O
we	O
can	O
then	O
use	O
a	O
proposal	O
of	O
the	O
form	O
kfkd	O
k	O
where	O
is	O
a	O
standard	O
data-independent	O
proposal	O
random	O
walk	O
and	O
qk	O
updates	O
the	O
k	O
th	O
component	O
of	O
the	O
state	B
space	I
for	O
added	O
efficiency	O
the	O
discriminative	B
proposals	O
should	O
suggest	O
joint	O
changes	O
to	O
multiple	O
variables	O
but	O
this	O
is	O
often	O
hard	O
to	O
do	O
the	O
overall	O
procedure	O
is	O
a	O
form	O
of	O
generate	B
and	I
test	I
the	O
discriminative	B
proposals	O
generate	O
new	O
hypotheses	O
which	O
are	O
then	O
tested	O
by	O
computing	O
the	O
posterior	O
ratio	O
pxd	O
to	O
see	O
if	O
the	O
new	O
hypothesis	O
is	O
better	O
or	O
worse	O
by	O
adding	O
an	O
annealing	B
step	O
one	O
can	O
modify	O
the	O
algorithm	O
to	O
find	O
posterior	O
modes	O
this	O
is	O
called	O
simulated	B
annealing	B
and	O
is	O
described	O
in	O
section	O
one	O
advantage	O
of	O
using	O
the	O
mode-seeking	O
version	O
of	O
the	O
algorithm	O
is	O
that	O
we	O
do	O
not	O
need	O
to	O
ensure	O
the	O
proposal	B
distribution	I
is	O
reversible	O
adaptive	B
mcmc	B
one	O
can	O
change	O
the	O
parameters	O
of	O
the	O
proposal	O
as	O
the	O
algorithm	O
is	O
running	O
to	O
increase	O
efficiency	O
this	O
is	O
called	O
adaptive	B
mcmc	B
this	O
allows	O
one	O
to	O
start	O
with	O
a	O
broad	O
covariance	B
allowing	O
large	O
moves	O
through	O
the	O
space	O
until	O
a	O
mode	B
is	O
found	O
followed	O
by	O
a	O
narrowing	O
of	O
the	O
covariance	B
to	O
ensure	O
careful	O
exploration	O
of	O
the	O
region	O
around	O
the	O
mode	B
however	O
one	O
must	O
be	O
careful	O
not	O
to	O
violate	O
the	O
markov	B
property	O
thus	O
the	O
parameters	O
of	O
the	O
proposal	O
should	O
not	O
depend	O
on	O
the	O
entire	O
history	O
of	O
the	O
chain	O
it	O
turns	O
out	O
that	O
a	O
sufficient	O
condition	O
to	O
ensure	O
this	O
is	O
that	O
the	O
adaption	O
is	O
faded	O
out	O
gradually	O
over	O
time	O
see	O
e	O
g	O
and	O
thoms	O
for	O
details	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
initialization	O
and	O
mode	B
hopping	O
it	O
is	O
necessary	O
to	O
start	O
mcmc	B
in	O
an	O
initial	O
state	B
that	O
has	O
non-zero	O
probability	O
if	O
the	O
model	O
has	O
deterministic	O
constraints	O
finding	O
such	O
a	O
legal	O
configuration	O
may	O
be	O
a	O
hard	O
problem	O
in	O
itself	O
it	O
is	O
therefore	O
common	O
to	O
initialize	O
mcmc	B
methods	O
at	O
a	O
local	O
mode	B
found	O
using	O
an	O
optimizer	O
in	O
some	O
domains	O
with	O
discrete	B
state	B
spaces	O
it	O
is	O
a	O
more	O
effective	O
use	O
of	O
computation	O
time	O
to	O
perform	O
multiple	B
restarts	I
of	O
an	O
optimizer	O
and	O
to	O
average	O
over	O
these	O
modes	O
rather	O
than	O
exploring	O
similar	B
points	O
around	O
a	O
local	O
mode	B
however	O
in	O
continuous	O
state	B
spaces	O
the	O
mode	B
contains	O
negligible	O
volume	O
so	O
it	O
is	O
necessary	O
to	O
locally	O
explore	O
around	O
each	O
mode	B
in	O
order	O
to	O
visit	O
enough	O
posterior	O
probability	O
mass	O
why	O
mh	B
works	O
to	O
prove	O
that	O
the	O
mh	B
procedure	O
generates	O
samples	B
from	O
p	O
chain	O
theory	O
so	O
be	O
sure	O
to	O
read	O
section	O
first	O
we	O
have	O
to	O
use	O
a	O
bit	O
of	O
markov	B
the	O
mh	B
algorithm	O
defines	O
a	O
markov	B
chain	I
with	O
the	O
following	O
transition	B
matrix	I
qxx	O
if	O
x	O
otherwise	O
this	O
follows	O
from	O
a	O
case	B
analysis	I
if	O
you	O
move	O
to	O
from	O
x	O
you	O
must	O
have	O
proposed	O
it	O
probability	O
and	O
it	O
must	O
have	O
been	O
accepted	O
probability	O
otherwise	O
you	O
stay	O
in	O
state	B
x	O
either	O
because	O
that	O
is	O
what	O
you	O
proposed	O
probability	O
qxx	O
or	O
because	O
you	O
proposed	O
something	O
else	O
probability	O
but	O
it	O
was	O
rejected	O
probability	O
let	O
us	O
analyse	O
this	O
markov	B
chain	I
recall	B
from	O
section	O
that	O
a	O
chain	O
satisfies	O
detailed	B
balance	I
if	O
we	O
also	O
showed	O
that	O
if	O
a	O
chain	O
satisfies	O
detailed	B
balance	I
then	O
p	O
is	O
its	O
stationary	B
distribution	I
our	O
goal	O
is	O
to	O
show	O
that	O
the	O
mh	B
algorithm	O
defines	O
a	O
transition	O
function	O
that	O
satisfies	O
detailed	B
balance	I
and	O
hence	O
that	O
p	O
is	O
its	O
stationary	B
distribution	I
equation	O
holds	O
we	O
say	O
that	O
p	O
is	O
an	O
invariant	B
distribution	I
wrt	O
the	O
markov	B
transition	O
kernel	B
q	O
theorem	O
if	O
the	O
transition	B
matrix	I
defined	O
by	O
the	O
mh	B
algorithm	O
by	O
equation	O
is	O
ergodic	B
and	O
irreducible	B
then	O
p	O
proof	O
consider	O
two	O
states	O
x	O
and	O
is	O
its	O
unique	O
limiting	B
distribution	I
p	O
either	O
p	O
or	O
p	O
p	O
we	O
will	O
ignore	O
ties	O
occur	O
with	O
probability	O
zero	O
for	O
continuous	O
distributions	O
without	O
loss	B
p	O
of	O
generality	O
assume	O
that	O
p	O
p	O
p	O
hence	O
metropolis	B
hastings	I
algorithm	O
hence	O
we	O
have	O
and	O
now	O
to	O
move	O
fromx	O
to	O
we	O
must	O
first	O
propose	B
p	O
p	O
hence	O
p	O
p	O
and	O
then	O
accept	B
it	O
hence	O
p	O
p	O
the	O
backwards	O
probability	O
is	O
since	O
inserting	O
this	O
into	O
equation	O
we	O
get	O
p	O
p	O
so	O
detailed	B
balance	I
holds	O
wrt	O
p	O
is	O
a	O
stationary	B
distribution	I
furthermore	O
from	O
theorem	O
this	O
distribution	O
is	O
unique	O
since	O
the	O
chain	O
is	O
ergodic	B
and	O
irreducible	B
hence	O
from	O
theorem	O
p	O
reversible	B
jump	I
mcmc	B
suppose	O
we	O
have	O
a	O
set	O
of	O
models	O
with	O
different	O
numbers	O
of	O
parameters	O
e	O
g	O
mixture	B
models	O
in	O
which	O
the	O
number	O
of	O
mixture	B
components	O
is	O
unknown	B
let	O
the	O
model	O
be	O
denoted	O
by	O
m	O
and	O
let	O
its	O
unknowns	O
parameters	O
be	O
denoted	O
by	O
xm	O
xm	O
xm	O
r	O
nm	O
where	O
nm	O
is	O
the	O
dimensionality	O
of	O
model	O
m	O
sampling	O
in	O
spaces	O
of	O
differing	O
dimensionality	O
is	O
called	O
transdimensional	O
mcmc	B
we	O
could	O
sample	O
the	O
model	O
indicator	O
m	O
m	O
and	O
xm	O
but	O
this	O
is	O
very	O
inefficient	O
it	O
is	O
sample	O
all	O
the	O
parameters	O
from	O
the	O
product	O
space	O
xm	O
where	O
we	O
only	O
worry	O
more	O
parsimonious	O
to	O
sample	O
in	O
the	O
union	O
space	O
x	O
m	O
about	O
parameters	O
for	O
the	O
currently	O
active	O
model	O
the	O
difficulty	O
with	O
this	O
approach	O
arises	O
when	O
we	O
move	O
between	O
models	O
of	O
different	O
dimensionality	O
the	O
trouble	O
is	O
that	O
when	O
we	O
compute	O
the	O
mh	B
acceptance	O
ratio	O
we	O
are	O
comparing	O
densities	O
defined	O
in	O
different	O
dimensionality	O
spaces	O
which	O
is	O
meaningless	O
it	O
is	O
like	O
trying	O
to	O
compare	O
a	O
sphere	O
with	O
a	O
circle	O
the	O
solution	O
proposed	O
by	O
and	O
known	O
as	O
reversible	B
jump	I
mcmc	B
or	O
rjmcmc	B
is	O
to	O
augment	O
the	O
low	O
dimensional	O
space	O
with	O
extra	O
random	O
variables	O
so	O
that	O
the	O
two	O
spaces	O
have	O
a	O
common	O
measure	O
unfortunately	O
we	O
do	O
not	O
have	O
space	O
to	O
go	O
into	O
details	O
here	O
suffice	O
it	O
to	O
say	O
that	O
the	O
method	O
can	O
be	O
made	O
to	O
work	O
in	O
theory	O
although	O
it	O
is	O
a	O
bit	O
tricky	O
in	O
practice	O
if	O
however	O
the	O
continuous	O
parameters	O
can	O
be	O
integrated	O
out	O
in	O
a	O
method	O
called	O
collapsed	O
rjmcmc	B
much	O
of	O
the	O
difficulty	O
goes	O
away	O
since	O
we	O
are	O
just	O
left	O
with	O
a	O
discrete	B
state	B
space	I
where	O
there	O
is	O
no	O
need	O
to	O
worry	O
about	O
change	O
of	O
measure	O
for	O
example	O
et	O
al	O
includes	O
many	O
examples	O
of	O
applications	O
of	O
collapsed	O
rjmcmc	B
applied	O
to	O
bayesian	B
inference	B
fro	O
adaptive	O
basis-function	O
models	O
they	O
sample	O
basis	B
functions	I
from	O
a	O
fixed	O
set	O
of	O
candidates	O
centered	O
on	O
the	O
data	O
points	O
and	O
integrate	B
out	I
the	O
other	O
parameters	O
analytically	O
this	O
provides	O
a	O
bayesian	B
alternative	O
to	O
using	O
rvms	O
or	O
svms	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
initial	O
condition	O
x	O
initial	O
condition	O
x	O
figure	O
illustration	O
of	O
convergence	O
to	O
the	O
uniform	B
distribution	I
over	O
using	O
a	O
symmetric	B
random	O
walk	O
starting	O
from	O
state	B
and	O
state	B
based	O
on	O
figures	O
and	O
of	O
figure	O
generated	O
by	O
speed	O
and	O
accuracy	O
of	O
mcmc	B
in	O
this	O
section	O
we	O
discuss	O
a	O
number	O
of	O
important	O
theoretical	O
and	O
practical	O
issues	O
to	O
do	O
with	O
mcmc	B
the	O
burn-in	B
phase	B
we	O
start	O
mcmc	B
from	O
an	O
arbitrary	O
initial	O
state	B
as	O
we	O
explained	O
in	O
section	O
only	O
when	O
the	O
chain	O
has	O
forgotten	O
where	O
it	O
started	O
from	O
will	O
the	O
samples	B
be	O
coming	O
from	O
the	O
chain	O
s	O
stationary	B
distribution	I
samples	B
collected	O
before	O
the	O
chain	O
has	O
reached	O
its	O
stationary	B
distribution	I
do	O
not	O
come	O
from	O
p	O
and	O
are	O
usually	O
thrown	O
away	O
the	O
initial	O
period	B
whose	O
samples	B
will	O
be	O
ignored	O
is	O
called	O
the	O
burn-in	B
phase	B
for	O
example	O
consider	O
a	O
uniform	B
distribution	I
on	O
the	O
integers	O
suppose	O
we	O
sample	O
from	O
this	O
using	O
a	O
symmetric	B
random	O
walk	O
in	O
figure	O
we	O
show	O
two	O
runs	O
of	O
the	O
algorithm	O
on	O
the	O
left	O
we	O
start	O
in	O
state	B
on	O
the	O
right	O
we	O
start	O
in	O
state	B
even	O
in	O
this	O
small	O
problem	O
it	O
takes	O
over	O
steps	O
until	O
the	O
chain	O
has	O
forgotten	O
where	O
it	O
started	O
from	O
it	O
is	O
difficult	O
to	O
diagnose	O
when	O
the	O
chain	O
has	O
burned	B
in	I
an	O
issue	O
we	O
discuss	O
in	O
more	O
detail	O
below	O
is	O
one	O
of	O
the	O
fundamental	O
weaknesses	O
of	O
mcmc	B
as	O
an	O
interesting	O
example	O
of	O
what	O
can	O
happen	O
if	O
you	O
start	O
collecting	O
samples	B
too	O
early	O
consider	O
the	O
potts	B
model	I
figure	O
shows	O
a	O
sample	O
after	O
iterations	O
of	O
gibbs	B
sampling	I
this	O
suggests	O
that	O
the	O
model	O
likes	O
speed	O
and	O
accuracy	O
of	O
mcmc	B
figure	O
illustration	O
of	O
problems	O
caused	O
by	O
poor	O
mixing	O
one	O
sample	O
from	O
a	O
potts	B
model	I
on	O
a	O
grid	O
with	O
nearest	B
neighbor	I
connectivity	O
and	O
j	O
in	O
and	O
geman	O
after	O
iterations	O
one	O
sample	O
from	O
the	O
same	O
model	O
after	O
iterations	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
medium-sized	O
regions	O
where	O
the	O
label	B
is	O
the	O
same	O
implying	O
the	O
model	O
would	O
make	O
a	O
good	O
prior	O
for	O
image	B
segmentation	I
indeed	O
this	O
was	O
suggested	O
in	O
the	O
original	O
gibbs	B
sampling	I
paper	O
and	O
geman	O
however	O
it	O
turns	O
out	O
that	O
if	O
you	O
run	O
the	O
chain	O
long	O
enough	O
you	O
get	O
isolated	O
speckles	O
as	O
in	O
figure	O
the	O
results	O
depend	O
on	O
the	O
coupling	O
strength	O
but	O
in	O
general	O
it	O
is	O
very	O
hard	O
to	O
find	O
a	O
setting	O
which	O
produces	O
nice	O
medium-sized	O
blobs	O
most	O
parameters	O
result	O
in	O
a	O
few	O
super-clusters	O
or	O
lots	O
of	O
small	O
fragments	O
in	O
fact	O
there	O
is	O
a	O
rapid	O
phase	B
transition	I
between	O
these	O
two	O
regimes	O
this	O
led	O
to	O
a	O
paper	O
called	O
the	O
isingpotts	O
model	O
is	O
not	O
well	O
suited	O
to	O
segmentation	O
tasks	O
et	O
al	O
it	O
is	O
possible	O
to	O
create	O
priors	O
more	O
suited	O
to	O
image	B
segmentation	I
and	O
jordan	O
but	O
the	O
main	O
point	O
here	O
is	O
that	O
sampling	O
before	O
reaching	O
convergence	O
can	O
lead	O
to	O
erroneous	O
conclusions	O
mixing	O
rates	O
of	O
markov	B
chains	O
the	O
amount	O
of	O
time	O
it	O
takes	O
for	O
a	O
markov	B
chain	I
to	O
converge	B
to	O
the	O
stationary	B
distribution	I
and	O
forget	O
its	O
initial	O
state	B
is	O
called	O
the	O
mixing	B
time	I
more	O
formally	O
we	O
say	O
that	O
the	O
mixing	B
time	I
from	O
state	B
is	O
the	O
minimal	B
time	O
such	O
that	O
for	O
any	O
constant	O
we	O
have	O
that	O
mint	O
t	O
p	O
where	O
is	O
a	O
distribution	O
with	O
all	O
its	O
mass	O
in	O
state	B
t	O
is	O
the	O
transition	B
matrix	I
of	O
the	O
chain	O
depends	O
on	O
the	O
target	O
p	O
and	O
the	O
proposal	O
q	O
and	O
t	O
is	O
the	O
distribution	O
after	O
t	O
steps	O
the	O
mixing	B
time	I
of	O
the	O
chain	O
is	O
defined	O
as	O
max	O
the	O
mixing	B
time	I
is	O
determined	O
by	O
the	O
eigengap	B
which	O
is	O
the	O
difference	O
of	O
the	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
figure	O
a	O
markov	B
chain	I
with	O
low	O
conductance	B
the	O
dotted	O
arcs	O
represent	O
transitions	O
with	O
very	O
low	O
probability	O
source	O
figure	O
of	O
and	O
friedman	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
first	O
and	O
second	O
eigenvalues	O
of	O
the	O
transition	B
matrix	I
in	O
particular	O
one	O
can	O
show	O
that	O
o	O
log	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
states	O
since	O
computing	O
the	O
transition	B
matrix	I
can	O
be	O
hard	O
to	O
do	O
especially	O
for	O
high	O
dimensional	O
andor	O
continuous	O
state	B
spaces	O
it	O
is	O
useful	O
to	O
find	O
other	O
ways	O
to	O
estimate	O
the	O
mixing	B
time	I
an	O
alternative	O
approach	O
is	O
to	O
examine	O
the	O
geometry	O
of	O
the	O
state	B
space	I
for	O
example	O
consider	O
the	O
chain	O
in	O
figure	O
we	O
see	O
that	O
the	O
state	B
space	I
consists	O
of	O
two	O
islands	O
each	O
of	O
which	O
is	O
connected	O
via	O
a	O
narrow	O
bottleneck	B
they	O
were	O
completely	O
disconnected	O
the	O
chain	O
would	O
not	O
be	O
ergodic	B
and	O
there	O
would	O
no	O
longer	O
be	O
a	O
unique	O
stationary	B
distribution	I
we	O
define	O
the	O
conductance	B
of	O
a	O
chain	O
as	O
the	O
minimum	O
probability	O
over	O
all	O
subsets	O
of	O
states	O
of	O
transitioning	O
from	O
that	O
set	O
to	O
its	O
complement	O
x	O
sc	O
t	O
p	O
min	O
p	O
one	O
can	O
show	O
that	O
o	O
log	O
n	O
hence	O
chains	O
with	O
low	O
conductance	B
have	O
high	O
mixing	B
time	I
for	O
example	O
distributions	O
with	O
well-separated	O
modes	O
usually	O
have	O
high	O
mixing	B
time	I
simple	O
mcmc	B
methods	O
often	O
do	O
not	O
work	O
well	O
in	O
such	O
cases	O
and	O
more	O
advanced	O
algorithms	O
such	O
as	O
parallel	B
tempering	I
are	O
necessary	O
e	O
g	O
practical	O
convergence	O
diagnostics	O
computing	O
the	O
mixing	B
time	I
of	O
a	O
chain	O
is	O
in	O
general	O
quite	O
difficult	O
since	O
the	O
transition	B
matrix	I
is	O
usually	O
very	O
hard	O
to	O
compute	O
in	O
practice	O
various	O
heuristics	B
have	O
been	O
proposed	O
to	O
diagnose	O
speed	O
and	O
accuracy	O
of	O
mcmc	B
convergence	O
see	O
cowles	O
and	O
carlin	O
brooks	O
and	O
roberts	O
for	O
a	O
review	O
strictly	O
speaking	O
these	O
methods	O
do	O
not	O
diagnose	O
convergence	O
but	O
rather	O
non-convergence	O
that	O
is	O
the	O
method	O
may	O
claim	O
the	O
chain	O
has	O
converged	O
when	O
in	O
fact	O
it	O
has	O
not	O
this	O
is	O
a	O
flaw	O
common	O
to	O
all	O
convergence	O
diagnostics	O
since	O
diagnosing	O
convergence	O
is	O
computationally	O
intractable	O
in	O
general	O
et	O
al	O
one	O
of	O
the	O
simplest	O
approaches	O
to	O
assessing	O
when	O
the	O
method	O
has	O
converged	O
is	O
to	O
run	O
multiple	O
chains	O
from	O
very	O
different	O
overdispersed	B
starting	O
points	O
and	O
to	O
plot	O
the	O
samples	B
of	O
some	O
variables	O
of	O
interest	O
this	O
is	O
called	O
a	O
trace	B
plot	I
if	O
the	O
chain	O
has	O
mixed	O
it	O
should	O
have	O
forgotten	O
where	O
it	O
started	O
from	O
so	O
the	O
trace	B
plots	O
should	O
converge	B
to	O
the	O
same	O
distribution	O
and	O
thus	O
overlap	O
with	O
each	O
other	O
figure	O
gives	O
an	O
example	O
we	O
show	O
the	O
traceplot	O
for	O
x	O
which	O
was	O
sampled	O
from	O
a	O
mixture	B
of	O
two	O
gaussians	O
using	O
four	O
different	O
methods	O
mh	B
with	O
a	O
symmetric	B
gaussian	B
proposal	O
of	O
variance	B
and	O
gibbs	B
sampling	I
we	O
see	O
that	O
has	O
not	O
mixed	O
which	O
is	O
also	O
evident	O
from	O
figure	O
which	O
shows	O
that	O
a	O
single	O
chain	O
never	O
leaves	B
the	O
area	O
where	O
it	O
started	O
the	O
results	O
for	O
the	O
other	O
methods	O
indicate	O
that	O
the	O
chains	O
rapidly	O
converge	B
to	O
sticky	B
nature	O
of	O
the	O
the	O
stationary	B
distribution	I
no	O
matter	O
where	O
they	O
started	O
proposal	O
is	O
very	O
evident	O
this	O
reduces	O
the	O
computational	O
efficiency	O
as	O
we	O
discuss	O
below	O
but	O
not	O
the	O
statistical	O
validity	O
estimated	B
potential	I
scale	I
reduction	I
we	O
can	O
assess	O
convergence	O
more	O
quantitatively	O
as	O
follows	O
the	O
basic	O
idea	O
is	O
to	O
compare	O
the	O
variance	B
of	O
a	O
quantity	O
within	O
each	O
chain	O
to	O
its	O
variance	B
across	O
chains	O
more	O
precisely	O
suppose	O
we	O
collect	O
s	O
samples	B
burn-in	O
from	O
each	O
of	O
c	O
chains	O
of	O
d	O
variables	O
xisc	O
i	O
d	O
s	O
c	O
let	O
ysc	O
be	O
a	O
scalar	O
quantity	O
of	O
interest	O
derived	O
from	O
ysc	O
xisc	O
for	O
some	O
chosen	O
i	O
define	O
the	O
within-sequence	O
mean	B
and	O
overall	O
mean	B
as	O
y	O
c	O
s	O
y	O
c	O
ysc	O
y	O
c	O
define	O
the	O
between-sequence	O
and	O
within-sequence	O
variance	B
as	O
b	O
s	O
c	O
c	O
y	O
w	O
c	O
s	O
y	O
we	O
can	O
now	O
construct	O
two	O
estimates	O
of	O
the	O
variance	B
of	O
y	O
the	O
first	O
estimate	O
is	O
w	O
this	O
should	O
underestimate	O
var	B
if	O
the	O
chains	O
have	O
not	O
ranged	O
over	O
the	O
full	B
posterior	O
the	O
second	O
estimate	O
is	O
w	O
s	O
b	O
s	O
s	O
v	O
this	O
is	O
an	O
estimate	O
of	O
var	B
that	O
is	O
unbiased	B
under	O
stationarity	O
but	O
is	O
an	O
overestimate	O
if	O
the	O
starting	O
points	O
were	O
overdispersed	B
and	O
rubin	O
from	O
this	O
we	O
can	O
define	O
the	O
following	O
convergence	O
diagnostic	O
statistic	O
known	O
as	O
the	O
estimated	B
potential	I
scale	I
reduction	I
or	O
epsr	B
r	O
v	O
w	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
mh	B
rhat	O
mh	B
rhat	O
mh	B
rhat	O
gibbs	O
rhat	O
figure	O
traceplots	O
for	O
mcmc	B
samplers	O
each	O
color	O
represents	O
the	O
samples	B
from	O
a	O
different	O
starting	O
point	O
gibbs	B
sampling	I
figure	O
generated	O
by	O
mcmcgmmdemo	O
mh	B
with	O
proposal	O
n	O
for	O
corresponding	O
to	O
figure	O
this	O
quantity	O
which	O
was	O
first	O
proposed	O
in	O
and	O
rubin	O
measures	O
the	O
degree	B
to	O
which	O
the	O
posterior	O
variance	B
would	O
decrease	O
if	O
we	O
were	O
to	O
continue	O
sampling	O
in	O
the	O
s	O
limit	O
if	O
r	O
for	O
any	O
given	O
quantity	O
then	O
that	O
estimate	O
is	O
reliable	O
at	O
least	O
is	O
not	O
unreliable	O
the	O
r	O
values	O
for	O
the	O
four	O
samplers	O
in	O
figure	O
are	O
and	O
so	O
this	O
diagnostic	O
has	O
correctly	O
identified	O
that	O
the	O
sampler	O
using	O
the	O
first	O
proposal	O
is	O
untrustworthy	O
accuracy	O
of	O
mcmc	B
the	O
samples	B
produced	O
by	O
mcmc	B
are	O
auto-correlated	O
and	O
this	O
reduces	O
their	O
information	B
content	O
relative	O
to	O
independent	O
or	O
perfect	O
samples	B
we	O
can	O
quantify	O
this	O
as	O
suppose	O
we	O
want	O
this	O
section	O
is	O
based	O
on	O
sec	O
speed	O
and	O
accuracy	O
of	O
mcmc	B
mh	B
mh	B
mh	B
gibbs	O
figure	O
autocorrelation	O
functions	O
corresponding	O
to	O
figure	O
figure	O
generated	O
by	O
mcmcgmmdemo	O
to	O
estimate	O
the	O
mean	B
of	O
f	O
for	O
some	O
function	O
f	O
wherex	O
p	O
denote	O
the	O
true	O
mean	B
by	O
f	O
e	O
f	O
s	O
a	O
monte	B
carlo	I
estimate	O
is	O
given	O
by	O
fs	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
where	O
fs	O
f	O
and	O
xs	O
px	O
an	O
mcmc	B
estimate	O
of	O
the	O
variance	B
of	O
this	O
estimate	O
is	O
given	O
by	O
e	O
f	O
f	O
f	O
varm	O
cm	O
cf	O
e	O
f	O
s	O
e	O
f	O
f	O
e	O
f	O
varm	O
cf	O
where	O
the	O
first	O
term	O
is	O
the	O
monte	B
carlo	I
estimate	O
of	O
the	O
variance	B
if	O
the	O
samples	B
weren	O
t	O
correlated	O
and	O
the	O
second	O
term	O
depends	O
on	O
the	O
correlation	O
of	O
the	O
samples	B
we	O
can	O
measure	O
this	O
as	O
follows	O
define	O
the	O
sample-based	O
auto-correlation	O
at	O
lag	B
t	O
of	O
a	O
set	O
of	O
samples	B
fs	O
as	O
follows	O
t	O
f	O
f	O
f	O
s	O
s	O
t	O
t	O
this	O
is	O
called	O
the	O
autocorrelation	B
function	I
this	O
is	O
plotted	O
in	O
figure	O
for	O
our	O
four	O
samplers	O
for	O
the	O
gaussian	B
mixture	B
model	I
we	O
see	O
that	O
the	O
acf	O
of	O
the	O
gibbs	B
sampler	I
right	O
dies	O
off	O
to	O
much	O
more	O
rapidly	O
than	O
the	O
mh	B
samplers	O
indicating	O
that	O
each	O
gibbs	O
sample	O
is	O
worth	O
more	O
than	O
each	O
mh	B
sample	O
a	O
simple	O
method	O
to	O
reduce	O
the	O
autocorrelation	O
is	O
to	O
use	O
thinning	B
in	O
which	O
we	O
keep	O
every	O
n	O
th	O
sample	O
this	O
does	O
not	O
increase	O
the	O
efficiency	O
of	O
the	O
underlying	O
sampler	O
but	O
it	O
does	O
save	O
space	O
since	O
it	O
avoids	O
storing	O
highly	O
correlated	O
samples	B
we	O
can	O
estimate	O
the	O
information	B
content	O
of	O
a	O
set	O
of	O
samples	B
by	O
computing	O
the	O
effective	B
sample	I
size	I
s	O
eff	O
defined	O
by	O
s	O
eff	O
varm	O
cf	O
varm	O
cm	O
cf	O
from	O
figure	O
it	O
is	O
clear	O
that	O
the	O
effective	B
sample	I
size	I
of	O
the	O
gibbs	B
sampler	I
is	O
higher	O
than	O
that	O
of	O
the	O
other	O
samplers	O
this	O
example	O
how	O
many	O
chains	O
a	O
natural	O
question	O
to	O
ask	O
is	O
how	O
many	O
chains	O
should	O
we	O
run	O
we	O
could	O
either	O
run	O
one	O
long	O
chain	O
to	O
ensure	O
convergence	O
and	O
then	O
collect	O
samples	B
spaced	O
far	O
apart	O
or	O
we	O
could	O
run	O
many	O
short	O
chains	O
but	O
that	O
wastes	O
the	O
burnin	O
time	O
in	O
practice	O
it	O
is	O
common	O
to	O
run	O
a	O
medium	O
number	O
of	O
chains	O
of	O
medium	O
length	O
steps	O
and	O
to	O
take	O
samples	B
from	O
each	O
after	O
discarding	O
the	O
first	O
half	O
of	O
the	O
samples	B
if	O
we	O
initialize	O
at	O
a	O
local	O
mode	B
we	O
may	O
be	O
able	O
to	O
use	O
all	O
the	O
samples	B
and	O
not	O
wait	O
for	O
burn-in	O
auxiliary	O
variable	O
mcmc	B
em	B
ep	B
gibbs	O
gibbs	O
with	O
ars	O
model	O
goal	O
method	O
probit	B
map	O
gradient	O
probit	B
map	O
post	O
probit	B
post	O
probit	B
post	O
probit	B
probit	B
post	O
mh	B
using	O
irls	B
proposal	O
map	O
gradient	O
logit	B
post	O
logit	B
logit	B
post	O
gibbs	O
with	O
student	O
gibbs	O
with	O
ks	O
reference	O
section	O
section	O
and	O
rasmussen	O
exercise	O
and	O
smith	O
section	O
and	O
fruhwirth	O
and	O
held	O
table	O
summary	O
of	O
some	O
possible	O
algorithms	O
for	O
estimation	O
and	O
inference	B
for	O
binary	O
classification	O
problems	O
using	O
gaussian	B
priors	O
abbreviations	O
aux	O
auxiliary	O
variable	O
sampling	O
ars	O
adaptive	B
rejection	B
sampling	I
ep	B
expectation	B
propagation	I
gibbs	O
gibbs	B
sampling	I
with	O
auxiliary	B
variables	I
irls	B
iterative	O
reweighted	O
least	B
squares	I
ks	O
kolmogorov	B
smirnov	I
map	O
maximum	B
a	I
posteriori	I
mh	B
metropolis	B
hastings	I
post	O
posterior	O
auxiliary	O
variable	O
mcmc	B
sometimes	O
we	O
can	O
dramatically	O
improve	O
the	O
efficiency	O
of	O
sampling	O
by	O
introducing	O
dummy	O
auxiliary	B
variables	I
in	O
order	O
to	O
reduce	O
correlation	O
between	O
the	O
original	O
variables	O
if	O
the	O
original	O
variables	O
are	O
denoted	O
by	O
x	O
and	O
the	O
auxiliary	B
variables	I
by	O
z	O
we	O
require	O
that	O
z	O
px	O
z	O
px	O
and	O
that	O
px	O
z	O
is	O
easier	O
to	O
sample	O
from	O
than	O
just	O
px	O
if	O
we	O
meet	O
these	O
two	O
conditions	O
we	O
can	O
sample	O
in	O
the	O
enlarged	O
model	O
and	O
then	O
throw	O
away	O
the	O
sampled	O
z	O
values	O
thereby	O
recovering	O
samples	B
from	O
px	O
we	O
give	O
some	O
examples	O
below	O
auxiliary	O
variable	O
sampling	O
for	O
logistic	B
regression	B
in	O
section	O
we	O
discussed	O
the	O
latent	B
variable	O
interpretation	O
of	O
probit	B
regression	B
recall	B
that	O
this	O
had	O
the	O
form	O
zi	O
wt	O
xi	O
n	O
yi	O
izi	O
we	O
exploited	O
this	O
representation	O
in	O
section	O
where	O
we	O
used	O
em	B
to	O
find	O
an	O
ml	O
estimate	O
it	O
is	O
straightforward	O
to	O
convert	O
this	O
into	O
an	O
auxiliary	O
variable	O
gibbs	B
sampler	I
since	O
pwd	O
is	O
gaussian	B
and	O
pzixi	O
yi	O
w	O
is	O
truncated	B
gaussian	B
both	O
of	O
which	O
are	O
easy	O
to	O
sample	O
from	O
now	O
let	O
us	O
discuss	O
how	O
to	O
derive	O
an	O
auxiliary	O
variable	O
gibbs	B
sampler	I
for	O
logistic	B
regression	B
let	O
follow	O
a	O
logistic	B
distribution	I
with	O
pdf	B
plogistic	O
e	O
e	O
with	O
mean	B
e	O
and	O
variance	B
var	B
the	O
cdf	B
has	O
the	O
form	O
f	O
sigm	O
which	O
pyi	O
w	O
wt	O
xi	O
f	O
f	O
f	O
xi	O
sigmwt	O
xi	O
as	O
required	O
we	O
can	O
derive	O
an	O
auxiliary	O
variable	O
gibbs	B
sampler	I
by	O
sampling	O
from	O
pzwd	O
and	O
pwzd	O
unfortunately	O
sampling	O
directly	O
from	O
pwzd	O
is	O
not	O
possible	O
one	O
approach	O
is	O
to	O
define	O
n	O
i	O
where	O
i	O
and	O
i	O
ks	O
the	O
kolmogorov	B
smirnov	I
distribution	O
and	O
then	O
to	O
sample	O
w	O
z	O
and	O
and	O
held	O
a	O
simpler	O
approach	O
is	O
to	O
approximate	O
the	O
logistic	B
distribution	I
by	O
the	O
student	O
distribution	O
and	O
chib	O
specifically	O
we	O
will	O
make	O
the	O
approximation	O
t	O
where	O
we	O
can	O
now	O
use	O
the	O
scale	O
mixture	B
of	I
gaussians	I
representation	O
of	O
the	O
student	O
to	O
simplify	O
inference	B
in	O
particular	O
we	O
write	O
i	O
ga	O
n	O
zi	O
wt	O
xi	O
yi	O
izi	O
i	O
all	O
of	O
the	O
full	B
conditionals	O
now	O
have	O
a	O
simple	O
form	O
see	O
exercise	O
for	O
the	O
details	O
note	O
that	O
if	O
we	O
set	O
then	O
zi	O
n	O
xi	O
which	O
is	O
equivalent	O
to	O
probit	B
regression	B
section	O
rather	O
than	O
choosing	O
between	O
probit	B
or	O
logit	B
regression	B
we	O
can	O
simply	O
estimate	O
the	O
parameter	B
there	O
is	O
no	O
convenient	O
conjugate	B
prior	I
but	O
we	O
can	O
consider	O
a	O
finite	O
range	O
of	O
possible	O
values	O
and	O
evaluate	O
the	O
posterior	O
as	O
follows	O
p	O
p	O
i	O
e	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
is	O
the	O
logistic	B
function	O
since	O
yi	O
iff	B
wt	O
xi	O
we	O
have	O
by	O
symmetry	O
that	O
wt	O
xi	O
furthermore	O
if	O
we	O
define	O
we	O
can	O
sample	O
as	O
well	O
for	O
example	O
suppose	O
we	O
use	O
a	O
ig	O
prior	O
for	O
the	O
posterior	O
is	O
given	O
by	O
ig	O
j	O
this	O
can	O
be	O
interleaved	O
with	O
the	O
other	O
gibbs	B
sampling	I
steps	O
and	O
provides	O
an	O
appealing	O
bayesian	B
alternative	O
to	O
cross	B
validation	I
for	O
setting	O
the	O
strength	O
of	O
the	O
regularizer	O
d	O
see	O
table	O
for	O
a	O
summary	O
of	O
various	O
algorithms	O
for	O
fitting	O
probit	B
and	O
logit	B
models	O
many	O
of	O
these	O
methods	O
can	O
also	O
be	O
extended	O
to	O
the	O
multinomial	B
logistic	B
regression	B
case	O
for	O
details	O
see	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
slice	B
sampling	I
consider	O
sampling	O
from	O
a	O
univariate	O
but	O
multimodal	O
distribution	O
px	O
we	O
can	O
sometimes	O
improve	O
the	O
ability	O
to	O
make	O
large	O
moves	O
by	O
adding	O
an	O
auxiliary	O
variable	O
u	O
we	O
define	O
the	O
joint	B
distribution	I
as	O
follows	O
if	O
u	O
px	O
otherwise	O
px	O
u	O
auxiliary	O
variable	O
mcmc	B
fx	O
u	O
x	O
x	O
x	O
illustration	O
of	O
the	O
principle	O
behind	O
slice	B
sampling	I
given	O
a	O
previous	O
sample	O
xi	O
we	O
figure	O
sample	O
uniformly	O
on	O
f	O
where	O
f	O
is	O
the	O
target	O
density	O
we	O
then	O
sample	O
along	O
the	O
slice	O
where	O
f	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
nando	O
de	O
freitas	O
slice	B
sampling	I
in	O
action	B
figure	O
generated	O
by	O
x	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
r	O
o	O
i	O
r	O
e	O
t	O
s	O
o	O
p	O
slope	O
intercept	O
figure	O
binomial	B
regression	B
for	O
data	O
approximation	O
figure	O
generated	O
by	O
grid	O
approximation	O
to	O
posterior	O
slice	B
sampling	I
where	O
zp	O
px	O
pxdx	O
the	O
marginal	B
distribution	I
over	O
x	O
is	O
given	O
by	O
px	O
udu	O
zp	O
du	O
px	O
zp	O
px	O
so	O
we	O
can	O
sample	O
from	O
px	O
by	O
sampling	O
from	O
px	O
u	O
and	O
then	O
ignoring	O
u	O
the	O
full	B
conditionals	O
have	O
the	O
form	O
pux	O
pxu	O
pxu	O
uax	O
where	O
a	O
px	O
u	O
is	O
the	O
set	O
of	O
points	O
on	O
or	O
above	O
the	O
chosen	O
height	O
u	O
this	O
corresponds	O
to	O
a	O
slice	O
through	O
the	O
distribution	O
hence	O
the	O
term	O
slice	B
sampling	I
see	O
figure	O
in	O
practice	O
it	O
can	O
be	O
difficult	O
to	O
identify	O
the	O
set	O
a	O
so	O
we	O
can	O
use	O
the	O
following	O
approach	O
construct	O
an	O
interval	O
xmin	O
x	O
xmax	O
around	O
the	O
current	O
point	O
xs	O
of	O
some	O
width	O
we	O
then	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
test	O
to	O
see	O
if	O
each	O
end	O
point	O
lies	O
within	O
the	O
slice	O
if	O
it	O
does	O
we	O
keep	O
extending	O
in	O
that	O
direction	O
until	O
it	O
lies	O
outside	O
the	O
slice	O
this	O
is	O
called	O
stepping	B
out	I
a	O
candidate	O
value	O
is	O
then	O
chosen	O
if	O
it	O
lies	O
within	O
the	O
slice	O
it	O
is	O
kept	O
so	O
otherwise	O
we	O
uniformly	O
from	O
this	O
region	O
shrink	O
the	O
region	O
such	O
that	O
forms	O
one	O
end	O
and	O
such	O
that	O
the	O
region	O
still	O
contains	O
xs	O
then	O
another	O
sample	O
is	O
drawn	O
we	O
continue	O
in	O
this	O
way	O
until	O
a	O
sample	O
is	O
accepted	O
to	O
apply	O
the	O
method	O
to	O
multivariate	O
distributions	O
we	O
can	O
sample	O
one	O
extra	O
auxiliary	O
variable	O
for	O
each	O
dimension	O
the	O
advantage	O
of	O
slice	B
sampling	I
over	O
gibbs	O
is	O
that	O
it	O
does	O
not	O
need	O
a	O
specification	O
of	O
the	O
full-conditionals	O
just	O
the	O
unnormalized	O
joint	O
the	O
advantage	O
of	O
slice	B
sampling	I
over	O
mh	B
is	O
that	O
it	O
does	O
not	O
need	O
a	O
user-specified	O
proposal	B
distribution	I
it	O
does	O
require	O
a	O
specification	O
of	O
the	O
width	O
of	O
the	O
stepping	B
out	I
interval	O
figure	O
illustrates	O
the	O
algorithm	O
in	O
action	B
on	O
a	O
synthetic	O
problem	O
figure	O
illustrates	O
its	O
behavior	O
on	O
a	O
slightly	O
harder	O
problem	O
namely	O
binomial	B
logistic	B
regression	B
the	O
model	O
has	O
the	O
form	O
yi	O
binni	O
logit	B
we	O
use	O
a	O
vague	O
gaussian	B
prior	O
for	O
the	O
j	O
s	O
figure	O
shows	O
a	O
grid-based	O
approximation	O
to	O
the	O
posterior	O
and	O
figure	O
shows	O
a	O
sample-based	O
approximation	O
in	O
this	O
example	O
the	O
grid	O
is	O
faster	O
to	O
compute	O
but	O
for	O
any	O
problem	O
with	O
more	O
than	O
dimensions	O
the	O
grid	O
approach	O
is	O
infeasible	O
swendsen	B
wang	I
consider	O
an	O
ising	B
model	I
of	O
the	O
following	O
form	O
z	O
e	O
fexe	O
px	O
where	O
xe	O
xj	O
for	O
edge	O
e	O
j	O
xi	O
and	O
the	O
edge	O
factor	B
fe	O
is	O
defined	O
by	O
ej	O
where	O
j	O
is	O
the	O
edge	O
strength	O
gibbs	B
sampling	I
in	O
such	O
models	O
can	O
be	O
slow	O
when	O
e	O
j	O
j	O
is	O
large	O
in	O
absolute	O
value	O
because	O
neighboring	O
states	O
can	O
be	O
highly	O
correlated	O
the	O
swendsen	B
wang	I
algorithm	O
and	O
wang	O
is	O
a	O
auxiliary	O
variable	O
mcmc	B
sampler	O
which	O
mixes	O
much	O
faster	O
at	O
least	O
for	O
the	O
case	O
of	O
attractive	O
or	O
ferromagnetic	O
models	O
with	O
j	O
e	O
j	O
ej	O
suppose	O
we	O
introduce	O
auxiliary	O
binary	O
variables	O
one	O
per	O
edge	O
these	O
are	O
called	O
bond	B
variables	I
and	O
will	O
be	O
denoted	O
by	O
z	O
we	O
then	O
define	O
an	O
extended	O
model	O
px	O
z	O
of	O
the	O
form	O
px	O
z	O
gexe	O
ze	O
e	O
where	O
ze	O
and	O
we	O
define	O
the	O
new	O
factor	B
as	O
follows	O
gexe	O
ze	O
ej	O
e	O
j	O
ej	O
e	O
j	O
e	O
j	O
e	O
j	O
e	O
j	O
e	O
j	O
and	O
gexe	O
ze	O
it	O
is	O
clear	O
that	O
gexe	O
ze	O
fexe	O
our	O
presentation	O
of	O
the	O
method	O
is	O
based	O
on	O
some	O
notes	O
by	O
david	O
mackay	O
available	O
from	O
httpwww	O
inference	B
auxiliary	O
variable	O
mcmc	B
figure	O
illustration	O
of	O
the	O
swendsen	B
wang	I
algorithm	O
on	O
a	O
grid	O
used	O
with	O
kind	O
permission	O
of	O
kevin	O
tang	O
z	O
px	O
z	O
px	O
so	O
if	O
we	O
can	O
sample	O
from	O
this	O
extended	O
model	O
we	O
can	O
just	O
and	O
hence	O
that	O
throw	O
away	O
the	O
z	O
samples	B
and	O
get	O
valid	O
x	O
samples	B
from	O
the	O
original	O
distribution	O
fortunately	O
it	O
is	O
easy	O
to	O
apply	O
gibbs	B
sampling	I
to	O
this	O
extended	O
model	O
the	O
full	B
conditional	I
pzx	O
factorizes	O
over	O
the	O
edges	B
since	O
the	O
bond	B
variables	I
are	O
conditionally	B
independent	I
given	O
the	O
node	O
variables	O
furthermore	O
the	O
full	B
conditional	I
pzexe	O
is	O
simple	O
to	O
compute	O
if	O
the	O
nodes	B
on	O
either	O
end	O
of	O
the	O
edge	O
are	O
in	O
the	O
same	O
state	B
xj	O
we	O
set	O
the	O
bond	O
ze	O
to	O
with	O
probability	O
p	O
e	O
otherwise	O
we	O
set	O
it	O
to	O
in	O
figure	O
right	O
the	O
bonds	O
that	O
could	O
be	O
turned	O
on	O
their	O
corresponding	O
nodes	B
are	O
in	O
the	O
same	O
state	B
are	O
represented	O
in	O
figure	O
right	O
the	O
bonds	O
that	O
are	O
randomly	O
turned	O
on	O
are	O
by	O
dotted	O
edges	B
represented	O
by	O
solid	O
edges	B
to	O
sample	O
pxz	O
we	O
proceed	O
as	O
follows	O
find	O
the	O
connected	O
components	O
defined	O
by	O
the	O
graph	B
induced	O
by	O
the	O
bonds	O
that	O
are	O
turned	O
on	O
that	O
a	O
connected	O
component	O
may	O
consist	O
of	O
a	O
singleton	O
node	O
pick	O
one	O
of	O
these	O
components	O
uniformly	O
at	O
random	O
all	O
the	O
nodes	B
in	O
each	O
such	O
component	O
must	O
have	O
the	O
same	O
state	B
since	O
the	O
off-diagonal	O
terms	O
in	O
the	O
gexe	O
ze	O
factor	B
are	O
pick	O
a	O
state	B
uniformly	O
at	O
random	O
and	O
force	O
all	O
the	O
variables	O
in	O
this	O
component	O
to	O
adopt	O
this	O
new	O
state	B
this	O
is	O
illustrated	O
in	O
figure	O
left	O
where	O
the	O
green	O
square	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
denotes	O
the	O
selected	O
connected	O
component	O
and	O
we	O
choose	O
to	O
force	O
all	O
nodes	B
within	O
in	O
to	O
enter	O
the	O
white	O
state	B
the	O
validity	O
of	O
this	O
algorithm	O
is	O
left	O
as	O
an	O
exercise	O
as	O
is	O
the	O
extension	B
to	O
handle	O
local	B
evidence	B
and	O
non-stationary	O
potentials	O
it	O
should	O
be	O
intuitively	O
clear	O
that	O
swendsen	B
wang	I
makes	O
much	O
larger	O
moves	O
through	O
the	O
state	B
space	I
than	O
gibbs	B
sampling	I
in	O
fact	O
sw	O
mixes	O
much	O
faster	O
than	O
gibbs	B
sampling	I
on	O
lattice	B
ising	O
models	O
for	O
a	O
variety	O
of	O
values	O
of	O
the	O
coupling	O
parameter	B
provided	O
j	O
more	O
precisely	O
let	O
the	O
edge	O
strength	O
be	O
parameterized	O
by	O
jt	O
where	O
t	O
is	O
a	O
computational	O
temperature	B
for	O
large	O
t	O
the	O
nodes	B
are	O
roughly	O
independent	O
so	O
both	O
methods	O
work	O
equally	O
well	O
however	O
as	O
t	O
approaches	O
a	O
critical	B
temperature	B
tc	O
the	O
typical	O
states	O
of	O
the	O
system	O
have	O
very	O
long	O
correlation	O
lengths	O
and	O
gibbs	B
sampling	I
takes	O
a	O
very	O
long	O
time	O
to	O
generate	O
independent	O
samples	B
as	O
the	O
temperature	B
continues	O
to	O
drop	O
the	O
typical	O
states	O
are	O
either	O
all	O
on	O
or	O
all	O
off	O
the	O
frequency	O
with	O
which	O
gibbs	B
sampling	I
moves	O
between	O
these	O
two	O
modes	O
is	O
exponentiall	O
small	O
by	O
contrast	O
sw	O
mixes	O
rapidly	O
at	O
all	O
temperatures	O
unfortunately	O
if	O
any	O
of	O
the	O
edge	O
weights	O
are	O
negative	O
j	O
the	O
system	O
is	O
frustrated	B
and	O
there	O
are	O
exponentially	O
many	O
modes	O
even	O
at	O
low	O
temperature	B
sw	O
does	O
not	O
work	O
very	O
well	O
in	O
this	O
setting	O
since	O
it	O
tries	O
to	O
force	O
many	O
neighboring	O
variables	O
to	O
have	O
the	O
same	O
state	B
in	O
fact	O
computation	O
in	O
this	O
regime	O
is	O
provably	O
hard	O
for	O
any	O
algorithm	O
and	O
sinclair	O
hybridhamiltonian	O
mcmc	B
in	O
this	O
section	O
we	O
briefly	O
mention	O
a	O
way	O
to	O
perform	O
mcmc	B
sampling	O
for	O
continuous	O
state	B
spaces	O
for	O
which	O
we	O
can	O
compute	O
the	O
gradient	O
of	O
the	O
log-posterior	O
this	O
is	O
the	O
case	O
in	O
neural	B
network	I
models	O
for	O
example	O
the	O
basic	O
idea	O
is	O
to	O
think	O
of	O
the	O
parameters	O
as	O
a	O
particle	O
in	O
space	O
and	O
to	O
create	O
auxiliary	B
variables	I
which	O
represent	O
the	O
momentum	B
of	O
this	O
particle	O
we	O
then	O
update	O
this	O
parameter	B
momentum	B
pair	O
according	O
to	O
certain	O
rules	B
e	O
g	O
et	O
al	O
neal	O
mackay	O
neal	O
for	O
details	O
the	O
resulting	O
method	O
is	O
called	O
hybrid	B
mcmc	B
or	O
hamiltonian	B
mcmc	B
the	O
two	O
main	O
parameters	O
that	O
the	O
user	O
must	O
specify	O
are	O
how	O
many	O
leapfrog	B
steps	I
to	O
take	O
when	O
updating	O
the	O
position	O
momentum	B
and	O
how	O
big	O
to	O
make	O
these	O
steps	O
performance	O
can	O
be	O
quite	O
sensitive	O
to	O
these	O
parameters	O
see	O
and	O
gelman	O
for	O
a	O
recent	O
way	O
to	O
set	O
them	O
automatically	O
this	O
method	O
can	O
be	O
combined	O
with	O
stochastic	B
gradient	B
descent	I
in	O
order	O
to	O
handle	O
large	O
datasets	O
as	O
explained	O
in	O
et	O
al	O
recently	O
a	O
more	O
powerful	O
extension	B
of	O
this	O
method	O
has	O
been	O
developed	O
that	O
exploits	O
second	B
order	I
gradient	O
information	B
see	O
et	O
al	O
for	O
details	O
annealing	B
methods	O
many	O
distributions	O
are	O
multimodal	O
and	O
hence	O
hard	O
to	O
sample	O
from	O
however	O
by	O
analogy	O
to	O
the	O
way	O
metals	O
are	O
heated	O
up	O
and	O
then	O
cooled	O
down	O
in	O
order	O
to	O
make	O
the	O
molecules	O
align	O
we	O
can	O
imagine	O
using	O
a	O
computational	O
temperature	B
parameter	B
to	O
smooth	O
out	O
a	O
distribution	O
gradually	O
cooling	O
it	O
to	O
recover	O
the	O
original	O
bumpy	O
distribution	O
we	O
first	O
explain	O
this	O
idea	O
in	O
more	O
detail	O
in	O
the	O
context	O
of	O
an	O
algorithm	O
for	O
map	O
estimation	O
we	O
then	O
discuss	O
extensions	O
to	O
the	O
sampling	O
case	O
annealing	B
methods	O
temp	O
temp	O
x	O
y	O
x	O
y	O
x	O
figure	O
an	O
energy	O
surface	O
at	O
different	O
temperatures	O
note	O
the	O
different	O
vertical	O
scales	O
t	O
figure	O
generated	O
by	O
sademopeaks	O
t	O
simulated	B
annealing	B
simulated	B
annealing	B
et	O
al	O
is	O
a	O
stochastic	B
algorithm	I
that	O
attempts	O
to	O
find	O
the	O
global	O
optimum	O
of	O
a	O
black-box	B
function	O
f	O
it	O
is	O
closely	O
related	O
to	O
the	O
metropolishastings	O
algorithm	O
for	O
generating	O
samples	B
from	O
a	O
probability	O
distribution	O
which	O
we	O
discussed	O
in	O
section	O
sa	O
can	O
be	O
used	O
for	O
both	O
discrete	B
and	O
continuous	O
optimization	B
the	O
method	O
is	O
inspired	O
by	O
statistical	O
physics	O
the	O
key	O
quantity	O
is	O
the	O
boltzmann	B
distribution	I
which	O
specifies	O
that	O
the	O
probability	O
of	O
being	O
in	O
any	O
particular	O
state	B
x	O
is	O
given	O
by	O
px	O
exp	O
f	O
where	O
f	O
is	O
the	O
energy	O
of	O
the	O
system	O
and	O
t	O
is	O
the	O
computational	O
temperature	B
as	O
the	O
temperature	B
approaches	O
the	O
system	O
is	O
cooled	O
the	O
system	O
spends	O
more	O
and	O
more	O
time	O
in	O
its	O
minimum	O
energy	O
probable	O
state	B
figure	O
gives	O
an	O
example	O
of	O
a	O
function	O
at	O
different	O
temperatures	O
at	O
high	O
temperatures	O
t	O
the	O
surface	O
is	O
approximately	O
flat	O
and	O
hence	O
it	O
is	O
easy	O
to	O
move	O
around	O
to	O
avoid	O
local	O
optima	O
as	O
the	O
temperature	B
cools	O
the	O
largest	O
peaks	O
become	O
larger	O
and	O
the	O
smallest	O
peaks	O
disappear	O
by	O
cooling	O
slowly	O
enough	O
it	O
is	O
possible	O
to	O
track	O
the	O
largest	O
peak	O
and	O
thus	O
find	O
the	O
global	O
optimum	O
this	O
is	O
an	O
example	O
of	O
a	O
continuation	B
method	I
we	O
can	O
generate	O
an	O
algorithm	O
from	O
this	O
as	O
follows	O
at	O
each	O
step	O
sample	O
a	O
new	O
state	B
according	O
to	O
some	O
proposal	B
distribution	I
q	O
for	O
real-valued	O
parameters	O
this	O
is	O
often	O
simply	O
a	O
xk	O
where	O
n	O
for	O
discrete	B
optimization	B
other	O
random	B
walk	I
proposal	I
kinds	O
of	O
local	O
moves	O
must	O
be	O
defined	O
having	O
proposed	O
a	O
new	O
state	B
we	O
compute	O
exp	O
f	O
we	O
then	O
accept	B
the	O
new	O
state	B
set	O
with	O
probability	O
otherwise	O
we	O
stay	O
in	O
the	O
current	O
state	B
set	O
xk	O
this	O
means	O
that	O
if	O
the	O
new	O
state	B
has	O
lower	O
energy	O
more	O
probable	O
we	O
will	O
definitely	O
accept	B
it	O
but	O
it	O
it	O
has	O
higher	O
energy	O
less	O
probable	O
we	O
might	O
still	O
accept	B
depending	O
on	O
the	O
current	O
temperature	B
thus	O
the	O
algorithm	O
allows	O
down-hill	O
moves	O
in	O
probability	O
space	O
in	O
energy	O
space	O
but	O
less	O
frequently	O
as	O
the	O
temperature	B
drops	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
temperature	B
vs	O
iteration	O
energy	O
vs	O
iteration	O
figure	O
a	O
run	O
of	O
simulated	B
annealing	B
on	O
the	O
energy	O
surface	O
in	O
figure	O
iteration	O
energy	O
vs	O
iteration	O
figure	O
generated	O
by	O
sademopeaks	O
temperature	B
vs	O
iter	O
temp	O
iter	O
temp	O
y	O
x	O
y	O
x	O
figure	O
histogram	B
of	O
samples	B
from	O
the	O
annealed	O
posterior	O
at	O
different	O
time	O
points	O
produced	O
by	O
simulated	B
annealing	B
on	O
the	O
energy	O
surface	O
shown	O
in	O
figure	O
note	O
that	O
at	O
cold	O
temperatures	O
most	O
of	O
the	O
samples	B
are	O
concentrated	O
near	O
the	O
peak	O
at	O
figure	O
generated	O
by	O
sademopeaks	O
the	O
rate	B
at	O
which	O
the	O
temperature	B
changes	O
over	O
time	O
is	O
called	O
the	O
cooling	B
schedule	B
it	O
has	O
been	O
shown	O
et	O
al	O
that	O
if	O
one	O
cools	O
sufficiently	O
slowly	O
the	O
algorithm	O
will	O
provably	O
find	O
the	O
global	O
optimum	O
however	O
it	O
is	O
not	O
clear	O
what	O
sufficient	O
slowly	O
means	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
an	O
exponential	B
cooling	B
schedule	B
of	O
the	O
following	O
form	O
tk	O
k	O
where	O
is	O
the	O
initial	O
temperature	B
and	O
c	O
is	O
the	O
cooling	O
rate	B
c	O
see	O
figure	O
for	O
a	O
plot	O
of	O
this	O
cooling	B
schedule	B
cooling	O
too	O
quickly	O
means	O
one	O
can	O
get	O
stuck	O
in	O
a	O
local	O
maximum	O
but	O
cooling	O
too	O
slowly	O
just	O
wastes	O
time	O
the	O
best	O
cooling	B
schedule	B
is	O
difficult	O
to	O
determine	O
this	O
is	O
one	O
of	O
the	O
main	O
drawbacks	O
of	O
simulated	B
annealing	B
figure	O
shows	O
an	O
example	O
of	O
simulated	B
annealing	B
applied	O
to	O
the	O
function	O
in	O
figure	O
using	O
a	O
random	B
walk	I
proposal	I
we	O
see	O
that	O
the	O
method	O
stochastically	O
reduces	O
the	O
energy	O
over	O
time	O
figures	O
illustrate	O
histogram	B
of	O
samples	B
drawn	O
from	O
the	O
cooled	O
probability	O
distribution	O
over	O
time	O
we	O
see	O
that	O
most	O
of	O
the	O
samples	B
are	O
concentrated	O
near	O
the	O
global	O
maximum	O
when	O
the	O
algorithm	O
has	O
converged	O
we	O
just	O
return	O
the	O
largest	O
value	O
found	O
annealing	B
methods	O
annealed	B
importance	B
sampling	I
we	O
now	O
describe	O
a	O
method	O
known	O
as	O
annealed	B
importance	B
sampling	I
that	O
combines	O
ideas	O
from	O
simulated	B
annealing	B
and	O
importance	B
sampling	I
in	O
order	O
to	O
draw	O
independent	O
samples	B
from	O
difficult	O
multimodal	O
distributions	O
suppose	O
we	O
want	O
to	O
sample	O
from	O
but	O
we	O
cannot	O
do	O
so	O
easily	O
for	O
example	O
this	O
might	O
represent	O
a	O
multimodal	O
posterior	O
suppose	O
however	O
that	O
there	O
is	O
an	O
easier	O
distribution	O
which	O
we	O
can	O
sample	O
from	O
call	O
it	O
pnx	O
fnx	O
for	O
example	O
this	O
might	O
be	O
the	O
prior	O
we	O
can	O
now	O
construct	O
a	O
sequence	O
of	O
intermediate	O
distributions	O
than	O
move	O
slowly	O
from	O
pn	O
to	O
as	O
follows	O
fjx	O
j	O
j	O
where	O
n	O
where	O
j	O
is	O
an	O
inverse	O
temperature	B
this	O
to	O
the	O
scheme	O
used	O
by	O
simulated	B
annealing	B
which	O
has	O
the	O
form	O
fjx	O
j	O
this	O
makes	O
it	O
hard	O
to	O
sample	O
from	O
pn	O
furthermore	O
suppose	O
we	O
have	O
a	O
series	O
of	O
markov	B
chains	O
tjx	O
x	O
to	O
which	O
leave	O
each	O
pj	O
invariant	B
given	O
this	O
we	O
can	O
sample	O
x	O
from	O
by	O
first	O
sampling	O
a	O
sequence	O
z	O
as	O
follows	O
sample	O
zn	O
pn	O
sample	O
zn	O
tn	O
sample	O
finally	O
we	O
set	O
x	O
and	O
give	O
it	O
weight	O
w	O
fn	O
fnzn	O
fn	O
fn	O
this	O
can	O
be	O
shown	O
to	O
be	O
correct	O
by	O
viewing	O
the	O
algorithm	O
as	O
a	O
form	O
of	O
importance	B
sampling	I
in	O
an	O
extended	O
state	B
space	I
z	O
zn	O
consider	O
the	O
following	O
distribution	O
on	O
this	O
state	B
space	I
pz	O
f	O
tn	O
zn	O
where	O
tj	O
is	O
the	O
reversal	O
of	O
tj	O
tjz	O
f	O
so	O
we	O
can	O
safely	O
just	O
use	O
the	O
part	O
of	O
these	O
it	O
is	O
clear	O
that	O
sequences	O
to	O
recover	O
the	O
original	O
ditribution	O
now	O
consider	O
the	O
proposal	B
distribution	I
defined	O
by	O
the	O
algorithm	O
qz	O
gz	O
fnzn	O
zn	O
t	O
one	O
can	O
show	O
that	O
the	O
importance	B
weights	I
w	O
f	O
are	O
given	O
by	O
equation	O
parallel	B
tempering	I
another	O
way	O
to	O
combine	O
mcmc	B
and	O
annealing	B
is	O
to	O
run	O
multiple	O
chains	O
in	O
parallel	O
at	O
different	O
temperatures	O
and	O
allow	O
one	O
chain	O
to	O
sample	O
from	O
another	O
chain	O
at	O
a	O
neighboring	O
temperature	B
in	O
this	O
way	O
the	O
high	O
temperature	B
chain	O
can	O
make	O
long	O
distance	O
moves	O
through	O
the	O
state	B
space	I
and	O
have	O
this	O
influence	O
lower	O
temperature	B
chains	O
this	O
is	O
known	O
as	O
parallel	B
tempering	I
see	O
e	O
g	O
and	O
deem	O
for	O
details	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
approximating	O
the	O
marginal	B
likelihood	B
the	O
marginal	B
likelihood	B
pdm	O
is	O
a	O
key	O
quantity	O
for	O
bayesian	B
model	B
selection	I
and	O
is	O
given	O
by	O
pdm	O
pd	O
m	O
unfortunately	O
this	O
integral	O
is	O
often	O
intractable	O
to	O
compute	O
for	O
example	O
if	O
we	O
have	O
non	O
conjugate	O
in	O
this	O
section	O
we	O
briefly	O
discuss	O
some	O
ways	O
to	O
priors	O
andor	O
we	O
have	O
hidden	B
variables	I
approximate	O
this	O
expression	O
using	O
monte	B
carlo	I
see	O
and	O
meng	O
for	O
a	O
more	O
extensive	O
review	O
the	O
candidate	B
method	I
there	O
is	O
a	O
simple	O
method	O
for	O
approximating	O
the	O
marginal	B
likelihood	B
known	O
as	O
the	O
candidate	B
method	I
this	O
exploits	O
the	O
following	O
identity	O
pdm	O
pd	O
m	O
p	O
m	O
this	O
holds	O
for	O
any	O
value	O
of	O
once	O
we	O
have	O
picked	O
some	O
value	O
we	O
can	O
evaluate	O
pd	O
m	O
and	O
p	O
quite	O
easily	O
if	O
we	O
have	O
some	O
estimate	O
of	O
the	O
posterior	O
near	O
we	O
can	O
then	O
evaluate	O
the	O
denominator	O
as	O
well	O
this	O
posterior	O
is	O
often	O
approximated	O
using	O
mcmc	B
the	O
flaw	O
with	O
this	O
method	O
is	O
that	O
it	O
relies	O
on	O
the	O
assumption	O
that	O
p	O
m	O
has	O
marginalized	O
over	O
all	O
the	O
modes	O
of	O
the	O
posterior	O
which	O
in	O
practice	O
is	O
rarely	O
possible	O
consequently	O
the	O
method	O
can	O
give	O
very	O
inaccurate	O
results	O
in	O
practice	O
harmonic	B
mean	B
estimate	O
newton	O
and	O
raftery	O
proposed	O
a	O
simple	O
method	O
for	O
approximating	O
pd	O
using	O
the	O
output	O
of	O
mcmc	B
as	O
follows	O
s	O
where	O
s	O
p	O
this	O
expression	O
is	O
the	O
harmonic	B
mean	B
of	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
each	O
sample	O
the	O
theoretical	O
correctness	O
of	O
this	O
expression	O
follows	O
from	O
the	O
following	O
identity	O
pd	O
s	O
pd	O
p	O
pd	O
pd	O
pd	O
d	O
pd	O
p	O
pd	O
unfortunately	O
in	O
practice	O
this	O
method	O
works	O
very	O
poorly	O
indeed	O
radford	O
neal	O
called	O
this	O
the	O
worst	O
monte	B
carlo	I
method	O
ever	O
the	O
reason	O
it	O
is	O
so	O
bad	O
is	O
that	O
it	O
depends	O
only	O
on	O
samples	B
drawn	O
from	O
the	O
posterior	O
but	O
the	O
posterior	O
is	O
often	O
very	O
insensitive	O
to	O
the	O
prior	O
whereas	O
the	O
marginal	B
likelihood	B
is	O
not	O
we	O
only	O
mention	O
this	O
method	O
in	O
order	O
to	O
warn	O
against	O
its	O
use	O
we	O
present	O
a	O
better	O
method	O
below	O
source	O
te-carlo-method-ever	O
approximating	O
the	O
marginal	B
likelihood	B
annealed	B
importance	B
sampling	I
f	O
zn	O
f	O
gzdz	O
gz	O
gzdz	O
gzdz	O
eq	O
we	O
can	O
use	O
annealed	B
importance	B
sampling	I
to	O
evaluate	O
a	O
ratio	O
of	O
partition	O
functions	O
notice	O
that	O
gzdz	O
hence	O
f	O
and	O
zn	O
fnxdx	O
f	O
gz	O
s	O
ws	O
if	O
fn	O
is	O
a	O
prior	O
and	O
is	O
the	O
posterior	O
we	O
can	O
estimate	O
zn	O
pd	O
using	O
the	O
above	O
equation	O
provided	O
the	O
prior	O
has	O
a	O
known	O
normalization	O
constant	O
this	O
is	O
generally	O
considered	O
the	O
method	O
of	O
choice	O
for	O
evaluating	O
difficult	O
partition	O
functions	O
exercises	O
exercise	O
gibbs	B
sampling	I
from	O
a	O
gaussian	B
suppose	O
x	O
n	O
where	O
and	O
derive	O
the	O
full	B
conditionals	O
and	O
implement	O
the	O
algorithm	O
and	O
plot	O
the	O
marginals	O
and	O
as	O
histograms	O
superimpose	O
a	O
plot	O
of	O
the	O
exact	O
marginals	O
exercise	O
gibbs	B
sampling	I
for	O
a	O
gaussian	B
mixture	B
model	I
consider	O
applying	O
gibbs	B
sampling	I
to	O
a	O
univariate	O
mixture	B
of	I
gaussians	I
as	O
in	O
section	O
derive	O
the	O
expressions	O
for	O
the	O
full	B
conditionals	O
hint	O
if	O
we	O
know	O
zn	O
j	O
then	O
j	O
gets	O
connected	O
to	O
xn	O
but	O
all	O
other	O
values	O
of	O
i	O
for	O
all	O
i	O
j	O
are	O
irrelevant	O
is	O
an	O
example	O
of	O
context-specific	O
independence	O
where	O
the	O
structure	O
of	O
the	O
graph	B
simplifies	O
once	O
we	O
have	O
assigned	O
values	O
to	O
some	O
of	O
the	O
nodes	B
hence	O
given	O
all	O
the	O
zn	O
values	O
the	O
posteriors	O
of	O
the	O
s	O
should	O
be	O
independent	O
so	O
the	O
conditional	O
of	O
j	O
should	O
be	O
independent	O
of	O
j	O
for	O
j	O
exercise	O
gibbs	B
sampling	I
from	O
the	O
potts	B
model	I
modify	O
the	O
code	O
in	O
gibbsdemoising	O
to	O
draw	O
samples	B
from	O
a	O
potts	O
prior	O
at	O
different	O
temperatures	O
as	O
in	O
figure	O
exercise	O
full	B
conditionals	O
for	O
hierarchical	O
model	O
of	O
gaussian	B
means	O
let	O
us	O
reconsider	O
the	O
gaussian-gaussian	O
model	O
parameters	O
j	O
that	O
we	O
use	O
the	O
following	O
conjugate	B
priors	I
on	O
the	O
hyper-parameters	B
in	O
section	O
for	O
modelling	O
multiple	O
related	O
mean	B
in	O
this	O
exercise	O
we	O
derive	O
a	O
gibbs	B
sampler	I
instead	O
of	O
using	O
eb	B
suppose	O
following	O
n	O
ig	O
ig	O
chapter	O
markov	B
chain	I
monte	B
carlo	I
inference	B
we	O
can	O
set	O
to	O
uninformative	B
values	O
given	O
this	O
model	O
specification	O
show	O
that	O
the	O
full	B
conditionals	O
for	O
and	O
the	O
j	O
are	O
as	O
follows	O
p	O
d	O
d	O
p	O
j	O
j	O
njxj	O
nj	O
p	O
ig	O
d	O
p	O
ig	O
nj	O
j	O
j	O
exercise	O
gibbs	B
sampling	I
for	O
robust	B
linear	B
regression	B
with	O
a	O
student	B
t	I
likelihood	B
modify	O
the	O
em	B
algorithm	O
in	O
exercise	O
to	O
perform	O
gibbs	B
sampling	I
for	O
pw	O
zd	O
exercise	O
gibbs	B
sampling	I
for	O
probit	B
regression	B
modify	O
the	O
em	B
algorithm	O
in	O
section	O
to	O
perform	O
gibbs	B
sampling	I
for	O
pw	O
zd	O
hint	O
we	O
can	O
sample	O
from	O
a	O
truncated	B
gaussian	B
n	O
z	O
b	O
in	O
two	O
steps	O
first	O
sample	O
u	O
u	O
then	O
set	O
z	O
exercise	O
gibbs	B
sampling	I
for	O
logistic	B
regression	B
with	O
the	O
student	O
approximation	O
derive	O
the	O
full	B
conditionals	O
for	O
the	O
joint	O
model	O
defined	O
by	O
equations	O
to	O
clustering	B
introduction	O
clustering	B
is	O
the	O
process	O
of	O
grouping	O
similar	B
objects	O
together	O
there	O
are	O
two	O
kinds	O
of	O
inputs	O
we	O
might	O
use	O
in	O
similarity-based	B
clustering	B
the	O
input	O
to	O
the	O
algorithm	O
is	O
an	O
n	O
n	O
dissimilarity	B
matrix	I
or	O
distance	B
matrix	I
d	O
in	O
feature-based	B
clustering	B
the	O
input	O
to	O
the	O
algorithm	O
is	O
an	O
n	O
d	O
feature	B
matrix	I
or	O
design	B
matrix	I
x	O
similarity-based	B
clustering	B
has	O
the	O
advantage	O
that	O
it	O
allows	O
for	O
easy	O
inclusion	O
of	O
domain-specific	O
similarity	O
or	O
kernel	B
functions	O
featurebased	O
clustering	B
has	O
the	O
advantage	O
that	O
it	O
is	O
applicable	O
to	O
raw	O
potentially	O
noisy	O
data	O
we	O
will	O
see	O
examples	O
of	O
both	O
below	O
in	O
addition	O
to	O
the	O
two	O
types	O
of	O
input	O
there	O
are	O
two	O
possible	O
types	O
of	O
output	O
flat	O
clustering	B
also	O
called	O
partitional	B
clustering	B
where	O
we	O
partition	O
the	O
objects	O
into	O
disjoint	O
sets	O
and	O
hierarchical	B
clustering	B
where	O
we	O
create	O
a	O
nested	O
tree	B
of	O
partitions	O
we	O
will	O
discuss	O
both	O
of	O
these	O
below	O
not	O
surprisingly	O
flat	O
clusterings	O
are	O
usually	O
faster	O
to	O
create	O
d	O
for	O
flat	O
vs	O
on	O
log	O
n	O
for	O
hierarchical	O
but	O
hierarchical	O
clusterings	O
are	O
often	O
more	O
useful	O
furthermore	O
most	O
hierarchical	B
clustering	B
algorithms	O
are	O
deterministic	O
and	O
do	O
not	O
require	O
the	O
specification	O
of	O
k	O
the	O
number	O
of	O
clusters	B
whereas	O
most	O
flat	O
clustering	B
algorithms	O
are	O
sensitive	O
to	O
the	O
initial	O
conditions	O
and	O
require	O
some	O
model	B
selection	I
method	O
for	O
k	O
will	O
discuss	O
how	O
to	O
choose	O
k	O
in	O
more	O
detail	O
below	O
the	O
final	O
distinction	O
we	O
will	O
make	O
in	O
this	O
chapter	O
is	O
whether	O
the	O
method	O
is	O
based	O
on	O
a	O
probabilistic	O
model	O
or	O
not	O
one	O
might	O
wonder	O
why	O
we	O
even	O
bother	O
discussing	O
non-probabilistic	O
methods	O
for	O
clustering	B
the	O
reason	O
is	O
two-fold	O
first	O
they	O
are	O
widely	O
used	O
so	O
readers	O
should	O
know	O
about	O
them	O
second	O
they	O
often	O
contain	O
good	O
ideas	O
which	O
can	O
be	O
used	O
to	O
speed	O
up	O
inference	B
in	O
a	O
probabilistic	O
models	O
measuring	O
a	O
dissimilarity	B
matrix	I
d	O
is	O
a	O
matrix	O
where	O
dii	O
and	O
dij	O
is	O
a	O
measure	O
of	O
distance	O
between	O
objects	O
i	O
and	O
j	O
subjectively	O
judged	O
dissimilarities	O
are	O
seldom	O
distances	O
in	O
the	O
strict	B
sense	O
since	O
the	O
triangle	B
inequality	I
dij	O
dik	O
djk	O
often	O
does	O
not	O
hold	O
some	O
algorithms	O
require	O
d	O
to	O
be	O
a	O
true	O
distance	B
matrix	I
but	O
many	O
do	O
not	O
if	O
we	O
have	O
a	O
similarity	O
matrix	O
s	O
we	O
can	O
convert	O
it	O
to	O
a	O
dissimilarity	B
matrix	I
by	O
applying	O
any	O
monotonically	O
decreasing	O
function	O
e	O
g	O
d	O
maxs	O
s	O
the	O
most	O
common	O
way	O
to	O
define	O
dissimilarity	O
between	O
objects	O
is	O
in	O
terms	O
of	O
the	O
dissimilarity	O
chapter	O
clustering	B
of	O
their	O
attributes	B
jxij	O
some	O
common	O
attribute	O
dissimilarity	O
functions	O
are	O
as	O
follows	O
squared	O
distance	O
jxij	O
of	O
course	O
this	O
only	O
makes	O
sense	O
if	O
attribute	O
j	O
is	O
real-valued	O
squared	O
distance	O
strongly	O
emphasizes	O
large	O
differences	O
differences	O
are	O
squared	O
a	O
more	O
robust	B
alternative	O
is	O
to	O
use	O
an	O
distance	O
jxij	O
this	O
is	O
also	O
called	O
city	B
block	I
distance	I
since	O
in	O
the	O
distance	O
can	O
be	O
computed	O
by	O
counting	O
how	O
many	O
rows	O
and	O
columns	O
we	O
have	O
to	O
move	O
horizontally	O
and	O
vertically	O
to	O
get	O
from	O
xi	O
to	O
if	O
xi	O
is	O
a	O
vector	O
a	O
time-series	O
of	O
real-valued	O
data	O
it	O
is	O
common	O
to	O
use	O
the	O
correlation	O
j	O
coefficient	O
section	O
jxij	O
corr	O
so	O
clustering	B
based	O
on	O
correlation	O
and	O
hence	O
is	O
equivalent	O
to	O
clustering	B
based	O
on	O
squared	O
distance	O
if	O
the	O
data	O
is	O
standardized	B
then	O
corr	O
for	O
ordinal	B
variables	I
such	O
as	O
medium	O
high	O
it	O
is	O
standard	O
to	O
encode	O
the	O
values	O
as	O
real-valued	O
numbers	O
say	O
if	O
there	O
are	O
possible	O
values	O
one	O
can	O
then	O
apply	O
any	O
dissimilarity	O
function	O
for	O
quantitative	O
variables	O
such	O
as	O
squared	O
distance	O
for	O
categorical	B
variables	I
such	O
as	O
green	O
blue	O
we	O
usually	O
assign	O
a	O
distance	O
of	O
if	O
the	O
features	B
are	O
different	O
and	O
a	O
distance	O
of	O
otherwise	O
summing	O
up	O
over	O
all	O
the	O
categorical	B
features	B
gives	O
xi	O
ixij	O
this	O
is	O
called	O
the	O
hamming	B
distance	I
evaluating	O
the	O
output	O
of	O
clustering	B
methods	O
the	O
validation	O
of	O
clustering	B
structures	O
is	O
the	O
most	O
difficult	O
and	O
frustrating	O
part	O
of	O
cluster	O
analysis	O
without	O
a	O
strong	O
effort	O
in	O
this	O
direction	O
cluster	O
analysis	O
will	O
remain	O
a	O
black	O
art	O
accessible	O
only	O
to	O
those	O
true	O
believers	O
who	O
have	O
experience	O
and	O
great	O
courage	O
jain	O
and	O
dubes	O
and	O
dubes	O
introduction	O
figure	O
three	O
clusters	B
with	O
labeled	O
objects	O
inside	O
based	O
on	O
figure	O
of	O
et	O
al	O
clustering	B
is	O
an	O
unupervised	O
learning	B
technique	O
so	O
it	O
is	O
hard	O
to	O
evaluate	O
the	O
quality	O
of	O
the	O
output	O
of	O
any	O
given	O
method	O
if	O
we	O
use	O
probabilistic	O
models	O
we	O
can	O
always	O
evaluate	O
the	O
likelihood	B
of	O
a	O
test	O
set	O
but	O
this	O
has	O
two	O
drawbacks	O
first	O
it	O
does	O
not	O
directly	O
assess	O
any	O
clustering	B
that	O
is	O
discovered	O
by	O
the	O
model	O
and	O
second	O
it	O
does	O
not	O
apply	O
to	O
non-probabilistic	O
methods	O
so	O
now	O
we	O
discuss	O
some	O
performance	O
measures	O
not	O
based	O
on	O
likelihood	B
intuitively	O
the	O
goal	O
of	O
clustering	B
is	O
to	O
assign	O
points	O
that	O
are	O
similar	B
to	O
the	O
same	O
cluster	O
and	O
to	O
ensure	O
that	O
points	O
that	O
are	O
dissimilar	O
are	O
in	O
different	O
clusters	B
there	O
are	O
several	O
ways	O
of	O
measuring	O
these	O
quantities	O
e	O
g	O
see	O
and	O
dubes	O
kaufman	O
and	O
rousseeuw	O
however	O
these	O
internal	O
criteria	O
may	O
be	O
of	O
limited	O
use	O
an	O
alternative	O
is	O
to	O
rely	O
on	O
some	O
external	O
form	O
of	O
data	O
with	O
which	O
to	O
validate	O
the	O
method	O
for	O
example	O
suppose	O
we	O
have	O
labels	O
for	O
each	O
object	O
as	O
in	O
figure	O
we	O
can	O
have	O
a	O
reference	O
clustering	B
given	O
a	O
clustering	B
we	O
can	O
induce	O
a	O
set	O
of	O
labels	O
and	O
vice	O
versa	O
then	O
we	O
can	O
compare	O
the	O
clustering	B
with	O
the	O
labels	O
using	O
various	O
metrics	O
which	O
we	O
describe	O
below	O
we	O
will	O
use	O
some	O
of	O
these	O
metrics	O
later	O
when	O
we	O
compare	O
clustering	B
methods	O
purity	B
i	O
let	O
nij	O
be	O
the	O
number	O
of	O
objects	O
in	O
cluster	O
i	O
that	O
belong	O
to	O
class	O
j	O
and	O
let	O
ni	O
nij	O
be	O
the	O
total	O
number	O
of	O
objects	O
in	O
cluster	O
i	O
define	O
pij	O
nijni	O
this	O
is	O
the	O
empirical	B
distribution	I
over	O
class	O
labels	O
for	O
cluster	O
i	O
we	O
define	O
the	O
purity	B
of	O
a	O
cluster	O
as	O
pi	O
maxj	O
pij	O
and	O
the	O
overall	O
purity	B
of	O
a	O
clustering	B
as	O
purity	B
ni	O
n	O
pi	O
for	O
example	O
in	O
figure	O
we	O
have	O
that	O
the	O
purity	B
is	O
the	O
purity	B
ranges	O
between	O
and	O
however	O
we	O
can	O
trivially	O
achieve	O
a	O
purity	B
of	O
by	O
putting	O
each	O
object	O
into	O
its	O
own	O
cluster	O
so	O
this	O
measure	O
does	O
not	O
penalize	O
for	O
the	O
number	O
of	O
clusters	B
rand	B
index	I
let	O
u	O
ur	O
and	O
v	O
vc	B
be	O
two	O
different	O
partitions	O
of	O
the	O
n	O
data	O
points	O
i	O
e	O
two	O
different	O
clusterings	O
for	O
example	O
u	O
might	O
be	O
the	O
estimated	O
clustering	B
and	O
v	O
is	O
reference	O
clustering	B
derived	O
from	O
the	O
class	O
labels	O
now	O
define	O
a	O
contingency	B
table	I
chapter	O
clustering	B
containing	O
the	O
following	O
numbers	O
t	O
p	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
same	O
cluster	O
in	O
both	O
u	O
and	O
v	O
positives	O
t	O
n	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
different	O
clusters	B
in	O
both	O
u	O
and	O
v	O
negatives	O
f	O
n	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
different	O
clusters	B
in	O
u	O
but	O
the	O
same	O
cluster	O
in	O
v	O
negatives	O
and	O
f	O
p	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
same	O
cluster	O
in	O
u	O
but	O
different	O
clusters	B
in	O
v	O
positives	O
a	O
common	O
summary	O
statistic	O
is	O
the	O
rand	B
index	I
r	O
t	O
p	O
t	O
n	O
t	O
p	O
f	O
p	O
f	O
n	O
t	O
n	O
this	O
can	O
be	O
interpreted	O
as	O
the	O
fraction	O
of	O
clustering	B
decisions	O
that	O
are	O
correct	O
clearly	O
r	O
for	O
example	O
consider	O
figure	O
the	O
three	O
clusters	B
contain	O
and	O
points	O
so	O
the	O
number	O
of	O
positives	O
pairs	O
of	O
objects	O
put	O
in	O
the	O
same	O
cluster	O
regardless	O
of	O
label	B
is	O
t	O
p	O
f	O
p	O
of	O
these	O
the	O
number	O
of	O
true	O
positives	O
is	O
given	O
by	O
t	O
p	O
where	O
the	O
last	O
two	O
terms	O
come	O
from	O
cluster	O
there	O
are	O
pairs	O
labeled	O
a	O
so	O
f	O
p	O
similarly	O
one	O
can	O
show	O
f	O
n	O
and	O
t	O
n	O
so	O
the	O
rand	B
index	I
is	O
pairs	O
labeled	O
c	O
and	O
one	O
can	O
define	O
an	O
adjusted	B
rand	B
index	I
and	O
arabie	O
as	O
follows	O
the	O
rand	B
index	I
only	O
achieves	O
its	O
lower	O
bound	O
of	O
if	O
t	O
p	O
t	O
n	O
which	O
is	O
a	O
rare	B
event	I
ar	O
index	O
expected	O
index	O
max	O
index	O
expected	O
index	O
here	O
the	O
model	O
of	O
randomness	O
is	O
based	O
on	O
using	O
the	O
generalized	O
hyper-geometric	O
distribution	O
i	O
e	O
the	O
two	O
partitions	O
are	O
picked	O
at	O
random	O
subject	O
to	O
having	O
the	O
original	O
number	O
of	O
classes	O
and	O
objects	O
in	O
each	O
and	O
then	O
the	O
expected	B
value	I
of	O
t	O
p	O
t	O
n	O
is	O
computed	O
this	O
model	O
can	O
be	O
used	O
to	O
compute	O
the	O
statistical	O
significance	O
of	O
the	O
rand	B
index	I
the	O
rand	B
index	I
weights	O
false	O
positives	O
and	O
false	O
negatives	O
equally	O
various	O
other	O
summary	O
statistics	O
for	O
binary	O
decision	B
problems	O
such	O
as	O
the	O
f-score	O
can	O
also	O
be	O
used	O
one	O
can	O
compute	O
their	O
frequentist	B
sampling	B
distribution	I
and	O
hence	O
their	O
statistical	O
significance	O
using	O
methods	O
such	O
as	O
bootstrap	B
mutual	B
information	B
another	O
way	O
to	O
measure	O
cluster	O
quality	O
is	O
to	O
compute	O
the	O
mutual	B
information	B
between	O
u	O
and	O
v	O
and	O
dom	O
to	O
do	O
this	O
let	O
pu	O
v	O
j	O
be	O
the	O
probability	O
that	O
a	O
randomly	O
chosen	O
object	O
belongs	O
to	O
cluster	O
ui	O
in	O
u	O
and	O
vj	O
in	O
v	O
also	O
let	O
pu	O
be	O
the	O
be	O
the	O
probability	O
that	O
a	O
randomly	O
chosen	O
object	O
belongs	O
to	O
cluster	O
ui	O
in	O
u	O
define	O
vj	O
n	O
dirichlet	B
process	I
mixture	B
models	I
pv	O
similarly	O
then	O
we	O
have	O
pu	O
v	O
j	O
pu	O
iu	O
v	O
pu	O
v	O
j	O
log	O
this	O
lies	O
between	O
and	O
minh	O
h	O
unfortunately	O
the	O
maximum	O
value	O
can	O
be	O
achieved	O
by	O
using	O
lots	O
of	O
small	O
clusters	B
which	O
have	O
low	O
entropy	B
to	O
compensate	O
for	O
this	O
we	O
can	O
use	O
the	O
normalized	B
mutual	B
information	B
n	O
m	O
iu	O
v	O
iu	O
v	O
this	O
lies	O
between	O
and	O
a	O
version	O
of	O
this	O
that	O
is	O
adjusted	O
for	O
chance	O
a	O
particular	O
random	O
data	O
model	O
is	O
described	O
in	O
et	O
al	O
another	O
variant	O
called	O
variation	B
of	I
information	B
is	O
described	O
in	O
dirichlet	B
process	I
mixture	B
models	I
the	O
simplest	O
approach	O
to	O
clustering	B
is	O
to	O
use	O
a	O
finite	O
mixture	B
model	I
as	O
we	O
discussed	O
in	O
section	O
this	O
is	O
sometimes	O
called	O
model-based	B
clustering	B
since	O
we	O
define	O
a	O
probabilistic	O
model	O
of	O
the	O
data	O
and	O
optimize	O
a	O
well-defined	O
objective	O
likelihood	B
or	O
posterior	O
as	O
opposed	O
to	O
just	O
using	O
some	O
heuristic	O
algorithm	O
the	O
principle	O
problem	O
with	O
finite	O
mixture	B
models	O
is	O
how	O
to	O
choose	O
the	O
number	O
of	O
components	O
k	O
we	O
discussed	O
several	O
techniques	O
in	O
section	O
however	O
in	O
many	O
cases	O
there	O
is	O
no	O
welldefined	O
number	O
of	O
clusters	B
even	O
in	O
the	O
simple	O
height-weight	O
data	O
it	O
is	O
not	O
clear	O
if	O
the	O
correct	O
value	O
of	O
k	O
should	O
be	O
or	O
it	O
would	O
be	O
much	O
better	O
if	O
we	O
did	O
not	O
have	O
to	O
choose	O
k	O
at	O
all	O
in	O
this	O
section	O
we	O
discuss	O
infinite	O
mixture	B
models	O
in	O
which	O
we	O
do	O
not	O
impose	O
any	O
a	O
priori	O
bound	O
on	O
k	O
to	O
do	O
this	O
we	O
will	O
use	O
a	O
non-parametric	B
prior	I
based	O
on	O
the	O
dirichlet	B
process	I
this	O
allows	O
the	O
number	O
of	O
clusters	B
to	O
grow	O
as	O
the	O
amount	O
of	O
data	O
increases	O
it	O
will	O
also	O
prove	O
useful	O
later	O
when	O
we	O
discuss	O
hiearchical	O
clustering	B
the	O
topic	B
of	O
non-parametric	B
bayes	I
is	O
currently	O
very	O
active	O
and	O
we	O
do	O
not	O
have	O
space	O
to	O
go	O
into	O
details	O
et	O
al	O
for	O
a	O
recent	O
book	O
on	O
the	O
topic	B
instead	O
we	O
just	O
give	O
a	O
brief	O
review	O
of	O
the	O
dp	O
and	O
its	O
application	O
to	O
mixture	B
modeling	O
based	O
on	O
the	O
presentation	O
in	O
sec	O
from	O
finite	O
to	O
infinite	O
mixture	B
models	O
consider	O
a	O
finite	O
mixture	B
model	I
as	O
shown	O
in	O
figure	O
the	O
usual	O
representation	O
is	O
as	O
follows	O
pxizi	O
k	O
xi	O
k	O
pzi	O
k	O
k	O
p	O
dir	O
the	O
form	O
of	O
p	O
k	O
is	O
chosen	O
to	O
be	O
conjugate	O
to	O
pxi	O
k	O
we	O
can	O
write	O
pxi	O
k	O
as	O
xi	O
f	O
zi	O
where	O
f	O
is	O
the	O
observation	B
distribution	O
similarly	O
we	O
can	O
write	O
k	O
h	O
where	O
h	O
is	O
the	O
prior	O
chapter	O
clustering	B
figure	O
two	O
different	O
representations	O
of	O
a	O
finite	O
mixture	B
model	I
left	O
traditional	O
representation	O
right	O
representation	O
where	O
parameters	O
are	O
samples	B
from	O
g	O
a	O
discrete	B
measure	O
the	O
picture	O
on	O
the	O
right	O
illustrates	O
the	O
case	O
where	O
k	O
and	O
we	O
sample	O
gaussian	B
means	O
k	O
from	O
a	O
gaussian	B
prior	O
h	O
the	O
height	O
of	O
the	O
spikes	O
reflects	O
the	O
mixing	B
weights	I
k	O
this	O
weighted	O
sum	O
of	O
delta	O
functions	O
is	O
g	O
we	O
then	O
generate	O
two	O
parameters	O
and	O
from	O
g	O
one	O
per	O
data	O
point	O
finally	O
we	O
generate	O
two	O
data	O
points	O
and	O
from	O
n	O
and	O
n	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
an	O
equivalent	O
representation	O
for	O
this	O
model	O
is	O
the	O
parameter	B
used	O
to	O
generate	O
observation	B
xi	O
these	O
parameters	O
are	O
sampled	O
from	O
distribution	O
g	O
which	O
has	O
the	O
form	O
is	O
shown	O
in	O
figure	O
here	O
i	O
g	O
k	O
k	O
k	O
and	O
k	O
h	O
thus	O
we	O
see	O
that	O
g	O
is	O
a	O
finite	O
mixture	B
of	O
delta	O
functions	O
where	O
dir	O
centered	O
on	O
the	O
cluster	O
parameters	O
k	O
the	O
probability	O
that	O
i	O
is	O
equal	O
to	O
k	O
is	O
exactly	O
k	O
the	O
prior	O
probability	O
for	O
that	O
cluster	O
if	O
we	O
sample	O
from	O
this	O
model	O
we	O
will	O
always	O
probability	O
one	O
get	O
exactly	O
k	O
clusters	B
with	O
data	O
points	O
scattered	O
around	O
the	O
cluster	O
centers	O
we	O
would	O
like	O
a	O
more	O
flexible	O
model	O
that	O
can	O
generate	O
a	O
variable	O
number	O
of	O
clusters	B
furthermore	O
the	O
more	O
data	O
we	O
generate	O
the	O
more	O
likely	O
we	O
should	O
be	O
to	O
see	O
a	O
new	O
cluster	O
the	O
way	O
to	O
do	O
this	O
is	O
to	O
replace	O
the	O
discrete	B
distribution	O
g	O
with	O
a	O
random	B
probability	I
measure	I
below	O
we	O
will	O
show	O
that	O
the	O
dirichlet	B
process	I
denoted	O
g	O
dp	O
h	O
is	O
one	O
way	O
to	O
do	O
this	O
before	O
we	O
go	O
into	O
the	O
details	O
we	O
show	O
some	O
samples	B
from	O
this	O
non-parametric	B
model	I
in	O
figure	O
we	O
see	O
that	O
it	O
has	O
the	O
desired	O
properties	O
of	O
generating	O
a	O
variable	O
number	O
of	O
clusters	B
with	O
more	O
clusters	B
as	O
the	O
amount	O
of	O
data	O
increases	O
the	O
resulting	O
samples	B
look	O
much	O
more	O
like	O
real	O
data	O
than	O
samples	B
from	O
a	O
finite	O
mixture	B
model	I
of	O
course	O
working	O
with	O
an	O
infinite	O
model	O
sounds	O
scary	O
fortunately	O
as	O
we	O
show	O
below	O
even	O
though	O
this	O
model	O
is	O
potentially	O
infinite	O
we	O
can	O
perform	O
inference	B
using	O
an	O
amount	O
of	O
computation	O
that	O
is	O
not	O
only	O
tractable	O
but	O
is	O
often	O
much	O
less	O
than	O
that	O
required	O
to	O
fit	O
a	O
set	O
dirichlet	B
process	I
mixture	B
models	I
figure	O
some	O
samples	B
from	O
a	O
dirichlet	B
process	I
mixture	B
model	I
of	O
gaussians	O
with	O
concentration	B
parameter	B
from	O
left	O
to	O
right	O
we	O
show	O
n	O
n	O
and	O
n	O
samples	B
each	O
row	O
is	O
a	O
different	O
run	O
we	O
also	O
show	O
the	O
model	O
parameters	O
as	O
ellipses	O
which	O
are	O
sampled	O
from	O
a	O
vague	O
niw	B
base	B
distribution	I
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
dpmsampledemo	O
written	O
by	O
yee-whye	O
teh	O
of	O
finite	O
mixture	B
models	O
for	O
different	O
k	O
the	O
intuitive	O
reason	O
is	O
that	O
we	O
can	O
get	O
evidence	B
that	O
certain	O
values	O
of	O
k	O
are	O
appropriate	O
high	O
posterior	O
support	B
long	O
before	O
we	O
have	O
been	O
able	O
to	O
estimate	O
the	O
parameters	O
so	O
we	O
can	O
focus	O
our	O
computational	O
efforts	O
on	O
models	O
of	O
appropriate	O
complexity	O
thus	O
going	O
to	O
the	O
infinite	O
limit	O
can	O
sometimes	O
be	O
faster	O
this	O
is	O
especially	O
true	O
when	O
we	O
have	O
multiple	O
model	B
selection	I
problems	O
to	O
solve	O
chapter	O
clustering	B
a	O
base	B
measure	I
h	O
on	O
a	O
space	O
figure	O
where	O
the	O
shading	O
of	O
cell	O
tk	O
is	O
proportional	O
to	O
e	O
htk	O
regions	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
one	O
possible	O
partition	O
into	O
k	O
regions	O
a	O
refined	O
partition	O
into	O
k	O
the	O
dirichlet	B
process	I
recall	B
from	O
chapter	O
that	O
a	O
gaussian	B
process	I
is	O
a	O
distribution	O
over	O
functions	O
of	O
the	O
form	O
f	O
x	O
r	O
it	O
is	O
defined	O
implicitly	O
by	O
the	O
requirement	O
that	O
pf	O
f	O
be	O
jointly	O
gaussian	B
for	O
any	O
set	O
of	O
points	O
xi	O
x	O
the	O
parameters	O
of	O
this	O
gaussian	B
can	O
be	O
computed	O
using	O
a	O
mean	B
function	I
and	O
covariance	B
function	O
k	O
we	O
write	O
f	O
gp	O
k	O
furthermore	O
the	O
gp	O
is	O
consistently	O
defined	O
so	O
that	O
pf	O
can	O
be	O
derived	O
from	O
pf	O
f	O
etc	O
require	O
g	O
and	O
gtk	O
has	O
a	O
joint	O
dirichlet	B
distribution	I
where	O
we	O
g	O
the	O
dp	O
is	O
defined	O
implicitly	O
by	O
the	O
requirement	O
that	O
a	O
dirichlet	B
process	I
is	O
a	O
distribution	O
over	O
probability	O
measures	O
g	O
r	O
dir	O
htk	O
for	O
any	O
finite	O
partition	O
tk	O
of	O
if	O
this	O
is	O
the	O
case	O
we	O
write	O
g	O
dp	O
h	O
where	O
is	O
called	O
the	O
concentration	B
parameter	B
and	O
h	O
is	O
called	O
the	O
base	O
an	O
example	O
of	O
a	O
dp	O
is	O
shown	O
in	O
figure	O
where	O
the	O
base	B
measure	I
is	O
a	O
gaussian	B
the	O
distribution	O
over	O
all	O
the	O
cells	O
gtk	O
is	O
dirichlet	B
so	O
the	O
marginals	O
in	O
each	O
cell	O
are	O
beta	O
distributed	O
beta	O
hti	O
htj	O
the	O
dp	O
is	O
consistently	O
defined	O
in	O
the	O
sense	O
that	O
if	O
and	O
form	O
a	O
partition	O
of	O
then	O
and	O
g	O
both	O
follow	O
the	O
same	O
beta	B
distribution	I
recall	B
that	O
if	O
dir	O
and	O
z	O
cat	O
then	O
we	O
can	O
integrate	B
out	I
to	O
get	O
the	O
predictive	B
distribution	O
for	O
the	O
dirichlet-multinoulli	O
model	O
z	O
cat	O
k	O
unlike	O
a	O
gp	O
knowing	O
something	O
about	O
gtk	O
does	O
not	O
tell	O
us	O
anything	O
about	O
beyond	O
the	O
sum-to-one	O
constraint	O
we	O
say	O
that	O
the	O
dp	O
is	O
a	O
neutral	B
process	I
other	O
stochastic	B
processes	I
can	O
be	O
defined	O
that	O
do	O
not	O
have	O
this	O
property	O
but	O
they	O
are	O
not	O
so	O
computationally	O
convenient	O
dirichlet	B
process	I
mixture	B
models	I
illustration	O
of	O
the	O
stick	O
breaking	O
construction	O
figure	O
we	O
have	O
a	O
unit	O
length	O
stick	O
which	O
we	O
break	O
at	O
a	O
random	O
point	O
the	O
length	O
of	O
the	O
piece	O
we	O
keep	O
is	O
called	O
we	O
then	O
recursively	O
break	O
off	O
pieces	O
of	O
the	O
remaining	O
stick	O
to	O
generate	O
source	O
figure	O
of	O
used	O
with	O
samples	B
of	O
k	O
from	O
this	O
process	O
for	O
row	O
and	O
kind	O
permission	O
of	O
erik	O
sudderth	O
row	O
figure	O
generated	O
by	O
stickbreakingdemo	O
written	O
by	O
yee-whye	O
teh	O
where	O
given	O
one	O
observation	B
is	O
given	O
by	O
k	O
k	O
in	O
other	O
words	O
pz	O
k	O
k	O
also	O
the	O
updated	O
posterior	O
for	O
dir	O
iz	O
k	O
iz	O
k	O
the	O
dp	O
generalizes	O
this	O
to	O
arbitrary	O
partitions	O
if	O
g	O
dp	O
h	O
then	O
p	O
ti	O
hti	O
and	O
the	O
posterior	O
is	O
gtk	O
h	O
dir	O
htk	O
this	O
holds	O
for	O
any	O
set	O
of	O
partitions	O
hence	O
if	O
we	O
observe	O
multiple	O
samples	B
i	O
g	O
the	O
new	O
posterior	O
is	O
given	O
by	O
g	O
n	O
h	O
dp	O
n	O
n	O
h	O
i	O
thus	O
we	O
see	O
that	O
the	O
dp	O
effectively	O
defines	O
a	O
conjugate	B
prior	I
for	O
arbitrary	O
measurable	O
spaces	O
the	O
concentration	B
parameter	B
is	O
like	O
the	O
effective	B
sample	I
size	I
of	O
the	O
base	B
measure	I
h	O
stick	O
breaking	O
construction	O
of	O
the	O
dp	O
our	O
discussion	O
so	O
far	O
has	O
been	O
very	O
abstract	O
we	O
now	O
give	O
a	O
constructive	O
definition	O
for	O
the	O
dp	O
known	O
as	O
the	O
stick-breaking	B
construction	I
be	O
an	O
infinite	O
sequence	O
of	O
mixture	B
weights	O
derived	O
from	O
the	O
following	O
process	O
let	O
k	O
k	O
k	O
k	O
k	O
l	O
k	O
l	O
chapter	O
clustering	B
this	O
is	O
often	O
denoted	O
by	O
gem	B
where	O
gem	B
stands	O
for	O
griffiths	O
engen	O
and	O
mccloskey	O
term	O
is	O
due	O
to	O
some	O
samples	B
from	O
this	O
process	O
are	O
shown	O
in	O
figure	O
one	O
can	O
show	O
that	O
this	O
process	O
process	O
will	O
terminate	O
with	O
probability	O
although	O
the	O
number	O
of	O
elements	O
it	O
generates	O
increases	O
with	O
furthermore	O
the	O
size	O
of	O
the	O
k	O
components	O
decreases	O
on	O
average	O
now	O
define	O
g	O
k	O
k	O
where	O
gem	B
and	O
k	O
h	O
then	O
one	O
can	O
show	O
that	O
g	O
dp	O
h	O
as	O
a	O
consequence	O
of	O
this	O
construction	O
we	O
see	O
that	O
samples	B
from	O
a	O
dp	O
are	O
discrete	B
with	I
probability	I
one	I
in	O
other	O
words	O
if	O
you	O
keep	O
sampling	O
it	O
you	O
will	O
get	O
more	O
and	O
more	O
repetitions	O
of	O
previously	O
generated	O
values	O
so	O
if	O
we	O
sample	O
i	O
g	O
we	O
will	O
see	O
repeated	O
values	O
let	O
us	O
number	O
the	O
unique	O
values	O
etc	O
data	O
sampled	O
from	O
i	O
will	O
therefore	O
cluster	O
around	O
the	O
k	O
this	O
is	O
evident	O
in	O
figure	O
where	O
most	O
data	O
comes	O
from	O
the	O
gaussians	O
with	O
large	O
k	O
values	O
represented	O
by	O
ellipses	O
with	O
thick	O
borders	O
this	O
is	O
our	O
first	O
indication	O
that	O
the	O
dp	O
might	O
be	O
useful	O
for	O
clustering	B
the	O
chinese	B
restaurant	I
process	I
working	O
with	O
infinite	O
dimensional	O
sticks	O
is	O
problematic	O
however	O
we	O
can	O
exploit	O
the	O
clustering	B
property	O
to	O
draw	O
samples	B
form	O
a	O
gp	O
as	O
we	O
now	O
show	O
if	O
i	O
g	O
are	O
n	O
observations	O
from	O
g	O
dp	O
h	O
taking	O
on	O
k	O
the	O
key	O
result	O
is	O
this	O
distinct	O
values	O
k	O
then	O
the	O
predictive	B
distribution	O
of	O
the	O
next	O
observation	B
is	O
given	O
by	O
p	O
n	O
h	O
n	O
h	O
nk	O
k	O
where	O
nk	O
is	O
the	O
number	O
of	O
previous	O
observations	O
equal	O
to	O
k	O
this	O
is	O
called	O
the	O
polya	B
urn	I
or	O
blackwell-macqueen	B
sampling	O
scheme	O
this	O
provides	O
a	O
constructive	O
way	O
to	O
sample	O
from	O
a	O
dp	O
it	O
is	O
much	O
more	O
convenient	O
to	O
work	O
with	O
discrete	B
variables	O
zi	O
which	O
specify	O
which	O
value	O
of	O
k	O
to	O
use	O
that	O
is	O
we	O
define	O
i	O
zi	O
based	O
on	O
the	O
above	O
expression	O
we	O
have	O
pzn	O
n	O
iz	O
k	O
nkiz	O
k	O
where	O
k	O
represents	O
a	O
new	O
cluster	O
index	O
that	O
has	O
not	O
yet	O
been	O
used	O
this	O
is	O
called	O
the	O
chinese	B
restaurant	I
process	I
or	O
crp	B
based	O
on	O
the	O
seemingly	O
infinite	O
supply	O
of	O
tables	O
at	O
certain	O
chinese	O
restaurants	O
the	O
analogy	O
is	O
as	O
follows	O
the	O
tables	O
are	O
like	O
clusters	B
and	O
the	O
customers	O
are	O
like	O
observations	O
when	O
a	O
person	O
enters	O
the	O
restaurant	O
he	O
may	O
choose	O
to	O
join	O
an	O
existing	O
table	O
with	O
probability	O
proportional	O
to	O
the	O
number	O
of	O
people	O
already	O
sitting	O
at	O
this	O
table	O
nk	O
otherwise	O
with	O
a	O
probability	O
that	O
diminishes	O
as	O
more	O
people	O
enter	O
the	O
room	O
to	O
the	O
n	O
term	O
dirichlet	B
process	I
mixture	B
models	I
figure	O
two	O
views	B
of	O
a	O
dp	O
mixture	B
model	I
left	O
gem	B
right	O
g	O
is	O
drawn	O
from	O
a	O
dp	O
compare	O
to	O
figure	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
infinite	O
number	O
of	O
clusters	B
parameters	O
k	O
and	O
source	O
figure	O
of	O
he	O
may	O
choose	O
to	O
sit	O
at	O
a	O
new	O
table	O
k	O
integers	O
which	O
is	O
like	O
a	O
distribution	O
of	O
customers	O
to	O
tables	O
the	O
result	O
is	O
a	O
distribution	O
over	O
partitions	O
of	O
the	O
the	O
fact	O
that	O
currently	O
occupied	O
tables	O
are	O
more	O
likely	O
to	O
get	O
new	O
customers	O
is	O
sometimes	O
called	O
the	O
rich	B
get	I
richer	I
phenomenon	O
indeed	O
one	O
can	O
derive	O
an	O
expression	O
for	O
the	O
distribution	O
of	O
cluster	O
sizes	O
induced	O
by	O
this	O
prior	O
process	O
it	O
is	O
basically	O
a	O
power	B
law	I
the	O
number	O
of	O
occupied	O
tables	O
k	O
almost	O
surely	O
approaches	O
logn	O
as	O
n	O
showing	O
that	O
the	O
model	O
complexity	O
will	O
indeed	O
grow	O
logarithmically	O
with	O
dataset	O
size	O
more	O
flexible	O
priors	O
over	O
cluster	O
sizes	O
can	O
also	O
be	O
defined	O
such	O
as	O
the	O
two-parameter	O
pitman-yor	B
process	I
applying	O
dirichlet	B
processes	O
to	O
mixture	B
modeling	O
the	O
dp	O
is	O
not	O
particularly	O
useful	O
as	O
a	O
model	O
for	O
data	O
directly	O
since	O
data	O
vectors	O
rarely	O
repeat	O
exactly	O
however	O
it	O
is	O
useful	O
as	O
a	O
prior	O
for	O
the	O
parameters	O
of	O
a	O
stochastic	O
data	O
generating	O
mechanism	O
such	O
as	O
a	O
mixture	B
model	I
to	O
create	O
such	O
a	O
model	O
we	O
follow	O
exactly	O
the	O
same	O
setup	O
as	O
section	O
but	O
we	O
define	O
g	O
dp	O
h	O
equivalently	O
we	O
can	O
write	O
the	O
model	O
as	O
follows	O
gem	B
zi	O
k	O
h	O
xi	O
f	O
zi	O
this	O
is	O
illustrated	O
in	O
figure	O
we	O
see	O
that	O
g	O
is	O
now	O
a	O
random	O
draw	O
of	O
an	O
unbounded	O
number	O
of	O
parameters	O
k	O
from	O
the	O
base	B
distribution	I
h	O
each	O
with	O
weight	O
k	O
each	O
data	O
point	O
xi	O
is	O
generated	O
by	O
sampling	O
its	O
own	O
private	O
parameter	B
i	O
from	O
g	O
as	O
we	O
get	O
more	O
and	O
more	O
data	O
it	O
becomes	O
increasingly	O
likely	O
that	O
i	O
will	O
be	O
equal	O
to	O
one	O
of	O
the	O
k	O
s	O
we	O
have	O
seen	O
before	O
and	O
thus	O
xi	O
will	O
be	O
generated	O
close	O
to	O
an	O
existing	O
datapoint	O
chapter	O
clustering	B
fitting	O
a	O
dp	O
mixture	B
model	I
the	O
simplest	O
way	O
to	O
fit	O
a	O
dpmm	O
is	O
to	O
modify	O
the	O
collapsed	B
gibbs	B
sampler	I
of	O
section	O
from	O
equation	O
we	O
have	O
pzi	O
kz	O
i	O
x	O
pzi	O
kz	O
i	O
i	O
zi	O
k	O
z	O
i	O
n	O
by	O
exchangeability	O
we	O
can	O
assume	O
that	O
zi	O
is	O
the	O
last	O
customer	O
to	O
enter	O
the	O
restaurant	O
hence	O
the	O
first	O
term	O
is	O
given	O
by	O
pziz	O
i	O
izi	O
k	O
nk	O
iizi	O
k	O
where	O
k	O
is	O
the	O
number	O
of	O
clusters	B
used	O
by	O
z	O
i	O
and	O
k	O
this	O
is	O
as	O
follows	O
is	O
a	O
new	O
cluster	O
another	O
way	O
to	O
write	O
nk	O
i	O
pzi	O
kz	O
i	O
if	O
k	O
has	O
been	O
seen	O
before	O
if	O
k	O
is	O
a	O
new	O
cluster	O
nk	O
i	O
interestingly	O
this	O
is	O
equivalent	O
to	O
equation	O
which	O
has	O
the	O
form	O
pzi	O
kz	O
i	O
to	O
compute	O
the	O
second	O
term	O
pxix	O
i	O
zi	O
k	O
z	O
i	O
let	O
us	O
partition	O
the	O
data	O
x	O
i	O
into	O
clusters	B
based	O
on	O
z	O
i	O
let	O
x	O
ic	O
zj	O
c	O
j	O
i	O
be	O
the	O
data	O
assigned	O
to	O
cluster	O
c	O
if	O
zi	O
k	O
then	O
xi	O
is	O
conditionally	B
independent	I
of	O
all	O
the	O
data	O
points	O
except	O
those	O
assigned	O
to	O
cluster	O
k	O
hence	O
we	O
have	O
in	O
thek	O
limit	O
neal	O
pxix	O
i	O
z	O
i	O
zi	O
k	O
xix	O
ik	O
where	O
pxi	O
x	O
ik	O
pxi	O
k	O
pxi	O
x	O
ik	O
px	O
ik	O
h	O
k	O
k	O
pxj	O
k	O
is	O
the	O
marginal	B
likelihood	B
of	O
all	O
the	O
data	O
assigned	O
to	O
cluster	O
k	O
including	O
i	O
and	O
px	O
ik	O
is	O
an	O
analogous	O
expression	O
excluding	O
i	O
thus	O
we	O
see	O
that	O
the	O
term	O
pxix	O
i	O
z	O
i	O
zi	O
k	O
is	O
the	O
posterior	O
preditive	O
distribution	O
for	O
cluster	O
k	O
evaluated	O
at	O
xi	O
corresponding	O
to	O
a	O
new	O
cluster	O
we	O
have	O
if	O
zi	O
k	O
pxix	O
i	O
z	O
i	O
zi	O
k	O
pxi	O
pxi	O
which	O
is	O
just	O
the	O
prior	O
predictive	B
distribution	O
for	O
a	O
new	O
cluster	O
evaluated	O
at	O
xi	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
is	O
called	O
algorithm	O
in	O
this	O
is	O
very	O
similar	B
to	O
collapsed	O
gibbs	O
for	O
finite	O
mixtures	O
except	O
that	O
we	O
have	O
to	O
consider	O
the	O
case	O
zi	O
k	O
an	O
example	O
of	O
this	O
procedure	O
in	O
action	B
is	O
shown	O
in	O
figure	O
the	O
sample	O
clusterings	O
and	O
the	O
induced	O
posterior	O
over	O
k	O
seems	O
reasonable	O
the	O
method	O
tends	O
to	O
rapidly	O
discover	O
a	O
good	O
clustering	B
by	O
contrast	O
gibbs	B
sampling	I
em	B
for	O
a	O
finite	O
mixture	B
model	I
often	O
gets	O
stuck	O
in	O
affinity	B
propagation	I
algorithm	O
collapsed	B
gibbs	B
sampler	I
for	O
dp	O
mixtures	O
for	O
each	O
i	O
in	O
random	O
order	O
do	O
remove	O
xi	O
s	O
sufficient	B
statistics	I
from	O
old	O
cluster	O
zi	O
for	O
each	O
k	O
do	O
compute	O
pkxi	O
pxix	O
ik	O
set	O
nk	O
i	O
dimx	O
ik	O
compute	O
pzi	O
kz	O
id	O
nk	O
i	O
compute	O
p	O
pxi	O
compute	O
pzi	O
id	O
normalize	O
pzi	O
sample	O
zi	O
pzi	O
add	O
xi	O
s	O
sufficient	B
statistics	I
to	O
new	O
cluster	O
zi	O
if	O
any	O
cluster	O
is	O
empty	O
remove	O
it	O
and	O
decrease	O
k	O
poor	O
local	O
optima	O
shown	O
this	O
is	O
because	O
the	O
dpmm	O
is	O
able	O
to	O
create	O
extra	O
redundant	O
clusters	B
early	O
on	O
and	O
to	O
use	O
them	O
to	O
escape	O
local	O
optima	O
figure	O
shows	O
that	O
most	O
of	O
the	O
time	O
the	O
dpmm	O
converges	O
more	O
rapidly	O
than	O
a	O
finite	O
mixture	B
model	I
a	O
variety	O
of	O
other	O
fitting	O
methods	O
have	O
been	O
proposed	O
shows	O
how	O
one	O
can	O
use	O
a	B
star	I
search	I
and	O
beam	B
search	I
to	O
quickly	O
find	O
an	O
approximate	O
map	B
estimate	I
et	O
al	O
discusses	O
how	O
to	O
fit	O
a	O
dpmm	O
online	O
using	O
particle	O
filtering	B
which	O
is	O
a	O
like	O
a	O
stochastic	O
version	O
of	O
beam	B
search	I
this	O
can	O
be	O
more	O
efficient	O
than	O
gibbs	B
sampling	I
particularly	O
for	O
large	O
datasets	O
et	O
al	O
develops	O
a	O
variational	O
approximation	O
that	O
is	O
even	O
faster	O
also	O
extensions	O
to	O
the	O
case	O
of	O
non-conjugate	O
priors	O
are	O
discussed	O
in	O
another	O
important	O
issue	O
is	O
how	O
to	O
set	O
the	O
hyper-parameters	B
for	O
the	O
dp	O
the	O
value	O
of	O
does	O
not	O
have	O
much	O
impact	O
on	O
predictive	B
accuracy	O
but	O
it	O
does	O
affect	O
the	O
number	O
of	O
clusters	B
one	O
approach	O
is	O
to	O
put	O
a	O
gaa	O
b	O
prior	O
for	O
and	O
then	O
to	O
from	O
its	O
posterior	O
p	O
n	O
a	O
b	O
using	O
auxiliary	O
variable	O
methods	O
and	O
west	O
alternatively	O
one	O
can	O
use	O
empirical	B
bayes	I
et	O
al	O
similarly	O
for	O
the	O
base	B
distribution	I
we	O
can	O
either	O
sample	O
the	O
hyper-parameters	B
or	O
use	O
empirical	B
bayes	I
et	O
al	O
affinity	B
propagation	I
mixture	B
models	O
whether	O
finite	O
or	O
infinite	O
require	O
access	O
to	O
the	O
raw	O
n	O
d	O
data	O
matrix	O
and	O
need	O
to	O
specify	O
a	O
generative	O
model	O
of	O
the	O
data	O
an	O
alternative	O
approach	O
takes	O
as	O
input	O
an	O
n	O
n	O
similarity	O
matrix	O
and	O
then	O
tries	O
to	O
identify	O
examplars	O
which	O
will	O
act	O
as	O
cluster	O
centers	O
the	O
k-medoids	O
or	O
k-centers	B
algorithm	O
is	O
one	O
approach	O
but	O
it	O
can	O
suffer	O
from	O
local	O
minima	O
here	O
we	O
describe	O
an	O
alternative	O
approach	O
called	O
affinity	B
propagation	I
and	O
dueck	O
that	O
works	O
substantially	O
better	O
in	O
practice	O
the	O
idea	O
is	O
that	O
each	O
data	O
point	O
must	O
choose	O
another	O
data	O
point	O
as	O
its	O
exemplar	O
or	O
centroid	B
some	O
data	O
points	O
will	O
choose	O
themselves	O
as	O
centroids	B
and	O
this	O
will	O
automatically	O
determine	O
the	O
number	O
of	O
clusters	B
more	O
precisely	O
let	O
ci	B
n	O
represent	O
the	O
centroid	B
for	O
datapoint	O
i	O
iter	O
iter	O
chapter	O
clustering	B
iter	O
figure	O
data	O
points	O
in	O
are	O
clustered	O
using	O
a	O
dp	O
mixture	B
fit	O
with	O
collapsed	B
gibbs	B
sampling	I
we	O
show	O
samples	B
from	O
the	O
posterior	O
after	O
samples	B
we	O
also	O
show	O
the	O
posterior	O
over	O
k	O
based	O
on	O
samples	B
discarding	O
the	O
first	O
as	O
burnin	O
figure	O
generated	O
by	O
written	O
by	O
yee	O
whye	O
teh	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
following	O
function	O
sc	O
si	O
ci	B
kc	O
the	O
first	O
term	O
measures	O
the	O
similarity	O
of	O
each	O
point	O
to	O
its	O
centroid	B
the	O
second	O
term	O
is	O
a	O
penalty	O
term	O
that	O
is	O
if	O
some	O
data	O
point	O
i	O
has	O
chosen	O
k	O
as	O
its	O
exemplar	O
ci	B
k	O
but	O
k	O
has	O
not	O
chosen	O
itself	O
as	O
an	O
exemplar	O
we	O
do	O
not	O
have	O
ck	O
k	O
more	O
formally	O
if	O
ck	O
k	O
but	O
i	O
ci	B
k	O
kc	O
otherwise	O
the	O
objective	O
function	O
can	O
be	O
represented	O
as	O
a	O
factor	B
graph	B
we	O
can	O
either	O
use	O
n	O
nodes	B
affinity	B
propagation	I
x	O
p	O
g	O
o	O
l	O
dirichlet	B
process	I
mixture	B
finite	O
mixture	B
iteration	O
x	O
p	O
g	O
o	O
l	O
dirichlet	B
process	I
mixture	B
finite	O
mixture	B
iteration	O
figure	O
comparison	O
of	O
collapsed	O
gibbs	O
samplers	O
for	O
a	O
dp	O
mixture	B
blue	O
and	O
a	O
finite	O
mixture	B
red	O
with	O
k	O
applied	O
to	O
n	O
data	O
points	O
in	O
figure	O
left	O
logprob	O
vs	O
iteration	O
for	O
different	O
starting	O
values	O
right	O
median	B
line	O
and	O
quantiles	O
lines	O
over	O
different	O
starting	O
values	O
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
a	O
k	O
n	O
ci	B
cn	O
si	O
sn	O
figure	O
factor	B
graphs	O
for	O
affinity	B
propagation	I
circles	O
are	O
variables	O
squares	O
are	O
factors	B
each	O
ci	B
node	O
has	O
n	O
possible	O
states	O
from	O
figure	O
of	O
and	O
dueck	O
used	O
with	O
kind	O
permission	O
of	O
brendan	O
frey	O
each	O
with	O
n	O
possible	O
values	O
as	O
shown	O
in	O
figure	O
or	O
we	O
can	O
use	O
n	O
binary	O
nodes	B
and	O
frey	O
for	O
the	O
details	O
we	O
will	O
assume	O
the	O
former	O
representation	O
we	O
can	O
find	O
a	O
strong	O
local	O
maximum	O
of	O
the	O
objective	O
by	O
using	O
max-product	B
loopy	B
belief	B
propagation	I
referring	O
to	O
the	O
model	O
in	O
figure	O
each	O
variable	O
nodes	B
ci	B
sends	O
a	O
message	O
to	O
each	O
factor	B
node	O
k	O
it	O
turns	O
out	O
that	O
this	O
vector	O
of	O
n	O
numbers	O
can	O
be	O
reduced	O
to	O
a	O
scalar	O
message	O
denote	O
ri	O
k	O
known	O
as	O
the	O
responsibility	B
this	O
is	O
a	O
measure	O
of	O
how	O
much	O
i	O
thinks	O
k	O
would	O
make	O
a	O
good	O
exemplar	O
compared	O
to	O
all	O
the	O
other	O
exemplars	O
i	O
has	O
looked	O
at	O
in	O
addition	O
each	O
factor	B
node	O
k	O
sends	O
a	O
message	O
to	O
each	O
variable	O
node	O
ci	B
again	O
this	O
can	O
be	O
reduced	O
to	O
a	O
scalar	O
message	O
ai	B
k	O
known	O
as	O
the	O
availability	O
this	O
is	O
a	O
measure	O
of	O
how	O
strongly	O
k	O
believes	O
it	O
should	O
an	O
exemplar	O
for	O
i	O
based	O
on	O
all	O
the	O
other	O
data	O
points	O
k	O
has	O
looked	O
at	O
as	O
usual	O
with	O
loopy	O
bp	B
the	O
method	O
might	O
oscillate	O
and	O
convergence	O
is	O
not	O
guaranteed	O
chapter	O
clustering	B
figure	O
example	O
of	O
affinity	B
propagation	I
each	O
point	O
is	O
colored	O
coded	O
by	O
how	O
much	O
it	O
wants	O
to	O
be	O
an	O
exemplar	O
is	O
the	O
most	O
green	O
is	O
the	O
least	O
this	O
can	O
be	O
computed	O
by	O
summing	O
up	O
all	O
the	O
incoming	O
availability	O
messages	O
and	O
the	O
self-similarity	O
term	O
the	O
darkness	O
of	O
the	O
i	O
k	O
arrow	O
reflects	O
how	O
much	O
point	O
i	O
wants	O
to	O
belong	O
to	O
exemplar	O
k	O
from	O
figure	O
of	O
and	O
dueck	O
used	O
with	O
kind	O
permission	O
of	O
brendan	O
frey	O
however	O
by	O
using	O
damping	B
the	O
method	O
is	O
very	O
reliable	O
in	O
practice	O
if	O
the	O
graph	B
is	O
densely	O
connected	O
message	B
passing	I
takes	O
on	O
time	O
but	O
with	O
sparse	B
similarity	O
matrices	O
it	O
only	O
takes	O
oe	O
time	O
where	O
e	O
is	O
the	O
number	O
of	O
edges	B
or	O
non-zero	O
entries	O
in	O
s	O
the	O
number	O
of	O
clusters	B
can	O
be	O
controlled	O
by	O
scaling	O
the	O
diagonal	B
terms	O
si	O
i	O
which	O
reflect	O
how	O
much	O
each	O
data	O
point	O
wants	O
to	O
be	O
an	O
exemplar	O
figure	O
gives	O
a	O
simple	O
example	O
of	O
some	O
data	O
where	O
the	O
negative	O
euclidean	B
distance	I
was	O
used	O
to	O
measured	O
similarity	O
the	O
si	O
i	O
values	O
were	O
set	O
to	O
be	O
the	O
median	B
of	O
all	O
the	O
pairwise	O
similarities	O
the	O
result	O
is	O
clusters	B
many	O
other	O
results	O
are	O
reported	O
in	O
and	O
dueck	O
who	O
show	O
that	O
the	O
method	O
significantly	O
outperforms	O
k-medoids	O
spectral	B
clustering	B
an	O
alternative	O
view	O
of	O
clustering	B
is	O
in	O
terms	O
of	O
graph	B
cuts	I
the	O
idea	O
is	O
we	O
create	O
a	O
weighted	O
undirected	B
graph	B
w	O
from	O
the	O
similarity	O
matrix	O
s	O
typically	O
by	O
using	O
the	O
nearest	O
neighbors	B
of	O
each	O
point	O
this	O
ensures	O
the	O
graph	B
is	O
sparse	B
which	O
speeds	O
computation	O
if	O
we	O
want	O
to	O
find	O
a	O
partition	O
into	O
k	O
clusters	B
say	O
ak	O
one	O
natural	O
criterion	O
is	O
to	O
minimize	O
ak	O
w	O
ak	O
spectral	B
clustering	B
where	O
ak	O
v	O
ak	O
is	O
the	O
complement	O
of	O
ak	O
and	O
w	O
b	O
i	O
aj	O
b	O
wij	O
for	O
k	O
this	O
problem	O
is	O
easy	O
to	O
solve	O
unfortunately	O
the	O
optimal	O
solution	O
often	O
just	O
partitions	O
off	O
a	O
single	O
data	O
point	O
from	O
the	O
rest	O
to	O
ensure	O
the	O
sets	O
are	O
reasonably	O
large	O
we	O
can	O
define	O
the	O
normalized	B
cut	I
to	O
be	O
ak	O
cutak	O
ak	O
volak	O
i	O
a	O
di	O
and	O
di	O
where	O
vola	O
wij	O
is	O
the	O
weighted	O
degree	B
of	O
node	O
i	O
this	O
splits	O
the	O
graph	B
into	O
k	O
clusters	B
such	O
that	O
nodes	B
within	O
each	O
cluster	O
are	O
similar	B
to	O
each	O
other	O
but	O
are	O
different	O
to	O
nodes	B
in	O
other	O
clusters	B
we	O
can	O
formulate	O
the	O
ncut	O
problem	O
in	O
terms	O
of	O
searching	O
for	O
binary	O
vectors	O
ci	B
where	O
cik	O
if	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
that	O
minimize	O
the	O
objective	O
unfortunately	O
this	O
is	O
np-hard	B
and	O
wagner	O
affinity	B
propagation	I
is	O
one	O
way	O
to	O
solve	O
the	O
problem	O
another	O
is	O
to	O
relax	O
the	O
constraints	O
that	O
ci	B
be	O
binary	O
and	O
allow	O
them	O
to	O
be	O
real-valued	O
the	O
result	O
turns	O
into	O
an	O
eigenvector	O
problem	O
known	O
as	O
spectral	B
clustering	B
e	O
g	O
and	O
malik	O
in	O
general	O
the	O
technique	O
of	O
performing	O
eigenalysis	O
of	O
graphs	O
is	O
called	O
spectral	B
graph	B
theory	I
going	O
into	O
the	O
details	O
would	O
take	O
us	O
too	O
far	O
afield	O
but	O
below	O
we	O
give	O
a	O
very	O
brief	O
summary	O
based	O
on	O
luxburg	O
since	O
we	O
will	O
encounter	O
some	O
of	O
these	O
ideas	O
later	O
on	O
graph	B
laplacian	I
let	O
w	O
be	O
a	O
symmetric	B
weight	O
matrix	O
for	O
a	O
graph	B
where	O
wij	O
wji	O
let	O
d	O
diagdi	O
be	O
a	O
diaogonal	O
matrix	O
containing	O
the	O
weighted	O
degree	B
of	O
each	O
node	O
we	O
define	O
the	O
graph	B
laplacian	I
as	O
follows	O
l	O
d	O
w	O
this	O
matrix	O
has	O
various	O
important	O
properties	O
because	O
each	O
row	O
sums	O
to	O
zero	O
we	O
have	O
that	O
is	O
an	O
eigenvector	O
with	O
eigenvalue	O
furthermore	O
the	O
matrix	O
is	O
symmetric	B
and	O
positive	O
semi-definite	O
to	O
see	O
this	O
note	O
that	O
f	O
t	O
lf	O
f	O
t	O
df	O
f	O
t	O
wf	O
fifjwij	O
i	O
dif	O
ij	O
i	O
wijfi	O
i	O
dif	O
fifjwij	O
djf	O
j	O
i	O
ij	O
j	O
ij	O
hence	O
f	O
t	O
lf	O
for	O
all	O
f	O
r	O
eigenvalues	O
n	O
n	O
consequently	O
we	O
see	O
that	O
l	O
has	O
n	O
non-negative	O
real-valued	O
to	O
get	O
some	O
intuition	O
as	O
to	O
why	O
l	O
might	O
be	O
useful	O
for	O
graph-based	O
clustering	B
we	O
note	O
the	O
following	O
result	O
theorem	O
the	O
set	O
of	O
eigenvectors	O
of	O
l	O
with	O
eigenvalue	O
is	O
spanned	O
by	O
the	O
indicator	O
vectors	O
where	O
ak	O
are	O
the	O
k	O
connected	O
components	O
of	O
the	O
graph	B
chapter	O
clustering	B
proof	O
let	O
us	O
start	O
with	O
the	O
case	O
k	O
if	O
f	O
is	O
an	O
eigenvector	O
with	O
eigenvalue	O
then	O
ij	O
wijfi	O
if	O
two	O
nodes	B
are	O
connected	O
so	O
wij	O
we	O
must	O
have	O
that	O
fi	O
fj	O
hence	O
f	O
is	O
constant	O
for	O
all	O
vertices	B
which	O
are	O
connected	O
by	O
a	O
path	B
in	O
the	O
graph	B
now	O
suppose	O
k	O
in	O
this	O
case	O
l	O
will	O
be	O
block	O
diagonal	B
a	O
similar	B
argument	O
to	O
the	O
above	O
shows	O
that	O
we	O
will	O
have	O
k	O
indicator	O
functions	O
which	O
select	O
out	O
the	O
connected	O
components	O
this	O
suggests	O
the	O
following	O
algorithm	O
compute	O
the	O
first	O
k	O
eigenvectors	O
uk	O
of	O
l	O
let	O
u	O
uk	O
be	O
an	O
n	O
k	O
matrix	O
with	O
the	O
eigenvectors	O
in	O
its	O
columns	O
let	O
yi	O
r	O
k	O
be	O
the	O
i	O
th	O
row	O
of	O
u	O
since	O
these	O
yi	O
will	O
be	O
piecewise	O
constant	O
we	O
can	O
apply	O
k-means	O
clustering	B
to	O
them	O
to	O
recover	O
the	O
connected	O
components	O
now	O
assign	O
point	O
i	O
to	O
cluster	O
k	O
iff	B
row	O
i	O
of	O
y	O
was	O
assigned	O
to	O
cluster	O
k	O
in	O
reality	O
we	O
do	O
not	O
expect	O
a	O
graph	B
derived	O
from	O
a	O
real	O
similarity	O
matrix	O
to	O
have	O
isolated	O
connected	O
components	O
that	O
would	O
be	O
too	O
easy	O
but	O
it	O
is	O
reasonable	O
to	O
suppose	O
the	O
graph	B
is	O
a	O
small	O
perturbation	O
from	O
such	O
an	O
ideal	O
in	O
this	O
case	O
one	O
can	O
use	O
results	O
from	O
perturbation	B
theory	I
to	O
show	O
that	O
the	O
eigenvectors	O
of	O
the	O
perturbed	O
laplacian	O
will	O
be	O
close	O
to	O
these	O
ideal	O
indicator	O
functions	O
et	O
al	O
note	O
that	O
this	O
approach	O
is	O
related	O
to	O
kernel	B
pca	B
in	O
particular	O
kpca	O
uses	O
the	O
largest	O
eigenvectors	O
of	O
w	O
these	O
are	O
equivalent	O
to	O
the	O
smallest	O
eigenvectors	O
of	O
i	O
w	O
this	O
is	O
similar	B
to	O
the	O
above	O
method	O
which	O
computes	O
the	O
smallest	O
eigenvectors	O
of	O
l	O
d	O
w	O
see	O
in	O
practice	O
spectral	B
clustering	B
gives	O
much	O
better	O
results	O
than	O
et	O
al	O
for	O
details	O
kpca	O
normalized	O
graph	B
laplacian	I
in	O
practice	O
it	O
is	O
important	O
to	O
normalize	O
the	O
graph	B
laplacian	I
to	O
account	O
for	O
the	O
fact	O
that	O
some	O
nodes	B
are	O
more	O
highly	O
connected	O
than	O
others	O
there	O
are	O
two	O
comon	O
ways	O
to	O
do	O
this	O
one	O
method	O
used	O
in	O
e	O
g	O
and	O
malik	O
meila	O
creates	O
a	O
stochastic	B
matrix	I
where	O
each	O
row	O
sums	O
to	O
one	O
lrw	O
d	O
i	O
d	O
the	O
eigenvalues	O
and	O
eigenvectors	O
of	O
l	O
and	O
lrw	O
are	O
closely	O
related	O
to	O
each	O
other	O
luxburg	O
for	O
details	O
furthemore	O
one	O
can	O
show	O
that	O
for	O
lrw	O
the	O
eigenspace	O
of	O
is	O
again	O
spanned	O
by	O
the	O
indicator	O
vectors	O
this	O
suggests	O
the	O
following	O
algorithm	O
find	O
the	O
smallest	O
k	O
eigenvectors	O
of	O
lrw	O
create	O
u	O
cluster	O
the	O
rows	O
of	O
u	O
using	O
k-means	O
then	O
infer	O
the	O
partitioning	B
of	O
the	O
original	O
points	O
and	O
malik	O
that	O
the	O
eigenvectors	O
values	O
of	O
lrw	O
are	O
equivalent	O
to	O
the	O
generalized	O
eigenvectors	O
values	O
of	O
l	O
which	O
solve	O
lu	O
du	O
ld	O
k	O
wd	O
i	O
d	O
another	O
method	O
used	O
in	O
e	O
g	O
et	O
al	O
creates	O
a	O
symmetric	B
matrix	O
lsym	O
d	O
this	O
time	O
the	O
eigenspace	O
of	O
is	O
spanned	O
by	O
d	O
this	O
suggest	O
the	O
following	O
algorithm	O
find	O
the	O
smallest	O
k	O
eigenvectors	O
of	O
lsym	O
create	O
u	O
normalize	O
each	O
row	O
to	O
unit	O
norm	O
by	O
creating	O
tij	O
uij	O
ik	O
cluster	O
the	O
rows	O
of	O
t	O
using	O
k-means	O
then	O
infer	O
the	O
partitioning	B
of	O
the	O
original	O
points	O
et	O
al	O
there	O
is	O
an	O
interesting	O
connection	O
between	O
ncuts	O
and	O
random	O
walks	O
on	O
a	O
graph	B
first	O
note	O
that	O
p	O
d	O
i	O
lrw	O
is	O
a	O
stochastic	B
matrix	I
where	O
pij	O
wijdi	O
hierarchical	B
clustering	B
y	O
k	O
means	O
clustering	B
x	O
y	O
spectral	B
clustering	B
x	O
figure	O
clustering	B
data	O
consisting	O
of	O
spirals	O
k-means	O
spectral	B
clustering	B
figure	O
generated	O
by	O
spectralclusteringdemo	O
written	O
by	O
wei-lwun	O
lu	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
of	O
going	O
from	O
i	O
to	O
j	O
if	O
the	O
graph	B
is	O
connected	O
and	O
non-bipartite	O
it	O
possesses	O
a	O
unique	O
stationary	B
distribution	I
n	O
where	O
i	O
divolv	O
furthermore	O
one	O
can	O
show	O
that	O
ncuta	O
a	O
paa	O
aa	O
this	O
means	O
that	O
we	O
are	O
looking	O
for	O
a	O
cut	O
such	O
that	O
a	O
random	O
walk	O
rarely	O
makes	O
transitions	O
from	O
a	O
to	O
a	O
or	O
vice	O
versa	O
example	O
figure	O
illustrates	O
the	O
method	O
in	O
action	B
in	O
figure	O
we	O
see	O
that	O
k-means	O
does	O
a	O
poor	O
job	O
of	O
clustering	B
since	O
it	O
implicitly	O
assumes	O
each	O
cluster	O
corresponds	O
to	O
a	O
spherical	B
gaussian	B
next	O
we	O
try	O
spectral	B
clustering	B
we	O
define	O
a	O
similarity	O
matrix	O
using	O
the	O
gaussian	B
kernel	B
we	O
compute	O
the	O
first	O
two	O
eigenvectors	O
of	O
the	O
laplacian	O
from	O
this	O
we	O
can	O
infer	O
the	O
clustering	B
in	O
figure	O
since	O
the	O
method	O
is	O
based	O
on	O
finding	O
the	O
smallest	O
k	O
eigenvectors	O
of	O
a	O
sparse	B
matrix	O
it	O
takes	O
on	O
time	O
however	O
a	O
variety	O
of	O
methods	O
can	O
be	O
used	O
to	O
scale	O
it	O
up	O
for	O
large	O
datasets	O
e	O
g	O
et	O
al	O
hierarchical	B
clustering	B
mixture	B
models	O
whether	O
finite	O
or	O
infinite	O
produce	O
a	O
flat	O
clustering	B
often	O
we	O
want	O
to	O
learn	O
a	O
hierarchical	B
clustering	B
where	O
clusters	B
can	O
be	O
nested	O
inside	O
each	O
other	O
there	O
are	O
two	O
main	O
approaches	O
to	O
hierarchical	B
clustering	B
bottom-up	O
or	O
agglomerative	B
clustering	B
and	O
top-down	O
or	O
divisive	B
clustering	B
both	O
methods	O
take	O
as	O
input	O
a	O
dissimilarity	B
matrix	I
between	O
the	O
objects	O
in	O
the	O
bottom-up	O
approach	O
the	O
most	O
similar	B
groups	O
are	O
merged	O
at	O
each	O
chapter	O
clustering	B
figure	O
an	O
example	O
of	O
single	B
link	I
clustering	B
using	O
city	B
block	I
distance	I
pairs	O
and	O
are	O
both	O
distance	O
apart	O
so	O
get	O
merged	O
first	O
the	O
resulting	O
dendrogram	B
based	O
on	O
figure	O
of	O
figure	O
generated	O
by	O
agglomdemo	O
hierarchical	B
clustering	B
of	O
profiles	O
figure	O
hierarchical	B
clustering	B
applied	O
to	O
the	O
yeast	O
gene	O
expression	O
data	O
the	O
rows	O
are	O
permuted	O
according	O
to	O
a	O
hierarchical	B
clustering	B
scheme	O
link	O
agglomerative	B
clustering	B
in	O
order	O
to	O
bring	O
similar	B
rows	O
close	O
together	O
clusters	B
induced	O
by	O
cutting	O
the	O
average	O
linkage	O
tree	B
at	O
a	O
certain	O
height	O
figure	O
generated	O
by	O
hclustyeastdemo	O
in	O
the	O
top-down	O
approach	O
groups	O
are	O
split	O
using	O
various	O
different	O
criteria	O
we	O
give	O
the	O
step	O
details	O
below	O
note	O
that	O
agglomerative	O
and	O
divisive	B
clustering	B
are	O
both	O
just	O
heuristics	B
which	O
do	O
not	O
optimize	O
any	O
well-defined	O
objective	O
function	O
thus	O
it	O
is	O
hard	O
to	O
assess	O
the	O
quality	O
of	O
the	O
clustering	B
they	O
produce	O
in	O
any	O
formal	O
sense	O
furthermore	O
they	O
will	O
always	O
produce	O
a	O
clustering	B
of	O
the	O
input	O
data	O
even	O
if	O
the	O
data	O
has	O
no	O
structure	O
at	O
all	O
it	O
is	O
random	O
noise	O
later	O
in	O
this	O
section	O
we	O
will	O
discuss	O
a	O
probabilistic	O
version	O
of	O
hierarchical	B
clustering	B
that	O
solves	O
both	O
these	O
problems	O
hierarchical	B
clustering	B
algorithm	O
agglomerative	B
clustering	B
initialize	O
clusters	B
as	O
singletons	O
for	O
i	O
to	O
n	O
do	O
ci	B
initialize	O
set	O
of	O
clusters	B
available	O
for	O
merging	O
s	O
n	O
repeat	O
pick	O
most	O
similar	B
clusters	B
to	O
merge	O
k	O
arg	O
minjk	O
s	O
djk	O
create	O
new	O
cluster	O
cj	O
ck	O
mark	O
j	O
and	O
k	O
as	O
unavailable	O
s	O
s	O
k	O
if	O
n	O
then	O
foreach	O
i	O
s	O
do	O
mark	O
as	O
available	O
s	O
s	O
update	O
dissimilarity	B
matrix	I
di	O
until	O
no	O
more	O
clusters	B
are	O
available	O
for	O
merging	O
figure	O
illustration	O
of	O
single	O
linkage	O
complete	B
linkage	O
average	O
linkage	O
agglomerative	B
clustering	B
agglomerative	B
clustering	B
starts	O
with	O
n	O
groups	O
each	O
initially	O
containing	O
one	O
object	O
and	O
then	O
at	O
each	O
step	O
it	O
merges	O
the	O
two	O
most	O
similar	B
groups	O
until	O
there	O
is	O
a	O
single	O
group	O
containing	O
all	O
the	O
data	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
since	O
picking	O
the	O
two	O
most	O
similar	B
clusters	B
to	O
merge	O
takes	O
on	O
time	O
and	O
there	O
are	O
on	O
steps	O
in	O
the	O
algorithm	O
the	O
total	O
running	O
time	O
is	O
on	O
however	O
by	O
using	O
a	O
priority	O
queue	O
this	O
can	O
be	O
reduced	O
to	O
on	O
log	O
n	O
e	O
g	O
et	O
al	O
ch	O
for	O
details	O
for	O
large	O
n	O
a	O
common	O
heuristic	O
is	O
to	O
first	O
run	O
k-means	O
which	O
takes	O
okn	O
d	O
time	O
and	O
then	O
apply	O
hierarchical	B
clustering	B
to	O
the	O
estimated	O
cluster	O
centers	O
the	O
merging	O
process	O
can	O
be	O
represented	O
by	O
a	O
binary	B
tree	B
called	O
a	O
dendrogram	B
as	O
shown	O
in	O
figure	O
the	O
initial	O
groups	O
are	O
at	O
the	O
leaves	B
the	O
bottom	O
of	O
the	O
figure	O
and	O
every	O
time	O
two	O
groups	O
are	O
merged	O
we	O
join	O
them	O
in	O
the	O
tree	B
the	O
height	O
of	O
the	O
branches	O
represents	O
the	O
dissimilarity	O
between	O
the	O
groups	O
that	O
are	O
being	O
joined	O
the	O
root	B
of	O
the	O
tree	B
is	O
at	O
the	O
top	O
represents	O
a	O
group	O
containing	O
all	O
the	O
data	O
if	O
we	O
cut	O
the	O
tree	B
at	O
any	O
given	O
height	O
we	O
induce	O
a	O
clustering	B
of	O
a	O
given	O
size	O
for	O
example	O
if	O
we	O
cut	O
the	O
tree	B
in	O
figure	O
at	O
height	O
we	O
get	O
the	O
clustering	B
we	O
discuss	O
the	O
issue	O
of	O
how	O
to	O
choose	O
the	O
height	O
number	O
of	O
clusters	B
below	O
a	O
more	O
complex	O
example	O
is	O
shown	O
in	O
figure	O
where	O
we	O
show	O
some	O
gene	O
expression	O
if	O
we	O
cut	O
the	O
tree	B
in	O
figure	O
at	O
a	O
certain	O
height	O
we	O
get	O
the	O
clusters	B
shown	O
in	O
data	O
figure	O
there	O
are	O
actually	O
three	O
variants	O
of	O
agglomerative	B
clustering	B
depending	O
on	O
how	O
we	O
define	O
the	O
dissimilarity	O
between	O
groups	O
of	O
objects	O
these	O
can	O
give	O
quite	O
different	O
results	O
as	O
shown	O
in	O
chapter	O
clustering	B
single	O
link	O
complete	B
link	O
average	O
link	O
figure	O
hierarchical	B
clustering	B
of	O
yeast	O
gene	O
expression	O
data	O
single	O
linkage	O
complete	B
linkage	O
average	O
linkage	O
figure	O
generated	O
by	O
hclustyeastdemo	O
hierarchical	B
clustering	B
figure	O
we	O
give	O
the	O
details	O
below	O
single	O
link	O
in	O
single	B
link	I
clustering	B
also	O
called	O
nearest	B
neighbor	I
clustering	B
the	O
distance	O
between	O
two	O
groups	O
g	O
and	O
h	O
is	O
defined	O
as	O
the	O
distance	O
between	O
the	O
two	O
closest	O
members	O
of	O
each	O
group	O
dslg	O
h	O
min	O
i	O
h	O
see	O
figure	O
the	O
tree	B
built	O
using	O
single	B
link	I
clustering	B
is	O
a	O
minimum	B
spanning	I
tree	B
of	O
the	O
data	O
which	O
is	O
a	O
tree	B
that	O
connects	O
all	O
the	O
objects	O
in	O
a	O
way	O
that	O
minimizes	O
the	O
sum	O
of	O
the	O
edge	O
weights	O
to	O
see	O
this	O
note	O
that	O
when	O
we	O
merge	O
two	O
clusters	B
we	O
connect	O
together	O
the	O
two	O
closest	O
members	O
of	O
the	O
clusters	B
this	O
adds	O
an	O
edge	O
between	O
the	O
corresponding	O
nodes	B
and	O
this	O
is	O
guaranteed	O
to	O
be	O
the	O
lightest	O
weight	O
edge	O
joining	O
these	O
two	O
clusters	B
and	O
once	O
two	O
clusters	B
have	O
been	O
merged	O
they	O
will	O
never	O
be	O
considered	O
again	O
so	O
we	O
cannot	O
create	O
cycles	O
as	O
a	O
consequence	O
of	O
this	O
we	O
can	O
actually	O
implement	O
single	B
link	I
clustering	B
in	O
on	O
time	O
whereas	O
the	O
other	O
variants	O
take	O
on	O
time	O
complete	B
link	O
in	O
complete	B
link	I
clustering	B
also	O
called	O
furthest	B
neighbor	I
clustering	B
the	O
distance	O
between	O
two	O
groups	O
is	O
defined	O
as	O
the	O
distance	O
between	O
the	O
two	O
most	O
distant	O
pairs	O
dclg	O
h	O
max	O
i	O
h	O
see	O
figure	O
single	O
linkage	O
only	O
requires	O
that	O
a	O
single	O
pair	O
of	O
objects	O
be	O
close	O
for	O
the	O
two	O
groups	O
to	O
be	O
considered	O
close	O
together	O
regardless	O
of	O
the	O
similarity	O
of	O
the	O
other	O
members	O
of	O
the	O
group	O
thus	O
clusters	B
can	O
be	O
formed	O
that	O
violate	O
the	O
compactness	B
property	O
which	O
says	O
that	O
all	O
the	O
observations	O
within	O
a	O
group	O
should	O
be	O
similar	B
to	O
each	O
other	O
in	O
particular	O
if	O
we	O
define	O
the	O
diameter	B
of	O
a	O
group	O
as	O
the	O
largest	O
dissimilarity	O
of	O
its	O
members	O
dg	O
maxi	O
g	O
then	O
we	O
can	O
see	O
that	O
single	O
linkage	O
can	O
produce	O
clusters	B
with	O
large	O
diameters	O
complete	B
linkage	O
represents	O
the	O
opposite	O
extreme	O
two	O
groups	O
are	O
considered	O
close	O
only	O
if	O
all	O
of	O
the	O
observations	O
in	O
their	O
union	O
are	O
relatively	O
similar	B
this	O
will	O
tend	O
to	O
produce	O
clusterings	O
with	O
small	O
diameter	B
i	O
e	O
compact	O
clusters	B
average	O
link	O
in	O
practice	O
the	O
preferred	O
method	O
is	O
average	B
link	I
clustering	B
which	O
measures	O
the	O
average	O
distance	O
between	O
all	B
pairs	I
ngnh	O
i	O
g	O
h	O
davgg	O
h	O
where	O
ng	O
and	O
nh	O
are	O
the	O
number	O
of	O
elements	O
in	O
groups	O
g	O
and	O
h	O
see	O
figure	O
average	B
link	I
clustering	B
represents	O
a	O
compromise	O
between	O
single	O
and	O
complete	B
link	I
clustering	B
it	O
tends	O
to	O
produce	O
relatively	O
compact	O
clusters	B
that	O
are	O
relatively	O
far	O
apart	O
however	O
since	O
it	O
chapter	O
clustering	B
involves	O
averaging	O
of	O
the	O
s	O
any	O
change	O
to	O
the	O
measurement	O
scale	O
can	O
change	O
the	O
result	O
in	O
contrast	O
single	O
linkage	O
and	O
complete	B
linkage	O
are	O
invariant	B
to	O
monotonic	O
transformations	O
of	O
since	O
they	O
leave	O
the	O
relative	O
ordering	O
the	O
same	O
divisive	B
clustering	B
divisive	B
clustering	B
starts	O
with	O
all	O
the	O
data	O
in	O
a	O
single	O
cluster	O
and	O
then	O
recursively	O
divides	O
each	O
cluster	O
into	O
two	O
daughter	O
clusters	B
in	O
a	O
top-down	O
fashion	O
since	O
there	O
are	O
ways	O
to	O
split	O
a	O
group	O
of	O
n	O
items	O
into	O
groups	O
it	O
is	O
hard	O
to	O
compute	O
the	O
optimal	O
split	O
so	O
various	O
heuristics	B
are	O
used	O
one	O
approach	O
is	O
pick	O
the	O
cluster	O
with	O
the	O
largest	O
diameter	B
and	O
split	O
it	O
in	O
two	O
using	O
the	O
k-means	O
or	O
k-medoids	O
algorithm	O
with	O
k	O
this	O
is	O
called	O
the	O
bisecting	B
k-means	B
algorithm	I
et	O
al	O
we	O
can	O
repeat	O
this	O
until	O
we	O
have	O
any	O
desired	O
number	O
of	O
clusters	B
this	O
can	O
be	O
used	O
as	O
an	O
alternative	O
to	O
regular	B
k-means	O
but	O
it	O
also	O
induces	O
a	O
hierarchical	B
clustering	B
another	O
method	O
is	O
to	O
build	O
a	O
minimum	B
spanning	I
tree	B
from	O
the	O
dissimilarity	O
graph	B
and	O
then	O
to	O
make	O
new	O
clusters	B
by	O
breaking	O
the	O
link	O
corresponding	O
to	O
the	O
largest	O
dissimilarity	O
actually	O
gives	O
the	O
same	O
results	O
as	O
single	O
link	O
agglomerative	B
clustering	B
is	O
as	O
follows	O
we	O
start	O
with	O
a	O
single	O
cluster	O
containing	O
all	O
the	O
data	O
g	O
n	O
we	O
then	O
measure	O
the	O
average	O
dissimilarity	O
of	O
i	O
g	O
to	O
all	O
the	O
other	O
g	O
yet	O
another	O
method	O
called	O
dissimilarity	B
analysis	I
et	O
al	O
g	O
dg	O
i	O
ng	O
we	O
remove	O
the	O
most	O
dissimilar	O
object	O
and	O
put	O
it	O
in	O
its	O
own	O
cluster	O
h	O
i	O
arg	O
max	O
i	O
g	O
i	O
g	O
g	O
h	O
dg	O
we	O
now	O
continue	O
to	O
move	O
objects	O
from	O
g	O
to	O
h	O
until	O
some	O
stopping	O
criterion	O
is	O
met	O
specifically	O
to	O
move	O
that	O
maximizes	O
the	O
average	O
dissimilarity	O
to	O
each	O
g	O
but	O
minimizes	O
we	O
pick	O
a	O
point	O
i	O
the	O
average	O
dissimilarity	O
to	O
each	O
h	O
dh	O
i	O
nh	O
h	O
i	O
i	O
dh	O
dg	O
i	O
arg	O
max	O
i	O
g	O
i	O
dh	O
i	O
we	O
continue	O
to	O
do	O
this	O
until	O
dg	O
is	O
negative	O
the	O
final	O
result	O
is	O
that	O
we	O
have	O
split	O
g	O
into	O
two	O
daughter	O
clusters	B
g	O
and	O
h	O
we	O
can	O
then	O
recursively	O
call	O
the	O
algorithm	O
on	O
g	O
andor	O
h	O
or	O
on	O
any	O
other	O
node	O
in	O
the	O
tree	B
for	O
example	O
we	O
might	O
choose	O
to	O
split	O
the	O
node	O
g	O
whose	O
average	O
dissimilarity	O
is	O
highest	O
or	O
whose	O
maximum	O
dissimilarity	O
diameter	B
is	O
highest	O
we	O
continue	O
the	O
process	O
until	O
the	O
average	O
dissimilarity	O
within	O
each	O
cluster	O
is	O
below	O
some	O
threshold	O
andor	O
all	O
clusters	B
are	O
singletons	O
divisive	B
clustering	B
is	O
less	O
popular	O
than	O
agglomerative	B
clustering	B
but	O
it	O
has	O
two	O
advantages	O
first	O
it	O
can	O
be	O
faster	O
since	O
if	O
we	O
only	O
split	O
for	O
a	O
constant	O
number	O
of	O
levels	O
it	O
takes	O
just	O
on	O
time	O
second	O
the	O
splitting	O
decisions	O
are	O
made	O
in	O
the	O
context	O
of	O
seeing	O
all	O
the	O
data	O
whereas	O
bottom-up	O
methods	O
make	O
myopic	O
merge	O
decisions	O
hierarchical	B
clustering	B
choosing	O
the	O
number	O
of	O
clusters	B
it	O
is	O
difficult	O
to	O
choose	O
the	O
right	O
number	O
of	O
clusters	B
since	O
a	O
hierarchical	B
clustering	B
algorithm	O
will	O
always	O
create	O
a	O
hierarchy	O
even	O
if	O
the	O
data	O
is	O
completely	O
random	O
but	O
as	O
with	O
choosing	O
k	O
for	O
k-means	O
there	O
is	O
the	O
hope	O
that	O
there	O
will	O
be	O
a	O
visible	B
gap	B
in	O
the	O
lengths	O
of	O
the	O
links	O
in	O
the	O
dendrogram	B
the	O
dissimilarity	O
between	O
merged	O
groups	O
between	O
natural	O
clusters	B
and	O
unnatural	O
clusters	B
of	O
course	O
on	O
real	O
data	O
this	O
gap	B
might	O
be	O
hard	O
to	O
detect	O
in	O
section	O
we	O
will	O
present	O
a	O
bayesian	B
approach	O
to	O
hierarchical	B
clustering	B
that	O
nicely	O
solves	O
this	O
problem	O
bayesian	B
hierarchical	B
clustering	B
there	O
are	O
several	O
ways	O
to	O
make	O
probabilistic	O
models	O
which	O
produce	O
results	O
similar	B
to	O
hierarchical	B
clustering	B
e	O
g	O
neal	O
castro	O
et	O
al	O
lau	O
and	O
green	O
here	O
we	O
present	O
one	O
particular	O
approach	O
called	O
bayesian	B
hierarchical	B
clustering	B
and	O
ghahramani	O
algorithmically	O
it	O
is	O
very	O
similar	B
to	O
standard	O
bottom-up	O
agglomerative	B
clustering	B
and	O
takes	O
comparable	O
time	O
whereas	O
several	O
of	O
the	O
other	O
techniques	O
referenced	O
above	O
are	O
much	O
slower	O
however	O
it	O
uses	O
bayesian	B
hypothesis	O
tests	O
to	O
decide	O
which	O
clusters	B
to	O
merge	O
any	O
rather	O
than	O
computing	O
the	O
similarity	O
between	O
groups	O
of	O
points	O
in	O
some	O
ad-hoc	O
way	O
these	O
hypothesis	O
tests	O
are	O
closely	O
related	O
to	O
the	O
calculations	O
required	O
to	O
do	O
inference	B
in	O
a	O
dirichlet	B
process	I
mixture	B
model	I
as	O
we	O
will	O
see	O
furthermore	O
the	O
input	O
to	O
the	O
model	O
is	O
a	O
data	O
matrix	O
not	O
a	O
dissimilarity	B
matrix	I
the	O
algorithm	O
let	O
d	O
xn	O
represent	O
all	O
the	O
data	O
and	O
let	O
di	O
be	O
the	O
set	O
of	O
datapoints	O
at	O
the	O
leaves	B
of	O
the	O
substree	O
ti	O
at	O
each	O
step	O
we	O
compare	O
two	O
trees	O
ti	O
and	O
tj	O
to	O
see	O
if	O
they	O
should	O
be	O
merged	O
into	O
a	O
new	O
tree	B
define	O
dij	O
as	O
their	O
merged	O
data	O
and	O
let	O
mij	O
if	O
they	O
should	O
be	O
merged	O
and	O
mij	O
otherwise	O
the	O
probability	O
of	O
a	O
merge	O
is	O
given	O
by	O
pdijtij	O
rij	O
pdijmij	O
pdijtij	O
dijmij	O
pdijmij	O
here	O
pmij	O
is	O
the	O
prior	O
probability	O
of	O
a	O
merge	O
which	O
can	O
be	O
computed	O
using	O
a	O
bottom-up	O
algorithm	O
described	O
below	O
we	O
now	O
turn	O
to	O
the	O
likelihood	B
terms	O
if	O
mij	O
the	O
data	O
in	O
dij	O
is	O
assumed	O
to	O
come	O
from	O
the	O
same	O
model	O
and	O
hence	O
p	O
pdijmij	O
pxn	O
if	O
mij	O
the	O
data	O
in	O
dij	O
is	O
assumed	O
to	O
have	O
been	O
generated	O
by	O
each	O
tree	B
independently	O
so	O
pdijmij	O
pditipdjtj	O
xn	O
dij	O
these	O
two	O
terms	O
will	O
have	O
already	O
been	O
computed	O
by	O
the	O
bottom-up	O
process	O
consequently	O
we	O
have	O
all	O
the	O
quantities	O
we	O
need	O
to	O
decide	O
which	O
trees	O
to	O
merge	O
see	O
algorithm	O
for	O
the	O
pseudocode	O
assuming	O
pmij	O
is	O
uniform	O
when	O
finished	O
we	O
can	O
cut	O
the	O
tree	B
at	O
points	O
where	O
rij	O
chapter	O
clustering	B
algorithm	O
bayesian	B
hierarchical	B
clustering	B
initialize	O
di	O
i	O
compute	O
pditi	O
i	O
repeat	O
for	O
each	O
pair	O
of	O
clusters	B
i	O
j	O
do	O
compute	O
pdijtij	O
find	O
the	O
pair	O
di	O
and	O
dj	O
with	O
highest	O
merge	O
probability	O
rij	O
merge	O
dk	O
di	O
dj	O
delete	O
di	O
dj	O
until	O
all	O
clusters	B
merged	O
the	O
connection	O
with	O
dirichlet	B
process	I
mixture	B
models	I
in	O
this	O
section	O
we	O
will	O
establish	O
the	O
connection	O
between	O
bhc	O
and	O
dpmms	O
this	O
will	O
in	O
turn	O
give	O
us	O
an	O
algorithm	O
to	O
compute	O
the	O
prior	O
probabilities	O
pmij	O
note	O
that	O
the	O
marginal	B
likelihood	B
of	O
a	O
dpmm	O
summing	O
over	O
all	O
partitions	O
is	O
given	O
by	O
pdk	O
pvpdv	O
l	O
v	O
v	O
mv	O
pdv	O
l	O
pv	O
pdv	O
l	O
where	O
v	O
is	O
the	O
set	O
of	O
all	O
possible	O
partitions	O
of	O
dk	O
pv	O
is	O
the	O
probability	O
of	O
partition	O
v	O
mv	O
is	O
is	O
the	O
number	O
of	O
points	O
in	O
cluster	O
l	O
of	O
partition	O
v	O
dv	O
the	O
number	O
of	O
clusters	B
in	O
partition	O
v	O
nv	O
are	O
the	O
points	O
in	O
cluster	O
l	O
of	O
partition	O
v	O
and	O
nk	O
are	O
the	O
number	O
of	O
points	O
in	O
dk	O
l	O
one	O
can	O
show	O
and	O
ghahramani	O
that	O
pdktk	O
computed	O
by	O
the	O
bhc	O
algorithm	O
is	O
similar	B
to	O
pdk	O
given	O
above	O
except	O
for	O
the	O
fact	O
that	O
it	O
only	O
sums	O
over	O
partitions	O
which	O
are	O
consistent	B
with	O
tree	B
tk	O
number	O
of	O
tree-consistent	O
partitions	O
is	O
exponential	O
in	O
the	O
number	O
of	O
data	O
points	O
for	O
balanced	O
binary	O
trees	O
but	O
this	O
is	O
obviously	O
a	O
subset	O
of	O
all	O
possible	O
partitions	O
in	O
this	O
way	O
we	O
can	O
use	O
the	O
bhc	O
algorithm	O
to	O
compute	O
a	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	B
of	O
the	O
data	O
from	O
a	O
dpmm	O
furthermore	O
we	O
can	O
interpret	O
the	O
algorithm	O
as	O
greedily	O
searching	O
through	O
the	O
exponentially	O
large	O
space	O
of	O
tree-consistent	O
partitions	O
to	O
find	O
the	O
best	O
ones	O
of	O
a	O
given	O
size	O
at	O
each	O
step	O
we	O
are	O
now	O
in	O
a	O
position	O
to	O
compute	O
k	O
pmk	O
for	O
each	O
node	O
k	O
with	O
children	B
i	O
and	O
j	O
this	O
is	O
equal	O
to	O
the	O
probability	O
of	O
cluster	O
dk	O
coming	O
from	O
the	O
dpmm	O
relative	O
to	O
all	O
other	O
partitions	O
of	O
dk	O
consistent	B
with	O
the	O
current	O
tree	B
this	O
can	O
be	O
computed	O
as	O
follows	O
initialize	O
di	O
and	O
i	O
for	O
each	O
leaf	B
i	O
then	O
as	O
we	O
build	O
the	O
tree	B
for	O
each	O
internal	O
node	O
k	O
compute	O
dk	O
idj	O
and	O
k	O
wherei	O
and	O
j	O
are	O
k	O
s	O
left	O
and	O
right	O
children	B
dk	O
clustering	B
datapoints	O
and	O
features	B
data	O
set	O
synthetic	O
newsgroups	O
spambase	O
digits	O
fglass	O
single	O
linkage	O
complete	B
linkage	O
average	O
linkage	O
bhc	O
table	O
purity	B
scores	B
for	O
various	O
hierarchical	B
clustering	B
schemes	O
applied	O
to	O
various	O
data	O
sets	O
the	O
synthetic	O
data	O
has	O
n	O
d	O
c	O
and	O
real	O
features	B
newsgroups	O
is	O
extracted	O
from	O
the	O
newsgroups	O
dataset	O
n	O
c	O
binary	O
features	B
spambase	O
has	O
n	O
c	O
d	O
binary	O
features	B
digits	O
is	O
the	O
cedar	O
buffalo	O
digits	O
c	O
d	O
binarized	O
features	B
fglass	O
is	O
forensic	O
glass	O
dataset	O
c	O
d	O
real	O
features	B
source	O
table	O
of	O
and	O
ghahramani	O
used	O
with	O
kind	O
permission	O
of	O
katherine	O
heller	O
learning	B
the	O
hyper-parameters	B
the	O
model	O
has	O
two	O
free-parameters	O
and	O
where	O
are	O
the	O
hyper-parameters	B
for	O
the	O
prior	O
on	O
the	O
parameters	O
in	O
and	O
ghahramani	O
they	O
show	O
how	O
one	O
can	O
back-propagate	O
gradients	O
of	O
the	O
form	O
pdktk	O
through	O
the	O
tree	B
and	O
thus	O
perform	O
an	O
empirical	B
bayes	I
estimate	O
of	O
the	O
hyper-parameters	B
experimental	O
results	O
and	O
ghahramani	O
compared	O
bhc	O
with	O
traditional	O
agglomerative	B
clustering	B
algorithms	O
on	O
various	O
data	O
sets	O
in	O
terms	O
of	O
purity	B
scores	B
the	O
results	O
are	O
shown	O
in	O
table	O
we	O
see	O
that	O
bhc	O
did	O
much	O
better	O
than	O
the	O
other	O
methods	O
on	O
all	O
datasets	O
except	O
the	O
forensic	O
glass	O
one	O
figure	O
visualizes	O
the	O
tree	B
structure	O
estimated	O
by	O
bhc	O
and	O
agglomerative	B
hierarchical	B
clustering	B
on	O
the	O
newsgroup	O
data	O
a	O
beta-bernoulli	O
model	O
the	O
bhc	O
tree	B
is	O
clearly	O
superior	O
at	O
the	O
colors	O
at	O
the	O
leaves	B
which	O
represent	O
class	O
labels	O
figure	O
is	O
a	O
zoom-in	O
on	O
the	O
top	O
few	O
nodes	B
of	O
these	O
two	O
trees	O
bhc	O
splits	O
off	O
clusters	B
concerning	O
sports	O
from	O
clusters	B
concerning	O
cars	O
and	O
space	O
ahc	O
keeps	O
sports	O
and	O
cars	O
merged	O
together	O
although	O
sports	O
and	O
cars	O
both	O
fall	O
under	O
the	O
same	O
rec	O
newsgroup	O
heading	O
opposed	O
to	O
space	O
that	O
comes	O
under	O
the	O
sci	O
newsgroup	O
heading	O
the	O
bhc	O
clustering	B
still	O
seems	O
more	O
reasonable	O
and	O
this	O
is	O
borne	O
out	O
by	O
the	O
quantitative	O
purity	B
scores	B
bhc	O
has	O
also	O
been	O
applied	O
to	O
gene	O
expression	O
data	O
with	O
good	O
results	O
et	O
al	O
clustering	B
datapoints	O
and	O
features	B
so	O
far	O
we	O
have	O
been	O
concentrating	O
on	O
clustering	B
datapoints	O
but	O
each	O
datapoint	O
is	O
often	O
described	O
by	O
multiple	O
features	B
and	O
we	O
might	O
be	O
interested	O
in	O
clustering	B
them	O
as	O
well	O
below	O
we	O
describe	O
some	O
methods	O
for	O
doing	O
this	O
chapter	O
clustering	B
newsgroups	O
average	O
linkage	O
clustering	B
newsgroups	O
bayesian	B
hierarchical	B
clustering	B
figure	O
hierarchical	B
clustering	B
applied	O
to	O
documents	O
from	O
newsgroups	O
is	O
rec	O
autos	O
blue	O
is	O
rec	O
sport	O
baseball	O
green	O
is	O
rec	O
sport	O
hockey	O
and	O
magenta	O
is	O
sci	O
space	O
top	O
average	O
linkage	O
hierarchical	B
clustering	B
bottom	O
bayesian	B
hierarchical	B
clustering	B
each	O
of	O
the	O
leaves	B
is	O
labeled	O
with	O
a	O
color	O
according	O
to	O
which	O
newsgroup	O
that	O
document	O
came	O
from	O
we	O
see	O
that	O
the	O
bayesian	B
method	O
results	O
in	O
a	O
clustering	B
that	O
is	O
more	O
consistent	B
with	O
these	O
labels	O
were	O
not	O
used	O
during	O
model	O
fitting	O
source	O
figure	O
of	O
and	O
ghahramani	O
used	O
with	O
kind	O
permission	O
of	O
katherine	O
heller	O
clustering	B
datapoints	O
and	O
features	B
all	O
data	O
game	O
team	O
play	O
car	O
space	O
nasa	O
baseball	O
pitch	O
hit	O
nhl	O
hockey	O
round	O
car	O
dealer	O
drive	O
space	O
nasa	O
orbit	O
all	O
data	O
quebec	O
jet	O
boston	O
car	O
baseball	O
engine	O
pitcher	O
boston	O
ball	O
car	O
player	O
space	O
team	O
game	O
hockey	O
vehicle	O
dealer	O
driver	O
figure	O
zoom-in	O
on	O
the	O
top	O
nodes	B
in	O
the	O
trees	O
of	O
figure	O
average	O
linkage	O
we	O
show	O
the	O
most	O
probable	O
words	O
per	O
cluster	O
the	O
number	O
of	O
documents	O
at	O
each	O
cluster	O
is	O
also	O
given	O
source	O
figure	O
of	O
and	O
ghahramani	O
used	O
with	O
kind	O
permission	O
of	O
katherine	O
heller	O
bayesian	B
method	O
biclustering	B
clustering	B
the	O
rows	O
and	O
columns	O
is	O
known	O
as	O
biclustering	B
or	O
coclustering	B
this	O
is	O
widely	O
used	O
in	O
bioinformatics	O
where	O
the	O
rows	O
often	O
represent	O
genes	O
and	O
the	O
columns	O
represent	O
conditions	O
it	O
can	O
also	O
be	O
used	O
for	O
collaborative	O
filtering	B
where	O
the	O
rows	O
represent	O
users	O
and	O
the	O
columns	O
represent	O
movies	O
a	O
variety	O
of	O
ad	O
hoc	O
methods	O
for	O
biclustering	B
have	O
been	O
proposed	O
see	O
and	O
oliveira	O
for	O
a	O
review	O
here	O
we	O
present	O
a	O
simple	O
probabilistic	O
generative	O
model	O
based	O
on	O
et	O
al	O
also	O
et	O
al	O
for	O
a	O
related	O
approach	O
the	O
idea	O
is	O
to	O
associate	O
each	O
row	O
and	O
each	O
column	O
with	O
a	O
latent	B
indicator	O
ri	O
k	O
r	O
cj	O
k	O
c	O
we	O
then	O
assume	O
the	O
data	O
are	O
iid	B
across	O
samples	B
and	O
across	O
features	B
within	O
each	O
block	O
pxr	O
c	O
pxijri	O
cj	O
pxij	O
ricj	O
i	O
j	O
where	O
ab	O
are	O
the	O
parameters	O
for	O
row	O
cluster	O
a	O
and	O
column	O
cluster	O
b	O
rather	O
than	O
using	O
a	O
finite	O
number	O
of	O
clusters	B
for	O
the	O
rows	O
and	O
columns	O
we	O
can	O
use	O
a	O
dirchlet	B
process	I
as	O
in	O
the	O
infinite	O
relational	O
model	O
which	O
we	O
discuss	O
in	O
section	O
we	O
can	O
fit	O
this	O
model	O
using	O
e	O
g	O
gibbs	B
sampling	I
the	O
behavior	O
of	O
this	O
model	O
is	O
illustrated	O
in	O
figure	O
the	O
data	O
has	O
the	O
form	O
xi	O
j	O
iff	B
animal	O
i	O
has	O
feature	O
j	O
where	O
i	O
and	O
j	O
the	O
animals	O
represent	O
whales	O
bears	O
horses	O
etc	O
the	O
features	B
represent	O
properties	O
of	O
the	O
habitat	O
tree	B
coastal	O
or	O
anatomical	O
properties	O
teeth	O
quadrapedal	O
or	O
behavioral	O
properties	O
eats	O
meat	O
etc	O
the	O
model	O
using	O
a	O
bernoulli	B
likelihood	B
was	O
fit	O
to	O
the	O
data	O
it	O
discovered	O
animal	O
clusters	B
and	O
feature	O
clusters	B
for	O
example	O
it	O
discovered	O
a	O
bicluster	O
that	O
represents	O
the	O
fact	O
that	O
mammals	O
tend	O
to	O
have	O
aquatic	O
features	B
multi-view	O
clustering	B
the	O
problem	O
with	O
biclustering	B
is	O
that	O
each	O
object	O
can	O
only	O
belong	O
to	O
one	O
cluster	O
intuitively	O
an	O
object	O
can	O
have	O
multiple	O
roles	O
and	O
can	O
be	O
assigned	O
to	O
different	O
clusters	B
depending	O
on	O
which	O
chapter	O
clustering	B
killer	O
whale	O
blue	O
whale	O
humpback	O
seal	O
walrus	O
dolphin	O
antelope	O
horse	O
giraffe	O
zebra	O
deer	O
monkey	O
gorilla	O
chimp	O
hippo	O
elephant	O
rhino	O
grizzly	O
bear	O
polar	B
bear	O
flippers	O
strain	O
teeth	O
swims	O
arctic	O
coastal	O
ocean	O
water	O
hooves	O
long	O
neck	O
horns	O
hands	O
bipedal	O
jungle	O
tree	B
bulbous	O
body	O
shape	O
slow	O
inactive	O
meat	O
teeth	O
eats	O
meat	O
hunter	O
fierce	O
walks	O
quadrapedal	O
ground	O
figure	O
illustration	O
of	O
biclustering	B
we	O
show	O
of	O
the	O
animal	O
clusters	B
and	O
of	O
the	O
feature	O
clusters	B
the	O
original	O
data	O
matrix	O
is	O
shown	O
partitioned	O
according	O
to	O
the	O
discovered	O
clusters	B
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
cj	O
jk	O
k	O
j	O
d	O
riv	O
v	O
xij	O
i	O
n	O
figure	O
illustration	O
of	O
multi-view	O
clustering	B
here	O
we	O
have	O
views	B
partitions	O
in	O
the	O
first	O
view	O
we	O
have	O
clusters	B
partitions	O
in	O
the	O
third	O
view	O
we	O
have	O
clusters	B
the	O
number	O
of	O
views	B
and	O
partitions	O
are	O
inferred	O
from	O
data	O
rows	O
within	O
each	O
colored	O
block	O
are	O
assumed	O
to	O
generated	O
iid	B
however	O
each	O
column	O
can	O
have	O
a	O
different	O
distributional	O
form	O
which	O
is	O
useful	O
for	O
modeling	O
discrete	B
and	O
continuous	O
data	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
jennifer	O
dy	O
corresponding	O
dgm	B
in	O
the	O
second	O
view	O
we	O
have	O
clusters	B
subset	O
of	O
features	B
you	O
use	O
for	O
example	O
in	O
the	O
animal	O
dataset	O
we	O
may	O
want	O
to	O
group	O
the	O
animals	O
on	O
the	O
basis	O
of	O
anatomical	O
features	B
mammals	O
are	O
warm	O
blooded	O
reptiles	O
are	O
not	O
or	O
on	O
the	O
basis	O
of	O
behavioral	O
features	B
predators	O
vs	O
prey	O
we	O
now	O
present	O
a	O
model	O
that	O
can	O
capture	O
this	O
phenomenon	O
this	O
model	O
was	O
independently	O
proposed	O
in	O
et	O
al	O
mansinghka	O
et	O
al	O
who	O
call	O
it	O
crosscat	B
crosscategorization	O
and	O
in	O
et	O
al	O
cui	O
et	O
al	O
who	O
call	O
it	O
multi-clust	B
also	O
and	O
ghosh	O
for	O
a	O
very	O
similar	B
model	O
the	O
idea	O
is	O
that	O
we	O
partition	O
the	O
columns	O
into	O
v	O
groups	O
or	O
views	B
so	O
cj	O
v	O
where	O
j	O
d	O
indexes	O
clustering	B
datapoints	O
and	O
features	B
features	B
we	O
will	O
use	O
a	O
dirichlet	B
process	I
prior	O
for	O
pc	O
which	O
allows	O
v	O
to	O
grow	O
automatically	O
then	O
for	O
each	O
partition	O
of	O
the	O
columns	O
each	O
view	O
call	O
it	O
v	O
we	O
partition	O
the	O
rows	O
again	O
using	O
a	O
dp	O
as	O
illustrated	O
in	O
figure	O
let	O
riv	O
kv	O
be	O
the	O
cluster	O
to	O
which	O
the	O
i	O
th	O
row	O
belongs	O
in	O
view	O
v	O
finally	O
having	O
partitioned	O
the	O
rows	O
and	O
columns	O
we	O
generate	O
the	O
data	O
we	O
assume	O
all	O
the	O
rows	O
and	O
columns	O
within	O
a	O
block	O
are	O
iid	B
we	O
can	O
define	O
the	O
model	O
more	O
precisely	O
as	O
follows	O
pc	O
rd	O
cprcpdr	O
c	O
pc	O
c	O
prc	O
v	O
v	O
pdr	O
c	O
dprv	O
pxij	O
jkp	O
jkd	O
jk	O
jcj	O
irivk	O
see	O
figure	O
for	O
the	O
if	O
the	O
data	O
is	O
binary	O
and	O
we	O
use	O
a	O
beta	O
prior	O
for	O
jk	O
the	O
likelihood	B
reduces	O
to	O
v	O
jcj	O
pdr	O
c	O
betanjkv	O
njkv	O
beta	O
where	O
njkv	O
irivk	O
ixij	O
counts	O
the	O
number	O
of	O
features	B
which	O
are	O
on	O
in	O
the	O
j	O
th	O
column	O
for	O
view	O
v	O
and	O
for	O
row	O
cluster	O
k	O
similarly	O
njkv	O
counts	O
how	O
many	O
features	B
are	O
off	O
the	O
model	O
is	O
easily	O
extended	O
to	O
other	O
kinds	O
of	O
data	O
by	O
replacing	O
the	O
beta-bernoulli	O
with	O
say	O
the	O
gaussian-gamma-gaussian	O
model	O
as	O
discussed	O
in	O
et	O
al	O
mansinghka	O
et	O
al	O
approximate	O
map	O
estimation	O
can	O
be	O
done	O
using	O
stochastic	B
search	I
et	O
al	O
and	O
approximate	B
inference	B
can	O
be	O
done	O
using	O
variational	B
bayes	I
et	O
al	O
or	O
gibbs	B
sampling	I
et	O
al	O
the	O
hyper-parameter	O
for	O
the	O
likelihood	B
can	O
usually	O
be	O
set	O
in	O
a	O
noninformative	O
way	O
but	O
results	O
are	O
more	O
sensitive	O
to	O
the	O
other	O
two	O
parameters	O
since	O
controls	O
the	O
number	O
of	O
column	O
partitions	O
and	O
controls	O
the	O
number	O
of	O
row	O
partitions	O
hence	O
a	O
more	O
robust	B
technique	O
is	O
to	O
infer	O
the	O
hyper-parameters	B
using	O
mh	B
this	O
also	O
speeds	O
up	O
convergence	O
et	O
al	O
figure	O
illustrates	O
the	O
model	O
applied	O
to	O
some	O
binary	O
data	O
containing	O
animals	O
and	O
features	B
the	O
figures	O
shows	O
the	O
map	O
partition	O
the	O
first	O
partition	O
of	O
the	O
columns	O
contains	O
taxonomic	O
features	B
such	O
as	O
has	O
bones	O
is	O
warm-blooded	O
lays	O
eggs	O
etc	O
this	O
divides	O
the	O
animals	O
into	O
birds	O
reptiles	O
amphibians	O
mammals	O
and	O
invertebrates	O
the	O
second	O
partition	O
of	O
the	O
columns	O
contains	O
features	B
that	O
are	O
treated	O
as	O
noise	O
with	O
no	O
apparent	O
structure	O
for	O
the	O
single	O
row	O
labeled	O
frog	O
the	O
third	O
partition	O
of	O
the	O
columns	O
contains	O
ecological	O
features	B
like	O
dangerous	O
carnivorous	O
lives	O
in	O
water	O
etc	O
this	O
divides	O
the	O
animals	O
into	O
prey	O
land	O
predators	O
sea	O
predators	O
and	O
air	O
predators	O
thus	O
each	O
animal	O
can	O
belong	O
to	O
a	O
different	O
the	O
dependence	O
between	O
r	O
and	O
c	O
is	O
not	O
shown	O
since	O
it	O
is	O
not	O
a	O
dependence	O
between	O
the	O
values	O
of	O
riv	O
and	O
cj	O
but	O
between	O
the	O
cardinality	O
of	O
v	O
and	O
cj	O
in	O
other	O
words	O
the	O
number	O
of	O
row	O
partitions	O
we	O
need	O
to	O
specify	O
number	O
of	O
views	B
indexed	O
by	O
v	O
depends	O
on	O
the	O
number	O
of	O
column	O
partitions	O
that	O
we	O
have	O
a	O
leopard	O
sheep	O
seal	O
dolphin	O
monkey	O
bat	O
alligator	O
iguana	O
frog	O
python	O
finch	O
ostrich	O
seagull	O
owl	O
penguin	O
eagle	O
grasshopper	O
ant	O
bee	O
jellyfish	O
octopus	O
dragonfly	O
b	O
c	O
frog	O
chapter	O
clustering	B
leopard	O
alligator	O
python	O
seal	O
dolphin	O
frog	O
jellyfish	O
octopus	O
penguin	O
finch	O
seagull	O
owl	O
eagle	O
dragonfly	O
bat	O
grasshopper	O
ant	O
bee	O
sheep	O
monkey	O
iguana	O
ostrich	O
d	O
r	O
a	O
z	O
i	O
l	O
d	O
r	O
o	O
c	O
l	O
n	O
e	O
e	O
r	O
g	O
s	O
i	O
a	O
s	O
i	O
s	O
k	O
w	O
a	O
u	O
q	O
s	O
k	O
a	O
e	O
b	O
a	O
s	O
a	O
h	O
e	O
u	O
g	O
n	O
o	O
t	O
a	O
s	O
a	O
h	O
s	O
r	O
e	O
p	O
p	O
i	O
l	O
f	O
s	O
a	O
h	O
l	O
i	O
a	O
t	O
a	O
s	O
a	O
h	O
i	O
s	O
w	O
a	O
p	O
s	O
a	O
h	O
n	O
a	O
r	O
b	O
e	O
g	O
r	O
a	O
y	O
r	O
r	O
u	O
f	O
s	O
i	O
n	O
w	O
o	O
r	O
b	O
s	O
i	O
e	O
c	O
i	O
m	O
s	O
t	O
a	O
e	O
s	O
t	O
n	O
e	O
d	O
o	O
r	O
s	O
t	O
a	O
e	O
t	O
u	O
o	O
n	O
s	O
a	O
s	O
a	O
h	O
e	O
a	O
n	O
n	O
e	O
t	O
n	O
a	O
s	O
a	O
h	O
i	O
a	O
n	O
p	O
s	O
a	O
s	O
a	O
h	O
l	O
a	O
s	O
a	O
h	O
s	O
g	O
g	O
e	O
s	O
y	O
a	O
s	O
e	O
n	O
o	O
b	O
s	O
a	O
h	O
l	O
l	O
a	O
m	O
m	O
a	O
m	O
a	O
s	O
i	O
l	O
d	O
e	O
d	O
o	O
o	O
b	O
m	O
r	O
a	O
w	O
s	O
i	O
t	O
e	O
e	O
f	O
s	O
a	O
h	O
h	O
t	O
e	O
e	O
t	O
s	O
a	O
h	O
t	O
r	O
a	O
m	O
s	O
s	O
i	O
s	O
p	O
u	O
o	O
r	O
g	O
n	O
i	O
s	O
l	O
e	O
v	O
a	O
r	O
t	O
s	O
e	O
s	O
i	O
o	O
n	O
d	O
u	O
o	O
l	O
s	O
e	O
k	O
a	O
m	O
l	O
l	O
a	O
t	O
s	O
i	O
h	O
s	O
i	O
f	O
a	O
s	O
i	O
y	O
m	O
i	O
l	O
s	O
s	O
i	O
s	O
r	O
a	O
o	O
r	O
s	O
n	O
i	O
f	O
s	O
a	O
h	O
e	O
n	O
i	O
l	O
e	O
f	O
a	O
s	O
i	O
s	O
n	O
r	O
o	O
h	O
s	O
a	O
h	O
s	O
e	O
v	O
o	O
o	O
h	O
s	O
a	O
h	O
t	O
n	O
e	O
d	O
o	O
r	O
a	O
s	O
i	O
s	O
e	O
k	O
a	O
l	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
i	O
i	O
n	O
a	O
b	O
h	O
p	O
m	O
a	O
n	O
a	O
s	O
i	O
h	O
t	O
o	O
o	O
m	O
s	O
e	O
e	O
r	O
t	O
n	O
e	O
g	O
r	O
a	O
l	O
s	O
i	O
s	O
t	O
u	O
n	O
s	O
t	O
a	O
e	O
s	O
s	O
i	O
i	O
s	O
e	O
v	O
i	O
l	O
t	O
e	O
e	O
f	O
d	O
e	O
b	O
b	O
e	O
w	O
s	O
a	O
h	O
s	O
e	O
t	O
a	O
m	O
l	O
i	O
l	O
c	O
d	O
o	O
c	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
i	O
s	O
u	O
o	O
i	O
c	O
o	O
r	O
e	O
f	O
s	O
i	O
s	O
u	O
o	O
r	O
e	O
g	O
n	O
a	O
d	O
s	O
i	O
e	O
r	O
o	O
v	O
n	O
r	O
a	O
c	O
a	O
s	O
i	O
r	O
o	O
t	O
a	O
d	O
e	O
r	O
p	O
a	O
s	O
i	O
r	O
e	O
t	O
a	O
w	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
s	O
e	O
i	O
l	O
f	O
g	O
n	O
o	O
l	O
s	O
i	O
s	O
s	O
a	O
r	O
g	O
n	O
h	O
s	O
i	O
f	O
s	O
t	O
a	O
e	O
s	O
e	O
t	O
a	O
m	O
s	O
e	O
v	O
a	O
e	O
l	O
s	O
t	O
a	O
e	O
i	O
s	O
l	O
a	O
m	O
n	O
a	O
s	O
t	O
a	O
e	O
i	O
s	O
e	O
v	O
i	O
l	O
i	O
l	O
c	O
t	O
o	O
h	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
figure	O
map	B
estimate	I
produced	O
by	O
the	O
crosscat	B
system	O
when	O
applied	O
to	O
a	O
binary	O
data	O
matrix	O
of	O
animals	O
by	O
features	B
see	O
text	O
for	O
details	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
vikash	O
mansingkha	O
cluster	O
depending	O
on	O
what	O
set	O
of	O
features	B
are	O
considered	O
uncertainty	B
about	O
the	O
partitions	O
can	O
be	O
handled	O
by	O
sampling	O
it	O
is	O
interesting	O
to	O
compare	O
this	O
model	O
to	O
a	O
standard	O
infinite	O
mixture	B
model	I
while	O
the	O
standard	B
model	I
can	O
represent	O
any	O
density	O
on	O
fixed-sized	O
vectors	O
as	O
n	O
it	O
cannot	O
cope	O
with	O
d	O
since	O
it	O
has	O
no	O
way	O
to	O
handle	O
irrelevant	O
noisy	O
or	O
redundant	O
features	B
by	O
contrast	O
the	O
crosscatmulti-clust	O
system	O
is	O
robust	B
to	O
irrelevant	O
features	B
it	O
can	O
just	O
partition	O
them	O
off	O
and	O
cluster	O
the	O
rows	O
only	O
using	O
the	O
relevant	O
features	B
note	O
however	O
that	O
it	O
does	O
not	O
need	O
a	O
separate	O
background	O
model	O
since	O
everything	O
is	O
modelled	O
using	O
the	O
same	O
mechanism	O
this	O
is	O
useful	O
since	O
one	O
s	O
person	O
s	O
noise	O
is	O
another	O
person	O
s	O
signal	O
this	O
symmetry	O
may	O
explain	O
why	O
multi-clust	B
outperformed	O
the	O
sparse	B
mixture	B
model	I
approach	O
of	O
et	O
al	O
in	O
the	O
experiments	O
reported	O
in	O
et	O
al	O
graphical	B
model	I
structure	B
learning	B
introduction	O
we	O
have	O
seen	O
how	O
graphical	B
models	I
can	O
be	O
used	O
to	O
express	O
conditional	B
independence	I
assumptions	O
between	O
variables	O
in	O
this	O
chapter	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
structure	O
of	O
the	O
graphical	B
model	I
itself	O
that	O
is	O
we	O
want	O
to	O
compute	O
pgd	O
whereg	O
is	O
the	O
graph	B
structure	O
represented	O
as	O
an	O
v	O
v	O
adjacency	B
matrix	I
as	O
we	O
discussed	O
in	O
section	O
there	O
are	O
two	O
main	O
applications	O
of	O
structure	B
learning	B
knowledge	B
discovery	I
and	O
density	B
estimation	I
the	O
former	O
just	O
requires	O
a	O
graph	B
topology	O
whereas	O
the	O
latter	O
requires	O
a	O
fully	O
specified	O
model	O
the	O
main	O
obstacle	O
in	O
structure	B
learning	B
is	O
that	O
the	O
number	O
of	O
possible	O
graphs	O
is	O
exponential	O
in	O
the	O
number	O
of	O
nodes	B
a	O
simple	O
upper	O
bound	O
is	O
thus	O
the	O
full	B
posterior	O
pgd	O
is	O
prohibitively	O
large	O
even	O
if	O
we	O
could	O
afford	O
to	O
compute	O
it	O
we	O
could	O
not	O
even	O
store	O
it	O
so	O
we	O
will	O
seek	O
appropriate	O
summaries	O
of	O
the	O
posterior	O
these	O
summary	O
statistics	O
depend	O
on	O
our	O
task	O
is	O
knowledge	B
discovery	I
we	O
may	O
want	O
to	O
compute	O
posterior	O
edge	O
marginals	O
pgst	O
we	O
can	O
then	O
plot	O
the	O
corresponding	O
graph	B
where	O
the	O
thickness	O
of	O
each	O
edge	O
represents	O
our	O
confidence	O
in	O
its	O
presence	O
by	O
setting	O
a	O
threshold	O
we	O
can	O
generate	O
a	O
sparse	B
graph	B
which	O
can	O
be	O
useful	O
for	O
visualization	O
purposes	O
figure	O
if	O
our	O
goal	O
is	O
density	B
estimation	I
we	O
may	O
want	O
to	O
compute	O
the	O
map	O
graph	B
g	O
argmaxg	O
pgd	O
if	O
our	O
goal	O
in	O
most	O
cases	O
finding	O
the	O
globally	O
optimal	O
graph	B
will	O
take	O
exponential	O
time	O
so	O
we	O
will	O
use	O
discrete	B
optimization	B
methods	O
such	O
as	O
heuristic	O
search	O
however	O
in	O
the	O
case	O
of	O
trees	O
we	O
can	O
find	O
the	O
globally	O
optimal	O
graph	B
structure	O
quite	O
efficiently	O
using	O
exact	O
methods	O
as	O
we	O
discuss	O
in	O
section	O
if	O
density	B
estimation	I
is	O
our	O
only	O
goal	O
it	O
is	O
worth	O
considering	O
whether	O
it	O
would	O
be	O
more	O
appropriate	O
to	O
learn	O
a	O
latent	B
variable	O
model	O
which	O
can	O
capture	O
correlation	O
between	O
the	O
visible	B
variables	I
via	O
a	O
set	O
of	O
latent	B
common	O
causes	O
chapters	O
and	O
such	O
models	O
are	O
often	O
easier	O
to	O
learn	O
and	O
perhaps	O
more	O
importantly	O
they	O
can	O
be	O
applied	O
prediction	O
purposes	O
much	O
more	O
efficiently	O
since	O
they	O
do	O
not	O
require	O
performing	O
inference	B
in	O
a	O
learned	O
graph	B
with	O
potentially	O
high	O
treewidth	B
the	O
downside	O
with	O
such	O
models	O
is	O
that	O
the	O
latent	B
factors	B
are	O
often	O
unidentifiable	O
and	O
hence	O
hard	O
to	O
interpret	O
of	O
course	O
we	O
can	O
combine	O
graphical	B
model	I
structure	B
learning	B
and	O
latent	B
variable	O
learning	B
as	O
we	O
will	O
show	O
later	O
in	O
this	O
chapter	O
in	O
some	O
cases	O
we	O
don	O
t	O
just	O
want	O
to	O
model	O
the	O
observed	O
correlation	O
between	O
variables	O
instead	O
we	O
want	O
to	O
model	O
the	O
causal	O
structure	O
behind	O
the	O
data	O
so	O
we	O
can	O
predict	O
the	O
effects	O
of	O
manipulating	O
variables	O
this	O
is	O
a	O
much	O
more	O
challenging	O
task	O
which	O
we	O
briefly	O
discuss	O
in	O
chapter	O
graphical	B
model	I
structure	B
learning	B
bible	O
case	O
course	O
evidence	B
children	B
mission	O
launch	O
christian	O
fact	O
israel	O
government	O
earth	O
gun	O
nasa	O
lunar	O
god	O
human	O
jews	O
war	O
president	O
law	O
orbit	O
shuttle	O
moon	O
jesus	O
computer	O
religion	O
world	O
rights	O
solar	O
space	O
science	O
university	O
state	B
figure	O
part	O
of	O
a	O
relevance	B
network	I
constructed	O
from	O
the	O
data	O
shown	O
in	O
figure	O
we	O
show	O
edges	B
whose	O
mutual	B
information	B
is	O
greater	O
than	O
or	O
equal	O
to	O
of	O
the	O
maximum	O
pairwise	O
mi	B
for	O
clarity	O
the	O
graph	B
has	O
been	O
cropped	O
so	O
we	O
only	O
show	O
a	O
subset	O
of	O
the	O
nodes	B
and	O
edges	B
figure	O
generated	O
by	O
relevancenetworknewsgroupdemo	O
section	O
structure	B
learning	B
for	O
knowledge	B
discovery	I
since	O
computing	O
the	O
map	O
graph	B
or	O
the	O
exact	O
posterior	O
edge	O
marginals	O
is	O
in	O
general	O
computationally	O
intractable	O
in	O
this	O
section	O
we	O
discuss	O
some	O
quick	O
and	O
dirty	O
methods	O
for	O
learning	B
graph	B
structures	O
which	O
can	O
be	O
used	O
to	O
visualize	O
one	O
s	O
data	O
the	O
resulting	O
models	O
do	O
not	O
constitute	O
consistent	B
joint	O
probability	O
distributions	O
so	O
they	O
cannot	O
be	O
used	O
for	O
prediction	O
and	O
they	O
cannot	O
even	O
be	O
formally	O
evaluated	O
in	O
terms	O
of	O
goodness	O
of	O
fit	O
nevertheless	O
these	O
methods	O
are	O
a	O
useful	O
ad	O
hoc	O
tool	O
to	O
have	O
in	O
one	O
s	O
data	O
visualization	O
toolbox	O
in	O
view	O
of	O
their	O
speed	O
and	O
simplicity	O
relevance	O
networks	O
a	O
relevance	B
network	I
is	O
a	O
way	O
of	O
visualizing	B
the	O
pairwise	O
mutual	B
information	B
between	O
multiple	O
random	O
variables	O
we	O
simply	O
choose	O
a	O
threshold	O
and	O
draw	O
an	O
edge	O
from	O
node	O
i	O
to	O
node	O
j	O
if	O
i	O
xj	O
is	O
above	O
this	O
threshold	O
in	O
the	O
gaussian	B
case	O
i	O
xj	O
ij	O
where	O
ij	O
is	O
the	O
correlation	B
coefficient	I
exercise	O
so	O
we	O
are	O
essentially	O
visualizing	B
this	O
is	O
known	O
as	O
the	O
covariance	B
graph	B
this	O
method	O
is	O
quite	O
popular	O
in	O
systems	B
biology	I
et	O
al	O
where	O
it	O
is	O
used	O
to	O
visualize	O
the	O
interaction	O
between	O
genes	O
the	O
trouble	O
with	O
biological	O
examples	O
is	O
that	O
they	O
are	O
hard	O
for	O
non-biologists	O
to	O
understand	O
so	O
let	O
us	O
instead	O
illustrate	O
the	O
idea	O
using	O
natural	O
language	O
text	O
figure	O
gives	O
an	O
example	O
where	O
we	O
visualize	O
the	O
mi	B
between	O
words	O
in	O
the	O
newsgroup	O
dataset	O
from	O
figure	O
the	O
results	O
seem	O
intuitively	O
reasonable	O
however	O
relevance	O
networks	O
suffer	O
from	O
a	O
major	O
problem	O
the	O
graphs	O
are	O
usually	O
very	O
dense	O
since	O
most	O
variables	O
are	O
dependent	O
on	O
most	O
other	O
variables	O
even	O
after	O
thresholding	O
the	O
mis	O
for	O
example	O
suppose	O
directly	O
influences	O
which	O
directly	O
influences	O
these	O
form	O
components	O
of	O
a	O
signalling	O
cascade	B
then	O
has	O
non-zero	O
mi	B
with	O
vice	O
versa	O
so	O
there	O
will	O
be	O
a	O
edge	O
in	O
the	O
relevance	B
network	I
indeed	O
most	O
pairs	O
will	O
be	O
structure	B
learning	B
for	O
knowledge	B
discovery	I
children	B
case	O
course	O
fact	O
question	O
earth	O
bible	O
christian	O
food	O
baseball	O
mission	O
god	O
disk	O
mac	O
car	O
aids	O
doctor	O
fans	O
nasa	O
jesus	O
pc	O
dos	O
drive	O
bmw	O
israel	O
government	O
health	O
games	O
hockey	O
hit	O
launch	O
memory	O
scsi	O
jews	O
engine	O
dealer	O
state	B
war	O
computer	O
president	O
medicine	O
season	O
puck	O
nhl	O
shuttle	O
religion	O
data	O
card	O
honda	O
power	O
oil	O
world	O
insurance	O
science	O
studies	O
team	O
software	O
solar	O
graphics	O
driver	O
gun	O
research	O
university	O
water	O
human	O
cancer	O
win	O
league	O
lunar	O
system	O
display	O
video	O
windows	O
law	O
disease	O
won	O
players	O
moon	O
server	O
files	O
rights	O
problem	O
evidence	B
space	O
program	O
format	O
patients	O
msg	O
help	O
mars	B
orbit	O
technology	O
ftp	O
image	O
number	O
vitamin	O
email	O
satellite	O
version	O
phone	B
figure	O
a	O
dependency	B
network	I
constructed	O
from	O
the	O
data	O
we	O
show	O
all	O
edges	B
with	O
regression	B
weight	O
above	O
in	O
the	O
markov	B
blankets	O
estimated	O
by	O
penalized	O
logistic	B
regression	B
undirected	B
edges	B
represent	O
cases	O
where	O
a	O
directed	B
edge	O
was	O
found	O
in	O
both	O
directions	O
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
connected	O
a	O
better	O
approach	O
is	O
to	O
use	O
graphical	B
models	I
which	O
represent	O
conditional	B
independence	I
in	O
the	O
above	O
example	O
is	O
conditionally	B
independent	I
of	O
given	O
rather	O
than	O
dependence	O
so	O
there	O
will	O
not	O
be	O
a	O
edge	O
consequently	O
graphical	B
models	I
are	O
usually	O
much	O
sparser	O
than	O
relevance	O
networks	O
and	O
hence	O
are	O
a	O
more	O
useful	O
way	O
of	O
visualizing	B
interactions	O
between	O
multiple	O
variables	O
dependency	B
networks	I
a	O
simple	O
and	O
efficient	O
way	O
to	O
learn	O
a	O
graphical	B
model	I
structure	O
is	O
to	O
independently	O
fit	O
d	O
sparse	B
full-conditional	O
distributions	O
pxtx	O
t	O
this	O
is	O
called	O
a	O
dependency	B
network	I
et	O
al	O
the	O
chosen	O
variables	O
constitute	O
the	O
inputs	O
to	O
the	O
node	O
i	O
e	O
its	O
markov	B
blanket	I
we	O
can	O
then	O
visualize	O
the	O
resulting	O
sparse	B
graph	B
the	O
advantage	O
over	O
relevance	O
networks	O
is	O
that	O
redundant	O
variables	O
will	O
not	O
be	O
selected	O
as	O
inputs	O
we	O
can	O
use	O
any	O
kind	O
of	O
sparse	B
regression	B
or	O
classification	O
method	O
to	O
fit	O
each	O
cpd	B
and	O
buhlmann	O
use	O
al	O
uses	O
classification	O
regression	B
trees	O
regularized	O
linear	B
regression	B
et	O
al	O
use	O
logistic	B
regression	B
depnetfit	O
for	O
some	O
code	O
uses	O
bayesian	B
variable	O
selection	O
etc	O
chapter	O
graphical	B
model	I
structure	B
learning	B
and	O
buhlmann	O
discuss	O
theoretical	O
conditions	O
under	O
which	O
linear	B
regression	B
can	O
recover	O
the	O
true	O
graph	B
structure	O
assuming	O
the	O
data	O
was	O
generated	O
from	O
a	O
sparse	B
gaussian	B
graphical	B
model	I
figure	O
shows	O
a	O
dependency	B
network	I
that	O
was	O
learned	O
from	O
the	O
data	O
using	O
regularized	O
logistic	B
regression	B
where	O
the	O
penalty	O
parameter	B
was	O
chosen	O
by	O
bic	B
many	O
of	O
the	O
words	O
present	O
in	O
these	O
estimated	O
markov	B
blankets	O
represent	O
fairly	O
natural	O
associations	O
baseballfans	O
biblegod	O
bmwcar	O
cancerpatients	O
etc	O
however	O
some	O
of	O
the	O
estimated	O
statistical	O
dependencies	O
seem	O
less	O
intuitive	O
such	O
as	O
baseballwindows	O
and	O
bmwchristian	O
we	O
can	O
gain	O
more	O
insight	O
if	O
we	O
look	O
not	O
only	O
at	O
the	O
sparsity	B
pattern	B
but	O
also	O
the	O
values	O
of	O
the	O
regression	B
weights	O
for	O
example	O
here	O
are	O
the	O
incoming	O
weights	O
for	O
the	O
first	O
words	O
aids	O
children	B
disease	O
fact	O
health	O
president	O
research	O
baseball	O
christian	O
drive	O
games	O
god	O
government	O
hit	O
memory	O
players	O
season	O
software	O
windows	O
bible	O
car	O
card	O
christian	O
fact	O
god	O
jesus	O
orbit	O
program	O
religion	O
version	O
bmw	O
car	O
christian	O
engine	O
god	O
government	O
help	O
windows	O
cancer	O
disease	O
medicine	O
patients	O
research	O
studies	O
words	O
in	O
italic	O
red	O
have	O
negative	O
weights	O
which	O
represents	O
a	O
dissociative	O
relationship	O
for	O
example	O
the	O
model	O
reflects	O
that	O
baseballwindows	O
is	O
an	O
unlikely	O
combination	O
it	O
turns	O
out	O
that	O
most	O
of	O
the	O
weights	O
are	O
negative	O
negative	O
positive	O
zero	O
in	O
this	O
model	O
in	O
addition	O
to	O
visualizing	B
the	O
data	O
a	O
dependency	B
network	I
can	O
be	O
used	O
for	O
inference	B
however	O
the	O
only	O
algorithm	O
we	O
can	O
use	O
is	O
gibbs	B
sampling	I
where	O
we	O
repeatedly	O
sample	O
the	O
nodes	B
with	O
missing	B
values	O
from	O
their	O
full	B
conditionals	O
unfortunately	O
a	O
product	O
of	O
full	B
conditionals	O
does	O
not	O
in	O
general	O
constitute	O
a	O
representation	O
of	O
any	O
valid	O
joint	B
distribution	I
et	O
al	O
so	O
the	O
output	O
of	O
the	O
gibbs	B
sampler	I
may	O
not	O
be	O
meaningful	O
nevertheless	O
the	O
method	O
can	O
sometimes	O
give	O
reasonable	O
results	O
if	O
there	O
is	O
not	O
much	O
missing	B
data	I
and	O
it	O
is	O
a	O
useful	O
method	O
for	O
data	O
imputation	B
and	O
raghunathan	O
in	O
addition	O
the	O
method	O
can	O
be	O
used	O
as	O
an	O
initialization	O
technique	O
for	O
more	O
complex	O
structure	B
learning	B
methods	O
that	O
we	O
discuss	O
below	O
learning	B
tree	B
structures	O
for	O
the	O
rest	O
of	O
this	O
chapter	O
we	O
focus	O
on	O
learning	B
fully	O
specified	O
joint	O
probability	O
models	O
which	O
can	O
be	O
used	O
for	O
density	B
estimation	I
prediction	O
and	O
knowledge	B
discovery	I
since	O
the	O
problem	O
of	O
structure	B
learning	B
for	O
general	O
graphs	O
is	O
np-hard	B
we	O
start	O
by	O
considering	O
the	O
special	O
case	O
of	O
trees	O
trees	O
are	O
special	O
because	O
we	O
can	O
learn	O
their	O
structure	O
efficiently	O
as	O
we	O
disuscs	O
below	O
and	O
because	O
once	O
we	O
have	O
learned	O
the	O
tree	B
we	O
can	O
use	O
them	O
for	O
efficient	O
exact	O
inference	B
as	O
discussed	O
in	O
section	O
learning	B
tree	B
structures	O
figure	O
an	O
undirected	B
tree	B
and	O
two	O
equivalent	O
directed	B
trees	O
directed	B
or	O
undirected	B
tree	B
before	O
continuing	O
we	O
need	O
to	O
discuss	O
the	O
issue	O
of	O
whether	O
we	O
should	O
use	O
directed	B
or	O
undirected	B
trees	O
a	O
directed	B
tree	B
with	O
a	O
single	O
root	B
node	O
r	O
defines	O
a	O
joint	B
distribution	I
as	O
follows	O
pxt	O
pxtxpat	O
where	O
we	O
define	O
par	O
for	O
example	O
in	O
figure	O
we	O
have	O
we	O
see	O
that	O
the	O
choice	O
of	O
root	B
does	O
not	O
matter	O
both	O
of	O
these	O
models	O
are	O
equivalent	O
to	O
make	O
the	O
model	O
more	O
symmetric	B
it	O
is	O
preferable	O
to	O
use	O
an	O
undirected	B
tree	B
this	O
can	O
be	O
represented	O
as	O
follows	O
pxt	O
pxt	O
e	O
t	O
v	O
t	O
v	O
pxs	O
xt	O
pxspxt	O
where	O
pxs	O
xt	O
is	O
an	O
edge	O
marginal	O
and	O
pxt	O
is	O
a	O
node	O
marginal	O
for	O
example	O
in	O
figure	O
we	O
have	O
to	O
see	O
the	O
equivalence	O
with	O
the	O
directed	B
representation	O
let	O
us	O
cancel	O
terms	O
to	O
get	O
where	O
pxtxs	O
pxs	O
xtpxs	O
thus	O
a	O
tree	B
can	O
be	O
represented	O
as	O
either	O
an	O
undirected	B
or	O
directed	B
graph	B
the	O
number	O
of	O
parameters	O
is	O
the	O
same	O
and	O
hence	O
the	O
complexity	O
of	O
learning	B
is	O
the	O
same	O
and	O
of	O
course	O
inference	B
is	O
the	O
same	O
in	O
both	O
representations	O
too	O
the	O
undirected	B
representation	O
which	O
is	O
symmetric	B
is	O
useful	O
for	O
structure	B
learning	B
but	O
the	O
directed	B
representation	O
is	O
more	O
convenient	O
for	O
parameter	B
learning	B
chapter	O
graphical	B
model	I
structure	B
learning	B
chow-liu	B
algorithm	I
for	O
finding	O
the	O
ml	O
tree	B
structure	O
using	O
equation	O
we	O
can	O
write	O
the	O
log-likelihood	O
for	O
a	O
tree	B
as	O
follows	O
log	O
pd	O
t	O
ntk	O
log	O
pxt	O
k	O
k	O
t	O
st	O
jk	O
nstjk	O
log	O
pxs	O
j	O
xt	O
k	O
pxs	O
j	O
k	O
where	O
nstjk	O
is	O
the	O
number	O
of	O
times	O
node	O
s	O
is	O
in	O
state	B
j	O
and	O
node	O
t	O
is	O
in	O
state	B
k	O
and	O
ntk	O
is	O
the	O
number	O
of	O
times	O
node	O
t	O
is	O
in	O
state	B
k	O
we	O
can	O
rewrite	O
these	O
counts	O
in	O
terms	O
of	O
the	O
empirical	B
distribution	I
nstjk	O
n	O
pempxs	O
j	O
xt	O
k	O
and	O
ntk	O
n	O
pempxt	O
k	O
setting	O
to	O
the	O
mles	O
this	O
becomes	O
log	O
pd	O
t	O
pempxt	O
k	O
log	O
pempxt	O
k	O
n	O
t	O
v	O
ixs	O
xt	O
st	O
k	O
et	O
where	O
ixs	O
xt	O
st	O
is	O
the	O
mutual	B
information	B
between	O
xs	O
and	O
xt	O
given	O
the	O
empirical	B
distribution	I
ixs	O
xt	O
st	O
pempxs	O
j	O
xt	O
k	O
log	O
pempxs	O
j	O
xt	O
k	O
pempxs	O
jpempxt	O
k	O
j	O
k	O
since	O
the	O
first	O
term	O
in	O
equation	O
is	O
independent	O
of	O
the	O
topology	O
t	O
we	O
can	O
ignore	O
it	O
when	O
learning	B
structure	O
thus	O
the	O
tree	B
topology	O
that	O
maximizes	O
the	O
likelihood	B
can	O
be	O
found	O
by	O
computing	O
the	O
maximum	B
weight	I
spanning	I
tree	B
where	O
the	O
edge	O
weights	O
are	O
the	O
pairwise	O
mutual	O
informations	O
iys	O
yt	O
st	O
this	O
is	O
called	O
the	O
chow-liu	B
algorithm	I
and	O
liu	O
there	O
are	O
several	O
algorithms	O
for	O
finding	O
a	O
max	O
spanning	O
tree	B
the	O
two	O
best	O
known	O
are	O
prim	O
s	O
algorithm	O
and	O
kruskal	O
s	O
algorithm	O
both	O
can	O
be	O
implemented	O
to	O
run	O
in	O
oe	O
log	O
v	O
time	O
where	O
e	O
v	O
is	O
the	O
number	O
of	O
edges	B
and	O
v	O
is	O
the	O
number	O
of	O
nodes	B
see	O
e	O
g	O
and	O
wayne	O
for	O
details	O
thus	O
the	O
overall	O
running	O
time	O
is	O
on	O
v	O
v	O
log	O
v	O
where	O
the	O
first	O
term	O
is	O
the	O
cost	O
of	O
computing	O
the	O
sufficient	B
statistics	I
figure	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
applied	O
to	O
the	O
binary	O
newsgroups	O
data	O
shown	O
in	O
figure	O
the	O
tree	B
has	O
been	O
arbitrarily	O
rooted	O
at	O
the	O
node	O
representing	O
email	O
the	O
connections	O
that	O
are	O
learned	O
seem	O
intuitively	O
reasonable	O
finding	O
the	O
map	O
forest	B
since	O
all	O
trees	O
have	O
the	O
same	O
number	O
of	O
parameters	O
we	O
can	O
safely	O
used	O
the	O
maximum	O
likelihood	B
score	O
as	O
a	O
model	B
selection	I
criterion	O
without	O
worrying	O
about	O
overfitting	O
however	O
sometimes	O
we	O
may	O
want	O
to	O
fit	O
a	O
forest	B
rather	O
than	O
a	O
single	O
tree	B
since	O
inference	B
in	O
a	O
forest	B
is	O
much	O
faster	O
than	O
in	O
a	O
tree	B
can	O
run	O
belief	B
propagation	I
in	O
each	O
tree	B
in	O
the	O
forest	B
in	O
parallel	O
the	O
mle	B
criterion	O
will	O
never	O
choose	O
to	O
omit	O
an	O
edge	O
however	O
if	O
we	O
use	O
the	O
marginal	B
likelihood	B
or	O
a	O
penalized	O
likelihood	B
as	O
bic	B
the	O
optimal	O
solution	O
may	O
be	O
a	O
forest	B
below	O
we	O
give	O
the	O
details	O
for	O
the	O
marginal	B
likelihood	B
case	O
learning	B
tree	B
structures	O
email	O
ftp	O
phone	B
files	O
number	O
disk	O
format	O
windows	O
drive	O
memory	O
system	O
image	O
card	O
dos	O
driver	O
pc	O
program	O
version	O
car	O
scsi	O
data	O
problem	O
display	O
graphics	O
video	O
software	O
space	O
win	O
team	O
won	O
bmw	O
dealer	O
engine	O
honda	O
mac	O
help	O
server	O
launch	O
moon	O
nasa	O
orbit	O
shuttle	O
technology	O
fans	O
games	O
hockey	O
league	O
players	O
puck	O
season	O
oil	O
lunar	O
mars	B
earth	O
satellite	O
solar	O
mission	O
nhl	O
baseball	O
god	O
hit	O
bible	O
christian	O
jesus	O
religion	O
jews	O
government	O
israel	O
children	B
power	O
president	O
rights	O
state	B
war	O
health	O
human	O
law	O
university	O
world	O
aids	O
food	O
insurance	O
medicine	O
fact	O
gun	O
research	O
science	O
msg	O
water	O
patients	O
studies	O
case	O
course	O
evidence	B
question	O
computer	O
cancer	O
disease	O
doctor	O
vitamin	O
figure	O
the	O
mle	B
tree	B
on	O
the	O
data	O
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
topologically	O
equivalent	O
tree	B
can	O
be	O
produced	O
using	O
chowliutreedemo	O
in	O
section	O
we	O
explain	O
how	O
to	O
compute	O
the	O
marginal	B
likelihood	B
of	O
any	O
dag	B
using	O
a	O
dirichlet	B
prior	O
for	O
the	O
cpts	B
the	O
resulting	O
expression	O
can	O
be	O
written	O
as	O
follows	O
log	O
pdt	O
pxitxipat	O
tp	O
td	O
t	O
scorentpat	O
log	O
t	O
v	O
t	O
where	O
ntpat	O
are	O
the	O
counts	O
statistics	O
for	O
node	O
t	O
and	O
its	O
parents	B
and	O
score	O
is	O
defined	O
in	O
equation	O
now	O
suppose	O
we	O
only	O
allow	O
dags	O
with	O
at	O
most	O
one	O
parent	O
following	O
et	O
al	O
let	O
us	O
associate	O
a	O
weight	O
with	O
each	O
s	O
t	O
edge	O
wst	O
scorets	O
where	O
is	O
the	O
score	O
when	O
t	O
has	O
no	O
parents	B
note	O
that	O
the	O
weights	O
might	O
be	O
negative	O
the	O
mle	B
case	O
where	O
edge	O
weights	O
are	O
aways	O
non-negative	O
because	O
they	O
correspond	O
to	O
mutual	B
information	B
then	O
we	O
can	O
rewrite	O
the	O
objective	O
as	O
follows	O
log	O
pdt	O
scoretpat	O
wpatt	O
t	O
t	O
t	O
the	O
last	O
term	O
is	O
the	O
same	O
for	O
all	O
trees	O
t	O
so	O
we	O
can	O
ignore	O
it	O
thus	O
finding	O
the	O
most	O
probable	O
tree	B
amounts	O
to	O
finding	O
a	O
maximal	B
branching	I
in	O
the	O
corresponding	O
weighted	O
directed	B
graph	B
this	O
can	O
be	O
found	O
using	O
the	O
algorithm	O
in	O
et	O
al	O
chapter	O
graphical	B
model	I
structure	B
learning	B
if	O
the	O
scoring	O
function	O
is	O
prior	O
and	O
likelihood	B
equivalent	I
terms	O
are	O
explained	O
in	O
sec	O
tion	O
we	O
have	O
scorest	O
scorets	O
and	O
hence	O
the	O
weight	O
matrix	O
is	O
symmetric	B
in	O
this	O
case	O
the	O
maximal	B
branching	I
is	O
the	O
same	O
as	O
the	O
maximal	O
weight	O
forest	B
we	O
can	O
apply	O
a	O
slightly	O
modified	O
version	O
of	O
the	O
mst	O
algorithm	O
to	O
find	O
this	O
et	O
al	O
to	O
see	O
this	O
let	O
g	O
e	O
be	O
a	O
graph	B
with	O
both	O
positive	O
and	O
negative	O
edge	O
weights	O
now	O
let	O
be	O
a	O
graph	B
obtained	O
by	O
omitting	O
all	O
the	O
negative	O
edges	B
from	O
g	O
this	O
cannot	O
reduce	O
the	O
total	O
weight	O
so	O
we	O
can	O
find	O
the	O
maximum	O
weight	O
forest	B
of	O
g	O
by	O
finding	O
the	O
mst	O
for	O
each	O
connected	O
component	O
of	O
we	O
can	O
do	O
this	O
by	O
running	O
kruskal	O
s	O
algorithm	O
directly	O
on	O
there	O
is	O
no	O
need	O
to	O
find	O
the	O
connected	O
components	O
explicitly	O
mixtures	O
of	O
trees	O
a	O
single	O
tree	B
is	O
rather	O
limited	O
in	O
its	O
expressive	O
power	O
later	O
in	O
this	O
chapter	O
we	O
discuss	O
ways	O
to	O
learn	O
more	O
general	O
graphs	O
however	O
the	O
resulting	O
graphs	O
can	O
be	O
expensive	O
to	O
do	O
inference	B
in	O
an	O
interesting	O
alternative	O
is	O
to	O
learn	O
a	O
mixture	B
of	I
trees	I
and	O
jordan	O
where	O
each	O
mixture	B
component	O
may	O
have	O
a	O
different	O
tree	B
topology	O
this	O
is	O
like	O
an	O
unsupervised	O
version	O
of	O
the	O
tan	B
classifier	O
discussed	O
in	O
section	O
we	O
can	O
fit	O
a	O
mixture	B
of	I
trees	I
by	O
using	O
em	B
in	O
the	O
e	B
step	I
we	O
compute	O
the	O
responsibilities	O
of	O
each	O
cluster	O
for	O
each	O
data	O
point	O
and	O
in	O
the	O
m	B
step	I
we	O
use	O
a	O
weighted	O
version	O
of	O
the	O
chow-liu	B
algorithm	I
see	O
and	O
jordan	O
for	O
details	O
in	O
fact	O
it	O
is	O
possible	O
to	O
create	O
an	O
infinite	O
mixture	B
of	I
trees	I
by	O
integrating	O
out	O
over	O
all	O
possible	O
trees	O
remarkably	O
this	O
can	O
be	O
done	O
in	O
v	O
time	O
using	O
the	O
matrix	B
tree	B
theorem	I
this	O
allows	O
us	O
to	O
perform	O
exact	O
bayesian	B
inference	B
of	O
posterior	O
edge	O
marginals	O
etc	O
however	O
it	O
is	O
not	O
tractable	O
to	O
use	O
this	O
infinite	O
mixture	B
for	O
inference	B
of	O
hidden	B
nodes	B
see	O
and	O
jaakkola	O
for	O
details	O
learning	B
dag	B
structures	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
compute	O
of	O
pgd	O
where	O
g	O
is	O
constrained	O
to	O
be	O
a	O
dag	B
this	O
is	O
often	O
called	O
bayesian	B
network	I
structure	B
learning	B
in	O
this	O
section	O
we	O
assume	O
there	O
is	O
no	O
missing	B
data	I
and	O
that	O
there	O
are	O
no	O
hidden	B
variables	I
this	O
is	O
called	O
the	O
complete	B
data	I
assumption	I
for	O
simplicity	O
we	O
will	O
focus	O
on	O
the	O
case	O
where	O
all	O
the	O
variables	O
are	O
categorical	B
and	O
all	O
the	O
cpds	O
are	O
tables	O
although	O
the	O
results	O
generalize	B
to	O
real-valued	O
data	O
and	O
other	O
kinds	O
of	O
cpds	O
such	O
as	O
linear-gaussian	O
cpds	O
our	O
presentation	O
is	O
based	O
in	O
part	O
on	O
et	O
al	O
although	O
we	O
will	O
follow	O
the	O
notation	O
of	O
section	O
in	O
particular	O
let	O
xit	O
kt	O
be	O
the	O
value	O
of	O
node	O
t	O
in	O
case	O
i	O
where	O
kt	O
is	O
the	O
number	O
of	O
states	O
for	O
node	O
t	O
let	O
tck	O
pxt	O
kxpat	O
c	O
for	O
k	O
t	O
and	O
c	O
t	O
where	O
ct	O
is	O
the	O
number	O
of	O
parent	O
combinations	O
conditioning	B
cases	O
for	O
notational	O
simplicity	O
we	O
will	O
often	O
assume	O
kt	O
k	O
so	O
all	O
nodes	B
have	O
the	O
same	O
number	O
of	O
states	O
we	O
will	O
also	O
let	O
dt	O
dimpat	O
be	O
the	O
degree	B
or	O
fan-in	B
of	O
node	O
t	O
so	O
that	O
ct	O
k	O
dt	O
markov	B
equivalence	I
in	O
this	O
section	O
we	O
discuss	O
some	O
fundamental	O
limits	O
to	O
our	O
ability	O
to	O
learn	O
dag	B
structures	O
from	O
data	O
learning	B
dag	B
structures	O
figure	O
three	O
dags	O
and	O
are	O
markov	B
equivalent	I
is	O
not	O
consider	O
the	O
following	O
dgms	O
x	O
y	O
z	O
x	O
y	O
z	O
and	O
x	O
y	O
z	O
these	O
all	O
represent	O
the	O
same	O
set	O
of	O
ci	B
statements	O
namely	O
x	O
zy	O
x	O
z	O
we	O
say	O
these	O
graphs	O
are	O
markov	B
equivalent	I
since	O
they	O
encode	O
the	O
same	O
set	O
of	O
ci	B
assumptions	O
that	O
is	O
they	O
all	O
belong	O
to	O
the	O
same	O
markov	B
equivalence	B
class	I
however	O
the	O
v-structure	B
x	O
y	O
z	O
encodes	O
x	O
z	O
and	O
x	O
zy	O
which	O
represents	O
the	O
opposite	O
set	O
of	O
ci	B
assumptions	O
one	O
can	O
prove	O
the	O
following	O
theorem	O
theorem	O
and	O
pearl	O
and	O
pearl	O
two	O
structures	O
are	O
markov	B
equivalent	I
iff	B
they	O
have	O
the	O
same	O
undirected	B
skeleton	O
and	O
the	O
same	O
set	O
of	O
v-structures	O
for	O
example	O
referring	O
to	O
figure	O
we	O
see	O
that	O
since	O
reversing	O
the	O
arc	O
creates	O
a	O
new	O
v-structure	B
however	O
since	O
reversing	O
the	O
arc	O
does	O
not	O
create	O
a	O
new	O
v-structure	B
we	O
can	O
represent	O
a	O
markov	B
equivalence	B
class	I
using	O
a	O
single	O
partially	B
directed	B
acyclic	I
graph	B
also	O
called	O
an	O
essential	B
graph	B
or	O
pattern	B
in	O
which	O
some	O
edges	B
are	O
directed	B
and	O
some	O
undirected	B
the	O
undirected	B
edges	B
represent	O
reversible	O
edges	B
any	O
combination	O
is	O
possible	O
so	O
long	O
as	O
no	O
new	O
v-structures	O
are	O
created	O
the	O
directed	B
edges	B
are	O
called	O
compelled	B
edges	B
since	O
changing	O
their	O
orientation	O
would	O
change	O
the	O
v-structures	O
and	O
hence	O
change	O
the	O
equivalence	B
class	I
for	O
example	O
the	O
pdag	B
x	O
y	O
z	O
represents	O
y	O
z	O
x	O
y	O
z	O
x	O
y	O
z	O
which	O
encodes	O
x	O
z	O
and	O
x	O
zy	O
see	O
figure	O
the	O
significance	O
of	O
the	O
above	O
theorem	O
is	O
that	O
when	O
we	O
learn	O
the	O
dag	B
structure	O
from	O
data	O
we	O
will	O
not	O
be	O
able	O
to	O
uniquely	O
identify	O
all	O
of	O
the	O
edge	O
directions	O
even	O
given	O
an	O
infinite	O
amount	O
of	O
data	O
we	O
say	O
that	O
we	O
can	O
learn	O
dag	B
structure	O
up	O
to	O
markov	B
equivalence	I
this	O
also	O
cautions	O
us	O
not	O
to	O
read	O
too	O
much	O
into	O
the	O
meaning	O
of	O
particular	O
edge	O
orientations	O
since	O
we	O
can	O
often	O
change	O
them	O
without	O
changing	O
the	O
model	O
in	O
any	O
observable	O
way	O
chapter	O
graphical	B
model	I
structure	B
learning	B
x	O
y	O
z	O
x	O
y	O
z	O
x	O
y	O
z	O
x	O
y	O
z	O
x	O
y	O
z	O
x	O
y	O
z	O
figure	O
pdag	B
representation	O
of	O
markov	B
equivalent	I
dags	O
exact	O
structural	O
inference	B
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
compute	O
the	O
exact	O
posterior	O
over	O
graphs	O
pgd	O
ignoring	O
for	O
now	O
the	O
issue	O
of	O
computational	O
tractability	O
deriving	O
the	O
likelihood	B
assuming	O
there	O
is	O
no	O
missing	B
data	I
and	O
that	O
all	O
cpds	O
are	O
tabular	O
the	O
likelihood	B
can	O
be	O
written	O
as	O
follows	O
pdg	O
catxitxipat	O
t	O
catxit	O
tcixipatc	O
ixitkxipatc	O
tck	O
ntck	O
tck	O
where	O
ntck	O
is	O
the	O
number	O
of	O
times	O
node	O
t	O
is	O
in	O
state	B
k	O
and	O
its	O
parents	B
are	O
in	O
state	B
c	O
these	O
counts	O
depend	O
on	O
the	O
graph	B
structure	O
g	O
but	O
we	O
drop	O
this	O
from	O
the	O
notation	O
deriving	O
the	O
marginal	B
likelihood	B
of	O
course	O
choosing	O
the	O
graph	B
with	O
the	O
maximum	O
likelihood	B
will	O
always	O
pick	O
a	O
fully	O
connected	O
graph	B
to	O
the	O
acyclicity	O
constraint	O
since	O
this	O
maximizes	O
the	O
number	O
of	O
parameters	O
to	O
avoid	O
such	O
overfitting	O
we	O
will	O
choose	O
the	O
graph	B
with	O
the	O
maximum	O
marginal	B
likelihood	B
pdg	O
the	O
magic	O
of	O
the	O
bayesian	B
occam	O
s	O
razor	O
will	O
then	O
penalize	O
overly	O
complex	O
graphs	O
to	O
compute	O
the	O
marginal	B
likelihood	B
we	O
need	O
to	O
specify	O
priors	O
on	O
the	O
parameters	O
we	O
will	O
make	O
two	O
standard	O
assumptions	O
first	O
we	O
assume	O
global	B
prior	I
parameter	B
independence	I
which	O
means	O
p	O
p	O
t	O
learning	B
dag	B
structures	O
second	O
we	O
assume	O
local	B
prior	I
parameter	B
independence	I
which	O
means	O
p	O
t	O
p	O
tc	O
for	O
each	O
t	O
must	O
be	O
a	O
dirichlet	B
and	O
heckerman	O
that	O
is	O
it	O
turns	O
out	O
that	O
these	O
assumtions	O
imply	O
that	O
the	O
prior	O
for	O
each	O
row	O
of	O
each	O
cpt	O
p	O
tc	O
dir	O
tc	O
tc	O
given	O
these	O
assumptions	O
and	O
using	O
the	O
results	O
of	O
section	O
we	O
can	O
write	O
down	O
the	O
marginal	B
likelihood	B
of	O
any	O
dag	B
as	O
follows	O
dir	O
tcd	O
tc	O
catxit	O
tc	O
ixipatc	O
bntc	O
tc	O
b	O
tc	O
g	O
tck	O
tc	O
g	O
ijk	O
pdg	O
scorentpat	O
where	O
ntc	O
node	O
t	O
and	O
its	O
parents	B
and	O
score	O
is	O
a	O
local	O
scoring	O
function	O
defined	O
by	O
k	O
tck	O
ntpat	O
is	O
the	O
vector	O
of	O
counts	O
statistics	O
for	O
k	O
ntck	O
tc	O
scorentpat	O
bntc	O
tc	O
b	O
tc	O
we	O
say	O
that	O
the	O
marginal	B
likelihood	B
decomposes	B
or	O
factorizes	O
according	O
to	O
the	O
graph	B
structure	O
setting	O
the	O
prior	O
how	O
should	O
we	O
set	O
the	O
hyper-parameters	B
tck	O
it	O
is	O
tempting	O
to	O
use	O
a	O
jeffreys	B
prior	I
of	O
the	O
form	O
tck	O
however	O
it	O
turns	O
out	O
that	O
this	O
violates	O
a	O
property	O
called	O
likelihood	B
equivalence	I
which	O
is	O
sometimes	O
considered	O
desirable	O
this	O
property	O
says	O
that	O
if	O
and	O
are	O
markov	B
equivalent	I
they	O
should	O
have	O
the	O
same	O
marginal	B
likelihood	B
since	O
they	O
are	O
essentially	O
equivalent	O
models	O
geiger	O
and	O
heckerman	O
proved	O
that	O
for	O
complete	B
graphs	O
the	O
only	O
prior	O
that	O
satisfies	O
likelihood	B
equivalence	I
and	O
parameter	B
independence	O
is	O
the	O
dirichlet	B
prior	O
where	O
the	O
pseudo	B
counts	I
have	O
the	O
form	O
tck	O
k	O
xpat	O
c	O
where	O
is	O
called	O
the	O
equivalent	B
sample	I
size	I
and	O
is	O
some	O
prior	O
joint	B
probability	I
distribution	I
this	O
is	O
called	O
the	O
bde	B
prior	O
which	O
stands	O
for	O
bayesian	B
dirichlet	B
likelihood	B
equivalent	I
chapter	O
graphical	B
model	I
structure	B
learning	B
to	O
derive	O
the	O
hyper-parameters	B
for	O
other	O
graph	B
structures	O
geiger	O
and	O
heckerman	O
invoked	O
an	O
additional	O
assumption	O
called	O
parameter	B
modularity	I
which	O
says	O
that	O
if	O
node	O
xt	O
has	O
the	O
same	O
parents	B
in	O
and	O
then	O
p	O
with	O
this	O
assumption	O
we	O
can	O
always	O
derive	O
t	O
for	O
a	O
node	O
t	O
in	O
any	O
other	O
graph	B
by	O
marginalizing	O
the	O
pseudo	B
counts	I
in	O
equation	O
typically	O
the	O
prior	O
distribution	O
is	O
assumed	O
to	O
be	O
uniform	O
over	O
all	O
possible	O
joint	O
configura	O
tions	O
in	O
this	O
case	O
we	O
have	O
tck	O
ktct	O
ktct	O
thus	O
if	O
we	O
sum	O
the	O
pseudo	B
counts	I
over	O
all	O
ct	O
kt	O
since	O
k	O
xpat	O
c	O
entries	O
in	O
the	O
cpt	O
we	O
get	O
a	O
total	O
equivalent	B
sample	I
size	I
of	O
this	O
is	O
called	O
the	O
bdeu	B
prior	O
where	O
the	O
u	O
stands	O
for	O
uniform	O
this	O
is	O
the	O
most	O
widely	O
used	O
prior	O
for	O
learning	B
bayes	O
net	O
structures	O
for	O
advice	O
on	O
setting	O
the	O
global	O
tuning	O
parameter	B
see	O
et	O
al	O
simple	O
worked	O
example	O
we	O
now	O
give	O
a	O
very	O
simple	O
worked	O
example	O
from	O
suppose	O
we	O
have	O
just	O
binary	O
nodes	B
and	O
the	O
following	O
data	O
cases	O
suppose	O
we	O
are	O
interested	O
in	O
two	O
possible	O
graphs	O
is	O
and	O
is	O
the	O
disconnected	O
graph	B
the	O
empirical	O
counts	O
for	O
node	O
in	O
are	O
and	O
for	O
node	O
are	O
the	O
bdeu	B
prior	O
for	O
is	O
and	O
for	O
the	O
prior	O
for	O
is	O
the	O
same	O
and	O
for	O
it	O
is	O
if	O
we	O
set	O
and	O
use	O
the	O
bdeu	B
prior	O
we	O
find	O
and	O
hence	O
the	O
posterior	O
probabilites	O
under	O
a	O
uniform	O
graph	B
prior	O
are	O
and	O
and	O
example	O
analysis	O
of	O
the	O
college	O
plans	O
dataset	O
we	O
now	O
consider	O
a	O
more	O
interesting	O
example	O
from	O
et	O
al	O
consider	O
the	O
data	O
set	O
collected	O
in	O
by	O
sewell	O
and	O
shah	O
which	O
measured	O
variables	O
that	O
might	O
influence	O
the	O
decision	B
of	O
high	O
school	O
students	O
about	O
whether	O
to	O
attend	O
college	O
specifically	O
the	O
variables	O
are	O
as	O
follows	O
learning	B
dag	B
structures	O
figure	O
the	O
two	O
most	O
probable	O
dags	O
learned	O
from	O
the	O
sewell-shah	O
data	O
source	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
david	O
heckerman	O
sex	O
male	O
or	O
female	O
ses	O
socio	O
economic	O
status	O
low	O
lower	O
middle	O
upper	O
middle	O
or	O
high	O
pe	O
parental	O
encouragment	O
low	O
or	O
high	O
cp	O
college	O
plans	O
yes	O
or	O
no	O
iq	O
intelligence	O
quotient	O
discretized	O
into	O
low	O
lower	O
middle	O
upper	O
middle	O
or	O
high	O
these	O
variables	O
were	O
measured	O
for	O
wisconsin	O
high	O
school	O
seniors	O
there	O
are	O
possible	O
joint	O
configurations	O
heckerman	O
et	O
al	O
computed	O
the	O
exact	O
posterior	O
over	O
all	O
possible	O
node	O
dags	O
except	O
for	O
ones	O
in	O
which	O
sex	O
andor	O
ses	O
have	O
parents	B
andor	O
cp	O
have	O
children	B
prior	O
probability	O
of	O
these	O
graphs	O
was	O
set	O
to	O
based	O
on	O
domain	O
knowledge	O
they	O
used	O
the	O
bdeu	B
score	O
with	O
although	O
they	O
said	O
that	O
the	O
results	O
were	O
robust	B
to	O
any	O
in	O
the	O
range	O
to	O
the	O
top	O
two	O
graphs	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
most	O
probable	O
one	O
has	O
approximately	O
all	O
of	O
the	O
probability	O
mass	O
so	O
the	O
posterior	O
is	O
extremely	O
peaked	O
it	O
is	O
tempting	O
to	O
interpret	O
this	O
graph	B
in	O
terms	O
of	O
causality	B
section	O
in	O
particular	O
it	O
seems	O
that	O
socio-economic	O
status	O
iq	O
and	O
parental	O
encouragment	O
all	O
causally	O
influence	O
the	O
decision	B
about	O
whether	O
to	O
go	O
to	O
college	O
which	O
makes	O
sense	O
also	O
sex	O
influences	O
college	O
plans	O
only	O
indirectly	O
through	O
parental	O
encouragement	O
which	O
also	O
makes	O
sense	O
however	O
the	O
direct	O
link	O
from	O
socio	O
economic	O
status	O
to	O
iq	O
seems	O
surprising	O
this	O
may	O
be	O
due	O
to	O
a	O
hidden	B
common	O
cause	O
in	O
section	O
we	O
will	O
re-examine	O
this	O
dataset	O
allowing	O
for	O
the	O
presence	O
of	O
hidden	B
variables	I
the	O
algorithm	O
suppose	O
we	O
know	O
a	O
total	B
ordering	I
of	O
the	O
nodes	B
then	O
we	O
can	O
compute	O
the	O
distribution	O
over	O
parents	B
for	O
each	O
node	O
independently	O
without	O
the	O
risk	B
of	O
introducing	O
any	O
directed	B
cycles	O
we	O
chapter	O
graphical	B
model	I
structure	B
learning	B
simply	O
enumerate	O
over	O
all	O
possible	O
subsets	O
of	O
ancestors	B
and	O
compute	O
their	O
marginal	O
if	O
we	O
just	O
return	O
the	O
best	O
set	O
of	O
parents	B
for	O
each	O
node	O
we	O
get	O
the	O
the	O
algorithm	O
and	O
herskovits	O
handling	O
non-tabular	O
cpds	O
if	O
all	O
cpds	O
are	O
linear	B
gaussian	B
we	O
can	O
replace	O
the	O
dirichlet-multinomial	O
model	O
with	O
the	O
normalgamma	O
model	O
and	O
thus	O
derive	O
a	O
different	O
exact	O
expression	O
for	O
the	O
marginal	B
likelihood	B
see	O
and	O
heckerman	O
for	O
the	O
details	O
in	O
fact	O
we	O
can	O
easily	O
combine	O
discrete	B
nodes	B
and	O
gaussian	B
nodes	B
as	O
long	O
as	O
the	O
discrete	B
nodes	B
always	O
have	O
discrete	B
parents	B
this	O
is	O
called	O
a	O
conditional	B
gaussian	B
dag	B
again	O
we	O
can	O
compute	O
the	O
marginal	B
likelihood	B
in	O
closed	O
form	O
see	O
and	O
dethlefsen	O
for	O
the	O
details	O
in	O
the	O
general	O
case	O
everything	O
except	O
gaussians	O
and	O
cpts	B
we	O
need	O
to	O
approximate	O
the	O
marginal	B
likelihood	B
the	O
simplest	O
approach	O
is	O
to	O
use	O
the	O
bic	B
approximation	O
which	O
has	O
the	O
form	O
log	O
pdt	O
t	O
ktct	O
log	O
n	O
t	O
scaling	O
up	O
to	O
larger	O
graphs	O
the	O
main	O
challenge	O
in	O
computing	O
the	O
posterior	O
over	O
dags	O
is	O
that	O
there	O
are	O
so	O
many	O
possible	O
graphs	O
more	O
precisely	O
showed	O
that	O
the	O
number	O
of	O
dags	O
on	O
d	O
nodes	B
satisfies	O
the	O
following	O
recurrence	O
if	O
i	O
f	O
d	O
i	O
for	O
d	O
the	O
base	O
case	O
is	O
f	O
solving	O
this	O
recurrence	O
yields	O
the	O
following	O
sequence	O
in	O
view	O
of	O
the	O
enormous	O
size	O
of	O
the	O
hypothesis	B
space	I
we	O
are	O
generally	O
forced	O
to	O
use	O
approximate	O
methods	O
some	O
of	O
which	O
we	O
review	O
below	O
approximating	O
the	O
mode	B
of	O
the	O
posterior	O
we	O
can	O
use	O
dynamic	B
programming	I
to	O
find	O
the	O
globally	O
optimal	O
map	O
dag	B
to	O
markov	B
equivalence	I
and	O
sood	O
silander	O
and	O
myllmaki	O
unfortunately	O
this	O
method	O
takes	O
v	O
time	O
and	O
space	O
making	O
it	O
intractable	O
beyond	O
about	O
nodes	B
indeed	O
the	O
general	O
problem	O
of	O
finding	O
the	O
globally	O
optimal	O
map	O
dag	B
is	O
provably	O
np-complete	B
consequently	O
we	O
must	O
settle	O
for	O
finding	O
a	O
locally	O
optimal	O
map	O
dag	B
the	O
most	O
common	O
method	O
is	O
greedy	O
hill	B
climbing	I
at	O
each	O
step	O
the	O
algorithm	O
proposes	O
small	O
changes	O
to	O
the	O
current	O
graph	B
such	O
as	O
adding	O
deleting	O
or	O
reversing	O
a	O
single	O
edge	O
it	O
then	O
moves	O
to	O
the	O
neighboring	O
graph	B
which	O
most	O
increases	O
the	O
posterior	O
the	O
method	O
stops	O
when	O
it	O
reaches	O
a	O
local	O
maximum	O
it	O
is	O
important	O
that	O
the	O
method	O
only	O
proposes	O
local	O
changes	O
to	O
the	O
graph	B
we	O
can	O
make	O
this	O
method	O
more	O
efficient	O
by	O
using	O
to	O
select	O
the	O
parents	B
et	O
al	O
in	O
this	O
case	O
we	O
need	O
to	O
approximate	O
the	O
marginal	O
likelhood	O
as	O
we	O
discuss	O
below	O
a	O
longer	O
list	O
of	O
values	O
can	O
be	O
found	O
at	O
interestingly	O
the	O
number	O
of	O
dags	O
is	O
equal	O
to	O
the	O
number	O
of	O
matrices	O
all	O
of	O
whose	O
eigenvalues	O
are	O
positive	O
real	O
numbers	O
et	O
al	O
learning	B
dag	B
structures	O
evidence	B
case	O
course	O
question	O
msg	O
fact	O
god	O
gun	O
christian	O
nasa	O
shuttle	O
drive	O
scsi	O
disk	O
government	O
religion	O
jesus	O
car	O
disease	O
mission	O
space	O
law	O
jews	O
engine	O
patients	O
orbit	O
games	O
program	O
rights	O
power	O
bible	O
honda	O
computer	O
bmw	O
medicine	O
earth	O
solar	O
season	O
launch	O
technology	O
dos	O
dealer	O
science	O
moon	O
system	O
team	O
satellite	O
files	O
problem	O
studies	O
mars	B
lunar	O
players	O
version	O
human	O
israel	O
war	O
president	O
hockey	O
hit	O
windows	O
university	O
nhl	O
puck	O
baseball	O
won	O
email	O
memory	O
ftp	O
state	B
research	O
league	O
fans	O
win	O
phone	B
format	O
video	O
mac	O
children	B
world	O
oil	O
cancer	O
number	O
image	O
data	O
driver	O
software	O
water	O
health	O
food	O
aids	O
insurance	O
doctor	O
help	O
vitamin	O
pc	O
card	O
server	O
graphics	O
display	O
figure	O
a	O
locally	O
optimal	O
dag	B
learned	O
from	O
the	O
data	O
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
since	O
this	O
enables	O
the	O
change	O
in	O
marginal	B
likelihood	B
hence	O
the	O
posterior	O
to	O
be	O
computed	O
in	O
constant	O
time	O
we	O
cache	O
the	O
sufficient	B
statistics	I
this	O
is	O
because	O
all	O
but	O
one	O
or	O
two	O
of	O
the	O
terms	O
in	O
equation	O
will	O
cancel	O
out	O
when	O
computing	O
the	O
log	O
bayes	B
factor	B
log	O
log	O
pgd	O
we	O
can	O
initialize	O
the	O
search	O
from	O
the	O
best	O
tree	B
which	O
can	O
be	O
found	O
using	O
exact	O
methods	O
discussed	O
in	O
section	O
for	O
speed	O
we	O
can	O
restrict	O
the	O
search	O
so	O
it	O
only	O
adds	O
edges	B
which	O
are	O
part	O
of	O
the	O
markov	B
blankets	O
estimated	O
from	O
a	O
dependency	B
network	I
figure	O
gives	O
an	O
example	O
of	O
a	O
dag	B
learned	O
in	O
this	O
way	O
from	O
the	O
data	O
we	O
can	O
use	O
techniques	O
such	O
as	O
multiple	B
random	I
restarts	I
to	O
increase	O
the	O
chance	O
of	O
finding	O
a	O
good	O
local	O
maximum	O
we	O
can	O
also	O
use	O
more	O
sophisticated	O
local	O
search	O
methods	O
such	O
as	O
genetic	B
algorithms	I
or	O
simulated	B
annealing	B
for	O
structure	B
learning	B
approximating	O
other	O
functions	O
of	O
the	O
posterior	O
if	O
our	O
goal	O
is	O
knowledge	B
discovery	I
the	O
map	O
dag	B
can	O
be	O
misleading	O
for	O
reasons	O
we	O
discussed	O
in	O
section	O
a	O
better	O
approach	O
is	O
to	O
compute	O
the	O
probability	O
that	O
each	O
edge	O
is	O
present	O
pgst	O
of	O
the	O
probability	O
there	O
is	O
a	O
path	B
from	O
s	O
to	O
t	O
we	O
can	O
do	O
this	O
exactly	O
using	O
dynamic	B
programming	I
parviainen	O
and	O
koivisto	O
unfortunately	O
these	O
methods	O
take	O
v	O
time	O
in	O
the	O
general	O
case	O
making	O
them	O
intractable	O
for	O
graphs	O
with	O
more	O
than	O
about	O
chapter	O
graphical	B
model	I
structure	B
learning	B
nodes	B
an	O
approximate	O
method	O
is	O
to	O
sample	O
dags	O
from	O
the	O
posterior	O
and	O
then	O
to	O
compute	O
the	O
fraction	O
of	O
times	O
there	O
is	O
an	O
s	O
t	O
edge	O
or	O
path	B
for	O
each	O
t	O
pair	O
the	O
standard	O
way	O
to	O
draw	O
samples	B
is	O
to	O
use	O
the	O
metropolis	B
hastings	I
algorithm	O
where	O
we	O
use	O
the	O
same	O
local	O
proposal	O
as	O
we	O
did	O
in	O
greedy	O
search	O
and	O
raftery	O
a	O
faster-mixing	O
method	O
is	O
to	O
use	O
a	O
collapsed	O
mh	B
sampler	O
as	O
suggested	O
in	O
and	O
koller	O
this	O
exploits	O
the	O
fact	O
that	O
if	O
a	O
total	B
ordering	I
of	O
the	O
nodes	B
is	O
known	O
we	O
can	O
select	O
the	O
parents	B
for	O
each	O
node	O
independently	O
without	O
worrying	O
about	O
cycles	O
as	O
discussed	O
in	O
section	O
by	O
summing	O
over	O
all	O
possible	O
choice	O
of	O
parents	B
we	O
can	O
marginalize	O
out	O
this	O
part	O
of	O
the	O
problem	O
and	O
just	O
sample	O
total	O
orders	O
and	O
wong	O
also	O
use	O
order-space	O
mcmc	B
but	O
this	O
time	O
with	O
a	O
parallel	B
tempering	I
mcmc	B
algorithm	O
learning	B
dag	B
structure	O
with	O
latent	B
variables	O
sometimes	O
the	O
complete	B
data	I
assumption	I
does	O
not	O
hold	O
either	O
because	O
we	O
have	O
missing	B
data	I
and	O
or	O
because	O
we	O
have	O
hidden	B
variables	I
in	O
this	O
case	O
the	O
marginal	B
likelihood	B
is	O
given	O
by	O
pdg	O
pd	O
h	O
gp	O
pd	O
h	O
gp	O
h	O
h	O
where	O
h	O
represents	O
the	O
hidden	B
or	O
missing	B
data	I
in	O
general	O
this	O
is	O
intractable	O
to	O
compute	O
for	O
example	O
consider	O
a	O
mixture	B
model	I
where	O
in	O
this	O
case	O
there	O
are	O
kn	O
possible	O
completions	O
of	O
the	O
we	O
don	O
t	O
observe	O
the	O
cluster	O
label	B
data	O
we	O
have	O
k	O
clusters	B
we	O
can	O
evaluate	O
the	O
inner	O
integral	O
for	O
each	O
one	O
of	O
these	O
assignments	O
to	O
h	O
but	O
we	O
cannot	O
afford	O
to	O
evaluate	O
all	O
of	O
the	O
integrals	O
course	O
most	O
of	O
these	O
integrals	O
will	O
correspond	O
to	O
hypotheses	O
with	O
little	O
posterior	O
support	B
such	O
as	O
assigning	O
single	O
data	O
points	O
to	O
isolated	O
clusters	B
but	O
we	O
don	O
t	O
know	O
ahead	O
of	O
time	O
the	O
relative	O
weight	O
of	O
these	O
assignments	O
in	O
this	O
section	O
we	O
discuss	O
some	O
ways	O
for	O
learning	B
dag	B
structure	O
when	O
we	O
have	O
latent	B
variables	O
andor	O
missing	B
data	I
approximating	O
the	O
marginal	B
likelihood	B
when	O
we	O
have	O
missing	B
data	I
the	O
simplest	O
approach	O
is	O
to	O
use	O
standard	O
structure	B
learning	B
methods	O
for	O
fully	O
visible	B
dags	O
but	O
to	O
approximate	O
the	O
marginal	B
likelihood	B
in	O
section	O
we	O
discussed	O
some	O
monte	B
carlo	I
methods	O
for	O
approximating	O
the	O
marginal	B
likelihood	B
however	O
these	O
are	O
usually	O
too	O
slow	O
to	O
use	O
inside	O
of	O
a	O
search	O
over	O
models	O
below	O
we	O
mention	O
some	O
faster	O
deterministic	O
approximations	O
bic	B
approximation	O
a	O
simple	O
approximation	O
is	O
to	O
use	O
the	O
bic	B
score	O
which	O
is	O
given	O
by	O
bicg	O
log	O
pd	O
g	O
log	O
n	O
dimg	O
where	O
dimg	O
is	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
model	O
and	O
is	O
the	O
map	O
or	O
ml	O
estimate	O
however	O
the	O
bic	B
score	O
often	O
severely	O
underestimates	O
the	O
true	O
marginal	B
likelihood	B
and	O
heckerman	O
resulting	O
in	O
it	O
selecting	O
overly	O
simple	O
models	O
learning	B
dag	B
structure	O
with	O
latent	B
variables	O
cheeseman-stutz	B
approximation	I
we	O
now	O
present	O
a	O
better	O
method	O
known	O
as	O
the	O
cheeseman-stutz	B
approximation	I
and	O
stutz	O
we	O
first	O
compute	O
a	O
map	B
estimate	I
of	O
the	O
parameters	O
using	O
em	B
denote	O
the	O
expected	B
sufficient	B
statistics	I
of	O
the	O
data	O
by	O
d	O
d	O
in	O
the	O
case	O
of	O
discrete	B
variables	O
we	O
just	O
fill	O
in	O
the	O
hidden	B
variables	I
with	O
their	O
expectation	O
we	O
then	O
use	O
the	O
exact	O
marginal	B
likelihood	B
equation	O
on	O
this	O
filled-in	O
data	O
pd	O
gp	O
pdg	O
pdg	O
however	O
comparing	O
this	O
to	O
equation	O
we	O
can	O
see	O
that	O
the	O
value	O
will	O
be	O
exponentially	O
smaller	O
since	O
it	O
does	O
not	O
sum	O
over	O
all	O
values	O
of	O
h	O
to	O
correct	O
for	O
this	O
we	O
first	O
write	O
log	O
pdg	O
log	O
pdg	O
log	O
pdg	O
log	O
pdg	O
and	O
then	O
we	O
apply	O
a	O
bic	B
approximation	O
to	O
the	O
last	O
two	O
terms	O
log	O
pdg	O
log	O
pdg	O
dim	O
log	O
pd	O
g	O
n	O
log	O
pd	O
g	O
n	O
dim	O
log	O
pd	O
g	O
log	O
pd	O
g	O
putting	O
it	O
altogether	O
we	O
get	O
log	O
pdg	O
log	O
pdg	O
log	O
pd	O
g	O
log	O
pd	O
g	O
the	O
first	O
term	O
pdg	O
can	O
be	O
computed	O
by	O
plugging	O
in	O
the	O
filled-in	O
data	O
into	O
the	O
exact	O
marginal	B
likelihood	B
the	O
second	O
term	O
pd	O
g	O
which	O
involves	O
an	O
exponential	O
sum	O
matching	O
the	O
dimensionality	O
of	O
the	O
left	O
hand	O
side	O
can	O
be	O
computed	O
using	O
an	O
inference	B
algorithm	O
the	O
final	O
term	O
pd	O
g	O
can	O
be	O
computed	O
by	O
plugging	O
in	O
the	O
filled-in	O
data	O
into	O
the	O
regular	B
likelihood	B
variational	B
bayes	I
em	B
an	O
even	O
more	O
accurate	O
approach	O
is	O
to	O
use	O
the	O
variational	B
bayes	I
em	B
algorithm	O
recall	B
from	O
section	O
that	O
the	O
key	O
idea	O
is	O
to	O
make	O
the	O
following	O
factorization	O
assumption	O
p	O
q	O
q	O
qzi	O
i	O
where	O
zi	O
are	O
the	O
hidden	B
variables	I
in	O
case	O
i	O
in	O
the	O
e	B
step	I
we	O
update	O
the	O
qzi	O
and	O
in	O
the	O
m	B
step	I
we	O
update	O
q	O
the	O
corresponding	O
variational	B
free	B
energy	I
provides	O
a	O
lower	O
bound	O
on	O
the	O
log	O
marginal	B
likelihood	B
in	O
and	O
ghahramani	O
it	O
is	O
shown	O
that	O
this	O
bound	O
is	O
a	O
much	O
better	O
approximation	O
to	O
the	O
true	O
log	O
marginal	B
likelihood	B
estimated	O
by	O
a	O
slow	O
annealed	B
importance	B
sampling	I
procedure	O
than	O
either	O
bic	B
or	O
cs	O
in	O
fact	O
one	O
can	O
prove	O
that	O
the	O
variational	O
bound	O
will	O
always	O
be	O
more	O
accurate	O
than	O
cs	O
in	O
turn	O
is	O
always	O
more	O
accurate	O
than	O
bic	B
chapter	O
graphical	B
model	I
structure	B
learning	B
pe	O
low	O
low	O
high	O
high	O
h	O
piqhighpeh	O
pmale	O
iq	O
ses	O
low	O
low	O
high	O
high	O
sex	O
ppehighsessex	O
male	O
female	O
male	O
female	O
h	O
ses	O
sex	O
pe	O
cp	O
h	O
pseshighh	O
ses	O
low	O
low	O
low	O
low	O
high	O
high	O
high	O
high	O
iq	O
low	O
low	O
high	O
high	O
low	O
low	O
high	O
high	O
pe	O
low	O
high	O
low	O
high	O
low	O
high	O
low	O
high	O
pcpyessesiqpe	O
figure	O
the	O
most	O
probable	O
dag	B
with	O
a	O
single	O
binary	O
hidden	B
variable	I
learned	O
from	O
the	O
sewell-shah	O
data	O
map	O
estimates	O
of	O
the	O
cpt	O
entries	O
are	O
shown	O
for	O
some	O
of	O
the	O
nodes	B
source	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
david	O
heckerman	O
example	O
college	O
plans	O
revisited	O
let	O
us	O
revisit	O
the	O
college	O
plans	O
dataset	O
from	O
section	O
recall	B
that	O
if	O
we	O
ignore	O
the	O
possibility	O
of	O
hidden	B
variables	I
there	O
was	O
a	O
direct	O
link	O
from	O
socio	O
economic	O
status	O
to	O
iq	O
in	O
the	O
map	O
dag	B
heckerman	O
et	O
al	O
decided	O
to	O
see	O
what	O
would	O
happen	O
if	O
they	O
introduced	O
a	O
hidden	B
variable	I
h	O
which	O
they	O
made	O
a	O
parent	O
of	O
both	O
ses	O
and	O
iq	O
representing	O
a	O
hidden	B
common	O
cause	O
they	O
also	O
considered	O
a	O
variant	O
in	O
which	O
h	O
points	O
to	O
ses	O
iq	O
and	O
pe	O
for	O
both	O
such	O
cases	O
they	O
considered	O
dropping	O
none	O
one	O
or	O
both	O
of	O
the	O
ses-pe	O
and	O
pe-iq	O
edges	B
they	O
varied	O
the	O
number	O
of	O
states	O
for	O
the	O
hidden	B
node	O
from	O
to	O
thus	O
they	O
computed	O
the	O
approximate	O
posterior	O
over	O
different	O
models	O
using	O
the	O
cs	O
approximation	O
the	O
most	O
probable	O
model	O
which	O
they	O
found	O
is	O
shown	O
in	O
figure	O
this	O
is	O
times	O
it	O
is	O
also	O
times	O
more	O
more	O
likely	O
than	O
the	O
best	O
model	O
containing	O
no	O
hidden	B
variable	I
likely	O
than	O
the	O
second	O
most	O
probable	O
model	O
with	O
a	O
hidden	B
variable	I
so	O
again	O
the	O
posterior	O
is	O
very	O
peaked	O
these	O
results	O
suggests	O
that	O
there	O
is	O
indeed	O
a	O
hidden	B
common	O
cause	O
underlying	O
both	O
the	O
socio-economic	O
status	O
of	O
the	O
parents	B
and	O
the	O
iq	O
of	O
the	O
children	B
by	O
examining	O
the	O
cpt	O
entries	O
we	O
see	O
that	O
both	O
ses	O
and	O
iq	O
are	O
more	O
likely	O
to	O
be	O
high	O
when	O
h	O
takes	O
on	O
the	O
value	O
they	O
interpret	O
this	O
to	O
mean	B
that	O
the	O
hidden	B
variable	I
represents	O
parent	O
quality	O
a	O
genetic	O
factor	B
note	O
however	O
that	O
the	O
arc	O
between	O
h	O
and	O
ses	O
can	O
be	O
reversed	O
without	O
changing	O
the	O
vstructures	O
in	O
the	O
graph	B
and	O
thus	O
without	O
affecting	O
the	O
likelihood	B
this	O
underscores	O
the	O
difficulty	O
in	O
interpreting	O
hidden	B
variables	I
interestingly	O
the	O
hidden	B
variable	I
model	O
has	O
the	O
same	O
conditional	B
independence	I
assumptions	O
amongst	O
the	O
visible	B
variables	I
as	O
the	O
most	O
probable	O
visible	B
variable	O
model	O
so	O
it	O
is	O
not	O
possible	O
to	O
distinguish	O
between	O
these	O
hypotheses	O
by	O
merely	O
looking	O
at	O
the	O
empirical	O
conditional	O
independencies	O
in	O
the	O
data	O
is	O
the	O
basis	O
of	O
the	O
constraint-based	B
approach	I
to	O
structure	B
learning	B
and	O
verma	O
spirtes	O
et	O
al	O
instead	O
by	O
adopting	O
a	O
bayesian	B
approach	O
which	O
takes	O
parsimony	O
into	O
account	O
not	O
just	O
conditional	B
independence	I
we	O
can	O
discover	O
learning	B
dag	B
structure	O
with	O
latent	B
variables	O
the	O
possible	O
existence	O
of	O
hidden	B
factors	B
this	O
is	O
the	O
basis	O
of	O
much	O
of	O
scientific	O
and	O
everday	O
human	O
reasoning	O
e	O
g	O
and	O
tenenbaum	O
for	O
a	O
discussion	O
structural	B
em	B
one	O
way	O
to	O
perform	O
structural	O
inference	B
in	O
the	O
presence	O
of	O
missing	B
data	I
is	O
to	O
use	O
a	O
standard	O
search	O
procedure	O
or	O
stochastic	O
and	O
to	O
use	O
the	O
methods	O
from	O
section	O
to	O
estimate	O
the	O
marginal	B
likelihood	B
however	O
this	O
approach	O
is	O
very	O
efficient	O
because	O
the	O
marginal	B
likelihood	B
does	O
not	O
decompose	O
when	O
we	O
have	O
missing	B
data	I
and	O
nor	O
do	O
its	O
approximations	O
for	O
example	O
if	O
we	O
use	O
the	O
cs	O
approximation	O
or	O
the	O
vbem	B
approximation	O
we	O
have	O
to	O
perform	O
inference	B
in	O
every	O
neighboring	O
model	O
just	O
to	O
evaluate	O
the	O
quality	O
of	O
a	O
single	O
move	O
thiesson	O
et	O
al	O
presents	O
a	O
much	O
more	O
efficient	O
approach	O
called	O
the	O
structural	B
em	B
algorithm	O
the	O
basic	O
idea	O
is	O
this	O
instead	O
of	O
fitting	O
each	O
candidate	O
neighboring	O
graph	B
and	O
then	O
filling	O
in	O
its	O
data	O
fill	O
in	O
the	O
data	O
once	O
and	O
use	O
this	O
filled-in	O
data	O
to	O
evaluate	O
the	O
score	O
of	O
all	O
the	O
neighbors	B
although	O
this	O
might	O
be	O
a	O
bad	O
approximation	O
to	O
the	O
marginal	B
likelihood	B
it	O
can	O
be	O
a	O
good	O
enough	O
approximation	O
of	O
the	O
difference	O
in	O
marginal	O
likelihoods	O
between	O
different	O
models	O
which	O
is	O
all	O
we	O
need	O
in	O
order	O
to	O
pick	O
the	O
best	O
neighbor	O
more	O
precisely	O
define	O
to	O
be	O
the	O
data	O
filled	O
in	O
using	O
model	O
with	O
map	O
parameters	O
now	O
define	O
a	O
modified	O
bic	B
score	O
as	O
follows	O
scorebicgd	O
log	O
pd	O
g	O
log	O
n	O
dimg	O
log	O
pg	O
log	O
p	O
where	O
we	O
have	O
included	O
the	O
log	O
prior	O
for	O
the	O
graph	B
and	O
parameters	O
one	O
can	O
show	O
that	O
if	O
we	O
pick	O
a	O
graph	B
g	O
which	O
increases	O
the	O
bic	B
score	O
relative	O
to	O
on	O
the	O
expected	O
data	O
it	O
will	O
also	O
increase	O
the	O
score	O
on	O
the	O
actual	O
data	O
i	O
e	O
scorebicgd	O
to	O
convert	O
this	O
into	O
an	O
algorithm	O
we	O
proceed	O
as	O
follows	O
first	O
we	O
initialize	O
with	O
some	O
graph	B
and	O
some	O
set	O
of	O
parameters	O
then	O
we	O
fill-in	O
the	O
data	O
using	O
the	O
current	O
parameters	O
in	O
practice	O
this	O
means	O
when	O
we	O
ask	O
for	O
the	O
expected	O
counts	O
for	O
any	O
particular	O
family	B
we	O
perform	O
inference	B
using	O
our	O
current	O
model	O
we	O
know	O
which	O
counts	O
we	O
will	O
need	O
we	O
can	O
precompute	O
all	O
of	O
them	O
which	O
is	O
much	O
faster	O
we	O
then	O
evaluate	O
the	O
bic	B
score	O
of	O
all	O
of	O
our	O
neighbors	B
using	O
the	O
filled-in	O
data	O
and	O
we	O
pick	O
the	O
best	O
neighbor	O
we	O
then	O
refit	O
the	O
model	O
parameters	O
fill-in	O
the	O
data	O
again	O
and	O
repeat	O
for	O
increased	O
speed	O
we	O
may	O
choose	O
to	O
only	O
refit	O
the	O
model	O
every	O
few	O
steps	O
since	O
small	O
changes	O
to	O
the	O
structure	O
hopefully	O
won	O
t	O
invalidate	O
the	O
parameter	B
estimates	O
and	O
the	O
filled-in	O
data	O
too	O
much	O
one	O
interesting	O
application	O
is	O
to	O
learn	O
a	O
phylogenetic	B
tree	B
structure	O
here	O
the	O
observed	O
leaves	B
are	O
the	O
dna	O
or	O
protein	O
sequences	O
of	O
currently	O
alive	O
species	O
and	O
the	O
goal	O
is	O
to	O
infer	O
the	O
topology	O
of	O
the	O
tree	B
and	O
the	O
values	O
of	O
the	O
missing	B
internal	O
nodes	B
there	O
are	O
many	O
classical	B
algorithms	O
for	O
this	O
task	O
e	O
g	O
et	O
al	O
but	O
one	O
that	O
uses	O
sem	O
is	O
discussed	O
in	O
et	O
al	O
another	O
interesting	O
application	O
of	O
this	O
method	O
is	O
to	O
learn	O
sparse	B
mixture	B
models	O
and	O
friedman	O
the	O
idea	O
is	O
that	O
we	O
have	O
one	O
hidden	B
variable	I
c	O
specifying	O
the	O
cluster	O
and	O
we	O
have	O
to	O
choose	O
whether	O
to	O
add	O
edges	B
c	O
xt	O
for	O
each	O
possible	O
feature	O
xt	O
thus	O
some	O
features	B
also	O
et	O
al	O
will	O
be	O
dependent	O
on	O
the	O
cluster	O
id	O
and	O
some	O
will	O
be	O
independent	O
chapter	O
graphical	B
model	I
structure	B
learning	B
figure	O
part	O
of	O
a	O
hierarchical	O
latent	B
tree	B
learned	O
from	O
the	O
data	O
from	O
figure	O
of	O
and	O
williams	O
used	O
with	O
kind	O
permission	O
of	O
stefan	O
harmeling	O
for	O
a	O
different	O
way	O
to	O
perform	O
this	O
task	O
using	O
regular	B
em	B
and	O
a	O
set	O
of	O
bits	B
one	O
per	O
feature	O
that	O
are	O
free	O
to	O
change	O
across	O
data	O
cases	O
discovering	O
hidden	B
variables	I
in	O
section	O
we	O
introduced	O
a	O
hidden	B
variable	I
by	O
hand	O
and	O
then	O
figured	O
out	O
the	O
local	O
topology	O
by	O
fitting	O
a	O
series	O
of	O
different	O
models	O
and	O
computing	O
the	O
one	O
with	O
the	O
best	O
marginal	B
likelihood	B
how	O
can	O
we	O
automate	O
this	O
process	O
figure	O
provides	O
one	O
useful	O
intuition	O
if	O
there	O
is	O
a	O
hidden	B
variable	I
in	O
the	O
true	O
model	O
then	O
its	O
children	B
are	O
likely	O
to	O
be	O
densely	O
connected	O
this	O
suggest	O
the	O
following	O
heuristic	O
et	O
al	O
perform	O
structure	B
learning	B
in	O
the	O
visible	B
domain	O
and	O
then	O
look	O
for	O
structural	B
signatures	I
such	O
as	O
sets	O
of	O
densely	O
connected	O
nodes	B
introduce	O
a	O
hidden	B
variable	I
and	O
connect	O
it	O
to	O
all	O
nodes	B
in	O
this	O
near-clique	O
and	O
then	O
let	O
structural	B
em	B
sort	O
out	O
the	O
details	O
unfortunately	O
this	O
technique	O
does	O
not	O
work	O
too	O
well	O
since	O
structure	B
learning	B
algorithms	O
are	O
biased	O
against	O
fitting	O
models	O
with	O
densely	O
connected	O
cliques	B
another	O
useful	O
intuition	O
comes	O
from	O
clustering	B
in	O
a	O
flat	O
mixture	B
model	I
also	O
called	O
a	O
latent	B
class	I
model	I
the	O
discrete	B
latent	B
variable	O
provides	O
a	O
compressed	O
representation	O
of	O
its	O
children	B
thus	O
we	O
want	O
to	O
create	O
hidden	B
variables	I
with	O
high	O
mutual	B
information	B
with	O
their	O
children	B
one	O
way	O
to	O
do	O
this	O
is	O
to	O
create	O
a	O
tree-structured	O
hierarchy	O
of	O
latent	B
variables	O
each	O
of	O
which	O
calls	O
this	O
a	O
hierarchical	O
latent	B
class	O
only	O
has	O
to	O
explain	O
a	O
small	O
set	O
of	O
children	B
model	O
they	O
propose	B
a	O
greedy	O
local	O
search	O
algorithm	O
to	O
learn	O
such	O
structures	O
based	O
on	O
adding	O
or	O
deleting	O
hidden	B
nodes	B
adding	O
or	O
deleting	O
edges	B
etc	O
that	O
learning	B
the	O
optimal	O
latent	B
learning	B
dag	B
structure	O
with	O
latent	B
variables	O
president	O
government	O
power	O
children	B
war	O
religion	O
earth	O
lunar	O
orbit	O
satellite	O
solar	O
law	O
state	B
human	O
rights	O
world	O
israel	O
jews	O
bible	O
god	O
gun	O
christian	O
jesus	O
moon	O
mars	B
technology	O
mission	O
space	O
launch	O
shuttle	O
nasa	O
health	O
case	O
course	O
evidence	B
fact	O
question	O
program	O
food	O
aids	O
insurance	O
msg	O
water	O
studies	O
medicine	O
car	O
dealer	O
cancer	O
disease	O
doctor	O
patients	O
vitamin	O
bmw	O
engine	O
honda	O
oil	O
version	O
files	O
ftp	O
email	O
format	O
phone	B
windows	O
image	O
number	O
card	O
driver	O
dos	O
puck	O
season	O
team	O
win	O
video	O
disk	O
memory	O
pc	O
software	O
display	O
server	O
games	O
baseball	O
league	O
players	O
fans	O
hockey	O
nhl	O
won	O
graphics	O
system	O
data	O
scsi	O
drive	O
computer	O
hit	O
problem	O
help	O
mac	O
science	O
university	O
research	O
figure	O
a	O
partially	O
latent	B
tree	B
learned	O
from	O
the	O
data	O
note	O
that	O
some	O
words	O
can	O
have	O
multiple	O
meanings	O
and	O
get	O
connected	O
to	O
different	O
latent	B
variables	O
representing	O
different	O
topics	O
for	O
example	O
the	O
word	O
win	O
can	O
refer	O
to	O
a	O
sports	O
context	O
by	O
or	O
the	O
microsoft	B
windows	O
context	O
by	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
jin	O
choi	O
tree	B
is	O
np-hard	B
recently	O
and	O
williams	O
proposed	O
a	O
faster	O
greedy	O
algorithm	O
for	O
learning	B
such	O
models	O
based	O
on	O
agglomerative	B
hierarchical	B
clustering	B
rather	O
than	O
go	O
into	O
details	O
we	O
just	O
give	O
an	O
example	O
of	O
what	O
this	O
system	O
can	O
learn	O
figure	O
shows	O
part	O
of	O
a	O
latent	B
forest	B
learned	O
from	O
the	O
data	O
the	O
algorithm	O
imposes	O
the	O
constraint	O
that	O
each	O
latent	B
node	O
has	O
exactly	O
two	O
children	B
for	O
speed	O
reasons	O
nevertheless	O
we	O
see	O
interpretable	O
clusters	B
arising	O
for	O
example	O
figure	O
shows	O
separate	O
clusters	B
concerning	O
medicine	O
sports	O
and	O
religion	O
this	O
provides	O
an	O
alternative	O
to	O
lda	B
and	O
other	O
topic	B
models	O
with	O
the	O
added	O
advantage	O
that	O
inference	B
in	O
latent	B
trees	O
is	O
exact	O
and	O
takes	O
time	O
linear	O
in	O
the	O
number	O
of	O
nodes	B
an	O
alternative	O
approach	O
is	O
proposed	O
in	O
et	O
al	O
in	O
which	O
the	O
observed	O
data	O
is	O
not	O
constrained	O
to	O
be	O
at	O
the	O
leaves	B
this	O
method	O
starts	O
with	O
the	O
chow-liu	O
tree	B
on	O
the	O
observed	O
data	O
and	O
then	O
adds	O
hidden	B
variables	I
to	O
capture	O
higher-order	O
dependencies	O
between	O
internal	O
nodes	B
this	O
results	O
in	O
much	O
more	O
compact	O
models	O
as	O
shown	O
in	O
figure	O
this	O
model	O
also	O
has	O
better	O
predictive	B
accuracy	O
than	O
other	O
approaches	O
such	O
as	O
mixture	B
models	O
or	O
trees	O
where	O
all	O
the	O
observed	O
data	O
is	O
forced	O
to	O
be	O
at	O
the	O
leaves	B
interestingly	O
one	O
can	O
show	O
that	O
this	O
method	O
can	O
recover	O
the	O
exact	O
latent	B
tree	B
structure	O
providing	O
the	O
data	O
is	O
generated	O
from	O
a	O
tree	B
see	O
chapter	O
graphical	B
model	I
structure	B
learning	B
figure	O
google	O
s	O
rephil	B
model	O
leaves	B
represent	O
presence	O
or	O
absence	O
of	O
words	O
internal	O
nodes	B
represent	O
clusters	B
of	O
co-occuring	O
words	O
or	O
concepts	O
all	O
nodes	B
are	O
binary	O
and	O
all	O
cpds	O
are	O
noisy-or	B
the	O
model	O
contains	O
million	O
word	O
nodes	B
million	O
latent	B
cluster	O
nodes	B
and	O
million	O
edges	B
used	O
with	O
kind	O
permission	O
of	O
brian	O
milch	O
et	O
al	O
for	O
details	O
note	O
however	O
that	O
this	O
approach	O
unlike	O
harmeling	O
and	O
williams	O
requires	O
that	O
the	O
cardinality	O
of	O
all	O
the	O
variables	O
hidden	B
and	O
observed	O
be	O
the	O
same	O
furthermore	O
if	O
the	O
observed	O
variables	O
are	O
gaussian	B
the	O
hidden	B
variables	I
must	O
be	O
gaussian	B
also	O
case	O
study	O
google	O
s	O
rephil	B
in	O
this	O
section	O
we	O
describe	O
a	O
huge	O
dgm	B
called	O
rephil	B
which	O
was	O
automatically	O
learned	O
from	O
the	O
model	O
is	O
widely	O
used	O
inside	O
google	O
for	O
various	O
purposes	O
including	O
their	O
famous	O
adsense	B
the	O
model	O
structure	O
is	O
shown	O
in	O
figure	O
the	O
leaves	B
are	O
binary	O
nodes	B
and	O
represent	O
the	O
presence	O
or	O
absence	O
of	O
words	O
or	O
compounds	O
as	O
new	O
york	O
city	O
in	O
a	O
text	O
document	O
or	O
query	O
the	O
latent	B
variables	O
are	O
also	O
binary	O
and	O
represent	O
clusters	B
of	O
co-occuring	O
words	O
all	O
cpds	O
are	O
noisy-or	B
since	O
some	O
leaf	B
nodes	B
words	O
can	O
have	O
many	O
parents	B
this	O
means	O
each	O
edge	O
can	O
be	O
augmented	O
with	O
a	O
hidden	B
variable	I
specifying	O
if	O
the	O
link	O
was	O
activated	O
or	O
not	O
if	O
the	O
link	O
is	O
not	O
active	O
then	O
the	O
parent	O
cannot	O
turn	O
the	O
child	O
on	O
very	O
similar	B
model	O
was	O
proposed	O
independently	O
in	O
and	O
hauskrecht	O
parameter	B
learning	B
is	O
based	O
on	O
em	B
where	O
the	O
hidden	B
activation	B
status	O
of	O
each	O
edge	O
needs	O
to	O
be	O
inferred	O
and	O
heckerman	O
structure	B
learning	B
is	O
based	O
on	O
the	O
old	O
neuroscience	O
the	O
original	O
system	O
called	O
phil	O
was	O
developed	O
by	O
georges	O
harik	O
and	O
noam	O
shazeer	O
it	O
has	O
been	O
published	O
as	O
us	O
patent	O
method	O
and	O
apparatus	O
for	O
learning	B
a	O
probabilistic	O
generative	O
model	O
for	O
text	O
filed	O
in	O
rephil	B
is	O
a	O
more	O
probabilistically	O
sound	O
version	O
of	O
the	O
method	O
developed	O
by	O
uri	O
lerner	O
et	O
al	O
the	O
summary	O
below	O
is	O
based	O
on	O
notes	O
by	O
brian	O
milch	O
also	O
works	O
at	O
google	O
adsense	B
is	O
google	O
s	O
system	O
for	O
matching	O
web	O
pages	O
with	O
content-appropriate	O
ads	O
in	O
an	O
automatic	O
way	O
by	O
extracting	O
semantic	O
keywords	O
from	O
web	O
pages	O
these	O
keywords	O
play	O
a	O
role	O
analogous	O
to	O
the	O
words	O
that	O
users	O
type	O
in	O
when	O
searching	O
this	O
latter	O
form	O
of	O
information	B
is	O
used	O
by	O
google	O
s	O
adwords	B
system	O
the	O
details	O
are	O
secret	O
but	O
gives	O
an	O
overview	O
learning	B
dag	B
structure	O
with	O
latent	B
variables	O
idea	O
that	O
nodes	B
that	O
fire	O
together	O
should	O
wire	O
together	O
to	O
implement	O
this	O
we	O
run	O
inference	B
and	O
check	O
for	O
cluster-word	O
and	O
cluster-cluster	O
pairs	O
that	O
frequently	O
turn	O
on	O
together	O
we	O
then	O
add	O
an	O
edge	O
from	O
parent	O
to	O
child	O
if	O
the	O
link	O
can	O
significantly	O
increase	O
the	O
probability	O
of	O
the	O
child	O
links	O
that	O
are	O
not	O
activated	O
very	O
often	O
are	O
pruned	O
out	O
we	O
initialize	O
with	O
one	O
cluster	O
per	O
document	O
to	O
a	O
set	O
of	O
semantically	O
related	O
phrases	O
we	O
then	O
merge	O
clusters	B
a	O
and	O
b	O
if	O
a	O
explains	O
b	O
s	O
top	O
words	O
and	O
vice	O
versa	O
we	O
can	O
also	O
discard	O
clusters	B
that	O
are	O
used	O
too	O
rarely	O
the	O
model	O
was	O
trained	O
on	O
about	O
billion	O
text	O
snippets	O
or	O
search	O
queries	O
this	O
takes	O
several	O
weeks	O
even	O
on	O
a	O
parallel	O
distributed	O
computing	O
architecture	O
the	O
resulting	O
model	O
contains	O
million	O
word	O
nodes	B
and	O
about	O
million	O
latent	B
cluster	O
nodes	B
there	O
are	O
about	O
million	O
links	O
in	O
the	O
model	O
including	O
many	O
cluster-cluster	O
dependencies	O
the	O
longest	O
path	B
in	O
the	O
graph	B
has	O
length	O
so	O
the	O
model	O
is	O
quite	O
deep	B
exact	O
inference	B
in	O
this	O
model	O
is	O
obviously	O
infeasible	O
however	O
note	O
that	O
most	O
leaves	B
will	O
be	O
off	O
since	O
most	O
words	O
do	O
not	O
occur	O
in	O
a	O
given	O
query	O
such	O
leaves	B
can	O
be	O
analytically	O
removed	O
as	O
shown	O
in	O
exercise	O
we	O
an	O
also	O
prune	O
out	O
unlikely	O
hidden	B
nodes	B
by	O
following	O
the	O
strongest	O
links	O
from	O
the	O
words	O
that	O
are	O
on	O
up	O
to	O
their	O
parents	B
to	O
get	O
a	O
candidate	O
set	O
of	O
concepts	O
we	O
then	O
perform	O
iterative	B
conditional	I
modes	I
to	O
find	O
a	O
good	O
set	O
of	O
local	O
maxima	O
at	O
each	O
step	O
of	O
icm	O
each	O
node	O
sets	O
its	O
value	O
to	O
its	O
most	O
probable	O
state	B
given	O
the	O
values	O
of	O
its	O
neighbors	B
in	O
its	O
markov	B
blanket	I
this	O
continues	O
until	O
it	O
reaches	O
a	O
local	O
maximum	O
we	O
can	O
repeat	O
this	O
process	O
a	O
few	O
times	O
from	O
random	O
starting	O
configurations	O
at	O
google	O
this	O
can	O
be	O
made	O
to	O
run	O
in	O
milliseconds	O
structural	B
equation	I
models	I
a	O
structural	B
equation	I
model	I
is	O
a	O
special	O
kind	O
of	O
directed	B
mixed	I
graph	B
possibly	O
cyclic	O
in	O
which	O
all	O
cpds	O
are	O
linear	B
gaussian	B
and	O
in	O
which	O
all	O
bidirected	O
edges	B
represent	O
correlated	O
gaussian	B
noise	O
such	O
models	O
are	O
also	O
called	O
path	B
diagrams	I
sems	O
are	O
widely	O
used	O
especially	O
in	O
economics	O
and	O
social	O
science	O
it	O
is	O
common	O
to	O
interpret	O
the	O
edge	O
directions	O
in	O
terms	O
of	O
causality	B
where	O
directed	B
cycles	O
are	O
interpreted	O
is	O
in	O
terms	O
of	O
feedback	B
loops	I
e	O
g	O
however	O
the	O
model	O
is	O
really	O
just	O
a	O
way	O
of	O
specifying	O
a	O
joint	O
gaussian	B
as	O
we	O
show	O
below	O
there	O
is	O
nothing	O
inherently	O
causal	O
about	O
it	O
at	O
all	O
discuss	O
causality	B
in	O
section	O
we	O
can	O
define	O
an	O
sem	O
as	O
a	O
series	O
of	O
full	B
conditionals	O
as	O
follows	O
xi	O
i	O
wijxj	O
where	O
n	O
we	O
can	O
rewrite	O
the	O
model	O
in	O
matrix	O
form	O
as	O
follows	O
x	O
wx	O
x	O
w	O
hence	O
the	O
joint	B
distribution	I
is	O
given	O
by	O
px	O
n	O
where	O
w	O
w	O
t	O
we	O
draw	O
an	O
arc	O
xi	O
xj	O
if	O
if	O
w	O
is	O
lower	O
triangular	O
then	O
the	O
graph	B
is	O
acyclic	O
if	O
in	O
addition	O
is	O
diagonal	B
then	O
the	O
model	O
is	O
equivalent	O
to	O
a	O
gaussian	B
dgm	B
as	O
discussed	O
in	O
section	O
such	O
models	O
are	O
called	O
recursive	B
if	O
is	O
not	O
diagonal	B
then	O
we	O
draw	O
a	O
bidirected	O
chapter	O
graphical	B
model	I
structure	B
learning	B
figure	O
a	O
cyclic	O
directed	B
mixed	I
graphical	B
model	I
sem	O
note	O
the	O
feedback	O
loop	B
arc	O
xi	O
xj	O
for	O
each	O
non-zero	O
off-diagonal	O
term	O
such	O
edges	B
represent	O
correlation	O
possibly	O
due	O
to	O
a	O
hidden	B
common	O
cause	O
when	O
using	O
structural	B
equation	I
models	I
it	O
is	O
common	O
to	O
partition	O
the	O
variables	O
into	O
latent	B
variables	O
zt	O
and	O
observed	O
or	O
manifest	B
variables	O
yt	O
for	O
example	O
figure	O
illustrates	O
the	O
following	O
model	O
where	O
the	O
presence	O
of	O
a	O
feedback	O
loop	B
is	O
evident	O
from	O
the	O
fact	O
that	O
w	O
is	O
not	O
lower	O
triangular	O
also	O
the	O
presence	O
of	O
confounding	O
between	O
and	O
is	O
evident	O
in	O
the	O
off-diagonal	O
terms	O
in	O
often	O
we	O
assume	O
there	O
are	O
multiple	O
observations	O
for	O
each	O
latent	B
variable	O
to	O
ensure	O
identifiability	O
we	O
can	O
set	O
the	O
mean	B
of	O
the	O
latent	B
variables	O
zt	O
to	O
and	O
we	O
can	O
set	O
the	O
regression	B
weights	O
of	O
zt	O
yt	O
to	O
this	O
essentially	O
defines	O
the	O
scale	O
of	O
each	O
latent	B
variable	O
addition	O
to	O
the	O
z	O
s	O
there	O
are	O
the	O
extra	O
hidden	B
variables	I
implied	O
by	O
the	O
presence	O
of	O
the	O
bidirected	O
edges	B
the	O
standard	O
practice	O
in	O
the	O
sem	O
community	O
as	O
exemplified	O
by	O
the	O
popular	O
commercial	O
software	O
package	O
called	O
lisrel	B
from	O
httpwww	O
ssicentral	O
comlisrel	O
is	O
to	O
learning	B
causal	O
dags	O
build	O
the	O
structure	O
by	O
hand	O
to	O
estimate	O
the	O
parameters	O
by	O
maximum	O
likelihood	B
and	O
then	O
to	O
test	O
if	O
any	O
of	O
the	O
regression	B
weights	O
are	O
significantly	O
different	O
from	O
using	O
standard	O
frequentist	B
methods	O
however	O
one	O
can	O
also	O
use	O
bayesian	B
inference	B
for	O
the	O
parameters	O
e	O
g	O
et	O
al	O
structure	B
learning	B
in	O
sems	O
is	O
rare	O
but	O
since	O
recursive	B
sems	O
are	O
equivalent	O
to	O
gaussian	B
dags	O
many	O
of	O
the	O
techniques	O
we	O
have	O
been	O
discussing	O
in	O
this	O
section	O
can	O
be	O
applied	O
sems	O
are	O
closely	O
related	O
to	O
factor	B
analysis	I
models	O
the	O
basic	O
difference	O
is	O
that	O
in	O
an	O
fa	B
model	O
the	O
latent	B
gaussian	B
has	O
a	O
low-rank	O
covariance	B
matrix	I
and	O
the	O
observed	O
noise	O
has	O
a	O
diagonal	B
covariance	B
no	O
bidirected	O
edges	B
in	O
an	O
sem	O
the	O
covariance	B
of	O
the	O
latent	B
gaussian	B
has	O
a	O
sparse	B
cholesky	B
decomposition	I
least	O
if	O
w	O
is	O
acyclic	O
and	O
the	O
observed	O
noise	O
might	O
have	O
a	O
full	B
covariance	B
matrix	I
note	O
that	O
sems	O
can	O
be	O
extended	O
in	O
many	O
ways	O
for	O
example	O
we	O
can	O
add	O
covariates	B
input	O
variables	O
noisily	O
observed	O
we	O
can	O
make	O
some	O
of	O
the	O
observations	O
be	O
discrete	B
by	O
using	O
probit	B
links	O
and	O
so	O
on	O
learning	B
causal	O
dags	O
causal	B
models	I
are	O
models	O
which	O
can	O
predict	O
the	O
effects	O
of	O
interventions	B
to	O
or	O
manipulations	O
of	O
a	O
system	O
for	O
example	O
an	O
electronic	O
circuit	O
diagram	O
implicitly	O
provides	O
a	O
compact	O
encoding	O
of	O
what	O
will	O
happen	O
if	O
one	O
removes	O
any	O
given	O
component	O
or	O
cuts	O
any	O
wire	O
a	O
causal	O
medical	O
model	O
might	O
predict	O
that	O
if	O
i	O
continue	O
to	O
smoke	O
i	O
am	O
likely	O
to	O
get	O
lung	O
cancer	O
hence	O
if	O
i	O
cease	O
smoking	O
i	O
am	O
less	O
likely	O
to	O
get	O
lung	O
cancer	O
causal	O
claims	O
are	O
inherently	O
stronger	O
yet	O
more	O
useful	O
than	O
purely	O
associative	B
claims	O
such	O
as	O
people	O
who	O
smoke	O
often	O
have	O
lung	O
cancer	O
causal	B
models	I
are	O
often	O
represented	O
by	O
dags	O
although	O
this	O
is	O
somewhat	O
controversial	O
we	O
explain	O
this	O
causal	O
interpretation	O
of	O
dags	O
below	O
we	O
then	O
show	O
how	O
to	O
use	O
a	O
dag	B
to	O
do	O
causal	O
reasoning	O
finally	O
we	O
briefly	O
discuss	O
how	O
to	O
learn	O
the	O
structure	O
of	O
causal	O
dags	O
a	O
more	O
detailed	O
description	O
of	O
this	O
topic	B
can	O
be	O
found	O
in	O
and	O
and	O
friedman	O
causal	O
interpretation	O
of	O
dags	O
in	O
this	O
section	O
we	O
define	O
a	O
directed	B
edge	O
a	O
b	O
in	O
a	O
dag	B
to	O
mean	B
that	O
a	O
directly	O
causes	O
b	O
so	O
if	O
we	O
manipulate	O
a	O
then	O
b	O
will	O
change	O
this	O
is	O
known	O
as	O
the	O
causal	B
markov	B
assumption	I
course	O
we	O
have	O
not	O
defined	O
the	O
word	O
causes	O
and	O
we	O
cannot	O
do	O
that	O
by	O
appealing	O
to	O
a	O
dag	B
lest	O
we	O
end	O
up	O
with	O
a	O
cyclic	O
definition	O
see	O
for	O
further	O
disussion	O
of	O
this	O
point	O
we	O
will	O
also	O
assume	O
that	O
all	O
relevant	O
variables	O
are	O
included	O
in	O
the	O
model	O
i	O
e	O
there	O
are	O
no	O
unknown	B
confounders	B
reflecting	O
hidden	B
common	O
causes	O
this	O
is	O
called	O
the	O
causal	B
sufficiency	I
assumption	O
there	O
are	O
known	O
to	O
be	O
confounders	B
they	O
should	O
be	O
added	O
to	O
the	O
model	O
although	O
one	O
can	O
sometimes	O
use	O
mixed	B
directed	B
graphs	I
as	O
a	O
way	O
to	O
avoid	O
having	O
to	O
model	O
confounders	B
explicitly	O
assuming	O
we	O
are	O
willing	O
to	O
make	O
the	O
causal	O
markov	B
and	O
causal	B
sufficiency	I
assumptions	O
we	O
can	O
use	O
dags	O
to	O
answer	O
causal	O
questions	O
the	O
key	O
abstraction	O
is	O
that	O
of	O
a	O
perfect	B
intervention	I
this	O
represents	O
the	O
act	O
of	O
setting	O
a	O
variable	O
to	O
some	O
known	O
value	O
say	O
setting	O
xi	O
to	O
xi	O
a	O
real	O
world	O
example	O
of	O
such	O
a	O
perfect	B
intervention	I
is	O
a	O
gene	B
knockout	I
experiment	I
in	O
which	O
a	O
gene	O
is	O
silenced	O
we	O
need	O
some	O
notational	O
convention	O
to	O
distinguish	O
this	O
from	O
observing	O
that	O
xi	O
chapter	O
graphical	B
model	I
structure	B
learning	B
g	O
gdoxx	O
x	O
x	O
figure	O
surgical	O
intervention	O
on	O
x	O
based	O
on	O
er	O
happens	O
to	O
have	O
value	O
xi	O
we	O
use	O
pearl	O
s	O
do	B
calculus	I
notation	O
in	O
the	O
verb	O
to	O
do	O
and	O
write	O
doxi	O
xi	O
to	O
denote	O
the	O
event	O
that	O
we	O
set	O
xi	O
to	O
xi	O
a	O
causal	O
model	O
can	O
be	O
used	O
to	O
make	O
inferences	O
of	O
the	O
form	O
pxdoxi	O
xi	O
which	O
is	O
different	O
from	O
making	O
inferences	O
of	O
the	O
form	O
pxxi	O
xi	O
to	O
understand	O
the	O
difference	O
between	O
conditioning	B
on	O
interventions	B
and	O
conditioning	B
on	O
observations	O
the	O
difference	O
between	O
doing	O
and	O
seeing	O
consider	O
a	O
node	O
dgm	B
s	O
y	O
in	O
which	O
s	O
if	O
you	O
smoke	O
and	O
s	O
otherwise	O
and	O
y	O
if	O
you	O
have	O
yellow-stained	O
fingers	O
and	O
y	O
otherwise	O
if	O
i	O
observe	O
you	O
have	O
yellow	O
fingers	O
i	O
am	O
licensed	O
to	O
infer	O
that	O
you	O
are	O
probably	O
a	O
smoker	O
nicotine	O
causes	O
yellow	O
stains	O
ps	O
p	O
however	O
if	O
i	O
intervene	O
and	O
paint	O
your	O
fingers	O
yellow	O
i	O
am	O
no	O
longer	O
licensed	O
to	O
infer	O
this	O
since	O
i	O
have	O
disrupted	O
the	O
normal	B
causal	O
mechanism	O
thus	O
ps	O
s	O
one	O
way	O
to	O
model	O
perfect	O
interventions	B
is	O
to	O
use	O
graph	B
surgery	I
represent	O
the	O
joint	B
distribution	I
by	O
a	O
dgm	B
and	O
then	O
cut	O
the	O
arcs	O
coming	O
into	O
any	O
nodes	B
that	O
were	O
set	O
by	O
intervention	O
see	O
figure	O
for	O
an	O
example	O
this	O
prevents	O
any	O
information	B
flow	O
from	O
the	O
nodes	B
that	O
were	O
intervened	O
on	O
from	O
being	O
sent	O
back	O
up	O
to	O
their	O
parents	B
having	O
perform	O
this	O
surgery	O
we	O
can	O
then	O
perform	O
probabilistic	B
inference	B
in	O
the	O
resulting	O
mutilated	O
graph	B
in	O
the	O
usual	O
way	O
to	O
reason	O
about	O
the	O
effects	O
of	O
interventions	B
we	O
state	B
this	O
formally	O
as	O
follows	O
theorem	O
theorem	O
spirtes	O
et	O
al	O
to	O
compute	O
pxidoxj	O
for	O
sets	O
of	O
nodes	B
i	O
j	O
we	O
can	O
perform	O
surgical	O
intervention	O
on	O
the	O
xj	O
nodes	B
and	O
then	O
use	O
standard	O
probabilistic	B
inference	B
in	O
the	O
mutilated	O
graph	B
we	O
can	O
generalize	B
the	O
notion	O
of	O
a	O
perfect	B
intervention	I
by	O
adding	O
interventions	B
as	O
explicit	O
action	B
nodes	B
to	O
the	O
graph	B
the	O
result	O
is	O
like	O
an	O
influence	O
diagram	O
except	O
there	O
are	O
no	O
utility	B
nodes	B
dawid	O
this	O
has	O
been	O
called	O
the	O
augmented	B
dag	B
we	O
learning	B
causal	O
dags	O
y	O
x	O
figure	O
illustration	O
of	O
simpson	O
s	O
paradox	O
figure	O
generated	O
by	O
simpsonsparadoxgraph	O
can	O
then	O
define	O
the	O
cpd	B
pxidoxi	O
to	O
be	O
anything	O
we	O
want	O
we	O
can	O
also	O
allow	O
an	O
action	B
to	O
affect	O
multiple	O
nodes	B
this	O
is	O
called	O
a	O
fat	B
hand	I
intervention	O
a	O
reference	O
to	O
someone	O
trying	O
to	O
change	O
a	O
single	O
component	O
of	O
some	O
system	O
an	O
electronic	O
circuit	O
but	O
accidently	O
touching	O
multiple	O
components	O
and	O
thereby	O
causing	O
various	O
side	O
effects	O
and	O
murphy	O
for	O
a	O
way	O
to	O
model	O
this	O
using	O
augmented	O
dags	O
using	O
causal	O
dags	O
to	O
resolve	O
simpson	O
s	O
paradox	O
in	O
this	O
section	O
we	O
assume	O
we	O
know	O
the	O
causal	O
dag	B
we	O
can	O
then	O
do	O
causal	O
reasoning	O
by	O
applying	O
d-separation	O
to	O
the	O
mutilated	O
graph	B
in	O
this	O
section	O
we	O
give	O
an	O
example	O
of	O
this	O
and	O
show	O
how	O
causal	O
reasoning	O
can	O
help	O
resolve	O
a	O
famous	O
paradox	O
known	O
as	O
simpon	O
s	O
paradox	O
simpson	O
s	O
paradox	O
says	O
that	O
any	O
statistical	O
relationship	O
between	O
two	O
variables	O
can	O
be	O
reversed	O
by	O
including	O
additional	O
factors	B
in	O
the	O
analysis	O
for	O
example	O
suppose	O
some	O
cause	O
c	O
taking	O
a	O
drug	O
makes	O
some	O
effect	O
e	O
getting	O
better	O
more	O
likely	O
p	O
p	O
c	O
and	O
yet	O
when	O
we	O
condition	O
on	O
the	O
gender	O
of	O
the	O
patient	O
we	O
find	O
that	O
taking	O
the	O
drug	O
makes	O
the	O
effect	O
less	O
likely	O
in	O
both	O
females	O
and	O
males	O
f	O
p	O
f	O
p	O
c	O
f	O
p	O
f	O
p	O
c	O
f	O
this	O
seems	O
impossible	O
but	O
by	O
the	O
rules	B
of	O
probability	O
this	O
is	O
perfectly	O
possible	O
because	O
the	O
event	O
space	O
where	O
we	O
condition	O
on	O
c	O
f	O
or	O
c	O
f	O
can	O
be	O
completely	O
different	O
to	O
the	O
event	O
space	O
when	O
we	O
just	O
condition	O
on	O
c	O
the	O
table	O
of	O
numbers	O
below	O
shows	O
a	O
concrete	O
example	O
e	O
c	O
c	O
total	O
combined	O
e	O
total	O
rate	B
e	O
male	O
e	O
total	O
rate	B
e	O
e	O
female	O
total	O
rate	B
chapter	O
graphical	B
model	I
structure	B
learning	B
from	O
this	O
table	O
of	O
numbers	O
we	O
see	O
that	O
pec	O
pe	O
c	O
pec	O
f	O
pe	O
c	O
f	O
pec	O
f	O
pe	O
f	O
a	O
visual	O
representation	O
of	O
the	O
paradox	O
is	O
given	O
in	O
in	O
figure	O
the	O
line	O
which	O
goes	O
up	O
and	O
to	O
the	O
right	O
shows	O
that	O
the	O
effect	O
increases	O
as	O
the	O
cause	O
increases	O
however	O
the	O
dots	O
represent	O
the	O
data	O
for	O
females	O
and	O
the	O
crosses	O
represent	O
the	O
data	O
for	O
males	O
within	O
each	O
subgroup	O
we	O
see	O
that	O
the	O
effect	O
decreases	O
as	O
we	O
increase	O
the	O
cause	O
it	O
is	O
clear	O
that	O
the	O
effect	O
is	O
real	O
but	O
it	O
is	O
still	O
very	O
counter-intuitive	O
the	O
reason	O
the	O
paradox	O
arises	O
is	O
that	O
we	O
are	O
interpreting	O
the	O
statements	O
causally	O
but	O
we	O
are	O
not	O
using	O
proper	O
causal	O
reasoning	O
when	O
performing	O
our	O
calculations	O
the	O
statement	O
that	O
the	O
drug	O
c	O
causes	O
recovery	O
e	O
is	O
p	O
p	O
c	O
whereas	O
the	O
data	O
merely	O
tell	O
us	O
p	O
p	O
c	O
this	O
is	O
not	O
a	O
contradiction	O
observing	O
c	O
is	O
positive	O
evidence	B
for	O
e	O
since	O
more	O
males	O
than	O
females	O
take	O
the	O
drug	O
and	O
the	O
male	O
recovery	O
rate	B
is	O
higher	O
of	O
the	O
drug	O
thus	O
equation	O
does	O
not	O
imply	O
equation	O
nevertheless	O
we	O
are	O
left	O
with	O
a	O
practical	O
question	O
should	O
we	O
use	O
the	O
drug	O
or	O
not	O
it	O
seems	O
like	O
if	O
we	O
don	O
t	O
know	O
the	O
patient	O
s	O
gender	O
we	O
should	O
use	O
the	O
drug	O
but	O
as	O
soon	O
as	O
we	O
discover	O
if	O
they	O
are	O
male	O
or	O
female	O
we	O
should	O
stop	O
using	O
it	O
obviously	O
this	O
conclusion	O
is	O
ridiculous	O
to	O
answer	O
the	O
question	O
we	O
need	O
to	O
make	O
our	O
assumptions	O
more	O
explicit	O
suppose	O
reality	O
can	O
be	O
modeled	O
by	O
the	O
causal	O
dag	B
in	O
figure	O
to	O
compute	O
the	O
causal	O
effect	O
of	O
c	O
on	O
e	O
we	O
need	O
to	O
adjust	B
for	I
condition	O
on	O
the	O
confounding	B
variable	I
f	O
this	O
is	O
necessary	O
because	O
there	O
is	O
a	O
backdoor	B
path	B
from	O
c	O
to	O
e	O
via	O
f	O
so	O
we	O
need	O
to	O
check	O
the	O
c	O
e	O
relationship	O
for	O
each	O
value	O
of	O
f	O
separately	O
to	O
make	O
sure	O
the	O
relationship	O
between	O
c	O
and	O
e	O
is	O
not	O
affected	O
by	O
any	O
value	O
of	O
f	O
suppose	O
that	O
for	O
each	O
value	O
of	O
f	O
taking	O
the	O
drug	O
is	O
harmful	O
that	O
is	O
pedoc	O
f	O
pedo	O
c	O
f	O
pedoc	O
f	O
pedo	O
c	O
f	O
then	O
we	O
can	O
show	O
that	O
taking	O
the	O
drug	O
is	O
harmful	O
overall	O
pedoc	O
pedo	O
c	O
the	O
proof	O
is	O
as	O
follows	O
first	O
from	O
our	O
assumptions	O
in	O
figure	O
we	O
see	O
that	O
drugs	O
have	O
no	O
effect	O
on	O
gender	O
pfdoc	O
pfdo	O
c	O
pf	O
now	O
using	O
the	O
law	O
of	O
total	O
probability	O
pedoc	O
pedoc	O
f	O
pedoc	O
f	O
fdoc	O
pedoc	O
f	O
f	O
f	O
learning	B
causal	O
dags	O
treatment	O
gender	O
treatment	O
blood	O
pressure	O
c	O
f	O
c	O
f	O
e	O
recovery	O
e	O
recovery	O
figure	O
two	O
different	O
models	O
uses	O
to	O
illustrate	O
simpson	O
s	O
paradox	O
f	O
is	O
gender	O
and	O
is	O
a	O
confounder	B
for	O
c	O
and	O
e	O
f	O
is	O
blood	O
pressure	O
and	O
is	O
caused	O
by	O
c	O
similarly	O
pedo	O
c	O
pedo	O
c	O
f	O
edo	O
c	O
f	O
f	O
since	O
every	O
term	O
in	O
equation	O
is	O
less	O
than	O
the	O
corresponding	O
term	O
in	O
equation	O
we	O
conclude	O
that	O
pedoc	O
pedo	O
c	O
so	O
if	O
the	O
model	O
in	O
figure	O
is	O
correct	O
we	O
should	O
not	O
administer	O
the	O
drug	O
since	O
it	O
reduces	O
the	O
probability	O
of	O
the	O
effect	O
now	O
consider	O
a	O
different	O
version	O
of	O
this	O
example	O
suppose	O
we	O
keep	O
the	O
data	O
the	O
same	O
but	O
interpret	O
f	O
as	O
something	O
that	O
is	O
affected	O
by	O
c	O
such	O
as	O
blood	O
pressure	O
see	O
figure	O
in	O
this	O
case	O
we	O
can	O
no	O
longer	O
assume	O
pfdoc	O
pfdo	O
c	O
pf	O
and	O
the	O
above	O
proof	O
breaks	O
down	O
so	O
pedoc	O
pedo	O
c	O
may	O
be	O
positive	O
or	O
negaitve	O
in	O
the	O
true	O
model	O
is	O
figure	O
then	O
we	O
should	O
not	O
condition	O
on	O
f	O
when	O
assessing	O
the	O
effect	O
of	O
c	O
on	O
e	O
since	O
there	O
is	O
no	O
backdoor	B
path	B
in	O
this	O
case	O
because	O
of	O
the	O
v-structure	B
at	O
f	O
that	O
is	O
conditioning	B
on	O
f	O
might	O
block	O
one	O
of	O
the	O
causal	O
pathways	O
in	O
other	O
words	O
by	O
comparing	O
patients	O
with	O
the	O
same	O
post-treatment	O
blood	O
pressure	O
of	O
f	O
we	O
may	O
mask	O
the	O
effect	O
of	O
one	O
of	O
the	O
two	O
pathways	O
by	O
which	O
the	O
drug	O
operates	O
to	O
bring	O
about	O
recovery	O
thus	O
we	O
see	O
that	O
different	O
causal	O
assumptions	O
lead	O
to	O
different	O
causal	O
conclusions	O
and	O
hence	O
different	O
courses	O
of	O
action	B
this	O
raises	O
the	O
question	O
on	O
whether	O
we	O
can	O
learn	O
the	O
causal	O
model	O
from	O
data	O
we	O
discuss	O
this	O
issue	O
below	O
learning	B
causal	O
dag	B
structures	O
in	O
this	O
section	O
we	O
discuss	O
some	O
ways	O
to	O
learn	O
causal	O
dag	B
structures	O
chapter	O
graphical	B
model	I
structure	B
learning	B
learning	B
from	O
observational	O
data	O
in	O
section	O
we	O
discussed	O
various	O
methods	O
for	O
learning	B
dag	B
structures	O
from	O
observational	O
data	O
it	O
is	O
natural	O
to	O
ask	O
whether	O
these	O
methods	O
can	O
recover	O
the	O
true	O
dag	B
structure	O
that	O
was	O
used	O
to	O
generate	O
the	O
data	O
clearly	O
even	O
if	O
we	O
have	O
infinite	O
data	O
an	O
optimal	O
method	O
can	O
only	O
identify	O
the	O
dag	B
up	O
to	O
markov	B
equivalence	I
that	O
is	O
it	O
can	O
identify	O
the	O
pdag	B
directed	B
acylic	O
graph	B
but	O
not	O
the	O
complete	B
dag	B
structure	O
because	O
all	O
dags	O
which	O
are	O
markov	B
equivalent	I
have	O
the	O
same	O
likelihood	B
there	O
are	O
several	O
algorithms	O
the	O
greedy	B
equivalence	I
search	I
method	O
of	O
that	O
are	O
consistent	B
estimators	I
of	O
pdag	B
structure	O
in	O
the	O
sense	O
that	O
they	O
identify	O
the	O
true	O
markov	B
equivalence	B
class	I
as	O
the	O
sample	O
size	O
goes	O
to	O
infinity	O
assuming	O
we	O
observe	O
all	O
the	O
variables	O
however	O
we	O
also	O
have	O
to	O
assume	O
that	O
the	O
generating	O
distribution	O
p	O
is	O
faithful	B
to	O
the	O
generating	O
dag	B
g	O
this	O
means	O
that	O
all	O
the	O
conditional	O
indepence	O
properties	O
of	O
p	O
are	O
exactly	O
captured	O
by	O
the	O
graphical	O
structure	O
so	O
ip	B
ig	O
this	O
means	O
there	O
cannot	O
be	O
any	O
ci	B
properties	O
in	O
p	O
that	O
are	O
due	O
to	O
particular	O
settings	O
of	O
the	O
parameters	O
as	O
zeros	O
in	O
a	O
regression	B
matrix	O
that	O
are	O
not	O
graphically	O
explicit	O
for	O
this	O
reason	O
a	O
faithful	B
distribution	O
is	O
also	O
called	O
a	O
stable	B
distribution	O
suppose	O
the	O
assumptions	O
hold	O
and	O
we	O
learn	O
a	O
pdag	B
what	O
can	O
we	O
do	O
with	O
it	O
instead	O
of	O
recovering	O
the	O
full	B
graph	B
we	O
can	O
focus	O
on	O
the	O
causal	O
analog	O
of	O
edge	O
marginals	O
by	O
computing	O
the	O
magnitude	O
of	O
the	O
causal	O
effect	O
of	O
one	O
node	O
on	O
another	O
a	O
on	O
b	O
if	O
we	O
know	O
the	O
dag	B
we	O
can	O
do	O
this	O
using	O
techniques	O
described	O
in	O
if	O
the	O
dag	B
is	O
unknown	B
we	O
can	O
compute	O
a	O
lower	O
bound	O
on	O
the	O
effect	O
as	O
follows	O
et	O
al	O
learn	O
an	O
equivalence	B
class	I
from	O
data	O
enumerate	O
all	O
the	O
dags	O
in	O
the	O
equivalence	B
class	I
apply	O
pearl	O
s	O
do-calculus	O
to	O
compute	O
the	O
magnitude	O
of	O
the	O
causal	O
effect	O
of	O
a	O
on	O
b	O
in	O
each	O
dag	B
finally	O
take	O
the	O
minimum	O
of	O
these	O
effects	O
as	O
the	O
lower	O
bound	O
it	O
is	O
usually	O
computationally	O
infeasible	O
to	O
compute	O
all	O
dags	O
in	O
the	O
equivalence	B
class	I
but	O
fortunately	O
one	O
only	O
needs	O
to	O
be	O
able	O
to	O
identify	O
the	O
local	O
neighborhood	O
of	O
a	O
and	O
b	O
which	O
can	O
be	O
esimated	O
more	O
efficiently	O
as	O
described	O
in	O
et	O
al	O
this	O
technique	O
is	O
called	O
ida	B
which	O
is	O
short	O
for	O
intervention-calculus	O
when	O
the	O
dag	B
is	O
absent	O
in	O
et	O
al	O
this	O
technique	O
was	O
applied	O
to	O
some	O
yeast	O
gene	O
expression	O
data	O
gene	O
knockout	O
data	O
was	O
used	O
to	O
estimate	O
the	O
ground	O
truth	O
effect	O
of	O
each	O
single-gene	O
deletions	O
on	O
the	O
remaining	O
genes	O
then	O
the	O
algorithm	O
was	O
applied	O
to	O
unperturbed	O
samples	B
and	O
was	O
used	O
to	O
rank	O
order	O
the	O
likely	O
targets	O
of	O
each	O
of	O
the	O
genes	O
the	O
method	O
had	O
a	O
precision	B
of	O
when	O
the	O
recall	B
was	O
set	O
to	O
while	O
low	O
this	O
is	O
substantially	O
more	O
than	O
rival	O
variable-selection	O
methods	O
such	O
as	O
lasso	B
and	O
elastic	B
net	I
which	O
were	O
only	O
slightly	O
above	O
chance	O
learning	B
from	O
interventional	B
data	I
if	O
we	O
want	O
to	O
distinguish	O
between	O
dags	O
within	O
the	O
equivalence	B
class	I
we	O
need	O
to	O
use	O
interventional	B
data	I
where	O
certain	O
variables	O
have	O
been	O
set	O
and	O
the	O
consequences	O
have	O
been	O
measured	O
an	O
example	O
of	O
this	O
is	O
the	O
dataset	O
in	O
figure	O
where	O
proteins	O
in	O
a	O
signalling	O
pathway	O
were	O
perturbed	O
and	O
their	O
phosphorylation	O
status	O
was	O
measured	O
using	O
a	O
technique	O
called	O
flow	O
cytometry	O
et	O
al	O
it	O
is	O
straightforward	O
to	O
modify	O
the	O
standard	O
bayesian	B
scoring	O
criteria	O
such	O
as	O
the	O
marginal	B
likelihood	B
or	O
bic	B
score	O
to	O
handle	O
learning	B
from	O
mixed	O
observational	O
and	O
experimental	O
data	O
we	O
learning	B
causal	O
dags	O
psitect	O
akt	O
inh	O
plcy	O
present	O
missing	B
int	O
edge	O
pkc	O
pma	O
raf	O
f	O
akt	O
pka	O
erk	O
jnk	O
figure	O
a	O
design	B
matrix	I
consisting	O
of	O
data	O
points	O
measuring	O
the	O
status	O
flow	O
cytometry	O
of	O
proteins	O
under	O
different	O
experimental	O
conditions	O
the	O
data	O
has	O
been	O
discretized	O
low	O
medium	O
and	O
high	O
some	O
proteins	O
were	O
explicitly	O
controlled	O
using	O
into	O
states	O
activating	O
or	O
inhibiting	O
chemicals	O
a	O
directed	B
graphical	B
model	I
representing	O
dependencies	O
between	O
various	O
proteins	O
circles	O
and	O
various	O
experimental	O
interventions	B
ovals	O
which	O
was	O
inferred	O
from	O
this	O
data	O
we	O
plot	O
all	O
edges	B
for	O
which	O
pgst	O
dotted	O
edges	B
are	O
believed	O
to	O
exist	O
in	O
nature	O
but	O
were	O
not	O
discovered	O
by	O
the	O
algorithm	O
false	B
negative	I
solid	O
edges	B
are	O
true	O
positives	O
the	O
light	O
colored	O
edges	B
represent	O
the	O
effects	O
of	O
intervention	O
source	O
figure	O
of	O
and	O
murphy	O
this	O
figure	O
can	O
be	O
reproduced	O
using	O
the	O
code	O
at	O
httpwww	O
cs	O
ubc	O
camurphyksoftwarebdaglindex	O
html	O
chapter	O
graphical	B
model	I
structure	B
learning	B
just	O
compute	O
the	O
sufficient	B
statistics	I
for	O
a	O
cpd	B
s	O
parameter	B
by	O
skipping	O
over	O
the	O
cases	O
where	O
that	O
node	O
was	O
set	O
by	O
intervention	O
and	O
yoo	O
for	O
example	O
when	O
using	O
tabular	O
cpds	O
we	O
modify	O
the	O
counts	O
as	O
follows	O
ntck	O
ixit	O
k	O
xipat	O
c	O
ixit	O
not	O
set	O
the	O
justification	O
for	O
this	O
is	O
that	O
in	O
cases	O
where	O
node	O
t	O
is	O
set	O
by	O
force	O
it	O
is	O
not	O
sampled	O
from	O
its	O
usual	O
mechanism	O
so	O
such	O
cases	O
should	O
be	O
ignored	O
when	O
inferring	O
the	O
parameter	B
t	O
the	O
modified	O
scoring	O
criterion	O
can	O
be	O
combined	O
with	O
any	O
of	O
the	O
standard	O
structure	B
learning	B
algorithms	O
and	O
geng	O
discusses	O
some	O
methods	O
for	O
choosing	O
which	O
interventions	B
to	O
perform	O
so	O
as	O
to	O
reduce	O
the	O
posterior	O
uncertainty	B
as	O
quickly	O
as	O
possible	O
form	O
of	O
active	B
learning	B
the	O
preceeding	O
method	O
assumes	O
the	O
interventions	B
are	O
perfect	O
in	O
reality	O
experimenters	O
can	O
rarely	O
control	O
the	O
state	B
of	O
individual	O
molecules	O
instead	O
they	O
inject	O
various	O
stimulant	O
or	O
inhibitor	O
chemicals	O
which	O
are	O
designed	O
to	O
target	O
specific	O
molecules	O
but	O
which	O
may	O
have	O
side	O
effects	O
we	O
can	O
model	O
this	O
quite	O
simply	O
by	O
adding	O
the	O
intervention	O
nodes	B
to	O
the	O
dag	B
and	O
then	O
learning	B
a	O
larger	O
augmented	B
dag	B
structure	O
with	O
the	O
constraint	O
that	O
there	O
are	O
no	O
edges	B
between	O
the	O
intervention	O
nodes	B
and	O
no	O
edges	B
from	O
the	O
regular	B
nodes	B
back	O
to	O
the	O
intervention	O
nodes	B
figure	O
shows	O
the	O
augmented	B
dag	B
that	O
was	O
learned	O
from	O
the	O
interventional	O
flow	O
in	O
particular	O
we	O
plot	O
the	O
median	B
graph	B
which	O
cytometry	O
data	O
depicted	O
in	O
figure	O
includes	O
all	O
edges	B
for	O
which	O
pgij	O
these	O
were	O
computed	O
using	O
the	O
exact	O
it	O
turns	O
out	O
that	O
in	O
this	O
example	O
the	O
median	B
model	I
has	O
exactly	O
algorithm	O
of	O
the	O
same	O
structure	O
as	O
the	O
optimal	O
map	O
model	O
argmaxg	O
pgd	O
which	O
was	O
computed	O
using	O
the	O
algorithm	O
of	O
and	O
sood	O
silander	O
and	O
myllmaki	O
learning	B
undirected	B
gaussian	B
graphical	B
models	I
learning	B
the	O
structured	O
of	O
undirected	B
graphical	B
models	I
is	O
easier	O
than	O
learning	B
dag	B
structure	O
because	O
we	O
don	O
t	O
need	O
to	O
worry	O
about	O
acyclicity	O
on	O
the	O
other	O
hand	O
it	O
is	O
harder	O
than	O
learning	B
dag	B
structure	O
since	O
the	O
likelihood	B
does	O
not	O
decompose	O
section	O
this	O
precludes	O
the	O
kind	O
of	O
local	O
search	O
methods	O
greedy	O
search	O
and	O
mcmc	B
sampling	O
we	O
used	O
to	O
learn	O
dag	B
structures	O
because	O
the	O
cost	O
of	O
evaluating	O
each	O
neighboring	O
graph	B
is	O
too	O
high	O
since	O
we	O
have	O
to	O
refit	O
each	O
model	O
from	O
scratch	O
is	O
no	O
way	O
to	O
incrementally	O
update	O
the	O
score	O
of	O
a	O
model	O
in	O
this	O
section	O
we	O
discuss	O
several	O
solutions	O
to	O
this	O
problem	O
in	O
the	O
context	O
of	O
gaussian	B
random	O
fields	O
or	O
undirected	B
gaussian	B
graphical	B
models	I
we	O
consider	O
structure	B
learning	B
for	O
discrete	B
undirected	B
models	O
in	O
section	O
mle	B
for	O
a	O
ggm	O
before	O
discussing	O
structure	B
learning	B
we	O
need	O
to	O
discuss	O
parameter	B
estimation	O
the	O
task	O
of	O
computing	O
the	O
mle	B
for	O
a	O
ggm	O
is	O
called	O
covariance	B
selection	I
from	O
equation	O
the	O
log	O
likelihood	B
can	O
be	O
written	O
as	O
log	O
det	O
trs	O
learning	B
undirected	B
gaussian	B
graphical	B
models	I
xxi	O
xt	O
is	O
the	O
empirical	O
notational	O
simplicity	O
we	O
assume	O
we	O
have	O
already	O
estimated	O
x	O
n	O
where	O
is	O
the	O
precision	B
matrix	I
and	O
s	O
covariance	B
matrix	I
one	O
can	O
show	O
that	O
the	O
gradient	O
of	O
this	O
is	O
given	O
by	O
s	O
however	O
we	O
have	O
to	O
enforce	O
the	O
constraints	O
that	O
st	O
if	O
gst	O
zeros	O
and	O
that	O
is	O
positive	O
definite	O
the	O
former	O
constraint	O
is	O
easy	O
to	O
enforce	O
but	O
the	O
latter	O
is	O
somewhat	O
challenging	O
still	O
a	O
convex	B
constraint	O
one	O
approach	O
is	O
to	O
add	O
a	O
penalty	O
term	O
to	O
the	O
objective	O
if	O
leaves	B
the	O
positive	O
definite	O
cone	O
this	O
is	O
the	O
approach	O
used	O
in	O
ggmfitminfunc	O
also	O
et	O
al	O
another	O
approach	O
is	O
to	O
use	O
a	O
coordinate	O
descent	O
method	O
described	O
in	O
et	O
al	O
and	O
implemented	O
in	O
ggmfithtf	O
yet	O
another	O
approach	O
is	O
to	O
use	O
iterative	O
proportional	O
fitting	O
described	O
in	O
section	O
however	O
ipf	B
requires	O
identifying	O
the	O
cliques	B
of	O
the	O
graph	B
which	O
is	O
np-hard	B
in	O
general	O
interestingly	O
one	O
can	O
show	O
that	O
the	O
mle	B
must	O
satisfy	O
the	O
following	O
property	O
st	O
sst	O
if	O
gst	O
or	O
s	O
t	O
i	O
e	O
the	O
covariance	B
of	O
a	O
pair	O
that	O
are	O
connected	O
by	O
an	O
edge	O
must	O
match	O
the	O
in	O
addition	O
we	O
have	O
st	O
if	O
gst	O
by	O
definition	O
of	O
a	O
ggm	O
i	O
e	O
empirical	O
covariance	B
the	O
precision	B
of	O
a	O
pair	O
that	O
are	O
not	O
connected	O
must	O
be	O
we	O
say	O
that	O
is	O
a	O
positive	O
definite	O
matrix	B
completion	I
of	O
s	O
since	O
it	O
retains	O
as	O
many	O
of	O
the	O
entries	O
in	O
s	O
as	O
possible	O
corresponding	O
to	O
the	O
edges	B
in	O
the	O
graph	B
subject	O
to	O
the	O
required	O
sparsity	B
pattern	B
on	O
corresponding	O
to	O
the	O
absent	O
edges	B
the	O
remaining	O
entries	O
in	O
are	O
filled	O
in	O
so	O
as	O
to	O
maximize	O
the	O
likelihood	B
let	O
us	O
consider	O
a	O
worked	O
example	O
from	O
et	O
al	O
we	O
will	O
use	O
the	O
following	O
adjacency	B
matrix	I
representing	O
the	O
cyclic	O
structure	O
and	O
the	O
following	O
empirical	O
covariance	B
matrix	I
s	O
g	O
the	O
mle	B
is	O
given	O
by	O
ggmfitdemo	O
for	O
the	O
code	O
to	O
reproduce	O
these	O
numbers	O
the	O
constrained	O
elements	O
in	O
and	O
the	O
free	O
elements	O
in	O
both	O
of	O
which	O
correspond	O
to	O
absent	O
edges	B
have	O
been	O
highlighted	O
graphical	B
lasso	B
we	O
now	O
discuss	O
one	O
way	O
to	O
learn	O
a	O
sparse	B
grf	O
structure	O
which	O
exploits	O
the	O
fact	O
that	O
there	O
is	O
a	O
correspondence	B
between	O
zeros	O
in	O
the	O
precision	B
matrix	I
and	O
absent	O
edges	B
in	O
the	O
graph	B
this	O
suggests	O
that	O
we	O
can	O
learn	O
a	O
sparse	B
graph	B
structure	O
by	O
using	O
an	O
objective	O
that	O
encourages	O
zeros	O
in	O
the	O
precision	B
matrix	I
by	O
analogy	O
to	O
lasso	B
section	O
one	O
can	O
define	O
the	O
following	O
penalized	O
nll	B
j	O
log	O
det	O
trs	O
chapter	O
graphical	B
model	I
structure	B
learning	B
figure	O
sparse	B
ggms	O
learned	O
using	O
graphical	B
lasso	B
applied	O
to	O
the	O
flow	O
cytometry	O
data	O
figure	O
generated	O
by	O
ggmlassodemo	O
where	O
glasso	B
jk	O
jk	O
is	O
the	O
of	O
the	O
matrix	O
this	O
is	O
called	O
the	O
graphical	B
lasso	B
or	O
although	O
the	O
objective	O
is	O
convex	B
it	O
is	O
non-smooth	B
of	O
the	O
non-differentiable	O
penalty	O
and	O
is	O
constrained	O
must	O
be	O
a	O
positive	O
definite	O
matrix	O
several	O
algorithms	O
have	O
been	O
proposed	O
for	O
optimizing	O
this	O
objective	O
and	O
lin	O
banerjee	O
et	O
al	O
duchi	O
et	O
al	O
although	O
arguably	O
the	O
simplest	O
is	O
the	O
one	O
in	O
et	O
al	O
which	O
uses	O
a	O
coordinate	O
descent	O
algorithm	O
similar	B
to	O
the	O
shooting	B
algorithm	O
for	O
lasso	B
see	O
ggmlassohtf	O
for	O
an	O
implementation	O
also	O
and	O
hastie	O
for	O
a	O
more	O
recent	O
version	O
of	O
this	O
algorithm	O
as	O
an	O
example	O
let	O
us	O
apply	O
the	O
method	O
to	O
the	O
flow	O
cytometry	O
dataset	O
from	O
et	O
al	O
a	O
discretized	O
version	O
of	O
the	O
data	O
is	O
shown	O
in	O
figure	O
here	O
we	O
use	O
the	O
original	O
continuous	O
data	O
however	O
we	O
are	O
ignoring	O
the	O
fact	O
that	O
the	O
data	O
was	O
sampled	O
under	O
intervention	O
in	O
figure	O
we	O
illustrate	O
the	O
graph	B
structures	O
that	O
are	O
learned	O
as	O
we	O
sweep	O
from	O
to	O
a	O
large	O
value	O
these	O
represent	O
a	O
range	O
of	O
plausible	O
hypotheses	O
about	O
the	O
connectivity	O
of	O
these	O
proteins	O
it	O
is	O
worth	O
comparing	O
this	O
with	O
the	O
dag	B
that	O
was	O
learned	O
in	O
figure	O
the	O
dag	B
has	O
the	O
advantage	O
that	O
it	O
can	O
easily	O
model	O
the	O
interventional	O
nature	O
of	O
the	O
data	O
but	O
the	O
disadvantage	O
that	O
it	O
cannot	O
model	O
the	O
feedback	B
loops	I
that	O
are	O
known	O
to	O
exist	O
in	O
this	O
biological	O
pathway	O
the	O
discussion	O
in	O
and	O
murphy	O
note	O
that	O
the	O
fact	O
that	O
we	O
show	O
many	O
ugms	O
and	O
only	O
one	O
dag	B
is	O
incidental	O
we	O
could	O
easily	O
use	O
bic	B
to	O
pick	O
the	O
best	O
ugm	B
and	O
conversely	O
we	O
learning	B
undirected	B
gaussian	B
graphical	B
models	I
could	O
easily	O
display	O
several	O
dag	B
structures	O
sampled	O
from	O
the	O
posterior	O
bayesian	B
inference	B
for	O
ggm	O
structure	O
although	O
the	O
graphical	B
lasso	B
is	O
reasonably	O
fast	O
it	O
only	O
gives	O
a	O
point	B
estimate	I
of	O
the	O
structure	O
furthermore	O
it	O
is	O
not	O
model-selection	O
consistent	B
meaning	O
it	O
cannot	O
recover	O
the	O
true	O
graph	B
even	O
as	O
n	O
it	O
would	O
be	O
preferable	O
to	O
integrate	B
out	I
the	O
parameters	O
and	O
perform	O
posterior	O
inference	B
in	O
the	O
space	O
of	O
graphs	O
i	O
e	O
to	O
compute	O
pgd	O
we	O
can	O
then	O
extract	O
summaries	O
of	O
the	O
posterior	O
such	O
as	O
posterior	O
edge	O
marginals	O
pgij	O
just	O
as	O
we	O
did	O
for	O
dags	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
do	O
this	O
note	O
that	O
the	O
situation	O
is	O
analogous	O
to	O
chapter	O
where	O
we	O
discussed	O
variable	O
selection	O
in	O
section	O
we	O
discussed	O
bayesian	B
variable	O
selection	O
where	O
we	O
integrated	O
out	O
the	O
regression	B
weights	O
and	O
computed	O
p	O
and	O
the	O
marginal	O
inclusion	B
probabilities	I
p	O
j	O
then	O
in	O
section	O
we	O
discussed	O
methods	O
based	O
on	O
regularization	B
here	O
we	O
have	O
the	O
same	O
dichotomy	O
but	O
we	O
are	O
presenting	O
them	O
in	O
the	O
opposite	O
order	O
if	O
the	O
graph	B
is	O
decomposable	B
and	O
if	O
we	O
use	O
conjugate	B
priors	I
we	O
can	O
compute	O
the	O
marginal	B
likelihood	B
in	O
closed	O
form	O
and	O
lauritzen	O
furthermore	O
we	O
can	O
efficiently	O
identify	O
the	O
decomposable	B
neighbors	B
of	O
a	O
graph	B
and	O
green	O
i	O
e	O
the	O
set	O
of	O
legal	O
edge	O
additions	O
and	O
removals	O
this	O
means	O
that	O
we	O
can	O
perform	O
relatively	O
efficient	O
stochastic	O
local	O
search	O
to	O
approximate	O
the	O
posterior	O
e	O
g	O
and	O
green	O
armstrong	O
et	O
al	O
scott	O
and	O
carvalho	O
however	O
the	O
restriction	O
to	O
decomposable	B
graphs	I
is	O
rather	O
limiting	O
if	O
one	O
s	O
goal	O
is	O
knowledge	B
discovery	I
since	O
the	O
number	O
of	O
decomposable	B
graphs	I
is	O
much	O
less	O
than	O
the	O
number	O
of	O
general	O
undirected	B
a	O
few	O
authors	O
have	O
looked	O
at	O
bayesian	B
inference	B
for	O
ggm	O
structure	O
in	O
the	O
non-decomposable	O
case	O
et	O
al	O
wong	O
et	O
al	O
jones	O
et	O
al	O
but	O
such	O
methods	O
cannot	O
scale	O
to	O
large	O
models	O
because	O
they	O
use	O
an	O
expensive	O
monte	B
carlo	I
approximation	O
to	O
the	O
marginal	B
likelihood	B
and	O
massam	O
and	O
dobra	O
suggested	O
using	O
a	O
laplace	B
approxmation	O
this	O
requires	O
computing	O
the	O
map	B
estimate	I
of	O
the	O
parameters	O
for	O
under	O
a	O
gwishart	O
prior	O
in	O
and	O
dobra	O
they	O
used	O
the	O
iterative	O
proportional	O
scaling	O
algorithm	O
and	O
kiiveri	O
hara	O
and	O
takimura	O
to	O
find	O
the	O
mode	B
however	O
this	O
is	O
very	O
slow	O
since	O
it	O
requires	O
knowing	O
the	O
maximal	O
cliques	B
of	O
the	O
graph	B
which	O
is	O
np-hard	B
in	O
general	O
in	O
et	O
al	O
a	O
much	O
faster	O
method	O
is	O
proposed	O
in	O
particular	O
they	O
modify	O
the	O
gradient-based	O
methods	O
from	O
section	O
to	O
find	O
the	O
map	B
estimate	I
these	O
algorithms	O
do	O
not	O
need	O
to	O
know	O
the	O
cliques	B
of	O
the	O
graph	B
a	O
further	O
speedup	O
is	O
obtained	O
by	O
just	O
using	O
a	O
diagonal	B
laplace	B
approximation	I
which	O
is	O
more	O
accurate	O
than	O
bic	B
but	O
has	O
essentially	O
the	O
same	O
cost	O
this	O
plus	O
the	O
lack	O
of	O
restriction	O
to	O
decomposable	B
graphs	I
enables	O
fairly	O
fast	O
stochastic	B
search	I
methods	O
to	O
be	O
used	O
to	O
approximate	O
pgd	O
and	O
its	O
mode	B
this	O
approach	O
significantly	O
outperfomed	O
graphical	B
lasso	B
both	O
in	O
terms	O
of	O
predictive	B
accuracy	O
and	O
structural	O
recovery	O
for	O
a	O
comparable	O
computational	O
cost	O
the	O
number	O
of	O
decomposable	B
graphs	I
on	O
v	O
nodes	B
for	O
v	O
is	O
as	O
follows	O
if	O
we	O
divide	O
these	O
numbers	O
by	O
the	O
number	O
of	O
undirected	B
graphs	O
which	O
is	O
we	O
find	O
the	O
ratios	O
are	O
so	O
we	O
see	O
that	O
decomposable	B
graphs	I
form	O
a	O
vanishing	O
fraction	O
of	O
the	O
total	O
hypothesis	B
space	I
chapter	O
graphical	B
model	I
structure	B
learning	B
handling	O
non-gaussian	O
data	O
using	O
copulas	O
the	O
graphical	B
lasso	B
and	O
variants	O
is	O
inhertently	O
limited	O
to	O
data	O
that	O
is	O
jointly	O
gaussian	B
which	O
is	O
a	O
rather	O
severe	O
restriction	O
fortunately	O
the	O
method	O
can	O
be	O
generalized	O
to	O
handle	O
non-gaussian	O
but	O
still	O
continuous	O
data	O
in	O
a	O
fairly	O
simple	O
fashion	O
the	O
basic	O
idea	O
is	O
to	O
estimate	O
a	O
set	O
of	O
d	O
univariate	O
monotonic	O
transformations	O
fj	O
one	O
per	O
variable	O
j	O
such	O
that	O
the	O
resulting	O
transformed	O
data	O
is	O
jointly	O
gaussian	B
if	O
this	O
is	O
possible	O
we	O
say	O
the	O
data	O
belongs	O
to	O
the	O
nonparametric	O
normal	B
distribution	O
or	O
nonparanormal	B
distribution	O
et	O
al	O
this	O
is	O
equivalent	O
to	O
the	O
family	B
of	O
gaussian	B
copulas	I
and	O
wellner	O
details	O
on	O
how	O
to	O
estimate	O
the	O
fj	O
transformations	O
from	O
the	O
empirical	O
cdf	B
s	O
of	O
each	O
variable	O
can	O
be	O
found	O
in	O
et	O
al	O
after	O
transforming	O
the	O
data	O
we	O
can	O
compute	O
the	O
correlation	B
matrix	I
and	O
then	O
apply	O
glasso	B
in	O
the	O
usual	O
way	O
one	O
can	O
show	O
under	O
various	O
assumptions	O
that	O
this	O
is	O
a	O
consistent	B
estimator	B
of	O
the	O
graph	B
structure	O
representing	O
the	O
ci	B
assumptions	O
of	O
the	O
original	O
distributionliu	O
et	O
al	O
learning	B
undirected	B
discrete	B
graphical	B
models	I
the	O
problem	O
of	O
learning	B
the	O
structure	O
for	O
ugms	O
with	O
discrete	B
variables	O
is	O
harder	O
than	O
the	O
gaussian	B
case	O
because	O
computing	O
the	O
partition	B
function	I
z	O
which	O
is	O
needed	O
for	O
parameter	B
estimation	O
has	O
complexity	O
comparable	O
to	O
computing	O
the	O
permanent	B
of	O
a	O
matrix	O
which	O
in	O
general	O
is	O
intractable	O
et	O
al	O
by	O
contrast	O
in	O
the	O
gaussian	B
case	O
computing	O
z	O
only	O
requires	O
computing	O
a	O
matrix	O
determinant	O
which	O
is	O
at	O
most	O
ov	O
since	O
stochastic	O
local	O
search	O
is	O
not	O
tractable	O
for	O
general	O
discrete	B
ugms	O
below	O
we	O
mention	O
some	O
possible	O
alternative	O
approaches	O
that	O
have	O
been	O
tried	O
graphical	B
lasso	B
for	O
mrfscrfs	O
it	O
is	O
possible	O
to	O
extend	O
the	O
graphical	B
lasso	B
idea	O
to	O
the	O
discrete	B
mrf	B
and	O
crf	B
case	O
however	O
now	O
there	O
is	O
a	O
set	O
of	O
parameters	O
associated	O
with	O
each	O
edge	O
in	O
the	O
graph	B
so	O
we	O
have	O
to	O
use	O
the	O
graph	B
analog	O
of	O
group	B
lasso	B
section	O
for	O
example	O
consider	O
a	O
pairwise	O
crf	B
with	O
ternary	O
nodes	B
and	O
node	O
and	O
edge	O
potentials	O
given	O
by	O
stys	O
yt	O
x	O
wt	O
vt	O
vt	O
vt	O
wt	O
wt	O
wt	O
wt	O
wt	O
wt	O
wt	O
wt	O
tyt	O
x	O
where	O
we	O
assume	O
x	O
begins	O
with	O
a	O
constant	O
term	O
to	O
account	O
for	O
the	O
offset	O
x	O
only	O
contains	O
the	O
crf	B
reduces	O
to	O
an	O
mrf	B
note	O
that	O
we	O
may	O
choose	O
to	O
set	O
some	O
of	O
the	O
vtk	O
and	O
wstjk	O
weights	O
to	O
to	O
ensure	O
identifiability	O
although	O
this	O
can	O
also	O
be	O
taken	O
care	O
of	O
by	O
the	O
prior	O
as	O
shown	O
in	O
exercise	O
to	O
learn	O
sparse	B
structure	O
we	O
can	O
minimize	O
the	O
following	O
objective	O
j	O
t	O
log	O
tyit	O
xi	O
vt	O
log	O
styis	O
yit	O
xi	O
wst	O
learning	B
undirected	B
discrete	B
graphical	B
models	I
case	O
course	O
children	B
bible	O
health	O
christian	O
insurance	O
computer	O
evidence	B
disk	O
email	O
display	O
card	O
fact	O
earth	O
files	O
graphics	O
government	O
god	O
dos	O
format	O
help	O
data	O
image	O
video	O
gun	O
human	O
car	O
president	O
israel	O
jesus	O
drive	O
memory	O
number	O
power	B
law	I
engine	O
dealer	O
jews	O
baseball	O
ftp	O
mac	O
scsi	O
problem	O
rights	O
war	O
religion	O
games	O
fans	O
pc	O
program	O
phone	B
nasa	O
state	B
question	O
software	O
research	O
shuttle	O
launch	O
moon	O
science	O
orbit	O
space	O
university	O
world	O
system	O
driver	O
version	O
technology	O
windows	O
hockey	O
league	O
nhl	O
players	O
season	O
team	O
win	O
won	O
figure	O
an	O
mrf	B
estimated	O
from	O
the	O
data	O
using	O
group	O
regularization	B
with	O
isolated	O
nodes	B
are	O
not	O
plotted	O
from	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
where	O
is	O
the	O
p-norm	O
common	O
choices	O
are	O
p	O
or	O
p	O
as	O
explained	O
in	O
section	O
this	O
method	O
of	O
crf	B
structure	B
learning	B
was	O
first	O
suggested	O
in	O
et	O
al	O
use	O
of	O
regularization	B
for	O
learning	B
the	O
structure	O
of	O
binary	O
mrfs	O
was	O
proposed	O
in	O
et	O
al	O
although	O
this	O
objective	O
is	O
convex	B
it	O
can	O
be	O
costly	O
to	O
evaluate	O
since	O
we	O
need	O
to	O
perform	O
inference	B
to	O
compute	O
its	O
gradient	O
as	O
explained	O
in	O
section	O
is	O
true	O
also	O
for	O
mrfs	O
we	O
should	O
therefore	O
use	O
an	O
optimizer	O
that	O
does	O
not	O
make	O
too	O
many	O
calls	O
to	O
the	O
objective	O
function	O
or	O
its	O
gradient	O
such	O
as	O
the	O
projected	O
quasi-newton	B
method	O
in	O
et	O
al	O
in	O
addition	O
we	O
can	O
use	O
approximate	B
inference	B
such	O
as	O
convex	B
belief	B
propagation	I
to	O
compute	O
an	O
approximate	O
objective	O
and	O
gradient	O
more	O
quickly	O
another	O
approach	O
is	O
to	O
apply	O
the	O
group	B
lasso	B
penalty	O
to	O
the	O
pseudo-likelihood	B
discussed	O
in	O
section	O
this	O
is	O
much	O
faster	O
since	O
inference	B
is	O
no	O
longer	O
required	O
and	O
tibshirani	O
figure	O
shows	O
the	O
result	O
of	O
applying	O
this	O
procedure	O
to	O
the	O
data	O
where	O
yit	O
indicates	O
the	O
presence	O
of	O
word	O
t	O
in	O
document	O
i	O
and	O
xi	O
the	O
model	O
is	O
an	O
mrf	B
chapter	O
graphical	B
model	I
structure	B
learning	B
figure	O
water	O
sprinkler	O
dgm	B
with	O
corresponding	O
binary	O
cpts	B
t	O
and	O
f	O
stand	O
for	O
true	O
and	O
false	O
thin	B
junction	B
trees	I
so	O
far	O
we	O
have	O
been	O
concerned	O
with	O
learning	B
sparse	B
graphs	O
but	O
these	O
do	O
not	O
necessarily	O
have	O
low	O
treewidth	B
for	O
example	O
a	O
d	O
d	O
grid	O
is	O
sparse	B
but	O
has	O
treewidth	B
od	O
this	O
means	O
that	O
the	O
models	O
we	O
learn	O
may	O
be	O
intractable	O
to	O
use	O
for	O
inference	B
purposes	O
which	O
defeats	O
one	O
of	O
the	O
two	O
main	O
reasons	O
to	O
learn	O
graph	B
structure	O
in	O
the	O
first	O
place	O
other	O
reason	O
being	O
knowledge	B
discovery	I
there	O
have	O
been	O
various	O
attempts	O
to	O
learn	O
graphical	B
models	I
with	O
bounded	O
treewidth	B
and	O
jordan	O
srebro	O
elidan	O
and	O
gould	O
shahaf	O
et	O
al	O
also	O
known	O
as	O
thin	B
junction	B
trees	I
but	O
the	O
exact	O
problem	O
in	O
general	O
is	O
hard	O
an	O
alternative	O
approach	O
is	O
to	O
learn	O
a	O
model	O
with	O
low	O
circuit	B
complexity	I
et	O
al	O
poon	O
and	O
domingos	O
such	O
models	O
may	O
have	O
high	O
treewidth	B
but	O
they	O
exploit	O
contextspecific	O
independence	O
and	O
determinism	B
to	O
enable	O
fast	O
exact	O
inference	B
e	O
g	O
exercises	O
exercise	O
causal	O
reasoning	O
in	O
the	O
sprinkler	O
network	O
consider	O
the	O
causal	O
network	O
in	O
figure	O
let	O
t	O
represent	O
true	O
and	O
f	O
represent	O
false	O
a	O
suppose	O
i	O
perform	O
a	O
perfect	B
intervention	I
and	O
make	O
the	O
grass	O
wet	O
what	O
is	O
the	O
probability	O
the	O
sprinkler	O
b	O
suppose	O
i	O
perform	O
a	O
perfect	B
intervention	I
and	O
make	O
the	O
grass	O
dry	O
what	O
is	O
the	O
probability	O
the	O
sprinkler	O
is	O
on	O
ps	O
tdow	O
t	O
is	O
on	O
ps	O
tdow	O
f	O
is	O
the	O
probability	O
the	O
sprinkler	O
is	O
on	O
ps	O
tdoc	O
t	O
c	O
suppose	O
i	O
perform	O
a	O
perfect	B
intervention	I
and	O
make	O
the	O
clouds	O
turn	O
on	O
by	O
seeding	O
them	O
what	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
introduction	O
in	O
this	O
chapter	O
we	O
are	O
concerned	O
with	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
such	O
as	O
bit	O
vectors	O
sequences	O
of	O
categorical	B
variables	I
count	O
vectors	O
graph	B
structures	O
relational	O
data	O
etc	O
these	O
models	O
can	O
be	O
used	O
to	O
analyze	O
voting	O
records	O
text	O
and	O
document	O
collections	O
low-intensity	O
images	O
movie	O
ratings	O
etc	O
however	O
we	O
will	O
mostly	O
focus	O
on	O
text	O
analysis	O
and	O
this	O
will	O
be	O
reflected	O
in	O
our	O
terminology	O
since	O
we	O
will	O
be	O
dealing	O
with	O
so	O
many	O
different	O
kinds	O
of	O
data	O
we	O
need	O
some	O
precise	O
notation	O
to	O
keep	O
things	O
clear	O
when	O
modeling	O
variable-length	O
sequences	O
of	O
categorical	B
variables	I
let	O
yil	O
v	O
represent	O
symbols	O
or	O
tokens	B
such	O
as	O
words	O
in	O
a	O
document	O
we	O
will	O
the	O
identity	O
of	O
the	O
l	O
th	O
word	O
in	O
document	O
i	O
where	O
v	O
is	O
the	O
number	O
of	O
possible	O
words	O
in	O
the	O
vocabulary	O
we	O
assume	O
l	O
li	O
where	O
li	O
is	O
the	O
length	O
of	O
document	O
i	O
and	O
i	O
wheren	O
is	O
the	O
number	O
of	O
documents	O
we	O
will	O
often	O
ignore	O
the	O
word	O
order	O
resulting	O
in	O
a	O
bag	B
of	I
words	I
this	O
can	O
be	O
reduced	O
to	O
a	O
fixed	O
length	O
vector	O
of	O
counts	O
histogram	B
we	O
will	O
use	O
niv	O
li	O
to	O
denote	O
the	O
number	O
of	O
times	O
word	O
v	O
occurs	O
in	O
document	O
i	O
for	O
v	O
note	O
that	O
the	O
n	O
v	O
count	O
matrix	O
n	O
is	O
often	O
large	O
but	O
sparse	B
since	O
we	O
typically	O
have	O
many	O
documents	O
but	O
most	O
words	O
do	O
not	O
occur	O
in	O
any	O
given	O
document	O
in	O
some	O
cases	O
we	O
might	O
have	O
multiple	O
different	O
bags	O
of	O
words	O
e	O
g	O
bags	O
of	O
text	O
words	O
and	O
bags	O
of	O
visual	B
words	I
these	O
correspond	O
to	O
different	O
channels	O
or	O
types	O
of	O
features	B
we	O
will	O
denote	O
these	O
by	O
yirl	O
for	O
r	O
number	O
of	O
responses	O
and	O
l	O
ir	O
if	O
lir	O
it	O
means	O
we	O
have	O
a	O
single	O
token	O
bag	O
of	O
length	O
in	O
this	O
case	O
we	O
just	O
write	O
yir	O
vr	O
if	O
every	O
channel	O
is	O
just	O
a	O
single	O
token	O
we	O
write	O
the	O
fixed-size	O
response	O
vector	O
as	O
for	O
brevity	O
in	O
this	O
case	O
the	O
n	O
r	O
design	B
matrix	I
y	O
will	O
not	O
be	O
sparse	B
for	O
example	O
in	O
social	O
science	O
surveys	O
yir	O
could	O
be	O
the	O
response	O
of	O
person	O
i	O
to	O
the	O
r	O
th	O
multi-choice	O
question	O
out	O
goal	O
is	O
to	O
build	O
joint	O
probability	O
models	O
of	O
pyi	O
or	O
pni	O
using	O
latent	B
variables	O
to	O
capture	O
the	O
correlations	O
we	O
will	O
then	O
try	O
to	O
interpret	O
the	O
latent	B
variables	O
which	O
provide	O
a	O
compressed	O
representation	O
of	O
the	O
data	O
we	O
provide	O
an	O
overview	O
of	O
some	O
approaches	O
in	O
section	O
before	O
going	O
into	O
more	O
detail	O
in	O
later	O
sections	O
towards	O
the	O
end	O
of	O
the	O
chapter	O
we	O
will	O
consider	O
modeling	O
graphs	O
and	O
relations	O
which	O
can	O
also	O
be	O
represented	O
as	O
sparse	B
discrete	B
matrices	O
for	O
example	O
we	O
might	O
want	O
to	O
model	O
the	O
graph	B
of	O
which	O
papers	O
mycite	O
which	O
other	O
papers	O
we	O
will	O
denote	O
these	O
relations	O
by	O
r	O
reserving	O
the	O
symbol	O
y	O
for	O
any	O
categorical	B
data	O
text	O
associated	O
with	O
the	O
nodes	B
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
distributed	O
state	B
lvms	O
for	O
discrete	B
data	O
in	O
this	O
section	O
we	O
summarize	O
a	O
variety	O
of	O
possible	O
approaches	O
for	O
constructing	O
models	O
of	O
the	O
form	O
for	O
bags	O
of	O
tokens	B
for	O
vectors	O
of	O
tokens	B
and	O
pni	O
for	O
vectors	O
of	O
integer	O
counts	O
mixture	B
models	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
finite	O
mixture	B
model	I
this	O
associates	O
a	O
single	O
discrete	B
latent	B
variable	O
qi	O
k	O
with	O
every	O
document	O
where	O
k	O
is	O
the	O
number	O
of	O
clusters	B
we	O
will	O
use	O
a	O
discrete	B
prior	O
qi	O
cat	O
for	O
variable	O
length	O
documents	O
we	O
can	O
define	O
pyilqi	O
k	O
bkv	O
where	O
bkv	O
is	O
the	O
probability	O
that	O
cluster	O
k	O
generates	O
word	O
v	O
the	O
value	O
of	O
qi	O
is	O
known	O
as	O
a	O
topic	B
and	O
the	O
vector	O
bk	O
is	O
the	O
k	O
th	O
topic	B
s	O
word	O
distribution	O
that	O
is	O
the	O
likelihood	B
has	O
the	O
form	O
k	O
catyilbk	O
catyilbk	O
k	O
the	O
induced	O
distribution	O
on	O
the	O
visible	B
data	O
is	O
given	O
by	O
if	O
the	O
sum	O
is	O
unknown	B
we	O
can	O
use	O
a	O
poisson	B
class-conditional	B
density	I
to	O
give	O
pniqi	O
k	O
poiniv	O
vk	O
in	O
this	O
case	O
liqi	O
k	O
poi	O
v	O
vk	O
k	O
the	O
generative	O
story	O
which	O
this	O
encodes	O
is	O
as	O
follows	O
for	O
document	O
i	O
pick	O
a	O
topic	B
qi	O
from	O
call	O
it	O
k	O
and	O
then	O
for	O
each	O
word	O
l	O
i	O
pick	O
a	O
word	O
from	O
bk	O
we	O
will	O
consider	O
more	O
sophisticated	O
generative	O
models	O
later	O
in	O
this	O
chapter	O
if	O
we	O
have	O
a	O
fixed	O
set	O
of	O
categorical	B
observations	O
we	O
can	O
use	O
a	O
different	O
topic	B
matrix	O
for	O
each	O
output	O
variable	O
k	O
catyilbr	O
k	O
this	O
is	O
an	O
unsupervised	O
analog	O
of	O
naive	O
bayes	O
classification	O
if	O
the	O
sum	O
li	O
we	O
can	O
also	O
model	O
count	O
vectors	O
multinomial	B
pnili	O
qi	O
k	O
munili	O
bk	O
v	O
niv	O
is	O
known	O
we	O
can	O
use	O
a	O
distributed	O
state	B
lvms	O
for	O
discrete	B
data	O
exponential	B
family	B
pca	B
unfortunately	O
finite	O
mixture	B
models	O
are	O
very	O
limited	O
in	O
their	O
expressive	O
power	O
a	O
more	O
flexible	O
model	O
is	O
to	O
use	O
a	O
vector	O
of	O
real-valued	O
continuous	O
latent	B
variables	O
similar	B
to	O
the	O
factor	B
analysis	I
in	O
pca	B
we	O
use	O
a	O
gaussian	B
prior	O
of	O
the	O
form	O
pzi	O
and	O
pca	B
models	O
in	O
chapter	O
n	O
where	O
zi	O
r	O
k	O
and	O
a	O
gaussian	B
likelihood	B
of	O
the	O
form	O
pyizi	O
n	O
indeed	O
the	O
method	O
known	O
this	O
method	O
can	O
certainly	O
be	O
applied	O
to	O
discrete	B
or	O
count	O
data	O
as	O
latent	B
semantic	I
analysis	I
orlatent	O
semantic	O
indexing	O
et	O
al	O
dumais	O
and	O
landauer	O
is	O
exactly	O
equivalent	O
to	O
applying	O
pca	B
to	O
a	O
term	O
by	O
document	O
count	O
matrix	O
a	O
better	O
method	O
for	O
modeling	O
categorical	B
data	O
is	O
to	O
use	O
a	O
multinoulli	O
or	O
multinomial	B
distribu	O
tion	O
we	O
just	O
have	O
to	O
change	O
the	O
likelihood	B
to	O
catyilswzi	O
where	O
w	O
r	O
of	O
categorical	B
responses	O
we	O
can	O
use	O
v	O
k	O
is	O
a	O
weight	O
matrix	O
and	O
s	O
is	O
the	O
softmax	B
function	O
if	O
we	O
have	O
a	O
fixed	O
number	O
catyirswrzi	O
where	O
wr	O
r	O
v	O
k	O
is	O
the	O
weight	O
matrix	O
for	O
the	O
r	O
th	O
response	B
variable	I
this	O
model	O
is	O
called	O
categorical	B
pca	B
and	O
is	O
illustrated	O
in	O
figure	O
see	O
section	O
for	O
further	O
discussion	O
if	O
we	O
have	O
counts	O
we	O
can	O
use	O
a	O
multinomial	B
model	O
all	O
of	O
these	O
models	O
are	O
examples	O
of	O
exponential	B
family	B
pca	B
or	O
epca	B
et	O
al	O
mohamed	O
et	O
al	O
which	O
is	O
an	O
unsupervised	O
analog	O
of	O
glms	O
the	O
corresponding	O
induced	O
distribution	O
on	O
the	O
visible	B
variables	I
has	O
the	O
form	O
pyilzi	O
w	O
n	O
fitting	O
this	O
model	O
is	O
tricky	O
due	O
to	O
the	O
lack	O
of	O
conjugacy	O
et	O
al	O
proposed	O
a	O
coordinate	O
ascent	O
method	O
that	O
alternates	O
between	O
estimating	O
the	O
zi	O
and	O
w	O
this	O
can	O
be	O
regarded	O
as	O
a	O
degenerate	B
version	O
of	O
em	B
that	O
computes	O
a	O
point	B
estimate	I
of	O
zi	O
in	O
the	O
e	B
step	I
the	O
problem	O
with	O
the	O
degenerate	B
approach	O
is	O
that	O
it	O
is	O
very	O
prone	O
to	O
overfitting	O
since	O
the	O
number	O
of	O
latent	B
variables	O
is	O
proportional	O
to	O
the	O
number	O
of	O
datacases	O
et	O
al	O
a	O
true	O
em	B
algorithm	O
would	O
marginalize	O
out	O
the	O
latent	B
variables	O
zi	O
a	O
way	O
to	O
do	O
this	O
for	O
categorical	B
pca	B
using	O
variational	B
em	B
is	O
discussed	O
in	O
section	O
for	O
more	O
general	O
models	O
one	O
can	O
use	O
mcmc	B
et	O
al	O
pnili	O
zi	O
muniliswzi	O
or	O
a	O
poisson	B
model	O
pnizi	O
poiniv	O
expwt	O
vzi	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
w	O
yir	O
n	O
w	O
rkv	O
zi	O
i	O
bkv	O
niv	O
li	O
n	O
figure	O
two	O
lvms	O
for	O
discrete	B
data	O
circles	O
are	O
scalar	O
nodes	B
ellipses	O
are	O
vector	O
nodes	B
squares	O
are	O
matrix	O
nodes	B
categorical	B
pca	B
multinomial	B
pca	B
lda	B
and	O
mpca	B
in	O
epca	B
the	O
quantity	O
wzi	O
represents	O
the	O
natural	B
parameters	I
of	O
the	O
exponential	B
family	B
sometimes	O
it	O
is	O
more	O
convenient	O
to	O
use	O
the	O
dual	O
parameters	O
for	O
example	O
for	O
the	O
multinomial	B
the	O
dual	O
parameter	B
is	O
the	O
probability	O
vector	O
whereas	O
the	O
natural	O
parameter	B
is	O
the	O
vector	O
of	O
log	O
odds	O
if	O
we	O
want	O
to	O
use	O
the	O
dual	O
parameters	O
we	O
need	O
to	O
constrain	O
the	O
latent	B
variables	O
so	O
they	O
live	O
in	O
the	O
appropriate	O
parameter	B
space	O
in	O
the	O
case	O
of	O
categorical	B
data	O
we	O
will	O
need	O
to	O
ensure	O
the	O
latent	B
vector	O
lives	O
in	O
sk	O
the	O
k-dimensional	O
probability	B
simplex	I
to	O
avoid	O
confusion	O
with	O
epca	B
we	O
will	O
denote	O
such	O
a	O
latent	B
vector	O
by	O
i	O
in	O
this	O
case	O
the	O
natural	O
prior	O
for	O
the	O
latent	B
variables	O
is	O
the	O
dirichlet	B
i	O
dir	O
typically	O
we	O
set	O
if	O
we	O
set	O
we	O
encourage	O
i	O
to	O
be	O
sparse	B
as	O
shown	O
in	O
figure	O
when	O
we	O
have	O
a	O
count	O
vector	O
whose	O
total	O
sum	O
is	O
known	O
the	O
likelihood	B
is	O
given	O
by	O
pnili	O
i	O
munili	O
b	O
i	O
this	O
model	O
is	O
called	O
multinomial	B
pca	B
or	O
mpca	B
buntine	O
and	O
jakulin	O
see	O
figure	O
since	O
we	O
are	O
assuming	O
niv	O
k	O
bvk	O
iv	O
this	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
matrix	B
factorization	I
for	O
the	O
count	O
matrix	O
note	O
that	O
we	O
use	O
bvk	O
to	O
denote	O
the	O
parameter	B
vector	O
rather	O
than	O
wvk	O
since	O
we	O
impose	O
the	O
constraints	O
that	O
bvk	O
and	O
v	O
bvk	O
the	O
corresponding	O
marginal	B
distribution	I
has	O
the	O
form	O
pnili	O
munili	O
b	O
idir	O
i	O
i	O
unfortunately	O
this	O
integral	O
cannot	O
be	O
computed	O
analytically	O
if	O
we	O
have	O
a	O
variable	O
length	O
sequence	O
known	O
length	O
we	O
can	O
use	O
i	O
catyilb	O
i	O
distributed	O
state	B
lvms	O
for	O
discrete	B
data	O
this	O
is	O
called	O
latent	B
dirichlet	B
allocation	I
or	O
lda	B
et	O
al	O
and	O
will	O
be	O
described	O
in	O
much	O
greater	O
detail	O
below	O
lda	B
can	O
be	O
thought	O
of	O
as	O
a	O
probabilistic	O
extension	B
of	O
lsa	B
where	O
the	O
latent	B
quantities	O
ik	O
are	O
non-negative	O
and	O
sum	O
to	O
one	O
by	O
contrast	O
in	O
lsa	B
zik	O
can	O
be	O
negative	O
which	O
makes	O
interpetation	O
difficult	O
a	O
predecessor	O
to	O
lda	B
known	O
as	O
probabilistic	B
latent	B
semantic	I
indexing	I
or	O
plsi	B
uses	O
the	O
same	O
model	O
but	O
computes	O
a	O
point	B
estimate	I
of	O
i	O
for	O
each	O
document	O
to	O
epca	B
rather	O
than	O
integrating	O
it	O
out	O
thus	O
in	O
plsi	B
there	O
is	O
no	O
prior	O
for	O
i	O
we	O
can	O
modify	O
lda	B
to	O
handle	O
a	O
fixed	O
number	O
of	O
different	O
categorical	B
responses	O
as	O
follows	O
i	O
catyilbr	O
i	O
this	O
has	O
been	O
called	O
the	O
user	O
rating	O
profile	O
model	O
and	O
the	O
simplex	B
factor	B
model	I
and	O
dunson	O
gap	B
model	O
and	O
non-negative	B
matrix	B
factorization	I
now	O
consider	O
modeling	O
count	O
vectors	O
where	O
we	O
do	O
not	O
constrain	O
the	O
sum	O
to	O
be	O
observed	O
this	O
case	O
the	O
latent	B
variables	O
just	O
need	O
to	O
be	O
non-negative	O
so	O
we	O
will	O
denote	O
them	O
by	O
z	O
can	O
be	O
ensured	O
by	O
using	O
a	O
prior	O
of	O
the	O
form	O
in	O
i	O
this	O
this	O
is	O
called	O
the	O
gap	B
model	O
see	O
figure	O
in	O
and	O
jakulin	O
it	O
is	O
shown	O
that	O
the	O
gap	B
model	O
when	O
conditioned	O
on	O
a	O
fixed	O
li	O
reduces	O
to	O
the	O
mpca	B
model	O
this	O
follows	O
since	O
a	O
set	O
of	O
poisson	B
random	O
variables	O
when	O
conditioned	O
on	O
their	O
sum	O
becomes	O
a	O
multinomial	B
distribution	O
e	O
g	O
if	O
we	O
set	O
k	O
k	O
in	O
the	O
gap	B
model	O
we	O
recover	O
a	O
method	O
known	O
as	O
non-negative	B
matrix	B
factorization	I
or	O
nmf	B
and	O
seung	O
as	O
shown	O
in	O
and	O
jakulin	O
nmf	B
is	O
not	O
a	O
probabilistic	O
generative	O
model	O
since	O
it	O
does	O
not	O
specify	O
a	O
proper	O
prior	O
for	O
z	O
i	O
furthermore	O
the	O
algorithm	O
proposed	O
in	O
and	O
seung	O
is	O
another	O
degenerate	B
em	B
algorithm	O
so	O
suffers	O
from	O
overfitting	O
some	O
procedures	O
to	O
fit	O
the	O
gap	B
model	O
which	O
overcome	O
these	O
problems	O
are	O
given	O
in	O
and	O
jakulin	O
to	O
encourage	O
z	O
i	O
to	O
be	O
sparse	B
we	O
can	O
modify	O
the	O
prior	O
to	O
be	O
a	O
spike-and-gamma	O
type	O
prior	O
as	O
follows	O
pz	O
ik	O
kiz	O
ik	O
kgaz	O
ik	O
k	O
k	O
where	O
k	O
is	O
the	O
probability	O
of	O
the	O
spike	O
at	O
this	O
is	O
called	O
the	O
conditional	B
gamma	B
poisson	B
model	O
and	O
jakulin	O
it	O
is	O
simple	O
to	O
modify	O
gibbs	B
sampling	I
to	O
handle	O
this	O
kind	O
of	O
prior	O
although	O
we	O
will	O
not	O
go	O
into	O
detail	O
here	O
pz	O
i	O
gaz	O
ik	O
k	O
k	O
the	O
likelihood	B
is	O
given	O
by	O
pniz	O
i	O
poinivbt	O
vz	O
i	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
k	O
k	O
z	O
ik	O
niv	O
n	O
z	O
b	O
i	O
qil	O
yil	O
b	O
li	O
n	O
figure	O
gaussian-poisson	O
model	O
latent	B
dirichlet	B
allocation	I
model	O
latent	B
dirichlet	B
allocation	I
in	O
this	O
section	O
we	O
explain	O
the	O
latent	B
dirichlet	B
allocation	I
or	O
lda	B
et	O
al	O
model	O
in	O
detail	O
basics	O
in	O
a	O
mixture	B
of	O
multinoullis	O
every	O
document	O
is	O
assigned	O
to	O
a	O
single	O
topic	B
qi	O
k	O
in	O
lda	B
every	O
word	O
is	O
assigned	O
to	O
its	O
own	O
topic	B
qil	O
drawn	O
from	O
a	O
global	O
distribution	O
k	O
drawn	O
from	O
a	O
document-specific	O
distribution	O
i	O
since	O
a	O
document	O
belongs	O
to	O
a	O
distribution	O
over	O
topics	O
rather	O
than	O
a	O
single	O
topic	B
the	O
model	O
is	O
called	O
an	O
admixture	B
mixture	B
or	O
mixed	B
membership	I
model	I
et	O
al	O
this	O
model	O
has	O
many	O
other	O
applications	O
beyond	O
text	O
analysis	O
e	O
g	O
genetics	O
et	O
al	O
health	O
science	O
et	O
al	O
social	O
network	O
analysis	O
et	O
al	O
etc	O
adding	O
conjugate	B
priors	I
to	O
the	O
parameters	O
the	O
full	B
model	O
is	O
as	O
i	O
dir	O
qil	O
i	O
cat	O
i	O
bk	O
dir	O
yilqil	O
k	O
b	O
catbk	O
this	O
is	O
illustrated	O
in	O
figure	O
we	O
can	O
marginalize	O
out	O
the	O
qi	O
variables	O
thereby	O
creating	O
a	O
our	O
notation	O
is	O
similar	B
to	O
the	O
one	O
we	O
use	O
elsewhere	O
in	O
this	O
book	O
but	O
is	O
different	O
from	O
that	O
used	O
by	O
most	O
lda	B
papers	O
they	O
typically	O
use	O
wnd	O
for	O
the	O
identity	O
of	O
word	O
n	O
in	O
document	O
d	O
znd	O
to	O
represent	O
the	O
discrete	B
indicator	O
d	O
as	O
the	O
continuous	O
latent	B
vector	O
for	O
document	O
d	O
and	O
k	O
as	O
the	O
k	O
th	O
topic	B
vector	O
latent	B
dirichlet	B
allocation	I
topic	B
observed	O
document	O
generated	O
document	O
figure	O
geometric	O
interpretation	O
of	O
lda	B
we	O
have	O
k	O
topics	O
and	O
v	O
words	O
each	O
document	O
dots	O
and	O
each	O
topic	B
dots	O
is	O
a	O
point	O
in	O
the	O
simplex	O
source	O
figure	O
of	O
and	O
griffiths	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
direct	O
arc	O
from	O
i	O
to	O
yil	O
with	O
the	O
following	O
cpd	B
pyil	O
v	O
i	O
pyil	O
vqil	O
kpqil	O
k	O
k	O
k	O
ikbkv	O
as	O
we	O
mentioned	O
in	O
the	O
introduction	O
this	O
is	O
very	O
similar	B
to	O
the	O
multinomial	B
pca	B
model	O
proposed	O
in	O
which	O
in	O
turn	O
is	O
closely	O
related	O
to	O
categorical	B
pca	B
gap	B
nmf	B
etc	O
lda	B
has	O
an	O
interesting	O
geometric	O
interpretation	O
each	O
vector	O
bk	O
defines	O
a	O
distribution	O
over	O
v	O
words	O
each	O
k	O
is	O
known	O
as	O
a	O
topic	B
each	O
document	O
vector	O
i	O
defines	O
a	O
distribution	O
over	O
k	O
topics	O
so	O
we	O
model	O
each	O
document	O
as	O
an	O
admixture	O
over	O
topics	O
equivalently	O
we	O
can	O
think	O
of	O
lda	B
as	O
a	O
form	O
of	O
dimensionality	B
reduction	I
k	O
v	O
as	O
is	O
usually	O
the	O
case	O
where	O
we	O
project	O
a	O
point	O
in	O
the	O
v	O
simplex	O
normalized	O
document	O
count	O
vector	O
xi	O
onto	O
the	O
k-dimensional	O
simplex	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
have	O
v	O
words	O
and	O
k	O
topics	O
the	O
observed	O
documents	O
live	O
in	O
the	O
simplex	O
are	O
approximated	O
as	O
living	O
on	O
a	O
simplex	O
spanned	O
by	O
the	O
topic	B
vectors	O
each	O
of	O
which	O
lives	O
in	O
the	O
simplex	O
one	O
advantage	O
of	O
using	O
the	O
simplex	O
as	O
our	O
latent	B
space	O
rather	O
than	O
euclidean	O
space	O
is	O
that	O
the	O
simplex	O
can	O
handle	O
ambiguity	O
this	O
is	O
importance	O
since	O
in	O
natural	O
language	O
words	O
can	O
often	O
have	O
multiple	O
meanings	O
a	O
phenomomen	O
known	O
as	O
polysemy	B
for	O
example	O
play	O
might	O
refer	O
to	O
a	O
verb	O
to	O
play	O
ball	O
or	O
to	O
play	O
the	O
coronet	O
or	O
to	O
a	O
noun	O
shakespeare	O
s	O
play	O
in	O
lda	B
we	O
can	O
have	O
multiple	O
topics	O
each	O
of	O
which	O
can	O
generate	O
the	O
word	O
play	O
as	O
shown	O
in	O
figure	O
reflecting	O
this	O
ambiguity	O
given	O
word	O
l	O
in	O
document	O
i	O
we	O
can	O
compute	O
pqil	O
kyi	O
and	O
thus	O
infer	O
its	O
most	O
likely	O
topic	B
by	O
looking	O
at	O
the	O
word	O
in	O
isolation	O
it	O
might	O
be	O
hard	O
to	O
know	O
what	O
sense	O
of	O
the	O
word	O
is	O
meant	O
but	O
we	O
can	O
disambiguate	O
this	O
by	O
looking	O
at	O
other	O
words	O
in	O
the	O
document	O
in	O
particular	O
given	O
xi	O
we	O
can	O
infer	O
the	O
topic	B
distribution	O
i	O
for	O
the	O
document	O
this	O
acts	O
as	O
a	O
prior	O
for	O
disambiguating	O
qil	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
we	O
show	O
three	O
documents	O
from	O
the	O
tasa	B
in	O
the	O
first	O
document	O
there	O
are	O
a	O
variety	O
of	O
music	O
related	O
words	O
which	O
suggest	O
the	O
tasa	B
corpus	B
is	O
a	O
collection	O
of	O
high-school	O
level	O
english	O
documents	O
comprising	O
over	O
million	O
words	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
topic	B
topic	B
topic	B
word	O
prob	O
music	O
dance	O
song	O
play	O
sing	O
singing	O
band	O
played	O
sang	O
songs	O
dancing	O
piano	O
playing	O
rhythm	O
albert	O
musical	O
word	O
literature	O
prob	O
poem	O
poetry	O
poet	O
plays	O
poems	O
play	O
literary	O
writers	O
drama	O
wrote	O
poets	O
writer	O
written	O
stage	O
shakespeare	O
hit	O
word	O
prob	O
play	O
ball	O
game	O
playing	O
played	O
baseball	O
games	O
bat	O
run	O
throw	O
balls	O
tennis	O
home	O
catch	O
field	O
figure	O
three	O
topics	O
related	O
to	O
the	O
word	O
play	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
source	O
figure	O
of	O
and	O
griffiths	O
document	O
bix	O
beiderbecke	O
at	O
on	O
the	O
of	O
a	O
the	O
he	O
was	O
to	O
from	O
a	O
riverboat	O
the	O
had	O
already	O
his	O
as	O
well	O
as	O
his	O
it	O
was	O
bix	O
beiderbecke	O
had	O
already	O
had	O
he	O
on	O
the	O
and	O
his	O
he	O
might	O
becoming	O
a	O
but	O
bix	O
was	O
in	O
another	O
of	O
he	O
to	O
the	O
cornet	O
and	O
he	O
to	O
document	O
there	O
is	O
a	O
why	O
there	O
are	O
so	O
few	O
of	O
really	O
great	O
in	O
our	O
whole	O
world	O
too	O
many	O
have	O
to	O
come	O
right	O
at	O
the	O
very	O
same	O
time	O
the	O
dramatists	O
must	O
have	O
the	O
right	O
the	O
must	O
have	O
the	O
right	O
playhouses	O
the	O
playhouses	O
must	O
have	O
the	O
right	O
we	O
must	O
that	O
to	O
be	O
not	O
to	O
be	O
even	O
when	O
you	O
a	O
to	O
yourself	O
to	O
it	O
to	O
it	O
on	O
a	O
as	O
you	O
go	O
along	O
as	O
as	O
a	O
has	O
to	O
be	O
then	O
some	O
of	O
document	O
has	O
a	O
the	O
a	O
for	O
one	O
the	O
the	O
for	O
one	O
the	O
into	O
the	O
and	O
the	O
the	O
see	O
a	O
for	O
two	O
the	O
two	O
the	O
the	O
the	O
for	O
two	O
the	O
like	O
the	O
into	O
the	O
and	O
and	O
the	O
they	O
see	O
a	O
for	O
three	O
and	O
and	O
the	O
they	O
figure	O
three	O
documents	O
from	O
the	O
tasa	B
corpus	B
containing	O
different	O
senses	O
of	O
the	O
word	O
play	O
grayed	O
out	O
words	O
were	O
ignored	O
by	O
the	O
model	O
because	O
they	O
correspond	O
to	O
uninteresting	O
stop	B
words	I
as	O
and	O
the	O
etc	O
or	O
very	O
low	O
frequency	O
words	O
source	O
figure	O
of	O
and	O
griffiths	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
latent	B
dirichlet	B
allocation	I
i	O
will	O
put	O
most	O
of	O
its	O
mass	O
on	O
the	O
music	O
topic	B
this	O
in	O
turn	O
makes	O
the	O
music	O
interpretation	O
of	O
play	O
the	O
most	O
likely	O
as	O
shown	O
by	O
the	O
superscript	O
the	O
second	O
document	O
interprets	O
play	O
in	O
the	O
theatrical	O
sense	O
and	O
the	O
third	O
in	O
the	O
sports	O
sense	O
note	O
that	O
is	O
crucial	O
that	O
i	O
be	O
a	O
latent	B
variable	O
so	O
information	B
can	O
flow	O
between	O
the	O
qil	O
s	O
thus	O
enabling	O
local	O
disambiguation	O
to	O
use	O
the	O
full	B
set	O
of	O
words	O
unsupervised	O
discovery	O
of	O
topics	O
one	O
of	O
the	O
main	O
purposes	O
of	O
lda	B
is	O
discover	O
topics	O
in	O
a	O
large	O
collection	O
or	O
corpus	B
of	O
documents	O
figure	O
for	O
an	O
example	O
unfortunately	O
since	O
the	O
model	O
is	O
unidentifiable	O
the	O
interpertation	O
of	O
the	O
topics	O
can	O
be	O
difficult	O
et	O
al	O
one	O
approach	O
known	O
as	O
labeled	B
lda	B
et	O
al	O
exploits	O
the	O
existence	O
of	O
tags	O
on	O
documents	O
as	O
a	O
way	O
to	O
ensure	O
identifiability	O
in	O
particular	O
it	O
forces	O
the	O
topics	O
to	O
correspond	O
to	O
the	O
tags	O
and	O
then	O
it	O
learns	O
a	O
distribution	O
over	O
words	O
for	O
each	O
tag	O
this	O
can	O
make	O
the	O
results	O
easier	O
to	O
interpret	O
quantitatively	O
evaluating	O
lda	B
as	O
a	O
language	B
model	I
in	O
order	O
to	O
evaluate	O
lda	B
quantitatively	O
we	O
can	O
treat	O
it	O
as	O
a	O
language	B
model	I
i	O
e	O
a	O
probability	O
distribution	O
over	O
sequences	O
of	O
words	O
of	O
course	O
it	O
is	O
not	O
a	O
very	O
good	O
language	B
model	I
since	O
it	O
ignores	O
word	O
order	O
and	O
just	O
looks	O
at	O
single	O
words	O
but	O
it	O
is	O
interesting	O
to	O
compare	O
lda	B
to	O
other	O
unigram-based	O
models	O
such	O
as	O
mixtures	O
of	O
multinoullis	O
and	O
plsi	B
such	O
simple	O
language	B
models	I
are	O
sometimes	O
useful	O
for	O
information	B
retrieval	I
purposes	O
the	O
standard	O
way	O
to	O
measure	O
the	O
quality	O
of	O
a	O
language	B
model	I
is	O
to	O
use	O
perplexity	B
which	O
we	O
now	O
define	O
below	O
perplexity	B
the	O
perplexity	B
of	O
language	B
model	I
q	O
given	O
a	O
stochastic	O
p	O
is	O
defined	O
as	O
perplexityp	O
q	O
hp	O
q	O
lim	O
n	O
n	O
where	O
hp	O
q	O
is	O
the	O
cross-entropy	B
of	O
the	O
two	O
stochastic	B
processes	I
defined	O
as	O
log	O
the	O
cross	B
entropy	B
hence	O
perplexity	B
is	O
minimized	O
if	O
q	O
p	O
in	O
this	O
case	O
the	O
model	O
can	O
predict	O
as	O
well	O
as	O
the	O
true	O
distribution	O
we	O
can	O
approximate	O
the	O
stochastic	B
process	I
by	O
using	O
a	O
single	O
long	O
test	O
sequence	O
of	O
multiple	O
documents	O
and	O
multiple	O
sentences	O
complete	B
with	O
end-of-sentence	O
markers	O
call	O
it	O
y	O
approximation	O
becomes	O
more	O
and	O
more	O
accurate	O
as	O
the	O
sequence	O
gets	O
longer	O
provided	O
the	O
process	O
is	O
stationary	B
and	O
ergodic	B
and	O
thomas	O
define	O
the	O
empirical	B
distribution	I
approximation	O
to	O
the	O
stochastic	B
process	I
as	O
y	O
collated	O
by	O
a	O
company	O
formerly	O
known	O
as	O
touchstone	O
applied	O
science	O
associates	O
but	O
now	O
known	O
as	O
questar	O
assessment	O
inc	O
www	O
questarai	O
com	O
a	O
stochastic	B
process	I
is	O
one	O
which	O
can	O
define	O
a	O
joint	B
distribution	I
over	O
an	O
arbitrary	O
number	O
of	O
random	O
variables	O
we	O
can	O
think	O
of	O
natural	O
language	O
as	O
a	O
stochastic	B
process	I
since	O
it	O
can	O
generate	O
an	O
infinite	O
stream	O
of	O
words	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
in	O
this	O
case	O
the	O
cross-entropy	B
becomes	O
hpemp	O
q	O
n	O
log	O
qy	O
and	O
the	O
perplexity	B
becomes	O
perplexitypemp	O
q	O
qy	O
n	O
i	O
qy	O
we	O
see	O
that	O
this	O
is	O
the	O
geometric	O
mean	B
of	O
the	O
inverse	O
predictive	B
probabilities	O
which	O
is	O
the	O
usual	O
definition	O
of	O
perplexity	B
and	O
martin	O
in	O
the	O
case	O
of	O
unigram	O
models	O
the	O
cross	B
entropy	B
term	O
is	O
given	O
by	O
log	O
qyil	O
li	O
h	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
documents	O
and	O
li	O
is	O
the	O
number	O
of	O
words	O
in	O
document	O
i	O
hence	O
the	O
perplexity	B
of	O
model	O
q	O
is	O
given	O
by	O
perplexitypemp	O
p	O
exp	O
log	O
qyil	O
li	O
n	O
intuitively	O
perplexity	B
mesures	O
the	O
weighted	B
average	I
branching	B
factor	B
of	O
the	O
model	O
s	O
predictive	B
distribution	O
suppose	O
the	O
model	O
predicts	O
that	O
each	O
symbol	O
word	O
whatever	O
is	O
equally	O
likely	O
so	O
then	O
the	O
perplexity	B
is	O
k	O
if	O
some	O
symbols	O
are	O
more	O
likely	O
than	O
others	O
and	O
the	O
model	O
correctly	O
reflects	O
this	O
its	O
perplexity	B
will	O
be	O
lower	O
than	O
k	O
of	O
course	O
hp	O
p	O
hp	O
q	O
so	O
we	O
can	O
never	O
reduce	O
the	O
perplexity	B
below	O
the	O
entropy	B
of	O
the	O
underlying	O
stochastic	B
process	I
perplexity	B
of	O
lda	B
the	O
key	O
quantity	O
is	O
pv	O
the	O
predictive	B
distribution	O
of	O
the	O
model	O
over	O
possible	O
words	O
is	O
implicitly	O
conditioned	O
on	O
the	O
training	B
set	I
for	O
lda	B
this	O
can	O
be	O
approximated	O
by	O
plugging	O
in	O
b	O
the	O
posterior	B
mean	B
estimate	O
and	O
approximately	O
integrating	O
out	O
q	O
using	O
mean	B
field	O
inference	B
et	O
al	O
for	O
a	O
more	O
accurate	O
way	O
to	O
approximate	O
the	O
predictive	B
likelihood	B
in	O
figure	O
we	O
compare	O
lda	B
to	O
several	O
other	O
simple	O
unigram	O
models	O
namely	O
map	O
estimation	O
of	O
a	O
multinoulli	O
map	O
estimation	O
of	O
a	O
mixture	B
of	O
multinoullis	O
and	O
plsi	B
performing	O
map	O
estimation	O
the	O
same	O
dirichlet	B
prior	O
on	O
b	O
was	O
used	O
as	O
in	O
the	O
lda	B
model	O
the	O
metric	B
is	O
perplexity	B
as	O
in	O
equation	O
and	O
the	O
data	O
is	O
a	O
subset	O
of	O
the	O
trec	O
ap	O
corpus	B
containing	O
newswire	O
articles	O
with	O
unique	O
terms	O
we	O
see	O
that	O
lda	B
significantly	O
outperforms	O
these	O
other	O
methods	O
latent	B
dirichlet	B
allocation	I
unigram	O
mixtures	O
of	O
unigrams	B
lda	B
fold	O
in	O
plsi	B
l	O
y	O
t	O
i	O
x	O
e	O
p	O
r	O
e	O
p	O
number	O
of	O
topics	O
figure	O
perplexity	B
vs	O
number	O
of	O
topics	O
on	O
the	O
trec	O
ap	O
corpus	B
for	O
various	O
language	B
models	I
based	O
on	O
figure	O
of	O
et	O
al	O
figure	O
generated	O
by	O
bleildaperplexityplot	O
n	O
qnln	O
ynln	O
qnln	O
ynln	O
k	O
figure	O
the	O
bk	O
lda	B
unrolled	B
for	O
n	O
documents	O
collapsed	O
lda	B
where	O
we	O
integrate	B
out	I
the	O
i	O
and	O
fitting	O
using	O
gibbs	B
sampling	I
it	O
is	O
straightforward	O
to	O
derive	O
a	O
gibbs	B
sampling	I
algorithm	O
for	O
lda	B
the	O
full	B
conditionals	O
are	O
as	O
follows	O
pqil	O
k	O
explog	O
ik	O
log	O
bkxil	O
l	O
p	O
i	O
dir	O
k	O
pbk	O
dir	O
v	O
izil	O
k	O
ixil	O
v	O
zil	O
k	O
however	O
one	O
can	O
get	O
better	O
performance	O
by	O
analytically	O
integrating	O
out	O
the	O
i	O
s	O
and	O
the	O
bk	O
s	O
i	O
l	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
both	O
of	O
which	O
have	O
a	O
dirichlet	B
distribution	I
and	O
just	O
sampling	O
the	O
discrete	B
qil	O
s	O
this	O
approach	O
was	O
first	O
suggested	O
in	O
and	O
steyvers	O
and	O
is	O
an	O
example	O
of	O
collapsed	B
gibbs	B
sampling	I
figure	O
shows	O
that	O
now	O
all	O
the	O
qil	O
variables	O
are	O
fully	O
correlated	O
however	O
we	O
can	O
sample	O
them	O
one	O
at	O
a	O
time	O
as	O
we	O
explain	O
below	O
first	O
we	O
need	O
some	O
notation	O
let	O
civk	O
word	O
v	O
is	O
assigned	O
to	O
topic	B
k	O
in	O
document	O
i	O
let	O
cik	O
word	O
from	O
document	O
i	O
has	O
been	O
assigned	O
to	O
topic	B
k	O
let	O
cvk	O
word	O
v	O
has	O
been	O
assigned	O
to	O
topic	B
k	O
in	O
any	O
document	O
let	O
niv	O
times	O
word	O
v	O
occurs	O
in	O
document	O
i	O
this	O
is	O
observed	O
let	O
ck	O
assigned	O
to	O
topic	B
k	O
finally	O
let	O
li	O
observed	O
iqil	O
k	O
yil	O
v	O
be	O
the	O
number	O
of	O
times	O
v	O
civk	O
be	O
the	O
number	O
of	O
times	O
any	O
i	O
civk	O
be	O
the	O
number	O
of	O
times	O
k	O
civk	O
be	O
the	O
number	O
of	O
v	O
cvk	O
be	O
the	O
number	O
of	O
words	O
k	O
cik	O
be	O
the	O
number	O
of	O
words	O
in	O
document	O
i	O
this	O
is	O
we	O
can	O
now	O
derive	O
the	O
marginal	O
prior	O
by	O
applying	O
equation	O
one	O
can	O
show	O
that	O
pq	O
dir	O
i	O
i	O
i	O
catqil	O
i	O
ilqilk	O
k	O
catyilbk	O
v	O
pyq	O
k	O
by	O
similar	B
reasoning	O
one	O
can	O
show	O
dirbk	O
from	O
the	O
above	O
equations	O
and	O
using	O
the	O
fact	O
that	O
x	O
we	O
can	O
derive	O
the	O
full	B
conditional	I
for	O
pqilq	O
il	O
define	O
c	O
ivk	O
to	O
be	O
the	O
same	O
as	O
civk	O
except	O
it	O
is	O
compute	O
by	O
summing	O
over	O
all	O
locations	O
in	O
document	O
i	O
except	O
for	O
qil	O
also	O
let	O
yil	O
v	O
then	O
pqil	O
kq	O
il	O
y	O
c	O
vk	O
c	O
k	O
v	O
c	O
ik	O
li	O
k	O
we	O
see	O
that	O
a	O
word	O
in	O
a	O
document	O
is	O
assigned	O
to	O
a	O
topic	B
based	O
both	O
on	O
how	O
often	O
that	O
word	O
is	O
generated	O
by	O
the	O
topic	B
term	O
and	O
also	O
on	O
how	O
often	O
that	O
topic	B
is	O
used	O
in	O
that	O
document	O
term	O
given	O
equation	O
we	O
can	O
implement	O
the	O
collapsed	B
gibbs	B
sampler	I
as	O
follows	O
we	O
randomly	O
assign	O
a	O
topic	B
to	O
each	O
word	O
qil	O
k	O
we	O
can	O
then	O
sample	O
a	O
new	O
topic	B
as	O
follows	O
for	O
a	O
given	O
word	O
in	O
the	O
corpus	B
decrement	O
the	O
relevant	O
counts	O
based	O
on	O
the	O
topic	B
assigned	O
to	O
the	O
current	O
word	O
draw	O
a	O
new	O
topic	B
from	O
equation	O
update	O
the	O
count	O
matrices	O
and	O
repeat	O
this	O
algorithm	O
can	O
be	O
made	O
efficient	O
since	O
the	O
count	O
matrices	O
are	O
very	O
sparse	B
example	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
on	O
a	O
small	O
example	O
with	O
two	O
topics	O
and	O
five	O
words	O
the	O
left	O
part	O
of	O
the	O
figure	O
illustrates	O
documents	O
that	O
were	O
sampled	O
from	O
the	O
lda	B
model	O
using	O
latent	B
dirichlet	B
allocation	I
river	O
stream	O
bank	O
money	O
loan	O
river	O
stream	O
bank	O
money	O
loan	O
figure	O
illustration	O
of	O
gibbs	B
sampling	I
applied	O
to	O
a	O
small	O
lda	B
example	O
there	O
are	O
n	O
documents	O
each	O
containing	O
a	O
variable	O
number	O
of	O
words	O
drawn	O
from	O
a	O
vocabulary	O
of	O
v	O
words	O
there	O
are	O
two	O
topics	O
a	O
white	O
dot	O
means	O
word	O
the	O
word	O
is	O
assigned	O
to	O
topic	B
a	O
black	O
dot	O
means	O
the	O
word	O
is	O
assigned	O
to	O
topic	B
a	O
sample	O
from	O
the	O
posterior	O
after	O
steps	O
of	O
gibbs	B
sampling	I
source	O
figure	O
of	O
and	O
griffiths	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
the	O
initial	O
random	O
assignment	O
of	O
states	O
pmoneyk	O
k	O
k	O
and	O
priverk	O
k	O
pbankk	O
for	O
example	O
we	O
see	O
that	O
the	O
first	O
document	O
contains	O
the	O
word	O
bank	O
times	O
by	O
the	O
four	O
dots	O
in	O
row	O
of	O
the	O
bank	O
column	O
as	O
well	O
as	O
various	O
other	O
financial	O
terms	O
the	O
right	O
part	O
of	O
the	O
figure	O
shows	O
the	O
state	B
of	O
the	O
gibbs	B
sampler	I
after	O
iterations	O
the	O
correct	O
topic	B
has	O
been	O
assigned	O
to	O
each	O
token	O
in	O
most	O
cases	O
for	O
example	O
in	O
document	O
we	O
see	O
that	O
the	O
word	O
bank	O
has	O
been	O
correctly	O
assigned	O
to	O
the	O
financial	O
topic	B
based	O
on	O
the	O
presence	O
of	O
the	O
words	O
money	O
and	O
loan	O
the	O
posterior	B
mean	B
estimate	O
of	O
the	O
parameters	O
is	O
given	O
by	O
pmoneyk	O
ploank	O
pbankk	O
priverk	O
pstreamk	O
and	O
pbankk	O
which	O
is	O
impressively	O
accurate	O
given	O
that	O
there	O
are	O
only	O
training	O
examples	O
fitting	O
using	O
batch	B
variational	B
inference	B
a	O
faster	O
alternative	O
to	O
mcmc	B
is	O
to	O
use	O
variational	B
em	B
cannot	O
use	O
exact	O
em	B
since	O
exact	O
inference	B
of	O
i	O
and	O
qi	O
is	O
intractable	O
we	O
give	O
the	O
details	O
below	O
sequence	O
version	O
l	O
following	O
et	O
al	O
we	O
will	O
use	O
a	O
fully	O
factorized	O
field	O
approximation	O
of	O
the	O
form	O
q	O
i	O
qi	O
dir	O
i	O
i	O
catqil	O
qil	O
we	O
will	O
follow	O
the	O
usual	O
mean	B
field	O
recipe	O
for	O
qqil	O
we	O
use	O
bayes	B
rule	I
but	O
where	O
we	O
need	O
to	O
take	O
expectations	O
over	O
the	O
prior	O
qilk	O
byilk	O
expe	O
ik	O
where	O
e	O
ik	O
k	O
i	O
ik	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
where	O
is	O
the	O
digamma	B
function	O
the	O
update	O
for	O
q	O
i	O
is	O
obtained	O
by	O
adding	O
up	O
the	O
expected	O
counts	O
the	O
m	B
step	I
is	O
obtained	O
by	O
adding	O
up	O
the	O
expected	O
counts	O
and	O
normalizing	O
ik	O
k	O
qilk	O
l	O
bvk	O
v	O
qilkiyil	O
v	O
count	O
version	O
i	O
liv	O
k	O
space	O
to	O
store	O
the	O
qilk	O
note	O
that	O
the	O
e	B
step	I
takes	O
o	O
it	O
is	O
much	O
more	O
space	O
efficient	O
to	O
perform	O
inference	B
in	O
the	O
mpca	B
version	O
of	O
the	O
model	O
which	O
works	O
with	O
counts	O
these	O
only	O
take	O
on	O
v	O
k	O
space	O
which	O
is	O
a	O
big	O
savings	O
if	O
documents	O
are	O
long	O
contrast	O
the	O
collapsed	B
gibbs	B
sampler	I
must	O
work	O
explicitly	O
with	O
the	O
qil	O
variables	O
we	O
will	O
focus	O
on	O
approximating	O
p	O
i	O
cini	O
li	O
where	O
we	O
write	O
ci	B
as	O
shorthand	O
for	O
ci	B
we	O
will	O
again	O
use	O
a	O
fully	O
factorized	O
field	O
approximation	O
of	O
the	O
form	O
v	O
muciv	O
niv	O
civ	O
q	O
i	O
ci	B
dir	O
i	O
i	O
the	O
new	O
e	B
step	I
becomes	O
ik	O
k	O
civk	O
bvk	O
expe	O
ik	O
niv	O
civk	O
v	O
the	O
new	O
m	B
step	I
becomes	O
bvk	O
v	O
niv	O
civk	O
i	O
vb	B
version	O
we	O
now	O
modify	O
the	O
algorithm	O
to	O
use	O
vb	B
instead	O
of	O
em	B
so	O
that	O
we	O
infer	O
the	O
parameters	O
as	O
well	O
as	O
the	O
latent	B
variables	O
there	O
are	O
two	O
advantages	O
to	O
this	O
first	O
by	O
setting	O
vb	B
will	O
encourage	O
b	O
to	O
be	O
sparse	B
in	O
section	O
second	O
we	O
will	O
be	O
able	O
to	O
generalize	B
this	O
to	O
the	O
online	B
learning	B
setting	O
as	O
we	O
discuss	O
below	O
our	O
new	O
posterior	O
approximation	O
becomes	O
q	O
i	O
ci	B
b	O
dir	O
i	O
i	O
muciv	O
niv	O
civ	O
v	O
k	O
the	O
update	O
for	O
civk	O
changes	O
to	O
the	O
following	O
civk	O
exp	O
bvk	O
ik	O
dirb	O
k	O
b	O
k	O
latent	B
dirichlet	B
allocation	I
algorithm	O
batch	B
vb	B
for	O
lda	B
input	O
niv	O
k	O
k	O
v	O
estimate	O
bvk	O
using	O
em	B
for	O
multinomial	B
mixtures	O
initialize	O
counts	O
niv	O
while	O
not	O
converged	O
do	O
e	B
step	I
svk	O
expected	B
sufficient	B
statistics	I
for	O
each	O
document	O
i	O
do	O
i	O
ci	B
estepni	O
b	O
svk	O
niv	O
civk	O
m	B
step	I
for	O
each	O
topic	B
k	O
do	O
bvk	O
v	O
svk	O
function	O
i	O
ci	B
estepni	O
b	O
initialize	O
ik	O
k	O
repeat	O
old	O
i	O
i	O
ik	O
k	O
for	O
each	O
word	O
v	O
do	O
for	O
each	O
topic	B
k	O
do	O
civk	O
exp	O
k	O
bv	O
k	O
old	O
i	O
civ	O
normalize	O
civ	O
ik	O
niv	O
civk	O
k	O
ik	O
old	O
ik	O
thresh	O
until	O
k	O
also	O
the	O
m	B
step	I
becomes	O
bvk	O
v	O
civk	O
i	O
no	O
normalization	O
is	O
required	O
since	O
we	O
are	O
just	O
updating	O
the	O
pseudcounts	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
fitting	O
using	O
online	O
variational	B
inference	B
in	O
the	O
bathc	O
version	O
the	O
e	B
step	I
clearly	O
takes	O
on	O
kv	O
t	O
time	O
where	O
t	O
is	O
the	O
number	O
of	O
mean	B
field	O
updates	O
t	O
this	O
can	O
be	O
slow	O
if	O
we	O
have	O
many	O
documents	O
this	O
can	O
be	O
reduced	O
by	O
using	O
stochastic	B
gradient	B
descent	I
to	O
perform	O
online	O
variational	B
inference	B
as	O
we	O
now	O
explain	O
we	O
can	O
derive	O
an	O
online	O
version	O
following	O
et	O
al	O
we	O
perform	O
an	O
e	B
step	I
in	O
the	O
usual	O
way	O
we	O
then	O
compute	O
the	O
variational	O
parameters	O
for	O
b	O
treating	O
the	O
expected	B
sufficient	B
statistics	I
from	O
the	O
single	O
data	O
case	O
as	O
if	O
the	O
whole	O
data	O
set	O
had	O
those	O
statistics	O
finally	O
we	O
make	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
algorithm	O
online	O
variational	B
bayes	I
for	O
lda	B
input	O
niv	O
k	O
k	O
v	O
initialize	O
bvk	O
randomly	O
for	O
t	O
do	O
set	O
step	B
size	I
t	O
t	O
pick	O
document	O
i	O
it	O
i	O
ci	B
estepni	O
b	O
bnew	O
vk	O
v	O
n	O
niv	O
civk	O
bvk	O
t	O
bvk	O
t	O
bnew	O
vk	O
l	O
y	O
t	O
i	O
x	O
e	O
p	O
r	O
e	O
p	O
online	O
online	O
batch	B
documents	O
seen	O
scale	O
figure	O
test	O
perplexity	B
vs	O
number	O
of	O
training	O
documents	O
for	O
batch	B
and	O
online	O
vb-lda	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
a	O
partial	O
update	O
for	O
the	O
variational	O
parameters	O
for	O
b	O
putting	O
weight	O
t	O
on	O
the	O
new	O
estimate	O
and	O
weight	O
t	O
on	O
the	O
old	O
estimate	O
the	O
step	B
size	I
t	O
decays	O
over	O
time	O
as	O
in	O
equation	O
in	O
practice	O
we	O
should	O
use	O
mini-batches	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
as	O
explained	O
in	O
section	O
in	O
et	O
al	O
they	O
used	O
a	O
batch	B
of	O
size	O
figure	O
plots	O
the	O
perplexity	B
on	O
a	O
test	O
set	O
of	O
size	O
vs	O
number	O
of	O
analyzed	O
documents	O
steps	O
where	O
the	O
data	O
is	O
drawn	O
from	O
wikipedia	O
the	O
figure	O
shows	O
that	O
online	O
variational	B
inference	B
is	O
much	O
faster	O
than	O
offline	B
inference	B
yet	O
produces	O
similar	B
results	O
determining	O
the	O
number	O
of	O
topics	O
choosing	O
k	O
the	O
number	O
of	O
topics	O
is	O
a	O
standard	B
model	B
selection	I
problem	O
here	O
are	O
some	O
approaches	O
that	O
have	O
been	O
taken	O
use	O
annealed	B
importance	B
sampling	I
to	O
approximate	O
the	O
evidence	B
et	O
al	O
cross	B
validation	I
using	O
the	O
log	O
likelihood	B
on	O
a	O
test	O
set	O
extensions	O
of	O
lda	B
use	O
the	O
variational	O
lower	O
bound	O
as	O
a	O
proxy	O
for	O
log	O
pdk	O
use	O
non-parametric	O
bayesian	B
methods	O
et	O
al	O
extensions	O
of	O
lda	B
many	O
extensions	O
of	O
lda	B
have	O
been	O
proposed	O
since	O
the	O
first	O
paper	O
came	O
out	O
in	O
we	O
briefly	O
discuss	O
a	O
few	O
of	O
these	O
below	O
correlated	B
topic	B
model	I
one	O
weakness	O
of	O
lda	B
is	O
that	O
it	O
cannot	O
capture	O
correlation	O
between	O
topics	O
for	O
example	O
if	O
a	O
document	O
has	O
the	O
business	O
topic	B
it	O
is	O
reasonable	O
to	O
expect	O
the	O
finance	O
topic	B
to	O
co-occcur	O
the	O
source	O
of	O
the	O
problem	O
is	O
the	O
use	O
of	O
a	O
dirichlet	B
prior	O
for	O
i	O
the	O
problem	O
with	O
the	O
dirichelt	O
it	O
that	O
it	O
is	O
characterized	O
by	O
just	O
a	O
mean	B
vector	O
and	O
a	O
strength	O
parameter	B
but	O
its	O
covariance	B
is	O
fixed	O
ij	O
i	O
j	O
rather	O
than	O
being	O
a	O
free	O
parameter	B
one	O
way	O
around	O
this	O
is	O
to	O
replace	O
the	O
dirichlet	B
prior	O
with	O
the	O
logistic	B
normal	B
distribution	O
as	O
in	O
categorical	B
pca	B
the	O
model	O
becomes	O
bk	O
dir	O
zi	O
n	O
izi	O
szi	O
qil	O
i	O
cat	O
i	O
yilqil	O
k	O
b	O
catbk	O
this	O
is	O
known	O
as	O
the	O
correlated	B
topic	B
model	I
and	O
lafferty	O
this	O
is	O
very	O
similar	B
to	O
categorical	B
pca	B
but	O
slightly	O
different	O
to	O
see	O
the	O
difference	O
let	O
us	O
marginalize	O
out	O
the	O
qil	O
and	O
i	O
then	O
in	O
the	O
ctm	O
we	O
have	O
yil	O
catbszi	O
where	O
b	O
is	O
a	O
stochastic	B
matrix	I
by	O
contrast	O
in	O
catpca	O
we	O
have	O
yil	O
catswzi	O
where	O
w	O
is	O
an	O
unconstrained	O
matrix	O
fitting	O
this	O
model	O
is	O
tricky	O
since	O
the	O
prior	O
for	O
i	O
is	O
no	O
longer	O
conjugate	O
to	O
the	O
multinomial	B
likelihood	B
for	O
qil	O
however	O
we	O
can	O
use	O
any	O
of	O
the	O
variational	O
methods	O
in	O
section	O
where	O
we	O
discussed	O
bayesian	B
multiclass	O
logistic	B
regression	B
in	O
the	O
ctm	O
case	O
things	O
are	O
even	O
harder	O
since	O
the	O
categorical	B
response	O
variables	O
qi	O
are	O
hidden	B
but	O
we	O
can	O
handle	O
this	O
by	O
using	O
an	O
additional	O
mean	B
field	O
approximation	O
see	O
and	O
lafferty	O
for	O
details	O
having	O
fit	O
the	O
model	O
one	O
can	O
then	O
convert	O
to	O
a	O
sparse	B
precision	B
matrix	I
by	O
pruning	B
low-strength	O
edges	B
to	O
get	O
a	O
sparse	B
gaussian	B
graphical	B
model	I
this	O
allows	O
you	O
to	O
visualize	O
the	O
correlation	O
between	O
topics	O
figure	O
shows	O
the	O
result	O
of	O
applying	O
this	O
procedure	O
to	O
articles	O
from	O
science	O
magazine	O
from	O
corpus	B
contains	O
documents	O
and	O
words	O
of	O
them	O
unique	O
after	O
stop-word	O
and	O
low-frequency	O
removal	O
nodes	B
represent	O
topics	O
with	O
the	O
top	O
words	O
per	O
topic	B
listed	O
inside	O
the	O
font	O
size	O
reflects	O
the	O
overall	O
prevalence	B
of	O
the	O
topic	B
in	O
the	O
corpus	B
edges	B
represent	O
significant	O
elements	O
of	O
the	O
precision	B
matrix	I
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
activated	O
tyrosine	O
phosphorylation	O
activation	B
phosphorylation	O
kinase	O
research	O
funding	O
support	B
nih	O
program	O
science	O
scientists	O
says	O
research	O
people	O
united	O
states	O
women	O
universities	O
students	O
education	O
receptor	O
receptors	O
ligand	O
ligands	O
apoptosis	O
cells	O
cell	O
expression	O
cell	O
lines	O
bone	O
marrow	O
amino	O
acids	O
cdna	O
sequence	O
isolated	O
protein	O
cell	O
cycle	B
activity	O
cyclin	O
regulation	O
wild	O
type	O
mutant	O
mutations	O
mutants	O
mutation	O
mice	O
antigen	O
t	O
cells	O
antigens	O
immune	O
response	O
virus	O
hiv	O
aids	O
infection	O
viruses	O
patients	O
disease	O
treatment	O
drugs	O
clinical	O
bacteria	O
bacterial	O
host	O
resistance	O
parasite	O
gene	O
disease	O
mutations	O
families	O
mutation	O
cells	O
proteins	O
researchers	O
protein	O
found	O
enzyme	O
enzymes	O
iron	O
active	O
site	O
reduction	O
plants	O
plant	O
gene	O
genes	O
arabidopsis	O
development	O
embryos	O
drosophila	O
genes	O
expression	O
proteins	O
protein	O
binding	O
domain	O
domains	O
rna	O
dna	O
rna	O
polymerase	O
cleavage	O
site	O
brain	O
memory	O
subjects	O
left	O
task	O
computer	O
problem	O
information	B
computers	O
problems	O
sequence	O
sequences	O
genome	B
dna	O
sequencing	O
surface	O
liquid	O
surfaces	O
fluid	O
model	O
magnetic	O
magnetic	O
field	O
spin	B
superconductivity	O
superconducting	O
fossil	O
record	O
birds	O
fossils	O
dinosaurs	O
fossil	O
species	O
forest	B
forests	O
populations	O
ecosystems	O
genetic	O
population	O
populations	O
differences	O
variation	O
ancient	O
found	O
impact	O
million	O
years	O
ago	O
africa	O
neurons	O
stimulus	O
motor	O
visual	O
cortical	O
materials	O
organic	O
polymer	O
polymers	O
molecules	O
synapses	O
ltp	O
glutamate	O
synaptic	O
neurons	O
physicists	O
particles	O
physics	O
particle	O
experiment	O
surface	O
tip	O
image	O
sample	O
device	O
laser	O
optical	O
light	O
electrons	O
quantum	O
reaction	O
reactions	O
molecule	O
molecules	O
transition	O
state	B
stars	O
astronomers	O
universe	O
galaxies	O
galaxy	O
pressure	O
high	O
pressure	O
pressures	O
core	O
inner	O
core	O
mantle	O
crust	O
upper	O
mantle	O
meteorites	O
ratios	O
sun	O
solar	O
wind	O
earth	O
planets	O
planet	O
earthquake	O
earthquakes	O
fault	O
images	O
data	O
carbon	O
carbon	O
dioxide	O
methane	O
water	O
volcanic	O
deposits	O
magma	O
eruption	O
volcanism	O
climate	O
ocean	O
ice	O
changes	O
climate	O
change	O
ozone	O
atmospheric	O
measurements	O
stratosphere	O
concentrations	O
figure	O
output	O
of	O
the	O
correlated	B
topic	B
model	I
k	O
topics	O
when	O
applied	O
to	O
articles	O
from	O
science	O
nodes	B
represent	O
topics	O
with	O
the	O
most	O
probable	O
phrases	O
from	O
each	O
topic	B
shown	O
inside	O
font	O
size	O
reflects	O
overall	O
prevalence	B
of	O
the	O
topic	B
see	O
httpwww	O
cs	O
cmu	O
edulemurscience	O
for	O
an	O
interactive	O
version	O
of	O
this	O
model	O
with	O
topics	O
source	O
figure	O
of	O
and	O
lafferty	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
dynamic	B
topic	B
model	I
in	O
lda	B
the	O
topics	O
over	O
words	O
are	O
assumed	O
to	O
be	O
static	O
in	O
some	O
cases	O
it	O
makes	O
sense	O
to	O
allow	O
these	O
distributions	O
to	O
evolve	O
smoothly	O
over	O
time	O
for	O
example	O
an	O
article	O
might	O
use	O
the	O
topic	B
neuroscience	O
but	O
if	O
it	O
was	O
written	O
in	O
the	O
it	O
is	O
more	O
likely	O
to	O
use	O
words	O
like	O
nerve	O
whereas	O
if	O
it	O
was	O
written	O
in	O
the	O
it	O
is	O
more	O
likely	O
to	O
use	O
words	O
like	O
calcium	O
receptor	O
reflects	O
the	O
general	O
trend	O
of	O
neuroscience	O
towards	O
molecular	O
biology	O
one	O
way	O
to	O
model	O
this	O
is	O
use	O
a	O
dynamic	O
logistic	B
normal	B
model	O
as	O
illustrated	O
in	O
figure	O
in	O
particular	O
we	O
assume	O
the	O
topic	B
distributions	O
evolve	O
according	O
to	O
a	O
gaussian	B
random	O
walk	O
and	O
then	O
we	O
map	O
these	O
gaussian	B
vectors	O
to	O
probabilities	O
via	O
the	O
softmax	B
function	O
btkbt	O
n	O
i	O
dir	O
t	O
i	O
cat	O
t	O
il	O
t	O
qt	O
i	O
il	O
k	O
bt	O
catsbt	O
k	O
ilqt	O
yt	O
this	O
is	O
known	O
as	O
a	O
dynamic	B
topic	B
model	I
and	O
lafferty	O
extensions	O
of	O
lda	B
t	O
i	O
qt	O
il	O
yt	O
il	O
t	O
i	O
qt	O
il	O
yt	O
il	O
i	O
il	O
il	O
n	O
n	O
bt	O
k	O
bt	O
k	O
k	O
n	O
k	O
figure	O
the	O
dynamic	B
topic	B
model	I
one	O
can	O
perform	O
approximate	O
infernece	O
in	O
this	O
model	O
using	O
a	O
structured	O
mean	B
field	O
method	O
that	O
exploits	O
the	O
kalman	B
smoothing	B
algorithm	O
to	O
perform	O
exact	O
inference	B
on	O
the	O
linear-gaussian	O
chain	O
between	O
the	O
btk	O
nodes	B
and	O
lafferty	O
for	O
details	O
figure	O
illustrates	O
a	O
typical	O
output	O
of	O
the	O
system	O
when	O
applied	O
to	O
years	O
of	O
articles	O
from	O
science	O
on	O
the	O
top	O
we	O
visualize	O
the	O
top	O
words	O
from	O
a	O
specific	O
topic	B
seems	O
to	O
be	O
related	O
to	O
neuroscience	O
after	O
year	O
intervals	O
on	O
the	O
bottom	O
left	O
we	O
plot	O
the	O
probability	O
of	O
some	O
specific	O
words	O
belonging	O
to	O
this	O
topic	B
on	O
the	O
bottom	O
right	O
we	O
list	O
the	O
titles	O
of	O
some	O
articles	O
that	O
contained	O
this	O
topic	B
one	O
interesting	O
application	O
of	O
this	O
model	O
is	O
to	O
perform	O
temporally-corrected	O
document	O
retrieval	O
that	O
is	O
suppose	O
we	O
look	O
for	O
documents	O
about	O
the	O
inheritance	O
of	O
disease	O
modern	O
articles	O
will	O
use	O
words	O
like	O
dna	O
but	O
older	O
articles	O
the	O
discovery	O
of	O
dna	O
may	O
use	O
other	O
terms	O
such	O
as	O
heritable	O
unit	O
but	O
both	O
articles	O
are	O
likely	O
to	O
use	O
the	O
same	O
topics	O
similar	B
ideas	O
can	O
be	O
used	O
to	O
perform	O
cross-language	B
information	B
retrieval	I
see	O
e	O
g	O
et	O
al	O
lda-hmm	B
the	O
lda	B
model	O
assumes	O
words	O
are	O
exchangeable	B
which	O
is	O
clearly	O
not	O
true	O
a	O
simple	O
way	O
to	O
model	O
sequential	B
dependence	O
between	O
words	O
is	O
to	O
use	O
a	O
hidden	B
markov	B
model	I
or	O
hmm	B
the	O
trouble	O
with	O
hmms	B
is	O
that	O
they	O
can	O
only	O
model	O
short-range	O
dependencies	O
so	O
they	O
cannot	O
capture	O
the	O
overall	O
gist	B
of	O
a	O
document	O
hence	O
they	O
can	O
generate	O
syntactically	O
correct	O
sentences	O
e	O
g	O
table	O
but	O
not	O
semantically	O
plausible	O
ones	O
it	O
is	O
possible	O
to	O
combine	O
lda	B
with	O
hmm	B
to	O
create	O
a	O
model	O
called	O
lda-hmm	B
et	O
al	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
brain	O
movement	O
action	B
right	O
eye	O
hand	O
left	O
muscle	O
nerve	O
sound	O
movement	O
eye	O
right	O
hand	O
brain	O
left	O
action	B
muscle	O
sound	O
experiment	O
brain	O
eye	O
movement	O
right	O
left	O
hand	O
nerve	O
vision	O
sound	O
muscle	O
movement	O
movement	O
brain	O
sound	O
nerve	O
active	O
muscle	O
left	O
eye	O
right	O
nervous	O
sound	O
muscle	O
active	O
nerve	O
stimulate	O
fiber	O
reaction	O
brain	O
response	O
stimulate	O
muscle	O
sound	O
movement	O
response	O
nerve	O
frequency	O
fiber	O
active	O
brain	O
record	O
nerve	O
stimulate	O
response	O
muscle	O
electrode	O
active	O
brain	O
fiber	O
potential	O
respons	O
record	O
stimulate	O
nerve	O
muscle	O
active	O
frequency	O
electrode	O
potential	O
study	O
response	O
stimulate	O
record	O
condition	O
active	O
potential	O
stimulus	O
nerve	O
subject	O
eye	O
respons	O
cell	O
potential	O
stimul	O
neuron	O
active	O
nerve	O
eye	O
record	O
abstract	O
cell	O
neuron	O
response	O
active	O
brain	O
stimul	O
muscle	O
system	O
nerve	O
receptor	O
cell	O
channel	O
neuron	O
active	O
brain	O
receptor	O
muscle	O
respons	O
current	O
neuron	O
active	O
brain	O
cell	O
fig	O
response	O
channel	O
receptor	O
synapse	O
signal	O
nerve	O
neuron	O
mental	O
science	O
hemianopsia	O
in	O
migraine	O
a	O
defence	O
of	O
the	O
phrenology	O
the	O
synchronal	O
flashing	O
of	O
fireflies	O
myoesthesis	O
and	O
imageless	O
thought	O
acetylcholine	O
and	O
the	O
physiology	O
of	O
the	O
nervous	O
system	O
brain	O
waves	O
and	O
unit	O
discharge	O
in	O
cerebral	O
cortex	O
errorless	O
discrimination	O
learning	B
in	O
the	O
pigeon	O
temporal	O
summation	O
of	O
light	O
by	O
a	O
vertebrate	O
visual	O
receptor	O
hysteresis	O
in	O
the	O
force-calcium	O
relation	B
in	O
muscle	O
gaba-activated	O
chloride	O
channels	O
in	O
secretory	O
nerve	O
endings	O
figure	O
part	O
of	O
the	O
output	O
of	O
the	O
dynamic	B
topic	B
model	I
when	O
applied	O
to	O
articles	O
from	O
science	O
we	O
show	O
the	O
top	O
words	O
for	O
the	O
neuroscience	O
topic	B
over	O
time	O
we	O
also	O
show	O
the	O
probability	O
of	O
three	O
words	O
within	O
this	O
topic	B
over	O
time	O
and	O
some	O
articles	O
that	O
contained	O
this	O
topic	B
source	O
figure	O
of	O
and	O
lafferty	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
this	O
model	O
uses	O
the	O
hmm	B
states	O
to	O
model	O
function	O
or	O
syntactic	O
words	O
such	O
as	O
and	O
or	O
however	O
and	O
uses	O
the	O
lda	B
to	O
model	O
content	O
or	O
semantic	O
words	O
which	O
are	O
harder	O
to	O
predict	O
there	O
is	O
a	O
distinguished	O
hmm	B
state	B
which	O
specifies	O
when	O
the	O
lda	B
model	O
should	O
be	O
used	O
to	O
generate	O
the	O
word	O
the	O
rest	O
of	O
the	O
time	O
the	O
hmm	B
generates	O
the	O
word	O
more	O
formally	O
for	O
each	O
document	O
i	O
the	O
model	O
defines	O
an	O
hmm	B
with	O
states	O
zil	O
c	O
in	O
addition	O
each	O
document	O
has	O
an	O
lda	B
model	O
associated	O
with	O
it	O
if	O
zil	O
we	O
generate	O
word	O
yil	O
from	O
the	O
semantic	O
lda	B
model	O
with	O
topic	B
specified	O
by	O
qil	O
otherwise	O
we	O
generate	O
word	O
yil	O
from	O
the	O
syntactic	O
hmm	B
model	O
the	O
dgm	B
is	O
shown	O
in	O
figure	O
the	O
cpds	O
are	O
as	O
follows	O
p	O
i	O
dir	O
i	O
pzil	O
c	O
hmm	B
pqil	O
k	O
i	O
ik	O
pyil	O
vqil	O
k	O
zil	O
c	O
if	O
c	O
if	O
c	O
bldak	O
v	O
bhmm	O
v	O
where	O
blda	O
is	O
the	O
usual	O
topic-word	O
matrix	O
bhmm	O
is	O
the	O
state-word	O
hmm	B
emission	O
matrix	O
and	O
ahmm	O
is	O
the	O
state-state	O
hmm	B
transition	B
matrix	I
inference	B
in	O
this	O
model	O
can	O
be	O
done	O
with	O
collapsed	B
gibbs	B
sampling	I
analytically	O
integrating	O
out	O
all	O
the	O
continuous	O
quantities	O
see	O
et	O
al	O
for	O
the	O
details	O
the	O
results	O
of	O
applying	O
this	O
model	O
k	O
lda	B
topics	O
and	O
c	O
hmm	B
states	O
to	O
the	O
combined	O
brown	O
and	O
tasa	B
are	O
shown	O
in	O
table	O
we	O
see	O
that	O
the	O
hmm	B
generally	O
is	O
the	O
brown	O
corpus	B
consists	O
of	O
documents	O
and	O
word	O
tokens	B
with	O
part-of-speech	O
tags	O
for	O
each	O
token	O
extensions	O
of	O
lda	B
i	O
qil	O
yil	O
zil	O
qil	O
yil	O
zil	O
blda	O
n	O
ahmm	O
bhmm	O
figure	O
lda-hmm	B
model	O
in	O
contrast	O
to	O
this	O
approach	O
we	O
study	O
here	O
how	O
the	O
overall	O
network	O
activity	O
can	O
control	O
single	O
cell	O
parameters	O
such	O
as	O
input	O
resistance	O
as	O
well	O
as	O
time	O
and	O
space	O
constants	O
parameters	O
that	O
are	O
crucial	O
for	O
excitability	O
and	O
spariotemporal	O
integration	O
the	O
integrated	O
architecture	O
in	O
this	O
paper	O
combines	O
feed	O
forward	O
control	O
and	O
error	O
feedback	O
adaptive	O
control	O
using	O
neural	B
networks	I
in	O
other	O
words	O
for	O
our	O
proof	O
of	O
convergence	O
we	O
require	O
the	O
softassign	O
algorithm	O
to	O
return	O
a	O
doubly	O
stochastic	B
matrix	I
as	O
theorem	O
guarantees	O
that	O
it	O
will	O
instead	O
of	O
a	O
matrix	O
which	O
is	O
merely	O
close	O
to	O
being	O
doubly	O
stochastic	O
based	O
on	O
some	O
reasonable	O
metric	B
the	O
aim	O
is	O
to	O
construct	O
a	O
portfolio	O
with	O
a	O
maximal	O
expected	O
return	O
for	O
a	O
given	O
risk	B
level	O
and	O
time	O
horizon	B
while	O
simultaneously	O
obeying	O
or	O
required	O
constraints	O
the	O
left	O
graph	B
is	O
the	O
standard	O
experiment	O
the	O
right	O
from	O
a	O
training	O
with	O
samples	B
the	O
graph	B
g	O
is	O
called	O
the	O
graph	B
and	O
h	O
is	O
called	O
the	O
host	O
graph	B
figure	O
function	O
and	O
content	O
words	O
in	O
the	O
nips	O
corpus	B
as	O
distinguished	O
by	O
the	O
lda-hmm	B
model	O
graylevel	O
indicates	O
posterior	O
probability	O
of	O
assignment	O
to	O
lda	B
component	O
with	O
black	O
being	O
highest	O
the	O
boxed	O
word	O
appears	O
as	O
a	O
function	O
word	O
in	O
one	O
sentence	O
and	O
as	O
a	O
content	O
word	O
in	O
another	O
sentence	O
asterisked	O
words	O
had	O
low	O
frequency	O
and	O
were	O
treated	O
as	O
a	O
single	O
word	O
type	O
by	O
the	O
model	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
the	O
blood	O
of	O
body	O
heart	O
and	O
in	O
to	O
is	O
blood	O
heart	O
pressure	O
body	O
lungs	O
oxygen	O
vessels	O
arteries	O
breathing	O
the	O
a	O
his	O
this	O
their	O
these	O
your	O
her	O
my	O
some	O
the	O
and	O
of	O
a	O
in	O
trees	O
tree	B
with	O
on	O
forest	B
trees	O
forests	O
land	O
soil	O
areas	O
park	O
wildlife	O
area	O
rain	O
in	O
for	O
to	O
on	O
with	O
at	O
by	O
from	O
as	O
into	O
the	O
and	O
of	O
in	O
land	O
to	O
the	O
of	O
to	O
in	O
and	O
classes	O
farmers	O
government	O
for	O
farm	O
farmers	O
land	O
crops	O
farm	O
food	O
people	O
farming	O
wheat	O
farms	O
corn	O
he	O
it	O
you	O
they	O
i	O
she	O
we	O
there	O
this	O
who	O
a	O
state	B
government	O
state	B
federal	O
public	O
local	O
act	O
states	O
national	O
laws	O
department	O
new	O
other	O
first	O
same	O
great	O
good	O
small	O
little	O
old	O
the	O
a	O
of	O
in	O
to	O
picture	O
film	O
image	O
lens	O
light	O
eye	O
lens	O
image	O
mirror	O
eyes	O
glass	O
object	O
objects	O
lenses	O
be	O
have	O
see	O
make	O
do	O
know	O
get	O
go	O
take	O
find	O
a	O
the	O
of	O
in	O
water	O
is	O
and	O
matter	O
are	O
water	O
matter	O
molecules	O
liquid	O
particles	O
gas	O
solid	O
substance	O
temperature	B
changes	O
said	O
made	O
used	O
came	O
went	O
found	O
called	O
the	O
a	O
of	O
and	O
drink	O
alcohol	O
to	O
bottle	O
in	O
drugs	O
drug	O
alcohol	O
people	O
drinking	O
person	O
effects	O
marijuana	O
body	O
use	O
time	O
way	O
years	O
day	O
part	O
number	O
kind	O
place	O
the	O
of	O
a	O
and	O
in	O
story	O
is	O
to	O
as	O
story	O
stories	O
poem	O
characters	O
poetry	O
character	O
author	O
poems	O
life	O
poet	O
can	O
would	O
will	O
could	O
may	O
had	O
must	O
do	O
have	O
did	O
the	O
a	O
in	O
game	O
ball	O
and	O
team	O
to	O
play	O
ball	O
game	O
team	O
baseball	O
players	O
football	O
player	O
field	O
basketball	O
table	O
upper	O
row	O
topics	O
extracted	O
by	O
the	O
lda	B
model	O
when	O
trained	O
on	O
the	O
combined	O
brown	O
and	O
tasa	B
corpora	O
middle	O
row	O
topics	O
extracted	O
by	O
lda	B
part	O
of	O
lda-hmm	B
model	O
bottom	O
row	O
topics	O
extracted	O
by	O
hmm	B
part	O
of	O
lda-hmm	B
model	O
each	O
column	O
represents	O
a	O
single	O
topicclass	O
and	O
words	O
appear	O
in	O
order	O
of	O
probability	O
in	O
that	O
topicclass	O
since	O
some	O
classes	O
give	O
almost	O
all	O
probability	O
to	O
only	O
a	O
few	O
words	O
a	O
list	O
is	O
terminated	O
when	O
the	O
words	O
account	O
for	O
of	O
the	O
probability	O
mass	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
responsible	O
for	O
syntactic	O
words	O
and	O
the	O
lda	B
for	O
semantics	O
words	O
if	O
we	O
did	O
not	O
have	O
the	O
hmm	B
the	O
lda	B
topics	O
would	O
get	O
polluted	O
by	O
function	O
words	O
top	O
of	O
figure	O
which	O
is	O
why	O
such	O
words	O
are	O
normally	O
removed	O
during	O
preprocessing	O
the	O
model	O
can	O
also	O
help	O
disambiguate	O
when	O
the	O
same	O
word	O
is	O
being	O
used	O
syntactically	O
or	O
semantically	O
figure	O
shows	O
some	O
examples	O
when	O
the	O
model	O
was	O
applied	O
to	O
the	O
nips	O
we	O
see	O
that	O
the	O
roles	O
of	O
words	O
are	O
distinguished	O
e	O
g	O
we	O
require	O
the	O
algorithm	O
to	O
return	O
a	O
matrix	O
vs	O
the	O
maximal	O
expected	O
return	O
in	O
principle	O
a	O
part	B
of	I
speech	I
tagger	O
could	O
disambiguate	O
these	O
two	O
uses	O
but	O
note	O
that	O
the	O
lda-hmm	B
method	O
is	O
fully	O
unsupervised	O
pos	O
tags	O
were	O
used	O
and	O
sometimes	O
a	O
word	O
can	O
have	O
the	O
same	O
pos	O
tag	O
but	O
different	O
senses	O
e	O
g	O
the	O
left	O
graph	B
synactic	O
role	O
vs	O
the	O
graph	B
g	O
semantic	O
role	O
the	O
topic	B
of	O
probabilistic	O
models	O
for	O
syntax	O
and	O
semantics	O
is	O
a	O
vast	O
one	O
which	O
we	O
do	O
not	O
the	O
tasa	B
corpus	B
is	O
an	O
untagged	O
collection	O
of	O
educational	O
materials	O
consisting	O
of	O
documents	O
and	O
word	O
tokens	B
words	O
appearing	O
in	O
fewer	O
than	O
documents	O
were	O
replaced	O
with	O
an	O
asterisk	O
but	O
punctuation	O
was	O
included	O
the	O
combined	O
vocabulary	O
was	O
of	O
size	O
unique	O
words	O
nips	O
stands	O
for	O
neural	O
information	B
processing	O
systems	O
nips	O
corpus	B
volumes	O
contains	O
documents	O
it	O
is	O
one	O
of	O
the	O
top	O
machine	B
learning	B
conferences	O
the	O
extensions	O
of	O
lda	B
i	O
qil	O
yil	O
qi	O
ci	B
n	O
b	O
w	O
a	O
i	O
qil	O
yil	O
ci	B
n	O
b	O
figure	O
supervised	B
lda	B
discriminative	B
lda	B
have	O
space	O
to	O
delve	O
into	O
any	O
more	O
see	O
e	O
g	O
and	O
martin	O
for	O
further	O
information	B
supervised	B
lda	B
in	O
this	O
section	O
we	O
discuss	O
extensions	O
of	O
lda	B
to	O
handle	O
side	B
information	B
of	O
various	O
kinds	O
beyond	O
just	O
words	O
generative	O
supervised	B
lda	B
suppose	O
we	O
have	O
a	O
variable	O
length	O
sequence	O
of	O
words	O
yil	O
v	O
as	O
usual	O
but	O
we	O
also	O
have	O
a	O
class	O
label	B
ci	B
c	O
how	O
can	O
we	O
predict	O
ci	B
from	O
yi	O
there	O
are	O
many	O
possible	O
approaches	O
but	O
most	O
are	O
direct	O
mappings	O
from	O
the	O
words	O
to	O
the	O
class	O
in	O
some	O
cases	O
such	O
as	O
sentiment	B
analysis	I
we	O
can	O
get	O
better	O
performance	O
by	O
first	O
performing	O
inference	B
to	O
try	O
to	O
disambiguate	O
the	O
meaning	O
of	O
words	O
for	O
example	O
suppose	O
the	O
goal	O
is	O
to	O
determine	O
if	O
a	O
document	O
is	O
a	O
favorable	O
review	O
of	O
a	O
movie	O
or	O
not	O
if	O
we	O
encounter	O
the	O
phrase	O
brad	O
pitt	O
was	O
excellent	O
until	O
the	O
middle	O
of	O
the	O
movie	O
the	O
word	O
excellent	O
may	O
lead	O
us	O
to	O
think	O
the	O
review	O
is	O
positive	O
but	O
clearly	O
the	O
overall	O
sentiment	O
is	O
negative	O
one	O
way	O
to	O
tackle	O
such	O
problems	O
is	O
to	O
build	O
a	O
joint	O
model	O
of	O
the	O
form	O
pci	O
yi	O
and	O
mcauliffe	O
proposes	O
an	O
approach	O
called	O
supervised	B
lda	B
where	O
the	O
class	O
label	B
ci	B
is	O
generated	O
from	O
the	O
topics	O
as	O
follows	O
pciqi	O
bersigmwt	O
qi	O
here	O
qi	O
is	O
the	O
empirical	O
topic	B
distribution	O
for	O
document	O
i	O
qik	O
li	O
qilk	O
see	O
figure	O
for	O
an	O
illustration	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
w	O
xi	O
i	O
qil	O
yil	O
n	O
b	O
xi	O
w	O
i	O
i	O
qil	O
yil	O
n	O
b	O
w	O
xi	O
i	O
i	O
b	O
qil	O
yil	O
n	O
figure	O
discriminative	B
variants	O
of	O
lda	B
mixture	B
of	I
experts	I
aka	O
mr-lda	O
the	O
double	O
ring	O
denotes	O
a	O
node	O
that	O
i	O
a	O
deterministic	O
function	O
of	O
its	O
parents	B
dmr-lda	O
mixture	B
of	I
experts	I
with	O
random	B
effects	I
we	O
can	O
fit	O
this	O
model	O
using	O
monte	B
carlo	I
em	B
run	O
the	O
collapsed	B
gibbs	B
sampler	I
in	O
the	O
e	B
step	I
to	O
compute	O
e	O
and	O
then	O
use	O
this	O
as	O
the	O
input	O
feature	O
to	O
a	O
standard	O
logistic	B
regression	B
package	O
discriminative	B
supervised	B
lda	B
an	O
alternative	O
approach	O
known	O
as	O
discriminative	B
lda	B
et	O
al	O
is	O
shown	O
in	O
figure	O
this	O
is	O
a	O
discriminative	B
model	O
of	O
the	O
form	O
pyici	O
the	O
only	O
change	O
from	O
regular	B
lda	B
is	O
that	O
the	O
topic	B
prior	O
becomes	O
input	O
dependent	O
as	O
follows	O
pqil	O
i	O
ci	B
c	O
catac	O
where	O
ac	O
is	O
a	O
k	O
k	O
stochastic	B
matrix	I
so	O
far	O
we	O
have	O
assumed	O
the	O
side	B
information	B
is	O
a	O
single	O
categorical	B
variable	O
ci	B
often	O
we	O
have	O
high	O
dimensional	O
covariates	B
xi	O
r	O
d	O
for	O
example	O
consider	O
the	O
task	O
of	O
image	B
tagging	I
the	O
idea	O
is	O
that	O
yil	O
represent	O
correlated	O
tags	O
or	O
labels	O
which	O
we	O
want	O
to	O
predict	O
given	O
xi	O
we	O
now	O
discuss	O
several	O
attempts	O
to	O
extend	O
lda	B
so	O
that	O
it	O
can	O
generate	O
tags	O
given	O
the	O
inputs	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
mixture	B
of	I
experts	I
with	O
multiple	O
outputs	O
this	O
is	O
just	O
like	O
lda	B
except	O
we	O
replace	O
the	O
dirichlet	B
prior	O
on	O
i	O
with	O
a	O
deterministic	O
function	O
of	O
the	O
input	O
i	O
swxi	O
in	O
et	O
al	O
this	O
is	O
called	O
multinomial	B
regression	B
lda	B
see	O
figure	O
eliminating	O
the	O
deterministic	O
i	O
we	O
have	O
pqilxi	O
w	O
catswxi	O
we	O
can	O
fit	O
this	O
with	O
em	B
in	O
the	O
usual	O
way	O
however	O
et	O
al	O
suggest	O
an	O
alternative	O
first	O
fit	O
an	O
unsupervised	O
lda	B
model	O
based	O
only	O
on	O
yi	O
then	O
treat	O
the	O
inferred	O
i	O
as	O
data	O
and	O
extensions	O
of	O
lda	B
fit	O
a	O
multinomial	B
logistic	B
regression	B
model	O
mapping	O
xi	O
to	O
i	O
although	O
this	O
is	O
fast	O
fitting	O
lda	B
in	O
an	O
unsupervised	O
fashion	O
does	O
not	O
necessarily	O
result	O
in	O
a	O
discriminative	B
set	O
of	O
latent	B
variables	O
as	O
discussed	O
in	O
and	O
mcauliffe	O
there	O
is	O
a	O
more	O
subtle	O
problem	O
with	O
this	O
model	O
since	O
i	O
is	O
a	O
deterministic	O
function	O
of	O
the	O
inputs	O
it	O
is	O
effectively	O
observed	O
rendering	O
the	O
qil	O
hence	O
the	O
tags	O
yil	O
independent	O
in	O
other	O
words	O
pyixi	O
pyilxi	O
pyilqil	O
k	O
bpqil	O
kxi	O
w	O
k	O
this	O
means	O
that	O
if	O
we	O
observe	O
the	O
value	O
of	O
one	O
tag	O
it	O
will	O
have	O
no	O
influence	O
on	O
any	O
of	O
the	O
others	O
this	O
may	O
explain	O
why	O
the	O
results	O
in	O
et	O
al	O
only	O
show	O
negligible	O
improvement	O
over	O
predicting	O
each	O
tag	O
independently	O
one	O
way	O
to	O
induce	O
correlations	O
is	O
to	O
make	O
w	O
a	O
random	O
variable	O
the	O
resulting	O
model	O
is	O
shown	O
in	O
figure	O
we	O
call	O
this	O
a	O
random	B
effects	I
mixture	B
of	I
experts	I
we	O
typically	O
assume	O
a	O
gaussian	B
prior	O
on	O
wi	O
if	O
xi	O
then	O
pqilxi	O
wi	O
catswi	O
so	O
we	O
recover	O
the	O
correlated	B
topic	B
model	I
it	O
is	O
possible	O
to	O
extend	O
this	O
model	O
by	O
adding	O
markovian	O
dynamics	O
to	O
the	O
qil	O
variables	O
this	O
is	O
called	O
a	O
conditional	O
topic	B
random	O
field	O
and	O
xing	O
a	O
closely	O
related	O
approach	O
known	O
as	O
dirichlet	B
multinomial	B
regression	B
lda	B
and	O
mccallum	O
is	O
shown	O
in	O
figure	O
this	O
is	O
identical	O
to	O
standard	O
lda	B
except	O
we	O
make	O
a	O
function	O
of	O
the	O
input	O
i	O
expwxi	O
where	O
w	O
is	O
a	O
k	O
d	O
matrix	O
eliminating	O
the	O
deterministic	O
i	O
we	O
have	O
i	O
direxpwxi	O
is	O
known	O
as	O
labeled	B
lda	B
et	O
al	O
unlike	O
et	O
al	O
this	O
model	O
allows	O
information	B
to	O
flow	O
between	O
tags	O
via	O
the	O
latent	B
i	O
a	O
variant	O
of	O
this	O
model	O
where	O
xi	O
corresponds	O
to	O
a	O
bag	O
of	O
discrete	B
labels	O
and	O
i	O
dir	O
in	O
this	O
case	O
the	O
labels	O
xi	O
are	O
in	O
xi	O
correspondence	B
with	O
the	O
latent	B
topics	O
which	O
makes	O
the	O
resulting	O
topics	O
much	O
more	O
interpretable	O
an	O
extension	B
known	O
as	O
partially	B
labeled	B
lda	B
et	O
al	O
allows	O
each	O
label	B
to	O
have	O
multiple	O
latent	B
sub-topics	O
this	O
model	O
includes	O
lda	B
labeled	B
lda	B
and	O
a	O
multinomial	B
mixture	B
model	I
as	O
special	O
cases	O
discriminative	B
categorical	B
pca	B
an	O
alternative	O
to	O
using	O
lda	B
is	O
to	O
expand	O
the	O
categorical	B
pca	B
model	O
with	O
inputs	O
as	O
shown	O
in	O
figure	O
since	O
the	O
latent	B
space	O
is	O
now	O
real-valued	O
we	O
can	O
use	O
simple	B
linear	B
regression	B
for	O
the	O
input-hidden	O
mapping	O
for	O
the	O
hidden-output	O
mapping	O
we	O
use	O
traditional	O
catpca	O
pzixi	O
v	O
pyizi	O
w	O
catyilswzi	O
l	O
this	O
model	O
is	O
essentially	O
a	O
probabilistic	O
neural	B
network	I
with	O
one	O
hidden	B
layer	I
as	O
shown	O
in	O
figure	O
but	O
with	O
exchangeable	B
output	O
to	O
handle	O
variable	O
numbers	O
of	O
tags	O
the	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
xi	O
zi	O
yil	O
n	O
v	O
w	O
xid	O
zik	O
vk	O
yili	O
n	O
w	O
figure	O
vector	O
nodes	B
expanded	O
out	O
categorical	B
pca	B
with	O
inputs	O
and	O
exchangeable	B
outputs	O
same	O
as	O
but	O
with	O
the	O
key	O
difference	O
from	O
a	O
neural	O
net	O
is	O
that	O
information	B
can	O
flow	O
between	O
the	O
yil	O
s	O
via	O
the	O
latent	B
bottleneck	B
layer	I
zi	O
this	O
should	O
work	O
better	O
than	O
a	O
conventional	O
neural	O
net	O
when	O
the	O
output	O
labels	O
are	O
highly	O
correlated	O
even	O
after	O
conditioning	B
on	O
the	O
features	B
this	O
problem	O
frequently	O
arises	O
in	O
multi	O
label	B
classification	O
note	O
that	O
we	O
could	O
allow	O
a	O
direct	O
xi	O
to	O
yi	O
arc	O
but	O
this	O
would	O
require	O
too	O
many	O
parameters	O
if	O
the	O
number	O
of	O
labels	O
is	O
we	O
can	O
fit	O
this	O
model	O
with	O
a	O
small	O
modification	O
of	O
the	O
variational	B
em	B
algorithm	O
in	O
section	O
if	O
we	O
use	O
this	O
model	O
for	O
regression	B
rather	O
than	O
classification	O
we	O
can	O
perform	O
the	O
e	B
step	I
exactly	O
by	O
modifying	O
the	O
em	B
algorithm	O
for	O
factor	B
analysis	I
et	O
al	O
reports	O
that	O
this	O
method	O
converges	O
faster	O
than	O
standard	O
backpropagation	B
we	O
can	O
also	O
extend	O
the	O
model	O
so	O
that	O
the	O
prior	O
on	O
zi	O
is	O
a	O
mixture	B
of	I
gaussians	I
using	O
inputif	O
the	O
output	O
is	O
gaussian	B
this	O
corresponds	O
to	O
a	O
mixture	B
of	O
discriminative	B
dependent	O
means	O
factor	B
analysers	O
zhou	O
and	O
liu	O
if	O
the	O
output	O
is	O
categorical	B
this	O
would	O
be	O
an	O
yet	O
unpublished	O
model	O
which	O
we	O
could	O
call	O
discriminative	B
mixtures	O
of	O
categorical	B
factor	B
analyzers	O
lvms	O
for	O
graph-structured	O
data	O
another	O
source	O
of	O
discrete	B
data	O
is	O
when	O
modeling	O
graph	B
or	O
network	O
structures	O
to	O
see	O
the	O
connection	O
recall	B
that	O
any	O
graph	B
on	O
d	O
nodes	B
can	O
be	O
represented	O
as	O
a	O
d	O
d	O
adjacency	B
matrix	I
g	O
where	O
gi	O
j	O
iff	B
there	O
is	O
an	O
edge	O
from	O
node	O
i	O
to	O
node	O
j	O
such	O
matrices	O
are	O
binary	O
and	O
often	O
very	O
sparse	B
see	O
figure	O
for	O
an	O
example	O
graphs	O
arise	O
in	O
many	O
application	O
areas	O
such	O
as	O
modeling	O
social	B
networks	I
protein-protein	B
interaction	I
networks	I
or	O
patterns	O
of	O
disease	B
transmission	I
between	O
people	O
or	O
animals	O
there	O
are	O
usually	O
two	O
primary	O
goals	O
when	O
analysing	O
such	O
data	O
first	O
try	O
to	O
discover	O
some	O
interesting	O
a	O
non-probabilistic	O
version	O
of	O
this	O
idea	O
using	O
squared	B
loss	B
was	O
proposed	O
in	O
et	O
al	O
this	O
is	O
similar	B
to	O
a	O
linear	O
feed-forward	O
neural	B
network	I
with	O
an	O
additional	O
edge	O
from	O
xi	O
directly	O
to	O
yi	O
lvms	O
for	O
graph-structured	O
data	O
figure	O
a	O
directed	B
graph	B
the	O
same	O
graph	B
with	O
the	O
nodes	B
partitioned	O
into	O
groups	O
making	O
the	O
block	O
structure	O
more	O
apparent	O
z	O
r	O
figure	O
adjacency	B
matrix	I
for	O
the	O
graph	B
in	O
figure	O
rows	O
and	O
columns	O
are	O
shown	O
permuted	O
to	O
show	O
the	O
block	O
structure	O
we	O
also	O
sketch	O
of	O
how	O
the	O
stochastic	B
block	I
model	I
can	O
generate	O
this	O
graph	B
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
structure	O
in	O
the	O
graph	B
such	O
as	O
clusters	B
or	O
communities	O
second	O
try	O
to	O
predict	O
which	O
links	O
might	O
occur	O
in	O
the	O
future	O
who	O
will	O
make	O
friends	O
with	O
whom	O
below	O
we	O
summarize	O
some	O
models	O
that	O
have	O
been	O
proposed	O
for	O
these	O
tasks	O
some	O
of	O
which	O
are	O
related	O
to	O
lda	B
futher	O
details	O
on	O
these	O
and	O
other	O
approaches	O
can	O
be	O
found	O
in	O
e	O
g	O
et	O
al	O
and	O
the	O
references	O
therein	O
stochastic	B
block	I
model	I
in	O
figure	O
we	O
show	O
a	O
directed	B
graph	B
on	O
nodes	B
there	O
is	O
no	O
apparent	O
structure	O
however	O
if	O
we	O
look	O
more	O
deeply	O
we	O
see	O
it	O
is	O
possible	O
to	O
partition	O
the	O
nodes	B
into	O
three	O
groups	O
or	O
blocks	O
and	O
such	O
that	O
most	O
of	O
the	O
connections	O
go	O
from	O
nodes	B
in	O
to	O
or	O
from	O
to	O
or	O
from	O
to	O
this	O
is	O
illustrated	O
in	O
figure	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
m	O
e	O
t	O
s	O
y	O
s	O
l	O
a	O
n	O
o	O
i	O
t	O
l	O
a	O
e	O
r	O
x	O
i	O
r	O
t	O
a	O
m	O
d	O
e	O
t	O
r	O
o	O
s	O
a	O
b	O
c	O
d	O
a	O
d	O
b	O
c	O
a	O
b	O
c	O
d	O
a	O
b	O
d	O
g	O
e	O
h	O
c	O
f	O
a	O
c	O
b	O
d	O
e	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
a	O
edcb	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
a	O
b	O
c	O
d	O
e	O
figure	O
some	O
examples	O
of	O
graphs	O
generated	O
using	O
the	O
stochastic	B
block	I
model	I
with	O
different	O
kinds	O
of	O
connectivity	O
patterns	O
between	O
the	O
blocks	O
the	O
abstract	O
graph	B
blocks	O
represent	O
a	O
ring	O
a	O
dominance	O
hierarchy	O
a	O
common-cause	O
structure	O
and	O
a	O
common-effect	O
structure	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
the	O
problem	O
is	O
easier	O
to	O
understand	O
if	O
we	O
plot	O
the	O
adjacency	O
matrices	O
figure	O
shows	O
the	O
matrix	O
for	O
the	O
graph	B
with	O
the	O
nodes	B
in	O
their	O
original	O
ordering	O
figure	O
shows	O
the	O
matrix	O
for	O
the	O
graph	B
with	O
the	O
nodes	B
in	O
their	O
permtuted	O
ordering	O
it	O
is	O
clear	O
that	O
there	O
is	O
block	O
structure	O
we	O
can	O
make	O
a	O
generative	O
model	O
of	O
block	O
structured	O
graphs	O
as	O
follows	O
first	O
for	O
every	O
node	O
sample	O
a	O
latent	B
block	O
qi	O
cat	O
where	O
k	O
is	O
the	O
probability	O
of	O
choosing	O
block	O
k	O
for	O
k	O
k	O
second	O
choose	O
the	O
probability	O
of	O
connecting	O
group	O
a	O
to	O
group	O
b	O
for	O
all	B
pairs	I
of	O
groups	O
let	O
us	O
denote	O
this	O
probability	O
by	O
ab	O
this	O
can	O
come	O
from	O
a	O
beta	O
prior	O
finally	O
generate	O
each	O
edge	O
rij	O
using	O
the	O
following	O
model	O
prij	O
rqi	O
a	O
qj	O
b	O
berr	O
ab	O
this	O
is	O
called	O
the	O
stochastic	B
block	I
model	I
and	O
snijders	O
figure	O
illustrates	O
the	O
model	O
as	O
a	O
dgm	B
and	O
figure	O
illustrates	O
how	O
this	O
model	O
can	O
be	O
used	O
to	O
cluster	O
the	O
nodes	B
in	O
our	O
example	O
note	O
that	O
this	O
is	O
quite	O
different	O
from	O
a	O
conventional	O
clustering	B
problem	O
for	O
example	O
we	O
see	O
that	O
all	O
the	O
nodes	B
in	O
block	O
are	O
grouped	O
together	O
even	O
though	O
there	O
are	O
no	O
connections	O
between	O
them	O
what	O
they	O
share	O
is	O
the	O
property	O
that	O
they	O
like	O
to	O
connect	O
to	O
nodes	B
in	O
block	O
and	O
to	O
receive	O
connections	O
from	O
nodes	B
in	O
block	O
figure	O
illustrates	O
the	O
power	O
of	O
the	O
model	O
for	O
generating	O
many	O
different	O
kinds	O
of	O
graph	B
structure	O
for	O
example	O
some	O
social	B
networks	I
have	O
hierarchical	O
structure	O
which	O
can	O
be	O
modeled	O
by	O
clustering	B
people	O
into	O
different	O
social	O
strata	O
whereas	O
others	O
consist	O
of	O
a	O
set	O
of	O
cliques	B
unlike	O
a	O
standard	O
mixture	B
model	I
it	O
is	O
not	O
possible	O
to	O
fit	O
this	O
model	O
using	O
exact	O
em	B
because	O
all	O
the	O
latent	B
qi	O
variables	O
become	O
correlated	O
however	O
one	O
can	O
use	O
variational	B
em	B
et	O
al	O
lvms	O
for	O
graph-structured	O
data	O
qi	O
qj	O
rij	O
i	O
j	O
ab	O
j	O
qi	O
j	O
rij	O
i	O
j	O
ab	O
i	O
qi	O
j	O
figure	O
stochastic	B
block	I
model	I
mixed	B
membership	I
stochastic	B
block	I
model	I
collapsed	B
gibbs	B
sampling	I
et	O
al	O
etc	O
we	O
omit	O
the	O
details	O
are	O
similar	B
to	O
the	O
lda	B
case	O
in	O
et	O
al	O
they	O
lifted	O
the	O
restriction	O
that	O
the	O
number	O
of	O
blocks	O
k	O
be	O
fixed	O
by	O
replacing	O
the	O
dirichlet	B
prior	O
on	O
by	O
a	O
dirichlet	B
process	I
section	O
this	O
is	O
known	O
as	O
the	O
infinite	O
relational	O
model	O
see	O
section	O
for	O
details	O
if	O
we	O
have	O
features	B
associated	O
with	O
each	O
node	O
we	O
can	O
make	O
a	O
discriminative	B
version	O
of	O
this	O
model	O
for	O
example	O
by	O
defining	O
prij	O
rqi	O
a	O
qj	O
b	O
xi	O
xj	O
berrwt	O
abf	O
xj	O
where	O
f	O
xj	O
is	O
some	O
way	O
of	O
combining	O
the	O
feature	O
vectors	O
for	O
example	O
we	O
could	O
use	O
concatenation	O
xj	O
or	O
elementwise	O
product	O
xi	O
xj	O
as	O
in	O
supervised	B
lda	B
the	O
overall	O
model	O
is	O
like	O
a	O
relational	O
extension	B
of	O
the	O
mixture	B
of	I
experts	I
model	O
mixed	B
membership	I
stochastic	B
block	I
model	I
in	O
et	O
al	O
they	O
lifted	O
the	O
restriction	O
that	O
each	O
node	O
only	O
belong	O
to	O
one	O
cluster	O
that	O
is	O
they	O
replaced	O
qi	O
k	O
with	O
i	O
sk	O
this	O
is	O
known	O
as	O
the	O
mixed	B
membership	I
stochastic	B
block	I
model	I
and	O
is	O
similar	B
in	O
spirit	O
to	O
fuzzy	B
clustering	B
or	O
soft	B
clustering	B
note	O
that	O
ik	O
is	O
not	O
the	O
same	O
as	O
pzi	O
kd	O
the	O
former	O
represents	O
ontological	B
uncertainty	B
what	O
degree	B
does	O
each	O
object	O
belong	O
to	O
a	O
cluster	O
wheras	O
the	O
latter	O
represents	O
epistemological	B
uncertainty	B
cluster	O
does	O
an	O
object	O
belong	O
to	O
if	O
we	O
want	O
to	O
combine	O
epistemological	O
and	O
ontological	B
uncertainty	B
we	O
can	O
compute	O
p	O
id	O
in	O
more	O
detail	O
the	O
generative	O
process	O
is	O
as	O
follows	O
first	O
each	O
node	O
picks	O
a	O
distribution	O
over	O
blocks	O
i	O
dir	O
second	O
choose	O
the	O
probability	O
of	O
connecting	O
group	O
a	O
to	O
group	O
b	O
for	O
all	B
pairs	I
of	O
groups	O
ab	O
third	O
for	O
each	O
edge	O
sample	O
two	O
discrete	B
variables	O
one	O
for	O
each	O
direction	O
qi	O
j	O
cat	O
i	O
qi	O
j	O
cat	O
j	O
finally	O
generate	O
each	O
edge	O
rij	O
using	O
the	O
following	O
model	O
prij	O
j	O
a	O
qi	O
j	O
b	O
ab	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
outcasts	O
waverers	O
loyal	O
opposition	O
young	O
turks	O
ambrose	O
boniface	O
mark	O
winfrid	O
elias	O
basil	O
simplicius	O
berthold	O
john	O
bosco	O
victor	O
bonaventure	O
amand	O
louis	O
albert	O
peter	O
gregory	O
hugh	O
figure	O
who-likes-whom	O
graph	B
for	O
sampson	O
s	O
monks	B
one	O
of	O
three	O
groups	O
from	O
figures	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
edo	O
airoldi	O
mixed	O
membership	O
of	O
each	O
monk	O
in	O
see	O
figure	O
for	O
the	O
dgm	B
unlike	O
the	O
regular	B
stochastic	B
block	I
model	I
each	O
node	O
can	O
play	O
a	O
different	O
role	O
depending	O
on	O
who	O
it	O
is	O
connecting	O
to	O
as	O
an	O
illustration	O
of	O
this	O
we	O
will	O
consider	O
a	O
data	O
set	O
that	O
is	O
widely	O
used	O
in	O
the	O
social	B
networks	I
analysis	O
literature	O
the	O
data	O
concerns	O
who-likes-whom	O
amongst	O
of	O
group	O
of	O
monks	B
it	O
was	O
collected	O
by	O
hand	O
in	O
by	O
sampson	O
over	O
a	O
period	B
of	O
months	O
days	O
in	O
the	O
era	O
of	O
social	O
media	O
such	O
as	O
facebook	B
a	O
social	O
network	O
with	O
only	O
people	O
is	O
trivially	O
small	O
but	O
the	O
methods	O
we	O
are	O
discussing	O
can	O
be	O
made	O
to	O
scale	O
figure	O
plots	O
the	O
raw	O
data	O
and	O
figure	O
plots	O
e	O
for	O
each	O
monk	O
where	O
k	O
we	O
see	O
that	O
most	O
of	O
the	O
monk	O
belong	O
to	O
one	O
of	O
the	O
three	O
clusters	B
known	O
as	O
the	O
young	O
turks	O
the	O
outcasts	O
and	O
the	O
loyal	O
opposition	O
however	O
some	O
individuals	O
notably	O
monk	O
belong	O
to	O
two	O
clusters	B
sampson	O
called	O
these	O
monks	B
the	O
waverers	O
it	O
is	O
interesting	O
to	O
see	O
that	O
the	O
model	O
can	O
recover	O
the	O
same	O
kinds	O
of	O
insights	O
as	O
sampson	O
derived	O
by	O
hand	O
one	O
prevalent	O
problem	O
in	O
social	O
network	O
analysis	O
is	O
missing	B
data	I
for	O
example	O
if	O
rij	O
it	O
may	O
be	O
due	O
to	O
the	O
fact	O
that	O
person	O
i	O
and	O
j	O
have	O
not	O
had	O
an	O
opportunity	O
to	O
interact	O
or	O
that	O
data	O
is	O
not	O
available	O
for	O
that	O
interaction	O
as	O
opposed	O
to	O
the	O
fact	O
that	O
these	O
people	O
don	O
t	O
want	O
to	O
interact	O
in	O
other	O
words	O
absence	O
of	O
evidence	B
is	O
not	O
evidence	B
of	O
absence	O
we	O
can	O
model	O
this	O
by	O
modifying	O
the	O
observation	B
model	I
so	O
that	O
with	O
probability	O
we	O
generate	O
a	O
from	O
the	O
background	O
model	O
and	O
we	O
only	O
force	O
the	O
model	O
to	O
explain	O
observed	O
with	O
probability	O
in	O
other	O
words	O
we	O
robustify	O
the	O
observation	B
model	I
to	O
allow	O
for	O
outliers	B
as	O
follows	O
prij	O
rqi	O
j	O
a	O
qi	O
j	O
b	O
ab	O
see	O
et	O
al	O
for	O
details	O
relational	B
topic	B
model	I
in	O
many	O
cases	O
the	O
nodes	B
in	O
our	O
network	O
have	O
atttributes	O
for	O
example	O
if	O
the	O
nodes	B
represent	O
academic	O
papers	O
and	O
the	O
edges	B
represent	O
citations	O
then	O
the	O
attributes	B
include	O
the	O
text	O
of	O
the	O
document	O
itself	O
it	O
is	O
therefore	O
desirable	O
to	O
create	O
a	O
model	O
that	O
can	O
explain	O
the	O
text	O
and	O
the	O
link	O
structure	O
concurrently	O
such	O
a	O
model	O
can	O
predict	O
links	O
given	O
text	O
or	O
even	O
vice	O
versa	O
the	O
relational	B
topic	B
model	I
and	O
blei	O
is	O
one	O
way	O
to	O
do	O
this	O
this	O
is	O
a	O
lvms	O
for	O
relational	O
data	O
b	O
i	O
yil	O
qil	O
qi	O
yjl	O
qjl	O
j	O
qj	O
w	O
rij	O
i	O
j	O
figure	O
dgm	B
for	O
the	O
relational	B
topic	B
model	I
simple	O
extension	B
of	O
supervised	B
lda	B
where	O
the	O
response	B
variable	I
rij	O
represents	O
whether	O
there	O
is	O
an	O
edge	O
between	O
nodes	B
i	O
and	O
j	O
is	O
modeled	O
as	O
follows	O
prij	O
qj	O
sigmwt	O
qj	O
qilk	O
see	O
recall	B
that	O
qi	O
is	O
the	O
empirical	O
topic	B
distribution	O
for	O
document	O
i	O
qik	O
figure	O
li	O
note	O
that	O
it	O
is	O
important	O
that	O
rij	O
depend	O
on	O
the	O
actual	O
topics	O
chosen	O
qi	O
and	O
qj	O
and	O
not	O
on	O
the	O
topic	B
distributions	O
i	O
and	O
j	O
otherwise	O
predictive	B
performance	O
is	O
not	O
as	O
good	O
the	O
if	O
rij	O
is	O
a	O
child	O
of	O
i	O
and	O
j	O
it	O
will	O
be	O
treated	O
as	O
just	O
intuitive	O
reason	O
for	O
this	O
is	O
as	O
follows	O
another	O
word	O
similar	B
to	O
the	O
qil	O
s	O
and	O
yil	O
s	O
but	O
since	O
there	O
are	O
many	O
more	O
words	O
than	O
edges	B
the	O
graph	B
structure	O
information	B
will	O
get	O
washed	O
out	O
by	O
making	O
rij	O
a	O
child	O
of	O
qi	O
and	O
qj	O
the	O
graph	B
information	B
can	O
influence	O
the	O
choice	O
of	O
topics	O
more	O
directly	O
one	O
can	O
fit	O
this	O
model	O
in	O
a	O
manner	O
similar	B
to	O
slda	O
see	O
and	O
blei	O
for	O
details	O
the	O
method	O
does	O
better	O
at	O
predicting	O
missing	B
links	O
than	O
the	O
simpler	O
approach	O
of	O
first	O
fitting	O
an	O
lda	B
model	O
and	O
then	O
using	O
the	O
qi	O
s	O
as	O
inputs	O
to	O
a	O
logistic	B
regression	B
problem	O
the	O
reason	O
is	O
analogous	O
to	O
the	O
superiority	O
of	O
partial	B
least	B
squares	I
to	O
pca	B
linear	B
regression	B
namely	O
that	O
the	O
rtm	O
learns	O
a	O
latent	B
space	O
that	O
is	O
forced	O
to	O
be	O
predictive	B
of	O
the	O
graph	B
structure	O
and	O
words	O
whereas	O
lda	B
might	O
learn	O
a	O
latent	B
space	O
that	O
is	O
not	O
useful	O
for	O
predicting	O
the	O
graph	B
lvms	O
for	O
relational	O
data	O
graphs	O
can	O
be	O
used	O
to	O
represent	O
data	O
which	O
represents	O
the	O
relation	B
amongst	O
variables	O
of	O
a	O
certain	O
type	O
e	O
g	O
friendship	O
relationships	O
between	O
people	O
but	O
often	O
we	O
have	O
multiple	O
types	O
of	O
objects	O
and	O
multiple	O
types	O
of	O
relations	O
for	O
example	O
figure	O
illustrates	O
two	O
relations	O
one	O
between	O
people	O
and	O
people	O
and	O
one	O
between	O
people	O
and	O
movies	O
in	O
general	O
we	O
define	O
a	O
k-ary	O
relation	B
r	O
as	O
a	O
subset	O
of	O
k-tuples	O
of	O
the	O
appropriate	O
types	O
r	O
t	O
k	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
figure	O
example	O
of	O
relational	O
data	O
there	O
are	O
two	O
types	O
of	O
objects	O
people	O
and	O
movies	O
one	O
relation	B
friends	O
people	O
people	O
and	O
one	O
function	O
rates	O
people	O
movie	O
r	O
age	O
and	O
sex	O
are	O
attributes	B
functions	O
of	O
the	O
people	O
class	O
where	O
ti	O
are	O
sets	O
or	O
types	O
a	O
binary	O
pairwise	O
or	O
dyadic	B
relation	B
is	O
a	O
relation	B
defined	O
on	O
pairs	O
of	O
objects	O
for	O
example	O
the	O
seen	O
relation	B
between	O
people	O
and	O
movies	O
might	O
be	O
represented	O
as	O
the	O
set	O
of	O
movies	O
that	O
people	O
have	O
seen	O
we	O
can	O
either	O
represent	O
this	O
explicitly	O
as	O
a	O
set	O
such	O
as	O
seen	O
starwars	O
tombraider	O
jaws	O
or	O
implicitly	O
using	O
an	O
indicator	B
function	I
for	O
the	O
set	O
seenbob	O
seenbob	O
seenalice	O
a	O
relation	B
between	O
two	O
entities	O
of	O
types	O
t	O
and	O
t	O
can	O
be	O
represented	O
as	O
a	O
binary	O
function	O
r	O
t	O
t	O
and	O
hence	O
as	O
a	O
binary	O
matrix	O
this	O
can	O
also	O
be	O
represented	O
as	O
a	O
bipartite	B
graph	B
in	O
which	O
we	O
have	O
nodes	B
of	O
two	O
types	O
if	O
t	O
t	O
this	O
becomes	O
a	O
regular	B
directed	B
graph	B
as	O
in	O
section	O
however	O
there	O
are	O
some	O
situations	O
that	O
are	O
not	O
so	O
easily	O
modelled	O
by	O
graphs	O
but	O
which	O
can	O
still	O
be	O
modelled	O
by	O
relations	O
for	O
example	O
we	O
might	O
have	O
a	O
ternary	O
relation	B
r	O
t	O
t	O
t	O
where	O
say	O
ri	O
j	O
k	O
iff	B
protein	O
i	O
interacts	O
with	O
protein	O
j	O
when	O
chemical	O
k	O
is	O
present	O
this	O
can	O
be	O
modelled	O
by	O
a	O
binary	O
matrix	O
we	O
will	O
give	O
some	O
examples	O
of	O
this	O
in	O
section	O
making	O
probabilistic	O
models	O
of	O
relational	O
data	O
is	O
called	O
statistical	B
relational	I
learning	B
and	O
taskar	O
one	O
approach	O
is	O
to	O
directly	O
model	O
the	O
relationship	O
between	O
the	O
variables	O
using	O
graphical	B
models	I
this	O
is	O
known	O
as	O
probabilistic	B
relational	I
modeling	I
another	O
approach	O
is	O
to	O
use	O
latent	B
variable	I
models	I
as	O
we	O
discuss	O
below	O
infinite	O
relational	O
model	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
stochastic	B
block	I
model	I
to	O
model	O
relational	O
data	O
we	O
just	O
i	O
kt	O
with	O
each	O
entity	O
i	O
of	O
each	O
type	O
t	O
we	O
then	O
define	O
associate	O
a	O
latent	B
variable	O
qt	O
the	O
probability	O
of	O
the	O
relation	B
holding	O
between	O
specific	O
entities	O
by	O
looking	O
up	O
the	O
probability	O
of	O
the	O
relation	B
holding	O
between	O
entities	O
of	O
that	O
type	O
for	O
example	O
if	O
r	O
t	O
t	O
t	O
we	O
have	O
pri	O
j	O
k	O
k	O
c	O
abc	O
i	O
a	O
j	O
b	O
if	O
we	O
allow	O
the	O
number	O
of	O
clusters	B
kt	O
for	O
each	O
type	O
to	O
be	O
unbounded	O
by	O
using	O
a	O
dirichlet	B
process	I
the	O
model	O
is	O
called	O
the	O
infinite	O
relational	O
model	O
et	O
al	O
an	O
essentially	O
lvms	O
for	O
relational	O
data	O
interact	O
with	O
affects	O
causes	O
causes	O
affects	O
affects	O
causes	O
complicates	O
causes	O
complicates	O
affects	O
complicates	O
disrupts	O
affects	O
complicates	O
process	O
of	O
manifestation	O
of	O
affects	O
process	O
of	O
result	O
of	O
manifestation	O
of	O
result	O
of	O
affects	O
process	O
of	O
result	O
of	O
result	O
of	O
result	O
of	O
affects	O
process	O
of	O
manifestation	O
of	O
associated	O
with	O
manifestation	O
of	O
affects	O
process	O
of	O
manifestation	O
of	O
associated	O
with	O
figure	O
illustration	O
of	O
an	O
ontology	B
learned	O
by	O
irm	B
applied	O
to	O
the	O
unified	O
medical	O
language	O
system	O
the	O
boxes	O
represent	O
of	O
the	O
concept	B
clusters	B
predicates	O
that	O
belong	O
to	O
the	O
same	O
cluster	O
are	O
grouped	O
together	O
and	O
associated	O
with	O
edges	B
to	O
which	O
they	O
pertain	O
all	O
links	O
with	O
weight	O
above	O
have	O
been	O
included	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
identical	O
model	O
under	O
the	O
name	O
infinite	O
hidden	B
relational	O
model	O
was	O
concurrently	O
proposed	O
in	O
et	O
al	O
we	O
can	O
fit	O
this	O
model	O
with	O
variational	B
bayes	I
et	O
al	O
or	O
collapsed	B
gibbs	B
sampling	I
et	O
al	O
rather	O
than	O
go	O
into	O
algorithmic	O
detail	O
we	O
just	O
sketch	O
some	O
interesting	O
applications	O
learning	B
ontologies	O
an	O
ontology	B
refers	O
to	O
an	O
organisation	O
of	O
knowledge	O
e	O
g	O
and	O
norvig	O
but	O
it	O
is	O
interesting	O
to	O
try	O
and	O
learn	O
them	O
from	O
data	O
et	O
al	O
they	O
show	O
how	O
this	O
can	O
be	O
done	O
using	O
the	O
irm	B
in	O
ai	B
ontologies	O
are	O
often	O
built	O
by	O
hand	O
in	O
the	O
data	O
comes	O
from	O
the	O
unified	O
medical	O
language	O
system	O
which	O
defines	O
a	O
semantic	B
network	I
with	O
concepts	O
as	O
disease	O
or	O
syndrome	O
diagnostic	O
procedure	O
animal	O
and	O
binary	O
predicates	O
as	O
affects	O
prevents	O
we	O
can	O
represent	O
this	O
as	O
a	O
ternary	O
relation	B
r	O
t	O
t	O
t	O
where	O
t	O
is	O
the	O
set	O
of	O
concepts	O
and	O
t	O
is	O
the	O
set	O
of	O
binary	O
predicates	O
the	O
result	O
is	O
a	O
cube	O
we	O
can	O
then	O
apply	O
the	O
irm	B
to	O
partition	O
the	O
cube	O
into	O
regions	O
of	O
roughly	O
homogoneous	O
response	O
the	O
system	O
found	O
concept	B
clusters	B
and	O
predicate	O
clusters	B
some	O
of	O
these	O
are	O
shown	O
in	O
figure	O
the	O
system	O
learns	O
for	O
example	O
that	O
biological	O
functions	O
affect	O
organisms	O
abc	O
where	O
a	O
represents	O
the	O
biological	O
function	O
cluster	O
b	O
represents	O
the	O
organism	O
cluster	O
and	O
c	O
represents	O
the	O
affects	O
cluster	O
clustering	B
based	O
on	O
relations	O
and	O
features	B
we	O
can	O
also	O
use	O
irm	B
to	O
cluster	O
objects	O
based	O
on	O
their	O
relations	O
and	O
their	O
features	B
for	O
example	O
et	O
al	O
consider	O
a	O
political	O
dataset	O
consisting	O
of	O
countries	O
binary	O
a	O
brazil	O
netherlands	O
uk	O
usa	O
burma	O
indonesia	O
jordan	O
egypt	O
india	O
israel	O
china	O
cuba	O
poland	O
ussr	O
b	O
military	O
alliance	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
t	O
v	O
o	O
g	O
l	O
a	O
n	O
o	O
i	O
t	O
u	O
t	O
i	O
t	O
s	O
n	O
o	O
c	O
l	O
i	O
c	O
o	O
b	O
t	O
s	O
n	O
u	O
m	O
m	O
o	O
c	O
s	O
n	O
o	O
i	O
t	O
c	O
e	O
e	O
e	O
e	O
r	O
f	O
l	O
i	O
t	O
s	O
n	O
u	O
m	O
m	O
o	O
c	O
n	O
o	O
n	O
l	O
c	O
o	O
b	O
n	O
r	O
e	O
t	O
s	O
e	O
w	O
l	O
i	O
s	O
n	O
o	O
i	O
t	O
c	O
e	O
e	O
e	O
e	O
r	O
f	O
o	O
n	O
p	O
h	O
s	O
r	O
o	O
s	O
n	O
e	O
c	O
h	O
g	O
h	O
i	O
y	O
c	O
a	O
r	O
e	O
t	O
i	O
l	O
l	O
i	O
l	O
i	O
e	O
c	O
n	O
e	O
o	O
v	O
c	O
i	O
t	O
s	O
e	O
m	O
o	O
d	O
i	O
s	O
t	O
s	O
n	O
u	O
m	O
m	O
o	O
c	O
n	O
a	O
i	O
r	O
a	O
t	O
i	O
l	O
a	O
t	O
o	O
t	O
t	O
s	O
i	O
t	O
i	O
l	O
e	O
l	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
t	O
v	O
o	O
g	O
e	O
n	O
n	O
o	O
s	O
r	O
e	O
p	O
y	O
r	O
a	O
t	O
i	O
l	O
i	O
m	O
s	O
d	O
o	O
o	O
g	O
e	O
n	O
r	O
o	O
b	O
a	O
e	O
s	O
s	O
k	O
o	O
o	O
b	O
s	O
u	O
o	O
g	O
i	O
i	O
l	O
e	O
r	O
p	O
n	O
g	O
s	O
t	O
r	O
o	O
p	O
x	O
e	O
t	O
n	O
e	O
u	O
q	O
n	O
i	O
l	O
e	O
d	O
n	O
u	O
l	O
c	O
o	O
b	O
l	O
a	O
r	O
t	O
u	O
e	O
n	O
l	O
s	O
n	O
o	O
i	O
t	O
a	O
n	O
s	O
s	O
a	O
s	O
s	O
a	O
n	O
o	O
i	O
t	O
u	O
o	O
v	O
e	O
r	O
t	O
v	O
o	O
g	O
i	O
s	O
n	O
o	O
g	O
i	O
i	O
l	O
e	O
r	O
m	O
u	O
n	O
i	O
s	O
s	O
i	O
r	O
c	O
t	O
v	O
o	O
g	O
y	O
r	O
a	O
t	O
i	O
l	O
i	O
i	O
m	O
g	O
n	O
n	O
e	O
v	O
r	O
e	O
t	O
n	O
i	O
d	O
e	O
m	O
u	O
s	O
n	O
o	O
c	O
y	O
g	O
r	O
e	O
n	O
e	O
p	O
h	O
s	O
r	O
o	O
s	O
n	O
e	O
c	O
e	O
m	O
o	O
s	O
i	O
s	O
u	O
m	O
o	O
r	O
f	O
r	O
a	O
f	O
s	O
e	O
g	O
r	O
u	O
p	O
l	O
l	O
a	O
f	O
n	O
a	O
r	O
i	O
s	O
t	O
n	O
e	O
d	O
u	O
t	O
s	O
n	O
g	O
e	O
r	O
o	O
f	O
i	O
y	O
r	O
t	O
n	O
u	O
o	O
c	O
f	O
o	O
e	O
g	O
a	O
h	O
t	O
g	O
n	O
e	O
l	O
d	O
a	O
o	O
r	O
l	O
i	O
a	O
r	O
s	O
o	O
g	O
n	O
w	O
a	O
l	O
s	O
e	O
g	O
a	O
u	O
g	O
n	O
a	O
l	O
m	O
u	O
n	O
n	O
e	O
k	O
a	O
t	O
d	O
a	O
i	O
t	O
n	O
e	O
s	O
l	O
i	O
a	O
m	O
n	O
g	O
e	O
r	O
o	O
f	O
i	O
s	O
r	O
e	O
k	O
r	O
o	O
w	O
e	O
a	O
m	O
e	O
f	O
l	O
i	O
t	O
e	O
d	O
n	O
i	O
n	O
e	O
t	O
o	O
r	O
p	O
i	O
s	O
t	O
n	O
e	O
m	O
t	O
s	O
e	O
v	O
n	O
i	O
n	O
e	O
k	O
a	O
t	O
d	O
a	O
s	O
u	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
n	O
p	O
o	O
p	O
i	O
a	O
e	O
r	O
a	O
d	O
n	O
a	O
l	O
s	O
o	O
g	O
n	O
s	O
t	O
r	O
a	O
y	O
h	O
c	O
r	O
a	O
n	O
o	O
m	O
h	O
t	O
g	O
n	O
e	O
l	O
d	O
a	O
o	O
r	O
s	O
t	O
n	O
a	O
r	O
g	O
m	O
e	O
i	O
l	O
e	O
b	O
a	O
r	O
a	O
i	O
t	O
e	O
d	O
n	O
i	O
s	O
e	O
i	O
r	O
o	O
a	O
c	O
l	O
l	O
d	O
e	O
y	O
o	O
p	O
m	O
e	O
n	O
u	O
e	O
n	O
o	O
h	O
p	O
e	O
e	O
t	O
n	O
o	O
i	O
t	O
a	O
u	O
p	O
o	O
p	O
e	O
s	O
n	O
e	O
f	O
e	O
d	O
l	O
l	O
s	O
c	O
i	O
l	O
o	O
h	O
t	O
a	O
c	O
s	O
t	O
s	O
e	O
t	O
o	O
r	O
p	O
s	O
t	O
a	O
e	O
r	O
h	O
t	O
p	O
n	O
g	O
sends	O
tourists	O
to	O
exports	O
books	O
to	O
exports	O
to	O
treaties	O
conferences	O
membership	O
of	O
igos	O
c	O
d	O
joint	O
joint	O
membership	O
of	O
ngos	O
e	O
negative	O
behavior	O
negative	O
communications	O
accusations	O
protests	O
f	O
book	O
translations	O
g	O
h	O
economic	O
aid	O
emigration	O
i	O
common	O
bloc	O
membership	O
figure	O
illustration	O
of	O
irm	B
applied	O
to	O
some	O
political	O
data	O
containing	O
features	B
and	O
pairwise	O
interactions	O
top	O
row	O
the	O
partition	O
of	O
the	O
countries	O
into	O
clusters	B
and	O
the	O
features	B
into	O
clusters	B
every	O
second	O
column	O
is	O
labelled	O
with	O
the	O
name	O
of	O
the	O
corresponding	O
feature	O
small	O
squares	O
at	O
bottom	O
these	O
are	O
of	O
the	O
clusters	B
of	O
interaction	O
types	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
predicates	O
representing	O
interaction	O
types	O
between	O
countries	O
sends	O
tourists	O
to	O
economic	O
aid	O
and	O
features	B
communist	O
monarchy	O
to	O
create	O
a	O
binary	O
dataset	O
real-valued	O
features	B
were	O
thresholded	O
at	O
their	O
mean	B
and	O
categorical	B
variables	I
were	O
dummy-encoded	O
the	O
data	O
has	O
types	O
t	O
represents	O
countries	O
t	O
represents	O
interactions	O
and	O
t	O
represents	O
features	B
we	O
have	O
two	O
relations	O
t	O
t	O
t	O
and	O
t	O
t	O
problem	O
therefore	O
combines	O
aspects	O
of	O
both	O
the	O
biclustering	B
model	O
and	O
the	O
ontology	B
discovery	O
model	O
when	O
given	O
multiple	O
relations	O
the	O
irm	B
treats	O
them	O
as	O
conditionally	B
independent	I
in	O
this	O
case	O
we	O
have	O
the	O
results	O
are	O
shown	O
in	O
figure	O
the	O
irm	B
divides	O
the	O
features	B
into	O
clusters	B
the	O
first	O
of	O
which	O
contains	O
noncommunist	O
which	O
captures	O
one	O
of	O
the	O
most	O
important	O
aspects	O
of	O
this	O
cold-war	O
era	O
dataset	O
it	O
also	O
clusters	B
the	O
countries	O
into	O
clusters	B
reflecting	O
natural	O
geo-political	O
groupings	O
us	O
and	O
uk	O
or	O
the	O
communist	O
bloc	O
and	O
the	O
predicates	O
into	O
clusters	B
reflecting	O
similar	B
relationships	O
negative	O
behavior	O
and	O
accusations	O
lvms	O
for	O
relational	O
data	O
probabilistic	B
matrix	B
factorization	I
for	O
collaborative	O
filtering	B
as	O
discussed	O
in	O
section	O
collaborative	O
filtering	B
requires	O
predicting	O
entries	O
in	O
a	O
matrix	O
r	O
t	O
t	O
r	O
where	O
for	O
example	O
ri	O
j	O
is	O
the	O
rating	O
that	O
user	O
i	O
gave	O
to	O
movie	O
j	O
thus	O
we	O
see	O
that	O
cf	O
is	O
a	O
kind	O
of	O
relational	O
learning	B
problem	O
one	O
with	O
particular	O
commercial	O
importance	O
much	O
of	O
the	O
work	O
in	O
this	O
area	O
makes	O
use	O
of	O
the	O
data	O
that	O
netflix	O
made	O
available	O
in	O
their	O
competition	O
in	O
particular	O
a	O
large	O
movie	O
x	O
user	O
ratings	O
matrix	O
is	O
provided	O
the	O
full	B
matrix	O
would	O
have	O
entries	O
but	O
only	O
of	O
the	O
entries	O
are	O
observed	O
so	O
the	O
matrix	O
is	O
extremely	O
sparse	B
in	O
addition	O
the	O
data	O
is	O
quite	O
imbalanced	O
with	O
many	O
users	O
rating	O
fewer	O
than	O
movies	O
and	O
a	O
few	O
users	O
rating	O
over	O
movies	O
the	O
validation	B
set	I
is	O
pairs	O
finally	O
there	O
is	O
a	O
separate	O
test	O
set	O
with	O
pairs	O
for	O
which	O
the	O
ranking	B
is	O
known	O
but	O
withheld	O
from	O
contestants	O
the	O
performance	O
measure	O
is	O
root	B
mean	B
square	I
error	I
rm	O
se	O
ui	O
xmi	O
n	O
where	O
xmi	O
ui	O
is	O
the	O
true	O
rating	O
of	O
user	O
ui	O
on	O
movie	O
mi	B
and	O
xmi	O
ui	O
is	O
the	O
prediction	O
the	O
baseline	O
system	O
known	O
as	O
cinematch	O
had	O
an	O
rmse	O
on	O
the	O
training	B
set	I
of	O
and	O
on	O
the	O
test	O
set	O
of	O
to	O
qualify	O
for	O
the	O
grand	O
prize	O
teams	O
needed	O
to	O
reduce	O
the	O
test	O
rmse	O
by	O
i	O
e	O
get	O
a	O
test	O
rmse	O
of	O
or	O
less	O
we	O
will	O
discuss	O
some	O
of	O
the	O
basic	O
methods	O
used	O
byt	O
the	O
winning	O
team	O
below	O
since	O
the	O
ratings	O
are	O
drawn	O
from	O
the	O
set	O
it	O
is	O
tempting	O
to	O
use	O
a	O
categorical	B
observation	B
model	I
however	O
this	O
does	O
not	O
capture	O
the	O
fact	O
that	O
the	O
ratings	O
are	O
ordered	O
although	O
we	O
could	O
use	O
an	O
ordinal	B
observation	B
model	I
in	O
practice	O
people	O
use	O
a	O
gaussian	B
observation	B
model	I
for	O
simplicity	O
one	O
way	O
to	O
make	O
the	O
model	O
better	O
match	O
the	O
data	O
is	O
to	O
pass	O
the	O
model	O
s	O
predicted	O
mean	B
response	O
through	O
a	O
sigmoid	B
and	O
then	O
to	O
map	O
the	O
interval	O
to	O
and	O
mnih	O
alternatively	O
we	O
can	O
make	O
the	O
data	O
a	O
better	O
match	O
to	O
the	O
gaussian	B
model	O
by	O
transforming	O
the	O
data	O
using	O
rij	O
rij	O
and	O
merugu	O
we	O
could	O
use	O
the	O
irm	B
for	O
the	O
cf	O
task	O
by	O
associating	O
a	O
discrete	B
latent	B
variable	O
for	O
each	O
user	O
i	O
and	O
for	O
each	O
movie	O
or	O
video	O
qv	O
qu	O
prij	O
rqu	O
j	O
b	O
n	O
ab	O
j	O
and	O
then	O
defining	O
i	O
a	O
qv	O
this	O
is	O
just	O
another	O
example	O
of	O
co-clustering	B
we	O
can	O
also	O
extend	O
the	O
model	O
to	O
generate	O
side	B
information	B
such	O
as	O
attributes	B
about	O
each	O
user	O
andor	O
movie	O
see	O
figure	O
for	O
an	O
illustration	O
another	O
possibility	O
is	O
to	O
replace	O
the	O
discrete	B
latent	B
variables	O
with	O
continuous	O
latent	B
variables	O
i	O
sku	O
and	O
v	O
j	O
skv	O
however	O
it	O
has	O
been	O
found	O
e	O
g	O
and	O
banerjee	O
that	O
u	O
one	O
obtains	O
much	O
better	O
results	O
by	O
using	O
unconstrained	O
real-valued	O
latent	B
factors	B
for	O
each	O
user	O
ui	O
r	O
we	O
then	O
use	O
a	O
likelihood	B
of	O
the	O
form	O
k	O
and	O
each	O
movie	O
vj	O
r	O
prij	O
rui	O
vj	O
n	O
i	O
vj	O
good	O
results	O
with	O
discrete	B
latent	B
variables	O
have	O
been	O
obtained	O
on	O
some	O
datasets	O
that	O
are	O
smaller	O
than	O
netflix	O
such	O
as	O
movielens	O
and	O
eachmovie	O
however	O
these	O
datasets	O
are	O
much	O
easier	O
to	O
predict	O
because	O
there	O
is	O
less	O
imbalance	O
between	O
the	O
number	O
of	O
reviews	O
performed	O
by	O
different	O
users	O
netflix	O
some	O
users	O
have	O
rated	O
more	O
than	O
movies	O
whereas	O
others	O
have	O
rated	O
less	O
than	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
figure	O
visualization	O
of	O
a	O
small	O
relational	O
dataset	O
where	O
we	O
have	O
one	O
relation	B
likesuser	O
movie	O
and	O
features	B
for	O
movies	O
genre	O
and	O
users	O
occupation	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
zhao	O
xu	O
r	O
o	O
t	O
c	O
e	O
v	O
r	O
o	O
t	O
c	O
a	O
f	O
v	O
v	O
vj	O
rij	O
t	O
t	O
ui	O
u	O
u	O
the	O
royal	O
tenenbau	O
m	O
s	O
julien	O
donkey	O
boy	O
punch	O
drunk	O
love	O
i	O
heart	O
huckabees	O
lost	O
in	O
translation	O
being	O
john	O
m	O
alkovich	O
belle	O
de	O
jour	O
kill	O
bill	O
vol	O
natural	O
born	O
killers	O
citizen	O
kane	O
scarface	O
freddy	O
g	O
ot	O
fingered	O
half	O
baked	O
freddy	O
vs	O
jason	O
road	O
trip	O
the	O
longest	O
yard	O
the	O
fast	O
and	O
the	O
furious	O
arm	O
ageddon	O
catwo	O
m	O
an	O
coyote	O
ugly	O
the	O
wizard	O
of	O
oz	O
runaway	O
bride	O
sister	O
act	O
step	O
m	O
o	O
m	O
m	O
aid	O
in	O
m	O
anhattan	O
annie	O
hall	O
sophie	O
s	O
choice	O
m	O
oonstruck	O
the	O
w	O
ay	O
w	O
e	O
w	O
ere	O
the	O
sound	O
of	O
m	O
usic	O
the	O
w	O
altons	O
season	O
factor	B
vector	O
figure	O
a	O
dgm	B
for	O
probabilistic	B
matrix	B
factorization	I
visualization	O
of	O
the	O
first	O
two	O
factors	B
in	O
the	O
pmf	B
model	O
estimated	O
from	O
the	O
netflix	O
challenge	O
data	O
each	O
movie	O
j	O
is	O
plotted	O
at	O
the	O
location	O
specified	O
vj	O
on	O
the	O
left	O
we	O
have	O
low-brow	O
humor	O
and	O
horror	O
movies	O
baked	O
freddy	O
vs	O
jason	O
and	O
on	O
the	O
right	O
we	O
have	O
more	O
serious	O
dramas	O
s	O
choice	O
moonstruck	O
on	O
the	O
top	O
we	O
have	O
critically	O
acclaimed	O
independent	O
movies	O
love	O
i	O
heart	O
huckabees	O
and	O
on	O
the	O
bottom	O
we	O
have	O
mainstream	O
hollywood	O
blockbusters	O
runway	O
bride	O
the	O
wizard	O
of	O
oz	O
is	O
right	O
in	O
the	O
middle	O
of	O
these	O
axes	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yehuda	O
koren	O
this	O
has	O
been	O
called	O
probabilistic	B
matrix	B
factorization	I
and	O
mnih	O
see	O
figure	O
for	O
the	O
dgm	B
the	O
intuition	O
behind	O
this	O
method	O
is	O
that	O
each	O
user	O
and	O
each	O
movie	O
get	O
embedded	O
into	O
the	O
same	O
low-dimensional	O
continuous	O
space	O
figure	O
if	O
a	O
user	O
is	O
close	O
to	O
a	O
movie	O
in	O
that	O
space	O
they	O
are	O
likely	O
to	O
rate	B
it	O
highly	O
all	O
of	O
the	O
best	O
entries	O
in	O
the	O
netflix	O
competition	O
used	O
this	O
approach	O
in	O
one	O
form	O
or	O
pmf	B
is	O
closely	O
related	O
to	O
the	O
svd	B
in	O
particular	O
if	O
there	O
is	O
no	O
missing	B
data	I
then	O
computing	O
the	O
mle	B
for	O
the	O
ui	O
s	O
and	O
the	O
vj	O
s	O
is	O
equivalent	O
to	O
finding	O
a	O
rank	O
k	O
approximation	O
to	O
r	O
however	O
as	O
soon	O
as	O
we	O
have	O
missing	B
data	I
the	O
problem	O
becomes	O
non-convex	O
as	O
shown	O
in	O
the	O
winning	O
entry	O
was	O
actually	O
an	O
ensemble	B
of	O
different	O
methods	O
including	O
pmf	B
nearest	B
neighbor	I
methods	O
etc	O
lvms	O
for	O
relational	O
data	O
e	O
s	O
m	O
r	O
netflix	O
baseline	O
score	O
svd	B
pmf	B
constrained	O
pmf	B
epochs	O
e	O
s	O
m	O
r	O
plain	O
wbiases	O
wimplicit	O
feedback	O
wtemporal	O
dynamics	O
millions	O
of	O
parameters	O
figure	O
rmse	O
on	O
the	O
validation	B
set	I
for	O
different	O
pmf	B
variants	O
vs	O
number	O
of	O
passes	O
through	O
the	O
data	O
svd	B
is	O
the	O
unregularized	O
version	O
u	O
v	O
corresponds	O
to	O
u	O
and	O
v	O
while	O
corresponds	O
to	O
u	O
and	O
v	O
corresponds	O
to	O
a	O
version	O
where	O
the	O
mean	B
and	O
diagonal	B
covariance	B
of	O
the	O
gaussian	B
prior	O
were	O
learned	O
from	O
data	O
from	O
figure	O
of	O
and	O
mnih	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
rmse	O
on	O
the	O
test	O
set	O
portion	O
vs	O
number	O
of	O
parameters	O
for	O
several	O
different	O
models	O
plain	O
is	O
the	O
baseline	O
pmf	B
with	O
suitably	O
chosen	O
u	O
v	O
with	O
biases	O
adds	O
fi	O
and	O
gj	O
offset	O
terms	O
with	O
implicit	B
feedback	I
with	O
temporal	O
dynamics	O
allows	O
the	O
offset	O
terms	O
to	O
change	O
over	O
time	O
the	O
netflix	O
baseline	O
system	O
achieves	O
an	O
rmse	O
of	O
and	O
the	O
grand	O
prize	O
s	O
required	O
accuracy	O
is	O
was	O
obtained	O
on	O
september	O
figure	O
generated	O
by	O
netflixresultsplot	O
from	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
yehuda	O
koren	O
and	O
jaakkola	O
and	O
standard	O
svd	B
methods	O
cannot	O
be	O
applied	O
netflix	O
challenge	O
only	O
about	O
of	O
the	O
matrix	O
is	O
observed	O
the	O
most	O
straightforward	O
way	O
to	O
fit	O
the	O
pmf	B
model	O
is	O
to	O
minimize	O
the	O
overall	O
nll	B
i	O
vj	O
that	O
in	O
the	O
ju	O
v	O
log	O
pru	O
v	O
o	O
log	O
where	O
oij	O
if	O
user	O
i	O
has	O
seen	O
movie	O
j	O
since	O
this	O
is	O
non-convex	O
we	O
can	O
just	O
find	O
a	O
locally	O
optimal	O
mle	B
since	O
the	O
netflix	O
data	O
is	O
so	O
large	O
million	O
observed	O
entries	O
it	O
is	O
common	O
to	O
use	O
stochastic	B
gradient	B
descent	I
for	O
this	O
task	O
the	O
gradient	O
for	O
ui	O
is	O
given	O
by	O
dj	O
dui	O
d	O
dui	O
ioij	O
ut	O
i	O
ij	O
joij	O
eijvj	O
where	O
eij	O
rij	O
ut	O
i	O
has	O
watched	O
the	O
update	O
takes	O
the	O
following	O
simple	O
form	O
i	O
vj	O
is	O
the	O
error	O
term	O
by	O
stochastically	O
sampling	O
a	O
single	O
movie	O
j	O
that	O
user	O
ui	O
ui	O
eijvj	O
where	O
is	O
the	O
learning	B
rate	B
the	O
update	O
for	O
vj	O
is	O
similar	B
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
of	O
course	O
just	O
maximizing	O
the	O
likelihood	B
results	O
in	O
overfitting	O
as	O
shown	O
in	O
figure	O
we	O
can	O
regularize	O
this	O
by	O
imposing	O
gaussian	B
priors	O
n	O
v	O
v	O
n	O
u	O
u	O
pu	O
v	O
i	O
j	O
if	O
we	O
use	O
u	O
v	O
u	O
u	O
ik	O
and	O
v	O
v	O
ik	O
the	O
new	O
objective	O
becomes	O
ju	O
v	O
log	O
pr	O
u	O
vo	O
ioij	O
ut	O
u	O
v	O
i	O
i	O
j	O
const	O
i	O
j	O
where	O
we	O
have	O
defined	O
u	O
v	O
by	O
varying	O
the	O
regularizers	O
we	O
can	O
reduce	O
the	O
effect	O
of	O
overfitting	O
as	O
shown	O
in	O
figure	O
we	O
can	O
find	O
map	O
estimates	O
using	O
stochastic	B
gradient	B
descent	I
we	O
can	O
also	O
compute	O
approximate	O
posteriors	O
using	O
variational	B
bayes	I
and	O
raiko	O
u	O
and	O
v	O
if	O
we	O
use	O
diagonal	B
covariances	O
for	O
the	O
priors	O
we	O
can	O
penalize	O
each	O
latent	B
dimension	O
by	O
a	O
different	O
amount	O
also	O
if	O
we	O
use	O
non-zero	O
means	O
for	O
the	O
priors	O
we	O
can	O
account	O
for	O
offset	O
terms	O
optimizing	O
the	O
prior	O
parameters	O
u	O
u	O
v	O
v	O
at	O
the	O
same	O
time	O
as	O
the	O
model	O
parameters	O
v	O
is	O
a	O
way	O
to	O
create	O
an	O
adaptive	O
prior	O
this	O
avoids	O
the	O
need	O
to	O
search	O
for	O
the	O
optimal	O
values	O
of	O
u	O
and	O
v	O
and	O
gives	O
even	O
better	O
results	O
as	O
shown	O
in	O
figure	O
it	O
turns	O
out	O
that	O
much	O
of	O
the	O
variation	O
in	O
the	O
data	O
can	O
be	O
explained	O
by	O
movie-specific	O
or	O
user-specific	O
effects	O
for	O
example	O
some	O
movies	O
are	O
popular	O
for	O
all	O
types	O
of	O
users	O
and	O
some	O
users	O
give	O
low	O
scores	B
for	O
all	O
types	O
of	O
movies	O
we	O
can	O
model	O
this	O
by	O
allowing	O
for	O
user	O
and	O
movie	O
specific	O
offset	O
or	O
bias	B
terms	O
as	O
follows	O
prij	O
rui	O
vj	O
n	O
i	O
vj	O
fi	O
gj	O
where	O
is	O
the	O
overall	O
mean	B
fi	O
is	O
the	O
user	O
bias	B
gj	O
is	O
the	O
movie	O
bias	B
and	O
ut	O
i	O
vj	O
is	O
the	O
interaction	O
term	O
this	O
is	O
equivalent	O
to	O
applying	O
pmf	B
just	O
to	O
the	O
residual	B
matrix	O
and	O
gives	O
much	O
better	O
results	O
as	O
shown	O
in	O
figure	O
we	O
can	O
estimate	O
the	O
fi	O
gj	O
and	O
terms	O
using	O
stochastic	B
gradient	B
descent	I
just	O
as	O
we	O
estimated	O
u	O
v	O
and	O
we	O
can	O
also	O
allow	O
the	O
bias	B
terms	O
to	O
evolve	O
over	O
time	O
to	O
reflect	O
the	O
changing	O
preferences	B
of	O
users	O
this	O
is	O
important	O
since	O
in	O
the	O
netflix	O
competition	O
the	O
test	O
data	O
was	O
more	O
recent	O
than	O
the	O
training	O
data	O
figure	O
shows	O
that	O
allowing	O
for	O
temporal	O
dynamics	O
can	O
help	O
a	O
lot	O
often	O
we	O
also	O
have	O
side	B
information	B
of	O
various	O
kinds	O
in	O
the	O
netflix	O
competition	O
entrants	O
knew	O
which	O
movies	O
the	O
user	O
had	O
rated	O
in	O
the	O
test	O
set	O
even	O
though	O
they	O
did	O
not	O
know	O
the	O
values	O
of	O
these	O
ratings	O
that	O
is	O
they	O
knew	O
the	O
value	O
of	O
the	O
o	O
matrix	O
even	O
on	O
the	O
test	O
set	O
if	O
a	O
user	O
chooses	O
to	O
rate	B
a	O
movie	O
it	O
is	O
likely	O
because	O
they	O
have	O
seen	O
it	O
which	O
in	O
turns	O
means	O
they	O
thought	O
they	O
would	O
like	O
it	O
thus	O
the	O
very	O
act	O
of	O
rating	O
reveals	O
information	B
conversely	O
if	O
a	O
user	O
chooses	O
not	O
rate	B
a	O
movie	O
it	O
suggests	O
they	O
knew	O
they	O
would	O
not	O
like	O
it	O
so	O
the	O
data	O
is	O
not	B
missing	B
at	I
random	I
e	O
g	O
and	O
zemel	O
exploiting	O
this	O
can	O
improve	O
performance	O
as	O
shown	O
in	O
figure	O
in	O
real	O
problems	O
information	B
on	O
the	O
test	O
set	O
is	O
not	O
available	O
however	O
we	O
often	O
know	O
which	O
movies	O
the	O
user	O
has	O
watched	O
or	O
declined	O
to	O
restricted	O
boltzmann	O
machines	O
watch	O
even	O
if	O
they	O
did	O
not	O
rate	B
them	O
is	O
called	O
implicit	B
feedback	I
and	O
this	O
can	O
be	O
used	O
as	O
useful	O
side	B
information	B
another	O
source	O
of	O
side	B
information	B
concerns	O
the	O
content	O
of	O
the	O
movie	O
such	O
as	O
the	O
movie	O
genre	O
the	O
list	O
of	O
the	O
actors	O
or	O
a	O
synopsis	O
of	O
the	O
plot	O
this	O
can	O
be	O
denoted	O
by	O
xv	O
the	O
features	B
the	O
case	O
where	O
we	O
just	O
have	O
the	O
id	O
of	O
the	O
video	O
we	O
can	O
treat	O
xv	O
as	O
a	O
the	O
video	O
dimensional	O
bit	O
vector	O
with	O
just	O
one	O
bit	O
turned	O
on	O
we	O
may	O
also	O
know	O
features	B
about	O
the	O
user	O
which	O
we	O
can	O
denote	O
by	O
xu	O
in	O
some	O
cases	O
we	O
only	O
know	O
if	O
the	O
user	O
clicked	O
on	O
the	O
video	O
or	O
not	O
that	O
is	O
we	O
may	O
not	O
have	O
a	O
numerical	O
rating	O
we	O
can	O
then	O
modify	O
the	O
model	O
as	O
follows	O
pru	O
vxu	O
xv	O
berru	O
vuxut	O
where	O
u	O
is	O
a	O
k	O
matrix	O
and	O
v	O
is	O
a	O
k	O
matrix	O
can	O
incorporate	O
an	O
offset	O
term	O
by	O
appending	O
a	O
to	O
xu	O
and	O
xv	O
in	O
the	O
usual	O
way	O
a	O
method	O
for	O
computing	O
the	O
approximate	O
posterior	O
pu	O
vd	O
in	O
an	O
online	O
fashion	O
using	O
adf	B
and	O
ep	B
was	O
described	O
in	O
et	O
al	O
this	O
was	O
implemented	O
by	O
microsoft	B
and	O
has	O
been	O
deployed	O
to	O
predict	O
click	O
through	O
rates	O
on	O
all	O
the	O
ads	O
used	O
by	O
bing	B
unfortunately	O
fitting	O
this	O
model	O
just	O
from	O
positive	O
binary	O
data	O
can	O
result	O
in	O
an	O
over	O
prediction	O
of	O
links	O
since	O
no	O
negative	B
examples	I
are	O
included	O
better	O
performance	O
is	O
obtained	O
if	O
one	O
has	O
access	O
to	O
the	O
set	O
of	O
all	O
videos	O
shown	O
to	O
the	O
user	O
of	O
which	O
at	O
most	O
one	O
was	O
picked	O
data	O
of	O
this	O
form	O
is	O
known	O
as	O
an	O
impression	B
log	I
in	O
this	O
case	O
we	O
can	O
use	O
a	O
multinomial	B
model	O
instead	O
of	O
a	O
binary	O
model	O
in	O
et	O
al	O
this	O
was	O
shown	O
to	O
work	O
much	O
better	O
than	O
a	O
binary	O
model	O
to	O
understand	O
why	O
suppose	O
some	O
is	O
presented	O
with	O
a	O
choice	O
of	O
an	O
action	B
movie	O
starring	O
arnold	O
schwarzenegger	O
an	O
action	B
movie	O
starring	O
vin	O
diesel	O
and	O
a	O
comedy	O
starring	O
hugh	O
grant	O
if	O
the	O
user	O
picks	O
arnold	O
schwarzenegger	O
we	O
learn	O
not	O
only	O
that	O
they	O
like	O
prefer	O
action	B
movies	O
to	O
comedies	O
but	O
also	O
that	O
they	O
prefer	O
schwarzenegger	O
to	O
diesel	O
this	O
is	O
more	O
informative	O
than	O
just	O
knowing	O
that	O
they	O
like	O
schwarzenegger	O
and	O
action	B
movies	O
restricted	O
boltzmann	O
machines	O
so	O
far	O
all	O
the	O
models	O
we	O
have	O
proposed	O
in	O
this	O
chapter	O
have	O
been	O
representable	O
by	O
directed	B
graphical	B
models	I
but	O
some	O
models	O
are	O
better	O
represented	O
using	O
undirected	B
graphs	O
for	O
example	O
the	O
boltzmann	B
machine	I
et	O
al	O
is	O
a	O
pairwise	B
mrf	B
with	O
hidden	B
nodes	B
h	O
and	O
visible	B
nodes	B
v	O
as	O
shown	O
in	O
figure	O
the	O
main	O
problem	O
with	O
the	O
boltzmann	B
machine	I
is	O
that	O
exact	O
inference	B
is	O
intractable	O
and	O
even	O
approximate	B
inference	B
using	O
e	O
g	O
gibbs	B
sampling	I
can	O
be	O
slow	O
however	O
suppose	O
we	O
restrict	O
the	O
architecture	O
so	O
that	O
the	O
nodes	B
are	O
arranged	O
in	O
layers	O
and	O
so	O
that	O
there	O
are	O
no	O
connections	O
between	O
nodes	B
within	O
the	O
same	O
layer	O
figure	O
then	O
the	O
model	O
has	O
the	O
form	O
ph	O
v	O
z	O
rkvr	O
hk	O
where	O
r	O
is	O
the	O
number	O
of	O
visible	B
variables	I
k	O
is	O
the	O
number	O
of	O
hidden	B
variables	I
and	O
v	O
plays	O
the	O
role	O
of	O
y	O
earlier	O
in	O
this	O
chapter	O
this	O
model	O
is	O
known	O
as	O
a	O
restricted	B
boltzmann	B
machine	I
or	O
a	O
harmonium	B
an	O
rbm	B
is	O
a	O
special	O
case	O
of	O
a	O
product	B
of	I
experts	I
which	O
is	O
so-called	O
because	O
we	O
are	O
multiplying	O
together	O
a	O
set	O
of	O
experts	O
potential	O
functions	O
on	O
each	O
edge	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
h	O
v	O
figure	O
a	O
general	O
boltzmann	B
machine	I
with	O
an	O
arbitrary	O
graph	B
structure	O
the	O
shaded	O
nodes	B
are	O
partitioned	O
into	O
input	O
and	O
output	O
although	O
the	O
model	O
is	O
actually	O
symmetric	B
and	O
defines	O
a	O
joint	O
density	O
on	O
all	O
the	O
nodes	B
a	O
restricted	B
boltzmann	B
machine	I
with	O
a	O
bipartite	O
structure	O
note	O
the	O
lack	O
of	O
intra-layer	O
connections	O
and	O
then	O
normalizing	O
whereas	O
in	O
a	O
mixture	B
of	I
experts	I
we	O
take	O
a	O
convex	B
combination	I
of	O
normalized	O
distributions	O
the	O
intuitive	O
reason	O
why	O
poe	O
models	O
might	O
work	O
better	O
than	O
a	O
mixture	B
is	O
that	O
each	O
expert	O
can	O
enforce	O
a	O
constraint	O
the	O
expert	O
has	O
a	O
value	O
which	O
is	O
or	O
or	O
a	O
don	O
t	O
care	O
condition	O
the	O
expert	O
has	O
value	O
by	O
multiplying	O
these	O
experts	O
together	O
in	O
different	O
ways	O
we	O
can	O
create	O
sharp	O
distributions	O
which	O
predict	O
data	O
which	O
satisfies	O
the	O
specified	O
constraints	O
and	O
teh	O
for	O
example	O
consider	O
a	O
distributed	O
model	O
of	O
text	O
a	O
given	O
document	O
might	O
have	O
the	O
topics	O
government	O
mafia	O
and	O
playboy	O
if	O
we	O
multiply	O
the	O
predictions	O
of	O
each	O
topic	B
together	O
the	O
model	O
may	O
give	O
very	O
high	O
probability	O
to	O
the	O
word	O
berlusconi	O
and	O
hinton	O
by	O
contrast	O
adding	O
together	O
experts	O
can	O
only	O
make	O
the	O
distribution	O
broader	O
figure	O
typically	O
the	O
hidden	B
nodes	B
in	O
an	O
rbm	B
are	O
binary	O
so	O
h	O
specifies	O
which	O
constraints	O
are	O
active	O
it	O
is	O
worth	O
comparing	O
this	O
with	O
the	O
directed	B
models	O
we	O
have	O
discussed	O
in	O
a	O
mixture	B
model	I
we	O
have	O
one	O
hidden	B
variable	I
q	O
k	O
we	O
can	O
represent	O
this	O
using	O
a	O
set	O
of	O
k	O
bits	B
with	O
the	O
restriction	O
that	O
exactly	O
one	O
bit	O
is	O
on	O
at	O
a	O
time	O
this	O
is	O
called	O
a	O
localist	B
encoding	I
since	O
only	O
one	O
hidden	B
unit	O
is	O
used	O
to	O
generate	O
the	O
response	O
vector	O
this	O
is	O
analogous	O
to	O
the	O
hypothetical	O
notion	O
of	O
grandmother	B
cells	I
in	O
the	O
brain	O
that	O
are	O
able	O
to	O
recognize	O
only	O
one	O
kind	O
of	O
object	O
by	O
contrast	O
an	O
rbm	B
uses	O
a	O
distributed	B
encoding	I
where	O
many	O
units	O
are	O
involved	O
in	O
generating	O
each	O
output	O
models	O
that	O
used	O
vector-valued	O
hidden	B
variables	I
such	O
as	O
sk	O
as	O
in	O
mpca	B
lda	B
or	O
z	O
r	O
k	O
as	O
in	O
epca	B
also	O
use	O
distributed	O
encodings	O
the	O
main	O
difference	O
between	O
an	O
rbm	B
and	O
directed	B
two-layer	O
models	O
is	O
that	O
the	O
hidden	B
variables	I
are	O
conditionally	B
independent	I
given	O
the	O
visible	B
variables	I
so	O
the	O
posterior	O
factorizes	O
phv	O
phkv	O
k	O
this	O
makes	O
inference	B
much	O
simpler	O
than	O
in	O
a	O
directed	B
model	O
since	O
we	O
can	O
estimate	O
each	O
hk	O
silvio	O
berlusconi	O
is	O
the	O
current	O
prime	O
minister	O
of	O
italy	O
restricted	O
boltzmann	O
machines	O
visible	B
binary	O
gaussian	B
categorical	B
multiple	O
categorical	B
gaussian	B
binary	O
hidden	B
binary	O
binary	O
binary	O
binary	O
gaussian	B
gaussian	B
name	O
binary	O
rbm	B
gaussian	B
rbm	B
categorical	B
rbm	B
replicated	O
softmax	B
undirected	B
lda	B
undirected	B
pca	B
undirected	B
binary	O
pca	B
reference	O
and	O
sutton	O
et	O
al	O
and	O
hinton	O
and	O
movellan	O
and	O
sutton	O
table	O
summary	O
of	O
different	O
kinds	O
of	O
rbm	B
independently	O
and	O
in	O
parallel	O
as	O
in	O
a	O
feedforward	B
neural	B
network	I
the	O
disadvantage	O
is	O
that	O
training	O
undirected	B
models	O
is	O
much	O
harder	O
as	O
we	O
discuss	O
below	O
varieties	O
of	O
rbms	O
in	O
this	O
section	O
we	O
describe	O
various	O
forms	O
of	O
rbms	O
by	O
defining	O
different	O
pairwise	O
potential	O
functions	O
see	O
table	O
for	O
a	O
summary	O
all	O
of	O
these	O
are	O
special	O
cases	O
of	O
the	O
exponential	B
family	B
harmonium	B
et	O
al	O
binary	O
rbms	O
the	O
most	O
common	O
form	O
of	O
rbm	B
has	O
binary	O
hidden	B
nodes	B
and	O
binary	O
visible	B
nodes	B
the	O
joint	B
distribution	I
then	O
has	O
the	O
following	O
form	O
z	O
pv	O
h	O
exp	O
ev	O
h	O
ev	O
h	O
vrhkwrk	O
wh	O
vt	O
b	O
ht	O
c	O
exp	O
ev	O
h	O
z	O
vrbr	O
hkck	O
v	O
h	O
where	O
e	O
is	O
the	O
energy	B
function	I
w	O
is	O
a	O
r	O
k	O
weight	O
matrix	O
b	O
are	O
the	O
visible	B
bias	B
terms	O
c	O
are	O
the	O
hidden	B
bias	B
terms	O
and	O
b	O
c	O
are	O
all	O
the	O
parameters	O
for	O
notational	O
simplicity	O
we	O
will	O
absorb	O
the	O
bias	B
terms	O
into	O
the	O
weight	O
matrix	O
by	O
clamping	B
dummy	O
units	O
and	O
and	O
setting	O
c	O
and	O
b	O
note	O
that	O
naively	O
computing	O
z	O
takes	O
time	O
but	O
we	O
can	O
reduce	O
this	O
to	O
time	O
when	O
using	O
a	O
binary	O
rbm	B
the	O
posterior	O
can	O
be	O
computed	O
as	O
follows	O
berhksigmwt	O
phv	O
pvh	O
phkv	O
k	O
by	O
symmetry	O
one	O
can	O
show	O
that	O
we	O
can	O
generate	O
data	O
given	O
the	O
hidden	B
variables	I
as	O
follows	O
pvrh	O
bervrsigmwt	O
rh	O
r	O
r	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
we	O
can	O
write	O
this	O
in	O
matrix-vetor	O
notation	O
as	O
follows	O
e	O
sigmwt	O
v	O
e	O
sigmwh	O
the	O
weights	O
in	O
w	O
are	O
called	O
the	O
generative	B
weights	I
since	O
they	O
are	O
used	O
to	O
generate	O
the	O
observations	O
and	O
the	O
weights	O
in	O
wt	O
are	O
called	O
the	O
recognition	B
weights	I
since	O
they	O
are	O
used	O
to	O
recognize	O
the	O
input	O
from	O
equation	O
we	O
see	O
that	O
we	O
activate	O
hidden	B
node	O
k	O
in	O
proportion	O
to	O
how	O
much	O
the	O
input	O
vector	O
v	O
looks	O
like	O
the	O
weight	B
vector	I
wk	O
to	O
scaling	O
factors	B
thus	O
each	O
hidden	B
node	O
captures	O
certain	O
features	B
of	O
the	O
input	O
as	O
encoded	O
in	O
its	O
weight	B
vector	I
similar	B
to	O
a	O
feedforward	B
neural	B
network	I
categorical	B
rbm	B
we	O
can	O
extend	O
the	O
binary	O
rbm	B
to	O
categorical	B
visible	B
variables	I
by	O
using	O
a	O
encoding	O
where	O
c	O
is	O
the	O
number	O
of	O
states	O
for	O
each	O
vir	O
we	O
define	O
a	O
new	O
energy	B
function	I
as	O
follows	O
et	O
al	O
salakhutdinov	O
and	O
hinton	O
ev	O
h	O
the	O
full	B
conditionals	O
are	O
given	O
by	O
pvrh	O
catsbc	O
phk	O
sigmck	O
vc	B
rhkw	O
c	O
rk	O
hkw	O
c	O
k	O
r	O
vc	B
rw	O
c	O
rk	O
rkc	O
r	O
vc	B
rbc	O
hkck	O
gaussian	B
rbm	B
r	O
c	O
we	O
can	O
generalize	B
the	O
model	O
to	O
handle	O
real-valued	O
data	O
in	O
particular	O
a	O
gaussian	B
rbm	B
has	O
the	O
following	O
energy	B
function	I
ev	O
h	O
wrkhkvr	O
akhk	O
the	O
parameters	O
of	O
the	O
model	O
are	O
ak	O
br	O
have	O
assumed	O
the	O
data	O
is	O
standardized	B
so	O
we	O
fix	O
the	O
variance	B
to	O
compare	O
this	O
to	O
a	O
gaussian	B
in	O
information	B
form	I
ncv	O
exp	O
t	O
v	O
vt	O
v	O
where	O
we	O
see	O
that	O
we	O
have	O
set	O
i	O
and	O
given	O
by	O
k	O
hkwk	O
thus	O
the	O
mean	B
is	O
k	O
hkwk	O
the	O
full	B
conditionals	O
which	O
are	O
needed	O
for	O
inference	B
and	O
restricted	O
boltzmann	O
machines	O
learning	B
are	O
given	O
by	O
pvrh	O
wrkhk	O
phk	O
sigm	O
ck	O
wrkvr	O
k	O
r	O
we	O
see	O
that	O
each	O
visible	B
unit	O
has	O
a	O
gaussian	B
distribution	O
whose	O
mean	B
is	O
a	O
function	O
of	O
the	O
hidden	B
bit	O
vector	O
more	O
powerful	O
models	O
which	O
make	O
the	O
depend	O
on	O
the	O
hidden	B
state	B
can	O
also	O
be	O
developed	O
and	O
hinton	O
rbms	O
with	O
gaussian	B
hidden	B
units	I
if	O
we	O
use	O
gaussian	B
latent	B
variables	O
and	O
gaussian	B
visible	B
variables	I
we	O
get	O
an	O
undirected	B
version	O
of	O
factor	B
analysis	I
however	O
it	O
turns	O
out	O
that	O
it	O
is	O
identical	O
to	O
the	O
standard	O
directed	B
version	O
and	O
movellan	O
if	O
we	O
use	O
gaussian	B
latent	B
variables	O
and	O
categorical	B
observed	O
variables	O
we	O
get	O
an	O
undirected	B
version	O
of	O
categorical	B
pca	B
in	O
et	O
al	O
this	O
was	O
applied	O
to	O
the	O
netflix	O
collaborative	O
filtering	B
problem	O
but	O
was	O
found	O
to	O
be	O
significantly	O
inferior	O
to	O
using	O
binary	O
latent	B
variables	O
which	O
have	O
more	O
expressive	O
power	O
learning	B
rbms	O
in	O
this	O
section	O
we	O
discuss	O
some	O
ways	O
to	O
compute	O
ml	O
parameter	B
estimates	O
of	O
rbms	O
using	O
gradient-based	O
optimizers	O
it	O
is	O
common	O
to	O
use	O
stochastic	B
gradient	B
descent	I
since	O
rbms	O
often	O
have	O
many	O
parameters	O
and	O
therefore	O
need	O
to	O
be	O
trained	O
on	O
very	O
large	O
datasets	O
in	O
addition	O
it	O
is	O
standard	O
to	O
use	O
regularization	B
a	O
technique	O
that	O
is	O
often	O
called	O
weight	B
decay	I
in	O
this	O
context	O
this	O
requires	O
a	O
very	O
small	O
change	O
to	O
the	O
objective	O
and	O
gradient	O
as	O
discussed	O
in	O
section	O
deriving	O
the	O
gradient	O
using	O
ph	O
v	O
to	O
compute	O
the	O
gradient	O
we	O
can	O
modify	O
the	O
equations	O
from	O
section	O
which	O
show	O
how	O
to	O
fit	O
a	O
generic	O
latent	B
variable	O
maxent	B
model	O
in	O
the	O
context	O
of	O
the	O
boltzmann	B
machine	I
we	O
have	O
one	O
feature	O
per	O
edge	O
so	O
the	O
gradient	O
is	O
given	O
by	O
e	O
e	O
we	O
can	O
write	O
this	O
in	O
matrix-vector	O
form	O
as	O
follows	O
wrk	O
n	O
ep	B
vht	O
w	O
epemp	O
vht	O
where	O
pempv	O
h	O
phv	O
and	O
pempv	O
distribution	O
hk	O
vi	O
is	O
the	O
empirical	O
can	O
derive	O
a	O
similar	B
expression	O
for	O
the	O
bias	B
terms	O
by	O
setting	O
vr	O
or	O
n	O
the	O
first	O
term	O
on	O
the	O
gradient	O
when	O
v	O
is	O
fixed	O
to	O
a	O
data	O
case	O
is	O
sometimes	O
called	O
the	O
clamped	B
phase	B
and	O
the	O
second	O
term	O
when	O
v	O
is	O
free	O
is	O
sometimes	O
called	O
the	O
unclamped	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
phase	B
when	O
the	O
model	O
expectations	O
match	O
the	O
empirical	O
expectations	O
the	O
two	O
terms	O
cancel	O
out	O
the	O
gradient	O
becomes	O
zero	O
and	O
learning	B
stops	O
this	O
algorithm	O
was	O
first	O
proposed	O
in	O
et	O
al	O
the	O
main	O
problem	O
is	O
efficiently	O
computing	O
the	O
expectations	O
we	O
discuss	O
some	O
ways	O
to	O
do	O
this	O
below	O
deriving	O
the	O
gradient	O
using	O
pv	O
we	O
now	O
present	O
an	O
alternative	O
way	O
to	O
derive	O
equation	O
which	O
also	O
applies	O
to	O
other	O
energy	B
based	I
models	I
first	O
we	O
marginalize	O
out	O
the	O
hidden	B
variables	I
and	O
write	O
the	O
rbm	B
in	O
the	O
form	O
pv	O
z	O
exp	O
f	O
wheref	O
is	O
the	O
free	B
energy	I
ev	O
h	O
exp	O
vrhkwrk	O
h	O
h	O
h	O
exp	O
hr	O
vrhrwrk	O
exp	O
vrwrk	O
exp	O
vrhkwrk	O
f	O
using	O
the	O
fact	O
that	O
z	O
n	O
next	O
we	O
write	O
the	O
log-likelihood	O
in	O
the	O
following	O
form	O
f	O
log	O
z	O
n	O
v	O
exp	O
f	O
we	O
have	O
f	O
z	O
log	O
pvi	O
n	O
f	O
f	O
z	O
z	O
v	O
f	O
f	O
exp	O
f	O
n	O
n	O
plugging	O
in	O
the	O
free	B
energy	I
one	O
can	O
show	O
that	O
f	O
vr	O
e	O
e	O
n	O
e	O
e	O
which	O
matches	O
equation	O
wrk	O
hence	O
wrk	O
restricted	O
boltzmann	O
machines	O
figure	O
illustration	O
of	O
gibbs	B
sampling	I
in	O
an	O
rbm	B
the	O
visible	B
nodes	B
are	O
initialized	O
at	O
a	O
datavector	O
then	O
we	O
sample	O
a	O
hidden	B
vector	O
then	O
another	O
visible	B
vector	O
etc	O
eventually	O
infinity	O
we	O
will	O
be	O
producing	O
samples	B
from	O
the	O
joint	B
distribution	I
pv	O
h	O
approximating	O
the	O
expectations	O
we	O
can	O
approximate	O
the	O
expectations	O
needed	O
to	O
evaluate	O
the	O
gradient	O
by	O
performing	O
block	O
gibbs	B
sampling	I
using	O
equations	O
and	O
in	O
more	O
detail	O
we	O
can	O
sample	O
from	O
the	O
joint	B
distribution	I
pv	O
h	O
as	O
follows	O
initialize	O
the	O
chain	O
at	O
by	O
setting	O
vi	O
for	O
some	O
data	O
vector	O
and	O
then	O
sample	O
from	O
then	O
from	O
then	O
from	O
etc	O
see	O
figure	O
for	O
an	O
illustration	O
note	O
however	O
that	O
we	O
have	O
to	O
wait	O
until	O
the	O
markov	B
chain	I
reaches	O
equilibrium	O
until	O
it	O
has	O
burned	B
in	I
before	O
we	O
can	O
interpret	O
the	O
samples	B
as	O
coming	O
from	O
the	O
joint	B
distribution	I
of	O
interest	O
and	O
this	O
might	O
take	O
a	O
long	O
time	O
a	O
faster	O
alternative	O
is	O
to	O
use	O
mean	B
field	O
where	O
we	O
make	O
the	O
approximation	O
e	O
e	O
e	O
however	O
since	O
pv	O
h	O
is	O
typically	O
multimodal	O
this	O
is	O
usually	O
a	O
very	O
poor	O
approximation	O
since	O
it	O
will	O
average	O
over	O
different	O
modes	O
section	O
furthermore	O
there	O
is	O
a	O
more	O
subtle	O
reason	O
not	O
to	O
use	O
mean	B
field	O
since	O
the	O
gradient	O
has	O
the	O
form	O
e	O
e	O
we	O
see	O
that	O
the	O
negative	O
sign	O
in	O
front	O
means	O
that	O
the	O
method	O
will	O
try	O
to	O
make	O
the	O
variational	O
bound	O
as	O
loose	O
as	O
possible	O
and	O
hinton	O
this	O
explains	O
why	O
earlier	O
attempts	O
to	O
use	O
mean	B
field	O
to	O
learn	O
boltzmann	O
machines	O
and	O
rodriguez	O
did	O
not	O
work	O
contrastive	B
divergence	I
the	O
problem	O
with	O
using	O
gibbs	B
sampling	I
to	O
compute	O
the	O
gradient	O
is	O
that	O
it	O
is	O
slow	O
we	O
now	O
present	O
a	O
faster	O
method	O
known	O
as	O
contrastive	B
divergence	I
or	O
cd	B
cd	B
was	O
originally	O
derived	O
by	O
approximating	O
an	O
objective	O
function	O
defined	O
as	O
the	O
difference	O
of	O
two	O
kl	O
divergences	O
rather	O
than	O
trying	O
to	O
maximize	O
the	O
likelihood	B
itself	O
however	O
from	O
an	O
algorithmic	O
point	O
of	O
view	O
it	O
can	O
be	O
thought	O
of	O
as	O
similar	B
to	O
stochastic	B
gradient	B
descent	I
except	O
it	O
approximates	O
the	O
unclamped	O
expectations	O
with	O
brief	O
gibbs	B
sampling	I
where	O
we	O
initialize	O
each	O
markov	B
chain	I
at	O
the	O
data	O
vectors	O
that	O
is	O
we	O
approximate	O
the	O
gradient	O
for	O
one	O
datavector	O
as	O
follows	O
eq	O
w	O
e	O
vhtvi	O
vht	O
where	O
q	O
corresponds	O
to	O
the	O
distribution	O
generated	O
by	O
k	O
up-down	B
gibbs	O
sweeps	O
started	O
at	O
vi	O
as	O
in	O
figure	O
this	O
is	O
known	O
as	O
cd-k	O
in	O
more	O
detail	O
the	O
procedure	O
k	O
is	O
as	O
follows	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
hi	O
phvi	O
i	O
pvhi	O
i	O
i	O
vht	O
eq	O
it	O
we	O
then	O
make	O
the	O
approximation	O
such	O
samples	B
are	O
sometimes	O
called	O
fantasy	B
data	I
we	O
can	O
think	O
of	O
i	O
as	O
the	O
model	O
s	O
best	O
attempt	O
at	O
reconstructing	O
vi	O
after	O
being	O
coded	O
and	O
then	O
decoded	O
by	O
the	O
model	O
this	O
is	O
similar	B
to	O
the	O
way	O
we	O
train	O
auto-encoders	B
which	O
are	O
models	O
which	O
try	O
to	O
squeeze	O
the	O
data	O
through	O
a	O
restricted	O
parametric	O
bottleneck	B
section	O
i	O
in	O
the	O
final	O
upwards	O
pass	O
since	O
this	O
reduces	O
the	O
variance	B
however	O
it	O
is	O
not	O
valid	O
to	O
use	O
e	O
instead	O
of	O
sampling	O
hi	O
phvi	O
in	O
the	O
earlier	O
upwards	O
passes	O
because	O
then	O
each	O
hidden	B
unit	O
would	O
be	O
able	O
to	O
pass	O
more	O
than	O
bit	O
of	O
information	B
so	O
it	O
would	O
not	O
act	O
as	O
much	O
of	O
a	O
bottleneck	B
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
e	O
i	O
instead	O
of	O
a	O
sampled	O
value	O
the	O
whole	O
procedure	O
is	O
summarized	O
in	O
algorithm	O
that	O
we	O
follow	O
the	O
positive	O
gradient	O
since	O
we	O
are	O
maximizing	O
likelihood	B
various	O
tricks	O
can	O
be	O
used	O
to	O
speed	O
this	O
algorithm	O
up	O
such	O
as	O
using	O
a	O
momentum	B
term	O
using	O
mini-batches	O
averaging	O
the	O
updates	O
etc	O
such	O
details	O
can	O
be	O
found	O
in	O
swersky	O
et	O
al	O
algorithm	O
training	O
for	O
an	O
rbm	B
with	O
binary	O
hidden	B
and	O
visible	B
units	O
initialize	O
weights	O
w	O
r	O
t	O
for	O
each	O
epoch	B
do	O
r	O
k	O
randomly	O
t	O
t	O
for	O
each	O
minibatch	O
of	O
size	O
b	O
do	O
set	O
minibatch	O
gradient	O
to	O
zero	O
g	O
for	O
each	O
case	O
vi	O
in	O
the	O
minibatch	O
do	O
compute	O
i	O
e	O
w	O
sample	O
hi	O
phvi	O
w	O
i	O
pvhi	O
w	O
sample	O
i	O
e	O
compute	O
i	O
w	O
compute	O
gradient	O
w	O
it	O
accumulate	O
g	O
g	O
w	O
update	O
parameters	O
w	O
w	O
tbg	O
i	O
it	O
persistent	B
cd	B
in	O
section	O
we	O
presented	O
a	O
technique	O
called	O
stochastic	B
maximum	I
likelihood	B
for	O
fitting	O
maxent	B
models	O
this	O
avoids	O
the	O
need	O
to	O
run	O
mcmc	B
to	O
convergence	O
at	O
each	O
iteration	O
restricted	O
boltzmann	O
machines	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
parameters	O
are	O
changing	O
slowly	O
so	O
the	O
markov	B
chains	O
will	O
not	O
be	O
pushed	O
too	O
far	O
from	O
equilibrium	O
after	O
each	O
update	O
in	O
other	O
words	O
there	O
are	O
two	O
dynamical	O
processes	O
running	O
at	O
different	O
time	O
scales	O
the	O
states	O
change	O
quickly	O
and	O
the	O
parameters	O
change	O
slowly	O
this	O
algorithm	O
was	O
independently	O
rediscovered	O
in	O
who	O
called	O
it	O
persistent	B
cd	B
see	O
algorithm	O
for	O
the	O
pseudocode	O
pcd	O
often	O
works	O
better	O
than	O
cd	B
e	O
g	O
marlin	O
et	O
al	O
swersky	O
et	O
al	O
although	O
cd	B
can	O
be	O
faster	O
in	O
the	O
early	O
stages	O
of	O
learning	B
algorithm	O
persistent	B
cd	B
for	O
training	O
an	O
rbm	B
with	O
binary	O
hidden	B
and	O
visible	B
units	O
initialize	O
weights	O
w	O
r	O
initialize	O
chains	O
hss	O
for	O
t	O
do	O
d	O
l	O
randomly	O
randomly	O
mean	B
field	O
updates	O
for	O
each	O
case	O
i	O
n	O
do	O
i	O
wk	O
ik	O
sigmvt	O
mcmc	B
updates	O
for	O
each	O
sample	O
s	O
s	O
do	O
vi	O
it	O
parameter	B
updates	O
g	O
n	O
s	O
w	O
w	O
tg	O
decrease	O
t	O
vshst	O
generate	O
hs	O
by	O
brief	O
gibbs	B
sampling	I
from	O
old	O
hs	O
applications	O
of	O
rbms	O
the	O
main	O
application	O
of	O
rbms	O
is	O
as	O
a	O
building	O
block	O
for	O
deep	B
generative	O
models	O
which	O
we	O
discuss	O
in	O
section	O
but	O
they	O
can	O
also	O
be	O
used	O
as	O
substitutes	O
for	O
directed	B
two-layer	O
models	O
they	O
are	O
particularly	O
useful	O
in	O
cases	O
where	O
inference	B
of	O
the	O
hidden	B
states	O
at	O
test	O
time	O
must	O
be	O
fast	O
we	O
give	O
some	O
examples	O
below	O
language	B
modeling	I
and	O
document	O
retrieval	O
we	O
can	O
use	O
a	O
categorical	B
rbm	B
to	O
define	O
a	O
generative	O
model	O
for	O
bag-of-words	B
as	O
an	O
alternative	O
to	O
lda	B
one	O
subtlety	O
is	O
that	O
the	O
partition	B
function	I
in	O
an	O
undirected	B
models	O
depends	O
on	O
how	O
big	O
the	O
graph	B
is	O
and	O
therefore	O
on	O
how	O
long	O
the	O
document	O
is	O
a	O
solution	O
to	O
this	O
was	O
proposed	O
in	O
and	O
hinton	O
use	O
a	O
categorical	B
rbm	B
with	O
tied	B
weights	O
but	O
multiply	O
the	O
hidden	B
activation	B
bias	B
terms	O
ck	O
by	O
the	O
document	O
length	O
l	O
to	O
compensate	O
form	O
the	O
fact	O
that	O
the	O
observed	O
word-count	O
vector	O
v	O
is	O
larger	O
in	O
magnitude	O
ev	O
h	O
k	O
vchkw	O
c	O
vcbc	O
r	O
l	O
hkck	O
chapter	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
data	O
set	O
nips	O
reuters	O
number	O
of	O
docs	O
train	O
test	O
k	O
d	O
st	O
dev	O
avg	O
test	O
perplexity	B
per	O
word	O
nats	B
r	O
unigram	O
figure	O
comparison	O
of	O
rbm	B
softmax	B
and	O
lda	B
on	O
three	O
corpora	O
k	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
vocabulary	O
d	O
is	O
the	O
average	O
document	O
length	O
and	O
st	O
dev	O
is	O
the	O
standard	B
deviation	I
of	O
the	O
document	O
length	O
source	O
and	O
hinton	O
replicated	O
softmax	B
d	O
lda	B
d	O
n	O
o	O
i	O
s	O
i	O
c	O
e	O
r	O
p	O
reuters	O
replicated	O
softmax	B
d	O
lda	B
d	O
i	O
n	O
o	O
s	O
c	O
e	O
r	O
p	O
i	O
recall	B
recall	B
figure	O
precision-recall	O
curves	O
for	O
rbm	B
softmax	B
and	O
lda	B
on	O
two	O
corpora	O
from	O
figure	O
of	O
and	O
hinton	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
where	O
vc	B
iyil	O
c	O
this	O
is	O
like	O
having	O
a	O
single	O
multinomial	B
node	O
we	O
have	O
dropped	O
the	O
r	O
subscript	O
with	O
c	O
states	O
where	O
c	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
vocabulary	O
this	O
is	O
called	O
the	O
replicated	B
softmax	B
model	I
and	O
hinton	O
and	O
is	O
an	O
undirected	B
alternative	O
to	O
mpca	B
lda	B
we	O
can	O
compare	O
the	O
modeling	O
power	O
of	O
rbms	O
vs	O
lda	B
by	O
measuring	O
the	O
perplexity	B
on	O
a	O
test	O
set	O
this	O
can	O
be	O
approximated	O
using	O
annealing	B
importance	B
sampling	I
the	O
results	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
lda	B
is	O
significantly	O
better	O
than	O
a	O
unigram	O
model	O
but	O
that	O
an	O
rbm	B
is	O
significantly	O
better	O
than	O
lda	B
just	O
a	O
single	O
matrix-vector	O
another	O
advantage	O
of	O
the	O
lda	B
is	O
that	O
inference	B
is	O
fast	O
and	O
exact	O
multiply	O
followed	O
by	O
a	O
sigmoid	B
nonlinearity	O
as	O
in	O
equation	O
in	O
addition	O
to	O
being	O
faster	O
the	O
rbm	B
is	O
more	O
accurate	O
this	O
is	O
illustrated	O
in	O
figure	O
which	O
shows	O
precision-recall	O
curves	O
for	O
rbms	O
and	O
lda	B
on	O
two	O
different	O
corpora	O
these	O
curves	O
were	O
generated	O
as	O
follows	O
a	O
query	O
document	O
from	O
the	O
test	O
set	O
is	O
taken	O
its	O
similarity	O
to	O
all	O
the	O
training	O
documents	O
is	O
computed	O
where	O
the	O
similarity	O
is	O
defined	O
as	O
the	O
cosine	O
of	O
the	O
angle	O
between	O
the	O
two	O
topic	B
vectors	O
and	O
then	O
the	O
top	O
m	O
documents	O
are	O
returned	O
for	O
varying	O
m	O
a	O
retrieved	O
document	O
is	O
considered	O
relevant	O
if	O
it	O
has	O
the	O
same	O
class	O
label	B
as	O
that	O
of	O
the	O
query	O
s	O
is	O
the	O
only	O
place	O
where	O
labels	O
are	O
used	O
restricted	O
boltzmann	O
machines	O
rbms	O
for	O
collaborative	O
filtering	B
rbms	O
have	O
been	O
applied	O
to	O
the	O
netflix	O
collaborative	O
filtering	B
competition	O
et	O
al	O
in	O
fact	O
an	O
rbm	B
with	O
binary	O
hidden	B
nodes	B
and	O
categorical	B
visible	B
nodes	B
can	O
slightly	O
outperform	O
svd	B
by	O
combining	O
the	O
two	O
methods	O
performance	O
can	O
be	O
further	O
improved	O
winning	O
entry	O
in	O
the	O
challenge	O
was	O
an	O
ensemble	B
of	O
many	O
different	O
types	O
of	O
model	O
exercises	O
exercise	O
partition	B
function	I
for	O
an	O
rbm	B
show	O
how	O
to	O
compute	O
z	O
for	O
an	O
rbm	B
with	O
k	O
binary	O
hidden	B
nodes	B
and	O
r	O
binary	O
observed	O
nodes	B
in	O
time	O
assuming	O
k	O
r	O
deep	B
learning	B
introduction	O
many	O
of	O
the	O
models	O
we	O
have	O
looked	O
at	O
in	O
this	O
book	O
have	O
a	O
simple	O
two-layer	O
architecture	O
of	O
the	O
form	O
z	O
y	O
for	O
unsupervised	O
latent	B
variable	I
models	I
or	O
x	O
y	O
for	O
supervised	O
models	O
however	O
when	O
we	O
look	O
at	O
the	O
brain	O
we	O
seem	O
many	O
levels	O
of	O
processing	O
it	O
is	O
believed	O
that	O
each	O
level	O
is	O
learning	B
features	B
or	O
representations	O
at	O
increasing	O
levels	O
of	O
abstraction	O
for	O
example	O
the	O
standard	B
model	I
of	O
the	O
visual	O
cortex	O
and	O
wiesel	O
serre	O
et	O
al	O
ranzato	O
et	O
al	O
suggests	O
that	O
speaking	O
the	O
brain	O
first	O
extracts	O
edges	B
then	O
patches	O
then	O
surfaces	O
then	O
objects	O
etc	O
e	O
g	O
kandel	O
et	O
al	O
for	O
more	O
information	B
about	O
how	O
the	O
brain	O
might	O
perform	O
vision	O
this	O
observation	B
has	O
inspired	O
a	O
recent	O
trend	O
in	O
machine	B
learning	B
known	O
as	O
deep	B
learning	B
deeplearning	O
net	O
and	O
the	O
references	O
therein	O
which	O
attempts	O
to	O
the	O
idea	O
can	O
be	O
applied	O
to	O
non-vision	O
e	O
g	O
replicate	O
this	O
kind	O
of	O
architecture	O
in	O
a	O
computer	O
problems	O
as	O
well	O
such	O
as	O
speech	O
and	O
language	O
in	O
this	O
chapter	O
we	O
give	O
a	O
brief	O
overview	O
of	O
this	O
new	O
field	O
however	O
we	O
caution	O
the	O
reader	O
that	O
the	O
topic	B
of	O
deep	B
learning	B
is	O
currently	O
evolving	O
very	O
quickly	O
so	O
the	O
material	O
in	O
this	O
chapter	O
may	O
soon	O
be	O
outdated	O
deep	B
generative	O
models	O
deep	B
models	O
often	O
have	O
millions	O
of	O
parameters	O
acquiring	O
enough	O
labeled	O
data	O
to	O
train	O
such	O
models	O
is	O
diffcult	O
despite	O
crowd	B
sourcing	I
sites	O
such	O
as	O
mechanical	B
turk	I
in	O
simple	O
settings	O
such	O
as	O
hand-written	O
character	O
recognition	O
it	O
is	O
possible	O
to	O
generate	O
lots	O
of	O
labeled	O
data	O
by	O
making	O
modified	O
copies	O
of	O
a	O
small	O
manually	O
labeled	O
training	B
set	I
e	O
g	O
figure	O
but	O
it	O
seems	O
unlikely	O
that	O
this	O
approach	O
will	O
scale	O
to	O
complex	O
to	O
overcome	O
the	O
problem	O
of	O
needing	O
labeled	O
training	O
data	O
we	O
will	O
focus	O
on	O
unsupervised	B
learning	B
the	O
most	O
natural	O
way	O
to	O
perform	O
this	O
is	O
to	O
use	O
generative	O
models	O
in	O
this	O
section	O
we	O
discuss	O
three	O
different	O
kinds	O
of	O
deep	B
generative	O
models	O
directed	B
undirected	B
and	O
mixed	O
there	O
have	O
been	O
some	O
attempts	O
to	O
use	O
computer	O
graphics	O
and	O
video	O
games	O
to	O
generate	O
realistic-looking	O
images	O
of	O
complex	O
scenes	O
and	O
then	O
to	O
use	O
this	O
as	O
training	O
data	O
for	O
computer	O
vision	O
systems	O
however	O
often	O
graphics	O
programs	O
cut	O
corners	O
in	O
order	O
to	O
make	O
perceptually	O
appealing	O
images	O
which	O
are	O
not	O
reflective	O
of	O
the	O
natural	O
statistics	O
of	O
real-world	O
images	O
chapter	O
deep	B
learning	B
figure	O
some	O
deep	B
multi-layer	O
graphical	B
models	I
observed	O
variables	O
are	O
at	O
the	O
bottom	O
a	O
directed	B
model	O
an	O
undirected	B
model	O
boltzmann	B
machine	I
a	O
mixed	O
directed-undirected	O
model	O
belief	O
net	O
deep	B
directed	B
networks	I
perhaps	O
the	O
most	O
natural	O
way	O
to	O
build	O
a	O
deep	B
generative	O
model	O
is	O
to	O
construct	O
a	O
deep	B
directed	B
graphical	B
model	I
as	O
shown	O
in	O
figure	O
the	O
bottom	O
level	O
contains	O
the	O
observed	O
pixels	O
whatever	O
the	O
data	O
is	O
and	O
the	O
remaining	O
layers	O
are	O
hidden	B
we	O
have	O
assumed	O
just	O
layers	O
for	O
notational	O
simplicity	O
the	O
number	O
and	O
size	O
of	O
layers	O
is	O
usually	O
chosen	O
by	O
hand	O
although	O
one	O
can	O
also	O
use	O
non-parametric	O
bayesian	B
methods	O
et	O
al	O
or	O
boosting	B
et	O
al	O
to	O
infer	O
the	O
model	O
structure	O
we	O
shall	O
call	O
models	O
of	O
this	O
form	O
deep	B
directed	B
networks	I
or	O
ddns	O
binary	O
and	O
all	O
cpds	O
are	O
logistic	B
functions	O
this	O
is	O
called	O
a	O
sigmoid	B
belief	I
net	I
this	O
case	O
the	O
model	O
defines	O
the	O
following	O
joint	B
distribution	I
if	O
all	O
the	O
nodes	B
are	O
in	O
i	O
bervisigmht	O
j	O
v	O
k	O
l	O
unfortunately	O
inference	B
in	O
directed	B
models	O
such	O
as	O
these	O
is	O
intractable	O
because	O
the	O
posterior	O
on	O
the	O
hidden	B
nodes	B
is	O
correlated	O
due	O
to	O
explaining	B
away	I
one	O
can	O
use	O
fast	O
mean	B
field	O
approximations	O
and	O
jordan	O
saul	O
and	O
jordan	O
but	O
these	O
may	O
not	O
be	O
very	O
accurate	O
since	O
they	O
approximate	O
the	O
correlated	O
posterior	O
with	O
a	O
factorial	O
posterior	O
one	O
can	O
also	O
use	O
mcmc	B
inference	B
adams	O
et	O
al	O
but	O
this	O
can	O
be	O
quite	O
slow	O
because	O
the	O
variables	O
are	O
highly	O
correlated	O
slow	O
inference	B
also	O
results	O
in	O
slow	O
learning	B
deep	B
boltzmann	O
machines	O
a	O
natural	O
alternative	O
to	O
a	O
directed	B
model	O
is	O
to	O
construct	O
a	O
deep	B
undirected	B
model	O
for	O
example	O
we	O
can	O
stack	O
a	O
series	O
of	O
rbms	O
on	O
top	O
of	O
each	O
other	O
as	O
shown	O
in	O
figure	O
this	O
is	O
known	O
as	O
a	O
deep	B
boltzmann	B
machine	I
or	O
dbm	B
and	O
hinton	O
if	O
we	O
have	O
hidden	B
layers	O
the	O
model	O
is	O
defined	O
as	O
follows	O
v	O
z	O
exp	O
ij	O
jk	O
kl	O
deep	B
generative	O
models	O
where	O
we	O
are	O
ignoring	O
constant	O
offset	O
or	O
bias	B
terms	O
the	O
main	O
advantage	O
over	O
the	O
directed	B
model	O
is	O
that	O
one	O
can	O
perform	O
efficient	O
block	O
gibbs	B
sampling	I
or	O
block	O
mean	B
field	O
since	O
all	O
the	O
nodes	B
in	O
each	O
layer	O
are	O
conditionally	B
independent	I
of	O
each	O
other	O
given	O
the	O
layers	O
above	O
and	O
below	O
and	O
larochelle	O
the	O
main	O
disadvantage	O
is	O
that	O
training	O
undirected	B
models	O
is	O
more	O
difficult	O
because	O
of	O
the	O
partition	B
function	I
however	O
below	O
we	O
will	O
see	O
a	O
greedy	O
layer-wise	O
strategy	O
for	O
learning	B
deep	B
undirected	B
models	O
deep	B
belief	B
networks	I
an	O
interesting	O
compromise	O
is	O
to	O
use	O
a	O
model	O
that	O
is	O
partially	O
directed	B
and	O
partially	O
undirected	B
in	O
particular	O
suppose	O
we	O
construct	O
a	O
layered	O
model	O
which	O
has	O
directed	B
arrows	O
except	O
at	O
the	O
top	O
where	O
there	O
is	O
an	O
undirected	B
bipartite	B
graph	B
as	O
shown	O
in	O
figure	O
this	O
model	O
is	O
known	O
as	O
a	O
deep	B
belief	I
network	I
et	O
al	O
or	O
if	O
we	O
have	O
hidden	B
layers	O
the	O
model	O
is	O
defined	O
as	O
follows	O
v	O
bervisigmht	O
j	O
exp	O
kl	O
i	O
z	O
essentially	O
the	O
top	O
two	O
layers	O
act	O
as	O
an	O
associative	B
memory	I
and	O
the	O
remaining	O
layers	O
then	O
generate	O
the	O
output	O
has	O
the	O
form	O
the	O
advantage	O
of	O
this	O
peculiar	O
architecture	O
is	O
that	O
we	O
can	O
infer	O
the	O
hidden	B
states	O
in	O
a	O
fast	O
bottom-up	O
fashion	O
to	O
see	O
why	O
suppose	O
we	O
only	O
have	O
two	O
hidden	B
layers	O
and	O
that	O
wt	O
so	O
the	O
second	O
level	O
weights	O
are	O
tied	B
to	O
the	O
first	O
level	O
weights	O
figure	O
this	O
defines	O
a	O
model	O
of	O
the	O
form	O
one	O
can	O
show	O
that	O
the	O
distribution	O
expvt	O
which	O
is	O
equivalent	O
to	O
an	O
rbm	B
since	O
the	O
dbn	B
is	O
equivalent	O
to	O
the	O
rbm	B
as	O
far	O
as	O
is	O
concerned	O
we	O
can	O
infer	O
the	O
posterior	O
in	O
the	O
dbn	B
exactly	O
as	O
in	O
the	O
rbm	B
this	O
posterior	O
is	O
exact	O
even	O
though	O
it	O
is	O
fully	O
factorized	O
now	O
the	O
only	O
way	O
to	O
get	O
a	O
factored	O
posterior	O
is	O
if	O
the	O
prior	O
is	O
a	O
complementary	B
prior	I
this	O
is	O
a	O
prior	O
which	O
when	O
multiplied	O
by	O
the	O
likelihood	B
results	O
in	O
a	O
perfectly	O
factored	O
posterior	O
thus	O
we	O
see	O
that	O
the	O
top	O
level	O
rbm	B
in	O
a	O
dbn	B
acts	O
as	O
a	O
complementary	B
prior	I
for	O
the	O
bottom	O
level	O
directed	B
sigmoidal	O
likelihood	B
function	O
if	O
we	O
have	O
multiple	O
hidden	B
levels	O
andor	O
if	O
the	O
weights	O
are	O
not	O
tied	B
the	O
correspondence	B
between	O
the	O
dbn	B
and	O
the	O
rbm	B
does	O
not	O
hold	O
exactly	O
any	O
more	O
but	O
we	O
can	O
still	O
use	O
the	O
factored	O
inference	B
rule	O
as	O
a	O
form	O
of	O
approximate	O
bottom-up	O
inference	B
below	O
we	O
show	O
that	O
this	O
is	O
a	O
valid	O
variational	O
lower	O
bound	O
this	O
bound	O
also	O
suggests	O
a	O
layer-wise	O
training	O
strategy	O
that	O
we	O
will	O
explain	O
in	O
more	O
detail	O
later	O
note	O
however	O
that	O
top-down	O
inference	B
in	O
a	O
dbn	B
is	O
not	O
tractable	O
so	O
dbns	O
are	O
usually	O
only	O
used	O
in	O
a	O
feedforward	O
manner	O
unforuntately	O
the	O
acronym	O
dbn	B
also	O
stands	O
for	O
dynamic	B
bayes	I
net	I
geoff	O
hinton	O
who	O
invented	O
deep	B
belief	B
networks	I
has	O
suggested	O
the	O
acronyms	O
deebns	B
and	O
dybns	B
for	O
these	O
two	O
different	O
meanings	O
however	O
this	O
terminology	O
is	O
non-standard	O
chapter	O
deep	B
learning	B
figure	O
a	O
dbn	B
with	O
two	O
hidden	B
layers	O
and	O
tied	B
weights	O
that	O
is	O
equivalent	O
to	O
an	O
rbm	B
source	O
figure	O
of	O
the	O
corresponding	O
dbn	B
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
a	O
stack	O
of	O
rbms	O
trained	O
greedily	O
greedy	O
layer-wise	O
learning	B
of	O
dbns	O
the	O
equivalence	O
between	O
dbns	O
and	O
rbms	O
suggests	O
the	O
following	O
strategy	O
for	O
learning	B
a	O
dbn	B
fit	O
an	O
rbm	B
to	O
learn	O
using	O
methods	O
described	O
in	O
section	O
unroll	O
the	O
rbm	B
into	O
a	O
dbn	B
with	O
hidden	B
layers	O
as	O
in	O
figure	O
now	O
freeze	O
the	O
directed	B
weights	O
and	O
let	O
be	O
untied	O
so	O
it	O
is	O
no	O
longer	O
forced	O
to	O
be	O
equal	O
to	O
wt	O
we	O
will	O
now	O
learn	O
a	O
better	O
prior	O
for	O
by	O
fitting	O
a	O
second	O
rbm	B
the	O
input	O
data	O
to	O
this	O
new	O
rbm	B
is	O
the	O
activation	B
of	O
the	O
hidden	B
units	I
e	O
which	O
can	O
be	O
computed	O
using	O
a	O
factorial	O
approximation	O
continue	O
to	O
add	O
more	O
hidden	B
layers	O
until	O
some	O
stopping	O
criterion	O
is	O
satisified	O
e	O
g	O
you	O
run	O
out	O
of	O
time	O
or	O
memory	O
or	O
you	O
start	O
to	O
overfit	O
the	O
validation	B
set	I
construct	O
the	O
dbn	B
from	O
these	O
rbms	O
as	O
illustrated	O
in	O
figure	O
one	O
can	O
show	O
et	O
al	O
that	O
this	O
procedure	O
always	O
increases	O
a	O
lower	O
bound	O
the	O
observed	O
data	O
likelihood	B
of	O
course	O
this	O
procedure	O
might	O
result	O
in	O
overfitting	O
but	O
that	O
is	O
a	O
different	O
matter	O
in	O
practice	O
we	O
want	O
to	O
be	O
able	O
to	O
use	O
any	O
number	O
of	O
hidden	B
units	I
in	O
each	O
level	O
this	O
means	O
we	O
will	O
not	O
be	O
able	O
to	O
initialize	O
the	O
weights	O
so	O
that	O
wt	O
this	O
voids	O
the	O
theoretical	O
guarantee	O
nevertheless	O
the	O
method	O
works	O
well	O
in	O
practice	O
as	O
we	O
will	O
see	O
the	O
method	O
can	O
also	O
be	O
extended	O
to	O
train	O
dbms	O
in	O
a	O
greedy	O
way	O
and	O
larochelle	O
after	O
using	O
the	O
greedy	O
layer-wise	O
training	O
strategy	O
it	O
is	O
standard	O
to	O
fine	O
tune	O
the	O
weights	O
using	O
a	O
technique	O
called	O
backfitting	O
this	O
works	O
as	O
follows	O
perform	O
an	O
upwards	O
sampling	O
pass	O
to	O
the	O
top	O
then	O
perform	O
brief	O
gibbs	B
sampling	I
in	O
the	O
top	O
level	O
rbm	B
and	O
perform	O
a	O
cd	B
update	O
of	O
the	O
rbm	B
parameters	O
finally	O
perform	O
a	O
downwards	O
ancestral	B
sampling	I
pass	O
is	O
an	O
approximate	O
sample	O
from	O
the	O
posterior	O
and	O
update	O
the	O
logistic	B
cpd	B
parameters	O
using	O
a	O
small	O
gradient	O
step	O
this	O
is	O
called	O
the	O
up-down	B
procedure	O
et	O
al	O
unfortunately	O
this	O
procedure	O
is	O
very	O
slow	O
deep	B
neural	B
networks	I
deep	B
neural	B
networks	I
given	O
that	O
dbns	O
are	O
often	O
only	O
used	O
in	O
a	O
feed-forward	O
or	O
bottom-up	O
mode	B
they	O
are	O
effectively	O
acting	O
like	O
neural	B
networks	I
in	O
view	O
of	O
this	O
it	O
is	O
natural	O
to	O
dispense	O
with	O
the	O
generative	O
story	O
and	O
try	O
to	O
fit	O
deep	B
neural	B
networks	I
directly	O
as	O
we	O
discuss	O
below	O
the	O
resulting	O
training	O
methods	O
are	O
often	O
simpler	O
to	O
implement	O
and	O
can	O
be	O
faster	O
note	O
however	O
that	O
performance	O
with	O
deep	B
neural	O
nets	O
is	O
sometimes	O
not	O
as	O
good	O
as	O
with	O
probabilistic	O
models	O
et	O
al	O
one	O
reason	O
for	O
this	O
is	O
that	O
probabilistic	O
models	O
support	B
top-down	O
inference	B
as	O
well	O
as	O
bottom-up	O
inference	B
do	O
not	O
support	B
efficient	O
top-down	O
inference	B
but	O
dbms	O
do	O
and	O
this	O
has	O
been	O
shown	O
to	O
help	O
and	O
larochelle	O
top-down	O
inference	B
is	O
useful	O
when	O
there	O
is	O
a	O
lot	O
of	O
ambiguity	O
about	O
the	O
correct	O
interpretation	O
of	O
the	O
signal	O
it	O
is	O
interesting	O
to	O
note	O
that	O
in	O
the	O
mammalian	O
visual	O
cortex	O
there	O
are	O
many	O
more	O
feedback	O
connections	O
than	O
there	O
are	O
feedforward	O
connections	O
e	O
g	O
kandel	O
et	O
al	O
the	O
role	O
of	O
these	O
feedback	O
connections	O
is	O
not	O
precisely	O
understood	O
but	O
they	O
presumably	O
provide	O
contextual	O
prior	O
information	B
coming	O
from	O
the	O
previous	O
frame	O
or	O
retinal	O
glance	O
which	O
can	O
be	O
used	O
to	O
disambiguate	O
the	O
current	O
bottom-up	O
signals	O
and	O
mumford	O
of	O
course	O
we	O
can	O
simulate	O
the	O
effect	O
of	O
top-down	O
inference	B
using	O
a	O
neural	B
network	I
however	O
the	O
models	O
we	O
discuss	O
below	O
do	O
not	O
do	O
this	O
deep	B
multi-layer	O
perceptrons	O
many	O
decision	B
problems	O
can	O
be	O
reduced	O
to	O
classification	O
e	O
g	O
predict	O
which	O
object	O
any	O
is	O
present	O
in	O
an	O
image	O
patch	O
or	O
predict	O
which	O
phoneme	O
is	O
present	O
in	O
a	O
given	O
acoustic	O
feature	O
vector	O
we	O
can	O
solve	O
such	O
problems	O
by	O
creating	O
a	O
deep	B
feedforward	B
neural	B
network	I
or	O
multilayer	O
perceptron	B
as	O
in	O
section	O
and	O
then	O
fitting	O
the	O
parameters	O
using	O
gradient	B
descent	I
back-propagation	B
unfortunately	O
this	O
method	O
does	O
not	O
work	O
very	O
well	O
one	O
problem	O
is	O
that	O
the	O
gradient	O
becomes	O
weaker	O
the	O
further	O
we	O
move	O
away	O
from	O
the	O
data	O
this	O
is	O
known	O
as	O
the	O
vanishing	B
gradient	I
problem	O
and	O
frasconi	O
a	O
related	O
problem	O
is	O
that	O
there	O
can	O
be	O
large	O
plateaus	O
in	O
the	O
error	O
surface	O
which	O
cause	O
simple	O
first-order	O
gadient-based	O
methods	O
to	O
get	O
stuck	O
and	O
bengio	O
consequently	O
early	O
attempts	O
to	O
learn	O
deep	B
neural	B
networks	I
proved	O
unsuccesful	O
recently	O
there	O
has	O
been	O
some	O
progress	O
due	O
to	O
the	O
adoption	O
of	O
gpus	B
et	O
al	O
and	O
second-order	O
optimization	B
algorithms	O
nevertheless	O
such	O
models	O
remain	O
difficult	O
to	O
train	O
below	O
we	O
discuss	O
a	O
way	O
to	O
initialize	O
the	O
parameters	O
using	O
unsupervised	B
learning	B
this	O
is	O
called	O
generative	B
pre-training	I
the	O
advantage	O
of	O
performing	O
unsupervised	B
learning	B
first	O
is	O
that	O
the	O
model	O
is	O
forced	O
to	O
model	O
a	O
high-dimensional	O
response	O
namely	O
the	O
input	O
feature	O
vector	O
rather	O
than	O
just	O
predicting	O
a	O
scalar	O
response	O
this	O
acts	O
like	O
a	O
data-induced	O
regularizer	O
and	O
helps	O
backpropagation	B
find	O
local	O
minima	O
with	O
good	O
generalization	B
properties	O
et	O
al	O
glorot	O
and	O
bengio	O
chapter	O
deep	B
learning	B
figure	O
training	O
a	O
deep	B
autoencoder	O
first	O
we	O
greedily	O
train	O
some	O
rbms	O
then	O
we	O
construct	O
the	O
auto-encoder	B
by	O
replicating	O
the	O
weights	O
finally	O
we	O
fine-tune	O
the	O
weights	O
using	O
back-propagation	B
from	O
figure	O
of	O
and	O
salakhutdinov	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
deep	B
auto-encoders	B
an	O
auto-encoder	B
is	O
a	O
kind	O
of	O
unsupervised	O
neural	B
network	I
that	O
is	O
used	O
for	O
dimensionality	B
reduction	I
and	O
feature	O
discovery	O
more	O
precisely	O
an	O
auto-encoder	B
is	O
a	O
feedforward	B
neural	B
network	I
that	O
is	O
trained	O
to	O
predict	O
the	O
input	O
itself	O
to	O
prevent	O
the	O
system	O
from	O
learning	B
the	O
trivial	O
identity	O
mapping	O
the	O
hidden	B
layer	I
in	O
the	O
middle	O
is	O
usually	O
constrained	O
to	O
be	O
a	O
narrow	O
bottleneck	B
the	O
system	O
can	O
minimize	O
the	O
reconstruction	B
error	I
by	O
ensuring	O
the	O
hidden	B
units	I
capture	O
the	O
most	O
relevant	O
aspects	O
of	O
the	O
data	O
suppose	O
the	O
system	O
has	O
one	O
hidden	B
layer	I
so	O
the	O
model	O
has	O
the	O
form	O
v	O
h	O
v	O
further	O
in	O
this	O
case	O
one	O
can	O
show	O
that	O
the	O
weights	O
to	O
the	O
k	O
suppose	O
all	O
the	O
functions	O
are	O
linear	O
hidden	B
units	I
will	O
span	O
the	O
same	O
subspace	O
as	O
the	O
first	O
k	O
principal	B
components	I
of	O
the	O
data	O
and	O
joutsensalo	O
japkowicz	O
et	O
al	O
in	O
other	O
words	O
linear	O
auto-encoders	B
are	O
equivalent	O
to	O
pca	B
however	O
by	O
using	O
nonlinear	O
activation	B
functions	O
one	O
can	O
discover	O
nonlinear	O
representations	O
of	O
the	O
data	O
more	O
powerful	O
representations	O
can	O
be	O
learned	O
by	O
using	O
deep	B
auto-encoders	B
unfortunately	O
training	O
such	O
models	O
using	O
back-propagation	B
does	O
not	O
work	O
well	O
because	O
the	O
gradient	O
signal	O
becomes	O
too	O
small	O
as	O
it	O
passes	O
back	O
through	O
multiple	O
layers	O
and	O
the	O
learning	B
algorithm	O
often	O
gets	O
stuck	O
in	O
poor	O
local	O
minima	O
one	O
solution	O
to	O
this	O
problem	O
is	O
to	O
greedily	O
train	O
a	O
series	O
of	O
rbms	O
and	O
to	O
use	O
these	O
to	O
initialize	O
an	O
auto-encoder	B
as	O
illustrated	O
in	O
figure	O
the	O
whole	O
system	O
can	O
then	O
be	O
fine-tuned	O
using	O
backprop	O
in	O
the	O
usual	O
fashion	O
this	O
approach	O
first	O
suggested	O
in	O
and	O
salakhutdinov	O
applications	O
of	O
deep	B
networks	I
top-level	O
units	O
label	B
units	O
units	O
this	O
could	O
be	O
the	O
top	O
level	O
of	O
another	O
sensory	O
pathway	O
units	O
x	O
pixel	O
image	O
figure	O
used	O
with	O
kind	O
permission	O
of	O
geoff	O
hinton	O
test	O
cases	O
of	O
mnist	B
above	O
each	O
image	O
is	O
the	O
estimated	O
label	B
used	O
with	O
kind	O
permission	O
of	O
geoff	O
hinton	O
compare	O
to	O
figure	O
a	O
dbn	B
architecture	O
for	O
classifying	O
mnist	B
digits	O
source	O
figure	O
of	O
et	O
al	O
these	O
are	O
the	O
errors	O
made	O
by	O
the	O
dbn	B
on	O
the	O
source	O
figure	O
of	O
et	O
al	O
works	O
much	O
better	O
than	O
trying	O
to	O
fit	O
the	O
deep	B
auto-encoder	B
directly	O
starting	O
with	O
random	O
weights	O
stacked	O
denoising	O
auto-encoders	B
a	O
standard	O
way	O
to	O
train	O
an	O
auto-encoder	B
is	O
to	O
ensure	O
that	O
the	O
hidden	B
layer	I
is	O
narrower	O
than	O
the	O
visible	B
layer	O
this	O
prevents	O
the	O
model	O
from	O
learning	B
the	O
identity	O
function	O
but	O
there	O
are	O
other	O
ways	O
to	O
prevent	O
this	O
trivial	O
solution	O
which	O
allow	O
for	O
the	O
use	O
of	O
an	O
over-complete	B
representation	O
one	O
approach	O
is	O
to	O
impose	O
sparsity	B
constraints	O
on	O
the	O
activation	B
of	O
the	O
hidden	B
units	I
et	O
al	O
another	O
approach	O
is	O
to	O
add	O
noise	O
to	O
the	O
inputs	O
this	O
is	O
called	O
a	O
denoising	O
autoencoder	O
et	O
al	O
for	O
example	O
we	O
can	O
corrupt	O
some	O
of	O
the	O
inputs	O
for	O
example	O
by	O
setting	O
them	O
to	O
zero	O
so	O
the	O
model	O
has	O
to	O
learn	O
to	O
predict	O
the	O
missing	B
entries	O
this	O
can	O
be	O
shown	O
to	O
be	O
equivalent	O
to	O
a	O
certain	O
approximate	O
form	O
of	O
maximum	O
likelihood	B
training	O
as	O
score	B
matching	I
applied	O
to	O
an	O
rbm	B
of	O
course	O
we	O
can	O
stack	O
these	O
models	O
on	O
top	O
of	O
each	O
other	O
to	O
learn	O
a	O
deep	B
stacked	B
denoising	B
auto-encoder	B
which	O
can	O
be	O
discriminatively	O
fine-tuned	O
just	O
like	O
a	O
feedforward	B
neural	B
network	I
if	O
desired	O
applications	O
of	O
deep	B
networks	I
in	O
this	O
section	O
we	O
mention	O
a	O
few	O
applications	O
of	O
the	O
models	O
we	O
have	O
been	O
discussing	O
handwritten	O
digit	O
classification	O
using	O
dbns	O
figure	O
shows	O
a	O
dbn	B
et	O
al	O
consisting	O
of	O
hidden	B
layers	O
the	O
visible	B
layer	O
corresponds	O
to	O
binary	O
images	O
of	O
handwritten	O
digits	O
from	O
the	O
mnist	B
data	O
set	O
in	O
addition	O
the	O
top	O
rbm	B
is	O
connected	O
to	O
a	O
softmax	B
layer	O
with	O
units	O
representing	O
the	O
class	O
label	B
chapter	O
deep	B
learning	B
interbank	O
markets	O
european	O
community	O
monetaryeconomic	O
energy	O
markets	O
leading	O
economic	O
indicators	O
disasters	O
and	O
accidents	O
legaljudicial	O
accounts	O
earnings	O
government	O
borrowings	O
figure	O
visualization	O
of	O
some	O
bag	B
of	I
words	I
data	O
from	O
the	O
reuters	O
corpus	B
results	O
of	O
using	O
lsa	B
results	O
of	O
using	O
a	O
deep	B
auto-encoder	B
source	O
figure	O
of	O
and	O
salakhutdinov	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
the	O
first	O
hidden	B
layers	O
were	O
trained	O
in	O
a	O
greedy	O
unsupervised	O
fashion	O
from	O
mnist	B
digits	O
using	O
epochs	O
over	O
the	O
data	O
and	O
stochastic	B
gradient	B
descent	I
with	O
the	O
cd	B
heuristic	O
this	O
process	O
took	O
a	O
few	O
hours	O
per	O
layer	O
et	O
al	O
then	O
the	O
top	O
layer	O
was	O
trained	O
using	O
as	O
input	O
the	O
activations	O
of	O
the	O
lower	O
hidden	B
layer	I
as	O
well	O
as	O
the	O
class	O
labels	O
the	O
corresponding	O
generative	O
model	O
had	O
a	O
test	O
error	O
of	O
about	O
the	O
network	O
weights	O
were	O
then	O
carefully	O
fine-tuned	O
on	O
all	O
training	O
images	O
using	O
the	O
up-down	B
procedure	O
this	O
process	O
took	O
about	O
a	O
week	O
et	O
al	O
the	O
model	O
can	O
be	O
used	O
to	O
classify	O
by	O
performing	O
a	O
deterministic	O
bottom-up	O
pass	O
and	O
then	O
computing	O
the	O
free	B
energy	I
for	O
the	O
top-level	O
rbm	B
for	O
each	O
possible	O
class	O
label	B
the	O
final	O
error	O
on	O
the	O
test	O
set	O
was	O
about	O
the	O
misclassified	O
examples	O
are	O
shown	O
in	O
figure	O
this	O
was	O
the	O
best	O
error	O
rate	B
of	O
any	O
method	O
on	O
the	O
permutation-invariant	O
version	O
of	O
mnist	B
at	O
that	O
time	O
permutation-invariant	O
we	O
mean	B
a	O
method	O
that	O
does	O
not	O
exploit	O
the	O
fact	O
that	O
the	O
input	O
is	O
an	O
image	O
generic	O
methods	O
work	O
just	O
as	O
well	O
on	O
permuted	O
versions	O
of	O
the	O
input	O
figure	O
and	O
can	O
therefore	O
be	O
applied	O
to	O
other	O
kinds	O
of	O
datasets	O
the	O
only	O
other	O
method	O
that	O
comes	O
close	O
is	O
an	O
svm	B
with	O
a	O
degree	B
polynomial	B
kernel	B
which	O
has	O
achieved	O
an	O
error	O
rate	B
of	O
and	O
schoelkopf	O
by	O
way	O
of	O
comparison	O
neighbor	O
all	O
examples	O
achieves	O
this	O
is	O
not	O
as	O
good	O
although	O
is	O
much	O
data	O
visualization	O
and	O
feature	O
discovery	O
using	O
deep	B
auto-encoders	B
deep	B
autoencoders	O
can	O
learn	O
informative	O
features	B
from	O
raw	O
data	O
such	O
features	B
are	O
often	O
used	O
as	O
input	O
to	O
standard	O
supervised	B
learning	B
methods	O
to	O
illustrate	O
this	O
consider	O
fitting	O
a	O
deep	B
auto-encoder	B
with	O
a	O
hidden	B
bottleneck	B
to	O
some	O
one	O
can	O
get	O
much	O
improved	O
performance	O
on	O
this	O
task	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
input	O
is	O
an	O
image	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
create	O
distorted	B
versions	O
of	O
the	O
input	O
adding	O
small	O
shifts	O
and	O
translations	O
figure	O
for	O
some	O
examples	O
applying	O
this	O
trick	O
reduced	O
the	O
svm	B
error	O
rate	B
to	O
similar	B
error	O
rates	O
can	O
be	O
achieved	O
using	O
convolutional	O
neural	B
networks	I
trained	O
on	O
distorted	B
images	O
et	O
al	O
got	O
however	O
the	O
point	O
of	O
dbns	O
is	O
that	O
they	O
offer	O
a	O
way	O
to	O
learn	O
such	O
prior	O
knowledge	O
without	O
it	O
having	O
to	O
be	O
hand-crafted	O
applications	O
of	O
deep	B
networks	I
figure	O
precision-recall	O
curves	O
for	O
document	O
retrieval	O
in	O
the	O
reuters	O
corpus	B
source	O
figure	O
of	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
text	O
data	O
the	O
results	O
are	O
shown	O
in	O
figure	O
on	O
the	O
left	O
we	O
show	O
the	O
embedding	B
produced	O
by	O
lsa	B
and	O
on	O
the	O
right	O
the	O
embedding	B
produced	O
by	O
the	O
auto-encoder	B
it	O
is	O
clear	O
that	O
the	O
low-dimensional	O
representation	O
created	O
by	O
the	O
auto-encoder	B
has	O
captured	O
a	O
lot	O
of	O
the	O
meaning	O
of	O
the	O
documents	O
even	O
though	O
class	O
labels	O
were	O
not	O
note	O
that	O
various	O
other	O
ways	O
of	O
learning	B
low-dimensional	O
continuous	O
embeddings	O
of	O
words	O
have	O
been	O
proposed	O
see	O
e	O
g	O
et	O
al	O
for	O
details	O
information	B
retrieval	I
using	O
deep	B
auto-encoders	B
hashing	O
in	O
view	O
of	O
the	O
sucess	O
of	O
rbms	O
for	O
information	B
retrieval	I
discussed	O
in	O
section	O
it	O
is	O
natural	O
to	O
wonder	O
if	O
deep	B
models	O
can	O
do	O
even	O
better	O
in	O
fact	O
they	O
can	O
as	O
is	O
shown	O
in	O
figure	O
more	O
interestingly	O
we	O
can	O
use	O
a	O
binary	O
low-dimensional	O
representation	O
in	O
the	O
middle	O
layer	O
of	O
the	O
deep	B
auto-encoder	B
rather	O
than	O
a	O
continuous	O
representation	O
as	O
we	O
used	O
above	O
this	O
enables	O
very	O
fast	O
retrieval	O
of	O
related	O
documents	O
for	O
example	O
if	O
we	O
use	O
a	O
code	O
we	O
can	O
precompute	O
the	O
binary	O
representation	O
for	O
all	O
the	O
documents	O
and	O
then	O
create	O
a	O
hash-table	O
mapping	O
codewords	O
to	O
documents	O
this	O
approach	O
is	O
known	O
as	O
semantic	B
hashing	I
since	O
the	O
binary	O
representation	O
of	O
semantically	O
similar	B
documents	O
will	O
be	O
close	O
in	O
hamming	B
distance	I
for	O
the	O
test	O
documents	O
in	O
reuters	O
this	O
results	O
in	O
about	O
documents	O
per	O
entry	O
in	O
the	O
table	O
at	O
test	O
time	O
we	O
compute	O
the	O
codeword	O
for	O
the	O
query	O
and	O
then	O
simply	O
retrieve	O
the	O
relevant	O
documents	O
in	O
constant	O
time	O
by	O
looking	O
up	O
the	O
contents	O
of	O
the	O
relevant	O
address	O
in	O
memory	O
to	O
find	O
other	O
other	O
related	O
documents	O
we	O
can	O
compute	O
all	O
the	O
codewords	O
within	O
a	O
some	O
details	O
salakhutdinov	O
and	O
hinton	O
used	O
the	O
reuters	O
data	O
set	O
which	O
consists	O
of	O
newswire	O
articles	O
manually	O
classified	O
into	O
topics	O
they	O
represent	O
each	O
document	O
by	O
counting	O
how	O
many	O
times	O
each	O
of	O
the	O
top	O
most	O
frequent	O
words	O
occurs	O
they	O
trained	O
a	O
deep	B
auto-encoder	B
with	O
layers	O
on	O
half	O
of	O
the	O
data	O
the	O
visible	B
units	O
use	O
a	O
replicated	O
softmax	B
distribution	O
the	O
hidden	B
units	I
in	O
the	O
middle	O
layer	O
have	O
a	O
gaussian	B
distribution	O
and	O
the	O
remaining	O
units	O
have	O
the	O
usual	O
bernoulli-logistic	O
distribution	O
when	O
fine	O
tuning	O
the	O
auto-encoder	B
a	O
cross-entropy	B
loss	B
function	I
to	O
maximum	O
likelihood	B
under	O
a	O
multinoulli	B
distribution	I
was	O
used	O
see	O
and	O
salakhutdinov	O
for	O
further	O
details	O
chapter	O
deep	B
learning	B
figure	O
a	O
small	O
convolutional	O
rbm	B
with	O
two	O
groups	O
of	O
hidden	B
units	I
each	O
associated	O
with	O
a	O
filter	O
of	O
size	O
are	O
two	O
different	O
views	B
of	O
the	O
data	O
in	O
the	O
first	O
window	O
the	O
first	O
view	O
is	O
computed	O
using	O
the	O
filter	O
the	O
second	O
view	O
using	O
filter	O
similarly	O
are	O
the	O
views	B
of	O
the	O
data	O
in	O
the	O
second	O
window	O
computed	O
using	O
and	O
respectively	O
and	O
and	O
hamming	B
distance	I
of	O
say	O
this	O
results	O
in	O
retrieving	O
about	O
the	O
key	O
point	O
is	O
that	O
the	O
total	O
time	O
is	O
independent	O
of	O
the	O
size	O
of	O
the	O
corpus	B
of	O
course	O
there	O
are	O
other	O
techniques	O
for	O
fast	O
document	O
retrieval	O
such	O
as	O
inverted	B
indices	I
these	O
rely	O
on	O
the	O
fact	O
that	O
individual	O
words	O
are	O
quite	O
informative	O
so	O
we	O
can	O
simply	O
intersect	O
all	O
the	O
documents	O
that	O
contain	O
each	O
word	O
however	O
when	O
performing	O
image	O
retrieval	O
it	O
is	O
clear	O
that	O
we	O
do	O
not	O
want	O
to	O
work	O
at	O
the	O
pixel	O
level	O
recently	O
and	O
hinton	O
showed	O
that	O
a	O
deep	B
autoencoder	O
could	O
learn	O
a	O
good	O
semantic	B
hashing	I
function	O
that	O
outperformed	O
previous	O
techniques	O
et	O
al	O
weiss	O
et	O
al	O
on	O
the	O
million	O
tiny	O
images	O
dataset	O
it	O
is	O
hard	O
to	O
apply	O
inverted	O
indexing	O
techniques	O
to	O
real-valued	O
data	O
one	O
could	O
imagine	O
vector	O
quantizing	O
image	O
patches	O
learning	B
audio	O
features	B
using	O
convolutional	B
dbns	I
to	O
apply	O
dbns	O
to	O
time	O
series	O
of	O
unbounded	O
length	O
it	O
is	O
necessary	O
to	O
use	O
some	O
form	O
of	O
parameter	B
tying	I
one	O
way	O
to	O
do	O
this	O
is	O
to	O
use	O
convolutional	B
dbns	I
et	O
al	O
desjardins	O
and	O
bengio	O
which	O
use	O
convolutional	O
rbms	O
as	O
their	O
basic	O
unit	O
these	O
models	O
are	O
a	O
generative	O
version	O
of	O
convolutional	B
neural	I
nets	I
discussed	O
in	O
section	O
the	O
basic	O
idea	O
is	O
illustrated	O
in	O
figure	O
the	O
hidden	B
activation	B
vector	O
for	O
each	O
group	O
is	O
computed	O
by	O
convolving	O
the	O
input	O
vector	O
with	O
that	O
group	O
s	O
filter	O
vector	O
or	O
matrix	O
in	O
other	O
words	O
each	O
node	O
within	O
a	O
hidden	B
group	O
is	O
a	O
weighted	O
combination	O
of	O
a	O
subset	O
of	O
the	O
inputs	O
we	O
compute	O
the	O
activaton	O
of	O
all	O
the	O
hidden	B
nodes	B
by	O
sliding	O
this	O
weight	B
vector	I
over	O
the	O
input	O
this	O
allows	O
us	O
to	O
model	O
translation	B
invariance	I
since	O
we	O
use	O
the	O
same	O
weights	O
no	O
matter	O
where	O
in	O
the	O
input	O
vector	O
the	O
pattern	B
each	O
group	O
has	O
its	O
own	O
filter	O
corresponding	O
to	O
its	O
own	O
pattern	B
detector	O
k	O
is	O
the	O
number	O
of	O
bit	O
vectors	O
that	O
are	O
up	O
to	O
a	O
hamming	B
distance	I
of	O
away	O
note	O
that	O
it	O
is	O
often	O
said	O
that	O
the	O
goal	O
of	O
deep	B
learnng	O
is	O
to	O
discover	O
invariant	B
features	B
e	O
g	O
a	O
representation	O
of	O
an	O
object	O
that	O
does	O
not	O
change	O
even	O
as	O
nuisance	B
variables	I
such	O
as	O
the	O
lighting	O
do	O
change	O
however	O
sometimes	O
these	O
so-called	O
nuisance	B
variables	I
may	O
be	O
the	O
variables	O
of	O
interest	O
for	O
example	O
if	O
the	O
task	O
is	O
to	O
determine	O
if	O
a	O
photograph	O
was	O
taken	O
in	O
the	O
morning	O
or	O
the	O
evening	O
then	O
lighting	O
is	O
one	O
of	O
the	O
more	O
salient	O
features	B
and	O
object	O
identity	O
may	O
be	O
less	O
relevant	O
as	O
always	O
one	O
task	O
s	O
signal	O
is	O
another	O
task	O
s	O
noise	O
so	O
it	O
unwise	O
to	O
throw	O
away	O
apparently	O
irrelevant	O
information	B
discussion	O
more	O
formally	O
for	O
binary	O
signals	O
we	O
can	O
define	O
the	O
full	B
conditionals	O
in	O
a	O
convolutional	O
rbm	B
as	O
follows	O
et	O
al	O
t	O
sigmwk	O
vt	O
bt	O
phk	O
pvs	O
sigm	O
hks	O
cs	O
k	O
where	O
wk	O
is	O
the	O
weight	B
vector	I
for	O
group	O
k	O
bt	O
and	O
cs	O
are	O
bias	B
terms	O
and	O
a	O
b	O
represents	O
the	O
convolution	O
of	O
vectors	O
a	O
and	O
b	O
it	O
is	O
common	O
to	O
add	O
a	O
max	B
pooling	I
layer	O
as	O
well	O
as	O
a	O
convolutional	O
layer	O
which	O
computes	O
a	O
local	O
maximum	O
over	O
the	O
filtered	O
response	O
this	O
allows	O
for	O
a	O
small	O
amount	O
of	O
translation	B
invariance	I
it	O
also	O
reduces	O
the	O
size	O
of	O
the	O
higher	O
levels	O
which	O
speeds	O
up	O
computation	O
considerably	O
defining	O
this	O
for	O
a	O
neural	B
network	I
is	O
simple	O
but	O
defining	O
this	O
in	O
a	O
way	O
which	O
allows	O
for	O
information	B
flow	O
backwards	O
as	O
well	O
as	O
forwards	O
is	O
a	O
bit	O
more	O
involved	O
the	O
basic	O
idea	O
is	O
similar	B
to	O
a	O
noisy-or	B
cpd	B
where	O
we	O
define	O
a	O
probabilistic	O
relationship	O
between	O
the	O
max	O
node	O
and	O
the	O
parts	O
it	O
is	O
maxing	O
over	O
see	O
et	O
al	O
for	O
details	O
note	O
however	O
that	O
the	O
top-down	O
generative	O
process	O
will	O
be	O
difficult	O
since	O
the	O
max	B
pooling	I
operation	O
throws	O
away	O
so	O
much	O
information	B
et	O
al	O
applies	O
convolutional	B
dbns	I
of	O
depth	O
to	O
auditory	O
data	O
when	O
the	O
input	O
consists	O
of	O
speech	O
signals	O
the	O
method	O
recovers	O
a	O
representation	O
that	O
is	O
similar	B
to	O
phonemes	B
when	O
applied	O
to	O
music	O
classification	O
and	O
speaker	O
identification	O
their	O
method	O
outperforms	O
techniques	O
using	O
standard	O
features	B
such	O
as	O
mfcc	B
features	B
were	O
fed	O
into	O
the	O
same	O
discriminative	B
classifier	O
in	O
et	O
al	O
a	O
deep	B
neural	O
net	O
was	O
used	O
in	O
place	O
of	O
a	O
gmm	B
inside	O
a	O
conventional	O
hmm	B
the	O
use	O
of	O
dnns	O
significantly	O
improved	O
performance	O
on	O
conversational	O
speech	B
recognition	I
in	O
an	O
interview	O
the	O
tech	O
lead	O
of	O
this	O
project	O
said	O
historically	O
there	O
have	O
been	O
very	O
few	O
individual	O
technologies	O
in	O
speech	B
recognition	I
that	O
have	O
led	O
to	O
improvements	O
of	O
this	O
magnitude	O
learning	B
image	O
features	B
using	O
convolutional	B
dbns	I
we	O
can	O
extend	O
a	O
convolutional	O
dbn	B
from	O
to	O
in	O
a	O
straightforward	O
way	O
et	O
al	O
as	O
illustrated	O
in	O
figure	O
the	O
results	O
of	O
a	O
layer	O
system	O
trained	O
on	O
four	O
classes	O
of	O
visual	O
objects	O
motorbikes	O
faces	O
and	O
airplanes	O
from	O
the	O
caltech	O
dataset	O
are	O
shown	O
in	O
figure	O
we	O
only	O
show	O
the	O
results	O
for	O
layers	O
and	O
because	O
layer	O
learns	O
gabor-like	O
filters	O
that	O
are	O
very	O
similar	B
to	O
those	O
learned	O
by	O
sparse	B
coding	I
shown	O
in	O
figure	O
we	O
see	O
that	O
layer	O
has	O
learned	O
some	O
generic	O
visual	O
parts	O
that	O
are	O
shared	B
amongst	O
object	O
classes	O
and	O
layer	O
seems	O
to	O
have	O
learned	O
filters	O
that	O
look	O
like	O
grandmother	B
cells	I
that	O
are	O
specific	O
to	O
individual	O
object	O
classes	O
and	O
in	O
some	O
cases	O
to	O
individual	O
objects	O
discussion	O
so	O
far	O
we	O
have	O
been	O
discussing	O
models	O
inspired	O
by	O
low-level	O
processing	O
in	O
the	O
brain	O
these	O
models	O
have	O
produced	O
useful	O
features	B
for	O
simple	O
classification	O
tasks	O
but	O
can	O
this	O
pure	B
bottom-up	O
too	O
early	O
source	O
chapter	O
deep	B
learning	B
figure	O
a	O
convolutional	O
rbm	B
with	O
max-pooling	O
layers	O
the	O
input	O
signal	O
is	O
a	O
stack	O
of	O
images	O
color	O
planes	O
each	O
input	O
layer	O
is	O
passed	O
through	O
a	O
different	O
set	O
of	O
filters	O
each	O
hidden	B
unit	O
is	O
obtained	O
by	O
convolving	O
with	O
the	O
appropriate	O
filter	O
and	O
then	O
summing	O
over	O
the	O
input	O
planes	O
the	O
final	O
layer	O
is	O
obtained	O
by	O
computing	O
the	O
local	O
maximum	O
within	O
a	O
small	O
window	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
bo	O
chen	O
faces	O
cars	O
airplanes	O
motorbikes	O
figure	O
visualization	O
of	O
the	O
filters	O
learned	O
by	O
a	O
convolutional	O
dbn	B
in	O
layers	O
two	O
and	O
three	O
source	O
figure	O
of	O
et	O
al	O
used	O
with	O
kind	O
permission	O
of	O
honglak	O
lee	O
approach	O
scale	O
to	O
more	O
challenging	O
problems	O
such	O
as	O
scene	O
interpretation	O
or	O
natural	O
language	O
understanding	O
to	O
put	O
the	O
problem	O
in	O
perspective	O
consider	O
the	O
dbn	B
for	O
handwritten	O
digit	O
classification	O
in	O
figure	O
this	O
has	O
about	O
free	O
parameters	O
although	O
this	O
is	O
a	O
lot	O
it	O
is	O
tiny	O
compared	O
to	O
the	O
number	O
of	O
neurons	O
in	O
the	O
brain	O
as	O
hinton	O
says	O
this	O
is	O
about	O
as	O
many	O
parameters	O
as	O
cubic	O
millimetres	O
of	O
mouse	O
cortex	O
and	O
several	O
hundred	O
networks	O
of	O
this	O
complexity	O
could	O
fit	O
within	O
a	O
single	O
voxel	O
of	O
a	O
high-resolution	O
fmri	O
scan	O
this	O
suggests	O
that	O
much	O
bigger	O
networks	O
may	O
be	O
required	O
to	O
compete	O
with	O
human	O
shape	O
recognition	O
abilities	O
et	O
al	O
to	O
scale	O
up	O
to	O
more	O
challenging	O
problems	O
various	O
groups	O
are	O
using	O
gpus	B
e	O
g	O
et	O
al	O
andor	O
parallel	O
computing	O
but	O
perhaps	O
a	O
more	O
efficient	O
approach	O
is	O
to	O
work	O
at	O
a	O
higher	O
level	O
of	O
abstraction	O
where	O
inference	B
is	O
done	O
in	O
the	O
space	O
of	O
objects	O
or	O
their	O
parts	O
rather	O
discussion	O
than	O
in	O
the	O
space	O
of	O
bits	B
and	O
pixels	O
that	O
is	O
we	O
want	O
to	O
bridge	O
the	O
signal-to-symbol	B
divide	O
where	O
by	O
symbol	O
we	O
mean	B
something	O
atomic	O
that	O
can	O
be	O
combined	O
with	O
other	O
symbols	O
in	O
a	O
compositional	O
way	O
the	O
question	O
of	O
how	O
to	O
convert	O
low	O
level	O
signals	O
into	O
a	O
more	O
structured	O
semantic	O
representation	O
is	O
known	O
as	O
the	O
symbol	B
grounding	I
problem	O
traditionally	O
such	O
symbols	O
are	O
associated	O
with	O
words	O
in	O
natural	O
language	O
but	O
it	O
seems	O
unlikely	O
we	O
can	O
jump	O
directly	O
from	O
low-level	O
signals	O
to	O
high-level	O
semantic	O
concepts	O
instead	O
what	O
we	O
need	O
is	O
an	O
intermediate	O
level	O
of	O
symbolic	O
or	O
atomic	O
parts	O
a	O
very	O
simple	O
way	O
to	O
create	O
such	O
parts	O
from	O
real-valued	O
signals	O
such	O
as	O
images	O
is	O
to	O
apply	O
vector	B
quantization	I
this	O
generates	O
a	O
set	O
of	O
visual	B
words	I
these	O
can	O
then	O
be	O
modelled	O
using	O
some	O
of	O
the	O
techniques	O
from	O
chapter	O
for	O
modeling	O
bags	O
of	O
words	O
such	O
models	O
however	O
are	O
still	O
quite	O
shallow	O
it	O
is	O
possible	O
to	O
define	O
and	O
learn	O
deep	B
models	O
which	O
use	O
discrete	B
latent	B
parts	O
here	O
we	O
just	O
mention	O
a	O
few	O
recent	O
approaches	O
to	O
give	O
a	O
flavor	O
of	O
the	O
possibilites	O
et	O
al	O
combine	O
rbms	O
with	O
hierarchical	O
latent	B
dirichlet	B
allocation	I
methods	O
trained	O
in	O
an	O
unsupervised	O
way	O
et	O
al	O
use	O
latent	B
and-or	B
graphs	I
trained	O
in	O
a	O
manner	O
similar	B
to	O
a	O
latent	B
structural	O
svm	B
a	O
similar	B
approach	O
based	O
on	O
grammars	B
is	O
described	O
in	O
et	O
al	O
what	O
is	O
interesting	O
about	O
these	O
techniques	O
is	O
that	O
they	O
apply	O
data-driven	O
machine	B
learning	B
methods	O
to	O
rich	O
structuredsymbolic	O
ai-style	O
models	O
this	O
seems	O
like	O
a	O
promising	O
future	O
direction	O
for	O
machine	B
learning	B
notation	O
introduction	O
it	O
is	O
very	O
difficult	O
to	O
come	O
up	O
with	O
a	O
single	O
consistent	B
notation	O
to	O
cover	O
the	O
wide	O
variety	O
of	O
data	O
models	O
and	O
algorithms	O
that	O
we	O
discuss	O
furthermore	O
conventions	O
differ	O
between	O
machine	B
learning	B
and	O
statistics	O
and	O
between	O
different	O
books	O
and	O
papers	O
nevertheless	O
we	O
have	O
tried	O
to	O
be	O
as	O
consistent	B
as	O
possible	O
below	O
we	O
summarize	O
most	O
of	O
the	O
notation	O
used	O
in	O
this	O
book	O
although	O
individual	O
sections	O
may	O
introduce	O
new	O
notation	O
note	O
also	O
that	O
the	O
same	O
symbol	O
may	O
have	O
different	O
meanings	O
depending	O
on	O
the	O
context	O
although	O
we	O
try	O
to	O
avoid	O
this	O
where	O
possible	O
general	O
math	O
notation	O
symbol	O
x	O
y	O
x	O
y	O
a	O
b	O
a	O
b	O
a	O
ix	O
n	O
o	O
r	O
argmaxx	O
f	O
meaning	O
floor	O
of	O
x	O
i	O
e	O
round	O
down	O
to	O
nearest	O
integer	O
ceiling	O
of	O
x	O
i	O
e	O
round	O
up	O
to	O
nearest	O
integer	O
convolution	O
of	O
x	O
and	O
y	O
hadamard	B
product	I
of	O
x	O
and	O
y	O
logical	O
and	O
logical	O
or	O
logical	O
not	O
indicator	B
function	I
ix	O
if	O
x	O
is	O
true	O
else	O
ix	O
infinity	O
tends	O
towards	O
e	O
g	O
n	O
proportional	O
to	O
so	O
y	O
ax	O
can	O
be	O
written	O
as	O
y	O
x	O
absolute	O
value	O
size	O
of	O
a	O
set	O
factorial	O
function	O
vector	O
of	O
first	O
derivatives	O
hessian	B
matrix	O
of	O
second	O
derivatives	O
defined	O
as	O
big-o	O
roughly	O
means	O
order	O
of	O
magnitude	O
the	O
real	O
numbers	O
range	O
convention	O
n	O
approximately	O
equal	O
to	O
argmax	O
the	O
value	O
x	O
that	O
maximizes	O
f	O
notation	O
ba	O
b	O
b	O
n	O
k	O
ij	O
xy	O
expx	O
x	O
k	O
k	O
k	O
k	O
beta	B
function	I
ba	O
b	O
multivariate	O
beta	B
function	I
n	O
choose	O
k	O
equal	O
to	O
n	O
k	O
n	O
k	O
dirac	B
delta	I
function	I
if	O
x	O
else	O
kronecker	O
delta	O
equals	O
if	O
i	O
j	O
otherwise	O
equals	O
kronecker	O
delta	O
equals	O
if	O
x	O
y	O
otherwise	O
equals	O
exponential	O
function	O
ex	O
gamma	B
function	I
digamma	B
function	O
d	O
a	O
set	O
from	O
which	O
values	O
are	O
drawn	O
x	O
r	O
d	O
ux	O
udu	O
dx	O
log	O
linear	O
algebra	O
notation	O
we	O
use	O
boldface	O
lowercase	O
to	O
denote	O
vectors	O
such	O
as	O
a	O
and	O
boldface	O
uppercase	O
to	O
denote	O
matrices	O
such	O
as	O
a	O
vectors	O
are	O
assumed	O
to	O
be	O
column	O
vectors	O
unless	O
noted	O
otherwise	O
symbol	O
a	O
tra	O
deta	O
a	O
a	O
at	O
at	O
diaga	O
diaga	O
i	O
or	O
id	O
or	O
or	O
aj	O
ai	B
aij	O
x	O
y	O
meaning	O
a	O
is	O
a	O
positive	O
definite	O
matrix	O
trace	B
of	O
a	O
matrix	O
determinant	O
of	O
matrix	O
a	O
determinant	O
of	O
matrix	O
a	O
inverse	O
of	O
a	O
matrix	O
pseudo-inverse	O
of	O
a	O
matrix	O
transpose	O
of	O
a	O
matrix	O
transpose	O
of	O
a	O
vector	O
diagonal	B
matrix	O
made	O
from	O
vector	O
a	O
diagonal	B
vector	O
extracted	O
from	O
matrix	O
a	O
identity	O
matrix	O
of	O
size	O
d	O
d	O
on	O
diagonal	B
zeros	O
off	O
vector	O
of	O
ones	O
length	O
d	O
vector	O
of	O
zeros	O
length	O
d	O
euclidean	O
or	O
norm	O
norm	O
j	O
th	O
column	O
of	O
matrix	O
transpose	O
of	O
i	O
th	O
row	O
of	O
matrix	O
column	O
vector	O
element	O
j	O
of	O
matrix	O
a	O
tensor	B
product	I
of	O
x	O
and	O
y	O
j	O
probability	O
notation	O
we	O
denote	O
random	O
and	O
fixed	O
scalars	O
by	O
lower	O
case	O
random	O
and	O
fixed	O
vectors	O
by	O
bold	O
lower	O
case	O
and	O
random	O
and	O
fixed	O
matrices	O
by	O
bold	O
upper	O
case	O
occastionally	O
we	O
use	O
non-bold	O
upper	O
case	O
to	O
denote	O
scalar	O
random	O
variables	O
also	O
we	O
use	O
p	O
for	O
both	O
discrete	B
and	O
continuous	O
random	O
variables	O
notation	O
symbol	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
p	O
cov	O
e	O
eq	O
h	O
or	O
h	O
i	O
y	O
kl	O
l	O
a	O
mode	B
px	O
pxy	O
sigmx	O
var	B
z	O
meaning	O
x	O
is	O
independent	O
of	O
y	O
x	O
is	O
not	O
independent	O
of	O
y	O
x	O
is	O
conditionally	B
independent	I
of	O
y	O
given	O
z	O
x	O
is	O
not	O
conditionally	B
independent	I
of	O
y	O
given	O
z	O
x	O
is	O
distributed	O
according	O
to	O
distribution	O
p	O
parameters	O
of	O
a	O
beta	O
or	O
dirichlet	B
distribution	I
covariance	B
of	O
x	O
expected	B
value	I
of	O
x	O
expected	B
value	I
of	O
x	O
wrt	O
distribution	O
q	O
entropy	B
of	O
distribution	O
px	O
mutual	B
information	B
between	O
x	O
and	O
y	O
kl	B
divergence	I
from	O
distribution	O
p	O
to	O
q	O
log-likelihood	O
function	O
loss	B
function	I
for	O
taking	O
action	B
a	O
when	O
true	O
state	B
of	O
nature	O
is	O
precision	B
variance	B
precision	B
matrix	I
most	O
probable	O
value	O
of	O
x	O
mean	B
of	O
a	O
scalar	O
distribution	O
mean	B
of	O
a	O
multivariate	O
distribution	O
probability	O
density	O
or	O
mass	O
function	O
conditional	B
probability	I
density	O
of	O
x	O
given	O
y	O
cdf	B
of	O
standard	B
normal	B
pdf	B
of	O
standard	B
normal	B
multinomial	B
parameter	B
vector	O
stationary	B
distribution	I
of	O
markov	B
chain	I
correlation	B
coefficient	I
sigmoid	B
function	O
variance	B
covariance	B
matrix	I
variance	B
of	O
x	O
degrees	B
of	I
freedom	I
parameter	B
normalization	O
constant	O
of	O
a	O
probability	O
distribution	O
x	O
machine	O
learningstatistics	O
notation	O
in	O
general	O
we	O
use	O
upper	O
case	O
letters	O
to	O
denote	O
constants	O
such	O
as	O
c	O
d	O
k	O
n	O
s	O
t	O
etc	O
we	O
use	O
lower	O
case	O
letters	O
as	O
dummy	O
indexes	O
of	O
the	O
appropriate	O
range	O
such	O
as	O
c	O
to	O
index	O
classes	O
j	O
to	O
index	O
input	O
features	B
k	O
to	O
index	O
states	O
or	O
clusters	B
s	O
to	O
index	O
samples	B
t	O
to	O
index	O
time	O
etc	O
to	O
index	O
data	O
cases	O
we	O
use	O
the	O
notation	O
i	O
although	O
the	O
notation	O
n	O
n	O
is	O
also	O
widely	O
used	O
we	O
use	O
x	O
to	O
represent	O
an	O
observed	O
data	O
vector	O
in	O
a	O
supervised	O
problem	O
we	O
use	O
y	O
or	O
y	O
to	O
represent	O
the	O
desired	O
output	O
label	B
we	O
use	O
z	O
to	O
represent	O
a	O
hidden	B
variable	I
sometimes	O
we	O
also	O
use	O
q	O
to	O
represent	O
a	O
hidden	B
discrete	B
variable	O
notation	O
symbol	O
c	O
d	O
r	O
d	O
dtest	O
j	O
k	O
y	O
k	O
n	O
nc	O
q	O
q	O
old	O
s	O
t	O
t	O
t	O
m	O
l	O
m	O
ap	O
w	O
w	O
xij	O
xi	O
x	O
x	O
x	O
x	O
y	O
zij	O
iyn	O
c	O
meaning	O
number	O
of	O
classes	O
dimensionality	O
of	O
data	O
vector	O
of	O
features	B
number	O
of	O
outputs	O
variables	O
training	O
data	O
d	O
or	O
d	O
yii	O
test	O
data	O
cost	O
function	O
number	O
of	O
states	O
or	O
dimensions	O
of	O
a	O
variable	O
latent	B
kernel	B
function	I
kernel	B
matrix	O
strength	O
of	O
or	O
regularizer	O
number	O
of	O
data	O
cases	O
number	O
of	O
examples	O
of	O
class	O
c	O
nc	O
basis	B
function	I
expansion	I
of	O
feature	O
vector	O
x	O
basis	B
function	I
expansion	I
of	O
design	B
matrix	I
x	O
approximate	O
or	O
proposal	B
distribution	I
auxiliary	B
function	I
in	O
em	B
number	O
of	O
samples	B
length	O
of	O
a	O
sequence	O
test	B
statistic	I
for	O
data	O
transition	B
matrix	I
of	O
markov	B
chain	I
parameter	B
vector	O
s	O
th	O
sample	O
of	O
parameter	B
vector	O
estimate	O
mle	B
or	O
map	O
of	O
maximum	B
likelihood	B
estimate	I
of	O
map	B
estimate	I
of	O
estimate	O
posterior	B
mean	B
of	O
vector	O
of	O
regression	B
weights	O
in	O
statistics	O
matrix	O
of	O
regression	B
weights	O
component	O
feature	O
j	O
of	O
data	O
case	O
i	O
fori	O
d	O
training	O
case	O
i	O
design	B
matrix	I
of	O
size	O
n	O
d	O
empirical	O
mean	B
x	O
n	O
future	O
test	O
case	O
future	O
test	O
case	O
vector	O
of	O
all	O
training	O
labels	O
y	O
yn	O
latent	B
component	O
j	O
for	O
case	O
i	O
xi	O
graphical	B
model	I
notation	O
in	O
graphical	B
models	I
we	O
index	O
nodes	B
by	O
s	O
t	O
u	O
v	O
and	O
states	O
by	O
i	O
j	O
k	O
x	O
notation	O
symbol	O
meaning	O
s	O
t	O
bel	O
c	O
chj	O
descj	O
g	O
e	O
mbt	O
nbdt	O
pat	O
predt	O
cxc	O
s	O
sjk	O
v	O
node	O
s	O
is	O
connected	O
to	O
node	O
t	O
belief	O
function	O
cliques	B
of	O
a	O
graph	B
child	O
of	O
node	O
j	O
in	O
a	O
dag	B
descendants	B
of	O
node	O
j	O
in	O
a	O
dag	B
a	O
graph	B
edges	B
of	O
a	O
graph	B
markov	B
blanket	I
of	O
node	O
t	O
neighborhood	O
of	O
node	O
t	O
parents	B
of	O
node	O
t	O
in	O
a	O
dag	B
predecessors	O
of	O
node	O
t	O
in	O
a	O
dag	B
wrt	O
some	O
ordering	O
potential	B
function	I
for	O
clique	B
c	O
separators	O
of	O
a	O
graph	B
prob	O
node	O
s	O
is	O
in	O
state	B
k	O
given	O
its	O
parents	B
are	O
in	O
states	O
j	O
nodes	B
of	O
a	O
graph	B
notation	O
list	O
of	O
commonly	O
used	O
abbreviations	O
abbreviation	O
meaning	O
cdf	B
cpd	B
cpt	O
crf	B
dag	B
dgm	B
eb	B
em	B
ep	B
glm	B
gmm	B
hmm	B
iid	B
iff	B
kl	O
lds	B
lhs	O
map	O
mcmc	B
mh	B
mle	B
mpm	B
mrf	B
mse	B
nll	B
ols	B
pd	O
pdf	B
pmf	B
rbpf	B
rhs	O
rjmcmc	B
rss	O
slds	O
sse	O
ugm	B
vb	B
wrt	O
cumulative	B
distribution	I
function	I
conditional	B
probability	I
distribution	I
conditional	B
probability	I
table	O
conditional	O
random	O
field	O
directed	B
acyclic	O
graphic	O
directed	B
graphical	B
model	I
empirical	B
bayes	I
expectation	B
maximization	I
algorithm	O
expectation	B
propagation	I
generalized	B
linear	I
model	I
gaussian	B
mixture	B
model	I
hidden	B
markov	B
model	I
independent	B
and	I
identically	I
distributed	I
if	O
and	O
only	O
if	O
kullback	O
leibler	O
divergence	O
linear	B
dynamical	I
system	I
left	O
hand	O
side	O
an	O
equation	O
maximum	O
a	O
posterior	O
estimate	O
markov	B
chain	I
monte	B
carlo	I
metropolis	B
hastings	I
maximum	B
likelihood	B
estimate	I
maximum	O
of	O
posterior	O
marginals	O
markov	B
random	O
field	O
mean	B
squared	B
error	I
negative	B
log	I
likelihood	B
ordinary	B
least	B
squares	I
positive	O
definite	O
probability	B
density	I
function	I
probability	B
mass	I
function	I
rao-blackwellised	O
particle	O
filter	O
right	O
hand	O
side	O
an	O
equation	O
reversible	B
jump	I
mcmc	B
residual	B
sum	B
of	I
squares	I
switching	B
linear	B
dynamical	I
system	I
sum	B
of	I
squared	I
errors	I
undirected	B
graphical	B
model	I
variational	B
bayes	I
with	O
respect	O
to	O
aji	O
s	O
m	O
and	O
r	O
j	O
mceliece	O
march	O
the	O
generalized	O
distributive	B
law	I
info	O
theory	O
ieee	O
trans	O
alag	O
s	O
and	O
a	O
agogino	O
inference	B
using	O
message	O
propogation	O
and	O
topology	O
transformation	O
in	O
vector	O
gaussian	B
continuous	O
networks	O
in	O
uai	O
albers	O
c	O
m	O
leisink	O
and	O
h	O
kappen	O
the	O
cluster	O
variation	O
method	O
for	O
efficient	O
linkage	O
analysis	O
on	O
extended	O
pedigrees	O
bmc	O
bioinformatics	O
albert	O
j	O
and	O
s	O
chib	O
bayesian	B
analysis	O
of	O
binary	O
and	O
polychotomous	O
response	O
data	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
allwein	O
e	O
r	O
schapire	O
and	O
y	O
singer	O
reducing	O
multiclass	O
to	O
binary	O
a	O
unifying	O
approach	O
for	O
margin	B
classifiers	O
j	O
of	O
machine	B
learning	B
research	O
aloise	O
d	O
a	O
deshpande	O
p	O
hansen	O
and	O
p	O
popat	O
np-hardness	O
of	O
euclidean	O
sum-of-squares	O
clustering	B
machine	B
learning	B
alpaydin	O
e	O
machine	B
learning	B
mit	O
press	O
introduction	O
to	O
altun	O
y	O
t	O
hofmann	O
and	O
i	O
tsochantaridis	O
large	O
margin	B
methods	O
for	O
structured	O
and	O
interdependent	O
output	O
variables	O
in	O
g	O
bakir	O
t	O
hofmann	O
b	O
scholkopf	O
a	O
smola	O
b	O
taskar	O
and	O
s	O
vishwanathan	O
machine	B
learning	B
with	O
structured	O
outputs	O
mit	O
press	O
amir	O
e	O
approximation	O
algorithms	O
for	O
treewidth	B
algorithmica	O
amir	O
e	O
and	O
s	O
mcilraith	O
partition-based	O
reasoning	O
for	O
first-order	O
and	O
propositional	O
artificial	O
intelligence	O
theories	O
logical	O
bibliography	O
abend	O
k	O
t	O
j	O
harley	O
and	O
l	O
n	O
kanal	O
classification	O
of	O
binary	O
random	O
patterns	O
ieee	O
transactions	O
on	O
information	B
theory	I
ackley	O
d	O
g	O
hinton	O
and	O
t	O
sejnowski	O
a	O
learning	B
algorithm	O
for	O
boltzmann	O
machines	O
cognitive	O
science	O
adams	O
r	O
p	O
h	O
wallach	O
and	O
z	O
ghahramani	O
learning	B
the	O
structure	O
of	O
deep	B
sparse	B
graphical	B
models	I
in	O
aistatistics	O
aggarwal	O
d	O
and	O
s	O
merugu	O
predictive	B
discrete	B
latent	B
factor	B
models	O
for	O
large	O
scale	O
dyadic	B
data	O
in	O
proc	O
of	O
the	O
int	O
l	O
conf	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
ahmed	O
a	O
and	O
e	O
xing	O
on	O
tight	O
approximate	B
inference	B
of	O
the	O
logistic-normal	O
topic	B
admixture	O
model	O
in	O
aistatistics	O
ahn	O
j	O
-h	O
and	O
j	O
-h	O
oh	O
a	O
constrained	O
em	B
algorithm	O
for	O
principal	B
component	I
analysis	O
neural	O
computation	O
ahn	O
s	O
a	O
korattikara	O
and	O
m	O
welling	O
bayesian	B
posterior	O
sampling	O
via	O
stochastic	O
gradient	O
fisher	O
scoring	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
airoldi	O
e	O
d	O
blei	O
s	O
fienberg	O
and	O
e	O
xing	O
mixed-membership	O
stochastic	O
blockmodels	O
j	O
of	O
machine	B
learning	B
research	O
ando	O
r	O
and	O
t	O
zhang	O
a	O
framework	O
for	O
learning	B
predictive	B
structures	O
from	O
multiple	O
tasks	O
and	O
unlabeled	O
data	O
j	O
of	O
machine	B
learning	B
research	O
aitchison	O
j	O
the	O
statistical	O
analysis	O
of	O
compositional	O
data	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
andrews	O
d	O
and	O
c	O
mallows	O
scale	O
mixtures	O
of	O
normal	B
distributions	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
andrieu	O
c	O
n	O
de	O
freitas	O
and	O
a	O
doucet	O
sequential	B
bayesian	B
estimation	O
and	O
model	B
selection	I
for	O
dynamic	O
kernel	B
machines	O
technical	O
report	O
cambridge	O
univ	O
andrieu	O
c	O
n	O
de	O
freitas	O
and	O
a	O
doucet	O
robust	B
full	B
bayesian	B
learning	B
for	O
radial	O
basis	O
networks	O
neural	O
computation	O
andrieu	O
c	O
n	O
de	O
freitas	O
a	O
doucet	O
and	O
m	O
jordan	O
an	O
introduction	O
to	O
mcmc	B
for	O
machine	B
learning	B
machine	B
learning	B
andrieu	O
c	O
a	O
doucet	O
and	O
v	O
tadic	O
online	B
em	B
for	O
parameter	B
estimation	O
in	O
nonlinear-non	O
gaussian	B
state-space	O
models	O
in	O
proc	O
ieee	O
cdc	O
andrieu	O
c	O
and	O
j	O
thoms	O
a	O
tutorial	O
on	O
adaptive	B
mcmc	B
statistical	O
computing	O
aoki	O
m	O
state	B
space	I
modeling	O
of	O
time	O
series	O
springer	O
archambeau	O
c	O
and	O
f	O
bach	O
sparse	B
probabilistic	O
projections	O
in	O
nips	O
argyriou	O
a	O
t	O
evgeniou	O
and	O
m	O
pontil	O
convex	B
multi-task	O
feature	O
learning	B
machine	B
learning	B
armagan	O
a	O
d	O
dunson	O
and	O
j	O
lee	O
generalized	O
double	O
pareto	O
shrinkage	B
technical	O
report	O
duke	O
armstrong	O
h	O
bayesian	B
estimation	O
of	O
decomposable	B
gaussian	B
graphical	B
models	I
thesis	O
unsw	O
ph	O
d	O
armstrong	O
h	O
c	O
carter	O
k	O
wong	O
and	O
r	O
kohn	O
bayesian	B
covariance	B
matrix	I
estimation	O
using	O
a	O
mixture	B
of	O
decomposable	B
graphical	B
models	I
statistics	O
and	O
computing	O
arnborg	O
s	O
d	O
g	O
corneil	O
and	O
a	O
proskurowski	O
complexity	O
of	O
finding	O
embeddings	O
in	O
a	O
ktree	O
siam	O
j	O
on	O
algebraic	O
and	O
discrete	B
methods	O
arora	O
s	O
and	O
b	O
barak	O
complexity	O
theory	O
a	O
modern	O
approach	O
cambridge	O
arthur	O
d	O
and	O
s	O
vassilvitskii	O
kmeans	O
the	O
advantages	O
of	O
careful	O
seeding	O
in	O
proc	O
acm-siam	O
symp	O
on	O
discrete	B
algorithms	O
pp	O
a	O
bibliography	O
arulampalam	O
m	O
s	O
maskell	O
n	O
gordon	O
and	O
t	O
clapp	O
february	O
a	O
tutorial	O
on	O
particle	O
filters	O
for	O
online	O
nonlinearnongaussian	O
bayesian	B
tracking	B
ieee	O
trans	O
on	O
signal	B
processing	I
asavathiratham	O
c	O
the	O
influence	O
model	O
a	O
tractable	O
representation	O
for	O
the	O
dynamics	O
of	O
networked	O
markov	B
chains	O
ph	O
d	O
thesis	O
mit	O
dept	O
eecs	O
atay-kayis	O
a	O
and	O
h	O
massam	O
a	O
monte	B
carlo	I
method	O
for	O
computing	O
the	O
marginal	B
likelihood	B
in	O
nondecomposable	O
gaussian	B
graphical	B
models	I
biometrika	O
attenberg	O
j	O
k	O
weinberger	O
a	O
smola	O
a	O
dasgupta	O
and	O
m	O
zinkevich	O
collaborative	O
spam	B
filtering	B
with	O
the	O
hashing	O
trick	O
in	O
virus	O
bulletin	O
attias	O
h	O
independent	O
factor	B
analysis	I
neural	O
computation	O
attias	O
h	O
a	O
variational	O
bayesian	B
framework	O
for	O
graphical	B
models	I
in	O
bach	O
f	O
bolasso	B
model	O
consistent	B
lasso	B
estimation	O
through	O
the	O
bootstrap	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
bach	O
f	O
and	O
m	O
jordan	O
thin	B
junction	B
trees	I
in	O
nips	O
bach	O
f	O
and	O
m	O
jordan	O
a	O
probabilistic	O
interpretation	O
of	O
canonical	B
correlation	I
analysis	I
technical	O
report	O
u	O
c	O
berkeley	O
bach	O
f	O
and	O
e	O
moulines	O
nonasymptotic	O
analysis	O
of	O
stochastic	B
approximation	I
algorithms	O
for	O
machine	B
learning	B
in	O
nips	O
bahmani	O
b	O
b	O
moseley	O
a	O
vattani	O
r	O
kumar	O
and	O
s	O
vassilvitskii	O
scalable	O
k-means	O
in	O
vldb	O
bakker	O
b	O
and	O
t	O
heskes	O
task	O
clustering	B
and	O
gating	O
for	O
bayesian	B
multitask	O
learning	B
j	O
of	O
machine	B
learning	B
research	O
baldi	O
p	O
and	O
y	O
chauvin	O
smooth	O
online	B
learning	B
algorithms	O
for	O
hidden	B
markov	B
models	I
neural	O
computation	O
balding	O
d	O
a	O
tutorial	O
on	O
statistical	O
methods	O
for	O
population	O
association	O
studies	O
nature	O
reviews	O
genetics	O
technique	O
occuring	O
in	O
the	O
statistical	O
analysis	O
of	O
probabalistic	O
functions	O
in	O
markov	B
chains	O
the	O
annals	O
of	O
mathematical	O
statistics	O
beal	O
m	O
variational	O
algorithms	O
for	O
approximate	O
bayesian	B
inference	B
ph	O
d	O
thesis	O
gatsby	O
unit	O
beal	O
m	O
and	O
z	O
ghahramani	O
variational	O
bayesian	B
learning	B
of	O
directed	B
graphical	B
models	I
with	O
hidden	B
variables	I
bayesian	B
analysis	O
beal	O
m	O
j	O
z	O
ghahramani	O
and	O
c	O
e	O
rasmussen	O
the	O
infinite	O
hidden	B
markov	B
model	I
in	O
beck	O
a	O
and	O
m	O
teboulle	O
a	O
fast	O
iterative	O
shrinkage-thresholding	O
algorothm	O
for	O
linear	O
inverse	B
problems	I
siam	O
j	O
on	O
imaging	O
sciences	O
beinlich	O
i	O
h	O
suermondt	O
r	O
chavez	O
and	O
g	O
cooper	O
the	O
alarm	O
monitoring	O
system	O
a	O
case	O
study	O
with	O
two	O
probabilistic	B
inference	B
techniques	O
for	O
belief	B
networks	I
in	O
proc	O
of	O
the	O
second	O
european	O
conf	O
on	O
ai	B
in	O
medicine	O
pp	O
bekkerman	O
r	O
m	O
bilenko	O
and	O
j	O
langford	O
scaling	O
up	O
machine	B
learning	B
cambridge	O
bell	O
a	O
j	O
and	O
t	O
j	O
sejnowski	O
an	O
information	B
maximisation	O
approach	O
to	O
blind	O
separation	O
and	O
blind	O
deconvolution	O
neural	O
computation	O
bengio	O
y	O
learning	B
deep	B
architectures	O
for	O
ai	B
foundations	O
and	O
trends	O
in	O
machine	B
learning	B
bengio	O
y	O
and	O
s	O
bengio	O
modeling	O
high-dimensional	O
discrete	B
data	O
with	O
multi-layer	O
neural	B
networks	I
in	O
nips	O
banerjee	O
o	O
l	O
e	O
ghaoui	O
and	O
a	O
d	O
aspremont	O
model	B
selection	I
through	O
sparse	B
maximum	O
likelihood	B
estimation	O
for	O
multivariate	B
gaussian	B
or	O
binary	O
data	O
j	O
of	O
machine	B
learning	B
research	O
bar-shalom	O
y	O
and	O
t	O
fortmann	O
tracking	B
and	O
data	B
association	I
academic	O
press	O
bar-shalom	O
y	O
and	O
x	O
li	O
estimation	O
and	O
tracking	B
principles	O
techniques	O
and	O
software	O
artech	O
house	O
barash	O
y	O
and	O
n	O
friedman	O
context-specific	O
bayesian	B
clustering	B
for	O
gene	O
expression	O
data	O
j	O
comp	O
bio	B
barber	O
d	O
expectation	B
correction	I
for	O
smoothed	O
inference	B
in	O
switching	O
linear	O
dynamical	O
systems	O
j	O
of	O
machine	B
learning	B
research	O
barber	O
d	O
and	O
c	O
bishop	O
ensemble	B
learning	B
in	O
bayesian	B
neural	B
networks	I
in	O
c	O
bishop	O
neural	B
networks	I
and	O
machine	B
learning	B
pp	O
springer	O
barber	O
d	O
and	O
s	O
chiappa	O
unified	O
inference	B
for	O
variational	O
bayesian	B
linear	B
gaussian	B
state	B
space	I
models	O
in	O
nips	O
barbieri	O
m	O
and	O
j	O
berger	O
optimal	O
predictive	B
model	B
selection	I
annals	O
of	O
statistics	O
bartlett	O
p	O
m	O
jordan	O
and	O
j	O
mcauliffe	O
convexity	O
classification	O
and	O
risk	B
bounds	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
baruniak	O
r	O
compressive	O
sensieee	O
signal	B
processing	I
maga	O
ing	O
zine	O
barzilai	O
j	O
and	O
j	O
borwein	O
two	O
point	O
step	B
size	I
gradient	O
methods	O
ima	O
j	O
of	O
numerical	O
analysis	O
p	O
vincent	O
bengio	O
y	O
o	O
delalleau	O
n	O
roux	O
and	O
j	O
paiement	O
m	O
ouimet	O
learning	B
eigenfunctions	O
links	O
spectral	B
embedding	B
and	O
kernel	B
pca	B
neural	O
computation	O
bengio	O
y	O
and	O
p	O
frasconi	O
diffusion	O
of	O
context	O
and	O
credit	O
information	B
in	O
markovian	O
models	O
j	O
of	O
ai	B
research	O
bengio	O
y	O
and	O
p	O
frasconi	O
inputoutput	O
hmms	B
for	O
sequence	O
processing	O
ieee	O
trans	O
on	O
neural	B
networks	I
human	O
basu	O
s	O
t	O
choudhury	O
b	O
clarkson	O
learnand	O
a	O
pentland	O
interactions	O
with	O
ing	O
the	O
influence	O
model	O
technical	O
report	O
mit	O
media	O
lab	O
baum	O
l	O
e	O
t	O
petrie	O
g	O
soules	O
and	O
n	O
weiss	O
a	O
maximization	O
bibliography	O
bengio	O
y	O
p	O
lamblin	O
d	O
popovici	O
and	O
h	O
larochelle	O
greedy	O
layer-wise	O
training	O
of	O
deep	B
networks	I
in	O
nips	O
berchtold	O
a	O
the	O
double	O
chain	O
markov	B
model	I
comm	O
stat	O
theor	O
methods	O
berger	O
j	O
bayesian	B
salesmanship	O
in	O
p	O
k	O
goel	O
and	O
a	O
zellner	O
bayesian	B
inference	B
and	O
decision	B
techniques	O
with	O
applications	O
essays	O
in	O
honor	O
of	O
bruno	O
definetti	O
north-holland	O
berger	O
j	O
and	O
r	O
wolpert	O
the	O
likelihood	B
principle	I
the	O
institute	O
of	O
mathematical	O
statistics	O
edition	O
berkhin	O
p	O
a	O
survey	O
of	O
clustering	B
datamining	O
techniques	O
in	O
j	O
kogan	O
c	O
nicholas	O
and	O
m	O
teboulle	O
grouping	O
multidimensional	O
data	O
recent	O
advances	O
in	O
clustering	B
pp	O
springer	O
bernardo	O
j	O
and	O
a	O
smith	O
bayesian	B
theory	O
john	O
wiley	O
berrou	O
c	O
a	O
glavieux	O
and	O
p	O
thitimajashima	O
near	O
shannon	O
limit	O
error-correcting	O
coding	O
and	O
decoding	B
turbo	B
codes	I
proc	O
ieee	O
intl	O
comm	O
conf	O
berry	O
d	O
and	O
y	O
hochberg	O
bayesian	B
perspectives	O
on	O
multiple	O
comparisons	O
j	O
statist	O
planning	O
and	O
inference	B
bertele	O
u	O
and	O
f	O
brioschi	O
nonserial	O
dynamic	B
programming	I
academic	O
press	O
bertsekas	O
d	O
parallel	O
and	O
distribution	O
computation	O
numerical	O
methods	O
athena	O
scientific	O
bertsekas	O
d	O
nonlinear	O
proathena	O
gramming	O
ed	O
scientific	O
bertsekas	O
d	O
and	O
j	O
tsitsiklis	O
introduction	O
to	O
probability	O
athena	O
scientific	O
edition	O
besag	O
j	O
statistical	O
analysis	O
of	O
non-lattice	O
data	O
the	O
statistician	O
bhattacharya	O
a	O
and	O
d	O
b	O
dunson	O
simplex	O
factor	B
models	O
for	O
multivariate	O
unordered	O
categorical	B
data	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
to	O
appear	O
bickel	O
p	O
and	O
e	O
levina	O
some	O
theory	O
for	O
fisher	O
s	O
linear	O
discriminant	B
function	I
bayes	O
and	O
some	O
alternatives	O
when	O
there	O
are	O
many	O
more	O
variables	O
than	O
observations	O
bernoulli	B
bickson	O
d	O
gaussian	B
belief	B
propagation	I
theory	O
and	O
application	O
ph	O
d	O
thesis	O
hebrew	O
university	O
of	O
jerusalem	O
bilmes	O
j	O
dynamic	O
bayesian	B
multinets	O
in	O
uai	O
bilmes	O
j	O
a	O
graphical	B
models	I
and	O
automatic	B
speech	B
recognition	I
technical	O
report	O
univ	O
washington	O
dept	O
of	O
elec	O
eng	O
binder	O
j	O
d	O
koller	O
s	O
j	O
russell	O
and	O
k	O
kanazawa	O
adaptive	O
probabilistic	O
networks	O
with	O
hidden	B
variables	I
machine	B
learning	B
binder	O
j	O
k	O
murphy	O
and	O
s	O
russell	O
space-efficient	O
inference	B
in	O
dynamic	O
probabilistic	O
networks	O
in	O
intl	O
joint	O
conf	O
on	O
ai	B
birnbaum	O
a	O
on	O
the	O
foundaj	O
of	O
tions	O
of	O
statistical	O
infernece	O
the	O
am	O
stat	O
assoc	O
bishop	O
c	O
bayesian	B
pca	B
nips	O
in	O
bishop	O
c	O
pattern	B
recognition	I
and	O
machine	B
learning	B
springer	O
bishop	O
c	O
pattern	B
recognition	I
and	O
machine	B
learning	B
springer	O
bishop	O
c	O
and	O
g	O
james	O
analysis	O
of	O
multiphase	O
flows	O
using	O
dualenergy	O
densitometry	O
and	O
neural	B
networks	I
nuclear	O
instruments	O
and	O
methods	O
in	O
physics	O
research	O
bishop	O
c	O
and	O
m	O
svens	O
n	O
bayesian	B
hierarchical	O
mixtures	O
of	O
experts	O
in	O
uai	O
bishop	O
c	O
and	O
m	O
tipping	O
variational	O
relevance	O
vector	O
machines	O
in	O
uai	O
bishop	O
c	O
m	O
neural	B
networks	I
for	O
pattern	B
recognition	I
clarendon	O
press	O
bishop	O
y	O
s	O
fienberg	O
and	O
p	O
holland	O
discrete	B
multivariate	O
analysis	O
theory	O
and	O
practice	O
mit	O
press	O
bistarelli	O
s	O
u	O
montanari	O
and	O
semiring-based	O
f	O
rossi	O
constraint	O
satisfaction	O
and	O
optimization	B
j	O
of	O
the	O
acm	O
blake	O
a	O
p	O
kohli	O
and	O
c	O
rother	O
advances	O
in	O
markov	B
random	O
fields	O
for	O
vision	O
and	O
image	O
processing	O
mit	O
press	O
blei	O
d	O
and	O
j	O
lafferty	O
corre	O
lated	O
topic	B
models	O
in	O
nips	O
blei	O
d	O
and	O
j	O
lafferty	O
dynamic	O
topic	B
models	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
pp	O
blei	O
d	O
and	O
j	O
lafferty	O
a	O
correlated	B
topic	B
model	I
of	O
annals	O
of	O
applied	O
stat	O
blei	O
d	O
and	O
j	O
mcauliffe	O
march	O
supervised	O
topic	B
models	O
technical	O
report	O
princeton	O
blei	O
d	O
a	O
ng	O
and	O
m	O
jordan	O
latent	B
dirichlet	B
allocation	I
j	O
of	O
machine	B
learning	B
research	O
blumensath	O
t	O
and	O
m	O
davies	O
on	O
the	O
difference	O
between	O
orthogonal	O
matching	B
pursuit	I
and	O
orthogonal	B
least	B
squares	I
technical	O
report	O
u	O
edinburgh	O
bo	O
l	O
c	O
sminchisescu	O
a	O
kanaujia	O
and	O
d	O
metaxas	O
fast	O
algorithms	O
for	O
large	O
scale	O
conditional	O
prediction	O
in	O
cvpr	O
bohning	O
d	O
multinomial	B
logistic	B
regression	B
algorithm	O
annals	O
of	O
the	O
inst	O
of	O
statistical	O
math	O
bollen	O
k	O
structural	O
equation	O
john	O
models	O
with	O
latent	B
variables	O
wiley	O
sons	O
july	O
bordes	O
a	O
l	O
bottou	O
and	O
p	O
gallinari	O
sgd-qn	O
careful	O
quasi-newton	B
stochastic	B
gradient	B
descent	I
j	O
of	O
machine	B
learning	B
research	O
bhatnagar	O
n	O
c	O
bogdanov	O
and	O
e	O
mossel	O
the	O
computational	O
complexity	O
of	O
estimating	O
convergence	O
time	O
technical	O
report	O
bishop	O
c	O
m	O
mixture	B
density	O
networks	O
technical	O
report	O
ncrg	O
neural	O
computing	O
research	O
group	O
department	O
of	O
computer	O
science	O
aston	O
university	O
bordes	O
a	O
l	O
bottou	O
p	O
gallinari	O
j	O
chang	O
and	O
s	O
a	O
smith	O
erratum	O
sgdqn	O
is	O
less	O
careful	O
than	O
expected	O
j	O
of	O
machine	B
learning	B
research	O
bibliography	O
boser	O
b	O
e	O
i	O
m	O
guyon	O
and	O
v	O
n	O
vapnik	O
a	O
training	O
algorithm	O
for	O
optimal	O
margin	B
classifiers	O
in	O
proc	O
of	O
the	O
workshop	O
on	O
computational	B
learning	B
theory	I
brand	O
m	O
structure	B
learning	B
in	O
conditional	B
probability	I
models	O
via	O
an	O
entropic	O
prior	O
and	O
parameter	B
extinction	O
neural	O
computation	O
brown	O
p	O
m	O
vannucci	O
and	O
t	O
fearn	O
multivariate	O
bayesian	B
variable	O
selection	O
and	O
prediction	O
j	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
bottcher	O
s	O
g	O
and	O
c	O
dethlefsen	O
deal	O
a	O
package	O
for	O
learning	B
bayesian	B
networks	I
j	O
of	O
statistical	O
software	O
braun	O
m	O
and	O
j	O
mcauliffe	O
variational	B
inference	B
for	O
large-scale	O
models	O
of	O
discrete	B
choice	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
bottolo	O
l	O
and	O
s	O
richardson	O
search	O
evolutionary	O
bayesian	B
analysis	O
stochastic	O
bottou	O
l	O
online	O
algorithms	O
and	O
stochastic	O
approximations	O
in	O
d	O
saad	O
online	B
learning	B
and	O
neural	B
networks	I
cambridge	O
bottou	O
l	O
learning	B
with	O
large	O
datasets	O
tutorial	O
bottou	O
l	O
o	O
chapelle	O
d	O
decoste	O
and	O
j	O
weston	O
large	O
scale	O
kernel	B
machines	O
mit	O
press	O
bouchard	O
g	O
efficient	O
bounds	O
for	O
the	O
softmax	B
and	O
applications	O
to	O
approximate	B
inference	B
in	O
hybrid	O
models	O
in	O
nips	O
workshop	O
on	O
approximate	B
inference	B
in	O
hybrid	O
models	O
bouchard-cote	O
a	O
and	O
m	O
jordan	O
optimization	B
of	O
structured	O
mean	B
field	O
objectives	O
in	O
uai	O
bowman	O
a	O
and	O
a	O
azzalini	O
applied	O
smoothing	B
techniques	O
for	O
data	O
analysis	O
oxford	O
box	O
g	O
and	O
n	O
draper	O
empirical	O
model-building	O
and	O
response	O
surfaces	O
wiley	O
box	O
g	O
and	O
g	O
tiao	O
bayesian	B
in	O
statistical	O
analysis	O
inference	B
addison-wesley	O
boyd	O
s	O
and	O
l	O
vandenberghe	O
convex	B
optimization	B
cambridge	O
boyen	O
x	O
and	O
d	O
koller	O
tractable	O
inference	B
for	O
complex	O
stochastic	B
processes	I
in	O
uai	O
boykov	O
y	O
o	O
veksler	O
and	O
r	O
zabih	O
fast	O
approximate	O
energy	O
minimization	O
via	O
graph	B
cuts	I
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
brand	O
m	O
coupled	O
hidden	B
markov	B
models	I
for	O
modeling	O
interacting	O
processes	O
technical	O
report	O
mit	O
lab	O
for	O
perceptual	O
computing	O
breiman	O
l	O
bagging	B
predictors	O
machine	B
learning	B
breiman	O
l	O
arcing	O
classifiers	O
annals	O
of	O
statistics	O
breiman	O
l	O
random	B
forests	I
machine	B
learning	B
breiman	O
l	O
statistical	O
modeling	O
the	O
two	O
cultures	O
statistical	O
science	O
breiman	O
l	O
j	O
friedman	O
and	O
r	O
olshen	O
classification	O
and	O
regression	B
trees	O
wadsworth	O
breslow	O
n	O
e	O
and	O
d	O
g	O
clayton	O
approximate	B
inference	B
in	O
generalized	O
linear	O
mixed	O
models	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
briers	O
m	O
a	O
doucet	O
and	O
s	O
maskel	O
smoothing	B
algorithms	O
for	O
state-space	O
models	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
brochu	O
e	O
m	O
cora	O
and	O
n	O
de	O
freitas	O
november	O
a	O
tutorial	O
on	O
bayesian	B
optimization	B
of	O
expensive	O
cost	O
functions	O
with	O
application	O
to	O
active	O
user	O
modeling	O
and	O
hierarchical	O
learning	B
technical	O
report	O
department	O
of	O
computer	O
science	O
university	O
of	O
british	O
columbia	O
reinforcement	O
brooks	O
s	O
and	O
g	O
roberts	O
assessing	O
convergence	O
of	O
markov	B
chain	I
monte	B
carlo	I
algorithms	O
statistics	O
and	O
computing	O
brown	O
l	O
t	O
cai	O
and	O
a	O
dasgupta	O
interval	O
estimation	O
for	O
a	O
binomial	B
proportion	O
statistical	O
science	O
brown	O
m	O
p	O
r	O
hughey	O
a	O
krogh	O
i	O
s	O
mian	O
k	O
sj	O
lander	O
and	O
d	O
haussler	O
using	O
dirichlet	B
mixtures	O
priors	O
to	O
derive	O
hidden	B
markov	B
models	I
for	O
protein	O
families	O
in	O
intl	O
conf	O
on	O
intelligent	O
systems	O
for	O
molecular	O
biology	O
pp	O
bruckstein	O
a	O
d	O
donoho	O
and	O
m	O
elad	O
from	O
sparse	B
solutions	O
of	O
systems	O
of	O
equations	O
to	O
sparse	B
modeling	O
of	O
signals	O
and	O
images	O
siam	O
review	O
bryson	O
a	O
and	O
y	O
-c	O
ho	O
applied	O
optimal	O
control	O
optimization	B
estimation	O
and	O
control	O
blaisdell	O
publishing	O
company	O
buhlmann	O
p	O
and	O
t	O
hothorn	O
boosting	B
algorithms	O
regularization	B
prediction	O
and	O
model	O
fitting	O
statistical	O
science	O
buhlmann	O
s	O
p	O
de	O
and	O
geer	O
statistics	O
for	O
highdimensional	O
data	O
methodology	O
theory	O
and	O
applications	O
springer	O
van	O
buhlmann	O
p	O
and	O
b	O
yu	O
boosting	B
with	O
the	O
loss	B
regression	B
and	O
classification	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
buhlmann	O
p	O
and	O
b	O
yu	O
sparse	B
j	O
of	O
machine	B
learning	B
boosting	B
research	O
bui	O
h	O
s	O
venkatesh	O
and	O
g	O
west	O
policy	B
recognition	O
in	O
the	O
abstract	O
hidden	B
markov	B
model	I
j	O
of	O
ai	B
research	O
buntine	O
w	O
variational	O
extensions	O
to	O
em	B
and	O
multinomial	B
pca	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
buntine	O
w	O
and	O
a	O
jakulin	O
applying	O
discrete	B
pca	B
in	O
data	O
analysis	O
in	O
uai	O
buntine	O
w	O
and	O
a	O
jakulin	O
discrete	B
component	O
analysis	O
in	O
subspace	O
latent	B
structure	O
and	O
feature	B
selection	I
statistical	O
and	O
optimization	B
perspectives	O
workshop	O
buntine	O
w	O
and	O
a	O
weigend	O
bayesian	B
backpropagation	B
complex	O
systems	O
burges	O
c	O
j	O
t	O
shaked	O
e	O
renshaw	O
a	O
lazier	O
m	O
deeds	O
n	O
hamilton	O
and	O
g	O
hullender	O
learning	B
to	I
rank	I
using	O
gradient	B
descent	I
in	O
intl	O
conf	O
on	O
machine	B
learning	B
pp	O
burkard	O
r	O
m	O
dell	O
amico	O
and	O
assignment	O
s	O
martello	O
problems	O
siam	O
bibliography	O
byran	O
k	O
and	O
t	O
leise	O
the	O
eigenvector	O
the	O
linear	O
algebra	O
behind	O
google	O
siam	O
review	O
calvetti	O
d	O
and	O
e	O
somersalo	O
introduction	O
to	O
bayesian	B
scientific	O
computing	O
springer	O
candes	O
e	O
j	O
romberg	O
and	O
t	O
tao	O
robust	B
uncertainty	B
principles	O
exact	O
signal	O
reconstruction	O
from	O
highly	O
incomplete	O
frequency	O
information	B
ieee	O
trans	O
inform	O
theory	O
candes	O
e	O
and	O
m	O
wakin	O
march	O
an	O
introduction	O
to	O
compressive	O
sampling	O
ieee	O
signal	B
processing	I
magazine	O
candes	O
e	O
m	O
wakin	O
and	O
s	O
boyd	O
enhancing	O
sparsity	B
by	O
reweighted	O
minimization	O
j	O
of	O
fourier	O
analysis	O
and	O
applications	O
cannings	O
c	O
e	O
a	O
thompson	O
and	O
m	O
h	O
skolnick	O
probability	O
functions	O
in	O
complex	O
pedigrees	O
advances	O
in	O
applied	O
probability	O
canny	O
j	O
gap	B
a	O
factor	B
model	O
in	O
proc	O
anintl	O
acm	O
sigir	O
conference	O
for	O
discrete	B
data	O
nual	O
pp	O
cao	O
z	O
t	O
qin	O
t	O
-y	O
liu	O
m	O
-f	O
tsai	O
and	O
h	O
li	O
learning	B
to	I
rank	I
from	O
pairwise	O
approach	O
to	O
listwise	O
approach	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
pp	O
a	O
cappe	O
o	O
online	O
expectation	O
maximisation	O
in	O
k	O
mengersen	O
m	O
titterington	O
and	O
c	O
robert	O
mixtures	O
cappe	O
o	O
and	O
e	O
mouline	O
june	O
online	B
em	B
algorithm	O
for	O
latent	B
data	O
models	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
cappe	O
o	O
e	O
moulines	O
and	O
t	O
ryden	O
inference	B
in	O
hidden	B
markov	B
models	I
springer	O
carbonetto	O
p	O
unsupervised	O
statistical	O
models	O
for	O
general	O
object	O
recognition	O
master	B
s	O
thesis	O
university	O
of	O
british	O
columbia	O
caron	O
f	O
and	O
a	O
doucet	O
sparse	B
bayesian	B
nonparametric	O
regression	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
carreira-perpinan	O
m	O
and	O
c	O
williams	O
an	O
isotropic	B
gaussian	B
mixture	B
can	O
have	O
more	O
modes	O
than	O
components	O
technical	O
report	O
school	O
of	O
informatics	O
u	O
edinburgh	O
carter	O
c	O
and	O
r	O
kohn	O
on	O
gibbs	B
sampling	I
for	O
state	B
space	I
models	O
biometrika	O
carterette	O
b	O
p	O
bennett	O
d	O
chickering	O
and	O
s	O
dumais	O
here	O
or	O
there	O
preference	O
judgments	O
for	O
relevance	O
in	O
proc	O
ecir	O
caruana	O
r	O
a	O
dozen	O
tricks	O
with	O
in	O
g	O
orr	O
and	O
multitask	O
learning	B
k	O
-r	O
mueller	O
neural	B
networks	I
tricks	O
of	O
the	O
trade	O
springerverlag	O
caruana	O
r	O
and	O
a	O
niculescu-mizil	O
an	O
empirical	O
comparison	O
of	O
supervised	B
learning	B
algorithms	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
carvahlo	O
c	O
n	O
polson	O
and	O
j	O
scott	O
the	O
horseshoe	O
estimator	B
for	O
sparse	B
signals	O
biometrika	O
carvahlo	O
l	O
and	O
c	O
lawrence	O
centroid	B
estimation	O
in	O
discrete	B
high-dimensional	O
spaces	O
with	O
applications	O
in	O
biology	O
proc	O
of	O
the	O
national	O
academy	O
of	O
science	O
usa	O
carvalho	O
c	O
m	O
and	O
m	O
west	O
dynamic	O
matrix-variate	O
graphical	B
models	I
bayesian	B
analysis	O
casella	O
g	O
and	O
r	O
berger	O
statistical	O
inference	B
duxbury	O
edition	O
castro	O
m	O
m	O
coates	O
and	O
r	O
d	O
nowak	O
likelihood	B
based	O
hierarchical	B
clustering	B
ieee	O
trans	O
in	O
signal	B
processing	I
celeux	O
g	O
and	O
j	O
diebolt	O
the	O
sem	O
algorithm	O
a	O
probabilistic	O
teacher	O
derive	O
from	O
the	O
em	B
algorithm	O
for	O
the	O
mixture	B
problem	O
computational	O
statistics	O
quarterly	O
carlin	O
b	O
p	O
and	O
t	O
a	O
louis	O
bayes	O
and	O
empirical	B
bayes	I
methods	O
for	O
data	O
analysis	O
chapman	O
and	O
hall	O
cemgil	O
a	O
t	O
a	O
technique	O
for	O
painless	O
derivation	O
of	O
kalman	O
filtering	B
recursions	O
technical	O
report	O
u	O
nijmegen	O
cesa-bianchi	O
n	O
and	O
g	O
lugosi	O
learning	B
and	O
games	O
prediction	O
cambridge	O
university	O
press	O
cevher	O
v	O
learning	B
with	O
com	O
pressible	O
priors	O
in	O
nips	O
chai	O
k	O
m	O
a	O
multi-task	B
learning	B
with	O
gaussian	B
processes	I
ph	O
d	O
thesis	O
u	O
edinburgh	O
chang	O
h	O
y	O
weiss	O
and	O
w	O
freeman	O
informative	O
sensing	O
technical	O
report	O
hebrew	O
u	O
submitted	O
to	O
ieee	O
transactions	O
on	O
info	O
theory	O
chang	O
j	O
and	O
d	O
blei	O
hierarchical	O
relational	O
models	O
for	O
document	O
networks	O
the	O
annals	O
of	O
applied	O
statistics	O
chang	O
j	O
j	O
boyd-graber	O
s	O
gerrish	O
c	O
wang	O
and	O
d	O
blei	O
reading	O
tea	O
leaves	B
how	O
humans	O
interpret	O
topic	B
models	O
in	O
nips	O
chapelle	O
o	O
and	O
l	O
li	O
an	O
empirical	O
evaluation	O
of	O
thompson	B
sampling	I
in	O
nips	O
chartrand	O
r	O
and	O
w	O
yin	O
iteratively	O
reweighted	O
algorithms	O
for	O
compressive	B
sensing	I
in	O
intl	O
conf	O
on	O
acoustics	O
speech	O
and	O
signal	O
proc	O
chechik	O
g	O
a	O
g	O
n	O
tishby	O
and	O
information	B
boty	O
weiss	O
tleneck	O
for	O
gaussian	B
variables	O
j	O
of	O
machine	B
learning	B
research	O
a	O
cheeseman	O
p	O
j	O
kelly	O
m	O
self	O
j	O
stutz	O
w	O
taylor	O
and	O
d	O
freeman	O
autoclass	B
a	O
bayesian	B
classification	O
system	O
in	O
proc	O
of	O
the	O
fifth	O
intl	O
workshop	O
on	O
machine	B
learning	B
cheeseman	O
p	O
and	O
j	O
stutz	O
bayesian	B
classification	O
in	O
fayyad	O
theory	O
and	O
results	O
and	O
pratetsky-shapiro	O
uthurasamy	O
advances	O
in	O
knowledge	B
discovery	I
and	O
data	O
mining	O
mit	O
press	O
smyth	O
chen	O
b	O
k	O
swersky	O
b	O
marlin	O
and	O
n	O
de	O
freitas	O
sparsity	B
priors	O
and	O
boosting	B
for	O
learning	B
localized	O
distributed	O
feature	O
representations	O
technical	O
report	O
ubc	O
chen	O
b	O
j	O
-a	O
ting	O
b	O
marlin	O
and	O
n	O
de	O
freitas	O
deep	B
learning	B
of	O
features	B
from	O
video	O
in	O
nips	O
workshop	O
on	O
deep	B
learning	B
invariant	B
spatio-temporal	O
bibliography	O
chen	O
m	O
d	O
carlson	O
a	O
zaas	O
c	O
woods	O
g	O
ginsburg	O
a	O
hero	O
j	O
lucas	O
and	O
l	O
carin	O
march	O
the	O
bayesian	B
elastic	B
net	I
classifying	O
multi-task	O
geneexpression	O
data	O
ieee	O
trans	O
biomed	O
eng	O
chen	O
r	O
and	O
s	O
liu	O
mixture	B
kalman	O
filters	O
j	O
royal	O
stat	O
soc	O
b	O
chen	O
s	O
and	O
j	O
goodman	O
an	O
empirical	O
study	O
of	O
smoothing	B
techniques	O
for	O
language	B
modeling	I
in	O
proc	O
acl	O
pp	O
chen	O
s	O
and	O
j	O
goodman	O
an	O
empirical	O
study	O
of	O
smoothing	B
techniques	O
for	O
language	B
modeling	I
technical	O
report	O
dept	O
comp	O
sci	O
harvard	O
chen	O
s	O
and	O
j	O
wigger	O
july	O
fast	O
orthogonal	B
least	B
squares	I
algorithm	O
for	O
efficient	O
subset	O
model	B
selection	I
ieee	O
trans	O
signal	B
processing	I
chen	O
s	O
s	O
d	O
l	O
donoho	O
and	O
m	O
a	O
saunders	O
atomic	O
decomposition	O
by	O
basis	O
pursuit	O
siam	O
journal	O
on	O
scientific	O
computing	O
chen	O
x	O
s	O
kim	O
q	O
lin	O
j	O
g	O
carbonell	O
and	O
e	O
p	O
xing	O
graph-structured	O
multi-task	O
regression	B
and	O
an	O
efficient	O
optimization	B
method	O
for	O
general	O
fused	B
lasso	B
technical	O
report	O
cmu	O
chib	O
s	O
marginal	O
from	O
the	O
gibbs	O
output	O
am	O
stat	O
assoc	O
likelihood	B
j	O
of	O
the	O
chickering	O
d	O
learning	B
bayesian	B
networks	I
is	O
np-complete	B
in	O
aistats	O
v	O
chickering	O
d	O
and	O
d	O
heckerman	O
efficient	O
approximations	O
for	O
the	O
marginal	B
likelihood	B
of	O
incomplete	O
data	O
given	O
a	O
bayesian	B
network	O
machine	B
learning	B
chickering	O
d	O
m	O
optimal	O
structure	O
identification	O
with	O
greedy	O
search	O
journal	O
of	O
machine	B
learning	B
research	O
chipman	O
h	O
e	O
george	O
and	O
r	O
mcbayesian	O
cart	B
j	O
of	O
the	O
am	O
stat	O
culloch	O
model	O
search	O
assoc	O
chipman	O
h	O
e	O
george	O
and	O
r	O
mcculloch	O
the	O
practical	O
implementation	O
of	O
bayesian	B
model	B
selection	I
model	B
selection	I
ims	O
lecture	O
notes	O
chipman	O
h	O
e	O
george	O
and	O
r	O
mcculloch	O
bayesian	B
ensemble	B
learning	B
in	O
nips	O
chipman	O
h	O
e	O
george	O
and	O
r	O
mcculloch	O
bart	B
bayesian	B
additive	O
regression	B
trees	O
ann	O
appl	O
stat	O
choi	O
m	O
v	O
tan	B
a	O
anandkumar	O
and	O
a	O
willsky	O
learning	B
latent	B
tree	B
graphical	B
models	I
j	O
of	O
machine	B
learning	B
research	O
j	O
trees	O
and	O
bechoi	O
m	O
exploiting	O
and	O
improving	O
yond	O
tree-structured	O
graphical	B
models	I
ph	O
d	O
thesis	O
mit	O
choset	O
h	O
and	O
k	O
nagatani	O
topological	O
simultaneous	B
localization	I
and	I
mapping	I
toward	O
exact	O
localization	O
without	O
explicit	O
localization	O
ieee	O
trans	O
robotics	O
and	O
automation	O
chow	O
c	O
k	O
and	O
c	O
n	O
liu	O
approximating	O
discrete	B
probability	O
distributions	O
with	O
dependence	O
trees	O
ieee	O
trans	O
on	O
info	O
theory	O
christensen	O
o	O
g	O
roberts	O
and	O
m	O
sk	O
uld	O
robust	B
markov	B
chain	I
monte	B
carlo	I
methods	O
for	O
spatial	O
generalized	O
linear	O
mixed	O
models	O
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
chung	O
f	O
spectral	B
graph	B
the	O
ory	O
ams	O
cimiano	O
p	O
a	O
schultz	O
s	O
sizov	O
p	O
sorg	O
and	O
s	O
staab	O
explicit	O
versus	O
latent	B
concept	B
models	O
for	O
cross-language	B
information	B
retrieval	I
in	O
intl	O
joint	O
conf	O
on	O
ai	B
cipra	O
b	O
the	O
ising	B
model	I
is	O
np-complete	B
siam	O
news	O
ciresan	O
d	O
c	O
u	O
meier	O
l	O
m	O
gambardella	O
and	O
j	O
schmidhuber	O
deep	B
big	O
simple	O
neural	O
nets	O
for	O
handwritten	O
digit	O
recognition	O
neural	O
computation	O
clarke	O
b	O
bayes	B
model	I
averaging	I
and	O
stacking	B
when	O
model	O
approximation	B
error	I
cannot	O
be	O
ignored	O
j	O
of	O
machine	B
learning	B
research	O
cleveland	O
w	O
and	O
s	O
devlin	O
locally-weighted	O
regression	B
an	O
approach	O
to	O
regression	B
analysis	O
by	O
local	O
fitting	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
collins	O
m	O
discriminative	B
training	O
methods	O
for	O
hidden	B
markov	B
models	I
theory	O
and	O
experiments	O
with	O
perceptron	B
algorithms	O
in	O
emnlp	O
collins	O
m	O
s	O
dasgupta	O
and	O
r	O
e	O
schapire	O
a	O
generalization	B
of	O
principal	B
components	I
analysis	I
to	O
the	O
exponential	B
family	B
in	O
collins	O
m	O
and	O
n	O
duffy	O
conlan	O
volution	O
kernels	O
for	O
natural	O
guage	O
in	O
nips	O
collobert	O
r	O
and	O
j	O
weston	O
a	O
unified	O
architecture	O
for	O
natural	O
language	O
processing	O
deep	B
neural	B
networks	I
with	O
multitask	O
learning	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
combettes	O
p	O
and	O
v	O
wajs	O
signal	O
recovery	O
by	O
proximal	O
forwardbackward	O
splitting	O
siam	O
j	O
multiscale	O
model	O
simul	O
cook	O
j	O
exact	O
calculation	O
of	O
beta	O
inequalities	O
technical	O
report	O
m	O
d	O
anderson	O
cancer	O
center	O
dept	O
biostatistics	O
cooper	O
g	O
and	O
e	O
herskovits	O
a	O
bayesian	B
method	O
for	O
the	O
induction	B
of	O
probabilistic	O
networks	O
from	O
data	O
machine	B
learning	B
cooper	O
g	O
and	O
c	O
yoo	O
causal	O
discovery	O
from	O
a	O
mixture	B
of	O
experimental	O
and	O
observational	O
data	O
in	O
uai	O
cover	O
t	O
and	O
p	O
hart	O
nearest	B
neighbor	I
pattern	B
classification	O
ieee	O
trans	O
inform	O
theory	O
cover	O
t	O
m	O
and	O
j	O
a	O
thomas	O
elements	O
of	O
information	B
theory	I
john	O
wiley	O
cover	O
t	O
m	O
and	O
j	O
a	O
thomas	O
information	B
theory	I
john	O
wiley	O
edition	O
elements	O
of	O
clarke	O
b	O
e	O
fokoue	O
and	O
h	O
h	O
zhang	O
principles	O
and	O
theory	O
for	O
data	O
mining	O
and	O
machine	B
learning	B
springer	O
cowles	O
m	O
and	O
b	O
carlin	O
markov	B
chain	I
monte	B
carlo	I
convergence	O
diagnostics	O
a	O
comparative	O
review	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
bibliography	O
crisan	O
d	O
p	O
d	O
moral	O
and	O
t	O
lyons	O
discrete	B
filtering	B
using	O
branching	O
and	O
interacting	O
particle	O
systems	O
markov	B
processes	O
and	O
related	O
fields	O
dawid	O
a	O
p	O
and	O
s	O
l	O
lauritzen	O
hyper-markov	O
laws	O
in	O
the	O
statistical	O
analysis	O
of	O
decomposable	B
graphical	B
models	I
the	O
annals	O
of	O
statistics	O
dempster	O
a	O
p	O
n	O
m	O
laird	O
and	O
d	O
b	O
rubin	O
maximum	O
likelihood	B
from	O
incomplete	O
data	O
via	O
the	O
em	B
algorithm	O
j	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
cui	O
y	O
x	O
z	O
fern	O
and	O
j	O
g	O
dy	O
learning	B
multiple	O
nonredundant	O
clusterings	O
acm	O
transactions	O
on	O
knowledge	B
discovery	I
from	O
data	O
de	O
freitas	O
n	O
r	O
dearden	O
f	O
hutter	O
r	O
morales-menendez	O
j	O
mutch	O
and	O
d	O
poole	O
diagnosis	O
by	O
a	O
waiter	O
and	O
a	O
mars	B
explorer	O
proc	O
ieee	O
cukier	O
k	O
february	O
data	O
data	O
everywhere	O
dagum	O
p	O
and	O
m	O
luby	O
approximating	O
probabilistic	B
inference	B
in	O
bayesian	B
belief	B
networks	I
is	O
nphard	O
artificial	O
intelligence	O
dahl	O
j	O
l	O
vandenberghe	O
and	O
v	O
roychowdhury	O
august	O
covariance	B
selection	I
for	O
non-chordal	O
graphs	O
via	O
chordal	B
embedding	B
optimization	B
methods	O
and	O
software	O
dahlhaus	O
r	O
and	O
m	O
eichler	O
causality	B
and	O
graphical	B
models	I
for	O
time	O
series	O
in	O
p	O
green	O
n	O
hjort	O
and	O
s	O
richardson	O
highly	O
structured	O
stochastic	O
systems	O
oxford	O
university	O
press	O
dallal	O
s	O
and	O
w	O
hall	O
approximating	O
priors	O
by	O
mixtures	O
of	O
natural	O
conjugate	B
priors	I
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
darwiche	O
a	O
modeling	O
and	O
reasoning	O
with	O
bayesian	B
networks	I
cambridge	O
de	O
freitas	O
n	O
m	O
niranjan	O
and	O
a	O
gee	B
hierarchical	B
bayesian	B
models	I
for	O
regularisation	O
in	O
sequential	B
learning	B
neural	O
computation	O
dechter	O
r	O
bucket	B
elimination	I
a	O
unifying	O
framework	O
for	O
probabilistic	B
inference	B
in	O
uai	O
dechter	O
r	O
constraint	O
process	O
ing	O
morgan	O
kaufmann	O
decoste	O
d	O
and	O
b	O
schoelkopf	O
training	O
invariant	B
support	B
vector	I
machines	I
machine	O
learnng	O
deerwester	O
s	O
s	O
dumais	O
g	O
furnas	O
t	O
landauer	O
and	O
r	O
harshman	O
indexing	O
by	O
latent	B
semantic	I
analysis	I
j	O
of	O
the	O
american	O
society	O
for	O
information	B
science	O
degroot	O
m	O
optimal	O
statistical	O
decisions	O
mcgraw-hill	O
deisenroth	O
m	O
c	O
rasmussen	O
and	O
j	O
peters	O
gaussian	B
process	I
dynamic	B
programming	I
neurocomputing	O
daume	O
h	O
fast	O
search	O
for	O
dirichlet	B
process	I
mixture	B
models	I
in	O
aistatistics	O
daume	O
h	O
frustratingly	O
easy	O
domain	B
adaptation	I
in	O
proc	O
the	O
assoc	O
for	O
comp	O
ling	O
dawid	O
a	O
p	O
applications	O
of	O
a	O
general	O
propagation	O
algorithm	O
for	O
probabilistic	O
expert	O
systems	O
statistics	O
and	O
computing	O
dawid	O
a	O
p	O
influence	O
diagrams	O
for	O
causal	O
modelling	O
and	O
inference	B
intl	O
stat	O
review	O
corrections	O
dellaportas	O
p	O
p	O
giudici	O
and	O
g	O
roberts	O
bayesian	B
inference	B
for	O
nondecomposable	O
graphical	O
gaussian	B
models	O
sankhya	O
ser	O
a	O
dellaportas	O
p	O
and	O
a	O
f	O
m	O
smith	O
bayesian	B
inference	B
for	O
generalized	O
linear	O
and	O
proportional	O
hazards	O
models	O
via	O
gibbs	B
sampling	I
the	O
royal	O
statistical	O
society	O
series	O
c	O
statistics	O
j	O
of	O
delyon	O
lavielle	O
b	O
m	O
and	O
e	O
moulines	O
convergence	O
of	O
a	O
stochastic	B
approximation	I
version	O
of	O
the	O
em	B
algorithm	O
annals	O
of	O
statistics	O
dawid	O
a	O
p	O
beware	O
of	O
the	O
dag	B
j	O
of	O
machine	B
learning	B
research	O
dempster	O
a	O
covariance	B
selec	O
tion	O
biometrics	O
denison	O
d	O
c	O
holmes	O
b	O
mallick	O
and	O
a	O
smith	O
bayesian	B
methods	O
for	O
nonlinear	O
classification	O
and	O
regression	B
wiley	O
denison	O
d	O
b	O
mallick	O
and	O
a	O
smith	O
a	O
bayesian	B
cart	B
algorithm	O
biometrika	O
desjardins	O
g	O
and	O
y	O
bengio	O
empirical	O
evaluation	O
of	O
convolutional	O
rbms	O
for	O
vision	O
technical	O
report	O
u	O
montreal	O
dey	O
d	O
s	O
ghosh	O
and	O
b	O
mallick	O
generalized	B
linear	I
models	I
a	O
bayesian	B
perspective	O
chapman	O
hallcrc	O
biostatistics	O
series	O
diaconis	O
p	O
s	O
holmes	O
and	O
r	O
montgomery	O
dynamical	O
bias	B
in	O
the	O
coin	O
toss	O
siam	O
review	O
diaconis	O
p	O
and	O
d	O
ylvisaker	O
in	O
quantifying	O
prior	O
opinion	O
bayesian	B
statistics	I
dietterich	O
t	O
g	O
and	O
g	O
bakiri	O
solving	O
multiclass	O
learning	B
problems	O
via	O
ecocs	O
j	O
of	O
ai	B
research	O
diggle	O
p	O
and	O
p	O
ribeiro	O
model	O
based	O
geostatistics	O
springer	O
ding	O
y	O
and	O
r	O
harrison	O
a	O
sparse	B
multinomial	B
probit	B
model	O
for	O
classification	O
pattern	B
analysis	O
and	O
applications	O
dobra	O
a	O
dependency	B
networks	I
for	O
genome-wide	O
data	O
technical	O
report	O
u	O
washington	O
dobra	O
a	O
and	O
h	O
massam	O
the	O
mode	B
oriented	O
stochastic	B
search	I
algorithm	O
for	O
log-linear	B
models	O
with	O
conjugate	B
priors	I
statistical	O
methodology	O
domingos	O
p	O
and	O
d	O
lowd	O
markov	B
logic	O
an	O
interface	O
layer	O
for	O
ai	B
morgan	O
claypool	O
domingos	O
p	O
and	O
m	O
pazzani	O
on	O
the	O
optimality	O
of	O
the	O
simple	O
bayesian	B
classifier	O
under	O
zero-one	O
loss	B
machine	B
learning	B
bibliography	O
domke	O
j	O
a	O
karapurkar	O
and	O
y	O
aloimonos	O
who	O
killed	O
the	O
directed	B
model	O
in	O
cvpr	O
duda	O
r	O
o	O
p	O
e	O
hart	O
and	O
d	O
g	O
stork	O
pattern	B
classification	O
wiley	O
interscience	O
edition	O
doucet	O
a	O
n	O
de	O
freitas	O
and	O
n	O
j	O
gordon	O
sequential	B
monte	B
carlo	I
methods	O
in	O
practice	O
springer	O
verlag	O
doucet	O
a	O
n	O
gordon	O
and	O
v	O
krishnamurthy	O
particle	O
filters	O
for	O
state	B
estimation	I
of	O
jump	O
markov	B
linear	O
systems	O
ieee	O
trans	O
on	O
signal	B
processing	I
dow	O
j	O
and	O
j	O
endersby	O
multinomial	B
probit	B
and	O
multinomial	B
logit	B
a	O
comparison	O
of	O
choice	O
models	O
for	O
voting	O
research	O
electoral	O
studies	O
drineas	O
p	O
a	O
frieze	O
r	O
kannan	O
s	O
vempala	O
and	O
v	O
vinay	O
clustering	B
large	O
graphs	O
via	O
the	O
singular	B
value	I
decomposition	I
machine	B
learning	B
drugowitsch	O
j	O
bayesian	B
linear	B
regression	B
technical	O
report	O
u	O
rochester	O
druilhet	O
p	O
and	O
j	O
-m	O
marin	O
invariant	B
hpd	B
credible	O
sets	O
and	O
map	O
estimators	O
bayesian	B
analysis	O
duane	O
s	O
a	O
kennedy	O
b	O
pendleton	O
and	O
d	O
roweth	O
hybrid	B
monte	B
carlo	I
physics	O
letters	O
b	O
duchi	O
j	O
s	O
gould	O
and	O
d	O
koller	O
projected	O
subgradient	B
methods	O
for	O
learning	B
sparse	B
gaussians	O
in	O
uai	O
duchi	O
for	O
online	B
learning	B
j	O
e	O
hazan	O
and	O
y	O
singer	O
adaptive	O
subgradient	B
methand	O
ods	O
stochastic	B
optimization	B
in	O
proc	O
of	O
the	O
workshop	O
on	O
computational	B
learning	B
theory	I
duchi	O
j	O
s	O
shalev-shwartz	O
y	O
singer	O
and	O
t	O
chandra	O
efficient	O
for	O
projections	O
onto	O
the	O
learning	B
in	O
high	O
dimensions	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
duchi	O
j	O
and	O
y	O
singer	O
boosting	B
with	O
structural	O
sparsity	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
duchi	O
j	O
d	O
tarlow	O
g	O
elidan	O
and	O
d	O
koller	O
using	O
combinatorial	O
optimization	B
within	O
maxproduct	O
belief	B
propagation	I
in	O
nips	O
dumais	O
s	O
and	O
t	O
landauer	O
a	O
solution	O
to	O
plato	O
s	O
problem	O
the	O
latent	B
semantic	I
analysis	I
theory	O
of	O
acquisition	O
induction	B
and	O
representation	O
of	O
knowledge	O
psychological	O
review	O
elad	O
m	O
and	O
i	O
yavnch	O
a	O
plurality	O
of	O
sparse	B
representations	O
is	O
better	O
than	O
the	O
sparsest	O
one	O
alone	O
ieee	O
trans	O
on	O
info	O
theory	O
elidan	O
g	O
and	O
s	O
gould	O
learning	B
bounded	O
treewidth	B
bayesian	B
networks	I
j	O
of	O
machine	B
learning	B
research	O
dunson	O
d	O
j	O
palomo	O
and	O
k	O
bollen	O
bayesian	B
structural	O
equation	O
modeling	O
technical	O
report	O
samsi	O
elidan	O
g	O
n	O
lotner	O
n	O
friedman	O
and	O
d	O
koller	O
discovering	O
hidden	B
variables	I
a	O
structure-based	O
approach	O
in	O
nips	O
durbin	O
j	O
and	O
s	O
j	O
koopman	O
time	O
series	O
analysis	O
by	O
state	B
space	I
methods	O
oxford	O
university	O
press	O
durbin	O
r	O
s	O
eddy	O
a	O
krogh	O
and	O
g	O
mitchison	O
biological	O
sequence	O
analysis	O
probabilistic	O
models	O
of	O
proteins	O
and	O
nucleic	O
acids	O
cambridge	O
cambridge	O
university	O
press	O
earl	O
d	O
and	O
m	O
deem	O
parallel	B
tempering	I
theory	O
applications	O
and	O
new	O
perspectives	O
phys	O
chem	O
chem	O
phys	O
eaton	O
d	O
and	O
k	O
murphy	O
exact	O
bayesian	B
structure	B
learning	B
from	O
uncertain	O
interventions	B
in	O
aistatistics	O
elidan	O
g	O
i	O
mcgraw	O
and	O
d	O
koller	O
residual	B
belief	O
gation	O
informed	O
scheduling	O
for	O
asynchronous	O
message	B
passing	I
in	O
uai	O
elkan	O
c	O
using	O
the	O
triangle	O
inin	O
equality	O
to	O
accelerate	O
k-means	O
intl	O
conf	O
on	O
machine	B
learning	B
elkan	O
c	O
deriving	O
tf-idf	B
as	O
a	O
fisher	B
kernel	B
in	O
proc	O
intl	O
symp	O
on	O
string	O
processing	O
and	O
information	B
retrieval	I
pp	O
elkan	O
c	O
clustering	B
documents	O
with	O
an	O
exponential	O
fmaily	O
the	O
dirichlet	B
approximation	O
of	O
compoind	O
multinomial	B
model	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
edakunni	O
n	O
s	O
schaal	O
and	O
s	O
vijayakumar	O
probabilistic	O
incremental	O
locally	O
weighted	O
learning	B
using	O
randomly	O
varying	O
coefficient	O
model	O
technical	O
report	O
usc	O
ellis	O
b	O
and	O
w	O
h	O
wong	O
learning	B
causal	O
bayesian	B
network	O
structures	O
from	O
experimental	O
data	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
edwards	O
d	O
g	O
de	O
abreu	O
and	O
r	O
labouriau	O
selecting	O
highdimensional	O
mixed	O
graphical	B
models	I
using	O
minimal	B
aic	B
or	O
bic	B
forests	O
bmc	O
bioinformatics	O
efron	O
b	O
why	O
isn	O
t	O
everyone	O
a	O
bayesian	B
the	O
american	O
statistician	O
efron	O
b	O
large-scale	O
inference	B
empirical	B
bayes	I
methods	O
for	O
estimation	O
testing	O
and	O
prediction	O
cambridge	O
efron	O
b	O
i	O
johnstone	O
t	O
hastie	O
and	O
r	O
tibshirani	O
least	O
angle	O
regression	B
annals	O
of	O
statistics	O
efron	O
b	O
and	O
c	O
morris	O
data	O
analysis	O
using	O
stein	O
s	O
estimator	B
and	O
its	O
generalizations	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
engel	O
y	O
s	O
mannor	O
and	O
r	O
meir	O
reinforcement	B
learning	B
in	O
intl	O
with	O
gaussian	B
processes	I
conf	O
on	O
machine	B
learning	B
erhan	O
d	O
y	O
bengio	O
a	O
courville	O
p	O
-a	O
manzagol	O
p	O
vincent	O
and	O
s	O
bengio	O
why	O
does	O
unsupervised	O
pre-training	O
help	O
deep	B
learning	B
j	O
of	O
machine	B
learning	B
research	O
erosheva	O
s	O
e	O
fienberg	O
joutard	O
and	O
c	O
describing	O
disability	O
through	O
individual-level	O
mixture	B
models	O
for	O
multivariate	O
binary	O
data	O
annals	O
of	O
applied	O
statistics	O
erosheva	O
e	O
s	O
fienberg	O
and	O
j	O
lafferty	O
mixed-membership	O
models	O
of	O
scientific	O
publications	O
proc	O
of	O
the	O
national	O
academy	O
of	O
science	O
usa	O
bibliography	O
escobar	O
m	O
d	O
and	O
m	O
west	O
bayesian	B
density	B
estimation	I
and	O
inference	B
using	O
mixtures	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
ewens	O
w	O
population	O
genetics	O
theory	O
the	O
past	O
and	O
the	O
future	O
in	O
s	O
lessard	O
mathemetical	O
and	O
statistica	O
developments	O
of	O
evolutionary	O
theory	O
pp	O
reidel	O
fan	O
j	O
and	O
r	O
z	O
li	O
variable	O
selection	O
via	O
non-concave	O
penalized	O
likelihood	B
and	O
its	O
oracle	O
properties	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
fearnhead	O
p	O
exact	O
bayesian	B
curve	O
fitting	O
and	O
signal	O
segmentation	O
ieee	O
trans	O
signal	B
processing	I
felzenszwalb	O
p	O
and	O
d	O
huttenlocher	O
efficient	O
belief	B
propagation	I
for	O
early	O
vision	O
intl	O
j	O
computer	O
vision	O
ferrucci	O
d	O
e	O
brown	O
j	O
chu-carroll	O
j	O
fan	O
d	O
gondek	O
a	O
kalyanpur	O
a	O
lally	O
j	O
w	O
murdock	O
e	O
n	O
amd	O
j	O
prager	O
n	O
schlaefter	O
and	O
c	O
welty	O
building	O
watson	B
an	O
overview	O
of	O
the	O
deepqa	O
project	O
ai	B
magazine	O
fienberg	O
s	O
an	O
iterative	O
procedure	O
for	O
estimation	O
in	O
contingency	O
tables	O
annals	O
of	O
mathematical	O
statistics	O
a	O
figueiredo	O
m	O
adaptive	O
sparseness	O
for	O
supervised	B
learning	B
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
figueiredo	O
m	O
r	O
nowak	O
and	O
s	O
wright	O
gradient	O
projection	B
for	O
sparse	B
reconstruction	O
application	O
to	O
compressed	B
sensing	I
and	O
other	O
inverse	B
problems	I
ieee	O
j	O
on	O
selected	O
topics	O
in	O
signal	B
processing	I
figueiredo	O
m	O
a	O
t	O
and	O
a	O
k	O
jain	O
unsupervised	B
learning	B
of	O
finite	O
mixture	B
models	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
matlab	O
code	O
at	O
httpwww	O
lx	O
it	O
pt	O
mtfmixturecode	O
zip	O
fine	O
s	O
y	O
singer	O
and	O
n	O
tishby	O
the	O
hierarchical	O
hidden	B
markov	B
model	I
analysis	O
and	O
applications	O
machine	B
learning	B
finkel	O
j	O
and	O
c	O
manning	O
hierarchical	O
bayesian	B
domain	B
adaptation	I
in	O
proc	O
naacl	O
pp	O
fischer	O
b	O
and	O
j	O
schumann	O
autobayes	O
a	O
system	O
for	O
generating	O
data	O
analysis	O
programs	O
from	O
statistical	O
models	O
j	O
functional	O
programming	O
fishelson	O
m	O
and	O
d	O
geiger	O
exact	O
genetic	O
linkage	O
computations	O
for	O
general	O
pedigrees	O
bmc	O
bioinformatics	O
fletcher	O
r	O
on	O
the	O
barzilaiapplied	O
opti	O
borwein	O
method	O
mization	O
fokoue	O
e	O
mixtures	O
of	O
factor	B
analyzers	O
an	O
extension	B
with	O
covariates	B
j	O
multivariate	O
analysis	O
forbes	O
j	O
t	O
huang	O
k	O
kanazawa	O
and	O
s	O
russell	O
the	O
batmobile	O
towards	O
a	O
bayesian	B
automated	O
taxi	O
in	O
intl	O
joint	O
conf	O
on	O
ai	B
forsyth	O
d	O
and	O
j	O
ponce	O
computer	O
vision	O
a	O
modern	O
approach	O
prentice	O
hall	O
fraley	O
c	O
and	O
a	O
raftery	O
model-based	B
clustering	B
discriminant	B
analysis	I
and	O
density	B
estimation	I
j	O
of	O
the	O
am	O
stat	O
assoc	O
fraley	O
c	O
and	O
a	O
raftery	O
bayesian	B
regularization	B
for	O
normal	B
mixture	B
estimation	O
and	O
modelbased	O
clustering	B
j	O
of	O
classification	O
franc	O
v	O
a	O
zien	O
and	O
b	O
schoelkopf	O
support	B
vector	I
machines	I
as	O
probabilistic	O
models	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
frank	O
i	O
and	O
j	O
friedman	O
a	O
statistical	O
view	O
of	O
some	O
chemometrics	O
regression	B
tools	O
technometrics	O
fraser	O
a	O
hidden	B
markov	B
models	I
and	O
dynamical	O
systems	O
siam	O
press	O
freund	O
y	O
and	O
r	O
r	O
schapire	O
experiments	O
with	O
a	O
new	O
boosting	B
algorithm	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
frey	O
b	O
graphical	B
models	I
for	O
machine	B
learning	B
and	O
digital	O
communication	O
mit	O
press	O
frey	O
b	O
extending	O
factor	B
graphs	O
so	O
as	O
to	O
unify	O
directed	B
and	O
undirected	B
graphical	B
models	I
in	O
uai	O
frey	O
b	O
and	O
d	O
dueck	O
february	O
clustering	B
by	O
passing	O
messages	O
between	O
data	O
points	O
science	O
a	O
friedman	O
j	O
multivariate	B
adaptive	I
regression	B
splines	I
ann	O
statist	O
friedman	O
j	O
on	O
bias	B
variance	B
loss	B
and	O
the	O
curse	B
of	I
dimensionality	I
j	O
data	O
mining	O
and	O
knowledge	B
discovery	I
friedman	O
j	O
greedy	O
function	B
approximation	I
a	O
gradient	B
boosting	B
machine	O
annals	O
of	O
statistics	O
friedman	O
j	O
t	O
hastie	O
and	O
r	O
tibshirani	O
additive	O
logistic	B
regression	B
a	O
statistical	O
view	O
of	O
boosting	B
annals	O
of	O
statistics	O
friedman	O
j	O
t	O
hastie	O
and	O
r	O
tibshirani	O
sparse	B
inverse	O
covariance	B
estimation	O
the	O
graphical	B
lasso	B
biostatistics	O
friedman	O
j	O
t	O
hastie	O
and	O
r	O
tibshirani	O
februrary	O
regularization	B
paths	O
for	O
generalized	B
linear	I
models	I
via	O
coordinate	O
descent	O
j	O
of	O
statistical	O
software	O
friedman	O
n	O
learning	B
bayesian	B
networks	I
in	O
the	O
presence	O
of	O
missing	B
values	O
and	O
hidden	B
variables	I
in	O
uai	O
friedman	O
n	O
d	O
geiger	O
and	O
m	O
goldszmidt	O
bayesian	B
network	O
classifiers	O
machine	B
learning	B
j	O
friedman	O
n	O
d	O
geiger	O
and	O
n	O
lotner	O
likelihood	B
computation	O
with	O
value	O
abstraction	O
in	O
uai	O
friedman	O
n	O
and	O
d	O
koller	O
being	O
bayesian	B
about	O
network	O
structure	O
a	O
bayesian	B
approach	O
to	O
structure	O
discovery	O
in	O
bayesian	B
networks	I
machine	B
learning	B
friedman	O
n	O
m	O
ninion	O
i	O
pe	O
er	O
and	O
t	O
pupko	O
a	O
structural	B
em	B
algorithm	O
for	O
phylogenetic	O
inference	B
j	O
comp	O
bio	B
friedman	O
n	O
and	O
y	O
singer	O
efficient	O
bayesian	B
parameter	B
estimation	O
in	O
large	O
discrete	B
domains	O
in	O
bibliography	O
fruhwirth-schnatter	O
s	O
finite	O
mixture	B
and	O
markov	B
switching	I
models	I
springer	O
fruhwirth-schnatter	O
s	O
and	O
r	O
fruhwirth	O
data	B
augmentation	I
and	O
mcmc	B
for	O
binary	O
and	O
multinomial	B
logit	B
models	O
in	O
t	O
kneib	O
and	O
g	O
tutz	O
statistical	O
modelling	O
and	O
regression	B
structures	O
pp	O
springer	O
fu	O
w	O
penalized	O
regressions	O
the	O
bridge	O
verus	O
the	O
lasso	B
j	O
computational	O
and	O
graphical	O
statistics	O
gelman	O
a	O
j	O
carlin	O
h	O
stern	O
and	O
d	O
rubin	O
bayesian	B
data	O
analysis	O
chapman	O
and	O
hall	O
edition	O
gelman	O
a	O
and	O
j	O
hill	O
data	O
analysis	O
using	O
regression	B
and	O
multilevel	O
hierarchical	O
models	O
cambridge	O
gelman	O
a	O
and	O
x	O
-l	O
meng	O
simulating	O
normalizing	O
constants	O
from	O
importance	O
to	O
bridge	O
sampling	O
to	O
path	B
sampling	O
statisical	O
science	O
sampling	O
fukushima	O
k	O
cognitron	O
a	O
self-organizing	O
multilayered	O
neural	B
network	I
biological	O
cybernetics	O
gelman	O
a	O
and	O
t	O
raghunathan	O
using	O
conditional	O
distributions	O
for	O
missing-data	O
imputation	B
statistical	O
science	O
fung	O
r	O
and	O
k	O
chang	O
weighting	O
and	O
integrating	O
evidence	B
for	O
stochastic	O
simulation	O
in	O
bayesian	B
networks	I
in	O
uai	O
gelman	O
a	O
and	O
d	O
rubin	O
inference	B
from	O
iterative	O
simulation	O
using	O
multiple	O
sequences	O
statistical	O
science	O
efficient	O
gabow	O
h	O
z	O
galil	O
and	O
t	O
spencer	O
implementation	O
of	O
graph	B
algorithms	O
using	O
contraction	O
in	O
ieee	O
symposium	O
on	O
the	O
foundations	O
of	O
computer	O
science	O
gales	O
m	O
maximum	O
likelihood	B
multiple	O
subspace	O
projections	O
for	O
hidden	B
markov	B
models	I
ieee	O
trans	O
on	O
speech	O
and	O
audio	O
processing	O
gales	O
m	O
j	O
f	O
semi-tied	O
covariance	B
matrices	O
for	O
hidden	B
markov	B
models	I
ieee	O
trans	O
on	O
speech	O
and	O
audio	O
processing	O
gamerman	O
d	O
efficient	O
sampling	O
from	O
the	O
posterior	O
distribution	O
in	O
generalized	O
linear	O
mixed	O
models	O
statistics	O
and	O
computing	O
geiger	O
d	O
and	O
d	O
heckerman	O
in	O
learning	B
gaussian	B
networks	O
uai	O
volume	O
pp	O
geiger	O
d	O
and	O
d	O
heckerman	O
a	O
characterization	O
of	O
dirchlet	O
distributions	O
through	O
local	O
and	O
global	O
independence	O
annals	O
of	O
statistics	O
gelfand	O
a	O
model	O
determination	O
using	O
sampling-based	O
methods	O
in	O
gilks	O
richardson	O
and	O
spiegelhalter	O
markov	B
chain	I
monte	B
carlo	I
in	O
practice	O
chapman	O
hall	O
geman	O
s	O
e	O
bienenstock	O
and	O
r	O
doursat	O
neural	B
networks	I
and	O
the	O
bias-variance	O
dilemma	O
neural	O
computing	O
geman	O
s	O
and	O
d	O
geman	O
stochastic	O
relaxation	O
gibbs	O
distributions	O
and	O
the	O
bayesian	B
restoration	O
of	O
images	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
geoffrion	O
a	O
lagrangian	B
relaxation	I
for	O
integer	O
programming	O
mathematical	O
programming	O
study	O
george	O
e	O
and	O
d	O
foster	O
calibration	B
and	O
empirical	B
bayes	I
variable	O
selection	O
biometrika	O
getoor	O
l	O
and	O
b	O
taskar	O
introduction	O
to	O
relational	O
statistical	O
learning	B
mit	O
press	O
geyer	O
c	O
practical	O
markov	B
chain	I
monte	B
carlo	I
statistical	O
science	O
ghahramani	O
z	O
and	O
m	O
beal	O
inference	B
for	O
bayesian	B
in	O
variational	O
mixtures	O
of	O
factor	B
analysers	O
ghahramani	O
z	O
and	O
m	O
beal	O
propagation	O
algorithms	O
for	O
variational	O
bayesian	B
learning	B
in	O
gelfand	O
a	O
and	O
a	O
smith	O
sampling-based	O
approaches	O
to	O
calculating	O
marginal	O
densities	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
ghahramani	O
z	O
and	O
g	O
hinton	O
the	O
em	B
algorithm	O
for	O
mixtures	O
of	O
factor	B
analyzers	O
technical	O
report	O
dept	O
of	O
comp	O
sci	O
uni	O
toronto	O
ghahramani	O
z	O
and	O
g	O
hinton	O
parameter	B
estimation	O
for	O
linear	O
dynamical	O
systems	O
technical	O
report	O
dept	O
comp	O
sci	O
univ	O
toronto	O
ghahramani	O
z	O
and	O
m	O
jordan	O
factorial	O
hidden	B
markov	B
models	I
machine	B
learning	B
a	O
moving	O
gilks	O
w	O
and	O
c	O
berzuini	O
following	O
target	O
monte	B
carlo	I
infernece	O
for	O
dynamic	O
bayesian	B
models	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
gilks	O
w	O
n	O
best	O
and	O
k	O
tan	B
adaptive	B
rejection	I
metropolis	I
sampling	I
applied	O
statistics	O
gilks	O
w	O
and	O
p	O
wild	O
adaptive	B
rejection	B
sampling	I
for	O
gibbs	B
sampling	I
applied	O
statistics	O
girolami	O
m	O
b	O
calderhead	O
and	O
s	O
chin	O
riemannian	O
manifold	O
hamiltonian	O
monte	B
carlo	I
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
to	O
appear	O
girolami	O
m	O
and	O
s	O
rogers	O
hierarchic	O
bayesian	B
models	O
for	O
kernel	B
learning	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
pp	O
girolami	O
m	O
and	O
s	O
rogers	O
variational	O
bayesian	B
multinomial	B
probit	B
regression	B
with	O
gaussian	B
process	I
priors	O
neural	O
comptuation	O
girshick	O
r	O
p	O
felzenszwalb	O
and	O
d	O
mcallester	O
object	B
detection	I
with	O
grammar	O
models	O
in	O
nips	O
gittins	O
j	O
multi-armed	B
bandit	I
allocation	O
indices	O
wiley	O
giudici	O
p	O
and	O
p	O
green	O
gausdetermination	O
decomposable	B
sian	O
model	O
biometrika	O
graphical	O
givoni	O
i	O
e	O
and	O
b	O
j	O
frey	O
june	O
a	O
binary	O
variable	O
model	O
for	O
affinity	B
propagation	I
neural	O
computation	O
globerson	O
a	O
and	O
t	O
jaakkola	O
fixing	O
max-product	B
convergent	O
message	B
passing	I
algorithms	O
for	O
map	O
lp-relaxations	O
in	O
nips	O
glorot	O
x	O
and	O
y	O
bengio	O
may	O
understanding	O
the	O
difficulty	O
of	O
training	O
deep	B
feedforward	O
neural	B
networks	I
in	O
aistatistics	O
volume	O
pp	O
bibliography	O
gogate	O
v	O
w	O
a	O
webb	O
and	O
p	O
dominlearning	O
efficient	O
gos	O
markov	B
networks	O
in	O
nips	O
goldenberg	O
a	O
a	O
x	O
zheng	O
s	O
e	O
fienberg	O
and	O
e	O
m	O
airoldi	O
a	O
survey	O
of	O
statistical	O
network	O
models	O
foundations	O
and	O
trends	O
in	O
machine	B
learning	B
golub	O
g	O
and	O
c	O
f	O
van	O
loan	O
johns	O
hop	O
matrix	O
computations	O
kins	O
university	O
press	O
gonen	O
m	O
w	O
johnson	O
y	O
lu	O
and	O
p	O
westfall	O
august	O
the	O
bayesian	B
two-sample	O
t	O
test	O
the	O
american	O
statistician	O
gonzales	O
t	O
clustering	B
to	O
minimize	O
the	O
maximum	O
intercluster	O
distance	O
theor	O
comp	O
sci	O
gorder	O
p	O
f	O
novdec	O
neural	B
networks	I
show	O
new	O
promise	O
for	O
machine	O
vision	O
computing	O
in	O
science	O
engineering	O
gordon	O
n	O
approach	O
to	O
nonlinearnon-gaussian	O
bayesian	B
state	B
estimation	I
iee	O
proceedings	O
novel	O
graepel	O
t	O
j	O
quinonero-candela	O
t	O
borchert	O
and	O
r	O
herbrich	O
web-scale	O
bayesian	B
clickthrough	O
rate	B
prediction	O
for	O
sponsored	O
search	O
advertising	O
in	O
microsoft	B
a	O
zs	O
bing	B
search	O
engine	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
grauman	O
k	O
and	O
t	O
darrell	O
april	O
the	O
pyramid	B
match	I
kernel	B
efficient	O
learning	B
with	O
sets	O
of	O
features	B
j	O
of	O
machine	B
learning	B
research	O
green	O
p	O
reversible	O
jump	O
markov	B
chain	I
monte	B
carlo	I
computation	O
and	O
bayesian	B
model	O
determination	O
biometrika	O
green	O
p	O
tutorial	O
on	O
transdimensional	O
mcmc	B
in	O
p	O
green	O
n	O
hjort	O
and	O
s	O
richardson	O
highly	O
structured	O
stochastic	O
systems	O
oup	O
greenshtein	O
e	O
and	O
j	O
park	O
application	O
of	O
non	O
parametric	O
empirical	B
bayes	I
estimation	O
to	O
high	O
dimensional	O
classification	O
j	O
of	O
machine	B
learning	B
research	O
greig	O
d	O
b	O
porteous	O
and	O
a	O
seheult	O
exact	O
maximum	B
a	I
posteriori	I
estimation	O
for	O
binary	O
images	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
griffin	O
j	O
and	O
p	O
brown	O
bayesian	B
adaptive	O
lassos	O
with	O
nonconvex	O
penalization	O
technical	O
report	O
u	O
kent	O
griffin	O
j	O
and	O
p	O
brown	O
inference	B
with	O
normal-gamma	O
prior	O
distributions	O
in	O
regression	B
problems	O
bayesian	B
analysis	O
griffiths	O
t	O
and	O
j	O
tenenbaum	O
theory-based	O
causal	O
induction	B
psychological	O
review	O
griffiths	O
t	O
and	O
m	O
steyvers	O
finding	O
scientific	O
topics	O
proc	O
of	O
the	O
national	O
academy	O
of	O
science	O
usa	O
griffiths	O
t	O
m	O
steyvers	O
d	O
blei	O
and	O
integrating	O
j	O
tenenbaum	O
topics	O
and	O
syntax	O
in	O
nips	O
griffiths	O
t	O
and	O
j	O
tenenbaum	O
using	O
vocabulary	O
knowledge	O
in	O
bayesian	B
multinomial	B
estimation	O
in	O
nips	O
pp	O
griffiths	O
t	O
and	O
j	O
tenenbaum	O
structure	O
and	O
strength	O
in	O
causal	O
induction	B
cognitive	O
psychology	O
grimmett	O
g	O
and	O
d	O
stirzaker	O
probability	O
and	O
random	O
processes	O
oxford	O
guan	O
y	O
j	O
dy	O
d	O
niu	O
and	O
z	O
ghahramani	O
variational	B
inference	B
for	O
nonparametric	O
multiple	O
clustering	B
in	O
intl	O
workshop	O
on	O
discovering	O
summarizing	O
and	O
using	O
multiple	O
clustering	B
guedon	O
y	O
estimating	O
hidden	B
semi-markov	O
chains	O
from	O
discrete	B
sequences	O
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
gustafsson	O
m	O
bilistic	O
derivation	O
of	O
least-squares	O
algorithm	O
chemical	O
ing	O
a	O
probathe	O
partial	O
journal	O
of	O
information	B
and	O
model	O
guyon	O
i	O
s	O
gunn	O
m	O
nikravesh	O
and	O
l	O
zadeh	O
feature	B
extraction	I
foundations	O
and	O
applications	O
springer	O
hacker	O
j	O
and	O
p	O
pierson	O
winner-take-all	O
how	O
washington	O
made	O
the	O
rich	O
richer	O
and	O
turned	O
its	O
back	O
on	O
the	O
middle	O
class	O
simon	O
schuster	O
politics	O
halevy	O
a	O
p	O
norvig	O
and	O
f	O
pereira	O
the	O
unreasonable	O
effectiveness	O
of	O
data	O
ieee	O
intelligent	O
systems	O
hall	O
p	O
j	O
t	O
ormerod	O
and	O
m	O
p	O
wand	O
theory	O
of	O
gaussian	B
variational	O
approximation	O
for	O
a	O
generalised	O
linear	O
mixed	B
model	I
statistica	O
sinica	O
hamilton	O
j	O
analysis	O
of	O
time	O
series	O
subject	O
to	O
changes	O
in	O
regime	O
j	O
econometrics	O
hans	O
c	O
bayesian	B
lasso	B
regression	B
biometrika	O
hansen	O
m	O
and	O
b	O
yu	O
model	B
selection	I
and	O
the	O
principle	O
of	O
minimum	B
description	I
length	I
j	O
of	O
the	O
am	O
stat	O
assoc	O
hara	O
h	O
and	O
a	O
takimura	O
a	O
localization	O
approach	O
to	O
improve	O
iterative	O
proportional	O
scaling	O
in	O
gaussian	B
graphical	B
models	I
communications	O
in	O
statistics	O
theory	O
and	O
method	O
to	O
appear	O
hardin	O
j	O
and	O
j	O
hilbe	O
generalized	B
estimating	I
equations	I
chapman	O
and	O
hallcrc	O
harmeling	O
s	O
and	O
c	O
k	O
i	O
williams	O
greedy	O
learning	B
of	O
binary	O
latent	B
trees	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
harnard	O
s	O
the	O
symbol	B
grounding	I
problem	O
physica	O
d	O
green	O
p	O
and	O
b	O
silverman	O
nonparametric	O
regression	B
and	O
generalized	B
linear	I
models	I
chapman	O
and	O
hall	O
guo	O
y	O
supervised	O
exponential	B
family	B
principal	B
component	I
analysis	O
via	O
convex	B
optimization	B
in	O
nips	O
harvey	O
a	O
c	O
forecasting	O
structural	B
time	I
series	I
models	O
and	O
the	O
kalman	O
filter	O
cambridge	O
univerity	O
press	O
bibliography	O
hastie	O
t	O
s	O
rosset	O
r	O
tibshirani	O
and	O
j	O
zhu	O
the	O
entire	O
regularization	B
path	B
for	O
the	O
support	B
vector	I
machine	I
j	O
of	O
machine	B
learning	B
research	O
hastie	O
t	O
and	O
r	O
tibshirani	O
generalized	O
additive	O
models	O
chapman	O
and	O
hall	O
hastie	O
t	O
r	O
tibshirani	O
and	O
j	O
friedman	O
the	O
elements	O
of	O
statistical	O
learning	B
springer	O
hastie	O
t	O
r	O
tibshirani	O
and	O
j	O
friedman	O
the	O
elements	O
of	O
statistical	O
learning	B
springer	O
edition	O
hastings	O
w	O
monte	B
carlo	I
sampling	O
methods	O
using	O
markov	B
chains	O
applications	O
biometrika	O
their	O
and	O
haykin	O
s	O
neural	B
networks	I
a	O
comprehensive	O
foundation	O
prentice	O
hall	O
edition	O
haykin	O
s	O
kalman	O
filter	O
ing	O
and	O
neural	B
networks	I
wiley	O
hazan	O
t	O
and	O
a	O
shashua	O
convergent	O
message-passing	O
algorithms	O
for	O
inference	B
over	O
general	O
graphs	O
with	O
convex	B
free	B
energy	I
in	O
uai	O
hazan	O
t	O
and	O
a	O
shashua	O
norm-product	O
belief	B
propagation	I
primal-dual	O
message	B
passing	I
for	O
approximate	B
inference	B
ieee	O
trans	O
on	O
info	O
theory	O
he	O
y	O
-b	O
and	O
z	O
geng	O
active	B
learning	B
of	O
causal	B
networks	I
with	O
intervention	O
experiments	O
and	O
optimal	O
designs	O
j	O
of	O
machine	B
learning	B
research	O
heaton	O
m	O
and	O
j	O
bayesian	B
computation	O
and	O
the	O
linear	O
model	O
technical	O
report	O
duke	O
scott	O
heckerman	O
d	O
d	O
chickering	O
and	O
c	O
meek	O
r	O
rounthwaite	O
c	O
kadie	O
dependency	B
networks	I
for	O
density	B
estimation	I
collaborative	O
filtering	B
and	O
data	O
visualization	O
j	O
of	O
machine	B
learning	B
research	O
heckerman	O
d	O
d	O
geiger	O
and	O
learning	B
m	O
chickering	O
bayesian	B
networks	I
the	O
combination	O
of	O
knowledge	O
and	O
statistical	O
data	O
machine	B
learning	B
heckerman	O
d	O
c	O
meek	O
february	O
and	O
g	O
cooper	O
a	O
bayesian	B
approach	O
to	O
causal	O
discovery	O
technical	O
report	O
microsoft	B
research	O
heckerman	O
d	O
c	O
meek	O
and	O
d	O
koller	O
probabilistic	O
models	O
for	O
relational	O
data	O
technical	O
report	O
microsoft	B
research	O
heller	O
k	O
and	O
z	O
ghahramani	O
bayesian	B
hierarchical	B
clustering	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
henrion	O
m	O
propagation	O
of	O
uncertainty	B
by	O
logic	B
sampling	I
in	O
bayes	O
networks	O
in	O
uai	O
pp	O
herbrich	O
r	O
t	O
minka	O
and	O
t	O
graepel	O
trueskill	B
a	O
bayesian	B
skill	O
rating	O
system	O
in	O
nips	O
hertz	O
j	O
a	O
krogh	O
and	O
r	O
g	O
palmer	O
an	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
comptuation	O
addisonwesley	O
hillar	O
c	O
j	O
sohl-dickstein	O
and	O
k	O
koepsell	O
april	O
efficient	O
and	O
optimal	O
binary	O
hopfield	O
associative	B
memory	I
storage	O
using	O
minimum	O
probability	O
flow	O
technical	O
report	O
hinton	O
g	O
products	O
of	O
experts	O
in	O
proc	O
intl	O
conf	O
on	O
artif	O
neural	B
networks	I
volume	O
pp	O
hinton	O
g	O
training	O
products	O
of	O
experts	O
by	O
minimizing	O
contrastive	B
divergence	I
neural	O
computation	O
hinton	O
g	O
a	O
practical	O
guide	O
to	O
training	O
restricted	O
boltzmann	O
machines	O
technical	O
report	O
u	O
toronto	O
hinton	O
g	O
and	O
d	O
v	O
camp	O
keeping	O
neural	B
networks	I
simple	O
by	O
minimizing	O
the	O
description	O
length	O
of	O
the	O
weights	O
in	O
in	O
proc	O
of	O
the	O
ann	O
acm	O
conf	O
on	O
computational	B
learning	B
theory	I
pp	O
acm	O
press	O
hinton	O
g	O
s	O
osindero	O
and	O
y	O
teh	O
a	O
fast	O
learning	B
algorithm	O
for	O
deep	B
belief	O
nets	O
neural	O
computation	O
hinton	O
g	O
and	O
r	O
salakhutdinov	O
july	O
reducing	O
the	O
dimensionality	O
of	O
data	O
with	O
neural	B
networks	I
science	O
hinton	O
g	O
e	O
p	O
dayan	O
and	O
m	O
revow	O
modeling	O
the	O
manifolds	O
of	O
images	O
of	O
handwritten	O
digits	O
ieee	O
trans	O
on	O
neural	B
networks	I
hinton	O
g	O
e	O
and	O
y	O
teh	O
discovering	O
multiple	O
constraints	O
that	O
are	O
frequently	O
approximately	O
satis	O
n	O
aed	O
in	O
uai	O
hjort	O
n	O
c	O
holmes	O
p	O
muller	O
and	O
bayesian	B
s	O
walker	O
nonparametrics	O
cambridge	O
hoefling	O
h	O
a	O
path	B
algorithm	O
for	O
the	O
fused	B
lasso	B
signal	O
approximator	O
technical	O
report	O
stanford	O
hoefling	O
h	O
r	O
and	O
tibshirani	O
estimation	O
of	O
sparse	B
binary	O
pairwise	O
markov	B
networks	O
using	O
pseudo-likelihoods	O
j	O
of	O
machine	B
learning	B
research	O
hoeting	O
j	O
d	O
madigan	O
a	O
raftery	O
and	O
c	O
volinsky	O
bayesian	B
model	O
averaging	O
a	O
tutorial	O
statistical	O
science	O
hoff	O
p	O
d	O
a	O
first	O
course	O
in	O
bayesian	B
statistical	O
methods	O
springer	O
july	O
hoffman	O
m	O
d	O
blei	O
and	O
f	O
bach	O
online	B
learning	B
for	O
latent	B
dirichlet	B
allocation	I
in	O
nips	O
hoffman	O
m	O
and	O
a	O
gelman	O
the	O
no-u-turn	O
sampler	O
adaptively	O
setting	O
path	B
lengths	O
in	O
hamiltonian	O
monte	B
carlo	I
technical	O
report	O
columbia	O
u	O
hofmann	O
t	O
probabilistic	B
latent	B
semantic	I
indexing	I
research	O
and	O
development	O
in	O
information	B
retrieval	I
holmes	O
c	O
and	O
l	O
held	O
bayesian	B
auxiliary	O
variable	O
models	O
for	O
binary	O
and	O
multinomial	B
regression	B
bayesian	B
analysis	O
honkela	O
a	O
and	O
h	O
valpola	O
variational	O
learning	B
and	O
bits-back	B
coding	O
an	O
information-theoretic	O
view	O
to	O
bayesian	B
learning	B
ieee	O
trans	O
on	O
neural	B
networks	I
honkela	O
a	O
h	O
valpola	O
and	O
j	O
karhunen	O
accelerating	O
cyclic	O
update	O
algorithms	O
for	O
parameter	B
estimation	O
by	O
pattern	B
searches	O
neural	O
processing	O
letters	O
bibliography	O
hopfield	O
j	O
j	O
april	O
neural	B
networks	I
and	O
physical	O
systems	O
with	O
emergent	O
collective	O
computational	O
abilities	O
proc	O
of	O
the	O
national	O
academy	O
of	O
science	O
usa	O
a	O
hornik	O
k	O
approximation	O
capabilities	O
of	O
multilayer	O
feedforward	O
networks	O
neural	B
networks	I
a	O
horvitz	O
e	O
j	O
apacible	O
r	O
sarin	O
and	O
l	O
liao	O
prediction	O
expectation	O
and	O
surprise	O
methods	O
designs	O
and	O
study	O
of	O
a	O
deployed	O
traffic	O
forecasting	O
service	O
in	O
uai	O
howard	O
r	O
and	O
j	O
matheson	O
influence	O
diagrams	O
in	O
r	O
howard	O
and	O
j	O
matheson	O
readings	O
on	O
the	O
principles	O
and	O
applications	O
of	O
decision	B
analysis	O
volume	O
ii	O
strategic	O
decisions	O
group	O
hoyer	O
p	O
non-negative	O
matrix	O
factorizaton	O
with	O
sparseness	O
constraints	O
j	O
of	O
machine	B
learning	B
research	O
hsu	O
c	O
-w	O
c	O
-c	O
chang	O
and	O
c	O
-j	O
lin	O
a	O
practical	O
guide	O
to	O
support	B
vector	O
classification	O
technical	O
report	O
dept	O
comp	O
sci	O
national	O
taiwan	O
university	O
hu	O
d	O
l	O
van	O
der	O
maaten	O
y	O
cho	O
l	O
saul	O
and	O
s	O
lerner	O
latent	B
variable	I
models	I
for	O
predicting	O
file	O
dependencies	O
in	O
large-scale	O
software	O
development	O
in	O
nips	O
hu	O
m	O
c	O
ingram	O
m	O
sirski	O
c	O
pal	O
s	O
swamy	O
and	O
c	O
patten	O
a	O
hierarchical	B
hmm	B
implementation	O
for	O
vertebrate	O
gene	O
splice	O
site	O
prediction	O
technical	O
report	O
dept	O
computer	O
science	O
univ	O
waterloo	O
huang	O
j	O
q	O
morris	O
and	O
b	O
frey	O
bayesian	B
inference	B
of	O
microrna	O
targets	O
from	O
sequence	O
and	O
expression	O
data	O
j	O
comp	O
bio	B
hubel	O
d	O
and	O
t	O
wiesel	O
receptive	O
fields	O
binocular	O
itneraction	O
and	O
functional	O
architecture	O
in	O
the	O
cat	O
s	O
visual	O
cortex	O
j	O
physiology	O
huber	O
p	O
robust	B
estimation	O
of	O
a	O
location	O
parameter	B
annals	O
of	O
statistics	O
a	O
hubert	O
l	O
and	O
p	O
arabie	O
comj	O
of	O
classifica	O
paring	O
partitions	O
tion	O
hunter	O
d	O
and	O
r	O
li	O
variable	O
selection	O
using	O
mm	B
algorithms	O
annals	O
of	O
statistics	O
jacob	O
l	O
f	O
bach	O
and	O
j	O
-p	O
vert	O
a	O
clustered	O
multi-task	B
learning	B
convex	B
formulation	O
in	O
nips	O
hunter	O
d	O
r	O
and	O
k	O
lange	O
a	O
tutorial	O
on	O
mm	B
algorithms	O
the	O
american	O
statistician	O
jain	O
a	O
and	O
r	O
dubes	O
algorithms	O
for	O
clustering	B
data	O
prentice	O
hall	O
hyafil	O
l	O
and	O
r	O
rivest	O
constructing	O
optimal	O
binary	O
decision	B
trees	I
is	O
np-complete	B
information	B
processing	O
letters	O
james	O
g	O
and	O
t	O
hastie	O
the	O
error	O
coding	O
method	O
and	O
picts	O
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
hyvarinen	O
a	O
j	O
hurri	O
and	O
p	O
hoyer	O
natural	O
image	O
statistics	O
a	O
probabilistic	O
approach	O
to	O
early	O
computational	O
vision	O
springer	O
hyvarinen	O
a	O
and	O
e	O
oja	O
independent	B
component	I
analysis	I
algorithms	O
and	O
applications	O
neural	B
networks	I
ilin	O
a	O
and	O
t	O
raiko	O
practical	O
approaches	O
to	O
principal	B
component	I
analysis	O
in	O
the	O
presence	O
of	O
missing	B
values	O
j	O
of	O
machine	B
learning	B
research	O
insua	O
d	O
r	O
and	O
f	O
ruggeri	O
robust	B
bayesian	B
analysis	O
springer	O
isard	O
m	O
pampas	O
real-valued	O
graphical	B
models	I
for	O
computer	O
vision	O
in	O
cvpr	O
volume	O
pp	O
isard	O
m	O
and	O
a	O
blake	O
condensation	B
conditional	O
density	O
propagation	O
for	O
visual	O
tracking	B
intl	O
j	O
of	O
computer	O
vision	O
jaakkola	O
t	O
tutorial	O
on	O
variational	O
approximation	O
methods	O
in	O
m	O
opper	O
and	O
d	O
saad	O
advanced	O
mean	B
field	O
methods	O
mit	O
press	O
jaakkola	O
t	O
and	O
d	O
haussler	O
exploiting	O
generative	O
models	O
in	O
discriminative	B
classifiers	O
in	O
nips	O
pp	O
jaakkola	O
t	O
and	O
m	O
upper	O
computing	O
and	O
bounds	O
on	O
likelihoods	O
tractable	O
networks	O
in	O
uai	O
jordan	O
lower	O
in	O
in	O
jaakkola	O
t	O
and	O
m	O
jordan	O
a	O
variational	O
approach	O
to	O
bayesian	B
logistic	B
regression	B
problems	O
and	O
their	O
extensions	O
in	O
ai	B
statistics	O
japkowicz	O
n	O
s	O
hanson	O
and	O
m	O
gluck	O
nonlinear	O
autoassociation	O
is	O
not	O
equivalent	O
to	O
pca	B
neural	O
computation	O
jaynes	O
e	O
t	O
probability	B
theory	I
the	O
logic	O
of	O
science	O
cambridge	O
university	O
press	O
jebara	O
t	O
r	O
kondor	O
and	O
a	O
howard	O
probability	O
product	O
kernels	O
j	O
of	O
machine	B
learning	B
research	O
jeffreys	O
h	O
theory	O
of	O
probability	O
oxford	O
jelinek	O
f	O
statistical	O
methods	O
for	O
speech	B
recognition	I
mit	O
press	O
jensen	O
c	O
s	O
a	O
kong	O
and	O
u	O
kjaerulff	O
blocking-gibbs	O
sampling	O
in	O
very	O
large	O
probabilistic	O
expert	O
systems	O
intl	O
j	O
human-computer	O
studies	O
jermyn	O
i	O
invariant	B
bayesian	B
estimation	O
on	O
manifolds	O
annals	O
of	O
statistics	O
jerrum	O
m	O
and	O
a	O
sinclair	O
polynomial-time	O
approximation	O
algorithms	O
for	O
the	O
ising	B
model	I
siam	O
j	O
on	O
computing	O
jerrum	O
m	O
and	O
a	O
sinclair	O
the	O
markov	B
chain	I
monte	B
carlo	I
method	O
an	O
approach	O
to	O
approximate	O
counting	O
and	O
integration	O
in	O
d	O
s	O
hochbaum	O
approximation	O
algorithms	O
for	O
np-hard	B
problems	O
pws	O
publishing	O
jerrum	O
m	O
a	O
sinclair	O
and	O
e	O
vigoda	O
a	O
polynomial-time	O
approximation	O
algorithm	O
for	O
the	O
permanent	B
of	O
a	O
matrix	O
with	O
non-negative	O
entries	O
journal	O
of	O
the	O
acm	O
jaakkola	O
t	O
s	O
and	O
m	O
i	O
jordan	O
bayesian	B
parameter	B
estimation	O
via	O
variational	O
methods	O
statistics	O
and	O
computing	O
ji	O
and	O
l	O
carin	O
s	O
d	O
dunson	O
multi-task	O
compressive	B
sensing	I
ieee	O
trans	O
signal	B
processing	I
bibliography	O
ji	O
s	O
l	O
tang	O
s	O
yu	O
and	O
j	O
ye	O
a	O
shared-subspace	O
learning	B
framework	O
for	O
multi-label	O
classification	O
acm	O
trans	O
on	O
knowledge	B
discovery	I
from	O
data	O
jirousek	O
r	O
and	O
s	O
preucil	O
on	O
the	O
effective	O
implementation	O
of	O
the	O
iterative	O
proportional	O
fitting	O
procedure	O
computational	O
statistics	O
data	O
analysis	O
joachims	O
t	O
training	O
linear	O
svms	O
in	O
linear	O
time	O
in	O
proc	O
of	O
the	O
int	O
l	O
conf	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
joachims	O
t	O
t	O
finley	O
and	O
c	O
-n	O
yu	O
cutting-plane	O
training	O
of	O
structural	O
svms	O
machine	B
learning	B
johnson	O
j	O
k	O
d	O
m	O
malioutov	O
and	O
a	O
s	O
willsky	O
walk-sum	O
interpretation	O
and	O
analysis	O
of	O
gaussian	B
belief	B
propagation	I
in	O
nips	O
pp	O
johnson	O
m	O
capacity	O
and	O
complexity	O
of	O
hmm	B
duration	O
modeling	O
techniques	O
signal	B
processing	I
letters	O
johnson	O
n	O
a	O
study	O
of	O
the	O
nips	O
feature	B
selection	I
challenge	O
technical	O
report	O
stanford	O
johnson	O
v	O
and	O
j	O
albert	O
ordi	O
nal	O
data	O
modeling	O
springer	O
jones	O
b	O
a	O
dobra	O
c	O
carvalho	O
c	O
hans	O
c	O
carter	O
and	O
m	O
west	O
experiments	O
in	O
stochastic	O
computation	O
for	O
high-dimensional	O
graphical	B
models	I
statistical	O
science	O
jordan	O
m	O
i	O
an	O
introduction	O
to	O
in	O
probabilistic	O
graphical	B
models	I
preparation	O
jordan	O
m	O
i	O
the	O
era	O
of	O
big	O
in	O
isba	O
bulletin	O
volume	O
data	O
pp	O
jordan	O
m	O
i	O
z	O
ghahramani	O
t	O
s	O
jaakkola	O
and	O
l	O
k	O
saul	O
an	O
introduction	O
to	O
variational	O
methods	O
for	O
graphical	B
models	I
in	O
m	O
jordan	O
learning	B
in	O
graphical	B
models	I
mit	O
press	O
journee	O
m	O
y	O
nesterov	O
p	O
richtarik	O
and	O
r	O
sepulchre	O
generalized	O
power	B
method	I
for	O
sparse	B
principal	B
components	I
analysis	I
j	O
of	O
machine	B
learning	B
research	O
julier	O
s	O
and	O
j	O
uhlmann	O
a	O
new	O
extension	B
of	O
the	O
kalman	O
filter	O
to	O
nonlinear	O
systems	O
in	O
proc	O
of	O
aerosense	O
the	O
intl	O
symp	O
on	O
aerospacedefence	O
sensing	O
simulation	O
and	O
controls	O
jurafsky	O
d	O
and	O
j	O
h	O
martin	O
speech	O
and	O
language	O
processing	O
an	O
introduction	O
to	O
natural	O
language	O
processing	O
computational	O
linguistics	O
and	O
speech	B
recognition	I
prentice-hall	O
jurafsky	O
d	O
and	O
j	O
h	O
martin	O
speech	O
and	O
language	O
processing	O
an	O
introduction	O
to	O
natural	O
language	O
processing	O
computational	O
linguistics	O
and	O
speech	B
recognition	I
prentice-hall	O
edition	O
kaariainen	O
m	O
and	O
j	O
langford	O
a	O
comparison	O
of	O
tight	O
generalization	B
bounds	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
kaelbling	O
l	O
m	O
a	O
moore	O
learning	B
a	O
survey	O
search	O
and	O
littman	O
reinforcement	O
j	O
of	O
ai	B
re	O
kaelbling	O
l	O
p	O
m	O
littman	O
and	O
planning	O
a	O
cassandra	O
and	O
acting	O
in	O
partially	O
observable	O
stochastic	O
domains	O
artificial	O
intelligence	O
kaiser	O
h	O
the	O
varimax	B
criterion	O
for	O
analytic	O
rotation	O
in	O
factor	B
analysis	I
psychometrika	O
kakade	O
s	O
y	O
w	O
teh	O
and	O
s	O
roweis	O
an	O
alternate	O
objective	O
function	O
for	O
markovian	O
fields	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
kanazawa	O
k	O
d	O
koller	O
and	O
s	O
russell	O
stochastic	O
simulation	O
algorithms	O
for	O
dynamic	O
probabilistic	O
networks	O
in	O
uai	O
kandel	O
e	O
j	O
schwarts	O
and	O
t	O
jessell	O
principles	O
of	O
neural	O
science	O
mcgraw-hill	O
jordan	O
m	O
i	O
and	O
r	O
a	O
jacobs	O
hierarchical	O
mixtures	O
of	O
experts	O
and	O
the	O
em	B
algorithm	O
neural	O
computation	O
kappen	O
h	O
and	O
f	O
rodriguez	O
boltzmann	B
machine	B
learning	B
using	O
mean	B
field	O
theory	O
and	O
linear	O
response	O
correction	O
in	O
nips	O
karhunen	O
j	O
and	O
j	O
joutsensalo	O
generalizations	O
of	O
principal	B
component	I
analysis	O
optimization	B
problems	O
and	O
neural	B
networks	I
neural	B
networks	I
kass	O
r	O
and	O
l	O
wasserman	O
a	O
reference	O
bayesian	B
test	O
for	O
nested	O
hypotheses	O
and	O
its	O
relationship	O
to	O
the	O
schwarz	O
criterio	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
katayama	O
t	O
subspace	O
methods	O
for	O
systems	O
identification	O
springer	O
verlag	O
kaufman	O
l	O
and	O
p	O
rousseeuw	O
finding	O
groups	O
in	O
data	O
an	O
introduction	O
to	O
cluster	O
analysis	O
wiley	O
kawakatsu	O
h	O
and	O
a	O
largey	O
em	B
algorithms	O
for	O
ordered	O
probit	B
models	O
with	O
endogenous	O
regressors	O
the	O
econometrics	O
journal	O
kearns	O
m	O
j	O
and	O
u	O
v	O
vazirani	O
an	O
introduction	O
to	O
computational	B
learning	B
theory	I
mit	O
press	O
kelley	O
j	O
e	O
the	O
cutting-plane	O
method	O
for	O
solving	O
convex	B
programs	O
j	O
of	O
the	O
soc	O
for	O
industrial	O
and	O
applied	O
math	O
kemp	O
c	O
j	O
tenenbaum	O
s	O
niyogi	O
and	O
t	O
griffiths	O
a	O
probabilistic	O
model	O
of	O
theory	O
formation	O
cognition	O
kemp	O
c	O
j	O
tenenbaum	O
t	O
y	O
t	O
griffiths	O
and	O
and	O
n	O
ueda	O
learning	B
systems	O
of	O
concepts	O
with	O
an	O
infinite	O
relational	O
model	O
in	O
aaai	O
kersting	O
k	O
s	O
natarajan	O
and	O
d	O
poole	O
statistical	B
relational	I
ai	B
logic	O
probability	O
and	O
computation	O
technical	O
report	O
ubc	O
khan	O
m	O
e	O
b	O
marlin	O
g	O
bouchard	O
and	O
k	O
p	O
murphy	O
variational	O
bounds	O
for	O
mixed-data	O
factor	B
analysis	I
in	O
nips	O
khan	O
z	O
t	O
balch	O
and	O
f	O
dellaert	O
mcmc	B
data	B
association	I
and	O
sparse	B
factorization	O
updating	O
for	O
real	O
time	O
multitarget	O
tracking	B
with	O
merged	O
and	O
multiple	O
measurements	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
kirkpatrick	O
s	O
c	O
g	O
jr	O
and	O
m	O
vecchi	O
optimization	B
by	O
simulated	B
annealing	B
science	O
bibliography	O
kitagawa	O
g	O
the	O
two-filter	O
formula	O
for	O
smoothing	B
and	O
an	O
implementation	O
of	O
the	O
gaussian-sum	O
smoother	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
koller	O
d	O
and	O
u	O
lerner	O
sampling	O
in	O
factored	O
dynamic	O
systems	O
in	O
a	O
doucet	O
n	O
de	O
freitas	O
and	O
n	O
gordon	O
sequential	B
monte	B
carlo	I
methods	O
in	O
practice	O
springer	O
kjaerulff	O
u	O
triangulation	O
of	O
graphs	O
algorithms	O
giving	O
small	O
total	O
state	B
space	I
technical	O
report	O
dept	O
of	O
math	O
and	O
comp	O
sci	O
aalborg	O
univ	O
denmark	O
kjaerulff	O
u	O
and	O
a	O
madsen	O
bayesian	B
networks	I
and	O
influence	O
diagrams	O
a	O
guide	O
to	O
construction	O
and	O
analysis	O
springer	O
klaassen	O
c	O
and	O
j	O
a	O
wellner	O
efficient	O
estimation	O
in	O
the	O
bivariate	O
noramal	O
copula	O
model	O
normal	B
margins	O
are	O
least	O
favorable	O
bernoulli	B
klami	O
a	O
and	O
s	O
kaski	O
probabilistic	O
approach	O
to	O
detecting	O
dependencies	O
between	O
data	O
sets	O
neurocomputing	O
klami	O
a	O
s	O
virtanen	O
and	O
s	O
kaski	O
bayesian	B
exponential	B
family	B
projections	O
for	O
coupled	O
data	O
sources	O
in	O
uai	O
kleiner	O
a	O
a	O
talwalkar	O
p	O
sarkar	O
and	O
m	O
i	O
jordan	O
a	O
scalable	O
bootstrap	B
for	O
massive	O
data	O
technical	O
report	O
uc	O
berkeley	O
kneser	O
r	O
and	O
h	O
ney	O
improved	O
backing-off	O
for	O
n-gram	B
language	B
modeling	I
in	O
intl	O
conf	O
on	O
acoustics	O
speech	O
and	O
signal	O
proc	O
volume	O
pp	O
ko	O
j	O
and	O
d	O
fox	O
gpbayesfilters	O
bayesian	B
filtering	B
using	O
gaussian	B
process	I
prediction	O
and	O
observation	B
models	O
autonomous	O
robots	O
journal	O
kohn	O
r	O
m	O
smith	O
and	O
d	O
chan	O
nonparametric	O
regression	B
using	O
linear	O
combinations	O
of	O
basis	B
functions	I
statistical	O
computing	O
koivisto	O
m	O
advances	O
in	O
exact	O
bayesian	B
structure	O
discovery	O
in	O
bayesian	B
networks	I
in	O
uai	O
koivisto	O
m	O
and	O
k	O
sood	O
exact	O
bayesian	B
structure	O
discovery	O
in	O
bayesian	B
networks	I
j	O
of	O
machine	B
learning	B
research	O
kolmogorov	O
v	O
october	O
convergent	O
tree-reweighted	O
message	B
passing	I
for	O
energy	O
minimization	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
kolmogorov	O
v	O
and	O
m	O
wainwright	O
on	O
optimality	O
properties	O
of	O
tree-reweighted	O
message	B
passing	I
in	O
uai	O
pp	O
kolmogorov	O
v	O
and	O
r	O
zabin	O
what	O
energy	O
functions	O
can	O
be	O
minimized	O
via	O
graph	B
cuts	I
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
komodakis	O
n	O
n	O
paragios	O
and	O
g	O
tziritas	O
mrf	B
energy	O
minimization	O
and	O
beyond	O
via	O
dual	B
decomposition	I
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
koo	O
t	O
a	O
m	O
rush	O
m	O
collins	O
t	O
jaakkola	O
and	O
d	O
sontag	O
dual	B
decomposition	I
for	O
parsing	O
with	O
non-projective	O
head	O
automata	O
in	O
proc	O
emnlp	O
pp	O
a	O
koren	O
y	O
the	O
bellkor	O
solution	O
to	O
the	O
netflix	O
grand	O
prize	O
technical	O
report	O
yahoo	O
research	O
koren	O
y	O
collaborative	O
filtering	B
with	O
temporal	O
dynamics	O
in	O
proc	O
of	O
the	O
int	O
l	O
conf	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
koren	O
y	O
r	O
bell	O
and	O
c	O
volinsky	O
matrix	B
factorization	I
techniques	O
for	O
recommender	O
systems	O
ieee	O
computer	O
krishnapuram	O
l	O
b	O
carin	O
m	O
figueiredo	O
and	O
a	O
hartemink	O
learning	B
sparse	B
bayesian	B
classifiers	O
multi-class	O
formulation	O
fast	O
algorithms	O
and	O
generalization	B
bounds	O
ieee	O
transaction	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
kschischang	O
f	O
b	O
frey	O
and	O
h	O
-a	O
loeliger	O
february	O
factor	B
graphs	O
and	O
the	O
sum-product	B
algorithm	I
ieee	O
trans	O
info	O
theory	O
kuan	O
p	O
g	O
pan	O
j	O
a	O
thomson	O
r	O
stewart	O
and	O
s	O
keles	O
a	O
hierarchical	O
semi-markov	B
model	I
for	O
detecting	O
enrichment	O
with	O
application	O
to	O
chip-seq	B
experiments	O
technical	O
report	O
u	O
wisconsin	O
kulesza	O
a	O
and	O
b	O
taskar	O
learning	B
determinantal	O
point	O
processes	O
in	O
uai	O
kumar	O
n	O
and	O
a	O
andreo	O
heteroscedastic	O
discriminant	B
analysis	I
and	O
reduced	O
rank	O
hmms	B
for	O
improved	O
speech	B
recognition	I
speech	O
communication	O
kumar	O
s	O
and	O
m	O
hebert	O
discriminative	B
random	O
fields	O
a	O
discriminative	B
framework	O
for	O
contextual	O
interaction	O
in	O
classification	O
in	O
intl	O
conf	O
on	O
computer	O
vision	O
kuo	O
l	O
and	O
b	O
mallick	O
variable	O
selection	O
for	O
regression	B
models	O
sankhya	O
series	O
b	O
kurihara	O
k	O
m	O
welling	O
and	O
n	O
vlassis	O
accelerated	O
variational	O
dp	O
mixture	B
models	O
in	O
nips	O
kushner	O
h	O
and	O
g	O
yin	O
stochastic	B
approximation	I
and	O
recursive	B
algorithms	O
and	O
applications	O
springer	O
kuss	O
and	O
c	O
rasmussen	O
assessing	O
approximate	B
inference	B
for	O
binary	O
gaussian	B
process	I
classification	O
j	O
of	O
machine	B
learning	B
research	O
kwon	O
j	O
and	O
k	O
murphy	O
modeling	O
freeway	O
traffic	O
with	O
coupled	O
hmms	B
technical	O
report	O
univ	O
california	O
berkeley	O
kyung	O
m	O
j	O
gill	O
m	O
ghosh	O
and	O
g	O
casella	O
penalized	O
regression	B
standard	B
errors	I
and	O
bayesian	B
lassos	O
bayesian	B
analysis	O
lacoste-julien	O
s	O
f	O
huszar	O
and	O
z	O
ghahramani	O
approximate	B
inference	B
for	O
the	O
loss-calibrated	O
bayesian	B
in	O
aistatistics	O
koller	O
d	O
and	O
n	O
friedman	O
probabilistic	O
graphical	B
models	I
principles	O
and	O
techniques	O
mit	O
press	O
krizhevsky	O
a	O
and	O
g	O
hinton	O
using	O
very	O
deep	B
autoencoders	O
for	O
content-based	O
image	O
retrieval	O
submitted	O
lacoste-julien	O
s	O
f	O
sha	O
and	O
m	O
i	O
jordan	O
disclda	O
discriminative	B
learning	B
for	O
dimensionality	B
reduction	I
and	O
classification	O
in	O
nips	O
bibliography	O
lafferty	O
j	O
a	O
mccallum	O
and	O
f	O
pereira	O
conditional	O
random	O
fields	O
probabilistic	O
models	O
for	O
segmenting	O
and	O
labeling	O
sequence	O
data	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
lange	O
k	O
r	O
little	O
and	O
j	O
taylor	O
robust	B
statistical	O
modeling	O
using	O
the	O
t	O
disribution	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
langville	O
a	O
and	O
c	O
meyer	O
updating	O
markov	B
chains	O
with	O
an	O
eye	O
on	O
google	O
s	O
pagerank	B
siam	O
j	O
on	O
matrix	O
analysis	O
and	O
applications	O
larranaga	O
p	O
c	O
m	O
h	O
kuijpers	O
m	O
poza	O
and	O
r	O
h	O
murga	O
decomposing	O
bayesian	B
networks	I
triangulation	O
of	O
the	O
moral	O
graph	B
with	O
genetic	B
algorithms	I
statistics	O
and	O
computing	O
lashkari	O
d	O
and	O
p	O
golland	O
convex	B
clustering	B
with	O
examplarbased	O
models	O
in	O
nips	O
lasserre	O
j	O
c	O
bishop	O
and	O
t	O
minka	O
principled	O
hybrids	O
of	O
generative	O
and	O
discriminative	B
models	O
in	O
cvpr	O
lau	O
j	O
and	O
p	O
green	O
bayesian	B
model-based	B
clustering	B
procedures	O
journal	O
of	O
computational	O
and	O
graphical	O
statistics	O
lauritzen	O
s	O
graphical	B
models	I
oup	O
lauritzen	O
s	O
causal	O
inference	B
from	O
graphical	B
models	I
in	O
d	O
r	O
c	O
o	O
e	O
barndoff-nielsen	O
and	O
c	O
klueppelberg	O
complex	O
stochastic	O
systems	O
chapman	O
and	O
hall	O
lauritzen	O
s	O
and	O
d	O
nilsson	O
representing	O
and	O
solving	O
decision	B
problems	O
with	O
limited	O
information	B
management	O
science	O
lauritzen	O
s	O
l	O
december	O
propagation	O
of	O
probabilities	O
means	O
and	O
variances	O
in	O
mixed	O
graphical	O
association	O
models	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
lauritzen	O
s	O
l	O
and	O
d	O
j	O
spiegelhalter	O
local	O
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
applications	O
to	O
expert	O
systems	O
j	O
r	O
stat	O
soc	O
b	O
law	O
e	O
b	O
settles	O
and	O
t	O
mitchell	O
learning	B
to	O
tag	O
from	O
open	O
vocabulary	O
labels	O
in	O
proc	O
european	O
conf	O
on	O
machine	B
learning	B
law	O
m	O
m	O
figueiredo	O
and	O
a	O
jain	O
simultaneous	O
feature	B
selection	I
and	O
clustering	B
using	O
mixture	B
models	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
lawrence	O
n	O
d	O
probabilistic	O
non-linear	O
principal	B
component	I
analysis	O
with	O
gaussian	B
process	I
latent	B
variable	I
models	I
j	O
of	O
machine	B
learning	B
research	O
lawrence	O
n	O
d	O
a	O
unifying	O
probabilistic	O
perspective	O
for	O
specintral	O
dimensionality	B
reduction	I
sights	O
and	O
new	O
models	O
j	O
of	O
machine	B
learning	B
research	O
learned-miller	O
e	O
hyperspacings	O
and	O
the	O
estimation	O
of	O
information	B
theoretic	O
quantities	O
technical	O
report	O
u	O
mass	O
amherst	O
comp	O
sci	O
dept	O
lecun	O
y	O
b	O
boser	O
j	O
s	O
denker	O
d	O
henderson	O
r	O
e	O
howard	O
w	O
hubbard	O
and	O
l	O
d	O
jackel	O
winter	O
backpropagation	B
applied	O
to	O
handwritten	O
zip	O
code	O
recognition	O
neural	O
computation	O
lecun	O
y	O
l	O
bottou	O
y	O
bengio	O
and	O
p	O
haffner	O
november	O
gradient-based	O
learning	B
applied	O
to	O
document	O
recognition	O
proceedings	O
of	O
the	O
ieee	O
lecun	O
y	O
s	O
chopra	O
r	O
hadsell	O
f	O
-j	O
huang	O
and	O
m	O
-a	O
ranzato	O
a	O
tutorial	O
on	O
energy-based	O
learning	B
predicting	O
structured	O
outputs	O
mit	O
press	O
in	O
b	O
et	O
al	O
ledoit	O
o	O
and	O
m	O
wolf	O
honey	O
i	O
shrunk	O
the	O
sample	O
covariance	B
matrix	I
j	O
of	O
portfolio	O
management	O
lee	O
a	O
f	O
caron	O
a	O
doucet	O
and	O
c	O
holmes	O
a	O
hierarchical	O
bayesian	B
framework	O
for	O
constructing	O
sparsity-inducing	O
priors	O
technical	O
report	O
u	O
oxford	O
lee	O
a	O
f	O
caron	O
a	O
doucet	O
and	O
c	O
holmes	O
bayesian	B
sparsitypath-analysis	O
of	O
genetic	O
association	O
signal	O
using	O
generalized	O
t	O
prior	O
technical	O
report	O
u	O
oxford	O
lee	O
d	O
and	O
s	O
seung	O
algorithms	O
for	O
non-negative	B
matrix	B
factorization	I
in	O
nips	O
lee	O
h	O
r	O
grosse	O
r	O
ranganath	O
and	O
a	O
ng	O
convolutional	O
deep	B
belief	B
networks	I
for	O
scalable	O
unsupervised	B
learning	B
of	O
hierarchical	O
representations	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
lee	O
h	O
y	O
largman	O
p	O
pham	O
and	O
a	O
ng	O
unsupervised	O
feature	O
learning	B
for	O
audio	O
classification	O
using	O
convolutional	O
deep	B
belief	B
networks	I
in	O
nips	O
lee	O
s	O
-i	O
v	O
ganapathi	O
and	O
d	O
koller	O
efficient	O
structure	B
learning	B
of	O
markov	B
networks	O
using	O
in	O
nips	O
lee	O
t	O
s	O
and	O
d	O
mumford	O
hierarchical	O
bayesian	B
inference	B
in	O
the	O
visual	O
cortex	O
j	O
of	O
optical	O
society	O
of	O
america	O
a	O
lenk	O
p	O
w	O
s	O
desarbo	O
p	O
green	O
and	O
m	O
young	O
hierarchical	O
bayes	O
conjoint	B
analysis	I
recovery	O
of	O
partworth	O
heterogeneity	O
from	O
reduced	O
experimental	O
designs	O
marketing	O
science	O
lenkoski	O
a	O
and	O
a	O
dobra	O
bayesian	B
structural	O
learning	B
and	O
estimation	O
in	O
gaussian	B
graphical	B
models	I
technical	O
report	O
department	O
of	O
statistics	O
university	O
of	O
washington	O
of	O
comparison	O
lepar	O
v	O
and	O
p	O
p	O
shenoy	O
a	O
lauritzenspiegelhalter	O
hugin	B
and	O
shenoyshafer	O
architectures	O
for	O
computing	O
marginals	O
of	O
probability	O
distributions	O
in	O
g	O
cooper	O
and	O
s	O
moral	O
uai	O
pp	O
morgan	O
kaufmann	O
lauritzen	O
s	O
l	O
the	O
em	B
algorithm	O
for	O
graphical	O
association	O
models	O
with	O
missing	B
data	I
computational	O
statistics	O
and	O
data	O
analysis	O
ledoit	O
o	O
and	O
m	O
wolf	O
a	O
wellconditioned	O
estimator	B
largedimensional	O
covariance	B
matrices	O
j	O
of	O
multivariate	O
analysis	O
for	O
lerner	O
u	O
and	O
r	O
parr	O
inference	B
in	O
hybrid	O
networks	O
theoretical	O
limits	O
and	O
practical	O
algorithms	O
in	O
uai	O
bibliography	O
leslie	O
c	O
e	O
eskin	O
a	O
cohen	O
j	O
weston	O
and	O
w	O
noble	O
mismatch	O
string	O
kernels	O
for	O
discriminative	B
protein	O
classification	O
bioinformatics	O
levy	O
s	O
in	O
the	O
plex	O
how	O
google	O
thinks	O
works	O
and	O
shapes	O
our	O
lives	O
simon	O
schuster	O
li	O
l	O
w	O
chu	O
j	O
langford	O
and	O
x	O
wang	O
unbiased	B
offline	B
evaluation	O
of	O
contextual-banditbased	O
news	O
article	O
recommendation	O
algorithms	O
in	O
wsdm	O
liang	O
f	O
s	O
mukherjee	O
and	O
m	O
west	O
understanding	O
the	O
use	O
of	O
unlabelled	O
data	O
in	O
predictive	B
modelling	O
statistical	O
science	O
liang	O
f	O
r	O
paulo	O
g	O
molina	O
m	O
clyde	O
and	O
j	O
berger	O
mixtures	O
of	O
g-priors	O
for	O
bayesian	B
variable	O
selection	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
i	O
liang	O
p	O
and	O
m	O
jordan	O
an	O
asymptotic	O
analysis	O
of	O
generative	O
discriminative	B
and	O
pseudolikelihood	O
estimators	O
in	O
international	O
conference	O
on	O
machine	B
learning	B
liang	O
p	O
and	O
d	O
klein	O
online	B
em	B
in	O
proc	O
for	O
unsupervised	O
models	O
naacl	O
conference	O
liao	O
l	O
d	O
j	O
patterson	O
d	O
fox	O
and	O
h	O
kautz	O
learning	B
and	O
inferring	O
transportation	O
routines	O
artificial	O
intelligence	O
lindley	O
d	O
scoring	O
rules	B
and	O
isi	O
the	O
inevetability	O
of	O
probability	O
review	O
lindley	O
d	O
v	O
bayesian	B
statistics	I
a	O
review	O
siam	O
lindley	O
d	O
v	O
and	O
l	O
d	O
phillips	O
inference	B
for	O
a	O
bernoulli	B
process	O
bayesian	B
view	O
the	O
american	O
statistician	O
lindsay	O
b	O
composite	O
likecontemporary	O
lihood	O
methods	O
mathematics	O
lipton	O
r	O
j	O
and	O
r	O
e	O
tarjan	O
theorem	O
for	O
planar	O
siam	O
journal	O
of	O
applied	O
a	O
separator	O
graphs	O
math	O
little	O
r	O
j	O
and	O
d	O
b	O
rubin	O
statistical	O
analysis	O
with	O
missing	B
data	I
new	O
york	O
wiley	O
and	O
son	O
liu	O
c	O
and	O
d	O
rubin	O
ml	O
estimation	O
of	O
the	O
t	O
distribution	O
using	O
em	B
and	O
its	O
extensions	O
ecm	B
and	O
ecme	B
statistica	O
sinica	O
liu	O
h	O
j	O
lafferty	O
and	O
l	O
wasserman	O
the	O
nonparanormal	B
semiparametric	O
estimation	O
of	O
high	O
dimensional	O
undirected	B
graphs	O
j	O
of	O
machine	B
learning	B
research	O
liu	O
j	O
monte	B
carlo	I
strategies	O
in	O
scientific	O
computation	O
springer	O
liu	O
j	O
s	O
w	O
h	O
wong	O
and	O
a	O
kong	O
covariance	B
structure	O
of	O
the	O
gibbs	B
sampler	I
with	O
applications	O
to	O
the	O
comparisons	O
of	O
estimators	O
and	O
augmentation	O
schemes	O
biometrika	O
liu	O
t	O
-y	O
learning	B
to	I
rank	I
for	O
information	B
retrieval	I
foundations	O
and	O
trends	O
in	O
information	B
retrieval	I
lizotte	O
d	O
practical	O
bayesian	B
optimization	B
ph	O
d	O
thesis	O
u	O
alberta	O
ljung	O
l	O
system	O
identificiation	O
theory	O
for	O
the	O
user	O
prentice	O
hall	O
lo	O
c	O
h	O
statistical	O
methods	O
for	O
high	B
throughput	I
genomics	O
ph	O
d	O
thesis	O
ubc	O
lo	O
k	O
f	O
hahne	O
r	O
brinkman	O
r	O
ryan	O
and	O
r	O
gottardo	O
may	O
flowclust	O
a	O
bioconductor	O
package	O
for	O
automated	O
gating	O
of	O
flow	O
cytometry	O
data	O
bmc	O
bioinformatics	O
lopes	O
h	O
and	O
m	O
west	O
bayesian	B
model	O
assessment	O
in	O
factor	B
analysis	I
statisica	O
sinica	O
lowe	O
d	O
g	O
object	O
recognition	O
from	O
local	O
scale-invariant	O
features	B
in	O
proc	O
of	O
the	O
international	O
conference	O
on	O
computer	O
vision	O
iccv	O
corfu	O
pp	O
luce	O
r	O
individual	O
choice	O
behavior	O
a	O
theoretical	O
analysis	O
wiley	O
lunn	O
d	O
n	O
best	O
and	O
j	O
whittaker	O
generic	O
reversible	B
jump	I
mcmc	B
using	O
graphical	B
models	I
statistics	O
and	O
computing	O
lunn	O
d	O
a	O
thomas	O
n	O
best	O
and	O
d	O
spiegelhalter	O
winbugs	O
a	O
bayesian	B
modelling	O
framework	O
concepts	O
structure	O
and	O
extensibility	O
statistics	O
and	O
computing	O
ma	O
h	O
h	O
yang	O
m	O
lyu	O
and	O
i	O
king	O
sorec	O
social	O
recommendation	O
using	O
probabilistic	B
matrix	B
factorization	I
in	O
proc	O
of	O
conf	O
on	O
information	B
and	O
knowledge	O
management	O
ma	O
s	O
c	O
ji	O
and	O
j	O
farmer	O
an	O
efficient	O
em-based	O
training	O
algorithm	O
for	O
feedforward	O
neural	B
networks	I
neural	B
networks	I
maathuis	O
m	O
d	O
colombo	O
m	O
kalisch	O
and	O
p	O
b	O
ijhlmann	O
predicting	O
causal	O
effects	O
in	O
large-scale	O
systems	O
from	O
observational	O
data	O
nature	O
methods	O
maathuis	O
m	O
m	O
kalisch	O
and	O
p	O
b	O
ijhlmann	O
estimating	O
high-dimensional	O
intervention	O
effects	O
from	O
observational	O
data	O
annals	O
of	O
statistics	O
mackay	O
d	O
bayesian	B
interpolation	O
neural	O
computation	O
mackay	O
d	O
developments	O
in	O
probabilistic	O
modeling	O
with	O
neural	B
networks	I
ensemble	B
learning	B
in	O
proc	O
ann	O
symp	O
neural	B
networks	I
mackay	O
d	O
probable	O
networks	O
and	O
plausible	O
predictions	O
a	O
review	O
of	O
practical	O
bayesian	B
methods	O
for	O
supervised	O
neural	B
networks	I
network	O
mackay	O
d	O
ensemble	B
learning	B
for	O
hidden	B
markov	B
models	I
technical	O
report	O
u	O
cambridge	O
mackay	O
d	O
comparision	O
of	O
approximate	O
methods	O
for	O
handling	O
hyperparameters	O
neural	O
computation	O
mackay	O
d	O
information	B
theory	I
inference	B
and	O
learning	B
algorithms	O
cambridge	O
university	O
press	O
macnaughton-smith	O
p	O
w	O
t	O
williams	O
m	O
b	O
dale	O
and	O
g	O
mockett	O
dissimilarity	B
analysis	I
a	O
new	O
technique	O
of	O
hierarchical	O
sub-division	O
nature	O
bibliography	O
madeira	O
s	O
c	O
and	O
a	O
l	O
oliveira	O
biclustering	B
algorithms	O
for	O
biological	O
data	O
analysis	O
a	O
survey	O
ieeeacm	O
transactions	O
on	O
computational	O
biology	O
and	O
bioinformatics	O
madigan	O
d	O
and	O
a	O
raftery	O
model	B
selection	I
and	O
accounting	O
for	O
model	O
uncertainty	B
in	O
graphical	B
models	I
using	O
occam	O
s	O
window	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
madsen	O
r	O
d	O
kauchak	O
and	O
c	O
elkan	O
modeling	O
word	O
burstiness	B
using	O
the	O
dirichlet	B
distribution	I
in	O
intl	O
conf	O
on	O
machine	B
learning	B
mairal	O
j	O
f	O
bach	O
j	O
ponce	O
and	O
g	O
sapiro	O
online	B
learning	B
for	O
matrix	B
factorization	I
and	O
sparse	B
coding	I
j	O
of	O
machine	B
learning	B
research	O
mairal	O
j	O
m	O
elad	O
and	O
g	O
sapiro	O
sparse	B
representation	I
for	O
color	O
image	O
restoration	O
ieee	O
trans	O
on	O
image	O
processing	O
malioutov	O
d	O
j	O
johnson	O
and	O
a	O
willsky	O
walk-sums	O
and	O
belief	B
propagation	I
in	O
gaussian	B
graphical	B
models	I
j	O
of	O
machine	B
learning	B
research	O
july	O
mallat	O
s	O
g	O
davis	O
and	O
z	O
zhang	O
adaptive	O
frequency	O
decompositions	O
spie	O
journal	O
of	O
optical	O
engineering	O
mallat	O
s	O
and	O
z	O
zhang	O
matching	B
pursuits	I
with	O
time-frequency	O
dictionaries	O
ieee	O
transactions	O
on	O
signal	B
processing	I
malouf	O
r	O
a	O
comparison	O
of	O
algorithms	O
for	O
maximum	B
entropy	B
parameter	B
estimation	O
in	O
proc	O
sixth	O
conference	O
on	O
natural	O
language	O
learning	B
pp	O
manning	O
c	O
p	O
raghavan	O
and	O
h	O
schuetze	O
introduction	O
to	O
information	B
retrieval	I
cambridge	O
university	O
press	O
manning	O
c	O
and	O
h	O
schuetze	O
statistical	O
natural	O
foundations	O
of	O
language	O
processing	O
mit	O
press	O
mansinghka	O
v	O
d	O
roy	O
r	O
rifkin	O
and	O
j	O
tenenbaum	O
aclass	O
an	O
online	O
algorithm	O
for	O
generative	O
classification	O
in	O
aistatistics	O
mansinghka	O
v	O
p	O
shafto	O
e	O
cross-categorization	O
jonas	O
c	O
petschulat	O
and	O
j	O
tenenbaum	O
a	O
nonparametric	O
bayesian	B
method	O
for	O
modeling	O
heterogeneous	O
high	O
dimensional	O
data	O
technical	O
report	O
mit	O
margolin	O
a	O
i	O
nemenman	O
k	O
basso	O
c	O
wiggins	O
g	O
stolovitzky	O
and	O
r	O
f	O
abd	O
a	O
califano	O
aracne	O
an	O
algorithm	O
for	O
the	O
reconstruction	O
of	O
gene	O
regulatory	O
networks	O
in	O
a	O
mammalian	O
cellular	O
context	O
bmc	O
bionformatics	O
marin	O
j	O
-m	O
and	O
c	O
robert	O
bayesian	B
core	O
a	O
practical	O
approach	O
to	O
computational	O
bayesian	B
statistics	I
springer	O
marks	O
t	O
k	O
and	O
j	O
r	O
movellan	O
diffusion	O
networks	O
products	O
of	O
experts	O
and	O
factor	B
analysis	I
technical	O
report	O
university	O
of	O
california	O
san	O
diego	O
marlin	O
b	O
modeling	O
user	O
rating	O
profiles	O
for	O
collaborative	O
filtering	B
in	O
nips	O
marlin	O
b	O
missing	B
data	I
problems	O
in	O
machine	B
learning	B
ph	O
d	O
thesis	O
u	O
toronto	O
marlin	O
b	O
e	O
khan	O
and	O
k	O
murphy	O
piecewise	O
bounds	O
for	O
estimating	O
bernoulli-logistic	O
latent	B
gaussian	B
models	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
marlin	O
b	O
and	O
r	O
zemel	O
collaborative	O
prediction	O
and	O
ranking	B
with	O
non-random	O
missing	B
data	I
in	O
proc	O
of	O
the	O
acm	O
conference	O
on	O
recommender	O
systems	O
marlin	O
b	O
m	O
k	O
swersky	O
b	O
chen	O
and	O
n	O
de	O
freitas	O
inductive	O
principles	O
for	O
restricted	B
boltzmann	B
machine	B
learning	B
in	O
aistatistics	O
marroquin	O
j	O
s	O
mitter	O
and	O
t	O
poggio	O
probabilistic	O
solution	O
of	O
ill-posed	O
problems	O
in	O
computational	O
vision	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
martens	O
j	O
deep	B
learning	B
via	O
in	O
intl	O
hessian-free	O
optimization	B
conf	O
on	O
machine	B
learning	B
maruyama	O
y	O
and	O
e	O
george	O
a	O
g-prior	B
extension	B
for	O
p	O
n	O
technical	O
report	O
u	O
tokyo	O
mason	O
l	O
j	O
baxter	O
p	O
bartlett	O
and	O
boosting	B
algoin	O
m	O
frean	O
rithms	O
as	O
gradient	B
descent	I
nips	O
volume	O
pp	O
matthews	O
r	O
bayesian	B
critique	O
of	O
statistics	O
in	O
health	O
the	O
great	O
health	O
hoax	O
maybeck	O
p	O
stochastic	O
models	O
estimation	O
and	O
control	O
academic	O
press	O
mazumder	O
r	O
and	O
t	O
hastie	O
the	O
graphical	B
lasso	B
new	O
insights	O
and	O
alternatives	O
technical	O
report	O
mcauliffe	O
j	O
d	O
blei	O
and	O
m	O
jordan	O
nonparametric	O
empirical	B
bayes	I
for	O
the	O
dirichlet	B
process	I
mixture	B
model	I
statistics	O
and	O
computing	O
mccallum	O
a	O
efficiently	O
inducing	O
features	B
of	O
conditional	O
random	O
fields	O
in	O
uai	O
mccallum	O
freitag	O
a	O
d	O
and	O
f	O
pereira	O
maximum	B
entropy	B
markov	B
models	I
for	O
information	B
extraction	I
and	O
segmentation	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
mccallum	O
a	O
and	O
k	O
nigam	O
a	O
comparison	O
of	O
event	O
models	O
for	O
naive	O
bayes	O
text	O
classification	O
in	O
aaaiicml	O
workshop	O
on	O
learning	B
for	O
text	O
categorization	O
mccray	O
a	O
an	O
upper	O
level	O
ontology	B
for	O
the	O
biomedical	O
domain	O
comparative	O
and	O
functional	O
genomics	O
mccullagh	O
p	O
and	O
j	O
nelder	O
generalized	B
linear	I
models	I
chapman	O
and	O
hall	O
edition	O
mccullich	O
w	O
and	O
w	O
pitts	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
bulletin	O
of	O
mathematical	O
biophysics	O
mcdonald	O
j	O
and	O
w	O
newey	O
partially	O
adaptive	O
estimation	O
of	O
regression	B
models	O
via	O
the	O
generalized	B
t	I
distribution	I
econometric	O
theory	O
mceliece	O
r	O
j	O
d	O
j	O
c	O
mackay	O
and	O
j	O
f	O
cheng	O
turbo	O
decoding	B
as	O
an	O
instance	O
of	O
pearl	O
s	O
belief	B
propagation	I
algorithm	O
ieee	O
j	O
on	O
selected	O
areas	O
in	O
comm	O
mcfadden	O
d	O
conditional	O
logit	B
analysis	O
of	O
qualitative	O
choice	O
behavior	O
in	O
p	O
zarembka	O
frontiers	O
in	O
econometrics	O
pp	O
academic	O
press	O
bibliography	O
mcgrayne	O
s	O
b	O
the	O
theory	O
that	O
would	O
not	O
die	O
how	O
bayes	B
rule	I
cracked	O
the	O
enigma	O
code	O
hunted	O
down	O
russian	O
submarines	O
and	O
emerged	O
triumphant	O
from	O
two	O
centuries	O
of	O
controversy	O
yale	O
university	O
press	O
mckay	O
b	O
d	O
f	O
e	O
oggier	O
g	O
f	O
royle	O
n	O
j	O
a	O
sloane	O
i	O
m	O
wanless	O
and	O
h	O
s	O
wilf	O
acyclic	O
digraphs	O
and	O
eigenvalues	O
of	O
j	O
integer	O
sequences	O
mckay	O
d	O
and	O
l	O
c	O
b	O
peto	O
language	O
a	O
hierarchical	O
dirichlet	B
model	O
natural	O
language	O
engineering	O
mclachlan	O
g	O
j	O
and	O
t	O
krishnan	O
the	O
em	B
algorithm	O
and	O
extensions	O
wiley	O
meek	O
c	O
and	O
d	O
heckerman	O
learnindependence	O
and	O
in	O
uai	O
structure	O
and	O
parameter	B
ing	O
for	O
causal	O
causal	O
interaction	O
models	O
pp	O
meek	O
c	O
b	O
thiesson	O
and	O
d	O
heckerman	O
staged	O
mixture	B
modelling	O
and	O
boosting	B
in	O
uai	O
san	O
francisco	O
ca	O
pp	O
morgan	O
kaufmann	O
meila	O
m	O
a	O
random	O
walks	O
view	O
in	O
ais	O
of	O
spectral	B
segmentation	O
tatistics	O
meila	O
m	O
comparing	O
clusterin	O
intl	O
ings	O
an	O
axiomatic	O
view	O
conf	O
on	O
machine	B
learning	B
meila	O
m	O
and	O
t	O
jaakkola	O
tractable	O
bayesian	B
learning	B
of	O
tree	B
belief	B
networks	I
statistics	O
and	O
computing	O
i	O
meila	O
m	O
and	O
m	O
jordan	O
learning	B
with	O
mixtures	O
of	O
trees	O
j	O
of	O
machine	B
learning	B
research	O
meinshausen	O
n	O
a	O
note	O
on	O
the	O
lasso	B
for	O
gaussian	B
graphical	B
model	B
selection	I
technical	O
report	O
eth	O
seminar	O
fur	O
statistik	O
meinshausen	O
n	O
and	O
p	O
buhlmann	O
high	O
dimensional	O
graphs	O
and	O
variable	O
selection	O
with	O
the	O
lasso	B
the	O
annals	O
of	O
statistics	O
globally	O
optimal	O
meltzer	O
t	O
c	O
yanover	O
and	O
y	O
weiss	O
solutions	O
for	O
energy	O
minimization	O
in	O
stereo	O
vision	O
using	O
reweighted	O
belief	B
propagation	I
in	O
iccv	O
pp	O
meng	O
x	O
l	O
and	O
d	O
van	O
dyk	O
the	O
em	B
algorithm	O
an	O
old	O
folk	O
song	O
sung	O
to	O
a	O
fast	O
new	O
tune	O
discussion	O
j	O
royal	O
stat	O
soc	O
b	O
mesot	O
b	O
and	O
d	O
barber	O
a	O
simple	O
alternative	O
derivation	O
of	O
the	O
expectation	B
correction	I
algorithm	O
ieee	O
signal	B
processing	I
letters	O
metropolis	O
a	O
n	O
rosenbluth	O
m	O
rosenbluth	O
a	O
teller	O
and	O
e	O
teller	O
equation	O
of	O
state	B
calculations	O
by	O
fast	O
computing	O
machines	O
j	O
of	O
chemical	O
physics	O
metz	O
c	O
google	O
behavioral	O
ad	O
targeter	O
is	O
a	O
smart	O
ass	O
the	O
register	O
miller	O
a	O
subset	O
selection	O
in	O
regression	B
chapman	O
and	O
hall	O
edition	O
mimno	O
d	O
and	O
a	O
mccallum	O
topic	B
models	O
conditioned	O
on	O
arbitrary	O
features	B
with	O
dirichletmultinomial	O
regression	B
in	O
uai	O
minka	O
t	O
pathologies	B
of	O
orthodox	O
statisics	O
technical	O
report	O
mit	O
media	O
lab	O
minka	O
t	O
automatical	O
choice	O
of	O
dimensionality	O
for	O
pca	B
technical	O
report	O
mit	O
minka	O
t	O
bayesian	B
linear	O
re	O
gression	O
technical	O
report	O
mit	O
minka	O
t	O
bayesian	B
model	O
averaging	O
is	O
not	O
model	O
combination	O
technical	O
report	O
mit	O
media	O
lab	O
minka	O
t	O
empirical	B
risk	B
minimization	I
is	O
an	O
incomplete	O
inductive	O
principle	O
technical	O
report	O
mit	O
minka	O
t	O
estimating	O
a	O
dirichlet	B
distribution	I
technical	O
report	O
mit	O
minka	O
t	O
inferring	O
a	O
gaussian	B
distribution	O
technical	O
report	O
mit	O
meinshausen	O
n	O
and	O
p	O
b	O
ijhlmann	O
stability	B
selection	I
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
minka	O
t	O
bayesian	B
inference	B
of	O
a	O
uniform	B
distribution	I
technical	O
report	O
mit	O
minka	O
t	O
empirical	B
risk	B
minimization	I
is	O
an	O
incomplete	O
inductive	O
principle	O
technical	O
report	O
mit	O
minka	O
t	O
expectation	B
propagation	I
for	O
approximate	O
bayesian	B
inference	B
in	O
uai	O
minka	O
t	O
a	O
family	B
of	O
algorithms	O
for	O
approximate	O
bayesian	B
inference	B
ph	O
d	O
thesis	O
mit	O
minka	O
t	O
statistical	O
approaches	O
to	O
learning	B
and	O
discovery	O
homework	O
assignment	O
question	O
technical	O
report	O
cmu	O
minka	O
t	O
a	O
comparison	O
of	O
numerical	O
optimizers	O
for	O
logistic	B
regression	B
technical	O
report	O
msr	O
minka	O
t	O
divergence	O
measures	O
and	O
message	B
passing	I
technical	O
report	O
msr	O
cambridge	O
minka	O
t	O
and	O
y	O
qi	O
treestructured	O
approximations	O
by	O
expectation	B
propagation	I
in	O
nips	O
minka	O
t	O
j	O
winn	O
d	O
knowles	O
microsoft	B
httpresearch	O
microsoft	B
cominfernet	O
j	O
guiver	O
and	O
infer	O
net	O
research	O
cambridge	O
minsky	O
m	O
and	O
s	O
papert	O
per	O
ceptrons	O
mit	O
press	O
mitchell	O
t	O
machine	B
learning	B
mcgraw	O
hill	O
mitchell	O
t	O
and	O
j	O
beauchamp	O
bayesian	B
variable	O
selection	O
in	O
linear	B
regression	B
j	O
of	O
the	O
am	O
stat	O
assoc	O
mobahi	O
h	O
r	O
collobert	O
and	O
j	O
weston	O
deep	B
learning	B
from	O
temporal	O
coherence	O
in	O
video	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
mockus	O
j	O
w	O
eddy	O
a	O
mockus	O
l	O
mockus	O
and	O
g	O
reklaitis	O
bayesian	B
heuristic	O
approach	O
to	O
discrete	B
and	O
global	O
optimization	B
algorithms	O
visualization	O
software	O
and	O
applications	O
kluwer	O
moghaddam	O
b	O
a	O
gruber	O
y	O
weiss	O
and	O
s	O
avidan	O
sparse	B
regression	B
as	O
a	O
sparse	B
eigenvalue	O
problem	O
in	O
information	B
theory	I
applications	O
workshop	O
moghaddam	O
b	O
b	O
marlin	O
e	O
khan	O
and	O
k	O
murphy	O
accelerating	O
bayesian	B
structural	O
inference	B
for	O
non-decomposable	O
gaussian	B
graphical	B
models	I
in	O
nips	O
bibliography	O
moghaddam	O
b	O
and	O
a	O
pentland	O
probabilistic	O
visual	O
learning	B
for	O
object	B
detection	I
in	O
intl	O
conf	O
on	O
computer	O
vision	O
mohamed	O
s	O
and	O
z	O
ghahramani	O
bayesian	B
exponential	B
family	B
pca	B
in	O
nips	O
k	O
heller	O
moler	O
c	O
numerical	O
computing	O
with	O
matlab	O
siam	O
morris	O
r	O
d	O
x	O
descombes	O
and	O
j	O
zerubia	O
the	O
isingpotts	O
model	O
is	O
not	O
well	O
suited	O
to	O
segmentation	O
tasks	O
in	O
ieee	O
dsp	O
workshop	O
mosterman	O
p	O
j	O
and	O
g	O
biswas	O
diagnosis	O
of	O
continuous	O
valued	O
systems	O
in	O
transient	B
operating	O
regions	O
ieee	O
trans	O
on	O
systems	O
man	O
and	O
cybernetics	O
part	O
a	O
moulines	O
e	O
j	O
-f	O
cardoso	O
and	O
e	O
gassiat	O
maximum	O
likelihood	B
for	O
blind	O
separation	O
and	O
deconvolution	O
of	O
noisy	O
signals	O
using	O
mixture	B
models	O
ieee	O
int	O
conf	O
on	O
acoustics	O
speech	O
and	O
signal	B
processing	I
munich	O
germany	O
pp	O
in	O
proc	O
muller	O
p	O
g	O
parmigiani	O
c	O
robert	O
and	O
j	O
rousseau	O
optimal	O
sample	O
size	O
for	O
multiple	O
testing	O
the	O
case	O
of	O
gene	O
expression	O
microarrays	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
mumford	O
d	O
neuronal	O
architectures	O
for	O
pattern-theoretic	O
problems	O
in	O
c	O
koch	O
and	O
j	O
davis	O
large	O
scale	O
neuronal	O
theories	O
of	O
the	O
brain	O
mit	O
press	O
murphy	O
k	O
bayesian	B
map	O
learning	B
in	O
dynamic	O
environments	O
in	O
nips	O
volume	O
murphy	O
k	O
and	O
m	O
paskin	O
linear	O
time	O
inference	B
in	O
hierarchical	O
hmms	B
in	O
nips	O
murphy	O
k	O
y	O
weiss	O
and	O
m	O
jordan	O
loopy	B
belief	B
propagation	I
for	O
approximate	B
inference	B
an	O
empirical	O
study	O
in	O
uai	O
murphy	O
k	O
p	O
filtering	B
and	O
smoothing	B
in	O
linear	O
dynamical	O
systems	O
using	O
the	O
junction	B
tree	B
algorithm	I
technical	O
report	O
u	O
c	O
berkeley	O
dept	O
comp	O
sci	O
murray	O
i	O
and	O
z	O
ghahramani	O
a	O
note	O
on	O
the	O
evidence	B
and	O
bayesian	B
occam	O
s	O
razor	O
technical	O
report	O
gatsby	O
musso	O
c	O
n	O
oudjane	O
and	O
f	O
legland	O
improving	O
regularized	O
ticle	O
filters	O
in	O
a	O
doucet	O
j	O
f	O
g	O
de	O
freitas	O
and	O
n	O
gordon	O
sequential	B
monte	B
carlo	I
methods	O
in	O
practice	O
springer	O
nabney	O
i	O
netlab	O
algorithms	O
for	O
pattern	B
recognition	I
springer	O
neal	O
r	O
connectionist	O
learning	B
of	O
belief	B
networks	I
artificial	O
intelligence	O
neal	O
r	O
probabilistic	B
inference	B
using	O
markov	B
chain	I
monte	B
carlo	I
methods	O
technical	O
report	O
univ	O
toronto	O
neal	O
r	O
bayesian	B
learning	B
for	O
neural	B
networks	I
springer	O
neal	O
r	O
monte	B
carlo	I
implementation	O
of	O
gaussian	B
process	I
models	O
for	O
bayesian	B
regression	B
and	O
classification	O
technical	O
report	O
u	O
toronto	O
neal	O
r	O
erroneous	O
results	O
in	O
marginal	B
likelihood	B
from	O
the	O
gibbs	O
output	O
technical	O
report	O
u	O
toronto	O
neal	O
r	O
markov	B
chain	I
sampling	O
methods	O
for	O
dirichlet	B
process	I
mixture	B
models	I
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
neal	O
r	O
slice	B
sampling	I
an	O
nals	O
of	O
statistics	O
neal	O
r	O
mcmc	B
using	O
hamilin	O
s	O
brooks	O
tonian	O
dynamics	O
a	O
gelman	O
g	O
jones	O
and	O
x	O
-l	O
meng	O
handbook	O
of	O
markov	B
chain	I
monte	B
carlo	I
chapman	O
hall	O
neal	O
r	O
and	O
d	O
mackay	O
likelihood-based	O
boosting	B
technical	O
report	O
u	O
toronto	O
neal	O
r	O
and	O
j	O
zhang	O
high	O
dimensional	O
classification	O
bayesian	B
neural	B
networks	I
and	O
dirichlet	B
diffusion	O
trees	O
in	O
i	O
guyon	O
s	O
gunn	O
m	O
nikravesh	O
and	O
l	O
zadeh	O
feature	B
extraction	I
springer	O
neal	O
r	O
m	O
annealed	B
importance	B
sampling	I
statistics	O
and	O
computing	O
neal	O
r	O
m	O
density	O
modeling	O
and	O
clustering	B
using	O
dirichlet	B
diffusion	O
trees	O
in	O
j	O
m	O
bernardo	O
et	O
al	O
bayesian	B
statistics	I
pp	O
oxford	O
university	O
press	O
neal	O
r	O
m	O
and	O
g	O
e	O
hinton	O
a	O
new	O
view	O
of	O
the	O
em	B
algorithm	O
that	O
justifies	O
incremental	O
and	O
other	O
variants	O
in	O
m	O
jordan	O
learning	B
in	O
graphical	B
models	I
mit	O
press	O
neapolitan	O
r	O
bayesian	B
networks	I
prentice	O
hall	O
learning	B
nefian	O
a	O
l	O
liang	O
x	O
pi	O
x	O
liu	O
and	O
k	O
murphy	O
dynamic	O
for	O
audiobayesian	O
networks	O
visual	O
speech	B
recognition	I
j	O
applied	O
signal	B
processing	I
nemirovski	O
a	O
and	O
d	O
yudin	O
on	O
cezari	O
s	O
convergence	O
of	O
the	O
steepest	B
descent	I
method	O
for	O
approximating	O
saddle	O
points	O
of	O
convexconcave	O
functions	O
soviet	O
math	O
dokl	O
nesterov	O
y	O
introductory	O
lectures	O
on	O
convex	B
optimization	B
a	O
basic	O
course	O
kluwer	O
newton	O
m	O
d	O
noueiry	O
d	O
sarkar	O
and	O
p	O
ahlquist	O
detecting	O
differential	O
gene	O
expression	O
with	O
a	O
semiparametric	O
hierarchical	O
mixture	B
method	O
biostatistics	O
newton	O
m	O
and	O
a	O
raftery	O
approximate	O
bayesian	B
inference	B
with	O
the	O
weighted	O
likelihood	B
bootstrap	B
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
ng	O
a	O
m	O
jordan	O
and	O
y	O
weiss	O
on	O
spectral	B
clustering	B
analysis	O
and	O
an	O
algorithm	O
in	O
nips	O
ng	O
a	O
y	O
and	O
m	O
i	O
jordan	O
on	O
discriminative	B
vs	O
generative	O
classifiers	O
a	O
comparison	O
of	O
logistic	B
regression	B
and	O
naive	O
bayes	O
in	O
nickisch	O
h	O
and	O
c	O
rasmussen	O
approximations	O
for	O
binary	O
gaussian	B
process	I
classification	O
j	O
of	O
machine	B
learning	B
research	O
nilsson	O
d	O
an	O
efficient	O
algorithm	O
for	O
finding	O
the	O
m	O
most	O
probable	O
configurations	O
in	O
a	O
probabilistic	B
expert	I
system	I
statistics	O
and	O
computing	O
nilsson	O
d	O
and	O
j	O
goldberger	O
sequentially	O
finding	O
the	O
n-best	B
list	I
in	O
hidden	B
markov	B
models	I
in	O
intl	O
joint	O
conf	O
on	O
ai	B
pp	O
nocedal	O
j	O
and	O
s	O
wright	O
nu	O
merical	O
optimization	B
springer	O
bibliography	O
nowicki	O
k	O
and	O
t	O
a	O
b	O
snijders	O
estimation	O
and	O
prediction	O
for	O
stochastic	O
blockstructures	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
nowlan	O
s	O
and	O
g	O
hinton	O
simplifying	O
neural	B
networks	I
by	O
soft	B
weight	I
sharing	I
neural	O
computation	O
nummiaro	O
k	O
e	O
koller-meier	O
and	O
an	O
adaptive	O
l	O
v	O
gool	O
color-based	O
particle	O
filter	O
image	O
and	O
vision	O
computing	O
obozinski	O
g	O
b	O
taskar	O
and	O
m	O
i	O
jordan	O
joint	O
covariate	O
selection	O
for	O
grouped	O
classification	O
technical	O
report	O
uc	O
berkeley	O
oh	O
m	O
-s	O
and	O
j	O
berger	O
adaptive	B
importance	B
sampling	I
in	O
monte	B
carlo	I
integration	I
j	O
of	O
statistical	O
computation	O
and	O
simulation	O
oh	O
s	O
s	O
russell	O
and	O
s	O
sastry	O
markov	B
chain	I
monte	B
carlo	I
data	B
association	I
for	O
multi-target	B
tracking	B
ieee	O
trans	O
on	O
automatic	O
control	O
o	O
hagan	O
a	O
curve	O
fitting	O
and	O
optimal	O
design	O
for	O
prediction	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
o	O
hara	O
r	O
and	O
m	O
sillanpaa	O
a	O
review	O
of	O
bayesian	B
variable	O
selection	O
methods	O
what	O
how	O
and	O
which	O
bayesian	B
analysis	O
olshausen	O
b	O
a	O
and	O
d	O
j	O
field	O
emergence	O
of	O
simple	O
cell	O
receptive	O
field	O
properties	O
by	O
learning	B
a	O
sparse	B
code	O
for	O
natural	O
images	O
nature	O
opper	O
m	O
a	O
bayesian	B
approach	O
to	O
online	B
learning	B
in	O
d	O
saad	O
on-line	O
learning	B
in	O
neural	B
networks	I
cambridge	O
opper	O
m	O
and	O
c	O
archambeau	O
the	O
variational	O
gaussian	B
approximation	I
revisited	O
neural	O
computation	O
opper	O
m	O
and	O
d	O
saad	O
advanced	O
mean	B
field	O
methods	O
theory	O
and	O
practice	O
mit	O
press	O
osborne	O
m	O
r	O
b	O
presnell	O
and	O
b	O
a	O
turlach	O
a	O
new	O
approach	O
to	O
variable	O
selection	O
in	O
least	B
squares	I
problems	O
ima	O
journal	O
of	O
numerical	O
analysis	O
osborne	O
m	O
r	O
b	O
presnell	O
and	O
b	O
a	O
turlach	O
on	O
the	O
lasso	B
and	O
its	O
dual	O
j	O
computational	O
and	O
graphical	O
statistics	O
ostendorf	O
m	O
v	O
digalakis	O
and	O
o	O
kimball	O
from	O
hmms	B
to	O
segment	O
models	O
a	O
unified	O
view	O
of	O
stochastic	O
modeling	O
for	O
speech	B
recognition	I
ieee	O
trans	O
on	O
speech	O
and	O
audio	O
processing	O
overschee	O
p	O
v	O
and	O
b	O
d	O
moor	O
subspace	O
identification	O
for	O
linear	O
systems	O
theory	O
implementation	O
applications	O
kluwer	O
academic	O
publishers	O
paatero	O
p	O
and	O
u	O
tapper	O
positive	O
matrix	B
factorization	I
a	O
nonnegative	O
factor	B
model	O
with	O
optimal	O
utilization	O
of	O
error	O
estimates	O
of	O
data	O
values	O
environmetrics	O
padadimitriou	O
c	O
and	O
k	O
steiglitz	O
combinatorial	O
optimization	B
algorithms	O
and	O
complexity	O
prentice	O
hall	O
paisley	O
j	O
and	O
l	O
carin	O
nonparametric	O
factor	B
analysis	I
with	O
beta	B
process	I
priors	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
palmer	O
s	O
vision	O
science	O
photons	O
to	O
phenomenology	O
mit	O
press	O
parise	O
s	O
and	O
m	O
welling	O
random	O
learning	B
fields	O
an	O
empirical	O
study	O
in	O
joint	O
statistical	O
meeting	O
in	O
markov	B
park	O
t	O
and	O
g	O
casella	O
the	O
j	O
of	O
the	O
am	O
stat	O
bayesian	B
lasso	B
assoc	O
parviainen	O
p	O
and	O
m	O
koivisto	O
ancestor	O
relations	O
in	O
the	O
presence	O
of	O
unobserved	O
variables	O
in	O
proc	O
european	O
conf	O
on	O
machine	B
learning	B
paskin	O
m	O
thin	O
junction	B
tree	B
filters	O
for	O
simultaneous	B
localization	I
and	I
mapping	I
in	O
intl	O
joint	O
conf	O
on	O
ai	B
pearl	O
j	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
networks	O
of	O
plausible	O
inference	B
morgan	O
kaufmann	O
pearl	O
j	O
and	O
t	O
verma	O
a	O
theory	O
of	O
inferred	O
causation	O
in	O
knowledge	O
representation	O
pp	O
pe	O
er	O
d	O
april	O
bayesian	B
network	O
analysis	O
of	O
signaling	O
networks	O
a	O
primer	O
science	O
stke	O
peng	O
f	O
r	O
jacobs	O
and	O
m	O
tanner	O
bayesian	B
inference	B
in	O
mixtures-of-experts	O
and	O
hierarchical	O
mixtures-of-experts	O
models	O
with	O
an	O
application	O
to	O
speech	B
recognition	I
j	O
of	O
the	O
am	O
stat	O
assoc	O
petris	O
g	O
s	O
petrone	O
and	O
p	O
campagnoli	O
dynamic	O
linear	O
models	O
with	O
r	O
springer	O
pham	O
d	O
-t	O
and	O
p	O
garrat	O
blind	O
separation	O
of	O
mixture	B
of	O
independent	O
sources	O
through	O
a	O
quasiapproach	O
maximum	O
likelihood	B
ieee	O
trans	O
on	O
signal	B
processing	I
pietra	O
s	O
d	O
v	O
d	O
pietra	O
and	O
j	O
lafferty	O
inducing	O
features	B
of	O
random	O
fields	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
plackett	O
r	O
the	O
analysis	O
of	O
permutations	O
applied	O
stat	O
platt	O
j	O
using	O
analytic	O
qp	B
and	O
sparseness	O
to	O
speed	O
training	O
of	O
support	B
vector	I
machines	I
in	O
nips	O
platt	O
j	O
probabilities	O
for	O
sv	O
main	O
a	O
smola	O
p	O
bartlett	O
chines	O
b	O
schoelkopf	O
and	O
d	O
schuurmans	O
advances	O
in	O
large	O
margin	B
classifiers	O
mit	O
press	O
platt	O
j	O
n	O
cristianini	O
and	O
j	O
shawetaylor	O
large	O
margin	B
dags	O
for	O
multiclass	O
classification	O
in	O
nips	O
volume	O
pp	O
plummer	O
m	O
jags	B
a	O
program	O
for	O
analysis	O
of	O
bayesian	B
graphical	B
models	I
using	O
gibbs	B
sampling	I
in	O
proc	O
intl	O
workshop	O
on	O
distributed	O
statistical	O
computing	O
polson	O
n	O
and	O
s	O
scott	O
data	B
augmentation	I
for	O
support	B
vector	I
machines	I
bayesian	B
analysis	O
pontil	O
m	O
s	O
mukherjee	O
and	O
f	O
girosi	O
on	O
the	O
noise	O
model	O
of	O
support	B
vector	I
machine	I
regression	B
technical	O
report	O
mit	O
ai	B
lab	O
pearl	O
j	O
causality	B
models	O
reasoning	O
and	O
inference	B
cambridge	O
univ	O
press	O
poon	O
h	O
and	O
p	O
domingos	O
sumproduct	O
networks	O
a	O
new	O
deep	B
architecture	O
in	O
uai	O
bibliography	O
pourahmadi	O
m	O
simultaneous	O
modelling	O
of	O
covariance	B
matrices	O
glm	B
bayesian	B
and	O
nonparametric	O
perspectives	O
technical	O
report	O
northern	O
illinois	O
university	O
prado	O
r	O
and	O
m	O
west	O
time	O
series	O
modelling	O
computation	O
and	O
inference	B
crc	O
press	O
press	O
s	O
j	O
applied	O
multivariate	O
analysis	O
using	O
bayesian	B
and	O
frequentist	B
methods	O
of	O
inference	B
dover	O
second	O
edition	O
press	O
w	O
w	O
vetterling	O
s	O
teukolosky	O
and	O
b	O
flannery	O
numerical	O
recipes	O
in	O
c	O
the	O
art	O
of	O
scientific	O
computing	O
ed	O
cambridge	O
university	O
press	O
prince	O
s	O
computer	O
vision	O
models	O
learning	B
and	O
inference	B
cambridge	O
pritchard	O
j	O
m	O
m	O
stephens	O
and	O
p	O
donnelly	O
inference	B
of	O
population	O
structure	O
using	O
multilocus	O
genotype	B
data	O
genetics	O
qi	O
y	O
and	O
t	O
jaakkola	O
parameter	B
expanded	O
variational	O
bayesian	B
methods	O
in	O
nips	O
qi	O
y	O
m	O
szummer	O
and	O
t	O
minka	O
bayesian	B
conditional	O
random	O
fields	O
in	O
intl	O
workshop	O
on	O
aistatistics	O
quinlan	O
j	O
learning	B
logical	O
definitions	O
from	O
relations	O
machine	B
learning	B
quinlan	O
j	O
r	O
induction	B
of	O
decision	B
trees	I
machine	B
learning	B
quinlan	O
j	O
r	O
programs	O
for	O
machine	B
learning	B
morgan	O
kauffman	O
quinonero-candela	O
j	O
c	O
rasmussen	O
and	O
c	O
williams	O
approximation	O
methods	O
for	O
gaussian	B
process	I
regression	B
in	O
l	O
bottou	O
o	O
chapelle	O
d	O
decoste	O
and	O
j	O
weston	O
large	O
scale	O
kernel	B
machines	O
pp	O
mit	O
press	O
rabiner	O
l	O
r	O
a	O
tutorial	O
on	O
hidden	B
markov	B
models	I
and	O
selected	O
applications	O
in	O
speech	B
recognition	I
proc	O
of	O
the	O
ieee	O
rai	O
p	O
and	O
h	O
daume	O
multilabel	O
prediction	O
via	O
sparse	B
infinite	O
cca	B
in	O
nips	O
raiffa	O
h	O
decision	B
analysis	O
ad	O
dison	O
wesley	O
raina	O
r	O
a	O
madhavan	O
and	O
a	O
ng	O
large-scale	O
deep	B
unsupervised	B
learning	B
using	O
graphics	O
processors	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
raina	O
r	O
a	O
ng	O
and	O
d	O
koller	O
transfer	B
learning	B
by	O
constructing	O
informative	O
priors	O
in	O
nips	O
rajaraman	O
a	O
and	O
j	O
ullman	O
self	O
mining	O
of	O
massive	O
datasets	O
published	O
rajaraman	O
a	O
and	O
j	O
ullman	O
mining	O
of	O
massive	O
datasets	O
cambridge	O
rakotomamonjy	O
a	O
f	O
bach	O
s	O
canu	O
and	O
y	O
grandvalet	O
simplemkl	O
j	O
of	O
machine	B
learning	B
research	O
ramage	O
d	O
d	O
hall	O
r	O
nallapati	O
and	O
c	O
manning	O
labeled	B
lda	B
a	O
supervised	O
topic	B
model	I
for	O
credit	O
attribution	O
in	O
multi-labeled	O
corpora	O
in	O
emnlp	O
ramage	O
d	O
c	O
manning	O
and	O
s	O
dumais	O
partially	O
labeled	O
topic	B
models	O
for	O
interpretable	O
text	O
mining	O
in	O
proc	O
of	O
the	O
int	O
l	O
conf	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
ramaswamy	O
s	O
p	O
tamayo	O
r	O
rifkin	O
s	O
mukherjee	O
c	O
yeang	O
m	O
angelo	O
c	O
ladd	O
m	O
reich	O
e	O
latulippe	O
j	O
mesirov	O
t	O
poggio	O
w	O
gerald	O
m	O
loda	O
e	O
lander	O
and	O
t	O
golub	O
multiclass	O
cancer	O
diagnosis	O
using	O
tumor	O
gene	O
expression	O
signature	O
proc	O
of	O
the	O
national	O
academy	O
of	O
science	O
usa	O
ranzato	O
m	O
and	O
g	O
hinton	O
modeling	O
pixel	O
means	O
and	O
covariances	O
using	O
factored	O
third-order	O
boltzmann	O
machines	O
in	O
cvpr	O
ranzato	O
m	O
f	O
-j	O
huang	O
y	O
-l	O
boureau	O
and	O
y	O
lecun	O
unsupervised	B
learning	B
of	O
invariant	B
feature	O
hierarchies	O
with	O
applications	O
to	O
object	O
recognition	O
in	O
cvpr	O
ranzato	O
m	O
c	O
poultney	O
s	O
chopra	O
and	O
y	O
lecun	O
efficient	O
learning	B
of	O
sparse	B
representations	O
with	O
an	O
energy-based	O
model	O
in	O
nips	O
rao	O
a	O
and	O
k	O
rose	O
february	O
deterministically	O
annealed	O
design	O
of	O
hidden	B
markov	B
model	I
speech	O
recognizers	O
ieee	O
trans	O
on	O
speech	O
and	O
audio	O
proc	O
rasmussen	O
c	O
the	O
infinite	O
gaussian	B
mixture	B
model	I
in	O
nips	O
rasmussen	O
c	O
e	O
and	O
j	O
qui	O
onerocandela	O
healing	O
the	O
relevance	B
vector	I
machine	I
by	O
augmentation	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
pp	O
rasmussen	O
c	O
e	O
and	O
c	O
k	O
i	O
williams	O
gaussian	B
processes	I
for	O
machine	B
learning	B
mit	O
press	O
ratsch	O
g	O
t	O
onoda	O
and	O
k	O
muller	O
soft	O
margins	O
for	O
adaboost	O
machine	B
learning	B
rattray	O
m	O
o	O
stegle	O
k	O
sharp	O
and	O
j	O
winn	O
inference	B
algorithms	O
and	O
learning	B
theory	O
for	O
bayesian	B
sparse	B
factor	B
analysis	I
in	O
proc	O
intl	O
workshop	O
on	O
statisticalmechanical	O
informatics	O
rauch	O
h	O
e	O
f	O
tung	O
and	O
c	O
t	O
striebel	O
maximum	O
likelihood	B
estimates	O
of	O
linear	O
dynamic	O
systems	O
aiaa	O
journal	O
ravikumar	O
p	O
j	O
lafferty	O
h	O
liu	O
and	O
l	O
wasserman	O
sparse	B
additive	O
models	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
raydan	O
m	O
the	O
barzilai	O
and	O
borwein	O
gradient	O
method	O
for	O
the	O
large	O
scale	O
unconstrained	O
minimization	O
problem	O
siam	O
j	O
on	O
optimization	B
rennie	O
j	O
why	O
sums	O
are	O
bad	O
technical	O
report	O
mit	O
rennie	O
j	O
l	O
shih	O
j	O
teevan	O
and	O
d	O
karger	O
tackling	O
the	O
poor	O
assumptions	O
of	O
naive	O
bayes	O
text	O
classifiers	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
reshed	O
d	O
y	O
reshef	O
h	O
finucane	O
s	O
grossman	O
g	O
mcvean	O
p	O
turnbaugh	O
e	O
lander	O
m	O
mitzenmacher	O
and	O
p	O
sabeti	O
december	O
detecting	O
novel	O
associations	O
in	O
large	O
data	O
sets	O
science	O
resnick	O
s	O
i	O
adventures	O
in	O
stochastic	B
processes	I
birkhauser	O
rice	O
j	O
mathematical	O
statistics	O
and	O
data	O
analysis	O
duxbury	O
edition	O
bibliography	O
richardson	O
s	O
and	O
p	O
green	O
on	O
bayesian	B
analysis	O
of	O
mixtures	O
with	O
an	O
unknown	B
number	O
of	O
components	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
riesenhuber	O
m	O
and	O
t	O
poggio	O
hierarchical	O
models	O
of	O
object	O
recognition	O
in	O
cortex	O
nature	O
neuroscience	O
rish	O
i	O
g	O
grabarnik	O
g	O
cecchi	O
f	O
pereira	O
and	O
g	O
gordon	O
closed-form	O
supervised	O
dimensionality	B
reduction	I
with	O
generalized	B
linear	I
models	I
in	O
intl	O
conf	O
on	O
machine	B
learning	B
ristic	O
b	O
s	O
arulampalam	O
and	O
n	O
gordon	O
beyond	O
the	O
kalman	O
filter	O
particle	O
filters	O
for	O
tracking	B
applications	O
artech	O
house	O
radar	B
library	O
robert	O
c	O
simulation	O
of	O
truncated	O
normal	B
distributions	O
statistics	O
and	O
computing	O
robert	O
c	O
and	O
g	O
casella	O
monte	B
carlo	I
statisical	O
methods	O
springer	O
edition	O
roberts	O
g	O
and	O
j	O
rosenthal	O
scaling	O
optimal	O
metropolis-hastings	O
statistical	O
science	O
for	O
various	O
algorithms	O
roberts	O
g	O
o	O
and	O
s	O
k	O
sahu	O
updating	O
schemes	O
correlation	O
structure	O
blocking	O
and	O
parameterization	O
for	O
the	O
gibbs	B
sampler	I
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
robinson	O
r	O
w	O
counting	O
labeled	O
acyclic	O
digraphs	O
in	O
f	O
harary	O
new	O
directions	O
in	O
the	O
theory	O
of	O
graphs	O
pp	O
academic	O
press	O
roch	O
s	O
a	O
short	O
proof	O
that	O
phylogenetic	B
tree	B
reconstrution	O
by	O
maximum	O
likelihood	B
is	O
hard	O
ieeeacm	O
trans	O
comp	O
bio	B
bioinformatics	O
rodriguez	O
a	O
and	O
k	O
modeling	O
through	O
nested	O
data	O
models	O
biometrika	O
to	O
appear	O
ghosh	O
relational	O
partition	O
rose	O
k	O
november	O
deterministic	B
annealing	B
for	O
clustering	B
compression	O
classification	O
regression	B
and	O
related	O
optimization	B
problems	O
proc	O
ieee	O
rosenblatt	B
f	O
the	O
perceptron	B
a	O
probabilistic	O
model	O
for	O
information	B
storage	O
and	O
organization	O
in	O
the	O
brain	O
psychological	O
review	O
a	O
ross	O
s	O
introduction	O
to	O
proba	O
bility	O
models	O
academic	O
press	O
rosset	O
s	O
j	O
zhu	O
and	O
t	O
hastie	O
boosting	B
as	O
a	O
regularized	O
path	B
to	O
a	O
maximum	O
margin	B
classifier	O
j	O
of	O
machine	B
learning	B
research	O
rossi	O
p	O
g	O
allenby	O
and	O
r	O
mcculloch	O
bayesian	B
statistics	I
and	O
marketing	O
wiley	O
roth	O
d	O
apr	O
on	O
the	O
hardness	O
of	O
approximate	O
reasoning	O
artificial	O
intelligence	O
rother	O
c	O
p	O
kohli	O
w	O
feng	O
and	O
j	O
jia	O
minimizing	O
sparse	B
higher	O
order	O
energy	O
functions	O
of	O
discrete	B
variables	O
in	O
cvpr	O
pp	O
rouder	O
j	O
p	O
speckman	O
d	O
sun	O
and	O
r	O
morey	O
bayesian	B
t	O
tests	O
for	O
accepting	O
and	O
rejecting	O
the	O
null	B
hypothesis	I
pyschonomic	O
bulletin	O
review	O
roverato	O
a	O
hyper	O
inverse	B
wishart	B
distribution	O
for	O
nondecomposable	O
graphs	O
and	O
its	O
application	O
to	O
bayesian	B
inference	B
for	O
gaussian	B
graphical	B
models	I
scand	O
j	O
statistics	O
roweis	O
s	O
em	B
algorithms	O
for	O
pca	B
and	O
spca	O
in	O
nips	O
rubin	O
d	O
using	O
the	O
sir	B
algorithm	O
to	O
simulate	O
posterior	O
distributions	O
in	O
bayesian	B
statistics	I
rue	O
h	O
and	O
l	O
held	O
gaussian	B
markov	B
random	O
fields	O
theory	O
and	O
applications	O
volume	O
of	O
monographs	O
on	O
statistics	O
and	O
applied	O
probability	O
london	O
chapman	O
hall	O
rue	O
h	O
s	O
martino	O
and	O
n	O
chopin	O
approximate	O
bayesian	B
inference	B
for	O
latent	B
gaussian	B
models	O
using	O
integrated	O
nested	O
laplace	B
approximations	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
rumelhart	O
d	O
g	O
hinton	O
and	O
r	O
williams	O
learning	B
internal	O
representations	O
by	O
error	O
propagation	O
in	O
d	O
rumelhart	O
j	O
mcclelland	O
and	O
the	O
pdd	O
research	O
group	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
mit	O
press	O
ruppert	O
d	O
m	O
wand	O
and	O
r	O
carroll	O
semiparametric	O
regression	B
cambridge	O
university	O
press	O
rush	O
a	O
m	O
and	O
m	O
collins	O
a	O
tutorial	O
on	O
lagrangian	B
relaxation	I
and	O
dual	B
decomposition	I
for	O
nlp	O
technical	O
report	O
columbia	O
u	O
russell	O
s	O
j	O
binder	O
d	O
koller	O
and	O
k	O
kanazawa	O
local	O
learning	B
in	O
probabilistic	O
networks	O
with	O
hidden	B
variables	I
in	O
intl	O
joint	O
conf	O
on	O
ai	B
russell	O
s	O
and	O
p	O
norvig	O
artificial	O
intelligence	O
a	O
modern	O
approach	O
englewood	O
cliffs	O
nj	O
prentice	O
hall	O
russell	O
s	O
and	O
p	O
norvig	O
artificial	O
intelligence	O
a	O
modern	O
approach	O
prentice	O
hall	O
edition	O
russell	O
s	O
and	O
p	O
norvig	O
artificial	O
intelligence	O
a	O
modern	O
approach	O
prentice	O
hall	O
edition	O
s	O
and	O
m	O
black	O
april	O
fields	O
j	O
computer	O
vi	O
intl	O
of	O
experts	O
sion	O
sachs	O
k	O
o	O
perez	O
d	O
pe	O
er	O
d	O
lauffenburger	O
and	O
g	O
nolan	O
causal	O
protein-signaling	O
networks	O
derived	O
from	O
multiparameter	O
science	O
single-cell	O
data	O
sahami	O
m	O
and	O
t	O
heilman	O
a	O
web-based	O
kernel	B
function	I
for	O
measuring	O
the	O
similarity	O
of	O
short	O
text	O
snippets	O
in	O
www	O
conferenec	O
salakhutdinov	O
r	O
erative	O
models	O
toronto	O
deep	B
genthesis	O
u	O
ph	O
d	O
salakhutdinov	O
r	O
and	O
g	O
hinton	O
deep	B
boltzmann	O
machines	O
in	O
aistatistics	O
volume	O
pp	O
salakhutdinov	O
r	O
and	O
g	O
hinton	O
an	O
undirected	B
topic	B
model	I
in	O
nips	O
replicated	O
softmax	B
salakhutdinov	O
r	O
and	O
h	O
larochelle	O
efficient	O
learning	B
of	O
deep	B
boltzmann	O
machines	O
in	O
aistatistics	O
salakhutdinov	O
r	O
and	O
a	O
mnih	O
probabilistic	B
matrix	B
factorization	I
in	O
nips	O
volume	O
bibliography	O
salakhutdinov	O
r	O
and	O
s	O
roweis	O
adaptive	O
overrelaxed	O
bound	B
optimization	B
methods	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	B
learning	B
volume	O
pp	O
schaefer	O
j	O
and	O
k	O
strimmer	O
a	O
shrinkage	B
approach	O
to	O
largescale	O
covariance	B
matrix	I
estimation	O
and	O
implications	O
for	O
functional	O
genomics	O
statist	O
appl	O
genet	O
mol	O
biol	O
salakhutdinov	O
r	O
j	O
tenenbaum	O
and	O
a	O
torralba	O
learning	B
to	I
learn	I
with	O
compound	O
hd	O
models	O
in	O
nips	O
salakhutdinov	O
r	O
r	O
a	O
mnih	O
and	O
g	O
e	O
hinton	O
restricted	O
boltzmann	O
machines	O
for	O
collaborative	O
filtering	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
volume	O
pp	O
salojarvi	O
j	O
k	O
puolamaki	O
and	O
s	O
klaski	O
on	O
discriminative	B
joint	O
density	O
modeling	O
in	O
proc	O
european	O
conf	O
on	O
machine	B
learning	B
sampson	O
f	O
a	O
novitiate	O
in	O
a	O
period	B
of	O
change	O
an	O
experimental	O
and	O
case	O
study	O
of	O
social	O
relationships	O
ph	O
d	O
thesis	O
cornell	O
santner	O
t	O
b	O
williams	O
and	O
w	O
notz	O
the	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
springer	O
sarkar	O
j	O
one-armed	B
bandit	I
problems	O
with	O
covariates	B
the	O
annals	O
of	O
statistics	O
sato	O
m	O
and	O
s	O
ishii	O
on-line	O
em	B
algorithm	O
for	O
the	O
normalized	O
gaussian	B
network	O
neural	O
computation	O
saul	O
l	O
t	O
jaakkola	O
and	O
m	O
jordan	O
mean	B
field	O
theory	O
for	O
sigmoid	B
belief	B
networks	I
j	O
of	O
ai	B
research	O
saul	O
l	O
and	O
m	O
jordan	O
exploiting	O
tractable	O
substructures	O
in	O
intractable	O
networks	O
in	O
nips	O
volume	O
saul	O
l	O
and	O
m	O
jordan	O
attractor	O
dynamics	O
in	O
feedforward	O
neural	B
networks	I
neural	O
computation	O
saunders	O
c	O
j	O
shawe-taylor	O
and	O
a	O
vinokourov	O
string	O
kernels	O
fisher	O
kernels	O
and	O
finite	O
state	B
automata	O
in	O
nips	O
savage	O
r	O
k	O
heller	O
y	O
xi	O
z	O
ghahramani	O
w	O
truman	O
m	O
grant	O
k	O
denby	O
and	O
d	O
wild	O
rbhc	O
fast	O
bayesian	B
hierarchical	B
clustering	B
for	O
microarray	O
data	O
bmc	O
bioinformatics	O
schapire	O
r	O
the	O
strength	O
of	O
weak	O
learnability	O
machine	B
learning	B
schapire	O
r	O
and	O
y	O
freund	O
foundations	O
and	O
algo	O
boosting	B
rithms	O
mit	O
press	O
schapire	O
r	O
y	O
freund	O
p	O
bartlett	O
and	O
w	O
lee	O
boosting	B
the	O
margin	B
a	O
new	O
explanation	O
for	O
the	O
effectiveness	O
of	O
voting	O
methods	O
annals	O
of	O
statistics	O
scharstein	O
d	O
and	O
r	O
szeliski	O
a	O
taxonomy	O
and	O
evaluation	O
of	O
dense	O
two-frame	O
stereo	O
correspondence	B
algorithms	O
j	O
computer	O
vision	O
intl	O
schaul	O
t	O
s	O
zhang	O
and	O
y	O
lecun	O
no	O
more	O
pesky	O
learning	B
rates	O
technical	O
report	O
courant	O
instite	O
of	O
mathematical	O
sciences	O
schmee	O
j	O
and	O
g	O
hahn	O
a	O
simple	O
method	O
for	O
regresssion	O
analysis	O
with	O
censored	O
data	O
technometrics	O
schmidt	O
m	O
graphical	B
model	I
structure	B
learning	B
with	O
regularization	B
ph	O
d	O
thesis	O
ubc	O
schmidt	O
m	O
g	O
fung	O
and	O
r	O
rosales	O
optimization	B
methods	O
for	O
regularization	B
technical	O
report	O
u	O
british	O
columbia	O
schmidt	O
m	O
and	O
k	O
murphy	O
modeling	O
discrete	B
interventional	B
data	I
using	O
directed	B
cyclic	O
graphical	B
models	I
in	O
uai	O
schmidt	O
m	O
k	O
murphy	O
g	O
fung	O
and	O
r	O
rosales	O
structure	B
learning	B
in	O
random	O
fields	O
for	O
heart	O
motion	O
abnormality	O
detection	O
in	O
cvpr	O
schmidt	O
m	O
a	O
niculescu-mizil	O
and	O
k	O
murphy	O
learning	B
graphical	B
model	I
structure	O
using	O
paths	O
in	O
aaai	O
schmidt	O
m	O
e	O
van	O
den	O
berg	O
m	O
friedlander	O
and	O
k	O
murphy	O
optimizing	O
costly	O
functions	O
with	O
simple	O
constraints	O
a	O
limited-memory	O
projected	O
quasinewton	O
algorithm	O
in	O
ai	B
statistics	O
schniter	O
p	O
l	O
c	O
potter	O
and	O
j	O
ziniel	O
fast	O
bayesian	B
matching	B
pursuit	I
model	O
uncertainty	B
and	O
parameter	B
estimation	O
for	O
sparse	B
linear	O
models	O
technical	O
report	O
u	O
ohio	O
submitted	O
to	O
ieee	O
trans	O
on	O
signal	B
processing	I
schnitzspan	O
p	O
s	O
roth	O
and	O
b	O
schiele	O
automatic	O
discovery	O
of	O
meaningful	O
object	O
parts	O
with	O
latent	B
crfs	O
in	O
cvpr	O
schoelkopf	O
b	O
and	O
a	O
smola	O
learning	B
with	O
kernels	O
support	B
vector	I
machines	I
regularization	B
optimization	B
and	O
beyond	O
mit	O
press	O
schoelkopf	O
b	O
a	O
smola	O
and	O
k	O
-r	O
mueller	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	B
eigenvalue	O
problem	O
neural	O
computation	O
schraudolph	O
n	O
n	O
j	O
yu	O
and	O
s	O
g	O
nter	O
a	O
stochastic	O
quasinewton	O
method	O
for	O
online	O
convex	B
optimization	B
in	O
aistatistics	O
pp	O
schwarz	O
g	O
estimating	O
the	O
dimension	O
of	O
a	O
model	O
annals	O
of	O
statistics	O
a	O
schwarz	O
r	O
and	O
y	O
chow	O
the	O
n-best	O
algorithm	O
an	O
efficient	O
and	O
exact	O
procedure	O
for	O
finding	O
the	O
n	O
most	O
in	O
intl	O
conf	O
on	O
acoustics	O
speech	O
and	O
signal	O
proc	O
likely	O
hypotheses	O
schweikerta	O
g	O
a	O
zien	O
g	O
zeller	O
j	O
behr	O
c	O
dieterich	O
c	O
ong	O
p	O
philips	O
f	O
d	O
bona	O
l	O
hartmann	O
a	O
bohlen	O
n	O
kr	O
ijger	O
s	O
sonnenburg	O
and	O
g	O
r	O
d	O
tsch	O
mgene	O
accurate	O
svm-based	O
gene	O
finding	O
with	O
an	O
application	O
to	O
nematode	O
genomes	O
genome	B
research	O
scott	O
d	O
and	O
biometrika	O
data-based	O
on	O
optimal	O
histograms	O
scott	O
j	O
g	O
and	O
c	O
m	O
carvalho	O
feature-inclusion	O
stochastic	B
search	I
for	O
gaussian	B
graphical	B
models	I
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
scott	O
s	O
data	B
augmentation	I
frequentist	B
estimation	O
and	O
the	O
bayesian	B
analysis	O
of	O
multinomial	B
logit	B
models	O
statistical	O
papers	O
scott	O
s	O
a	O
modern	O
bayesian	B
look	O
at	O
the	O
multi-armed	B
bandit	I
applied	O
stochastic	O
models	O
in	O
business	O
and	O
industry	O
bibliography	O
sedgewick	O
r	O
and	O
k	O
wayne	O
al	O
gorithms	O
addison	O
wesley	O
seeger	O
m	O
bayesian	B
inference	B
and	O
optimal	O
design	O
in	O
the	O
sparse	B
linear	O
model	O
j	O
of	O
machine	B
learning	B
research	O
seeger	O
m	O
and	O
h	O
nickish	O
compressed	B
sensing	I
and	O
bayesian	B
experimental	O
design	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
segal	O
d	O
february	O
the	O
dirty	O
little	O
secrets	O
of	O
search	O
new	O
york	O
times	O
seide	O
f	O
g	O
li	O
and	O
d	O
yu	O
conversational	O
speech	O
transcription	O
using	O
context-dependent	O
deep	B
neural	B
networks	I
in	O
interspeech	O
sejnowski	O
t	O
and	O
c	O
rosenberg	O
parallel	O
networks	O
that	O
learn	O
to	O
pronounce	O
english	O
text	O
complex	O
systems	O
sellke	O
t	O
m	O
j	O
bayarri	O
and	O
j	O
berger	O
calibration	B
of	O
p	O
values	O
for	O
testing	O
precise	O
null	O
hypotheses	O
the	O
american	O
statistician	O
serre	O
t	O
l	O
wolf	O
and	O
t	O
poggio	O
recognition	O
with	O
features	B
object	O
inspired	O
by	O
visual	O
cortex	O
in	O
cvpr	O
pp	O
shachter	O
r	O
bayes-ball	O
the	O
rational	O
pastime	O
determining	O
irrelevance	O
and	O
requisite	O
information	B
in	O
belief	B
networks	I
and	O
influence	O
diagrams	O
in	O
uai	O
r	O
and	O
c	O
r	O
kenley	O
shachter	O
gaussian	B
influence	O
diagrams	O
managment	O
science	O
shachter	O
r	O
d	O
and	O
m	O
a	O
peot	O
simulation	O
approaches	O
to	O
general	O
probabilistic	B
inference	B
on	O
belief	B
networks	I
in	O
uai	O
volume	O
shafer	O
g	O
r	O
and	O
p	O
p	O
shenoy	O
probability	O
propagation	O
annals	O
of	O
mathematics	O
and	O
ai	B
shafto	O
p	O
c	O
kemp	O
v	O
mansinghka	O
m	O
gordon	O
and	O
j	O
b	O
tenenbaum	O
learning	B
cross-cutting	O
systems	O
of	O
categories	O
in	O
cognitive	O
science	O
conference	O
shahaf	O
d	O
a	O
chechetka	O
and	O
c	O
guestrin	O
learning	B
thin	B
junction	B
trees	I
via	O
graph	B
cuts	I
in	O
aistats	O
shalev-shwartz	O
s	O
y	O
singer	O
and	O
n	O
srebro	O
pegasos	B
primal	O
estimated	O
sub-gradient	O
solver	O
for	O
svm	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
shalizi	O
c	O
cs	O
lecture	O
principal	B
components	I
mathematics	O
example	O
interpretation	O
shan	O
h	O
and	O
a	O
banerjee	O
residual	B
bayesian	B
co-clustering	B
for	O
matrix	O
approximation	O
in	O
siam	O
intl	O
conf	O
on	O
data	O
mining	O
shawe-taylor	O
j	O
and	O
n	O
cristianini	O
kernel	B
methods	O
for	O
pattern	B
analysis	O
cambridge	O
sheng	O
q	O
y	O
moreau	O
and	O
b	O
d	O
moor	O
biclustering	B
microarray	O
data	O
by	O
gibbs	B
sampling	I
bioinformatics	O
shi	O
j	O
and	O
j	O
malik	O
normalized	O
cuts	O
and	O
image	B
segmentation	I
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
shoham	O
y	O
and	O
k	O
leyton-brown	O
multiagent	O
systems	O
algorithmic	O
game-	O
theoretic	O
and	O
logical	O
foundations	O
cambridge	O
university	O
press	O
shotton	O
j	O
a	O
fitzgibbon	O
m	O
cook	O
t	O
sharp	O
m	O
finocchio	O
r	O
moore	O
a	O
kipman	O
and	O
a	O
blake	O
real-time	O
human	O
pose	O
recognition	O
in	O
parts	O
from	O
a	O
single	O
depth	O
image	O
in	O
cvpr	O
shwe	O
m	O
b	O
middleton	O
d	O
heckerman	O
m	O
henrion	O
e	O
horvitz	O
h	O
lehmann	O
and	O
g	O
cooper	O
probabilistic	O
diagnosis	O
using	O
a	O
reformulation	O
of	O
the	O
knowledge	B
base	I
inf	O
med	O
methods	O
siddiqi	O
s	O
b	O
boots	O
and	O
g	O
gordon	O
a	O
constraint	O
generation	O
approach	O
to	O
learning	B
stable	B
linear	O
dynamical	O
systems	O
in	O
nips	O
siepel	O
a	O
and	O
d	O
haussler	O
combining	O
phylogenetic	O
and	O
hidden	B
markov	B
models	I
in	O
biosequence	B
analysis	I
in	O
proc	O
intl	O
conf	O
on	O
computational	O
molecular	O
biology	O
silander	O
t	O
p	O
kontkanen	O
and	O
p	O
myllym	O
d	O
ki	O
on	O
sensitivity	B
of	O
the	O
map	O
bayesian	B
network	O
structure	O
to	O
the	O
equivalent	B
sample	I
size	I
parameter	B
in	O
uai	O
pp	O
silander	O
t	O
and	O
p	O
myllmaki	O
a	O
simple	O
approach	O
for	O
finding	O
the	O
globally	O
optimal	O
bayesian	B
network	O
structure	O
in	O
uai	O
sill	O
j	O
g	O
takacs	O
l	O
mackey	O
and	O
d	O
lin	O
feature-weighted	O
linear	O
stacking	B
technical	O
report	O
silverman	O
b	O
w	O
spline	B
smoothing	B
the	O
equivalent	O
variable	O
kernel	B
method	O
annals	O
of	O
statistics	O
simard	O
p	O
d	O
steinkraus	O
and	O
j	O
platt	O
best	O
practices	O
for	O
convolutional	O
neural	B
networks	I
applied	O
to	O
visual	O
document	O
analysis	O
in	O
intl	O
conf	O
on	O
document	O
analysis	O
and	O
recognition	O
simon	O
d	O
optimal	O
state	B
estimation	I
kalman	O
h	O
infinity	O
and	O
nonlinear	O
approaches	O
wiley	O
singliar	O
t	O
and	O
m	O
hauskrecht	O
noisy-or	B
component	O
analysis	O
and	O
its	O
application	O
to	O
link	O
analysis	O
j	O
of	O
machine	B
learning	B
research	O
smidl	O
v	O
and	O
a	O
quinn	O
the	O
variational	B
bayes	I
method	O
in	O
signal	B
processing	I
springer	O
smith	O
a	O
f	O
m	O
and	O
a	O
e	O
gelfand	O
bayesian	B
statistics	I
without	O
tears	O
a	O
sampling-resampling	O
perspective	O
the	O
american	O
statistician	O
smith	O
r	O
and	O
p	O
cheeseman	O
on	O
the	O
representation	O
and	O
estimation	O
of	O
spatial	O
uncertainty	B
intl	O
j	O
robotics	O
research	O
smith	O
t	O
v	O
j	O
yu	O
smulders	O
a	O
hartemink	O
and	O
e	O
jarvis	O
computational	O
inference	B
of	O
neural	O
information	B
flow	O
networks	O
plos	O
computational	O
biology	O
smolensky	O
p	O
information	B
processing	O
in	O
dynamical	O
systems	O
foundations	O
of	O
harmony	O
theory	O
in	O
d	O
rumehart	O
and	O
j	O
mcclelland	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
volume	O
mcgraw-hill	O
smyth	O
p	O
d	O
heckerman	O
and	O
m	O
i	O
jordan	O
probabilistic	O
independence	O
networks	O
for	O
hidden	B
markov	B
probability	O
models	O
neural	O
computation	O
sohl-dickstein	O
m	O
deweese	O
on	O
machine	B
learning	B
j	O
p	O
battaglino	O
and	O
in	O
intl	O
conf	O
bibliography	O
sollich	O
p	O
bayesian	B
methods	O
for	O
support	B
vector	I
machines	I
evidence	B
and	O
predictive	B
class	O
probabilities	O
machine	B
learning	B
sontag	O
d	O
a	O
globerson	O
and	O
t	O
jaakkola	O
introduction	O
to	O
dual	B
decomposition	I
for	O
inference	B
in	O
s	O
sra	O
s	O
nowozin	O
and	O
s	O
j	O
wright	O
optimization	B
for	O
machine	B
learning	B
mit	O
press	O
sorenson	O
h	O
and	O
d	O
alspach	O
recursive	B
bayesian	B
estimation	O
using	O
gaussian	B
sums	O
automatica	O
a	O
s	O
soussen	O
c	O
j	O
iier	O
d	O
brie	O
and	O
j	O
duan	O
from	O
bernoulligaussian	O
deconvolution	O
to	O
sparse	B
signal	O
restoration	O
technical	O
report	O
centre	O
de	O
recherche	O
en	O
automatique	O
de	O
nancy	O
spaan	O
m	O
and	O
n	O
vlassis	O
perseus	O
randomized	O
point-based	O
value	O
iteration	O
for	O
pomdps	O
j	O
of	O
ai	B
research	O
spall	O
j	O
introduction	O
to	O
stochastic	B
search	I
and	O
optimization	B
estimation	O
simulation	O
and	O
control	O
wiley	O
speed	O
t	O
december	O
a	O
correlation	O
for	O
the	O
century	O
science	O
speed	O
t	O
and	O
h	O
kiiveri	O
gaussian	B
markov	B
distributions	O
over	O
finite	O
graphs	O
annals	O
of	O
statistics	O
spiegelhalter	O
d	O
j	O
and	O
s	O
l	O
lauritzen	O
sequential	B
updating	O
of	O
conditional	O
probabilities	O
on	O
directed	B
graphical	O
structures	O
networks	O
spirtes	O
p	O
scheines	O
c	O
glymour	O
and	O
r	O
causation	O
prediction	O
and	O
search	O
mit	O
press	O
edition	O
srebro	O
n	O
maximum	O
likelihood	B
bounded	O
tree-width	O
markov	B
networks	O
in	O
uai	O
srebro	O
n	O
and	O
t	O
jaakkola	O
approximain	O
intl	O
conf	O
on	O
machine	O
low-rank	O
weighted	O
tions	O
learning	B
steinbach	O
m	O
g	O
karypis	O
and	O
v	O
kumar	O
a	O
comparison	O
of	O
document	O
clustering	B
techniques	O
in	O
kdd	O
workshop	O
on	O
text	O
mining	O
stephens	O
m	O
dealing	O
with	O
label-switching	O
in	O
mixture	B
models	O
j	O
royal	O
statistical	O
society	O
series	O
b	O
stern	O
d	O
r	O
herbrich	O
and	O
t	O
graepel	O
matchbox	O
large	O
scale	O
bayesian	B
recommendations	O
in	O
proc	O
intl	O
world	O
wide	O
web	O
conference	O
steyvers	O
m	O
and	O
t	O
griffiths	O
probabilistic	O
topic	B
models	O
in	O
t	O
landauer	O
d	O
mcnamara	O
s	O
dennis	O
and	O
w	O
kintsch	O
latent	B
semantic	I
analysis	I
a	O
road	O
to	O
meaning	O
laurence	O
erlbaum	O
stigler	O
s	O
the	O
history	O
of	O
statis	O
tics	O
harvard	O
university	O
press	O
stolcke	O
a	O
and	O
s	O
m	O
omohundro	O
hidden	B
markov	B
model	I
induction	B
by	O
bayesian	B
model	O
merging	O
in	O
stoyanov	O
v	O
a	O
ropson	O
and	O
j	O
eisner	O
empirical	B
risk	B
minimization	I
of	O
graphical	B
model	I
parameters	O
given	O
approximate	B
inference	B
decoding	B
and	O
model	O
structure	O
in	O
aistatistics	O
sudderth	O
e	O
graphical	B
models	I
for	O
visual	O
object	O
recognition	O
and	O
tracking	B
ph	O
d	O
thesis	O
mit	O
sudderth	O
e	O
and	O
w	O
freeman	O
march	O
signal	O
and	O
image	O
processing	O
with	O
belief	B
propagation	I
ieee	O
signal	B
processing	I
magazine	O
sudderth	O
e	O
a	O
ihler	O
w	O
freeman	O
and	O
a	O
willsky	O
nonparametric	O
belief	B
propagation	I
in	O
cvpr	O
sudderth	O
e	O
a	O
ihler	O
m	O
isard	O
w	O
freeman	O
and	O
a	O
willsky	O
nonparametric	O
belief	B
propagation	I
comm	O
of	O
the	O
acm	O
sudderth	O
e	O
and	O
m	O
jordan	O
shared	B
segmentation	O
of	O
natural	O
scenes	O
using	O
dependent	O
pitmanyor	O
processes	O
in	O
nips	O
sudderth	O
e	O
m	O
wainwright	O
and	O
a	O
willsky	O
loop	B
series	O
and	O
bethe	B
variational	O
bounds	O
for	O
attractive	O
graphical	B
models	I
in	O
nips	O
sun	O
l	O
s	O
ji	O
s	O
yu	O
and	O
j	O
ye	O
on	O
the	O
equivalence	O
between	O
canonical	B
correlation	I
analysis	I
and	O
orthonormalized	O
partial	O
in	O
intl	O
joint	O
conf	O
on	O
ai	B
least	B
squares	I
sunehag	O
p	O
j	O
trumpf	O
s	O
v	O
n	O
vishwanathan	O
and	O
n	O
n	O
schraudolph	O
variable	O
metric	B
stochastic	B
approximation	I
theory	O
in	O
aistatistics	O
pp	O
sutton	O
c	O
and	O
a	O
mccallum	O
improved	O
dynamic	O
schedules	O
for	O
belief	B
propagation	I
in	O
uai	O
sutton	O
r	O
and	O
a	O
barto	O
reinforcment	O
learning	B
an	O
introduction	O
mit	O
press	O
swendsen	O
r	O
and	O
j	O
-s	O
wang	O
nonuniversal	O
critical	O
dynamics	O
in	O
monte	B
carlo	I
simulations	O
physical	O
review	O
letters	O
swersky	O
k	O
b	O
chen	O
b	O
marlin	O
and	O
n	O
de	O
freitas	O
a	O
tutorial	O
on	O
stochastic	B
approximation	I
algorithms	O
for	O
training	O
restricted	O
boltzmann	O
machines	O
and	O
deep	B
belief	O
nets	O
in	O
information	B
theory	I
and	O
applications	O
workshop	O
szeliski	O
r	O
computer	O
vision	O
algorithms	O
and	O
applications	O
springer	O
szeliski	O
r	O
r	O
zabih	O
d	O
scharstein	O
o	O
veksler	O
v	O
kolmogorov	O
a	O
agarwala	O
m	O
tappen	O
and	O
c	O
rother	O
a	O
comparative	O
study	O
of	O
energy	O
minimization	O
methods	O
for	O
markov	B
random	O
fields	O
with	O
smoothness-based	O
priors	O
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
szepesvari	O
c	O
algorithms	O
for	O
reinforcement	B
learning	B
morgan	O
claypool	O
taleb	O
n	O
the	O
black	O
swan	O
the	O
impact	O
of	O
the	O
highly	O
improbable	O
random	O
house	O
talhouk	O
a	O
k	O
murphy	O
and	O
a	O
doucet	O
efficient	O
bayesian	B
inference	B
for	O
multivariate	B
probit	B
models	O
with	O
sparse	B
inverse	O
correlation	O
matrices	O
j	O
comp	O
graph	B
statist	O
tanner	O
m	O
tools	O
for	O
statistical	O
inference	B
springer	O
sun	O
j	O
n	O
zheng	O
and	O
h	O
shum	O
stereo	O
matching	O
using	O
belief	B
propagation	I
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
tanner	O
m	O
and	O
w	O
wong	O
the	O
calculation	O
of	O
posterior	O
distributions	O
by	O
data	B
augmentation	I
j	O
of	O
the	O
am	O
stat	O
assoc	O
bibliography	O
tarlow	O
d	O
i	O
givoni	O
and	O
r	O
zemel	O
hop-map	O
efficient	O
message	B
passing	I
with	O
high	O
order	O
potentials	O
in	O
aistatistics	O
taskar	O
b	O
c	O
guestrin	O
and	O
d	O
koller	O
max-margin	O
markov	B
networks	O
in	O
nips	O
taskar	O
b	O
d	O
klein	O
m	O
collins	O
d	O
koller	O
and	O
c	O
manning	O
max-margin	O
parsing	O
in	O
proc	O
empirical	O
methods	O
in	O
natural	O
language	O
processing	O
teh	O
y	O
w	O
a	O
hierarchical	O
bayesian	B
language	B
model	I
based	O
on	O
pitman-yor	O
processes	O
in	O
proc	O
of	O
the	O
assoc	O
for	O
computational	O
linguistics	O
pp	O
teh	O
y	O
-w	O
m	O
jordan	O
m	O
beal	O
and	O
d	O
blei	O
hierarchical	O
dirichlet	B
processes	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
tenenbaum	O
j	O
framework	O
for	O
ph	O
d	O
thesis	O
mit	O
a	O
bayesian	B
learning	B
concept	B
tenenbaum	O
j	O
b	O
and	O
f	O
xu	O
word	O
learning	B
as	O
bayesian	B
inference	B
in	O
proc	O
annual	O
conf	O
of	O
the	O
cognitive	O
science	O
society	O
theocharous	O
g	O
k	O
murphy	O
and	O
l	O
kaelbling	O
representing	O
hierarchical	O
pomdps	O
as	O
dbns	O
for	O
multi-scale	O
robot	O
localization	O
in	O
ieee	O
intl	O
conf	O
on	O
robotics	O
and	O
automation	O
thiesson	O
b	O
c	O
meek	O
d	O
chickering	O
and	O
d	O
heckerman	O
learning	B
mixtures	O
of	O
dag	B
models	O
in	O
uai	O
thomas	O
a	O
and	O
p	O
green	O
enumerating	O
the	O
decomposable	B
neighbours	O
of	O
a	O
decomposable	B
graph	B
under	O
a	O
simple	O
perturbation	O
scheme	O
comp	O
statistics	O
and	O
data	O
analysis	O
thrun	O
s	O
w	O
burgard	O
and	O
d	O
fox	O
probabilistic	O
robotics	O
mit	O
press	O
thrun	O
s	O
m	O
montemerlo	O
d	O
koller	O
b	O
wegbreit	O
j	O
nieto	O
and	O
e	O
nebot	O
fastslam	B
an	O
efficient	O
solution	O
to	O
the	O
simultaneous	B
localization	I
and	I
mapping	I
problem	O
with	O
unknown	B
data	B
association	I
j	O
of	O
machine	B
learning	B
research	O
thrun	O
s	O
and	O
l	O
pratt	O
learning	B
to	I
learn	I
kluwer	O
tibshirani	O
r	O
regression	B
shrinkage	B
and	O
selection	O
via	O
the	O
lasso	B
j	O
royal	O
statist	O
soc	O
b	O
tibshirani	O
r	O
g	O
walther	O
and	O
t	O
hastie	O
estimating	O
the	O
number	O
of	O
clusters	B
in	O
a	O
dataset	O
via	O
the	O
gap	B
statistic	I
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
tieleman	O
t	O
training	O
restricted	O
boltzmann	O
machines	O
using	O
approximations	O
to	O
the	O
likelihood	B
gradient	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	B
learning	B
pp	O
acm	O
new	O
york	O
ny	O
usa	O
ting	O
j	O
a	O
d	O
souza	O
s	O
vijayakumar	O
and	O
s	O
schaal	O
efficient	O
learning	B
and	O
feature	B
selection	I
in	O
high-dimensional	O
regression	B
neural	O
computation	O
tipping	O
m	O
probabilistic	O
visualization	O
of	O
high-dimensional	O
binary	O
data	O
in	O
nips	O
tipping	O
m	O
sparse	B
bayesian	B
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
j	O
of	O
machine	B
learning	B
research	O
tipping	O
m	O
and	O
c	O
bishop	O
probabilistic	O
principal	B
component	I
analysis	O
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
tipping	O
m	O
and	O
a	O
faul	O
fast	O
likelihood	B
maximisation	O
marginal	O
for	O
sparse	B
bayesian	B
models	O
in	O
aistats	O
tishby	O
n	O
f	O
pereira	O
and	O
w	O
biale	O
the	O
information	B
neck	O
method	O
in	O
the	O
annual	O
allerton	O
conf	O
on	O
communication	O
control	O
and	O
computing	O
pp	O
a	O
tomas	O
m	O
d	O
anoop	O
k	O
stefan	O
b	O
lukas	O
and	O
c	O
jan	O
empirical	O
evaluation	O
and	O
combination	O
of	O
advanced	O
language	B
modeling	I
techniques	O
in	O
proc	O
annual	O
conf	O
of	O
the	O
intl	O
speech	O
communication	O
association	O
torralba	O
a	O
r	O
fergus	O
and	O
y	O
weiss	O
small	O
codes	O
and	O
large	O
image	O
databases	O
for	O
recognition	O
in	O
cvpr	O
train	O
k	O
discrete	B
choice	O
methcambridge	O
ods	O
with	O
simulation	O
university	O
press	O
second	O
edition	O
tseng	O
p	O
on	O
accelerated	O
proximal	O
gradient	O
methods	O
for	O
convexconcave	O
optimization	B
technical	O
report	O
u	O
washington	O
tsochantaridis	O
i	O
t	O
joachims	O
t	O
hofmann	O
and	O
y	O
altun	O
september	O
large	O
margin	B
methods	O
for	O
structured	O
and	O
interdependent	O
output	O
variables	O
j	O
of	O
machine	B
learning	B
research	O
tu	O
z	O
and	O
s	O
zhu	O
image	B
segmentation	I
by	O
data-driven	O
markov	B
chain	I
monte	B
carlo	I
ieee	O
trans	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
turian	O
j	O
l	O
ratinov	O
and	O
y	O
bengio	O
word	O
representations	O
a	O
simple	O
and	O
general	O
method	O
for	O
semi-supervised	B
learning	B
in	O
proc	O
acl	O
turlach	O
b	O
w	O
venables	O
and	O
s	O
wright	O
simultaneous	O
variable	O
selection	O
technometrics	O
turner	O
r	O
p	O
berkes	O
m	O
sahani	O
and	O
d	O
mackay	O
counterexamples	O
to	O
variational	B
free	B
energy	I
compactness	B
folk	O
theorems	O
technical	O
report	O
u	O
cambridge	O
ueda	O
n	O
and	O
r	O
nakano	O
deterministic	B
annealing	B
em	B
algorithm	O
neural	B
networks	I
usunier	O
n	O
d	O
buffoni	O
and	O
p	O
gallinari	O
ranking	B
with	O
ordered	O
weighted	O
pairwise	O
classification	O
vaithyanathan	O
s	O
and	O
b	O
dom	O
model	B
selection	I
in	O
unsupervised	B
learning	B
with	O
applications	O
to	O
document	O
clustering	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
van	O
der	O
merwe	O
r	O
a	O
doucet	O
n	O
de	O
freitas	O
and	O
e	O
wan	O
the	O
unscented	O
particle	O
filter	O
in	O
van	O
dyk	O
d	O
and	O
x	O
-l	O
meng	O
the	O
art	O
of	O
data	B
augmentation	I
j	O
computational	O
and	O
graphical	O
statistics	O
vandenberghe	O
l	O
applied	O
numerical	O
computing	O
lecture	O
notes	O
vandenberghe	O
l	O
optimization	B
methods	O
for	O
large-scale	O
systems	O
vanhatalo	O
j	O
speeding	O
up	O
the	O
inference	B
in	O
gaussian	B
process	I
models	O
ph	O
d	O
thesis	O
helsinki	O
univ	O
technology	O
bibliography	O
vanhatalo	O
j	O
v	O
pietil	O
d	O
inen	O
and	O
a	O
vehtari	O
approximate	B
inference	B
for	O
disease	B
mapping	I
with	O
sparse	B
gaussian	B
processes	I
statistics	O
in	O
medicine	O
vapnik	O
v	O
statistical	B
learning	B
theory	I
wiley	O
vapnik	O
v	O
s	O
golowich	O
and	O
a	O
smola	O
support	B
vector	O
method	O
for	O
function	B
approximation	I
regression	B
estimation	O
and	O
signal	B
processing	I
in	O
nips	O
varian	O
h	O
structural	B
time	I
series	I
in	O
r	O
a	O
tutorial	O
technical	O
report	O
google	O
verma	O
t	O
and	O
j	O
pearl	O
equivalence	O
and	O
synthesis	O
of	O
causal	B
models	I
in	O
uai	O
viinikanoja	O
j	O
a	O
klami	O
and	O
s	O
kaski	O
variational	O
bayesian	B
mixture	B
of	O
robust	B
cca	B
models	O
in	O
proc	O
european	O
conf	O
on	O
machine	B
learning	B
vincent	O
p	O
a	O
connection	O
between	O
score	B
matching	I
and	O
denoising	O
autoencoders	O
neural	O
computation	O
vincent	O
p	O
h	O
larochelle	O
i	O
lajoie	O
y	O
bengio	O
and	O
p	O
-a	O
manzagol	O
stacked	O
denoising	O
autoencoders	O
learning	B
useful	O
representations	O
in	O
a	O
deep	B
network	O
with	O
a	O
local	O
denoising	O
criterion	O
j	O
of	O
machine	B
learning	B
research	O
vinh	O
n	O
j	O
epps	O
and	O
j	O
bailey	O
information	B
theoretic	O
measures	O
for	O
is	O
a	O
corclusterings	O
comparison	O
rection	O
for	O
chance	O
necessary	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
vinyals	O
m	O
j	O
cerquides	O
j	O
rodriguezaguilar	O
and	O
a	O
farinelli	O
worst-case	O
bounds	O
on	O
the	O
quality	O
of	O
max-product	B
fixed-points	O
in	O
nips	O
viola	O
p	O
and	O
m	O
jones	O
rapid	O
object	B
detection	I
using	O
a	O
boosted	O
cascade	B
of	O
simple	O
classifiers	O
in	O
cvpr	O
virtanen	O
s	O
bayesian	B
exponential	B
family	B
projections	O
master	B
s	O
thesis	O
aalto	O
university	O
vishwanathan	O
s	O
v	O
n	O
and	O
a	O
smola	O
fast	O
kernels	O
for	O
string	O
and	O
tree	B
matching	O
in	O
nips	O
viterbi	B
a	O
error	O
bounds	O
for	O
convolutional	O
codes	O
and	O
an	O
asymptotically	O
optimum	O
decoding	B
algorithm	O
ieee	O
trans	O
on	O
information	B
theory	I
a	O
von	O
luxburg	O
u	O
a	O
tutorial	O
on	O
statistics	O
and	O
spectral	B
clustering	B
computing	O
wagenmakers	O
e	O
-j	O
r	O
wetzels	O
d	O
borsboom	O
and	O
h	O
van	O
der	O
maas	O
why	O
psychologists	O
must	O
change	O
the	O
way	O
they	O
analyze	O
their	O
data	O
the	O
case	O
of	O
psi	O
journal	O
of	O
personality	O
and	O
social	O
psychology	O
wagner	O
d	O
and	O
f	O
wagner	O
between	O
min	O
cut	O
and	O
graph	B
bisection	O
in	O
proc	O
intl	O
symp	O
on	O
math	O
found	O
of	O
comp	O
sci	O
pp	O
wainwright	O
m	O
t	O
jaakkola	O
and	O
a	O
willsky	O
tree-based	O
reparameterization	O
for	O
approximate	O
estimation	O
on	O
loopy	O
graphs	O
in	O
jaakkola	O
and	O
wainwright	O
m	O
t	O
a	O
willsky	O
a	O
new	O
class	O
of	O
upper	O
bounds	O
on	O
the	O
log	B
partition	B
function	I
ieee	O
trans	O
info	O
theory	O
wainwright	O
m	O
p	O
ravikumar	O
and	O
inferring	O
graphj	O
lafferty	O
ical	O
model	O
structure	O
using	O
pseudo-likelihood	B
in	O
nips	O
wainwright	O
m	O
j	O
t	O
s	O
jaakkola	O
and	O
a	O
s	O
willsky	O
tree-based	O
reparameterization	O
framework	O
for	O
analysis	O
of	O
sum-product	B
and	O
related	O
algorithms	O
ieee	O
trans	O
on	O
information	B
theory	I
wainwright	O
m	O
j	O
and	O
m	O
jordan	O
graphical	B
models	I
exponential	O
families	O
and	O
variational	B
inference	B
foundations	O
and	O
trends	O
in	O
machine	B
learning	B
i	O
wainwright	O
m	O
j	O
and	O
m	O
jordan	O
graphical	B
models	I
exponential	O
families	O
and	O
variational	B
inference	B
foundations	O
and	O
trends	O
in	O
machine	B
learning	B
i	O
wallach	O
h	O
i	O
murray	O
r	O
salakhutdinov	O
and	O
d	O
mimno	O
evaluation	O
methods	O
for	O
topic	B
models	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
wan	O
e	O
a	O
and	O
r	O
v	O
der	O
merwe	O
the	O
unscented	O
kalman	O
filter	O
in	O
s	O
haykin	O
kalman	O
filtering	B
and	O
neural	B
networks	I
wiley	O
wand	O
m	O
semiparametric	O
regression	B
and	O
graphical	B
models	I
aust	O
n	O
z	O
j	O
stat	O
wand	O
m	O
p	O
j	O
t	O
ormerod	O
s	O
a	O
padoan	O
and	O
r	O
fruhrwirth	O
mean	B
field	O
variational	B
bayes	I
for	O
elaborate	O
distributions	O
bayesian	B
analysis	O
wang	O
c	O
variational	O
bayesian	B
approach	O
to	O
canonical	B
correlation	I
analysis	I
ieee	O
trans	O
on	O
neural	B
networks	I
wasserman	O
l	O
all	O
of	O
statistics	O
a	O
concise	O
course	O
in	O
statistical	O
inference	B
springer	O
wei	O
g	O
and	O
m	O
tanner	O
a	O
monte	B
carlo	I
implementation	O
of	O
the	O
em	B
algorithm	O
and	O
the	O
poor	O
man	O
s	O
data	B
augmentation	I
algorithms	O
j	O
of	O
the	O
am	O
stat	O
assoc	O
weinberger	O
k	O
a	O
dasgupta	O
j	O
attenberg	O
j	O
langford	O
and	O
a	O
smola	O
feature	O
hashing	O
for	O
large	O
scale	O
multitask	O
learning	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
weiss	O
d	O
b	O
sapp	O
and	O
b	O
taskar	O
sidestepping	O
intractable	O
inference	B
with	O
structured	O
ensemble	B
cascades	O
in	O
nips	O
weiss	O
y	O
correctness	O
of	O
local	O
probability	O
propagation	O
in	O
graphical	B
models	I
with	O
loops	O
neural	O
computation	O
weiss	O
y	O
comparing	O
the	O
mean	B
field	O
method	O
and	O
belief	B
propagation	I
for	O
approximate	B
inference	B
in	O
mrfs	O
in	O
saad	O
and	O
opper	O
advanced	O
mean	B
field	O
methods	O
mit	O
press	O
weiss	O
y	O
and	O
w	O
t	O
freeman	O
correctness	O
of	O
belief	B
propagation	I
in	O
gaussian	B
graphical	B
models	I
of	O
arbitrary	O
topology	O
in	O
weiss	O
y	O
and	O
w	O
t	O
freeman	O
correctness	O
of	O
belief	B
propagation	I
in	O
gaussian	B
graphical	B
models	I
of	O
arbitrary	O
topology	O
neural	O
computation	O
weiss	O
y	O
and	O
w	O
t	O
freeman	O
on	O
the	O
optimality	O
of	O
solutions	O
of	O
the	O
max-product	B
belief	B
propagation	I
algorithm	O
in	O
arbitrary	O
graphs	O
ieee	O
trans	O
information	B
theory	I
special	O
issue	O
on	O
codes	O
on	O
graphs	O
and	O
iterative	O
algorithms	O
weiss	O
y	O
a	O
torralba	O
and	O
r	O
fergus	O
spectral	B
hashing	O
in	O
nips	O
bibliography	O
welling	O
m	O
c	O
chemudugunta	O
and	O
n	O
sutter	O
deterministic	O
latent	B
variable	I
models	I
and	O
their	O
pitfalls	O
in	O
intl	O
conf	O
on	O
data	O
mining	O
welling	O
m	O
t	O
minka	O
and	O
y	O
w	O
teh	O
structured	O
region	O
graphs	O
morphing	O
ep	B
into	O
gbp	O
in	O
uai	O
welling	O
m	O
m	O
rosen-zvi	O
and	O
g	O
hinton	O
exponential	B
family	B
harmoniums	O
with	O
an	O
application	O
to	O
information	B
retrieval	I
in	O
welling	O
m	O
and	O
c	O
sutton	O
learning	B
in	O
markov	B
random	O
fields	O
with	O
contrastive	O
free	O
energies	O
in	O
tenth	O
international	O
workshop	O
on	O
artificial	O
intelligence	O
and	O
statistics	O
welling	O
m	O
and	O
y	O
-w	O
teh	O
belief	O
optimization	B
for	O
binary	O
networks	O
a	O
stable	B
alternative	O
to	O
loopy	B
belief	B
propagation	I
in	O
uai	O
werbos	O
p	O
beyond	O
regression	B
new	O
tools	O
for	O
prediction	O
and	O
analysis	O
in	O
the	O
behavioral	O
sciences	O
ph	O
d	O
thesis	O
harvard	O
west	O
m	O
of	O
tures	O
biometrika	O
normal	B
on	O
scale	O
mixdistributions	O
west	O
m	O
bayesian	B
factor	B
regression	B
models	O
in	O
the	O
p	O
small	B
n	I
paradigm	O
bayesian	B
statistics	I
west	O
m	O
and	O
j	O
harrison	O
bayesian	B
forecasting	O
and	O
dynamic	O
models	O
springer	O
weston	O
j	O
s	O
bengio	O
and	O
n	O
usunier	O
large	O
scale	O
image	O
annotation	O
learning	B
to	I
rank	I
with	O
joint	O
word-image	O
embeddings	O
in	O
proc	O
european	O
conf	O
on	O
machine	B
learning	B
weston	O
j	O
f	O
ratle	O
and	O
r	O
collobert	O
deep	B
learning	B
via	O
semisupervised	O
embedding	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
weston	O
j	O
and	O
c	O
watkins	O
support	B
vector	O
ma	O
multi-lcass	O
chines	O
in	O
esann	O
wiering	O
m	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	B
state-of-the-art	O
springer	O
wilkinson	O
d	O
and	O
s	O
yeung	O
conditional	O
simulation	O
from	O
highly	O
structured	O
gaussian	B
systems	O
with	O
application	O
to	O
blocking-mcmc	O
for	O
the	O
bayesian	B
analysis	O
of	O
very	O
large	O
linear	O
models	O
statistics	O
and	O
computing	O
williams	O
c	O
computation	O
with	O
infinite	O
networks	O
neural	O
computation	O
williams	O
c	O
a	O
mcmc	B
approach	O
to	O
hierarchical	O
mixture	B
modelling	O
in	O
s	O
a	O
solla	O
t	O
k	O
leen	O
and	O
k	O
-r	O
m	O
ller	O
nips	O
mit	O
press	O
williams	O
c	O
on	O
a	O
connection	O
between	O
kernel	B
pca	B
and	O
metric	B
multidimensional	B
scaling	I
machine	B
learning	B
j	O
williams	O
o	O
and	O
a	O
fitzgibbon	O
gaussian	B
process	I
implicit	O
surfaces	O
in	O
gaussian	B
processes	I
in	O
practice	O
williamson	O
s	O
and	O
z	O
ghahramani	O
probabilistic	O
models	O
for	O
data	O
combination	O
in	O
recommender	O
systems	O
in	O
nips	O
workshop	O
on	O
learning	B
from	O
multiple	O
sources	O
winn	O
j	O
and	O
c	O
bishop	O
variational	B
message	B
passing	I
j	O
of	O
machine	B
learning	B
research	O
wipf	O
d	O
and	O
s	O
nagarajan	O
a	O
new	O
view	O
of	O
automatic	B
relevancy	I
determination	I
in	O
nips	O
wipf	O
d	O
and	O
s	O
nagarajan	O
april	O
iterative	O
reweighted	O
and	O
methods	O
for	O
finding	O
sparse	B
solutions	O
j	O
of	O
selected	O
topics	O
in	O
signal	B
processing	I
issue	O
on	O
compressive	B
sensing	I
wipf	O
d	O
b	O
rao	O
and	O
s	O
nagarajan	O
latent	B
variable	O
bayesian	B
models	O
for	O
promoting	O
sparsity	B
ieee	O
transactions	O
on	O
information	B
theory	I
witten	O
d	O
r	O
tibshirani	O
and	O
t	O
hastie	O
a	O
penalized	O
matrix	O
decomposition	O
with	O
applications	O
to	O
sparse	B
principal	B
components	I
and	O
canonical	B
correlation	I
analysis	I
biostatistics	O
wolpert	O
d	O
stacked	O
generalization	B
neural	B
networks	I
wolpert	O
d	O
the	O
lack	O
of	O
a	O
priori	O
distinctions	O
between	O
learning	B
algorithms	O
neural	O
computation	O
wong	O
f	O
c	O
carter	O
and	O
r	O
kohn	O
efficient	O
estimation	O
of	O
models	O
covariance	B
biometrika	O
selection	O
wood	O
f	O
c	O
archambeau	O
j	O
gasthaus	O
l	O
james	O
and	O
y	O
w	O
teh	O
a	O
stochastic	O
memoizer	O
for	O
sequence	O
data	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
wright	O
r	O
s	O
nowak	O
and	O
m	O
figueiredo	O
sparse	B
reconstruction	O
by	O
separable	O
approximation	O
ieee	O
trans	O
on	O
signal	B
processing	I
wu	O
t	O
t	O
and	O
k	O
lange	O
coordinate	O
descent	O
algorithms	O
for	O
lasso	B
penalized	O
regression	B
ann	O
appl	O
stat	O
wu	O
y	O
h	O
tjelmeland	O
and	O
m	O
west	O
bayesian	B
cart	B
prior	O
structure	O
and	O
mcmc	B
computations	O
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
xu	O
f	O
and	O
j	O
tenenbaum	O
word	O
learning	B
as	O
bayesian	B
inference	B
psychological	O
review	O
xu	O
z	O
v	O
tresp	O
a	O
rettinger	O
and	O
k	O
kersting	O
social	O
network	O
mining	O
with	O
nonparametric	O
relational	O
models	O
in	O
acm	O
workshop	O
on	O
social	O
network	O
mining	O
and	O
analysis	O
xu	O
z	O
v	O
tresp	O
k	O
yu	O
and	O
h	O
-p	O
kriegel	O
infinite	O
hidden	B
relational	O
models	O
in	O
uai	O
xu	O
z	O
v	O
tresp	O
s	O
yu	O
k	O
yu	O
and	O
h	O
-p	O
kriegel	O
fast	O
inference	B
in	O
infinite	O
hidden	B
relational	O
models	O
in	O
workshop	O
on	O
mining	O
and	O
learning	B
with	O
graphs	O
xue	O
y	O
x	O
liao	O
l	O
carin	O
and	O
b	O
krishnapuram	O
multi-task	B
learning	B
for	O
classification	O
with	O
dirichlet	B
process	I
priors	O
j	O
of	O
machine	B
learning	B
research	O
yadollahpour	O
p	O
d	O
batra	O
and	O
g	O
shakhnarovich	O
diverse	O
mbest	O
solutions	O
in	O
mrfs	O
in	O
nips	O
workshop	O
on	O
disrete	O
optimization	B
in	O
machine	B
learning	B
yan	O
d	O
l	O
huang	O
and	O
m	O
i	O
jordan	O
fast	O
approximate	O
spectral	B
clustering	B
in	O
acm	O
conf	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
fast	O
feb	O
yang	O
a	O
a	O
ganesh	O
s	O
sastry	O
and	O
y	O
ma	O
algorithms	O
and	O
an	O
application	O
in	O
robust	B
face	B
recognition	I
a	O
review	O
technical	O
report	O
eecs	O
department	O
university	O
of	O
california	O
berkeley	O
bibliography	O
yang	O
c	O
r	O
duraiswami	O
and	O
l	O
david	O
efficient	O
kernel	B
machines	O
using	O
the	O
improved	O
fast	B
gauss	I
transform	I
in	O
nips	O
yang	O
s	O
b	O
long	O
a	O
smola	O
h	O
zha	O
and	O
z	O
zheng	O
collaborative	O
competitive	O
filtering	B
learning	B
recommender	O
using	O
context	O
of	O
user	O
choice	O
in	O
proc	O
annual	O
intl	O
acm	O
sigir	O
conference	O
yanover	O
c	O
o	O
schueler-furman	O
and	O
y	O
weiss	O
minimizing	O
and	O
learning	B
energy	O
functions	O
for	O
side-chain	O
prediction	O
in	O
recomb	O
yaun	O
g	O
-x	O
k	O
-w	O
chang	O
c	O
-j	O
hsieh	O
and	O
c	O
-j	O
lin	O
a	O
comparison	O
of	O
optimization	B
methods	O
and	O
software	O
for	O
large-scale	O
linear	O
classification	O
j	O
of	O
machine	B
learning	B
research	O
yedidia	O
j	O
w	O
t	O
freeman	O
and	O
y	O
weiss	O
understanding	O
belief	B
propagation	I
and	O
its	O
generalizations	O
in	O
intl	O
joint	O
conf	O
on	O
ai	B
yoshida	O
r	O
and	O
m	O
west	O
bayesian	B
learning	B
in	O
sparse	B
graphical	O
factor	B
models	O
via	O
annealed	O
entropy	B
j	O
of	O
machine	B
learning	B
research	O
younes	O
l	O
parameter	B
estimation	O
for	O
imperfectly	O
observed	O
gibbsian	O
fields	O
probab	O
theory	O
and	O
related	O
fields	O
yu	O
c	O
and	O
t	O
joachims	O
learning	B
structural	O
svms	O
with	O
latent	B
variables	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
yu	O
s	O
k	O
yu	O
v	O
tresp	O
k	O
h-p	O
and	O
m	O
wu	O
supervised	O
probabilistic	O
principal	B
component	I
analysis	O
in	O
proc	O
of	O
the	O
int	O
l	O
conf	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
yu	O
s	O
-z	O
and	O
h	O
kobayashi	O
practical	O
implementation	O
of	O
an	O
efficient	O
forward-backward	O
algorithm	O
for	O
an	O
explicit-duration	O
hidden	B
markov	B
model	I
ieee	O
trans	O
on	O
signal	B
processing	I
yuan	O
m	O
and	O
y	O
lin	O
model	B
selection	I
and	O
estimation	O
in	O
regression	B
with	O
grouped	O
variables	O
j	O
royal	O
statistical	O
society	O
series	O
b	O
y	O
lin	O
yuan	O
m	O
and	O
model	B
selection	I
and	O
estimation	O
in	O
the	O
gaussian	B
graphical	B
model	I
biometrika	O
yuille	O
a	O
cccp	B
algorithms	O
to	O
minimze	O
the	O
bethe	B
and	O
kikuchi	O
free	O
energies	O
convergent	O
alternatives	O
to	O
belief	B
propagation	I
neural	O
computation	O
yuille	O
a	O
and	O
a	O
rangarajan	O
concave-convex	B
procedure	I
the	O
neural	O
computation	O
yuille	O
a	O
and	O
s	O
zheng	O
compositional	O
noisy-logical	O
learning	B
in	O
intl	O
conf	O
on	O
machine	B
learning	B
yuille	O
a	O
l	O
and	O
x	O
he	O
probabilistic	O
models	O
of	O
vision	O
and	O
maxmargin	O
methods	O
frontiers	O
of	O
electrical	O
and	O
electronic	O
engineering	O
zellner	O
a	O
on	O
assessing	O
prior	O
distributions	O
and	O
bayesian	B
regression	B
analysis	O
with	O
g-prior	B
distributions	O
in	O
bayesian	B
inference	B
and	O
decision	B
techniques	O
studies	O
of	O
bayesian	B
and	O
econometrics	O
and	O
statistics	O
volume	O
north	O
holland	O
zhai	O
c	O
and	O
j	O
lafferty	O
a	O
study	O
of	O
smoothing	B
methods	O
for	O
language	B
models	I
applied	O
to	O
information	B
retrieval	I
acm	O
trans	O
on	O
information	B
systems	O
zhang	O
n	O
hierarchical	O
latnet	O
class	O
models	O
for	O
cluster	O
analysis	O
j	O
of	O
machine	B
learning	B
research	O
zhang	O
n	O
and	O
d	O
poole	O
exindependence	O
in	O
j	O
of	O
ploiting	O
causal	O
bayesian	B
network	O
inference	B
ai	B
research	O
zhang	O
t	O
adaptive	O
forwardbackward	O
greedy	O
algorithm	O
for	O
sparse	B
learning	B
with	O
linear	O
models	O
in	O
nips	O
zhang	O
x	O
t	O
graepel	O
and	O
r	O
herbrich	O
bayesian	B
online	B
learning	B
for	O
multi-label	O
and	O
multi-variate	O
performance	O
measures	O
in	O
aistatistics	O
zhao	O
j	O
-h	O
and	O
p	O
l	O
h	O
yu	O
november	O
fast	O
ml	O
estimation	O
for	O
the	O
mixture	B
of	O
factor	B
analyzers	O
via	O
an	O
ecm	B
algorithm	O
ieee	O
trans	O
on	O
neural	B
networks	I
zhao	O
p	O
and	O
b	O
yu	O
stagewise	O
j	O
of	O
machine	B
learning	B
re	O
lasso	B
search	O
zhou	O
h	O
d	O
karakos	O
s	O
khudanpur	O
a	O
andreou	O
and	O
c	O
priebe	O
on	O
projections	O
of	O
gaussian	B
distributions	O
using	O
maximum	O
likelihood	B
criteria	O
in	O
proc	O
of	O
the	O
workshop	O
on	O
information	B
theory	I
and	O
its	O
applications	O
zhou	O
m	O
h	O
chen	O
j	O
paisley	O
l	O
ren	O
g	O
sapiro	O
and	O
l	O
carin	O
non-parametric	O
bayesian	B
dictionary	B
learning	B
for	O
sparse	B
image	O
representations	O
in	O
nips	O
zhou	O
x	O
and	O
x	O
liu	O
the	O
em	B
algorithm	O
for	O
the	O
extended	O
finite	O
mixture	B
of	O
the	O
factor	B
analyzers	O
model	O
computational	O
statistics	O
and	O
data	O
analysis	O
zhu	O
c	O
s	O
n	O
y	O
wu	O
and	O
d	O
mumford	O
november	O
minimax	O
entropy	B
principle	O
and	O
its	O
application	O
to	O
texture	O
modeling	O
neural	O
computation	O
zhu	O
j	O
and	O
e	O
xing	O
conditional	O
in	O
intl	O
conf	O
topic	B
random	O
fields	O
on	O
machine	B
learning	B
zhu	O
l	O
y	O
chen	O
a	O
yuille	O
and	O
w	O
freeman	O
latent	B
hierarchical	O
structure	B
learning	B
for	O
object	B
detection	I
in	O
cvpr	O
zhu	O
m	O
and	O
a	O
ghodsi	O
automatic	O
dimensionality	O
selection	O
from	O
the	O
scree	B
plot	I
via	O
the	O
use	O
of	O
profile	O
likelihood	B
computational	O
statistics	O
data	O
analysis	O
zhu	O
m	O
and	O
a	O
lu	O
the	O
counterintuitive	O
non-informative	B
prior	O
for	O
the	O
bernoulli	B
family	B
j	O
statistics	O
education	O
zinkevich	O
m	O
online	O
convex	B
programming	O
and	O
generalized	O
infinitesimal	O
gradient	O
ascent	O
in	O
intl	O
conf	O
on	O
machine	B
learning	B
pp	O
a	O
zobay	O
o	O
mean	B
field	O
inference	B
for	O
the	O
dirichlet	B
process	I
mixture	B
model	I
electronic	O
j	O
of	O
statistics	O
zhao	O
p	O
g	O
rocha	O
and	O
b	O
yu	O
grouped	O
and	O
hierarchical	O
model	B
selection	I
through	O
composite	O
absolute	O
penalties	O
technical	O
report	O
uc	O
berkeley	O
zoeter	O
o	O
bayesian	B
generalized	B
linear	I
models	I
in	O
a	O
terabyte	O
world	O
in	O
proc	O
international	O
symposium	O
on	O
image	O
and	O
signal	B
processing	I
and	O
analysis	O
bibliography	O
zou	O
h	O
the	O
adaptive	B
lasso	B
j	O
of	O
the	O
and	O
its	O
oracle	O
properties	O
am	O
stat	O
assoc	O
nent	O
analysis	O
j	O
of	O
computational	O
and	O
graphical	O
statistics	O
cave	O
penalized	O
likelihood	B
models	O
annals	O
of	O
statistics	O
zou	O
h	O
and	O
t	O
hastie	O
regularization	B
and	O
variable	O
selection	O
via	O
the	O
elastic	B
net	I
j	O
of	O
royal	O
stat	O
soc	O
series	O
b	O
zou	O
h	O
t	O
hastie	O
and	O
r	O
tibshirani	O
on	O
the	O
of	O
freedom	O
of	O
the	O
lasso	B
annals	O
of	O
statistics	O
zou	O
h	O
t	O
hastie	O
and	O
r	O
tibshirani	O
sparse	B
principal	O
compo	O
zou	O
h	O
and	O
r	O
li	O
onestep	O
sparse	B
estimates	O
in	O
noncon	O
zweig	O
g	O
and	O
m	O
padmanabhan	O
exact	O
alpha-beta	O
computation	O
in	O
logarithmic	O
space	O
with	O
application	O
to	O
map	O
word	O
graph	B
construction	O
in	O
proc	O
intl	O
conf	O
spoken	O
lang	O
index	O
to	O
code	O
agglomdemo	O
amazonsellerdemo	O
arsdemo	O
arsenvelope	O
bayeschangeofvar	O
bayesttestdemo	O
beliefpropagation	O
bernoullientropyfig	O
besselk	O
betabinompostpreddemo	O
betacredibleint	O
betahpd	O
betaplotdemo	O
bimodaldemo	O
binaryfademotipping	O
binomdistplot	O
binomialbetaposteriordemo	O
bleildaperplexityplot	O
bolassodemo	O
boostingdemo	O
bootstrapdemober	O
cancerhighdimclassifdemo	O
cancerrateseb	O
casinodemo	O
centrallimitdemo	O
chowliutreedemo	O
coinsmodelseldemo	O
contoursssedemo	O
convexfnhand	O
cursedimensionality	O
demard	O
depnetfit	O
dirichlethistogramdemo	O
discreteprobdistfig	O
discrimanalysisdboundariesdemo	O
discrimanalysisfit	O
discrimanalysisheightweightdemo	O
discrimanalysispredict	O
dpmsampledemo	O
dtfit	O
dtreedemoiris	O
elasticdistortionsdemo	O
emloglikelihoodmax	O
fabiplotdemo	O
fisherdiscrimvoweldemo	O
fisheririsdemo	O
fisherldademo	O
fmgibbs	O
gammaplotdemo	O
gammarainfalldemo	O
gampdf	O
gaussheightweight	O
gaussimputationdemo	O
gaussinterpdemo	O
gaussinterpnoisydemo	O
gaussmissingfitem	O
gaussmissingfitgibbs	O
gaussplotdemo	O
generativevsdiscrim	O
geomridge	O
ggmfitdemo	O
ggmfithtf	O
ggmfitminfunc	O
ggmlassodemo	O
ggmlassohtf	O
gibbsdemoising	O
gibbsgaussdemo	O
ginidemo	O
gpnndemo	O
gprdemoard	O
gprdemochangehparams	O
gprdemomarglik	O
gprdemonoisefree	O
gpspatialdemolaplace	O
grouplassodemo	O
hclustyeastdemo	O
hingelossplot	O
hmmfilter	O
hmmfwdback	O
hmmlillypaddemo	O
hmmselfloopdist	O
hopfielddemo	O
huberlossdemo	O
icabasisdemo	O
icademo	O
icademouniform	O
isingimagedenoisedemo	O
kalmanfilter	O
kalmantrackingdemo	O
kernelbinaryclassifdemo	O
kernelregrdemo	O
kernelregressiondemo	O
klfwdreversemixgauss	O
klpqgauss	O
kmeansheightweight	O
kmeansyeastdemo	O
knnclassifydemo	O
knnvoronoi	O
kpcascholkopf	O
lassopathprostate	O
lassoshooting	O
leastsquaresprojection	O
linregallsubsetsgraycodedemo	O
linregbayescaterpillar	O
linregcensoredschmeehahndemo	O
indexes	O
linregebmodelselvsn	O
linregonlinedemokalman	O
linregpolylassodemo	O
linregpolyvsdegree	O
linregpolyvsn	O
linregpolyvsregdemo	O
linregpostpreddemo	O
linregrbfdemo	O
linregrobustdemocombined	O
lmsdemo	O
logregfit	O
logreglaplacegirolamidemo	O
logregmultinomkerneldemo	O
logregsatdemo	O
logregsatdemobayes	O
logregsatmhdemo	O
logregxordemo	O
logsumexp	O
lossfunctionfig	O
lsicode	O
marsdemo	O
mcaccuracydemo	O
mcestimatepi	O
mcmcgmmdemo	O
mcquantiledemo	O
mcstatdist	O
mimixeddemo	O
mixbermnistem	O
mixbetademo	O
mixexpdemo	O
mixexpdemoonetomany	O
mixgaussdemofaithful	O
mixgaussliksurfacedemo	O
mixgaussmlvsmap	O
mixgaussoverrelaxedemdemo	O
mixgaussplotdemo	O
mixgausssingularity	O
mixgaussvbdemofaithful	O
mixppcademonetlab	O
mixstudentbankruptcydemo	O
mlppriorsdemo	O
mlpregevidencedemo	O
mlpreghmcdemo	O
multilevellinregdemo	O
mutualinfoallpairsmixed	O
naivebayesbowdemo	O
naivebayesfit	O
naivebayespredict	O
netflixresultsplot	O
newsgroupsvisualize	O
newtonsmethodminquad	O
newtonsmethodnonconvex	O
ngramplot	O
normalgammapenaltyplotdemo	O
normalgammathresholdplotdemo	O
numbersgame	O
pagerankdemo	O
pagerankdemopmtk	O
paretoplot	O
pcademoheightweight	O
pcaemstepbystep	O
pcaimagedemo	O
pcaoverfitdemo	O
pcapmtk	O
pfcolortrackerdemo	O
poissonplotdemo	O
postdensityintervals	O
prhand	O
probitplot	O
probitregdemo	O
prostatecomparison	O
prostatesubsets	O
quantiledemo	O
rbpfmaneuverdemo	O
rbpfslamdemo	O
rdafit	O
regtreesurfacedemo	O
rejectionsamplingdemo	O
relevancenetworknewsgroupdemo	O
residualsdemo	O
ridgepathprostate	O
riskfngauss	O
robustdemo	O
robustpriordemo	O
sademopeaks	O
samplecdf	O
samplingdistgaussshrinkage	O
sensorfusionunknownprec	O
seqlogodemo	O
shrinkagedemobaseball	O
shrinkcov	O
shrinkcovdemo	O
shrunkencentroidsfit	O
shrunkencentroidssrbctdemo	O
shuffleddigitsdemo	O
sigmoidlowerbounds	O
sigmoidplot	O
simpsonsparadoxgraph	O
smoothingkernelplot	O
sparsa	B
sparsedictdemo	O
sparsennetdemo	O
sparsepostplot	O
sparsesensingdemo	O
spectralclusteringdemo	O
splinebasisdemo	O
ssmtimeseriessimple	O
steepestdescentdemo	O
stickbreakingdemo	O
studentlaplacepdfplot	O
subgradientplot	O
subsupergaussplot	O
surfacefitdemo	O
svdimagedemo	O
svmcgammademo	O
tanhplot	O
trueskilldemo	O
index	O
to	O
code	O
trueskillplot	O
unigaussvbdemo	O
varembound	O
variableelimination	O
visdirichletgui	O
visualizealarmnetwork	O
vqdemo	O
wiplotdemo	O
index	O
to	O
keywords	O
loss	B
a	B
star	I
search	I
absorbing	B
state	B
accept	B
action	B
action	B
nodes	B
action	B
space	I
actions	B
activation	B
active	B
learning	B
active	B
set	I
active	B
set	I
activity	B
recognition	I
adagrad	B
adaline	B
adaptive	B
basis-function	I
model	I
adaptive	B
importance	B
sampling	I
adaptive	B
lasso	B
adaptive	B
mcmc	B
adaptive	B
rejection	I
metropolis	I
sampling	I
adaptive	B
rejection	B
sampling	I
add-one	B
smoothing	B
adf	B
adjacency	B
matrix	I
adjust	B
for	I
adjusted	B
rand	B
index	I
admissible	B
admixture	B
mixture	B
adsense	B
adwords	B
affinity	B
propagation	I
agglomerative	B
clustering	B
agglomerative	B
hierarchical	B
clustering	B
aha	B
ai	B
aic	B
akaike	B
information	B
criterion	I
alarm	B
network	I
alignment	B
all	B
pairs	I
alleles	B
alpha	B
divergence	I
alpha	B
expansion	I
alpha-beta	B
swap	I
alternative	B
hypothesis	I
analysis	B
view	I
analysis-synthesis	B
ancestors	B
ancestral	B
graph	B
ancestral	B
sampling	I
and-or	B
graphs	I
annealed	B
importance	B
sampling	I
annealing	B
annealing	B
importance	B
sampling	I
anova	B
anti-ferromagnets	B
aperiodic	B
approximate	B
inference	B
approximation	B
error	I
ard	B
ard	B
kernel	B
area	B
under	I
the	I
curve	I
arma	B
array	B
cgh	I
association	B
rules	B
associative	B
associative	B
markov	B
network	I
associative	B
memory	I
associative	B
mrf	B
assumed	O
density	O
filter	O
assumed	O
density	O
filtering	B
asymptotically	B
normal	B
asymptotically	B
optimal	I
asynchronous	B
updates	I
atom	B
atomic	B
bomb	I
attractive	B
mrf	B
attributes	B
auc	B
audio-visual	B
speech	B
recognition	I
augmented	B
dag	B
auto-encoder	B
auto-encoders	B
auto-regressive	B
hmm	B
autoclass	B
autocorrelation	B
function	I
automatic	B
relevance	I
determination	I
automatic	B
relevancy	I
determination	I
automatic	B
speech	B
recognition	I
automatic	B
speech	B
recognition	I
auxiliary	B
function	I
auxiliary	B
variables	I
average	B
link	I
clustering	B
average	B
precision	B
average	B
precision	B
at	I
k	I
axis	B
aligned	I
axis	B
parallel	I
splits	I
back-propagation	B
backdoor	B
path	B
backfitting	O
background	B
knowledge	I
backoff	B
smoothing	B
backpropagation	B
backpropagation	B
algorithm	I
backslash	B
operator	I
backwards	B
selection	I
bag	B
of	I
words	I
bag-of-characters	B
bag-of-words	B
bagging	B
bandwidth	B
barren	B
node	I
removal	I
bart	B
barzilai-borwein	B
base	B
distribution	I
base	B
learner	I
base	B
measure	I
base	B
rate	B
fallacy	I
basic	B
feasible	I
solution	I
basis	B
function	I
expansion	I
basis	B
functions	I
basis	B
pursuit	I
denoising	I
batch	B
baum-welch	B
bayes	B
ball	I
algorithm	I
bayes	B
decision	B
rule	I
bayes	B
estimator	B
index	O
to	O
keywords	O
bayes	B
factor	B
bayes	B
model	I
averaging	I
bayes	B
point	I
bayes	B
risk	B
bayes	B
rule	I
bayes	B
theorem	I
bayesian	B
xxvii	O
bayesian	B
adaptive	I
regression	B
trees	I
bayesian	B
factor	B
regression	B
bayesian	B
hierarchical	B
clustering	B
bayesian	B
information	B
criterion	I
bayesian	B
ipf	B
bayesian	B
lasso	B
bayesian	B
model	B
selection	I
bayesian	B
network	I
structure	B
learning	B
bayesian	B
networks	I
bayesian	B
occam	O
s	O
razor	O
bayesian	B
statistics	I
bde	B
bdeu	B
beam	B
search	I
belief	B
networks	I
belief	B
propagation	I
belief	B
state	B
belief	B
state	B
mdp	B
belief	B
updating	I
bell	B
curve	I
berkson	O
s	O
paradox	O
bernoulli	B
bernoulli	B
product	I
model	I
bernoulli-gaussian	B
bessel	B
function	I
beta	B
distribution	I
beta	B
function	I
beta	B
process	I
beta-binomial	B
bethe	B
bethe	B
energy	B
functional	I
bethe	B
free	B
energy	I
bfgs	B
bhattacharya	B
distance	I
bi-directed	B
graph	B
bias	B
bias	B
term	I
bias-variance	B
tradeoff	I
bic	B
biclustering	B
big	B
data	I
bigram	B
model	I
binary	O
classification	O
binary	B
entropy	B
function	I
binary	B
independence	I
model	I
binary	B
mask	I
binary	B
tree	B
bing	B
binomial	B
binomial	B
coefficient	I
binomial	B
distribution	I
binomial	B
regression	B
binomialboost	B
bio	B
biosequence	B
analysis	I
bipartite	B
graph	B
biplot	B
birth	B
moves	I
bisecting	B
k-means	I
bits	B
bits-back	B
black	B
swan	I
paradox	I
black-box	B
blackwell-macqueen	B
blank	B
slate	I
blind	B
signal	I
separation	I
blind	B
source	I
separation	I
blocked	B
gibbs	B
sampling	I
blocking	B
gibbs	B
sampling	I
bloodtype	B
bolasso	B
boltzmann	B
distribution	I
boltzmann	B
machine	I
bond	B
variables	I
boosting	B
boosting	B
bootstrap	B
bootstrap	B
filter	O
bootstrap	B
lasso	B
bootstrap	B
resampling	I
borrow	B
statistical	I
strength	I
bottleneck	B
bottleneck	B
layer	I
bound	B
optimization	B
box	B
constraints	I
box-muller	B
boxcar	B
kernel	B
boyen-koller	B
bp	B
bpdn	B
bradley	B
terry	I
branch	B
and	I
bound	I
branching	B
factor	B
bridge	B
regression	B
brownian	B
motion	I
bucket	B
elimination	I
bugs	B
buried	B
markov	B
models	I
burn-in	B
phase	B
burned	B
in	I
burstiness	B
bursty	B
calculus	B
of	I
variations	I
calibration	B
candidate	B
method	I
canonical	B
correlation	I
analysis	I
canonical	B
form	I
canonical	B
link	B
function	I
canonical	B
parameters	I
cardinality	B
constraints	I
cart	B
cartesian	B
cascade	B
case	B
analysis	I
categorical	B
categorical	B
pca	B
categorical	B
variables	I
cauchy	B
causal	B
markov	B
assumption	I
causal	B
models	I
causal	B
mrf	B
causal	B
networks	I
causal	B
sufficiency	I
causality	B
cca	B
cccp	B
cd	B
cdf	B
censored	B
regression	B
censored	B
regression	B
indexes	O
centering	B
matrix	I
central	B
composite	I
design	I
central	B
interval	I
central	B
limit	I
theorem	I
central	B
moment	I
central-limit	B
theorem	I
centroid	B
centroids	B
certainty	B
factors	B
chain	B
graph	B
chain	B
rule	I
chance	B
nodes	B
change	B
of	I
variables	I
channel	B
coding	I
chapman-kolmogorov	B
characteristic	B
length	I
scale	I
cheeseman-stutz	B
approximation	I
chi-squared	B
distribution	I
chi-squared	B
statistic	I
children	B
chinese	B
restaurant	I
process	I
chip-seq	B
cholesky	B
decomposition	I
chomsky	B
normal	B
form	I
chordal	B
chordal	B
graph	B
chow-liu	B
algorithm	I
ci	B
circuit	B
complexity	I
city	B
block	I
distance	I
clamped	B
phase	B
clamped	B
term	I
clamping	B
class	B
imbalance	I
class-conditional	B
density	I
classical	B
classical	B
statistics	I
classification	O
classification	O
and	O
regression	B
trees	O
clausal	B
form	I
clause	B
click-through	B
rate	B
clique	B
cliques	B
closing	B
the	I
loop	B
closure	B
cluster	B
variational	I
method	I
clustering	B
clustering	B
clusters	B
clutter	B
problem	I
co-clustering	B
co-occurrence	B
matrix	I
co-parents	B
coarse-to-fine	O
grid	O
cocktail	B
party	I
problem	I
coclustering	B
codebook	B
collaborative	O
filtering	B
collapsed	B
gibbs	B
sampler	I
collapsed	B
gibbs	B
sampling	I
collapsed	B
particles	I
collect	B
evidence	B
collect-to-root	B
collider	B
colt	B
committee	B
method	I
commutative	B
semi-ring	I
commutative	B
semiring	I
compactness	B
compelled	B
edges	B
complementary	B
prior	I
complete	B
complete	B
data	I
complete	B
data	I
assumption	I
complete	B
data	I
log	I
likelihood	B
complete	B
link	I
clustering	B
completing	B
the	I
square	I
composite	B
likelihood	B
compressed	B
sensing	I
compressive	B
sensing	I
computation	B
tree	B
computational	B
learning	B
theory	I
computationalism	B
concave	B
concave-convex	B
procedure	I
concentration	B
matrix	I
concentration	B
parameter	B
concept	B
concept	B
learning	B
condensation	B
conditional	B
entropy	B
conditional	B
gamma	B
poisson	B
conditional	B
gaussian	B
conditional	B
independence	I
conditional	B
likelihood	B
conditional	B
logit	B
model	I
conditional	B
probability	I
conditional	B
probability	I
distribution	I
conditional	B
probability	I
tables	I
conditional	O
random	O
field	O
conditional	O
random	O
fields	O
conditional	O
topic	B
random	O
field	O
conditionally	B
conjugate	I
conditionally	B
independent	I
conditioning	B
conditioning	B
case	I
conductance	B
confidence	O
interval	O
confidence	O
intervals	O
confounder	B
confounders	B
confounding	B
variable	I
confusion	B
matrix	I
conjoint	B
analysis	I
conjugate	B
gradients	I
conjugate	B
prior	I
conjugate	B
priors	I
conjunctive	B
normal	B
form	I
connectionism	B
consensus	B
sequence	I
conservation	B
of	I
probability	I
mass	I
consistent	B
consistent	B
estimator	B
consistent	B
estimators	I
constant	B
symbols	I
constraint	B
satisfaction	I
problems	I
constraint-based	B
approach	I
content	B
addressable	I
memory	I
context	B
free	I
grammar	I
context	O
specific	O
independence	O
context-specific	O
independence	O
contextual	B
bandit	I
contingency	B
table	I
continuation	B
method	I
contrastive	B
divergence	I
contrastive	B
term	I
control	B
signal	I
converge	B
convex	B
index	O
to	O
keywords	O
convex	B
belief	B
propagation	I
convex	B
combination	I
convex	B
hull	I
convolutional	B
dbns	I
convolutional	B
neural	I
nets	I
convolutional	B
neural	B
network	I
cooling	B
schedule	B
corpus	B
correlated	B
topic	B
model	I
correlation	B
coefficient	I
correlation	B
matrix	I
correspondence	B
cosine	B
similarity	I
cost-benefit	O
analysis	O
coupled	B
hmm	B
covariance	B
covariance	B
graph	B
covariance	B
matrix	I
covariance	B
selection	I
covariates	B
cpd	B
cpts	B
cramer-rao	B
inequality	I
cramer-rao	B
lower	I
bound	I
credible	B
interval	I
crf	B
critical	B
temperature	B
critical	B
value	I
cross	B
entropy	B
cross	B
over	I
rate	B
cross	B
validation	I
cross-entropy	B
cross-language	B
information	B
retrieval	I
crosscat	B
crowd	B
sourcing	I
crp	B
ctr	B
cubic	B
spline	B
cumulant	B
function	I
cumulants	B
cumulative	B
distribution	I
function	I
curse	B
of	I
dimensionality	I
curved	B
exponential	B
family	B
cutting	B
plane	I
cv	B
cycle	B
cyclic	B
permutation	I
property	I
d-prime	B
d-separated	B
dace	B
dag	B
damped	B
updates	I
damping	B
dasher	B
data	B
association	I
data	B
augmentation	I
data	B
compression	I
data	B
fragmentation	I
data	B
fusion	I
data	B
overwhelms	I
the	I
prior	I
data-driven	B
mcmc	B
data-driven	B
proposals	I
dbm	B
dbn	B
dcm	B
dct	B
death	B
moves	I
debiasing	B
decision	B
decision	B
boundary	I
decision	B
diagram	I
decision	B
nodes	B
decision	B
problem	I
decision	B
procedure	I
decision	B
rule	I
decision	B
trees	I
decoding	B
decomposable	B
decomposable	B
graphs	I
decomposes	B
deebn	B
deebns	B
deep	B
deep	B
auto-encoders	B
deep	B
belief	I
network	I
deep	B
boltzmann	B
machine	I
deep	B
directed	B
networks	I
deep	B
learning	B
deep	B
networks	I
defender	O
s	O
fallacy	O
deflated	O
matrix	O
degeneracy	B
problem	I
degenerate	B
degree	B
degrees	B
of	I
freedom	I
deleted	B
interpolation	I
delta	B
rule	I
dendrogram	B
denoising	B
auto-encoder	B
dense	B
stereo	I
reconstruction	I
density	B
estimation	I
dependency	B
network	I
dependency	B
networks	I
derivative	O
free	O
filter	O
descendants	B
descriptive	B
design	B
matrix	I
detailed	B
balance	I
detailed	B
balance	I
equations	I
determinism	B
deterministic	B
annealing	B
deviance	B
dgm	B
diagonal	B
diagonal	B
covariance	B
lda	B
diagonal	B
lda	B
diameter	B
dictionary	B
digamma	B
digital	B
cameras	I
dimensionality	B
reduction	I
dirac	B
delta	I
function	I
dirac	B
measure	I
dirchlet	B
process	I
direct	B
posterior	I
probability	I
approach	I
directed	B
directed	B
acyclic	I
graph	B
directed	B
graphical	B
model	I
directed	B
local	I
markov	B
property	I
directed	B
mixed	I
graph	B
directed	B
mixed	I
graphical	B
model	I
dirichlet	B
dirichlet	B
compound	I
multinomial	B
dirichlet	B
distribution	I
dirichlet	B
multinomial	B
regression	B
lda	B
dirichlet	B
process	I
dirichlet	B
process	I
mixture	B
models	I
discontinuity	B
preserving	I
discounted	B
cumulative	I
gain	I
indexes	O
discrete	B
discrete	B
adaboost	I
discrete	B
choice	I
modeling	I
discrete	B
random	I
variable	I
discrete	B
with	I
probability	I
one	I
discretize	B
discriminability	B
discriminant	B
analysis	I
discriminant	B
function	I
discriminative	B
discriminative	B
classifier	O
discriminative	B
lda	B
discriminative	B
random	O
field	O
disease	B
mapping	I
disease	B
transmission	I
disparity	B
dispersion	B
parameter	B
dissimilarity	B
analysis	I
dissimilarity	B
matrix	I
distance	B
matrix	I
distance	B
transform	I
distorted	B
distortion	B
distribute	B
evidence	B
distribute-from-root	B
distributed	B
encoding	I
distributed	B
representation	I
distributional	B
particles	I
distributive	B
law	I
divisive	B
clustering	B
dna	B
sequences	I
do	B
calculus	I
document	O
classification	O
document	O
classification	O
domain	B
adaptation	I
domain	B
adaptation	I
dominates	B
double	B
loop	B
algorithms	I
double	B
pareto	B
distribution	I
double	B
sided	I
exponential	I
drum	B
dual	B
decomposition	I
dual	B
variables	I
dummy	B
encoding	I
dyadic	B
dybn	B
dybns	B
dynamic	B
bayes	I
net	I
dynamic	B
bayesian	B
network	I
dynamic	B
linear	I
model	I
dynamic	B
programming	I
dynamic	B
topic	B
model	I
e	B
step	I
e-commerce	B
early	B
stopping	I
eb	B
ecm	B
ecme	B
ecoc	B
econometric	B
forecasting	I
economy	B
sized	I
svd	B
edge	B
appearance	I
probability	I
edges	B
edit	B
distance	I
eer	B
effective	B
sample	I
size	I
efficient	B
ipf	B
efficiently	B
pac-learnable	I
eigendecomposition	B
eigenfaces	B
eigengap	B
eigenvalue	B
spectrum	I
ekf	B
elastic	B
net	I
elimination	B
order	I
em	B
email	O
spam	B
filtering	B
embedding	B
empirical	B
bayes	I
empirical	B
distribution	I
empirical	B
measure	I
empirical	B
risk	B
empirical	B
risk	B
minimization	I
end	B
effector	I
energy	B
based	I
models	I
energy	B
function	I
energy	B
functional	I
ensemble	B
ensemble	B
learning	B
ensemble	B
learning	B
entanglement	B
entanglement	B
problem	I
entropy	B
entropy	B
ep	B
epanechnikov	B
kernel	B
epca	B
epigraph	B
epistemological	B
uncertainty	B
epoch	B
epsilon	B
insensitive	I
loss	B
function	I
epsr	B
equal	B
error	I
rate	B
equilibrium	B
distribution	I
equivalence	B
class	I
equivalent	B
kernel	B
equivalent	B
sample	I
size	I
erf	B
ergodic	B
erlang	B
distribution	I
erm	B
error	B
bar	I
error	B
correcting	I
codes	I
error	B
correction	I
error	B
function	I
error	B
signal	I
error-correcting	B
output	I
codes	I
ess	B
essential	B
graph	B
estimated	B
potential	I
scale	I
reduction	I
estimator	B
euclidean	B
distance	I
evidence	B
evidence	B
procedure	I
evolutionary	B
mcmc	B
exchangeable	B
exclusive	B
or	I
expectation	B
correction	I
expectation	B
maximization	I
expectation	B
proagation	I
expectation	B
propagation	I
expectation	B
propagation	I
expected	B
complete	B
data	I
log	I
likelihood	B
expected	O
profit	O
expected	B
sufficient	B
statistics	I
expected	B
value	I
explaining	B
away	I
explicit	B
duration	I
hmm	B
exploration-exploitation	B
index	O
to	O
keywords	O
exploratory	B
data	I
analysis	I
exponential	B
cooling	B
schedule	B
exponential	B
distribution	I
exponential	B
family	B
exponential	B
family	B
harmonium	B
exponential	B
family	B
pca	B
exponential	B
loss	B
exponential	B
power	I
distribution	I
extended	O
kalman	O
filter	O
extension	B
external	O
field	O
f	B
score	I
score	O
fa	B
face	B
detection	I
face	B
detector	I
face	B
recognition	I
facebook	B
factor	B
factor	B
analysis	I
factor	B
analysis	I
distance	I
factor	B
graph	B
factor	B
loading	I
matrix	I
factorial	B
hmm	B
factorial	B
prior	I
factors	B
faithful	B
false	B
alarm	I
false	B
alarm	I
rate	B
false	B
discovery	I
rate	B
false	B
negative	I
false	B
positive	I
false	B
positive	I
rate	B
family	B
family	B
marginal	I
fan-in	B
fantasy	B
data	I
farthest	B
point	I
clustering	B
fast	B
fourier	I
transform	I
fast	B
gauss	I
transform	I
fast	B
ica	B
fast	B
iterative	I
shrinkage	B
thesholding	I
algorithm	I
fastslam	B
fat	B
hand	I
fault	B
diagnosis	I
feature	B
construction	I
feature	B
extraction	I
feature	B
function	I
feature	B
induction	B
feature	B
maps	I
feature	B
matrix	I
feature	B
selection	I
feature-based	B
clustering	B
features	B
feedback	B
loops	I
feedforward	B
neural	B
network	I
ferro-magnets	B
fft	B
fields	O
of	O
experts	O
fill-in	O
edges	B
filtering	B
filtering	B
finite	O
difference	O
matrix	O
finite	O
mixture	B
model	I
first-order	O
logic	O
fisher	B
information	B
fisher	B
information	B
matrix	I
fisher	B
kernel	B
fisher	B
scoring	I
method	I
fisher	O
s	O
linear	B
discriminant	B
analysis	I
fista	B
fit-predict	O
cycle	B
fixed	O
effect	O
fixed	B
lag	B
smoothing	B
fixed	O
point	O
flat	O
clustering	B
flda	B
flow	O
cytometry	O
folds	B
forest	B
forward	B
stagewise	I
additive	I
modeling	I
forward	B
stagewise	I
linear	B
regression	B
forwards	B
kl	I
forwards	B
model	I
forwards	B
selection	I
forwards-backwards	B
forwards-backwards	B
algorithm	I
founder	B
model	I
founder	B
variables	I
fourier	B
basis	I
fraction	B
of	I
variance	B
explained	I
free	B
energy	I
free-form	B
optimization	B
frequent	B
itemset	I
mining	I
frequentist	B
frequentist	B
statistics	I
frobenius	B
norm	I
frustrated	B
frustrated	B
system	I
full	B
full	B
conditional	I
function	B
approximation	I
functional	B
data	I
analysis	I
functional	B
gradient	B
descent	I
furthest	B
neighbor	I
clustering	B
fused	B
lasso	B
fuzzy	B
clustering	B
fuzzy	B
set	I
theory	I
g-prior	B
game	B
against	I
nature	I
game	B
theory	I
gamma	B
gamma	B
distribution	I
gamma	B
function	I
gap	B
gap	B
statistic	I
gating	B
function	I
gauss-seidel	B
gaussian	B
gaussian	B
approximation	I
gaussian	B
bayes	I
net	I
gaussian	B
copulas	I
gaussian	B
graphical	B
models	I
gaussian	B
kernel	B
gaussian	B
mixture	B
model	I
gaussian	B
mrf	B
gaussian	B
process	I
gaussian	B
processes	I
gaussian	B
random	O
fields	O
gaussian	B
rbm	B
gaussian	B
scale	I
mixture	B
gaussian	B
sum	O
filter	O
gda	B
gee	B
gem	B
gene	O
finding	O
gene	O
finding	O
gene	B
knockout	I
experiment	I
indexes	O
gene	B
microarrays	I
generalization	B
generalization	B
error	I
generalization	B
gradient	I
generalize	B
generalized	B
additive	I
model	I
generalized	B
belief	B
propagation	I
generalized	B
cross	B
validation	I
generalized	B
eigenvalue	I
generalized	B
em	B
generalized	B
estimating	I
equations	I
generalized	B
linear	I
mixed	I
effects	I
model	I
generalized	B
linear	I
model	I
generalized	B
linear	I
models	I
generalized	O
pseudo	O
bayes	O
filter	O
generalized	B
t	I
distribution	I
generate	B
and	I
test	I
generative	B
approach	I
generative	O
classifier	O
generative	B
pre-training	I
generative	B
weights	I
genetic	B
algorithms	I
genetic	B
linkage	I
analysis	I
genome	B
genotype	B
geometric	B
distribution	I
gibbs	B
distribution	I
gibbs	B
sampler	I
gibbs	B
sampling	I
gini	B
index	I
gist	B
gittins	B
indices	I
glasso	B
glauber	B
dynamics	I
glm	B
glmm	B
glmnet	B
global	B
balance	I
equations	I
global	B
convergence	I
global	B
localization	I
global	B
markov	B
property	I
global	B
minimum	I
global	B
prior	I
parameter	B
independence	I
globally	B
normalized	I
gm	B
gmm	B
gp-lvm	B
gps	B
gpus	B
gradient	B
boosting	B
gradient	B
descent	I
gram	B
matrix	I
grammars	B
grandmother	B
cells	I
graph	B
graph	B
cuts	I
graph	B
laplacian	I
graph	B
surgery	I
graph-guided	B
fused	B
lasso	B
graphcuts	B
graphical	B
lasso	B
graphical	B
model	I
graphical	B
models	I
xxviii	O
gray	B
code	I
greatest	B
common	I
divisor	I
greedy	B
equivalence	I
search	I
ground	B
network	I
ground	B
states	I
group	B
lasso	B
grouping	B
effect	I
gumbel	B
hadamard	B
product	I
haldane	B
prior	I
ham	B
hamiltonian	B
mcmc	B
hammersley-clifford	B
hamming	B
distance	I
handwriting	B
recognition	I
haplotype	B
hard	B
clustering	B
hard	B
em	B
hard	B
thresholding	I
harmonic	B
mean	B
harmonium	B
hastings	B
correction	I
hat	B
matrix	I
hdi	B
heat	B
bath	I
heavy	B
ball	I
method	I
heavy	B
tails	I
hellinger	B
distance	I
helmholtz	B
free	B
energy	I
hessian	B
heteroscedastic	B
lda	B
heuristics	B
hidden	B
hidden	B
layer	I
hidden	B
markov	B
model	I
hidden	B
nodes	B
hidden	B
semi-markov	B
model	I
hidden	B
units	I
hidden	B
variable	I
hidden	B
variables	I
hierarchical	B
adaptive	B
lasso	B
hierarchical	B
bayesian	B
model	I
hierarchical	B
bayesian	B
models	I
hierarchical	B
clustering	B
hierarchical	B
dirichlet	B
process	I
hierarchical	B
hmm	B
hierarchical	B
latent	B
class	I
model	I
hierarchical	B
mixture	B
of	I
experts	I
high	B
throughput	I
high	B
variance	B
estimators	I
highest	B
density	I
interval	I
highest	B
posterior	I
density	I
hill	B
climbing	I
hindsight	B
hinge	B
loss	B
hinton	B
diagram	I
hinton	B
diagrams	I
histogram	B
hit	B
rate	B
hmm	B
hmm	B
filter	O
hmms	B
hoeffding	O
s	O
inequality	O
homogeneous	B
homotopy	B
hopfield	O
network	O
horizon	B
horn	B
clauses	I
hpd	B
hsmm	B
huber	B
loss	B
hugin	B
hungarian	B
algorithm	I
hybrid	B
mcmc	B
hybrid	B
monte	B
carlo	I
hybrid	B
systems	I
index	O
to	O
keywords	O
hyper-parameters	B
hypothesis	B
space	I
i-map	B
i-projection	B
ica	B
ida	B
identifiable	O
identifiable	O
in	O
the	O
limit	O
iff	B
iid	B
ill-conditioned	B
image	O
classification	O
image	B
compression	I
image	B
denoising	I
image	B
inpainting	I
image	B
segmentation	I
image	B
tagging	I
imm	B
implicit	B
feedback	I
importance	B
sampling	I
importance	B
weights	I
impression	B
log	I
improper	B
prior	I
imputation	B
imputation	B
posterior	I
in-degree	B
inclusion	B
probabilities	I
incremental	B
em	B
independence	B
sampler	I
independent	B
and	I
identically	I
distributed	I
independent	B
component	I
analysis	I
indicator	B
function	I
induced	B
width	I
induction	B
inductive	B
bias	B
infer	O
net	O
inference	B
infinite	O
hidden	B
relational	O
model	O
infinite	O
hmm	B
infinite	O
mixture	B
models	O
infinite	O
relational	O
model	O
influence	O
diagram	O
influence	O
model	O
infomax	B
information	B
information	B
arc	I
information	B
bottleneck	B
information	B
extraction	I
information	B
filter	O
information	B
form	I
information	B
gain	I
information	B
inequality	I
information	B
projection	B
information	B
retrieval	I
information	B
theory	I
inheritance	B
model	I
inner	B
approximation	I
innovation	B
inside	B
outside	I
inside-outside	B
algorithm	I
instance-based	B
learning	B
integrate	B
out	I
integrated	B
likelihood	B
integrated	B
risk	B
intensive	B
care	I
unit	I
inter-causal	B
reasoning	I
interaction	B
effects	I
interactive	B
multiple	I
models	I
interest	B
point	I
detector	I
interpolate	B
interpolated	B
kneser-ney	I
interpolator	B
interval	B
censored	I
interventional	B
data	I
interventions	B
intrinsic	O
gaussian	B
random	O
field	O
invariant	B
invariant	B
distribution	I
invariant	B
features	B
inverse	B
chi-squared	B
distribution	I
inverse	B
gamma	B
inverse	B
gamma	B
inverse	B
gaussian	B
inverse	B
probability	I
transform	I
inverse	B
problem	I
inverse	B
problems	I
inverse	B
reinforcement	B
learning	B
inverse	B
wishart	B
inverted	B
index	I
inverted	B
indices	I
ip	B
ipf	B
iris	B
irls	B
irm	B
irreducible	B
ising	B
model	I
isotropic	B
iterated	B
ekf	B
iterative	B
conditional	I
modes	I
iterative	O
proportional	O
fitting	O
iterative	B
scaling	I
iterative	B
shrinkage	B
and	I
thresholding	I
algorithm	I
iterative	B
soft	B
thresholding	I
iteratively	B
reweighted	I
least	B
squares	I
jacobi	B
jacobian	B
jacobian	B
matrix	I
jags	B
jambayes	B
james	B
stein	I
estimator	B
james-stein	B
estimator	B
jc	B
penney	I
jeffreys	B
prior	I
jeffreys-lindley	B
paradox	I
jensen	O
s	O
inequality	O
jensen-shannon	B
divergence	I
jeopardy	B
jittered	B
jj	B
bound	I
joint	B
distribution	I
joint	B
probability	I
distribution	I
jta	B
jump	B
markov	B
linear	I
system	I
junction	B
tree	B
junction	B
tree	B
algorithm	I
junction	B
trees	I
k-centers	B
k-means	B
algorithm	I
k-means	O
k-medoids	B
algorothm	I
k-spectrum	B
kernel	B
algorithm	O
kalman	O
filter	O
kalman	B
gain	I
matrix	I
kalman	B
smoother	I
kalman	B
smoothing	B
karhunen	B
loeve	I
karl	B
popper	I
kde	B
kendall	O
s	O
kernel	B
kernel	B
density	B
estimation	I
kernel	B
density	I
estimator	B
kernel	B
function	I
kernel	B
machine	I
kernel	B
pca	B
kernel	B
regression	B
kernel	B
smoothing	B
kernel	B
trick	I
kernelised	B
feature	I
vector	I
kikuchi	B
free	B
energy	I
kinect	B
kinematic	B
tracking	B
kink	B
kl	B
divergence	I
kleene	B
star	I
knee	B
knn	B
knots	B
knowledge	B
base	I
knowledge	B
discovery	I
knowledge	B
engineering	I
kolmogorov	B
smirnov	I
kriging	B
kronecker	B
product	I
kruskal	O
s	O
algorithm	O
kullback-leibler	B
divergence	I
kurtosis	B
l-bfgs	B
pseudo-norm	O
regularization	B
loss	B
regularization	B
loss	B
norm	O
regularization	B
label	B
label	B
bias	B
label	B
switching	I
label	B
taxonomy	I
labeled	B
lda	B
lag	B
lagrange	B
multiplier	I
lagrange	B
multipliers	I
lagrangian	B
lagrangian	B
relaxation	I
lanczos	B
algorithm	I
language	B
model	I
language	B
modeling	I
language	B
models	I
laplace	B
laplace	B
approximation	I
laplace	B
distribution	I
laplace	B
s	O
rule	O
of	O
succession	O
lar	B
large	O
margin	B
classifier	O
large	B
margin	B
principle	I
lars	B
lasso	B
latent	B
indexes	O
latent	B
class	I
model	I
latent	B
crf	B
latent	B
dirichlet	B
allocation	I
latent	B
factors	B
latent	B
semantic	I
analysis	I
latent	B
semantic	I
indexing	I
latent	B
svms	I
latent	B
variable	I
models	I
lattice	B
lauritzen-spiegelhalter	B
lbp	B
lda	B
lda-hmm	B
ldpc	B
lds	B
leaf	B
leak	B
node	I
leapfrog	B
steps	I
learning	B
learning	B
curve	I
learning	B
rate	B
learning	B
to	I
learn	I
learning	B
to	I
rank	I
least	B
favorable	I
prior	I
least	B
mean	B
squares	I
least	B
squares	I
least	B
squares	I
boosting	B
leave	B
one	I
out	I
cross	B
validation	I
leave-one	B
out	I
cross	B
validation	I
leaves	B
left	B
censored	I
left-to-right	B
left-to-right	B
transition	B
matrix	I
leptokurtic	B
letor	B
level	B
sets	I
levenberg	B
marquardt	I
levinson-durbin	B
lg-ssm	B
likelihood	B
likelihood	B
equivalence	I
likelihood	B
equivalent	I
likelihood	B
principle	I
likelihood	B
ratio	I
likelihood	B
weighting	I
limited	B
memory	I
bfgs	B
limiting	B
distribution	I
line	B
minimization	I
line	B
search	I
linear	B
discriminant	B
analysis	I
linear	B
dynamical	I
system	I
linear	B
gaussian	B
linear	B
gaussian	B
system	I
linear	B
kernel	B
linear	B
program	I
linear	B
programming	I
relaxtion	I
linear	B
regression	B
linear	B
smoother	I
linear	B
threshold	I
unit	I
linear	B
trend	I
linear-gaussian	B
cpd	B
linear-gaussian	B
ssm	B
linearity	B
of	I
expectation	I
linearly	B
separable	I
link	B
farms	I
link	B
function	I
lisrel	B
listnet	B
lms	B
index	O
to	O
keywords	O
local	B
consistency	I
local	B
evidence	B
local	B
level	I
model	I
local	B
prior	I
parameter	B
independence	I
local	B
variational	I
approximation	I
localist	B
encoding	I
locally	B
decodable	I
locally	B
normalized	I
locally	B
weighted	I
regression	B
loess	B
log	B
partition	B
function	I
log-linear	B
log-loss	B
log-odds	B
ratio	I
log-sum-exp	B
logic	B
sampling	I
logical	B
reasoning	I
problems	I
logistic	B
logistic	B
distribution	I
logistic	B
normal	B
logistic	B
regression	B
logit	B
logitboost	B
long	B
tail	I
long	B
tails	I
loocv	B
look-ahead	B
rbpf	B
loop	B
loopy	B
belief	B
propagation	I
lorentz	B
loss	B
loss	B
function	I
loss	B
matrix	I
loss-augmented	B
decoding	B
loss-calibrated	B
inference	B
lossy	B
compression	I
low	B
density	I
parity	I
check	I
low-level	B
vision	I
lowess	B
lsa	B
lse	B
lsi	B
lvm	B
m	B
step	I
m-projection	B
machine	B
learning	B
macro-averaged	O
mahalanobis	B
distance	I
mammogram	B
maneuvering	B
target	I
tracking	B
manifest	B
map	B
estimate	I
mar	B
margin	B
margin	B
re-rescaling	O
marginal	B
distribution	I
marginal	B
likelihood	B
marginal	B
polytope	I
marginalizing	B
out	I
marginally	B
independent	I
marker	B
market	B
basket	I
analysis	I
markov	B
markov	B
assumption	I
markov	B
blanket	I
markov	B
chain	I
markov	B
chain	I
monte	B
carlo	I
markov	B
chain	I
monte	B
carlo	I
markov	B
decision	B
process	I
markov	B
equivalence	I
markov	B
equivalent	I
markov	B
logic	I
network	I
markov	B
mesh	I
markov	B
model	I
markov	B
models	I
markov	B
network	I
markov	B
random	O
field	O
markov	B
switching	I
models	I
mars	B
mart	B
master	B
matching	B
pursuit	I
matching	B
pursuits	I
matern	B
kernel	B
matlab	O
xxviii	O
matrix	B
completion	I
matrix	B
determinant	I
lemma	I
matrix	B
factorization	I
matrix	B
inversion	I
lemma	I
matrix	B
permanent	B
matrix	B
tree	B
theorem	I
max	O
flowmin	O
cut	O
max	B
margin	B
markov	B
networks	I
max	B
pooling	I
max	B
product	I
linear	I
programming	I
max-product	B
max-product	B
belief	B
propagation	I
maxent	B
maximal	B
branching	I
maximal	B
clique	B
maximal	B
information	B
coefficient	I
maximal	B
weight	I
bipartite	I
matching	I
maximizer	B
of	I
the	I
posterior	I
marginals	I
maximum	B
a	I
posteriori	I
maximum	B
entropy	B
maximum	B
entropy	B
classifier	O
maximum	B
entropy	B
markov	B
model	I
maximum	B
expected	I
utility	I
principle	I
maximum	B
likelihood	B
estimate	I
maximum	B
risk	B
maximum	B
weight	I
spanning	I
tree	B
mcar	B
mcem	B
mcmc	B
mdl	B
mdp	B
mds	B
mean	B
mean	B
absolute	I
deviation	I
mean	B
average	B
precision	B
mean	B
field	O
mean	B
field	O
energy	B
functional	I
mean	B
function	I
mean	B
precision	B
mean	B
reciprocal	I
rank	I
mean	B
squared	B
error	I
mechanical	B
turk	I
median	B
median	B
model	I
memm	B
memory-based	B
learning	B
mendelian	B
inheritance	I
mercer	B
kernel	B
mercer	O
s	O
theorem	O
message	B
passing	I
metric	B
metric	B
crf	B
metric	B
mrf	B
indexes	O
metropolis	B
hastings	I
metropolis-hastings	B
algorithm	I
mfcc	B
mh	B
mi	B
micro-averaged	O
microsoft	B
mini-batch	B
minimal	B
minimal	B
i-map	B
minimax	B
rule	I
minimum	B
description	I
length	I
minimum	B
entropy	B
prior	I
minimum	B
mean	B
squared	B
error	I
minimum	B
spanning	I
tree	B
minorize-maximize	B
misclassification	O
loss	B
misclassification	O
rate	B
misclassification	O
rate	B
missed	B
detection	I
missing	B
missing	B
at	I
random	I
missing	B
completely	I
at	I
random	I
missing	B
data	I
missing	B
data	I
problem	I
mixed	B
directed	B
graphs	I
mixed	B
membership	I
model	I
mixed	B
membership	I
stochastic	B
block	I
model	I
mixed	B
model	I
mixing	B
matrix	I
mixing	B
time	I
mixing	B
weights	I
mixture	B
mixture	B
density	I
network	I
mixture	B
model	I
mixture	B
of	I
conjugate	B
priors	I
mixture	B
of	I
experts	I
mixture	B
of	I
factor	B
analysers	I
mixture	B
of	I
gaussians	I
mixture	B
of	O
kalman	O
filters	O
mixture	B
of	I
trees	I
mixture	B
proposal	I
mle	B
mlp	B
mm	B
mmse	B
mnist	B
mobious	B
numbers	I
mode	B
model	B
based	I
clustering	B
model	B
selection	I
model	B
selection	I
consistent	B
model-based	O
approach	O
xxvii	O
model-based	B
clustering	B
moderated	B
output	I
modularity	O
xxviii	O
moe	B
moment	B
matching	I
moment	B
parameters	I
moment	B
projection	B
momentum	B
monks	B
monte	B
carlo	I
monte	B
carlo	I
em	B
monte	B
carlo	I
integration	I
monte	B
carlo	I
localization	I
moralization	B
motes	B
motif	B
mpca	B
mpe	B
mpm	B
mrf	B
mse	B
multi	O
label	B
classification	O
multi	B
net	I
multi-armed	B
bandit	I
multi-class	B
logistic	B
regression	B
multi-clust	B
multi-grid	B
techniques	I
multi-information	B
multi-label	O
classification	O
multi-layer	B
perceptron	B
multi-level	B
model	I
multi-level	B
modeling	I
multi-stage	B
multi-target	B
tracking	B
multi-task	B
feature	B
selection	I
multi-task	B
learning	B
multiclass	O
classification	O
multidimensional	B
scaling	I
multinomial	B
multinomial	B
coefficient	I
multinomial	B
logistic	B
regression	B
multinomial	B
pca	B
multinomial	B
probit	B
multinomial	B
regression	B
lda	B
multinomial	B
resampling	I
multinoulli	B
distribution	I
multiple	B
hypothesis	I
testing	I
multiple	B
hypothesis	I
tracking	B
multiple	B
imputation	B
multiple	B
kernel	B
learning	B
multiple	B
lda	B
multiple	B
output	I
model	I
multiple	B
random	I
restarts	I
multiple	B
restarts	I
multivariate	B
adaptive	I
regression	B
splines	I
multivariate	B
bernoulli	B
naive	I
bayes	I
multivariate	B
delta	I
method	I
multivariate	B
gamma	B
function	I
multivariate	B
gamma	B
function	I
multivariate	B
gaussian	B
multivariate	B
normal	B
multivariate	B
probit	B
multivariate	B
student	B
t	I
mutual	B
information	B
mutual	B
inhibition	I
mutually	B
independent	I
mvn	B
n-best	B
list	I
n-gram	B
n-gram	B
models	I
nadaraya-watson	B
naive	O
bayes	O
classifier	O
naive	O
bayes	O
classifiers	O
named	B
entity	I
extraction	I
nan	B
nats	B
natural	B
exponential	B
family	B
natural	B
gradient	I
natural	B
parameters	I
ndcg	B
nearest	O
centroids	B
classifier	O
nearest	O
medoid	O
classification	O
nearest	B
neighbor	I
nearest	B
neighbor	I
clustering	B
nearest	B
neighbor	I
data	B
association	I
nearest	B
shrunken	I
centroids	B
index	O
to	O
keywords	O
negative	B
binomial	B
negative	B
binomial	B
distribution	I
negative	B
examples	I
negative	B
log	I
likelihood	B
negative	B
transfer	I
negentropy	B
neighbors	B
neocognitron	B
nested	B
plate	I
nesterov	O
s	O
method	O
netflix	O
nettalk	B
neural	B
network	I
neural	B
networks	I
neutral	B
process	I
newton	O
s	O
algorithm	O
nhst	B
niw	B
nix	B
nll	B
nmar	B
nmf	B
no	B
forgetting	I
no	B
free	I
lunch	I
theorem	I
nodes	B
nodes	B
that	O
fire	O
together	O
should	O
wire	O
together	O
noise	O
floor	O
noisy-or	B
nominal	B
non-descendants	B
non-factorial	B
non-informative	B
non-negative	B
matrix	B
factorization	I
non-negative	B
sparse	B
coding	I
non-null	B
recurrent	B
non-parametric	B
bayes	I
non-parametric	B
bootstrap	B
non-parametric	B
bp	B
non-parametric	B
model	I
non-parametric	B
prior	I
non-serial	B
dynamic	B
programming	I
non-smooth	B
non-terminals	B
nonparanormal	B
norm	B
of	I
a	I
function	I
normal	B
normal	B
equation	I
normal	B
gamma	B
normal	B
inverse	I
chi-squared	I
normal-inverse-wishart	B
normalized	B
cut	I
normalized	B
discounted	B
cumulative	I
gain	I
normalized	B
mutual	B
information	B
not	B
missing	B
at	I
random	I
noun	B
phrase	I
chunking	I
np-complete	B
np-hard	B
classifier	O
nuisance	B
variables	I
null	B
hypothesis	I
null	B
hypothesis	I
significance	O
testing	O
number	B
game	I
numerical	O
underflow	O
object	B
detection	I
object	B
localization	I
observation	B
observation	B
model	I
observed	B
data	I
log	I
likelihood	B
observed	B
information	B
observed	B
information	B
matrix	I
occam	B
factor	B
occam	O
s	O
razor	O
occasionally	B
dishonest	I
casino	I
occupancy	B
grid	I
octave	O
xxviii	O
offline	B
oil	B
wild-catter	I
ols	B
omp	B
one-armed	B
bandit	I
one-hot	B
encoding	I
one-of-c	B
encoding	I
one-shot	B
decision	B
problem	I
one-standard	B
error	I
rule	I
one-step-ahead	B
predictive	B
density	I
one-versus-one	B
one-versus-the-rest	B
one-vs-all	B
online	B
em	B
online	B
gradient	B
descent	I
online	B
learning	B
ontological	B
uncertainty	B
ontology	B
open	B
class	I
open	B
directory	I
project	I
open	B
universe	I
optimal	B
action	B
optimism	B
of	I
the	I
training	I
error	I
optimization	B
ordered	B
markov	B
property	I
ordinal	B
ordinal	B
regression	B
ordinal	B
variables	I
ordinary	B
least	B
squares	I
ornstein-uhlenbeck	B
process	I
orthodox	B
statistics	I
orthogonal	B
least	B
squares	I
orthogonal	B
matching	B
pursuits	I
orthogonal	B
projection	B
out-degree	B
out-of-clique	B
query	I
outer	B
approximation	I
outliers	B
over-complete	B
overcomplete	B
overcounting	B
number	I
overdispersed	B
overfit	O
overfitting	O
overrelaxed	B
em	B
algorithm	I
p-value	B
pac	B
pagerank	B
paired	B
t-test	B
pairwise	B
independent	I
pairwise	B
markov	B
property	I
pairwise	B
mrf	B
parallel	B
tempering	I
parameter	B
parameter	B
expansion	I
parameter	B
modularity	I
parameter	B
sharing	I
parameter	B
tying	I
parametric	B
bootstrap	B
parametric	B
model	I
parents	B
pareto	B
distribution	I
part	B
of	I
speech	I
indexes	O
part	B
of	I
speech	I
tagging	I
partial	B
dependence	I
plot	I
partial	B
least	B
squares	I
partially	B
directed	B
acyclic	I
graph	B
partially	B
labeled	B
lda	B
partially	B
observed	I
markov	B
decision	B
process	I
partially	B
observed	I
mrf	B
particle	O
filtering	B
particle	O
filtering	B
partition	B
function	I
partitional	B
clustering	B
partitioned	B
inverse	I
formula	I
partitioning	B
partitions	B
of	I
the	I
integers	I
parzen	B
window	I
density	I
estimator	B
passing	O
a	O
flow	O
path	B
path	B
diagrams	I
pathologies	B
pattern	B
pattern	B
completion	I
pattern	B
recognition	I
pattern	B
search	I
pca	B
pcfg	B
pdag	B
pdf	B
pedigree	B
graph	B
peeling	B
algorithm	I
pegasos	B
penalized	B
least	B
squares	I
penalized	B
log	I
likelihood	B
penalized	B
splines	I
penetrance	B
model	I
perception-action	B
perceptron	B
perceptron	B
algorithm	I
perceptual	B
aliasing	I
perfect	B
intervention	I
perfect	B
map	I
period	B
permanent	B
perplexity	B
persistent	B
cd	B
persistent	B
contrastive	B
divergence	I
personalized	B
recommendation	I
personalized	O
spam	B
filtering	B
perturbation	B
theory	I
phase	B
phase	B
transition	I
phenotypes	B
phone	B
phonemes	B
phylogenetic	B
hmm	B
phylogenetic	B
tree	B
piecewise	B
polynomial	I
pilot	B
runs	I
pipeline	B
pitman-koopman-darmois	B
theorem	I
pitman-yor	B
process	I
plackett-luce	B
plates	B
platykurtic	B
pls	B
plsi	B
plug-in	B
plug-in	B
approximation	I
plutocracies	B
pmf	B
pmtk	O
xxviii	O
point	B
estimate	I
pointwise	B
approach	I
pointwise	B
marginal	I
credibility	I
intervals	I
pointwise	B
mutual	B
information	B
poisson	B
poisson	B
regression	B
polar	B
policy	B
polya	B
urn	I
polyak-ruppert	B
averaging	I
polynomial	B
kernel	B
polynomial	B
regression	B
polynomial	B
time	I
approximation	I
schemes	I
polysemy	B
polytree	B
pomdp	B
pooled	B
pooled	B
empirical	I
variance	B
population	B
minimizer	I
positive	O
definite	O
positive	O
definite	O
kernel	B
positive	B
examples	I
posterior	B
expected	I
loss	B
posterior	B
mean	B
posterior	B
median	B
posterior	B
mode	B
posterior	B
predictive	B
density	I
posterior	B
predictive	B
distribution	I
potential	B
function	I
potts	B
model	I
power	B
law	I
power	B
method	I
ppca	B
precision	B
precision	B
at	I
k	I
precision	B
matrix	I
precision	B
recall	B
curve	I
predict-update	B
cycle	B
predict-update-project	B
predictive	B
preferences	B
preposterior	B
risk	B
prevalence	B
prim	O
s	O
algorithm	O
primal	B
variables	I
principal	B
component	I
principal	B
components	I
principal	B
components	I
analysis	I
principal	B
components	I
regression	B
principle	B
of	I
insufficient	I
reason	I
probabilistic	B
decision	B
tree	B
probabilistic	B
expert	I
system	I
probabilistic	B
inference	B
probabilistic	B
latent	B
semantic	I
indexing	I
probabilistic	B
matrix	B
factorization	I
probabilistic	B
pca	B
probabilistic	B
principal	B
components	I
analysis	I
probabilistic	B
relational	I
modeling	I
probability	B
density	I
function	I
probability	B
mass	I
function	I
probability	B
of	I
the	I
evidence	B
probability	B
product	I
kernel	B
probability	B
simplex	I
probability	B
theory	I
xxvii	O
probably	B
approximately	I
correct	I
probe	B
probit	B
probit	B
regression	B
product	B
of	I
experts	I
product	B
rule	I
index	O
to	O
keywords	O
production	B
rules	B
profile	O
hmm	B
profile	O
log	O
likelihood	B
projected	B
gradient	B
descent	I
projection	B
projection	B
pursuit	I
prolog	B
proposal	B
distribution	I
propose	B
prosecutor	O
s	O
fallacy	O
protein	B
sequence	I
alignment	B
protein-protein	B
interaction	I
networks	I
prototype	B
proximal	B
operator	I
pruning	B
pseudo	B
counts	I
pseudo	B
likelihood	B
pseudo	B
marginals	I
pseudo	B
random	I
number	I
generator	I
pseudo-likelihood	B
pure	B
purity	B
pushing	B
sums	I
inside	I
products	I
pyramid	B
match	I
kernel	B
qaly	B
qmr	B
qp	B
qq-plot	B
qr	B
decomposition	I
quadratic	B
discriminant	B
analysis	I
quadratic	B
loss	B
quadratic	B
program	I
quantile	B
quantize	B
quartiles	B
quasi-newton	B
query	B
logs	I
query	B
variables	I
quick	B
medical	I
reference	I
radar	B
radial	B
basis	I
function	I
rand	B
index	I
random	B
accelerations	I
model	I
random	B
effects	I
random	B
effects	I
mixture	B
of	I
experts	I
random	B
forests	I
random	B
probability	I
measure	I
random	B
utility	I
model	I
random	B
walk	I
metropolis	I
algorithm	I
random	B
walk	I
on	I
the	I
integers	I
random	B
walk	I
proposal	I
rank	B
correlation	I
rank	B
one	I
update	I
ranking	B
ranknet	B
rao-blackwell	B
rao-blackwellisation	B
rao-blackwellised	O
particle	O
filtering	B
rao-blackwellized	O
particle	O
filtering	B
rare	B
event	I
rate	B
rational	B
behavior	I
rbf	B
rbf	B
kernel	B
rbf	B
network	I
rbm	B
rbpf	B
real	B
adaboost	I
recall	B
receiver	B
operating	I
characteristic	I
receptive	O
fields	O
recognition	B
weights	I
recombination	B
model	I
reconstruction	B
error	I
recurrent	B
recurrent	B
neural	B
network	I
recurrent	B
neural	B
networks	I
recursive	B
recursive	B
least	B
squares	I
reflecting	O
pair	O
regime	B
switching	I
regime	B
switching	I
markov	B
model	I
regression	B
regression	B
spline	B
regret	B
regular	B
regularization	B
regularization	B
path	B
regularized	B
discriminant	B
analysis	I
regularized	B
estimation	I
regularized	O
particle	O
filter	O
regularized	B
risk	B
minimization	I
reinforcement	B
learning	B
reject	B
action	B
rejection	B
sampling	I
rejuvenation	B
relation	B
relational	B
probabilistic	I
models	I
relational	B
topic	B
model	I
relative	B
entropy	B
relative	B
importance	I
of	I
predictor	I
variables	I
relative	B
risk	B
relevance	B
network	I
relevance	B
vector	I
machine	I
rephil	B
replicated	B
softmax	B
model	I
representer	B
theorem	I
reproducing	B
kernel	B
hilbert	I
space	I
reproducing	B
property	I
rerank	B
resample-move	B
residual	B
residual	B
analysis	I
residual	B
belief	B
propagation	I
residual	B
error	I
residual	B
resampling	I
residual	B
sum	B
of	I
squares	I
response	B
variable	I
responsibility	B
restricted	B
boltzmann	B
machine	I
reverse	B
kl	I
reversible	B
jump	I
mcmc	B
reward	B
ricatti	B
equations	I
rich	B
get	I
richer	I
ridge	B
regression	B
right	B
censored	I
risk	B
risk	B
averse	I
rjmcmc	B
rkhs	B
rls	B
robbins-monro	B
robust	B
robust	B
priors	I
robustness	B
roc	B
rocking	B
indexes	O
root	B
root	B
mean	B
square	I
error	I
rosenblatt	B
rotamers	B
rts	B
smoother	I
rule	B
of	I
iterated	I
expectation	I
rule	B
of	I
total	I
probability	I
rules	B
rum	B
running	B
intersection	I
property	I
rvm	B
saddle	B
point	I
approximation	I
sample	B
impoverishment	I
sample	B
standard	B
deviation	I
samples	B
sampling	B
distribution	I
sampling	B
importance	I
resampling	I
sampling	B
period	B
satisfying	B
assignment	I
saturated	B
model	I
sbl	B
scalar	B
product	I
scale	B
invariant	B
prior	I
scale	B
of	I
evidence	B
scatter	B
plot	I
scfgs	B
schedule	B
schur	B
complement	I
scientific	O
method	O
scope	B
score	B
function	I
score	B
matching	I
score	B
vector	I
scores	B
scree	B
plot	I
screening	B
search	B
engine	I
optimization	B
second	B
order	I
second	B
order	I
markov	B
chain	I
second-order	B
markov	B
model	I
self	B
loops	I
semantic	B
hashing	I
semantic	B
network	I
semantic	B
role	I
labeling	I
semi-conjugate	B
semi-continuous	B
hmm	B
semi-markov	B
model	I
semi-metric	B
semi-parametric	B
model	I
semi-supervised	B
semi-supervised	B
embedding	B
semi-supervised	B
learning	B
sensible	B
pca	B
sensitivity	B
sensitivity	B
analysis	I
sensor	B
fusion	I
sentiment	B
analysis	I
separating	B
set	I
separation	B
oracle	I
sequence	B
logo	I
sequential	B
sequential	B
minimal	B
optimization	B
sequential	B
trbp	B
sgd	B
shafer-shenoy	B
shallow	B
parsing	I
shared	B
sherman-morrison-woodbury	B
formula	I
shooting	B
shrinkage	B
shrinkage	B
estimation	I
shrinkage	B
factor	B
side	B
chains	I
side	B
information	B
sift	B
sifting	B
property	I
sigma	B
points	I
sigmoid	B
sigmoid	B
belief	I
net	I
sigmoid	B
belief	I
nets	I
sigmoid	B
kernel	B
signal	B
detection	I
theory	I
signal	B
processing	I
signal-to-noise	B
ratio	I
signal-to-symbol	B
similar	B
similarity-based	B
clustering	B
simple	B
cells	I
simple	B
linear	B
regression	B
simplex	B
factor	B
model	I
simpon	O
s	O
paradox	O
simulated	B
annealing	B
simulated	B
annealing	B
simulation	B
based	I
simultaneous	B
localization	I
and	I
mapping	I
single	B
best	I
replacement	I
single	B
link	I
clustering	B
single	B
site	I
updating	I
singular	B
value	I
decomposition	I
singular	B
values	I
sir	B
size	B
principle	I
skewness	B
skip	B
arcs	I
skip-chain	B
crf	B
slack	B
re-scaling	I
slack	B
variables	I
slam	B
slaves	B
slice	B
sampling	I
sliding	B
window	I
detector	I
slippage	B
slot	B
machine	I
small	B
n	I
large	O
d	O
smartass	B
sml	B
smo	B
smoothing	B
smoothing	B
kernel	B
smoothing	B
splines	I
social	B
networks	I
soft	B
clustering	B
soft	B
margin	B
constraints	I
soft	B
thresholding	I
soft	B
weight	I
sharing	I
softmax	B
source	B
coding	I
spam	B
spam	B
spanning	B
tree	B
polytope	I
sparsa	B
sparse	B
sparse	B
bayesian	B
learning	B
sparse	B
boosting	B
sparse	B
coding	I
sparse	B
data	I
problem	I
sparse	B
kernel	B
machine	I
sparse	B
matrix	B
factorization	I
sparse	B
pca	B
index	O
to	O
keywords	O
sparse	B
representation	I
sparse	B
vector	I
machine	I
sparsity	B
sparsity-promoting	B
prior	I
spectral	B
spectral	B
clustering	B
spectral	B
graph	B
theory	I
speech	B
recognition	I
sphereing	B
spherical	B
spike	B
and	I
slab	I
spin	B
spline	B
split	B
merge	I
split	B
variable	I
square	O
root	B
filter	O
squared	B
error	I
squared	B
exponential	I
kernel	B
squared	B
loss	B
squashing	B
function	I
ssm	B
ssvms	B
stability	B
selection	I
stable	B
stacked	B
denoising	B
auto-encoder	B
stacking	B
standard	B
deviation	I
standard	B
error	I
standard	B
error	I
of	I
the	I
mean	B
standard	B
errors	I
standard	B
model	I
standard	B
normal	B
standard	B
overcomplete	B
representation	I
standardized	B
standardizing	B
state	B
state	B
estimation	I
state	B
space	I
state	B
space	I
model	I
state	B
transition	I
diagram	I
state	B
transition	B
matrix	I
stationary	B
stationary	B
distribution	I
statistical	B
learning	B
theory	I
statistical	B
relational	I
ai	B
statistical	B
relational	I
learning	B
statistically	O
significant	O
steepest	B
descent	I
stein	O
s	O
paradox	O
stemming	B
step	B
size	I
stepping	B
out	I
stepwise	B
em	B
stick-breaking	B
construction	I
sticky	B
stochastic	B
algorithm	I
stochastic	B
approximation	I
stochastic	B
approximation	I
em	B
stochastic	B
automaton	I
stochastic	B
block	I
model	I
stochastic	B
context	I
free	I
grammars	B
stochastic	B
em	B
stochastic	B
gradient	B
boosting	B
stochastic	B
gradient	B
descent	I
stochastic	B
matrix	I
stochastic	B
maximum	I
likelihood	B
stochastic	B
optimization	B
stochastic	B
process	I
stochastic	B
processes	I
stochastic	B
search	I
stochastic	B
volatility	I
stop	B
words	I
stopping	B
rule	I
stratified	O
cv	B
stratified	O
sampling	O
streaming	B
data	I
streetview	B
strict	B
strictly	B
convex	B
string	B
kernel	B
strong	B
local	I
optimum	I
strong	B
sampling	I
assumption	I
structural	B
em	B
structural	B
equation	I
model	I
structural	B
equation	I
models	I
structural	B
error	I
structural	B
risk	B
minimization	I
structural	B
signatures	I
structural	B
support	B
vector	I
machines	I
structural	B
time	I
series	I
structural	B
zeros	I
structure	B
learning	B
structured	O
mean	B
field	O
structured	B
output	I
structured	B
perceptron	B
algorithm	I
structured-output	O
classification	O
problems	O
student	B
t	I
student	B
t	I
distribution	I
sub-gaussian	B
subderivative	B
subdifferential	B
subgradient	B
subgraph	B
subjective	B
subjective	B
probability	I
submodular	B
subsampling	B
subspace	B
method	I
sufficiency	B
principle	I
sufficient	B
statistics	I
suffix	B
trees	I
sum	B
of	I
squared	I
errors	I
sum	B
of	I
squares	I
sum	B
rule	I
sum-product	B
sum-product	B
algorithm	I
super	B
efficient	I
super-gaussian	B
supermodular	B
supervised	B
lda	B
supervised	B
learning	B
supervised	B
pca	B
support	B
support	B
vector	I
machine	I
support	B
vector	I
machines	I
support	B
vectors	I
surrogate	B
loss	B
surrogate	B
loss	B
function	I
surrogate	B
splits	I
survival	O
of	O
the	O
fittest	O
suspicious	B
coincidence	I
suspicious	B
coincidences	I
svd	B
svm	B
svmstruct	B
swendsen	B
wang	I
switching	B
linear	B
dynamical	I
system	I
switching	B
state	B
space	I
model	I
symbol	B
grounding	I
symmetric	B
synchronous	B
updates	I
syntactic	B
sugar	I
synthesis	B
view	I
systematic	B
resampling	I
systems	B
biology	I
systems	O
identification	O
systolic	B
array	I
t	B
statistic	I
t-test	B
tabula	B
rasa	I
tail	B
area	I
probabilities	I
tail	B
area	I
probability	I
tan	B
tasa	B
taylor	B
series	I
taylor	B
series	I
expansion	I
taylor	O
s	O
theorem	O
temperature	B
template	B
template	B
matching	I
tensor	B
product	I
tensor	B
product	I
basis	I
terminals	B
test	B
statistic	I
tf-idf	B
thin	O
junction	B
tree	B
filter	O
thin	B
junction	B
trees	I
thin	B
plate	I
spline	B
thin	B
svd	B
thinning	B
thompson	B
sampling	I
tied	B
tied-mixture	B
hmm	B
tikhonov	B
regularization	B
time	B
reversible	I
time-invariant	B
time-series	B
forecasting	I
tobit	B
model	I
toeplitz	B
tokens	B
topic	B
topic	B
model	I
topological	B
ordering	I
total	B
ordering	I
trace	B
trace	B
plot	I
trace	B
trick	I
traceback	B
tracking	B
tracking	B
by	I
detection	I
tractable	B
substructure	I
trail	B
training	B
set	I
trans-dimensional	B
mcmc	B
transfer	B
function	I
transfer	B
learning	B
transient	B
transition	B
matrix	I
transition	B
model	I
translation	B
invariance	I
translation	B
invariant	B
translation	B
invariant	B
prior	I
trbp	B
trbp-s	B
tree	B
tree	B
ep	B
tree	B
reparameterization	I
tree	B
reweighted	I
belief	B
propagation	I
tree-augmented	O
naive	O
bayes	O
classifier	O
indexes	O
treewidth	B
trellis	B
trellis	B
diagram	I
tri-cube	B
kernel	B
triangle	B
inequality	I
triangulated	B
tridiagonal	B
trigram	B
model	I
true	B
positive	I
rate	B
trueskill	B
truncated	B
gaussian	B
truncated	B
gaussian	B
potential	I
truncated	B
newton	I
truncated	B
svd	B
trw	B
trw-s	B
tube	B
tuples	B
turbo	B
codes	I
two-filter	O
smoothing	B
two-slice	B
marginal	I
type	B
i	I
type	B
i	I
error	I
rate	B
type	B
ii	I
maximum	I
likelihood	B
type-ii	B
maximum	I
likelihood	B
u-shaped	B
curve	I
ucb	B
ugm	B
ukf	B
unbiased	B
uncertainty	B
unclamped	B
phase	B
unclamped	B
term	I
unconditionally	B
independent	I
underfits	O
undirected	B
undirected	B
graphical	B
model	I
undirected	B
local	I
markov	B
property	I
unfaithful	B
unidentifiable	O
unified	O
medical	O
language	O
system	O
uniform	B
distribution	I
unigram	B
statistics	I
unigrams	B
uninformative	B
union	B
bound	I
unit	B
information	B
prior	I
universal	B
approximator	I
unk	B
unknown	B
unrolled	B
unscented	O
kalman	O
filter	O
unscented	O
particle	O
filter	O
unscented	B
transform	I
unstable	B
unsupervised	B
learning	B
up-down	B
user	O
rating	O
profile	O
utilities	B
utility	B
function	I
utility	B
nodes	B
v-structure	B
validation	B
set	I
value	B
nodes	B
value	B
of	I
perfect	I
information	B
vanishing	B
gradient	I
vapnik-chervonenkis	B
var	B
zipf	O
s	O
law	O
index	O
to	O
keywords	O
variable	B
duration	I
hmm	B
variable	B
elimination	I
variance	B
variance	B
stabilizing	I
transform	I
variation	B
of	I
information	B
variational	B
bayes	I
variational	B
bayes	I
em	B
variational	B
em	B
variational	B
free	B
energy	I
variational	B
inference	B
variational	B
message	B
passing	I
varimax	B
vb	B
vbem	B
vc	B
vc	B
dimension	I
vector	B
auto-regressive	I
vector	B
quantization	I
version	B
space	I
vertices	B
vibes	B
views	B
visible	B
visible	B
nodes	B
visible	B
variables	I
visual	B
words	I
visualizing	B
viterbi	B
viterbi	B
decoding	B
viterbi	B
training	I
vmp	B
voronoi	B
tessellation	I
vq	B
wald	B
wald	B
interval	I
warm	B
starting	I
warp	B
watson	B
wavelet	B
wavelet	B
transforms	I
weak	B
conditionality	I
weak	B
learner	I
weak	B
marginalization	I
web	B
crawling	I
web	B
spam	B
weight	B
decay	I
weight	B
function	I
weight	B
vector	I
weighted	B
approximate-rank	I
pairwise	I
weighted	B
average	I
weighted	B
least	B
squares	I
weighted	B
least	B
squares	I
problem	I
whitening	B
whitening	B
widrow-hoff	B
rule	I
wishart	B
working	B
response	I
world	B
health	I
organization	I
wrapper	B
method	I
xbox	B
xor	B
zellner	O
s	O
g-prior	B
zero	B
avoiding	I
zero	B
count	I
problem	I
zero	B
forcing	I
zero	B
temperature	B
limit	I
zig-zag	B
