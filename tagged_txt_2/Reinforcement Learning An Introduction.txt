reinforcement	B
learning	I
an	O
introduction	O
second	O
edition	O
richard	O
s	O
sutton	O
and	O
andrew	O
g	O
barto	O
a	O
bradford	O
book	O
the	O
mit	O
press	O
cambridge	O
massachusetts	O
london	O
england	O
in	O
memory	O
of	O
a	O
harry	O
klopf	B
contents	O
preface	O
to	O
the	O
second	O
edition	O
preface	O
to	O
the	O
first	O
edition	O
summary	O
of	O
notation	O
ix	O
xv	O
xvii	O
introduction	O
reinforcement	B
learning	I
examples	O
elements	O
of	O
reinforcement	B
learning	I
limitations	O
and	O
scope	O
an	O
extended	O
example	O
tic-tac-toe	B
summary	O
early	O
history	B
of	I
reinforcement	B
learning	I
i	O
tabular	B
solution	I
methods	I
multi-armed	B
bandits	I
a	O
k	O
bandit	O
problem	O
action-value	B
methods	I
the	O
testbed	O
incremental	B
implementation	I
tracking	O
a	O
nonstationary	O
problem	O
optimistic	B
initial	I
values	I
upper-confidence-bound	O
action	B
selection	O
gradient	B
bandit	O
algorithms	O
associative	B
search	I
bandits	O
summary	O
iii	O
iv	O
contents	O
finite	O
markov	O
decision	O
processes	O
the	O
agent	O
environment	B
interface	O
goals	O
and	O
rewards	O
returns	O
and	O
episodes	B
unified	O
notation	O
for	O
episodic	O
and	O
continuing	B
tasks	I
policies	O
and	O
value	B
functions	O
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
optimality	O
and	O
approximation	O
summary	O
dynamic	B
programming	I
policy	B
evaluation	O
policy	B
improvement	I
policy	B
iteration	I
value	B
iteration	I
asynchronous	B
dynamic	B
programming	I
generalized	O
policy	B
iteration	I
efficiency	O
of	O
dynamic	B
programming	I
summary	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
prediction	B
monte	B
carlo	I
estimation	O
of	O
action	B
values	O
monte	B
carlo	I
control	B
monte	B
carlo	I
control	B
without	O
exploring	B
starts	I
off-policy	B
prediction	B
via	O
importance	B
sampling	I
incremental	B
implementation	I
off-policy	B
monte	B
carlo	I
control	B
importance	B
sampling	I
importance	B
sampling	I
summary	O
temporal-difference	B
learning	I
td	B
prediction	B
advantages	B
of	I
td	B
prediction	B
methods	O
optimality	B
of	I
sarsa	B
on-policy	O
td	B
control	B
q-learning	B
off-policy	B
td	B
control	B
expected	B
sarsa	B
maximization	B
bias	I
and	O
double	B
learning	I
games	O
afterstates	B
and	O
other	O
special	O
cases	O
summary	O
contents	O
v	O
n-step	B
bootstrapping	B
n-step	B
td	B
prediction	B
n-step	B
sarsa	B
n-step	B
off-policy	B
learning	O
by	O
importance	B
sampling	I
off-policy	B
methods	I
with	B
control	B
variates	I
off-policy	B
learning	O
without	O
importance	B
sampling	I
the	O
n-step	B
tree	B
backup	I
algorithm	O
unifying	O
algorithm	O
n-step	B
q	O
summary	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
models	O
and	O
planning	B
dyna	O
integrated	O
planning	B
acting	O
and	O
learning	O
when	O
the	O
model	O
is	O
wrong	O
prioritized	B
sweeping	I
expected	B
vs	O
sample	O
updates	O
trajectory	B
sampling	I
real-time	B
dynamic	B
programming	I
planning	B
at	O
decision	O
time	O
heuristic	B
search	I
rollout	B
algorithms	I
monte	B
carlo	I
tree	O
search	O
summary	O
of	O
the	O
chapter	O
summary	O
of	O
part	O
i	O
dimensions	O
ii	O
approximate	O
solution	O
methods	O
on-policy	O
prediction	B
with	B
approximation	I
value-function	B
approximation	I
the	O
prediction	B
objective	O
stochastic-gradient	O
and	O
semi-gradient	B
methods	I
linear	O
methods	O
feature	B
construction	I
for	O
linear	O
methods	O
polynomials	O
fourier	B
basis	I
coarse	B
coding	I
tile	B
coding	I
radial	O
basis	O
functions	O
selecting	O
step-size	O
parameters	O
manually	O
nonlinear	O
function	B
approximation	I
artificial	B
neural	B
networks	I
vi	O
contents	O
least-squares	O
td	B
memory-based	O
function	B
approximation	I
kernel-based	B
function	B
approximation	I
looking	O
deeper	O
at	O
on-policy	O
learning	O
interest	B
and	I
emphasis	I
summary	O
on-policy	O
control	B
with	B
approximation	I
episodic	O
semi-gradient	O
control	B
semi-gradient	O
n-step	B
sarsa	B
average	O
reward	O
a	O
new	O
problem	O
setting	O
for	O
continuing	B
tasks	I
deprecating	O
the	O
discounted	O
setting	O
differential	B
semi-gradient	O
n-step	B
sarsa	B
summary	O
methods	O
with	B
approximation	I
semi-gradient	B
methods	I
examples	O
of	O
off-policy	B
divergence	O
the	O
deadly	B
triad	I
linear	O
value-function	O
geometry	O
gradient	B
descent	O
in	B
the	I
bellman	B
error	I
the	O
bellman	B
error	I
is	O
not	O
learnable	O
gradient-td	B
methods	I
emphatic-td	B
methods	I
reducing	B
variance	I
summary	O
eligibility	B
traces	I
the	O
td	B
n-step	B
truncated	B
methods	O
redoing	O
updates	O
the	O
online	B
algorithm	O
true	B
online	B
td	B
dutch	B
traces	O
in	O
monte	B
carlo	I
learning	O
sarsa	B
variable	O
and	O
off-policy	B
eligibility	B
traces	I
with	B
control	B
variates	I
watkins	B
s	O
q	O
to	O
tree-backup	O
stable	O
off-policy	B
methods	I
with	O
traces	O
implementation	O
issues	O
conclusions	O
contents	O
vii	O
policy	B
gradient	B
methods	I
policy	B
approximation	I
and	O
its	O
advantages	O
the	O
policy	B
gradient	B
theorem	B
reinforce	B
monte	B
carlo	I
policy	B
gradient	B
reinforce	B
with	B
baseline	B
actor	O
critic	O
methods	O
policy	B
gradient	B
for	O
continuing	O
problems	O
policy	B
parameterization	O
for	O
continuous	O
actions	O
summary	O
iii	O
looking	O
deeper	O
psychology	B
prediction	B
and	B
control	B
classical	B
conditioning	I
blocking	B
and	B
higher-order	I
conditioning	I
the	O
rescorla	O
wagner	O
model	O
the	O
td	B
model	I
td	B
model	I
simulations	O
instrumental	O
conditioning	O
delayed	B
reinforcement	I
cognitive	B
maps	I
habitual	O
and	O
goal-directed	O
behavior	O
summary	O
neuroscience	B
neuroscience	B
basics	O
reward	O
signals	O
reinforcement	O
signals	O
values	O
and	O
prediction	B
errors	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
dopamine	B
experimental	O
support	O
for	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
td	B
errordopamine	O
correspondence	O
neural	B
actor	O
critic	O
actor	O
and	O
critic	O
learning	O
rules	O
hedonistic	B
neurons	I
collective	B
reinforcement	B
learning	I
model-based	O
methods	O
in	O
the	O
brain	O
addiction	B
summary	O
viii	O
contents	O
applications	B
and	I
case	I
studies	I
td-gammon	B
samuel	O
s	O
checkers	O
player	O
watson	O
s	O
daily-double	O
wagering	O
optimizing	B
memory	I
control	B
human-level	O
video	O
game	O
play	O
mastering	O
the	O
game	O
of	O
go	O
alphago	B
alphago	B
zero	O
personalized	O
web	O
services	O
thermal	O
soaring	O
frontiers	O
general	O
value	B
functions	O
and	O
auxiliary	B
tasks	I
temporal	B
abstraction	I
via	O
options	B
observations	O
and	O
state	B
designing	O
reward	O
signals	O
remaining	O
issues	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artificial	B
intelligence	I
references	O
index	O
preface	O
to	O
the	O
second	O
edition	O
the	O
twenty	O
years	O
since	O
the	O
publication	O
of	O
the	O
first	O
edition	O
of	O
this	O
book	O
have	O
seen	O
tremendous	O
progress	O
in	O
artificial	B
intelligence	I
propelled	O
in	O
large	O
part	O
by	O
advances	O
in	O
machine	O
learning	O
including	O
advances	O
in	O
reinforcement	B
learning	I
although	O
the	O
impressive	O
computational	O
power	O
that	O
became	O
available	O
is	O
responsible	O
for	O
some	O
of	O
these	O
advances	O
new	O
developments	O
in	O
theory	O
and	O
algorithms	O
have	O
been	O
driving	O
forces	O
as	O
well	O
in	O
the	O
face	O
of	O
this	O
progress	O
a	O
second	O
edition	O
of	O
our	O
book	O
was	O
long	O
overdue	O
and	O
we	O
finally	O
began	O
the	O
project	O
in	O
our	O
goal	O
for	O
the	O
second	O
edition	O
was	O
the	O
same	O
as	O
our	O
goal	O
for	O
the	O
first	O
to	O
provide	O
a	O
clear	O
and	O
simple	O
account	O
of	O
the	O
key	O
ideas	O
and	O
algorithms	O
of	O
reinforcement	B
learning	I
that	O
is	O
accessible	O
to	O
readers	O
in	O
all	O
the	O
related	O
disciplines	O
the	O
edition	O
remains	O
an	O
introduction	O
and	O
we	O
retain	O
a	O
focus	O
on	O
core	O
online	B
learning	O
algorithms	O
this	O
edition	O
includes	O
some	O
new	O
topics	O
that	O
rose	O
to	O
importance	O
over	O
the	O
intervening	O
years	O
and	O
we	O
expanded	O
coverage	O
of	O
topics	O
that	O
we	O
now	O
understand	O
better	O
but	O
we	O
made	O
no	O
attempt	O
to	O
provide	O
comprehensive	O
coverage	O
of	O
the	O
field	O
which	O
has	O
exploded	O
in	O
many	O
different	O
directions	O
with	O
outstanding	O
contributions	O
by	O
many	O
active	O
researchers	O
we	O
apologize	O
for	O
having	O
to	O
leave	O
out	O
all	O
but	O
a	O
handful	O
of	O
these	O
contributions	O
as	O
in	O
the	O
first	O
edition	O
we	O
chose	O
not	O
to	O
produce	O
a	O
rigorous	O
formal	O
treatment	O
of	O
reinforcement	B
learning	I
or	O
to	O
formulate	O
it	O
in	O
the	O
most	O
general	O
terms	O
however	O
since	O
the	O
first	O
edition	O
our	O
deeper	O
understanding	O
of	O
some	O
topics	O
required	O
a	O
bit	O
more	O
mathematics	O
to	O
explain	O
we	O
have	O
set	O
off	O
the	O
more	O
mathematical	O
parts	O
in	O
shaded	O
boxes	B
that	O
the	O
nonmathematically-inclined	O
may	O
choose	O
to	O
skip	O
we	O
also	O
use	O
a	O
slightly	O
different	O
notation	O
than	O
was	O
used	O
in	O
the	O
first	O
edition	O
in	O
teaching	O
we	O
have	O
found	O
that	O
the	O
new	O
notation	O
helps	O
to	O
address	O
some	O
common	O
points	O
of	O
confusion	O
it	O
emphasizes	O
the	O
difference	O
between	O
random	O
variables	O
denoted	O
with	O
capital	O
letters	O
and	O
their	O
instantiations	O
denoted	O
in	O
lower	O
case	O
for	O
example	O
the	O
state	B
action	B
and	O
reward	O
at	O
time	O
step	O
t	O
are	O
denoted	O
st	O
at	O
and	O
rt	O
while	O
their	O
possible	O
values	O
might	O
be	O
denoted	O
s	O
a	O
and	O
r	O
along	O
with	O
this	O
it	O
is	O
natural	O
to	O
use	O
lower	O
case	O
for	O
value	B
functions	O
v	O
and	O
restrict	O
capitals	O
to	O
their	O
tabular	O
estimates	O
qts	O
a	O
approximate	O
value	B
functions	O
are	O
deterministic	O
functions	O
of	O
random	O
parameters	O
and	O
are	O
thus	O
also	O
in	O
lower	O
case	O
vswt	O
v	O
vectors	O
such	O
as	O
the	O
weight	O
vector	B
wt	O
t	O
and	O
the	O
feature	O
vector	B
xt	O
t	O
are	O
bold	O
and	O
written	O
in	O
lowercase	O
even	O
if	O
they	O
are	O
random	O
variables	O
uppercase	O
bold	O
is	O
reserved	O
for	O
matrices	O
in	O
the	O
first	O
edition	O
we	O
used	O
special	O
notations	O
pa	O
for	O
the	O
transition	B
probabilities	I
and	O
expected	B
rewards	O
one	O
weakness	O
of	O
that	O
notation	O
is	O
that	O
it	O
still	O
did	O
not	O
fully	O
characterize	O
the	O
dynamics	O
of	O
the	O
rewards	O
giving	O
only	O
their	O
expectations	O
and	O
ra	O
ix	O
x	O
preface	O
to	O
the	O
second	O
edition	O
which	O
is	O
sufficient	O
for	B
dynamic	B
programming	I
but	O
not	O
for	O
reinforcement	B
learning	I
another	O
weakness	O
is	O
the	O
excess	O
of	O
subscripts	O
and	O
superscripts	O
in	O
this	O
edition	O
we	O
use	O
the	O
explicit	O
notation	O
of	O
rs	O
a	O
for	O
the	O
joint	O
probability	O
for	O
the	O
next	O
state	B
and	O
reward	O
given	O
the	O
current	O
state	B
and	O
action	B
all	O
the	O
changes	O
in	O
notation	O
are	O
summarized	O
in	O
a	O
table	O
on	O
page	O
xvii	O
the	O
second	O
edition	O
is	O
significantly	O
expanded	O
and	O
its	O
top-level	O
organization	O
has	O
been	O
revamped	O
after	O
the	O
introductory	O
first	O
chapter	O
the	O
second	O
edition	O
is	O
divided	O
into	O
three	O
new	O
parts	O
the	O
first	O
part	O
treats	O
as	O
much	O
of	O
reinforcement	B
learning	I
as	O
possible	O
without	O
going	O
beyond	O
the	O
tabular	O
case	O
for	O
which	O
exact	O
solutions	O
can	O
be	O
found	O
we	O
cover	O
both	O
learning	O
and	O
planning	B
methods	O
for	O
the	O
tabular	O
case	O
as	O
well	O
as	O
their	O
unification	O
in	O
n-step	B
methods	I
and	O
in	O
dyna	O
many	O
algorithms	O
presented	O
in	O
this	O
part	O
are	O
new	O
to	O
the	O
second	O
edition	O
including	O
ucb	O
expected	B
sarsa	B
double	B
learning	I
tree-backup	O
q	O
rtdp	O
and	O
mcts	O
doing	O
the	O
tabular	O
case	O
first	O
and	O
thoroughly	O
enables	O
core	O
ideas	O
to	O
be	O
developed	O
in	O
the	O
simplest	O
possible	O
setting	O
the	O
whole	O
second	O
part	O
of	O
the	O
book	O
is	O
then	O
devoted	O
to	O
extending	O
the	O
ideas	O
to	O
function	B
approximation	I
it	O
has	O
new	O
sections	O
on	O
artificial	B
neural	B
networks	I
the	O
fourier	B
basis	I
lstd	O
kernel-based	O
methods	O
gradient-td	O
and	O
emphatic-td	B
methods	I
average-reward	O
methods	O
true	B
online	B
td	B
and	O
policy-gradient	O
methods	O
the	O
second	O
edition	O
significantly	O
expands	O
the	O
treatment	O
of	O
off-policy	B
learning	O
first	O
for	O
the	O
tabular	O
case	O
in	O
chapters	O
then	O
with	B
function	B
approximation	I
in	O
chapters	O
and	O
another	O
change	O
is	O
that	O
the	O
second	O
edition	O
separates	O
the	O
forward-view	O
idea	O
of	O
n-step	B
bootstrapping	B
treated	O
more	O
fully	O
in	O
chapter	O
from	O
the	O
backward-view	O
idea	O
of	O
eligibility	B
traces	I
treated	O
independently	O
in	O
chapter	O
the	O
third	O
part	O
of	O
the	O
book	O
has	O
large	O
new	O
chapters	O
on	O
reinforcement	B
learning	I
s	O
relationships	O
to	O
psychology	B
and	O
neuroscience	B
as	O
well	O
as	O
an	O
updated	O
case-studies	O
chapter	O
including	O
atari	O
game	O
playing	O
watson	O
s	O
wagering	O
strategy	O
and	O
the	O
go	O
playing	O
programs	O
alphago	B
and	O
alphago	B
zero	O
still	O
out	O
of	O
necessity	O
we	O
have	O
included	O
only	O
a	O
small	O
subset	O
of	O
all	O
that	O
has	O
been	O
done	O
in	O
the	O
field	O
our	O
choices	O
reflect	O
our	O
long-standing	O
interests	O
in	O
inexpensive	O
model-free	O
methods	O
that	O
should	O
scale	O
well	O
to	O
large	O
applications	O
the	O
final	O
chapter	O
now	O
includes	O
a	O
discussion	O
of	O
the	O
future	O
societal	O
impacts	O
of	O
reinforcement	B
learning	I
for	O
better	O
or	O
worse	O
the	O
second	O
edition	O
is	O
about	O
twice	O
as	O
large	O
as	O
the	O
first	O
this	O
book	O
is	O
designed	O
to	O
be	O
used	O
as	O
the	O
primary	O
text	O
for	O
a	O
one-	O
or	O
two-semester	O
course	O
on	O
reinforcement	B
learning	I
for	O
a	O
one-semester	O
course	O
the	O
first	O
ten	O
chapters	O
should	O
be	O
covered	O
in	O
order	O
and	O
form	O
a	O
good	O
core	O
to	O
which	O
can	O
be	O
added	O
material	O
from	O
the	O
other	O
chapters	O
from	O
other	O
books	O
such	O
as	O
bertsekas	O
and	O
tsitsiklis	O
wiering	O
and	O
van	O
otterlo	O
and	O
szepesv	O
ari	O
or	O
from	O
the	O
literature	O
according	O
to	O
taste	O
depending	O
of	O
the	O
students	O
background	O
some	O
additional	O
material	O
on	O
online	B
supervised	B
learning	I
may	O
be	O
helpful	O
the	O
ideas	O
of	O
options	B
and	O
option	B
models	I
are	O
a	O
natural	O
addition	O
precup	O
and	O
singh	O
a	O
two-semester	O
course	O
can	O
cover	O
all	O
the	O
chapters	O
as	O
well	O
as	O
supplementary	O
material	O
the	O
book	O
can	O
also	O
be	O
used	O
as	O
part	O
of	O
broader	O
courses	O
on	O
machine	O
learning	O
artificial	B
intelligence	I
or	O
neural	B
networks	O
in	O
this	O
case	O
it	O
may	O
be	O
desirable	O
to	O
cover	O
only	O
a	O
subset	O
of	O
the	O
material	O
we	O
recommend	O
covering	O
chapter	O
for	O
a	O
brief	O
overview	O
chapter	O
through	O
section	O
chapter	O
and	O
then	O
selecting	O
sections	O
from	O
the	O
remaining	O
chapters	O
according	O
to	O
time	O
and	O
interests	O
chapter	O
is	O
the	O
most	O
important	O
for	O
the	O
subject	O
and	O
for	O
the	O
rest	O
of	O
the	O
book	O
a	O
course	O
focusing	O
on	O
machine	O
preface	O
to	O
the	O
second	O
edition	O
xi	O
learning	O
or	O
neural	B
networks	O
should	O
cover	O
chapters	O
and	O
and	O
a	O
course	O
focusing	O
on	O
artificial	B
intelligence	I
or	O
planning	B
should	O
cover	O
chapter	O
throughout	O
the	O
book	O
sections	O
and	O
chapters	O
that	O
are	O
more	O
difficult	O
and	O
not	O
essential	O
to	O
the	O
rest	O
of	O
the	O
book	O
are	O
marked	O
with	O
a	O
these	O
can	O
be	O
omitted	O
on	O
first	O
reading	O
without	O
creating	O
problems	O
later	O
on	O
some	O
exercises	O
are	O
also	O
marked	O
with	O
a	O
to	O
indicate	O
that	O
they	O
are	O
more	O
advanced	O
and	O
not	O
essential	O
to	O
understanding	O
the	O
basic	O
material	O
of	O
the	O
chapter	O
most	O
chapters	O
end	O
with	O
a	O
section	O
entitled	O
bibliographical	O
and	O
historical	O
remarks	O
wherein	O
we	O
credit	O
the	O
sources	O
of	O
the	O
ideas	O
presented	O
in	O
that	O
chapter	O
provide	O
pointers	O
to	O
further	O
reading	O
and	O
ongoing	O
research	O
and	O
describe	O
relevant	O
historical	O
background	O
despite	O
our	O
attempts	O
to	O
make	O
these	O
sections	O
authoritative	O
and	O
complete	O
we	O
have	O
undoubtedly	O
left	O
out	O
some	O
important	O
prior	O
work	O
for	O
that	O
we	O
again	O
apologize	O
and	O
we	O
welcome	O
corrections	O
and	O
extensions	O
for	O
incorporation	O
into	O
the	O
electronic	O
version	O
of	O
the	O
book	O
like	O
the	O
first	O
edition	O
this	O
edition	O
of	O
the	O
book	O
is	O
dedicated	O
to	O
the	O
memory	O
of	O
a	O
harry	O
klopf	B
it	O
was	O
harry	O
who	O
introduced	O
us	O
to	O
each	O
other	O
and	O
it	O
was	O
his	O
ideas	O
about	O
the	O
brain	O
and	B
artificial	B
intelligence	I
that	O
launched	O
our	O
long	O
excursion	O
into	O
reinforcement	B
learning	I
trained	O
in	O
neurophysiology	O
and	O
long	O
interested	O
in	O
machine	O
intelligence	O
harry	O
was	O
a	O
senior	O
scientist	O
affiliated	O
with	O
the	O
avionics	O
directorate	O
of	O
the	O
air	O
force	O
office	O
of	O
scientific	O
research	O
at	O
wright-patterson	O
air	O
force	O
base	O
ohio	O
he	O
was	O
dissatisfied	O
with	O
the	O
great	O
importance	O
attributed	O
to	O
equilibrium-seeking	O
processes	O
including	O
homeostasis	O
and	O
error-correcting	O
pattern	O
classification	O
methods	O
in	O
explaining	O
natural	O
intelligence	O
and	O
in	O
providing	O
a	O
basis	O
for	O
machine	O
intelligence	O
he	O
noted	O
that	O
systems	O
that	O
try	O
to	O
maximize	O
something	O
that	O
might	O
be	O
are	O
qualitatively	O
different	O
from	O
equilibrium-seeking	O
systems	O
and	O
he	O
argued	O
that	O
maximizing	O
systems	O
hold	O
the	O
key	O
to	O
understanding	O
important	O
aspects	O
of	O
natural	O
intelligence	O
and	O
for	O
building	O
artificial	O
intelligences	O
harry	O
was	O
instrumental	O
in	O
obtaining	O
funding	O
from	O
afosr	O
for	O
a	O
project	O
to	O
assess	O
the	O
scientific	O
merit	O
of	O
these	O
and	O
related	O
ideas	O
this	O
project	O
was	O
conducted	O
in	O
the	O
late	O
at	O
the	O
university	O
of	O
massachusetts	O
amherst	O
amherst	O
initially	O
under	O
the	O
direction	O
of	O
michael	O
arbib	O
william	O
kilmer	O
and	O
nico	O
spinelli	O
professors	O
in	O
the	O
department	O
of	O
computer	O
and	O
information	O
science	O
at	O
umass	O
amherst	O
and	O
founding	O
members	O
of	O
the	O
cybernetics	B
center	O
for	O
systems	O
neuroscience	B
at	O
the	O
university	O
a	O
farsighted	O
group	O
focusing	O
on	O
the	O
intersection	O
of	O
neuroscience	B
and	B
artificial	B
intelligence	I
barto	O
a	O
recent	O
ph	O
d	O
from	O
the	O
university	O
of	O
michigan	O
was	O
hired	O
as	O
post	O
doctoral	O
researcher	O
on	O
the	O
project	O
meanwhile	O
sutton	O
an	O
undergraduate	O
studying	O
computer	O
science	O
and	O
psychology	B
at	O
stanford	O
had	O
been	O
corresponding	O
with	O
harry	O
regarding	O
their	O
mutual	O
interest	O
in	O
the	O
role	O
of	O
stimulus	O
timing	O
in	O
classical	B
conditioning	I
harry	O
suggested	O
to	O
the	O
umass	O
group	O
that	O
sutton	O
would	O
be	O
a	O
great	O
addition	O
to	O
the	O
project	O
thus	O
sutton	O
became	O
a	O
umass	O
graduate	O
student	O
whose	O
ph	O
d	O
was	O
directed	O
by	O
barto	O
who	O
had	O
become	O
an	O
associate	O
professor	O
the	O
study	O
of	O
reinforcement	B
learning	I
as	O
presented	O
in	O
this	O
book	O
is	O
rightfully	O
an	O
outcome	O
of	O
that	O
project	O
instigated	O
by	O
harry	O
and	O
inspired	O
by	O
his	O
ideas	O
further	O
harry	O
was	O
responsible	O
for	O
bringing	O
us	O
the	O
authors	O
together	O
in	O
what	O
has	O
been	O
a	O
long	O
and	O
enjoyable	O
interaction	O
by	O
dedicating	O
this	O
book	O
to	O
harry	O
we	O
honor	O
his	O
essential	O
contributions	O
not	O
only	O
to	O
the	O
field	O
of	O
reinforcement	B
learning	I
but	O
also	O
to	O
our	O
collaboration	O
we	O
also	O
thank	O
professors	O
arbib	O
kilmer	O
and	O
spinelli	O
for	O
the	O
opportunity	O
they	O
provided	O
to	O
us	O
to	O
begin	O
exploring	O
these	O
ideas	O
finally	O
we	O
thank	O
afosr	O
for	O
generous	O
support	O
over	O
the	O
early	O
years	O
of	O
our	O
xii	O
preface	O
to	O
the	O
second	O
edition	O
research	O
and	O
the	O
nsf	O
for	O
its	O
generous	O
support	O
over	O
many	O
of	O
the	O
following	O
years	O
we	O
have	O
very	O
many	O
people	O
to	O
thank	O
for	O
their	O
inspiration	O
and	O
help	O
with	O
this	O
second	O
edition	O
everyone	O
we	O
acknowledged	O
for	O
their	O
inspiration	O
and	O
help	O
with	O
the	O
first	O
edition	O
deserve	O
our	O
deepest	O
gratitude	O
for	O
this	O
edition	O
as	O
well	O
which	O
would	O
not	O
exist	O
were	O
it	O
not	O
for	O
their	O
contributions	O
to	O
edition	O
number	O
one	O
to	O
that	O
long	O
list	O
we	O
must	O
add	O
many	O
others	O
who	O
contributed	O
specifically	O
to	O
the	O
second	O
edition	O
our	O
students	O
over	O
the	O
many	O
years	O
that	O
we	O
have	O
taught	O
this	O
material	O
contributed	O
in	O
countless	O
ways	O
exposing	O
errors	O
offering	O
fixes	O
and	O
not	O
the	O
least	O
being	O
confused	O
in	O
places	O
where	O
we	O
could	O
have	O
explained	O
things	O
better	O
we	O
especially	O
thank	O
martha	O
steenstrup	O
for	O
reading	O
and	O
providing	O
detailed	O
comments	O
throughout	O
the	O
chapters	O
on	O
psychology	B
and	O
neuroscience	B
could	O
not	O
have	O
been	O
written	O
without	O
the	O
help	O
of	O
many	O
experts	O
in	O
those	O
fields	O
we	O
thank	O
john	O
moore	O
for	O
his	O
patient	O
tutoring	O
over	O
many	O
many	O
years	O
on	O
animal	O
learning	O
experiments	O
theory	O
and	O
neuroscience	B
and	O
for	O
his	O
careful	O
reading	O
of	O
multiple	O
drafts	O
of	O
chapters	O
and	O
we	O
also	O
thank	O
matt	O
botvinick	O
nathaniel	O
daw	O
peter	O
dayan	O
and	O
yael	O
niv	O
for	O
their	O
penetrating	O
comments	O
on	O
drafts	O
of	O
these	O
chapter	O
their	O
essential	O
guidance	O
through	O
the	O
massive	O
literature	O
and	O
their	O
interception	O
of	O
many	O
of	O
our	O
errors	O
in	O
early	O
drafts	O
of	O
course	O
the	O
remaining	O
errors	O
in	O
these	O
chapters	O
and	O
there	O
must	O
still	O
be	O
some	O
are	O
totally	O
our	O
own	O
we	O
thank	O
phil	O
thomas	O
for	O
helping	O
us	O
make	O
these	O
chapters	O
accessible	O
to	O
non-psychologists	O
and	O
non-neuroscientists	O
and	O
we	O
thank	O
peter	O
sterling	O
for	O
helping	O
us	O
improve	O
the	O
exposition	O
we	O
are	O
grateful	O
to	O
jim	O
houk	O
for	O
introducing	O
us	O
to	O
the	O
subject	O
of	O
information	O
processing	O
in	O
the	O
basal	B
ganglia	I
and	O
for	O
alerting	O
us	O
to	O
other	O
relevant	O
aspects	O
of	O
neuroscience	B
jos	O
e	O
mart	O
nez	O
terry	O
sejnowski	O
david	O
silver	O
gerry	O
tesauro	O
georgios	O
theocharous	O
and	O
phil	O
thomas	O
generously	O
helped	O
us	O
understand	O
details	O
of	O
their	O
reinforcement	B
learning	I
applications	O
for	O
inclusion	O
in	O
the	O
case-studies	O
chapter	O
and	O
they	O
provided	O
helpful	O
comments	O
on	O
drafts	O
of	O
these	O
sections	O
special	O
thanks	O
are	O
owed	O
to	O
david	O
silver	O
for	O
helping	O
us	O
better	O
understand	O
monte	B
carlo	I
tree	O
search	O
and	O
the	O
deepmind	O
go-playing	O
programs	O
we	O
thank	O
george	O
konidaris	O
for	O
his	O
help	O
with	O
the	O
section	O
on	O
the	O
fourier	B
basis	I
emilio	O
cartoni	O
thomas	O
cederborg	O
stefan	O
dernbach	O
clemens	O
rosenbaum	O
patrick	O
taylor	O
and	O
pierre-luc	O
bacon	O
helped	O
us	O
in	O
a	O
number	O
important	O
ways	O
for	O
which	O
we	O
are	O
most	O
grateful	O
sutton	O
would	O
also	O
like	O
to	O
thank	O
the	O
members	O
of	O
the	O
reinforcement	B
learning	I
and	B
artificial	B
intelligence	I
laboratory	O
at	O
the	O
university	O
of	O
alberta	O
for	O
contributions	O
to	O
the	O
second	O
edition	O
he	O
owes	O
a	O
particular	O
debt	O
to	O
rupam	O
mahmood	O
for	O
essential	O
contributions	O
to	O
the	O
treatment	O
of	O
off-policy	B
monte	B
carlo	I
methods	I
in	O
chapter	O
to	O
hamid	O
maei	O
for	O
helping	O
develop	O
the	O
perspective	O
on	O
off-policy	B
learning	O
presented	O
in	O
chapter	O
to	O
eric	O
graves	O
for	O
conducting	O
the	O
experiments	O
in	O
chapter	O
to	O
shangtong	O
zhang	O
for	O
replicating	O
and	O
thus	O
verifying	O
almost	O
all	O
the	O
experimental	O
results	O
to	O
kris	O
de	O
asis	O
for	O
improving	O
the	O
new	O
technical	O
content	O
of	O
chapters	O
and	O
and	O
to	O
harm	O
van	O
seijen	O
for	O
insights	O
that	O
led	O
to	O
the	O
separation	O
of	O
n-step	B
methods	I
from	O
eligibility	B
traces	I
and	O
with	O
hado	O
van	O
hasselt	O
for	O
the	O
ideas	O
involving	O
exact	O
equivalence	O
of	O
forward	O
and	O
backward	O
views	O
of	O
eligibility	B
traces	I
presented	O
in	O
chapter	O
sutton	O
also	O
gratefully	O
acknowledges	O
the	O
support	O
and	O
freedom	O
he	O
was	O
granted	O
by	O
the	O
government	O
of	O
alberta	O
and	O
the	O
national	O
science	O
and	O
engineering	O
research	O
council	O
of	O
canada	O
throughout	O
the	O
period	O
during	O
which	O
the	O
second	O
edition	O
was	O
conceived	O
and	O
written	O
in	O
particular	O
he	O
would	O
like	O
to	O
thank	O
randy	O
goebel	O
for	O
creating	O
a	O
supportive	O
and	O
far-sighted	O
environment	B
for	O
research	O
in	O
alberta	O
he	O
preface	O
to	O
the	O
second	O
edition	O
xiii	O
would	O
also	O
like	O
to	O
thank	O
deepmind	O
their	O
support	O
in	O
the	O
last	O
six	O
months	O
of	O
writing	O
the	O
book	O
finally	O
we	O
owe	O
thanks	O
to	O
the	O
many	O
careful	O
readers	O
of	O
drafts	O
of	O
the	O
second	O
edition	O
that	O
we	O
posted	O
on	O
the	O
internet	O
they	O
found	O
many	O
errors	O
that	O
we	O
had	O
missed	O
and	O
alerted	O
us	O
to	O
potential	O
points	O
of	O
confusion	O
preface	O
to	O
the	O
first	O
edition	O
we	O
first	O
came	O
to	O
focus	O
on	O
what	O
is	O
now	O
known	O
as	O
reinforcement	B
learning	I
in	O
late	O
we	O
were	O
both	O
at	O
the	O
university	O
of	O
massachusetts	O
working	O
on	O
one	O
of	O
the	O
earliest	O
projects	O
to	O
revive	O
the	O
idea	O
that	O
networks	O
of	O
neuronlike	O
adaptive	O
elements	O
might	O
prove	O
to	O
be	O
a	O
promising	O
approach	O
to	O
artificial	O
adaptive	O
intelligence	O
the	O
project	O
explored	O
the	O
heterostatic	O
theory	O
of	O
adaptive	O
systems	O
developed	O
by	O
a	O
harry	O
klopf	B
harry	O
s	O
work	O
was	O
a	O
rich	O
source	O
of	O
ideas	O
and	O
we	O
were	O
permitted	O
to	O
explore	O
them	O
critically	O
and	O
compare	O
them	O
with	O
the	O
long	O
history	B
of	I
prior	O
work	O
in	O
adaptive	O
systems	O
our	O
task	O
became	O
one	O
of	O
teasing	O
the	O
ideas	O
apart	O
and	O
understanding	O
their	O
relationships	O
and	O
relative	O
importance	O
this	O
continues	O
today	O
but	O
in	O
we	O
came	O
to	O
realize	O
that	O
perhaps	O
the	O
simplest	O
of	O
the	O
ideas	O
which	O
had	O
long	O
been	O
taken	O
for	O
granted	O
had	O
received	O
surprisingly	O
little	O
attention	O
from	O
a	O
computational	O
perspective	O
this	O
was	O
simply	O
the	O
idea	O
of	O
a	O
learning	O
system	O
that	O
wants	O
something	O
that	O
adapts	O
its	O
behavior	O
in	O
order	O
to	O
maximize	O
a	O
special	O
signal	O
from	O
its	O
environment	B
this	O
was	O
the	O
idea	O
of	O
a	O
hedonistic	O
learning	O
system	O
or	O
as	O
we	O
would	O
say	O
now	O
the	O
idea	O
of	O
reinforcement	B
learning	I
like	O
others	O
we	O
had	O
a	O
sense	O
that	O
reinforcement	B
learning	I
had	O
been	O
thoroughly	O
explored	O
in	O
the	O
early	O
days	O
of	O
cybernetics	B
and	B
artificial	B
intelligence	I
on	O
closer	O
inspection	O
though	O
we	O
found	O
that	O
it	O
had	O
been	O
explored	O
only	O
slightly	O
while	O
reinforcement	B
learning	I
had	O
clearly	O
motivated	O
some	O
of	O
the	O
earliest	O
computational	O
studies	O
of	O
learning	O
most	O
of	O
these	O
researchers	O
had	O
gone	O
on	O
to	O
other	O
things	O
such	O
as	O
pattern	O
classification	O
supervised	B
learning	I
and	O
adaptive	O
control	B
or	O
they	O
had	O
abandoned	O
the	O
study	O
of	O
learning	O
altogether	O
as	O
a	O
result	O
the	O
special	O
issues	O
involved	O
in	O
learning	O
how	O
to	O
get	O
something	O
from	O
the	O
environment	B
received	O
relatively	O
little	O
attention	O
in	O
retrospect	O
focusing	O
on	O
this	O
idea	O
was	O
the	O
critical	O
step	O
that	O
set	O
this	O
branch	O
of	O
research	O
in	O
motion	O
little	O
progress	O
could	O
be	O
made	O
in	O
the	O
computational	O
study	O
of	O
reinforcement	B
learning	I
until	O
it	O
was	O
recognized	O
that	O
such	O
a	O
fundamental	O
idea	O
had	O
not	O
yet	O
been	O
thoroughly	O
explored	O
the	O
field	O
has	O
come	O
a	O
long	O
way	O
since	O
then	O
evolving	O
and	O
maturing	O
in	O
several	O
directions	O
reinforcement	B
learning	I
has	O
gradually	O
become	O
one	O
of	O
the	O
most	O
active	O
research	O
areas	O
in	O
machine	O
learning	O
artificial	B
intelligence	I
and	O
neural	B
network	O
research	O
the	O
field	O
has	O
developed	O
strong	O
mathematical	O
foundations	O
and	O
impressive	O
applications	O
the	O
computational	O
study	O
of	O
reinforcement	B
learning	I
is	O
now	O
a	O
large	O
field	O
with	O
hundreds	O
of	O
active	O
researchers	O
around	O
the	O
world	O
in	O
diverse	O
disciplines	O
such	O
as	O
psychology	B
control	B
theory	I
artificial	B
intelligence	I
and	O
neuroscience	B
particularly	O
important	O
have	O
been	O
the	O
contributions	O
establishing	O
and	O
developing	O
the	O
relationships	O
to	O
the	O
theory	O
of	O
optimal	B
control	B
and	O
xv	O
xvi	O
preface	O
to	O
the	O
first	O
edition	O
dynamic	B
programming	I
the	O
overall	O
problem	O
of	O
learning	O
from	O
interaction	O
to	O
achieve	O
goals	O
is	O
still	O
far	O
from	O
being	O
solved	O
but	O
our	O
understanding	O
of	O
it	O
has	O
improved	O
significantly	O
we	O
can	O
now	O
place	O
component	O
ideas	O
such	O
as	O
temporal-difference	B
learning	I
dynamic	B
programming	I
and	B
function	B
approximation	I
within	O
a	O
coherent	O
perspective	O
with	O
respect	O
to	O
the	O
overall	O
problem	O
our	O
goal	O
in	O
writing	O
this	O
book	O
was	O
to	O
provide	O
a	O
clear	O
and	O
simple	O
account	O
of	O
the	O
key	O
ideas	O
and	O
algorithms	O
of	O
reinforcement	B
learning	I
we	O
wanted	O
our	O
treatment	O
to	O
be	O
accessible	O
to	O
readers	O
in	O
all	O
of	O
the	O
related	O
disciplines	O
but	O
we	O
could	O
not	O
cover	O
all	O
of	O
these	O
perspectives	O
in	O
detail	O
for	O
the	O
most	O
part	O
our	O
treatment	O
takes	O
the	O
point	O
of	O
view	O
of	O
artificial	B
intelligence	I
and	O
engineering	O
coverage	O
of	O
connections	O
to	O
other	O
fields	O
we	O
leave	O
to	O
others	O
or	O
to	O
another	O
time	O
we	O
also	O
chose	O
not	O
to	O
produce	O
a	O
rigorous	O
formal	O
treatment	O
of	O
reinforcement	B
learning	I
we	O
did	O
not	O
reach	O
for	O
the	O
highest	O
possible	O
level	O
of	O
mathematical	O
abstraction	O
and	O
did	O
not	O
rely	O
on	O
a	O
theorem	B
proof	B
format	O
we	O
tried	O
to	O
choose	O
a	O
level	O
of	O
mathematical	O
detail	O
that	O
points	O
the	O
mathematically	O
inclined	O
in	O
the	O
right	O
directions	O
without	O
distracting	O
from	O
the	O
simplicity	O
and	O
potential	O
generality	O
of	O
the	O
underlying	O
ideas	O
paragraphs	O
elided	O
in	O
favor	O
of	O
updated	O
content	O
in	O
the	O
second	O
edition	O
in	O
some	O
sense	O
we	O
have	O
been	O
working	O
toward	O
this	O
book	O
for	O
thirty	O
years	O
and	O
we	O
have	O
lots	O
of	O
people	O
to	O
thank	O
first	O
we	O
thank	O
those	O
who	O
have	O
personally	O
helped	O
us	O
develop	O
the	O
overall	O
view	O
presented	O
in	O
this	O
book	O
harry	O
klopf	B
for	O
helping	O
us	O
recognize	O
that	O
reinforcement	B
learning	I
needed	O
to	O
be	O
revived	O
chris	O
watkins	B
dimitri	O
bertsekas	O
john	O
tsitsiklis	O
and	O
paul	O
werbos	B
for	O
helping	O
us	O
see	O
the	O
value	B
of	O
the	O
relationships	O
to	O
dynamic	B
programming	I
john	O
moore	O
and	O
jim	O
kehoe	O
for	O
insights	O
and	O
inspirations	O
from	O
animal	O
learning	O
theory	O
oliver	O
selfridge	O
for	O
emphasizing	O
the	O
breadth	O
and	O
importance	O
of	O
adaptation	O
and	O
more	O
generally	O
our	O
colleagues	O
and	O
students	O
who	O
have	O
contributed	O
in	O
countless	O
ways	O
ron	O
williams	O
charles	O
anderson	O
satinder	O
singh	O
sridhar	O
mahadevan	O
steve	O
bradtke	O
bob	O
crites	O
peter	O
dayan	O
and	O
leemon	O
baird	O
our	O
view	O
of	O
reinforcement	B
learning	I
has	O
been	O
significantly	O
enriched	O
by	O
discussions	O
with	O
paul	O
cohen	O
paul	O
utgoff	O
martha	O
steenstrup	O
gerry	O
tesauro	O
mike	O
jordan	O
leslie	O
kaelbling	O
andrew	O
moore	O
chris	O
atkeson	O
tom	O
mitchell	O
nils	O
nilsson	O
stuart	O
russell	O
tom	O
dietterich	O
tom	O
dean	O
and	O
bob	O
narendra	O
we	O
thank	O
michael	O
littman	O
gerry	O
tesauro	O
bob	O
crites	O
satinder	O
singh	O
and	O
wei	O
zhang	O
for	O
providing	O
specifics	O
of	O
sections	O
and	O
respectively	O
we	O
thank	O
the	O
air	O
force	O
office	O
of	O
scientific	O
research	O
the	O
national	O
science	O
foundation	O
and	O
gte	O
laboratories	O
for	O
their	O
long	O
and	O
farsighted	O
support	O
we	O
also	O
wish	O
to	O
thank	O
the	O
many	O
people	O
who	O
have	O
read	O
drafts	O
of	O
this	O
book	O
and	O
provided	O
valuable	O
comments	O
including	O
tom	O
kalt	O
john	O
tsitsiklis	O
pawel	O
cichosz	O
olle	O
g	O
allmo	O
chuck	O
anderson	O
stuart	O
russell	O
ben	O
van	O
roy	O
paul	O
steenstrup	O
paul	O
cohen	O
sridhar	O
mahadevan	O
jette	O
randlov	O
brian	O
sheppard	O
thomas	O
o	O
connell	O
richard	O
coggins	O
cristina	O
versino	O
john	O
h	O
hiett	O
andreas	O
badelt	O
jay	O
ponte	O
joe	O
beck	O
justus	O
piater	O
martha	O
steenstrup	O
satinder	O
singh	O
tommi	O
jaakkola	O
dimitri	O
bertsekas	O
torbj	O
orn	O
ekman	O
christina	O
bj	O
orkman	O
jakob	O
carlstr	O
om	O
and	O
olle	O
palmgren	O
finally	O
we	O
thank	O
gwyn	O
mitchell	O
for	O
helping	O
in	O
many	O
ways	O
and	O
harry	O
stanton	O
and	O
bob	O
prior	O
for	O
being	O
our	O
champions	O
at	O
mit	O
press	O
summary	O
of	O
notation	O
capital	O
letters	O
are	O
used	O
for	O
random	O
variables	O
whereas	O
lower	O
case	O
letters	O
are	O
used	O
for	O
the	O
values	O
of	O
random	O
variables	O
and	O
for	O
scalar	O
functions	O
quantities	O
that	O
are	O
required	O
to	O
be	O
real-valued	O
vectors	O
are	O
written	O
in	O
bold	O
and	O
in	O
lower	O
case	O
if	O
random	O
variables	O
matrices	O
are	O
bold	O
capitals	O
equality	O
relationship	O
that	O
is	O
true	O
by	O
definition	O
approximately	O
equal	O
proportional	O
to	O
probability	O
that	O
a	O
random	O
variable	O
x	O
takes	O
on	O
the	O
value	B
x	O
random	O
variable	O
x	O
selected	O
from	O
distribution	O
px	O
prx	O
x	O
x	O
p	O
ex	O
argmaxa	O
f	O
a	O
value	B
of	O
a	O
at	O
which	O
f	O
takes	O
its	O
maximal	O
value	B
ln	O
x	O
ex	O
r	O
f	O
x	O
y	O
b	O
prx	O
x	O
expectation	O
of	O
a	O
random	O
variable	O
x	O
i	O
e	O
ex	O
pxx	O
natural	O
logarithm	O
of	O
x	O
the	O
base	O
of	O
the	O
natural	O
logarithm	O
e	O
carried	O
to	O
the	O
power	O
x	O
eln	O
x	O
x	O
set	O
of	O
real	O
numbers	O
function	O
f	O
from	O
elements	O
of	O
set	O
x	O
to	O
elements	O
of	O
set	O
y	O
assignment	O
the	O
real	O
interval	O
between	O
a	O
and	O
b	O
including	O
b	O
but	O
not	O
including	O
a	O
probability	O
of	O
taking	O
a	O
random	O
action	B
in	O
an	O
policy	B
step-size	O
parameters	O
discount-rate	O
parameter	O
decay-rate	O
parameter	O
for	O
eligibility	B
traces	I
indicator	O
function	O
if	O
the	O
predicate	O
is	O
true	O
else	O
in	O
a	O
multi-arm	O
bandit	O
problem	O
k	O
t	O
q	O
qta	O
nta	O
hta	O
ta	O
rt	O
number	O
of	O
actions	O
discrete	O
time	O
step	O
or	O
play	O
number	O
true	O
value	B
reward	O
of	O
action	B
a	O
estimate	O
at	O
time	O
t	O
of	O
q	O
number	O
of	O
times	O
action	B
a	O
has	O
been	O
selected	O
up	O
prior	O
to	O
time	O
t	O
learned	O
preference	O
for	O
selecting	O
action	B
a	O
probability	O
of	O
selecting	O
action	B
a	O
at	O
time	O
t	O
estimate	O
at	O
time	O
t	O
of	O
the	O
expected	B
reward	O
given	O
xvii	O
xviii	O
summary	O
of	O
notation	O
in	O
a	O
markov	O
decision	O
process	O
s	O
a	O
r	O
s	O
s	O
a	O
r	O
t	O
t	O
t	O
at	O
st	O
rt	O
gt	O
gttn	O
gth	O
gth	O
g	O
t	O
g	O
th	O
g	O
s	O
t	O
g	O
a	O
t	O
states	O
an	O
action	B
a	O
reward	O
set	O
of	O
all	O
nonterminal	O
states	O
set	O
of	O
all	O
states	O
including	O
the	O
terminal	O
state	B
set	O
of	O
all	O
actions	O
set	O
of	O
all	O
possible	O
rewards	O
a	O
finite	O
subset	O
of	O
r	O
subset	O
of	O
e	O
g	O
r	O
r	O
is	O
an	O
element	O
of	O
e	O
g	O
s	O
s	O
r	O
r	O
number	O
of	O
elements	O
in	O
set	O
s	O
discrete	O
time	O
step	O
final	O
time	O
step	O
of	O
an	O
episode	O
or	O
of	O
the	O
episode	O
including	O
time	O
step	O
t	O
action	B
at	O
time	O
t	O
state	B
at	O
time	O
t	O
typically	O
due	O
stochastically	O
to	O
st	O
and	O
at	O
reward	O
at	O
time	O
t	O
typically	O
due	O
stochastically	O
to	O
st	O
and	O
at	O
policy	B
rule	O
action	B
taken	O
in	O
state	B
s	O
under	O
deterministic	O
policy	B
probability	O
of	O
taking	O
action	B
a	O
in	O
state	B
s	O
under	O
stochastic	O
policy	B
probability	O
of	O
taking	O
action	B
a	O
in	O
state	B
s	O
given	O
parameter	O
vector	B
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
with	O
respect	O
to	O
return	B
following	O
time	O
t	O
n-step	B
return	B
from	O
t	O
to	O
hdiscounted	O
and	O
corrected	O
flat	O
return	B
and	O
uncorrected	O
from	O
t	O
to	O
h	O
truncated	B
corrected	O
corrected	O
by	O
estimated	O
state	B
values	O
corrected	O
by	O
estimated	O
action	B
values	O
rs	O
a	O
a	O
rs	O
a	O
probability	O
of	O
transition	O
to	O
state	B
with	O
reward	O
r	O
from	O
state	B
s	O
and	O
action	B
a	O
probability	O
of	O
transition	O
to	O
state	B
from	O
state	B
s	O
taking	O
action	B
a	O
expected	B
immediate	O
reward	O
on	O
transition	O
from	O
s	O
to	O
under	O
action	B
a	O
v	O
v	O
q	O
a	O
q	O
a	O
v	O
vt	O
q	O
qt	O
vts	O
ut	O
value	B
of	O
state	B
s	O
under	O
policy	B
return	B
value	B
of	O
state	B
s	O
under	O
the	O
optimal	O
policy	B
value	B
of	O
taking	O
action	B
a	O
in	O
state	B
s	O
under	O
policy	B
value	B
of	O
taking	O
action	B
a	O
in	O
state	B
s	O
under	O
the	O
optimal	O
policy	B
array	O
estimates	O
of	O
state-value	O
function	O
v	O
or	O
v	O
array	O
estimates	O
of	O
action-value	O
function	O
q	O
or	O
q	O
expected	B
approximate	O
action	B
value	B
a	O
target	O
for	O
estimate	O
at	O
time	O
t	O
summary	O
of	O
notation	O
xix	O
temporal-difference	O
error	O
at	O
t	O
random	O
variable	O
d-vector	O
of	O
weights	O
underlying	O
an	O
approximate	O
value	B
function	I
ith	O
component	O
of	O
learnable	O
weight	O
vector	B
dimensionality	O
the	O
number	O
of	O
components	O
of	O
w	O
alternate	O
dimensionality	O
the	O
number	O
of	O
components	O
of	O
number	O
of	O
in	O
a	O
sparse	B
binary	O
feature	O
vector	B
approximate	O
value	B
of	O
state	B
s	O
given	O
weight	O
vector	B
w	O
alternate	O
notation	O
for	O
vsw	O
approximate	O
value	B
of	O
state	B
action	B
pair	O
s	O
a	O
given	O
weight	O
vector	B
w	O
vector	B
of	O
features	O
visible	O
when	O
in	O
state	B
s	O
vector	B
of	O
features	O
visible	O
when	O
in	O
state	B
s	O
taking	O
action	B
a	O
t	O
w	O
wt	O
wi	O
wti	O
d	O
m	O
vsw	O
vws	O
qs	O
a	O
w	O
xs	O
xs	O
a	O
xis	O
xis	O
a	O
ith	O
component	O
of	O
vector	B
xs	O
or	O
xs	O
a	O
xt	O
v	O
vt	O
zt	O
vsw	O
qs	O
a	O
w	O
t	O
j	O
j	O
j	O
hs	O
a	O
shorthand	O
for	O
xst	O
or	O
xst	O
at	O
inner	O
product	O
of	O
vectors	O
on-policy	B
distribution	I
over	O
states	O
of	O
the	O
for	O
all	O
s	O
s	O
wixi	O
e	O
g	O
vsw	O
norm	O
of	O
any	O
vector	B
xs	O
i	O
secondary	O
d-vector	O
of	O
weights	O
used	O
to	O
learn	O
w	O
d-vector	O
of	O
eligibility	B
traces	I
at	O
time	O
t	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
vsw	O
with	O
respect	O
to	O
w	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
qs	O
a	O
w	O
with	O
respect	O
to	O
w	O
parameter	O
vector	B
of	O
target	O
policy	B
policy	B
corresponding	O
to	O
parameter	O
performance	O
measure	O
for	O
policy	B
or	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
j	O
with	O
respect	O
to	O
preference	O
for	O
selecting	O
action	B
a	O
in	O
state	B
s	O
based	O
on	O
b	O
behavior	O
policy	B
used	O
to	O
select	O
actions	O
while	O
learning	O
about	O
target	O
policy	B
th	O
t	O
r	O
rt	O
a	O
b	O
wtd	O
i	O
p	O
d	O
x	O
or	O
a	O
baseline	B
function	O
b	O
s	O
r	O
for	O
policy-gradient	O
methods	O
or	O
a	O
branching	B
factor	I
for	O
a	O
search	O
tree	O
importance	B
sampling	I
ratio	B
for	O
time	O
t	O
through	O
time	O
h	O
importance	B
sampling	I
ratio	B
for	O
time	O
t	O
alone	O
t	O
tt	O
average	O
reward	O
rate	O
for	O
policy	B
estimate	O
of	O
r	O
at	O
time	O
t	O
d	O
d	O
matrix	O
a	O
d-dimensional	O
vector	B
b	O
a	O
d-vector	O
td	B
fixed	O
point	O
wtd	O
identity	O
matrix	O
matrix	O
of	O
state-transition	O
probabilities	O
under	O
diagonal	O
matrix	O
with	O
the	O
on	O
its	O
diagonal	O
d	O
matrix	O
with	O
xs	O
as	O
its	O
rows	O
chapter	O
introduction	O
the	O
idea	O
that	O
we	O
learn	O
by	O
interacting	O
with	O
our	O
environment	B
is	O
probably	O
the	O
first	O
to	O
occur	O
to	O
us	O
when	O
we	O
think	O
about	O
the	O
nature	O
of	O
learning	O
when	O
an	O
infant	O
plays	O
waves	O
its	O
arms	O
or	O
looks	O
about	O
it	O
has	O
no	O
explicit	O
teacher	O
but	O
it	O
does	O
have	O
a	O
direct	O
sensorimotor	O
connection	O
to	O
its	O
environment	B
exercising	O
this	O
connection	O
produces	O
a	O
wealth	O
of	O
information	O
about	O
cause	O
and	O
effect	O
about	O
the	O
consequences	O
of	O
actions	O
and	O
about	O
what	O
to	O
do	O
in	O
order	O
to	O
achieve	O
goals	O
throughout	O
our	O
lives	O
such	O
interactions	O
are	O
undoubtedly	O
a	O
major	O
source	O
of	O
knowledge	O
about	O
our	O
environment	B
and	O
ourselves	O
whether	O
we	O
are	O
learning	O
to	O
drive	O
a	O
car	O
or	O
to	O
hold	O
a	O
conversation	O
we	O
are	O
acutely	O
aware	O
of	O
how	O
our	O
environment	B
responds	O
to	O
what	O
we	O
do	O
and	O
we	O
seek	O
to	O
influence	O
what	O
happens	O
through	O
our	O
behavior	O
learning	O
from	O
interaction	O
is	O
a	O
foundational	O
idea	O
underlying	O
nearly	O
all	O
theories	O
of	O
learning	O
and	O
intelligence	O
in	O
this	O
book	O
we	O
explore	O
a	O
computational	O
approach	O
to	O
learning	O
from	O
interaction	O
rather	O
than	O
directly	O
theorizing	O
about	O
how	O
people	O
or	O
animals	O
learn	O
we	O
primarily	O
explore	O
idealized	O
learning	O
situations	O
and	O
evaluate	O
the	O
effectiveness	O
of	O
various	O
learning	O
that	O
is	O
we	O
adopt	O
the	O
perspective	O
of	O
an	O
artificial	B
intelligence	I
researcher	O
or	O
engineer	O
we	O
explore	O
designs	O
for	O
machines	O
that	O
are	O
effective	O
in	O
solving	O
learning	O
problems	O
of	O
scientific	O
or	O
economic	O
interest	O
evaluating	O
the	O
designs	O
through	O
mathematical	O
analysis	O
or	O
computational	O
experiments	O
the	O
approach	O
we	O
explore	O
called	O
reinforcement	B
learning	I
is	O
much	O
more	O
focused	O
on	O
goal-directed	O
learning	O
from	O
interaction	O
than	O
are	O
other	O
approaches	O
to	O
machine	O
learning	O
reinforcement	B
learning	I
reinforcement	B
learning	I
is	O
learning	O
what	O
to	O
do	O
how	O
to	O
map	O
situations	O
to	O
actions	O
so	O
as	O
to	O
maximize	O
a	O
numerical	O
reward	B
signal	I
the	O
learner	O
is	O
not	O
told	O
which	O
actions	O
to	O
take	O
but	O
instead	O
must	O
discover	O
which	O
actions	O
yield	O
the	O
most	O
reward	O
by	O
trying	O
them	O
in	O
the	O
most	O
interesting	O
and	O
challenging	O
cases	O
actions	O
may	O
affect	O
not	O
only	O
the	O
immediate	O
relationships	O
to	O
psychology	B
and	O
neuroscience	B
are	O
summarized	O
in	O
chapters	O
and	O
chapter	O
introduction	O
reward	O
but	O
also	O
the	O
next	O
situation	O
and	O
through	O
that	O
all	O
subsequent	O
rewards	O
these	O
two	O
characteristics	O
trial-and-error	O
search	O
and	O
delayed	B
reward	I
are	O
the	O
two	O
most	O
important	O
distinguishing	O
features	O
of	O
reinforcement	B
learning	I
reinforcement	B
learning	I
like	O
many	O
topics	O
whose	O
names	O
end	O
with	O
ing	O
such	O
as	O
machine	O
learning	O
and	O
mountaineering	O
is	O
simultaneously	O
a	O
problem	O
a	O
class	O
of	O
solution	O
methods	O
that	O
work	O
well	O
on	O
the	O
problem	O
and	O
the	O
field	O
that	O
studies	O
this	O
problem	O
and	O
its	O
solution	O
methods	O
it	O
is	O
convenient	O
to	O
use	O
a	O
single	O
name	O
for	O
all	O
three	O
things	O
but	O
at	O
the	O
same	O
time	O
essential	O
to	O
keep	O
the	O
three	O
conceptually	O
separate	O
in	O
particular	O
the	O
distinction	O
between	O
problems	O
and	O
solution	O
methods	O
is	O
very	O
important	O
in	O
reinforcement	B
learning	I
failing	O
to	O
make	O
this	O
distinction	O
is	O
the	O
source	O
of	O
many	O
confusions	O
we	O
formalize	O
the	O
problem	O
of	O
reinforcement	B
learning	I
using	O
ideas	O
from	O
dynamical	O
systems	O
theory	O
specifically	O
as	O
the	O
optimal	B
control	B
of	O
incompletely-known	O
markov	O
decision	O
processes	O
the	O
details	O
of	O
this	O
formalization	O
must	O
wait	O
until	O
chapter	O
but	O
the	O
basic	O
idea	O
is	O
simply	O
to	O
capture	O
the	O
most	O
important	O
aspects	O
of	O
the	O
real	O
problem	O
facing	O
a	O
learning	O
agent	O
interacting	O
over	O
time	O
with	O
its	O
environment	B
to	O
achieve	O
a	O
goal	O
a	O
learning	O
agent	O
must	O
be	O
able	O
to	O
sense	O
the	O
state	B
of	O
its	O
environment	B
to	O
some	O
extent	O
and	O
must	O
be	O
able	O
to	O
take	O
actions	O
that	O
affect	O
the	O
state	B
the	O
agent	O
also	O
must	O
have	O
a	O
goal	O
or	O
goals	O
relating	O
to	O
the	O
state	B
of	O
the	O
environment	B
markov	O
decision	O
processes	O
are	O
intended	O
to	O
include	O
just	O
these	O
three	O
aspects	O
sensation	O
action	B
and	O
goal	O
in	O
their	O
simplest	O
possible	O
forms	O
without	O
trivializing	O
any	O
of	O
them	O
any	O
method	O
that	O
is	O
well	O
suited	O
to	O
solving	O
such	O
problems	O
we	O
consider	O
to	O
be	O
a	O
reinforcement	B
learning	I
method	O
reinforcement	B
learning	I
is	O
different	O
from	O
supervised	B
learning	I
the	O
kind	O
of	O
learning	O
studied	O
in	O
most	O
current	O
research	O
in	O
the	O
field	O
of	O
machine	O
learning	O
supervised	B
learning	I
is	O
learning	O
from	O
a	O
training	O
set	O
of	O
labeled	O
examples	O
provided	O
by	O
a	O
knowledgable	O
external	O
supervisor	O
each	O
example	O
is	O
a	O
description	O
of	O
a	O
situation	O
together	O
with	O
a	O
specification	O
the	O
label	O
of	O
the	O
correct	O
action	B
the	O
system	O
should	O
take	O
to	O
that	O
situation	O
which	O
is	O
often	O
to	O
identify	O
a	O
category	O
to	O
which	O
the	O
situation	O
belongs	O
the	O
object	O
of	O
this	O
kind	O
of	O
learning	O
is	O
for	O
the	O
system	O
to	O
extrapolate	O
or	O
generalize	O
its	O
responses	O
so	O
that	O
it	O
acts	O
correctly	O
in	O
situations	O
not	O
present	O
in	O
the	O
training	O
set	O
this	O
is	O
an	O
important	O
kind	O
of	O
learning	O
but	O
alone	O
it	O
is	O
not	O
adequate	O
for	O
learning	O
from	O
interaction	O
in	O
interactive	O
problems	O
it	O
is	O
often	O
impractical	O
to	O
obtain	O
examples	O
of	O
desired	O
behavior	O
that	O
are	O
both	O
correct	O
and	O
representative	O
of	O
all	O
the	O
situations	O
in	O
which	O
the	O
agent	O
has	O
to	O
act	O
in	O
uncharted	O
territory	O
where	O
one	O
would	O
expect	O
learning	O
to	O
be	O
most	O
beneficial	O
an	O
agent	O
must	O
be	O
able	O
to	O
learn	O
from	O
its	O
own	O
experience	O
reinforcement	B
learning	I
is	O
also	O
different	O
from	O
what	O
machine	O
learning	O
researchers	O
call	O
unsupervised	B
learning	I
which	O
is	O
typically	O
about	O
finding	O
structure	O
hidden	O
in	O
collections	O
of	O
unlabeled	O
data	O
the	O
terms	O
supervised	B
learning	I
and	O
unsupervised	B
learning	I
would	O
seem	O
to	O
exhaustively	O
classify	O
machine	O
learning	O
paradigms	O
but	O
they	O
do	O
not	O
although	O
one	O
might	O
be	O
tempted	O
to	O
think	O
of	O
reinforcement	B
learning	I
as	O
a	O
kind	O
of	O
unsupervised	B
learning	I
because	O
it	O
does	O
not	O
rely	O
on	O
examples	O
of	O
correct	O
behavior	O
reinforcement	B
learning	I
is	O
trying	O
to	O
maximize	O
a	O
reward	B
signal	I
instead	O
of	O
trying	O
to	O
find	O
hidden	O
structure	O
uncovering	O
structure	O
in	O
an	O
agent	O
s	O
experience	O
can	O
certainly	O
be	O
useful	O
in	O
reinforcement	B
learning	I
but	O
by	O
itself	O
does	O
not	O
address	O
the	O
reinforcement	B
learning	I
problem	O
of	O
maximizing	O
a	O
reward	B
signal	I
we	O
therefore	O
consider	O
reinforcement	B
learning	I
to	O
be	O
a	O
third	O
machine	O
learning	O
reinforcement	B
learning	I
paradigm	O
alongside	O
supervised	B
learning	I
and	O
unsupervised	B
learning	I
and	O
perhaps	O
other	O
paradigms	O
as	O
well	O
one	O
of	O
the	O
challenges	O
that	O
arise	O
in	O
reinforcement	B
learning	I
and	O
not	O
in	O
other	O
kinds	O
of	O
learning	O
is	O
the	O
trade-off	O
between	O
exploration	O
and	O
exploitation	O
to	O
obtain	O
a	O
lot	O
of	O
reward	O
a	O
reinforcement	B
learning	I
agent	O
must	O
prefer	O
actions	O
that	O
it	O
has	O
tried	O
in	O
the	O
past	O
and	O
found	O
to	O
be	O
effective	O
in	O
producing	O
reward	O
but	O
to	O
discover	O
such	O
actions	O
it	O
has	O
to	O
try	O
actions	O
that	O
it	O
has	O
not	O
selected	O
before	O
the	O
agent	O
has	O
to	O
exploit	O
what	O
it	O
has	O
already	O
experienced	O
in	O
order	O
to	O
obtain	O
reward	O
but	O
it	O
also	O
has	O
to	O
explore	O
in	O
order	O
to	O
make	O
better	O
action	B
selections	O
in	O
the	O
future	O
the	O
dilemma	O
is	O
that	O
neither	O
exploration	O
nor	O
exploitation	O
can	O
be	O
pursued	O
exclusively	O
without	O
failing	O
at	O
the	O
task	O
the	O
agent	O
must	O
try	O
a	O
variety	O
of	O
actions	O
and	O
progressively	O
favor	O
those	O
that	O
appear	O
to	O
be	O
best	O
on	O
a	O
stochastic	O
task	O
each	O
action	B
must	O
be	O
tried	O
many	O
times	O
to	O
gain	O
a	O
reliable	O
estimate	O
of	O
its	O
expected	B
reward	O
the	O
exploration	O
exploitation	O
dilemma	O
has	O
been	O
intensively	O
studied	O
by	O
mathematicians	O
for	O
many	O
decades	O
yet	O
remains	O
unresolved	O
for	O
now	O
we	O
simply	O
note	O
that	O
the	O
entire	O
issue	O
of	O
balancing	O
exploration	O
and	O
exploitation	O
does	O
not	O
even	O
arise	O
in	O
supervised	O
and	O
unsupervised	B
learning	I
at	O
least	O
in	O
their	O
purest	O
forms	O
another	O
key	O
feature	O
of	O
reinforcement	B
learning	I
is	O
that	O
it	O
explicitly	O
considers	O
the	O
whole	O
problem	O
of	O
a	O
goal-directed	O
agent	O
interacting	O
with	O
an	O
uncertain	O
environment	B
this	O
is	O
in	O
contrast	O
to	O
many	O
approaches	O
that	O
consider	O
subproblems	O
without	O
addressing	O
how	O
they	O
might	O
fit	O
into	O
a	O
larger	O
picture	O
for	O
example	O
we	O
have	O
mentioned	O
that	O
much	O
of	O
machine	O
learning	O
research	O
is	O
concerned	O
with	O
supervised	B
learning	I
without	O
explicitly	O
specifying	O
how	O
such	O
an	O
ability	O
would	O
finally	O
be	O
useful	O
other	O
researchers	O
have	O
developed	O
theories	O
of	O
planning	B
with	O
general	O
goals	O
but	O
without	O
considering	O
planning	B
s	O
role	O
in	O
real-time	O
decision	O
making	O
or	O
the	O
question	O
of	O
where	O
the	O
predictive	O
models	O
necessary	O
for	O
planning	B
would	O
come	O
from	O
although	O
these	O
approaches	O
have	O
yielded	O
many	O
useful	O
results	O
their	O
focus	O
on	O
isolated	O
subproblems	O
is	O
a	O
significant	O
limitation	O
reinforcement	B
learning	I
takes	O
the	O
opposite	O
tack	O
starting	O
with	O
a	O
complete	O
interactive	O
goal-seeking	O
agent	O
all	O
reinforcement	B
learning	I
agents	O
have	O
explicit	O
goals	O
can	O
sense	O
aspects	O
of	O
their	O
environments	O
and	O
can	O
choose	O
actions	O
to	O
influence	O
their	O
environments	O
moreover	O
it	O
is	O
usually	O
assumed	O
from	O
the	O
beginning	O
that	O
the	O
agent	O
has	O
to	O
operate	O
despite	O
significant	O
uncertainty	O
about	O
the	O
environment	B
it	O
faces	O
when	O
reinforcement	B
learning	I
involves	O
planning	B
it	O
has	O
to	O
address	O
the	O
interplay	O
between	O
planning	B
and	O
real-time	O
action	B
selection	O
as	O
well	O
as	O
the	O
question	O
of	O
how	O
environment	B
models	O
are	O
acquired	O
and	O
improved	O
when	O
reinforcement	B
learning	I
involves	O
supervised	B
learning	I
it	O
does	O
so	O
for	O
specific	O
reasons	O
that	O
determine	O
which	O
capabilities	O
are	O
critical	O
and	O
which	O
are	O
not	O
for	O
learning	O
research	O
to	O
make	O
progress	O
important	O
subproblems	O
have	O
to	O
be	O
isolated	O
and	O
studied	O
but	O
they	O
should	O
be	O
subproblems	O
that	O
play	O
clear	O
roles	O
in	O
complete	O
interactive	O
goal-seeking	O
agents	O
even	O
if	O
all	O
the	O
details	O
of	O
the	O
complete	O
agent	O
cannot	O
yet	O
be	O
filled	O
in	O
by	O
a	O
complete	O
interactive	O
goal-seeking	O
agent	O
we	O
do	O
not	O
always	O
mean	O
something	O
like	O
a	O
complete	O
organism	O
or	O
robot	O
these	O
are	O
clearly	O
examples	O
but	O
a	O
complete	O
interactive	O
goal-seeking	O
agent	O
can	O
also	O
be	O
a	O
component	O
of	O
a	O
larger	O
behaving	O
system	O
in	O
this	O
case	O
the	O
agent	O
directly	O
interacts	O
with	O
the	O
rest	O
of	O
the	O
larger	O
system	O
and	O
indirectly	O
interacts	O
with	O
the	O
larger	O
system	O
s	O
environment	B
a	O
simple	O
example	O
is	O
an	O
agent	O
that	O
monitors	O
the	O
charge	O
level	O
of	O
robot	O
s	O
battery	O
and	O
sends	O
commands	O
to	O
the	O
robot	O
s	O
control	B
architecture	O
chapter	O
introduction	O
this	O
agent	O
s	O
environment	B
is	O
the	O
rest	O
of	O
the	O
robot	O
together	O
with	O
the	O
robot	O
s	O
environment	B
one	O
must	O
look	O
beyond	O
the	O
most	O
obvious	O
examples	O
of	O
agents	O
and	O
their	O
environments	O
to	O
appreciate	O
the	O
generality	O
of	O
the	O
reinforcement	B
learning	I
framework	O
one	O
of	O
the	O
most	O
exciting	O
aspects	O
of	O
modern	O
reinforcement	B
learning	I
is	O
its	O
substantive	O
and	O
fruitful	O
interactions	O
with	O
other	O
engineering	O
and	O
scientific	O
disciplines	O
reinforcement	B
learning	I
is	O
part	O
of	O
a	O
decades-long	O
trend	O
within	O
artificial	B
intelligence	I
and	O
machine	O
learning	O
toward	O
greater	O
integration	O
with	O
statistics	O
optimization	O
and	O
other	O
mathematical	O
subjects	O
for	O
example	O
the	O
ability	O
of	O
some	O
reinforcement	B
learning	I
methods	O
to	O
learn	O
with	O
parameterized	O
approximators	O
addresses	O
the	O
classical	O
curse	B
of	I
dimensionality	I
in	O
operations	O
research	O
and	B
control	B
theory	I
more	O
distinctively	O
reinforcement	B
learning	I
has	O
also	O
interacted	O
strongly	O
with	O
psychology	B
and	O
neuroscience	B
with	O
substantial	O
benefits	O
going	O
both	O
ways	O
of	O
all	O
the	O
forms	O
of	O
machine	O
learning	O
reinforcement	B
learning	I
is	O
the	O
closest	O
to	O
the	O
kind	O
of	O
learning	O
that	O
humans	O
and	O
other	O
animals	O
do	O
and	O
many	O
of	O
the	O
core	O
algorithms	O
of	O
reinforcement	B
learning	I
were	O
originally	O
inspired	O
by	O
biological	O
learning	O
systems	O
reinforcement	B
learning	I
has	O
also	O
given	O
back	O
both	O
through	O
a	O
psychological	O
model	O
of	O
animal	O
learning	O
that	O
better	O
matches	O
some	O
of	O
the	O
empirical	O
data	O
and	O
through	O
an	O
influential	O
model	O
of	O
parts	O
of	O
the	O
brain	O
s	O
reward	O
system	O
the	O
body	O
of	O
this	O
book	O
develops	O
the	O
ideas	O
of	O
reinforcement	B
learning	I
that	O
pertain	O
to	O
engineering	O
and	B
artificial	B
intelligence	I
with	O
connections	O
to	O
psychology	B
and	O
neuroscience	B
summarized	O
in	O
chapters	O
and	O
finally	O
reinforcement	B
learning	I
is	O
also	O
part	O
of	O
a	O
larger	O
trend	O
in	O
artificial	B
intelligence	I
back	O
toward	O
simple	O
general	O
principles	O
since	O
the	O
late	O
s	O
many	O
artificial	B
intelligence	I
researchers	O
presumed	O
that	O
there	O
are	O
no	O
general	O
principles	O
to	O
be	O
discovered	O
that	O
intelligence	O
is	O
instead	O
due	O
to	O
the	O
possession	O
of	O
a	O
vast	O
number	O
of	O
special	O
purpose	O
tricks	O
procedures	O
and	O
heuristics	O
it	O
was	O
sometimes	O
said	O
that	O
if	O
we	O
could	O
just	O
get	O
enough	O
relevant	O
facts	O
into	O
a	O
machine	O
say	O
one	O
million	O
or	O
one	O
billion	O
then	O
it	O
would	O
become	O
intelligent	O
methods	O
based	O
on	O
general	O
principles	O
such	O
as	O
search	O
or	O
learning	O
were	O
characterized	O
as	O
weak	O
methods	O
whereas	O
those	O
based	O
on	O
specific	O
knowledge	O
were	O
called	O
strong	O
methods	O
this	O
view	O
is	O
still	O
common	O
today	O
but	O
not	O
dominant	O
from	O
our	O
point	O
of	O
view	O
it	O
was	O
simply	O
premature	O
too	O
little	O
effort	O
had	O
been	O
put	O
into	O
the	O
search	O
for	O
general	O
principles	O
to	O
conclude	O
that	O
there	O
were	O
none	O
modern	O
artificial	B
intelligence	I
now	O
includes	O
much	O
research	O
looking	O
for	O
general	O
principles	O
of	O
learning	O
search	O
and	O
decision	O
making	O
as	O
well	O
as	O
trying	O
to	O
incorporate	O
vast	O
amounts	O
of	O
domain	O
knowledge	O
it	O
is	O
not	O
clear	O
how	O
far	O
back	O
the	O
pendulum	O
will	O
swing	O
but	O
reinforcement	B
learning	I
research	O
is	O
certainly	O
part	O
of	O
the	O
swing	O
back	O
toward	O
simpler	O
and	O
fewer	O
general	O
principles	O
of	O
artificial	B
intelligence	I
examples	O
a	O
good	O
way	O
to	O
understand	O
reinforcement	B
learning	I
is	O
to	O
consider	O
some	O
of	O
the	O
examples	O
and	O
possible	O
applications	O
that	O
have	O
guided	O
its	O
development	O
a	O
master	O
chess	B
player	O
makes	O
a	O
move	O
the	O
choice	O
is	O
informed	O
both	O
by	O
planning	B
anticipating	O
possible	O
replies	O
and	O
counterreplies	O
and	O
by	O
immediate	O
intuitive	O
judgments	O
of	O
the	O
desirability	O
of	O
particular	O
positions	O
and	O
moves	O
an	O
adaptive	O
controller	O
adjusts	O
parameters	O
of	O
a	O
petroleum	O
refinery	O
s	O
operation	O
in	O
examples	O
real	O
time	O
the	O
controller	O
optimizes	O
the	O
yieldcostquality	O
trade-off	O
on	O
the	O
basis	O
of	O
specified	O
marginal	O
costs	O
without	O
sticking	O
strictly	O
to	O
the	O
set	O
points	O
originally	O
suggested	O
by	O
engineers	O
a	O
gazelle	O
calf	O
struggles	O
to	O
its	O
feet	O
minutes	O
after	O
being	O
born	O
half	O
an	O
hour	O
later	O
it	O
is	O
running	O
at	O
miles	O
per	O
hour	O
a	O
mobile	O
robot	O
decides	O
whether	O
it	O
should	O
enter	O
a	O
new	O
room	O
in	O
search	O
of	O
more	O
trash	O
to	O
collect	O
or	O
start	O
trying	O
to	O
find	O
its	O
way	O
back	O
to	O
its	O
battery	O
recharging	O
station	O
it	O
makes	O
its	O
decision	O
based	O
on	O
the	O
current	O
charge	O
level	O
of	O
its	O
battery	O
and	O
how	O
quickly	O
and	O
easily	O
it	O
has	O
been	O
able	O
to	O
find	O
the	O
recharger	O
in	O
the	O
past	O
phil	O
prepares	O
his	O
breakfast	O
closely	O
examined	O
even	O
this	O
apparently	O
mundane	O
activity	O
reveals	O
a	O
complex	O
web	O
of	O
conditional	O
behavior	O
and	O
interlocking	O
goal	O
subgoal	O
relationships	O
walking	O
to	O
the	O
cupboard	O
opening	O
it	O
selecting	O
a	O
cereal	O
box	O
then	O
reaching	O
for	O
grasping	O
and	O
retrieving	O
the	O
box	O
other	O
complex	O
tuned	O
interactive	O
sequences	O
of	O
behavior	O
are	O
required	O
to	O
obtain	O
a	O
bowl	O
spoon	O
and	O
milk	O
jug	O
each	O
step	O
involves	O
a	O
series	O
of	O
eye	O
movements	O
to	O
obtain	O
information	O
and	O
to	O
guide	O
reaching	O
and	O
locomotion	O
rapid	O
judgments	O
are	O
continually	O
made	O
about	O
how	O
to	O
carry	O
the	O
objects	O
or	O
whether	O
it	O
is	O
better	O
to	O
ferry	O
some	O
of	O
them	O
to	O
the	O
dining	O
table	O
before	O
obtaining	O
others	O
each	O
step	O
is	O
guided	O
by	O
goals	O
such	O
as	O
grasping	O
a	O
spoon	O
or	O
getting	O
to	O
the	O
refrigerator	O
and	O
is	O
in	O
service	O
of	O
other	O
goals	O
such	O
as	O
having	O
the	O
spoon	O
to	O
eat	O
with	O
once	O
the	O
cereal	O
is	O
prepared	O
and	O
ultimately	O
obtaining	O
nourishment	O
whether	O
he	O
is	O
aware	O
of	O
it	O
or	O
not	O
phil	O
is	O
accessing	O
information	O
about	O
the	O
state	B
of	O
his	O
body	O
that	O
determines	O
his	O
nutritional	O
needs	O
level	O
of	O
hunger	O
and	O
food	O
preferences	O
these	O
examples	O
share	O
features	O
that	O
are	O
so	O
basic	O
that	O
they	O
are	O
easy	O
to	O
overlook	O
all	O
involve	O
interaction	O
between	O
an	O
active	O
decision-making	O
agent	O
and	O
its	O
environment	B
within	O
which	O
the	O
agent	O
seeks	O
to	O
achieve	O
a	O
goal	O
despite	O
uncertainty	O
about	O
its	O
environment	B
the	O
agent	O
s	O
actions	O
are	O
permitted	O
to	O
affect	O
the	O
future	O
state	B
of	O
the	O
environment	B
the	O
next	O
chess	B
position	O
the	O
level	O
of	O
reservoirs	O
of	O
the	O
refinery	O
the	O
robot	O
s	O
next	O
location	O
and	O
the	O
future	O
charge	O
level	O
of	O
its	O
battery	O
thereby	O
affecting	O
the	O
options	B
and	O
opportunities	O
available	O
to	O
the	O
agent	O
at	O
later	O
times	O
correct	O
choice	O
requires	O
taking	O
into	O
account	O
indirect	O
delayed	O
consequences	O
of	O
actions	O
and	O
thus	O
may	O
require	O
foresight	O
or	O
planning	B
at	O
the	O
same	O
time	O
in	O
all	O
these	O
examples	O
the	O
effects	O
of	O
actions	O
cannot	O
be	O
fully	O
predicted	O
thus	O
the	O
agent	O
must	O
monitor	O
its	O
environment	B
frequently	O
and	O
react	O
appropriately	O
for	O
example	O
phil	O
must	O
watch	O
the	O
milk	O
he	O
pours	O
into	O
his	O
cereal	O
bowl	O
to	O
keep	O
it	O
from	O
overflowing	O
all	O
these	O
examples	O
involve	O
goals	O
that	O
are	O
explicit	O
in	O
the	O
sense	O
that	O
the	O
agent	O
can	O
judge	O
progress	O
toward	O
its	O
goal	O
based	O
on	O
what	O
it	O
can	O
sense	O
directly	O
the	O
chess	B
player	O
knows	O
whether	O
or	O
not	O
he	O
wins	O
the	O
refinery	O
controller	O
knows	O
how	O
much	O
petroleum	O
is	O
being	O
produced	O
the	O
gazelle	O
calf	O
knows	O
when	O
it	O
falls	O
the	O
mobile	O
robot	O
knows	O
when	O
its	O
batteries	O
run	O
down	O
and	O
phil	O
knows	O
whether	O
or	O
not	O
he	O
is	O
enjoying	O
his	O
breakfast	O
in	O
all	O
of	O
these	O
examples	O
the	O
agent	O
can	O
use	O
its	O
experience	O
to	O
improve	O
its	O
performance	O
over	O
time	O
the	O
chess	B
player	O
refines	O
the	O
intuition	O
he	O
uses	O
to	O
evaluate	O
positions	O
thereby	O
improving	O
his	O
play	O
the	O
gazelle	O
calf	O
improves	O
the	O
efficiency	O
with	O
which	O
it	O
can	O
run	O
phil	O
learns	O
to	O
streamline	O
making	O
his	O
breakfast	O
the	O
knowledge	O
the	O
agent	O
brings	O
to	O
the	O
task	O
chapter	O
introduction	O
at	O
the	O
start	O
either	O
from	O
previous	O
experience	O
with	O
related	O
tasks	O
or	O
built	O
into	O
it	O
by	O
design	O
or	O
evolution	B
influences	O
what	O
is	O
useful	O
or	O
easy	O
to	O
learn	O
but	O
interaction	O
with	O
the	O
environment	B
is	O
essential	O
for	O
adjusting	O
behavior	O
to	O
exploit	O
specific	O
features	O
of	O
the	O
task	O
elements	O
of	O
reinforcement	B
learning	I
beyond	O
the	O
agent	O
and	O
the	O
environment	B
one	O
can	O
identify	O
four	O
main	O
subelements	O
of	O
a	O
reinforcement	B
learning	I
system	O
a	O
policy	B
a	O
reward	B
signal	I
a	O
value	B
function	I
and	O
optionally	O
a	O
model	B
of	I
the	I
environment	B
a	O
policy	B
defines	O
the	O
learning	O
agent	O
s	O
way	O
of	O
behaving	O
at	O
a	O
given	O
time	O
roughly	O
speaking	O
a	O
policy	B
is	O
a	O
mapping	O
from	O
perceived	O
states	O
of	O
the	O
environment	B
to	O
actions	O
to	O
be	O
taken	O
when	O
in	O
those	O
states	O
it	O
corresponds	O
to	O
what	O
in	B
psychology	B
would	O
be	O
called	O
a	O
set	O
of	O
stimulus	O
response	O
rules	O
or	O
associations	O
in	O
some	O
cases	O
the	O
policy	B
may	O
be	O
a	O
simple	O
function	O
or	O
lookup	O
table	O
whereas	O
in	O
others	O
it	O
may	O
involve	O
extensive	O
computation	O
such	O
as	O
a	O
search	O
process	O
the	O
policy	B
is	O
the	O
core	O
of	O
a	O
reinforcement	B
learning	I
agent	O
in	O
the	O
sense	O
that	O
it	O
alone	O
is	O
sufficient	O
to	O
determine	O
behavior	O
in	O
general	O
policies	O
may	O
be	O
stochastic	O
a	O
reward	B
signal	I
defines	O
the	O
goal	O
in	O
a	O
reinforcement	B
learning	I
problem	O
on	O
each	O
time	O
step	O
the	O
environment	B
sends	O
to	O
the	O
reinforcement	B
learning	I
agent	O
a	O
single	O
number	O
called	O
the	O
reward	O
the	O
agent	O
s	O
sole	O
objective	O
is	O
to	O
maximize	O
the	O
total	O
reward	O
it	O
receives	O
over	O
the	O
long	O
run	O
the	O
reward	B
signal	I
thus	O
defines	O
what	O
are	O
the	O
good	O
and	O
bad	O
events	O
for	O
the	O
agent	O
in	O
a	O
biological	O
system	O
we	O
might	O
think	O
of	O
rewards	O
as	O
analogous	O
to	O
the	O
experiences	O
of	O
pleasure	O
or	O
pain	O
they	O
are	O
the	O
immediate	O
and	O
defining	O
features	O
of	O
the	O
problem	O
faced	O
by	O
the	O
agent	O
the	O
reward	B
signal	I
is	O
the	O
primary	O
basis	O
for	O
altering	O
the	O
policy	B
if	O
an	O
action	B
selected	O
by	O
the	O
policy	B
is	O
followed	O
by	O
low	O
reward	O
then	O
the	O
policy	B
may	O
be	O
changed	O
to	O
select	O
some	O
other	O
action	B
in	O
that	O
situation	O
in	O
the	O
future	O
in	O
general	O
reward	O
signals	O
may	O
be	O
stochastic	O
functions	O
of	O
the	O
state	B
of	O
the	O
environment	B
and	O
the	O
actions	O
taken	O
whereas	O
the	O
reward	B
signal	I
indicates	O
what	O
is	O
good	O
in	O
an	O
immediate	O
sense	O
a	O
value	B
function	I
specifies	O
what	O
is	O
good	O
in	O
the	O
long	O
run	O
roughly	O
speaking	O
the	O
value	B
of	O
a	O
state	B
is	O
the	O
total	O
amount	O
of	O
reward	O
an	O
agent	O
can	O
expect	O
to	O
accumulate	O
over	O
the	O
future	O
starting	O
from	O
that	O
state	B
whereas	O
rewards	O
determine	O
the	O
immediate	O
intrinsic	B
desirability	O
of	O
environmental	O
states	O
values	O
indicate	O
the	O
long-term	O
desirability	O
of	O
states	O
after	O
taking	O
into	O
account	O
the	O
states	O
that	O
are	O
likely	O
to	O
follow	O
and	O
the	O
rewards	O
available	O
in	O
those	O
states	O
for	O
example	O
a	O
state	B
might	O
always	O
yield	O
a	O
low	O
immediate	O
reward	O
but	O
still	O
have	O
a	O
high	O
value	B
because	O
it	O
is	O
regularly	O
followed	O
by	O
other	O
states	O
that	O
yield	O
high	O
rewards	O
or	O
the	O
reverse	O
could	O
be	O
true	O
to	O
make	O
a	O
human	O
analogy	O
rewards	O
are	O
somewhat	O
like	O
pleasure	O
high	O
and	O
pain	O
low	O
whereas	O
values	O
correspond	O
to	O
a	O
more	O
refined	O
and	O
farsighted	O
judgment	O
of	O
how	O
pleased	O
or	O
displeased	O
we	O
are	O
that	O
our	O
environment	B
is	O
in	O
a	O
particular	O
state	B
rewards	O
are	O
in	O
a	O
sense	O
primary	O
whereas	O
values	O
as	O
predictions	O
of	O
rewards	O
are	O
secondary	O
without	O
rewards	O
there	O
could	O
be	O
no	O
values	O
and	O
the	O
only	O
purpose	O
of	O
estimating	O
values	O
is	O
to	O
achieve	O
more	O
reward	O
nevertheless	O
it	O
is	O
values	O
with	O
which	O
we	O
are	O
most	O
concerned	O
when	O
making	O
and	O
evaluating	O
decisions	O
action	B
choices	O
are	O
made	O
based	O
on	O
value	B
judgments	O
we	O
seek	O
actions	O
that	O
bring	O
about	O
states	O
of	O
highest	O
value	B
not	O
highest	O
reward	O
because	O
these	O
actions	O
obtain	O
the	O
greatest	O
amount	O
of	O
reward	O
for	O
us	O
over	O
the	O
long	O
run	O
unfortunately	O
it	O
is	O
much	O
harder	O
to	O
determine	O
values	O
than	O
it	O
is	O
to	O
determine	O
limitations	O
and	O
scope	O
rewards	O
rewards	O
are	O
basically	O
given	O
directly	O
by	O
the	O
environment	B
but	O
values	O
must	O
be	O
estimated	O
and	O
re-estimated	O
from	O
the	O
sequences	O
of	O
observations	O
an	O
agent	O
makes	O
over	O
its	O
entire	O
lifetime	O
in	O
fact	O
the	O
most	O
important	O
component	O
of	O
almost	O
all	O
reinforcement	B
learning	I
algorithms	O
we	O
consider	O
is	O
a	O
method	O
for	O
efficiently	O
estimating	O
values	O
the	O
central	O
role	O
of	O
value	B
estimation	O
is	O
arguably	O
the	O
most	O
important	O
thing	O
that	O
has	O
been	O
learned	O
about	O
reinforcement	B
learning	I
over	O
the	O
last	O
six	O
decades	O
the	O
fourth	O
and	O
final	O
element	O
of	O
some	O
reinforcement	B
learning	I
systems	O
is	O
a	O
model	B
of	I
the	I
environment	B
this	O
is	O
something	O
that	O
mimics	O
the	O
behavior	O
of	O
the	O
environment	B
or	O
more	O
generally	O
that	O
allows	O
inferences	O
to	O
be	O
made	O
about	O
how	O
the	O
environment	B
will	O
behave	O
for	O
example	O
given	O
a	O
state	B
and	O
action	B
the	O
model	O
might	O
predict	O
the	O
resultant	O
next	O
state	B
and	O
next	O
reward	O
models	O
are	O
used	O
for	O
planning	B
by	O
which	O
we	O
mean	O
any	O
way	O
of	O
deciding	O
on	O
a	O
course	O
of	O
action	B
by	O
considering	O
possible	O
future	O
situations	O
before	O
they	O
are	O
actually	O
experienced	O
methods	O
for	O
solving	O
reinforcement	B
learning	I
problems	O
that	O
use	O
models	O
and	O
planning	B
are	O
called	O
model-based	O
methods	O
as	O
opposed	O
to	O
simpler	O
model-free	O
methods	O
that	O
are	O
explicitly	O
trial-and-error	O
learners	O
viewed	O
as	O
almost	O
the	O
opposite	O
of	O
planning	B
in	O
chapter	O
we	O
explore	O
reinforcement	B
learning	I
systems	O
that	O
simultaneously	O
learn	O
by	O
trial	O
and	O
error	O
learn	O
a	O
model	B
of	I
the	I
environment	B
and	O
use	O
the	O
model	O
for	O
planning	B
modern	O
reinforcement	B
learning	I
spans	O
the	O
spectrum	O
from	O
low-level	O
trial-and-error	O
learning	O
to	O
high-level	O
deliberative	O
planning	B
limitations	O
and	O
scope	O
reinforcement	B
learning	I
relies	O
heavily	O
on	O
the	O
concept	O
of	O
state	B
as	O
input	O
to	O
the	O
policy	B
and	O
value	B
function	I
and	O
as	O
both	O
input	O
to	O
and	O
output	O
from	O
the	O
model	O
informally	O
we	O
can	O
think	O
of	O
the	O
state	B
as	O
a	O
signal	O
conveying	O
to	O
the	O
agent	O
some	O
sense	O
of	O
how	O
the	O
environment	B
is	O
at	O
a	O
particular	O
time	O
the	O
formal	O
definition	O
of	O
state	B
as	O
we	O
use	O
it	O
here	O
is	O
given	O
by	O
the	O
framework	O
of	O
markov	O
decision	O
processes	O
presented	O
in	O
chapter	O
more	O
generally	O
however	O
we	O
encourage	O
the	O
reader	O
to	O
follow	O
the	O
informal	O
meaning	O
and	O
think	O
of	O
the	O
state	B
as	O
whatever	O
information	O
is	O
available	O
to	O
the	O
agent	O
about	O
its	O
environment	B
in	O
effect	O
we	O
assume	O
that	O
the	O
state	B
signal	O
is	O
produced	O
by	O
some	O
preprocessing	O
system	O
that	O
is	O
nominally	O
part	O
of	O
the	O
agent	O
s	O
environment	B
we	O
do	O
not	O
address	O
the	O
issues	O
of	O
constructing	O
changing	O
or	O
learning	O
the	O
state	B
signal	O
in	O
this	O
book	O
than	O
briefly	O
in	O
section	O
we	O
take	O
this	O
approach	O
not	O
because	O
we	O
consider	O
state	B
representation	O
to	O
be	O
unimportant	O
but	O
in	O
order	O
to	O
focus	O
fully	O
on	O
the	O
decision-making	O
issues	O
in	O
other	O
words	O
our	O
main	O
concern	O
is	O
not	O
with	O
designing	O
the	O
state	B
signal	O
but	O
with	O
deciding	O
what	O
action	B
to	O
take	O
as	O
a	O
function	O
of	O
whatever	O
state	B
signal	O
is	O
available	O
most	O
of	O
the	O
reinforcement	B
learning	I
methods	O
we	O
consider	O
in	O
this	O
book	O
are	O
structured	O
around	O
estimating	O
value	B
functions	O
but	O
it	O
is	O
not	O
strictly	O
necessary	O
to	O
do	O
this	O
to	O
solve	O
reinforcement	B
learning	I
problems	O
for	O
example	O
solution	O
methods	O
such	O
as	O
genetic	B
algorithms	I
genetic	O
programming	O
simulated	O
annealing	O
and	O
other	O
optimization	O
methods	O
never	O
estimate	O
value	B
functions	O
these	O
methods	O
apply	O
multiple	O
static	O
policies	O
each	O
interacting	O
over	O
an	O
extended	O
period	O
of	O
time	O
with	O
a	O
separate	O
instance	O
of	O
the	O
environment	B
the	O
policies	O
that	O
obtain	O
the	O
most	O
reward	O
and	O
random	O
variations	O
of	O
them	O
are	O
carried	O
over	O
to	O
the	O
next	O
generation	O
of	O
policies	O
and	O
the	O
process	O
repeats	O
we	O
call	O
these	O
evolutionary	B
methods	I
chapter	O
introduction	O
because	O
their	O
operation	O
is	O
analogous	O
to	O
the	O
way	O
biological	O
evolution	B
produces	O
organisms	O
with	O
skilled	O
behavior	O
even	O
if	O
they	O
do	O
not	O
learn	O
during	O
their	O
individual	O
lifetimes	O
if	O
the	O
space	O
of	O
policies	O
is	O
sufficiently	O
small	O
or	O
can	O
be	O
structured	O
so	O
that	O
good	O
policies	O
are	O
common	O
or	O
easy	O
to	O
find	O
or	O
if	O
a	O
lot	O
of	O
time	O
is	O
available	O
for	O
the	O
search	O
then	O
evolutionary	B
methods	I
can	O
be	O
effective	O
in	O
addition	O
evolutionary	B
methods	I
have	O
advantages	O
on	O
problems	O
in	O
which	O
the	O
learning	O
agent	O
cannot	O
sense	O
the	O
complete	O
state	B
of	O
its	O
environment	B
our	O
focus	O
is	O
on	O
reinforcement	B
learning	I
methods	O
that	O
learn	O
while	O
interacting	O
with	O
the	O
environment	B
which	O
evolutionary	B
methods	I
do	O
not	O
do	O
methods	O
able	O
to	O
take	O
advantage	O
of	O
the	O
details	O
of	O
individual	O
behavioral	O
interactions	O
can	O
be	O
much	O
more	O
efficient	O
than	O
evolutionary	B
methods	I
in	O
many	O
cases	O
evolutionary	B
methods	I
ignore	O
much	O
of	O
the	O
useful	O
structure	O
of	O
the	O
reinforcement	B
learning	I
problem	O
they	O
do	O
not	O
use	O
the	O
fact	O
that	O
the	O
policy	B
they	O
are	O
searching	O
for	O
is	O
a	O
function	O
from	O
states	O
to	O
actions	O
they	O
do	O
not	O
notice	O
which	O
states	O
an	O
individual	O
passes	O
through	O
during	O
its	O
lifetime	O
or	O
which	O
actions	O
it	O
selects	O
in	O
some	O
cases	O
this	O
information	O
can	O
be	O
misleading	O
when	O
states	O
are	O
misperceived	O
but	O
more	O
often	O
it	O
should	O
enable	O
more	O
efficient	O
search	O
although	O
evolution	B
and	O
learning	O
share	O
many	O
features	O
and	O
naturally	O
work	O
together	O
we	O
do	O
not	O
consider	O
evolutionary	B
methods	I
by	O
themselves	O
to	O
be	O
especially	O
well	O
suited	O
to	O
reinforcement	B
learning	I
problems	O
and	O
accordingly	O
we	O
do	O
not	O
cover	O
them	O
in	O
this	O
book	O
an	O
extended	O
example	O
tic-tac-toe	B
to	O
illustrate	O
the	O
general	O
idea	O
of	O
reinforcement	B
learning	I
and	O
contrast	O
it	O
with	O
other	O
approaches	O
we	O
next	O
consider	O
a	O
single	O
example	O
in	O
more	O
detail	O
consider	O
the	O
familiar	O
child	O
s	O
game	O
of	O
tic-tac-toe	B
two	O
players	O
take	O
turns	O
playing	O
on	O
a	O
three-by-three	O
board	O
one	O
player	O
plays	O
xs	O
and	O
the	O
other	O
os	O
until	O
one	O
player	O
wins	O
by	O
placing	O
three	O
marks	O
in	O
a	O
row	O
horizontally	O
vertically	O
or	O
diagonally	O
as	O
the	O
x	O
player	O
has	O
in	O
the	O
game	O
shown	O
to	O
the	O
right	O
if	O
the	O
board	O
fills	O
up	O
with	O
neither	O
player	O
getting	O
three	O
in	O
a	O
row	O
the	O
game	O
is	O
a	O
draw	O
because	O
a	O
skilled	O
player	O
can	O
play	O
so	O
as	O
never	O
to	O
lose	O
let	O
us	O
assume	O
that	O
we	O
are	O
playing	O
against	O
an	O
imperfect	O
player	O
one	O
whose	O
play	O
is	O
sometimes	O
incorrect	O
and	O
allows	O
us	O
to	O
win	O
for	O
the	O
moment	O
in	O
fact	O
let	O
us	O
consider	O
draws	O
and	O
losses	O
to	O
be	O
equally	O
bad	O
for	O
us	O
how	O
might	O
we	O
construct	O
a	O
player	O
that	O
will	O
find	O
the	O
imperfections	O
in	O
its	O
opponent	O
s	O
play	O
and	O
learn	O
to	O
maximize	O
its	O
chances	O
of	O
winning	O
although	O
this	O
is	O
a	O
simple	O
problem	O
it	O
cannot	O
readily	O
be	O
solved	O
in	O
a	O
satisfactory	O
way	O
through	O
classical	O
techniques	O
for	O
example	O
the	O
classical	O
minimax	O
solution	O
from	O
game	B
theory	I
is	O
not	O
correct	O
here	O
because	O
it	O
assumes	O
a	O
particular	O
way	O
of	O
playing	O
by	O
the	O
opponent	O
for	O
example	O
a	O
minimax	O
player	O
would	O
never	O
reach	O
a	O
game	O
state	B
from	O
which	O
it	O
could	O
lose	O
even	O
if	O
in	O
fact	O
it	O
always	O
won	O
from	O
that	O
state	B
because	O
of	O
incorrect	O
play	O
by	O
the	O
opponent	O
classical	O
optimization	O
methods	O
for	O
sequential	O
decision	O
problems	O
such	O
as	O
dynamic	B
programming	I
can	O
compute	O
an	O
optimal	O
solution	O
for	O
any	O
opponent	O
but	O
require	O
as	O
input	O
a	O
complete	O
specification	O
of	O
that	O
opponent	O
including	O
the	O
probabilities	O
with	O
which	O
the	O
opponent	O
makes	O
each	O
move	O
in	O
each	O
board	O
state	B
let	O
us	O
assume	O
that	O
this	O
information	O
xxxooxo	O
an	O
extended	O
example	O
tic-tac-toe	B
is	O
not	O
available	O
a	O
priori	O
for	O
this	O
problem	O
as	O
it	O
is	O
not	O
for	O
the	O
vast	O
majority	O
of	O
problems	O
of	O
practical	O
interest	O
on	O
the	O
other	O
hand	O
such	O
information	O
can	O
be	O
estimated	O
from	O
experience	O
in	O
this	O
case	O
by	O
playing	O
many	O
games	O
against	O
the	O
opponent	O
about	O
the	O
best	O
one	O
can	O
do	O
on	O
this	O
problem	O
is	O
first	O
to	O
learn	O
a	O
model	O
of	O
the	O
opponent	O
s	O
behavior	O
up	O
to	O
some	O
level	O
of	O
confidence	O
and	O
then	O
apply	O
dynamic	B
programming	I
to	O
compute	O
an	O
optimal	O
solution	O
given	O
the	O
approximate	O
opponent	O
model	O
in	O
the	O
end	O
this	O
is	O
not	O
that	O
different	O
from	O
some	O
of	O
the	O
reinforcement	B
learning	I
methods	O
we	O
examine	O
later	O
in	O
this	O
book	O
an	O
evolutionary	O
method	O
applied	O
to	O
this	O
problem	O
would	O
directly	O
search	O
the	O
space	O
of	O
possible	O
policies	O
for	O
one	O
with	O
a	O
high	O
probability	O
of	O
winning	O
against	O
the	O
opponent	O
here	O
a	O
policy	B
is	O
a	O
rule	O
that	O
tells	O
the	O
player	O
what	O
move	O
to	O
make	O
for	O
every	O
state	B
of	O
the	O
game	O
every	O
possible	O
configuration	O
of	O
xs	O
and	O
os	O
on	O
the	O
three-by-three	O
board	O
for	O
each	O
policy	B
considered	O
an	O
estimate	O
of	O
its	O
winning	O
probability	O
would	O
be	O
obtained	O
by	O
playing	O
some	O
number	O
of	O
games	O
against	O
the	O
opponent	O
this	O
evaluation	O
would	O
then	O
direct	O
which	O
policy	B
or	O
policies	O
were	O
considered	O
next	O
a	O
typical	O
evolutionary	O
method	O
would	O
hill-climb	O
in	O
policy	B
space	O
successively	O
generating	O
and	O
evaluating	O
policies	O
in	O
an	O
attempt	O
to	O
obtain	O
incremental	O
improvements	O
or	O
perhaps	O
a	O
genetic-style	O
algorithm	O
could	O
be	O
used	O
that	O
would	O
maintain	O
and	O
evaluate	O
a	O
population	O
of	O
policies	O
literally	O
hundreds	O
of	O
different	O
optimization	O
methods	O
could	O
be	O
applied	O
here	O
is	O
how	O
the	O
tic-tac-toe	B
problem	O
would	O
be	O
approached	O
with	O
a	O
method	O
making	O
use	O
of	O
a	O
value	B
function	I
first	O
we	O
set	O
up	O
a	O
table	O
of	O
numbers	O
one	O
for	O
each	O
possible	O
state	B
of	O
the	O
game	O
each	O
number	O
will	O
be	O
the	O
latest	O
estimate	O
of	O
the	O
probability	O
of	O
our	O
winning	O
from	O
that	O
state	B
we	O
treat	O
this	O
estimate	O
as	O
the	O
state	B
s	O
value	B
and	O
the	O
whole	O
table	O
is	O
the	O
learned	O
value	B
function	I
state	B
a	O
has	O
higher	O
value	B
than	O
state	B
b	O
or	O
is	O
considered	O
better	O
than	O
state	B
b	O
if	O
the	O
current	O
estimate	O
of	O
the	O
probability	O
of	O
our	O
winning	O
from	O
a	O
is	O
higher	O
than	O
it	O
is	O
from	O
b	O
assuming	O
we	O
always	O
play	O
xs	O
then	O
for	O
all	O
states	O
with	O
three	O
xs	O
in	O
a	O
row	O
the	O
probability	O
of	O
winning	O
is	O
because	O
we	O
have	O
already	O
won	O
similarly	O
for	O
all	O
states	O
with	O
three	O
os	O
in	O
a	O
row	O
or	O
that	O
are	O
filled	O
up	O
the	O
correct	O
probability	O
is	O
as	O
we	O
cannot	O
win	O
from	O
them	O
we	O
set	O
the	O
initial	O
values	O
of	O
all	O
the	O
other	O
states	O
to	O
representing	O
a	O
guess	O
that	O
we	O
have	O
a	O
chance	O
of	O
winning	O
we	O
play	O
many	O
games	O
against	O
the	O
opponent	O
to	O
select	O
our	O
moves	O
we	O
examine	O
the	O
states	O
that	O
would	O
result	O
from	O
each	O
of	O
our	O
possible	O
moves	O
for	O
each	O
blank	O
space	O
on	O
the	O
board	O
and	O
look	O
up	O
their	O
current	O
values	O
in	O
the	O
table	O
most	O
of	O
the	O
time	O
we	O
move	O
greedily	O
selecting	O
the	O
move	O
that	O
leads	O
to	O
the	O
state	B
with	O
greatest	O
value	B
that	O
is	O
with	O
the	O
highest	O
estimated	O
probability	O
of	O
winning	O
occasionally	O
however	O
we	O
select	O
randomly	O
from	O
among	O
the	O
other	O
moves	O
instead	O
these	O
are	O
called	O
exploratory	O
moves	O
because	O
they	O
cause	O
us	O
to	O
experience	O
states	O
that	O
we	O
might	O
otherwise	O
never	O
see	O
a	O
sequence	O
of	O
moves	O
made	O
and	O
considered	O
during	O
a	O
game	O
can	O
be	O
diagrammed	O
as	O
in	O
figure	O
while	O
we	O
are	O
playing	O
we	O
change	O
the	O
values	O
of	O
the	O
states	O
in	O
which	O
we	O
find	O
ourselves	O
during	O
the	O
game	O
we	O
attempt	O
to	O
make	O
them	O
more	O
accurate	O
estimates	O
of	O
the	O
probabilities	O
of	O
winning	O
to	O
do	O
this	O
we	O
back	O
up	O
the	O
value	B
of	O
the	O
state	B
after	O
each	O
greedy	O
move	O
to	O
the	O
state	B
before	O
the	O
move	O
as	O
suggested	O
by	O
the	O
arrows	O
in	O
figure	O
more	O
precisely	O
the	O
current	O
value	B
of	O
the	O
earlier	O
state	B
is	O
updated	O
to	O
be	O
closer	O
to	O
the	O
value	B
of	O
the	O
later	O
state	B
this	O
can	O
be	O
done	O
by	O
moving	O
the	O
earlier	O
state	B
s	O
value	B
a	O
fraction	O
of	O
the	O
way	O
toward	O
the	O
value	B
of	O
the	O
later	O
state	B
if	O
we	O
let	O
s	O
denote	O
the	O
state	B
before	O
the	O
greedy	O
move	O
and	O
the	O
chapter	O
introduction	O
figure	O
a	O
sequence	O
of	O
tic-tac-toe	B
moves	O
the	O
solid	O
lines	O
represent	O
the	O
moves	O
taken	O
during	O
a	O
game	O
the	O
dashed	O
lines	O
represent	O
moves	O
that	O
we	O
reinforcement	B
learning	I
player	O
considered	O
but	O
did	O
not	O
make	O
our	O
second	O
move	O
was	O
an	O
exploratory	O
move	O
meaning	O
that	O
it	O
was	O
taken	O
even	O
though	O
another	O
sibling	O
move	O
the	O
one	O
leading	O
to	O
e	O
was	O
ranked	O
higher	O
exploratory	O
moves	O
do	O
not	O
result	O
in	O
any	O
learning	O
but	O
each	O
of	O
our	O
other	O
moves	O
does	O
causing	O
updates	O
as	O
suggested	O
by	O
the	O
red	O
arrows	O
in	O
which	O
estimated	O
values	O
are	O
moved	O
up	O
the	O
tree	O
from	O
later	O
nodes	O
to	O
earlier	O
as	O
detailed	O
in	O
the	O
text	O
state	B
after	O
the	O
move	O
then	O
the	O
update	O
to	O
the	O
estimated	O
value	B
of	O
s	O
denoted	O
v	O
can	O
be	O
written	O
as	O
v	O
v	O
v	O
where	O
is	O
a	O
small	O
positive	O
fraction	O
called	O
the	O
step-size	B
parameter	I
which	O
influences	O
the	O
rate	O
of	O
learning	O
this	O
update	O
rule	O
is	O
an	O
example	O
of	O
a	O
temporal-difference	B
learning	I
method	O
so	O
called	O
because	O
its	O
changes	O
are	O
based	O
on	O
a	O
difference	O
v	O
v	O
between	O
estimates	O
at	O
two	O
different	O
times	O
the	O
method	O
described	O
above	O
performs	O
quite	O
well	O
on	O
this	O
task	O
for	O
example	O
if	O
the	O
step-size	B
parameter	I
is	O
reduced	O
properly	O
over	O
time	O
then	O
this	O
method	O
converges	O
for	O
any	O
fixed	O
opponent	O
to	O
the	O
true	O
probabilities	O
of	O
winning	O
from	O
each	O
state	B
given	O
optimal	O
play	O
by	O
our	O
player	O
furthermore	O
the	O
moves	O
then	O
taken	O
on	O
exploratory	O
moves	O
are	O
in	O
fact	O
the	O
optimal	O
moves	O
against	O
this	O
opponent	O
in	O
other	O
words	O
the	O
method	O
converges	O
to	O
an	O
optimal	O
policy	B
for	O
playing	O
the	O
game	O
against	O
this	O
opponent	O
if	O
the	O
stepsize	O
parameter	O
is	O
not	O
reduced	O
all	O
the	O
way	O
to	O
zero	O
over	O
time	O
then	O
this	O
player	O
also	O
plays	O
well	O
against	O
opponents	O
that	O
slowly	O
change	O
their	O
way	O
of	O
playing	O
our	O
moveopponents	O
moveour	O
movestarting	O
position	O
abcdeeopponents	O
movec	O
f	O
ggopponents	O
moveour	O
move	O
agcstarting	O
positionbcdeefg	O
an	O
extended	O
example	O
tic-tac-toe	B
this	O
example	O
illustrates	O
the	O
differences	O
between	O
evolutionary	B
methods	I
and	O
methods	O
that	O
learn	O
value	B
functions	O
to	O
evaluate	O
a	O
policy	B
an	O
evolutionary	O
method	O
holds	O
the	O
policy	B
fixed	O
and	O
plays	O
many	O
games	O
against	O
the	O
opponent	O
or	O
simulates	O
many	O
games	O
using	O
a	O
model	O
of	O
the	O
opponent	O
the	O
frequency	O
of	O
wins	O
gives	O
an	O
unbiased	O
estimate	O
of	O
the	O
probability	O
of	O
winning	O
with	O
that	O
policy	B
and	O
can	O
be	O
used	O
to	O
direct	O
the	O
next	O
policy	B
selection	O
but	O
each	O
policy	B
change	O
is	O
made	O
only	O
after	O
many	O
games	O
and	O
only	O
the	O
final	O
outcome	O
of	O
each	O
game	O
is	O
used	O
what	O
happens	O
during	O
the	O
games	O
is	O
ignored	O
for	O
example	O
if	O
the	O
player	O
wins	O
then	O
all	O
of	O
its	O
behavior	O
in	O
the	O
game	O
is	O
given	O
credit	O
independently	O
of	O
how	O
specific	O
moves	O
might	O
have	O
been	O
critical	O
to	O
the	O
win	O
credit	O
is	O
even	O
given	O
to	O
moves	O
that	O
never	O
occurred	O
value	B
function	I
methods	O
in	O
contrast	O
allow	O
individual	O
states	O
to	O
be	O
evaluated	O
in	O
the	O
end	O
evolutionary	O
and	O
value	B
function	I
methods	O
both	O
search	O
the	O
space	O
of	O
policies	O
but	O
learning	O
a	O
value	B
function	I
takes	O
advantage	O
of	O
information	O
available	O
during	O
the	O
course	O
of	O
play	O
this	O
simple	O
example	O
illustrates	O
some	O
of	O
the	O
key	O
features	O
of	O
reinforcement	B
learning	I
methods	O
first	O
there	O
is	O
the	O
emphasis	O
on	O
learning	O
while	O
interacting	O
with	O
an	O
environment	B
in	O
this	O
case	O
with	O
an	O
opponent	O
player	O
second	O
there	O
is	O
a	O
clear	O
goal	O
and	O
correct	O
behavior	O
requires	O
planning	B
or	O
foresight	O
that	O
takes	O
into	O
account	O
delayed	O
effects	O
of	O
one	O
s	O
choices	O
for	O
example	O
the	O
simple	O
reinforcement	B
learning	I
player	O
would	O
learn	O
to	O
set	O
up	O
multi-move	O
traps	O
for	O
a	O
shortsighted	O
opponent	O
it	O
is	O
a	O
striking	O
feature	O
of	O
the	O
reinforcement	B
learning	I
solution	O
that	O
it	O
can	O
achieve	O
the	O
effects	O
of	O
planning	B
and	O
lookahead	O
without	O
using	O
a	O
model	O
of	O
the	O
opponent	O
and	O
without	O
conducting	O
an	O
explicit	O
search	O
over	O
possible	O
sequences	O
of	O
future	O
states	O
and	O
actions	O
while	O
this	O
example	O
illustrates	O
some	O
of	O
the	O
key	O
features	O
of	O
reinforcement	B
learning	I
it	O
is	O
so	O
simple	O
that	O
it	O
might	O
give	O
the	O
impression	O
that	O
reinforcement	B
learning	I
is	O
more	O
limited	O
than	O
it	O
really	O
is	O
although	O
tic-tac-toe	B
is	O
a	O
two-person	O
game	O
reinforcement	B
learning	I
also	O
applies	O
in	O
the	O
case	O
in	O
which	O
there	O
is	O
no	O
external	O
adversary	O
that	O
is	O
in	O
the	O
case	O
of	O
a	O
game	O
against	O
nature	O
reinforcement	B
learning	I
also	O
is	O
not	O
restricted	O
to	O
problems	O
in	O
which	O
behavior	O
breaks	O
down	O
into	O
separate	O
episodes	B
like	O
the	O
separate	O
games	O
of	O
tic-tactoe	O
with	O
reward	O
only	O
at	O
the	O
end	O
of	O
each	O
episode	O
it	O
is	O
just	O
as	O
applicable	O
when	O
behavior	O
continues	O
indefinitely	O
and	O
when	O
rewards	O
of	O
various	O
magnitudes	O
can	O
be	O
received	O
at	O
any	O
time	O
reinforcement	B
learning	I
is	O
also	O
applicable	O
to	O
problems	O
that	O
do	O
not	O
even	O
break	O
down	O
into	O
discrete	O
time	O
steps	O
like	O
the	O
plays	O
of	O
tic-tac-toe	B
the	O
general	O
principles	O
apply	O
to	O
continuous-time	O
problems	O
as	O
well	O
although	O
the	O
theory	O
gets	O
more	O
complicated	O
and	O
we	O
omit	O
it	O
from	O
this	O
introductory	O
treatment	O
tic-tac-toe	B
has	O
a	O
relatively	O
small	O
finite	O
state	B
set	O
whereas	O
reinforcement	B
learning	I
can	O
be	O
used	O
when	O
the	O
state	B
set	O
is	O
very	O
large	O
or	O
even	O
infinite	O
for	O
example	O
gerry	O
tesauro	O
combined	O
the	O
algorithm	O
described	O
above	O
with	O
an	O
artificial	O
neural	B
network	O
to	O
learn	O
to	O
play	O
backgammon	B
which	O
has	O
approximately	O
states	O
with	O
this	O
many	O
states	O
it	O
is	O
impossible	O
ever	O
to	O
experience	O
more	O
than	O
a	O
small	O
fraction	O
of	O
them	O
tesauro	O
s	O
program	O
learned	O
to	O
play	O
far	O
better	O
than	O
any	O
previous	O
program	O
and	O
now	O
plays	O
at	O
the	O
level	O
of	O
the	O
world	O
s	O
best	O
human	O
players	O
chapter	O
the	O
neural	B
network	O
provides	O
the	O
program	O
with	O
the	O
ability	O
to	O
generalize	O
from	O
its	O
experience	O
so	O
that	O
in	O
new	O
states	O
it	O
selects	O
moves	O
based	O
on	O
information	O
saved	O
from	O
similar	O
states	O
faced	O
in	O
the	O
past	O
as	O
determined	O
by	O
its	O
network	O
how	O
well	O
a	O
reinforcement	B
learning	I
system	O
can	O
work	O
in	O
problems	O
with	O
such	O
large	O
chapter	O
introduction	O
state	B
sets	O
is	O
intimately	O
tied	O
to	O
how	O
appropriately	O
it	O
can	O
generalize	O
from	O
past	O
experience	O
it	O
is	O
in	O
this	O
role	O
that	O
we	O
have	O
the	O
greatest	O
need	O
for	O
supervised	B
learning	I
methods	O
with	O
reinforcement	B
learning	I
neural	B
networks	O
and	O
deep	B
learning	I
are	O
not	O
the	O
only	O
or	O
necessarily	O
the	O
best	O
way	O
to	O
do	O
this	O
in	O
this	O
tic-tac-toe	B
example	O
learning	O
started	O
with	O
no	O
prior	B
knowledge	I
beyond	O
the	O
rules	O
of	O
the	O
game	O
but	O
reinforcement	B
learning	I
by	O
no	O
means	O
entails	O
a	O
tabula	O
rasa	O
view	O
of	O
learning	O
and	O
intelligence	O
on	O
the	O
contrary	O
prior	O
information	O
can	O
be	O
incorporated	O
into	O
reinforcement	B
learning	I
in	O
a	O
variety	O
of	O
ways	O
that	O
can	O
be	O
critical	O
for	O
efficient	O
learning	O
we	O
also	O
had	O
access	O
to	O
the	O
true	O
state	B
in	O
the	O
tic-tac-toe	B
example	O
whereas	O
reinforcement	B
learning	I
can	O
also	O
be	O
applied	O
when	O
part	O
of	O
the	O
state	B
is	O
hidden	O
or	O
when	O
different	O
states	O
appear	O
to	O
the	O
learner	O
to	O
be	O
the	O
same	O
finally	O
the	O
tic-tac-toe	B
player	O
was	O
able	O
to	O
look	O
ahead	O
and	O
know	O
the	O
states	O
that	O
would	O
result	O
from	O
each	O
of	O
its	O
possible	O
moves	O
to	O
do	O
this	O
it	O
had	O
to	O
have	O
a	O
model	O
of	O
the	O
game	O
that	O
allowed	O
it	O
to	O
foresee	O
how	O
its	O
environment	B
would	O
change	O
in	O
response	O
to	O
moves	O
that	O
it	O
might	O
never	O
make	O
many	O
problems	O
are	O
like	O
this	O
but	O
in	O
others	O
even	O
a	O
short-term	O
model	O
of	O
the	O
effects	O
of	O
actions	O
is	O
lacking	O
reinforcement	B
learning	I
can	O
be	O
applied	O
in	O
either	O
case	O
no	O
model	O
is	O
required	O
but	O
models	O
can	O
easily	O
be	O
used	O
if	O
they	O
are	O
available	O
or	O
can	O
be	O
learned	O
on	O
the	O
other	O
hand	O
there	O
are	O
reinforcement	B
learning	I
methods	O
that	O
do	O
not	O
need	O
any	O
kind	O
of	O
environment	B
model	O
at	O
all	O
model-free	O
systems	O
cannot	O
even	O
think	O
about	O
how	O
their	O
environments	O
will	O
change	O
in	O
response	O
to	O
a	O
single	O
action	B
the	O
tic-tac-toe	B
player	O
is	O
modelfree	O
in	O
this	O
sense	O
with	O
respect	O
to	O
its	O
opponent	O
it	O
has	O
no	O
model	O
of	O
its	O
opponent	O
of	O
any	O
kind	O
because	O
models	O
have	O
to	O
be	O
reasonably	O
accurate	O
to	O
be	O
useful	O
model-free	O
methods	O
can	O
have	O
advantages	O
over	O
more	O
complex	O
methods	O
when	O
the	O
real	O
bottleneck	O
in	O
solving	O
a	O
problem	O
is	O
the	O
difficulty	O
of	O
constructing	O
a	O
sufficiently	O
accurate	O
environment	B
model	O
model-free	O
methods	O
are	O
also	O
important	O
building	O
blocks	O
for	O
model-based	O
methods	O
in	O
this	O
book	O
we	O
devote	O
several	O
chapters	O
to	O
model-free	O
methods	O
before	O
we	O
discuss	O
how	O
they	O
can	O
be	O
used	O
as	O
components	O
of	O
more	O
complex	O
model-based	O
methods	O
reinforcement	B
learning	I
can	O
be	O
used	O
at	O
both	O
high	O
and	O
low	O
levels	O
in	O
a	O
system	O
although	O
the	O
tic-tac-toe	B
player	O
learned	O
only	O
about	O
the	O
basic	O
moves	O
of	O
the	O
game	O
nothing	O
prevents	O
reinforcement	B
learning	I
from	O
working	O
at	O
higher	O
levels	O
where	O
each	O
of	O
the	O
actions	O
may	O
itself	O
be	O
the	O
application	O
of	O
a	O
possibly	O
elaborate	O
problem-solving	O
method	O
in	O
hierarchical	O
learning	O
systems	O
reinforcement	B
learning	I
can	O
work	O
simultaneously	O
on	O
several	O
levels	O
exercise	O
self-play	O
suppose	O
instead	O
of	O
playing	O
against	O
a	O
random	O
opponent	O
the	O
reinforcement	B
learning	I
algorithm	O
described	O
above	O
played	O
against	O
itself	O
with	O
both	O
sides	O
learning	O
what	O
do	O
you	O
think	O
would	O
happen	O
in	O
this	O
case	O
would	O
it	O
learn	O
a	O
different	O
policy	B
for	O
selecting	O
moves	O
exercise	O
symmetries	O
many	O
tic-tac-toe	B
positions	O
appear	O
different	O
but	O
are	O
really	O
the	O
same	O
because	O
of	O
symmetries	O
how	O
might	O
we	O
amend	O
the	O
learning	O
process	O
described	O
above	O
to	O
take	O
advantage	O
of	O
this	O
in	O
what	O
ways	O
would	O
this	O
change	O
improve	O
the	O
learning	O
process	O
now	O
think	O
again	O
suppose	O
the	O
opponent	O
did	O
not	O
take	O
advantage	O
of	O
symmetries	O
in	O
that	O
case	O
should	O
we	O
is	O
it	O
true	O
then	O
that	O
symmetrically	O
equivalent	O
positions	O
should	O
necessarily	O
have	O
the	O
same	O
value	B
exercise	O
greedy	O
play	O
suppose	O
the	O
reinforcement	B
learning	I
player	O
was	O
greedy	O
that	O
early	O
history	B
of	I
reinforcement	B
learning	I
is	O
it	O
always	O
played	O
the	O
move	O
that	O
brought	O
it	O
to	O
the	O
position	O
that	O
it	O
rated	O
the	O
best	O
might	O
it	O
learn	O
to	O
play	O
better	O
or	O
worse	O
than	O
a	O
nongreedy	O
player	O
what	O
problems	O
might	O
occur	O
exercise	O
learning	O
from	O
exploration	O
suppose	O
learning	O
updates	O
occurred	O
after	O
all	O
moves	O
including	O
exploratory	O
moves	O
if	O
the	O
step-size	B
parameter	I
is	O
appropriately	O
reduced	O
over	O
time	O
not	O
the	O
tendency	O
to	O
explore	O
then	O
the	O
state	B
values	O
would	O
converge	O
to	O
a	O
set	O
of	O
probabilities	O
what	O
are	O
the	O
two	O
sets	O
of	O
probabilities	O
computed	O
when	O
we	O
do	O
and	O
when	O
we	O
do	O
not	O
learn	O
from	O
exploratory	O
moves	O
assuming	O
that	O
we	O
do	O
continue	O
to	O
make	O
exploratory	O
moves	O
which	O
set	O
of	O
probabilities	O
might	O
be	O
better	O
to	O
learn	O
which	O
would	O
result	O
in	O
more	O
wins	O
exercise	O
other	O
improvements	O
can	O
you	O
think	O
of	O
other	O
ways	O
to	O
improve	O
the	O
reinforcement	B
learning	I
player	O
can	O
you	O
think	O
of	O
any	O
better	O
way	O
to	O
solve	O
the	O
tic-tac-toe	B
problem	O
as	O
posed	O
summary	O
reinforcement	B
learning	I
is	O
a	O
computational	O
approach	O
to	O
understanding	O
and	O
automating	O
goal-directed	O
learning	O
and	O
decision	O
making	O
it	O
is	O
distinguished	O
from	O
other	O
computational	O
approaches	O
by	O
its	O
emphasis	O
on	O
learning	O
by	O
an	O
agent	O
from	O
direct	O
interaction	O
with	O
its	O
environment	B
without	O
relying	O
on	O
exemplary	O
supervision	O
or	O
complete	O
models	O
of	O
the	O
environment	B
in	O
our	O
opinion	O
reinforcement	B
learning	I
is	O
the	O
first	O
field	O
to	O
seriously	O
address	O
the	O
computational	O
issues	O
that	O
arise	O
when	O
learning	O
from	O
interaction	O
with	O
an	O
environment	B
in	O
order	O
to	O
achieve	O
long-term	O
goals	O
reinforcement	B
learning	I
uses	O
the	O
formal	O
framework	O
of	O
markov	O
decision	O
processes	O
to	O
define	O
the	O
interaction	O
between	O
a	O
learning	O
agent	O
and	O
its	O
environment	B
in	O
terms	O
of	O
states	O
actions	O
and	O
rewards	O
this	O
framework	O
is	O
intended	O
to	O
be	O
a	O
simple	O
way	O
of	O
representing	O
essential	O
features	O
of	O
the	O
artificial	B
intelligence	I
problem	O
these	O
features	O
include	O
a	O
sense	O
of	O
cause	O
and	O
effect	O
a	O
sense	O
of	O
uncertainty	O
and	O
nondeterminism	O
and	O
the	O
existence	O
of	O
explicit	O
goals	O
the	O
concepts	O
of	O
value	B
and	O
value	B
function	I
are	O
key	O
to	O
most	O
of	O
the	O
reinforcement	B
learning	I
methods	O
that	O
we	O
consider	O
in	O
this	O
book	O
we	O
take	O
the	O
position	O
that	O
value	B
functions	O
are	O
important	O
for	O
efficient	O
search	O
in	O
the	O
space	O
of	O
policies	O
the	O
use	O
of	O
value	B
functions	O
distinguishes	O
reinforcement	B
learning	I
methods	O
from	O
evolutionary	B
methods	I
that	O
search	O
directly	O
in	O
policy	B
space	O
guided	O
by	O
scalar	O
evaluations	O
of	O
entire	O
policies	O
early	O
history	B
of	I
reinforcement	B
learning	I
the	O
early	O
history	B
of	I
reinforcement	B
learning	I
has	O
two	O
main	O
threads	O
both	O
long	O
and	O
rich	O
that	O
were	O
pursued	O
independently	O
before	O
intertwining	O
in	O
modern	O
reinforcement	B
learning	I
one	O
thread	O
concerns	O
learning	O
by	O
trial	O
and	O
error	O
that	O
started	O
in	O
the	O
psychology	B
of	O
animal	O
learning	O
this	O
thread	O
runs	O
through	O
some	O
of	O
the	O
earliest	O
work	O
in	O
artificial	B
intelligence	I
and	O
led	O
to	O
the	O
revival	O
of	O
reinforcement	B
learning	I
in	O
the	O
early	O
the	O
other	O
thread	O
chapter	O
introduction	O
concerns	O
the	O
problem	O
of	O
optimal	B
control	B
and	O
its	O
solution	O
using	O
value	B
functions	O
and	B
dynamic	B
programming	I
for	O
the	O
most	O
part	O
this	O
thread	O
did	O
not	O
involve	O
learning	O
although	O
the	O
two	O
threads	O
have	O
been	O
largely	O
independent	O
the	O
exceptions	O
revolve	O
around	O
a	O
third	O
less	O
distinct	O
thread	O
concerning	O
temporal-difference	O
methods	O
such	O
as	O
the	O
one	O
used	O
in	O
the	O
tic-tac-toe	B
example	O
in	O
this	O
chapter	O
all	O
three	O
threads	O
came	O
together	O
in	O
the	O
late	O
to	O
produce	O
the	O
modern	O
field	O
of	O
reinforcement	B
learning	I
as	O
we	O
present	O
it	O
in	O
this	O
book	O
the	O
thread	O
focusing	O
on	O
trial-and-error	O
learning	O
is	O
the	O
one	O
with	O
which	O
we	O
are	O
most	O
familiar	O
and	O
about	O
which	O
we	O
have	O
the	O
most	O
to	O
say	O
in	O
this	O
brief	O
history	O
before	O
doing	O
that	O
however	O
we	O
briefly	O
discuss	O
the	O
optimal	B
control	B
thread	O
the	O
term	O
optimal	B
control	B
came	O
into	O
use	O
in	O
the	O
late	O
to	O
describe	O
the	O
problem	O
of	O
designing	O
a	O
controller	O
to	O
minimize	O
a	O
measure	O
of	O
a	O
dynamical	O
system	O
s	O
behavior	O
over	O
time	O
one	O
of	O
the	O
approaches	O
to	O
this	O
problem	O
was	O
developed	O
in	O
the	O
by	O
richard	O
bellman	B
and	O
others	O
through	O
extending	O
a	O
nineteenth	O
century	O
theory	O
of	O
hamilton	O
and	O
jacobi	O
this	O
approach	O
uses	O
the	O
concepts	O
of	O
a	O
dynamical	O
system	O
s	O
state	B
and	O
of	O
a	O
value	B
function	I
or	O
optimal	O
return	B
function	O
to	O
define	O
a	O
functional	O
equation	O
now	O
often	O
called	O
the	O
bellman	B
equation	I
the	O
class	O
of	O
methods	O
for	O
solving	O
optimal	B
control	B
problems	O
by	O
solving	O
this	O
equation	O
came	O
to	O
be	O
known	O
as	O
dynamic	B
programming	I
bellman	B
also	O
introduced	O
the	O
discrete	O
stochastic	O
version	O
of	O
the	O
optimal	B
control	B
problem	O
known	O
as	O
markov	O
decision	O
processes	O
and	O
ronald	O
howard	O
devised	O
the	O
policy	B
iteration	I
method	O
for	O
mdps	O
all	O
of	O
these	O
are	O
essential	O
elements	O
underlying	O
the	O
theory	O
and	O
algorithms	O
of	O
modern	O
reinforcement	B
learning	I
dynamic	B
programming	I
is	O
widely	O
considered	O
the	O
only	O
feasible	O
way	O
of	O
solving	O
general	O
stochastic	O
optimal	B
control	B
problems	O
it	O
suffers	O
from	O
what	O
bellman	B
called	O
the	O
curse	B
of	I
dimensionality	I
meaning	O
that	O
its	O
computational	O
requirements	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
state	B
variables	O
but	O
it	O
is	O
still	O
far	O
more	O
efficient	O
and	O
more	O
widely	O
applicable	O
than	O
any	O
other	O
general	O
method	O
dynamic	B
programming	I
has	O
been	O
extensively	O
developed	O
since	O
the	O
late	O
including	O
extensions	O
to	O
partially	B
observable	I
mdps	I
by	O
lovejoy	O
many	O
applications	O
by	O
white	O
approximation	O
methods	O
by	O
rust	O
and	O
asynchronous	O
methods	O
many	O
excellent	O
modern	O
treatments	O
of	O
dynamic	B
programming	I
are	O
available	O
bertsekas	O
puterman	O
ross	O
and	O
whittle	O
bryson	O
provides	O
an	O
authoritative	O
history	B
of	I
optimal	B
control	B
connections	O
between	O
optimal	B
control	B
and	B
dynamic	B
programming	I
on	O
the	O
one	O
hand	O
and	O
learning	O
on	O
the	O
other	O
were	O
slow	O
to	O
be	O
recognized	O
we	O
cannot	O
be	O
sure	O
about	O
what	O
accounted	O
for	O
this	O
separation	O
but	O
its	O
main	O
cause	O
was	O
likely	O
the	O
separation	O
between	O
the	O
disciplines	O
involved	O
and	O
their	O
different	O
goals	O
also	O
contributing	O
may	O
have	O
been	O
the	O
prevalent	O
view	O
of	O
dynamic	B
programming	I
as	O
an	O
off-line	B
computation	O
depending	O
essentially	O
on	O
accurate	O
system	O
models	O
and	O
analytic	O
solutions	O
to	O
the	O
bellman	B
equation	I
further	O
the	O
simplest	O
form	O
of	O
dynamic	B
programming	I
is	O
a	O
computation	O
that	O
proceeds	O
backwards	O
in	O
time	O
making	O
it	O
difficult	O
to	O
see	O
how	O
it	O
could	O
be	O
involved	O
in	O
a	O
learning	O
process	O
that	O
must	O
proceed	O
in	O
a	O
forward	O
direction	O
some	O
of	O
the	O
earliest	O
work	O
in	O
dynamic	B
programming	I
such	O
as	O
that	O
by	O
bellman	B
and	O
dreyfus	O
might	O
now	O
be	O
classified	O
as	O
following	O
a	O
learning	O
approach	O
witten	B
s	O
work	O
below	O
certainly	O
qualifies	O
as	O
a	O
combination	O
of	O
learning	O
and	O
dynamic-programming	O
ideas	O
werbos	B
argued	O
explicitly	O
early	O
history	B
of	I
reinforcement	B
learning	I
for	O
greater	O
interrelation	O
of	O
dynamic	B
programming	I
and	O
learning	O
methods	O
and	O
its	O
relevance	O
to	O
understanding	O
neural	B
and	O
cognitive	O
mechanisms	O
for	O
us	O
the	O
full	O
integration	O
of	O
dynamic	B
programming	I
methods	O
with	O
online	B
learning	O
did	O
not	O
occur	O
until	O
the	O
work	O
of	O
chris	O
watkins	B
in	O
whose	O
treatment	O
of	O
reinforcement	B
learning	I
using	O
the	O
mdp	O
formalism	O
has	O
been	O
widely	O
adopted	O
since	O
then	O
these	O
relationships	O
have	O
been	O
extensively	O
developed	O
by	O
many	O
researchers	O
most	O
particularly	O
by	O
dimitri	O
bertsekas	O
and	O
john	O
tsitsiklis	O
who	O
coined	O
the	O
term	O
neurodynamic	B
programming	I
to	O
refer	O
to	O
the	O
combination	O
of	O
dynamic	B
programming	I
and	O
neural	B
networks	O
another	O
term	O
currently	O
in	O
use	O
is	O
approximate	B
dynamic	B
programming	I
these	O
various	O
approaches	O
emphasize	O
different	O
aspects	O
of	O
the	O
subject	O
but	O
they	O
all	O
share	O
with	O
reinforcement	B
learning	I
an	O
interest	O
in	O
circumventing	O
the	O
classical	O
shortcomings	O
of	O
dynamic	B
programming	I
we	O
would	O
consider	O
all	O
of	O
the	O
work	O
in	O
optimal	B
control	B
also	O
to	O
be	O
in	O
a	O
sense	O
work	O
in	O
reinforcement	B
learning	I
we	O
define	O
a	O
reinforcement	B
learning	I
method	O
as	O
any	O
effective	O
way	O
of	O
solving	O
reinforcement	B
learning	I
problems	O
and	O
it	O
is	O
now	O
clear	O
that	O
these	O
problems	O
are	O
closely	O
related	O
to	O
optimal	B
control	B
problems	O
particularly	O
stochastic	O
optimal	B
control	B
problems	O
such	O
as	O
those	O
formulated	O
as	O
mdps	O
accordingly	O
we	O
must	O
consider	O
the	O
solution	O
methods	O
of	O
optimal	B
control	B
such	O
as	O
dynamic	B
programming	I
also	O
to	O
be	O
reinforcement	B
learning	I
methods	O
because	O
almost	O
all	O
of	O
the	O
conventional	O
methods	O
require	O
complete	O
knowledge	O
of	O
the	O
system	O
to	O
be	O
controlled	O
it	O
feels	O
a	O
little	O
unnatural	O
to	O
say	O
that	O
they	O
are	O
part	O
of	O
reinforcement	B
learning	I
on	O
the	O
other	O
hand	O
many	O
dynamic	B
programming	I
algorithms	O
are	O
incremental	O
and	O
iterative	B
like	O
learning	O
methods	O
they	O
gradually	O
reach	O
the	O
correct	O
answer	O
through	O
successive	O
approximations	O
as	O
we	O
show	O
in	O
the	O
rest	O
of	O
this	O
book	O
these	O
similarities	O
are	O
far	O
more	O
than	O
superficial	O
the	O
theories	O
and	O
solution	O
methods	O
for	O
the	O
cases	O
of	O
complete	O
and	O
incomplete	O
knowledge	O
are	O
so	O
closely	O
related	O
that	O
we	O
feel	O
they	O
must	O
be	O
considered	O
together	O
as	O
part	O
of	O
the	O
same	O
subject	O
matter	O
let	O
us	O
return	B
now	O
to	O
the	O
other	O
major	O
thread	O
leading	O
to	O
the	O
modern	O
field	O
of	O
reinforcement	B
learning	I
that	O
centered	O
on	O
the	O
idea	O
of	O
trial-and-error	O
learning	O
we	O
only	O
touch	O
on	O
the	O
major	O
points	O
of	O
contact	O
here	O
taking	O
up	O
this	O
topic	O
in	O
more	O
detail	O
in	O
section	O
according	O
to	O
american	O
psychologist	O
r	O
s	O
woodworth	O
the	O
idea	O
of	O
trial-and-error	O
learning	O
goes	O
as	O
far	O
back	O
as	O
the	O
to	O
alexander	O
bain	O
s	O
discussion	O
of	O
learning	O
by	O
groping	O
and	O
experiment	O
and	O
more	O
explicitly	O
to	O
the	O
british	O
ethologist	O
and	O
psychologist	O
conway	O
lloyd	O
morgan	O
s	O
use	O
of	O
the	O
term	O
to	O
describe	O
his	O
observations	O
of	O
animal	O
behavior	O
perhaps	O
the	O
first	O
to	O
succinctly	O
express	O
the	O
essence	O
of	O
trial-and-error	O
learning	O
as	O
a	O
principle	O
of	O
learning	O
was	O
edward	O
thorndike	O
of	O
several	O
responses	O
made	O
to	O
the	O
same	O
situation	O
those	O
which	O
are	O
accompanied	O
or	O
closely	O
followed	O
by	O
satisfaction	O
to	O
the	O
animal	O
will	O
other	O
things	O
being	O
equal	O
be	O
more	O
firmly	O
connected	O
with	O
the	O
situation	O
so	O
that	O
when	O
it	O
recurs	O
they	O
will	O
be	O
more	O
likely	O
to	O
recur	O
those	O
which	O
are	O
accompanied	O
or	O
closely	O
followed	O
by	O
discomfort	O
to	O
the	O
animal	O
will	O
other	O
things	O
being	O
equal	O
have	O
their	O
connections	O
with	O
that	O
situation	O
weakened	O
so	O
that	O
when	O
it	O
recurs	O
they	O
will	O
be	O
less	O
likely	O
to	O
occur	O
the	O
greater	O
the	O
satisfaction	O
or	O
discomfort	O
the	O
greater	O
the	O
strengthening	O
or	O
weakening	O
of	O
the	O
bond	O
p	O
thorndike	O
called	O
this	O
the	O
law	B
of	I
effect	I
because	O
it	O
describes	O
the	O
effect	O
of	O
reinforcing	O
events	O
on	O
the	O
tendency	O
to	O
select	O
actions	O
thorndike	O
later	O
modified	O
the	O
law	O
to	O
better	O
chapter	O
introduction	O
account	O
for	O
accumulating	B
data	O
on	O
animal	O
learning	O
as	O
differences	O
between	O
the	O
effects	O
of	O
reward	O
and	O
punishment	O
and	O
the	O
law	O
in	O
its	O
various	O
forms	O
has	O
generated	O
considerable	O
controversy	O
among	O
learning	O
theorists	O
see	O
gallistel	O
herrnstein	O
kimble	O
mazur	O
despite	O
this	O
the	O
law	B
of	I
effect	I
in	O
one	O
form	O
or	O
another	O
is	O
widely	O
regarded	O
as	O
a	O
basic	O
principle	O
underlying	O
much	O
behavior	O
hilgard	O
and	O
bower	O
dennett	O
campbell	O
cziko	O
it	O
is	O
the	O
basis	O
of	O
the	O
influential	O
learning	O
theories	O
of	O
clark	O
hull	B
and	O
experimental	O
methods	O
of	O
b	O
f	O
skinner	B
hull	B
skinner	B
the	O
term	O
reinforcement	O
in	O
the	O
context	O
of	O
animal	O
learning	O
came	O
into	O
use	O
well	O
after	O
thorndike	O
s	O
expression	O
of	O
the	O
law	B
of	I
effect	I
to	O
the	O
best	O
of	O
our	O
knowledge	O
first	O
appearing	O
in	O
this	O
context	O
in	O
the	O
english	O
translation	O
of	O
pavlov	B
s	O
monograph	O
on	O
conditioned	O
reflexes	O
reinforcement	O
is	O
the	O
strengthening	O
of	O
a	O
pattern	O
of	O
behavior	O
as	O
a	O
result	O
of	O
an	O
animal	O
receiving	O
a	O
stimulus	O
a	O
reinforcer	O
in	O
an	O
appropriate	O
temporal	O
relationship	O
with	O
another	O
stimulus	O
or	O
with	O
a	O
response	O
some	O
psychologists	O
extended	O
its	O
meaning	O
to	O
include	O
the	O
process	O
of	O
weakening	O
in	O
addition	O
to	O
strengthening	O
as	O
well	O
applying	O
when	O
the	O
omission	O
or	O
termination	O
of	O
an	O
event	O
changes	O
behavior	O
reinforcement	O
produces	O
changes	O
in	O
behavior	O
that	O
persist	O
after	O
the	O
reinforcer	O
is	O
withdrawn	O
so	O
that	O
a	O
stimulus	O
that	O
attracts	O
an	O
animal	O
s	O
attention	O
or	O
that	O
energizes	O
its	O
behavior	O
without	O
producing	O
lasting	O
changes	O
is	O
not	O
considered	O
to	O
be	O
a	O
reinforcer	O
the	O
idea	O
of	O
implementing	O
trial-and-error	O
learning	O
in	O
a	O
computer	O
appeared	O
among	O
the	O
earliest	O
thoughts	O
about	O
the	O
possibility	O
of	O
artificial	B
intelligence	I
in	O
a	O
report	O
alan	O
turing	O
described	O
a	O
design	O
for	O
a	O
pleasure-pain	O
system	O
that	O
worked	O
along	O
the	O
lines	O
of	O
the	O
law	B
of	I
effect	I
when	O
a	O
configuration	O
is	O
reached	O
for	O
which	O
the	O
action	B
is	O
undetermined	O
a	O
random	O
choice	O
for	O
the	O
missing	O
data	O
is	O
made	O
and	O
the	O
appropriate	O
entry	O
is	O
made	O
in	O
the	O
description	O
tentatively	O
and	O
is	O
applied	O
when	O
a	O
pain	O
stimulus	O
occurs	O
all	O
tentative	O
entries	O
are	O
cancelled	O
and	O
when	O
a	O
pleasure	O
stimulus	O
occurs	O
they	O
are	O
all	O
made	O
permanent	O
many	O
ingenious	O
electro-mechanical	O
machines	O
were	O
constructed	O
that	O
demonstrated	O
trialand-error	O
learning	O
the	O
earliest	O
may	O
have	O
been	O
a	O
machine	O
built	O
by	O
thomas	O
ross	O
that	O
was	O
able	O
to	O
find	O
its	O
way	O
through	O
a	O
simple	O
maze	O
and	O
remember	O
the	O
path	O
through	O
the	O
settings	O
of	O
switches	O
in	O
w	O
grey	O
walter	O
already	O
known	O
for	O
his	O
mechanical	O
tortoise	O
built	O
a	O
version	O
capable	O
of	O
a	O
simple	O
form	O
of	O
learning	O
in	O
claude	O
shannon	B
demonstrated	O
a	O
maze-running	O
mouse	O
named	O
theseus	O
that	O
used	O
trial	O
and	O
error	O
to	O
find	O
its	O
way	O
through	O
a	O
maze	O
with	O
the	O
maze	O
itself	O
remembering	O
the	O
successful	O
directions	O
via	O
magnets	O
and	O
relays	O
under	O
its	O
floor	O
also	O
shannon	B
j	O
a	O
deutsch	O
described	O
a	O
maze-solving	O
machine	O
based	O
on	O
his	O
behavior	O
theory	O
that	O
has	O
some	O
properties	O
in	O
common	O
with	O
model-based	B
reinforcement	B
learning	I
in	O
his	O
ph	O
d	O
dissertation	O
marvin	O
minsky	B
discussed	O
computational	O
models	O
of	O
reinforcement	B
learning	I
and	O
described	O
his	O
construction	O
of	O
an	O
analog	O
machine	O
composed	O
of	O
components	O
he	O
called	O
snarcs	O
neural-analog	O
reinforcement	O
calculators	O
meant	O
to	O
resemble	O
modifiable	O
synaptic	O
connections	O
in	O
the	O
brain	O
the	O
fascinating	O
web	O
site	O
cyberneticzoo	O
com	O
contains	O
a	O
wealth	O
of	O
information	O
on	O
these	O
and	O
many	O
other	O
electro-mechanical	O
learning	O
machines	O
early	O
history	B
of	I
reinforcement	B
learning	I
building	O
electro-mechanical	O
learning	O
machines	O
gave	O
way	O
to	O
programming	O
digital	O
computers	O
to	O
perform	O
various	O
types	O
of	O
learning	O
some	O
of	O
which	O
implemented	O
trial-and-error	O
learning	O
farley	O
and	O
clark	O
described	O
a	O
digital	O
simulation	O
of	O
a	O
neural-network	O
learning	O
machine	O
that	O
learned	O
by	O
trial	O
and	O
error	O
but	O
their	O
interests	O
soon	O
shifted	O
from	O
trial-and-error	O
learning	O
to	O
generalization	O
and	O
pattern	O
recognition	O
that	O
is	O
from	O
reinforcement	B
learning	I
to	O
supervised	B
learning	I
and	O
farley	O
this	O
began	O
a	O
pattern	O
of	O
confusion	O
about	O
the	O
relationship	O
between	O
these	O
types	O
of	O
learning	O
many	O
researchers	O
seemed	O
to	O
believe	O
that	O
they	O
were	O
studying	O
reinforcement	B
learning	I
when	O
they	O
were	O
actually	O
studying	O
supervised	B
learning	I
for	O
example	O
neural	B
network	O
pioneers	O
such	O
as	O
rosenblatt	O
and	O
widrow	O
and	O
hoff	O
were	O
clearly	O
motivated	O
by	O
reinforcement	B
learning	I
they	O
used	O
the	O
language	O
of	O
rewards	O
and	O
punishments	O
but	O
the	O
systems	O
they	O
studied	O
were	O
supervised	B
learning	I
systems	O
suitable	O
for	O
pattern	O
recognition	O
and	O
perceptual	O
learning	O
even	O
today	O
some	O
researchers	O
and	O
textbooks	O
minimize	O
or	O
blur	O
the	O
distinction	O
between	O
these	O
types	O
of	O
learning	O
for	O
example	O
some	O
neural-network	O
textbooks	O
have	O
used	O
the	O
term	O
trial-and-error	O
to	O
describe	O
networks	O
that	O
learn	O
from	O
training	O
examples	O
this	O
is	O
an	O
understandable	O
confusion	O
because	O
these	O
networks	O
use	O
error	O
information	O
to	O
update	O
connection	O
weights	O
but	O
this	O
misses	O
the	O
essential	O
character	O
of	O
trial-and-error	O
learning	O
as	O
selecting	O
actions	O
on	O
the	O
basis	O
of	O
evaluative	B
feedback	I
that	O
does	O
not	O
rely	O
on	O
knowledge	O
of	O
what	O
the	O
correct	O
action	B
should	O
be	O
partly	O
as	O
a	O
result	O
of	O
these	O
confusions	O
research	O
into	O
genuine	O
trial-and-error	O
learning	O
became	O
rare	O
in	O
the	O
and	O
although	O
there	O
were	O
notable	O
exceptions	O
in	O
the	O
the	O
terms	O
reinforcement	O
and	B
reinforcement	B
learning	I
were	O
used	O
in	O
the	O
engineering	O
literature	O
for	O
the	O
first	O
time	O
to	O
describe	O
engineering	O
uses	O
of	O
trial-and-error	O
learning	O
waltz	O
and	O
fu	O
mendel	O
fu	O
mendel	O
and	O
mcclaren	O
particularly	O
influential	O
was	O
minsky	B
s	O
paper	O
steps	O
toward	O
artificial	B
intelligence	I
which	O
discussed	O
several	O
issues	O
relevant	O
to	O
trial-and-error	O
learning	O
including	O
prediction	B
expectation	O
and	O
what	O
he	O
called	O
the	O
basic	O
credit-assignment	O
problem	O
for	O
complex	O
reinforcement	B
learning	I
systems	O
how	O
do	O
you	O
distribute	O
credit	O
for	O
success	O
among	O
the	O
many	O
decisions	O
that	O
may	O
have	O
been	O
involved	O
in	O
producing	O
it	O
all	O
of	O
the	O
methods	O
we	O
discuss	O
in	O
this	O
book	O
are	O
in	O
a	O
sense	O
directed	O
toward	O
solving	O
this	O
problem	O
minsky	B
s	O
paper	O
is	O
well	O
worth	O
reading	O
today	O
in	O
the	O
next	O
few	O
paragraphs	O
we	O
discuss	O
some	O
of	O
the	O
other	O
exceptions	O
and	O
partial	O
exceptions	O
to	O
the	O
relative	O
neglect	O
of	O
computational	O
and	O
theoretical	O
study	O
of	O
genuine	O
trialand-error	O
learning	O
in	O
the	O
and	O
one	O
of	O
these	O
was	O
the	O
work	O
by	O
a	O
new	O
zealand	O
researcher	O
named	O
john	O
andreae	B
andreae	B
developed	O
a	O
system	O
called	O
stella	O
that	O
learned	O
by	O
trial	O
and	O
error	O
in	O
interaction	O
with	O
its	O
environment	B
this	O
system	O
included	O
an	O
internal	O
model	O
of	O
the	O
world	O
and	O
later	O
an	O
internal	O
monologue	O
to	O
deal	O
with	O
problems	O
of	O
hidden	O
state	B
andreae	B
s	O
later	O
work	O
placed	O
more	O
emphasis	O
on	O
learning	O
from	O
a	O
teacher	O
but	O
still	O
included	O
learning	O
by	O
trial	O
and	O
error	O
with	O
the	O
generation	O
of	O
novel	O
events	O
being	O
one	O
of	O
the	O
system	O
s	O
goals	O
a	O
feature	O
of	O
this	O
work	O
was	O
a	O
leakback	O
process	O
elaborated	O
more	O
fully	O
in	O
andreae	B
that	O
implemented	O
a	O
credit-assignment	O
mechanism	O
similar	O
to	O
the	O
backing-up	O
update	O
operations	O
that	O
we	O
describe	O
unfortunately	O
his	O
pioneering	O
research	O
was	O
not	O
well	O
known	O
and	O
did	O
not	O
greatly	O
impact	O
subsequent	O
reinforcement	B
learning	I
research	O
chapter	O
introduction	O
more	O
influential	O
was	O
the	O
work	O
of	O
donald	O
michie	B
in	O
and	O
he	O
described	O
a	O
simple	O
trial-and-error	O
learning	O
system	O
for	O
learning	O
how	O
to	O
play	O
tic-tac-toe	B
naughts	O
and	O
crosses	O
called	O
menace	O
matchbox	O
educable	O
naughts	O
and	O
crosses	O
engine	O
it	O
consisted	O
of	O
a	O
matchbox	O
for	O
each	O
possible	O
game	O
position	O
each	O
matchbox	O
containing	O
a	O
number	O
of	O
colored	O
beads	O
a	O
different	O
color	O
for	O
each	O
possible	O
move	O
from	O
that	O
position	O
by	O
drawing	O
a	O
bead	O
at	O
random	O
from	O
the	O
matchbox	O
corresponding	O
to	O
the	O
current	O
game	O
position	O
one	O
could	O
determine	O
menace	O
s	O
move	O
when	O
a	O
game	O
was	O
over	O
beads	O
were	O
added	O
to	O
or	O
removed	O
from	O
the	O
boxes	B
used	O
during	O
play	O
to	O
reinforce	B
or	O
punish	O
menace	O
s	O
decisions	O
michie	B
and	O
chambers	O
described	O
another	O
tic-tac-toe	B
reinforcement	O
learner	O
called	O
glee	O
learning	O
expectimaxing	O
engine	O
and	O
a	O
reinforcement	B
learning	I
controller	O
called	O
boxes	B
they	O
applied	O
boxes	B
to	O
the	O
task	O
of	O
learning	O
to	O
balance	O
a	O
pole	O
hinged	O
to	O
a	O
movable	O
cart	O
on	O
the	O
basis	O
of	O
a	O
failure	O
signal	O
occurring	O
only	O
when	O
the	O
pole	O
fell	O
or	O
the	O
cart	O
reached	O
the	O
end	O
of	O
a	O
track	O
this	O
task	O
was	O
adapted	O
from	O
the	O
earlier	O
work	O
of	O
widrow	O
and	O
smith	O
who	O
used	O
supervised	B
learning	I
methods	O
assuming	O
instruction	O
from	O
a	O
teacher	O
already	O
able	O
to	O
balance	O
the	O
pole	O
michie	B
and	O
chambers	O
s	O
version	O
of	O
pole-balancing	O
is	O
one	O
of	O
the	O
best	O
early	O
examples	O
of	O
a	O
reinforcement	B
learning	I
task	O
under	O
conditions	O
of	O
incomplete	O
knowledge	O
it	O
influenced	O
much	O
later	O
work	O
in	O
reinforcement	B
learning	I
beginning	O
with	O
some	O
of	O
our	O
own	O
studies	O
sutton	O
and	O
anderson	O
sutton	O
michie	B
consistently	O
emphasized	O
the	O
role	O
of	O
trial	O
and	O
error	O
and	O
learning	O
as	O
essential	O
aspects	O
of	O
artificial	B
intelligence	I
widrow	O
gupta	O
and	O
maitra	O
modified	O
the	O
least-mean-square	O
algorithm	O
of	O
widrow	O
and	O
hoff	O
to	O
produce	O
a	O
reinforcement	B
learning	I
rule	O
that	O
could	O
learn	O
from	O
success	O
and	O
failure	O
signals	O
instead	O
of	O
from	O
training	O
examples	O
they	O
called	O
this	O
form	O
of	O
learning	O
selective	B
bootstrap	I
adaptation	I
and	O
described	O
it	O
as	O
learning	O
with	O
a	O
critic	O
instead	O
of	O
learning	O
with	O
a	O
teacher	O
they	O
analyzed	O
this	O
rule	O
and	O
showed	O
how	O
it	O
could	O
learn	O
to	O
play	O
blackjack	O
this	O
was	O
an	O
isolated	O
foray	O
into	O
reinforcement	B
learning	I
by	O
widrow	O
whose	O
contributions	O
to	O
supervised	B
learning	I
were	O
much	O
more	O
influential	O
our	O
use	O
of	O
the	O
term	O
critic	O
is	O
derived	O
from	O
widrow	O
gupta	O
and	O
maitra	O
s	O
paper	O
buchanan	O
mitchell	O
smith	O
and	O
johnson	O
independently	O
used	O
the	O
term	O
critic	O
in	O
the	O
context	O
of	O
machine	O
learning	O
also	O
dietterich	O
and	O
buchanan	O
but	O
for	O
them	O
a	O
critic	O
is	O
an	O
expert	O
system	O
able	O
to	O
do	O
more	O
than	O
evaluate	O
performance	O
research	O
on	O
learning	B
automata	I
had	O
a	O
more	O
direct	O
influence	O
on	O
the	O
trial-and-error	O
thread	O
leading	O
to	O
modern	O
reinforcement	B
learning	I
research	O
these	O
are	O
methods	O
for	O
solving	O
a	O
nonassociative	O
purely	O
selectional	O
learning	O
problem	O
known	O
as	O
the	O
k-armed	O
bandit	O
by	O
analogy	O
to	O
a	O
slot	O
machine	O
or	O
one-armed	O
bandit	O
except	O
with	O
k	O
levers	O
chapter	O
learning	B
automata	I
are	O
simple	O
low-memory	O
machines	O
for	O
improving	O
the	O
probability	O
of	O
reward	O
in	O
these	O
problems	O
learning	B
automata	I
originated	O
with	O
work	O
in	O
the	O
of	O
the	O
russian	O
mathematician	O
and	O
physicist	O
m	O
l	O
tsetlin	O
and	O
colleagues	O
posthumously	O
in	O
tsetlin	O
and	O
has	O
been	O
extensively	O
developed	O
since	O
then	O
within	O
engineering	O
narendra	O
and	O
thathachar	O
these	O
developments	O
included	O
the	O
study	O
of	O
stochastic	O
learning	B
automata	I
which	O
are	O
methods	O
for	O
updating	O
action	B
probabilities	O
on	O
the	O
basis	O
of	O
reward	O
signals	O
although	O
not	O
developed	O
in	O
the	O
tradition	O
of	O
stochastic	O
learning	B
automata	I
harth	O
and	O
tzanakou	O
s	O
alopex	O
algorithm	O
algorithm	O
of	O
pattern	O
extraction	O
is	O
a	O
stochastic	O
method	O
for	O
detecting	O
correlations	O
between	O
actions	O
and	B
reinforcement	I
that	O
influenced	O
some	O
of	O
our	O
early	O
research	O
sutton	O
and	O
brouwer	O
early	O
history	B
of	I
reinforcement	B
learning	I
stochastic	O
learning	B
automata	I
were	O
foreshadowed	O
by	O
earlier	O
work	O
in	B
psychology	B
beginning	O
with	O
william	O
estes	O
effort	O
toward	O
a	O
statistical	O
theory	O
of	O
learning	O
and	O
further	O
developed	O
by	O
others	O
most	O
famously	O
by	O
psychologist	O
robert	O
bush	O
and	O
statistician	O
frederick	O
mosteller	O
and	O
mosteller	O
the	O
statistical	O
learning	O
theories	O
developed	O
in	B
psychology	B
were	O
adopted	O
by	O
researchers	O
in	O
economics	O
leading	O
to	O
a	O
thread	O
of	O
research	O
in	O
that	O
field	O
devoted	O
to	O
reinforcement	B
learning	I
this	O
work	O
began	O
in	O
with	O
the	O
application	O
of	O
bush	O
and	O
mosteller	O
s	O
learning	O
theory	O
to	O
a	O
collection	O
of	O
classical	O
economic	O
models	O
one	O
goal	O
of	O
this	O
research	O
was	O
to	O
study	O
artificial	O
agents	O
that	O
act	O
more	O
like	O
real	O
people	O
than	O
do	O
traditional	O
idealized	O
economic	O
agents	O
this	O
approach	O
expanded	O
to	O
the	O
study	O
of	O
reinforcement	B
learning	I
in	O
the	O
context	O
of	O
game	B
theory	I
although	O
reinforcement	B
learning	I
in	O
economics	O
developed	O
largely	O
independently	O
of	O
the	O
early	O
work	O
in	O
artificial	B
intelligence	I
reinforcement	B
learning	I
and	O
game	B
theory	I
is	O
a	O
topic	O
of	O
current	O
interest	O
in	O
both	O
fields	O
but	O
one	O
that	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
camerer	O
discusses	O
the	O
reinforcement	B
learning	I
tradition	O
in	O
economics	O
and	O
now	O
e	O
et	O
al	O
provide	O
an	O
overview	O
of	O
the	O
subject	O
from	O
the	O
point	O
of	O
view	O
of	O
multi-agent	O
extensions	O
to	O
the	O
approach	O
that	O
we	O
introduce	O
in	O
this	O
book	O
reinforcement	O
in	O
the	O
context	O
of	O
game	B
theory	I
is	O
a	O
much	O
different	O
subject	O
than	O
reinforcement	B
learning	I
used	O
in	O
programs	O
to	O
play	O
tic-tac-toe	B
checkers	O
and	O
other	O
recreational	O
games	O
see	O
for	O
example	O
szita	O
for	O
an	O
overview	O
of	O
this	O
aspect	O
of	O
reinforcement	B
learning	I
and	O
games	O
john	O
holland	B
outlined	O
a	O
general	O
theory	O
of	O
adaptive	O
systems	O
based	O
on	O
selectional	O
principles	O
his	O
early	O
work	O
concerned	O
trial	O
and	O
error	O
primarily	O
in	O
its	O
nonassociative	O
form	O
as	O
in	O
evolutionary	B
methods	I
and	O
the	O
k-armed	O
bandit	O
in	O
and	O
more	O
fully	O
in	O
he	O
introduced	O
classifier	B
systems	I
true	O
reinforcement	B
learning	I
systems	O
including	O
association	O
and	O
value	B
functions	O
a	O
key	O
component	O
of	O
holland	B
s	O
classifier	B
systems	I
was	O
the	O
bucket-brigade	B
algorithm	I
for	O
credit	B
assignment	I
that	O
is	O
closely	O
related	O
to	O
the	O
temporal	O
difference	O
algorithm	O
used	O
in	O
our	O
tic-tac-toe	B
example	O
and	O
discussed	O
in	O
chapter	O
another	O
key	O
component	O
was	O
a	O
genetic	O
algorithm	O
an	O
evolutionary	O
method	O
whose	O
role	O
was	O
to	O
evolve	O
useful	O
representations	O
classifier	B
systems	I
have	O
been	O
extensively	O
developed	O
by	O
many	O
researchers	O
to	O
form	O
a	O
major	O
branch	O
of	O
reinforcement	B
learning	I
research	O
by	O
urbanowicz	O
and	O
moore	O
but	O
genetic	B
algorithms	I
which	O
we	O
do	O
not	O
consider	O
to	O
be	O
reinforcement	B
learning	I
systems	O
by	O
themselves	O
have	O
received	O
much	O
more	O
attention	O
as	O
have	O
other	O
approaches	O
to	O
evolutionary	O
computation	O
fogel	O
owens	O
and	O
walsh	O
and	O
koza	O
the	O
individual	O
most	O
responsible	O
for	O
reviving	O
the	O
trial-and-error	O
thread	O
to	O
reinforcement	B
learning	I
within	O
artificial	B
intelligence	I
was	O
harry	O
klopf	B
klopf	B
recognized	O
that	O
essential	O
aspects	O
of	O
adaptive	O
behavior	O
were	O
being	O
lost	O
as	O
learning	O
researchers	O
came	O
to	O
focus	O
almost	O
exclusively	O
on	O
supervised	B
learning	I
what	O
was	O
missing	O
according	O
to	O
klopf	B
were	O
the	O
hedonic	O
aspects	O
of	O
behavior	O
the	O
drive	O
to	O
achieve	O
some	O
result	O
from	O
the	O
environment	B
to	O
control	B
the	O
environment	B
toward	O
desired	O
ends	O
and	O
away	O
from	O
undesired	O
ends	O
this	O
is	O
the	O
essential	O
idea	O
of	O
trial-and-error	O
learning	O
klopf	B
s	O
ideas	O
were	O
especially	O
influential	O
on	O
the	O
authors	O
because	O
our	O
assessment	B
of	I
them	O
and	O
sutton	O
led	O
to	O
our	O
appreciation	O
of	O
the	O
distinction	O
between	O
supervised	O
and	B
reinforcement	B
learning	I
and	O
to	O
our	O
eventual	O
focus	O
on	O
reinforcement	B
learning	I
much	O
of	O
the	O
early	O
work	O
chapter	O
introduction	O
that	O
we	O
and	O
colleagues	O
accomplished	O
was	O
directed	O
toward	O
showing	O
that	O
reinforcement	B
learning	I
and	O
supervised	B
learning	I
were	O
indeed	O
different	O
sutton	O
and	O
brouwer	O
barto	O
and	O
sutton	O
barto	O
and	O
anandan	O
other	O
studies	O
showed	O
how	O
reinforcement	B
learning	I
could	O
address	O
important	O
problems	O
in	O
neural	B
network	O
learning	O
in	O
particular	O
how	O
it	O
could	O
produce	O
learning	O
algorithms	O
for	O
multilayer	O
networks	O
anderson	O
and	O
sutton	O
barto	O
and	O
anderson	O
barto	O
barto	O
and	O
jordan	O
we	O
say	O
more	O
about	O
reinforcement	B
learning	I
and	O
neural	B
networks	O
in	O
chapter	O
we	O
turn	O
now	O
to	O
the	O
third	O
thread	O
to	O
the	O
history	B
of	I
reinforcement	B
learning	I
that	O
concerning	O
temporal-difference	B
learning	I
temporal-difference	B
learning	I
methods	O
are	O
distinctive	O
in	O
being	O
driven	O
by	O
the	O
difference	O
between	O
temporally	O
successive	O
estimates	O
of	O
the	O
same	O
quantity	O
for	O
example	O
of	O
the	O
probability	O
of	O
winning	O
in	O
the	O
tic-tac-toe	B
example	O
this	O
thread	O
is	O
smaller	O
and	O
less	O
distinct	O
than	O
the	O
other	O
two	O
but	O
it	O
has	O
played	O
a	O
particularly	O
important	O
role	O
in	O
the	O
field	O
in	O
part	O
because	O
temporal-difference	O
methods	O
seem	O
to	O
be	O
new	O
and	O
unique	O
to	O
reinforcement	B
learning	I
the	O
origins	O
of	O
temporal-difference	B
learning	I
are	O
in	O
part	O
in	B
animal	I
learning	I
psychology	B
in	O
particular	O
in	O
the	O
notion	O
of	O
secondary	O
reinforcers	O
a	O
secondary	O
reinforcer	O
is	O
a	O
stimulus	O
that	O
has	O
been	O
paired	O
with	O
a	O
primary	O
reinforcer	O
such	O
as	O
food	O
or	O
pain	O
and	O
as	O
a	O
result	O
has	O
come	O
to	O
take	O
on	O
similar	O
reinforcing	O
properties	O
minsky	B
may	O
have	O
been	O
the	O
first	O
to	O
realize	O
that	O
this	O
psychological	O
principle	O
could	O
be	O
important	O
for	O
artificial	O
learning	O
systems	O
arthur	O
samuel	O
was	O
the	O
first	O
to	O
propose	O
and	O
implement	O
a	O
learning	O
method	O
that	O
included	O
temporal-difference	O
ideas	O
as	O
part	O
of	O
his	O
celebrated	O
checkers-playing	O
program	O
samuel	O
made	O
no	O
reference	O
to	O
minsky	B
s	O
work	O
or	O
to	O
possible	O
connections	O
to	O
animal	O
learning	O
his	O
inspiration	O
apparently	O
came	O
from	O
claude	O
shannon	B
s	O
suggestion	O
that	O
a	O
computer	O
could	O
be	O
programmed	O
to	O
use	O
an	O
evaluation	O
function	O
to	O
play	O
chess	B
and	O
that	O
it	O
might	O
be	O
able	O
to	O
improve	O
its	O
play	O
by	O
modifying	O
this	O
function	O
online	B
is	O
possible	O
that	O
these	O
ideas	O
of	O
shannon	B
s	O
also	O
influenced	O
bellman	B
but	O
we	O
know	O
of	O
no	O
evidence	O
for	O
this	O
minsky	B
extensively	O
discussed	O
samuel	O
s	O
work	O
in	O
his	O
steps	O
paper	O
suggesting	O
the	O
connection	O
to	O
secondary	B
reinforcement	I
theories	O
both	O
natural	O
and	O
artificial	O
as	O
we	O
have	O
discussed	O
in	O
the	O
decade	O
following	O
the	O
work	O
of	O
minsky	B
and	O
samuel	O
little	O
computational	O
work	O
was	O
done	O
on	O
trial-and-error	O
learning	O
and	O
apparently	O
no	O
computational	O
work	O
at	O
all	O
was	O
done	O
on	O
temporal-difference	B
learning	I
in	O
klopf	B
brought	O
trial-and-error	O
learning	O
together	O
with	O
an	O
important	O
component	O
of	O
temporal-difference	B
learning	I
klopf	B
was	O
interested	O
in	O
principles	O
that	O
would	O
scale	O
to	O
learning	O
in	O
large	O
systems	O
and	O
thus	O
was	O
intrigued	O
by	O
notions	O
of	O
local	O
reinforcement	O
whereby	O
subcomponents	O
of	O
an	O
overall	O
learning	O
system	O
could	O
reinforce	B
one	O
another	O
he	O
developed	O
the	O
idea	O
of	O
generalized	O
reinforcement	O
whereby	O
every	O
component	O
every	O
neuron	O
views	O
all	O
of	O
its	O
inputs	O
in	O
reinforcement	O
terms	O
excitatory	O
inputs	O
as	O
rewards	O
and	O
inhibitory	O
inputs	O
as	O
punishments	O
this	O
is	O
not	O
the	O
same	O
idea	O
as	O
what	O
we	O
now	O
know	O
as	O
temporal-difference	B
learning	I
and	O
in	O
retrospect	O
it	O
is	O
farther	O
from	O
it	O
than	O
was	O
samuel	O
s	O
work	O
on	O
the	O
other	O
hand	O
klopf	B
linked	O
the	O
idea	O
with	O
trial-and-error	O
learning	O
and	O
related	O
it	O
to	O
the	O
massive	O
empirical	O
database	O
of	O
animal	O
learning	O
psychology	B
sutton	O
developed	O
klopf	B
s	O
ideas	O
further	O
particularly	O
the	O
links	O
to	O
animal	O
learning	O
theories	O
describing	O
learning	O
rules	O
driven	O
by	O
changes	O
in	O
temporally	O
early	O
history	B
of	I
reinforcement	B
learning	I
successive	O
predictions	O
he	O
and	O
barto	O
refined	O
these	O
ideas	O
and	O
developed	O
a	O
psychological	O
model	O
of	O
classical	B
conditioning	I
based	O
on	O
temporal-difference	B
learning	I
and	O
barto	O
barto	O
and	O
sutton	O
there	O
followed	O
several	O
other	O
influential	O
psychological	O
models	O
of	O
classical	B
conditioning	I
based	O
on	O
temporal-difference	B
learning	I
klopf	B
moore	O
et	O
al	O
sutton	O
and	O
barto	O
some	O
neuroscience	B
models	O
developed	O
at	O
this	O
time	O
are	O
well	O
interpreted	O
in	O
terms	O
of	O
temporal-difference	B
learning	I
and	O
kandel	O
byrne	O
gingrich	O
and	O
baxter	O
gelperin	O
hopfield	O
and	O
tank	O
tesauro	O
friston	O
et	O
al	O
although	O
in	O
most	O
cases	O
there	O
was	O
no	O
historical	O
connection	O
our	O
early	O
work	O
on	O
temporal-difference	B
learning	I
was	O
strongly	O
influenced	O
by	O
animal	O
learning	O
theories	O
and	O
by	O
klopf	B
s	O
work	O
relationships	O
to	O
minsky	B
s	O
steps	O
paper	O
and	O
to	O
samuel	O
s	O
checkers	O
players	O
appear	O
to	O
have	O
been	O
recognized	O
only	O
afterward	O
by	O
however	O
we	O
were	O
fully	O
aware	O
of	O
all	O
the	O
prior	O
work	O
mentioned	O
above	O
as	O
part	O
of	O
the	O
temporal-difference	O
and	O
trial-and-error	O
threads	O
at	O
this	O
time	O
we	O
developed	O
a	O
method	O
for	O
using	O
temporal-difference	B
learning	I
combined	O
with	O
trial-and-error	O
learning	O
known	O
as	O
the	O
actor	O
critic	O
architecture	O
and	O
applied	O
this	O
method	O
to	O
michie	B
and	O
chambers	O
s	O
polebalancing	O
problem	O
sutton	O
and	O
anderson	O
this	O
method	O
was	O
extensively	O
studied	O
in	O
sutton	O
s	O
ph	O
d	O
dissertation	O
and	O
extended	O
to	O
use	O
backpropagation	B
neural	B
networks	O
in	O
anderson	O
s	O
ph	O
d	O
dissertation	O
around	O
this	O
time	O
holland	B
incorporated	O
temporal-difference	O
ideas	O
explicitly	O
into	O
his	O
classifier	B
systems	I
in	O
the	O
form	O
of	O
his	O
bucket-brigade	B
algorithm	I
a	O
key	O
step	O
was	O
taken	O
by	O
sutton	O
in	O
by	O
separating	O
temporal-difference	B
learning	I
from	O
control	B
treating	O
it	O
as	O
a	O
general	O
prediction	B
method	O
that	O
paper	O
also	O
introduced	O
the	O
td	B
algorithm	O
and	O
proved	O
some	O
of	O
its	O
convergence	O
properties	O
as	O
we	O
were	O
finalizing	O
our	O
work	O
on	O
the	O
actor	O
critic	O
architecture	O
in	O
we	O
discovered	O
a	O
paper	O
by	O
ian	O
witten	B
which	O
appears	O
to	O
be	O
the	O
earliest	O
publication	O
of	O
a	O
temporal-difference	B
learning	I
rule	O
he	O
proposed	O
the	O
method	O
that	O
we	O
now	O
call	O
tabular	O
for	O
use	O
as	O
part	O
of	O
an	O
adaptive	O
controller	O
for	O
solving	O
mdps	O
witten	B
s	O
work	O
was	O
a	O
descendant	O
of	O
andreae	B
s	O
early	O
experiments	O
with	O
stella	O
and	O
other	O
trial-and-error	O
learning	O
systems	O
thus	O
witten	B
s	O
paper	O
spanned	O
both	O
major	O
threads	O
of	O
reinforcement	B
learning	I
research	O
trial-and-error	O
learning	O
and	O
optimal	B
control	B
while	O
making	O
a	O
distinct	O
early	O
contribution	O
to	O
temporal-difference	B
learning	I
the	O
temporal-difference	O
and	O
optimal	B
control	B
threads	O
were	O
fully	O
brought	O
together	O
in	O
with	O
chris	O
watkins	B
s	O
development	O
of	O
q-learning	B
this	O
work	O
extended	O
and	O
integrated	O
prior	O
work	O
in	O
all	O
three	O
threads	O
of	O
reinforcement	B
learning	I
research	O
paul	O
werbos	B
contributed	O
to	O
this	O
integration	O
by	O
arguing	O
for	O
the	O
convergence	O
of	O
trial-and-error	O
learning	O
and	B
dynamic	B
programming	I
since	O
by	O
the	O
time	O
of	O
watkins	B
s	O
work	O
there	O
had	O
been	O
tremendous	O
growth	O
in	O
reinforcement	B
learning	I
research	O
primarily	O
in	O
the	O
machine	O
learning	O
subfield	O
of	O
artificial	B
intelligence	I
but	O
also	O
in	O
neural	B
networks	O
and	B
artificial	B
intelligence	I
more	O
broadly	O
in	O
the	O
remarkable	O
success	O
of	O
gerry	O
tesauro	O
s	O
backgammon	B
playing	O
program	O
td-gammon	B
brought	O
additional	O
attention	O
to	O
the	O
field	O
in	O
the	O
time	O
since	O
publication	O
of	O
the	O
first	O
edition	O
of	O
this	O
book	O
a	O
flourishing	O
subfield	O
of	O
neuroscience	B
developed	O
that	O
focuses	O
on	O
the	O
relationship	O
between	O
reinforcement	B
learning	I
algorithms	O
and	B
reinforcement	B
learning	I
in	O
the	O
nervous	O
system	O
most	O
responsible	O
for	O
this	O
chapter	O
introduction	O
is	O
an	O
uncanny	O
similarity	O
between	O
the	O
behavior	O
of	O
temporal-difference	O
algorithms	O
and	O
the	O
activity	O
of	O
dopamine	B
producing	O
neurons	O
in	O
the	O
brain	O
as	O
pointed	O
out	O
by	O
a	O
number	O
of	O
researchers	O
et	O
al	O
barto	O
houk	O
adams	O
and	O
barto	O
montague	O
dayan	O
and	O
sejnowski	O
and	O
schultz	B
dayan	O
and	O
montague	O
chapter	O
provides	O
an	O
introduction	O
to	O
this	O
exciting	O
aspect	O
of	O
reinforcement	B
learning	I
other	O
important	O
contributions	O
made	O
in	O
the	O
recent	O
history	B
of	I
reinforcement	B
learning	I
are	O
too	O
numerous	O
to	O
mention	O
in	O
this	O
brief	O
account	O
we	O
cite	O
many	O
of	O
these	O
at	O
the	O
end	O
of	O
the	O
individual	O
chapters	O
in	O
which	O
they	O
arise	O
bibliographical	O
remarks	O
for	O
additional	O
general	O
coverage	O
of	O
reinforcement	B
learning	I
we	O
refer	O
the	O
reader	O
to	O
the	O
books	O
by	O
szepesv	O
ari	O
bertsekas	O
and	O
tsitsiklis	O
kaelbling	O
and	O
sugiyama	O
hachiya	O
and	O
morimura	O
books	O
that	O
take	O
a	O
control	B
or	O
operations	O
research	O
perspective	O
include	O
those	O
of	O
si	O
et	O
al	O
powell	O
lewis	O
and	O
liu	O
and	O
bertsekas	O
cao	O
s	O
review	O
places	O
reinforcement	B
learning	I
in	O
the	O
context	O
of	O
other	O
approaches	O
to	O
learning	O
and	O
optimization	O
of	O
stochastic	O
dynamic	O
systems	O
three	O
special	O
issues	O
of	O
the	O
journal	O
machine	O
learning	O
focus	O
on	O
reinforcement	B
learning	I
sutton	O
kaelbling	O
and	O
singh	O
useful	O
surveys	O
are	O
provided	O
by	O
barto	O
kaelbling	O
littman	O
and	O
moore	O
and	O
keerthi	O
and	O
ravindran	O
the	O
volume	O
edited	O
by	O
weiring	O
and	O
van	O
otterlo	O
provides	O
an	O
excellent	O
overview	O
of	O
recent	O
developments	O
the	O
example	O
of	O
phil	O
s	O
breakfast	O
in	O
this	O
chapter	O
was	O
inspired	O
by	O
agre	O
the	O
temporal-difference	O
method	O
used	O
in	O
the	O
tic-tac-toe	B
example	O
is	O
developed	O
in	O
chapter	O
part	O
i	O
tabular	B
solution	I
methods	I
in	O
this	O
part	O
of	O
the	O
book	O
we	O
describe	O
almost	O
all	O
the	O
core	O
ideas	O
of	O
reinforcement	B
learning	I
algorithms	O
in	O
their	O
simplest	O
forms	O
that	O
in	O
which	O
the	O
state	B
and	O
action	B
spaces	O
are	O
small	O
enough	O
for	O
the	O
approximate	O
value	B
functions	O
to	O
be	O
represented	O
as	O
arrays	O
or	O
tables	O
in	O
this	O
case	O
the	O
methods	O
can	O
often	O
find	O
exact	O
solutions	O
that	O
is	O
they	O
can	O
often	O
find	O
exactly	O
the	O
optimal	O
value	B
function	I
and	O
the	O
optimal	O
policy	B
this	O
contrasts	O
with	O
the	O
approximate	O
methods	O
described	O
in	O
the	O
next	O
part	O
of	O
the	O
book	O
which	O
only	O
find	O
approximate	O
solutions	O
but	O
which	O
in	O
return	B
can	O
be	O
applied	O
effectively	O
to	O
much	O
larger	O
problems	O
the	O
first	O
chapter	O
of	O
this	O
part	O
of	O
the	O
book	O
describes	O
solution	O
methods	O
for	O
the	O
special	O
case	O
of	O
the	O
reinforcement	B
learning	I
problem	O
in	O
which	O
there	O
is	O
only	O
a	O
single	O
state	B
called	O
bandit	B
problems	I
the	O
second	O
chapter	O
describes	O
the	O
general	O
problem	O
formulation	O
that	O
we	O
treat	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
finite	O
markov	O
decision	O
processes	O
and	O
its	O
main	O
ideas	O
including	O
bellman	B
equations	O
and	O
value	B
functions	O
the	O
next	O
three	O
chapters	O
describe	O
three	O
fundamental	O
classes	O
of	O
methods	O
for	O
solving	O
finite	O
markov	O
decision	O
problems	O
dynamic	B
programming	I
monte	B
carlo	I
methods	I
and	O
temporaldifference	O
learning	O
each	O
class	O
of	O
methods	O
has	O
its	O
strengths	O
and	O
weaknesses	O
dynamic	B
programming	I
methods	O
are	O
well	O
developed	O
mathematically	O
but	O
require	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	B
monte	B
carlo	I
methods	I
don	O
t	O
require	O
a	O
model	O
and	O
are	O
conceptually	O
simple	O
but	O
are	O
not	O
well	O
suited	O
for	O
step-by-step	O
incremental	O
computation	O
finally	O
temporal-difference	O
methods	O
require	O
no	O
model	O
and	O
are	O
fully	O
incremental	O
but	O
are	O
more	O
complex	O
to	O
analyze	O
the	O
methods	O
also	O
differ	O
in	O
several	O
ways	O
with	O
respect	O
to	O
their	O
efficiency	O
and	O
speed	O
of	O
convergence	O
the	O
remaining	O
two	O
chapters	O
describe	O
how	O
these	O
three	O
classes	O
of	O
methods	O
can	O
be	O
combined	O
to	O
obtain	O
the	O
best	O
features	O
of	O
each	O
of	O
them	O
in	O
one	O
chapter	O
we	O
describe	O
how	O
the	O
strengths	O
of	O
monte	B
carlo	I
methods	I
can	O
be	O
combined	O
with	O
the	O
strengths	O
of	O
temporaldifference	O
methods	O
via	O
multi-step	O
bootstrapping	B
methods	O
in	O
the	O
final	O
chapter	O
of	O
this	O
part	O
of	O
the	O
book	O
we	O
show	O
how	O
temporal-difference	B
learning	I
methods	O
can	O
be	O
combined	O
with	O
model	O
learning	O
and	O
planning	B
methods	O
as	O
dynamic	B
programming	I
for	O
a	O
complete	O
and	O
unified	O
solution	O
to	O
the	O
tabular	O
reinforcement	B
learning	I
problem	O
chapter	O
multi-armed	B
bandits	I
the	O
most	O
important	O
feature	O
distinguishing	O
reinforcement	B
learning	I
from	O
other	O
types	O
of	O
learning	O
is	O
that	O
it	O
uses	O
training	O
information	O
that	O
evaluates	O
the	O
actions	O
taken	O
rather	O
than	O
instructs	O
by	O
giving	O
correct	O
actions	O
this	O
is	O
what	O
creates	O
the	O
need	O
for	O
active	O
exploration	O
for	O
an	O
explicit	O
search	O
for	O
good	O
behavior	O
purely	O
evaluative	B
feedback	I
indicates	O
how	O
good	O
the	O
action	B
taken	O
was	O
but	O
not	O
whether	O
it	O
was	O
the	O
best	O
or	O
the	O
worst	O
action	B
possible	O
purely	O
instructive	O
feedback	O
on	O
the	O
other	O
hand	O
indicates	O
the	O
correct	O
action	B
to	O
take	O
independently	O
of	O
the	O
action	B
actually	O
taken	O
this	O
kind	O
of	O
feedback	O
is	O
the	O
basis	O
of	O
supervised	B
learning	I
which	O
includes	O
large	O
parts	O
of	O
pattern	O
classification	O
artificial	B
neural	B
networks	I
and	O
system	B
identification	I
in	O
their	O
pure	O
forms	O
these	O
two	O
kinds	O
of	O
feedback	O
are	O
quite	O
distinct	O
evaluative	B
feedback	I
depends	O
entirely	O
on	O
the	O
action	B
taken	O
whereas	O
instructive	O
feedback	O
is	O
independent	O
of	O
the	O
action	B
taken	O
in	O
this	O
chapter	O
we	O
study	O
the	O
evaluative	O
aspect	O
of	O
reinforcement	B
learning	I
in	O
a	O
simplified	O
setting	O
one	O
that	O
does	O
not	O
involve	O
learning	O
to	O
act	O
in	O
more	O
than	O
one	O
situation	O
this	O
nonassociative	O
setting	O
is	O
the	O
one	O
in	O
which	O
most	O
prior	O
work	O
involving	O
evaluative	B
feedback	I
has	O
been	O
done	O
and	O
it	O
avoids	O
much	O
of	O
the	O
complexity	O
of	O
the	O
full	O
reinforcement	B
learning	I
problem	O
studying	O
this	O
case	O
enables	O
us	O
to	O
see	O
most	O
clearly	O
how	O
evaluative	B
feedback	I
differs	O
from	O
and	O
yet	O
can	O
be	O
combined	O
with	O
instructive	O
feedback	O
the	O
particular	O
nonassociative	O
evaluative	B
feedback	I
problem	O
that	O
we	O
explore	O
is	O
a	O
simple	O
version	O
of	O
the	O
k-armed	O
bandit	O
problem	O
we	O
use	O
this	O
problem	O
to	O
introduce	O
a	O
number	O
of	O
basic	O
learning	O
methods	O
which	O
we	O
extend	O
in	O
later	O
chapters	O
to	O
apply	O
to	O
the	O
full	O
reinforcement	B
learning	I
problem	O
at	O
the	O
end	O
of	O
this	O
chapter	O
we	O
take	O
a	O
step	O
closer	O
to	O
the	O
full	O
reinforcement	B
learning	I
problem	O
by	O
discussing	O
what	O
happens	O
when	O
the	O
bandit	O
problem	O
becomes	O
associative	O
that	O
is	O
when	O
actions	O
are	O
taken	O
in	O
more	O
than	O
one	O
situation	O
a	O
k	O
bandit	O
problem	O
consider	O
the	O
following	O
learning	O
problem	O
you	O
are	O
faced	O
repeatedly	O
with	O
a	O
choice	O
among	O
k	O
different	O
options	B
or	O
actions	O
after	O
each	O
choice	O
you	O
receive	O
a	O
numerical	O
reward	O
chosen	O
from	O
a	O
stationary	O
probability	O
distribution	O
that	O
depends	O
on	O
the	O
action	B
you	O
selected	O
your	O
chapter	O
multi-armed	B
bandits	I
objective	O
is	O
to	O
maximize	O
the	O
expected	B
total	O
reward	O
over	O
some	O
time	O
period	O
for	O
example	O
over	O
action	B
selections	O
or	O
time	O
steps	O
this	O
is	O
the	O
original	O
form	O
of	O
the	O
k-armed	O
bandit	O
problem	O
so	O
named	O
by	O
analogy	O
to	O
a	O
slot	O
machine	O
or	O
one-armed	O
bandit	O
except	O
that	O
it	O
has	O
k	O
levers	O
instead	O
of	O
one	O
each	O
action	B
selection	O
is	O
like	O
a	O
play	O
of	O
one	O
of	O
the	O
slot	O
machine	O
s	O
levers	O
and	O
the	O
rewards	O
are	O
the	O
payoffs	O
for	O
hitting	O
the	O
jackpot	O
through	O
repeated	O
action	B
selections	O
you	O
are	O
to	O
maximize	O
your	O
winnings	O
by	O
concentrating	O
your	O
actions	O
on	O
the	O
best	O
levers	O
another	O
analogy	O
is	O
that	O
of	O
a	O
doctor	O
choosing	O
between	O
experimental	O
treatments	O
for	O
a	O
series	O
of	O
seriously	O
ill	O
patients	O
each	O
action	B
is	O
the	O
selection	O
of	O
a	O
treatment	O
and	O
each	O
reward	O
is	O
the	O
survival	O
or	O
well-being	O
of	O
the	O
patient	O
today	O
the	O
term	O
bandit	O
problem	O
is	O
sometimes	O
used	O
for	O
a	O
generalization	O
of	O
the	O
problem	O
described	O
above	O
but	O
in	O
this	O
book	O
we	O
use	O
it	O
to	O
refer	O
just	O
to	O
this	O
simple	O
case	O
in	O
our	O
k-armed	O
bandit	O
problem	O
each	O
of	O
the	O
k	O
actions	O
has	O
an	O
expected	B
or	O
mean	O
reward	O
given	O
that	O
that	O
action	B
is	O
selected	O
let	O
us	O
call	O
this	O
the	O
value	B
of	O
that	O
action	B
we	O
denote	O
the	O
action	B
selected	O
on	O
time	O
step	O
t	O
as	O
at	O
and	O
the	O
corresponding	O
reward	O
as	O
rt	O
the	O
value	B
then	O
of	O
an	O
arbitrary	O
action	B
a	O
denoted	O
q	O
is	O
the	O
expected	B
reward	O
given	O
that	O
a	O
is	O
selected	O
q	O
ert	O
at	O
a	O
if	O
you	O
knew	O
the	O
value	B
of	O
each	O
action	B
then	O
it	O
would	O
be	O
trivial	O
to	O
solve	O
the	O
k-armed	O
bandit	O
problem	O
you	O
would	O
always	O
select	O
the	O
action	B
with	O
highest	O
value	B
we	O
assume	O
that	O
you	O
do	O
not	O
know	O
the	O
action	B
values	O
with	O
certainty	O
although	O
you	O
may	O
have	O
estimates	O
we	O
denote	O
the	O
estimated	O
value	B
of	O
action	B
a	O
at	O
time	O
step	O
t	O
as	O
qta	O
we	O
would	O
like	O
qta	O
to	O
be	O
close	O
to	O
q	O
if	O
you	O
maintain	O
estimates	O
of	O
the	O
action	B
values	O
then	O
at	O
any	O
time	O
step	O
there	O
is	O
at	O
least	O
one	O
action	B
whose	O
estimated	O
value	B
is	O
greatest	O
we	O
call	O
these	O
the	O
greedy	O
actions	O
when	O
you	O
select	O
one	O
of	O
these	O
actions	O
we	O
say	O
that	O
you	O
are	O
exploiting	O
your	O
current	O
knowledge	O
of	O
the	O
values	O
of	O
the	O
actions	O
if	O
instead	O
you	O
select	O
one	O
of	O
the	O
nongreedy	O
actions	O
then	O
we	O
say	O
you	O
are	O
exploring	O
because	O
this	O
enables	O
you	O
to	O
improve	O
your	O
estimate	O
of	O
the	O
nongreedy	O
action	B
s	O
value	B
exploitation	O
is	O
the	O
right	O
thing	O
to	O
do	O
to	O
maximize	O
the	O
expected	B
reward	O
on	O
the	O
one	O
step	O
but	O
exploration	O
may	O
produce	O
the	O
greater	O
total	O
reward	O
in	O
the	O
long	O
run	O
for	O
example	O
suppose	O
a	O
greedy	O
action	B
s	O
value	B
is	O
known	O
with	O
certainty	O
while	O
several	O
other	O
actions	O
are	O
estimated	O
to	O
be	O
nearly	O
as	O
good	O
but	O
with	O
substantial	O
uncertainty	O
the	O
uncertainty	O
is	O
such	O
that	O
at	O
least	O
one	O
of	O
these	O
other	O
actions	O
probably	O
is	O
actually	O
better	O
than	O
the	O
greedy	O
action	B
but	O
you	O
don	O
t	O
know	O
which	O
one	O
if	O
you	O
have	O
many	O
time	O
steps	O
ahead	O
on	O
which	O
to	O
make	O
action	B
selections	O
then	O
it	O
may	O
be	O
better	O
to	O
explore	O
the	O
nongreedy	O
actions	O
and	O
discover	O
which	O
of	O
them	O
are	O
better	O
than	O
the	O
greedy	O
action	B
reward	O
is	O
lower	O
in	O
the	O
short	O
run	O
during	O
exploration	O
but	O
higher	O
in	O
the	O
long	O
run	O
because	O
after	O
you	O
have	O
discovered	O
the	O
better	O
actions	O
you	O
can	O
exploit	O
them	O
many	O
times	O
because	O
it	O
is	O
not	O
possible	O
both	O
to	O
explore	O
and	O
to	O
exploit	O
with	O
any	O
single	O
action	B
selection	O
one	O
often	O
refers	O
to	O
the	O
conflict	O
between	O
exploration	O
and	O
exploitation	O
in	O
any	O
specific	O
case	O
whether	O
it	O
is	O
better	O
to	O
explore	O
or	O
exploit	O
depends	O
in	O
a	O
complex	O
way	O
on	O
the	O
precise	O
values	O
of	O
the	O
estimates	O
uncertainties	O
and	O
the	O
number	O
of	O
remaining	O
steps	O
there	O
are	O
many	O
sophisticated	O
methods	O
for	O
balancing	O
exploration	O
and	O
exploitation	O
for	O
particular	O
mathematical	O
formulations	O
of	O
the	O
k-armed	O
bandit	O
and	O
related	O
problems	O
action-value	B
methods	I
however	O
most	O
of	O
these	O
methods	O
make	O
strong	O
assumptions	O
about	O
stationarity	O
and	O
prior	B
knowledge	I
that	O
are	O
either	O
violated	O
or	O
impossible	O
to	O
verify	O
in	O
applications	O
and	O
in	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
we	O
consider	O
in	O
subsequent	O
chapters	O
the	O
guarantees	O
of	O
optimality	O
or	O
bounded	O
loss	O
for	O
these	O
methods	O
are	O
of	O
little	O
comfort	O
when	O
the	O
assumptions	O
of	O
their	O
theory	O
do	O
not	O
apply	O
in	O
this	O
book	O
we	O
do	O
not	O
worry	O
about	O
balancing	O
exploration	O
and	O
exploitation	O
in	O
a	O
sophisticated	O
way	O
we	O
worry	O
only	O
about	O
balancing	O
them	O
at	O
all	O
in	O
this	O
chapter	O
we	O
present	O
several	O
simple	O
balancing	O
methods	O
for	O
the	O
k-armed	O
bandit	O
problem	O
and	O
show	O
that	O
they	O
work	O
much	O
better	O
than	O
methods	O
that	O
always	O
exploit	O
the	O
need	O
to	O
balance	O
exploration	O
and	O
exploitation	O
is	O
a	O
distinctive	O
challenge	O
that	O
arises	O
in	O
reinforcement	B
learning	I
the	O
simplicity	O
of	O
our	O
version	O
of	O
the	O
k-armed	O
bandit	O
problem	O
enables	O
us	O
to	O
show	O
this	O
in	O
a	O
particularly	O
clear	O
form	O
action-value	B
methods	I
we	O
begin	O
by	O
looking	O
more	O
closely	O
at	O
methods	O
for	O
estimating	O
the	O
values	O
of	O
actions	O
and	O
for	O
using	O
the	O
estimates	O
to	O
make	O
action	B
selection	O
decisions	O
which	O
we	O
collectively	O
call	O
actionvalue	O
methods	O
recall	O
that	O
the	O
true	O
value	B
of	O
an	O
action	B
is	O
the	O
mean	O
reward	O
when	O
that	O
action	B
is	O
selected	O
one	O
natural	O
way	O
to	O
estimate	O
this	O
is	O
by	O
averaging	O
the	O
rewards	O
actually	O
received	O
qta	O
sum	O
of	O
rewards	O
when	O
a	O
taken	O
prior	O
to	O
t	O
number	O
of	O
times	O
a	O
taken	O
prior	O
to	O
t	O
ri	O
where	O
denotes	O
the	O
random	O
variable	O
that	O
is	O
if	O
predicate	O
is	O
true	O
and	O
if	O
it	O
is	O
not	O
if	O
the	O
denominator	O
is	O
zero	O
then	O
we	O
instead	O
define	O
qta	O
as	O
some	O
default	O
value	B
such	O
as	O
as	O
the	O
denominator	O
goes	O
to	O
infinity	O
by	O
the	O
law	O
of	O
large	O
numbers	O
qta	O
converges	O
to	O
q	O
we	O
call	O
this	O
the	O
sample-average	B
method	I
for	O
estimating	O
action	B
values	O
because	O
each	O
estimate	O
is	O
an	O
average	O
of	O
the	O
sample	O
of	O
relevant	O
rewards	O
of	O
course	O
this	O
is	O
just	O
one	O
way	O
to	O
estimate	O
action	B
values	O
and	O
not	O
necessarily	O
the	O
best	O
one	O
nevertheless	O
for	O
now	O
let	O
us	O
stay	O
with	O
this	O
simple	O
estimation	O
method	O
and	O
turn	O
to	O
the	O
question	O
of	O
how	O
the	O
estimates	O
might	O
be	O
used	O
to	O
select	O
actions	O
the	O
simplest	O
action	B
selection	O
rule	O
is	O
to	O
select	O
one	O
of	O
the	O
actions	O
with	O
the	O
highest	O
estimated	O
value	B
that	O
is	O
one	O
of	O
the	O
greedy	O
actions	O
as	O
defined	O
in	O
the	O
previous	O
section	O
if	O
there	O
is	O
more	O
than	O
one	O
greedy	O
action	B
then	O
a	O
selection	O
is	O
made	O
among	O
them	O
in	O
some	O
arbitrary	O
way	O
perhaps	O
randomly	O
we	O
write	O
this	O
greedy	O
action	B
selection	O
method	O
as	O
at	O
argmax	O
a	O
qta	O
where	O
argmaxa	O
denotes	O
the	O
action	B
a	O
for	O
which	O
the	O
expression	O
that	O
follows	O
is	O
maximized	O
with	O
ties	O
broken	O
arbitrarily	O
greedy	O
action	B
selection	O
always	O
exploits	O
current	O
knowledge	O
to	O
maximize	O
immediate	O
reward	O
it	O
spends	O
no	O
time	O
at	O
all	O
sampling	O
apparently	O
inferior	O
actions	O
to	O
see	O
if	O
they	O
might	O
really	O
be	O
better	O
a	O
simple	O
alternative	O
is	O
to	O
behave	O
greedily	O
most	O
of	O
the	O
time	O
but	O
every	O
once	O
in	O
a	O
while	O
say	O
with	O
small	O
probability	O
instead	O
select	O
randomly	O
from	O
among	O
all	O
the	O
actions	O
with	O
equal	O
probability	O
independently	O
chapter	O
multi-armed	B
bandits	I
of	O
the	O
action-value	O
estimates	O
we	O
call	O
methods	O
using	O
this	O
near-greedy	O
action	B
selection	O
rule	O
methods	O
an	O
advantage	O
of	O
these	O
methods	O
is	O
that	O
in	O
the	O
limit	O
as	O
the	O
number	O
of	O
steps	O
increases	O
every	O
action	B
will	O
be	O
sampled	O
an	O
infinite	O
number	O
of	O
times	O
thus	O
ensuring	O
that	O
all	O
the	O
qta	O
converge	O
to	O
q	O
this	O
of	O
course	O
implies	O
that	O
the	O
probability	O
of	O
selecting	O
the	O
optimal	O
action	B
converges	O
to	O
greater	O
than	O
that	O
is	O
to	O
near	O
certainty	O
these	O
are	O
just	O
asymptotic	O
guarantees	O
however	O
and	O
say	O
little	O
about	O
the	O
practical	O
effectiveness	O
of	O
the	O
methods	O
exercise	O
in	O
action	B
selection	O
for	O
the	O
case	O
of	O
two	O
actions	O
and	O
what	O
is	O
the	O
probability	O
that	O
the	O
greedy	O
action	B
is	O
selected	O
the	O
testbed	O
to	O
roughly	O
assess	O
the	O
relative	O
effectiveness	O
of	O
the	O
greedy	O
and	O
action-value	B
methods	I
we	O
compared	O
them	O
numerically	O
on	O
a	O
suite	O
of	O
test	O
problems	O
this	O
was	O
a	O
set	O
of	O
randomly	O
generated	O
k-armed	O
bandit	B
problems	I
with	O
k	O
for	O
each	O
bandit	O
problem	O
such	O
as	O
the	O
one	O
shown	O
in	O
figure	O
the	O
action	B
values	O
q	O
a	O
were	O
selected	O
according	O
to	O
a	O
normal	O
distribution	O
with	O
mean	O
and	O
variance	O
then	O
when	O
figure	O
an	O
example	O
bandit	O
problem	O
from	O
the	O
testbed	O
the	O
true	O
value	B
q	O
of	O
each	O
of	O
the	O
ten	O
actions	O
was	O
selected	O
according	O
to	O
a	O
normal	O
distribution	O
with	O
mean	O
zero	O
and	O
unit	O
variance	O
and	O
then	O
the	O
actual	O
rewards	O
were	O
selected	O
according	O
to	O
a	O
mean	O
q	O
unit	O
variance	O
normal	O
distribution	O
as	O
suggested	O
by	O
these	O
gray	O
distributions	O
the	O
testbed	O
a	O
learning	O
method	O
applied	O
to	O
that	O
problem	O
selected	O
action	B
at	O
at	O
time	O
step	O
t	O
the	O
actual	O
reward	O
rt	O
was	O
selected	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
q	O
and	O
variance	O
these	O
distributions	O
are	O
shown	O
in	O
gray	O
in	O
figure	O
we	O
call	O
this	O
suite	O
of	O
test	O
tasks	O
the	O
testbed	O
for	O
any	O
learning	O
method	O
we	O
can	O
measure	O
its	O
performance	O
and	O
behavior	O
as	O
it	O
improves	O
with	O
experience	O
over	O
time	O
steps	O
when	O
applied	O
to	O
one	O
of	O
the	O
bandit	B
problems	I
this	O
makes	O
up	O
one	O
run	O
repeating	O
this	O
for	O
independent	O
runs	O
each	O
with	O
a	O
different	O
bandit	O
problem	O
we	O
obtained	O
measures	O
of	O
the	O
learning	O
algorithm	O
s	O
average	O
behavior	O
figure	O
compares	O
a	O
greedy	O
method	O
with	O
two	O
methods	O
and	O
as	O
described	O
above	O
on	O
the	O
testbed	O
all	O
the	O
methods	O
formed	O
their	O
action-value	O
estimates	O
using	O
the	O
sample-average	O
technique	O
the	O
upper	O
graph	O
shows	O
the	O
increase	O
in	O
expected	B
reward	O
with	O
experience	O
the	O
greedy	O
method	O
improved	O
slightly	O
faster	O
than	O
the	O
other	O
methods	O
at	O
the	O
very	O
beginning	O
but	O
then	O
leveled	O
off	O
at	O
a	O
lower	O
level	O
it	O
achieved	O
a	O
reward-per-step	O
of	O
only	O
about	O
compared	O
with	O
the	O
best	O
possible	O
of	O
about	O
on	O
this	O
testbed	O
the	O
greedy	O
method	O
performed	O
significantly	O
worse	O
in	O
the	O
long	O
run	O
because	O
it	O
often	O
got	O
stuck	O
performing	O
suboptimal	O
actions	O
the	O
lower	O
graph	O
shows	O
that	O
the	O
greedy	O
figure	O
average	O
performance	O
of	O
action-value	B
methods	I
on	O
the	O
testbed	O
these	O
data	O
are	O
averages	O
over	O
runs	O
with	O
different	O
bandit	B
problems	I
all	O
methods	O
used	O
sample	O
averages	O
as	O
their	O
action-value	O
estimates	O
chapter	O
multi-armed	B
bandits	I
method	O
found	O
the	O
optimal	O
action	B
in	O
only	O
approximately	O
one-third	O
of	O
the	O
tasks	O
in	O
the	O
other	O
two-thirds	O
its	O
initial	O
samples	O
of	O
the	O
optimal	O
action	B
were	O
disappointing	O
and	O
it	O
never	O
returned	O
to	O
it	O
the	O
methods	O
eventually	O
performed	O
better	O
because	O
they	O
continued	O
to	O
explore	O
and	O
to	O
improve	O
their	O
chances	O
of	O
recognizing	O
the	O
optimal	O
action	B
the	O
method	O
explored	O
more	O
and	O
usually	O
found	O
the	O
optimal	O
action	B
earlier	O
but	O
it	O
never	O
selected	O
that	O
action	B
more	O
than	O
of	O
the	O
time	O
the	O
method	O
improved	O
more	O
slowly	O
but	O
eventually	O
would	O
perform	O
better	O
than	O
the	O
method	O
on	O
both	O
performance	O
measures	O
shown	O
in	O
the	O
figure	O
it	O
is	O
also	O
possible	O
to	O
reduce	O
over	O
time	O
to	O
try	O
to	O
get	O
the	O
best	O
of	O
both	O
high	O
and	O
low	O
values	O
the	O
advantage	O
of	O
over	O
greedy	O
methods	O
depends	O
on	O
the	O
task	O
for	O
example	O
suppose	O
the	O
reward	O
variance	O
had	O
been	O
larger	O
say	O
instead	O
of	O
with	O
noisier	O
rewards	O
it	O
takes	O
more	O
exploration	O
to	O
find	O
the	O
optimal	O
action	B
and	O
methods	O
should	O
fare	O
even	O
better	O
relative	O
to	O
the	O
greedy	O
method	O
on	O
the	O
other	O
hand	O
if	O
the	O
reward	O
variances	O
were	O
zero	O
then	O
the	O
greedy	O
method	O
would	O
know	O
the	O
true	O
value	B
of	O
each	O
action	B
after	O
trying	O
it	O
once	O
in	O
this	O
case	O
the	O
greedy	O
method	O
might	O
actually	O
perform	O
best	O
because	O
it	O
would	O
soon	O
find	O
the	O
optimal	O
action	B
and	O
then	O
never	O
explore	O
but	O
even	O
in	O
the	O
deterministic	O
case	O
there	O
is	O
a	O
large	O
advantage	O
to	O
exploring	O
if	O
we	O
weaken	O
some	O
of	O
the	O
other	O
assumptions	O
for	O
example	O
suppose	O
the	O
bandit	O
task	O
were	O
nonstationary	O
that	O
is	O
the	O
true	O
values	O
of	O
the	O
actions	O
changed	O
over	O
time	O
in	O
this	O
case	O
exploration	O
is	O
needed	O
even	O
in	O
the	O
deterministic	O
case	O
to	O
make	O
sure	O
one	O
of	O
the	O
nongreedy	O
actions	O
has	O
not	O
changed	O
to	O
become	O
better	O
than	O
the	O
greedy	O
one	O
as	O
we	O
shall	O
see	O
in	O
the	O
next	O
few	O
chapters	O
nonstationarity	B
is	O
the	O
case	O
most	O
commonly	O
encountered	O
in	O
reinforcement	B
learning	I
even	O
if	O
the	O
underlying	O
task	O
is	O
stationary	O
and	O
deterministic	O
the	O
learner	O
faces	O
a	O
set	O
of	O
banditlike	O
decision	O
tasks	O
each	O
of	O
which	O
changes	O
over	O
time	O
as	O
learning	O
proceeds	O
and	O
the	O
agent	O
s	O
decision-making	O
policy	B
changes	O
reinforcement	B
learning	I
requires	O
a	O
balance	O
between	O
exploration	O
and	O
exploitation	O
exercise	O
bandit	O
example	O
consider	O
a	O
k-armed	O
bandit	O
problem	O
with	O
k	O
actions	O
denoted	O
and	O
consider	O
applying	O
to	O
this	O
problem	O
a	O
bandit	B
algorithm	I
using	O
action	B
selection	O
sample-average	O
action-value	O
estimates	O
and	O
initial	O
estimates	O
of	O
for	O
all	O
a	O
suppose	O
the	O
initial	O
sequence	O
of	O
actions	O
and	O
rewards	O
is	O
on	O
some	O
of	O
these	O
time	O
steps	O
the	O
case	O
may	O
have	O
occurred	O
causing	O
an	O
action	B
to	O
be	O
selected	O
at	O
random	O
on	O
which	O
time	O
steps	O
did	O
this	O
definitely	O
occur	O
on	O
which	O
time	O
steps	O
could	O
this	O
possibly	O
have	O
occurred	O
exercise	O
in	O
the	O
comparison	O
shown	O
in	O
figure	O
which	O
method	O
will	O
perform	O
best	O
in	O
the	O
long	O
run	O
in	O
terms	O
of	O
cumulative	O
reward	O
and	O
probability	O
of	O
selecting	O
the	O
best	O
action	B
how	O
much	O
better	O
will	O
it	O
be	O
express	O
your	O
answer	O
quantitatively	O
incremental	B
implementation	I
the	O
action-value	B
methods	I
we	O
have	O
discussed	O
so	O
far	O
all	O
estimate	O
action	B
values	O
as	O
sample	O
averages	O
of	O
observed	O
rewards	O
we	O
now	O
turn	O
to	O
the	O
question	O
of	O
how	O
these	O
averages	O
can	O
be	O
computed	O
in	O
a	O
computationally	O
efficient	O
manner	O
in	O
particular	O
with	O
constant	O
memory	O
and	O
constant	O
per-time-step	O
computation	O
incremental	B
implementation	I
to	O
simplify	O
notation	O
we	O
concentrate	O
on	O
a	O
single	O
action	B
let	O
ri	O
now	O
denote	O
the	O
reward	O
received	O
after	O
the	O
ith	O
selection	O
of	O
this	O
action	B
and	O
let	O
qn	O
denote	O
the	O
estimate	O
of	O
its	O
action	B
value	B
after	O
it	O
has	O
been	O
selected	O
n	O
times	O
which	O
we	O
can	O
now	O
write	O
simply	O
as	O
qn	O
rn	O
n	O
the	O
obvious	O
implementation	O
would	O
be	O
to	O
maintain	O
a	O
record	O
of	O
all	O
the	O
rewards	O
and	O
then	O
perform	O
this	O
computation	O
whenever	O
the	O
estimated	O
value	B
was	O
needed	O
however	O
if	O
this	O
is	O
done	O
then	O
the	O
memory	O
and	O
computational	O
requirements	O
would	O
grow	O
over	O
time	O
as	O
more	O
rewards	O
are	O
seen	O
each	O
additional	O
reward	O
would	O
require	O
additional	O
memory	O
to	O
store	O
it	O
and	O
additional	O
computation	O
to	O
compute	O
the	O
sum	O
in	O
the	O
numerator	O
as	O
you	O
might	O
suspect	O
this	O
is	O
not	O
really	O
necessary	O
it	O
is	O
easy	O
to	O
devise	O
incremental	O
formulas	O
for	O
updating	O
averages	O
with	O
small	O
constant	O
computation	O
required	O
to	O
process	O
each	O
new	O
reward	O
given	O
qn	O
and	O
the	O
nth	O
reward	O
rn	O
the	O
new	O
average	O
of	O
all	O
n	O
rewards	O
can	O
be	O
computed	O
by	O
ri	O
n	O
n	O
nqn	O
n	O
n	O
qn	O
which	O
holds	O
even	O
for	O
n	O
obtaining	O
for	O
arbitrary	O
this	O
implementation	O
requires	O
memory	O
only	O
for	O
qn	O
and	O
n	O
and	O
only	O
the	O
small	O
computation	O
for	O
each	O
new	O
reward	O
this	O
update	O
rule	O
is	O
of	O
a	O
form	O
that	O
occurs	O
frequently	O
throughout	O
this	O
book	O
the	O
general	O
form	O
is	O
newestimate	O
oldestimate	O
the	O
is	O
an	O
error	O
in	O
the	O
estimate	O
it	O
is	O
reduced	O
by	O
taking	O
a	O
step	O
toward	O
the	O
target	O
the	O
target	O
is	O
presumed	O
to	O
indicate	O
a	O
desirable	O
direction	O
in	O
which	O
to	O
move	O
though	O
it	O
may	O
be	O
noisy	O
in	O
the	O
case	O
above	O
for	O
example	O
the	O
target	O
is	O
the	O
nth	O
reward	O
note	O
that	O
the	O
step-size	B
parameter	I
used	O
in	O
the	O
incremental	O
method	O
in	O
processing	O
the	O
nth	O
reward	O
for	O
action	B
a	O
the	O
changes	O
from	O
time	O
step	O
to	O
time	O
step	O
chapter	O
multi-armed	B
bandits	I
method	O
uses	O
the	O
step-size	B
parameter	I
by	O
or	O
more	O
generally	O
by	O
ta	O
n	O
in	O
this	O
book	O
we	O
denote	O
the	O
step-size	B
parameter	I
pseudocode	O
for	O
a	O
complete	O
bandit	B
algorithm	I
using	O
incrementally	O
computed	O
sample	O
averages	O
and	O
action	B
selection	O
is	O
shown	O
in	O
the	O
box	O
below	O
the	O
function	O
bandita	O
is	O
assumed	O
to	O
take	O
an	O
action	B
and	O
return	B
a	O
corresponding	O
reward	O
a	O
simple	O
bandit	B
algorithm	I
initialize	O
for	O
a	O
to	O
k	O
qa	O
n	O
loop	O
forever	O
a	O
arg	O
maxa	O
qa	O
r	O
bandita	O
n	O
n	O
qa	O
qa	O
n	O
a	O
random	O
action	B
with	O
probability	O
with	O
probability	O
ties	O
randomly	O
tracking	O
a	O
nonstationary	O
problem	O
the	O
averaging	O
methods	O
discussed	O
so	O
far	O
are	O
appropriate	O
for	O
stationary	O
bandit	B
problems	I
that	O
is	O
for	B
bandit	B
problems	I
in	O
which	O
the	O
reward	O
probabilities	O
do	O
not	O
change	O
over	O
time	O
as	O
noted	O
earlier	O
we	O
often	O
encounter	O
reinforcement	B
learning	I
problems	O
that	O
are	O
effectively	O
nonstationary	O
in	O
such	O
cases	O
it	O
makes	O
sense	O
to	O
give	O
more	O
weight	O
to	O
recent	O
rewards	O
than	O
to	O
long-past	O
rewards	O
one	O
of	O
the	O
most	O
popular	O
ways	O
of	O
doing	O
this	O
is	O
to	O
use	O
a	O
constant	O
step-size	B
parameter	I
for	O
example	O
the	O
incremental	O
update	O
rule	O
for	O
updating	O
an	O
average	O
qn	O
of	O
the	O
n	O
past	O
rewards	O
is	O
modified	O
to	O
be	O
where	O
the	O
step-size	B
parameter	I
is	O
constant	O
this	O
results	O
in	O
being	O
a	O
weighted	O
average	O
of	O
past	O
rewards	O
and	O
the	O
initial	O
estimate	O
qn	O
qn	O
rn	O
rn	O
rn	O
rn	O
rn	O
rn	O
rn	O
rn	O
iri	O
optimistic	B
initial	I
values	I
we	O
call	O
this	O
a	O
weighted	O
average	O
because	O
the	O
sum	O
of	O
the	O
weights	O
is	O
i	O
as	O
you	O
can	O
check	O
for	O
yourself	O
note	O
that	O
the	O
weight	O
i	O
given	O
to	O
the	O
reward	O
ri	O
depends	O
on	O
how	O
many	O
rewards	O
ago	O
n	O
i	O
it	O
was	O
observed	O
the	O
quantity	O
is	O
less	O
than	O
and	O
thus	O
the	O
weight	O
given	O
to	O
ri	O
decreases	O
as	O
the	O
number	O
of	O
intervening	O
rewards	O
increases	O
in	O
fact	O
the	O
weight	O
decays	O
exponentially	O
according	O
to	O
the	O
exponent	O
on	O
then	O
all	O
the	O
weight	O
goes	O
on	O
the	O
very	O
last	O
reward	O
rn	O
because	O
of	O
the	O
convention	O
that	O
accordingly	O
this	O
is	O
sometimes	O
called	O
an	O
exponential	O
recency-weighted	O
average	O
sometimes	O
it	O
is	O
convenient	O
to	O
vary	O
the	O
step-size	B
parameter	I
from	O
step	O
to	O
step	O
let	O
na	O
denote	O
the	O
step-size	B
parameter	I
used	O
to	O
process	O
the	O
reward	O
received	O
after	O
the	O
nth	O
selection	O
of	O
action	B
a	O
as	O
we	O
have	O
noted	O
the	O
choice	O
na	O
n	O
results	O
in	O
the	O
sampleaverage	O
method	O
which	O
is	O
guaranteed	O
to	O
converge	O
to	O
the	O
true	O
action	B
values	O
by	O
the	O
law	O
of	O
large	O
numbers	O
but	O
of	O
course	O
convergence	O
is	O
not	O
guaranteed	O
for	O
all	O
choices	O
of	O
the	O
sequence	O
na	O
a	O
well-known	O
result	O
in	O
stochastic	O
approximation	O
theory	O
gives	O
us	O
the	O
conditions	O
required	O
to	O
assure	O
convergence	O
with	O
probability	O
na	O
and	O
na	O
the	O
first	O
condition	O
is	O
required	O
to	O
guarantee	O
that	O
the	O
steps	O
are	O
large	O
enough	O
to	O
eventually	O
overcome	O
any	O
initial	O
conditions	O
or	O
random	O
fluctuations	O
the	O
second	O
condition	O
guarantees	O
that	O
eventually	O
the	O
steps	O
become	O
small	O
enough	O
to	O
assure	O
convergence	O
note	O
that	O
both	O
convergence	O
conditions	O
are	O
met	O
for	O
the	O
sample-average	O
case	O
na	O
n	O
but	O
not	O
for	O
the	O
case	O
of	O
constant	O
step-size	B
parameter	I
na	O
in	O
the	O
latter	O
case	O
the	O
second	O
condition	O
is	O
not	O
met	O
indicating	O
that	O
the	O
estimates	O
never	O
completely	O
converge	O
but	O
continue	O
to	O
vary	O
in	O
response	O
to	O
the	O
most	O
recently	O
received	O
rewards	O
as	O
we	O
mentioned	O
above	O
this	O
is	O
actually	O
desirable	O
in	O
a	O
nonstationary	O
environment	B
and	O
problems	O
that	O
are	O
effectively	O
nonstationary	O
are	O
the	O
most	O
common	O
in	O
reinforcement	B
learning	I
in	O
addition	O
sequences	O
of	O
step-size	O
parameters	O
that	O
meet	O
the	O
conditions	O
often	O
converge	O
very	O
slowly	O
or	O
need	O
considerable	O
tuning	O
in	O
order	O
to	O
obtain	O
a	O
satisfactory	O
convergence	O
rate	O
although	O
sequences	O
of	O
step-size	O
parameters	O
that	O
meet	O
these	O
convergence	O
conditions	O
are	O
often	O
used	O
in	O
theoretical	O
work	O
they	O
are	O
seldom	O
used	O
in	O
applications	O
and	O
empirical	O
research	O
exercise	O
if	O
the	O
step-size	O
parameters	O
n	O
are	O
not	O
constant	O
then	O
the	O
estimate	O
qn	O
is	O
a	O
weighted	O
average	O
of	O
previously	O
received	O
rewards	O
with	O
a	O
weighting	O
different	O
from	O
that	O
given	O
by	O
what	O
is	O
the	O
weighting	O
on	O
each	O
prior	O
reward	O
for	O
the	O
general	O
case	O
analogous	O
to	O
in	O
terms	O
of	O
the	O
sequence	O
of	O
step-size	O
parameters	O
exercise	O
design	O
and	O
conduct	O
an	O
experiment	O
to	O
demonstrate	O
the	O
difficulties	O
that	O
sample-average	O
methods	O
have	O
for	O
nonstationary	O
problems	O
use	O
a	O
modified	O
version	O
of	O
the	O
testbed	O
in	O
which	O
all	O
the	O
q	O
start	O
out	O
equal	O
and	O
then	O
take	O
independent	O
random	O
walks	O
by	O
adding	O
a	O
normally	O
distributed	O
increment	O
with	O
mean	O
zero	O
and	O
standard	O
deviation	O
to	O
all	O
the	O
q	O
on	O
each	O
step	O
prepare	O
plots	O
like	O
figure	O
for	O
an	O
action-value	O
method	O
using	O
sample	O
averages	O
incrementally	O
computed	O
and	O
another	O
action-value	O
method	O
using	O
a	O
constant	O
step-size	B
parameter	I
use	O
and	O
longer	O
runs	O
say	O
of	O
steps	O
chapter	O
multi-armed	B
bandits	I
optimistic	B
initial	I
values	I
all	O
the	O
methods	O
we	O
have	O
discussed	O
so	O
far	O
are	O
dependent	O
to	O
some	O
extent	O
on	O
the	O
initial	O
action-value	O
estimates	O
in	O
the	O
language	O
of	O
statistics	O
these	O
methods	O
are	O
biased	O
by	O
their	O
initial	O
estimates	O
for	O
the	O
sample-average	O
methods	O
the	O
bias	O
disappears	O
once	O
all	O
actions	O
have	O
been	O
selected	O
at	O
least	O
once	O
but	O
for	O
methods	O
with	O
constant	O
the	O
bias	O
is	O
permanent	O
though	O
decreasing	O
over	O
time	O
as	O
given	O
by	O
in	O
practice	O
this	O
kind	O
of	O
bias	O
is	O
usually	O
not	O
a	O
problem	O
and	O
can	O
sometimes	O
be	O
very	O
helpful	O
the	O
downside	O
is	O
that	O
the	O
initial	O
estimates	O
become	O
in	O
effect	O
a	O
set	O
of	O
parameters	O
that	O
must	O
be	O
picked	O
by	O
the	O
user	O
if	O
only	O
to	O
set	O
them	O
all	O
to	O
zero	O
the	O
upside	O
is	O
that	O
they	O
provide	O
an	O
easy	O
way	O
to	O
supply	O
some	O
prior	B
knowledge	I
about	O
what	O
level	O
of	O
rewards	O
can	O
be	O
expected	B
initial	O
action	B
values	O
can	O
also	O
be	O
used	O
as	O
a	O
simple	O
way	O
to	O
encourage	O
exploration	O
suppose	O
that	O
instead	O
of	O
setting	O
the	O
initial	O
action	B
values	O
to	O
zero	O
as	O
we	O
did	O
in	O
the	O
testbed	O
we	O
set	O
them	O
all	O
to	O
recall	O
that	O
the	O
q	O
in	O
this	O
problem	O
are	O
selected	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
and	O
variance	O
an	O
initial	O
estimate	O
of	O
is	O
thus	O
wildly	O
optimistic	O
but	O
this	O
optimism	O
encourages	O
action-value	B
methods	I
to	O
explore	O
whichever	O
actions	O
are	O
initially	O
selected	O
the	O
reward	O
is	O
less	O
than	O
the	O
starting	O
estimates	O
the	O
learner	O
switches	O
to	O
other	O
actions	O
being	O
disappointed	O
with	O
the	O
rewards	O
it	O
is	O
receiving	O
the	O
result	O
is	O
that	O
all	O
actions	O
are	O
tried	O
several	O
times	O
before	O
the	O
value	B
estimates	O
converge	O
the	O
system	O
does	O
a	O
fair	O
amount	O
of	O
exploration	O
even	O
if	O
greedy	O
actions	O
are	O
selected	O
all	O
the	O
time	O
figure	O
shows	O
the	O
performance	O
on	O
the	O
bandit	O
testbed	O
of	O
a	O
greedy	O
method	O
using	O
for	O
all	O
a	O
for	O
comparison	O
also	O
shown	O
is	O
an	O
method	O
with	O
initially	O
the	O
optimistic	O
method	O
performs	O
worse	O
because	O
it	O
explores	O
more	O
but	O
eventually	O
it	O
performs	O
better	O
because	O
its	O
exploration	O
decreases	O
with	O
time	O
we	O
call	O
this	O
technique	O
for	O
encouraging	O
exploration	O
optimistic	B
initial	I
values	I
we	O
regard	O
it	O
as	O
a	O
simple	O
trick	O
that	O
can	O
be	O
quite	O
effective	O
on	O
stationary	O
problems	O
but	O
it	O
is	O
far	O
from	O
being	O
a	O
generally	O
useful	O
approach	O
to	O
encouraging	O
exploration	O
for	O
example	O
it	O
is	O
not	O
well	O
suited	O
to	O
nonstationary	O
problems	O
because	O
its	O
drive	O
for	O
exploration	O
is	O
inherently	O
figure	O
the	O
effect	O
of	O
optimistic	O
initial	O
action-value	O
estimates	O
on	O
the	O
testbed	O
both	O
methods	O
used	O
a	O
constant	O
step-size	B
parameter	I
upper-confidence-bound	O
action	B
selection	O
temporary	O
if	O
the	O
task	O
changes	O
creating	O
a	O
renewed	O
need	O
for	O
exploration	O
this	O
method	O
cannot	O
help	O
indeed	O
any	O
method	O
that	O
focuses	O
on	O
the	O
initial	O
conditions	O
in	O
any	O
special	O
way	O
is	O
unlikely	O
to	O
help	O
with	O
the	O
general	O
nonstationary	O
case	O
the	O
beginning	O
of	O
time	O
occurs	O
only	O
once	O
and	O
thus	O
we	O
should	O
not	O
focus	O
on	O
it	O
too	O
much	O
this	O
criticism	O
applies	O
as	O
well	O
to	O
the	O
sample-average	O
methods	O
which	O
also	O
treat	O
the	O
beginning	O
of	O
time	O
as	O
a	O
special	O
event	O
averaging	O
all	O
subsequent	O
rewards	O
with	O
equal	O
weights	O
nevertheless	O
all	O
of	O
these	O
methods	O
are	O
very	O
simple	O
and	O
one	O
of	O
them	O
or	O
some	O
simple	O
combination	O
of	O
them	O
is	O
often	O
adequate	O
in	O
practice	O
in	O
the	O
rest	O
of	O
this	O
book	O
we	O
make	O
frequent	O
use	O
of	O
several	O
of	O
these	O
simple	O
exploration	O
techniques	O
exercise	O
mysterious	O
spikes	O
the	O
results	O
shown	O
in	O
figure	O
should	O
be	O
quite	O
reliable	O
because	O
they	O
are	O
averages	O
over	O
individual	O
randomly	O
chosen	O
bandit	O
tasks	O
why	O
then	O
are	O
there	O
oscillations	O
and	O
spikes	O
in	O
the	O
early	O
part	O
of	O
the	O
curve	O
for	O
the	O
optimistic	O
method	O
in	O
other	O
words	O
what	O
might	O
make	O
this	O
method	O
perform	O
particularly	O
better	O
or	O
worse	O
on	O
average	O
on	O
particular	O
early	O
steps	O
exercise	O
unbiased	O
constant-step-size	O
trick	O
in	O
most	O
of	O
this	O
chapter	O
we	O
have	O
used	O
sample	O
averages	O
to	O
estimate	O
action	B
values	O
because	O
sample	O
averages	O
do	O
not	O
produce	O
the	O
initial	O
bias	O
that	O
constant	O
step	O
sizes	O
do	O
the	O
analysis	O
in	O
however	O
sample	O
averages	O
are	O
not	O
a	O
completely	O
satisfactory	O
solution	O
because	O
they	O
may	O
perform	O
poorly	O
on	O
nonstationary	O
problems	O
is	O
it	O
possible	O
to	O
avoid	O
the	O
bias	O
of	O
constant	O
step	O
sizes	O
while	O
retaining	O
their	O
advantages	O
on	O
nonstationary	O
problems	O
one	O
way	O
is	O
to	O
use	O
a	O
step	O
size	O
of	O
ot	O
t	O
where	O
is	O
a	O
conventional	O
constant	O
step	O
size	O
and	O
ot	O
is	O
a	O
trace	O
of	O
one	O
that	O
starts	O
at	O
ot	O
ot	O
for	O
t	O
with	O
carry	O
out	O
an	O
analysis	O
like	O
that	O
in	O
to	O
show	O
that	O
t	O
is	O
an	O
exponential	O
recency-weighted	O
average	O
without	O
initial	O
bias	O
upper-confidence-bound	O
action	B
selection	O
exploration	O
is	O
needed	O
because	O
there	O
is	O
always	O
uncertainty	O
about	O
the	O
accuracy	O
of	O
the	O
action-value	O
estimates	O
the	O
greedy	O
actions	O
are	O
those	O
that	O
look	O
best	O
at	O
present	O
but	O
some	O
of	O
the	O
other	O
actions	O
may	O
actually	O
be	O
better	O
action	B
selection	O
forces	O
the	O
non-greedy	O
actions	O
to	O
be	O
tried	O
but	O
indiscriminately	O
with	O
no	O
preference	O
for	O
those	O
that	O
are	O
nearly	O
greedy	O
or	O
particularly	O
uncertain	O
it	O
would	O
be	O
better	O
to	O
select	O
among	O
the	O
non-greedy	O
actions	O
according	O
to	O
their	O
potential	O
for	O
actually	O
being	O
optimal	O
taking	O
into	O
account	O
both	O
how	O
close	O
their	O
estimates	O
are	O
to	O
being	O
maximal	O
and	O
the	O
uncertainties	O
in	O
those	O
estimates	O
one	O
effective	O
way	O
of	O
doing	O
this	O
is	O
to	O
select	O
actions	O
according	O
to	O
at	O
argmax	O
a	O
ln	O
t	O
chapter	O
multi-armed	B
bandits	I
where	O
ln	O
t	O
denotes	O
the	O
natural	O
logarithm	O
of	O
t	O
number	O
that	O
e	O
would	O
have	O
to	O
be	O
raised	O
to	O
in	O
order	O
to	O
equal	O
t	O
nta	O
denotes	O
the	O
number	O
of	O
times	O
that	O
action	B
a	O
has	O
been	O
selected	O
prior	O
to	O
time	O
t	O
denominator	O
in	O
and	O
the	O
number	O
c	O
controls	O
the	O
degree	O
of	O
exploration	O
if	O
nta	O
then	O
a	O
is	O
considered	O
to	O
be	O
a	O
maximizing	O
action	B
the	O
idea	O
of	O
this	O
upper	O
confidence	O
bound	O
action	B
selection	O
is	O
that	O
the	O
square-root	O
term	O
is	O
a	O
measure	O
of	O
the	O
uncertainty	O
or	O
variance	O
in	O
the	O
estimate	O
of	O
a	O
s	O
value	B
the	O
quantity	O
being	O
max	O
ed	O
over	O
is	O
thus	O
a	O
sort	O
of	O
upper	O
bound	O
on	O
the	O
possible	O
true	O
value	B
of	O
action	B
a	O
with	O
c	O
determining	O
the	O
confidence	O
level	O
each	O
time	O
a	O
is	O
selected	O
the	O
uncertainty	O
is	O
presumably	O
reduced	O
nta	O
increments	O
and	O
as	O
it	O
appears	O
in	O
the	O
denominator	O
the	O
uncertainty	O
term	O
decreases	O
on	O
the	O
other	O
hand	O
each	O
time	O
an	O
action	B
other	O
than	O
a	O
is	O
selected	O
t	O
increases	O
but	O
nta	O
does	O
not	O
because	O
t	O
appears	O
in	O
the	O
numerator	O
the	O
uncertainty	O
estimate	O
increases	O
the	O
use	O
of	O
the	O
natural	O
logarithm	O
means	O
that	O
the	O
increases	O
get	O
smaller	O
over	O
time	O
but	O
are	O
unbounded	O
all	O
actions	O
will	O
eventually	O
be	O
selected	O
but	O
actions	O
with	O
lower	O
value	B
estimates	O
or	O
that	O
have	O
already	O
been	O
selected	O
frequently	O
will	O
be	O
selected	O
with	O
decreasing	O
frequency	O
over	O
time	O
results	O
with	O
ucb	O
on	O
the	O
testbed	O
are	O
shown	O
in	O
figure	O
ucb	O
often	O
performs	O
well	O
as	O
shown	O
here	O
but	O
is	O
more	O
difficult	O
than	O
to	O
extend	O
beyond	O
bandits	O
to	O
the	O
more	O
general	O
reinforcement	B
learning	I
settings	O
considered	O
in	O
the	O
rest	O
of	O
this	O
book	O
one	O
difficulty	O
is	O
in	O
dealing	O
with	O
nonstationary	O
problems	O
methods	O
more	O
complex	O
than	O
those	O
presented	O
in	O
section	O
would	O
be	O
needed	O
another	O
difficulty	O
is	O
dealing	O
with	O
large	O
state	B
spaces	O
particularly	O
when	O
using	O
function	B
approximation	I
as	O
developed	O
in	O
part	O
ii	O
of	O
this	O
book	O
in	O
these	O
more	O
advanced	O
settings	O
the	O
idea	O
of	O
ucb	O
action	B
selection	O
is	O
usually	O
not	O
practical	O
exercise	O
ucb	O
spikes	O
in	O
figure	O
the	O
ucb	O
algorithm	O
shows	O
a	O
distinct	O
spike	O
in	O
performance	O
on	O
the	O
step	O
why	O
is	O
this	O
note	O
that	O
for	O
your	O
answer	O
to	O
be	O
fully	O
satisfactory	O
it	O
must	O
explain	O
both	O
why	O
the	O
reward	O
increases	O
on	O
the	O
step	O
and	O
why	O
it	O
figure	O
average	O
performance	O
of	O
ucb	O
action	B
selection	O
on	O
the	O
testbed	O
as	O
shown	O
ucb	O
generally	O
performs	O
better	O
than	O
action	B
selection	O
except	O
in	O
the	O
first	O
k	O
steps	O
when	O
it	O
selects	O
randomly	O
among	O
the	O
as-yet-untried	O
actions	O
c	O
gradient	B
bandit	O
algorithms	O
decreases	O
on	O
the	O
subsequent	O
steps	O
hint	O
if	O
c	O
then	O
the	O
spike	O
is	O
less	O
prominent	O
gradient	B
bandit	O
algorithms	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
methods	O
that	O
estimate	O
action	B
values	O
and	O
use	O
those	O
estimates	O
to	O
select	O
actions	O
this	O
is	O
often	O
a	O
good	O
approach	O
but	O
it	O
is	O
not	O
the	O
only	O
one	O
possible	O
in	O
this	O
section	O
we	O
consider	O
learning	O
a	O
numerical	O
preference	O
for	O
each	O
action	B
a	O
which	O
we	O
denote	O
hta	O
the	O
larger	O
the	O
preference	O
the	O
more	O
often	O
that	O
action	B
is	O
taken	O
but	O
the	O
preference	O
has	O
no	O
interpretation	O
in	O
terms	O
of	O
reward	O
only	O
the	O
relative	O
preference	O
of	O
one	O
action	B
over	O
another	O
is	O
important	O
if	O
we	O
add	O
to	O
all	O
the	O
action	B
preferences	I
there	O
is	O
no	O
effect	O
on	O
the	O
action	B
probabilities	O
which	O
are	O
determined	O
according	O
to	O
a	O
soft-max	B
distribution	O
gibbs	O
or	O
boltzmann	O
distribution	O
as	O
follows	O
prat	O
a	O
ehta	O
ehtb	O
ta	O
where	O
here	O
we	O
have	O
also	O
introduced	O
a	O
useful	O
new	O
notation	O
ta	O
for	O
the	O
probability	O
of	O
taking	O
action	B
a	O
at	O
time	O
t	O
initially	O
all	O
action	B
preferences	I
are	O
the	O
same	O
for	O
all	O
a	O
so	O
that	O
all	O
actions	O
have	O
an	O
equal	O
probability	O
of	O
being	O
selected	O
exercise	O
show	O
that	O
in	O
the	O
case	O
of	O
two	O
actions	O
the	O
soft-max	B
distribution	O
is	O
the	O
same	O
as	O
that	O
given	O
by	O
the	O
logistic	O
or	O
sigmoid	O
function	O
often	O
used	O
in	O
statistics	O
and	O
artificial	B
neural	B
networks	I
there	O
is	O
a	O
natural	O
learning	O
algorithm	O
for	O
this	O
setting	O
based	O
on	O
the	O
idea	O
of	O
stochastic	O
gradient	B
ascent	O
on	O
each	O
step	O
after	O
selecting	O
action	B
at	O
and	O
receiving	O
the	O
reward	O
rt	O
the	O
action	B
preferences	I
are	O
updated	O
by	O
htat	O
hta	O
ta	O
and	O
for	O
all	O
a	O
at	O
where	O
is	O
a	O
step-size	B
parameter	I
and	O
rt	O
r	O
is	O
the	O
average	O
of	O
all	O
the	O
rewards	O
up	O
through	O
and	O
including	O
time	O
t	O
which	O
can	O
be	O
computed	O
incrementally	O
as	O
described	O
in	O
section	O
section	O
if	O
the	O
problem	O
is	O
nonstationary	O
the	O
rt	O
term	O
serves	O
as	O
a	O
baseline	B
with	O
which	O
the	O
reward	O
is	O
compared	O
if	O
the	O
reward	O
is	O
higher	O
than	O
the	O
baseline	B
then	O
the	O
probability	O
of	O
taking	O
at	O
in	O
the	O
future	O
is	O
increased	O
and	O
if	O
the	O
reward	O
is	O
below	O
baseline	B
then	O
probability	O
is	O
decreased	O
the	O
non-selected	O
actions	O
move	O
in	O
the	O
opposite	O
direction	O
figure	O
shows	O
results	O
with	O
the	O
gradient	B
bandit	B
algorithm	I
on	O
a	O
variant	O
of	O
the	O
testbed	O
in	O
which	O
the	O
true	O
expected	B
rewards	O
were	O
selected	O
according	O
to	O
a	O
normal	O
distribution	O
with	O
a	O
mean	O
of	O
instead	O
of	O
zero	O
with	O
unit	O
variance	O
as	O
before	O
this	O
shifting	O
up	O
of	O
all	O
the	O
rewards	O
has	O
absolutely	O
no	O
effect	O
on	O
the	O
gradient	B
bandit	B
algorithm	I
because	O
of	O
the	O
reward	O
baseline	B
term	O
which	O
instantaneously	O
adapts	O
to	O
the	O
new	O
level	O
but	O
if	O
the	O
baseline	B
were	O
omitted	O
is	O
if	O
rt	O
was	O
taken	O
to	O
be	O
constant	O
zero	O
in	O
then	O
performance	O
would	O
be	O
significantly	O
degraded	O
as	O
shown	O
in	O
the	O
figure	O
chapter	O
multi-armed	B
bandits	I
figure	O
average	O
performance	O
of	O
the	O
gradient	B
bandit	B
algorithm	I
with	O
and	O
without	O
a	O
reward	O
baseline	B
on	O
the	O
testbed	O
when	O
the	O
q	O
are	O
chosen	O
to	O
be	O
near	O
rather	O
than	O
near	O
zero	O
the	O
bandit	O
gradient	B
algorithm	O
as	O
stochastic	O
gradient	B
ascent	O
one	O
can	O
gain	O
a	O
deeper	O
insight	O
into	O
the	O
gradient	B
bandit	B
algorithm	I
by	O
understanding	O
it	O
as	O
a	O
stochastic	O
approximation	O
to	O
gradient	B
ascent	O
in	O
exact	O
gradient	B
ascent	O
each	O
action	B
preference	O
hta	O
would	O
be	O
incremented	O
proportional	O
to	O
the	O
increment	O
s	O
effect	O
on	O
performance	O
hta	O
e	O
hta	O
where	O
the	O
measure	O
of	O
performance	O
here	O
is	O
the	O
expected	B
reward	O
ert	O
txq	O
and	O
the	O
measure	O
of	O
the	O
increment	O
s	O
effect	O
is	O
the	O
partial	O
derivative	O
of	O
this	O
performance	O
measure	O
with	O
respect	O
to	O
the	O
action	B
preference	O
of	O
course	O
it	O
is	O
not	O
possible	O
to	O
implement	O
gradient	B
ascent	O
exactly	O
in	O
our	O
case	O
because	O
by	O
assumption	O
we	O
do	O
not	O
know	O
the	O
q	O
but	O
in	O
fact	O
the	O
updates	O
of	O
our	O
algorithm	O
are	O
equal	O
to	O
in	O
expected	B
value	B
making	O
the	O
algorithm	O
an	O
instance	O
of	O
stochastic	O
gradient	B
ascent	O
the	O
calculations	O
showing	O
this	O
require	O
only	O
beginning	O
calculus	O
but	O
take	O
baselinewith	O
gradient	B
bandit	O
algorithms	O
several	O
steps	O
first	O
we	O
take	O
a	O
closer	O
look	O
at	O
the	O
exact	O
performance	O
gradient	B
ert	O
hta	O
txq	O
tx	O
tx	O
hta	O
q	O
hta	O
where	O
bt	O
called	O
the	O
baseline	B
can	O
be	O
any	O
scalar	O
that	O
does	O
not	O
depend	O
on	O
x	O
we	O
can	O
include	O
a	O
baseline	B
here	O
without	O
changing	O
the	O
equality	O
because	O
the	O
gradient	B
sums	O
tx	O
hta	O
as	O
hta	O
is	O
changed	O
some	O
actions	O
probabilities	O
go	O
up	O
and	O
some	O
go	O
down	O
but	O
the	O
sum	O
of	O
the	O
changes	O
must	O
be	O
zero	O
because	O
the	O
sum	O
of	O
the	O
probabilities	O
is	O
always	O
one	O
to	O
zero	O
over	O
all	O
the	O
actions	O
next	O
we	O
multiply	O
each	O
term	O
of	O
the	O
sum	O
by	O
tx	O
tx	O
ert	O
hta	O
tx	O
hta	O
tx	O
the	O
equation	O
is	O
now	O
in	O
the	O
form	O
of	O
an	O
expectation	O
summing	O
over	O
all	O
possible	O
values	O
x	O
of	O
the	O
random	O
variable	O
at	O
then	O
multiplying	O
by	O
the	O
probability	O
of	O
taking	O
those	O
values	O
thus	O
tat	O
tat	O
hta	O
hta	O
where	O
here	O
we	O
have	O
chosen	O
the	O
baseline	B
bt	O
rt	O
and	O
substituted	O
rt	O
for	O
q	O
which	O
is	O
permitted	O
because	O
ertat	O
q	O
shortly	O
we	O
will	O
establish	O
that	O
hta	O
where	O
is	O
defined	O
to	O
be	O
if	O
a	O
x	O
else	O
assuming	O
that	O
for	O
now	O
we	O
have	O
tx	O
recall	O
that	O
our	O
plan	O
has	O
been	O
to	O
write	O
the	O
performance	O
gradient	B
as	O
an	O
expectation	O
of	O
something	O
that	O
we	O
can	O
sample	O
on	O
each	O
step	O
as	O
we	O
have	O
just	O
done	O
and	O
then	O
update	O
on	O
each	O
step	O
proportional	O
to	O
the	O
sample	O
substituting	O
a	O
sample	O
of	O
the	O
expectation	O
above	O
for	O
the	O
performance	O
gradient	B
in	O
yields	O
hta	O
for	O
all	O
a	O
which	O
you	O
may	O
recognize	O
as	O
being	O
equivalent	O
to	O
our	O
original	O
algorithm	O
chapter	O
multi-armed	B
bandits	I
thus	O
it	O
remains	O
only	O
to	O
show	O
that	O
tx	O
recall	O
the	O
standard	O
quotient	O
rule	O
for	O
derivatives	O
hta	O
as	O
we	O
assumed	O
f	O
f	O
x	O
gx	O
f	O
gx	O
x	O
using	O
this	O
we	O
can	O
write	O
tx	O
hta	O
tx	O
hta	O
ehtx	O
ehtx	O
ehty	O
ehtx	O
hta	O
ehty	O
tx	O
tx	O
ta	O
ehtxehta	O
ehty	O
ehtxehta	O
ehty	O
hta	O
the	O
quotient	O
rule	O
ex	O
x	O
ex	O
q	O
e	O
d	O
we	O
have	O
just	O
shown	O
that	O
the	O
expected	B
update	I
of	O
the	O
gradient	B
bandit	B
algorithm	I
is	O
equal	O
to	O
the	O
gradient	B
of	O
expected	B
reward	O
and	O
thus	O
that	O
the	O
algorithm	O
is	O
an	O
instance	O
of	O
stochastic	O
gradient	B
ascent	O
this	O
assures	O
us	O
that	O
the	O
algorithm	O
has	O
robust	O
convergence	O
properties	O
note	O
that	O
we	O
did	O
not	O
require	O
any	O
properties	O
of	O
the	O
reward	O
baseline	B
other	O
than	O
that	O
it	O
does	O
not	O
depend	O
on	O
the	O
selected	O
action	B
for	O
example	O
we	O
could	O
have	O
set	O
it	O
to	O
zero	O
or	O
to	O
and	O
the	O
algorithm	O
would	O
still	O
be	O
an	O
instance	O
of	O
stochastic	O
gradient	B
ascent	O
the	O
choice	O
of	O
the	O
baseline	B
does	O
not	O
affect	O
the	O
expected	B
update	I
of	O
the	O
algorithm	O
but	O
it	O
does	O
affect	O
the	O
variance	O
of	O
the	O
update	O
and	O
thus	O
the	O
rate	O
of	O
convergence	O
shown	O
e	O
g	O
in	O
figure	O
choosing	O
it	O
as	O
the	O
average	O
of	O
the	O
rewards	O
may	O
not	O
be	O
the	O
very	O
best	O
but	O
it	O
is	O
simple	O
and	O
works	O
well	O
in	O
practice	O
summary	O
associative	B
search	I
bandits	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
only	O
nonassociative	O
tasks	O
that	O
is	O
tasks	O
in	O
which	O
there	O
is	O
no	O
need	O
to	O
associate	O
different	O
actions	O
with	O
different	O
situations	O
in	O
these	O
tasks	O
the	O
learner	O
either	O
tries	O
to	O
find	O
a	O
single	O
best	O
action	B
when	O
the	O
task	O
is	O
stationary	O
or	O
tries	O
to	O
track	O
the	O
best	O
action	B
as	O
it	O
changes	O
over	O
time	O
when	O
the	O
task	O
is	O
nonstationary	O
however	O
in	O
a	O
general	O
reinforcement	B
learning	I
task	O
there	O
is	O
more	O
than	O
one	O
situation	O
and	O
the	O
goal	O
is	O
to	O
learn	O
a	O
policy	B
a	O
mapping	O
from	O
situations	O
to	O
the	O
actions	O
that	O
are	O
best	O
in	O
those	O
situations	O
to	O
set	O
the	O
stage	O
for	O
the	O
full	O
problem	O
we	O
briefly	O
discuss	O
the	O
simplest	O
way	O
in	O
which	O
nonassociative	O
tasks	O
extend	O
to	O
the	O
associative	O
setting	O
as	O
an	O
example	O
suppose	O
there	O
are	O
several	O
different	O
k-armed	O
bandit	O
tasks	O
and	O
that	O
on	O
each	O
step	O
you	O
confront	O
one	O
of	O
these	O
chosen	O
at	O
random	O
thus	O
the	O
bandit	O
task	O
changes	O
randomly	O
from	O
step	O
to	O
step	O
this	O
would	O
appear	O
to	O
you	O
as	O
a	O
single	O
nonstationary	O
k-armed	O
bandit	O
task	O
whose	O
true	O
action	B
values	O
change	O
randomly	O
from	O
step	O
to	O
step	O
you	O
could	O
try	O
using	O
one	O
of	O
the	O
methods	O
described	O
in	O
this	O
chapter	O
that	O
can	O
handle	O
nonstationarity	B
but	O
unless	O
the	O
true	O
action	B
values	O
change	O
slowly	O
these	O
methods	O
will	O
not	O
work	O
very	O
well	O
now	O
suppose	O
however	O
that	O
when	O
a	O
bandit	O
task	O
is	O
selected	O
for	O
you	O
you	O
are	O
given	O
some	O
distinctive	O
clue	O
about	O
its	O
identity	O
not	O
its	O
action	B
values	O
maybe	O
you	O
are	O
facing	O
an	O
actual	O
slot	O
machine	O
that	O
changes	O
the	O
color	O
of	O
its	O
display	O
as	O
it	O
changes	O
its	O
action	B
values	O
now	O
you	O
can	O
learn	O
a	O
policy	B
associating	O
each	O
task	O
signaled	O
by	O
the	O
color	O
you	O
see	O
with	O
the	O
best	O
action	B
to	O
take	O
when	O
facing	O
that	O
task	O
for	O
instance	O
if	O
red	O
select	O
arm	O
if	O
green	O
select	O
arm	O
with	O
the	O
right	O
policy	B
you	O
can	O
usually	O
do	O
much	O
better	O
than	O
you	O
could	O
in	O
the	O
absence	O
of	O
any	O
information	O
distinguishing	O
one	O
bandit	O
task	O
from	O
another	O
this	O
is	O
an	O
example	O
of	O
an	O
associative	B
search	I
task	O
so	O
called	O
because	O
it	O
involves	O
both	O
trial-and-error	O
learning	O
to	O
search	O
for	O
the	O
best	O
actions	O
and	O
association	O
of	O
these	O
actions	O
with	O
the	O
situations	O
in	O
which	O
they	O
are	O
best	O
associative	B
search	I
tasks	O
are	O
often	O
now	O
called	O
contextual	B
bandits	I
in	O
the	O
literature	O
associative	B
search	I
tasks	O
are	O
intermediate	O
between	O
the	O
k-armed	O
bandit	O
problem	O
and	O
the	O
full	O
reinforcement	B
learning	I
problem	O
they	O
are	O
like	O
the	O
full	O
reinforcement	B
learning	I
problem	O
in	O
that	O
they	O
involve	O
learning	O
a	O
policy	B
but	O
like	O
our	O
version	O
of	O
the	O
k-armed	O
bandit	O
problem	O
in	O
that	O
each	O
action	B
affects	O
only	O
the	O
immediate	O
reward	O
if	O
actions	O
are	O
allowed	O
to	O
affect	O
the	O
next	O
situation	O
as	O
well	O
as	O
the	O
reward	O
then	O
we	O
have	O
the	O
full	O
reinforcement	B
learning	I
problem	O
we	O
present	O
this	O
problem	O
in	O
the	O
next	O
chapter	O
and	O
consider	O
its	O
ramifications	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
exercise	O
suppose	O
you	O
face	O
a	O
bandit	O
task	O
whose	O
true	O
action	B
values	O
change	O
randomly	O
from	O
time	O
step	O
to	O
time	O
step	O
specifically	O
suppose	O
that	O
for	O
any	O
time	O
step	O
the	O
true	O
values	O
of	O
actions	O
and	O
are	O
respectively	O
and	O
with	O
probability	O
a	O
and	O
and	O
with	O
probability	O
b	O
if	O
you	O
are	O
not	O
able	O
to	O
tell	O
which	O
case	O
you	O
face	O
at	O
any	O
step	O
what	O
is	O
the	O
best	O
expectation	O
of	O
success	O
you	O
can	O
achieve	O
and	O
how	O
should	O
you	O
behave	O
to	O
achieve	O
it	O
now	O
suppose	O
that	O
on	O
each	O
step	O
you	O
are	O
told	O
whether	O
you	O
are	O
facing	O
case	O
a	O
or	O
case	O
b	O
you	O
still	O
don	O
t	O
know	O
the	O
true	O
action	B
values	O
this	O
is	O
an	O
associative	B
search	I
task	O
what	O
is	O
the	O
best	O
expectation	O
of	O
success	O
you	O
can	O
achieve	O
in	O
this	O
task	O
and	O
how	O
should	O
you	O
behave	O
to	O
achieve	O
it	O
chapter	O
multi-armed	B
bandits	I
summary	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
several	O
simple	O
ways	O
of	O
balancing	O
exploration	O
and	O
exploitation	O
the	O
methods	O
choose	O
randomly	O
a	O
small	O
fraction	O
of	O
the	O
time	O
whereas	O
ucb	O
methods	O
choose	O
deterministically	O
but	O
achieve	O
exploration	O
by	O
subtly	O
favoring	O
at	O
each	O
step	O
the	O
actions	O
that	O
have	O
so	O
far	O
received	O
fewer	O
samples	O
gradient	B
bandit	O
algorithms	O
estimate	O
not	O
action	B
values	O
but	O
action	B
preferences	I
and	O
favor	O
the	O
more	O
preferred	O
actions	O
in	O
a	O
graded	O
probabilistic	O
manner	O
using	O
a	O
soft-max	B
distribution	O
the	O
simple	O
expedient	O
of	O
initializing	O
estimates	O
optimistically	O
causes	O
even	O
greedy	O
methods	O
to	O
explore	O
significantly	O
it	O
is	O
natural	O
to	O
ask	O
which	O
of	O
these	O
methods	O
is	O
best	O
although	O
this	O
is	O
a	O
difficult	O
question	O
to	O
answer	O
in	O
general	O
we	O
can	O
certainly	O
run	O
them	O
all	O
on	O
the	O
testbed	O
that	O
we	O
have	O
used	O
throughout	O
this	O
chapter	O
and	O
compare	O
their	O
performances	O
a	O
complication	O
is	O
that	O
they	O
all	O
have	O
a	O
parameter	O
to	O
get	O
a	O
meaningful	O
comparison	O
we	O
have	O
to	O
consider	O
their	O
performance	O
as	O
a	O
function	O
of	O
their	O
parameter	O
our	O
graphs	O
so	O
far	O
have	O
shown	O
the	O
course	O
of	O
learning	O
over	O
time	O
for	O
each	O
algorithm	O
and	O
parameter	O
setting	O
to	O
produce	O
a	O
learning	O
curve	O
for	O
that	O
algorithm	O
and	O
parameter	O
setting	O
if	O
we	O
plotted	O
learning	O
curves	O
for	O
all	O
algorithms	O
and	O
all	O
parameter	O
settings	O
then	O
the	O
graph	O
would	O
be	O
too	O
complex	O
and	O
crowded	O
to	O
make	O
clear	O
comparisons	O
instead	O
we	O
summarize	O
a	O
complete	O
learning	O
curve	O
by	O
its	O
average	O
value	B
over	O
the	O
steps	O
this	O
value	B
is	O
proportional	O
to	O
the	O
area	O
under	O
the	O
learning	O
curve	O
figure	O
shows	O
this	O
measure	O
for	O
the	O
various	O
bandit	O
algorithms	O
from	O
this	O
chapter	O
each	O
as	O
a	O
function	O
of	O
its	O
own	O
parameter	O
shown	O
on	O
a	O
single	O
scale	O
on	O
the	O
x-axis	O
this	O
kind	O
of	O
graph	O
is	O
called	O
a	O
parameter	O
study	O
note	O
that	O
the	O
parameter	O
values	O
are	O
varied	O
by	O
factors	O
of	O
two	O
and	O
presented	O
on	O
a	O
log	O
scale	O
note	O
also	O
the	O
characteristic	O
inverted-u	O
shapes	O
of	O
each	O
algorithm	O
s	O
performance	O
all	O
the	O
algorithms	O
perform	O
best	O
at	O
an	O
intermediate	O
value	B
of	O
their	O
parameter	O
neither	O
too	O
large	O
nor	O
too	O
small	O
in	O
assessing	O
figure	O
a	O
parameter	O
study	O
of	O
the	O
various	O
bandit	O
algorithms	O
presented	O
in	O
this	O
chapter	O
each	O
point	O
is	O
the	O
average	O
reward	O
obtained	O
over	O
steps	O
with	O
a	O
particular	O
algorithm	O
at	O
a	O
particular	O
setting	O
of	O
its	O
parameter	O
averagerewardover	O
first	O
withoptimisticinitialization	O
summary	O
a	O
method	O
we	O
should	O
attend	O
not	O
just	O
to	O
how	O
well	O
it	O
does	O
at	O
its	O
best	O
parameter	O
setting	O
but	O
also	O
to	O
how	O
sensitive	O
it	O
is	O
to	O
its	O
parameter	O
value	B
all	O
of	O
these	O
algorithms	O
are	O
fairly	O
insensitive	O
performing	O
well	O
over	O
a	O
range	O
of	O
parameter	O
values	O
varying	O
by	O
about	O
an	O
order	O
of	O
magnitude	O
overall	O
on	O
this	O
problem	O
ucb	O
seems	O
to	O
perform	O
best	O
despite	O
their	O
simplicity	O
in	O
our	O
opinion	O
the	O
methods	O
presented	O
in	O
this	O
chapter	O
can	O
fairly	O
be	O
considered	O
the	O
state	B
of	O
the	O
art	O
there	O
are	O
more	O
sophisticated	O
methods	O
but	O
their	O
complexity	O
and	O
assumptions	O
make	O
them	O
impractical	O
for	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
is	O
our	O
real	O
focus	O
starting	O
in	O
chapter	O
we	O
present	O
learning	O
methods	O
for	O
solving	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
use	O
in	O
part	O
the	O
simple	O
methods	O
explored	O
in	O
this	O
chapter	O
although	O
the	O
simple	O
methods	O
explored	O
in	O
this	O
chapter	O
may	O
be	O
the	O
best	O
we	O
can	O
do	O
at	O
present	O
they	O
are	O
far	O
from	O
a	O
fully	O
satisfactory	O
solution	O
to	O
the	O
problem	O
of	O
balancing	O
exploration	O
and	O
exploitation	O
one	O
well-studied	O
approach	O
to	O
balancing	O
exploration	O
and	O
exploitation	O
in	O
k-armed	O
bandit	B
problems	I
is	O
to	O
compute	O
special	O
functions	O
called	O
gittins	O
indices	O
these	O
provide	O
an	O
optimal	O
solution	O
to	O
a	O
certain	O
kind	O
of	O
bandit	O
problem	O
more	O
general	O
than	O
that	O
considered	O
here	O
but	O
this	O
approach	O
assumes	O
that	O
the	O
prior	O
distribution	O
of	O
possible	O
problems	O
is	O
known	O
unfortunately	O
neither	O
the	O
theory	O
nor	O
the	O
computational	O
tractability	O
of	O
this	O
method	O
appear	O
to	O
generalize	O
to	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
we	O
consider	O
in	O
the	O
rest	O
of	O
the	O
book	O
bayesian	O
methods	O
assume	O
a	O
known	O
initial	O
distribution	O
over	O
the	O
action	B
values	O
and	O
then	O
update	O
the	O
distribution	O
exactly	O
after	O
each	O
step	O
that	O
the	O
true	O
action	B
values	O
are	O
stationary	O
in	O
general	O
the	O
update	O
computations	O
can	O
be	O
very	O
complex	O
but	O
for	O
certain	O
special	O
distributions	O
conjugate	O
priors	O
they	O
are	O
easy	O
one	O
possibility	O
is	O
to	O
then	O
select	O
actions	O
at	O
each	O
step	O
according	O
to	O
their	O
posterior	O
probability	O
of	O
being	O
the	O
best	O
action	B
this	O
method	O
sometimes	O
called	O
posterior	O
sampling	O
or	O
thompson	O
sampling	O
often	O
performs	O
similarly	O
to	O
the	O
best	O
of	O
the	O
distribution-free	O
methods	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
in	O
the	O
bayesian	O
setting	O
it	O
is	O
even	O
conceivable	O
to	O
compute	O
the	O
optimal	O
balance	O
between	O
exploration	O
and	O
exploitation	O
one	O
can	O
compute	O
for	O
any	O
possible	O
action	B
the	O
probability	O
of	O
each	O
possible	O
immediate	O
reward	O
and	O
the	O
resultant	O
posterior	O
distributions	O
over	O
action	B
values	O
this	O
evolving	O
distribution	O
becomes	O
the	O
information	O
state	B
of	O
the	O
problem	O
given	O
a	O
horizon	O
say	O
of	O
steps	O
one	O
can	O
consider	O
all	O
possible	O
actions	O
all	O
possible	O
resulting	O
rewards	O
all	O
possible	O
next	O
actions	O
all	O
next	O
rewards	O
and	O
so	O
on	O
for	O
all	O
steps	O
given	O
the	O
assumptions	O
the	O
rewards	O
and	O
probabilities	O
of	O
each	O
possible	O
chain	O
of	O
events	O
can	O
be	O
determined	O
and	O
one	O
need	O
only	O
pick	O
the	O
best	O
but	O
the	O
tree	O
of	O
possibilities	O
grows	O
extremely	O
rapidly	O
even	O
if	O
there	O
were	O
only	O
two	O
actions	O
and	O
two	O
rewards	O
the	O
tree	O
would	O
have	O
leaves	O
it	O
is	O
generally	O
not	O
feasible	O
to	O
perform	O
this	O
immense	O
computation	O
exactly	O
but	O
perhaps	O
it	O
could	O
be	O
approximated	O
efficiently	O
this	O
approach	O
would	O
effectively	O
turn	O
the	O
bandit	O
problem	O
into	O
an	O
instance	O
of	O
the	O
full	O
reinforcement	B
learning	I
problem	O
in	O
the	O
end	O
we	O
may	O
be	O
able	O
to	O
use	O
approximate	O
reinforcement	B
learning	I
methods	O
such	O
as	O
those	O
presented	O
in	O
part	O
ii	O
of	O
this	O
book	O
to	O
approach	O
this	O
optimal	O
solution	O
but	O
that	O
is	O
a	O
topic	O
for	O
research	O
and	O
beyond	O
the	O
scope	O
of	O
this	O
introductory	O
book	O
exercise	O
make	O
a	O
figure	O
analogous	O
to	O
figure	O
for	O
the	O
nonstationary	O
chapter	O
multi-armed	B
bandits	I
case	O
outlined	O
in	O
exercise	O
include	O
the	O
constant-step-size	O
algorithm	O
with	O
use	O
runs	O
of	O
steps	O
and	O
as	O
a	O
performance	O
measure	O
for	O
each	O
algorithm	O
and	O
parameter	O
setting	O
use	O
the	O
average	O
reward	O
over	O
the	O
last	O
steps	O
bibliographical	O
and	O
historical	O
remarks	O
bandit	B
problems	I
have	O
been	O
studied	O
in	O
statistics	O
engineering	O
and	O
psychology	B
in	O
statistics	O
bandit	B
problems	I
fall	O
under	O
the	O
heading	O
sequential	O
design	B
of	I
experiments	O
introduced	O
by	O
thompson	O
and	O
robbins	O
and	O
studied	O
by	O
bellman	B
berry	O
and	O
fristedt	O
provide	O
an	O
extensive	O
treatment	O
of	O
bandit	B
problems	I
from	O
the	O
perspective	O
of	O
statistics	O
narendra	O
and	O
thathachar	O
treat	O
bandit	B
problems	I
from	O
the	O
engineering	O
perspective	O
providing	O
a	O
good	O
discussion	O
of	O
the	O
various	O
theoretical	O
traditions	O
that	O
have	O
focused	O
on	O
them	O
in	B
psychology	B
bandit	B
problems	I
have	O
played	O
roles	O
in	O
statistical	O
learning	O
theory	O
bush	O
and	O
mosteller	O
estes	O
the	O
term	O
greedy	O
is	O
often	O
used	O
in	O
the	O
heuristic	B
search	I
literature	O
pearl	O
the	O
conflict	O
between	O
exploration	O
and	O
exploitation	O
is	O
known	O
in	O
control	B
engineering	O
as	O
the	O
conflict	O
between	O
identification	O
estimation	O
and	B
control	B
witten	B
feldbaum	O
called	O
it	O
the	O
dual	O
control	B
problem	O
referring	O
to	O
the	O
need	O
to	O
solve	O
the	O
two	O
problems	O
of	O
identification	O
and	B
control	B
simultaneously	O
when	O
trying	O
to	O
control	B
a	O
system	O
under	O
uncertainty	O
in	O
discussing	O
aspects	O
of	O
genetic	B
algorithms	I
holland	B
emphasized	O
the	O
importance	O
of	O
this	O
conflict	O
referring	O
to	O
it	O
as	O
the	O
conflict	O
between	O
the	O
need	O
to	O
exploit	O
and	O
the	O
need	O
for	O
new	O
information	O
action-value	B
methods	I
for	O
our	O
k-armed	O
bandit	O
problem	O
were	O
first	O
proposed	O
by	O
thathachar	O
and	O
sastry	O
these	O
are	O
often	O
called	O
estimator	O
algorithms	O
in	O
the	O
learning	B
automata	I
literature	O
the	O
term	O
action	B
value	B
is	O
due	O
to	O
watkins	B
the	O
first	O
to	O
use	O
methods	O
may	O
also	O
have	O
been	O
watkins	B
p	O
but	O
the	O
idea	O
is	O
so	O
simple	O
that	O
some	O
earlier	O
use	O
seems	O
likely	O
this	O
material	O
falls	O
under	O
the	O
general	O
heading	O
of	O
stochastic	O
iterative	B
algorithms	O
which	O
is	O
well	O
covered	O
by	O
bertsekas	O
and	O
tsitsiklis	O
optimistic	O
initialization	O
was	O
used	O
in	O
reinforcement	B
learning	I
by	O
sutton	O
early	O
work	O
on	O
using	O
estimates	O
of	O
the	O
upper	O
confidence	O
bound	O
to	O
select	O
actions	O
was	O
done	O
by	O
lai	O
and	O
robbins	O
kaelbling	O
and	O
agrawal	O
the	O
ucb	O
algorithm	O
we	O
present	O
here	O
is	O
called	O
in	O
the	O
literature	O
and	O
was	O
first	O
developed	O
by	O
auer	O
cesa-bianchi	O
and	O
fischer	O
gradient	B
bandit	O
algorithms	O
are	O
a	O
special	O
case	O
of	O
the	O
gradient-based	O
reinforcement	B
learning	I
algorithms	O
introduced	O
by	O
williams	O
and	O
that	O
later	O
developed	O
into	O
the	O
actor	O
critic	O
and	O
policy-gradient	O
algorithms	O
that	O
we	O
treat	O
later	O
in	O
this	O
book	O
our	O
development	O
here	O
was	O
influenced	O
by	O
that	O
by	O
balaraman	O
ravindran	O
communication	O
further	O
discussion	O
of	O
the	O
choice	O
of	O
baseline	B
is	O
provided	O
there	O
summary	O
and	O
by	O
greensmith	O
bartlett	O
and	O
baxter	O
and	O
dick	O
early	O
systematic	O
studies	O
of	O
algorithms	O
like	O
this	O
were	O
done	O
by	O
sutton	O
the	O
term	O
soft-max	B
for	O
the	O
action	B
selection	O
rule	O
is	O
due	O
to	O
bridle	O
this	O
rule	O
appears	O
to	O
have	O
been	O
first	O
proposed	O
by	O
luce	O
the	O
term	O
associative	B
search	I
and	O
the	O
corresponding	O
problem	O
were	O
introduced	O
by	O
barto	O
sutton	O
and	O
brouwer	O
the	O
term	O
associative	B
reinforcement	B
learning	I
has	O
also	O
been	O
used	O
for	O
associative	B
search	I
and	O
anandan	O
but	O
we	O
prefer	O
to	O
reserve	O
that	O
term	O
as	O
a	O
synonym	O
for	O
the	O
full	O
reinforcement	B
learning	I
problem	O
in	O
sutton	O
as	O
we	O
noted	O
the	O
modern	O
literature	O
also	O
uses	O
the	O
term	O
contextual	B
bandits	I
for	O
this	O
problem	O
we	O
note	O
that	O
thorndike	O
s	O
law	B
of	I
effect	I
in	O
chapter	O
describes	O
associative	B
search	I
by	O
referring	O
to	O
the	O
formation	O
of	O
associative	O
links	O
between	O
situations	O
and	O
actions	O
according	O
to	O
the	O
terminology	O
of	O
operant	O
or	O
instrumental	O
conditioning	O
skinner	B
a	O
discriminative	O
stimulus	O
is	O
a	O
stimulus	O
that	O
signals	O
the	O
presence	O
of	O
a	O
particular	O
reinforcement	O
contingency	O
in	O
our	O
terms	O
different	O
discriminative	O
stimuli	O
correspond	O
to	O
different	O
states	O
bellman	B
was	O
the	O
first	O
to	O
show	O
how	O
dynamic	B
programming	I
could	O
be	O
used	O
to	O
compute	O
the	O
optimal	O
balance	O
between	O
exploration	O
and	O
exploitation	O
within	O
a	O
bayesian	O
formulation	O
of	O
the	O
problem	O
the	O
gittins	B
index	I
approach	O
is	O
due	O
to	O
gittins	O
and	O
jones	O
duff	O
showed	O
how	O
it	O
is	O
possible	O
to	O
learn	O
gittins	O
indices	O
for	B
bandit	B
problems	I
through	O
reinforcement	B
learning	I
the	O
survey	O
by	O
kumar	O
provides	O
a	O
good	O
discussion	O
of	O
bayesian	O
and	O
non-bayesian	O
approaches	O
to	O
these	O
problems	O
the	O
term	O
information	O
state	B
comes	O
from	O
the	O
literature	O
on	O
partially	B
observable	I
mdps	I
see	O
e	O
g	O
lovejoy	O
other	O
theoretical	O
research	O
focuses	O
on	O
the	O
efficiency	O
of	O
exploration	O
usually	O
expressed	O
as	O
how	O
quickly	O
an	O
algorithm	O
can	O
approach	O
an	O
optimal	O
decision-making	O
policy	B
one	O
way	O
to	O
formalize	O
exploration	O
efficiency	O
is	O
by	O
adapting	O
to	O
reinforcement	B
learning	I
the	O
notion	O
of	O
sample	O
complexity	O
for	O
a	O
supervised	B
learning	I
algorithm	O
which	O
is	O
the	O
number	O
of	O
training	O
examples	O
the	O
algorithm	O
needs	O
to	O
attain	O
a	O
desired	O
degree	O
of	O
accuracy	O
in	O
learning	O
the	O
target	O
function	O
a	O
definition	O
of	O
the	O
sample	O
complexity	O
of	O
exploration	O
for	O
a	O
reinforcement	B
learning	I
algorithm	O
is	O
the	O
number	O
of	O
time	O
steps	O
in	O
which	O
the	O
algorithm	O
does	O
not	O
select	O
near-optimal	O
actions	O
li	O
discusses	O
this	O
and	O
several	O
other	O
approaches	O
in	O
a	O
survey	O
of	O
theoretical	O
approaches	O
to	O
exploration	O
efficiency	O
in	O
reinforcement	B
learning	I
chapter	O
finite	O
markov	O
decision	O
processes	O
in	O
this	O
chapter	O
we	O
introduce	O
the	O
formal	O
problem	O
of	O
finite	O
markov	O
decision	O
processes	O
or	O
finite	O
mdps	O
which	O
we	O
try	O
to	O
solve	O
in	O
the	O
rest	O
of	O
the	O
book	O
this	O
problem	O
involves	O
evaluative	B
feedback	I
as	O
in	O
bandits	O
but	O
also	O
an	O
associative	O
aspect	O
choosing	O
different	O
actions	O
in	O
different	O
situations	O
mdps	O
are	O
a	O
classical	O
formalization	O
of	O
sequential	O
decision	O
making	O
where	O
actions	O
influence	O
not	O
just	O
immediate	O
rewards	O
but	O
also	O
subsequent	O
situations	O
or	O
states	O
and	O
through	O
those	O
future	O
rewards	O
thus	O
mdps	O
involve	O
delayed	B
reward	I
and	O
the	O
need	O
to	O
tradeoff	O
immediate	O
and	O
delayed	B
reward	I
whereas	O
in	B
bandit	B
problems	I
we	O
estimated	O
the	O
value	B
q	O
of	O
each	O
action	B
a	O
in	O
mdps	O
we	O
estimate	O
the	O
value	B
q	O
a	O
of	O
each	O
action	B
a	O
in	O
each	O
state	B
s	O
or	O
we	O
estimate	O
the	O
value	B
v	O
of	O
each	O
state	B
given	O
optimal	O
action	B
selections	O
these	O
state-dependent	O
quantities	O
are	O
essential	O
to	O
accurately	O
assigning	O
credit	O
for	O
long-term	O
consequences	O
to	O
individual	O
action	B
selections	O
mdps	O
are	O
a	O
mathematically	O
idealized	O
form	O
of	O
the	O
reinforcement	B
learning	I
problem	O
for	O
which	O
precise	O
theoretical	O
statements	O
can	O
be	O
made	O
we	O
introduce	O
key	O
elements	O
of	O
the	O
problem	O
s	O
mathematical	O
structure	O
such	O
as	O
returns	O
value	B
functions	O
and	O
bellman	B
equations	O
we	O
try	O
to	O
convey	O
the	O
wide	O
range	O
of	O
applications	O
that	O
can	O
be	O
formulated	O
as	O
finite	O
mdps	O
as	O
in	O
all	O
of	O
artificial	B
intelligence	I
there	O
is	O
a	O
tension	O
between	O
breadth	O
of	O
applicability	O
and	O
mathematical	O
tractability	O
in	O
this	O
chapter	O
we	O
introduce	O
this	O
tension	O
and	O
discuss	O
some	O
of	O
the	O
trade-offs	O
and	O
challenges	O
that	O
it	O
implies	O
some	O
ways	O
in	O
which	O
reinforcement	B
learning	I
can	O
be	O
taken	O
beyond	O
mdps	O
are	O
treated	O
in	O
chapter	O
the	O
agent	O
environment	B
interface	O
mdps	O
are	O
meant	O
to	O
be	O
a	O
straightforward	O
framing	O
of	O
the	O
problem	O
of	O
learning	O
from	O
interaction	O
to	O
achieve	O
a	O
goal	O
the	O
learner	O
and	O
decision	O
maker	O
is	O
called	O
the	O
agent	O
the	O
thing	O
it	O
interacts	O
with	O
comprising	O
everything	O
outside	O
the	O
agent	O
is	O
called	O
the	O
environment	B
these	O
interact	O
continually	O
the	O
agent	O
selecting	O
actions	O
and	O
the	O
environment	B
responding	O
to	O
these	O
chapter	O
finite	O
markov	O
decision	O
processes	O
actions	O
and	O
presenting	O
new	O
situations	O
to	O
the	O
the	O
environment	B
also	O
gives	O
rise	O
to	O
rewards	O
special	O
numerical	O
values	O
that	O
the	O
agent	O
seeks	O
to	O
maximize	O
over	O
time	O
through	O
its	O
choice	O
of	O
actions	O
figure	O
the	O
agent	O
environment	B
interaction	O
in	O
a	O
markov	O
decision	O
process	O
more	O
specifically	O
the	O
agent	O
and	O
environment	B
interact	O
at	O
each	O
of	O
a	O
sequence	O
of	O
discrete	O
time	O
steps	O
t	O
at	O
each	O
time	O
step	O
t	O
the	O
agent	O
receives	O
some	O
representation	O
of	O
the	O
environment	B
s	O
state	B
st	O
s	O
and	O
on	O
that	O
basis	O
selects	O
an	O
action	B
at	O
one	O
time	O
step	O
later	O
in	O
part	O
as	O
a	O
consequence	O
of	O
its	O
action	B
the	O
agent	O
receives	O
a	O
numerical	O
reward	O
r	O
r	O
and	O
finds	O
itself	O
in	O
a	O
new	O
state	B
the	O
mdp	O
and	O
agent	O
together	O
thereby	O
give	O
rise	O
to	O
a	O
sequence	O
or	O
trajectory	O
that	O
begins	O
like	O
this	O
in	O
a	O
finite	O
mdp	O
the	O
sets	O
of	O
states	O
actions	O
and	O
rewards	O
a	O
and	O
r	O
all	O
have	O
a	O
finite	O
number	O
of	O
elements	O
in	O
this	O
case	O
the	O
random	O
variables	O
rt	O
and	O
st	O
have	O
well	O
defined	O
discrete	O
probability	O
distributions	O
dependent	O
only	O
on	O
the	O
preceding	O
state	B
and	O
action	B
that	O
is	O
for	O
particular	O
values	O
of	O
these	O
random	O
variables	O
s	O
and	O
r	O
r	O
there	O
is	O
a	O
probability	O
of	O
those	O
values	O
occurring	O
at	O
time	O
t	O
given	O
particular	O
values	O
of	O
the	O
preceding	O
state	B
and	O
action	B
rs	O
a	O
prst	O
rt	O
r	O
st	O
s	O
at	O
a	O
for	O
all	O
s	O
s	O
r	O
r	O
and	O
a	O
as	O
the	O
function	O
p	O
defines	O
the	O
dynamics	O
of	O
the	O
mdp	O
the	O
dot	O
over	O
the	O
equals	O
sign	O
in	O
the	O
equation	O
reminds	O
us	O
that	O
it	O
is	O
a	O
definition	O
this	O
case	O
of	O
the	O
function	O
p	O
rather	O
than	O
a	O
fact	O
that	O
follows	O
from	O
previous	O
definitions	O
the	O
dynamics	O
function	O
p	O
s	O
r	O
s	O
a	O
is	O
an	O
ordinary	O
deterministic	O
function	O
of	O
four	O
arguments	O
the	O
in	O
the	O
middle	O
of	O
it	O
comes	O
from	O
the	O
notation	O
for	O
conditional	O
use	O
the	O
terms	O
agent	O
environment	B
and	O
action	B
instead	O
of	O
the	O
engineers	O
terms	O
controller	O
con	O
trolled	O
system	O
plant	O
and	B
control	B
signal	O
because	O
they	O
are	O
meaningful	O
to	O
a	O
wider	O
audience	O
restrict	O
attention	O
to	O
discrete	O
time	O
to	O
keep	O
things	O
as	O
simple	O
as	O
possible	O
even	O
though	O
many	O
of	O
the	O
ideas	O
can	O
be	O
extended	O
to	O
the	O
continuous-time	O
case	O
see	O
bertsekas	O
and	O
tsitsiklis	O
doya	O
simplify	O
notation	O
we	O
sometimes	O
assume	O
the	O
special	O
case	O
in	O
which	O
the	O
action	B
set	O
is	O
the	O
same	O
in	O
all	O
states	O
and	O
write	O
it	O
simply	O
as	O
a	O
use	O
instead	O
of	O
rt	O
to	O
denote	O
the	O
reward	O
due	O
to	O
at	O
because	O
it	O
emphasizes	O
that	O
the	O
next	O
reward	O
and	O
next	O
state	B
and	O
are	O
jointly	O
determined	O
unfortunately	O
both	O
conventions	O
are	O
widely	O
used	O
in	O
the	O
literature	O
the	O
agent	O
environment	B
interface	O
probability	O
but	O
here	O
it	O
just	O
reminds	O
us	O
that	O
p	O
specifies	O
a	O
probability	O
distribution	O
for	O
each	O
choice	O
of	O
s	O
and	O
a	O
that	O
is	O
that	O
r	O
rs	O
a	O
for	O
all	O
s	O
s	O
a	O
as	O
in	O
a	O
markov	O
decision	O
process	O
the	O
probabilities	O
given	O
by	O
p	O
completely	O
characterize	O
the	O
environment	B
s	O
dynamics	O
that	O
is	O
the	O
probability	O
of	O
each	O
possible	O
value	B
for	O
st	O
and	O
rt	O
depends	O
only	O
on	O
the	O
immediately	O
preceding	O
state	B
and	O
action	B
st	O
and	O
at	O
and	O
given	O
them	O
not	O
at	O
all	O
on	O
earlier	O
states	O
and	O
actions	O
this	O
is	O
best	O
viewed	O
a	O
restriction	O
not	O
on	O
the	O
decision	O
process	O
but	O
on	O
the	O
state	B
the	O
state	B
must	O
include	O
information	O
about	O
all	O
aspects	O
of	O
the	O
past	O
agent	O
environment	B
interaction	O
that	O
make	O
a	O
difference	O
for	O
the	O
future	O
if	O
it	O
does	O
then	O
the	O
state	B
is	O
said	O
to	O
have	O
the	O
markov	B
property	I
we	O
will	O
assume	O
the	O
markov	B
property	I
throughout	O
this	O
book	O
though	O
starting	O
in	O
part	O
ii	O
we	O
will	O
consider	O
approximation	O
methods	O
that	O
do	O
not	O
rely	O
on	O
it	O
and	O
in	O
chapter	O
we	O
consider	O
how	O
a	O
markov	O
state	B
can	O
be	O
learned	O
and	O
constructed	O
from	O
non-markov	O
observations	O
from	O
the	O
four-argument	O
dynamics	O
function	O
p	O
one	O
can	O
compute	O
anything	O
else	O
one	O
might	O
want	O
to	O
know	O
about	O
the	O
environment	B
such	O
as	O
the	O
state-transition	O
probabilities	O
we	O
denote	O
with	O
a	O
slight	O
abuse	O
of	O
notation	O
as	O
a	O
three-argument	O
function	O
p	O
s	O
s	O
a	O
a	O
prst	O
st	O
s	O
at	O
a	O
r	O
rs	O
a	O
we	O
can	O
also	O
compute	O
the	O
expected	B
rewards	O
for	O
state	B
action	B
pairs	O
as	O
a	O
two-argument	O
function	O
r	O
s	O
a	O
r	O
rs	O
a	O
ert	O
st	O
s	O
at	O
a	O
r	O
s	O
rs	O
a	O
and	O
the	O
expected	B
rewards	O
for	O
state	B
action	B
next-state	O
triples	O
as	O
a	O
three-argument	O
function	O
r	O
s	O
a	O
s	O
r	O
ert	O
st	O
s	O
at	O
a	O
st	O
r	O
r	O
rs	O
a	O
a	O
rs	O
a	O
in	O
this	O
book	O
we	O
usually	O
use	O
the	O
four-argument	O
p	O
function	O
but	O
each	O
of	O
these	O
other	O
notations	O
are	O
also	O
occasionally	O
convenient	O
the	O
mdp	O
framework	O
is	O
abstract	O
and	O
flexible	O
and	O
can	O
be	O
applied	O
to	O
many	O
different	O
problems	O
in	O
many	O
different	O
ways	O
for	O
example	O
the	O
time	O
steps	O
need	O
not	O
refer	O
to	O
fixed	O
intervals	O
of	O
real	O
time	O
they	O
can	O
refer	O
to	O
arbitrary	O
successive	O
stages	O
of	O
decision	O
making	O
and	O
acting	O
the	O
actions	O
can	O
be	O
low-level	O
controls	O
such	O
as	O
the	O
voltages	O
applied	O
to	O
the	O
motors	O
of	O
a	O
robot	O
arm	O
or	O
high-level	O
decisions	O
such	O
as	O
whether	O
or	O
not	O
to	O
have	O
lunch	O
or	O
to	O
go	O
to	O
graduate	O
school	O
similarly	O
the	O
states	O
can	O
take	O
a	O
wide	O
variety	O
of	O
forms	O
they	O
can	O
be	O
completely	O
determined	O
by	O
low-level	O
sensations	O
such	O
as	O
direct	O
sensor	O
readings	O
or	O
they	O
can	O
be	O
more	O
high-level	O
and	O
abstract	O
such	O
as	O
symbolic	O
descriptions	O
of	O
objects	O
in	O
a	O
room	O
some	O
of	O
what	O
makes	O
up	O
a	O
state	B
could	O
be	O
based	O
on	O
memory	O
of	O
past	O
sensations	O
or	O
chapter	O
finite	O
markov	O
decision	O
processes	O
even	O
be	O
entirely	O
mental	O
or	O
subjective	O
for	O
example	O
an	O
agent	O
could	O
be	O
in	O
the	O
state	B
of	O
not	O
being	O
sure	O
where	O
an	O
object	O
is	O
or	O
of	O
having	O
just	O
been	O
surprised	O
in	O
some	O
clearly	O
defined	O
sense	O
similarly	O
some	O
actions	O
might	O
be	O
totally	O
mental	O
or	O
computational	O
for	O
example	O
some	O
actions	O
might	O
control	B
what	O
an	O
agent	O
chooses	O
to	O
think	O
about	O
or	O
where	O
it	O
focuses	O
its	O
attention	O
in	O
general	O
actions	O
can	O
be	O
any	O
decisions	O
we	O
want	O
to	O
learn	O
how	O
to	O
make	O
and	O
the	O
states	O
can	O
be	O
anything	O
we	O
can	O
know	O
that	O
might	O
be	O
useful	O
in	O
making	O
them	O
in	O
particular	O
the	O
boundary	O
between	O
agent	O
and	O
environment	B
is	O
typically	O
not	O
the	O
same	O
as	O
the	O
physical	O
boundary	O
of	O
robot	O
s	O
or	O
animal	O
s	O
body	O
usually	O
the	O
boundary	O
is	O
drawn	O
closer	O
to	O
the	O
agent	O
than	O
that	O
for	O
example	O
the	O
motors	O
and	O
mechanical	O
linkages	O
of	O
a	O
robot	O
and	O
its	O
sensing	O
hardware	O
should	O
usually	O
be	O
considered	O
parts	O
of	O
the	O
environment	B
rather	O
than	O
parts	O
of	O
the	O
agent	O
similarly	O
if	O
we	O
apply	O
the	O
mdp	O
framework	O
to	O
a	O
person	O
or	O
animal	O
the	O
muscles	O
skeleton	O
and	O
sensory	O
organs	O
should	O
be	O
considered	O
part	O
of	O
the	O
environment	B
rewards	O
too	O
presumably	O
are	O
computed	O
inside	O
the	O
physical	O
bodies	O
of	O
natural	O
and	O
artificial	O
learning	O
systems	O
but	O
are	O
considered	O
external	O
to	O
the	O
agent	O
the	O
general	O
rule	O
we	O
follow	O
is	O
that	O
anything	O
that	O
cannot	O
be	O
changed	O
arbitrarily	O
by	O
the	O
agent	O
is	O
considered	O
to	O
be	O
outside	O
of	O
it	O
and	O
thus	O
part	O
of	O
its	O
environment	B
we	O
do	O
not	O
assume	O
that	O
everything	O
in	O
the	O
environment	B
is	O
unknown	O
to	O
the	O
agent	O
for	O
example	O
the	O
agent	O
often	O
knows	O
quite	O
a	O
bit	O
about	O
how	O
its	O
rewards	O
are	O
computed	O
as	O
a	O
function	O
of	O
its	O
actions	O
and	O
the	O
states	O
in	O
which	O
they	O
are	O
taken	O
but	O
we	O
always	O
consider	O
the	O
reward	O
computation	O
to	O
be	O
external	O
to	O
the	O
agent	O
because	O
it	O
defines	O
the	O
task	O
facing	O
the	O
agent	O
and	O
thus	O
must	O
be	O
beyond	O
its	O
ability	O
to	O
change	O
arbitrarily	O
in	O
fact	O
in	O
some	O
cases	O
the	O
agent	O
may	O
know	O
everything	O
about	O
how	O
its	O
environment	B
works	O
and	O
still	O
face	O
a	O
difficult	O
reinforcement	B
learning	I
task	O
just	O
as	O
we	O
may	O
know	O
exactly	O
how	O
a	O
puzzle	O
like	O
rubik	O
s	O
cube	O
works	O
but	O
still	O
be	O
unable	O
to	O
solve	O
it	O
the	O
agent	O
environment	B
boundary	O
represents	O
the	O
limit	O
of	O
the	O
agent	O
s	O
absolute	O
control	B
not	O
of	O
its	O
knowledge	O
the	O
agent	O
environment	B
boundary	O
can	O
be	O
located	O
at	O
different	O
places	O
for	O
different	O
purposes	O
in	O
a	O
complicated	O
robot	O
many	O
different	O
agents	O
may	O
be	O
operating	O
at	O
once	O
each	O
with	O
its	O
own	O
boundary	O
for	O
example	O
one	O
agent	O
may	O
make	O
high-level	O
decisions	O
which	O
form	O
part	O
of	O
the	O
states	O
faced	O
by	O
a	O
lower-level	O
agent	O
that	O
implements	O
the	O
high-level	O
decisions	O
in	O
practice	O
the	O
agent	O
environment	B
boundary	O
is	O
determined	O
once	O
one	O
has	O
selected	O
particular	O
states	O
actions	O
and	O
rewards	O
and	O
thus	O
has	O
identified	O
a	O
specific	O
decision	O
making	O
task	O
of	O
interest	O
the	O
mdp	O
framework	O
is	O
a	O
considerable	O
abstraction	O
of	O
the	O
problem	O
of	O
goal-directed	O
learning	O
from	O
interaction	O
it	O
proposes	O
that	O
whatever	O
the	O
details	O
of	O
the	O
sensory	O
memory	O
and	B
control	B
apparatus	O
and	O
whatever	O
objective	O
one	O
is	O
trying	O
to	O
achieve	O
any	O
problem	O
of	O
learning	O
goal-directed	O
behavior	O
can	O
be	O
reduced	O
to	O
three	O
signals	O
passing	O
back	O
and	O
forth	O
between	O
an	O
agent	O
and	O
its	O
environment	B
one	O
signal	O
to	O
represent	O
the	O
choices	O
made	O
by	O
the	O
agent	O
actions	O
one	O
signal	O
to	O
represent	O
the	O
basis	O
on	O
which	O
the	O
choices	O
are	O
made	O
states	O
and	O
one	O
signal	O
to	O
define	O
the	O
agent	O
s	O
goal	O
rewards	O
this	O
framework	O
may	O
not	O
be	O
sufficient	O
to	O
represent	O
all	O
decision-learning	O
problems	O
usefully	O
but	O
it	O
has	O
proved	O
to	O
be	O
widely	O
useful	O
and	O
applicable	O
of	O
course	O
the	O
particular	O
states	O
and	O
actions	O
vary	O
greatly	O
from	O
task	O
to	O
task	O
and	O
how	O
they	O
are	O
represented	O
can	O
strongly	O
affect	O
performance	O
in	O
reinforcement	B
learning	I
as	O
in	O
other	O
kinds	O
of	O
learning	O
such	O
representational	O
choices	O
are	O
at	O
present	O
more	O
art	O
than	O
science	O
the	O
agent	O
environment	B
interface	O
in	O
this	O
book	O
we	O
offer	O
some	O
advice	O
and	O
examples	O
regarding	O
good	O
ways	O
of	O
representing	O
states	O
and	O
actions	O
but	O
our	O
primary	O
focus	O
is	O
on	O
general	O
principles	O
for	O
learning	O
how	O
to	O
behave	O
once	O
the	O
representations	O
have	O
been	O
selected	O
example	O
bioreactor	O
suppose	O
reinforcement	B
learning	I
is	O
being	O
applied	O
to	O
determine	O
moment-by-moment	O
temperatures	O
and	O
stirring	O
rates	O
for	O
a	O
bioreactor	O
large	O
vat	O
of	O
nutrients	O
and	O
bacteria	O
used	O
to	O
produce	O
useful	O
chemicals	O
the	O
actions	O
in	O
such	O
an	O
application	O
might	O
be	O
target	O
temperatures	O
and	O
target	O
stirring	O
rates	O
that	O
are	O
passed	O
to	O
lower-level	O
control	B
systems	O
that	O
in	O
turn	O
directly	O
activate	O
heating	O
elements	O
and	O
motors	O
to	O
attain	O
the	O
targets	O
the	O
states	O
are	O
likely	O
to	O
be	O
thermocouple	O
and	O
other	O
sensory	O
readings	O
perhaps	O
filtered	O
and	O
delayed	O
plus	O
symbolic	O
inputs	O
representing	O
the	O
ingredients	O
in	O
the	O
vat	O
and	O
the	O
target	O
chemical	O
the	O
rewards	O
might	O
be	O
moment-by-moment	O
measures	O
of	O
the	O
rate	O
at	O
which	O
the	O
useful	O
chemical	O
is	O
produced	O
by	O
the	O
bioreactor	O
notice	O
that	O
here	O
each	O
state	B
is	O
a	O
list	O
or	O
vector	B
of	O
sensor	O
readings	O
and	O
symbolic	O
inputs	O
and	O
each	O
action	B
is	O
a	O
vector	B
consisting	O
of	O
a	O
target	O
temperature	O
and	O
a	O
stirring	O
rate	O
it	O
is	O
typical	O
of	O
reinforcement	B
learning	I
tasks	O
to	O
have	O
states	O
and	O
actions	O
with	O
such	O
structured	O
representations	O
rewards	O
on	O
the	O
other	O
hand	O
are	O
always	O
single	O
numbers	O
example	O
pick-and-place	O
robot	O
consider	O
using	O
reinforcement	B
learning	I
to	O
control	B
the	O
motion	O
of	O
a	O
robot	O
arm	O
in	O
a	O
repetitive	O
pick-and-place	O
task	O
if	O
we	O
want	O
to	O
learn	O
movements	O
that	O
are	O
fast	O
and	O
smooth	O
the	O
learning	O
agent	O
will	O
have	O
to	O
control	B
the	O
motors	O
directly	O
and	O
have	O
low-latency	O
information	O
about	O
the	O
current	O
positions	O
and	O
velocities	O
of	O
the	O
mechanical	O
linkages	O
the	O
actions	O
in	O
this	O
case	O
might	O
be	O
the	O
voltages	O
applied	O
to	O
each	O
motor	O
at	O
each	O
joint	O
and	O
the	O
states	O
might	O
be	O
the	O
latest	O
readings	O
of	O
joint	O
angles	O
and	O
velocities	O
the	O
reward	O
might	O
be	O
for	O
each	O
object	O
successfully	O
picked	O
up	O
and	O
placed	O
to	O
encourage	O
smooth	O
movements	O
on	O
each	O
time	O
step	O
a	O
small	O
negative	O
reward	O
can	O
be	O
given	O
as	O
a	O
function	O
of	O
the	O
moment-to-moment	O
jerkiness	O
of	O
the	O
motion	O
exercise	O
devise	O
three	O
example	O
tasks	O
of	O
your	O
own	O
that	O
fit	O
into	O
the	O
mdp	O
framework	O
identifying	O
for	O
each	O
its	O
states	O
actions	O
and	O
rewards	O
make	O
the	O
three	O
examples	O
as	O
different	O
from	O
each	O
other	O
as	O
possible	O
the	O
framework	O
is	O
abstract	O
and	O
flexible	O
and	O
can	O
be	O
applied	O
in	O
many	O
different	O
ways	O
stretch	O
its	O
limits	O
in	O
some	O
way	O
in	O
at	O
least	O
one	O
of	O
your	O
examples	O
exercise	O
is	O
the	O
mdp	O
framework	O
adequate	O
to	O
usefully	O
represent	O
all	O
goal-directed	O
learning	O
tasks	O
can	O
you	O
think	O
of	O
any	O
clear	O
exceptions	O
exercise	O
consider	O
the	O
problem	O
of	O
driving	O
you	O
could	O
define	O
the	O
actions	O
in	O
terms	O
of	O
the	O
accelerator	O
steering	O
wheel	O
and	O
brake	O
that	O
is	O
where	O
your	O
body	O
meets	O
the	O
machine	O
or	O
you	O
could	O
define	O
them	O
farther	O
out	O
say	O
where	O
the	O
rubber	O
meets	O
the	O
road	O
considering	O
your	O
actions	O
to	O
be	O
tire	O
torques	O
or	O
you	O
could	O
define	O
them	O
farther	O
in	O
say	O
where	O
your	O
brain	O
meets	O
your	O
body	O
the	O
actions	O
being	O
muscle	O
twitches	O
to	O
control	B
your	O
limbs	O
or	O
you	O
could	O
go	O
to	O
a	O
really	O
high	O
level	O
and	O
say	O
that	O
your	O
actions	O
are	O
your	O
choices	O
of	O
where	O
to	O
drive	O
what	O
is	O
the	O
right	O
level	O
the	O
right	O
place	O
to	O
draw	O
the	O
line	O
between	O
agent	O
and	O
environment	B
on	O
what	O
basis	O
is	O
one	O
location	O
of	O
the	O
line	O
to	O
be	O
preferred	O
over	O
another	O
is	O
there	O
any	O
fundamental	O
reason	O
for	O
preferring	O
one	O
location	O
over	O
another	O
or	O
is	O
it	O
a	O
free	O
choice	O
chapter	O
finite	O
markov	O
decision	O
processes	O
example	O
recycling	O
robot	O
a	O
mobile	O
robot	O
has	O
the	O
job	O
of	O
collecting	O
empty	O
soda	O
cans	O
in	O
an	O
office	O
environment	B
it	O
has	O
sensors	O
for	O
detecting	O
cans	O
and	O
an	O
arm	O
and	O
gripper	O
that	O
can	O
pick	O
them	O
up	O
and	O
place	O
them	O
in	O
an	O
onboard	O
bin	O
it	O
runs	O
on	O
a	O
rechargeable	O
battery	O
the	O
robot	O
s	O
control	B
system	O
has	O
components	O
for	O
interpreting	O
sensory	O
information	O
for	O
navigating	O
and	O
for	O
controlling	O
the	O
arm	O
and	O
gripper	O
high-level	O
decisions	O
about	O
how	O
to	O
search	O
for	O
cans	O
are	O
made	O
by	O
a	O
reinforcement	B
learning	I
agent	O
based	O
on	O
the	O
current	O
charge	O
level	O
of	O
the	O
battery	O
to	O
make	O
a	O
simple	O
example	O
we	O
assume	O
that	O
only	O
two	O
charge	O
levels	O
can	O
be	O
distinguished	O
comprising	O
a	O
small	O
state	B
set	O
s	O
low	O
in	O
each	O
state	B
the	O
agent	O
can	O
decide	O
whether	O
to	O
actively	O
search	O
for	O
a	O
can	O
for	O
a	O
certain	O
period	O
of	O
time	O
remain	O
stationary	O
and	O
wait	O
for	O
someone	O
to	O
bring	O
it	O
a	O
can	O
or	O
head	O
back	O
to	O
its	O
home	O
base	O
to	O
recharge	O
its	O
battery	O
when	O
the	O
energy	O
level	O
is	O
high	O
recharging	O
would	O
always	O
be	O
foolish	O
so	O
we	O
do	O
not	O
include	O
it	O
in	O
the	O
action	B
set	O
for	O
this	O
state	B
the	O
action	B
sets	O
are	O
then	O
ahigh	O
wait	O
and	O
alow	O
wait	O
recharge	O
the	O
rewards	O
are	O
zero	O
most	O
of	O
the	O
time	O
but	O
become	O
positive	O
when	O
the	O
robot	O
secures	O
an	O
empty	O
can	O
or	O
large	O
and	O
negative	O
if	O
the	O
battery	O
runs	O
all	O
the	O
way	O
down	O
the	O
best	O
way	O
to	O
find	O
cans	O
is	O
to	O
actively	O
search	O
for	O
them	O
but	O
this	O
runs	O
down	O
the	O
robot	O
s	O
battery	O
whereas	O
waiting	O
does	O
not	O
whenever	O
the	O
robot	O
is	O
searching	O
the	O
possibility	O
exists	O
that	O
its	O
battery	O
will	O
become	O
depleted	O
in	O
this	O
case	O
the	O
robot	O
must	O
shut	O
down	O
and	O
wait	O
to	O
be	O
rescued	O
a	O
low	O
reward	O
if	O
the	O
energy	O
level	O
is	O
high	O
then	O
a	O
period	O
of	O
active	O
search	O
can	O
always	O
be	O
completed	O
without	O
risk	O
of	O
depleting	O
the	O
battery	O
a	O
period	O
of	O
searching	O
that	O
begins	O
with	O
a	O
high	O
energy	O
level	O
leaves	O
the	O
energy	O
level	O
high	O
with	O
probability	O
and	O
reduces	O
it	O
to	O
low	O
with	O
probability	O
on	O
the	O
other	O
hand	O
a	O
period	O
of	O
searching	O
undertaken	O
when	O
the	O
energy	O
level	O
is	O
low	O
leaves	O
it	O
low	O
with	O
probability	O
and	O
depletes	O
the	O
battery	O
with	O
probability	O
in	O
the	O
latter	O
case	O
the	O
robot	O
must	O
be	O
rescued	O
and	O
the	O
battery	O
is	O
then	O
recharged	O
back	O
to	O
high	O
each	O
can	O
collected	O
by	O
the	O
robot	O
counts	O
as	O
a	O
unit	O
reward	O
whereas	O
a	O
reward	O
of	O
results	O
whenever	O
the	O
robot	O
has	O
to	O
be	O
rescued	O
let	O
rsearch	O
and	O
rwait	O
with	O
rsearch	O
rwait	O
respectively	O
denote	O
the	O
expected	B
number	O
of	O
cans	O
the	O
robot	O
will	O
collect	O
hence	O
the	O
expected	B
reward	O
while	O
searching	O
and	O
while	O
waiting	O
finally	O
suppose	O
that	O
no	O
cans	O
can	O
be	O
collected	O
during	O
a	O
run	O
home	O
for	O
recharging	O
and	O
that	O
no	O
cans	O
can	O
be	O
collected	O
on	O
a	O
step	O
in	O
which	O
the	O
battery	O
is	O
depleted	O
this	O
system	O
is	O
then	O
a	O
finite	O
mdp	O
and	O
we	O
can	O
write	O
down	O
the	O
transition	B
probabilities	I
and	O
the	O
expected	B
rewards	O
with	O
dynamics	O
as	O
indicated	O
in	O
the	O
table	O
on	O
the	O
left	O
s	O
high	O
high	O
low	O
low	O
high	O
high	O
low	O
low	O
low	O
low	O
a	O
search	O
search	O
search	O
search	O
wait	O
wait	O
wait	O
wait	O
recharge	O
recharge	O
high	O
low	O
high	O
low	O
high	O
low	O
high	O
low	O
high	O
low	O
a	O
rs	O
a	O
rsearch	O
rsearch	O
rsearch	O
rwait	O
rwait	O
rwait	O
rwait	O
note	O
that	O
there	O
is	O
a	O
row	O
in	O
the	O
table	O
for	O
each	O
possible	O
combination	O
of	O
current	O
state	B
s	O
action	B
a	O
as	O
and	O
next	O
state	B
another	O
useful	O
way	O
of	O
summarizing	O
the	O
dynamics	O
of	O
a	O
finite	O
mdp	O
is	O
as	O
a	O
transition	O
graph	O
as	O
shown	O
above	O
on	O
the	O
right	O
there	O
are	O
two	O
kinds	O
of	O
goals	O
and	O
rewards	O
nodes	O
state	B
nodes	O
and	O
action	B
nodes	O
there	O
is	O
a	O
state	B
node	O
for	O
each	O
possible	O
state	B
large	O
open	O
circle	O
labeled	O
by	O
the	O
name	O
of	O
the	O
state	B
and	O
an	O
action	B
node	O
for	O
each	O
state	B
action	B
pair	O
small	O
solid	O
circle	O
labeled	O
by	O
the	O
name	O
of	O
the	O
action	B
and	O
connected	O
by	O
a	O
line	O
to	O
the	O
state	B
node	O
starting	O
in	O
state	B
s	O
and	O
taking	O
action	B
a	O
moves	O
you	O
along	O
the	O
line	O
from	O
state	B
node	O
s	O
to	O
action	B
node	O
a	O
then	O
the	O
environment	B
responds	O
with	O
a	O
transition	O
to	O
the	O
next	O
state	B
s	O
node	O
via	O
one	O
of	O
the	O
arrows	O
leaving	O
action	B
node	O
a	O
each	O
arrow	O
corresponds	O
to	O
a	O
triple	O
a	O
where	O
is	O
the	O
next	O
state	B
and	O
we	O
label	O
the	O
arrow	O
with	O
the	O
transition	O
probability	O
a	O
and	O
the	O
expected	B
reward	O
for	O
that	O
transition	O
rs	O
a	O
note	O
that	O
the	O
transition	B
probabilities	I
labeling	O
the	O
arrows	O
leaving	O
an	O
action	B
node	O
always	O
sum	O
to	O
exercise	O
give	O
a	O
table	O
analogous	O
to	O
that	O
in	O
example	O
but	O
for	O
rs	O
a	O
it	O
should	O
have	O
columns	O
for	O
s	O
a	O
r	O
and	O
rs	O
a	O
and	O
a	O
row	O
for	O
every	O
for	O
which	O
rs	O
a	O
goals	O
and	O
rewards	O
in	O
reinforcement	B
learning	I
the	O
purpose	O
or	O
goal	O
of	O
the	O
agent	O
is	O
formalized	O
in	O
terms	O
of	O
a	O
special	O
signal	O
called	O
the	O
reward	O
passing	O
from	O
the	O
environment	B
to	O
the	O
agent	O
at	O
each	O
time	O
step	O
the	O
reward	O
is	O
a	O
simple	O
number	O
rt	O
r	O
informally	O
the	O
agent	O
s	O
goal	O
is	O
to	O
maximize	O
the	O
total	O
amount	O
of	O
reward	O
it	O
receives	O
this	O
means	O
maximizing	O
not	O
immediate	O
reward	O
but	O
cumulative	O
reward	O
in	O
the	O
long	O
run	O
we	O
can	O
clearly	O
state	B
this	O
informal	O
idea	O
as	O
the	O
reward	O
hypothesis	O
that	O
all	O
of	O
what	O
we	O
mean	O
by	O
goals	O
and	O
purposes	O
can	O
be	O
well	O
thought	O
of	O
as	O
the	O
maximization	O
of	O
the	O
expected	B
value	B
of	O
the	O
cumulative	O
sum	O
of	O
a	O
received	O
scalar	O
signal	O
reward	O
the	O
use	O
of	O
a	O
reward	B
signal	I
to	O
formalize	O
the	O
idea	O
of	O
a	O
goal	O
is	O
one	O
of	O
the	O
most	O
distinctive	O
features	O
of	O
reinforcement	B
learning	I
although	O
formulating	O
goals	O
in	O
terms	O
of	O
reward	O
signals	O
might	O
at	O
first	O
appear	O
limiting	O
in	O
practice	O
it	O
has	O
proved	O
to	O
be	O
flexible	O
and	O
widely	O
applicable	O
the	O
best	O
way	O
to	O
see	O
this	O
is	O
to	O
consider	O
examples	O
of	O
how	O
it	O
has	O
been	O
or	O
could	O
be	O
used	O
for	O
example	O
to	O
make	O
a	O
robot	O
learn	O
to	O
walk	O
researchers	O
have	O
provided	O
reward	O
on	O
each	O
time	O
step	O
proportional	O
to	O
the	O
robot	O
s	O
forward	O
motion	O
in	O
making	O
a	O
robot	O
learn	O
how	O
to	O
escape	O
from	O
a	O
maze	O
the	O
reward	O
is	O
often	O
for	O
every	O
time	O
step	O
that	O
passes	O
prior	O
to	O
escape	O
this	O
encourages	O
the	O
agent	O
to	O
escape	O
as	O
quickly	O
as	O
possible	O
to	O
make	O
a	O
robot	O
learn	O
to	O
find	O
and	O
collect	O
empty	O
soda	O
cans	O
for	O
recycling	O
one	O
might	O
give	O
it	O
a	O
reward	O
of	O
zero	O
most	O
of	O
the	O
time	O
and	O
then	O
a	O
reward	O
of	O
for	O
each	O
can	O
collected	O
one	O
might	O
also	O
want	O
to	O
give	O
the	O
robot	O
negative	O
rewards	O
when	O
it	O
bumps	O
into	O
things	O
or	O
when	O
somebody	O
yells	O
at	O
it	O
for	O
an	O
agent	O
to	O
learn	O
to	O
play	O
checkers	O
or	O
chess	B
the	O
natural	O
rewards	O
are	O
for	O
winning	O
for	O
losing	O
and	O
for	O
drawing	O
and	O
for	O
all	O
nonterminal	O
positions	O
you	O
can	O
see	O
what	O
is	O
happening	O
in	O
all	O
of	O
these	O
examples	O
the	O
agent	O
always	O
learns	O
to	O
maximize	O
its	O
reward	O
if	O
we	O
want	O
it	O
to	O
do	O
something	O
for	O
us	O
we	O
must	O
provide	O
rewards	O
to	O
it	O
in	O
such	O
a	O
way	O
that	O
in	O
maximizing	O
them	O
the	O
agent	O
will	O
also	O
achieve	O
our	O
goals	O
it	O
is	O
thus	O
critical	O
that	O
the	O
rewards	O
we	O
set	O
up	O
truly	O
indicate	O
what	O
we	O
want	O
accomplished	O
chapter	O
finite	O
markov	O
decision	O
processes	O
in	O
particular	O
the	O
reward	B
signal	I
is	O
not	O
the	O
place	O
to	O
impart	O
to	O
the	O
agent	O
prior	B
knowledge	I
about	O
how	O
to	O
achieve	O
what	O
we	O
want	O
it	O
to	O
for	O
example	O
a	O
chess-playing	O
agent	O
should	O
be	O
rewarded	O
only	O
for	O
actually	O
winning	O
not	O
for	O
achieving	O
subgoals	O
such	O
as	O
taking	O
its	O
opponent	O
s	O
pieces	O
or	O
gaining	O
control	B
of	O
the	O
center	O
of	O
the	O
board	O
if	O
achieving	O
these	O
sorts	O
of	O
subgoals	O
were	O
rewarded	O
then	O
the	O
agent	O
might	O
find	O
a	O
way	O
to	O
achieve	O
them	O
without	O
achieving	O
the	O
real	O
goal	O
for	O
example	O
it	O
might	O
find	O
a	O
way	O
to	O
take	O
the	O
opponent	O
s	O
pieces	O
even	O
at	O
the	O
cost	O
of	O
losing	O
the	O
game	O
the	O
reward	B
signal	I
is	O
your	O
way	O
of	O
communicating	O
to	O
the	O
robot	O
what	O
you	O
want	O
it	O
to	O
achieve	O
not	O
how	O
you	O
want	O
it	O
returns	O
and	O
episodes	B
so	O
far	O
we	O
have	O
discussed	O
the	O
objective	O
of	O
learning	O
informally	O
we	O
have	O
said	O
that	O
the	O
agent	O
s	O
goal	O
is	O
to	O
maximize	O
the	O
cumulative	O
reward	O
it	O
receives	O
in	O
the	O
long	O
run	O
how	O
might	O
this	O
be	O
defined	O
formally	O
if	O
the	O
sequence	O
of	O
rewards	O
received	O
after	O
time	O
step	O
t	O
is	O
denoted	O
then	O
what	O
precise	O
aspect	O
of	O
this	O
sequence	O
do	O
we	O
wish	O
to	O
maximize	O
in	O
general	O
we	O
seek	O
to	O
maximize	O
the	O
expected	B
return	B
where	O
the	O
return	B
denoted	O
gt	O
is	O
defined	O
as	O
some	O
specific	O
function	O
of	O
the	O
reward	O
sequence	O
in	O
the	O
simplest	O
case	O
the	O
return	B
is	O
the	O
sum	O
of	O
the	O
rewards	O
gt	O
rt	O
where	O
t	O
is	O
a	O
final	O
time	O
step	O
this	O
approach	O
makes	O
sense	O
in	O
applications	O
in	O
which	O
there	O
is	O
a	O
natural	O
notion	O
of	O
final	O
time	O
step	O
that	O
is	O
when	O
the	O
agent	O
environment	B
interaction	O
breaks	O
naturally	O
into	O
subsequences	O
which	O
we	O
call	O
such	O
as	O
plays	O
of	O
a	O
game	O
trips	O
through	O
a	O
maze	O
or	O
any	O
sort	O
of	O
repeated	O
interaction	O
each	O
episode	O
ends	O
in	O
a	O
special	O
state	B
called	O
the	O
terminal	O
state	B
followed	O
by	O
a	O
reset	O
to	O
a	O
standard	O
starting	O
state	B
or	O
to	O
a	O
sample	O
from	O
a	O
standard	O
distribution	O
of	O
starting	O
states	O
even	O
if	O
you	O
think	O
of	O
episodes	B
as	O
ending	O
in	O
different	O
ways	O
such	O
as	O
winning	O
and	O
losing	O
a	O
game	O
the	O
next	O
episode	O
begins	O
independently	O
of	O
how	O
the	O
previous	O
one	O
ended	O
thus	O
the	O
episodes	B
can	O
all	O
be	O
considered	O
to	O
end	O
in	O
the	O
same	O
terminal	O
state	B
with	O
different	O
rewards	O
for	O
the	O
different	O
outcomes	O
tasks	O
with	O
episodes	B
of	O
this	O
kind	O
are	O
called	O
episodic	O
tasks	O
in	O
episodic	O
tasks	O
we	O
sometimes	O
need	O
to	O
distinguish	O
the	O
set	O
of	O
all	O
nonterminal	O
states	O
denoted	O
s	O
from	O
the	O
set	O
of	O
all	O
states	O
plus	O
the	O
terminal	O
state	B
denoted	O
s	O
the	O
time	O
of	O
termination	O
t	O
is	O
a	O
random	O
variable	O
that	O
normally	O
varies	O
from	O
episode	O
to	O
episode	O
on	O
the	O
other	O
hand	O
in	O
many	O
cases	O
the	O
agent	O
environment	B
interaction	O
does	O
not	O
break	O
naturally	O
into	O
identifiable	O
episodes	B
but	O
goes	O
on	O
continually	O
without	O
limit	O
for	O
example	O
this	O
would	O
be	O
the	O
natural	O
way	O
to	O
formulate	O
an	O
on-going	O
process-control	O
task	O
or	O
an	O
application	O
to	O
a	O
robot	O
with	O
a	O
long	O
life	O
span	O
we	O
call	O
these	O
continuing	B
tasks	I
the	O
return	B
formulation	O
is	O
problematic	O
for	O
continuing	B
tasks	I
because	O
the	O
final	O
time	O
step	O
would	O
be	O
t	O
and	O
the	O
return	B
which	O
is	O
what	O
we	O
are	O
trying	O
to	O
maximize	O
could	O
itself	O
easily	O
places	O
for	O
imparting	O
this	O
kind	O
of	O
prior	B
knowledge	I
are	O
the	O
initial	O
policy	B
or	O
initial	O
value	B
function	I
or	O
in	O
influences	O
on	O
these	O
delves	O
further	O
into	O
the	O
issue	O
of	O
designing	O
effective	O
reward	O
signals	O
are	O
sometimes	O
called	O
trials	O
in	O
the	O
literature	O
returns	O
and	O
episodes	B
be	O
infinite	O
example	O
suppose	O
the	O
agent	O
receives	O
a	O
reward	O
of	O
at	O
each	O
time	O
step	O
thus	O
in	O
this	O
book	O
we	O
usually	O
use	O
a	O
definition	O
of	O
return	B
that	O
is	O
slightly	O
more	O
complex	O
conceptually	O
but	O
much	O
simpler	O
mathematically	O
the	O
additional	O
concept	O
that	O
we	O
need	O
is	O
that	O
of	O
discounting	B
according	O
to	O
this	O
approach	O
the	O
agent	O
tries	O
to	O
select	O
actions	O
so	O
that	O
the	O
sum	O
of	O
the	O
discounted	O
rewards	O
it	O
receives	O
over	O
the	O
future	O
is	O
maximized	O
in	O
particular	O
it	O
chooses	O
at	O
to	O
maximize	O
the	O
expected	B
discounted	O
return	B
gt	O
where	O
is	O
a	O
parameter	O
called	O
the	O
discount	O
rate	O
the	O
discount	O
rate	O
determines	O
the	O
present	O
value	B
of	O
future	O
rewards	O
a	O
reward	O
received	O
k	O
time	O
steps	O
in	O
the	O
future	O
is	O
worth	O
only	O
k	O
times	O
what	O
it	O
would	O
be	O
worth	O
if	O
it	O
were	O
received	O
immediately	O
if	O
the	O
infinite	O
sum	O
in	O
has	O
a	O
finite	O
value	B
as	O
long	O
as	O
the	O
reward	O
sequence	O
is	O
bounded	O
if	O
the	O
agent	O
is	O
myopic	O
in	O
being	O
concerned	O
its	O
objective	O
in	O
this	O
case	O
is	O
to	O
learn	O
how	O
to	O
only	O
with	O
maximizing	O
immediate	O
rewards	O
choose	O
at	O
so	O
as	O
to	O
maximize	O
only	O
if	O
each	O
of	O
the	O
agent	O
s	O
actions	O
happened	O
to	O
influence	O
only	O
the	O
immediate	O
reward	O
not	O
future	O
rewards	O
as	O
well	O
then	O
a	O
myopic	O
agent	O
could	O
maximize	O
by	O
separately	O
maximizing	O
each	O
immediate	O
reward	O
but	O
in	O
general	O
acting	O
to	O
maximize	O
immediate	O
reward	O
can	O
reduce	O
access	O
to	O
future	O
rewards	O
so	O
that	O
the	O
return	B
is	O
reduced	O
as	O
approaches	O
the	O
return	B
objective	O
takes	O
future	O
rewards	O
into	O
account	O
more	O
strongly	O
the	O
agent	O
becomes	O
more	O
farsighted	O
returns	O
at	O
successive	O
time	O
steps	O
are	O
related	O
to	O
each	O
other	O
in	O
a	O
way	O
that	O
is	O
important	O
for	O
the	O
theory	O
and	O
algorithms	O
of	O
reinforcement	B
learning	I
gt	O
note	O
that	O
this	O
works	O
for	O
all	O
time	O
steps	O
t	O
t	O
even	O
if	O
termination	O
occurs	O
at	O
t	O
if	O
we	O
define	O
gt	O
this	O
often	O
makes	O
it	O
easy	O
to	O
compute	O
returns	O
from	O
reward	O
sequences	O
note	O
that	O
although	O
the	O
return	B
is	O
a	O
sum	O
of	O
an	O
infinite	O
number	O
of	O
terms	O
it	O
is	O
still	O
finite	O
if	O
the	O
reward	O
is	O
nonzero	O
and	O
constant	O
if	O
for	O
example	O
if	O
the	O
reward	O
is	O
a	O
constant	O
then	O
the	O
return	B
is	O
gt	O
k	O
exercise	O
the	O
equations	O
in	O
section	O
are	O
for	O
the	O
continuing	O
case	O
and	O
need	O
to	O
be	O
modified	O
slightly	O
to	O
apply	O
to	O
episodic	O
tasks	O
show	O
that	O
you	O
know	O
the	O
modifications	O
needed	O
by	O
giving	O
the	O
modified	O
version	O
of	O
chapter	O
finite	O
markov	O
decision	O
processes	O
example	O
pole-balancing	O
the	O
objective	O
in	O
this	O
task	O
is	O
to	O
apply	O
forces	O
to	O
a	O
cart	O
moving	O
along	O
a	O
track	O
so	O
as	O
to	O
keep	O
a	O
pole	O
hinged	O
to	O
the	O
cart	O
from	O
falling	O
over	O
a	O
failure	O
is	O
said	O
to	O
occur	O
if	O
the	O
pole	O
falls	O
past	O
a	O
given	O
angle	O
from	O
vertical	O
or	O
if	O
the	O
cart	O
runs	O
off	O
the	O
track	O
the	O
pole	O
is	O
reset	O
to	O
vertical	O
after	O
each	O
failure	O
this	O
task	O
could	O
be	O
treated	O
as	O
episodic	O
where	O
the	O
natural	O
episodes	B
are	O
the	O
repeated	O
attempts	O
to	O
balance	O
the	O
pole	O
the	O
reward	O
in	O
this	O
case	O
could	O
be	O
for	O
every	O
time	O
step	O
on	O
which	O
failure	O
did	O
not	O
occur	O
so	O
that	O
the	O
return	B
at	O
each	O
time	O
would	O
be	O
the	O
number	O
of	O
steps	O
until	O
failure	O
in	O
this	O
case	O
successful	O
balancing	O
forever	O
would	O
mean	O
a	O
return	B
of	O
infinity	O
alternatively	O
we	O
could	O
treat	O
pole-balancing	O
as	O
a	O
continuing	O
task	O
using	O
discounting	B
in	O
this	O
case	O
the	O
reward	O
would	O
be	O
on	O
each	O
failure	O
and	O
zero	O
at	O
all	O
other	O
times	O
the	O
return	B
at	O
each	O
time	O
would	O
then	O
be	O
related	O
to	O
k	O
where	O
k	O
is	O
the	O
number	O
of	O
time	O
steps	O
before	O
failure	O
in	O
either	O
case	O
the	O
return	B
is	O
maximized	O
by	O
keeping	O
the	O
pole	O
balanced	O
for	O
as	O
long	O
as	O
possible	O
exercise	O
suppose	O
you	O
treated	O
pole-balancing	O
as	O
an	O
episodic	O
task	O
but	O
also	O
used	O
discounting	B
with	O
all	O
rewards	O
zero	O
except	O
for	O
upon	O
failure	O
what	O
then	O
would	O
the	O
return	B
be	O
at	O
each	O
time	O
how	O
does	O
this	O
return	B
differ	O
from	O
that	O
in	O
the	O
discounted	O
continuing	O
formulation	O
of	O
this	O
task	O
exercise	O
imagine	O
that	O
you	O
are	O
designing	O
a	O
robot	O
to	O
run	O
a	O
maze	O
you	O
decide	O
to	O
give	O
it	O
a	O
reward	O
of	O
for	O
escaping	O
from	O
the	O
maze	O
and	O
a	O
reward	O
of	O
zero	O
at	O
all	O
other	O
times	O
the	O
task	O
seems	O
to	O
break	O
down	O
naturally	O
into	O
episodes	B
the	O
successive	O
runs	O
through	O
the	O
maze	O
so	O
you	O
decide	O
to	O
treat	O
it	O
as	O
an	O
episodic	O
task	O
where	O
the	O
goal	O
is	O
to	O
maximize	O
expected	B
total	O
reward	O
after	O
running	O
the	O
learning	O
agent	O
for	O
a	O
while	O
you	O
find	O
that	O
it	O
is	O
showing	O
no	O
improvement	O
in	O
escaping	O
from	O
the	O
maze	O
what	O
is	O
going	O
wrong	O
have	O
you	O
effectively	O
communicated	O
to	O
the	O
agent	O
what	O
you	O
want	O
it	O
to	O
achieve	O
exercise	O
suppose	O
and	O
the	O
following	O
sequence	O
of	O
rewards	O
is	O
received	O
and	O
with	O
t	O
what	O
are	O
hint	O
work	O
backwards	O
exercise	O
suppose	O
and	O
the	O
reward	O
sequence	O
is	O
followed	O
by	O
an	O
infinite	O
sequence	O
of	O
what	O
are	O
and	O
exercise	O
prove	O
unified	O
notation	O
for	O
episodic	O
and	O
continuing	B
tasks	I
unified	O
notation	O
for	O
episodic	O
and	O
continuing	B
tasks	I
in	O
the	O
preceding	O
section	O
we	O
described	O
two	O
kinds	O
of	O
reinforcement	B
learning	I
tasks	O
one	O
in	O
which	O
the	O
agent	O
environment	B
interaction	O
naturally	O
breaks	O
down	O
into	O
a	O
sequence	O
of	O
separate	O
episodes	B
tasks	O
and	O
one	O
in	O
which	O
it	O
does	O
not	O
tasks	O
the	O
former	O
case	O
is	O
mathematically	O
easier	O
because	O
each	O
action	B
affects	O
only	O
the	O
finite	O
number	O
of	O
rewards	O
subsequently	O
received	O
during	O
the	O
episode	O
in	O
this	O
book	O
we	O
consider	O
sometimes	O
one	O
kind	O
of	O
problem	O
and	O
sometimes	O
the	O
other	O
but	O
often	O
both	O
it	O
is	O
therefore	O
useful	O
to	O
establish	O
one	O
notation	O
that	O
enables	O
us	O
to	O
talk	O
precisely	O
about	O
both	O
cases	O
simultaneously	O
to	O
be	O
precise	O
about	O
episodic	O
tasks	O
requires	O
some	O
additional	O
notation	O
rather	O
than	O
one	O
long	O
sequence	O
of	O
time	O
steps	O
we	O
need	O
to	O
consider	O
a	O
series	O
of	O
episodes	B
each	O
of	O
which	O
consists	O
of	O
a	O
finite	O
sequence	O
of	O
time	O
steps	O
we	O
number	O
the	O
time	O
steps	O
of	O
each	O
episode	O
starting	O
anew	O
from	O
zero	O
therefore	O
we	O
have	O
to	O
refer	O
not	O
just	O
to	O
st	O
the	O
state	B
representation	O
at	O
time	O
t	O
but	O
to	O
sti	O
the	O
state	B
representation	O
at	O
time	O
t	O
of	O
episode	O
i	O
similarly	O
for	O
ati	O
rti	O
ti	O
ti	O
etc	O
however	O
it	O
turns	O
out	O
that	O
when	O
we	O
discuss	O
episodic	O
tasks	O
we	O
almost	O
never	O
have	O
to	O
distinguish	O
between	O
different	O
episodes	B
we	O
are	O
almost	O
always	O
considering	O
a	O
particular	O
single	O
episode	O
or	O
stating	O
something	O
that	O
is	O
true	O
for	O
all	O
episodes	B
accordingly	O
in	O
practice	O
we	O
almost	O
always	O
abuse	O
notation	O
slightly	O
by	O
dropping	O
the	O
explicit	O
reference	O
to	O
episode	O
number	O
that	O
is	O
we	O
write	O
st	O
to	O
refer	O
to	O
sti	O
and	O
so	O
on	O
we	O
need	O
one	O
other	O
convention	O
to	O
obtain	O
a	O
single	O
notation	O
that	O
covers	O
both	O
episodic	O
and	O
continuing	B
tasks	I
we	O
have	O
defined	O
the	O
return	B
as	O
a	O
sum	O
over	O
a	O
finite	O
number	O
of	O
terms	O
in	O
one	O
case	O
and	O
as	O
a	O
sum	O
over	O
an	O
infinite	O
number	O
of	O
terms	O
in	O
the	O
other	O
these	O
can	O
be	O
unified	O
by	O
considering	O
episode	O
termination	O
to	O
be	O
the	O
entering	O
of	O
a	O
special	O
absorbing	B
state	B
that	O
transitions	O
only	O
to	O
itself	O
and	O
that	O
generates	O
only	O
rewards	O
of	O
zero	O
for	O
example	O
consider	O
the	O
state	B
transition	O
diagram	O
here	O
the	O
solid	O
square	O
represents	O
the	O
special	O
absorbing	B
state	B
corresponding	O
to	O
the	O
end	O
of	O
an	O
episode	O
starting	O
from	O
we	O
get	O
the	O
reward	O
sequence	O
summing	O
these	O
we	O
get	O
the	O
same	O
return	B
whether	O
we	O
sum	O
over	O
the	O
first	O
t	O
rewards	O
t	O
or	O
over	O
the	O
full	O
infinite	O
sequence	O
this	O
remains	O
true	O
even	O
if	O
we	O
introduce	O
discounting	B
thus	O
we	O
can	O
define	O
the	O
return	B
in	O
general	O
according	O
to	O
using	O
the	O
convention	O
of	O
omitting	O
episode	O
numbers	O
when	O
they	O
are	O
not	O
needed	O
and	O
including	O
the	O
possibility	O
that	O
if	O
the	O
sum	O
remains	O
defined	O
because	O
all	O
episodes	B
terminate	O
alternatively	O
we	O
can	O
also	O
write	O
the	O
return	B
as	O
gt	O
k	O
t	O
including	O
the	O
possibility	O
that	O
t	O
or	O
not	O
both	O
we	O
use	O
these	O
conventions	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
to	O
simplify	O
notation	O
and	O
to	O
express	O
the	O
close	O
parallels	O
between	O
episodic	O
and	O
continuing	B
tasks	I
in	O
chapter	O
we	O
will	O
introduce	O
a	O
chapter	O
finite	O
markov	O
decision	O
processes	O
formulation	O
that	O
is	O
both	O
continuing	O
and	O
undiscounted	O
policies	O
and	O
value	B
functions	O
almost	O
all	O
reinforcement	B
learning	I
algorithms	O
involve	O
estimating	O
value	B
functions	O
functions	O
of	O
states	O
of	O
state	B
action	B
pairs	O
that	O
estimate	O
how	O
good	O
it	O
is	O
for	O
the	O
agent	O
to	O
be	O
in	O
a	O
given	O
state	B
how	O
good	O
it	O
is	O
to	O
perform	O
a	O
given	O
action	B
in	O
a	O
given	O
state	B
the	O
notion	O
of	O
how	O
good	O
here	O
is	O
defined	O
in	O
terms	O
of	O
future	O
rewards	O
that	O
can	O
be	O
expected	B
or	O
to	O
be	O
precise	O
in	O
terms	O
of	O
expected	B
return	B
of	O
course	O
the	O
rewards	O
the	O
agent	O
can	O
expect	O
to	O
receive	O
in	O
the	O
future	O
depend	O
on	O
what	O
actions	O
it	O
will	O
take	O
accordingly	O
value	B
functions	O
are	O
defined	O
with	O
respect	O
to	O
particular	O
ways	O
of	O
acting	O
called	O
policies	O
formally	O
a	O
policy	B
is	O
a	O
mapping	O
from	O
states	O
to	O
probabilities	O
of	O
selecting	O
each	O
possible	O
action	B
if	O
the	O
agent	O
is	O
following	O
policy	B
at	O
time	O
t	O
then	O
is	O
the	O
probability	O
that	O
at	O
a	O
if	O
st	O
s	O
like	O
p	O
is	O
an	O
ordinary	O
function	O
the	O
in	O
the	O
middle	O
of	O
merely	O
reminds	O
that	O
it	O
defines	O
a	O
probability	O
distribution	O
over	O
a	O
as	O
for	O
each	O
s	O
s	O
reinforcement	B
learning	I
methods	O
specify	O
how	O
the	O
agent	O
s	O
policy	B
is	O
changed	O
as	O
a	O
result	O
of	O
its	O
experience	O
exercise	O
if	O
the	O
current	O
state	B
is	O
st	O
and	O
actions	O
are	O
selected	O
according	O
to	O
stochastic	O
policy	B
then	O
what	O
is	O
the	O
expectation	O
of	O
in	O
terms	O
of	O
and	O
the	O
four-argument	O
function	O
p	O
the	O
value	B
function	I
of	O
a	O
state	B
s	O
under	O
a	O
policy	B
denoted	O
v	O
is	O
the	O
expected	B
return	B
when	O
starting	O
in	O
s	O
and	O
following	O
thereafter	O
for	O
mdps	O
we	O
can	O
define	O
v	O
formally	O
by	O
v	O
e	O
st	O
s	O
e	O
st	O
for	O
all	O
s	O
s	O
where	O
e	O
denotes	O
the	O
expected	B
value	B
of	O
a	O
random	O
variable	O
given	O
that	O
the	O
agent	O
follows	O
policy	B
and	O
t	O
is	O
any	O
time	O
step	O
note	O
that	O
the	O
value	B
of	O
the	O
terminal	O
state	B
if	O
any	O
is	O
always	O
zero	O
we	O
call	O
the	O
function	O
v	O
the	O
state-value	O
function	O
for	O
policy	B
similarly	O
we	O
define	O
the	O
value	B
of	O
taking	O
action	B
a	O
in	O
state	B
s	O
under	O
a	O
policy	B
denoted	O
q	O
a	O
as	O
the	O
expected	B
return	B
starting	O
from	O
s	O
taking	O
the	O
action	B
a	O
and	O
thereafter	O
following	O
policy	B
q	O
a	O
e	O
st	O
s	O
at	O
a	O
e	O
we	O
call	O
q	O
the	O
action-value	O
function	O
for	O
policy	B
st	O
s	O
at	O
the	O
value	B
functions	O
v	O
and	O
q	O
can	O
be	O
estimated	O
from	O
experience	O
for	O
example	O
if	O
an	O
agent	O
follows	O
policy	B
and	O
maintains	O
an	O
average	O
for	O
each	O
state	B
encountered	O
of	O
the	O
actual	O
returns	O
that	O
have	O
followed	O
that	O
state	B
then	O
the	O
average	O
will	O
converge	O
to	O
the	O
state	B
s	O
value	B
v	O
as	O
the	O
number	O
of	O
times	O
that	O
state	B
is	O
encountered	O
approaches	O
infinity	O
if	O
separate	O
averages	O
are	O
kept	O
for	O
each	O
action	B
taken	O
in	O
each	O
state	B
then	O
these	O
averages	O
will	O
similarly	O
converge	O
to	O
the	O
action	B
values	O
q	O
a	O
we	O
call	O
estimation	O
methods	O
of	O
this	O
kind	O
monte	B
carlo	I
methods	I
because	O
they	O
involve	O
averaging	O
over	O
many	O
random	O
samples	O
of	O
unified	O
notation	O
for	O
episodic	O
and	O
continuing	B
tasks	I
actual	O
returns	O
these	O
kinds	O
of	O
methods	O
are	O
presented	O
in	O
chapter	O
of	O
course	O
if	O
there	O
are	O
very	O
many	O
states	O
then	O
it	O
may	O
not	O
be	O
practical	O
to	O
keep	O
separate	O
averages	O
for	O
each	O
state	B
individually	O
instead	O
the	O
agent	O
would	O
have	O
to	O
maintain	O
v	O
and	O
q	O
as	O
parameterized	O
functions	O
fewer	O
parameters	O
than	O
states	O
and	O
adjust	O
the	O
parameters	O
to	O
better	O
match	O
the	O
observed	O
returns	O
this	O
can	O
also	O
produce	O
accurate	O
estimates	O
although	O
much	O
depends	O
on	O
the	O
nature	O
of	O
the	O
parameterized	O
function	O
approximator	O
these	O
possibilities	O
are	O
discussed	O
in	O
part	O
ii	O
of	O
the	O
book	O
a	O
fundamental	O
property	O
of	O
value	B
functions	O
used	O
throughout	O
reinforcement	B
learning	I
and	B
dynamic	B
programming	I
is	O
that	O
they	O
satisfy	O
recursive	O
relationships	O
similar	O
to	O
that	O
which	O
we	O
have	O
already	O
established	O
for	O
the	O
return	B
for	O
any	O
policy	B
and	O
any	O
state	B
s	O
the	O
following	O
consistency	O
condition	O
holds	O
between	O
the	O
value	B
of	O
s	O
and	O
the	O
value	B
of	O
its	O
possible	O
successor	O
states	O
v	O
e	O
st	O
s	O
e	O
st	O
s	O
rs	O
e	O
rs	O
v	O
for	O
all	O
s	O
s	O
where	O
it	O
is	O
implicit	O
that	O
the	O
actions	O
a	O
are	O
taken	O
from	O
the	O
set	O
as	O
that	O
the	O
next	O
states	O
are	O
taken	O
from	O
the	O
set	O
s	O
from	O
s	O
in	O
the	O
case	O
of	O
an	O
episodic	O
problem	O
and	O
that	O
the	O
rewards	O
r	O
are	O
taken	O
from	O
the	O
set	O
r	O
note	O
also	O
how	O
in	O
the	O
last	O
equation	O
we	O
have	O
merged	O
the	O
two	O
sums	O
one	O
over	O
all	O
the	O
values	O
of	O
and	O
the	O
other	O
over	O
all	O
the	O
values	O
of	O
r	O
into	O
one	O
sum	O
over	O
all	O
the	O
possible	O
values	O
of	O
both	O
we	O
use	O
this	O
kind	O
of	O
merged	O
sum	O
often	O
to	O
simplify	O
formulas	O
note	O
how	O
the	O
final	O
expression	O
can	O
be	O
read	O
easily	O
as	O
an	O
expected	B
value	B
it	O
is	O
really	O
a	O
sum	O
over	O
all	O
values	O
of	O
the	O
three	O
variables	O
a	O
and	O
r	O
for	O
each	O
triple	O
we	O
compute	O
its	O
probability	O
rs	O
a	O
weight	O
the	O
quantity	O
in	O
brackets	O
by	O
that	O
probability	O
then	O
sum	O
over	O
all	O
possibilities	O
to	O
get	O
an	O
expected	B
value	B
equation	O
is	O
the	O
bellman	B
equation	I
for	O
v	O
it	O
expresses	O
a	O
relationship	O
between	O
the	O
value	B
of	O
a	O
state	B
and	O
the	O
values	O
of	O
its	O
successor	O
states	O
think	O
of	O
looking	O
ahead	O
from	O
a	O
state	B
to	O
its	O
possible	O
successor	O
states	O
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
each	O
open	O
circle	O
represents	O
a	O
state	B
and	O
each	O
solid	O
circle	O
represents	O
a	O
state	B
action	B
pair	O
starting	O
from	O
state	B
s	O
the	O
root	O
node	O
at	O
the	O
top	O
the	O
agent	O
could	O
take	O
any	O
of	O
some	O
set	O
of	O
actions	O
three	O
are	O
shown	O
in	O
the	O
diagram	O
based	O
on	O
its	O
policy	B
from	O
each	O
of	O
these	O
the	O
environment	B
could	O
respond	O
with	O
one	O
of	O
several	O
next	O
states	O
are	O
shown	O
in	O
the	O
figure	O
along	O
with	O
a	O
reward	O
r	O
depending	O
on	O
its	O
dynamics	O
given	O
by	O
the	O
function	O
p	O
the	O
bellman	B
equation	I
averages	O
over	O
all	O
the	O
possibilities	O
weighting	O
each	O
by	O
its	O
probability	O
of	O
occurring	O
it	O
states	O
that	O
the	O
value	B
of	O
the	O
start	O
state	B
must	O
equal	O
the	O
value	B
of	O
the	O
expected	B
next	O
state	B
plus	O
the	O
reward	O
expected	B
along	O
the	O
way	O
backup	B
diagram	I
for	O
v	O
the	O
value	B
function	I
v	O
is	O
the	O
unique	O
solution	O
to	O
its	O
bellman	B
equation	I
we	O
show	O
in	O
rpa	O
chapter	O
finite	O
markov	O
decision	O
processes	O
subsequent	O
chapters	O
how	O
this	O
bellman	B
equation	I
forms	O
the	O
basis	O
of	O
a	O
number	O
of	O
ways	O
to	O
compute	O
approximate	O
and	O
learn	O
v	O
we	O
call	O
diagrams	O
like	O
that	O
above	O
backup	O
diagrams	O
because	O
they	O
diagram	O
relationships	O
that	O
form	O
the	O
basis	O
of	O
the	O
update	O
or	O
backup	O
operations	O
that	O
are	O
at	O
the	O
heart	O
of	O
reinforcement	B
learning	I
methods	O
these	O
operations	O
transfer	O
value	B
information	O
back	O
to	O
a	O
state	B
a	O
state	B
action	B
pair	O
from	O
its	O
successor	O
states	O
state	B
action	B
pairs	O
we	O
use	O
backup	O
diagrams	O
throughout	O
the	O
book	O
to	O
provide	O
graphical	O
summaries	O
of	O
the	O
algorithms	O
we	O
discuss	O
that	O
unlike	O
transition	O
graphs	O
the	O
state	B
nodes	O
of	O
backup	O
diagrams	O
do	O
not	O
necessarily	O
represent	O
distinct	O
states	O
for	O
example	O
a	O
state	B
might	O
be	O
its	O
own	O
successor	O
example	O
gridworld	O
figure	O
shows	O
a	O
rectangular	O
gridworld	O
representation	O
of	O
a	O
simple	O
finite	O
mdp	O
the	O
cells	O
of	O
the	O
grid	O
correspond	O
to	O
the	O
states	O
of	O
the	O
environment	B
at	O
each	O
cell	O
four	O
actions	O
are	O
possible	O
north	O
south	O
east	O
and	O
west	O
which	O
deterministically	O
cause	O
the	O
agent	O
to	O
move	O
one	O
cell	O
in	O
the	O
respective	O
direction	O
on	O
the	O
grid	O
actions	O
that	O
would	O
take	O
the	O
agent	O
off	O
the	O
grid	O
leave	O
its	O
location	O
unchanged	O
but	O
also	O
result	O
in	O
a	O
reward	O
of	O
other	O
actions	O
result	O
in	O
a	O
reward	O
of	O
except	O
those	O
that	O
move	O
the	O
agent	O
out	O
of	O
the	O
special	O
states	O
a	O
and	O
b	O
from	O
state	B
a	O
all	O
four	O
actions	O
yield	O
a	O
reward	O
of	O
and	O
take	O
the	O
agent	O
to	O
from	O
state	B
b	O
all	O
actions	O
yield	O
a	O
reward	O
of	O
and	O
take	O
the	O
agent	O
to	O
figure	O
gridworld	O
example	O
exceptional	O
reward	O
dynamics	O
and	O
state-value	O
function	O
for	O
the	O
equiprobable	O
random	O
policy	B
suppose	O
the	O
agent	O
selects	O
all	O
four	O
actions	O
with	O
equal	O
probability	O
in	O
all	O
states	O
figure	O
shows	O
the	O
value	B
function	I
v	O
for	O
this	O
policy	B
for	O
the	O
discounted	O
reward	O
case	O
with	O
this	O
value	B
function	I
was	O
computed	O
by	O
solving	O
the	O
system	O
of	O
linear	O
equations	O
notice	O
the	O
negative	O
values	O
near	O
the	O
lower	O
edge	O
these	O
are	O
the	O
result	O
of	O
the	O
high	O
probability	O
of	O
hitting	O
the	O
edge	O
of	O
the	O
grid	O
there	O
under	O
the	O
random	O
policy	B
state	B
a	O
is	O
the	O
best	O
state	B
to	O
be	O
in	O
under	O
this	O
policy	B
but	O
its	O
expected	B
return	B
is	O
less	O
than	O
its	O
immediate	O
reward	O
because	O
from	O
a	O
the	O
agent	O
is	O
taken	O
to	O
from	O
which	O
it	O
is	O
likely	O
to	O
run	O
into	O
the	O
edge	O
of	O
the	O
grid	O
state	B
b	O
on	O
the	O
other	O
hand	O
is	O
valued	O
more	O
than	O
its	O
immediate	O
reward	O
because	O
from	O
b	O
the	O
agent	O
is	O
taken	O
to	O
which	O
has	O
a	O
positive	O
value	B
from	O
the	O
expected	B
penalty	O
reward	O
for	O
possibly	O
running	O
into	O
an	O
edge	O
is	O
more	O
than	O
compensated	O
for	O
by	O
the	O
expected	B
gain	O
for	O
possibly	O
stumbling	O
onto	O
a	O
or	O
b	O
exercise	O
the	O
bellman	B
equation	I
must	O
hold	O
for	O
each	O
state	B
for	O
the	O
value	B
function	I
v	O
shown	O
in	O
figure	O
of	O
example	O
show	O
numerically	O
that	O
this	O
equation	O
holds	O
for	O
the	O
center	O
state	B
valued	O
at	O
with	O
respect	O
to	O
its	O
four	O
neighboring	O
states	O
valued	O
at	O
and	O
numbers	O
are	O
accurate	O
only	O
to	O
one	O
decimal	O
place	O
andbq	O
unified	O
notation	O
for	O
episodic	O
and	O
continuing	B
tasks	I
exercise	O
what	O
is	O
the	O
bellman	B
equation	I
for	B
action	B
values	I
that	O
is	O
for	O
q	O
it	O
must	O
give	O
the	O
action	B
value	B
q	O
a	O
in	O
terms	O
of	O
the	O
action	B
values	O
q	O
of	O
possible	O
successors	O
to	O
the	O
state	B
action	B
pair	O
a	O
hint	O
the	O
backup	B
diagram	I
to	O
the	O
right	O
corresponds	O
to	O
this	O
equation	O
show	O
the	O
sequence	O
of	O
equations	O
analogous	O
to	O
but	O
for	B
action	B
values	I
example	O
golf	O
to	O
formulate	O
playing	O
a	O
hole	O
of	O
golf	O
as	O
a	O
reinforcement	B
learning	I
task	O
we	O
count	O
a	O
penalty	O
reward	O
of	O
for	O
each	O
stroke	O
until	O
we	O
hit	O
the	O
ball	O
into	O
the	O
hole	O
the	O
state	B
is	O
the	O
location	O
of	O
the	O
ball	O
the	O
value	B
of	O
a	O
state	B
is	O
the	O
negative	O
of	O
the	O
number	O
of	O
strokes	O
to	O
the	O
hole	O
from	O
that	O
location	O
our	O
actions	O
are	O
how	O
we	O
aim	O
and	O
swing	O
at	O
the	O
ball	O
of	O
course	O
and	O
which	O
club	O
we	O
select	O
let	O
us	O
take	O
the	O
former	O
as	O
given	O
and	O
consider	O
just	O
the	O
choice	O
of	O
club	O
which	O
we	O
assume	O
is	O
either	O
a	O
putter	O
or	O
a	O
driver	O
the	O
upper	O
part	O
of	O
figure	O
shows	O
a	O
possible	O
state-value	O
function	O
vputts	O
for	O
the	O
policy	B
that	O
always	O
uses	O
the	O
putter	O
the	O
terminal	O
state	B
in-the-hole	O
has	O
a	O
value	B
of	O
from	O
anywhere	O
on	O
the	O
green	O
we	O
assume	O
we	O
can	O
make	O
a	O
putt	O
these	O
states	O
have	O
value	B
off	O
the	O
green	O
we	O
cannot	O
reach	O
the	O
hole	O
by	O
putting	O
and	O
the	O
value	B
is	O
greater	O
if	O
we	O
can	O
reach	O
the	O
green	O
from	O
a	O
state	B
by	O
putting	O
then	O
that	O
state	B
must	O
have	O
value	B
one	O
less	O
than	O
the	O
green	O
s	O
value	B
that	O
is	O
for	O
simplicity	O
let	O
us	O
assume	O
we	O
can	O
putt	O
very	O
precisely	O
and	O
deterministically	O
but	O
with	O
a	O
limited	O
range	O
this	O
gives	O
us	O
the	O
sharp	O
contour	O
line	O
labeled	O
in	O
the	O
figure	O
all	O
locations	O
between	O
that	O
line	O
and	O
the	O
green	O
require	O
exactly	O
two	O
strokes	O
to	O
complete	O
the	O
hole	O
similarly	O
any	O
location	O
within	O
putting	O
range	O
of	O
the	O
contour	O
line	O
must	O
have	O
a	O
value	B
of	O
and	O
so	O
on	O
to	O
get	O
all	O
the	O
contour	O
lines	O
shown	O
in	O
the	O
figure	O
putting	O
doesn	O
t	O
get	O
us	O
out	O
of	O
sand	O
traps	O
so	O
they	O
have	O
a	O
value	B
of	O
overall	O
it	O
takes	O
us	O
six	O
strokes	O
to	O
get	O
from	O
the	O
tee	O
to	O
the	O
hole	O
by	O
putting	O
figure	O
a	O
golf	B
example	I
the	O
state-value	O
function	O
for	O
putting	O
and	O
the	O
optimal	O
action-value	O
function	O
for	O
using	O
the	O
driver	O
exercise	O
in	O
the	O
gridworld	O
example	O
rewards	O
are	O
positive	O
for	O
goals	O
negative	O
for	O
running	O
into	O
the	O
edge	O
of	O
the	O
world	O
and	O
zero	O
the	O
rest	O
of	O
the	O
time	O
are	O
the	O
signs	O
of	O
these	O
rewards	O
important	O
or	O
only	O
the	O
intervals	O
between	O
them	O
prove	O
using	O
that	O
adding	O
a	O
constant	O
c	O
to	O
all	O
the	O
rewards	O
adds	O
a	O
constant	O
vc	O
to	O
the	O
values	O
of	O
all	O
states	O
and	O
thus	O
does	O
not	O
affect	O
the	O
relative	O
values	O
of	O
any	O
states	O
under	O
any	O
policies	O
what	O
is	O
vc	O
in	O
terms	O
pq	O
chapter	O
finite	O
markov	O
decision	O
processes	O
of	O
c	O
and	O
exercise	O
now	O
consider	O
adding	O
a	O
constant	O
c	O
to	O
all	O
the	O
rewards	O
in	O
an	O
episodic	O
task	O
such	O
as	O
maze	O
running	O
would	O
this	O
have	O
any	O
effect	O
or	O
would	O
it	O
leave	O
the	O
task	O
unchanged	O
as	O
in	O
the	O
continuing	O
task	O
above	O
why	O
or	O
why	O
not	O
give	O
an	O
example	O
exercise	O
the	O
value	B
of	O
a	O
state	B
depends	O
on	O
the	O
values	O
of	O
the	O
actions	O
possible	O
in	O
that	O
state	B
and	O
on	O
how	O
likely	O
each	O
action	B
is	O
to	O
be	O
taken	O
under	O
the	O
current	O
policy	B
we	O
can	O
think	O
of	O
this	O
in	O
terms	O
of	O
a	O
small	O
backup	B
diagram	I
rooted	O
at	O
the	O
state	B
and	O
considering	O
each	O
possible	O
action	B
give	O
the	O
equation	O
corresponding	O
to	O
this	O
intuition	O
and	O
diagram	O
for	O
the	O
value	B
at	O
the	O
root	O
node	O
v	O
in	O
terms	O
of	O
the	O
value	B
at	O
the	O
expected	B
leaf	O
node	O
q	O
a	O
given	O
st	O
s	O
this	O
equation	O
should	O
include	O
an	O
expectation	O
conditioned	O
on	O
following	O
the	O
policy	B
then	O
give	O
a	O
second	O
equation	O
in	O
which	O
the	O
expected	B
value	B
is	O
written	O
out	O
explicitly	O
in	O
terms	O
of	O
such	O
that	O
no	O
expected	B
value	B
notation	O
appears	O
in	O
the	O
equation	O
exercise	O
the	O
value	B
of	O
an	O
action	B
q	O
a	O
depends	O
on	O
the	O
expected	B
next	O
reward	O
and	O
the	O
expected	B
sum	O
of	O
the	O
remaining	O
rewards	O
again	O
we	O
can	O
think	O
of	O
this	O
in	O
terms	O
of	O
a	O
small	O
backup	B
diagram	I
this	O
one	O
rooted	O
at	O
an	O
action	B
action	B
pair	O
and	O
branching	O
to	O
the	O
possible	O
next	O
states	O
give	O
the	O
equation	O
corresponding	O
to	O
this	O
intuition	O
and	O
diagram	O
for	O
the	O
action	B
value	B
q	O
a	O
in	O
terms	O
of	O
the	O
expected	B
next	O
reward	O
and	O
the	O
expected	B
next	O
state	B
value	B
v	O
given	O
that	O
st	O
s	O
and	O
at	O
a	O
this	O
equation	O
should	O
include	O
an	O
expectation	O
but	O
not	O
one	O
conditioned	O
on	O
following	O
the	O
policy	B
then	O
give	O
a	O
second	O
equation	O
writing	O
out	O
the	O
expected	B
value	B
explicitly	O
in	O
terms	O
of	O
rs	O
a	O
defined	O
by	O
such	O
that	O
no	O
expected	B
value	B
notation	O
appears	O
in	O
the	O
equation	O
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
solving	O
a	O
reinforcement	B
learning	I
task	O
means	O
roughly	O
finding	O
a	O
policy	B
that	O
achieves	O
a	O
lot	O
of	O
reward	O
over	O
the	O
long	O
run	O
for	O
finite	O
mdps	O
we	O
can	O
precisely	O
define	O
an	O
optimal	O
policy	B
in	O
the	O
following	O
way	O
value	B
functions	O
define	O
a	O
partial	O
ordering	O
over	O
policies	O
a	O
policy	B
is	O
defined	O
to	O
be	O
better	O
than	O
or	O
equal	O
to	O
a	O
policy	B
if	O
its	O
expected	B
return	B
is	O
greater	O
than	O
or	O
equal	O
to	O
that	O
of	O
for	O
all	O
states	O
in	O
other	O
words	O
if	O
and	O
only	O
if	O
v	O
v	O
for	O
all	O
s	O
s	O
there	O
is	O
always	O
at	O
least	O
one	O
policy	B
that	O
is	O
better	O
than	O
or	O
equal	O
to	O
all	O
other	O
policies	O
this	O
is	O
an	O
optimal	O
policy	B
although	O
there	O
may	O
be	O
more	O
than	O
one	O
we	O
denote	O
all	O
the	O
optimal	O
policies	O
by	O
they	O
share	O
the	O
same	O
state-value	O
function	O
called	O
the	O
optimal	O
stakenwithprobability	O
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
state-value	O
function	O
denoted	O
v	O
and	O
defined	O
as	O
max	O
v	O
v	O
for	O
all	O
s	O
s	O
defined	O
as	O
optimal	O
policies	O
also	O
share	O
the	O
same	O
optimal	O
action-value	O
function	O
denoted	O
q	O
and	O
q	O
a	O
max	O
q	O
a	O
for	O
all	O
s	O
s	O
and	O
a	O
as	O
for	O
the	O
state	B
action	B
pair	O
a	O
this	O
function	O
gives	O
the	O
expected	B
return	B
for	O
taking	O
action	B
a	O
in	O
state	B
s	O
and	O
thereafter	O
following	O
an	O
optimal	O
policy	B
thus	O
we	O
can	O
write	O
q	O
in	O
terms	O
of	O
v	O
as	O
follows	O
q	O
a	O
v	O
st	O
s	O
at	O
a	O
example	O
optimal	O
value	B
functions	O
for	O
golf	O
the	O
lower	O
part	O
of	O
figure	O
shows	O
the	O
contours	O
of	O
a	O
possible	O
optimal	O
action-value	O
function	O
q	O
driver	O
these	O
are	O
the	O
values	O
of	O
each	O
state	B
if	O
we	O
first	O
play	O
a	O
stroke	O
with	O
the	O
driver	O
and	O
afterward	O
select	O
either	O
the	O
driver	O
or	O
the	O
putter	O
whichever	O
is	O
better	O
the	O
driver	O
enables	O
us	O
to	O
hit	O
the	O
ball	O
farther	O
but	O
with	O
less	O
accuracy	O
we	O
can	O
reach	O
the	O
hole	O
in	O
one	O
shot	O
using	O
the	O
driver	O
only	O
if	O
we	O
are	O
already	O
very	O
close	O
thus	O
the	O
contour	O
for	O
q	O
driver	O
covers	O
only	O
a	O
small	O
portion	O
of	O
the	O
green	O
if	O
we	O
have	O
two	O
strokes	O
however	O
then	O
we	O
can	O
reach	O
the	O
hole	O
from	O
much	O
farther	O
away	O
as	O
shown	O
by	O
the	O
contour	O
in	O
this	O
case	O
we	O
don	O
t	O
have	O
to	O
drive	O
all	O
the	O
way	O
to	O
within	O
the	O
small	O
contour	O
but	O
only	O
to	O
anywhere	O
on	O
the	O
green	O
from	O
there	O
we	O
can	O
use	O
the	O
putter	O
the	O
optimal	O
action-value	O
function	O
gives	O
the	O
values	O
after	O
committing	O
to	O
a	O
particular	O
first	O
action	B
in	O
this	O
case	O
to	O
the	O
driver	O
but	O
afterward	O
using	O
whichever	O
actions	O
are	O
best	O
the	O
contour	O
is	O
still	O
farther	O
out	O
and	O
includes	O
the	O
starting	O
tee	O
from	O
the	O
tee	O
the	O
best	O
sequence	O
of	O
actions	O
is	O
two	O
drives	O
and	O
one	O
putt	O
sinking	O
the	O
ball	O
in	O
three	O
strokes	O
because	O
v	O
is	O
the	O
value	B
function	I
for	O
a	O
policy	B
it	O
must	O
satisfy	O
the	O
self-consistency	O
condition	O
given	O
by	O
the	O
bellman	B
equation	I
for	O
state	B
values	O
because	O
it	O
is	O
the	O
optimal	O
value	B
function	I
however	O
v	O
s	O
consistency	O
condition	O
can	O
be	O
written	O
in	O
a	O
special	O
form	O
without	O
reference	O
to	O
any	O
specific	O
policy	B
this	O
is	O
the	O
bellman	B
equation	I
for	O
v	O
or	O
the	O
bellman	B
optimality	O
equation	O
intuitively	O
the	O
bellman	B
optimality	O
equation	O
expresses	O
the	O
fact	O
that	O
the	O
value	B
of	O
a	O
state	B
under	O
an	O
optimal	O
policy	B
must	O
equal	O
the	O
expected	B
return	B
for	O
the	O
best	O
action	B
from	O
that	O
state	B
v	O
max	O
a	O
as	O
max	O
a	O
a	O
a	O
q	O
e	O
st	O
s	O
at	O
a	O
e	O
st	O
s	O
at	O
a	O
v	O
st	O
s	O
at	O
a	O
a	O
rs	O
v	O
a	O
max	O
max	O
max	O
chapter	O
finite	O
markov	O
decision	O
processes	O
the	O
last	O
two	O
equations	O
are	O
two	O
forms	O
of	O
the	O
bellman	B
optimality	O
equation	O
for	O
v	O
the	O
bellman	B
optimality	O
equation	O
for	O
q	O
is	O
q	O
a	O
max	O
q	O
st	O
s	O
at	O
q	O
rs	O
max	O
the	O
backup	O
diagrams	O
in	O
figure	O
show	O
graphically	O
the	O
spans	O
of	O
future	O
states	O
and	O
actions	O
considered	O
in	O
the	O
bellman	B
optimality	O
equations	O
for	O
v	O
and	O
q	O
these	O
are	O
the	O
same	O
as	O
the	O
backup	O
diagrams	O
for	O
v	O
and	O
q	O
presented	O
earlier	O
except	O
that	O
arcs	O
have	O
been	O
added	O
at	O
the	O
agent	O
s	O
choice	O
points	O
to	O
represent	O
that	O
the	O
maximum	O
over	O
that	O
choice	O
is	O
taken	O
rather	O
than	O
the	O
expected	B
value	B
given	O
some	O
policy	B
the	O
backup	B
diagram	I
on	O
the	O
left	O
graphically	O
represents	O
the	O
bellman	B
optimality	O
equation	O
and	O
the	O
backup	B
diagram	I
on	O
the	O
right	O
graphically	O
represents	O
figure	O
backup	O
diagrams	O
for	O
v	O
and	O
q	O
for	O
finite	O
mdps	O
the	O
bellman	B
optimality	O
equation	O
for	O
v	O
has	O
a	O
unique	O
solution	O
independent	O
of	O
the	O
policy	B
the	O
bellman	B
optimality	O
equation	O
is	O
actually	O
a	O
system	O
of	O
equations	O
one	O
for	O
each	O
state	B
so	O
if	O
there	O
are	O
n	O
states	O
then	O
there	O
are	O
n	O
equations	O
in	O
n	O
unknowns	O
if	O
the	O
dynamics	O
p	O
of	O
the	O
environment	B
are	O
known	O
then	O
in	O
principle	O
one	O
can	O
solve	O
this	O
system	O
of	O
equations	O
for	O
v	O
using	O
any	O
one	O
of	O
a	O
variety	O
of	O
methods	O
for	O
solving	O
systems	O
of	O
nonlinear	O
equations	O
one	O
can	O
solve	O
a	O
related	O
set	O
of	O
equations	O
for	O
q	O
once	O
one	O
has	O
v	O
it	O
is	O
relatively	O
easy	O
to	O
determine	O
an	O
optimal	O
policy	B
for	O
each	O
state	B
s	O
there	O
will	O
be	O
one	O
or	O
more	O
actions	O
at	O
which	O
the	O
maximum	O
is	O
obtained	O
in	O
the	O
bellman	B
optimality	O
equation	O
any	O
policy	B
that	O
assigns	O
nonzero	O
probability	O
only	O
to	O
these	O
actions	O
is	O
an	O
optimal	O
policy	B
you	O
can	O
think	O
of	O
this	O
as	O
a	O
one-step	O
search	O
if	O
you	O
have	O
the	O
optimal	O
value	B
function	I
v	O
then	O
the	O
actions	O
that	O
appear	O
best	O
after	O
a	O
one-step	O
search	O
will	O
be	O
optimal	O
actions	O
another	O
way	O
of	O
saying	O
this	O
is	O
that	O
any	O
policy	B
that	O
is	O
greedy	O
with	O
respect	O
to	O
the	O
optimal	O
evaluation	O
function	O
v	O
is	O
an	O
optimal	O
policy	B
the	O
term	O
greedy	O
is	O
used	O
in	O
computer	O
science	O
to	O
describe	O
any	O
search	O
or	O
decision	O
procedure	O
that	O
selects	O
alternatives	O
based	O
only	O
on	O
local	O
or	O
immediate	O
considerations	O
without	O
considering	O
the	O
possibility	O
that	O
such	O
a	O
selection	O
may	O
prevent	O
future	O
access	O
to	O
even	O
better	O
alternatives	O
consequently	O
it	O
describes	O
policies	O
that	O
select	O
actions	O
based	O
only	O
on	O
their	O
short-term	O
consequences	O
the	O
beauty	O
of	O
v	O
is	O
that	O
if	O
one	O
uses	O
it	O
to	O
evaluate	O
the	O
short-term	O
consequences	O
of	O
actions	O
specifically	O
the	O
one-step	O
consequences	O
then	O
a	O
greedy	O
policy	B
is	O
actually	O
optimal	O
in	O
the	O
long-term	O
sense	O
in	O
which	O
we	O
are	O
interested	O
because	O
v	O
already	O
takes	O
into	O
account	O
the	O
reward	O
consequences	O
of	O
all	O
possible	O
future	O
behavior	O
by	O
means	O
of	O
v	O
the	O
optimal	O
expected	B
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
long-term	O
return	B
is	O
turned	O
into	O
a	O
quantity	O
that	O
is	O
locally	O
and	O
immediately	O
available	O
for	O
each	O
state	B
hence	O
a	O
one-step-ahead	O
search	O
yields	O
the	O
long-term	O
optimal	O
actions	O
having	O
q	O
makes	O
choosing	O
optimal	O
actions	O
even	O
easier	O
with	O
q	O
the	O
agent	O
does	O
not	O
even	O
have	O
to	O
do	O
a	O
one-step-ahead	O
search	O
for	O
any	O
state	B
s	O
it	O
can	O
simply	O
find	O
any	O
action	B
that	O
maximizes	O
q	O
a	O
the	O
action-value	O
function	O
effectively	O
caches	O
the	O
results	O
of	O
all	O
one-step-ahead	O
searches	O
it	O
provides	O
the	O
optimal	O
expected	B
long-term	O
return	B
as	O
a	O
value	B
that	O
is	O
locally	O
and	O
immediately	O
available	O
for	O
each	O
state	B
action	B
pair	O
hence	O
at	O
the	O
cost	O
of	O
representing	O
a	O
function	O
of	O
state	B
action	B
pairs	O
instead	O
of	O
just	O
of	O
states	O
the	O
optimal	O
actionvalue	O
function	O
allows	O
optimal	O
actions	O
to	O
be	O
selected	O
without	O
having	O
to	O
know	O
anything	O
about	O
possible	O
successor	O
states	O
and	O
their	O
values	O
that	O
is	O
without	O
having	O
to	O
know	O
anything	O
about	O
the	O
environment	B
s	O
dynamics	O
example	O
solving	O
the	O
gridworld	O
suppose	O
we	O
solve	O
the	O
bellman	B
equation	I
for	O
v	O
for	O
the	O
simple	O
grid	O
task	O
introduced	O
in	O
example	O
and	O
shown	O
again	O
in	O
figure	O
recall	O
that	O
state	B
a	O
is	O
followed	O
by	O
a	O
reward	O
of	O
and	O
transition	O
to	O
state	B
while	O
state	B
b	O
is	O
followed	O
by	O
a	O
reward	O
of	O
and	O
transition	O
to	O
state	B
figure	O
shows	O
the	O
optimal	O
value	B
function	I
and	O
figure	O
shows	O
the	O
corresponding	O
optimal	O
policies	O
where	O
there	O
are	O
multiple	O
arrows	O
in	O
a	O
cell	O
all	O
of	O
the	O
corresponding	O
actions	O
are	O
optimal	O
figure	O
optimal	O
solutions	O
to	O
the	O
gridworld	O
example	O
example	O
bellman	B
optimality	O
equations	O
for	O
the	O
recycling	O
robot	O
using	O
we	O
can	O
explicitly	O
give	O
the	O
bellman	B
optimality	O
equation	O
for	O
the	O
recycling	B
robot	I
example	I
to	O
make	O
things	O
more	O
compact	O
we	O
abbreviate	O
the	O
states	O
high	O
and	O
low	O
and	O
the	O
actions	O
search	O
wait	O
and	O
recharge	O
respectively	O
by	O
h	O
l	O
s	O
w	O
and	O
re	O
because	O
there	O
are	O
only	O
two	O
states	O
the	O
bellman	B
optimality	O
equation	O
consists	O
of	O
two	O
equations	O
the	O
equation	O
for	O
v	O
can	O
be	O
written	O
as	O
follows	O
v	O
phh	O
srh	O
s	O
h	O
v	O
plh	O
srh	O
s	O
l	O
v	O
phh	O
wrh	O
w	O
h	O
v	O
plh	O
wrh	O
w	O
l	O
v	O
v	O
v	O
rs	O
v	O
v	O
v	O
rw	O
v	O
a	O
gridworldb	O
vc	O
v	O
max	O
rs	O
v	O
rw	O
v	O
v	O
chapter	O
finite	O
markov	O
decision	O
processes	O
following	O
the	O
same	O
procedure	O
for	O
v	O
yields	O
the	O
equation	O
for	O
any	O
choice	O
of	O
rs	O
rw	O
and	O
with	O
there	O
is	O
exactly	O
one	O
pair	O
of	O
numbers	O
v	O
and	O
v	O
that	O
simultaneously	O
satisfy	O
these	O
two	O
nonlinear	O
equations	O
explicitly	O
solving	O
the	O
bellman	B
optimality	O
equation	O
provides	O
one	O
route	O
to	O
finding	O
an	O
optimal	O
policy	B
and	O
thus	O
to	O
solving	O
the	O
reinforcement	B
learning	I
problem	O
however	O
this	O
solution	O
is	O
rarely	O
directly	O
useful	O
it	O
is	O
akin	O
to	O
an	O
exhaustive	O
search	O
looking	O
ahead	O
at	O
all	O
possibilities	O
computing	O
their	O
probabilities	O
of	O
occurrence	O
and	O
their	O
desirabilities	O
in	O
terms	O
of	O
expected	B
rewards	O
this	O
solution	O
relies	O
on	O
at	O
least	O
three	O
assumptions	O
that	O
are	O
rarely	O
true	O
in	O
practice	O
we	O
accurately	O
know	O
the	O
dynamics	O
of	O
the	O
environment	B
we	O
have	O
enough	O
computational	O
resources	O
to	O
complete	O
the	O
computation	O
of	O
the	O
solution	O
and	O
the	O
markov	B
property	I
for	O
the	O
kinds	O
of	O
tasks	O
in	O
which	O
we	O
are	O
interested	O
one	O
is	O
generally	O
not	O
able	O
to	O
implement	O
this	O
solution	O
exactly	O
because	O
various	O
combinations	O
of	O
these	O
assumptions	O
are	O
violated	O
for	O
example	O
although	O
the	O
first	O
and	O
third	O
assumptions	O
present	O
no	O
problems	O
for	O
the	O
game	O
of	O
backgammon	B
the	O
second	O
is	O
a	O
major	O
impediment	O
because	O
the	O
game	O
has	O
about	O
states	O
it	O
would	O
take	O
thousands	O
of	O
years	O
on	O
today	O
s	O
fastest	O
computers	O
to	O
solve	O
the	O
bellman	B
equation	I
for	O
v	O
and	O
the	O
same	O
is	O
true	O
for	O
finding	O
q	O
in	O
reinforcement	B
learning	I
one	O
typically	O
has	O
to	O
settle	O
for	O
approximate	O
solutions	O
many	O
different	O
decision-making	O
methods	O
can	O
be	O
viewed	O
as	O
ways	O
of	O
approximately	O
solving	O
the	O
bellman	B
optimality	O
equation	O
for	O
example	O
heuristic	B
search	I
methods	O
can	O
be	O
viewed	O
as	O
expanding	O
the	O
right-hand	O
side	O
of	O
several	O
times	O
up	O
to	O
some	O
depth	O
forming	O
a	O
tree	O
of	O
possibilities	O
and	O
then	O
using	O
a	O
heuristic	O
evaluation	O
function	O
to	O
approximate	O
v	O
at	O
the	O
leaf	O
nodes	O
search	O
methods	O
such	O
as	O
a	O
are	O
almost	O
always	O
based	O
on	O
the	O
episodic	O
case	O
the	O
methods	O
of	O
dynamic	B
programming	I
can	O
be	O
related	O
even	O
more	O
closely	O
to	O
the	O
bellman	B
optimality	O
equation	O
many	O
reinforcement	B
learning	I
methods	O
can	O
be	O
clearly	O
understood	O
as	O
approximately	O
solving	O
the	O
bellman	B
optimality	O
equation	O
using	O
actual	O
experienced	O
transitions	O
in	O
place	O
of	O
knowledge	O
of	O
the	O
expected	B
transitions	O
we	O
consider	O
a	O
variety	O
of	O
such	O
methods	O
in	O
the	O
following	O
chapters	O
exercise	O
draw	O
or	O
describe	O
the	O
optimal	O
state-value	O
function	O
for	O
the	O
golf	B
example	I
exercise	O
draw	O
or	O
describe	O
the	O
contours	O
of	O
the	O
optimal	O
action-value	O
function	O
for	O
putting	O
q	O
putter	O
for	O
the	O
golf	B
example	I
exercise	O
consider	O
the	O
continuing	O
mdp	O
shown	O
on	O
to	O
the	O
right	O
the	O
only	O
decision	O
to	O
be	O
made	O
is	O
that	O
in	O
the	O
top	O
state	B
where	O
two	O
actions	O
are	O
available	O
left	O
and	O
right	O
the	O
numbers	O
show	O
the	O
rewards	O
that	O
are	O
received	O
deterministically	O
after	O
each	O
action	B
there	O
are	O
exactly	O
two	O
deterministic	O
policies	O
left	O
and	O
right	O
what	O
policy	B
is	O
optimal	O
if	O
if	O
if	O
optimality	O
and	O
approximation	O
exercise	O
give	O
the	O
bellman	B
equation	I
for	O
q	O
for	O
the	O
recycling	O
robot	O
exercise	O
figure	O
gives	O
the	O
optimal	O
value	B
of	O
the	O
best	O
state	B
of	O
the	O
gridworld	O
as	O
to	O
one	O
decimal	O
place	O
use	O
your	O
knowledge	O
of	O
the	O
optimal	O
policy	B
and	O
to	O
express	O
this	O
value	B
symbolically	O
and	O
then	O
to	O
compute	O
it	O
to	O
three	O
decimal	O
places	O
exercise	O
give	O
an	O
equation	O
for	O
v	O
in	O
terms	O
of	O
q	O
exercise	O
give	O
an	O
equation	O
for	O
q	O
in	O
terms	O
of	O
v	O
and	O
the	O
world	O
s	O
dynamics	O
rs	O
a	O
exercise	O
give	O
an	O
equation	O
for	O
in	O
terms	O
of	O
q	O
exercise	O
give	O
an	O
equation	O
for	O
in	O
terms	O
of	O
v	O
and	O
the	O
world	O
s	O
dynamics	O
rs	O
a	O
optimality	O
and	O
approximation	O
we	O
have	O
defined	O
optimal	O
value	B
functions	O
and	O
optimal	O
policies	O
clearly	O
an	O
agent	O
that	O
learns	O
an	O
optimal	O
policy	B
has	O
done	O
very	O
well	O
but	O
in	O
practice	O
this	O
rarely	O
happens	O
for	O
the	O
kinds	O
of	O
tasks	O
in	O
which	O
we	O
are	O
interested	O
optimal	O
policies	O
can	O
be	O
generated	O
only	O
with	O
extreme	O
computational	O
cost	O
a	O
well-defined	O
notion	O
of	O
optimality	O
organizes	O
the	O
approach	O
to	O
learning	O
we	O
describe	O
in	O
this	O
book	O
and	O
provides	O
a	O
way	O
to	O
understand	O
the	O
theoretical	O
properties	O
of	O
various	O
learning	O
algorithms	O
but	O
it	O
is	O
an	O
ideal	O
that	O
agents	O
can	O
only	O
approximate	O
to	O
varying	O
degrees	O
as	O
we	O
discussed	O
above	O
even	O
if	O
we	O
have	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	B
s	O
dynamics	O
it	O
is	O
usually	O
not	O
possible	O
to	O
simply	O
compute	O
an	O
optimal	O
policy	B
by	O
solving	O
the	O
bellman	B
optimality	O
equation	O
for	O
example	O
board	O
games	O
such	O
as	O
chess	B
are	O
a	O
tiny	O
fraction	O
of	O
human	O
experience	O
yet	O
large	O
customdesigned	O
computers	O
still	O
cannot	O
compute	O
the	O
optimal	O
moves	O
a	O
critical	O
aspect	O
of	O
the	O
problem	O
facing	O
the	O
agent	O
is	O
always	O
the	O
computational	O
power	O
available	O
to	O
it	O
in	O
particular	O
the	O
amount	O
of	O
computation	O
it	O
can	O
perform	O
in	O
a	O
single	O
time	O
step	O
the	O
memory	O
available	O
is	O
also	O
an	O
important	O
constraint	O
a	O
large	O
amount	O
of	O
memory	O
is	O
often	O
required	O
to	O
build	O
up	O
approximations	O
of	O
value	B
functions	O
policies	O
and	O
models	O
in	O
tasks	O
with	O
small	O
finite	O
state	B
sets	O
it	O
is	O
possible	O
to	O
form	O
these	O
approximations	O
using	O
arrays	O
or	O
tables	O
with	O
one	O
entry	O
for	O
each	O
state	B
state	B
action	B
pair	O
this	O
we	O
call	O
the	O
tabular	O
case	O
and	O
the	O
corresponding	O
methods	O
we	O
call	O
tabular	O
methods	O
in	O
many	O
cases	O
of	O
practical	O
interest	O
however	O
there	O
are	O
far	O
more	O
states	O
than	O
could	O
possibly	O
be	O
entries	O
in	O
a	O
table	O
in	O
these	O
cases	O
the	O
functions	O
must	O
be	O
approximated	O
using	O
some	O
sort	O
of	O
more	O
compact	O
parameterized	O
function	O
representation	O
our	O
framing	O
of	O
the	O
reinforcement	B
learning	I
problem	O
forces	O
us	O
to	O
settle	O
for	O
approximations	O
however	O
it	O
also	O
presents	O
us	O
with	O
some	O
unique	O
opportunities	O
for	O
achieving	O
useful	O
approximations	O
for	O
example	O
in	O
approximating	O
optimal	O
behavior	O
there	O
may	O
be	O
many	O
states	O
that	O
the	O
agent	O
faces	O
with	O
such	O
a	O
low	O
probability	O
that	O
selecting	O
suboptimal	O
actions	O
for	O
them	O
has	O
little	O
impact	O
on	O
the	O
amount	O
of	O
reward	O
the	O
agent	O
receives	O
tesauro	O
s	O
backgammon	B
player	O
for	O
example	O
plays	O
with	O
exceptional	O
skill	O
even	O
though	O
it	O
might	O
make	O
very	O
bad	O
decisions	O
on	O
board	O
configurations	O
that	O
never	O
occur	O
in	O
games	O
against	O
experts	O
in	O
fact	O
it	O
is	O
possible	O
that	O
td-gammon	B
makes	O
bad	O
decisions	O
for	O
a	O
large	O
fraction	O
of	O
the	O
chapter	O
finite	O
markov	O
decision	O
processes	O
game	O
s	O
state	B
set	O
the	O
online	B
nature	O
of	O
reinforcement	B
learning	I
makes	O
it	O
possible	O
to	O
approximate	O
optimal	O
policies	O
in	O
ways	O
that	O
put	O
more	O
effort	O
into	O
learning	O
to	O
make	O
good	O
decisions	O
for	O
frequently	O
encountered	O
states	O
at	O
the	O
expense	O
of	O
less	O
effort	O
for	O
infrequently	O
encountered	O
states	O
this	O
is	O
one	O
key	O
property	O
that	O
distinguishes	O
reinforcement	B
learning	I
from	O
other	O
approaches	O
to	O
approximately	O
solving	O
mdps	O
summary	O
let	O
us	O
summarize	O
the	O
elements	O
of	O
the	O
reinforcement	B
learning	I
problem	O
that	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
reinforcement	B
learning	I
is	O
about	O
learning	O
from	O
interaction	O
how	O
to	O
behave	O
in	O
order	O
to	O
achieve	O
a	O
goal	O
the	O
reinforcement	B
learning	I
agent	O
and	O
its	O
environment	B
interact	O
over	O
a	O
sequence	O
of	O
discrete	O
time	O
steps	O
the	O
specification	O
of	O
their	O
interface	O
defines	O
a	O
particular	O
task	O
the	O
actions	O
are	O
the	O
choices	O
made	O
by	O
the	O
agent	O
the	O
states	O
are	O
the	O
basis	O
for	O
making	O
the	O
choices	O
and	O
the	O
rewards	O
are	O
the	O
basis	O
for	O
evaluating	O
the	O
choices	O
everything	O
inside	O
the	O
agent	O
is	O
completely	O
known	O
and	O
controllable	O
by	O
the	O
agent	O
everything	O
outside	O
is	O
incompletely	O
controllable	O
but	O
may	O
or	O
may	O
not	O
be	O
completely	O
known	O
a	O
policy	B
is	O
a	O
stochastic	O
rule	O
by	O
which	O
the	O
agent	O
selects	O
actions	O
as	O
a	O
function	O
of	O
states	O
the	O
agent	O
s	O
objective	O
is	O
to	O
maximize	O
the	O
amount	O
of	O
reward	O
it	O
receives	O
over	O
time	O
when	O
the	O
reinforcement	B
learning	I
setup	O
described	O
above	O
is	O
formulated	O
with	O
well	O
defined	O
transition	B
probabilities	I
it	O
constitutes	O
a	O
markov	O
decision	O
process	O
a	O
finite	O
mdp	O
is	O
an	O
mdp	O
with	O
finite	O
state	B
action	B
and	O
we	O
formulate	O
it	O
here	O
reward	O
sets	O
much	O
of	O
the	O
current	O
theory	O
of	O
reinforcement	B
learning	I
is	O
restricted	O
to	O
finite	O
mdps	O
but	O
the	O
methods	O
and	O
ideas	O
apply	O
more	O
generally	O
the	O
return	B
is	O
the	O
function	O
of	O
future	O
rewards	O
that	O
the	O
agent	O
seeks	O
to	O
maximize	O
expected	B
value	B
it	O
has	O
several	O
different	O
definitions	O
depending	O
upon	O
the	O
nature	O
of	O
the	O
task	O
and	O
whether	O
one	O
wishes	O
to	O
discount	O
delayed	B
reward	I
the	O
undiscounted	O
formulation	O
is	O
appropriate	O
for	O
episodic	O
tasks	O
in	O
which	O
the	O
agent	O
environment	B
interaction	O
breaks	O
naturally	O
into	O
episodes	B
the	O
discounted	O
formulation	O
is	O
appropriate	O
for	O
continuing	B
tasks	I
in	O
which	O
the	O
interaction	O
does	O
not	O
naturally	O
break	O
into	O
episodes	B
but	O
continues	O
without	O
limit	O
we	O
try	O
to	O
define	O
the	O
returns	O
for	O
the	O
two	O
kinds	O
of	O
tasks	O
such	O
that	O
one	O
set	O
of	O
equations	O
can	O
apply	O
to	O
both	O
the	O
episodic	O
and	O
continuing	O
cases	O
a	O
policy	B
s	O
value	B
functions	O
assign	O
to	O
each	O
state	B
or	O
state	B
action	B
pair	O
the	O
expected	B
return	B
from	O
that	O
state	B
or	O
state	B
action	B
pair	O
given	O
that	O
the	O
agent	O
uses	O
the	O
policy	B
the	O
optimal	O
value	B
functions	O
assign	O
to	O
each	O
state	B
or	O
state	B
action	B
pair	O
the	O
largest	O
expected	B
return	B
achievable	O
by	O
any	O
policy	B
a	O
policy	B
whose	O
value	B
functions	O
are	O
optimal	O
is	O
an	O
optimal	O
policy	B
whereas	O
the	O
optimal	O
value	B
functions	O
for	O
states	O
and	O
state	B
action	B
pairs	O
are	O
unique	O
for	O
a	O
given	O
mdp	O
there	O
can	O
be	O
many	O
optimal	O
policies	O
any	O
policy	B
that	O
is	O
greedy	O
with	O
respect	O
to	O
the	O
optimal	O
value	B
functions	O
must	O
be	O
an	O
optimal	O
policy	B
the	O
bellman	B
optimality	O
equations	O
are	O
special	O
consistency	O
conditions	O
that	O
the	O
optimal	O
value	B
functions	O
must	O
satisfy	O
and	O
that	O
can	O
in	O
principle	O
be	O
solved	O
for	O
the	O
optimal	O
value	B
functions	O
from	O
which	O
an	O
optimal	O
policy	B
can	O
be	O
determined	O
with	O
relative	O
ease	O
a	O
reinforcement	B
learning	I
problem	O
can	O
be	O
posed	O
in	O
a	O
variety	O
of	O
different	O
ways	O
depending	O
on	O
assumptions	O
about	O
the	O
level	O
of	O
knowledge	O
initially	O
available	O
to	O
the	O
agent	O
in	O
problems	O
of	O
complete	O
knowledge	O
the	O
agent	O
has	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	B
s	O
summary	O
dynamics	O
if	O
the	O
environment	B
is	O
an	O
mdp	O
then	O
such	O
a	O
model	O
consists	O
of	O
the	O
complete	O
fourargument	O
dynamics	O
function	O
p	O
in	O
problems	O
of	O
incomplete	O
knowledge	O
a	O
complete	O
and	O
perfect	O
model	B
of	I
the	I
environment	B
is	O
not	O
available	O
even	O
if	O
the	O
agent	O
has	O
a	O
complete	O
and	O
accurate	O
environment	B
model	O
the	O
agent	O
is	O
typically	O
unable	O
to	O
perform	O
enough	O
computation	O
per	O
time	O
step	O
to	O
fully	O
use	O
it	O
the	O
memory	O
available	O
is	O
also	O
an	O
important	O
constraint	O
memory	O
may	O
be	O
required	O
to	O
build	O
up	O
accurate	O
approximations	O
of	O
value	B
functions	O
policies	O
and	O
models	O
in	O
most	O
cases	O
of	O
practical	O
interest	O
there	O
are	O
far	O
more	O
states	O
than	O
could	O
possibly	O
be	O
entries	O
in	O
a	O
table	O
and	O
approximations	O
must	O
be	O
made	O
a	O
well-defined	O
notion	O
of	O
optimality	O
organizes	O
the	O
approach	O
to	O
learning	O
we	O
describe	O
in	O
this	O
book	O
and	O
provides	O
a	O
way	O
to	O
understand	O
the	O
theoretical	O
properties	O
of	O
various	O
learning	O
algorithms	O
but	O
it	O
is	O
an	O
ideal	O
that	O
reinforcement	B
learning	I
agents	O
can	O
only	O
approximate	O
to	O
varying	O
degrees	O
in	O
reinforcement	B
learning	I
we	O
are	O
very	O
much	O
concerned	O
with	O
cases	O
in	O
which	O
optimal	O
solutions	O
cannot	O
be	O
found	O
but	O
must	O
be	O
approximated	O
in	O
some	O
way	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
reinforcement	B
learning	I
problem	O
is	O
deeply	O
indebted	O
to	O
the	O
idea	O
of	O
markov	O
decision	O
processes	O
from	O
the	O
field	O
of	O
optimal	B
control	B
these	O
historical	O
influences	O
and	O
other	O
major	O
influences	O
from	O
psychology	B
are	O
described	O
in	O
the	O
brief	O
history	O
given	O
in	O
chapter	O
reinforcement	B
learning	I
adds	O
to	O
mdps	O
a	O
focus	O
on	O
approximation	O
and	O
incomplete	O
information	O
for	O
realistically	O
large	O
problems	O
mdps	O
and	O
the	O
reinforcement	B
learning	I
problem	O
are	O
only	O
weakly	O
linked	O
to	O
traditional	O
learning	O
and	O
decision-making	O
problems	O
in	O
artificial	B
intelligence	I
however	O
artificial	B
intelligence	I
is	O
now	O
vigorously	O
exploring	O
mdp	O
formulations	O
for	O
planning	B
and	O
decision	O
making	O
from	O
a	O
variety	O
of	O
perspectives	O
mdps	O
are	O
more	O
general	O
than	O
previous	O
formulations	O
used	O
in	O
artificial	B
intelligence	I
in	O
that	O
they	O
permit	O
more	O
general	O
kinds	O
of	O
goals	O
and	O
uncertainty	O
the	O
theory	O
of	O
mdps	O
is	O
treated	O
by	O
e	O
g	O
bertsekas	O
white	O
whittle	O
and	O
puterman	O
a	O
particularly	O
compact	O
treatment	O
of	O
the	O
finite	O
case	O
is	O
given	O
by	O
ross	O
mdps	O
are	O
also	O
studied	O
under	O
the	O
heading	O
of	O
stochastic	O
optimal	B
control	B
where	O
adaptive	O
optimal	B
control	B
methods	O
are	O
most	O
closely	O
related	O
to	O
reinforcement	B
learning	I
kumar	O
kumar	O
and	O
varaiya	O
the	O
theory	O
of	O
mdps	O
evolved	O
from	O
efforts	O
to	O
understand	O
the	O
problem	O
of	O
making	O
sequences	O
of	O
decisions	O
under	O
uncertainty	O
where	O
each	O
decision	O
can	O
depend	O
on	O
the	O
previous	O
decisions	O
and	O
their	O
outcomes	O
it	O
is	O
sometimes	O
called	O
the	O
theory	O
of	O
multistage	O
decision	O
processes	O
or	O
sequential	O
decision	O
processes	O
and	O
has	O
roots	O
in	O
the	O
statistical	O
literature	O
on	O
sequential	O
sampling	O
beginning	O
with	O
the	O
papers	O
by	O
thompson	O
and	O
robbins	O
that	O
we	O
cited	O
in	O
chapter	O
in	O
connection	O
with	O
bandit	B
problems	I
are	O
prototypical	O
mdps	O
if	O
formulated	O
as	O
multiple-situation	O
problems	O
the	O
earliest	O
instance	O
of	O
which	O
we	O
are	O
aware	O
in	O
which	O
reinforcement	B
learning	I
was	O
discussed	O
using	O
the	O
mdp	O
formalism	O
is	O
andreae	B
s	O
description	O
of	O
a	O
unified	O
view	O
of	O
learning	O
machines	O
witten	B
and	O
corbin	O
experimented	O
with	O
a	O
reinforcement	B
learning	I
system	O
later	O
analyzed	O
by	O
witten	B
using	O
the	O
mdp	O
formalism	O
although	O
he	O
did	O
not	O
explicitly	O
mention	O
mdps	O
werbos	B
suggested	O
approximate	O
solution	O
methods	O
chapter	O
finite	O
markov	O
decision	O
processes	O
for	O
stochastic	O
optimal	B
control	B
problems	O
that	O
are	O
related	O
to	O
modern	O
reinforcement	B
learning	I
methods	O
also	O
werbos	B
although	O
werbos	B
s	O
ideas	O
were	O
not	O
widely	O
recognized	O
at	O
the	O
time	O
they	O
were	O
prescient	O
in	O
emphasizing	O
the	O
importance	O
of	O
approximately	O
solving	O
optimal	B
control	B
problems	O
in	O
a	O
variety	O
of	O
domains	O
including	O
artificial	B
intelligence	I
the	O
most	O
influential	O
integration	O
of	O
reinforcement	B
learning	I
and	O
mdps	O
is	O
due	O
to	O
watkins	B
our	O
characterization	O
of	O
the	O
dynamics	O
of	O
an	O
mdp	O
in	O
terms	O
of	O
rs	O
a	O
is	O
it	O
is	O
more	O
common	O
in	O
the	O
mdp	O
literature	O
to	O
describe	O
the	O
slightly	O
unusual	O
dynamics	O
in	O
terms	O
of	O
the	O
state	B
transition	B
probabilities	I
a	O
and	O
expected	B
next	O
rewards	O
rs	O
a	O
in	O
reinforcement	B
learning	I
however	O
we	O
more	O
often	O
have	O
to	O
refer	O
to	O
individual	O
actual	O
or	O
sample	O
rewards	O
than	O
just	O
their	O
expected	B
values	O
our	O
notation	O
also	O
makes	O
it	O
plainer	O
that	O
st	O
and	O
rt	O
are	O
in	O
general	O
jointly	O
determined	O
and	O
thus	O
must	O
have	O
the	O
same	O
time	O
index	O
in	O
teaching	O
reinforcement	B
learning	I
we	O
have	O
found	O
our	O
notation	O
to	O
be	O
more	O
straightforward	O
conceptually	O
and	O
easier	O
to	O
understand	O
for	O
a	O
good	O
intuitive	O
discussion	O
of	O
the	O
system-theoretic	O
concept	O
of	O
state	B
see	O
minsky	B
the	O
bioreactor	B
example	I
is	O
based	O
on	O
the	O
work	O
of	O
ungar	O
and	O
miller	O
and	O
williams	O
the	O
recycling	B
robot	I
example	I
was	O
inspired	O
by	O
the	O
can-collecting	O
robot	O
built	O
by	O
jonathan	O
connell	O
kober	O
and	O
peters	O
present	O
a	O
collection	O
of	O
robotics	O
applications	O
of	O
reinforcement	B
learning	I
the	O
reward	O
hypothesis	O
was	O
suggested	O
by	O
michael	O
littman	O
communication	O
the	O
terminology	O
of	O
episodic	O
and	O
continuing	B
tasks	I
is	O
different	O
from	O
that	O
usually	O
used	O
in	O
the	O
mdp	O
literature	O
in	O
that	O
literature	O
it	O
is	O
common	O
to	O
distinguish	O
three	O
types	O
of	O
tasks	O
finite-horizon	O
tasks	O
in	O
which	O
interaction	O
terminates	O
after	O
a	O
particular	O
fixed	O
number	O
of	O
time	O
steps	O
indefinite-horizon	O
tasks	O
in	O
which	O
interaction	O
can	O
last	O
arbitrarily	O
long	O
but	O
must	O
eventually	O
terminate	O
and	O
infinite-horizon	O
tasks	O
in	O
which	O
interaction	O
does	O
not	O
terminate	O
our	O
episodic	O
and	O
continuing	B
tasks	I
are	O
similar	O
to	O
indefinite-horizon	O
and	O
infinite-horizon	O
tasks	O
respectively	O
but	O
we	O
prefer	O
to	O
emphasize	O
the	O
difference	O
in	O
the	O
nature	O
of	O
the	O
interaction	O
this	O
difference	O
seems	O
more	O
fundamental	O
than	O
the	O
difference	O
in	O
the	O
objective	O
functions	O
emphasized	O
by	O
the	O
usual	O
terms	O
often	O
episodic	O
tasks	O
use	O
an	O
indefinite-horizon	O
objective	O
function	O
and	O
continuing	B
tasks	I
an	O
infinite-horizon	O
objective	O
function	O
but	O
we	O
see	O
this	O
as	O
a	O
common	O
coincidence	O
rather	O
than	O
a	O
fundamental	O
difference	O
the	O
pole-balancing	O
example	O
is	O
from	O
michie	B
and	O
chambers	O
and	O
barto	O
sutton	O
and	O
anderson	O
assigning	O
value	B
on	O
the	O
basis	O
of	O
what	O
is	O
good	O
or	O
bad	O
in	O
the	O
long	O
run	O
has	O
ancient	O
roots	O
in	O
control	B
theory	I
mapping	O
states	O
to	O
numerical	O
values	O
representing	O
the	O
long-term	O
consequences	O
of	O
control	B
decisions	O
is	O
a	O
key	O
part	O
of	O
optimal	B
control	B
summary	O
theory	O
which	O
was	O
developed	O
in	O
the	O
by	O
extending	O
nineteenth	O
century	O
statefunction	O
theories	O
of	O
classical	O
mechanics	O
e	O
g	O
schultz	B
and	O
melsa	O
in	O
describing	O
how	O
a	O
computer	O
could	O
be	O
programmed	O
to	O
play	O
chess	B
shannon	B
suggested	O
using	O
an	O
evaluation	O
function	O
that	O
took	O
into	O
account	O
the	O
long-term	O
advantages	O
and	O
disadvantages	O
of	O
chess	B
positions	O
watkins	B
s	O
q-learning	B
algorithm	O
for	O
estimating	O
q	O
made	O
actionvalue	O
functions	O
an	O
important	O
part	O
of	O
reinforcement	B
learning	I
and	O
consequently	O
these	O
functions	O
are	O
often	O
called	O
q-functions	O
but	O
the	O
idea	O
of	O
an	O
action-value	O
function	O
is	O
much	O
older	O
than	O
this	O
shannon	B
suggested	O
that	O
a	O
function	O
hp	O
m	O
could	O
be	O
used	O
by	O
a	O
chess-playing	O
program	O
to	O
decide	O
whether	O
a	O
move	O
m	O
in	O
position	O
p	O
is	O
worth	O
exploring	O
michie	B
s	O
menace	O
system	O
and	O
michie	B
and	O
chambers	O
s	O
boxes	B
system	O
can	O
be	O
understood	O
as	O
estimating	O
action-value	O
functions	O
in	O
classical	O
physics	O
hamilton	O
s	O
principal	O
function	O
is	O
an	O
action-value	O
function	O
newtonian	O
dynamics	O
are	O
greedy	O
with	O
respect	O
to	O
this	O
function	O
goldstein	O
action-value	O
functions	O
also	O
played	O
a	O
central	O
role	O
in	O
denardo	O
s	O
theoretical	O
treatment	O
of	O
dynamic	B
programming	I
in	O
terms	O
of	O
contraction	O
mappings	O
the	O
bellman	B
optimality	O
equation	O
v	O
was	O
popularized	O
by	O
richard	O
bellman	B
who	O
called	O
it	O
the	O
basic	O
functional	O
equation	O
the	O
counterpart	O
of	O
the	O
bellman	B
optimality	O
equation	O
for	O
continuous	B
time	I
and	O
state	B
problems	O
is	O
known	O
as	O
the	O
hamilton	O
jacobi	O
bellman	B
equation	I
often	O
just	O
the	O
hamilton	O
jacobi	O
equation	O
indicating	O
its	O
roots	O
in	O
classical	O
physics	O
schultz	B
and	O
melsa	O
the	O
golf	B
example	I
was	O
suggested	O
by	O
chris	O
watkins	B
chapter	O
dynamic	B
programming	I
the	O
term	O
dynamic	B
programming	I
refers	O
to	O
a	O
collection	O
of	O
algorithms	O
that	O
can	O
be	O
used	O
to	O
compute	O
optimal	O
policies	O
given	O
a	O
perfect	O
model	B
of	I
the	I
environment	B
as	O
a	O
markov	O
decision	O
process	O
classical	O
dp	O
algorithms	O
are	O
of	O
limited	O
utility	O
in	O
reinforcement	B
learning	I
both	O
because	O
of	O
their	O
assumption	O
of	O
a	O
perfect	O
model	O
and	O
because	O
of	O
their	O
great	O
computational	O
expense	O
but	O
they	O
are	O
still	O
important	O
theoretically	O
dp	O
provides	O
an	O
essential	O
foundation	O
for	O
the	O
understanding	O
of	O
the	O
methods	O
presented	O
in	O
the	O
rest	O
of	O
this	O
book	O
in	O
fact	O
all	O
of	O
these	O
methods	O
can	O
be	O
viewed	O
as	O
attempts	O
to	O
achieve	O
much	O
the	O
same	O
effect	O
as	O
dp	O
only	O
with	O
less	O
computation	O
and	O
without	O
assuming	O
a	O
perfect	O
model	B
of	I
the	I
environment	B
starting	O
with	O
this	O
chapter	O
we	O
usually	O
assume	O
that	O
the	O
environment	B
is	O
a	O
finite	O
mdp	O
that	O
is	O
we	O
assume	O
that	O
its	O
state	B
action	B
and	O
reward	O
sets	O
s	O
a	O
and	O
r	O
are	O
finite	O
and	O
that	O
its	O
dynamics	O
are	O
given	O
by	O
a	O
set	O
of	O
probabilities	O
rs	O
a	O
for	O
all	O
s	O
s	O
a	O
as	O
r	O
r	O
and	O
s	O
is	O
s	O
plus	O
a	O
terminal	O
state	B
if	O
the	O
problem	O
is	O
episodic	O
although	O
dp	O
ideas	O
can	O
be	O
applied	O
to	O
problems	O
with	O
continuous	B
state	B
and	O
action	B
spaces	O
exact	O
solutions	O
are	O
possible	O
only	O
in	O
special	O
cases	O
a	O
common	O
way	O
of	O
obtaining	O
approximate	O
solutions	O
for	O
tasks	O
with	O
continuous	O
states	O
and	O
actions	O
is	O
to	O
quantize	O
the	O
state	B
and	O
action	B
spaces	O
and	O
then	O
apply	O
finite-state	O
dp	O
methods	O
the	O
methods	O
we	O
explore	O
in	O
chapter	O
are	O
applicable	O
to	O
continuous	O
problems	O
and	O
are	O
a	O
significant	O
extension	O
of	O
that	O
approach	O
the	O
key	O
idea	O
of	O
dp	O
and	O
of	O
reinforcement	B
learning	I
generally	O
is	O
the	O
use	O
of	O
value	B
functions	O
to	O
organize	O
and	O
structure	O
the	O
search	O
for	O
good	O
policies	O
in	O
this	O
chapter	O
we	O
show	O
how	O
dp	O
can	O
be	O
used	O
to	O
compute	O
the	O
value	B
functions	O
defined	O
in	O
chapter	O
as	O
discussed	O
there	O
we	O
can	O
easily	O
obtain	O
optimal	O
policies	O
once	O
we	O
have	O
found	O
the	O
optimal	O
value	B
functions	O
v	O
or	O
q	O
which	O
satisfy	O
the	O
bellman	B
optimality	O
equations	O
v	O
max	O
max	O
a	O
v	O
st	O
s	O
at	O
a	O
rs	O
v	O
a	O
or	O
chapter	O
dynamic	B
programming	I
q	O
a	O
max	O
q	O
st	O
s	O
at	O
q	O
rs	O
max	O
for	O
all	O
s	O
s	O
a	O
as	O
and	O
s	O
as	O
we	O
shall	O
see	O
dp	O
algorithms	O
are	O
obtained	O
by	O
turning	O
bellman	B
equations	O
such	O
as	O
these	O
into	O
assignments	O
that	O
is	O
into	O
update	O
rules	O
for	O
improving	O
approximations	O
of	O
the	O
desired	O
value	B
functions	O
policy	B
evaluation	O
first	O
we	O
consider	O
how	O
to	O
compute	O
the	O
state-value	O
function	O
v	O
for	O
an	O
arbitrary	O
policy	B
this	O
is	O
called	O
policy	B
evaluation	O
in	O
the	O
dp	O
literature	O
we	O
also	O
refer	O
to	O
it	O
as	O
the	O
prediction	B
problem	O
recall	O
from	O
chapter	O
that	O
for	O
all	O
s	O
s	O
v	O
e	O
st	O
s	O
e	O
st	O
s	O
e	O
v	O
st	O
s	O
rs	O
v	O
where	O
is	O
the	O
probability	O
of	O
taking	O
action	B
a	O
in	O
state	B
s	O
under	O
policy	B
and	O
the	O
expectations	O
are	O
subscripted	O
by	O
to	O
indicate	O
that	O
they	O
are	O
conditional	O
on	O
being	O
followed	O
the	O
existence	O
and	O
uniqueness	O
of	O
v	O
are	O
guaranteed	O
as	O
long	O
as	O
either	O
or	O
eventual	O
termination	O
is	O
guaranteed	O
from	O
all	O
states	O
under	O
the	O
policy	B
if	O
the	O
environment	B
s	O
dynamics	O
are	O
completely	O
known	O
then	O
is	O
a	O
system	O
of	O
simultaneous	O
linear	O
equations	O
in	O
unknowns	O
v	O
s	O
s	O
in	O
principle	O
its	O
solution	O
is	O
a	O
straightforward	O
if	O
tedious	O
computation	O
for	O
our	O
purposes	O
iterative	B
solution	O
methods	O
are	O
most	O
suitable	O
consider	O
a	O
sequence	O
of	O
approximate	O
value	B
functions	O
each	O
mapping	O
s	O
to	O
r	O
real	O
numbers	O
the	O
initial	O
approximation	O
is	O
chosen	O
arbitrarily	O
that	O
the	O
terminal	O
state	B
if	O
any	O
must	O
be	O
given	O
value	B
and	O
each	O
successive	O
approximation	O
is	O
obtained	O
by	O
using	O
the	O
bellman	B
equation	I
for	O
v	O
as	O
an	O
update	O
rule	O
e	O
st	O
s	O
rs	O
for	O
all	O
s	O
s	O
clearly	O
vk	O
v	O
is	O
a	O
fixed	O
point	O
for	O
this	O
update	O
rule	O
because	O
the	O
bellman	B
equation	I
for	O
v	O
assures	O
us	O
of	O
equality	O
in	O
this	O
case	O
indeed	O
the	O
sequence	O
can	O
be	O
shown	O
in	O
general	O
to	O
converge	O
to	O
v	O
as	O
k	O
under	O
the	O
same	O
conditions	O
that	O
guarantee	O
the	O
existence	O
of	O
v	O
this	O
algorithm	O
is	O
called	O
iterative	B
policy	B
evaluation	O
to	O
produce	O
each	O
successive	O
approximation	O
from	O
vk	O
iterative	B
policy	B
evaluation	O
applies	O
the	O
same	O
operation	O
to	O
each	O
state	B
s	O
it	O
replaces	O
the	O
old	O
value	B
of	O
s	O
with	O
a	O
new	O
value	B
policy	B
evaluation	O
obtained	O
from	O
the	O
old	O
values	O
of	O
the	O
successor	O
states	O
of	O
s	O
and	O
the	O
expected	B
immediate	O
rewards	O
along	O
all	O
the	O
one-step	O
transitions	O
possible	O
under	O
the	O
policy	B
being	O
evaluated	O
we	O
call	O
this	O
kind	O
of	O
operation	O
an	O
expected	B
update	I
each	O
iteration	O
of	O
iterative	B
policy	B
evaluation	O
updates	O
the	O
value	B
of	O
every	O
state	B
once	O
to	O
produce	O
the	O
new	O
approximate	O
value	B
function	I
there	O
are	O
several	O
different	O
kinds	O
of	O
expected	B
updates	O
depending	O
on	O
whether	O
a	O
state	B
here	O
or	O
a	O
state	B
action	B
pair	O
is	O
being	O
updated	O
and	O
depending	O
on	O
the	O
precise	O
way	O
the	O
estimated	O
values	O
of	O
the	O
successor	O
states	O
are	O
combined	O
all	O
the	O
updates	O
done	O
in	O
dp	O
algorithms	O
are	O
called	O
expected	B
updates	O
because	O
they	O
are	O
based	O
on	O
an	O
expectation	O
over	O
all	O
possible	O
next	O
states	O
rather	O
than	O
on	O
a	O
sample	O
next	O
state	B
the	O
nature	O
of	O
an	O
update	O
can	O
be	O
expressed	O
in	O
an	O
equation	O
as	O
above	O
or	O
in	O
a	O
backup	B
diagram	I
like	O
those	O
introduced	O
in	O
chapter	O
for	O
example	O
the	O
backup	B
diagram	I
corresponding	O
to	O
the	O
expected	B
update	I
used	O
in	O
iterative	B
policy	B
evaluation	O
is	O
shown	O
on	O
page	O
to	O
write	O
a	O
sequential	O
computer	O
program	O
to	O
implement	O
iterative	B
policy	B
evaluation	O
as	O
given	O
by	O
you	O
would	O
have	O
to	O
use	O
two	O
arrays	O
one	O
for	O
the	O
old	O
values	O
vks	O
and	O
one	O
for	O
the	O
new	O
values	O
with	O
two	O
arrays	O
the	O
new	O
values	O
can	O
be	O
computed	O
one	O
by	O
one	O
from	O
the	O
old	O
values	O
without	O
the	O
old	O
values	O
being	O
changed	O
of	O
course	O
it	O
is	O
easier	O
to	O
use	O
one	O
array	O
and	O
update	O
the	O
values	O
in	O
place	O
that	O
is	O
with	O
each	O
new	O
value	B
immediately	O
overwriting	O
the	O
old	O
one	O
then	O
depending	O
on	O
the	O
order	O
in	O
which	O
the	O
states	O
are	O
updated	O
sometimes	O
new	O
values	O
are	O
used	O
instead	O
of	O
old	O
ones	O
on	O
the	O
right-hand	O
side	O
of	O
this	O
in-place	O
algorithm	O
also	O
converges	O
to	O
v	O
in	O
fact	O
it	O
usually	O
converges	O
faster	O
than	O
the	O
two-array	O
version	O
as	O
you	O
might	O
expect	O
because	O
it	O
uses	O
new	O
data	O
as	O
soon	O
as	O
they	O
are	O
available	O
we	O
think	O
of	O
the	O
updates	O
as	O
being	O
done	O
in	O
a	O
sweep	O
through	O
the	O
state	B
space	O
for	O
the	O
in-place	O
algorithm	O
the	O
order	O
in	O
which	O
states	O
have	O
their	O
values	O
updated	O
during	O
the	O
sweep	O
has	O
a	O
significant	O
influence	O
on	O
the	O
rate	O
of	O
convergence	O
we	O
usually	O
have	O
the	O
in-place	O
version	O
in	O
mind	O
when	O
we	O
think	O
of	O
dp	O
algorithms	O
a	O
complete	O
in-place	O
version	O
of	O
iterative	B
policy	B
evaluation	O
is	O
shown	O
in	O
pseudocode	O
in	O
the	O
box	O
below	O
note	O
how	O
it	O
handles	O
termination	O
formally	O
iterative	B
policy	B
evaluation	O
converges	O
only	O
in	O
the	O
limit	O
but	O
in	O
practice	O
it	O
must	O
be	O
halted	O
short	O
of	O
this	O
the	O
pseudocode	O
tests	O
the	O
quantity	O
maxs	O
s	O
vks	O
after	O
each	O
sweep	O
and	O
stops	O
when	O
it	O
is	O
sufficiently	O
small	O
iterative	B
policy	B
evaluation	O
for	O
estimating	O
v	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
algorithm	O
parameter	O
a	O
small	O
threshold	O
determining	O
accuracy	O
of	O
estimation	O
initialize	O
v	O
for	O
all	O
s	O
s	O
arbitrarily	O
except	O
that	O
v	O
loop	O
loop	O
for	O
each	O
s	O
s	O
until	O
v	O
v	O
max	O
v	O
v	O
rs	O
v	O
chapter	O
dynamic	B
programming	I
example	O
consider	O
the	O
gridworld	O
shown	O
below	O
the	O
nonterminal	O
states	O
are	O
s	O
there	O
are	O
four	O
actions	O
possible	O
in	O
each	O
state	B
a	O
down	O
right	O
left	O
which	O
deterministically	O
cause	O
the	O
corresponding	O
state	B
transitions	O
except	O
that	O
actions	O
that	O
would	O
take	O
the	O
agent	O
off	O
the	O
grid	O
in	O
fact	O
leave	O
the	O
state	B
unchanged	O
thus	O
for	O
instance	O
right	O
right	O
and	O
right	O
for	O
all	O
r	O
r	O
this	O
is	O
an	O
undiscounted	O
episodic	O
task	O
the	O
reward	O
is	O
on	O
all	O
transitions	O
until	O
the	O
terminal	O
state	B
is	O
reached	O
the	O
terminal	O
state	B
is	O
shaded	O
in	O
the	O
figure	O
it	O
is	O
shown	O
in	O
two	O
places	O
it	O
is	O
formally	O
one	O
state	B
the	O
expected	B
reward	O
function	O
is	O
thus	O
rs	O
a	O
for	O
all	O
states	O
s	O
and	O
actions	O
a	O
suppose	O
the	O
agent	O
follows	O
the	O
equiprobable	O
random	O
policy	B
actions	O
equally	O
likely	O
the	O
left	O
side	O
of	O
figure	O
shows	O
the	O
sequence	O
of	O
value	B
functions	O
computed	O
by	O
iterative	B
policy	B
evaluation	O
the	O
final	O
estimate	O
is	O
in	O
fact	O
v	O
which	O
in	O
this	O
case	O
gives	O
for	O
each	O
state	B
the	O
negation	O
of	O
the	O
expected	B
number	O
of	O
steps	O
from	O
that	O
state	B
until	O
termination	O
exercise	O
in	O
example	O
if	O
is	O
the	O
equiprobable	O
random	O
policy	B
what	O
is	O
q	O
down	O
what	O
is	O
q	O
down	O
exercise	O
in	O
example	O
suppose	O
a	O
new	O
state	B
is	O
added	O
to	O
the	O
gridworld	O
just	O
below	O
state	B
and	O
its	O
actions	O
left	O
up	O
right	O
and	O
down	O
take	O
the	O
agent	O
to	O
states	O
and	O
respectively	O
assume	O
that	O
the	O
transitions	O
from	O
the	O
original	O
states	O
are	O
unchanged	O
what	O
then	O
is	O
v	O
for	O
the	O
equiprobable	O
random	O
policy	B
now	O
suppose	O
the	O
dynamics	O
of	O
state	B
are	O
also	O
changed	O
such	O
that	O
action	B
down	O
from	O
state	B
takes	O
the	O
agent	O
to	O
the	O
new	O
state	B
what	O
is	O
v	O
for	O
the	O
equiprobable	O
random	O
policy	B
in	O
this	O
case	O
exercise	O
what	O
are	O
the	O
equations	O
analogous	O
to	O
and	O
for	O
the	O
actionvalue	O
function	O
q	O
and	O
its	O
successive	O
approximation	O
by	O
a	O
sequence	O
of	O
functions	O
policy	B
improvement	I
our	O
reason	O
for	O
computing	O
the	O
value	B
function	I
for	O
a	O
policy	B
is	O
to	O
help	O
find	O
better	O
policies	O
suppose	O
we	O
have	O
determined	O
the	O
value	B
function	I
v	O
for	O
an	O
arbitrary	O
deterministic	O
policy	B
for	O
some	O
state	B
s	O
we	O
would	O
like	O
to	O
know	O
whether	O
or	O
not	O
we	O
should	O
change	O
the	O
policy	B
to	O
deterministically	O
choose	O
an	O
action	B
a	O
we	O
know	O
how	O
good	O
it	O
is	O
to	O
follow	O
the	O
current	O
policy	B
from	O
s	O
that	O
is	O
v	O
but	O
would	O
it	O
be	O
better	O
or	O
worse	O
to	O
change	O
to	O
the	O
new	O
policy	B
one	O
way	O
to	O
answer	O
this	O
question	O
is	O
to	O
consider	O
selecting	O
a	O
in	O
s	O
and	O
thereafter	O
actionsr	O
all	O
policy	B
improvement	I
figure	O
convergence	O
of	O
iterative	B
policy	B
evaluation	O
on	O
a	O
small	O
gridworld	O
the	O
left	O
column	O
is	O
the	O
sequence	O
of	O
approximations	O
of	O
the	O
state-value	O
function	O
for	O
the	O
random	O
policy	B
actions	O
equal	O
the	O
right	O
column	O
is	O
the	O
sequence	O
of	O
greedy	O
policies	O
corresponding	O
to	O
the	O
value	B
function	I
estimates	O
are	O
shown	O
for	O
all	O
actions	O
achieving	O
the	O
maximum	O
the	O
last	O
policy	B
is	O
guaranteed	O
only	O
to	O
be	O
an	O
improvement	O
over	O
the	O
random	O
policy	B
but	O
in	O
this	O
case	O
it	O
and	O
all	O
policies	O
after	O
the	O
third	O
iteration	O
are	O
optimal	O
for	O
therandom	O
policygreedy	O
policyw	O
r	O
t	O
vkk	O
policyrandom	O
policy	B
for	O
therandom	O
policyvkgreedy	O
policy	B
w	O
r	O
t	O
vk	O
chapter	O
dynamic	B
programming	I
following	O
the	O
existing	O
policy	B
the	O
value	B
of	O
this	O
way	O
of	O
behaving	O
is	O
q	O
a	O
v	O
st	O
s	O
at	O
a	O
rs	O
v	O
the	O
key	O
criterion	O
is	O
whether	O
this	O
is	O
greater	O
than	O
or	O
less	O
than	O
v	O
if	O
it	O
is	O
greater	O
that	O
is	O
if	O
it	O
is	O
better	O
to	O
select	O
a	O
once	O
in	O
s	O
and	O
thereafter	O
follow	O
than	O
it	O
would	O
be	O
to	O
follow	O
all	O
the	O
time	O
then	O
one	O
would	O
expect	O
it	O
to	O
be	O
better	O
still	O
to	O
select	O
a	O
every	O
time	O
s	O
is	O
encountered	O
and	O
that	O
the	O
new	O
policy	B
would	O
in	O
fact	O
be	O
a	O
better	O
one	O
overall	O
that	O
this	O
is	O
true	O
is	O
a	O
special	O
case	O
of	O
a	O
general	O
result	O
called	O
the	O
policy	B
improvement	I
theorem	B
let	O
and	O
be	O
any	O
pair	O
of	O
deterministic	O
policies	O
such	O
that	O
for	O
all	O
s	O
s	O
q	O
v	O
then	O
the	O
policy	B
must	O
be	O
as	O
good	O
as	O
or	O
better	O
than	O
that	O
is	O
it	O
must	O
obtain	O
greater	O
or	O
equal	O
expected	B
return	B
from	O
all	O
states	O
s	O
s	O
v	O
v	O
moreover	O
if	O
there	O
is	O
strict	O
inequality	O
of	O
at	O
any	O
state	B
then	O
there	O
must	O
be	O
strict	O
inequality	O
of	O
at	O
at	O
least	O
one	O
state	B
this	O
result	O
applies	O
in	O
particular	O
to	O
the	O
two	O
policies	O
that	O
we	O
considered	O
in	O
the	O
previous	O
paragraph	O
an	O
original	O
deterministic	O
policy	B
and	O
a	O
changed	O
policy	B
that	O
is	O
identical	O
to	O
except	O
that	O
a	O
obviously	O
holds	O
at	O
all	O
states	O
other	O
than	O
s	O
thus	O
if	O
q	O
a	O
v	O
then	O
the	O
changed	O
policy	B
is	O
indeed	O
better	O
than	O
the	O
idea	O
behind	O
the	O
proof	B
of	O
the	O
policy	B
improvement	I
theorem	B
is	O
easy	O
to	O
understand	O
starting	O
from	O
we	O
keep	O
expanding	O
the	O
q	O
side	O
with	O
and	O
reapplying	O
until	O
we	O
get	O
v	O
v	O
q	O
v	O
st	O
s	O
at	O
e	O
v	O
st	O
s	O
e	O
q	O
st	O
s	O
e	O
e	O
v	O
st	O
s	O
e	O
st	O
e	O
st	O
e	O
st	O
v	O
so	O
far	O
we	O
have	O
seen	O
how	O
given	O
a	O
policy	B
and	O
its	O
value	B
function	I
we	O
can	O
easily	O
evaluate	O
a	O
change	O
in	O
the	O
policy	B
at	O
a	O
single	O
state	B
to	O
a	O
particular	O
action	B
it	O
is	O
a	O
natural	O
extension	O
policy	B
improvement	I
to	O
consider	O
changes	O
at	O
all	O
states	O
and	O
to	O
all	O
possible	O
actions	O
selecting	O
at	O
each	O
state	B
the	O
action	B
that	O
appears	O
best	O
according	O
to	O
q	O
a	O
in	O
other	O
words	O
to	O
consider	O
the	O
new	O
greedy	O
policy	B
given	O
by	O
argmax	O
a	O
argmax	O
argmax	O
q	O
a	O
v	O
st	O
s	O
at	O
a	O
rs	O
v	O
a	O
a	O
where	O
argmaxa	O
denotes	O
the	O
value	B
of	O
a	O
at	O
which	O
the	O
expression	O
that	O
follows	O
is	O
maximized	O
ties	O
broken	O
arbitrarily	O
the	O
greedy	O
policy	B
takes	O
the	O
action	B
that	O
looks	O
best	O
in	O
the	O
short	O
term	O
after	O
one	O
step	O
of	O
lookahead	O
according	O
to	O
v	O
by	O
construction	O
the	O
greedy	O
policy	B
meets	O
the	O
conditions	O
of	O
the	O
policy	B
improvement	I
theorem	B
so	O
we	O
know	O
that	O
it	O
is	O
as	O
good	O
as	O
or	O
better	O
than	O
the	O
original	O
policy	B
the	O
process	O
of	O
making	O
a	O
new	O
policy	B
that	O
improves	O
on	O
an	O
original	O
policy	B
by	O
making	O
it	O
greedy	O
with	O
respect	O
to	O
the	O
value	B
function	I
of	O
the	O
original	O
policy	B
is	O
called	O
policy	B
improvement	I
suppose	O
the	O
new	O
greedy	O
policy	B
is	O
as	O
good	O
as	O
but	O
not	O
better	O
than	O
the	O
old	O
policy	B
then	O
v	O
v	O
and	O
from	O
it	O
follows	O
that	O
for	O
all	O
s	O
s	O
v	O
max	O
max	O
a	O
v	O
st	O
s	O
at	O
a	O
rs	O
v	O
a	O
but	O
this	O
is	O
the	O
same	O
as	O
the	O
bellman	B
optimality	O
equation	O
and	O
therefore	O
v	O
must	O
be	O
v	O
and	O
both	O
and	O
must	O
be	O
optimal	O
policies	O
policy	B
improvement	I
thus	O
must	O
give	O
us	O
a	O
strictly	O
better	O
policy	B
except	O
when	O
the	O
original	O
policy	B
is	O
already	O
optimal	O
so	O
far	O
in	O
this	O
section	O
we	O
have	O
considered	O
the	O
special	O
case	O
of	O
deterministic	O
policies	O
in	O
the	O
general	O
case	O
a	O
stochastic	O
policy	B
specifies	O
probabilities	O
for	O
taking	O
each	O
action	B
a	O
in	O
each	O
state	B
s	O
we	O
will	O
not	O
go	O
through	O
the	O
details	O
but	O
in	O
fact	O
all	O
the	O
ideas	O
of	O
this	O
section	O
extend	O
easily	O
to	O
stochastic	O
policies	O
in	O
particular	O
the	O
policy	B
improvement	I
theorem	B
carries	O
through	O
as	O
stated	O
for	O
the	O
stochastic	O
case	O
in	O
addition	O
if	O
there	O
are	O
ties	O
in	O
policy	B
improvement	I
steps	O
such	O
as	O
that	O
is	O
if	O
there	O
are	O
several	O
actions	O
at	O
which	O
the	O
maximum	O
is	O
achieved	O
then	O
in	O
the	O
stochastic	O
case	O
we	O
need	O
not	O
select	O
a	O
single	O
action	B
from	O
among	O
them	O
instead	O
each	O
maximizing	O
action	B
can	O
be	O
given	O
a	O
portion	O
of	O
the	O
probability	O
of	O
being	O
selected	O
in	O
the	O
new	O
greedy	O
policy	B
any	O
apportioning	O
scheme	O
is	O
allowed	O
as	O
long	O
as	O
all	O
submaximal	O
actions	O
are	O
given	O
zero	O
probability	O
the	O
last	O
row	O
of	O
figure	O
shows	O
an	O
example	O
of	O
policy	B
improvement	I
for	O
stochastic	O
policies	O
here	O
the	O
original	O
policy	B
is	O
the	O
equiprobable	O
random	O
policy	B
and	O
the	O
new	O
policy	B
is	O
greedy	O
with	O
respect	O
to	O
v	O
the	O
value	B
function	I
v	O
is	O
shown	O
in	O
the	O
bottomleft	O
diagram	O
and	O
the	O
set	O
of	O
possible	O
is	O
shown	O
in	O
the	O
bottom-right	O
diagram	O
the	O
states	O
with	O
multiple	O
arrows	O
in	O
the	O
diagram	O
are	O
those	O
in	O
which	O
several	O
actions	O
achieve	O
the	O
maximum	O
in	O
any	O
apportionment	O
of	O
probability	O
among	O
these	O
actions	O
is	O
permitted	O
the	O
value	B
function	I
of	O
any	O
such	O
policy	B
v	O
can	O
be	O
seen	O
by	O
inspection	O
to	O
be	O
either	O
or	O
at	O
all	O
states	O
s	O
s	O
whereas	O
v	O
is	O
at	O
most	O
thus	O
v	O
v	O
for	O
all	O
chapter	O
dynamic	B
programming	I
s	O
s	O
illustrating	O
policy	B
improvement	I
although	O
in	O
this	O
case	O
the	O
new	O
policy	B
happens	O
to	O
be	O
optimal	O
in	O
general	O
only	O
an	O
improvement	O
is	O
guaranteed	O
policy	B
iteration	I
once	O
a	O
policy	B
has	O
been	O
improved	O
using	O
v	O
to	O
yield	O
a	O
better	O
policy	B
we	O
can	O
then	O
compute	O
v	O
and	O
improve	O
it	O
again	O
to	O
yield	O
an	O
even	O
better	O
we	O
can	O
thus	O
obtain	O
a	O
sequence	O
of	O
monotonically	O
improving	O
policies	O
and	O
value	B
functions	O
i	O
i	O
e	O
v	O
e	O
v	O
e	O
v	O
e	O
e	O
denotes	O
a	O
policy	B
evaluation	O
and	O
i	O
i	O
denotes	O
a	O
policy	B
improvement	I
each	O
where	O
policy	B
is	O
guaranteed	O
to	O
be	O
a	O
strict	O
improvement	O
over	O
the	O
previous	O
one	O
it	O
is	O
already	O
optimal	O
because	O
a	O
finite	O
mdp	O
has	O
only	O
a	O
finite	O
number	O
of	O
policies	O
this	O
process	O
must	O
converge	O
to	O
an	O
optimal	O
policy	B
and	O
optimal	O
value	B
function	I
in	O
a	O
finite	O
number	O
of	O
iterations	O
this	O
way	O
of	O
finding	O
an	O
optimal	O
policy	B
is	O
called	O
policy	B
iteration	I
a	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
below	O
note	O
that	O
each	O
policy	B
evaluation	O
itself	O
an	O
iterative	B
computation	O
is	O
started	O
with	O
the	O
value	B
function	I
for	O
the	O
previous	O
policy	B
this	O
typically	O
results	O
in	O
a	O
great	O
increase	O
in	O
the	O
speed	O
of	O
convergence	O
of	O
policy	B
evaluation	O
because	O
the	O
value	B
function	I
changes	O
little	O
from	O
one	O
policy	B
to	O
the	O
next	O
policy	B
iteration	I
iterative	B
policy	B
evaluation	O
for	O
estimating	O
initialization	O
v	O
r	O
and	O
as	O
arbitrarily	O
for	O
all	O
s	O
s	O
policy	B
evaluation	O
loop	O
loop	O
for	O
each	O
s	O
s	O
v	O
v	O
max	O
v	O
v	O
rs	O
v	O
until	O
small	O
positive	O
number	O
determining	O
the	O
accuracy	O
of	O
estimation	O
policy	B
improvement	I
policy-stable	O
true	O
for	O
each	O
s	O
s	O
old-action	O
if	O
old-action	O
then	O
policy-stable	O
f	O
alse	O
rs	O
v	O
if	O
policy-stable	O
then	O
stop	O
and	O
return	B
v	O
v	O
and	O
else	O
go	O
to	O
policy	B
iteration	I
policy	B
iteration	I
often	O
converges	O
in	O
surprisingly	O
few	O
iterations	O
this	O
is	O
illustrated	O
by	O
the	O
example	O
in	O
figure	O
the	O
bottom-left	O
diagram	O
shows	O
the	O
value	B
function	I
for	O
the	O
equiprobable	O
random	O
policy	B
and	O
the	O
bottom-right	O
diagram	O
shows	O
a	O
greedy	O
policy	B
for	O
this	O
value	B
function	I
the	O
policy	B
improvement	I
theorem	B
assures	O
us	O
that	O
these	O
policies	O
are	O
better	O
than	O
the	O
original	O
random	O
policy	B
in	O
this	O
case	O
however	O
these	O
policies	O
are	O
not	O
just	O
better	O
but	O
optimal	O
proceeding	O
to	O
the	O
terminal	O
states	O
in	O
the	O
minimum	O
number	O
of	O
steps	O
in	O
this	O
example	O
policy	B
iteration	I
would	O
find	O
the	O
optimal	O
policy	B
after	O
just	O
one	O
iteration	O
exercise	O
the	O
policy	B
iteration	I
algorithm	O
on	O
the	O
previous	O
page	O
has	O
a	O
subtle	O
bug	O
in	O
that	O
it	O
may	O
never	O
terminate	O
if	O
the	O
policy	B
continually	O
switches	O
between	O
two	O
or	O
more	O
policies	O
that	O
are	O
equally	O
good	O
this	O
is	O
ok	O
for	O
pedagogy	O
but	O
not	O
for	O
actual	O
use	O
modify	O
the	O
pseudocode	O
so	O
that	O
convergence	O
is	O
guaranteed	O
exercise	O
how	O
would	O
policy	B
iteration	I
be	O
defined	O
for	B
action	B
values	I
give	O
a	O
complete	O
algorithm	O
for	O
computing	O
q	O
analogous	O
to	O
that	O
on	O
page	O
for	O
computing	O
v	O
please	O
pay	O
special	O
attention	O
to	O
this	O
exercise	O
because	O
the	O
ideas	O
involved	O
will	O
be	O
used	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
exercise	O
suppose	O
you	O
are	O
restricted	O
to	O
considering	O
only	O
policies	O
that	O
are	O
meaning	O
that	O
the	O
probability	O
of	O
selecting	O
each	O
action	B
in	O
each	O
state	B
s	O
is	O
at	O
least	O
describe	O
qualitatively	O
the	O
changes	O
that	O
would	O
be	O
required	O
in	O
each	O
of	O
the	O
steps	O
and	O
in	O
that	O
order	O
of	O
the	O
policy	B
iteration	I
algorithm	O
for	O
v	O
example	O
jack	O
s	O
car	O
rental	O
jack	O
manages	O
two	O
locations	O
for	O
a	O
nationwide	O
car	O
rental	O
company	O
each	O
day	O
some	O
number	O
of	O
customers	O
arrive	O
at	O
each	O
location	O
to	O
rent	O
cars	O
if	O
jack	O
has	O
a	O
car	O
available	O
he	O
rents	O
it	O
out	O
and	O
is	O
credited	O
by	O
the	O
national	O
company	O
if	O
he	O
is	O
out	O
of	O
cars	O
at	O
that	O
location	O
then	O
the	O
business	O
is	O
lost	O
cars	O
become	O
available	O
for	O
renting	O
the	O
day	O
after	O
they	O
are	O
returned	O
to	O
help	O
ensure	O
that	O
cars	O
are	O
available	O
where	O
they	O
are	O
needed	O
jack	O
can	O
move	O
them	O
between	O
the	O
two	O
locations	O
overnight	O
at	O
a	O
cost	O
of	O
per	O
car	O
moved	O
we	O
assume	O
that	O
the	O
number	O
of	O
cars	O
requested	O
and	O
returned	O
at	O
each	O
location	O
are	O
poisson	O
random	O
variables	O
meaning	O
that	O
the	O
probability	O
that	O
the	O
number	O
is	O
n	O
is	O
n	O
n	O
e	O
where	O
is	O
the	O
expected	B
number	O
suppose	O
is	O
and	O
for	O
rental	O
requests	O
at	O
the	O
first	O
and	O
second	O
locations	O
and	O
and	O
for	O
returns	O
to	O
simplify	O
the	O
problem	O
slightly	O
we	O
assume	O
that	O
there	O
can	O
be	O
no	O
more	O
than	O
cars	O
at	O
each	O
location	O
additional	O
cars	O
are	O
returned	O
to	O
the	O
nationwide	O
company	O
and	O
thus	O
disappear	O
from	O
the	O
problem	O
and	O
a	O
maximum	O
of	O
five	O
cars	O
can	O
be	O
moved	O
from	O
one	O
location	O
to	O
the	O
other	O
in	O
one	O
night	O
we	O
take	O
the	O
discount	O
rate	O
to	O
be	O
and	O
formulate	O
this	O
as	O
a	O
continuing	O
finite	O
mdp	O
where	O
the	O
time	O
steps	O
are	O
days	O
the	O
state	B
is	O
the	O
number	O
of	O
cars	O
at	O
each	O
location	O
at	O
the	O
end	O
of	O
the	O
day	O
and	O
the	O
actions	O
are	O
the	O
net	O
numbers	O
of	O
cars	O
moved	O
between	O
the	O
two	O
locations	O
overnight	O
figure	O
shows	O
the	O
sequence	O
of	O
policies	O
found	O
by	O
policy	B
iteration	I
starting	O
from	O
the	O
policy	B
that	O
never	O
moves	O
any	O
cars	O
exercise	O
write	O
a	O
program	O
for	O
policy	B
iteration	I
and	O
re-solve	O
jack	O
s	O
car	O
rental	O
problem	O
with	O
the	O
following	O
changes	O
one	O
of	O
jack	O
s	O
employees	O
at	O
the	O
first	O
location	O
rides	O
a	O
bus	O
home	O
each	O
night	O
and	O
lives	O
near	O
the	O
second	O
location	O
she	O
is	O
happy	O
to	O
shuttle	O
one	O
car	O
to	O
the	O
second	O
location	O
for	O
free	O
each	O
additional	O
car	O
still	O
costs	O
as	O
do	O
all	O
cars	O
moved	O
in	O
the	O
other	O
direction	O
in	O
addition	O
jack	O
has	O
limited	O
parking	O
space	O
at	O
each	O
location	O
if	O
more	O
than	O
cars	O
are	O
kept	O
overnight	O
at	O
a	O
location	O
any	O
moving	O
of	O
cars	O
then	O
an	O
chapter	O
dynamic	B
programming	I
figure	O
the	O
sequence	O
of	O
policies	O
found	O
by	O
policy	B
iteration	I
on	O
jack	O
s	O
car	O
rental	O
problem	O
and	O
the	O
final	O
state-value	O
function	O
the	O
first	O
five	O
diagrams	O
show	O
for	O
each	O
number	O
of	O
cars	O
at	O
each	O
location	O
at	O
the	O
end	O
of	O
the	O
day	O
the	O
number	O
of	O
cars	O
to	O
be	O
moved	O
from	O
the	O
first	O
location	O
to	O
the	O
second	O
numbers	O
indicate	O
transfers	O
from	O
the	O
second	O
location	O
to	O
the	O
first	O
each	O
successive	O
policy	B
is	O
a	O
strict	O
improvement	O
over	O
the	O
previous	O
policy	B
and	O
the	O
last	O
policy	B
is	O
optimal	O
additional	O
cost	O
of	O
must	O
be	O
incurred	O
to	O
use	O
a	O
second	O
parking	O
lot	O
of	O
how	O
many	O
cars	O
are	O
kept	O
there	O
these	O
sorts	O
of	O
nonlinearities	O
and	O
arbitrary	O
dynamics	O
often	O
occur	O
in	O
real	O
problems	O
and	O
cannot	O
easily	O
be	O
handled	O
by	O
optimization	O
methods	O
other	O
than	O
dynamic	B
programming	I
to	O
check	O
your	O
program	O
first	O
replicate	O
the	O
results	O
given	O
for	O
the	O
original	O
problem	O
if	O
your	O
computer	O
is	O
too	O
slow	O
for	O
the	O
full	O
problem	O
cut	O
all	O
the	O
numbers	O
of	O
cars	O
in	O
half	O
value	B
iteration	I
one	O
drawback	O
to	O
policy	B
iteration	I
is	O
that	O
each	O
of	O
its	O
iterations	O
involves	O
policy	B
evaluation	O
which	O
may	O
itself	O
be	O
a	O
protracted	O
iterative	B
computation	O
requiring	O
multiple	O
sweeps	O
through	O
the	O
state	B
set	O
if	O
policy	B
evaluation	O
is	O
done	O
iteratively	O
then	O
convergence	O
exactly	O
to	O
v	O
occurs	O
only	O
in	O
the	O
limit	O
must	O
we	O
wait	O
for	O
exact	O
convergence	O
or	O
can	O
we	O
stop	O
short	O
of	O
that	O
the	O
example	O
in	O
figure	O
certainly	O
suggests	O
that	O
it	O
may	O
be	O
possible	O
to	O
truncate	O
policy	B
evaluation	O
in	O
that	O
example	O
policy	B
evaluation	O
iterations	O
beyond	O
the	O
first	O
three	O
have	O
no	O
effect	O
on	O
the	O
corresponding	O
greedy	O
policy	B
at	O
second	O
at	O
first	O
at	O
second	O
locationcars	O
at	O
first	O
value	B
iteration	I
in	O
fact	O
the	O
policy	B
evaluation	O
step	O
of	O
policy	B
iteration	I
can	O
be	O
truncated	B
in	O
several	O
ways	O
without	O
losing	O
the	O
convergence	O
guarantees	O
of	O
policy	B
iteration	I
one	O
important	O
special	O
case	O
is	O
when	O
policy	B
evaluation	O
is	O
stopped	O
after	O
just	O
one	O
sweep	O
update	O
of	O
each	O
state	B
this	O
algorithm	O
is	O
called	O
value	B
iteration	I
it	O
can	O
be	O
written	O
as	O
a	O
particularly	O
simple	O
update	O
operation	O
that	O
combines	O
the	O
policy	B
improvement	I
and	O
truncated	B
policy	B
evaluation	O
steps	O
max	O
max	O
a	O
st	O
s	O
at	O
a	O
rs	O
a	O
for	O
all	O
s	O
s	O
for	O
arbitrary	O
the	O
sequence	O
can	O
be	O
shown	O
to	O
converge	O
to	O
v	O
under	O
the	O
same	O
conditions	O
that	O
guarantee	O
the	O
existence	O
of	O
v	O
another	O
way	O
of	O
understanding	O
value	B
iteration	I
is	O
by	O
reference	O
to	O
the	O
bellman	B
optimality	O
equation	O
note	O
that	O
value	B
iteration	I
is	O
obtained	O
simply	O
by	O
turning	O
the	O
bellman	B
optimality	O
equation	O
into	O
an	O
update	O
rule	O
also	O
note	O
how	O
the	O
value	B
iteration	I
update	O
is	O
identical	O
to	O
the	O
policy	B
evaluation	O
update	O
except	O
that	O
it	O
requires	O
the	O
maximum	O
to	O
be	O
taken	O
over	O
all	O
actions	O
another	O
way	O
of	O
seeing	O
this	O
close	O
relationship	O
is	O
to	O
compare	O
the	O
backup	O
diagrams	O
for	O
these	O
algorithms	O
on	O
page	O
evaluation	O
and	O
on	O
the	O
left	O
of	O
figure	O
iteration	O
these	O
two	O
are	O
the	O
natural	O
backup	O
operations	O
for	O
computing	O
v	O
and	O
v	O
finally	O
let	O
us	O
consider	O
how	O
value	B
iteration	I
terminates	O
like	O
policy	B
evaluation	O
value	B
iteration	I
formally	O
requires	O
an	O
infinite	O
number	O
of	O
iterations	O
to	O
converge	O
exactly	O
to	O
v	O
in	O
practice	O
we	O
stop	O
once	O
the	O
value	B
function	I
changes	O
by	O
only	O
a	O
small	O
amount	O
in	O
a	O
sweep	O
the	O
box	O
below	O
shows	O
a	O
complete	O
algorithm	O
with	O
this	O
kind	O
of	O
termination	O
condition	O
value	B
iteration	I
for	O
estimating	O
algorithm	O
parameter	O
a	O
small	O
threshold	O
determining	O
accuracy	O
of	O
estimation	O
initialize	O
v	O
for	O
all	O
s	O
s	O
arbitrarily	O
except	O
that	O
v	O
loop	O
loop	O
for	O
each	O
s	O
s	O
until	O
output	O
a	O
deterministic	O
policy	B
such	O
that	O
v	O
rs	O
v	O
rs	O
v	O
v	O
v	O
max	O
v	O
value	B
iteration	I
effectively	O
combines	O
in	O
each	O
of	O
its	O
sweeps	O
one	O
sweep	O
of	O
policy	B
evaluation	O
and	O
one	O
sweep	O
of	O
policy	B
improvement	I
faster	O
convergence	O
is	O
often	O
achieved	O
by	O
interposing	O
multiple	O
policy	B
evaluation	O
sweeps	O
between	O
each	O
policy	B
improvement	I
sweep	O
in	O
general	O
the	O
entire	O
class	O
of	O
truncated	B
policy	B
iteration	I
algorithms	O
can	O
be	O
thought	O
of	O
chapter	O
dynamic	B
programming	I
as	O
sequences	O
of	O
sweeps	O
some	O
of	O
which	O
use	O
policy	B
evaluation	O
updates	O
and	O
some	O
of	O
which	O
use	O
value	B
iteration	I
updates	O
because	O
the	O
max	O
operation	O
in	O
is	O
the	O
only	O
difference	O
between	O
these	O
updates	O
this	O
just	O
means	O
that	O
the	O
max	O
operation	O
is	O
added	O
to	O
some	O
sweeps	O
of	O
policy	B
evaluation	O
all	O
of	O
these	O
algorithms	O
converge	O
to	O
an	O
optimal	O
policy	B
for	O
discounted	O
finite	O
mdps	O
example	O
gambler	O
s	O
problem	O
a	O
gambler	O
has	O
the	O
opportunity	O
to	O
make	O
bets	O
on	O
the	O
outcomes	O
of	O
a	O
sequence	O
of	O
coin	O
flips	O
if	O
the	O
coin	O
comes	O
up	O
heads	O
he	O
wins	O
as	O
many	O
dollars	O
as	O
he	O
has	O
staked	O
on	O
that	O
flip	O
if	O
it	O
is	O
tails	O
he	O
loses	O
his	O
stake	O
the	O
game	O
ends	O
when	O
the	O
gambler	O
wins	O
by	O
reaching	O
his	O
goal	O
of	O
or	O
loses	O
by	O
running	O
out	O
of	O
money	O
on	O
each	O
flip	O
the	O
gambler	O
must	O
decide	O
what	O
portion	O
of	O
his	O
capital	O
to	O
stake	O
in	O
integer	O
numbers	O
of	O
dollars	O
this	O
problem	O
can	O
be	O
formulated	O
as	O
an	O
undiscounted	O
episodic	O
finite	O
mdp	O
the	O
state	B
is	O
the	O
gambler	O
s	O
capital	O
s	O
and	O
the	O
actions	O
are	O
stakes	O
a	O
mins	O
s	O
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
except	O
those	O
on	O
which	O
the	O
gambler	O
reaches	O
his	O
goal	O
when	O
it	O
is	O
the	O
statevalue	O
function	O
then	O
gives	O
the	O
probability	O
of	O
winning	O
from	O
each	O
state	B
a	O
policy	B
is	O
a	O
mapping	O
from	O
levels	O
of	O
capital	O
to	O
stakes	O
the	O
optimal	O
policy	B
maximizes	O
the	O
probability	O
of	O
reaching	O
the	O
goal	O
let	O
ph	O
denote	O
the	O
probability	O
of	O
the	O
coin	O
coming	O
up	O
heads	O
if	O
ph	O
is	O
known	O
then	O
the	O
entire	O
problem	O
is	O
known	O
and	O
it	O
can	O
be	O
solved	O
for	O
instance	O
by	O
value	B
iteration	I
figure	O
shows	O
the	O
change	O
in	O
the	O
value	B
function	I
over	O
successive	O
sweeps	O
of	O
value	B
iteration	I
and	O
the	O
final	O
policy	B
found	O
for	O
the	O
case	O
of	O
ph	O
this	O
policy	B
is	O
optimal	O
but	O
not	O
unique	O
in	O
fact	O
there	O
is	O
a	O
whole	O
family	O
of	O
optimal	O
policies	O
all	O
corresponding	O
to	O
ties	O
for	O
the	O
argmax	O
action	B
selection	O
with	O
respect	O
to	O
the	O
optimal	O
value	B
function	I
can	O
you	O
guess	O
what	O
the	O
entire	O
family	O
looks	O
like	O
figure	O
the	O
solution	O
to	O
the	O
gambler	O
s	O
problem	O
for	O
ph	O
the	O
upper	O
graph	O
shows	O
the	O
value	B
function	I
found	O
by	O
successive	O
sweeps	O
of	O
value	B
iteration	I
the	O
lower	O
graph	O
shows	O
the	O
final	O
policy	B
exercise	O
why	O
does	O
the	O
optimal	O
policy	B
for	O
the	O
gambler	O
s	O
problem	O
have	O
such	O
a	O
curious	O
form	O
in	O
particular	O
for	O
capital	O
of	O
it	O
bets	O
it	O
all	O
on	O
one	O
flip	O
but	O
for	O
capital	O
of	O
it	O
does	O
not	O
why	O
is	O
this	O
a	O
good	O
policy	B
exercise	O
implement	O
value	B
iteration	I
for	O
the	O
gambler	O
s	O
problem	O
and	O
solve	O
it	O
for	O
ph	O
and	O
ph	O
in	O
programming	O
you	O
may	O
find	O
it	O
convenient	O
valuefunction	O
asynchronous	B
dynamic	B
programming	I
to	O
introduce	O
two	O
dummy	O
states	O
corresponding	O
to	O
termination	O
with	O
capital	O
of	O
and	O
giving	O
them	O
values	O
of	O
and	O
respectively	O
show	O
your	O
results	O
graphically	O
as	O
in	O
figure	O
are	O
your	O
results	O
stable	O
as	O
exercise	O
what	O
is	O
the	O
analog	O
of	O
the	O
value	B
iteration	I
update	O
for	B
action	B
values	I
a	O
asynchronous	B
dynamic	B
programming	I
a	O
major	O
drawback	O
to	O
the	O
dp	O
methods	O
that	O
we	O
have	O
discussed	O
so	O
far	O
is	O
that	O
they	O
involve	O
operations	O
over	O
the	O
entire	O
state	B
set	O
of	O
the	O
mdp	O
that	O
is	O
they	O
require	O
sweeps	O
of	O
the	O
state	B
set	O
if	O
the	O
state	B
set	O
is	O
very	O
large	O
then	O
even	O
a	O
single	O
sweep	O
can	O
be	O
prohibitively	O
expensive	O
for	O
example	O
the	O
game	O
of	O
backgammon	B
has	O
over	O
states	O
even	O
if	O
we	O
could	O
perform	O
the	O
value	B
iteration	I
update	O
on	O
a	O
million	O
states	O
per	O
second	O
it	O
would	O
take	O
over	O
a	O
thousand	O
years	O
to	O
complete	O
a	O
single	O
sweep	O
asynchronous	O
dp	O
algorithms	O
are	O
in-place	O
iterative	B
dp	O
algorithms	O
that	O
are	O
not	O
organized	O
in	O
terms	O
of	O
systematic	O
sweeps	O
of	O
the	O
state	B
set	O
these	O
algorithms	O
update	O
the	O
values	O
of	O
states	O
in	O
any	O
order	O
whatsoever	O
using	O
whatever	O
values	O
of	O
other	O
states	O
happen	O
to	O
be	O
available	O
the	O
values	O
of	O
some	O
states	O
may	O
be	O
updated	O
several	O
times	O
before	O
the	O
values	O
of	O
others	O
are	O
updated	O
once	O
to	O
converge	O
correctly	O
however	O
an	O
asynchronous	O
algorithm	O
must	O
continue	O
to	O
update	O
the	O
values	O
of	O
all	O
the	O
states	O
it	O
can	O
t	O
ignore	O
any	O
state	B
after	O
some	O
point	O
in	O
the	O
computation	O
asynchronous	O
dp	O
algorithms	O
allow	O
great	O
flexibility	O
in	O
selecting	O
states	O
to	O
update	O
for	O
example	O
one	O
version	O
of	O
asynchronous	O
value	B
iteration	I
updates	O
the	O
value	B
in	O
place	O
of	O
only	O
one	O
state	B
sk	O
on	O
each	O
step	O
k	O
using	O
the	O
value	B
iteration	I
update	O
if	O
asymptotic	O
convergence	O
to	O
v	O
is	O
guaranteed	O
given	O
only	O
that	O
all	O
states	O
occur	O
in	O
the	O
sequence	O
an	O
infinite	O
number	O
of	O
times	O
sequence	O
could	O
even	O
be	O
stochastic	O
the	O
undiscounted	O
episodic	O
case	O
it	O
is	O
possible	O
that	O
there	O
are	O
some	O
orderings	O
of	O
updates	O
that	O
do	O
not	O
result	O
in	O
convergence	O
but	O
it	O
is	O
relatively	O
easy	O
to	O
avoid	O
these	O
similarly	O
it	O
is	O
possible	O
to	O
intermix	O
policy	B
evaluation	O
and	O
value	B
iteration	I
updates	O
to	O
produce	O
a	O
kind	O
of	O
asynchronous	O
truncated	B
policy	B
iteration	I
although	O
the	O
details	O
of	O
this	O
and	O
other	O
more	O
unusual	O
dp	O
algorithms	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
it	O
is	O
clear	O
that	O
a	O
few	O
different	O
updates	O
form	O
building	O
blocks	O
that	O
can	O
be	O
used	O
flexibly	O
in	O
a	O
wide	O
variety	O
of	O
sweepless	O
dp	O
algorithms	O
of	O
course	O
avoiding	O
sweeps	O
does	O
not	O
necessarily	O
mean	O
that	O
we	O
can	O
get	O
away	O
with	O
less	O
computation	O
it	O
just	O
means	O
that	O
an	O
algorithm	O
does	O
not	O
need	O
to	O
get	O
locked	O
into	O
any	O
hopelessly	O
long	O
sweep	O
before	O
it	O
can	O
make	O
progress	O
improving	O
a	O
policy	B
we	O
can	O
try	O
to	O
take	O
advantage	O
of	O
this	O
flexibility	O
by	O
selecting	O
the	O
states	O
to	O
which	O
we	O
apply	O
updates	O
so	O
as	O
to	O
improve	O
the	O
algorithm	O
s	O
rate	O
of	O
progress	O
we	O
can	O
try	O
to	O
order	O
the	O
updates	O
to	O
let	O
value	B
information	O
propagate	O
from	O
state	B
to	O
state	B
in	O
an	O
efficient	O
way	O
some	O
states	O
may	O
not	O
need	O
their	O
values	O
updated	O
as	O
often	O
as	O
others	O
we	O
might	O
even	O
try	O
to	O
skip	O
updating	O
some	O
states	O
entirely	O
if	O
they	O
are	O
not	O
relevant	O
to	O
optimal	O
behavior	O
some	O
ideas	O
for	O
doing	O
this	O
are	O
discussed	O
in	O
chapter	O
asynchronous	O
algorithms	O
also	O
make	O
it	O
easier	O
to	O
intermix	O
computation	O
with	O
real-time	O
chapter	O
dynamic	B
programming	I
interaction	O
to	O
solve	O
a	O
given	O
mdp	O
we	O
can	O
run	O
an	O
iterative	B
dp	O
algorithm	O
at	O
the	O
same	O
time	O
that	O
an	O
agent	O
is	O
actually	O
experiencing	O
the	O
mdp	O
the	O
agent	O
s	O
experience	O
can	O
be	O
used	O
to	O
determine	O
the	O
states	O
to	O
which	O
the	O
dp	O
algorithm	O
applies	O
its	O
updates	O
at	O
the	O
same	O
time	O
the	O
latest	O
value	B
and	O
policy	B
information	O
from	O
the	O
dp	O
algorithm	O
can	O
guide	O
the	O
agent	O
s	O
decision	O
making	O
for	O
example	O
we	O
can	O
apply	O
updates	O
to	O
states	O
as	O
the	O
agent	O
visits	O
them	O
this	O
makes	O
it	O
possible	O
to	O
focus	O
the	O
dp	O
algorithm	O
s	O
updates	O
onto	O
parts	O
of	O
the	O
state	B
set	O
that	O
are	O
most	O
relevant	O
to	O
the	O
agent	O
this	O
kind	O
of	O
focusing	O
is	O
a	O
repeated	O
theme	O
in	O
reinforcement	B
learning	I
generalized	O
policy	B
iteration	I
policy	B
iteration	I
consists	O
of	O
two	O
simultaneous	O
interacting	O
processes	O
one	O
making	O
the	O
value	B
function	I
consistent	O
with	O
the	O
current	O
policy	B
evaluation	O
and	O
the	O
other	O
making	O
the	O
policy	B
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
improvement	O
in	O
policy	B
iteration	I
these	O
two	O
processes	O
alternate	O
each	O
completing	O
before	O
the	O
other	O
begins	O
but	O
this	O
is	O
not	O
really	O
necessary	O
in	O
value	B
iteration	I
for	O
example	O
only	O
a	O
single	O
iteration	O
of	O
policy	B
evaluation	O
is	O
performed	O
in	O
between	O
each	O
policy	B
improvement	I
in	O
asynchronous	O
dp	O
methods	O
the	O
evaluation	O
and	O
improvement	O
processes	O
are	O
interleaved	O
at	O
an	O
even	O
finer	O
grain	O
in	O
some	O
cases	O
a	O
single	O
state	B
is	O
updated	O
in	O
one	O
process	O
before	O
returning	O
to	O
the	O
other	O
as	O
long	O
as	O
both	O
processes	O
continue	O
to	O
update	O
all	O
states	O
the	O
ultimate	O
result	O
is	O
typically	O
the	O
same	O
convergence	O
to	O
the	O
optimal	O
value	B
function	I
and	O
an	O
optimal	O
policy	B
we	O
use	O
the	O
term	O
generalized	O
policy	B
iteration	I
to	O
refer	O
to	O
the	O
general	O
idea	O
of	O
letting	O
policy	B
evaluation	O
and	O
policy	B
improvement	I
processes	O
interact	O
independent	O
of	O
the	O
granularity	O
and	O
other	O
details	O
of	O
the	O
two	O
processes	O
almost	O
all	O
reinforcement	B
learning	I
methods	O
are	O
well	O
described	O
as	O
gpi	O
that	O
is	O
all	O
have	O
identifiable	O
policies	O
and	O
value	B
functions	O
with	O
the	O
policy	B
always	O
being	O
improved	O
with	O
respect	O
to	O
the	O
value	B
function	I
and	O
the	O
value	B
function	I
always	O
being	O
driven	O
toward	O
the	O
value	B
function	I
for	O
the	O
policy	B
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
it	O
is	O
easy	O
to	O
see	O
that	O
if	O
both	O
the	O
evaluation	O
process	O
and	O
the	O
improvement	O
process	O
stabilize	O
that	O
is	O
no	O
longer	O
produce	O
changes	O
then	O
the	O
value	B
function	I
and	O
policy	B
must	O
be	O
optimal	O
the	O
value	B
function	I
stabilizes	O
only	O
when	O
it	O
is	O
consistent	O
with	O
the	O
current	O
policy	B
and	O
the	O
policy	B
stabilizes	O
only	O
when	O
it	O
is	O
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
thus	O
both	O
processes	O
stabilize	O
only	O
when	O
a	O
policy	B
has	O
been	O
found	O
that	O
is	O
greedy	O
with	O
respect	O
to	O
its	O
own	O
evaluation	O
function	O
this	O
implies	O
that	O
the	O
bellman	B
optimality	O
equation	O
holds	O
and	O
thus	O
that	O
the	O
policy	B
and	O
the	O
value	B
function	I
are	O
optimal	O
the	O
evaluation	O
and	O
improvement	O
processes	O
in	O
gpi	O
can	O
be	O
viewed	O
as	O
both	O
competing	O
and	O
cooperating	O
they	O
compete	O
in	O
the	O
sense	O
that	O
they	O
pull	O
in	O
opposing	O
directions	O
making	O
the	O
policy	B
greedy	O
with	O
respect	O
to	O
the	O
value	B
function	I
typically	O
makes	O
the	O
value	B
function	I
incorrect	O
for	O
the	O
changed	O
policy	B
and	O
making	O
the	O
value	B
function	I
consistent	O
with	O
the	O
policy	B
typically	O
causes	O
that	O
policy	B
no	O
longer	O
to	O
be	O
greedy	O
in	O
the	O
long	O
run	O
however	O
evaluationimprovement	O
greedyvv	O
v	O
v	O
v	O
efficiency	O
of	O
dynamic	B
programming	I
these	O
two	O
processes	O
interact	O
to	O
find	O
a	O
single	O
joint	O
solution	O
the	O
optimal	O
value	B
function	I
and	O
an	O
optimal	O
policy	B
one	O
might	O
also	O
think	O
of	O
the	O
interaction	O
between	O
the	O
evaluation	O
and	O
improvement	O
processes	O
in	O
gpi	O
in	O
terms	O
of	O
two	O
constraints	O
or	O
goals	O
for	O
example	O
as	O
two	O
lines	O
in	O
two-dimensional	O
space	O
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
although	O
the	O
real	O
geometry	O
is	O
much	O
more	O
complicated	O
than	O
this	O
the	O
diagram	O
suggests	O
what	O
happens	O
in	O
the	O
real	O
case	O
each	O
process	O
drives	O
the	O
value	B
function	I
or	O
policy	B
toward	O
one	O
of	O
the	O
lines	O
representing	O
a	O
solution	O
to	O
one	O
of	O
the	O
two	O
goals	O
the	O
goals	O
interact	O
because	O
the	O
two	O
lines	O
are	O
not	O
orthogonal	O
driving	O
directly	O
toward	O
one	O
goal	O
causes	O
some	O
movement	O
away	O
from	O
the	O
other	O
goal	O
inevitably	O
however	O
the	O
joint	O
process	O
is	O
brought	O
closer	O
to	O
the	O
overall	O
goal	O
of	O
optimality	O
the	O
arrows	O
in	O
this	O
diagram	O
correspond	O
to	O
the	O
behavior	O
of	O
policy	B
iteration	I
in	O
that	O
each	O
takes	O
the	O
system	O
all	O
the	O
way	O
to	O
achieving	O
one	O
of	O
the	O
two	O
goals	O
completely	O
in	O
gpi	O
one	O
could	O
also	O
take	O
smaller	O
incomplete	O
steps	O
toward	O
each	O
goal	O
in	O
either	O
case	O
the	O
two	O
processes	O
together	O
achieve	O
the	O
overall	O
goal	O
of	O
optimality	O
even	O
though	O
neither	O
is	O
attempting	O
to	O
achieve	O
it	O
directly	O
efficiency	O
of	O
dynamic	B
programming	I
dp	O
may	O
not	O
be	O
practical	O
for	O
very	O
large	O
problems	O
but	O
compared	O
with	O
other	O
methods	O
for	O
solving	O
mdps	O
dp	O
methods	O
are	O
actually	O
quite	O
efficient	O
if	O
we	O
ignore	O
a	O
few	O
technical	O
details	O
then	O
the	O
case	O
time	O
dp	O
methods	O
take	O
to	O
find	O
an	O
optimal	O
policy	B
is	O
polynomial	O
in	O
the	O
number	O
of	O
states	O
and	O
actions	O
if	O
n	O
and	O
k	O
denote	O
the	O
number	O
of	O
states	O
and	O
actions	O
this	O
means	O
that	O
a	O
dp	O
method	O
takes	O
a	O
number	O
of	O
computational	O
operations	O
that	O
is	O
less	O
than	O
some	O
polynomial	O
function	O
of	O
n	O
and	O
k	O
a	O
dp	O
method	O
is	O
guaranteed	O
to	O
find	O
an	O
optimal	O
policy	B
in	O
polynomial	O
time	O
even	O
though	O
the	O
total	O
number	O
of	O
policies	O
is	O
kn	O
in	O
this	O
sense	O
dp	O
is	O
exponentially	O
faster	O
than	O
any	O
direct	O
search	O
in	O
policy	B
space	O
could	O
be	O
because	O
direct	O
search	O
would	O
have	O
to	O
exhaustively	O
examine	O
each	O
policy	B
to	O
provide	O
the	O
same	O
guarantee	O
linear	O
programming	O
methods	O
can	O
also	O
be	O
used	O
to	O
solve	O
mdps	O
and	O
in	O
some	O
cases	O
their	O
worst-case	O
convergence	O
guarantees	O
are	O
better	O
than	O
those	O
of	O
dp	O
methods	O
but	O
linear	O
programming	O
methods	O
become	O
impractical	O
at	O
a	O
much	O
smaller	O
number	O
of	O
states	O
than	O
do	O
dp	O
methods	O
a	O
factor	O
of	O
about	O
for	O
the	O
largest	O
problems	O
only	O
dp	O
methods	O
are	O
feasible	O
dp	O
is	O
sometimes	O
thought	O
to	O
be	O
of	O
limited	O
applicability	O
because	O
of	O
the	O
curse	B
of	I
dimensionality	I
the	O
fact	O
that	O
the	O
number	O
of	O
states	O
often	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
state	B
variables	O
large	O
state	B
sets	O
do	O
create	O
difficulties	O
but	O
these	O
are	O
inherent	B
difficulties	O
of	O
the	O
problem	O
not	O
of	O
dp	O
as	O
a	O
solution	O
method	O
in	O
fact	O
dp	O
is	O
comparatively	O
better	O
suited	O
to	O
handling	O
large	O
state	B
spaces	O
than	O
competing	O
methods	O
such	O
as	O
direct	O
search	O
and	O
linear	O
programming	O
in	O
practice	O
dp	O
methods	O
can	O
be	O
used	O
with	O
today	O
s	O
computers	O
to	O
solve	O
mdps	O
with	O
millions	O
of	O
states	O
both	O
policy	B
iteration	I
and	O
value	B
iteration	I
are	O
widely	O
used	O
and	O
it	O
is	O
not	O
v	O
vv	O
chapter	O
dynamic	B
programming	I
clear	O
which	O
if	O
either	O
is	O
better	O
in	O
general	O
in	O
practice	O
these	O
methods	O
usually	O
converge	O
much	O
faster	O
than	O
their	O
theoretical	O
worst-case	O
run	O
times	O
particularly	O
if	O
they	O
are	O
started	O
with	O
good	O
initial	O
value	B
functions	O
or	O
policies	O
on	O
problems	O
with	O
large	O
state	B
spaces	O
asynchronous	O
dp	O
methods	O
are	O
often	O
preferred	O
to	O
complete	O
even	O
one	O
sweep	O
of	O
a	O
synchronous	O
method	O
requires	O
computation	O
and	O
memory	O
for	O
every	O
state	B
for	O
some	O
problems	O
even	O
this	O
much	O
memory	O
and	O
computation	O
is	O
impractical	O
yet	O
the	O
problem	O
is	O
still	O
potentially	O
solvable	O
because	O
relatively	O
few	O
states	O
occur	O
along	O
optimal	O
solution	O
trajectories	O
asynchronous	O
methods	O
and	O
other	O
variations	O
of	O
gpi	O
can	O
be	O
applied	O
in	O
such	O
cases	O
and	O
may	O
find	O
good	O
or	O
optimal	O
policies	O
much	O
faster	O
than	O
synchronous	O
methods	O
can	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
become	O
familiar	O
with	O
the	O
basic	O
ideas	O
and	O
algorithms	O
of	O
dynamic	B
programming	I
as	O
they	O
relate	O
to	O
solving	O
finite	O
mdps	O
policy	B
evaluation	O
refers	O
to	O
the	O
iterative	B
computation	O
of	O
the	O
value	B
functions	O
for	O
a	O
given	O
policy	B
policy	B
improvement	I
refers	O
to	O
the	O
computation	O
of	O
an	O
improved	O
policy	B
given	O
the	O
value	B
function	I
for	O
that	O
policy	B
putting	O
these	O
two	O
computations	O
together	O
we	O
obtain	O
policy	B
iteration	I
and	O
value	B
iteration	I
the	O
two	O
most	O
popular	O
dp	O
methods	O
either	O
of	O
these	O
can	O
be	O
used	O
to	O
reliably	O
compute	O
optimal	O
policies	O
and	O
value	B
functions	O
for	O
finite	O
mdps	O
given	O
complete	O
knowledge	O
of	O
the	O
mdp	O
classical	O
dp	O
methods	O
operate	O
in	O
sweeps	O
through	O
the	O
state	B
set	O
performing	O
an	O
expected	B
update	I
operation	O
on	O
each	O
state	B
each	O
such	O
operation	O
updates	O
the	O
value	B
of	O
one	O
state	B
based	O
on	O
the	O
values	O
of	O
all	O
possible	O
successor	O
states	O
and	O
their	O
probabilities	O
of	O
occurring	O
expected	B
updates	O
are	O
closely	O
related	O
to	O
bellman	B
equations	O
they	O
are	O
little	O
more	O
than	O
these	O
equations	O
turned	O
into	O
assignment	O
statements	O
when	O
the	O
updates	O
no	O
longer	O
result	O
in	O
any	O
changes	O
in	O
value	B
convergence	O
has	O
occurred	O
to	O
values	O
that	O
satisfy	O
the	O
corresponding	O
bellman	B
equation	I
just	O
as	O
there	O
are	O
four	O
primary	O
value	B
functions	O
v	O
q	O
and	O
q	O
there	O
are	O
four	O
corresponding	O
bellman	B
equations	O
and	O
four	O
corresponding	O
expected	B
updates	O
an	O
intuitive	O
view	O
of	O
the	O
operation	O
of	O
dp	O
updates	O
is	O
given	O
by	O
their	O
backup	O
diagrams	O
insight	O
into	O
dp	O
methods	O
and	O
in	O
fact	O
into	O
almost	O
all	O
reinforcement	B
learning	I
methods	O
can	O
be	O
gained	O
by	O
viewing	O
them	O
as	O
generalized	O
policy	B
iteration	I
gpi	O
is	O
the	O
general	O
idea	O
of	O
two	O
interacting	O
processes	O
revolving	O
around	O
an	O
approximate	O
policy	B
and	O
an	O
approximate	O
value	B
function	I
one	O
process	O
takes	O
the	O
policy	B
as	O
given	O
and	O
performs	O
some	O
form	O
of	O
policy	B
evaluation	O
changing	O
the	O
value	B
function	I
to	O
be	O
more	O
like	O
the	O
true	O
value	B
function	I
for	O
the	O
policy	B
the	O
other	O
process	O
takes	O
the	O
value	B
function	I
as	O
given	O
and	O
performs	O
some	O
form	O
of	O
policy	B
improvement	I
changing	O
the	O
policy	B
to	O
make	O
it	O
better	O
assuming	O
that	O
the	O
value	B
function	I
is	O
its	O
value	B
function	I
although	O
each	O
process	O
changes	O
the	O
basis	O
for	O
the	O
other	O
overall	O
they	O
work	O
together	O
to	O
find	O
a	O
joint	O
solution	O
a	O
policy	B
and	O
value	B
function	I
that	O
are	O
unchanged	O
by	O
either	O
process	O
and	O
consequently	O
are	O
optimal	O
in	O
some	O
cases	O
gpi	O
can	O
be	O
proved	O
to	O
converge	O
most	O
notably	O
for	O
the	O
classical	O
dp	O
methods	O
that	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
in	O
other	O
cases	O
convergence	O
has	O
not	O
been	O
proved	O
but	O
still	O
the	O
idea	O
of	O
gpi	O
improves	O
our	O
understanding	O
of	O
the	O
methods	O
it	O
is	O
not	O
necessary	O
to	O
perform	O
dp	O
methods	O
in	O
complete	O
sweeps	O
through	O
the	O
state	B
summary	O
set	O
asynchronous	O
dp	O
methods	O
are	O
in-place	O
iterative	B
methods	O
that	O
update	O
states	O
in	O
an	O
arbitrary	O
order	O
perhaps	O
stochastically	O
determined	O
and	O
using	O
out-of-date	O
information	O
many	O
of	O
these	O
methods	O
can	O
be	O
viewed	O
as	O
fine-grained	O
forms	O
of	O
gpi	O
finally	O
we	O
note	O
one	O
last	O
special	O
property	O
of	O
dp	O
methods	O
all	O
of	O
them	O
update	O
estimates	O
of	O
the	O
values	O
of	O
states	O
based	O
on	O
estimates	O
of	O
the	O
values	O
of	O
successor	O
states	O
that	O
is	O
they	O
update	O
estimates	O
on	O
the	O
basis	O
of	O
other	O
estimates	O
we	O
call	O
this	O
general	O
idea	O
bootstrapping	B
many	O
reinforcement	B
learning	I
methods	O
perform	O
bootstrapping	B
even	O
those	O
that	O
do	O
not	O
require	O
as	O
dp	O
requires	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	B
in	O
the	O
next	O
chapter	O
we	O
explore	O
reinforcement	B
learning	I
methods	O
that	O
do	O
not	O
require	O
a	O
model	O
and	O
do	O
not	O
bootstrap	O
in	O
the	O
chapter	O
after	O
that	O
we	O
explore	O
methods	O
that	O
do	O
not	O
require	O
a	O
model	O
but	O
do	O
bootstrap	O
these	O
key	O
features	O
and	O
properties	O
are	O
separable	O
yet	O
can	O
be	O
mixed	O
in	O
interesting	O
combinations	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
term	O
dynamic	B
programming	I
is	O
due	O
to	O
bellman	B
who	O
showed	O
how	O
these	O
methods	O
could	O
be	O
applied	O
to	O
a	O
wide	O
range	O
of	O
problems	O
extensive	O
treatments	O
of	O
dp	O
can	O
be	O
found	O
in	O
many	O
texts	O
including	O
bertsekas	O
bertsekas	O
and	O
tsitsiklis	O
dreyfus	O
and	O
law	O
ross	O
white	O
and	O
whittle	O
our	O
interest	O
in	O
dp	O
is	O
restricted	O
to	O
its	O
use	O
in	O
solving	O
mdps	O
but	O
dp	O
also	O
applies	O
to	O
other	O
types	O
of	O
problems	O
kumar	O
and	O
kanal	O
provide	O
a	O
more	O
general	O
look	O
at	O
dp	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
the	O
first	O
connection	O
between	O
dp	O
and	B
reinforcement	B
learning	I
was	O
made	O
by	O
minsky	B
in	O
commenting	O
on	O
samuel	O
s	O
checkers	O
player	O
in	O
a	O
footnote	O
minsky	B
mentioned	O
that	O
it	O
is	O
possible	O
to	O
apply	O
dp	O
to	O
problems	O
in	O
which	O
samuel	O
s	O
backing-up	O
process	O
can	O
be	O
handled	O
in	O
closed	O
analytic	O
form	O
this	O
remark	O
may	O
have	O
misled	O
artificial	B
intelligence	I
researchers	O
into	O
believing	O
that	O
dp	O
was	O
restricted	O
to	O
analytically	O
tractable	O
problems	O
and	O
therefore	O
largely	O
irrelevant	O
to	O
artificial	B
intelligence	I
andreae	B
mentioned	O
dp	O
in	O
the	O
context	O
of	O
reinforcement	B
learning	I
specifically	O
policy	B
iteration	I
although	O
he	O
did	O
not	O
make	O
specific	O
connections	O
between	O
dp	O
and	O
learning	O
algorithms	O
werbos	B
suggested	O
an	O
approach	O
to	O
approximating	O
dp	O
called	O
heuristic	O
dynamic	B
programming	I
that	O
emphasizes	O
gradient-descent	O
methods	O
for	O
continuous-state	O
problems	O
these	O
methods	O
are	O
closely	O
related	O
to	O
the	O
reinforcement	B
learning	I
algorithms	O
that	O
we	O
discuss	O
in	O
this	O
book	O
watkins	B
was	O
explicit	O
in	O
connecting	O
reinforcement	B
learning	I
to	O
dp	O
characterizing	O
a	O
class	O
of	O
reinforcement	B
learning	I
methods	O
as	O
incremental	O
dynamic	B
programming	I
these	O
sections	O
describe	O
well-established	O
dp	O
algorithms	O
that	O
are	O
covered	O
in	O
any	O
of	O
the	O
general	O
dp	O
references	O
cited	O
above	O
the	O
policy	B
improvement	I
theorem	B
and	O
the	O
policy	B
iteration	I
algorithm	O
are	O
due	O
to	O
bellman	B
and	O
howard	O
our	O
presentation	O
was	O
influenced	O
by	O
the	O
local	O
view	O
of	O
policy	B
improvement	I
taken	O
by	O
watkins	B
our	O
discussion	O
of	O
value	B
iteration	I
as	O
a	O
form	O
of	O
truncated	B
policy	B
iteration	I
is	O
based	O
on	O
the	O
approach	O
of	O
puterman	O
and	O
shin	O
who	O
presented	O
a	O
class	O
of	O
algorithms	O
called	O
modified	O
policy	B
iteration	I
which	O
includes	O
policy	B
iteration	I
and	O
value	B
iteration	I
as	O
special	O
cases	O
an	O
analysis	O
showing	O
how	O
chapter	O
dynamic	B
programming	I
value	B
iteration	I
can	O
be	O
made	O
to	O
find	O
an	O
optimal	O
policy	B
in	O
finite	O
time	O
is	O
given	O
by	O
bertsekas	O
iterative	B
policy	B
evaluation	O
is	O
an	O
example	O
of	O
a	O
classical	O
successive	O
approximation	O
algorithm	O
for	O
solving	O
a	O
system	O
of	O
linear	O
equations	O
the	O
version	O
of	O
the	O
algorithm	O
that	O
uses	O
two	O
arrays	O
one	O
holding	O
the	O
old	O
values	O
while	O
the	O
other	O
is	O
updated	O
is	O
often	O
called	O
a	O
jacobi-style	O
algorithm	O
after	O
jacobi	O
s	O
classical	O
use	O
of	O
this	O
method	O
it	O
is	O
also	O
sometimes	O
called	O
a	O
synchronous	O
algorithm	O
because	O
the	O
effect	O
is	O
as	O
if	O
all	O
the	O
values	O
are	O
updated	O
at	O
the	O
same	O
time	O
the	O
second	O
array	O
is	O
needed	O
to	O
simulate	O
this	O
parallel	O
computation	O
sequentially	O
the	O
in-place	O
version	O
of	O
the	O
algorithm	O
is	O
often	O
called	O
a	O
gauss	O
seidel-style	O
algorithm	O
after	O
the	O
classical	O
gauss	O
seidel	O
algorithm	O
for	O
solving	O
systems	O
of	O
linear	O
equations	O
in	O
addition	O
to	O
iterative	B
policy	B
evaluation	O
other	O
dp	O
algorithms	O
can	O
be	O
implemented	O
in	O
these	O
different	O
versions	O
bertsekas	O
and	O
tsitsiklis	O
provide	O
excellent	O
coverage	O
of	O
these	O
variations	O
and	O
their	O
performance	O
differences	O
asynchronous	O
dp	O
algorithms	O
are	O
due	O
to	O
bertsekas	O
who	O
also	O
called	O
them	O
distributed	O
dp	O
algorithms	O
the	O
original	O
motivation	B
for	O
asynchronous	O
dp	O
was	O
its	O
implementation	O
on	O
a	O
multiprocessor	O
system	O
with	O
communication	O
delays	O
between	O
processors	O
and	O
no	O
global	O
synchronizing	O
clock	O
these	O
algorithms	O
are	O
extensively	O
discussed	O
by	O
bertsekas	O
and	O
tsitsiklis	O
jacobi-style	O
and	O
gauss	O
seidel-style	O
dp	O
algorithms	O
are	O
special	O
cases	O
of	O
the	O
asynchronous	O
version	O
williams	O
and	O
baird	O
presented	O
dp	O
algorithms	O
that	O
are	O
asynchronous	O
at	O
a	O
finer	O
grain	O
than	O
the	O
ones	O
we	O
have	O
discussed	O
the	O
update	O
operations	O
themselves	O
are	O
broken	O
into	O
steps	O
that	O
can	O
be	O
performed	O
asynchronously	O
this	O
section	O
written	O
with	O
the	O
help	O
of	O
michael	O
littman	O
is	O
based	O
on	O
littman	O
dean	O
and	O
kaelbling	O
the	O
phrase	O
curse	B
of	I
dimensionality	I
is	O
due	O
to	O
bellman	B
chapter	O
monte	B
carlo	I
methods	I
in	O
this	O
chapter	O
we	O
consider	O
our	O
first	O
learning	O
methods	O
for	O
estimating	O
value	B
functions	O
and	O
discovering	O
optimal	O
policies	O
unlike	O
the	O
previous	O
chapter	O
here	O
we	O
do	O
not	O
assume	O
complete	O
knowledge	O
of	O
the	O
environment	B
monte	B
carlo	I
methods	I
require	O
only	O
experience	O
sample	O
sequences	O
of	O
states	O
actions	O
and	O
rewards	O
from	O
actual	O
or	O
simulated	O
interaction	O
with	O
an	O
environment	B
learning	O
from	O
actual	O
experience	O
is	O
striking	O
because	O
it	O
requires	O
no	O
prior	B
knowledge	I
of	O
the	O
environment	B
s	O
dynamics	O
yet	O
can	O
still	O
attain	O
optimal	O
behavior	O
learning	O
from	O
simulated	O
experience	O
is	O
also	O
powerful	O
although	O
a	O
model	O
is	O
required	O
the	O
model	O
need	O
only	O
generate	O
sample	O
transitions	O
not	O
the	O
complete	O
probability	O
distributions	O
of	O
all	O
possible	O
transitions	O
that	O
is	O
required	O
for	B
dynamic	B
programming	I
in	O
surprisingly	O
many	O
cases	O
it	O
is	O
easy	O
to	O
generate	O
experience	O
sampled	O
according	O
to	O
the	O
desired	O
probability	O
distributions	O
but	O
infeasible	O
to	O
obtain	O
the	O
distributions	O
in	O
explicit	O
form	O
monte	B
carlo	I
methods	I
are	O
ways	O
of	O
solving	O
the	O
reinforcement	B
learning	I
problem	O
based	O
on	O
averaging	O
sample	O
returns	O
to	O
ensure	O
that	O
well-defined	O
returns	O
are	O
available	O
here	O
we	O
define	O
monte	B
carlo	I
methods	I
only	O
for	O
episodic	O
tasks	O
that	O
is	O
we	O
assume	O
experience	O
is	O
divided	O
into	O
episodes	B
and	O
that	O
all	O
episodes	B
eventually	O
terminate	O
no	O
matter	O
what	O
actions	O
are	O
selected	O
only	O
on	O
the	O
completion	O
of	O
an	O
episode	O
are	O
value	B
estimates	O
and	O
policies	O
changed	O
monte	B
carlo	I
methods	I
can	O
thus	O
be	O
incremental	O
in	O
an	O
episode-by-episode	O
sense	O
but	O
not	O
in	O
a	O
step-by-step	O
sense	O
the	O
term	O
monte	B
carlo	I
is	O
often	O
used	O
more	O
broadly	O
for	O
any	O
estimation	O
method	O
whose	O
operation	O
involves	O
a	O
significant	O
random	O
component	O
here	O
we	O
use	O
it	O
specifically	O
for	O
methods	O
based	O
on	O
averaging	O
complete	O
returns	O
opposed	O
to	O
methods	O
that	O
learn	O
from	O
partial	O
returns	O
considered	O
in	O
the	O
next	O
chapter	O
monte	B
carlo	I
methods	I
sample	O
and	O
average	O
returns	O
for	O
each	O
state	B
action	B
pair	O
much	O
like	O
the	O
bandit	O
methods	O
we	O
explored	O
in	O
chapter	O
sample	O
and	O
average	O
rewards	O
for	O
each	O
action	B
the	O
main	O
difference	O
is	O
that	O
now	O
there	O
are	O
multiple	O
states	O
each	O
acting	O
like	O
a	O
different	O
bandit	O
problem	O
an	O
associative-search	O
or	O
contextual	O
bandit	O
and	O
the	O
different	O
bandit	B
problems	I
are	O
interrelated	O
that	O
is	O
the	O
return	B
after	O
taking	O
an	O
action	B
in	O
one	O
state	B
depends	O
on	O
the	O
actions	O
taken	O
in	O
later	O
states	O
in	O
the	O
same	O
episode	O
because	O
all	O
the	O
action	B
selections	O
are	O
undergoing	O
learning	O
the	O
problem	O
becomes	O
nonstationary	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
earlier	O
state	B
chapter	O
monte	B
carlo	I
methods	I
to	O
handle	O
the	O
nonstationarity	B
we	O
adapt	O
the	O
idea	O
of	O
general	O
policy	B
iteration	I
developed	O
in	O
chapter	O
for	O
dp	O
whereas	O
there	O
we	O
computed	O
value	B
functions	O
from	O
knowledge	O
of	O
the	O
mdp	O
here	O
we	O
learn	O
value	B
functions	O
from	O
sample	O
returns	O
with	O
the	O
mdp	O
the	O
value	B
functions	O
and	O
corresponding	O
policies	O
still	O
interact	O
to	O
attain	O
optimality	O
in	O
essentially	O
the	O
same	O
way	O
as	O
in	O
the	O
dp	O
chapter	O
first	O
we	O
consider	O
the	O
prediction	B
problem	O
computation	O
of	O
v	O
and	O
q	O
for	O
a	O
fixed	O
arbitrary	O
policy	B
then	O
policy	B
improvement	I
and	O
finally	O
the	O
control	B
problem	O
and	O
its	O
solution	O
by	O
gpi	O
each	O
of	O
these	O
ideas	O
taken	O
from	O
dp	O
is	O
extended	O
to	O
the	O
monte	B
carlo	I
case	O
in	O
which	O
only	O
sample	O
experience	O
is	O
available	O
monte	B
carlo	I
prediction	B
we	O
begin	O
by	O
considering	O
monte	B
carlo	I
methods	I
for	O
learning	O
the	O
state-value	O
function	O
for	O
a	O
given	O
policy	B
recall	O
that	O
the	O
value	B
of	O
a	O
state	B
is	O
the	O
expected	B
return	B
expected	B
cumulative	O
future	O
discounted	O
reward	O
starting	O
from	O
that	O
state	B
an	O
obvious	O
way	O
to	O
estimate	O
it	O
from	O
experience	O
then	O
is	O
simply	O
to	O
average	O
the	O
returns	O
observed	O
after	O
visits	O
to	O
that	O
state	B
as	O
more	O
returns	O
are	O
observed	O
the	O
average	O
should	O
converge	O
to	O
the	O
expected	B
value	B
this	O
idea	O
underlies	O
all	O
monte	B
carlo	I
methods	I
in	O
particular	O
suppose	O
we	O
wish	O
to	O
estimate	O
v	O
the	O
value	B
of	O
a	O
state	B
s	O
under	O
policy	B
given	O
a	O
set	O
of	O
episodes	B
obtained	O
by	O
following	O
and	O
passing	O
through	O
s	O
each	O
occurrence	O
of	O
state	B
s	O
in	O
an	O
episode	O
is	O
called	O
a	O
visit	O
to	O
s	O
of	O
course	O
s	O
may	O
be	O
visited	O
multiple	O
times	O
in	O
the	O
same	O
episode	O
let	O
us	O
call	O
the	O
first	O
time	O
it	O
is	O
visited	O
in	O
an	O
episode	O
the	O
first	O
visit	O
to	O
s	O
the	O
first-visit	O
mc	O
method	O
estimates	O
v	O
as	O
the	O
average	O
of	O
the	O
returns	O
following	O
first	O
visits	O
to	O
s	O
whereas	O
the	O
every-visit	O
mc	O
method	O
averages	O
the	O
returns	O
following	O
all	O
visits	O
to	O
s	O
these	O
two	O
monte	B
carlo	I
methods	I
are	O
very	O
similar	O
but	O
have	O
slightly	O
different	O
theoretical	O
properties	O
first-visit	O
mc	O
has	O
been	O
most	O
widely	O
studied	O
dating	O
back	O
to	O
the	O
and	O
is	O
the	O
one	O
we	O
focus	O
on	O
in	O
this	O
chapter	O
every-visit	O
mc	O
extends	O
more	O
naturally	O
to	O
function	B
approximation	I
and	B
eligibility	B
traces	I
as	O
discussed	O
in	O
chapters	O
and	O
first-visit	O
mc	O
is	O
shown	O
in	O
procedural	O
form	O
in	O
the	O
box	O
every-visit	O
mc	O
would	O
be	O
the	O
same	O
except	O
without	O
the	O
check	O
for	O
st	O
having	O
occurred	O
earlier	O
in	O
the	O
episode	O
first-visit	O
mc	O
prediction	B
for	O
estimating	O
v	O
v	O
input	O
a	O
policy	B
to	O
be	O
evaluated	O
initialize	O
v	O
r	O
arbitrarily	O
for	O
all	O
s	O
s	O
returnss	O
an	O
empty	O
list	O
for	O
all	O
s	O
s	O
loop	O
forever	O
each	O
episode	O
generate	O
an	O
episode	O
following	O
st	O
at	O
rt	O
g	O
loop	O
for	O
each	O
step	O
of	O
episode	O
t	O
t	O
t	O
g	O
g	O
unless	O
st	O
appears	O
in	O
st	O
append	O
g	O
to	O
returnsst	O
v	O
averagereturnsst	O
monte	B
carlo	I
prediction	B
both	O
first-visit	O
mc	O
and	O
every-visit	O
mc	O
converge	O
to	O
v	O
as	O
the	O
number	O
of	O
visits	O
first	O
visits	O
to	O
s	O
goes	O
to	O
infinity	O
this	O
is	O
easy	O
to	O
see	O
for	O
the	O
case	O
of	O
first-visit	O
mc	O
in	O
this	O
case	O
each	O
return	B
is	O
an	O
independent	O
identically	O
distributed	O
estimate	O
of	O
v	O
with	O
finite	O
variance	O
by	O
the	O
law	O
of	O
large	O
numbers	O
the	O
sequence	O
of	B
averages	I
of	O
these	O
estimates	O
converges	O
to	O
their	O
expected	B
value	B
each	O
average	O
is	O
itself	O
an	O
unbiased	O
estimate	O
and	O
the	O
standard	O
deviation	O
of	O
its	O
error	O
falls	O
as	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
returns	O
averaged	O
every-visit	O
mc	O
is	O
less	O
straightforward	O
but	O
its	O
estimates	O
also	O
converge	O
quadratically	O
to	O
v	O
and	O
sutton	O
the	O
use	O
of	O
monte	B
carlo	I
methods	I
is	O
best	O
illustrated	O
through	O
an	O
example	O
example	O
blackjack	O
the	O
object	O
of	O
the	O
popular	O
casino	O
card	O
game	O
of	O
blackjack	O
is	O
to	O
obtain	O
cards	O
the	O
sum	O
of	O
whose	O
numerical	O
values	O
is	O
as	O
great	O
as	O
possible	O
without	O
exceeding	O
all	O
face	O
cards	O
count	O
as	O
and	O
an	O
ace	O
can	O
count	O
either	O
as	O
or	O
as	O
we	O
consider	O
the	O
version	O
in	O
which	O
each	O
player	O
competes	O
independently	O
against	O
the	O
dealer	O
the	O
game	O
begins	O
with	O
two	O
cards	O
dealt	O
to	O
both	O
dealer	O
and	O
player	O
one	O
of	O
the	O
dealer	O
s	O
cards	O
is	O
face	O
up	O
and	O
the	O
other	O
is	O
face	O
down	O
if	O
the	O
player	O
has	O
immediately	O
ace	O
and	O
a	O
it	O
is	O
called	O
a	O
natural	O
he	O
then	O
wins	O
unless	O
the	O
dealer	O
also	O
has	O
a	O
natural	O
in	O
which	O
case	O
the	O
game	O
is	O
a	O
draw	O
if	O
the	O
player	O
does	O
not	O
have	O
a	O
natural	O
then	O
he	O
can	O
request	O
additional	O
cards	O
one	O
by	O
one	O
until	O
he	O
either	O
stops	O
or	O
exceeds	O
bust	O
if	O
he	O
goes	O
bust	O
he	O
loses	O
if	O
he	O
sticks	O
then	O
it	O
becomes	O
the	O
dealer	O
s	O
turn	O
the	O
dealer	O
hits	O
or	O
sticks	O
according	O
to	O
a	O
fixed	O
strategy	O
without	O
choice	O
he	O
sticks	O
on	O
any	O
sum	O
of	O
or	O
greater	O
and	O
hits	O
otherwise	O
if	O
the	O
dealer	O
goes	O
bust	O
then	O
the	O
player	O
wins	O
otherwise	O
the	O
outcome	O
win	O
lose	O
or	O
draw	O
is	O
determined	O
by	O
whose	O
final	O
sum	O
is	O
closer	O
to	O
playing	O
blackjack	O
is	O
naturally	O
formulated	O
as	O
an	O
episodic	O
finite	O
mdp	O
each	O
game	O
of	O
blackjack	O
is	O
an	O
episode	O
rewards	O
of	O
and	O
are	O
given	O
for	O
winning	O
losing	O
and	O
drawing	O
respectively	O
all	O
rewards	O
within	O
a	O
game	O
are	O
zero	O
and	O
we	O
do	O
not	O
discount	O
therefore	O
these	O
terminal	O
rewards	O
are	O
also	O
the	O
returns	O
the	O
player	O
s	O
actions	O
are	O
to	O
hit	O
or	O
to	O
stick	O
the	O
states	O
depend	O
on	O
the	O
player	O
s	O
cards	O
and	O
the	O
dealer	O
s	O
showing	O
card	O
we	O
assume	O
that	O
cards	O
are	O
dealt	O
from	O
an	O
infinite	O
deck	O
with	O
replacement	O
so	O
that	O
there	O
is	O
no	O
advantage	O
to	O
keeping	O
track	O
of	O
the	O
cards	O
already	O
dealt	O
if	O
the	O
player	O
holds	O
an	O
ace	O
that	O
he	O
could	O
count	O
as	O
without	O
going	O
bust	O
then	O
the	O
ace	O
is	O
said	O
to	O
be	O
usable	O
in	O
this	O
case	O
it	O
is	O
always	O
counted	O
as	O
because	O
counting	O
it	O
as	O
would	O
make	O
the	O
sum	O
or	O
less	O
in	O
which	O
case	O
there	O
is	O
no	O
decision	O
to	O
be	O
made	O
because	O
obviously	O
the	O
player	O
should	O
always	O
hit	O
thus	O
the	O
player	O
makes	O
decisions	O
on	O
the	O
basis	O
of	O
three	O
variables	O
his	O
current	O
sum	O
the	O
dealer	O
s	O
one	O
showing	O
card	O
and	O
whether	O
or	O
not	O
he	O
holds	O
a	O
usable	O
ace	O
this	O
makes	O
for	O
a	O
total	O
of	O
states	O
consider	O
the	O
policy	B
that	O
sticks	O
if	O
the	O
player	O
s	O
sum	O
is	O
or	O
and	O
otherwise	O
hits	O
to	O
find	O
the	O
state-value	O
function	O
for	O
this	O
policy	B
by	O
a	O
monte	B
carlo	I
approach	O
one	O
simulates	O
many	O
blackjack	O
games	O
using	O
the	O
policy	B
and	O
averages	O
the	O
returns	O
following	O
each	O
state	B
in	O
this	O
way	O
we	O
obtained	O
the	O
estimates	O
of	O
the	O
state-value	O
function	O
shown	O
in	O
figure	O
the	O
estimates	O
for	O
states	O
with	O
a	O
usable	O
ace	O
are	O
less	O
certain	O
and	O
less	O
regular	O
because	O
these	O
states	O
are	O
less	O
common	O
in	O
any	O
event	O
after	O
games	O
the	O
value	B
function	I
is	O
very	O
well	O
approximated	O
chapter	O
monte	B
carlo	I
methods	I
figure	O
approximate	O
state-value	O
functions	O
for	O
the	O
blackjack	O
policy	B
that	O
sticks	O
only	O
on	O
or	O
computed	O
by	O
monte	B
carlo	I
policy	B
evaluation	O
exercise	O
consider	O
the	O
diagrams	O
on	O
the	O
right	O
in	O
figure	O
why	O
does	O
the	O
estimated	O
value	B
function	I
jump	O
up	O
for	O
the	O
last	O
two	O
rows	O
in	O
the	O
rear	O
why	O
does	O
it	O
drop	O
off	O
for	O
the	O
whole	O
last	O
row	O
on	O
the	O
left	O
why	O
are	O
the	O
frontmost	O
values	O
higher	O
in	O
the	O
upper	O
diagrams	O
than	O
in	O
the	O
lower	O
exercise	O
suppose	O
every-visit	O
mc	O
was	O
used	O
instead	O
of	O
first-visit	O
mc	O
on	O
the	O
blackjack	O
task	O
would	O
you	O
expect	O
the	O
results	O
to	O
be	O
very	O
different	O
why	O
or	O
why	O
not	O
although	O
we	O
have	O
complete	O
knowledge	O
of	O
the	O
environment	B
in	O
the	O
blackjack	O
task	O
it	O
would	O
not	O
be	O
easy	O
to	O
apply	O
dp	O
methods	O
to	O
compute	O
the	O
value	B
function	I
dp	O
methods	O
require	O
the	O
distribution	O
of	O
next	O
events	O
in	O
particular	O
they	O
require	O
the	O
environments	O
dynamics	O
as	O
given	O
by	O
the	O
four-argument	O
function	O
p	O
and	O
it	O
is	O
not	O
easy	O
to	O
determine	O
this	O
for	O
blackjack	O
for	O
example	O
suppose	O
the	O
player	O
s	O
sum	O
is	O
and	O
he	O
chooses	O
to	O
stick	O
what	O
is	O
his	O
probability	O
of	O
terminating	O
with	O
a	O
reward	O
of	O
as	O
a	O
function	O
of	O
the	O
dealer	O
s	O
showing	O
card	O
all	O
of	O
the	O
probabilities	O
must	O
be	O
computed	O
before	O
dp	O
can	O
be	O
applied	O
and	O
such	O
computations	O
are	O
often	O
complex	O
and	O
error-prone	O
in	O
contrast	O
generating	O
the	O
sample	O
games	O
required	O
by	O
monte	B
carlo	I
methods	I
is	O
easy	O
this	O
is	O
the	O
case	O
surprisingly	O
often	O
the	O
ability	O
of	O
monte	B
carlo	I
methods	I
to	O
work	O
with	O
sample	O
episodes	B
alone	O
can	O
be	O
a	O
significant	O
advantage	O
even	O
when	O
one	O
has	O
complete	O
knowledge	O
of	O
the	O
environment	B
s	O
dynamics	O
can	O
we	O
generalize	O
the	O
idea	O
of	O
backup	O
diagrams	O
to	O
monte	B
carlo	I
algorithms	O
the	O
general	O
idea	O
of	O
a	O
backup	B
diagram	I
is	O
to	O
show	O
at	O
the	O
top	O
the	O
root	O
node	O
to	O
be	O
updated	O
and	O
to	O
show	O
below	O
all	O
the	O
transitions	O
and	O
leaf	O
nodes	O
whose	O
rewards	O
and	O
estimated	O
values	O
contribute	O
to	O
the	O
update	O
for	O
monte	B
carlo	I
estimation	O
of	O
v	O
the	O
root	O
is	O
a	O
state	B
node	O
and	O
below	O
it	O
is	O
the	O
entire	O
trajectory	O
of	O
transitions	O
along	O
a	O
particular	O
single	O
episode	O
ending	O
episodesafter	O
episodesusableacenousableace	O
monte	B
carlo	I
prediction	B
at	O
the	O
terminal	O
state	B
as	O
shown	O
to	O
the	O
right	O
whereas	O
the	O
dp	O
diagram	O
shows	O
all	O
possible	O
transitions	O
the	O
monte	B
carlo	I
diagram	O
shows	O
only	O
those	O
sampled	O
on	O
the	O
one	O
episode	O
whereas	O
the	O
dp	O
diagram	O
includes	O
only	O
one-step	O
transitions	O
the	O
monte	B
carlo	I
diagram	O
goes	O
all	O
the	O
way	O
to	O
the	O
end	O
of	O
the	O
episode	O
these	O
differences	O
in	O
the	O
diagrams	O
accurately	O
reflect	O
the	O
fundamental	O
differences	O
between	O
the	O
algorithms	O
an	O
important	O
fact	O
about	O
monte	B
carlo	I
methods	I
is	O
that	O
the	O
estimates	O
for	O
each	O
state	B
are	O
independent	O
the	O
estimate	O
for	O
one	O
state	B
does	O
not	O
build	O
upon	O
the	O
estimate	O
of	O
any	O
other	O
state	B
as	O
is	O
the	O
case	O
in	O
dp	O
in	O
other	O
words	O
monte	B
carlo	I
methods	I
do	O
not	O
bootstrap	O
as	O
we	O
defined	O
it	O
in	O
the	O
previous	O
chapter	O
in	O
particular	O
note	O
that	O
the	O
computational	O
expense	O
of	O
estimating	O
the	O
value	B
of	O
a	O
single	O
state	B
is	O
independent	O
of	O
the	O
number	O
of	O
states	O
this	O
can	O
make	O
monte	B
carlo	I
methods	I
particularly	O
attractive	O
when	O
one	O
requires	O
the	O
value	B
of	O
only	O
one	O
or	O
a	O
subset	O
of	O
states	O
one	O
can	O
generate	O
many	O
sample	O
episodes	B
starting	O
from	O
the	O
states	O
of	O
interest	O
averaging	O
returns	O
from	O
only	O
these	O
states	O
ignoring	O
all	O
others	O
this	O
is	O
a	O
third	O
advantage	O
monte	B
carlo	I
methods	I
can	O
have	O
over	O
dp	O
methods	O
the	O
ability	O
to	O
learn	O
from	O
actual	O
experience	O
and	O
from	O
simulated	O
experience	O
example	O
soap	O
bubble	O
suppose	O
a	O
wire	O
frame	O
forming	O
a	O
closed	O
loop	O
is	O
dunked	O
in	O
soapy	O
water	O
to	O
form	O
a	O
soap	O
surface	O
or	O
bubble	O
conforming	O
at	O
its	O
edges	O
to	O
the	O
wire	O
frame	O
if	O
the	O
geometry	O
of	O
the	O
wire	O
frame	O
is	O
irregular	O
but	O
known	O
how	O
can	O
you	O
compute	O
the	O
shape	O
of	O
the	O
surface	O
the	O
shape	O
has	O
the	O
property	O
that	O
the	O
total	O
force	O
on	O
each	O
point	O
exerted	O
by	O
neighboring	O
points	O
is	O
zero	O
else	O
the	O
shape	O
would	O
change	O
this	O
means	O
that	O
the	O
surface	O
s	O
height	O
at	O
any	O
point	O
is	O
the	O
average	O
of	O
its	O
heights	O
at	O
points	O
in	O
a	O
small	O
circle	O
around	O
that	O
point	O
in	O
addition	O
the	O
surface	O
must	O
meet	O
at	O
its	O
boundaries	O
with	O
the	O
wire	O
frame	O
the	O
usual	O
approach	O
to	O
problems	O
of	O
this	O
kind	O
is	O
to	O
put	O
a	O
grid	O
over	O
the	O
area	O
covered	O
by	O
the	O
surface	O
and	O
solve	O
for	O
its	O
height	O
at	O
the	O
grid	O
points	O
by	O
an	O
iterative	B
computation	O
grid	O
points	O
at	O
the	O
boundary	O
are	O
forced	O
to	O
the	O
wire	O
frame	O
and	O
all	O
others	O
are	O
adjusted	O
toward	O
the	O
average	O
of	O
the	O
heights	O
of	O
their	O
four	O
nearest	O
neighbors	O
this	O
process	O
then	O
iterates	O
much	O
like	O
dp	O
s	O
iterative	B
policy	B
evaluation	O
and	O
ultimately	O
converges	O
to	O
a	O
close	O
approximation	O
to	O
the	O
desired	O
surface	O
from	O
hersh	O
and	O
griego	O
reproduced	O
with	O
permission	O
copyright	O
scientific	O
american	O
a	O
division	O
of	O
nature	O
america	O
inc	O
a	O
bubble	O
on	O
a	O
wire	O
loop	O
all	O
rights	O
reserved	O
this	O
is	O
similar	O
to	O
the	O
kind	O
of	O
problem	O
for	O
which	O
monte	B
carlo	I
methods	I
were	O
originally	O
designed	O
instead	O
of	O
the	O
iterative	B
computation	O
described	O
above	O
imagine	O
standing	O
on	O
the	O
surface	O
and	O
taking	O
a	O
random	B
walk	I
stepping	O
randomly	O
from	O
grid	O
point	O
to	O
neighboring	O
grid	O
point	O
with	O
equal	O
probability	O
until	O
you	O
reach	O
the	O
boundary	O
it	O
turns	O
out	O
that	O
the	O
expected	B
value	B
of	O
the	O
height	O
at	O
the	O
boundary	O
is	O
a	O
close	O
approximation	O
to	O
the	O
height	O
of	O
the	O
desired	O
surface	O
at	O
the	O
starting	O
point	O
fact	O
it	O
is	O
exactly	O
the	O
value	B
computed	O
by	O
the	O
chapter	O
monte	B
carlo	I
methods	I
iterative	B
method	O
described	O
above	O
thus	O
one	O
can	O
closely	O
approximate	O
the	O
height	O
of	O
the	O
surface	O
at	O
a	O
point	O
by	O
simply	O
averaging	O
the	O
boundary	O
heights	O
of	O
many	O
walks	O
started	O
at	O
the	O
point	O
if	O
one	O
is	O
interested	O
in	O
only	O
the	O
value	B
at	O
one	O
point	O
or	O
any	O
fixed	O
small	O
set	O
of	O
points	O
then	O
this	O
monte	B
carlo	I
method	O
can	O
be	O
far	O
more	O
efficient	O
than	O
the	O
iterative	B
method	O
based	O
on	O
local	O
consistency	O
monte	B
carlo	I
estimation	O
of	O
action	B
values	O
if	O
a	O
model	O
is	O
not	O
available	O
then	O
it	O
is	O
particularly	O
useful	O
to	O
estimate	O
action	B
values	O
values	O
of	O
state	B
action	B
pairs	O
rather	O
than	O
state	B
values	O
with	O
a	O
model	O
state	B
values	O
alone	O
are	O
sufficient	O
to	O
determine	O
a	O
policy	B
one	O
simply	O
looks	O
ahead	O
one	O
step	O
and	O
chooses	O
whichever	O
action	B
leads	O
to	O
the	O
best	O
combination	O
of	O
reward	O
and	O
next	O
state	B
as	O
we	O
did	O
in	O
the	O
chapter	O
on	O
dp	O
without	O
a	O
model	O
however	O
state	B
values	O
alone	O
are	O
not	O
sufficient	O
one	O
must	O
explicitly	O
estimate	O
the	O
value	B
of	O
each	O
action	B
in	O
order	O
for	O
the	O
values	O
to	O
be	O
useful	O
in	O
suggesting	O
a	O
policy	B
thus	O
one	O
of	O
our	O
primary	O
goals	O
for	B
monte	B
carlo	I
methods	I
is	O
to	O
estimate	O
q	O
to	O
achieve	O
this	O
we	O
first	O
consider	O
the	O
policy	B
evaluation	O
problem	O
for	B
action	B
values	I
the	O
policy	B
evaluation	O
problem	O
for	B
action	B
values	I
is	O
to	O
estimate	O
q	O
a	O
the	O
expected	B
return	B
when	O
starting	O
in	O
state	B
s	O
taking	O
action	B
a	O
and	O
thereafter	O
following	O
policy	B
the	O
monte	B
carlo	I
methods	I
for	O
this	O
are	O
essentially	O
the	O
same	O
as	O
just	O
presented	O
for	O
state	B
values	O
except	O
now	O
we	O
talk	O
about	O
visits	O
to	O
a	O
state	B
action	B
pair	O
rather	O
than	O
to	O
a	O
state	B
a	O
state	B
action	B
pair	O
s	O
a	O
is	O
said	O
to	O
be	O
visited	O
in	O
an	O
episode	O
if	O
ever	O
the	O
state	B
s	O
is	O
visited	O
and	O
action	B
a	O
is	O
taken	O
in	O
it	O
the	O
every-visit	O
mc	O
method	O
estimates	O
the	O
value	B
of	O
a	O
state	B
action	B
pair	O
as	O
the	O
average	O
of	O
the	O
returns	O
that	O
have	O
followed	O
all	O
the	O
visits	O
to	O
it	O
the	O
first-visit	O
mc	O
method	O
averages	O
the	O
returns	O
following	O
the	O
first	O
time	O
in	O
each	O
episode	O
that	O
the	O
state	B
was	O
visited	O
and	O
the	O
action	B
was	O
selected	O
these	O
methods	O
converge	O
quadratically	O
as	O
before	O
to	O
the	O
true	O
expected	B
values	O
as	O
the	O
number	O
of	O
visits	O
to	O
each	O
state	B
action	B
pair	O
approaches	O
infinity	O
the	O
only	O
complication	O
is	O
that	O
many	O
state	B
action	B
pairs	O
may	O
never	O
be	O
visited	O
if	O
is	O
a	O
deterministic	O
policy	B
then	O
in	O
following	O
one	O
will	O
observe	O
returns	O
only	O
for	O
one	O
of	O
the	O
actions	O
from	O
each	O
state	B
with	O
no	O
returns	O
to	O
average	O
the	O
monte	B
carlo	I
estimates	O
of	O
the	O
other	O
actions	O
will	O
not	O
improve	O
with	O
experience	O
this	O
is	O
a	O
serious	O
problem	O
because	O
the	O
purpose	O
of	O
learning	O
action	B
values	O
is	O
to	O
help	O
in	O
choosing	O
among	O
the	O
actions	O
available	O
in	O
each	O
state	B
to	O
compare	O
alternatives	O
we	O
need	O
to	O
estimate	O
the	O
value	B
of	O
all	O
the	O
actions	O
from	O
each	O
state	B
not	O
just	O
the	O
one	O
we	O
currently	O
favor	O
this	O
is	O
the	O
general	O
problem	O
of	O
maintaining	O
exploration	O
as	O
discussed	O
in	O
the	O
context	O
of	O
the	O
k-armed	O
bandit	O
problem	O
in	O
chapter	O
for	O
policy	B
evaluation	O
to	O
work	O
for	B
action	B
values	I
we	O
must	O
assure	O
continual	O
exploration	O
one	O
way	O
to	O
do	O
this	O
is	O
by	O
specifying	O
that	O
the	O
episodes	B
start	O
in	O
a	O
state	B
action	B
pair	O
and	O
that	O
every	O
pair	O
has	O
a	O
nonzero	O
probability	O
of	O
being	O
selected	O
as	O
the	O
start	O
this	O
guarantees	O
that	O
all	O
state	B
action	B
pairs	O
will	O
be	O
visited	O
an	O
infinite	O
number	O
of	O
times	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
of	O
episodes	B
we	O
call	O
this	O
the	O
assumption	O
of	O
exploring	B
starts	I
the	O
assumption	O
of	O
exploring	B
starts	I
is	O
sometimes	O
useful	O
but	O
of	O
course	O
it	O
cannot	O
be	O
relied	O
upon	O
in	O
general	O
particularly	O
when	O
learning	O
directly	O
from	O
actual	O
interaction	O
with	O
an	O
environment	B
in	O
that	O
case	O
the	O
starting	O
conditions	O
are	O
unlikely	O
to	O
be	O
so	O
helpful	O
the	O
monte	B
carlo	I
control	B
most	O
common	O
alternative	O
approach	O
to	O
assuring	O
that	O
all	O
state	B
action	B
pairs	O
are	O
encountered	O
is	O
to	O
consider	O
only	O
policies	O
that	O
are	O
stochastic	O
with	O
a	O
nonzero	O
probability	O
of	O
selecting	O
all	O
actions	O
in	O
each	O
state	B
we	O
discuss	O
two	O
important	O
variants	O
of	O
this	O
approach	O
in	O
later	O
sections	O
for	O
now	O
we	O
retain	O
the	O
assumption	O
of	O
exploring	B
starts	I
and	O
complete	O
the	O
presentation	O
of	O
a	O
full	O
monte	B
carlo	I
control	B
method	O
exercise	O
what	O
is	O
the	O
backup	B
diagram	I
for	O
monte	B
carlo	I
estimation	O
of	O
q	O
monte	B
carlo	I
control	B
we	O
are	O
now	O
ready	O
to	O
consider	O
how	O
monte	B
carlo	I
estimation	O
can	O
be	O
used	O
in	O
control	B
that	O
is	O
to	O
approximate	O
optimal	O
policies	O
the	O
overall	O
idea	O
is	O
to	O
proceed	O
according	O
to	O
the	O
same	O
pattern	O
as	O
in	O
the	O
dp	O
chapter	O
that	O
is	O
according	O
to	O
the	O
idea	O
of	O
generalized	O
policy	B
iteration	I
in	O
gpi	O
one	O
maintains	O
both	O
an	O
approximate	O
policy	B
and	O
an	O
approximate	O
value	B
function	I
the	O
value	B
function	I
is	O
repeatedly	O
altered	O
to	O
more	O
closely	O
approximate	O
the	O
value	B
function	I
for	O
the	O
current	O
policy	B
and	O
the	O
policy	B
is	O
repeatedly	O
improved	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
these	O
two	O
kinds	O
of	O
changes	O
work	O
against	O
each	O
other	O
to	O
some	O
extent	O
as	O
each	O
creates	O
a	O
moving	O
target	O
for	O
the	O
other	O
but	O
together	O
they	O
cause	O
both	O
policy	B
and	O
value	B
function	I
to	O
approach	O
optimality	O
to	O
begin	O
let	O
us	O
consider	O
a	O
monte	B
carlo	I
version	O
of	O
classical	O
policy	B
iteration	I
in	O
this	O
method	O
we	O
perform	O
alternating	O
complete	O
steps	O
of	O
policy	B
evaluation	O
and	O
policy	B
improvement	I
beginning	O
with	O
an	O
arbitrary	O
policy	B
and	O
ending	O
with	O
the	O
optimal	O
policy	B
and	O
optimal	O
action-value	O
function	O
e	O
q	O
e	O
i	O
i	O
e	O
q	O
i	O
e	O
denotes	O
a	O
complete	O
policy	B
evaluation	O
and	O
e	O
q	O
i	O
denotes	O
a	O
complete	O
policy	B
where	O
improvement	O
policy	B
evaluation	O
is	O
done	O
exactly	O
as	O
described	O
in	O
the	O
preceding	O
section	O
many	O
episodes	B
are	O
experienced	O
with	O
the	O
approximate	O
action-value	O
function	O
approaching	O
the	O
true	O
function	O
asymptotically	O
for	O
the	O
moment	O
let	O
us	O
assume	O
that	O
we	O
do	O
indeed	O
observe	O
an	O
infinite	O
number	O
of	O
episodes	B
and	O
that	O
in	O
addition	O
the	O
episodes	B
are	O
generated	O
with	O
exploring	B
starts	I
under	O
these	O
assumptions	O
the	O
monte	B
carlo	I
methods	I
will	O
compute	O
each	O
q	O
k	O
exactly	O
for	O
arbitrary	O
k	O
policy	B
improvement	I
is	O
done	O
by	O
making	O
the	O
policy	B
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
in	O
this	O
case	O
we	O
have	O
an	O
action-value	O
function	O
and	O
therefore	O
no	O
model	O
is	O
needed	O
to	O
construct	O
the	O
greedy	O
policy	B
for	O
any	O
action-value	O
function	O
q	O
the	O
corresponding	O
greedy	O
policy	B
is	O
the	O
one	O
that	O
for	O
each	O
s	O
s	O
deterministically	O
chooses	O
an	O
action	B
with	O
maximal	O
action-value	O
arg	O
max	O
a	O
qs	O
a	O
policy	B
improvement	I
then	O
can	O
be	O
done	O
by	O
constructing	O
each	O
as	O
the	O
greedy	O
policy	B
with	O
respect	O
to	O
q	O
k	O
the	O
policy	B
improvement	I
theorem	B
then	O
applies	O
to	O
k	O
evaluationimprovement	O
q	O
greedyqq	O
q	O
chapter	O
monte	B
carlo	I
methods	I
and	O
because	O
for	O
all	O
s	O
s	O
q	O
k	O
q	O
k	O
argmax	O
a	O
q	O
k	O
a	O
a	O
max	O
q	O
k	O
a	O
q	O
k	O
ks	O
v	O
k	O
as	O
we	O
discussed	O
in	O
the	O
previous	O
chapter	O
the	O
theorem	B
assures	O
us	O
that	O
each	O
is	O
uniformly	O
better	O
than	O
k	O
or	O
just	O
as	O
good	O
as	O
k	O
in	O
which	O
case	O
they	O
are	O
both	O
optimal	O
policies	O
this	O
in	O
turn	O
assures	O
us	O
that	O
the	O
overall	O
process	O
converges	O
to	O
the	O
optimal	O
policy	B
and	O
optimal	O
value	B
function	I
in	O
this	O
way	O
monte	B
carlo	I
methods	I
can	O
be	O
used	O
to	O
find	O
optimal	O
policies	O
given	O
only	O
sample	O
episodes	B
and	O
no	O
other	O
knowledge	O
of	O
the	O
environment	B
s	O
dynamics	O
we	O
made	O
two	O
unlikely	O
assumptions	O
above	O
in	O
order	O
to	O
easily	O
obtain	O
this	O
guarantee	O
of	O
convergence	O
for	O
the	O
monte	B
carlo	I
method	O
one	O
was	O
that	O
the	O
episodes	B
have	O
exploring	B
starts	I
and	O
the	O
other	O
was	O
that	O
policy	B
evaluation	O
could	O
be	O
done	O
with	O
an	O
infinite	O
number	O
of	O
episodes	B
to	O
obtain	O
a	O
practical	O
algorithm	O
we	O
will	O
have	O
to	O
remove	O
both	O
assumptions	O
we	O
postpone	O
consideration	O
of	O
the	O
first	O
assumption	O
until	O
later	O
in	O
this	O
chapter	O
for	O
now	O
we	O
focus	O
on	O
the	O
assumption	O
that	O
policy	B
evaluation	O
operates	O
on	O
an	O
infinite	O
number	O
of	O
episodes	B
this	O
assumption	O
is	O
relatively	O
easy	O
to	O
remove	O
in	O
fact	O
the	O
same	O
issue	O
arises	O
even	O
in	O
classical	O
dp	O
methods	O
such	O
as	O
iterative	B
policy	B
evaluation	O
which	O
also	O
converge	O
only	O
asymptotically	O
to	O
the	O
true	O
value	B
function	I
in	O
both	O
dp	O
and	O
monte	B
carlo	I
cases	O
there	O
are	O
two	O
ways	O
to	O
solve	O
the	O
problem	O
one	O
is	O
to	O
hold	O
firm	O
to	O
the	O
idea	O
of	O
approximating	O
q	O
k	O
in	O
each	O
policy	B
evaluation	O
measurements	O
and	O
assumptions	O
are	O
made	O
to	O
obtain	O
bounds	O
on	O
the	O
magnitude	O
and	O
probability	O
of	O
error	O
in	O
the	O
estimates	O
and	O
then	O
sufficient	O
steps	O
are	O
taken	O
during	O
each	O
policy	B
evaluation	O
to	O
assure	O
that	O
these	O
bounds	O
are	O
sufficiently	O
small	O
this	O
approach	O
can	O
probably	O
be	O
made	O
completely	O
satisfactory	O
in	O
the	O
sense	O
of	O
guaranteeing	O
correct	O
convergence	O
up	O
to	O
some	O
level	O
of	O
approximation	O
however	O
it	O
is	O
also	O
likely	O
to	O
require	O
far	O
too	O
many	O
episodes	B
to	O
be	O
useful	O
in	O
practice	O
on	O
any	O
but	O
the	O
smallest	O
problems	O
there	O
is	O
a	O
second	O
approach	O
to	O
avoiding	O
the	O
infinite	O
number	O
of	O
episodes	B
nominally	O
required	O
for	O
policy	B
evaluation	O
in	O
which	O
we	O
give	O
up	O
trying	O
to	O
complete	O
policy	B
evaluation	O
before	O
returning	O
to	O
policy	B
improvement	I
on	O
each	O
evaluation	O
step	O
we	O
move	O
the	O
value	B
function	I
toward	O
q	O
k	O
but	O
we	O
do	O
not	O
expect	O
to	O
actually	O
get	O
close	O
except	O
over	O
many	O
steps	O
we	O
used	O
this	O
idea	O
when	O
we	O
first	O
introduced	O
the	O
idea	O
of	O
gpi	O
in	O
section	O
one	O
extreme	O
form	O
of	O
the	O
idea	O
is	O
value	B
iteration	I
in	O
which	O
only	O
one	O
iteration	O
of	O
iterative	B
policy	B
evaluation	O
is	O
performed	O
between	O
each	O
step	O
of	O
policy	B
improvement	I
the	O
in-place	O
version	O
of	O
value	B
iteration	I
is	O
even	O
more	O
extreme	O
there	O
we	O
alternate	O
between	O
improvement	O
and	O
evaluation	O
steps	O
for	O
single	O
states	O
for	O
monte	B
carlo	I
policy	B
evaluation	O
it	O
is	O
natural	O
to	O
alternate	O
between	O
evaluation	O
and	O
improvement	O
on	O
an	O
episode-by-episode	O
basis	O
after	O
each	O
episode	O
the	O
observed	O
returns	O
are	O
used	O
for	O
policy	B
evaluation	O
and	O
then	O
the	O
policy	B
is	O
improved	O
at	O
all	O
the	O
states	O
visited	O
in	O
the	O
episode	O
a	O
complete	O
simple	O
algorithm	O
along	O
these	O
lines	O
which	O
we	O
call	O
monte	B
carlo	I
es	O
for	O
monte	B
carlo	I
with	O
exploring	B
starts	I
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
monte	B
carlo	I
control	B
without	O
exploring	B
starts	I
monte	B
carlo	I
es	O
starts	O
for	O
estimating	O
initialize	O
as	O
for	O
all	O
s	O
s	O
qs	O
a	O
r	O
for	O
all	O
s	O
s	O
a	O
as	O
returnss	O
a	O
empty	O
list	O
for	O
all	O
s	O
s	O
a	O
as	O
loop	O
forever	O
each	O
episode	O
choose	O
s	O
and	O
such	O
that	O
all	O
pairs	O
have	O
probability	O
generate	O
an	O
episode	O
from	O
following	O
st	O
at	O
rt	O
g	O
loop	O
for	O
each	O
step	O
of	O
episode	O
t	O
t	O
t	O
g	O
g	O
unless	O
the	O
pair	O
st	O
at	O
appears	O
in	O
st	O
at	O
append	O
g	O
to	O
returnsst	O
at	O
qst	O
at	O
averagereturnsst	O
at	O
argmaxa	O
qst	O
a	O
in	O
monte	B
carlo	I
es	O
all	O
the	O
returns	O
for	O
each	O
state	B
action	B
pair	O
are	O
accumulated	O
and	O
averaged	O
irrespective	O
of	O
what	O
policy	B
was	O
in	O
force	O
when	O
they	O
were	O
observed	O
it	O
is	O
easy	O
to	O
see	O
that	O
monte	B
carlo	I
es	O
cannot	O
converge	O
to	O
any	O
suboptimal	O
policy	B
if	O
it	O
did	O
then	O
the	O
value	B
function	I
would	O
eventually	O
converge	O
to	O
the	O
value	B
function	I
for	O
that	O
policy	B
and	O
that	O
in	O
turn	O
would	O
cause	O
the	O
policy	B
to	O
change	O
stability	O
is	O
achieved	O
only	O
when	O
both	O
the	O
policy	B
and	O
the	O
value	B
function	I
are	O
optimal	O
convergence	O
to	O
this	O
optimal	O
fixed	O
point	O
seems	O
inevitable	O
as	O
the	O
changes	O
to	O
the	O
action-value	O
function	O
decrease	O
over	O
time	O
but	O
has	O
not	O
yet	O
been	O
formally	O
proved	O
in	O
our	O
opinion	O
this	O
is	O
one	O
of	O
the	O
most	O
fundamental	O
open	O
theoretical	O
questions	O
in	O
reinforcement	B
learning	I
a	O
partial	O
solution	O
see	O
tsitsiklis	O
example	O
solving	O
blackjack	O
it	O
is	O
straightforward	O
to	O
apply	O
monte	B
carlo	I
es	O
to	O
blackjack	O
because	O
the	O
episodes	B
are	O
all	O
simulated	O
games	O
it	O
is	O
easy	O
to	O
arrange	O
for	O
exploring	B
starts	I
that	O
include	O
all	O
possibilities	O
in	O
this	O
case	O
one	O
simply	O
picks	O
the	O
dealer	O
s	O
cards	O
the	O
player	O
s	O
sum	O
and	O
whether	O
or	O
not	O
the	O
player	O
has	O
a	O
usable	O
ace	O
all	O
at	O
random	O
with	O
equal	O
probability	O
as	O
the	O
initial	O
policy	B
we	O
use	O
the	O
policy	B
evaluated	O
in	O
the	O
previous	O
blackjack	B
example	I
that	O
which	O
sticks	O
only	O
on	O
or	O
the	O
initial	O
action-value	O
function	O
can	O
be	O
zero	O
for	O
all	O
state	B
action	B
pairs	O
figure	O
shows	O
the	O
optimal	O
policy	B
for	O
blackjack	O
found	O
by	O
monte	B
carlo	I
es	O
this	O
policy	B
is	O
the	O
same	O
as	O
the	O
basic	O
strategy	O
of	O
thorp	O
with	O
the	O
sole	O
exception	O
of	O
the	O
leftmost	O
notch	O
in	O
the	O
policy	B
for	O
a	O
usable	O
ace	O
which	O
is	O
not	O
present	O
in	O
thorp	O
s	O
strategy	O
we	O
are	O
uncertain	O
of	O
the	O
reason	O
for	O
this	O
discrepancy	O
but	O
confident	O
that	O
what	O
is	O
shown	O
here	O
is	O
indeed	O
the	O
optimal	O
policy	B
for	O
the	O
version	O
of	O
blackjack	O
we	O
have	O
described	O
chapter	O
monte	B
carlo	I
methods	I
figure	O
the	O
optimal	O
policy	B
and	O
state-value	O
function	O
for	O
blackjack	O
found	O
by	O
monte	B
carlo	I
es	O
the	O
state-value	O
function	O
shown	O
was	O
computed	O
from	O
the	O
action-value	O
function	O
found	O
by	O
monte	B
carlo	I
es	O
monte	B
carlo	I
control	B
without	O
exploring	B
starts	I
how	O
can	O
we	O
avoid	O
the	O
unlikely	O
assumption	O
of	O
exploring	B
starts	I
the	O
only	O
general	O
way	O
to	O
ensure	O
that	O
all	O
actions	O
are	O
selected	O
infinitely	O
often	O
is	O
for	O
the	O
agent	O
to	O
continue	O
to	O
select	O
them	O
there	O
are	O
two	O
approaches	O
to	O
ensuring	O
this	O
resulting	O
in	O
what	O
we	O
call	O
on-policy	B
methods	I
and	O
off-policy	B
methods	I
on-policy	B
methods	I
attempt	O
to	O
evaluate	O
or	O
improve	O
the	O
policy	B
that	O
is	O
used	O
to	O
make	O
decisions	O
whereas	O
off-policy	B
methods	I
evaluate	O
or	O
improve	O
a	O
policy	B
different	O
from	O
that	O
used	O
to	O
generate	O
the	O
data	O
the	O
monte	B
carlo	I
es	O
method	O
developed	O
above	O
is	O
an	O
example	O
of	O
an	O
on-policy	O
method	O
in	O
this	O
section	O
we	O
show	O
how	O
an	O
on-policy	O
monte	B
carlo	I
control	B
method	O
can	O
be	O
designed	O
that	O
does	O
not	O
use	O
the	O
unrealistic	O
assumption	O
of	O
exploring	B
starts	I
off-policy	B
methods	I
are	O
considered	O
in	O
the	O
next	O
section	O
in	O
on-policy	O
control	B
methods	O
the	O
policy	B
is	O
generally	O
soft	O
meaning	O
that	O
for	O
all	O
s	O
s	O
and	O
all	O
a	O
as	O
but	O
gradually	O
shifted	O
closer	O
and	O
closer	O
to	O
a	O
deterministic	O
optimal	O
policy	B
many	O
of	O
the	O
methods	O
discussed	O
in	O
chapter	O
provide	O
mechanisms	O
for	O
this	O
the	O
on-policy	O
method	O
we	O
present	O
in	O
this	O
section	O
uses	O
policies	O
meaning	O
that	O
most	O
of	O
the	O
time	O
they	O
choose	O
an	O
action	B
that	O
has	O
maximal	O
estimated	O
action	B
value	B
but	O
with	O
probability	O
they	O
instead	O
select	O
an	O
action	B
at	O
random	O
that	O
is	O
all	O
nongreedy	O
and	O
the	O
remaining	O
bulk	O
of	O
actions	O
are	O
given	O
the	O
minimal	O
probability	O
of	O
selection	O
the	O
probability	O
is	O
given	O
to	O
the	O
greedy	O
action	B
the	O
policies	O
are	O
examples	O
of	O
policies	O
defined	O
as	O
policies	O
for	O
which	O
for	O
all	O
states	O
and	O
actions	O
for	O
some	O
among	O
policies	O
policies	O
are	O
in	O
some	O
sense	O
those	O
that	O
are	O
closest	O
to	O
greedy	O
showingplayer	O
showingplayer	O
showingplayer	O
showingplayer	O
showingplayer	O
showingplayer	O
showingplayer	O
sum	O
monte	B
carlo	I
control	B
without	O
exploring	B
starts	I
the	O
overall	O
idea	O
of	O
on-policy	O
monte	B
carlo	I
control	B
is	O
still	O
that	O
of	O
gpi	O
as	O
in	O
monte	B
carlo	I
es	O
we	O
use	O
first-visit	O
mc	O
methods	O
to	O
estimate	O
the	O
action-value	O
function	O
for	O
the	O
current	O
policy	B
without	O
the	O
assumption	O
of	O
exploring	B
starts	I
however	O
we	O
cannot	O
simply	O
improve	O
the	O
policy	B
by	O
making	O
it	O
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
because	O
that	O
would	O
prevent	O
further	O
exploration	O
of	O
nongreedy	O
actions	O
fortunately	O
gpi	O
does	O
not	O
require	O
that	O
the	O
policy	B
be	O
taken	O
all	O
the	O
way	O
to	O
a	O
greedy	O
policy	B
only	O
that	O
it	O
be	O
moved	O
toward	O
a	O
greedy	O
policy	B
in	O
our	O
on-policy	O
method	O
we	O
will	O
move	O
it	O
only	O
to	O
an	O
policy	B
for	O
any	O
policy	B
any	O
policy	B
with	O
respect	O
to	O
q	O
is	O
guaranteed	O
to	O
be	O
better	O
than	O
or	O
equal	O
to	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
below	O
on-policy	O
first-visit	O
mc	O
control	B
policies	O
estimates	O
algorithm	O
parameter	O
small	O
initialize	O
an	O
arbitrary	O
policy	B
qs	O
a	O
r	O
for	O
all	O
s	O
s	O
a	O
as	O
returnss	O
a	O
empty	O
list	O
for	O
all	O
s	O
s	O
a	O
as	O
repeat	O
forever	O
each	O
episode	O
generate	O
an	O
episode	O
following	O
st	O
at	O
rt	O
g	O
loop	O
for	O
each	O
step	O
of	O
episode	O
t	O
t	O
t	O
g	O
g	O
unless	O
the	O
pair	O
st	O
at	O
appears	O
in	O
st	O
at	O
append	O
g	O
to	O
returnsst	O
at	O
qst	O
at	O
averagereturnsst	O
at	O
a	O
arg	O
maxa	O
qst	O
a	O
for	O
all	O
a	O
ast	O
ties	O
broken	O
arbitrarily	O
if	O
a	O
a	O
if	O
a	O
a	O
that	O
any	O
policy	B
with	O
respect	O
to	O
q	O
is	O
an	O
improvement	O
over	O
any	O
policy	B
is	O
assured	O
by	O
the	O
policy	B
improvement	I
theorem	B
let	O
be	O
the	O
policy	B
the	O
conditions	O
of	O
the	O
policy	B
improvement	I
theorem	B
apply	O
because	O
for	O
any	O
s	O
s	O
q	O
a	O
q	O
a	O
max	O
a	O
q	O
a	O
q	O
a	O
q	O
a	O
sum	O
is	O
a	O
weighted	O
average	O
with	O
nonnegative	O
weights	O
summing	O
to	O
and	O
as	O
such	O
it	O
chapter	O
monte	B
carlo	I
methods	I
must	O
be	O
less	O
than	O
or	O
equal	O
to	O
the	O
largest	O
number	O
averaged	O
q	O
a	O
q	O
a	O
a	O
v	O
thus	O
by	O
the	O
policy	B
improvement	I
theorem	B
v	O
v	O
for	O
all	O
s	O
s	O
we	O
now	O
prove	O
that	O
equality	O
can	O
hold	O
only	O
when	O
both	O
and	O
are	O
optimal	O
among	O
the	O
policies	O
that	O
is	O
when	O
they	O
are	O
better	O
than	O
or	O
equal	O
to	O
all	O
other	O
policies	O
consider	O
a	O
new	O
environment	B
that	O
is	O
just	O
like	O
the	O
original	O
environment	B
except	O
with	O
the	O
requirement	O
that	O
policies	O
be	O
moved	O
inside	O
the	O
environment	B
the	O
new	O
environment	B
has	O
the	O
same	O
action	B
and	O
state	B
set	O
as	O
the	O
original	O
and	O
behaves	O
as	O
follows	O
if	O
in	O
state	B
s	O
and	O
taking	O
action	B
a	O
then	O
with	O
probability	O
the	O
new	O
environment	B
behaves	O
exactly	O
like	O
the	O
old	O
environment	B
with	O
probability	O
it	O
repicks	O
the	O
action	B
at	O
random	O
with	O
equal	O
probabilities	O
and	O
then	O
behaves	O
like	O
the	O
old	O
environment	B
with	O
the	O
new	O
random	O
action	B
the	O
best	O
one	O
can	O
do	O
in	O
this	O
new	O
environment	B
with	O
general	O
policies	O
is	O
the	O
same	O
as	O
the	O
the	O
optimal	O
value	B
functions	O
for	O
the	O
new	O
environment	B
then	O
a	O
policy	B
is	O
optimal	O
among	O
best	O
one	O
could	O
do	O
in	O
the	O
original	O
environment	B
with	O
policies	O
and	O
denote	O
policies	O
if	O
and	O
only	O
if	O
v	O
from	O
the	O
definition	O
of	O
we	O
know	O
that	O
it	O
is	O
the	O
unique	O
solution	O
to	O
v	O
max	O
a	O
q	O
a	O
max	O
a	O
however	O
this	O
equation	O
is	O
the	O
same	O
as	O
the	O
previous	O
one	O
except	O
for	O
the	O
substitution	O
of	O
v	O
rs	O
v	O
is	O
the	O
unique	O
solution	O
it	O
must	O
be	O
that	O
v	O
in	O
essence	O
we	O
have	O
shown	O
in	O
the	O
last	O
few	O
pages	O
that	O
policy	B
iteration	I
works	O
for	O
policies	O
using	O
the	O
natural	O
notion	O
of	O
greedy	O
policy	B
for	O
policies	O
one	O
is	O
assured	O
of	O
improvement	O
on	O
every	O
step	O
except	O
when	O
the	O
best	O
policy	B
has	O
been	O
found	O
among	O
the	O
policies	O
this	O
analysis	O
is	O
independent	O
of	O
how	O
the	O
action-value	O
functions	O
are	O
determined	O
at	O
each	O
stage	O
but	O
it	O
does	O
assume	O
that	O
they	O
are	O
computed	O
exactly	O
this	O
brings	O
us	O
to	O
max	O
a	O
a	O
a	O
max	O
a	O
rs	O
rs	O
rs	O
v	O
q	O
a	O
when	O
equality	O
holds	O
and	O
the	O
policy	B
is	O
no	O
longer	O
improved	O
then	O
we	O
also	O
know	O
from	O
that	O
off-policy	B
prediction	B
via	O
importance	B
sampling	I
roughly	O
the	O
same	O
point	O
as	O
in	O
the	O
previous	O
section	O
now	O
we	O
only	O
achieve	O
the	O
best	O
policy	B
among	O
the	O
policies	O
but	O
on	O
the	O
other	O
hand	O
we	O
have	O
eliminated	O
the	O
assumption	O
of	O
exploring	B
starts	I
off-policy	B
prediction	B
via	O
importance	B
sampling	I
all	O
learning	O
control	B
methods	O
face	O
a	O
dilemma	O
they	O
seek	O
to	O
learn	O
action	B
values	O
conditional	O
on	O
subsequent	O
optimal	O
behavior	O
but	O
they	O
need	O
to	O
behave	O
non-optimally	O
in	O
order	O
to	O
explore	O
all	O
actions	O
find	O
the	O
optimal	O
actions	O
how	O
can	O
they	O
learn	O
about	O
the	O
optimal	O
policy	B
while	O
behaving	O
according	O
to	O
an	O
exploratory	O
policy	B
the	O
on-policy	O
approach	O
in	O
the	O
preceding	O
section	O
is	O
actually	O
a	O
compromise	O
it	O
learns	O
action	B
values	O
not	O
for	O
the	O
optimal	O
policy	B
but	O
for	O
a	O
near-optimal	O
policy	B
that	O
still	O
explores	O
a	O
more	O
straightforward	O
approach	O
is	O
to	O
use	O
two	O
policies	O
one	O
that	O
is	O
learned	O
about	O
and	O
that	O
becomes	O
the	O
optimal	O
policy	B
and	O
one	O
that	O
is	O
more	O
exploratory	O
and	O
is	O
used	O
to	O
generate	O
behavior	O
the	O
policy	B
being	O
learned	O
about	O
is	O
called	O
the	O
target	O
policy	B
and	O
the	O
policy	B
used	O
to	O
generate	O
behavior	O
is	O
called	O
the	O
behavior	O
policy	B
in	O
this	O
case	O
we	O
say	O
that	O
learning	O
is	O
from	O
data	O
off	O
the	O
target	O
policy	B
and	O
the	O
overall	O
process	O
is	O
termed	O
off-policy	B
learning	O
throughout	O
the	O
rest	O
of	O
this	O
book	O
we	O
consider	O
both	O
on-policy	O
and	O
off-policy	B
methods	I
on-policy	B
methods	I
are	O
generally	O
simpler	O
and	O
are	O
considered	O
first	O
off-policy	B
methods	I
require	O
additional	O
concepts	O
and	O
notation	O
and	O
because	O
the	O
data	O
is	O
due	O
to	O
a	O
different	O
policy	B
off-policy	B
methods	I
are	O
often	O
of	O
greater	O
variance	O
and	O
are	O
slower	O
to	O
converge	O
on	O
the	O
other	O
hand	O
off-policy	B
methods	I
are	O
more	O
powerful	O
and	O
general	O
they	O
include	O
onpolicy	O
methods	O
as	O
the	O
special	O
case	O
in	O
which	O
the	O
target	O
and	O
behavior	O
policies	O
are	O
the	O
same	O
off-policy	B
methods	I
also	O
have	O
a	O
variety	O
of	O
additional	O
uses	O
in	O
applications	O
for	O
example	O
they	O
can	O
often	O
be	O
applied	O
to	O
learn	O
from	O
data	O
generated	O
by	O
a	O
conventional	O
nonlearning	O
controller	O
or	O
from	O
a	O
human	O
expert	O
off-policy	B
learning	O
is	O
also	O
seen	O
by	O
some	O
as	O
key	O
to	O
learning	O
multi-step	O
predictive	O
models	O
of	O
the	O
world	O
s	O
dynamics	O
section	O
sutton	O
sutton	O
et	O
al	O
in	O
this	O
section	O
we	O
begin	O
the	O
study	O
of	O
off-policy	B
methods	I
by	O
considering	O
the	O
prediction	B
problem	O
in	O
which	O
both	O
target	O
and	O
behavior	O
policies	O
are	O
fixed	O
that	O
is	O
suppose	O
we	O
wish	O
to	O
estimate	O
v	O
or	O
q	O
but	O
all	O
we	O
have	O
are	O
episodes	B
following	O
another	O
policy	B
b	O
where	O
b	O
in	O
this	O
case	O
is	O
the	O
target	O
policy	B
b	O
is	O
the	O
behavior	O
policy	B
and	O
both	O
policies	O
are	O
considered	O
fixed	O
and	O
given	O
in	O
order	O
to	O
use	O
episodes	B
from	O
b	O
to	O
estimate	O
values	O
for	O
we	O
require	O
that	O
every	O
action	B
taken	O
under	O
is	O
also	O
taken	O
at	O
least	O
occasionally	O
under	O
b	O
that	O
is	O
we	O
require	O
that	O
implies	O
bas	O
this	O
is	O
called	O
the	O
assumption	O
of	O
coverage	O
it	O
follows	O
from	O
coverage	O
that	O
b	O
must	O
be	O
stochastic	O
in	O
states	O
where	O
it	O
is	O
not	O
identical	O
to	O
the	O
target	O
policy	B
on	O
the	O
other	O
hand	O
may	O
be	O
deterministic	O
and	O
in	O
fact	O
this	O
is	O
a	O
case	O
of	O
particular	O
interest	O
in	O
control	B
applications	O
in	O
control	B
the	O
target	O
policy	B
is	O
typically	O
the	O
deterministic	O
greedy	O
policy	B
with	O
respect	O
to	O
the	O
current	O
estimate	O
of	O
the	O
action-value	O
function	O
this	O
policy	B
becomes	O
a	O
deterministic	O
optimal	O
policy	B
while	O
the	O
behavior	O
policy	B
remains	O
stochastic	O
and	O
more	O
exploratory	O
for	O
example	O
an	O
policy	B
in	O
this	O
section	O
however	O
we	O
consider	O
the	O
prediction	B
problem	O
in	O
which	O
is	O
unchanging	O
and	O
given	O
almost	O
all	O
off-policy	B
methods	I
utilize	O
importance	B
sampling	I
a	O
general	O
technique	O
for	O
chapter	O
monte	B
carlo	I
methods	I
estimating	O
expected	B
values	O
under	O
one	O
distribution	O
given	O
samples	O
from	O
another	O
we	O
apply	O
importance	B
sampling	I
to	O
off-policy	B
learning	O
by	O
weighting	O
returns	O
according	O
to	O
the	O
relative	O
probability	O
of	O
their	O
trajectories	O
occurring	O
under	O
the	O
target	O
and	O
behavior	O
policies	O
called	O
the	O
importance-sampling	O
ratio	B
given	O
a	O
starting	O
state	B
st	O
the	O
probability	O
of	O
the	O
subsequent	O
state	B
action	B
trajectory	O
at	O
st	O
occurring	O
under	O
any	O
policy	B
is	O
prat	O
st	O
st	O
att	O
at	O
pst	O
at	O
ak	O
t	O
where	O
p	O
here	O
is	O
the	O
state-transition	O
probability	O
function	O
defined	O
by	O
thus	O
the	O
relative	O
probability	O
of	O
the	O
trajectory	O
under	O
the	O
target	O
and	O
behavior	O
policies	O
importancesampling	O
ratio	B
is	O
tt	O
kt	O
ak	O
kt	O
ak	O
t	O
baksk	O
although	O
the	O
trajectory	O
probabilities	O
depend	O
on	O
the	O
mdp	O
s	O
transition	B
probabilities	I
which	O
are	O
generally	O
unknown	O
they	O
appear	O
identically	O
in	O
both	O
the	O
numerator	O
and	O
denominator	O
and	O
thus	O
cancel	O
the	O
importance	B
sampling	I
ratio	B
ends	O
up	O
depending	O
only	O
on	O
the	O
two	O
policies	O
and	O
the	O
sequence	O
not	O
on	O
the	O
mdp	O
recall	O
that	O
we	O
wish	O
to	O
estimate	O
the	O
expected	B
returns	O
under	O
the	O
target	O
policy	B
but	O
all	O
we	O
have	O
are	O
returns	O
gt	O
due	O
to	O
the	O
behavior	O
policy	B
these	O
returns	O
have	O
the	O
wrong	O
expectation	O
egtst	O
s	O
vbs	O
and	O
so	O
cannot	O
be	O
averaged	O
to	O
obtain	O
v	O
this	O
is	O
where	O
importance	B
sampling	I
comes	O
in	O
the	O
ratio	B
tt	O
transforms	O
the	O
returns	O
to	O
have	O
the	O
right	O
expected	B
value	B
e	O
tt	O
st	O
s	O
v	O
now	O
we	O
are	O
ready	O
to	O
give	O
a	O
monte	B
carlo	I
algorithm	O
that	O
averages	O
returns	O
from	O
a	O
batch	O
of	O
observed	O
episodes	B
following	O
policy	B
b	O
to	O
estimate	O
v	O
it	O
is	O
convenient	O
here	O
to	O
number	O
time	O
steps	O
in	O
a	O
way	O
that	O
increases	O
across	O
episode	O
boundaries	O
that	O
is	O
if	O
the	O
first	O
episode	O
of	O
the	O
batch	O
ends	O
in	O
a	O
terminal	O
state	B
at	O
time	O
then	O
the	O
next	O
episode	O
begins	O
at	O
time	O
t	O
this	O
enables	O
us	O
to	O
use	O
time-step	O
numbers	O
to	O
refer	O
to	O
particular	O
steps	O
in	O
particular	O
episodes	B
in	O
particular	O
we	O
can	O
define	O
the	O
set	O
of	O
all	O
time	O
steps	O
in	O
which	O
state	B
s	O
is	O
visited	O
denoted	O
ts	O
this	O
is	O
for	O
an	O
every-visit	O
method	O
for	O
a	O
first-visit	O
method	O
ts	O
would	O
only	O
include	O
time	O
steps	O
that	O
were	O
first	O
visits	O
to	O
s	O
within	O
their	O
episodes	B
also	O
let	O
t	O
denote	O
the	O
first	O
time	O
of	O
termination	O
following	O
time	O
t	O
and	O
gt	O
denote	O
the	O
return	B
after	O
t	O
up	O
through	O
t	O
then	O
ts	O
are	O
the	O
returns	O
that	O
pertain	O
to	O
state	B
s	O
and	O
tt	O
ts	O
are	O
the	O
corresponding	O
importance-sampling	O
ratios	O
to	O
estimate	O
v	O
we	O
simply	O
scale	O
the	O
returns	O
by	O
the	O
ratios	O
and	O
average	O
the	O
results	O
v	O
ts	O
tt	O
off-policy	B
prediction	B
via	O
importance	B
sampling	I
when	O
importance	B
sampling	I
is	O
done	O
as	O
a	O
simple	O
average	O
in	O
this	O
way	O
it	O
is	O
called	O
ordinary	O
importance	B
sampling	I
an	O
important	O
alternative	O
is	O
weighted	O
importance	B
sampling	I
which	O
uses	O
a	O
weighted	O
av	O
erage	O
defined	O
as	O
v	O
ts	O
tt	O
ts	O
tt	O
or	O
zero	O
if	O
the	O
denominator	O
is	O
zero	O
to	O
understand	O
these	O
two	O
varieties	O
of	O
importance	B
sampling	I
consider	O
their	O
estimates	O
after	O
observing	O
a	O
single	O
return	B
in	O
the	O
weightedaverage	O
estimate	O
the	O
ratio	B
tt	O
for	O
the	O
single	O
return	B
cancels	O
in	O
the	O
numerator	O
and	O
denominator	O
so	O
that	O
the	O
estimate	O
is	O
equal	O
to	O
the	O
observed	O
return	B
independent	O
of	O
the	O
ratio	B
the	O
ratio	B
is	O
nonzero	O
given	O
that	O
this	O
return	B
was	O
the	O
only	O
one	O
observed	O
this	O
is	O
a	O
reasonable	O
estimate	O
but	O
its	O
expectation	O
is	O
vbs	O
rather	O
than	O
v	O
and	O
in	O
this	O
statistical	O
sense	O
it	O
is	O
biased	O
in	O
contrast	O
the	O
simple	O
average	O
is	O
always	O
v	O
in	O
expectation	O
is	O
unbiased	O
but	O
it	O
can	O
be	O
extreme	O
suppose	O
the	O
ratio	B
were	O
ten	O
indicating	O
that	O
the	O
trajectory	O
observed	O
is	O
ten	O
times	O
as	O
likely	O
under	O
the	O
target	O
policy	B
as	O
under	O
the	O
behavior	O
policy	B
in	O
this	O
case	O
the	O
ordinary	O
importance-sampling	O
estimate	O
would	O
be	O
ten	O
times	O
the	O
observed	O
return	B
that	O
is	O
it	O
would	O
be	O
quite	O
far	O
from	O
the	O
observed	O
return	B
even	O
though	O
the	O
episode	O
s	O
trajectory	O
is	O
considered	O
very	O
representative	O
of	O
the	O
target	O
policy	B
formally	O
the	O
difference	O
between	O
the	O
two	O
kinds	O
of	O
importance	B
sampling	I
is	O
expressed	O
in	O
their	O
biases	O
and	O
variances	O
the	O
ordinary	O
importance-sampling	O
estimator	O
is	O
unbiased	O
whereas	O
the	O
weighted	O
importance-sampling	O
estimator	O
is	O
biased	O
bias	O
converges	O
asymptotically	O
to	O
zero	O
on	O
the	O
other	O
hand	O
the	O
variance	O
of	O
the	O
ordinary	O
importancesampling	O
estimator	O
is	O
in	O
general	O
unbounded	O
because	O
the	O
variance	O
of	O
the	O
ratios	O
can	O
be	O
unbounded	O
whereas	O
in	O
the	O
weighted	O
estimator	O
the	O
largest	O
weight	O
on	O
any	O
single	O
return	B
is	O
one	O
in	O
fact	O
assuming	O
bounded	O
returns	O
the	O
variance	O
of	O
the	O
weighted	O
importancesampling	O
estimator	O
converges	O
to	O
zero	O
even	O
if	O
the	O
variance	O
of	O
the	O
ratios	O
themselves	O
is	O
infinite	O
sutton	O
and	O
dasgupta	O
in	O
practice	O
the	O
weighted	O
estimator	O
usually	O
has	O
dramatically	O
lower	O
variance	O
and	O
is	O
strongly	O
preferred	O
nevertheless	O
we	O
will	O
not	O
totally	O
abandon	O
ordinary	O
importance	B
sampling	I
as	O
it	O
is	O
easier	O
to	O
extend	O
to	O
the	O
approximate	O
methods	O
using	O
function	B
approximation	I
that	O
we	O
explore	O
in	O
the	O
second	O
part	O
of	O
this	O
book	O
a	O
complete	O
every-visit	O
mc	O
algorithm	O
for	O
off-policy	B
policy	B
evaluation	O
using	O
weighted	O
importance	B
sampling	I
is	O
given	O
in	O
the	O
next	O
section	O
on	O
page	O
example	O
off-policy	B
estimation	O
of	O
a	O
blackjack	O
state	B
value	B
we	O
applied	O
both	O
ordinary	O
and	O
weighted	O
importance-sampling	O
methods	O
to	O
estimate	O
the	O
value	B
of	O
a	O
single	O
blackjack	O
state	B
from	O
off-policy	B
data	O
recall	O
that	O
one	O
of	O
the	O
advantages	B
of	I
monte	B
carlo	I
methods	I
is	O
that	O
they	O
can	O
be	O
used	O
to	O
evaluate	O
a	O
single	O
state	B
without	O
forming	O
estimates	O
for	O
any	O
other	O
states	O
in	O
this	O
example	O
we	O
evaluated	O
the	O
state	B
in	O
which	O
the	O
dealer	O
is	O
showing	O
a	O
deuce	O
the	O
sum	O
of	O
the	O
player	O
s	O
cards	O
is	O
and	O
the	O
player	O
has	O
a	O
usable	O
ace	O
is	O
the	O
player	O
holds	O
an	O
ace	O
and	O
a	O
deuce	O
or	O
equivalently	O
three	O
aces	O
the	O
data	O
was	O
generated	O
by	O
starting	O
in	O
this	O
state	B
then	O
choosing	O
to	O
hit	O
or	O
stick	O
at	O
random	O
with	O
equal	O
probability	O
behavior	O
policy	B
the	O
target	O
policy	B
was	O
to	O
stick	O
only	O
on	O
a	O
sum	O
of	O
or	O
as	O
in	O
example	O
the	O
value	B
of	O
this	O
state	B
under	O
the	O
target	O
policy	B
chapter	O
monte	B
carlo	I
methods	I
is	O
approximately	O
was	O
determined	O
by	O
separately	O
generating	O
one-hundred	O
million	O
episodes	B
using	O
the	O
target	O
policy	B
and	O
averaging	O
their	O
returns	O
both	O
off-policy	B
methods	I
closely	O
approximated	O
this	O
value	B
after	O
off-policy	B
episodes	B
using	O
the	O
random	O
policy	B
to	O
make	O
sure	O
they	O
did	O
this	O
reliably	O
we	O
performed	O
independent	O
runs	O
each	O
starting	O
from	O
estimates	O
of	O
zero	O
and	O
learning	O
for	O
episodes	B
figure	O
shows	O
the	O
resultant	O
learning	O
curves	O
the	O
squared	O
error	O
of	O
the	O
estimates	O
of	O
each	O
method	O
as	O
a	O
function	O
of	O
number	O
of	O
episodes	B
averaged	O
over	O
the	O
runs	O
the	O
error	O
approaches	O
zero	O
for	O
both	O
algorithms	O
but	O
the	O
weighted	O
importance-sampling	O
method	O
has	O
much	O
lower	O
error	O
at	O
the	O
beginning	O
as	O
is	O
typical	O
in	O
practice	O
figure	O
weighted	O
importance	B
sampling	I
produces	O
lower	O
error	O
estimates	O
of	O
the	O
value	B
of	O
a	O
single	O
blackjack	O
state	B
from	O
off-policy	B
episodes	B
example	O
example	O
infinite	O
variance	O
the	O
estimates	O
of	O
ordinary	O
importance	B
sampling	I
will	O
typically	O
have	O
infinite	O
variance	O
and	O
thus	O
unsatisfactory	O
convergence	O
properties	O
whenever	O
the	O
scaled	O
returns	O
have	O
infinite	O
variance	O
and	O
this	O
can	O
easily	O
happen	O
in	O
off-policy	B
learning	O
when	O
trajectories	O
contain	O
loops	O
a	O
simple	O
example	O
is	O
shown	O
inset	O
in	O
figure	O
there	O
is	O
only	O
one	O
nonterminal	O
state	B
s	O
and	O
two	O
actions	O
right	O
and	O
left	O
the	O
right	O
action	B
causes	O
a	O
deterministic	O
transition	O
to	O
termination	O
whereas	O
the	O
left	O
action	B
transitions	O
with	O
probability	O
back	O
to	O
s	O
or	O
with	O
probability	O
on	O
to	O
termination	O
the	O
rewards	O
are	O
on	O
the	O
latter	O
transition	O
and	O
otherwise	O
zero	O
consider	O
the	O
target	O
policy	B
that	O
always	O
selects	O
left	O
all	O
episodes	B
under	O
this	O
policy	B
consist	O
of	O
some	O
number	O
zero	O
of	O
transitions	O
back	O
to	O
s	O
followed	O
by	O
termination	O
with	O
a	O
reward	O
and	O
return	B
of	O
thus	O
the	O
value	B
of	O
s	O
under	O
the	O
target	O
policy	B
is	O
suppose	O
we	O
are	O
estimating	O
this	O
value	B
from	O
off-policy	B
data	O
using	O
the	O
behavior	O
policy	B
that	O
selects	O
right	O
and	O
left	O
with	O
equal	O
probability	O
the	O
lower	O
part	O
of	O
figure	O
shows	O
ten	O
independent	O
runs	O
of	O
the	O
first-visit	O
mc	O
algorithm	O
using	O
ordinary	O
importance	B
sampling	I
even	O
after	O
millions	O
of	O
episodes	B
the	O
estimates	O
fail	O
to	O
converge	O
to	O
the	O
correct	O
value	B
of	O
in	O
contrast	O
the	O
weighted	O
importance-sampling	O
algorithm	O
would	O
give	O
an	O
estimate	O
of	O
exactly	O
forever	O
after	O
the	O
first	O
episode	O
that	O
ended	O
with	O
the	O
left	O
action	B
all	O
returns	O
not	O
equal	O
to	O
is	O
ending	O
with	O
the	O
right	O
action	B
would	O
be	O
inconsistent	O
with	O
the	O
target	O
policy	B
and	O
thus	O
would	O
have	O
a	O
tt	O
of	O
zero	O
and	O
contribute	O
neither	O
to	O
the	O
numerator	O
nor	O
denominator	O
of	O
the	O
weighted	O
importance	O
ordinary	O
importance	O
samplingweighted	O
importance	O
samplingepisodes	O
off-policy	B
prediction	B
via	O
importance	B
sampling	I
figure	O
ordinary	O
importance	B
sampling	I
produces	O
surprisingly	O
unstable	O
estimates	O
on	O
the	O
one-state	O
mdp	O
shown	O
inset	O
the	O
correct	O
estimate	O
here	O
is	O
and	O
even	O
though	O
this	O
is	O
the	O
expected	B
value	B
of	O
a	O
sample	O
return	B
importance	B
sampling	I
the	O
variance	O
of	O
the	O
samples	O
is	O
infinite	O
and	O
the	O
estimates	O
do	O
not	O
converge	O
to	O
this	O
value	B
these	O
results	O
are	O
for	O
off-policy	B
first-visit	O
mc	O
sampling	O
algorithm	O
produces	O
a	O
weighted	O
average	O
of	O
only	O
the	O
returns	O
consistent	O
with	O
the	O
target	O
policy	B
and	O
all	O
of	O
these	O
would	O
be	O
exactly	O
we	O
can	O
verify	O
that	O
the	O
variance	O
of	O
the	O
importance-sampling-scaled	O
returns	O
is	O
infinite	O
in	O
this	O
example	O
by	O
a	O
simple	O
calculation	O
the	O
variance	O
of	O
any	O
random	O
variable	O
x	O
is	O
the	O
expected	B
value	B
of	O
the	O
deviation	O
from	O
its	O
mean	O
x	O
which	O
can	O
be	O
written	O
varx	O
x	O
x	O
x	O
thus	O
if	O
the	O
mean	O
is	O
finite	O
as	O
it	O
is	O
in	O
our	O
case	O
the	O
variance	O
is	O
infinite	O
if	O
and	O
only	O
if	O
the	O
expectation	O
of	O
the	O
square	O
of	O
the	O
random	O
variable	O
is	O
infinite	O
thus	O
we	O
need	O
only	O
show	O
that	O
the	O
expected	B
square	O
of	O
the	O
importance-sampling-scaled	O
return	B
is	O
infinite	O
eb	O
batst	O
to	O
compute	O
this	O
expectation	O
we	O
break	O
it	O
down	O
into	O
cases	O
based	O
on	O
episode	O
length	O
and	O
termination	O
first	O
note	O
that	O
for	O
any	O
episode	O
ending	O
with	O
the	O
right	O
action	B
the	O
importance	B
sampling	I
ratio	B
is	O
zero	O
because	O
the	O
target	O
policy	B
would	O
never	O
take	O
this	O
action	B
these	O
episodes	B
thus	O
contribute	O
nothing	O
to	O
the	O
expectation	O
quantity	O
in	O
parenthesis	O
will	O
be	O
zero	O
and	O
can	O
be	O
ignored	O
we	O
need	O
only	O
consider	O
episodes	B
that	O
involve	O
some	O
number	O
zero	O
of	O
left	O
actions	O
that	O
transition	O
back	O
to	O
the	O
nonterminal	O
state	B
followed	O
by	O
estimate	O
of	O
with	O
ordinaryimportance	O
samplingten	O
runsepisodes	O
chapter	O
monte	B
carlo	I
methods	I
a	O
left	O
action	B
transitioning	O
to	O
termination	O
all	O
of	O
these	O
episodes	B
have	O
a	O
return	B
of	O
so	O
the	O
factor	O
can	O
be	O
ignored	O
to	O
get	O
the	O
expected	B
square	O
we	O
need	O
only	O
consider	O
each	O
length	O
of	O
episode	O
multiplying	O
the	O
probability	O
of	O
the	O
episode	O
s	O
occurrence	O
by	O
the	O
square	O
of	O
its	O
importance-sampling	O
ratio	B
and	O
add	O
these	O
up	O
length	O
episode	O
length	O
episode	O
length	O
episode	O
exercise	O
what	O
is	O
the	O
equation	O
analogous	O
to	O
for	B
action	B
values	I
qs	O
a	O
instead	O
of	O
state	B
values	O
v	O
again	O
given	O
returns	O
generated	O
using	O
b	O
exercise	O
in	O
learning	O
curves	O
such	O
as	O
those	O
shown	O
in	O
figure	O
error	O
generally	O
decreases	O
with	O
training	O
as	O
indeed	O
happened	O
for	O
the	O
ordinary	O
importance-sampling	O
method	O
but	O
for	O
the	O
weighted	O
importance-sampling	O
method	O
error	O
first	O
increased	O
and	O
then	O
decreased	O
why	O
do	O
you	O
think	O
this	O
happened	O
exercise	O
the	O
results	O
with	O
example	O
and	O
shown	O
in	O
figure	O
used	O
a	O
first-visit	O
mc	O
method	O
suppose	O
that	O
instead	O
an	O
every-visit	O
mc	O
method	O
was	O
used	O
on	O
the	O
same	O
problem	O
would	O
the	O
variance	O
of	O
the	O
estimator	O
still	O
be	O
infinite	O
why	O
or	O
why	O
not	O
incremental	B
implementation	I
monte	B
carlo	I
prediction	B
methods	O
can	O
be	O
implemented	O
incrementally	O
on	O
an	O
episode-byepisode	O
basis	O
using	O
extensions	O
of	O
the	O
techniques	O
described	O
in	O
chapter	O
whereas	O
in	O
chapter	O
we	O
averaged	O
rewards	O
in	O
monte	B
carlo	I
methods	I
we	O
average	O
returns	O
in	O
all	O
other	O
respects	O
exactly	O
the	O
same	O
methods	O
as	O
used	O
in	O
chapter	O
can	O
be	O
used	O
for	O
on-policy	O
monte	B
carlo	I
methods	I
for	O
off-policy	B
monte	B
carlo	I
methods	I
we	O
need	O
to	O
separately	O
consider	O
those	O
that	O
use	O
ordinary	O
importance	B
sampling	I
and	O
those	O
that	O
use	O
weighted	O
importance	B
sampling	I
in	O
ordinary	O
importance	B
sampling	I
the	O
returns	O
are	O
scaled	O
by	O
the	O
importance	B
sampling	I
ratio	B
tt	O
then	O
simply	O
averaged	O
for	O
these	O
methods	O
we	O
can	O
again	O
use	O
the	O
incremental	O
methods	O
of	O
chapter	O
but	O
using	O
the	O
scaled	O
returns	O
in	O
place	O
of	O
the	O
rewards	O
of	O
that	O
chapter	O
this	O
leaves	O
the	O
case	O
of	O
off-policy	B
methods	I
using	O
weighted	O
importance	B
sampling	I
here	O
we	O
have	O
to	O
form	O
a	O
weighted	O
average	O
of	O
the	O
returns	O
and	O
a	O
slightly	O
different	O
incremental	O
algorithm	O
is	O
required	O
suppose	O
we	O
have	O
a	O
sequence	O
of	O
returns	O
gn	O
all	O
starting	O
in	O
the	O
same	O
state	B
and	O
each	O
with	O
a	O
corresponding	O
random	O
weight	O
wi	O
wi	O
tt	O
we	O
wish	O
to	O
off-policy	B
monte	B
carlo	I
control	B
form	O
the	O
estimate	O
wkgk	O
wk	O
vn	O
n	O
and	O
keep	O
it	O
up-to-date	O
as	O
we	O
obtain	O
a	O
single	O
additional	O
return	B
gn	O
in	O
addition	O
to	O
keeping	O
track	O
of	O
vn	O
we	O
must	O
maintain	O
for	O
each	O
state	B
the	O
cumulative	O
sum	O
cn	O
of	O
the	O
weights	O
given	O
to	O
the	O
first	O
n	O
returns	O
the	O
update	O
rule	O
for	O
vn	O
is	O
vn	O
wn	O
n	O
and	O
cn	O
where	O
is	O
arbitrary	O
and	O
thus	O
need	O
not	O
be	O
specified	O
the	O
box	O
below	O
contains	O
a	O
complete	O
episode-by-episode	O
incremental	O
algorithm	O
for	O
monte	B
carlo	I
policy	B
evaluation	O
the	O
algorithm	O
is	O
nominally	O
for	O
the	O
off-policy	B
case	O
using	O
weighted	O
importance	B
sampling	I
but	O
applies	O
as	O
well	O
to	O
the	O
on-policy	O
case	O
just	O
by	O
choosing	O
the	O
target	O
and	O
behavior	O
policies	O
as	O
the	O
same	O
which	O
case	O
b	O
w	O
is	O
always	O
the	O
approximation	O
q	O
converges	O
to	O
q	O
all	O
encountered	O
state	B
action	B
pairs	O
while	O
actions	O
are	O
selected	O
according	O
to	O
a	O
potentially	O
different	O
policy	B
b	O
off-policy	B
mc	O
prediction	B
evaluation	O
for	O
estimating	O
q	O
q	O
input	O
an	O
arbitrary	O
target	O
policy	B
initialize	O
for	O
all	O
s	O
s	O
a	O
as	O
qs	O
a	O
r	O
cs	O
a	O
loop	O
forever	O
each	O
episode	O
b	O
any	O
policy	B
with	O
coverage	O
of	O
generate	O
an	O
episode	O
following	O
b	O
st	O
at	O
rt	O
g	O
w	O
loop	O
for	O
each	O
step	O
of	O
episode	O
t	O
t	O
t	O
g	O
g	O
cst	O
at	O
cst	O
at	O
w	O
qst	O
at	O
qst	O
at	O
w	O
w	O
w	O
batst	O
if	O
w	O
then	O
exit	O
for	O
loop	O
cstat	O
qst	O
at	O
exercise	O
modify	O
the	O
algorithm	O
for	O
first-visit	O
mc	O
policy	B
evaluation	O
to	O
use	O
the	O
incremental	B
implementation	I
for	O
sample	O
averages	O
described	O
in	O
section	O
exercise	O
derive	O
the	O
weighted-average	O
update	O
rule	O
from	O
follow	O
the	O
pattern	O
of	O
the	O
derivation	O
of	O
the	O
unweighted	O
rule	O
chapter	O
monte	B
carlo	I
methods	I
off-policy	B
monte	B
carlo	I
control	B
we	O
are	O
now	O
ready	O
to	O
present	O
an	O
example	O
of	O
the	O
second	O
class	O
of	O
learning	O
control	B
methods	O
we	O
consider	O
in	O
this	O
book	O
off-policy	B
methods	I
recall	O
that	O
the	O
distinguishing	O
feature	O
of	O
on-policy	B
methods	I
is	O
that	O
they	O
estimate	O
the	O
value	B
of	O
a	O
policy	B
while	O
using	O
it	O
for	O
control	B
in	O
off-policy	B
methods	I
these	O
two	O
functions	O
are	O
separated	O
the	O
policy	B
used	O
to	O
generate	O
behavior	O
called	O
the	O
behavior	O
policy	B
may	O
in	O
fact	O
be	O
unrelated	O
to	O
the	O
policy	B
that	O
is	O
evaluated	O
and	O
improved	O
called	O
the	O
target	O
policy	B
an	O
advantage	O
of	O
this	O
separation	O
is	O
that	O
the	O
target	O
policy	B
may	O
be	O
deterministic	O
greedy	O
while	O
the	O
behavior	O
policy	B
can	O
continue	O
to	O
sample	O
all	O
possible	O
actions	O
off-policy	B
monte	B
carlo	I
control	B
methods	O
use	O
one	O
of	O
the	O
techniques	O
presented	O
in	O
the	O
preceding	O
two	O
sections	O
they	O
follow	O
the	O
behavior	O
policy	B
while	O
learning	O
about	O
and	O
improving	O
the	O
target	O
policy	B
these	O
techniques	O
require	O
that	O
the	O
behavior	O
policy	B
has	O
a	O
nonzero	O
probability	O
of	O
selecting	O
all	O
actions	O
that	O
might	O
be	O
selected	O
by	O
the	O
target	O
policy	B
to	O
explore	O
all	O
possibilities	O
we	O
require	O
that	O
the	O
behavior	O
policy	B
be	O
soft	O
that	O
it	O
select	O
all	O
actions	O
in	O
all	O
states	O
with	O
nonzero	O
probability	O
the	O
box	O
below	O
shows	O
an	O
off-policy	B
monte	B
carlo	I
control	B
method	O
based	O
on	O
gpi	O
and	O
weighted	O
importance	B
sampling	I
for	O
estimating	O
and	O
q	O
the	O
target	O
policy	B
is	O
the	O
greedy	O
policy	B
with	O
respect	O
to	O
q	O
which	O
is	O
an	O
estimate	O
of	O
q	O
the	O
behavior	O
policy	B
b	O
can	O
be	O
anything	O
but	O
in	O
order	O
to	O
assure	O
convergence	O
of	O
to	O
the	O
optimal	O
policy	B
an	O
infinite	O
number	O
of	O
returns	O
must	O
be	O
obtained	O
for	O
each	O
pair	O
of	O
state	B
and	O
action	B
this	O
can	O
be	O
assured	O
by	O
choosing	O
b	O
to	O
be	O
the	O
policy	B
converges	O
to	O
optimal	O
at	O
all	O
encountered	O
states	O
even	O
though	O
actions	O
are	O
selected	O
according	O
to	O
a	O
different	O
soft	O
policy	B
b	O
which	O
may	O
change	O
between	O
or	O
even	O
within	O
episodes	B
off-policy	B
mc	O
control	B
for	O
estimating	O
initialize	O
for	O
all	O
s	O
s	O
a	O
as	O
qs	O
a	O
r	O
cs	O
a	O
argmaxa	O
qs	O
a	O
loop	O
forever	O
each	O
episode	O
ties	O
broken	O
consistently	O
b	O
any	O
soft	O
policy	B
generate	O
an	O
episode	O
using	O
b	O
st	O
at	O
rt	O
g	O
w	O
loop	O
for	O
each	O
step	O
of	O
episode	O
t	O
t	O
t	O
g	O
g	O
cst	O
at	O
cst	O
at	O
w	O
qst	O
at	O
qst	O
at	O
w	O
argmaxa	O
qst	O
a	O
if	O
at	O
then	O
exit	O
for	O
loop	O
w	O
w	O
batst	O
cstat	O
qst	O
at	O
ties	O
broken	O
consistently	O
off-policy	B
monte	B
carlo	I
control	B
a	O
potential	O
problem	O
is	O
that	O
this	O
method	O
learns	O
only	O
from	O
the	O
tails	O
of	O
episodes	B
when	O
all	O
of	O
the	O
remaining	O
actions	O
in	O
the	O
episode	O
are	O
greedy	O
if	O
nongreedy	O
actions	O
are	O
common	O
then	O
learning	O
will	O
be	O
slow	O
particularly	O
for	O
states	O
appearing	O
in	O
the	O
early	O
portions	O
of	O
long	O
episodes	B
potentially	O
this	O
could	O
greatly	O
slow	O
learning	O
there	O
has	O
been	O
insufficient	O
experience	O
with	O
off-policy	B
monte	B
carlo	I
methods	I
to	O
assess	O
how	O
serious	O
this	O
problem	O
is	O
if	O
it	O
is	O
serious	O
the	O
most	O
important	O
way	O
to	O
address	O
it	O
is	O
probably	O
by	O
incorporating	O
temporaldifference	O
learning	O
the	O
algorithmic	O
idea	O
developed	O
in	O
the	O
next	O
chapter	O
alternatively	O
if	O
is	O
less	O
than	O
then	O
the	O
idea	O
developed	O
in	O
the	O
next	O
section	O
may	O
also	O
help	O
significantly	O
batst	O
why	O
is	O
this	O
nevertheless	O
correct	O
exercise	O
in	O
the	O
boxed	O
algorithm	O
for	O
off-policy	B
mc	O
control	B
you	O
may	O
have	O
been	O
expecting	O
the	O
w	O
update	O
to	O
have	O
involved	O
the	O
importance-sampling	O
ratio	B
batst	O
but	O
instead	O
it	O
involves	O
exercise	O
racetrack	O
consider	O
driving	O
a	O
race	O
car	O
around	O
a	O
turn	O
like	O
those	O
shown	O
in	O
figure	O
you	O
want	O
to	O
go	O
as	O
fast	O
as	O
possible	O
but	O
not	O
so	O
fast	O
as	O
to	O
run	O
off	O
the	O
track	O
in	O
our	O
simplified	O
racetrack	O
the	O
car	O
is	O
at	O
one	O
of	O
a	O
discrete	O
set	O
of	O
grid	O
positions	O
the	O
cells	O
in	O
the	O
diagram	O
the	O
velocity	O
is	O
also	O
discrete	O
a	O
number	O
of	O
grid	O
cells	O
moved	O
horizontally	O
and	O
vertically	O
per	O
time	O
step	O
the	O
actions	O
are	O
increments	O
to	O
the	O
velocity	O
components	O
each	O
may	O
be	O
changed	O
by	O
or	O
in	O
each	O
step	O
for	O
a	O
total	O
of	O
nine	O
actions	O
both	O
velocity	O
components	O
are	O
restricted	O
to	O
be	O
nonnegative	O
and	O
less	O
than	O
and	O
they	O
cannot	O
both	O
be	O
zero	O
except	O
at	O
the	O
starting	O
line	O
each	O
episode	O
begins	O
in	O
one	O
of	O
the	O
randomly	O
selected	O
start	O
states	O
with	O
both	O
velocity	O
components	O
zero	O
and	O
ends	O
when	O
the	O
car	O
crosses	O
the	O
finish	O
line	O
the	O
rewards	O
are	O
for	O
each	O
step	O
until	O
the	O
car	O
crosses	O
the	O
finish	O
line	O
if	O
the	O
car	O
hits	O
the	O
track	O
boundary	O
it	O
is	O
moved	O
back	O
to	O
a	O
random	O
position	O
on	O
the	O
starting	O
line	O
both	O
velocity	O
components	O
are	O
reduced	O
to	O
zero	O
and	O
the	O
episode	O
continues	O
before	O
updating	O
the	O
car	O
s	O
location	O
at	O
each	O
time	O
step	O
check	O
to	O
see	O
if	O
the	O
projected	O
path	O
of	O
the	O
car	O
intersects	O
the	O
track	O
boundary	O
if	O
it	O
intersects	O
the	O
finish	O
line	O
the	O
episode	O
ends	O
if	O
it	O
intersects	O
anywhere	O
else	O
the	O
car	O
is	O
considered	O
to	O
have	O
figure	O
a	O
couple	O
of	O
right	O
turns	O
for	O
the	O
racetrack	O
task	O
starting	O
linefinishlinestarting	O
linefinishline	O
chapter	O
monte	B
carlo	I
methods	I
hit	O
the	O
track	O
boundary	O
and	O
is	O
sent	O
back	O
to	O
the	O
starting	O
line	O
to	O
make	O
the	O
task	O
more	O
challenging	O
with	O
probability	O
at	O
each	O
time	O
step	O
the	O
velocity	O
increments	O
are	O
both	O
zero	O
independently	O
of	O
the	O
intended	O
increments	O
apply	O
a	O
monte	B
carlo	I
control	B
method	O
to	O
this	O
task	O
to	O
compute	O
the	O
optimal	O
policy	B
from	O
each	O
starting	O
state	B
exhibit	O
several	O
trajectories	O
following	O
the	O
optimal	O
policy	B
turn	O
the	O
noise	O
off	O
for	O
these	O
trajectories	O
importance	B
sampling	I
the	O
off-policy	B
methods	I
that	O
we	O
have	O
considered	O
so	O
far	O
are	O
based	O
on	O
forming	O
importancesampling	O
weights	O
for	O
returns	O
considered	O
as	O
unitary	O
wholes	O
without	O
taking	O
into	O
account	O
the	O
returns	O
internal	O
structures	O
as	O
sums	O
of	O
discounted	O
rewards	O
we	O
now	O
briefly	O
consider	O
cutting-edge	O
research	O
ideas	O
for	O
using	O
this	O
structure	O
to	O
significantly	O
reduce	O
the	O
variance	O
of	O
off-policy	B
estimators	O
for	O
example	O
consider	O
the	O
case	O
where	O
episodes	B
are	O
long	O
and	O
is	O
significantly	O
less	O
than	O
for	O
concreteness	O
say	O
that	O
episodes	B
last	O
steps	O
and	O
that	O
the	O
return	B
from	O
time	O
will	O
then	O
be	O
just	O
but	O
its	O
importance	B
sampling	I
ratio	B
will	O
be	O
a	O
product	O
of	O
factors	O
in	O
ordinary	O
importance	B
sampling	I
the	O
return	B
will	O
be	O
scaled	O
by	O
the	O
entire	O
product	O
but	O
it	O
is	O
really	O
only	O
necessary	O
to	O
scale	O
by	O
the	O
first	O
factor	O
by	O
are	O
irrelevant	O
because	O
after	O
the	O
first	O
reward	O
the	O
return	B
has	O
already	O
been	O
determined	O
these	O
later	O
factors	O
are	O
all	O
independent	O
of	O
the	O
return	B
and	O
of	O
expected	B
value	B
they	O
do	O
not	O
change	O
the	O
expected	B
update	I
but	O
they	O
add	O
enormously	O
to	O
its	O
variance	O
in	O
some	O
cases	O
they	O
could	O
even	O
make	O
the	O
variance	O
infinite	O
let	O
us	O
now	O
consider	O
an	O
idea	O
for	O
avoiding	O
this	O
large	O
extraneous	O
variance	O
the	O
other	O
factors	O
the	O
essence	O
of	O
the	O
idea	O
is	O
to	O
think	O
of	O
discounting	B
as	O
determining	O
a	O
probability	O
of	O
termination	O
or	O
equivalently	O
a	O
degree	O
of	O
partial	O
termination	O
for	O
any	O
we	O
can	O
think	O
of	O
the	O
return	B
as	O
partly	O
terminating	O
in	O
one	O
step	O
to	O
the	O
degree	O
producing	O
a	O
return	B
of	O
just	O
the	O
first	O
reward	O
and	O
as	O
partly	O
terminating	O
after	O
two	O
steps	O
to	O
the	O
degree	O
producing	O
a	O
return	B
of	O
and	O
so	O
on	O
the	O
latter	O
degree	O
corresponds	O
to	O
terminating	O
on	O
the	O
second	O
step	O
and	O
not	O
having	O
already	O
terminated	O
on	O
the	O
first	O
step	O
the	O
degree	O
of	O
termination	O
on	O
the	O
third	O
step	O
is	O
thus	O
with	O
the	O
reflecting	O
that	O
termination	O
did	O
not	O
occur	O
on	O
either	O
of	O
the	O
first	O
two	O
steps	O
the	O
partial	O
returns	O
here	O
are	O
called	O
flat	O
partial	O
returns	O
gth	O
rh	O
t	O
h	O
t	O
where	O
flat	O
denotes	O
the	O
absence	O
of	O
discounting	B
and	O
partial	O
denotes	O
that	O
these	O
returns	O
do	O
not	O
extend	O
all	O
the	O
way	O
to	O
termination	O
but	O
instead	O
stop	O
at	O
h	O
called	O
the	O
horizon	O
t	O
is	O
the	O
time	O
of	O
termination	O
of	O
the	O
episode	O
the	O
conventional	O
full	O
return	B
gt	O
can	O
be	O
per-decision	B
importance	B
sampling	I
viewed	O
as	O
a	O
sum	O
of	O
flat	O
partial	O
returns	O
as	O
suggested	O
above	O
as	O
follows	O
gt	O
t	O
t	O
t	O
t	O
rt	O
t	O
t	O
rt	O
h	O
t	O
gth	O
t	O
t	O
gtt	O
t	O
now	O
we	O
need	O
to	O
scale	O
the	O
flat	O
partial	O
returns	O
by	O
an	O
importance	B
sampling	I
ratio	B
that	O
is	O
similarly	O
truncated	B
as	O
gth	O
only	O
involves	O
rewards	O
up	O
to	O
a	O
horizon	O
h	O
we	O
only	O
need	O
the	O
ratio	B
of	O
the	O
probabilities	O
up	O
to	O
h	O
we	O
define	O
an	O
ordinary	O
importance-sampling	O
estimator	O
analogous	O
to	O
as	O
v	O
h	O
t	O
th	O
gth	O
t	O
t	O
tt	O
gtt	O
and	O
a	O
weighted	O
importance-sampling	O
estimator	O
analogous	O
to	O
as	O
v	O
h	O
t	O
th	O
gth	O
t	O
t	O
tt	O
h	O
t	O
th	O
t	O
t	O
tt	O
gtt	O
we	O
call	O
these	O
two	O
estimators	O
discounting-aware	O
importance	B
sampling	I
estimators	O
they	O
take	O
into	O
account	O
the	O
discount	O
rate	O
but	O
have	O
no	O
effect	O
the	O
same	O
as	O
the	O
off-policy	B
estimators	O
from	O
section	O
if	O
importance	B
sampling	I
there	O
is	O
one	O
more	O
way	O
in	O
which	O
the	O
structure	O
of	O
the	O
return	B
as	O
a	O
sum	O
of	O
rewards	O
can	O
be	O
taken	O
into	O
account	O
in	O
off-policy	B
importance	B
sampling	I
a	O
way	O
that	O
may	O
be	O
able	O
to	O
reduce	O
variance	O
even	O
in	O
the	O
absence	O
of	O
discounting	B
is	O
even	O
if	O
in	O
the	O
off-policy	B
estimators	O
and	O
each	O
term	O
of	O
the	O
sum	O
in	O
the	O
numerator	O
is	O
itself	O
a	O
sum	O
tt	O
tt	O
t	O
t	O
tt	O
tt	O
t	O
t	O
tt	O
chapter	O
monte	B
carlo	I
methods	I
the	O
off-policy	B
estimators	O
rely	O
on	O
the	O
expected	B
values	O
of	O
these	O
terms	O
let	O
us	O
see	O
if	O
we	O
can	O
write	O
them	O
in	O
a	O
simpler	O
way	O
note	O
that	O
each	O
sub-term	O
of	O
is	O
a	O
product	O
of	O
a	O
random	O
reward	O
and	O
a	O
random	O
importance-sampling	O
ratio	B
for	O
example	O
the	O
first	O
sub-term	O
can	O
be	O
written	O
using	O
as	O
tt	O
batst	O
bat	O
now	O
notice	O
that	O
of	O
all	O
these	O
factors	O
only	O
the	O
first	O
and	O
the	O
last	O
reward	O
are	O
correlated	O
all	O
the	O
other	O
ratios	O
are	O
independent	O
random	O
variables	O
whose	O
expected	B
value	B
is	O
one	O
bask	O
bask	O
thus	O
because	O
the	O
expectation	O
of	O
the	O
product	O
of	O
independent	O
random	O
variables	O
is	O
the	O
product	O
of	O
their	O
expectations	O
all	O
the	O
ratios	O
except	O
the	O
first	O
drop	O
out	O
in	O
expectation	O
leaving	O
just	O
e	O
tt	O
e	O
if	O
we	O
repeat	O
this	O
analysis	O
for	O
the	O
kth	O
term	O
of	O
we	O
get	O
e	O
tt	O
e	O
ttk	O
it	O
follows	O
then	O
that	O
the	O
expectation	O
of	O
our	O
original	O
term	O
can	O
be	O
written	O
where	O
e	O
tt	O
gt	O
t	O
t	O
tt	O
we	O
call	O
this	O
idea	O
per-decision	B
importance	B
sampling	I
it	O
follows	O
immediately	O
that	O
there	O
is	O
an	O
alternate	O
importance-sampling	O
estimator	O
with	O
the	O
same	O
unbiased	O
expectation	O
as	O
the	O
ordinary-importance-sampling	O
estimator	O
using	O
gt	O
v	O
ts	O
gt	O
which	O
we	O
might	O
expect	O
to	O
sometimes	O
be	O
of	O
lower	O
variance	O
is	O
there	O
a	O
per-decision	B
version	O
of	O
weighted	O
importance	B
sampling	I
this	O
is	O
less	O
clear	O
so	O
far	O
all	O
the	O
estimators	O
that	O
have	O
been	O
proposed	O
for	O
this	O
that	O
we	O
know	O
of	O
are	O
not	O
consistent	O
is	O
they	O
do	O
not	O
converge	O
to	O
the	O
true	O
value	B
with	O
infinite	O
data	O
exercise	O
modify	O
the	O
algorithm	O
for	O
off-policy	B
monte	B
carlo	I
control	B
to	O
use	O
the	O
idea	O
of	O
the	O
truncated	B
weighted-average	O
estimator	O
note	O
that	O
you	O
will	O
first	O
need	O
to	O
convert	O
this	O
equation	O
to	O
action	B
values	O
summary	O
summary	O
the	O
monte	B
carlo	I
methods	I
presented	O
in	O
this	O
chapter	O
learn	O
value	B
functions	O
and	O
optimal	O
policies	O
from	O
experience	O
in	O
the	O
form	O
of	O
sample	O
episodes	B
this	O
gives	O
them	O
at	O
least	O
three	O
kinds	O
of	O
advantages	O
over	O
dp	O
methods	O
first	O
they	O
can	O
be	O
used	O
to	O
learn	O
optimal	O
behavior	O
directly	O
from	O
interaction	O
with	O
the	O
environment	B
with	O
no	O
model	B
of	I
the	I
environment	B
s	O
dynamics	O
second	O
they	O
can	O
be	O
used	O
with	O
simulation	O
or	O
sample	O
models	O
for	O
surprisingly	O
many	O
applications	O
it	O
is	O
easy	O
to	O
simulate	O
sample	O
episodes	B
even	O
though	O
it	O
is	O
difficult	O
to	O
construct	O
the	O
kind	O
of	O
explicit	O
model	O
of	O
transition	B
probabilities	I
required	O
by	O
dp	O
methods	O
third	O
it	O
is	O
easy	O
and	O
efficient	O
to	O
focus	O
monte	B
carlo	I
methods	I
on	O
a	O
small	O
subset	O
of	O
the	O
states	O
a	O
region	O
of	O
special	O
interest	O
can	O
be	O
accurately	O
evaluated	O
without	O
going	O
to	O
the	O
expense	O
of	O
accurately	O
evaluating	O
the	O
rest	O
of	O
the	O
state	B
set	O
explore	O
this	O
further	O
in	O
chapter	O
a	O
fourth	O
advantage	O
of	O
monte	B
carlo	I
methods	I
which	O
we	O
discuss	O
later	O
in	O
the	O
book	O
is	O
that	O
they	O
may	O
be	O
less	O
harmed	O
by	O
violations	O
of	O
the	O
markov	B
property	I
this	O
is	O
because	O
they	O
do	O
not	O
update	O
their	O
value	B
estimates	O
on	O
the	O
basis	O
of	O
the	O
value	B
estimates	O
of	O
successor	O
states	O
in	O
other	O
words	O
it	O
is	O
because	O
they	O
do	O
not	O
bootstrap	O
in	O
designing	O
monte	B
carlo	I
control	B
methods	O
we	O
have	O
followed	O
the	O
overall	O
schema	O
of	O
generalized	O
policy	B
iteration	I
introduced	O
in	O
chapter	O
gpi	O
involves	O
interacting	O
processes	O
of	O
policy	B
evaluation	O
and	O
policy	B
improvement	I
monte	B
carlo	I
methods	I
provide	O
an	O
alternative	O
policy	B
evaluation	O
process	O
rather	O
than	O
use	O
a	O
model	O
to	O
compute	O
the	O
value	B
of	O
each	O
state	B
they	O
simply	O
average	O
many	O
returns	O
that	O
start	O
in	O
the	O
state	B
because	O
a	O
state	B
s	O
value	B
is	O
the	O
expected	B
return	B
this	O
average	O
can	O
become	O
a	O
good	O
approximation	O
to	O
the	O
value	B
in	O
control	B
methods	O
we	O
are	O
particularly	O
interested	O
in	O
approximating	O
action-value	O
functions	O
because	O
these	O
can	O
be	O
used	O
to	O
improve	O
the	O
policy	B
without	O
requiring	O
a	O
model	B
of	I
the	I
environment	B
s	O
transition	O
dynamics	O
monte	B
carlo	I
methods	I
intermix	O
policy	B
evaluation	O
and	O
policy	B
improvement	I
steps	O
on	O
an	O
episode-by-episode	O
basis	O
and	O
can	O
be	O
incrementally	O
implemented	O
on	O
an	O
episode-by-episode	O
basis	O
maintaining	O
sufficient	O
exploration	O
is	O
an	O
issue	O
in	O
monte	B
carlo	I
control	B
methods	O
it	O
is	O
not	O
enough	O
just	O
to	O
select	O
the	O
actions	O
currently	O
estimated	O
to	O
be	O
best	O
because	O
then	O
no	O
returns	O
will	O
be	O
obtained	O
for	O
alternative	O
actions	O
and	O
it	O
may	O
never	O
be	O
learned	O
that	O
they	O
are	O
actually	O
better	O
one	O
approach	O
is	O
to	O
ignore	O
this	O
problem	O
by	O
assuming	O
that	O
episodes	B
begin	O
with	O
state	B
action	B
pairs	O
randomly	O
selected	O
to	O
cover	O
all	O
possibilities	O
such	O
exploring	B
starts	I
can	O
sometimes	O
be	O
arranged	O
in	O
applications	O
with	O
simulated	O
episodes	B
but	O
are	O
unlikely	O
in	O
learning	O
from	O
real	O
experience	O
in	O
on-policy	B
methods	I
the	O
agent	O
commits	O
to	O
always	O
exploring	O
and	O
tries	O
to	O
find	O
the	O
best	O
policy	B
that	O
still	O
explores	O
in	O
off-policy	B
methods	I
the	O
agent	O
also	O
explores	O
but	O
learns	O
a	O
deterministic	O
optimal	O
policy	B
that	O
may	O
be	O
unrelated	O
to	O
the	O
policy	B
followed	O
off-policy	B
prediction	B
refers	O
to	O
learning	O
the	O
value	B
function	I
of	O
a	O
target	O
policy	B
from	O
data	O
generated	O
by	O
a	O
different	O
behavior	O
policy	B
such	O
learning	O
methods	O
are	O
based	O
on	O
some	O
form	O
of	O
importance	B
sampling	I
that	O
is	O
on	O
weighting	O
returns	O
by	O
the	O
ratio	B
of	O
the	O
probabilities	O
of	O
taking	O
the	O
observed	O
actions	O
under	O
the	O
two	O
policies	O
thereby	O
transforming	O
their	O
expectations	O
from	O
the	O
behavior	O
policy	B
to	O
the	O
target	O
policy	B
ordinary	O
importance	B
sampling	I
uses	O
a	O
simple	O
average	O
of	O
the	O
weighted	O
returns	O
whereas	O
weighted	O
importance	B
sampling	I
uses	O
chapter	O
monte	B
carlo	I
methods	I
a	O
weighted	O
average	O
ordinary	O
importance	B
sampling	I
produces	O
unbiased	O
estimates	O
but	O
has	O
larger	O
possibly	O
infinite	O
variance	O
whereas	O
weighted	O
importance	B
sampling	I
always	O
has	O
finite	O
variance	O
and	O
is	O
preferred	O
in	O
practice	O
despite	O
their	O
conceptual	O
simplicity	O
off-policy	B
monte	B
carlo	I
methods	I
for	O
both	O
prediction	B
and	B
control	B
remain	O
unsettled	O
and	O
are	O
a	O
subject	O
of	O
ongoing	O
research	O
the	O
monte	B
carlo	I
methods	I
treated	O
in	O
this	O
chapter	O
differ	O
from	O
the	O
dp	O
methods	O
treated	O
in	O
the	O
previous	O
chapter	O
in	O
two	O
major	O
ways	O
first	O
they	O
operate	O
on	O
sample	O
experience	O
and	O
thus	O
can	O
be	O
used	O
for	O
direct	O
learning	O
without	O
a	O
model	O
second	O
they	O
do	O
not	O
bootstrap	O
that	O
is	O
they	O
do	O
not	O
update	O
their	O
value	B
estimates	O
on	O
the	O
basis	O
of	O
other	O
value	B
estimates	O
these	O
two	O
differences	O
are	O
not	O
tightly	O
linked	O
and	O
can	O
be	O
separated	O
in	O
the	O
next	O
chapter	O
we	O
consider	O
methods	O
that	O
learn	O
from	O
experience	O
like	O
monte	B
carlo	I
methods	I
but	O
also	O
bootstrap	O
like	O
dp	O
methods	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
term	O
monte	B
carlo	I
dates	O
from	O
the	O
when	O
physicists	O
at	O
los	O
alamos	O
devised	O
games	O
of	O
chance	O
that	O
they	O
could	O
study	O
to	O
help	O
understand	O
complex	O
physical	O
phenomena	O
relating	O
to	O
the	O
atom	O
bomb	O
coverage	O
of	O
monte	B
carlo	I
methods	I
in	O
this	O
sense	O
can	O
be	O
found	O
in	O
several	O
textbooks	O
kalos	O
and	O
whitlock	O
rubinstein	O
singh	O
and	O
sutton	O
distinguished	O
between	O
every-visit	O
and	O
first-visit	O
mc	O
methods	O
and	O
proved	O
results	O
relating	O
these	O
methods	O
to	O
reinforcement	B
learning	I
algorithms	O
the	O
blackjack	B
example	I
is	O
based	O
on	O
an	O
example	O
used	O
by	O
widrow	O
gupta	O
and	O
maitra	O
the	O
soap	B
bubble	I
example	I
is	O
a	O
classical	O
dirichlet	O
problem	O
whose	O
monte	B
carlo	I
solution	O
was	O
first	O
proposed	O
by	O
kakutani	O
see	O
hersh	O
and	O
griego	O
doyle	O
and	O
snell	O
barto	O
and	O
duff	O
discussed	O
policy	B
evaluation	O
in	O
the	O
context	O
of	O
classical	O
monte	B
carlo	I
algorithms	O
for	O
solving	O
systems	O
of	O
linear	O
equations	O
they	O
used	O
the	O
analysis	O
of	O
curtiss	O
to	O
point	O
out	O
the	O
computational	O
advantages	B
of	I
monte	B
carlo	I
policy	B
evaluation	O
for	O
large	O
problems	O
monte	B
carlo	I
es	O
was	O
introduced	O
in	O
the	O
edition	O
of	O
this	O
book	O
that	O
may	O
have	O
been	O
the	O
first	O
explicit	O
connection	O
between	O
monte	B
carlo	I
estimation	O
and	B
control	B
methods	O
based	O
on	O
policy	B
iteration	I
an	O
early	O
use	O
of	O
monte	B
carlo	I
methods	I
to	O
estimate	O
action	B
values	O
in	O
a	O
reinforcement	B
learning	I
context	O
was	O
by	O
michie	B
and	O
chambers	O
in	B
pole	I
balancing	I
they	O
used	O
averages	O
of	O
episode	O
durations	O
to	O
assess	O
the	O
worth	O
balancing	O
life	O
of	O
each	O
possible	O
action	B
in	O
each	O
state	B
and	O
then	O
used	O
these	O
assessments	O
to	O
control	B
action	B
selections	O
their	O
method	O
is	O
similar	O
in	O
spirit	O
to	O
monte	B
carlo	I
es	O
with	O
every-visit	O
mc	O
estimates	O
narendra	O
and	O
wheeler	O
studied	O
a	O
monte	B
carlo	I
method	O
for	O
ergodic	O
finite	O
markov	O
chains	O
that	O
used	O
the	O
return	B
accumulated	O
between	O
successive	O
visits	O
to	O
the	O
same	O
state	B
as	O
a	O
reward	O
for	O
adjusting	O
a	O
learning	O
automaton	O
s	O
action	B
probabilities	O
efficient	O
off-policy	B
learning	O
has	O
become	O
recognized	O
as	O
an	O
important	O
challenge	O
that	O
arises	O
in	O
several	O
fields	O
for	O
example	O
it	O
is	O
closely	O
related	O
to	O
the	O
idea	O
of	O
in	O
summary	O
terventions	O
and	O
counterfactuals	O
in	O
probabalistic	O
graphical	O
models	O
pearl	O
balke	O
and	O
pearl	O
off-policy	B
methods	I
using	O
importance	B
sampling	I
have	O
a	O
long	O
history	O
and	O
yet	O
still	O
are	O
not	O
well	O
understood	O
weighted	O
importance	B
sampling	I
which	O
is	O
also	O
sometimes	O
called	O
normalized	O
importance	B
sampling	I
koller	O
and	O
friedman	O
is	O
discussed	O
by	O
rubinstein	O
hesterberg	O
shelton	O
and	O
liu	O
among	O
others	O
the	O
target	O
policy	B
in	O
off-policy	B
learning	O
is	O
sometimes	O
referred	O
to	O
in	O
the	O
literature	O
as	O
the	O
estimation	O
policy	B
as	O
it	O
was	O
in	O
the	O
first	O
edition	O
of	O
this	O
book	O
the	O
racetrack	B
exercise	I
is	O
adapted	O
from	O
barto	O
bradtke	O
and	O
singh	O
and	O
from	O
gardner	O
our	O
treatment	O
of	O
the	O
idea	O
of	O
discounting-aware	O
importance	B
sampling	I
is	O
based	O
on	O
the	O
analysis	O
of	O
sutton	O
mahmood	O
precup	O
and	O
van	O
hasselt	O
it	O
has	O
been	O
worked	O
out	O
most	O
fully	O
to	O
date	O
by	O
mahmood	O
mahmood	O
van	O
hasselt	O
and	O
sutton	O
per-decision	B
importance	B
sampling	I
was	O
introduced	O
by	O
precup	O
sutton	O
and	O
singh	O
they	O
also	O
combined	O
off-policy	B
learning	O
with	O
temporal-difference	B
learning	I
eligibility	B
traces	I
and	O
approximation	O
methods	O
introducing	O
subtle	O
issues	O
that	O
we	O
consider	O
in	O
later	O
chapters	O
chapter	O
temporal-difference	B
learning	I
if	O
one	O
had	O
to	O
identify	O
one	O
idea	O
as	O
central	O
and	O
novel	O
to	O
reinforcement	B
learning	I
it	O
would	O
undoubtedly	O
be	O
temporal-difference	B
learning	I
td	B
learning	O
is	O
a	O
combination	O
of	O
monte	B
carlo	I
ideas	O
and	B
dynamic	B
programming	I
ideas	O
like	O
monte	B
carlo	I
methods	I
td	B
methods	O
can	O
learn	O
directly	O
from	O
raw	O
experience	O
without	O
a	O
model	B
of	I
the	I
environment	B
s	O
dynamics	O
like	O
dp	O
td	B
methods	O
update	O
estimates	O
based	O
in	O
part	O
on	O
other	O
learned	O
estimates	O
without	O
waiting	O
for	O
a	O
final	O
outcome	O
bootstrap	O
the	O
relationship	O
between	O
td	B
dp	O
and	B
monte	B
carlo	I
methods	I
is	O
a	O
recurring	O
theme	O
in	O
the	O
theory	O
of	O
reinforcement	B
learning	I
this	O
chapter	O
is	O
the	O
beginning	O
of	O
our	O
exploration	O
of	O
it	O
before	O
we	O
are	O
done	O
we	O
will	O
see	O
that	O
these	O
ideas	O
and	O
methods	O
blend	O
into	O
each	O
other	O
and	O
can	O
be	O
combined	O
in	O
many	O
ways	O
in	O
particular	O
in	O
chapter	O
we	O
introduce	O
n-step	B
algorithms	O
which	O
provide	O
a	O
bridge	O
from	O
td	B
to	O
monte	B
carlo	I
methods	I
and	O
in	O
chapter	O
we	O
introduce	O
the	O
td	B
algorithm	O
which	O
seamlessly	O
unifies	O
them	O
as	O
usual	O
we	O
start	O
by	O
focusing	O
on	O
the	O
policy	B
evaluation	O
or	O
prediction	B
problem	O
the	O
problem	O
of	O
estimating	O
the	O
value	B
function	I
v	O
for	O
a	O
given	O
policy	B
for	O
the	O
control	B
problem	O
an	O
optimal	O
policy	B
dp	O
td	B
and	B
monte	B
carlo	I
methods	I
all	O
use	O
some	O
variation	O
of	O
generalized	O
policy	B
iteration	I
the	O
differences	O
in	O
the	O
methods	O
are	O
primarily	O
differences	O
in	O
their	O
approaches	O
to	O
the	O
prediction	B
problem	O
td	B
prediction	B
both	O
td	B
and	B
monte	B
carlo	I
methods	I
use	O
experience	O
to	O
solve	O
the	O
prediction	B
problem	O
given	O
some	O
experience	O
following	O
a	O
policy	B
both	O
methods	O
update	O
their	O
estimate	O
v	O
of	O
v	O
for	O
the	O
nonterminal	O
states	O
st	O
occurring	O
in	O
that	O
experience	O
roughly	O
speaking	O
monte	B
carlo	I
methods	I
wait	O
until	O
the	O
return	B
following	O
the	O
visit	O
is	O
known	O
then	O
use	O
that	O
return	B
as	O
a	O
target	O
for	O
v	O
a	O
simple	O
every-visit	O
monte	B
carlo	I
method	O
suitable	O
for	O
nonstationary	O
environments	O
is	O
v	O
v	O
v	O
chapter	O
temporal-difference	B
learning	I
where	O
gt	O
is	O
the	O
actual	O
return	B
following	O
time	O
t	O
and	O
is	O
a	O
constant	O
step-size	B
parameter	I
equation	O
let	O
us	O
call	O
this	O
method	O
constant-	O
mc	O
whereas	O
monte	B
carlo	I
methods	I
must	O
wait	O
until	O
the	O
end	O
of	O
the	O
episode	O
to	O
determine	O
the	O
increment	O
to	O
v	O
then	O
is	O
gt	O
known	O
td	B
methods	O
need	O
to	O
wait	O
only	O
until	O
the	O
next	O
time	O
step	O
at	O
time	O
t	O
they	O
immediately	O
form	O
a	O
target	O
and	O
make	O
a	O
useful	O
update	O
using	O
the	O
observed	O
reward	O
and	O
the	O
estimate	O
v	O
the	O
simplest	O
td	B
method	O
makes	O
the	O
update	O
v	O
v	O
v	O
v	O
immediately	O
on	O
transition	O
to	O
and	O
receiving	O
in	O
effect	O
the	O
target	O
for	O
the	O
monte	B
carlo	I
update	O
is	O
gt	O
whereas	O
the	O
target	O
for	O
the	O
td	B
update	O
is	O
v	O
this	O
td	B
method	O
is	O
called	O
or	O
one-step	O
td	B
because	O
it	O
is	O
a	O
special	O
case	O
of	O
the	O
td	B
and	O
n-step	B
td	B
methods	O
developed	O
in	O
chapter	O
and	O
chapter	O
the	O
box	O
below	O
specifies	O
completely	O
in	O
procedural	O
form	O
tabular	O
for	O
estimating	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
algorithm	O
parameter	O
step	O
size	O
initialize	O
v	O
for	O
all	O
s	O
s	O
arbitrarily	O
except	O
that	O
v	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
a	O
action	B
given	O
by	O
for	O
s	O
take	O
action	B
a	O
observe	O
r	O
v	O
v	O
v	O
v	O
s	O
until	O
s	O
is	O
terminal	O
because	O
bases	O
its	O
update	O
in	O
part	O
on	O
an	O
existing	O
estimate	O
we	O
say	O
that	O
it	O
is	O
a	O
bootstrapping	B
method	O
like	O
dp	O
we	O
know	O
from	O
chapter	O
that	O
v	O
e	O
st	O
s	O
e	O
st	O
s	O
e	O
v	O
st	O
s	O
roughly	O
speaking	O
monte	B
carlo	I
methods	I
use	O
an	O
estimate	O
of	O
as	O
a	O
target	O
whereas	O
dp	O
methods	O
use	O
an	O
estimate	O
of	O
as	O
a	O
target	O
the	O
monte	B
carlo	I
target	O
is	O
an	O
estimate	O
because	O
the	O
expected	B
value	B
in	O
is	O
not	O
known	O
a	O
sample	O
return	B
is	O
used	O
in	O
place	O
of	O
the	O
real	O
expected	B
return	B
the	O
dp	O
target	O
is	O
an	O
estimate	O
not	O
because	O
of	O
the	O
expected	B
values	O
which	O
are	O
assumed	O
to	O
be	O
completely	O
provided	O
by	O
a	O
model	B
of	I
the	I
environment	B
but	O
because	O
v	O
is	O
not	O
known	O
and	O
the	O
current	O
estimate	O
v	O
is	O
used	O
instead	O
the	O
td	B
target	O
is	O
an	O
estimate	O
for	O
both	O
reasons	O
it	O
samples	O
the	O
expected	B
values	O
in	O
and	O
it	O
uses	O
the	O
current	O
estimate	O
v	O
instead	O
of	O
the	O
true	O
v	O
thus	O
td	B
methods	O
combine	O
td	B
prediction	B
the	O
sampling	O
of	O
monte	B
carlo	I
with	O
the	O
bootstrapping	B
of	O
dp	O
as	O
we	O
shall	O
see	O
with	O
care	O
and	O
imagination	O
this	O
can	O
take	O
us	O
a	O
long	O
way	O
toward	O
obtaining	O
the	O
advantages	B
of	I
both	O
monte	B
carlo	I
and	O
dp	O
methods	O
shown	O
to	O
the	O
right	O
is	O
the	O
backup	B
diagram	I
for	O
tabular	O
the	O
value	B
estimate	O
for	O
the	O
state	B
node	O
at	O
the	O
top	O
of	O
the	O
backup	B
diagram	I
is	O
updated	O
on	O
the	O
basis	O
of	O
the	O
one	O
sample	O
transition	O
from	O
it	O
to	O
the	O
immediately	O
following	O
state	B
we	O
refer	O
to	O
td	B
and	O
monte	B
carlo	I
updates	O
as	O
sample	O
updates	O
because	O
they	O
involve	O
looking	O
ahead	O
to	O
a	O
sample	O
successor	O
state	B
state	B
action	B
pair	O
using	O
the	O
value	B
of	O
the	O
successor	O
and	O
the	O
reward	O
along	O
the	O
way	O
to	O
compute	O
a	O
backed-up	O
value	B
and	O
then	O
updating	O
the	O
value	B
of	O
the	O
original	O
state	B
state	B
action	B
pair	O
accordingly	O
sample	O
updates	O
differ	O
from	O
the	O
expected	B
updates	O
of	O
dp	O
methods	O
in	O
that	O
they	O
are	O
based	O
on	O
a	O
single	O
sample	O
successor	O
rather	O
than	O
on	O
a	O
complete	O
distribution	O
of	O
all	O
possible	O
successors	O
finally	O
note	O
that	O
the	O
quantity	O
in	O
brackets	O
in	O
the	O
update	O
is	O
a	O
sort	O
of	O
error	O
measuring	O
the	O
difference	O
between	O
the	O
estimated	O
value	B
of	O
st	O
and	O
the	O
better	O
estimate	O
v	O
this	O
quantity	O
called	O
the	O
td	B
error	I
arises	O
in	O
various	O
forms	O
throughout	O
reinforcement	B
learning	I
t	O
v	O
v	O
notice	O
that	O
the	O
td	B
error	I
at	O
each	O
time	O
is	O
the	O
error	O
in	O
the	O
estimate	O
made	O
at	O
that	O
time	O
because	O
the	O
td	B
error	I
depends	O
on	O
the	O
next	O
state	B
and	O
next	O
reward	O
it	O
is	O
not	O
actually	O
available	O
until	O
one	O
time	O
step	O
later	O
that	O
is	O
t	O
is	O
the	O
error	O
in	O
v	O
available	O
at	O
time	O
t	O
also	O
note	O
that	O
if	O
the	O
array	O
v	O
does	O
not	O
change	O
during	O
the	O
episode	O
it	O
does	O
not	O
in	O
monte	B
carlo	I
methods	I
then	O
the	O
monte	B
carlo	I
error	O
can	O
be	O
written	O
as	O
a	O
sum	O
of	O
td	B
errors	O
gt	O
v	O
v	O
v	O
v	O
t	O
v	O
t	O
v	O
t	O
t	O
t	O
t	O
t	O
v	O
t	O
t	O
t	O
t	O
t	O
t	O
k	O
t	O
k	O
this	O
identity	O
is	O
not	O
exact	O
if	O
v	O
is	O
updated	O
during	O
the	O
episode	O
it	O
is	O
in	O
but	O
if	O
the	O
step	O
size	O
is	O
small	O
then	O
it	O
may	O
still	O
hold	O
approximately	O
generalizations	O
of	O
this	O
identity	O
play	O
an	O
important	O
role	O
in	O
the	O
theory	O
and	O
algorithms	O
of	O
temporal-difference	B
learning	I
exercise	O
if	O
v	O
changes	O
during	O
the	O
episode	O
then	O
only	O
holds	O
approximately	O
what	O
would	O
the	O
difference	O
be	O
between	O
the	O
two	O
sides	O
let	O
vt	O
denote	O
the	O
array	O
of	O
state	B
values	O
used	O
at	O
time	O
t	O
in	O
the	O
td	B
error	I
and	O
in	O
the	O
td	B
update	O
redo	O
the	O
derivation	O
above	O
to	O
determine	O
the	O
additional	O
amount	O
that	O
must	O
be	O
added	O
to	O
the	O
sum	O
of	O
td	B
errors	O
in	O
order	O
to	O
equal	O
the	O
monte	B
carlo	I
error	O
chapter	O
temporal-difference	B
learning	I
example	O
driving	O
home	O
each	O
day	O
as	O
you	O
drive	O
home	O
from	O
work	O
you	O
try	O
to	O
predict	O
how	O
long	O
it	O
will	O
take	O
to	O
get	O
home	O
when	O
you	O
leave	O
your	O
office	O
you	O
note	O
the	O
time	O
the	O
day	O
of	O
week	O
the	O
weather	O
and	O
anything	O
else	O
that	O
might	O
be	O
relevant	O
say	O
on	O
this	O
friday	O
you	O
are	O
leaving	O
at	O
exactly	O
o	O
clock	O
and	O
you	O
estimate	O
that	O
it	O
will	O
take	O
minutes	O
to	O
get	O
home	O
as	O
you	O
reach	O
your	O
car	O
it	O
is	O
and	O
you	O
notice	O
it	O
is	O
starting	O
to	O
rain	O
traffic	O
is	O
often	O
slower	O
in	O
the	O
rain	O
so	O
you	O
reestimate	O
that	O
it	O
will	O
take	O
minutes	O
from	O
then	O
or	O
a	O
total	O
of	O
minutes	O
fifteen	O
minutes	O
later	O
you	O
have	O
completed	O
the	O
highway	O
portion	O
of	O
your	O
journey	O
in	O
good	O
time	O
as	O
you	O
exit	O
onto	O
a	O
secondary	O
road	O
you	O
cut	O
your	O
estimate	O
of	O
total	O
travel	O
time	O
to	O
minutes	O
unfortunately	O
at	O
this	O
point	O
you	O
get	O
stuck	O
behind	O
a	O
slow	O
truck	O
and	O
the	O
road	O
is	O
too	O
narrow	O
to	O
pass	O
you	O
end	O
up	O
having	O
to	O
follow	O
the	O
truck	O
until	O
you	O
turn	O
onto	O
the	O
side	O
street	O
where	O
you	O
live	O
at	O
three	O
minutes	O
later	O
you	O
are	O
home	O
the	O
sequence	O
of	O
states	O
times	O
and	O
predictions	O
is	O
thus	O
as	O
follows	O
state	B
leaving	O
office	O
friday	O
at	O
reach	O
car	O
raining	O
exiting	O
highway	O
road	O
behind	O
truck	O
entering	O
home	O
street	O
arrive	O
home	O
elapsed	O
time	O
predicted	O
predicted	O
time	O
to	O
go	O
total	O
time	O
the	O
rewards	O
in	O
this	O
example	O
are	O
the	O
elapsed	O
times	O
on	O
each	O
leg	O
of	O
the	O
we	O
are	O
not	O
discounting	B
and	O
thus	O
the	O
return	B
for	O
each	O
state	B
is	O
the	O
actual	O
time	O
to	O
go	O
from	O
that	O
state	B
the	O
value	B
of	O
each	O
state	B
is	O
the	O
expected	B
time	O
to	O
go	O
the	O
second	O
column	O
of	O
numbers	O
gives	O
the	O
current	O
estimated	O
value	B
for	O
each	O
state	B
encountered	O
a	O
simple	O
way	O
to	O
view	O
the	O
operation	O
of	O
monte	B
carlo	I
methods	I
is	O
to	O
plot	O
the	O
predicted	O
total	O
time	O
last	O
column	O
over	O
the	O
sequence	O
as	O
in	O
figure	O
the	O
red	O
arrows	O
show	O
the	O
changes	O
in	O
predictions	O
recommended	O
by	O
the	O
constant-	O
mc	O
method	O
for	O
these	O
are	O
exactly	O
the	O
errors	O
between	O
the	O
estimated	O
value	B
time	O
to	O
go	O
in	O
each	O
state	B
and	O
the	O
actual	O
return	B
time	O
to	O
go	O
for	O
example	O
when	O
you	O
exited	O
the	O
highway	O
you	O
thought	O
it	O
would	O
take	O
only	O
minutes	O
more	O
to	O
get	O
home	O
but	O
in	O
fact	O
it	O
took	O
minutes	O
equation	O
applies	O
at	O
this	O
point	O
and	O
determines	O
an	O
increment	O
in	O
the	O
estimate	O
of	O
time	O
to	O
go	O
after	O
exiting	O
the	O
highway	O
the	O
error	O
gt	O
v	O
at	O
this	O
time	O
is	O
eight	O
minutes	O
suppose	O
the	O
step-size	B
parameter	I
is	O
then	O
the	O
predicted	O
time	O
to	O
go	O
after	O
exiting	O
the	O
highway	O
would	O
be	O
revised	O
upward	O
by	O
four	O
minutes	O
as	O
a	O
result	O
of	O
this	O
experience	O
this	O
is	O
probably	O
too	O
large	O
a	O
change	O
in	O
this	O
case	O
the	O
truck	O
was	O
probably	O
just	O
an	O
unlucky	O
break	O
in	O
any	O
event	O
the	O
change	O
can	O
only	O
be	O
made	O
off-line	B
that	O
is	O
after	O
you	O
have	O
reached	O
home	O
only	O
at	O
this	O
point	O
do	O
you	O
know	O
any	O
of	O
the	O
actual	O
returns	O
is	O
it	O
necessary	O
to	O
wait	O
until	O
the	O
final	O
outcome	O
is	O
known	O
before	O
learning	O
can	O
begin	O
suppose	O
on	O
another	O
day	O
you	O
again	O
estimate	O
when	O
leaving	O
your	O
office	O
that	O
it	O
will	O
take	O
minutes	O
to	O
drive	O
home	O
but	O
then	O
you	O
become	O
stuck	O
in	O
a	O
massive	O
traffic	O
jam	O
twenty-five	O
minutes	O
after	O
leaving	O
the	O
office	O
you	O
are	O
still	O
bumper-to-bumper	O
on	O
the	O
highway	O
you	O
now	O
this	O
were	O
a	O
control	B
problem	O
with	O
the	O
objective	O
of	O
minimizing	O
travel	O
time	O
then	O
we	O
would	O
of	O
course	O
make	O
the	O
rewards	O
the	O
negative	O
of	O
the	O
elapsed	O
time	O
but	O
because	O
we	O
are	O
concerned	O
here	O
only	O
with	O
prediction	B
evaluation	O
we	O
can	O
keep	O
things	O
simple	O
by	O
using	O
positive	O
numbers	O
advantages	B
of	I
td	B
prediction	B
methods	O
figure	O
changes	O
recommended	O
in	O
the	O
driving	O
home	O
example	O
by	O
monte	B
carlo	I
methods	I
and	O
td	B
methods	O
estimate	O
that	O
it	O
will	O
take	O
another	O
minutes	O
to	O
get	O
home	O
for	O
a	O
total	O
of	O
minutes	O
as	O
you	O
wait	O
in	O
traffic	O
you	O
already	O
know	O
that	O
your	O
initial	O
estimate	O
of	O
minutes	O
was	O
too	O
optimistic	O
must	O
you	O
wait	O
until	O
you	O
get	O
home	O
before	O
increasing	O
your	O
estimate	O
for	O
the	O
initial	O
state	B
according	O
to	O
the	O
monte	B
carlo	I
approach	O
you	O
must	O
because	O
you	O
don	O
t	O
yet	O
know	O
the	O
true	O
return	B
according	O
to	O
a	O
td	B
approach	O
on	O
the	O
other	O
hand	O
you	O
would	O
learn	O
immediately	O
shifting	O
your	O
initial	O
estimate	O
from	O
minutes	O
toward	O
in	O
fact	O
each	O
estimate	O
would	O
be	O
shifted	O
toward	O
the	O
estimate	O
that	O
immediately	O
follows	O
it	O
returning	O
to	O
our	O
first	O
day	O
of	O
driving	O
figure	O
shows	O
the	O
changes	O
in	O
the	O
predictions	O
recommended	O
by	O
the	O
td	B
rule	O
are	O
the	O
changes	O
made	O
by	O
the	O
rule	O
if	O
each	O
error	O
is	O
proportional	O
to	O
the	O
change	O
over	O
time	O
of	O
the	O
prediction	B
that	O
is	O
to	O
the	O
temporal	O
differences	O
in	O
predictions	O
besides	O
giving	O
you	O
something	O
to	O
do	O
while	O
waiting	O
in	O
traffic	O
there	O
are	O
several	O
computational	O
reasons	O
why	O
it	O
is	O
advantageous	O
to	O
learn	O
based	O
on	O
your	O
current	O
predictions	O
rather	O
than	O
waiting	O
until	O
termination	O
when	O
you	O
know	O
the	O
actual	O
return	B
we	O
briefly	O
discuss	O
some	O
of	O
these	O
in	O
the	O
next	O
section	O
exercise	O
this	O
is	O
an	O
exercise	O
to	O
help	O
develop	O
your	O
intuition	O
about	O
why	O
td	B
methods	O
are	O
often	O
more	O
efficient	O
than	O
monte	B
carlo	I
methods	I
consider	O
the	O
driving	O
home	O
example	O
and	O
how	O
it	O
is	O
addressed	O
by	O
td	B
and	B
monte	B
carlo	I
methods	I
can	O
you	O
imagine	O
a	O
scenario	O
in	O
which	O
a	O
td	B
update	O
would	O
be	O
better	O
on	O
average	O
than	O
a	O
monte	B
carlo	I
update	O
give	O
an	O
example	O
scenario	O
a	O
description	O
of	O
past	O
experience	O
and	O
a	O
current	O
state	B
in	O
which	O
you	O
would	O
expect	O
the	O
td	B
update	O
to	O
be	O
better	O
here	O
s	O
a	O
hint	O
suppose	O
you	O
have	O
lots	O
of	O
experience	O
driving	O
home	O
from	O
work	O
then	O
you	O
move	O
to	O
a	O
new	O
building	O
and	O
a	O
new	O
parking	O
lot	O
you	O
still	O
enter	O
the	O
highway	O
at	O
the	O
same	O
place	O
now	O
you	O
are	O
starting	O
to	O
learn	O
predictions	O
for	O
the	O
new	O
building	O
can	O
you	O
see	O
why	O
td	B
updates	O
are	O
likely	O
to	O
be	O
much	O
better	O
at	O
least	O
initially	O
in	O
this	O
case	O
might	O
the	O
same	O
sort	O
of	O
thing	O
happen	O
in	O
the	O
original	O
task	O
chapter	O
temporal-difference	B
learning	I
advantages	B
of	I
td	B
prediction	B
methods	O
td	B
methods	O
update	O
their	O
estimates	O
based	O
in	O
part	O
on	O
other	O
estimates	O
they	O
learn	O
a	O
guess	O
from	O
a	O
guess	O
they	O
bootstrap	O
is	O
this	O
a	O
good	O
thing	O
to	O
do	O
what	O
advantages	O
do	O
td	B
methods	O
have	O
over	O
monte	B
carlo	I
and	O
dp	O
methods	O
developing	O
and	O
answering	O
such	O
questions	O
will	O
take	O
the	O
rest	O
of	O
this	O
book	O
and	O
more	O
in	O
this	O
section	O
we	O
briefly	O
anticipate	O
some	O
of	O
the	O
answers	O
obviously	O
td	B
methods	O
have	O
an	O
advantage	O
over	O
dp	O
methods	O
in	O
that	O
they	O
do	O
not	O
require	O
a	O
model	B
of	I
the	I
environment	B
of	O
its	O
reward	O
and	O
next-state	O
probability	O
distributions	O
the	O
next	O
most	O
obvious	O
advantage	O
of	O
td	B
methods	O
over	O
monte	B
carlo	I
methods	I
is	O
that	O
they	O
are	O
naturally	O
implemented	O
in	O
an	O
online	B
fully	O
incremental	O
fashion	O
with	O
monte	B
carlo	I
methods	I
one	O
must	O
wait	O
until	O
the	O
end	O
of	O
an	O
episode	O
because	O
only	O
then	O
is	O
the	O
return	B
known	O
whereas	O
with	O
td	B
methods	O
one	O
need	O
wait	O
only	O
one	O
time	O
step	O
surprisingly	O
often	O
this	O
turns	O
out	O
to	O
be	O
a	O
critical	O
consideration	O
some	O
applications	O
have	O
very	O
long	O
episodes	B
so	O
that	O
delaying	O
all	O
learning	O
until	O
the	O
end	O
of	O
the	O
episode	O
is	O
too	O
slow	O
other	O
applications	O
are	O
continuing	B
tasks	I
and	O
have	O
no	O
episodes	B
at	O
all	O
finally	O
as	O
we	O
noted	O
in	O
the	O
previous	O
chapter	O
some	O
monte	B
carlo	I
methods	I
must	O
ignore	O
or	O
discount	O
episodes	B
on	O
which	O
experimental	O
actions	O
are	O
taken	O
which	O
can	O
greatly	O
slow	O
learning	O
td	B
methods	O
are	O
much	O
less	O
susceptible	O
to	O
these	O
problems	O
because	O
they	O
learn	O
from	O
each	O
transition	O
regardless	O
of	O
what	O
subsequent	O
actions	O
are	O
taken	O
but	O
are	O
td	B
methods	O
sound	O
certainly	O
it	O
is	O
convenient	O
to	O
learn	O
one	O
guess	O
from	O
the	O
next	O
without	O
waiting	O
for	O
an	O
actual	O
outcome	O
but	O
can	O
we	O
still	O
guarantee	O
convergence	O
to	O
the	O
correct	O
answer	O
happily	O
the	O
answer	O
is	O
yes	O
for	O
any	O
fixed	O
policy	B
has	O
been	O
proved	O
to	O
converge	O
to	O
v	O
in	O
the	O
mean	O
for	O
a	O
constant	O
step-size	B
parameter	I
if	O
it	O
is	O
sufficiently	O
small	O
and	O
with	O
probability	O
if	O
the	O
step-size	B
parameter	I
decreases	O
according	O
to	O
the	O
usual	O
stochastic	O
approximation	O
conditions	O
most	O
convergence	O
proofs	O
apply	O
only	O
to	O
the	O
table-based	O
case	O
of	O
the	O
algorithm	O
presented	O
above	O
but	O
some	O
also	O
apply	O
to	O
the	O
case	O
of	O
general	O
linear	O
function	B
approximation	I
these	O
results	O
are	O
discussed	O
in	O
a	O
more	O
general	O
setting	O
in	O
chapter	O
if	O
both	O
td	B
and	B
monte	B
carlo	I
methods	I
converge	O
asymptotically	O
to	O
the	O
correct	O
predictions	O
then	O
a	O
natural	O
next	O
question	O
is	O
which	O
gets	O
there	O
first	O
in	O
other	O
words	O
which	O
method	O
learns	O
faster	O
which	O
makes	O
the	O
more	O
efficient	O
use	O
of	O
limited	O
data	O
at	O
the	O
current	O
time	O
this	O
is	O
an	O
open	O
question	O
in	O
the	O
sense	O
that	O
no	O
one	O
has	O
been	O
able	O
to	O
prove	O
mathematically	O
that	O
one	O
method	O
converges	O
faster	O
than	O
the	O
other	O
in	O
fact	O
it	O
is	O
not	O
even	O
clear	O
what	O
is	O
the	O
most	O
appropriate	O
formal	O
way	O
to	O
phrase	O
this	O
question	O
in	O
practice	O
however	O
td	B
methods	O
have	O
usually	O
been	O
found	O
to	O
converge	O
faster	O
than	O
constant-	O
mc	O
methods	O
on	O
stochastic	O
tasks	O
as	O
illustrated	O
in	O
example	O
advantages	B
of	I
td	B
prediction	B
methods	O
example	O
random	B
walk	I
in	O
this	O
example	O
we	O
empirically	O
compare	O
the	O
prediction	B
abilities	O
of	O
and	O
constant-	O
mc	O
when	O
applied	O
to	O
the	O
following	O
markov	O
reward	O
process	O
a	O
markov	O
reward	O
process	O
or	O
mrp	O
is	O
a	O
markov	O
decision	O
process	O
without	O
actions	O
we	O
will	O
often	O
use	O
mrps	O
when	O
focusing	O
on	O
the	O
prediction	B
problem	O
in	O
which	O
there	O
is	O
no	O
need	O
to	O
distinguish	O
the	O
dynamics	O
due	O
to	O
the	O
environment	B
from	O
those	O
due	O
to	O
the	O
agent	O
in	O
this	O
mrp	O
all	O
episodes	B
start	O
in	O
the	O
center	O
state	B
c	O
then	O
proceed	O
either	O
left	O
or	O
right	O
by	O
one	O
state	B
on	O
each	O
step	O
with	O
equal	O
probability	O
episodes	B
terminate	O
either	O
on	O
the	O
extreme	O
left	O
or	O
the	O
extreme	O
right	O
when	O
an	O
episode	O
terminates	O
on	O
the	O
right	O
a	O
reward	O
of	O
occurs	O
all	O
other	O
rewards	O
are	O
zero	O
for	O
example	O
a	O
typical	O
episode	O
might	O
consist	O
of	O
the	O
following	O
state-and-reward	O
sequence	O
c	O
b	O
c	O
d	O
e	O
because	O
this	O
task	O
is	O
undiscounted	O
the	O
true	O
value	B
of	O
each	O
state	B
is	O
the	O
probability	O
of	O
terminating	O
on	O
the	O
right	O
if	O
starting	O
from	O
that	O
state	B
thus	O
the	O
true	O
value	B
of	O
the	O
center	O
state	B
is	O
v	O
the	O
true	O
values	O
of	O
all	O
the	O
states	O
a	O
through	O
e	O
are	O
and	O
the	O
left	O
graph	O
above	O
shows	O
the	O
values	O
learned	O
after	O
various	O
numbers	O
of	O
episodes	B
on	O
a	O
single	O
run	O
of	O
the	O
estimates	O
after	O
episodes	B
are	O
about	O
as	O
close	O
as	O
they	O
ever	O
come	O
to	O
the	O
true	O
values	O
with	O
a	O
constant	O
step-size	B
parameter	I
in	O
this	O
example	O
the	O
values	O
fluctuate	O
indefinitely	O
in	O
response	O
to	O
the	O
outcomes	O
of	O
the	O
most	O
recent	O
episodes	B
the	O
right	O
graph	O
shows	O
learning	O
curves	O
for	O
the	O
two	O
methods	O
for	O
various	O
values	O
of	O
the	O
performance	O
measure	O
shown	O
is	O
the	O
root	O
mean-squared	O
error	O
between	O
the	O
value	B
function	I
learned	O
and	O
the	O
true	O
value	B
function	I
averaged	O
over	O
the	O
five	O
states	O
then	O
averaged	O
over	O
runs	O
in	O
all	O
cases	O
the	O
approximate	O
value	B
function	I
was	O
initialized	O
to	O
the	O
intermediate	O
value	B
v	O
for	O
all	O
s	O
the	O
td	B
method	O
was	O
consistently	O
better	O
than	O
the	O
mc	O
method	O
on	O
this	O
task	O
episodestdmcrms	O
erroraveragedover	O
states	O
rms	O
error	O
averaged	O
over	O
states	O
chapter	O
temporal-difference	B
learning	I
exercise	O
from	O
the	O
results	O
shown	O
in	O
the	O
left	O
graph	O
of	O
the	O
random	B
walk	I
example	O
it	O
appears	O
that	O
the	O
first	O
episode	O
results	O
in	O
a	O
change	O
in	O
only	O
v	O
what	O
does	O
this	O
tell	O
you	O
about	O
what	O
happened	O
on	O
the	O
first	O
episode	O
why	O
was	O
only	O
the	O
estimate	O
for	O
this	O
one	O
state	B
changed	O
by	O
exactly	O
how	O
much	O
was	O
it	O
changed	O
exercise	O
the	O
specific	O
results	O
shown	O
in	O
the	O
right	O
graph	O
of	O
the	O
random	B
walk	I
example	O
are	O
dependent	O
on	O
the	O
value	B
of	O
the	O
step-size	B
parameter	I
do	O
you	O
think	O
the	O
conclusions	O
about	O
which	O
algorithm	O
is	O
better	O
would	O
be	O
affected	O
if	O
a	O
wider	O
range	O
of	O
values	O
were	O
used	O
is	O
there	O
a	O
different	O
fixed	O
value	B
of	O
at	O
which	O
either	O
algorithm	O
would	O
have	O
performed	O
significantly	O
better	O
than	O
shown	O
why	O
or	O
why	O
not	O
exercise	O
in	O
the	O
right	O
graph	O
of	O
the	O
random	B
walk	I
example	O
the	O
rms	O
error	O
of	O
the	O
td	B
method	O
seems	O
to	O
go	O
down	O
and	O
then	O
up	O
again	O
particularly	O
at	O
high	O
s	O
what	O
could	O
have	O
caused	O
this	O
do	O
you	O
think	O
this	O
always	O
occurs	O
or	O
might	O
it	O
be	O
a	O
function	O
of	O
how	O
the	O
approximate	O
value	B
function	I
was	O
initialized	O
exercise	O
in	O
example	O
we	O
stated	O
that	O
the	O
true	O
values	O
for	O
the	O
random	B
walk	I
example	O
are	O
for	O
states	O
a	O
through	O
e	O
describe	O
at	O
least	O
two	O
different	O
ways	O
that	O
these	O
could	O
have	O
been	O
computed	O
which	O
would	O
you	O
guess	O
we	O
actually	O
used	O
why	O
and	O
optimality	B
of	I
suppose	O
there	O
is	O
available	O
only	O
a	O
finite	O
amount	O
of	O
experience	O
say	O
episodes	B
or	O
time	O
steps	O
in	O
this	O
case	O
a	O
common	O
approach	O
with	O
incremental	O
learning	O
methods	O
is	O
to	O
present	O
the	O
experience	O
repeatedly	O
until	O
the	O
method	O
converges	O
upon	O
an	O
answer	O
given	O
an	O
approximate	O
value	B
function	I
v	O
the	O
increments	O
specified	O
by	O
or	O
are	O
computed	O
for	O
every	O
time	O
step	O
t	O
at	O
which	O
a	O
nonterminal	O
state	B
is	O
visited	O
but	O
the	O
value	B
function	I
is	O
changed	O
only	O
once	O
by	O
the	O
sum	O
of	O
all	O
the	O
increments	O
then	O
all	O
the	O
available	O
experience	O
is	O
processed	O
again	O
with	O
the	O
new	O
value	B
function	I
to	O
produce	O
a	O
new	O
overall	O
increment	O
and	O
so	O
on	O
until	O
the	O
value	B
function	I
converges	O
we	O
call	O
this	O
batch	O
updating	O
because	O
updates	O
are	O
made	O
only	O
after	O
processing	O
each	O
complete	O
batch	O
of	O
training	O
data	O
under	O
batch	O
updating	O
converges	O
deterministically	O
to	O
a	O
single	O
answer	O
independent	O
of	O
the	O
step-size	B
parameter	I
as	O
long	O
as	O
is	O
chosen	O
to	O
be	O
sufficiently	O
small	O
the	O
constant-	O
mc	O
method	O
also	O
converges	O
deterministically	O
under	O
the	O
same	O
conditions	O
but	O
to	O
a	O
different	O
answer	O
understanding	O
these	O
two	O
answers	O
will	O
help	O
us	O
understand	O
the	O
difference	O
between	O
the	O
two	O
methods	O
under	O
normal	O
updating	O
the	O
methods	O
do	O
not	O
move	O
all	O
the	O
way	O
to	O
their	O
respective	O
batch	O
answers	O
but	O
in	O
some	O
sense	O
they	O
take	O
steps	O
in	O
these	O
directions	O
before	O
trying	O
to	O
understand	O
the	O
two	O
answers	O
in	O
general	O
for	O
all	O
possible	O
tasks	O
we	O
first	O
look	O
at	O
a	O
few	O
examples	O
example	O
random	B
walk	I
under	O
batch	O
updating	O
batch-updating	O
versions	O
of	O
and	O
constant-	O
mc	O
were	O
applied	O
as	O
follows	O
to	O
the	O
random	B
walk	I
prediction	B
example	O
after	O
each	O
new	O
episode	O
all	O
episodes	B
seen	O
so	O
far	O
were	O
treated	O
as	O
a	O
batch	O
they	O
were	O
repeatedly	O
presented	O
to	O
the	O
algorithm	O
either	O
or	O
constant-	O
mc	O
with	O
sufficiently	O
small	O
that	O
the	O
value	B
function	I
converged	O
the	O
resulting	O
value	B
function	I
was	O
then	O
compared	O
with	O
v	O
and	O
the	O
average	O
root	O
mean-squared	O
error	O
across	O
the	O
five	O
states	O
across	O
independent	O
repetitions	O
of	O
the	O
whole	O
experiment	O
was	O
plotted	O
to	O
obtain	O
optimality	B
of	I
the	O
learning	O
curves	O
shown	O
in	O
figure	O
note	O
that	O
the	O
batch	O
td	B
method	O
was	O
consistently	O
better	O
than	O
the	O
batch	O
monte	B
carlo	I
method	O
under	O
batch	O
training	O
constant-	O
mc	O
converges	O
to	O
values	O
v	O
that	O
are	O
sample	O
averages	O
of	O
the	O
actual	O
returns	O
experienced	O
after	O
visiting	O
each	O
state	B
s	O
these	O
are	O
optimal	O
estimates	O
in	O
the	O
sense	O
that	O
they	O
minimize	O
the	O
mean-squared	O
error	O
from	O
the	O
actual	O
returns	O
in	O
the	O
training	O
set	O
in	O
this	O
sense	O
it	O
is	O
surprising	O
that	O
the	O
batch	O
td	B
method	O
was	O
able	O
to	O
perform	O
better	O
according	O
to	O
the	O
root	O
mean-squared	O
error	O
measure	O
shown	O
in	O
the	O
figure	O
to	O
the	O
right	O
how	O
is	O
it	O
that	O
batch	O
td	B
was	O
able	O
to	O
perform	O
better	O
than	O
this	O
optimal	O
method	O
the	O
answer	O
is	O
that	O
the	O
monte	B
carlo	I
method	O
is	O
optimal	O
only	O
in	O
a	O
limited	O
way	O
and	O
that	O
td	B
is	O
optimal	O
in	O
a	O
way	O
that	O
is	O
more	O
relevant	O
to	O
predicting	O
returns	O
figure	O
performance	O
of	O
and	O
constant-	O
mc	O
under	O
batch	O
training	O
on	O
the	O
random	B
walk	I
task	O
example	O
you	O
are	O
the	O
predictor	O
place	O
yourself	O
now	O
in	O
the	O
role	O
of	O
the	O
predictor	O
of	O
returns	O
for	O
an	O
unknown	O
markov	O
reward	O
process	O
suppose	O
you	O
observe	O
the	O
following	O
eight	O
episodes	B
a	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
this	O
means	O
that	O
the	O
first	O
episode	O
started	O
in	O
state	B
a	O
transitioned	O
to	O
b	O
with	O
a	O
reward	O
of	O
and	O
then	O
terminated	O
from	O
b	O
with	O
a	O
reward	O
of	O
the	O
other	O
seven	O
episodes	B
were	O
even	O
shorter	O
starting	O
from	O
b	O
and	O
terminating	O
immediately	O
given	O
this	O
batch	O
of	O
data	O
what	O
would	O
you	O
say	O
are	O
the	O
optimal	O
predictions	O
the	O
best	O
values	O
for	O
the	O
estimates	O
v	O
and	O
v	O
everyone	O
would	O
probably	O
agree	O
that	O
the	O
optimal	O
value	B
for	O
v	O
is	O
because	O
six	O
out	O
of	O
the	O
eight	O
times	O
in	O
state	B
b	O
the	O
process	O
terminated	O
immediately	O
with	O
a	O
return	B
of	O
and	O
the	O
other	O
two	O
times	O
in	O
b	O
the	O
process	O
terminated	O
immediately	O
with	O
a	O
return	B
of	O
but	O
what	O
is	O
the	O
optimal	O
value	B
for	O
the	O
estimate	O
v	O
given	O
this	O
data	O
here	O
there	O
are	O
two	O
reasonable	O
answers	O
one	O
is	O
to	O
observe	O
that	O
of	O
the	O
times	O
the	O
process	O
was	O
in	O
state	B
a	O
it	O
traversed	O
immediately	O
to	O
b	O
a	O
reward	O
of	O
and	O
because	O
we	O
have	O
already	O
decided	O
that	O
b	O
has	O
value	B
therefore	O
a	O
must	O
have	O
value	B
as	O
well	O
one	O
way	O
of	O
viewing	O
this	O
answer	O
is	O
that	O
it	O
is	O
based	O
on	O
first	O
modeling	O
the	O
markov	O
process	O
in	O
this	O
case	O
as	O
shown	O
to	O
the	O
right	O
and	O
then	O
computing	O
the	O
correct	O
estimates	O
given	O
the	O
model	O
which	O
indeed	O
in	O
this	O
case	O
gives	O
v	O
this	O
is	O
trainingwalks	O
episodesrms	O
erroraveragedover	O
statesabr	O
chapter	O
temporal-difference	B
learning	I
also	O
the	O
answer	O
that	O
batch	O
gives	O
the	O
other	O
reasonable	O
answer	O
is	O
simply	O
to	O
observe	O
that	O
we	O
have	O
seen	O
a	O
once	O
and	O
the	O
return	B
that	O
followed	O
it	O
was	O
we	O
therefore	O
estimate	O
v	O
as	O
this	O
is	O
the	O
answer	O
that	O
batch	O
monte	B
carlo	I
methods	I
give	O
notice	O
that	O
it	O
is	O
also	O
the	O
answer	O
that	O
gives	O
minimum	O
squared	O
error	O
on	O
the	O
training	O
data	O
in	O
fact	O
it	O
gives	O
zero	O
error	O
on	O
the	O
data	O
but	O
still	O
we	O
expect	O
the	O
first	O
answer	O
to	O
be	O
better	O
if	O
the	O
process	O
is	O
markov	O
we	O
expect	O
that	O
the	O
first	O
answer	O
will	O
produce	O
lower	O
error	O
on	O
future	O
data	O
even	O
though	O
the	O
monte	B
carlo	I
answer	O
is	O
better	O
on	O
the	O
existing	O
data	O
example	O
illustrates	O
a	O
general	O
difference	O
between	O
the	O
estimates	O
found	O
by	O
batch	O
and	O
batch	O
monte	B
carlo	I
methods	I
batch	O
monte	B
carlo	I
methods	I
always	O
find	O
the	O
estimates	O
that	O
minimize	O
mean-squared	O
error	O
on	O
the	O
training	O
set	O
whereas	O
batch	O
always	O
finds	O
the	O
estimates	O
that	O
would	O
be	O
exactly	O
correct	O
for	O
the	O
maximum-likelihood	O
model	O
of	O
the	O
markov	O
process	O
in	O
general	O
the	O
maximum-likelihood	B
estimate	I
of	O
a	O
parameter	O
is	O
the	O
parameter	O
value	B
whose	O
probability	O
of	O
generating	O
the	O
data	O
is	O
greatest	O
in	O
this	O
case	O
the	O
maximum-likelihood	B
estimate	I
is	O
the	O
model	O
of	O
the	O
markov	O
process	O
formed	O
in	O
the	O
obvious	O
way	O
from	O
the	O
observed	O
episodes	B
the	O
estimated	O
transition	O
probability	O
from	O
i	O
to	O
j	O
is	O
the	O
fraction	O
of	O
observed	O
transitions	O
from	O
i	O
that	O
went	O
to	O
j	O
and	O
the	O
associated	O
expected	B
reward	O
is	O
the	O
average	O
of	O
the	O
rewards	O
observed	O
on	O
those	O
transitions	O
given	O
this	O
model	O
we	O
can	O
compute	O
the	O
estimate	O
of	O
the	O
value	B
function	I
that	O
would	O
be	O
exactly	O
correct	O
if	O
the	O
model	O
were	O
exactly	O
correct	O
this	O
is	O
called	O
the	O
certainty-equivalence	B
estimate	I
because	O
it	O
is	O
equivalent	O
to	O
assuming	O
that	O
the	O
estimate	O
of	O
the	O
underlying	O
process	O
was	O
known	O
with	O
certainty	O
rather	O
than	O
being	O
approximated	O
in	O
general	O
batch	O
converges	O
to	O
the	O
certainty-equivalence	B
estimate	I
this	O
helps	O
explain	O
why	O
td	B
methods	O
converge	O
more	O
quickly	O
than	O
monte	B
carlo	I
methods	I
in	O
batch	O
form	O
is	O
faster	O
than	O
monte	B
carlo	I
methods	I
because	O
it	O
computes	O
the	O
true	O
certainty-equivalence	B
estimate	I
this	O
explains	O
the	O
advantage	O
of	O
shown	O
in	O
the	O
batch	O
results	O
on	O
the	O
random	B
walk	I
task	O
the	O
relationship	O
to	O
the	O
certaintyequivalence	O
estimate	O
may	O
also	O
explain	O
in	O
part	O
the	O
speed	O
advantage	O
of	O
nonbatch	O
example	O
page	O
right	O
graph	O
although	O
the	O
nonbatch	O
methods	O
do	O
not	O
achieve	O
either	O
the	O
certainty-equivalence	O
or	O
the	O
minimum	O
squared-error	O
estimates	O
they	O
can	O
be	O
understood	O
as	O
moving	O
roughly	O
in	O
these	O
directions	O
nonbatch	O
may	O
be	O
faster	O
than	O
constant-	O
mc	O
because	O
it	O
is	O
moving	O
toward	O
a	O
better	O
estimate	O
even	O
though	O
it	O
is	O
not	O
getting	O
all	O
the	O
way	O
there	O
at	O
the	O
current	O
time	O
nothing	O
more	O
definite	O
can	O
be	O
said	O
about	O
the	O
relative	O
efficiency	O
of	O
online	B
td	B
and	B
monte	B
carlo	I
methods	I
finally	O
it	O
is	O
worth	O
noting	O
that	O
although	O
the	O
certainty-equivalence	B
estimate	I
is	O
in	O
some	O
sense	O
an	O
optimal	O
solution	O
it	O
is	O
almost	O
never	O
feasible	O
to	O
compute	O
it	O
directly	O
if	O
n	O
is	O
the	O
number	O
of	O
states	O
then	O
just	O
forming	O
the	O
maximum-likelihood	B
estimate	I
of	O
the	O
process	O
may	O
require	O
on	O
the	O
order	O
of	O
memory	O
and	O
computing	O
the	O
corresponding	O
value	B
function	I
requires	O
on	O
the	O
order	O
of	O
computational	O
steps	O
if	O
done	O
conventionally	O
in	O
these	O
terms	O
it	O
is	O
indeed	O
striking	O
that	O
td	B
methods	O
can	O
approximate	O
the	O
same	O
solution	O
using	O
memory	O
no	O
more	O
than	O
order	O
n	O
and	O
repeated	O
computations	O
over	O
the	O
training	O
set	O
on	O
tasks	O
with	O
large	O
state	B
spaces	O
td	B
methods	O
may	O
be	O
the	O
only	O
feasible	O
way	O
of	O
approximating	O
the	O
certainty-equivalence	O
solution	O
exercise	O
design	O
an	O
off-policy	B
version	O
of	O
the	O
update	O
that	O
can	O
be	O
used	O
with	O
arbi	O
sarsa	B
on-policy	O
td	B
control	B
trary	O
target	O
policy	B
and	O
covering	O
behavior	O
policy	B
b	O
using	O
at	O
each	O
step	O
t	O
the	O
importance	B
sampling	I
ratio	B
tt	O
sarsa	B
on-policy	O
td	B
control	B
we	O
turn	O
now	O
to	O
the	O
use	O
of	O
td	B
prediction	B
methods	O
for	O
the	O
control	B
problem	O
as	O
usual	O
we	O
follow	O
the	O
pattern	O
of	O
generalized	O
policy	B
iteration	I
only	O
this	O
time	O
using	O
td	B
methods	O
for	O
the	O
evaluation	O
or	O
prediction	B
part	O
as	O
with	O
monte	B
carlo	I
methods	I
we	O
face	O
the	O
need	O
to	O
trade	O
off	O
exploration	O
and	O
exploitation	O
and	O
again	O
approaches	O
fall	O
into	O
two	O
main	O
classes	O
on-policy	O
and	O
off-policy	B
in	O
this	O
section	O
we	O
present	O
an	O
on-policy	O
td	B
control	B
method	O
the	O
first	O
step	O
is	O
to	O
learn	O
an	O
action-value	O
function	O
rather	O
than	O
a	O
state-value	O
function	O
in	O
particular	O
for	O
an	O
on-policy	O
method	O
we	O
must	O
estimate	O
q	O
a	O
for	O
the	O
current	O
behavior	O
policy	B
and	O
for	O
all	O
states	O
s	O
and	O
actions	O
a	O
this	O
can	O
be	O
done	O
using	O
essentially	O
the	O
same	O
td	B
method	O
described	O
above	O
for	O
learning	O
v	O
recall	O
that	O
an	O
episode	O
consists	O
of	O
an	O
alternating	O
sequence	O
of	O
states	O
and	O
state	B
action	B
pairs	O
in	O
the	O
previous	O
section	O
we	O
considered	O
transitions	O
from	O
state	B
to	O
state	B
and	O
learned	O
the	O
values	O
of	O
states	O
now	O
we	O
consider	O
transitions	O
from	O
state	B
action	B
pair	O
to	O
state	B
action	B
pair	O
and	O
learn	O
the	O
values	O
of	O
state	B
action	B
pairs	O
formally	O
these	O
cases	O
are	O
identical	O
they	O
are	O
both	O
markov	O
chains	O
with	O
a	O
reward	O
process	O
the	O
theorems	O
assuring	O
the	O
convergence	O
of	O
state	B
values	O
under	O
also	O
apply	O
to	O
the	O
corresponding	O
algorithm	O
for	B
action	B
values	I
qst	O
at	O
qst	O
at	O
qst	O
if	O
this	O
update	O
is	O
done	O
after	O
every	O
transition	O
from	O
a	O
nonterminal	O
state	B
st	O
is	O
terminal	O
then	O
is	O
defined	O
as	O
zero	O
this	O
rule	O
uses	O
every	O
element	O
of	O
the	O
quintuple	O
of	O
events	O
at	O
that	O
make	O
up	O
a	O
transition	O
from	O
one	O
state	B
action	B
pair	O
to	O
the	O
next	O
this	O
quintuple	O
gives	O
rise	O
to	O
the	O
name	O
sarsa	B
for	O
the	O
algorithm	O
the	O
backup	B
diagram	I
for	B
sarsa	B
is	O
as	O
shown	O
to	O
the	O
right	O
sarsa	B
exercise	O
show	O
that	O
an	O
action-value	O
version	O
of	O
holds	O
for	O
the	O
action-value	O
form	O
of	O
the	O
td	B
error	I
t	O
qst	O
at	O
again	O
assuming	O
that	O
the	O
values	O
don	O
t	O
change	O
from	O
step	O
to	O
step	O
it	O
is	O
straightforward	O
to	O
design	O
an	O
on-policy	O
control	B
algorithm	O
based	O
on	O
the	O
sarsa	B
prediction	B
method	O
as	O
in	O
all	O
on-policy	B
methods	I
we	O
continually	O
estimate	O
q	O
for	O
the	O
behavior	O
policy	B
and	O
at	O
the	O
same	O
time	O
change	O
toward	O
greediness	O
with	O
respect	O
to	O
q	O
the	O
general	O
form	O
of	O
the	O
sarsa	B
control	B
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
the	O
convergence	O
properties	O
of	O
the	O
sarsa	B
algorithm	O
depend	O
on	O
the	O
nature	O
of	O
the	O
policy	B
s	O
dependence	O
on	O
q	O
for	O
example	O
one	O
could	O
use	O
or	O
policies	O
sarsa	B
converges	O
with	O
probability	O
to	O
an	O
optimal	O
policy	B
and	O
action-value	O
function	O
as	O
long	O
as	O
all	O
state	B
action	B
pairs	O
are	O
visited	O
an	O
infinite	O
number	O
of	O
times	O
and	O
the	O
policy	B
converges	O
in	O
the	O
limit	O
to	O
the	O
greedy	O
policy	B
can	O
be	O
arranged	O
for	O
example	O
with	O
policies	O
by	O
setting	O
chapter	O
temporal-difference	B
learning	I
sarsa	B
td	B
control	B
for	O
estimating	O
q	O
q	O
algorithm	O
parameters	O
step	O
size	O
small	O
initialize	O
qs	O
a	O
for	O
all	O
s	O
s	O
a	O
as	O
arbitrarily	O
except	O
that	O
qterminal	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
choose	O
a	O
from	O
s	O
using	O
policy	B
derived	O
from	O
q	O
loop	O
for	O
each	O
step	O
of	O
episode	O
take	O
action	B
a	O
observe	O
r	O
choose	O
from	O
using	O
policy	B
derived	O
from	O
q	O
qs	O
a	O
qs	O
a	O
qs	O
s	O
a	O
until	O
s	O
is	O
terminal	O
example	O
windy	B
gridworld	O
shown	O
inset	O
below	O
is	O
a	O
standard	O
gridworld	O
with	O
start	O
and	O
goal	O
states	O
but	O
with	O
one	O
difference	O
there	O
is	O
a	O
crosswind	O
running	O
upward	O
through	O
the	O
middle	O
of	O
the	O
grid	O
the	O
actions	O
are	O
the	O
standard	O
four	O
up	O
down	O
right	O
and	O
left	O
but	O
in	O
the	O
middle	O
region	O
the	O
resultant	O
next	O
states	O
are	O
shifted	O
upward	O
by	O
a	O
wind	O
the	O
strength	O
of	O
which	O
varies	O
from	O
column	O
to	O
column	O
the	O
strength	O
of	O
the	O
wind	O
is	O
given	O
below	O
each	O
column	O
in	O
number	O
of	O
cells	O
shifted	O
upward	O
for	O
example	O
if	O
you	O
are	O
one	O
cell	O
to	O
the	O
right	O
of	O
the	O
goal	O
then	O
the	O
action	B
left	O
takes	O
you	O
to	O
the	O
cell	O
just	O
above	O
the	O
goal	O
this	O
is	O
an	O
undiscounted	O
episodic	O
task	O
with	O
constant	O
rewards	O
of	O
until	O
the	O
goal	O
state	B
is	O
reached	O
the	O
graph	O
shows	O
the	O
results	O
of	O
applying	O
sarsa	B
to	O
this	O
task	O
with	O
and	O
the	O
initial	O
values	O
qs	O
a	O
for	O
all	O
s	O
a	O
the	O
increasing	O
slope	O
of	O
the	O
graph	O
shows	O
that	O
the	O
goal	O
was	O
reached	O
more	O
quickly	O
over	O
time	O
by	O
time	O
steps	O
the	O
greedy	O
policy	B
was	O
long	O
since	O
optimal	O
trajectory	O
from	O
it	O
is	O
shown	O
inset	O
continued	O
exploration	O
kept	O
the	O
average	O
episode	O
length	O
at	O
about	O
steps	O
two	O
more	O
than	O
the	O
minimum	O
of	O
note	O
that	O
monte	B
carlo	I
methods	I
cannot	O
easily	O
be	O
used	O
on	O
this	O
task	O
because	O
termination	O
is	O
not	O
guaranteed	O
for	O
all	O
policies	O
if	O
a	O
policy	B
was	O
ever	O
found	O
that	O
caused	O
the	O
agent	O
to	O
stay	O
in	O
the	O
same	O
state	B
then	O
the	O
next	O
episode	O
would	O
never	O
end	O
step-by-step	O
learning	O
methods	O
such	O
as	O
sarsa	B
do	O
not	O
have	O
this	O
problem	O
because	O
they	O
quickly	O
learn	O
during	O
the	O
episode	O
that	O
such	O
policies	O
are	O
poor	O
and	O
switch	O
to	O
something	O
else	O
exercise	O
windy	B
gridworld	O
with	O
king	O
s	O
moves	O
re-solve	O
the	O
windy	B
gridworld	O
assum	O
q-learning	B
off-policy	B
td	B
control	B
ing	O
eight	O
possible	O
actions	O
including	O
the	O
diagonal	O
moves	O
rather	O
than	O
the	O
usual	O
four	O
how	O
much	O
better	O
can	O
you	O
do	O
with	O
the	O
extra	O
actions	O
can	O
you	O
do	O
even	O
better	O
by	O
including	O
a	O
ninth	O
action	B
that	O
causes	O
no	O
movement	O
at	O
all	O
other	O
than	O
that	O
caused	O
by	O
the	O
wind	O
exercise	O
stochastic	O
wind	O
re-solve	O
the	O
windy	B
gridworld	O
task	O
with	O
king	O
s	O
moves	O
assuming	O
that	O
the	O
effect	O
of	O
the	O
wind	O
if	O
there	O
is	O
any	O
is	O
stochastic	O
sometimes	O
varying	O
by	O
from	O
the	O
mean	O
values	O
given	O
for	O
each	O
column	O
that	O
is	O
a	O
third	O
of	O
the	O
time	O
you	O
move	O
exactly	O
according	O
to	O
these	O
values	O
as	O
in	O
the	O
previous	O
exercise	O
but	O
also	O
a	O
third	O
of	O
the	O
time	O
you	O
move	O
one	O
cell	O
above	O
that	O
and	O
another	O
third	O
of	O
the	O
time	O
you	O
move	O
one	O
cell	O
below	O
that	O
for	O
example	O
if	O
you	O
are	O
one	O
cell	O
to	O
the	O
right	O
of	O
the	O
goal	O
and	O
you	O
move	O
left	O
then	O
one-third	O
of	O
the	O
time	O
you	O
move	O
one	O
cell	O
above	O
the	O
goal	O
one-third	O
of	O
the	O
time	O
you	O
move	O
two	O
cells	O
above	O
the	O
goal	O
and	O
one-third	O
of	O
the	O
time	O
you	O
move	O
to	O
the	O
goal	O
q-learning	B
off-policy	B
td	B
control	B
one	O
of	O
the	O
early	O
breakthroughs	O
in	O
reinforcement	B
learning	I
was	O
the	O
development	O
of	O
an	O
off-policy	B
td	B
control	B
algorithm	O
known	O
as	O
q-learning	B
defined	O
by	O
qst	O
at	O
qst	O
at	O
max	O
a	O
a	O
qst	O
in	O
this	O
case	O
the	O
learned	O
action-value	O
function	O
q	O
directly	O
approximates	O
q	O
the	O
optimal	O
action-value	O
function	O
independent	O
of	O
the	O
policy	B
being	O
followed	O
this	O
dramatically	O
simplifies	O
the	O
analysis	O
of	O
the	O
algorithm	O
and	O
enabled	O
early	O
convergence	O
proofs	O
the	O
policy	B
still	O
has	O
an	O
effect	O
in	O
that	O
it	O
determines	O
which	O
state	B
action	B
pairs	O
are	O
visited	O
and	O
updated	O
however	O
all	O
that	O
is	O
required	O
for	O
correct	O
convergence	O
is	O
that	O
all	O
pairs	O
continue	O
to	O
be	O
updated	O
as	O
we	O
observed	O
in	O
chapter	O
this	O
is	O
a	O
minimal	O
requirement	O
in	O
the	O
sense	O
that	O
any	O
method	O
guaranteed	O
to	O
find	O
optimal	O
behavior	O
in	O
the	O
general	O
case	O
must	O
require	O
it	O
under	O
this	O
assumption	O
and	O
a	O
variant	O
of	O
the	O
usual	O
stochastic	O
approximation	O
conditions	O
on	O
the	O
sequence	O
of	O
step-size	O
parameters	O
q	O
has	O
been	O
shown	O
to	O
converge	O
with	O
probability	O
to	O
q	O
the	O
q-learning	B
algorithm	O
is	O
shown	O
below	O
in	O
procedural	O
form	O
q-learning	B
td	B
control	B
for	O
estimating	O
algorithm	O
parameters	O
step	O
size	O
small	O
initialize	O
qs	O
a	O
for	O
all	O
s	O
s	O
a	O
as	O
arbitrarily	O
except	O
that	O
qterminal	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
choose	O
a	O
from	O
s	O
using	O
policy	B
derived	O
from	O
q	O
take	O
action	B
a	O
observe	O
r	O
qs	O
a	O
qs	O
a	O
maxa	O
a	O
qs	O
s	O
until	O
s	O
is	O
terminal	O
chapter	O
temporal-difference	B
learning	I
what	O
is	O
the	O
backup	B
diagram	I
for	B
q-learning	B
the	O
rule	O
updates	O
a	O
state	B
action	B
pair	O
so	O
the	O
top	O
node	O
the	O
root	O
of	O
the	O
update	O
must	O
be	O
a	O
small	O
filled	O
action	B
node	O
the	O
update	O
is	O
also	O
from	O
action	B
nodes	O
maximizing	O
over	O
all	O
those	O
actions	O
possible	O
in	O
the	O
next	O
state	B
thus	O
the	O
bottom	O
nodes	O
of	O
the	O
backup	B
diagram	I
should	O
be	O
all	O
these	O
action	B
nodes	O
finally	O
remember	O
that	O
we	O
indicate	O
taking	O
the	O
maximum	O
of	O
these	O
next	O
action	B
nodes	O
with	O
an	O
arc	O
across	O
them	O
can	O
you	O
guess	O
now	O
what	O
the	O
diagram	O
is	O
if	O
so	O
please	O
do	O
make	O
a	O
guess	O
before	O
turning	O
to	O
the	O
answer	O
in	O
figure	O
on	O
page	O
example	O
cliff	B
walking	I
this	O
gridworld	O
example	O
compares	O
sarsa	B
and	O
q-learning	B
highlighting	O
the	O
difference	O
between	O
on-policy	O
and	O
off-policy	B
methods	I
consider	O
the	O
gridworld	O
shown	O
to	O
the	O
right	O
this	O
is	O
a	O
standard	O
undiscounted	O
episodic	O
task	O
with	O
start	O
and	O
goal	O
states	O
and	O
the	O
usual	O
actions	O
causing	O
movement	O
up	O
down	O
right	O
and	O
left	O
reward	O
is	O
on	O
all	O
transitions	O
except	O
those	O
into	O
the	O
region	O
marked	O
the	O
cliff	O
stepping	O
into	O
this	O
region	O
incurs	O
a	O
reward	O
of	O
and	O
sends	O
the	O
agent	O
instantly	O
back	O
to	O
the	O
start	O
the	O
graph	O
to	O
the	O
right	O
shows	O
the	O
performance	O
of	O
the	O
sarsa	B
and	O
qlearning	O
methods	O
with	O
action	B
selection	O
after	O
an	O
initial	O
transient	O
q-learning	B
learns	O
values	O
for	O
the	O
optimal	O
policy	B
that	O
which	O
travels	O
right	O
along	O
the	O
edge	O
of	O
the	O
cliff	O
unfortunately	O
this	O
results	O
in	O
its	O
occasionally	O
falling	O
off	O
the	O
cliff	O
because	O
of	O
the	O
action	B
selection	O
sarsa	B
on	O
the	O
other	O
hand	O
takes	O
the	O
action	B
selection	O
into	O
account	O
and	O
learns	O
the	O
longer	O
but	O
safer	O
path	O
through	O
the	O
upper	O
part	O
of	O
the	O
grid	O
although	O
q-learning	B
actually	O
learns	O
the	O
values	O
of	O
the	O
optimal	O
policy	B
its	O
online	B
performance	O
is	O
worse	O
than	O
that	O
of	O
sarsa	B
which	O
learns	O
the	O
roundabout	O
policy	B
of	O
course	O
if	O
were	O
gradually	O
reduced	O
then	O
both	O
methods	O
would	O
asymptotically	O
converge	O
to	O
the	O
optimal	O
policy	B
exercise	O
why	O
is	O
q-learning	B
considered	O
an	O
off-policy	B
control	B
method	O
exercise	O
suppose	O
action	B
selection	O
is	O
greedy	O
is	O
q-learning	B
then	O
exactly	O
the	O
same	O
algorithm	O
as	O
sarsa	B
will	O
they	O
make	O
exactly	O
the	O
same	O
action	B
selections	O
and	O
weight	O
updates	O
rewardperepsiode	O
episodessarsaq-learningsg	O
the	O
cliff	O
rsum	O
of	O
rewardsduringepisoder	O
pathoptimal	O
pathr	O
the	O
cliffr	O
of	O
rewardsduringepisoder	O
pathoptimal	O
pathr	O
expected	B
sarsa	B
expected	B
sarsa	B
consider	O
the	O
learning	O
algorithm	O
that	O
is	O
just	O
like	O
q-learning	B
except	O
that	O
instead	O
of	O
the	O
maximum	O
over	O
next	O
state	B
action	B
pairs	O
it	O
uses	O
the	O
expected	B
value	B
taking	O
into	O
account	O
how	O
likely	O
each	O
action	B
is	O
under	O
the	O
current	O
policy	B
that	O
is	O
consider	O
the	O
algorithm	O
with	O
the	O
update	O
rule	O
qst	O
at	O
qst	O
at	O
qst	O
a	O
qst	O
qst	O
at	O
but	O
that	O
otherwise	O
follows	O
the	O
schema	O
of	O
q-learning	B
given	O
the	O
next	O
state	B
this	O
algorithm	O
moves	O
deterministically	O
in	O
the	O
same	O
direction	O
as	O
sarsa	B
moves	O
in	O
expectation	O
and	O
accordingly	O
it	O
is	O
called	O
expected	B
sarsa	B
its	O
backup	B
diagram	I
is	O
shown	O
on	O
the	O
right	O
in	O
figure	O
expected	B
sarsa	B
is	O
more	O
complex	O
computationally	O
than	O
sarsa	B
but	O
in	O
return	B
it	O
eliminates	O
the	O
variance	O
due	O
to	O
the	O
random	O
selection	O
of	O
given	O
the	O
same	O
amount	O
of	O
experience	O
we	O
might	O
expect	O
it	O
to	O
perform	O
slightly	O
better	O
than	O
sarsa	B
and	O
indeed	O
it	O
generally	O
does	O
figure	O
shows	O
summary	O
results	O
on	O
the	O
cliff-walking	O
task	O
with	O
expected	B
sarsa	B
compared	O
to	O
sarsa	B
and	O
q-learning	B
expected	B
sarsa	B
retains	O
the	O
significant	O
advantage	O
of	O
sarsa	B
over	O
q-learning	B
on	O
this	O
problem	O
in	O
addition	O
expected	B
sarsa	B
shows	O
a	O
figure	O
interim	O
and	O
asymptotic	O
performance	O
of	O
td	B
control	B
methods	O
on	O
the	O
cliff-walking	O
task	O
as	O
a	O
function	O
of	O
all	O
algorithms	O
used	O
an	O
policy	B
with	O
asymptotic	O
performance	O
is	O
an	O
average	O
over	O
episodes	B
whereas	O
interim	O
performance	O
is	O
an	O
average	O
over	O
the	O
first	O
episodes	B
these	O
data	O
are	O
averages	O
of	O
over	O
and	O
runs	O
for	O
the	O
interim	O
and	O
asymptotic	O
cases	O
respectively	O
the	O
solid	O
circles	O
mark	O
the	O
best	O
interim	O
performance	O
of	O
each	O
method	O
adapted	O
from	O
van	O
seijen	O
et	O
al	O
usingan	O
valuesincaseofexpectedsarsaandq-learning	O
thisindicatesthatthealgorithmshaveconvergedlongbeforetheendoftherunforall	O
valuessincewedonotseeanyeffectoftheinitiallearningphase	O
forsarsatheperformancecomesclosetotheperformanceofexpectedsarsaonlyfor	O
return	B
n	O
sarsan	O
q	O
learningn	O
expected	B
sarsan	O
sarsan	O
q	O
learningn	O
expected	B
up	O
up	O
directioncorrespondingwiththewindstrength	O
forexamplewhentheagentisinthesquarerightofthegoalandtakesa	O
left	O
sarsasarsaq-learningasymptotic	O
performanceinterim	O
performanceq-learningsum	O
of	O
rewardsper	O
episode	O
chapter	O
temporal-difference	B
learning	I
q-learning	B
expected	B
sarsa	B
figure	O
the	O
backup	O
diagrams	O
for	B
q-learning	B
and	O
expected	B
sarsa	B
significant	O
improvement	O
over	O
sarsa	B
over	O
a	O
wide	O
range	O
of	O
values	O
for	O
the	O
step-size	B
parameter	I
in	O
cliff	B
walking	I
the	O
state	B
transitions	O
are	O
all	O
deterministic	O
and	O
all	O
randomness	O
comes	O
from	O
the	O
policy	B
in	O
such	O
cases	O
expected	B
sarsa	B
can	O
safely	O
set	O
without	O
suffering	O
any	O
degradation	O
of	O
asymptotic	O
performance	O
whereas	O
sarsa	B
can	O
only	O
perform	O
well	O
in	O
the	O
long	O
run	O
at	O
a	O
small	O
value	B
of	O
at	O
which	O
short-term	O
performance	O
is	O
poor	O
in	O
this	O
and	O
other	O
examples	O
there	O
is	O
a	O
consistent	O
empirical	O
advantage	O
of	O
expected	B
sarsa	B
over	O
sarsa	B
in	O
these	O
cliff	B
walking	I
results	O
expected	B
sarsa	B
was	O
used	O
on-policy	O
but	O
in	O
general	O
it	O
might	O
use	O
a	O
policy	B
different	O
from	O
the	O
target	O
policy	B
to	O
generate	O
behavior	O
in	O
which	O
case	O
it	O
becomes	O
an	O
off-policy	B
algorithm	O
for	O
example	O
suppose	O
is	O
the	O
greedy	O
policy	B
while	O
behavior	O
is	O
more	O
exploratory	O
then	O
expected	B
sarsa	B
is	O
exactly	O
q-learning	B
in	O
this	O
sense	O
expected	B
sarsa	B
subsumes	O
and	O
generalizes	O
q-learning	B
while	O
reliably	O
improving	O
over	O
sarsa	B
except	O
for	O
the	O
small	O
additional	O
computational	O
cost	O
expected	B
sarsa	B
may	O
completely	O
dominate	O
both	O
of	O
the	O
other	O
more-well-known	O
td	B
control	B
algorithms	O
maximization	B
bias	I
and	O
double	B
learning	I
all	O
the	O
control	B
algorithms	O
that	O
we	O
have	O
discussed	O
so	O
far	O
involve	O
maximization	O
in	O
the	O
construction	O
of	O
their	O
target	O
policies	O
for	O
example	O
in	O
q-learning	B
the	O
target	O
policy	B
is	O
the	O
greedy	O
policy	B
given	O
the	O
current	O
action	B
values	O
which	O
is	O
defined	O
with	O
a	O
max	O
and	O
in	O
sarsa	B
the	O
policy	B
is	O
often	O
which	O
also	O
involves	O
a	O
maximization	O
operation	O
in	O
these	O
algorithms	O
a	O
maximum	O
over	O
estimated	O
values	O
is	O
used	O
implicitly	O
as	O
an	O
estimate	O
of	O
the	O
maximum	O
value	B
which	O
can	O
lead	O
to	O
a	O
significant	O
positive	O
bias	O
to	O
see	O
why	O
consider	O
a	O
single	O
state	B
s	O
where	O
there	O
are	O
many	O
actions	O
a	O
whose	O
true	O
values	O
qs	O
a	O
are	O
all	O
zero	O
but	O
whose	O
estimated	O
values	O
qs	O
a	O
are	O
uncertain	O
and	O
thus	O
distributed	O
some	O
above	O
and	O
some	O
below	O
zero	O
the	O
maximum	O
of	O
the	O
true	O
values	O
is	O
zero	O
but	O
the	O
maximum	O
of	O
the	O
estimates	O
is	O
positive	O
a	O
positive	O
bias	O
we	O
call	O
this	O
maximization	B
bias	I
example	O
maximization	B
bias	I
example	O
the	O
small	O
mdp	O
shown	O
inset	O
in	O
figure	O
provides	O
a	O
simple	O
example	O
of	O
how	O
maximization	B
bias	I
can	O
harm	O
the	O
performance	O
of	O
td	B
control	B
algorithms	O
the	O
mdp	O
has	O
two	O
non-terminal	O
states	O
a	O
and	O
b	O
episodes	B
always	O
start	O
in	O
a	O
with	O
a	O
choice	O
between	O
two	O
actions	O
left	O
and	O
right	O
the	O
right	O
action	B
transitions	O
immediately	O
to	O
the	O
terminal	O
state	B
with	O
a	O
reward	O
and	O
return	B
of	O
zero	O
the	O
left	O
action	B
transitions	O
to	O
b	O
also	O
with	O
a	O
reward	O
of	O
zero	O
from	O
which	O
there	O
are	O
many	O
possible	O
actions	O
all	O
of	O
which	O
cause	O
immediate	O
termination	O
with	O
a	O
reward	O
drawn	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
and	O
variance	O
thus	O
the	O
expected	B
return	B
for	O
any	O
trajectory	O
starting	O
with	O
left	O
is	O
and	O
thus	O
taking	O
left	O
in	O
state	B
a	O
is	O
always	O
a	O
mistake	O
nevertheless	O
our	O
maximization	B
bias	I
and	O
double	B
learning	I
figure	O
comparison	O
of	O
q-learning	B
and	O
double	B
q-learning	B
on	O
a	O
simple	O
episodic	O
mdp	O
inset	O
q-learning	B
initially	O
learns	O
to	O
take	O
the	O
left	O
action	B
much	O
more	O
often	O
than	O
the	O
right	O
action	B
and	O
always	O
takes	O
it	O
significantly	O
more	O
often	O
than	O
the	O
minimum	O
probability	O
enforced	O
by	O
action	B
selection	O
with	O
in	O
contrast	O
double	B
q-learning	B
is	O
essentially	O
unaffected	O
by	O
maximization	B
bias	I
these	O
data	O
are	O
averaged	O
over	O
runs	O
the	O
initial	O
actionvalue	O
estimates	O
were	O
zero	O
any	O
ties	O
in	O
action	B
selection	O
were	O
broken	O
randomly	O
control	B
methods	O
may	O
favor	O
left	O
because	O
of	O
maximization	B
bias	I
making	O
b	O
appear	O
to	O
have	O
a	O
positive	O
value	B
figure	O
shows	O
that	O
q-learning	B
with	O
action	B
selection	O
initially	O
learns	O
to	O
strongly	O
favor	O
the	O
left	O
action	B
on	O
this	O
example	O
even	O
at	O
asymptote	O
q-learning	B
takes	O
the	O
left	O
action	B
about	O
more	O
often	O
than	O
is	O
optimal	O
at	O
our	O
parameter	O
settings	O
and	O
are	O
there	O
algorithms	O
that	O
avoid	O
maximization	B
bias	I
to	O
start	O
consider	O
a	O
bandit	O
case	O
in	O
which	O
we	O
have	O
noisy	O
estimates	O
of	O
the	O
value	B
of	O
each	O
of	O
many	O
actions	O
obtained	O
as	O
sample	O
averages	O
of	O
the	O
rewards	O
received	O
on	O
all	O
the	O
plays	O
with	O
each	O
action	B
as	O
we	O
discussed	O
above	O
there	O
will	O
be	O
a	O
positive	O
maximization	B
bias	I
if	O
we	O
use	O
the	O
maximum	O
of	O
the	O
estimates	O
as	O
an	O
estimate	O
of	O
the	O
maximum	O
of	O
the	O
true	O
values	O
one	O
way	O
to	O
view	O
the	O
problem	O
is	O
that	O
it	O
is	O
due	O
to	O
using	O
the	O
same	O
samples	O
both	O
to	O
determine	O
the	O
maximizing	O
action	B
and	O
to	O
estimate	O
its	O
value	B
suppose	O
we	O
divided	O
the	O
plays	O
in	O
two	O
sets	O
and	O
used	O
them	O
to	O
learn	O
two	O
independent	O
estimates	O
call	O
them	O
and	O
each	O
an	O
estimate	O
of	O
the	O
true	O
value	B
qa	O
for	O
all	O
a	O
a	O
we	O
could	O
then	O
use	O
one	O
estimate	O
say	O
to	O
determine	O
the	O
maximizing	O
action	B
a	O
argmaxa	O
and	O
the	O
other	O
to	O
provide	O
the	O
estimate	O
of	O
its	O
value	B
this	O
estimate	O
will	O
then	O
be	O
unbiased	O
in	O
the	O
sense	O
that	O
qa	O
we	O
can	O
also	O
repeat	O
the	O
process	O
with	O
the	O
role	O
of	O
the	O
two	O
estimates	O
reversed	O
to	O
yield	O
a	O
second	O
unbiased	O
estimate	O
this	O
is	O
the	O
idea	O
of	O
double	B
learning	I
note	O
that	O
although	O
we	O
learn	O
two	O
estimates	O
only	O
one	O
estimate	O
is	O
updated	O
on	O
each	O
play	O
double	B
learning	I
doubles	O
the	O
memory	O
requirements	O
but	O
does	O
not	O
increase	O
the	O
amount	O
of	O
computation	O
per	O
step	O
the	O
idea	O
of	O
double	B
learning	I
extends	O
naturally	O
to	O
algorithms	O
for	O
full	O
mdps	O
for	O
example	O
the	O
double	B
learning	I
algorithm	O
analogous	O
to	O
q-learning	B
called	O
double	B
q-learning	B
divides	O
the	O
time	O
steps	O
in	O
two	O
perhaps	O
by	O
flipping	O
a	O
coin	O
on	O
each	O
step	O
if	O
the	O
coin	O
comes	O
leftactionsfrom	O
chapter	O
temporal-difference	B
learning	I
up	O
heads	O
the	O
update	O
is	O
at	O
at	O
argmax	O
a	O
if	O
the	O
coin	O
comes	O
up	O
tails	O
then	O
the	O
same	O
update	O
is	O
done	O
with	O
and	O
switched	O
so	O
that	O
is	O
updated	O
the	O
two	O
approximate	O
value	B
functions	O
are	O
treated	O
completely	O
symmetrically	O
the	O
behavior	O
policy	B
can	O
use	O
both	O
action-value	O
estimates	O
for	O
example	O
an	O
policy	B
for	O
double	B
q-learning	B
could	O
be	O
based	O
on	O
the	O
average	O
sum	O
of	O
the	O
two	O
action-value	O
estimates	O
a	O
complete	O
algorithm	O
for	O
double	B
q-learning	B
is	O
given	O
in	O
the	O
box	O
below	O
this	O
is	O
the	O
algorithm	O
used	O
to	O
produce	O
the	O
results	O
in	O
figure	O
in	O
that	O
example	O
double	B
learning	I
seems	O
to	O
eliminate	O
the	O
harm	O
caused	O
by	O
maximization	B
bias	I
of	O
course	O
there	O
are	O
also	O
double	B
versions	O
of	O
sarsa	B
and	O
expected	B
sarsa	B
double	B
q-learning	B
for	O
estimating	O
q	O
algorithm	O
parameters	O
step	O
size	O
small	O
initialize	O
a	O
and	O
a	O
for	O
all	O
s	O
s	O
a	O
as	O
such	O
that	O
qterminal	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
choose	O
a	O
from	O
s	O
using	O
the	O
policy	B
in	O
take	O
action	B
a	O
observe	O
r	O
with	O
probabilility	O
else	O
a	O
a	O
argmaxa	O
a	O
a	O
argmaxa	O
s	O
until	O
s	O
is	O
terminal	O
exercise	O
what	O
are	O
the	O
update	O
equations	O
for	O
double	B
expected	B
sarsa	B
with	O
an	O
greedy	O
target	O
policy	B
games	O
afterstates	B
and	O
other	O
special	O
cases	O
in	O
this	O
book	O
we	O
try	O
to	O
present	O
a	O
uniform	O
approach	O
to	O
a	O
wide	O
class	O
of	O
tasks	O
but	O
of	O
course	O
there	O
are	O
always	O
exceptional	O
tasks	O
that	O
are	O
better	O
treated	O
in	O
a	O
specialized	O
way	O
for	O
example	O
our	O
general	O
approach	O
involves	O
learning	O
an	O
action-value	O
function	O
but	O
in	O
chapter	O
we	O
presented	O
a	O
td	B
method	O
for	O
learning	O
to	O
play	O
tic-tac-toe	B
that	O
learned	O
something	O
much	O
more	O
like	O
a	O
state-value	O
function	O
if	O
we	O
look	O
closely	O
at	O
that	O
example	O
it	O
becomes	O
apparent	O
that	O
the	O
function	O
learned	O
there	O
is	O
neither	O
an	O
action-value	O
function	O
nor	O
a	O
state-value	O
function	O
in	O
the	O
usual	O
sense	O
a	O
conventional	O
state-value	O
function	O
evaluates	O
states	O
in	O
which	O
the	O
agent	O
has	O
the	O
option	O
of	O
selecting	O
an	O
action	B
but	O
the	O
state-value	O
function	O
used	O
in	O
tictac-toe	O
evaluates	O
board	O
positions	O
after	O
the	O
agent	O
has	O
made	O
its	O
move	O
let	O
us	O
call	O
these	O
summary	O
afterstates	B
and	O
value	B
functions	O
over	O
these	O
afterstate	O
value	B
functions	O
afterstates	B
are	O
useful	O
when	O
we	O
have	O
knowledge	O
of	O
an	O
initial	O
part	O
of	O
the	O
environment	B
s	O
dynamics	O
but	O
not	O
necessarily	O
of	O
the	O
full	O
dynamics	O
for	O
example	O
in	O
games	O
we	O
typically	O
know	O
the	O
immediate	O
effects	O
of	O
our	O
moves	O
we	O
know	O
for	O
each	O
possible	O
chess	B
move	O
what	O
the	O
resulting	O
position	O
will	O
be	O
but	O
not	O
how	O
our	O
opponent	O
will	O
reply	O
afterstate	O
value	B
functions	O
are	O
a	O
natural	O
way	O
to	O
take	O
advantage	O
of	O
this	O
kind	O
of	O
knowledge	O
and	O
thereby	O
produce	O
a	O
more	O
efficient	O
learning	O
method	O
the	O
reason	O
it	O
is	O
more	O
efficient	O
to	O
design	O
algorithms	O
in	O
terms	O
of	O
afterstates	B
is	O
apparent	O
from	O
the	O
tic-tac-toe	B
example	O
a	O
conventional	O
action-value	O
function	O
would	O
map	O
from	O
positions	O
and	O
moves	O
to	O
an	O
estimate	O
of	O
the	O
value	B
but	O
many	O
position	O
move	O
pairs	O
produce	O
the	O
same	O
resulting	O
position	O
as	O
in	O
the	O
example	O
shown	O
to	O
the	O
right	O
in	O
such	O
cases	O
the	O
position	O
move	O
pairs	O
are	O
different	O
but	O
produce	O
the	O
same	O
afterposition	O
and	O
thus	O
must	O
have	O
the	O
same	O
value	B
a	O
conventional	O
action-value	O
function	O
would	O
have	O
to	O
separately	O
assess	O
both	O
pairs	O
whereas	O
an	O
afterstate	O
value	B
function	I
would	O
immediately	O
assess	O
both	O
equally	O
any	O
learning	O
about	O
the	O
position	O
move	O
pair	O
on	O
the	O
left	O
would	O
immediately	O
transfer	O
to	O
the	O
pair	O
on	O
the	O
right	O
afterstates	B
arise	O
in	O
many	O
tasks	O
not	O
just	O
games	O
for	O
example	O
in	O
queuing	O
tasks	O
there	O
are	O
actions	O
such	O
as	O
assigning	O
customers	O
to	O
servers	O
rejecting	O
customers	O
or	O
discarding	O
information	O
in	O
such	O
cases	O
the	O
actions	O
are	O
in	O
fact	O
defined	O
in	O
terms	O
of	O
their	O
immediate	O
effects	O
which	O
are	O
completely	O
known	O
it	O
is	O
impossible	O
to	O
describe	O
all	O
the	O
possible	O
kinds	O
of	O
specialized	O
problems	O
and	O
corresponding	O
specialized	O
learning	O
algorithms	O
however	O
the	O
principles	O
developed	O
in	O
this	O
book	O
should	O
apply	O
widely	O
for	O
example	O
afterstate	O
methods	O
are	O
still	O
aptly	O
described	O
in	O
terms	O
of	O
generalized	O
policy	B
iteration	I
with	O
a	O
policy	B
and	O
value	B
function	I
interacting	O
in	O
essentially	O
the	O
same	O
way	O
in	O
many	O
cases	O
one	O
will	O
still	O
face	O
the	O
choice	O
between	O
on-policy	O
and	O
off-policy	B
methods	I
for	O
managing	O
the	O
need	O
for	O
persistent	O
exploration	O
exercise	O
describe	O
how	O
the	O
task	O
of	O
jack	O
s	O
car	O
rental	O
could	O
be	O
reformulated	O
in	O
terms	O
of	O
afterstates	B
why	O
in	O
terms	O
of	O
this	O
specific	O
task	O
would	O
such	O
a	O
reformulation	O
be	O
likely	O
to	O
speed	O
convergence	O
summary	O
in	O
this	O
chapter	O
we	O
introduced	O
a	O
new	O
kind	O
of	O
learning	O
method	O
temporal-difference	B
learning	I
and	O
showed	O
how	O
it	O
can	O
be	O
applied	O
to	O
the	O
reinforcement	B
learning	I
problem	O
as	O
usual	O
we	O
divided	O
the	O
overall	O
problem	O
into	O
a	O
prediction	B
problem	O
and	O
a	O
control	B
problem	O
td	B
methods	O
are	O
alternatives	O
to	O
monte	B
carlo	I
methods	I
for	O
solving	O
the	O
prediction	B
problem	O
in	O
both	O
cases	O
the	O
extension	O
to	O
the	O
control	B
problem	O
is	O
via	O
the	O
idea	O
of	O
generalized	O
policy	B
iteration	I
that	O
we	O
abstracted	O
from	O
dynamic	B
programming	I
this	O
is	O
the	O
idea	O
that	O
xoxxoxoxx	O
chapter	O
temporal-difference	B
learning	I
approximate	O
policy	B
and	O
value	B
functions	O
should	O
interact	O
in	O
such	O
a	O
way	O
that	O
they	O
both	O
move	O
toward	O
their	O
optimal	O
values	O
one	O
of	O
the	O
two	O
processes	O
making	O
up	O
gpi	O
drives	O
the	O
value	B
function	I
to	O
accurately	O
predict	O
returns	O
for	O
the	O
current	O
policy	B
this	O
is	O
the	O
prediction	B
problem	O
the	O
other	O
process	O
drives	O
the	O
policy	B
to	O
improve	O
locally	O
to	O
be	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
when	O
the	O
first	O
process	O
is	O
based	O
on	O
experience	O
a	O
complication	O
arises	O
concerning	O
maintaining	O
sufficient	O
exploration	O
we	O
can	O
classify	O
td	B
control	B
methods	O
according	O
to	O
whether	O
they	O
deal	O
with	O
this	O
complication	O
by	O
using	O
an	O
on-policy	O
or	O
off-policy	B
approach	O
sarsa	B
is	O
an	O
on-policy	O
method	O
and	O
q-learning	B
is	O
an	O
off-policy	B
method	O
expected	B
sarsa	B
is	O
also	O
an	O
off-policy	B
method	O
as	O
we	O
present	O
it	O
here	O
there	O
is	O
a	O
third	O
way	O
in	O
which	O
td	B
methods	O
can	O
be	O
extended	O
to	O
control	B
which	O
we	O
did	O
not	O
include	O
in	O
this	O
chapter	O
called	O
actor	O
critic	O
methods	O
these	O
methods	O
are	O
covered	O
in	O
full	O
in	O
chapter	O
the	O
methods	O
presented	O
in	O
this	O
chapter	O
are	O
today	O
the	O
most	O
widely	O
used	O
reinforcement	B
learning	I
methods	O
this	O
is	O
probably	O
due	O
to	O
their	O
great	O
simplicity	O
they	O
can	O
be	O
applied	O
online	B
with	O
a	O
minimal	O
amount	O
of	O
computation	O
to	O
experience	O
generated	O
from	O
interaction	O
with	O
an	O
environment	B
they	O
can	O
be	O
expressed	O
nearly	O
completely	O
by	O
single	O
equations	O
that	O
can	O
be	O
implemented	O
with	O
small	O
computer	O
programs	O
in	O
the	O
next	O
few	O
chapters	O
we	O
extend	O
these	O
algorithms	O
making	O
them	O
slightly	O
more	O
complicated	O
and	O
significantly	O
more	O
powerful	O
all	O
the	O
new	O
algorithms	O
will	O
retain	O
the	O
essence	O
of	O
those	O
introduced	O
here	O
they	O
will	O
be	O
able	O
to	O
process	O
experience	O
online	B
with	O
relatively	O
little	O
computation	O
and	O
they	O
will	O
be	O
driven	O
by	O
td	B
errors	O
the	O
special	O
cases	O
of	O
td	B
methods	O
introduced	O
in	O
the	O
present	O
chapter	O
should	O
rightly	O
be	O
called	O
one-step	O
tabular	O
model-free	O
td	B
methods	O
in	O
the	O
next	O
two	O
chapters	O
we	O
extend	O
them	O
to	O
n-step	B
forms	O
link	O
to	O
monte	B
carlo	I
methods	I
and	O
forms	O
that	O
include	O
a	O
model	B
of	I
the	I
environment	B
link	O
to	O
planning	B
and	B
dynamic	B
programming	I
then	O
in	O
the	O
second	O
part	O
of	O
the	O
book	O
we	O
extend	O
them	O
to	O
various	O
forms	O
of	O
function	B
approximation	I
rather	O
than	O
tables	O
link	O
to	O
deep	B
learning	I
and	O
artificial	B
neural	B
networks	I
finally	O
in	O
this	O
chapter	O
we	O
have	O
discussed	O
td	B
methods	O
entirely	O
within	O
the	O
context	O
of	O
reinforcement	B
learning	I
problems	O
but	O
td	B
methods	O
are	O
actually	O
more	O
general	O
than	O
this	O
they	O
are	O
general	O
methods	O
for	O
learning	O
to	O
make	O
long-term	O
predictions	O
about	O
dynamical	O
systems	O
for	O
example	O
td	B
methods	O
may	O
be	O
relevant	O
to	O
predicting	O
financial	O
data	O
life	O
spans	O
election	O
outcomes	O
weather	O
patterns	O
animal	O
behavior	O
demands	O
on	O
power	O
stations	O
or	O
customer	O
purchases	O
it	O
was	O
only	O
when	O
td	B
methods	O
were	O
analyzed	O
as	O
pure	O
prediction	B
methods	O
independent	O
of	O
their	O
use	O
in	O
reinforcement	B
learning	I
that	O
their	O
theoretical	O
properties	O
first	O
came	O
to	O
be	O
well	O
understood	O
even	O
so	O
these	O
other	O
potential	O
applications	O
of	O
td	B
learning	O
methods	O
have	O
not	O
yet	O
been	O
extensively	O
explored	O
summary	O
bibliographical	O
and	O
historical	O
remarks	O
as	O
we	O
outlined	O
in	O
chapter	O
the	O
idea	O
of	O
td	B
learning	O
has	O
its	O
early	O
roots	O
in	B
animal	I
learning	I
psychology	B
and	B
artificial	B
intelligence	I
most	O
notably	O
the	O
work	O
of	O
samuel	O
and	O
klopf	B
samuel	O
s	O
work	O
is	O
described	O
as	O
a	O
case	O
study	O
in	O
section	O
also	O
related	O
to	O
td	B
learning	O
are	O
holland	B
s	O
early	O
ideas	O
about	O
consistency	O
among	O
value	B
predictions	O
these	O
influenced	O
one	O
of	O
the	O
authors	O
who	O
was	O
a	O
graduate	O
student	O
from	O
to	O
at	O
the	O
university	O
of	O
michigan	O
where	O
holland	B
was	O
teaching	O
holland	B
s	O
ideas	O
led	O
to	O
a	O
number	O
of	O
td-related	O
systems	O
including	O
the	O
work	O
of	O
booker	O
and	O
the	O
bucket	O
brigade	O
of	O
holland	B
which	O
is	O
related	O
to	O
sarsa	B
as	O
discussed	O
below	O
most	O
of	O
the	O
specific	O
material	O
from	O
these	O
sections	O
is	O
from	O
sutton	O
including	O
the	O
algorithm	O
the	O
random	B
walk	I
example	O
and	O
the	O
term	O
temporaldifference	O
learning	O
the	O
characterization	O
of	O
the	O
relationship	O
to	O
dynamic	B
programming	I
and	B
monte	B
carlo	I
methods	I
was	O
influenced	O
by	O
watkins	B
werbos	B
and	O
others	O
the	O
use	O
of	O
backup	O
diagrams	O
was	O
new	O
to	O
the	O
first	O
edition	O
of	O
this	O
book	O
tabular	O
was	O
proved	O
to	O
converge	O
in	O
the	O
mean	O
by	O
sutton	O
and	O
with	O
probability	O
by	O
dayan	O
based	O
on	O
the	O
work	O
of	O
watkins	B
and	O
dayan	O
these	O
results	O
were	O
extended	O
and	O
strengthened	O
by	O
jaakkola	O
jordan	O
and	O
singh	O
and	O
tsitsiklis	O
by	O
using	O
extensions	O
of	O
the	O
powerful	O
existing	O
theory	O
of	O
stochastic	O
approximation	O
other	O
extensions	O
and	O
generalizations	O
are	O
covered	O
in	O
later	O
chapters	O
the	O
optimality	B
of	I
the	O
td	B
algorithm	O
under	O
batch	O
training	O
was	O
established	O
by	O
sutton	O
illuminating	O
this	O
result	O
is	O
barnard	O
s	O
derivation	O
of	O
the	O
td	B
algorithm	O
as	O
a	O
combination	O
of	O
one	O
step	O
of	O
an	O
incremental	O
method	O
for	O
learning	O
a	O
model	O
of	O
the	O
markov	O
chain	O
and	O
one	O
step	O
of	O
a	O
method	O
for	O
computing	O
predictions	O
from	O
the	O
model	O
the	O
term	O
certainty	O
equivalence	O
is	O
from	O
the	O
adaptive	O
control	B
literature	O
goodwin	O
and	O
sin	O
the	O
sarsa	B
algorithm	O
was	O
introduced	O
by	O
rummery	O
and	O
niranjan	O
they	O
explored	O
it	O
in	O
conjunction	O
with	O
neural	B
networks	O
and	O
called	O
it	O
modified	O
connectionist	O
q-learning	B
the	O
name	O
sarsa	B
was	O
introduced	O
by	O
sutton	O
the	O
convergence	O
of	O
one-step	O
tabular	O
sarsa	B
form	O
treated	O
in	O
this	O
chapter	O
has	O
been	O
proved	O
by	O
singh	O
jaakkola	O
littman	O
and	O
szepesv	O
ari	O
the	O
windy	B
gridworld	O
example	O
was	O
suggested	O
by	O
tom	O
kalt	O
holland	B
s	O
bucket	O
brigade	O
idea	O
evolved	O
into	O
an	O
algorithm	O
closely	O
related	O
to	O
sarsa	B
the	O
original	O
idea	O
of	O
the	O
bucket	O
brigade	O
involved	O
chains	O
of	O
rules	O
triggering	O
each	O
other	O
it	O
focused	O
on	O
passing	O
credit	O
back	O
from	O
the	O
current	O
rule	O
to	O
the	O
rules	O
that	O
triggered	O
it	O
over	O
time	O
the	O
bucket	O
brigade	O
came	O
to	O
be	O
more	O
like	O
td	B
learning	O
in	O
passing	O
credit	O
back	O
to	O
any	O
temporally	O
preceding	O
rule	O
not	O
just	O
to	O
the	O
ones	O
that	O
triggered	O
the	O
current	O
rule	O
the	O
modern	O
form	O
of	O
the	O
bucket	O
brigade	O
when	O
simplified	O
in	O
various	O
natural	O
ways	O
is	O
nearly	O
identical	O
to	O
one-step	O
sarsa	B
as	O
detailed	O
by	O
wilson	O
chapter	O
temporal-difference	B
learning	I
q-learning	B
was	O
introduced	O
by	O
watkins	B
whose	O
outline	O
of	O
a	O
convergence	O
proof	B
was	O
made	O
rigorous	O
by	O
watkins	B
and	O
dayan	O
more	O
general	O
convergence	O
results	O
were	O
proved	O
by	O
jaakkola	O
jordan	O
and	O
singh	O
and	O
tsitsiklis	O
the	O
expected	B
sarsa	B
algorithm	O
was	O
introduced	O
by	O
george	O
john	O
who	O
called	O
it	O
q-learning	B
and	O
stressed	O
its	O
advantages	O
over	O
q-learning	B
as	O
an	O
offpolicy	O
algorithm	O
john	O
s	O
work	O
was	O
not	O
known	O
to	O
us	O
when	O
we	O
presented	O
expected	B
sarsa	B
in	O
the	O
first	O
edition	O
of	O
this	O
book	O
as	O
an	O
exercise	O
or	O
to	O
van	O
seijen	O
van	O
hasselt	O
whiteson	O
and	O
weiring	O
when	O
they	O
established	O
expected	B
sarsa	B
s	O
convergence	O
properties	O
and	O
conditions	O
under	O
which	O
it	O
will	O
outperform	O
regular	O
sarsa	B
and	O
q-learning	B
our	O
figure	O
is	O
adapted	O
from	O
their	O
results	O
van	O
seijen	O
et	O
al	O
defined	O
expected	B
sarsa	B
to	O
be	O
an	O
on-policy	O
method	O
exclusively	O
we	O
did	O
in	O
the	O
first	O
edition	O
whereas	O
now	O
we	O
use	O
this	O
name	O
for	O
the	O
general	O
algorithm	O
in	O
which	O
the	O
target	O
and	O
behavior	O
policies	O
may	O
differ	O
the	O
general	O
off-policy	B
view	O
of	O
expected	B
sarsa	B
was	O
noted	O
by	O
van	O
hasselt	O
who	O
called	O
it	O
general	O
q-learning	B
maximization	B
bias	I
and	O
double	B
learning	I
were	O
introduced	O
and	O
extensively	O
investigated	O
by	O
van	O
hasselt	O
the	O
example	O
mdp	O
in	O
figure	O
was	O
adapted	O
from	O
that	O
in	O
his	O
figure	O
hasselt	O
the	O
notion	O
of	O
an	O
afterstate	O
is	O
the	O
same	O
as	O
that	O
of	O
a	O
post-decision	O
state	B
roy	O
bertsekas	O
lee	O
and	O
tsitsiklis	O
powell	O
chapter	O
n-step	B
bootstrapping	B
in	O
this	O
chapter	O
we	O
unify	O
the	O
monte	B
carlo	I
methods	I
and	O
the	O
one-step	O
temporaldifference	O
methods	O
presented	O
in	O
the	O
previous	O
two	O
chapters	O
neither	O
mc	O
methods	O
nor	O
one-step	O
td	B
methods	O
are	O
always	O
the	O
best	O
in	O
this	O
chapter	O
we	O
present	O
n-step	B
td	B
methods	O
that	O
generalize	O
both	O
methods	O
so	O
that	O
one	O
can	O
shift	O
from	O
one	O
to	O
the	O
other	O
smoothly	O
as	O
needed	O
to	O
meet	O
the	O
demands	O
of	O
a	O
particular	O
task	O
n-step	B
methods	I
span	O
a	O
spectrum	O
with	O
mc	O
methods	O
at	O
one	O
end	O
and	O
one-step	O
td	B
methods	O
at	O
the	O
other	O
the	O
best	O
methods	O
are	O
often	O
intermediate	O
between	O
the	O
two	O
extremes	O
another	O
way	O
of	O
looking	O
at	O
the	O
benefits	O
of	O
n-step	B
methods	I
is	O
that	O
they	O
free	O
you	O
from	O
the	O
tyranny	O
of	O
the	O
time	O
step	O
with	O
one-step	O
td	B
methods	O
the	O
same	O
time	O
step	O
determines	O
how	O
often	O
the	O
action	B
can	O
be	O
changed	O
and	O
the	O
time	O
interval	O
over	O
which	O
bootstrapping	B
is	O
done	O
in	O
many	O
applications	O
one	O
wants	O
to	O
be	O
able	O
to	O
update	O
the	O
action	B
very	O
fast	O
to	O
take	O
into	O
account	O
anything	O
that	O
has	O
changed	O
but	O
bootstrapping	B
works	O
best	O
if	O
it	O
is	O
over	O
a	O
length	O
of	O
time	O
in	O
which	O
a	O
significant	O
and	O
recognizable	O
state	B
change	O
has	O
occurred	O
with	O
one-step	O
td	B
methods	O
these	O
time	O
intervals	O
are	O
the	O
same	O
and	O
so	O
a	O
compromise	O
must	O
be	O
made	O
n-step	B
methods	I
enable	O
bootstrapping	B
to	O
occur	O
over	O
multiple	O
steps	O
freeing	O
us	O
from	O
the	O
tyranny	O
of	O
the	O
single	O
time	O
step	O
the	O
idea	O
of	O
n-step	B
methods	I
is	O
usually	O
used	O
as	O
an	O
introduction	O
to	O
the	O
algorithmic	O
idea	O
of	O
eligibility	B
traces	I
which	O
enable	O
bootstrapping	B
over	O
multiple	O
time	O
intervals	O
simultaneously	O
here	O
we	O
instead	O
consider	O
the	O
n-step	B
bootstrapping	B
idea	O
on	O
its	O
own	O
postponing	O
the	O
treatment	O
of	O
eligibility-trace	O
mechanisms	O
until	O
later	O
this	O
allows	O
us	O
to	O
separate	O
the	O
issues	O
better	O
dealing	O
with	O
as	O
many	O
of	O
them	O
as	O
possible	O
in	O
the	O
simpler	O
n-step	B
setting	O
as	O
usual	O
we	O
first	O
consider	O
the	O
prediction	B
problem	O
and	O
then	O
the	O
control	B
problem	O
that	O
is	O
we	O
first	O
consider	O
how	O
n-step	B
methods	I
can	O
help	O
in	O
predicting	O
returns	O
as	O
a	O
function	O
of	O
state	B
for	O
a	O
fixed	O
policy	B
in	O
estimating	O
v	O
then	O
we	O
extend	O
the	O
ideas	O
to	O
action	B
values	O
and	B
control	B
methods	O
chapter	O
n-step	B
bootstrapping	B
n-step	B
td	B
prediction	B
what	O
is	O
the	O
space	O
of	O
methods	O
lying	O
between	O
monte	B
carlo	I
and	O
td	B
methods	O
consider	O
estimating	O
v	O
from	O
sample	O
episodes	B
generated	O
using	O
monte	B
carlo	I
methods	I
perform	O
an	O
update	O
for	O
each	O
state	B
based	O
on	O
the	O
entire	O
sequence	O
of	O
observed	O
rewards	O
from	O
that	O
state	B
until	O
the	O
end	O
of	O
the	O
episode	O
the	O
update	O
of	O
one-step	O
td	B
methods	O
on	O
the	O
other	O
hand	O
is	O
based	O
on	O
just	O
the	O
one	O
next	O
reward	O
bootstrapping	B
from	O
the	O
value	B
of	O
the	O
state	B
one	O
step	O
later	O
as	O
a	O
proxy	O
for	O
the	O
remaining	O
rewards	O
one	O
kind	O
of	O
intermediate	O
method	O
then	O
would	O
perform	O
an	O
update	O
based	O
on	O
an	O
intermediate	O
number	O
of	O
rewards	O
more	O
than	O
one	O
but	O
less	O
than	O
all	O
of	O
them	O
until	O
termination	O
for	O
example	O
a	O
two-step	O
update	O
would	O
be	O
based	O
on	O
the	O
first	O
two	O
rewards	O
and	O
the	O
estimated	O
value	B
of	O
the	O
state	B
two	O
steps	O
later	O
similarly	O
we	O
could	O
have	O
three-step	O
updates	O
four-step	O
updates	O
and	O
so	O
on	O
figure	O
shows	O
the	O
backup	O
diagrams	O
of	O
the	O
spectrum	O
of	O
n-step	B
updates	O
for	O
v	O
with	O
the	O
one-step	O
td	B
update	O
on	O
the	O
left	O
and	O
the	O
up-until-termination	O
monte	B
carlo	I
update	O
on	O
the	O
right	O
figure	O
the	O
backup	O
diagrams	O
of	O
n-step	B
methods	I
these	O
methods	O
form	O
a	O
spectrum	O
ranging	O
from	O
one-step	O
td	B
methods	O
to	O
monte	B
carlo	I
methods	I
the	O
methods	O
that	O
use	O
n-step	B
updates	O
are	O
still	O
td	B
methods	O
because	O
they	O
still	O
change	O
an	O
earlier	O
estimate	O
based	O
on	O
how	O
it	O
differs	O
from	O
a	O
later	O
estimate	O
now	O
the	O
later	O
estimate	O
is	O
not	O
one	O
step	O
later	O
but	O
n	O
steps	O
later	O
methods	O
in	O
which	O
the	O
temporal	O
difference	O
extends	O
over	O
n	O
steps	O
are	O
called	O
n-step	B
td	B
methods	O
the	O
td	B
methods	O
introduced	O
in	O
the	O
previous	O
chapter	O
all	O
used	O
one-step	O
updates	O
which	O
is	O
why	O
we	O
called	O
them	O
one-step	O
td	B
methods	O
more	O
formally	O
consider	O
the	O
update	O
of	O
the	O
estimated	O
value	B
of	O
state	B
st	O
as	O
a	O
result	O
of	O
the	O
state	B
reward	O
sequence	O
st	O
rt	O
st	O
the	O
actions	O
we	O
know	O
that	O
in	O
monte	B
carlo	I
updates	O
the	O
estimate	O
of	O
v	O
is	O
updated	O
in	O
the	O
direction	O
of	O
tdand	O
tdn-step	O
td	B
tdand	O
monte	B
carlo	I
n-step	B
td	B
prediction	B
the	O
complete	O
return	B
gt	O
t	O
t	O
where	O
t	O
is	O
the	O
last	O
time	O
step	O
of	O
the	O
episode	O
let	O
us	O
call	O
this	O
quantity	O
the	O
target	O
of	O
the	O
update	O
whereas	O
in	O
monte	B
carlo	I
updates	O
the	O
target	O
is	O
the	O
return	B
in	O
one-step	O
updates	O
the	O
target	O
is	O
the	O
first	O
reward	O
plus	O
the	O
discounted	O
estimated	O
value	B
of	O
the	O
next	O
state	B
which	O
we	O
call	O
the	O
one-step	O
return	B
where	O
vt	O
s	O
r	O
here	O
is	O
the	O
estimate	O
at	O
time	O
t	O
of	O
v	O
the	O
subscripts	O
on	O
indicate	O
that	O
it	O
is	O
a	O
truncated	B
return	B
for	O
time	O
t	O
using	O
rewards	O
up	O
until	O
time	O
t	O
with	O
the	O
discounted	O
estimate	O
taking	O
the	O
place	O
of	O
the	O
other	O
terms	O
t	O
t	O
of	O
the	O
full	O
return	B
as	O
discussed	O
in	O
the	O
previous	O
chapter	O
our	O
point	O
now	O
is	O
that	O
this	O
idea	O
makes	O
just	O
as	O
much	O
sense	O
after	O
two	O
steps	O
as	O
it	O
does	O
after	O
one	O
the	O
target	O
for	O
a	O
two-step	O
update	O
is	O
the	O
two-step	O
return	B
where	O
now	O
corrects	O
for	O
the	O
absence	O
of	O
the	O
terms	O
t	O
t	O
similarly	O
the	O
target	O
for	O
an	O
arbitrary	O
n-step	B
update	O
is	O
the	O
n-step	B
return	B
gttn	O
n	O
nvtn	O
for	O
all	O
n	O
t	O
such	O
that	O
n	O
and	O
t	O
t	O
n	O
all	O
n-step	B
returns	O
can	O
be	O
considered	O
approximations	O
to	O
the	O
full	O
return	B
truncated	B
after	O
n	O
steps	O
and	O
then	O
corrected	O
for	O
the	O
remaining	O
missing	O
terms	O
by	O
vtn	O
if	O
t	O
n	O
t	O
the	O
n-step	B
return	B
extends	O
to	O
or	O
beyond	O
termination	O
then	O
all	O
the	O
missing	O
terms	O
are	O
taken	O
as	O
zero	O
and	O
the	O
n-step	B
return	B
defined	O
to	O
be	O
equal	O
to	O
the	O
ordinary	O
full	O
return	B
gt	O
if	O
t	O
n	O
t	O
note	O
that	O
n-step	B
returns	O
for	O
n	O
involve	O
future	O
rewards	O
and	O
states	O
that	O
are	O
not	O
available	O
at	O
the	O
time	O
of	O
transition	O
from	O
t	O
to	O
t	O
no	O
real	O
algorithm	O
can	O
use	O
the	O
nstep	O
return	B
until	O
after	O
it	O
has	O
seen	O
rtn	O
and	O
computed	O
vtn	O
the	O
first	O
time	O
these	O
are	O
available	O
is	O
t	O
n	O
the	O
natural	O
state-value	O
learning	O
algorithm	O
for	O
using	O
n-step	B
returns	O
is	O
thus	O
vtnst	O
vtn	O
vtn	O
t	O
t	O
while	O
the	O
values	O
of	O
all	O
other	O
states	O
remain	O
unchanged	O
vtns	O
vtn	O
for	O
all	O
st	O
we	O
call	O
this	O
algorithm	O
n-step	B
td	B
note	O
that	O
no	O
changes	O
at	O
all	O
are	O
made	O
during	O
the	O
first	O
n	O
steps	O
of	O
each	O
episode	O
to	O
make	O
up	O
for	O
that	O
an	O
equal	O
number	O
of	O
additional	O
updates	O
are	O
made	O
at	O
the	O
end	O
of	O
the	O
episode	O
after	O
termination	O
and	O
before	O
starting	O
the	O
next	O
episode	O
complete	O
pseudocode	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
exercise	O
in	O
chapter	O
we	O
noted	O
that	O
the	O
monte	B
carlo	I
error	O
can	O
be	O
written	O
as	O
the	O
sum	O
of	O
td	B
errors	O
if	O
the	O
value	B
estimates	O
don	O
t	O
change	O
from	O
step	O
to	O
step	O
show	O
that	O
the	O
n-step	B
error	O
used	O
in	O
can	O
also	O
be	O
written	O
as	O
a	O
sum	O
td	B
errors	O
if	O
the	O
value	B
estimates	O
don	O
t	O
change	O
generalizing	O
the	O
earlier	O
result	O
exercise	O
with	O
an	O
n-step	B
method	O
the	O
value	B
estimates	O
do	O
change	O
from	O
step	O
to	O
step	O
so	O
an	O
algorithm	O
that	O
used	O
the	O
sum	O
of	O
td	B
errors	O
previous	O
exercise	O
in	O
chapter	O
n-step	B
bootstrapping	B
n-step	B
td	B
for	O
estimating	O
v	O
v	O
input	O
a	O
policy	B
algorithm	O
parameters	O
step	O
size	O
a	O
positive	O
integer	O
n	O
initialize	O
v	O
arbitrarily	O
for	O
all	O
s	O
s	O
all	O
store	O
and	O
access	O
operations	O
st	O
and	O
rt	O
can	O
take	O
their	O
index	O
mod	O
n	O
loop	O
for	O
each	O
episode	O
take	O
an	O
action	B
according	O
to	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
and	O
the	O
next	O
state	B
as	O
if	O
is	O
terminal	O
then	O
t	O
t	O
is	O
the	O
time	O
whose	O
state	B
s	O
estimate	O
is	O
being	O
updated	O
if	O
t	O
t	O
then	O
initialize	O
and	O
store	O
terminal	O
t	O
loop	O
for	O
t	O
until	O
t	O
g	O
t	O
n	O
if	O
i	O
i	O
if	O
n	O
t	O
then	O
g	O
g	O
nv	O
v	O
v	O
v	O
place	O
of	O
the	O
error	O
in	O
would	O
actually	O
be	O
a	O
slightly	O
different	O
algorithm	O
would	O
it	O
be	O
a	O
better	O
algorithm	O
or	O
a	O
worse	O
one	O
devise	O
and	O
program	O
a	O
small	O
experiment	O
to	O
answer	O
this	O
question	O
empirically	O
the	O
n-step	B
return	B
uses	O
the	O
value	B
function	I
vtn	O
to	O
correct	O
for	O
the	O
missing	O
rewards	O
beyond	O
rtn	O
an	O
important	O
property	O
of	O
n-step	B
returns	O
is	O
that	O
their	O
expectation	O
is	O
guaranteed	O
to	O
be	O
a	O
better	O
estimate	O
of	O
v	O
than	O
vtn	O
is	O
in	O
a	O
worst-state	O
sense	O
that	O
is	O
the	O
worst	O
error	O
of	O
the	O
expected	B
n-step	B
return	B
is	O
guaranteed	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
n	O
times	O
the	O
worst	O
error	O
under	O
vtn	O
max	O
s	O
s	O
v	O
n	O
max	O
s	O
v	O
for	O
all	O
n	O
this	O
is	O
called	O
the	O
error	B
reduction	I
property	I
of	O
n-step	B
returns	O
because	O
of	O
the	O
error	B
reduction	I
property	I
one	O
can	O
show	O
formally	O
that	O
all	O
n-step	B
td	B
methods	O
converge	O
to	O
the	O
correct	O
predictions	O
under	O
appropriate	O
technical	O
conditions	O
the	O
n-step	B
td	B
methods	O
thus	O
form	O
a	O
family	O
of	O
sound	O
methods	O
with	O
one-step	O
td	B
methods	O
and	B
monte	B
carlo	I
methods	I
as	O
extreme	O
members	O
example	O
n-step	B
td	B
methods	O
on	O
the	O
random	B
walk	I
consider	O
using	O
n-step	B
td	B
methods	O
on	O
the	O
random	B
walk	I
task	O
described	O
in	O
example	O
suppose	O
the	O
first	O
episode	O
progressed	O
directly	O
from	O
the	O
center	O
state	B
c	O
to	O
the	O
right	O
through	O
d	O
and	O
e	O
and	O
then	O
terminated	O
on	O
the	O
right	O
with	O
a	O
return	B
of	O
recall	O
that	O
the	O
estimated	O
values	O
of	O
all	O
the	O
states	O
started	O
at	O
an	O
intermediate	O
value	B
v	O
as	O
a	O
result	O
of	O
this	O
experience	O
a	O
one-step	O
method	O
would	O
change	O
only	O
the	O
estimate	O
for	O
the	O
last	O
state	B
n-step	B
sarsa	B
v	O
which	O
would	O
be	O
incremented	O
toward	O
the	O
observed	O
return	B
a	O
two-step	O
method	O
on	O
the	O
other	O
hand	O
would	O
increment	O
the	O
values	O
of	O
the	O
two	O
states	O
preceding	O
termination	O
v	O
and	O
v	O
both	O
would	O
be	O
incremented	O
toward	O
a	O
three-step	O
method	O
or	O
any	O
n-step	B
method	O
for	O
n	O
would	O
increment	O
the	O
values	O
of	O
all	O
three	O
of	O
the	O
visited	O
states	O
toward	O
all	O
by	O
the	O
same	O
amount	O
figure	O
performance	O
of	O
n-step	B
td	B
methods	O
as	O
a	O
function	O
of	O
for	O
various	O
values	O
of	O
n	O
on	O
a	O
random	B
walk	I
task	O
which	O
value	B
of	O
n	O
is	O
better	O
figure	O
shows	O
the	O
results	O
of	O
a	O
simple	O
empirical	O
test	O
for	O
a	O
larger	O
random	B
walk	I
process	O
with	O
states	O
instead	O
of	O
with	O
a	O
outcome	O
on	O
the	O
left	O
all	O
values	O
initialized	O
to	O
which	O
we	O
use	O
as	O
a	O
running	O
example	O
in	O
this	O
chapter	O
results	O
are	O
shown	O
for	B
n-step	B
td	B
methods	O
with	O
a	O
range	O
of	O
values	O
for	O
n	O
and	O
the	O
performance	O
measure	O
for	O
each	O
parameter	O
setting	O
shown	O
on	O
the	O
vertical	O
axis	O
is	O
the	O
square-root	O
of	O
the	O
average	O
squared	O
error	O
between	O
the	O
predictions	O
at	O
the	O
end	O
of	O
the	O
episode	O
for	O
the	O
states	O
and	O
their	O
true	O
values	O
then	O
averaged	O
over	O
the	O
first	O
episodes	B
and	O
repetitions	O
of	O
the	O
whole	O
experiment	O
same	O
sets	O
of	O
walks	O
were	O
used	O
for	O
all	O
parameter	O
settings	O
note	O
that	O
methods	O
with	O
an	O
intermediate	O
value	B
of	O
n	O
worked	O
best	O
this	O
illustrates	O
how	O
the	O
generalization	O
of	O
td	B
and	B
monte	B
carlo	I
methods	I
to	O
n-step	B
methods	I
can	O
potentially	O
perform	O
better	O
than	O
either	O
of	O
the	O
two	O
extreme	O
methods	O
exercise	O
why	O
do	O
you	O
think	O
a	O
larger	O
random	B
walk	I
task	O
states	O
instead	O
of	O
was	O
used	O
in	O
the	O
examples	O
of	O
this	O
chapter	O
would	O
a	O
smaller	O
walk	O
have	O
shifted	O
the	O
advantage	O
to	O
a	O
different	O
value	B
of	O
n	O
how	O
about	O
the	O
change	O
in	O
left-side	O
outcome	O
from	O
to	O
made	O
in	O
the	O
larger	O
walk	O
do	O
you	O
think	O
that	O
made	O
any	O
difference	O
in	O
the	O
best	O
value	B
of	O
n	O
n-step	B
sarsa	B
how	O
can	O
n-step	B
methods	I
be	O
used	O
not	O
just	O
for	O
prediction	B
but	O
for	O
control	B
in	O
this	O
section	O
we	O
show	O
how	O
n-step	B
methods	I
can	O
be	O
combined	O
with	O
sarsa	B
in	O
a	O
straightforward	O
way	O
to	O
averagerms	O
errorover	O
statesand	O
first	O
chapter	O
n-step	B
bootstrapping	B
produce	O
an	O
on-policy	O
td	B
control	B
method	O
the	O
n-step	B
version	O
of	O
sarsa	B
we	O
call	O
n-step	B
sarsa	B
and	O
the	O
original	O
version	O
presented	O
in	O
the	O
previous	O
chapter	O
we	O
henceforth	O
call	O
one-step	O
sarsa	B
or	O
the	O
main	O
idea	O
is	O
to	O
simply	O
switch	O
states	O
for	O
actions	O
action	B
pairs	O
and	O
then	O
use	O
an	O
policy	B
the	O
backup	O
diagrams	O
for	B
n-step	B
sarsa	B
in	O
figure	O
like	O
those	O
of	O
n-step	B
td	B
are	O
strings	O
of	O
alternating	O
states	O
and	O
actions	O
except	O
that	O
the	O
sarsa	B
ones	O
all	O
start	O
and	O
end	O
with	O
an	O
action	B
rather	O
a	O
state	B
we	O
redefine	O
n-step	B
returns	O
targets	O
in	O
terms	O
of	O
estimated	O
action	B
values	O
gttn	O
n	O
nqtn	O
atn	O
n	O
t	O
t	O
n	O
with	O
gttn	O
gt	O
if	O
t	O
n	O
t	O
the	O
natural	O
algorithm	O
is	O
then	O
qtnst	O
at	O
qtn	O
at	O
qtn	O
at	O
t	O
t	O
while	O
the	O
values	O
of	O
all	O
other	O
states	O
remain	O
unchanged	O
qtns	O
a	O
qtn	O
a	O
for	O
all	O
s	O
a	O
such	O
that	O
s	O
st	O
or	O
a	O
at	O
this	O
is	O
the	O
algorithm	O
we	O
call	O
n-step	B
sarsa	B
pseudocode	O
is	O
shown	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
and	O
an	O
example	O
of	O
why	O
it	O
can	O
speed	O
up	O
learning	O
compared	O
to	O
one-step	O
methods	O
is	O
given	O
in	O
figure	O
figure	O
the	O
backup	O
diagrams	O
for	O
the	O
spectrum	O
of	O
n-step	B
methods	I
for	O
state	B
action	B
values	O
they	O
range	O
from	O
the	O
one-step	O
update	O
of	O
to	O
the	O
up-until-termination	O
update	O
of	O
the	O
monte	B
carlo	I
method	O
in	O
between	O
are	O
the	O
n-step	B
updates	O
based	O
on	O
n	O
steps	O
of	O
real	O
rewards	O
and	O
the	O
estimated	O
value	B
of	O
the	O
nth	O
next	O
state	B
action	B
pair	O
all	O
appropriately	O
discounted	O
on	O
the	O
far	O
right	O
is	O
the	O
backup	B
diagram	I
for	B
n-step	B
expected	B
sarsa	B
sarsaaka	O
sarsan-step	O
sarsa	B
sarsaaka	O
monte	O
carlon-step	O
expected	B
sarsa	B
n-step	B
sarsa	B
n-step	B
sarsa	B
for	O
estimating	O
q	O
q	O
or	O
q	O
initialize	O
qs	O
a	O
arbitrarily	O
for	O
all	O
s	O
s	O
a	O
a	O
initialize	O
to	O
be	O
with	O
respect	O
to	O
q	O
or	O
to	O
a	O
fixed	O
given	O
policy	B
algorithm	O
parameters	O
step	O
size	O
small	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
st	O
at	O
and	O
rt	O
can	O
take	O
their	O
index	O
mod	O
n	O
loop	O
for	O
each	O
episode	O
if	O
t	O
t	O
then	O
initialize	O
and	O
store	O
terminal	O
select	O
and	O
store	O
an	O
action	B
t	O
loop	O
for	O
t	O
until	O
t	O
g	O
t	O
n	O
if	O
i	O
i	O
else	O
t	O
t	O
select	O
and	O
store	O
an	O
action	B
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
and	O
the	O
next	O
state	B
as	O
if	O
is	O
terminal	O
then	O
if	O
n	O
t	O
then	O
g	O
g	O
nqs	O
a	O
qs	O
a	O
qs	O
a	O
qs	O
a	O
if	O
is	O
being	O
learned	O
then	O
ensure	O
that	O
is	O
wrt	O
q	O
figure	O
gridworld	O
example	O
of	O
the	O
speedup	O
of	O
policy	B
learning	O
due	O
to	O
the	O
use	O
of	O
n-step	B
methods	I
the	O
first	O
panel	O
shows	O
the	O
path	O
taken	O
by	O
an	O
agent	O
in	O
a	O
single	O
episode	O
ending	O
at	O
a	O
location	O
of	O
high	O
reward	O
marked	O
by	O
the	O
g	O
in	O
this	O
example	O
the	O
values	O
were	O
all	O
initially	O
and	O
all	O
rewards	O
were	O
zero	O
except	O
for	O
a	O
positive	O
reward	O
at	O
g	O
the	O
arrows	O
in	O
the	O
other	O
two	O
panels	O
show	O
which	O
action	B
values	O
were	O
strengthened	O
as	O
a	O
result	O
of	O
this	O
path	O
by	O
one-step	O
and	O
n-step	B
sarsa	B
methods	O
the	O
one-step	O
method	O
strengthens	O
only	O
the	O
last	O
action	B
of	O
the	O
sequence	O
of	O
actions	O
that	O
led	O
to	O
the	O
high	O
reward	O
whereas	O
the	O
n-step	B
method	O
strengthens	O
the	O
last	O
n	O
actions	O
of	O
the	O
sequence	O
so	O
that	O
much	O
more	O
is	O
learned	O
from	O
the	O
one	O
episode	O
path	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increased	O
by	O
sarsaggg	O
chapter	O
n-step	B
bootstrapping	B
exercise	O
prove	O
that	O
the	O
n-step	B
return	B
of	O
sarsa	B
can	O
be	O
written	O
exactly	O
in	O
terms	O
of	O
a	O
novel	O
td	B
error	I
as	O
mintnt	O
gttn	O
qt	O
at	O
k	O
t	O
qk	O
ak	O
what	O
about	O
expected	B
sarsa	B
the	O
backup	B
diagram	I
for	O
the	O
n-step	B
version	O
of	O
expected	B
sarsa	B
is	O
shown	O
on	O
the	O
far	O
right	O
in	O
figure	O
it	O
consists	O
of	O
a	O
linear	O
string	O
of	O
sample	O
actions	O
and	O
states	O
just	O
as	O
in	O
n-step	B
sarsa	B
except	O
that	O
its	O
last	O
element	O
is	O
a	O
branch	O
over	O
all	O
action	B
possibilities	O
weighted	O
as	O
always	O
by	O
their	O
probability	O
under	O
this	O
algorithm	O
can	O
be	O
described	O
by	O
the	O
same	O
equation	O
as	O
n-step	B
sarsa	B
except	O
with	O
the	O
n-step	B
return	B
redefined	O
as	O
gttn	O
vts	O
gttn	O
s	O
using	O
the	O
estimated	O
action	B
values	O
at	O
time	O
t	O
under	O
the	O
target	O
policy	B
n	O
n	O
vtn	O
gt	O
for	O
t	O
n	O
t	O
where	O
vts	O
is	O
the	O
expected	B
approximate	I
value	B
of	O
state	B
a	O
for	O
all	O
s	O
s	O
expected	B
approximate	O
values	O
are	O
used	O
in	O
developing	O
many	O
of	O
the	O
action-value	B
methods	I
in	O
the	O
rest	O
of	O
this	O
book	O
if	O
s	O
is	O
terminal	O
then	O
its	O
expected	B
approximate	I
value	B
is	O
defined	O
to	O
be	O
t	O
n	O
t	O
n-step	B
off-policy	B
learning	O
by	O
importance	O
sam	O
pling	O
recall	O
that	O
off-policy	B
learning	O
is	O
learning	O
the	O
value	B
function	I
for	O
one	O
policy	B
while	O
following	O
another	O
policy	B
b	O
often	O
is	O
the	O
greedy	O
policy	B
for	O
the	O
current	O
action-valuefunction	O
estimate	O
and	O
b	O
is	O
a	O
more	O
exploratory	O
policy	B
perhaps	O
in	O
order	O
to	O
use	O
the	O
data	O
from	O
b	O
we	O
must	O
take	O
into	O
account	O
the	O
difference	O
between	O
the	O
two	O
policies	O
using	O
their	O
relative	O
probability	O
of	O
taking	O
the	O
actions	O
that	O
were	O
taken	O
section	O
in	O
n-step	B
methods	I
returns	O
are	O
constructed	O
over	O
n	O
steps	O
so	O
we	O
are	O
interested	O
in	O
the	O
relative	O
probability	O
of	O
just	O
those	O
n	O
actions	O
for	O
example	O
to	O
make	O
a	O
simple	O
off-policy	B
version	O
of	O
n-step	B
td	B
the	O
update	O
for	O
time	O
t	O
made	O
at	O
time	O
t	O
n	O
can	O
simply	O
be	O
weighted	O
by	O
ttn	O
vtn	O
ttn	O
vtn	O
where	O
ttn	O
called	O
the	O
importance	B
sampling	I
ratio	B
is	O
the	O
relative	O
probability	O
under	O
the	O
two	O
policies	O
of	O
taking	O
the	O
n	O
actions	O
from	O
at	O
to	O
atn	O
eq	O
t	O
t	O
vtnst	O
th	O
minht	O
baksk	O
n-step	B
off-policy	B
learning	O
by	O
importance	B
sampling	I
for	O
example	O
if	O
any	O
one	O
of	O
the	O
actions	O
would	O
never	O
be	O
taken	O
by	O
then	O
the	O
n-step	B
return	B
should	O
be	O
given	O
zero	O
weight	O
and	O
be	O
totally	O
ignored	O
on	O
the	O
other	O
hand	O
if	O
by	O
chance	O
an	O
action	B
is	O
taken	O
that	O
would	O
take	O
with	O
much	O
greater	O
probability	O
than	O
b	O
does	O
then	O
this	O
will	O
increase	O
the	O
weight	O
that	O
would	O
otherwise	O
be	O
given	O
to	O
the	O
return	B
this	O
makes	O
sense	O
because	O
that	O
action	B
is	O
characteristic	O
of	O
therefore	O
we	O
want	O
to	O
learn	O
about	O
it	O
but	O
is	O
selected	O
only	O
rarely	O
by	O
b	O
and	O
thus	O
rarely	O
appears	O
in	O
the	O
data	O
to	O
make	O
up	O
for	O
this	O
we	O
have	O
to	O
over-weight	O
it	O
when	O
it	O
does	O
occur	O
note	O
that	O
if	O
the	O
two	O
policies	O
are	O
actually	O
the	O
same	O
on-policy	O
case	O
then	O
the	O
importance	B
sampling	I
ratio	B
is	O
always	O
thus	O
our	O
new	O
update	O
generalizes	O
and	O
can	O
completely	O
replace	O
our	O
earlier	O
n-step	B
td	B
update	O
similarly	O
our	O
previous	O
n-step	B
sarsa	B
update	O
can	O
be	O
completely	O
replaced	O
by	O
a	O
simple	O
off-policy	B
form	O
qtnst	O
at	O
qtn	O
at	O
qtn	O
at	O
for	O
t	O
t	O
note	O
that	O
the	O
importance	B
sampling	I
ratio	B
here	O
starts	O
one	O
step	O
later	O
than	O
for	B
n-step	B
td	B
this	O
is	O
because	O
here	O
we	O
are	O
updating	O
a	O
state	B
action	B
pair	O
we	O
do	O
not	O
have	O
to	O
care	O
how	O
likely	O
we	O
were	O
to	O
select	O
the	O
action	B
now	O
that	O
we	O
have	O
selected	O
it	O
we	O
want	O
to	O
learn	O
fully	O
from	O
what	O
happens	O
with	O
importance	B
sampling	I
only	O
for	O
subsequent	O
actions	O
pseudocode	O
for	O
the	O
full	O
algorithm	O
is	O
shown	O
in	O
the	O
box	O
below	O
off-policy	B
n-step	B
sarsa	B
for	O
estimating	O
q	O
q	O
or	O
q	O
input	O
an	O
arbitrary	O
behavior	O
policy	B
b	O
such	O
that	O
bas	O
for	O
all	O
s	O
s	O
a	O
a	O
initialize	O
qs	O
a	O
arbitrarily	O
for	O
all	O
s	O
s	O
a	O
a	O
initialize	O
to	O
be	O
greedy	O
with	O
respect	O
to	O
q	O
or	O
as	O
a	O
fixed	O
given	O
policy	B
algorithm	O
parameters	O
step	O
size	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
st	O
at	O
and	O
rt	O
can	O
take	O
their	O
index	O
mod	O
n	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
and	O
the	O
next	O
state	B
as	O
if	O
is	O
terminal	O
then	O
t	O
t	O
select	O
and	O
store	O
an	O
action	B
b	O
else	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
loop	O
for	O
each	O
episode	O
if	O
t	O
t	O
then	O
initialize	O
and	O
store	O
terminal	O
select	O
and	O
store	O
an	O
action	B
b	O
t	O
loop	O
for	O
t	O
until	O
t	O
g	O
t	O
n	O
if	O
baisi	O
i	O
i	O
i	O
if	O
n	O
t	O
then	O
g	O
g	O
nqs	O
a	O
qs	O
a	O
qs	O
a	O
qs	O
a	O
if	O
is	O
being	O
learned	O
then	O
ensure	O
that	O
is	O
greedy	O
wrt	O
q	O
chapter	O
n-step	B
bootstrapping	B
the	O
off-policy	B
version	O
of	O
n-step	B
expected	B
sarsa	B
would	O
use	O
the	O
same	O
update	O
as	O
above	O
for	B
n-step	B
sarsa	B
except	O
that	O
the	O
importance	B
sampling	I
ratio	B
would	O
have	O
one	O
less	O
factor	O
in	O
it	O
that	O
is	O
the	O
above	O
equation	O
would	O
use	O
instead	O
of	O
and	O
of	O
course	O
it	O
would	O
use	O
the	O
expected	B
sarsa	B
version	O
of	O
the	O
n-step	B
return	B
this	O
is	O
because	O
in	O
expected	B
sarsa	B
all	O
possible	O
actions	O
are	O
taken	O
into	O
account	O
in	O
the	O
last	O
state	B
the	O
one	O
actually	O
taken	O
has	O
no	O
effect	O
and	O
does	O
not	O
have	O
to	O
be	O
corrected	O
for	O
off-policy	B
methods	I
with	B
control	B
variates	I
the	O
multi-step	O
off-policy	B
methods	I
presented	O
in	O
the	O
previous	O
section	O
are	O
simple	O
and	O
conceptually	O
clear	O
but	O
are	O
probably	O
not	O
the	O
most	O
efficient	O
a	O
more	O
sophisticated	O
approach	O
would	O
use	O
per-decision	B
importance	B
sampling	I
ideas	O
such	O
as	O
were	O
introduced	O
in	O
section	O
to	O
understand	O
this	O
approach	O
first	O
note	O
that	O
the	O
ordinary	O
n-step	B
return	B
like	O
all	O
returns	O
can	O
be	O
written	O
recursively	O
for	O
the	O
n	O
steps	O
ending	O
at	O
horizon	O
h	O
the	O
n-step	B
return	B
can	O
be	O
written	O
gth	O
t	O
h	O
t	O
where	O
ghh	O
vh	O
that	O
this	O
return	B
is	O
used	O
at	O
time	O
h	O
previously	O
denoted	O
t	O
n	O
now	O
consider	O
the	O
effect	O
of	O
following	O
a	O
behavior	O
policy	B
b	O
that	O
is	O
not	O
the	O
same	O
as	O
the	O
target	O
policy	B
all	O
of	O
the	O
resulting	O
experience	O
including	O
the	O
first	O
reward	O
and	O
the	O
next	O
state	B
must	O
be	O
weighted	O
by	O
the	O
importance	B
sampling	I
ratio	B
for	O
time	O
t	O
t	O
batst	O
one	O
might	O
be	O
tempted	O
to	O
simply	O
weight	O
the	O
righthand	O
side	O
of	O
the	O
above	O
equation	O
but	O
one	O
can	O
do	O
better	O
suppose	O
the	O
action	B
at	O
time	O
t	O
would	O
never	O
be	O
selected	O
by	O
so	O
that	O
t	O
is	O
zero	O
then	O
a	O
simple	O
weighting	O
would	O
result	O
in	O
the	O
n-step	B
return	B
being	O
zero	O
which	O
could	O
result	O
in	O
high	O
variance	O
when	O
it	O
was	O
used	O
as	O
a	O
target	O
instead	O
in	O
this	O
more	O
sophisticated	O
approach	O
one	O
uses	O
an	O
alternate	O
off-policy	B
definition	O
of	O
the	O
n-step	B
return	B
ending	O
at	O
horizon	O
h	O
as	O
gth	O
t	O
tvh	O
t	O
h	O
t	O
where	O
again	O
ghh	O
vh	O
in	O
this	O
approach	O
if	O
t	O
is	O
zero	O
then	O
instead	O
of	O
the	O
target	O
being	O
zero	O
and	O
causing	O
the	O
estimate	O
to	O
shrink	O
the	O
target	O
is	O
the	O
same	O
as	O
the	O
estimate	O
and	O
causes	O
no	O
change	O
the	O
importance	B
sampling	I
ratio	B
being	O
zero	O
means	O
we	O
should	O
ignore	O
the	O
sample	O
so	O
leaving	O
the	O
estimate	O
unchanged	O
seems	O
appropriate	O
the	O
second	O
additional	O
term	O
in	O
is	O
called	O
a	O
control	B
variate	O
obscure	O
reasons	O
notice	O
that	O
the	O
control	B
variate	O
does	O
not	O
change	O
the	O
expected	B
update	I
the	O
importance	B
sampling	I
ratio	B
has	O
expected	B
value	B
one	O
and	O
is	O
uncorrelated	O
with	O
the	O
estimate	O
so	O
the	O
expected	B
value	B
of	O
the	O
control	B
variate	O
is	O
zero	O
also	O
note	O
that	O
the	O
off-policy	B
definition	O
is	O
a	O
strict	O
generalization	O
of	O
the	O
earlier	O
on-policy	O
definition	O
of	O
the	O
n-step	B
return	B
as	O
the	O
two	O
are	O
identical	O
in	O
the	O
on-policy	O
case	O
in	O
which	O
t	O
is	O
always	O
for	O
a	O
conventional	O
n-step	B
method	O
the	O
learning	O
rule	O
to	O
use	O
in	O
conjunction	O
with	O
is	O
the	O
n-step	B
td	B
update	O
which	O
has	O
no	O
explicit	O
importance	B
sampling	I
ratios	O
other	O
per-decision	B
off-policy	B
methods	I
with	B
control	B
variates	I
than	O
those	O
embedded	O
in	O
the	O
return	B
exercise	O
write	O
the	O
pseudocode	O
for	O
the	O
off-policy	B
state-value	O
prediction	B
algorithm	O
described	O
above	O
for	B
action	B
values	I
the	O
off-policy	B
definition	O
of	O
the	O
n-step	B
return	B
is	O
a	O
little	O
different	O
because	O
the	O
first	O
action	B
does	O
not	O
play	O
a	O
role	O
in	O
the	O
importance	B
sampling	I
that	O
first	O
action	B
is	O
the	O
one	O
being	O
learned	O
it	O
does	O
not	O
matter	O
if	O
it	O
was	O
unlikely	O
or	O
even	O
impossible	O
under	O
the	O
target	O
policy	B
it	O
has	O
been	O
taken	O
and	O
now	O
full	O
unit	O
weight	O
must	O
be	O
given	O
to	O
the	O
reward	O
and	O
state	B
that	O
follows	O
it	O
importance	B
sampling	I
will	O
apply	O
only	O
to	O
the	O
actions	O
that	O
follow	O
it	O
first	O
note	O
that	O
for	B
action	B
values	I
the	O
n-step	B
on-policy	O
return	B
ending	O
at	O
horizon	O
h	O
expectation	O
form	O
can	O
be	O
written	O
recursively	O
just	O
as	O
in	O
except	O
that	O
for	O
vh	O
as	O
in	O
an	O
off-policy	B
form	O
action	B
values	O
the	O
recursion	O
ends	O
with	O
ghh	O
with	B
control	B
variates	I
is	O
gth	O
vh	O
qh	O
vh	O
t	O
h	O
t	O
if	O
h	O
t	O
then	O
the	O
recursion	O
ends	O
with	O
ghh	O
sion	O
ends	O
with	O
and	O
gt	O
with	O
is	O
analogous	O
to	O
expected	B
sarsa	B
qh	O
ah	O
whereas	O
if	O
h	O
t	O
the	O
recur	O
rt	O
the	O
resultant	O
prediction	B
algorithm	O
combining	O
exercise	O
prove	O
that	O
the	O
control	B
variate	O
in	O
the	O
above	O
equations	O
does	O
not	O
change	O
the	O
expected	B
value	B
of	O
the	O
return	B
exercise	O
write	O
the	O
pseudocode	O
for	O
the	O
off-policy	B
action-value	O
prediction	B
algorithm	O
described	O
immediately	O
above	O
pay	O
particular	O
attention	O
to	O
the	O
termination	O
conditions	O
for	O
the	O
recursion	O
upon	O
hitting	O
the	O
horizon	O
or	O
the	O
end	O
of	O
episode	O
exercise	O
show	O
that	O
the	O
general	O
version	O
of	O
the	O
n-step	B
return	B
can	O
still	O
be	O
written	O
exactly	O
and	O
compactly	O
as	O
the	O
sum	O
of	O
state-based	O
td	B
errors	O
if	O
the	O
approximate	O
state	B
value	B
function	I
does	O
not	O
change	O
exercise	O
repeat	O
the	O
above	O
exercise	O
for	O
the	O
action	B
version	O
of	O
the	O
off-policy	B
n-step	B
return	B
and	O
the	O
expected	B
sarsa	B
td	B
error	I
quantity	O
in	O
brackets	O
in	O
equation	O
exercise	O
devise	O
a	O
small	O
off-policy	B
prediction	B
problem	O
and	O
use	O
it	O
to	O
show	O
that	O
the	O
off-policy	B
learning	O
algorithm	O
using	O
and	O
is	O
more	O
data	O
efficient	O
than	O
the	O
simpler	O
algorithm	O
using	O
and	O
the	O
importance	B
sampling	I
that	O
we	O
have	O
used	O
in	O
this	O
section	O
the	O
previous	O
section	O
and	O
in	O
chapter	O
enables	O
sound	O
off-policy	B
learning	O
but	O
also	O
results	O
in	O
high	O
variance	O
updates	O
forcing	O
the	O
use	O
of	O
a	O
small	O
step-size	B
parameter	I
and	O
thereby	O
causing	O
learning	O
to	O
be	O
slow	O
it	O
is	O
probably	O
inevitable	O
that	O
off-policy	B
training	O
is	O
slower	O
than	O
on-policy	O
training	O
after	O
all	O
the	O
data	O
is	O
less	O
relevant	O
to	O
what	O
is	O
being	O
learned	O
however	O
it	O
is	O
probably	O
also	O
true	O
that	O
these	O
methods	O
can	O
be	O
improved	O
on	O
the	O
control	B
variates	I
are	O
one	O
way	O
of	O
reducing	O
the	O
variance	O
another	O
is	O
to	O
rapidly	O
adapt	O
the	O
step	O
sizes	O
to	O
the	O
observed	O
variance	O
as	O
in	O
the	O
chapter	O
n-step	B
bootstrapping	B
autostep	O
method	O
sutton	O
degris	O
and	O
pilarski	O
yet	O
another	O
promising	O
approach	O
is	O
the	O
invariant	O
updates	O
of	O
karampatziakis	O
and	O
langford	O
as	O
extended	O
to	O
td	B
by	O
tian	O
preparation	O
the	O
usage	O
technique	O
of	O
mahmood	O
mahmood	O
and	O
sutton	O
may	O
also	O
be	O
part	O
of	O
the	O
solution	O
in	O
the	O
next	O
section	O
we	O
consider	O
an	O
off-policy	B
learning	O
method	O
that	O
does	O
not	O
use	O
importance	B
sampling	I
off-policy	B
learning	O
without	O
importance	B
sampling	I
the	O
n-step	B
tree	B
backup	I
algorithm	O
is	O
off-policy	B
learning	O
possible	O
without	O
importance	B
sampling	I
q-learning	B
and	O
expected	B
sarsa	B
from	O
chapter	O
do	O
this	O
for	O
the	O
one-step	O
case	O
but	O
is	O
there	O
a	O
corresponding	O
multi-step	O
algorithm	O
in	O
this	O
section	O
we	O
present	O
just	O
such	O
an	O
n-step	B
method	O
called	O
the	O
tree-backup	O
algorithm	O
the	O
idea	O
of	O
the	O
algorithm	O
is	O
suggested	O
by	O
the	O
tree-backup	O
backup	B
diagram	I
shown	O
to	O
the	O
right	O
down	O
the	O
central	O
spine	O
and	O
labeled	O
in	O
the	O
diagram	O
are	O
three	O
sample	O
states	O
and	O
rewards	O
and	O
two	O
sample	O
actions	O
these	O
are	O
the	O
random	O
variables	O
representing	O
the	O
events	O
occurring	O
after	O
the	O
initial	O
state	B
action	B
pair	O
st	O
at	O
hanging	O
off	O
to	O
the	O
sides	O
of	O
each	O
state	B
are	O
the	O
actions	O
that	O
were	O
not	O
selected	O
the	O
last	O
state	B
all	O
the	O
actions	O
are	O
considered	O
to	O
have	O
not	O
been	O
selected	O
because	O
we	O
have	O
no	O
sample	O
data	O
for	O
the	O
unselected	O
actions	O
we	O
bootstrap	O
and	O
use	O
the	O
estimates	O
of	O
their	O
values	O
in	O
forming	O
the	O
target	O
for	O
the	O
update	O
this	O
slightly	O
extends	O
the	O
idea	O
of	O
a	O
backup	B
diagram	I
so	O
far	O
we	O
have	O
always	O
updated	O
the	O
estimated	O
value	B
of	O
the	O
node	O
at	O
the	O
top	O
of	O
the	O
diagram	O
toward	O
a	O
target	O
combining	O
the	O
rewards	O
along	O
the	O
way	O
discounted	O
and	O
the	O
estimated	O
values	O
of	O
the	O
nodes	O
at	O
the	O
bottom	O
in	O
the	O
tree-backup	O
update	O
the	O
target	O
includes	O
all	O
these	O
things	O
plus	O
the	O
estimated	O
values	O
of	O
the	O
dangling	O
action	B
nodes	O
hanging	O
off	O
the	O
sides	O
at	O
all	O
levels	O
this	O
is	O
why	O
it	O
is	O
called	O
a	O
tree-backup	O
update	O
it	O
is	O
an	O
update	O
from	O
the	O
entire	O
tree	O
of	O
estimated	O
action	B
values	O
the	O
tree-backup	O
update	O
more	O
precisely	O
the	O
update	O
is	O
from	O
the	O
estimated	O
action	B
values	O
of	O
the	O
leaf	O
nodes	O
of	O
the	O
tree	O
the	O
action	B
nodes	O
in	O
the	O
interior	O
corresponding	O
to	O
the	O
actual	O
actions	O
taken	O
do	O
not	O
participate	O
each	O
leaf	O
node	O
contributes	O
to	O
the	O
target	O
with	O
a	O
weight	O
proportional	O
to	O
its	O
probability	O
of	O
occurring	O
under	O
the	O
target	O
policy	B
thus	O
each	O
first-level	O
action	B
a	O
contributes	O
with	O
a	O
weight	O
of	O
except	O
that	O
the	O
action	B
actually	O
taken	O
does	O
not	O
contribute	O
at	O
all	O
its	O
probability	O
is	O
used	O
to	O
weight	O
all	O
the	O
second-level	O
action	B
values	O
thus	O
each	O
non-selected	O
secondlevel	O
action	B
contributes	O
with	O
weight	O
each	O
third-level	O
action	B
contributes	O
with	O
weight	O
and	O
so	O
on	O
it	O
is	O
as	O
if	O
each	O
arrow	O
to	O
an	O
action	B
node	O
in	O
the	O
diagram	O
is	O
weighted	O
by	O
the	O
action	B
s	O
probability	O
of	O
being	O
selected	O
under	O
the	O
target	O
policy	B
and	O
if	O
there	O
is	O
a	O
tree	O
below	O
the	O
action	B
then	O
that	O
weight	O
applies	O
to	O
all	O
the	O
leaf	O
nodes	O
in	O
the	O
tree	O
off-policy	B
learning	O
without	O
importance	B
sampling	I
n-step	B
tree	B
backup	I
we	O
can	O
think	O
of	O
the	O
tree-backup	O
update	O
as	O
consisting	O
of	O
half-steps	O
alternating	O
between	O
sample	O
half-steps	O
from	O
an	O
action	B
to	O
a	O
subsequent	O
state	B
and	O
expected	B
half-steps	O
considering	O
from	O
that	O
state	B
all	O
possible	O
actions	O
with	O
their	O
probabilities	O
of	O
occuring	O
under	O
the	O
policy	B
now	O
let	O
us	O
develop	O
the	O
detailed	O
equations	O
for	O
the	O
n-step	B
tree-backup	O
algorithm	O
the	O
one-step	O
return	B
is	O
the	O
same	O
as	O
that	O
of	O
expected	B
sarsa	B
a	O
t	O
t	O
and	O
the	O
two-step	O
tree-backup	O
return	B
is	O
a	O
a	O
t	O
t	O
the	O
latter	O
form	O
suggests	O
the	O
general	O
recursive	O
definition	O
of	O
the	O
tree-backup	O
n-step	B
return	B
gttn	O
a	O
t	O
t	O
with	O
the	O
n	O
case	O
handled	O
by	O
gt	O
the	O
usual	O
action-value	O
update	O
rule	O
from	O
n-step	B
sarsa	B
rt	O
and	O
this	O
target	O
is	O
then	O
used	O
with	O
qtnst	O
at	O
qtn	O
at	O
qtn	O
at	O
t	O
t	O
while	O
the	O
values	O
of	O
all	O
other	O
state	B
action	B
pairs	O
remain	O
unchanged	O
qtns	O
a	O
qtn	O
a	O
for	O
all	O
s	O
a	O
such	O
that	O
st	O
or	O
at	O
pseudocode	O
for	O
this	O
algorithm	O
is	O
shown	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
exercise	O
show	O
that	O
if	O
the	O
approximate	O
action	B
values	O
are	O
unchanging	O
then	O
the	O
tree-backup	O
return	B
can	O
be	O
written	O
as	O
a	O
sum	O
of	O
expectation-based	O
td	B
errors	O
gttn	O
qst	O
at	O
mintn	O
k	O
where	O
t	O
qst	O
at	O
and	O
vt	O
is	O
given	O
by	O
chapter	O
n-step	B
bootstrapping	B
n-step	B
tree	B
backup	I
for	O
estimating	O
q	O
q	O
or	O
q	O
initialize	O
qs	O
a	O
arbitrarily	O
for	O
all	O
s	O
s	O
a	O
a	O
initialize	O
to	O
be	O
greedy	O
with	O
respect	O
to	O
q	O
or	O
as	O
a	O
fixed	O
given	O
policy	B
algorithm	O
parameters	O
step	O
size	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
can	O
take	O
their	O
index	O
mod	O
n	O
loop	O
for	O
each	O
episode	O
if	O
t	O
t	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
and	O
state	B
as	O
if	O
is	O
terminal	O
else	O
t	O
t	O
choose	O
an	O
action	B
arbitrarily	O
as	O
a	O
function	O
of	O
store	O
initialize	O
and	O
store	O
terminal	O
choose	O
an	O
action	B
arbitrarily	O
as	O
a	O
function	O
of	O
store	O
t	O
loop	O
for	O
t	O
until	O
t	O
g	O
a	O
g	O
rk	O
t	O
n	O
if	O
if	O
t	O
t	O
g	O
rt	O
else	O
loop	O
for	O
k	O
mint	O
t	O
down	O
through	O
qs	O
a	O
qs	O
a	O
qs	O
a	O
if	O
is	O
being	O
learned	O
then	O
ensure	O
that	O
is	O
greedy	O
wrt	O
q	O
a	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
unifying	O
algorithm	O
n-step	B
q	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
three	O
different	O
kinds	O
of	O
action-value	O
algorithms	O
corresponding	O
to	O
the	O
first	O
three	O
backup	O
diagrams	O
shown	O
in	O
figure	O
n-step	B
sarsa	B
has	O
all	O
sample	O
transitions	O
the	O
tree-backup	O
algorithm	O
has	O
all	O
state-to-action	O
transitions	O
fully	O
branched	O
without	O
sampling	O
and	O
n-step	B
expected	B
sarsa	B
has	O
all	O
sample	O
transitions	O
except	O
for	O
the	O
last	O
state-to-action	O
one	O
which	O
is	O
fully	O
branched	O
with	O
an	O
expected	B
value	B
to	O
what	O
extent	O
can	O
these	O
algorithms	O
be	O
unified	O
one	O
idea	O
for	O
unification	O
is	O
suggested	O
by	O
the	O
fourth	O
backup	B
diagram	I
in	O
figure	O
this	O
is	O
the	O
idea	O
that	O
one	O
might	O
decide	O
on	O
a	O
step-by-step	O
basis	O
whether	O
one	O
wanted	O
to	O
take	O
the	O
action	B
as	O
a	O
sample	O
as	O
in	O
sarsa	B
or	O
consider	O
the	O
expectation	O
over	O
all	O
actions	O
instead	O
as	O
in	O
the	O
tree-backup	O
update	O
then	O
if	O
one	O
chose	O
always	O
to	O
sample	O
one	O
would	O
obtain	O
sarsa	B
whereas	O
if	O
one	O
chose	O
never	O
to	O
sample	O
one	O
would	O
get	O
the	O
tree-backup	O
algorithm	O
expected	B
sarsa	B
would	O
be	O
the	O
case	O
where	O
one	O
chose	O
to	O
sample	O
for	O
all	O
steps	O
except	O
for	O
a	O
unifying	O
algorithm	O
n-step	B
q	O
figure	O
the	O
backup	O
diagrams	O
of	O
the	O
three	O
kinds	O
of	O
n-step	B
action-value	O
updates	O
considered	O
so	O
far	O
in	O
this	O
chapter	O
case	O
plus	O
the	O
backup	B
diagram	I
of	O
a	O
fourth	O
kind	O
of	B
update	I
that	O
unifies	O
them	O
all	O
the	O
s	O
indicate	O
half	O
transitions	O
on	O
which	O
importance	B
sampling	I
is	O
required	O
in	O
the	O
off-policy	B
case	O
the	O
fourth	O
kind	O
of	B
update	I
unifies	O
all	O
the	O
others	O
by	O
choosing	O
on	O
a	O
state-by-state	O
basis	O
whether	O
to	O
sample	O
t	O
or	O
not	O
t	O
the	O
last	O
one	O
and	O
of	O
course	O
there	O
would	O
be	O
many	O
other	O
possibilities	O
as	O
suggested	O
by	O
the	O
last	O
diagram	O
in	O
the	O
figure	O
to	O
increase	O
the	O
possibilities	O
even	O
further	O
we	O
can	O
consider	O
a	O
continuous	O
variation	O
between	O
sampling	O
and	O
expectation	O
let	O
t	O
denote	O
the	O
degree	O
of	O
sampling	O
on	O
step	O
t	O
with	O
denoting	O
full	O
sampling	O
and	O
denoting	O
a	O
pure	O
expectation	O
with	O
no	O
sampling	O
the	O
random	O
variable	O
t	O
might	O
be	O
set	O
as	O
a	O
function	O
of	O
the	O
state	B
action	B
or	O
state	B
action	B
pair	O
at	O
time	O
t	O
we	O
call	O
this	O
proposed	O
new	O
algorithm	O
n-step	B
q	O
now	O
let	O
us	O
develop	O
the	O
equations	O
of	O
n-step	B
q	O
first	O
we	O
write	O
the	O
tree-backup	O
n-step	B
return	B
in	O
terms	O
of	O
the	O
horizon	O
h	O
t	O
n	O
and	O
then	O
the	O
expected	B
approximate	I
value	B
v	O
gth	O
a	O
vh	O
qh	O
vh	O
after	O
which	O
it	O
is	O
exactly	O
like	O
the	O
n-step	B
return	B
for	B
sarsa	B
with	B
control	B
variates	I
except	O
with	O
the	O
action	B
probability	O
substituted	O
for	O
the	O
importance-sampling	O
ratio	B
q	O
chapter	O
n-step	B
bootstrapping	B
for	O
q	O
we	O
slide	O
linearly	O
between	O
these	O
two	O
cases	O
gth	O
qh	O
vh	O
for	O
t	O
h	O
t	O
the	O
recursion	O
ends	O
with	O
ghh	O
rt	O
if	O
h	O
t	O
then	O
we	O
the	O
usual	O
general	O
update	O
for	B
n-step	B
sarsa	B
a	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
if	O
h	O
t	O
or	O
with	O
gt	O
off-policy	B
n-step	B
q	O
for	O
estimating	O
q	O
q	O
or	O
q	O
input	O
an	O
arbitrary	O
behavior	O
policy	B
b	O
such	O
that	O
bas	O
for	O
all	O
s	O
s	O
a	O
a	O
initialize	O
qs	O
a	O
arbitrarily	O
for	O
all	O
s	O
s	O
a	O
a	O
initialize	O
to	O
be	O
with	O
respect	O
to	O
q	O
or	O
as	O
a	O
fixed	O
given	O
policy	B
algorithm	O
parameters	O
step	O
size	O
small	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
can	O
take	O
their	O
index	O
mod	O
n	O
loop	O
for	O
each	O
episode	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
and	O
state	B
as	O
if	O
is	O
terminal	O
else	O
t	O
t	O
choose	O
and	O
store	O
an	O
action	B
b	O
select	O
and	O
store	O
store	O
if	O
t	O
t	O
initialize	O
and	O
store	O
terminal	O
choose	O
and	O
store	O
an	O
action	B
b	O
t	O
loop	O
for	O
t	O
until	O
t	O
as	O
g	O
rt	O
if	O
k	O
t	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
t	O
n	O
if	O
g	O
loop	O
for	O
k	O
mint	O
t	O
down	O
through	O
else	O
v	O
a	O
g	O
rk	O
k	O
k	O
k	O
qsk	O
v	O
qs	O
a	O
qs	O
a	O
qs	O
a	O
if	O
is	O
being	O
learned	O
then	O
ensure	O
that	O
is	O
greedy	O
wrt	O
q	O
summary	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
developed	O
a	O
range	O
of	O
temporal-difference	B
learning	I
methods	O
that	O
lie	O
in	O
between	O
the	O
one-step	O
td	B
methods	O
of	O
the	O
previous	O
chapter	O
and	O
the	O
monte	B
carlo	I
methods	I
of	O
the	O
chapter	O
before	O
methods	O
that	O
involve	O
an	O
intermediate	O
amount	O
of	O
bootstrapping	B
are	O
important	O
because	O
they	O
will	O
typically	O
perform	O
better	O
than	O
either	O
extreme	O
our	O
focus	O
in	O
this	O
chapter	O
has	O
been	O
on	O
n-step	B
methods	I
which	O
look	O
ahead	O
to	O
the	O
next	O
n	O
rewards	O
states	O
and	O
actions	O
the	O
two	O
backup	O
diagrams	O
to	O
the	O
right	O
together	O
summarize	O
most	O
of	O
the	O
methods	O
introduced	O
the	O
state-value	O
update	O
shown	O
is	O
for	O
nstep	O
td	B
with	O
importance	B
sampling	I
and	O
the	O
action-value	O
update	O
is	O
for	O
n-step	B
q	O
which	O
generalizes	O
expected	B
sarsa	B
and	O
q-learning	B
all	O
n-step	B
methods	I
involve	O
a	O
delay	O
of	O
n	O
time	O
steps	O
before	O
updating	O
as	O
only	O
then	O
are	O
all	O
the	O
required	O
future	O
events	O
known	O
a	O
further	O
drawback	O
is	O
that	O
they	O
involve	O
more	O
computation	O
per	O
time	O
step	O
than	O
previous	O
methods	O
compared	O
to	O
one-step	O
methods	O
n-step	B
methods	I
also	O
require	O
more	O
memory	O
to	O
record	O
the	O
states	O
actions	O
rewards	O
and	O
sometimes	O
other	O
variables	O
over	O
the	O
last	O
n	O
time	O
steps	O
eventually	O
in	O
chapter	O
we	O
will	O
see	O
how	O
multi-step	O
td	B
methods	O
can	O
be	O
implemented	O
with	O
minimal	O
memory	O
and	O
computational	O
complexity	O
using	O
eligibility	B
traces	I
but	O
there	O
will	O
always	O
be	O
some	O
additional	O
computation	O
beyond	O
one-step	O
methods	O
such	O
costs	O
can	O
be	O
well	O
worth	O
paying	O
to	O
escape	O
the	O
tyranny	O
of	O
the	O
single	O
time	O
step	O
although	O
n-step	B
methods	I
are	O
more	O
complex	O
than	O
those	O
using	O
eligibility	B
traces	I
they	O
have	O
the	O
great	O
benefit	O
of	O
being	O
conceptually	O
clear	O
we	O
have	O
sought	O
to	O
take	O
advantage	O
of	O
this	O
by	O
developing	O
two	O
approaches	O
to	O
off-policy	B
learning	O
in	O
the	O
n-step	B
case	O
one	O
based	O
on	O
importance	B
sampling	I
is	O
conceptually	O
simple	O
but	O
can	O
be	O
of	O
high	O
variance	O
if	O
the	O
target	O
and	O
behavior	O
policies	O
are	O
very	O
different	O
it	O
probably	O
needs	O
some	O
new	O
algorithmic	O
ideas	O
before	O
it	O
can	O
be	O
efficient	O
and	O
practical	O
the	O
other	O
based	O
on	O
tree-backup	O
updates	O
is	O
the	O
natural	O
extension	O
of	O
q-learning	B
to	O
the	O
multi-step	O
case	O
with	O
stochastic	O
target	O
policies	O
it	O
involves	O
no	O
importance	B
sampling	I
but	O
again	O
if	O
the	O
target	O
and	O
behavior	O
policies	O
are	O
substantially	O
different	O
the	O
bootstrapping	B
may	O
span	O
only	O
a	O
few	O
steps	O
even	O
if	O
n	O
is	O
large	O
q	O
chapter	O
n-step	B
bootstrapping	B
bibliographical	O
and	O
historical	O
remarks	O
the	O
notion	O
of	O
n-step	B
returns	O
is	O
due	O
to	O
watkins	B
who	O
also	O
first	O
discussed	O
their	O
error	B
reduction	I
property	I
n-step	B
algorithms	O
were	O
explored	O
in	O
the	O
first	O
edition	O
of	O
this	O
book	O
in	O
which	O
they	O
were	O
treated	O
as	O
of	O
conceptual	O
interest	O
but	O
not	O
feasible	O
in	O
practice	O
the	O
work	O
of	O
cichosz	O
and	O
particularly	O
van	O
seijen	O
showed	O
that	O
they	O
are	O
actually	O
completely	O
practical	O
algorithms	O
given	O
this	O
and	O
their	O
conceptual	O
clarity	O
and	O
simplicity	O
we	O
have	O
chosen	O
to	O
highlight	O
them	O
here	O
in	O
the	O
second	O
edition	O
in	O
particular	O
we	O
now	O
postpone	O
all	O
discussion	O
of	O
the	O
backward	O
view	O
and	O
of	O
eligibility	B
traces	I
until	O
chapter	O
the	O
results	O
in	O
the	O
random	B
walk	I
examples	O
were	O
made	O
for	O
this	O
text	O
based	O
on	O
work	O
of	O
sutton	O
and	O
singh	O
and	O
sutton	O
the	O
use	O
of	O
backup	O
diagrams	O
to	O
describe	O
these	O
and	O
other	O
algorithms	O
in	O
this	O
chapter	O
is	O
new	O
the	O
developments	O
in	O
these	O
sections	O
are	O
based	O
on	O
the	O
work	O
of	O
precup	O
sutton	O
and	O
singh	O
precup	O
sutton	O
and	O
dasgupta	O
and	O
sutton	O
mahmood	O
precup	O
and	O
van	O
hasselt	O
the	O
tree-backup	O
algorithm	O
is	O
due	O
to	O
precup	O
sutton	O
and	O
singh	O
but	O
the	O
presentation	O
of	O
it	O
here	O
is	O
new	O
the	O
q	O
algorithm	O
is	O
new	O
to	O
this	O
text	O
but	O
closely	O
related	O
algorithms	O
have	O
been	O
explored	O
further	O
by	O
de	O
asis	O
hernandez-garcia	O
holland	B
and	O
sutton	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
in	O
this	O
chapter	O
we	O
develop	O
a	O
unified	O
view	O
of	O
reinforcement	B
learning	I
methods	O
that	O
require	O
a	O
model	B
of	I
the	I
environment	B
such	O
as	O
dynamic	B
programming	I
and	O
heuristic	B
search	I
and	O
methods	O
that	O
can	O
be	O
used	O
without	O
a	O
model	O
such	O
as	O
monte	B
carlo	I
and	O
temporal-difference	O
methods	O
these	O
are	O
respectively	O
called	O
model-based	O
and	O
model-free	O
reinforcement	B
learning	I
methods	O
model-based	O
methods	O
rely	O
on	O
planning	B
as	O
their	O
primary	O
component	O
while	O
model-free	O
methods	O
primarily	O
rely	O
on	O
learning	O
although	O
there	O
are	O
real	O
differences	O
between	O
these	O
two	O
kinds	O
of	O
methods	O
there	O
are	O
also	O
great	O
similarities	O
in	O
particular	O
the	O
heart	O
of	O
both	O
kinds	O
of	O
methods	O
is	O
the	O
computation	O
of	O
value	B
functions	O
moreover	O
all	O
the	O
methods	O
are	O
based	O
on	O
looking	O
ahead	O
to	O
future	O
events	O
computing	O
a	O
backed-up	O
value	B
and	O
then	O
using	O
it	O
as	O
an	O
update	O
target	O
for	O
an	O
approximate	O
value	B
function	I
earlier	O
in	O
this	O
book	O
we	O
presented	O
monte	B
carlo	I
and	O
temporal-difference	O
methods	O
as	O
distinct	O
alternatives	O
then	O
showed	O
how	O
they	O
can	O
be	O
unified	O
by	O
n-step	B
methods	I
our	O
goal	O
in	O
this	O
chapter	O
is	O
a	O
similar	O
integration	O
of	O
model-based	B
and	I
model-free	I
methods	I
having	O
established	O
these	O
as	O
distinct	O
in	O
earlier	O
chapters	O
we	O
now	O
explore	O
the	O
extent	O
to	O
which	O
they	O
can	O
be	O
intermixed	O
models	O
and	O
planning	B
by	O
a	O
model	B
of	I
the	I
environment	B
we	O
mean	O
anything	O
that	O
an	O
agent	O
can	O
use	O
to	O
predict	O
how	O
the	O
environment	B
will	O
respond	O
to	O
its	O
actions	O
given	O
a	O
state	B
and	O
an	O
action	B
a	O
model	O
produces	O
a	O
prediction	B
of	O
the	O
resultant	O
next	O
state	B
and	O
next	O
reward	O
if	O
the	O
model	O
is	O
stochastic	O
then	O
there	O
are	O
several	O
possible	O
next	O
states	O
and	O
next	O
rewards	O
each	O
with	O
some	O
probability	O
of	O
occurring	O
some	O
models	O
produce	O
a	O
description	O
of	O
all	O
possibilities	O
and	O
their	O
probabilities	O
these	O
we	O
call	O
distribution	B
models	I
other	O
models	O
produce	O
just	O
one	O
of	O
the	O
possibilities	O
sampled	O
according	O
to	O
the	O
probabilities	O
these	O
we	O
call	O
sample	O
models	O
for	O
example	O
consider	O
modeling	O
the	O
sum	O
of	O
a	O
dozen	O
dice	O
a	O
distribution	O
model	O
would	O
produce	O
all	O
possible	O
sums	O
and	O
their	O
probabilities	O
of	O
occurring	O
whereas	O
a	O
sample	O
model	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
would	O
produce	O
an	O
individual	O
sum	O
drawn	O
according	O
to	O
this	O
probability	O
distribution	O
the	O
kind	O
of	O
model	O
assumed	O
in	O
dynamic	B
programming	I
estimates	O
of	O
the	O
mdp	O
s	O
dynamics	O
rs	O
a	O
is	O
a	O
distribution	O
model	O
the	O
kind	O
of	O
model	O
used	O
in	O
the	O
blackjack	B
example	I
in	O
chapter	O
is	O
a	O
sample	O
model	O
distribution	B
models	I
are	O
stronger	O
than	O
sample	O
models	O
in	O
that	O
they	O
can	O
always	O
be	O
used	O
to	O
produce	O
samples	O
however	O
in	O
many	O
applications	O
it	O
is	O
much	O
easier	O
to	O
obtain	O
sample	O
models	O
than	O
distribution	B
models	I
the	O
dozen	O
dice	O
are	O
a	O
simple	O
example	O
of	O
this	O
it	O
would	O
be	O
easy	O
to	O
write	O
a	O
computer	O
program	O
to	O
simulate	O
the	O
dice	O
rolls	O
and	O
return	B
the	O
sum	O
but	O
harder	O
and	O
more	O
error-prone	O
to	O
figure	O
out	O
all	O
the	O
possible	O
sums	O
and	O
their	O
probabilities	O
models	O
can	O
be	O
used	O
to	O
mimic	O
or	O
simulate	O
experience	O
given	O
a	O
starting	O
state	B
and	O
action	B
a	O
sample	O
model	O
produces	O
a	O
possible	O
transition	O
and	O
a	O
distribution	O
model	O
generates	O
all	O
possible	O
transitions	O
weighted	O
by	O
their	O
probabilities	O
of	O
occurring	O
given	O
a	O
starting	O
state	B
and	O
a	O
policy	B
a	O
sample	O
model	O
could	O
produce	O
an	O
entire	O
episode	O
and	O
a	O
distribution	O
model	O
could	O
generate	O
all	O
possible	O
episodes	B
and	O
their	O
probabilities	O
in	O
either	O
case	O
we	O
say	O
the	O
model	O
is	O
used	O
to	O
simulate	O
the	O
environment	B
and	O
produce	O
simulated	O
experience	O
the	O
word	O
planning	B
is	O
used	O
in	O
several	O
different	O
ways	O
in	O
different	O
fields	O
we	O
use	O
the	O
term	O
to	O
refer	O
to	O
any	O
computational	O
process	O
that	O
takes	O
a	O
model	O
as	O
input	O
and	O
produces	O
or	O
improves	O
a	O
policy	B
for	O
interacting	O
with	O
the	O
modeled	O
environment	B
in	O
artificial	B
intelligence	I
there	O
are	O
two	O
distinct	O
approaches	O
to	O
planning	B
according	O
to	O
our	O
definition	O
state-space	O
planning	B
which	O
includes	O
the	O
approach	O
we	O
take	O
in	O
this	O
book	O
is	O
viewed	O
primarily	O
as	O
a	O
search	O
through	O
the	O
state	B
space	O
for	O
an	O
optimal	O
policy	B
or	O
an	O
optimal	O
path	O
to	O
a	O
goal	O
actions	O
cause	O
transitions	O
from	O
state	B
to	O
state	B
and	O
value	B
functions	O
are	O
computed	O
over	O
states	O
in	O
what	O
we	O
call	O
plan-space	O
planning	B
planning	B
is	O
instead	O
a	O
search	O
through	O
the	O
space	O
of	O
plans	O
operators	O
transform	O
one	O
plan	O
into	O
another	O
and	O
value	B
functions	O
if	O
any	O
are	O
defined	O
over	O
the	O
space	O
of	O
plans	O
plan-space	O
planning	B
includes	O
evolutionary	B
methods	I
and	O
partial-order	O
planning	B
a	O
common	O
kind	O
of	O
planning	B
in	O
artificial	B
intelligence	I
in	O
which	O
the	O
ordering	O
of	O
steps	O
is	O
not	O
completely	O
determined	O
at	O
all	O
stages	O
of	O
planning	B
plan-space	O
methods	O
are	O
difficult	O
to	O
apply	O
efficiently	O
to	O
the	O
stochastic	O
sequential	O
decision	O
problems	O
that	O
are	O
the	O
focus	O
in	O
reinforcement	B
learning	I
and	O
we	O
do	O
not	O
consider	O
them	O
further	O
see	O
e	O
g	O
russell	O
and	O
norvig	O
the	O
unified	O
view	O
we	O
present	O
in	O
this	O
chapter	O
is	O
that	O
all	O
state-space	O
planning	B
methods	O
share	O
a	O
common	O
structure	O
a	O
structure	O
that	O
is	O
also	O
present	O
in	O
the	O
learning	O
methods	O
presented	O
in	O
this	O
book	O
it	O
takes	O
the	O
rest	O
of	O
the	O
chapter	O
to	O
develop	O
this	O
view	O
but	O
there	O
are	O
two	O
basic	O
ideas	O
all	O
state-space	O
planning	B
methods	O
involve	O
computing	O
value	B
functions	O
as	O
a	O
key	O
intermediate	O
step	O
toward	O
improving	O
the	O
policy	B
and	O
they	O
compute	O
value	B
functions	O
by	O
updates	O
or	O
backup	O
operations	O
applied	O
to	O
simulated	O
experience	O
this	O
common	O
structure	O
can	O
be	O
diagrammed	O
as	O
follows	O
dynamic	B
programming	I
methods	O
clearly	O
fit	O
this	O
structure	O
they	O
make	O
sweeps	O
through	O
the	O
space	O
of	O
states	O
generating	O
for	O
each	O
state	B
the	O
distribution	O
of	O
possible	O
transitions	O
each	O
planningmodelpolicyvaluesbackupsmodelsimulatedexperiencepolicyupdatesbackups	O
dyna	O
integrated	O
planning	B
acting	O
and	O
learning	O
distribution	O
is	O
then	O
used	O
to	O
compute	O
a	O
backed-up	O
value	B
target	O
and	O
update	O
the	O
state	B
s	O
estimated	O
value	B
in	O
this	O
chapter	O
we	O
argue	O
that	O
various	O
other	O
state-space	O
planning	B
methods	O
also	O
fit	O
this	O
structure	O
with	O
individual	O
methods	O
differing	O
only	O
in	O
the	O
kinds	O
of	O
updates	O
they	O
do	O
the	O
order	O
in	O
which	O
they	O
do	O
them	O
and	O
in	O
how	O
long	O
the	O
backed-up	O
information	O
is	O
retained	O
viewing	O
planning	B
methods	O
in	O
this	O
way	O
emphasizes	O
their	O
relationship	O
to	O
the	O
learning	O
methods	O
that	O
we	O
have	O
described	O
in	O
this	O
book	O
the	O
heart	O
of	O
both	O
learning	O
and	O
planning	B
methods	O
is	O
the	O
estimation	O
of	O
value	B
functions	O
by	O
backing-up	O
update	O
operations	O
the	O
difference	O
is	O
that	O
whereas	O
planning	B
uses	O
simulated	O
experience	O
generated	O
by	O
a	O
model	O
learning	O
methods	O
use	O
real	O
experience	O
generated	O
by	O
the	O
environment	B
of	O
course	O
this	O
difference	O
leads	O
to	O
a	O
number	O
of	O
other	O
differences	O
for	O
example	O
in	O
how	O
performance	O
is	O
assessed	O
and	O
in	O
how	O
flexibly	O
experience	O
can	O
be	O
generated	O
but	O
the	O
common	O
structure	O
means	O
that	O
many	O
ideas	O
and	O
algorithms	O
can	O
be	O
transferred	O
between	O
planning	B
and	O
learning	O
in	O
particular	O
in	O
many	O
cases	O
a	O
learning	O
algorithm	O
can	O
be	O
substituted	O
for	O
the	O
key	O
update	O
step	O
of	O
a	O
planning	B
method	O
learning	O
methods	O
require	O
only	O
experience	O
as	O
input	O
and	O
in	O
many	O
cases	O
they	O
can	O
be	O
applied	O
to	O
simulated	O
experience	O
just	O
as	O
well	O
as	O
to	O
real	O
experience	O
the	O
box	O
below	O
shows	O
a	O
simple	O
example	O
of	O
a	O
planning	B
method	O
based	O
on	O
one-step	O
tabular	O
q-learning	B
and	O
on	O
random	O
samples	O
from	O
a	O
sample	O
model	O
this	O
method	O
which	O
we	O
call	O
random-sample	O
one-step	O
tabular	O
q-planning	B
converges	O
to	O
the	O
optimal	O
policy	B
for	O
the	O
model	O
under	O
the	O
same	O
conditions	O
that	O
one-step	O
tabular	O
q-learning	B
converges	O
to	O
the	O
optimal	O
policy	B
for	O
the	O
real	O
environment	B
state	B
action	B
pair	O
must	O
be	O
selected	O
an	O
infinite	O
number	O
of	O
times	O
in	O
step	O
and	O
must	O
decrease	O
appropriately	O
over	O
time	O
random-sample	O
one-step	O
tabular	O
q-planning	B
loop	O
forever	O
select	O
a	O
state	B
s	O
s	O
and	O
an	O
action	B
a	O
as	O
at	O
random	O
send	O
s	O
a	O
to	O
a	O
sample	O
model	O
and	O
obtain	O
a	O
sample	O
next	O
reward	O
r	O
and	O
a	O
sample	O
next	O
state	B
apply	O
one-step	O
tabular	O
q-learning	B
to	O
s	O
a	O
r	O
qs	O
a	O
qs	O
a	O
maxa	O
a	O
qs	O
in	O
addition	O
to	O
the	O
unified	O
view	O
of	O
planning	B
and	O
learning	O
methods	O
a	O
second	O
theme	O
in	O
this	O
chapter	O
is	O
the	O
benefits	O
of	O
planning	B
in	O
small	O
incremental	O
steps	O
this	O
enables	O
planning	B
to	O
be	O
interrupted	O
or	O
redirected	O
at	O
any	O
time	O
with	O
little	O
wasted	O
computation	O
which	O
appears	O
to	O
be	O
a	O
key	O
requirement	O
for	O
efficiently	O
intermixing	O
planning	B
with	O
acting	O
and	O
with	O
learning	O
of	O
the	O
model	O
planning	B
in	O
very	O
small	O
steps	O
may	O
be	O
the	O
most	O
efficient	O
approach	O
even	O
on	O
pure	O
planning	B
problems	O
if	O
the	O
problem	O
is	O
too	O
large	O
to	O
be	O
solved	O
exactly	O
dyna	O
integrated	O
planning	B
acting	O
and	O
learning	O
when	O
planning	B
is	O
done	O
online	B
while	O
interacting	O
with	O
the	O
environment	B
a	O
number	O
of	O
interesting	O
issues	O
arise	O
new	O
information	O
gained	O
from	O
the	O
interaction	O
may	O
change	O
the	O
model	O
and	O
thereby	O
interact	O
with	O
planning	B
it	O
may	O
be	O
desirable	O
to	O
customize	O
the	O
planning	B
process	O
in	O
some	O
way	O
to	O
the	O
states	O
or	O
decisions	O
currently	O
under	O
consideration	O
or	O
expected	B
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
in	O
the	O
near	O
future	O
if	O
decision	O
making	O
and	O
model	O
learning	O
are	O
both	O
computation-intensive	O
processes	O
then	O
the	O
available	O
computational	O
resources	O
may	O
need	O
to	O
be	O
divided	O
between	O
them	O
to	O
begin	O
exploring	O
these	O
issues	O
in	O
this	O
section	O
we	O
present	O
dyna-q	O
a	O
simple	O
architecture	O
integrating	O
the	O
major	O
functions	O
needed	O
in	O
an	O
online	B
planning	B
agent	O
each	O
function	O
appears	O
in	O
dyna-q	O
in	O
a	O
simple	O
almost	O
trivial	O
form	O
in	O
subsequent	O
sections	O
we	O
elaborate	O
some	O
of	O
the	O
alternate	O
ways	O
of	O
achieving	O
each	O
function	O
and	O
the	O
trade-offs	O
between	O
them	O
for	O
now	O
we	O
seek	O
merely	O
to	O
illustrate	O
the	O
ideas	O
and	O
stimulate	O
your	O
intuition	O
within	O
a	O
planning	B
agent	O
there	O
are	O
at	O
least	O
two	O
roles	O
for	O
real	O
experience	O
it	O
can	O
be	O
used	O
to	O
improve	O
the	O
model	O
make	O
it	O
more	O
accurately	O
match	O
the	O
real	O
environment	B
and	O
it	O
can	O
be	O
used	O
to	O
directly	O
improve	O
the	O
value	B
function	I
and	O
policy	B
using	O
the	O
kinds	O
of	O
reinforcement	B
learning	I
methods	O
we	O
have	O
discussed	O
in	O
previous	O
chapters	O
the	O
former	O
we	O
call	O
modellearning	O
and	O
the	O
latter	O
we	O
call	O
direct	O
reinforcement	B
learning	I
rl	O
the	O
possible	O
relationships	O
between	O
experience	O
model	O
values	O
and	O
policy	B
are	O
summarized	O
in	O
the	O
diagram	O
to	O
the	O
right	O
each	O
arrow	O
shows	O
a	O
relationship	O
of	O
influence	O
and	O
presumed	O
improvement	O
note	O
how	O
experience	O
can	O
improve	O
value	B
functions	O
and	O
policies	O
either	O
directly	O
or	O
indirectly	O
via	O
the	O
model	O
it	O
is	O
the	O
latter	O
which	O
is	O
sometimes	O
called	O
indirect	O
reinforcement	B
learning	I
that	O
is	O
involved	O
in	O
planning	B
both	O
direct	O
and	O
indirect	O
methods	O
have	O
advantages	O
and	O
disadvantages	O
indirect	O
methods	O
often	O
make	O
fuller	O
use	O
of	O
a	O
limited	O
amount	O
of	O
experience	O
and	O
thus	O
achieve	O
a	O
better	O
policy	B
with	O
fewer	O
environmental	O
interactions	O
on	O
the	O
other	O
hand	O
direct	O
methods	O
are	O
much	O
simpler	O
and	O
are	O
not	O
affected	O
by	O
biases	O
in	O
the	O
design	B
of	I
the	O
model	O
some	O
have	O
argued	O
that	O
indirect	O
methods	O
are	O
always	O
superior	O
to	O
direct	O
ones	O
while	O
others	O
have	O
argued	O
that	O
direct	O
methods	O
are	O
responsible	O
for	O
most	O
human	O
and	O
animal	O
learning	O
related	O
debates	O
in	B
psychology	B
and	B
artificial	B
intelligence	I
concern	O
the	O
relative	O
importance	O
of	O
cognition	O
as	O
opposed	O
to	O
trial-and-error	O
learning	O
and	O
of	O
deliberative	O
planning	B
as	O
opposed	O
to	O
reactive	O
decision	O
making	O
chapter	O
for	O
discussion	O
of	O
some	O
of	O
these	O
issues	O
from	O
the	O
perspective	O
of	O
psychology	B
our	O
view	O
is	O
that	O
the	O
contrast	O
between	O
the	O
alternatives	O
in	O
all	O
these	O
debates	O
has	O
been	O
exaggerated	O
that	O
more	O
insight	O
can	O
be	O
gained	O
by	O
recognizing	O
the	O
similarities	O
between	O
these	O
two	O
sides	O
than	O
by	O
opposing	O
them	O
for	O
example	O
in	O
this	O
book	O
we	O
have	O
emphasized	O
the	O
deep	O
similarities	O
between	O
dynamic	B
programming	I
and	O
temporal-difference	O
methods	O
even	O
though	O
one	O
was	O
designed	O
for	O
planning	B
and	O
the	O
other	O
for	O
model-free	O
learning	O
dyna-q	O
includes	O
all	O
of	O
the	O
processes	O
shown	O
in	O
the	O
diagram	O
above	O
planning	B
acting	O
model-learning	O
and	O
direct	O
rl	O
all	O
occurring	O
continually	O
the	O
planning	B
method	O
is	O
the	O
random-sample	O
one-step	O
tabular	O
q-planning	B
method	O
on	O
page	O
the	O
direct	O
rl	O
method	O
is	O
one-step	O
tabular	O
q-learning	B
the	O
model-learning	O
method	O
is	O
also	O
table-based	O
and	O
assumes	O
the	O
environment	B
is	O
deterministic	O
after	O
each	O
transition	O
st	O
at	O
the	O
model	O
records	O
in	O
its	O
table	O
entry	O
for	O
st	O
at	O
the	O
prediction	B
that	O
will	O
deterministically	O
follow	O
thus	O
if	O
the	O
model	O
is	O
queried	O
with	O
a	O
state	B
action	B
pair	O
that	O
has	O
been	O
planningvaluepolicyexperiencemodelmodellearningactingdirectrl	O
dyna	O
integrated	O
planning	B
acting	O
and	O
learning	O
experienced	O
before	O
it	O
simply	O
returns	O
the	O
last-observed	O
next	O
state	B
and	O
next	O
reward	O
as	O
its	O
prediction	B
during	O
planning	B
the	O
q-planning	B
algorithm	O
randomly	O
samples	O
only	O
from	O
state	B
action	B
pairs	O
that	O
have	O
previously	O
been	O
experienced	O
step	O
so	O
the	O
model	O
is	O
never	O
queried	O
with	O
a	O
pair	O
about	O
which	O
it	O
has	O
no	O
information	O
the	O
overall	O
architecture	O
of	O
dyna	O
agents	O
of	O
which	O
the	O
dyna-q	O
algorithm	O
is	O
one	O
example	O
is	O
shown	O
in	O
figure	O
the	O
central	O
column	O
represents	O
the	O
basic	O
interaction	O
between	O
agent	O
and	O
environment	B
giving	O
rise	O
to	O
a	O
trajectory	O
of	O
real	O
experience	O
the	O
arrow	O
on	O
the	O
left	O
of	O
the	O
figure	O
represents	O
direct	O
reinforcement	B
learning	I
operating	O
on	O
real	O
experience	O
to	O
improve	O
the	O
value	B
function	I
and	O
the	O
policy	B
on	O
the	O
right	O
are	O
model-based	O
processes	O
the	O
model	O
is	O
learned	O
from	O
real	O
experience	O
and	O
gives	O
rise	O
to	O
simulated	O
experience	O
we	O
use	O
the	O
term	O
search	B
control	B
to	O
refer	O
to	O
the	O
process	O
that	O
selects	O
the	O
starting	O
states	O
and	O
actions	O
for	O
the	O
simulated	O
experiences	O
generated	O
by	O
the	O
model	O
finally	O
planning	B
is	O
achieved	O
by	O
applying	O
reinforcement	B
learning	I
methods	O
to	O
the	O
simulated	O
experiences	O
just	O
as	O
if	O
they	O
had	O
really	O
happened	O
typically	O
as	O
in	O
dyna-q	O
the	O
same	O
reinforcement	B
learning	I
method	O
is	O
used	O
both	O
for	O
learning	O
from	O
real	O
experience	O
and	O
for	O
planning	B
from	O
simulated	O
experience	O
the	O
reinforcement	B
learning	I
method	O
is	O
thus	O
the	O
final	O
common	O
path	O
for	O
both	O
learning	O
and	O
planning	B
learning	O
and	O
planning	B
are	O
deeply	O
integrated	O
in	O
the	O
sense	O
that	O
they	O
share	O
almost	O
all	O
the	O
same	O
machinery	O
differing	O
only	O
in	O
the	O
source	O
of	O
their	O
experience	O
figure	O
the	O
general	O
dyna	B
architecture	I
real	O
experience	O
passing	O
back	O
and	O
forth	O
between	O
the	O
environment	B
and	O
the	O
policy	B
affects	O
policy	B
and	O
value	B
functions	O
in	O
much	O
the	O
same	O
way	O
as	O
does	O
simulated	O
experience	O
generated	O
by	O
the	O
model	B
of	I
the	I
environment	B
conceptually	O
planning	B
acting	O
model-learning	O
and	O
direct	O
rl	O
occur	O
simultaneously	O
and	O
in	O
parallel	O
in	O
dyna	O
agents	O
for	O
concreteness	O
and	O
implementation	O
on	O
a	O
serial	O
computer	O
however	O
we	O
fully	O
specify	O
the	O
order	O
in	O
which	O
they	O
occur	O
within	O
a	O
time	O
step	O
in	O
dyna-q	O
the	O
acting	O
model-learning	O
and	O
direct	O
rl	O
processes	O
require	O
little	O
computation	O
and	O
we	O
assume	O
they	O
consume	O
just	O
a	O
fraction	O
of	O
the	O
time	O
the	O
remaining	O
time	O
in	O
each	O
step	O
can	O
be	O
devoted	O
to	O
the	O
planning	B
process	O
which	O
is	O
inherently	O
computation-intensive	O
realdirect	O
rlupdatemodelplanning	O
updatesearchcontrolpolicyvalue	O
functionsexperiencemodellearningenvironmentsimulatedexperience	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
let	O
us	O
assume	O
that	O
there	O
is	O
time	O
in	O
each	O
step	O
after	O
acting	O
model-learning	O
and	O
direct	O
rl	O
to	O
complete	O
n	O
iterations	O
of	O
the	O
q-planning	B
algorithm	O
in	O
the	O
pseudocode	O
algorithm	O
for	O
dyna-q	O
in	O
the	O
box	O
below	O
m	O
odels	O
a	O
denotes	O
the	O
contents	O
of	O
the	O
next	O
state	B
and	O
reward	O
for	O
state	B
action	B
pair	O
a	O
direct	O
reinforcement	B
learning	I
model-learning	O
and	O
planning	B
are	O
implemented	O
by	O
steps	O
and	O
respectively	O
if	O
and	O
were	O
omitted	O
the	O
remaining	O
algorithm	O
would	O
be	O
one-step	O
tabular	O
q-learning	B
tabular	O
dyna-q	O
initialize	O
qs	O
a	O
and	O
m	O
odels	O
a	O
for	O
all	O
s	O
s	O
and	O
a	O
as	O
loop	O
forever	O
s	O
current	O
state	B
a	O
q	O
take	O
action	B
a	O
observe	O
resultant	O
reward	O
r	O
and	O
state	B
qs	O
a	O
qs	O
a	O
maxa	O
a	O
qs	O
m	O
odels	O
a	O
r	O
deterministic	O
environment	B
loop	O
repeat	O
n	O
times	O
s	O
random	O
previously	O
observed	O
state	B
a	O
random	O
action	B
previously	O
taken	O
in	O
s	O
r	O
m	O
odels	O
a	O
qs	O
a	O
qs	O
a	O
maxa	O
a	O
qs	O
example	O
dyna	B
maze	I
consider	O
the	O
simple	O
maze	O
shown	O
inset	O
in	O
figure	O
in	O
each	O
of	O
the	O
states	O
there	O
are	O
four	O
actions	O
up	O
down	O
right	O
and	O
left	O
which	O
take	O
the	O
agent	O
deterministically	O
to	O
the	O
corresponding	O
neighboring	O
states	O
except	O
when	O
movement	O
is	O
blocked	O
by	O
an	O
obstacle	O
or	O
the	O
edge	O
of	O
the	O
maze	O
in	O
which	O
case	O
the	O
agent	O
remains	O
where	O
it	O
is	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
except	O
those	O
into	O
the	O
goal	O
state	B
on	O
which	O
it	O
is	O
after	O
reaching	O
the	O
goal	O
state	B
the	O
agent	O
returns	O
to	O
the	O
start	O
state	B
to	O
begin	O
a	O
new	O
episode	O
this	O
is	O
a	O
discounted	O
episodic	O
task	O
with	O
the	O
main	O
part	O
of	O
figure	O
shows	O
average	O
learning	O
curves	O
from	O
an	O
experiment	O
in	O
which	O
dyna-q	O
agents	O
were	O
applied	O
to	O
the	O
maze	O
task	O
the	O
initial	O
action	B
values	O
were	O
zero	O
the	O
step-size	B
parameter	I
was	O
and	O
the	O
exploration	O
parameter	O
was	O
when	O
selecting	O
greedily	O
among	O
actions	O
ties	O
were	O
broken	O
randomly	O
the	O
agents	O
varied	O
in	O
the	O
number	O
of	O
planning	B
steps	O
n	O
they	O
performed	O
per	O
real	O
step	O
for	O
each	O
n	O
the	O
curves	O
show	O
the	O
number	O
of	O
steps	O
taken	O
by	O
the	O
agent	O
to	O
reach	O
the	O
goal	O
in	O
each	O
episode	O
averaged	O
over	O
repetitions	O
of	O
the	O
experiment	O
in	O
each	O
repetition	O
the	O
initial	O
seed	O
for	O
the	O
random	O
number	O
generator	O
was	O
held	O
constant	O
across	O
algorithms	O
because	O
of	O
this	O
the	O
first	O
episode	O
was	O
exactly	O
the	O
same	O
steps	O
for	O
all	O
values	O
of	O
n	O
and	O
its	O
data	O
are	O
not	O
shown	O
in	O
the	O
figure	O
after	O
the	O
first	O
episode	O
performance	O
improved	O
for	O
all	O
values	O
of	O
n	O
but	O
much	O
more	O
rapidly	O
for	O
larger	O
values	O
recall	O
that	O
the	O
n	O
agent	O
is	O
a	O
nonplanning	O
agent	O
using	O
only	O
direct	O
reinforcement	B
learning	I
tabular	O
q-learning	B
this	O
was	O
by	O
far	O
the	O
slowest	O
agent	O
on	O
this	O
problem	O
despite	O
the	O
fact	O
that	O
the	O
parameter	O
values	O
and	O
were	O
optimized	O
for	O
it	O
the	O
nonplanning	O
agent	O
took	O
about	O
episodes	B
to	O
reach	O
performance	O
whereas	O
the	O
n	O
agent	O
took	O
about	O
five	O
episodes	B
and	O
the	O
n	O
agent	O
took	O
only	O
three	O
episodes	B
dyna	O
integrated	O
planning	B
acting	O
and	O
learning	O
figure	O
a	O
simple	O
maze	O
and	O
the	O
average	O
learning	O
curves	O
for	O
dyna-q	O
agents	O
varying	O
in	O
their	O
number	O
of	O
planning	B
steps	O
per	O
real	O
step	O
the	O
task	O
is	O
to	O
travel	O
from	O
s	O
to	O
g	O
as	O
quickly	O
as	O
possible	O
figure	O
shows	O
why	O
the	O
planning	B
agents	O
found	O
the	O
solution	O
so	O
much	O
faster	O
than	O
the	O
nonplanning	O
agent	O
shown	O
are	O
the	O
policies	O
found	O
by	O
the	O
n	O
and	O
n	O
agents	O
halfway	O
through	O
the	O
second	O
episode	O
without	O
planning	B
each	O
episode	O
adds	O
only	O
one	O
additional	O
step	O
to	O
the	O
policy	B
and	O
so	O
only	O
one	O
step	O
last	O
has	O
been	O
learned	O
so	O
far	O
with	O
planning	B
again	O
only	O
one	O
step	O
is	O
learned	O
during	O
the	O
first	O
episode	O
but	O
here	O
during	O
the	O
second	O
episode	O
an	O
extensive	O
policy	B
has	O
been	O
developed	O
that	O
by	O
the	O
end	O
of	O
the	O
episode	O
will	O
reach	O
almost	O
back	O
to	O
the	O
start	O
state	B
this	O
policy	B
is	O
built	O
by	O
the	O
planning	B
process	O
while	O
the	O
agent	O
is	O
still	O
wandering	O
near	O
the	O
start	O
state	B
by	O
the	O
end	O
of	O
the	O
third	O
episode	O
a	O
complete	O
optimal	O
policy	B
will	O
have	O
been	O
found	O
and	O
perfect	O
performance	O
attained	O
figure	O
policies	O
found	O
by	O
planning	B
and	O
nonplanning	O
dyna-q	O
agents	O
halfway	O
through	O
the	O
second	O
episode	O
the	O
arrows	O
indicate	O
the	O
greedy	O
action	B
in	O
each	O
state	B
if	O
no	O
arrow	O
is	O
shown	O
for	O
a	O
state	B
then	O
all	O
of	O
its	O
action	B
values	O
were	O
equal	O
the	O
black	O
square	O
indicates	O
the	O
location	O
of	O
the	O
agent	O
rl	O
planning	B
steps	O
planning	B
planning	B
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
in	O
dyna-q	O
learning	O
and	O
planning	B
are	O
accomplished	O
by	O
exactly	O
the	O
same	O
algorithm	O
operating	O
on	O
real	O
experience	O
for	O
learning	O
and	O
on	O
simulated	O
experience	O
for	O
planning	B
because	O
planning	B
proceeds	O
incrementally	O
it	O
is	O
trivial	O
to	O
intermix	O
planning	B
and	O
acting	O
both	O
proceed	O
as	O
fast	O
as	O
they	O
can	O
the	O
agent	O
is	O
always	O
reactive	O
and	O
always	O
deliberative	O
responding	O
instantly	O
to	O
the	O
latest	O
sensory	O
information	O
and	O
yet	O
always	O
planning	B
in	O
the	O
background	O
also	O
ongoing	O
in	O
the	O
background	O
is	O
the	O
model-learning	O
process	O
as	O
new	O
information	O
is	O
gained	O
the	O
model	O
is	O
updated	O
to	O
better	O
match	O
reality	O
as	O
the	O
model	O
changes	O
the	O
ongoing	O
planning	B
process	O
will	O
gradually	O
compute	O
a	O
different	O
way	O
of	O
behaving	O
to	O
match	O
the	O
new	O
model	O
exercise	O
the	O
nonplanning	O
method	O
looks	O
particularly	O
poor	O
in	O
figure	O
because	O
it	O
is	O
a	O
one-step	O
method	O
a	O
method	O
using	O
multi-step	O
bootstrapping	B
would	O
do	O
better	O
do	O
you	O
think	O
one	O
of	O
the	O
multi-step	O
bootstrapping	B
methods	O
from	O
chapter	O
could	O
do	O
as	O
well	O
as	O
the	O
dyna	O
method	O
explain	O
why	O
or	O
why	O
not	O
when	O
the	O
model	O
is	O
wrong	O
in	O
the	O
maze	O
example	O
presented	O
in	O
the	O
previous	O
section	O
the	O
changes	O
in	O
the	O
model	O
were	O
relatively	O
modest	O
the	O
model	O
started	O
out	O
empty	O
and	O
was	O
then	O
filled	O
only	O
with	O
exactly	O
correct	O
information	O
in	O
general	O
we	O
cannot	O
expect	O
to	O
be	O
so	O
fortunate	O
models	O
may	O
be	O
incorrect	O
because	O
the	O
environment	B
is	O
stochastic	O
and	O
only	O
a	O
limited	O
number	O
of	O
samples	O
have	O
been	O
observed	O
or	O
because	O
the	O
model	O
was	O
learned	O
using	O
function	B
approximation	I
that	O
has	O
generalized	O
imperfectly	O
or	O
simply	O
because	O
the	O
environment	B
has	O
changed	O
and	O
its	O
new	O
behavior	O
has	O
not	O
yet	O
been	O
observed	O
when	O
the	O
model	O
is	O
incorrect	O
the	O
planning	B
process	O
is	O
likely	O
to	O
compute	O
a	O
suboptimal	O
policy	B
in	O
some	O
cases	O
the	O
suboptimal	O
policy	B
computed	O
by	O
planning	B
quickly	O
leads	O
to	O
the	O
discovery	O
and	O
correction	O
of	O
the	O
modeling	O
error	O
this	O
tends	O
to	O
happen	O
when	O
the	O
model	O
is	O
optimistic	O
in	O
the	O
sense	O
of	O
predicting	O
greater	O
reward	O
or	O
better	O
state	B
transitions	O
than	O
are	O
actually	O
possible	O
the	O
planned	O
policy	B
attempts	O
to	O
exploit	O
these	O
opportunities	O
and	O
in	O
doing	O
so	O
discovers	O
that	O
they	O
do	O
not	O
exist	O
example	O
blocking	B
maze	O
a	O
maze	O
example	O
illustrating	O
this	O
relatively	O
minor	O
kind	O
of	O
modeling	O
error	O
and	O
recovery	O
from	O
it	O
is	O
shown	O
in	O
figure	O
initially	O
there	O
is	O
a	O
short	O
path	O
from	O
start	O
to	O
goal	O
to	O
the	O
right	O
of	O
the	O
barrier	O
as	O
shown	O
in	O
the	O
upper	O
left	O
of	O
the	O
figure	O
after	O
time	O
steps	O
the	O
short	O
path	O
is	O
blocked	O
and	O
a	O
longer	O
path	O
is	O
opened	O
up	O
along	O
the	O
left-hand	O
side	O
of	O
the	O
barrier	O
as	O
shown	O
in	O
upper	O
right	O
of	O
the	O
figure	O
the	O
graph	O
shows	O
average	O
cumulative	O
reward	O
for	O
a	O
dyna-q	O
agent	O
and	O
an	O
enhanced	O
dyna-q	O
agent	O
to	O
be	O
described	O
shortly	O
the	O
first	O
part	O
of	O
the	O
graph	O
shows	O
that	O
both	O
dyna	O
agents	O
found	O
the	O
short	O
path	O
within	O
steps	O
when	O
the	O
environment	B
changed	O
the	O
graphs	O
become	O
flat	O
indicating	O
a	O
period	O
during	O
which	O
the	O
agents	O
obtained	O
no	O
reward	O
because	O
they	O
were	O
wandering	O
around	O
behind	O
the	O
barrier	O
after	O
a	O
while	O
however	O
they	O
were	O
able	O
to	O
find	O
the	O
new	O
opening	O
and	O
the	O
new	O
optimal	O
behavior	O
greater	O
difficulties	O
arise	O
when	O
the	O
environment	B
changes	O
to	O
become	O
better	O
than	O
it	O
was	O
before	O
and	O
yet	O
the	O
formerly	O
correct	O
policy	B
does	O
not	O
reveal	O
the	O
improvement	O
in	O
these	O
cases	O
the	O
modeling	O
error	O
may	O
not	O
be	O
detected	O
for	O
a	O
long	O
time	O
if	O
ever	O
when	O
the	O
model	O
is	O
wrong	O
figure	O
average	O
performance	O
of	O
dyna	O
agents	O
on	O
a	O
blocking	B
task	O
the	O
left	O
environment	B
was	O
used	O
for	O
the	O
first	O
steps	O
the	O
right	O
environment	B
for	O
the	O
rest	O
dyna-q	O
is	O
dyna-q	O
with	O
an	O
exploration	O
bonus	O
that	O
encourages	O
exploration	O
example	O
shortcut	O
maze	O
the	O
problem	O
caused	O
by	O
this	O
kind	O
of	O
environmental	O
change	O
is	O
illustrated	O
by	O
the	O
maze	O
example	O
shown	O
in	O
figure	O
initially	O
the	O
optimal	O
path	O
is	O
to	O
go	O
around	O
the	O
left	O
side	O
of	O
the	O
barrier	O
left	O
after	O
steps	O
however	O
a	O
shorter	O
path	O
is	O
opened	O
up	O
along	O
the	O
right	O
side	O
without	O
disturbing	O
the	O
longer	O
path	O
right	O
the	O
graph	O
shows	O
that	O
the	O
regular	O
dyna-q	O
agent	O
never	O
switched	O
to	O
the	O
shortcut	O
in	O
fact	O
it	O
never	O
realized	O
that	O
it	O
existed	O
its	O
model	O
said	O
that	O
there	O
was	O
no	O
shortcut	O
so	O
the	O
more	O
it	O
planned	O
the	O
less	O
likely	O
it	O
was	O
to	O
step	O
to	O
the	O
right	O
and	O
discover	O
it	O
even	O
with	O
an	O
policy	B
it	O
is	O
very	O
unlikely	O
that	O
an	O
agent	O
will	O
take	O
so	O
many	O
exploratory	O
actions	O
as	O
to	O
discover	O
the	O
shortcut	O
figure	O
average	O
performance	O
of	O
dyna	O
agents	O
on	O
a	O
shortcut	O
task	O
the	O
left	O
environment	B
was	O
used	O
for	O
the	O
first	O
steps	O
the	O
right	O
environment	B
for	O
the	O
rest	O
the	O
general	O
problem	O
here	O
is	O
another	O
version	O
of	O
the	O
conflict	O
between	O
exploration	O
and	O
exploitation	O
in	O
a	O
planning	B
context	O
exploration	O
means	O
trying	O
actions	O
that	O
improve	O
the	O
model	O
whereas	O
exploitation	O
means	O
behaving	O
in	O
the	O
optimal	O
way	O
given	O
the	O
current	O
model	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
we	O
want	O
the	O
agent	O
to	O
explore	O
to	O
find	O
changes	O
in	O
the	O
environment	B
but	O
not	O
so	O
much	O
that	O
performance	O
is	O
greatly	O
degraded	O
as	O
in	O
the	O
earlier	O
explorationexploitation	O
conflict	O
there	O
probably	O
is	O
no	O
solution	O
that	O
is	O
both	O
perfect	O
and	O
practical	O
but	O
simple	O
heuristics	O
are	O
often	O
effective	O
the	O
dyna-q	O
agent	O
that	O
did	O
solve	O
the	O
shortcut	O
maze	O
uses	O
one	O
such	O
heuristic	O
this	O
agent	O
keeps	O
track	O
for	O
each	O
state	B
action	B
pair	O
of	O
how	O
many	O
time	O
steps	O
have	O
elapsed	O
since	O
the	O
pair	O
was	O
last	O
tried	O
in	O
a	O
real	O
interaction	O
with	O
the	O
environment	B
the	O
more	O
time	O
that	O
has	O
elapsed	O
the	O
greater	O
might	O
presume	O
the	O
chance	O
that	O
the	O
dynamics	O
of	O
this	O
pair	O
has	O
changed	O
and	O
that	O
the	O
model	O
of	O
it	O
is	O
incorrect	O
to	O
encourage	O
behavior	O
that	O
tests	O
longuntried	O
actions	O
a	O
special	O
bonus	O
reward	O
is	O
given	O
on	O
simulated	O
experiences	O
involving	O
these	O
actions	O
in	O
particular	O
if	O
the	O
modeled	O
reward	O
for	O
a	O
transition	O
is	O
r	O
and	O
the	O
transition	O
has	O
not	O
been	O
tried	O
in	O
time	O
steps	O
then	O
planning	B
updates	O
are	O
done	O
as	O
if	O
that	O
transition	O
produced	O
a	O
reward	O
of	O
r	O
for	O
some	O
small	O
this	O
encourages	O
the	O
agent	O
to	O
keep	O
testing	O
all	O
accessible	O
state	B
transitions	O
and	O
even	O
to	O
find	O
long	O
sequences	O
of	O
actions	O
in	O
order	O
to	O
carry	O
out	O
such	O
of	O
course	O
all	O
this	O
testing	O
has	O
its	O
cost	O
but	O
in	O
many	O
cases	O
as	O
in	O
the	O
shortcut	O
maze	O
this	O
kind	O
of	O
computational	O
curiosity	B
is	O
well	O
worth	O
the	O
extra	O
exploration	O
exercise	O
why	O
did	O
the	O
dyna	O
agent	O
with	O
exploration	O
bonus	O
dyna-q	O
perform	O
better	O
in	O
the	O
first	O
phase	O
as	O
well	O
as	O
in	O
the	O
second	O
phase	O
of	O
the	O
blocking	B
and	O
shortcut	O
experiments	O
exercise	O
careful	O
inspection	O
of	O
figure	O
reveals	O
that	O
the	O
difference	O
between	O
dynaq	O
and	O
dyna-q	O
narrowed	O
slightly	O
over	O
the	O
first	O
part	O
of	O
the	O
experiment	O
what	O
is	O
the	O
reason	O
for	O
this	O
exercise	O
the	O
exploration	O
bonus	O
described	O
above	O
actually	O
changes	O
the	O
estimated	O
values	O
of	O
states	O
and	O
actions	O
is	O
this	O
necessary	O
suppose	O
the	O
bonus	O
was	O
used	O
not	O
in	O
updates	O
but	O
solely	O
in	O
action	B
selection	O
that	O
is	O
suppose	O
the	O
action	B
selected	O
was	O
always	O
that	O
for	O
which	O
qst	O
a	O
a	O
was	O
maximal	O
carry	O
out	O
a	O
gridworld	O
experiment	O
that	O
tests	O
and	O
illustrates	O
the	O
strengths	O
and	O
weaknesses	O
of	O
this	O
alternate	O
approach	O
exercise	O
how	O
might	O
the	O
tabular	O
dyna-q	O
algorithm	O
shown	O
on	O
page	O
be	O
modified	O
to	O
handle	O
stochastic	O
environments	O
how	O
might	O
this	O
modification	O
perform	O
poorly	O
on	O
changing	O
environments	O
such	O
as	O
considered	O
in	O
this	O
section	O
how	O
could	O
the	O
algorithm	O
be	O
modified	O
to	O
handle	O
stochastic	O
environments	O
and	O
changing	O
environments	O
prioritized	B
sweeping	I
in	O
the	O
dyna	O
agents	O
presented	O
in	O
the	O
preceding	O
sections	O
simulated	O
transitions	O
are	O
started	O
in	O
state	B
action	B
pairs	O
selected	O
uniformly	O
at	O
random	O
from	O
all	O
previously	O
experienced	O
pairs	O
but	O
a	O
uniform	O
selection	O
is	O
usually	O
not	O
the	O
best	O
planning	B
can	O
be	O
much	O
more	O
efficient	O
if	O
simulated	O
transitions	O
and	O
updates	O
are	O
focused	O
on	O
particular	O
state	B
action	B
pairs	O
for	O
dyna-q	O
agent	O
was	O
changed	O
in	O
two	O
other	O
ways	O
as	O
well	O
first	O
actions	O
that	O
had	O
never	O
been	O
tried	O
before	O
from	O
a	O
state	B
were	O
allowed	O
to	O
be	O
considered	O
in	O
the	O
planning	B
step	O
of	O
the	O
tabular	O
dyna-q	O
algorithm	O
in	O
the	O
box	O
above	O
second	O
the	O
initial	O
model	O
for	O
such	O
actions	O
was	O
that	O
they	O
would	O
lead	O
back	O
to	O
the	O
same	O
state	B
with	O
a	O
reward	O
of	O
zero	O
prioritized	B
sweeping	I
example	O
consider	O
what	O
happens	O
during	O
the	O
second	O
episode	O
of	O
the	O
first	O
maze	O
task	O
at	O
the	O
beginning	O
of	O
the	O
second	O
episode	O
only	O
the	O
state	B
action	B
pair	O
leading	O
directly	O
into	O
the	O
goal	O
has	O
a	O
positive	O
value	B
the	O
values	O
of	O
all	O
other	O
pairs	O
are	O
still	O
zero	O
this	O
means	O
that	O
it	O
is	O
pointless	O
to	O
perform	O
updates	O
along	O
almost	O
all	O
transitions	O
because	O
they	O
take	O
the	O
agent	O
from	O
one	O
zero-valued	O
state	B
to	O
another	O
and	O
thus	O
the	O
updates	O
would	O
have	O
no	O
effect	O
only	O
an	O
update	O
along	O
a	O
transition	O
into	O
the	O
state	B
just	O
prior	O
to	O
the	O
goal	O
or	O
from	O
it	O
will	O
change	O
any	O
values	O
if	O
simulated	O
transitions	O
are	O
generated	O
uniformly	O
then	O
many	O
wasteful	O
updates	O
will	O
be	O
made	O
before	O
stumbling	O
onto	O
one	O
of	O
these	O
useful	O
ones	O
as	O
planning	B
progresses	O
the	O
region	O
of	O
useful	O
updates	O
grows	O
but	O
planning	B
is	O
still	O
far	O
less	O
efficient	O
than	O
it	O
would	O
be	O
if	O
focused	O
where	O
it	O
would	O
do	O
the	O
most	O
good	O
in	O
the	O
much	O
larger	O
problems	O
that	O
are	O
our	O
real	O
objective	O
the	O
number	O
of	O
states	O
is	O
so	O
large	O
that	O
an	O
unfocused	O
search	O
would	O
be	O
extremely	O
inefficient	O
this	O
example	O
suggests	O
that	O
search	O
might	O
be	O
usefully	O
focused	O
by	O
working	O
backward	O
from	O
goal	O
states	O
of	O
course	O
we	O
do	O
not	O
really	O
want	O
to	O
use	O
any	O
methods	O
specific	O
to	O
the	O
idea	O
of	O
goal	O
state	B
we	O
want	O
methods	O
that	O
work	O
for	O
general	O
reward	O
functions	O
goal	O
states	O
are	O
just	O
a	O
special	O
case	O
convenient	O
for	O
stimulating	O
intuition	O
in	O
general	O
we	O
want	O
to	O
work	O
back	O
not	O
just	O
from	O
goal	O
states	O
but	O
from	O
any	O
state	B
whose	O
value	B
has	O
changed	O
suppose	O
that	O
the	O
values	O
are	O
initially	O
correct	O
given	O
the	O
model	O
as	O
they	O
were	O
in	O
the	O
maze	O
example	O
prior	O
to	O
discovering	O
the	O
goal	O
suppose	O
now	O
that	O
the	O
agent	O
discovers	O
a	O
change	O
in	O
the	O
environment	B
and	O
changes	O
its	O
estimated	O
value	B
of	O
one	O
state	B
either	O
up	O
or	O
down	O
typically	O
this	O
will	O
imply	O
that	O
the	O
values	O
of	O
many	O
other	O
states	O
should	O
also	O
be	O
changed	O
but	O
the	O
only	O
useful	O
one-step	O
updates	O
are	O
those	O
of	O
actions	O
that	O
lead	O
directly	O
into	O
the	O
one	O
state	B
whose	O
value	B
has	O
been	O
changed	O
if	O
the	O
values	O
of	O
these	O
actions	O
are	O
updated	O
then	O
the	O
values	O
of	O
the	O
predecessor	O
states	O
may	O
change	O
in	O
turn	O
if	O
so	O
then	O
actions	O
leading	O
into	O
them	O
need	O
to	O
be	O
updated	O
and	O
then	O
their	O
predecessor	O
states	O
may	O
have	O
changed	O
in	O
this	O
way	O
one	O
can	O
work	O
backward	O
from	O
arbitrary	O
states	O
that	O
have	O
changed	O
in	O
value	B
either	O
performing	O
useful	O
updates	O
or	O
terminating	O
the	O
propagation	O
this	O
general	O
idea	O
might	O
be	O
termed	O
backward	O
focusing	O
of	O
planning	B
computations	O
as	O
the	O
frontier	O
of	O
useful	O
updates	O
propagates	O
backward	O
it	O
often	O
grows	O
rapidly	O
producing	O
many	O
state	B
action	B
pairs	O
that	O
could	O
usefully	O
be	O
updated	O
but	O
not	O
all	O
of	O
these	O
will	O
be	O
equally	O
useful	O
the	O
values	O
of	O
some	O
states	O
may	O
have	O
changed	O
a	O
lot	O
whereas	O
others	O
may	O
have	O
changed	O
little	O
the	O
predecessor	O
pairs	O
of	O
those	O
that	O
have	O
changed	O
a	O
lot	O
are	O
more	O
likely	O
to	O
also	O
change	O
a	O
lot	O
in	O
a	O
stochastic	O
environment	B
variations	O
in	O
estimated	O
transition	B
probabilities	I
also	O
contribute	O
to	O
variations	O
in	O
the	O
sizes	O
of	O
changes	O
and	O
in	O
the	O
urgency	O
with	O
which	O
pairs	O
need	O
to	O
be	O
updated	O
it	O
is	O
natural	O
to	O
prioritize	O
the	O
updates	O
according	O
to	O
a	O
measure	O
of	O
their	O
urgency	O
and	O
perform	O
them	O
in	O
order	O
of	O
priority	O
this	O
is	O
the	O
idea	O
behind	O
prioritized	B
sweeping	I
a	O
queue	O
is	O
maintained	O
of	O
every	O
state	B
action	B
pair	O
whose	O
estimated	O
value	B
would	O
change	O
nontrivially	O
if	O
updated	O
prioritized	O
by	O
the	O
size	O
of	O
the	O
change	O
when	O
the	O
top	O
pair	O
in	O
the	O
queue	O
is	O
updated	O
the	O
effect	O
on	O
each	O
of	O
its	O
predecessor	O
pairs	O
is	O
computed	O
if	O
the	O
effect	O
is	O
greater	O
than	O
some	O
small	O
threshold	O
then	O
the	O
pair	O
is	O
inserted	O
in	O
the	O
queue	O
with	O
the	O
new	O
priority	O
there	O
is	O
a	O
previous	O
entry	O
of	O
the	O
pair	O
in	O
the	O
queue	O
then	O
insertion	O
results	O
in	O
only	O
the	O
higher	O
priority	O
entry	O
remaining	O
in	O
the	O
queue	O
in	O
this	O
way	O
the	O
effects	O
of	O
changes	O
are	O
efficiently	O
propagated	O
backward	O
until	O
quiescence	O
the	O
full	O
algorithm	O
for	O
the	O
case	O
of	O
deterministic	O
environments	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
prioritized	B
sweeping	I
for	O
a	O
deterministic	O
environment	B
initialize	O
qs	O
a	O
m	O
odels	O
a	O
for	O
all	O
s	O
a	O
and	O
p	O
queue	O
to	O
empty	O
loop	O
forever	O
s	O
current	O
state	B
a	O
policys	O
q	O
take	O
action	B
a	O
observe	O
resultant	O
reward	O
r	O
and	O
state	B
m	O
odels	O
a	O
r	O
p	O
maxa	O
a	O
qs	O
a	O
if	O
p	O
then	O
insert	O
s	O
a	O
into	O
p	O
queue	O
with	O
priority	O
p	O
loop	O
repeat	O
n	O
times	O
while	O
p	O
queue	O
is	O
not	O
empty	O
s	O
a	O
f	O
irstp	O
queue	O
r	O
m	O
odels	O
a	O
qs	O
a	O
qs	O
a	O
maxa	O
a	O
qs	O
loop	O
for	O
all	O
s	O
a	O
predicted	O
to	O
lead	O
to	O
s	O
r	O
predicted	O
reward	O
for	O
s	O
a	O
s	O
p	O
r	O
maxa	O
qs	O
a	O
q	O
s	O
a	O
if	O
p	O
then	O
insert	O
s	O
a	O
into	O
p	O
queue	O
with	O
priority	O
p	O
example	O
prioritized	B
sweeping	I
on	O
mazes	O
prioritized	B
sweeping	I
has	O
been	O
found	O
to	O
dramatically	O
increase	O
the	O
speed	O
at	O
which	O
optimal	O
solutions	O
are	O
found	O
in	O
maze	O
tasks	O
often	O
by	O
a	O
factor	O
of	O
to	O
a	O
typical	O
example	O
is	O
shown	O
to	O
the	O
right	O
these	O
data	O
are	O
for	O
a	O
sequence	O
of	O
maze	O
tasks	O
of	O
exactly	O
the	O
same	O
structure	O
as	O
the	O
one	O
shown	O
in	O
figure	O
except	O
that	O
they	O
vary	O
in	O
the	O
grid	O
resolution	O
prioritized	B
sweeping	I
maintained	O
a	O
decisive	O
advantage	O
over	O
unprioritized	O
dynaq	O
both	O
systems	O
made	O
at	O
most	O
n	O
updates	O
per	O
environmental	O
interaction	O
adapted	O
from	O
peng	O
and	O
williams	O
extensions	O
of	O
prioritized	B
sweeping	I
to	O
stochastic	O
environments	O
are	O
straightforward	O
the	O
model	O
is	O
maintained	O
by	O
keeping	O
counts	O
of	O
the	O
number	O
of	O
times	O
each	O
state	B
action	B
pair	O
has	O
been	O
experienced	O
and	O
of	O
what	O
the	O
next	O
states	O
were	O
it	O
is	O
natural	O
then	O
to	O
update	O
each	O
pair	O
not	O
with	O
a	O
sample	O
update	O
as	O
we	O
have	O
been	O
using	O
so	O
far	O
but	O
with	O
an	O
expected	B
update	I
taking	O
into	O
account	O
all	O
possible	O
next	O
states	O
and	O
their	O
probabilities	O
of	O
occurring	O
prioritized	B
sweeping	I
is	O
just	O
one	O
way	O
of	O
distributing	O
computations	O
to	O
improve	O
planning	B
efficiency	O
and	O
probably	O
not	O
the	O
best	O
way	O
one	O
of	O
prioritized	B
sweeping	I
s	O
limitations	O
is	O
that	O
it	O
uses	O
expected	B
updates	O
which	O
in	O
stochastic	O
environments	O
may	O
waste	O
lots	O
of	O
computation	O
on	O
low-probability	O
transitions	O
as	O
we	O
show	O
in	O
the	O
following	O
section	O
sample	O
updates	O
can	O
in	O
many	O
cases	O
get	O
closer	O
to	O
the	O
true	O
value	B
function	I
with	O
less	O
computation	O
despite	O
the	O
variance	O
introduced	O
by	O
sampling	O
sample	O
updates	O
can	O
win	O
because	O
they	O
size	O
expected	B
vs	O
sample	O
updates	O
example	O
prioritized	B
sweeping	I
for	O
rod	O
maneuvering	O
the	O
objective	O
in	O
this	O
task	O
is	O
to	O
maneuver	O
a	O
rod	O
around	O
some	O
awkwardly	O
placed	O
obstacles	O
within	O
a	O
limited	O
rectangular	O
work	O
space	O
to	O
a	O
goal	O
position	O
in	O
the	O
fewest	O
number	O
of	O
steps	O
the	O
rod	O
can	O
be	O
translated	O
along	O
its	O
long	O
axis	O
or	O
perpendicular	O
to	O
that	O
axis	O
or	O
it	O
can	O
be	O
rotated	O
in	O
either	O
direction	O
around	O
its	O
center	O
the	O
distance	O
of	O
each	O
movement	O
is	O
approximately	O
of	O
the	O
work	O
space	O
and	O
the	O
rotation	O
increment	O
is	O
degrees	O
translations	O
are	O
deterministic	O
and	O
quantized	O
to	O
one	O
of	O
positions	O
to	O
the	O
right	O
is	O
shown	O
the	O
obstacles	O
and	O
the	O
shortest	O
solution	O
from	O
start	O
to	O
goal	O
found	O
by	O
prioritized	B
sweeping	I
this	O
problem	O
is	O
deterministic	O
but	O
has	O
four	O
actions	O
and	O
potential	O
states	O
of	O
these	O
are	O
unreachable	O
because	O
of	O
the	O
obstacles	O
this	O
problem	O
is	O
probably	O
too	O
large	O
to	O
be	O
solved	O
with	O
unprioritized	O
methods	O
figure	O
reprinted	O
from	O
moore	O
and	O
atkeson	O
break	O
the	O
overall	O
backing-up	O
computation	O
into	O
smaller	O
pieces	O
those	O
corresponding	O
to	O
individual	O
transitions	O
which	O
then	O
enables	O
it	O
to	O
be	O
focused	O
more	O
narrowly	O
on	O
the	O
pieces	O
that	O
will	O
have	O
the	O
largest	O
impact	O
this	O
idea	O
was	O
taken	O
to	O
what	O
may	O
be	O
its	O
logical	O
limit	O
in	O
the	O
small	O
backups	O
introduced	O
by	O
van	O
seijen	O
and	O
sutton	O
these	O
are	O
updates	O
along	O
a	O
single	O
transition	O
like	O
a	O
sample	O
update	O
but	O
based	O
on	O
the	O
probability	O
of	O
the	O
transition	O
without	O
sampling	O
as	O
in	O
an	O
expected	B
update	I
by	O
selecting	O
the	O
order	O
in	O
which	O
small	O
updates	O
are	O
done	O
it	O
is	O
possible	O
to	O
greatly	O
improve	O
planning	B
efficiency	O
beyond	O
that	O
possible	O
with	O
prioritized	B
sweeping	I
we	O
have	O
suggested	O
in	O
this	O
chapter	O
that	O
all	O
kinds	O
of	O
state-space	O
planning	B
can	O
be	O
viewed	O
as	O
sequences	O
of	O
value	B
updates	O
varying	O
only	O
in	O
the	O
type	O
of	B
update	I
expected	B
or	O
sample	O
large	O
or	O
small	O
and	O
in	O
the	O
order	O
in	O
which	O
the	O
updates	O
are	O
done	O
in	O
this	O
section	O
we	O
have	O
emphasized	O
backward	O
focusing	O
but	O
this	O
is	O
just	O
one	O
strategy	O
for	O
example	O
another	O
would	O
be	O
to	O
focus	O
on	O
states	O
according	O
to	O
how	O
easily	O
they	O
can	O
be	O
reached	O
from	O
the	O
states	O
that	O
are	O
visited	O
frequently	O
under	O
the	O
current	O
policy	B
which	O
might	O
be	O
called	O
forward	O
focusing	O
peng	O
and	O
williams	O
and	O
barto	O
bradtke	O
and	O
singh	O
have	O
explored	O
versions	O
of	O
forward	O
focusing	O
and	O
the	O
methods	O
introduced	O
in	O
the	O
next	O
few	O
sections	O
take	O
it	O
to	O
an	O
extreme	O
form	O
startgoal	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
expected	B
vs	O
sample	O
updates	O
the	O
examples	O
in	O
the	O
previous	O
sections	O
give	O
some	O
idea	O
of	O
the	O
range	O
of	O
possibilities	O
for	O
combining	O
methods	O
of	O
learning	O
and	O
planning	B
in	O
the	O
rest	O
of	O
this	O
chapter	O
we	O
analyze	O
some	O
of	O
the	O
component	O
ideas	O
involved	O
starting	O
with	O
the	O
relative	O
advantages	B
of	I
expected	B
and	O
sample	O
updates	O
much	O
of	O
this	O
book	O
has	O
been	O
about	O
different	O
kinds	O
of	O
value-function	O
updates	O
and	O
we	O
have	O
considered	O
a	O
great	O
many	O
varieties	O
focusing	O
for	O
the	O
moment	O
on	O
one-step	O
updates	O
they	O
vary	O
primarily	O
along	O
three	O
binary	O
dimensions	O
the	O
first	O
two	O
dimensions	O
are	O
whether	O
they	O
update	O
state	B
values	O
or	O
action	B
values	O
and	O
whether	O
they	O
estimate	O
the	O
value	B
for	O
the	O
optimal	O
policy	B
or	O
for	O
an	O
arbitrary	O
given	O
policy	B
these	O
two	O
dimensions	O
give	O
rise	O
to	O
four	O
classes	O
of	O
updates	O
for	O
approximating	O
the	O
four	O
value	B
functions	O
q	O
v	O
q	O
and	O
v	O
the	O
other	O
binary	O
dimension	O
is	O
whether	O
the	O
updates	O
are	O
expected	B
updates	O
considering	O
all	O
possible	O
events	O
that	O
might	O
happen	O
or	O
sample	O
updates	O
considering	O
a	O
single	O
sample	O
of	O
what	O
might	O
happen	O
these	O
three	O
binary	O
dimensions	O
give	O
rise	O
to	O
eight	O
cases	O
seven	O
of	O
which	O
correspond	O
to	O
specific	O
algorithms	O
as	O
shown	O
in	O
the	O
figure	O
to	O
the	O
right	O
eighth	O
case	O
does	O
not	O
seem	O
to	O
correspond	O
to	O
any	O
useful	O
update	O
any	O
of	O
these	O
one-step	O
updates	O
can	O
be	O
used	O
in	O
planning	B
methods	O
the	O
dyna-q	O
agents	O
discussed	O
earlier	O
use	O
q	O
sample	O
updates	O
but	O
they	O
could	O
just	O
as	O
well	O
use	O
q	O
expected	B
updates	O
or	O
either	O
expected	B
or	O
sample	O
q	O
updates	O
the	O
dyna-ac	O
system	O
uses	O
v	O
sample	O
updates	O
together	O
with	O
a	O
learning	O
policy	B
structure	O
in	O
chapter	O
for	O
stochastic	O
problems	O
prioritized	B
sweeping	I
is	O
always	O
done	O
using	O
one	O
of	O
the	O
expected	B
updates	O
when	O
we	O
introduced	O
one-step	O
sample	O
updates	O
in	O
chapter	O
we	O
presented	O
them	O
as	O
substitutes	O
for	O
expected	B
updates	O
in	O
the	O
absence	O
of	O
a	O
distribution	O
model	O
expected	B
updates	O
are	O
not	O
possible	O
but	O
sample	O
updates	O
can	O
be	O
done	O
using	O
sample	O
transitions	O
from	O
the	O
environment	B
or	O
a	O
sample	O
model	O
implicit	O
in	O
that	O
point	O
of	O
view	O
is	O
that	O
expected	B
updates	O
if	O
possible	O
are	O
preferable	O
to	O
sample	O
updates	O
but	O
are	O
they	O
expected	B
figure	O
backup	O
diagrams	O
for	O
all	O
the	O
one-step	O
updates	O
considered	O
in	O
this	O
book	O
valueestimatedexpected	O
updatesdpsample	O
updates	O
td	B
rpaq	O
evaluationvalue	O
pq-policy	O
expected	B
vs	O
sample	O
updates	O
updates	O
certainly	O
yield	O
a	O
better	O
estimate	O
because	O
they	O
are	O
uncorrupted	O
by	O
sampling	O
error	O
but	O
they	O
also	O
require	O
more	O
computation	O
and	O
computation	O
is	O
often	O
the	O
limiting	O
resource	O
in	O
planning	B
to	O
properly	O
assess	O
the	O
relative	O
merits	O
of	O
expected	B
and	O
sample	O
updates	O
for	O
planning	B
we	O
must	O
control	B
for	O
their	O
different	O
computational	O
requirements	O
for	O
concreteness	O
consider	O
the	O
expected	B
and	O
sample	O
updates	O
for	O
approximating	O
q	O
and	O
the	O
special	O
case	O
of	O
discrete	O
states	O
and	O
actions	O
a	O
table-lookup	O
representation	O
of	O
the	O
approximate	O
value	B
function	I
q	O
and	O
a	O
model	O
in	O
the	O
form	O
of	O
estimated	O
dynamics	O
rs	O
a	O
the	O
expected	B
update	I
for	O
a	O
state	B
action	B
pair	O
s	O
a	O
is	O
qs	O
a	O
rs	O
max	O
the	O
corresponding	O
sample	O
update	O
for	O
s	O
a	O
given	O
a	O
sample	O
next	O
state	B
and	O
reward	O
and	O
r	O
the	O
model	O
is	O
the	O
q-learning-like	O
update	O
qs	O
a	O
qs	O
a	O
max	O
qs	O
where	O
is	O
the	O
usual	O
positive	O
step-size	B
parameter	I
the	O
difference	O
between	O
these	O
expected	B
and	O
sample	O
updates	O
is	O
significant	O
to	O
the	O
extent	O
that	O
the	O
environment	B
is	O
stochastic	O
specifically	O
to	O
the	O
extent	O
that	O
given	O
a	O
state	B
and	O
action	B
many	O
possible	O
next	O
states	O
may	O
occur	O
with	O
various	O
probabilities	O
if	O
only	O
one	O
next	O
state	B
is	O
possible	O
then	O
the	O
expected	B
and	O
sample	O
updates	O
given	O
above	O
are	O
identical	O
if	O
there	O
are	O
many	O
possible	O
next	O
states	O
then	O
there	O
may	O
be	O
significant	O
differences	O
in	O
favor	O
of	O
the	O
expected	B
update	I
is	O
that	O
it	O
is	O
an	O
exact	O
computation	O
resulting	O
in	O
a	O
new	O
qs	O
a	O
whose	O
correctness	O
is	O
limited	O
only	O
by	O
the	O
correctness	O
of	O
the	O
at	O
successor	O
states	O
the	O
sample	O
update	O
is	O
in	O
addition	O
affected	O
by	O
sampling	O
error	O
on	O
the	O
other	O
hand	O
the	O
sample	O
update	O
is	O
cheaper	O
computationally	O
because	O
it	O
considers	O
only	O
one	O
next	O
state	B
not	O
all	O
possible	O
next	O
states	O
in	O
practice	O
the	O
computation	O
required	O
by	O
update	O
operations	O
is	O
usually	O
dominated	O
by	O
the	O
number	O
of	O
state	B
action	B
pairs	O
at	O
which	O
q	O
is	O
evaluated	O
for	O
a	O
particular	O
starting	O
pair	O
s	O
a	O
let	O
b	O
be	O
the	O
branching	B
factor	I
the	O
number	O
of	O
possible	O
next	O
states	O
for	O
which	O
a	O
then	O
an	O
expected	B
update	I
of	O
this	O
pair	O
requires	O
roughly	O
b	O
times	O
as	O
much	O
computation	O
as	O
a	O
sample	O
update	O
if	O
there	O
is	O
enough	O
time	O
to	O
complete	O
an	O
expected	B
update	I
then	O
the	O
resulting	O
estimate	O
is	O
generally	O
better	O
than	O
that	O
of	O
b	O
sample	O
updates	O
because	O
of	O
the	O
absence	O
of	O
sampling	O
error	O
but	O
if	O
there	O
is	O
insufficient	O
time	O
to	O
complete	O
an	O
expected	B
update	I
then	O
sample	O
updates	O
are	O
always	O
preferable	O
because	O
they	O
at	O
least	O
make	O
some	O
improvement	O
in	O
the	O
value	B
estimate	O
with	O
fewer	O
than	O
b	O
updates	O
in	O
a	O
large	O
problem	O
with	O
many	O
state	B
action	B
pairs	O
we	O
are	O
often	O
in	O
the	O
latter	O
situation	O
with	O
so	O
many	O
state	B
action	B
pairs	O
expected	B
updates	O
of	O
all	O
of	O
them	O
would	O
take	O
a	O
very	O
long	O
time	O
before	O
that	O
we	O
may	O
be	O
much	O
better	O
off	O
with	O
a	O
few	O
sample	O
updates	O
at	O
many	O
state	B
action	B
pairs	O
than	O
with	O
expected	B
updates	O
at	O
a	O
few	O
pairs	O
given	O
a	O
unit	O
of	O
computational	O
effort	O
is	O
it	O
better	O
devoted	O
to	O
a	O
few	O
expected	B
updates	O
or	O
to	O
b	O
times	O
as	O
many	O
sample	O
updates	O
figure	O
shows	O
the	O
results	O
of	O
an	O
analysis	O
that	O
suggests	O
an	O
answer	O
to	O
this	O
question	O
it	O
shows	O
the	O
estimation	O
error	O
as	O
a	O
function	O
of	O
computation	O
time	O
for	O
expected	B
and	O
sample	O
updates	O
for	O
a	O
variety	O
of	O
branching	O
factors	O
b	O
the	O
case	O
considered	O
is	O
that	O
in	O
which	O
all	O
b	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
figure	O
comparison	O
of	O
efficiency	O
of	O
expected	B
and	O
sample	O
updates	O
b	O
successor	O
states	O
are	O
equally	O
likely	O
and	O
in	O
which	O
the	O
error	O
in	O
the	O
initial	O
estimate	O
is	O
the	O
values	O
at	O
the	O
next	O
states	O
are	O
assumed	O
correct	O
so	O
the	O
expected	B
update	I
reduces	O
the	O
error	O
to	O
zero	O
upon	O
its	O
completion	O
in	O
this	O
case	O
sample	O
updates	O
reduce	O
the	O
error	O
according	O
bt	O
where	O
t	O
is	O
the	O
number	O
of	O
sample	O
updates	O
that	O
have	O
been	O
performed	O
sample	O
averages	O
i	O
e	O
the	O
key	O
observation	O
is	O
that	O
for	O
moderately	O
large	O
b	O
the	O
error	O
falls	O
dramatically	O
with	O
a	O
tiny	O
fraction	O
of	O
b	O
updates	O
for	O
these	O
cases	O
many	O
state	B
action	B
pairs	O
could	O
have	O
their	O
values	O
improved	O
dramatically	O
to	O
within	O
a	O
few	O
percent	O
of	O
the	O
effect	O
of	O
an	O
expected	B
update	I
in	O
the	O
same	O
time	O
that	O
a	O
single	O
state	B
action	B
pair	O
could	O
undergo	O
an	O
expected	B
update	I
the	O
advantage	O
of	O
sample	O
updates	O
shown	O
in	O
figure	O
is	O
probably	O
an	O
underestimate	O
of	O
the	O
real	O
effect	O
in	O
a	O
real	O
problem	O
the	O
values	O
of	O
the	O
successor	O
states	O
would	O
be	O
estimates	O
that	O
are	O
themselves	O
updated	O
by	O
causing	O
estimates	O
to	O
be	O
more	O
accurate	O
sooner	O
sample	O
updates	O
will	O
have	O
a	O
second	O
advantage	O
in	O
that	O
the	O
values	O
backed	O
up	O
from	O
the	O
successor	O
states	O
will	O
be	O
more	O
accurate	O
these	O
results	O
suggest	O
that	O
sample	O
updates	O
are	O
likely	O
to	O
be	O
superior	O
to	O
expected	B
updates	O
on	O
problems	O
with	O
large	O
stochastic	O
branching	O
factors	O
and	O
too	O
many	O
states	O
to	O
be	O
solved	O
exactly	O
exercise	O
the	O
analysis	O
above	O
assumed	O
that	O
all	O
of	O
the	O
b	O
possible	O
next	O
states	O
were	O
equally	O
likely	O
to	O
occur	O
suppose	O
instead	O
that	O
the	O
distribution	O
was	O
highly	O
skewed	O
that	O
some	O
of	O
the	O
b	O
states	O
were	O
much	O
more	O
likely	O
to	O
occur	O
than	O
most	O
would	O
this	O
strengthen	O
or	O
weaken	O
the	O
case	O
for	O
sample	O
updates	O
over	O
expected	B
updates	O
support	O
your	O
answer	O
trajectory	B
sampling	I
in	O
this	O
section	O
we	O
compare	O
two	O
ways	O
of	O
distributing	O
updates	O
the	O
classical	O
approach	O
from	O
dynamic	B
programming	I
is	O
to	O
perform	O
sweeps	O
through	O
the	O
entire	O
state	B
state	B
action	B
space	O
updating	O
each	O
state	B
state	B
action	B
pair	O
once	O
per	O
sweep	O
this	O
is	O
problematic	O
b	O
factorb	O
errorin	O
valueestimatenumber	O
of	O
trajectory	B
sampling	I
on	O
large	O
tasks	O
because	O
there	O
may	O
not	O
be	O
time	O
to	O
complete	O
even	O
one	O
sweep	O
in	O
many	O
tasks	O
the	O
vast	O
majority	O
of	O
the	O
states	O
are	O
irrelevant	O
because	O
they	O
are	O
visited	O
only	O
under	O
very	O
poor	O
policies	O
or	O
with	O
very	O
low	O
probability	O
exhaustive	O
sweeps	O
implicitly	O
devote	O
equal	O
time	O
to	O
all	O
parts	O
of	O
the	O
state	B
space	O
rather	O
than	O
focusing	O
where	O
it	O
is	O
needed	O
as	O
we	O
discussed	O
in	O
chapter	O
exhaustive	O
sweeps	O
and	O
the	O
equal	O
treatment	O
of	O
all	O
states	O
that	O
they	O
imply	O
are	O
not	O
necessary	O
properties	O
of	O
dynamic	B
programming	I
in	O
principle	O
updates	O
can	O
be	O
distributed	O
any	O
way	O
one	O
likes	O
assure	O
convergence	O
all	O
states	O
or	O
state	B
action	B
pairs	O
must	O
be	O
visited	O
in	O
the	O
limit	O
an	O
infinite	O
number	O
of	O
times	O
although	O
an	O
exception	O
to	O
this	O
is	O
discussed	O
in	O
section	O
below	O
but	O
in	O
practice	O
exhaustive	O
sweeps	O
are	O
often	O
used	O
the	O
second	O
approach	O
is	O
to	O
sample	O
from	O
the	O
state	B
or	O
state	B
action	B
space	O
according	O
to	O
some	O
distribution	O
one	O
could	O
sample	O
uniformly	O
as	O
in	O
the	O
dyna-q	O
agent	O
but	O
this	O
would	O
suffer	O
from	O
some	O
of	O
the	O
same	O
problems	O
as	O
exhaustive	O
sweeps	O
more	O
appealing	O
is	O
to	O
distribute	O
updates	O
according	O
to	O
the	O
on-policy	B
distribution	I
that	O
is	O
according	O
to	O
the	O
distribution	O
observed	O
when	O
following	O
the	O
current	O
policy	B
one	O
advantage	O
of	O
this	O
distribution	O
is	O
that	O
it	O
is	O
easily	O
generated	O
one	O
simply	O
interacts	O
with	O
the	O
model	O
following	O
the	O
current	O
policy	B
in	O
an	O
episodic	O
task	O
one	O
starts	O
in	O
a	O
start	O
state	B
according	O
to	O
the	O
starting-state	O
distribution	O
and	O
simulates	O
until	O
the	O
terminal	O
state	B
in	O
a	O
continuing	O
task	O
one	O
starts	O
anywhere	O
and	O
just	O
keeps	O
simulating	O
in	O
either	O
case	O
sample	O
state	B
transitions	O
and	O
rewards	O
are	O
given	O
by	O
the	O
model	O
and	O
sample	O
actions	O
are	O
given	O
by	O
the	O
current	O
policy	B
in	O
other	O
words	O
one	O
simulates	O
explicit	O
individual	O
trajectories	O
and	O
performs	O
updates	O
at	O
the	O
state	B
or	O
state	B
action	B
pairs	O
encountered	O
along	O
the	O
way	O
we	O
call	O
this	O
way	O
of	O
generating	O
experience	O
and	O
updates	O
trajectory	B
sampling	I
it	O
is	O
hard	O
to	O
imagine	O
any	O
efficient	O
way	O
of	O
distributing	O
updates	O
according	O
to	O
the	O
onpolicy	O
distribution	O
other	O
than	O
by	O
trajectory	B
sampling	I
if	O
one	O
had	O
an	O
explicit	O
representation	O
of	O
the	O
on-policy	B
distribution	I
then	O
one	O
could	O
sweep	O
through	O
all	O
states	O
weighting	O
the	O
update	O
of	O
each	O
according	O
to	O
the	O
on-policy	B
distribution	I
but	O
this	O
leaves	O
us	O
again	O
with	O
all	O
the	O
computational	O
costs	O
of	O
exhaustive	O
sweeps	O
possibly	O
one	O
could	O
sample	O
and	O
update	O
individual	O
state	B
action	B
pairs	O
from	O
the	O
distribution	O
but	O
even	O
if	O
this	O
could	O
be	O
done	O
efficiently	O
what	O
benefit	O
would	O
this	O
provide	O
over	O
simulating	O
trajectories	O
even	O
knowing	O
the	O
on-policy	B
distribution	I
in	O
an	O
explicit	O
form	O
is	O
unlikely	O
the	O
distribution	O
changes	O
whenever	O
the	O
policy	B
changes	O
and	O
computing	O
the	O
distribution	O
requires	O
computation	O
comparable	O
to	O
a	O
complete	O
policy	B
evaluation	O
consideration	O
of	O
such	O
other	O
possibilities	O
makes	O
trajectory	B
sampling	I
seem	O
both	O
efficient	O
and	O
elegant	O
is	O
the	O
on-policy	B
distribution	I
of	O
updates	O
a	O
good	O
one	O
intuitively	O
it	O
seems	O
like	O
a	O
good	O
choice	O
at	O
least	O
better	O
than	O
the	O
uniform	O
distribution	O
for	O
example	O
if	O
you	O
are	O
learning	O
to	O
play	O
chess	B
you	O
study	O
positions	O
that	O
might	O
arise	O
in	O
real	O
games	O
not	O
random	O
positions	O
of	O
chess	B
pieces	O
the	O
latter	O
may	O
be	O
valid	O
states	O
but	O
to	O
be	O
able	O
to	O
accurately	O
value	B
them	O
is	O
a	O
different	O
skill	O
from	O
evaluating	O
positions	O
in	O
real	O
games	O
we	O
will	O
also	O
see	O
in	O
part	O
ii	O
that	O
the	O
on-policy	B
distribution	I
has	O
significant	O
advantages	O
when	O
function	B
approximation	I
is	O
used	O
whether	O
or	O
not	O
function	B
approximation	I
is	O
used	O
one	O
might	O
expect	O
on-policy	O
focusing	O
to	O
significantly	O
improve	O
the	O
speed	O
of	O
planning	B
focusing	O
on	O
the	O
on-policy	B
distribution	I
could	O
be	O
beneficial	O
because	O
it	O
causes	O
vast	O
uninteresting	O
parts	O
of	O
the	O
space	O
to	O
be	O
ignored	O
or	O
it	O
could	O
be	O
detrimental	O
because	O
it	O
causes	O
the	O
same	O
old	O
parts	O
of	O
the	O
space	O
to	O
be	O
updated	O
over	O
and	O
over	O
we	O
conducted	O
a	O
small	O
experi	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
ment	O
to	O
assess	O
the	O
effect	O
empirically	O
to	O
isolate	O
the	O
effect	O
of	O
the	O
update	O
distribution	O
we	O
used	O
entirely	O
one-step	O
expected	B
tabular	O
updates	O
as	O
defined	O
by	O
in	O
the	O
uniform	O
case	O
we	O
cycled	O
through	O
all	O
state	B
action	B
pairs	O
updating	O
each	O
in	O
place	O
and	O
in	O
the	O
on-policy	O
case	O
we	O
simulated	O
episodes	B
all	O
starting	O
in	O
the	O
same	O
state	B
updating	O
each	O
state	B
action	B
pair	O
that	O
occurred	O
under	O
the	O
current	O
policy	B
the	O
tasks	O
were	O
undiscounted	O
episodic	O
tasks	O
generated	O
randomly	O
as	O
follows	O
from	O
each	O
of	O
the	O
states	O
two	O
actions	O
were	O
possible	O
each	O
of	O
which	O
resulted	O
in	O
one	O
of	O
b	O
next	O
states	O
all	O
equally	O
likely	O
with	O
a	O
different	O
random	O
selection	O
of	O
b	O
states	O
for	O
each	O
state	B
action	B
pair	O
the	O
branching	B
factor	I
b	O
was	O
the	O
same	O
for	O
all	O
state	B
action	B
pairs	O
in	O
addition	O
on	O
all	O
transitions	O
there	O
was	O
a	O
probability	O
of	O
transition	O
to	O
the	O
terminal	O
state	B
ending	O
the	O
episode	O
the	O
expected	B
reward	O
on	O
each	O
transition	O
was	O
selected	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	O
and	O
variance	O
at	O
any	O
point	O
in	O
the	O
planning	B
process	O
one	O
can	O
stop	O
and	O
exhaustively	O
compute	O
v	O
the	O
true	O
value	B
of	O
the	O
start	O
state	B
under	O
the	O
greedy	O
policy	B
given	O
the	O
current	O
action-value	O
function	O
q	O
as	O
an	O
indication	O
of	O
how	O
well	O
the	O
agent	O
would	O
do	O
on	O
a	O
new	O
episode	O
on	O
which	O
it	O
acted	O
greedily	O
the	O
while	O
assuming	O
the	O
model	O
is	O
correct	O
the	O
upper	O
part	O
of	O
the	O
figure	O
to	O
the	O
right	O
shows	O
results	O
averaged	O
over	O
sample	O
tasks	O
with	O
states	O
and	O
branching	O
factors	O
of	O
and	O
the	O
quality	O
of	O
the	O
policies	O
found	O
is	O
plotted	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
expected	B
updates	O
completed	O
in	O
all	O
cases	O
sampling	O
according	O
to	O
the	O
on-policy	B
distribution	I
resulted	O
in	O
faster	O
planning	B
initially	O
and	O
retarded	O
planning	B
in	O
the	O
long	O
run	O
the	O
effect	O
was	O
stronger	O
and	O
the	O
initial	O
period	O
of	O
faster	O
planning	B
was	O
longer	O
at	O
smaller	O
branching	O
factors	O
in	O
other	O
experiments	O
we	O
found	O
that	O
these	O
effects	O
also	O
became	O
stronger	O
as	O
the	O
number	O
of	O
states	O
increased	O
for	O
example	O
the	O
lower	O
part	O
of	O
the	O
figure	O
shows	O
results	O
for	O
a	O
branching	B
factor	I
of	O
for	O
tasks	O
with	O
states	O
in	O
this	O
case	O
the	O
advantage	O
of	O
on-policy	O
focusing	O
is	O
large	O
and	O
long-lasting	O
all	O
of	O
these	O
results	O
make	O
sense	O
in	O
the	O
short	O
term	O
sampling	O
according	O
to	O
the	O
on-policy	B
distribution	I
helps	O
by	O
focusing	O
on	O
states	O
that	O
are	O
near	O
descen	O
figure	O
relative	O
efficiency	O
of	O
updates	O
distributed	O
uniformly	O
across	O
the	O
state	B
space	O
versus	O
focused	O
on	O
simulated	O
on-policy	O
trajectories	O
each	O
starting	O
in	O
the	O
same	O
state	B
results	O
are	O
for	O
randomly	O
generated	O
tasks	O
of	O
two	O
sizes	O
and	O
various	O
branching	O
factors	O
b	O
ofstart	O
time	O
in	O
full	O
ofstart	O
time	O
in	O
full	O
backupsuniformuniformon-policyon-policyexpected	O
updatesexpected	O
states	O
real-time	B
dynamic	B
programming	I
if	O
there	O
are	O
dants	O
of	O
the	O
start	O
state	B
many	O
states	O
and	O
a	O
small	O
branching	B
factor	I
this	O
effect	O
will	O
be	O
large	O
and	O
long-lasting	O
in	O
the	O
long	O
run	O
focusing	O
on	O
the	O
on-policy	B
distribution	I
may	O
hurt	O
because	O
the	O
commonly	O
occurring	O
states	O
all	O
already	O
have	O
their	O
correct	O
values	O
sampling	O
them	O
is	O
useless	O
whereas	O
sampling	O
other	O
states	O
may	O
actually	O
perform	O
some	O
useful	O
work	O
this	O
presumably	O
is	O
why	O
the	O
exhaustive	O
unfocused	O
approach	O
does	O
better	O
in	O
the	O
long	O
run	O
at	O
least	O
for	O
small	O
problems	O
these	O
results	O
are	O
not	O
conclusive	O
because	O
they	O
are	O
only	O
for	O
problems	O
generated	O
in	O
a	O
particular	O
random	O
way	O
but	O
they	O
do	O
suggest	O
that	O
sampling	O
according	O
to	O
the	O
on-policy	B
distribution	I
can	O
be	O
a	O
great	O
advantage	O
for	O
large	O
problems	O
in	O
particular	O
for	O
problems	O
in	O
which	O
a	O
small	O
subset	O
of	O
the	O
state	B
action	B
space	O
is	O
visited	O
under	O
the	O
on-policy	B
distribution	I
exercise	O
some	O
of	O
the	O
graphs	O
in	O
figure	O
seem	O
to	O
be	O
scalloped	O
in	O
their	O
early	O
portions	O
particularly	O
the	O
upper	O
graph	O
for	O
b	O
and	O
the	O
uniform	O
distribution	O
why	O
do	O
you	O
think	O
this	O
is	O
what	O
aspects	O
of	O
the	O
data	O
shown	O
support	O
your	O
hypothesis	O
exercise	O
replicate	O
the	O
experiment	O
whose	O
results	O
are	O
shown	O
in	O
the	O
lower	O
part	O
of	O
figure	O
then	O
try	O
the	O
same	O
experiment	O
but	O
with	O
b	O
discuss	O
the	O
meaning	O
of	O
your	O
results	O
real-time	B
dynamic	B
programming	I
real-time	B
dynamic	B
programming	I
or	O
rtdp	O
is	O
an	O
on-policy	O
trajectory-sampling	O
version	O
of	O
the	O
value-iteration	O
algorithm	O
of	O
dynamic	B
programming	I
because	O
it	O
is	O
closely	O
related	O
to	O
conventional	O
sweep-based	O
policy	B
iteration	I
rtdp	O
illustrates	O
in	O
a	O
particularly	O
clear	O
way	O
some	O
of	O
the	O
advantages	O
that	O
on-policy	O
trajectory	B
sampling	I
can	O
provide	O
rtdp	O
updates	O
the	O
values	O
of	O
states	O
visited	O
in	O
actual	O
or	O
simulated	O
trajectories	O
by	O
means	O
of	O
expected	B
tabular	O
value-iteration	O
updates	O
as	O
defined	O
by	O
it	O
is	O
basically	O
the	O
algorithm	O
that	O
produced	O
the	O
on-policy	O
results	O
shown	O
in	O
figure	O
the	O
close	O
connection	O
between	O
rtdp	O
and	O
conventional	O
dp	O
makes	O
it	O
possible	O
to	O
derive	O
some	O
theoretical	O
results	O
by	O
adapting	O
existing	O
theory	O
rtdp	O
is	O
an	O
example	O
of	O
an	O
asynchronous	O
dp	O
algorithm	O
as	O
described	O
in	O
section	O
asynchronous	O
dp	O
algorithms	O
are	O
not	O
organized	O
in	O
terms	O
of	O
systematic	O
sweeps	O
of	O
the	O
state	B
set	O
they	O
update	O
state	B
values	O
in	O
any	O
order	O
whatsoever	O
using	O
whatever	O
values	O
of	O
other	O
states	O
happen	O
to	O
be	O
available	O
in	O
rtdp	O
the	O
update	O
order	O
is	O
dictated	O
by	O
the	O
order	O
states	O
are	O
visited	O
in	O
real	O
or	O
simulated	O
trajectories	O
if	O
trajectories	O
can	O
start	O
only	O
from	O
a	O
designated	O
set	O
of	O
start	O
states	O
and	O
if	O
you	O
are	O
interested	O
in	O
the	O
prediction	B
problem	O
for	O
a	O
given	O
policy	B
then	O
on-policy	O
trajectory	B
sampling	I
allows	O
the	O
algorithm	O
to	O
completely	O
skip	O
states	O
that	O
cannot	O
be	O
reached	O
by	O
the	O
given	O
policy	B
from	O
any	O
of	O
the	O
start	O
states	O
unreachable	O
states	O
are	O
irrelevant	O
to	O
the	O
prediction	B
problem	O
for	O
a	O
control	B
problem	O
where	O
the	O
goal	O
is	O
to	O
find	O
an	O
optimal	O
policy	B
instead	O
of	O
evaluating	O
a	O
given	O
policy	B
there	O
might	O
well	O
be	O
states	O
that	O
cannot	O
be	O
reached	O
by	O
any	O
optimal	O
policy	B
from	O
any	O
of	O
the	O
start	O
states	O
and	O
there	O
is	O
no	O
need	O
to	O
specify	O
optimal	O
actions	O
for	O
these	O
irrelevant	O
states	O
what	O
is	O
needed	O
is	O
an	O
optimal	O
partial	O
policy	B
meaning	O
a	O
policy	B
that	O
is	O
optimal	O
for	O
the	O
relevant	O
states	O
but	O
can	O
specify	O
arbitrary	O
actions	O
or	O
even	O
be	O
undefined	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
for	O
the	O
irrelevant	O
states	O
illustration	O
to	O
the	O
right	O
but	O
finding	O
such	O
an	O
optimal	O
partial	O
policy	B
with	O
an	O
on-policy	O
trajectory-sampling	O
control	B
method	O
such	O
as	O
sarsa	B
in	O
general	O
requires	O
visiting	O
all	O
state	B
action	B
pairs	O
even	O
those	O
that	O
will	O
turn	O
out	O
to	O
be	O
irrelevant	O
an	O
infinite	O
number	O
of	O
times	O
this	O
can	O
be	O
done	O
for	O
example	O
by	O
using	O
exploring	B
starts	I
this	O
is	O
true	O
for	O
rtdp	O
as	O
well	O
for	O
episodic	O
tasks	O
with	O
exploring	B
starts	I
rtdp	O
is	O
an	O
asynchronous	O
value-iteration	O
algorithm	O
that	O
converges	O
to	O
optimal	O
polices	O
for	O
discounted	O
finite	O
mdps	O
for	O
the	O
undiscounted	O
case	O
under	O
certain	O
conditions	O
unlike	O
the	O
situation	O
for	O
a	O
prediction	B
problem	O
it	O
is	O
generally	O
not	O
possible	O
to	O
stop	O
updating	O
any	O
state	B
or	O
state	B
action	B
pair	O
if	O
convergence	O
to	O
an	O
optimal	O
policy	B
is	O
important	O
the	O
most	O
interesting	O
result	O
for	O
rtdp	O
is	O
that	O
for	O
certain	O
types	O
of	O
problems	O
satisfying	O
reasonable	O
conditions	O
rtdp	O
is	O
guaranteed	O
to	O
find	O
a	O
policy	B
that	O
is	O
optimal	O
on	O
the	O
relevant	O
states	O
without	O
visiting	O
every	O
state	B
infinitely	O
often	O
or	O
even	O
without	O
visiting	O
some	O
states	O
at	O
all	O
indeed	O
in	O
some	O
problems	O
only	O
a	O
small	O
fraction	O
of	O
the	O
states	O
need	O
to	O
be	O
visited	O
this	O
can	O
be	O
a	O
great	O
advantage	O
for	O
problems	O
with	O
very	O
large	O
state	B
sets	O
where	O
even	O
a	O
single	O
sweep	O
may	O
not	O
be	O
feasible	O
the	O
tasks	O
for	O
which	O
this	O
result	O
holds	O
are	O
undiscounted	O
episodic	O
tasks	O
for	O
mdps	O
with	O
absorbing	O
goal	O
states	O
that	O
generate	O
zero	O
rewards	O
as	O
described	O
in	O
section	O
at	O
every	O
step	O
of	O
a	O
real	O
or	O
simulated	O
trajectory	O
rtdp	O
selects	O
a	O
greedy	O
action	B
ties	O
randomly	O
and	O
applies	O
the	O
expected	B
value-iteration	O
update	O
operation	O
to	O
the	O
current	O
state	B
it	O
can	O
also	O
update	O
the	O
values	O
of	O
an	O
arbitrary	O
collection	O
of	O
other	O
states	O
at	O
each	O
step	O
for	O
example	O
it	O
can	O
update	O
the	O
values	O
of	O
states	O
visited	O
in	O
a	O
limited-horizon	O
look-ahead	O
search	O
from	O
the	O
current	O
state	B
for	O
these	O
problems	O
with	O
each	O
episode	O
beginning	O
in	O
a	O
state	B
randomly	O
chosen	O
from	O
the	O
set	O
of	O
start	O
states	O
and	O
ending	O
at	O
a	O
goal	O
state	B
rtdp	O
converges	O
with	O
probability	O
one	O
to	O
a	O
policy	B
that	O
is	O
optimal	O
for	O
all	O
the	O
relevant	O
states	O
provided	O
the	O
initial	O
value	B
of	O
every	O
goal	O
state	B
is	O
zero	O
there	O
exists	O
at	O
least	O
one	O
policy	B
that	O
guarantees	O
that	O
a	O
goal	O
state	B
will	O
be	O
reached	O
with	O
probability	O
one	O
from	O
any	O
start	O
state	B
all	O
rewards	O
for	O
transitions	O
from	O
non-goal	O
states	O
are	O
strictly	O
negative	O
and	O
all	O
the	O
initial	O
values	O
are	O
equal	O
to	O
or	O
greater	O
than	O
their	O
optimal	O
values	O
can	O
be	O
satisfied	O
by	O
simply	O
setting	O
the	O
initial	O
values	O
of	O
all	O
states	O
to	O
zero	O
this	O
result	O
was	O
proved	O
by	O
barto	O
bradtke	O
and	O
singh	O
by	O
combining	O
results	O
for	O
asynchronous	O
dp	O
with	O
results	O
about	O
a	O
heuristic	B
search	I
algorithm	O
known	O
as	O
learning	O
real-time	O
a	O
due	O
to	O
korf	O
tasks	O
having	O
these	O
properties	O
are	O
examples	O
of	O
stochastic	O
optimal	O
path	O
problems	O
which	O
are	O
usually	O
stated	O
in	O
terms	O
of	O
cost	O
minimization	O
instead	O
of	O
as	O
reward	O
maximization	O
as	O
we	O
do	O
here	O
maximizing	O
the	O
negative	O
returns	O
in	O
our	O
version	O
is	O
equivalent	O
to	O
minimizing	O
the	O
costs	O
of	O
paths	O
from	O
a	O
start	O
state	B
to	O
a	O
goal	O
state	B
examples	O
of	O
this	O
kind	O
of	O
task	O
are	O
minimum-time	O
control	B
tasks	O
where	O
each	O
time	O
step	O
required	O
to	O
reach	O
a	O
goal	O
produces	O
a	O
start	O
statesirrelevant	O
states	O
unreachable	O
from	O
any	O
start	O
stateunder	O
any	O
optimal	O
policyrelevant	O
statesreachable	O
from	O
some	O
start	O
state	B
under	O
some	O
optimal	O
policy	B
real-time	B
dynamic	B
programming	I
reward	O
of	O
or	O
problems	O
like	O
the	O
golf	B
example	I
in	O
section	O
whose	O
objective	O
is	O
to	O
hit	O
the	O
hole	O
with	O
the	O
fewest	O
strokes	O
example	O
rtdp	O
on	O
the	O
racetrack	O
the	O
racetrack	O
problem	O
of	O
exercise	O
is	O
a	O
stochastic	O
optimal	O
path	O
problem	O
comparing	O
rtdp	O
and	O
the	O
conventional	O
dp	O
value	B
iteration	I
algorithm	O
on	O
an	O
example	O
racetrack	O
problem	O
illustrates	O
some	O
of	O
the	O
advantages	B
of	I
on-policy	O
trajectory	B
sampling	I
recall	O
from	O
the	O
exercise	O
that	O
an	O
agent	O
has	O
to	O
learn	O
how	O
to	O
drive	O
a	O
car	O
around	O
a	O
turn	O
like	O
those	O
shown	O
in	O
figure	O
and	O
cross	O
the	O
finish	O
line	O
as	O
quickly	O
as	O
possible	O
while	O
staying	O
on	O
the	O
track	O
start	O
states	O
are	O
all	O
the	O
zero-speed	O
states	O
on	O
the	O
starting	O
line	O
the	O
goal	O
states	O
are	O
all	O
the	O
states	O
that	O
can	O
be	O
reached	O
in	O
one	O
time	O
step	O
by	O
crossing	O
the	O
finish	O
line	O
from	O
inside	O
the	O
track	O
unlike	O
exercise	O
here	O
there	O
is	O
no	O
limit	O
on	O
the	O
car	O
s	O
speed	O
so	O
the	O
state	B
set	O
is	O
potentially	O
infinite	O
however	O
the	O
set	O
of	O
states	O
that	O
can	O
be	O
reached	O
from	O
the	O
set	O
of	O
start	O
states	O
via	O
any	O
policy	B
is	O
finite	O
and	O
can	O
be	O
considered	O
to	O
be	O
the	O
state	B
set	O
of	O
the	O
problem	O
each	O
episode	O
begins	O
in	O
a	O
randomly	O
selected	O
start	O
state	B
and	O
ends	O
when	O
the	O
car	O
crosses	O
the	O
finish	O
line	O
the	O
rewards	O
are	O
for	O
each	O
step	O
until	O
the	O
car	O
crosses	O
the	O
finish	O
line	O
if	O
the	O
car	O
hits	O
the	O
track	O
boundary	O
it	O
is	O
moved	O
back	O
to	O
a	O
random	O
start	O
state	B
and	O
the	O
episode	O
continues	O
a	O
racetrack	O
similar	O
to	O
the	O
small	O
racetrack	O
on	O
the	O
left	O
of	O
figure	O
has	O
states	O
reachable	O
from	O
start	O
states	O
by	O
any	O
policy	B
only	O
of	O
which	O
are	O
relevant	O
meaning	O
that	O
they	O
are	O
reachable	O
from	O
some	O
start	O
state	B
via	O
some	O
optimal	O
policy	B
number	O
of	O
relevant	O
states	O
was	O
estimated	O
by	O
counting	O
the	O
states	O
visited	O
while	O
executing	O
optimal	O
actions	O
for	O
episodes	B
the	O
table	O
below	O
compares	O
solving	O
this	O
task	O
by	O
conventional	O
dp	O
and	O
by	O
rtdp	O
these	O
results	O
are	O
averages	O
over	O
runs	O
each	O
begun	O
with	O
a	O
different	O
random	O
number	O
seed	O
conventional	O
dp	O
in	O
this	O
case	O
is	O
value	B
iteration	I
using	O
exhaustive	O
sweeps	O
of	O
the	O
state	B
set	O
with	O
values	O
updated	O
one	O
state	B
at	O
a	O
time	O
in	O
place	O
meaning	O
that	O
the	O
update	O
for	O
each	O
state	B
uses	O
the	O
most	O
recent	O
values	O
of	O
the	O
other	O
states	O
is	O
the	O
gauss-seidel	O
version	O
of	O
value	B
iteration	I
which	O
was	O
found	O
to	O
be	O
approximately	O
twice	O
as	O
fast	O
as	O
the	O
jacobi	O
version	O
on	O
this	O
problem	O
see	O
section	O
no	O
special	O
attention	O
was	O
paid	O
to	O
the	O
ordering	O
of	O
the	O
updates	O
other	O
orderings	O
could	O
have	O
produced	O
faster	O
convergence	O
initial	O
values	O
were	O
all	O
zero	O
for	O
each	O
run	O
of	O
both	O
methods	O
dp	O
was	O
judged	O
to	O
have	O
converged	O
when	O
the	O
maximum	O
change	O
in	O
a	O
state	B
value	B
over	O
a	O
sweep	O
was	O
less	O
than	O
and	O
rtdp	O
was	O
judged	O
to	O
have	O
converged	O
when	O
the	O
average	O
time	O
to	O
cross	O
the	O
finish	O
line	O
over	O
episodes	B
appeared	O
to	O
stabilize	O
at	O
an	O
asymptotic	O
number	O
of	O
steps	O
this	O
version	O
of	O
rtdp	O
updated	O
only	O
the	O
value	B
of	O
the	O
current	O
state	B
on	O
each	O
step	O
average	O
computation	O
to	O
convergence	O
average	O
number	O
of	O
updates	O
to	O
convergence	O
average	O
number	O
of	O
updates	O
per	O
episode	O
of	O
states	O
updated	O
times	O
of	O
states	O
updated	O
times	O
of	O
states	O
updated	O
times	O
dp	O
sweeps	O
rtdp	O
episodes	B
both	O
methods	O
produced	O
policies	O
averaging	O
between	O
and	O
steps	O
to	O
cross	O
the	O
finish	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
line	O
but	O
rtdp	O
required	O
only	O
roughly	O
half	O
of	O
the	O
updates	O
that	O
dp	O
did	O
this	O
is	O
the	O
result	O
of	O
rtdp	O
s	O
on-policy	O
trajectory	B
sampling	I
whereas	O
the	O
value	B
of	O
every	O
state	B
was	O
updated	O
in	O
each	O
sweep	O
of	O
dp	O
rtdp	O
focused	O
updates	O
on	O
fewer	O
states	O
in	O
an	O
average	O
run	O
rtdp	O
updated	O
the	O
values	O
of	O
of	O
the	O
states	O
no	O
more	O
than	O
times	O
and	O
of	O
the	O
states	O
no	O
more	O
than	O
times	O
the	O
values	O
of	O
about	O
states	O
were	O
not	O
updated	O
at	O
all	O
in	O
an	O
average	O
run	O
another	O
advantage	O
of	O
rtdp	O
is	O
that	O
as	O
the	O
value	B
function	I
approaches	O
the	O
optimal	O
value	B
function	I
v	O
the	O
policy	B
used	O
by	O
the	O
agent	O
to	O
generate	O
trajectories	O
approaches	O
an	O
optimal	O
policy	B
because	O
it	O
is	O
always	O
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
this	O
is	O
in	O
contrast	O
to	O
the	O
situation	O
in	O
conventional	O
value	B
iteration	I
in	O
practice	O
value	B
iteration	I
terminates	O
when	O
the	O
value	B
function	I
changes	O
by	O
only	O
a	O
small	O
amount	O
in	O
a	O
sweep	O
which	O
is	O
how	O
we	O
terminated	O
it	O
to	O
obtain	O
the	O
results	O
in	O
the	O
table	O
above	O
at	O
this	O
point	O
the	O
value	B
function	I
closely	O
approximates	O
v	O
and	O
a	O
greedy	O
policy	B
is	O
close	O
to	O
an	O
optimal	O
policy	B
however	O
it	O
is	O
possible	O
that	O
policies	O
that	O
are	O
greedy	O
with	O
respect	O
to	O
the	O
latest	O
value	B
function	I
were	O
optimal	O
or	O
nearly	O
so	O
long	O
before	O
value	B
iteration	I
terminates	O
from	O
chapter	O
that	O
optimal	O
policies	O
can	O
be	O
greedy	O
with	O
respect	O
to	O
many	O
different	O
value	B
functions	O
not	O
just	O
v	O
checking	O
for	O
the	O
emergence	O
of	O
an	O
optimal	O
policy	B
before	O
value	B
iteration	I
converges	O
is	O
not	O
a	O
part	O
of	O
the	O
conventional	O
dp	O
algorithm	O
and	O
requires	O
considerable	O
additional	O
computation	O
in	O
the	O
racetrack	O
example	O
by	O
running	O
many	O
test	O
episodes	B
after	O
each	O
dp	O
sweep	O
with	O
actions	O
selected	O
greedily	O
according	O
to	O
the	O
result	O
of	O
that	O
sweep	O
it	O
was	O
possible	O
to	O
estimate	O
the	O
earliest	O
point	O
in	O
the	O
dp	O
computation	O
at	O
which	O
the	O
approximated	O
optimal	O
evaluation	O
function	O
was	O
good	O
enough	O
so	O
that	O
the	O
corresponding	O
greedy	O
policy	B
was	O
nearly	O
optimal	O
for	O
this	O
racetrack	O
a	O
close-to-optimal	O
policy	B
emerged	O
after	O
sweeps	O
of	O
value	B
iteration	I
or	O
after	O
value-iteration	O
updates	O
this	O
is	O
considerably	O
less	O
than	O
the	O
updates	O
dp	O
needed	O
to	O
converge	O
to	O
v	O
but	O
sill	O
more	O
than	O
the	O
updates	O
rtdp	O
required	O
although	O
these	O
simulations	O
are	O
certainly	O
not	O
definitive	O
comparisons	O
of	O
the	O
rtdp	O
with	O
conventional	O
sweep-based	O
value	B
iteration	I
they	O
illustrate	O
some	O
of	O
advantages	B
of	I
on-policy	O
trajectory	B
sampling	I
whereas	O
conventional	O
value	B
iteration	I
continued	O
to	O
update	O
the	O
value	B
of	O
all	O
the	O
states	O
rtdp	O
strongly	O
focused	O
on	O
subsets	O
of	O
the	O
states	O
that	O
were	O
relevant	O
to	O
the	O
problem	O
s	O
objective	O
this	O
focus	O
became	O
increasingly	O
narrow	O
as	O
learning	O
continued	O
because	O
the	O
convergence	O
theorem	B
for	O
rtdp	O
applies	O
to	O
the	O
simulations	O
we	O
know	O
that	O
rtdp	O
eventually	O
would	O
have	O
focused	O
only	O
on	O
relevant	O
states	O
i	O
e	O
on	O
states	O
making	O
up	O
optimal	O
paths	O
rtdp	O
achieved	O
nearly	O
optimal	B
control	B
with	O
about	O
of	O
the	O
computation	O
required	O
by	O
sweep-based	O
value	B
iteration	I
planning	B
at	O
decision	O
time	O
planning	B
can	O
be	O
used	O
in	O
at	O
least	O
two	O
ways	O
the	O
one	O
we	O
have	O
considered	O
so	O
far	O
in	O
this	O
chapter	O
typified	O
by	O
dynamic	B
programming	I
and	O
dyna	O
is	O
to	O
use	O
planning	B
to	O
gradually	O
improve	O
a	O
policy	B
or	O
value	B
function	I
on	O
the	O
basis	O
of	O
simulated	O
experience	O
obtained	O
from	O
a	O
model	O
a	O
sample	O
or	O
a	O
distribution	O
model	O
selecting	O
actions	O
is	O
then	O
a	O
matter	O
of	O
comparing	O
the	O
current	O
state	B
s	O
action	B
values	O
obtained	O
from	O
a	O
table	O
in	O
the	O
tabular	O
case	O
we	O
have	O
thus	O
far	O
considered	O
or	O
by	O
evaluating	O
a	O
mathematical	O
expression	O
in	O
the	O
heuristic	B
search	I
approximate	O
methods	O
we	O
consider	O
in	O
part	O
ii	O
below	O
well	O
before	O
an	O
action	B
is	O
selected	O
for	O
any	O
current	O
state	B
st	O
planning	B
has	O
played	O
a	O
part	O
in	O
improving	O
the	O
table	O
entries	O
or	O
the	O
mathematical	O
expression	O
needed	O
to	O
select	O
the	O
action	B
for	O
many	O
states	O
including	O
st	O
used	O
this	O
way	O
planning	B
is	O
not	O
focussed	O
on	O
the	O
current	O
state	B
we	O
call	O
planning	B
used	O
in	O
this	O
way	O
background	O
planning	B
the	O
other	O
way	O
to	O
use	O
planning	B
is	O
to	O
begin	O
and	O
complete	O
it	O
after	O
encountering	O
each	O
new	O
state	B
st	O
as	O
a	O
computation	O
whose	O
output	O
is	O
the	O
selection	O
of	O
a	O
single	O
action	B
at	O
on	O
the	O
next	O
step	O
planning	B
begins	O
anew	O
with	O
to	O
produce	O
and	O
so	O
on	O
the	O
simplest	O
and	O
almost	O
degenerate	O
example	O
of	O
this	O
use	O
of	O
planning	B
is	O
when	O
only	O
state	B
values	O
are	O
available	O
and	O
an	O
action	B
is	O
selected	O
by	O
comparing	O
the	O
values	O
of	O
model-predicted	O
next	O
states	O
for	O
each	O
action	B
by	O
comparing	O
the	O
values	O
of	O
afterstates	B
as	O
in	O
the	O
tic-tac-toe	B
example	O
in	O
chapter	O
more	O
generally	O
planning	B
used	O
in	O
this	O
way	O
can	O
look	O
much	O
deeper	O
than	O
one-step-ahead	O
and	O
evaluate	O
action	B
choices	O
leading	O
to	O
many	O
different	O
predicted	O
state	B
and	O
reward	O
trajectories	O
unlike	O
the	O
first	O
use	O
of	O
planning	B
here	O
planning	B
focuses	O
on	O
a	O
particular	O
state	B
we	O
call	O
this	O
decision-time	O
planning	B
these	O
two	O
ways	O
of	O
thinking	O
about	O
planning	B
using	O
simulated	O
experience	O
to	O
gradually	O
improve	O
a	O
policy	B
or	O
value	B
function	I
or	O
using	O
simulated	O
experience	O
to	O
select	O
an	O
action	B
for	O
the	O
current	O
state	B
can	O
blend	O
together	O
in	O
natural	O
and	O
interesting	O
ways	O
but	O
they	O
have	O
tended	O
to	O
be	O
studied	O
separately	O
and	O
that	O
is	O
a	O
good	O
way	O
to	O
first	O
understand	O
them	O
let	O
us	O
now	O
take	O
a	O
closer	O
look	O
at	O
decision-time	O
planning	B
even	O
when	O
planning	B
is	O
only	O
done	O
at	O
decision	O
time	O
we	O
can	O
still	O
view	O
it	O
as	O
we	O
did	O
in	O
section	O
as	O
proceeding	O
from	O
simulated	O
experience	O
to	O
updates	O
and	O
values	O
and	O
ultimately	O
to	O
a	O
policy	B
it	O
is	O
just	O
that	O
now	O
the	O
values	O
and	O
policy	B
are	O
specific	O
to	O
the	O
current	O
state	B
and	O
the	O
action	B
choices	O
available	O
there	O
so	O
much	O
so	O
that	O
the	O
values	O
and	O
policy	B
created	O
by	O
the	O
planning	B
process	O
are	O
typically	O
discarded	O
after	O
being	O
used	O
to	O
select	O
the	O
current	O
action	B
in	O
many	O
applications	O
this	O
is	O
not	O
a	O
great	O
loss	O
because	O
there	O
are	O
very	O
many	O
states	O
and	O
we	O
are	O
unlikely	O
to	O
return	B
to	O
the	O
same	O
state	B
for	O
a	O
long	O
time	O
in	O
general	O
one	O
may	O
want	O
to	O
do	O
a	O
mix	O
of	O
both	O
focus	O
planning	B
on	O
the	O
current	O
state	B
and	O
store	O
the	O
results	O
of	O
planning	B
so	O
as	O
to	O
be	O
that	O
much	O
farther	O
along	O
should	O
one	O
return	B
to	O
the	O
same	O
state	B
later	O
decision-time	O
planning	B
is	O
most	O
useful	O
in	O
applications	O
in	O
which	O
fast	O
responses	O
are	O
not	O
required	O
in	O
chess	B
playing	O
programs	O
for	O
example	O
one	O
may	O
be	O
permitted	O
seconds	O
or	O
minutes	O
of	O
computation	O
for	O
each	O
move	O
and	O
strong	O
programs	O
may	O
plan	O
dozens	O
of	O
moves	O
ahead	O
within	O
this	O
time	O
on	O
the	O
other	O
hand	O
if	O
low	O
latency	O
action	B
selection	O
is	O
the	O
priority	O
then	O
one	O
is	O
generally	O
better	O
off	O
doing	O
planning	B
in	O
the	O
background	O
to	O
compute	O
a	O
policy	B
that	O
can	O
then	O
be	O
rapidly	O
applied	O
to	O
each	O
newly	O
encountered	O
state	B
heuristic	B
search	I
the	O
classical	O
state-space	O
planning	B
methods	O
in	O
artificial	B
intelligence	I
are	O
decision-time	O
planning	B
methods	O
collectively	O
known	O
as	O
heuristic	B
search	I
in	O
heuristic	B
search	I
for	O
each	O
state	B
encountered	O
a	O
large	O
tree	O
of	O
possible	O
continuations	O
is	O
considered	O
the	O
approximate	O
value	B
function	I
is	O
applied	O
to	O
the	O
leaf	O
nodes	O
and	O
then	O
backed	O
up	O
toward	O
the	O
current	O
state	B
at	O
the	O
root	O
the	O
backing	O
up	O
within	O
the	O
search	O
tree	O
is	O
just	O
the	O
same	O
as	O
in	O
the	O
expected	B
updates	O
with	O
maxes	O
for	O
v	O
and	O
q	O
discussed	O
throughout	O
this	O
book	O
the	O
backing	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
up	O
stops	O
at	O
the	O
state	B
action	B
nodes	O
for	O
the	O
current	O
state	B
once	O
the	O
backed-up	O
values	O
of	O
these	O
nodes	O
are	O
computed	O
the	O
best	O
of	O
them	O
is	O
chosen	O
as	O
the	O
current	O
action	B
and	O
then	O
all	O
backed-up	O
values	O
are	O
discarded	O
in	O
conventional	O
heuristic	B
search	I
no	O
effort	O
is	O
made	O
to	O
save	O
the	O
backed-up	O
values	O
by	O
changing	O
the	O
approximate	O
value	B
function	I
in	O
fact	O
the	O
value	B
function	I
is	O
generally	O
designed	O
by	O
people	O
and	O
never	O
changed	O
as	O
a	O
result	O
of	O
search	O
however	O
it	O
is	O
natural	O
to	O
consider	O
allowing	O
the	O
value	B
function	I
to	O
be	O
improved	O
over	O
time	O
using	O
either	O
the	O
backed-up	O
values	O
computed	O
during	O
heuristic	B
search	I
or	O
any	O
of	O
the	O
other	O
methods	O
presented	O
throughout	O
this	O
book	O
in	O
a	O
sense	O
we	O
have	O
taken	O
this	O
approach	O
all	O
along	O
our	O
greedy	O
and	O
ucb	O
action-selection	O
methods	O
are	O
not	O
unlike	O
heuristic	B
search	I
albeit	O
on	O
a	O
smaller	O
scale	O
for	O
example	O
to	O
compute	O
the	O
greedy	O
action	B
given	O
a	O
model	O
and	O
a	O
state-value	O
function	O
we	O
must	O
look	O
ahead	O
from	O
each	O
possible	O
action	B
to	O
each	O
possible	O
next	O
state	B
take	O
into	O
account	O
the	O
rewards	O
and	O
estimated	O
values	O
and	O
then	O
pick	O
the	O
best	O
action	B
just	O
as	O
in	O
conventional	O
heuristic	B
search	I
this	O
process	O
computes	O
backed-up	O
values	O
of	O
the	O
possible	O
actions	O
but	O
does	O
not	O
attempt	O
to	O
save	O
them	O
thus	O
heuristic	B
search	I
can	O
be	O
viewed	O
as	O
an	O
extension	O
of	O
the	O
idea	O
of	O
a	O
greedy	O
policy	B
beyond	O
a	O
single	O
step	O
the	O
point	O
of	O
searching	O
deeper	O
than	O
one	O
step	O
is	O
to	O
obtain	O
better	O
action	B
selections	O
if	O
one	O
has	O
a	O
perfect	O
model	O
and	O
an	O
imperfect	O
action-value	O
function	O
then	O
in	O
fact	O
deeper	O
search	O
will	O
usually	O
yield	O
better	O
certainly	O
if	O
the	O
search	O
is	O
all	O
the	O
way	O
to	O
the	O
end	O
of	O
the	O
episode	O
then	O
the	O
effect	O
of	O
the	O
imperfect	O
value	B
function	I
is	O
eliminated	O
and	O
the	O
action	B
determined	O
in	O
this	O
way	O
must	O
be	O
optimal	O
if	O
the	O
search	O
is	O
of	O
sufficient	O
depth	O
k	O
such	O
that	O
k	O
is	O
very	O
small	O
then	O
the	O
actions	O
will	O
be	O
correspondingly	O
near	O
optimal	O
on	O
the	O
other	O
hand	O
the	O
deeper	O
the	O
search	O
the	O
more	O
computation	O
is	O
required	O
usually	O
resulting	O
in	O
a	O
slower	O
response	O
time	O
a	O
good	O
example	O
is	O
provided	O
by	O
tesauro	O
s	O
grandmaster-level	O
backgammon	B
player	O
td-gammon	B
this	O
system	O
used	O
td	B
learning	O
to	O
learn	O
an	O
afterstate	O
value	B
function	I
through	O
many	O
games	O
of	O
self-play	O
using	O
a	O
form	O
of	O
heuristic	B
search	I
to	O
make	O
its	O
moves	O
as	O
a	O
model	O
td-gammon	B
used	O
a	O
priori	O
knowledge	O
of	O
the	O
probabilities	O
of	O
dice	O
rolls	O
and	O
the	O
assumption	O
that	O
the	O
opponent	O
always	O
selected	O
the	O
actions	O
that	O
td-gammon	B
rated	O
as	O
best	O
for	O
it	O
tesauro	O
found	O
that	O
the	O
deeper	O
the	O
heuristic	B
search	I
the	O
better	O
the	O
moves	O
made	O
by	O
td-gammon	B
but	O
the	O
longer	O
it	O
took	O
to	O
make	O
each	O
move	O
backgammon	B
has	O
a	O
large	O
branching	B
factor	I
yet	O
moves	O
must	O
be	O
made	O
within	O
a	O
few	O
seconds	O
it	O
was	O
only	O
feasible	O
to	O
search	O
ahead	O
selectively	O
a	O
few	O
steps	O
but	O
even	O
so	O
the	O
search	O
resulted	O
in	O
significantly	O
better	O
action	B
selections	O
we	O
should	O
not	O
overlook	O
the	O
most	O
obvious	O
way	O
in	O
which	O
heuristic	B
search	I
focuses	O
updates	O
on	O
the	O
current	O
state	B
much	O
of	O
the	O
effectiveness	O
of	O
heuristic	B
search	I
is	O
due	O
to	O
its	O
search	O
tree	O
being	O
tightly	O
focused	O
on	O
the	O
states	O
and	O
actions	O
that	O
might	O
immediately	O
follow	O
the	O
current	O
state	B
you	O
may	O
spend	O
more	O
of	O
your	O
life	O
playing	O
chess	B
than	O
checkers	O
but	O
when	O
you	O
play	O
checkers	O
it	O
pays	O
to	O
think	O
about	O
checkers	O
and	O
about	O
your	O
particular	O
checkers	O
position	O
your	O
likely	O
next	O
moves	O
and	O
successor	O
positions	O
no	O
matter	O
how	O
you	O
select	O
actions	O
it	O
is	O
these	O
states	O
and	O
actions	O
that	O
are	O
of	O
highest	O
priority	O
for	O
updates	O
and	O
where	O
you	O
most	O
urgently	O
want	O
your	O
approximate	O
value	B
function	I
to	O
be	O
accurate	O
not	O
only	O
should	O
your	O
computation	O
be	O
preferentially	O
devoted	O
to	O
imminent	O
events	O
but	O
so	O
should	O
your	O
limited	O
memory	O
resources	O
in	O
chess	B
for	O
example	O
there	O
are	O
far	O
too	O
many	O
possible	O
are	O
interesting	O
exceptions	O
to	O
this	O
see	O
e	O
g	O
pearl	O
rollout	B
algorithms	I
positions	O
to	O
store	O
distinct	O
value	B
estimates	O
for	O
each	O
of	O
them	O
but	O
chess	B
programs	O
based	O
on	O
heuristic	B
search	I
can	O
easily	O
store	O
distinct	O
estimates	O
for	O
the	O
millions	O
of	O
positions	O
they	O
encounter	O
looking	O
ahead	O
from	O
a	O
single	O
position	O
this	O
great	O
focusing	O
of	O
memory	O
and	O
computational	O
resources	O
on	O
the	O
current	O
decision	O
is	O
presumably	O
the	O
reason	O
why	O
heuristic	B
search	I
can	O
be	O
so	O
effective	O
the	O
distribution	O
of	O
updates	O
can	O
be	O
altered	O
in	O
similar	O
ways	O
to	O
focus	O
on	O
the	O
current	O
state	B
and	O
its	O
likely	O
successors	O
as	O
a	O
limiting	O
case	O
we	O
might	O
use	O
exactly	O
the	O
methods	O
of	O
heuristic	B
search	I
to	O
construct	O
a	O
search	O
tree	O
and	O
then	O
perform	O
the	O
individual	O
one-step	O
updates	O
from	O
bottom	O
up	O
as	O
suggested	O
by	O
figure	O
if	O
the	O
updates	O
are	O
ordered	O
in	O
this	O
way	O
and	O
a	O
tabular	O
representation	O
is	O
used	O
then	O
exactly	O
the	O
same	O
overall	O
update	O
would	O
be	O
achieved	O
as	O
in	O
depth-first	O
heuristic	B
search	I
any	O
state-space	O
search	O
can	O
be	O
viewed	O
in	O
this	O
way	O
as	O
the	O
piecing	O
together	O
of	O
a	O
large	O
number	O
of	O
individual	O
one-step	O
updates	O
thus	O
the	O
performance	O
improvement	O
observed	O
with	O
deeper	O
searches	O
is	O
not	O
due	O
to	O
the	O
use	O
of	O
multistep	O
updates	O
as	O
such	O
instead	O
it	O
is	O
due	O
to	O
the	O
focus	O
and	O
concentration	O
of	O
updates	O
on	O
states	O
and	O
actions	O
immediately	O
downstream	O
from	O
the	O
current	O
state	B
by	O
devoting	O
a	O
large	O
amount	O
of	O
computation	O
specifically	O
relevant	O
to	O
the	O
candidate	O
actions	O
decision-time	O
planning	B
can	O
produce	O
better	O
decisions	O
than	O
can	O
be	O
produced	O
by	O
relying	O
on	O
unfocused	O
updates	O
figure	O
heuristic	B
search	I
can	O
be	O
implemented	O
as	O
a	O
sequence	O
of	O
one-step	O
updates	O
here	O
outlined	O
in	O
blue	O
backing	O
up	O
values	O
from	O
the	O
leaf	O
nodes	O
toward	O
the	O
root	O
the	O
ordering	O
shown	O
here	O
is	O
for	O
a	O
selective	O
depth-first	O
search	O
rollout	B
algorithms	I
rollout	B
algorithms	I
are	O
decision-time	O
planning	B
algorithms	O
based	O
on	O
monte	B
carlo	I
control	B
applied	O
to	O
simulated	O
trajectories	O
that	O
all	O
begin	O
at	O
the	O
current	O
environment	B
state	B
they	O
estimate	O
action	B
values	O
for	O
a	O
given	O
policy	B
by	O
averaging	O
the	O
returns	O
of	O
many	O
simulated	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
trajectories	O
that	O
start	O
with	O
each	O
possible	O
action	B
and	O
then	O
follow	O
the	O
given	O
policy	B
when	O
the	O
action-value	O
estimates	O
are	O
considered	O
to	O
be	O
accurate	O
enough	O
the	O
action	B
one	O
of	O
the	O
actions	O
having	O
the	O
highest	O
estimated	O
value	B
is	O
executed	O
after	O
which	O
the	O
process	O
is	O
carried	O
out	O
anew	O
from	O
the	O
resulting	O
next	O
state	B
as	O
explained	O
by	O
tesauro	O
and	O
galperin	O
who	O
experimented	O
with	O
rollout	B
algorithms	I
for	O
playing	O
backgammon	B
the	O
term	O
rollout	O
comes	O
from	O
estimating	O
the	O
value	B
of	O
a	O
backgammon	B
position	O
by	O
playing	O
out	O
i	O
e	O
rolling	O
out	O
the	O
position	O
many	O
times	O
to	O
the	O
game	O
s	O
end	O
with	O
randomly	O
generated	O
sequences	O
of	O
dice	O
rolls	O
where	O
the	O
moves	O
of	O
both	O
players	O
are	O
made	O
by	O
some	O
fixed	O
policy	B
unlike	O
the	O
monte	B
carlo	I
control	B
algorithms	O
described	O
in	O
chapter	O
the	O
goal	O
of	O
a	O
rollout	O
algorithm	O
is	O
not	O
to	O
estimate	O
a	O
complete	O
optimal	O
action-value	O
function	O
q	O
or	O
a	O
complete	O
action-value	O
function	O
q	O
for	O
a	O
given	O
policy	B
instead	O
they	O
produce	O
monte	B
carlo	I
estimates	O
of	O
action	B
values	O
only	O
for	O
each	O
current	O
state	B
and	O
for	O
a	O
given	O
policy	B
usually	O
called	O
the	O
rollout	O
policy	B
as	O
decision-time	O
planning	B
algorithms	O
rollout	B
algorithms	I
make	O
immediate	O
use	O
of	O
these	O
action-value	O
estimates	O
then	O
discard	O
them	O
this	O
makes	O
rollout	B
algorithms	I
relatively	O
simple	O
to	O
implement	O
because	O
there	O
is	O
no	O
need	O
to	O
sample	O
outcomes	O
for	O
every	O
state-action	O
pair	O
and	O
there	O
is	O
no	O
need	O
to	O
approximate	O
a	O
function	O
over	O
either	O
the	O
state	B
space	O
or	O
the	O
state-action	O
space	O
what	O
then	O
do	O
rollout	B
algorithms	I
accomplish	O
the	O
policy	B
improvement	I
theorem	B
described	O
in	O
section	O
tells	O
us	O
that	O
given	O
any	O
two	O
policies	O
and	O
that	O
are	O
identical	O
except	O
that	O
a	O
for	O
some	O
state	B
s	O
if	O
q	O
a	O
v	O
then	O
policy	B
is	O
as	O
good	O
as	O
or	O
better	O
than	O
moreover	O
if	O
the	O
inequality	O
is	O
strict	O
then	O
is	O
in	O
fact	O
better	O
than	O
this	O
applies	O
to	O
rollout	B
algorithms	I
where	O
s	O
is	O
the	O
current	O
state	B
and	O
is	O
the	O
rollout	O
policy	B
averaging	O
the	O
returns	O
of	O
the	O
simulated	O
trajectories	O
produces	O
estimates	O
of	O
q	O
for	O
each	O
action	B
as	O
then	O
the	O
policy	B
that	O
selects	O
an	O
action	B
in	O
s	O
that	O
maximizes	O
these	O
estimates	O
and	O
thereafter	O
follows	O
is	O
a	O
good	O
candidate	O
for	O
a	O
policy	B
that	O
improves	O
over	O
the	O
result	O
is	O
like	O
one	O
step	O
of	O
the	O
policy-iteration	O
algorithm	O
of	O
dynamic	B
programming	I
discussed	O
in	O
section	O
it	O
is	O
more	O
like	O
one	O
step	O
of	O
asynchronous	O
value	B
iteration	I
described	O
in	O
section	O
because	O
it	O
changes	O
the	O
action	B
for	O
just	O
the	O
current	O
state	B
in	O
other	O
words	O
the	O
aim	O
of	O
a	O
rollout	O
algorithm	O
is	O
to	O
improve	O
upon	O
the	O
rollout	O
policy	B
not	O
to	O
find	O
an	O
optimal	O
policy	B
experience	O
has	O
shown	O
that	O
rollout	B
algorithms	I
can	O
be	O
surprisingly	O
effective	O
for	O
example	O
tesauro	O
and	O
galperin	O
were	O
surprised	O
by	O
the	O
dramatic	O
improvements	O
in	O
backgammon	B
playing	O
ability	O
produced	O
by	O
the	O
rollout	O
method	O
in	O
some	O
applications	O
a	O
rollout	O
algorithm	O
can	O
produce	O
good	O
performance	O
even	O
if	O
the	O
rollout	O
policy	B
is	O
completely	O
random	O
but	O
the	O
performance	O
of	O
the	O
improved	O
policy	B
depends	O
on	O
properties	O
of	O
the	O
rollout	O
policy	B
and	O
the	O
ranking	O
of	O
actions	O
produced	O
by	O
the	O
monte	B
carlo	I
value	B
estimates	O
intuition	O
suggests	O
that	O
the	O
better	O
the	O
rollout	O
policy	B
and	O
the	O
more	O
accurate	O
the	O
value	B
estimates	O
the	O
better	O
the	O
policy	B
produced	O
by	O
a	O
rollout	O
algorithm	O
is	O
likely	O
be	O
see	O
gelly	O
and	O
silver	O
this	O
involves	O
important	O
tradeoffs	O
because	O
better	O
rollout	O
policies	O
typically	O
mean	O
that	O
more	O
time	O
is	O
needed	O
to	O
simulate	O
enough	O
trajectories	O
to	O
obtain	O
good	O
value	B
estimates	O
as	O
decision-time	O
planning	B
methods	O
rollout	B
algorithms	I
usually	O
have	O
to	O
meet	O
strict	O
time	O
constraints	O
the	O
computation	O
time	O
needed	O
by	O
a	O
rollout	O
algorithm	O
depends	O
on	O
the	O
number	O
of	O
actions	O
that	O
have	O
to	O
be	O
evaluated	O
for	O
each	O
decision	O
the	O
number	O
of	O
time	O
steps	O
in	O
the	O
monte	B
carlo	I
tree	O
search	O
simulated	O
trajectories	O
needed	O
to	O
obtain	O
useful	O
sample	O
returns	O
the	O
time	O
it	O
takes	O
the	O
rollout	O
policy	B
to	O
make	O
decisions	O
and	O
the	O
number	O
of	O
simulated	O
trajectories	O
needed	O
to	O
obtain	O
good	O
monte	B
carlo	I
action-value	O
estimates	O
balancing	O
these	O
factors	O
is	O
important	O
in	O
any	O
application	O
of	O
rollout	O
methods	O
though	O
there	O
are	O
several	O
ways	O
to	O
ease	O
the	O
challenge	O
because	O
the	O
monte	B
carlo	I
trials	O
are	O
independent	O
of	O
one	O
another	O
it	O
is	O
possible	O
to	O
run	O
many	O
trials	O
in	O
parallel	O
on	O
separate	O
processors	O
another	O
approach	O
is	O
to	O
truncate	O
the	O
simulated	O
trajectories	O
short	O
of	O
complete	O
episodes	B
correcting	O
the	O
truncated	B
returns	O
by	O
means	O
of	O
a	O
stored	O
evaluation	O
function	O
brings	O
into	O
play	O
all	O
that	O
we	O
have	O
said	O
about	O
truncated	B
returns	O
and	O
updates	O
in	O
the	O
preceding	O
chapters	O
it	O
is	O
also	O
possible	O
as	O
tesauro	O
and	O
galperin	O
suggest	O
to	O
monitor	O
the	O
monte	B
carlo	I
simulations	O
and	O
prune	O
away	O
candidate	O
actions	O
that	O
are	O
unlikely	O
to	O
turn	O
out	O
to	O
be	O
the	O
best	O
or	O
whose	O
values	O
are	O
close	O
enough	O
to	O
that	O
of	O
the	O
current	O
best	O
that	O
choosing	O
them	O
instead	O
would	O
make	O
no	O
real	O
difference	O
tesauro	O
and	O
galperin	O
point	O
out	O
that	O
this	O
would	O
complicate	O
a	O
parallel	O
implementation	O
we	O
do	O
not	O
ordinarily	O
think	O
of	O
rollout	B
algorithms	I
as	O
learning	O
algorithms	O
because	O
they	O
do	O
not	O
maintain	O
long-term	O
memories	O
of	O
values	O
or	O
policies	O
however	O
these	O
algorithms	O
take	O
advantage	O
of	O
some	O
of	O
the	O
features	O
of	O
reinforcement	B
learning	I
that	O
we	O
have	O
emphasized	O
in	O
this	O
book	O
as	O
instances	O
of	O
monte	B
carlo	I
control	B
they	O
estimate	O
action	B
values	O
by	O
averaging	O
the	O
returns	O
of	O
a	O
collection	O
of	O
sample	O
trajectories	O
in	O
this	O
case	O
trajectories	O
of	O
simulated	O
interactions	O
with	O
a	O
sample	O
model	B
of	I
the	I
environment	B
in	O
this	O
way	O
they	O
are	O
like	O
reinforcement	B
learning	I
algorithms	O
in	O
avoiding	O
the	O
exhaustive	O
sweeps	O
of	O
dynamic	B
programming	I
by	O
trajectory	B
sampling	I
and	O
in	O
avoiding	O
the	O
need	O
for	O
distribution	B
models	I
by	O
relying	O
on	O
sample	O
instead	O
of	O
expected	B
updates	O
finally	O
rollout	B
algorithms	I
take	O
advantage	O
of	O
the	O
policy	B
improvement	I
property	O
by	O
acting	O
greedily	O
with	O
respect	O
to	O
the	O
estimated	O
action	B
values	O
monte	B
carlo	I
tree	O
search	O
monte	B
carlo	I
tree	O
search	O
is	O
a	O
recent	O
and	O
strikingly	O
successful	O
example	O
of	O
decision-time	O
planning	B
at	O
its	O
base	O
mcts	O
is	O
a	O
rollout	O
algorithm	O
as	O
described	O
above	O
but	O
enhanced	O
by	O
the	O
addition	O
of	O
a	O
means	O
for	O
accumulating	B
value	B
estimates	O
obtained	O
from	O
the	O
monte	B
carlo	I
simulations	O
in	O
order	O
to	O
successively	O
direct	O
simulations	O
toward	O
more	O
highlyrewarding	O
trajectories	O
mcts	O
is	O
largely	O
responsible	O
for	O
the	O
improvement	O
in	O
computer	O
go	O
from	O
a	O
weak	O
amateur	O
level	O
in	O
to	O
a	O
grandmaster	O
level	O
dan	O
or	O
more	O
in	O
many	O
variations	O
of	O
the	O
basic	O
algorithm	O
have	O
been	O
developed	O
including	O
a	O
variant	O
that	O
we	O
discuss	O
in	O
section	O
that	O
was	O
critical	O
for	O
the	O
stunning	O
victories	O
of	O
the	O
program	O
alphago	B
over	O
an	O
world	O
champion	O
go	O
player	O
mcts	O
has	O
proved	O
to	O
be	O
effective	O
in	O
a	O
wide	O
variety	O
of	O
competitive	O
settings	O
including	O
general	O
game	O
playing	O
see	O
finnsson	O
and	O
bj	O
ornsson	O
genesereth	O
and	O
thielscher	O
but	O
it	O
is	O
not	O
limited	O
to	O
games	O
it	O
can	O
be	O
effective	O
for	O
single-agent	O
sequential	O
decision	O
problems	O
if	O
there	O
is	O
an	O
environment	B
model	O
simple	O
enough	O
for	O
fast	O
multistep	O
simulation	O
mcts	O
is	O
executed	O
after	O
encountering	O
each	O
new	O
state	B
to	O
select	O
the	O
agent	O
s	O
action	B
for	O
that	O
state	B
it	O
is	O
executed	O
again	O
to	O
select	O
the	O
action	B
for	O
the	O
next	O
state	B
and	O
so	O
on	O
as	O
in	O
a	O
rollout	O
algorithm	O
each	O
execution	O
is	O
an	O
iterative	B
process	O
that	O
simulates	O
many	O
trajectories	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
starting	O
from	O
the	O
current	O
state	B
and	O
running	O
to	O
a	O
terminal	O
state	B
until	O
discounting	B
makes	O
any	O
further	O
reward	O
negligible	O
as	O
a	O
contribution	O
to	O
the	O
return	B
the	O
core	O
idea	O
of	O
mcts	O
is	O
to	O
successively	O
focus	O
multiple	O
simulations	O
starting	O
at	O
the	O
current	O
state	B
by	O
extending	O
the	O
initial	O
portions	O
of	O
trajectories	O
that	O
have	O
received	O
high	O
evaluations	O
from	O
earlier	O
simulations	O
mcts	O
does	O
not	O
have	O
to	O
retain	O
approximate	O
value	B
functions	O
or	O
policies	O
from	O
one	O
action	B
selection	O
to	O
the	O
next	O
though	O
in	O
many	O
implementations	O
it	O
retains	O
selected	O
action	B
values	O
likely	O
to	O
be	O
useful	O
for	O
its	O
next	O
execution	O
for	O
the	O
most	O
part	O
the	O
actions	O
in	O
the	O
simulated	O
trajectories	O
are	O
generated	O
using	O
a	O
simple	O
policy	B
usually	O
called	O
a	O
rollout	O
policy	B
as	O
it	O
is	O
for	O
simpler	O
rollout	B
algorithms	I
when	O
both	O
the	O
rollout	O
policy	B
and	O
the	O
model	O
do	O
not	O
require	O
a	O
lot	O
of	O
computation	O
many	O
simulated	O
trajectories	O
can	O
be	O
generated	O
in	O
a	O
short	O
period	O
of	O
time	O
as	O
in	O
any	O
tabular	O
monte	B
carlo	I
method	O
the	O
value	B
of	O
a	O
state	B
action	B
pair	O
is	O
estimated	O
as	O
the	O
average	O
of	O
the	O
returns	O
from	O
that	O
pair	O
monte	B
carlo	I
value	B
estimates	O
are	O
maintained	O
only	O
for	O
the	O
subset	O
of	O
state	B
action	B
pairs	O
that	O
are	O
most	O
likely	O
to	O
be	O
reached	O
in	O
a	O
few	O
steps	O
which	O
form	O
a	O
tree	O
rooted	O
at	O
the	O
current	O
state	B
as	O
illustrated	O
in	O
figure	O
mcts	O
incrementally	O
extends	O
the	O
tree	O
by	O
adding	O
nodes	O
representing	O
states	O
that	O
look	O
promising	O
based	O
on	O
the	O
results	O
of	O
the	O
simulated	O
trajectories	O
any	O
simulated	O
trajectory	O
will	O
pass	O
through	O
the	O
tree	O
and	O
then	O
exit	O
it	O
at	O
some	O
leaf	O
node	O
outside	O
the	O
tree	O
and	O
at	O
the	O
leaf	O
nodes	O
the	O
rollout	O
policy	B
is	O
used	O
for	O
action	B
selections	O
but	O
at	O
the	O
states	O
inside	O
the	O
tree	O
something	O
better	O
is	O
possible	O
for	O
these	O
states	O
we	O
have	O
value	B
estimates	O
for	O
of	O
at	O
least	O
some	O
of	O
the	O
actions	O
so	O
we	O
can	O
pick	O
among	O
them	O
using	O
an	O
informed	O
policy	B
called	O
the	O
tree	O
policy	B
that	O
balances	O
exploration	O
and	O
exploitation	O
for	O
example	O
the	O
tree	O
policy	B
could	O
select	O
actions	O
using	O
an	O
or	O
ucb	O
selection	O
rule	O
in	O
more	O
detail	O
each	O
iteration	O
of	O
a	O
basic	O
version	O
of	O
mcts	O
consists	O
of	O
the	O
following	O
four	O
steps	O
as	O
illustrated	O
in	O
figure	O
selection	O
starting	O
at	O
the	O
root	O
node	O
a	O
tree	O
policy	B
based	O
on	O
the	O
action	B
values	O
attached	O
to	O
the	O
edges	O
of	O
the	O
tree	O
traverses	O
the	O
tree	O
to	O
select	O
a	O
leaf	O
node	O
expansion	O
on	O
some	O
iterations	O
on	O
details	O
of	O
the	O
application	O
the	O
tree	O
is	O
expanded	O
from	O
the	O
selected	O
leaf	O
node	O
by	O
adding	O
one	O
or	O
more	O
child	O
nodes	O
reached	O
from	O
the	O
selected	O
node	O
via	O
unexplored	O
actions	O
simulation	O
from	O
the	O
selected	O
node	O
or	O
from	O
one	O
of	O
its	O
newly-added	O
child	O
nodes	O
any	O
simulation	O
of	O
a	O
complete	O
episode	O
is	O
run	O
with	O
actions	O
selected	O
by	O
the	O
rollout	O
policy	B
the	O
result	O
is	O
a	O
monte	B
carlo	I
trial	O
with	O
actions	O
selected	O
first	O
by	O
the	O
tree	O
policy	B
and	O
beyond	O
the	O
tree	O
by	O
the	O
rollout	O
policy	B
backup	O
the	O
return	B
generated	O
by	O
the	O
simulated	O
episode	O
is	O
backed	O
up	O
to	O
update	O
or	O
to	O
initialize	O
the	O
action	B
values	O
attached	O
to	O
the	O
edges	O
of	O
the	O
tree	O
traversed	O
by	O
the	O
tree	O
policy	B
in	O
this	O
iteration	O
of	O
mcts	O
no	O
values	O
are	O
saved	O
for	O
the	O
states	O
and	O
actions	O
visited	O
by	O
the	O
rollout	O
policy	B
beyond	O
the	O
tree	O
figure	O
illustrates	O
this	O
by	O
showing	O
a	O
backup	O
from	O
the	O
terminal	O
state	B
of	O
the	O
simulated	O
trajectory	O
directly	O
to	O
the	O
state	B
action	B
node	O
in	O
the	O
tree	O
where	O
the	O
rollout	O
policy	B
began	O
in	O
general	O
the	O
entire	O
return	B
over	O
the	O
simulated	O
trajectory	O
is	O
backed	O
up	O
to	O
this	O
state	B
action	B
node	O
monte	B
carlo	I
tree	O
search	O
figure	O
monte	B
carlo	I
tree	O
search	O
when	O
the	O
environment	B
changes	O
to	O
a	O
new	O
state	B
mcts	O
executes	O
as	O
many	O
iterations	O
as	O
possible	O
before	O
an	O
action	B
needs	O
to	O
be	O
selected	O
incrementally	O
building	O
a	O
tree	O
whose	O
root	O
node	O
represents	O
the	O
current	O
state	B
each	O
iteration	O
consists	O
of	O
the	O
four	O
operations	O
selection	O
expansion	O
possibly	O
skipped	O
on	O
some	O
iterations	O
simulation	O
and	O
backup	O
as	O
explained	O
in	O
the	O
text	O
and	O
illustrated	O
by	O
the	O
bold	O
arrows	O
in	O
the	O
trees	O
adapted	O
from	O
chaslot	O
bakkes	O
szita	O
and	O
spronck	O
mcts	O
continues	O
executing	O
these	O
four	O
steps	O
starting	O
each	O
time	O
at	O
the	O
tree	O
s	O
root	O
node	O
until	O
no	O
more	O
time	O
is	O
left	O
or	O
some	O
other	O
computational	O
resource	O
is	O
exhausted	O
then	O
finally	O
an	O
action	B
from	O
the	O
root	O
node	O
still	O
represents	O
the	O
current	O
state	B
of	O
the	O
environment	B
is	O
selected	O
according	O
to	O
some	O
mechanism	O
that	O
depends	O
on	O
the	O
accumulated	O
statistics	O
in	O
the	O
tree	O
for	O
example	O
it	O
may	O
be	O
an	O
action	B
having	O
the	O
largest	O
action	B
value	B
of	O
all	O
the	O
actions	O
available	O
from	O
the	O
root	O
state	B
or	O
perhaps	O
the	O
action	B
with	O
the	O
largest	O
visit	O
count	O
to	O
avoid	O
selecting	O
outliers	O
this	O
is	O
the	O
action	B
mcts	O
actually	O
selects	O
after	O
the	O
environment	B
transitions	O
to	O
a	O
new	O
state	B
mcts	O
is	O
run	O
again	O
sometimes	O
starting	O
with	O
a	O
tree	O
of	O
a	O
single	O
root	O
node	O
representing	O
the	O
new	O
state	B
but	O
often	O
starting	O
with	O
a	O
tree	O
containing	O
any	O
descendants	O
of	O
this	O
node	O
left	O
over	O
from	O
the	O
tree	O
constructed	O
by	O
the	O
previous	O
execution	O
of	O
mcts	O
all	O
the	O
remaining	O
nodes	O
are	O
discarded	O
along	O
with	O
the	O
action	B
values	O
associated	O
with	O
them	O
mcts	O
was	O
first	O
proposed	O
to	O
select	O
moves	O
in	O
programs	O
playing	O
two-person	O
competitive	O
games	O
such	O
as	O
go	O
for	O
game	O
playing	O
each	O
simulated	O
episode	O
is	O
one	O
complete	O
play	O
of	O
the	O
game	O
in	O
which	O
both	O
players	O
select	O
actions	O
by	O
the	O
tree	O
and	O
rollout	O
policies	O
section	O
describes	O
an	O
extension	O
of	O
mcts	O
used	O
in	O
the	O
alphago	B
program	O
that	O
combines	O
the	O
monte	O
selectionsimulationexpansionbackuprepeat	O
while	O
time	O
remains	O
tree	O
policyrolloutpolicy	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
carlo	O
evaluations	O
of	O
mcts	O
with	O
action	B
values	O
learned	O
by	O
a	O
deep	O
ann	O
via	O
self-play	O
reinforcement	B
learning	I
relating	O
mcts	O
to	O
the	O
reinforcement	B
learning	I
principles	O
we	O
describe	O
in	O
this	O
book	O
provides	O
some	O
insight	O
into	O
how	O
it	O
achieves	O
such	O
impressive	O
results	O
at	O
its	O
base	O
mcts	O
is	O
a	O
decision-time	O
planning	B
algorithm	O
based	O
on	O
monte	B
carlo	I
control	B
applied	O
to	O
simulations	O
that	O
start	O
from	O
the	O
root	O
state	B
that	O
is	O
it	O
is	O
a	O
kind	O
of	O
rollout	O
algorithm	O
as	O
described	O
in	O
the	O
previous	O
section	O
it	O
therefore	O
benefits	O
from	O
online	B
incremental	O
sample-based	O
value	B
estimation	O
and	O
policy	B
improvement	I
beyond	O
this	O
it	O
saves	O
action-value	O
estimates	O
attached	O
to	O
the	O
tree	O
edges	O
and	O
updates	O
them	O
using	O
reinforcement	B
learning	I
s	O
sample	O
updates	O
this	O
has	O
the	O
effect	O
of	O
focusing	O
the	O
monte	B
carlo	I
trials	O
on	O
trajectories	O
whose	O
initial	O
segments	O
are	O
common	O
to	O
high-return	O
trajectories	O
previously	O
simulated	O
further	O
by	O
incrementally	O
expanding	O
the	O
tree	O
mcts	O
effectively	O
grows	O
a	O
lookup	O
table	O
to	O
store	O
a	O
partial	O
action-value	O
function	O
with	O
memory	O
allocated	O
to	O
the	O
estimated	O
values	O
of	O
state	B
action	B
pairs	O
visited	O
in	O
the	O
initial	O
segments	O
of	O
high-yielding	O
sample	O
trajectories	O
mcts	O
thus	O
avoids	O
the	O
problem	O
of	O
globally	O
approximating	O
an	O
action-value	O
function	O
while	O
it	O
retains	O
the	O
benefit	O
of	O
using	O
past	O
experience	O
to	O
guide	O
exploration	O
the	O
striking	O
success	O
of	O
decision-time	O
planning	B
by	O
mcts	O
has	O
deeply	O
influenced	O
artificial	B
intelligence	I
and	O
many	O
researchers	O
are	O
studying	O
modifications	O
and	O
extensions	O
of	O
the	O
basic	O
procedure	O
for	O
use	O
in	O
both	O
games	O
and	O
single-agent	O
applications	O
summary	O
of	O
the	O
chapter	O
planning	B
requires	O
a	O
model	B
of	I
the	I
environment	B
a	O
distribution	O
model	O
consists	O
of	O
the	O
probabilities	O
of	O
next	O
states	O
and	O
rewards	O
for	O
possible	O
actions	O
a	O
sample	O
model	O
produces	O
single	O
transitions	O
and	O
rewards	O
generated	O
according	O
to	O
these	O
probabilities	O
dynamic	B
programming	I
requires	O
a	O
distribution	O
model	O
because	O
it	O
uses	O
expected	B
updates	O
which	O
involve	O
computing	O
expectations	O
over	O
all	O
the	O
possible	O
next	O
states	O
and	O
rewards	O
a	O
sample	O
model	O
on	O
the	O
other	O
hand	O
is	O
what	O
is	O
needed	O
to	O
simulate	O
interacting	O
with	O
the	O
environment	B
during	O
which	O
sample	O
updates	O
like	O
those	O
used	O
by	O
many	O
reinforcement	B
learning	I
algorithms	O
can	O
be	O
used	O
sample	O
models	O
are	O
generally	O
much	O
easier	O
to	O
obtain	O
than	O
distribution	B
models	I
we	O
have	O
presented	O
a	O
perspective	O
emphasizing	O
the	O
surprisingly	O
close	O
relationships	O
between	O
planning	B
optimal	O
behavior	O
and	O
learning	O
optimal	O
behavior	O
both	O
involve	O
estimating	O
the	O
same	O
value	B
functions	O
and	O
in	O
both	O
cases	O
it	O
is	O
natural	O
to	O
update	O
the	O
estimates	O
incrementally	O
in	O
a	O
long	O
series	O
of	O
small	O
backing-up	O
operations	O
this	O
makes	O
it	O
straightforward	O
to	O
integrate	O
learning	O
and	O
planning	B
processes	O
simply	O
by	O
allowing	O
both	O
to	O
update	O
the	O
same	O
estimated	O
value	B
function	I
in	O
addition	O
any	O
of	O
the	O
learning	O
methods	O
can	O
be	O
converted	O
into	O
planning	B
methods	O
simply	O
by	O
applying	O
them	O
to	O
simulated	O
experience	O
rather	O
than	O
to	O
real	O
experience	O
in	O
this	O
case	O
learning	O
and	O
planning	B
become	O
even	O
more	O
similar	O
they	O
are	O
possibly	O
identical	O
algorithms	O
operating	O
on	O
two	O
different	O
sources	O
of	O
experience	O
it	O
is	O
straightforward	O
to	O
integrate	O
incremental	O
planning	B
methods	O
with	O
acting	O
and	O
modellearning	O
planning	B
acting	O
and	O
model-learning	O
interact	O
in	O
a	O
circular	O
fashion	O
in	O
the	O
diagram	O
on	O
page	O
each	O
producing	O
what	O
the	O
other	O
needs	O
to	O
improve	O
no	O
other	O
interaction	O
among	O
them	O
is	O
either	O
required	O
or	O
prohibited	O
the	O
most	O
natural	O
approach	O
is	O
for	O
all	O
summary	O
of	O
part	O
i	O
dimensions	O
processes	O
to	O
proceed	O
asynchronously	O
and	O
in	O
parallel	O
if	O
the	O
processes	O
must	O
share	O
computational	O
resources	O
then	O
the	O
division	O
can	O
be	O
handled	O
almost	O
arbitrarily	O
by	O
whatever	O
organization	O
is	O
most	O
convenient	O
and	O
efficient	O
for	O
the	O
task	O
at	O
hand	O
in	O
this	O
chapter	O
we	O
have	O
touched	O
upon	O
a	O
number	O
of	O
dimensions	O
of	O
variation	O
among	O
state-space	O
planning	B
methods	O
one	O
dimension	O
is	O
the	O
variation	O
in	O
the	O
size	O
of	O
updates	O
the	O
smaller	O
the	O
updates	O
the	O
more	O
incremental	O
the	O
planning	B
methods	O
can	O
be	O
among	O
the	O
smallest	O
updates	O
are	O
one-step	O
sample	O
updates	O
as	O
in	O
dyna	O
another	O
important	O
dimension	O
is	O
the	O
distribution	O
of	O
updates	O
that	O
is	O
of	O
the	O
focus	O
of	O
search	O
prioritized	B
sweeping	I
focuses	O
backward	O
on	O
the	O
predecessors	O
of	O
states	O
whose	O
values	O
have	O
recently	O
changed	O
on-policy	O
trajectory	B
sampling	I
focuses	O
on	O
states	O
or	O
state	B
action	B
pairs	O
that	O
the	O
agent	O
is	O
likely	O
to	O
encounter	O
when	O
controlling	O
its	O
environment	B
this	O
can	O
allow	O
computation	O
to	O
skip	O
over	O
parts	O
of	O
the	O
state	B
space	O
that	O
are	O
irrelevant	O
to	O
the	O
prediction	B
or	O
control	B
problem	O
realtime	O
dynamic	B
programming	I
an	O
on-policy	O
trajectory	B
sampling	I
version	O
of	O
value	B
iteration	I
illustrates	O
some	O
of	O
the	O
advantages	O
this	O
strategy	O
has	O
over	O
conventional	O
sweep-based	O
policy	B
iteration	I
planning	B
can	O
also	O
focus	O
forward	O
from	O
pertinent	O
states	O
such	O
as	O
states	O
actually	O
encountered	O
during	O
an	O
agent-environment	O
interaction	O
the	O
most	O
important	O
form	O
of	O
this	O
is	O
when	O
planning	B
is	O
done	O
at	O
decision	O
time	O
that	O
is	O
as	O
part	O
of	O
the	O
action-selection	O
process	O
classical	O
heuristic	B
search	I
as	O
studied	O
in	O
artificial	B
intelligence	I
is	O
an	O
example	O
of	O
this	O
other	O
examples	O
are	O
rollout	B
algorithms	I
and	O
monte	B
carlo	I
tree	O
search	O
that	O
benefit	O
from	O
online	B
incremental	O
sample-based	O
value	B
estimation	O
and	O
policy	B
improvement	I
summary	O
of	O
part	O
i	O
dimensions	O
this	O
chapter	O
concludes	O
part	O
i	O
of	O
this	O
book	O
in	O
it	O
we	O
have	O
tried	O
to	O
present	O
reinforcement	B
learning	I
not	O
as	O
a	O
collection	O
of	O
individual	O
methods	O
but	O
as	O
a	O
coherent	O
set	O
of	O
ideas	O
cutting	O
across	O
methods	O
each	O
idea	O
can	O
be	O
viewed	O
as	O
a	O
dimension	O
along	O
which	O
methods	O
vary	O
the	O
set	O
of	O
such	O
dimensions	O
spans	O
a	O
large	O
space	O
of	O
possible	O
methods	O
by	O
exploring	O
this	O
space	O
at	O
the	O
level	O
of	O
dimensions	O
we	O
hope	O
to	O
obtain	O
the	O
broadest	O
and	O
most	O
lasting	O
understanding	O
in	O
this	O
section	O
we	O
use	O
the	O
concept	O
of	O
dimensions	O
in	O
method	O
space	O
to	O
recapitulate	O
the	O
view	O
of	O
reinforcement	B
learning	I
developed	O
so	O
far	O
in	O
this	O
book	O
all	O
of	O
the	O
methods	O
we	O
have	O
explored	O
so	O
far	O
in	O
this	O
book	O
have	O
three	O
key	O
ideas	O
in	O
common	O
first	O
they	O
all	O
seek	O
to	O
estimate	O
value	B
functions	O
second	O
they	O
all	O
operate	O
by	O
backing	O
up	O
values	O
along	O
actual	O
or	O
possible	O
state	B
trajectories	O
and	O
third	O
they	O
all	O
follow	O
the	O
general	O
strategy	O
of	O
generalized	O
policy	B
iteration	I
meaning	O
that	O
they	O
maintain	O
an	O
approximate	O
value	B
function	I
and	O
an	O
approximate	O
policy	B
and	O
they	O
continually	O
try	O
to	O
improve	O
each	O
on	O
the	O
basis	O
of	O
the	O
other	O
these	O
three	O
ideas	O
are	O
central	O
to	O
the	O
subjects	O
covered	O
in	O
this	O
book	O
we	O
suggest	O
that	O
value	B
functions	O
backing	O
up	O
value	B
updates	O
and	O
gpi	O
are	O
powerful	O
organizing	O
principles	O
potentially	O
relevant	O
to	O
any	O
model	O
of	O
intelligence	O
whether	O
artificial	O
or	O
natural	O
two	O
of	O
the	O
most	O
important	O
dimensions	O
along	O
which	O
the	O
methods	O
vary	O
are	O
shown	O
in	O
figure	O
these	O
dimensions	O
have	O
to	O
do	O
with	O
the	O
kind	O
of	B
update	I
used	O
to	O
improve	O
the	O
value	B
function	I
the	O
horizontal	O
dimension	O
is	O
whether	O
they	O
are	O
sample	O
updates	O
on	O
a	O
sample	O
trajectory	O
or	O
expected	B
updates	O
on	O
a	O
distribution	O
of	O
possible	O
trajectories	O
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
figure	O
a	O
slice	O
through	O
the	O
space	O
of	O
reinforcement	B
learning	I
methods	O
highlighting	O
the	O
two	O
of	O
the	O
most	O
important	O
dimensions	O
explored	O
in	O
part	O
i	O
of	O
this	O
book	O
the	O
depth	O
and	O
width	O
of	O
the	O
updates	O
expected	B
updates	O
require	O
a	O
distribution	O
model	O
whereas	O
sample	O
updates	O
need	O
only	O
a	O
sample	O
model	O
or	O
can	O
be	O
done	O
from	O
actual	O
experience	O
with	O
no	O
model	O
at	O
all	O
dimension	O
of	O
variation	O
the	O
vertical	O
dimension	O
of	O
figure	O
corresponds	O
to	O
the	O
depth	O
of	O
updates	O
that	O
is	O
to	O
the	O
degree	O
of	O
bootstrapping	B
at	O
three	O
of	O
the	O
four	O
corners	O
of	O
the	O
space	O
are	O
the	O
three	O
primary	O
methods	O
for	O
estimating	O
values	O
dynamic	B
programming	I
td	B
and	O
monte	B
carlo	I
along	O
the	O
left	O
edge	O
of	O
the	O
space	O
are	O
the	O
sample-update	O
methods	O
ranging	O
from	O
one-step	O
td	B
updates	O
to	O
full-return	O
monte	B
carlo	I
updates	O
between	O
these	O
is	O
a	O
spectrum	O
including	O
methods	O
based	O
on	O
n-step	B
updates	O
in	O
chapter	O
we	O
will	O
extend	O
this	O
to	O
mixtures	O
of	O
n-step	B
updates	O
such	O
as	O
the	O
implemented	O
by	O
eligibility	B
traces	I
dynamic	B
programming	I
methods	O
are	O
shown	O
in	O
the	O
extreme	O
upper-right	O
corner	O
of	O
the	O
space	O
because	O
they	O
involve	O
one-step	O
expected	B
updates	O
the	O
lower-right	O
corner	O
is	O
the	O
extreme	O
case	O
of	O
expected	B
updates	O
so	O
deep	O
that	O
they	O
run	O
all	O
the	O
way	O
to	O
terminal	O
states	O
in	O
a	O
continuing	O
task	O
until	O
discounting	B
has	O
reduced	O
the	O
contribution	O
of	O
any	O
further	O
rewards	O
to	O
a	O
negligible	O
level	O
this	O
is	O
the	O
case	O
of	O
exhaustive	O
search	O
intermediate	O
methods	O
along	O
this	O
dimension	O
include	O
heuristic	B
search	I
and	O
related	O
methods	O
that	O
search	O
and	O
update	O
up	O
to	O
a	O
limited	O
depth	O
perhaps	O
selectively	O
there	O
are	O
also	O
methods	O
that	O
are	O
intermediate	O
widthof	O
updatedepthlengthof	O
updatetemporal-differencelearningdynamicprogrammingmontecarlo	O
exhaustivesearch	O
summary	O
of	O
part	O
i	O
dimensions	O
along	O
the	O
horizontal	O
dimension	O
these	O
include	O
methods	O
that	O
mix	O
expected	B
and	O
sample	O
updates	O
as	O
well	O
as	O
the	O
possibility	O
of	O
methods	O
that	O
mix	O
samples	O
and	O
distributions	O
within	O
a	O
single	O
update	O
the	O
interior	O
of	O
the	O
square	O
is	O
filled	O
in	O
to	O
represent	O
the	O
space	O
of	O
all	O
such	O
intermediate	O
methods	O
a	O
third	O
dimension	O
that	O
we	O
have	O
emphasized	O
in	O
this	O
book	O
is	O
the	O
binary	O
distinction	O
between	O
on-policy	O
and	O
off-policy	B
methods	I
in	O
the	O
former	O
case	O
the	O
agent	O
learns	O
the	O
value	B
function	I
for	O
the	O
policy	B
it	O
is	O
currently	O
following	O
whereas	O
in	O
the	O
latter	O
case	O
it	O
learns	O
the	O
value	B
function	I
for	O
the	O
policy	B
for	O
a	O
different	O
policy	B
often	O
the	O
one	O
that	O
the	O
agent	O
currently	O
thinks	O
is	O
best	O
the	O
policy	B
generating	O
behavior	O
is	O
typically	O
different	O
from	O
what	O
is	O
currently	O
thought	O
best	O
because	O
of	O
the	O
need	O
to	O
explore	O
this	O
third	O
dimension	O
might	O
be	O
visualized	O
as	O
perpendicular	O
to	O
the	O
plane	O
of	O
the	O
page	O
in	O
figure	O
in	O
addition	O
to	O
the	O
three	O
dimensions	O
just	O
discussed	O
we	O
have	O
identified	O
a	O
number	O
of	O
others	O
throughout	O
the	O
book	O
definition	O
of	O
return	B
is	O
the	O
task	O
episodic	O
or	O
continuing	O
discounted	O
or	O
undiscounted	O
action	B
values	O
vs	O
state	B
values	O
vs	O
afterstate	O
values	O
what	O
kind	O
of	O
values	O
should	O
be	O
estimated	O
if	O
only	O
state	B
values	O
are	O
estimated	O
then	O
either	O
a	O
model	O
or	O
a	O
separate	O
policy	B
in	O
actor	O
critic	O
methods	O
is	O
required	O
for	O
action	B
selection	O
action	B
selectionexploration	O
how	O
are	O
actions	O
selected	O
to	O
ensure	O
a	O
suitable	O
tradeoff	O
between	O
exploration	O
and	O
exploitation	O
we	O
have	O
considered	O
only	O
the	O
simplest	O
ways	O
to	O
do	O
this	O
optimistic	O
initialization	O
of	O
values	O
soft-max	B
and	O
upper	O
confidence	O
bound	O
synchronous	O
vs	O
asynchronous	O
are	O
the	O
updates	O
for	O
all	O
states	O
performed	O
simultane	O
ously	O
or	O
one	O
by	O
one	O
in	O
some	O
order	O
real	O
vs	O
simulated	O
should	O
one	O
update	O
based	O
on	O
real	O
experience	O
or	O
simulated	O
experi	O
ence	O
if	O
both	O
how	O
much	O
of	O
each	O
location	O
of	O
updates	O
what	O
states	O
or	O
state	B
action	B
pairs	O
should	O
be	O
updated	O
modelfree	O
methods	O
can	O
choose	O
only	O
among	O
the	O
states	O
and	O
state	B
action	B
pairs	O
actually	O
encountered	O
but	O
model-based	O
methods	O
can	O
choose	O
arbitrarily	O
there	O
are	O
many	O
possibilities	O
here	O
timing	O
of	O
updates	O
should	O
updates	O
be	O
done	O
as	O
part	O
of	O
selecting	O
actions	O
or	O
only	O
after	O
ward	O
memory	O
for	O
updates	O
how	O
long	O
should	O
updated	O
values	O
be	O
retained	O
should	O
they	O
be	O
retained	O
permanently	O
or	O
only	O
while	O
computing	O
an	O
action	B
selection	O
as	O
in	O
heuristic	B
search	I
of	O
course	O
these	O
dimensions	O
are	O
neither	O
exhaustive	O
nor	O
mutually	O
exclusive	O
individual	O
algorithms	O
differ	O
in	O
many	O
other	O
ways	O
as	O
well	O
and	O
many	O
algorithms	O
lie	O
in	O
several	O
places	O
along	O
several	O
dimensions	O
for	O
example	O
dyna	O
methods	O
use	O
both	O
real	O
and	O
simulated	O
experience	O
to	O
affect	O
the	O
same	O
value	B
function	I
it	O
is	O
also	O
perfectly	O
sensible	O
to	O
maintain	O
multiple	O
value	B
functions	O
computed	O
in	O
different	O
ways	O
or	O
over	O
different	O
state	B
and	O
action	B
chapter	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
representations	O
these	O
dimensions	O
do	O
however	O
constitute	O
a	O
coherent	O
set	O
of	O
ideas	O
for	O
describing	O
and	O
exploring	O
a	O
wide	O
space	O
of	O
possible	O
methods	O
the	O
most	O
important	O
dimension	O
not	O
mentioned	O
here	O
and	O
not	O
covered	O
in	O
part	O
i	O
of	O
this	O
book	O
is	O
that	O
of	O
function	B
approximation	I
function	B
approximation	I
can	O
be	O
viewed	O
as	O
an	O
orthogonal	O
spectrum	O
of	O
possibilities	O
ranging	O
from	O
tabular	O
methods	O
at	O
one	O
extreme	O
through	O
state	B
aggregation	I
a	O
variety	O
of	O
linear	O
methods	O
and	O
then	O
a	O
diverse	O
set	O
of	O
nonlinear	O
methods	O
this	O
dimension	O
is	O
explored	O
in	O
part	O
ii	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
overall	O
view	O
of	O
planning	B
and	O
learning	O
presented	O
here	O
has	O
developed	O
gradually	O
over	O
a	O
number	O
of	O
years	O
in	O
part	O
by	O
the	O
authors	O
barto	O
bradtke	O
and	O
singh	O
sutton	O
and	O
pinette	O
sutton	O
and	O
barto	O
it	O
has	O
been	O
strongly	O
influenced	O
by	O
agre	O
and	O
chapman	O
agre	O
bertsekas	O
and	O
tsitsiklis	O
singh	O
and	O
others	O
the	O
authors	O
were	O
also	O
strongly	O
influenced	O
by	O
psychological	O
studies	O
of	O
latent	B
learning	I
and	O
by	O
psychological	O
views	O
of	O
the	O
nature	O
of	O
thought	O
galanter	O
and	O
gerstenhaber	O
craik	O
campbell	O
dennett	O
in	O
part	O
iii	O
of	O
the	O
book	O
section	O
relates	O
model-based	B
and	I
model-free	I
methods	I
to	O
psychological	O
theories	O
of	O
learning	O
and	O
behavior	O
and	O
section	O
discusses	O
ideas	O
about	O
how	O
the	O
brain	O
might	O
implement	O
these	O
types	O
of	O
methods	O
the	O
terms	O
direct	O
and	O
indirect	O
which	O
we	O
use	O
to	O
describe	O
different	O
kinds	O
of	O
reinforcement	B
learning	I
are	O
from	O
the	O
adaptive	O
control	B
literature	O
goodwin	O
and	O
sin	O
where	O
they	O
are	O
used	O
to	O
make	O
the	O
same	O
kind	O
of	O
distinction	O
the	O
term	O
system	B
identification	I
is	O
used	O
in	O
adaptive	O
control	B
for	O
what	O
we	O
call	O
modellearning	O
goodwin	O
and	O
sin	O
ljung	O
and	O
s	O
oderstrom	O
young	O
the	O
dyna	B
architecture	I
is	O
due	O
to	O
sutton	O
and	O
the	O
results	O
in	O
this	O
and	O
the	O
next	O
section	O
are	O
based	O
on	O
results	O
reported	O
there	O
barto	O
and	O
singh	O
consider	O
some	O
of	O
the	O
issues	O
in	O
comparing	O
direct	O
and	O
indirect	O
reinforcement	B
learning	I
methods	O
there	O
have	O
been	O
several	O
works	O
with	O
model-based	B
reinforcement	B
learning	I
that	O
take	O
the	O
idea	O
of	O
exploration	O
bonuses	O
and	O
optimistic	O
initialization	O
to	O
its	O
logical	O
extreme	O
in	O
which	O
all	O
incompletely	O
explored	O
choices	O
are	O
assumed	O
maximally	O
rewarding	O
and	O
optimal	O
paths	O
are	O
computed	O
to	O
test	O
them	O
the	O
algorithm	O
of	O
kearns	O
and	O
singh	O
and	O
the	O
r-max	O
algorithm	O
of	O
brafman	O
and	O
tennenholtz	O
are	O
guaranteed	O
to	O
find	O
a	O
near-optimal	O
solution	O
in	O
time	O
polynomial	O
in	O
the	O
number	O
of	O
states	O
and	O
actions	O
this	O
is	O
usually	O
too	O
slow	O
for	O
practical	O
algorithms	O
but	O
is	O
probably	O
the	O
best	O
that	O
can	O
be	O
done	O
in	O
the	O
worst	O
case	O
prioritized	B
sweeping	I
was	O
developed	O
simultaneously	O
and	O
independently	O
by	O
moore	O
and	O
atkeson	O
and	O
peng	O
and	O
williams	O
the	O
results	O
in	O
the	O
box	O
on	O
page	O
are	O
due	O
to	O
peng	O
and	O
williams	O
the	O
results	O
in	O
the	O
box	O
on	O
page	O
are	O
due	O
to	O
moore	O
and	O
atkeson	O
key	O
subsequent	O
work	O
in	O
this	O
area	O
summary	O
of	O
part	O
i	O
dimensions	O
includes	O
that	O
by	O
mcmahan	O
and	O
gordon	O
and	O
by	O
van	O
seijen	O
and	O
sutton	O
this	O
section	O
was	O
strongly	O
influenced	O
by	O
the	O
experiments	O
of	O
singh	O
trajectory	B
sampling	I
has	O
implicitly	O
been	O
a	O
part	O
of	O
reinforcement	B
learning	I
from	O
the	O
outset	O
but	O
it	O
was	O
most	O
explicitly	O
emphasized	O
by	O
barto	O
bradtke	O
and	O
singh	O
in	O
their	O
introduction	O
of	O
rtdp	O
they	O
recognized	O
that	O
korf	O
s	O
learning	O
real-time	O
a	O
algorithm	O
is	O
an	O
asynchronous	O
dp	O
algorithm	O
that	O
applies	O
to	O
stochastic	O
problems	O
as	O
well	O
as	O
the	O
deterministic	O
problems	O
on	O
which	O
korf	O
focused	O
beyond	O
lrta	O
rtdp	O
includes	O
the	O
option	O
of	O
updating	O
the	O
values	O
of	O
many	O
states	O
in	O
the	O
time	O
intervals	O
between	O
the	O
execution	O
of	O
actions	O
barto	O
et	O
al	O
proved	O
the	O
convergence	O
result	O
described	O
here	O
by	O
combining	O
korf	O
s	O
convergence	O
proof	B
for	O
lrta	O
with	O
the	O
result	O
of	O
bertsekas	O
bertsekas	O
and	O
tsitsiklis	O
ensuring	O
convergence	O
of	O
asynchronous	O
dp	O
for	O
stochastic	O
shortest	O
path	O
problems	O
in	O
the	O
undiscounted	O
case	O
combining	O
model-learning	O
with	O
rtdp	O
is	O
called	O
adaptive	O
rtdp	O
also	O
presented	O
by	O
barto	O
et	O
al	O
and	O
discussed	O
by	O
barto	O
for	O
further	O
reading	O
on	O
heuristic	B
search	I
the	O
reader	O
is	O
encouraged	O
to	O
consult	O
texts	O
and	O
surveys	O
such	O
as	O
those	O
by	O
russell	O
and	O
norvig	O
and	O
korf	O
peng	O
and	O
williams	O
explored	O
a	O
forward	O
focusing	O
of	O
updates	O
much	O
as	O
is	O
suggested	O
in	O
this	O
section	O
abramson	O
s	O
expected-outcome	O
model	O
is	O
a	O
rollout	O
algorithm	O
applied	O
to	O
two-person	O
games	O
in	O
which	O
the	O
play	O
of	O
both	O
simulated	O
players	O
is	O
random	O
he	O
argued	O
that	O
even	O
with	O
random	O
play	O
it	O
is	O
a	O
powerful	O
heuristic	O
that	O
is	O
precise	O
accurate	O
easily	O
estimable	O
efficiently	O
calculable	O
and	O
domain-independent	O
tesauro	O
and	O
galperin	O
demonstrated	O
the	O
effectiveness	O
of	O
rollout	B
algorithms	I
for	O
improving	O
the	O
play	O
of	O
backgammon	B
programs	O
adopting	O
the	O
term	O
rollout	O
from	O
its	O
use	O
in	O
evaluating	O
backgammon	B
positions	O
by	O
playing	O
out	O
positions	O
with	O
different	O
randomly	O
generating	O
sequences	O
of	O
dice	O
rolls	O
bertsekas	O
tsitsiklis	O
and	O
wu	O
examine	O
rollout	B
algorithms	I
applied	O
to	O
combinatorial	O
optimization	O
problems	O
and	O
bertsekas	O
surveys	O
their	O
use	O
in	O
discrete	O
deterministic	O
optimization	O
problems	O
remarking	O
that	O
they	O
are	O
often	O
surprisingly	O
effective	O
the	O
central	O
ideas	O
of	O
mcts	O
were	O
introduced	O
by	O
coulom	O
and	O
by	O
kocsis	O
and	O
szepesv	O
ari	O
they	O
built	O
upon	O
previous	O
research	O
with	O
monte	B
carlo	I
planning	B
algorithms	O
as	O
reviewed	O
by	O
these	O
authors	O
browne	O
powley	O
whitehouse	O
lucas	O
cowling	O
rohlfshagen	O
tavener	O
perez	O
samothrakis	O
and	O
colton	O
is	O
an	O
excellent	O
survey	O
of	O
mcts	O
methods	O
and	O
their	O
applications	O
david	O
silver	O
contributed	O
to	O
the	O
ideas	O
and	O
presentation	O
in	O
this	O
section	O
part	O
ii	O
approximate	O
solution	O
methods	O
in	O
the	O
second	O
part	O
of	O
the	O
book	O
we	O
extend	O
the	O
tabular	O
methods	O
presented	O
in	O
the	O
first	O
part	O
to	O
apply	O
to	O
problems	O
with	O
arbitrarily	O
large	O
state	B
spaces	O
in	O
many	O
of	O
the	O
tasks	O
to	O
which	O
we	O
would	O
like	O
to	O
apply	O
reinforcement	B
learning	I
the	O
state	B
space	O
is	O
combinatorial	O
and	O
enormous	O
the	O
number	O
of	O
possible	O
camera	O
images	O
for	O
example	O
is	O
much	O
larger	O
than	O
the	O
number	O
of	O
atoms	O
in	O
the	O
universe	O
in	O
such	O
cases	O
we	O
cannot	O
expect	O
to	O
find	O
an	O
optimal	O
policy	B
or	O
the	O
optimal	O
value	B
function	I
even	O
in	O
the	O
limit	O
of	O
infinite	O
time	O
and	O
data	O
our	O
goal	O
instead	O
is	O
to	O
find	O
a	O
good	O
approximate	O
solution	O
using	O
limited	O
computational	O
resources	O
in	O
this	O
part	O
of	O
the	O
book	O
we	O
explore	O
such	O
approximate	O
solution	O
methods	O
the	O
problem	O
with	O
large	O
state	B
spaces	O
is	O
not	O
just	O
the	O
memory	O
needed	O
for	O
large	O
tables	O
but	O
the	O
time	O
and	O
data	O
needed	O
to	O
fill	O
them	O
accurately	O
in	O
many	O
of	O
our	O
target	O
tasks	O
almost	O
every	O
state	B
encountered	O
will	O
never	O
have	O
been	O
seen	O
before	O
to	O
make	O
sensible	O
decisions	O
in	O
such	O
states	O
it	O
is	O
necessary	O
to	O
generalize	O
from	O
previous	O
encounters	O
with	O
different	O
states	O
that	O
are	O
in	O
some	O
sense	O
similar	O
to	O
the	O
current	O
one	O
in	O
other	O
words	O
the	O
key	O
issue	O
is	O
that	O
of	O
generalization	O
how	O
can	O
experience	O
with	O
a	O
limited	O
subset	O
of	O
the	O
state	B
space	O
be	O
usefully	O
generalized	O
to	O
produce	O
a	O
good	O
approximation	O
over	O
a	O
much	O
larger	O
subset	O
fortunately	O
generalization	O
from	O
examples	O
has	O
already	O
been	O
extensively	O
studied	O
and	O
we	O
do	O
not	O
need	O
to	O
invent	O
totally	O
new	O
methods	O
for	O
use	O
in	O
reinforcement	B
learning	I
to	O
some	O
extent	O
we	O
need	O
only	O
combine	O
reinforcement	B
learning	I
methods	O
with	O
existing	O
generalization	O
methods	O
the	O
kind	O
of	O
generalization	O
we	O
require	O
is	O
often	O
called	O
function	B
approximation	I
because	O
it	O
takes	O
examples	O
from	O
a	O
desired	O
function	O
a	O
value	B
function	I
and	O
attempts	O
to	O
generalize	O
from	O
them	O
to	O
construct	O
an	O
approximation	O
of	O
the	O
entire	O
function	O
function	B
approximation	I
is	O
an	O
instance	O
of	O
supervised	B
learning	I
the	O
primary	O
topic	O
studied	O
in	O
machine	O
learning	O
artificial	B
neural	B
networks	I
pattern	O
recognition	O
and	O
statistical	O
curve	O
fitting	O
in	O
theory	O
any	O
of	O
the	O
methods	O
studied	O
in	O
these	O
fields	O
can	O
be	O
used	O
in	O
the	O
role	O
of	O
function	O
approximator	O
within	O
reinforcement	B
learning	I
algorithms	O
although	O
in	O
practice	O
some	O
fit	O
more	O
easily	O
into	O
this	O
role	O
than	O
others	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
involves	O
a	O
number	O
of	O
new	O
issues	O
that	O
do	O
not	O
normally	O
arise	O
in	O
conventional	O
supervised	B
learning	I
such	O
as	O
nonstationarity	B
bootstrapping	B
and	O
delayed	O
targets	O
we	O
introduce	O
these	O
and	O
other	O
issues	O
successively	O
over	O
the	O
five	O
chapters	O
of	O
this	O
part	O
initially	O
we	O
restrict	O
attention	O
to	O
on-policy	O
training	O
treating	O
in	O
chapter	O
the	O
prediction	B
case	O
in	O
which	O
the	O
policy	B
is	O
given	O
and	O
only	O
its	O
value	B
function	I
is	O
approximated	O
and	O
then	O
in	O
chapter	O
the	O
control	B
case	O
in	O
which	O
an	O
approximation	O
to	O
the	O
optimal	O
policy	B
is	O
found	O
the	O
challenging	O
problem	O
of	O
off-policy	B
learning	O
with	B
function	B
approximation	I
is	O
treated	O
in	O
chapter	O
in	O
each	O
of	O
these	O
three	O
chapters	O
we	O
will	O
have	O
to	O
return	B
to	O
first	O
principles	O
and	O
re-examine	O
the	O
objectives	O
of	O
the	O
learning	O
to	O
take	O
into	O
account	O
function	B
approximation	I
chapter	O
introduces	O
and	O
analyzes	O
the	O
algorithmic	O
mechanism	O
of	O
eligibility	B
traces	I
which	O
dramatically	O
improves	O
the	O
computational	O
properties	O
of	O
multi-step	O
reinforcement	B
learning	I
methods	O
in	O
many	O
cases	O
the	O
final	O
chapter	O
of	O
this	O
part	O
explores	O
a	O
different	O
approach	O
to	O
control	B
policy-gradient	O
methods	O
which	O
approximate	O
the	O
optimal	O
policy	B
directly	O
and	O
need	O
never	O
form	O
an	O
approximate	O
value	B
function	I
they	O
may	O
be	O
much	O
more	O
efficient	O
if	O
they	O
do	O
approximate	O
a	O
value	B
function	I
as	O
well	O
the	O
policy	B
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
in	O
this	O
chapter	O
we	O
begin	O
our	O
study	O
of	O
function	B
approximation	I
in	O
reinforcement	B
learning	I
by	O
considering	O
its	O
use	O
in	O
estimating	O
the	O
state-value	O
function	O
from	O
on-policy	O
data	O
that	O
is	O
in	O
approximating	O
v	O
from	O
experience	O
generated	O
using	O
a	O
known	O
policy	B
the	O
novelty	O
in	O
this	O
chapter	O
is	O
that	O
the	O
approximate	O
value	B
function	I
is	O
represented	O
not	O
as	O
a	O
table	O
but	O
as	O
a	O
parameterized	O
functional	O
form	O
with	O
weight	O
vector	B
w	O
rd	O
we	O
will	O
write	O
vsw	O
v	O
for	O
the	O
approximate	O
value	B
of	O
state	B
s	O
given	O
weight	O
vector	B
w	O
for	O
example	O
v	O
might	O
be	O
a	O
linear	O
function	O
in	O
features	O
of	O
the	O
state	B
with	O
w	O
the	O
vector	B
of	O
feature	O
weights	O
more	O
generally	O
v	O
might	O
be	O
the	O
function	O
computed	O
by	O
a	O
multi-layer	O
artificial	O
neural	B
network	O
with	O
w	O
the	O
vector	B
of	O
connection	O
weights	O
in	O
all	O
the	O
layers	O
by	O
adjusting	O
the	O
weights	O
any	O
of	O
a	O
wide	O
range	O
of	O
different	O
functions	O
can	O
be	O
implemented	O
by	O
the	O
network	O
or	O
v	O
might	O
be	O
the	O
function	O
computed	O
by	O
a	O
decision	O
tree	O
where	O
w	O
is	O
all	O
the	O
numbers	O
defining	O
the	O
split	O
points	O
and	O
leaf	O
values	O
of	O
the	O
tree	O
typically	O
the	O
number	O
of	O
weights	O
dimensionality	O
of	O
w	O
is	O
much	O
less	O
than	O
the	O
number	O
of	O
states	O
and	O
changing	O
one	O
weight	O
changes	O
the	O
estimated	O
value	B
of	O
many	O
states	O
consequently	O
when	O
a	O
single	O
state	B
is	O
updated	O
the	O
change	O
generalizes	O
from	O
that	O
state	B
to	O
affect	O
the	O
values	O
of	O
many	O
other	O
states	O
such	O
generalization	O
makes	O
the	O
learning	O
potentially	O
more	O
powerful	O
but	O
also	O
potentially	O
more	O
difficult	O
to	O
manage	O
and	O
understand	O
perhaps	O
surprisingly	O
extending	O
reinforcement	B
learning	I
to	O
function	B
approximation	I
also	O
makes	O
it	O
applicable	O
to	O
partially	O
observable	O
problems	O
in	O
which	O
the	O
full	O
state	B
is	O
not	O
available	O
to	O
the	O
agent	O
if	O
the	O
parameterized	O
function	O
form	O
for	O
v	O
does	O
not	O
allow	O
the	O
estimated	O
value	B
to	O
depend	O
on	O
certain	O
aspects	O
of	O
the	O
state	B
then	O
it	O
is	O
just	O
as	O
if	O
those	O
aspects	O
are	O
unobservable	O
in	O
fact	O
all	O
the	O
theoretical	O
results	O
for	O
methods	O
using	O
function	B
approximation	I
presented	O
in	O
this	O
part	O
of	O
the	O
book	O
apply	O
equally	O
well	O
to	O
cases	O
of	O
partial	O
observability	O
what	O
function	B
approximation	I
can	O
t	O
do	O
however	O
is	O
augment	O
the	O
state	B
representation	O
with	O
memories	O
of	O
past	O
observations	O
some	O
such	O
possible	O
further	O
extensions	O
are	O
discussed	O
briefly	O
in	O
section	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
value-function	B
approximation	I
all	O
of	O
the	O
prediction	B
methods	O
covered	O
in	O
this	O
book	O
have	O
been	O
described	O
as	O
updates	O
to	O
an	O
estimated	O
value	B
function	I
that	O
shift	O
its	O
value	B
at	O
particular	O
states	O
toward	O
a	O
backedup	O
value	B
or	O
update	O
target	O
for	O
that	O
state	B
let	O
us	O
refer	O
to	O
an	O
individual	O
update	O
by	O
the	O
notation	O
s	O
u	O
where	O
s	O
is	O
the	O
state	B
updated	O
and	O
u	O
is	O
the	O
update	O
target	O
that	O
s	O
s	O
estimated	O
value	B
is	O
shifted	O
toward	O
for	O
example	O
the	O
monte	B
carlo	I
update	O
for	O
value	B
prediction	B
is	O
st	O
gt	O
the	O
update	O
is	O
st	O
and	O
the	O
n-step	B
td	B
update	O
is	O
st	O
gttn	O
in	O
the	O
dp	O
programming	O
policy-evaluation	O
update	O
s	O
e	O
st	O
s	O
an	O
arbitrary	O
state	B
s	O
is	O
updated	O
whereas	O
in	O
the	O
other	O
cases	O
the	O
state	B
encountered	O
in	O
actual	O
experience	O
st	O
is	O
updated	O
it	O
is	O
natural	O
to	O
interpret	O
each	O
update	O
as	O
specifying	O
an	O
example	O
of	O
the	O
desired	O
input	O
output	O
behavior	O
of	O
the	O
value	B
function	I
in	O
a	O
sense	O
the	O
update	O
s	O
u	O
means	O
that	O
the	O
estimated	O
value	B
for	O
state	B
s	O
should	O
be	O
more	O
like	O
the	O
update	O
target	O
u	O
up	O
to	O
now	O
the	O
actual	O
update	O
has	O
been	O
trivial	O
the	O
table	O
entry	O
for	O
s	O
s	O
estimated	O
value	B
has	O
simply	O
been	O
shifted	O
a	O
fraction	O
of	O
the	O
way	O
toward	O
u	O
and	O
the	O
estimated	O
values	O
of	O
all	O
other	O
states	O
were	O
left	O
unchanged	O
now	O
we	O
permit	O
arbitrarily	O
complex	O
and	O
sophisticated	O
methods	O
to	O
implement	O
the	O
update	O
and	O
updating	O
at	O
s	O
generalizes	O
so	O
that	O
the	O
estimated	O
values	O
of	O
many	O
other	O
states	O
are	O
changed	O
as	O
well	O
machine	O
learning	O
methods	O
that	O
learn	O
to	O
mimic	O
input	O
output	O
examples	O
in	O
this	O
way	O
are	O
called	O
supervised	B
learning	I
methods	O
and	O
when	O
the	O
outputs	O
are	O
numbers	O
like	O
u	O
the	O
process	O
is	O
often	O
called	O
function	B
approximation	I
function	B
approximation	I
methods	O
expect	O
to	O
receive	O
examples	O
of	O
the	O
desired	O
input	O
output	O
behavior	O
of	O
the	O
function	O
they	O
are	O
trying	O
to	O
approximate	O
we	O
use	O
these	O
methods	O
for	O
value	B
prediction	B
simply	O
by	O
passing	O
to	O
them	O
the	O
s	O
g	O
of	O
each	O
update	O
as	O
a	O
training	O
example	O
we	O
then	O
interpret	O
the	O
approximate	O
function	O
they	O
produce	O
as	O
an	O
estimated	O
value	B
function	I
viewing	O
each	O
update	O
as	O
a	O
conventional	O
training	O
example	O
in	O
this	O
way	O
enables	O
us	O
to	O
use	O
any	O
of	O
a	O
wide	O
range	O
of	O
existing	O
function	B
approximation	I
methods	O
for	O
value	B
prediction	B
in	O
principle	O
we	O
can	O
use	O
any	O
method	O
for	O
supervised	B
learning	I
from	O
examples	O
including	O
artificial	B
neural	B
networks	I
decision	O
trees	O
and	O
various	O
kinds	O
of	O
multivariate	O
regression	O
however	O
not	O
all	O
function	B
approximation	I
methods	O
are	O
equally	O
well	O
suited	O
for	O
use	O
in	O
reinforcement	B
learning	I
the	O
most	O
sophisticated	O
neural	B
network	O
and	O
statistical	O
methods	O
all	O
assume	O
a	O
static	O
training	O
set	O
over	O
which	O
multiple	O
passes	O
are	O
made	O
in	O
reinforcement	B
learning	I
however	O
it	O
is	O
important	O
that	O
learning	O
be	O
able	O
to	O
occur	O
online	B
while	O
the	O
agent	O
interacts	O
with	O
its	O
environment	B
or	O
with	O
a	O
model	O
of	O
its	O
environment	B
to	O
do	O
this	O
requires	O
methods	O
that	O
are	O
able	O
to	O
learn	O
efficiently	O
from	O
incrementally	O
acquired	O
data	O
in	O
addition	O
reinforcement	B
learning	I
generally	O
requires	O
function	B
approximation	I
methods	O
able	O
to	O
handle	O
nonstationary	O
target	O
functions	O
functions	O
that	O
change	O
over	O
time	O
for	O
example	O
in	O
control	B
methods	O
based	O
on	O
gpi	O
policy	B
iteration	I
we	O
often	O
seek	O
to	O
learn	O
q	O
while	O
changes	O
even	O
if	O
the	O
policy	B
remains	O
the	O
same	O
the	O
target	O
values	O
of	O
training	O
examples	O
are	O
nonstationary	O
if	O
they	O
are	O
generated	O
by	O
bootstrapping	B
methods	O
and	B
td	B
learning	I
methods	O
that	O
cannot	O
easily	O
handle	O
such	O
nonstationarity	B
are	O
less	O
suitable	O
for	O
reinforcement	B
learning	I
the	O
prediction	B
objective	O
the	O
prediction	B
objective	O
up	O
to	O
now	O
we	O
have	O
not	O
specified	O
an	O
explicit	O
objective	O
for	O
prediction	B
in	O
the	O
tabular	O
case	O
a	O
continuous	O
measure	O
of	O
prediction	B
quality	O
was	O
not	O
necessary	O
because	O
the	O
learned	O
value	B
function	I
could	O
come	O
to	O
equal	O
the	O
true	O
value	B
function	I
exactly	O
moreover	O
the	O
learned	O
values	O
at	O
each	O
state	B
were	O
decoupled	O
an	O
update	O
at	O
one	O
state	B
affected	O
no	O
other	O
but	O
with	O
genuine	O
approximation	O
an	O
update	O
at	O
one	O
state	B
affects	O
many	O
others	O
and	O
it	O
is	O
not	O
possible	O
to	O
get	O
the	O
values	O
of	O
all	O
states	O
exactly	O
correct	O
by	O
assumption	O
we	O
have	O
far	O
more	O
states	O
than	O
weights	O
so	O
making	O
one	O
state	B
s	O
estimate	O
more	O
accurate	O
invariably	O
means	O
making	O
others	O
less	O
accurate	O
we	O
are	O
obligated	O
then	O
to	O
say	O
which	O
states	O
we	O
care	O
most	O
about	O
we	O
must	O
specify	O
a	O
state	B
distribution	O
representing	O
how	O
much	O
we	O
care	O
about	O
the	O
error	O
in	O
each	O
state	B
s	O
by	O
the	O
error	O
in	O
a	O
state	B
s	O
we	O
mean	O
the	O
square	O
of	O
the	O
difference	O
between	O
the	O
approximate	O
value	B
vsw	O
and	O
the	O
true	O
value	B
v	O
weighting	O
this	O
over	O
the	O
state	B
space	O
by	O
we	O
obtain	O
a	O
natural	O
objective	O
function	O
the	O
mean	O
squared	O
value	B
error	I
denoted	O
ve	O
vew	O
s	O
the	O
square	O
root	O
of	O
this	O
measure	O
the	O
root	O
ve	O
gives	O
a	O
rough	O
measure	O
of	O
how	O
much	O
the	O
approximate	O
values	O
differ	O
from	O
the	O
true	O
values	O
and	O
is	O
often	O
used	O
in	O
plots	O
often	O
is	O
chosen	O
to	O
be	O
the	O
fraction	O
of	O
time	O
spent	O
in	O
s	O
under	O
on-policy	O
training	O
this	O
is	O
called	O
the	O
on-policy	B
distribution	I
we	O
focus	O
entirely	O
on	O
this	O
case	O
in	O
this	O
chapter	O
in	O
continuing	B
tasks	I
the	O
on-policy	B
distribution	I
is	O
the	O
stationary	O
distribution	O
under	O
the	O
on-policy	B
distribution	I
in	O
episodic	O
tasks	O
in	O
an	O
episodic	O
task	O
the	O
on-policy	B
distribution	I
is	O
a	O
little	O
different	O
in	O
that	O
it	O
depends	O
on	O
how	O
the	O
initial	O
states	O
of	O
episodes	B
are	O
chosen	O
let	O
hs	O
denote	O
the	O
probability	O
that	O
an	O
episode	O
begins	O
in	O
each	O
state	B
s	O
and	O
let	O
denote	O
the	O
number	O
of	O
time	O
steps	O
spent	O
on	O
average	O
in	O
state	B
s	O
in	O
a	O
single	O
episode	O
time	O
is	O
spent	O
in	O
a	O
state	B
s	O
if	O
episodes	B
start	O
in	O
s	O
or	O
if	O
transitions	O
are	O
made	O
into	O
s	O
from	O
a	O
preceding	O
state	B
s	O
in	O
which	O
time	O
is	O
spent	O
hs	O
s	O
sps	O
s	O
a	O
for	O
all	O
s	O
s	O
this	O
system	O
of	O
equations	O
can	O
be	O
solved	O
for	O
the	O
expected	B
number	O
of	O
visits	O
the	O
on-policy	B
distribution	I
is	O
then	O
the	O
fraction	O
of	O
time	O
spent	O
in	O
each	O
state	B
normalized	O
to	O
sum	O
to	O
one	O
for	O
all	O
s	O
s	O
this	O
is	O
the	O
natural	O
choice	O
without	O
discounting	B
if	O
there	O
is	O
discounting	B
it	O
should	O
be	O
treated	O
as	O
a	O
form	O
of	O
termination	O
which	O
can	O
be	O
done	O
simply	O
by	O
including	O
a	O
factor	O
of	O
in	O
the	O
second	O
term	O
of	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
the	O
two	O
cases	O
continuing	O
and	O
episodic	O
behave	O
similarly	O
but	O
with	B
approximation	I
they	O
must	O
be	O
treated	O
separately	O
in	O
formal	O
analyses	O
as	O
we	O
will	O
see	O
repeatedly	O
in	O
this	O
part	O
of	O
the	O
book	O
this	O
completes	O
the	O
specification	O
of	O
the	O
learning	O
objective	O
but	O
it	O
is	O
not	O
completely	O
clear	O
that	O
the	O
ve	O
is	O
the	O
right	O
performance	O
objective	O
for	O
reinforcement	B
learning	I
remember	O
that	O
our	O
ultimate	O
purpose	O
the	O
reason	O
we	O
are	O
learning	O
a	O
value	B
function	I
is	O
to	O
find	O
a	O
better	O
policy	B
the	O
best	O
value	B
function	I
for	O
this	O
purpose	O
is	O
not	O
necessarily	O
the	O
best	O
for	O
minimizing	O
ve	O
nevertheless	O
it	O
is	O
not	O
yet	O
clear	O
what	O
a	O
more	O
useful	O
alternative	O
goal	O
for	O
value	B
prediction	B
might	O
be	O
for	O
now	O
we	O
will	O
focus	O
on	O
ve	O
an	O
ideal	O
goal	O
in	O
terms	O
of	O
ve	O
would	O
be	O
to	O
find	O
a	O
global	O
optimum	O
a	O
weight	O
vector	B
w	O
for	O
which	O
vew	O
vew	O
for	O
all	O
possible	O
w	O
reaching	O
this	O
goal	O
is	O
sometimes	O
possible	O
for	O
simple	O
function	O
approximators	O
such	O
as	O
linear	O
ones	O
but	O
is	O
rarely	O
possible	O
for	O
complex	O
function	O
approximators	O
such	O
as	O
artificial	B
neural	B
networks	I
and	O
decision	O
trees	O
short	O
of	O
this	O
complex	O
function	O
approximators	O
may	O
seek	O
to	O
converge	O
instead	O
to	O
a	O
local	O
optimum	O
a	O
weight	O
vector	B
w	O
for	O
which	O
vew	O
vew	O
for	O
all	O
w	O
in	O
some	O
neighborhood	O
of	O
w	O
although	O
this	O
guarantee	O
is	O
only	O
slightly	O
reassuring	O
it	O
is	O
typically	O
the	O
best	O
that	O
can	O
be	O
said	O
for	O
nonlinear	O
function	O
approximators	O
and	O
often	O
it	O
is	O
enough	O
still	O
for	O
many	O
cases	O
of	O
interest	O
in	O
reinforcement	B
learning	I
there	O
is	O
no	O
guarantee	O
of	O
convergence	O
to	O
an	O
optimum	O
or	O
even	O
to	O
within	O
a	O
bounded	O
distance	O
of	O
an	O
optimum	O
some	O
methods	O
may	O
in	O
fact	O
diverge	O
with	O
their	O
ve	O
approaching	O
infinity	O
in	O
the	O
limit	O
in	O
the	O
last	O
two	O
sections	O
we	O
outlined	O
a	O
framework	O
for	O
combining	O
a	O
wide	O
range	O
of	O
reinforcement	B
learning	I
methods	O
for	O
value	B
prediction	B
with	O
a	O
wide	O
range	O
of	O
function	B
approximation	I
methods	O
using	O
the	O
updates	O
of	O
the	O
former	O
to	O
generate	O
training	O
examples	O
for	O
the	O
latter	O
we	O
also	O
described	O
a	O
ve	O
performance	O
measure	O
which	O
these	O
methods	O
may	O
aspire	O
to	O
minimize	O
the	O
range	O
of	O
possible	O
function	B
approximation	I
methods	O
is	O
far	O
too	O
large	O
to	O
cover	O
all	O
and	O
anyway	O
too	O
little	O
is	O
known	O
about	O
most	O
of	O
them	O
to	O
make	O
a	O
reliable	O
evaluation	O
or	O
recommendation	O
of	O
necessity	O
we	O
consider	O
only	O
a	O
few	O
possibilities	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
we	O
focus	O
on	O
function	B
approximation	I
methods	O
based	O
on	O
gradient	B
principles	O
and	O
on	O
linear	O
gradient-descent	O
methods	O
in	O
particular	O
we	O
focus	O
on	O
these	O
methods	O
in	O
part	O
because	O
we	O
consider	O
them	O
to	O
be	O
particularly	O
promising	O
and	O
because	O
they	O
reveal	O
key	O
theoretical	O
issues	O
but	O
also	O
because	O
they	O
are	O
simple	O
and	O
our	O
space	O
is	O
limited	O
stochastic-gradient	O
and	O
semi-gradient	B
methods	I
we	O
now	O
develop	O
in	O
detail	O
one	O
class	O
of	O
learning	O
methods	O
for	O
function	B
approximation	I
in	O
value	B
prediction	B
those	O
based	O
on	O
stochastic	O
gradient	B
descent	O
sgd	O
methods	O
are	O
among	O
the	O
most	O
widely	O
used	O
of	O
all	O
function	B
approximation	I
methods	O
and	O
are	O
particularly	O
well	O
suited	O
to	O
online	B
reinforcement	B
learning	I
in	O
gradient-descent	O
methods	O
the	O
weight	O
vector	B
is	O
a	O
column	O
vector	B
with	O
a	O
fixed	O
number	O
and	O
the	O
approximate	O
value	B
function	I
of	O
real	O
valued	O
components	O
w	O
vsw	O
is	O
a	O
differentiable	O
function	O
of	O
w	O
for	O
all	O
s	O
s	O
we	O
will	O
be	O
updating	O
w	O
at	O
each	O
of	O
a	O
series	O
of	O
discrete	O
time	O
steps	O
t	O
so	O
we	O
will	O
need	O
a	O
notation	O
wt	O
for	O
the	O
denotes	O
transpose	O
needed	O
here	O
to	O
turn	O
the	O
horizontal	O
row	O
vector	B
in	O
the	O
text	O
into	O
a	O
vertical	O
column	O
vector	B
in	O
this	O
book	O
vectors	O
are	O
generally	O
taken	O
to	O
be	O
column	O
vectors	O
unless	O
explicitly	O
written	O
out	O
horizontally	O
or	O
transposed	O
stochastic-gradient	O
and	O
semi-gradient	B
methods	I
weight	O
vector	B
at	O
each	O
step	O
for	O
now	O
let	O
us	O
assume	O
that	O
on	O
each	O
step	O
we	O
observe	O
a	O
new	O
example	O
st	O
v	O
consisting	O
of	O
a	O
randomly	O
selected	O
state	B
st	O
and	O
its	O
true	O
value	B
under	O
the	O
policy	B
these	O
states	O
might	O
be	O
successive	O
states	O
from	O
an	O
interaction	O
with	O
the	O
environment	B
but	O
for	O
now	O
we	O
do	O
not	O
assume	O
so	O
even	O
though	O
we	O
are	O
given	O
the	O
exact	O
correct	O
values	O
v	O
for	O
each	O
st	O
there	O
is	O
still	O
a	O
difficult	O
problem	O
because	O
our	O
function	O
approximator	O
has	O
limited	O
resources	O
and	O
thus	O
limited	O
resolution	O
in	O
particular	O
there	O
is	O
generally	O
no	O
w	O
that	O
gets	O
all	O
the	O
states	O
or	O
even	O
all	O
the	O
examples	O
exactly	O
correct	O
in	O
addition	O
we	O
must	O
generalize	O
to	O
all	O
the	O
other	O
states	O
that	O
have	O
not	O
appeared	O
in	O
examples	O
we	O
assume	O
that	O
states	O
appear	O
in	O
examples	O
with	O
the	O
same	O
distribution	O
over	O
which	O
we	O
are	O
trying	O
to	O
minimize	O
the	O
ve	O
as	O
given	O
by	O
a	O
good	O
strategy	O
in	O
this	O
case	O
is	O
to	O
try	O
to	O
minimize	O
error	O
on	O
the	O
observed	O
examples	O
stochastic	O
gradient-descent	O
methods	O
do	O
this	O
by	O
adjusting	O
the	O
weight	O
vector	B
after	O
each	O
example	O
by	O
a	O
small	O
amount	O
in	O
the	O
direction	O
that	O
would	O
most	O
reduce	O
the	O
error	O
on	O
that	O
example	O
wt	O
wt	O
vstwt	O
where	O
is	O
a	O
positive	O
step-size	B
parameter	I
and	O
f	O
for	O
any	O
scalar	O
expression	O
f	O
that	O
is	O
a	O
function	O
of	O
a	O
vector	B
w	O
denotes	O
the	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
the	O
expression	O
with	O
respect	O
to	O
the	O
components	O
of	O
the	O
vector	B
f	O
f	O
f	O
f	O
wd	O
this	O
derivative	O
vector	B
is	O
the	O
gradient	B
of	O
f	O
with	O
respect	O
to	O
w	O
sgd	O
methods	O
are	O
gradient	B
descent	O
methods	O
because	O
the	O
overall	O
step	O
in	O
wt	O
is	O
proportional	O
to	O
the	O
negative	O
gradient	B
of	O
the	O
example	O
s	O
squared	O
error	O
this	O
is	O
the	O
direction	O
in	O
which	O
the	O
error	O
falls	O
most	O
rapidly	O
gradient	B
descent	O
methods	O
are	O
called	O
stochastic	O
when	O
the	O
update	O
is	O
done	O
as	O
here	O
on	O
only	O
a	O
single	O
example	O
which	O
might	O
have	O
been	O
selected	O
stochastically	O
over	O
many	O
examples	O
making	O
small	O
steps	O
the	O
overall	O
effect	O
is	O
to	O
minimize	O
an	O
average	O
performance	O
measure	O
such	O
as	O
the	O
ve	O
it	O
may	O
not	O
be	O
immediately	O
apparent	O
why	O
sgd	O
takes	O
only	O
a	O
small	O
step	O
in	O
the	O
direction	O
of	O
the	O
gradient	B
could	O
we	O
not	O
move	O
all	O
the	O
way	O
in	O
this	O
direction	O
and	O
completely	O
eliminate	O
the	O
error	O
on	O
the	O
example	O
in	O
many	O
cases	O
this	O
could	O
be	O
done	O
but	O
usually	O
it	O
is	O
not	O
desirable	O
remember	O
that	O
we	O
do	O
not	O
seek	O
or	O
expect	O
to	O
find	O
a	O
value	B
function	I
that	O
has	O
zero	O
error	O
for	O
all	O
states	O
but	O
only	O
an	O
approximation	O
that	O
balances	O
the	O
errors	O
in	O
different	O
states	O
if	O
we	O
completely	O
corrected	O
each	O
example	O
in	O
one	O
step	O
then	O
we	O
would	O
not	O
find	O
such	O
a	O
balance	O
in	O
fact	O
the	O
convergence	O
results	O
for	O
sgd	O
methods	O
assume	O
that	O
decreases	O
over	O
time	O
if	O
it	O
decreases	O
in	O
such	O
a	O
way	O
as	O
to	O
satisfy	O
the	O
standard	O
stochastic	O
approximation	O
conditions	O
then	O
the	O
sgd	O
method	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
local	O
optimum	O
we	O
turn	O
now	O
to	O
the	O
case	O
in	O
which	O
the	O
target	O
output	O
here	O
denoted	O
ut	O
r	O
of	O
the	O
tth	O
training	O
example	O
st	O
ut	O
is	O
not	O
the	O
true	O
value	B
v	O
but	O
some	O
possibly	O
random	O
approximation	O
to	O
it	O
for	O
example	O
ut	O
might	O
be	O
a	O
noise-corrupted	O
version	O
of	O
v	O
or	O
it	O
might	O
be	O
one	O
of	O
the	O
bootstrapping	B
targets	O
using	O
v	O
mentioned	O
in	O
the	O
previous	O
section	O
in	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
these	O
cases	O
we	O
cannot	O
perform	O
the	O
exact	O
update	O
because	O
v	O
is	O
unknown	O
but	O
we	O
can	O
approximate	O
it	O
by	O
substituting	O
ut	O
in	O
place	O
of	O
v	O
this	O
yields	O
the	O
following	O
general	O
sgd	O
method	O
for	O
state-value	O
prediction	B
wt	O
vstwt	O
if	O
ut	O
is	O
an	O
unbiased	O
estimate	O
that	O
is	O
if	O
eutst	O
s	O
v	O
for	O
each	O
t	O
then	O
wt	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
local	O
optimum	O
under	O
the	O
usual	O
stochastic	O
approximation	O
conditions	O
for	O
decreasing	O
for	O
example	O
suppose	O
the	O
states	O
in	O
the	O
examples	O
are	O
the	O
states	O
generated	O
by	O
interaction	O
simulated	O
interaction	O
with	O
the	O
environment	B
using	O
policy	B
because	O
the	O
true	O
value	B
of	O
a	O
state	B
is	O
the	O
expected	B
value	B
of	O
the	O
return	B
following	O
it	O
the	O
monte	B
carlo	I
target	O
ut	O
gt	O
is	O
by	O
definition	O
an	O
unbiased	O
estimate	O
of	O
v	O
with	O
this	O
choice	O
the	O
general	O
sgd	O
method	O
converges	O
to	O
a	O
locally	O
optimal	O
approximation	O
to	O
v	O
thus	O
the	O
gradient-descent	O
version	O
of	O
monte	B
carlo	I
state-value	O
prediction	B
is	O
guaranteed	O
to	O
find	O
a	O
locally	O
optimal	O
solution	O
pseudocode	O
for	O
a	O
complete	O
algorithm	O
is	O
shown	O
in	O
the	O
box	O
below	O
gradient	B
monte	B
carlo	I
algorithm	O
for	O
estimating	O
v	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
input	O
a	O
differentiable	O
function	O
v	O
s	O
rd	O
r	O
algorithm	O
parameter	O
step	O
size	O
initialize	O
value-function	O
weights	O
w	O
rd	O
arbitrarily	O
w	O
loop	O
forever	O
each	O
episode	O
generate	O
an	O
episode	O
rt	O
st	O
using	O
loop	O
for	O
each	O
step	O
of	O
episode	O
t	O
t	O
w	O
w	O
vstw	O
one	O
does	O
not	O
obtain	O
the	O
same	O
guarantees	O
if	O
a	O
bootstrapping	B
estimate	O
of	O
v	O
is	O
used	O
as	O
the	O
target	O
ut	O
in	O
bootstrapping	B
targets	O
such	O
as	O
n-step	B
returns	O
gttn	O
or	O
the	O
dp	O
rst	O
ar	O
all	O
depend	O
on	O
the	O
current	O
value	B
of	O
the	O
weight	O
vector	B
wt	O
which	O
implies	O
that	O
they	O
will	O
be	O
biased	O
and	O
that	O
they	O
will	O
not	O
produce	O
a	O
true	O
gradient-descent	O
method	O
one	O
way	O
to	O
look	O
at	O
this	O
is	O
that	O
the	O
key	O
step	O
from	O
to	O
relies	O
on	O
the	O
target	O
being	O
independent	O
of	O
wt	O
this	O
step	O
would	O
not	O
be	O
valid	O
if	O
a	O
bootstrapping	B
estimate	O
were	O
used	O
in	O
place	O
of	O
v	O
bootstrapping	B
methods	O
are	O
not	O
in	O
fact	O
instances	O
of	O
true	O
gradient	B
descent	O
they	O
take	O
into	O
account	O
the	O
effect	O
of	O
changing	O
the	O
weight	O
vector	B
wt	O
on	O
the	O
estimate	O
but	O
ignore	O
its	O
effect	O
on	O
the	O
target	O
they	O
include	O
only	O
a	O
part	O
of	O
the	O
gradient	B
and	O
accordingly	O
we	O
call	O
them	O
semi-gradient	B
methods	I
although	O
semi-gradient	B
methods	I
do	O
not	O
converge	O
as	O
robustly	O
as	O
gradient	B
methods	O
they	O
do	O
converge	O
reliably	O
in	O
important	O
cases	O
such	O
as	O
the	O
linear	O
case	O
discussed	O
in	O
the	O
next	O
section	O
moreover	O
they	O
offer	O
important	O
advantages	O
that	O
make	O
them	O
often	O
clearly	O
preferred	O
one	O
reason	O
for	O
this	O
is	O
that	O
they	O
typically	O
enable	O
significantly	O
faster	O
learning	O
as	O
we	O
have	O
seen	O
in	O
chapters	O
and	O
another	O
is	O
that	O
they	O
enable	O
learning	O
stochastic-gradient	O
and	O
semi-gradient	B
methods	I
to	O
be	O
continual	O
and	O
online	B
without	O
waiting	O
for	O
the	O
end	O
of	O
an	O
episode	O
this	O
enables	O
them	O
to	O
be	O
used	O
on	O
continuing	O
problems	O
and	O
provides	O
computational	O
advantages	O
a	O
prototyp	O
ical	O
semi-gradient	O
method	O
is	O
semi-gradient	O
which	O
uses	O
ut	O
as	O
its	O
target	O
complete	O
pseudocode	O
for	O
this	O
method	O
is	O
given	O
in	O
the	O
box	O
below	O
semi-gradient	O
for	O
estimating	O
v	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
input	O
a	O
differentiable	O
function	O
v	O
s	O
rd	O
r	O
such	O
that	O
vterminal	O
algorithm	O
parameter	O
step	O
size	O
initialize	O
value-function	O
weights	O
w	O
rd	O
arbitrarily	O
w	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
choose	O
a	O
take	O
action	B
a	O
observe	O
r	O
w	O
w	O
vsw	O
s	O
until	O
is	O
terminal	O
state	B
aggregation	I
is	O
a	O
simple	O
form	O
of	O
generalizing	O
function	B
approximation	I
in	O
which	O
states	O
are	O
grouped	O
together	O
with	O
one	O
estimated	O
value	B
component	O
of	O
the	O
weight	O
vector	B
w	O
for	O
each	O
group	O
the	O
value	B
of	O
a	O
state	B
is	O
estimated	O
as	O
its	O
group	O
s	O
component	O
and	O
when	O
the	O
state	B
is	O
updated	O
that	O
component	O
alone	O
is	O
updated	O
state	B
aggregation	I
is	O
a	O
special	O
case	O
of	O
sgd	O
in	O
which	O
the	O
gradient	B
vstwt	O
is	O
for	O
st	O
s	O
group	O
s	O
component	O
and	O
for	O
the	O
other	O
components	O
example	O
state	B
aggregation	I
on	O
the	O
random	B
walk	I
consider	O
a	O
version	O
of	O
the	O
random	B
walk	I
task	O
and	O
on	O
pages	O
and	O
the	O
states	O
are	O
numbered	O
from	O
to	O
left	O
to	O
right	O
and	O
all	O
episodes	B
begin	O
near	O
the	O
center	O
in	O
state	B
state	B
transitions	O
are	O
from	O
the	O
current	O
state	B
to	O
one	O
of	O
the	O
neighboring	O
states	O
to	O
its	O
left	O
or	O
to	O
one	O
of	O
the	O
neighboring	O
states	O
to	O
its	O
right	O
all	O
with	O
equal	O
probability	O
of	O
course	O
if	O
the	O
current	O
state	B
is	O
near	O
an	O
edge	O
then	O
there	O
may	O
be	O
fewer	O
than	O
neighbors	O
on	O
that	O
side	O
of	O
it	O
in	O
this	O
case	O
all	O
the	O
probability	O
that	O
would	O
have	O
gone	O
into	O
those	O
missing	O
neighbors	O
goes	O
into	O
the	O
probability	O
of	O
terminating	O
on	O
that	O
side	O
state	B
has	O
a	O
chance	O
of	O
terminating	O
on	O
the	O
left	O
and	O
state	B
has	O
a	O
chance	O
of	O
terminating	O
on	O
the	O
right	O
as	O
usual	O
termination	O
on	O
the	O
left	O
produces	O
a	O
reward	O
of	O
and	O
termination	O
on	O
the	O
right	O
produces	O
a	O
reward	O
of	O
all	O
other	O
transitions	O
have	O
a	O
reward	O
of	O
zero	O
we	O
use	O
this	O
task	O
as	O
a	O
running	O
example	O
throughout	O
this	O
section	O
figure	O
shows	O
the	O
true	O
value	B
function	I
v	O
for	O
this	O
task	O
it	O
is	O
nearly	O
a	O
straight	O
line	O
but	O
curving	O
slightly	O
toward	O
the	O
horizontal	O
for	O
the	O
last	O
states	O
at	O
each	O
end	O
also	O
shown	O
is	O
the	O
final	O
approximate	O
value	B
function	I
learned	O
by	O
the	O
gradient	B
monte-carlo	O
algorithm	O
with	O
state	B
aggregation	I
after	O
episodes	B
with	O
a	O
step	O
size	O
of	O
for	O
the	O
state	B
aggregation	I
the	O
states	O
were	O
partitioned	O
into	O
groups	O
of	O
states	O
each	O
states	O
were	O
one	O
group	O
states	O
were	O
another	O
and	O
so	O
on	O
the	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
figure	O
function	B
approximation	I
by	O
state	B
aggregation	I
on	O
the	O
random	B
walk	I
task	O
using	O
the	O
gradient	B
monte	B
carlo	I
algorithm	O
staircase	O
effect	O
shown	O
in	O
the	O
figure	O
is	O
typical	O
of	O
state	B
aggregation	I
within	O
each	O
group	O
the	O
approximate	O
value	B
is	O
constant	O
and	O
it	O
changes	O
abruptly	O
from	O
one	O
group	O
to	O
the	O
next	O
these	O
approximate	O
values	O
are	O
close	O
to	O
the	O
global	O
minimum	O
of	O
the	O
ve	O
some	O
of	O
the	O
details	O
of	O
the	O
approximate	O
values	O
are	O
best	O
appreciated	O
by	O
reference	O
to	O
the	O
state	B
distribution	O
for	O
this	O
task	O
shown	O
in	O
the	O
lower	O
portion	O
of	O
the	O
figure	O
with	O
a	O
rightside	O
scale	O
state	B
in	O
the	O
center	O
is	O
the	O
first	O
state	B
of	O
every	O
episode	O
but	O
is	O
rarely	O
visited	O
again	O
on	O
average	O
about	O
of	O
the	O
time	O
steps	O
are	O
spent	O
in	O
the	O
start	O
state	B
the	O
states	O
reachable	O
in	O
one	O
step	O
from	O
the	O
start	O
state	B
are	O
the	O
second	O
most	O
visited	O
with	O
about	O
of	O
the	O
time	O
steps	O
being	O
spent	O
in	O
each	O
of	O
them	O
from	O
there	O
falls	O
off	O
almost	O
linearly	O
reaching	O
about	O
at	O
the	O
extreme	O
states	O
and	O
the	O
most	O
visible	O
effect	O
of	O
the	O
distribution	O
is	O
on	O
the	O
leftmost	O
groups	O
whose	O
values	O
are	O
clearly	O
shifted	O
higher	O
than	O
the	O
unweighted	O
average	O
of	O
the	O
true	O
values	O
of	O
states	O
within	O
the	O
group	O
and	O
on	O
the	O
rightmost	O
groups	O
whose	O
values	O
are	O
clearly	O
shifted	O
lower	O
this	O
is	O
due	O
to	O
the	O
states	O
in	O
these	O
areas	O
having	O
the	O
greatest	O
asymmetry	O
in	O
their	O
weightings	O
by	O
for	O
example	O
in	O
the	O
leftmost	O
group	O
state	B
is	O
weighted	O
more	O
than	O
times	O
more	O
strongly	O
than	O
state	B
thus	O
the	O
estimate	O
for	O
the	O
group	O
is	O
biased	O
toward	O
the	O
true	O
value	B
of	O
state	B
which	O
is	O
higher	O
than	O
the	O
true	O
value	B
of	O
state	B
linear	O
methods	O
one	O
of	O
the	O
most	O
important	O
special	O
cases	O
of	O
function	B
approximation	I
is	O
that	O
in	O
which	O
the	O
approximate	O
function	O
v	O
is	O
a	O
linear	O
function	O
of	O
the	O
weight	O
vector	B
w	O
corresponding	O
with	O
the	O
to	O
every	O
state	B
s	O
there	O
is	O
a	O
real-valued	O
vector	B
xs	O
same	O
number	O
of	O
components	O
as	O
w	O
linear	O
methods	O
approximate	O
state-value	O
function	O
by	O
true	O
valuev	O
approximate	O
mc	O
value	B
v	O
state	B
distribution	O
linear	O
methods	O
the	O
inner	O
product	O
between	O
w	O
and	O
xs	O
vsw	O
wixis	O
in	O
this	O
case	O
the	O
approximate	O
value	B
function	I
is	O
said	O
to	O
be	O
linear	O
in	O
the	O
weights	O
or	O
simply	O
linear	O
the	O
vector	B
xs	O
is	O
called	O
a	O
feature	O
vector	B
representing	O
state	B
s	O
each	O
component	O
xis	O
of	O
xs	O
is	O
the	O
value	B
of	O
a	O
function	O
xi	O
s	O
r	O
we	O
think	O
of	O
a	O
feature	O
as	O
the	O
entirety	O
of	O
one	O
of	O
these	O
functions	O
and	O
we	O
call	O
its	O
value	B
for	O
a	O
state	B
s	O
a	O
feature	O
of	O
s	O
for	O
linear	O
methods	O
features	O
are	O
basis	O
functions	O
because	O
they	O
form	O
a	O
linear	O
basis	O
for	O
the	O
set	O
of	O
approximate	O
functions	O
constructing	O
d-dimensional	O
feature	O
vectors	O
to	O
represent	O
states	O
is	O
the	O
same	O
as	O
selecting	O
a	O
set	O
of	O
d	O
basis	O
functions	O
features	O
may	O
be	O
defined	O
in	O
many	O
different	O
ways	O
we	O
cover	O
a	O
few	O
possibilities	O
in	O
the	O
next	O
sections	O
it	O
is	O
natural	O
to	O
use	O
sgd	O
updates	O
with	O
linear	O
function	B
approximation	I
the	O
gradient	B
of	O
the	O
approximate	O
value	B
function	I
with	O
respect	O
to	O
w	O
in	O
this	O
case	O
is	O
vsw	O
xs	O
thus	O
in	O
the	O
linear	O
case	O
the	O
general	O
sgd	O
update	O
reduces	O
to	O
a	O
particularly	O
simple	O
form	O
wt	O
because	O
it	O
is	O
so	O
simple	O
the	O
linear	O
sgd	O
case	O
is	O
one	O
of	O
the	O
most	O
favorable	O
for	O
mathematical	O
analysis	O
almost	O
all	O
useful	O
convergence	O
results	O
for	O
learning	O
systems	O
of	O
all	O
kinds	O
are	O
for	O
linear	O
simpler	O
function	B
approximation	I
methods	O
in	O
particular	O
in	O
the	O
linear	O
case	O
there	O
is	O
only	O
one	O
optimum	O
in	O
degenerate	O
cases	O
one	O
set	O
of	O
equally	O
good	O
optima	O
and	O
thus	O
any	O
method	O
that	O
is	O
guaranteed	O
to	O
converge	O
to	O
or	O
near	O
a	O
local	O
optimum	O
is	O
automatically	O
guaranteed	O
to	O
converge	O
to	O
or	O
near	O
the	O
global	O
optimum	O
for	O
example	O
the	O
gradient	B
monte	B
carlo	I
algorithm	O
presented	O
in	O
the	O
previous	O
section	O
converges	O
to	O
the	O
global	O
optimum	O
of	O
the	O
ve	O
under	O
linear	O
function	B
approximation	I
if	O
is	O
reduced	O
over	O
time	O
according	O
to	O
the	O
usual	O
conditions	O
the	O
semi-gradient	O
algorithm	O
presented	O
in	O
the	O
previous	O
section	O
also	O
converges	O
under	O
linear	O
function	B
approximation	I
but	O
this	O
does	O
not	O
follow	O
from	O
general	O
results	O
on	O
sgd	O
a	O
separate	O
theorem	B
is	O
necessary	O
the	O
weight	O
vector	B
converged	O
to	O
is	O
also	O
not	O
the	O
global	O
optimum	O
but	O
rather	O
a	O
point	O
near	O
the	O
local	O
optimum	O
it	O
is	O
useful	O
to	O
consider	O
this	O
important	O
case	O
in	O
more	O
detail	O
specifically	O
for	O
the	O
continuing	O
case	O
the	O
update	O
at	O
each	O
time	O
t	O
is	O
wt	O
wt	O
where	O
here	O
we	O
have	O
used	O
the	O
notational	O
shorthand	O
xt	O
xst	O
once	O
the	O
system	O
has	O
reached	O
steady	O
state	B
for	O
any	O
given	O
wt	O
the	O
expected	B
next	O
weight	O
vector	B
can	O
be	O
written	O
wt	O
awt	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
rd	O
rd	O
where	O
b	O
rd	O
and	O
a	O
from	O
it	O
is	O
clear	O
that	O
if	O
the	O
system	O
converges	O
it	O
must	O
converge	O
to	O
the	O
weight	O
vector	B
wtd	O
at	O
which	O
b	O
awtd	O
b	O
awtd	O
a	O
wtd	O
this	O
quantity	O
is	O
called	O
the	O
td	B
fixed	O
point	O
in	O
fact	O
linear	O
semi-gradient	O
converges	O
to	O
this	O
point	O
some	O
of	O
the	O
theory	O
proving	O
its	O
convergence	O
and	O
the	O
existence	O
of	O
the	O
inverse	O
above	O
is	O
given	O
in	O
the	O
box	O
proof	B
of	O
convergence	O
of	O
linear	O
what	O
properties	O
assure	O
convergence	O
of	O
the	O
linear	O
algorithm	O
some	O
insight	O
can	O
be	O
gained	O
by	O
rewriting	O
as	O
awt	O
b	O
note	O
that	O
the	O
matrix	O
a	O
multiplies	O
the	O
weight	O
vector	B
wt	O
and	O
not	O
b	O
only	O
a	O
is	O
important	O
to	O
convergence	O
to	O
develop	O
intuition	O
consider	O
the	O
special	O
case	O
in	O
which	O
a	O
is	O
a	O
diagonal	O
matrix	O
if	O
any	O
of	O
the	O
diagonal	O
elements	O
are	O
negative	O
then	O
the	O
corresponding	O
diagonal	O
element	O
of	O
i	O
a	O
will	O
be	O
greater	O
than	O
one	O
and	O
the	O
corresponding	O
component	O
of	O
wt	O
will	O
be	O
amplified	O
which	O
will	O
lead	O
to	O
divergence	O
if	O
continued	O
on	O
the	O
other	O
hand	O
if	O
the	O
diagonal	O
elements	O
of	O
a	O
are	O
all	O
positive	O
then	O
can	O
be	O
chosen	O
smaller	O
than	O
one	O
over	O
the	O
largest	O
of	O
them	O
such	O
that	O
i	O
a	O
is	O
diagonal	O
with	O
all	O
diagonal	O
elements	O
between	O
and	O
in	O
this	O
case	O
the	O
first	O
term	O
of	O
the	O
update	O
tends	O
to	O
shrink	O
wt	O
and	B
stability	I
is	O
assured	O
in	O
general	O
case	O
wt	O
will	O
be	O
reduced	O
toward	O
zero	O
whenever	O
a	O
is	O
positive	O
definite	O
meaning	O
for	O
real	O
vector	B
y	O
positive	O
definiteness	O
also	O
ensures	O
that	O
the	O
inverse	O
a	O
exists	O
for	O
linear	O
in	O
the	O
continuing	O
case	O
with	O
the	O
a	O
matrix	O
can	O
be	O
written	O
pr	O
a	O
px	O
where	O
is	O
the	O
stationary	O
distribution	O
under	O
is	O
the	O
probability	O
of	O
linear	O
methods	O
transition	O
from	O
s	O
to	O
under	O
policy	B
p	O
is	O
the	O
matrix	O
of	O
these	O
probabilities	O
d	O
is	O
the	O
diagonal	O
matrix	O
with	O
the	O
on	O
its	O
diagonal	O
and	O
x	O
is	O
the	O
d	O
matrix	O
with	O
xs	O
as	O
its	O
rows	O
from	O
here	O
it	O
is	O
clear	O
that	O
the	O
inner	O
matrix	O
di	O
p	O
is	O
key	O
to	O
determining	O
the	O
positive	O
definiteness	O
of	O
a	O
for	O
a	O
key	O
matrix	O
of	O
this	O
type	O
positive	O
definiteness	O
is	O
assured	O
if	O
all	O
of	O
its	O
columns	O
sum	O
to	O
a	O
nonnegative	O
number	O
this	O
was	O
shown	O
by	O
sutton	O
p	O
based	O
on	O
two	O
previously	O
established	O
theorems	O
one	O
theorem	B
says	O
that	O
any	O
matrix	O
m	O
is	O
positive	O
definite	O
if	O
and	O
only	O
if	O
the	O
symmetric	O
matrix	O
s	O
is	O
positive	O
definite	O
appendix	O
the	O
second	O
theorem	B
says	O
that	O
any	O
symmetric	O
real	O
matrix	O
s	O
is	O
positive	O
definite	O
if	O
all	O
of	O
its	O
diagonal	O
entries	O
are	O
positive	O
and	O
greater	O
than	O
the	O
sum	O
of	O
the	O
absolute	O
values	O
of	O
the	O
corresponding	O
off-diagonal	O
entries	O
p	O
for	O
our	O
key	O
matrix	O
di	O
p	O
the	O
diagonal	O
entries	O
are	O
positive	O
and	O
the	O
off-diagonal	O
entries	O
are	O
negative	O
so	O
all	O
we	O
have	O
to	O
show	O
is	O
that	O
each	O
row	O
sum	O
plus	O
the	O
corresponding	O
column	O
sum	O
is	O
positive	O
the	O
row	O
sums	O
are	O
all	O
positive	O
because	O
p	O
is	O
a	O
stochastic	O
matrix	O
and	O
thus	O
it	O
only	O
remains	O
to	O
show	O
that	O
the	O
column	O
sums	O
are	O
nonnegative	O
note	O
that	O
the	O
row	O
vector	B
of	O
the	O
column	O
sums	O
of	O
any	O
matrix	O
m	O
can	O
be	O
written	O
as	O
where	O
is	O
the	O
column	O
vector	B
with	O
all	O
components	O
equal	O
to	O
let	O
denote	O
the	O
of	O
the	O
where	O
by	O
virtue	O
of	O
being	O
the	O
stationary	O
distribution	O
the	O
column	O
sums	O
of	O
our	O
key	O
matrix	O
then	O
are	O
p	O
p	O
is	O
the	O
stationary	O
distribution	O
all	O
components	O
of	O
which	O
are	O
positive	O
thus	O
the	O
key	O
matrix	O
and	O
its	O
a	O
matrix	O
are	O
positive	O
definite	O
and	O
on-policy	O
is	O
stable	O
conditions	O
and	O
a	O
schedule	O
for	O
reducing	O
over	O
time	O
are	O
needed	O
to	O
prove	O
convergence	O
with	O
probability	O
one	O
at	O
the	O
td	B
fixed	O
point	O
it	O
has	O
also	O
been	O
proven	O
the	O
continuing	O
case	O
that	O
the	O
ve	O
is	O
within	O
a	O
bounded	O
expansion	O
of	O
the	O
lowest	O
possible	O
error	O
vewtd	O
min	O
w	O
vew	O
that	O
is	O
the	O
asymptotic	O
error	O
of	O
the	O
td	B
method	O
is	O
no	O
more	O
than	O
times	O
the	O
smallest	O
possible	O
error	O
that	O
attained	O
in	O
the	O
limit	O
by	O
the	O
monte	B
carlo	I
method	O
because	O
is	O
often	O
near	O
one	O
this	O
expansion	O
factor	O
can	O
be	O
quite	O
large	O
so	O
there	O
is	O
substantial	O
potential	O
loss	O
in	O
asymptotic	O
performance	O
with	O
the	O
td	B
method	O
on	O
the	O
other	O
hand	O
recall	O
that	O
the	O
td	B
methods	O
are	O
often	O
of	O
vastly	O
reduced	O
variance	O
compared	O
to	O
monte	B
carlo	I
methods	I
and	O
thus	O
faster	O
as	O
we	O
saw	O
in	O
chapters	O
and	O
which	O
method	O
will	O
be	O
best	O
depends	O
on	O
the	O
nature	O
of	O
the	O
approximation	O
and	O
problem	O
and	O
on	O
how	O
long	O
learning	O
continues	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
a	O
bound	O
analogous	O
to	O
applies	O
to	O
other	O
on-policy	O
bootstrapping	B
methods	O
as	O
well	O
for	O
example	O
linear	O
semi-gradient	O
dp	O
with	O
ut	O
with	O
updates	O
according	O
to	O
the	O
on-policy	B
distribution	I
will	O
also	O
converge	O
to	O
the	O
td	B
fixed	O
point	O
one-step	O
semi-gradient	O
action-value	B
methods	I
such	O
as	O
semi-gradient	O
covered	O
in	O
the	O
next	O
chapter	O
converge	O
to	O
an	O
analogous	O
fixed	O
point	O
and	O
an	O
analogous	O
bound	O
for	O
episodic	O
tasks	O
there	O
is	O
a	O
slightly	O
different	O
but	O
related	O
bound	O
bertsekas	O
and	O
tsitsiklis	O
there	O
are	O
also	O
a	O
few	O
technical	O
conditions	O
on	O
the	O
rewards	O
features	O
and	O
decrease	O
in	O
the	O
step-size	B
parameter	I
which	O
we	O
have	O
omitted	O
here	O
the	O
full	O
details	O
can	O
be	O
found	O
in	O
the	O
original	O
paper	O
and	O
van	O
roy	O
rst	O
ar	O
critical	O
to	O
the	O
these	O
convergence	O
results	O
is	O
that	O
states	O
are	O
updated	O
according	O
to	O
the	O
on-policy	B
distribution	I
for	O
other	O
update	O
distributions	O
bootstrapping	B
methods	O
using	O
function	B
approximation	I
may	O
actually	O
diverge	O
to	O
infinity	O
examples	O
of	O
this	O
and	O
a	O
discussion	O
of	O
possible	O
solution	O
methods	O
are	O
given	O
in	O
chapter	O
example	O
bootstrapping	B
on	O
the	O
random	B
walk	I
state	B
aggregation	I
is	O
a	O
special	O
case	O
of	O
linear	O
function	B
approximation	I
so	O
let	O
s	O
return	B
to	O
the	O
random	B
walk	I
to	O
illustrate	O
some	O
of	O
the	O
observations	O
made	O
in	O
this	O
chapter	O
the	O
left	O
panel	O
of	O
figure	O
shows	O
the	O
final	O
value	B
function	I
learned	O
by	O
the	O
semi-gradient	O
algorithm	O
using	O
the	O
same	O
state	B
aggregation	I
as	O
in	O
example	O
we	O
see	O
that	O
the	O
near-asymptotic	O
td	B
approximation	O
is	O
indeed	O
farther	O
from	O
the	O
true	O
values	O
than	O
the	O
monte	B
carlo	I
approximation	O
shown	O
in	O
figure	O
nevertheless	O
td	B
methods	O
retain	O
large	O
potential	O
advantages	O
in	O
learning	O
rate	O
and	O
generalize	O
monte	B
carlo	I
methods	I
as	O
we	O
investigated	O
fully	O
with	O
n-step	B
td	B
methods	O
in	O
chapter	O
the	O
right	O
panel	O
of	O
figure	O
shows	O
results	O
with	O
an	O
n-step	B
semi-gradient	O
td	B
method	O
using	O
state	B
aggregation	I
on	O
the	O
random	B
walk	I
that	O
are	O
strikingly	O
similar	O
to	O
those	O
we	O
obtained	O
earlier	O
with	O
tabular	O
methods	O
and	O
the	O
random	B
walk	I
to	O
obtain	O
such	O
quantitatively	O
similar	O
results	O
we	O
switched	O
the	O
state	B
aggregation	I
to	O
groups	O
of	O
states	O
each	O
the	O
groups	O
were	O
then	O
quantitatively	O
close	O
figure	O
bootstrapping	B
with	O
state	B
aggregation	I
on	O
the	O
random	B
walk	I
task	O
left	O
asymptotic	O
values	O
of	O
semi-gradient	O
td	B
are	O
worse	O
than	O
the	O
asymptotic	O
monte	B
carlo	I
values	O
in	O
figure	O
right	O
performance	O
of	O
n-step	B
methods	I
with	O
state-aggregation	O
are	O
strikingly	O
similar	O
to	O
those	O
with	O
tabular	O
representations	O
figure	O
these	O
data	O
are	O
averages	O
over	O
runs	O
averagerms	O
errorover	O
statesand	O
first	O
true	O
valuev	O
approximate	O
td	B
v	O
feature	B
construction	I
for	O
linear	O
methods	O
to	O
the	O
states	O
of	O
the	O
tabular	O
problem	O
in	O
particular	O
recall	O
that	O
state	B
transitions	O
were	O
up	O
to	O
states	O
to	O
the	O
left	O
or	O
right	O
a	O
typical	O
transition	O
would	O
then	O
be	O
of	O
states	O
to	O
the	O
right	O
or	O
left	O
which	O
is	O
quantitively	O
analogous	O
to	O
the	O
single-state	O
state	B
transitions	O
of	O
the	O
tabular	O
system	O
to	O
complete	O
the	O
match	O
we	O
use	O
here	O
the	O
same	O
performance	O
measure	O
an	O
unweighted	O
average	O
of	O
the	O
rms	O
error	O
over	O
all	O
states	O
and	O
over	O
the	O
first	O
episodes	B
rather	O
than	O
a	O
ve	O
objective	O
as	O
is	O
otherwise	O
more	O
appropriate	O
when	O
using	O
function	B
approximation	I
the	O
semi-gradient	O
n-step	B
td	B
algorithm	O
used	O
in	O
this	O
example	O
is	O
the	O
natural	O
extension	O
of	O
the	O
tabular	O
n-step	B
td	B
algorithm	O
presented	O
in	O
chapter	O
to	O
semi-gradient	O
function	B
approximation	I
pseudocode	O
is	O
given	O
in	O
the	O
box	O
below	O
n-step	B
semi-gradient	O
td	B
for	O
estimating	O
v	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
input	O
a	O
differentiable	O
function	O
v	O
s	O
rd	O
r	O
such	O
that	O
vterminal	O
algorithm	O
parameters	O
step	O
size	O
a	O
positive	O
integer	O
n	O
initialize	O
value-function	O
weights	O
w	O
arbitrarily	O
w	O
all	O
store	O
and	O
access	O
operations	O
and	O
rt	O
can	O
take	O
their	O
index	O
mod	O
n	O
loop	O
for	O
each	O
episode	O
if	O
t	O
t	O
then	O
initialize	O
and	O
store	O
terminal	O
t	O
loop	O
for	O
t	O
until	O
t	O
g	O
t	O
n	O
if	O
i	O
i	O
if	O
n	O
t	O
then	O
g	O
g	O
n	O
vs	O
w	O
w	O
vs	O
vs	O
take	O
an	O
action	B
according	O
to	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
and	O
the	O
next	O
state	B
as	O
if	O
is	O
terminal	O
then	O
t	O
t	O
is	O
the	O
time	O
whose	O
state	B
s	O
estimate	O
is	O
being	O
updated	O
the	O
key	O
equation	O
of	O
this	O
algorithm	O
analogous	O
to	O
is	O
wtn	O
wtn	O
vstwtn	O
vstwtn	O
t	O
t	O
where	O
the	O
n-step	B
return	B
is	O
generalized	O
from	O
to	O
gttn	O
n	O
n	O
vstnwtn	O
t	O
t	O
n	O
exercise	O
show	O
that	O
tabular	O
methods	O
such	O
as	O
presented	O
in	O
part	O
i	O
of	O
this	O
book	O
are	O
a	O
special	O
case	O
of	O
linear	O
function	B
approximation	I
what	O
would	O
the	O
feature	O
vectors	O
be	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
feature	B
construction	I
for	O
linear	O
methods	O
linear	O
methods	O
are	O
interesting	O
because	O
of	O
their	O
convergence	O
guarantees	O
but	O
also	O
because	O
in	O
practice	O
they	O
can	O
be	O
very	O
efficient	O
in	O
terms	O
of	O
both	O
data	O
and	O
computation	O
whether	O
or	O
not	O
this	O
is	O
so	O
depends	O
critically	O
on	O
how	O
the	O
states	O
are	O
represented	O
in	O
terms	O
of	O
features	O
which	O
we	O
investigate	O
in	O
this	O
large	O
section	O
choosing	O
features	O
appropriate	O
to	O
the	O
task	O
is	O
an	O
important	O
way	O
of	O
adding	O
prior	O
domain	O
knowledge	O
to	O
reinforcement	B
learning	I
systems	O
intuitively	O
the	O
features	O
should	O
correspond	O
to	O
the	O
aspects	O
of	O
the	O
state	B
space	O
along	O
which	O
generalization	O
may	O
be	O
appropriate	O
if	O
we	O
are	O
valuing	O
geometric	O
objects	O
for	O
example	O
we	O
might	O
want	O
to	O
have	O
features	O
for	O
each	O
possible	O
shape	O
color	O
size	O
or	O
function	O
if	O
we	O
are	O
valuing	O
states	O
of	O
a	O
mobile	O
robot	O
then	O
we	O
might	O
want	O
to	O
have	O
features	O
for	O
locations	O
degrees	O
of	O
remaining	O
battery	O
power	O
recent	O
sonar	O
readings	O
and	O
so	O
on	O
a	O
limitation	O
of	O
the	O
linear	O
form	O
is	O
that	O
it	O
cannot	O
take	O
into	O
account	O
any	O
interactions	O
between	O
features	O
such	O
as	O
the	O
presence	O
of	O
feature	O
i	O
being	O
good	O
only	O
in	O
the	O
absence	O
of	O
feature	O
j	O
for	O
example	O
in	O
the	O
pole-balancing	O
task	O
high	O
angular	O
velocity	O
can	O
be	O
either	O
good	O
or	O
bad	O
depending	O
on	O
the	O
angle	O
if	O
the	O
angle	O
is	O
high	O
then	O
high	O
angular	O
velocity	O
means	O
an	O
imminent	O
danger	O
of	O
falling	O
a	O
bad	O
state	B
whereas	O
if	O
the	O
angle	O
is	O
low	O
then	O
high	O
angular	O
velocity	O
means	O
the	O
pole	O
is	O
righting	O
itself	O
a	O
good	O
state	B
a	O
linear	O
value	B
function	I
could	O
not	O
represent	O
this	O
if	O
its	O
features	O
coded	O
separately	O
for	O
the	O
angle	O
and	O
the	O
angular	O
velocity	O
it	O
needs	O
instead	O
or	O
in	O
addition	O
features	O
for	O
combinations	O
of	O
these	O
two	O
underlying	O
state	B
dimensions	O
in	O
the	O
following	O
subsections	O
we	O
consider	O
a	O
variety	O
of	O
general	O
ways	O
of	O
doing	O
this	O
polynomials	O
the	O
states	O
of	O
many	O
problems	O
are	O
initially	O
expressed	O
as	O
numbers	O
such	O
as	O
positions	O
and	O
velocities	O
in	O
the	O
pole-balancing	O
task	O
the	O
number	O
of	O
cars	O
in	O
each	O
lot	O
in	O
the	O
jack	O
s	O
car	O
rental	O
problem	O
or	O
the	O
gambler	O
s	O
capital	O
in	O
the	O
gambler	O
problem	O
in	O
these	O
types	O
of	O
problems	O
function	B
approximation	I
for	O
reinforcement	B
learning	I
has	O
much	O
in	O
common	O
with	O
the	O
familiar	O
tasks	O
of	O
interpolation	O
and	O
regression	O
various	O
families	O
of	O
features	O
commonly	O
used	O
for	O
interpolation	O
and	O
regression	O
can	O
also	O
be	O
used	O
in	O
reinforcement	B
learning	I
polynomials	O
make	O
up	O
one	O
of	O
the	O
simplest	O
families	O
of	O
features	O
used	O
for	O
interpolation	O
and	O
regression	O
while	O
the	O
basic	O
polynomial	O
features	O
we	O
discuss	O
here	O
do	O
not	O
work	O
as	O
well	O
as	O
other	O
types	O
of	O
features	O
in	O
reinforcement	B
learning	I
they	O
serve	O
as	O
a	O
good	O
introduction	O
because	O
they	O
are	O
simple	O
and	O
familiar	O
as	O
an	O
example	O
suppose	O
a	O
reinforcement	B
learning	I
problem	O
has	O
states	O
with	O
two	O
numerical	O
dimensions	O
for	O
a	O
single	O
representative	O
state	B
s	O
let	O
its	O
two	O
numbers	O
be	O
r	O
and	O
r	O
you	O
might	O
choose	O
to	O
represent	O
s	O
simply	O
by	O
its	O
two	O
state	B
dimensions	O
so	O
that	O
xs	O
but	O
then	O
you	O
would	O
not	O
be	O
able	O
to	O
take	O
into	O
account	O
any	O
interactions	O
between	O
these	O
dimensions	O
in	O
addition	O
if	O
both	O
and	O
were	O
zero	O
then	O
the	O
approximate	O
value	B
would	O
have	O
to	O
also	O
be	O
zero	O
both	O
limitations	O
can	O
be	O
overcome	O
by	O
instead	O
representing	O
s	O
by	O
the	O
four-dimensional	O
feature	O
vector	B
xs	O
the	O
initial	O
feature	O
allows	O
the	O
representation	O
of	O
affine	O
functions	O
in	O
the	O
original	O
state	B
numbers	O
and	O
the	O
final	O
product	O
feature	O
enables	O
interactions	O
to	O
be	O
taken	O
into	O
account	O
or	O
you	O
might	O
choose	O
to	O
use	O
higher-dimensional	O
feature	O
vectors	O
like	O
xs	O
feature	B
construction	I
for	O
linear	O
methods	O
to	O
take	O
more	O
complex	O
interactions	O
into	O
account	O
such	O
feature	O
vectors	O
enable	O
approximations	O
as	O
arbitrary	O
quadratic	O
functions	O
of	O
the	O
state	B
numbers	O
even	O
though	O
the	O
approximation	O
is	O
still	O
linear	O
in	O
the	O
weights	O
that	O
have	O
to	O
be	O
learned	O
generalizing	O
this	O
example	O
from	O
two	O
to	O
k	O
numbers	O
we	O
can	O
represent	O
highlycomplex	O
interactions	O
among	O
a	O
problem	O
s	O
state	B
dimensions	O
suppose	O
each	O
state	B
s	O
corresponds	O
to	O
k	O
numbers	O
sk	O
with	O
each	O
si	O
r	O
for	O
this	O
k-dimensional	O
state	B
space	O
each	O
order-n	O
polynomial-basis	O
feature	O
xi	O
can	O
be	O
written	O
as	O
xis	O
k	O
j	O
where	O
each	O
cij	O
is	O
an	O
integer	O
in	O
the	O
set	O
n	O
for	O
an	O
integer	O
n	O
these	O
features	O
make	O
up	O
the	O
order-n	O
polynomial	B
basis	I
for	O
dimension	O
k	O
which	O
contains	O
different	O
features	O
higher-order	O
polynomial	O
bases	O
allow	O
for	O
more	O
accurate	O
approximations	O
of	O
more	O
complicated	O
functions	O
but	O
because	O
the	O
number	O
of	O
features	O
in	O
an	O
order-n	O
polynomial	B
basis	I
grows	O
exponentially	O
with	O
the	O
dimension	O
k	O
of	O
the	O
natural	O
state	B
space	O
n	O
it	O
is	O
generally	O
necessary	O
to	O
select	O
a	O
subset	O
of	O
them	O
for	O
function	B
approximation	I
this	O
can	O
be	O
done	O
using	O
prior	O
beliefs	O
about	O
the	O
nature	O
of	O
the	O
function	O
to	O
be	O
approximated	O
and	O
some	O
automated	O
selection	O
methods	O
developed	O
for	O
polynomial	O
regression	O
can	O
be	O
adapted	O
to	O
deal	O
with	O
the	O
incremental	O
and	O
nonstationary	O
nature	O
of	O
reinforcement	B
learning	I
exercise	O
why	O
does	O
define	O
distinct	O
features	O
for	O
dimension	O
k	O
exercise	O
what	O
n	O
and	O
cij	O
produce	O
the	O
feature	O
vectors	O
xs	O
fourier	B
basis	I
another	O
linear	O
function	B
approximation	I
method	O
is	O
based	O
on	O
the	O
time-honored	O
fourier	O
series	O
which	O
expresses	O
periodic	O
functions	O
as	O
weighted	O
sums	O
of	O
sine	O
and	O
cosine	O
basis	O
functions	O
of	O
different	O
frequencies	O
function	O
f	O
is	O
periodic	O
if	O
f	O
f	O
for	O
all	O
x	O
and	O
some	O
period	O
the	O
fourier	O
series	O
and	O
the	O
more	O
general	O
fourier	O
transform	O
are	O
widely	O
used	O
in	O
applied	O
sciences	O
in	O
part	O
because	O
if	O
a	O
function	O
to	O
be	O
approximated	O
is	O
known	O
then	O
the	O
basis	O
function	O
weights	O
are	O
given	O
by	O
simple	O
formulae	O
and	O
further	O
with	O
enough	O
basis	O
functions	O
essentially	O
any	O
function	O
can	O
be	O
approximated	O
as	O
accurately	O
as	O
desired	O
in	O
reinforcement	B
learning	I
where	O
the	O
functions	O
to	O
be	O
approximated	O
are	O
unknown	O
fourier	B
basis	I
functions	O
are	O
of	O
interest	O
because	O
they	O
are	O
easy	O
to	O
use	O
and	O
can	O
perform	O
well	O
in	O
a	O
range	O
of	O
reinforcement	B
learning	I
problems	O
first	O
consider	O
the	O
one-dimensional	O
case	O
the	O
usual	O
fourier	O
series	O
representation	O
of	O
a	O
function	O
of	O
one	O
dimension	O
having	O
period	O
represents	O
the	O
function	O
as	O
a	O
linear	O
combination	O
of	O
sine	O
and	O
cosine	O
functions	O
that	O
are	O
each	O
periodic	O
with	O
periods	O
that	O
evenly	O
divide	O
other	O
words	O
whose	O
frequencies	O
are	O
integer	O
multiples	O
of	O
a	O
fundamental	O
frequency	O
but	O
if	O
you	O
are	O
interested	O
in	O
approximating	O
an	O
aperiodic	O
function	O
defined	O
over	O
a	O
bounded	O
interval	O
then	O
you	O
can	O
use	O
these	O
fourier	B
basis	I
featues	O
with	O
set	O
to	O
the	O
length	O
the	O
interval	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
the	O
function	O
of	O
interest	O
is	O
then	O
just	O
one	O
period	O
of	O
the	O
periodic	O
linear	O
combination	O
of	O
the	O
sine	O
and	O
cosine	O
features	O
furthermore	O
if	O
you	O
set	O
to	O
twice	O
the	O
length	O
of	O
the	O
interval	O
of	O
interest	O
and	O
restrict	O
attention	O
to	O
the	O
approximation	O
over	O
the	O
half	O
interval	O
then	O
you	O
can	O
use	O
just	O
the	O
cosine	O
features	O
this	O
is	O
possible	O
because	O
you	O
can	O
represent	O
any	O
even	O
function	O
that	O
is	O
any	O
function	O
that	O
is	O
symmetric	O
about	O
the	O
origin	O
with	O
just	O
the	O
cosine	O
basis	O
so	O
any	O
function	O
over	O
the	O
half-period	O
can	O
be	O
approximated	O
as	O
closely	O
as	O
desired	O
with	O
enough	O
cosine	O
features	O
any	O
function	O
is	O
not	O
exactly	O
correct	O
because	O
the	O
function	O
has	O
to	O
be	O
mathematically	O
well-behaved	O
but	O
we	O
skip	O
this	O
technicality	O
here	O
alternatively	O
it	O
is	O
possible	O
to	O
use	O
just	O
sine	O
features	O
linear	O
combinations	O
of	O
which	O
are	O
always	O
odd	O
functions	O
that	O
is	O
functions	O
that	O
are	O
anti-symmetric	O
about	O
the	O
origin	O
but	O
it	O
is	O
generally	O
better	O
to	O
keep	O
just	O
the	O
cosine	O
features	O
because	O
half-even	O
functions	O
tend	O
to	O
be	O
easier	O
to	O
approximate	O
than	O
half-odd	O
functions	O
because	O
the	O
latter	O
are	O
often	O
discontinuous	O
at	O
the	O
origin	O
of	O
course	O
this	O
does	O
not	O
rule	O
out	O
using	O
both	O
sine	O
and	O
cosine	O
features	O
to	O
approximate	O
over	O
the	O
interval	O
which	O
might	O
be	O
advantageous	O
in	O
some	O
circumstances	O
following	O
this	O
logic	O
and	O
letting	O
so	O
that	O
the	O
features	O
are	O
defined	O
over	O
the	O
half	O
interval	O
the	O
one-dimensional	O
order-n	O
fourier	O
cosine	O
basis	O
consists	O
of	O
the	O
n	O
features	O
xis	O
cosi	O
s	O
s	O
for	O
i	O
n	O
figure	O
shows	O
one-dimensional	O
fourier	O
cosine	O
features	O
xi	O
for	O
i	O
is	O
a	O
constant	O
function	O
figure	O
one-dimensional	O
fourier	O
cosine-basis	O
features	O
xi	O
i	O
for	O
approximating	O
functions	O
over	O
the	O
interval	O
after	O
konidaris	O
et	O
al	O
this	O
same	O
reasoning	O
applies	O
to	O
the	O
fourier	O
cosine	O
series	O
approximation	O
in	O
the	O
multi	O
dimensional	O
case	O
as	O
described	O
in	O
the	O
box	O
below	O
suppose	O
each	O
state	B
s	O
corresponds	O
to	O
a	O
vector	B
of	O
k	O
numbers	O
s	O
with	O
each	O
si	O
the	O
ith	O
feature	O
in	O
the	O
order-n	O
fourier	O
cosine	O
basis	O
can	O
then	O
be	O
written	O
xis	O
ci	O
where	O
ci	O
j	O
n	O
for	O
j	O
k	O
and	O
i	O
this	O
defines	O
a	O
feature	O
for	O
each	O
of	O
the	O
possible	O
integer	O
vectors	O
ci	O
with	O
ci	O
fourier	B
basis	I
function	O
fourier	B
basis	I
function	O
fourier	B
basis	I
function	O
fourier	B
basis	I
function	O
feature	B
construction	I
for	O
linear	O
methods	O
the	O
inner	O
product	O
has	O
the	O
effect	O
of	O
assigning	O
an	O
integer	O
in	O
n	O
to	O
each	O
dimension	O
of	O
s	O
as	O
in	O
the	O
one-dimensional	O
case	O
this	O
integer	O
determines	O
the	O
feature	O
s	O
frequency	O
along	O
that	O
dimension	O
the	O
features	O
can	O
of	O
course	O
be	O
shifted	O
and	O
scaled	O
to	O
suit	O
the	O
bounded	O
state	B
space	O
of	O
a	O
particular	O
application	O
as	O
an	O
example	O
consider	O
the	O
k	O
case	O
in	O
which	O
s	O
where	O
each	O
ci	O
figure	O
shows	O
a	O
selection	O
of	O
six	O
fourier	O
cosine	O
features	O
each	O
labeled	O
by	O
the	O
vector	B
ci	O
that	O
defines	O
it	O
is	O
the	O
horizontal	O
axis	O
and	O
ci	O
is	O
shown	O
as	O
a	O
row	O
vector	B
with	O
the	O
index	O
i	O
omitted	O
any	O
zero	O
in	O
c	O
means	O
the	O
feature	O
is	O
constant	O
along	O
that	O
state	B
dimension	O
so	O
if	O
c	O
the	O
feature	O
is	O
constant	O
over	O
both	O
dimensions	O
if	O
c	O
the	O
feature	O
is	O
constant	O
over	O
the	O
second	O
dimension	O
and	O
varies	O
over	O
the	O
first	O
with	O
frequency	O
depending	O
on	O
and	O
similarly	O
for	O
c	O
when	O
c	O
with	O
neither	O
cj	O
the	O
feature	O
varies	O
along	O
both	O
dimensions	O
and	O
represents	O
an	O
interaction	O
between	O
the	O
two	O
state	B
variables	O
the	O
values	O
of	O
and	O
determine	O
the	O
frequency	O
along	O
each	O
dimension	O
and	O
their	O
ratio	B
gives	O
the	O
direction	O
of	O
the	O
interaction	O
ci	O
figure	O
a	O
selection	O
of	O
six	O
two-dimensional	O
fourier	O
cosine	O
features	O
each	O
labeled	O
by	O
the	O
vector	B
ci	O
that	O
defines	O
it	O
is	O
the	O
horizontal	O
axis	O
and	O
ci	O
is	O
shown	O
with	O
the	O
index	O
i	O
omitted	O
after	O
konidaris	O
et	O
al	O
when	O
using	O
fourier	O
cosine	O
features	O
with	O
a	O
learning	O
algorithm	O
such	O
as	O
semigradient	O
or	O
semi-gradient	O
sarsa	B
it	O
may	O
be	O
helpful	O
to	O
use	O
a	O
different	O
step-size	B
parameter	I
for	O
each	O
feature	O
if	O
is	O
the	O
basic	O
step-size	B
parameter	I
then	O
konidaris	O
osentoski	O
and	O
thomas	O
suggest	O
setting	O
the	O
step-size	B
parameter	I
for	O
feature	O
xi	O
to	O
when	O
each	O
ci	O
j	O
in	O
which	O
case	O
i	O
i	O
fourier	O
cosine	O
features	O
with	O
sarsa	B
can	O
produce	O
good	O
performance	O
compared	O
to	O
several	O
c	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
other	O
collections	O
of	O
basis	O
functions	O
including	O
polynomial	O
and	O
radial	O
basis	O
functions	O
not	O
surprisingly	O
however	O
fourier	O
features	O
have	O
trouble	O
with	O
discontinuities	O
because	O
it	O
is	O
difficult	O
to	O
avoid	O
ringing	O
around	O
points	O
of	O
discontinuity	O
unless	O
very	O
high	O
frequency	O
basis	O
functions	O
are	O
included	O
the	O
number	O
of	O
features	O
in	O
the	O
order-n	O
fourier	B
basis	I
grows	O
exponentially	O
with	O
the	O
dimension	O
of	O
the	O
state	B
space	O
but	O
if	O
that	O
dimension	O
is	O
small	O
enough	O
k	O
then	O
one	O
can	O
select	O
n	O
so	O
that	O
all	O
of	O
the	O
order-n	O
fourier	O
features	O
can	O
be	O
used	O
this	O
makes	O
the	O
selection	O
of	O
features	O
more-or-less	O
automatic	O
for	O
high	O
dimension	O
state	B
spaces	O
however	O
it	O
is	O
necessary	O
to	O
select	O
a	O
subset	O
of	O
these	O
features	O
this	O
can	O
be	O
done	O
using	O
prior	O
beliefs	O
about	O
the	O
nature	O
of	O
the	O
function	O
to	O
be	O
approximated	O
and	O
some	O
automated	O
selection	O
methods	O
can	O
be	O
adapted	O
to	O
deal	O
with	O
the	O
incremental	O
and	O
nonstationary	O
nature	O
of	O
reinforcement	B
learning	I
an	O
advantage	O
of	O
fourier	B
basis	I
features	O
in	O
this	O
regard	O
is	O
that	O
it	O
is	O
easy	O
to	O
select	O
features	O
by	O
setting	O
the	O
ci	O
vectors	O
to	O
account	O
for	O
suspected	O
interactions	O
among	O
the	O
state	B
variables	O
and	O
by	O
limiting	O
the	O
values	O
in	O
the	O
cj	O
vectors	O
so	O
that	O
the	O
approximation	O
can	O
filter	O
out	O
high	O
frequency	O
components	O
considered	O
to	O
be	O
noise	O
on	O
the	O
other	O
hand	O
because	O
fourier	O
features	O
are	O
non-zero	O
over	O
the	O
entire	O
state	B
space	O
the	O
few	O
zeros	O
excepted	O
they	O
represent	O
global	O
properties	O
of	O
states	O
which	O
can	O
make	O
it	O
difficult	O
to	O
find	O
good	O
ways	O
to	O
represent	O
local	O
properties	O
figure	O
shows	O
learning	O
curves	O
comparing	O
the	O
fourier	B
and	I
polynomial	I
bases	I
on	O
the	O
random	B
walk	I
example	O
in	O
general	O
we	O
do	O
not	O
recommend	O
using	O
polynomials	O
for	O
online	B
figure	O
fourier	B
basis	I
vs	O
polynomials	O
on	O
the	O
random	B
walk	I
shown	O
are	O
learning	O
curves	O
for	O
the	O
gradient	B
monte	B
carlo	I
method	O
with	O
fourier	B
and	I
polynomial	I
bases	I
of	O
order	O
and	O
the	O
step-size	O
parameters	O
were	O
roughly	O
optimized	O
for	O
each	O
case	O
for	O
the	O
polynomial	B
basis	I
and	O
for	O
the	O
fourier	B
basis	I
the	O
performance	O
measure	O
is	O
the	O
root	O
mean	O
squared	O
value	B
error	I
are	O
families	O
of	O
polynomials	O
more	O
complicated	O
than	O
those	O
we	O
have	O
discussed	O
for	O
example	O
different	O
families	O
of	O
orthogonal	O
polynomials	O
and	O
these	O
might	O
work	O
better	O
but	O
at	O
present	O
there	O
is	O
little	O
experience	O
with	O
them	O
in	O
reinforcement	B
learning	I
basisfourier	O
basispve	O
averagedover	O
runs	O
feature	B
construction	I
for	O
linear	O
methods	O
coarse	B
coding	I
consider	O
a	O
task	O
in	O
which	O
the	O
natural	O
representation	O
of	O
the	O
state	B
set	O
is	O
a	O
continuous	O
twodimensional	O
space	O
one	O
kind	O
of	O
representation	O
for	O
this	O
case	O
is	O
made	O
up	O
of	O
features	O
corresponding	O
to	O
circles	O
in	O
state	B
space	O
as	O
shown	O
to	O
the	O
right	O
if	O
the	O
state	B
is	O
inside	O
a	O
circle	O
then	O
the	O
corresponding	O
feature	O
has	O
the	O
value	B
and	O
is	O
said	O
to	O
be	O
present	O
otherwise	O
the	O
feature	O
is	O
and	O
is	O
said	O
to	O
be	O
absent	O
this	O
kind	O
of	O
feature	O
is	O
called	O
a	O
binary	O
feature	O
given	O
a	O
state	B
which	O
binary	B
features	I
are	O
present	O
indicate	O
within	O
which	O
circles	O
the	O
state	B
lies	O
and	O
thus	O
coarsely	O
code	O
for	O
its	O
location	O
representing	O
a	O
state	B
with	O
features	O
that	O
overlap	O
in	O
this	O
way	O
they	O
need	O
not	O
be	O
circles	O
or	O
binary	O
is	O
known	O
as	O
coarse	B
coding	I
figure	O
coarse	B
coding	I
generalization	O
from	O
state	B
s	O
to	O
state	B
depends	O
on	O
the	O
number	O
of	O
their	O
features	O
whose	O
receptive	O
fields	O
this	O
case	O
circles	O
overlap	O
these	O
states	O
have	O
one	O
feature	O
in	O
common	O
so	O
there	O
will	O
be	O
slight	O
generalization	O
between	O
them	O
assuming	O
linear	O
gradient-descent	O
function	B
approximation	I
consider	O
the	O
effect	O
of	O
the	O
size	O
and	O
density	O
of	O
the	O
circles	O
corresponding	O
to	O
each	O
circle	O
is	O
a	O
single	O
weight	O
component	O
of	O
w	O
that	O
is	O
affected	O
by	O
learning	O
if	O
we	O
train	O
at	O
one	O
state	B
a	O
point	O
in	O
the	O
space	O
then	O
the	O
weights	O
of	O
all	O
circles	O
intersecting	O
that	O
state	B
will	O
be	O
affected	O
thus	O
by	O
the	O
approximate	O
value	B
function	I
will	O
be	O
affected	O
at	O
all	O
states	O
within	O
the	O
union	O
of	O
the	O
circles	O
with	O
a	O
greater	O
effect	O
the	O
more	O
circles	O
a	O
point	O
has	O
in	O
common	O
with	O
the	O
state	B
as	O
shown	O
in	O
figure	O
if	O
the	O
circles	O
are	O
small	O
then	O
the	O
generalization	O
will	O
be	O
over	O
a	O
short	O
distance	O
as	O
in	O
figure	O
whereas	O
if	O
they	O
are	O
large	O
it	O
will	O
be	O
over	O
a	O
large	O
distance	O
as	O
in	O
figure	O
figure	O
generalization	O
in	O
linear	O
function	B
approximation	I
methods	O
is	O
determined	O
by	O
the	O
sizes	O
and	O
shapes	O
of	O
the	O
features	O
receptive	O
fields	O
all	O
three	O
of	O
these	O
cases	O
have	O
roughly	O
the	O
same	O
number	O
and	O
density	O
of	O
features	O
narrow	O
generalizationb	O
broad	O
generalizationc	O
asymmetric	O
generalization	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
dle	O
moreover	O
the	O
shape	O
of	O
the	O
features	O
will	O
determine	O
the	O
nature	O
of	O
the	O
generalization	O
for	O
example	O
if	O
they	O
are	O
not	O
strictly	O
circular	O
but	O
are	O
elongated	O
in	O
one	O
direction	O
then	O
generalization	O
will	O
be	O
similarly	O
affected	O
as	O
in	O
figure	O
features	O
with	O
large	O
receptive	O
fields	O
give	O
broad	O
generalization	O
but	O
might	O
also	O
seem	O
to	O
limit	O
the	O
learned	O
function	O
to	O
a	O
coarse	O
approximation	O
unable	O
to	O
make	O
discriminations	O
much	O
finer	O
than	O
the	O
width	O
of	O
the	O
receptive	O
fields	O
happily	O
this	O
is	O
not	O
the	O
case	O
initial	O
generalization	O
from	O
one	O
point	O
to	O
another	O
is	O
indeed	O
controlled	O
by	O
the	O
size	O
and	O
shape	O
of	O
the	O
receptive	O
fields	O
but	O
acuity	O
the	O
finest	O
discrimination	O
ultimately	O
possible	O
is	O
controlled	O
more	O
by	O
the	O
total	O
number	O
of	O
features	O
example	O
coarseness	O
of	O
coarse	B
coding	I
this	O
example	O
illustrates	O
the	O
effect	O
on	O
learning	O
of	O
the	O
size	O
of	O
the	O
receptive	O
fields	O
in	O
coarse	B
coding	I
linear	O
function	B
approximation	I
based	O
on	O
coarse	B
coding	I
and	O
was	O
used	O
to	O
learn	O
a	O
one-dimensional	O
square-wave	O
function	O
at	O
the	O
top	O
of	O
figure	O
the	O
values	O
of	O
this	O
function	O
were	O
used	O
as	O
the	O
targets	O
ut	O
with	O
just	O
one	O
dimension	O
the	O
receptive	O
fields	O
were	O
intervals	O
rather	O
than	O
circles	O
learning	O
was	O
repeated	O
with	O
three	O
different	O
sizes	O
of	O
the	O
intervals	O
narrow	O
medium	O
and	O
broad	O
as	O
shown	O
at	O
the	O
bottom	O
of	O
the	O
figure	O
all	O
three	O
cases	O
had	O
the	O
same	O
density	O
of	O
features	O
about	O
over	O
the	O
extent	O
of	O
the	O
function	O
being	O
learned	O
training	O
examples	O
were	O
generated	O
uniformly	O
at	O
random	O
over	O
this	O
extent	O
the	O
step-size	B
parameter	I
was	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
features	O
that	O
were	O
present	O
at	O
one	O
time	O
figure	O
shows	O
the	O
functions	O
learned	O
in	O
all	O
three	O
cases	O
over	O
the	O
course	O
of	O
learning	O
note	O
that	O
the	O
width	O
of	O
the	O
features	O
had	O
a	O
strong	O
effect	O
early	O
in	O
learning	O
with	O
broad	O
features	O
the	O
generalization	O
tended	O
to	O
be	O
broad	O
with	O
narrow	O
features	O
only	O
the	O
close	O
neighbors	O
of	O
each	O
trained	O
point	O
were	O
changed	O
causing	O
the	O
function	O
learned	O
to	O
be	O
more	O
bumpy	O
however	O
the	O
final	O
function	O
learned	O
was	O
affected	O
only	O
slightly	O
by	O
the	O
width	O
of	O
the	O
features	O
receptive	O
field	O
shape	O
figure	O
example	O
of	O
feature	O
width	O
s	O
strong	O
effect	O
on	O
initial	O
generalization	O
row	O
and	O
weak	O
effect	O
on	O
asymptotic	O
accuracy	O
row	O
feature	B
construction	I
for	O
linear	O
methods	O
tends	O
to	O
have	O
a	O
strong	O
effect	O
on	O
generalization	O
but	O
little	O
effect	O
on	O
asymptotic	O
solution	O
quality	O
tile	B
coding	I
tile	B
coding	I
is	O
a	O
form	O
of	O
coarse	B
coding	I
for	O
multi-dimensional	O
continuous	O
spaces	O
that	O
is	O
flexible	O
and	O
computationally	O
efficient	O
it	O
may	O
be	O
the	O
most	O
practical	O
feature	O
representation	O
for	O
modern	O
sequential	O
digital	O
computers	O
open-source	O
software	O
is	O
available	O
for	O
many	O
kinds	O
of	O
tile	B
coding	I
in	O
tile	B
coding	I
the	O
receptive	O
fields	O
of	O
the	O
features	O
are	O
grouped	O
into	O
partitions	O
of	O
the	O
state	B
space	O
each	O
such	O
partition	O
is	O
called	O
a	O
tiling	O
and	O
each	O
element	O
of	O
the	O
partition	O
is	O
called	O
a	O
tile	O
for	O
example	O
the	O
simplest	O
tiling	O
of	O
a	O
two-dimensional	O
state	B
space	O
is	O
a	O
uniform	O
grid	O
such	O
as	O
that	O
shown	O
on	O
the	O
left	O
side	O
of	O
figure	O
the	O
tiles	O
or	O
receptive	O
field	O
here	O
are	O
squares	O
rather	O
than	O
the	O
circles	O
in	O
figure	O
if	O
just	O
this	O
single	O
tiling	O
were	O
used	O
then	O
the	O
state	B
indicated	O
by	O
the	O
white	O
spot	O
would	O
be	O
represented	O
by	O
the	O
single	O
feature	O
whose	O
tile	O
it	O
falls	O
within	O
generalization	O
would	O
be	O
complete	O
to	O
all	O
states	O
within	O
the	O
same	O
tile	O
and	O
nonexistent	O
to	O
states	O
outside	O
it	O
with	O
just	O
one	O
tiling	O
we	O
would	O
not	O
have	O
coarse	B
coding	I
but	O
just	O
a	O
case	O
of	O
state	B
aggregation	I
figure	O
multiple	O
overlapping	O
grid-tilings	O
on	O
a	O
limited	O
two-dimensional	O
space	O
these	O
tilings	O
are	O
offset	O
from	O
one	O
another	O
by	O
a	O
uniform	O
amount	O
in	O
each	O
dimension	O
to	O
get	O
the	O
strengths	O
of	O
coarse	B
coding	I
requires	O
overlapping	O
receptive	O
fields	O
and	O
by	O
definition	O
the	O
tiles	O
of	O
a	O
partition	O
do	O
not	O
overlap	O
to	O
get	O
true	O
coarse	B
coding	I
with	B
tile	B
coding	I
multiple	O
tilings	O
are	O
used	O
each	O
offset	O
by	O
a	O
fraction	O
of	O
a	O
tile	O
width	O
a	O
simple	O
case	O
with	O
four	O
tilings	O
is	O
shown	O
on	O
the	O
right	O
side	O
of	O
figure	O
every	O
state	B
such	O
as	O
that	O
indicated	O
by	O
the	O
white	O
spot	O
falls	O
in	O
exactly	O
one	O
tile	O
in	O
each	O
of	O
the	O
four	O
tilings	O
these	O
four	O
tiles	O
correspond	O
to	O
four	O
features	O
that	O
become	O
active	O
when	O
the	O
state	B
occurs	O
specifically	O
the	O
feature	O
vector	B
xs	O
has	O
one	O
component	O
for	O
each	O
tile	O
in	O
each	O
tiling	O
in	O
this	O
example	O
there	O
are	O
components	O
all	O
of	O
which	O
will	O
be	O
except	O
for	O
the	O
four	O
corresponding	O
to	O
the	O
tiles	O
that	O
s	O
falls	O
within	O
figure	O
shows	O
the	O
advantage	O
of	O
multiple	O
offset	O
tilings	O
coding	O
over	O
a	O
single	O
tiling	O
on	O
the	O
random	B
walk	I
example	O
point	O
in	O
state	B
spaceto	O
berepresentedtiling	O
state	B
spacefour	O
activetilesfeatures	O
overlap	O
the	O
pointand	O
are	O
used	O
to	O
represent	O
it	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
figure	O
why	O
we	O
use	O
coarse	B
coding	I
shown	O
are	O
learning	O
curves	O
on	O
the	O
random	B
walk	I
example	O
for	O
the	O
gradient	B
monte	B
carlo	I
algorithm	O
with	O
a	O
single	O
tiling	O
and	O
with	O
multiple	O
tilings	O
the	O
space	O
of	O
states	O
was	O
treated	O
as	O
a	O
single	O
continuous	O
dimension	O
covered	O
with	O
tiles	O
each	O
states	O
wide	O
the	O
multiple	O
tilings	O
were	O
offset	O
from	O
each	O
other	O
by	O
states	O
the	O
step-size	B
parameter	I
was	O
set	O
so	O
that	O
the	O
initial	O
learning	O
rate	O
in	O
the	O
two	O
cases	O
was	O
the	O
same	O
for	O
the	O
single	O
tiling	O
and	O
for	O
the	O
tilings	O
an	O
immediate	O
practical	O
advantage	O
of	O
tile	B
coding	I
is	O
that	O
because	O
it	O
works	O
with	O
partitions	O
the	O
overall	O
number	O
of	O
features	O
that	O
are	O
active	O
at	O
one	O
time	O
is	O
the	O
same	O
for	O
any	O
state	B
exactly	O
one	O
feature	O
is	O
present	O
in	O
each	O
tiling	O
so	O
the	O
total	O
number	O
of	O
features	O
present	O
is	O
always	O
the	O
same	O
as	O
the	O
number	O
of	O
tilings	O
this	O
allows	O
the	O
step-size	B
parameter	I
to	O
be	O
set	O
in	O
an	O
easy	O
intuitive	O
way	O
for	O
example	O
choosing	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
tilings	O
results	O
in	O
exact	O
one-trial	O
learning	O
if	O
the	O
example	O
s	O
v	O
is	O
trained	O
on	O
then	O
whatever	O
the	O
prior	O
estimate	O
vswt	O
the	O
new	O
estimate	O
will	O
be	O
v	O
usually	O
one	O
wishes	O
to	O
change	O
more	O
slowly	O
than	O
this	O
to	O
allow	O
for	O
generalization	O
and	O
stochastic	O
variation	O
in	O
target	O
outputs	O
for	O
example	O
one	O
might	O
choose	O
in	O
which	O
case	O
the	O
estimate	O
for	O
the	O
trained	O
state	B
would	O
move	O
one-tenth	O
of	O
the	O
way	O
to	O
the	O
target	O
in	O
one	O
update	O
and	O
neighboring	O
states	O
will	O
be	O
moved	O
less	O
proportional	O
to	O
the	O
number	O
of	O
tiles	O
they	O
have	O
in	O
common	O
tile	B
coding	I
also	O
gains	O
computational	O
advantages	O
from	O
its	O
use	O
of	O
binary	O
feature	O
vectors	O
because	O
each	O
component	O
is	O
either	O
or	O
the	O
weighted	O
sum	O
making	O
up	O
the	O
approximate	O
value	B
function	I
is	O
almost	O
trivial	O
to	O
compute	O
rather	O
than	O
performing	O
d	O
multiplications	O
and	O
additions	O
one	O
simply	O
computes	O
the	O
indices	O
of	O
the	O
n	O
d	O
active	O
features	O
and	O
then	O
adds	O
up	O
the	O
n	O
corresponding	O
components	O
of	O
the	O
weight	O
vector	B
generalization	O
occurs	O
to	O
states	O
other	O
than	O
the	O
one	O
trained	O
if	O
those	O
states	O
fall	O
within	O
any	O
of	O
the	O
same	O
tiles	O
proportional	O
to	O
the	O
number	O
of	O
tiles	O
in	O
common	O
even	O
the	O
choice	O
of	O
how	O
to	O
offset	O
the	O
tilings	O
from	O
each	O
other	O
affects	O
generalization	O
if	O
they	O
are	O
offset	O
uniformly	O
in	O
each	O
dimension	O
as	O
they	O
were	O
in	O
figure	O
then	O
different	O
states	O
can	O
generalize	O
in	O
qualitatively	O
different	O
ways	O
as	O
shown	O
in	O
the	O
upper	O
half	O
of	O
figure	O
each	O
of	O
the	O
eight	O
subfigures	O
show	O
the	O
pattern	O
of	O
generalization	O
from	O
a	O
trained	O
state	B
to	O
nearby	O
points	O
in	O
this	O
example	O
there	O
are	O
eight	O
tilings	O
thus	O
subregions	O
within	O
a	O
tile	O
that	O
generalize	O
distinctly	O
but	O
all	O
according	O
to	O
one	O
of	O
these	O
eight	O
patterns	O
note	O
how	O
uniform	O
offsets	O
averagedover	O
aggregationone	O
tilingtile	O
coding	O
tilingspve	O
feature	B
construction	I
for	O
linear	O
methods	O
figure	O
why	O
tile	O
asymmetrical	O
offsets	O
are	O
preferred	O
in	O
tile	B
coding	I
shown	O
is	O
the	O
strength	O
of	O
generalization	O
from	O
a	O
trained	O
state	B
indicated	O
by	O
the	O
small	O
black	O
plus	O
to	O
nearby	O
states	O
for	O
the	O
case	O
of	O
eight	O
tilings	O
if	O
the	O
tilings	O
are	O
uniformly	O
offset	O
then	O
there	O
are	O
diagonal	O
artifacts	O
and	O
substantial	O
variations	O
in	O
the	O
generalization	O
whereas	O
with	O
asymmetrically	O
offset	O
tilings	O
the	O
generalization	O
is	O
more	O
spherical	O
and	O
homogeneous	O
result	O
in	O
a	O
strong	O
effect	O
along	O
the	O
diagonal	O
in	O
many	O
patterns	O
these	O
artifacts	O
can	O
be	O
avoided	O
if	O
the	O
tilings	O
are	O
offset	O
asymmetrically	O
as	O
shown	O
in	O
the	O
lower	O
half	O
of	O
the	O
figure	O
these	O
lower	O
generalization	O
patterns	O
are	O
better	O
because	O
they	O
are	O
all	O
well	O
centered	O
on	O
the	O
trained	O
state	B
with	O
no	O
obvious	O
asymmetries	O
tilings	O
in	O
all	O
cases	O
are	O
offset	O
from	O
each	O
other	O
by	O
a	O
fraction	O
of	O
a	O
tile	O
width	O
in	O
each	O
dimension	O
if	O
w	O
denotes	O
the	O
tile	O
width	O
and	O
n	O
the	O
number	O
of	O
tilings	O
then	O
w	O
n	O
is	O
a	O
fundamental	O
unit	O
within	O
small	O
squares	O
w	O
n	O
on	O
a	O
side	O
all	O
states	O
activate	O
the	O
same	O
tiles	O
have	O
the	O
same	O
feature	O
representation	O
and	O
the	O
same	O
approximated	O
value	B
if	O
a	O
state	B
is	O
moved	O
by	O
w	O
n	O
in	O
any	O
cartesian	O
direction	O
the	O
feature	O
representation	O
changes	O
by	O
one	O
componenttile	O
uniformly	O
offset	O
tilings	O
are	O
offset	O
from	O
each	O
other	O
by	O
exactly	O
this	O
unit	O
distance	O
for	O
a	O
two-dimensional	O
space	O
we	O
say	O
that	O
each	O
tiling	O
is	O
offset	O
by	O
the	O
displacement	O
vector	B
meaning	O
that	O
it	O
is	O
offset	O
from	O
the	O
previous	O
tiling	O
by	O
w	O
n	O
times	O
this	O
vector	B
in	O
these	O
terms	O
possible	O
generalizations	O
for	O
uniformly	O
offset	O
tilingspossible	O
generalizationsfor	O
asymmetrically	O
offset	O
tilings	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
the	O
asymmetrically	O
offset	O
tilings	O
shown	O
in	O
the	O
lower	O
part	O
of	O
figure	O
are	O
offset	O
by	O
a	O
displacement	O
vector	B
of	O
extensive	O
studies	O
have	O
been	O
made	O
of	O
the	O
effect	O
of	O
different	O
displacement	O
vectors	O
on	O
the	O
generalization	O
of	O
tile	B
coding	I
and	O
militzer	O
an	O
an	O
miller	O
and	O
parks	O
miller	O
an	O
glanz	O
and	O
carter	O
assessing	O
their	O
homegeneity	O
and	O
tendency	O
toward	O
diagonal	O
artifacts	O
like	O
those	O
seen	O
for	O
the	O
displacement	O
vectors	O
based	O
on	O
this	O
work	O
miller	O
and	O
glanz	O
recommend	O
using	O
displacement	O
vectors	O
consisting	O
of	O
the	O
first	O
odd	O
integers	O
in	O
particular	O
for	O
a	O
continuous	O
space	O
of	O
dimension	O
k	O
a	O
good	O
choice	O
is	O
to	O
use	O
the	O
first	O
odd	O
integers	O
with	O
n	O
number	O
of	O
tilings	O
set	O
to	O
an	O
integer	O
power	O
of	O
greater	O
than	O
or	O
equal	O
to	O
this	O
is	O
what	O
we	O
have	O
done	O
to	O
produce	O
the	O
tilings	O
in	O
the	O
lower	O
half	O
of	O
figure	O
in	O
which	O
k	O
n	O
and	O
the	O
displacement	O
vector	B
is	O
in	O
a	O
three-dimensional	O
case	O
the	O
first	O
four	O
tilings	O
would	O
be	O
offset	O
in	O
total	O
from	O
a	O
base	O
position	O
by	O
and	O
opensource	O
software	O
that	O
can	O
efficiently	O
make	O
tilings	O
like	O
this	O
for	O
any	O
k	O
is	O
readily	O
available	O
in	O
choosing	O
a	O
tiling	O
strategy	O
one	O
has	O
to	O
pick	O
the	O
number	O
of	O
the	O
tilings	O
and	O
the	O
shape	O
of	O
the	O
tiles	O
the	O
number	O
of	O
tilings	O
along	O
with	O
the	O
size	O
of	O
the	O
tiles	O
determines	O
the	O
resolution	O
or	O
fineness	O
of	O
the	O
asymptotic	O
approximation	O
as	O
in	O
general	O
coarse	B
coding	I
and	O
illustrated	O
in	O
figure	O
the	O
shape	O
of	O
the	O
tiles	O
will	O
determine	O
the	O
nature	O
of	O
generalization	O
as	O
in	O
figure	O
square	O
tiles	O
will	O
generalize	O
roughly	O
equally	O
in	O
each	O
dimension	O
as	O
indicated	O
in	O
figure	O
tiles	O
that	O
are	O
elongated	O
along	O
one	O
dimension	O
such	O
as	O
the	O
stripe	O
tilings	O
in	O
figure	O
will	O
promote	O
generalization	O
along	O
that	O
dimension	O
the	O
tilings	O
in	O
figure	O
are	O
also	O
denser	O
and	O
thinner	O
on	O
the	O
left	O
promoting	O
discrimination	O
along	O
the	O
horizonal	O
dimension	O
at	O
lower	O
values	O
along	O
that	O
dimension	O
the	O
diagonal	O
stripe	O
tiling	O
in	O
figure	O
will	O
promote	O
generalization	O
along	O
one	O
diagonal	O
in	O
higher	O
dimensions	O
axis-aligned	O
stripes	O
correspond	O
to	O
ignoring	O
some	O
of	O
the	O
dimensions	O
in	O
some	O
of	O
the	O
tilings	O
that	O
is	O
to	O
hyperplanar	O
slices	O
irregular	O
tilings	O
such	O
as	O
shown	O
in	O
figure	O
are	O
also	O
possible	O
though	O
rare	O
in	O
practice	O
and	O
beyond	O
the	O
standard	O
software	O
figure	O
tilings	O
need	O
not	O
be	O
grids	O
they	O
can	O
be	O
arbitrarily	O
shaped	O
and	O
non-uniform	O
while	O
still	O
in	O
many	O
cases	O
being	O
computationally	O
efficient	O
to	O
compute	O
in	O
practice	O
it	O
is	O
often	O
desirable	O
to	O
use	O
different	O
shaped	O
tiles	O
in	O
different	O
tilings	O
for	O
example	O
one	O
might	O
use	O
some	O
vertical	O
stripe	O
tilings	O
and	O
some	O
horizontal	O
stripe	O
tilings	O
this	O
would	O
encourage	O
generalization	O
along	O
either	O
dimension	O
however	O
with	O
stripe	O
tilings	O
alone	O
it	O
is	O
not	O
possible	O
to	O
learn	O
that	O
a	O
particular	O
conjunction	O
of	O
horizontal	O
and	O
vertical	O
a	O
irregularb	O
log	O
stripesc	O
diagonal	O
stripes	O
feature	B
construction	I
for	O
linear	O
methods	O
coordinates	O
has	O
a	O
distinctive	O
value	B
is	O
learned	O
for	O
it	O
will	O
bleed	O
into	O
states	O
with	O
the	O
same	O
horizontal	O
and	O
vertical	O
coordinates	O
for	O
this	O
one	O
needs	O
the	O
conjunctive	O
rectangular	O
tiles	O
such	O
as	O
originally	O
shown	O
in	O
figure	O
with	O
multiple	O
tilings	O
some	O
horizontal	O
some	O
vertical	O
and	O
some	O
conjunctive	O
one	O
can	O
get	O
everything	O
a	O
preference	O
for	O
generalizing	O
along	O
each	O
dimension	O
yet	O
the	O
ability	O
to	O
learn	O
specific	O
values	O
for	O
conjunctions	O
sutton	O
for	O
examples	O
the	O
choice	O
of	O
tilings	O
determines	O
generalization	O
and	O
until	O
this	O
choice	O
can	O
be	O
effectively	O
automated	O
it	O
is	O
important	O
that	O
tile	B
coding	I
enables	O
the	O
choice	O
to	O
be	O
made	O
flexibly	O
and	O
in	O
a	O
way	O
that	O
makes	O
sense	O
to	O
people	O
another	O
useful	O
trick	O
for	O
reducing	O
memory	O
requirements	O
is	O
hashing	O
a	O
consistent	O
pseudorandom	O
collapsing	O
of	O
a	O
large	O
tiling	O
into	O
a	O
much	O
smaller	O
set	O
of	O
tiles	O
hashing	O
produces	O
tiles	O
consisting	O
of	O
noncontiguous	O
disjoint	O
regions	O
randomly	O
spread	O
throughout	O
the	O
state	B
space	O
but	O
that	O
still	O
form	O
an	O
exhaustive	O
partition	O
for	O
example	O
one	O
tile	O
might	O
consist	O
of	O
the	O
four	O
subtiles	O
shown	O
to	O
the	O
right	O
through	O
hashing	O
memory	O
requirements	O
are	O
often	O
reduced	O
by	O
large	O
factors	O
with	O
little	O
loss	O
of	O
performance	O
this	O
is	O
possible	O
because	O
high	O
resolution	O
is	O
needed	O
in	O
only	O
a	O
small	O
fraction	O
of	O
the	O
state	B
space	O
hashing	O
frees	O
us	O
from	O
the	O
curse	B
of	I
dimensionality	I
in	O
the	O
sense	O
that	O
memory	O
requirements	O
need	O
not	O
be	O
exponential	O
in	O
the	O
number	O
of	O
dimensions	O
but	O
need	O
merely	O
match	O
the	O
real	O
demands	O
of	O
the	O
task	O
good	O
open-source	O
implementations	O
of	O
tile	B
coding	I
including	O
hashing	O
are	O
widely	O
available	O
exercise	O
suppose	O
we	O
believe	O
that	O
one	O
of	O
two	O
state	B
dimensions	O
is	O
more	O
likely	O
to	O
have	O
an	O
effect	O
on	O
the	O
value	B
function	I
than	O
is	O
the	O
other	O
that	O
generalization	O
should	O
be	O
primarily	O
across	O
this	O
dimension	O
rather	O
than	O
along	O
it	O
what	O
kind	O
of	O
tilings	O
could	O
be	O
used	O
to	O
take	O
advantage	O
of	O
this	O
prior	B
knowledge	I
radial	O
basis	O
functions	O
radial	O
basis	O
functions	O
are	O
the	O
natural	O
generalization	O
of	O
coarse	B
coding	I
to	O
continuousvalued	O
features	O
rather	O
than	O
each	O
feature	O
being	O
either	O
or	O
it	O
can	O
be	O
anything	O
in	O
the	O
interval	O
reflecting	O
various	O
degrees	O
to	O
which	O
the	O
feature	O
is	O
present	O
a	O
typical	O
rbf	O
feature	O
xi	O
has	O
a	O
gaussian	O
response	O
xis	O
dependent	O
only	O
on	O
the	O
distance	O
between	O
the	O
state	B
s	O
and	O
the	O
feature	O
s	O
prototypical	O
or	O
center	O
state	B
ci	O
and	O
relative	O
to	O
the	O
feature	O
s	O
width	O
i	O
xis	O
i	O
the	O
norm	O
or	O
distance	O
metric	O
of	O
course	O
can	O
be	O
chosen	O
in	O
whatever	O
way	O
seems	O
most	O
appropriate	O
to	O
the	O
states	O
and	O
task	O
at	O
hand	O
figure	O
shows	O
a	O
one-dimensional	O
example	O
with	O
a	O
euclidean	O
distance	O
metric	O
the	O
primary	O
advantage	O
of	O
rbfs	O
over	O
binary	B
features	I
is	O
that	O
they	O
produce	O
approximate	O
functions	O
that	O
vary	O
smoothly	O
and	O
are	O
differentiable	O
although	O
this	O
is	O
appealing	O
in	O
most	O
cases	O
it	O
has	O
no	O
practical	O
significance	O
nevertheless	O
extensive	O
studies	O
have	O
been	O
made	O
of	O
graded	O
response	O
functions	O
such	O
as	O
rbfs	O
in	O
the	O
context	O
of	O
tile	B
coding	I
miller	O
onetile	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
figure	O
one-dimensional	O
radial	O
basis	O
functions	O
et	O
al	O
an	O
et	O
al	O
lane	O
handelman	O
and	O
gelfand	O
all	O
of	O
these	O
methods	O
require	O
substantial	O
additional	O
computational	O
complexity	O
tile	B
coding	I
and	O
often	O
reduce	O
performance	O
when	O
there	O
are	O
more	O
than	O
two	O
state	B
dimensions	O
in	O
high	O
dimensions	O
the	O
edges	O
of	O
tiles	O
are	O
much	O
more	O
important	O
and	O
it	O
has	O
proven	O
difficult	O
to	O
obtain	O
well	O
controlled	O
graded	O
tile	O
activations	O
near	O
the	O
edges	O
an	O
rbf	O
network	O
is	O
a	O
linear	O
function	O
approximator	O
using	O
rbfs	O
for	O
its	O
features	O
learning	O
is	O
defined	O
by	O
equations	O
and	O
exactly	O
as	O
in	O
other	O
linear	O
function	O
approximators	O
in	O
addition	O
some	O
learning	O
methods	O
for	O
rbf	O
networks	O
change	O
the	O
centers	O
and	O
widths	O
of	O
the	O
features	O
as	O
well	O
bringing	O
them	O
into	O
the	O
realm	O
of	O
nonlinear	O
function	O
approximators	O
nonlinear	O
methods	O
may	O
be	O
able	O
to	O
fit	O
target	O
functions	O
much	O
more	O
precisely	O
the	O
downside	O
to	O
rbf	O
networks	O
and	O
to	O
nonlinear	O
rbf	O
networks	O
especially	O
is	O
greater	O
computational	O
complexity	O
and	O
often	O
more	O
manual	O
tuning	O
before	O
learning	O
is	O
robust	O
and	O
efficient	O
selecting	O
step-size	O
parameters	O
manually	O
most	O
sgd	O
methods	O
require	O
the	O
designer	O
to	O
select	O
an	O
appropriate	O
step-size	B
parameter	I
ideally	O
this	O
selection	O
would	O
be	O
automated	O
and	O
in	O
some	O
cases	O
it	O
has	O
been	O
but	O
for	O
most	O
cases	O
it	O
is	O
still	O
common	O
practice	O
to	O
set	O
it	O
manually	O
to	O
do	O
this	O
and	O
to	O
better	O
understand	O
the	O
algorithms	O
it	O
is	O
useful	O
to	O
develop	O
some	O
intuitive	O
sense	O
of	O
the	O
role	O
of	O
the	O
step-size	B
parameter	I
can	O
we	O
say	O
in	O
general	O
how	O
it	O
should	O
be	O
set	O
theoretical	O
considerations	O
are	O
unfortunately	O
of	O
little	O
help	O
the	O
theory	O
of	O
stochastic	O
approximation	O
gives	O
us	O
conditions	O
on	O
a	O
slowly	O
decreasing	O
step-size	O
sequence	O
that	O
are	O
sufficient	O
to	O
guarantee	O
convergence	O
but	O
these	O
tend	O
to	O
result	O
in	O
learning	O
that	O
is	O
too	O
slow	O
the	O
classical	O
choice	O
t	O
which	O
produces	O
sample	O
averages	O
in	O
tabular	O
mc	O
methods	O
is	O
not	O
appropriate	O
for	O
td	B
methods	O
for	O
nonstationary	O
problems	O
or	O
for	O
any	O
method	O
using	O
function	B
approximation	I
for	O
linear	O
methods	O
there	O
are	O
recursive	O
least-squares	O
methods	O
that	O
set	O
an	O
optimal	O
matrix	O
step	O
size	O
and	O
these	O
methods	O
can	O
be	O
extended	O
to	O
temporaldifference	O
learning	O
as	O
in	O
the	O
lstd	O
method	O
described	O
in	O
section	O
but	O
these	O
require	O
step-size	O
parameters	O
or	O
d	O
times	O
more	O
parameters	O
than	O
we	O
are	O
learning	O
for	O
this	O
reason	O
we	O
rule	O
them	O
out	O
for	O
use	O
on	O
large	O
problems	O
where	O
function	B
approximation	I
is	O
most	O
needed	O
to	O
get	O
some	O
intuitive	O
feel	O
for	O
how	O
to	O
set	O
the	O
step-size	B
parameter	I
manually	O
it	O
is	O
best	O
to	O
go	O
back	O
momentarily	O
to	O
the	O
tabular	O
case	O
there	O
we	O
can	O
understand	O
that	O
a	O
step	O
size	O
of	O
will	O
result	O
in	O
a	O
complete	O
elimination	O
of	O
the	O
sample	O
error	O
after	O
one	O
target	O
nonlinear	O
function	B
approximation	I
artificial	B
neural	B
networks	I
with	O
a	O
step	O
size	O
of	O
one	O
as	O
discussed	O
on	O
page	O
we	O
usually	O
want	O
to	O
learn	O
slower	O
than	O
this	O
in	O
the	O
tabular	O
case	O
a	O
step	O
size	O
of	O
would	O
take	O
about	O
experiences	O
to	O
converge	O
approximately	O
to	O
their	O
mean	O
target	O
and	O
if	O
we	O
wanted	O
to	O
learn	O
in	O
experiences	O
we	O
would	O
use	O
then	O
the	O
tabular	O
estimate	O
for	O
a	O
state	B
will	O
approach	O
the	O
mean	O
of	O
its	O
targets	O
with	O
the	O
most	O
recent	O
targets	O
having	O
the	O
greatest	O
effect	O
after	O
about	O
experiences	O
with	O
the	O
state	B
in	O
general	O
if	O
with	O
general	O
function	B
approximation	I
there	O
is	O
not	O
such	O
a	O
clear	O
notion	O
of	O
number	O
of	O
experiences	O
with	O
a	O
state	B
as	O
each	O
state	B
may	O
be	O
similar	O
to	O
and	O
dissimilar	O
from	O
all	O
the	O
others	O
to	O
various	O
degrees	O
however	O
there	O
is	O
a	O
similar	O
rule	O
that	O
gives	O
similar	O
behavior	O
in	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
suppose	O
you	O
wanted	O
to	O
learn	O
in	O
about	O
experiences	O
with	O
substantially	O
the	O
same	O
feature	O
vector	B
a	O
good	O
rule	O
of	O
thumb	O
for	O
setting	O
the	O
step-size	B
parameter	I
of	O
linear	O
sgd	O
methods	O
is	O
then	O
where	O
x	O
is	O
a	O
random	O
feature	O
vector	B
chosen	O
from	O
the	O
same	O
distribution	O
as	O
input	O
vectors	O
will	O
be	O
in	O
the	O
sgd	O
this	O
method	O
works	O
best	O
if	O
the	O
feature	O
vectors	O
do	O
not	O
vary	O
greatly	O
in	O
length	O
ideally	O
is	O
a	O
constant	O
exercise	O
suppose	O
you	O
are	O
using	O
tile	B
coding	I
to	O
transform	O
a	O
seven-dimensional	O
continuous	B
state	B
space	O
into	O
binary	O
feature	O
vectors	O
to	O
estimate	O
a	O
state	B
value	B
function	I
vsw	O
v	O
you	O
believe	O
that	O
the	O
dimensions	O
do	O
not	O
interact	O
strongly	O
so	O
you	O
decide	O
to	O
use	O
eight	O
tilings	O
of	O
each	O
dimension	O
separately	O
tilings	O
for	O
tilings	O
in	O
addition	O
in	O
case	O
there	O
are	O
some	O
pairwise	O
interactions	O
between	O
the	O
dimensions	O
you	O
also	O
take	O
all	O
pairs	O
of	O
dimensions	O
and	O
tile	O
each	O
pair	O
conjunctively	O
with	O
rectangular	O
tiles	O
you	O
make	O
two	O
tilings	O
for	O
each	O
pair	O
of	O
dimensions	O
making	O
a	O
grand	O
total	O
of	O
tilings	O
given	O
these	O
feature	O
vectors	O
you	O
suspect	O
that	O
you	O
still	O
have	O
to	O
average	O
out	O
some	O
noise	O
so	O
you	O
decide	O
that	O
you	O
want	O
learning	O
to	O
be	O
gradual	O
taking	O
about	O
presentations	O
with	O
the	O
same	O
feature	O
vector	B
before	O
learning	O
nears	O
its	O
asymptote	O
what	O
step-size	B
parameter	I
should	O
you	O
use	O
why	O
nonlinear	O
function	B
approximation	I
artificial	O
neu	O
ral	O
networks	O
artificial	B
neural	B
networks	I
are	O
widely	O
used	O
for	O
nonlinear	O
function	B
approximation	I
an	O
ann	O
is	O
a	O
network	O
of	O
interconnected	O
units	O
that	O
have	O
some	O
of	O
the	O
properties	O
of	O
neurons	O
the	O
main	O
components	O
of	O
nervous	O
systems	O
anns	O
have	O
a	O
long	O
history	O
with	O
the	O
latest	O
advances	O
in	O
training	O
deeply-layered	O
anns	O
learning	O
being	O
responsible	O
for	O
some	O
of	O
the	O
most	O
impressive	O
abilities	O
of	O
machine	O
learning	O
systems	O
including	O
reinforcement	B
learning	I
systems	O
in	O
chapter	O
we	O
describe	O
several	O
impressive	O
examples	O
of	O
reinforcement	B
learning	I
systems	O
that	O
use	O
ann	O
function	B
approximation	I
figure	O
shows	O
a	O
generic	O
feedforward	O
ann	O
meaning	O
that	O
there	O
are	O
no	O
loops	O
in	O
the	O
network	O
that	O
is	O
there	O
are	O
no	O
paths	O
within	O
the	O
network	O
by	O
which	O
a	O
unit	O
s	O
output	O
can	O
influence	O
its	O
input	O
the	O
network	O
in	O
the	O
figure	O
has	O
an	O
output	O
layer	O
consisting	O
of	O
two	O
output	O
units	O
an	O
input	O
layer	O
with	O
four	O
input	O
units	O
and	O
two	O
hidden	O
layers	O
layers	O
that	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
figure	O
a	O
generic	O
feedforward	O
neural	B
network	O
with	O
four	O
input	O
units	O
two	O
output	O
units	O
and	O
two	O
hidden	O
layers	O
are	O
neither	O
input	O
nor	O
output	O
layers	O
a	O
real-valued	O
weight	O
is	O
associated	O
with	O
each	O
link	O
a	O
weight	O
roughly	O
corresponds	O
to	O
the	O
efficacy	O
of	O
a	O
synaptic	O
connection	O
in	O
a	O
real	O
neural	B
network	O
section	O
if	O
an	O
ann	O
has	O
at	O
least	O
one	O
loop	O
in	O
its	O
connections	O
it	O
is	O
a	O
recurrent	O
rather	O
than	O
a	O
feedforward	O
ann	O
although	O
both	O
feedforward	O
and	O
recurrent	O
anns	O
have	O
been	O
used	O
in	O
reinforcement	B
learning	I
here	O
we	O
look	O
only	O
at	O
the	O
simpler	O
feedforward	O
case	O
the	O
units	O
circles	O
in	O
figure	O
are	O
typically	O
semi-linear	O
units	O
meaning	O
that	O
they	O
compute	O
a	O
weighted	O
sum	O
of	O
their	O
input	O
signals	O
and	O
then	O
apply	O
to	O
the	O
result	O
a	O
nonlinear	O
function	O
called	O
the	O
activation	O
function	O
to	O
produce	O
the	O
unit	O
s	O
output	O
or	O
activation	O
different	O
activation	O
functions	O
are	O
used	O
but	O
they	O
are	O
typically	O
s-shaped	O
or	O
sigmoid	O
functions	O
such	O
as	O
the	O
logistic	O
function	O
f	O
e	O
x	O
though	O
sometimes	O
the	O
rectifier	O
nonlinearity	O
f	O
x	O
is	O
used	O
a	O
step	O
function	O
like	O
f	O
if	O
x	O
and	O
otherwise	O
results	O
in	O
a	O
binary	O
unit	O
with	O
threshold	O
the	O
units	O
in	O
a	O
network	O
s	O
input	O
layer	O
are	O
somewhat	O
different	O
in	O
having	O
their	O
activations	O
set	O
to	O
externally-supplied	O
values	O
that	O
are	O
the	O
inputs	O
to	O
the	O
function	O
the	O
network	O
is	O
approximating	O
the	O
activation	O
of	O
each	O
output	O
unit	O
of	O
a	O
feedforward	O
ann	O
is	O
a	O
nonlinear	O
function	O
of	O
the	O
activation	O
patterns	O
over	O
the	O
network	O
s	O
input	O
units	O
the	O
functions	O
are	O
parameterized	O
by	O
the	O
network	O
s	O
connection	O
weights	O
an	O
ann	O
with	O
no	O
hidden	O
layers	O
can	O
represent	O
only	O
a	O
very	O
small	O
fraction	O
of	O
the	O
possible	O
input-output	O
functions	O
however	O
an	O
ann	O
with	O
a	O
single	O
hidden	O
layer	O
containing	O
a	O
large	O
enough	O
finite	O
number	O
of	O
sigmoid	O
units	O
can	O
approximate	O
any	O
continuous	O
function	O
on	O
a	O
compact	O
region	O
of	O
the	O
network	O
s	O
input	O
space	O
to	O
any	O
degree	O
of	O
accuracy	O
this	O
is	O
also	O
true	O
for	O
other	O
nonlinear	O
activation	O
functions	O
that	O
satisfy	O
mild	O
conditions	O
but	O
nonlinearity	O
is	O
essential	O
if	O
all	O
the	O
units	O
in	O
a	O
multi-layer	O
feedforward	O
ann	O
have	O
linear	O
activation	O
functions	O
the	O
entire	O
network	O
is	O
equivalent	O
to	O
a	O
network	O
with	O
no	O
hidden	O
layers	O
linear	O
functions	O
of	O
linear	O
functions	O
are	O
themselves	O
linear	O
nonlinear	O
function	B
approximation	I
artificial	B
neural	B
networks	I
despite	O
this	O
universal	O
approximation	O
property	O
of	O
one-hidden-layer	O
anns	O
both	O
experience	O
and	O
theory	O
show	O
that	O
approximating	O
the	O
complex	O
functions	O
needed	O
for	O
many	O
artificial	B
intelligence	I
tasks	O
is	O
made	O
easier	O
indeed	O
may	O
require	O
abstractions	O
that	O
are	O
hierarchical	O
compositions	O
of	O
many	O
layers	O
of	O
lower-level	O
abstractions	O
that	O
is	O
abstractions	O
produced	O
by	O
deep	O
architectures	O
such	O
as	O
anns	O
with	O
many	O
hidden	O
layers	O
bengio	O
for	O
a	O
thorough	O
review	O
the	O
successive	O
layers	O
of	O
a	O
deep	O
ann	O
compute	O
increasingly	O
abstract	O
representations	O
of	O
the	O
network	O
s	O
raw	O
input	O
with	O
each	O
unit	O
providing	O
a	O
feature	O
contributing	O
to	O
a	O
hierarchical	O
representation	O
of	O
the	O
overall	O
input-output	O
function	O
of	O
the	O
network	O
training	O
the	O
hidden	O
layers	O
of	O
an	O
ann	O
is	O
therefore	O
a	O
way	O
to	O
automatically	O
create	O
features	O
appropriate	O
for	O
a	O
given	O
problem	O
so	O
that	O
hierarchical	O
representations	O
can	O
be	O
produced	O
without	O
relying	O
exclusively	O
on	O
hand-crafted	O
features	O
this	O
has	O
been	O
an	O
enduring	O
challenge	O
for	O
artificial	B
intelligence	I
and	O
explains	O
why	O
learning	O
algorithms	O
for	O
anns	O
with	O
hidden	O
layers	O
have	O
received	O
so	O
much	O
attention	O
over	O
the	O
years	O
anns	O
typically	O
learn	O
by	O
a	O
stochastic	O
gradient	B
method	O
each	O
weight	O
is	O
adjusted	O
in	O
a	O
direction	O
aimed	O
at	O
improving	O
the	O
network	O
s	O
overall	O
performance	O
as	O
measured	O
by	O
an	O
objective	O
function	O
to	O
be	O
either	O
minimized	O
or	O
maximized	O
in	O
the	O
most	O
common	O
supervised	B
learning	I
case	O
the	O
objective	O
function	O
is	O
the	O
expected	B
error	O
or	O
loss	O
over	O
a	O
set	O
of	O
labeled	O
training	O
examples	O
in	O
reinforcement	B
learning	I
anns	O
can	O
use	O
td	B
errors	O
to	O
learn	O
value	B
functions	O
or	O
they	O
can	O
aim	O
to	O
maximize	O
expected	B
reward	O
as	O
in	O
a	O
gradient	B
bandit	O
or	O
a	O
policygradient	O
algorithm	O
in	O
all	O
of	O
these	O
cases	O
it	O
is	O
necessary	O
to	O
estimate	O
how	O
a	O
change	O
in	O
each	O
connection	O
weight	O
would	O
influence	O
the	O
network	O
s	O
overall	O
performance	O
in	O
other	O
words	O
to	O
estimate	O
the	O
partial	O
derivative	O
of	O
an	O
objective	O
function	O
with	O
respect	O
to	O
each	O
weight	O
given	O
the	O
current	O
values	O
of	O
all	O
the	O
network	O
s	O
weights	O
the	O
gradient	B
is	O
the	O
vector	B
of	O
these	O
partial	O
derivatives	O
the	O
most	O
successful	O
way	O
to	O
do	O
this	O
for	O
anns	O
with	O
hidden	O
layers	O
the	O
units	O
have	O
differentiable	O
activation	O
functions	O
is	O
the	O
backpropagation	B
algorithm	O
which	O
consists	O
of	O
alternating	O
forward	O
and	O
backward	O
passes	O
through	O
the	O
network	O
each	O
forward	O
pass	O
computes	O
the	O
activation	O
of	O
each	O
unit	O
given	O
the	O
current	O
activations	O
of	O
the	O
network	O
s	O
input	O
units	O
after	O
each	O
forward	O
pass	O
a	O
backward	O
pass	O
efficiently	O
computes	O
a	O
partial	O
derivative	O
for	O
each	O
weight	O
in	O
other	O
stochastic	O
gradient	B
learning	O
algorithms	O
the	O
vector	B
of	O
these	O
partial	O
derivatives	O
is	O
an	O
estimate	O
of	O
the	O
true	O
gradient	B
in	O
section	O
we	O
discuss	O
methods	O
for	O
training	O
anns	O
with	O
hidden	O
layers	O
that	O
use	O
reinforcement	B
learning	I
principles	O
instead	O
of	O
backpropagation	B
these	O
methods	O
are	O
less	O
efficient	O
than	O
the	O
backpropagation	B
algorithm	O
but	O
they	O
may	O
be	O
closer	O
to	O
how	O
real	O
neural	B
networks	O
learn	O
the	O
backpropagation	B
algorithm	O
can	O
produce	O
good	O
results	O
for	O
shallow	O
networks	O
having	O
or	O
hidden	O
layers	O
but	O
it	O
may	O
not	O
work	O
well	O
for	O
deeper	O
anns	O
in	O
fact	O
training	O
a	O
network	O
with	O
k	O
hidden	O
layers	O
can	O
actually	O
result	O
in	O
poorer	O
performance	O
than	O
training	O
a	O
network	O
with	O
k	O
hidden	O
layers	O
even	O
though	O
the	O
deeper	O
network	O
can	O
represent	O
all	O
the	O
functions	O
that	O
the	O
shallower	O
network	O
can	O
explaining	O
results	O
like	O
these	O
is	O
not	O
easy	O
but	O
several	O
factors	O
are	O
important	O
first	O
the	O
large	O
number	O
of	O
weights	O
in	O
a	O
typical	O
deep	O
ann	O
makes	O
it	O
difficult	O
to	O
avoid	O
the	O
problem	O
of	O
overfitting	O
that	O
is	O
the	O
problem	O
of	O
failing	O
to	O
generalize	O
correctly	O
to	O
cases	O
on	O
which	O
the	O
network	O
has	O
not	O
been	O
trained	O
second	O
backpropagation	B
does	O
not	O
work	O
well	O
for	O
deep	O
anns	O
because	O
the	O
partial	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
derivatives	O
computed	O
by	O
its	O
backward	O
passes	O
either	O
decay	O
rapidly	O
toward	O
the	O
input	O
side	O
of	O
the	O
network	O
making	O
learning	O
by	O
deep	O
layers	O
extremely	O
slow	O
or	O
the	O
partial	O
derivatives	O
grow	O
rapidly	O
toward	O
the	O
input	O
side	O
of	O
the	O
network	O
making	O
learning	O
unstable	O
methods	O
for	O
dealing	O
with	O
these	O
problems	O
are	O
largely	O
responsible	O
for	O
many	O
impressive	O
recent	O
results	O
achieved	O
by	O
systems	O
that	O
use	O
deep	O
anns	O
overfitting	O
is	O
a	O
problem	O
for	O
any	O
function	B
approximation	I
method	O
that	O
adjusts	O
functions	O
with	O
many	O
degrees	O
of	O
freedom	O
on	O
the	O
basis	O
of	O
limited	O
training	O
data	O
it	O
is	O
less	O
of	O
a	O
problem	O
for	O
online	B
reinforcement	B
learning	I
that	O
does	O
not	O
rely	O
on	O
limited	O
training	O
sets	O
but	O
generalizing	O
effectively	O
is	O
still	O
an	O
important	O
issue	O
overfitting	O
is	O
a	O
problem	O
for	O
anns	O
in	O
general	O
but	O
especially	O
so	O
for	O
deep	O
anns	O
because	O
they	O
tend	O
to	O
have	O
very	O
large	O
numbers	O
of	O
weights	O
many	O
methods	O
have	O
been	O
developed	O
for	O
reducing	O
overfitting	O
these	O
include	O
stopping	O
training	O
when	O
performance	O
begins	O
to	O
decrease	O
on	O
validation	O
data	O
different	O
from	O
the	O
training	O
data	O
validation	O
modifying	O
the	O
objective	O
function	O
to	O
discourage	O
complexity	O
of	O
the	O
approximation	O
and	O
introducing	O
dependencies	O
among	O
the	O
weights	O
to	O
reduce	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
weight	O
sharing	O
a	O
particularly	O
effective	O
method	O
for	O
reducing	O
overfitting	O
by	O
deep	O
anns	O
is	O
the	O
dropout	O
method	O
introduced	O
by	O
srivastava	O
hinton	O
krizhevsky	O
sutskever	O
and	O
salakhutdinov	O
during	O
training	O
units	O
are	O
randomly	O
removed	O
from	O
the	O
network	O
out	O
along	O
with	O
their	O
connections	O
this	O
can	O
be	O
thought	O
of	O
as	O
training	O
a	O
large	O
number	O
of	O
thinned	O
networks	O
combining	O
the	O
results	O
of	O
these	O
thinned	O
networks	O
at	O
test	O
time	O
is	O
a	O
way	O
to	O
improve	O
generalization	O
performance	O
the	O
dropout	O
method	O
efficiently	O
approximates	O
this	O
combination	O
by	O
multiplying	O
each	O
outgoing	O
weight	O
of	O
a	O
unit	O
by	O
the	O
probability	O
that	O
that	O
unit	O
was	O
retained	O
during	O
training	O
srivastava	O
et	O
al	O
found	O
that	O
this	O
method	O
significantly	O
improves	O
generalization	O
performance	O
it	O
encourages	O
individual	O
hidden	O
units	O
to	O
learn	O
features	O
that	O
work	O
well	O
with	O
random	O
collections	O
of	O
other	O
features	O
this	O
increases	O
the	O
versatility	O
of	O
the	O
features	O
formed	O
by	O
the	O
hidden	O
units	O
so	O
that	O
the	O
network	O
does	O
not	O
overly	O
specialize	O
to	O
rarely-occurring	O
cases	O
hinton	O
osindero	O
and	O
teh	O
took	O
a	O
major	O
step	O
toward	O
solving	O
the	O
problem	O
of	O
training	O
the	O
deep	O
layers	O
of	O
a	O
deep	O
ann	O
in	O
their	O
work	O
with	O
deep	O
belief	O
networks	O
layered	O
networks	O
closely	O
related	O
to	O
the	O
deep	O
anns	O
discussed	O
here	O
in	O
their	O
method	O
the	O
deepest	O
layers	O
are	O
trained	O
one	O
at	O
a	O
time	O
using	O
an	O
unsupervised	B
learning	I
algorithm	O
without	O
relying	O
on	O
the	O
overall	O
objective	O
function	O
unsupervised	B
learning	I
can	O
extract	O
features	O
that	O
capture	O
statistical	O
regularities	O
of	O
the	O
input	O
stream	O
the	O
deepest	O
layer	O
is	O
trained	O
first	O
then	O
with	O
input	O
provided	O
by	O
this	O
trained	O
layer	O
the	O
next	O
deepest	O
layer	O
is	O
trained	O
and	O
so	O
on	O
until	O
the	O
weights	O
in	O
all	O
or	O
many	O
of	O
the	O
network	O
s	O
layers	O
are	O
set	O
to	O
values	O
that	O
now	O
act	O
as	O
initial	O
values	O
for	O
supervised	B
learning	I
the	O
network	O
is	O
then	O
fine-tuned	O
by	O
backpropagation	B
with	O
respect	O
to	O
the	O
overall	O
objective	O
function	O
studies	O
show	O
that	O
this	O
approach	O
generally	O
works	O
much	O
better	O
than	O
backpropagation	B
with	O
weights	O
initialized	O
with	O
random	O
values	O
the	O
better	O
performance	O
of	O
networks	O
trained	O
with	O
weights	O
initialized	O
this	O
way	O
could	O
be	O
due	O
to	O
many	O
factors	O
but	O
one	O
idea	O
is	O
that	O
this	O
method	O
places	O
the	O
network	O
in	O
a	O
region	O
of	O
weight	O
space	O
from	O
which	O
a	O
gradient-based	O
algorithm	O
can	O
make	O
good	O
progress	O
batch	O
normalization	O
and	O
szegedy	O
is	O
another	O
technique	O
that	O
makes	O
it	O
easier	O
to	O
train	O
deep	O
anns	O
it	O
has	O
long	O
been	O
known	O
that	O
ann	O
learning	O
is	O
easier	O
if	O
the	O
network	O
input	O
is	O
normalized	O
for	O
example	O
by	O
adjusting	O
each	O
input	O
variable	O
to	O
have	O
zero	O
nonlinear	O
function	B
approximation	I
artificial	B
neural	B
networks	I
mean	O
and	O
unit	O
variance	O
batch	O
normalization	O
for	O
training	O
deep	O
anns	O
normalizes	O
the	O
output	O
of	O
deep	O
layers	O
before	O
they	O
feed	O
into	O
the	O
following	O
layer	O
ioffe	O
and	O
szegedy	O
used	O
statistics	O
from	O
subsets	O
or	O
mini-batches	O
of	O
training	O
examples	O
to	O
normalize	O
these	O
between-layer	O
signals	O
to	O
improve	O
the	O
learning	O
rate	O
of	O
deep	O
anns	O
another	O
technique	O
useful	O
for	O
training	O
deep	O
anns	O
is	O
deep	B
residual	I
learning	I
zhang	O
ren	O
and	O
sun	O
sometimes	O
it	O
is	O
easier	O
to	O
learn	O
how	O
a	O
function	O
differs	O
from	O
the	O
identity	O
function	O
than	O
to	O
learn	O
the	O
function	O
itself	O
then	O
adding	O
this	O
difference	O
or	O
residual	O
function	O
to	O
the	O
input	O
produces	O
the	O
desired	O
function	O
in	O
deep	O
anns	O
a	O
block	O
of	O
layers	O
can	O
be	O
made	O
to	O
learn	O
a	O
residual	O
function	O
simply	O
by	O
adding	O
shortcut	O
or	O
skip	O
connections	O
around	O
the	O
block	O
these	O
connections	O
add	O
the	O
input	O
to	O
the	O
block	O
to	O
its	O
output	O
and	O
no	O
additional	O
weights	O
are	O
needed	O
he	O
et	O
al	O
evaluated	O
this	O
method	O
using	O
deep	O
convolutional	O
networks	O
with	O
skip	O
connections	O
around	O
every	O
pair	O
of	O
adjacent	O
layers	O
finding	O
substantial	O
improvement	O
over	O
networks	O
without	O
the	O
skip	O
connections	O
on	O
benchmark	O
image	O
classification	O
tasks	O
both	O
batch	O
normalization	O
and	O
deep	B
residual	I
learning	I
were	O
used	O
in	O
the	O
reinforcement	B
learning	I
application	O
to	O
the	O
game	O
of	O
go	O
that	O
we	O
describe	O
in	O
chapter	O
a	O
type	O
of	O
deep	O
ann	O
that	O
has	O
proven	O
to	O
be	O
very	O
successful	O
in	O
applications	O
including	O
impressive	O
reinforcement	B
learning	I
applications	O
is	O
the	O
deep	O
convolutional	O
network	O
this	O
type	O
of	O
network	O
is	O
specialized	O
for	O
processing	O
high-dimensional	O
data	O
arranged	O
in	O
spatial	O
arrays	O
such	O
as	O
images	O
it	O
was	O
inspired	O
by	O
how	O
early	O
visual	O
processing	O
works	O
in	O
the	O
brain	O
bottou	O
bengio	O
and	O
haffner	O
because	O
of	O
its	O
special	O
architecture	O
a	O
deep	O
convolutional	O
network	O
can	O
be	O
trained	O
by	O
backpropagation	B
without	O
resorting	O
to	O
methods	O
like	O
those	O
described	O
above	O
to	O
train	O
the	O
deep	O
layers	O
figure	O
illustrates	O
the	O
architecture	O
of	O
a	O
deep	O
convolutional	O
network	O
this	O
instance	O
from	O
lecun	O
et	O
al	O
was	O
designed	O
to	O
recognize	O
hand-written	O
characters	O
it	O
consists	O
of	O
alternating	O
convolutional	O
and	O
subsampling	O
layers	O
followed	O
by	O
several	O
fully	O
connected	O
final	O
layers	O
each	O
convolutional	O
layer	O
produces	O
a	O
number	O
of	O
feature	O
maps	O
a	O
feature	O
map	O
is	O
a	O
pattern	O
of	O
activity	O
over	O
an	O
array	O
of	O
units	O
where	O
each	O
unit	O
performs	O
the	O
same	O
figure	O
deep	O
convolutional	O
network	O
republished	O
with	O
permission	O
of	O
proceedings	O
of	O
the	O
ieee	O
from	O
gradient-based	O
learning	O
applied	O
to	O
document	O
recognition	O
lecun	O
bottou	O
bengio	O
and	O
haffner	O
volume	O
permission	O
conveyed	O
through	O
copyright	O
clearance	O
center	O
inc	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
operation	O
on	O
data	O
in	O
its	O
receptive	O
field	O
which	O
is	O
the	O
part	O
of	O
the	O
data	O
it	O
sees	O
from	O
the	O
preceding	O
layer	O
from	O
the	O
external	O
input	O
in	O
the	O
case	O
of	O
the	O
first	O
convolutional	O
layer	O
the	O
units	O
of	O
a	O
feature	O
map	O
are	O
identical	O
to	O
one	O
another	O
except	O
that	O
their	O
receptive	O
fields	O
which	O
are	O
all	O
the	O
same	O
size	O
and	O
shape	O
are	O
shifted	O
to	O
different	O
locations	O
on	O
the	O
arrays	O
of	O
incoming	O
data	O
units	O
in	O
the	O
same	O
feature	O
map	O
share	O
the	O
same	O
weights	O
this	O
means	O
that	O
a	O
feature	O
map	O
detects	O
the	O
same	O
feature	O
no	O
matter	O
where	O
it	O
is	O
located	O
in	O
the	O
input	O
array	O
in	O
the	O
network	O
in	O
figure	O
for	O
example	O
the	O
first	O
convolutional	O
layer	O
produces	O
feature	O
maps	O
each	O
consisting	O
of	O
units	O
each	O
unit	O
in	O
each	O
feature	O
map	O
has	O
a	O
receptive	O
field	O
and	O
these	O
receptive	O
fields	O
overlap	O
this	O
case	O
by	O
four	O
columns	O
and	O
four	O
rows	O
consequently	O
each	O
of	O
the	O
feature	O
maps	O
is	O
specified	O
by	O
just	O
adjustable	O
weights	O
the	O
subsampling	O
layers	O
of	O
a	O
deep	O
convolutional	O
network	O
reduce	O
the	O
spatial	O
resolution	O
of	O
the	O
feature	O
maps	O
each	O
feature	O
map	O
in	O
a	O
subsampling	O
layer	O
consists	O
of	O
units	O
that	O
average	O
over	O
a	O
receptive	O
field	O
of	O
units	O
in	O
the	O
feature	O
maps	O
of	O
the	O
preceding	O
convolutional	O
layer	O
for	O
example	O
each	O
unit	O
in	O
each	O
of	O
the	O
feature	O
maps	O
in	O
the	O
first	O
subsampling	O
layer	O
of	O
the	O
network	O
of	O
figure	O
averages	O
over	O
a	O
non-overlapping	O
receptive	O
field	O
over	O
one	O
of	O
the	O
feature	O
maps	O
produced	O
by	O
the	O
first	O
convolutional	O
layer	O
resulting	O
in	O
six	O
feature	O
maps	O
subsampling	O
layers	O
reduce	O
the	O
network	O
s	O
sensitivity	O
to	O
the	O
spatial	O
locations	O
of	O
the	O
features	O
detected	O
that	O
is	O
they	O
help	O
make	O
the	O
network	O
s	O
responses	O
spatially	O
invariant	O
this	O
is	O
useful	O
because	O
a	O
feature	O
detected	O
at	O
one	O
place	O
in	O
an	O
image	O
is	O
likely	O
to	O
be	O
useful	O
at	O
other	O
places	O
as	O
well	O
advances	O
in	O
the	O
design	O
and	O
training	O
of	O
anns	O
of	O
which	O
we	O
have	O
only	O
mentioned	O
a	O
few	O
all	O
contribute	O
to	O
reinforcement	B
learning	I
although	O
current	O
reinforcement	B
learning	I
theory	O
is	O
mostly	O
limited	O
to	O
methods	O
using	O
tabular	O
or	O
linear	O
function	B
approximation	I
methods	O
the	O
impressive	O
performances	O
of	O
notable	O
reinforcement	B
learning	I
applications	O
owe	O
much	O
of	O
their	O
success	O
to	O
nonlinear	O
function	B
approximation	I
by	O
multi-layer	O
anns	O
we	O
discuss	O
several	O
of	O
these	O
applications	O
in	O
chapter	O
least-squares	O
td	B
all	O
the	O
methods	O
we	O
have	O
discussed	O
so	O
far	O
in	O
this	O
chapter	O
have	O
required	O
computation	O
per	O
time	O
step	O
proportional	O
to	O
the	O
number	O
of	O
parameters	O
with	O
more	O
computation	O
however	O
one	O
can	O
do	O
better	O
in	O
this	O
section	O
we	O
present	O
a	O
method	O
for	O
linear	O
function	B
approximation	I
that	O
is	O
arguably	O
the	O
best	O
that	O
can	O
be	O
done	O
for	O
this	O
case	O
as	O
we	O
established	O
in	O
section	O
with	O
linear	O
function	B
approximation	I
converges	O
asymptotically	O
appropriately	O
decreasing	O
step	O
sizes	O
to	O
the	O
td	B
fixed	O
point	O
wtd	O
a	O
where	O
a	O
and	O
b	O
why	O
one	O
might	O
ask	O
must	O
we	O
compute	O
this	O
solution	O
iteratively	O
this	O
is	O
wasteful	O
of	O
data	O
could	O
one	O
not	O
do	O
better	O
by	O
computing	O
estimates	O
of	O
a	O
and	O
b	O
and	O
then	O
directly	O
least-squares	O
td	B
computing	O
the	O
td	B
fixed	O
point	O
the	O
least-squares	O
td	B
algorithm	O
commonly	O
known	O
as	O
lstd	O
does	O
exactly	O
this	O
it	O
forms	O
the	O
natural	O
estimates	O
t	O
xkxk	O
i	O
and	O
t	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
and	O
i	O
for	O
some	O
small	O
ensures	O
that	O
is	O
always	O
invertible	O
it	O
might	O
seem	O
that	O
these	O
estimates	O
should	O
both	O
be	O
divided	O
by	O
t	O
and	O
indeed	O
they	O
should	O
as	O
defined	O
here	O
these	O
are	O
really	O
estimates	O
of	O
t	O
times	O
a	O
and	O
t	O
times	O
b	O
however	O
the	O
t	O
factor	O
will	O
not	O
matter	O
as	O
when	O
we	O
use	O
these	O
estimates	O
we	O
will	O
be	O
effectively	O
dividing	O
one	O
by	O
the	O
other	O
lstd	O
estimates	O
the	O
td	B
fixed	O
point	O
as	O
wt	O
t	O
this	O
algorithm	O
is	O
the	O
most	O
data	O
efficient	O
form	O
of	O
linear	O
but	O
it	O
is	O
also	O
more	O
expensive	O
computationally	O
recall	O
that	O
semi-gradient	O
requires	O
memory	O
and	O
per-step	O
computation	O
that	O
is	O
only	O
od	O
how	O
complex	O
is	O
lstd	O
as	O
it	O
is	O
written	O
above	O
the	O
complexity	O
seems	O
to	O
increase	O
with	O
t	O
but	O
the	O
two	O
approximations	O
in	O
could	O
be	O
implemented	O
incrementally	O
using	O
the	O
techniques	O
we	O
have	O
covered	O
earlier	O
in	O
chapter	O
so	O
that	O
they	O
can	O
be	O
done	O
in	O
would	O
be	O
column	O
vector	B
times	O
a	O
row	O
vector	B
and	O
thus	O
would	O
be	O
a	O
matrix	O
update	O
its	O
computational	O
constant	O
time	O
per	O
step	O
even	O
so	O
the	O
update	O
for	O
would	O
involve	O
an	O
outer	O
product	O
complexity	O
would	O
be	O
and	O
of	O
course	O
the	O
memory	O
required	O
to	O
hold	O
the	O
matrix	O
of	O
and	O
the	O
computational	O
complexity	O
of	O
a	O
general	O
inverse	O
computation	O
is	O
fortunately	O
an	O
inverse	O
of	O
a	O
matrix	O
of	O
our	O
special	O
form	O
a	O
sum	O
of	O
outer	O
products	O
can	O
also	O
be	O
updated	O
incrementally	O
with	O
only	O
computations	O
as	O
a	O
potentially	O
greater	O
problem	O
is	O
that	O
our	O
final	O
computation	O
uses	O
the	O
inverse	O
t	O
t	O
xtxt	O
t	O
for	O
t	O
with	O
multiplications	O
and	O
thus	O
is	O
only	O
thus	O
we	O
can	O
store	O
the	O
inverse	O
matrix	O
i	O
although	O
the	O
identity	O
known	O
as	O
the	O
sherman-morrison	O
formula	O
is	O
superficially	O
complicated	O
it	O
involves	O
only	O
vector-matrix	O
and	O
vector-vector	O
maintain	O
it	O
with	O
and	O
then	O
use	O
it	O
in	O
all	O
with	O
only	O
memory	O
and	O
per-step	O
computation	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
t	O
t	O
t	O
of	O
course	O
is	O
still	O
significantly	O
more	O
expensive	O
than	O
the	O
od	O
of	O
semi-gradient	O
td	B
whether	O
the	O
greater	O
data	O
efficiency	O
of	O
lstd	O
is	O
worth	O
this	O
computational	O
expense	O
depends	O
on	O
how	O
large	O
d	O
is	O
how	O
important	O
it	O
is	O
to	O
learn	O
quickly	O
and	O
the	O
expense	O
of	O
other	O
parts	O
of	O
the	O
system	O
the	O
fact	O
that	O
lstd	O
requires	O
no	O
step-size	B
parameter	I
is	O
sometimes	O
also	O
touted	O
but	O
the	O
advantage	O
of	O
this	O
is	O
probably	O
overstated	O
lstd	O
does	O
not	O
require	O
a	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
lstd	O
for	O
estimating	O
v	O
v	O
version	O
input	O
feature	O
representation	O
x	O
s	O
rd	O
such	O
that	O
xterminal	O
algorithm	O
parameter	O
small	O
loop	O
for	O
each	O
episode	O
a	O
d	O
d	O
matrix	O
a	O
d-dimensional	O
vector	B
initialize	O
s	O
x	O
xs	O
loop	O
for	O
each	O
step	O
of	O
episode	O
choose	O
and	O
take	O
action	B
a	O
observe	O
r	O
v	O
rx	O
w	O
s	O
x	O
until	O
is	O
terminal	O
step	O
size	O
but	O
it	O
does	O
requires	O
if	O
is	O
chosen	O
too	O
small	O
the	O
sequence	O
of	O
inverses	O
can	O
vary	O
wildly	O
and	O
if	O
is	O
chosen	O
too	O
large	O
then	O
learning	O
is	O
slowed	O
in	O
addition	O
lstd	O
s	O
lack	O
of	O
a	O
step-size	B
parameter	I
means	O
that	O
it	O
never	O
forgets	O
this	O
is	O
sometimes	O
desirable	O
but	O
it	O
is	O
problematic	O
if	O
the	O
target	O
policy	B
changes	O
as	O
it	O
does	O
in	O
reinforcement	B
learning	I
and	O
gpi	O
in	O
control	B
applications	O
lstd	O
typically	O
has	O
to	O
be	O
combined	O
with	O
some	O
other	O
mechanism	O
to	O
induce	O
forgeting	O
mooting	O
any	O
initial	O
advantage	O
of	O
not	O
requiring	O
a	O
step-size	B
parameter	I
memory-based	O
function	B
approximation	I
so	O
far	O
we	O
have	O
discussed	O
the	O
parametric	O
approach	O
to	O
approximating	O
value	B
functions	O
in	O
this	O
approach	O
a	O
learning	O
algorithm	O
adjusts	O
the	O
parameters	O
of	O
a	O
functional	O
form	O
intended	O
to	O
approximate	O
the	O
value	B
function	I
over	O
a	O
problem	O
s	O
entire	O
state	B
space	O
each	O
update	O
s	O
g	O
is	O
a	O
training	O
example	O
used	O
by	O
the	O
learning	O
algorithm	O
to	O
change	O
the	O
parameters	O
with	O
the	O
aim	O
of	O
reducing	O
the	O
approximation	O
error	O
after	O
the	O
update	O
the	O
training	O
example	O
can	O
be	O
discarded	O
it	O
might	O
be	O
saved	O
to	O
be	O
used	O
again	O
when	O
an	O
approximate	O
value	B
of	O
a	O
state	B
we	O
will	O
call	O
the	O
query	O
state	B
is	O
needed	O
the	O
function	O
is	O
simply	O
evaluated	O
at	O
that	O
state	B
using	O
the	O
latest	O
parameters	O
produced	O
by	O
the	O
learning	O
algorithm	O
memory-based	O
function	B
approximation	I
methods	O
are	O
very	O
different	O
they	O
simply	O
save	O
training	O
examples	O
in	O
memory	O
as	O
they	O
arrive	O
at	O
least	O
save	O
a	O
subset	O
of	O
the	O
examples	O
without	O
updating	O
any	O
parameters	O
then	O
whenever	O
a	O
query	O
state	B
s	O
value	B
estimate	O
is	O
needed	O
a	O
set	O
of	O
examples	O
is	O
retrieved	O
from	O
memory	O
and	O
used	O
to	O
compute	O
a	O
value	B
estimate	O
for	O
the	O
query	O
state	B
this	O
approach	O
is	O
sometimes	O
called	O
lazy	O
learning	O
because	O
processing	O
training	O
examples	O
is	O
postponed	O
until	O
the	O
system	O
is	O
queried	O
to	O
provide	O
an	O
output	O
memory-based	O
function	B
approximation	I
methods	O
are	O
prime	O
examples	O
of	O
nonparametric	O
memory-based	O
function	B
approximation	I
methods	O
unlike	O
parametric	O
methods	O
the	O
approximating	O
function	O
s	O
form	O
is	O
not	O
limited	O
to	O
a	O
fixed	O
parameterized	O
class	O
of	O
functions	O
such	O
as	O
linear	O
functions	O
or	O
polynomials	O
but	O
is	O
instead	O
determined	O
by	O
the	O
training	O
examples	O
themselves	O
together	O
with	O
some	O
means	O
for	O
combining	O
them	O
to	O
output	O
estimated	O
values	O
for	O
query	O
states	O
as	O
more	O
training	O
examples	O
accumulate	O
in	O
memory	O
one	O
expects	O
nonparametric	O
methods	O
to	O
produce	O
increasingly	O
accurate	O
approximations	O
of	O
any	O
target	O
function	O
there	O
are	O
many	O
different	O
memory-based	O
methods	O
depending	O
on	O
how	O
the	O
stored	O
training	O
examples	O
are	O
selected	O
and	O
how	O
they	O
are	O
used	O
to	O
respond	O
to	O
a	O
query	O
here	O
we	O
focus	O
on	O
local-learning	O
methods	O
that	O
approximate	O
a	O
value	B
function	I
only	O
locally	O
in	O
the	O
neighborhood	O
of	O
the	O
current	O
query	O
state	B
these	O
methods	O
retrieve	O
a	O
set	O
of	O
training	O
examples	O
from	O
memory	O
whose	O
states	O
are	O
judged	O
to	O
be	O
the	O
most	O
relevant	O
to	O
the	O
query	O
state	B
where	O
relevance	O
usually	O
depends	O
on	O
the	O
distance	O
between	O
states	O
the	O
closer	O
a	O
training	O
example	O
s	O
state	B
is	O
to	O
the	O
query	O
state	B
the	O
more	O
relevant	O
it	O
is	O
considered	O
to	O
be	O
where	O
distance	O
can	O
be	O
defined	O
in	O
many	O
different	O
ways	O
after	O
the	O
query	O
state	B
is	O
given	O
a	O
value	B
the	O
local	O
approximation	O
is	O
discarded	O
the	O
simplest	O
example	O
of	O
the	O
memory-based	O
approach	O
is	O
the	O
nearest	O
neighbor	O
method	O
which	O
simply	O
finds	O
the	O
example	O
in	O
memory	O
whose	O
state	B
is	O
closest	O
to	O
the	O
query	O
state	B
and	O
returns	O
that	O
example	O
s	O
value	B
as	O
the	O
approximate	O
value	B
of	O
the	O
query	O
state	B
in	O
other	O
words	O
if	O
the	O
query	O
state	B
is	O
s	O
and	O
g	O
is	O
the	O
example	O
in	O
memory	O
in	O
which	O
is	O
the	O
closest	O
state	B
to	O
s	O
then	O
g	O
is	O
returned	O
as	O
the	O
approximate	O
value	B
of	O
s	O
slightly	O
more	O
complicated	O
are	O
weighted	O
average	O
methods	O
that	O
retrieve	O
a	O
set	O
of	O
nearest	O
neighbor	O
examples	O
and	O
return	B
a	O
weighted	O
average	O
of	O
their	O
target	O
values	O
where	O
the	O
weights	O
generally	O
decrease	O
with	O
increasing	O
distance	O
between	O
their	O
states	O
and	O
the	O
query	O
state	B
locally	O
weighted	O
regression	O
is	O
similar	O
but	O
it	O
fits	O
a	O
surface	O
to	O
the	O
values	O
of	O
a	O
set	O
of	O
nearest	O
states	O
by	O
means	O
of	O
a	O
parametric	O
approximation	O
method	O
that	O
minimizes	O
a	O
weighted	O
error	O
measure	O
like	O
where	O
the	O
weights	O
depend	O
on	O
distances	O
from	O
the	O
query	O
state	B
the	O
value	B
returned	O
is	O
the	O
evaluation	O
of	O
the	O
locally-fitted	O
surface	O
at	O
the	O
query	O
state	B
after	O
which	O
the	O
local	O
approximation	O
surface	O
is	O
discarded	O
being	O
nonparametric	O
memory-based	O
methods	O
have	O
the	O
advantage	O
over	O
parametric	O
methods	O
of	O
not	O
limiting	O
approximations	O
to	O
pre-specified	O
functional	O
forms	O
this	O
allows	O
accuracy	O
to	O
improve	O
as	O
more	O
data	O
accumulates	O
memory-based	O
local	O
approximation	O
methods	O
have	O
other	O
properties	O
that	O
make	O
them	O
well	O
suited	O
for	O
reinforcement	B
learning	I
because	O
trajectory	B
sampling	I
is	O
of	O
such	O
importance	O
in	O
reinforcement	B
learning	I
as	O
discussed	O
in	O
section	O
memory-based	O
local	O
methods	O
can	O
focus	O
function	B
approximation	I
on	O
local	O
neighborhoods	O
of	O
states	O
state	B
action	B
pairs	O
visited	O
in	O
real	O
or	O
simulated	O
trajectories	O
there	O
may	O
be	O
no	O
need	O
for	O
global	O
approximation	O
because	O
many	O
areas	O
of	O
the	O
state	B
space	O
will	O
never	O
almost	O
never	O
be	O
reached	O
in	O
addition	O
memory-based	O
methods	O
allow	O
an	O
agent	O
s	O
experience	O
to	O
have	O
a	O
relatively	O
immediate	O
affect	O
on	O
value	B
estimates	O
in	O
the	O
neighborhood	O
of	O
the	O
current	O
state	B
in	O
contrast	O
with	O
a	O
parametric	O
method	O
s	O
need	O
to	O
incrementally	O
adjust	O
parameters	O
of	O
a	O
global	O
approximation	O
avoiding	O
global	O
approximation	O
is	O
also	O
a	O
way	O
to	O
address	O
the	O
curse	B
of	I
dimensionality	I
for	O
example	O
for	O
a	O
state	B
space	O
with	O
k	O
dimensions	O
a	O
tabular	O
method	O
storing	O
a	O
global	O
approximation	O
requires	O
memory	O
exponential	O
in	O
k	O
on	O
the	O
other	O
hand	O
in	O
storing	O
examples	O
for	O
a	O
memory-based	O
method	O
each	O
example	O
requires	O
memory	O
proportional	O
to	O
k	O
and	O
the	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
memory	O
required	O
to	O
store	O
say	O
n	O
examples	O
is	O
linear	O
in	O
n	O
nothing	O
is	O
exponential	O
in	O
k	O
or	O
n	O
of	O
course	O
the	O
critical	O
remaining	O
issue	O
is	O
whether	O
a	O
memory-based	O
method	O
can	O
answer	O
queries	O
quickly	O
enough	O
to	O
be	O
useful	O
to	O
an	O
agent	O
a	O
related	O
concern	O
is	O
how	O
speed	O
degrades	O
as	O
the	O
size	O
of	O
the	O
memory	O
grows	O
finding	O
nearest	O
neighbors	O
in	O
a	O
large	O
database	O
can	O
take	O
too	O
long	O
to	O
be	O
practical	O
in	O
many	O
applications	O
proponents	O
of	O
memory-based	O
methods	O
have	O
developed	O
ways	O
to	O
accelerate	O
the	O
nearest	O
neighbor	O
search	O
using	O
parallel	O
computers	O
or	O
special	O
purpose	O
hardware	O
is	O
one	O
approach	O
another	O
is	O
the	O
use	O
of	O
special	O
multi-dimensional	O
data	O
structures	O
to	O
store	O
the	O
training	O
data	O
one	O
data	O
structure	O
studied	O
for	O
this	O
application	O
is	O
the	O
k-d	O
tree	O
for	O
k-dimensional	O
tree	O
which	O
recursively	O
splits	O
a	O
k-dimensional	O
space	O
into	O
regions	O
arranged	O
as	O
nodes	O
of	O
a	O
binary	O
tree	O
depending	O
on	O
the	O
amount	O
of	O
data	O
and	O
how	O
it	O
is	O
distributed	O
over	O
the	O
state	B
space	O
nearest-neighbor	O
search	O
using	O
k-d	O
trees	O
can	O
quickly	O
eliminate	O
large	O
regions	O
of	O
the	O
space	O
in	O
the	O
search	O
for	O
neighbors	O
making	O
the	O
searches	O
feasible	O
in	O
some	O
problems	O
where	O
naive	B
searches	O
would	O
take	O
too	O
long	O
locally	O
weighted	O
regression	O
additionally	O
requires	O
fast	O
ways	O
to	O
do	O
the	O
local	O
regression	O
computations	O
which	O
have	O
to	O
be	O
repeated	O
to	O
answer	O
each	O
query	O
researchers	O
have	O
developed	O
many	O
ways	O
to	O
address	O
these	O
problems	O
including	O
methods	O
for	O
forgetting	O
entries	O
in	O
order	O
to	O
keep	O
the	O
size	O
of	O
the	O
database	O
within	O
bounds	O
the	O
bibliographic	O
and	O
historical	O
comments	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
points	O
to	O
some	O
of	O
the	O
relevant	O
literature	O
including	O
a	O
selection	O
of	O
papers	O
describing	O
applications	O
of	O
memory-based	O
learning	O
to	O
reinforcement	B
learning	I
kernel-based	B
function	B
approximation	I
memory-based	O
methods	O
such	O
as	O
the	O
weighted	O
average	O
and	O
locally	O
weighted	O
regression	O
methods	O
described	O
above	O
depend	O
on	O
assigning	O
weights	O
to	O
examples	O
g	O
in	O
the	O
database	O
depending	O
on	O
the	O
distance	O
between	O
and	O
a	O
query	O
states	O
s	O
the	O
function	O
that	O
assigns	O
these	O
weights	O
is	O
called	O
a	O
kernel	O
function	O
or	O
simply	O
a	O
kernel	O
in	O
the	O
weighted	O
average	O
and	O
locally	O
weighted	O
regressions	O
methods	O
for	O
example	O
a	O
kernel	O
function	O
k	O
r	O
r	O
assigns	O
weights	O
to	O
distances	O
between	O
states	O
more	O
generally	O
weights	O
do	O
not	O
have	O
to	O
depend	O
on	O
distances	O
they	O
can	O
depend	O
on	O
some	O
other	O
measure	O
of	O
similarity	O
between	O
states	O
in	O
this	O
case	O
k	O
s	O
s	O
r	O
so	O
that	O
ks	O
is	O
the	O
weight	O
given	O
to	O
data	O
about	O
in	O
its	O
influence	O
on	O
answering	O
queries	O
about	O
s	O
viewed	O
slightly	O
differently	O
ks	O
is	O
a	O
measure	O
of	O
the	O
strength	O
of	O
generalization	O
from	O
to	O
s	O
kernel	O
functions	O
numerically	O
express	O
how	O
relevant	O
knowledge	O
about	O
any	O
state	B
is	O
to	O
any	O
other	O
state	B
as	O
an	O
example	O
the	O
strengths	O
of	O
generalization	O
for	O
tile	B
coding	I
shown	O
in	O
figure	O
correspond	O
to	O
different	O
kernel	O
functions	O
resulting	O
from	O
uniform	O
and	O
asymmetrical	O
tile	O
offsets	O
although	O
tile	B
coding	I
does	O
not	O
explicitly	O
use	O
a	O
kernel	O
function	O
in	O
its	O
operation	O
it	O
generalizes	O
according	O
to	O
one	O
in	O
fact	O
as	O
we	O
discuss	O
more	O
below	O
the	O
strength	O
of	O
generalization	O
resulting	O
from	O
linear	O
parametric	O
function	B
approximation	I
can	O
always	O
be	O
described	O
by	O
a	O
kernel	O
function	O
kernel	O
regression	O
is	O
the	O
memory-based	O
method	O
that	O
computes	O
a	O
kernel	O
weighted	O
average	O
of	O
the	O
targets	O
of	O
all	O
examples	O
stored	O
in	O
memory	O
assigning	O
the	O
result	O
to	O
the	O
query	O
state	B
if	O
d	O
is	O
the	O
set	O
of	O
stored	O
examples	O
and	O
denotes	O
the	O
target	O
for	O
state	B
in	O
a	O
looking	O
deeper	O
at	O
on-policy	O
learning	O
interest	B
and	I
emphasis	I
stored	O
example	O
then	O
kernel	O
regression	O
approximates	O
the	O
target	O
function	O
in	O
this	O
case	O
a	O
value	B
function	I
depending	O
on	O
d	O
as	O
ks	O
vsd	O
d	O
the	O
weighted	O
average	O
method	O
described	O
above	O
is	O
a	O
special	O
case	O
in	O
which	O
ks	O
is	O
nonzero	O
only	O
when	O
s	O
and	O
are	O
close	O
to	O
one	O
another	O
so	O
that	O
the	O
sum	O
need	O
not	O
be	O
computed	O
over	O
all	O
of	O
d	O
a	O
common	O
kernel	O
is	O
the	O
gaussian	O
radial	O
basis	O
function	O
used	O
in	O
rbf	O
function	B
approximation	I
as	O
described	O
in	O
section	O
in	O
the	O
method	O
described	O
there	O
rbfs	O
are	O
features	O
whose	O
centers	O
and	O
widths	O
are	O
either	O
fixed	O
from	O
the	O
start	O
with	O
centers	O
presumably	O
concentrated	O
in	O
areas	O
where	O
many	O
examples	O
are	O
expected	B
to	O
fall	O
or	O
are	O
adjusted	O
in	O
some	O
way	O
during	O
learning	O
barring	O
methods	O
that	O
adjust	O
centers	O
and	O
widths	O
this	O
is	O
a	O
linear	O
parametric	O
method	O
whose	O
parameters	O
are	O
the	O
weights	O
of	O
each	O
rbf	O
which	O
are	O
typically	O
learned	O
by	O
stochastic	O
gradient	B
or	O
semi-gradient	O
descent	O
the	O
form	O
of	O
the	O
approximation	O
is	O
a	O
linear	O
combination	O
of	O
the	O
pre-determined	O
rbfs	O
kernel	O
regression	O
with	O
an	O
rbf	O
kernel	O
differs	O
from	O
this	O
in	O
two	O
ways	O
first	O
it	O
is	O
memory-based	O
the	O
rbfs	O
are	O
centered	O
on	O
the	O
states	O
of	O
the	O
stored	O
examples	O
second	O
it	O
is	O
nonparametric	O
there	O
are	O
no	O
parameters	O
to	O
learn	O
the	O
response	O
to	O
a	O
query	O
is	O
given	O
by	O
of	O
course	O
many	O
issues	O
have	O
to	O
be	O
addressed	O
for	O
practical	O
implementation	O
of	O
kernel	O
regression	O
issues	O
that	O
are	O
beyond	O
the	O
scope	O
or	O
our	O
brief	O
discussion	O
however	O
it	O
turns	O
out	O
that	O
any	O
linear	O
parametric	O
regression	O
method	O
like	O
those	O
we	O
described	O
in	O
section	O
with	O
states	O
represented	O
by	O
feature	O
vectors	O
xs	O
can	O
be	O
recast	O
as	O
kernel	O
regression	O
where	O
ks	O
is	O
the	O
inner	O
product	O
of	O
the	O
feature	O
vector	B
representations	O
of	O
s	O
and	O
that	O
is	O
ks	O
kernel	O
regression	O
with	O
this	O
kernel	O
function	O
produces	O
the	O
same	O
approximation	O
that	O
a	O
linear	O
parametric	O
method	O
would	O
if	O
it	O
used	O
these	O
feature	O
vectors	O
and	O
learned	O
with	O
the	O
same	O
training	O
data	O
we	O
skip	O
the	O
mathematical	O
justification	O
for	O
this	O
which	O
can	O
be	O
found	O
in	O
any	O
modern	O
machine	O
learning	O
text	O
such	O
as	O
bishop	O
and	O
simply	O
point	O
out	O
an	O
important	O
implication	O
instead	O
of	O
constructing	O
features	O
for	O
linear	O
parametric	O
function	O
approximators	O
one	O
can	O
instead	O
construct	O
kernel	O
functions	O
directly	O
without	O
referring	O
at	O
all	O
to	O
feature	O
vectors	O
not	O
all	O
kernel	O
functions	O
can	O
be	O
expressed	O
as	O
inner	O
products	O
of	O
feature	O
vectors	O
as	O
in	O
but	O
a	O
kernel	O
function	O
that	O
can	O
be	O
expressed	O
like	O
this	O
can	O
offer	O
significant	O
advantages	O
over	O
the	O
equivalent	O
parametric	O
method	O
for	O
many	O
sets	O
of	O
feature	O
vectors	O
has	O
a	O
compact	O
functional	O
form	O
that	O
can	O
be	O
evaluated	O
without	O
any	O
computation	O
taking	O
place	O
in	O
the	O
d-dimensional	O
feature	O
space	O
in	O
these	O
cases	O
kernel	O
regression	O
is	O
much	O
less	O
complex	O
than	O
directly	O
using	O
a	O
linear	O
parametric	O
method	O
with	O
states	O
represented	O
by	O
these	O
feature	O
vectors	O
this	O
is	O
the	O
so-called	O
kernel	O
trick	O
that	O
allows	O
effectively	O
working	O
in	O
the	O
high-dimension	O
of	O
an	O
expansive	O
feature	O
space	O
while	O
actually	O
working	O
only	O
with	O
the	O
set	O
of	O
stored	O
training	O
examples	O
the	O
kernel	O
trick	O
is	O
the	O
basis	O
of	O
many	O
machine	O
learning	O
methods	O
and	O
researchers	O
have	O
shown	O
how	O
it	O
can	O
sometimes	O
benefit	O
reinforcement	B
learning	I
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
looking	O
deeper	O
at	O
on-policy	O
learning	O
interest	B
and	I
emphasis	I
the	O
algorithms	O
we	O
have	O
considered	O
so	O
far	O
in	O
this	O
chapter	O
have	O
treated	O
all	O
the	O
states	O
encountered	O
equally	O
as	O
if	O
they	O
were	O
all	O
equally	O
important	O
in	O
some	O
cases	O
however	O
we	O
are	O
more	O
interested	O
in	O
some	O
states	O
than	O
others	O
in	O
discounted	O
episodic	O
problems	O
for	O
example	O
we	O
may	O
be	O
more	O
interested	O
in	O
accurately	O
valuing	O
early	O
states	O
in	O
the	O
episode	O
than	O
in	O
later	O
states	O
where	O
discounting	B
may	O
have	O
made	O
the	O
rewards	O
much	O
less	O
important	O
to	O
the	O
value	B
of	O
the	O
start	O
state	B
or	O
if	O
an	O
action-value	O
function	O
is	O
being	O
learned	O
it	O
may	O
be	O
less	O
important	O
to	O
accurately	O
value	B
poor	O
actions	O
whose	O
value	B
is	O
much	O
less	O
than	O
the	O
greedy	O
action	B
function	B
approximation	I
resources	O
are	O
always	O
limited	O
and	O
if	O
they	O
were	O
used	O
in	O
a	O
more	O
targeted	O
way	O
then	O
performance	O
could	O
be	O
improved	O
one	O
reason	O
we	O
have	O
treated	O
all	O
states	O
encountered	O
equally	O
is	O
that	O
then	O
we	O
are	O
updating	O
according	O
to	O
the	O
on-policy	B
distribution	I
for	O
which	O
stronger	O
theoretical	O
results	O
are	O
available	O
for	O
semi-gradient	B
methods	I
recall	O
that	O
the	O
on-policy	B
distribution	I
was	O
defined	O
as	O
the	O
distribution	O
of	O
states	O
encountered	O
in	O
an	O
mdp	O
while	O
following	O
the	O
target	O
policy	B
now	O
we	O
will	O
generalize	O
this	O
concept	O
significantly	O
rather	O
than	O
having	O
one	O
on-policy	B
distribution	I
for	O
the	O
mdp	O
we	O
will	O
have	O
many	O
all	O
of	O
them	O
will	O
have	O
in	O
common	O
that	O
they	O
are	O
a	O
distribution	O
of	O
states	O
encountered	O
in	O
trajectories	O
while	O
following	O
the	O
target	O
policy	B
but	O
they	O
will	O
vary	O
in	O
how	O
the	O
trajectories	O
are	O
in	O
a	O
sense	O
initiated	O
we	O
now	O
introduce	O
some	O
new	O
concepts	O
first	O
we	O
introduce	O
a	O
non-negative	O
scalar	O
measure	O
a	O
random	O
variable	O
it	O
called	O
interest	O
indicating	O
the	O
degree	O
to	O
which	O
we	O
are	O
interested	O
in	O
accurately	O
valuing	O
the	O
state	B
state	B
action	B
pair	O
at	O
time	O
t	O
if	O
we	O
don	O
t	O
care	O
at	O
all	O
about	O
the	O
state	B
then	O
the	O
interest	O
should	O
be	O
zero	O
if	O
we	O
fully	O
care	O
it	O
might	O
be	O
one	O
though	O
it	O
is	O
formally	O
allowed	O
take	O
any	O
non-negative	O
value	B
the	O
interest	O
can	O
be	O
set	O
in	O
any	O
causal	O
way	O
for	O
example	O
it	O
may	O
depend	O
on	O
the	O
trajectory	O
up	O
to	O
time	O
t	O
or	O
the	O
learned	O
parameters	O
at	O
time	O
t	O
the	O
distribution	O
in	O
the	O
ve	O
is	O
then	O
defined	O
as	O
the	O
distribution	O
of	O
states	O
encountered	O
while	O
following	O
the	O
target	O
policy	B
weighted	O
by	O
the	O
interest	O
second	O
we	O
introduce	O
another	O
non-negative	O
scalar	O
random	O
variable	O
the	O
emphasis	O
mt	O
this	O
scalar	O
multiplies	O
the	O
learning	O
update	O
and	O
thus	O
emphasizes	O
or	O
de-emphasizes	O
the	O
learning	O
done	O
at	O
time	O
t	O
the	O
general	O
n-step	B
learning	O
rule	O
replacing	B
is	O
wtn	O
wtn	O
mt	O
vstwtn	O
vstwtn	O
t	O
t	O
with	O
the	O
n-step	B
return	B
given	O
by	O
and	O
the	O
emphasis	O
determined	O
recursively	O
from	O
the	O
interest	O
by	O
mt	O
it	O
nmt	O
n	O
t	O
t	O
with	O
mt	O
for	O
all	O
t	O
these	O
equations	O
are	O
taken	O
to	O
include	O
the	O
monte	B
carlo	I
case	O
for	O
which	O
gttn	O
gt	O
all	O
the	O
updates	O
are	O
made	O
at	O
end	O
of	O
the	O
episode	O
n	O
t	O
t	O
and	O
mt	O
it	O
summary	O
example	O
illustrates	O
how	O
interest	B
and	I
emphasis	I
can	O
result	O
in	O
more	O
accurate	O
value	B
estimates	O
example	O
interest	B
and	I
emphasis	I
to	O
see	O
the	O
potential	O
benefits	O
of	O
using	O
interest	B
and	I
emphasis	I
consider	O
the	O
four-state	O
markov	O
reward	O
process	O
shown	O
below	O
episodes	B
start	O
in	O
the	O
leftmost	O
state	B
then	O
transition	O
one	O
state	B
to	O
the	O
right	O
with	O
a	O
reward	O
of	O
on	O
each	O
step	O
until	O
the	O
terminal	O
state	B
is	O
reached	O
the	O
true	O
value	B
of	O
the	O
first	O
state	B
is	O
thus	O
of	O
the	O
second	O
state	B
and	O
so	O
on	O
as	O
shown	O
below	O
each	O
state	B
these	O
are	O
the	O
true	O
values	O
the	O
estimated	O
values	O
can	O
only	O
approximate	O
these	O
because	O
they	O
are	O
constrained	O
by	O
the	O
parameterization	O
there	O
are	O
two	O
components	O
to	O
the	O
parameter	O
vector	B
w	O
and	O
the	O
parameterization	O
is	O
as	O
written	O
inside	O
each	O
state	B
the	O
estimated	O
values	O
of	O
the	O
first	O
two	O
states	O
are	O
given	O
by	O
alone	O
and	O
thus	O
must	O
be	O
the	O
same	O
even	O
though	O
their	O
true	O
values	O
are	O
different	O
similarly	O
the	O
estimated	O
values	O
of	O
the	O
third	O
and	O
fourth	O
states	O
are	O
given	O
by	O
alone	O
and	O
must	O
be	O
the	O
same	O
even	O
though	O
their	O
true	O
values	O
are	O
different	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
accurately	O
valuing	O
only	O
the	O
leftmost	O
state	B
we	O
assign	O
it	O
an	O
interest	O
of	O
while	O
all	O
the	O
other	O
states	O
are	O
assigned	O
an	O
interest	O
of	O
as	O
indicated	O
above	O
the	O
states	O
first	O
consider	O
applying	O
gradient	B
monte	B
carlo	I
algorithms	O
to	O
this	O
problem	O
the	O
algorithms	O
presented	O
earlier	O
in	O
this	O
chapter	O
that	O
do	O
not	O
take	O
into	O
account	O
interest	B
and	I
emphasis	I
and	O
the	O
box	O
on	O
page	O
will	O
converge	O
decreasing	O
step	O
sizes	O
to	O
the	O
parameter	O
vector	B
w	O
which	O
gives	O
the	O
first	O
state	B
the	O
only	O
one	O
we	O
are	O
interested	O
in	O
a	O
value	B
of	O
intermediate	O
between	O
the	O
true	O
values	O
of	O
the	O
first	O
and	O
second	O
states	O
the	O
methods	O
presented	O
in	O
this	O
section	O
that	O
do	O
use	O
interest	B
and	I
emphasis	I
on	O
the	O
other	O
hand	O
will	O
learn	O
the	O
value	B
of	O
the	O
first	O
state	B
exactly	O
correctly	O
will	O
converge	O
to	O
while	O
will	O
never	O
be	O
updated	O
because	O
the	O
emphasis	O
is	O
zero	O
in	O
all	O
states	O
save	O
the	O
leftmost	O
now	O
consider	O
applying	O
two-step	O
semi-gradient	O
td	B
methods	O
the	O
methods	O
from	O
earlier	O
in	O
this	O
chapter	O
without	O
interest	B
and	I
emphasis	I
and	O
and	O
the	O
box	O
on	O
page	O
will	O
again	O
converge	O
to	O
w	O
while	O
the	O
methods	O
with	O
interest	B
and	I
emphasis	I
converge	O
to	O
w	O
the	O
latter	O
produces	O
the	O
exactly	O
correct	O
values	O
for	O
the	O
first	O
state	B
and	O
for	O
the	O
third	O
state	B
the	O
first	O
state	B
bootstraps	O
from	O
while	O
never	O
making	O
any	O
updates	O
corresponding	O
to	O
the	O
second	O
or	O
fourth	O
states	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
summary	O
reinforcement	B
learning	I
systems	O
must	O
be	O
capable	O
of	O
generalization	O
if	O
they	O
are	O
to	O
be	O
applicable	O
to	O
artificial	B
intelligence	I
or	O
to	O
large	O
engineering	O
applications	O
to	O
achieve	O
this	O
any	O
of	O
a	O
broad	O
range	O
of	O
existing	O
methods	O
for	O
supervised-learning	O
function	B
approximation	I
can	O
be	O
used	O
simply	O
by	O
treating	O
each	O
update	O
as	O
a	O
training	O
example	O
perhaps	O
the	O
most	O
suitable	O
supervised	B
learning	I
methods	O
are	O
those	O
using	O
parameterized	O
function	B
approximation	I
in	O
which	O
the	O
policy	B
is	O
parameterized	O
by	O
a	O
weight	O
vector	B
w	O
although	O
the	O
weight	O
vector	B
has	O
many	O
components	O
the	O
state	B
space	O
is	O
much	O
larger	O
still	O
and	O
we	O
must	O
settle	O
for	O
an	O
approximate	O
solution	O
we	O
defined	O
the	O
mean	O
squared	O
value	B
error	I
vew	O
as	O
a	O
measure	O
of	O
the	O
error	O
in	O
the	O
values	O
v	O
w	O
for	O
a	O
weight	O
vector	B
w	O
under	O
the	O
on-policy	B
distribution	I
the	O
ve	O
gives	O
us	O
a	O
clear	O
way	O
to	O
rank	O
different	O
value-function	O
approximations	O
in	O
the	O
on-policy	O
case	O
to	O
find	O
a	O
good	O
weight	O
vector	B
the	O
most	O
popular	O
methods	O
are	O
variations	O
of	O
stochastic	O
gradient	B
descent	O
in	O
this	O
chapter	O
we	O
have	O
focused	O
on	O
the	O
on-policy	O
case	O
with	O
a	O
fixed	O
policy	B
also	O
known	O
as	O
policy	B
evaluation	O
or	O
prediction	B
a	O
natural	O
learning	O
algorithm	O
for	O
this	O
case	O
is	O
n-step	B
semi-gradient	O
td	B
which	O
includes	O
gradient	B
monte	B
carlo	I
and	O
semi-gradient	O
algorithms	O
as	O
the	O
special	O
cases	O
when	O
n	O
and	O
n	O
respectively	O
semi-gradient	O
td	B
methods	O
are	O
not	O
true	O
gradient	B
methods	O
in	O
such	O
bootstrapping	B
methods	O
dp	O
the	O
weight	O
vector	B
appears	O
in	O
the	O
update	O
target	O
yet	O
this	O
is	O
not	O
taken	O
into	O
account	O
in	O
computing	O
the	O
gradient	B
thus	O
they	O
are	O
semi	O
methods	O
as	O
such	O
they	O
cannot	O
rely	O
on	O
classical	O
sgd	O
results	O
nevertheless	O
good	O
results	O
can	O
be	O
obtained	O
for	O
semi-gradient	B
methods	I
in	O
the	O
special	O
case	O
of	O
linear	O
function	B
approximation	I
in	O
which	O
the	O
value	B
estimates	O
are	O
sums	O
of	O
features	O
times	O
corresponding	O
weights	O
the	O
linear	O
case	O
is	O
the	O
most	O
well	O
understood	O
theoretically	O
and	O
works	O
well	O
in	O
practice	O
when	O
provided	O
with	O
appropriate	O
features	O
choosing	O
the	O
features	O
is	O
one	O
of	O
the	O
most	O
important	O
ways	O
of	O
adding	O
prior	O
domain	O
knowledge	O
to	O
reinforcement	B
learning	I
systems	O
they	O
can	O
be	O
chosen	O
as	O
polynomials	O
but	O
this	O
case	O
generalizes	O
poorly	O
in	O
the	O
online	B
learning	O
setting	O
typically	O
considered	O
in	O
reinforcement	B
learning	I
better	O
is	O
to	O
choose	O
features	O
according	O
the	O
fourier	B
basis	I
or	O
according	O
to	O
some	O
form	O
of	O
coarse	B
coding	I
with	O
sparse	B
overlapping	O
receptive	O
fields	O
tile	B
coding	I
is	O
a	O
form	O
of	O
coarse	B
coding	I
that	O
is	O
particularly	O
computationally	O
efficient	O
and	O
flexible	O
radial	O
basis	O
functions	O
are	O
useful	O
for	O
one-	O
or	O
two-dimensional	O
tasks	O
in	O
which	O
a	O
smoothly	O
varying	O
response	O
is	O
important	O
lstd	O
is	O
the	O
most	O
data-efficient	O
linear	O
td	B
prediction	B
method	O
but	O
requires	O
computation	O
proportional	O
to	O
the	O
square	O
of	O
the	O
number	O
of	O
weights	O
whereas	O
all	O
the	O
other	O
methods	O
are	O
of	O
complexity	O
linear	O
in	O
the	O
number	O
of	O
weights	O
nonlinear	O
methods	O
include	O
artificial	B
neural	B
networks	I
trained	O
by	O
backpropagation	B
and	O
variations	O
of	O
sgd	O
these	O
methods	O
have	O
become	O
very	O
popular	O
in	O
recent	O
years	O
under	O
the	O
name	O
deep	B
reinforcement	B
learning	I
linear	O
semi-gradient	O
n-step	B
td	B
is	O
guaranteed	O
to	O
converge	O
under	O
standard	O
conditions	O
for	O
all	O
n	O
to	O
a	O
ve	O
that	O
is	O
within	O
a	O
bound	O
of	O
the	O
optimal	O
error	O
asymptotically	O
by	O
monte	B
carlo	I
methods	I
this	O
bound	O
is	O
always	O
tighter	O
for	O
higher	O
n	O
and	O
approaches	O
zero	O
as	O
n	O
however	O
in	O
practice	O
very	O
high	O
n	O
results	O
in	O
very	O
slow	O
learning	O
and	O
some	O
degree	O
of	O
bootstrapping	B
n	O
is	O
usually	O
preferrable	O
just	O
as	O
we	O
saw	O
in	O
comparisons	O
of	O
tabular	O
n-step	B
methods	I
in	O
chapter	O
and	O
in	O
comparisons	O
of	O
tabular	O
td	B
and	B
monte	B
carlo	I
methods	I
in	O
chapter	O
summary	O
bibliographical	O
and	O
historical	O
remarks	O
generalization	O
and	B
function	B
approximation	I
have	O
always	O
been	O
an	O
integral	O
part	O
of	O
reinforcement	B
learning	I
bertsekas	O
and	O
tsitsiklis	O
bertsekas	O
and	O
sugiyama	O
et	O
al	O
present	O
the	O
state	B
of	O
the	O
art	O
in	O
function	B
approximation	I
in	O
reinforcement	B
learning	I
some	O
of	O
the	O
early	O
work	O
with	B
function	B
approximation	I
in	O
reinforcement	B
learning	I
is	O
discussed	O
at	O
the	O
end	O
of	O
this	O
section	O
gradient-descent	O
methods	O
for	O
minimizing	O
mean-squared	O
error	O
in	O
supervised	B
learning	I
are	O
well	O
known	O
widrow	O
and	O
hoff	O
introduced	O
the	O
least-mean-square	O
algorithm	O
which	O
is	O
the	O
prototypical	O
incremental	O
gradient-descent	O
algorithm	O
details	O
of	O
this	O
and	O
related	O
algorithms	O
are	O
provided	O
in	O
many	O
texts	O
widrow	O
and	O
stearns	O
bishop	O
duda	O
and	O
hart	O
semi-gradient	O
was	O
first	O
explored	O
by	O
sutton	O
as	O
part	O
of	O
the	O
linear	O
td	B
algorithm	O
that	O
we	O
will	O
treat	O
in	O
chapter	O
the	O
term	O
semigradient	O
to	O
describe	O
these	O
bootstrapping	B
methods	O
is	O
new	O
to	O
the	O
second	O
edition	O
of	O
this	O
book	O
the	O
earliest	O
use	O
of	O
state	B
aggregation	I
in	O
reinforcement	B
learning	I
may	O
have	O
been	O
michie	B
and	O
chambers	O
s	O
boxes	B
system	O
the	O
theory	O
of	O
state	B
aggregation	I
in	O
reinforcement	B
learning	I
has	O
been	O
developed	O
by	O
singh	O
jaakkola	O
and	O
jordan	O
and	O
tsitsiklis	O
and	O
van	O
roy	O
state	B
aggregation	I
has	O
been	O
used	O
in	O
dynamic	B
programming	I
from	O
its	O
earliest	O
days	O
bellman	B
sutton	O
proved	O
convergence	O
of	O
linear	O
in	O
the	O
mean	O
to	O
the	O
minimal	O
ve	O
solution	O
for	O
the	O
case	O
in	O
which	O
the	O
feature	O
vectors	O
s	O
s	O
are	O
linearly	O
independent	O
convergence	O
with	O
probability	O
was	O
proved	O
by	O
several	O
researchers	O
at	O
about	O
the	O
same	O
time	O
dayan	O
and	O
sejnowski	O
tsitsiklis	O
gurvits	O
lin	O
and	O
hanson	O
in	O
addition	O
jaakkola	O
jordan	O
and	O
singh	O
proved	O
convergence	O
under	O
online	B
updating	O
all	O
of	O
these	O
results	O
assumed	O
linearly	O
independent	O
feature	O
vectors	O
which	O
implies	O
at	O
least	O
as	O
many	O
components	O
to	O
wt	O
as	O
there	O
are	O
states	O
convergence	O
for	O
the	O
more	O
important	O
case	O
of	O
general	O
feature	O
vectors	O
was	O
first	O
shown	O
by	O
dayan	O
a	O
significant	O
generalization	O
and	O
strengthening	O
of	O
dayan	O
s	O
result	O
was	O
proved	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
they	O
proved	O
the	O
main	O
result	O
presented	O
in	O
this	O
section	O
the	O
bound	O
on	O
the	O
asymptotic	O
error	O
of	O
linear	O
bootstrapping	B
methods	O
our	O
presentation	O
of	O
the	O
range	O
of	O
possibilities	O
for	O
linear	O
function	B
approximation	I
is	O
based	O
on	O
that	O
by	O
barto	O
konidaris	O
osentoski	O
and	O
thomas	O
introduced	O
the	O
fourier	B
basis	I
in	O
a	O
simple	O
form	O
suitable	O
for	O
reinforcement	B
learning	I
problems	O
with	O
multi-dimensional	O
continuous	B
state	B
spaces	O
and	O
functions	O
that	O
do	O
not	O
have	O
to	O
be	O
periodic	O
the	O
term	O
coarse	B
coding	I
is	O
due	O
to	O
hinton	O
and	O
our	O
figure	O
is	O
based	O
on	O
one	O
of	O
his	O
figures	O
waltz	O
and	O
fu	O
provide	O
an	O
early	O
example	O
of	O
this	O
type	O
of	O
function	B
approximation	I
in	O
a	O
reinforcement	B
learning	I
system	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
tile	B
coding	I
including	O
hashing	O
was	O
introduced	O
by	O
albus	O
he	O
described	O
it	O
in	O
terms	O
of	O
his	O
cerebellar	O
model	O
articulator	O
controller	O
or	O
cmac	O
as	O
tile	B
coding	I
is	O
sometimes	O
known	O
in	O
the	O
literature	O
the	O
term	O
tile	B
coding	I
was	O
new	O
to	O
the	O
first	O
edition	O
of	O
this	O
book	O
though	O
the	O
idea	O
of	O
describing	O
cmac	O
in	O
these	O
terms	O
is	O
taken	O
from	O
watkins	B
tile	B
coding	I
has	O
been	O
used	O
in	O
many	O
reinforcement	B
learning	I
systems	O
shewchuk	O
and	O
dean	O
lin	O
and	O
kim	O
miller	O
scalera	O
and	O
kim	O
sofge	O
and	O
white	O
tham	O
sutton	O
watkins	B
as	O
well	O
as	O
in	O
other	O
types	O
of	O
learning	O
control	B
systems	O
kraft	O
and	O
campagna	O
kraft	O
miller	O
and	O
dietz	O
this	O
section	O
draws	O
heavily	O
on	O
the	O
work	O
of	O
miller	O
and	O
glanz	O
general	O
software	O
for	O
tile	B
coding	I
is	O
available	O
on	O
the	O
web	O
in	O
several	O
languages	O
see	O
function	B
approximation	I
using	O
radial	O
basis	O
functions	O
has	O
received	O
wide	O
attention	O
ever	O
since	O
being	O
related	O
to	O
neural	B
networks	O
by	O
broomhead	O
and	O
lowe	O
powell	O
reviewed	O
earlier	O
uses	O
of	O
rbfs	O
and	O
poggio	O
and	O
girosi	O
extensively	O
developed	O
and	O
applied	O
this	O
approach	O
automatic	O
methods	O
for	O
adapting	O
the	O
step-size	B
parameter	I
include	O
rmsprop	O
and	O
hinton	O
adam	O
and	O
ba	O
stochastic	O
meta-descent	O
methods	O
such	O
as	O
delta-bar-delta	O
its	O
incremental	O
generalization	O
c	O
mahmood	O
et	O
al	O
and	O
nonlinear	O
generalizations	O
methods	O
explicitly	O
designed	O
for	O
reinforcement	B
learning	I
include	O
alphabound	O
and	O
barto	O
sid	O
and	O
nosid	O
tidbd	O
et	O
al	O
in	O
preparation	O
and	O
the	O
application	O
of	O
stochastic	O
metadescent	O
to	O
policy	B
gradient	B
learning	O
yu	O
and	O
aberdeen	O
the	O
introduction	O
of	O
the	O
threshold	O
logic	O
unit	O
as	O
an	O
abstract	O
model	O
neuron	O
by	O
mcculloch	O
and	O
pitts	O
was	O
the	O
beginning	O
of	O
artificial	B
neural	B
networks	I
the	O
history	B
of	I
anns	O
as	O
learning	O
methods	O
for	O
classification	O
or	O
regression	O
has	O
passed	O
through	O
several	O
stages	O
roughly	O
the	O
perceptron	O
and	O
adaline	O
linear	O
element	O
and	O
hoff	O
stage	O
of	O
learning	O
by	O
single-layer	O
anns	O
the	O
error-backpropagation	O
stage	O
rumelhart	O
hinton	O
and	O
williams	O
of	O
learning	O
by	O
multi-layer	O
anns	O
and	O
the	O
current	O
deep-learning	O
stage	O
with	O
its	O
emphasis	O
on	O
representation	B
learning	I
bengio	O
courville	O
and	O
vincent	O
goodfellow	O
bengio	O
and	O
courville	O
examples	O
of	O
the	O
many	O
books	O
on	O
anns	O
are	O
haykin	O
bishop	O
and	O
ripley	O
anns	O
as	O
function	B
approximation	I
for	O
reinforcement	B
learning	I
goes	O
back	O
to	O
the	O
early	O
neural	B
networks	O
of	O
farley	O
and	O
clark	O
who	O
used	O
reinforcement-like	O
learning	O
to	O
modify	O
the	O
weights	O
of	O
linear	O
threshold	O
functions	O
representing	O
policies	O
widrow	O
gupta	O
and	O
maitra	O
presented	O
a	O
neuron-like	O
linear	O
threshold	O
unit	O
implementing	O
a	O
learning	O
process	O
they	O
called	O
learning	O
with	O
a	O
critic	O
or	O
selective	B
bootstrap	I
adaptation	I
a	O
reinforcement-learning	O
variant	O
of	O
the	O
adaline	O
algorithm	O
werbos	B
developed	O
an	O
approach	O
to	O
prediction	B
and	B
control	B
summary	O
that	O
uses	O
anns	O
trained	O
by	O
error	O
backpropation	O
to	O
learn	O
policies	O
and	O
value	B
functions	O
using	O
td-like	O
algorithms	O
barto	O
sutton	O
and	O
brouwer	O
and	O
barto	O
and	O
sutton	O
extended	O
the	O
idea	O
of	O
an	O
associative	O
memory	O
network	O
kohonen	O
anderson	O
silverstein	O
ritz	O
and	O
jones	O
to	O
reinforcement	B
learning	I
barto	O
anderson	O
and	O
sutton	O
used	O
a	O
two-layer	O
ann	O
to	O
learn	O
a	O
nonlinear	O
control	B
policy	B
and	O
emphasized	O
the	O
first	O
layer	O
s	O
role	O
of	O
learning	O
a	O
suitable	O
representation	O
hampson	O
was	O
an	O
early	O
proponent	O
of	O
multilayer	O
anns	O
for	O
learning	O
value	B
functions	O
barto	O
sutton	O
and	O
anderson	O
presented	O
an	O
actor	O
critic	O
algorithm	O
in	O
the	O
form	O
of	O
an	O
ann	O
learning	O
to	O
balance	O
a	O
simulated	O
pole	O
sections	O
and	O
barto	O
and	O
anandan	O
introduced	O
a	O
stochastic	O
version	O
of	O
widrow	O
et	O
al	O
s	O
selective	O
bootstrap	O
algorithm	O
called	O
the	O
associative	O
reward-penalty	O
p	O
algorithm	O
barto	O
and	O
barto	O
and	O
jordan	O
described	O
multi-layer	O
anns	O
consisting	O
of	O
ar	O
p	O
units	O
trained	O
with	O
a	O
globally-broadcast	O
reinforcement	B
signal	I
to	O
learn	O
classification	O
rules	O
that	O
are	O
not	O
linearly	O
separable	O
barto	O
discussed	O
this	O
approach	O
to	O
anns	O
and	O
how	O
this	O
type	O
of	O
learning	O
rule	O
is	O
related	O
to	O
others	O
in	O
the	O
literature	O
at	O
that	O
time	O
section	O
for	O
additional	O
discussion	O
of	O
this	O
approach	O
to	O
training	O
multi-layer	O
anns	O
anderson	O
evaluated	O
numerous	O
methods	O
for	O
training	O
multilayer	O
anns	O
and	O
showed	O
that	O
an	O
actor	O
critic	O
algorithm	O
in	O
which	O
both	O
the	O
actor	O
and	O
critic	O
were	O
implemented	O
by	O
two-layer	O
anns	O
trained	O
by	O
error	O
backpropagation	B
outperformed	O
single-layer	O
anns	O
in	O
the	O
pole-balancing	O
and	O
tower	O
of	O
hanoi	O
tasks	O
williams	O
described	O
several	O
ways	O
that	O
backpropagation	B
and	B
reinforcement	B
learning	I
can	O
be	O
combined	O
for	O
training	O
anns	O
gullapalli	O
and	O
williams	O
devised	O
reinforcement	B
learning	I
algorithms	O
for	O
neuron-like	O
units	O
having	O
continuous	O
rather	O
than	O
binary	O
outputs	O
barto	O
sutton	O
and	O
watkins	B
argued	O
that	O
anns	O
can	O
play	O
significant	O
roles	O
for	O
approximating	O
functions	O
required	O
for	O
solving	O
sequential	O
decision	O
problems	O
williams	O
related	O
reinforce	B
learning	O
rules	O
to	O
the	O
error	O
backpropagation	B
method	O
for	O
training	O
multi-layer	O
anns	O
tesauro	O
s	O
td-gammon	B
section	O
influentially	O
demonstrated	O
the	O
learning	O
abilities	O
of	O
td	B
algorithm	O
with	B
function	B
approximation	I
by	O
multi-layer	O
anns	O
in	O
learning	O
to	O
play	O
backgammon	B
the	O
alphago	B
alphago	B
zero	O
and	O
alphazero	O
programs	O
of	O
silver	O
et	O
al	O
b	O
section	O
used	O
reinforcement	B
learning	I
with	O
deep	O
convolutional	O
anns	O
in	O
achieving	O
impressive	O
results	O
with	O
the	O
game	O
of	O
go	O
schmidhuber	O
reviews	O
applications	O
of	O
anns	O
in	O
reinforcement	B
learning	I
including	O
applications	O
of	O
recurrent	O
anns	O
lstd	O
is	O
due	O
to	O
bradtke	O
and	O
barto	O
bradtke	O
bradtke	O
and	O
barto	O
bradtke	O
ydstie	O
and	O
barto	O
and	O
was	O
further	O
developed	O
by	O
boyan	O
nedi	O
c	O
and	O
bertsekas	O
and	O
yu	O
the	O
incremental	O
update	O
of	O
the	O
inverse	O
matrix	O
has	O
been	O
known	O
at	O
least	O
since	O
and	O
morrison	O
an	O
extension	O
of	O
least-squares	O
methods	O
to	O
control	B
was	O
introduced	O
by	O
lagoudakis	O
and	O
parr	O
bu	O
soniu	O
lazaric	O
ghavamzadeh	O
munos	O
babu	O
ska	O
and	O
de	O
schutter	O
chapter	O
on-policy	O
prediction	B
with	B
approximation	I
our	O
discussion	O
of	O
memory-based	O
function	B
approximation	I
is	O
largely	O
based	O
on	O
the	O
review	O
of	O
locally	O
weighted	O
learning	O
by	O
atkeson	O
moore	O
and	O
schaal	O
atkeson	O
discussed	O
the	O
use	O
of	O
locally	O
weighted	O
regression	O
in	O
memory-based	O
robot	O
learning	O
and	O
supplied	O
an	O
extensive	O
bibliography	O
covering	O
the	O
history	B
of	I
the	O
idea	O
stanfill	O
and	O
waltz	O
influentially	O
argued	O
for	O
the	O
importance	O
of	O
memory	O
based	O
methods	O
in	O
artificial	B
intelligence	I
especially	O
in	O
light	O
of	O
parallel	O
architectures	O
then	O
becoming	O
available	O
such	O
as	O
the	O
connection	O
machine	O
baird	O
and	O
klopf	B
introduced	O
a	O
novel	O
memory-based	O
approach	O
and	O
used	O
it	O
as	O
the	O
function	B
approximation	I
method	O
for	B
q-learning	B
applied	O
to	O
the	O
pole-balancing	O
task	O
schaal	O
and	O
atkeson	O
applied	O
locally	O
weighted	O
regression	O
to	O
a	O
robot	O
juggling	O
control	B
problem	O
where	O
it	O
was	O
used	O
to	O
learn	O
a	O
system	O
model	O
peng	O
used	O
the	O
pole-balancing	O
task	O
to	O
experiment	O
with	O
several	O
nearest-neighbor	O
methods	O
for	O
approximating	O
value	B
functions	O
policies	O
and	O
environment	B
models	O
tadepalli	O
and	O
ok	O
obtained	O
promising	O
results	O
with	O
locally-weighted	O
linear	O
regression	O
to	O
learn	O
a	O
value	B
function	I
for	O
a	O
simulated	O
automatic	O
guided	O
vehicle	O
task	O
bottou	O
and	O
vapnik	O
demonstrated	O
surprising	O
efficiency	O
of	O
several	O
local	O
learning	O
algorithms	O
compared	O
to	O
non-local	O
algorithms	O
in	O
some	O
pattern	O
recognition	O
tasks	O
discussing	O
the	O
impact	O
of	O
local	O
learning	O
on	O
generalization	O
bentley	O
introduced	O
k-d	O
trees	O
and	O
reported	O
observing	O
average	O
running	O
time	O
of	O
olog	O
n	O
for	O
nearest	O
neighbor	O
search	O
over	O
n	O
records	O
friedman	O
bentley	O
and	O
finkel	O
clarified	O
the	O
algorithm	O
for	O
nearest	O
neighbor	O
search	O
with	O
kd	O
trees	O
omohundro	O
discussed	O
efficiency	O
gains	O
possible	O
with	O
hierarchical	O
data	O
structures	O
such	O
as	O
k-d-trees	O
moore	O
schneider	O
and	O
deng	O
introduced	O
the	O
use	O
of	O
k-d	O
trees	O
for	O
efficient	O
locally	O
weighted	O
regression	O
the	O
origin	O
of	O
kernel	O
regression	O
is	O
the	O
method	O
of	O
potential	O
functions	O
of	O
aizerman	O
braverman	O
and	O
rozonoer	O
they	O
likened	O
the	O
data	O
to	O
point	O
electric	O
charges	O
of	O
various	O
signs	O
and	O
magnitudes	O
distributed	O
over	O
space	O
the	O
resulting	O
electric	O
potential	O
over	O
space	O
produced	O
by	O
summing	O
the	O
potentials	O
of	O
the	O
point	O
charges	O
corresponded	O
to	O
the	O
interpolated	O
surface	O
in	O
this	O
analogy	O
the	O
kernel	O
function	O
is	O
the	O
potential	O
of	O
a	O
point	O
charge	O
which	O
falls	O
off	O
as	O
the	O
reciprocal	O
of	O
the	O
distance	O
from	O
the	O
charge	O
connell	O
and	O
utgoff	O
applied	O
an	O
actor	O
critic	O
method	O
to	O
the	O
pole-balancing	O
task	O
in	O
which	O
the	O
critic	O
approximated	O
the	O
value	B
function	I
using	O
kernel	O
regression	O
with	O
an	O
inverse-distance	O
weighting	O
predating	O
widespread	O
interest	O
in	O
kernel	O
regression	O
in	O
machine	O
learning	O
these	O
authors	O
did	O
not	O
use	O
the	O
term	O
kernel	O
but	O
referred	O
to	O
shepard	O
s	O
method	O
other	O
kernel-based	O
approaches	O
to	O
reinforcement	B
learning	I
include	O
those	O
of	O
ormoneit	O
and	O
sen	O
dietterich	O
and	O
wang	O
xu	O
xie	O
hu	O
and	O
lu	O
taylor	O
and	O
parr	O
barreto	O
precup	O
and	O
pineau	O
and	O
bhat	O
farias	O
and	O
moallemi	O
for	O
emphatic-td	B
methods	I
see	O
the	O
bibliographical	O
notes	O
to	O
section	O
summary	O
the	O
earliest	O
example	O
we	O
know	O
of	O
in	O
which	O
function	B
approximation	I
methods	O
were	O
used	O
for	O
learning	O
value	B
functions	O
was	O
samuel	O
s	O
checkers	O
player	O
samuel	O
followed	O
shannon	B
s	O
suggestion	O
that	O
a	O
value	B
function	I
did	O
not	O
have	O
to	O
be	O
exact	O
to	O
be	O
a	O
useful	O
guide	O
to	O
selecting	O
moves	O
in	O
a	O
game	O
and	O
that	O
it	O
might	O
be	O
approximated	O
by	O
linear	O
combination	O
of	O
features	O
in	O
addition	O
to	O
linear	O
function	B
approximation	I
samuel	O
experimented	O
with	O
lookup	O
tables	O
and	O
hierarchical	O
lookup	O
tables	O
called	O
signature	O
tables	O
page	O
biermann	O
fairfield	O
and	O
beres	O
at	O
about	O
the	O
same	O
time	O
as	O
samuel	O
s	O
work	O
bellman	B
and	O
dreyfus	O
proposed	O
using	O
function	B
approximation	I
methods	O
with	O
dp	O
is	O
tempting	O
to	O
think	O
that	O
bellman	B
and	O
samuel	O
had	O
some	O
influence	O
on	O
one	O
another	O
but	O
we	O
know	O
of	O
no	O
reference	O
to	O
the	O
other	O
in	O
the	O
work	O
of	O
either	O
there	O
is	O
now	O
a	O
fairly	O
extensive	O
literature	O
on	O
function	B
approximation	I
methods	O
and	O
dp	O
such	O
as	O
multigrid	O
methods	O
and	O
methods	O
using	O
splines	O
and	O
orthogonal	O
polynomials	O
bellman	B
and	O
dreyfus	O
bellman	B
kalaba	O
and	O
kotkin	O
daniel	O
whitt	O
reetz	O
schweitzer	O
and	O
seidmann	O
chow	O
and	O
tsitsiklis	O
kushner	O
and	O
dupuis	O
rust	O
holland	B
s	O
classifier	O
system	O
used	O
a	O
selective	O
feature-match	O
technique	O
to	O
generalize	O
evaluation	O
information	O
across	O
state	B
action	B
pairs	O
each	O
classifier	O
matched	O
a	O
subset	O
of	O
states	O
having	O
specified	O
values	O
for	O
a	O
subset	O
of	O
features	O
with	O
the	O
remaining	O
features	O
having	O
arbitrary	O
values	O
wild	O
cards	O
these	O
subsets	O
were	O
then	O
used	O
in	O
a	O
conventional	O
state-aggregation	O
approach	O
to	O
function	B
approximation	I
holland	B
s	O
idea	O
was	O
to	O
use	O
a	O
genetic	O
algorithm	O
to	O
evolve	O
a	O
set	O
of	O
classifiers	O
that	O
collectively	O
would	O
implement	O
a	O
useful	O
action-value	O
function	O
holland	B
s	O
ideas	O
influenced	O
the	O
early	O
research	O
of	O
the	O
authors	O
on	O
reinforcement	B
learning	I
but	O
we	O
focused	O
on	O
different	O
approaches	O
to	O
function	B
approximation	I
as	O
function	O
approximators	O
classifiers	O
are	O
limited	O
in	O
several	O
ways	O
first	O
they	O
are	O
state-aggregation	O
methods	O
with	O
concomitant	O
limitations	O
in	O
scaling	O
and	O
in	O
representing	O
smooth	O
functions	O
efficiently	O
in	O
addition	O
the	O
matching	O
rules	O
of	O
classifiers	O
can	O
implement	O
only	O
aggregation	O
boundaries	O
that	O
are	O
parallel	O
to	O
the	O
feature	O
axes	O
perhaps	O
the	O
most	O
important	O
limitation	O
of	O
conventional	O
classifier	B
systems	I
is	O
that	O
the	O
classifiers	O
are	O
learned	O
via	O
the	O
genetic	O
algorithm	O
an	O
evolutionary	O
method	O
as	O
we	O
discussed	O
in	O
chapter	O
there	O
is	O
available	O
during	O
learning	O
much	O
more	O
detailed	O
information	O
about	O
how	O
to	O
learn	O
than	O
can	O
be	O
used	O
by	O
evolutionary	B
methods	I
this	O
perspective	O
led	O
us	O
to	O
instead	O
adapt	O
supervised	B
learning	I
methods	O
for	O
use	O
in	O
reinforcement	B
learning	I
specifically	O
gradient-descent	O
and	O
neural	B
network	O
methods	O
these	O
differences	O
between	O
holland	B
s	O
approach	O
and	O
ours	O
are	O
not	O
surprising	O
because	O
holland	B
s	O
ideas	O
were	O
developed	O
during	O
a	O
period	O
when	O
neural	B
networks	O
were	O
generally	O
regarded	O
as	O
being	O
too	O
weak	O
in	O
computational	O
power	O
to	O
be	O
useful	O
whereas	O
our	O
work	O
was	O
at	O
the	O
beginning	O
of	O
the	O
period	O
that	O
saw	O
widespread	O
questioning	O
of	O
that	O
conventional	O
wisdom	O
there	O
remain	O
many	O
opportunities	O
for	O
combining	O
aspects	O
of	O
these	O
different	O
approaches	O
christensen	O
and	O
korf	O
experimented	O
with	O
regression	O
methods	O
for	O
modifying	O
coefficients	O
of	O
linear	O
value	B
function	I
approximations	O
in	O
the	O
game	O
of	O
chess	B
chapman	O
and	O
kaelbling	O
and	O
tan	O
adapted	O
decision-tree	O
methods	O
for	O
learning	O
value	B
functions	O
explanation-based	O
learning	O
methods	O
have	O
also	O
been	O
adapted	O
for	O
learning	O
value	B
functions	O
yielding	O
compact	O
representations	O
saxena	O
utgoff	O
and	O
barto	O
dietterich	O
and	O
flann	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
in	O
this	O
chapter	O
we	O
return	B
to	O
the	O
control	B
problem	O
now	O
with	O
parametric	O
approximation	O
of	O
the	O
action-value	O
function	O
qs	O
a	O
w	O
q	O
a	O
where	O
w	O
rd	O
is	O
a	O
finite-dimensional	O
weight	O
vector	B
we	O
continue	O
to	O
restrict	O
attention	O
to	O
the	O
on-policy	O
case	O
leaving	O
off-policy	B
methods	I
to	O
chapter	O
the	O
present	O
chapter	O
features	O
the	O
semi-gradient	O
sarsa	B
algorithm	O
the	O
natural	O
extension	O
of	O
semi-gradient	O
chapter	O
to	O
action	B
values	O
and	O
to	O
onpolicy	O
control	B
in	O
the	O
episodic	O
case	O
the	O
extension	O
is	O
straightforward	O
but	O
in	O
the	O
continuing	O
case	O
we	O
have	O
to	O
take	O
a	O
few	O
steps	O
backward	O
and	O
re-examine	O
how	O
we	O
have	O
used	O
discounting	B
to	O
define	O
an	O
optimal	O
policy	B
surprisingly	O
once	O
we	O
have	O
genuine	O
function	B
approximation	I
we	O
have	O
to	O
give	O
up	O
discounting	B
and	O
switch	O
to	O
a	O
new	O
average-reward	O
formulation	O
of	O
the	O
control	B
problem	O
with	O
new	O
differential	B
value	B
functions	O
starting	O
first	O
in	O
the	O
episodic	O
case	O
we	O
extend	O
the	O
function	B
approximation	I
ideas	O
presented	O
in	O
the	O
last	O
chapter	O
from	O
state	B
values	O
to	O
action	B
values	O
then	O
we	O
extend	O
them	O
to	O
control	B
following	O
the	O
general	O
pattern	O
of	O
on-policy	O
gpi	O
using	O
for	O
action	B
selection	O
we	O
show	O
results	O
for	O
n-step	B
linear	O
sarsa	B
on	O
the	O
mountain	O
car	O
problem	O
then	O
we	O
turn	O
to	O
the	O
continuing	O
case	O
and	O
repeat	O
the	O
development	O
of	O
these	O
ideas	O
for	O
the	O
average-reward	O
case	O
with	O
differential	B
values	O
episodic	O
semi-gradient	O
control	B
the	O
extension	O
of	O
the	O
semi-gradient	O
prediction	B
methods	O
of	O
chapter	O
to	O
action	B
values	O
is	O
straightforward	O
in	O
this	O
case	O
it	O
is	O
the	O
approximate	O
action-value	O
function	O
q	O
q	O
that	O
is	O
represented	O
as	O
a	O
parameterized	O
functional	O
form	O
with	O
weight	O
vector	B
w	O
whereas	O
before	O
we	O
considered	O
random	O
training	O
examples	O
of	O
the	O
form	O
st	O
ut	O
now	O
we	O
consider	O
examples	O
of	O
the	O
form	O
st	O
at	O
ut	O
the	O
update	O
target	O
ut	O
can	O
be	O
any	O
approximation	O
of	O
q	O
at	O
including	O
the	O
usual	O
backed-up	O
values	O
such	O
as	O
the	O
full	O
monte	B
carlo	I
return	B
or	O
any	O
of	O
the	O
n-step	B
sarsa	B
returns	O
the	O
general	O
gradient-descent	O
update	O
for	O
action-value	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
prediction	B
is	O
for	O
example	O
the	O
update	O
for	O
the	O
one-step	O
sarsa	B
method	O
is	O
wt	O
qst	O
at	O
qst	O
at	O
wt	O
wt	O
wt	O
qst	O
at	O
qst	O
at	O
wt	O
we	O
call	O
this	O
method	O
episodic	O
semi-gradient	O
one-step	O
sarsa	B
for	O
a	O
constant	O
policy	B
this	O
method	O
converges	O
in	O
the	O
same	O
way	O
that	O
does	O
with	O
the	O
same	O
kind	O
of	O
error	O
bound	O
to	O
form	O
control	B
methods	O
we	O
need	O
to	O
couple	O
such	O
action-value	O
prediction	B
methods	O
with	O
techniques	O
for	O
policy	B
improvement	I
and	O
action	B
selection	O
suitable	O
techniques	O
applicable	O
to	O
continuous	O
actions	O
or	O
to	O
actions	O
from	O
large	O
discrete	O
sets	O
are	O
a	O
topic	O
of	O
ongoing	O
research	O
with	O
as	O
yet	O
no	O
clear	O
resolution	O
on	O
the	O
other	O
hand	O
if	O
the	O
action	B
set	O
is	O
discrete	O
and	O
not	O
too	O
large	O
then	O
we	O
can	O
use	O
the	O
techniques	O
already	O
developed	O
in	O
previous	O
chapters	O
that	O
is	O
for	O
each	O
possible	O
action	B
a	O
available	O
in	O
the	O
current	O
state	B
st	O
we	O
can	O
compute	O
qst	O
a	O
wt	O
and	O
then	O
find	O
the	O
greedy	O
action	B
a	O
t	O
argmaxa	O
qst	O
a	O
wt	O
policy	B
improvement	I
is	O
then	O
done	O
the	O
on-policy	O
case	O
treated	O
in	O
this	O
chapter	O
by	O
changing	O
the	O
estimation	O
policy	B
to	O
a	O
soft	O
approximation	O
of	O
the	O
greedy	O
policy	B
such	O
as	O
the	O
policy	B
actions	O
are	O
selected	O
according	O
to	O
this	O
same	O
policy	B
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
episodic	O
semi-gradient	O
sarsa	B
for	O
estimating	O
q	O
q	O
input	O
a	O
differentiable	O
action-value	O
function	O
parameterization	O
q	O
s	O
a	O
rd	O
r	O
algorithm	O
parameters	O
step	O
size	O
small	O
initialize	O
value-function	O
weights	O
w	O
rd	O
arbitrarily	O
w	O
loop	O
for	O
each	O
episode	O
s	O
a	O
initial	O
state	B
and	O
action	B
of	O
episode	O
loop	O
for	O
each	O
step	O
of	O
episode	O
take	O
action	B
a	O
observe	O
r	O
if	O
is	O
terminal	O
go	O
to	O
next	O
episode	O
w	O
w	O
qs	O
a	O
qs	O
a	O
w	O
choose	O
as	O
a	O
function	O
of	O
w	O
w	O
w	O
w	O
qs	O
a	O
qs	O
a	O
w	O
s	O
a	O
example	O
mountain	O
car	O
task	O
consider	O
the	O
task	O
of	O
driving	O
an	O
underpowered	O
car	O
up	O
a	O
steep	O
mountain	O
road	O
as	O
suggested	O
by	O
the	O
diagram	O
in	O
the	O
upper	O
left	O
of	O
figure	O
the	O
difficulty	O
is	O
that	O
gravity	O
is	O
stronger	O
than	O
the	O
car	O
s	O
engine	O
and	O
even	O
at	O
full	O
throttle	O
the	O
car	O
cannot	O
accelerate	O
up	O
the	O
steep	O
slope	O
the	O
only	O
solution	O
is	O
to	O
first	O
move	O
away	O
from	O
episodic	O
semi-gradient	O
control	B
figure	O
the	O
mountain	O
car	O
task	O
left	O
panel	O
and	O
the	O
cost-to-go	O
function	O
maxa	O
qs	O
a	O
w	O
learned	O
during	O
one	O
run	O
the	O
goal	O
and	O
up	O
the	O
opposite	O
slope	O
on	O
the	O
left	O
then	O
by	O
applying	O
full	O
throttle	O
the	O
car	O
can	O
build	O
up	O
enough	O
inertia	O
to	O
carry	O
it	O
up	O
the	O
steep	O
slope	O
even	O
though	O
it	O
is	O
slowing	O
down	O
the	O
whole	O
way	O
this	O
is	O
a	O
simple	O
example	O
of	O
a	O
continuous	O
control	B
task	O
where	O
things	O
have	O
to	O
get	O
worse	O
in	O
a	O
sense	O
from	O
the	O
goal	O
before	O
they	O
can	O
get	O
better	O
many	O
control	B
methodologies	O
have	O
great	O
difficulties	O
with	O
tasks	O
of	O
this	O
kind	O
unless	O
explicitly	O
aided	O
by	O
a	O
human	O
designer	O
the	O
reward	O
in	O
this	O
problem	O
is	O
on	O
all	O
time	O
steps	O
until	O
the	O
car	O
moves	O
past	O
its	O
goal	O
position	O
at	O
the	O
top	O
of	O
the	O
mountain	O
which	O
ends	O
the	O
episode	O
there	O
are	O
three	O
possible	O
actions	O
full	O
throttle	O
forward	O
full	O
throttle	O
reverse	O
and	O
zero	O
throttle	O
the	O
car	O
moves	O
according	O
to	O
a	O
simplified	O
physics	O
its	O
position	O
xt	O
and	O
velocity	O
xt	O
are	O
updated	O
by	O
xt	O
where	O
the	O
bound	O
operation	O
enforces	O
and	O
in	O
addition	O
when	O
reached	O
the	O
left	O
bound	O
was	O
reset	O
to	O
zero	O
when	O
it	O
reached	O
the	O
right	O
bound	O
the	O
goal	O
was	O
reached	O
and	O
the	O
episode	O
was	O
terminated	O
each	O
episode	O
started	O
from	O
a	O
random	O
position	O
xt	O
and	O
zero	O
velocity	O
to	O
convert	O
the	O
two	O
continuous	B
state	B
variables	O
to	O
binary	B
features	I
we	O
used	O
grid-tilings	O
as	O
in	O
figure	O
we	O
used	O
tilings	O
with	O
each	O
tile	O
covering	O
of	O
the	O
bounded	O
distance	O
in	O
each	O
dimension	O
and	O
cargoal	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
asymmetrical	O
offsets	O
as	O
described	O
in	O
section	O
the	O
feature	O
vectors	O
xs	O
a	O
created	O
by	O
tile	B
coding	I
were	O
then	O
combined	O
linearly	O
with	O
the	O
parameter	O
vector	B
to	O
approximate	O
the	O
action-value	O
function	O
qs	O
a	O
w	O
a	O
wi	O
xis	O
a	O
for	O
each	O
pair	O
of	O
state	B
s	O
and	O
action	B
a	O
figure	O
shows	O
what	O
typically	O
happens	O
while	O
learning	O
to	O
solve	O
this	O
task	O
with	O
this	O
form	O
of	O
function	O
shown	O
is	O
the	O
negative	O
of	O
the	O
value	B
function	I
costto-go	O
function	O
learned	O
on	O
a	O
single	O
run	O
the	O
initial	O
action	B
values	O
were	O
all	O
zero	O
which	O
was	O
optimistic	O
true	O
values	O
are	O
negative	O
in	O
this	O
task	O
causing	O
extensive	O
exploration	O
to	O
occur	O
even	O
though	O
the	O
exploration	O
parameter	O
was	O
this	O
can	O
be	O
seen	O
in	O
the	O
middle-top	O
panel	O
of	O
the	O
figure	O
labeled	O
step	O
at	O
this	O
time	O
not	O
even	O
one	O
episode	O
had	O
been	O
completed	O
but	O
the	O
car	O
has	O
oscillated	O
back	O
and	O
forth	O
in	O
the	O
valley	O
following	O
circular	O
trajectories	O
in	O
state	B
space	O
all	O
the	O
states	O
visited	O
frequently	O
are	O
valued	O
worse	O
than	O
unexplored	O
states	O
because	O
the	O
actual	O
rewards	O
have	O
been	O
worse	O
than	O
what	O
was	O
expected	B
this	O
continually	O
drives	O
the	O
agent	O
away	O
from	O
wherever	O
it	O
has	O
been	O
to	O
explore	O
new	O
states	O
until	O
a	O
solution	O
is	O
found	O
figure	O
shows	O
several	O
learning	O
curves	O
for	O
semi-gradient	O
sarsa	B
on	O
this	O
problem	O
with	O
various	O
step	O
sizes	O
figure	O
mountain	O
car	O
learning	O
curves	O
for	O
the	O
semi-gradient	O
sarsa	B
method	O
with	O
tile-coding	O
function	B
approximation	I
and	O
action	B
selection	O
particular	O
we	O
used	O
the	O
tile-coding	O
software	O
available	O
at	O
httpincompleteideas	O
nettiles	O
with	O
and	O
to	O
get	O
the	O
indices	O
of	O
the	O
ones	O
in	O
the	O
feature	O
vector	B
for	O
state	B
xdot	O
and	O
action	B
a	O
data	O
is	O
actually	O
from	O
the	O
semi-gradient	O
sarsa	B
algorithm	O
that	O
we	O
will	O
not	O
meet	O
until	O
chapter	O
but	O
semi-gradient	O
sarsa	B
would	O
behave	O
similarly	O
carsteps	O
per	O
episodelog	O
scaleaveraged	O
over	O
semi-gradient	O
n-step	B
sarsa	B
semi-gradient	O
n-step	B
sarsa	B
we	O
can	O
obtain	O
an	O
n-step	B
version	O
of	O
episodic	O
semi-gradient	O
sarsa	B
by	O
using	O
an	O
n-step	B
return	B
as	O
the	O
update	O
target	O
in	O
the	O
semi-gradient	O
sarsa	B
update	O
equation	O
the	O
n-step	B
return	B
immediately	O
generalizes	O
from	O
its	O
tabular	O
form	O
to	O
a	O
function	B
approximation	I
form	O
gttn	O
with	O
gttn	O
n	O
n	O
qstn	O
atn	O
wtn	O
gt	O
if	O
t	O
n	O
t	O
as	O
usual	O
the	O
n-step	B
update	O
equation	O
is	O
wtn	O
qst	O
at	O
wtn	O
qst	O
at	O
wtn	O
wtn	O
tn	O
t	O
t	O
t	O
complete	O
pseudocode	O
is	O
given	O
in	O
the	O
box	O
below	O
episodic	O
semi-gradient	O
n-step	B
sarsa	B
for	O
estimating	O
q	O
q	O
or	O
q	O
input	O
a	O
differentiable	O
action-value	O
function	O
parameterization	O
q	O
s	O
a	O
rd	O
r	O
input	O
a	O
policy	B
estimating	O
q	O
algorithm	O
parameters	O
step	O
size	O
small	O
a	O
positive	O
integer	O
n	O
initialize	O
value-function	O
weights	O
w	O
rd	O
arbitrarily	O
w	O
all	O
store	O
and	O
access	O
operations	O
at	O
and	O
rt	O
can	O
take	O
their	O
index	O
mod	O
n	O
loop	O
for	O
each	O
episode	O
if	O
t	O
t	O
then	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
and	O
the	O
next	O
state	B
as	O
if	O
is	O
terminal	O
then	O
initialize	O
and	O
store	O
terminal	O
select	O
and	O
store	O
an	O
action	B
or	O
wrt	O
w	O
t	O
loop	O
for	O
t	O
until	O
t	O
if	O
n	O
t	O
then	O
g	O
g	O
n	O
qs	O
a	O
w	O
w	O
w	O
qs	O
a	O
w	O
qs	O
a	O
w	O
g	O
t	O
t	O
select	O
and	O
store	O
or	O
wrt	O
w	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
t	O
n	O
if	O
i	O
i	O
else	O
as	O
we	O
have	O
seen	O
before	O
performance	O
is	O
best	O
if	O
an	O
intermediate	O
level	O
of	O
bootstrapping	B
is	O
used	O
corresponding	O
to	O
an	O
n	O
larger	O
than	O
figure	O
shows	O
how	O
this	O
algorithm	O
tends	O
to	O
learn	O
faster	O
and	O
obtain	O
a	O
better	O
asymptotic	O
performance	O
at	O
n	O
than	O
at	O
n	O
on	O
the	O
mountain	O
car	O
task	O
figure	O
shows	O
the	O
results	O
of	O
a	O
more	O
detailed	O
study	O
of	O
the	O
effect	O
of	O
the	O
parameters	O
and	O
n	O
on	O
the	O
rate	O
of	O
learning	O
on	O
this	O
task	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
figure	O
performance	O
of	O
one-step	O
vs	O
semi-gradient	O
sarsa	B
on	O
the	O
mountain	O
car	O
task	O
good	O
step	O
sizes	O
were	O
used	O
for	O
n	O
and	O
for	O
n	O
figure	O
effect	O
of	O
the	O
and	O
n	O
on	O
early	O
performance	O
of	O
n-step	B
semi-gradient	O
sarsa	B
and	O
tile-coding	O
function	B
approximation	I
on	O
the	O
mountain	O
car	O
task	O
as	O
usual	O
an	O
intermediate	O
level	O
of	O
bootstrapping	B
performed	O
best	O
these	O
results	O
are	O
for	O
selected	O
values	O
on	O
a	O
log	O
scale	O
and	O
then	O
connected	O
by	O
straight	O
lines	O
the	O
standard	O
errors	O
ranged	O
from	O
than	O
the	O
line	O
width	O
for	O
n	O
to	O
about	O
for	O
n	O
so	O
the	O
main	O
effects	O
are	O
all	O
statistically	O
significant	O
exercise	O
we	O
not	O
explicitly	O
considered	O
or	O
given	O
pseudocode	O
for	O
any	O
monte	B
carlo	I
methods	I
or	O
in	O
this	O
chapter	O
what	O
would	O
they	O
be	O
like	O
why	O
is	O
it	O
reasonable	O
not	O
to	O
give	O
pseudocode	O
for	O
them	O
how	O
would	O
they	O
perform	O
on	O
the	O
mountain	O
car	O
task	O
exercise	O
give	O
pseudocode	O
for	O
semi-gradient	O
one-step	O
expected	B
sarsa	B
for	O
control	B
exercise	O
why	O
do	O
the	O
results	O
shown	O
in	O
figure	O
have	O
higher	O
standard	O
errors	O
at	O
large	O
n	O
than	O
at	O
small	O
n	O
carsteps	O
per	O
episodelog	O
scaleaveraged	O
over	O
carsteps	O
per	O
episodeaveraged	O
overfirst	O
episodesand	O
runs	O
number	O
of	O
tilings	O
average	O
reward	O
a	O
new	O
problem	O
setting	O
for	O
continuing	B
tasks	I
average	O
reward	O
a	O
new	O
problem	O
setting	O
for	O
continuing	B
tasks	I
we	O
now	O
introduce	O
a	O
third	O
classical	O
setting	O
alongside	O
the	O
episodic	O
and	O
discounted	O
settings	O
for	O
formulating	O
the	O
goal	O
in	O
markov	O
decision	O
problems	O
like	O
the	O
discounted	O
setting	O
the	O
average	B
reward	I
setting	I
applies	O
to	O
continuing	O
problems	O
problems	O
for	O
which	O
the	O
interaction	O
between	O
agent	O
and	O
environment	B
goes	O
on	O
and	O
on	O
forever	O
without	O
termination	O
or	O
start	O
states	O
unlike	O
that	O
setting	O
however	O
there	O
is	O
no	O
discounting	B
the	O
agent	O
cares	O
just	O
as	O
much	O
about	O
delayed	O
rewards	O
as	O
it	O
does	O
about	O
immediate	O
reward	O
the	O
averagereward	O
setting	O
is	O
one	O
of	O
the	O
major	O
settings	O
commonly	O
considered	O
in	O
the	O
classical	O
theory	O
of	O
dynamic	B
programming	I
and	O
less-commonly	O
in	O
reinforcement	B
learning	I
as	O
we	O
discuss	O
in	O
the	O
next	O
section	O
the	O
discounted	O
setting	O
is	O
problematic	O
with	B
function	B
approximation	I
and	O
thus	O
the	O
average-reward	O
setting	O
is	O
needed	O
to	O
replace	O
it	O
in	O
the	O
average-reward	O
setting	O
the	O
quality	O
of	O
a	O
policy	B
is	O
defined	O
as	O
the	O
average	O
rate	O
of	O
reward	O
or	O
simply	O
average	O
reward	O
while	O
following	O
that	O
policy	B
which	O
we	O
denote	O
as	O
r	O
r	O
lim	O
h	O
lim	O
t	O
ert	O
h	O
ert	O
rs	O
ar	O
where	O
the	O
expectations	O
are	O
conditioned	O
on	O
the	O
prior	O
actions	O
at	O
being	O
limt	O
prst	O
s	O
taken	O
according	O
to	O
and	O
is	O
the	O
steady-state	O
distribution	O
which	O
is	O
assumed	O
to	O
exist	O
for	O
any	O
and	O
to	O
be	O
independent	O
of	O
this	O
assumption	O
about	O
the	O
mdp	O
is	O
known	O
as	O
ergodicity	O
it	O
means	O
that	O
where	O
the	O
mdp	O
starts	O
or	O
any	O
early	O
decision	O
made	O
by	O
the	O
agent	O
can	O
have	O
only	O
a	O
temporary	O
effect	O
in	O
the	O
long	O
run	O
the	O
expectation	O
of	O
being	O
in	O
a	O
state	B
depends	O
only	O
on	O
the	O
policy	B
and	O
the	O
mdp	O
transition	B
probabilities	I
ergodicity	O
is	O
sufficient	O
to	O
guarantee	O
the	O
existence	O
of	O
the	O
limits	O
in	O
the	O
equations	O
above	O
there	O
are	O
subtle	O
distinctions	O
that	O
can	O
be	O
drawn	O
between	O
different	O
kinds	O
of	O
optimality	O
in	O
the	O
undiscounted	O
continuing	O
case	O
nevertheless	O
for	O
most	O
practical	O
purposes	O
it	O
may	O
be	O
adequate	O
simply	O
to	O
order	O
policies	O
according	O
to	O
their	O
average	O
reward	O
per	O
time	O
step	O
in	O
other	O
words	O
according	O
to	O
their	O
r	O
this	O
quantity	O
is	O
essentially	O
the	O
average	O
reward	O
under	O
as	O
suggested	O
by	O
in	O
particular	O
we	O
consider	O
all	O
policies	O
that	O
attain	O
the	O
maximal	O
value	B
of	O
r	O
to	O
be	O
optimal	O
note	O
that	O
the	O
steady	O
state	B
distribution	O
is	O
the	O
special	O
distribution	O
under	O
which	O
if	O
you	O
select	O
actions	O
according	O
to	O
you	O
remain	O
in	O
the	O
same	O
distribution	O
that	O
is	O
for	O
which	O
a	O
in	O
the	O
average-reward	O
setting	O
returns	O
are	O
defined	O
in	O
terms	O
of	O
differences	O
between	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
rewards	O
and	O
the	O
average	O
reward	O
gt	O
r	O
r	O
r	O
this	O
is	O
known	O
as	O
the	O
differential	B
return	B
and	O
the	O
corresponding	O
value	B
functions	O
are	O
known	O
as	O
differential	B
value	B
functions	O
they	O
are	O
defined	O
in	O
the	O
same	O
way	O
and	O
we	O
will	O
use	O
the	O
same	O
notation	O
for	O
them	O
as	O
we	O
have	O
all	O
along	O
v	O
e	O
s	O
at	O
a	O
for	O
v	O
and	O
q	O
differential	B
value	B
functions	O
also	O
have	O
bellman	B
equations	O
just	O
slightly	O
different	O
from	O
those	O
we	O
have	O
seen	O
earlier	O
we	O
simply	O
remove	O
all	O
s	O
and	O
replace	O
all	O
rewards	O
by	O
the	O
difference	O
between	O
the	O
reward	O
and	O
the	O
true	O
average	O
reward	O
e	O
s	O
and	O
q	O
a	O
rs	O
r	O
v	O
v	O
rs	O
r	O
q	O
a	O
rs	O
max	O
a	O
rs	O
max	O
q	O
a	O
v	O
max	O
r	O
v	O
and	O
q	O
r	O
max	O
exercise	O
and	O
there	O
is	O
also	O
a	O
differential	B
form	O
of	O
the	O
two	O
td	B
errors	O
t	O
and	O
t	O
vstwt	O
wt	O
qst	O
at	O
wt	O
where	O
rt	O
is	O
an	O
estimate	O
at	O
time	O
t	O
of	O
the	O
average	O
reward	O
r	O
with	O
these	O
alternate	O
definitions	O
most	O
of	O
our	O
algorithms	O
and	O
many	O
theoretical	O
results	O
carry	O
through	O
to	O
the	O
average-reward	O
setting	O
without	O
change	O
for	O
example	O
the	O
average	O
reward	O
version	O
of	O
semi-gradient	O
sarsa	B
is	O
defined	O
just	O
as	O
in	O
except	O
with	O
the	O
differential	B
version	O
of	O
the	O
td	B
error	I
that	O
is	O
by	O
wt	O
t	O
qst	O
at	O
wt	O
with	O
t	O
given	O
by	O
the	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
exercise	O
give	O
pseudocode	O
for	O
a	O
differential	B
version	O
of	O
semi-gradient	O
q-learning	B
exercise	O
what	O
equations	O
are	O
needed	O
to	O
specify	O
the	O
differential	B
version	O
of	O
average	O
reward	O
a	O
new	O
problem	O
setting	O
for	O
continuing	B
tasks	I
differential	B
semi-gradient	O
sarsa	B
for	O
estimating	O
q	O
q	O
input	O
a	O
differentiable	O
action-value	O
function	O
parameterization	O
q	O
s	O
a	O
rd	O
r	O
algorithm	O
parameters	O
step	O
sizes	O
initialize	O
value-function	O
weights	O
w	O
rd	O
arbitrarily	O
w	O
initialize	O
average	O
reward	O
estimate	O
r	O
r	O
arbitrarily	O
r	O
initialize	O
state	B
s	O
and	O
action	B
a	O
loop	O
for	O
each	O
step	O
take	O
action	B
a	O
observe	O
r	O
choose	O
as	O
a	O
function	O
of	O
w	O
r	O
r	O
w	O
qs	O
a	O
w	O
r	O
r	O
w	O
w	O
qs	O
a	O
w	O
s	O
a	O
example	O
an	O
access-control	O
queuing	O
task	O
this	O
is	O
a	O
decision	O
task	O
involving	O
access	O
control	B
to	O
a	O
set	O
of	O
k	O
servers	O
customers	O
of	O
four	O
different	O
priorities	O
arrive	O
at	O
a	O
single	O
queue	O
if	O
given	O
access	O
to	O
a	O
server	O
the	O
customers	O
pay	O
a	O
reward	O
of	O
or	O
to	O
the	O
server	O
depending	O
on	O
their	O
priority	O
with	O
higher	O
priority	O
customers	O
paying	O
more	O
in	O
each	O
time	O
step	O
the	O
customer	O
at	O
the	O
head	O
of	O
the	O
queue	O
is	O
either	O
accepted	O
to	O
one	O
of	O
the	O
servers	O
or	O
rejected	O
from	O
the	O
queue	O
with	O
a	O
reward	O
of	O
zero	O
in	O
either	O
case	O
on	O
the	O
next	O
time	O
step	O
the	O
next	O
customer	O
in	O
the	O
queue	O
is	O
considered	O
the	O
queue	O
never	O
empties	O
and	O
the	O
priorities	O
of	O
the	O
customers	O
in	O
the	O
queue	O
are	O
equally	O
randomly	O
distributed	O
of	O
course	O
a	O
customer	O
cannot	O
be	O
served	O
if	O
there	O
is	O
no	O
free	O
server	O
the	O
customer	O
is	O
always	O
rejected	O
in	O
this	O
case	O
each	O
busy	O
server	O
becomes	O
free	O
with	O
probability	O
p	O
on	O
each	O
time	O
step	O
although	O
we	O
have	O
just	O
described	O
them	O
for	O
definiteness	O
let	O
us	O
assume	O
the	O
statistics	O
of	O
arrivals	O
and	O
departures	O
are	O
unknown	O
the	O
task	O
is	O
to	O
decide	O
on	O
each	O
step	O
whether	O
to	O
accept	O
or	O
reject	O
the	O
next	O
customer	O
on	O
the	O
basis	O
of	O
his	O
priority	O
and	O
the	O
number	O
of	O
free	O
servers	O
so	O
as	O
to	O
maximize	O
long-term	O
reward	O
without	O
discounting	B
in	O
this	O
example	O
we	O
consider	O
a	O
tabular	O
solution	O
to	O
this	O
problem	O
although	O
there	O
is	O
no	O
generalization	O
between	O
states	O
we	O
can	O
still	O
consider	O
it	O
in	O
the	O
general	O
function	B
approximation	I
setting	O
as	O
this	O
setting	O
generalizes	O
the	O
tabular	O
setting	O
thus	O
we	O
have	O
a	O
differential	B
action-value	O
estimate	O
for	O
each	O
pair	O
of	O
state	B
of	O
free	O
servers	O
and	O
priority	O
of	O
the	O
customer	O
at	O
the	O
head	O
of	O
the	O
queue	O
and	O
action	B
or	O
reject	O
figure	O
shows	O
the	O
solution	O
found	O
by	O
differential	B
semi-gradient	O
sarsa	B
for	O
this	O
task	O
with	O
k	O
and	O
p	O
the	O
algorithm	O
parameters	O
were	O
and	O
the	O
initial	O
action	B
values	O
and	O
r	O
were	O
zero	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
figure	O
the	O
policy	B
and	O
value	B
function	I
found	O
by	O
differential	B
semi-gradient	O
one-step	O
sarsa	B
on	O
the	O
access-control	O
queuing	O
task	O
after	O
million	O
steps	O
the	O
drop	O
on	O
the	O
right	O
of	O
the	O
graph	O
is	O
probably	O
due	O
to	O
insufficient	O
data	O
many	O
of	O
these	O
states	O
were	O
never	O
experienced	O
the	O
value	B
learned	O
for	O
r	O
was	O
about	O
exercise	O
consider	O
an	O
markov	O
reward	O
process	O
consisting	O
of	O
a	O
ring	O
of	O
three	O
states	O
a	O
b	O
and	O
c	O
with	O
state	B
transitions	O
going	O
deterministically	O
around	O
the	O
ring	O
a	O
reward	O
of	O
is	O
received	O
upon	O
arrival	O
in	O
a	O
and	O
otherwise	O
the	O
reward	O
is	O
what	O
are	O
the	O
differential	B
values	O
of	O
the	O
three	O
states	O
exercise	O
the	O
pseudocode	O
in	O
the	O
box	O
on	O
page	O
updates	O
using	O
t	O
as	O
an	O
error	O
rather	O
than	O
simply	O
both	O
errors	O
work	O
but	O
using	O
t	O
is	O
better	O
to	O
see	O
why	O
consider	O
the	O
ring	O
mrp	O
of	O
three	O
states	O
from	O
the	O
previous	O
exercise	O
the	O
estimate	O
of	O
the	O
average	O
reward	O
should	O
tend	O
towards	O
its	O
true	O
value	B
of	O
suppose	O
it	O
was	O
already	O
there	O
and	O
was	O
held	O
stuck	O
there	O
what	O
would	O
the	O
sequence	O
of	O
rt	O
rt	O
errors	O
be	O
what	O
would	O
the	O
sequence	O
of	O
t	O
errors	O
be	O
which	O
error	O
sequence	O
would	O
produce	O
a	O
more	O
stable	O
estimate	O
of	O
the	O
average	O
reward	O
if	O
the	O
estimate	O
were	O
allowed	O
to	O
change	O
in	O
response	O
to	O
the	O
errors	O
why	O
exercise	O
suppose	O
there	O
is	O
an	O
mdp	O
that	O
under	O
any	O
policy	B
produces	O
the	O
deterministic	O
sequence	O
of	O
rewards	O
going	O
on	O
forever	O
technically	O
this	O
is	O
not	O
allowed	O
because	O
it	O
violates	O
ergodicity	O
there	O
is	O
no	O
stationary	O
limiting	O
distribution	O
and	O
the	O
limit	O
does	O
not	O
exist	O
nevertheless	O
the	O
average	O
reward	O
is	O
well	O
defined	O
what	O
is	O
it	O
now	O
consider	O
two	O
states	O
in	O
this	O
mdp	O
from	O
a	O
the	O
reward	O
sequence	O
is	O
exactly	O
as	O
described	O
above	O
starting	O
with	O
a	O
whereas	O
from	O
b	O
the	O
reward	O
sequence	O
starts	O
with	O
of	O
best	O
actionnumber	O
of	O
free	O
of	O
free	O
of	O
free	O
ofbest	O
deprecating	O
the	O
discounted	O
setting	O
a	O
and	O
then	O
continues	O
with	O
the	O
differential	B
return	B
is	O
not	O
well	O
defined	O
for	O
this	O
case	O
as	O
the	O
limit	O
does	O
not	O
exist	O
to	O
repair	O
this	O
one	O
could	O
alternately	O
define	O
the	O
value	B
of	O
a	O
state	B
as	O
v	O
lim	O
h	O
under	O
this	O
definition	O
what	O
are	O
the	O
values	O
of	O
states	O
a	O
and	O
b	O
s	O
r	O
deprecating	O
the	O
discounted	O
setting	O
the	O
continuing	O
discounted	O
problem	O
formulation	O
has	O
been	O
very	O
useful	O
in	O
the	O
tabular	O
case	O
in	O
which	O
the	O
returns	O
from	O
each	O
state	B
can	O
be	O
separately	O
identified	O
and	O
averaged	O
but	O
in	O
the	O
approximate	O
case	O
it	O
is	O
questionable	O
whether	O
one	O
should	O
ever	O
use	O
this	O
problem	O
formulation	O
to	O
see	O
why	O
consider	O
an	O
infinite	O
sequence	O
of	O
returns	O
with	O
no	O
beginning	O
or	O
end	O
and	O
no	O
clearly	O
identified	O
states	O
the	O
states	O
might	O
be	O
represented	O
only	O
by	O
feature	O
vectors	O
which	O
may	O
do	O
little	O
to	O
distinguish	O
the	O
states	O
from	O
each	O
other	O
as	O
a	O
special	O
case	O
all	O
of	O
the	O
feature	O
vectors	O
may	O
be	O
the	O
same	O
thus	O
one	O
really	O
has	O
only	O
the	O
reward	O
sequence	O
the	O
actions	O
and	O
performance	O
has	O
to	O
be	O
assessed	O
purely	O
from	O
these	O
how	O
could	O
it	O
be	O
done	O
one	O
way	O
is	O
by	O
averaging	O
the	O
rewards	O
over	O
a	O
long	O
interval	O
this	O
is	O
the	O
idea	O
of	O
the	O
average-reward	O
setting	O
how	O
could	O
discounting	B
be	O
used	O
well	O
for	O
each	O
time	O
step	O
we	O
could	O
measure	O
the	O
discounted	O
return	B
some	O
returns	O
would	O
be	O
small	O
and	O
some	O
big	O
so	O
again	O
we	O
would	O
have	O
to	O
average	O
them	O
over	O
a	O
sufficiently	O
large	O
time	O
interval	O
in	O
the	O
continuing	O
setting	O
there	O
are	O
no	O
starts	O
and	O
ends	O
and	O
no	O
special	O
time	O
steps	O
so	O
there	O
is	O
nothing	O
else	O
that	O
could	O
be	O
done	O
however	O
if	O
you	O
do	O
this	O
it	O
turns	O
out	O
that	O
the	O
average	O
of	O
the	O
discounted	O
returns	O
is	O
proportional	O
to	O
the	O
average	O
reward	O
in	O
fact	O
for	O
policy	B
the	O
average	O
of	O
the	O
discounted	O
returns	O
is	O
always	O
r	O
that	O
is	O
it	O
is	O
essentially	O
the	O
average	O
reward	O
r	O
in	O
particular	O
the	O
ordering	O
of	O
all	O
policies	O
in	O
the	O
average	O
discounted	O
return	B
setting	O
would	O
be	O
exactly	O
the	O
same	O
as	O
in	O
the	O
average-reward	O
setting	O
the	O
discount	O
rate	O
thus	O
has	O
no	O
effect	O
on	O
the	O
problem	O
formulation	O
it	O
could	O
in	O
fact	O
be	O
zero	O
and	O
the	O
ranking	O
would	O
be	O
unchanged	O
this	O
surprising	O
fact	O
is	O
proven	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
but	O
the	O
basic	O
idea	O
can	O
be	O
seen	O
via	O
a	O
symmetry	O
argument	O
each	O
time	O
step	O
is	O
exactly	O
the	O
same	O
as	O
every	O
other	O
with	O
discounting	B
every	O
reward	O
will	O
appear	O
exactly	O
once	O
in	O
each	O
position	O
in	O
some	O
return	B
the	O
tth	O
reward	O
will	O
appear	O
undiscounted	O
in	O
the	O
t	O
return	B
discounted	O
once	O
in	O
the	O
t	O
return	B
and	O
discounted	O
times	O
in	O
the	O
t	O
return	B
the	O
weight	O
on	O
the	O
tth	O
reward	O
is	O
thus	O
because	O
all	O
states	O
are	O
the	O
same	O
they	O
are	O
all	O
weighted	O
by	O
this	O
and	O
thus	O
the	O
average	O
of	O
the	O
returns	O
will	O
be	O
this	O
times	O
the	O
average	O
reward	O
or	O
r	O
this	O
example	O
and	O
the	O
more	O
general	O
argument	O
in	O
the	O
box	O
show	O
that	O
if	O
we	O
optimized	O
discounted	O
value	B
over	O
the	O
on-policy	B
distribution	I
then	O
the	O
effect	O
would	O
be	O
identical	O
to	O
optimizing	O
undiscounted	O
average	O
reward	O
the	O
actual	O
value	B
of	O
would	O
have	O
no	O
effect	O
this	O
strongly	O
suggests	O
that	O
discounting	B
has	O
no	O
role	O
to	O
play	O
in	O
the	O
definition	O
of	O
the	O
control	B
problem	O
with	B
function	B
approximation	I
one	O
can	O
nevertheless	O
go	O
ahead	O
and	O
use	O
discounting	B
in	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
the	O
futility	O
of	O
discounting	B
in	O
continuing	O
problems	O
perhaps	O
discounting	B
can	O
be	O
saved	O
by	O
choosing	O
an	O
objective	O
that	O
sums	O
discounted	O
values	O
over	O
the	O
distribution	O
with	O
which	O
states	O
occur	O
under	O
the	O
policy	B
v	O
is	O
the	O
discounted	O
value	B
function	I
rs	O
a	O
v	O
rs	O
a	O
v	O
a	O
v	O
v	O
eq	O
j	O
r	O
r	O
r	O
r	O
j	O
r	O
r	O
r	O
r	O
r	O
the	O
proposed	O
discounted	O
objective	O
orders	O
policies	O
identically	O
to	O
the	O
undiscounted	O
reward	O
objective	O
the	O
discount	O
rate	O
does	O
not	O
influence	O
the	O
ordering	O
solution	O
methods	O
the	O
discounting	B
parameter	O
changes	O
from	O
a	O
problem	O
parameter	O
to	O
a	O
solution	O
method	O
parameter	O
however	O
in	O
this	O
case	O
we	O
unfortunately	O
would	O
not	O
be	O
guaranteed	O
to	O
optimize	O
average	O
reward	O
the	O
equivalent	O
discounted	O
value	B
over	O
the	O
on-policy	B
distribution	I
the	O
root	O
cause	O
of	O
the	O
difficulties	O
with	O
the	O
discounted	O
control	B
setting	O
is	O
that	O
with	B
function	B
approximation	I
we	O
have	O
lost	O
the	O
policy	B
improvement	I
theorem	B
it	O
is	O
no	O
longer	O
true	O
that	O
if	O
we	O
change	O
the	O
policy	B
to	O
improve	O
the	O
discounted	O
value	B
of	O
one	O
state	B
then	O
we	O
are	O
guaranteed	O
to	O
have	O
improved	O
the	O
overall	O
policy	B
in	O
any	O
useful	O
sense	O
that	O
guarantee	O
was	O
key	O
to	O
the	O
theory	O
of	O
our	O
reinforcement	B
learning	I
control	B
methods	O
with	B
function	B
approximation	I
we	O
have	O
lost	O
it	O
in	O
fact	O
the	O
lack	O
of	O
a	O
policy	B
improvement	I
theorem	B
is	O
also	O
a	O
theoretical	O
lacuna	O
for	O
the	O
total-episodic	O
and	O
average-reward	O
settings	O
once	O
we	O
introduce	O
function	B
approximation	I
we	O
can	O
no	O
longer	O
guarantee	O
improvement	O
for	O
any	O
setting	O
in	O
chapter	O
we	O
introduce	O
an	O
alternative	O
class	O
of	O
reinforcement	B
learning	I
algorithms	O
based	O
on	O
parameterized	O
policies	O
and	O
there	O
we	O
have	O
a	O
theoretical	O
guarantee	O
called	O
the	O
policy-gradient	O
theorem	B
which	O
plays	O
a	O
similar	O
role	O
as	O
the	O
policy	B
improvement	I
theorem	B
but	O
for	O
methods	O
that	O
learn	O
action	B
values	O
we	O
seem	O
to	O
be	O
currently	O
without	O
a	O
local	O
improvement	O
guarantee	O
differential	B
semi-gradient	O
n-step	B
sarsa	B
the	O
approach	O
taken	O
by	O
perkins	O
and	O
precup	O
may	O
provide	O
a	O
part	O
of	O
the	O
answer	O
we	O
do	O
know	O
that	O
may	O
sometimes	O
result	O
in	O
an	O
inferior	O
policy	B
as	O
policies	O
may	O
chatter	O
among	O
good	O
policies	O
rather	O
than	O
converge	O
this	O
is	O
an	O
area	O
with	O
multiple	O
open	O
theoretical	O
questions	O
differential	B
semi-gradient	O
n-step	B
sarsa	B
in	O
order	O
to	O
generalize	O
to	O
n-step	B
bootstrapping	B
we	O
need	O
an	O
n-step	B
version	O
of	O
the	O
td	B
error	I
we	O
begin	O
by	O
generalizing	O
the	O
n-step	B
return	B
to	O
its	O
differential	B
form	O
with	B
function	B
approximation	I
gttn	O
rtn	O
qstn	O
atn	O
wtn	O
where	O
r	O
is	O
an	O
estimate	O
of	O
r	O
n	O
and	O
t	O
n	O
t	O
if	O
t	O
n	O
t	O
then	O
we	O
define	O
gttn	O
gt	O
as	O
usual	O
the	O
n-step	B
td	B
error	I
is	O
then	O
t	O
gttn	O
qst	O
at	O
w	O
after	O
which	O
we	O
can	O
apply	O
our	O
usual	O
semi-gradient	O
sarsa	B
update	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
differential	B
semi-gradient	O
n-step	B
sarsa	B
for	O
estimating	O
q	O
q	O
or	O
q	O
input	O
a	O
differentiable	O
function	O
q	O
s	O
a	O
rd	O
r	O
a	O
policy	B
initialize	O
value-function	O
weights	O
w	O
rd	O
arbitrarily	O
w	O
initialize	O
average-reward	O
estimate	O
r	O
r	O
arbitrarily	O
r	O
algorithm	O
parameters	O
step	O
size	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
at	O
and	O
rt	O
can	O
take	O
their	O
index	O
mod	O
n	O
initialize	O
and	O
store	O
and	O
loop	O
for	O
each	O
step	O
t	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
and	O
the	O
next	O
state	B
as	O
select	O
and	O
store	O
an	O
action	B
or	O
wrt	O
w	O
t	O
n	O
if	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
i	O
r	O
qs	O
a	O
w	O
qs	O
a	O
w	O
r	O
r	O
w	O
w	O
qs	O
a	O
w	O
exercise	O
in	O
the	O
differential	B
semi-gradient	O
n-step	B
sarsa	B
algorithm	O
the	O
step-size	B
parameter	I
on	O
the	O
average	O
reward	O
needs	O
to	O
be	O
quite	O
small	O
so	O
that	O
r	O
becomes	O
a	O
good	O
long-term	O
estimate	O
of	O
the	O
average	O
reward	O
unfortunately	O
r	O
will	O
then	O
be	O
biased	O
by	O
its	O
chapter	O
on-policy	O
control	B
with	B
approximation	I
initial	O
value	B
for	O
many	O
steps	O
which	O
may	O
make	O
learning	O
inefficient	O
alternatively	O
one	O
could	O
use	O
a	O
sample	O
average	O
of	O
the	O
observed	O
rewards	O
for	O
r	O
that	O
would	O
initially	O
adapt	O
rapidly	O
but	O
in	O
the	O
long	O
run	O
would	O
also	O
adapt	O
slowly	O
as	O
the	O
policy	B
slowly	O
changed	O
r	O
would	O
also	O
change	O
the	O
potential	O
for	O
such	O
long-term	O
nonstationarity	B
makes	O
sample-average	O
methods	O
ill-suited	O
in	O
fact	O
the	O
step-size	B
parameter	I
on	O
the	O
average	O
reward	O
is	O
a	O
perfect	O
place	O
to	O
use	O
the	O
unbiased	O
constant-step-size	O
trick	O
from	O
exercise	O
describe	O
the	O
specific	O
changes	O
needed	O
to	O
the	O
boxed	O
algorithm	O
for	O
differential	B
semi-gradient	O
n-step	B
sarsa	B
to	O
use	O
this	O
trick	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
extended	O
the	O
ideas	O
of	O
parameterized	O
function	B
approximation	I
and	O
semi-gradient	O
descent	O
introduced	O
in	O
the	O
previous	O
chapter	O
to	O
control	B
the	O
extension	O
is	O
immediate	O
for	O
the	O
episodic	O
case	O
but	O
for	O
the	O
continuing	O
case	O
we	O
have	O
to	O
introduce	O
a	O
whole	O
new	O
problem	O
formulation	O
based	O
on	O
maximizing	O
the	O
average	B
reward	I
setting	I
per	O
time	O
step	O
surprisingly	O
the	O
discounted	O
formulation	O
cannot	O
be	O
carried	O
over	O
to	O
control	B
in	O
the	O
presence	O
of	O
approximations	O
in	O
the	O
approximate	O
case	O
most	O
policies	O
cannot	O
be	O
represented	O
by	O
a	O
value	B
function	I
the	O
arbitrary	O
policies	O
that	O
remain	O
need	O
to	O
be	O
ranked	O
and	O
the	O
scalar	O
average	O
reward	O
r	O
provides	O
an	O
effective	O
way	O
to	O
do	O
this	O
the	O
average	O
reward	O
formulation	O
involves	O
new	O
differential	B
versions	O
of	O
value	B
functions	O
bellman	B
equations	O
and	O
td	B
errors	O
but	O
all	O
of	O
these	O
parallel	O
the	O
old	O
ones	O
and	O
the	O
conceptual	O
changes	O
are	O
small	O
there	O
is	O
also	O
a	O
new	O
parallel	O
set	O
of	O
differential	B
algorithms	O
for	O
the	O
average-reward	O
case	O
bibliographical	O
and	O
historical	O
remarks	O
semi-gradient	O
sarsa	B
with	B
function	B
approximation	I
was	O
first	O
explored	O
by	O
rummery	O
and	O
niranjan	O
linear	O
semi-gradient	O
sarsa	B
with	O
action	B
selection	O
does	O
not	O
converge	O
in	O
the	O
usual	O
sense	O
but	O
does	O
enter	O
a	O
bounded	O
region	O
near	O
the	O
best	O
solution	O
precup	O
and	O
perkins	O
showed	O
convergence	O
in	O
a	O
differentiable	O
action	B
selection	O
setting	O
see	O
also	O
perkins	O
and	O
pendrith	O
and	O
melo	O
meyn	O
and	O
ribeiro	O
the	O
mountain	B
car	I
example	I
is	O
based	O
on	O
a	O
similar	O
task	O
studied	O
by	O
moore	O
but	O
the	O
exact	O
form	O
used	O
here	O
is	O
from	O
sutton	O
episodic	O
n-step	B
semi-gradient	O
sarsa	B
is	O
based	O
on	O
the	O
forward	O
sarsa	B
algorithm	O
of	O
van	O
seijen	O
the	O
empirical	O
results	O
shown	O
here	O
are	O
new	O
to	O
the	O
second	O
edition	O
of	O
this	O
text	O
the	O
average-reward	O
formulation	O
has	O
been	O
described	O
for	B
dynamic	B
programming	I
puterman	O
and	O
from	O
the	O
point	O
of	O
view	O
of	O
reinforcement	B
learning	I
tadepalli	O
and	O
ok	O
bertsekas	O
and	O
tsitiklis	O
tsitsiklis	O
and	O
van	O
roy	O
the	O
algorithm	O
described	O
here	O
is	O
the	O
on-policy	O
analog	O
of	O
the	O
r-learning	B
algorithm	O
introduced	O
by	O
schwartz	O
the	O
name	O
r-learning	B
summary	O
was	O
probably	O
meant	O
to	O
be	O
the	O
alphabetic	O
successor	O
to	O
q-learning	B
but	O
we	O
prefer	O
to	O
think	O
of	O
it	O
as	O
a	O
reference	O
to	O
the	O
learning	O
of	O
differential	B
or	O
relative	O
values	O
the	O
access-control	B
queuing	B
example	I
was	O
suggested	O
by	O
the	O
work	O
of	O
carlstr	O
om	O
and	O
nordstr	O
om	O
the	O
recognition	O
of	O
the	O
limitations	O
of	O
discounting	B
as	O
a	O
formulation	O
of	O
the	O
reinforcement	B
learning	I
problem	O
with	B
function	B
approximation	I
became	O
apparent	O
to	O
the	O
authors	O
shortly	O
after	O
the	O
publication	O
of	O
the	O
first	O
edition	O
of	O
this	O
text	O
singh	O
jaakkola	O
and	O
jordan	O
may	O
have	O
been	O
the	O
first	O
to	O
observe	O
it	O
in	O
print	O
the	O
differential	B
version	O
of	O
n-step	B
semi-gradient	O
sarsa	B
is	O
new	O
to	O
this	O
text	O
and	O
has	O
not	O
been	O
significantly	O
studied	O
chapter	O
methods	O
with	B
approximation	I
this	O
book	O
has	O
treated	O
on-policy	O
and	O
off-policy	B
learning	O
methods	O
since	O
chapter	O
primarily	O
as	O
two	O
alternative	O
ways	O
of	O
handling	O
the	O
conflict	O
between	O
exploitation	O
and	O
exploration	O
inherent	B
in	O
learning	O
forms	O
of	O
generalized	O
policy	B
iteration	I
the	O
two	O
chapters	O
preceding	O
this	O
have	O
treated	O
the	O
on-policy	O
case	O
with	B
function	B
approximation	I
and	O
in	O
this	O
chapter	O
we	O
treat	O
the	O
off	O
case	O
with	B
function	B
approximation	I
the	O
extension	O
to	O
function	B
approximation	I
turns	O
out	O
to	O
be	O
significantly	O
different	O
and	O
harder	O
for	O
off-policy	B
learning	O
than	O
it	O
is	O
for	O
on-policy	O
learning	O
the	O
tabular	O
off-policy	B
methods	I
developed	O
in	O
chapters	O
and	O
readily	O
extend	O
to	O
semi-gradient	O
algorithms	O
but	O
these	O
algorithms	O
do	O
not	O
converge	O
as	O
robustly	O
as	O
they	O
do	O
under	O
on-policy	O
training	O
in	O
this	O
chapter	O
we	O
explore	O
the	O
convergence	O
problems	O
take	O
a	O
closer	O
look	O
at	O
the	O
theory	O
of	O
linear	O
function	B
approximation	I
introduce	O
a	O
notion	O
of	O
learnability	O
and	O
then	O
discuss	O
new	O
algorithms	O
with	O
stronger	O
convergence	O
guarantees	O
for	O
the	O
off-policy	B
case	O
in	O
the	O
end	O
we	O
will	O
have	O
improved	O
methods	O
but	O
the	O
theoretical	O
results	O
will	O
not	O
be	O
as	O
strong	O
nor	O
the	O
empirical	O
results	O
as	O
satisfying	O
as	O
they	O
are	O
for	O
on-policy	O
learning	O
along	O
the	O
way	O
we	O
will	O
gain	O
a	O
deeper	O
understanding	O
of	O
approximation	O
in	O
reinforcement	B
learning	I
for	O
on-policy	O
learning	O
as	O
well	O
as	O
off-policy	B
learning	O
recall	O
that	O
in	O
off-policy	B
learning	O
we	O
seek	O
to	O
learn	O
a	O
value	B
function	I
for	O
a	O
target	O
policy	B
given	O
data	O
due	O
to	O
a	O
different	O
behavior	O
policy	B
b	O
in	O
the	O
prediction	B
case	O
both	O
policies	O
are	O
static	O
and	O
given	O
and	O
we	O
seek	O
to	O
learn	O
either	O
state	B
values	O
v	O
v	O
or	O
action	B
values	O
q	O
q	O
in	O
the	O
control	B
case	O
action	B
values	O
are	O
learned	O
and	O
both	O
policies	O
typically	O
change	O
during	O
learning	O
being	O
the	O
greedy	O
policy	B
with	O
respect	O
to	O
q	O
and	O
b	O
being	O
something	O
more	O
exploratory	O
such	O
as	O
the	O
policy	B
with	O
respect	O
to	O
q	O
the	O
challenge	O
of	O
off-policy	B
learning	O
can	O
be	O
divided	O
into	O
two	O
parts	O
one	O
that	O
arises	O
in	O
the	O
tabular	O
case	O
and	O
one	O
that	O
arises	O
only	O
with	B
function	B
approximation	I
the	O
first	O
part	O
of	O
the	O
challenge	O
has	O
to	O
do	O
with	O
the	O
target	O
of	O
the	O
update	O
to	O
be	O
confused	O
with	O
the	O
target	O
policy	B
and	O
the	O
second	O
part	O
has	O
to	O
do	O
with	O
the	O
distribution	O
of	O
the	O
updates	O
the	O
techniques	O
related	O
to	O
importance	B
sampling	I
developed	O
in	O
chapters	O
and	O
deal	O
with	O
chapter	O
methods	O
with	B
approximation	I
the	O
first	O
part	O
these	O
may	O
increase	O
variance	O
but	O
are	O
needed	O
in	O
all	O
successful	O
algorithms	O
tabular	O
and	O
approximate	O
the	O
extension	O
of	O
these	O
techniques	O
to	O
function	B
approximation	I
are	O
quickly	O
dealt	O
with	O
in	O
the	O
first	O
section	O
of	O
this	O
chapter	O
something	O
more	O
is	O
needed	O
for	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
off-policy	B
learning	O
with	B
function	B
approximation	I
because	O
the	O
distribution	O
of	O
updates	O
in	O
the	O
off-policy	B
case	O
is	O
not	O
according	O
to	O
the	O
on-policy	B
distribution	I
the	O
on-policy	B
distribution	I
is	O
important	O
to	O
the	O
stability	O
of	O
semi-gradient	B
methods	I
two	O
general	O
approaches	O
have	O
been	O
explored	O
to	O
deal	O
with	O
this	O
one	O
is	O
to	O
use	O
importance	B
sampling	I
methods	O
again	O
this	O
time	O
to	O
warp	O
the	O
update	O
distribution	O
back	O
to	O
the	O
on-policy	B
distribution	I
so	O
that	O
semi-gradient	B
methods	I
are	O
guaranteed	O
to	O
converge	O
the	O
linear	O
case	O
the	O
other	O
is	O
to	O
develop	O
true	O
gradient	B
methods	O
that	O
do	O
not	O
rely	O
on	O
any	O
special	O
distribution	O
for	O
stability	O
we	O
present	O
methods	O
based	O
on	O
both	O
approaches	O
this	O
is	O
a	O
cutting-edge	O
research	O
area	O
and	O
it	O
is	O
not	O
clear	O
which	O
of	O
these	O
approaches	O
is	O
most	O
effective	O
in	O
practice	O
semi-gradient	B
methods	I
we	O
begin	O
by	O
describing	O
how	O
the	O
methods	O
developed	O
in	O
earlier	O
chapters	O
for	O
the	O
off-policy	B
case	O
extend	O
readily	O
to	O
function	B
approximation	I
as	O
semi-gradient	B
methods	I
these	O
methods	O
address	O
the	O
first	O
part	O
of	O
the	O
challenge	O
of	O
off-policy	B
learning	O
the	O
update	O
targets	O
but	O
not	O
the	O
second	O
part	O
the	O
update	O
distribution	O
accordingly	O
these	O
methods	O
may	O
diverge	O
in	O
some	O
cases	O
and	O
in	O
that	O
sense	O
are	O
not	O
sound	O
but	O
still	O
they	O
are	O
often	O
successfully	O
used	O
remember	O
that	O
these	O
methods	O
are	O
guaranteed	O
stable	O
and	O
asymptotically	O
unbiased	O
for	O
the	O
tabular	O
case	O
which	O
corresponds	O
to	O
a	O
special	O
case	O
of	O
function	B
approximation	I
so	O
it	O
may	O
still	O
be	O
possible	O
to	O
combine	O
them	O
with	O
feature	O
selection	O
methods	O
in	O
such	O
a	O
way	O
that	O
the	O
combined	O
system	O
could	O
be	O
assured	O
stable	O
in	O
any	O
event	O
these	O
methods	O
are	O
simple	O
and	O
thus	O
a	O
good	O
place	O
to	O
start	O
in	O
chapter	O
we	O
described	O
a	O
variety	O
of	O
tabular	O
off-policy	B
algorithms	O
to	O
convert	O
them	O
to	O
semi-gradient	O
form	O
we	O
simply	O
replace	O
the	O
update	O
to	O
an	O
array	O
or	O
q	O
to	O
an	O
update	O
to	O
a	O
weight	O
vector	B
using	O
the	O
approximate	O
value	B
function	I
v	O
or	O
q	O
and	O
its	O
gradient	B
many	O
of	O
these	O
algorithms	O
use	O
the	O
per-step	O
importance	B
sampling	I
ratio	B
tt	O
t	O
batst	O
for	O
example	O
the	O
one-step	O
state-value	O
algorithm	O
is	O
semi-gradient	O
off-policy	B
which	O
is	O
just	O
like	O
the	O
corresponding	O
on-policy	O
algorithm	O
except	O
for	O
the	O
addition	O
of	O
t	O
wt	O
t	O
t	O
vstwt	O
where	O
t	O
is	O
defined	O
appropriately	O
depending	O
on	O
whether	O
the	O
problem	O
is	O
episodic	O
and	O
discounted	O
or	O
continuing	O
and	O
undiscounted	O
using	O
average	O
reward	O
t	O
vstwt	O
or	O
t	O
rt	O
vstwt	O
examples	O
of	O
off-policy	B
divergence	O
for	B
action	B
values	I
the	O
one-step	O
algorithm	O
is	O
semi-gradient	O
expected	B
sarsa	B
wt	O
t	O
qst	O
at	O
wt	O
with	O
t	O
t	O
rt	O
a	O
wt	O
qst	O
at	O
wt	O
or	O
a	O
wt	O
qst	O
at	O
wt	O
note	O
that	O
this	O
algorithm	O
does	O
not	O
use	O
importance	B
sampling	I
in	O
the	O
tabular	O
case	O
it	O
is	O
clear	O
that	O
this	O
is	O
appropriate	O
because	O
the	O
only	O
sample	O
action	B
is	O
at	O
and	O
in	O
learning	O
its	O
value	B
we	O
do	O
not	O
have	O
to	O
consider	O
any	O
other	O
actions	O
with	B
function	B
approximation	I
it	O
is	O
less	O
clear	O
because	O
we	O
might	O
want	O
to	O
weight	O
different	O
state	B
action	B
pairs	O
differently	O
once	O
they	O
all	O
contribute	O
to	O
the	O
same	O
overall	O
approximation	O
proper	O
resolution	O
of	O
this	O
issue	O
awaits	O
a	O
more	O
thorough	O
understanding	O
of	O
the	O
theory	O
of	O
function	B
approximation	I
in	O
reinforcement	B
learning	I
in	O
the	O
multi-step	O
generalizations	O
of	O
these	O
algorithms	O
both	O
the	O
state-value	O
and	O
actionvalue	O
algorithms	O
involve	O
importance	B
sampling	I
for	O
example	O
the	O
n-step	B
version	O
of	O
semigradient	O
expected	B
sarsa	B
is	O
wtn	O
wtn	O
tn	O
qst	O
at	O
wtn	O
qst	O
at	O
wtn	O
with	O
gttn	O
gttn	O
n	O
n	O
qstn	O
atn	O
wtn	O
or	O
rt	O
rtn	O
rtn	O
qstn	O
atn	O
wtn	O
where	O
here	O
we	O
are	O
being	O
slightly	O
informal	O
in	O
our	O
treatment	O
of	O
the	O
ends	O
of	O
episodes	B
in	O
the	O
first	O
equation	O
the	O
ks	O
for	O
k	O
t	O
t	O
is	O
the	O
last	O
time	O
step	O
of	O
the	O
episode	O
should	O
be	O
taken	O
to	O
be	O
and	O
gtn	O
should	O
be	O
taken	O
to	O
be	O
gt	O
if	O
t	O
n	O
t	O
recall	O
that	O
we	O
also	O
presented	O
in	O
chapter	O
an	O
off-policy	B
algorithm	O
that	O
does	O
not	O
involve	O
importance	B
sampling	I
at	O
all	O
the	O
n-step	B
tree-backup	O
algorithm	O
here	O
is	O
its	O
semi-gradient	O
version	O
wtn	O
gttn	O
wtn	O
qst	O
at	O
wtn	O
qst	O
at	O
wtn	O
qst	O
at	O
wt	O
k	O
tn	O
with	O
t	O
as	O
defined	O
at	O
the	O
top	O
of	O
this	O
page	O
for	B
expected	B
sarsa	B
we	O
also	O
defined	O
in	O
chapter	O
an	O
algorithm	O
that	O
unifies	O
all	O
action-value	O
algorithms	O
n-step	B
q	O
we	O
leave	O
the	O
semigradient	O
form	O
of	O
that	O
algorithm	O
and	O
also	O
of	O
the	O
n-step	B
state-value	O
algorithm	O
as	O
exercises	O
for	O
the	O
reader	O
exercise	O
convert	O
the	O
equation	O
of	O
n-step	B
off-policy	B
td	B
to	O
semi-gradient	O
form	O
give	O
accompanying	O
definitions	O
of	O
the	O
return	B
for	O
both	O
the	O
episodic	O
and	O
continuing	O
cases	O
exercise	O
convert	O
the	O
equations	O
of	O
n-step	B
q	O
and	O
to	O
semi-gradient	O
form	O
give	O
definitions	O
that	O
cover	O
both	O
the	O
episodic	O
and	O
continuing	O
cases	O
chapter	O
methods	O
with	B
approximation	I
examples	O
of	O
off-policy	B
divergence	O
in	O
this	O
section	O
we	O
begin	O
to	O
discuss	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
off-policy	B
learning	O
with	B
function	B
approximation	I
that	O
the	O
distribution	O
of	O
updates	O
does	O
not	O
match	O
the	O
onpolicy	O
distribution	O
we	O
describe	O
some	O
instructive	O
counterexamples	O
to	O
off-policy	B
learning	O
cases	O
where	O
semi-gradient	O
and	O
other	O
simple	O
algorithms	O
are	O
unstable	O
and	O
diverge	O
to	O
establish	O
intuitions	O
it	O
is	O
best	O
to	O
consider	O
first	O
a	O
very	O
simple	O
example	O
suppose	O
perhaps	O
as	O
part	O
of	O
a	O
larger	O
mdp	O
there	O
are	O
two	O
states	O
whose	O
estimated	O
values	O
are	O
of	O
the	O
functional	O
form	O
w	O
and	O
where	O
the	O
parameter	O
vector	B
w	O
consists	O
of	O
only	O
a	O
single	O
component	O
w	O
this	O
occurs	O
under	O
linear	O
function	B
approximation	I
if	O
the	O
feature	O
vectors	O
for	O
the	O
two	O
states	O
are	O
each	O
simple	O
numbers	O
vectors	O
in	O
this	O
case	O
and	O
in	O
the	O
first	O
state	B
only	O
one	O
action	B
is	O
available	O
and	O
it	O
results	O
deterministically	O
in	O
a	O
transition	O
to	O
the	O
second	O
state	B
with	O
a	O
reward	O
of	O
where	O
the	O
expressions	O
inside	O
the	O
two	O
circles	O
indicate	O
the	O
two	O
state	B
s	O
values	O
suppose	O
initially	O
w	O
the	O
transition	O
will	O
then	O
be	O
from	O
a	O
state	B
of	O
estimated	O
value	B
to	O
a	O
state	B
of	O
estimated	O
value	B
it	O
will	O
look	O
like	O
a	O
good	O
transition	O
and	O
w	O
will	O
be	O
increased	O
to	O
raise	O
the	O
first	O
state	B
s	O
estimated	O
value	B
if	O
is	O
nearly	O
then	O
the	O
td	B
error	I
will	O
be	O
nearly	O
and	O
if	O
then	O
w	O
will	O
be	O
increased	O
to	O
nearly	O
in	O
trying	O
to	O
reduce	O
the	O
td	B
error	I
however	O
the	O
second	O
state	B
s	O
estimated	O
value	B
will	O
also	O
be	O
increased	O
to	O
nearly	O
if	O
the	O
transition	O
occurs	O
again	O
then	O
it	O
will	O
be	O
from	O
a	O
state	B
of	O
estimated	O
value	B
to	O
a	O
state	B
of	O
estimated	O
value	B
for	O
a	O
td	B
error	I
of	O
larger	O
not	O
smaller	O
than	O
before	O
it	O
will	O
look	O
even	O
more	O
like	O
the	O
first	O
state	B
is	O
undervalued	O
and	O
its	O
value	B
will	O
be	O
increased	O
again	O
this	O
time	O
to	O
this	O
looks	O
bad	O
and	O
in	O
fact	O
with	O
further	O
updates	O
w	O
will	O
diverge	O
to	O
infinity	O
to	O
see	O
this	O
definitively	O
we	O
have	O
to	O
look	O
more	O
carefully	O
at	O
the	O
sequence	O
of	O
updates	O
the	O
td	B
error	I
on	O
a	O
transition	O
between	O
the	O
two	O
states	O
is	O
t	O
vstwt	O
wt	O
and	O
the	O
off-policy	B
semi-gradient	O
update	O
is	O
wt	O
t	O
t	O
vstwt	O
wt	O
note	O
that	O
the	O
importance	B
sampling	I
ratio	B
t	O
is	O
on	O
this	O
transition	O
because	O
there	O
is	O
only	O
one	O
action	B
available	O
from	O
the	O
first	O
state	B
so	O
its	O
probabilities	O
of	O
being	O
taken	O
under	O
the	O
target	O
and	O
behavior	O
policies	O
must	O
both	O
be	O
in	O
the	O
final	O
update	O
above	O
the	O
new	O
parameter	O
is	O
the	O
old	O
parameter	O
times	O
a	O
scalar	O
constant	O
if	O
this	O
constant	O
is	O
greater	O
than	O
then	O
the	O
system	O
is	O
unstable	O
and	O
w	O
will	O
go	O
to	O
positive	O
or	O
negative	O
infinity	O
depending	O
on	O
its	O
initial	O
value	B
here	O
this	O
constant	O
is	O
greater	O
than	O
whenever	O
note	O
that	O
stability	O
does	O
not	O
depend	O
on	O
the	O
specific	O
step	O
size	O
as	O
long	O
as	O
smaller	O
or	O
larger	O
step	O
sizes	O
would	O
affect	O
the	O
rate	O
at	O
which	O
w	O
goes	O
to	O
infinity	O
but	O
not	O
whether	O
it	O
goes	O
there	O
or	O
not	O
key	O
to	O
this	O
example	O
is	O
that	O
the	O
one	O
transition	O
occurs	O
repeatedly	O
without	O
w	O
being	O
updated	O
on	O
other	O
transitions	O
this	O
is	O
possible	O
under	O
off-policy	B
training	O
because	O
the	O
examples	O
of	O
off-policy	B
divergence	O
behavior	O
policy	B
might	O
select	O
actions	O
on	O
those	O
other	O
transitions	O
which	O
the	O
target	O
policy	B
never	O
would	O
for	O
these	O
transitions	O
t	O
would	O
be	O
zero	O
and	O
no	O
update	O
would	O
be	O
made	O
under	O
on-policy	O
training	O
however	O
t	O
is	O
always	O
one	O
each	O
time	O
there	O
is	O
a	O
transition	O
from	O
the	O
w	O
state	B
to	O
the	O
state	B
increasing	O
w	O
there	O
would	O
also	O
have	O
to	O
be	O
a	O
transition	O
out	O
of	O
the	O
state	B
that	O
transition	O
would	O
reduce	O
w	O
unless	O
it	O
were	O
to	O
a	O
state	B
whose	O
value	B
was	O
higher	O
than	O
and	O
then	O
that	O
state	B
would	O
have	O
to	O
be	O
followed	O
by	O
a	O
state	B
of	O
even	O
higher	O
value	B
or	O
else	O
again	O
w	O
would	O
be	O
reduced	O
each	O
state	B
can	O
support	O
the	O
one	O
before	O
only	O
by	O
creating	O
a	O
higher	O
expectation	O
eventually	O
the	O
piper	O
must	O
be	O
paid	O
in	O
the	O
on-policy	O
case	O
the	O
promise	O
of	O
future	O
reward	O
must	O
be	O
kept	O
and	O
the	O
system	O
is	O
kept	O
in	O
check	O
but	O
in	O
the	O
off-policy	B
case	O
a	O
promise	O
can	O
be	O
made	O
and	O
then	O
after	O
taking	O
an	O
action	B
that	O
the	O
target	O
policy	B
never	O
would	O
forgotten	O
and	O
forgiven	O
this	O
simple	O
example	O
communicates	O
much	O
of	O
the	O
reason	O
why	O
off-policy	B
training	O
can	O
lead	O
to	O
divergence	O
but	O
it	O
is	O
not	O
completely	O
convincing	O
because	O
it	O
is	O
not	O
complete	O
it	O
is	O
just	O
a	O
fragment	O
of	O
a	O
complete	O
mdp	O
can	O
there	O
really	O
be	O
a	O
complete	O
system	O
with	O
instability	O
a	O
simple	O
complete	O
example	O
of	O
divergence	O
is	O
baird	O
s	O
counterexample	O
consider	O
the	O
episodic	O
seven-state	O
two-action	O
mdp	O
shown	O
in	O
figure	O
the	O
dashed	O
action	B
takes	O
the	O
system	O
to	O
one	O
of	O
the	O
six	O
upper	O
states	O
with	O
equal	O
probability	O
whereas	O
the	O
solid	O
action	B
takes	O
the	O
system	O
to	O
the	O
seventh	O
state	B
the	O
behavior	O
policy	B
b	O
selects	O
the	O
dashed	O
and	O
solid	O
actions	O
with	O
probabilities	O
so	O
that	O
the	O
next-state	O
distribution	O
under	O
it	O
is	O
uniform	O
same	O
for	O
all	O
nonterminal	O
states	O
which	O
is	O
also	O
the	O
starting	O
distribution	O
for	O
each	O
episode	O
the	O
target	O
policy	B
always	O
takes	O
the	O
solid	O
action	B
and	O
so	O
the	O
on-policy	B
distribution	I
is	O
concentrated	O
in	O
the	O
seventh	O
state	B
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
the	O
discount	O
rate	O
is	O
and	O
consider	O
estimating	O
the	O
state-value	O
under	O
the	O
linear	O
parameterization	O
indicated	O
by	O
the	O
expression	O
shown	O
in	O
each	O
state	B
circle	O
for	O
example	O
the	O
estimated	O
value	B
of	O
the	O
leftmost	O
state	B
is	O
where	O
the	O
subscript	O
corresponds	O
to	O
the	O
component	O
of	O
the	O
figure	O
baird	O
s	O
counterexample	O
the	O
approximate	O
state-value	O
function	O
for	O
this	O
markov	O
process	O
is	O
of	O
the	O
form	O
shown	O
by	O
the	O
linear	O
expressions	O
inside	O
each	O
state	B
the	O
solid	O
action	B
usually	O
results	O
in	O
the	O
seventh	O
state	B
and	O
the	O
dashed	O
action	B
usually	O
results	O
in	O
one	O
of	O
the	O
other	O
six	O
states	O
each	O
with	O
equal	O
probability	O
the	O
reward	O
is	O
always	O
zero	O
chapter	O
methods	O
with	B
approximation	I
overall	O
weight	O
vector	B
w	O
this	O
corresponds	O
to	O
a	O
feature	O
vector	B
for	O
the	O
first	O
state	B
being	O
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
so	O
the	O
true	O
value	B
function	I
is	O
v	O
for	O
all	O
s	O
which	O
can	O
be	O
exactly	O
approximated	O
if	O
w	O
in	O
fact	O
there	O
are	O
many	O
solutions	O
as	O
there	O
are	O
more	O
components	O
to	O
the	O
weight	O
vector	B
than	O
there	O
are	O
nonterminal	O
states	O
moreover	O
the	O
set	O
of	O
feature	O
vectors	O
s	O
s	O
is	O
a	O
linearly	O
independent	O
set	O
in	O
all	O
these	O
ways	O
this	O
task	O
seems	O
a	O
favorable	O
case	O
for	O
linear	O
function	B
approximation	I
if	O
we	O
apply	O
semi-gradient	O
to	O
this	O
problem	O
then	O
the	O
weights	O
diverge	O
to	O
infinity	O
as	O
shown	O
in	O
figure	O
the	O
instability	O
occurs	O
for	O
any	O
positive	O
step	O
size	O
no	O
matter	O
how	O
small	O
in	O
fact	O
it	O
even	O
occurs	O
if	O
a	O
expected	B
update	I
is	O
done	O
as	O
in	O
dynamic	B
programming	I
if	O
we	O
do	O
a	O
dp-style	O
expected	B
update	I
instead	O
of	O
a	O
sample	O
update	O
as	O
shown	O
in	O
figure	O
that	O
is	O
if	O
the	O
weight	O
vector	B
wk	O
is	O
updated	O
for	O
all	O
states	O
all	O
at	O
the	O
same	O
time	O
in	O
a	O
semi-gradient	O
way	O
using	O
the	O
dp	O
target	O
wk	O
st	O
s	O
vswk	O
in	O
this	O
case	O
there	O
is	O
no	O
randomness	O
and	O
no	O
asynchrony	O
just	O
as	O
in	O
a	O
classical	O
dp	O
update	O
the	O
method	O
is	O
conventional	O
except	O
in	O
its	O
use	O
of	O
semi-gradient	O
function	B
approximation	I
yet	O
still	O
the	O
system	O
is	O
unstable	O
if	O
we	O
alter	O
just	O
the	O
distribution	O
of	O
dp	O
updates	O
in	O
baird	O
s	O
counterexample	O
from	O
the	O
uniform	O
distribution	O
to	O
the	O
on-policy	B
distribution	I
generally	O
requires	O
asynchronous	O
updating	O
then	O
convergence	O
is	O
guaranteed	O
to	O
a	O
solution	O
with	O
error	O
bounded	O
by	O
figure	O
demonstration	O
of	O
instability	O
on	O
baird	O
s	O
counterexample	O
shown	O
are	O
the	O
evolution	B
of	O
the	O
components	O
of	O
the	O
parameter	O
vector	B
w	O
of	O
the	O
two	O
semi-gradient	O
algorithms	O
the	O
step	O
size	O
was	O
and	O
the	O
initial	O
weights	O
were	O
w	O
off-policy	B
tdsemi-gradient	O
examples	O
of	O
off-policy	B
divergence	O
this	O
example	O
is	O
striking	O
because	O
the	O
td	B
and	O
dp	O
methods	O
used	O
are	O
arguably	O
the	O
simplest	O
and	O
best-understood	O
bootstrapping	B
methods	O
and	O
the	O
linear	O
semi-descent	O
method	O
used	O
is	O
arguably	O
the	O
simplest	O
and	O
best-understood	O
kind	O
of	O
function	B
approximation	I
the	O
example	O
shows	O
that	O
even	O
the	O
simplest	O
combination	O
of	O
bootstrapping	B
and	B
function	B
approximation	I
can	O
be	O
unstable	O
if	O
the	O
updates	O
are	O
not	O
done	O
according	O
to	O
the	O
on-policy	B
distribution	I
there	O
are	O
also	O
counterexamples	O
similar	O
to	O
baird	O
s	O
showing	O
divergence	O
for	B
q-learning	B
this	O
is	O
cause	O
for	O
concern	O
because	O
otherwise	O
q-learning	B
has	O
the	O
best	O
convergence	O
guarantees	O
of	O
all	O
control	B
methods	O
considerable	O
effort	O
has	O
gone	O
into	O
trying	O
to	O
find	O
a	O
remedy	O
to	O
this	O
problem	O
or	O
to	O
obtain	O
some	O
weaker	O
but	O
still	O
workable	O
guarantee	O
for	O
example	O
it	O
may	O
be	O
possible	O
to	O
guarantee	O
convergence	O
of	O
q-learning	B
as	O
long	O
as	O
the	O
behavior	O
policy	B
is	O
sufficiently	O
close	O
to	O
the	O
target	O
policy	B
for	O
example	O
when	O
it	O
is	O
the	O
policy	B
to	O
the	O
best	O
of	O
our	O
knowledge	O
q-learning	B
has	O
never	O
been	O
found	O
to	O
diverge	O
in	O
this	O
case	O
but	O
there	O
has	O
been	O
no	O
theoretical	O
analysis	O
in	O
the	O
rest	O
of	O
this	O
section	O
we	O
present	O
several	O
other	O
ideas	O
that	O
have	O
been	O
explored	O
suppose	O
that	O
instead	O
of	O
taking	O
just	O
a	O
step	O
toward	O
the	O
expected	B
one-step	O
return	B
on	O
each	O
iteration	O
as	O
in	O
baird	O
s	O
counterexample	O
we	O
actually	O
change	O
the	O
value	B
function	I
all	O
the	O
way	O
to	O
the	O
best	O
least-squares	O
approximation	O
would	O
this	O
solve	O
the	O
instability	O
problem	O
of	O
course	O
it	O
would	O
if	O
the	O
feature	O
vectors	O
s	O
s	O
formed	O
a	O
linearly	O
independent	O
set	O
as	O
they	O
do	O
in	O
baird	O
s	O
counterexample	O
because	O
then	O
exact	O
approximation	O
is	O
possible	O
on	O
each	O
iteration	O
and	O
the	O
method	O
reduces	O
to	O
standard	O
tabular	O
dp	O
but	O
of	O
course	O
the	O
point	O
here	O
is	O
to	O
consider	O
the	O
case	O
when	O
an	O
exact	O
solution	O
is	O
not	O
possible	O
in	O
this	O
case	O
stability	O
is	O
not	O
guaranteed	O
even	O
when	O
forming	O
the	O
best	O
approximation	O
at	O
each	O
iteration	O
as	O
shown	O
in	O
the	O
example	O
example	O
tsitsiklis	O
and	O
van	O
roy	O
s	O
counterexample	O
this	O
example	O
shows	O
that	O
linear	O
function	B
approximation	I
would	O
not	O
work	O
with	O
dp	O
even	O
if	O
the	O
least-squares	O
solution	O
was	O
found	O
at	O
each	O
step	O
the	O
counterexample	O
is	O
formed	O
by	O
extending	O
the	O
example	O
earlier	O
in	O
this	O
section	O
with	O
a	O
terminal	O
state	B
as	O
shown	O
to	O
the	O
right	O
as	O
before	O
the	O
estimated	O
value	B
of	O
the	O
first	O
state	B
is	O
w	O
and	O
the	O
estimated	O
value	B
of	O
the	O
second	O
state	B
is	O
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
so	O
the	O
true	O
values	O
are	O
zero	O
at	O
both	O
states	O
which	O
is	O
exactly	O
representable	O
with	O
w	O
if	O
we	O
set	O
at	O
each	O
step	O
so	O
as	O
to	O
minimize	O
the	O
ve	O
between	O
the	O
estimated	O
value	B
and	O
the	O
expected	B
one-step	O
return	B
then	O
we	O
have	O
arg	O
min	O
w	O
r	O
vsw	O
e	O
st	O
w	O
r	O
arg	O
min	O
wk	O
the	O
sequence	O
diverges	O
when	O
and	O
chapter	O
methods	O
with	B
approximation	I
another	O
way	O
to	O
try	O
to	O
prevent	O
instability	O
is	O
to	O
use	O
special	O
methods	O
for	O
function	B
approximation	I
in	O
particular	O
stability	O
is	O
guaranteed	O
for	O
function	B
approximation	I
methods	O
that	O
do	O
not	O
extrapolate	O
from	O
the	O
observed	O
targets	O
these	O
methods	O
called	O
averagers	B
include	O
nearest	O
neighbor	O
methods	O
and	O
locally	O
weighted	O
regression	O
but	O
not	O
popular	O
methods	O
such	O
as	O
tile	B
coding	I
and	O
artificial	B
neural	B
networks	I
exercise	O
apply	O
one-step	O
semi-gradient	O
q-learning	B
to	O
baird	O
s	O
terexample	O
and	O
show	O
empirically	O
that	O
it	O
s	O
weights	O
diverge	O
the	O
deadly	B
triad	I
our	O
discussion	O
so	O
far	O
can	O
be	O
summarized	O
by	O
saying	O
that	O
the	O
danger	O
of	O
instability	O
and	O
divergence	O
arises	O
whenever	O
we	O
combine	O
all	O
of	O
the	O
following	O
three	O
elements	O
making	O
up	O
what	O
we	O
call	O
the	O
deadly	B
triad	I
function	B
approximation	I
a	O
powerful	O
scalable	O
way	O
of	O
generalizing	O
from	O
a	O
state	B
space	O
much	O
larger	O
than	O
the	O
memory	O
and	O
computational	O
resources	O
linear	O
function	B
approximation	I
or	O
artificial	B
neural	B
networks	I
bootstrapping	B
update	O
targets	O
that	O
include	O
existing	O
estimates	O
in	O
dynamic	B
programming	I
or	O
td	B
methods	O
rather	O
than	O
relying	O
exclusively	O
on	O
actual	O
rewards	O
and	O
complete	O
returns	O
in	O
mc	O
methods	O
off-policy	B
training	O
training	O
on	O
a	O
distribution	O
of	O
transitions	O
other	O
than	O
that	O
produced	O
by	O
the	O
target	O
policy	B
sweeping	O
through	O
the	O
state	B
space	O
and	O
updating	O
all	O
states	O
uniformly	O
as	O
in	O
dynamic	B
programming	I
does	O
not	O
respect	O
the	O
target	O
policy	B
and	O
is	O
an	O
example	O
of	O
off-policy	B
training	O
in	O
particular	O
note	O
that	O
the	O
danger	O
is	O
not	O
due	O
to	O
control	B
or	O
to	O
generalized	O
policy	B
iteration	I
those	O
cases	O
are	O
more	O
complex	O
to	O
analyze	O
but	O
the	O
instability	O
arises	O
in	O
the	O
simpler	O
prediction	B
case	O
whenever	O
it	O
includes	O
all	O
three	O
elements	O
of	O
the	O
deadly	B
triad	I
the	O
danger	O
is	O
also	O
not	O
due	O
to	O
learning	O
or	O
to	O
uncertainties	O
about	O
the	O
environment	B
because	O
it	O
occurs	O
just	O
as	O
strongly	O
in	O
planning	B
methods	O
such	O
as	O
dynamic	B
programming	I
in	O
which	O
the	O
environment	B
is	O
completely	O
known	O
if	O
any	O
two	O
elements	O
of	O
the	O
deadly	B
triad	I
are	O
present	O
but	O
not	O
all	O
three	O
then	O
instability	O
can	O
be	O
avoided	O
it	O
is	O
natural	O
then	O
to	O
go	O
through	O
the	O
three	O
and	O
see	O
if	O
there	O
is	O
any	O
one	O
that	O
can	O
be	O
given	O
up	O
of	O
the	O
three	O
function	B
approximation	I
most	O
clearly	O
cannot	O
be	O
given	O
up	O
we	O
need	O
methods	O
that	O
scale	O
to	O
large	O
problems	O
and	O
to	O
great	O
expressive	O
power	O
we	O
need	O
at	O
least	O
linear	O
function	B
approximation	I
with	O
many	O
features	O
and	O
parameters	O
state	B
aggregation	I
or	O
nonparametric	O
methods	O
whose	O
complexity	O
grows	O
with	O
data	O
are	O
too	O
weak	O
or	O
too	O
expensive	O
least-squares	O
methods	O
such	O
as	O
lstd	O
are	O
of	O
quadratic	O
complexity	O
and	O
are	O
therefore	O
too	O
expensive	O
for	O
large	O
problems	O
doing	O
without	O
bootstrapping	B
is	O
possible	O
at	O
the	O
cost	O
of	O
computational	O
and	O
data	O
efficiency	O
perhaps	O
most	O
important	O
are	O
the	O
losses	O
in	O
computational	O
efficiency	O
monte	B
carlo	I
methods	I
require	O
memory	O
to	O
save	O
everything	O
that	O
happens	O
between	O
linear	O
value-function	O
geometry	O
making	O
each	O
prediction	B
and	O
obtaining	O
the	O
final	O
return	B
and	O
all	O
their	O
computation	O
is	O
done	O
once	O
the	O
final	O
return	B
is	O
obtained	O
the	O
cost	O
of	O
these	O
computational	O
issues	O
is	O
not	O
apparent	O
on	O
serial	O
von	O
neumann	O
computers	O
but	O
would	O
be	O
on	O
specialized	O
hardware	O
with	O
bootstrapping	B
and	B
eligibility	B
traces	I
data	O
can	O
be	O
dealt	O
with	O
when	O
and	O
where	O
it	O
is	O
generated	O
then	O
need	O
never	O
be	O
used	O
again	O
the	O
savings	O
in	O
communication	O
and	O
memory	O
made	O
possible	O
by	O
bootstrapping	B
are	O
great	O
the	O
losses	O
in	O
data	O
efficiency	O
by	O
giving	O
up	O
bootstrapping	B
are	O
also	O
significant	O
we	O
have	O
seen	O
this	O
repeatedly	O
such	O
as	O
in	O
chapters	O
and	O
where	O
some	O
degree	O
of	O
bootstrapping	B
performed	O
much	O
better	O
than	O
monte	B
carlo	I
methods	I
on	O
the	O
randomwalk	O
prediction	B
task	O
and	O
in	O
chapter	O
where	O
the	O
same	O
was	O
seen	O
on	O
the	O
mountain-car	O
control	B
task	O
many	O
other	O
problems	O
show	O
much	O
faster	O
learning	O
with	O
bootstrapping	B
see	O
figure	O
bootstrapping	B
often	O
results	O
in	O
faster	O
learning	O
because	O
it	O
allows	O
learning	O
to	O
take	O
advantage	O
of	O
the	O
state	B
property	O
the	O
ability	O
to	O
recognize	O
a	O
state	B
upon	O
returning	O
to	O
it	O
on	O
the	O
other	O
hand	O
bootstrapping	B
can	O
impair	O
learning	O
on	O
problems	O
where	O
the	O
state	B
representation	O
is	O
poor	O
and	O
causes	O
poor	O
generalization	O
this	O
seems	O
to	O
be	O
the	O
case	O
on	O
tetris	O
see	O
s	O
im	O
sek	O
alg	O
orta	O
and	O
kothiyal	O
a	O
poor	O
state	B
representation	O
can	O
also	O
result	O
in	O
bias	O
this	O
is	O
the	O
reason	O
for	O
the	O
poorer	O
bound	O
on	O
the	O
asymptotic	O
approximation	O
quality	O
of	O
bootstrapping	B
methods	O
on	O
balance	O
the	O
ability	O
to	O
bootstrap	O
has	O
to	O
be	O
considered	O
extremely	O
valuable	O
one	O
may	O
sometimes	O
choose	O
not	O
to	O
use	O
it	O
by	O
selecting	O
long	O
n-step	B
updates	O
a	O
large	O
bootstrapping	B
parameter	O
see	O
chapter	O
but	O
often	O
bootstrapping	B
greatly	O
increases	O
efficiency	O
it	O
is	O
an	O
ability	O
that	O
we	O
would	O
very	O
much	O
like	O
to	O
keep	O
in	O
our	O
toolkit	O
finally	O
there	O
is	O
off-policy	B
learning	O
can	O
we	O
give	O
that	O
up	O
on-policy	B
methods	I
are	O
often	O
adequate	O
for	O
model-free	O
reinforcement	B
learning	I
one	O
can	O
simply	O
use	O
sarsa	B
rather	O
than	O
q-learning	B
off-policy	B
methods	I
free	O
behavior	O
from	O
the	O
target	O
policy	B
this	O
could	O
be	O
considered	O
an	O
appealing	O
convenience	O
but	O
not	O
a	O
necessity	O
however	O
off-policy	B
learning	O
is	O
essential	O
to	O
other	O
anticipated	O
use	O
cases	O
cases	O
that	O
we	O
have	O
not	O
yet	O
mentioned	O
in	O
this	O
book	O
but	O
may	O
be	O
important	O
to	O
the	O
larger	O
goal	O
of	O
creating	O
a	O
powerful	O
intelligent	O
agent	O
in	O
these	O
use	O
cases	O
the	O
agent	O
learns	O
not	O
just	O
a	O
single	O
value	B
function	I
and	O
single	O
policy	B
but	O
large	O
numbers	O
of	O
them	O
in	O
parallel	O
there	O
is	O
extensive	O
psychological	O
evidence	O
that	O
people	O
and	O
animals	O
learn	O
to	O
predict	O
many	O
different	O
sensory	O
events	O
not	O
just	O
rewards	O
we	O
can	O
be	O
surprised	O
by	O
unusual	O
events	O
and	O
correct	O
our	O
predictions	O
about	O
them	O
even	O
if	O
they	O
are	O
of	O
neutral	O
valence	O
good	O
nor	O
bad	O
this	O
kind	O
of	O
prediction	B
presumably	O
underlies	O
predictive	O
models	O
of	O
the	O
world	O
such	O
as	O
are	O
used	O
in	O
planning	B
we	O
predict	O
what	O
we	O
will	O
see	O
after	O
eye	O
movements	O
how	O
long	O
it	O
will	O
take	O
to	O
walk	O
home	O
the	O
probability	O
of	O
making	O
a	O
jump	O
shot	O
in	O
basketball	O
and	O
the	O
satisfaction	O
we	O
will	O
get	O
from	O
taking	O
on	O
a	O
new	O
project	O
in	O
all	O
these	O
cases	O
the	O
events	O
we	O
would	O
like	O
to	O
predict	O
depend	O
on	O
our	O
acting	O
in	O
a	O
certain	O
way	O
to	O
learn	O
them	O
all	O
in	O
parallel	O
requires	O
learning	O
from	O
the	O
one	O
stream	O
of	O
experience	O
there	O
are	O
many	O
target	O
policies	O
and	O
thus	O
the	O
one	O
behavior	O
policy	B
cannot	O
equal	O
all	O
of	O
them	O
yet	O
parallel	O
learning	O
is	O
conceptually	O
possible	O
because	O
the	O
behavior	O
policy	B
may	O
overlap	O
in	O
part	O
with	O
many	O
of	O
the	O
target	O
policies	O
to	O
take	O
full	O
advantage	O
of	O
this	O
requires	O
off-policy	B
learning	O
chapter	O
methods	O
with	B
approximation	I
linear	O
value-function	O
geometry	O
to	O
better	O
understand	O
the	O
stability	O
challenge	O
of	O
off-policy	B
learning	O
it	O
is	O
helpful	O
to	O
think	O
about	O
value	B
function	B
approximation	I
more	O
abstractly	O
and	O
independently	O
of	O
how	O
learning	O
is	O
done	O
we	O
can	O
imagine	O
the	O
space	O
of	O
all	O
possible	O
state-value	O
functions	O
all	O
functions	O
from	O
states	O
to	O
real	O
numbers	O
v	O
s	O
r	O
most	O
of	O
these	O
value	B
functions	O
do	O
not	O
correspond	O
to	O
any	O
policy	B
more	O
important	O
for	O
our	O
purposes	O
is	O
that	O
most	O
are	O
not	O
representable	O
by	O
the	O
function	O
approximator	O
which	O
by	O
design	O
has	O
far	O
fewer	O
parameters	O
than	O
there	O
are	O
states	O
given	O
an	O
enumeration	O
of	O
the	O
state	B
space	O
s	O
ss	O
any	O
value	B
function	I
v	O
corresponds	O
to	O
a	O
vector	B
listing	O
the	O
value	B
of	O
each	O
state	B
in	O
order	O
this	O
vector	B
representation	O
of	O
a	O
value	B
function	I
has	O
as	O
many	O
components	O
as	O
there	O
are	O
states	O
in	O
most	O
cases	O
where	O
we	O
want	O
to	O
use	O
function	B
approximation	I
this	O
would	O
be	O
far	O
too	O
many	O
components	O
to	O
represent	O
the	O
vector	B
explicitly	O
nevertheless	O
the	O
idea	O
of	O
this	O
vector	B
is	O
conceptually	O
useful	O
in	O
the	O
following	O
we	O
treat	O
a	O
value	B
function	I
and	O
its	O
vector	B
representation	O
interchangably	O
to	O
develop	O
intuitions	O
consider	O
the	O
case	O
with	O
three	O
states	O
s	O
and	O
two	O
parameters	O
w	O
we	O
can	O
then	O
view	O
all	O
value	B
functionsvectors	O
as	O
points	O
in	O
a	O
three-dimensional	O
space	O
the	O
parameters	O
provide	O
an	O
alternative	O
coordinate	O
system	O
over	O
a	O
two-dimensional	O
subspace	O
any	O
weight	O
vector	B
w	O
is	O
a	O
point	O
in	O
the	O
two-dimensional	O
subspace	O
and	O
thus	O
also	O
a	O
complete	O
value	B
function	I
vw	O
that	O
assigns	O
values	O
to	O
all	O
three	O
states	O
with	O
general	O
function	B
approximation	I
the	O
relationship	O
between	O
the	O
full	O
space	O
and	O
the	O
subspace	O
of	O
representable	O
functions	O
could	O
be	O
complex	O
but	O
in	O
the	O
case	O
of	O
linear	O
value-function	B
approximation	I
the	O
subspace	O
is	O
a	O
simple	O
plane	O
as	O
suggested	O
by	O
figure	O
now	O
consider	O
a	O
single	O
fixed	O
policy	B
we	O
assume	O
that	O
its	O
true	O
value	B
function	I
v	O
is	O
too	O
complex	O
to	O
be	O
represented	O
exactly	O
as	O
an	O
approximation	O
thus	O
v	O
is	O
not	O
in	O
the	O
subspace	O
in	O
the	O
figure	O
it	O
is	O
depicted	O
as	O
being	O
above	O
the	O
planar	O
subspace	O
of	O
representable	O
functions	O
if	O
v	O
cannot	O
be	O
represented	O
exactly	O
what	O
representable	O
value	B
function	I
is	O
closest	O
to	O
it	O
this	O
turns	O
out	O
to	O
be	O
a	O
subtle	O
question	O
with	O
multiple	O
answers	O
to	O
begin	O
we	O
need	O
a	O
measure	O
of	O
the	O
distance	O
between	O
two	O
value	B
functions	O
given	O
two	O
value	B
functions	O
and	O
we	O
can	O
talk	O
about	O
the	O
vector	B
difference	O
between	O
them	O
v	O
if	O
v	O
is	O
small	O
then	O
the	O
two	O
value	B
functions	O
are	O
close	O
to	O
each	O
other	O
but	O
how	O
are	O
we	O
to	O
measure	O
the	O
size	O
of	O
this	O
difference	O
vector	B
the	O
conventional	O
euclidean	O
norm	O
is	O
not	O
appropriate	O
because	O
as	O
discussed	O
in	O
section	O
some	O
states	O
are	O
more	O
important	O
than	O
others	O
because	O
they	O
occur	O
more	O
frequently	O
or	O
because	O
we	O
are	O
more	O
interested	O
in	O
them	O
as	O
in	O
section	O
let	O
us	O
use	O
the	O
distribution	O
s	O
to	O
specify	O
the	O
degree	O
to	O
which	O
we	O
care	O
about	O
different	O
states	O
being	O
accurately	O
valued	O
taken	O
to	O
be	O
the	O
on-policy	B
distribution	I
we	O
can	O
then	O
define	O
the	O
distance	O
between	O
value	B
functions	O
using	O
the	O
norm	O
s	O
note	O
that	O
the	O
ve	O
from	O
section	O
can	O
be	O
written	O
simply	O
using	O
this	O
norm	O
as	O
vew	O
v	O
for	O
any	O
value	B
function	I
v	O
the	O
operation	O
of	O
finding	O
its	O
closest	O
value	B
function	I
in	O
the	O
subspace	O
of	O
representable	O
value	B
functions	O
is	O
a	O
projection	O
operation	O
we	O
define	O
a	O
linear	O
value-function	O
geometry	O
figure	O
the	O
geometry	O
of	O
linear	O
value-function	B
approximation	I
shown	O
is	O
the	O
threedimensional	O
space	O
of	O
all	O
value	B
functions	O
over	O
three	O
states	O
while	O
shown	O
as	O
a	O
plane	O
is	O
the	O
subspace	O
of	O
all	O
value	B
functions	O
representable	O
by	O
a	O
linear	O
function	O
approximator	O
with	O
parameter	O
w	O
the	O
true	O
value	B
function	I
v	O
is	O
in	O
the	O
larger	O
space	O
and	O
can	O
be	O
projected	O
down	O
the	O
subspace	O
using	O
a	O
projection	O
operator	O
to	O
its	O
best	O
approximation	O
in	O
the	O
value	B
error	I
sense	O
the	O
best	O
approximators	O
in	B
the	I
bellman	B
error	I
projected	B
bellman	B
error	I
and	O
temporal	O
difference	O
error	O
senses	O
are	O
all	O
potentially	O
different	O
and	O
are	O
shown	O
in	O
the	O
lower	O
right	O
be	O
and	O
pbe	O
are	O
all	O
treated	O
as	O
the	O
corresponding	O
vectors	O
in	O
this	O
figure	O
the	O
bellman	B
operator	I
takes	O
a	O
value	B
function	I
in	O
the	O
plane	O
to	O
one	O
outside	O
which	O
can	O
then	O
be	O
projected	O
back	O
if	O
you	O
iteratively	O
applied	O
the	O
bellman	B
operator	I
outside	O
the	O
space	O
in	O
gray	O
above	O
you	O
would	O
reach	O
the	O
true	O
value	B
function	I
as	O
in	O
conventional	O
dynamic	B
programming	I
if	O
instead	O
you	O
kept	O
projecting	O
back	O
into	O
the	O
subspace	O
at	O
each	O
step	O
as	O
in	O
the	O
lower	O
step	O
shown	O
in	O
gray	O
then	O
the	O
fixed	O
point	O
would	O
be	O
the	O
point	O
of	O
vector-zero	O
pbe	O
projection	O
operator	O
that	O
takes	O
an	O
arbitrary	O
value	B
function	I
to	O
the	O
representable	O
function	O
that	O
is	O
closest	O
in	O
our	O
norm	O
v	O
vw	O
where	O
w	O
arg	O
min	O
w	O
the	O
representable	O
value	B
function	I
that	O
is	O
closest	O
to	O
the	O
true	O
value	B
function	I
v	O
is	O
thus	O
its	O
projection	O
v	O
as	O
suggested	O
in	O
figure	O
this	O
is	O
the	O
solution	O
asymptotically	O
found	O
by	O
monte	B
carlo	I
methods	I
albeit	O
often	O
very	O
slowly	O
the	O
projection	O
operation	O
is	O
discussed	O
more	O
fully	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
td	B
methods	O
find	O
different	O
solutions	O
to	O
understand	O
their	O
rationale	O
recall	O
that	O
the	O
bellman	B
equation	I
for	O
value	B
function	I
v	O
is	O
v	O
rs	O
a	O
v	O
for	O
all	O
s	O
s	O
accordingtoastationarydecisionmakingpolicy	O
v	O
beingusedtoselectactions	O
thefunctionv	O
iscalledthestate-valuefunctionforpolicy	O
foragivenpolicy	O
criticmethods	O
ifthestatespaceisfinitethentheestimatedvaluefunctionmayberepresentedinacomputerasalargearraywithoneentryforeachstateandtheentriesdirectlyupdatedtoformtheestimate	O
suchtabularmethodscanhandlelargestatespacesevencontinuousonesthroughdiscretizationstateaggregationandinterpolationbutasthedimensionalityofthestatespaceincreasesthesemethodsrapidlybecomecomputationallyinfeasibleorine	O
ective	O
thisisthee	O
ectwhichgaverisetothephrase	O
thecurseofdimensionality	O
amoregeneralandflexibleapproachistorepresentthevaluefunctionbyafunctionalformoffixedsizeandfixedstructurewithmanyvariableparametersorweights	O
theweightsarethenchangedtoreshapetheapproximatevaluefunctiontobettermatchthetruevaluefunction	O
wedenotetheparameterizedvaluefunctionapproximatorasv	O
v	O
erentiablewithrespecttotheweights	O
forexampleitcouldbeacubicsplineoritcouldimplementedbyamulti-layerneuralnetworkwhere	O
istheconcatenationofalltheconnectionweights	O
henceforthreferto	O
exclusivelyastheweightsorweightvectorandreservetheword	O
parameter	O
forthingslikethediscount-rateparameterandstep-sizeparameters	O
animportantspecialcaseisthatinwhichtheapproximatevaluefunctionislinearintheweightsandinfeaturesofthestatev	O
subspace	O
of	O
all	O
value	B
functions	O
representable	O
as	O
bellman	B
error	I
vector	B
v	O
istheuniquesolutiontothebellmanequationthebellmanequationcanbeviewedasanalternatewayofdefiningv	O
vs	O
thediscrepancybetweenthetwosidesofthebellmanequationv	O
v	O
slengthinthed-metric	O
thatistominimizethemean-squaredbellmanerrorbe	O
v	O
isnotrepresentablethenitisnotbepossibletoreducethebellmanerrortozero	O
foranyv	O
v	O
b	O
v	O
tbesolvedexactlyyoucanminimizethemean-squaredprojectedbellmanerrorpbe	O
v	O
v	O
r	O
v	O
v	O
vev	O
pbev	O
v	O
istheuniquesolutiontothebellmanequationandinthissensethebellmanequationcanbeviewedasanalternatewayofdefiningv	O
notequaltov	O
b	O
v	O
v	O
b	O
v	O
b	O
v	O
error	O
vw	O
b	O
space	O
of	O
all	O
value	B
functions	O
over	O
states	O
chapter	O
methods	O
with	B
approximation	I
the	O
projection	O
matrix	O
for	O
a	O
linear	O
function	O
approximator	O
the	O
projection	O
operation	O
is	O
linear	O
which	O
implies	O
that	O
it	O
can	O
be	O
represented	O
as	O
an	O
matrix	O
where	O
as	O
in	O
section	O
d	O
denotes	O
the	O
diagonal	O
matrix	O
with	O
the	O
on	O
the	O
diagonal	O
and	O
x	O
denotes	O
the	O
d	O
matrix	O
whose	O
rows	O
are	O
the	O
feature	O
vectors	O
one	O
for	O
each	O
state	B
s	O
if	O
the	O
inverse	O
in	O
does	O
not	O
exist	O
then	O
the	O
pseudoinverse	O
is	O
substituted	O
using	O
these	O
matrices	O
the	O
norm	O
of	O
a	O
vector	B
can	O
be	O
written	O
and	O
the	O
approximate	O
linear	O
value	B
function	I
can	O
be	O
written	O
vw	O
xw	O
the	O
true	O
value	B
function	I
v	O
is	O
the	O
only	O
value	B
function	I
that	O
solves	O
exactly	O
if	O
an	O
approximate	O
value	B
function	I
vw	O
were	O
substituted	O
for	O
v	O
the	O
difference	O
between	O
the	O
right	O
and	O
left	O
sides	O
of	O
the	O
modified	O
equation	O
could	O
be	O
used	O
as	O
a	O
measure	O
of	O
how	O
far	O
off	O
vw	O
is	O
from	O
v	O
we	O
call	O
this	O
the	O
bellman	B
error	I
at	O
state	B
s	O
ws	O
rs	O
a	O
vws	O
st	O
s	O
at	O
which	O
shows	O
clearly	O
the	O
relationship	O
of	O
the	O
bellman	B
error	I
to	O
the	O
td	B
error	I
the	O
bellman	B
error	I
is	O
the	O
expectation	O
of	O
the	O
td	B
error	I
the	O
vector	B
of	O
all	O
the	O
bellman	B
errors	O
at	O
all	O
states	O
w	O
rs	O
is	O
called	O
the	O
bellman	B
error	I
vector	B
as	O
be	O
in	O
figure	O
the	O
overall	O
size	O
of	O
this	O
vector	B
in	O
the	O
norm	O
is	O
an	O
overall	O
measure	O
of	O
the	O
error	O
in	O
the	O
value	B
function	I
called	O
the	O
mean	O
squared	O
bellman	B
error	I
bew	O
it	O
is	O
not	O
possible	O
in	O
general	O
to	O
reduce	O
the	O
be	O
to	O
zero	O
which	O
point	O
vw	O
v	O
but	O
for	O
linear	O
function	B
approximation	I
there	O
is	O
a	O
unique	O
value	B
of	O
w	O
for	O
which	O
the	O
be	O
is	O
minimized	O
this	O
point	O
in	O
the	O
representable-function	O
subspace	O
min	O
be	O
in	O
figure	O
is	O
different	O
in	O
general	O
from	O
that	O
which	O
minimizes	O
the	O
ve	O
as	O
v	O
methods	O
that	O
seek	O
to	O
minimize	O
the	O
be	O
are	O
discussed	O
in	O
the	O
next	O
two	O
sections	O
the	O
bellman	B
error	I
vector	B
is	O
shown	O
in	O
figure	O
as	O
the	O
result	O
of	O
applying	O
the	O
bellman	B
operator	I
b	O
rs	O
rs	O
to	O
the	O
approximate	O
value	B
function	I
the	O
bellman	B
operator	I
is	O
gradient	B
descent	O
in	B
the	I
bellman	B
error	I
defined	O
by	O
vs	O
rs	O
a	O
for	O
all	O
s	O
s	O
and	O
v	O
s	O
r	O
the	O
bellman	B
error	I
vector	B
for	O
v	O
can	O
be	O
written	O
w	O
b	O
vw	O
vw	O
if	O
the	O
bellman	B
operator	I
is	O
applied	O
to	O
a	O
value	B
function	I
in	O
the	O
representable	O
subspace	O
then	O
in	O
general	O
it	O
will	O
produce	O
a	O
new	O
value	B
function	I
that	O
is	O
outside	O
the	O
subspace	O
as	O
suggested	O
in	O
the	O
figure	O
in	O
dynamic	B
programming	I
function	B
approximation	I
this	O
operator	O
is	O
applied	O
repeatedly	O
to	O
the	O
points	O
outside	O
the	O
representable	O
space	O
as	O
suggested	O
by	O
the	O
gray	O
arrows	O
in	O
the	O
top	O
of	O
figure	O
eventually	O
that	O
process	O
converges	O
to	O
the	O
true	O
value	B
function	I
v	O
the	O
only	O
fixed	O
point	O
for	O
the	O
bellman	B
operator	I
the	O
only	O
value	B
function	I
for	O
which	O
v	O
b	O
v	O
which	O
is	O
just	O
another	O
way	O
of	O
writing	O
the	O
bellman	B
equation	I
for	O
with	B
function	B
approximation	I
however	O
the	O
intermediate	O
value	B
functions	O
lying	O
outside	O
the	O
subspace	O
cannot	O
be	O
represented	O
the	O
gray	O
arrows	O
in	O
the	O
upper	O
part	O
of	O
figure	O
cannot	O
be	O
followed	O
because	O
after	O
the	O
first	O
update	O
line	O
the	O
value	B
function	I
must	O
be	O
projected	O
back	O
into	O
something	O
representable	O
the	O
next	O
iteration	O
then	O
begins	O
within	O
the	O
subspace	O
the	O
value	B
function	I
is	O
again	O
taken	O
outside	O
of	O
the	O
subspace	O
by	O
the	O
bellman	B
operator	I
and	O
then	O
mapped	O
back	O
by	O
the	O
projection	O
operator	O
as	O
suggested	O
by	O
the	O
lower	O
gray	O
arrow	O
and	O
line	O
following	O
these	O
arrows	O
is	O
a	O
dp-like	O
process	O
with	B
approximation	I
in	O
this	O
case	O
we	O
are	O
interested	O
in	O
the	O
projection	O
of	O
the	O
bellman	B
error	I
vector	B
back	O
into	O
the	O
representable	O
space	O
this	O
is	O
the	O
projected	B
bellman	B
error	I
vector	B
vw	O
shown	O
in	O
figure	O
as	O
pbe	O
the	O
size	O
of	O
this	O
vector	B
in	O
the	O
norm	O
is	O
another	O
measure	O
of	O
error	O
in	O
the	O
approximate	O
value	B
function	I
for	O
any	O
approximate	O
value	B
function	I
v	O
we	O
define	O
the	O
mean	O
square	O
projected	B
bellman	B
error	I
denoted	O
pbe	O
as	O
pbew	O
with	O
linear	O
function	B
approximation	I
there	O
always	O
exists	O
an	O
approximate	O
value	B
function	I
the	O
subspace	O
with	O
zero	O
pbe	O
this	O
is	O
the	O
td	B
fixed	O
point	O
wtd	O
introduced	O
in	O
section	O
as	O
we	O
have	O
seen	O
this	O
point	O
is	O
not	O
always	O
stable	O
under	O
semi-gradient	O
td	B
methods	O
and	O
off-policy	B
training	O
as	O
shown	O
in	O
the	O
figure	O
this	O
value	B
function	I
is	O
generally	O
different	O
from	O
those	O
minimizing	O
ve	O
or	O
be	O
methods	O
that	O
are	O
guaranteed	O
to	O
converge	O
to	O
it	O
are	O
discussed	O
in	O
sections	O
and	O
gradient	B
descent	O
in	B
the	I
bellman	B
error	I
armed	O
with	O
a	O
better	O
understanding	O
of	O
value	B
function	B
approximation	I
and	O
its	O
various	O
objectives	O
we	O
return	B
now	O
to	O
the	O
challenge	O
of	O
stability	O
in	O
off-policy	B
learning	O
we	O
would	O
like	O
to	O
apply	O
the	O
approach	O
of	O
stochastic	O
gradient	B
descent	O
section	O
in	O
which	O
updates	O
chapter	O
methods	O
with	B
approximation	I
are	O
made	O
that	O
in	O
expectation	O
are	O
equal	O
to	O
the	O
negative	O
gradient	B
of	O
an	O
objective	O
function	O
these	O
methods	O
always	O
go	O
downhill	O
expectation	O
in	O
the	O
objective	O
and	O
because	O
of	O
this	O
are	O
typically	O
stable	O
with	O
excellent	O
convergence	O
properties	O
among	O
the	O
algorithms	O
investigated	O
so	O
far	O
in	O
this	O
book	O
only	O
the	O
monte	B
carlo	I
methods	I
are	O
true	O
sgd	O
methods	O
these	O
methods	O
converge	O
robustly	O
under	O
both	O
on-policy	O
and	O
off-policy	B
training	O
as	O
well	O
as	O
for	O
general	O
nonlinear	O
function	O
approximators	O
though	O
they	O
are	O
often	O
slower	O
than	O
semi-gradient	B
methods	I
with	O
bootstrapping	B
which	O
are	O
not	O
sgd	O
methods	O
semi-gradient	B
methods	I
may	O
diverge	O
under	O
off-policy	B
training	O
as	O
we	O
have	O
seen	O
earlier	O
in	O
this	O
chapter	O
and	O
under	O
contrived	O
cases	O
of	O
nonlinear	O
function	B
approximation	I
and	O
van	O
roy	O
with	O
a	O
true	O
sgd	O
method	O
such	O
divergence	O
would	O
not	O
be	O
possible	O
the	O
appeal	O
of	O
sgd	O
is	O
so	O
strong	O
that	O
great	O
effort	O
has	O
gone	O
into	O
finding	O
a	O
practical	O
way	O
of	O
harnessing	O
it	O
for	O
reinforcement	B
learning	I
the	O
starting	O
place	O
of	O
all	O
such	O
efforts	O
is	O
the	O
choice	O
of	O
an	O
error	O
or	O
objective	O
function	O
to	O
optimize	O
in	O
this	O
and	O
the	O
next	O
section	O
we	O
explore	O
the	O
origins	O
and	O
limits	O
of	O
the	O
most	O
popular	O
proposed	O
objective	O
function	O
that	O
based	O
on	O
the	O
bellman	B
error	I
introduced	O
in	O
the	O
previous	O
section	O
although	O
this	O
has	O
been	O
a	O
popular	O
and	O
influential	O
approach	O
the	O
conclusion	O
that	O
we	O
reach	O
here	O
is	O
that	O
it	O
is	O
a	O
misstep	O
and	O
yields	O
no	O
good	O
learning	O
algorithms	O
on	O
the	O
other	O
hand	O
this	O
approach	O
fails	O
in	O
an	O
interesting	O
way	O
that	O
offers	O
insight	O
into	O
what	O
might	O
constitute	O
a	O
good	O
approach	O
to	O
begin	O
let	O
us	O
consider	O
not	O
the	O
bellman	B
error	I
but	O
something	O
more	O
immediate	O
and	O
naive	B
temporal	O
difference	O
learning	O
is	O
driven	O
by	O
the	O
td	B
error	I
why	O
not	O
take	O
the	O
minimization	O
of	O
the	O
expected	B
square	O
of	O
the	O
td	B
error	I
as	O
the	O
objective	O
in	O
the	O
general	O
functionapproximation	O
case	O
the	O
one-step	O
td	B
error	I
with	O
discounting	B
is	O
t	O
vstwt	O
a	O
possible	O
objective	O
function	O
then	O
is	O
what	O
one	O
might	O
call	O
the	O
mean	O
squared	O
td	B
error	I
tdew	O
s	O
s	O
t	O
t	O
t	O
st	O
s	O
at	O
t	O
st	O
s	O
at	O
is	O
the	O
distribution	O
encountered	O
under	O
b	O
the	O
last	O
equation	O
is	O
of	O
the	O
form	O
needed	O
for	O
sgd	O
it	O
gives	O
the	O
objective	O
as	O
an	O
expectation	O
that	O
can	O
be	O
sampled	O
from	O
experience	O
the	O
experience	O
is	O
due	O
to	O
the	O
behavior	O
policy	B
b	O
thus	O
following	O
the	O
standard	O
sgd	O
approach	O
one	O
can	O
derive	O
the	O
per-step	O
update	O
based	O
on	O
a	O
sample	O
of	O
this	O
expected	B
value	B
wt	O
t	O
t	O
wt	O
t	O
t	O
t	O
wt	O
t	O
vstwt	O
which	O
you	O
will	O
recognize	O
as	O
the	O
same	O
as	O
the	O
semi-gradient	O
td	B
algorithm	O
except	O
for	O
the	O
additional	O
final	O
term	O
this	O
term	O
completes	O
the	O
gradient	B
and	O
makes	O
this	O
a	O
true	O
sgd	O
gradient	B
descent	O
in	B
the	I
bellman	B
error	I
algorithm	O
with	O
excellent	O
convergence	O
guarantees	O
let	O
us	O
call	O
this	O
algorithm	O
the	O
naive	B
residual-gradient	B
algorithm	I
baird	O
although	O
the	O
naive	B
residual-gradient	B
algorithm	I
converges	O
robustly	O
it	O
does	O
not	O
necessarily	O
converge	O
to	O
a	O
desirable	O
place	O
example	O
a-split	O
example	O
showing	O
the	O
naivet	O
e	O
of	O
the	O
naive	B
residual-gradient	B
algorithm	I
consider	O
the	O
three-state	O
episodic	O
mrp	O
shown	O
to	O
the	O
right	O
episodes	B
begin	O
in	O
state	B
a	O
and	O
then	O
split	O
stochastically	O
half	O
the	O
time	O
going	O
to	O
b	O
then	O
invariably	O
going	O
on	O
to	O
terminate	O
with	O
a	O
reward	O
of	O
and	O
half	O
the	O
time	O
going	O
to	O
state	B
c	O
then	O
invariably	O
terminating	O
with	O
a	O
reward	O
of	O
zero	O
reward	O
for	O
the	O
first	O
transition	O
out	O
of	O
a	O
is	O
always	O
zero	O
whichever	O
way	O
the	O
episode	O
goes	O
as	O
this	O
is	O
an	O
episodic	O
problem	O
we	O
can	O
take	O
to	O
be	O
we	O
also	O
assume	O
on-policy	O
training	O
so	O
that	O
t	O
is	O
always	O
and	O
tabular	O
function	B
approximation	I
so	O
that	O
the	O
learning	O
algorithms	O
are	O
free	O
to	O
give	O
arbitrary	O
independent	O
values	O
to	O
all	O
three	O
states	O
thus	O
this	O
should	O
be	O
an	O
easy	O
problem	O
what	O
should	O
the	O
values	O
be	O
from	O
a	O
half	O
the	O
time	O
the	O
return	B
is	O
and	O
half	O
the	O
time	O
the	O
return	B
is	O
a	O
should	O
have	O
value	B
from	O
b	O
the	O
return	B
is	O
always	O
so	O
its	O
value	B
should	O
be	O
and	O
similarly	O
from	O
c	O
the	O
return	B
is	O
always	O
so	O
its	O
value	B
should	O
be	O
these	O
are	O
the	O
true	O
values	O
and	O
as	O
this	O
is	O
a	O
tabular	O
problem	O
all	O
the	O
methods	O
presented	O
previously	O
converge	O
to	O
them	O
exactly	O
however	O
the	O
naive	B
residual-gradient	B
algorithm	I
finds	O
different	O
values	O
for	O
b	O
and	O
c	O
converges	O
it	O
converges	O
with	O
b	O
having	O
a	O
value	B
of	O
correctly	O
to	O
and	O
c	O
having	O
a	O
value	B
of	O
these	O
are	O
in	O
fact	O
the	O
values	O
that	O
minimize	O
the	O
tde	O
to	O
b	O
s	O
to	O
c	O
s	O
a	O
change	O
of	O
or	O
down	O
from	O
a	O
s	O
let	O
us	O
compute	O
the	O
tde	O
for	O
these	O
values	O
the	O
first	O
transition	O
of	O
each	O
episode	O
is	O
a	O
change	O
because	O
the	O
reward	O
is	O
zero	O
on	O
these	O
transitions	O
and	O
these	O
changes	O
are	O
on	O
the	O
first	O
transition	O
to	O
a	O
reward	O
of	O
a	O
to	O
a	O
reward	O
of	O
with	O
a	O
terminal	O
on	O
the	O
either	O
up	O
from	O
a	O
s	O
of	O
the	O
td	B
errors	O
and	O
thus	O
the	O
squared	O
td	B
error	I
is	O
always	O
the	O
second	O
transition	O
is	O
similar	O
it	O
is	O
either	O
up	O
from	O
b	O
s	O
terminal	O
state	B
of	O
value	B
or	O
down	O
from	O
c	O
s	O
state	B
of	O
value	B
thus	O
the	O
td	B
error	I
is	O
always	O
second	O
step	O
thus	O
for	O
this	O
set	O
of	O
values	O
the	O
tde	O
on	O
both	O
steps	O
is	O
for	O
a	O
squared	O
error	O
of	O
up	O
to	O
at	O
b	O
or	O
from	O
now	O
let	O
s	O
compute	O
the	O
tde	O
for	O
the	O
true	O
values	O
at	O
c	O
at	O
and	O
a	O
at	O
in	O
this	O
case	O
the	O
first	O
transition	O
is	O
either	O
from	O
down	O
to	O
at	O
c	O
in	O
either	O
case	O
the	O
absolute	O
error	O
is	O
the	O
second	O
transition	O
has	O
zero	O
error	O
because	O
the	O
starting	O
value	B
either	O
or	O
depending	O
on	O
whether	O
the	O
transition	O
is	O
from	O
b	O
or	O
c	O
always	O
exactly	O
matches	O
the	O
immediate	O
reward	O
and	O
return	B
thus	O
the	O
squared	O
td	B
error	I
is	O
on	O
the	O
first	O
transition	O
and	O
on	O
the	O
second	O
for	O
a	O
mean	O
reward	O
over	O
the	O
two	O
transitions	O
of	O
this	O
solution	O
is	O
worse	O
according	O
to	O
the	O
tde	O
on	O
this	O
simple	O
problem	O
the	O
true	O
values	O
do	O
not	O
have	O
the	O
smallest	O
tde	O
and	O
the	O
squared	O
error	O
is	O
is	O
bigger	O
that	O
as	O
chapter	O
methods	O
with	B
approximation	I
a	O
tabular	O
representation	O
is	O
used	O
in	O
the	O
a-split	O
example	O
so	O
the	O
true	O
state	B
values	O
can	O
be	O
exactly	O
represented	O
yet	O
the	O
naive	B
residual-gradient	B
algorithm	I
finds	O
different	O
values	O
and	O
these	O
values	O
have	O
lower	O
tde	O
than	O
do	O
the	O
true	O
values	O
minimizing	O
the	O
tde	O
is	O
naive	B
by	O
penalizing	O
all	O
td	B
errors	O
it	O
achieves	O
something	O
more	O
like	O
temporal	O
smoothing	O
than	O
accurate	O
prediction	B
a	O
better	O
idea	O
would	O
seem	O
to	O
be	O
minimizing	O
the	O
bellman	B
error	I
if	O
the	O
exact	O
values	O
are	O
learned	O
the	O
bellman	B
error	I
is	O
zero	O
everywhere	O
thus	O
a	O
bellman-error-minimizing	O
algorithm	O
should	O
have	O
no	O
trouble	O
with	O
the	O
a-split	O
example	O
we	O
cannot	O
expect	O
to	O
achieve	O
zero	O
bellman	B
error	I
in	O
general	O
as	O
it	O
would	O
involve	O
finding	O
the	O
true	O
value	B
function	I
which	O
we	O
presume	O
is	O
outside	O
the	O
space	O
of	O
representable	O
value	B
functions	O
but	O
getting	O
close	O
to	O
this	O
ideal	O
is	O
a	O
natural-seeming	O
goal	O
as	O
we	O
have	O
seen	O
the	O
bellman	B
error	I
is	O
also	O
closely	O
related	O
to	O
the	O
td	B
error	I
the	O
bellman	B
error	I
for	O
a	O
state	B
is	O
the	O
expected	B
td	B
error	I
in	O
that	O
state	B
so	O
let	O
s	O
repeat	O
the	O
derivation	O
above	O
with	O
the	O
expected	B
td	B
error	I
expectations	O
here	O
are	O
implicitly	O
conditional	O
on	O
st	O
t	O
wt	O
wt	O
wt	O
eb	O
t	O
t	O
eb	O
t	O
t	O
wt	O
eb	O
t	O
t	O
wt	O
vstw	O
t	O
this	O
update	O
and	O
various	O
ways	O
of	O
sampling	O
it	O
are	O
referred	O
to	O
as	O
the	O
residual-gradient	B
algorithm	I
if	O
you	O
simply	O
used	O
the	O
sample	O
values	O
in	O
all	O
the	O
expectations	O
then	O
the	O
equation	O
above	O
reduces	O
almost	O
exactly	O
to	O
the	O
naive	B
residual-gradient	O
but	O
this	O
is	O
naive	B
because	O
the	O
equation	O
above	O
involves	O
the	O
next	O
state	B
appearing	O
in	O
two	O
expectations	O
that	O
are	O
multiplied	O
together	O
to	O
get	O
an	O
unbiased	O
sample	O
of	O
the	O
product	O
two	O
independent	O
samples	O
of	O
the	O
next	O
state	B
are	O
required	O
but	O
during	O
normal	O
interaction	O
with	O
an	O
external	O
environment	B
only	O
one	O
is	O
obtained	O
one	O
expectation	O
or	O
the	O
other	O
can	O
be	O
sampled	O
but	O
not	O
both	O
there	O
are	O
two	O
ways	O
to	O
make	O
the	O
residual-gradient	B
algorithm	I
work	O
one	O
is	O
in	O
the	O
case	O
of	O
deterministic	O
environments	O
if	O
the	O
transition	O
to	O
the	O
next	O
state	B
is	O
deterministic	O
then	O
the	O
two	O
samples	O
will	O
necessarily	O
be	O
the	O
same	O
and	O
the	O
naive	B
algorithm	O
is	O
valid	O
the	O
other	O
way	O
is	O
to	O
obtain	O
two	O
independent	O
samples	O
of	O
the	O
next	O
state	B
from	O
st	O
one	O
for	O
the	O
first	O
expectation	O
and	O
another	O
for	O
the	O
second	O
expectation	O
in	O
real	O
interaction	O
with	O
an	O
environment	B
this	O
would	O
not	O
seem	O
possible	O
but	O
when	O
interacting	O
with	O
a	O
simulated	O
environment	B
it	O
is	O
one	O
simply	O
rolls	O
back	O
to	O
the	O
previous	O
state	B
and	O
obtains	O
an	O
alternate	O
next	O
state	B
before	O
proceeding	O
forward	O
from	O
the	O
first	O
next	O
state	B
in	O
either	O
of	O
these	O
cases	O
the	O
residual-gradient	B
algorithm	I
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
minimum	O
of	O
the	O
be	O
under	O
the	O
usual	O
conditions	O
on	O
the	O
step-size	B
parameter	I
as	O
a	O
true	O
sgd	O
method	O
this	O
convergence	O
state	B
values	O
there	O
remains	O
a	O
small	O
difference	O
in	O
the	O
treatment	O
of	O
the	O
importance	B
sampling	I
ratio	B
t	O
in	O
the	O
analagous	O
action-value	O
case	O
is	O
the	O
most	O
important	O
case	O
for	O
control	B
algorithms	O
the	O
residual-gradient	B
algorithm	I
would	O
reduce	O
exactly	O
to	O
the	O
naive	B
version	O
gradient	B
descent	O
in	B
the	I
bellman	B
error	I
is	O
robust	O
applying	O
to	O
both	O
linear	O
and	O
nonlinear	O
function	O
approximators	O
in	O
the	O
linear	O
case	O
convergence	O
is	O
always	O
to	O
the	O
unique	O
w	O
that	O
minimizes	O
the	O
be	O
however	O
there	O
remain	O
at	O
least	O
three	O
ways	O
in	O
which	O
the	O
convergence	O
of	O
the	O
residualgradient	O
method	O
is	O
unsatisfactory	O
the	O
first	O
of	O
these	O
is	O
that	O
empirically	O
it	O
is	O
slow	O
much	O
slower	O
that	O
semi-gradient	B
methods	I
indeed	O
proponents	O
of	O
this	O
method	O
have	O
proposed	O
increasing	O
its	O
speed	O
by	O
combining	O
it	O
with	O
faster	O
semi-gradient	B
methods	I
initially	O
then	O
gradually	O
switching	O
over	O
to	O
residual	O
gradient	B
for	O
the	O
convergence	O
guarantee	O
and	O
moore	O
the	O
second	O
way	O
in	O
which	O
the	O
residual-gradient	B
algorithm	I
is	O
unsatisfactory	O
is	O
that	O
it	O
still	O
seems	O
to	O
converge	O
to	O
the	O
wrong	O
values	O
it	O
does	O
get	O
the	O
right	O
values	O
in	O
all	O
tabular	O
cases	O
such	O
as	O
the	O
a-split	O
example	O
as	O
for	O
those	O
an	O
exact	O
solution	O
to	O
the	O
bellman	B
equation	I
is	O
possible	O
but	O
if	O
we	O
examine	O
examples	O
with	O
genuine	O
function	B
approximation	I
example	O
a-presplit	O
example	O
a	O
counterexample	O
for	O
the	O
be	O
consider	O
the	O
three-state	O
episodic	O
mrp	O
shown	O
to	O
the	O
right	O
episodes	B
start	O
in	O
either	O
or	O
with	O
equal	O
probability	O
these	O
two	O
states	O
look	O
exactly	O
the	O
same	O
to	O
the	O
function	O
approximator	O
like	O
a	O
single	O
state	B
a	O
whose	O
feature	O
representation	O
is	O
distinct	O
from	O
and	O
unrelated	O
to	O
the	O
feature	O
representation	O
of	O
the	O
other	O
two	O
states	O
b	O
and	O
c	O
which	O
are	O
also	O
distinct	O
from	O
each	O
other	O
specifically	O
the	O
parameter	O
of	O
the	O
function	O
approximator	O
has	O
three	O
components	O
one	O
giving	O
the	O
value	B
of	O
state	B
b	O
one	O
giving	O
the	O
value	B
of	O
state	B
c	O
and	O
one	O
giving	O
the	O
value	B
of	O
both	O
states	O
and	O
other	O
than	O
the	O
selection	O
of	O
the	O
initial	O
state	B
the	O
system	O
is	O
deterministic	O
if	O
it	O
starts	O
in	O
then	O
it	O
transitions	O
to	O
b	O
with	O
a	O
reward	O
of	O
and	O
then	O
on	O
to	O
termination	O
with	O
a	O
reward	O
of	O
if	O
it	O
starts	O
in	O
then	O
it	O
transitions	O
to	O
c	O
and	O
then	O
to	O
termination	O
with	O
both	O
rewards	O
zero	O
to	O
a	O
learning	O
algorithm	O
seeing	O
only	O
the	O
features	O
the	O
system	O
looks	O
identical	O
to	O
the	O
a-split	O
example	O
the	O
system	O
seems	O
to	O
always	O
start	O
in	O
a	O
followed	O
by	O
either	O
b	O
or	O
c	O
with	O
equal	O
probability	O
and	O
then	O
terminating	O
with	O
a	O
or	O
a	O
depending	O
deterministically	O
on	O
the	O
previous	O
state	B
as	O
in	O
the	O
a-split	O
example	O
the	O
true	O
values	O
of	O
b	O
and	O
c	O
are	O
and	O
and	O
the	O
best	O
shared	O
value	B
of	O
and	O
is	O
by	O
symmetry	O
because	O
this	O
problem	O
appears	O
externally	O
identical	O
to	O
the	O
a-split	O
example	O
we	O
already	O
know	O
what	O
values	O
will	O
be	O
found	O
by	O
the	O
algorithms	O
semi-gradient	O
td	B
converges	O
to	O
the	O
ideal	O
values	O
just	O
mentioned	O
while	O
the	O
naive	B
residual-gradient	B
algorithm	I
converges	O
to	O
values	O
of	O
for	O
b	O
and	O
c	O
respectively	O
all	O
state	B
transitions	O
are	O
deterministic	O
so	O
the	O
non-naive	O
residual-gradient	B
algorithm	I
will	O
also	O
converge	O
to	O
these	O
values	O
is	O
the	O
same	O
algorithm	O
in	O
this	O
case	O
it	O
follows	O
then	O
that	O
this	O
naive	B
solution	O
must	O
also	O
be	O
the	O
one	O
that	O
minimizes	O
the	O
be	O
and	O
so	O
it	O
is	O
on	O
a	O
deterministic	O
problem	O
the	O
bellman	B
errors	O
and	O
td	B
errors	O
are	O
all	O
the	O
same	O
so	O
the	O
be	O
is	O
always	O
the	O
same	O
as	O
the	O
tde	O
optimizing	O
the	O
be	O
on	O
this	O
example	O
gives	O
rise	O
to	O
the	O
same	O
failure	O
mode	O
as	O
with	O
the	O
naive	B
residual-gradient	B
algorithm	I
on	O
the	O
a-split	O
example	O
and	O
chapter	O
methods	O
with	B
approximation	I
then	O
the	O
residual-gradient	B
algorithm	I
and	O
indeed	O
the	O
be	O
objective	O
seem	O
to	O
find	O
the	O
wrong	O
value	B
functions	O
one	O
of	O
the	O
most	O
telling	O
such	O
examples	O
is	O
the	O
variation	O
on	O
the	O
a-split	O
example	O
known	O
as	O
the	O
a-presplit	O
example	O
shown	O
on	O
the	O
preceding	O
page	O
in	O
which	O
the	O
residual-gradient	B
algorithm	I
finds	O
the	O
same	O
poor	O
solution	O
as	O
its	O
naive	B
version	O
this	O
example	O
shows	O
intuitively	O
that	O
minimizing	O
the	O
be	O
the	O
residual-gradient	B
algorithm	I
surely	O
does	O
may	O
not	O
be	O
a	O
desirable	O
goal	O
the	O
third	O
way	O
in	O
which	O
the	O
convergence	O
of	O
the	O
residual-gradient	B
algorithm	I
is	O
not	O
satisfactory	O
is	O
explained	O
in	O
the	O
next	O
section	O
like	O
the	O
second	O
way	O
the	O
third	O
way	O
is	O
also	O
a	O
problem	O
with	O
the	O
be	O
objective	O
itself	O
rather	O
than	O
with	O
any	O
particular	O
algorithm	O
for	O
achieving	O
it	O
the	O
bellman	B
error	I
is	O
not	O
learnable	O
the	O
concept	O
of	O
learnability	O
that	O
we	O
introduce	O
in	O
this	O
section	O
is	O
different	O
from	O
that	O
commonly	O
used	O
in	O
machine	O
learning	O
there	O
a	O
hypothesis	O
is	O
said	O
to	O
be	O
learnable	O
if	O
it	O
is	O
efficiently	O
learnable	O
meaning	O
that	O
it	O
can	O
be	O
learned	O
within	O
a	O
polynomial	O
rather	O
than	O
an	O
exponential	O
number	O
of	O
examples	O
here	O
we	O
use	O
the	O
term	O
in	O
a	O
more	O
basic	O
way	O
to	O
mean	O
learnable	O
at	O
all	O
with	O
any	O
amount	O
of	O
experience	O
it	O
turns	O
out	O
many	O
quantities	O
of	O
apparent	O
interest	O
in	O
reinforcement	B
learning	I
cannot	O
be	O
learned	O
even	O
from	O
an	O
infinite	O
amount	O
of	O
experiential	O
data	O
these	O
quantities	O
are	O
well	O
defined	O
and	O
can	O
be	O
computed	O
given	O
knowledge	O
of	O
the	O
internal	O
structure	O
of	O
the	O
environment	B
but	O
cannot	O
be	O
computed	O
or	O
estimated	O
from	O
the	O
observed	O
sequence	O
of	O
feature	O
vectors	O
actions	O
and	O
we	O
say	O
that	O
they	O
are	O
not	O
learnable	O
it	O
will	O
turn	O
out	O
that	O
the	O
bellman	B
error	I
objective	O
introduced	O
in	O
the	O
last	O
two	O
sections	O
is	O
not	O
learnable	O
in	O
this	O
sense	O
that	O
the	O
bellman	B
error	I
objective	O
cannot	O
be	O
learned	O
from	O
the	O
observable	O
data	O
is	O
probably	O
the	O
strongest	O
reason	O
not	O
to	O
seek	O
it	O
to	O
make	O
the	O
concept	O
of	O
learnability	O
clear	O
let	O
s	O
start	O
with	O
some	O
simple	O
examples	O
consider	O
the	O
two	O
markov	O
reward	O
diagrammed	O
below	O
where	O
two	O
edges	O
leave	O
a	O
state	B
both	O
transitions	O
are	O
assumed	O
to	O
occur	O
with	O
equal	O
probability	O
and	O
the	O
numbers	O
indicate	O
the	O
reward	O
received	O
all	O
the	O
states	O
appear	O
the	O
same	O
they	O
all	O
produce	O
the	O
same	O
single-component	O
feature	O
vector	B
x	O
and	O
have	O
approximated	O
value	B
w	O
thus	O
the	O
only	O
varying	O
part	O
of	O
the	O
data	O
trajectory	O
is	O
the	O
reward	O
sequence	O
the	O
left	O
mrp	O
stays	O
in	O
the	O
same	O
state	B
and	O
emits	O
an	O
endless	O
stream	O
of	O
and	O
at	O
random	O
each	O
with	O
probability	O
the	O
right	O
mrp	O
on	O
every	O
step	O
either	O
stays	O
in	O
its	O
current	O
state	B
or	O
switches	O
to	O
the	O
other	O
with	O
equal	O
probability	O
the	O
reward	O
is	O
deterministic	O
in	O
this	O
mrp	O
always	O
a	O
from	O
one	O
state	B
and	O
always	O
a	O
from	O
the	O
other	O
but	O
because	O
the	O
each	O
state	B
is	O
would	O
of	O
course	O
be	O
estimated	O
if	O
the	O
state	B
sequence	O
were	O
observed	O
rather	O
than	O
only	O
the	O
corre	O
sponding	O
feature	O
vectors	O
mrps	O
can	O
be	O
considered	O
mdps	O
with	O
a	O
single	O
action	B
in	O
all	O
states	O
what	O
we	O
conclude	O
about	O
mrps	O
here	O
applies	O
as	O
well	O
to	O
mdps	O
the	O
bellman	B
error	I
is	O
not	O
learnable	O
equally	O
likely	O
on	O
each	O
step	O
the	O
observable	O
data	O
is	O
again	O
an	O
endless	O
stream	O
of	O
and	O
at	O
random	O
identical	O
to	O
that	O
produced	O
by	O
the	O
left	O
mrp	O
can	O
assume	O
the	O
right	O
mrp	O
starts	O
in	O
one	O
of	O
two	O
states	O
at	O
random	O
with	O
equal	O
probability	O
thus	O
even	O
given	O
even	O
an	O
infinite	O
amount	O
of	O
data	O
it	O
would	O
not	O
be	O
possible	O
to	O
tell	O
which	O
of	O
these	O
two	O
mrps	O
was	O
generating	O
it	O
in	O
particular	O
we	O
could	O
not	O
tell	O
if	O
the	O
mrp	O
has	O
one	O
state	B
or	O
two	O
is	O
stochastic	O
or	O
deterministic	O
these	O
things	O
are	O
not	O
learnable	O
this	O
pair	O
of	O
mrps	O
also	O
illustrates	O
that	O
the	O
ve	O
objective	O
is	O
not	O
learnable	O
if	O
then	O
the	O
true	O
values	O
of	O
the	O
three	O
states	O
both	O
mrps	O
left	O
to	O
right	O
are	O
and	O
suppose	O
w	O
then	O
the	O
ve	O
is	O
for	O
the	O
left	O
mrp	O
and	O
for	O
the	O
right	O
mrp	O
because	O
the	O
ve	O
is	O
different	O
in	O
the	O
two	O
problems	O
yet	O
the	O
data	O
generated	O
has	O
the	O
same	O
distribution	O
the	O
ve	O
cannot	O
be	O
learned	O
the	O
ve	O
is	O
not	O
a	O
unique	O
function	O
of	O
the	O
data	O
distribution	O
and	O
if	O
it	O
cannot	O
be	O
learned	O
then	O
how	O
could	O
the	O
ve	O
possibly	O
be	O
useful	O
as	O
an	O
objective	O
for	O
learning	O
if	O
an	O
objective	O
cannot	O
be	O
learned	O
it	O
does	O
indeed	O
draw	O
its	O
utility	O
into	O
question	O
in	O
the	O
case	O
of	O
the	O
ve	O
however	O
there	O
is	O
a	O
way	O
out	O
note	O
that	O
the	O
same	O
solution	O
w	O
is	O
optimal	O
for	O
both	O
mrps	O
above	O
is	O
the	O
same	O
for	O
the	O
two	O
indistinguishable	O
states	O
in	O
the	O
right	O
mrp	O
is	O
this	O
a	O
coincidence	O
or	O
could	O
it	O
be	O
generally	O
true	O
that	O
all	O
mdps	O
with	O
the	O
same	O
data	O
distribution	O
also	O
have	O
the	O
same	O
optimal	O
parameter	O
vector	B
if	O
this	O
is	O
true	O
and	O
we	O
will	O
show	O
next	O
that	O
it	O
is	O
then	O
the	O
ve	O
remains	O
a	O
usable	O
objective	O
the	O
ve	O
is	O
not	O
learnable	O
but	O
the	O
parameter	O
that	O
optimizes	O
it	O
is	O
to	O
understand	O
this	O
it	O
is	O
useful	O
to	O
bring	O
in	O
another	O
natural	O
objective	O
function	O
this	O
time	O
one	O
that	O
is	O
clearly	O
learnable	O
one	O
error	O
that	O
is	O
always	O
observable	O
is	O
that	O
between	O
the	O
value	B
estimate	O
at	O
each	O
time	O
and	O
the	O
return	B
from	O
that	O
time	O
the	O
mean	O
square	O
return	B
error	I
denoted	O
re	O
is	O
the	O
expectation	O
under	O
of	O
the	O
square	O
of	O
this	O
error	O
in	O
the	O
on-policy	O
case	O
the	O
re	O
can	O
be	O
written	O
rew	O
vew	O
v	O
thus	O
the	O
two	O
objectives	O
are	O
the	O
same	O
except	O
for	O
a	O
variance	O
term	O
that	O
does	O
not	O
depend	O
on	O
the	O
parameter	O
vector	B
the	O
two	O
objectives	O
must	O
therefore	O
have	O
the	O
same	O
optimal	O
parameter	O
value	B
w	O
the	O
overall	O
relationships	O
are	O
summarized	O
in	O
the	O
left	O
side	O
of	O
figure	O
exercise	O
prove	O
hint	O
write	O
the	O
re	O
as	O
an	O
expectation	O
over	O
possible	O
states	O
s	O
of	O
the	O
expectation	O
of	O
the	O
squared	O
error	O
given	O
that	O
st	O
s	O
then	O
add	O
and	O
subtract	O
the	O
true	O
value	B
of	O
state	B
s	O
from	O
the	O
error	O
squaring	O
grouping	O
the	O
subtracted	O
true	O
value	B
with	O
the	O
return	B
and	O
the	O
added	O
true	O
value	B
with	O
the	O
estimated	O
value	B
then	O
if	O
you	O
expand	O
the	O
square	O
the	O
most	O
complex	O
term	O
will	O
end	O
up	O
being	O
zero	O
leaving	O
you	O
with	O
now	O
let	O
us	O
return	B
to	O
the	O
be	O
the	O
be	O
is	O
like	O
the	O
ve	O
in	O
that	O
it	O
can	O
be	O
computed	O
from	O
knowledge	O
of	O
the	O
mdp	O
but	O
is	O
not	O
learnable	O
from	O
data	O
but	O
it	O
is	O
not	O
like	O
the	O
ve	O
in	O
that	O
its	O
minimum	O
solution	O
is	O
not	O
learnable	O
the	O
box	O
on	O
the	O
next	O
page	O
presents	O
a	O
counterexample	O
two	O
mrps	O
that	O
generate	O
the	O
same	O
data	O
distribution	O
but	O
whose	O
minimizing	O
parameter	O
vector	B
is	O
different	O
proving	O
that	O
the	O
optimal	O
parameter	O
vector	B
is	O
not	O
a	O
function	O
of	O
the	O
data	O
and	O
thus	O
cannot	O
be	O
learned	O
from	O
it	O
the	O
other	O
bootstrapping	B
objectives	O
that	O
we	O
have	O
considered	O
the	O
pbe	O
and	O
tde	O
can	O
be	O
determined	O
from	O
data	O
chapter	O
methods	O
with	B
approximation	I
example	O
counterexample	O
to	O
the	O
learnability	B
of	I
the	O
bellman	B
error	I
to	O
show	O
the	O
full	O
range	O
of	O
possibilities	O
we	O
need	O
a	O
slightly	O
more	O
complex	O
pair	O
of	O
markov	O
reward	O
processes	O
than	O
those	O
considered	O
earlier	O
consider	O
the	O
following	O
two	O
mrps	O
where	O
two	O
edges	O
leave	O
a	O
state	B
both	O
transitions	O
are	O
assumed	O
to	O
occur	O
with	O
equal	O
probability	O
and	O
the	O
numbers	O
indicate	O
the	O
reward	O
received	O
the	O
mrp	O
on	O
the	O
left	O
has	O
two	O
states	O
that	O
are	O
represented	O
distinctly	O
the	O
mrp	O
on	O
the	O
right	O
has	O
three	O
states	O
two	O
of	O
which	O
b	O
and	O
appear	O
the	O
same	O
and	O
must	O
be	O
given	O
the	O
same	O
approximate	O
value	B
specifically	O
w	O
has	O
two	O
components	O
and	O
the	O
value	B
of	O
state	B
a	O
is	O
given	O
by	O
the	O
first	O
component	O
and	O
the	O
value	B
of	O
b	O
and	O
is	O
given	O
by	O
the	O
second	O
the	O
second	O
mrp	O
has	O
been	O
designed	O
so	O
that	O
equal	O
time	O
is	O
spent	O
in	O
all	O
three	O
states	O
so	O
we	O
can	O
take	O
for	O
all	O
s	O
note	O
that	O
the	O
observable	O
data	O
distribution	O
is	O
identical	O
for	O
the	O
two	O
mrps	O
in	O
both	O
cases	O
the	O
agent	O
will	O
see	O
single	O
occurrences	O
of	O
a	O
followed	O
by	O
a	O
then	O
some	O
number	O
of	O
apparent	O
bs	O
each	O
followed	O
by	O
a	O
except	O
the	O
last	O
which	O
is	O
followed	O
by	O
a	O
then	O
we	O
start	O
all	O
over	O
again	O
with	O
a	O
single	O
a	O
and	O
a	O
etc	O
all	O
the	O
statistical	O
details	O
are	O
the	O
same	O
as	O
well	O
in	O
both	O
mrps	O
the	O
probability	O
of	O
a	O
string	O
of	O
k	O
bs	O
is	O
k	O
now	O
suppose	O
w	O
in	O
the	O
first	O
mrp	O
this	O
is	O
an	O
exact	O
solution	O
and	O
the	O
be	O
is	O
zero	O
in	O
the	O
second	O
mrp	O
this	O
solution	O
produces	O
a	O
squared	O
error	O
in	O
both	O
b	O
and	O
of	O
such	O
that	O
be	O
these	O
two	O
mrps	O
which	O
generate	O
the	O
same	O
data	O
distribution	O
have	O
different	O
bes	O
the	O
be	O
is	O
not	O
learnable	O
moreover	O
unlike	O
the	O
earlier	O
example	O
for	O
the	O
ve	O
the	O
minimizing	O
value	B
of	O
w	O
is	O
different	O
for	O
the	O
two	O
mrps	O
for	O
the	O
first	O
mrp	O
w	O
minimizes	O
the	O
be	O
for	O
any	O
for	O
the	O
second	O
mrp	O
the	O
minimizing	O
w	O
is	O
a	O
complicated	O
function	O
of	O
but	O
in	O
the	O
limit	O
as	O
it	O
is	O
thus	O
the	O
solution	O
that	O
minimizes	O
be	O
cannot	O
be	O
estimated	O
from	O
data	O
alone	O
knowledge	O
of	O
the	O
mrp	O
beyond	O
what	O
is	O
revealed	O
in	O
the	O
data	O
is	O
required	O
in	O
this	O
sense	O
it	O
is	O
impossible	O
in	O
principle	O
to	O
pursue	O
the	O
be	O
as	O
an	O
objective	O
for	O
learning	O
it	O
may	O
be	O
surprising	O
that	O
in	O
the	O
second	O
mrp	O
the	O
be-minimizing	O
value	B
of	O
a	O
is	O
so	O
far	O
from	O
zero	O
recall	O
that	O
a	O
has	O
a	O
dedicated	O
weight	O
and	O
thus	O
its	O
value	B
is	O
unconstrained	O
by	O
function	B
approximation	I
a	O
is	O
followed	O
by	O
a	O
reward	O
of	O
and	O
transition	O
to	O
a	O
state	B
with	O
a	O
value	B
of	O
nearly	O
which	O
suggests	O
vwa	O
should	O
be	O
why	O
is	O
its	O
optimal	O
value	B
substantially	O
negative	O
rather	O
than	O
the	O
answer	O
is	O
that	O
making	O
vwa	O
negative	O
reduces	O
the	O
error	O
upon	O
arriving	O
in	O
a	O
from	O
b	O
the	O
reward	O
on	O
this	O
deterministic	O
transition	O
is	O
which	O
implies	O
that	O
b	O
should	O
have	O
a	O
value	B
more	O
than	O
a	O
because	O
b	O
s	O
value	B
is	O
approximately	O
zero	O
a	O
s	O
value	B
is	O
driven	O
toward	O
the	O
be-minimizing	O
value	B
of	O
for	O
a	O
is	O
a	O
compromise	O
between	O
reducing	O
the	O
errors	O
on	O
leaving	O
and	O
on	O
entering	O
a	O
the	O
bellman	B
error	I
is	O
not	O
learnable	O
figure	O
causal	O
relationships	O
among	O
the	O
data	O
distribution	O
mdps	O
and	O
various	O
objectives	O
left	O
monte	B
carlo	I
objectives	O
two	O
different	O
mdps	O
can	O
produce	O
the	O
same	O
data	O
distribution	O
yet	O
also	O
produce	O
different	O
ves	O
proving	O
that	O
the	O
ve	O
objective	O
cannot	O
be	O
determined	O
from	O
data	O
and	O
is	O
not	O
learnable	O
however	O
all	O
such	O
ves	O
must	O
have	O
the	O
same	O
optimal	O
parameter	O
vector	B
w	O
moreover	O
this	O
same	O
w	O
can	O
be	O
determined	O
from	O
another	O
objective	O
the	O
re	O
which	O
is	O
uniquely	O
determined	O
from	O
the	O
data	O
distribution	O
thus	O
w	O
and	O
the	O
re	O
are	O
learnable	O
even	O
though	O
the	O
ves	O
are	O
not	O
right	O
bootstrapping	B
objectives	O
two	O
different	O
mdps	O
can	O
produce	O
the	O
same	O
data	O
distribution	O
yet	O
also	O
produce	O
different	O
bes	O
and	O
have	O
different	O
minimizing	O
parameter	O
vectors	O
these	O
are	O
not	O
learnable	O
from	O
the	O
data	O
distribution	O
the	O
pbe	O
and	O
tde	O
objectives	O
and	O
their	O
minima	O
can	O
be	O
directly	O
determined	O
from	O
data	O
and	O
thus	O
are	O
learnable	O
learnable	O
and	O
determine	O
optimal	O
solutions	O
that	O
are	O
in	O
general	O
different	O
from	O
each	O
other	O
and	O
the	O
be	O
minimums	O
the	O
general	O
case	O
is	O
summarized	O
in	O
the	O
right	O
side	O
of	O
figure	O
thus	O
the	O
be	O
is	O
not	O
learnable	O
it	O
cannot	O
be	O
estimated	O
from	O
feature	O
vectors	O
and	O
other	O
observable	O
data	O
this	O
limits	O
the	O
be	O
to	O
model-based	O
settings	O
there	O
can	O
be	O
no	O
algorithm	O
that	O
minimizes	O
the	O
be	O
without	O
access	O
to	O
the	O
underlying	O
mdp	O
states	O
beyond	O
the	O
feature	O
vectors	O
the	O
residual-gradient	B
algorithm	I
is	O
only	O
able	O
to	O
minimize	O
be	O
because	O
it	O
is	O
allowed	O
to	O
double	B
sample	O
from	O
the	O
same	O
state	B
not	O
a	O
state	B
that	O
has	O
the	O
same	O
feature	O
vector	B
but	O
one	O
that	O
is	O
guaranteed	O
to	O
be	O
the	O
same	O
underlying	O
state	B
we	O
can	O
see	O
now	O
that	O
there	O
is	O
no	O
way	O
around	O
this	O
minimizing	O
the	O
be	O
requires	O
some	O
such	O
access	O
to	O
the	O
nominal	O
underlying	O
mdp	O
this	O
is	O
an	O
important	O
limitation	O
of	O
the	O
be	O
beyond	O
that	O
identified	O
in	O
the	O
a-presplit	O
example	O
on	O
page	O
all	O
this	O
directs	O
more	O
attention	O
toward	O
the	O
pbe	O
erentmdpsisidenticalineveryrespectyetthebeisdi	O
erentbes	O
thusthebecannotbeestimatedfromdataaloneknowledgeofthemdpbeyondwhatisrevealedinthedataisrequired	O
moreoverthetwomdpshavedi	O
erentmdpsisidenticalineveryrespectyetthebeisdi	O
erentbes	O
thusthebecannotbeestimatedfromdataaloneknowledgeofthemdpbeyondwhatisrevealedinthedataisrequired	O
moreoverthetwomdpshavedi	O
erentmdpsisidenticalineveryrespectyetthebeisdi	O
erentbes	O
thusthebecannotbeestimatedfromdataaloneknowledgeofthemdpbeyondwhatisrevealedinthedataisrequired	O
moreoverthetwomdpshavedi	O
erentmdpsisidenticalineveryrespectyetthebeisdi	O
erentbes	O
thusthebecannotbeestimatedfromdataaloneknowledgeofthemdpbeyondwhatisrevealedinthedataisrequired	O
moreoverthetwomdpshavedi	O
erentmdpsisidenticalineveryrespectyetthebeisdi	O
erentbes	O
thusthebecannotbeestimatedfromdataaloneknowledgeofthemdpbeyondwhatisrevealedinthedataisrequired	O
moreoverthetwomdpshavedi	O
w	O
carloobjectivesbootstrappingobjectives	O
chapter	O
methods	O
with	B
approximation	I
gradient-td	B
methods	I
we	O
now	O
consider	O
sgd	O
methods	O
for	O
minimizing	O
the	O
pbe	O
as	O
true	O
sgd	O
methods	O
these	O
gradient-td	B
methods	I
have	O
robust	O
convergence	O
properties	O
even	O
under	O
off-policy	B
training	O
and	O
nonlinear	O
function	B
approximation	I
remember	O
that	O
in	O
the	O
linear	O
case	O
there	O
is	O
always	O
an	O
exact	O
solution	O
the	O
td	B
fixed	O
point	O
wtd	O
at	O
which	O
the	O
pbe	O
is	O
zero	O
this	O
solution	O
could	O
be	O
found	O
by	O
least-squares	O
methods	O
but	O
only	O
by	O
methods	O
of	O
quadratic	O
complexity	O
in	O
the	O
number	O
of	O
parameters	O
we	O
seek	O
instead	O
an	O
sgd	O
method	O
which	O
should	O
be	O
od	O
and	O
have	O
robust	O
convergence	O
properties	O
gradient-td	B
methods	I
come	O
close	O
to	O
achieving	O
these	O
goals	O
at	O
the	O
cost	O
of	O
a	O
rough	O
doubling	O
of	O
computational	O
complexity	O
to	O
derive	O
an	O
sgd	O
method	O
for	O
the	O
pbe	O
linear	O
function	B
approximation	I
we	O
begin	O
by	O
expanding	O
and	O
rewriting	O
the	O
objective	O
in	O
matrix	O
terms	O
pbew	O
w	O
w	O
w	O
and	O
the	O
identity	O
the	O
gradient	B
with	O
respect	O
to	O
w	O
is	O
pbew	O
to	O
turn	O
this	O
into	O
an	O
sgd	O
method	O
we	O
have	O
to	O
sample	O
something	O
on	O
every	O
time	O
step	O
that	O
has	O
this	O
quantity	O
as	O
its	O
expected	B
value	B
let	O
us	O
take	O
to	O
be	O
the	O
distribution	O
of	O
states	O
visited	O
under	O
the	O
behavior	O
policy	B
all	O
three	O
of	O
the	O
factors	O
above	O
can	O
then	O
be	O
written	O
in	O
terms	O
of	O
expectations	O
under	O
this	O
distribution	O
for	O
example	O
the	O
last	O
factor	O
can	O
be	O
written	O
w	O
ws	O
e	O
t	O
txt	O
which	O
is	O
just	O
the	O
expectation	O
of	O
the	O
semi-gradient	O
update	O
the	O
first	O
factor	O
is	O
the	O
transpose	O
of	O
the	O
gradient	B
of	O
this	O
update	O
e	O
t	O
t	O
episodic	O
t	O
finally	O
the	O
middle	O
factor	O
is	O
the	O
inverse	O
of	O
the	O
expected	B
outer-product	O
matrix	O
of	O
the	O
feature	O
vectors	O
t	O
t	O
gradient-td	B
methods	I
substituting	O
these	O
expectations	O
for	O
the	O
three	O
factors	O
in	O
our	O
expression	O
for	O
the	O
gradient	B
of	O
the	O
pbe	O
we	O
get	O
pbew	O
t	O
e	O
t	O
txt	O
it	O
might	O
not	O
be	O
obvious	O
that	O
we	O
have	O
made	O
any	O
progress	O
by	O
writing	O
the	O
gradient	B
in	O
this	O
form	O
it	O
is	O
a	O
product	O
of	O
three	O
expressions	O
and	O
the	O
first	O
and	O
last	O
are	O
not	O
independent	O
they	O
both	O
depend	O
on	O
the	O
next	O
feature	O
vector	B
we	O
cannot	O
simply	O
sample	O
both	O
of	O
these	O
expectations	O
and	O
then	O
multiply	O
the	O
samples	O
this	O
would	O
give	O
us	O
a	O
biased	O
estmate	O
of	O
the	O
gradient	B
just	O
as	O
in	O
the	O
naive	B
residual-gradient	B
algorithm	I
another	O
idea	O
would	O
be	O
to	O
estimate	O
the	O
three	O
expectations	O
separately	O
and	O
then	O
combine	O
them	O
to	O
produce	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
this	O
would	O
work	O
but	O
would	O
require	O
a	O
lot	O
of	O
computational	O
resources	O
particularly	O
to	O
store	O
the	O
first	O
two	O
expectations	O
which	O
are	O
d	O
d	O
matrices	O
and	O
to	O
compute	O
the	O
inverse	O
of	O
the	O
second	O
this	O
idea	O
can	O
be	O
improved	O
if	O
two	O
of	O
the	O
three	O
expectations	O
are	O
estimated	O
and	O
stored	O
then	O
the	O
third	O
could	O
be	O
sampled	O
and	O
used	O
in	O
conjunction	O
with	O
the	O
two	O
stored	O
quantities	O
for	O
example	O
you	O
could	O
store	O
estimates	O
of	O
the	O
second	O
two	O
quantities	O
the	O
increment	O
inverse-updating	O
techniques	O
in	O
section	O
and	O
then	O
sample	O
the	O
first	O
expression	O
unfortunately	O
the	O
overall	O
algorithm	O
would	O
still	O
be	O
of	O
quadratic	O
complexity	O
order	O
the	O
idea	O
of	O
storing	O
some	O
estimates	O
separately	O
and	O
then	O
combining	O
them	O
with	O
samples	O
is	O
a	O
good	O
one	O
and	O
is	O
also	O
used	O
in	O
gradient-td	B
methods	I
gradient-td	B
methods	I
estimate	O
and	O
store	O
the	O
product	O
of	O
the	O
second	O
two	O
factors	O
in	O
these	O
factors	O
are	O
a	O
d	O
d	O
matrix	O
and	O
a	O
d-vector	O
so	O
their	O
product	O
is	O
just	O
a	O
d-vector	O
like	O
w	O
itself	O
we	O
denote	O
this	O
second	O
learned	O
vector	B
as	O
v	O
where	O
is	O
another	O
step-size	B
parameter	I
we	O
can	O
use	O
this	O
method	O
to	O
effectively	O
achieve	O
with	O
od	O
storage	O
and	O
per-step	O
computation	O
given	O
a	O
stored	O
estimate	O
vt	O
approximating	O
we	O
can	O
update	O
our	O
main	O
parameter	O
vector	B
wt	O
using	O
sgd	O
methods	O
based	O
on	O
the	O
simplest	O
such	O
rule	O
is	O
wt	O
wt	O
pbewt	O
t	O
e	O
t	O
txt	O
wt	O
txt	O
e	O
t	O
txt	O
wt	O
txt	O
vt	O
wt	O
t	O
vt	O
on	O
general	O
sgd	O
rule	O
this	O
form	O
is	O
familiar	O
to	O
students	O
of	O
linear	O
supervised	B
learning	I
it	O
is	O
the	O
solution	O
to	O
a	O
linear	O
least-squares	O
problem	O
that	O
tries	O
to	O
approximate	O
t	O
t	O
from	O
the	O
features	O
the	O
standard	O
sgd	O
method	O
for	O
incrementally	O
finding	O
the	O
vector	B
v	O
that	O
minimizes	O
the	O
expected	B
squared	O
is	O
known	O
as	O
the	O
least	O
mean	O
square	O
rule	O
augmented	O
v	O
e	O
t	O
txt	O
error	O
t	O
with	O
an	O
importance	B
sampling	I
ratio	B
vt	O
t	O
xt	O
chapter	O
methods	O
with	B
approximation	I
this	O
algorithm	O
is	O
called	O
note	O
that	O
if	O
the	O
final	O
inner	O
product	O
vt	O
is	O
done	O
first	O
then	O
the	O
entire	O
algorithm	O
is	O
of	O
od	O
complexity	O
a	O
slightly	O
better	O
algorithm	O
can	O
be	O
derived	O
by	O
doing	O
a	O
few	O
more	O
analytic	O
steps	O
before	O
substituting	O
in	O
vt	O
continuing	O
from	O
wt	O
txt	O
e	O
t	O
txt	O
wt	O
e	O
t	O
txt	O
wt	O
t	O
t	O
e	O
t	O
wt	O
t	O
t	O
wt	O
txt	O
which	O
again	O
is	O
od	O
if	O
the	O
final	O
product	O
vt	O
is	O
done	O
first	O
this	O
algorithm	O
is	O
known	O
as	O
either	O
with	O
gradient	B
correction	O
or	O
alternatively	O
as	O
figure	O
shows	O
a	O
sample	O
and	O
the	O
expected	B
behavior	O
of	O
tdc	O
on	O
baird	O
s	O
counterexample	O
as	O
intended	O
the	O
pbe	O
falls	O
to	O
zero	O
but	O
note	O
that	O
the	O
individual	O
components	O
of	O
the	O
parameter	O
vector	B
do	O
not	O
approach	O
zero	O
in	O
fact	O
these	O
values	O
are	O
still	O
far	O
from	O
an	O
optimal	O
solution	O
vs	O
for	O
all	O
s	O
for	O
which	O
w	O
would	O
have	O
to	O
be	O
proportional	O
to	O
after	O
iterations	O
we	O
are	O
still	O
far	O
from	O
an	O
optimal	O
solution	O
as	O
figure	O
the	O
behavior	O
of	O
the	O
tdc	O
algorithm	O
on	O
baird	O
s	O
counterexample	O
on	O
the	O
left	O
is	O
shown	O
a	O
typical	O
single	O
run	O
and	O
on	O
the	O
right	O
is	O
shown	O
the	O
expected	B
behavior	O
of	O
this	O
algorithm	O
if	O
the	O
updates	O
are	O
done	O
synchronously	O
to	O
except	O
for	O
the	O
two	O
tdc	O
parameter	O
vectors	O
the	O
step	O
sizes	O
were	O
and	O
emphatic-td	B
methods	I
we	O
can	O
see	O
from	O
the	O
ve	O
which	O
remains	O
almost	O
the	O
system	O
is	O
actually	O
converging	O
to	O
an	O
optimal	O
solution	O
but	O
progress	O
is	O
extremely	O
slow	O
because	O
the	O
pbe	O
is	O
already	O
so	O
close	O
to	O
zero	O
and	O
tdc	O
both	O
involve	O
two	O
learning	O
processes	O
a	O
primary	O
one	O
for	O
w	O
and	O
a	O
secondary	O
one	O
for	O
v	O
the	O
logic	O
of	O
the	O
primary	O
learning	O
process	O
relies	O
on	O
the	O
secondary	O
learning	O
process	O
having	O
finished	O
at	O
least	O
approximately	O
whereas	O
the	O
secondary	O
learning	O
process	O
proceeds	O
without	O
being	O
influenced	O
by	O
the	O
first	O
we	O
call	O
this	O
sort	O
of	O
asymmetrical	O
dependence	O
a	O
cascade	O
in	O
cascades	O
we	O
often	O
assume	O
that	O
the	O
secondary	O
learning	O
process	O
is	O
proceeding	O
faster	O
and	O
thus	O
is	O
always	O
at	O
its	O
asymptotic	O
value	B
ready	O
and	O
accurate	O
to	O
assist	O
the	O
primary	O
learning	O
process	O
the	O
convergence	O
proofs	O
for	O
these	O
methods	O
often	O
make	O
this	O
assumption	O
explicitly	O
these	O
are	O
called	O
two-time-scale	O
proofs	O
the	O
fast	O
time	O
scale	O
is	O
that	O
of	O
the	O
secondary	O
learning	O
process	O
and	O
the	O
slower	O
time	O
scale	O
is	O
that	O
of	O
the	O
primary	O
learning	O
process	O
if	O
is	O
the	O
step	O
size	O
of	O
the	O
primary	O
learning	O
process	O
and	O
is	O
the	O
step	O
size	O
of	O
the	O
secondary	O
learning	O
process	O
then	O
these	O
convergence	O
proofs	O
will	O
typically	O
require	O
that	O
in	O
the	O
limit	O
and	O
gradient-td	B
methods	I
are	O
currently	O
the	O
most	O
well	O
understood	O
and	O
widely	O
used	O
stable	O
off-policy	B
methods	I
there	O
are	O
extensions	O
to	O
action	B
values	O
and	B
control	B
maei	O
et	O
al	O
to	O
eligibility	B
traces	I
and	O
gq	O
maei	O
maei	O
and	O
sutton	O
and	O
to	O
nonlinear	O
function	B
approximation	I
et	O
al	O
there	O
have	O
also	O
been	O
proposed	O
hybrid	O
algorithms	O
midway	O
between	O
semi-gradient	O
td	B
and	O
gradient	B
td	B
white	O
and	O
white	O
hybrid-td	O
algorithms	O
behave	O
like	O
gradient-td	O
algorithms	O
in	O
states	O
where	O
the	O
target	O
and	O
behavior	O
policies	O
are	O
very	O
different	O
and	O
behave	O
like	O
semigradient	O
algorithms	O
in	O
states	O
where	O
the	O
target	O
and	O
behavior	O
policies	O
are	O
the	O
same	O
finally	O
the	O
gradient-td	O
idea	O
has	O
been	O
combined	O
with	O
the	O
ideas	O
of	O
proximal	O
methods	O
and	B
control	B
variates	I
to	O
produce	O
more	O
efficient	O
methods	O
et	O
al	O
emphatic-td	B
methods	I
we	O
turn	O
now	O
to	O
the	O
second	O
major	O
strategy	O
that	O
has	O
been	O
extensively	O
explored	O
for	O
obtaining	O
a	O
cheap	O
and	O
efficient	O
off-policy	B
learning	O
method	O
with	B
function	B
approximation	I
recall	O
that	O
linear	O
semi-gradient	O
td	B
methods	O
are	O
efficient	O
and	O
stable	O
when	O
trained	O
under	O
the	O
on-policy	B
distribution	I
and	O
that	O
we	O
showed	O
in	O
section	O
that	O
this	O
has	O
to	O
do	O
with	O
the	O
positive	O
definiteness	O
of	O
the	O
matrix	O
a	O
and	O
the	O
match	O
between	O
the	O
on-policy	O
state	B
distribution	O
and	O
the	O
state-transition	O
probabilities	O
pss	O
a	O
under	O
the	O
target	O
policy	B
in	O
off-policy	B
learning	O
we	O
reweight	O
the	O
state	B
transitions	O
using	O
importance	B
sampling	I
so	O
that	O
they	O
become	O
appropriate	O
for	O
learning	O
about	O
the	O
target	O
policy	B
but	O
the	O
state	B
distribution	O
is	O
still	O
that	O
of	O
the	O
behavior	O
policy	B
there	O
is	O
a	O
mismatch	O
a	O
natural	O
idea	O
is	O
to	O
somehow	O
reweight	O
the	O
states	O
emphasizing	O
some	O
and	O
de-emphasizing	O
others	O
so	O
as	O
to	O
return	B
the	O
distribution	O
of	O
updates	O
to	O
the	O
on-policy	B
distribution	I
there	O
would	O
then	O
be	O
a	O
match	O
and	B
stability	I
and	O
convergence	O
would	O
follow	O
from	O
existing	O
results	O
this	O
is	O
the	O
idea	O
of	O
emphatic-td	B
methods	I
first	O
introduced	O
for	O
on-policy	O
training	O
in	O
section	O
actually	O
the	O
notion	O
of	O
the	O
on-policy	B
distribution	I
is	O
not	O
quite	O
right	O
as	O
there	O
are	O
many	O
on-policy	O
distributions	O
and	O
any	O
one	O
of	O
these	O
is	O
sufficient	O
to	O
guarantee	O
stability	O
consider	O
an	O
undiscounted	O
episodic	O
problem	O
the	O
way	O
episodes	B
terminate	O
is	O
fully	O
determined	O
by	O
chapter	O
methods	O
with	B
approximation	I
the	O
transition	B
probabilities	I
but	O
there	O
may	O
be	O
several	O
different	O
ways	O
the	O
episodes	B
might	O
begin	O
however	O
the	O
episodes	B
start	O
if	O
all	O
state	B
transitions	O
are	O
due	O
to	O
the	O
target	O
policy	B
then	O
the	O
state	B
distribution	O
that	O
results	O
is	O
an	O
on-policy	B
distribution	I
you	O
might	O
start	O
close	O
to	O
the	O
terminal	O
state	B
and	O
visit	O
only	O
a	O
few	O
states	O
with	O
high	O
probability	O
before	O
ending	O
the	O
episode	O
or	O
you	O
might	O
start	O
far	O
away	O
and	O
pass	O
through	O
many	O
states	O
before	O
terminating	O
both	O
are	O
on-policy	O
distributions	O
and	O
training	O
on	O
both	O
with	O
a	O
linear	O
semi-gradient	O
method	O
would	O
be	O
guaranteed	O
to	O
be	O
stable	O
however	O
the	O
process	O
starts	O
an	O
on-policy	B
distribution	I
results	O
as	O
long	O
as	O
all	O
states	O
encountered	O
are	O
updated	O
up	O
until	O
termination	O
if	O
there	O
is	O
discounting	B
it	O
can	O
be	O
treated	O
as	O
partial	O
or	O
probabilistic	O
termination	O
for	O
these	O
purposes	O
if	O
then	O
we	O
can	O
consider	O
that	O
with	O
probability	O
the	O
process	O
terminates	O
on	O
every	O
time	O
step	O
and	O
then	O
immediately	O
restarts	O
in	O
the	O
state	B
that	O
is	O
transitioned	O
to	O
a	O
discounted	O
problem	O
is	O
one	O
that	O
is	O
continually	O
terminating	O
and	O
restarting	O
with	O
probability	O
on	O
every	O
step	O
this	O
way	O
of	O
thinking	O
about	O
discounting	B
is	O
an	O
example	O
of	O
a	O
more	O
general	O
notion	O
of	O
pseudo	B
termination	I
termination	O
that	O
does	O
not	O
affect	O
the	O
sequence	O
of	O
state	B
transitions	O
but	O
does	O
affect	O
the	O
learning	O
process	O
and	O
the	O
quantities	O
being	O
learned	O
this	O
kind	O
of	O
pseudo	B
termination	I
is	O
important	O
to	O
off-policy	B
learning	O
because	O
the	O
restarting	O
is	O
optional	O
remember	O
we	O
can	O
start	O
any	O
way	O
we	O
want	O
to	O
and	O
the	O
termination	O
relieves	O
the	O
need	O
to	O
keep	O
including	O
encountered	O
states	O
within	O
the	O
on-policy	B
distribution	I
that	O
is	O
if	O
we	O
don	O
t	O
consider	O
the	O
new	O
states	O
as	O
restarts	O
then	O
discounting	B
quickly	O
gives	O
us	O
a	O
limited	O
on-policy	B
distribution	I
the	O
one-step	O
emphatic-td	O
algorithm	O
for	O
learning	O
episodic	O
state	B
values	O
is	O
defined	O
by	O
t	O
vstwt	O
wt	O
mt	O
t	O
t	O
vstwt	O
mt	O
t	O
it	O
figure	O
the	O
behavior	O
of	O
the	O
one-step	O
emphatic-td	O
algorithm	O
in	O
expectation	O
on	O
baird	O
s	O
counterexample	O
the	O
step	O
size	O
was	O
reducing	B
variance	I
with	O
it	O
the	O
interest	O
being	O
arbitrary	O
and	O
mt	O
the	O
emphasis	O
being	O
initialized	O
to	O
mt	O
how	O
does	O
this	O
algorithm	O
perform	O
on	O
baird	O
s	O
counterexample	O
figure	O
shows	O
the	O
trajectory	O
in	O
expectation	O
of	O
the	O
components	O
of	O
the	O
parameter	O
vector	B
the	O
case	O
in	O
which	O
it	O
for	O
all	O
t	O
there	O
are	O
some	O
oscillations	O
but	O
eventually	O
everything	O
converges	O
and	O
the	O
ve	O
goes	O
to	O
zero	O
these	O
trajectories	O
are	O
obtained	O
by	O
iteratively	O
computing	O
the	O
expectation	O
of	O
the	O
parameter	O
vector	B
trajectory	O
without	O
any	O
of	O
the	O
variance	O
due	O
to	O
sampling	O
of	O
transitions	O
and	O
rewards	O
we	O
do	O
not	O
show	O
the	O
results	O
of	O
applying	O
the	O
emphatic-td	O
algorithm	O
directly	O
because	O
its	O
variance	O
on	O
baird	O
s	O
counterexample	O
is	O
so	O
high	O
that	O
it	O
is	O
nigh	O
impossible	O
to	O
get	O
consistent	O
results	O
in	O
computational	O
experiments	O
the	O
algorithm	O
converges	O
to	O
the	O
optimal	O
solution	O
in	O
theory	O
on	O
this	O
problem	O
but	O
in	O
practice	O
it	O
does	O
not	O
we	O
turn	O
to	O
the	O
topic	O
of	O
reducing	O
the	O
variance	O
of	O
all	O
these	O
algorithms	O
in	O
the	O
next	O
section	O
reducing	B
variance	I
off-policy	B
learning	O
is	O
inherently	O
of	O
greater	O
variance	O
than	O
on-policy	O
learning	O
this	O
is	O
not	O
surprising	O
if	O
you	O
receive	O
data	O
less	O
closely	O
related	O
to	O
a	O
policy	B
you	O
should	O
expect	O
to	O
learn	O
less	O
about	O
the	O
policy	B
s	O
values	O
in	O
the	O
extreme	O
one	O
may	O
be	O
able	O
to	O
learn	O
nothing	O
you	O
can	O
t	O
expect	O
to	O
learn	O
how	O
to	O
drive	O
by	O
cooking	O
dinner	O
for	O
example	O
only	O
if	O
the	O
target	O
and	O
behavior	O
policies	O
are	O
related	O
if	O
they	O
visit	O
similar	O
states	O
and	O
take	O
similar	O
actions	O
should	O
one	O
be	O
able	O
to	O
make	O
significant	O
progress	O
in	O
off-policy	B
training	O
on	O
the	O
other	O
hand	O
any	O
policy	B
has	O
many	O
neighbors	O
many	O
similar	O
policies	O
with	O
considerable	O
overlap	O
in	O
states	O
visited	O
and	O
actions	O
chosen	O
and	O
yet	O
which	O
are	O
not	O
identical	O
the	O
raison	O
d	O
etre	O
of	O
off-policy	B
learning	O
is	O
to	O
enable	O
generalization	O
to	O
this	O
vast	O
number	O
of	O
related-but-not-identical	O
policies	O
the	O
problem	O
remains	O
of	O
how	O
to	O
make	O
the	O
best	O
use	O
of	O
the	O
experience	O
now	O
that	O
we	O
have	O
some	O
methods	O
that	O
are	O
stable	O
in	O
expected	B
value	B
the	O
step	O
sizes	O
are	O
set	O
right	O
attention	O
naturally	O
turns	O
to	O
reducing	O
the	O
variance	O
of	O
the	O
estimates	O
there	O
are	O
many	O
possible	O
ideas	O
and	O
we	O
can	O
just	O
touch	O
on	O
of	O
a	O
few	O
of	O
them	O
in	O
this	O
introductory	O
text	O
why	O
is	O
controlling	O
variance	O
especially	O
critical	O
in	O
off-policy	B
methods	I
based	O
on	O
importance	B
sampling	I
as	O
we	O
have	O
seen	O
importance	B
sampling	I
often	O
involves	O
products	O
of	O
policy	B
ratios	O
the	O
ratios	O
are	O
always	O
one	O
in	O
expectation	O
but	O
their	O
actual	O
values	O
may	O
be	O
very	O
high	O
or	O
as	O
low	O
as	O
zero	O
successive	O
ratios	O
are	O
uncorrelated	O
so	O
their	O
products	O
are	O
also	O
always	O
one	O
in	O
expected	B
value	B
but	O
they	O
can	O
be	O
of	O
very	O
high	O
variance	O
recall	O
that	O
these	O
ratios	O
multiply	O
the	O
step	O
size	O
in	O
sgd	O
methods	O
so	O
high	O
variance	O
means	O
taking	O
steps	O
that	O
vary	O
greatly	O
in	O
their	O
sizes	O
this	O
is	O
problematic	O
for	O
sgd	O
because	O
of	O
the	O
occasional	O
very	O
large	O
steps	O
they	O
must	O
not	O
be	O
so	O
large	O
as	O
to	O
take	O
the	O
parameter	O
to	O
a	O
part	O
of	O
the	O
space	O
with	O
a	O
very	O
different	O
gradient	B
sgd	O
methods	O
rely	O
on	O
averaging	O
over	O
multiple	O
steps	O
to	O
get	O
a	O
good	O
sense	O
of	O
the	O
gradient	B
and	O
if	O
they	O
make	O
large	O
moves	O
from	O
single	O
samples	O
they	O
become	O
unreliable	O
if	O
the	O
step-size	B
parameter	I
is	O
set	O
small	O
enough	O
to	O
prevent	O
this	O
then	O
the	O
expected	B
step	O
can	O
end	O
up	O
being	O
very	O
small	O
resulting	O
in	O
very	O
slow	O
learning	O
the	O
notions	O
of	O
momentum	O
of	O
polyak-ruppert	O
averaging	O
ruppert	O
polyak	O
and	O
juditsky	O
or	O
further	O
extensions	O
of	O
these	O
ideas	O
may	O
significantly	O
help	O
methods	O
for	O
adaptively	O
setting	O
separate	O
step	O
sizes	O
for	O
different	O
components	O
of	O
the	O
chapter	O
methods	O
with	B
approximation	I
parameter	O
vector	B
are	O
also	O
pertinent	O
jacobs	O
sutton	O
c	O
as	O
are	O
the	O
importance	O
weight	O
aware	O
updates	O
of	O
karampatziakis	O
and	O
langford	O
in	O
chapter	O
we	O
saw	O
how	O
weighted	O
importance	B
sampling	I
is	O
significantly	O
better	O
behaved	O
with	O
lower	O
variance	O
updates	O
than	O
ordinary	O
importance	B
sampling	I
however	O
adapting	O
weighted	O
importance	B
sampling	I
to	O
function	B
approximation	I
is	O
challenging	O
and	O
can	O
probably	O
only	O
be	O
done	O
approximately	O
with	O
od	O
complexity	O
and	O
sutton	O
the	O
tree	B
backup	I
algorithm	O
shows	O
that	O
it	O
is	O
possible	O
to	O
perform	O
some	O
off-policy	B
learning	O
without	O
using	O
importance	B
sampling	I
this	O
idea	O
has	O
been	O
extended	O
to	O
the	O
off-policy	B
case	O
to	O
produce	O
stable	O
and	O
more	O
efficient	O
methods	O
by	O
munos	O
stepleton	O
harutyunyan	O
and	O
bellemare	O
and	O
by	O
mahmood	O
yu	O
and	O
sutton	O
another	O
complementary	O
strategy	O
is	O
to	O
allow	O
the	O
target	O
policy	B
to	O
be	O
determined	O
in	O
part	O
by	O
the	O
behavior	O
policy	B
in	O
such	O
a	O
way	O
that	O
it	O
never	O
can	O
be	O
so	O
different	O
from	O
it	O
to	O
create	O
large	O
importance	B
sampling	I
ratios	O
for	O
example	O
the	O
target	O
policy	B
can	O
be	O
defined	O
by	O
reference	O
to	O
the	O
behavior	O
policy	B
as	O
in	O
the	O
recognizers	O
proposed	O
by	O
precup	O
et	O
al	O
summary	O
off-policy	B
learning	O
is	O
a	O
tempting	O
challenge	O
testing	O
our	O
ingenuity	O
in	O
designing	O
stable	O
and	O
efficient	O
learning	O
algorithms	O
tabular	O
q-learning	B
makes	O
off-policy	B
learning	O
seem	O
easy	O
and	O
it	O
has	O
natural	O
generalizations	O
to	O
expected	B
sarsa	B
and	O
to	O
the	O
tree	B
backup	I
algorithm	O
but	O
as	O
we	O
have	O
seen	O
in	O
this	O
chapter	O
the	O
extension	O
of	O
these	O
ideas	O
to	O
significant	O
function	B
approximation	I
even	O
linear	O
function	B
approximation	I
involves	O
new	O
challenges	O
and	O
forces	O
us	O
to	O
deepen	O
our	O
understanding	O
of	O
reinforcement	B
learning	I
algorithms	O
why	O
go	O
to	O
such	O
lengths	O
one	O
reason	O
to	O
seek	O
off-policy	B
algorithms	O
is	O
to	O
give	O
flexibility	O
in	O
dealing	O
with	O
the	O
tradeoff	O
between	O
exploration	O
and	O
exploitation	O
another	O
is	O
to	O
free	O
behavior	O
from	O
learning	O
and	O
avoid	O
the	O
tyranny	O
of	O
the	O
target	O
policy	B
td	B
learning	O
appears	O
to	O
hold	O
out	O
the	O
possibility	O
of	O
learning	O
about	O
multiple	O
things	O
in	O
parallel	O
of	O
using	O
one	O
stream	O
of	O
experience	O
to	O
solve	O
many	O
tasks	O
simultaneously	O
we	O
can	O
certainly	O
do	O
this	O
in	O
special	O
cases	O
just	O
not	O
in	O
every	O
case	O
that	O
we	O
would	O
like	O
to	O
or	O
as	O
efficiently	O
as	O
we	O
would	O
like	O
to	O
in	O
this	O
chapter	O
we	O
divided	O
the	O
challenge	O
of	O
off-policy	B
learning	O
into	O
two	O
parts	O
the	O
first	O
part	O
correcting	O
the	O
targets	O
of	O
learning	O
for	O
the	O
behavior	O
policy	B
is	O
straightforwardly	O
dealt	O
with	O
using	O
the	O
techniques	O
devised	O
earlier	O
for	O
the	O
tabular	O
case	O
albeit	O
at	O
the	O
cost	O
of	O
increasing	O
the	O
variance	O
of	O
the	O
updates	O
and	O
thereby	O
slowing	O
learning	O
high	O
variance	O
will	O
probably	O
always	O
remains	O
a	O
challenge	O
for	O
off-policy	B
learning	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
off-policy	B
learning	O
emerges	O
as	O
the	O
instability	O
of	O
semi-gradient	O
td	B
methods	O
that	O
involve	O
bootstrapping	B
we	O
seek	O
powerful	O
function	B
approximation	I
off-policy	B
learning	O
and	O
the	O
efficiency	O
and	O
flexibility	O
of	O
bootstrapping	B
td	B
methods	O
but	O
it	O
is	O
challenging	O
to	O
combine	O
all	O
three	O
aspects	O
of	O
this	O
deadly	B
triad	I
in	O
one	O
algorithm	O
without	O
introducing	O
the	O
potential	O
for	O
instability	O
there	O
have	O
been	O
several	O
attempts	O
the	O
most	O
popular	O
has	O
been	O
to	O
seek	O
to	O
perform	O
true	O
stochastic	O
gradient	B
descent	O
in	B
the	I
bellman	B
error	I
the	O
bellman	B
residual	O
however	O
our	O
analysis	O
con	O
summary	O
cludes	O
that	O
this	O
is	O
not	O
an	O
appealing	O
goal	O
in	O
many	O
cases	O
and	O
that	O
anyway	O
it	O
is	O
impossible	O
to	O
achieve	O
with	O
a	O
learning	O
algorithm	O
the	O
gradient	B
of	O
the	O
be	O
is	O
not	O
learnable	O
from	O
experience	O
that	O
reveals	O
only	O
feature	O
vectors	O
and	O
not	O
underlying	O
states	O
another	O
approach	O
gradient-td	B
methods	I
performs	O
sgd	O
in	O
the	O
projected	B
bellman	B
error	I
the	O
gradient	B
of	O
the	O
pbe	O
is	O
learnable	O
with	O
od	O
complexity	O
but	O
at	O
the	O
cost	O
of	O
a	O
second	O
parameter	O
vector	B
with	O
a	O
second	O
step	O
size	O
the	O
newest	O
family	O
of	O
methods	O
emphatic-td	B
methods	I
refine	O
an	O
old	O
idea	O
for	O
reweighting	O
updates	O
emphasizing	O
some	O
and	O
de-emphasizing	O
others	O
in	O
this	O
way	O
they	O
restore	O
the	O
special	O
properties	O
that	O
make	O
on-policy	O
learning	O
stable	O
with	O
computationally	O
simple	O
semi-gradient	B
methods	I
the	O
whole	O
area	O
of	O
off-policy	B
learning	O
is	O
relatively	O
new	O
and	O
unsettled	O
which	O
methods	O
are	O
best	O
or	O
even	O
adequate	O
is	O
not	O
yet	O
clear	O
are	O
the	O
complexities	O
of	O
the	O
new	O
methods	O
introduced	O
at	O
the	O
end	O
of	O
this	O
chapter	O
really	O
necessary	O
which	O
of	O
them	O
can	O
be	O
combined	O
effectively	O
with	O
variance	O
reduction	O
methods	O
the	O
potential	O
for	O
off-policy	B
learning	O
remains	O
tantalizing	O
the	O
best	O
way	O
to	O
achieve	O
it	O
still	O
a	O
mystery	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
first	O
semi-gradient	O
method	O
was	O
linear	O
td	B
the	O
name	O
semi-gradient	O
is	O
more	O
recent	O
semi-gradient	O
off-policy	B
with	O
general	O
importance-sampling	O
ratio	B
may	O
not	O
have	O
been	O
explicitly	O
stated	O
until	O
sutton	O
mahmood	O
and	O
white	O
but	O
the	O
action-value	O
forms	O
were	O
introduced	O
by	O
precup	O
sutton	O
and	O
singh	O
who	O
also	O
did	O
eligibility	O
trace	O
forms	O
of	O
these	O
algorithms	O
chapter	O
their	O
continuing	O
undiscounted	O
forms	O
have	O
not	O
been	O
significantly	O
explored	O
the	O
atomic	O
n-step	B
forms	O
given	O
here	O
are	O
new	O
the	O
earliest	O
example	O
was	O
given	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
who	O
also	O
introduced	O
the	O
specific	O
counterexample	O
in	O
the	O
box	O
on	O
page	O
baird	O
s	O
counterexample	O
is	O
due	O
to	O
baird	O
though	O
the	O
version	O
we	O
present	O
here	O
is	O
slightly	O
modified	O
averaging	O
methods	O
for	O
function	B
approximation	I
were	O
developed	O
by	O
gordon	O
other	O
examples	O
of	O
instability	O
with	O
off-policy	B
dp	O
methods	O
and	O
more	O
complex	O
methods	O
of	O
function	B
approximation	I
are	O
given	O
by	O
boyan	O
and	O
moore	O
bradtke	O
gives	O
an	O
example	O
in	O
which	O
q-learning	B
using	O
linear	O
function	B
approximation	I
in	O
a	O
linear	O
quadratic	O
regulation	O
problem	O
converges	O
to	O
a	O
destabilizing	O
policy	B
the	O
deadly	B
triad	I
was	O
first	O
identified	O
by	O
sutton	O
and	O
thoroughly	O
analyzed	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
the	O
name	O
deadly	B
triad	I
is	O
due	O
to	O
sutton	O
this	O
kind	O
of	O
linear	O
analysis	O
was	O
pioneered	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
including	O
the	O
dynamic	B
programming	I
operator	O
diagrams	O
like	O
figure	O
were	O
introduced	O
by	O
lagoudakis	O
and	O
parr	O
what	O
we	O
have	O
called	O
the	O
bellman	B
operator	I
and	O
denoted	O
b	O
is	O
more	O
commonly	O
denoted	O
t	O
and	O
called	O
a	O
dynamic	B
programming	I
operator	O
while	O
a	O
generalized	O
chapter	O
methods	O
with	B
approximation	I
form	O
denoted	O
t	O
is	O
called	O
the	O
td	B
operator	O
and	O
van	O
roy	O
the	O
be	O
was	O
first	O
proposed	O
as	O
an	O
objective	O
function	O
for	B
dynamic	B
programming	I
by	O
schweitzer	O
and	O
seidmann	O
baird	O
extended	O
it	O
to	O
td	B
learning	O
based	O
on	O
stochastic	O
gradient	B
descent	O
in	O
the	O
literature	O
be	O
minimization	O
is	O
often	O
referred	O
to	O
as	O
bellman	B
residual	O
minimization	O
the	O
earliest	O
a-split	O
example	O
is	O
due	O
to	O
dayan	O
the	O
two	O
forms	O
given	O
here	O
were	O
introduced	O
by	O
sutton	O
et	O
al	O
the	O
contents	O
of	O
this	O
section	O
are	O
new	O
to	O
this	O
text	O
gradient-td	B
methods	I
were	O
introduced	O
by	O
sutton	O
szepesv	O
ari	O
and	O
maei	O
the	O
methods	O
highlighted	O
in	O
this	O
section	O
were	O
introduced	O
by	O
sutton	O
et	O
al	O
and	O
mahmood	O
et	O
al	O
the	O
most	O
sensitive	O
empirical	O
investigations	O
to	O
date	O
of	O
gradient-td	O
and	O
related	O
methods	O
are	O
given	O
by	O
geist	O
and	O
scherrer	O
dann	O
neumann	O
and	O
peters	O
and	O
white	O
the	O
latest	O
developments	O
in	O
the	O
theory	O
of	O
gradient-td	B
methods	I
are	O
developed	O
by	O
yu	O
emphatic-td	B
methods	I
were	O
introduced	O
by	O
sutton	O
mahmood	O
and	O
white	O
full	O
convergence	O
proofs	O
and	O
other	O
theory	O
were	O
later	O
established	O
by	O
yu	O
yu	O
mahmood	O
and	O
sutton	O
hallak	O
tamar	O
and	O
mannor	O
and	O
hallak	O
tamar	O
munos	O
and	O
mannor	O
chapter	O
eligibility	B
traces	I
eligibility	B
traces	I
are	O
one	O
of	O
the	O
basic	O
mechanisms	O
of	O
reinforcement	B
learning	I
for	O
example	O
in	O
the	O
popular	O
td	B
algorithm	O
the	O
refers	O
to	O
the	O
use	O
of	O
an	O
eligibility	O
trace	O
almost	O
any	O
temporal-difference	O
method	O
such	O
as	O
q-learning	B
or	O
sarsa	B
can	O
be	O
combined	O
with	B
eligibility	B
traces	I
to	O
obtain	O
a	O
more	O
general	O
method	O
that	O
may	O
learn	O
more	O
efficiently	O
eligibility	B
traces	I
unify	O
and	O
generalize	O
td	B
and	B
monte	B
carlo	I
methods	I
when	O
td	B
methods	O
are	O
augmented	O
with	B
eligibility	B
traces	I
they	O
produce	O
a	O
family	O
of	O
methods	O
spanning	O
a	O
spectrum	O
that	O
has	O
monte	B
carlo	I
methods	I
at	O
one	O
end	O
and	O
one-step	O
td	B
methods	O
at	O
the	O
other	O
in	O
between	O
are	O
intermediate	O
methods	O
that	O
are	O
often	O
better	O
than	O
either	O
extreme	O
method	O
eligibility	B
traces	I
also	O
provide	O
a	O
way	O
of	O
implementing	O
monte	B
carlo	I
methods	I
online	B
and	O
on	O
continuing	O
problems	O
without	O
episodes	B
of	O
course	O
we	O
have	O
already	O
seen	O
one	O
way	O
of	O
unifying	O
td	B
and	B
monte	B
carlo	I
methods	I
the	O
n-step	B
td	B
methods	O
of	O
chapter	O
what	O
eligibility	B
traces	I
offer	O
beyond	O
these	O
is	O
an	O
elegant	O
algorithmic	O
mechanism	O
with	O
significant	O
computational	O
advantages	O
the	O
mechanism	O
is	O
a	O
short-term	O
memory	O
vector	B
the	O
eligibility	O
trace	O
zt	O
rd	O
that	O
parallels	O
the	O
long-term	O
weight	O
vector	B
wt	O
rd	O
the	O
rough	O
idea	O
is	O
that	O
when	O
a	O
component	O
of	O
wt	O
participates	O
in	O
producing	O
an	O
estimated	O
value	B
then	O
the	O
corresponding	O
component	O
of	O
zt	O
is	O
bumped	O
up	O
and	O
then	O
begins	O
to	O
fade	O
away	O
learning	O
will	O
then	O
occur	O
in	O
that	O
component	O
of	O
wt	O
if	O
a	O
nonzero	O
td	B
error	I
occurs	O
before	O
the	O
trace	O
falls	O
back	O
to	O
zero	O
the	O
trace-decay	O
parameter	O
determines	O
the	O
rate	O
at	O
which	O
the	O
trace	O
falls	O
the	O
primary	O
computational	O
advantage	O
of	O
eligibility	B
traces	I
over	O
n-step	B
methods	I
is	O
that	O
only	O
a	O
single	O
trace	O
vector	B
is	O
required	O
rather	O
than	O
a	O
store	O
of	O
the	O
last	O
n	O
feature	O
vectors	O
learning	O
also	O
occurs	O
continually	O
and	O
uniformly	O
in	O
time	O
rather	O
than	O
being	O
delayed	O
and	O
then	O
catching	O
up	O
at	O
the	O
end	O
of	O
the	O
episode	O
in	O
addition	O
learning	O
can	O
occur	O
and	O
affect	O
behavior	O
immediately	O
after	O
a	O
state	B
is	O
encountered	O
rather	O
than	O
being	O
delayed	O
n	O
steps	O
eligibility	B
traces	I
illustrate	O
that	O
a	O
learning	O
algorithm	O
can	O
sometimes	O
be	O
implemented	O
in	O
a	O
different	O
way	O
to	O
obtain	O
computational	O
advantages	O
many	O
algorithms	O
are	O
most	O
naturally	O
formulated	O
and	O
understood	O
as	O
an	O
update	O
of	O
a	O
state	B
s	O
value	B
based	O
on	O
events	O
that	O
follow	O
that	O
state	B
over	O
multiple	O
future	O
time	O
steps	O
for	O
example	O
monte	B
carlo	I
methods	I
update	O
a	O
state	B
based	O
on	O
all	O
the	O
future	O
rewards	O
and	O
n-step	B
td	B
methods	O
chapter	O
eligibility	B
traces	I
update	O
based	O
on	O
the	O
next	O
n	O
rewards	O
and	O
state	B
n	O
steps	O
in	O
the	O
future	O
such	O
formulations	O
based	O
on	O
looking	O
forward	O
from	O
the	O
updated	O
state	B
are	O
called	O
forward	O
views	O
forward	O
views	O
are	O
always	O
somewhat	O
complex	O
to	O
implement	O
because	O
the	O
update	O
depends	O
on	O
later	O
things	O
that	O
are	O
not	O
available	O
at	O
the	O
time	O
however	O
as	O
we	O
show	O
in	O
this	O
chapter	O
it	O
is	O
often	O
possible	O
to	O
achieve	O
nearly	O
the	O
same	O
updates	O
and	O
sometimes	O
exactly	O
the	O
same	O
updates	O
with	O
an	O
algorithm	O
that	O
uses	O
the	O
current	O
td	B
error	I
looking	O
backward	O
to	O
recently	O
visited	O
states	O
using	O
an	O
eligibility	O
trace	O
these	O
alternate	O
ways	O
of	O
looking	O
at	O
and	O
implementing	O
learning	O
algorithms	O
are	O
called	O
backward	O
views	O
backward	O
views	O
transformations	O
between	O
forward-views	O
and	O
backward-views	O
and	O
equivalences	O
between	O
them	O
date	O
back	O
to	O
the	O
introduction	O
of	O
temporal	O
difference	O
learning	O
but	O
have	O
become	O
much	O
more	O
powerful	O
and	O
sophisticated	O
since	O
here	O
we	O
present	O
the	O
basics	O
of	O
the	O
modern	O
view	O
as	O
usual	O
first	O
we	O
fully	O
develop	O
the	O
ideas	O
for	O
state	B
values	O
and	O
prediction	B
then	O
extend	O
them	O
to	O
action	B
values	O
and	B
control	B
we	O
develop	O
them	O
first	O
for	O
the	O
on-policy	O
case	O
then	O
extend	O
them	O
to	O
off-policy	B
learning	O
our	O
treatment	O
pays	O
special	O
attention	O
to	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
for	O
which	O
the	O
results	O
with	B
eligibility	B
traces	I
are	O
stronger	O
all	O
these	O
results	O
apply	O
also	O
to	O
the	O
tabular	O
and	O
state	B
aggregation	I
cases	O
because	O
these	O
are	O
special	O
cases	O
of	O
linear	O
function	B
approximation	I
the	O
in	O
chapter	O
we	O
defined	O
an	O
n-step	B
return	B
as	O
the	O
sum	O
of	O
the	O
first	O
n	O
rewards	O
plus	O
the	O
estimated	O
value	B
of	O
the	O
state	B
reached	O
in	O
n	O
steps	O
each	O
appropriately	O
discounted	O
the	O
general	O
form	O
of	O
that	O
equation	O
for	O
any	O
parameterized	O
function	O
approximator	O
is	O
gttn	O
n	O
n	O
vstnwtn	O
t	O
t	O
n	O
we	O
noted	O
in	O
chapter	O
that	O
each	O
n-step	B
return	B
for	O
n	O
is	O
a	O
valid	O
update	O
target	O
for	O
a	O
tabular	O
learning	O
update	O
just	O
as	O
it	O
is	O
for	O
an	O
approximate	O
sgd	O
learning	O
update	O
such	O
as	O
now	O
we	O
note	O
that	O
a	O
valid	O
update	O
can	O
be	O
done	O
not	O
just	O
toward	O
any	O
n-step	B
return	B
but	O
toward	O
any	O
average	O
of	O
n-step	B
returns	O
for	O
example	O
an	O
update	O
can	O
be	O
done	O
toward	O
a	O
target	O
that	O
is	O
half	O
of	O
a	O
two-step	O
return	B
and	O
half	O
of	O
a	O
four-step	O
return	B
any	O
set	O
of	O
n-step	B
returns	O
can	O
be	O
averaged	O
in	O
this	O
way	O
even	O
an	O
infinite	O
set	O
as	O
long	O
as	O
the	O
weights	O
on	O
the	O
component	O
returns	O
are	O
positive	O
and	O
sum	O
to	O
the	O
composite	O
return	B
possesses	O
an	O
error	B
reduction	I
property	I
similar	O
to	O
that	O
of	O
individual	O
n-step	B
returns	O
and	O
thus	O
can	O
be	O
used	O
to	O
construct	O
updates	O
with	O
guaranteed	O
convergence	O
properties	O
averaging	O
produces	O
a	O
substantial	O
new	O
range	O
of	O
algorithms	O
for	O
example	O
one	O
could	O
average	O
one-step	O
and	O
infinite-step	O
returns	O
to	O
obtain	O
another	O
way	O
of	O
interrelating	O
td	B
and	B
monte	B
carlo	I
methods	I
in	O
principle	O
one	O
could	O
even	O
average	O
experience-based	O
updates	O
with	O
dp	O
updates	O
to	O
get	O
a	O
simple	O
combination	O
of	O
experience-based	O
and	O
model-based	O
methods	O
chapter	O
an	O
update	O
that	O
averages	O
simpler	O
component	O
updates	O
is	O
called	O
a	O
compound	B
update	O
the	O
backup	B
diagram	I
for	O
a	O
compound	B
update	O
consists	O
of	O
the	O
backup	O
diagrams	O
for	O
each	O
of	O
the	O
component	O
updates	O
with	O
a	O
horizontal	O
line	O
above	O
them	O
and	O
the	O
weighting	O
fractions	O
below	O
the	O
for	O
example	O
the	O
compound	B
update	O
for	O
the	O
case	O
mentioned	O
at	O
the	O
start	O
of	O
this	O
section	O
mixing	O
half	O
of	O
a	O
two-step	O
return	B
and	O
half	O
of	O
a	O
four-step	O
return	B
has	O
the	O
diagram	O
shown	O
to	O
the	O
right	O
a	O
compound	B
update	O
can	O
only	O
be	O
done	O
when	O
the	O
longest	O
of	O
its	O
component	O
updates	O
is	O
complete	O
the	O
update	O
at	O
the	O
right	O
for	O
example	O
could	O
only	O
be	O
done	O
at	O
time	O
for	O
the	O
estimate	O
formed	O
at	O
time	O
t	O
in	O
general	O
one	O
would	O
like	O
to	O
limit	O
the	O
length	O
of	O
the	O
longest	O
component	O
update	O
because	O
of	O
the	O
corresponding	O
delay	O
in	O
the	O
updates	O
the	O
td	B
algorithm	O
can	O
be	O
understood	O
as	O
one	O
particular	O
way	O
of	O
averaging	O
n-step	B
updates	O
this	O
average	O
contains	O
all	O
the	O
n-step	B
updates	O
each	O
weighted	O
proportional	O
to	O
n	O
and	O
is	O
normalized	O
by	O
a	O
factor	O
of	O
to	O
ensure	O
that	O
the	O
weights	O
sum	O
to	O
figure	O
the	O
resulting	O
update	O
is	O
toward	O
a	O
return	B
called	O
the	O
defined	O
in	O
its	O
statebased	O
form	O
by	O
g	O
t	O
n	O
figure	O
further	O
illustrates	O
the	O
weighting	O
on	O
the	O
sequence	O
of	O
n-step	B
returns	O
in	O
the	O
the	O
one-step	O
return	B
is	O
given	O
the	O
largest	O
weight	O
the	O
two-step	O
return	B
is	O
given	O
the	O
next	O
largest	O
weight	O
the	O
three-step	O
return	B
is	O
given	O
the	O
weight	O
and	O
so	O
on	O
the	O
weight	O
fades	O
by	O
with	O
each	O
additional	O
step	O
after	O
a	O
terminal	O
state	B
has	O
been	O
reached	O
all	O
subsequent	O
n-step	B
returns	O
are	O
equal	O
to	O
gt	O
if	O
we	O
want	O
we	O
can	O
separate	O
figure	O
the	O
backup	O
digram	O
for	O
td	B
if	O
then	O
the	O
overall	O
update	O
reduces	O
to	O
its	O
first	O
component	O
the	O
one-step	O
td	B
update	O
whereas	O
if	O
then	O
the	O
overall	O
update	O
reduces	O
to	O
its	O
last	O
component	O
the	O
monte	B
carlo	I
update	O
chapter	O
eligibility	B
traces	I
figure	O
weighting	O
given	O
in	O
the	O
to	O
each	O
of	O
the	O
n-step	B
returns	O
these	O
post-termination	O
terms	O
from	O
the	O
main	O
sum	O
yielding	O
g	O
t	O
t	O
t	O
n	O
t	O
t	O
as	O
indicated	O
in	O
the	O
figures	O
this	O
equation	O
makes	O
it	O
clearer	O
what	O
happens	O
when	O
in	O
this	O
case	O
the	O
main	O
sum	O
goes	O
to	O
zero	O
and	O
the	O
remaining	O
term	O
reduces	O
to	O
the	O
conventional	O
return	B
gt	O
thus	O
for	O
updating	O
according	O
to	O
the	O
is	O
a	O
monte	B
carlo	I
algorithm	O
on	O
the	O
other	O
hand	O
if	O
then	O
the	O
reduces	O
to	O
the	O
onestep	O
return	B
thus	O
for	O
updating	O
according	O
to	O
the	O
is	O
a	O
one-step	O
td	B
method	O
exercise	O
just	O
as	O
the	O
return	B
can	O
be	O
written	O
recursively	O
in	O
terms	O
of	O
the	O
first	O
reward	O
and	O
itself	O
one-step	O
later	O
so	O
can	O
the	O
derive	O
the	O
analogous	O
recursive	O
relationship	O
from	O
and	O
exercise	O
the	O
parameter	O
characterizes	O
how	O
fast	O
the	O
exponential	O
weighting	O
in	O
figure	O
falls	O
off	O
and	O
thus	O
how	O
far	O
into	O
the	O
future	O
the	O
algorithm	O
looks	O
in	O
determining	O
its	O
update	O
but	O
a	O
rate	O
factor	O
such	O
as	O
is	O
sometimes	O
an	O
awkward	O
way	O
of	O
characterizing	O
the	O
speed	O
of	O
the	O
decay	O
for	O
some	O
purposes	O
it	O
is	O
better	O
to	O
specify	O
a	O
time	O
constant	O
or	O
half-life	O
what	O
is	O
the	O
equation	O
relating	O
and	O
the	O
half-life	O
the	O
time	O
by	O
which	O
the	O
weighting	O
sequence	O
will	O
have	O
fallen	O
to	O
half	O
of	O
its	O
initial	O
value	B
we	O
are	O
now	O
ready	O
to	O
define	O
our	O
first	O
learning	O
algorithm	O
based	O
on	O
the	O
the	O
off-line	B
algorithm	O
as	O
an	O
off-line	B
algorithm	O
it	O
makes	O
no	O
changes	O
to	O
the	O
weight	O
vector	B
during	O
the	O
episode	O
then	O
at	O
the	O
end	O
of	O
the	O
episode	O
a	O
whole	O
sequence	O
of	O
off-line	B
updates	O
are	O
made	O
according	O
to	O
our	O
usual	O
semi-gradient	O
rule	O
using	O
the	O
as	O
the	O
target	O
wt	O
t	O
vstwt	O
t	O
t	O
given	O
tothe	O
returndecay	O
by	O
given	O
toactual	O
final	O
returntttimeweighttotal	O
area	O
the	O
the	O
gives	O
us	O
an	O
alternative	O
way	O
of	O
moving	O
smoothly	O
between	O
monte	B
carlo	I
and	O
one-step	O
td	B
methods	O
that	O
can	O
be	O
compared	O
with	O
the	O
n-step	B
td	B
way	O
of	O
chapter	O
there	O
we	O
assessed	O
effectiveness	O
on	O
a	O
random	B
walk	I
task	O
page	O
figure	O
shows	O
the	O
performance	O
of	O
the	O
off-line	B
algorithm	O
on	O
this	O
task	O
alongside	O
that	O
of	O
the	O
n-step	B
methods	I
from	O
figure	O
the	O
experiment	O
was	O
just	O
as	O
described	O
earlier	O
except	O
that	O
for	O
the	O
algorithm	O
we	O
varied	O
instead	O
of	O
n	O
the	O
performance	O
measure	O
used	O
is	O
the	O
estimated	O
root-mean-squared	O
error	O
between	O
the	O
correct	O
and	O
estimated	O
values	O
of	O
each	O
state	B
measured	O
at	O
the	O
end	O
of	O
the	O
episode	O
averaged	O
over	O
the	O
first	O
episodes	B
and	O
the	O
states	O
note	O
that	O
overall	O
performance	O
of	O
the	O
off-line	B
algorithms	O
is	O
comparable	O
to	O
that	O
of	O
the	O
n-step	B
algorithms	O
in	O
both	O
cases	O
we	O
get	O
best	O
performance	O
with	O
an	O
intermediate	O
value	B
of	O
the	O
bootstrapping	B
parameter	O
n	O
for	O
n-step	B
methods	I
and	O
for	O
the	O
off-line	B
algorithm	O
figure	O
random	B
walk	I
results	O
performance	O
of	O
the	O
off-line	B
return	B
algorithm	O
alongside	O
that	O
of	O
the	O
n-step	B
td	B
methods	O
in	O
both	O
case	O
intermediate	O
values	O
of	O
the	O
bootstrapping	B
parameter	O
or	O
n	O
performed	O
best	O
the	O
results	O
with	O
the	O
off-line	B
algorithm	O
are	O
slightly	O
better	O
at	O
the	O
best	O
values	O
of	O
and	O
and	O
at	O
high	O
the	O
approach	O
that	O
we	O
have	O
been	O
taking	O
so	O
far	O
is	O
what	O
we	O
call	O
the	O
theoretical	O
or	O
forward	O
view	O
of	O
a	O
learning	O
algorithm	O
for	O
each	O
state	B
visited	O
we	O
look	O
forward	O
in	O
time	O
to	O
all	O
the	O
future	O
rewards	O
and	O
decide	O
how	O
best	O
to	O
combine	O
them	O
we	O
might	O
imagine	O
ourselves	O
riding	O
the	O
stream	O
of	O
states	O
looking	O
forward	O
from	O
each	O
state	B
to	O
determine	O
its	O
update	O
as	O
suggested	O
by	O
figure	O
after	O
looking	O
forward	O
from	O
and	O
updating	O
one	O
state	B
we	O
move	O
on	O
to	O
the	O
next	O
and	O
never	O
have	O
to	O
work	O
with	O
the	O
preceding	O
state	B
again	O
future	O
states	O
on	O
the	O
other	O
hand	O
are	O
viewed	O
and	O
processed	O
repeatedly	O
once	O
from	O
each	O
vantage	O
point	O
preceding	O
them	O
n-step	B
td	B
methodsfrom	O
chapter	O
averagerms	O
errorover	O
statesand	O
first	O
algorithm	O
rms	O
errorat	O
the	O
end	O
of	O
the	O
episodeover	O
the	O
episodes	B
chapter	O
eligibility	B
traces	I
figure	O
the	O
forward	O
view	O
we	O
decide	O
how	O
to	O
update	O
each	O
state	B
by	O
looking	O
forward	O
to	O
future	O
rewards	O
and	O
states	O
td	B
td	B
is	O
one	O
of	O
the	O
oldest	O
and	O
most	O
widely	O
used	O
algorithms	O
in	O
reinforcement	B
learning	I
it	O
was	O
the	O
first	O
algorithm	O
for	O
which	O
a	O
formal	O
relationship	O
was	O
shown	O
between	O
a	O
more	O
theoretical	O
forward	O
view	O
and	O
a	O
more	O
computationally	O
congenial	O
backward	O
view	O
using	O
eligibility	B
traces	I
here	O
we	O
will	O
show	O
empirically	O
that	O
it	O
approximates	O
the	O
off-line	B
algorithm	O
presented	O
in	O
the	O
previous	O
section	O
td	B
improves	O
over	O
the	O
off-line	B
algorithm	O
in	O
three	O
ways	O
first	O
it	O
updates	O
the	O
weight	O
vector	B
on	O
every	O
step	O
of	O
an	O
episode	O
rather	O
than	O
only	O
at	O
the	O
end	O
and	O
thus	O
its	O
estimates	O
may	O
be	O
better	O
sooner	O
second	O
its	O
computations	O
are	O
equally	O
distributed	O
in	O
time	O
rather	O
that	O
all	O
at	O
the	O
end	O
of	O
the	O
episode	O
and	O
third	O
it	O
can	O
be	O
applied	O
to	O
continuing	O
problems	O
rather	O
than	O
just	O
episodic	O
problems	O
in	O
this	O
section	O
we	O
present	O
the	O
semi-gradient	O
version	O
of	O
td	B
with	B
function	B
approximation	I
with	B
function	B
approximation	I
the	O
eligibility	O
trace	O
is	O
a	O
vector	B
zt	O
rd	O
with	O
the	O
same	O
number	O
of	O
components	O
as	O
the	O
weight	O
vector	B
wt	O
whereas	O
the	O
weight	O
vector	B
is	O
a	O
long-term	O
memory	O
accumulating	B
over	O
the	O
lifetime	O
of	O
the	O
system	O
the	O
eligibility	O
trace	O
is	O
a	O
short-term	O
memory	O
typically	O
lasting	O
less	O
time	O
than	O
the	O
length	O
of	O
an	O
episode	O
eligibility	B
traces	I
assist	O
in	O
the	O
learning	O
process	O
their	O
only	O
consequence	O
is	O
that	O
they	O
affect	O
the	O
weight	O
vector	B
and	O
then	O
the	O
weight	O
vector	B
determines	O
the	O
estimated	O
value	B
in	O
td	B
the	O
eligibility	O
trace	O
vector	B
is	O
initialized	O
to	O
zero	O
at	O
the	O
beginning	O
of	O
the	O
episode	O
is	O
incremented	O
on	O
each	O
time	O
step	O
by	O
the	O
value	B
gradient	B
and	O
then	O
fades	O
away	O
by	O
z	O
zt	O
vstwt	O
zt	O
t	O
t	O
where	O
is	O
the	O
discount	O
rate	O
and	O
is	O
the	O
parameter	O
introduced	O
in	O
the	O
previous	O
section	O
which	O
we	O
henceforth	O
call	O
the	O
trace-decay	O
parameter	O
the	O
eligibility	O
trace	O
keeps	O
track	O
of	O
which	O
components	O
of	O
the	O
weight	O
vector	B
have	O
contributed	O
positively	O
or	O
negatively	O
to	O
recent	O
state	B
valuations	O
where	O
recent	O
is	O
defined	O
in	O
terms	O
of	O
that	O
in	O
linear	O
function	B
approximation	I
vstwt	O
is	O
just	O
the	O
feature	O
vector	B
xt	O
in	O
which	O
case	O
the	O
eligibility	O
trace	O
vector	B
is	O
just	O
a	O
sum	O
of	O
past	O
fading	O
input	O
vectors	O
the	O
trace	O
is	O
said	O
to	O
indicate	O
the	O
eligibility	O
of	O
each	O
component	O
of	O
the	O
weight	O
vector	B
for	O
undergoing	O
learning	O
td	B
changes	O
should	O
a	O
reinforcing	O
event	O
occur	O
the	O
reinforcing	O
events	O
we	O
are	O
concerned	O
with	O
are	O
the	O
moment-by-moment	O
one-step	O
td	B
errors	O
the	O
td	B
error	I
for	O
state-value	O
prediction	B
is	O
t	O
vstwt	O
in	O
td	B
the	O
weight	O
vector	B
is	O
updated	O
on	O
each	O
step	O
proportional	O
to	O
the	O
scalar	O
td	B
error	I
and	O
the	O
vector	B
eligibility	O
trace	O
wt	O
t	O
zt	O
semi-gradient	O
td	B
for	O
estimating	O
v	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
input	O
a	O
differentiable	O
function	O
v	O
s	O
rd	O
r	O
such	O
that	O
vterminal	O
algorithm	O
parameters	O
step	O
size	O
trace	O
decay	O
rate	O
initialize	O
value-function	O
weights	O
w	O
arbitrarily	O
w	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
z	O
loop	O
for	O
each	O
step	O
of	O
episode	O
choose	O
a	O
take	O
action	B
a	O
observe	O
r	O
z	O
z	O
vsw	O
r	O
vsw	O
w	O
w	O
z	O
s	O
until	O
is	O
terminal	O
d-dimensional	O
vector	B
figure	O
the	O
backward	O
or	O
mechanistic	O
view	O
of	O
td	B
each	O
update	O
depends	O
on	O
the	O
current	O
td	B
error	I
combined	O
with	O
the	O
current	O
eligibility	B
traces	I
of	O
past	O
events	O
chapter	O
eligibility	B
traces	I
td	B
is	O
oriented	O
backward	O
in	O
time	O
at	O
each	O
moment	O
we	O
look	O
at	O
the	O
current	O
td	B
error	I
and	O
assign	O
it	O
backward	O
to	O
each	O
prior	O
state	B
according	O
to	O
how	O
much	O
that	O
state	B
contributed	O
to	O
the	O
current	O
eligibility	O
trace	O
at	O
that	O
time	O
we	O
might	O
imagine	O
ourselves	O
riding	O
along	O
the	O
stream	O
of	O
states	O
computing	O
td	B
errors	O
and	O
shouting	O
them	O
back	O
to	O
the	O
previously	O
visited	O
states	O
as	O
suggested	O
by	O
figure	O
where	O
the	O
td	B
error	I
and	O
traces	O
come	O
together	O
we	O
get	O
the	O
update	O
given	O
by	O
changing	O
the	O
values	O
of	O
those	O
past	O
states	O
for	O
when	O
they	O
occur	O
again	O
in	O
the	O
future	O
to	O
better	O
understand	O
the	O
backward	O
view	O
of	O
td	B
consider	O
what	O
happens	O
at	O
various	O
values	O
of	O
if	O
then	O
by	O
the	O
trace	O
at	O
t	O
is	O
exactly	O
the	O
value	B
gradient	B
corresponding	O
to	O
st	O
thus	O
the	O
td	B
update	O
reduces	O
to	O
the	O
one-step	O
semi-gradient	O
td	B
update	O
treated	O
in	O
chapter	O
in	O
the	O
tabular	O
case	O
to	O
the	O
simple	O
td	B
rule	O
this	O
is	O
why	O
that	O
algorithm	O
was	O
called	O
in	O
terms	O
of	O
figure	O
is	O
the	O
case	O
in	O
which	O
only	O
the	O
one	O
state	B
preceding	O
the	O
current	O
one	O
is	O
changed	O
by	O
the	O
td	B
error	I
for	O
larger	O
values	O
of	O
but	O
still	O
more	O
of	O
the	O
preceding	O
states	O
are	O
changed	O
but	O
each	O
more	O
temporally	O
distant	O
state	B
is	O
changed	O
less	O
because	O
the	O
corresponding	O
eligibility	O
trace	O
is	O
smaller	O
as	O
suggested	O
by	O
the	O
figure	O
we	O
say	O
that	O
the	O
earlier	O
states	O
are	O
given	O
less	O
credit	O
for	O
the	O
td	B
error	I
if	O
then	O
the	O
credit	O
given	O
to	O
earlier	O
states	O
falls	O
only	O
by	O
per	O
step	O
this	O
turns	O
out	O
to	O
be	O
just	O
the	O
right	O
thing	O
to	O
do	O
to	O
achieve	O
monte	B
carlo	I
behavior	O
for	O
example	O
remember	O
that	O
the	O
td	B
error	I
t	O
includes	O
an	O
undiscounted	O
term	O
of	O
in	O
passing	O
this	O
back	O
k	O
steps	O
it	O
needs	O
to	O
be	O
discounted	O
like	O
any	O
reward	O
in	O
a	O
return	B
by	O
k	O
which	O
is	O
just	O
what	O
the	O
falling	O
eligibility	O
trace	O
achieves	O
if	O
and	O
then	O
the	O
eligibility	B
traces	I
do	O
not	O
decay	O
at	O
all	O
with	O
time	O
in	O
this	O
case	O
the	O
method	O
behaves	O
like	O
a	O
monte	B
carlo	I
method	O
for	O
an	O
undiscounted	O
episodic	O
task	O
if	O
the	O
algorithm	O
is	O
also	O
known	O
as	O
is	O
a	O
way	O
of	O
implementing	O
monte	B
carlo	I
algorithms	O
that	O
is	O
more	O
general	O
than	O
those	O
presented	O
earlier	O
and	O
that	O
significantly	O
increases	O
their	O
range	O
of	O
applicability	O
whereas	O
the	O
earlier	O
monte	B
carlo	I
methods	I
were	O
limited	O
to	O
episodic	O
tasks	O
can	O
be	O
applied	O
to	O
discounted	O
continuing	B
tasks	I
as	O
well	O
moreover	O
can	O
be	O
performed	O
incrementally	O
and	O
online	B
one	O
disadvantage	O
of	O
monte	B
carlo	I
methods	I
is	O
that	O
they	O
learn	O
nothing	O
from	O
an	O
episode	O
until	O
it	O
is	O
over	O
for	O
example	O
if	O
a	O
monte	B
carlo	I
control	B
method	O
takes	O
an	O
action	B
that	O
produces	O
a	O
very	O
poor	O
reward	O
but	O
does	O
not	O
end	O
the	O
episode	O
then	O
the	O
agent	O
s	O
tendency	O
to	O
repeat	O
the	O
action	B
will	O
be	O
undiminished	O
during	O
the	O
episode	O
online	B
on	O
the	O
other	O
hand	O
learns	O
in	O
an	O
n-step	B
td	B
way	O
from	O
the	O
incomplete	O
ongoing	O
episode	O
where	O
the	O
n	O
steps	O
are	O
all	O
the	O
way	O
up	O
to	O
the	O
current	O
step	O
if	O
something	O
unusually	O
good	O
or	O
bad	O
happens	O
during	O
an	O
episode	O
control	B
methods	O
based	O
on	O
can	O
learn	O
immediately	O
and	O
alter	O
their	O
behavior	O
on	O
that	O
same	O
episode	O
it	O
is	O
revealing	O
to	O
revisit	O
the	O
random	B
walk	I
example	O
to	O
see	O
how	O
well	O
td	B
ddoes	O
in	O
approximating	O
the	O
off-line	B
algorithm	O
the	O
results	O
for	O
both	O
algorithms	O
are	O
shown	O
in	O
figure	O
for	O
each	O
value	B
if	O
is	O
selected	O
optimally	O
for	O
it	O
smaller	O
then	O
the	O
two	O
algorithms	O
perform	O
virtually	O
identically	O
if	O
is	O
chosen	O
larger	O
than	O
is	O
optimal	O
however	O
then	O
the	O
algorithm	O
is	O
only	O
a	O
little	O
worse	O
whereas	O
td	B
is	O
much	O
worse	O
and	O
may	O
even	O
be	O
unstable	O
this	O
is	O
not	O
catastrophic	O
for	O
td	B
on	O
this	O
problem	O
as	O
these	O
higher	O
parameter	O
values	O
are	O
not	O
what	O
one	O
would	O
want	O
to	O
use	O
anyway	O
but	O
for	O
other	O
problems	O
it	O
can	O
be	O
a	O
significant	O
weakness	O
n-step	B
truncated	B
methods	O
figure	O
random	B
walk	I
results	O
performance	O
of	O
td	B
alongside	O
that	O
of	O
the	O
off-line	B
algorithm	O
the	O
two	O
algorithms	O
performed	O
virtually	O
identically	O
at	O
low	O
than	O
optimal	O
values	O
but	O
td	B
was	O
worse	O
at	O
high	O
values	O
linear	O
td	B
has	O
been	O
proved	O
to	O
converge	O
in	O
the	O
on-policy	O
case	O
if	O
the	O
step-size	B
parameter	I
is	O
reduced	O
over	O
time	O
according	O
to	O
the	O
usual	O
conditions	O
just	O
as	O
discussed	O
in	O
section	O
convergence	O
is	O
not	O
to	O
the	O
minimum-error	O
weight	O
vector	B
but	O
to	O
a	O
nearby	O
weight	O
vector	B
that	O
depends	O
on	O
the	O
bound	O
on	O
solution	O
quality	O
presented	O
in	O
that	O
section	O
can	O
now	O
be	O
generalized	O
to	O
apply	O
to	O
any	O
for	O
the	O
continuing	O
discounted	O
case	O
vew	O
min	O
w	O
vew	O
that	O
is	O
the	O
asymptotic	O
error	O
is	O
no	O
more	O
than	O
times	O
the	O
smallest	O
possible	O
error	O
as	O
approaches	O
the	O
bound	O
approaches	O
the	O
minimum	O
error	O
it	O
is	O
loosest	O
at	O
in	O
practice	O
however	O
is	O
often	O
the	O
poorest	O
choice	O
as	O
will	O
be	O
illustrated	O
later	O
in	O
figure	O
exercise	O
some	O
insight	O
into	O
how	O
td	B
can	O
closely	O
approximate	O
the	O
off-line	B
algorithm	O
can	O
be	O
gained	O
by	O
seeing	O
that	O
the	O
latter	O
s	O
error	O
term	O
brackets	O
in	O
can	O
be	O
written	O
as	O
the	O
sum	O
of	O
td	B
errors	O
for	O
a	O
single	O
fixed	O
w	O
show	O
this	O
following	O
the	O
pattern	O
of	O
and	O
using	O
the	O
recursive	O
relationship	O
for	O
the	O
you	O
obtained	O
in	O
exercise	O
exercise	O
use	O
your	O
result	O
from	O
the	O
preceding	O
exercise	O
to	O
show	O
that	O
if	O
the	O
weight	O
updates	O
over	O
an	O
episode	O
were	O
computed	O
on	O
each	O
step	O
but	O
not	O
actually	O
used	O
to	O
change	O
the	O
weights	O
remained	O
fixed	O
then	O
the	O
sum	O
of	O
td	B
s	O
weight	O
updates	O
would	O
be	O
the	O
same	O
as	O
the	O
sum	O
of	O
the	O
off-line	B
algorithm	O
s	O
updates	O
n-step	B
truncated	B
methods	O
the	O
off-line	B
algorithm	O
is	O
an	O
important	O
ideal	O
but	O
it	O
s	O
of	O
limited	O
utility	O
because	O
it	O
uses	O
the	O
which	O
is	O
not	O
known	O
until	O
the	O
end	O
of	O
the	O
episode	O
in	O
the	O
off-line	B
algorithmfrom	O
the	O
previous	O
section	O
errorat	O
the	O
end	O
of	O
the	O
episodeover	O
the	O
chapter	O
eligibility	B
traces	I
continuing	O
case	O
the	O
is	O
technically	O
never	O
known	O
as	O
it	O
depends	O
on	O
n-step	B
returns	O
for	O
arbitrarily	O
large	O
n	O
and	O
thus	O
on	O
rewards	O
arbitrarily	O
far	O
in	O
the	O
future	O
however	O
the	O
dependence	O
gets	O
weaker	O
for	O
long-delayed	O
rewards	O
falling	O
by	O
for	O
each	O
step	O
of	O
delay	O
a	O
natural	O
approximation	O
then	O
would	O
be	O
to	O
truncate	O
the	O
sequence	O
after	O
some	O
number	O
of	O
steps	O
our	O
existing	O
notion	O
of	O
n-step	B
returns	O
provides	O
a	O
natural	O
way	O
to	O
do	O
this	O
in	O
which	O
the	O
missing	O
rewards	O
are	O
replaced	O
with	O
estimated	O
values	O
in	O
general	O
we	O
define	O
the	O
truncated	B
for	O
time	O
t	O
given	O
data	O
only	O
up	O
to	O
some	O
later	O
horizon	O
h	O
as	O
g	O
th	O
h	O
t	O
n	O
h	O
t	O
t	O
h	O
t	O
if	O
you	O
compare	O
this	O
equation	O
with	O
the	O
it	O
is	O
clear	O
that	O
the	O
horizon	O
h	O
is	O
playing	O
the	O
same	O
role	O
as	O
was	O
previously	O
played	O
by	O
t	O
the	O
time	O
of	O
termination	O
whereas	O
in	O
the	O
there	O
is	O
a	O
residual	O
weighting	O
given	O
to	O
the	O
true	O
return	B
here	O
it	O
is	O
given	O
to	O
the	O
longest	O
available	O
n-step	B
return	B
the	O
t-step	O
return	B
the	O
truncated	B
immediately	O
gives	O
rise	O
to	O
a	O
family	O
of	O
n-step	B
algorithms	O
similar	O
to	O
the	O
n-step	B
methods	I
of	O
chapter	O
in	O
all	O
these	O
algorithms	O
updates	O
are	O
delayed	O
by	O
n	O
steps	O
and	O
only	O
take	O
into	O
account	O
the	O
first	O
n	O
rewards	O
but	O
now	O
all	O
the	O
k-step	O
returns	O
are	O
included	O
for	O
k	O
n	O
the	O
earlier	O
n-step	B
algorithms	O
used	O
only	O
the	O
n-step	B
return	B
weighted	O
geometrically	O
as	O
in	O
figure	O
in	O
the	O
state-value	O
case	O
this	O
family	O
of	O
algorithms	O
is	O
known	O
as	O
truncated	B
td	B
or	O
ttd	O
the	O
compound	B
backup	B
diagram	I
shown	O
in	O
figure	O
is	O
similar	O
to	O
that	O
for	O
td	B
except	O
that	O
the	O
longest	O
component	O
update	O
is	O
at	O
most	O
n	O
steps	O
rather	O
than	O
always	O
going	O
all	O
the	O
way	O
to	O
the	O
end	O
of	O
figure	O
the	O
backup	B
diagram	I
for	O
truncated	B
td	B
redoing	O
updates	O
the	O
online	B
algorithm	O
the	O
episode	O
ttd	O
is	O
defined	O
by	O
wtn	O
wtn	O
ttn	O
vstwtn	O
vstwtn	O
t	O
t	O
this	O
algorithm	O
can	O
be	O
implemented	O
efficiently	O
so	O
that	O
per-step	O
computation	O
does	O
not	O
scale	O
with	O
n	O
of	O
course	O
memory	O
must	O
much	O
as	O
in	O
n-step	B
td	B
methods	O
no	O
updates	O
are	O
made	O
on	O
the	O
first	O
n	O
time	O
steps	O
and	O
n	O
additional	O
updates	O
are	O
made	O
upon	O
termination	O
efficient	O
implementation	O
relies	O
on	O
the	O
fact	O
that	O
the	O
k-step	O
can	O
be	O
written	O
exactly	O
as	O
g	O
ttk	O
vstwt	O
tk	O
t	O
where	O
vstwt	O
exercise	O
several	O
times	O
in	O
this	O
book	O
in	O
exercises	O
we	O
have	O
established	O
that	O
returns	O
can	O
be	O
written	O
as	O
sums	O
of	O
td	B
errors	O
if	O
the	O
value	B
function	I
is	O
held	O
constant	O
why	O
is	O
another	O
instance	O
of	O
this	O
prove	O
redoing	O
updates	O
the	O
online	B
algo	O
rithm	O
choosing	O
the	O
truncation	O
parameter	O
n	O
in	O
truncated	B
td	B
involves	O
a	O
tradeoff	O
n	O
should	O
be	O
large	O
so	O
that	O
the	O
method	O
closely	O
approximates	O
the	O
off-line	B
algorithm	O
but	O
it	O
should	O
also	O
be	O
small	O
so	O
that	O
the	O
updates	O
can	O
be	O
made	O
sooner	O
and	O
can	O
influence	O
behavior	O
sooner	O
can	O
we	O
get	O
the	O
best	O
of	O
both	O
well	O
yes	O
in	O
principle	O
we	O
can	O
albeit	O
at	O
the	O
cost	O
of	O
computational	O
complexity	O
the	O
idea	O
is	O
that	O
on	O
each	O
time	O
step	O
as	O
you	O
gather	O
a	O
new	O
increment	O
of	O
data	O
you	O
go	O
back	O
and	O
redo	O
all	O
the	O
updates	O
since	O
the	O
beginning	O
of	O
the	O
current	O
episode	O
the	O
new	O
updates	O
will	O
be	O
better	O
than	O
the	O
ones	O
you	O
previously	O
made	O
because	O
now	O
they	O
can	O
take	O
into	O
account	O
the	O
time	O
step	O
s	O
new	O
data	O
that	O
is	O
the	O
updates	O
are	O
always	O
towards	O
an	O
n-step	B
truncated	B
target	O
but	O
they	O
always	O
use	O
the	O
latest	O
horizon	O
in	O
each	O
pass	O
over	O
that	O
episode	O
you	O
can	O
use	O
a	O
slightly	O
longer	O
horizon	O
and	O
obtain	O
slightly	O
better	O
results	O
recall	O
that	O
the	O
n-step	B
truncated	B
is	O
defined	O
by	O
g	O
th	O
h	O
t	O
n	O
h	O
t	O
let	O
us	O
step	O
through	O
how	O
this	O
target	O
could	O
ideally	O
be	O
used	O
if	O
computational	O
complexity	O
was	O
not	O
an	O
issue	O
the	O
episode	O
begins	O
with	O
an	O
estimate	O
at	O
time	O
using	O
the	O
weights	O
from	O
the	O
end	O
of	O
the	O
previous	O
episode	O
learning	O
begins	O
when	O
the	O
data	O
horizon	O
is	O
extended	O
to	O
time	O
step	O
the	O
target	O
for	O
the	O
estimate	O
at	O
step	O
given	O
the	O
data	O
up	O
to	O
horizon	O
could	O
chapter	O
eligibility	B
traces	I
only	O
be	O
the	O
one-step	O
return	B
which	O
includes	O
and	O
bootstraps	O
from	O
the	O
estimate	O
note	O
that	O
this	O
is	O
exactly	O
what	O
g	O
is	O
with	O
the	O
sum	O
in	O
the	O
first	O
term	O
of	O
degenerating	O
to	O
zero	O
using	O
this	O
update	O
target	O
we	O
construct	O
then	O
after	O
advancing	O
the	O
data	O
horizon	O
to	O
step	O
what	O
do	O
we	O
do	O
we	O
have	O
new	O
data	O
in	O
the	O
form	O
of	O
and	O
as	O
well	O
as	O
the	O
new	O
so	O
now	O
we	O
can	O
construct	O
a	O
better	O
update	O
target	O
g	O
for	O
the	O
first	O
update	O
from	O
as	O
well	O
as	O
a	O
better	O
update	O
target	O
g	O
for	O
the	O
second	O
update	O
from	O
using	O
these	O
improved	O
targets	O
we	O
redo	O
the	O
updates	O
at	O
and	O
starting	O
again	O
from	O
to	O
produce	O
now	O
we	O
advance	O
the	O
horizon	O
to	O
step	O
and	O
repeat	O
going	O
all	O
the	O
way	O
back	O
to	O
produce	O
three	O
new	O
targets	O
redoing	O
all	O
updates	O
starting	O
from	O
the	O
original	O
to	O
produce	O
and	O
so	O
on	O
each	O
time	O
the	O
horizon	O
is	O
advanced	O
all	O
the	O
updates	O
are	O
redone	O
starting	O
from	O
using	O
the	O
weight	O
vector	B
from	O
the	O
preceding	O
horizon	O
this	O
conceptual	O
algorithm	O
involves	O
multiple	O
passes	O
over	O
the	O
episode	O
one	O
at	O
each	O
horizon	O
each	O
generating	O
a	O
different	O
sequence	O
of	O
weight	O
vectors	O
to	O
describe	O
it	O
clearly	O
we	O
have	O
to	O
distinguish	O
between	O
the	O
weight	O
vectors	O
computed	O
at	O
the	O
different	O
horizons	O
let	O
us	O
use	O
wh	O
t	O
to	O
denote	O
the	O
weights	O
used	O
to	O
generate	O
the	O
value	B
at	O
time	O
t	O
in	O
the	O
sequence	O
at	O
horizon	O
h	O
the	O
first	O
weight	O
vector	B
wh	O
in	O
each	O
sequence	O
is	O
that	O
inherited	O
from	O
the	O
previous	O
episode	O
and	O
the	O
last	O
weight	O
vector	B
wh	O
h	O
in	O
each	O
sequence	O
defines	O
the	O
ultimate	O
weight-vector	O
sequence	O
of	O
the	O
algorithm	O
at	O
the	O
final	O
horizon	O
h	O
t	O
we	O
obtain	O
the	O
final	O
weights	O
wt	O
t	O
which	O
will	O
be	O
passed	O
on	O
to	O
form	O
the	O
initial	O
weights	O
of	O
the	O
next	O
episode	O
with	O
these	O
conventions	O
the	O
three	O
first	O
sequences	O
described	O
in	O
the	O
previous	O
paragraph	O
can	O
be	O
given	O
explicitly	O
h	O
h	O
h	O
th	O
vstwh	O
wt	O
the	O
general	O
form	O
for	O
the	O
update	O
is	O
wh	O
wh	O
t	O
t	O
vstwh	O
t	O
t	O
h	O
t	O
this	O
update	O
together	O
with	O
wt	O
t	O
defines	O
the	O
online	B
algorithm	O
the	O
online	B
algorithm	O
is	O
fully	O
online	B
determining	O
a	O
new	O
weight	O
vector	B
wt	O
at	O
each	O
step	O
t	O
during	O
an	O
episode	O
using	O
only	O
information	O
available	O
at	O
time	O
t	O
its	O
main	O
drawback	O
is	O
that	O
it	O
is	O
computationally	O
complex	O
passing	O
over	O
the	O
entire	O
episode	O
so	O
far	O
on	O
every	O
step	O
note	O
that	O
it	O
is	O
strictly	O
more	O
complex	O
than	O
the	O
off-line	B
algorithm	O
which	O
passes	O
through	O
all	O
the	O
steps	O
at	O
the	O
time	O
of	O
termination	O
but	O
does	O
not	O
make	O
any	O
updates	O
during	O
the	O
episode	O
in	O
return	B
the	O
online	B
algorithm	O
can	O
be	O
expected	B
to	O
perform	O
better	O
than	O
the	O
off-line	B
one	O
not	O
only	O
during	O
the	O
episode	O
when	O
it	O
makes	O
an	O
update	O
while	O
true	B
online	B
td	B
the	O
off-line	B
algorithm	O
makes	O
none	O
but	O
also	O
at	O
the	O
end	O
of	O
the	O
episode	O
because	O
the	O
weight	O
vector	B
used	O
in	O
bootstrapping	B
g	O
th	O
has	O
had	O
a	O
greater	O
number	O
of	O
informative	O
updates	O
this	O
effect	O
can	O
be	O
seen	O
if	O
one	O
looks	O
carefully	O
at	O
figure	O
which	O
compares	O
the	O
two	O
algorithms	O
on	O
the	O
random	B
walk	I
task	O
figure	O
random	B
walk	I
results	O
performance	O
of	O
online	B
and	O
off-line	B
algorithms	O
the	O
performance	O
measure	O
here	O
is	O
the	O
ve	O
at	O
the	O
end	O
of	O
the	O
episode	O
which	O
should	O
be	O
the	O
best	O
case	O
for	O
the	O
off-line	B
algorithm	O
nevertheless	O
the	O
online	B
algorithm	O
performs	O
subtly	O
better	O
for	O
comparison	O
the	O
line	O
is	O
the	O
same	O
for	O
both	O
methods	O
true	B
online	B
td	B
the	O
online	B
algorithm	O
just	O
presented	O
is	O
currently	O
the	O
best	O
performing	O
temporaldifference	O
algorithm	O
it	O
is	O
an	O
ideal	O
which	O
online	B
td	B
only	O
approximates	O
as	O
presented	O
however	O
the	O
online	B
algorithm	O
is	O
very	O
complex	O
is	O
there	O
a	O
way	O
to	O
invert	O
this	O
forward-view	O
algorithm	O
to	O
produce	O
an	O
efficient	O
backward-view	O
algorithm	O
using	O
eligibility	B
traces	I
it	O
turns	O
out	O
that	O
there	O
is	O
indeed	O
an	O
exact	O
computationally	O
congenial	O
implementation	O
of	O
the	O
online	B
algorithm	O
for	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
this	O
implementation	O
is	O
known	O
as	O
the	O
true	B
online	B
td	B
algorithm	O
because	O
it	O
is	O
truer	O
to	O
the	O
ideal	O
of	O
the	O
online	B
algorithm	O
than	O
the	O
td	B
algorithm	O
is	O
the	O
derivation	O
of	O
true	B
online	B
td	B
is	O
a	O
little	O
too	O
complex	O
to	O
present	O
here	O
the	O
next	O
section	O
and	O
the	O
appendix	O
to	O
the	O
paper	O
by	O
van	O
seijen	O
et	O
al	O
but	O
its	O
strategy	O
is	O
simple	O
the	O
sequence	O
of	O
weight	O
vectors	O
produced	O
by	O
the	O
online	B
algorithm	O
can	O
be	O
arranged	O
in	O
a	O
triangle	O
wt	O
wt	O
wt	O
wt	O
wt	O
t	O
off-line	B
algorithmfrom	O
section	O
rms	O
errorat	O
the	O
end	O
of	O
the	O
episodeover	O
the	O
algorithm	O
true	B
online	B
td	B
chapter	O
eligibility	B
traces	I
t	O
are	O
really	O
needed	O
the	O
first	O
t	O
is	O
the	O
output	O
and	O
each	O
weight	O
vector	B
along	O
the	O
way	O
wt	O
it	O
turns	O
out	O
that	O
only	O
the	O
one	O
row	O
of	O
this	O
triangle	O
is	O
produced	O
on	O
each	O
time	O
step	O
weight	O
vectors	O
on	O
the	O
diagonal	O
the	O
wt	O
is	O
the	O
input	O
the	O
last	O
wt	O
t	O
plays	O
a	O
role	O
in	O
bootstrapping	B
in	O
the	O
n-step	B
returns	O
of	O
the	O
updates	O
in	O
the	O
final	O
algorithm	O
the	O
diagonal	O
weight	O
vectors	O
are	O
renamed	O
without	O
a	O
superscript	O
wt	O
t	O
the	O
strategy	O
then	O
is	O
to	O
find	O
a	O
compact	O
efficient	O
way	O
of	O
computing	O
each	O
wt	O
t	O
from	O
the	O
one	O
before	O
if	O
this	O
is	O
done	O
for	O
the	O
linear	O
case	O
in	O
which	O
vsw	O
then	O
we	O
arrive	O
at	O
the	O
true	B
online	B
td	B
algorithm	O
wt	O
xst	O
t	O
is	O
defined	O
as	O
in	O
td	B
and	O
zt	O
where	O
we	O
have	O
used	O
the	O
shorthand	O
xt	O
is	O
defined	O
by	O
wt	O
t	O
zt	O
xt	O
xt	O
zt	O
xt	O
zt	O
this	O
algorithm	O
has	O
been	O
proven	O
to	O
produce	O
exactly	O
the	O
same	O
sequence	O
of	O
weight	O
vectors	O
wt	O
t	O
t	O
as	O
the	O
online	B
algorithm	O
seijen	O
et	O
al	O
thus	O
the	O
results	O
on	O
the	O
random	B
walk	I
task	O
on	O
the	O
left	O
of	O
figure	O
are	O
also	O
its	O
results	O
on	O
that	O
task	O
now	O
however	O
the	O
algorithm	O
is	O
much	O
less	O
expensive	O
the	O
memory	O
requirements	O
of	O
true	B
online	B
td	B
are	O
identical	O
to	O
those	O
of	O
conventional	O
td	B
while	O
the	O
per-step	O
computation	O
is	O
increased	O
by	O
about	O
is	O
one	O
more	O
inner	O
product	O
in	O
the	O
eligibility-trace	O
update	O
overall	O
the	O
per-step	O
computational	O
complexity	O
remains	O
of	O
od	O
the	O
same	O
as	O
td	B
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
true	B
online	B
td	B
for	O
estimating	O
v	O
input	O
the	O
policy	B
to	O
be	O
evaluated	O
input	O
a	O
feature	O
function	O
x	O
s	O
rd	O
such	O
that	O
xterminal	O
algorithm	O
parameters	O
step	O
size	O
trace	O
decay	O
rate	O
initialize	O
value-function	O
weights	O
w	O
rd	O
w	O
loop	O
for	O
each	O
episode	O
d-dimensional	O
vector	B
temporary	O
scalar	O
variable	O
initialize	O
state	B
and	O
obtain	O
initial	O
feature	O
vector	B
x	O
z	O
vold	O
loop	O
for	O
each	O
step	O
of	O
episode	O
choose	O
a	O
take	O
action	B
a	O
observe	O
r	O
vector	B
of	O
the	O
next	O
state	B
v	O
v	O
r	O
v	O
v	O
z	O
z	O
x	O
w	O
w	O
v	O
voldz	O
voldx	O
vold	O
v	O
x	O
until	O
arrival	O
at	O
a	O
terminal	O
state	B
dutch	B
traces	O
in	O
monte	B
carlo	I
learning	O
the	O
eligibility	O
trace	O
used	O
in	O
true	B
online	B
td	B
is	O
called	O
a	O
dutch	B
trace	O
to	O
distinguish	O
it	O
from	O
the	O
trace	O
used	O
in	O
td	B
which	O
is	O
called	O
an	O
accumulating	B
trace	O
earlier	O
work	O
often	O
used	O
a	O
third	O
kind	O
of	O
trace	O
called	O
the	O
replacing	B
trace	O
defined	O
only	O
for	O
the	O
tabular	O
case	O
or	O
for	O
binary	O
feature	O
vectors	O
such	O
as	O
those	O
produced	O
by	O
tile	B
coding	I
the	O
replacing	B
trace	O
is	O
defined	O
on	O
a	O
component-by-component	O
basis	O
depending	O
on	O
whether	O
the	O
component	O
of	O
the	O
feature	O
vector	B
was	O
or	O
zit	O
zit	O
if	O
xit	O
otherwise	O
nowadays	O
use	O
of	O
the	O
replacing	B
trace	O
is	O
deprecated	B
a	O
dutch	B
trace	O
should	O
almost	O
always	O
be	O
used	O
instead	O
dutch	B
traces	O
in	O
monte	B
carlo	I
learning	O
although	O
eligibility	B
traces	I
are	O
closely	O
associated	O
historically	O
with	O
td	B
learning	O
in	O
fact	O
they	O
have	O
nothing	O
to	O
do	O
with	O
it	O
in	O
fact	O
eligibility	B
traces	I
arise	O
even	O
in	O
monte	B
carlo	I
learning	O
as	O
we	O
show	O
in	O
this	O
section	O
we	O
show	O
that	O
the	O
linear	O
mc	O
algorithm	O
taken	O
as	O
a	O
forward	O
view	O
can	O
be	O
used	O
to	O
derive	O
an	O
equivalent	O
yet	O
computationally	O
cheaper	O
backward-view	O
algorithm	O
using	O
dutch	B
traces	O
this	O
is	O
the	O
only	O
equivalence	O
of	O
forwardand	O
backward-views	O
that	O
we	O
explicitly	O
demonstrate	O
in	O
this	O
book	O
it	O
gives	O
some	O
of	O
the	O
flavor	O
of	O
the	O
proof	B
of	O
equivalence	O
of	O
true	B
online	B
td	B
and	O
the	O
online	B
algorithm	O
but	O
is	O
much	O
simpler	O
the	O
linear	O
version	O
of	O
the	O
gradient	B
monte	B
carlo	I
prediction	B
algorithm	O
makes	O
the	O
following	O
sequence	O
of	O
updates	O
one	O
for	O
each	O
time	O
step	O
of	O
the	O
episode	O
wt	O
xt	O
t	O
t	O
to	O
make	O
the	O
example	O
simpler	O
we	O
assume	O
here	O
that	O
the	O
return	B
g	O
is	O
a	O
single	O
reward	O
received	O
at	O
the	O
end	O
of	O
the	O
episode	O
is	O
why	O
g	O
is	O
not	O
subscripted	O
by	O
time	O
and	O
that	O
there	O
is	O
no	O
discounting	B
in	O
this	O
case	O
the	O
update	O
is	O
also	O
known	O
as	O
the	O
least	O
mean	O
square	O
rule	O
as	O
a	O
monte	B
carlo	I
algorithm	O
all	O
the	O
updates	O
depend	O
on	O
the	O
final	O
rewardreturn	O
so	O
none	O
can	O
be	O
made	O
until	O
the	O
end	O
of	O
the	O
episode	O
the	O
mc	O
algorithm	O
is	O
an	O
off-line	B
algorithm	O
and	O
we	O
do	O
not	O
seek	O
to	O
improve	O
this	O
aspect	O
of	O
it	O
rather	O
we	O
seek	O
merely	O
an	O
implementation	O
of	O
this	O
algorithm	O
with	O
computational	O
advantages	O
we	O
will	O
still	O
update	O
the	O
weight	O
vector	B
only	O
at	O
the	O
end	O
of	O
the	O
episode	O
but	O
we	O
will	O
do	O
some	O
computation	O
during	O
each	O
step	O
of	O
the	O
episode	O
and	O
less	O
at	O
its	O
end	O
this	O
will	O
give	O
a	O
more	O
equal	O
distribution	O
of	O
computation	O
od	O
per	O
step	O
and	O
also	O
remove	O
the	O
need	O
to	O
store	O
the	O
feature	O
vectors	O
at	O
each	O
step	O
for	O
use	O
later	O
at	O
the	O
end	O
of	O
each	O
episode	O
instead	O
we	O
will	O
introduce	O
an	O
additional	O
vector	B
memory	O
the	O
eligibility	O
trace	O
keeping	O
in	O
it	O
a	O
summary	O
of	O
all	O
the	O
feature	O
vectors	O
seen	O
so	O
far	O
this	O
will	O
be	O
sufficient	O
to	O
efficiently	O
recreate	O
exactly	O
the	O
same	O
overall	O
update	O
chapter	O
eligibility	B
traces	I
as	O
the	O
sequence	O
of	O
mc	O
updates	O
by	O
the	O
end	O
of	O
the	O
episode	O
wt	O
wt	O
xt	O
wt	O
xt	O
gxt	O
xt	O
wt	O
gxt	O
i	O
ft	O
gxt	O
where	O
ft	O
is	O
a	O
forgetting	O
or	O
fading	O
matrix	O
now	O
recursing	O
ft	O
gxt	O
gxt	O
ft	O
g	O
xt	O
ft	O
gxt	O
g	O
xt	O
ft	O
g	O
ft	O
xt	O
ft	O
ft	O
at	O
at	O
gzt	O
g	O
t	O
zt	O
where	O
at	O
and	O
zt	O
are	O
the	O
values	O
at	O
time	O
t	O
of	O
two	O
auxilary	O
memory	O
vectors	O
that	O
can	O
be	O
updated	O
incrementally	O
without	O
knowledge	O
of	O
g	O
and	O
with	O
od	O
complexity	O
per	O
time	O
step	O
the	O
zt	O
vector	B
is	O
in	O
fact	O
a	O
dutch-style	O
eligibility	O
trace	O
it	O
is	O
initialized	O
to	O
and	O
then	O
updated	O
according	O
to	O
ftft	O
t	O
t	O
which	O
is	O
the	O
dutch	B
trace	O
for	O
the	O
case	O
of	O
eq	O
the	O
at	O
auxilary	O
vector	B
is	O
initialized	O
to	O
and	O
then	O
updated	O
according	O
to	O
at	O
ftft	O
ftat	O
at	O
at	O
t	O
t	O
zt	O
t	O
t	O
ftft	O
xt	O
ft	O
ft	O
xt	O
ftzt	O
xt	O
zt	O
xt	O
zt	O
zt	O
xt	O
zt	O
xt	O
xt	O
zt	O
xt	O
sarsa	B
the	O
auxiliary	O
vectors	O
at	O
and	O
zt	O
are	O
updated	O
on	O
each	O
time	O
step	O
t	O
t	O
and	O
then	O
at	O
time	O
t	O
when	O
g	O
is	O
observed	O
they	O
are	O
used	O
in	O
to	O
compute	O
wt	O
in	O
this	O
way	O
we	O
achieve	O
exactly	O
the	O
same	O
final	O
result	O
as	O
the	O
mclms	O
algorithm	O
that	O
has	O
poor	O
computational	O
properties	O
but	O
now	O
with	O
an	O
incremental	O
algorithm	O
whose	O
time	O
and	O
memory	O
complexity	O
per	O
step	O
is	O
od	O
this	O
is	O
surprising	O
and	O
intriguing	O
because	O
the	O
notion	O
of	O
an	O
eligibility	O
trace	O
the	O
dutch	B
trace	O
in	O
particular	O
has	O
arisen	O
in	O
a	O
setting	O
without	O
temporal-difference	B
learning	I
contrast	O
to	O
van	O
seijen	O
and	O
sutton	O
it	O
seems	O
eligibility	B
traces	I
are	O
not	O
specific	O
to	O
td	B
learning	O
at	O
all	O
they	O
are	O
more	O
fundamental	O
than	O
that	O
the	O
need	O
for	O
eligibility	B
traces	I
seems	O
to	O
arise	O
whenever	O
one	O
tries	O
to	O
learn	O
long-term	O
predictions	O
in	O
an	O
efficient	O
manner	O
sarsa	B
very	O
few	O
changes	O
in	O
the	O
ideas	O
already	O
presented	O
in	O
this	O
chapter	O
are	O
required	O
in	O
order	O
to	O
extend	O
eligibility-traces	O
to	O
action-value	B
methods	I
to	O
learn	O
approximate	O
action	B
values	O
qs	O
a	O
w	O
rather	O
than	O
approximate	O
state	B
values	O
vsw	O
we	O
need	O
to	O
use	O
the	O
action-value	O
form	O
of	O
the	O
n-step	B
return	B
from	O
chapter	O
gttn	O
n	O
n	O
qstn	O
atn	O
wtn	O
gt	O
if	O
t	O
n	O
t	O
using	O
this	O
we	O
can	O
form	O
the	O
action-value	O
form	O
of	O
the	O
with	O
gttn	O
truncated	B
which	O
is	O
otherwise	O
identical	O
to	O
the	O
state-value	O
form	O
the	O
action-value	O
form	O
of	O
the	O
off-line	B
algorithm	O
simply	O
uses	O
q	O
rather	O
than	O
v	O
t	O
n	O
t	O
wt	O
t	O
qst	O
qst	O
at	O
wt	O
t	O
t	O
g	O
where	O
g	O
t	O
the	O
compound	B
backup	B
diagram	I
for	O
this	O
forward	O
view	O
is	O
shown	O
in	O
t	O
figure	O
notice	O
the	O
similarity	O
to	O
the	O
diagram	O
of	O
the	O
td	B
algorithm	O
the	O
first	O
update	O
looks	O
ahead	O
one	O
full	O
step	O
to	O
the	O
next	O
state	B
action	B
pair	O
the	O
second	O
looks	O
ahead	O
two	O
steps	O
to	O
the	O
second	O
state	B
action	B
pair	O
and	O
so	O
on	O
a	O
final	O
update	O
is	O
based	O
on	O
the	O
complete	O
return	B
the	O
weighting	O
of	O
each	O
n-step	B
update	O
in	O
the	O
is	O
just	O
as	O
in	O
td	B
and	O
the	O
algorithm	O
the	O
temporal-difference	O
method	O
for	B
action	B
values	I
known	O
as	O
sarsa	B
approximates	O
this	O
forward	O
view	O
it	O
has	O
the	O
same	O
update	O
rule	O
as	O
given	O
earlier	O
for	O
td	B
wt	O
t	O
zt	O
except	O
naturally	O
using	O
the	O
action-value	O
form	O
of	O
the	O
td	B
error	I
t	O
wt	O
qst	O
at	O
wt	O
and	O
the	O
action-value	O
form	O
of	O
the	O
eligibility	O
trace	O
z	O
zt	O
qst	O
at	O
wt	O
zt	O
t	O
t	O
chapter	O
eligibility	B
traces	I
figure	O
sarsa	B
s	O
backup	B
diagram	I
compare	O
with	O
figure	O
alternatively	O
the	O
replacing	B
trace	O
given	O
by	O
complete	O
pseudocode	O
for	B
sarsa	B
with	O
linear	O
function	B
approximation	I
binary	B
features	I
and	O
either	O
accumulating	B
or	O
replacing	B
traces	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
this	O
pseudocode	O
highlights	O
a	O
few	O
optimizations	O
possible	O
in	O
the	O
special	O
case	O
of	O
binary	B
features	I
are	O
either	O
active	O
or	O
inactive	O
example	O
traces	O
in	O
gridworld	O
the	O
use	O
of	O
eligibility	B
traces	I
can	O
substantially	O
increase	O
the	O
efficiency	O
of	O
control	B
algorithms	O
over	O
one-step	O
methods	O
and	O
even	O
over	O
n-step	B
methods	I
the	O
reason	O
for	O
this	O
is	O
illustrated	O
by	O
the	O
gridworld	O
example	O
below	O
the	O
first	O
panel	O
shows	O
the	O
path	O
taken	O
by	O
an	O
agent	O
in	O
a	O
single	O
episode	O
the	O
initial	O
estimated	O
values	O
were	O
zero	O
and	O
all	O
rewards	O
were	O
zero	O
except	O
for	O
a	O
positive	O
reward	O
at	O
the	O
goal	O
location	O
marked	O
by	O
g	O
the	O
arrows	O
in	O
the	O
other	O
panels	O
show	O
for	O
various	O
algorithms	O
which	O
action-values	O
would	O
be	O
increased	O
and	O
by	O
how	O
much	O
upon	O
reaching	O
the	O
goal	O
a	O
one-step	O
method	O
would	O
increment	O
only	O
the	O
last	O
action	B
value	B
whereas	O
an	O
n-step	B
method	O
would	O
equally	O
increment	O
the	O
last	O
n	O
actions	O
values	O
and	O
an	O
eligibility	O
trace	O
method	O
would	O
update	O
all	O
the	O
action	B
values	O
up	O
to	O
the	O
beginning	O
of	O
the	O
episode	O
to	O
different	O
degrees	O
fading	O
with	O
recency	O
the	O
fading	O
strategy	O
is	O
often	O
the	O
best	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increasedby	O
sarsa	B
with	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increasedby	O
sarsa	B
with	O
sarsagggpath	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increased	O
by	O
sarsaggg	O
sarsa	B
sarsa	B
with	O
binary	B
features	I
and	O
linear	O
function	B
approximation	I
for	O
estimating	O
q	O
or	O
q	O
input	O
a	O
function	O
fs	O
a	O
returning	O
the	O
set	O
of	O
of	O
active	O
features	O
for	O
s	O
a	O
input	O
a	O
policy	B
estimating	O
q	O
algorithm	O
parameters	O
step	O
size	O
trace	O
decay	O
rate	O
initialize	O
w	O
rd	O
w	O
z	O
rd	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
choose	O
a	O
or	O
according	O
to	O
qs	O
w	O
z	O
loop	O
for	O
each	O
step	O
of	O
episode	O
take	O
action	B
a	O
observe	O
r	O
r	O
loop	O
for	O
i	O
in	O
fs	O
a	O
wi	O
zi	O
zi	O
or	O
zi	O
w	O
w	O
z	O
go	O
to	O
next	O
episode	O
if	O
is	O
terminal	O
then	O
choose	O
or	O
near	O
greedily	O
w	O
loop	O
for	O
i	O
in	O
wi	O
w	O
w	O
z	O
z	O
z	O
s	O
a	O
traces	O
traces	O
exercise	O
modify	O
the	O
pseudocode	O
for	B
sarsa	B
to	O
use	O
dutch	B
traces	O
without	O
the	O
other	O
features	O
of	O
a	O
true	B
online	B
algorithm	O
assume	O
linear	O
function	B
approximation	I
and	O
binary	B
features	I
example	O
sarsa	B
on	O
mountain	O
car	O
figure	O
shows	O
results	O
with	O
sarsa	B
on	O
the	O
mountain	O
car	O
task	O
introduced	O
in	O
example	O
the	O
function	B
approximation	I
action	B
selection	O
and	O
environmental	O
details	O
were	O
exactly	O
as	O
in	O
chapter	O
and	O
thus	O
it	O
is	O
appropriate	O
to	O
numerically	O
compare	O
these	O
results	O
with	O
the	O
chapter	O
results	O
for	B
n-step	B
sarsa	B
side	O
of	O
the	O
figure	O
the	O
earlier	O
results	O
varied	O
the	O
update	O
length	O
n	O
whereas	O
here	O
for	B
sarsa	B
we	O
vary	O
the	O
trace	O
parameter	O
which	O
plays	O
a	O
similar	O
role	O
the	O
fading-trace	O
bootstrapping	B
strategy	O
of	O
sarsa	B
appears	O
to	O
result	O
in	O
more	O
efficient	O
learning	O
on	O
this	O
problem	O
there	O
is	O
also	O
an	O
action-value	O
version	O
of	O
our	O
ideal	O
td	B
method	O
the	O
online	B
algorithm	O
presented	O
in	O
section	O
everything	O
in	O
that	O
section	O
goes	O
through	O
without	O
change	O
other	O
than	O
to	O
use	O
the	O
action-value	O
form	O
of	O
the	O
n-step	B
return	B
given	O
at	O
the	O
beginning	O
of	O
this	O
section	O
in	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
the	O
ideal	O
algorithm	O
again	O
has	O
an	O
exact	O
efficient	O
od	O
implementation	O
called	O
true	B
online	B
sarsa	B
the	O
analyses	O
chapter	O
eligibility	B
traces	I
figure	O
early	O
performance	O
on	O
the	O
mountain	O
car	O
task	O
of	O
sarsa	B
with	O
replacing	B
traces	O
and	O
n-step	B
sarsa	B
from	O
figure	O
as	O
a	O
function	O
of	O
the	O
step	O
size	O
in	O
sections	O
and	O
carry	O
through	O
without	O
change	O
other	O
than	O
to	O
use	O
state	B
action	B
feature	O
vectors	O
xt	O
xst	O
at	O
instead	O
of	O
state	B
feature	O
vectors	O
xt	O
xst	O
the	O
pseudocode	O
for	O
this	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
figure	O
compares	O
the	O
performance	O
of	O
various	O
versions	O
of	O
sarsa	B
on	O
the	O
mountain	B
car	I
example	I
figure	O
summary	O
comparison	O
of	O
sarsa	B
algorithms	O
on	O
the	O
mountain	O
car	O
task	O
true	B
online	B
sarsa	B
performed	O
better	O
than	O
regular	O
sarsa	B
with	O
both	O
accumulating	B
and	O
replacing	B
traces	O
also	O
included	O
is	O
a	O
version	O
of	O
sarsa	B
with	O
replacing	B
traces	O
in	O
which	O
on	O
each	O
time	O
step	O
the	O
traces	O
for	O
the	O
state	B
and	O
the	O
actions	O
not	O
selected	O
were	O
set	O
to	O
zero	O
carsteps	O
per	O
episodeaveraged	O
overfirst	O
episodesand	O
number	O
of	O
tilings	O
number	O
of	O
tilings	O
sarsa	B
with	O
replacing	B
tracesn-step	O
sarsa	B
sizerms	O
error	O
td	B
accumulating	B
traces	O
task	O
sizerms	O
error	O
td	B
replacing	B
traces	O
task	O
sizerms	O
error	O
true	B
online	B
td	B
task	O
sizerms	O
error	O
td	B
accumulating	B
traces	O
task	O
sizerms	O
error	O
td	B
replacing	B
traces	O
task	O
sizerms	O
error	O
true	B
online	B
td	B
task	O
matchinganintuitivelyclearforwardviewevenintheonlinecase	O
sarsa	B
replacing	B
clearingsarsa	O
replacing	B
no	O
clearingsarsa	O
accumulatingtrue	O
online	B
sarsa	B
technologyfuturesandthenationalscienceandengineeringresearchcouncilofcanada	O
mountain	O
carreward	O
per	O
episodeaveraged	O
overfirst	O
episodesand	O
runs	O
number	O
of	O
tilings	O
true	B
online	B
sarsa	B
with	O
replacing	B
tracessarsa	O
with	O
replacing	B
tracesand	O
clearing	O
the	O
traces	O
of	O
other	O
actionssarsa	O
with	O
accumulating	B
traces	O
variable	O
and	O
true	B
online	B
sarsa	B
for	O
estimating	O
q	O
or	O
q	O
input	O
a	O
feature	O
function	O
x	O
s	O
a	O
rd	O
such	O
that	O
xterminal	O
input	O
a	O
policy	B
estimating	O
q	O
algorithm	O
parameters	O
step	O
size	O
trace	O
decay	O
rate	O
initialize	O
w	O
rd	O
w	O
loop	O
for	O
each	O
episode	O
initialize	O
s	O
choose	O
a	O
or	O
near	O
greedily	O
from	O
s	O
using	O
w	O
x	O
xs	O
a	O
z	O
qold	O
loop	O
for	O
each	O
step	O
of	O
episode	O
take	O
action	B
a	O
observe	O
r	O
choose	O
or	O
near	O
greedily	O
from	O
using	O
w	O
q	O
r	O
q	O
z	O
z	O
x	O
w	O
w	O
q	O
qoldz	O
qoldx	O
qold	O
x	O
a	O
until	O
is	O
terminal	O
variable	O
and	O
we	O
are	O
starting	O
now	O
to	O
reach	O
the	O
end	O
of	O
our	O
development	O
of	O
fundamental	O
td	B
learning	O
algorithms	O
to	O
present	O
the	O
final	O
algorithms	O
in	O
their	O
most	O
general	O
forms	O
it	O
is	O
useful	O
to	O
generalize	O
the	O
degree	O
of	O
bootstrapping	B
and	O
discounting	B
beyond	O
constant	O
parameters	O
to	O
functions	O
potentially	O
dependent	O
on	O
the	O
state	B
and	O
action	B
that	O
is	O
each	O
time	O
step	O
will	O
have	O
a	O
different	O
and	O
denoted	O
t	O
and	O
t	O
we	O
change	O
notation	O
now	O
so	O
that	O
s	O
a	O
is	O
now	O
a	O
whole	O
function	O
from	O
states	O
and	O
actions	O
to	O
the	O
unit	O
interval	O
at	O
and	O
similarly	O
s	O
is	O
a	O
function	O
from	O
states	O
to	O
the	O
such	O
that	O
t	O
unit	O
interval	O
such	O
that	O
t	O
introducing	O
the	O
function	O
the	O
termination	O
function	O
is	O
particularly	O
significant	O
because	O
it	O
changes	O
the	O
return	B
the	O
fundamental	O
random	O
variable	O
whose	O
expectation	O
we	O
seek	O
to	O
chapter	O
eligibility	B
traces	I
estimate	O
now	O
the	O
return	B
is	O
defined	O
more	O
generally	O
as	O
gt	O
i	O
where	O
to	O
assure	O
the	O
sums	O
are	O
finite	O
we	O
require	O
kt	O
k	O
with	O
probability	O
one	O
for	O
all	O
t	O
one	O
convenient	O
aspect	O
of	O
this	O
definition	O
is	O
that	O
it	O
allows	O
us	O
to	O
dispense	O
with	O
episodes	B
start	O
and	O
terminal	O
states	O
and	O
t	O
as	O
special	O
cases	O
and	O
quantities	O
a	O
terminal	O
state	B
just	O
becomes	O
a	O
state	B
at	O
which	O
and	O
which	O
transitions	O
to	O
the	O
start	O
state	B
in	O
that	O
way	O
by	O
choosing	O
as	O
a	O
constant	O
function	O
we	O
can	O
recover	O
the	O
classical	O
episodic	O
setting	O
as	O
a	O
special	O
case	O
state	B
dependent	I
termination	O
includes	O
other	O
prediction	B
cases	O
such	O
as	O
pseudo	B
termination	I
in	O
which	O
we	O
seek	O
to	O
predict	O
a	O
quantity	O
that	O
becomes	O
complete	O
but	O
does	O
not	O
alter	O
the	O
flow	O
of	O
the	O
markov	O
process	O
discounted	O
returns	O
themselves	O
can	O
be	O
thought	O
of	O
as	O
such	O
a	O
quantity	O
and	O
state	B
dependent	I
termination	O
is	O
a	O
deep	O
unification	O
of	O
the	O
episodic	O
and	O
discounted-continuing	O
cases	O
undiscounted-continuing	O
case	O
still	O
needs	O
some	O
special	O
treatment	O
the	O
generalization	O
to	O
variable	O
bootstrapping	B
is	O
not	O
a	O
change	O
in	O
the	O
problem	O
like	O
discounting	B
but	O
a	O
change	O
in	O
the	O
solution	O
strategy	O
the	O
generalization	O
affects	O
the	O
returns	O
for	O
states	O
and	O
actions	O
the	O
new	O
state-based	O
can	O
be	O
written	O
recursively	O
as	O
g	O
s	O
t	O
s	O
where	O
now	O
we	O
have	O
added	O
the	O
s	O
to	O
the	O
superscript	O
to	O
remind	O
us	O
that	O
this	O
is	O
a	O
return	B
that	O
bootstraps	O
from	O
state	B
values	O
distinguishing	O
it	O
from	O
returns	O
that	O
bootstrap	O
from	O
action	B
values	O
which	O
we	O
present	O
below	O
with	O
a	O
in	O
the	O
superscript	O
this	O
equation	O
says	O
that	O
the	O
is	O
the	O
first	O
reward	O
undiscounted	O
and	O
unaffected	O
by	O
bootstrapping	B
plus	O
possibly	O
a	O
second	O
term	O
to	O
the	O
extent	O
that	O
we	O
are	O
not	O
discounting	B
at	O
the	O
next	O
state	B
is	O
according	O
to	O
recall	O
that	O
this	O
is	O
zero	O
if	O
the	O
next	O
state	B
is	O
terminal	O
to	O
the	O
extent	O
that	O
we	O
aren	O
t	O
terminating	O
at	O
the	O
next	O
state	B
we	O
have	O
a	O
second	O
term	O
which	O
is	O
itself	O
divided	O
into	O
two	O
cases	O
depending	O
on	O
the	O
degree	O
of	O
bootstrapping	B
in	O
the	O
state	B
to	O
the	O
extent	O
we	O
are	O
bootstrapping	B
this	O
term	O
is	O
the	O
estimated	O
value	B
at	O
the	O
state	B
whereas	O
to	O
the	O
extent	O
that	O
we	O
not	O
bootstrapping	B
the	O
term	O
is	O
the	O
for	O
the	O
next	O
time	O
step	O
the	O
action-based	O
is	O
either	O
the	O
sarsa	B
form	O
or	O
the	O
expected	B
sarsa	B
form	O
g	O
a	O
t	O
g	O
a	O
t	O
vts	O
wt	O
a	O
a	O
qs	O
a	O
wt	O
where	O
is	O
generalized	O
to	O
function	B
approximation	I
as	O
off-policy	B
eligibility	B
traces	I
with	B
control	B
variates	I
exercise	O
generalize	O
the	O
three	O
recursive	O
equations	O
above	O
to	O
their	O
truncated	B
versions	O
defining	O
g	O
s	O
th	O
and	O
g	O
a	O
th	O
off-policy	B
eligibility	B
traces	I
with	O
control	B
vari	O
ates	O
the	O
final	O
step	O
is	O
to	O
incorporate	O
importance	B
sampling	I
unlike	O
in	O
the	O
case	O
of	O
n-step	B
methods	I
for	O
full	O
non-truncated	O
one	O
does	O
not	O
have	O
a	O
practical	O
option	O
in	O
which	O
the	O
importance	B
sampling	I
is	O
done	O
outside	O
the	O
target	O
return	B
instead	O
we	O
move	O
directly	O
to	O
the	O
bootstrapping	B
generalization	O
of	O
per-decision	B
importance	B
sampling	I
with	B
control	B
variates	I
in	O
the	O
state	B
case	O
our	O
final	O
definition	O
of	O
the	O
generalizes	O
after	O
the	O
model	O
of	O
to	O
g	O
s	O
t	O
s	O
t	O
vstwt	O
where	O
t	O
batst	O
is	O
the	O
usual	O
single-step	O
importance	B
sampling	I
ratio	B
much	O
like	O
the	O
other	O
returns	O
we	O
have	O
seen	O
in	O
this	O
book	O
the	O
truncated	B
version	O
of	O
this	O
return	B
can	O
be	O
approximated	O
simply	O
in	O
terms	O
of	O
sums	O
of	O
the	O
state-based	O
td	B
error	I
s	O
t	O
vstwt	O
as	O
g	O
s	O
t	O
vstwt	O
t	O
s	O
k	O
i	O
i	O
i	O
with	O
the	O
approximation	O
becoming	O
exact	O
if	O
the	O
approximate	O
value	B
function	I
does	O
not	O
change	O
exercise	O
prove	O
that	O
becomes	O
exact	O
if	O
the	O
value	B
function	I
does	O
not	O
change	O
to	O
save	O
writing	O
consider	O
the	O
case	O
of	O
t	O
and	O
use	O
the	O
notation	O
vk	O
exercise	O
the	O
truncated	B
version	O
of	O
the	O
general	O
off-policy	B
return	B
is	O
denoted	O
g	O
s	O
th	O
guess	O
the	O
correct	O
equation	O
based	O
on	O
the	O
above	O
form	O
of	O
the	O
is	O
convenient	O
to	O
use	O
in	O
a	O
forward-view	O
update	O
vskw	O
wt	O
s	O
wt	O
s	O
k	O
t	O
vstwt	O
i	O
i	O
vstwt	O
which	O
to	O
the	O
experienced	O
eye	O
looks	O
like	O
an	O
eligibility-based	O
td	B
update	O
the	O
product	O
is	O
like	O
an	O
eligibility	O
trace	O
and	O
it	O
is	O
multiplied	O
by	O
td	B
errors	O
but	O
this	O
is	O
just	O
one	O
time	O
step	O
of	O
a	O
forward	O
view	O
the	O
relationship	O
that	O
we	O
are	O
looking	O
for	O
is	O
that	O
the	O
forward-view	O
update	O
summed	O
over	O
time	O
is	O
approximately	O
equal	O
to	O
a	O
backward-view	O
update	O
summed	O
chapter	O
eligibility	B
traces	I
over	O
time	O
relationship	O
is	O
only	O
approximate	O
because	O
again	O
we	O
ignore	O
changes	O
in	O
the	O
value	B
function	I
the	O
sum	O
of	O
the	O
forward-view	O
update	O
over	O
time	O
is	O
wt	O
s	O
k	O
t	O
s	O
k	O
vstwt	O
i	O
i	O
i	O
k	O
i	O
i	O
i	O
t	O
vstwt	O
s	O
the	O
summation	O
rule	O
t	O
vstwt	O
i	O
i	O
i	O
kt	O
tx	O
which	O
would	O
be	O
in	O
the	O
form	O
of	O
the	O
sum	O
of	O
a	O
backward-view	O
td	B
update	O
if	O
the	O
entire	O
expression	O
from	O
the	O
second	O
sum	O
left	O
could	O
be	O
written	O
and	O
updated	O
incrementally	O
as	O
an	O
eligibility	O
trace	O
which	O
we	O
now	O
show	O
can	O
be	O
done	O
that	O
is	O
we	O
show	O
that	O
if	O
this	O
expression	O
was	O
the	O
trace	O
at	O
time	O
k	O
then	O
we	O
could	O
update	O
it	O
from	O
its	O
value	B
at	O
time	O
k	O
by	O
zk	O
k	O
t	O
vstwt	O
t	O
vstwt	O
i	O
i	O
i	O
i	O
i	O
i	O
k	O
vskwk	O
k	O
k	O
k	O
t	O
vstwt	O
k	O
k	O
k	O
kzk	O
zk	O
i	O
i	O
i	O
k	O
vskwk	O
which	O
changing	O
the	O
index	O
from	O
k	O
to	O
t	O
is	O
the	O
general	O
accumulating	B
trace	O
update	O
for	O
state	B
values	O
zt	O
t	O
tzt	O
this	O
eligibility	O
trace	O
together	O
with	O
the	O
usual	O
semi-gradient	O
parameter-update	O
rule	O
for	O
td	B
forms	O
a	O
general	O
td	B
algorithm	O
that	O
can	O
be	O
applied	O
to	O
either	O
on-policy	O
or	O
off-policy	B
data	O
in	O
the	O
on-policy	O
case	O
the	O
algorithm	O
is	O
exactly	O
td	B
because	O
t	O
is	O
alway	O
and	O
becomes	O
the	O
usual	O
accumulating	B
trace	O
to	O
variable	O
and	O
in	O
the	O
off-policy	B
case	O
the	O
algorithm	O
often	O
works	O
well	O
but	O
as	O
a	O
semi-gradient	O
method	O
is	O
not	O
guaranteed	O
to	O
be	O
stable	O
in	O
the	O
next	O
few	O
sections	O
we	O
will	O
consider	O
extensions	O
of	O
it	O
that	O
do	O
guarantee	O
stability	O
a	O
very	O
similar	O
series	O
of	O
steps	O
can	O
be	O
followed	O
to	O
derive	O
the	O
off-policy	B
eligibility	B
traces	I
for	O
action-value	B
methods	I
and	O
corresponding	O
general	O
sarsa	B
algorithms	O
one	O
could	O
start	O
with	O
either	O
recursive	O
form	O
for	O
the	O
general	O
action-based	O
or	O
off-policy	B
eligibility	B
traces	I
with	B
control	B
variates	I
but	O
the	O
latter	O
expected	B
sarsa	B
form	O
works	O
out	O
to	O
be	O
simpler	O
we	O
extend	O
to	O
the	O
off-policy	B
case	O
after	O
the	O
model	O
of	O
to	O
produce	O
g	O
a	O
t	O
a	O
a	O
where	O
is	O
as	O
given	O
by	O
again	O
the	O
can	O
be	O
written	O
approximately	O
as	O
the	O
sum	O
of	O
td	B
errors	O
g	O
a	O
t	O
qst	O
at	O
wt	O
a	O
k	O
i	O
i	O
i	O
using	O
the	O
expectation	O
form	O
of	O
the	O
action-based	O
td	B
error	I
a	O
t	O
qst	O
at	O
wt	O
as	O
before	O
the	O
approximation	O
becomes	O
exact	O
if	O
the	O
approximate	O
value	B
function	I
does	O
not	O
change	O
exercise	O
prove	O
that	O
becomes	O
exact	O
if	O
the	O
value	B
function	I
does	O
not	O
change	O
to	O
save	O
writing	O
consider	O
the	O
case	O
of	O
t	O
and	O
use	O
the	O
notation	O
qk	O
qsk	O
ak	O
w	O
hint	O
start	O
by	O
writing	O
out	O
a	O
exercise	O
the	O
truncated	B
version	O
of	O
the	O
general	O
off-policy	B
return	B
is	O
denoted	O
g	O
a	O
th	O
guess	O
the	O
correct	O
equation	O
for	O
it	O
based	O
on	O
using	O
steps	O
entirely	O
analogous	O
to	O
those	O
for	O
the	O
state	B
case	O
one	O
can	O
write	O
a	O
forward-view	O
update	O
based	O
on	O
transform	O
the	O
sum	O
of	O
the	O
updates	O
using	O
the	O
summation	O
rule	O
and	O
finally	O
derive	O
the	O
following	O
form	O
for	O
the	O
eligibility	O
trace	O
for	B
action	B
values	I
then	O
g	O
a	O
and	O
g	O
a	O
zt	O
t	O
t	O
tzt	O
qst	O
at	O
wt	O
this	O
eligibility	O
trace	O
together	O
with	O
the	O
expectation-based	O
td	B
error	I
and	O
the	O
usual	O
semi-gradient	O
parameter-update	O
rule	O
forms	O
an	O
elegant	O
efficient	O
expected	B
sarsa	B
algorithm	O
that	O
can	O
be	O
applied	O
to	O
either	O
on-policy	O
or	O
off-policy	B
data	O
it	O
is	O
probably	O
the	O
best	O
algorithm	O
of	O
this	O
type	O
at	O
the	O
current	O
time	O
of	O
course	O
it	O
is	O
not	O
guaranteed	O
to	O
be	O
stable	O
until	O
combined	O
in	O
some	O
way	O
with	O
one	O
of	O
the	O
methods	O
presented	O
in	O
the	O
following	O
sections	O
in	O
the	O
on-policy	O
case	O
with	O
constant	O
and	O
and	O
the	O
usual	O
state-action	O
td	B
error	I
the	O
algorithm	O
would	O
be	O
identical	O
to	O
the	O
sarsa	B
algorithm	O
presented	O
in	O
section	O
exercise	O
show	O
in	O
detail	O
the	O
steps	O
outlined	O
above	O
for	O
deriving	O
from	O
start	O
with	O
the	O
update	O
substitute	O
g	O
a	O
t	O
then	O
follow	O
similar	O
steps	O
as	O
led	O
to	O
at	O
these	O
algorithms	O
become	O
closely	O
related	O
to	O
corresponding	O
monte	B
carlo	I
algorithms	O
one	O
might	O
expect	O
that	O
an	O
exact	O
equivalence	O
would	O
hold	O
for	O
episodic	O
problems	O
and	O
off-line	B
updating	O
but	O
in	O
fact	O
the	O
relationship	O
is	O
subtler	O
and	O
slightly	O
weaker	O
than	O
that	O
under	O
these	O
most	O
favorable	O
conditions	O
still	O
there	O
is	O
not	O
an	O
episode	O
by	O
episode	O
equivalence	O
from	O
for	O
g	O
t	O
chapter	O
eligibility	B
traces	I
of	O
updates	O
only	O
of	O
their	O
expectations	O
this	O
should	O
not	O
be	O
surprising	O
as	O
these	O
method	O
make	O
irrevocable	O
updates	O
as	O
a	O
trajectory	O
unfolds	O
whereas	O
true	O
monte	B
carlo	I
methods	I
would	O
make	O
no	O
update	O
for	O
a	O
trajectory	O
if	O
any	O
action	B
within	O
it	O
has	O
zero	O
probability	O
under	O
the	O
target	O
policy	B
in	O
particular	O
all	O
of	O
these	O
methods	O
even	O
at	O
still	O
bootstrap	O
in	O
the	O
sense	O
that	O
their	O
targets	O
depend	O
on	O
the	O
current	O
value	B
estimates	O
it	O
s	O
just	O
that	O
the	O
dependence	O
cancels	O
out	O
in	O
expected	B
value	B
whether	O
this	O
is	O
a	O
good	O
or	O
bad	O
property	O
in	O
practice	O
is	O
another	O
question	O
recently	O
methods	O
have	O
been	O
proposed	O
that	O
do	O
achieve	O
an	O
exact	O
equivalence	O
mahmood	O
precup	O
and	O
van	O
hasselt	O
these	O
methods	O
require	O
an	O
additional	O
vector	B
of	O
provisional	O
weights	O
that	O
keep	O
track	O
of	O
updates	O
which	O
have	O
been	O
made	O
but	O
may	O
need	O
to	O
be	O
retracted	O
emphasized	O
depending	O
on	O
the	O
actions	O
taken	O
later	O
the	O
state	B
and	O
state	B
action	B
versions	O
of	O
these	O
methods	O
are	O
called	O
ptd	O
and	O
pq	O
respectively	O
where	O
the	O
p	O
stands	O
for	O
provisional	O
the	O
practical	O
consequences	O
of	O
all	O
these	O
new	O
off-policy	B
methods	I
have	O
not	O
yet	O
been	O
established	O
undoubtedly	O
issues	O
of	O
high	O
variance	O
will	O
arise	O
as	O
they	O
do	O
in	O
all	O
off-policy	B
methods	I
using	O
importance	B
sampling	I
if	O
then	O
all	O
these	O
off-policy	B
algorithms	O
involve	O
bootstrapping	B
and	B
the	I
deadly	B
triad	I
applies	O
meaning	O
that	O
they	O
can	O
be	O
guaranteed	O
stable	O
only	O
for	O
the	O
tabular	O
case	O
for	O
state	B
aggregation	I
and	O
for	O
other	O
limited	O
forms	O
of	O
function	B
approximation	I
for	O
linear	O
and	O
more-general	O
forms	O
of	O
function	B
approximation	I
the	O
parameter	O
vector	B
may	O
diverge	O
to	O
infinity	O
as	O
in	O
the	O
examples	O
in	O
chapter	O
as	O
we	O
discussed	O
there	O
the	O
challenge	O
of	O
off-policy	B
learning	O
has	O
two	O
parts	O
off-policy	B
eligibility	B
traces	I
deal	O
effectively	O
with	O
the	O
first	O
part	O
of	O
the	O
challenge	O
correcting	O
for	O
the	O
expected	B
value	B
of	O
the	O
targets	O
but	O
not	O
at	O
all	O
with	O
the	O
second	O
part	O
of	O
the	O
challenge	O
having	O
to	O
do	O
with	O
the	O
distribution	O
of	O
updates	O
algorithmic	O
strategies	O
for	O
meeting	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
off-policy	B
learning	O
with	B
eligibility	B
traces	I
are	O
summarized	O
in	O
section	O
exercise	O
what	O
are	O
the	O
dutch-trace	O
and	O
replacing-trace	O
versions	O
of	O
off-policy	B
bility	O
traces	O
for	O
state-value	O
and	O
action-value	B
methods	I
watkins	B
s	O
q	O
to	O
tree-backup	O
several	O
methods	O
have	O
been	O
proposed	O
over	O
the	O
years	O
to	O
extend	O
q-learning	B
to	O
eligibility	B
traces	I
the	O
original	O
is	O
watkins	B
s	O
q	O
which	O
decays	O
its	O
eligibility	B
traces	I
in	O
the	O
usual	O
way	O
as	O
long	O
as	O
a	O
greedy	O
action	B
was	O
taken	O
then	O
cuts	O
the	O
traces	O
to	O
zero	O
after	O
the	O
first	O
non-greedy	O
action	B
the	O
backup	B
diagram	I
for	O
watkins	B
s	O
q	O
is	O
shown	O
in	O
figure	O
in	O
chapter	O
we	O
unified	O
q-learning	B
and	O
expected	B
sarsa	B
in	O
the	O
off-policy	B
version	O
of	O
the	O
latter	O
which	O
includes	O
q-learning	B
as	O
a	O
special	O
case	O
and	O
generalizes	O
it	O
to	O
arbitrary	O
target	O
policies	O
and	O
in	O
the	O
previous	O
section	O
of	O
this	O
chapter	O
we	O
completed	O
our	O
treatment	O
of	O
expected	B
sarsa	B
by	O
generalizing	O
it	O
to	O
off-policy	B
eligibility	B
traces	I
in	O
chapter	O
however	O
we	O
distinguished	O
n-step	B
expected	B
sarsa	B
from	O
n-step	B
tree	B
backup	I
where	O
the	O
latter	O
retained	O
the	O
property	O
of	O
not	O
using	O
importance	B
sampling	I
it	O
remains	O
then	O
to	O
present	O
the	O
eligibility	O
trace	O
version	O
of	O
tree	B
backup	I
which	O
we	O
well	O
call	O
tree-backup	O
or	O
tb	O
for	O
short	O
this	O
is	O
arguably	O
the	O
true	O
successor	O
to	O
q-learning	B
because	O
it	O
retains	O
its	O
appealing	O
absence	O
of	O
importance	B
sampling	I
even	O
though	O
it	O
can	O
be	O
applied	O
to	O
off-policy	B
data	O
watkins	B
s	O
q	O
to	O
tree-backup	O
figure	O
the	O
backup	B
diagram	I
for	O
watkins	B
s	O
q	O
the	O
series	O
of	O
component	O
updates	O
ends	O
either	O
with	O
the	O
end	O
of	O
the	O
episode	O
or	O
with	O
the	O
first	O
nongreedy	O
action	B
whichever	O
comes	O
first	O
the	O
concept	O
of	O
tb	O
is	O
straightforward	O
as	O
shown	O
in	O
its	O
backup	B
diagram	I
in	O
figure	O
the	O
tree-backup	O
updates	O
of	O
each	O
length	O
section	O
are	O
weighted	O
in	O
the	O
usual	O
way	O
dependent	O
on	O
the	O
bootstrapping	B
parameter	O
to	O
get	O
the	O
detailed	O
equations	O
with	O
the	O
right	O
indices	O
on	O
the	O
general	O
bootstrapping	B
and	O
discounting	B
parameters	O
it	O
is	O
best	O
to	O
start	O
with	O
a	O
recursive	O
form	O
for	O
the	O
using	O
action	B
values	O
and	O
then	O
expand	O
the	O
bootstrapping	B
case	O
of	O
the	O
target	O
after	O
the	O
model	O
of	O
as	O
per	O
the	O
usual	O
pattern	O
it	O
can	O
also	O
be	O
written	O
approximately	O
changes	O
in	O
the	O
approximate	O
value	B
function	I
as	O
a	O
sum	O
of	O
td	B
errors	O
g	O
a	O
t	O
qst	O
at	O
wt	O
a	O
k	O
i	O
i	O
using	O
the	O
expectation	O
form	O
of	O
the	O
action-based	O
td	B
error	I
following	O
the	O
same	O
steps	O
as	O
in	O
the	O
previous	O
section	O
we	O
arrive	O
at	O
a	O
special	O
eligibility	O
trace	O
update	O
involving	O
the	O
target-policy	O
probabilities	O
of	O
the	O
selected	O
actions	O
zt	O
t	O
t	O
qst	O
at	O
wt	O
g	O
a	O
t	O
a	O
a	O
wt	O
a	O
sq	O
chapter	O
eligibility	B
traces	I
figure	O
the	O
backup	B
diagram	I
for	O
the	O
version	O
of	O
the	O
tree	B
backup	I
algorithm	O
this	O
together	O
with	O
the	O
usual	O
parameter-update	O
rule	O
defines	O
the	O
tb	O
algorithm	O
like	O
all	O
semi-gradient	O
algorithms	O
tb	O
is	O
not	O
guaranteed	O
to	O
be	O
stable	O
when	O
used	O
with	O
off-policy	B
data	O
and	O
with	O
a	O
powerful	O
function	O
approximator	O
to	O
obtain	O
those	O
assurances	O
tb	O
would	O
have	O
to	O
be	O
combined	O
with	O
one	O
of	O
the	O
methods	O
presented	O
in	O
the	O
next	O
section	O
exercise	O
how	O
might	O
double	B
expected	B
sarsa	B
be	O
extended	O
to	O
eligibility	B
traces	I
stable	O
off-policy	B
methods	I
with	O
traces	O
several	O
methods	O
using	O
eligibility	B
traces	I
have	O
been	O
proposed	O
that	O
achieve	O
guarantees	O
of	O
stability	O
under	O
off-policy	B
training	O
and	O
here	O
we	O
present	O
four	O
of	O
the	O
most	O
important	O
using	O
this	O
book	O
s	O
standard	O
notation	O
including	O
general	O
bootstrapping	B
and	O
discounting	B
functions	O
all	O
are	O
based	O
on	O
either	O
the	O
gradient-td	O
or	O
the	O
emphatic-td	O
ideas	O
presented	O
in	O
sections	O
and	O
all	O
the	O
algorithms	O
assume	O
linear	O
function	B
approximation	I
though	O
extensions	O
to	O
nonlinear	O
function	B
approximation	I
can	O
also	O
be	O
found	O
in	O
the	O
literature	O
gtd	O
is	O
the	O
eligibility-trace	O
algorithm	O
analogous	O
to	O
tdc	O
the	O
better	O
of	O
the	O
two	O
state-value	O
gradient-td	O
prediction	B
algorithms	O
discussed	O
in	O
section	O
its	O
goal	O
is	O
to	O
xs	O
v	O
even	O
from	O
data	O
that	O
is	O
due	O
learn	O
a	O
parameter	O
wt	O
such	O
that	O
vsw	O
to	O
following	O
another	O
policy	B
b	O
its	O
update	O
is	O
wt	O
s	O
t	O
zt	O
stable	O
off-policy	B
methods	I
with	O
traces	O
with	O
s	O
t	O
zt	O
and	O
t	O
defined	O
in	O
the	O
usual	O
ways	O
for	O
state	B
values	O
and	O
vt	O
s	O
t	O
zt	O
xt	O
where	O
as	O
in	O
section	O
v	O
rd	O
is	O
a	O
vector	B
of	O
the	O
same	O
dimension	O
as	O
w	O
initialized	O
to	O
and	O
is	O
a	O
second	O
step-size	B
parameter	I
gq	O
is	O
the	O
gradient-td	O
algorithm	O
for	B
action	B
values	I
with	B
eligibility	B
traces	I
its	O
goal	O
xs	O
a	O
q	O
a	O
from	O
off-policy	B
is	O
to	O
learn	O
a	O
parameter	O
wt	O
such	O
that	O
qs	O
a	O
wt	O
data	O
if	O
the	O
target	O
policy	B
is	O
or	O
otherwise	O
biased	O
toward	O
the	O
greedy	O
policy	B
for	O
q	O
then	O
gq	O
can	O
be	O
used	O
as	O
a	O
control	B
algorithm	O
its	O
update	O
is	O
wt	O
a	O
t	O
zt	O
where	O
xt	O
is	O
the	O
average	O
feature	O
vector	B
for	O
st	O
under	O
the	O
target	O
policy	B
xt	O
a	O
a	O
t	O
is	O
the	O
expectation	O
form	O
of	O
the	O
td	B
error	I
which	O
can	O
be	O
written	O
a	O
t	O
xt	O
zt	O
is	O
defined	O
in	O
the	O
usual	O
way	O
for	B
action	B
values	I
and	O
the	O
rest	O
is	O
as	O
in	O
gtd	O
including	O
the	O
update	O
for	O
vt	O
htd	O
is	O
a	O
hybrid	O
state-value	O
algorithm	O
combining	O
aspects	O
of	O
gtd	O
and	O
td	B
its	O
most	O
appealing	O
feature	O
is	O
that	O
it	O
is	O
a	O
strict	O
generalization	O
of	O
td	B
to	O
off-policy	B
learning	O
meaning	O
that	O
if	O
the	O
behavior	O
policy	B
happens	O
to	O
be	O
the	O
same	O
as	O
the	O
target	O
policy	B
then	O
htd	O
becomes	O
the	O
same	O
as	O
td	B
which	O
is	O
not	O
true	O
for	O
gtd	O
this	O
is	O
appealing	O
because	O
td	B
is	O
often	O
faster	O
than	O
gtd	O
when	O
both	O
algorithms	O
converge	O
and	O
td	B
requires	O
setting	O
only	O
a	O
single	O
step	O
size	O
htd	O
is	O
defined	O
by	O
zt	O
zb	O
t	O
wt	O
s	O
vt	O
s	O
t	O
zt	O
zb	O
t	O
zt	O
t	O
tzt	O
t	O
tzb	O
with	O
zb	O
t	O
xt	O
t	O
with	O
z	O
with	O
in	O
addition	O
to	O
the	O
second	O
set	O
of	O
where	O
again	O
is	O
a	O
second	O
step-size	B
parameter	I
weights	O
vt	O
htd	O
also	O
has	O
a	O
second	O
set	O
of	O
eligibility	B
traces	I
zb	O
t	O
these	O
are	O
conventional	O
accumulating	B
eligibility	B
traces	I
for	O
the	O
behavior	O
policy	B
and	O
become	O
equal	O
to	O
zt	O
if	O
all	O
the	O
t	O
are	O
which	O
causes	O
the	O
last	O
term	O
in	O
the	O
wt	O
update	O
to	O
be	O
zero	O
and	O
the	O
overall	O
update	O
to	O
reduce	O
to	O
td	B
emphatic	O
td	B
is	O
the	O
extension	O
of	O
the	O
one-step	O
emphatic-td	O
algorithm	O
and	O
to	O
eligibility	B
traces	I
the	O
resultant	O
algorithm	O
retains	O
strong	O
off-policy	B
convergence	O
guarantees	O
while	O
enabling	O
any	O
degree	O
of	O
bootstrapping	B
albeit	O
at	O
the	O
cost	O
of	O
chapter	O
eligibility	B
traces	I
high	O
variance	O
and	O
potentially	O
slow	O
convergence	O
emphatic	O
td	B
is	O
defined	O
by	O
t	O
zt	O
mt	O
ft	O
wt	O
tzt	O
xt	O
t	O
tzt	O
t	O
it	O
tft	O
t	O
tft	O
it	O
with	O
with	O
z	O
where	O
mt	O
is	O
the	O
general	O
form	O
of	O
emphasis	O
ft	O
is	O
termed	O
the	O
followon	O
trace	O
and	O
it	O
is	O
the	O
interest	O
as	O
described	O
in	O
section	O
note	O
that	O
mt	O
like	O
t	O
is	O
not	O
really	O
an	O
additional	O
memory	O
variable	O
it	O
can	O
be	O
removed	O
from	O
the	O
algorithm	O
by	O
substituting	O
its	O
definition	O
into	O
the	O
eligibility-trace	O
equation	O
pseudocode	O
and	O
software	O
for	O
the	O
true	B
online	B
version	O
of	O
emphatic-td	O
are	O
available	O
on	O
the	O
web	O
in	O
the	O
on-policy	O
case	O
t	O
for	O
all	O
t	O
emphatic-td	O
is	O
similar	O
to	O
conventional	O
td	B
but	O
still	O
significantly	O
different	O
in	O
fact	O
whereas	O
emphatic-td	O
is	O
guaranteed	O
to	O
converge	O
for	O
all	O
state-dependent	O
functions	O
td	B
is	O
not	O
td	B
is	O
guaranteed	O
convergent	O
only	O
for	O
all	O
constant	O
see	O
yu	O
s	O
counterexample	O
rafiee	O
and	O
sutton	O
implementation	O
issues	O
it	O
might	O
at	O
first	O
appear	O
that	O
tabular	O
methods	O
using	O
eligibility	B
traces	I
are	O
much	O
more	O
complex	O
than	O
one-step	O
methods	O
a	O
naive	B
implementation	O
would	O
require	O
every	O
state	B
state	B
action	B
pair	O
to	O
update	O
both	O
its	O
value	B
estimate	O
and	O
its	O
eligibility	O
trace	O
on	O
every	O
time	O
step	O
this	O
would	O
not	O
be	O
a	O
problem	O
for	O
implementations	O
on	O
single-instruction	O
multipledata	O
parallel	O
computers	O
or	O
in	O
plausible	O
neural	B
implementations	O
but	O
it	O
is	O
a	O
problem	O
for	O
implementations	O
on	O
conventional	O
serial	O
computers	O
fortunately	O
for	O
typical	O
values	O
of	O
and	O
the	O
eligibility	B
traces	I
of	O
almost	O
all	O
states	O
are	O
almost	O
always	O
nearly	O
zero	O
only	O
those	O
states	O
that	O
have	O
recently	O
been	O
visited	O
will	O
have	O
traces	O
significantly	O
greater	O
than	O
zero	O
and	O
only	O
these	O
few	O
states	O
need	O
to	O
be	O
updated	O
to	O
closely	O
approximate	O
these	O
algorithms	O
in	O
practice	O
then	O
implementations	O
on	O
conventional	O
computers	O
may	O
keep	O
track	O
of	O
and	O
update	O
only	O
the	O
few	O
traces	O
that	O
are	O
significantly	O
greater	O
than	O
zero	O
using	O
this	O
trick	O
the	O
computational	O
expense	O
of	O
using	O
traces	O
in	O
tabular	O
methods	O
is	O
typically	O
just	O
a	O
few	O
times	O
that	O
of	O
a	O
one-step	O
method	O
the	O
exact	O
multiple	O
of	O
course	O
depends	O
on	O
and	O
and	O
on	O
the	O
expense	O
of	O
the	O
other	O
computations	O
note	O
that	O
the	O
tabular	O
case	O
is	O
in	O
some	O
sense	O
the	O
worst	O
case	O
for	O
the	O
computational	O
complexity	O
of	O
eligibility	B
traces	I
when	O
function	B
approximation	I
is	O
used	O
the	O
computational	O
advantages	B
of	I
not	O
using	O
traces	O
generally	O
decrease	O
for	O
example	O
if	O
artificial	B
neural	B
networks	I
and	O
backpropagation	B
are	O
used	O
then	O
eligibility	B
traces	I
generally	O
cause	O
only	O
a	O
doubling	O
of	O
the	O
required	O
memory	O
and	O
computation	O
per	O
step	O
truncated	B
methods	O
can	O
be	O
computationally	O
efficient	O
on	O
conventional	O
computers	O
though	O
they	O
always	O
require	O
some	O
additional	O
memory	O
conclusions	O
conclusions	O
eligibility	B
traces	I
in	O
conjunction	O
with	O
td	B
errors	O
provide	O
an	O
efficient	O
incremental	O
way	O
of	O
shifting	O
and	O
choosing	O
between	O
monte	B
carlo	I
and	O
td	B
methods	O
the	O
atomic	O
n-step	B
methods	I
of	O
chapter	O
also	O
enabled	O
this	O
but	O
eligibility	O
trace	O
methods	O
are	O
more	O
general	O
often	O
faster	O
to	O
learn	O
and	O
offer	O
different	O
computational	O
complexity	O
tradeoffs	O
this	O
chapter	O
has	O
offered	O
an	O
introduction	O
to	O
the	O
elegant	O
emerging	O
theoretical	O
understanding	O
of	O
eligibility	B
traces	I
for	O
on-	O
and	O
off-policy	B
learning	O
and	O
for	O
variable	O
bootstrapping	B
and	O
discounting	B
one	O
aspect	O
of	O
this	O
elegant	O
theory	O
is	O
true	B
online	B
methods	O
which	O
exactly	O
reproduce	O
the	O
behavior	O
of	O
expensive	O
ideal	O
methods	O
while	O
retaining	O
the	O
computational	O
congeniality	O
of	O
conventional	O
td	B
methods	O
another	O
aspect	O
is	O
the	O
possibility	O
of	O
derivations	O
that	O
automatically	O
convert	O
from	O
intuitive	O
forward-view	O
methods	O
to	O
more	O
efficient	O
incremental	O
backward-view	O
algorithms	O
we	O
illustrated	O
this	O
general	O
idea	O
in	O
a	O
derivation	O
that	O
started	O
with	O
a	O
classical	O
expensive	O
monte	B
carlo	I
algorithm	O
and	O
ended	O
with	O
a	O
cheap	O
incremental	O
non-td	O
implementation	O
using	O
the	O
same	O
novel	O
eligibility	O
trace	O
used	O
in	O
true	B
online	B
td	B
methods	O
as	O
we	O
mentioned	O
in	O
chapter	O
monte	B
carlo	I
methods	I
may	O
have	O
advantages	O
in	O
nonmarkov	O
tasks	O
because	O
they	O
do	O
not	O
bootstrap	O
because	O
eligibility	B
traces	I
make	O
td	B
methods	O
more	O
like	O
monte	B
carlo	I
methods	I
they	O
also	O
can	O
have	O
advantages	O
in	O
these	O
cases	O
if	O
one	O
wants	O
to	O
use	O
td	B
methods	O
because	O
of	O
their	O
other	O
advantages	O
but	O
the	O
task	O
is	O
at	O
least	O
partially	O
non-markov	O
then	O
the	O
use	O
of	O
an	O
eligibility	O
trace	O
method	O
is	O
indicated	O
eligibility	B
traces	I
are	O
the	O
first	O
line	O
of	O
defense	O
against	O
both	O
long-delayed	O
rewards	O
and	O
non-markov	O
tasks	O
by	O
adjusting	O
we	O
can	O
place	O
eligibility	O
trace	O
methods	O
anywhere	O
along	O
a	O
continuum	O
from	O
monte	B
carlo	I
to	O
one-step	O
td	B
methods	O
where	O
shall	O
we	O
place	O
them	O
we	O
do	O
not	O
yet	O
have	O
a	O
good	O
theoretical	O
answer	O
to	O
this	O
question	O
but	O
a	O
clear	O
empirical	O
answer	O
appears	O
to	O
be	O
emerging	O
on	O
tasks	O
with	O
many	O
steps	O
per	O
episode	O
or	O
many	O
steps	O
within	O
the	O
half-life	O
of	O
discounting	B
it	O
appears	O
significantly	O
better	O
to	O
use	O
eligibility	B
traces	I
than	O
not	O
to	O
see	O
figure	O
on	O
the	O
other	O
hand	O
if	O
the	O
traces	O
are	O
so	O
long	O
as	O
to	O
produce	O
a	O
pure	O
monte	B
carlo	I
method	O
or	O
nearly	O
so	O
then	O
performance	O
degrades	O
sharply	O
an	O
intermediate	O
mixture	O
appears	O
to	O
be	O
the	O
best	O
choice	O
eligibility	B
traces	I
should	O
be	O
used	O
to	O
bring	O
us	O
toward	O
monte	B
carlo	I
methods	I
but	O
not	O
all	O
the	O
way	O
there	O
in	O
the	O
future	O
it	O
may	O
be	O
possible	O
to	O
more	O
finely	O
vary	O
the	O
trade-off	O
between	O
td	B
and	B
monte	B
carlo	I
methods	I
by	O
using	O
variable	O
but	O
at	O
present	O
it	O
is	O
not	O
clear	O
how	O
this	O
can	O
be	O
done	O
reliably	O
and	O
usefully	O
methods	O
using	O
eligibility	B
traces	I
require	O
more	O
computation	O
than	O
one-step	O
methods	O
but	O
in	O
return	B
they	O
offer	O
significantly	O
faster	O
learning	O
particularly	O
when	O
rewards	O
are	O
delayed	O
by	O
many	O
steps	O
thus	O
it	O
often	O
makes	O
sense	O
to	O
use	O
eligibility	B
traces	I
when	O
data	O
are	O
scarce	O
and	O
cannot	O
be	O
repeatedly	O
processed	O
as	O
is	O
often	O
the	O
case	O
in	O
online	B
applications	O
on	O
the	O
other	O
hand	O
in	O
off-line	B
applications	O
in	O
which	O
data	O
can	O
be	O
generated	O
cheaply	O
perhaps	O
from	O
an	O
inexpensive	O
simulation	O
then	O
it	O
often	O
does	O
not	O
pay	O
to	O
use	O
eligibility	B
traces	I
in	O
these	O
cases	O
the	O
objective	O
is	O
not	O
to	O
get	O
more	O
out	O
of	O
a	O
limited	O
amount	O
of	O
data	O
but	O
simply	O
to	O
process	O
as	O
much	O
data	O
as	O
possible	O
as	O
quickly	O
as	O
possible	O
in	O
these	O
cases	O
the	O
speedup	O
per	O
datum	O
due	O
to	O
traces	O
is	O
typically	O
not	O
worth	O
their	O
computational	O
cost	O
and	O
one-step	O
methods	O
are	O
favored	O
chapter	O
eligibility	B
traces	I
figure	O
the	O
effect	O
of	O
on	O
reinforcement	B
learning	I
performance	O
in	O
four	O
different	O
test	O
problems	O
in	O
all	O
cases	O
performance	O
is	O
generally	O
best	O
lower	O
number	O
in	O
the	O
graph	O
at	O
an	O
intermediate	O
value	B
of	O
the	O
two	O
left	O
panels	O
are	O
applications	O
to	O
simple	O
continuous-state	O
control	B
tasks	O
using	O
the	O
sarsa	B
algorithm	O
and	O
tile	B
coding	I
with	O
either	O
replacing	B
or	O
accumulating	B
traces	O
the	O
upper-right	O
panel	O
is	O
for	O
policy	B
evaluation	O
on	O
a	O
random	B
walk	I
task	O
using	O
td	B
and	O
sutton	O
the	O
lower	O
right	O
panel	O
is	O
unpublished	O
data	O
for	O
the	O
polebalancing	O
task	O
from	O
an	O
earlier	O
study	O
accumulatingtraces	O
and	O
steps	O
worldreplacingtracesaccumulatingtraces	O
replacingtracesaccumulatingtracesrms	O
error	O
conclusions	O
bibliographical	O
and	O
historical	O
remarks	O
eligibility	B
traces	I
came	O
into	O
reinforcement	B
learning	I
via	O
the	O
fecund	O
ideas	O
of	O
klopf	B
our	O
use	O
of	O
eligibility	B
traces	I
is	O
based	O
on	O
klopf	B
s	O
work	O
barto	O
and	O
sutton	O
sutton	O
and	O
barto	O
barto	O
sutton	O
and	O
anderson	O
sutton	O
we	O
may	O
have	O
been	O
the	O
first	O
to	O
use	O
the	O
term	O
eligibility	O
trace	O
and	O
barto	O
the	O
idea	O
that	O
stimuli	O
produce	O
after	O
effects	O
in	O
the	O
nervous	O
system	O
that	O
are	O
important	O
for	O
learning	O
is	O
very	O
old	O
chapter	O
some	O
of	O
the	O
earliest	O
uses	O
of	O
eligibility	B
traces	I
were	O
in	O
the	O
actor	O
critic	O
methods	O
discussed	O
in	O
chapter	O
sutton	O
and	O
anderson	O
sutton	O
compound	B
updates	O
were	O
called	O
complex	O
backups	O
in	O
the	O
first	O
edition	O
of	O
this	O
book	O
the	O
and	O
its	O
error-reduction	O
properties	O
were	O
introduced	O
by	O
watkins	B
and	O
further	O
developed	O
by	O
jaakkola	O
jordan	O
and	O
singh	O
the	O
random	B
walk	I
results	O
in	O
this	O
and	O
subsequent	O
sections	O
are	O
new	O
to	O
this	O
text	O
as	O
are	O
the	O
terms	O
forward	O
view	O
and	O
backward	O
view	O
the	O
notion	O
of	O
a	O
algorithm	O
was	O
introduced	O
in	O
the	O
first	O
edition	O
of	O
this	O
text	O
the	O
more	O
refined	O
treatment	O
presented	O
here	O
was	O
developed	O
in	O
conjunction	O
with	O
harm	O
van	O
seijen	O
van	O
seijen	O
and	O
sutton	O
td	B
with	O
accumulating	B
traces	O
was	O
introduced	O
by	O
sutton	O
convergence	O
in	O
the	O
mean	O
was	O
proved	O
by	O
dayan	O
and	O
with	O
probability	O
by	O
many	O
researchers	O
including	O
peng	O
dayan	O
and	O
sejnowski	O
tsitsiklis	O
and	O
gurvits	O
lin	O
and	O
hanson	O
the	O
bound	O
on	O
the	O
error	O
of	O
the	O
asymptotic	O
solution	O
of	O
linear	O
td	B
is	O
due	O
to	O
tsitsiklis	O
and	O
van	O
roy	O
truncated	B
td	B
methods	O
were	O
developed	O
by	O
cichosz	O
and	O
van	O
seijen	O
true	B
online	B
td	B
and	O
the	O
other	O
ideas	O
presented	O
in	O
these	O
sections	O
are	O
primarily	O
due	O
to	O
work	O
of	O
van	O
seijen	O
seijen	O
and	O
sutton	O
van	O
seijen	O
et	O
al	O
replacing	B
traces	O
are	O
due	O
to	O
singh	O
and	O
sutton	O
the	O
material	O
in	O
this	O
section	O
is	O
from	O
van	O
hasselt	O
and	O
sutton	O
sarsa	B
with	O
accumulating	B
traces	O
was	O
first	O
explored	O
as	O
a	O
control	B
method	O
by	O
rummery	O
and	O
niranjan	O
rummery	O
true	B
online	B
sarsa	B
was	O
introduced	O
by	O
van	O
seijen	O
and	O
sutton	O
the	O
algorithm	O
on	O
page	O
was	O
adapted	O
from	O
van	O
seijen	O
et	O
al	O
the	O
mountain	O
car	O
results	O
were	O
made	O
for	O
this	O
text	O
except	O
for	O
figure	O
which	O
is	O
adapted	O
from	O
van	O
seijen	O
and	O
sutton	O
perhaps	O
the	O
first	O
published	O
discussion	O
of	O
variable	O
was	O
by	O
watkins	B
who	O
pointed	O
out	O
that	O
the	O
cutting	O
off	O
of	O
the	O
update	O
sequence	O
in	O
his	O
q	O
when	O
a	O
nongreedy	O
action	B
was	O
selected	O
could	O
be	O
implemented	O
by	O
temporarily	O
setting	O
to	O
chapter	O
eligibility	B
traces	I
variable	O
was	O
introduced	O
in	O
the	O
first	O
edition	O
of	O
this	O
text	O
the	O
roots	O
of	O
variable	O
are	O
in	O
the	O
work	O
on	O
options	B
precup	O
and	O
singh	O
and	O
its	O
precursors	O
becoming	O
explicit	O
in	O
the	O
gq	O
paper	O
and	O
sutton	O
which	O
also	O
introduced	O
some	O
of	O
these	O
recursive	O
forms	O
for	O
the	O
a	O
different	O
notion	O
of	O
variable	O
has	O
been	O
developed	O
by	O
yu	O
off-policy	B
eligibility	B
traces	I
were	O
introduced	O
by	O
precup	O
et	O
al	O
then	O
further	O
developed	O
by	O
bertsekas	O
and	O
yu	O
maei	O
maei	O
and	O
sutton	O
yu	O
and	O
by	O
sutton	O
mahmood	O
precup	O
and	O
van	O
hasselt	O
the	O
last	O
reference	O
in	O
particular	O
gives	O
a	O
powerful	O
forward	O
view	O
for	O
off-policy	B
td	B
methods	O
with	O
general	O
state-dependent	O
and	O
the	O
presentation	O
here	O
seems	O
to	O
be	O
new	O
this	O
section	O
ends	O
with	O
an	O
elegant	O
expected	B
sarsa	B
algorithm	O
although	O
it	O
is	O
a	O
natural	O
algorithm	O
to	O
our	O
knowledge	O
it	O
has	O
not	O
previously	O
been	O
described	O
or	O
tested	O
in	O
the	O
literature	O
watkins	B
s	O
q	O
is	O
due	O
to	O
watkins	B
the	O
tabular	O
episodic	O
off-line	B
version	O
has	O
been	O
proven	O
convergent	O
by	O
munos	O
stepleton	O
harutyunyan	O
and	O
bellemare	O
alternative	O
q	O
algorithms	O
were	O
proposed	O
by	O
peng	O
and	O
williams	O
and	O
by	O
sutton	O
mahmood	O
precup	O
and	O
van	O
hasselt	O
tree	B
backup	I
is	O
due	O
to	O
precup	O
sutton	O
and	O
singh	O
gtd	O
is	O
due	O
to	O
maei	O
gq	O
is	O
due	O
to	O
maei	O
and	O
sutton	O
htd	O
is	O
due	O
to	O
white	O
and	O
white	O
based	O
on	O
the	O
one-step	O
htd	O
algorithm	O
introduced	O
by	O
hackman	O
the	O
latest	O
developments	O
in	O
the	O
theory	O
of	O
gradient-td	B
methods	I
are	O
by	O
yu	O
emphatic	O
td	B
was	O
introduced	O
by	O
sutton	O
mahmood	O
and	O
white	O
who	O
proved	O
its	O
stability	O
yu	O
proved	O
its	O
convergence	O
and	O
the	O
algorithm	O
was	O
developed	O
further	O
by	O
hallak	O
et	O
al	O
chapter	O
policy	B
gradient	B
methods	I
in	O
this	O
chapter	O
we	O
consider	O
something	O
new	O
so	O
far	O
in	O
this	O
book	O
almost	O
all	O
the	O
methods	O
have	O
been	O
action-value	B
methods	I
they	O
learned	O
the	O
values	O
of	O
actions	O
and	O
then	O
selected	O
actions	O
based	O
on	O
their	O
estimated	O
action	B
their	O
policies	O
would	O
not	O
even	O
exist	O
without	O
the	O
action-value	O
estimates	O
in	O
this	O
chapter	O
we	O
consider	O
methods	O
that	O
instead	O
learn	O
a	O
parameterized	O
policy	B
that	O
can	O
select	O
actions	O
without	O
consulting	O
a	O
value	B
function	I
a	O
value	B
function	I
may	O
still	O
be	O
used	O
to	O
learn	O
the	O
policy	B
parameter	O
but	O
is	O
not	O
required	O
for	O
action	B
selection	O
we	O
use	O
the	O
notation	O
for	O
the	O
policy	B
s	O
parameter	O
vector	B
thus	O
we	O
write	O
prat	O
a	O
st	O
s	O
t	O
for	O
the	O
probability	O
that	O
action	B
a	O
is	O
taken	O
at	O
time	O
t	O
given	O
that	O
the	O
environment	B
is	O
in	O
state	B
s	O
at	O
time	O
t	O
with	O
parameter	O
if	O
a	O
method	O
uses	O
a	O
learned	O
value	B
function	I
as	O
well	O
then	O
the	O
value	B
function	I
s	O
weight	O
vector	B
is	O
denoted	O
w	O
rd	O
as	O
usual	O
as	O
in	O
vsw	O
in	O
this	O
chapter	O
we	O
consider	O
methods	O
for	O
learning	O
the	O
policy	B
parameter	O
based	O
on	O
the	O
gradient	B
of	O
some	O
performance	O
measure	O
j	O
with	O
respect	O
to	O
the	O
policy	B
parameter	O
these	O
methods	O
seek	O
to	O
maximize	O
performance	O
so	O
their	O
updates	O
approximate	O
gradient	B
ascent	O
in	O
j	O
t	O
j	O
t	O
where	O
j	O
t	O
is	O
a	O
stochastic	O
estimate	O
whose	O
expectation	O
approximates	O
the	O
gradient	B
of	O
the	O
performance	O
measure	O
with	O
respect	O
to	O
its	O
argument	O
t	O
all	O
methods	O
that	O
follow	O
this	O
general	O
schema	O
we	O
call	O
policy	B
gradient	B
methods	I
whether	O
or	O
not	O
they	O
also	O
learn	O
an	O
approximate	O
value	B
function	I
methods	O
that	O
learn	O
approximations	O
to	O
both	O
policy	B
and	O
value	B
functions	O
are	O
often	O
called	O
actor	O
critic	O
methods	O
where	O
actor	O
is	O
a	O
reference	O
to	O
the	O
learned	O
policy	B
and	O
critic	O
refers	O
to	O
the	O
learned	O
value	B
function	I
usually	O
a	O
state-value	O
function	O
first	O
we	O
treat	O
the	O
episodic	O
case	O
in	O
which	O
performance	O
is	O
defined	O
as	O
the	O
value	B
of	O
the	O
start	O
state	B
under	O
the	O
parameterized	O
policy	B
before	O
going	O
on	O
to	O
consider	O
the	O
continuing	O
case	O
in	O
which	O
performance	O
is	O
defined	O
as	O
the	O
average	O
reward	O
rate	O
as	O
in	O
section	O
in	O
the	O
end	O
lone	O
exception	O
is	O
the	O
gradient	B
bandit	O
algorithms	O
of	O
section	O
in	O
fact	O
that	O
section	O
goes	O
through	O
many	O
of	O
the	O
same	O
steps	O
in	O
the	O
single-state	O
bandit	O
case	O
as	O
we	O
go	O
through	O
here	O
for	O
full	O
mdps	O
reviewing	O
that	O
section	O
would	O
be	O
good	O
preparation	O
for	O
fully	O
understanding	O
this	O
chapter	O
chapter	O
policy	B
gradient	B
methods	I
we	O
are	O
able	O
to	O
express	O
the	O
algorithms	O
for	O
both	O
cases	O
in	O
very	O
similar	O
terms	O
policy	B
approximation	I
and	O
its	O
advantages	O
in	O
policy	B
gradient	B
methods	I
the	O
policy	B
can	O
be	O
parameterized	O
in	O
any	O
way	O
as	O
long	O
as	O
is	O
differentiable	O
with	O
respect	O
to	O
its	O
parameters	O
that	O
is	O
as	O
long	O
as	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
with	O
respect	O
to	O
the	O
components	O
of	O
exists	O
and	O
is	O
finite	O
for	O
all	O
s	O
s	O
a	O
as	O
and	O
in	O
practice	O
to	O
ensure	O
exploration	O
we	O
generally	O
require	O
that	O
the	O
policy	B
never	O
becomes	O
deterministic	O
that	O
for	O
all	O
s	O
a	O
in	O
this	O
section	O
we	O
introduce	O
the	O
most	O
common	O
parameterization	O
for	O
discrete	O
action	B
spaces	O
and	O
point	O
out	O
the	O
advantages	O
it	O
offers	O
over	O
action-value	B
methods	I
policy-based	O
methods	O
also	O
offer	O
useful	O
ways	O
of	O
dealing	O
with	O
continuous	B
action	B
spaces	O
as	O
we	O
describe	O
later	O
in	O
section	O
if	O
the	O
action	B
space	O
is	O
discrete	O
and	O
not	O
too	O
large	O
then	O
a	O
natural	O
and	O
common	O
kind	O
of	O
parameterization	O
is	O
to	O
form	O
parameterized	O
numerical	O
preferences	O
hs	O
a	O
r	O
for	O
each	O
state	B
action	B
pair	O
the	O
actions	O
with	O
the	O
highest	O
preferences	O
in	O
each	O
state	B
are	O
given	O
the	O
highest	O
probabilities	O
of	O
being	O
selected	O
for	O
example	O
according	O
to	O
an	O
exponential	O
soft-max	B
distribution	O
ehsa	O
ehsb	O
where	O
e	O
is	O
the	O
base	O
of	O
the	O
natural	O
logarithm	O
note	O
that	O
the	O
denominator	O
here	O
is	O
just	O
what	O
is	O
required	O
so	O
that	O
the	O
action	B
probabilities	O
in	O
each	O
state	B
sum	O
to	O
one	O
we	O
call	O
this	O
kind	O
of	O
policy	B
parameterization	O
soft-max	B
in	O
action	B
preferences	I
the	O
action	B
preferences	I
themselves	O
can	O
be	O
parameterized	O
arbitrarily	O
for	O
example	O
they	O
might	O
be	O
computed	O
by	O
a	O
deep	O
neural	B
network	O
where	O
is	O
the	O
vector	B
of	O
all	O
the	O
connection	O
weights	O
of	O
the	O
network	O
in	O
the	O
alphago	B
system	O
described	O
in	O
section	O
or	O
the	O
preferences	O
could	O
simply	O
be	O
linear	O
in	O
features	O
hs	O
a	O
a	O
using	O
feature	O
vectors	O
xs	O
a	O
constructed	O
by	O
any	O
of	O
the	O
methods	O
described	O
in	O
chapter	O
one	O
advantage	O
of	O
parameterizing	O
policies	O
according	O
to	O
the	O
soft-max	B
in	O
action	B
preferences	I
is	O
that	O
the	O
approximate	O
policy	B
can	O
approach	O
a	O
deterministic	O
policy	B
whereas	O
with	O
action	B
selection	O
over	O
action	B
values	O
there	O
is	O
always	O
an	O
probability	O
of	O
selecting	O
a	O
random	O
action	B
of	O
course	O
one	O
could	O
select	O
according	O
to	O
a	O
soft-max	B
distribution	O
based	O
on	O
action	B
values	O
but	O
this	O
alone	O
would	O
not	O
allow	O
the	O
policy	B
to	O
approach	O
a	O
deterministic	O
policy	B
instead	O
the	O
action-value	O
estimates	O
would	O
converge	O
to	O
their	O
corresponding	O
true	O
values	O
which	O
would	O
differ	O
by	O
a	O
finite	O
amount	O
translating	O
to	O
specific	O
probabilities	O
other	O
than	O
and	O
if	O
the	O
soft-max	B
distribution	O
included	O
a	O
temperature	O
parameter	O
then	O
the	O
temperature	O
could	O
be	O
reduced	O
over	O
time	O
to	O
approach	O
determinism	O
but	O
in	O
practice	O
it	O
would	O
be	O
difficult	O
to	O
choose	O
the	O
reduction	O
schedule	O
or	O
even	O
the	O
initial	O
temperature	O
without	O
more	O
prior	B
knowledge	I
of	O
the	O
true	O
action	B
values	O
than	O
we	O
would	O
like	O
to	O
assume	O
policy	B
approximation	I
and	O
its	O
advantages	O
action	B
preferences	I
are	O
different	O
because	O
they	O
do	O
not	O
approach	O
specific	O
values	O
instead	O
they	O
are	O
driven	O
to	O
produce	O
the	O
optimal	O
stochastic	O
policy	B
if	O
the	O
optimal	O
policy	B
is	O
deterministic	O
then	O
the	O
preferences	O
of	O
the	O
optimal	O
actions	O
will	O
be	O
driven	O
infinitely	O
higher	O
than	O
all	O
suboptimal	O
actions	O
permitted	O
by	O
the	O
parameterization	O
a	O
second	O
advantage	O
of	O
parameterizing	O
policies	O
according	O
to	O
the	O
soft-max	B
in	O
action	B
preferences	I
is	O
that	O
it	O
enables	O
the	O
selection	O
of	O
actions	O
with	O
arbitrary	O
probabilities	O
in	O
problems	O
with	O
significant	O
function	B
approximation	I
the	O
best	O
approximate	O
policy	B
may	O
be	O
stochastic	O
for	O
example	O
in	O
card	O
games	O
with	O
imperfect	O
information	O
the	O
optimal	O
play	O
is	O
often	O
to	O
do	O
two	O
different	O
things	O
with	O
specific	O
probabilities	O
such	O
as	O
when	O
bluffing	O
in	O
poker	O
action-value	B
methods	I
have	O
no	O
natural	O
way	O
of	O
finding	O
stochastic	O
optimal	O
policies	O
whereas	O
policy	B
approximating	O
methods	O
can	O
as	O
shown	O
in	O
example	O
example	O
short	O
corridor	O
with	O
switched	O
actions	O
consider	O
the	O
small	O
corridor	O
gridworld	O
shown	O
inset	O
in	O
the	O
graph	O
below	O
the	O
reward	O
is	O
per	O
step	O
as	O
usual	O
in	O
each	O
of	O
the	O
three	O
nonterminal	O
states	O
there	O
are	O
only	O
two	O
actions	O
right	O
and	O
left	O
these	O
actions	O
have	O
their	O
usual	O
consequences	O
in	O
the	O
first	O
and	O
third	O
states	O
causes	O
no	O
movement	O
in	O
the	O
first	O
state	B
but	O
in	O
the	O
second	O
state	B
they	O
are	O
reversed	O
so	O
that	O
right	O
moves	O
to	O
the	O
left	O
and	O
left	O
moves	O
to	O
the	O
right	O
the	O
problem	O
is	O
difficult	O
because	O
all	O
the	O
states	O
appear	O
identical	O
under	O
the	O
function	B
approximation	I
in	O
particular	O
we	O
define	O
xs	O
right	O
and	O
xs	O
left	O
for	O
all	O
s	O
an	O
action-value	O
method	O
with	O
action	B
selection	O
is	O
forced	O
to	O
choose	O
between	O
just	O
two	O
policies	O
choosing	O
right	O
with	O
high	O
probability	O
on	O
all	O
steps	O
or	O
choosing	O
left	O
with	O
the	O
same	O
high	O
probability	O
on	O
all	O
time	O
steps	O
if	O
then	O
these	O
two	O
policies	O
achieve	O
a	O
value	B
the	O
start	O
state	B
of	O
less	O
than	O
and	O
respectively	O
as	O
shown	O
in	O
the	O
graph	O
a	O
method	O
can	O
do	O
significantly	O
better	O
if	O
it	O
can	O
learn	O
a	O
specific	O
probability	O
with	O
which	O
to	O
select	O
right	O
the	O
best	O
probability	O
is	O
about	O
which	O
achieves	O
a	O
value	B
of	O
about	O
probability	O
of	O
right	O
left	O
right	O
optimalstochasticpolicy	O
j	O
chapter	O
policy	B
gradient	B
methods	I
perhaps	O
the	O
simplest	O
advantage	O
that	O
policy	B
parameterization	O
may	O
have	O
over	O
actionvalue	O
parameterization	O
is	O
that	O
the	O
policy	B
may	O
be	O
a	O
simpler	O
function	O
to	O
approximate	O
problems	O
vary	O
in	O
the	O
complexity	O
of	O
their	O
policies	O
and	O
action-value	O
functions	O
for	O
some	O
the	O
action-value	O
function	O
is	O
simpler	O
and	O
thus	O
easier	O
to	O
approximate	O
for	O
others	O
the	O
policy	B
is	O
simpler	O
in	O
the	O
latter	O
case	O
a	O
policy-based	O
method	O
will	O
typically	O
learn	O
faster	O
and	O
yield	O
a	O
superior	O
asymptotic	O
policy	B
seems	O
to	O
be	O
the	O
case	O
with	O
tetris	O
see	O
s	O
im	O
sek	O
alg	O
orta	O
and	O
kothiyal	O
finally	O
we	O
note	O
that	O
the	O
choice	O
of	O
policy	B
parameterization	O
is	O
sometimes	O
a	O
good	O
way	O
of	O
injecting	O
prior	B
knowledge	I
about	O
the	O
desired	O
form	O
of	O
the	O
policy	B
into	O
the	O
reinforcement	B
learning	I
system	O
this	O
is	O
often	O
the	O
most	O
important	O
reason	O
for	O
using	O
a	O
policy-based	O
learning	O
method	O
exercise	O
use	O
your	O
knowledge	O
of	O
the	O
gridworld	O
and	O
its	O
dynamics	O
to	O
determine	O
an	O
exact	O
symbolic	O
expression	O
for	O
the	O
optimal	O
probability	O
of	O
selecting	O
the	O
right	O
action	B
in	O
example	O
the	O
policy	B
gradient	B
theorem	B
in	O
addition	O
to	O
the	O
practical	O
advantages	B
of	I
policy	B
parameterization	O
over	O
action	B
selection	O
there	O
is	O
also	O
an	O
important	O
theoretical	O
advantage	O
with	O
continuous	O
policy	B
parameterization	O
the	O
action	B
probabilities	O
change	O
smoothly	O
as	O
a	O
function	O
of	O
the	O
learned	O
parameter	O
whereas	O
in	O
selection	O
the	O
action	B
probabilities	O
may	O
change	O
dramatically	O
for	O
an	O
arbitrarily	O
small	O
change	O
in	O
the	O
estimated	O
action	B
values	O
if	O
that	O
change	O
results	O
in	O
a	O
different	O
action	B
having	O
the	O
maximal	O
value	B
largely	O
because	O
of	O
this	O
stronger	O
convergence	O
guarantees	O
are	O
available	O
for	O
policy-gradient	O
methods	O
than	O
for	O
action-value	B
methods	I
in	O
particular	O
it	O
is	O
the	O
continuity	O
of	O
the	O
policy	B
dependence	O
on	O
the	O
parameters	O
that	O
enables	O
policy-gradient	O
methods	O
to	O
approximate	O
gradient	B
ascent	O
the	O
episodic	O
and	O
continuing	O
cases	O
define	O
the	O
performance	O
measure	O
j	O
differently	O
and	O
thus	O
have	O
to	O
be	O
treated	O
separately	O
to	O
some	O
extent	O
nevertheless	O
we	O
will	O
try	O
to	O
present	O
both	O
cases	O
uniformly	O
and	O
we	O
develop	O
a	O
notation	O
so	O
that	O
the	O
major	O
theoretical	O
results	O
can	O
be	O
described	O
with	O
a	O
single	O
set	O
of	O
equations	O
in	O
this	O
section	O
we	O
treat	O
the	O
episodic	O
case	O
for	O
which	O
we	O
define	O
the	O
performance	O
measure	O
as	O
the	O
value	B
of	O
the	O
start	O
state	B
of	O
the	O
episode	O
we	O
can	O
simplify	O
the	O
notation	O
without	O
losing	O
any	O
meaningful	O
generality	O
by	O
assuming	O
that	O
every	O
episode	O
starts	O
in	O
some	O
particular	O
state	B
then	O
in	O
the	O
episodic	O
case	O
we	O
define	O
performance	O
as	O
j	O
v	O
where	O
v	O
is	O
the	O
true	O
value	B
function	I
for	O
the	O
policy	B
determined	O
by	O
from	O
here	O
on	O
in	O
our	O
discussion	O
we	O
will	O
assume	O
no	O
discounting	B
for	O
the	O
episodic	O
case	O
although	O
for	O
completeness	O
we	O
do	O
include	O
the	O
possibility	O
of	O
discounting	B
in	O
the	O
boxed	O
algorithms	O
with	B
function	B
approximation	I
it	O
may	O
seem	O
challenging	O
to	O
change	O
the	O
policy	B
parameter	O
in	O
a	O
way	O
that	O
ensures	O
improvement	O
the	O
problem	O
is	O
that	O
performance	O
depends	O
on	O
both	O
the	O
action	B
selections	O
and	O
the	O
distribution	O
of	O
states	O
in	O
which	O
those	O
selections	O
are	O
made	O
and	O
that	O
both	O
of	O
these	O
are	O
affected	O
by	O
the	O
policy	B
parameter	O
given	O
a	O
state	B
the	O
effect	O
of	O
the	O
policy	B
gradient	B
theorem	B
proof	B
of	O
the	O
policy	B
gradient	B
theorem	B
case	O
with	O
just	O
elementary	O
calculus	O
and	O
re-arranging	O
of	O
terms	O
we	O
can	O
prove	O
the	O
policy	B
gradient	B
theorem	B
from	O
first	O
principles	O
to	O
keep	O
the	O
notation	O
simple	O
we	O
leave	O
it	O
implicit	O
in	O
all	O
cases	O
that	O
is	O
a	O
function	O
of	O
and	O
all	O
gradients	O
are	O
also	O
implicitly	O
with	O
respect	O
to	O
first	O
note	O
that	O
the	O
gradient	B
of	O
the	O
state-value	O
function	O
can	O
be	O
written	O
in	O
terms	O
of	O
the	O
action-value	O
function	O
as	O
v	O
for	O
all	O
s	O
s	O
a	O
q	O
rule	O
of	O
calculus	O
a	O
a	O
a	O
rs	O
v	O
a	O
v	O
a	O
and	O
equation	O
prs	O
x	O
k	O
a	O
s	O
v	O
after	O
repeated	O
unrolling	O
where	O
prs	O
x	O
k	O
is	O
the	O
probability	O
of	O
transitioning	O
from	O
state	B
s	O
to	O
state	B
x	O
in	O
k	O
steps	O
under	O
policy	B
it	O
is	O
then	O
immediate	O
that	O
j	O
v	O
a	O
a	O
s	O
k	O
a	O
a	O
a	O
page	O
chapter	O
policy	B
gradient	B
methods	I
the	O
policy	B
parameter	O
on	O
the	O
actions	O
and	O
thus	O
on	O
reward	O
can	O
be	O
computed	O
in	O
a	O
relatively	O
straightforward	O
way	O
from	O
knowledge	O
of	O
the	O
parameterization	O
but	O
the	O
effect	O
of	O
the	O
policy	B
on	O
the	O
state	B
distribution	O
is	O
a	O
function	O
of	O
the	O
environment	B
and	O
is	O
typically	O
unknown	O
how	O
can	O
we	O
estimate	O
the	O
performance	O
gradient	B
with	O
respect	O
to	O
the	O
policy	B
parameter	O
when	O
the	O
gradient	B
depends	O
on	O
the	O
unknown	O
effect	O
of	O
policy	B
changes	O
on	O
the	O
state	B
distribution	O
fortunately	O
there	O
is	O
an	O
excellent	O
theoretical	O
answer	O
to	O
this	O
challenge	O
in	O
the	O
form	O
of	O
the	O
policy	B
gradient	B
theorem	B
which	O
provides	O
an	O
analytic	O
expression	O
for	O
the	O
gradient	B
of	O
performance	O
with	O
respect	O
to	O
the	O
policy	B
parameter	O
is	O
what	O
we	O
need	O
to	O
approximate	O
for	O
gradient	B
ascent	O
that	O
does	O
not	O
involve	O
the	O
derivative	O
of	O
the	O
state	B
distribution	O
the	O
policy	B
gradient	B
theorem	B
for	O
the	O
episodic	O
case	O
establishes	O
that	O
j	O
q	O
a	O
where	O
the	O
gradients	O
are	O
column	O
vectors	O
of	O
partial	O
derivatives	O
with	O
respect	O
to	O
the	O
components	O
of	O
and	O
denotes	O
the	O
policy	B
corresponding	O
to	O
parameter	O
vector	B
the	O
symbol	O
here	O
means	O
proportional	O
to	O
in	O
the	O
episodic	O
case	O
the	O
constant	O
of	O
proportionality	O
is	O
the	O
average	O
length	O
of	O
an	O
episode	O
and	O
in	O
the	O
continuing	O
case	O
it	O
is	O
so	O
that	O
the	O
relationship	O
is	O
actually	O
an	O
equality	O
the	O
distribution	O
here	O
in	O
chapters	O
and	O
is	O
the	O
on-policy	B
distribution	I
under	O
page	O
the	O
policy	B
gradient	B
theorem	B
is	O
proved	O
for	O
the	O
episodic	O
case	O
in	O
the	O
box	O
on	O
the	O
previous	O
page	O
reinforce	B
monte	B
carlo	I
policy	B
gradient	B
we	O
are	O
now	O
ready	O
for	O
our	O
first	O
policy-gradient	O
learning	O
algorithm	O
recall	O
our	O
overall	O
strategy	O
of	O
stochastic	O
gradient	B
ascent	O
which	O
requires	O
a	O
way	O
to	O
obtain	O
samples	O
such	O
that	O
the	O
expectation	O
of	O
the	O
sample	O
gradient	B
is	O
proportional	O
to	O
the	O
actual	O
gradient	B
of	O
the	O
performance	O
measure	O
as	O
a	O
function	O
of	O
the	O
parameter	O
the	O
sample	O
gradients	O
need	O
only	O
be	O
proportional	O
to	O
the	O
gradient	B
because	O
any	O
constant	O
of	O
proportionality	O
can	O
be	O
absorbed	O
into	O
the	O
step	O
size	O
which	O
is	O
otherwise	O
arbitrary	O
the	O
policy	B
gradient	B
theorem	B
gives	O
an	O
exact	O
expression	O
proportional	O
to	O
the	O
gradient	B
all	O
that	O
is	O
needed	O
is	O
some	O
way	O
of	O
sampling	O
whose	O
expectation	O
equals	O
or	O
approximates	O
this	O
expression	O
notice	O
that	O
the	O
right-hand	O
side	O
of	O
the	O
policy	B
gradient	B
theorem	B
is	O
a	O
sum	O
over	O
states	O
weighted	O
by	O
how	O
often	O
the	O
states	O
occur	O
under	O
the	O
target	O
policy	B
if	O
is	O
followed	O
then	O
states	O
will	O
be	O
encountered	O
in	O
these	O
proportions	O
thus	O
q	O
a	O
j	O
q	O
a	O
e	O
this	O
is	O
good	O
progress	O
and	O
we	O
would	O
like	O
to	O
carry	O
it	O
further	O
and	O
handle	O
the	O
action	B
in	O
the	O
same	O
way	O
replacing	B
a	O
with	O
the	O
sample	O
action	B
at	O
the	O
remaining	O
part	O
of	O
the	O
expectation	O
above	O
is	O
a	O
sum	O
over	O
actions	O
if	O
only	O
each	O
term	O
were	O
weighted	O
by	O
the	O
probability	O
of	O
selecting	O
the	O
actions	O
that	O
is	O
according	O
to	O
then	O
the	O
replacement	O
could	O
be	O
done	O
we	O
reinforce	B
monte	B
carlo	I
policy	B
gradient	B
can	O
arrange	O
for	O
this	O
by	O
multiplying	O
and	O
dividing	O
by	O
this	O
probability	O
continuing	O
from	O
the	O
previous	O
equation	O
this	O
gives	O
j	O
e	O
a	O
e	O
at	O
e	O
a	O
by	O
the	O
sample	O
at	O
e	O
at	O
q	O
at	O
where	O
gt	O
is	O
the	O
return	B
as	O
usual	O
the	O
final	O
expression	O
in	O
the	O
brackets	O
is	O
exactly	O
what	O
is	O
needed	O
a	O
quantity	O
that	O
can	O
be	O
sampled	O
on	O
each	O
time	O
step	O
whose	O
expectation	O
is	O
equal	O
to	O
the	O
gradient	B
using	O
this	O
sample	O
to	O
instantiate	O
our	O
generic	O
stochastic	O
gradient	B
ascent	O
algorithm	O
yields	O
the	O
update	O
t	O
gt	O
t	O
t	O
we	O
call	O
this	O
algorithm	O
reinforce	B
williams	O
its	O
update	O
has	O
an	O
intuitive	O
appeal	O
each	O
increment	O
is	O
proportional	O
to	O
the	O
product	O
of	O
a	O
return	B
gt	O
and	O
a	O
vector	B
the	O
gradient	B
of	O
the	O
probability	O
of	O
taking	O
the	O
action	B
actually	O
taken	O
divided	O
by	O
the	O
probability	O
of	O
taking	O
that	O
action	B
the	O
vector	B
is	O
the	O
direction	O
in	O
parameter	O
space	O
that	O
most	O
increases	O
the	O
probability	O
of	O
repeating	O
the	O
action	B
at	O
on	O
future	O
visits	O
to	O
state	B
st	O
the	O
update	O
increases	O
the	O
parameter	O
vector	B
in	O
this	O
direction	O
proportional	O
to	O
the	O
return	B
and	O
inversely	O
proportional	O
to	O
the	O
action	B
probability	O
the	O
former	O
makes	O
sense	O
because	O
it	O
causes	O
the	O
parameter	O
to	O
move	O
most	O
in	O
the	O
directions	O
that	O
favor	O
actions	O
that	O
yield	O
the	O
highest	O
return	B
the	O
latter	O
makes	O
sense	O
because	O
otherwise	O
actions	O
that	O
are	O
selected	O
frequently	O
are	O
at	O
an	O
advantage	O
updates	O
will	O
be	O
more	O
often	O
in	O
their	O
direction	O
and	O
might	O
win	O
out	O
even	O
if	O
they	O
do	O
not	O
yield	O
the	O
highest	O
return	B
note	O
that	O
reinforce	B
uses	O
the	O
complete	O
return	B
from	O
time	O
t	O
which	O
includes	O
all	O
future	O
rewards	O
up	O
until	O
the	O
end	O
of	O
the	O
episode	O
in	O
this	O
sense	O
reinforce	B
is	O
a	O
monte	B
carlo	I
algorithm	O
and	O
is	O
well	O
defined	O
only	O
for	O
the	O
episodic	O
case	O
with	O
all	O
updates	O
made	O
in	O
retrospect	O
after	O
the	O
episode	O
is	O
completed	O
the	O
monte	B
carlo	I
algorithms	O
in	O
chapter	O
this	O
is	O
shown	O
explicitly	O
in	O
the	O
boxed	O
on	O
the	O
next	O
page	O
notice	O
that	O
the	O
update	O
in	O
the	O
last	O
line	O
of	O
pseudocode	O
appears	O
rather	O
different	O
from	O
the	O
reinforce	B
update	O
rule	O
one	O
difference	O
is	O
that	O
the	O
pseudocode	O
uses	O
the	O
compact	O
expression	O
ln	O
t	O
for	O
the	O
fractional	O
vector	B
t	O
in	O
that	O
these	O
t	O
two	O
expressions	O
for	O
the	O
vector	B
are	O
equivalent	O
follows	O
from	O
the	O
identity	O
ln	O
x	O
x	O
x	O
this	O
vector	B
has	O
been	O
given	O
several	O
names	O
and	O
notations	O
in	O
the	O
literature	O
we	O
will	O
refer	O
to	O
it	O
simply	O
as	O
the	O
eligibility	O
vector	B
note	O
that	O
it	O
is	O
the	O
only	O
place	O
that	O
the	O
policy	B
parameterization	O
appears	O
in	O
the	O
algorithm	O
chapter	O
policy	B
gradient	B
methods	I
reinforce	B
monte-carlo	O
policy-gradient	O
control	B
for	O
input	O
a	O
differentiable	O
policy	B
parameterization	O
algorithm	O
parameter	O
step	O
size	O
initialize	O
policy	B
parameter	O
to	O
loop	O
forever	O
each	O
episode	O
generate	O
an	O
episode	O
st	O
at	O
rt	O
following	O
loop	O
for	O
each	O
step	O
of	O
the	O
episode	O
t	O
t	O
g	O
rk	O
t	O
g	O
ln	O
the	O
second	O
difference	O
between	O
the	O
pseudocode	O
update	O
and	O
the	O
reinforce	B
update	O
equation	O
is	O
that	O
the	O
former	O
includes	O
a	O
factor	O
of	O
t	O
this	O
is	O
because	O
as	O
mentioned	O
earlier	O
in	O
the	O
text	O
we	O
are	O
treating	O
the	O
non-discounted	O
case	O
while	O
in	O
the	O
boxed	O
algorithms	O
we	O
are	O
giving	O
the	O
algorithms	O
for	O
the	O
general	O
discounted	O
case	O
all	O
of	O
the	O
ideas	O
go	O
through	O
in	O
the	O
discounted	O
case	O
with	O
appropriate	O
adjustments	O
to	O
the	O
box	O
on	O
page	O
but	O
involve	O
additional	O
complexity	O
that	O
distracts	O
from	O
the	O
main	O
ideas	O
exercise	O
generalize	O
the	O
box	O
on	O
page	O
the	O
policy	B
gradient	B
theorem	B
the	O
proof	B
of	O
the	O
policy	B
gradient	B
theorem	B
and	O
the	O
steps	O
leading	O
to	O
the	O
reinforce	B
update	O
equation	O
so	O
that	O
ends	O
up	O
with	O
a	O
factor	O
of	O
t	O
and	O
thus	O
aligns	O
with	O
the	O
general	O
algorithm	O
given	O
in	O
the	O
pseudocode	O
figure	O
shows	O
the	O
performance	O
of	O
reinforce	B
averaged	O
over	O
runs	O
on	O
the	O
short-corridor	O
gridworld	O
from	O
example	O
figure	O
reinforce	B
on	O
the	O
short-corridor	O
gridworld	O
with	O
a	O
good	O
step	O
size	O
the	O
total	O
reward	O
per	O
episode	O
approaches	O
the	O
optimal	O
value	B
of	O
the	O
start	O
state	B
rewardon	O
reinforce	B
with	B
baseline	B
as	O
a	O
stochastic	O
gradient	B
method	O
reinforce	B
has	O
good	O
theoretical	O
convergence	O
properties	O
by	O
construction	O
the	O
expected	B
update	I
over	O
an	O
episode	O
is	O
in	O
the	O
same	O
direction	O
as	O
the	O
performance	O
gradient	B
this	O
assures	O
an	O
improvement	O
in	O
expected	B
performance	O
for	O
sufficiently	O
small	O
and	O
convergence	O
to	O
a	O
local	O
optimum	O
under	O
standard	O
stochastic	O
approximation	O
conditions	O
for	O
decreasing	O
however	O
as	O
a	O
monte	B
carlo	I
method	O
reinforce	B
may	O
be	O
of	O
high	O
variance	O
and	O
thus	O
produce	O
slow	O
learning	O
exercise	O
in	O
section	O
we	O
considered	O
policy	B
parameterizations	O
using	O
the	O
soft-max	B
in	O
action	B
preferences	I
with	O
linear	O
action	B
preferences	I
for	O
this	O
parameterization	O
prove	O
that	O
the	O
eligibility	O
vector	B
is	O
ln	O
xs	O
a	O
b	O
using	O
the	O
definitions	O
and	O
elementary	O
calculus	O
reinforce	B
with	B
baseline	B
the	O
policy	B
gradient	B
theorem	B
can	O
be	O
generalized	O
to	O
include	O
a	O
comparison	O
of	O
the	O
action	B
value	B
to	O
an	O
arbitrary	O
baseline	B
bs	O
j	O
a	O
the	O
baseline	B
can	O
be	O
any	O
function	O
even	O
a	O
random	O
variable	O
as	O
long	O
as	O
it	O
does	O
not	O
vary	O
with	O
a	O
the	O
equation	O
remains	O
valid	O
because	O
the	O
subtracted	O
quantity	O
is	O
zero	O
bs	O
bs	O
bs	O
the	O
policy	B
gradient	B
theorem	B
with	B
baseline	B
can	O
be	O
used	O
to	O
derive	O
an	O
update	O
rule	O
using	O
similar	O
steps	O
as	O
in	O
the	O
previous	O
section	O
the	O
update	O
rule	O
that	O
we	O
end	O
up	O
with	O
is	O
a	O
new	O
version	O
of	O
reinforce	B
that	O
includes	O
a	O
general	O
baseline	B
t	O
t	O
t	O
because	O
the	O
baseline	B
could	O
be	O
uniformly	O
zero	O
this	O
update	O
is	O
a	O
strict	O
generalization	O
of	O
reinforce	B
in	O
general	O
the	O
baseline	B
leaves	O
the	O
expected	B
value	B
of	O
the	O
update	O
unchanged	O
but	O
it	O
can	O
have	O
a	O
large	O
effect	O
on	O
its	O
variance	O
for	O
example	O
we	O
saw	O
in	O
section	O
that	O
an	O
analogous	O
baseline	B
can	O
significantly	O
reduce	O
the	O
variance	O
thus	O
speed	O
the	O
learning	O
of	O
gradient	B
bandit	O
algorithms	O
in	O
the	O
bandit	O
algorithms	O
the	O
baseline	B
was	O
just	O
a	O
number	O
average	O
of	O
the	O
rewards	O
seen	O
so	O
far	O
but	O
for	O
mdps	O
the	O
baseline	B
should	O
vary	O
with	O
state	B
in	O
some	O
states	O
all	O
actions	O
have	O
high	O
values	O
and	O
we	O
need	O
a	O
high	O
baseline	B
to	O
differentiate	O
the	O
higher	O
valued	O
actions	O
from	O
the	O
less	O
highly	O
valued	O
ones	O
in	O
other	O
states	O
all	O
actions	O
will	O
have	O
low	O
values	O
and	O
a	O
low	O
baseline	B
is	O
appropriate	O
one	O
natural	O
choice	O
for	O
the	O
baseline	B
is	O
an	O
estimate	O
of	O
the	O
state	B
value	B
vstw	O
where	O
w	O
rm	O
is	O
a	O
weight	O
vector	B
learned	O
by	O
one	O
of	O
the	O
methods	O
presented	O
in	O
previous	O
chapters	O
chapter	O
policy	B
gradient	B
methods	I
because	O
reinforce	B
is	O
a	O
monte	B
carlo	I
method	O
for	O
learning	O
the	O
policy	B
parameter	O
it	O
seems	O
natural	O
to	O
also	O
use	O
a	O
monte	B
carlo	I
method	O
to	O
learn	O
the	O
state-value	O
weights	O
w	O
a	O
complete	O
pseudocode	O
algorithm	O
for	O
reinforce	B
with	B
baseline	B
using	O
such	O
a	O
learned	O
state-value	O
function	O
as	O
the	O
baseline	B
is	O
given	O
in	O
the	O
box	O
below	O
reinforce	B
with	B
baseline	B
for	O
estimating	O
input	O
a	O
differentiable	O
policy	B
parameterization	O
input	O
a	O
differentiable	O
state-value	O
function	O
parameterization	O
vsw	O
algorithm	O
parameters	O
step	O
sizes	O
w	O
initialize	O
policy	B
parameter	O
and	O
state-value	O
weights	O
w	O
rd	O
to	O
loop	O
forever	O
each	O
episode	O
generate	O
an	O
episode	O
st	O
at	O
rt	O
following	O
loop	O
for	O
each	O
step	O
of	O
the	O
episode	O
t	O
t	O
g	O
rk	O
g	O
vstw	O
w	O
w	O
w	O
t	O
vstw	O
t	O
ln	O
this	O
algorithm	O
has	O
two	O
step	O
sizes	O
denoted	O
and	O
w	O
is	O
the	O
in	O
choosing	O
the	O
step	O
size	O
for	O
values	O
w	O
is	O
relatively	O
easy	O
in	O
the	O
linear	O
case	O
we	O
have	O
rules	O
of	O
thumb	O
for	O
setting	O
it	O
such	O
as	O
w	O
much	O
less	O
clear	O
how	O
to	O
set	O
the	O
step	O
size	O
for	O
the	O
policy	B
parameters	O
whose	O
best	O
value	B
depends	O
on	O
the	O
range	O
of	O
variation	O
of	O
the	O
rewards	O
and	O
on	O
the	O
policy	B
parameterization	O
section	O
it	O
is	O
figure	O
adding	O
a	O
baseline	B
to	O
reinforce	B
can	O
make	O
it	O
learn	O
much	O
faster	O
as	O
illustrated	O
here	O
on	O
the	O
short-corridor	O
gridworld	O
the	O
step	O
size	O
used	O
here	O
for	O
plain	O
reinforce	B
is	O
that	O
at	O
which	O
it	O
performs	O
best	O
the	O
nearest	O
power	O
of	O
two	O
see	O
figure	O
each	O
line	O
is	O
an	O
average	O
over	O
independent	O
runs	O
with	B
baseline	B
rewardon	O
actor	O
critic	O
methods	O
figure	O
compares	O
the	O
behavior	O
of	O
reinforce	B
with	O
and	O
without	O
a	O
baseline	B
on	O
the	O
short-corridor	O
gridword	O
here	O
the	O
approximate	O
state-value	O
function	O
used	O
in	O
the	O
baseline	B
is	O
vsw	O
w	O
that	O
is	O
w	O
is	O
a	O
single	O
component	O
w	O
actor	O
critic	O
methods	O
although	O
the	O
reinforce-with-baseline	O
method	O
learns	O
both	O
a	O
policy	B
and	O
a	O
state-value	O
function	O
we	O
do	O
not	O
consider	O
it	O
to	O
be	O
an	O
actor	O
critic	O
method	O
because	O
its	O
state-value	O
function	O
is	O
used	O
only	O
as	O
a	O
baseline	B
not	O
as	O
a	O
critic	O
that	O
is	O
it	O
is	O
not	O
used	O
for	O
bootstrapping	B
the	O
value	B
estimate	O
for	O
a	O
state	B
from	O
the	O
estimated	O
values	O
of	O
subsequent	O
states	O
but	O
only	O
as	O
a	O
baseline	B
for	O
the	O
state	B
whose	O
estimate	O
is	O
being	O
updated	O
this	O
is	O
a	O
useful	O
distinction	O
for	O
only	O
through	O
bootstrapping	B
do	O
we	O
introduce	O
bias	O
and	O
an	O
asymptotic	O
dependence	O
on	O
the	O
quality	O
of	O
the	O
function	B
approximation	I
as	O
we	O
have	O
seen	O
the	O
bias	O
introduced	O
through	O
bootstrapping	B
and	O
reliance	O
on	O
the	O
state	B
representation	O
is	O
often	O
beneficial	O
because	O
it	O
reduces	O
variance	O
and	O
accelerates	O
learning	O
reinforce	B
with	B
baseline	B
is	O
unbiased	O
and	O
will	O
converge	O
asymptotically	O
to	O
a	O
local	O
minimum	O
but	O
like	O
all	O
monte	B
carlo	I
methods	I
it	O
tends	O
to	O
learn	O
slowly	O
estimates	O
of	O
high	O
variance	O
and	O
to	O
be	O
inconvenient	O
to	O
implement	O
online	B
or	O
for	O
continuing	O
problems	O
as	O
we	O
have	O
seen	O
earlier	O
in	O
this	O
book	O
with	O
temporal-difference	O
methods	O
we	O
can	O
eliminate	O
these	O
inconveniences	O
and	O
through	O
multi-step	O
methods	O
we	O
can	O
flexibly	O
choose	O
the	O
degree	O
of	O
bootstrapping	B
in	O
order	O
to	O
gain	O
these	O
advantages	O
in	O
the	O
case	O
of	O
policy	B
gradient	B
methods	I
we	O
use	O
actor	O
critic	O
methods	O
with	O
a	O
bootstrapping	B
critic	O
first	O
consider	O
one-step	O
actor	O
critic	O
methods	O
the	O
analog	O
of	O
the	O
td	B
methods	O
introduced	O
in	O
chapter	O
such	O
as	O
and	O
q-learning	B
the	O
main	O
appeal	O
of	O
one-step	O
methods	O
is	O
that	O
they	O
are	O
fully	O
online	B
and	O
incremental	O
yet	O
avoid	O
the	O
complexities	O
of	O
eligibility	B
traces	I
they	O
are	O
a	O
special	O
case	O
of	O
the	O
eligibility	O
trace	O
methods	O
and	O
not	O
as	O
general	O
but	O
easier	O
to	O
understand	O
one-step	O
actor	O
critic	O
methods	O
replace	O
the	O
full	O
return	B
of	O
reinforce	B
with	O
the	O
one-step	O
return	B
use	O
a	O
learned	O
state-value	O
function	O
as	O
the	O
baseline	B
as	O
follows	O
t	O
t	O
t	O
t	O
t	O
t	O
t	O
t	O
t	O
t	O
the	O
natural	O
state-value-function	O
learning	O
method	O
to	O
pair	O
with	O
this	O
is	O
semi-gradient	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
note	O
that	O
it	O
is	O
now	O
a	O
fully	O
online	B
incremental	O
algorithm	O
with	O
states	O
actions	O
and	O
rewards	O
processed	O
as	O
they	O
occur	O
and	O
then	O
never	O
revisited	O
chapter	O
policy	B
gradient	B
methods	I
one-step	O
actor	O
critic	O
for	O
estimating	O
input	O
a	O
differentiable	O
policy	B
parameterization	O
input	O
a	O
differentiable	O
state-value	O
function	O
parameterization	O
vsw	O
parameters	O
step	O
sizes	O
w	O
initialize	O
policy	B
parameter	O
and	O
state-value	O
weights	O
w	O
rd	O
to	O
loop	O
forever	O
each	O
episode	O
initialize	O
s	O
state	B
of	O
episode	O
i	O
loop	O
while	O
s	O
is	O
not	O
terminal	O
each	O
time	O
step	O
a	O
take	O
action	B
a	O
observe	O
r	O
r	O
vsw	O
w	O
w	O
w	O
i	O
vsw	O
i	O
ln	O
i	O
i	O
s	O
is	O
terminal	O
then	O
the	O
generalizations	O
to	O
the	O
forward	O
view	O
of	O
n-step	B
methods	I
and	O
then	O
to	O
a	O
algorithm	O
are	O
straightforward	O
the	O
one-step	O
return	B
in	O
is	O
merely	O
replaced	O
by	O
gttn	O
or	O
g	O
t	O
respectively	O
the	O
backward	O
view	O
of	O
the	O
algorithm	O
is	O
also	O
straightforward	O
using	O
separate	O
eligibility	B
traces	I
for	O
the	O
actor	O
and	O
critic	O
each	O
after	O
the	O
patterns	O
in	O
chapter	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
below	O
actor	O
critic	O
with	B
eligibility	B
traces	I
for	O
estimating	O
input	O
a	O
differentiable	O
policy	B
parameterization	O
input	O
a	O
differentiable	O
state-value	O
function	O
parameterization	O
vsw	O
parameters	O
trace-decay	O
rates	O
w	O
step	O
sizes	O
w	O
initialize	O
policy	B
parameter	O
and	O
state-value	O
weights	O
w	O
rd	O
to	O
loop	O
forever	O
each	O
episode	O
initialize	O
s	O
state	B
of	O
episode	O
z	O
eligibility	O
trace	O
vector	B
zw	O
eligibility	O
trace	O
vector	B
i	O
loop	O
while	O
s	O
is	O
not	O
terminal	O
each	O
time	O
step	O
is	O
terminal	O
then	O
a	O
take	O
action	B
a	O
observe	O
r	O
r	O
vsw	O
zw	O
wzw	O
i	O
vsw	O
z	O
z	O
i	O
ln	O
w	O
w	O
w	O
zw	O
z	O
i	O
i	O
s	O
policy	B
gradient	B
for	O
continuing	O
problems	O
policy	B
gradient	B
for	O
continuing	O
problems	O
as	O
discussed	O
in	O
section	O
for	O
continuing	O
problems	O
without	O
episode	O
boundaries	O
we	O
need	O
to	O
define	O
performance	O
in	O
terms	O
of	O
the	O
average	O
rate	O
of	O
reward	O
per	O
time	O
step	O
j	O
r	O
ert	O
lim	O
h	O
lim	O
t	O
h	O
ert	O
rs	O
ar	O
limt	O
prst	O
where	O
is	O
the	O
steady-state	O
distribution	O
under	O
which	O
is	O
assumed	O
to	O
exist	O
and	O
to	O
be	O
independent	O
of	O
ergodicity	O
assumption	O
remember	O
that	O
this	O
is	O
the	O
special	O
distribution	O
under	O
which	O
if	O
you	O
select	O
actions	O
according	O
to	O
you	O
remain	O
in	O
the	O
same	O
distribution	O
a	O
for	O
all	O
s	O
we	O
also	O
define	O
values	O
v	O
respect	O
to	O
the	O
differential	B
return	B
e	O
s	O
and	O
q	O
a	O
e	O
s	O
at	O
a	O
with	O
gt	O
r	O
r	O
r	O
with	O
these	O
alternate	O
definitions	O
the	O
policy	B
gradient	B
theorem	B
as	O
given	O
for	O
the	O
episodic	O
case	O
remains	O
true	O
for	O
the	O
continuing	O
case	O
a	O
proof	B
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
the	O
forward	O
and	O
backward	O
view	O
equations	O
also	O
remain	O
the	O
same	O
complete	O
pseudocode	O
for	O
the	O
actor	O
critic	O
algorithm	O
in	O
the	O
continuing	O
case	O
view	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
page	O
after	O
next	O
chapter	O
policy	B
gradient	B
methods	I
proof	B
of	O
the	O
policy	B
gradient	B
theorem	B
case	O
the	O
proof	B
of	O
the	O
policy	B
gradient	B
theorem	B
for	O
the	O
continuing	O
case	O
begins	O
similarly	O
to	O
the	O
episodic	O
case	O
again	O
we	O
leave	O
it	O
implicit	O
in	O
all	O
cases	O
that	O
is	O
a	O
function	O
of	O
and	O
that	O
the	O
gradients	O
are	O
with	O
respect	O
to	O
recall	O
that	O
in	O
the	O
continuing	O
case	O
j	O
r	O
and	O
that	O
v	O
and	O
q	O
denote	O
values	O
with	O
respect	O
to	O
the	O
differential	B
return	B
the	O
gradient	B
of	O
the	O
state-value	O
function	O
can	O
be	O
written	O
for	O
any	O
s	O
s	O
as	O
rule	O
of	O
calculus	O
v	O
for	O
all	O
s	O
s	O
a	O
q	O
a	O
a	O
r	O
r	O
a	O
rs	O
r	O
v	O
a	O
v	O
a	O
v	O
v	O
after	O
re-arranging	O
terms	O
we	O
obtain	O
notice	O
that	O
the	O
left-hand	O
side	O
can	O
be	O
written	O
j	O
and	O
that	O
it	O
does	O
not	O
depend	O
on	O
s	O
thus	O
the	O
right-hand	O
side	O
does	O
not	O
depend	O
on	O
s	O
either	O
and	O
we	O
can	O
safely	O
sum	O
it	O
over	O
all	O
s	O
s	O
weighted	O
by	O
without	O
changing	O
it	O
a	O
v	O
v	O
j	O
a	O
a	O
v	O
v	O
a	O
a	O
v	O
a	O
v	O
a	O
a	O
v	O
v	O
q	O
e	O
d	O
policy	B
parameterization	O
for	O
continuous	O
actions	O
actor	O
critic	O
with	B
eligibility	B
traces	I
for	O
estimating	O
input	O
a	O
differentiable	O
policy	B
parameterization	O
input	O
a	O
differentiable	O
state-value	O
function	O
parameterization	O
vsw	O
algorithm	O
parameters	O
w	O
w	O
r	O
initialize	O
r	O
r	O
to	O
initialize	O
state-value	O
weights	O
w	O
rd	O
and	O
policy	B
parameter	O
to	O
initialize	O
s	O
s	O
to	O
zw	O
eligibility	O
trace	O
vector	B
z	O
eligibility	O
trace	O
vector	B
loop	O
forever	O
each	O
time	O
step	O
a	O
take	O
action	B
a	O
observe	O
r	O
r	O
r	O
vsw	O
r	O
r	O
r	O
zw	O
wzw	O
vsw	O
z	O
z	O
ln	O
w	O
w	O
w	O
zw	O
z	O
s	O
policy	B
parameterization	O
for	O
continuous	O
actions	O
policy-based	O
methods	O
offer	O
practical	O
ways	O
of	O
dealing	O
with	O
large	O
actions	O
spaces	O
even	O
continuous	O
spaces	O
with	O
an	O
infinite	O
number	O
of	O
actions	O
instead	O
of	O
computing	O
learned	O
probabilities	O
for	O
each	O
of	O
the	O
many	O
actions	O
we	O
instead	O
learn	O
statistics	O
of	O
the	O
probability	O
distribution	O
for	O
example	O
the	O
action	B
set	O
might	O
be	O
the	O
real	O
numbers	O
with	O
actions	O
chosen	O
from	O
a	O
normal	O
distribution	O
the	O
probability	O
density	O
function	O
for	O
the	O
normal	O
distribution	O
is	O
conventionally	O
written	O
px	O
where	O
and	O
here	O
are	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
normal	O
distribution	O
and	O
of	O
course	O
here	O
is	O
just	O
the	O
number	O
the	O
probability	O
density	O
functions	O
for	O
several	O
different	O
means	O
and	O
standard	O
deviations	O
are	O
shown	O
in	O
figure	O
the	O
value	B
px	O
is	O
the	O
density	O
of	O
the	O
probability	O
at	O
x	O
not	O
the	O
probability	O
it	O
can	O
be	O
greater	O
than	O
it	O
is	O
the	O
total	O
area	O
under	O
px	O
that	O
must	O
sum	O
to	O
in	O
general	O
one	O
can	O
take	O
the	O
integral	O
under	O
px	O
for	O
any	O
range	O
of	O
x	O
values	O
to	O
get	O
the	O
probability	O
of	O
x	O
falling	O
within	O
that	O
range	O
to	O
produce	O
a	O
policy	B
parameterization	O
the	O
policy	B
can	O
be	O
defined	O
as	O
the	O
normal	O
probability	O
density	O
over	O
a	O
real-valued	O
scalar	O
action	B
with	O
mean	O
and	O
standard	O
deviation	O
given	O
chapter	O
policy	B
gradient	B
methods	I
figure	O
the	O
probability	O
density	O
function	O
of	O
the	O
normal	O
distribution	O
for	O
different	O
means	O
and	O
variances	O
by	O
parametric	O
function	O
approximators	O
that	O
depend	O
on	O
the	O
state	B
that	O
is	O
where	O
s	O
r	O
and	O
s	O
r	O
are	O
two	O
parameterized	O
function	O
approximators	O
to	O
complete	O
the	O
example	O
we	O
need	O
only	O
give	O
a	O
form	O
for	O
these	O
approximators	O
for	O
this	O
we	O
divide	O
the	O
policy	B
s	O
parameter	O
vector	B
into	O
two	O
parts	O
one	O
part	O
to	O
be	O
used	O
for	O
the	O
approximation	O
of	O
the	O
mean	O
and	O
one	O
part	O
for	O
the	O
approximation	O
of	O
the	O
standard	O
deviation	O
the	O
mean	O
can	O
be	O
approximated	O
as	O
a	O
linear	O
function	O
the	O
standard	O
deviation	O
must	O
always	O
be	O
positive	O
and	O
is	O
better	O
approximated	O
as	O
the	O
exponential	O
of	O
a	O
linear	O
function	O
thus	O
and	O
where	O
x	O
and	O
x	O
are	O
state	B
feature	O
vectors	O
perhaps	O
constructed	O
by	O
one	O
of	O
the	O
methods	O
described	O
in	O
chapter	O
with	O
these	O
definitions	O
all	O
the	O
algorithms	O
described	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
can	O
be	O
applied	O
to	O
learn	O
to	O
select	O
real-valued	O
actions	O
exercise	O
show	O
that	O
for	O
the	O
gaussian	O
policy	B
parameterization	O
the	O
eligibility	O
vector	B
has	O
the	O
following	O
two	O
parts	O
ln	O
ln	O
and	O
x	O
exp	O
exp	O
summary	O
exercise	O
a	O
bernoulli-logistic	O
unit	O
is	O
a	O
stochastic	O
neuron-like	O
unit	O
used	O
in	O
some	O
artificial	B
neural	B
networks	I
its	O
input	O
at	O
time	O
t	O
is	O
a	O
feature	O
vector	B
xst	O
its	O
output	O
at	O
is	O
a	O
random	O
variable	O
having	O
two	O
values	O
and	O
with	O
prat	O
pt	O
and	O
prat	O
pt	O
bernoulli	O
distribution	O
let	O
hs	O
and	O
hs	O
be	O
the	O
preferences	O
in	O
state	B
s	O
for	O
the	O
unit	O
s	O
two	O
actions	O
given	O
policy	B
parameter	O
assume	O
that	O
the	O
difference	O
between	O
the	O
action	B
preferences	I
is	O
given	O
by	O
a	O
weighted	O
sum	O
of	O
the	O
unit	O
s	O
input	O
vector	B
that	O
is	O
assume	O
that	O
hs	O
hs	O
where	O
is	O
the	O
unit	O
s	O
weight	O
vector	B
show	O
that	O
if	O
the	O
exponential	O
soft-max	B
distribution	O
is	O
used	O
to	O
convert	O
action	B
preferences	I
to	O
policies	O
then	O
pt	O
t	O
exp	O
xst	O
logistic	O
function	O
what	O
is	O
the	O
monte-carlo	O
reinforce	B
update	O
of	O
t	O
to	O
upon	O
receipt	O
of	O
return	B
gt	O
express	O
the	O
eligibility	O
ln	O
for	O
a	O
bernoulli-logistic	O
unit	O
in	O
terms	O
of	O
a	O
xs	O
and	O
by	O
calculating	O
the	O
gradient	B
hint	O
separately	O
for	O
each	O
action	B
compute	O
the	O
derivative	O
of	O
the	O
logarithm	O
first	O
with	O
respect	O
to	O
pt	O
t	O
combine	O
the	O
two	O
results	O
into	O
one	O
expression	O
that	O
depends	O
on	O
a	O
and	O
pt	O
and	O
then	O
use	O
the	O
chain	O
rule	O
noting	O
that	O
the	O
derivative	O
of	O
the	O
logistic	O
function	O
f	O
is	O
f	O
f	O
summary	O
prior	O
to	O
this	O
chapter	O
this	O
book	O
focused	O
on	O
action-value	B
methods	I
meaning	O
methods	O
that	O
learn	O
action	B
values	O
and	O
then	O
use	O
them	O
to	O
determine	O
action	B
selections	O
in	O
this	O
chapter	O
on	O
the	O
other	O
hand	O
we	O
considered	O
methods	O
that	O
learn	O
a	O
parameterized	O
policy	B
that	O
enables	O
actions	O
to	O
be	O
taken	O
without	O
consulting	O
action-value	O
estimates	O
in	O
particular	O
we	O
have	O
considered	O
policy-gradient	O
methods	O
meaning	O
methods	O
that	O
update	O
the	O
policy	B
parameter	O
on	O
each	O
step	O
in	O
the	O
direction	O
of	O
an	O
estimate	O
of	O
the	O
gradient	B
of	O
performance	O
with	O
respect	O
to	O
the	O
policy	B
parameter	O
methods	O
that	O
learn	O
and	O
store	O
a	O
policy	B
parameter	O
have	O
many	O
advantages	O
they	O
can	O
learn	O
specific	O
probabilities	O
for	O
taking	O
the	O
actions	O
they	O
can	O
learn	O
appropriate	O
levels	O
of	O
exploration	O
and	O
approach	O
deterministic	O
policies	O
asymptotically	O
they	O
can	O
naturally	O
handle	O
continuous	B
action	B
spaces	O
all	O
these	O
things	O
are	O
easy	O
for	O
policy-based	O
methods	O
but	O
awkward	O
or	O
impossible	O
for	O
methods	O
and	O
for	O
action-value	B
methods	I
in	O
general	O
in	O
addition	O
on	O
some	O
problems	O
the	O
policy	B
is	O
just	O
simpler	O
to	O
represent	O
parametrically	O
than	O
the	O
value	B
function	I
these	O
problems	O
are	O
more	O
suited	O
to	O
parameterized	O
policy	B
methods	O
parameterized	O
policy	B
methods	O
also	O
have	O
an	O
important	O
theoretical	O
advantage	O
over	O
action-value	B
methods	I
in	O
the	O
form	O
of	O
the	O
policy	B
gradient	B
theorem	B
which	O
gives	O
an	O
exact	O
formula	O
for	O
how	O
performance	O
is	O
affected	O
by	O
the	O
policy	B
parameter	O
that	O
does	O
not	O
involve	O
derivatives	O
of	O
the	O
state	B
distribution	O
this	O
theorem	B
provides	O
a	O
theoretical	O
foundation	O
for	O
all	O
policy	B
gradient	B
methods	I
chapter	O
policy	B
gradient	B
methods	I
the	O
reinforce	B
method	O
follows	O
directly	O
from	O
the	O
policy	B
gradient	B
theorem	B
adding	O
a	O
state-value	O
function	O
as	O
a	O
baseline	B
reduces	O
reinforce	B
s	O
variance	O
without	O
introducing	O
bias	O
using	O
the	O
state-value	O
function	O
for	O
bootstrapping	B
introduces	O
bias	O
but	O
is	O
often	O
desirable	O
for	O
the	O
same	O
reason	O
that	O
bootstrapping	B
td	B
methods	O
are	O
often	O
superior	O
to	O
monte	B
carlo	I
methods	I
reduced	O
variance	O
the	O
state-value	O
function	O
assigns	O
credit	O
to	O
critizes	O
the	O
policy	B
s	O
action	B
selections	O
and	O
accordingly	O
the	O
former	O
is	O
termed	O
the	O
critic	O
and	O
the	O
latter	O
the	O
actor	O
and	O
these	O
overall	O
methods	O
are	O
termed	O
actor	O
critic	O
methods	O
overall	O
policy-gradient	O
methods	O
provide	O
a	O
significantly	O
different	O
set	O
of	O
strengths	O
and	O
weaknesses	O
than	O
action-value	B
methods	I
today	O
they	O
are	O
less	O
well	O
understood	O
in	O
some	O
respects	O
but	O
a	O
subject	O
of	O
excitement	O
and	O
ongoing	O
research	O
bibliographical	O
and	O
historical	O
remarks	O
methods	O
that	O
we	O
now	O
see	O
as	O
related	O
to	O
policy	B
gradients	O
were	O
actually	O
some	O
of	O
the	O
earliest	O
to	O
be	O
studied	O
in	O
reinforcement	B
learning	I
barto	O
sutton	O
and	O
anderson	O
sutton	O
williams	O
and	O
in	O
predecessor	O
fields	O
and	O
thathachar	O
they	O
were	O
largely	O
supplanted	O
in	O
the	O
by	O
the	O
action-value	B
methods	I
that	O
are	O
the	O
focus	O
of	O
the	O
other	O
chapters	O
of	O
this	O
book	O
in	O
recent	O
years	O
however	O
attention	O
has	O
returned	O
to	O
actor	O
critic	O
methods	O
and	O
to	O
policy-gradient	O
methods	O
in	O
general	O
among	O
the	O
further	O
developments	O
beyond	O
what	O
we	O
cover	O
here	O
are	O
natural-gradient	O
methods	O
kakade	O
peters	O
vijayakumar	O
and	O
schaal	O
peters	O
and	O
schall	O
park	O
kim	O
and	O
kang	O
bhatnagar	O
sutton	O
ghavamzadeh	O
and	O
lee	O
see	O
grondman	O
busoniu	O
lopes	O
and	O
babuska	O
and	O
deterministic	O
policy	B
gradient	B
et	O
al	O
major	O
applications	O
include	O
acrobatic	O
helicopter	O
autopilots	O
and	O
alphago	B
section	O
our	O
presentation	O
in	O
this	O
chapter	O
is	O
based	O
primarily	O
on	O
that	O
by	O
sutton	O
mcallester	O
singh	O
and	O
mansour	O
see	O
also	O
sutton	O
singh	O
and	O
mcallester	O
who	O
introduced	O
the	O
term	O
policy	B
gradient	B
methods	I
a	O
useful	O
overview	O
is	O
provided	O
by	O
bhatnagar	O
et	O
al	O
one	O
of	O
the	O
earliest	O
related	O
works	O
is	O
by	O
aleksandrov	O
sysoyev	O
and	O
shemeneva	O
thomas	O
first	O
realized	O
that	O
the	O
factor	O
of	O
t	O
as	O
specified	O
in	O
the	O
boxed	O
algorithms	O
of	O
this	O
chapter	O
was	O
needed	O
in	O
the	O
case	O
of	O
discounted	O
episodic	O
problems	O
example	O
and	O
the	O
results	O
with	O
it	O
in	O
this	O
chapter	O
were	O
developed	O
with	O
eric	O
graves	O
the	O
policy	B
gradient	B
theorem	B
here	O
and	O
on	O
page	O
was	O
first	O
obtained	O
by	O
marbach	O
and	O
tsitsiklis	O
and	O
then	O
independently	O
by	O
sutton	O
et	O
al	O
a	O
similar	O
expression	O
was	O
obtained	O
by	O
cao	O
and	O
chen	O
other	O
early	O
results	O
are	O
due	O
to	O
konda	O
and	O
tsitsiklis	O
baxter	O
and	O
bartlett	O
and	O
baxter	O
bartlett	O
and	O
weaver	O
some	O
additional	O
results	O
are	O
developed	O
by	O
sutton	O
singh	O
and	O
mcallester	O
reinforce	B
is	O
due	O
to	O
williams	O
the	O
use	O
of	O
a	O
power	O
of	O
the	O
discount	O
factor	O
in	O
the	O
update	O
in	O
the	O
boxed	O
algorithms	O
is	O
due	O
to	O
thomas	O
summary	O
phansalkar	O
and	O
thathachar	O
proved	O
both	O
local	O
and	O
global	O
convergence	O
theorems	O
for	O
modified	O
versions	O
of	O
reinforce	B
algorithms	O
the	O
baseline	B
was	O
introduced	O
in	O
williams	O
s	O
original	O
work	O
greensmith	O
bartlett	O
and	O
baxter	O
analyzed	O
an	O
arguably	O
better	O
baseline	B
dick	O
actor	O
critic	O
methods	O
were	O
among	O
the	O
earliest	O
to	O
be	O
investigated	O
in	O
reinforcement	B
learning	I
barto	O
sutton	O
and	O
anderson	O
sutton	O
the	O
algorithms	O
presented	O
here	O
and	O
in	O
section	O
are	O
based	O
on	O
the	O
work	O
of	O
degris	O
white	O
and	O
sutton	O
who	O
also	O
introduced	O
the	O
study	O
of	O
off-policy	B
policygradient	O
methods	O
the	O
first	O
to	O
show	O
how	O
continuous	O
actions	O
could	O
be	O
handled	O
this	O
way	O
appears	O
to	O
have	O
been	O
williams	O
figure	O
is	O
adapted	O
from	O
wikipedia	O
part	O
iii	O
looking	O
deeper	O
in	O
this	O
last	O
part	O
of	O
the	O
book	O
we	O
look	O
beyond	O
the	O
standard	O
reinforcement	B
learning	I
ideas	O
presented	O
in	O
the	O
first	O
two	O
parts	O
of	O
the	O
book	O
to	O
briefly	O
survey	O
their	O
relationships	O
with	O
psychology	B
and	O
neuroscience	B
a	O
sampling	O
of	O
reinforcement	B
learning	I
applications	O
and	O
some	O
of	O
the	O
active	O
frontiers	O
for	O
future	O
reinforcement	B
learning	I
research	O
chapter	O
psychology	B
in	O
previous	O
chapters	O
we	O
developed	O
ideas	O
for	O
algorithms	O
based	O
on	O
computational	O
considerations	O
alone	O
in	O
this	O
chapter	O
we	O
look	O
at	O
some	O
of	O
these	O
algorithms	O
from	O
another	O
perspective	O
the	O
perspective	O
of	O
psychology	B
and	O
its	O
study	O
of	O
how	O
animals	O
learn	O
the	O
goals	O
of	O
this	O
chapter	O
are	O
first	O
to	O
discuss	O
ways	O
that	O
reinforcement	B
learning	I
ideas	O
and	O
algorithms	O
correspond	O
to	O
what	O
psychologists	O
have	O
discovered	O
about	O
animal	O
learning	O
and	O
second	O
to	O
explain	O
the	O
influence	O
reinforcement	B
learning	I
is	O
having	O
on	O
the	O
study	O
of	O
animal	O
learning	O
the	O
clear	O
formalism	O
provided	O
by	O
reinforcement	B
learning	I
that	O
systemizes	O
tasks	O
returns	O
and	O
algorithms	O
is	O
proving	O
to	O
be	O
enormously	O
useful	O
in	O
making	O
sense	O
of	O
experimental	O
data	O
in	O
suggesting	O
new	O
kinds	O
of	O
experiments	O
and	O
in	O
pointing	O
to	O
factors	O
that	O
may	O
be	O
critical	O
to	O
manipulate	O
and	O
to	O
measure	O
the	O
idea	O
of	O
optimizing	O
return	B
over	O
the	O
long	O
term	O
that	O
is	O
at	O
the	O
core	O
of	O
reinforcement	B
learning	I
is	O
contributing	O
to	O
our	O
understanding	O
of	O
otherwise	O
puzzling	O
features	O
of	O
animal	O
learning	O
and	O
behavior	O
some	O
of	O
the	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
psychological	O
theories	O
are	O
not	O
surprising	O
because	O
the	O
development	O
of	O
reinforcement	B
learning	I
drew	O
inspiration	O
from	O
psychological	O
learning	O
theories	O
however	O
as	O
developed	O
in	O
this	O
book	O
reinforcement	B
learning	I
explores	O
idealized	O
situations	O
from	O
the	O
perspective	O
of	O
an	O
artificial	B
intelligence	I
researcher	O
or	O
engineer	O
with	O
the	O
goal	O
of	O
solving	O
computational	O
problems	O
with	O
efficient	O
algorithms	O
rather	O
than	O
to	O
replicate	O
or	O
explain	O
in	O
detail	O
how	O
animals	O
learn	O
as	O
a	O
result	O
some	O
of	O
the	O
correspondences	O
we	O
describe	O
connect	O
ideas	O
that	O
arose	O
independently	O
in	O
their	O
respective	O
fields	O
we	O
believe	O
these	O
points	O
of	O
contact	O
are	O
specially	O
meaningful	O
because	O
they	O
expose	O
computational	O
principles	O
important	O
to	O
learning	O
whether	O
it	O
is	O
learning	O
by	O
artificial	O
or	O
by	O
natural	O
systems	O
for	O
the	O
most	O
part	O
we	O
describe	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
learning	O
theories	O
developed	O
to	O
explain	O
how	O
animals	O
like	O
rats	O
pigeons	O
and	O
rabbits	O
learn	O
in	O
controlled	O
laboratory	O
experiments	O
thousands	O
of	O
these	O
experiments	O
were	O
conducted	O
throughout	O
the	O
century	O
and	O
many	O
are	O
still	O
being	O
conducted	O
today	O
although	O
sometimes	O
dismissed	O
as	O
irrelevant	O
to	O
wider	O
issues	O
in	B
psychology	B
these	O
experiments	O
probe	O
subtle	O
properties	O
of	O
animal	O
learning	O
often	O
motivated	O
by	O
precise	O
theoretical	O
questions	O
as	O
psychology	B
shifted	O
its	O
focus	O
to	O
more	O
cognitive	O
aspects	O
of	O
behavior	O
that	O
is	O
to	O
mental	O
processes	O
such	O
as	O
thought	O
and	O
reasoning	O
animal	O
learning	O
experiments	O
came	O
to	O
play	O
less	O
chapter	O
psychology	B
of	O
a	O
role	O
in	B
psychology	B
than	O
they	O
once	O
did	O
but	O
this	O
experimentation	O
led	O
to	O
the	O
discovery	O
of	O
learning	O
principles	O
that	O
are	O
elemental	O
and	O
widespread	O
throughout	O
the	O
animal	O
kingdom	O
principles	O
that	O
should	O
not	O
be	O
neglected	O
in	O
designing	O
artificial	O
learning	O
systems	O
in	O
addition	O
as	O
we	O
shall	O
see	O
some	O
aspects	O
of	O
cognitive	O
processing	O
connect	O
naturally	O
to	O
the	O
computational	O
perspective	O
provided	O
by	O
reinforcement	B
learning	I
this	O
chapter	O
s	O
final	O
section	O
includes	O
references	O
relevant	O
to	O
the	O
connections	O
we	O
discuss	O
as	O
well	O
as	O
to	O
connections	O
we	O
neglect	O
we	O
hope	O
this	O
chapter	O
encourages	O
readers	O
to	O
probe	O
all	O
of	O
these	O
connections	O
more	O
deeply	O
also	O
included	O
in	O
this	O
final	O
section	O
is	O
a	O
discussion	O
of	O
how	O
the	O
terminology	O
used	O
in	O
reinforcement	B
learning	I
relates	O
to	O
that	O
of	O
psychology	B
many	O
of	O
the	O
terms	O
and	O
phrases	O
used	O
in	O
reinforcement	B
learning	I
are	O
borrowed	O
from	O
animal	O
learning	O
theories	O
but	O
the	O
computationalengineering	O
meanings	O
of	O
these	O
terms	O
and	O
phrases	O
do	O
not	O
always	O
coincide	O
with	O
their	O
meanings	O
in	B
psychology	B
prediction	B
and	B
control	B
the	O
algorithms	O
we	O
describe	O
in	O
this	O
book	O
fall	O
into	O
two	O
broad	O
categories	O
algorithms	O
for	O
prediction	B
and	O
algorithms	O
for	O
control	B
these	O
categories	O
arise	O
naturally	O
in	O
solution	O
methods	O
for	O
the	O
reinforcement	B
learning	I
problem	O
presented	O
in	O
chapter	O
in	O
many	O
ways	O
these	O
categories	O
respectively	O
correspond	O
to	O
categories	O
of	O
learning	O
extensively	O
studied	O
by	O
psychologists	O
classical	O
or	O
pavlovian	O
conditioning	O
and	O
instrumental	O
or	O
operant	O
conditioning	O
these	O
correspondences	O
are	O
not	O
completely	O
accidental	O
because	O
of	O
psychology	B
s	O
influence	O
on	O
reinforcement	B
learning	I
but	O
they	O
are	O
nevertheless	O
striking	O
because	O
they	O
connect	O
ideas	O
arising	O
from	O
different	O
objectives	O
the	O
prediction	B
algorithms	O
presented	O
in	O
this	O
book	O
estimate	O
quantities	O
that	O
depend	O
on	O
how	O
features	O
of	O
an	O
agent	O
s	O
environment	B
are	O
expected	B
to	O
unfold	O
over	O
the	O
future	O
we	O
specifically	O
focus	O
on	O
estimating	O
the	O
amount	O
of	O
reward	O
an	O
agent	O
can	O
expect	O
to	O
receive	O
over	O
the	O
future	O
while	O
it	O
interacts	O
with	O
its	O
environment	B
in	O
this	O
role	O
prediction	B
algorithms	O
are	O
policy	B
evaluation	O
algorithms	O
which	O
are	O
integral	O
components	O
of	O
algorithms	O
for	O
improving	O
policies	O
but	O
prediction	B
algorithms	O
are	O
not	O
limited	O
to	O
predicting	O
future	O
reward	O
they	O
can	O
predict	O
any	O
feature	O
of	O
the	O
environment	B
for	O
example	O
modayil	O
white	O
and	O
sutton	O
the	O
correspondence	O
between	O
prediction	B
algorithms	O
and	O
classical	B
conditioning	I
rests	O
on	O
their	O
common	O
property	O
of	O
predicting	O
upcoming	O
stimuli	O
whether	O
or	O
not	O
those	O
stimuli	O
are	O
rewarding	O
punishing	O
the	O
situation	O
in	O
an	O
instrumental	O
or	O
operant	O
conditioning	O
experiment	O
is	O
different	O
here	O
the	O
experimental	O
apparatus	O
is	O
set	O
up	O
so	O
that	O
an	O
animal	O
is	O
given	O
something	O
it	O
likes	O
reward	O
or	O
something	O
it	O
dislikes	O
penalty	O
depending	O
on	O
what	O
the	O
animal	O
did	O
the	O
animal	O
learns	O
to	O
increase	O
its	O
tendency	O
to	O
produce	O
rewarded	O
behavior	O
and	O
to	O
decrease	O
its	O
tendency	O
to	O
produce	O
penalized	O
behavior	O
the	O
reinforcing	O
stimulus	O
is	O
said	O
to	O
be	O
contingent	O
on	O
the	O
animal	O
s	O
behavior	O
whereas	O
in	O
classical	B
conditioning	I
it	O
is	O
not	O
it	O
is	O
difficult	O
to	O
remove	O
all	O
behavior	O
contingencies	O
in	O
a	O
classical	B
conditioning	I
experiment	O
instrumental	O
conditioning	O
experiments	O
are	O
like	O
those	O
that	O
inspired	O
thorndike	O
s	O
law	B
of	I
effect	I
that	O
we	O
briefly	O
discuss	O
in	O
chapter	O
control	B
is	O
at	O
the	O
core	O
of	O
this	O
form	O
of	O
learning	O
which	O
corresponds	O
to	O
the	O
operation	O
of	O
reinforcement	B
learning	I
s	O
policy-improvement	O
classical	B
conditioning	I
thinking	O
of	O
classical	B
conditioning	I
in	O
terms	O
of	O
prediction	B
and	O
instrumental	O
conditioning	O
in	O
terms	O
of	O
control	B
is	O
a	O
starting	O
point	O
for	O
connecting	O
our	O
computational	O
view	O
of	O
reinforcement	B
learning	I
to	O
animal	O
learning	O
but	O
in	O
reality	O
the	O
situation	O
is	O
more	O
complicated	O
than	O
this	O
there	O
is	O
more	O
to	O
classical	B
conditioning	I
than	O
prediction	B
it	O
also	O
involves	O
action	B
and	O
so	O
is	O
a	O
mode	O
of	O
control	B
sometimes	O
called	O
pavlovian	O
control	B
further	O
classical	O
and	O
instrumental	O
conditioning	O
interact	O
in	O
interesting	O
ways	O
with	O
both	O
sorts	O
of	O
learning	O
likely	O
being	O
engaged	O
in	O
most	O
experimental	O
situations	O
despite	O
these	O
complications	O
aligning	O
the	O
classicalinstrumental	O
distinction	O
with	O
the	O
predictioncontrol	O
distinction	O
is	O
a	O
convenient	O
first	O
approximation	O
in	O
connecting	O
reinforcement	B
learning	I
to	O
animal	O
learning	O
in	B
psychology	B
the	O
term	O
reinforcement	O
is	O
used	O
to	O
describe	O
learning	O
in	O
both	O
classical	O
and	O
instrumental	O
conditioning	O
originally	O
referring	O
only	O
to	O
the	O
strengthening	O
of	O
a	O
pattern	O
of	O
behavior	O
it	O
is	O
frequently	O
also	O
used	O
for	O
the	O
weakening	O
of	O
a	O
pattern	O
of	O
behavior	O
a	O
stimulus	O
considered	O
to	O
be	O
the	O
cause	O
of	O
the	O
change	O
in	O
behavior	O
is	O
called	O
a	O
reinforcer	O
whether	O
or	O
not	O
it	O
is	O
contingent	O
on	O
the	O
animal	O
s	O
previous	O
behavior	O
at	O
the	O
end	O
of	O
this	O
chapter	O
we	O
discuss	O
this	O
terminology	O
in	O
more	O
detail	O
and	O
how	O
it	O
relates	O
to	O
terminology	O
used	O
in	O
machine	O
learning	O
classical	B
conditioning	I
while	O
studying	O
the	O
activity	O
of	O
the	O
digestive	O
system	O
the	O
celebrated	O
russian	O
physiologist	O
ivan	O
pavlov	B
found	O
that	O
an	O
animal	O
s	O
innate	O
responses	O
to	O
certain	O
triggering	O
stimuli	O
can	O
come	O
to	O
be	O
triggered	O
by	O
other	O
stimuli	O
that	O
are	O
quite	O
unrelated	O
to	O
the	O
inborn	O
triggers	O
his	O
experimental	O
subjects	O
were	O
dogs	O
that	O
had	O
undergone	O
minor	O
surgery	O
to	O
allow	O
the	O
intensity	O
of	O
their	O
salivary	O
reflex	O
to	O
be	O
accurately	O
measured	O
in	O
one	O
case	O
he	O
describes	O
the	O
dog	O
did	O
not	O
salivate	O
under	O
most	O
circumstances	O
but	O
about	O
seconds	O
after	O
being	O
presented	O
with	O
food	O
it	O
produced	O
about	O
six	O
drops	O
of	O
saliva	O
over	O
the	O
next	O
several	O
seconds	O
after	O
several	O
repetitions	O
of	O
presenting	O
another	O
stimulus	O
one	O
not	O
related	O
to	O
food	O
in	O
this	O
case	O
the	O
sound	O
of	O
a	O
metronome	O
shortly	O
before	O
the	O
introduction	O
of	O
food	O
the	O
dog	O
salivated	O
in	O
response	O
to	O
the	O
sound	O
of	O
the	O
metronome	O
in	O
the	O
same	O
way	O
it	O
did	O
to	O
the	O
food	O
the	O
activity	O
of	O
the	O
salivary	O
gland	O
has	O
thus	O
been	O
called	O
into	O
play	O
by	O
impulses	O
of	O
sound	O
a	O
stimulus	O
quite	O
alien	O
to	O
food	O
p	O
summarizing	O
the	O
significance	O
of	O
this	O
finding	O
pavlov	B
wrote	O
it	O
is	O
pretty	O
evident	O
that	O
under	O
natural	O
conditions	O
the	O
normal	O
animal	O
must	O
respond	O
not	O
only	O
to	O
stimuli	O
which	O
themselves	O
bring	O
immediate	O
benefit	O
or	O
harm	O
but	O
also	O
to	O
other	O
physical	O
or	O
chemical	O
agencies	O
waves	O
of	O
sound	O
light	O
and	O
the	O
like	O
which	O
in	O
themselves	O
only	O
signal	O
the	O
approach	O
of	O
these	O
stimuli	O
though	O
it	O
is	O
not	O
the	O
sight	O
and	O
sound	O
of	O
the	O
beast	O
of	O
prey	O
which	O
is	O
in	O
itself	O
harmful	O
to	O
the	O
smaller	O
animal	O
but	O
its	O
teeth	O
and	O
claws	O
p	O
control	B
means	O
for	O
us	O
is	O
different	O
from	O
what	O
it	O
typically	O
means	O
in	B
animal	I
learning	I
theories	O
there	O
the	O
environment	B
controls	O
the	O
agent	O
instead	O
of	O
the	O
other	O
way	O
around	O
see	O
our	O
comments	O
on	O
terminology	O
at	O
the	O
end	O
of	O
this	O
chapter	O
chapter	O
psychology	B
connecting	O
new	O
stimuli	O
to	O
innate	O
reflexes	O
in	O
this	O
way	O
is	O
now	O
called	O
classical	O
or	O
pavlovian	O
conditioning	O
pavlov	B
more	O
exactly	O
his	O
translators	O
called	O
inborn	O
responses	O
salivation	O
in	O
his	O
demonstration	O
described	O
above	O
unconditioned	O
responses	O
their	O
natural	O
triggering	O
stimuli	O
food	O
unconditioned	O
stimuli	O
and	O
new	O
responses	O
triggered	O
by	O
predictive	O
stimuli	O
here	O
also	O
salivation	O
conditioned	O
responses	O
a	O
stimulus	O
that	O
is	O
initially	O
neutral	O
meaning	O
that	O
it	O
does	O
not	O
normally	O
elicit	O
strong	O
responses	O
the	O
metronome	O
sound	O
becomes	O
a	O
conditioned	O
stimulus	O
as	O
the	O
animal	O
learns	O
that	O
it	O
predicts	O
the	O
us	O
and	O
so	O
comes	O
to	O
produce	O
a	O
cr	O
in	O
response	O
to	O
the	O
cs	O
these	O
terms	O
are	O
still	O
used	O
in	O
describing	O
classical	B
conditioning	I
experiments	O
better	O
translations	O
would	O
have	O
been	O
conditional	O
and	O
unconditional	O
instead	O
of	O
conditioned	O
and	O
unconditioned	O
the	O
us	O
is	O
called	O
a	O
reinforcer	O
because	O
it	O
reinforces	O
producing	O
a	O
cr	O
in	O
response	O
to	O
the	O
cs	O
the	O
arrangement	O
of	O
stimuli	O
in	O
two	O
common	O
types	O
of	O
classical	B
conditioning	I
experiments	O
is	O
shown	O
to	O
the	O
right	O
in	O
delay	O
conditioning	O
the	O
cs	O
extends	O
throughout	O
the	O
interstimulus	O
interval	O
or	O
isi	O
which	O
is	O
the	O
time	O
interval	O
between	O
the	O
cs	O
onset	O
and	O
the	O
us	O
onset	O
the	O
cs	O
ending	O
when	O
the	O
us	O
ends	O
in	O
a	O
common	O
version	O
shown	O
here	O
in	O
trace	O
conditioning	O
the	O
us	O
begins	O
after	O
the	O
cs	O
ends	O
and	O
the	O
time	O
interval	O
between	O
cs	O
offset	O
and	O
us	O
onset	O
is	O
called	O
the	O
trace	O
interval	O
the	O
salivation	O
of	O
pavlov	B
s	O
dogs	O
to	O
the	O
sound	O
of	O
a	O
metronome	O
is	O
just	O
one	O
example	O
of	O
classical	B
conditioning	I
which	O
has	O
been	O
intensively	O
studied	O
across	O
many	O
response	O
systems	O
of	O
many	O
species	O
of	O
animals	O
urs	O
are	O
often	O
preparatory	O
in	O
some	O
way	O
like	O
the	O
salivation	O
of	O
pavlov	B
s	O
dog	O
or	O
protective	O
in	O
some	O
way	O
like	O
an	O
eye	O
blink	O
in	O
response	O
to	O
something	O
irritating	O
to	O
the	O
eye	O
or	O
freezing	O
in	O
response	O
to	O
seeing	O
a	O
predator	O
experiencing	O
the	O
cs-us	O
predictive	O
relationship	O
over	O
a	O
series	O
of	O
trials	O
causes	O
the	O
animal	O
to	O
learn	O
that	O
the	O
cs	O
predicts	O
the	O
us	O
so	O
that	O
the	O
animal	O
can	O
respond	O
to	O
the	O
cs	O
with	O
a	O
cr	O
that	O
prepares	O
the	O
animal	O
for	O
or	O
protects	O
it	O
from	O
the	O
predicted	O
us	O
some	O
crs	O
are	O
similar	O
to	O
the	O
ur	O
but	O
begin	O
earlier	O
and	O
differ	O
in	O
ways	O
that	O
increase	O
their	O
effectiveness	O
in	O
one	O
intensively	O
studied	O
type	O
of	O
experiment	O
for	O
example	O
a	O
tone	O
cs	O
reliably	O
predicts	O
a	O
puff	O
of	O
air	O
us	O
to	O
a	O
rabbit	O
s	O
eye	O
triggering	O
a	O
ur	O
consisting	O
of	O
the	O
closure	O
of	O
a	O
protective	O
inner	O
eyelid	O
called	O
the	O
nictitating	O
membrane	O
after	O
one	O
or	O
more	O
trials	O
the	O
tone	O
comes	O
to	O
trigger	O
a	O
cr	O
consisting	O
of	O
membrane	O
closure	O
that	O
begins	O
before	O
the	O
air	O
puff	O
and	O
eventually	O
becomes	O
timed	O
so	O
that	O
peak	O
closure	O
occurs	O
just	O
when	O
the	O
air	O
puff	O
is	O
likely	O
to	O
occur	O
this	O
cr	O
being	O
initiated	O
in	O
anticipation	O
of	O
the	O
air	O
puff	O
and	O
appropriately	O
timed	O
offers	O
better	O
protection	O
than	O
simply	O
initiating	O
closure	O
as	O
a	O
reaction	O
to	O
the	O
irritating	O
us	O
the	O
ability	O
to	O
act	O
in	O
anticipation	O
of	O
important	O
events	O
by	O
learning	O
about	O
predictive	O
relationships	O
among	O
stimuli	O
is	O
so	O
beneficial	O
that	O
it	O
is	O
widely	O
present	O
across	O
the	O
animal	O
kingdom	O
ttrace	O
conditioningdelay	O
conditioningcsuscsusisi	O
classical	B
conditioning	I
blocking	B
and	B
higher-order	I
conditioning	I
many	O
interesting	O
properties	O
of	O
classical	B
conditioning	I
have	O
been	O
observed	O
in	O
experiments	O
beyond	O
the	O
anticipatory	O
nature	O
of	O
crs	O
two	O
widely	O
observed	O
properties	O
figured	O
prominently	O
in	O
the	O
development	O
of	O
classical	B
conditioning	I
models	O
blocking	B
and	B
higher-order	I
conditioning	I
blocking	B
occurs	O
when	O
an	O
animal	O
fails	O
to	O
learn	O
a	O
cr	O
when	O
a	O
potential	O
cs	O
is	O
presented	O
along	O
with	O
another	O
cs	O
that	O
had	O
been	O
used	O
previously	O
to	O
condition	O
the	O
animal	O
to	O
produce	O
that	O
cr	O
for	O
example	O
in	O
the	O
first	O
stage	O
of	O
a	O
blocking	B
experiment	O
involving	O
rabbit	O
nictitating	O
membrane	O
conditioning	O
a	O
rabbit	O
is	O
first	O
conditioned	O
with	O
a	O
tone	O
cs	O
and	O
an	O
air	O
puff	O
us	O
to	O
produce	O
the	O
cr	O
of	O
closing	O
its	O
nictitating	O
membrane	O
in	O
anticipation	O
of	O
the	O
air	O
puff	O
the	O
experiment	O
s	O
second	O
stage	O
consists	O
of	O
additional	O
trials	O
in	O
which	O
a	O
second	O
stimulus	O
say	O
a	O
light	O
is	O
added	O
to	O
the	O
tone	O
to	O
form	O
a	O
compound	B
tonelight	O
cs	O
followed	O
by	O
the	O
same	O
air	O
puff	O
us	O
in	O
the	O
experiment	O
s	O
third	O
phase	O
the	O
second	O
stimulus	O
alone	O
the	O
light	O
is	O
presented	O
to	O
the	O
rabbit	O
to	O
see	O
if	O
the	O
rabbit	O
has	O
learned	O
to	O
respond	O
to	O
it	O
with	O
a	O
cr	O
it	O
turns	O
out	O
that	O
the	O
rabbit	O
produces	O
very	O
few	O
or	O
no	O
crs	O
in	O
response	O
to	O
the	O
light	O
learning	O
to	O
the	O
light	O
had	O
been	O
blocked	O
by	O
the	O
previous	O
learning	O
to	O
the	O
blocking	B
results	O
like	O
this	O
challenged	O
the	O
idea	O
that	O
conditioning	O
depends	O
only	O
on	O
simple	O
temporal	O
contiguity	O
that	O
is	O
that	O
a	O
necessary	O
and	O
sufficient	O
condition	O
for	O
conditioning	O
is	O
that	O
a	O
us	O
frequently	O
follows	O
a	O
cs	O
closely	O
in	O
time	O
in	O
the	O
next	O
section	O
we	O
describe	O
the	O
rescorla	O
wagner	O
model	O
and	O
wagner	O
that	O
offered	O
an	O
influential	O
explanation	O
for	O
blocking	B
higher-order	O
conditioning	O
occurs	O
when	O
a	O
previously-conditioned	O
cs	O
acts	O
as	O
a	O
us	O
in	O
conditioning	O
another	O
initially	O
neutral	O
stimulus	O
pavlov	B
described	O
an	O
experiment	O
in	O
which	O
his	O
assistant	O
first	O
conditioned	O
a	O
dog	O
to	O
salivate	O
to	O
the	O
sound	O
of	O
a	O
metronome	O
that	O
predicted	O
a	O
food	O
us	O
as	O
described	O
above	O
after	O
this	O
stage	O
of	O
conditioning	O
a	O
number	O
of	O
trials	O
were	O
conducted	O
in	O
which	O
a	O
black	O
square	O
to	O
which	O
the	O
dog	O
was	O
initially	O
indifferent	O
was	O
placed	O
in	O
the	O
dog	O
s	O
line	O
of	O
vision	O
followed	O
by	O
the	O
sound	O
of	O
the	O
metronome	O
and	O
this	O
was	O
not	O
followed	O
by	O
food	O
in	O
just	O
ten	O
trials	O
the	O
dog	O
began	O
to	O
salivate	O
merely	O
upon	O
seeing	O
the	O
black	O
square	O
despite	O
the	O
fact	O
that	O
the	O
sight	O
of	O
it	O
had	O
never	O
been	O
followed	O
by	O
food	O
the	O
sound	O
of	O
the	O
metronome	O
itself	O
acted	O
as	O
a	O
us	O
in	O
conditioning	O
a	O
salivation	O
cr	O
to	O
the	O
black	O
square	O
cs	O
this	O
was	O
second-order	O
conditioning	O
if	O
the	O
black	O
square	O
had	O
been	O
used	O
as	O
a	O
us	O
to	O
establish	O
salivation	O
crs	O
to	O
another	O
otherwise	O
neutral	O
cs	O
it	O
would	O
have	O
been	O
third-order	O
conditioning	O
and	O
so	O
on	O
higher-order	O
conditioning	O
is	O
difficult	O
to	O
demonstrate	O
especially	O
above	O
the	O
second	O
order	O
in	O
part	O
because	O
a	O
higher-order	O
reinforcer	O
loses	O
its	O
reinforcing	O
value	B
due	O
to	O
not	O
being	O
repeatedly	O
followed	O
by	O
the	O
original	O
us	O
during	O
higher-order	O
conditioning	O
trials	O
but	O
under	O
the	O
right	O
conditions	O
such	O
as	O
intermixing	O
first-order	O
trials	O
with	O
higher-order	O
trials	O
or	O
by	O
providing	O
a	O
general	O
energizing	O
stimulus	O
higher-order	O
conditioning	O
beyond	O
the	O
second	O
order	O
can	O
be	O
demonstrated	O
as	O
we	O
describe	O
below	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
uses	O
the	O
bootstrapping	B
idea	O
that	O
is	O
central	O
to	O
our	O
approach	O
to	O
extend	O
the	O
rescorla	O
wagner	O
model	O
s	O
account	O
of	O
blocking	B
to	O
include	O
both	O
the	O
anticipatory	O
nature	O
of	O
crs	O
and	B
higher-order	I
conditioning	I
with	O
a	O
control	B
group	O
is	O
necessary	O
to	O
show	O
that	O
the	O
previous	O
conditioning	O
to	O
the	O
tone	O
is	O
responsible	O
for	O
blocking	B
learning	O
to	O
the	O
light	O
this	O
is	O
done	O
by	O
trials	O
with	O
the	O
tonelight	O
cs	O
but	O
with	O
no	O
prior	O
conditioning	O
to	O
the	O
tone	O
learning	O
to	O
the	O
light	O
in	O
this	O
case	O
is	O
unimpaired	O
moore	O
and	O
schmajuk	O
give	O
a	O
full	O
account	O
of	O
this	O
procedure	O
chapter	O
psychology	B
higher-order	O
instrumental	O
conditioning	O
occurs	O
as	O
well	O
in	O
this	O
case	O
a	O
stimulus	O
that	O
consistently	O
predicts	O
primary	O
reinforcement	O
becomes	O
a	O
reinforcer	O
itself	O
where	O
reinforcement	O
is	O
primary	O
if	O
its	O
rewarding	O
or	O
penalizing	O
quality	O
has	O
been	O
built	O
into	O
the	O
animal	O
by	O
evolution	B
the	O
predicting	O
stimulus	O
becomes	O
a	O
secondary	O
reinforcer	O
or	O
more	O
generally	O
a	O
higher-order	O
or	O
conditioned	O
reinforcer	O
the	O
latter	O
being	O
a	O
better	O
term	O
when	O
the	O
predicted	O
reinforcing	O
stimulus	O
is	O
itself	O
a	O
secondary	O
or	O
an	O
even	O
higher-order	O
reinforcer	O
a	O
conditioned	O
reinforcer	O
delivers	O
conditioned	O
reinforcement	O
conditioned	O
reward	O
or	O
conditioned	O
penalty	O
conditioned	O
reinforcement	O
acts	O
like	O
primary	O
reinforcement	O
in	O
increasing	O
an	O
animal	O
s	O
tendency	O
to	O
produce	O
behavior	O
that	O
leads	O
to	O
conditioned	O
reward	O
and	O
to	O
decrease	O
an	O
animal	O
s	O
tendency	O
to	O
produce	O
behavior	O
that	O
leads	O
to	O
conditioned	O
penalty	O
our	O
comments	O
at	O
the	O
end	O
of	O
this	O
chapter	O
that	O
explain	O
how	O
our	O
terminology	O
sometimes	O
differs	O
as	O
it	O
does	O
here	O
from	O
terminology	O
used	O
in	B
psychology	B
conditioned	O
reinforcement	O
is	O
a	O
key	O
phenomenon	O
that	O
explains	O
for	O
instance	O
why	O
we	O
work	O
for	O
the	O
conditioned	O
reinforcer	O
money	O
whose	O
worth	O
derives	O
solely	O
from	O
what	O
is	O
predicted	O
by	O
having	O
it	O
in	O
actor	O
critic	O
methods	O
described	O
in	O
section	O
discussed	O
in	O
the	O
context	O
of	O
neuroscience	B
in	O
sections	O
and	O
the	O
critic	O
uses	O
a	O
td	B
method	O
to	O
evaluate	O
the	O
actor	O
s	O
policy	B
and	O
its	O
value	B
estimates	O
provide	O
conditioned	O
reinforcement	O
to	O
the	O
actor	O
allowing	O
the	O
actor	O
to	O
improve	O
its	O
policy	B
this	O
analog	O
of	O
higher-order	O
instrumental	O
conditioning	O
helps	O
address	O
the	O
credit-assignment	O
problem	O
mentioned	O
in	O
section	O
because	O
the	O
critic	O
gives	O
moment-by-moment	O
reinforcement	O
to	O
the	O
actor	O
when	O
the	O
primary	O
reward	B
signal	I
is	O
delayed	O
we	O
discuss	O
this	O
more	O
below	O
in	O
section	O
the	O
rescorla	O
wagner	O
model	O
rescorla	O
and	O
wagner	O
created	O
their	O
model	O
mainly	O
to	O
account	O
for	O
blocking	B
the	O
core	O
idea	O
of	O
the	O
rescorla	O
wagner	O
model	O
is	O
that	O
an	O
animal	O
only	O
learns	O
when	O
events	O
violate	O
its	O
expectations	O
in	O
other	O
words	O
only	O
when	O
the	O
animal	O
is	O
surprised	O
without	O
necessarily	O
implying	O
any	O
conscious	O
expectation	O
or	O
emotion	O
we	O
first	O
present	O
rescorla	O
and	O
wagner	O
s	O
model	O
using	O
their	O
terminology	O
and	O
notation	O
before	O
shifting	O
to	O
the	O
terminology	O
and	O
notation	O
we	O
use	O
to	O
describe	O
the	O
td	B
model	I
here	O
is	O
how	O
rescorla	O
and	O
wagner	O
described	O
their	O
model	O
the	O
model	O
adjusts	O
the	O
associative	O
strength	O
of	O
each	O
component	O
stimulus	O
of	O
a	O
compound	B
cs	O
which	O
is	O
a	O
number	O
representing	O
how	O
strongly	O
or	O
reliably	O
that	O
component	O
is	O
predictive	O
of	O
a	O
us	O
when	O
a	O
compound	B
cs	O
consisting	O
of	O
several	O
component	O
stimuli	O
is	O
presented	O
in	O
a	O
classical	B
conditioning	I
trial	O
the	O
associative	O
strength	O
of	O
each	O
component	O
stimulus	O
changes	O
in	O
a	O
way	O
that	O
depends	O
on	O
an	O
associative	O
strength	O
associated	O
with	O
the	O
entire	O
stimulus	O
compound	B
called	O
the	O
aggregate	O
associative	O
strength	O
and	O
not	O
just	O
on	O
the	O
associative	O
strength	O
of	O
each	O
component	O
itself	O
rescorla	O
and	O
wagner	O
considered	O
a	O
compound	B
cs	O
ax	O
consisting	O
of	O
component	O
stimuli	O
a	O
and	O
x	O
where	O
the	O
animal	O
may	O
have	O
already	O
experienced	O
stimulus	O
a	O
and	O
stimulus	O
x	O
might	O
be	O
new	O
to	O
the	O
animal	O
let	O
va	O
vx	O
and	O
vax	O
respectively	O
denote	O
the	O
associative	O
strengths	O
of	O
stimuli	O
a	O
x	O
and	O
the	O
compound	B
ax	O
suppose	O
that	O
on	O
a	O
trial	O
the	O
compound	B
cs	O
ax	O
is	O
followed	O
by	O
a	O
us	O
which	O
we	O
label	O
stimulus	O
y	O
then	O
the	O
associative	O
strengths	O
classical	B
conditioning	I
of	O
the	O
stimulus	O
components	O
change	O
according	O
to	O
these	O
expressions	O
va	O
a	O
yry	O
vax	O
vx	O
x	O
yry	O
vax	O
where	O
a	O
y	O
and	O
x	O
y	O
are	O
the	O
step-size	O
parameters	O
which	O
depend	O
on	O
the	O
identities	O
of	O
the	O
cs	O
components	O
and	O
the	O
us	O
and	O
ry	O
is	O
the	O
asymptotic	O
level	O
of	O
associative	O
strength	O
that	O
the	O
us	O
y	O
can	O
support	O
and	O
wagner	O
used	O
here	O
instead	O
of	O
r	O
but	O
we	O
use	O
r	O
to	O
avoid	O
confusion	O
with	O
our	O
use	O
of	O
and	O
because	O
we	O
usually	O
think	O
of	O
this	O
as	O
the	O
magnitude	O
of	O
a	O
reward	B
signal	I
with	O
the	O
caveat	O
that	O
the	O
us	O
in	O
classical	B
conditioning	I
is	O
not	O
necessarily	O
rewarding	O
or	O
penalizing	O
a	O
key	O
assumption	O
of	O
the	O
model	O
is	O
that	O
the	O
aggregate	O
associative	O
strength	O
vax	O
is	O
equal	O
to	O
va	O
vx	O
the	O
associative	O
strengths	O
as	O
changed	O
by	O
these	O
s	O
become	O
the	O
associative	O
strengths	O
at	O
the	O
beginning	O
of	O
the	O
next	O
trial	O
to	O
be	O
complete	O
the	O
model	O
needs	O
a	O
response-generation	O
mechanism	O
which	O
is	O
a	O
way	O
of	O
mapping	O
values	O
of	O
v	O
s	O
to	O
crs	O
because	O
this	O
mapping	O
would	O
depend	O
on	O
details	O
of	O
the	O
experimental	O
situation	O
rescorla	O
and	O
wagner	O
did	O
not	O
specify	O
a	O
mapping	O
but	O
simply	O
assumed	O
that	O
larger	O
v	O
s	O
would	O
produce	O
stronger	O
or	O
more	O
likely	O
crs	O
and	O
that	O
negative	O
v	O
s	O
would	O
mean	O
that	O
there	O
would	O
be	O
no	O
crs	O
the	O
rescorla	O
wagner	O
model	O
accounts	O
for	O
the	O
acquisition	O
of	O
crs	O
in	O
a	O
way	O
that	O
explains	O
blocking	B
as	O
long	O
as	O
the	O
aggregate	O
associative	O
strength	O
vax	O
of	O
the	O
stimulus	O
compound	B
is	O
below	O
the	O
asymptotic	O
level	O
of	O
associative	O
strength	O
ry	O
that	O
the	O
us	O
y	O
can	O
support	O
the	O
prediction	B
error	O
ry	O
vax	O
is	O
positive	O
this	O
means	O
that	O
over	O
successive	O
trials	O
the	O
associative	O
strengths	O
va	O
and	O
vx	O
of	O
the	O
component	O
stimuli	O
increase	O
until	O
the	O
aggregate	O
associative	O
strength	O
vax	O
equals	O
ry	O
at	O
which	O
point	O
the	O
associative	O
strengths	O
stop	O
changing	O
the	O
us	O
changes	O
when	O
a	O
new	O
component	O
is	O
added	O
to	O
a	O
compound	B
cs	O
to	O
which	O
the	O
animal	O
has	O
already	O
been	O
conditioned	O
further	O
conditioning	O
with	O
the	O
augmented	O
compound	B
produces	O
little	O
or	O
no	O
increase	O
in	O
the	O
associative	O
strength	O
of	O
the	O
added	O
cs	O
component	O
because	O
the	O
error	O
has	O
already	O
been	O
reduced	O
to	O
zero	O
or	O
to	O
a	O
low	O
value	B
the	O
occurrence	O
of	O
the	O
us	O
is	O
already	O
predicted	O
nearly	O
perfectly	O
so	O
little	O
or	O
no	O
error	O
or	O
surprise	O
is	O
introduced	O
by	O
the	O
new	O
cs	O
component	O
prior	O
learning	O
blocks	O
learning	O
to	O
the	O
new	O
component	O
to	O
transition	O
from	O
rescorla	O
and	O
wagner	O
s	O
model	O
to	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
we	O
just	O
call	O
the	O
td	B
model	I
we	O
first	O
recast	O
their	O
model	O
in	O
terms	O
of	O
the	O
concepts	O
that	O
we	O
are	O
using	O
throughout	O
this	O
book	O
specifically	O
we	O
match	O
the	O
notation	O
we	O
use	O
for	O
learning	O
with	O
linear	O
function	B
approximation	I
and	O
we	O
think	O
of	O
the	O
conditioning	O
process	O
as	O
one	O
of	O
learning	O
to	O
predict	O
the	O
magnitude	O
of	O
the	O
us	O
on	O
a	O
trial	O
on	O
the	O
basis	O
of	O
the	O
compound	B
cs	O
presented	O
on	O
that	O
trial	O
where	O
the	O
magnitude	O
of	O
a	O
us	O
y	O
is	O
the	O
ry	O
of	O
the	O
rescorla	O
wagner	O
model	O
as	O
given	O
above	O
we	O
also	O
introduce	O
states	O
because	O
the	O
rescorla	O
wagner	O
model	O
is	O
a	O
trial-level	O
model	O
meaning	O
that	O
it	O
deals	O
with	O
how	O
associative	O
strengths	O
change	O
from	O
trial	O
to	O
trial	O
without	O
considering	O
any	O
details	O
about	O
what	O
happens	O
within	O
and	O
between	O
trials	O
we	O
do	O
not	O
have	O
to	O
consider	O
how	O
states	O
change	O
during	O
a	O
trial	O
until	O
we	O
present	O
the	O
full	O
td	B
model	I
in	O
the	O
following	O
section	O
instead	O
here	O
we	O
simply	O
think	O
of	O
a	O
state	B
as	O
a	O
way	O
of	O
labeling	O
a	O
trial	O
in	O
terms	O
of	O
the	O
collection	O
of	O
component	O
css	O
that	O
are	O
present	O
on	O
the	O
trial	O
therefore	O
assume	O
that	O
trial-type	O
or	O
state	B
s	O
is	O
described	O
by	O
a	O
real-valued	O
vector	B
of	O
chapter	O
psychology	B
features	O
xs	O
where	O
xis	O
if	O
csi	O
the	O
ith	O
component	O
of	O
a	O
compound	B
cs	O
is	O
present	O
on	O
the	O
trial	O
and	O
otherwise	O
then	O
if	O
the	O
d-dimensional	O
vector	B
of	O
associative	O
strengths	O
is	O
w	O
the	O
aggregate	O
associative	O
strength	O
for	O
trial-type	O
s	O
is	O
vsw	O
this	O
corresponds	O
to	O
a	O
value	B
estimate	O
in	O
reinforcement	B
learning	I
and	O
we	O
think	O
of	O
it	O
as	O
the	O
us	O
prediction	B
now	O
temporally	O
let	O
t	O
denote	O
the	O
number	O
of	O
a	O
complete	O
trial	O
and	O
not	O
its	O
usual	O
meaning	O
as	O
a	O
time	O
step	O
revert	O
to	O
t	O
s	O
usual	O
meaning	O
when	O
we	O
extend	O
this	O
to	O
the	O
td	B
model	I
below	O
and	O
assume	O
that	O
st	O
is	O
the	O
state	B
corresponding	O
to	O
trial	O
t	O
conditioning	O
trial	O
t	O
updates	O
the	O
associative	O
strength	O
vector	B
wt	O
to	O
as	O
follows	O
wt	O
t	O
xst	O
where	O
is	O
the	O
step-size	B
parameter	I
and	O
because	O
here	O
we	O
are	O
describing	O
the	O
rescorla	O
wagner	O
model	O
t	O
is	O
the	O
prediction	B
error	O
t	O
rt	O
vstwt	O
rt	O
is	O
the	O
target	O
of	O
the	O
prediction	B
on	O
trial	O
t	O
that	O
is	O
the	O
magnitude	O
of	O
the	O
us	O
or	O
in	O
rescorla	O
and	O
wagner	O
s	O
terms	O
the	O
associative	O
strength	O
that	O
the	O
us	O
on	O
the	O
trial	O
can	O
support	O
note	O
that	O
because	O
of	O
the	O
factor	O
xst	O
in	O
only	O
the	O
associative	O
strengths	O
of	O
cs	O
components	O
present	O
on	O
a	O
trial	O
are	O
adjusted	O
as	O
a	O
result	O
of	O
that	O
trial	O
you	O
can	O
think	O
of	O
the	O
prediction	B
error	O
as	O
a	O
measure	O
of	O
surprise	O
and	O
the	O
aggregate	O
associative	O
strength	O
as	O
the	O
animal	O
s	O
expectation	O
that	O
is	O
violated	O
when	O
it	O
does	O
not	O
match	O
the	O
target	O
us	O
magnitude	O
from	O
the	O
perspective	O
of	O
machine	O
learning	O
the	O
rescorla	O
wagner	O
model	O
is	O
an	O
errorcorrection	O
supervised	B
learning	I
rule	O
it	O
is	O
essentially	O
the	O
same	O
as	O
the	O
least	O
mean	O
square	O
or	O
widrow-hoff	O
learning	O
rule	O
and	O
hoff	O
that	O
finds	O
the	O
weights	O
here	O
the	O
associative	O
strengths	O
that	O
make	O
the	O
average	O
of	O
the	O
squares	O
of	O
all	O
the	O
errors	O
as	O
close	O
to	O
zero	O
as	O
possible	O
it	O
is	O
a	O
curve-fitting	O
or	O
regression	O
algorithm	O
that	O
is	O
widely	O
used	O
in	O
engineering	O
and	O
scientific	O
applications	O
section	O
the	O
rescorla	O
wagner	O
model	O
was	O
very	O
influential	O
in	O
the	O
history	B
of	I
animal	O
learning	O
theory	O
because	O
it	O
showed	O
that	O
a	O
mechanistic	O
theory	O
could	O
account	O
for	O
the	O
main	O
facts	O
about	O
blocking	B
without	O
resorting	O
to	O
more	O
complex	O
cognitive	O
theories	O
involving	O
for	O
example	O
an	O
animal	O
s	O
explicit	O
recognition	O
that	O
another	O
stimulus	O
component	O
had	O
been	O
added	O
and	O
then	O
scanning	O
its	O
short-term	O
memory	O
backward	O
to	O
reassess	O
the	O
predictive	O
relationships	O
involving	O
the	O
us	O
the	O
rescorla	O
wagner	O
model	O
showed	O
how	O
traditional	O
contiguity	O
theories	O
of	O
conditioning	O
that	O
temporal	O
contiguity	O
of	O
stimuli	O
was	O
a	O
necessary	O
and	O
sufficient	O
condition	O
for	O
learning	O
could	O
be	O
adjusted	O
in	O
a	O
simple	O
way	O
to	O
account	O
for	O
blocking	B
and	O
schmajuk	O
only	O
differences	O
between	O
the	O
lms	O
rule	O
and	O
the	O
rescorla	O
wagner	O
model	O
are	O
that	O
for	O
lms	O
the	O
input	O
vectors	O
xt	O
can	O
have	O
any	O
real	O
numbers	O
as	O
components	O
and	O
at	O
least	O
in	O
the	O
simplest	O
version	O
of	O
the	O
lms	O
rule	O
the	O
step-size	B
parameter	I
does	O
not	O
depend	O
on	O
the	O
input	O
vector	B
or	O
the	O
identity	O
of	O
the	O
stimulus	O
setting	O
the	O
prediction	B
target	O
classical	B
conditioning	I
the	O
rescorla	O
wagner	O
model	O
provides	O
a	O
simple	O
account	O
of	O
blocking	B
and	O
some	O
other	O
features	O
of	O
classical	B
conditioning	I
but	O
it	O
is	O
not	O
a	O
complete	O
or	O
perfect	O
model	O
of	O
classical	B
conditioning	I
different	O
ideas	O
account	O
for	O
a	O
variety	O
of	O
other	O
observed	O
effects	O
and	O
progress	O
is	O
still	O
being	O
made	O
toward	O
understanding	O
the	O
many	O
subtleties	O
of	O
classical	B
conditioning	I
the	O
td	B
model	I
which	O
we	O
describe	O
next	O
though	O
also	O
not	O
a	O
complete	O
or	O
perfect	O
model	O
model	O
of	O
classical	B
conditioning	I
extends	O
the	O
rescorla	O
wagner	O
model	O
to	O
address	O
how	O
within-trial	O
and	O
between-trial	O
timing	O
relationships	O
among	O
stimuli	O
can	O
influence	O
learning	O
and	O
how	O
higher-order	O
conditioning	O
might	O
arise	O
the	O
td	B
model	I
the	O
td	B
model	I
is	O
a	O
real-time	O
model	O
as	O
opposed	O
to	O
a	O
trial-level	O
model	O
like	O
the	O
rescorla	O
wagner	O
model	O
a	O
single	O
step	O
t	O
in	O
the	O
rescorla	O
wagner	O
model	O
represents	O
an	O
entire	O
conditioning	O
trial	O
the	O
model	O
does	O
not	O
apply	O
to	O
details	O
about	O
what	O
happens	O
during	O
the	O
time	O
a	O
trial	O
is	O
taking	O
place	O
or	O
what	O
might	O
happen	O
between	O
trials	O
within	O
each	O
trial	O
an	O
animal	O
might	O
experience	O
various	O
stimuli	O
whose	O
onsets	O
occur	O
at	O
particular	O
times	O
and	O
that	O
have	O
particular	O
durations	O
these	O
timing	O
relationships	O
strongly	O
influence	O
learning	O
the	O
rescorla	O
wagner	O
model	O
also	O
does	O
not	O
include	O
a	O
mechanism	O
for	O
higher-order	O
conditioning	O
whereas	O
for	O
the	O
td	B
model	I
higher-order	O
conditioning	O
is	O
a	O
natural	O
consequence	O
of	O
the	O
bootstrapping	B
idea	O
that	O
is	O
at	O
the	O
base	O
of	O
td	B
algorithms	O
to	O
describe	O
the	O
td	B
model	I
we	O
begin	O
with	O
the	O
formulation	O
of	O
the	O
rescorla	O
wagner	O
model	O
above	O
but	O
t	O
now	O
labels	O
time	O
steps	O
within	O
or	O
between	O
trials	O
instead	O
of	O
complete	O
trials	O
think	O
of	O
the	O
time	O
between	O
t	O
and	O
t	O
as	O
a	O
small	O
time	O
interval	O
say	O
second	O
and	O
think	O
of	O
a	O
trial	O
as	O
a	O
sequences	O
of	O
states	O
one	O
associated	O
with	O
each	O
time	O
step	O
where	O
the	O
state	B
at	O
step	O
t	O
now	O
represents	O
details	O
of	O
how	O
stimuli	O
are	O
represented	O
at	O
t	O
instead	O
of	O
just	O
a	O
label	O
for	O
the	O
cs	O
components	O
present	O
on	O
a	O
trial	O
in	O
fact	O
we	O
can	O
completely	O
abandon	O
the	O
idea	O
of	O
trials	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
animal	O
a	O
trial	O
is	O
just	O
a	O
fragment	O
of	O
its	O
continuing	O
experience	O
interacting	O
with	O
its	O
world	O
following	O
our	O
usual	O
view	O
of	O
an	O
agent	O
interacting	O
with	O
its	O
environment	B
imagine	O
that	O
the	O
animal	O
is	O
experiencing	O
an	O
endless	O
sequence	O
of	O
states	O
s	O
each	O
represented	O
by	O
a	O
feature	O
vector	B
xs	O
that	O
said	O
it	O
is	O
still	O
often	O
convenient	O
to	O
refer	O
to	O
trials	O
as	O
fragments	O
of	O
time	O
during	O
which	O
patterns	O
of	O
stimuli	O
repeat	O
in	O
an	O
experiment	O
state	B
features	O
are	O
not	O
restricted	O
to	O
describing	O
the	O
external	O
stimuli	O
that	O
an	O
animal	O
experiences	O
they	O
can	O
describe	O
neural	B
activity	O
patterns	O
that	O
external	O
stimuli	O
produce	O
in	O
an	O
animal	O
s	O
brain	O
and	O
these	O
patterns	O
can	O
be	O
history-dependent	O
meaning	O
that	O
they	O
can	O
be	O
persistent	O
patterns	O
produced	O
by	O
sequences	O
of	O
external	O
stimuli	O
of	O
course	O
we	O
do	O
not	O
know	O
exactly	O
what	O
these	O
neural	B
activity	O
patterns	O
are	O
but	O
a	O
real-time	O
model	O
like	O
the	O
td	B
model	I
allows	O
one	O
to	O
explore	O
the	O
consequences	O
on	O
learning	O
of	O
different	O
hypotheses	O
about	O
the	O
internal	O
representations	O
of	O
external	O
stimuli	O
for	O
these	O
reasons	O
the	O
td	B
model	I
does	O
not	O
commit	O
to	O
any	O
particular	O
state	B
representation	O
in	O
addition	O
because	O
the	O
td	B
model	I
includes	O
discounting	B
and	B
eligibility	B
traces	I
that	O
span	O
time	O
intervals	O
between	O
stimuli	O
the	O
model	O
also	O
makes	O
it	O
possible	O
to	O
explore	O
how	O
discounting	B
and	B
eligibility	B
traces	I
interact	O
with	O
stimulus	O
representations	O
in	O
making	O
predictions	O
about	O
the	O
results	O
of	O
classical	B
conditioning	I
experiments	O
chapter	O
psychology	B
below	O
we	O
describe	O
some	O
of	O
the	O
state	B
representations	O
that	O
have	O
been	O
used	O
with	O
the	O
td	B
model	I
and	O
some	O
of	O
their	O
implications	O
but	O
for	O
the	O
moment	O
we	O
stay	O
agnostic	O
about	O
the	O
representation	O
and	O
just	O
assume	O
that	O
each	O
state	B
s	O
is	O
represented	O
by	O
a	O
feature	O
vector	B
xs	O
then	O
the	O
aggregate	O
associative	O
strength	O
corresponding	O
to	O
a	O
state	B
s	O
is	O
given	O
by	O
the	O
same	O
as	O
for	O
the	O
rescorla-wgner	O
model	O
but	O
the	O
td	B
model	I
updates	O
the	O
associative	O
strength	O
vector	B
w	O
differently	O
with	O
t	O
now	O
labeling	O
a	O
time	O
step	O
instead	O
of	O
a	O
complete	O
trial	O
the	O
td	B
model	I
governs	O
learning	O
according	O
to	O
this	O
update	O
wt	O
t	O
zt	O
which	O
replaces	O
xtst	O
in	O
the	O
rescorla	O
wagner	O
update	O
with	O
zt	O
a	O
vector	B
of	O
eligibility	B
traces	I
and	O
instead	O
of	O
the	O
t	O
of	O
here	O
t	O
is	O
a	O
td	B
error	I
t	O
vstwt	O
where	O
is	O
a	O
discount	O
factor	O
and	O
rt	O
is	O
the	O
prediction	B
target	O
at	O
time	O
t	O
and	O
and	O
vstwt	O
are	O
aggregate	O
associative	O
strengths	O
at	O
t	O
and	O
t	O
as	O
defined	O
by	O
each	O
component	O
i	O
of	O
the	O
eligibility-trace	O
vector	B
zt	O
increments	O
or	O
decrements	O
according	O
to	O
the	O
component	O
xist	O
of	O
the	O
feature	O
vector	B
xst	O
and	O
otherwise	O
decays	O
with	O
a	O
rate	O
determined	O
by	O
zt	O
xst	O
here	O
is	O
the	O
usual	O
eligibility	O
trace	O
decay	O
parameter	O
note	O
that	O
if	O
the	O
td	B
model	I
reduces	O
to	O
the	O
rescorla	O
wagner	O
model	O
with	O
the	O
exceptions	O
that	O
the	O
meaning	O
of	O
t	O
is	O
different	O
in	O
each	O
case	O
trial	O
number	O
for	O
the	O
rescorla	O
wagner	O
model	O
and	O
a	O
time	O
step	O
for	O
the	O
td	B
model	I
and	O
in	O
the	O
td	B
model	I
there	O
is	O
a	O
one-time-step	O
lead	O
in	O
the	O
prediction	B
target	O
r	O
the	O
td	B
model	I
is	O
equivalent	O
to	O
the	O
backward	O
view	O
of	O
the	O
semi-gradient	O
td	B
algorithm	O
with	O
linear	O
function	B
approximation	I
except	O
that	O
rt	O
in	O
the	O
model	O
does	O
not	O
have	O
to	O
be	O
a	O
reward	B
signal	I
as	O
it	O
does	O
when	O
the	O
td	B
algorithm	O
is	O
used	O
to	O
learn	O
a	O
value	B
function	I
for	O
policy-improvement	O
td	B
model	I
simulations	O
real-time	O
conditioning	O
models	O
like	O
the	O
td	B
model	I
are	O
interesting	O
primarily	O
because	O
they	O
make	O
predictions	O
for	O
a	O
wide	O
range	O
of	O
situations	O
that	O
cannot	O
be	O
represented	O
by	O
trial-level	O
models	O
these	O
situations	O
involve	O
the	O
timing	O
and	O
durations	O
of	O
conditionable	O
stimuli	O
the	O
timing	O
of	O
these	O
stimuli	O
in	O
relation	O
to	O
the	O
timing	O
of	O
the	O
us	O
and	O
the	O
timing	O
and	O
shapes	O
of	O
crs	O
for	O
example	O
the	O
us	O
generally	O
must	O
begin	O
after	O
the	O
onset	O
of	O
a	O
neutral	O
stimulus	O
for	O
conditioning	O
to	O
occur	O
with	O
the	O
rate	O
and	O
effectiveness	O
of	O
learning	O
depending	O
on	O
the	O
interstimulus	O
interval	O
or	O
isi	O
the	O
interval	O
between	O
the	O
onsets	O
of	O
the	O
cs	O
and	O
the	O
us	O
when	O
crs	O
appear	O
they	O
generally	O
begin	O
before	O
the	O
appearance	O
of	O
the	O
us	O
and	O
their	O
temporal	O
profiles	O
change	O
during	O
learning	O
in	O
conditioning	O
with	O
compound	B
css	O
the	O
component	O
stimuli	O
of	O
the	O
compound	B
css	O
may	O
not	O
all	O
begin	O
and	O
end	O
at	O
the	O
same	O
time	O
sometimes	O
classical	B
conditioning	I
forming	O
what	O
is	O
called	O
a	O
serial	O
compound	B
in	O
which	O
the	O
component	O
stimuli	O
occur	O
in	O
a	O
sequence	O
over	O
time	O
timing	O
considerations	O
like	O
these	O
make	O
it	O
important	O
to	O
consider	O
how	O
stimuli	O
are	O
represented	O
how	O
these	O
representations	O
unfold	O
over	O
time	O
during	O
and	O
between	O
trials	O
and	O
how	O
they	O
interact	O
with	O
discounting	B
and	B
eligibility	B
traces	I
figure	O
shows	O
three	O
of	O
the	O
stimulus	O
representations	O
that	O
have	O
been	O
used	O
in	O
exploring	O
the	O
behavior	O
of	O
the	O
td	B
model	I
the	O
complete	O
serial	O
compound	B
the	O
microstimulus	O
and	O
the	O
presence	O
representations	O
sutton	O
and	O
kehoe	O
these	O
representations	O
differ	O
in	O
the	O
degree	O
to	O
which	O
they	O
force	O
generalization	O
among	O
nearby	O
time	O
points	O
during	O
which	O
a	O
stimulus	O
is	O
present	O
the	O
simplest	O
of	O
the	O
representations	O
shown	O
in	O
figure	O
is	O
the	O
presence	O
representation	O
in	O
the	O
figure	O
s	O
right	O
column	O
this	O
representation	O
has	O
a	O
single	O
feature	O
for	O
each	O
component	O
cs	O
present	O
on	O
a	O
trial	O
where	O
the	O
feature	O
has	O
value	B
whenever	O
that	O
component	O
is	O
present	O
figure	O
three	O
stimulus	O
representations	O
columns	O
sometimes	O
used	O
with	O
the	O
td	B
model	I
each	O
row	O
represents	O
one	O
element	O
of	O
the	O
stimulus	O
representation	O
the	O
three	O
representations	O
vary	O
along	O
a	O
temporal	O
generalization	O
gradient	B
with	O
no	O
generalization	O
between	O
nearby	O
time	O
points	O
in	O
the	O
complete	O
serial	O
compound	B
column	O
and	O
complete	O
generalization	O
between	O
nearby	O
time	O
points	O
in	O
the	O
presence	O
representation	O
column	O
the	O
microstimulus	O
representation	O
occupies	O
a	O
middle	O
ground	O
the	O
degree	O
of	O
temporal	O
generalization	O
determines	O
the	O
temporal	O
granularity	O
with	O
which	O
us	O
predictions	O
are	O
learned	O
adapted	O
with	O
minor	O
changes	O
from	O
learning	O
behavior	O
evaluating	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
volume	O
p	O
e	O
a	O
ludvig	O
r	O
s	O
sutton	O
e	O
j	O
kehoe	O
with	O
permission	O
of	O
springer	O
presencecomplete	O
serial	O
compoundstimulus	O
representationmicrostimulicsus	O
chapter	O
psychology	B
and	O
the	O
presence	O
representation	O
is	O
not	O
a	O
realistic	O
hypothesis	O
about	O
how	O
stimuli	O
are	O
represented	O
in	O
an	O
animal	O
s	O
brain	O
but	O
as	O
we	O
describe	O
below	O
the	O
td	B
model	I
with	O
this	O
representation	O
can	O
produce	O
many	O
of	O
the	O
timing	O
phenomena	O
seen	O
in	O
classical	B
conditioning	I
for	O
the	O
csc	O
representation	O
column	O
of	O
figure	O
the	O
onset	O
of	O
each	O
external	O
stimulus	O
initiates	O
a	O
sequence	O
of	O
precisely-timed	O
short-duration	O
internal	O
signals	O
that	O
continues	O
until	O
the	O
external	O
stimulus	O
this	O
is	O
like	O
assuming	O
the	O
animal	O
s	O
nervous	O
system	O
has	O
a	O
clock	O
that	O
keeps	O
precise	O
track	O
of	O
time	O
during	O
stimulus	O
presentations	O
it	O
is	O
what	O
engineers	O
call	O
a	O
tapped	O
delay	O
line	O
like	O
the	O
presence	O
representation	O
the	O
csc	O
representation	O
is	O
unrealistic	O
as	O
a	O
hypothesis	O
about	O
how	O
the	O
brain	O
internally	O
represents	O
stimuli	O
but	O
ludvig	O
et	O
al	O
call	O
it	O
a	O
useful	O
fiction	O
because	O
it	O
can	O
reveal	O
details	O
of	O
how	O
the	O
td	B
model	I
works	O
when	O
relatively	O
unconstrained	O
by	O
the	O
stimulus	O
representation	O
the	O
csc	O
representation	O
is	O
also	O
used	O
in	O
most	O
td	B
models	O
of	O
dopamine-producing	O
neurons	O
in	O
the	O
brain	O
a	O
topic	O
we	O
take	O
up	O
in	O
chapter	O
the	O
csc	O
representation	O
is	O
often	O
viewed	O
as	O
an	O
essential	O
part	O
of	O
the	O
td	B
model	I
although	O
this	O
view	O
is	O
mistaken	O
the	O
ms	O
representation	O
column	O
of	O
figure	O
is	O
like	O
the	O
csc	O
representation	O
in	O
that	O
each	O
external	O
stimulus	O
initiates	O
a	O
cascade	O
of	O
internal	O
stimuli	O
but	O
in	O
this	O
case	O
the	O
internal	O
stimuli	O
the	O
microstimuli	O
are	O
not	O
of	O
such	O
limited	O
and	O
non-overlapping	O
form	O
they	O
are	O
extended	O
over	O
time	O
and	O
overlap	O
as	O
time	O
elapses	O
from	O
stimulus	O
onset	O
different	O
sets	O
of	O
microstimuli	O
become	O
more	O
or	O
less	O
active	O
and	O
each	O
subsequent	O
microstimulus	O
becomes	O
progressively	O
wider	O
in	O
time	O
and	O
reaches	O
a	O
lower	O
maximal	O
level	O
of	O
course	O
there	O
are	O
many	O
ms	O
representations	O
depending	O
on	O
the	O
nature	O
of	O
the	O
microstimuli	O
and	O
a	O
number	O
of	O
examples	O
of	O
ms	O
representations	O
have	O
been	O
studied	O
in	O
the	O
literature	O
in	O
some	O
cases	O
along	O
with	O
proposals	O
for	O
how	O
an	O
animal	O
s	O
brain	O
might	O
generate	O
them	O
the	O
bibliographic	O
and	O
historical	O
comments	O
at	O
the	O
end	O
of	O
this	O
chapter	O
ms	O
representations	O
are	O
more	O
realistic	O
than	O
the	O
presence	O
or	O
csc	O
representations	O
as	O
hypotheses	O
about	O
neural	B
representations	O
of	O
stimuli	O
and	O
they	O
allow	O
the	O
behavior	O
of	O
the	O
td	B
model	I
to	O
be	O
related	O
to	O
a	O
broader	O
collection	O
of	O
phenomena	O
observed	O
in	O
animal	O
experiments	O
in	O
particular	O
by	O
assuming	O
that	O
cascades	O
of	O
microstimuli	O
are	O
initiated	O
by	O
uss	O
as	O
well	O
as	O
by	O
css	O
and	O
by	O
studying	O
the	O
significant	O
effects	O
on	O
learning	O
of	O
interactions	O
between	O
microstimuli	O
eligibility	B
traces	I
and	O
discounting	B
the	O
td	B
model	I
is	O
helping	O
to	O
frame	O
hypotheses	O
to	O
account	O
for	O
many	O
of	O
the	O
subtle	O
phenomena	O
of	O
classical	B
conditioning	I
and	O
how	O
an	O
animal	O
s	O
brain	O
might	O
produce	O
them	O
we	O
say	O
more	O
about	O
this	O
below	O
particularly	O
in	O
chapter	O
where	O
we	O
discuss	O
reinforcement	B
learning	I
and	O
neuroscience	B
even	O
with	O
the	O
simple	O
presence	O
representation	O
however	O
the	O
td	B
model	I
produces	O
all	O
the	O
basic	O
properties	O
of	O
classical	B
conditioning	I
that	O
are	O
accounted	O
for	O
by	O
the	O
rescorla	O
our	O
formalism	O
there	O
is	O
a	O
different	O
state	B
st	O
for	O
each	O
time	O
step	O
t	O
during	O
a	O
trial	O
and	O
for	O
a	O
trial	O
in	O
which	O
a	O
compound	B
cs	O
consists	O
of	O
n	O
component	O
css	O
of	O
various	O
durations	O
occurring	O
at	O
various	O
times	O
throughout	O
the	O
trial	O
there	O
is	O
a	O
feature	O
xi	O
for	O
each	O
component	O
csi	O
i	O
n	O
where	O
xist	O
for	O
all	O
times	O
t	O
when	O
the	O
csi	O
is	O
present	O
and	O
equals	O
zero	O
otherwise	O
our	O
formalism	O
for	O
each	O
cs	O
component	O
csi	O
present	O
on	O
a	O
trial	O
and	O
for	O
each	O
time	O
step	O
t	O
during	O
a	O
if	O
t	O
for	O
any	O
at	O
which	O
csi	O
is	O
present	O
and	O
trial	O
there	O
is	O
a	O
separate	O
feature	O
xt	O
equals	O
otherwise	O
this	O
is	O
different	O
from	O
the	O
csc	O
representation	O
in	O
sutton	O
and	O
barto	O
in	O
which	O
there	O
are	O
the	O
same	O
distinct	O
features	O
for	O
each	O
time	O
step	O
but	O
no	O
reference	O
to	O
external	O
stimuli	O
hence	O
the	O
name	O
complete	O
serial	O
compound	B
i	O
where	O
xt	O
classical	B
conditioning	I
wagner	O
model	O
plus	O
features	O
of	O
conditioning	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
trial-level	O
models	O
for	O
example	O
as	O
we	O
have	O
already	O
mentioned	O
a	O
conspicuous	O
feature	O
of	O
classical	B
conditioning	I
is	O
that	O
the	O
us	O
generally	O
must	O
begin	O
after	O
the	O
onset	O
of	O
a	O
neutral	O
stimulus	O
for	O
conditioning	O
to	O
occur	O
and	O
that	O
after	O
conditioning	O
the	O
cr	O
begins	O
before	O
the	O
appearance	O
of	O
the	O
us	O
in	O
other	O
words	O
conditioning	O
generally	O
requires	O
a	O
positive	O
isi	O
and	O
the	O
cr	O
generally	O
anticipates	O
the	O
us	O
how	O
the	O
strength	O
of	O
conditioning	O
the	O
percentage	O
of	O
crs	O
elicited	O
by	O
a	O
cs	O
depends	O
on	O
the	O
isi	O
varies	O
substantially	O
across	O
species	O
and	O
response	O
systems	O
but	O
it	O
typically	O
has	O
the	O
following	O
properties	O
it	O
is	O
negligible	O
for	O
a	O
zero	O
or	O
negative	O
isi	O
i	O
e	O
when	O
the	O
us	O
onset	O
occurs	O
simultaneously	O
with	O
or	O
earlier	O
than	O
the	O
cs	O
onset	O
research	O
has	O
found	O
that	O
associative	O
strengths	O
sometimes	O
increase	O
slightly	O
or	O
become	O
negative	O
with	O
negative	O
isis	O
it	O
increases	O
to	O
a	O
maximum	O
at	O
a	O
positive	O
isi	O
where	O
conditioning	O
is	O
most	O
effective	O
and	O
it	O
then	O
decreases	O
to	O
zero	O
after	O
an	O
interval	O
that	O
varies	O
widely	O
with	O
response	O
systems	O
the	O
precise	O
shape	O
of	O
this	O
dependency	O
for	O
the	O
td	B
model	I
depends	O
on	O
the	O
values	O
of	O
its	O
parameters	O
and	O
details	O
of	O
the	O
stimulus	O
representation	O
but	O
these	O
basic	O
features	O
of	O
isi-dependency	O
are	O
core	O
properties	O
of	O
the	O
td	B
model	I
one	O
of	O
the	O
theoretical	O
issues	O
arising	O
with	O
serial-compound	O
conditioning	O
that	O
is	O
conditioning	O
with	O
a	O
compound	B
cs	O
whose	O
components	O
occur	O
in	O
a	O
sequence	O
concerns	O
the	O
facilitation	O
of	O
remote	O
associations	O
it	O
has	O
been	O
found	O
that	O
if	O
the	O
empty	O
trace	O
interval	O
between	O
a	O
first	O
cs	O
and	O
the	O
us	O
is	O
filled	O
with	O
a	O
second	O
cs	O
to	O
form	O
a	O
serial-compound	O
stimulus	O
then	O
conditioning	O
to	O
csa	O
is	O
facilitated	O
shown	O
to	O
the	O
right	O
is	O
the	O
behavior	O
of	O
the	O
td	B
model	I
with	O
the	O
presence	O
representation	O
in	O
a	O
simulation	O
of	O
such	O
an	O
experiment	O
whose	O
timing	O
details	O
are	O
shown	O
above	O
consistent	O
with	O
the	O
experimental	O
results	O
the	O
model	O
shows	O
facilitation	O
of	O
both	O
the	O
rate	O
of	O
conditioning	O
and	O
the	O
asymptotic	O
level	O
of	O
conditioning	O
of	O
the	O
first	O
cs	O
due	O
to	O
the	O
presence	O
of	O
the	O
second	O
cs	O
figure	O
facilitation	O
of	O
a	O
remote	O
association	O
by	O
an	O
intervening	O
stimulus	O
in	O
the	O
td	B
model	I
adapted	O
from	O
sutton	O
and	O
barto	O
wcsa	O
chapter	O
psychology	B
a	O
well-known	O
demonstration	O
of	O
the	O
effects	O
on	O
conditioning	O
of	O
temporal	O
relationships	O
among	O
stimuli	O
within	O
a	O
trial	O
is	O
an	O
experiment	O
by	O
egger	O
and	O
miller	O
that	O
involved	O
two	O
overlapping	O
css	O
in	O
a	O
delay	O
configuration	O
as	O
shown	O
to	O
the	O
right	O
although	O
csb	O
was	O
in	O
a	O
better	O
temporal	O
relationship	O
with	O
the	O
us	O
the	O
presence	O
of	O
csa	O
substantially	O
reduced	O
conditioning	O
to	O
csb	O
as	O
compared	O
to	O
controls	O
in	O
which	O
csa	O
was	O
absent	O
the	O
bottom	O
panel	O
shows	O
the	O
same	O
result	O
being	O
generated	O
by	O
the	O
td	B
model	I
in	O
a	O
simulation	O
of	O
this	O
experiment	O
with	O
the	O
presence	O
representation	O
i	O
e	O
figure	O
the	O
egger-miller	O
or	O
primacy	O
effect	O
in	O
the	O
td	B
model	I
adapted	O
from	O
sutton	O
and	O
barto	O
the	O
td	B
model	I
accounts	O
for	O
blocking	B
because	O
it	O
is	O
an	O
error-correcting	O
learning	O
rule	O
like	O
the	O
rescorla	O
wagner	O
model	O
beyond	O
accounting	O
for	O
basic	O
blocking	B
results	O
however	O
the	O
td	B
model	I
predicts	O
the	O
presence	O
representation	O
and	O
more	O
complex	O
representations	O
a	O
well	O
that	O
blocking	B
is	O
reversed	O
if	O
the	O
blocked	O
stimulus	O
is	O
moved	O
earlier	O
in	O
time	O
so	O
that	O
its	O
onset	O
occurs	O
before	O
the	O
onset	O
of	O
the	O
blocking	B
stimulus	O
this	O
feature	O
of	O
the	O
td	B
model	I
s	O
behavior	O
deserves	O
attention	O
because	O
it	O
had	O
not	O
been	O
observed	O
at	O
the	O
time	O
of	O
the	O
model	O
s	O
introduction	O
recall	O
that	O
in	O
blocking	B
if	O
an	O
animal	O
has	O
already	O
learned	O
that	O
one	O
cs	O
predicts	O
a	O
us	O
then	O
learning	O
that	O
a	O
newly-added	O
second	O
cs	O
also	O
predicts	O
the	O
us	O
is	O
much	O
reduced	O
is	O
blocked	O
but	O
if	O
the	O
newly-added	O
second	O
cs	O
begins	O
earlier	O
than	O
the	O
pretrained	O
cs	O
then	O
according	O
to	O
the	O
td	B
model	I
learning	O
to	O
the	O
newly-added	O
cs	O
is	O
not	O
blocked	O
in	O
fact	O
as	O
training	O
continues	O
and	O
the	O
newly-added	O
cs	O
gains	O
associative	O
strength	O
the	O
pretrained	O
cs	O
loses	O
associative	O
strength	O
the	O
behavior	O
of	O
the	O
td	B
model	I
under	O
these	O
conditions	O
is	O
shown	O
to	O
the	O
right	O
this	O
simulation	O
experiment	O
differed	O
from	O
the	O
egger-miller	O
experiment	O
of	O
figure	O
in	O
that	O
the	O
shorter	O
cs	O
with	O
the	O
later	O
onset	O
was	O
given	O
prior	O
training	O
until	O
it	O
was	O
fully	O
associated	O
with	O
the	O
us	O
this	O
surprising	O
prediction	B
led	O
kehoe	O
schreurs	O
and	O
graham	O
to	O
conduct	O
the	O
experiment	O
using	O
the	O
well-studied	O
rabbit	O
nic	O
figure	O
temporal	O
primacy	O
overriding	O
blocking	B
in	O
the	O
td	B
model	I
adapted	O
from	O
sutton	O
and	O
barto	O
wcsbwcsb	O
classical	B
conditioning	I
titating	O
membrane	O
preparation	O
their	O
results	O
confirmed	O
the	O
model	O
s	O
prediction	B
and	O
they	O
noted	O
that	O
non-td	O
models	O
have	O
considerable	O
difficulty	O
explaining	O
their	O
data	O
with	O
the	O
td	B
model	I
an	O
earlier	O
predictive	O
stimulus	O
takes	O
precedence	O
over	O
a	O
later	O
predictive	O
stimulus	O
because	O
like	O
all	O
the	O
prediction	B
methods	O
described	O
in	O
this	O
book	O
the	O
td	B
model	I
is	O
based	O
on	O
the	O
backing-up	O
or	O
bootstrapping	B
idea	O
updates	O
to	O
associative	O
strengths	O
shift	O
the	O
strengths	O
at	O
a	O
particular	O
state	B
toward	O
the	O
strength	O
at	O
later	O
states	O
another	O
consequence	O
of	O
bootstrapping	B
is	O
that	O
the	O
td	B
model	I
provides	O
an	O
account	O
of	O
higher-order	O
conditioning	O
a	O
feature	O
of	O
classical	B
conditioning	I
that	O
is	O
beyond	O
the	O
scope	O
of	O
the	O
rescoral-wagner	O
and	O
similar	O
models	O
as	O
we	O
described	O
above	O
higher-order	O
conditioning	O
is	O
the	O
phenomenon	O
in	O
which	O
a	O
previously-conditioned	O
cs	O
can	O
act	O
as	O
a	O
us	O
in	O
conditioning	O
another	O
initially	O
neutral	O
stimulus	O
to	O
the	O
right	O
is	O
shown	O
the	O
behavior	O
of	O
the	O
td	B
model	I
with	O
the	O
presence	O
representation	O
in	O
a	O
higherorder	O
conditioning	O
experiment	O
in	O
this	O
case	O
it	O
is	O
second-order	O
conditioning	O
in	O
the	O
first	O
phase	O
shown	O
in	O
the	O
figure	O
csb	O
is	O
trained	O
to	O
predict	O
a	O
us	O
so	O
that	O
its	O
associative	O
strength	O
increases	O
here	O
to	O
in	O
the	O
second	O
phase	O
csa	O
is	O
paired	O
with	O
csb	O
in	O
the	O
absence	O
of	O
the	O
us	O
in	O
the	O
sequential	O
arrangement	O
shown	O
at	O
the	O
top	O
of	O
the	O
figure	O
csa	O
acquires	O
associative	O
strength	O
even	O
though	O
it	O
is	O
never	O
paired	O
with	O
the	O
us	O
with	O
continued	O
training	O
csa	O
s	O
associative	O
strength	O
reaches	O
a	O
peak	O
and	O
then	O
decreases	O
because	O
the	O
associative	O
strength	O
of	O
csb	O
the	O
secondary	O
reinforcer	O
decreases	O
so	O
that	O
it	O
loses	O
its	O
ability	O
to	O
provide	O
secondary	B
reinforcement	I
csb	O
s	O
associative	O
strength	O
decreases	O
because	O
the	O
us	O
does	O
not	O
occur	O
in	O
these	O
higher-order	O
conditioning	O
trials	O
these	O
are	O
extinction	O
trials	O
for	O
csb	O
because	O
its	O
predictive	O
relationship	O
to	O
the	O
us	O
is	O
disrupted	O
so	O
that	O
its	O
ability	O
to	O
act	O
as	O
a	O
reinforcer	O
decreases	O
this	O
same	O
pattern	O
is	O
seen	O
in	O
animal	O
experiments	O
this	O
extinction	O
of	O
conditioned	O
reinforcement	O
in	O
higher-order	O
conditioning	O
trials	O
makes	O
it	O
difficult	O
to	O
demonstrate	O
higher-order	O
conditioning	O
unless	O
the	O
original	O
predictive	O
relationships	O
are	O
periodically	O
refreshed	O
by	O
occasionally	O
inserting	O
first-order	O
trials	O
figure	O
second-order	O
conditioning	O
with	O
the	O
td	B
model	I
csb	O
has	O
an	O
initial	O
associative	O
strength	O
of	O
at	O
the	O
beginning	O
of	O
the	O
simulation	O
adapted	O
from	O
sutton	O
and	O
barto	O
the	O
td	B
model	I
produces	O
an	O
analog	O
of	O
second-	O
and	B
higher-order	I
conditioning	I
because	O
vstwt	O
appears	O
in	O
the	O
td	B
error	I
t	O
this	O
means	O
that	O
as	O
a	O
result	O
of	O
previous	O
learning	O
can	O
differ	O
from	O
vstwt	O
making	O
t	O
non-zero	O
temporal	O
difference	O
this	O
difference	O
has	O
the	O
same	O
status	O
as	O
in	O
implying	O
that	O
as	O
far	O
as	O
learning	O
is	O
concerned	O
there	O
is	O
no	O
difference	O
between	O
a	O
temporal	O
difference	O
and	O
the	O
occurrence	O
of	O
a	O
us	O
in	O
fact	O
this	O
feature	O
of	O
the	O
td	B
algorithm	O
is	O
one	O
of	O
the	O
major	O
reasons	O
for	O
its	O
development	O
which	O
we	O
now	O
understand	O
through	O
its	O
connection	O
to	O
dynamic	O
w	O
chapter	O
psychology	B
programming	O
as	O
described	O
in	O
chapter	O
bootstrapping	B
values	O
is	O
intimately	O
related	O
to	O
second-order	O
and	B
higher-order	I
conditioning	I
in	O
the	O
examples	O
of	O
the	O
td	B
model	I
s	O
behavior	O
described	O
above	O
we	O
examined	O
only	O
the	O
changes	O
in	O
the	O
associative	O
strengths	O
of	O
the	O
cs	O
components	O
we	O
did	O
not	O
look	O
at	O
what	O
the	O
model	O
predicts	O
about	O
properties	O
of	O
an	O
animal	O
s	O
conditioned	O
responses	O
their	O
timing	O
shape	O
and	O
how	O
they	O
develop	O
over	O
conditioning	O
trials	O
these	O
properties	O
depend	O
on	O
the	O
species	O
the	O
response	O
system	O
being	O
observed	O
and	O
parameters	O
of	O
the	O
conditioning	O
trials	O
but	O
in	O
many	O
experiments	O
with	O
different	O
animals	O
and	O
different	O
response	O
systems	O
the	O
magnitude	O
of	O
the	O
cr	O
or	O
the	O
probability	O
of	O
a	O
cr	O
increases	O
as	O
the	O
expected	B
time	O
of	O
the	O
us	O
approaches	O
for	O
example	O
in	O
classical	B
conditioning	I
of	O
a	O
rabbit	O
s	O
nictitating	O
membrane	O
response	O
that	O
we	O
mentioned	O
above	O
over	O
conditioning	O
trials	O
the	O
delay	O
from	O
cs	O
onset	O
to	O
when	O
the	O
nictitating	O
membrane	O
begins	O
to	O
move	O
across	O
the	O
eye	O
decreases	O
over	O
trials	O
and	O
the	O
amplitude	O
of	O
this	O
anticipatory	O
closure	O
gradually	O
increases	O
over	O
the	O
interval	O
between	O
the	O
cs	O
and	O
the	O
us	O
until	O
the	O
membrane	O
reaches	O
maximal	O
closure	O
at	O
the	O
expected	B
time	O
of	O
the	O
us	O
the	O
timing	O
and	O
shape	O
of	O
this	O
cr	O
is	O
critical	O
to	O
its	O
adaptive	O
significance	O
covering	O
the	O
eye	O
too	O
early	O
reduces	O
vision	O
though	O
the	O
nictitating	O
membrane	O
is	O
translucent	O
while	O
covering	O
it	O
too	O
late	O
is	O
of	O
little	O
protective	O
value	B
capturing	O
cr	O
features	O
like	O
these	O
is	O
challenging	O
for	O
models	O
of	O
classical	B
conditioning	I
the	O
td	B
model	I
does	O
not	O
include	O
as	O
part	O
of	O
its	O
definition	O
any	O
mechanism	O
for	O
translating	O
the	O
time	O
course	O
of	O
the	O
us	O
prediction	B
vstwt	O
into	O
a	O
profile	O
that	O
can	O
be	O
compared	O
with	O
the	O
properties	O
of	O
an	O
animal	O
s	O
cr	O
the	O
simplest	O
choice	O
is	O
to	O
let	O
the	O
time	O
course	O
of	O
a	O
simulated	O
cr	O
equal	O
the	O
time	O
course	O
of	O
the	O
us	O
prediction	B
in	O
this	O
case	O
features	O
of	O
simulated	O
crs	O
and	O
how	O
they	O
change	O
over	O
trials	O
depend	O
only	O
on	O
the	O
stimulus	O
representation	O
chosen	O
and	O
the	O
values	O
of	O
the	O
model	O
s	O
parameters	O
and	O
figure	O
shows	O
the	O
time	O
courses	O
of	O
us	O
predictions	O
at	O
different	O
points	O
during	O
learning	O
with	O
the	O
three	O
representations	O
shown	O
in	O
figure	O
for	O
these	O
simulations	O
the	O
us	O
occurred	O
time	O
steps	O
after	O
the	O
onset	O
of	O
the	O
cs	O
and	O
and	O
with	O
the	O
csc	O
representation	O
left	O
the	O
curve	O
of	O
the	O
us	O
prediction	B
formed	O
by	O
the	O
td	B
model	I
increases	O
exponentially	O
throughout	O
the	O
interval	O
between	O
the	O
cs	O
and	O
the	O
us	O
until	O
it	O
reaches	O
a	O
maximum	O
exactly	O
when	O
the	O
us	O
occurs	O
time	O
step	O
this	O
exponential	O
increase	O
is	O
the	O
result	O
of	O
discounting	B
in	O
the	O
td	B
model	I
learning	O
rule	O
with	O
the	O
presence	O
representation	O
middle	O
the	O
us	O
prediction	B
is	O
nearly	O
constant	O
while	O
the	O
stimulus	O
is	O
present	O
because	O
there	O
is	O
only	O
one	O
weight	O
or	O
associative	O
strength	O
to	O
be	O
learned	O
for	O
each	O
stimulus	O
consequently	O
the	O
td	B
model	I
with	O
the	O
presence	O
representation	O
cannot	O
recreate	O
many	O
features	O
of	O
cr	O
timing	O
with	O
an	O
ms	O
representation	O
right	O
the	O
development	O
of	O
the	O
td	B
model	I
s	O
us	O
prediction	B
is	O
more	O
complicated	O
after	O
trials	O
the	O
prediction	B
s	O
profile	O
is	O
a	O
reasonable	O
approximation	O
of	O
the	O
us	O
prediction	B
curve	O
produced	O
with	O
the	O
csc	O
representation	O
the	O
us	O
prediction	B
curves	O
shown	O
in	O
figure	O
were	O
not	O
intended	O
to	O
precisely	O
match	O
profiles	O
of	O
crs	O
as	O
they	O
develop	O
during	O
conditioning	O
in	O
any	O
particular	O
animal	O
experiment	O
but	O
they	O
illustrate	O
the	O
strong	O
influence	O
that	O
the	O
stimulus	O
representation	O
has	O
on	O
predictions	O
derived	O
from	O
the	O
td	B
model	I
further	O
although	O
we	O
can	O
only	O
mention	O
it	O
here	O
how	O
the	O
stimulus	O
representation	O
interacts	O
with	O
discounting	B
and	B
eligibility	B
traces	I
is	O
important	O
in	O
determining	O
properties	O
of	O
the	O
us	O
prediction	B
profiles	O
produced	O
by	O
the	O
td	B
model	I
classical	B
conditioning	I
figure	O
time	O
course	O
of	O
us	O
prediction	B
over	O
the	O
course	O
of	O
acquisition	O
for	O
the	O
td	B
model	I
with	O
three	O
different	O
stimulus	O
representations	O
left	O
with	O
the	O
complete	O
serial	O
compound	B
the	O
us	O
prediction	B
increases	O
exponentially	O
through	O
the	O
interval	O
peaking	O
at	O
the	O
time	O
of	O
the	O
us	O
at	O
asymptote	O
the	O
us	O
prediction	B
peaks	O
at	O
the	O
us	O
intensity	O
in	O
these	O
simulations	O
middle	O
with	O
the	O
presence	O
representation	O
the	O
us	O
prediction	B
converges	O
to	O
an	O
almost	O
constant	O
level	O
this	O
constant	O
level	O
is	O
determined	O
by	O
the	O
us	O
intensity	O
and	O
the	O
length	O
of	O
the	O
cs	O
us	O
interval	O
right	O
with	O
the	O
microstimulus	O
representation	O
at	O
asymptote	O
the	O
td	B
model	I
approximates	O
the	O
exponentially	O
increasing	O
time	O
course	O
depicted	O
with	O
the	O
csc	O
through	O
a	O
linear	O
combination	O
of	O
the	O
different	O
microstimuli	O
adapted	O
with	O
minor	O
changes	O
from	O
learning	O
behavior	O
evaluating	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
volume	O
e	O
a	O
ludvig	O
r	O
s	O
sutton	O
e	O
j	O
kehoe	O
with	O
permission	O
of	O
springer	O
another	O
dimension	O
beyond	O
what	O
we	O
can	O
discuss	O
here	O
is	O
the	O
influence	O
of	O
different	O
responsegeneration	O
mechanisms	O
that	O
translate	O
us	O
predictions	O
into	O
cr	O
profiles	O
the	O
profiles	O
shown	O
in	O
figure	O
are	O
raw	O
us	O
prediction	B
profiles	O
even	O
without	O
any	O
special	O
assumption	O
about	O
how	O
an	O
animal	O
s	O
brain	O
might	O
produce	O
overt	O
responses	O
from	O
us	O
predictions	O
however	O
the	O
profiles	O
in	O
figure	O
for	O
the	O
csc	O
and	O
ms	O
representations	O
increase	O
as	O
the	O
time	O
of	O
the	O
us	O
approaches	O
and	O
reach	O
a	O
maximum	O
at	O
the	O
time	O
of	O
the	O
us	O
as	O
is	O
seen	O
in	O
many	O
animal	O
conditioning	O
experiments	O
the	O
td	B
model	I
when	O
combined	O
with	O
particular	O
stimulus	O
representations	O
and	O
responsegeneration	O
mechanisms	O
is	O
able	O
to	O
account	O
for	O
a	O
surprisingly-wide	O
range	O
of	O
phenomena	O
observed	O
in	O
animal	O
classical	B
conditioning	I
experiments	O
but	O
it	O
is	O
far	O
from	O
being	O
a	O
perfect	O
model	O
to	O
generate	O
other	O
details	O
of	O
classical	B
conditioning	I
the	O
model	O
needs	O
to	O
be	O
extended	O
perhaps	O
by	O
adding	O
model-based	O
elements	O
and	O
mechanisms	O
for	O
adaptively	O
altering	O
some	O
of	O
its	O
parameters	O
other	O
approaches	O
to	O
modeling	O
classical	B
conditioning	I
depart	O
significantly	O
from	O
the	O
rescorla	O
wagner-style	O
error-correction	O
process	O
bayesian	O
models	O
for	O
example	O
work	O
within	O
a	O
probabilistic	O
framework	O
in	O
which	O
experience	O
revises	O
probability	O
estimates	O
all	O
of	O
these	O
models	O
usefully	O
contribute	O
to	O
our	O
understanding	O
of	O
classical	B
conditioning	I
perhaps	O
the	O
most	O
notable	O
feature	O
of	O
the	O
td	B
model	I
is	O
that	O
it	O
is	O
based	O
on	O
a	O
theory	O
the	O
theory	O
we	O
have	O
described	O
in	O
this	O
book	O
that	O
suggests	O
an	O
account	O
of	O
what	O
an	O
animal	O
s	O
nervous	O
system	O
is	O
trying	O
to	O
do	O
while	O
undergoing	O
conditioning	O
it	O
is	O
trying	O
to	O
form	O
accurate	O
long-term	O
predictions	O
consistent	O
with	O
the	O
limitations	O
imposed	O
by	O
the	O
way	O
stimuli	O
are	O
represented	O
and	O
how	O
the	O
nervous	O
system	O
works	O
in	O
other	O
words	O
it	O
suggests	O
a	O
normative	O
account	O
of	O
classical	B
conditioning	I
in	O
which	O
long-term	O
instead	O
of	O
immediate	O
prediction	B
is	O
v	O
v	O
v	O
a	O
key	O
feature	O
chapter	O
psychology	B
the	O
development	O
of	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
is	O
one	O
instance	O
in	O
which	O
the	O
explicit	O
goal	O
was	O
to	O
model	O
some	O
of	O
the	O
details	O
of	O
animal	O
learning	O
behavior	O
in	O
addition	O
to	O
its	O
standing	O
as	O
an	O
algorithm	O
then	O
td	B
learning	O
is	O
also	O
the	O
basis	O
of	O
this	O
model	O
of	O
aspects	O
of	O
biological	O
learning	O
as	O
we	O
discuss	O
in	O
chapter	O
td	B
learning	O
has	O
also	O
turned	O
out	O
to	O
underlie	O
an	O
influential	O
model	O
of	O
the	O
activity	O
of	O
neurons	O
that	O
produce	O
dopamine	B
a	O
chemical	O
in	O
the	O
brain	O
of	O
mammals	O
that	O
is	O
deeply	O
involved	O
in	O
reward	O
processing	O
these	O
are	O
instances	O
in	O
which	O
reinforcement	B
learning	I
theory	O
makes	O
detailed	O
contact	O
with	O
animal	O
behavioral	O
and	O
neural	B
data	O
we	O
now	O
turn	O
to	O
considering	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
animal	O
behavior	O
in	O
instrumental	O
conditioning	O
experiments	O
the	O
other	O
major	O
type	O
of	O
laboratory	O
experiment	O
studied	O
by	O
animal	O
learning	O
psychologists	O
instrumental	O
conditioning	O
in	O
instrumental	O
conditioning	O
experiments	O
learning	O
depends	O
on	O
the	O
consequences	O
of	O
behavior	O
the	O
delivery	O
of	O
a	O
reinforcing	O
stimulus	O
is	O
contingent	O
on	O
what	O
the	O
animal	O
does	O
in	O
classical	B
conditioning	I
experiments	O
in	O
contrast	O
the	O
reinforcing	O
stimulus	O
the	O
us	O
is	O
delivered	O
independently	O
of	O
the	O
animal	O
s	O
behavior	O
instrumental	O
conditioning	O
is	O
usually	O
considered	O
to	O
be	O
the	O
same	O
as	O
operant	O
conditioning	O
the	O
term	O
b	O
f	O
skinner	B
introduced	O
for	O
experiments	O
with	O
behavior-contingent	O
reinforcement	O
though	O
the	O
experiments	O
and	O
theories	O
of	O
those	O
who	O
use	O
these	O
two	O
terms	O
differ	O
in	O
a	O
number	O
of	O
ways	O
some	O
of	O
which	O
we	O
touch	O
on	O
below	O
we	O
will	O
exclusively	O
use	O
the	O
term	O
instrumental	O
conditioning	O
for	O
experiments	O
in	O
which	O
reinforcement	O
is	O
contingent	O
upon	O
behavior	O
the	O
roots	O
of	O
instrumental	O
conditioning	O
go	O
back	O
to	O
experiments	O
performed	O
by	O
the	O
american	O
psychologist	O
edward	O
thorndike	O
one	O
hundred	O
years	O
before	O
publication	O
of	O
the	O
first	O
edition	O
of	O
this	O
book	O
thorndike	O
observed	O
the	O
behavior	O
of	O
cats	O
when	O
they	O
were	O
placed	O
in	O
puzzle	O
boxes	B
such	O
as	O
the	O
one	O
at	O
the	O
right	O
from	O
which	O
they	O
could	O
escape	O
by	O
appropriate	O
actions	O
for	O
example	O
a	O
cat	O
could	O
open	O
the	O
door	O
of	O
one	O
box	O
by	O
performing	O
a	O
sequence	O
of	O
three	O
separate	O
actions	O
depressing	O
a	O
platform	O
at	O
the	O
back	O
of	O
the	O
box	O
pulling	O
a	O
string	O
by	O
clawing	O
at	O
it	O
and	O
pushing	O
a	O
bar	O
up	O
or	O
down	O
when	O
first	O
placed	O
in	O
a	O
puzzle	O
box	O
with	O
food	O
visible	O
outside	O
all	O
but	O
a	O
few	O
of	O
thorndike	O
s	O
cats	O
displayed	O
evident	O
signs	O
of	O
discomfort	O
and	O
extraordinarily	O
vigorous	O
activity	O
to	O
strive	O
instinctively	O
to	O
escape	O
from	O
confinement	O
in	O
experiments	O
with	O
different	O
cats	O
and	O
one	O
of	O
thorndike	O
s	O
puzzle	O
boxes	B
reprinted	O
from	O
thorndike	O
animal	O
intelligence	O
an	O
experimental	O
study	O
of	O
the	O
associative	O
processes	O
in	O
animals	O
the	O
psychological	O
review	O
series	O
of	O
monograph	O
supplements	O
macmillan	O
new	O
york	O
boxes	B
with	O
different	O
escape	O
mechanisms	O
thorndike	O
recorded	O
the	O
amounts	O
of	O
time	O
each	O
cat	O
took	O
to	O
escape	O
over	O
multiple	O
experiences	O
in	O
each	O
box	O
he	O
observed	O
that	O
the	O
time	O
instrumental	O
conditioning	O
almost	O
invariably	O
decreased	O
with	O
successive	O
experiences	O
for	O
example	O
from	O
seconds	O
to	O
or	O
seconds	O
he	O
described	O
cats	O
behavior	O
in	O
a	O
puzzle	O
box	O
like	O
this	O
the	O
cat	O
that	O
is	O
clawing	O
all	O
over	O
the	O
box	O
in	O
her	O
impulsive	O
struggle	O
will	O
probably	O
claw	O
the	O
string	O
or	O
loop	O
or	O
button	O
so	O
as	O
to	O
open	O
the	O
door	O
and	O
gradually	O
all	O
the	O
other	O
non-successful	O
impulses	O
will	O
be	O
stamped	O
out	O
and	O
the	O
particular	O
impulse	O
leading	O
to	O
the	O
successful	O
act	O
will	O
be	O
stamped	O
in	O
by	O
the	O
resulting	O
pleasure	O
until	O
after	O
many	O
trials	O
the	O
cat	O
will	O
when	O
put	O
in	O
the	O
box	O
immediately	O
claw	O
the	O
button	O
or	O
loop	O
in	O
a	O
definite	O
way	O
p	O
these	O
and	O
other	O
experiments	O
with	O
dogs	O
chicks	O
monkeys	O
and	O
even	O
fish	O
led	O
thorndike	O
to	O
formulate	O
a	O
number	O
of	O
laws	O
of	O
learning	O
the	O
most	O
influential	O
being	O
the	O
law	B
of	I
effect	I
a	O
version	O
of	O
which	O
we	O
quoted	O
in	O
chapter	O
this	O
law	O
describes	O
what	O
is	O
generally	O
known	O
as	O
learning	O
by	O
trial	O
and	O
error	O
as	O
mentioned	O
in	O
chapter	O
many	O
aspects	O
of	O
the	O
law	B
of	I
effect	I
have	O
generated	O
controversy	O
and	O
its	O
details	O
have	O
been	O
modified	O
over	O
the	O
years	O
still	O
the	O
law	O
in	O
one	O
form	O
or	O
another	O
expresses	O
an	O
enduring	O
principle	O
of	O
learning	O
essential	O
features	O
of	O
reinforcement	B
learning	I
algorithms	O
correspond	O
to	O
features	O
of	O
animal	O
learning	O
described	O
by	O
the	O
law	B
of	I
effect	I
first	O
reinforcement	B
learning	I
algorithms	O
are	O
selectional	O
meaning	O
that	O
they	O
try	O
alternatives	O
and	O
select	O
among	O
them	O
by	O
comparing	O
their	O
consequences	O
second	O
reinforcement	B
learning	I
algorithms	O
are	O
associative	O
meaning	O
that	O
the	O
alternatives	O
found	O
by	O
selection	O
are	O
associated	O
with	O
particular	O
situations	O
or	O
states	O
to	O
form	O
the	O
agent	O
s	O
policy	B
like	O
learning	O
described	O
by	O
the	O
law	B
of	I
effect	I
reinforcement	B
learning	I
is	O
not	O
just	O
the	O
process	O
of	O
finding	O
actions	O
that	O
produce	O
a	O
lot	O
of	O
reward	O
but	O
also	O
of	O
connecting	O
these	O
actions	O
to	O
situations	O
or	O
states	O
thorndike	O
used	O
the	O
phrase	O
learning	O
by	O
selecting	O
and	O
connecting	O
natural	O
selection	O
in	O
evolution	B
is	O
a	O
prime	O
example	O
of	O
a	O
selectional	O
process	O
but	O
it	O
is	O
not	O
associative	O
least	O
as	O
it	O
is	O
commonly	O
understood	O
supervised	B
learning	I
is	O
associative	O
but	O
it	O
is	O
not	O
selectional	O
because	O
it	O
relies	O
on	O
instructions	O
that	O
directly	O
tell	O
the	O
agent	O
how	O
to	O
change	O
its	O
behavior	O
in	O
computational	O
terms	O
the	O
law	B
of	I
effect	I
describes	O
an	O
elementary	O
way	O
of	O
combining	O
search	O
and	O
memory	O
search	O
in	O
the	O
form	O
of	O
trying	O
and	O
selecting	O
among	O
many	O
actions	O
in	O
each	O
situation	O
and	O
memory	O
in	O
the	O
form	O
of	O
associations	O
linking	O
situations	O
with	O
the	O
actions	O
found	O
so	O
far	O
to	O
work	O
best	O
in	O
those	O
situations	O
search	O
and	O
memory	O
are	O
essential	O
components	O
of	O
all	O
reinforcement	B
learning	I
algorithms	O
whether	O
memory	O
takes	O
the	O
form	O
of	O
an	O
agent	O
s	O
policy	B
value	B
function	I
or	O
environment	B
model	O
a	O
reinforcement	B
learning	I
algorithm	O
s	O
need	O
to	O
search	O
means	O
that	O
it	O
has	O
to	O
explore	O
in	O
some	O
way	O
animals	O
clearly	O
explore	O
as	O
well	O
and	O
early	O
animal	O
learning	O
researchers	O
disagreed	O
about	O
the	O
degree	O
of	O
guidance	O
an	O
animal	O
uses	O
in	O
selecting	O
its	O
actions	O
in	O
situations	O
like	O
thorndike	O
s	O
puzzle	O
boxes	B
are	O
actions	O
the	O
result	O
of	O
absolutely	O
random	O
blind	O
groping	O
p	O
or	O
is	O
there	O
some	O
degree	O
of	O
guidance	O
either	O
from	O
prior	O
learning	O
reasoning	O
or	O
other	O
means	O
although	O
some	O
thinkers	O
including	O
thorndike	O
seem	O
to	O
have	O
taken	O
the	O
former	O
position	O
others	O
favored	O
more	O
deliberate	O
exploration	O
reinforcement	B
learning	I
algorithms	O
allow	O
wide	O
latitude	O
for	O
how	O
much	O
guidance	O
an	O
agent	O
can	O
employ	O
in	O
selecting	O
actions	O
the	O
forms	O
of	O
exploration	O
we	O
have	O
used	O
in	O
the	O
algorithms	O
presented	O
in	O
this	O
book	O
such	O
as	O
and	O
upper-confidence-bound	O
action	B
selection	O
are	O
merely	O
chapter	O
psychology	B
among	O
the	O
simplest	O
more	O
sophisticated	O
methods	O
are	O
possible	O
with	O
the	O
only	O
stipulation	O
being	O
that	O
there	O
has	O
to	O
be	O
some	O
form	O
of	O
exploration	O
for	O
the	O
algorithms	O
to	O
work	O
effectively	O
the	O
feature	O
of	O
our	O
treatment	O
of	O
reinforcement	B
learning	I
allowing	O
the	O
set	O
of	O
actions	O
available	O
at	O
any	O
time	O
to	O
depend	O
on	O
the	O
environment	B
s	O
current	O
state	B
echoes	O
something	O
thorndike	O
observed	O
in	O
his	O
cats	O
puzzle-box	O
behaviors	O
the	O
cats	O
selected	O
actions	O
from	O
those	O
that	O
they	O
instinctively	O
perform	O
in	O
their	O
current	O
situation	O
which	O
thorndike	O
called	O
their	O
instinctual	O
impulses	O
first	O
placed	O
in	O
a	O
puzzle	O
box	O
a	O
cat	O
instinctively	O
scratches	O
claws	O
and	O
bites	O
with	O
great	O
energy	O
a	O
cat	O
s	O
instinctual	O
responses	O
to	O
finding	O
itself	O
in	O
a	O
confined	O
space	O
successful	O
actions	O
are	O
selected	O
from	O
these	O
and	O
not	O
from	O
every	O
possible	O
action	B
or	O
activity	O
this	O
is	O
like	O
the	O
feature	O
of	O
our	O
formalism	O
where	O
the	O
action	B
selected	O
from	O
a	O
state	B
s	O
belongs	O
to	O
a	O
set	O
of	O
admissible	O
actions	O
as	O
specifying	O
these	O
sets	O
is	O
an	O
important	O
aspect	O
of	O
reinforcement	B
learning	I
because	O
it	O
can	O
radically	O
simplify	O
learning	O
they	O
are	O
like	O
an	O
animal	O
s	O
instinctual	O
impulses	O
on	O
the	O
other	O
hand	O
thorndike	O
s	O
cats	O
might	O
have	O
been	O
exploring	O
according	O
to	O
an	O
instinctual	O
context-specific	O
ordering	O
over	O
actions	O
rather	O
than	O
by	O
just	O
selecting	O
from	O
a	O
set	O
of	O
instinctual	O
impulses	O
this	O
is	O
another	O
way	O
to	O
make	O
reinforcement	B
learning	I
easier	O
among	O
the	O
most	O
prominent	O
animal	O
learning	O
researchers	O
influenced	O
by	O
the	O
law	B
of	I
effect	I
were	O
clark	O
hull	B
hull	B
and	O
b	O
f	O
skinner	B
skinner	B
at	O
the	O
center	O
of	O
their	O
research	O
was	O
the	O
idea	O
of	O
selecting	O
behavior	O
on	O
the	O
basis	O
of	O
its	O
consequences	O
reinforcement	B
learning	I
has	O
features	O
in	O
common	O
with	O
hull	B
s	O
theory	O
which	O
included	O
eligibility-like	O
mechanisms	O
and	O
secondary	B
reinforcement	I
to	O
account	O
for	O
the	O
ability	O
to	O
learn	O
when	O
there	O
is	O
a	O
significant	O
time	O
interval	O
between	O
an	O
action	B
and	O
the	O
consequent	O
reinforcing	O
stimulus	O
section	O
randomness	O
also	O
played	O
a	O
role	O
in	O
hull	B
s	O
theory	O
through	O
what	O
he	O
called	O
behavioral	O
oscillation	O
to	O
introduce	O
exploratory	O
behavior	O
skinner	B
did	O
not	O
fully	O
subscribe	O
to	O
the	O
memory	O
aspect	O
of	O
the	O
law	B
of	I
effect	I
being	O
averse	O
to	O
the	O
idea	O
of	O
associative	O
linkages	O
he	O
instead	O
emphasized	O
selection	O
from	O
spontaneouslyemitted	O
behavior	O
he	O
introduced	O
the	O
term	O
operant	O
to	O
emphasize	O
the	O
key	O
role	O
of	O
an	O
action	B
s	O
effects	O
on	O
an	O
animal	O
s	O
environment	B
unlike	O
the	O
experiments	O
of	O
thorndike	O
and	O
others	O
which	O
consisted	O
of	O
sequences	O
of	O
separate	O
trials	O
skinner	B
s	O
operant	O
conditioning	O
experiments	O
allowed	O
animal	O
subjects	O
to	O
behave	O
for	O
extended	O
periods	O
of	O
time	O
without	O
interruption	O
he	O
invented	O
the	O
operant	O
conditioning	O
chamber	O
now	O
called	O
a	O
skinner	B
box	O
the	O
most	O
basic	O
version	O
of	O
which	O
contains	O
a	O
lever	O
or	O
key	O
that	O
an	O
animal	O
can	O
press	O
to	O
obtain	O
a	O
reward	O
such	O
as	O
food	O
or	O
water	O
which	O
would	O
be	O
delivered	O
according	O
to	O
a	O
welldefined	O
rule	O
called	O
a	O
reinforcement	O
schedule	O
by	O
recording	O
the	O
cumulative	O
number	O
of	O
lever	O
presses	O
as	O
a	O
function	O
of	O
time	O
skinner	B
and	O
his	O
followers	O
could	O
investigate	O
the	O
effect	O
of	O
different	O
reinforcement	O
schedules	O
on	O
the	O
animal	O
s	O
rate	O
of	O
lever-pressing	O
modeling	O
results	O
from	O
experiments	O
likes	O
these	O
using	O
the	O
reinforcement	B
learning	I
principles	O
we	O
present	O
in	O
this	O
book	O
is	O
not	O
well	O
developed	O
but	O
we	O
mention	O
some	O
exceptions	O
in	O
the	O
bibliographic	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
another	O
of	O
skinner	B
s	O
contributions	O
resulted	O
from	O
his	O
recognition	O
of	O
the	O
effectiveness	O
of	O
training	O
an	O
animal	O
by	O
reinforcing	O
successive	O
approximations	O
of	O
the	O
desired	O
behavior	O
a	O
process	O
he	O
called	O
shaping	B
although	O
this	O
technique	O
had	O
been	O
used	O
by	O
others	O
including	O
skinner	B
himself	O
its	O
significance	O
was	O
impressed	O
upon	O
him	O
when	O
he	O
and	O
colleagues	O
were	O
attempting	O
to	O
train	O
a	O
pigeon	O
to	O
bowl	O
by	O
swiping	O
a	O
wooden	O
ball	O
with	O
its	O
beak	O
after	O
instrumental	O
conditioning	O
waiting	O
for	O
a	O
long	O
time	O
without	O
seeing	O
any	O
swipe	O
that	O
they	O
could	O
reinforce	B
they	O
decided	O
to	O
reinforce	B
any	O
response	O
that	O
had	O
the	O
slightest	O
resemblance	O
to	O
a	O
swipe	O
perhaps	O
at	O
first	O
merely	O
the	O
behavior	O
of	O
looking	O
at	O
the	O
ball	O
and	O
then	O
to	O
select	O
responses	O
which	O
more	O
closely	O
approximated	O
the	O
final	O
form	O
the	O
result	O
amazed	O
us	O
in	O
a	O
few	O
minutes	O
the	O
ball	O
was	O
caroming	O
off	O
the	O
walls	O
of	O
the	O
box	O
as	O
if	O
the	O
pigeon	O
had	O
been	O
a	O
champion	O
squash	O
player	O
p	O
not	O
only	O
did	O
the	O
pigeon	O
learn	O
a	O
behavior	O
that	O
is	O
unusual	O
for	O
pigeons	O
it	O
learned	O
quickly	O
through	O
an	O
interactive	O
process	O
in	O
which	O
its	O
behavior	O
and	O
the	O
reinforcement	O
contingencies	O
changed	O
in	O
response	O
to	O
each	O
other	O
skinner	B
compared	O
the	O
process	O
of	O
altering	O
reinforcement	O
contingencies	O
to	O
the	O
work	O
of	O
a	O
sculptor	O
shaping	B
clay	O
into	O
a	O
desired	O
form	O
shaping	B
is	O
a	O
powerful	O
technique	O
for	O
computational	O
reinforcement	B
learning	I
systems	O
as	O
well	O
when	O
it	O
is	O
difficult	O
for	O
an	O
agent	O
to	O
receive	O
any	O
non-zero	O
reward	B
signal	I
at	O
all	O
either	O
due	O
to	O
sparseness	O
of	O
rewarding	O
situations	O
or	O
their	O
inaccessibility	O
given	O
initial	O
behavior	O
starting	O
with	O
an	O
easier	O
problem	O
and	O
incrementally	O
increasing	O
its	O
difficulty	O
as	O
the	O
agent	O
learns	O
can	O
be	O
an	O
effective	O
and	O
sometimes	O
indispensable	O
strategy	O
a	O
concept	O
from	O
psychology	B
that	O
is	O
especially	O
relevant	O
in	O
the	O
context	O
of	O
instrumental	O
conditioning	O
is	O
motivation	B
which	O
refers	O
to	O
processes	O
that	O
influence	O
the	O
direction	O
and	O
strength	O
or	O
vigor	O
of	O
behavior	O
thorndike	O
s	O
cats	O
for	O
example	O
were	O
motivated	O
to	O
escape	O
from	O
puzzle	O
boxes	B
because	O
they	O
wanted	O
the	O
food	O
that	O
was	O
sitting	O
just	O
outside	O
obtaining	O
this	O
goal	O
was	O
rewarding	O
to	O
them	O
and	O
reinforced	O
the	O
actions	O
allowing	O
them	O
to	O
escape	O
it	O
is	O
difficult	O
to	O
link	O
the	O
concept	O
of	O
motivation	B
which	O
has	O
many	O
dimensions	O
in	O
a	O
precise	O
way	O
to	O
reinforcement	B
learning	I
s	O
computational	O
perspective	O
but	O
there	O
are	O
clear	O
links	O
with	O
some	O
of	O
its	O
dimensions	O
in	O
one	O
sense	O
a	O
reinforcement	B
learning	I
agent	O
s	O
reward	B
signal	I
is	O
at	O
the	O
base	O
of	O
its	O
motivation	B
the	O
agent	O
is	O
motivated	O
to	O
maximize	O
the	O
total	O
reward	O
it	O
receives	O
over	O
the	O
long	O
run	O
a	O
key	O
facet	O
of	O
motivation	B
then	O
is	O
what	O
makes	O
an	O
agent	O
s	O
experience	O
rewarding	O
in	O
reinforcement	B
learning	I
reward	O
signals	O
depend	O
on	O
the	O
state	B
of	O
the	O
reinforcement	B
learning	I
agent	O
s	O
environment	B
and	O
the	O
agent	O
s	O
actions	O
further	O
as	O
pointed	O
out	O
in	O
chapter	O
the	O
state	B
of	O
the	O
agent	O
s	O
environment	B
not	O
only	O
includes	O
information	O
about	O
what	O
is	O
external	O
to	O
the	O
machine	O
like	O
an	O
organism	O
or	O
a	O
robot	O
that	O
houses	O
the	O
agent	O
but	O
also	O
what	O
is	O
internal	O
to	O
this	O
machine	O
some	O
internal	O
state	B
components	O
correspond	O
to	O
what	O
psychologists	O
call	O
an	O
animal	O
s	O
motivational	O
state	B
which	O
influences	O
what	O
is	O
rewarding	O
to	O
the	O
animal	O
for	O
example	O
an	O
animal	O
will	O
be	O
more	O
rewarded	O
by	O
eating	O
when	O
it	O
is	O
hungry	O
than	O
when	O
it	O
has	O
just	O
finished	O
a	O
satisfying	O
meal	O
the	O
concept	O
of	O
state	B
dependence	O
is	O
broad	O
enough	O
to	O
allow	O
for	O
many	O
types	O
of	O
modulating	O
influences	O
on	O
the	O
generation	O
of	O
reward	O
signals	O
value	B
functions	O
provide	O
a	O
further	O
link	O
to	O
psychologists	O
concept	O
of	O
motivation	B
if	O
the	O
most	O
basic	O
motive	O
for	O
selecting	O
an	O
action	B
is	O
to	O
obtain	O
as	O
much	O
reward	O
as	O
possible	O
for	O
a	O
reinforcement	B
learning	I
agent	O
that	O
selects	O
actions	O
using	O
a	O
value	B
function	I
a	O
more	O
proximal	O
motive	O
is	O
to	O
ascend	O
the	O
gradient	B
of	O
its	O
value	B
function	I
that	O
is	O
to	O
select	O
actions	O
expected	B
to	O
lead	O
to	O
the	O
most	O
highly-valued	O
next	O
states	O
what	O
is	O
essentially	O
the	O
same	O
thing	O
to	O
select	O
actions	O
with	O
the	O
greatest	O
action-values	O
for	O
these	O
agents	O
value	B
functions	O
are	O
the	O
main	O
driving	O
force	O
determining	O
the	O
direction	O
of	O
their	O
behavior	O
chapter	O
psychology	B
another	O
dimension	O
of	O
motivation	B
is	O
that	O
an	O
animal	O
s	O
motivational	O
state	B
not	O
only	O
influences	O
learning	O
but	O
also	O
influences	O
the	O
strength	O
or	O
vigor	O
of	O
the	O
animal	O
s	O
behavior	O
after	O
learning	O
for	O
example	O
after	O
learning	O
to	O
find	O
food	O
in	O
the	O
goal	O
box	O
of	O
a	O
maze	O
a	O
hungry	O
rat	O
will	O
run	O
faster	O
to	O
the	O
goal	O
box	O
than	O
one	O
that	O
is	O
not	O
hungry	O
this	O
aspect	O
of	O
motivation	B
does	O
not	O
link	O
so	O
cleanly	O
to	O
the	O
reinforcement	B
learning	I
framework	O
we	O
present	O
here	O
but	O
in	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
we	O
cite	O
several	O
publications	O
that	O
propose	O
theories	O
of	O
behavioral	O
vigor	O
based	O
on	O
reinforcement	B
learning	I
we	O
turn	O
now	O
to	O
the	O
subject	O
of	O
learning	O
when	O
reinforcing	O
stimuli	O
occur	O
well	O
after	O
the	O
events	O
they	O
reinforce	B
the	O
mechanisms	O
used	O
by	O
reinforcement	B
learning	I
algorithms	O
to	O
enable	O
learning	O
with	O
delayed	B
reinforcement	I
eligibility	B
traces	I
and	B
td	B
learning	I
closely	O
correspond	O
to	O
psychologists	O
hypotheses	O
about	O
how	O
animals	O
can	O
learn	O
under	O
these	O
conditions	O
delayed	B
reinforcement	I
the	O
law	B
of	I
effect	I
requires	O
a	O
backward	O
effect	O
on	O
connections	O
and	O
some	O
early	O
critics	O
of	O
the	O
law	O
could	O
not	O
conceive	O
of	O
how	O
the	O
present	O
could	O
affect	O
something	O
that	O
was	O
in	O
the	O
past	O
this	O
concern	O
was	O
amplified	O
by	O
the	O
fact	O
that	O
learning	O
can	O
even	O
occur	O
when	O
there	O
is	O
a	O
considerable	O
delay	O
between	O
an	O
action	B
and	O
the	O
consequent	O
reward	O
or	O
penalty	O
similarly	O
in	O
classical	B
conditioning	I
learning	O
can	O
occur	O
when	O
us	O
onset	O
follows	O
cs	O
offset	O
by	O
a	O
non-negligible	O
time	O
interval	O
we	O
call	O
this	O
the	O
problem	O
of	O
delayed	B
reinforcement	I
which	O
is	O
related	O
to	O
what	O
minsky	B
called	O
the	O
credit-assignment	O
problem	O
for	O
learning	O
systems	O
how	O
do	O
you	O
distribute	O
credit	O
for	O
success	O
among	O
the	O
many	O
decisions	O
that	O
may	O
have	O
been	O
involved	O
in	O
producing	O
it	O
the	O
reinforcement	B
learning	I
algorithms	O
presented	O
in	O
this	O
book	O
include	O
two	O
basic	O
mechanisms	O
for	O
addressing	O
this	O
problem	O
the	O
first	O
is	O
the	O
use	O
of	O
eligibility	B
traces	I
and	O
the	O
second	O
is	O
the	O
use	O
of	O
td	B
methods	O
to	O
learn	O
value	B
functions	O
that	O
provide	O
nearly	O
immediate	O
evaluations	O
of	O
actions	O
tasks	O
like	O
instrumental	O
conditioning	O
experiments	O
or	O
that	O
provide	O
immediate	O
prediction	B
targets	O
tasks	O
like	O
classical	B
conditioning	I
experiments	O
both	O
of	O
these	O
methods	O
correspond	O
to	O
similar	O
mechanisms	O
proposed	O
in	O
theories	O
of	O
animal	O
learning	O
pavlov	B
pointed	O
out	O
that	O
every	O
stimulus	O
must	O
leave	O
a	O
trace	O
in	O
the	O
nervous	O
system	O
that	O
persists	O
for	O
some	O
time	O
after	O
the	O
stimulus	O
ends	O
and	O
he	O
proposed	O
that	O
stimulus	O
traces	O
make	O
learning	O
possible	O
when	O
there	O
is	O
a	O
temporal	O
gap	O
between	O
the	O
cs	O
offset	O
and	O
the	O
us	O
onset	O
to	O
this	O
day	O
conditioning	O
under	O
these	O
conditions	O
is	O
called	O
trace	O
conditioning	O
assuming	O
a	O
trace	O
of	O
the	O
cs	O
remains	O
when	O
the	O
us	O
arrives	O
learning	O
occurs	O
through	O
the	O
simultaneous	O
presence	O
of	O
the	O
trace	O
and	O
the	O
us	O
we	O
discuss	O
some	O
proposals	O
for	O
trace	O
mechanisms	O
in	O
the	O
nervous	O
system	O
in	O
chapter	O
stimulus	O
traces	O
were	O
also	O
proposed	O
as	O
a	O
means	O
for	O
bridging	O
the	O
time	O
interval	O
between	O
actions	O
and	O
consequent	O
rewards	O
or	O
penalties	O
in	O
instrumental	O
conditioning	O
in	O
hull	B
s	O
influential	O
learning	O
theory	O
for	O
example	O
molar	O
stimulus	O
traces	O
accounted	O
for	O
what	O
he	O
called	O
an	O
animal	O
s	O
goal	O
gradient	B
a	O
description	O
of	O
how	O
the	O
maximum	O
strength	O
of	O
an	O
instrumentally-conditioned	O
response	O
decreases	O
with	O
increasing	O
delay	O
of	O
reinforcement	O
hull	B
hypothesized	O
that	O
an	O
animal	O
s	O
actions	O
leave	O
internal	O
stimuli	O
cognitive	B
maps	I
whose	O
traces	O
decay	O
exponentially	O
as	O
functions	O
of	O
time	O
since	O
an	O
action	B
was	O
taken	O
looking	O
at	O
the	O
animal	O
learning	O
data	O
available	O
at	O
the	O
time	O
he	O
hypothesized	O
that	O
the	O
traces	O
effectively	O
reach	O
zero	O
after	O
to	O
seconds	O
the	O
eligibility	B
traces	I
used	O
in	O
the	O
algorithms	O
described	O
in	O
this	O
book	O
are	O
like	O
hull	B
s	O
traces	O
they	O
are	O
decaying	O
traces	O
of	O
past	O
state	B
visitations	O
or	O
of	O
past	O
state	B
action	B
pairs	O
eligibility	B
traces	I
were	O
introduced	O
by	O
klopf	B
in	O
his	O
neuronal	O
theory	O
in	O
which	O
they	O
are	O
temporally-extended	O
traces	O
of	O
past	O
activity	O
at	O
synapses	O
the	O
connections	O
between	O
neurons	O
klopf	B
s	O
traces	O
are	O
more	O
complex	O
than	O
the	O
exponentially-decaying	O
traces	O
our	O
algorithms	O
use	O
and	O
we	O
discuss	O
this	O
more	O
when	O
we	O
take	O
up	O
his	O
theory	O
in	O
section	O
to	O
account	O
for	O
goal	O
gradients	O
that	O
extend	O
over	O
longer	O
time	O
periods	O
than	O
spanned	O
by	O
stimulus	O
traces	O
hull	B
proposed	O
that	O
longer	O
gradients	O
result	O
from	O
conditioned	O
reinforcement	O
passing	O
backwards	O
from	O
the	O
goal	O
a	O
process	O
acting	O
in	O
conjunction	O
with	O
his	O
molar	O
stimulus	O
traces	O
animal	O
experiments	O
showed	O
that	O
if	O
conditions	O
favor	O
the	O
development	O
of	O
conditioned	O
reinforcement	O
during	O
a	O
delay	O
period	O
learning	O
does	O
not	O
decrease	O
with	O
increased	O
delay	O
as	O
much	O
as	O
it	O
does	O
under	O
conditions	O
that	O
obstruct	O
secondary	B
reinforcement	I
conditioned	O
reinforcement	O
is	O
favored	O
if	O
there	O
are	O
stimuli	O
that	O
regularly	O
occur	O
during	O
the	O
delay	O
interval	O
then	O
it	O
is	O
as	O
if	O
reward	O
is	O
not	O
actually	O
delayed	O
because	O
there	O
is	O
more	O
immediate	O
conditioned	O
reinforcement	O
hull	B
therefore	O
envisioned	O
that	O
there	O
is	O
a	O
primary	O
gradient	B
based	O
on	O
the	O
delay	O
of	O
the	O
primary	O
reinforcement	O
mediated	O
by	O
stimulus	O
traces	O
and	O
that	O
this	O
is	O
progressively	O
modified	O
and	O
lengthened	O
by	O
conditioned	O
reinforcement	O
algorithms	O
presented	O
in	O
this	O
book	O
that	O
use	O
both	O
eligibility	B
traces	I
and	O
value	B
functions	O
to	O
enable	O
learning	O
with	O
delayed	B
reinforcement	I
correspond	O
to	O
hull	B
s	O
hypothesis	O
about	O
how	O
animals	O
are	O
able	O
to	O
learn	O
under	O
these	O
conditions	O
the	O
actor	O
critic	O
architecture	O
discussed	O
in	O
sections	O
and	O
illustrates	O
this	O
correspondence	O
most	O
clearly	O
the	O
critic	O
uses	O
a	O
td	B
algorithm	O
to	O
learn	O
a	O
value	B
function	I
associated	O
with	O
the	O
system	O
s	O
current	O
behavior	O
that	O
is	O
to	O
predict	O
the	O
current	O
policy	B
s	O
return	B
the	O
actor	O
updates	O
the	O
current	O
policy	B
based	O
on	O
the	O
critic	O
s	O
predictions	O
or	O
more	O
exactly	O
on	O
changes	O
in	O
the	O
critic	O
s	O
predictions	O
the	O
td	B
error	I
produced	O
by	O
the	O
critic	O
acts	O
as	O
a	O
conditioned	O
reinforcement	B
signal	I
for	O
the	O
actor	O
providing	O
an	O
immediate	O
evaluation	O
of	O
performance	O
even	O
when	O
the	O
primary	O
reward	B
signal	I
itself	O
is	O
considerably	O
delayed	O
algorithms	O
that	O
estimate	O
actionvalue	O
functions	O
such	O
as	O
q-learning	B
and	O
sarsa	B
similarly	O
use	O
td	B
learning	O
principles	O
to	O
enable	O
learning	O
with	O
delayed	B
reinforcement	I
by	O
means	O
of	O
conditioned	O
reinforcement	O
the	O
close	O
parallel	O
between	O
td	B
learning	O
and	O
the	O
activity	O
of	O
dopamine	B
producing	O
neurons	O
that	O
we	O
discuss	O
in	O
chapter	O
lends	O
additional	O
support	O
to	O
links	O
between	O
reinforcement	B
learning	I
algorithms	O
and	O
this	O
aspect	O
of	O
hull	B
s	O
learning	O
theory	O
cognitive	B
maps	I
model-based	B
reinforcement	B
learning	I
algorithms	O
use	O
environment	B
models	O
that	O
have	O
elements	O
in	O
common	O
with	O
what	O
psychologists	O
call	O
cognitive	B
maps	I
recall	O
from	O
our	O
discussion	O
of	O
planning	B
and	O
learning	O
in	O
chapter	O
that	O
by	O
an	O
environment	B
model	O
we	O
mean	O
anything	O
an	O
agent	O
can	O
use	O
to	O
predict	O
how	O
its	O
environment	B
will	O
respond	O
to	O
its	O
actions	O
in	O
terms	O
of	O
state	B
transitions	O
and	O
rewards	O
and	O
by	O
planning	B
we	O
mean	O
any	O
process	O
that	O
computes	O
a	O
chapter	O
psychology	B
policy	B
from	O
such	O
a	O
model	O
environment	B
models	O
consist	O
of	O
two	O
parts	O
the	O
state-transition	O
part	O
encodes	O
knowledge	O
about	O
the	O
effect	O
of	O
actions	O
on	O
state	B
changes	O
and	O
the	O
rewardmodel	O
part	O
encodes	O
knowledge	O
about	O
the	O
reward	O
signals	O
expected	B
for	O
each	O
state	B
or	O
each	O
state	B
action	B
pair	O
a	O
model-based	O
algorithm	O
selects	O
actions	O
by	O
using	O
a	O
model	O
to	O
predict	O
the	O
consequences	O
of	O
possible	O
courses	O
of	O
action	B
in	O
terms	O
of	O
future	O
states	O
and	O
the	O
reward	O
signals	O
expected	B
to	O
arise	O
from	O
those	O
states	O
the	O
simplest	O
kind	O
of	O
planning	B
is	O
to	O
compare	O
the	O
predicted	O
consequences	O
of	O
collections	O
of	O
imagined	O
sequences	O
of	O
decisions	O
questions	O
about	O
whether	O
or	O
not	O
animals	O
use	O
environment	B
models	O
and	O
if	O
so	O
what	O
are	O
the	O
models	O
like	O
and	O
how	O
are	O
they	O
learned	O
have	O
played	O
influential	O
roles	O
in	O
the	O
history	B
of	I
animal	O
learning	O
research	O
some	O
researchers	O
challenged	O
the	O
then-prevailing	O
stimulusresponse	O
r	O
view	O
of	O
learning	O
and	O
behavior	O
which	O
corresponds	O
to	O
the	O
simplest	O
modelfree	O
way	O
of	O
learning	O
policies	O
by	O
demonstrating	O
latent	B
learning	I
in	O
the	O
earliest	O
latent	B
learning	I
experiment	O
two	O
groups	O
of	O
rats	O
were	O
run	O
in	O
a	O
maze	O
for	O
the	O
experimental	O
group	O
there	O
was	O
no	O
reward	O
during	O
the	O
first	O
stage	O
of	O
the	O
experiment	O
but	O
food	O
was	O
suddenly	O
introduced	O
into	O
the	O
goal	O
box	O
of	O
the	O
maze	O
at	O
the	O
start	O
of	O
the	O
second	O
stage	O
for	O
the	O
control	B
group	O
food	O
was	O
in	O
the	O
goal	O
box	O
throughout	O
both	O
stages	O
the	O
question	O
was	O
whether	O
or	O
not	O
rats	O
in	O
the	O
experimental	O
group	O
would	O
have	O
learned	O
anything	O
during	O
the	O
first	O
stage	O
in	O
the	O
absence	O
of	O
food	O
reward	O
although	O
the	O
experimental	O
rats	O
did	O
not	O
appear	O
to	O
learn	O
much	O
during	O
the	O
first	O
unrewarded	O
stage	O
as	O
soon	O
as	O
they	O
discovered	O
the	O
food	O
that	O
was	O
introduced	O
in	O
the	O
second	O
stage	O
they	O
rapidly	O
caught	O
up	O
with	O
the	O
rats	O
in	O
the	O
control	B
group	O
it	O
was	O
concluded	O
that	O
during	O
the	O
non-reward	O
period	O
the	O
rats	O
the	O
experimental	O
group	O
were	O
developing	O
a	O
latent	B
learning	I
of	O
the	O
maze	O
which	O
they	O
were	O
able	O
to	O
utilize	O
as	O
soon	O
as	O
reward	O
was	O
introduced	O
latent	B
learning	I
is	O
most	O
closely	O
associated	O
with	O
the	O
psychologist	O
edward	O
tolman	B
who	O
interpreted	O
this	O
result	O
and	O
others	O
like	O
it	O
as	O
showing	O
that	O
animals	O
could	O
learn	O
a	O
cognitive	O
map	O
of	O
the	O
environment	B
in	O
the	O
absence	O
of	O
rewards	O
or	O
penalties	O
and	O
that	O
they	O
could	O
use	O
the	O
map	O
later	O
when	O
they	O
were	O
motivated	O
to	O
reach	O
a	O
goal	O
a	O
cognitive	O
map	O
could	O
also	O
allow	O
a	O
rat	O
to	O
plan	O
a	O
route	O
to	O
the	O
goal	O
that	O
was	O
different	O
from	O
the	O
route	O
the	O
rat	O
had	O
used	O
in	O
its	O
initial	O
exploration	O
explanations	O
of	O
results	O
like	O
these	O
led	O
to	O
the	O
enduring	O
controversy	O
lying	O
at	O
the	O
heart	O
of	O
the	O
behavioristcognitive	O
dichotomy	O
in	B
psychology	B
in	O
modern	O
terms	O
cognitive	B
maps	I
are	O
not	O
restricted	O
to	O
models	O
of	O
spatial	O
layouts	O
but	O
are	O
more	O
generally	O
environment	B
models	O
or	O
models	O
of	O
an	O
animal	O
s	O
task	O
space	O
wilson	O
takahashi	O
schoenbaum	O
and	O
niv	O
the	O
cognitive	O
map	O
explanation	O
of	O
latent	B
learning	I
experiments	O
is	O
analogous	O
to	O
the	O
claim	O
that	O
animals	O
use	O
model-based	O
algorithms	O
and	O
that	O
environment	B
models	O
can	O
be	O
learned	O
even	O
without	O
explicit	O
rewards	O
or	O
penalties	O
models	O
are	O
then	O
used	O
for	O
planning	B
when	O
the	O
animal	O
is	O
motivated	O
by	O
the	O
appearance	O
of	O
rewards	O
or	O
penalties	O
tolman	B
s	O
account	O
of	O
how	O
animals	O
learn	O
cognitive	B
maps	I
was	O
that	O
they	O
learn	O
stimulusstimulus	O
or	O
s	O
s	O
associations	O
by	O
experiencing	O
successions	O
of	O
stimuli	O
as	O
they	O
explore	O
an	O
environment	B
in	B
psychology	B
this	O
is	O
called	O
expectancy	O
theory	O
given	O
s	O
s	O
associations	O
the	O
occurrence	O
of	O
a	O
stimulus	O
generates	O
an	O
expectation	O
about	O
the	O
stimulus	O
to	O
come	O
next	O
this	O
is	O
much	O
like	O
what	O
control	B
engineers	O
call	O
system	B
identification	I
in	O
which	O
a	O
model	O
of	O
a	O
system	O
with	O
unknown	O
dynamics	O
is	O
learned	O
from	O
labeled	O
training	O
examples	O
in	O
the	O
simplest	O
discrete-time	O
versions	O
training	O
examples	O
are	O
s	O
pairs	O
where	O
s	O
is	O
a	O
state	B
habitual	O
and	O
goal-directed	O
behavior	O
and	O
the	O
subsequent	O
state	B
is	O
the	O
label	O
when	O
s	O
is	O
observed	O
the	O
model	O
creates	O
the	O
expectation	O
that	O
will	O
be	O
observed	O
next	O
models	O
more	O
useful	O
for	O
planning	B
involve	O
actions	O
as	O
well	O
so	O
that	O
examples	O
look	O
like	O
sa	O
where	O
is	O
expected	B
when	O
action	B
a	O
is	O
executed	O
in	O
state	B
s	O
it	O
is	O
also	O
useful	O
to	O
learn	O
how	O
the	O
environment	B
generates	O
rewards	O
in	O
this	O
case	O
examples	O
are	O
of	O
the	O
form	O
s	O
r	O
or	O
sa	O
r	O
where	O
r	O
is	O
a	O
reward	B
signal	I
associated	O
with	O
s	O
or	O
the	O
sa	O
pair	O
these	O
are	O
all	O
forms	O
of	O
supervised	B
learning	I
by	O
which	O
an	O
agent	O
can	O
acquire	O
cognitive-like	O
maps	O
whether	O
or	O
not	O
it	O
receives	O
any	O
non-zero	O
reward	O
signals	O
while	O
exploring	O
its	O
environment	B
habitual	O
and	O
goal-directed	O
behavior	O
the	O
distinction	O
between	O
model-free	O
and	O
model-based	B
reinforcement	B
learning	I
algorithms	O
corresponds	O
to	O
the	O
distinction	O
psychologists	O
make	O
between	O
habitual	B
and	I
goal-directed	I
control	B
of	O
learned	O
behavioral	O
patterns	O
habits	O
are	O
behavior	O
patterns	O
triggered	O
by	O
appropriate	O
stimuli	O
and	O
then	O
performed	O
more-or-less	O
automatically	O
goal-directed	O
behavior	O
according	O
to	O
how	O
psychologists	O
use	O
the	O
phrase	O
is	O
purposeful	O
in	O
the	O
sense	O
that	O
it	O
is	O
controlled	O
by	O
knowledge	O
of	O
the	O
value	B
of	O
goals	O
and	O
the	O
relationship	O
between	O
actions	O
and	O
their	O
consequences	O
habits	O
are	O
sometimes	O
said	O
to	O
be	O
controlled	O
by	O
antecedent	O
stimuli	O
whereas	O
goal-directed	O
behavior	O
is	O
said	O
to	O
be	O
controlled	O
by	O
its	O
consequences	O
goal-directed	O
control	B
has	O
the	O
advantage	O
that	O
it	O
can	O
rapidly	O
change	O
an	O
animal	O
s	O
behavior	O
when	O
the	O
environment	B
changes	O
its	O
way	O
of	O
reacting	O
to	O
the	O
animal	O
s	O
actions	O
while	O
habitual	O
behavior	O
responds	O
quickly	O
to	O
input	O
from	O
an	O
accustomed	O
environment	B
it	O
is	O
unable	O
to	O
quickly	O
adjust	O
to	O
changes	O
in	O
the	O
environment	B
the	O
development	O
of	O
goal-directed	O
behavioral	O
control	B
was	O
likely	O
a	O
major	O
advance	O
in	O
the	O
evolution	B
of	O
animal	O
intelligence	O
figure	O
illustrates	O
the	O
difference	O
between	O
model-free	O
and	O
model-based	O
decision	O
strategies	O
in	O
a	O
hypothetical	O
task	O
in	O
which	O
a	O
rat	O
has	O
to	O
navigate	O
a	O
maze	O
that	O
has	O
distinctive	O
goal	O
boxes	B
each	O
delivering	O
an	O
associated	O
reward	O
of	O
the	O
magnitude	O
shown	O
top	O
starting	O
at	O
the	O
rat	O
has	O
to	O
first	O
select	O
left	O
or	O
right	O
and	O
then	O
has	O
to	O
select	O
l	O
or	O
r	O
again	O
at	O
or	O
to	O
reach	O
one	O
of	O
the	O
goal	O
boxes	B
the	O
goal	O
boxes	B
are	O
the	O
terminal	O
states	O
of	O
each	O
episode	O
of	O
the	O
rat	O
s	O
episodic	O
task	O
a	O
model-free	O
strategy	O
lower	O
left	O
relies	O
on	O
stored	O
values	O
for	O
state	B
action	B
pairs	O
these	O
action	B
values	O
are	O
estimates	O
of	O
the	O
highest	O
return	B
the	O
rat	O
can	O
expect	O
for	O
each	O
action	B
taken	O
from	O
each	O
state	B
they	O
are	O
obtained	O
over	O
many	O
trials	O
of	O
running	O
the	O
maze	O
from	O
start	O
to	O
finish	O
when	O
the	O
action	B
values	O
have	O
become	O
good	O
enough	O
estimates	O
of	O
the	O
optimal	O
returns	O
the	O
rat	O
just	O
has	O
to	O
select	O
at	O
each	O
state	B
the	O
action	B
with	O
the	O
largest	O
action	B
value	B
in	O
order	O
to	O
make	O
optimal	O
decisions	O
in	O
this	O
case	O
when	O
the	O
action-value	O
estimates	O
become	O
accurate	O
enough	O
the	O
rat	O
selects	O
l	O
from	O
and	O
r	O
from	O
to	O
obtain	O
the	O
maximum	O
return	B
of	O
a	O
different	O
model-free	O
strategy	O
might	O
simply	O
rely	O
on	O
a	O
cached	O
policy	B
instead	O
of	O
action	B
values	O
making	O
direct	O
links	O
from	O
to	O
l	O
and	O
from	O
to	O
r	O
in	O
neither	O
of	O
these	O
strategies	O
do	O
decisions	O
rely	O
on	O
an	O
environment	B
model	O
there	O
is	O
no	O
need	O
to	O
consult	O
a	O
state-transition	O
model	O
and	O
no	O
connection	O
is	O
required	O
between	O
the	O
features	O
of	O
the	O
goal	O
boxes	B
and	O
the	O
rewards	O
they	O
deliver	O
figure	O
right	O
illustrates	O
a	O
model-based	O
strategy	O
it	O
uses	O
an	O
environment	B
model	O
consisting	O
of	O
a	O
state-transition	O
model	O
and	O
a	O
reward	O
model	O
the	O
state-transition	O
chapter	O
psychology	B
figure	O
model-based	O
and	O
model-free	O
strategies	O
to	O
solve	O
a	O
hypothetical	O
sequential	O
actionselection	O
problem	O
top	O
a	O
rat	O
navigates	O
a	O
maze	O
with	O
distinctive	O
goal	O
boxes	B
each	O
associated	O
with	O
a	O
reward	O
having	O
the	O
value	B
shown	O
lower	O
left	O
a	O
model-free	O
strategy	O
relies	O
on	O
stored	O
action	B
values	O
for	O
all	O
the	O
state	B
action	B
pairs	O
obtained	O
over	O
many	O
learning	O
trials	O
to	O
make	O
decisions	O
the	O
rat	O
just	O
has	O
to	O
select	O
at	O
each	O
state	B
the	O
action	B
with	O
the	O
largest	O
action	B
value	B
for	O
that	O
state	B
lower	O
right	O
in	O
a	O
model-based	O
strategy	O
the	O
rat	O
learns	O
an	O
environment	B
model	O
consisting	O
of	O
knowledge	O
of	O
state	B
action-next-state	O
transitions	O
and	O
a	O
reward	O
model	O
consisting	O
of	O
knowledge	O
of	O
the	O
reward	O
associated	O
with	O
each	O
distinctive	O
goal	O
box	O
the	O
rat	O
can	O
decide	O
which	O
way	O
to	O
turn	O
at	O
each	O
state	B
by	O
using	O
the	O
model	O
to	O
simulate	O
sequences	O
of	O
action	B
choices	O
to	O
find	O
a	O
path	O
yielding	O
the	O
highest	O
return	B
adapted	O
from	O
trends	O
in	O
cognitive	O
science	O
volume	O
number	O
y	O
niv	O
d	O
joel	O
and	O
p	O
dayan	O
a	O
normative	O
perspective	O
on	O
motivation	B
p	O
with	O
permission	O
from	O
elsevier	O
model	O
is	O
shown	O
as	O
a	O
decision	O
tree	O
and	O
the	O
reward	O
model	O
associates	O
the	O
distinctive	O
features	O
of	O
the	O
goal	O
boxes	B
with	O
the	O
rewards	O
to	O
be	O
found	O
in	O
each	O
rewards	O
associated	O
with	O
states	O
and	O
are	O
also	O
part	O
of	O
the	O
reward	O
model	O
but	O
here	O
they	O
are	O
zero	O
and	O
are	O
not	O
shown	O
a	O
model-based	O
agent	O
can	O
decide	O
which	O
way	O
to	O
turn	O
at	O
each	O
state	B
by	O
using	O
the	O
model	O
to	O
simulate	O
sequences	O
of	O
action	B
choices	O
to	O
find	O
a	O
path	O
yielding	O
the	O
highest	O
return	B
in	O
this	O
case	O
the	O
return	B
is	O
the	O
reward	O
obtained	O
from	O
the	O
outcome	O
at	O
the	O
end	O
of	O
the	O
path	O
here	O
with	O
a	O
sufficiently	O
accurate	O
model	O
the	O
rat	O
would	O
select	O
l	O
and	O
then	O
r	O
to	O
obtain	O
reward	O
of	O
comparing	O
the	O
predicted	O
returns	O
of	O
simulated	O
paths	O
is	O
a	O
simple	O
form	O
of	O
planning	B
which	O
can	O
be	O
done	O
in	O
a	O
variety	O
of	O
ways	O
as	O
discussed	O
in	O
chapter	O
when	O
the	O
environment	B
of	O
a	O
model-free	O
agent	O
changes	O
the	O
way	O
it	O
reacts	O
to	O
the	O
agent	O
s	O
actions	O
the	O
agent	O
has	O
to	O
acquire	O
new	O
experience	O
in	O
the	O
changed	O
environment	B
during	O
which	O
it	O
can	O
update	O
its	O
policy	B
andor	O
value	B
function	I
in	O
the	O
model-free	O
strategy	O
shown	O
habitual	O
and	O
goal-directed	O
behavior	O
in	O
figure	O
left	O
for	O
example	O
if	O
one	O
of	O
the	O
goal	O
boxes	B
were	O
to	O
somehow	O
shift	O
to	O
delivering	O
a	O
different	O
reward	O
the	O
rat	O
would	O
have	O
to	O
traverse	O
the	O
maze	O
possibly	O
many	O
times	O
to	O
experience	O
the	O
new	O
reward	O
upon	O
reaching	O
that	O
goal	O
box	O
all	O
the	O
while	O
updating	O
either	O
its	O
policy	B
or	O
its	O
action-value	O
function	O
both	O
based	O
on	O
this	O
experience	O
the	O
key	O
point	O
is	O
that	O
for	O
a	O
model-free	O
agent	O
to	O
change	O
the	O
action	B
its	O
policy	B
specifies	O
for	O
a	O
state	B
or	O
to	O
change	O
an	O
action	B
value	B
associated	O
with	O
a	O
state	B
it	O
has	O
to	O
move	O
to	O
that	O
state	B
act	O
from	O
it	O
possibly	O
many	O
times	O
and	O
experience	O
the	O
consequences	O
of	O
its	O
actions	O
a	O
model-based	O
agent	O
can	O
accommodate	O
changes	O
in	O
its	O
environment	B
without	O
this	O
kind	O
of	O
personal	O
experience	O
with	O
the	O
states	O
and	O
actions	O
affected	O
by	O
the	O
change	O
a	O
change	O
in	O
its	O
model	O
automatically	O
planning	B
changes	O
its	O
policy	B
planning	B
can	O
determine	O
the	O
consequences	O
of	O
changes	O
in	O
the	O
environment	B
that	O
have	O
never	O
been	O
linked	O
together	O
in	O
the	O
agent	O
s	O
own	O
experience	O
for	O
example	O
again	O
referring	O
to	O
the	O
maze	O
task	O
of	O
figure	O
imagine	O
that	O
a	O
rat	O
with	O
a	O
previously	O
learned	O
transition	O
and	O
reward	O
model	O
is	O
placed	O
directly	O
in	O
the	O
goal	O
box	O
to	O
the	O
right	O
of	O
to	O
find	O
that	O
the	O
reward	O
available	O
there	O
now	O
has	O
value	B
instead	O
of	O
the	O
rat	O
s	O
reward	O
model	O
will	O
change	O
even	O
though	O
the	O
action	B
choices	O
required	O
to	O
find	O
that	O
goal	O
box	O
in	O
the	O
maze	O
were	O
not	O
involved	O
the	O
planning	B
process	O
will	O
bring	O
knowledge	O
of	O
the	O
new	O
reward	O
to	O
bear	O
on	O
maze	O
running	O
without	O
the	O
need	O
for	O
additional	O
experience	O
in	O
the	O
maze	O
in	O
this	O
case	O
changing	O
the	O
policy	B
to	O
right	O
turns	O
at	O
both	O
and	O
to	O
obtain	O
a	O
return	B
of	O
exactly	O
this	O
logic	O
is	O
the	O
basis	O
of	O
outcome-devaluation	O
experiments	O
with	O
animals	O
results	O
from	O
these	O
experiments	O
provide	O
insight	O
into	O
whether	O
an	O
animal	O
has	O
learned	O
a	O
habit	O
or	O
if	O
its	O
behavior	O
is	O
under	O
goal-directed	O
control	B
outcome-devaluation	O
experiments	O
are	O
like	O
latent-learning	O
experiments	O
in	O
that	O
the	O
reward	O
changes	O
from	O
one	O
stage	O
to	O
the	O
next	O
after	O
an	O
initial	O
rewarded	O
stage	O
of	O
learning	O
the	O
reward	O
value	B
of	O
an	O
outcome	O
is	O
changed	O
including	O
being	O
shifted	O
to	O
zero	O
or	O
even	O
to	O
a	O
negative	O
value	B
an	O
early	O
important	O
experiment	O
of	O
this	O
type	O
was	O
conducted	O
by	O
adams	O
and	O
dickinson	O
they	O
trained	O
rats	O
via	O
instrumental	O
conditioning	O
until	O
the	O
rats	O
energetically	O
pressed	O
a	O
lever	O
for	O
sucrose	O
pellets	O
in	O
a	O
training	O
chamber	O
the	O
rats	O
were	O
then	O
placed	O
in	O
the	O
same	O
chamber	O
with	O
the	O
lever	O
retracted	O
and	O
allowed	O
non-contingent	O
food	O
meaning	O
that	O
pellets	O
were	O
made	O
available	O
to	O
them	O
independently	O
of	O
their	O
actions	O
after	O
of	O
this	O
free-access	O
to	O
the	O
pellets	O
rats	O
in	O
one	O
group	O
were	O
injected	O
with	O
the	O
nausea-inducing	O
poison	O
lithium	O
chloride	O
this	O
was	O
repeated	O
for	O
three	O
sessions	O
in	O
the	O
last	O
of	O
which	O
none	O
of	O
the	O
injected	O
rats	O
consumed	O
any	O
of	O
the	O
non-contingent	O
pellets	O
indicating	O
that	O
the	O
reward	O
value	B
of	O
the	O
pellets	O
had	O
been	O
decreased	O
the	O
pellets	O
had	O
been	O
devalued	O
in	O
the	O
next	O
stage	O
taking	O
place	O
a	O
day	O
later	O
the	O
rats	O
were	O
again	O
placed	O
in	O
the	O
chamber	O
and	O
given	O
a	O
session	O
of	O
extinction	O
training	O
meaning	O
that	O
the	O
response	O
lever	O
was	O
back	O
in	O
place	O
but	O
disconnected	O
from	O
the	O
pellet	O
dispenser	O
so	O
that	O
pressing	O
it	O
did	O
not	O
release	O
pellets	O
the	O
question	O
was	O
whether	O
the	O
rats	O
that	O
had	O
the	O
reward	O
value	B
of	O
the	O
pellets	O
decreased	O
would	O
lever-press	O
less	O
than	O
rats	O
that	O
did	O
not	O
have	O
the	O
reward	O
value	B
of	O
the	O
pellets	O
decreased	O
even	O
without	O
experiencing	O
the	O
devalued	O
reward	O
as	O
a	O
result	O
of	O
lever-pressing	O
it	O
turned	O
out	O
that	O
the	O
injected	O
rats	O
had	O
significantly	O
lower	O
response	O
rates	O
than	O
the	O
non-injected	O
rats	O
right	O
from	O
the	O
start	O
of	O
the	O
extinction	O
trials	O
adams	O
and	O
dickinson	O
concluded	O
that	O
the	O
injected	O
rats	O
associated	O
lever	O
pressing	O
with	O
consequent	O
nausea	O
by	O
means	O
of	O
a	O
cognitive	O
map	O
linking	O
lever	O
pressing	O
to	O
pellets	O
and	O
chapter	O
psychology	B
pellets	O
to	O
nausea	O
hence	O
in	O
the	O
extinction	O
trials	O
the	O
rats	O
knew	O
that	O
the	O
consequences	O
of	O
pressing	O
the	O
lever	O
would	O
be	O
something	O
they	O
did	O
not	O
want	O
and	O
so	O
they	O
reduced	O
their	O
lever-pressing	O
right	O
from	O
the	O
start	O
the	O
important	O
point	O
is	O
that	O
they	O
reduced	O
leverpressing	O
without	O
ever	O
having	O
experienced	O
lever-pressing	O
directly	O
followed	O
by	O
being	O
sick	O
no	O
lever	O
was	O
present	O
when	O
they	O
were	O
made	O
sick	O
they	O
seemed	O
able	O
to	O
combine	O
knowledge	O
of	O
the	O
outcome	O
of	O
a	O
behavioral	O
choice	O
the	O
lever	O
will	O
be	O
followed	O
by	O
getting	O
a	O
pellet	O
with	O
the	O
reward	O
value	B
of	O
the	O
outcome	O
are	O
to	O
be	O
avoided	O
and	O
hence	O
could	O
alter	O
their	O
behavior	O
accordingly	O
not	O
every	O
psychologist	O
agrees	O
with	O
this	O
cognitive	O
account	O
of	O
this	O
kind	O
of	O
experiment	O
and	O
it	O
is	O
not	O
the	O
only	O
possible	O
way	O
to	O
explain	O
these	O
results	O
but	O
the	O
model-based	O
planning	B
explanation	O
is	O
widely	O
accepted	O
nothing	O
prevents	O
an	O
agent	O
from	O
using	O
both	O
model-free	O
and	O
model-based	O
algorithms	O
and	O
there	O
are	O
good	O
reasons	O
for	O
using	O
both	O
we	O
know	O
from	O
our	O
own	O
experience	O
that	O
with	O
enough	O
repetition	O
goal-directed	O
behavior	O
tends	O
to	O
turn	O
into	O
habitual	O
behavior	O
experiments	O
show	O
that	O
this	O
happens	O
for	O
rats	O
too	O
adams	O
conducted	O
an	O
experiment	O
to	O
see	O
if	O
extended	O
training	O
would	O
convert	O
goal-directed	O
behavior	O
into	O
habitual	O
behavior	O
he	O
did	O
this	O
by	O
comparing	O
the	O
effect	O
of	O
outcome	O
devaluation	O
on	O
rats	O
that	O
experienced	O
different	O
amounts	O
of	O
training	O
if	O
extended	O
training	O
made	O
the	O
rats	O
less	O
sensitive	O
to	O
devaluation	O
compared	O
to	O
rats	O
that	O
received	O
less	O
training	O
this	O
would	O
be	O
evidence	O
that	O
extended	O
training	O
made	O
the	O
behavior	O
more	O
habitual	O
adams	O
experiment	O
closely	O
followed	O
the	O
adams	O
and	O
dickinson	O
experiment	O
just	O
described	O
simplifying	O
a	O
bit	O
rats	O
in	O
one	O
group	O
were	O
trained	O
until	O
they	O
made	O
rewarded	O
lever-presses	O
and	O
rats	O
in	O
the	O
other	O
group	O
the	O
overtrained	O
group	O
were	O
trained	O
until	O
they	O
made	O
rewarded	O
lever-presses	O
after	O
this	O
training	O
the	O
reward	O
value	B
of	O
the	O
pellets	O
was	O
decreased	O
lithium	O
chloride	O
injections	O
for	O
rats	O
in	O
both	O
groups	O
then	O
both	O
groups	O
of	O
rats	O
were	O
given	O
a	O
session	O
of	O
extinction	O
training	O
adams	O
question	O
was	O
whether	O
devaluation	O
would	O
effect	O
the	O
rate	O
of	O
lever-pressing	O
for	O
the	O
overtrained	O
rats	O
less	O
than	O
it	O
would	O
for	O
the	O
non-overtrained	O
rats	O
which	O
would	O
be	O
evidence	O
that	O
extended	O
training	O
reduces	O
sensitivity	O
to	O
outcome	O
devaluation	O
it	O
turned	O
out	O
that	O
devaluation	O
strongly	O
decreased	O
the	O
lever-pressing	O
rate	O
of	O
the	O
non-overtrained	O
rats	O
for	O
the	O
overtrained	O
rats	O
in	O
contrast	O
devaluation	O
had	O
little	O
effect	O
on	O
their	O
lever-pressing	O
in	O
fact	O
if	O
anything	O
it	O
made	O
it	O
more	O
vigorous	O
full	O
experiment	O
included	O
control	B
groups	O
showing	O
that	O
the	O
different	O
amounts	O
of	O
training	O
did	O
not	O
by	O
themselves	O
significantly	O
effect	O
lever-pressing	O
rates	O
after	O
learning	O
this	O
result	O
suggested	O
that	O
while	O
the	O
non-overtrained	O
rats	O
were	O
acting	O
in	O
a	O
goal-directed	O
manner	O
sensitive	O
to	O
their	O
knowledge	O
of	O
the	O
outcome	O
of	O
their	O
actions	O
the	O
overtrained	O
rats	O
had	O
developed	O
a	O
lever-pressing	O
habit	O
viewing	O
this	O
and	O
other	O
results	O
like	O
it	O
from	O
a	O
computational	O
perspective	O
provides	O
insight	O
as	O
to	O
why	O
one	O
might	O
expect	O
animals	O
to	O
behave	O
habitually	O
in	O
some	O
circumstances	O
in	O
a	O
goal-directed	O
way	O
in	O
others	O
and	O
why	O
they	O
shift	O
from	O
one	O
mode	O
of	O
control	B
to	O
another	O
as	O
they	O
continue	O
to	O
learn	O
while	O
animals	O
undoubtedly	O
use	O
algorithms	O
that	O
do	O
not	O
exactly	O
match	O
those	O
we	O
have	O
presented	O
in	O
this	O
book	O
one	O
can	O
gain	O
insight	O
into	O
animal	O
behavior	O
by	O
considering	O
the	O
tradeoffs	O
that	O
various	O
reinforcement	B
learning	I
algorithms	O
imply	O
an	O
idea	O
developed	O
by	O
computational	O
neuroscientists	O
daw	O
niv	O
and	O
dayan	O
is	O
that	O
animals	O
use	O
both	O
model-free	O
and	O
model-based	O
processes	O
each	O
process	O
proposes	O
an	O
action	B
and	O
the	O
action	B
chosen	O
for	O
execution	O
is	O
the	O
one	O
proposed	O
by	O
the	O
process	O
judged	O
to	O
be	O
the	O
more	O
trustworthy	O
of	O
the	O
two	O
as	O
determined	O
by	O
measures	O
of	O
confidence	O
that	O
are	O
maintained	O
summary	O
throughout	O
learning	O
early	O
in	O
learning	O
the	O
planning	B
process	O
of	O
a	O
model-based	O
system	O
is	O
more	O
trustworthy	O
because	O
it	O
chains	O
together	O
short-term	O
predictions	O
which	O
can	O
become	O
accurate	O
with	O
less	O
experience	O
than	O
long-term	O
predictions	O
of	O
the	O
model-free	O
process	O
but	O
with	O
continued	O
experience	O
the	O
model-free	O
process	O
becomes	O
more	O
trustworthy	O
because	O
planning	B
is	O
prone	O
to	O
making	O
mistakes	O
due	O
to	O
model	O
inaccuracies	O
and	O
short-cuts	O
necessary	O
to	O
make	O
planning	B
feasible	O
such	O
as	O
various	O
forms	O
of	O
tree-pruning	O
the	O
removal	O
of	O
unpromising	O
search	O
tree	O
branches	O
according	O
to	O
this	O
idea	O
one	O
would	O
expect	O
a	O
shift	O
from	O
goal-directed	O
behavior	O
to	O
habitual	O
behavior	O
as	O
more	O
experience	O
accumulates	O
other	O
ideas	O
have	O
been	O
proposed	O
for	O
how	O
animals	O
arbitrate	O
between	O
goal-directed	O
and	O
habitual	O
control	B
and	O
both	O
behavioral	O
and	O
neuroscience	B
research	O
continues	O
to	O
examine	O
this	O
and	O
related	O
questions	O
the	O
distinction	O
between	O
model-free	O
and	O
model-based	O
algorithms	O
is	O
proving	O
to	O
be	O
useful	O
for	O
this	O
research	O
one	O
can	O
examine	O
the	O
computational	O
implications	O
of	O
these	O
types	O
of	O
algorithms	O
in	O
abstract	O
settings	O
that	O
expose	O
basic	O
advantages	O
and	O
limitations	O
of	O
each	O
type	O
this	O
serves	O
both	O
to	O
suggest	O
and	O
to	O
sharpen	O
questions	O
that	O
guide	O
the	O
design	B
of	I
experiments	O
necessary	O
for	O
increasing	O
psychologists	O
understanding	O
of	O
habitual	O
and	O
goaldirected	O
behavioral	O
control	B
summary	O
our	O
goal	O
in	O
this	O
chapter	O
has	O
been	O
to	O
discuss	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
the	O
experimental	O
study	O
of	O
animal	O
learning	O
in	B
psychology	B
we	O
emphasized	O
at	O
the	O
outset	O
that	O
reinforcement	B
learning	I
as	O
described	O
in	O
this	O
book	O
is	O
not	O
intended	O
to	O
model	O
details	O
of	O
animal	O
behavior	O
it	O
is	O
an	O
abstract	O
computational	O
framework	O
that	O
explores	O
idealized	O
situations	O
from	O
the	O
perspective	O
of	O
artificial	B
intelligence	I
and	O
engineering	O
but	O
many	O
of	O
the	O
basic	O
reinforcement	B
learning	I
algorithms	O
were	O
inspired	O
by	O
psychological	O
theories	O
and	O
in	O
some	O
cases	O
these	O
algorithms	O
have	O
contributed	O
to	O
the	O
development	O
of	O
new	O
animal	O
learning	O
models	O
this	O
chapter	O
described	O
the	O
most	O
conspicuous	O
of	O
these	O
correspondences	O
the	O
distinction	O
in	O
reinforcement	B
learning	I
between	O
algorithms	O
for	O
prediction	B
and	O
algorithms	O
for	O
control	B
parallels	O
animal	O
learning	O
theory	O
s	O
distinction	O
between	O
classical	O
or	O
pavlovian	O
conditioning	O
and	O
instrumental	O
conditioning	O
the	O
key	O
difference	O
between	O
instrumental	O
and	O
classical	B
conditioning	I
experiments	O
is	O
that	O
in	O
the	O
former	O
the	O
reinforcing	O
stimulus	O
is	O
contingent	O
upon	O
the	O
animal	O
s	O
behavior	O
whereas	O
in	O
the	O
latter	O
it	O
is	O
not	O
learning	O
to	O
predict	O
via	O
a	O
td	B
algorithm	O
corresponds	O
to	O
classical	B
conditioning	I
and	O
we	O
described	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
as	O
one	O
instance	O
in	O
which	O
reinforcement	B
learning	I
principles	O
account	O
for	O
some	O
details	O
of	O
animal	O
learning	O
behavior	O
this	O
model	O
generalizes	O
the	O
influential	O
rescorla	O
wagner	O
model	O
by	O
including	O
the	O
temporal	O
dimension	O
where	O
events	O
within	O
individual	O
trials	O
influence	O
learning	O
and	O
it	O
provides	O
an	O
account	O
of	O
secondorder	O
conditioning	O
where	O
predictors	O
of	O
reinforcing	O
stimuli	O
become	O
reinforcing	O
themselves	O
it	O
also	O
is	O
the	O
basis	O
of	O
an	O
influential	O
view	O
of	O
the	O
activity	O
of	O
dopamine	B
neurons	O
in	O
the	O
brain	O
something	O
we	O
take	O
up	O
in	O
chapter	O
learning	O
by	O
trial	O
and	O
error	O
is	O
at	O
the	O
base	O
of	O
the	O
control	B
aspect	O
of	O
reinforcement	B
learning	I
we	O
presented	O
some	O
details	O
about	O
thorndike	O
s	O
experiments	O
with	O
cats	O
and	O
other	O
animals	O
chapter	O
psychology	B
that	O
led	O
to	O
his	O
law	B
of	I
effect	I
which	O
we	O
discussed	O
here	O
and	O
in	O
chapter	O
we	O
pointed	O
out	O
that	O
in	O
reinforcement	B
learning	I
exploration	O
does	O
not	O
have	O
to	O
be	O
limited	O
to	O
blind	O
groping	O
trials	O
can	O
be	O
generated	O
by	O
sophisticated	O
methods	O
using	O
innate	O
and	O
previously	O
learned	O
knowledge	O
as	O
long	O
as	O
there	O
is	O
some	O
exploration	O
we	O
discussed	O
the	O
training	O
method	O
b	O
f	O
skinner	B
called	O
shaping	B
in	O
which	O
reward	O
contingencies	O
are	O
progressively	O
altered	O
to	O
train	O
an	O
animal	O
to	O
successively	O
approximate	O
a	O
desired	O
behavior	O
shaping	B
is	O
not	O
only	O
indispensable	O
for	O
animal	O
training	O
it	O
is	O
also	O
an	O
effective	O
tool	O
for	O
training	O
reinforcement	B
learning	I
agents	O
there	O
is	O
also	O
a	O
connection	O
to	O
the	O
idea	O
of	O
an	O
animal	O
s	O
motivational	O
state	B
which	O
influences	O
what	O
an	O
animal	O
will	O
approach	O
or	O
avoid	O
and	O
what	O
events	O
are	O
rewarding	O
or	O
punishing	O
for	O
the	O
animal	O
the	O
reinforcement	B
learning	I
algorithms	O
presented	O
in	O
this	O
book	O
include	O
two	O
basic	O
mechanisms	O
for	O
addressing	O
the	O
problem	O
of	O
delayed	B
reinforcement	I
eligibility	B
traces	I
and	O
value	B
functions	O
learned	O
via	O
td	B
algorithms	O
both	O
mechanisms	O
have	O
antecedents	O
in	O
theories	O
of	O
animal	O
learning	O
eligibility	B
traces	I
are	O
similar	O
to	O
stimulus	O
traces	O
of	O
early	O
theories	O
and	O
value	B
functions	O
correspond	O
to	O
the	O
role	O
of	O
secondary	B
reinforcement	I
in	O
providing	O
nearly	O
immediate	O
evaluative	B
feedback	I
the	O
next	O
correspondence	O
the	O
chapter	O
addressed	O
is	O
that	O
between	O
reinforcement	B
learning	I
s	O
environment	B
models	O
and	O
what	O
psychologists	O
call	O
cognitive	B
maps	I
experiments	O
conducted	O
in	O
the	O
mid	O
century	O
purported	O
to	O
demonstrate	O
the	O
ability	O
of	O
animals	O
to	O
learn	O
cognitive	B
maps	I
as	O
alternatives	O
to	O
or	O
as	O
additions	O
to	O
state	B
action	B
associations	O
and	O
later	O
use	O
them	O
to	O
guide	O
behavior	O
especially	O
when	O
the	O
environment	B
changes	O
unexpectedly	O
environment	B
models	O
in	O
reinforcement	B
learning	I
are	O
like	O
cognitive	B
maps	I
in	O
that	O
they	O
can	O
be	O
learned	O
by	O
supervised	B
learning	I
methods	O
without	O
relying	O
on	O
reward	O
signals	O
and	O
then	O
they	O
can	O
be	O
used	O
later	O
to	O
plan	O
behavior	O
reinforcement	B
learning	I
s	O
distinction	O
between	O
model-free	O
and	O
model-based	O
algorithms	O
corresponds	O
to	O
the	O
distinction	O
in	B
psychology	B
between	O
habitual	O
and	O
goal-directed	O
behavior	O
model-free	O
algorithms	O
make	O
decisions	O
by	O
accessing	O
information	O
that	O
has	O
been	O
strored	O
in	O
a	O
policy	B
or	O
an	O
action-value	O
function	O
whereas	O
model-based	O
methods	O
select	O
actions	O
as	O
the	O
result	O
of	O
planning	B
ahead	O
using	O
a	O
model	O
of	O
the	O
agent	O
s	O
environment	B
outcome-devaluation	O
experiments	O
provide	O
information	O
about	O
whether	O
an	O
animal	O
s	O
behavior	O
is	O
habitual	O
or	O
under	O
goal-directed	O
control	B
reinforcement	B
learning	I
theory	O
has	O
helped	O
clarify	O
thinking	O
about	O
these	O
issues	O
animal	O
learning	O
clearly	O
informs	O
reinforcement	B
learning	I
but	O
as	O
a	O
type	O
of	O
machine	O
learning	O
reinforcement	B
learning	I
is	O
directed	O
toward	O
designing	O
and	O
understanding	O
effective	O
learning	O
algorithms	O
not	O
toward	O
replicating	O
or	O
explaining	O
details	O
of	O
animal	O
behavior	O
we	O
focused	O
on	O
aspects	O
of	O
animal	O
learning	O
that	O
relate	O
in	O
clear	O
ways	O
to	O
methods	O
for	O
solving	O
prediction	B
and	B
control	B
problems	O
highlighting	O
the	O
fruitful	O
two-way	O
flow	O
of	O
ideas	O
between	O
reinforcement	B
learning	I
and	O
psychology	B
without	O
venturing	O
deeply	O
into	O
many	O
of	O
the	O
behavioral	O
details	O
and	O
controversies	O
that	O
have	O
occupied	O
the	O
attention	O
of	O
animal	O
learning	O
researchers	O
future	O
development	O
of	O
reinforcement	B
learning	I
theory	O
and	O
algorithms	O
will	O
likely	O
exploit	O
links	O
to	O
many	O
other	O
features	O
of	O
animal	O
learning	O
as	O
the	O
computational	O
utility	O
of	O
these	O
features	O
becomes	O
better	O
appreciated	O
we	O
expect	O
that	O
a	O
flow	O
of	O
ideas	O
between	O
reinforcement	B
learning	I
and	O
psychology	B
will	O
continue	O
to	O
bear	O
fruit	O
for	O
both	O
disciplines	O
many	O
connections	O
between	O
reinforcement	B
learning	I
and	O
areas	O
of	O
psychology	B
and	O
other	O
summary	O
behavioral	O
sciences	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
chapter	O
we	O
largely	O
omit	O
discussing	O
links	O
to	O
the	O
psychology	B
of	O
decision	O
making	O
which	O
focuses	O
on	O
how	O
actions	O
are	O
selected	O
or	O
how	O
decisions	O
are	O
made	O
after	O
learning	O
has	O
taken	O
place	O
we	O
also	O
do	O
not	O
discuss	O
links	O
to	O
ecological	O
and	O
evolutionary	O
aspects	O
of	O
behavior	O
studied	O
by	O
ethologists	O
and	O
behavioral	O
ecologists	O
how	O
animals	O
relate	O
to	O
one	O
another	O
and	O
to	O
their	O
physical	O
surroundings	O
and	O
how	O
their	O
behavior	O
contributes	O
to	O
evolutionary	O
fitness	O
optimization	O
mdps	O
and	B
dynamic	B
programming	I
figure	O
prominently	O
in	O
these	O
fields	O
and	O
our	O
emphasis	O
on	O
agent	O
interaction	O
with	O
dynamic	O
environments	O
connects	O
to	O
the	O
study	O
of	O
agent	O
behavior	O
in	O
complex	O
ecologies	O
multi-agent	O
reinforcement	B
learning	I
omitted	O
in	O
this	O
book	O
has	O
connections	O
to	O
social	O
aspects	O
of	O
behavior	O
despite	O
the	O
lack	O
of	O
treatment	O
here	O
reinforcement	B
learning	I
should	O
by	O
no	O
means	O
be	O
interpreted	O
as	O
dismissing	O
evolutionary	O
perspectives	O
nothing	O
about	O
reinforcement	B
learning	I
implies	O
a	O
tabula	O
rasa	O
view	O
of	O
learning	O
and	O
behavior	O
indeed	O
experience	O
with	O
engineering	O
applications	O
has	O
highlighted	O
the	O
importance	O
of	O
building	O
into	O
reinforcement	B
learning	I
systems	O
knowledge	O
that	O
is	O
analogous	O
to	O
what	O
evolution	B
provides	O
to	O
animals	O
bibliographical	O
and	O
historical	O
remarks	O
ludvig	O
bellemare	O
and	O
pearson	O
and	O
shah	O
review	O
reinforcement	B
learning	I
in	O
the	O
contexts	O
of	O
psychology	B
and	O
neuroscience	B
these	O
publications	O
are	O
useful	O
companions	O
to	O
this	O
chapter	O
and	O
the	O
following	O
chapter	O
on	O
reinforcement	B
learning	I
and	O
neuroscience	B
dayan	O
niv	O
seymour	O
and	O
daw	O
focused	O
on	O
interactions	O
between	O
classical	O
and	O
instrumental	O
conditioning	O
particularly	O
situations	O
where	O
classicallyconditioned	O
and	O
instrumental	O
responses	O
are	O
in	O
conflict	O
they	O
proposed	O
a	O
qlearning	O
framework	O
for	O
modeling	O
aspects	O
of	O
this	O
interaction	O
modayil	O
and	O
sutton	O
used	O
a	O
mobile	O
robot	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
a	O
control	B
method	O
combining	O
a	O
fixed	O
response	O
with	O
online	B
prediction	B
learning	O
calling	O
this	O
pavlovian	O
control	B
they	O
emphasized	O
that	O
it	O
differs	O
from	O
the	O
usual	O
control	B
methods	O
of	O
reinforcement	B
learning	I
being	O
based	O
on	O
predictively	O
executing	O
fixed	O
responses	O
and	O
not	O
on	O
reward	O
maximization	O
the	O
electro-mechanical	O
machine	O
of	O
ross	O
and	O
especially	O
the	O
learning	O
version	O
of	O
walter	O
s	O
turtle	O
were	O
very	O
early	O
illustrations	O
of	O
pavlovian	O
control	B
kamin	O
first	O
reported	O
blocking	B
now	O
commonly	O
known	O
as	O
kamin	O
blocking	B
in	O
classical	B
conditioning	I
moore	O
and	O
schmajuk	O
provide	O
an	O
excellent	O
summary	O
of	O
the	O
blocking	B
phenomenon	O
the	O
research	O
it	O
stimulated	O
and	O
its	O
lasting	O
influence	O
on	O
animal	O
learning	O
theory	O
gibbs	O
cool	O
land	O
kehoe	O
and	O
gormezano	O
describe	O
second-order	O
conditioning	O
of	O
the	O
rabbit	O
s	O
nictitating	O
membrane	O
response	O
and	O
its	O
relationship	O
to	O
conditioning	O
with	O
serial-compound	O
stimuli	O
finch	O
and	O
culler	O
reported	O
obtaining	O
fifth-order	O
conditioning	O
of	O
a	O
dog	O
s	O
foreleg	O
withdrawal	O
when	O
the	O
motivation	B
of	O
the	O
animal	O
is	O
maintained	O
through	O
the	O
various	O
orders	O
the	O
idea	O
built	O
into	O
the	O
rescorla	O
wagner	O
model	O
that	O
learning	O
occurs	O
when	O
ani	O
chapter	O
psychology	B
mals	O
are	O
surprised	O
is	O
derived	O
from	O
kamin	O
models	O
of	O
classical	B
conditioning	I
other	O
than	O
rescorla	O
and	O
wagner	O
s	O
include	O
the	O
models	O
of	O
klopf	B
grossberg	O
mackintosh	O
moore	O
and	O
stickney	O
pearce	O
and	O
hall	O
and	O
courville	O
daw	O
and	O
touretzky	O
schmajuk	O
review	O
models	O
of	O
classical	B
conditioning	I
an	O
early	O
version	O
of	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
appeared	O
in	O
sutton	O
and	O
barto	O
which	O
also	O
included	O
the	O
early	O
model	O
s	O
prediction	B
that	O
temporal	O
primacy	O
overrides	O
blocking	B
later	O
shown	O
by	O
kehoe	O
schreurs	O
and	O
graham	O
to	O
occur	O
in	O
the	O
rabbit	O
nictitating	O
membrane	O
preparation	O
sutton	O
and	O
barto	O
contains	O
the	O
earliest	O
recognition	O
of	O
the	O
near	O
identity	O
between	O
the	O
rescorla	O
wagner	O
model	O
and	O
the	O
least-mean-square	O
or	O
widrow-hoff	O
learning	O
rule	O
and	O
hoff	O
this	O
early	O
model	O
was	O
revised	O
following	O
sutton	O
s	O
development	O
of	O
the	O
td	B
algorithm	O
and	O
was	O
first	O
presented	O
as	O
the	O
td	B
model	I
in	O
sutton	O
and	O
barto	O
and	O
more	O
completely	O
in	O
sutton	O
and	O
barto	O
upon	O
which	O
this	O
section	O
is	O
largely	O
based	O
additional	O
exploration	O
of	O
the	O
td	B
model	I
and	O
its	O
possible	O
neural	B
implementation	O
was	O
conducted	O
by	O
moore	O
and	O
colleagues	O
desmond	O
berthier	O
blazis	O
sutton	O
and	O
barto	O
moore	O
and	O
blazis	O
moore	O
choi	O
and	O
brunzell	O
moore	O
marks	O
castagna	O
and	O
polewan	O
klopf	B
s	O
drive-reinforcement	O
theory	O
of	O
classical	B
conditioning	I
extends	O
the	O
td	B
model	I
to	O
address	O
additional	O
experimental	O
details	O
such	O
as	O
the	O
s-shape	O
of	O
acquisition	O
curves	O
in	O
some	O
of	O
these	O
publications	O
td	B
is	O
taken	O
to	O
mean	O
time	O
derivative	O
instead	O
of	O
temporal	O
difference	O
ludvig	O
sutton	O
and	O
kehoe	O
evaluated	O
the	O
performance	O
of	O
the	O
td	B
model	I
in	O
previously	O
unexplored	O
tasks	O
involving	O
classical	B
conditioning	I
and	O
examined	O
the	O
influence	O
of	O
various	O
stimulus	O
representations	O
including	O
the	O
microstimulus	O
representation	O
that	O
they	O
introduced	O
earlier	O
sutton	O
and	O
kehoe	O
earlier	O
investigations	O
of	O
the	O
influence	O
of	O
various	O
stimulus	O
representations	O
and	O
their	O
possible	O
neural	B
implementations	O
on	O
response	O
timing	O
and	O
topography	O
in	O
the	O
context	O
of	O
the	O
td	B
model	I
are	O
those	O
of	O
moore	O
and	O
colleagues	O
cited	O
above	O
although	O
not	O
in	O
the	O
context	O
of	O
the	O
td	B
model	I
representations	O
like	O
the	O
microstimulus	O
representation	O
of	O
ludvig	O
et	O
al	O
have	O
been	O
proposed	O
and	O
studied	O
by	O
grossberg	O
and	O
schmajuk	O
brown	O
bullock	O
and	O
grossberg	O
buhusi	O
and	O
schmajuk	O
and	O
machado	O
section	O
includes	O
comments	O
on	O
the	O
history	B
of	I
trial-and-error	O
learning	O
and	O
the	O
law	B
of	I
effect	I
the	O
idea	O
that	O
thorndikes	O
cats	O
might	O
have	O
been	O
exploring	O
according	O
to	O
an	O
instinctual	O
context-specific	O
ordering	O
over	O
actions	O
rather	O
than	O
by	O
just	O
selecting	O
from	O
a	O
set	O
of	O
instinctual	O
impulses	O
was	O
suggested	O
by	O
peter	O
dayan	O
communication	O
selfridge	O
sutton	O
and	O
barto	O
illustrated	O
the	O
effectiveness	O
of	O
shaping	B
in	O
a	O
pole-balancing	O
reinforcement	B
learning	I
task	O
other	O
examples	O
of	O
shaping	B
in	O
reinforcement	B
learning	I
are	O
gullapalli	O
and	O
barto	O
mahadevan	O
and	O
connell	O
mataric	O
dorigo	O
and	O
colombette	O
saksida	O
raymond	O
and	O
touretzky	O
and	O
randl	O
v	O
and	O
alstr	O
m	O
ng	O
and	O
ng	O
harada	O
and	O
russell	O
used	O
the	O
term	O
shaping	B
in	O
a	O
sense	O
summary	O
somewhat	O
different	O
from	O
skinner	B
s	O
focussing	O
on	O
the	O
problem	O
of	O
how	O
to	O
alter	O
the	O
reward	B
signal	I
without	O
altering	O
the	O
set	O
of	O
optimal	O
policies	O
dickinson	O
and	O
balleine	O
discuss	O
the	O
complexity	O
of	O
the	O
interaction	O
between	O
learning	O
and	O
motivation	B
wise	O
provides	O
an	O
overview	O
of	O
reinforcement	B
learning	I
and	O
its	O
relation	O
to	O
motivation	B
daw	O
and	O
shohamy	O
link	O
motivation	B
and	O
learning	O
to	O
aspects	O
of	O
reinforcement	B
learning	I
theory	O
see	O
also	O
mcclure	O
daw	O
and	O
montague	O
niv	O
joel	O
and	O
dayan	O
rangel	O
camerer	O
and	O
montague	O
and	O
dayan	O
and	O
berridge	O
mcclure	O
et	O
al	O
niv	O
daw	O
and	O
dayan	O
and	O
niv	O
daw	O
joel	O
and	O
dayan	O
present	O
theories	O
of	O
behavioral	O
vigor	O
related	O
to	O
the	O
reinforcement	B
learning	I
framework	O
spence	O
hull	B
s	O
student	O
and	O
collaborator	O
at	O
yale	O
elaborated	O
the	O
role	O
of	O
higherorder	O
reinforcement	O
in	O
addressing	O
the	O
problem	O
of	O
delayed	B
reinforcement	B
learning	I
over	O
very	O
long	O
delays	O
as	O
in	O
taste-aversion	O
conditioning	O
with	O
delays	O
up	O
to	O
several	O
hours	O
led	O
to	O
interference	O
theories	O
as	O
alternatives	O
to	O
decayingtrace	O
theories	O
revusky	O
and	O
garcia	O
boakes	O
and	O
costa	O
other	O
views	O
of	O
learning	O
under	O
delayed	B
reinforcement	I
invoke	O
roles	O
for	O
awareness	O
and	O
working	O
memory	O
clark	O
and	O
squire	O
seo	O
barraclough	O
and	O
lee	O
thistlethwaite	O
provides	O
an	O
extensive	O
review	O
of	O
latent	B
learning	I
experiments	O
up	O
to	O
the	O
time	O
of	O
its	O
publication	O
ljung	O
provides	O
an	O
overview	O
of	O
model	O
learning	O
or	O
system	B
identification	I
techniques	O
in	O
engineering	O
gopnik	O
glymour	O
sobel	O
schulz	O
kushnir	O
and	O
danks	O
present	O
a	O
bayesian	O
theory	O
about	O
how	O
children	O
learn	O
models	O
connections	O
between	O
habitual	O
and	O
goal-directed	O
behavior	O
and	O
model-free	O
and	O
model-based	B
reinforcement	B
learning	I
were	O
first	O
proposed	O
by	O
daw	O
niv	O
and	O
dayan	O
the	O
hypothetical	O
maze	O
task	O
used	O
to	O
explain	O
habitual	O
and	O
goal-directed	O
behavioral	O
control	B
is	O
based	O
on	O
the	O
explanation	O
of	O
niv	O
joel	O
and	O
dayan	O
dolan	O
and	O
dayan	O
review	O
four	O
generations	O
of	O
experimental	O
research	O
related	O
to	O
this	O
issue	O
and	O
discuss	O
how	O
it	O
can	O
move	O
forward	O
on	O
the	O
basis	O
of	O
reinforcement	B
learning	I
s	O
model-freemodel-based	O
distinction	O
dickinson	O
and	O
dickinson	O
and	O
balleine	O
discuss	O
experimental	O
evidence	O
related	O
to	O
this	O
distinction	O
donahoe	O
and	O
burgos	O
alternatively	O
argue	O
that	O
model-free	O
processes	O
can	O
account	O
for	O
the	O
results	O
of	O
outcome-devaluation	O
experiments	O
dayan	O
and	O
berridge	O
argue	O
that	O
classical	B
conditioning	I
involves	O
model-based	O
processes	O
rangel	O
camerer	O
and	O
montague	O
review	O
many	O
of	O
the	O
outstanding	O
issues	O
involving	O
habitual	O
goal-directed	O
and	O
pavlovian	O
modes	O
of	O
control	B
comments	O
on	O
terminology	O
the	O
traditional	O
meaning	O
of	O
reinforcement	O
in	B
psychology	B
is	O
the	O
strengthening	O
of	O
a	O
pattern	O
of	O
behavior	O
increasing	O
either	O
its	O
intensity	O
or	O
frequency	O
as	O
a	O
result	O
of	O
an	O
animal	O
receiving	O
a	O
stimulus	O
experiencing	O
the	O
omission	O
of	O
a	O
stimulus	O
in	O
an	O
appropriate	O
temporal	O
relationship	O
with	O
another	O
stimulus	O
or	O
with	O
a	O
response	O
reinforcement	O
produces	O
changes	O
that	O
remain	O
in	O
future	O
behavior	O
sometimes	O
in	B
psychology	B
reinforcement	O
refers	O
to	O
the	O
process	O
of	O
producing	O
lasting	O
changes	O
in	O
behavior	O
whether	O
the	O
changes	O
strengthen	O
or	O
weaken	O
a	O
behavior	O
pattern	O
chapter	O
psychology	B
letting	O
reinforcement	O
refer	O
to	O
weakening	O
in	O
addition	O
to	O
strengthening	O
is	O
at	O
odds	O
with	O
the	O
everyday	O
meaning	O
of	O
reinforce	B
and	O
its	O
traditional	O
use	O
in	B
psychology	B
but	O
it	O
is	O
a	O
useful	O
extension	O
that	O
we	O
have	O
adopted	O
here	O
in	O
either	O
case	O
a	O
stimulus	O
considered	O
to	O
be	O
the	O
cause	O
of	O
the	O
behavioral	O
change	O
is	O
called	O
a	O
reinforcer	O
psychologists	O
do	O
not	O
generally	O
use	O
the	O
specific	O
phrase	O
reinforcement	B
learning	I
as	O
we	O
do	O
animal	O
learning	O
pioneers	O
probably	O
regarded	O
reinforcement	O
and	O
learning	O
as	O
being	O
synonymous	O
so	O
it	O
would	O
be	O
redundant	O
to	O
use	O
both	O
words	O
our	O
use	O
of	O
the	O
phrase	O
follows	O
its	O
use	O
in	O
computational	O
and	O
engineering	O
research	O
influenced	O
mostly	O
by	O
minsky	B
but	O
the	O
phrase	O
is	O
lately	O
gaining	O
currency	O
in	B
psychology	B
and	O
neuroscience	B
likely	O
because	O
strong	O
parallels	O
have	O
surfaced	O
between	O
reinforcement	B
learning	I
algorithms	O
and	O
animal	O
learning	O
parallels	O
described	O
in	O
this	O
chapter	O
and	O
the	O
next	O
according	O
to	O
common	O
usage	O
a	O
reward	O
is	O
an	O
object	O
or	O
event	O
that	O
an	O
animal	O
will	O
approach	O
and	O
work	O
for	O
a	O
reward	O
may	O
be	O
given	O
to	O
an	O
animal	O
in	O
recognition	O
of	O
its	O
good	O
behavior	O
or	O
given	O
in	O
order	O
to	O
make	O
the	O
animal	O
s	O
behavior	O
better	O
similarly	O
a	O
penalty	O
is	O
an	O
object	O
or	O
event	O
that	O
the	O
animal	O
usually	O
avoids	O
and	O
that	O
is	O
given	O
as	O
a	O
consequence	O
of	O
bad	O
behavior	O
usually	O
in	O
order	O
to	O
change	O
that	O
behavior	O
primary	O
reward	O
is	O
reward	O
due	O
to	O
machinery	O
built	O
into	O
an	O
animal	O
s	O
nervous	O
system	O
by	O
evolution	B
to	O
improve	O
its	O
chances	O
of	O
survival	O
and	O
reproduction	O
e	O
g	O
reward	O
produced	O
by	O
the	O
taste	O
of	O
nourishing	O
food	O
sexual	O
contact	O
successful	O
escape	O
and	O
many	O
other	O
stimuli	O
and	O
events	O
that	O
predicted	O
reproductive	O
success	O
over	O
the	O
animal	O
s	O
ancestral	O
history	O
as	O
explained	O
in	O
section	O
higher-order	O
reward	O
is	O
reward	O
delivered	O
by	O
stimuli	O
that	O
predict	O
primary	O
reward	O
either	O
directly	O
or	O
indirectly	O
by	O
predicting	O
other	O
stimuli	O
that	O
predict	O
primary	O
reward	O
reward	O
is	O
secondary	O
if	O
its	O
rewarding	O
quality	O
is	O
the	O
result	O
of	O
directly	O
predicting	O
primary	O
reward	O
in	O
this	O
book	O
we	O
call	O
rt	O
the	O
reward	B
signal	I
at	O
time	O
t	O
or	O
sometimes	O
just	O
the	O
reward	O
at	O
time	O
t	O
but	O
we	O
do	O
not	O
think	O
of	O
it	O
as	O
an	O
object	O
or	O
event	O
in	O
the	O
agent	O
s	O
environment	B
because	O
rt	O
is	O
a	O
number	O
not	O
an	O
object	O
or	O
an	O
event	O
it	O
is	O
more	O
like	O
a	O
reward	B
signal	I
in	B
neuroscience	B
which	O
is	O
a	O
signal	O
internal	O
to	O
the	O
brain	O
like	O
the	O
activity	O
of	O
neurons	O
that	O
influences	O
decision	O
making	O
and	O
learning	O
this	O
signal	O
might	O
be	O
triggered	O
when	O
the	O
animal	O
perceives	O
an	O
attractive	O
an	O
aversive	O
object	O
but	O
it	O
can	O
also	O
be	O
triggered	O
by	O
things	O
that	O
do	O
not	O
physically	O
exist	O
in	O
the	O
animal	O
s	O
external	O
environment	B
such	O
as	O
memories	O
ideas	O
or	O
hallucinations	O
because	O
our	O
rt	O
can	O
be	O
positive	O
negative	O
or	O
zero	O
it	O
might	O
be	O
better	O
to	O
call	O
a	O
negative	O
rt	O
a	O
penalty	O
and	O
an	O
rt	O
equal	O
to	O
zero	O
a	O
neutral	O
signal	O
but	O
for	O
simplicity	O
we	O
generally	O
avoid	O
these	O
terms	O
in	O
reinforcement	B
learning	I
the	O
process	O
that	O
generates	O
all	O
the	O
rts	O
defines	O
the	O
problem	O
the	O
agent	O
is	O
trying	O
to	O
solve	O
the	O
agent	O
s	O
objective	O
is	O
to	O
keep	O
the	O
magnitude	O
of	O
rt	O
as	O
large	O
as	O
possible	O
over	O
time	O
in	O
this	O
respect	O
rt	O
is	O
like	O
primary	O
reward	O
for	O
an	O
animal	O
if	O
we	O
think	O
of	O
the	O
problem	O
the	O
animal	O
faces	O
as	O
the	O
problem	O
of	O
obtaining	O
as	O
much	O
primary	O
reward	O
as	O
possible	O
over	O
its	O
lifetime	O
thereby	O
through	O
the	O
prospective	O
wisdom	O
of	O
evolution	B
improve	O
its	O
chances	O
of	O
solving	O
its	O
real	O
problem	O
which	O
is	O
to	O
pass	O
its	O
genes	O
on	O
to	O
future	O
generations	O
however	O
as	O
we	O
suggest	O
in	O
chapter	O
it	O
is	O
unlikely	O
that	O
there	O
is	O
a	O
single	O
master	O
reward	B
signal	I
like	O
rt	O
in	O
an	O
animal	O
s	O
brain	O
not	O
all	O
reinforcers	O
are	O
rewards	O
or	O
penalties	O
sometimes	O
reinforcement	O
is	O
not	O
the	O
result	O
of	O
an	O
animal	O
receiving	O
a	O
stimulus	O
that	O
evaluates	O
its	O
behavior	O
by	O
labeling	O
the	O
behavior	O
good	O
or	O
bad	O
a	O
behavior	O
pattern	O
can	O
be	O
reinforced	O
by	O
a	O
stimulus	O
that	O
arrives	O
to	O
an	O
animal	O
summary	O
no	O
matter	O
how	O
the	O
animal	O
behaved	O
as	O
described	O
in	O
section	O
whether	O
the	O
delivery	O
of	O
reinforcer	O
depends	O
or	O
does	O
not	O
depend	O
on	O
preceding	O
behavior	O
is	O
the	O
defining	O
difference	O
between	O
instrumental	O
or	O
operant	O
conditioning	O
experiments	O
and	O
classical	O
or	O
pavlovian	O
conditioning	O
experiments	O
reinforcement	O
is	O
at	O
work	O
in	O
both	O
types	O
of	O
experiments	O
but	O
only	O
in	O
the	O
former	O
is	O
it	O
feedback	O
that	O
evaluates	O
past	O
behavior	O
it	O
has	O
often	O
been	O
pointed	O
out	O
that	O
even	O
when	O
the	O
reinforcing	O
us	O
in	O
a	O
classical	B
conditioning	I
experiment	O
is	O
not	O
contingent	O
on	O
the	O
subject	O
s	O
preceding	O
behavior	O
its	O
reinforcing	O
value	B
can	O
be	O
influenced	O
by	O
this	O
behavior	O
an	O
example	O
being	O
that	O
a	O
closed	O
eye	O
makes	O
an	O
air	O
puff	O
to	O
the	O
eye	O
less	O
aversive	O
the	O
distinction	O
between	O
reward	O
signals	O
and	B
reinforcement	I
signals	O
is	O
a	O
crucial	O
point	O
when	O
we	O
discuss	O
neural	B
correlates	O
of	O
these	O
signals	O
in	O
the	O
next	O
chapter	O
like	O
a	O
reward	B
signal	I
for	O
us	O
the	O
reinforcement	B
signal	I
at	O
any	O
specific	O
time	O
is	O
a	O
positive	O
or	O
negative	O
number	O
or	O
zero	O
a	O
reinforcement	B
signal	I
is	O
the	O
major	O
factor	O
directing	O
changes	O
a	O
learning	O
algorithm	O
makes	O
in	O
an	O
agent	O
s	O
policy	B
value	B
estimates	O
or	O
environment	B
models	O
the	O
definition	O
that	O
makes	O
the	O
most	O
sense	O
to	O
us	O
is	O
that	O
a	O
reinforcement	B
signal	I
at	O
any	O
time	O
is	O
a	O
number	O
that	O
multiplies	O
along	O
with	O
some	O
constants	O
a	O
vector	B
to	O
determine	O
parameter	O
updates	O
in	O
some	O
learning	O
algorithm	O
for	O
some	O
algorithms	O
the	O
reward	B
signal	I
alone	O
is	O
the	O
critical	O
multiplier	O
in	O
the	O
parameterupdate	O
equation	O
for	O
these	O
algorithms	O
the	O
reinforcement	B
signal	I
is	O
the	O
same	O
as	O
the	O
reward	B
signal	I
but	O
for	O
most	O
of	O
the	O
algorithms	O
we	O
discuss	O
in	O
this	O
book	O
reinforcement	O
signals	O
include	O
terms	O
in	O
addition	O
to	O
the	O
reward	B
signal	I
an	O
example	O
being	O
a	O
td	B
error	I
t	O
v	O
v	O
which	O
is	O
the	O
reinforcement	B
signal	I
for	O
td	B
state-value	O
learning	O
analogous	O
td	B
errors	O
for	O
action-value	O
learning	O
in	O
this	O
reinforcement	B
signal	I
is	O
the	O
primary	O
reinforcement	O
contribution	O
and	O
the	O
temporal	O
difference	O
in	O
predicted	O
values	O
v	O
v	O
an	O
analogous	O
temporal	O
difference	O
for	B
action	B
values	I
is	O
the	O
conditioned	O
reinforcement	O
contribution	O
thus	O
whenever	O
v	O
v	O
t	O
signals	O
pure	O
primary	O
reinforcement	O
and	O
whenever	O
it	O
signals	O
pure	O
conditioned	O
reinforcement	O
but	O
it	O
often	O
signals	O
a	O
mixture	O
of	O
these	O
note	O
as	O
we	O
mentioned	O
in	O
section	O
this	O
t	O
is	O
not	O
available	O
until	O
time	O
t	O
we	O
therefore	O
think	O
of	O
t	O
as	O
the	O
reinforcement	B
signal	I
at	O
time	O
which	O
is	O
fitting	O
because	O
it	O
reinforces	O
predictions	O
andor	O
actions	O
made	O
earlier	O
at	O
step	O
t	O
a	O
possible	O
source	O
of	O
confusion	O
is	O
the	O
terminology	O
used	O
by	O
the	O
famous	O
psychologist	O
b	O
f	O
skinner	B
and	O
his	O
followers	O
for	O
skinner	B
positive	O
reinforcement	O
occurs	O
when	O
the	O
consequences	O
of	O
an	O
animal	O
s	O
behavior	O
increase	O
the	O
frequency	O
of	O
that	O
behavior	O
punishment	O
occurs	O
when	O
the	O
behavior	O
s	O
consequences	O
decrease	O
that	O
behavior	O
s	O
frequency	O
negative	O
reinforcement	O
occurs	O
when	O
behavior	O
leads	O
to	O
the	O
removal	O
of	O
an	O
aversive	O
stimulus	O
is	O
a	O
stimulus	O
the	O
animal	O
does	O
not	O
like	O
thereby	O
increasing	O
the	O
frequency	O
of	O
that	O
behavior	O
negative	O
punishment	O
on	O
the	O
other	O
hand	O
occurs	O
when	O
behavior	O
leads	O
to	O
the	O
removal	O
of	O
an	O
appetitive	O
stimulus	O
is	O
a	O
stimulus	O
the	O
animal	O
likes	O
thereby	O
decreasing	O
the	O
frequency	O
of	O
that	O
behavior	O
we	O
find	O
no	O
critical	O
need	O
for	O
these	O
distinctions	O
because	O
our	O
approach	O
is	O
more	O
abstract	O
than	O
this	O
with	O
both	O
reward	O
and	B
reinforcement	I
signals	O
allowed	O
to	O
take	O
on	O
both	O
positive	O
and	O
negative	O
values	O
note	O
especially	O
that	O
when	O
our	O
reinforcement	B
signal	I
is	O
negative	O
it	O
is	O
not	O
the	O
same	O
as	O
skinner	B
s	O
negative	O
reinforcement	O
chapter	O
psychology	B
on	O
the	O
other	O
hand	O
it	O
has	O
often	O
been	O
pointed	O
out	O
that	O
using	O
a	O
single	O
number	O
as	O
a	O
reward	O
or	O
a	O
penalty	O
signal	O
depending	O
only	O
on	O
its	O
sign	O
is	O
at	O
odds	O
with	O
the	O
fact	O
that	O
animals	O
appetitive	O
and	O
aversive	O
systems	O
have	O
qualitatively	O
different	O
properties	O
and	O
involve	O
different	O
brain	O
mechanisms	O
this	O
points	O
to	O
a	O
direction	O
in	O
which	O
the	O
reinforcement	B
learning	I
framework	O
might	O
be	O
developed	O
in	O
the	O
future	O
to	O
exploit	O
computational	O
advantages	B
of	I
separate	O
appetitive	O
and	O
aversive	O
systems	O
but	O
for	O
now	O
we	O
are	O
passing	O
over	O
these	O
possibilities	O
another	O
discrepancy	O
in	O
terminology	O
is	O
how	O
we	O
use	O
the	O
word	O
action	B
to	O
many	O
cognitive	O
scientists	O
an	O
action	B
is	O
purposeful	O
in	O
the	O
sense	O
of	O
being	O
the	O
result	O
of	O
an	O
animal	O
s	O
knowledge	O
about	O
the	O
relationship	O
between	O
the	O
behavior	O
in	O
question	O
and	O
the	O
consequences	O
of	O
that	O
behavior	O
an	O
action	B
is	O
goal-directed	O
and	O
the	O
result	O
of	O
a	O
decision	O
in	O
contrast	O
to	O
a	O
response	O
which	O
is	O
triggered	O
by	O
a	O
stimulus	O
the	O
result	O
of	O
a	O
reflex	O
or	O
a	O
habit	O
we	O
use	O
the	O
word	O
action	B
without	O
differentiating	O
among	O
what	O
others	O
call	O
actions	O
decisions	O
and	O
responses	O
these	O
are	O
important	O
distinctions	O
but	O
for	O
us	O
they	O
are	O
encompassed	O
by	O
differences	O
between	O
model-free	O
and	O
model-based	B
reinforcement	B
learning	I
algorithms	O
which	O
we	O
discussed	O
above	O
in	O
relation	O
to	O
habitual	O
and	O
goal-directed	O
behavior	O
in	O
section	O
dickinson	O
discusses	O
the	O
distinction	O
between	O
responses	O
and	O
actions	O
a	O
term	O
used	O
a	O
lot	O
in	O
this	O
book	O
is	O
control	B
what	O
we	O
mean	O
by	O
control	B
is	O
entirely	O
different	O
from	O
what	O
it	O
means	O
to	O
animal	O
learning	O
psychologists	O
by	O
control	B
we	O
mean	O
that	O
an	O
agent	O
influences	O
its	O
environment	B
to	O
bring	O
about	O
states	O
or	O
events	O
that	O
the	O
agent	O
prefers	O
the	O
agent	O
exerts	O
control	B
over	O
its	O
environment	B
this	O
is	O
the	O
sense	O
of	O
control	B
used	O
by	O
control	B
engineers	O
in	B
psychology	B
on	O
the	O
other	O
hand	O
control	B
typically	O
means	O
that	O
an	O
animal	O
s	O
behavior	O
is	O
influenced	O
by	O
is	O
controlled	O
by	O
the	O
stimuli	O
the	O
animal	O
receives	O
control	B
or	O
the	O
reinforcement	O
schedule	O
it	O
experiences	O
here	O
the	O
environment	B
is	O
controlling	O
the	O
agent	O
control	B
in	O
this	O
sense	O
is	O
the	O
basis	O
of	O
behavior	O
modification	O
therapy	O
of	O
course	O
both	O
of	O
these	O
directions	O
of	O
control	B
are	O
at	O
play	O
when	O
an	O
agent	O
interacts	O
with	O
its	O
environment	B
but	O
our	O
focus	O
is	O
on	O
the	O
agent	O
as	O
controller	O
not	O
the	O
environment	B
as	O
controller	O
a	O
view	O
equivalent	O
to	O
ours	O
and	O
perhaps	O
more	O
illuminating	O
is	O
that	O
the	O
agent	O
is	O
actually	O
controlling	O
the	O
input	O
it	O
receives	O
from	O
its	O
environment	B
this	O
is	O
not	O
what	O
psychologists	O
mean	O
by	O
stimulus	O
control	B
sometimes	O
reinforcement	B
learning	I
is	O
understood	O
to	O
refer	O
solely	O
to	O
learning	O
policies	O
directly	O
from	O
rewards	O
penalties	O
without	O
the	O
involvement	O
of	O
value	B
functions	O
or	O
environment	B
models	O
this	O
is	O
what	O
psychologists	O
call	O
stimulus-response	O
or	O
s-r	O
learning	O
but	O
for	O
us	O
along	O
with	O
most	O
of	O
today	O
s	O
psychologists	O
reinforcement	B
learning	I
is	O
much	O
broader	O
than	O
this	O
including	O
in	O
addition	O
to	O
s-r	O
learning	O
methods	O
involving	O
value	B
functions	O
environment	B
models	O
planning	B
and	O
other	O
processes	O
that	O
are	O
commonly	O
thought	O
to	O
belong	O
to	O
the	O
more	O
cognitive	O
side	O
of	O
mental	O
functioning	O
chapter	O
neuroscience	B
neuroscience	B
is	O
the	O
multidisciplinary	O
study	O
of	O
nervous	O
systems	O
how	O
they	O
regulate	O
bodily	O
functions	O
control	B
behavior	O
change	O
over	O
time	O
as	O
a	O
result	O
of	O
development	O
learning	O
and	O
aging	O
and	O
how	O
cellular	O
and	O
molecular	O
mechanisms	O
make	O
these	O
functions	O
possible	O
one	O
of	O
the	O
most	O
exciting	O
aspects	O
of	O
reinforcement	B
learning	I
is	O
the	O
mounting	O
evidence	O
from	O
neuroscience	B
that	O
the	O
nervous	O
systems	O
of	O
humans	O
and	O
many	O
other	O
animals	O
implement	O
algorithms	O
that	O
correspond	O
in	O
striking	O
ways	O
to	O
reinforcement	B
learning	I
algorithms	O
the	O
main	O
objective	O
of	O
this	O
chapter	O
is	O
to	O
explain	O
these	O
parallels	O
and	O
what	O
they	O
suggest	O
about	O
the	O
neural	B
basis	O
of	O
reward-related	O
learning	O
in	O
animals	O
the	O
most	O
remarkable	O
point	O
of	O
contact	O
between	O
reinforcement	B
learning	I
and	O
neuroscience	B
involves	O
dopamine	B
a	O
chemical	O
deeply	O
involved	O
in	O
reward	O
processing	O
in	O
the	O
brains	O
of	O
mammals	O
dopamine	B
appears	O
to	O
convey	O
temporal-difference	O
errors	O
to	O
brain	O
structures	O
where	O
learning	O
and	O
decision	O
making	O
take	O
place	O
this	O
parallel	O
is	O
expressed	O
by	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
a	O
hypothesis	O
that	O
resulted	O
from	O
the	O
convergence	O
of	O
computational	O
reinforcement	B
learning	I
and	O
results	O
of	O
neuroscience	B
experiments	O
in	O
this	O
chapter	O
we	O
discuss	O
this	O
hypothesis	O
the	O
neuroscience	B
findings	O
that	O
led	O
to	O
it	O
and	O
why	O
it	O
is	O
a	O
significant	O
contribution	O
to	O
understanding	O
brain	O
reward	O
systems	O
we	O
also	O
discuss	O
parallels	O
between	O
reinforcement	B
learning	I
and	O
neuroscience	B
that	O
are	O
less	O
striking	O
than	O
this	O
dopaminetd-error	O
parallel	O
but	O
that	O
provide	O
useful	O
conceptual	O
tools	O
for	O
thinking	O
about	O
reward-based	O
learning	O
in	O
animals	O
other	O
elements	O
of	O
reinforcement	B
learning	I
have	O
the	O
potential	O
to	O
impact	O
the	O
study	O
of	O
nervous	O
systems	O
but	O
their	O
connections	O
to	O
neuroscience	B
are	O
still	O
relatively	O
undeveloped	O
we	O
discuss	O
several	O
of	O
these	O
evolving	O
connections	O
that	O
we	O
think	O
will	O
grow	O
in	O
importance	O
over	O
time	O
as	O
we	O
outlined	O
in	O
the	O
history	O
section	O
of	O
this	O
book	O
s	O
introductory	O
chapter	O
many	O
aspects	O
of	O
reinforcement	B
learning	I
were	O
influenced	O
by	O
neuroscience	B
a	O
second	O
objective	O
of	O
this	O
chapter	O
is	O
to	O
acquaint	O
readers	O
with	O
ideas	O
about	O
brain	O
function	O
that	O
have	O
contributed	O
to	O
our	O
approach	O
to	O
reinforcement	B
learning	I
some	O
elements	O
of	O
reinforcement	B
learning	I
are	O
easier	O
to	O
understand	O
when	O
seen	O
in	O
light	O
of	O
theories	O
of	O
brain	O
function	O
this	O
is	O
particularly	O
true	O
for	O
the	O
idea	O
of	O
the	O
eligibility	O
trace	O
one	O
of	O
the	O
basic	O
mechanisms	O
of	O
reinforcement	B
learning	I
that	O
originated	O
as	O
a	O
conjectured	O
property	O
of	O
synapses	O
the	O
structures	O
by	O
which	O
nerve	O
cells	O
neurons	O
communicate	O
with	O
one	O
another	O
chapter	O
neuroscience	B
in	O
this	O
chapter	O
we	O
do	O
not	O
delve	O
very	O
deeply	O
into	O
the	O
enormous	O
complexity	O
of	O
the	O
neural	B
systems	O
underlying	O
reward-based	O
learning	O
in	O
animals	O
this	O
chapter	O
is	O
too	O
short	O
and	O
we	O
are	O
not	O
neuroscientists	O
we	O
do	O
not	O
try	O
to	O
describe	O
or	O
even	O
to	O
name	O
the	O
very	O
many	O
brain	O
structures	O
and	O
pathways	O
or	O
any	O
of	O
the	O
molecular	O
mechanisms	O
believed	O
to	O
be	O
involved	O
in	O
these	O
processes	O
we	O
also	O
do	O
not	O
do	O
justice	O
to	O
hypotheses	O
and	O
models	O
that	O
are	O
alternatives	O
to	O
those	O
that	O
align	O
so	O
well	O
with	O
reinforcement	B
learning	I
it	O
should	O
not	O
be	O
surprising	O
that	O
there	O
are	O
differing	O
views	O
among	O
experts	O
in	O
the	O
field	O
we	O
can	O
only	O
provide	O
a	O
glimpse	O
into	O
this	O
fascinating	O
and	O
developing	O
story	O
we	O
hope	O
though	O
that	O
this	O
chapter	O
convinces	O
you	O
that	O
a	O
very	O
fruitful	O
channel	O
has	O
emerged	O
connecting	O
reinforcement	B
learning	I
and	O
its	O
theoretical	O
underpinnings	O
to	O
the	O
neuroscience	B
of	O
reward-based	O
learning	O
in	O
animals	O
many	O
excellent	O
publications	O
cover	O
links	O
between	O
reinforcement	B
learning	I
and	O
neuroscience	B
some	O
of	O
which	O
we	O
cite	O
in	O
this	O
chapter	O
s	O
final	O
section	O
our	O
treatment	O
differs	O
from	O
most	O
of	O
these	O
because	O
we	O
assume	O
familiarity	O
with	O
reinforcement	B
learning	I
as	O
presented	O
in	O
the	O
earlier	O
chapters	O
of	O
this	O
book	O
but	O
we	O
do	O
not	O
assume	O
knowledge	O
of	O
neuroscience	B
we	O
begin	O
with	O
a	O
brief	O
introduction	O
to	O
the	O
neuroscience	B
concepts	O
needed	O
for	O
a	O
basic	O
understanding	O
of	O
what	O
is	O
to	O
follow	O
neuroscience	B
basics	O
some	O
basic	O
information	O
about	O
nervous	O
systems	O
is	O
helpful	O
for	O
following	O
what	O
we	O
cover	O
in	O
this	O
chapter	O
terms	O
that	O
we	O
refer	O
to	O
later	O
are	O
italicized	O
skipping	O
this	O
section	O
will	O
not	O
be	O
a	O
problem	O
if	O
you	O
already	O
have	O
an	O
elementary	O
knowledge	O
of	O
neuroscience	B
neurons	O
the	O
main	O
components	O
of	O
nervous	O
systems	O
are	O
cells	O
specialized	O
for	O
processing	O
and	O
transmitting	O
information	O
using	O
electrical	O
and	O
chemical	O
signals	O
they	O
come	O
in	O
many	O
forms	O
but	O
a	O
neuron	O
typically	O
has	O
a	O
cell	O
body	O
dendrites	O
and	O
a	O
single	O
axon	O
dendrites	O
are	O
structures	O
that	O
branch	O
from	O
the	O
cell	O
body	O
to	O
receive	O
input	O
from	O
other	O
neurons	O
to	O
also	O
receive	O
external	O
signals	O
in	O
the	O
case	O
of	O
sensory	O
neurons	O
a	O
neuron	O
s	O
axon	O
is	O
a	O
fiber	O
that	O
carries	O
the	O
neuron	O
s	O
output	O
to	O
other	O
neurons	O
to	O
muscles	O
or	O
glands	O
a	O
neuron	O
s	O
output	O
consists	O
of	O
sequences	O
of	O
electrical	O
pulses	O
called	O
action	B
potentials	O
that	O
travel	O
along	O
the	O
axon	O
action	B
potentials	O
are	O
also	O
called	O
spikes	O
and	O
a	O
neuron	O
is	O
said	O
to	O
fire	O
when	O
it	O
generates	O
a	O
spike	O
in	O
models	O
of	O
neural	B
networks	O
it	O
is	O
common	O
to	O
use	O
real	O
numbers	O
to	O
represent	O
a	O
neuron	O
s	O
firing	O
rate	O
the	O
average	O
number	O
of	O
spikes	O
per	O
some	O
unit	O
of	O
time	O
a	O
neuron	O
s	O
axon	O
can	O
branch	O
widely	O
so	O
that	O
the	O
neuron	O
s	O
action	B
potentials	O
reach	O
many	O
targets	O
the	O
branching	O
structure	O
of	O
a	O
neuron	O
s	O
axon	O
is	O
called	O
the	O
neuron	O
s	O
axonal	O
arbor	O
because	O
the	O
conduction	O
of	O
an	O
action	B
potential	O
is	O
an	O
active	O
process	O
not	O
unlike	O
the	O
burning	O
of	O
a	O
fuse	O
when	O
an	O
action	B
potential	O
reaches	O
an	O
axonal	O
branch	O
point	O
it	O
lights	O
up	O
action	B
potentials	O
on	O
all	O
of	O
the	O
outgoing	O
branches	O
propagation	O
to	O
a	O
branch	O
can	O
sometimes	O
fail	O
as	O
a	O
result	O
the	O
activity	O
of	O
a	O
neuron	O
with	O
a	O
large	O
axonal	O
arbor	O
can	O
influence	O
many	O
target	O
sites	O
a	O
synapse	O
is	O
a	O
structure	O
generally	O
at	O
the	O
termination	O
of	O
an	O
axon	O
branch	O
that	O
mediates	O
the	O
communication	O
of	O
one	O
neuron	O
to	O
another	O
a	O
synapse	O
transmits	O
information	O
from	O
the	O
presynaptic	O
neuron	O
s	O
axon	O
to	O
a	O
dendrite	O
or	O
cell	O
body	O
of	O
the	O
postsynaptic	O
neuron	O
with	O
a	O
few	O
exceptions	O
synapses	O
release	O
a	O
chemical	O
neurotransmitter	O
upon	O
the	O
arrival	O
reward	O
signals	O
reinforcement	O
signals	O
values	O
and	O
prediction	B
errors	O
of	O
an	O
action	B
potential	O
from	O
the	O
presynaptic	O
neuron	O
exceptions	O
are	O
cases	O
of	O
direct	O
electric	O
coupling	O
between	O
neurons	O
but	O
these	O
will	O
not	O
concern	O
us	O
here	O
neurotransmitter	O
molecules	O
released	O
from	O
the	O
presynaptic	O
side	O
of	O
the	O
synapse	O
diffuse	O
across	O
the	O
synaptic	O
cleft	O
the	O
very	O
small	O
space	O
between	O
the	O
presynaptic	O
ending	O
and	O
the	O
postsynaptic	O
neuron	O
and	O
then	O
bind	O
to	O
receptors	O
on	O
the	O
surface	O
of	O
the	O
postsynaptic	O
neuron	O
to	O
excite	O
or	O
inhibit	O
its	O
spike-generating	O
activity	O
or	O
to	O
modulate	O
its	O
behavior	O
in	O
other	O
ways	O
a	O
particular	O
neurotransmitter	O
may	O
bind	O
to	O
several	O
different	O
types	O
of	O
receptors	O
with	O
each	O
producing	O
a	O
different	O
effect	O
on	O
the	O
postsynaptic	O
neuron	O
for	O
example	O
there	O
are	O
at	O
least	O
five	O
different	O
receptor	O
types	O
by	O
which	O
the	O
neurotransmitter	O
dopamine	B
can	O
affect	O
a	O
postsynaptic	O
neuron	O
many	O
different	O
chemicals	O
have	O
been	O
identified	O
as	O
neurotransmitters	O
in	O
animal	O
nervous	O
systems	O
a	O
neuron	O
s	O
background	O
activity	O
is	O
its	O
level	O
of	O
activity	O
usually	O
its	O
firing	O
rate	O
when	O
the	O
neuron	O
does	O
not	O
appear	O
to	O
be	O
driven	O
by	O
synaptic	O
input	O
related	O
to	O
the	O
task	O
of	O
interest	O
to	O
the	O
experimenter	O
for	O
example	O
when	O
the	O
neuron	O
s	O
activity	O
is	O
not	O
correlated	O
with	O
a	O
stimulus	O
delivered	O
to	O
a	O
subject	O
as	O
part	O
of	O
an	O
experiment	O
background	O
activity	O
can	O
be	O
irregular	O
due	O
to	O
input	O
from	O
the	O
wider	O
network	O
or	O
due	O
to	O
noise	O
within	O
the	O
neuron	O
or	O
its	O
synapses	O
sometimes	O
background	O
activity	O
is	O
the	O
result	O
of	O
dynamic	O
processes	O
intrinsic	B
to	O
the	O
neuron	O
a	O
neuron	O
s	O
phasic	O
activity	O
in	O
contrast	O
to	O
its	O
background	O
activity	O
consists	O
of	O
bursts	O
of	O
spiking	O
activity	O
usually	O
caused	O
by	O
synaptic	O
input	O
activity	O
that	O
varies	O
slowly	O
and	O
often	O
in	O
a	O
graded	O
manner	O
whether	O
as	O
background	O
activity	O
or	O
not	O
is	O
called	O
a	O
neuron	O
s	O
tonic	O
activity	O
the	O
strength	O
or	O
effectiveness	O
by	O
which	O
the	O
neurotransmitter	O
released	O
at	O
a	O
synapse	O
influences	O
the	O
postsynaptic	O
neuron	O
is	O
the	O
synapse	O
s	O
efficacy	O
one	O
way	O
a	O
nervous	O
system	O
can	O
change	O
through	O
experience	O
is	O
through	O
changes	O
in	O
synaptic	O
efficacies	O
as	O
a	O
result	O
of	O
combinations	O
of	O
the	O
activities	O
of	O
the	O
presynaptic	O
and	O
postsynaptic	O
neurons	O
and	O
sometimes	O
by	O
the	O
presence	O
of	O
a	O
neuromodulator	O
which	O
is	O
a	O
neurotransmitter	O
having	O
effects	O
other	O
than	O
or	O
in	O
addition	O
to	O
direct	O
fast	O
excitation	O
or	O
inhibition	O
brains	O
contain	O
several	O
different	O
neuromodulation	O
systems	O
consisting	O
of	O
clusters	O
of	O
neurons	O
with	O
widely	O
branching	O
axonal	O
arbors	O
with	O
each	O
system	O
using	O
a	O
different	O
neurotransmitter	O
neuromodulation	O
can	O
alter	O
the	O
function	O
of	O
neural	B
circuits	O
mediate	O
motivation	B
arousal	O
attention	O
memory	O
mood	O
emotion	O
sleep	O
and	O
body	O
temperature	O
important	O
here	O
is	O
that	O
a	O
neuromodulatory	O
system	O
can	O
distribute	O
something	O
like	O
a	O
scalar	O
signal	O
such	O
as	O
a	O
reinforcement	B
signal	I
to	O
alter	O
the	O
operation	O
of	O
synapses	O
in	O
widely	O
distributed	O
sites	O
critical	O
for	O
learning	O
the	O
ability	O
of	O
synaptic	O
efficacies	O
to	O
change	O
is	O
called	O
synaptic	B
plasticity	I
it	O
is	O
one	O
of	O
the	O
primary	O
mechanisms	O
responsible	O
for	O
learning	O
the	O
parameters	O
or	O
weights	O
adjusted	O
by	O
learning	O
algorithms	O
correspond	O
to	O
synaptic	O
efficacies	O
as	O
we	O
detail	O
below	O
modulation	O
of	O
synaptic	B
plasticity	I
via	O
the	O
neuromodulator	O
dopamine	B
is	O
a	O
plausible	O
mechanism	O
for	O
how	O
the	O
brain	O
might	O
implement	O
learning	O
algorithms	O
like	O
many	O
of	O
those	O
described	O
in	O
this	O
book	O
chapter	O
neuroscience	B
reward	O
signals	O
reinforcement	O
signals	O
values	O
and	O
prediction	B
errors	O
links	O
between	O
neuroscience	B
and	O
computational	O
reinforcement	B
learning	I
begin	O
as	O
parallels	O
between	O
signals	O
in	O
the	O
brain	O
and	O
signals	O
playing	O
prominent	O
roles	O
in	O
reinforcement	B
learning	I
theory	O
and	O
algorithms	O
in	O
chapter	O
we	O
said	O
that	O
any	O
problem	O
of	O
learning	O
goal-directed	O
behavior	O
can	O
be	O
reduced	O
to	O
the	O
three	O
signals	O
representing	O
actions	O
states	O
and	O
rewards	O
however	O
to	O
explain	O
links	O
that	O
have	O
been	O
made	O
between	O
neuroscience	B
and	B
reinforcement	B
learning	I
we	O
have	O
to	O
be	O
less	O
abstract	O
than	O
this	O
and	O
consider	O
other	O
reinforcement	B
learning	I
signals	O
that	O
correspond	O
in	O
certain	O
ways	O
to	O
signals	O
in	O
the	O
brain	O
in	O
addition	O
to	O
reward	O
signals	O
these	O
include	O
reinforcement	O
signals	O
we	O
argue	O
are	O
different	O
from	O
reward	O
signals	O
value	B
signals	O
and	O
signals	O
conveying	O
prediction	B
errors	O
when	O
we	O
label	O
a	O
signal	O
by	O
its	O
function	O
in	O
this	O
way	O
we	O
are	O
doing	O
it	O
in	O
the	O
context	O
of	O
reinforcement	B
learning	I
theory	O
in	O
which	O
the	O
signal	O
corresponds	O
to	O
a	O
term	O
in	O
an	O
equation	O
or	O
an	O
algorithm	O
on	O
the	O
other	O
hand	O
when	O
we	O
refer	O
to	O
a	O
signal	O
in	O
the	O
brain	O
we	O
mean	O
a	O
physiological	O
event	O
such	O
as	O
a	O
burst	O
of	O
action	B
potentials	O
or	O
the	O
secretion	O
of	O
a	O
neurotransmitter	O
labeling	O
a	O
neural	B
signal	O
by	O
its	O
function	O
for	O
example	O
calling	O
the	O
phasic	O
activity	O
of	O
a	O
dopamine	B
neuron	O
a	O
reinforcement	B
signal	I
means	O
that	O
the	O
neural	B
signal	O
behaves	O
like	O
and	O
is	O
conjectured	O
to	O
function	O
like	O
the	O
corresponding	O
theoretical	O
signal	O
uncovering	O
evidence	O
for	O
these	O
correspondences	O
involves	O
many	O
challenges	O
neural	B
activity	O
related	O
to	O
reward	O
processing	O
can	O
be	O
found	O
in	O
nearly	O
every	O
part	O
of	O
the	O
brain	O
and	O
it	O
is	O
difficult	O
to	O
interpret	O
results	O
unambiguously	O
because	O
representations	O
of	O
different	O
rewardrelated	O
signals	O
tend	O
to	O
be	O
highly	O
correlated	O
with	O
one	O
another	O
experiments	O
need	O
to	O
be	O
carefully	O
designed	O
to	O
allow	O
one	O
type	O
of	O
reward-related	O
signal	O
to	O
be	O
distinguished	O
with	O
any	O
degree	O
of	O
certainty	O
from	O
others	O
or	O
from	O
an	O
abundance	O
of	O
other	O
signals	O
not	O
related	O
to	O
reward	O
processing	O
despite	O
these	O
difficulties	O
many	O
experiments	O
have	O
been	O
conducted	O
with	O
the	O
aim	O
of	O
reconciling	O
aspects	O
of	O
reinforcement	B
learning	I
theory	O
and	O
algorithms	O
with	O
neural	B
signals	O
and	O
some	O
compelling	O
links	O
have	O
been	O
established	O
to	O
prepare	O
for	O
examining	O
these	O
links	O
in	O
the	O
rest	O
of	O
this	O
section	O
we	O
remind	O
the	O
reader	O
of	O
what	O
various	O
reward-related	O
signals	O
mean	O
according	O
to	O
reinforcement	B
learning	I
theory	O
in	O
our	O
comments	O
on	O
terminology	O
at	O
the	O
end	O
of	O
the	O
previous	O
chapter	O
we	O
said	O
that	O
rt	O
is	O
like	O
a	O
reward	B
signal	I
in	O
an	O
animal	O
s	O
brain	O
and	O
not	O
an	O
object	O
or	O
event	O
in	O
the	O
animal	O
s	O
environment	B
in	O
reinforcement	B
learning	I
the	O
reward	B
signal	I
with	O
an	O
agent	O
s	O
environment	B
defines	O
the	O
problem	O
a	O
reinforcement	B
learning	I
agent	O
is	O
trying	O
to	O
solve	O
in	O
this	O
respect	O
rt	O
is	O
like	O
a	O
signal	O
in	O
an	O
animal	O
s	O
brain	O
that	O
distributes	O
primary	O
reward	O
to	O
sites	O
throughout	O
the	O
brain	O
but	O
it	O
is	O
unlikely	O
that	O
a	O
unitary	O
master	O
reward	B
signal	I
like	O
rt	O
exists	O
in	O
an	O
animal	O
s	O
brain	O
it	O
is	O
best	O
to	O
think	O
of	O
rt	O
as	O
an	O
abstraction	O
summarizing	O
the	O
overall	O
effect	O
of	O
a	O
multitude	O
of	O
neural	B
signals	O
generated	O
by	O
many	O
systems	O
in	O
the	O
brain	O
that	O
assess	O
the	O
rewarding	O
or	O
punishing	O
qualities	O
of	O
sensations	O
and	O
states	O
reinforcement	O
signals	O
in	O
reinforcement	B
learning	I
are	O
different	O
from	O
reward	O
signals	O
the	O
function	O
of	O
a	O
reinforcement	B
signal	I
is	O
to	O
direct	O
the	O
changes	O
a	O
learning	O
algorithm	O
makes	O
in	O
an	O
agent	O
s	O
policy	B
value	B
estimates	O
or	O
environment	B
models	O
for	O
a	O
td	B
method	O
for	O
instance	O
the	O
reinforcement	B
signal	I
at	O
time	O
t	O
is	O
the	O
td	B
error	I
t	O
rt	O
v	O
v	O
the	O
as	O
we	O
mentioned	O
in	O
section	O
t	O
in	O
our	O
notation	O
is	O
defined	O
to	O
be	O
v	O
v	O
so	O
t	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
reinforcement	B
signal	I
for	O
some	O
algorithms	O
could	O
be	O
just	O
the	O
reward	B
signal	I
but	O
for	O
most	O
of	O
the	O
algorithms	O
we	O
consider	O
the	O
reinforcement	B
signal	I
is	O
the	O
reward	B
signal	I
adjusted	O
by	O
other	O
information	O
such	O
as	O
the	O
value	B
estimates	O
in	O
td	B
errors	O
estimates	O
of	O
state	B
values	O
or	O
of	O
action	B
values	O
that	O
is	O
v	O
or	O
q	O
specify	O
what	O
is	O
good	O
or	O
bad	O
for	O
the	O
agent	O
over	O
the	O
long	O
run	O
they	O
are	O
predictions	O
of	O
the	O
total	O
reward	O
an	O
agent	O
can	O
expect	O
to	O
accumulate	O
over	O
the	O
future	O
agents	O
make	O
good	O
decisions	O
by	O
selecting	O
actions	O
leading	O
to	O
states	O
with	O
the	O
largest	O
estimated	O
state	B
values	O
or	O
by	O
selecting	O
actions	O
with	O
the	O
largest	O
estimated	O
action	B
values	O
prediction	B
errors	O
measure	O
discrepancies	O
between	O
expected	B
and	O
actual	O
signals	O
or	O
sensations	O
reward	O
prediction	B
errors	O
specifically	O
measure	O
discrepancies	O
between	O
the	O
expected	B
and	O
the	O
received	O
reward	B
signal	I
being	O
positive	O
when	O
the	O
reward	B
signal	I
is	O
greater	O
than	O
expected	B
and	O
negative	O
otherwise	O
td	B
errors	O
like	O
are	O
special	O
kinds	O
of	O
rpes	O
that	O
signal	O
discrepancies	O
between	O
current	O
and	O
earlier	O
expectations	O
of	O
reward	O
over	O
the	O
long-term	O
when	O
neuroscientists	O
refer	O
to	O
rpes	O
they	O
generally	O
not	O
always	O
mean	O
td	B
rpes	O
which	O
we	O
simply	O
call	O
td	B
errors	O
throughout	O
this	O
chapter	O
also	O
in	O
this	O
chapter	O
a	O
td	B
error	I
is	O
generally	O
one	O
that	O
does	O
not	O
depend	O
on	O
actions	O
as	O
opposed	O
to	O
td	B
errors	O
used	O
in	O
learning	O
action-values	O
by	O
algorithms	O
like	O
sarsa	B
and	O
q-learning	B
this	O
is	O
because	O
the	O
most	O
well-known	O
links	O
to	O
neuroscience	B
are	O
stated	O
in	O
terms	O
of	O
action-free	O
td	B
errors	O
but	O
we	O
do	O
not	O
mean	O
to	O
rule	O
out	O
possible	O
similar	O
links	O
involving	O
action-dependent	O
td	B
errors	O
errors	O
for	O
predicting	O
signals	O
other	O
than	O
rewards	O
are	O
useful	O
too	O
but	O
that	O
case	O
will	O
not	O
concern	O
us	O
here	O
see	O
for	O
example	O
modayil	O
white	O
and	O
sutton	O
one	O
can	O
ask	O
many	O
questions	O
about	O
links	O
between	O
neuroscience	B
data	O
and	O
these	O
theoretically	O
defined	O
signals	O
is	O
an	O
observed	O
signal	O
more	O
like	O
a	O
reward	B
signal	I
a	O
value	B
signal	O
a	O
prediction	B
error	O
a	O
reinforcement	B
signal	I
or	O
something	O
altogether	O
different	O
and	O
if	O
it	O
is	O
an	O
error	O
signal	O
is	O
it	O
an	O
rpe	O
a	O
td	B
error	I
or	O
a	O
simpler	O
error	O
like	O
the	O
rescorla	O
wagner	O
error	O
and	O
if	O
it	O
is	O
a	O
td	B
error	I
does	O
it	O
depend	O
on	O
actions	O
like	O
the	O
td	B
error	I
of	O
qlearning	O
or	O
sarsa	B
as	O
indicated	O
above	O
probing	O
the	O
brain	O
to	O
answer	O
questions	O
like	O
these	O
is	O
extremely	O
difficult	O
but	O
experimental	O
evidence	O
suggests	O
that	O
one	O
neurotransmitter	O
specifically	O
the	O
neurotransmitter	O
dopamine	B
signals	O
rpes	O
and	O
further	O
that	O
the	O
phasic	O
activity	O
of	O
dopamine-producing	O
neurons	O
in	O
fact	O
conveys	O
td	B
errors	O
section	O
for	O
a	O
definition	O
of	O
phasic	O
activity	O
this	O
evidence	O
led	O
to	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
which	O
we	O
describe	O
next	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
the	O
reward	B
prediction	B
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
proposes	O
that	O
one	O
of	O
the	O
functions	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine-producing	O
neurons	O
in	O
mammals	O
is	O
to	O
deliver	O
an	O
error	O
between	O
an	O
old	O
and	O
a	O
new	O
estimate	O
of	O
expected	B
future	O
reward	O
to	O
target	O
areas	O
throughout	O
the	O
brain	O
this	O
hypothesis	O
not	O
in	O
these	O
exact	O
words	O
was	O
first	O
explicitly	O
stated	O
by	O
montague	O
dayan	O
and	O
sejnowski	O
who	O
showed	O
how	O
the	O
td	B
error	I
concept	O
from	O
reinforcement	B
learning	I
accounts	O
for	O
many	O
features	O
of	O
the	O
phasic	O
is	O
not	O
available	O
until	O
time	O
t	O
the	O
td	B
error	I
available	O
at	O
t	O
is	O
actually	O
t	O
rt	O
v	O
v	O
because	O
we	O
are	O
thinking	O
of	O
time	O
steps	O
as	O
very	O
small	O
or	O
even	O
infinitesimal	O
time	O
intervals	O
one	O
should	O
not	O
attribute	O
undue	O
importance	O
to	O
this	O
one-step	O
time	O
shift	O
chapter	O
neuroscience	B
activity	O
of	O
dopamine	B
neurons	O
in	O
mammals	O
the	O
experiments	O
that	O
led	O
to	O
this	O
hypothesis	O
were	O
performed	O
in	O
the	O
and	O
early	O
in	O
the	O
laboratory	O
of	O
neuroscientist	O
wolfram	O
schultz	B
section	O
describes	O
these	O
influential	O
experiments	O
section	O
explains	O
how	O
the	O
results	O
of	O
these	O
experiments	O
align	O
with	O
td	B
errors	O
and	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
includes	O
a	O
guide	O
to	O
the	O
literature	O
surrounding	O
the	O
development	O
of	O
this	O
influential	O
hypothesis	O
montague	O
et	O
al	O
compared	O
the	O
td	B
errors	O
of	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
with	O
the	O
phasic	O
activity	O
of	O
dopamine-producing	O
neurons	O
during	O
classical	B
conditioning	I
experiments	O
recall	O
from	O
section	O
that	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
is	O
basically	O
the	O
semi-gradient-descent	O
td	B
algorithm	O
with	O
linear	O
function	B
approximation	I
montague	O
et	O
al	O
made	O
several	O
assumptions	O
to	O
set	O
up	O
this	O
comparison	O
first	O
because	O
a	O
td	B
error	I
can	O
be	O
negative	O
but	O
neurons	O
cannot	O
have	O
a	O
negative	O
firing	O
rate	O
they	O
assumed	O
that	O
the	O
quantity	O
corresponding	O
to	O
dopamine	B
neuron	O
activity	O
is	O
t	O
bt	O
where	O
bt	O
is	O
the	O
background	O
firing	O
rate	O
of	O
the	O
neuron	O
a	O
negative	O
td	B
error	I
corresponds	O
to	O
a	O
drop	O
in	O
a	O
dopamine	B
neuron	O
s	O
firing	O
rate	O
below	O
its	O
background	O
a	O
second	O
assumption	O
was	O
needed	O
about	O
the	O
states	O
visited	O
in	O
each	O
classical	B
conditioning	I
trial	O
and	O
how	O
they	O
are	O
represented	O
as	O
inputs	O
to	O
the	O
learning	O
algorithm	O
this	O
is	O
the	O
same	O
issue	O
we	O
discussed	O
in	O
section	O
for	O
the	O
td	B
model	I
montague	O
et	O
al	O
chose	O
a	O
complete	O
serial	O
compound	B
representation	O
as	O
shown	O
in	O
the	O
left	O
column	O
of	O
figure	O
but	O
where	O
the	O
sequence	O
of	O
short-duration	O
internal	O
signals	O
continues	O
until	O
the	O
onset	O
of	O
the	O
us	O
which	O
here	O
is	O
the	O
arrival	O
of	O
a	O
non-zero	O
reward	B
signal	I
this	O
representation	O
allows	O
the	O
td	B
error	I
to	O
mimic	O
the	O
fact	O
that	O
dopamine	B
neuron	O
activity	O
not	O
only	O
predicts	O
a	O
future	O
reward	O
but	O
that	O
it	O
is	O
also	O
sensitive	O
to	O
when	O
after	O
a	O
predictive	O
cue	O
that	O
reward	O
is	O
expected	B
to	O
arrive	O
there	O
has	O
to	O
be	O
some	O
way	O
to	O
keep	O
track	O
of	O
the	O
time	O
between	O
sensory	O
cues	O
and	O
the	O
arrival	O
of	O
reward	O
if	O
a	O
stimulus	O
initiates	O
a	O
sequence	O
of	O
internal	O
signals	O
that	O
continues	O
after	O
the	O
stimulus	O
ends	O
and	O
if	O
there	O
is	O
a	O
different	O
signal	O
for	O
each	O
time	O
step	O
following	O
the	O
stimulus	O
then	O
each	O
time	O
step	O
after	O
the	O
stimulus	O
is	O
represented	O
by	O
a	O
distinct	O
state	B
thus	O
the	O
td	B
error	I
being	O
state-dependent	O
can	O
be	O
sensitive	O
to	O
the	O
timing	O
of	O
events	O
within	O
a	O
trial	O
in	O
simulated	O
trials	O
with	O
these	O
assumptions	O
about	O
background	O
firing	O
rate	O
and	O
input	O
representation	O
td	B
errors	O
of	O
the	O
td	B
model	I
are	O
remarkably	O
similar	O
to	O
dopamine	B
neuron	O
phasic	O
activity	O
previewing	O
our	O
description	O
of	O
details	O
about	O
these	O
similarities	O
in	O
section	O
below	O
the	O
td	B
errors	O
parallel	O
the	O
following	O
features	O
of	O
dopamine	B
neuron	O
activity	O
the	O
phasic	O
response	O
of	O
a	O
dopamine	B
neuron	O
only	O
occurs	O
when	O
a	O
rewarding	O
event	O
is	O
unpredicted	O
early	O
in	O
learning	O
neutral	O
cues	O
that	O
precede	O
a	O
reward	O
do	O
not	O
cause	O
substantial	O
phasic	O
dopamine	B
responses	O
but	O
with	O
continued	O
learning	O
these	O
cues	O
gain	O
predictive	O
value	B
and	O
come	O
to	O
elicit	O
phasic	O
dopamine	B
responses	O
if	O
an	O
even	O
earlier	O
cue	O
reliably	O
precedes	O
a	O
cue	O
that	O
has	O
already	O
acquired	O
predictive	O
value	B
the	O
phasic	O
dopamine	B
response	O
shifts	O
to	O
the	O
earlier	O
cue	O
ceasing	O
for	O
the	O
later	O
cue	O
and	O
if	O
after	O
learning	O
the	O
predicted	O
rewarding	O
event	O
is	O
omitted	O
a	O
dopamine	B
neuron	O
s	O
response	O
decreases	O
below	O
its	O
baseline	B
level	O
shortly	O
after	O
the	O
expected	B
time	O
of	O
the	O
rewarding	O
event	O
although	O
not	O
every	O
dopamine	B
neuron	O
monitored	O
in	O
the	O
experiments	O
of	O
schultz	B
and	O
col	O
the	O
literature	O
relating	O
td	B
errors	O
to	O
the	O
activity	O
of	O
dopamine	B
neurons	O
their	O
t	O
is	O
the	O
same	O
as	O
our	O
t	O
rt	O
v	O
v	O
dopamine	B
leagues	O
behaved	O
in	O
all	O
of	O
these	O
ways	O
the	O
striking	O
correspondence	O
between	O
the	O
activities	O
of	O
most	O
of	O
the	O
monitored	O
neurons	O
and	O
td	B
errors	O
lends	O
strong	O
support	O
to	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
there	O
are	O
situations	O
however	O
in	O
which	O
predictions	O
based	O
on	O
the	O
hypothesis	O
do	O
not	O
match	O
what	O
is	O
observed	O
in	O
experiments	O
the	O
choice	O
of	O
input	O
representation	O
is	O
critical	O
to	O
how	O
closely	O
td	B
errors	O
match	O
some	O
of	O
the	O
details	O
of	O
dopamine	B
neuron	O
activity	O
particularly	O
details	O
about	O
the	O
timing	O
of	O
dopamine	B
neuron	O
responses	O
different	O
ideas	O
some	O
of	O
which	O
we	O
discuss	O
below	O
have	O
been	O
proposed	O
about	O
input	O
representations	O
and	O
other	O
features	O
of	O
td	B
learning	O
to	O
make	O
the	O
td	B
errors	O
fit	O
the	O
data	O
better	O
though	O
the	O
main	O
parallels	O
appear	O
with	O
the	O
csc	O
representation	O
that	O
montague	O
et	O
al	O
used	O
overall	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
has	O
received	O
wide	O
acceptance	O
among	O
neuroscientists	O
studying	O
reward-based	O
learning	O
and	O
it	O
has	O
proven	O
to	O
be	O
remarkably	O
resilient	O
in	O
the	O
face	O
of	O
accumulating	B
results	O
from	O
neuroscience	B
experiments	O
to	O
prepare	O
for	O
our	O
description	O
of	O
the	O
neuroscience	B
experiments	O
supporting	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
and	O
to	O
provide	O
some	O
context	O
so	O
that	O
the	O
significance	O
of	O
the	O
hypothesis	O
can	O
be	O
appreciated	O
we	O
next	O
present	O
some	O
of	O
what	O
is	O
known	O
about	O
dopamine	B
the	O
brain	O
structures	O
it	O
influences	O
and	O
how	O
it	O
is	O
involved	O
in	O
reward-based	O
learning	O
dopamine	B
dopamine	B
is	O
produced	O
as	O
a	O
neurotransmitter	O
by	O
neurons	O
whose	O
cell	O
bodies	O
lie	O
mainly	O
in	O
two	O
clusters	O
of	O
neurons	O
in	O
the	O
midbrain	O
of	O
mammals	O
the	O
substantia	O
nigra	O
pars	O
compacta	O
and	O
the	O
ventral	O
tegmental	O
area	O
dopamine	B
plays	O
essential	O
roles	O
in	O
many	O
processes	O
in	O
the	O
mammalian	O
brain	O
prominent	O
among	O
these	O
are	O
motivation	B
learning	O
action-selection	O
most	O
forms	O
of	O
addiction	B
and	O
the	O
disorders	O
schizophrenia	O
and	O
parkinson	O
s	O
disease	O
dopamine	B
is	O
called	O
a	O
neuromodulator	O
because	O
it	O
performs	O
many	O
functions	O
other	O
than	O
direct	O
fast	O
excitation	O
or	O
inhibition	O
of	O
targeted	O
neurons	O
although	O
much	O
remains	O
unknown	O
about	O
dopamine	B
s	O
functions	O
and	O
details	O
of	O
its	O
cellular	O
effects	O
it	O
is	O
clear	O
that	O
it	O
is	O
fundamental	O
to	O
reward	O
processing	O
in	O
the	O
mammalian	O
brain	O
dopamine	B
is	O
not	O
the	O
only	O
neuromodulator	O
involved	O
in	O
reward	O
processing	O
and	O
its	O
role	O
in	O
aversive	O
situations	O
punishment	O
remains	O
controversial	O
dopamine	B
also	O
can	O
function	O
differently	O
in	O
non-mammals	O
but	O
no	O
one	O
doubts	O
that	O
dopamine	B
is	O
essential	O
for	O
reward-related	O
processes	O
in	O
mammals	O
including	O
humans	O
an	O
early	O
traditional	O
view	O
is	O
that	O
dopamine	B
neurons	O
broadcast	O
a	O
reward	B
signal	I
to	O
multiple	O
brain	O
regions	O
implicated	O
in	O
learning	O
and	O
motivation	B
this	O
view	O
followed	O
from	O
a	O
famous	O
paper	O
by	O
james	O
olds	O
and	O
peter	O
milner	O
that	O
described	O
the	O
effects	O
of	O
electrical	O
stimulation	O
on	O
certain	O
areas	O
of	O
a	O
rat	O
s	O
brain	O
they	O
found	O
that	O
electrical	O
stimulation	O
to	O
particular	O
regions	O
acted	O
as	O
a	O
very	O
powerful	O
reward	O
in	O
controlling	O
the	O
rat	O
s	O
behavior	O
control	B
exercised	O
over	O
the	O
animal	O
s	O
behavior	O
by	O
means	O
of	O
this	O
reward	O
is	O
extreme	O
possibly	O
exceeding	O
that	O
exercised	O
by	O
any	O
other	O
reward	O
previously	O
used	O
in	O
animal	O
experimentation	O
and	O
milner	O
later	O
research	O
revealed	O
that	O
the	O
sites	O
at	O
which	O
stimulation	O
was	O
most	O
effective	O
in	O
producing	O
this	O
rewarding	O
effect	O
excited	O
dopamine	B
pathways	O
either	O
directly	O
or	O
indirectly	O
that	O
ordinarily	O
are	O
excited	O
by	O
natural	O
rewarding	O
stimuli	O
effects	O
similar	O
to	O
these	O
with	O
rats	O
were	O
also	O
observed	O
with	O
human	O
subjects	O
these	O
observations	O
strongly	O
suggested	O
that	O
dopamine	B
neuron	O
activity	O
signals	O
reward	O
chapter	O
neuroscience	B
but	O
if	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
is	O
correct	O
even	O
if	O
it	O
accounts	O
for	O
only	O
some	O
features	O
of	O
a	O
dopamine	B
neuron	O
s	O
activity	O
this	O
traditional	O
view	O
of	O
dopamine	B
neuron	O
activity	O
is	O
not	O
entirely	O
correct	O
phasic	O
responses	O
of	O
dopamine	B
neurons	O
signal	O
reward	O
prediction	B
errors	O
not	O
reward	O
itself	O
in	O
reinforcement	B
learning	I
s	O
terms	O
a	O
dopamine	B
neuron	O
s	O
phasic	O
response	O
at	O
a	O
time	O
t	O
corresponds	O
to	O
t	O
rt	O
v	O
v	O
not	O
to	O
rt	O
reinforcement	B
learning	I
theory	O
and	O
algorithms	O
help	O
reconcile	O
the	O
reward-predictionerror	O
view	O
with	O
the	O
conventional	O
notion	O
that	O
dopamine	B
signals	O
reward	O
in	O
many	O
of	O
the	O
algorithms	O
we	O
discuss	O
in	O
this	O
book	O
functions	O
as	O
a	O
reinforcement	B
signal	I
meaning	O
that	O
it	O
is	O
the	O
main	O
driver	O
of	O
learning	O
for	O
example	O
is	O
the	O
critical	O
factor	O
in	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
and	O
is	O
the	O
reinforcement	B
signal	I
for	O
learning	O
both	O
a	O
value	B
function	I
and	O
a	O
policy	B
in	O
an	O
actor	O
critic	O
architecture	O
and	O
action-dependent	O
forms	O
of	O
are	O
reinforcement	O
signals	O
for	B
q-learning	B
and	O
sarsa	B
the	O
reward	B
signal	I
rt	O
is	O
a	O
crucial	O
component	O
of	O
t	O
but	O
it	O
is	O
not	O
the	O
complete	O
determinant	O
of	O
its	O
reinforcing	O
effect	O
in	O
these	O
algorithms	O
the	O
additional	O
term	O
v	O
v	O
is	O
the	O
higher-order	O
reinforcement	O
part	O
of	O
t	O
and	O
even	O
if	O
reward	O
occurs	O
the	O
td	B
error	I
can	O
be	O
silent	O
if	O
the	O
reward	O
is	O
fully	O
predicted	O
is	O
fully	O
explained	O
in	O
section	O
below	O
a	O
closer	O
look	O
at	O
olds	O
and	O
milner	O
s	O
paper	O
in	O
fact	O
reveals	O
that	O
it	O
is	O
mainly	O
about	O
the	O
reinforcing	O
effect	O
of	O
electrical	O
stimulation	O
in	O
an	O
instrumental	O
conditioning	O
task	O
electrical	O
stimulation	O
not	O
only	O
energized	O
the	O
rats	O
behavior	O
through	O
dopamine	B
s	O
effect	O
on	O
motivation	B
it	O
also	O
led	O
to	O
the	O
rats	O
quickly	O
learning	O
to	O
stimulate	O
themselves	O
by	O
pressing	O
a	O
lever	O
which	O
they	O
would	O
do	O
frequently	O
for	O
long	O
periods	O
of	O
time	O
the	O
activity	O
of	O
dopamine	B
neurons	O
triggered	O
by	O
electrical	O
stimulation	O
reinforced	O
the	O
rats	O
lever	O
pressing	O
more	O
recent	O
experiments	O
using	O
optogenetic	O
methods	O
clinch	O
the	O
role	O
of	O
phasic	O
responses	O
of	O
dopamine	B
neurons	O
as	O
reinforcement	O
signals	O
these	O
methods	O
allow	O
neuroscientists	O
to	O
precisely	O
control	B
the	O
activity	O
of	O
selected	O
neuron	O
types	O
at	O
a	O
millisecond	O
timescale	O
in	O
awake	O
behaving	O
animals	O
optogenetic	O
methods	O
introduce	O
light-sensitive	O
proteins	O
into	O
selected	O
neuron	O
types	O
so	O
that	O
these	O
neurons	O
can	O
be	O
activated	O
or	O
silenced	O
by	O
means	O
of	O
flashes	O
of	O
laser	O
light	O
the	O
first	O
experiment	O
using	O
optogenetic	O
methods	O
to	O
study	O
dopamine	B
neurons	O
showed	O
that	O
optogenetic	O
stimulation	O
producing	O
phasic	O
activation	O
of	O
dopamine	B
neurons	O
in	O
mice	O
was	O
enough	O
to	O
condition	O
the	O
mice	O
to	O
prefer	O
the	O
side	O
of	O
a	O
chamber	O
where	O
they	O
received	O
this	O
stimulation	O
as	O
compared	O
to	O
the	O
chamber	O
s	O
other	O
side	O
where	O
they	O
received	O
no	O
or	O
lower-frequency	O
stimulation	O
et	O
al	O
in	O
another	O
example	O
steinberg	O
et	O
al	O
used	O
optogenetic	O
activation	O
of	O
dopamine	B
neurons	O
to	O
create	O
artificial	O
bursts	O
of	O
dopamine	B
neuron	O
activity	O
in	O
rats	O
at	O
the	O
times	O
when	O
rewarding	O
stimuli	O
were	O
expected	B
but	O
omitted	O
times	O
when	O
dopamine	B
neuron	O
activity	O
normally	O
pauses	O
with	O
these	O
pauses	O
replaced	O
by	O
artificial	O
bursts	O
responding	O
was	O
sustained	O
when	O
it	O
would	O
ordinarily	O
decrease	O
due	O
to	O
lack	O
of	O
reinforcement	O
extinction	O
trials	O
and	O
learning	O
was	O
enabled	O
when	O
it	O
would	O
ordinarily	O
be	O
blocked	O
due	O
to	O
the	O
reward	O
being	O
already	O
predicted	O
blocking	B
paradigm	O
section	O
additional	O
evidence	O
for	O
the	O
reinforcing	O
function	O
of	O
dopamine	B
comes	O
from	O
optogenetic	O
experiments	O
with	O
fruit	O
flies	O
except	O
in	O
these	O
animals	O
dopamine	B
s	O
effect	O
is	O
the	O
opposite	O
of	O
its	O
effect	O
in	O
mammals	O
optically	O
triggered	O
bursts	O
of	O
dopamine	B
neuron	O
activity	O
act	O
just	O
like	O
electric	O
foot	O
shock	O
in	O
reinforcing	O
avoidance	O
behavior	O
at	O
least	O
for	O
the	O
population	O
of	O
dopamine	B
neurons	O
activated	O
et	O
al	O
although	O
none	O
of	O
these	O
dopamine	B
optogenetic	O
experiments	O
showed	O
that	O
phasic	O
dopamine	B
neuron	O
activity	O
is	O
specifically	O
like	O
a	O
td	B
error	I
they	O
convincingly	O
demonstrated	O
that	O
phasic	O
dopamine	B
neuron	O
activity	O
acts	O
just	O
like	O
acts	O
perhaps	O
like	O
minus	O
acts	O
in	O
fruit	O
flies	O
as	O
the	O
reinforcement	B
signal	I
in	O
algorithms	O
for	O
both	O
prediction	B
conditioning	O
and	B
control	B
conditioning	O
dopamine	B
neurons	O
are	O
particularly	O
well	O
suited	O
to	O
broadcasting	O
a	O
reinforcement	B
signal	I
to	O
many	O
areas	O
of	O
the	O
brain	O
these	O
neurons	O
have	O
huge	O
axonal	O
arbors	O
each	O
releasing	O
dopamine	B
at	O
to	O
times	O
more	O
synaptic	O
sites	O
than	O
reached	O
by	O
the	O
axons	O
of	O
typical	O
neurons	O
shown	O
to	O
the	O
right	O
is	O
the	O
axonal	O
arbor	O
of	O
a	O
single	O
dopamine	B
neuron	O
whose	O
cell	O
body	O
is	O
in	O
the	O
snpc	O
of	O
a	O
rat	O
s	O
brain	O
each	O
axon	O
of	O
a	O
snpc	O
or	O
vta	O
dopamine	B
neuron	O
makes	O
roughly	O
synaptic	O
contacts	O
on	O
the	O
dendrites	O
of	O
neurons	O
in	O
targeted	O
brain	O
areas	O
axonal	O
arbor	O
of	O
a	O
single	O
neuron	O
producing	O
dopamine	B
as	O
a	O
neurotransmitter	O
these	O
axons	O
make	O
synaptic	O
contacts	O
with	O
a	O
huge	O
number	O
of	O
dendrites	O
of	O
neurons	O
in	O
targeted	O
brain	O
areas	O
adapted	O
from	O
the	O
journal	O
of	O
neuroscience	B
matsuda	O
furuta	O
naka	O
if	O
dopamine	B
neurons	O
broadcast	O
a	O
reinforcement	B
signal	I
like	O
reinforcement	B
learning	I
s	O
then	O
because	O
this	O
is	O
a	O
scalar	O
signal	O
i	O
e	O
a	O
single	O
number	O
all	O
dopamine	B
neurons	O
in	O
both	O
the	O
snpc	O
and	O
vta	O
would	O
be	O
expected	B
to	O
activate	O
more-orless	O
identically	O
so	O
that	O
they	O
would	O
act	O
in	O
near	O
synchrony	O
to	O
send	O
the	O
same	O
signal	O
to	O
all	O
of	O
the	O
sites	O
their	O
axons	O
target	O
although	O
it	O
has	O
been	O
a	O
common	O
belief	O
that	O
dopamine	B
neurons	O
do	O
act	O
together	O
like	O
this	O
modern	O
evidence	O
is	O
pointing	O
to	O
the	O
more	O
complicated	O
picture	O
that	O
different	O
subpopulations	O
of	O
dopamine	B
neurons	O
respond	O
to	O
input	O
differently	O
depending	O
on	O
the	O
structures	O
to	O
which	O
they	O
send	O
their	O
signals	O
and	O
the	O
different	O
ways	O
these	O
signals	O
act	O
on	O
their	O
target	O
structures	O
dopamine	B
has	O
functions	O
other	O
than	O
signaling	O
rpes	O
and	O
even	O
for	O
dopamine	B
neurons	O
that	O
do	O
signal	O
rpes	O
it	O
can	O
make	O
sense	O
to	O
send	O
different	O
rpes	O
to	O
different	O
structures	O
depending	O
on	O
the	O
roles	O
these	O
structures	O
play	O
in	O
producing	O
reinforced	O
behavior	O
this	O
is	O
beyond	O
what	O
we	O
treat	O
in	O
any	O
detail	O
in	O
this	O
book	O
but	O
vector-valued	O
rpe	O
signals	O
make	O
sense	O
from	O
the	O
perspective	O
of	O
reinforcement	B
learning	I
when	O
decisions	O
can	O
be	O
decomposed	O
into	O
separate	O
sub-decisions	O
or	O
more	O
generally	O
as	O
a	O
way	O
to	O
address	O
the	O
structural	B
version	O
of	O
the	O
credit	B
assignment	I
problem	O
how	O
do	O
you	O
distribute	O
credit	O
for	O
success	O
blame	O
for	O
failure	O
of	O
a	O
decision	O
among	O
the	O
many	O
component	O
structures	O
that	O
could	O
have	O
been	O
involved	O
in	O
producing	O
it	O
we	O
say	O
a	O
bit	O
more	O
about	O
this	O
in	O
section	O
below	O
mura	O
hioki	O
fujiyama	O
arai	O
and	O
kaneko	O
vol	O
ume	O
page	O
the	O
axons	O
of	O
most	O
dopamine	B
neurons	O
make	O
synaptic	O
contact	O
with	O
neurons	O
in	O
the	O
frontal	O
cortex	O
and	O
the	O
basal	B
ganglia	I
areas	O
of	O
the	O
brain	O
involved	O
in	O
voluntary	O
movement	O
decision	O
making	O
learning	O
and	O
cognitive	O
functions	O
such	O
as	O
planning	B
because	O
most	O
ideas	O
relating	O
chapter	O
neuroscience	B
dopamine	B
to	O
reinforcement	B
learning	I
focus	O
on	O
the	O
basal	B
ganglia	I
and	O
the	O
connections	O
from	O
dopamine	B
neurons	O
are	O
particularly	O
dense	O
there	O
we	O
focus	O
on	O
the	O
basal	B
ganglia	I
here	O
the	O
basal	B
ganglia	I
are	O
a	O
collection	O
of	O
neuron	O
groups	O
or	O
nuclei	O
lying	O
at	O
the	O
base	O
of	O
the	O
forebrain	O
the	O
main	O
input	O
structure	O
of	O
the	O
basal	B
ganglia	I
is	O
called	O
the	O
striatum	O
essentially	O
all	O
of	O
the	O
cerebral	O
cortex	O
among	O
other	O
structures	O
provides	O
input	O
to	O
the	O
striatum	O
the	O
activity	O
of	O
cortical	O
neurons	O
conveys	O
a	O
wealth	O
of	O
information	O
about	O
sensory	O
input	O
internal	O
states	O
and	O
motor	O
activity	O
the	O
axons	O
of	O
cortical	O
neurons	O
make	O
synaptic	O
contacts	O
on	O
the	O
dendrites	O
of	O
the	O
main	O
inputoutput	O
neurons	O
of	O
the	O
striatum	O
called	O
medium	O
spiny	O
neurons	O
output	O
from	O
the	O
striatum	O
loops	O
back	O
via	O
other	O
basal	B
ganglia	I
nuclei	O
and	O
the	O
thalamus	O
to	O
frontal	O
areas	O
of	O
cortex	O
and	O
to	O
motor	O
areas	O
making	O
it	O
possible	O
for	O
the	O
striatum	O
to	O
influence	O
movement	O
abstract	O
decision	O
processes	O
and	O
reward	O
processing	O
two	O
main	O
subdivisions	O
of	O
the	O
striatum	O
are	O
important	O
for	O
reinforcement	B
learning	I
the	O
dorsal	O
striatum	O
primarily	O
implicated	O
in	O
influencing	O
action	B
selection	O
and	O
the	O
ventral	O
striatum	O
thought	O
to	O
be	O
critical	O
for	O
different	O
aspects	O
of	O
reward	O
processing	O
including	O
the	O
assignment	O
of	O
affective	O
value	B
to	O
sensations	O
the	O
dendrites	O
of	O
medium	O
spiny	O
neurons	O
are	O
covered	O
with	O
spines	O
on	O
whose	O
tips	O
the	O
axons	O
of	O
neurons	O
in	O
the	O
cortex	O
make	O
synaptic	O
contact	O
also	O
making	O
synaptic	O
contact	O
with	O
these	O
spines	O
in	O
this	O
case	O
contacting	O
the	O
spine	O
stems	O
are	O
axons	O
of	O
dopamine	B
neurons	O
this	O
arrangement	O
brings	O
together	O
presynaptic	O
activity	O
of	O
cortical	O
neurons	O
figure	O
spine	O
of	O
a	O
striatal	O
neuron	O
showing	O
input	O
from	O
both	O
cortical	O
and	O
dopamine	B
neurons	O
axons	O
of	O
cortical	O
neurons	O
influence	O
striatal	O
neurons	O
via	O
corticostriatal	O
synapses	O
releasing	O
the	O
neurotransmitter	O
glutamate	O
at	O
the	O
tips	O
of	O
spines	O
covering	O
the	O
dendrites	O
of	O
striatal	O
neurons	O
an	O
axon	O
of	O
a	O
vta	O
or	O
snpc	O
dopamine	B
neuron	O
is	O
shown	O
passing	O
by	O
the	O
spine	O
the	O
lower	O
right	O
dopamine	B
varicosities	O
on	O
this	O
axon	O
release	O
dopamine	B
at	O
or	O
near	O
the	O
spine	O
stem	O
in	O
an	O
arrangement	O
that	O
brings	O
together	O
presynaptic	O
input	O
from	O
cortex	O
postsynaptic	O
activity	O
of	O
the	O
striatal	O
neuron	O
and	O
dopamine	B
making	O
it	O
possible	O
that	O
several	O
types	O
of	O
learning	O
rules	O
govern	O
the	O
plasticity	O
of	O
corticostriatal	O
synapses	O
each	O
axon	O
of	O
a	O
dopamine	B
neuron	O
makes	O
synaptic	O
contact	O
with	O
the	O
stems	O
of	O
roughly	O
spines	O
some	O
of	O
the	O
complexity	O
omitted	O
from	O
our	O
discussion	O
is	O
shown	O
here	O
by	O
other	O
neurotransmitter	O
pathways	O
and	O
multiple	O
receptor	O
types	O
such	O
as	O
an	O
dopamine	B
receptors	O
by	O
which	O
dopamine	B
can	O
produce	O
different	O
effects	O
at	O
spines	O
and	O
other	O
postsynaptic	O
sites	O
from	O
journal	O
of	O
neurophysiology	O
w	O
schultz	B
vol	O
page	O
experimental	O
support	O
for	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
postsynaptic	O
activity	O
of	O
medium	O
spiny	O
neurons	O
and	O
input	O
from	O
dopamine	B
neurons	O
what	O
actually	O
occurs	O
at	O
these	O
spines	O
is	O
complex	O
and	O
not	O
completely	O
understood	O
figure	O
hints	O
at	O
the	O
complexity	O
by	O
showing	O
two	O
types	O
of	O
receptors	O
for	O
dopamine	B
receptors	O
for	O
glutamate	O
the	O
neurotransmitter	O
of	O
the	O
cortical	O
inputs	O
and	O
multiple	O
ways	O
that	O
the	O
various	O
signals	O
can	O
interact	O
but	O
evidence	O
is	O
mounting	O
that	O
changes	O
in	O
the	O
efficacies	O
of	O
the	O
synapses	O
on	O
the	O
pathway	O
from	O
the	O
cortex	O
to	O
the	O
striatum	O
which	O
neuroscientists	O
call	O
corticostriatal	O
synapses	O
depend	O
critically	O
on	O
appropriately-timed	O
dopamine	B
signals	O
experimental	O
support	O
for	O
the	O
reward	O
predic	O
tion	O
error	O
hypothesis	O
dopamine	B
neurons	O
respond	O
with	O
bursts	O
of	O
activity	O
to	O
intense	O
novel	O
or	O
unexpected	O
visual	O
and	O
auditory	O
stimuli	O
that	O
trigger	O
eye	O
and	O
body	O
movements	O
but	O
very	O
little	O
of	O
their	O
activity	O
is	O
related	O
to	O
the	O
movements	O
themselves	O
this	O
is	O
surprising	O
because	O
degeneration	O
of	O
dopamine	B
neurons	O
is	O
a	O
cause	O
of	O
parkinson	O
s	O
disease	O
whose	O
symptoms	O
include	O
motor	O
disorders	O
particularly	O
deficits	O
in	O
self-initiated	O
movement	O
motivated	O
by	O
the	O
weak	O
relationship	O
between	O
dopamine	B
neuron	O
activity	O
and	O
stimulus-triggered	O
eye	O
and	O
body	O
movements	O
romo	O
and	O
schultz	B
and	O
schultz	B
and	O
romo	O
took	O
the	O
first	O
steps	O
toward	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
by	O
recording	O
the	O
activity	O
of	O
dopamine	B
neurons	O
and	O
muscle	O
activity	O
while	O
monkeys	O
moved	O
their	O
arms	O
they	O
trained	O
two	O
monkeys	O
to	O
reach	O
from	O
a	O
resting	O
hand	O
position	O
into	O
a	O
bin	O
containing	O
a	O
bit	O
of	O
apple	O
a	O
piece	O
of	O
cookie	O
or	O
a	O
raisin	O
when	O
the	O
monkey	O
saw	O
and	O
heard	O
the	O
bin	O
s	O
door	O
open	O
the	O
monkey	O
could	O
then	O
grab	O
and	O
bring	O
the	O
food	O
to	O
its	O
mouth	O
after	O
a	O
monkey	O
became	O
good	O
at	O
this	O
it	O
was	O
trained	O
on	O
two	O
additional	O
tasks	O
the	O
purpose	O
of	O
the	O
first	O
task	O
was	O
to	O
see	O
what	O
dopamine	B
neurons	O
do	O
when	O
movements	O
are	O
self-initiated	O
the	O
bin	O
was	O
left	O
open	O
but	O
covered	O
from	O
above	O
so	O
that	O
the	O
monkey	O
could	O
not	O
see	O
inside	O
but	O
could	O
reach	O
in	O
from	O
below	O
no	O
triggering	O
stimuli	O
were	O
presented	O
and	O
after	O
the	O
monkey	O
reached	O
for	O
and	O
ate	O
the	O
food	O
morsel	O
the	O
experimenter	O
usually	O
not	O
always	O
silently	O
and	O
unseen	O
by	O
the	O
monkey	O
replaced	O
food	O
in	O
the	O
bin	O
by	O
sticking	O
it	O
onto	O
a	O
rigid	O
wire	O
here	O
too	O
the	O
activity	O
of	O
the	O
dopamine	B
neurons	O
romo	O
and	O
schultz	B
monitored	O
was	O
not	O
related	O
to	O
the	O
monkey	O
s	O
movements	O
but	O
a	O
large	O
percentage	O
of	O
these	O
neurons	O
produced	O
phasic	O
responses	O
whenever	O
the	O
monkey	O
first	O
touched	O
a	O
food	O
morsel	O
these	O
neurons	O
did	O
not	O
respond	O
when	O
the	O
monkey	O
touched	O
just	O
the	O
wire	O
or	O
explored	O
the	O
bin	O
when	O
no	O
food	O
was	O
there	O
this	O
was	O
good	O
evidence	O
that	O
the	O
neurons	O
were	O
responding	O
to	O
the	O
food	O
and	O
not	O
to	O
other	O
aspects	O
of	O
the	O
task	O
the	O
purpose	O
of	O
romo	O
and	O
schultz	B
s	O
second	O
task	O
was	O
to	O
see	O
what	O
happens	O
when	O
movements	O
are	O
triggered	O
by	O
stimuli	O
this	O
task	O
used	O
a	O
different	O
bin	O
with	O
a	O
movable	O
cover	O
the	O
sight	O
and	O
sound	O
of	O
the	O
bin	O
opening	O
triggered	O
reaching	O
movements	O
to	O
the	O
bin	O
in	O
this	O
case	O
romo	O
and	O
schultz	B
found	O
that	O
after	O
some	O
period	O
of	O
training	O
the	O
dopamine	B
neurons	O
no	O
longer	O
responded	O
to	O
the	O
touch	O
of	O
the	O
food	O
but	O
instead	O
responded	O
to	O
the	O
sight	O
and	O
sound	O
of	O
the	O
opening	O
cover	O
of	O
the	O
food	O
bin	O
the	O
phasic	O
responses	O
of	O
these	O
neurons	O
had	O
shifted	O
from	O
the	O
reward	O
itself	O
to	O
stimuli	O
predicting	O
the	O
availability	O
of	O
the	O
reward	O
in	O
a	O
followup	O
study	O
romo	O
and	O
schultz	B
found	O
that	O
most	O
of	O
the	O
dopamine	B
neurons	O
whose	O
activity	O
they	O
chapter	O
neuroscience	B
monitored	O
did	O
not	O
respond	O
to	O
the	O
sight	O
and	O
sound	O
of	O
the	O
bin	O
opening	O
outside	O
the	O
context	O
of	O
the	O
behavioral	O
task	O
these	O
observations	O
suggested	O
that	O
the	O
dopamine	B
neurons	O
were	O
responding	O
neither	O
to	O
the	O
initiation	O
of	O
a	O
movement	O
nor	O
to	O
the	O
sensory	O
properties	O
of	O
the	O
stimuli	O
but	O
were	O
rather	O
signaling	O
an	O
expectation	O
of	O
reward	O
schultz	B
s	O
group	O
conducted	O
many	O
additional	O
studies	O
involving	O
both	O
snpc	O
and	O
vta	O
dopamine	B
neurons	O
a	O
particular	O
series	O
of	O
experiments	O
was	O
influential	O
in	O
suggesting	O
that	O
the	O
phasic	O
responses	O
of	O
dopamine	B
neurons	O
correspond	O
to	O
td	B
errors	O
and	O
not	O
to	O
simpler	O
errors	O
like	O
those	O
in	O
the	O
rescorla	O
wagner	O
model	O
in	O
the	O
first	O
of	O
these	O
experiments	O
apicella	O
and	O
schultz	B
monkeys	O
were	O
trained	O
to	O
depress	O
a	O
lever	O
after	O
a	O
light	O
was	O
illuminated	O
as	O
a	O
trigger	O
cue	O
to	O
obtain	O
a	O
drop	O
of	O
apple	O
juice	O
as	O
romo	O
and	O
schultz	B
had	O
observed	O
earlier	O
many	O
dopamine	B
neurons	O
initially	O
responded	O
to	O
the	O
reward	O
the	O
drop	O
of	O
juice	O
top	O
panel	O
but	O
many	O
of	O
these	O
neurons	O
lost	O
that	O
reward	O
response	O
as	O
training	O
continued	O
and	O
developed	O
responses	O
instead	O
to	O
the	O
illumination	O
of	O
the	O
light	O
that	O
predicted	O
the	O
reward	O
middle	O
panel	O
with	O
continued	O
training	O
lever	O
pressing	O
became	O
faster	O
while	O
the	O
number	O
of	O
dopamine	B
neurons	O
responding	O
to	O
the	O
trigger	O
cue	O
decreased	O
figure	O
the	O
response	O
of	O
dopamine	B
neurons	O
shifts	O
from	O
initial	O
responses	O
to	O
primary	O
reward	O
to	O
earlier	O
predictive	O
stimuli	O
these	O
are	O
plots	O
of	O
the	O
number	O
of	O
action	B
potentials	O
produced	O
by	O
monitored	O
dopamine	B
neurons	O
within	O
small	O
time	O
intervals	O
averaged	O
over	O
all	O
the	O
monitored	O
dopamine	B
neurons	O
from	O
to	O
neurons	O
for	O
these	O
data	O
top	O
dopamine	B
neurons	O
are	O
activated	O
by	O
the	O
unpredicted	O
delivery	O
of	O
drop	O
of	O
apple	O
juice	O
middle	O
with	O
learning	O
dopamine	B
neurons	O
developed	O
responses	O
to	O
the	O
reward-predicting	O
trigger	O
cue	O
and	O
lost	O
responsiveness	O
to	O
the	O
delivery	O
of	O
reward	O
bottom	O
with	O
the	O
addition	O
of	O
an	O
instruction	O
cue	O
preceding	O
the	O
trigger	O
cue	O
by	O
second	O
dopamine	B
neurons	O
shifted	O
their	O
responses	O
from	O
the	O
trigger	O
cue	O
to	O
the	O
earlier	O
instruction	O
cue	O
from	O
schultz	B
et	O
al	O
mit	O
press	O
following	O
this	O
study	O
the	O
same	O
monkeys	O
were	O
trained	O
on	O
a	O
new	O
task	O
apicella	O
and	O
ljungberg	O
here	O
the	O
monkeys	O
faced	O
two	O
levers	O
each	O
with	O
a	O
light	O
above	O
it	O
experimental	O
support	O
for	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
illuminating	O
one	O
of	O
these	O
lights	O
was	O
an	O
instruction	O
cue	O
indicating	O
which	O
of	O
the	O
two	O
levers	O
would	O
produce	O
a	O
drop	O
of	O
apple	O
juice	O
in	O
this	O
task	O
the	O
instruction	O
cue	O
preceded	O
the	O
trigger	O
cue	O
of	O
the	O
previous	O
task	O
by	O
a	O
fixed	O
interval	O
of	O
second	O
the	O
monkeys	O
learned	O
to	O
withhold	O
reaching	O
until	O
seeing	O
the	O
trigger	O
cue	O
and	O
dopamine	B
neuron	O
activity	O
increased	O
but	O
now	O
the	O
responses	O
of	O
the	O
monitored	O
dopamine	B
neurons	O
occurred	O
almost	O
exclusively	O
to	O
the	O
earlier	O
instruction	O
cue	O
and	O
not	O
to	O
the	O
trigger	O
cue	O
bottom	O
panel	O
here	O
again	O
the	O
number	O
of	O
dopamine	B
neurons	O
responding	O
to	O
the	O
instruction	O
cue	O
was	O
much	O
reduced	O
when	O
the	O
task	O
was	O
well	O
learned	O
during	O
learning	O
across	O
these	O
tasks	O
dopamine	B
neuron	O
activity	O
shifted	O
from	O
initially	O
responding	O
to	O
the	O
reward	O
to	O
responding	O
to	O
the	O
earlier	O
predictive	O
stimuli	O
first	O
progressing	O
to	O
the	O
trigger	O
stimulus	O
then	O
to	O
the	O
still	O
earlier	O
instruction	O
cue	O
as	O
responding	O
moved	O
earlier	O
in	O
time	O
it	O
disappeared	O
from	O
the	O
later	O
stimuli	O
this	O
shifting	O
of	O
responses	O
to	O
earlier	O
reward	O
predictors	O
while	O
losing	O
responses	O
to	O
later	O
predictors	O
is	O
a	O
hallmark	O
of	O
td	B
learning	O
for	O
example	O
figure	O
the	O
task	O
just	O
described	O
revealed	O
another	O
property	O
of	O
dopamine	B
neuron	O
activity	O
shared	O
with	O
td	B
learning	O
sometimes	O
pressed	O
the	O
wrong	O
key	O
that	O
is	O
the	O
key	O
other	O
than	O
the	O
instructed	O
one	O
and	O
consequently	O
received	O
no	O
reward	O
in	O
these	O
trials	O
many	O
of	O
the	O
dopamine	B
neurons	O
showed	O
a	O
sharp	O
decrease	O
in	O
their	O
firing	O
rates	O
below	O
baseline	B
shortly	O
after	O
the	O
reward	O
s	O
usual	O
time	O
of	O
delivery	O
and	O
this	O
happened	O
without	O
the	O
availability	O
of	O
any	O
the	O
monkeys	O
figure	O
the	O
response	O
of	O
dopamine	B
neurons	O
drops	O
below	O
baseline	B
shortly	O
after	O
the	O
time	O
when	O
an	O
expected	B
reward	O
fails	O
to	O
occur	O
top	O
dopamine	B
neurons	O
are	O
activated	O
by	O
the	O
unpredicted	O
delivery	O
of	O
a	O
drop	O
of	O
apple	O
juice	O
middle	O
dopamine	B
neurons	O
respond	O
to	O
a	O
conditioned	O
stimulus	O
that	O
predicts	O
reward	O
and	O
do	O
not	O
respond	O
to	O
the	O
reward	O
itself	O
bottom	O
when	O
the	O
reward	O
predicted	O
by	O
the	O
cs	O
fails	O
to	O
occur	O
the	O
activity	O
of	O
dopamine	B
neurons	O
drops	O
below	O
baseline	B
shortly	O
after	O
the	O
time	O
the	O
reward	O
is	O
expected	B
to	O
occur	O
at	O
the	O
top	O
of	O
each	O
of	O
these	O
panels	O
is	O
shown	O
the	O
average	O
number	O
of	O
action	B
potentials	O
produced	O
by	O
monitored	O
dopamine	B
neurons	O
within	O
small	O
time	O
intervals	O
around	O
the	O
indicated	O
times	O
the	O
raster	O
plots	O
below	O
show	O
the	O
activity	O
patterns	O
of	O
the	O
individual	O
dopamine	B
neurons	O
that	O
were	O
monitored	O
each	O
dot	O
represents	O
an	O
action	B
potential	O
from	O
schultz	B
dayan	O
and	O
montague	O
a	O
neural	B
substrate	O
of	O
prediction	B
and	O
reward	O
science	O
vol	O
issue	O
pages	O
march	O
reprinted	O
with	O
permission	O
from	O
aaas	O
chapter	O
neuroscience	B
external	O
cue	O
to	O
mark	O
the	O
usual	O
time	O
of	O
reward	O
delivery	O
somehow	O
the	O
monkeys	O
were	O
internally	O
keeping	O
track	O
of	O
the	O
timing	O
of	O
the	O
reward	O
timing	O
is	O
one	O
area	O
where	O
the	O
simplest	O
version	O
of	O
td	B
learning	O
needs	O
to	O
be	O
modified	O
to	O
account	O
for	O
some	O
of	O
the	O
details	O
of	O
the	O
timing	O
of	O
dopamine	B
neuron	O
responses	O
we	O
consider	O
this	O
issue	O
in	O
the	O
following	O
section	O
the	O
observations	O
from	O
the	O
studies	O
described	O
above	O
led	O
schultz	B
and	O
his	O
group	O
to	O
conclude	O
that	O
dopamine	B
neurons	O
respond	O
to	O
unpredicted	O
rewards	O
to	O
the	O
earliest	O
predictors	O
of	O
reward	O
and	O
that	O
dopamine	B
neuron	O
activity	O
decreases	O
below	O
baseline	B
if	O
a	O
reward	O
or	O
a	O
predictor	O
of	O
reward	O
does	O
not	O
occur	O
at	O
its	O
expected	B
time	O
researchers	O
familiar	O
with	O
reinforcement	B
learning	I
were	O
quick	O
to	O
recognize	O
that	O
these	O
results	O
are	O
strikingly	O
similar	O
to	O
how	O
the	O
td	B
error	I
behaves	O
as	O
the	O
reinforcement	B
signal	I
in	O
a	O
td	B
algorithm	O
the	O
next	O
section	O
explores	O
this	O
similarity	O
by	O
working	O
through	O
a	O
specific	O
example	O
in	O
detail	O
td	B
errordopamine	O
correspondence	O
this	O
section	O
explains	O
the	O
correspondence	O
between	O
the	O
td	B
error	I
and	O
the	O
phasic	O
responses	O
of	O
dopamine	B
neurons	O
observed	O
in	O
the	O
experiments	O
just	O
described	O
we	O
examine	O
how	O
changes	O
over	O
the	O
course	O
of	O
learning	O
in	O
a	O
task	O
something	O
like	O
the	O
one	O
described	O
above	O
where	O
a	O
monkey	O
first	O
sees	O
an	O
instruction	O
cue	O
and	O
then	O
a	O
fixed	O
time	O
later	O
has	O
to	O
respond	O
correctly	O
to	O
a	O
trigger	O
cue	O
in	O
order	O
to	O
obtain	O
reward	O
we	O
use	O
a	O
simple	O
idealized	O
version	O
of	O
this	O
task	O
but	O
we	O
go	O
into	O
a	O
lot	O
more	O
detail	O
than	O
is	O
usual	O
because	O
we	O
want	O
to	O
emphasize	O
the	O
theoretical	O
basis	O
of	O
the	O
parallel	O
between	O
td	B
errors	O
and	O
dopamine	B
neuron	O
activity	O
the	O
first	O
simplifying	O
assumption	O
is	O
that	O
the	O
agent	O
has	O
already	O
learned	O
the	O
actions	O
required	O
to	O
obtain	O
reward	O
then	O
its	O
task	O
is	O
just	O
to	O
learn	O
accurate	O
predictions	O
of	O
future	O
reward	O
for	O
the	O
sequence	O
of	O
states	O
it	O
experiences	O
this	O
is	O
then	O
a	O
prediction	B
task	O
or	O
more	O
technically	O
a	O
policy-evaluation	O
task	O
learning	O
the	O
value	B
function	I
for	O
a	O
fixed	O
policy	B
and	O
the	O
value	B
function	I
to	O
be	O
learned	O
assigns	O
to	O
each	O
state	B
a	O
value	B
that	O
predicts	O
the	O
return	B
that	O
will	O
follow	O
that	O
state	B
if	O
the	O
agent	O
selects	O
actions	O
according	O
to	O
the	O
given	O
policy	B
where	O
the	O
return	B
is	O
the	O
discounted	O
sum	O
of	O
all	O
the	O
future	O
rewards	O
this	O
is	O
unrealistic	O
as	O
a	O
model	O
of	O
the	O
monkey	O
s	O
situation	O
because	O
the	O
monkey	O
would	O
likely	O
learn	O
these	O
predictions	O
at	O
the	O
same	O
time	O
that	O
it	O
is	O
learning	O
to	O
act	O
correctly	O
would	O
a	O
reinforcement	B
learning	I
algorithm	O
that	O
learns	O
policies	O
as	O
well	O
as	O
value	B
functions	O
such	O
as	O
an	O
actor	O
critic	O
algorithm	O
but	O
this	O
scenario	O
is	O
simpler	O
to	O
describe	O
than	O
one	O
in	O
which	O
a	O
policy	B
and	O
a	O
value	B
function	I
are	O
learned	O
simultaneously	O
now	O
imagine	O
that	O
the	O
agent	O
s	O
experience	O
divides	O
into	O
multiple	O
trials	O
in	O
each	O
of	O
which	O
the	O
same	O
sequence	O
of	O
states	O
repeats	O
with	O
a	O
distinct	O
state	B
occurring	O
on	O
each	O
time	O
step	O
during	O
the	O
trial	O
further	O
imagine	O
that	O
the	O
return	B
being	O
predicted	O
is	O
limited	O
to	O
the	O
return	B
over	O
a	O
trial	O
which	O
makes	O
a	O
trial	O
analogous	O
to	O
a	O
reinforcement	B
learning	I
episode	O
as	O
we	O
have	O
defined	O
it	O
in	O
reality	O
of	O
course	O
the	O
returns	O
being	O
predicted	O
are	O
not	O
confined	O
to	O
single	O
trials	O
and	O
the	O
time	O
interval	O
between	O
trials	O
is	O
an	O
important	O
factor	O
in	O
determining	O
what	O
an	O
animal	O
learns	O
this	O
is	O
true	O
for	O
td	B
learning	O
as	O
well	O
but	O
here	O
we	O
assume	O
that	O
returns	O
do	O
not	O
accumulate	O
over	O
multiple	O
trials	O
given	O
this	O
then	O
a	O
trial	O
in	O
experiments	O
like	O
those	O
conducted	O
by	O
schultz	B
and	O
colleagues	O
is	O
equivalent	O
to	O
an	O
episode	O
of	O
reinforcement	B
learning	I
in	O
this	O
discussion	O
we	O
will	O
use	O
the	O
term	O
trial	O
instead	O
of	O
episode	O
to	O
td	B
errordopamine	O
correspondence	O
relate	O
better	O
to	O
the	O
experiments	O
as	O
usual	O
we	O
also	O
need	O
to	O
make	O
an	O
assumption	O
about	O
how	O
states	O
are	O
represented	O
as	O
inputs	O
to	O
the	O
learning	O
algorithm	O
an	O
assumption	O
that	O
influences	O
how	O
closely	O
the	O
td	B
error	I
corresponds	O
to	O
dopamine	B
neuron	O
activity	O
we	O
discuss	O
this	O
issue	O
later	O
but	O
for	O
now	O
we	O
assume	O
the	O
same	O
csc	O
representation	O
used	O
by	O
montague	O
et	O
al	O
in	O
which	O
there	O
is	O
a	O
separate	O
internal	O
stimulus	O
for	O
each	O
state	B
visited	O
at	O
each	O
time	O
step	O
in	O
a	O
trial	O
this	O
reduces	O
the	O
process	O
to	O
the	O
tabular	O
case	O
covered	O
in	O
the	O
first	O
part	O
of	O
this	O
book	O
finally	O
we	O
assume	O
that	O
the	O
agent	O
uses	O
to	O
learn	O
a	O
value	B
function	I
v	O
stored	O
in	O
a	O
lookup	O
table	O
initialized	O
to	O
be	O
zero	O
for	O
all	O
the	O
states	O
we	O
also	O
assume	O
that	O
this	O
is	O
a	O
deterministic	O
task	O
and	O
that	O
the	O
discount	O
factor	O
is	O
very	O
nearly	O
one	O
so	O
that	O
we	O
can	O
ignore	O
it	O
figure	O
shows	O
the	O
time	O
courses	O
of	O
r	O
v	O
and	O
at	O
several	O
stages	O
of	O
learning	O
in	O
this	O
policy-evaluation	O
task	O
the	O
time	O
axes	O
represent	O
the	O
time	O
interval	O
over	O
which	O
a	O
sequence	O
of	O
states	O
is	O
visited	O
in	O
a	O
trial	O
for	O
clarity	O
we	O
omit	O
showing	O
individual	O
states	O
the	O
reward	B
signal	I
is	O
zero	O
throughout	O
each	O
trial	O
except	O
when	O
the	O
agent	O
reaches	O
the	O
rewarding	O
state	B
shown	O
near	O
the	O
right	O
end	O
of	O
the	O
time	O
line	O
when	O
the	O
reward	B
signal	I
becomes	O
some	O
positive	O
number	O
say	O
the	O
goal	O
of	O
td	B
learning	O
is	O
to	O
predict	O
the	O
return	B
for	O
each	O
state	B
visited	O
in	O
a	O
trial	O
which	O
in	O
this	O
undiscounted	O
case	O
and	O
given	O
our	O
assumption	O
that	O
predictions	O
are	O
confined	O
to	O
individual	O
trials	O
is	O
simply	O
for	O
each	O
state	B
figure	O
the	O
behavior	O
of	O
the	O
td	B
error	I
during	O
td	B
learning	O
is	O
consistent	O
with	O
features	O
of	O
the	O
phasic	O
activation	O
of	O
dopamine	B
neurons	O
is	O
the	O
td	B
error	I
available	O
at	O
time	O
t	O
i	O
e	O
t	O
top	O
a	O
sequence	O
of	O
states	O
shown	O
as	O
an	O
interval	O
of	O
regular	O
predictors	O
is	O
followed	O
by	O
a	O
non-zero	O
reward	O
early	O
in	O
learning	O
the	O
initial	O
value	B
function	I
v	O
and	O
initial	O
which	O
at	O
first	O
is	O
equal	O
to	O
learning	O
complete	O
the	O
value	B
function	I
accurately	O
predicts	O
future	O
reward	O
is	O
positive	O
at	O
the	O
earliest	O
predictive	O
state	B
and	O
at	O
the	O
time	O
of	O
the	O
non-zero	O
reward	O
omitted	O
at	O
the	O
time	O
the	O
predicted	O
reward	O
is	O
omitted	O
becomes	O
negative	O
see	O
text	O
for	O
a	O
complete	O
explanation	O
of	O
why	O
this	O
happens	O
inlearninglearningcompleteomittedrregular	O
predictors	O
of	O
over	O
this	O
intervalrr	O
chapter	O
neuroscience	B
preceding	O
the	O
rewarding	O
state	B
is	O
a	O
sequence	O
of	O
reward-predicting	O
states	O
with	O
the	O
earliest	O
reward-predicting	O
state	B
shown	O
near	O
the	O
left	O
end	O
of	O
the	O
time	O
line	O
this	O
is	O
like	O
the	O
state	B
near	O
the	O
start	O
of	O
a	O
trial	O
for	O
example	O
like	O
the	O
state	B
marked	O
by	O
the	O
instruction	O
cue	O
in	O
a	O
trial	O
of	O
the	O
monkey	O
experiment	O
of	O
schultz	B
et	O
al	O
described	O
above	O
it	O
is	O
the	O
first	O
state	B
in	O
a	O
trial	O
that	O
reliably	O
predicts	O
that	O
trial	O
s	O
reward	O
course	O
in	O
reality	O
states	O
visited	O
on	O
preceding	O
trials	O
are	O
even	O
earlier	O
reward-predicting	O
states	O
but	O
because	O
we	O
are	O
confining	O
predictions	O
to	O
individual	O
trials	O
these	O
do	O
not	O
qualify	O
as	O
predictors	O
of	O
this	O
trial	O
s	O
reward	O
below	O
we	O
give	O
a	O
more	O
satisfactory	O
though	O
more	O
abstract	O
description	O
of	O
an	O
earliest	O
reward-predicting	O
state	B
the	O
latest	O
reward-predicting	O
state	B
in	O
a	O
trial	O
is	O
the	O
state	B
immediately	O
preceding	O
the	O
trial	O
s	O
rewarding	O
state	B
this	O
is	O
the	O
state	B
near	O
the	O
far	O
right	O
end	O
of	O
the	O
time	O
line	O
in	O
figure	O
note	O
that	O
the	O
rewarding	O
state	B
of	O
a	O
trial	O
does	O
not	O
predict	O
the	O
return	B
for	O
that	O
trial	O
the	O
value	B
of	O
this	O
state	B
would	O
come	O
to	O
predict	O
the	O
return	B
over	O
all	O
the	O
following	O
trials	O
which	O
here	O
we	O
are	O
assuming	O
to	O
be	O
zero	O
in	O
this	O
episodic	O
formulation	O
figure	O
shows	O
the	O
first-trial	O
time	O
courses	O
of	O
v	O
and	O
as	O
the	O
graphs	O
labeled	O
early	O
in	O
learning	O
because	O
the	O
reward	B
signal	I
is	O
zero	O
throughout	O
the	O
trial	O
except	O
when	O
the	O
rewarding	O
state	B
is	O
reached	O
and	O
all	O
the	O
v	O
are	O
zero	O
the	O
td	B
error	I
is	O
also	O
zero	O
until	O
it	O
becomes	O
at	O
the	O
rewarding	O
state	B
this	O
follows	O
because	O
t	O
rt	O
vt	O
vt	O
rt	O
rt	O
which	O
is	O
zero	O
until	O
it	O
equals	O
when	O
the	O
reward	O
occurs	O
here	O
vt	O
and	O
vt	O
are	O
respectively	O
the	O
estimated	O
values	O
of	O
the	O
states	O
visited	O
at	O
times	O
t	O
and	O
t	O
in	O
a	O
trial	O
the	O
td	B
error	I
at	O
this	O
stage	O
of	O
learning	O
is	O
analogous	O
to	O
a	O
dopamine	B
neuron	O
responding	O
to	O
an	O
unpredicted	O
reward	O
e	O
g	O
a	O
drop	O
apple	O
juice	O
at	O
the	O
start	O
of	O
training	O
throughout	O
this	O
first	O
trial	O
and	O
all	O
successive	O
trials	O
updates	O
occur	O
at	O
each	O
state	B
transition	O
as	O
described	O
in	O
chapter	O
this	O
successively	O
increases	O
the	O
values	O
of	O
the	O
reward-predicting	O
states	O
with	O
the	O
increases	O
spreading	O
backwards	O
from	O
the	O
rewarding	O
state	B
until	O
the	O
values	O
converge	O
to	O
the	O
correct	O
return	B
predictions	O
in	O
this	O
case	O
we	O
are	O
assuming	O
no	O
discounting	B
the	O
correct	O
predictions	O
are	O
equal	O
to	O
for	O
all	O
the	O
reward-predicting	O
states	O
this	O
can	O
be	O
seen	O
in	O
figure	O
as	O
the	O
graph	O
of	O
v	O
labeled	O
learning	O
complete	O
where	O
the	O
values	O
of	O
all	O
the	O
states	O
from	O
the	O
earliest	O
to	O
the	O
latest	O
reward-predicting	O
states	O
all	O
equal	O
the	O
values	O
of	O
the	O
states	O
preceding	O
the	O
earliest	O
reward-predicting	O
state	B
remain	O
low	O
figure	O
shows	O
as	O
zero	O
because	O
they	O
are	O
not	O
reliable	O
predictors	O
of	O
reward	O
when	O
learning	O
is	O
complete	O
that	O
is	O
when	O
v	O
attains	O
its	O
correct	O
values	O
the	O
td	B
errors	O
associated	O
with	O
transitions	O
from	O
any	O
reward-predicting	O
state	B
are	O
zero	O
because	O
the	O
predictions	O
are	O
now	O
accurate	O
this	O
is	O
because	O
for	O
a	O
transition	O
from	O
a	O
reward-predicting	O
state	B
to	O
another	O
reward-predicting	O
state	B
we	O
have	O
t	O
rt	O
vt	O
vt	O
and	O
for	O
the	O
transition	O
from	O
the	O
latest	O
reward-predicting	O
state	B
to	O
the	O
rewarding	O
state	B
we	O
have	O
t	O
rt	O
vt	O
vt	O
on	O
the	O
other	O
hand	O
the	O
td	B
error	I
on	O
a	O
transition	O
from	O
any	O
state	B
to	O
the	O
earliest	O
reward-predicting	O
state	B
is	O
positive	O
because	O
of	O
the	O
mismatch	O
between	O
this	O
state	B
s	O
low	O
value	B
and	O
the	O
larger	O
value	B
of	O
the	O
following	O
rewardpredicting	O
state	B
indeed	O
if	O
the	O
value	B
of	O
a	O
state	B
preceding	O
the	O
earliest	O
reward-predicting	O
state	B
were	O
zero	O
then	O
after	O
the	O
transition	O
to	O
the	O
earliest	O
reward-predicting	O
state	B
we	O
would	O
have	O
that	O
t	O
rt	O
vt	O
vt	O
the	O
learning	O
complete	O
graph	O
of	O
in	O
figure	O
shows	O
this	O
positive	O
value	B
at	O
the	O
earliest	O
reward-predicting	O
state	B
and	O
zeros	O
td	B
errordopamine	O
correspondence	O
everywhere	O
else	O
the	O
positive	O
td	B
error	I
upon	O
transitioning	O
to	O
the	O
earliest	O
reward-predicting	O
state	B
is	O
analogous	O
to	O
the	O
persistence	O
of	O
dopamine	B
responses	O
to	O
the	O
earliest	O
stimuli	O
predicting	O
reward	O
by	O
the	O
same	O
token	O
when	O
learning	O
is	O
complete	O
a	O
transition	O
from	O
the	O
latest	O
reward-predicting	O
state	B
to	O
the	O
rewarding	O
state	B
produces	O
a	O
zero	O
td	B
error	I
because	O
the	O
latest	O
reward-predicting	O
state	B
s	O
value	B
being	O
correct	O
cancels	O
the	O
reward	O
this	O
parallels	O
the	O
observation	O
that	O
fewer	O
dopamine	B
neurons	O
generate	O
a	O
phasic	O
response	O
to	O
a	O
fully	O
predicted	O
reward	O
than	O
to	O
an	O
unpredicted	O
reward	O
after	O
learning	O
if	O
the	O
reward	O
is	O
suddenly	O
omitted	O
the	O
td	B
error	I
goes	O
negative	O
at	O
the	O
usual	O
time	O
of	O
reward	O
because	O
the	O
value	B
of	O
the	O
latest	O
reward-predicting	O
state	B
is	O
then	O
too	O
high	O
t	O
rt	O
vt	O
vt	O
as	O
shown	O
at	O
the	O
right	O
end	O
of	O
the	O
r	O
omitted	O
graph	O
of	O
in	O
figure	O
this	O
is	O
like	O
dopamine	B
neuron	O
activity	O
decreasing	O
below	O
baseline	B
at	O
the	O
time	O
an	O
expected	B
reward	O
is	O
omitted	O
as	O
seen	O
in	O
the	O
experiment	O
of	O
schultz	B
et	O
al	O
described	O
above	O
and	O
shown	O
in	O
figure	O
the	O
idea	O
of	O
an	O
earliest	O
reward-predicting	O
state	B
deserves	O
more	O
attention	O
in	O
the	O
scenario	O
described	O
above	O
because	O
experience	O
is	O
divided	O
into	O
trials	O
and	O
we	O
assumed	O
that	O
predictions	O
are	O
confined	O
to	O
individual	O
trials	O
the	O
earliest	O
reward-predicting	O
state	B
is	O
always	O
the	O
first	O
state	B
of	O
a	O
trial	O
clearly	O
this	O
is	O
artificial	O
a	O
more	O
general	O
way	O
to	O
think	O
of	O
an	O
earliest	O
reward-predicting	O
state	B
is	O
that	O
it	O
is	O
an	O
unpredicted	O
predictor	O
of	O
reward	O
and	O
there	O
can	O
be	O
many	O
such	O
states	O
in	O
an	O
animal	O
s	O
life	O
many	O
different	O
states	O
may	O
precede	O
an	O
earliest	O
reward-predicting	O
state	B
however	O
because	O
these	O
states	O
are	O
more	O
often	O
followed	O
by	O
other	O
states	O
that	O
do	O
not	O
predict	O
reward	O
their	O
reward-predicting	O
powers	O
that	O
is	O
their	O
values	O
remain	O
low	O
a	O
td	B
algorithm	O
if	O
operating	O
throughout	O
the	O
animal	O
s	O
life	O
would	O
update	O
the	O
values	O
of	O
these	O
states	O
too	O
but	O
the	O
updates	O
would	O
not	O
consistently	O
accumulate	O
because	O
by	O
assumption	O
none	O
of	O
these	O
states	O
reliably	O
precedes	O
an	O
earliest	O
reward-predicting	O
state	B
if	O
any	O
of	O
them	O
did	O
they	O
would	O
be	O
reward-predicting	O
states	O
as	O
well	O
this	O
might	O
explain	O
why	O
with	O
overtraining	O
dopamine	B
responses	O
decrease	O
to	O
even	O
the	O
earliest	O
reward-predicting	O
stimulus	O
in	O
a	O
trial	O
with	O
overtraining	O
one	O
would	O
expect	O
that	O
even	O
a	O
formerly-unpredicted	O
predictor	O
state	B
would	O
become	O
predicted	O
by	O
stimuli	O
associated	O
with	O
earlier	O
states	O
the	O
animal	O
s	O
interaction	O
with	O
its	O
environment	B
both	O
inside	O
and	O
outside	O
of	O
an	O
experimental	O
task	O
would	O
become	O
commonplace	O
upon	O
breaking	O
this	O
routine	O
with	O
the	O
introduction	O
of	O
a	O
new	O
task	O
however	O
one	O
would	O
see	O
td	B
errors	O
reappear	O
as	O
indeed	O
is	O
observed	O
in	O
dopamine	B
neuron	O
activity	O
the	O
example	O
described	O
above	O
explains	O
why	O
the	O
td	B
error	I
shares	O
key	O
features	O
with	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
when	O
the	O
animal	O
is	O
learning	O
in	O
a	O
task	O
similar	O
to	O
the	O
idealized	O
task	O
of	O
our	O
example	O
but	O
not	O
every	O
property	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
coincides	O
so	O
neatly	O
with	O
properties	O
of	O
one	O
of	O
the	O
most	O
troubling	O
discrepancies	O
involves	O
what	O
happens	O
when	O
a	O
reward	O
occurs	O
earlier	O
than	O
expected	B
we	O
have	O
seen	O
that	O
the	O
omission	O
of	O
an	O
expected	B
reward	O
produces	O
a	O
negative	O
prediction	B
error	O
at	O
the	O
reward	O
s	O
expected	B
time	O
which	O
corresponds	O
to	O
the	O
activity	O
of	O
dopamine	B
neurons	O
decreasing	O
below	O
baseline	B
when	O
this	O
happens	O
if	O
the	O
reward	O
arrives	O
later	O
than	O
expected	B
it	O
is	O
then	O
an	O
unexpected	O
reward	O
and	O
generates	O
a	O
positive	O
prediction	B
error	O
this	O
happens	O
with	O
both	O
td	B
errors	O
and	O
dopamine	B
neuron	O
responses	O
but	O
when	O
reward	O
arrives	O
earlier	O
than	O
expected	B
dopamine	B
neurons	O
do	O
not	O
do	O
what	O
the	O
td	B
error	I
does	O
at	O
least	O
with	O
the	O
chapter	O
neuroscience	B
csc	O
representation	O
used	O
by	O
montague	O
et	O
al	O
and	O
by	O
us	O
in	O
our	O
example	O
dopamine	B
neurons	O
do	O
respond	O
to	O
the	O
early	O
reward	O
which	O
is	O
consistent	O
with	O
a	O
positive	O
td	B
error	I
because	O
the	O
reward	O
is	O
not	O
predicted	O
to	O
occur	O
then	O
however	O
at	O
the	O
later	O
time	O
when	O
the	O
reward	O
is	O
expected	B
but	O
omitted	O
the	O
td	B
error	I
is	O
negative	O
whereas	O
in	O
contrast	O
to	O
this	O
prediction	B
dopamine	B
neuron	O
activity	O
does	O
not	O
drop	O
below	O
baseline	B
in	O
the	O
way	O
the	O
td	B
model	I
predicts	O
and	O
schultz	B
something	O
more	O
complicated	O
is	O
going	O
on	O
in	O
the	O
animal	O
s	O
brain	O
than	O
simply	O
td	B
learning	O
with	O
a	O
csc	O
representation	O
some	O
of	O
the	O
mismatches	O
between	O
the	O
td	B
error	I
and	O
dopamine	B
neuron	O
activity	O
can	O
be	O
addressed	O
by	O
selecting	O
suitable	O
parameter	O
values	O
for	O
the	O
td	B
algorithm	O
and	O
by	O
using	O
stimulus	O
representations	O
other	O
than	O
the	O
csc	O
representation	O
for	O
instance	O
to	O
address	O
the	O
early-reward	O
mismatch	O
just	O
described	O
suri	O
and	O
schultz	B
proposed	O
a	O
csc	O
representation	O
in	O
which	O
the	O
sequences	O
of	O
internal	O
signals	O
initiated	O
by	O
earlier	O
stimuli	O
are	O
cancelled	O
by	O
the	O
occurrence	O
of	O
a	O
reward	O
another	O
proposal	O
by	O
daw	O
courville	O
and	O
touretzky	O
is	O
that	O
the	O
brain	O
s	O
td	B
system	O
uses	O
representations	O
produced	O
by	O
statistical	O
modeling	O
carried	O
out	O
in	O
sensory	O
cortex	O
rather	O
than	O
simpler	O
representations	O
based	O
on	O
raw	O
sensory	O
input	O
ludvig	O
sutton	O
and	O
kehoe	O
found	O
that	O
td	B
learning	O
with	O
a	O
microstimulus	O
representation	O
fits	O
the	O
activity	O
of	O
dopamine	B
neurons	O
in	O
the	O
early-reward	O
and	O
other	O
situations	O
better	O
than	O
when	O
a	O
csc	O
representation	O
is	O
used	O
pan	O
schmidt	O
wickens	O
and	O
hyland	O
found	O
that	O
even	O
with	O
the	O
csc	O
representation	O
prolonged	O
eligibility	B
traces	I
improve	O
the	O
fit	O
of	O
the	O
td	B
error	I
to	O
some	O
aspects	O
of	O
dopamine	B
neuron	O
activity	O
in	O
general	O
many	O
fine	O
details	O
of	O
td-error	O
behavior	O
depend	O
on	O
subtle	O
interactions	O
between	O
eligibility	B
traces	I
discounting	B
and	O
stimulus	O
representations	O
findings	O
like	O
these	O
elaborate	O
and	O
refine	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
without	O
refuting	O
its	O
core	O
claim	O
that	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
is	O
well	O
characterized	O
as	O
signaling	O
td	B
errors	O
on	O
the	O
other	O
hand	O
there	O
are	O
other	O
discrepancies	O
between	O
the	O
td	B
theory	O
and	O
experimental	O
data	O
that	O
are	O
not	O
so	O
easily	O
accommodated	O
by	O
selecting	O
parameter	O
values	O
and	O
stimulus	O
representations	O
mention	O
some	O
of	O
these	O
discrepancies	O
in	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
and	O
more	O
mismatches	O
are	O
likely	O
to	O
be	O
discovered	O
as	O
neuroscientists	O
conduct	O
ever	O
more	O
refined	O
experiments	O
but	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
has	O
been	O
functioning	O
very	O
effectively	O
as	O
a	O
catalyst	O
for	O
improving	O
our	O
understanding	O
of	O
how	O
the	O
brain	O
s	O
reward	O
system	O
works	O
intricate	O
experiments	O
have	O
been	O
designed	O
to	O
validate	O
or	O
refute	O
predictions	O
derived	O
from	O
the	O
hypothesis	O
and	O
experimental	O
results	O
have	O
in	O
turn	O
led	O
to	O
refinement	O
and	O
elaboration	O
of	O
the	O
td	B
errordopamine	O
hypothesis	O
a	O
remarkable	O
aspect	O
of	O
these	O
developments	O
is	O
that	O
the	O
reinforcement	B
learning	I
algorithms	O
and	O
theory	O
that	O
connect	O
so	O
well	O
with	O
properties	O
of	O
the	O
dopamine	B
system	O
were	O
developed	O
from	O
a	O
computational	O
perspective	O
in	O
total	O
absence	O
of	O
any	O
knowledge	O
about	O
the	O
relevant	O
properties	O
of	O
dopamine	B
neurons	O
remember	O
td	B
learning	O
and	O
its	O
connections	O
to	O
optimal	B
control	B
and	B
dynamic	B
programming	I
were	O
developed	O
many	O
years	O
before	O
any	O
of	O
the	O
experiments	O
were	O
conducted	O
that	O
revealed	O
the	O
td-like	O
nature	O
of	O
dopamine	B
neuron	O
activity	O
this	O
unplanned	O
correspondence	O
despite	O
not	O
being	O
perfect	O
suggests	O
that	O
the	O
td	B
errordopamine	O
parallel	O
captures	O
something	O
significant	O
about	O
brain	O
reward	O
processes	O
neural	B
actor	O
critic	O
in	O
addition	O
to	O
accounting	O
for	O
many	O
features	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
links	O
neuroscience	B
to	O
other	O
aspects	O
of	O
reinforcement	B
learning	I
in	O
particular	O
to	O
learning	O
algorithms	O
that	O
use	O
td	B
errors	O
as	O
reinforcement	O
signals	O
neuroscience	B
is	O
still	O
far	O
from	O
reaching	O
complete	O
understanding	O
of	O
the	O
circuits	O
molecular	O
mechanisms	O
and	O
functions	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
but	O
evidence	O
supporting	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
along	O
with	O
evidence	O
that	O
phasic	O
dopamine	B
responses	O
are	O
reinforcement	O
signals	O
for	O
learning	O
suggest	O
that	O
the	O
brain	O
might	O
implement	O
something	O
like	O
an	O
actor	O
critic	O
algorithm	O
in	O
which	O
td	B
errors	O
play	O
critical	O
roles	O
other	O
reinforcement	B
learning	I
algorithms	O
are	O
plausible	O
candidates	O
too	O
but	O
actor	O
critic	O
algorithms	O
fit	O
the	O
anatomy	O
and	O
physiology	O
of	O
the	O
mammalian	O
brain	O
particularly	O
well	O
as	O
we	O
describe	O
in	O
the	O
following	O
two	O
sections	O
neural	B
actor	O
critic	O
actor	O
critic	O
algorithms	O
learn	O
both	O
policies	O
and	O
value	B
functions	O
the	O
actor	O
is	O
the	O
component	O
that	O
learns	O
policies	O
and	O
the	O
critic	O
is	O
the	O
component	O
that	O
learns	O
about	O
whatever	O
policy	B
is	O
currently	O
being	O
followed	O
by	O
the	O
actor	O
in	O
order	O
to	O
criticize	O
the	O
actor	O
s	O
action	B
choices	O
the	O
critic	O
uses	O
a	O
td	B
algorithm	O
to	O
learn	O
the	O
state-value	O
function	O
for	O
the	O
actor	O
s	O
current	O
policy	B
the	O
value	B
function	I
allows	O
the	O
critic	O
to	O
critique	O
the	O
actor	O
s	O
action	B
choices	O
by	O
sending	O
td	B
errors	O
to	O
the	O
actor	O
a	O
positive	O
means	O
that	O
the	O
action	B
was	O
good	O
because	O
it	O
led	O
to	O
a	O
state	B
with	O
a	O
better-than-expected	O
value	B
a	O
negative	O
means	O
that	O
the	O
action	B
was	O
bad	O
because	O
it	O
led	O
to	O
a	O
state	B
with	O
a	O
worse-than-expected	O
value	B
based	O
on	O
these	O
critiques	O
the	O
actor	O
continually	O
updates	O
its	O
policy	B
two	O
distinctive	O
features	O
of	O
actor	O
critic	O
algorithms	O
are	O
responsible	O
for	O
thinking	O
that	O
the	O
brain	O
might	O
implement	O
an	O
algorithm	O
like	O
this	O
first	O
the	O
two	O
components	O
of	O
an	O
actor	O
critic	O
algorithm	O
the	O
actor	O
and	O
the	O
critic	O
suggest	O
that	O
two	O
parts	O
of	O
the	O
striatum	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
both	O
critical	O
for	O
reward-based	O
learning	O
may	O
function	O
respectively	O
something	O
like	O
an	O
actor	O
and	O
a	O
critic	O
a	O
second	O
property	O
of	O
actor	O
critic	O
algorithms	O
that	O
suggests	O
a	O
brain	O
implementation	O
is	O
that	O
the	O
td	B
error	I
has	O
the	O
dual	O
role	O
of	O
being	O
the	O
reinforcement	B
signal	I
for	O
both	O
the	O
actor	O
and	O
the	O
critic	O
though	O
it	O
has	O
a	O
different	O
influence	O
on	O
learning	O
in	O
each	O
of	O
these	O
components	O
this	O
fits	O
well	O
with	O
several	O
properties	O
of	O
the	O
neural	B
circuitry	O
axons	O
of	O
dopamine	B
neurons	O
target	O
both	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
dopamine	B
appears	O
to	O
be	O
critical	O
for	O
modulating	O
synaptic	B
plasticity	I
in	O
both	O
structures	O
and	O
how	O
a	O
neuromodulator	O
such	O
as	O
dopamine	B
acts	O
on	O
a	O
target	O
structure	O
depends	O
on	O
properties	O
of	O
the	O
target	O
structure	O
and	O
not	O
just	O
on	O
properties	O
of	O
the	O
neuromodulator	O
section	O
presents	O
actor	O
critic	O
algorithms	O
as	O
policy	B
gradient	B
methods	I
but	O
the	O
actor	O
critic	O
algorithm	O
of	O
barto	O
sutton	O
and	O
anderson	O
was	O
simpler	O
and	O
was	O
presented	O
as	O
an	O
artificial	O
neural	B
network	O
here	O
we	O
describe	O
an	O
artificial	O
neural	B
network	O
implementation	O
something	O
like	O
that	O
of	O
barto	O
et	O
al	O
and	O
we	O
follow	O
takahashi	O
schoenbaum	O
and	O
niv	O
in	O
giving	O
a	O
schematic	O
proposal	O
for	O
how	O
this	O
artificial	O
neural	B
network	O
might	O
be	O
implemented	O
by	O
real	O
neural	B
networks	O
in	O
the	O
brain	O
we	O
postpone	O
discussion	O
of	O
the	O
actor	O
and	O
critic	O
learning	O
rules	O
until	O
section	O
where	O
we	O
present	O
them	O
as	O
special	O
cases	O
of	O
the	O
policy-gradient	O
formulation	O
and	O
discuss	O
what	O
they	O
suggest	O
about	O
how	O
chapter	O
neuroscience	B
dopamine	B
might	O
modulate	O
synaptic	B
plasticity	I
figure	O
shows	O
an	O
implementation	O
of	O
an	O
actor	O
critic	O
algorithm	O
as	O
an	O
artificial	O
neural	B
network	O
with	O
component	O
networks	O
implementing	O
the	O
actor	O
and	O
the	O
critic	O
the	O
critic	O
consists	O
of	O
a	O
single	O
neuron-like	O
unit	O
v	O
whose	O
output	O
activity	O
represents	O
state	B
values	O
and	O
a	O
component	O
shown	O
as	O
the	O
diamond	O
labeled	O
td	B
that	O
computes	O
td	B
errors	O
by	O
combining	O
v	O
s	O
output	O
with	O
reward	O
signals	O
and	O
with	O
previous	O
state	B
values	O
suggested	O
by	O
the	O
loop	O
from	O
the	O
td	B
diamond	O
to	O
itself	O
the	O
actor	O
network	O
has	O
a	O
single	O
layer	O
of	O
k	O
actor	O
units	O
labeled	O
ai	O
i	O
k	O
the	O
output	O
of	O
each	O
actor	O
unit	O
is	O
a	O
component	O
of	O
a	O
k-dimensional	O
action	B
vector	B
an	O
alternative	O
is	O
that	O
there	O
are	O
k	O
separate	O
actions	O
one	O
commanded	O
by	O
each	O
actor	O
unit	O
that	O
compete	O
with	O
one	O
another	O
to	O
be	O
executed	O
but	O
here	O
we	O
will	O
think	O
of	O
the	O
entire	O
a-vector	O
as	O
an	O
action	B
figure	O
actor	O
critic	O
artificial	O
neural	B
network	O
and	O
a	O
hypothetical	O
neural	B
implementation	O
a	O
actor	O
critic	O
algorithm	O
as	O
an	O
artificial	O
neural	B
network	O
the	O
actor	O
adjusts	O
a	O
policy	B
based	O
on	O
the	O
td	B
error	I
it	O
receives	O
from	O
the	O
critic	O
the	O
critic	O
adjusts	O
state-value	O
parameters	O
using	O
the	O
same	O
the	O
critic	O
produces	O
a	O
td	B
error	I
from	O
the	O
reward	B
signal	I
r	O
and	O
the	O
current	O
change	O
in	O
its	O
estimate	O
of	O
state	B
values	O
the	O
actor	O
does	O
not	O
have	O
direct	O
access	O
to	O
the	O
reward	B
signal	I
and	O
the	O
critic	O
does	O
not	O
have	O
direct	O
access	O
to	O
the	O
action	B
b	O
hypothetical	O
neural	B
implementation	O
of	O
an	O
actor	O
critic	O
algorithm	O
the	O
actor	O
and	O
the	O
value-learning	O
part	O
of	O
the	O
critic	O
are	O
respectively	O
placed	O
in	O
the	O
ventral	O
and	O
dorsal	O
subdivisions	O
of	O
the	O
striatum	O
the	O
td	B
error	I
is	O
transmitted	O
by	O
dopamine	B
neurons	O
located	O
in	O
the	O
vta	O
and	O
snpc	O
to	O
modulate	O
changes	O
in	O
synaptic	O
efficacies	O
of	O
input	O
from	O
cortical	O
areas	O
to	O
the	O
ventral	O
and	O
dorsal	O
striatum	O
adapted	O
from	O
frontiers	O
in	B
neuroscience	B
vol	O
y	O
takahashi	O
g	O
schoenbaum	O
and	O
y	O
niv	O
silencing	O
the	O
critics	O
understanding	O
the	O
effects	O
of	O
cocaine	O
sensitization	O
on	O
dorsolateral	O
and	O
ventral	O
striatum	O
in	O
the	O
context	O
of	O
an	O
actorcritic	O
model	O
rewardactortd	O
actionsstatesstimulitd	O
striatumcortex	O
neural	B
actor	O
critic	O
both	O
the	O
critic	O
and	O
actor	O
networks	O
receive	O
input	O
consisting	O
of	O
multiple	O
features	O
representing	O
the	O
state	B
of	O
the	O
agent	O
s	O
environment	B
from	O
chapter	O
that	O
the	O
environment	B
of	O
a	O
reinforcement	B
learning	I
agent	O
includes	O
components	O
both	O
inside	O
and	O
outside	O
of	O
the	O
organism	O
containing	O
the	O
agent	O
the	O
figure	O
shows	O
these	O
features	O
as	O
the	O
circles	O
labeled	O
xn	O
shown	O
twice	O
just	O
to	O
keep	O
the	O
figure	O
simple	O
a	O
weight	O
representing	O
the	O
efficacy	O
of	O
a	O
synapse	O
is	O
associated	O
with	O
each	O
connection	O
from	O
each	O
feature	O
xi	O
to	O
the	O
critic	O
unit	O
v	O
and	O
to	O
each	O
of	O
the	O
action	B
units	O
ai	O
the	O
weights	O
in	O
the	O
critic	O
network	O
parameterize	O
the	O
value	B
function	I
and	O
the	O
weights	O
in	O
the	O
actor	O
network	O
parameterize	O
the	O
policy	B
the	O
networks	O
learn	O
as	O
these	O
weights	O
change	O
according	O
to	O
the	O
critic	O
and	O
actor	O
learning	O
rules	O
that	O
we	O
describe	O
in	O
the	O
following	O
section	O
the	O
td	B
error	I
produced	O
by	O
circuitry	O
in	O
the	O
critic	O
is	O
the	O
reinforcement	B
signal	I
for	O
changing	O
the	O
weights	O
in	O
both	O
the	O
critic	O
and	O
the	O
actor	O
networks	O
this	O
is	O
shown	O
in	O
figure	O
by	O
the	O
line	O
labeled	O
td	B
error	I
extending	O
across	O
all	O
of	O
the	O
connections	O
in	O
the	O
critic	O
and	O
actor	O
networks	O
this	O
aspect	O
of	O
the	O
network	O
implementation	O
together	O
with	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
and	O
the	O
fact	O
that	O
the	O
activity	O
of	O
dopamine	B
neurons	O
is	O
so	O
widely	O
distributed	O
by	O
the	O
extensive	O
axonal	O
arbors	O
of	O
these	O
neurons	O
suggests	O
that	O
an	O
actor	O
critic	O
network	O
something	O
like	O
this	O
may	O
not	O
be	O
too	O
farfetched	O
as	O
a	O
hypothesis	O
about	O
how	O
reward-related	O
learning	O
might	O
happen	O
in	O
the	O
brain	O
figure	O
suggests	O
very	O
schematically	O
how	O
the	O
artificial	O
neural	B
network	O
on	O
the	O
figure	O
s	O
left	O
might	O
map	O
onto	O
structures	O
in	O
the	O
brain	O
according	O
to	O
the	O
hypothesis	O
of	O
takahashi	O
et	O
al	O
the	O
hypothesis	O
puts	O
the	O
actor	O
and	O
the	O
value-learning	O
part	O
of	O
the	O
critic	O
respectively	O
in	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
the	O
input	O
structure	O
of	O
the	O
basal	B
ganglia	I
recall	O
from	O
section	O
that	O
the	O
dorsal	O
striatum	O
is	O
primarily	O
implicated	O
in	O
influencing	O
action	B
selection	O
and	O
the	O
ventral	O
striatum	O
is	O
thought	O
to	O
be	O
critical	O
for	O
different	O
aspects	O
of	O
reward	O
processing	O
including	O
the	O
assignment	O
of	O
affective	O
value	B
to	O
sensations	O
the	O
cerebral	O
cortex	O
along	O
with	O
other	O
structures	O
sends	O
input	O
to	O
the	O
striatum	O
conveying	O
information	O
about	O
stimuli	O
internal	O
states	O
and	O
motor	O
activity	O
in	O
this	O
hypothetical	O
actor	O
critic	O
brain	O
implementation	O
the	O
ventral	O
striatum	O
sends	O
value	B
information	O
to	O
the	O
vta	O
and	O
snpc	O
where	O
dopamine	B
neurons	O
in	O
these	O
nuclei	O
combine	O
it	O
with	O
information	O
about	O
reward	O
to	O
generate	O
activity	O
corresponding	O
to	O
td	B
errors	O
exactly	O
how	O
dopaminergic	O
neurons	O
calculate	O
these	O
errors	O
is	O
not	O
yet	O
understood	O
the	O
td	B
error	I
line	O
in	O
figure	O
becomes	O
the	O
line	O
labeled	O
dopamine	B
in	O
figure	O
which	O
represents	O
the	O
widely	O
branching	O
axons	O
of	O
dopamine	B
neurons	O
whose	O
cell	O
bodies	O
are	O
in	O
the	O
vta	O
and	O
snpc	O
referring	O
back	O
to	O
figure	O
these	O
axons	O
make	O
synaptic	O
contact	O
with	O
the	O
spines	O
on	O
the	O
dendrites	O
of	O
medium	O
spiny	O
neurons	O
the	O
main	O
inputoutput	O
neurons	O
of	O
both	O
the	O
dorsal	O
and	O
ventral	O
divisions	O
of	O
the	O
striatum	O
axons	O
of	O
the	O
cortical	O
neurons	O
that	O
send	O
input	O
to	O
the	O
striatum	O
make	O
synaptic	O
contact	O
on	O
the	O
tips	O
of	O
these	O
spines	O
according	O
to	O
the	O
hypothesis	O
it	O
is	O
at	O
these	O
spines	O
where	O
changes	O
in	O
the	O
efficacies	O
of	O
the	O
synapses	O
from	O
cortical	O
regions	O
to	O
the	O
stratum	O
are	O
governed	O
by	O
learning	O
rules	O
that	O
critically	O
depend	O
on	O
a	O
reinforcement	B
signal	I
supplied	O
by	O
dopamine	B
an	O
important	O
implication	O
of	O
the	O
hypothesis	O
illustrated	O
in	O
figure	O
is	O
that	O
the	O
dopamine	B
signal	O
is	O
not	O
the	O
master	O
reward	B
signal	I
like	O
the	O
scalar	O
rt	O
of	O
reinforcement	B
learning	I
in	O
fact	O
the	O
hypothesis	O
implies	O
that	O
one	O
should	O
not	O
necessarily	O
be	O
able	O
to	O
probe	O
the	O
brain	O
and	O
record	O
any	O
signal	O
like	O
rt	O
in	O
the	O
activity	O
of	O
any	O
single	O
neuron	O
many	O
inter	O
chapter	O
neuroscience	B
connected	O
neural	B
systems	O
generate	O
reward-related	O
information	O
with	O
different	O
structures	O
being	O
recruited	O
depending	O
on	O
different	O
types	O
of	O
rewards	O
dopamine	B
neurons	O
receive	O
information	O
from	O
many	O
different	O
brain	O
areas	O
so	O
the	O
input	O
to	O
the	O
snpc	O
and	O
vta	O
labeled	O
reward	O
in	O
figure	O
should	O
be	O
thought	O
of	O
as	O
vector	B
of	O
reward-related	O
information	O
arriving	O
to	O
neurons	O
in	O
these	O
nuclei	O
along	O
multiple	O
input	O
channels	O
what	O
the	O
theoretical	O
scalar	O
reward	B
signal	I
rt	O
might	O
correspond	O
to	O
then	O
is	O
the	O
net	O
contribution	O
of	O
all	O
rewardrelated	O
information	O
to	O
dopamine	B
neuron	O
activity	O
it	O
is	O
the	O
result	O
of	O
a	O
pattern	O
of	O
activity	O
across	O
many	O
neurons	O
in	O
different	O
areas	O
of	O
the	O
brain	O
although	O
the	O
actor	O
critic	O
neural	B
implementation	O
illustrated	O
in	O
figure	O
may	O
be	O
correct	O
on	O
some	O
counts	O
it	O
clearly	O
needs	O
to	O
be	O
refined	O
extended	O
and	O
modified	O
to	O
qualify	O
as	O
a	O
full-fledged	O
model	O
of	O
the	O
function	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
the	O
historical	O
and	O
bibliographic	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
cites	O
publications	O
that	O
discuss	O
in	O
more	O
detail	O
both	O
empirical	O
support	O
for	O
this	O
hypothesis	O
and	O
places	O
where	O
it	O
falls	O
short	O
we	O
now	O
look	O
in	O
detail	O
at	O
what	O
the	O
actor	O
and	O
critic	O
learning	O
algorithms	O
suggest	O
about	O
the	O
rules	O
governing	O
changes	O
in	O
synaptic	O
efficacies	O
of	O
corticostriatal	O
synapses	O
actor	O
and	O
critic	O
learning	O
rules	O
if	O
the	O
brain	O
does	O
implement	O
something	O
like	O
the	O
actor	O
critic	O
algorithm	O
and	O
assuming	O
populations	O
of	O
dopamine	B
neurons	O
broadcast	O
a	O
common	O
reinforcement	B
signal	I
to	O
the	O
corticostriatal	O
synapses	O
of	O
both	O
the	O
dorsal	O
and	O
ventral	O
striatum	O
as	O
illustrated	O
in	O
figure	O
is	O
likely	O
an	O
oversimplification	O
as	O
we	O
mentioned	O
above	O
then	O
this	O
reinforcement	B
signal	I
affects	O
the	O
synapses	O
of	O
these	O
two	O
structures	O
in	O
different	O
ways	O
the	O
learning	O
rules	O
for	O
the	O
critic	O
and	O
the	O
actor	O
use	O
the	O
same	O
reinforcement	B
signal	I
the	O
td	B
error	I
but	O
its	O
effect	O
on	O
learning	O
is	O
different	O
for	O
these	O
two	O
components	O
the	O
td	B
error	I
with	B
eligibility	B
traces	I
tells	O
the	O
actor	O
how	O
to	O
update	O
action	B
probabilities	O
in	O
order	O
to	O
reach	O
higher-valued	O
states	O
learning	O
by	O
the	O
actor	O
is	O
like	O
instrumental	O
conditioning	O
using	O
a	O
law-of-effect-type	O
learning	O
rule	O
the	O
actor	O
works	O
to	O
keep	O
as	O
positive	O
as	O
possible	O
on	O
the	O
other	O
hand	O
the	O
td	B
error	I
combined	O
with	B
eligibility	B
traces	I
tells	O
the	O
critic	O
the	O
direction	O
and	O
magnitude	O
in	O
which	O
to	O
change	O
the	O
parameters	O
of	O
the	O
value	B
function	I
in	O
order	O
to	O
improve	O
its	O
predictive	O
accuracy	O
the	O
critic	O
works	O
to	O
reduce	O
s	O
magnitude	O
to	O
be	O
as	O
close	O
to	O
zero	O
as	O
possible	O
using	O
a	O
learning	O
rule	O
like	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
the	O
difference	O
between	O
the	O
critic	O
and	O
actor	O
learning	O
rules	O
is	O
relatively	O
simple	O
but	O
this	O
difference	O
has	O
a	O
profound	O
effect	O
on	O
learning	O
and	O
is	O
essential	O
to	O
how	O
the	O
actor	O
critic	O
algorithm	O
works	O
the	O
difference	O
lies	O
solely	O
in	O
the	O
eligibility	B
traces	I
each	O
type	O
of	O
learning	O
rule	O
uses	O
more	O
than	O
one	O
set	O
of	O
learning	O
rules	O
can	O
be	O
used	O
in	O
actor	O
critic	O
neural	B
networks	O
like	O
those	O
in	O
figure	O
but	O
to	O
be	O
specific	O
here	O
we	O
focus	O
on	O
the	O
actor	O
critic	O
algorithm	O
for	O
continuing	O
problems	O
with	B
eligibility	B
traces	I
presented	O
in	O
section	O
on	O
each	O
transition	O
from	O
state	B
st	O
to	O
state	B
taking	O
action	B
at	O
and	O
receiving	O
action	B
that	O
algorithm	O
computes	O
the	O
td	B
error	I
and	O
then	O
updates	O
the	O
eligibility	O
trace	O
vectors	O
t	O
and	O
z	O
t	O
actor	O
and	O
critic	O
learning	O
rules	O
and	O
the	O
parameters	O
for	O
the	O
critic	O
and	O
actor	O
and	O
according	O
to	O
t	O
vstw	O
t	O
ln	O
t	O
vstw	O
t	O
wzw	O
zw	O
t	O
z	O
z	O
w	O
w	O
w	O
t	O
zw	O
t	O
z	O
t	O
where	O
is	O
a	O
discount-rate	O
parameter	O
wc	O
and	O
wa	O
are	O
bootstrapping	B
parameters	O
for	O
the	O
critic	O
and	O
the	O
actor	O
respectively	O
and	O
w	O
and	O
are	O
analogous	O
step-size	O
parameters	O
think	O
of	O
the	O
approximate	O
value	B
function	I
v	O
as	O
the	O
output	O
of	O
a	O
single	O
linear	O
neuron-like	O
unit	O
called	O
the	O
critic	O
unit	O
and	O
labeled	O
v	O
in	O
figure	O
then	O
the	O
value	B
function	I
is	O
a	O
linear	O
function	O
of	O
the	O
feature-vector	O
representation	O
of	O
state	B
s	O
xs	O
parameterized	O
by	O
a	O
weight	O
vector	B
w	O
vsw	O
each	O
xis	O
is	O
like	O
the	O
presynaptic	O
signal	O
to	O
a	O
neuron	O
s	O
synapse	O
whose	O
efficacy	O
is	O
wi	O
the	O
weights	O
of	O
the	O
critic	O
are	O
incremented	O
according	O
to	O
the	O
rule	O
above	O
by	O
w	O
tzw	O
t	O
where	O
the	O
reinforcement	B
signal	I
t	O
corresponds	O
to	O
a	O
dopamine	B
signal	O
being	O
broadcast	O
to	O
all	O
of	O
the	O
critic	O
unit	O
s	O
synapses	O
the	O
eligibility	O
trace	O
vector	B
zw	O
t	O
for	O
the	O
critic	O
unit	O
is	O
a	O
trace	O
of	O
recent	O
values	O
of	O
vstw	O
because	O
vsw	O
is	O
linear	O
in	O
the	O
weights	O
vstw	O
xst	O
in	O
neural	B
terms	O
this	O
means	O
that	O
each	O
synapse	O
has	O
its	O
own	O
eligibility	O
trace	O
which	O
is	O
one	O
component	O
of	O
the	O
vector	B
zw	O
t	O
a	O
synapse	O
s	O
eligibility	O
trace	O
accumulates	O
according	O
to	O
the	O
level	O
of	O
activity	O
arriving	O
at	O
that	O
synapse	O
that	O
is	O
the	O
level	O
of	O
presynaptic	O
activity	O
represented	O
here	O
by	O
the	O
component	O
of	O
the	O
feature	O
vector	B
xst	O
arriving	O
at	O
that	O
synapse	O
the	O
trace	O
otherwise	O
decays	O
toward	O
zero	O
at	O
a	O
rate	O
governed	O
by	O
the	O
fraction	O
w	O
a	O
synapse	O
is	O
eligible	O
for	O
modification	O
as	O
long	O
as	O
its	O
eligibility	O
trace	O
is	O
non-zero	O
how	O
the	O
synapse	O
s	O
efficacy	O
is	O
actually	O
modified	O
depends	O
on	O
the	O
reinforcement	O
signals	O
that	O
arrive	O
while	O
the	O
synapse	O
is	O
eligible	O
we	O
call	O
eligibility	B
traces	I
like	O
these	O
of	O
the	O
critic	O
unit	O
s	O
synapses	O
noncontingent	O
eligibility	B
traces	I
because	O
they	O
only	O
depend	O
on	O
presynaptic	O
activity	O
and	O
are	O
not	O
contingent	O
in	O
any	O
way	O
on	O
postsynaptic	O
activity	O
the	O
non-contingent	O
eligibility	B
traces	I
of	O
the	O
critic	O
unit	O
s	O
synapses	O
mean	O
that	O
the	O
critic	O
unit	O
s	O
learning	O
rule	O
is	O
essentially	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
described	O
in	O
section	O
with	O
the	O
definition	O
we	O
have	O
given	O
above	O
of	O
the	O
critic	O
unit	O
and	O
its	O
learning	O
rule	O
the	O
critic	O
in	O
figure	O
is	O
the	O
same	O
as	O
the	O
critic	O
in	O
the	O
neural	B
network	O
actor	O
critic	O
of	O
barto	O
et	O
al	O
clearly	O
a	O
critic	O
like	O
this	O
consisting	O
of	O
just	O
one	O
linear	O
neuron-like	O
unit	O
is	O
the	O
simplest	O
starting	O
point	O
this	O
critic	O
unit	O
is	O
a	O
proxy	O
for	O
a	O
more	O
complicated	O
neural	B
network	O
able	O
to	O
learn	O
value	B
functions	O
of	O
greater	O
complexity	O
the	O
actor	O
in	O
figure	O
is	O
a	O
one-layer	O
network	O
of	O
k	O
neuron-like	O
actor	O
units	O
each	O
receiving	O
at	O
time	O
t	O
the	O
same	O
feature	O
vector	B
xst	O
that	O
the	O
critic	O
unit	O
receives	O
each	O
actor	O
unit	O
j	O
j	O
k	O
has	O
its	O
own	O
weight	O
vector	B
j	O
but	O
because	O
the	O
actor	O
units	O
chapter	O
neuroscience	B
are	O
all	O
identical	O
we	O
describe	O
just	O
one	O
of	O
the	O
units	O
and	O
omit	O
the	O
subscript	O
one	O
way	O
for	O
these	O
units	O
to	O
follow	O
the	O
actor	O
critic	O
algorithm	O
given	O
in	O
the	O
equations	O
above	O
is	O
for	O
each	O
to	O
be	O
a	O
bernoulli-logistic	O
unit	O
this	O
means	O
that	O
the	O
output	O
of	O
each	O
actor	O
unit	O
at	O
each	O
time	O
is	O
a	O
random	O
variable	O
at	O
taking	O
value	B
or	O
think	O
of	O
value	B
as	O
the	O
neuron	O
firing	O
that	O
is	O
emitting	O
an	O
action	B
potential	O
the	O
weighted	O
sum	O
of	O
a	O
unit	O
s	O
input	O
vector	B
determines	O
the	O
unit	O
s	O
action	B
probabilities	O
via	O
the	O
exponential	O
soft-max	B
distribution	O
which	O
for	O
two	O
actions	O
is	O
the	O
logistic	O
function	O
exp	O
the	O
weights	O
of	O
each	O
actor	O
unit	O
are	O
incremented	O
as	O
above	O
by	O
t	O
z	O
t	O
where	O
again	O
corresponds	O
to	O
the	O
dopamine	B
signal	O
the	O
same	O
reinforcement	B
signal	I
that	O
is	O
sent	O
to	O
all	O
the	O
critic	O
unit	O
s	O
synapses	O
figure	O
shows	O
t	O
being	O
broadcast	O
to	O
all	O
the	O
synapses	O
of	O
all	O
the	O
actor	O
units	O
makes	O
this	O
actor	O
network	O
a	O
team	O
of	O
reinforcement	B
learning	I
agents	O
something	O
we	O
discuss	O
in	O
section	O
below	O
the	O
actor	O
eligibility	O
trace	O
vector	B
z	O
is	O
a	O
trace	O
of	O
recent	O
values	O
of	O
ln	O
to	O
understand	O
this	O
t	O
eligibility	O
trace	O
refer	O
to	O
exercise	O
which	O
defines	O
this	O
kind	O
of	O
unit	O
and	O
asks	O
you	O
to	O
give	O
a	O
learning	O
rule	O
for	O
it	O
that	O
exercise	O
asked	O
you	O
to	O
express	O
ln	O
in	O
terms	O
of	O
a	O
xs	O
and	O
arbitrary	O
state	B
s	O
and	O
action	B
a	O
by	O
calculating	O
the	O
gradient	B
for	O
the	O
action	B
and	O
state	B
actually	O
occurring	O
at	O
time	O
t	O
the	O
answer	O
is	O
unlike	O
the	O
non-contingent	O
eligibility	O
trace	O
of	O
a	O
critic	O
synapse	O
that	O
only	O
accumulates	O
the	O
presynaptic	O
activity	O
xst	O
the	O
eligibility	O
trace	O
of	O
an	O
actor	O
unit	O
s	O
synapse	O
in	O
addition	O
depends	O
on	O
the	O
activity	O
of	O
the	O
actor	O
unit	O
itself	O
we	O
call	O
this	O
a	O
contingent	O
eligibility	O
trace	O
because	O
it	O
is	O
contingent	O
on	O
this	O
postsynaptic	O
activity	O
the	O
eligibility	O
trace	O
at	O
each	O
synapse	O
continually	O
decays	O
but	O
increments	O
or	O
decrements	O
depending	O
on	O
the	O
activity	O
of	O
the	O
presynaptic	O
neuron	O
and	O
whether	O
or	O
not	O
the	O
postsynaptic	O
neuron	O
fires	O
the	O
factor	O
at	O
in	O
is	O
positive	O
when	O
at	O
and	O
negative	O
otherwise	O
the	O
postsynaptic	O
contingency	O
in	O
the	O
eligibility	B
traces	I
of	O
actor	O
units	O
is	O
the	O
only	O
difference	O
between	O
the	O
critic	O
and	O
actor	O
learning	O
rules	O
by	O
keeping	O
information	O
about	O
what	O
actions	O
were	O
taken	O
in	O
what	O
states	O
contingent	O
eligibility	B
traces	I
allow	O
credit	O
for	O
reward	O
or	O
blame	O
for	O
punishment	O
to	O
be	O
apportioned	O
among	O
the	O
policy	B
parameters	O
efficacies	O
of	O
the	O
actor	O
units	O
synapses	O
according	O
to	O
the	O
contributions	O
these	O
parameters	O
made	O
to	O
the	O
units	O
outputs	O
that	O
could	O
have	O
influenced	O
later	O
values	O
of	O
contingent	O
eligibility	B
traces	I
mark	O
the	O
synapses	O
as	O
to	O
how	O
they	O
should	O
be	O
modified	O
to	O
alter	O
the	O
units	O
future	O
responses	O
to	O
favor	O
positive	O
values	O
of	O
what	O
do	O
the	O
critic	O
and	O
actor	O
learning	O
rules	O
suggest	O
about	O
how	O
efficacies	O
of	O
corticostriatal	O
synapses	O
change	O
both	O
learning	O
rules	O
are	O
related	O
to	O
donald	O
hebb	O
s	O
classic	O
proposal	O
that	O
whenever	O
a	O
presynaptic	O
signal	O
participates	O
in	O
activating	O
the	O
postsynaptic	O
neuron	O
the	O
synapse	O
s	O
efficacy	O
increases	O
the	O
critic	O
and	O
actor	O
learning	O
rules	O
share	O
with	O
hebb	O
s	O
proposal	O
the	O
idea	O
that	O
changes	O
in	O
a	O
synapse	O
s	O
efficacy	O
depend	O
on	O
the	O
interaction	O
of	O
several	O
factors	O
in	O
the	O
critic	O
learning	O
rule	O
the	O
interaction	O
is	O
between	O
the	O
reinforcement	B
signal	I
and	B
eligibility	B
traces	I
that	O
depend	O
only	O
on	O
presynaptic	O
signals	O
neuroscientists	O
actor	O
and	O
critic	O
learning	O
rules	O
call	O
this	O
a	O
two-factor	O
learning	O
rule	O
because	O
the	O
interaction	O
is	O
between	O
two	O
signals	O
or	O
quantities	O
the	O
actor	O
learning	O
rule	O
on	O
the	O
other	O
hand	O
is	O
a	O
three-factor	O
learning	O
rule	O
because	O
in	O
addition	O
to	O
depending	O
on	O
its	O
eligibility	B
traces	I
depend	O
on	O
both	O
presynaptic	O
and	O
postsynaptic	O
activity	O
unlike	O
hebb	O
s	O
proposal	O
however	O
the	O
relative	O
timing	O
of	O
the	O
factors	O
is	O
critical	O
to	O
how	O
synaptic	O
efficacies	O
change	O
with	B
eligibility	B
traces	I
intervening	O
to	O
allow	O
the	O
reinforcement	B
signal	I
to	O
affect	O
synapses	O
that	O
were	O
active	O
in	O
the	O
recent	O
past	O
some	O
subtleties	O
about	O
signal	O
timing	O
for	O
the	O
actor	O
and	O
critic	O
learning	O
rules	O
deserve	O
closer	O
attention	O
in	O
defining	O
the	O
neuron-like	O
actor	O
and	O
critic	O
units	O
we	O
ignored	O
the	O
small	O
amount	O
of	O
time	O
it	O
takes	O
synaptic	O
input	O
to	O
effect	O
the	O
firing	O
of	O
a	O
real	O
neuron	O
when	O
an	O
action	B
potential	O
from	O
the	O
presynaptic	O
neuron	O
arrives	O
at	O
a	O
synapse	O
neurotransmitter	O
molecules	O
are	O
released	O
that	O
diffuse	O
across	O
the	O
synaptic	O
cleft	O
to	O
the	O
postsynaptic	O
neuron	O
where	O
they	O
bind	O
to	O
receptors	O
on	O
the	O
postsynaptic	O
neuron	O
s	O
surface	O
this	O
activates	O
molecular	O
machinery	O
that	O
causes	O
the	O
postsynaptic	O
neuron	O
to	O
fire	O
to	O
inhibit	O
its	O
firing	O
in	O
the	O
case	O
of	O
inhibitory	O
synaptic	O
input	O
this	O
process	O
can	O
take	O
several	O
tens	O
of	O
milliseconds	O
according	O
to	O
and	O
though	O
the	O
input	O
to	O
a	O
critic	O
and	O
actor	O
unit	O
instantaneously	O
produces	O
the	O
unit	O
s	O
output	O
ignoring	O
activation	O
time	O
like	O
this	O
is	O
common	O
in	O
abstract	O
models	O
of	O
hebbian-style	O
plasticity	O
in	O
which	O
synaptic	O
efficacies	O
change	O
according	O
to	O
a	O
simple	O
product	O
of	O
simultaneous	O
pre-	O
and	O
postsynaptic	O
activity	O
more	O
realistic	O
models	O
must	O
take	O
activation	O
time	O
into	O
account	O
activation	O
time	O
is	O
especially	O
important	O
for	O
a	O
more	O
realistic	O
actor	O
unit	O
because	O
it	O
influences	O
how	O
contingent	O
eligibility	B
traces	I
have	O
to	O
work	O
in	O
order	O
to	O
properly	O
apportion	O
credit	O
defining	O
contingent	O
eligibility	B
traces	I
for	O
the	O
actor	O
unit	O
s	O
learning	O
rule	O
given	O
above	O
in	O
works	O
because	O
by	O
ignoring	O
activation	O
time	O
the	O
presynaptic	O
activity	O
xst	O
participates	O
for	O
reinforcement	O
to	O
the	O
appropriate	O
synapses	O
the	O
cludes	O
the	O
postsynaptic	O
and	O
the	O
presynaptic	O
factor	O
xst	O
this	O
in	O
causing	O
the	O
postsynaptic	O
activity	O
appearing	O
in	O
to	O
assign	O
credit	O
for	O
reinforcement	O
correctly	O
the	O
presynaptic	O
factor	O
defining	O
the	O
eligibility	O
trace	O
must	O
be	O
a	O
cause	O
of	O
the	O
postsynaptic	O
factor	O
that	O
also	O
defines	O
the	O
trace	O
contingent	O
eligibility	B
traces	I
for	O
a	O
more	O
realistic	O
actor	O
unit	O
would	O
have	O
to	O
take	O
activation	O
time	O
into	O
account	O
time	O
should	O
not	O
be	O
confused	O
with	O
the	O
time	O
required	O
for	O
a	O
neuron	O
to	O
receive	O
a	O
reinforcement	B
signal	I
influenced	O
by	O
that	O
neuron	O
s	O
activity	O
the	O
function	O
of	O
eligibility	B
traces	I
is	O
to	O
span	O
this	O
time	O
interval	O
which	O
is	O
generally	O
much	O
longer	O
than	O
the	O
activation	O
time	O
we	O
discuss	O
this	O
further	O
in	O
the	O
following	O
section	O
there	O
are	O
hints	O
from	O
neuroscience	B
for	O
how	O
this	O
process	O
might	O
work	O
in	O
the	O
brain	O
neuroscientists	O
have	O
discovered	O
a	O
form	O
of	O
hebbian	B
plasticity	O
called	O
spike-timing-dependent	O
plasticity	O
that	O
lends	O
plausibility	O
to	O
the	O
existence	O
of	O
actor-like	O
synaptic	B
plasticity	I
in	O
the	O
brain	O
stdp	O
is	O
a	O
hebbian-style	O
plasticity	O
but	O
changes	O
in	O
a	O
synapse	O
s	O
efficacy	O
depend	O
on	O
the	O
relative	O
timing	O
of	O
presynaptic	O
and	O
postsynaptic	O
action	B
potentials	O
the	O
dependence	O
can	O
take	O
different	O
forms	O
but	O
in	O
the	O
one	O
most	O
studied	O
a	O
synapse	O
increases	O
in	O
strength	O
if	O
spikes	O
incoming	O
via	O
that	O
synapse	O
arrive	O
shortly	O
before	O
the	O
postsynaptic	O
neuron	O
fires	O
if	O
the	O
timing	O
relation	O
is	O
reversed	O
with	O
a	O
presynaptic	O
spike	O
arriving	O
shortly	O
after	O
the	O
postsynaptic	O
neuron	O
fires	O
then	O
the	O
strength	O
of	O
the	O
synapse	O
decreases	O
stdp	O
is	O
a	O
type	O
of	O
hebbian	B
plasticity	O
that	O
takes	O
the	O
activation	O
time	O
of	O
a	O
neuron	O
into	O
account	O
which	O
is	O
one	O
of	O
the	O
ingredients	O
needed	O
for	O
actor-like	O
learning	O
chapter	O
neuroscience	B
the	O
discovery	O
of	O
stdp	O
has	O
led	O
neuroscientists	O
to	O
investigate	O
the	O
possibility	O
of	O
a	O
threefactor	O
form	O
of	O
stdp	O
in	O
which	O
neuromodulatory	O
input	O
must	O
follow	O
appropriately-timed	O
pre-	O
and	O
postsynaptic	O
spikes	O
this	O
form	O
of	O
synaptic	B
plasticity	I
called	O
reward-modulated	O
stdp	O
is	O
much	O
like	O
the	O
actor	O
learning	O
rule	O
discussed	O
here	O
synaptic	O
changes	O
that	O
would	O
be	O
produced	O
by	O
regular	O
stdp	O
only	O
occur	O
if	O
there	O
is	O
neuromodulatory	O
input	O
within	O
a	O
time	O
window	O
after	O
a	O
presynaptic	O
spike	O
is	O
closely	O
followed	O
by	O
a	O
postsynaptic	O
spike	O
evidence	O
is	O
accumulating	B
that	O
reward-modulated	O
stdp	O
occurs	O
at	O
the	O
spines	O
of	O
medium	O
spiny	O
neurons	O
of	O
the	O
dorsal	O
striatum	O
with	O
dopamine	B
providing	O
the	O
neuromodulatory	O
factor	O
the	O
sites	O
where	O
actor	O
learning	O
takes	O
place	O
in	O
the	O
hypothetical	O
neural	B
implementation	O
of	O
an	O
actor	O
critic	O
algorithm	O
illustrated	O
in	O
figure	O
experiments	O
have	O
demonstrated	O
reward-modulated	O
stdp	O
in	O
which	O
lasting	O
changes	O
in	O
the	O
efficacies	O
of	O
corticostriatal	O
synapses	O
occur	O
only	O
if	O
a	O
neuromodulatory	O
pulse	O
arrives	O
within	O
a	O
time	O
window	O
that	O
can	O
last	O
up	O
to	O
seconds	O
after	O
a	O
presynaptic	O
spike	O
is	O
closely	O
followed	O
by	O
a	O
postsynaptic	O
spike	O
et	O
al	O
although	O
the	O
evidence	O
is	O
indirect	O
these	O
experiments	O
point	O
to	O
the	O
existence	O
of	O
contingent	O
eligibility	B
traces	I
having	O
prolonged	O
time	O
courses	O
the	O
molecular	O
mechanisms	O
producing	O
these	O
traces	O
as	O
well	O
as	O
the	O
much	O
shorter	O
traces	O
that	O
likely	O
underly	O
stdp	O
are	O
not	O
yet	O
understood	O
but	O
research	O
focusing	O
on	O
time-dependent	O
and	O
neuromodulator-dependent	O
synaptic	B
plasticity	I
is	O
continuing	O
the	O
neuron-like	O
actor	O
unit	O
that	O
we	O
have	O
described	O
here	O
with	O
its	O
law-of-effect-style	O
learning	O
rule	O
appeared	O
in	O
somewhat	O
simpler	O
form	O
in	O
the	O
actor	O
critic	O
network	O
of	O
barto	O
et	O
al	O
that	O
network	O
was	O
inspired	O
by	O
the	O
hedonistic	O
neuron	O
hypothesis	O
proposed	O
by	O
physiologist	O
a	O
h	O
klopf	B
not	O
all	O
the	O
details	O
of	O
klopf	B
s	O
hypothesis	O
are	O
consistent	O
with	O
what	O
has	O
been	O
learned	O
about	O
synaptic	B
plasticity	I
but	O
the	O
discovery	O
of	O
stdp	O
and	O
the	O
growing	O
evidence	O
for	O
a	O
reward-modulated	O
form	O
of	O
stdp	O
suggest	O
that	O
klopf	B
s	O
ideas	O
may	O
not	O
have	O
been	O
far	O
off	O
the	O
mark	O
we	O
discuss	O
klopf	B
s	O
hedonistic	O
neuron	O
hypothesis	O
next	O
hedonistic	B
neurons	I
in	O
his	O
hedonistic	O
neuron	O
hypothesis	O
klopf	B
conjectured	O
that	O
individual	O
neurons	O
seek	O
to	O
maximize	O
the	O
difference	O
between	O
synaptic	O
input	O
treated	O
as	O
rewarding	O
and	O
synaptic	O
input	O
treated	O
as	O
punishing	O
by	O
adjusting	O
the	O
efficacies	O
of	O
their	O
synapses	O
on	O
the	O
basis	O
of	O
rewarding	O
or	O
punishing	O
consequences	O
of	O
their	O
own	O
action	B
potentials	O
in	O
other	O
words	O
individual	O
neurons	O
can	O
be	O
trained	O
with	O
response-contingent	O
reinforcement	O
like	O
an	O
animal	O
can	O
be	O
trained	O
in	O
an	O
instrumental	O
conditioning	O
task	O
his	O
hypothesis	O
included	O
the	O
idea	O
that	O
rewards	O
and	O
punishments	O
are	O
conveyed	O
to	O
a	O
neuron	O
via	O
the	O
same	O
synaptic	O
input	O
that	O
excites	O
or	O
inhibits	O
the	O
neuron	O
s	O
spike-generating	O
activity	O
klopf	B
known	O
what	O
we	O
know	O
today	O
about	O
neuromodulatory	O
systems	O
he	O
might	O
have	O
assigned	O
the	O
reinforcing	O
role	O
to	O
neuromodulatory	O
input	O
but	O
he	O
wanted	O
to	O
avoid	O
any	O
centralized	O
source	O
of	O
training	O
information	O
synaptically-local	O
traces	O
of	O
past	O
pre-	O
and	O
postsynaptic	O
activity	O
had	O
the	O
key	O
function	O
in	O
klopf	B
s	O
hypothesis	O
of	O
making	O
synapses	O
eligible	O
the	O
term	O
he	O
introduced	O
for	O
modification	O
by	O
later	O
reward	O
or	O
punishment	O
he	O
conjectured	O
that	O
these	O
traces	O
are	O
implemented	O
by	O
molecular	O
mechanisms	O
local	O
to	O
each	O
synapse	O
and	O
therefore	O
different	O
from	O
the	O
electrical	O
activity	O
of	O
both	O
the	O
pre-	O
and	O
the	O
postsynaptic	O
neurons	O
in	O
hedonistic	B
neurons	I
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
of	O
this	O
chapter	O
we	O
bring	O
attention	O
to	O
some	O
similar	O
proposals	O
made	O
by	O
others	O
klopf	B
specifically	O
conjectured	O
that	O
synaptic	O
efficacies	O
change	O
in	O
the	O
following	O
way	O
when	O
a	O
neuron	O
fires	O
an	O
action	B
potential	O
all	O
of	O
its	O
synapses	O
that	O
were	O
active	O
in	O
contributing	O
to	O
that	O
action	B
potential	O
become	O
eligible	O
to	O
undergo	O
changes	O
in	O
their	O
efficacies	O
if	O
the	O
action	B
potential	O
is	O
followed	O
within	O
an	O
appropriate	O
time	O
period	O
by	O
an	O
increase	O
of	O
reward	O
the	O
efficacies	O
of	O
all	O
the	O
eligible	O
synapses	O
increase	O
symmetrically	O
if	O
the	O
action	B
potential	O
is	O
followed	O
within	O
an	O
appropriate	O
time	O
period	O
by	O
an	O
increase	O
of	O
punishment	O
the	O
efficacies	O
of	O
eligible	O
synapses	O
decrease	O
this	O
is	O
implemented	O
by	O
triggering	O
an	O
eligibility	O
trace	O
at	O
a	O
synapse	O
upon	O
a	O
coincidence	O
of	O
presynaptic	O
and	O
postsynaptic	O
activity	O
more	O
exactly	O
upon	O
pairing	O
of	O
presynaptic	O
activity	O
with	O
the	O
postsynaptic	O
activity	O
that	O
that	O
presynaptic	O
activity	O
participates	O
in	O
causing	O
what	O
we	O
call	O
a	O
contingent	O
eligibility	O
trace	O
this	O
is	O
essentially	O
the	O
three-factor	O
learning	O
rule	O
of	O
an	O
actor	O
unit	O
described	O
in	O
the	O
previous	O
section	O
the	O
shape	O
and	O
time	O
course	O
of	O
an	O
eligibility	O
trace	O
in	O
klopf	B
s	O
theory	O
reflects	O
the	O
durations	O
of	O
the	O
many	O
feedback	O
loops	O
in	O
which	O
the	O
neuron	O
is	O
embedded	O
some	O
of	O
which	O
lie	O
entirely	O
within	O
the	O
brain	O
and	O
body	O
of	O
the	O
organism	O
while	O
others	O
extend	O
out	O
through	O
the	O
organism	O
s	O
external	O
environment	B
as	O
mediated	O
by	O
its	O
motor	O
and	O
sensory	O
systems	O
his	O
idea	O
was	O
that	O
the	O
shape	O
of	O
a	O
synaptic	O
eligibility	O
trace	O
is	O
like	O
a	O
histogram	O
of	O
the	O
durations	O
of	O
the	O
feedback	O
loops	O
in	O
which	O
the	O
neuron	O
is	O
embedded	O
the	O
peak	O
of	O
an	O
eligibility	O
trace	O
would	O
then	O
occur	O
at	O
the	O
duration	O
of	O
the	O
most	O
prevalent	O
feedback	O
loops	O
in	O
which	O
that	O
neuron	O
participates	O
the	O
eligibility	B
traces	I
used	O
by	O
algorithms	O
described	O
in	O
this	O
book	O
are	O
simplified	O
versions	O
of	O
klopf	B
s	O
original	O
idea	O
being	O
exponentially	O
geometrically	O
decreasing	O
functions	O
controlled	O
by	O
the	O
parameters	O
and	O
this	O
simplifies	O
simulations	O
as	O
well	O
as	O
theory	O
but	O
we	O
regard	O
these	O
simple	O
eligibility	B
traces	I
as	O
a	O
placeholders	O
for	O
traces	O
closer	O
to	O
klopf	B
s	O
original	O
conception	O
which	O
would	O
have	O
computational	O
advantages	O
in	O
complex	O
reinforcement	B
learning	I
systems	O
by	O
refining	O
the	O
credit-assignment	O
process	O
klopf	B
s	O
hedonistic	O
neuron	O
hypothesis	O
is	O
not	O
as	O
implausible	O
as	O
it	O
may	O
at	O
first	O
appear	O
a	O
well-studied	O
example	O
of	O
a	O
single	O
cell	O
that	O
seeks	O
some	O
stimuli	O
and	O
avoids	O
others	O
is	O
the	O
bacterium	O
escherichia	O
coli	O
the	O
movement	O
of	O
this	O
single-cell	O
organism	O
is	O
influenced	O
by	O
chemical	O
stimuli	O
in	O
its	O
environment	B
behavior	O
known	O
as	O
chemotaxis	O
it	O
swims	O
in	O
its	O
liquid	O
environment	B
by	O
rotating	O
hairlike	O
structures	O
called	O
flagella	O
attached	O
to	O
its	O
surface	O
it	O
rotates	O
them	O
molecules	O
in	O
the	O
bacterium	O
s	O
environment	B
bind	O
to	O
receptors	O
on	O
its	O
surface	O
binding	O
events	O
modulate	O
the	O
frequency	O
with	O
which	O
the	O
bacterium	O
reverses	O
flagellar	O
rotation	O
each	O
reversal	O
causes	O
the	O
bacterium	O
to	O
tumble	O
in	O
place	O
and	O
then	O
head	O
off	O
in	O
a	O
random	O
new	O
direction	O
a	O
little	O
chemical	O
memory	O
and	O
computation	O
causes	O
the	O
frequency	O
of	O
flagellar	O
reversal	O
to	O
decrease	O
when	O
the	O
bacterium	O
swims	O
toward	O
higher	O
concentrations	O
of	O
molecules	O
it	O
needs	O
to	O
survive	O
and	O
increase	O
when	O
the	O
bacterium	O
swims	O
toward	O
higher	O
concentrations	O
of	O
molecules	O
that	O
are	O
harmful	O
the	O
result	O
is	O
that	O
the	O
bacterium	O
tends	O
to	O
persist	O
in	O
swimming	O
up	O
attractant	O
gradients	O
and	O
tends	O
to	O
avoid	O
swimming	O
up	O
repellant	O
gradients	O
the	O
chemotactic	O
behavior	O
just	O
described	O
is	O
called	O
klinokinesis	O
it	O
is	O
a	O
kind	O
of	O
trialand-error	O
behavior	O
although	O
it	O
is	O
unlikely	O
that	O
learning	O
is	O
involved	O
the	O
bacterium	O
needs	O
a	O
modicum	O
of	O
short-term	O
memory	O
to	O
detect	O
molecular	O
concentration	O
gradients	O
but	O
it	O
chapter	O
neuroscience	B
probably	O
does	O
not	O
maintain	O
long-term	O
memories	O
artificial	B
intelligence	I
pioneer	O
oliver	O
selfridge	O
called	O
this	O
strategy	O
run	O
and	O
twiddle	O
pointing	O
out	O
its	O
utility	O
as	O
a	O
basic	O
adaptive	O
strategy	O
keep	O
going	O
in	O
the	O
same	O
way	O
if	O
things	O
are	O
getting	O
better	O
and	O
otherwise	O
move	O
around	O
similarly	O
one	O
might	O
think	O
of	O
a	O
neuron	O
swimming	O
literally	O
of	O
course	O
in	O
a	O
medium	O
composed	O
of	O
the	O
complex	O
collection	O
of	O
feedback	O
loops	O
in	O
which	O
it	O
is	O
embedded	O
acting	O
to	O
obtain	O
one	O
type	O
of	O
input	O
signal	O
and	O
to	O
avoid	O
others	O
unlike	O
the	O
bacterium	O
however	O
the	O
neuron	O
s	O
synaptic	O
strengths	O
retain	O
information	O
about	O
its	O
past	O
trial-and-error	O
behavior	O
if	O
this	O
view	O
of	O
the	O
behavior	O
of	O
a	O
neuron	O
just	O
one	O
type	O
of	O
neuron	O
is	O
plausible	O
then	O
the	O
closed-loop	O
nature	O
of	O
how	O
the	O
neuron	O
interacts	O
with	O
its	O
environment	B
is	O
important	O
for	O
understanding	O
its	O
behavior	O
where	O
the	O
neuron	O
s	O
environment	B
consists	O
of	O
the	O
rest	O
of	O
the	O
animal	O
together	O
with	O
the	O
environment	B
with	O
which	O
the	O
animal	O
as	O
a	O
whole	O
interacts	O
klopf	B
s	O
hedonistic	O
neuron	O
hypothesis	O
extended	O
beyond	O
the	O
idea	O
that	O
individual	O
neurons	O
are	O
reinforcement	B
learning	I
agents	O
he	O
argued	O
that	O
many	O
aspects	O
of	O
intelligent	O
behavior	O
can	O
be	O
understood	O
as	O
the	O
result	O
of	O
the	O
collective	O
behavior	O
of	O
a	O
population	O
of	O
self-interested	O
hedonistic	B
neurons	I
interacting	O
with	O
one	O
another	O
in	O
an	O
immense	O
society	O
or	O
economic	O
system	O
making	O
up	O
an	O
animal	O
s	O
nervous	O
system	O
whether	O
or	O
not	O
this	O
view	O
of	O
nervous	O
systems	O
is	O
useful	O
the	O
collective	O
behavior	O
of	O
reinforcement	B
learning	I
agents	O
has	O
implications	O
for	O
neuroscience	B
we	O
take	O
up	O
this	O
subject	O
next	O
collective	B
reinforcement	B
learning	I
the	O
behavior	O
of	O
populations	O
of	O
reinforcement	B
learning	I
agents	O
is	O
deeply	O
relevant	O
to	O
the	O
study	O
of	O
social	O
and	O
economic	O
systems	O
and	O
if	O
anything	O
like	O
klopf	B
s	O
hedonistic	O
neuron	O
hypothesis	O
is	O
correct	O
to	O
neuroscience	B
as	O
well	O
the	O
hypothesis	O
described	O
above	O
about	O
how	O
an	O
actor	O
critic	O
algorithm	O
might	O
be	O
implemented	O
in	O
the	O
brain	O
only	O
narrowly	O
addresses	O
the	O
implications	O
of	O
the	O
fact	O
that	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
the	O
respective	O
locations	O
of	O
the	O
actor	O
and	O
the	O
critic	O
according	O
to	O
the	O
hypothesis	O
each	O
contain	O
millions	O
of	O
medium	O
spiny	O
neurons	O
whose	O
synapses	O
undergo	O
change	O
modulated	O
by	O
phasic	O
bursts	O
of	O
dopamine	B
neuron	O
activity	O
the	O
actor	O
in	O
figure	O
is	O
a	O
single-layer	O
network	O
of	O
k	O
actor	O
units	O
the	O
actions	O
produced	O
by	O
this	O
network	O
are	O
vectors	O
presumed	O
to	O
drive	O
the	O
animal	O
s	O
behavior	O
changes	O
in	O
the	O
efficacies	O
of	O
the	O
synapses	O
of	O
all	O
of	O
these	O
units	O
depend	O
on	O
the	O
reinforcement	B
signal	I
because	O
actor	O
units	O
attempt	O
to	O
make	O
as	O
large	O
as	O
possible	O
effectively	O
acts	O
as	O
a	O
reward	B
signal	I
for	O
them	O
in	O
this	O
case	O
reinforcement	O
is	O
the	O
same	O
as	O
reward	O
thus	O
each	O
actor	O
unit	O
is	O
itself	O
a	O
reinforcement	B
learning	I
agent	O
a	O
hedonistic	O
neuron	O
if	O
you	O
will	O
now	O
to	O
make	O
the	O
situation	O
as	O
simple	O
as	O
possible	O
assume	O
that	O
each	O
of	O
these	O
units	O
receives	O
the	O
same	O
reward	B
signal	I
at	O
the	O
same	O
time	O
as	O
indicated	O
above	O
the	O
assumption	O
that	O
dopamine	B
is	O
released	O
at	O
all	O
the	O
corticostriatal	O
synapses	O
under	O
the	O
same	O
conditions	O
and	O
at	O
the	O
same	O
times	O
is	O
likely	O
an	O
oversimplification	O
what	O
can	O
reinforcement	B
learning	I
theory	O
tell	O
us	O
about	O
what	O
happens	O
when	O
all	O
members	O
of	O
a	O
population	O
of	O
reinforcement	B
learning	I
agents	O
learn	O
according	O
to	O
a	O
common	O
reward	B
signal	I
the	O
field	O
of	O
multi-agent	O
reinforcement	B
learning	I
considers	O
many	O
aspects	O
of	O
learning	O
by	O
populations	O
of	O
reinforcement	B
learning	I
agents	O
although	O
this	O
field	O
is	O
beyond	O
the	O
collective	B
reinforcement	B
learning	I
scope	O
of	O
this	O
book	O
we	O
believe	O
that	O
some	O
of	O
its	O
basic	O
concepts	O
and	O
results	O
are	O
relevant	O
to	O
thinking	O
about	O
the	O
brain	O
s	O
diffuse	O
neuromodulatory	O
systems	O
in	O
multi-agent	O
reinforcement	B
learning	I
in	O
game	B
theory	I
the	O
scenario	O
in	O
which	O
all	O
the	O
agents	O
try	O
to	O
maximize	O
a	O
common	O
reward	B
signal	I
that	O
they	O
simultaneously	O
receive	O
is	O
known	O
as	O
a	O
cooperative	O
game	O
or	O
a	O
team	O
problem	O
what	O
makes	O
a	O
team	O
problem	O
interesting	O
and	O
challenging	O
is	O
that	O
the	O
common	O
reward	B
signal	I
sent	O
to	O
each	O
agent	O
evaluates	O
the	O
pattern	O
of	O
activity	O
produced	O
by	O
the	O
entire	O
population	O
that	O
is	O
it	O
evaluates	O
the	O
collective	O
action	B
of	O
the	O
team	O
members	O
this	O
means	O
that	O
any	O
individual	O
agent	O
has	O
only	O
limited	O
ability	O
to	O
affect	O
the	O
reward	B
signal	I
because	O
any	O
single	O
agent	O
contributes	O
just	O
one	O
component	O
of	O
the	O
collective	O
action	B
evaluated	O
by	O
the	O
common	O
reward	B
signal	I
effective	O
learning	O
in	O
this	O
scenario	O
requires	O
addressing	O
a	O
structural	B
credit	B
assignment	I
problem	O
which	O
team	O
members	O
or	O
groups	O
of	O
team	O
members	O
deserve	O
credit	O
for	O
a	O
favorable	O
reward	B
signal	I
or	O
blame	O
for	O
an	O
unfavorable	O
reward	B
signal	I
it	O
is	O
a	O
cooperative	O
game	O
or	O
a	O
team	O
problem	O
because	O
the	O
agents	O
are	O
united	O
in	O
seeking	O
to	O
increase	O
the	O
same	O
reward	B
signal	I
there	O
are	O
no	O
conflicts	O
of	O
interest	O
among	O
the	O
agents	O
the	O
scenario	O
would	O
be	O
a	O
competitive	O
game	O
if	O
different	O
agents	O
receive	O
different	O
reward	O
signals	O
where	O
each	O
reward	B
signal	I
again	O
evaluates	O
the	O
collective	O
action	B
of	O
the	O
population	O
and	O
the	O
objective	O
of	O
each	O
agent	O
is	O
to	O
increase	O
its	O
own	O
reward	B
signal	I
in	O
this	O
case	O
there	O
might	O
be	O
conflicts	O
of	O
interest	O
among	O
the	O
agents	O
meaning	O
that	O
actions	O
that	O
are	O
good	O
for	O
some	O
agents	O
are	O
bad	O
for	O
others	O
even	O
deciding	O
what	O
the	O
best	O
collective	O
action	B
should	O
be	O
is	O
a	O
non-trivial	O
aspect	O
of	O
game	B
theory	I
this	O
competitive	O
setting	O
might	O
be	O
relevant	O
to	O
neuroscience	B
too	O
example	O
to	O
account	O
for	O
heterogeneity	O
of	O
dopamine	B
neuron	O
activity	O
but	O
here	O
we	O
focus	O
only	O
on	O
the	O
cooperative	O
or	O
team	O
case	O
how	O
can	O
each	O
reinforcement	B
learning	I
agent	O
in	O
a	O
team	O
learn	O
to	O
do	O
the	O
right	O
thing	O
so	O
that	O
the	O
collective	O
action	B
of	O
the	O
team	O
is	O
highly	O
rewarded	O
an	O
interesting	O
result	O
is	O
that	O
if	O
each	O
agent	O
can	O
learn	O
effectively	O
despite	O
its	O
reward	B
signal	I
being	O
corrupted	O
by	O
a	O
large	O
amount	O
of	O
noise	O
and	O
despite	O
its	O
lack	O
of	O
access	O
to	O
complete	O
state	B
information	O
then	O
the	O
population	O
as	O
a	O
whole	O
will	O
learn	O
to	O
produce	O
collective	O
actions	O
that	O
improve	O
as	O
evaluated	O
by	O
the	O
common	O
reward	B
signal	I
even	O
when	O
the	O
agents	O
cannot	O
communicate	O
with	O
one	O
another	O
each	O
agent	O
faces	O
its	O
own	O
reinforcement	B
learning	I
task	O
in	O
which	O
its	O
influence	O
on	O
the	O
reward	B
signal	I
is	O
deeply	O
buried	O
in	O
the	O
noise	O
created	O
by	O
the	O
influences	O
of	O
other	O
agents	O
in	O
fact	O
for	O
any	O
agent	O
all	O
the	O
other	O
agents	O
are	O
part	O
of	O
its	O
environment	B
because	O
its	O
input	O
both	O
the	O
part	O
conveying	O
state	B
information	O
and	O
the	O
reward	O
part	O
depends	O
on	O
how	O
all	O
the	O
other	O
agents	O
are	O
behaving	O
furthermore	O
lacking	O
access	O
to	O
the	O
actions	O
of	O
the	O
other	O
agents	O
indeed	O
lacking	O
access	O
to	O
the	O
parameters	O
determining	O
their	O
policies	O
each	O
agent	O
can	O
only	O
partially	O
observe	O
the	O
state	B
of	O
its	O
environment	B
this	O
makes	O
each	O
team	O
member	O
s	O
learning	O
task	O
very	O
difficult	O
but	O
if	O
each	O
uses	O
a	O
reinforcement	B
learning	I
algorithm	O
able	O
to	O
increase	O
a	O
reward	B
signal	I
even	O
under	O
these	O
difficult	O
conditions	O
teams	O
of	O
reinforcement	B
learning	I
agents	O
can	O
learn	O
to	O
produce	O
collective	O
actions	O
that	O
improve	O
over	O
time	O
as	O
evaluated	O
by	O
the	O
team	O
s	O
common	O
reward	B
signal	I
if	O
the	O
team	O
members	O
are	O
neuron-like	O
units	O
then	O
each	O
unit	O
has	O
to	O
have	O
the	O
goal	O
of	O
increasing	O
the	O
amount	O
of	O
reward	O
it	O
receives	O
over	O
time	O
as	O
the	O
actor	O
unit	O
does	O
that	O
we	O
described	O
in	O
section	O
each	O
unit	O
s	O
learning	O
algorithm	O
has	O
to	O
have	O
two	O
essential	O
features	O
first	O
it	O
has	O
to	O
use	O
contingent	O
eligibility	B
traces	I
recall	O
that	O
a	O
contingent	O
eligibility	O
chapter	O
neuroscience	B
trace	O
in	O
neural	B
terms	O
is	O
initiated	O
increased	O
at	O
a	O
synapse	O
when	O
its	O
presynaptic	O
input	O
participates	O
in	O
causing	O
the	O
postsynaptic	O
neuron	O
to	O
fire	O
a	O
non-contingent	O
eligibility	O
trace	O
in	O
contrast	O
is	O
initiated	O
or	O
increased	O
by	O
presynaptic	O
input	O
independently	O
of	O
what	O
the	O
postsynaptic	O
neuron	O
does	O
as	O
explained	O
in	O
section	O
by	O
keeping	O
information	O
about	O
what	O
actions	O
were	O
taken	O
in	O
what	O
states	O
contingent	O
eligibility	B
traces	I
allow	O
credit	O
for	O
reward	O
or	O
blame	O
for	O
punishment	O
to	O
be	O
apportioned	O
to	O
an	O
agent	O
s	O
policy	B
parameters	O
according	O
to	O
the	O
contribution	O
the	O
values	O
of	O
these	O
parameters	O
made	O
in	O
determining	O
the	O
agent	O
s	O
action	B
by	O
similar	O
reasoning	O
a	O
team	O
member	O
must	O
remember	O
its	O
recent	O
action	B
so	O
that	O
it	O
can	O
either	O
increase	O
or	O
decrease	O
the	O
likelihood	O
of	O
producing	O
that	O
action	B
according	O
to	O
the	O
reward	B
signal	I
that	O
is	O
subsequently	O
received	O
the	O
action	B
component	O
of	O
a	O
contingent	O
eligibility	O
trace	O
implements	O
this	O
action	B
memory	O
because	O
of	O
the	O
complexity	O
of	O
the	O
learning	O
task	O
however	O
contingent	O
eligibility	O
is	O
merely	O
a	O
preliminary	O
step	O
in	O
the	O
credit	B
assignment	I
process	O
the	O
relationship	O
between	O
a	O
single	O
team	O
member	O
s	O
action	B
and	O
changes	O
in	O
the	O
team	O
s	O
reward	B
signal	I
is	O
a	O
statistical	O
correlation	O
that	O
has	O
to	O
be	O
estimated	O
over	O
many	O
trials	O
contingent	O
eligibility	O
is	O
an	O
essential	O
but	O
preliminary	O
step	O
in	O
this	O
process	O
learning	O
with	O
non-contingent	O
eligibility	B
traces	I
does	O
not	O
work	O
at	O
all	O
in	O
the	O
team	O
setting	O
because	O
it	O
does	O
not	O
provide	O
a	O
way	O
to	O
correlate	O
actions	O
with	O
consequent	O
changes	O
in	O
the	O
reward	B
signal	I
non-contingent	O
eligibility	B
traces	I
are	O
adequate	O
for	O
learning	O
to	O
predict	O
as	O
the	O
critic	O
component	O
of	O
the	O
actor	O
critic	O
algorithm	O
does	O
but	O
they	O
do	O
not	O
support	O
learning	O
to	O
control	B
as	O
the	O
actor	O
component	O
must	O
do	O
the	O
members	O
of	O
a	O
population	O
of	O
critic-like	O
agents	O
may	O
still	O
receive	O
a	O
common	O
reinforcement	B
signal	I
but	O
they	O
would	O
all	O
learn	O
to	O
predict	O
the	O
same	O
quantity	O
in	O
the	O
case	O
of	O
an	O
actor	O
critic	O
method	O
would	O
be	O
the	O
expected	B
return	B
for	O
the	O
current	O
policy	B
how	O
successful	O
each	O
member	O
of	O
the	O
population	O
would	O
be	O
in	O
learning	O
to	O
predict	O
the	O
expected	B
return	B
would	O
depend	O
on	O
the	O
information	O
it	O
receives	O
which	O
could	O
be	O
very	O
different	O
for	O
different	O
members	O
of	O
the	O
population	O
there	O
would	O
be	O
no	O
need	O
for	O
the	O
population	O
to	O
produce	O
differentiated	O
patterns	O
of	O
activity	O
this	O
is	O
not	O
a	O
team	O
problem	O
as	O
defined	O
here	O
a	O
second	O
requirement	O
for	O
collective	O
learning	O
in	O
a	O
team	O
problem	O
is	O
that	O
there	O
has	O
to	O
be	O
variability	O
in	O
the	O
actions	O
of	O
the	O
team	O
members	O
in	O
order	O
for	O
the	O
team	O
to	O
explore	O
the	O
space	O
of	O
collective	O
actions	O
the	O
simplest	O
way	O
for	O
a	O
team	O
of	O
reinforcement	B
learning	I
agents	O
to	O
do	O
this	O
is	O
for	O
each	O
member	O
to	O
independently	O
explore	O
its	O
own	O
action	B
space	O
through	O
persistent	O
variability	O
in	O
its	O
output	O
this	O
will	O
cause	O
the	O
team	O
as	O
a	O
whole	O
to	O
vary	O
its	O
collective	O
actions	O
for	O
example	O
a	O
team	O
of	O
the	O
actor	O
units	O
described	O
in	O
section	O
explores	O
the	O
space	O
of	O
collective	O
actions	O
because	O
the	O
output	O
of	O
each	O
unit	O
being	O
a	O
bernoulli-logistic	O
unit	O
probabilistically	O
depends	O
on	O
the	O
weighted	O
sum	O
of	O
its	O
input	O
vector	B
s	O
components	O
the	O
weighted	O
sum	O
biases	O
firing	O
probability	O
up	O
or	O
down	O
but	O
there	O
is	O
always	O
variability	O
because	O
each	O
unit	O
uses	O
a	O
reinforce	B
policy	B
gradient	B
algorithm	O
each	O
unit	O
adjusts	O
its	O
weights	O
with	O
the	O
goal	O
of	O
maximizing	O
the	O
average	O
reward	O
rate	O
it	O
experiences	O
while	O
stochastically	O
exploring	O
its	O
own	O
action	B
space	O
one	O
can	O
show	O
as	O
williams	O
did	O
that	O
a	O
team	O
of	O
bernoulli-logistic	O
reinforce	B
units	O
implements	O
a	O
policy	B
gradient	B
algorithm	O
as	O
a	O
whole	O
with	O
respect	O
to	O
average	O
rate	O
of	O
the	O
team	O
s	O
common	O
reward	B
signal	I
where	O
the	O
actions	O
are	O
the	O
collective	O
actions	O
of	O
the	O
team	O
further	O
williams	O
showed	O
that	O
a	O
team	O
of	O
bernoulli-logistic	O
units	O
using	O
reinforce	B
ascends	O
the	O
average	O
reward	O
gradient	B
when	O
the	O
units	O
in	O
the	O
team	O
are	O
intercon	O
model-based	O
methods	O
in	O
the	O
brain	O
nected	O
to	O
form	O
a	O
multilayer	O
neural	B
network	O
in	O
this	O
case	O
the	O
reward	B
signal	I
is	O
broadcast	O
to	O
all	O
the	O
units	O
in	O
the	O
network	O
though	O
reward	O
may	O
depend	O
only	O
on	O
the	O
collective	O
actions	O
of	O
the	O
network	O
s	O
output	O
units	O
this	O
means	O
that	O
a	O
multilayer	O
team	O
of	O
bernoulli-logistic	O
reinforce	B
units	O
learns	O
like	O
a	O
multilayer	O
network	O
trained	O
by	O
the	O
widely-used	O
error	O
backpropagation	B
method	O
but	O
in	O
this	O
case	O
the	O
backpropagation	B
process	O
is	O
replaced	O
by	O
the	O
broadcasted	O
reward	B
signal	I
in	O
practice	O
the	O
error	O
backpropagation	B
method	O
is	O
considerably	O
faster	O
but	O
the	O
reinforcement	B
learning	I
team	O
method	O
is	O
more	O
plausible	O
as	O
a	O
neural	B
mechanism	O
especially	O
in	O
light	O
of	O
what	O
is	O
being	O
learned	O
about	O
reward-modulated	O
stdp	O
as	O
discussed	O
in	O
section	O
exploration	O
through	O
independent	O
exploration	O
by	O
team	O
members	O
is	O
only	O
the	O
simplest	O
way	O
for	O
a	O
team	O
to	O
explore	O
more	O
sophisticated	O
methods	O
are	O
possible	O
if	O
the	O
team	O
members	O
coordinate	O
their	O
actions	O
to	O
focus	O
on	O
particular	O
parts	O
of	O
the	O
collective	O
action	B
space	O
either	O
by	O
communicating	O
with	O
one	O
another	O
or	O
by	O
responding	O
to	O
common	O
inputs	O
there	O
are	O
also	O
mechanisms	O
more	O
sophisticated	O
than	O
contingent	O
eligibility	B
traces	I
for	O
addressing	O
structural	B
credit	B
assignment	I
which	O
is	O
easier	O
in	O
a	O
team	O
problem	O
when	O
the	O
set	O
of	O
possible	O
collective	O
actions	O
is	O
restricted	O
in	O
some	O
way	O
an	O
extreme	O
case	O
is	O
a	O
winner-take-all	O
arrangement	O
example	O
the	O
result	O
of	O
lateral	O
inhibition	O
in	O
the	O
brain	O
that	O
restricts	O
collective	O
actions	O
to	O
those	O
to	O
which	O
only	O
one	O
or	O
a	O
few	O
team	O
members	O
contribute	O
in	O
this	O
case	O
the	O
winners	O
get	O
the	O
credit	O
or	O
blame	O
for	O
resulting	O
reward	O
or	O
punishment	O
details	O
of	O
learning	O
in	O
cooperative	O
games	O
team	O
problems	O
and	O
non-cooperative	O
game	O
problems	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
cites	O
a	O
selection	O
of	O
the	O
relevant	O
publications	O
including	O
extensive	O
references	O
to	O
research	O
on	O
implications	O
for	O
neuroscience	B
of	O
collective	B
reinforcement	B
learning	I
model-based	O
methods	O
in	O
the	O
brain	O
reinforcement	B
learning	I
s	O
distinction	O
between	O
model-free	O
and	O
model-based	O
algorithms	O
is	O
proving	O
to	O
be	O
useful	O
for	O
thinking	O
about	O
animal	O
learning	O
and	O
decision	O
processes	O
section	O
discusses	O
how	O
this	O
distinction	O
aligns	O
with	O
that	O
between	O
habitual	O
and	O
goaldirected	O
animal	O
behavior	O
the	O
hypothesis	O
discussed	O
above	O
about	O
how	O
the	O
brain	O
might	O
implement	O
an	O
actor	O
critic	O
algorithm	O
is	O
relevant	O
only	O
to	O
an	O
animal	O
s	O
habitual	O
mode	O
of	O
behavior	O
because	O
the	O
basic	O
actor	O
critic	O
method	O
is	O
model-free	O
what	O
neural	B
mechanisms	O
are	O
responsible	O
for	O
producing	O
goal-directed	O
behavior	O
and	O
how	O
do	O
they	O
interact	O
with	O
those	O
underlying	O
habitual	O
behavior	O
one	O
way	O
to	O
investigate	O
questions	O
about	O
the	O
brain	O
structures	O
involved	O
in	O
these	O
modes	O
of	O
behavior	O
is	O
to	O
inactivate	O
an	O
area	O
of	O
a	O
rat	O
s	O
brain	O
and	O
then	O
observe	O
what	O
the	O
rat	O
does	O
in	O
an	O
outcome-devaluation	O
experiment	O
results	O
from	O
experiments	O
like	O
these	O
indicate	O
that	O
the	O
actor	O
critic	O
hypothesis	O
described	O
above	O
is	O
too	O
simple	O
in	O
placing	O
the	O
actor	O
in	O
the	O
dorsal	O
striatum	O
inactivating	O
one	O
part	O
of	O
the	O
dorsal	O
striatum	O
the	O
dorsolateral	O
striatum	O
impairs	O
habit	O
learning	O
causing	O
the	O
animal	O
to	O
rely	O
more	O
on	O
goal-directed	O
processes	O
on	O
the	O
other	O
hand	O
inactivating	O
the	O
dorsomedial	O
striatum	O
impairs	O
goal-directed	O
processes	O
requiring	O
the	O
animal	O
to	O
rely	O
more	O
on	O
habit	O
learning	O
results	O
like	O
these	O
support	O
the	O
view	O
that	O
the	O
dls	O
in	O
rodents	O
is	O
more	O
involved	O
in	O
model-free	O
processes	O
chapter	O
neuroscience	B
whereas	O
their	O
dms	O
is	O
more	O
involved	O
in	O
model-based	O
processes	O
results	O
of	O
studies	O
with	O
human	O
subjects	O
in	O
similar	O
experiments	O
using	O
functional	O
neuroimaging	O
and	O
with	O
nonhuman	O
primates	O
support	O
the	O
view	O
that	O
the	O
analogous	O
structures	O
in	O
the	O
primate	O
brain	O
are	O
differentially	O
involved	O
in	O
habitual	O
and	O
goal-directed	O
modes	O
of	O
behavior	O
other	O
studies	O
identify	O
activity	O
associated	O
with	O
model-based	O
processes	O
in	O
the	O
prefrontal	O
cortex	O
of	O
the	O
human	O
brain	O
the	O
front-most	O
part	O
of	O
the	O
frontal	O
cortex	O
implicated	O
in	O
executive	O
function	O
including	O
planning	B
and	O
decision	O
making	O
specifically	O
implicated	O
is	O
the	O
orbitofrontal	O
cortex	O
the	O
part	O
of	O
the	O
prefrontal	O
cortex	O
immediately	O
above	O
the	O
eyes	O
functional	O
neuroimaging	O
in	O
humans	O
and	O
also	O
recordings	O
of	O
the	O
activities	O
of	O
single	O
neurons	O
in	O
monkeys	O
reveals	O
strong	O
activity	O
in	O
the	O
ofc	O
related	O
to	O
the	O
subjective	O
reward	O
value	B
of	O
biologically	O
significant	O
stimuli	O
as	O
well	O
as	O
activity	O
related	O
to	O
the	O
reward	O
expected	B
as	O
a	O
consequence	O
of	O
actions	O
although	O
not	O
free	O
of	O
controversy	O
these	O
results	O
suggest	O
significant	O
involvement	O
of	O
the	O
ofc	O
in	O
goal-directed	O
choice	O
it	O
may	O
be	O
critical	O
for	O
the	O
reward	O
part	O
of	O
an	O
animal	O
s	O
environment	B
model	O
another	O
structure	O
involved	O
in	O
model-based	O
behavior	O
is	O
the	O
hippocampus	O
a	O
structure	O
critical	O
for	O
memory	O
and	O
spatial	O
navigation	O
a	O
rat	O
s	O
hippocampus	O
plays	O
a	O
critical	O
role	O
in	O
the	O
rat	O
s	O
ability	O
to	O
navigate	O
a	O
maze	O
in	O
the	O
goal-directed	O
manner	O
that	O
led	O
tolman	B
to	O
the	O
idea	O
that	O
animals	O
use	O
models	O
or	O
cognitive	B
maps	I
in	O
selecting	O
actions	O
the	O
hippocampus	O
may	O
also	O
be	O
a	O
critical	O
component	O
of	O
our	O
human	O
ability	O
to	O
imagine	O
new	O
experiences	O
and	O
maguire	O
olafsd	O
ottir	O
barry	O
saleem	O
hassabis	O
and	O
spiers	O
the	O
findings	O
that	O
most	O
directly	O
implicate	O
the	O
hippocampus	O
in	O
planning	B
the	O
process	O
needed	O
to	O
enlist	O
an	O
environment	B
model	O
in	O
making	O
decisions	O
come	O
from	O
experiments	O
that	O
decode	O
the	O
activity	O
of	O
neurons	O
in	O
the	O
hippocampus	O
to	O
determine	O
what	O
part	O
of	O
space	O
hippocampal	O
activity	O
is	O
representing	O
on	O
a	O
moment-to-moment	O
basis	O
when	O
a	O
rat	O
pauses	O
at	O
a	O
choice	O
point	O
in	O
a	O
maze	O
the	O
representation	O
of	O
space	O
in	O
the	O
hippocampus	O
sweeps	O
forward	O
not	O
backwards	O
along	O
the	O
possible	O
paths	O
the	O
animal	O
can	O
take	O
from	O
that	O
point	O
and	O
redish	O
furthermore	O
the	O
spatial	O
trajectories	O
represented	O
by	O
these	O
sweeps	O
closely	O
correspond	O
to	O
the	O
rat	O
s	O
subsequent	O
navigational	O
behavior	O
and	O
foster	O
these	O
results	O
suggest	O
that	O
the	O
hippocampus	O
is	O
critical	O
for	O
the	O
statetransition	O
part	O
of	O
an	O
animal	O
s	O
environment	B
model	O
and	O
that	O
it	O
is	O
part	O
of	O
a	O
system	O
that	O
uses	O
the	O
model	O
to	O
simulate	O
possible	O
future	O
state	B
sequences	O
to	O
assess	O
the	O
consequences	O
of	O
possible	O
courses	O
of	O
action	B
a	O
form	O
of	O
planning	B
the	O
results	O
described	O
above	O
add	O
to	O
a	O
voluminous	O
literature	O
on	O
neural	B
mechanisms	O
underlying	O
goal-directed	O
or	O
model-based	O
learning	O
and	O
decision	O
making	O
but	O
many	O
questions	O
remain	O
unanswered	O
for	O
example	O
how	O
can	O
areas	O
as	O
structurally	O
similar	O
as	O
the	O
dls	O
and	O
dms	O
be	O
essential	O
components	O
of	O
modes	O
of	O
learning	O
and	O
behavior	O
that	O
are	O
as	O
different	O
as	O
model-free	O
and	O
model-based	O
algorithms	O
are	O
separate	O
structures	O
responsible	O
for	O
we	O
call	O
the	O
transition	O
and	O
reward	O
components	O
of	O
an	O
environment	B
model	O
is	O
all	O
planning	B
conducted	O
at	O
decision	O
time	O
via	O
simulations	O
of	O
possible	O
future	O
courses	O
of	O
action	B
as	O
the	O
forward	O
sweeping	O
activity	O
in	O
the	O
hippocampus	O
suggests	O
in	O
other	O
words	O
is	O
all	O
planning	B
something	O
like	O
a	O
rollout	O
algorithm	O
or	O
are	O
models	O
sometimes	O
engaged	O
in	O
the	O
background	O
to	O
refine	O
or	O
recompute	O
value	B
information	O
as	O
illustrated	O
by	O
the	O
dyna	B
architecture	I
how	O
does	O
the	O
brain	O
arbitrate	O
between	O
the	O
use	O
of	O
the	O
addiction	B
habit	O
and	O
goal-directed	O
systems	O
is	O
there	O
in	O
fact	O
a	O
clear	O
separation	O
between	O
the	O
neural	B
substrates	O
of	O
these	O
systems	O
the	O
evidence	O
is	O
not	O
pointing	O
to	O
a	O
positive	O
answer	O
to	O
this	O
last	O
question	O
summarizing	O
the	O
situation	O
doll	O
simon	O
and	O
daw	O
wrote	O
that	O
model-based	O
influences	O
appear	O
ubiquitous	O
more	O
or	O
less	O
wherever	O
the	O
brain	O
processes	O
reward	O
information	O
and	O
this	O
is	O
true	O
even	O
in	O
the	O
regions	O
thought	O
to	O
be	O
critical	O
for	O
model-free	O
learning	O
this	O
includes	O
the	O
dopamine	B
signals	O
themselves	O
which	O
can	O
exhibit	O
the	O
influence	O
of	O
model-based	O
information	O
in	O
addition	O
to	O
the	O
reward	O
prediction	B
errors	O
thought	O
to	O
be	O
the	O
basis	O
of	O
model-free	O
processes	O
continuing	O
neuroscience	B
research	O
informed	O
by	O
reinforcement	B
learning	I
s	O
model-free	O
and	O
model-based	O
distinction	O
has	O
the	O
potential	O
to	O
sharpen	O
our	O
understanding	O
of	O
habitual	O
and	O
goal-directed	O
processes	O
in	O
the	O
brain	O
a	O
better	O
grasp	O
of	O
these	O
neural	B
mechanisms	O
may	O
lead	O
to	O
algorithms	O
combining	O
model-free	O
and	O
model-based	O
methods	O
in	O
ways	O
that	O
have	O
not	O
yet	O
been	O
explored	O
in	O
computational	O
reinforcement	B
learning	I
addiction	B
understanding	O
the	O
neural	B
basis	O
of	O
drug	O
abuse	O
is	O
a	O
high-priority	O
goal	O
of	O
neuroscience	B
with	O
the	O
potential	O
to	O
produce	O
new	O
treatments	O
for	O
this	O
serious	O
public	O
health	O
problem	O
one	O
view	O
is	O
that	O
drug	O
craving	O
is	O
the	O
result	O
of	O
the	O
same	O
motivation	B
and	O
learning	O
processes	O
that	O
lead	O
us	O
to	O
seek	O
natural	O
rewarding	O
experiences	O
that	O
serve	O
our	O
biological	O
needs	O
addictive	O
substances	O
by	O
being	O
intensely	O
reinforcing	O
effectively	O
co-opt	O
our	O
natural	O
mechanisms	O
of	O
learning	O
and	O
decision	O
making	O
this	O
is	O
plausible	O
given	O
that	O
many	O
though	O
not	O
all	O
drugs	O
of	O
abuse	O
increase	O
levels	O
of	O
dopamine	B
either	O
directly	O
or	O
indirectly	O
in	O
regions	O
around	O
terminals	O
of	O
dopamine	B
neuron	O
axons	O
in	O
the	O
striatum	O
a	O
brain	O
structure	O
firmly	O
implicated	O
in	O
normal	O
reward-based	O
learning	O
but	O
the	O
self-destructive	O
behavior	O
associated	O
with	O
drug	O
addiction	B
is	O
not	O
characteristic	O
of	O
normal	O
learning	O
what	O
is	O
different	O
about	O
dopamine-mediated	O
learning	O
when	O
the	O
reward	O
is	O
the	O
result	O
of	O
an	O
addictive	O
drug	O
is	O
addiction	B
the	O
result	O
of	O
normal	O
learning	O
in	O
response	O
to	O
substances	O
that	O
were	O
largely	O
unavailable	O
throughout	O
our	O
evolutionary	O
history	O
so	O
that	O
evolution	B
could	O
not	O
select	O
against	O
their	O
damaging	O
effects	O
or	O
do	O
addictive	O
substances	O
somehow	O
interfere	O
with	O
normal	O
dopamine-mediated	O
learning	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
and	O
its	O
connection	O
to	O
td	B
learning	O
are	O
the	O
basis	O
of	O
a	O
model	O
due	O
to	O
redish	O
of	O
some	O
but	O
certainly	O
not	O
all	O
features	O
of	O
addiction	B
the	O
model	O
is	O
based	O
on	O
the	O
observation	O
that	O
administration	O
of	O
cocaine	O
and	O
some	O
other	O
addictive	O
drugs	O
produces	O
a	O
transient	O
increase	O
in	O
dopamine	B
in	O
the	O
model	O
this	O
dopamine	B
surge	O
is	O
assumed	O
to	O
increase	O
the	O
td	B
error	I
in	O
a	O
way	O
that	O
cannot	O
be	O
cancelled	O
out	O
by	O
changes	O
in	O
the	O
value	B
function	I
in	O
other	O
words	O
whereas	O
is	O
reduced	O
to	O
the	O
degree	O
that	O
a	O
normal	O
reward	O
is	O
predicted	O
by	O
antecedent	O
events	O
the	O
contribution	O
to	O
due	O
to	O
an	O
addictive	O
stimulus	O
does	O
not	O
decrease	O
as	O
the	O
reward	B
signal	I
becomes	O
predicted	O
drug	O
rewards	O
cannot	O
be	O
predicted	O
away	O
the	O
model	O
does	O
this	O
by	O
preventing	O
from	O
ever	O
becoming	O
negative	O
when	O
the	O
reward	B
signal	I
is	O
due	O
to	O
an	O
addictive	O
drug	O
thus	O
eliminating	O
the	O
error-correcting	O
feature	O
of	O
td	B
learning	O
for	O
states	O
associated	O
with	O
administration	O
of	O
the	O
drug	O
the	O
result	O
is	O
that	O
the	O
values	O
of	O
these	O
states	O
increase	O
without	O
bound	O
making	O
actions	O
leading	O
to	O
these	O
states	O
preferred	O
above	O
all	O
others	O
chapter	O
neuroscience	B
addictive	O
behavior	O
is	O
much	O
more	O
complicated	O
than	O
this	O
result	O
from	O
redish	O
s	O
model	O
but	O
the	O
model	O
s	O
main	O
idea	O
may	O
be	O
a	O
piece	O
of	O
the	O
puzzle	O
or	O
the	O
model	O
might	O
be	O
misleading	O
dopamine	B
appears	O
not	O
to	O
play	O
a	O
critical	O
role	O
in	O
all	O
forms	O
of	O
addiction	B
and	O
not	O
everyone	O
is	O
equally	O
susceptible	O
to	O
developing	O
addictive	O
behavior	O
moreover	O
the	O
model	O
does	O
not	O
include	O
the	O
changes	O
in	O
many	O
circuits	O
and	O
brain	O
regions	O
that	O
accompany	O
chronic	O
drug	O
taking	O
for	O
example	O
changes	O
that	O
lead	O
to	O
a	O
drug	O
s	O
diminishing	O
effect	O
with	O
repeated	O
use	O
it	O
is	O
also	O
likely	O
that	O
addiction	B
involves	O
model-based	O
processes	O
still	O
redish	O
s	O
model	O
illustrates	O
how	O
reinforcement	B
learning	I
theory	O
can	O
be	O
enlisted	O
in	O
the	O
effort	O
to	O
understand	O
a	O
major	O
health	O
problem	O
in	O
a	O
similar	O
manner	O
reinforcement	B
learning	I
theory	O
has	O
been	O
influential	O
in	O
the	O
development	O
of	O
the	O
new	O
field	O
of	O
computational	O
psychiatry	O
which	O
aims	O
to	O
improve	O
understanding	O
of	O
mental	O
disorders	O
through	O
mathematical	O
and	O
computational	O
methods	O
summary	O
the	O
neural	B
pathways	O
involved	O
in	O
the	O
brain	O
s	O
reward	O
system	O
are	O
complex	O
and	O
incompletely	O
understood	O
but	O
neuroscience	B
research	O
directed	O
toward	O
understanding	O
these	O
pathways	O
and	O
their	O
roles	O
in	O
behavior	O
is	O
progressing	O
rapidly	O
this	O
research	O
is	O
revealing	O
striking	O
correspondences	O
between	O
the	O
brain	O
s	O
reward	O
system	O
and	O
the	O
theory	O
of	O
reinforcement	B
learning	I
as	O
presented	O
in	O
this	O
book	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
was	O
proposed	O
by	O
scientists	O
who	O
recognized	O
striking	O
parallels	O
between	O
the	O
behavior	O
of	O
td	B
errors	O
and	O
the	O
activity	O
of	O
neurons	O
that	O
produce	O
dopamine	B
a	O
neurotransmitter	O
essential	O
in	O
mammals	O
for	O
reward-related	O
learning	O
and	O
behavior	O
experiments	O
conducted	O
in	O
the	O
late	O
and	O
in	O
the	O
laboratory	O
of	O
neuroscientist	O
wolfram	O
schultz	B
showed	O
that	O
dopamine	B
neurons	O
respond	O
to	O
rewarding	O
events	O
with	O
substantial	O
bursts	O
of	O
activity	O
called	O
phasic	O
responses	O
only	O
if	O
the	O
animal	O
does	O
not	O
expect	O
those	O
events	O
suggesting	O
that	O
dopamine	B
neurons	O
are	O
signaling	O
reward	O
prediction	B
errors	O
instead	O
of	O
reward	O
itself	O
further	O
these	O
experiments	O
showed	O
that	O
as	O
an	O
animal	O
learns	O
to	O
predict	O
a	O
rewarding	O
event	O
on	O
the	O
basis	O
of	O
preceding	O
sensory	O
cues	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
shifts	O
to	O
earlier	O
predictive	O
cues	O
while	O
decreasing	O
to	O
later	O
predictive	O
cues	O
this	O
parallels	O
the	O
backing-up	O
effect	O
of	O
the	O
td	B
error	I
as	O
a	O
reinforcement	B
learning	I
agent	O
learns	O
to	O
predict	O
reward	O
other	O
experimental	O
results	O
firmly	O
establish	O
that	O
the	O
phasic	O
activity	O
of	O
dopamine	B
neurons	O
is	O
a	O
reinforcement	B
signal	I
for	O
learning	O
that	O
reaches	O
multiple	O
areas	O
of	O
the	O
brain	O
by	O
means	O
of	O
profusely	O
branching	O
axons	O
of	O
dopamine	B
producing	O
neurons	O
these	O
results	O
are	O
consistent	O
with	O
the	O
distinction	O
we	O
make	O
between	O
a	O
reward	B
signal	I
rt	O
and	O
a	O
reinforcement	B
signal	I
which	O
is	O
the	O
td	B
error	I
t	O
in	O
most	O
of	O
the	O
algorithms	O
we	O
present	O
phasic	O
responses	O
of	O
dopamine	B
neurons	O
are	O
reinforcement	O
signals	O
not	O
reward	O
signals	O
a	O
prominent	O
hypothesis	O
is	O
that	O
the	O
brain	O
implements	O
something	O
like	O
an	O
actor	O
critic	O
algorithm	O
two	O
structures	O
in	O
the	O
brain	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
both	O
of	O
which	O
play	O
critical	O
roles	O
in	O
reward-based	O
learning	O
may	O
function	O
respectively	O
like	O
an	O
actor	O
and	O
a	O
critic	O
that	O
the	O
td	B
error	I
is	O
the	O
reinforcement	B
signal	I
for	O
both	O
the	O
actor	O
and	O
the	O
critic	O
fits	O
well	O
with	O
the	O
facts	O
that	O
dopamine	B
neuron	O
axons	O
target	O
both	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
that	O
dopamine	B
appears	O
to	O
be	O
critical	O
summary	O
for	O
modulating	O
synaptic	B
plasticity	I
in	O
both	O
structures	O
and	O
that	O
the	O
effect	O
on	O
a	O
target	O
structure	O
of	O
a	O
neuromodulator	O
such	O
as	O
dopamine	B
depends	O
on	O
properties	O
of	O
the	O
target	O
structure	O
and	O
not	O
just	O
on	O
properties	O
of	O
the	O
neuromodulator	O
the	O
actor	O
and	O
the	O
critic	O
can	O
be	O
implemented	O
by	O
artificial	B
neural	B
networks	I
consisting	O
of	O
neuron-like	O
units	O
having	O
learning	O
rules	O
based	O
on	O
the	O
policy-gradient	O
actor	O
critic	O
method	O
described	O
in	O
section	O
each	O
connection	O
in	O
these	O
networks	O
is	O
like	O
a	O
synapse	O
between	O
neurons	O
in	O
the	O
brain	O
and	O
the	O
learning	O
rules	O
correspond	O
to	O
rules	O
governing	O
how	O
synaptic	O
efficacies	O
change	O
as	O
functions	O
of	O
the	O
activities	O
of	O
the	O
presynaptic	O
and	O
the	O
postsynaptic	O
neurons	O
together	O
with	O
neuromodulatory	O
input	O
corresponding	O
to	O
input	O
from	O
dopamine	B
neurons	O
in	O
this	O
setting	O
each	O
synapse	O
has	O
its	O
own	O
eligibility	O
trace	O
that	O
records	O
past	O
activity	O
involving	O
that	O
synapse	O
the	O
only	O
difference	O
between	O
the	O
actor	O
and	O
critic	O
learning	O
rules	O
is	O
that	O
they	O
use	O
different	O
kinds	O
of	O
eligibility	B
traces	I
the	O
critic	O
unit	O
s	O
traces	O
are	O
noncontingent	O
because	O
they	O
do	O
not	O
involve	O
the	O
critic	O
unit	O
s	O
output	O
whereas	O
the	O
actor	O
unit	O
s	O
traces	O
are	O
contingent	O
because	O
in	O
addition	O
to	O
the	O
actor	O
unit	O
s	O
input	O
they	O
depend	O
on	O
the	O
actor	O
unit	O
s	O
output	O
in	O
the	O
hypothetical	O
implementation	O
of	O
an	O
actor	O
critic	O
system	O
in	O
the	O
brain	O
these	O
learning	O
rules	O
respectively	O
correspond	O
to	O
rules	O
governing	O
plasticity	O
of	O
corticostriatal	O
synapses	O
that	O
convey	O
signals	O
from	O
the	O
cortex	O
to	O
the	O
principal	O
neurons	O
in	O
the	O
dorsal	O
and	O
ventral	O
striatal	O
subdivisions	O
synapses	O
that	O
also	O
receive	O
inputs	O
from	O
dopamine	B
neurons	O
the	O
learning	O
rule	O
of	O
an	O
actor	O
unit	O
in	O
the	O
actor	O
critic	O
network	O
closely	O
corresponds	O
to	O
reward-modulated	O
spike-timing-dependent	O
plasticity	O
in	O
spike-timing-dependent	O
plasticity	O
the	O
relative	O
timing	O
of	O
pre-	O
and	O
postsynaptic	O
activity	O
determines	O
the	O
direction	O
of	O
synaptic	O
change	O
in	O
reward-modulated	O
stdp	O
changes	O
in	O
synapses	O
in	O
addition	O
depend	O
on	O
a	O
neuromodulator	O
such	O
as	O
dopamine	B
arriving	O
within	O
a	O
time	O
window	O
that	O
can	O
last	O
up	O
to	O
seconds	O
after	O
the	O
conditions	O
for	O
stdp	O
are	O
met	O
evidence	O
is	O
accumulating	B
that	O
reward-modulated	O
stdp	O
occurs	O
at	O
corticostriatal	O
synapses	O
where	O
the	O
actor	O
s	O
learning	O
takes	O
place	O
in	O
the	O
hypothetical	O
neural	B
implementation	O
of	O
an	O
actor	O
critic	O
system	O
adds	O
to	O
the	O
plausibility	O
of	O
the	O
hypothesis	O
that	O
something	O
like	O
an	O
actor	O
critic	O
system	O
exists	O
in	O
the	O
brains	O
of	O
some	O
animals	O
the	O
idea	O
of	O
synaptic	O
eligibility	O
and	O
basic	O
features	O
of	O
the	O
actor	O
learning	O
rule	O
derive	O
from	O
klopf	B
s	O
hypothesis	O
of	O
the	O
hedonistic	O
neuron	O
he	O
conjectured	O
that	O
individual	O
neurons	O
seek	O
to	O
obtain	O
reward	O
and	O
to	O
avoid	O
punishment	O
by	O
adjusting	O
the	O
efficacies	O
of	O
their	O
synapses	O
on	O
the	O
basis	O
of	O
rewarding	O
or	O
punishing	O
consequences	O
of	O
their	O
action	B
potentials	O
a	O
neuron	O
s	O
activity	O
can	O
affect	O
its	O
later	O
input	O
because	O
the	O
neuron	O
is	O
embedded	O
in	O
many	O
feedback	O
loops	O
some	O
within	O
the	O
animal	O
s	O
nervous	O
system	O
and	O
body	O
and	O
others	O
passing	O
through	O
the	O
animal	O
s	O
external	O
environment	B
klopf	B
s	O
idea	O
of	O
eligibility	O
is	O
that	O
synapses	O
are	O
temporarily	O
marked	O
as	O
eligible	O
for	O
modification	O
if	O
they	O
participated	O
in	O
the	O
neuron	O
s	O
firing	O
this	O
the	O
contingent	O
form	O
of	O
eligibility	O
trace	O
a	O
synapse	O
s	O
efficacy	O
is	O
modified	O
if	O
a	O
reinforcing	O
signal	O
arrives	O
while	O
the	O
synapse	O
is	O
eligible	O
we	O
alluded	O
to	O
the	O
chemotactic	O
behavior	O
of	O
a	O
bacterium	O
as	O
an	O
example	O
of	O
a	O
single	O
cell	O
that	O
directs	O
its	O
movements	O
in	O
order	O
to	O
seek	O
some	O
molecules	O
and	O
to	O
avoid	O
others	O
a	O
conspicuous	O
feature	O
of	O
the	O
dopamine	B
system	O
is	O
that	O
fibers	O
releasing	O
dopamine	B
project	O
widely	O
to	O
multiple	O
parts	O
of	O
the	O
brain	O
although	O
it	O
is	O
likely	O
that	O
only	O
some	O
populations	O
of	O
dopamine	B
neurons	O
broadcast	O
the	O
same	O
reinforcement	B
signal	I
if	O
this	O
signal	O
reaches	O
chapter	O
neuroscience	B
the	O
synapses	O
of	O
many	O
neurons	O
involved	O
in	O
actor-type	O
learning	O
then	O
the	O
situation	O
can	O
be	O
modeled	O
as	O
a	O
team	O
problem	O
in	O
this	O
type	O
of	O
problem	O
each	O
agent	O
in	O
a	O
collection	O
of	O
reinforcement	B
learning	I
agents	O
receives	O
the	O
same	O
reinforcement	B
signal	I
where	O
that	O
signal	O
depends	O
on	O
the	O
activities	O
of	O
all	O
members	O
of	O
the	O
collection	O
or	O
team	O
if	O
each	O
team	O
member	O
uses	O
a	O
sufficiently	O
capable	O
learning	O
algorithm	O
the	O
team	O
can	O
learn	O
collectively	O
to	O
improve	O
performance	O
of	O
the	O
entire	O
team	O
as	O
evaluated	O
by	O
the	O
globally-broadcast	O
reinforcement	B
signal	I
even	O
if	O
the	O
team	O
members	O
do	O
not	O
directly	O
communicate	O
with	O
one	O
another	O
this	O
is	O
consistent	O
with	O
the	O
wide	O
dispersion	O
of	O
dopamine	B
signals	O
in	O
the	O
brain	O
and	O
provides	O
a	O
neurally	O
plausible	O
alternative	O
to	O
the	O
widely-used	O
error-backpropagation	O
method	O
for	O
training	O
multilayer	O
networks	O
the	O
distinction	O
between	O
model-free	O
and	O
model-based	B
reinforcement	B
learning	I
is	O
helping	O
neuroscientists	O
investigate	O
the	O
neural	B
bases	O
of	O
habitual	O
and	O
goal-directed	O
learning	O
and	O
decision	O
making	O
research	O
so	O
far	O
points	O
to	O
their	O
being	O
some	O
brain	O
regions	O
more	O
involved	O
in	O
one	O
type	O
of	O
process	O
than	O
the	O
other	O
but	O
the	O
picture	O
remains	O
unclear	O
because	O
modelfree	O
and	O
model-based	O
processes	O
do	O
not	O
appear	O
to	O
be	O
neatly	O
separated	O
in	O
the	O
brain	O
many	O
questions	O
remain	O
unanswered	O
perhaps	O
most	O
intriguing	O
is	O
evidence	O
that	O
the	O
hippocampus	O
a	O
structure	O
traditionally	O
associated	O
with	O
spatial	O
navigation	O
and	O
memory	O
appears	O
to	O
be	O
involved	O
in	O
simulating	O
possible	O
future	O
courses	O
of	O
action	B
as	O
part	O
of	O
an	O
animal	O
s	O
decisionmaking	O
process	O
this	O
suggests	O
that	O
it	O
is	O
part	O
of	O
a	O
system	O
that	O
uses	O
an	O
environment	B
model	O
for	O
planning	B
reinforcement	B
learning	I
theory	O
is	O
also	O
influencing	O
thinking	O
about	O
neural	B
processes	O
underlying	O
drug	O
abuse	O
a	O
model	O
of	O
some	O
features	O
of	O
drug	O
addiction	B
is	O
based	O
on	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
it	O
proposes	O
that	O
an	O
addicting	O
stimulant	O
such	O
as	O
cocaine	O
destabilizes	O
td	B
learning	O
to	O
produce	O
unbounded	O
growth	O
in	O
the	O
values	O
of	O
actions	O
associated	O
with	O
drug	O
intake	O
this	O
is	O
far	O
from	O
a	O
complete	O
model	O
of	O
addiction	B
but	O
it	O
illustrates	O
how	O
a	O
computational	O
perspective	O
suggests	O
theories	O
that	O
can	O
be	O
tested	O
with	O
further	O
research	O
the	O
new	O
field	O
of	O
computational	O
psychiatry	O
similarly	O
focuses	O
on	O
the	O
use	O
of	O
computational	O
models	O
some	O
derived	O
from	O
reinforcement	B
learning	I
to	O
better	O
understand	O
mental	O
disorders	O
this	O
chapter	O
only	O
touched	O
the	O
surface	O
of	O
how	O
the	O
neuroscience	B
of	O
reinforcement	B
learning	I
and	O
the	O
development	O
of	O
reinforcement	B
learning	I
in	O
computer	O
science	O
and	O
engineering	O
have	O
influenced	O
one	O
another	O
most	O
features	O
of	O
reinforcement	B
learning	I
algorithms	O
owe	O
their	O
design	O
to	O
purely	O
computational	O
considerations	O
but	O
some	O
have	O
been	O
influenced	O
by	O
hypotheses	O
about	O
neural	B
learning	O
mechanisms	O
remarkably	O
as	O
experimental	O
data	O
has	O
accumulated	O
about	O
the	O
brain	O
s	O
reward	O
processes	O
many	O
of	O
the	O
purely	O
computationallymotivated	O
features	O
of	O
reinforcement	B
learning	I
algorithms	O
are	O
turning	O
out	O
to	O
be	O
consistent	O
with	O
neuroscience	B
data	O
other	O
features	O
of	O
computational	O
reinforcement	B
learning	I
such	O
eligibility	B
traces	I
and	O
the	O
ability	O
of	O
teams	O
of	O
reinforcement	B
learning	I
agents	O
to	O
learn	O
to	O
act	O
collectively	O
under	O
the	O
influence	O
of	O
a	O
globally-broadcast	O
reinforcement	B
signal	I
may	O
also	O
turn	O
out	O
to	O
parallel	O
experimental	O
data	O
as	O
neuroscientists	O
continue	O
to	O
unravel	O
the	O
neural	B
basis	O
of	O
reward-based	O
animal	O
learning	O
and	O
behavior	O
summary	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
number	O
of	O
publications	O
treating	O
parallels	O
between	O
the	O
neuroscience	B
of	O
learning	O
and	O
decision	O
making	O
and	O
the	O
approach	O
to	O
reinforcement	B
learning	I
presented	O
in	O
this	O
book	O
is	O
enormous	O
we	O
can	O
cite	O
only	O
a	O
small	O
selection	O
niv	O
dayan	O
and	O
niv	O
gimcher	O
ludvig	O
bellemare	O
and	O
pearson	O
and	O
shah	O
are	O
good	O
places	O
to	O
start	O
together	O
with	O
economics	O
evolutionary	O
biology	O
and	O
mathematical	O
psychology	B
reinforcement	B
learning	I
theory	O
is	O
helping	O
to	O
formulate	O
quantitative	O
models	O
of	O
the	O
neural	B
mechanisms	O
of	O
choice	O
in	O
humans	O
and	O
non-human	O
primates	O
with	O
its	O
focus	O
on	O
learning	O
this	O
chapter	O
only	O
lightly	O
touches	O
upon	O
the	O
neuroscience	B
of	O
decision	O
making	O
glimcher	O
introduced	O
the	O
field	O
of	O
neuroeconomics	B
in	O
which	O
reinforcement	B
learning	I
contributes	O
to	O
the	O
study	O
of	O
the	O
neural	B
basis	O
of	O
decision	O
making	O
from	O
an	O
economics	O
perspective	O
see	O
also	O
glimcher	O
and	O
fehr	O
the	O
text	O
on	O
computational	O
and	O
mathematical	O
modeling	O
in	B
neuroscience	B
by	O
dayan	O
and	O
abbott	O
includes	O
reinforcement	B
learning	I
s	O
role	O
in	O
these	O
approaches	O
sterling	O
and	O
laughlin	O
examined	O
the	O
neural	B
basis	O
of	O
learning	O
in	O
terms	O
of	O
general	O
design	O
principles	O
that	O
enable	O
efficient	O
adaptive	O
behavior	O
there	O
are	O
many	O
good	O
expositions	O
of	O
basic	O
neuroscience	B
kandel	O
schwartz	O
jessell	O
siegelbaum	O
and	O
hudspeth	O
is	O
an	O
authoritative	O
and	O
very	O
comprehensive	O
source	O
berridge	O
and	O
kringelbach	O
reviewed	O
the	O
neural	B
basis	O
of	O
reward	O
and	O
pleasure	O
pointing	O
out	O
that	O
reward	O
processing	O
has	O
many	O
dimensions	O
and	O
involves	O
many	O
neural	B
systems	O
space	O
prevents	O
discussion	O
of	O
the	O
influential	O
research	O
of	O
berridge	O
and	O
robinson	O
who	O
distinguish	O
between	O
the	O
hedonic	O
impact	O
of	O
a	O
stimulus	O
which	O
they	O
call	O
liking	O
and	O
the	O
motivational	O
effect	O
which	O
they	O
call	O
wanting	O
hare	O
o	O
doherty	O
camerer	O
schultz	B
and	O
rangel	O
examined	O
the	O
neural	B
basis	O
of	O
value-related	O
signals	O
from	O
an	O
economic	O
perspective	O
distinguishing	O
between	O
goal	O
values	O
decision	O
values	O
and	O
prediction	B
errors	O
decision	O
value	B
is	O
goal	O
value	B
minus	O
action	B
cost	O
see	O
also	O
rangel	O
camerer	O
and	O
montague	O
rangel	O
and	O
hare	O
and	O
peters	O
and	O
b	O
uchel	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
is	O
most	O
prominently	O
discussed	O
by	O
schultz	B
dayan	O
and	O
montague	O
the	O
hypothesis	O
was	O
first	O
explicitly	O
put	O
forward	O
by	O
montague	O
dayan	O
and	O
sejnowski	O
as	O
they	O
stated	O
the	O
hypothesis	O
it	O
referred	O
to	O
reward	O
prediction	B
errors	O
but	O
not	O
specifically	O
to	O
td	B
errors	O
however	O
their	O
development	O
of	O
the	O
hypothesis	O
made	O
it	O
clear	O
that	O
they	O
were	O
referring	O
to	O
td	B
errors	O
the	O
earliest	O
recognition	O
of	O
the	O
tderrordopamine	O
connection	O
of	O
which	O
we	O
are	O
aware	O
is	O
that	O
of	O
montague	O
dayan	O
nowlan	O
pouget	O
and	O
sejnowski	O
who	O
proposed	O
a	O
td-error-modulated	O
hebbian	B
learning	O
rule	O
motivated	O
by	O
results	O
on	O
dopamine	B
signaling	O
from	O
schultz	B
s	O
group	O
the	O
connection	O
was	O
also	O
pointed	O
out	O
in	O
an	O
abstract	O
by	O
quartz	O
dayan	O
montague	O
and	O
sejnowski	O
montague	O
and	O
sejnowski	O
emphasized	O
the	O
importance	O
of	O
prediction	B
in	O
the	O
brain	O
and	O
outlined	O
how	O
predictive	O
hebbian	B
chapter	O
neuroscience	B
learning	O
modulated	O
by	O
td	B
errors	O
could	O
be	O
implemented	O
via	O
a	O
diffuse	O
neuromodulatory	O
system	O
such	O
as	O
the	O
dopamine	B
system	O
friston	O
tononi	O
reeke	O
sporns	O
and	O
edelman	O
presented	O
a	O
model	O
of	O
value-dependent	O
learning	O
in	O
the	O
brain	O
in	O
which	O
synaptic	O
changes	O
are	O
mediated	O
by	O
a	O
td-like	O
error	O
provided	O
by	O
a	O
global	O
neuromodulatory	O
signal	O
they	O
did	O
not	O
single	O
out	O
dopamine	B
montague	O
dayan	O
person	O
and	O
sejnowski	O
presented	O
a	O
model	O
of	O
honeybee	O
foraging	O
using	O
the	O
td	B
error	I
the	O
model	O
is	O
based	O
on	O
research	O
by	O
hammer	O
menzel	O
and	O
colleagues	O
and	O
menzel	O
hammer	O
showing	O
that	O
the	O
neuromodulator	O
octopamine	O
acts	O
as	O
a	O
reinforcement	B
signal	I
in	O
the	O
honeybee	O
montague	O
et	O
al	O
pointed	O
out	O
that	O
dopamine	B
likely	O
plays	O
a	O
similar	O
role	O
in	O
the	O
vertebrate	O
brain	O
barto	O
related	O
the	O
actor	O
critic	O
architecture	O
to	O
basalganglionic	O
circuits	O
and	O
discussed	O
the	O
relationship	O
between	O
td	B
learning	O
and	O
the	O
main	O
results	O
from	O
schultz	B
s	O
group	O
houk	O
adams	O
and	O
barto	O
suggested	O
how	O
td	B
learning	O
and	O
the	O
actor	O
critic	O
architecture	O
might	O
map	O
onto	O
the	O
anatomy	O
physiology	O
and	O
molecular	O
mechanism	O
of	O
the	O
basal	B
ganglia	I
doya	O
and	O
sejnowski	O
extended	O
their	O
earlier	O
paper	O
on	O
a	O
model	O
of	O
birdsong	O
learning	O
and	O
sejnowski	O
by	O
including	O
a	O
td-like	O
error	O
identified	O
with	O
dopamine	B
to	O
reinforce	B
the	O
selection	O
of	O
auditory	O
input	O
to	O
be	O
memorized	O
o	O
reilly	O
and	O
frank	O
and	O
o	O
reilly	O
frank	O
hazy	O
and	O
watz	O
argued	O
that	O
phasic	O
dopamine	B
signals	O
are	O
rpes	O
but	O
not	O
td	B
errors	O
in	O
support	O
of	O
their	O
theory	O
they	O
cited	O
results	O
with	O
variable	O
interstimulus	O
intervals	O
that	O
do	O
not	O
match	O
predictions	O
of	O
a	O
simple	O
td	B
model	I
as	O
well	O
as	O
the	O
observation	O
that	O
higher-order	O
conditioning	O
beyond	O
second-order	O
conditioning	O
is	O
rarely	O
observed	O
while	O
td	B
learning	O
is	O
not	O
so	O
limited	O
dayan	O
and	O
niv	O
discussed	O
the	O
good	O
the	O
bad	O
and	O
the	O
ugly	O
of	O
how	O
reinforcement	B
learning	I
theory	O
and	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
align	O
with	O
experimental	O
data	O
glimcher	O
reviewed	O
the	O
empirical	O
findings	O
that	O
support	O
the	O
reward	B
prediction	B
error	I
hypothesis	I
and	O
emphasized	O
the	O
significance	O
of	O
the	O
hypothesis	O
for	O
contemporary	O
neuroscience	B
graybiel	O
is	O
a	O
brief	O
primer	O
on	O
the	O
basal	B
ganglia	I
the	O
experiments	O
mentioned	O
that	O
involve	O
optogenetic	O
activation	O
of	O
dopamine	B
neurons	O
were	O
conducted	O
by	O
tsai	O
zhang	O
adamantidis	O
stuber	O
bonci	O
de	O
lecea	O
and	O
deisseroth	O
steinberg	O
keiflin	O
boivin	O
witten	B
deisseroth	O
and	O
janak	O
and	O
claridgechang	O
roorda	O
vrontou	O
sjulson	O
li	O
hirsh	O
and	O
miesenb	O
ock	O
fiorillo	O
yun	O
and	O
song	O
lammel	O
lim	O
and	O
malenka	O
and	O
saddoris	O
cacciapaglia	O
wightmman	O
and	O
carelli	O
are	O
among	O
studies	O
showing	O
that	O
the	O
signaling	O
properties	O
of	O
dopamine	B
neurons	O
are	O
specialized	O
for	O
different	O
target	O
regions	O
rpe-signaling	O
neurons	O
may	O
belong	O
to	O
one	O
among	O
multiple	O
populations	O
of	O
dopamine	B
neurons	O
having	O
different	O
targets	O
and	O
subserving	O
different	O
functions	O
eshel	O
tian	O
bukwich	O
and	O
uchida	O
found	O
homogeneity	O
of	O
reward	O
prediction	B
error	O
responses	O
of	O
dopamine	B
neurons	O
in	O
the	O
lateral	O
vta	O
during	O
classical	B
conditioning	I
in	O
mice	O
though	O
their	O
results	O
do	O
not	O
rule	O
out	O
response	O
diversity	O
across	O
wider	O
areas	O
gershman	O
pesaran	O
and	O
daw	O
studied	O
reinforcement	B
learning	I
tasks	O
that	O
can	O
be	O
decomposed	O
into	O
independent	O
components	O
with	O
separate	O
reward	O
signals	O
finding	O
evidence	O
in	O
human	O
neuroimaging	O
data	O
suggesting	O
summary	O
that	O
the	O
brain	O
exploits	O
this	O
kind	O
of	O
structure	O
schultz	B
s	O
survey	O
article	O
is	O
a	O
good	O
entr	O
ee	O
into	O
the	O
very	O
extensive	O
literature	O
on	O
reward	O
predicting	O
signaling	O
of	O
dopamine	B
neurons	O
berns	O
mcclure	O
pagnoni	O
and	O
montague	O
breiter	O
aharon	O
kahneman	O
dale	O
and	O
shizgal	O
pagnoni	O
zink	O
montague	O
and	O
berns	O
and	O
o	O
doherty	O
dayan	O
friston	O
critchley	O
and	O
dolan	O
described	O
functional	O
brain	O
imaging	O
studies	O
supporting	O
the	O
existence	O
of	O
signals	O
like	O
td	B
errors	O
in	O
the	O
human	O
brain	O
this	O
section	O
roughly	O
follows	O
barto	O
in	O
explaining	O
how	O
td	B
errors	O
mimic	O
the	O
main	O
results	O
from	O
schultz	B
s	O
group	O
on	O
the	O
phasic	O
responses	O
of	O
dopamine	B
neurons	O
this	O
section	O
is	O
largely	O
based	O
on	O
takahashi	O
schoenbaum	O
and	O
niv	O
and	O
niv	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
barto	O
and	O
houk	O
adams	O
and	O
barto	O
first	O
speculated	O
about	O
possible	O
implementations	O
of	O
actor	O
critic	O
algorithms	O
in	O
the	O
basal	B
ganglia	I
on	O
the	O
basis	O
of	O
functional	O
magnetic	O
resonance	O
imaging	O
of	O
human	O
subjects	O
while	O
engaged	O
in	O
instrumental	O
conditioning	O
o	O
doherty	O
dayan	O
schultz	B
deichmann	O
friston	O
and	O
dolan	O
suggested	O
that	O
the	O
actor	O
and	O
the	O
critic	O
are	O
most	O
likely	O
located	O
respectively	O
in	O
the	O
dorsal	O
and	O
ventral	O
striatum	O
gershman	O
moustafa	O
and	O
ludvig	O
focused	O
on	O
how	O
time	O
is	O
represented	O
in	O
reinforcement	B
learning	I
models	O
of	O
the	O
basal	B
ganglia	I
discussing	O
evidence	O
for	O
and	O
implications	O
of	O
various	O
computational	O
approaches	O
to	O
time	O
representation	O
the	O
hypothetical	O
neural	B
implementation	O
of	O
the	O
actor	O
critic	O
architecture	O
described	O
in	O
this	O
section	O
includes	O
very	O
little	O
detail	O
about	O
known	O
basal	B
ganglia	I
anatomy	O
and	O
physiology	O
in	O
addition	O
to	O
the	O
more	O
detailed	O
hypothesis	O
of	O
houk	O
adams	O
and	O
barto	O
a	O
number	O
of	O
other	O
hypotheses	O
include	O
more	O
specific	O
connections	O
to	O
anatomy	O
and	O
physiology	O
and	O
are	O
claimed	O
to	O
explain	O
additional	O
data	O
these	O
include	O
hypotheses	O
proposed	O
by	O
suri	O
and	O
schultz	B
brown	O
bullock	O
and	O
grossberg	O
contreras-vidal	O
and	O
schultz	B
suri	O
bargas	O
and	O
arbib	O
o	O
reilly	O
and	O
frank	O
and	O
o	O
reilly	O
frank	O
hazy	O
and	O
watz	O
joel	O
niv	O
and	O
ruppin	O
critically	O
evaluated	O
the	O
anatomical	O
plausibility	O
of	O
several	O
of	O
these	O
models	O
and	O
present	O
an	O
alternative	O
intended	O
to	O
accommodate	O
some	O
neglected	O
features	O
of	O
basal	O
ganglionic	O
circuitry	O
the	O
actor	O
learning	O
rule	O
discussed	O
here	O
is	O
more	O
complicated	O
than	O
the	O
one	O
in	O
the	O
early	O
actor	O
critic	O
network	O
of	O
barto	O
et	O
al	O
actor-unit	O
eligibility	B
traces	I
in	O
that	O
network	O
were	O
traces	O
of	O
just	O
at	O
xst	O
instead	O
of	O
the	O
full	O
that	O
work	O
did	O
not	O
benefit	O
from	O
the	O
policy-gradient	O
theory	O
presented	O
in	O
chapter	O
or	O
the	O
contributions	O
of	O
williams	O
who	O
showed	O
how	O
an	O
artificial	O
neural	B
network	O
of	O
bernoulli-logistic	O
units	O
could	O
implement	O
a	O
policy-gradient	O
method	O
chapter	O
neuroscience	B
reynolds	O
and	O
wickens	O
proposed	O
a	O
three-factor	O
rule	O
for	O
synaptic	B
plasticity	I
in	O
the	O
corticostriatal	O
pathway	O
in	O
which	O
dopamine	B
modulates	O
changes	O
in	O
corticostriatal	O
synaptic	O
efficacy	O
they	O
discussed	O
the	O
experimental	O
support	O
for	O
this	O
kind	O
of	O
learning	O
rule	O
and	O
its	O
possible	O
molecular	O
basis	O
the	O
definitive	O
demonstration	O
of	O
spike-timing-dependent	O
plasticity	O
is	O
attributed	O
to	O
markram	O
l	O
ubke	O
frotscher	O
and	O
sakmann	O
with	O
evidence	O
from	O
earlier	O
experiments	O
by	O
levy	O
and	O
steward	O
and	O
others	O
that	O
the	O
relative	O
timing	O
of	O
pre-	O
and	O
postsynaptic	O
spikes	O
is	O
critical	O
for	O
inducing	O
changes	O
in	O
synaptic	O
efficacy	O
rao	O
and	O
sejnowski	O
suggested	O
how	O
stdp	O
could	O
be	O
the	O
result	O
of	O
a	O
td-like	O
mechanism	O
at	O
synapses	O
with	O
non-contingent	O
eligibility	B
traces	I
lasting	O
about	O
milliseconds	O
dayan	O
commented	O
that	O
this	O
would	O
require	O
an	O
error	O
as	O
in	O
sutton	O
and	O
barto	O
s	O
early	O
model	O
of	O
classical	B
conditioning	I
and	O
not	O
a	O
true	O
td	B
error	I
representative	O
publications	O
from	O
the	O
extensive	O
literature	O
on	O
rewardmodulated	O
stdp	O
are	O
wickens	O
reynolds	O
and	O
wickens	O
and	O
calabresi	O
picconi	O
tozzi	O
and	O
di	O
filippo	O
pawlak	O
and	O
kerr	O
showed	O
that	O
dopamine	B
is	O
necessary	O
to	O
induce	O
stdp	O
at	O
the	O
corticostriatal	O
synapses	O
of	O
medium	O
spiny	O
neurons	O
see	O
also	O
pawlak	O
wickens	O
kirkwood	O
and	O
kerr	O
yagishita	O
hayashi-takagi	O
ellis-davies	O
urakubo	O
ishii	O
and	O
kasai	O
found	O
that	O
dopamine	B
promotes	O
spine	O
enlargement	O
of	O
the	O
medium	O
spiny	O
neurons	O
of	O
mice	O
only	O
during	O
a	O
time	O
window	O
of	O
from	O
to	O
seconds	O
after	O
stdp	O
stimulation	O
izhikevich	O
proposed	O
and	O
explored	O
the	O
idea	O
of	O
using	O
stdp	O
timing	O
conditions	O
to	O
trigger	O
contingent	O
eligibility	B
traces	I
fr	O
emaux	O
sprekeler	O
and	O
gerstner	O
proposed	O
theoretical	O
conditions	O
for	O
successful	O
learning	O
by	O
rules	O
based	O
on	O
reward-modulated	O
stdp	O
klopf	B
s	O
hedonistic	O
neuron	O
hypothesis	O
inspired	O
our	O
actor	O
critic	O
algorithm	O
implemented	O
as	O
an	O
artificial	O
neural	B
network	O
with	O
a	O
single	O
neuronlike	O
unit	O
called	O
the	O
actor	O
unit	O
implementing	O
a	O
law-of-effect-like	O
learning	O
rule	O
sutton	O
and	O
anderson	O
ideas	O
related	O
to	O
klopf	B
s	O
synaptically-local	O
eligibility	O
have	O
been	O
proposed	O
by	O
others	O
crow	O
proposed	O
that	O
changes	O
in	O
the	O
synapses	O
of	O
cortical	O
neurons	O
are	O
sensitive	O
to	O
the	O
consequences	O
of	O
neural	B
activity	O
emphasizing	O
the	O
need	O
to	O
address	O
the	O
time	O
delay	O
between	O
neural	B
activity	O
and	O
its	O
consequences	O
in	O
a	O
reward-modulated	O
form	O
of	O
synaptic	B
plasticity	I
he	O
proposed	O
a	O
contingent	O
form	O
of	O
eligibility	O
but	O
associated	O
with	O
entire	O
neurons	O
instead	O
of	O
individual	O
synapses	O
according	O
to	O
his	O
hypothesis	O
a	O
wave	O
of	O
neuronal	O
activity	O
leads	O
to	O
a	O
short-term	O
change	O
in	O
the	O
cells	O
involved	O
in	O
the	O
wave	O
such	O
that	O
they	O
are	O
picked	O
out	O
from	O
a	O
background	O
of	O
cells	O
not	O
so	O
activated	O
such	O
cells	O
are	O
rendered	O
sensitive	O
by	O
the	O
short-term	O
change	O
to	O
a	O
reward	B
signal	I
in	O
such	O
a	O
way	O
that	O
if	O
such	O
a	O
signal	O
occurs	O
before	O
the	O
end	O
of	O
the	O
decay	O
time	O
of	O
the	O
change	O
the	O
synaptic	O
connexions	O
between	O
the	O
cells	O
are	O
made	O
more	O
effective	O
crow	O
argued	O
against	O
previous	O
proposals	O
that	O
reverberating	O
neural	B
circuits	O
play	O
this	O
role	O
by	O
pointing	O
out	O
that	O
the	O
effect	O
of	O
a	O
reward	B
signal	I
on	O
such	O
a	O
circuit	O
would	O
summary	O
the	O
synaptic	O
connexions	O
leading	O
to	O
the	O
reverberation	O
is	O
to	O
say	O
those	O
involved	O
in	O
activity	O
at	O
the	O
time	O
of	O
the	O
reward	B
signal	I
and	O
not	O
those	O
on	O
the	O
path	O
which	O
led	O
to	O
the	O
adaptive	O
motor	O
output	O
crow	O
further	O
postulated	O
that	O
reward	O
signals	O
are	O
delivered	O
via	O
a	O
distinct	O
neural	B
fiber	O
system	O
presumably	O
the	O
one	O
into	O
which	O
olds	O
and	O
milner	O
tapped	O
that	O
would	O
transform	O
synaptic	O
connections	O
from	O
a	O
short	O
into	O
a	O
long-term	O
form	O
in	O
another	O
farsighted	O
hypothesis	O
miller	O
proposed	O
a	O
law-of-effect-like	O
learning	O
rule	O
that	O
includes	O
synaptically-local	O
contingent	O
eligibility	B
traces	I
it	O
is	O
envisaged	O
that	O
in	O
a	O
particular	O
sensory	O
situation	O
neurone	O
b	O
by	O
chance	O
fires	O
a	O
meaningful	O
burst	O
of	O
activity	O
which	O
is	O
then	O
translated	O
into	O
motor	O
acts	O
which	O
then	O
change	O
the	O
situation	O
it	O
must	O
be	O
supposed	O
that	O
the	O
meaningful	O
burst	O
has	O
an	O
influence	O
at	O
the	O
neuronal	O
level	O
on	O
all	O
of	O
its	O
own	O
synapses	O
which	O
are	O
active	O
at	O
the	O
time	O
thereby	O
making	O
a	O
preliminary	O
selection	O
of	O
the	O
synapses	O
to	O
be	O
strengthened	O
though	O
not	O
yet	O
actually	O
strengthening	O
them	O
strengthening	O
signal	O
makes	O
the	O
final	O
selection	O
and	O
accomplishes	O
the	O
definitive	O
change	O
in	O
the	O
appropriate	O
synapses	O
p	O
miller	O
s	O
hypothesis	O
also	O
included	O
a	O
critic-like	O
mechanism	O
which	O
he	O
called	O
a	O
sensory	O
analyzer	O
unit	O
that	O
worked	O
according	O
to	O
classical	B
conditioning	I
principles	O
to	O
provide	O
reinforcement	O
signals	O
to	O
neurons	O
so	O
that	O
they	O
would	O
learn	O
to	O
move	O
from	O
lower-	O
to	O
higher-valued	O
states	O
thus	O
anticipating	O
the	O
use	O
of	O
the	O
td	B
error	I
as	O
a	O
reinforcement	B
signal	I
in	O
the	O
actor	O
critic	O
architecture	O
miller	O
s	O
idea	O
not	O
only	O
parallels	O
klopf	B
s	O
the	O
exception	O
of	O
its	O
explicit	O
invocation	O
of	O
a	O
distinct	O
strengthening	O
signal	O
it	O
also	O
anticipated	O
the	O
general	O
features	O
of	O
reward-modulated	O
stdp	O
a	O
related	O
though	O
different	O
idea	O
which	O
seung	O
called	O
the	O
hedonistic	O
synapse	O
is	O
that	O
synapses	O
individually	O
adjust	O
the	O
probability	O
that	O
they	O
release	O
neurotransmitter	O
in	O
the	O
manner	O
of	O
the	O
law	B
of	I
effect	I
if	O
reward	O
follows	O
release	O
the	O
release	O
probability	O
increases	O
and	O
decreases	O
if	O
reward	O
follows	O
failure	O
to	O
release	O
this	O
is	O
essentially	O
the	O
same	O
as	O
the	O
learning	O
scheme	O
minsky	B
used	O
in	O
his	O
princeton	O
ph	O
d	O
dissertation	O
where	O
he	O
called	O
the	O
synapse-like	O
learning	O
element	O
a	O
snarc	O
neural-analog	O
reinforcement	O
calculator	O
contingent	O
eligibility	O
is	O
involved	O
in	O
these	O
ideas	O
too	O
although	O
it	O
is	O
contingent	O
on	O
the	O
activity	O
of	O
an	O
individual	O
synapse	O
instead	O
of	O
the	O
postsynaptic	O
neuron	O
also	O
related	O
is	O
the	O
proposal	O
of	O
unnikrishman	O
and	O
venugopal	O
that	O
uses	O
the	O
correlation-based	O
method	O
of	O
harth	O
and	O
tzanakou	O
to	O
adjust	O
neural	B
network	O
weights	O
frey	O
and	O
morris	O
proposed	O
the	O
idea	O
of	O
a	O
synaptic	O
tag	O
for	O
the	O
induction	O
of	O
long-lasting	O
strengthening	O
of	O
synaptic	O
efficacy	O
though	O
not	O
unlike	O
klopf	B
s	O
eligibility	O
their	O
tag	O
was	O
hypothesized	O
to	O
consist	O
of	O
a	O
temporary	O
strengthening	O
of	O
a	O
synapse	O
that	O
could	O
be	O
transformed	O
into	O
a	O
long-lasting	O
strengthening	O
by	O
subsequent	O
neuron	O
activation	O
the	O
model	O
of	O
o	O
reilly	O
and	O
frank	O
and	O
o	O
reilly	O
frank	O
hazy	O
and	O
watz	O
uses	O
working	O
memory	O
to	O
bridge	O
temporal	O
intervals	O
instead	O
of	O
eligibility	B
traces	I
wickens	O
and	O
kotter	O
discuss	O
possible	O
mechanisms	O
for	O
synaptic	O
eligibility	O
he	O
huertas	O
hong	O
tie	O
hell	O
shouval	O
kirkwood	O
chapter	O
neuroscience	B
provide	O
evidence	O
supporting	O
the	O
existence	O
of	O
contingent	O
eligibility	B
traces	I
in	O
synapses	O
of	O
cortical	O
neurons	O
with	O
time	O
courses	O
like	O
those	O
of	O
the	O
eligibility	B
traces	I
klopf	B
postulated	O
the	O
metaphor	O
of	O
a	O
neuron	O
using	O
a	O
learning	O
rule	O
related	O
to	O
bacterial	O
chemotaxis	O
was	O
discussed	O
by	O
barto	O
koshland	O
s	O
extensive	O
study	O
of	O
bacterial	O
chemotaxis	O
was	O
in	O
part	O
motivated	O
by	O
similarities	O
between	O
features	O
of	O
bacteria	O
and	O
features	O
of	O
neurons	O
see	O
also	O
berg	O
shimansky	O
proposed	O
a	O
synaptic	O
learning	O
rule	O
somewhat	O
similar	O
to	O
seung	O
s	O
mentioned	O
above	O
in	O
which	O
each	O
synapse	O
individually	O
acts	O
like	O
a	O
chemotactic	O
bacterium	O
in	O
this	O
case	O
a	O
collection	O
of	O
synapses	O
swims	O
toward	O
attractants	O
in	O
the	O
high-dimensional	O
space	O
of	O
synaptic	O
weight	O
values	O
montague	O
dayan	O
person	O
and	O
sejnowski	O
proposed	O
a	O
chemotactic-like	O
model	O
of	O
the	O
bee	O
s	O
foraging	O
behavior	O
involving	O
the	O
neuromodulator	O
octopamine	O
research	O
on	O
the	O
behavior	O
of	O
reinforcement	B
learning	I
agents	O
in	O
team	O
and	O
game	O
problems	O
has	O
a	O
long	O
history	O
roughly	O
occurring	O
in	O
three	O
phases	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
the	O
first	O
phase	O
began	O
with	O
investigations	O
by	O
the	O
russian	O
mathematician	O
and	O
physicist	O
m	O
l	O
tsetlin	O
a	O
collection	O
of	O
his	O
work	O
was	O
published	O
as	O
tsetlin	O
after	O
his	O
death	O
in	O
our	O
sections	O
and	O
refer	O
to	O
his	O
study	O
of	O
learning	B
automata	I
in	O
connection	O
to	O
bandit	B
problems	I
the	O
tsetlin	O
collection	O
also	O
includes	O
studies	O
of	O
learning	B
automata	I
in	O
team	O
and	O
game	O
problems	O
which	O
led	O
to	O
later	O
work	O
in	O
this	O
area	O
using	O
stochastic	O
learning	B
automata	I
as	O
described	O
by	O
narendra	O
and	O
thathachar	O
viswanathan	O
and	O
narendra	O
lakshmivarahan	O
and	O
narendra	O
narendra	O
and	O
wheeler	O
and	O
thathachar	O
and	O
sastry	O
thathachar	O
and	O
sastry	O
is	O
a	O
more	O
recent	O
comprehensive	O
account	O
these	O
studies	O
were	O
mostly	O
restricted	O
to	O
nonassociative	O
learning	B
automata	I
meaning	O
that	O
they	O
did	O
not	O
address	O
associative	O
or	O
contextual	O
bandit	B
problems	I
the	O
second	O
phase	O
began	O
with	O
the	O
extension	O
of	O
learning	B
automata	I
to	O
the	O
associative	O
or	O
contextual	O
case	O
barto	O
sutton	O
and	O
brouwer	O
and	O
barto	O
and	O
sutton	O
experimented	O
with	O
associative	O
stochastic	O
learning	B
automata	I
in	O
single-layer	O
artificial	B
neural	B
networks	I
to	O
which	O
a	O
global	O
reinforcement	B
signal	I
was	O
broadcast	O
the	O
learning	O
algorithm	O
was	O
an	O
associative	O
extension	O
of	O
the	O
alopex	O
algorithm	O
of	O
harth	O
and	O
tzanakou	O
barto	O
et	O
al	O
called	O
neuron-like	O
elements	O
implementing	O
this	O
kind	O
of	O
learning	O
associative	B
search	I
elements	O
barto	O
and	O
anandan	O
introduced	O
an	O
associative	B
reinforcement	B
learning	I
algorithm	O
called	O
the	O
associative	O
reward-penalty	O
p	O
algorithm	O
they	O
proved	O
a	O
convergence	O
result	O
by	O
combining	O
theory	O
of	O
stochastic	O
learning	B
automata	I
with	O
theory	O
of	O
pattern	O
classification	O
barto	O
and	O
barto	O
and	O
jordan	O
described	O
results	O
with	O
teams	O
of	O
ar	O
p	O
units	O
connected	O
into	O
multi-layer	O
neural	B
networks	O
showing	O
that	O
they	O
could	O
learn	O
nonlinear	O
functions	O
such	O
as	O
xor	O
and	O
others	O
with	O
a	O
globally-broadcast	O
reinforcement	B
signal	I
barto	O
extensively	O
discussed	O
this	O
approach	O
to	O
artificial	B
neural	B
networks	I
and	O
how	O
this	O
type	O
of	O
learning	O
rule	O
is	O
related	O
to	O
others	O
in	O
the	O
literature	O
at	O
that	O
time	O
williams	O
mathematically	O
analyzed	O
and	O
broadened	O
this	O
class	O
of	O
learning	O
rules	O
and	O
related	O
summary	O
their	O
use	O
to	O
the	O
error	O
backpropagation	B
method	O
for	O
training	O
multilayer	O
artificial	B
neural	B
networks	I
williams	O
described	O
several	O
ways	O
that	O
backpropagation	B
and	B
reinforcement	B
learning	I
can	O
be	O
combined	O
for	O
training	O
artificial	B
neural	B
networks	I
williams	O
showed	O
that	O
a	O
special	O
case	O
of	O
the	O
ar	O
p	O
algorithm	O
is	O
a	O
reinforce	B
algorithm	O
although	O
better	O
results	O
were	O
obtained	O
with	O
the	O
general	O
ar	O
p	O
algorithm	O
the	O
third	O
phase	O
of	O
interest	O
in	O
teams	O
of	O
reinforcement	B
learning	I
agents	O
was	O
influenced	O
by	O
increased	O
understanding	O
of	O
the	O
role	O
of	O
dopamine	B
as	O
a	O
widely	O
broadcast	O
neuromodulator	O
and	O
speculation	O
about	O
the	O
existence	O
of	O
reward-modulated	O
stdp	O
much	O
more	O
so	O
than	O
earlier	O
research	O
this	O
research	O
considers	O
details	O
of	O
synaptic	B
plasticity	I
and	O
other	O
constraints	O
from	O
neuroscience	B
publications	O
include	O
the	O
following	O
and	O
alphabetically	O
bartlett	O
and	O
baxter	O
xie	O
and	O
seung	O
baras	O
and	O
meir	O
farries	O
and	O
fairhall	O
florian	O
izhikevich	O
pecevski	O
maass	O
and	O
legenstein	O
legenstein	O
pecevski	O
and	O
maass	O
kolodziejski	O
porr	O
and	O
w	O
org	O
otter	O
urbanczik	O
and	O
senn	O
and	O
vasilaki	O
fr	O
emaux	O
urbanczik	O
senn	O
and	O
gerstner	O
now	O
e	O
vrancx	O
and	O
de	O
hauwere	O
reviewed	O
more	O
recent	O
developments	O
in	O
the	O
wider	O
field	O
of	O
multi-agent	O
reinforcement	B
learning	I
yin	O
and	O
knowlton	O
reviewed	O
findings	O
from	O
outcome-devaluation	O
experiments	O
with	O
rodents	O
supporting	O
the	O
view	O
that	O
habitual	O
and	O
goal-directed	O
behavior	O
psychologists	O
use	O
the	O
phrase	O
are	O
respectively	O
most	O
associated	O
with	O
processing	O
in	O
the	O
dorsolateral	O
striatum	O
and	O
the	O
dorsomedial	O
striatum	O
results	O
of	O
functional	O
imaging	O
experiments	O
with	O
human	O
subjects	O
in	O
the	O
outcome-devaluation	O
setting	O
by	O
valentin	O
dickinson	O
and	O
o	O
doherty	O
suggest	O
that	O
the	O
orbitofrontal	O
cortex	O
is	O
an	O
important	O
component	O
of	O
goaldirected	O
choice	O
single	O
unit	O
recordings	O
in	O
monkeys	O
by	O
padoa-schioppa	O
and	O
assad	O
support	O
the	O
role	O
of	O
the	O
ofc	O
in	O
encoding	O
values	O
guiding	O
choice	O
behavior	O
rangel	O
camerer	O
and	O
montague	O
and	O
rangel	O
and	O
hare	O
reviewed	O
findings	O
from	O
the	O
perspective	O
of	O
neuroeconomics	B
about	O
how	O
the	O
brain	O
makes	O
goal-directed	O
decisions	O
pezzulo	O
van	O
der	O
meer	O
lansink	O
and	O
pennartz	O
reviewed	O
the	O
neuroscience	B
of	O
internally	O
generated	O
sequences	O
and	O
presented	O
a	O
model	O
of	O
how	O
these	O
mechanisms	O
might	O
be	O
components	O
of	O
model-based	O
planning	B
daw	O
and	O
shohamy	O
proposed	O
that	O
while	O
dopamine	B
signaling	O
connects	O
well	O
to	O
habitual	O
or	O
model-free	O
behavior	O
other	O
processes	O
are	O
involved	O
in	O
goal-directed	O
or	O
model-based	O
behavior	O
data	O
from	O
experiments	O
by	O
bromberg-martin	O
matsumoto	O
hong	O
and	O
hikosaka	O
indicate	O
that	O
dopamine	B
signals	O
contain	O
information	O
pertinent	O
to	O
both	O
habitual	O
and	O
goal-directed	O
behavior	O
doll	O
simon	O
and	O
daw	O
argued	O
that	O
there	O
may	O
not	O
a	O
clear	O
separation	O
in	O
the	O
brain	O
between	O
mechanisms	O
that	O
subserve	O
habitual	O
and	O
goal-directed	O
learning	O
and	O
choice	O
keiflin	O
and	O
janak	O
reviewed	O
connections	O
between	O
td	B
errors	O
and	B
addiction	B
nutt	O
lingford-hughes	O
erritzoe	O
and	O
stokes	O
critically	O
evaluated	O
the	O
hypothesis	O
that	O
addiction	B
is	O
due	O
to	O
a	O
disorder	O
of	O
the	O
dopamine	B
system	O
montague	O
dolan	O
friston	O
and	O
dayan	O
outlined	O
the	O
goals	O
and	O
early	O
efforts	O
chapter	O
neuroscience	B
in	O
the	O
field	O
of	O
computational	O
psychiatry	O
and	O
adams	O
huys	O
and	O
roiser	O
reviewed	O
more	O
recent	O
progress	O
chapter	O
applications	B
and	I
case	I
studies	I
in	O
this	O
chapter	O
we	O
present	O
a	O
few	O
case	O
studies	O
of	O
reinforcement	B
learning	I
several	O
of	O
these	O
are	O
substantial	O
applications	O
of	O
potential	O
economic	O
significance	O
one	O
samuel	O
s	O
checkers	O
player	O
is	O
primarily	O
of	O
historical	O
interest	O
our	O
presentations	O
are	O
intended	O
to	O
illustrate	O
some	O
of	O
the	O
trade-offs	O
and	O
issues	O
that	O
arise	O
in	O
real	O
applications	O
for	O
example	O
we	O
emphasize	O
how	O
domain	O
knowledge	O
is	O
incorporated	O
into	O
the	O
formulation	O
and	O
solution	O
of	O
the	O
problem	O
we	O
also	O
highlight	O
the	O
representation	O
issues	O
that	O
are	O
so	O
often	O
critical	O
to	O
successful	O
applications	O
the	O
algorithms	O
used	O
in	O
some	O
of	O
these	O
case	O
studies	O
are	O
substantially	O
more	O
complex	O
than	O
those	O
we	O
have	O
presented	O
in	O
the	O
rest	O
of	O
the	O
book	O
applications	O
of	O
reinforcement	B
learning	I
are	O
still	O
far	O
from	O
routine	O
and	O
typically	O
require	O
as	O
much	O
art	O
as	O
science	O
making	O
applications	O
easier	O
and	O
more	O
straightforward	O
is	O
one	O
of	O
the	O
goals	O
of	O
current	O
research	O
in	O
reinforcement	B
learning	I
td-gammon	B
one	O
of	O
the	O
most	O
impressive	O
applications	O
of	O
reinforcement	B
learning	I
to	O
date	O
is	O
that	O
by	O
gerald	O
tesauro	O
to	O
the	O
game	O
of	O
backgammon	B
tesauro	O
s	O
program	O
td-gammon	B
required	O
little	O
backgammon	B
knowledge	O
yet	O
learned	O
to	O
play	O
extremely	O
well	O
near	O
the	O
level	O
of	O
the	O
world	O
s	O
strongest	O
grandmasters	O
the	O
learning	O
algorithm	O
in	B
td-gammon	B
was	O
a	O
straightforward	O
combination	O
of	O
the	O
td	B
algorithm	O
and	O
nonlinear	O
function	B
approximation	I
using	O
a	O
multilayer	O
neural	B
network	O
trained	O
by	O
backpropagating	O
td	B
errors	O
backgammon	B
is	O
a	O
major	O
game	O
in	O
the	O
sense	O
that	O
it	O
is	O
played	O
throughout	O
the	O
world	O
with	O
numerous	O
tournaments	O
and	O
regular	O
world	O
championship	O
matches	O
it	O
is	O
in	O
part	O
a	O
game	O
of	O
chance	O
and	O
it	O
is	O
a	O
popular	O
vehicle	O
for	O
waging	O
significant	O
sums	O
of	O
money	O
there	O
are	O
probably	O
more	O
professional	O
backgammon	B
players	O
than	O
there	O
are	O
professional	O
chess	B
players	O
the	O
game	O
is	O
played	O
with	O
white	O
and	O
black	O
pieces	O
on	O
a	O
board	O
of	O
locations	O
called	O
points	O
to	O
the	O
right	O
on	O
the	O
next	O
page	O
is	O
shown	O
a	O
typical	O
position	O
early	O
in	O
the	O
game	O
seen	O
from	O
the	O
perspective	O
of	O
the	O
white	O
player	O
white	O
here	O
has	O
just	O
rolled	O
the	O
dice	O
and	O
obtained	O
a	O
and	O
a	O
this	O
means	O
that	O
he	O
can	O
move	O
one	O
of	O
his	O
pieces	O
steps	O
and	O
one	O
chapter	O
applications	B
and	I
case	I
studies	I
the	O
same	O
piece	O
steps	O
for	O
example	O
he	O
could	O
move	O
two	O
pieces	O
from	O
the	O
point	O
one	O
to	O
the	O
point	O
and	O
one	O
to	O
the	O
point	O
white	O
s	O
objective	O
is	O
to	O
advance	O
all	O
of	O
his	O
pieces	O
into	O
the	O
last	O
quadrant	O
and	O
then	O
off	O
the	O
board	O
the	O
first	O
player	O
to	O
remove	O
all	O
his	O
pieces	O
wins	O
one	O
complication	O
is	O
that	O
the	O
pieces	O
interact	O
as	O
they	O
pass	O
each	O
other	O
going	O
in	O
different	O
directions	O
for	O
example	O
if	O
it	O
were	O
black	O
s	O
move	O
he	O
could	O
use	O
the	O
dice	O
roll	O
of	O
to	O
move	O
a	O
piece	O
from	O
the	O
point	O
to	O
the	O
point	O
hitting	O
the	O
white	O
piece	O
there	O
pieces	O
that	O
have	O
been	O
hit	O
are	O
placed	O
on	O
the	O
bar	O
in	O
the	O
middle	O
of	O
the	O
board	O
we	O
already	O
see	O
one	O
previously	O
hit	O
black	O
piece	O
from	O
whence	O
they	O
reenter	O
the	O
race	O
from	O
the	O
start	O
however	O
if	O
there	O
are	O
two	O
pieces	O
on	O
a	O
point	O
then	O
the	O
opponent	O
cannot	O
move	O
to	O
that	O
point	O
the	O
pieces	O
are	O
protected	O
from	O
being	O
hit	O
thus	O
white	O
cannot	O
use	O
his	O
dice	O
roll	O
to	O
move	O
either	O
of	O
his	O
pieces	O
on	O
the	O
point	O
because	O
their	O
possible	O
resulting	O
points	O
are	O
occupied	O
by	O
groups	O
of	O
black	O
pieces	O
forming	O
contiguous	O
blocks	O
of	O
occupied	O
points	O
to	O
block	O
the	O
opponent	O
is	O
one	O
of	O
the	O
elementary	O
strategies	O
of	O
the	O
game	O
backgammon	B
involves	O
several	O
further	O
complications	O
but	O
the	O
above	O
description	O
gives	O
the	O
basic	O
idea	O
with	O
pieces	O
and	O
possible	O
locations	O
counting	O
the	O
bar	O
and	O
off-theboard	O
it	O
should	O
be	O
clear	O
that	O
the	O
number	O
of	O
possible	O
backgammon	B
positions	O
is	O
enormous	O
far	O
more	O
than	O
the	O
number	O
of	O
memory	O
elements	O
one	O
could	O
have	O
in	O
any	O
physically	O
realizable	O
computer	O
the	O
number	O
of	O
moves	O
possible	O
from	O
each	O
position	O
is	O
also	O
large	O
for	O
a	O
typical	O
dice	O
roll	O
there	O
might	O
be	O
different	O
ways	O
of	O
playing	O
in	O
considering	O
future	O
moves	O
such	O
as	O
the	O
response	O
of	O
the	O
opponent	O
one	O
must	O
consider	O
the	O
possible	O
dice	O
rolls	O
as	O
well	O
the	O
result	O
is	O
that	O
the	O
game	O
tree	O
has	O
an	O
effective	O
branching	B
factor	I
of	O
about	O
this	O
is	O
far	O
too	O
large	O
to	O
permit	O
effective	O
use	O
of	O
the	O
conventional	O
heuristic	B
search	I
methods	O
that	O
have	O
proved	O
so	O
effective	O
in	O
games	O
like	O
chess	B
and	O
checkers	O
on	O
the	O
other	O
hand	O
the	O
game	O
is	O
a	O
good	O
match	O
to	O
the	O
capabilities	O
of	O
td	B
learning	O
methods	O
although	O
the	O
game	O
is	O
highly	O
stochastic	O
a	O
complete	O
description	O
of	O
the	O
game	O
s	O
state	B
is	O
available	O
at	O
all	O
times	O
the	O
game	O
evolves	O
over	O
a	O
sequence	O
of	O
moves	O
and	O
positions	O
until	O
finally	O
ending	O
in	O
a	O
win	O
for	O
one	O
player	O
or	O
the	O
other	O
ending	O
the	O
game	O
the	O
outcome	O
can	O
be	O
interpreted	O
as	O
a	O
final	O
reward	O
to	O
be	O
predicted	O
on	O
the	O
other	O
hand	O
the	O
theoretical	O
results	O
we	O
have	O
described	O
so	O
far	O
cannot	O
be	O
usefully	O
applied	O
to	O
this	O
task	O
the	O
number	O
of	O
states	O
is	O
so	O
large	O
that	O
a	O
lookup	O
table	O
cannot	O
be	O
used	O
and	O
the	O
opponent	O
is	O
a	O
source	O
of	O
uncertainty	O
and	O
time	O
variation	O
td-gammon	B
used	O
a	O
nonlinear	O
form	O
of	O
td	B
the	O
estimated	O
value	B
vsw	O
of	O
any	O
state	B
position	O
s	O
was	O
meant	O
to	O
estimate	O
the	O
probability	O
of	O
winning	O
starting	O
from	O
state	B
s	O
to	O
achieve	O
this	O
rewards	O
were	O
defined	O
as	O
zero	O
for	O
all	O
time	O
steps	O
except	O
those	O
on	O
which	O
the	O
game	O
is	O
won	O
to	O
implement	O
the	O
value	B
function	I
td-gammon	B
used	O
a	O
standard	O
multilayer	O
neural	B
network	O
much	O
as	O
shown	O
to	O
the	O
right	O
on	O
the	O
next	O
page	O
real	O
network	O
had	O
two	O
additional	O
units	O
in	O
its	O
final	O
layer	O
to	O
estimate	O
the	O
probability	O
of	O
each	O
white	O
pieces	O
move	O
black	O
pieces	O
move	O
clockwise	O
td-gammon	B
player	O
s	O
winning	O
in	O
a	O
special	O
way	O
called	O
a	O
gammon	O
or	O
backgammon	B
the	O
network	O
consisted	O
of	O
a	O
layer	O
of	O
input	O
units	O
a	O
layer	O
of	O
hidden	O
units	O
and	O
a	O
final	O
output	O
unit	O
the	O
input	O
to	O
the	O
network	O
was	O
a	O
representation	O
of	O
a	O
backgammon	B
position	O
and	O
the	O
output	O
was	O
an	O
estimate	O
of	O
the	O
value	B
of	O
that	O
position	O
figure	O
the	O
td-gammon	B
neural	B
network	O
in	O
the	O
first	O
version	O
of	O
td-gammon	B
td-gammon	B
backgammon	B
positions	O
were	O
represented	O
to	O
the	O
network	O
in	O
a	O
relatively	O
direct	O
way	O
that	O
involved	O
little	O
backgammon	B
knowledge	O
it	O
did	O
however	O
involve	O
substantial	O
knowledge	O
of	O
how	O
neural	B
networks	O
work	O
and	O
how	O
information	O
is	O
best	O
presented	O
to	O
them	O
it	O
is	O
instructive	O
to	O
note	O
the	O
exact	O
representation	O
tesauro	O
chose	O
there	O
were	O
a	O
total	O
of	O
input	O
units	O
to	O
the	O
network	O
for	O
each	O
point	O
on	O
the	O
backgammon	B
board	O
four	O
units	O
indicated	O
the	O
number	O
of	O
white	O
pieces	O
on	O
the	O
point	O
if	O
there	O
were	O
no	O
white	O
pieces	O
then	O
all	O
four	O
units	O
took	O
on	O
the	O
value	B
zero	O
if	O
there	O
was	O
one	O
piece	O
then	O
the	O
first	O
unit	O
took	O
on	O
the	O
value	B
this	O
encoded	O
the	O
elementary	O
concept	O
of	O
a	O
blot	O
i	O
e	O
a	O
piece	O
that	O
can	O
be	O
hit	O
by	O
the	O
opponent	O
if	O
there	O
were	O
two	O
or	O
more	O
pieces	O
then	O
the	O
second	O
unit	O
was	O
set	O
to	O
this	O
encoded	O
the	O
basic	O
concept	O
of	O
a	O
made	O
point	O
on	O
which	O
the	O
opponent	O
cannot	O
land	O
if	O
there	O
were	O
exactly	O
three	O
pieces	O
on	O
the	O
point	O
then	O
the	O
third	O
unit	O
was	O
set	O
to	O
this	O
encoded	O
the	O
basic	O
concept	O
of	O
a	O
single	O
spare	O
i	O
e	O
an	O
extra	O
piece	O
in	O
addition	O
to	O
the	O
two	O
pieces	O
that	O
made	O
the	O
point	O
finally	O
if	O
there	O
were	O
more	O
than	O
three	O
pieces	O
the	O
fourth	O
unit	O
was	O
set	O
to	O
a	O
value	B
proportionate	O
to	O
the	O
number	O
of	O
additional	O
pieces	O
beyond	O
three	O
letting	O
n	O
denote	O
the	O
total	O
number	O
of	O
pieces	O
on	O
the	O
point	O
if	O
n	O
then	O
the	O
fourth	O
unit	O
took	O
on	O
the	O
value	B
this	O
encoded	O
a	O
linear	O
representation	O
of	O
multiple	O
spares	O
at	O
the	O
given	O
point	O
with	O
four	O
units	O
for	O
white	O
and	O
four	O
for	O
black	O
at	O
each	O
of	O
the	O
points	O
that	O
made	O
a	O
total	O
of	O
units	O
two	O
additional	O
units	O
encoded	O
the	O
number	O
of	O
white	O
and	O
black	O
pieces	O
on	O
the	O
bar	O
took	O
the	O
value	B
where	O
n	O
is	O
the	O
number	O
of	O
pieces	O
on	O
the	O
bar	O
and	O
two	O
more	O
encoded	O
the	O
number	O
of	O
black	O
and	O
white	O
pieces	O
already	O
successfully	O
removed	O
from	O
the	O
board	O
took	O
the	O
value	B
where	O
n	O
is	O
the	O
number	O
of	O
pieces	O
already	O
borne	O
off	O
finally	O
two	O
units	O
indicated	O
in	O
a	O
binary	O
fashion	O
whether	O
it	O
was	O
white	O
s	O
or	O
black	O
s	O
turn	O
to	O
move	O
the	O
general	O
logic	O
behind	O
these	O
choices	O
should	O
be	O
clear	O
basically	O
tesauro	O
tried	O
to	O
represent	O
the	O
position	O
in	O
a	O
straightforward	O
way	O
while	O
keeping	O
the	O
number	O
of	O
units	O
relatively	O
small	O
he	O
provided	O
one	O
unit	O
for	O
each	O
conceptually	O
distinct	O
possibility	O
that	O
seemed	O
likely	O
to	O
be	O
relevant	O
and	O
he	O
scaled	O
them	O
to	O
roughly	O
the	O
same	O
range	O
in	O
this	O
case	O
between	O
and	O
given	O
a	O
representation	O
of	O
a	O
backgammon	B
position	O
the	O
network	O
computed	O
its	O
estimated	O
value	B
in	O
the	O
standard	O
way	O
corresponding	O
to	O
each	O
connection	O
from	O
an	O
input	O
unit	O
to	O
a	O
hidden	O
unit	O
was	O
a	O
real-valued	O
weight	O
signals	O
from	O
each	O
input	O
unit	O
were	O
multiplied	O
by	O
their	O
corresponding	O
weights	O
and	O
summed	O
at	O
the	O
hidden	O
unit	O
the	O
output	O
hj	O
of	O
hidden	O
vthidden	O
units	O
position	O
input	O
unitspredicted	O
probabilityof	O
winning	O
vttd	O
error	O
snetworkwastrainedonalargetrainingcorpusofexemplarymovesprovidedbybackgammonex-pertsandinadditionstartedwithasetoffeaturesspeciallycraftedfortd	O
snetworkwastrainedonalargetrainingcorpusofexemplarymovesprovidedbybackgammonex-pertsandinadditionstartedwithasetoffeaturesspeciallycraftedforhidden	O
chapter	O
applications	B
and	I
case	I
studies	I
unit	O
j	O
was	O
a	O
nonlinear	O
sigmoid	O
function	O
of	O
the	O
weighted	O
sum	O
hj	O
e	O
i	O
wij	O
xi	O
where	O
xi	O
is	O
the	O
value	B
of	O
the	O
ith	O
input	O
unit	O
and	O
wij	O
is	O
the	O
weight	O
of	O
its	O
connection	O
to	O
the	O
jth	O
hidden	O
unit	O
the	O
weights	O
in	O
the	O
network	O
together	O
make	O
up	O
the	O
parameter	O
vector	B
w	O
the	O
output	O
of	O
the	O
sigmoid	O
is	O
always	O
between	O
and	O
and	O
has	O
a	O
natural	O
interpretation	O
as	O
a	O
probability	O
based	O
on	O
a	O
summation	O
of	O
evidence	O
the	O
computation	O
from	O
hidden	O
units	O
to	O
the	O
output	O
unit	O
was	O
entirely	O
analogous	O
each	O
connection	O
from	O
a	O
hidden	O
unit	O
to	O
the	O
output	O
unit	O
had	O
a	O
separate	O
weight	O
the	O
output	O
unit	O
formed	O
the	O
weighted	O
sum	O
and	O
then	O
passed	O
it	O
through	O
the	O
same	O
sigmoid	O
nonlinearity	O
td-gammon	B
used	O
the	O
semi-gradient	O
form	O
of	O
the	O
td	B
algorithm	O
described	O
in	O
section	O
with	O
the	O
gradients	O
computed	O
by	O
the	O
error	O
backpropagation	B
algorithm	O
hinton	O
and	O
williams	O
recall	O
that	O
the	O
general	O
update	O
rule	O
for	O
this	O
case	O
is	O
wt	O
where	O
wt	O
is	O
the	O
vector	B
of	O
all	O
modifiable	O
parameters	O
this	O
case	O
the	O
weights	O
of	O
the	O
network	O
and	O
zt	O
is	O
a	O
vector	B
of	O
eligibility	B
traces	I
one	O
for	O
each	O
component	O
of	O
wt	O
updated	O
by	O
zt	O
zt	O
vstwt	O
with	O
the	O
gradient	B
in	O
this	O
equation	O
can	O
be	O
computed	O
efficiently	O
by	O
the	O
backpropagation	B
procedure	O
for	O
the	O
backgammon	B
application	O
in	O
which	O
and	O
the	O
reward	O
is	O
always	O
zero	O
except	O
upon	O
winning	O
the	O
td	B
error	I
portion	O
of	O
the	O
learning	O
rule	O
is	O
usually	O
just	O
vstw	O
as	O
suggested	O
in	O
figure	O
to	O
apply	O
the	O
learning	O
rule	O
we	O
need	O
a	O
source	O
of	O
backgammon	B
games	O
tesauro	O
obtained	O
an	O
unending	O
sequence	O
of	O
games	O
by	O
playing	O
his	O
learning	O
backgammon	B
player	O
against	O
itself	O
to	O
choose	O
its	O
moves	O
td-gammon	B
considered	O
each	O
of	O
the	O
or	O
so	O
ways	O
it	O
could	O
play	O
its	O
dice	O
roll	O
and	O
the	O
corresponding	O
positions	O
that	O
would	O
result	O
the	O
resulting	O
positions	O
are	O
afterstates	B
as	O
discussed	O
in	O
section	O
the	O
network	O
was	O
consulted	O
to	O
estimate	O
each	O
of	O
their	O
values	O
the	O
move	O
was	O
then	O
selected	O
that	O
would	O
lead	O
to	O
the	O
position	O
with	O
the	O
highest	O
estimated	O
value	B
continuing	O
in	O
this	O
way	O
with	O
td-gammon	B
making	O
the	O
moves	O
for	O
both	O
sides	O
it	O
was	O
possible	O
to	O
easily	O
generate	O
large	O
numbers	O
of	O
backgammon	B
games	O
each	O
game	O
was	O
treated	O
as	O
an	O
episode	O
with	O
the	O
sequence	O
of	O
positions	O
acting	O
as	O
the	O
states	O
tesauro	O
applied	O
the	O
nonlinear	O
td	B
rule	O
fully	O
incrementally	O
that	O
is	O
after	O
each	O
individual	O
move	O
the	O
weights	O
of	O
the	O
network	O
were	O
set	O
initially	O
to	O
small	O
random	O
values	O
the	O
initial	O
evaluations	O
were	O
thus	O
entirely	O
arbitrary	O
because	O
the	O
moves	O
were	O
selected	O
on	O
the	O
basis	O
of	O
these	O
evaluations	O
the	O
initial	O
moves	O
were	O
inevitably	O
poor	O
and	O
the	O
initial	O
games	O
often	O
lasted	O
hundreds	O
or	O
thousands	O
of	O
moves	O
before	O
one	O
side	O
or	O
the	O
other	O
won	O
almost	O
by	O
accident	O
after	O
a	O
few	O
dozen	O
games	O
however	O
performance	O
improved	O
rapidly	O
td-gammon	B
after	O
playing	O
about	O
games	O
against	O
itself	O
td-gammon	B
as	O
described	O
above	O
learned	O
to	O
play	O
approximately	O
as	O
well	O
as	O
the	O
best	O
previous	O
backgammon	B
computer	O
programs	O
this	O
was	O
a	O
striking	O
result	O
because	O
all	O
the	O
previous	O
high-performance	O
computer	O
programs	O
had	O
used	O
extensive	O
backgammon	B
knowledge	O
for	O
example	O
the	O
reigning	O
champion	O
program	O
at	O
the	O
time	O
was	O
arguably	O
neurogammon	O
another	O
program	O
written	O
by	O
tesauro	O
that	O
used	O
a	O
neural	B
network	O
but	O
not	O
td	B
learning	O
neurogammon	O
s	O
network	O
was	O
trained	O
on	O
a	O
large	O
training	O
corpus	O
of	O
exemplary	O
moves	O
provided	O
by	O
backgammon	B
experts	O
and	O
in	O
addition	O
started	O
with	O
a	O
set	O
of	O
features	O
specially	O
crafted	O
for	O
backgammon	B
neurogammon	O
was	O
a	O
highly	O
tuned	O
highly	O
effective	O
backgammon	B
program	O
that	O
decisively	O
won	O
the	O
world	O
backgammon	B
olympiad	O
in	B
td-gammon	B
on	O
the	O
other	O
hand	O
was	O
constructed	O
with	O
essentially	O
zero	O
backgammon	B
knowledge	O
that	O
it	O
was	O
able	O
to	O
do	O
as	O
well	O
as	O
neurogammon	O
and	O
all	O
other	O
approaches	O
is	O
striking	O
testimony	O
to	O
the	O
potential	O
of	O
self-play	O
learning	O
methods	O
the	O
tournament	O
success	O
of	O
td-gammon	B
with	O
zero	O
expert	O
backgammon	B
knowledge	O
suggested	O
an	O
obvious	O
modification	O
add	O
the	O
specialized	O
backgammon	B
features	O
but	O
keep	O
the	O
self-play	O
td	B
learning	O
method	O
this	O
produced	O
td-gammon	B
td-gammon	B
was	O
clearly	O
substantially	O
better	O
than	O
all	O
previous	O
backgammon	B
programs	O
and	O
found	O
serious	O
competition	O
only	O
among	O
human	O
experts	O
later	O
versions	O
of	O
the	O
program	O
tdgammon	O
hidden	O
units	O
and	O
td-gammon	B
hidden	O
units	O
were	O
augmented	O
with	O
a	O
selective	O
two-ply	O
search	O
procedure	O
to	O
select	O
moves	O
these	O
programs	O
looked	O
ahead	O
not	O
just	O
to	O
the	O
positions	O
that	O
would	O
immediately	O
result	O
but	O
also	O
to	O
the	O
opponent	O
s	O
possible	O
dice	O
rolls	O
and	O
moves	O
assuming	O
the	O
opponent	O
always	O
took	O
the	O
move	O
that	O
appeared	O
immediately	O
best	O
for	O
him	O
the	O
expected	B
value	B
of	O
each	O
candidate	O
move	O
was	O
computed	O
and	O
the	O
best	O
was	O
selected	O
to	O
save	O
computer	O
time	O
the	O
second	O
ply	O
of	O
search	O
was	O
conducted	O
only	O
for	O
candidate	O
moves	O
that	O
were	O
ranked	O
highly	O
after	O
the	O
first	O
ply	O
about	O
four	O
or	O
five	O
moves	O
on	O
average	O
two-ply	O
search	O
affected	O
only	O
the	O
moves	O
selected	O
the	O
learning	O
process	O
proceeded	O
exactly	O
as	O
before	O
the	O
final	O
versions	O
of	O
the	O
program	O
td-gammon	B
and	O
used	O
hidden	O
units	O
and	O
a	O
selective	O
three-ply	O
search	O
td-gammon	B
illustrates	O
the	O
combination	O
of	O
learned	O
value	B
functions	O
and	O
decision-time	O
search	O
as	O
in	O
heuristic	B
search	I
and	O
mcts	O
methods	O
in	O
follow-on	O
work	O
tesauro	O
and	O
galperin	O
explored	O
trajectory	B
sampling	I
methods	O
as	O
an	O
alternative	O
to	O
full-width	O
search	O
which	O
reduced	O
the	O
error	O
rate	O
of	O
live	O
play	O
by	O
large	O
numerical	O
factors	O
while	O
keeping	O
the	O
think	O
time	O
reasonable	O
at	O
seconds	O
per	O
move	O
during	O
the	O
tesauro	O
was	O
able	O
to	O
play	O
his	O
programs	O
in	O
a	O
significant	O
number	O
of	O
program	O
td-gammon	B
td-gammon	B
td-gammon	B
td-gammon	B
td-gammon	B
hidden	O
training	O
games	O
units	O
opponents	O
results	O
other	O
programs	O
tied	O
for	O
best	O
robertie	O
magriel	O
pts	O
games	O
various	O
grandmasters	O
pts	O
games	O
pt	O
games	O
pts	O
games	O
robertie	O
kazaros	O
table	O
summary	O
of	O
td-gammon	B
results	O
chapter	O
applications	B
and	I
case	I
studies	I
games	O
against	O
world-class	O
human	O
players	O
a	O
summary	O
of	O
the	O
results	O
is	O
given	O
in	O
table	O
based	O
on	O
these	O
results	O
and	O
analyses	O
by	O
backgammon	B
grandmasters	O
see	O
tesauro	O
td-gammon	B
appeared	O
to	O
play	O
at	O
close	O
to	O
or	O
possibly	O
better	O
than	O
the	O
playing	O
strength	O
of	O
the	O
best	O
human	O
players	O
in	O
the	O
world	O
tesauro	O
reported	O
in	O
a	O
subsequent	O
article	O
the	O
results	O
of	O
an	O
extensive	O
rollout	O
analysis	O
of	O
the	O
move	O
decisions	O
and	O
doubling	O
decisions	O
of	O
td-gammon	B
relative	O
to	O
top	O
human	O
players	O
the	O
conclusion	O
was	O
that	O
td-gammon	B
had	O
a	O
lopsided	O
advantage	O
in	O
piece-movement	O
decisions	O
and	O
a	O
slight	O
edge	O
in	O
doubling	O
decisions	O
over	O
top	O
humans	O
td-gammon	B
had	O
a	O
significant	O
impact	O
on	O
the	O
way	O
the	O
best	O
human	O
players	O
play	O
the	O
game	O
for	O
example	O
it	O
learned	O
to	O
play	O
certain	O
opening	O
positions	O
differently	O
than	O
was	O
the	O
convention	O
among	O
the	O
best	O
human	O
players	O
based	O
on	O
td-gammon	B
s	O
success	O
and	O
further	O
analysis	O
the	O
best	O
human	O
players	O
now	O
play	O
these	O
positions	O
as	O
td-gammon	B
does	O
the	O
impact	O
on	O
human	O
play	O
was	O
greatly	O
accelerated	O
when	O
several	O
other	O
self-teaching	O
neural	B
net	O
backgammon	B
programs	O
inspired	O
by	O
td-gammon	B
such	O
as	O
jellyfish	O
snowie	O
and	O
gnubackgammon	O
became	O
widely	O
available	O
these	O
programs	O
enabled	O
wide	O
dissemination	O
of	O
new	O
knowledge	O
generated	O
by	O
the	O
neural	B
nets	O
resulting	O
in	O
great	O
improvements	O
in	O
the	O
overall	O
caliber	O
of	O
human	O
tournament	O
play	O
samuel	O
s	O
checkers	O
player	O
an	O
important	O
precursor	O
to	O
tesauro	O
s	O
td-gammon	B
was	O
the	O
seminal	O
work	O
of	O
arthur	O
samuel	O
in	O
constructing	O
programs	O
for	O
learning	O
to	O
play	O
checkers	O
samuel	O
was	O
one	O
of	O
the	O
first	O
to	O
make	O
effective	O
use	O
of	O
heuristic	B
search	I
methods	O
and	O
of	O
what	O
we	O
would	O
now	O
call	O
temporal-difference	B
learning	I
his	O
checkers	O
players	O
are	O
instructive	O
case	O
studies	O
in	O
addition	O
to	O
being	O
of	O
historical	O
interest	O
we	O
emphasize	O
the	O
relationship	O
of	O
samuel	O
s	O
methods	O
to	O
modern	O
reinforcement	B
learning	I
methods	O
and	O
try	O
to	O
convey	O
some	O
of	O
samuel	O
s	O
motivation	B
for	O
using	O
them	O
samuel	O
first	O
wrote	O
a	O
checkers-playing	O
program	O
for	O
the	O
ibm	O
in	O
his	O
first	O
learning	O
program	O
was	O
completed	O
in	O
and	O
was	O
demonstrated	O
on	O
television	O
in	O
later	O
versions	O
of	O
the	O
program	O
achieved	O
good	O
though	O
not	O
expert	O
playing	O
skill	O
samuel	O
was	O
attracted	O
to	O
game-playing	O
as	O
a	O
domain	O
for	O
studying	O
machine	O
learning	O
because	O
games	O
are	O
less	O
complicated	O
than	O
problems	O
taken	O
from	O
life	O
while	O
still	O
allowing	O
fruitful	O
study	O
of	O
how	O
heuristic	O
procedures	O
and	O
learning	O
can	O
be	O
used	O
together	O
he	O
chose	O
to	O
study	O
checkers	O
instead	O
of	O
chess	B
because	O
its	O
relative	O
simplicity	O
made	O
it	O
possible	O
to	O
focus	O
more	O
strongly	O
on	O
learning	O
samuel	O
s	O
programs	O
played	O
by	O
performing	O
a	O
lookahead	O
search	O
from	O
each	O
current	O
position	O
they	O
used	O
what	O
we	O
now	O
call	O
heuristic	B
search	I
methods	O
to	O
determine	O
how	O
to	O
expand	O
the	O
search	O
tree	O
and	O
when	O
to	O
stop	O
searching	O
the	O
terminal	O
board	O
positions	O
of	O
each	O
search	O
were	O
evaluated	O
or	O
scored	O
by	O
a	O
value	B
function	I
or	O
scoring	O
polynomial	O
using	O
linear	O
function	B
approximation	I
in	O
this	O
and	O
other	O
respects	O
samuel	O
s	O
work	O
seems	O
to	O
have	O
been	O
inspired	O
by	O
the	O
suggestions	O
of	O
shannon	B
in	O
particular	O
samuel	O
s	O
program	O
was	O
based	O
on	O
shannon	B
s	O
minimax	O
procedure	O
to	O
find	O
the	O
best	O
move	O
from	O
the	O
current	O
position	O
working	O
backward	O
through	O
the	O
search	O
tree	O
from	O
the	O
scored	O
terminal	O
positions	O
each	O
position	O
was	O
given	O
the	O
score	O
of	O
the	O
position	O
that	O
would	O
result	O
from	O
the	O
best	O
move	O
assuming	O
samuel	O
s	O
checkers	O
player	O
that	O
the	O
machine	O
would	O
always	O
try	O
to	O
maximize	O
the	O
score	O
while	O
the	O
opponent	O
would	O
always	O
try	O
to	O
minimize	O
it	O
samuel	O
called	O
this	O
the	O
backed-up	O
score	O
of	O
the	O
position	O
when	O
the	O
minimax	O
procedure	O
reached	O
the	O
search	O
tree	O
s	O
root	O
the	O
current	O
position	O
it	O
yielded	O
the	O
best	O
move	O
under	O
the	O
assumption	O
that	O
the	O
opponent	O
would	O
be	O
using	O
the	O
same	O
evaluation	O
criterion	O
shifted	O
to	O
its	O
point	O
of	O
view	O
some	O
versions	O
of	O
samuel	O
s	O
programs	O
used	O
sophisticated	O
search	B
control	B
methods	O
analogous	O
to	O
what	O
are	O
known	O
as	O
alpha-beta	O
cutoffs	O
see	O
pearl	O
samuel	O
used	O
two	O
main	O
learning	O
methods	O
the	O
simplest	O
of	O
which	O
he	O
called	O
rote	O
learning	O
it	O
consisted	O
simply	O
of	O
saving	O
a	O
description	O
of	O
each	O
board	O
position	O
encountered	O
during	O
play	O
together	O
with	O
its	O
backed-up	O
value	B
determined	O
by	O
the	O
minimax	O
procedure	O
the	O
result	O
was	O
that	O
if	O
a	O
position	O
that	O
had	O
already	O
been	O
encountered	O
were	O
to	O
occur	O
again	O
as	O
a	O
terminal	O
position	O
of	O
a	O
search	O
tree	O
the	O
depth	O
of	O
the	O
search	O
was	O
effectively	O
amplified	O
because	O
this	O
position	O
s	O
stored	O
value	B
cached	O
the	O
results	O
of	O
one	O
or	O
more	O
searches	O
conducted	O
earlier	O
one	O
initial	O
problem	O
was	O
that	O
the	O
program	O
was	O
not	O
encouraged	O
to	O
move	O
along	O
the	O
most	O
direct	O
path	O
to	O
a	O
win	O
samuel	O
gave	O
it	O
a	O
a	O
sense	O
of	O
direction	O
by	O
decreasing	O
a	O
position	O
s	O
value	B
a	O
small	O
amount	O
each	O
time	O
it	O
was	O
backed	O
up	O
a	O
level	O
a	O
ply	O
during	O
the	O
minimax	O
analysis	O
if	O
the	O
program	O
is	O
now	O
faced	O
with	O
a	O
choice	O
of	O
board	O
positions	O
whose	O
scores	O
differ	O
only	O
by	O
the	O
ply	O
number	O
it	O
will	O
automatically	O
make	O
the	O
most	O
advantageous	O
choice	O
choosing	O
a	O
low-ply	O
alternative	O
if	O
winning	O
and	O
a	O
high-ply	O
alternative	O
if	O
losing	O
p	O
samuel	O
found	O
this	O
discounting-like	O
technique	O
essential	O
to	O
successful	O
learning	O
rote	O
learning	O
produced	O
slow	O
but	O
continual	O
improvement	O
that	O
was	O
most	O
effective	O
for	O
opening	O
and	O
endgame	O
play	O
his	O
program	O
became	O
a	O
better-than-average	O
novice	O
after	O
learning	O
from	O
many	O
games	O
against	O
itself	O
a	O
variety	O
of	O
human	O
opponents	O
and	O
from	O
book	O
games	O
in	O
a	O
supervised	B
learning	I
mode	O
rote	O
learning	O
and	O
other	O
aspects	O
of	O
samuel	O
s	O
work	O
strongly	O
suggest	O
the	O
essential	O
idea	O
of	O
temporal-difference	B
learning	I
that	O
the	O
value	B
of	O
a	O
state	B
should	O
equal	O
the	O
value	B
of	O
likely	O
following	O
states	O
samuel	O
came	O
closest	O
to	O
this	O
idea	O
in	O
his	O
second	O
learning	O
method	O
his	O
learning	O
by	O
generalization	O
procedure	O
for	O
modifying	O
the	O
parameters	O
of	O
the	O
value	B
function	I
samuel	O
s	O
method	O
was	O
the	O
same	O
in	O
concept	O
as	O
that	O
used	O
much	O
later	O
by	O
tesauro	O
in	B
td-gammon	B
he	O
played	O
his	O
program	O
many	O
games	O
against	O
another	O
version	O
of	O
itself	O
and	O
performed	O
an	O
update	O
after	O
each	O
move	O
the	O
idea	O
of	O
samuel	O
s	O
update	O
is	O
suggested	O
by	O
the	O
backup	B
diagram	I
in	O
figure	O
each	O
open	O
circle	O
represents	O
a	O
position	O
where	O
the	O
program	O
moves	O
next	O
an	O
on-move	O
position	O
and	O
each	O
solid	O
circle	O
represents	O
a	O
position	O
where	O
the	O
opponent	O
moves	O
next	O
an	O
update	O
was	O
made	O
to	O
the	O
value	B
of	O
each	O
on-move	O
position	O
after	O
a	O
move	O
by	O
each	O
side	O
resulting	O
in	O
a	O
second	O
on-move	O
position	O
the	O
update	O
was	O
toward	O
the	O
minimax	O
value	B
of	O
a	O
search	O
launched	O
from	O
the	O
second	O
on-move	O
position	O
thus	O
the	O
overall	O
effect	O
was	O
that	O
of	O
a	O
backing-up	O
over	O
one	O
full	O
move	O
of	O
real	O
events	O
and	O
then	O
a	O
search	O
over	O
possible	O
events	O
as	O
suggested	O
by	O
figure	O
samuel	O
s	O
actual	O
algorithm	O
was	O
significantly	O
more	O
complex	O
than	O
this	O
for	O
computational	O
reasons	O
but	O
this	O
was	O
the	O
basic	O
idea	O
samuel	O
did	O
not	O
include	O
explicit	O
rewards	O
instead	O
he	O
fixed	O
the	O
weight	O
of	O
the	O
most	O
important	O
feature	O
the	O
piece	O
advantage	O
feature	O
which	O
measured	O
the	O
number	O
of	O
pieces	O
the	O
program	O
had	O
relative	O
to	O
how	O
many	O
its	O
opponent	O
had	O
giving	O
higher	O
weight	O
to	O
kings	O
and	O
including	O
refinements	O
so	O
that	O
it	O
was	O
better	O
to	O
trade	O
pieces	O
when	O
winning	O
than	O
when	O
losing	O
thus	O
the	O
goal	O
of	O
samuel	O
s	O
program	O
was	O
to	O
improve	O
its	O
piece	O
advantage	O
which	O
in	O
chapter	O
applications	B
and	I
case	I
studies	I
figure	O
the	O
backup	B
diagram	I
for	O
samuel	O
s	O
checkers	O
player	O
checkers	O
is	O
highly	O
correlated	O
with	O
winning	O
however	O
samuel	O
s	O
learning	O
method	O
may	O
have	O
been	O
missing	O
an	O
essential	O
part	O
of	O
a	O
sound	O
temporal-difference	O
algorithm	O
temporal-difference	B
learning	I
can	O
be	O
viewed	O
as	O
a	O
way	O
of	O
making	O
a	O
value	B
function	I
consistent	O
with	O
itself	O
and	O
this	O
we	O
can	O
clearly	O
see	O
in	O
samuel	O
s	O
method	O
but	O
also	O
needed	O
is	O
a	O
way	O
of	O
tying	O
the	O
value	B
function	I
to	O
the	O
true	O
value	B
of	O
the	O
states	O
we	O
have	O
enforced	O
this	O
via	O
rewards	O
and	O
by	O
discounting	B
or	O
giving	O
a	O
fixed	O
value	B
to	O
the	O
terminal	O
state	B
but	O
samuel	O
s	O
method	O
included	O
no	O
rewards	O
and	O
no	O
special	O
treatment	O
of	O
the	O
terminal	O
positions	O
of	O
games	O
as	O
samuel	O
himself	O
pointed	O
out	O
his	O
value	B
function	I
could	O
have	O
become	O
consistent	O
merely	O
by	O
giving	O
a	O
constant	O
value	B
to	O
all	O
positions	O
he	O
hoped	O
to	O
discourage	O
such	O
solutions	O
by	O
giving	O
his	O
piece-advantage	O
term	O
a	O
large	O
nonmodifiable	O
weight	O
but	O
although	O
this	O
may	O
decrease	O
the	O
likelihood	O
of	O
finding	O
useless	O
evaluation	O
functions	O
it	O
does	O
not	O
prohibit	O
them	O
for	O
example	O
a	O
constant	O
function	O
could	O
still	O
be	O
attained	O
by	O
setting	O
the	O
modifiable	O
weights	O
so	O
as	O
to	O
cancel	O
the	O
effect	O
of	O
the	O
nonmodifiable	O
one	O
because	O
samuel	O
s	O
learning	O
procedure	O
was	O
not	O
constrained	O
to	O
find	O
useful	O
evaluation	O
functions	O
it	O
should	O
have	O
been	O
possible	O
for	O
it	O
to	O
become	O
worse	O
with	O
experience	O
in	O
fact	O
samuel	O
reported	O
observing	O
this	O
during	O
extensive	O
self-play	O
training	O
sessions	O
to	O
get	O
the	O
program	O
improving	O
again	O
samuel	O
had	O
to	O
intervene	O
and	O
set	O
the	O
weight	O
with	O
the	O
largest	O
absolute	O
value	B
back	O
to	O
zero	O
his	O
interpretation	O
was	O
that	O
this	O
drastic	O
intervention	O
jarred	O
the	O
program	O
out	O
of	O
local	O
optima	O
but	O
another	O
possibility	O
is	O
that	O
it	O
jarred	O
the	O
program	O
out	O
of	O
evaluation	O
functions	O
that	O
were	O
consistent	O
but	O
had	O
little	O
to	O
do	O
with	O
winning	O
or	O
losing	O
the	O
game	O
despite	O
these	O
potential	O
problems	O
samuel	O
s	O
checkers	O
player	O
using	O
the	O
generalization	O
learning	O
method	O
approached	O
better-than-average	O
play	O
fairly	O
good	O
amateur	O
opponents	O
characterized	O
it	O
as	O
tricky	O
but	O
beatable	O
in	O
contrast	O
to	O
the	O
rote-learning	O
version	O
this	O
version	O
was	O
able	O
to	O
develop	O
a	O
good	O
middle	O
game	O
but	O
remained	O
weak	O
in	O
opening	O
and	O
endgame	O
play	O
this	O
program	O
also	O
included	O
an	O
ability	O
to	O
search	O
through	O
sets	O
of	O
hypothetical	O
eventsactual	O
eventsbackup	O
watson	O
s	O
daily-double	O
wagering	O
features	O
to	O
find	O
those	O
that	O
were	O
most	O
useful	O
in	O
forming	O
the	O
value	B
function	I
a	O
later	O
version	O
included	O
refinements	O
in	O
its	O
search	O
procedure	O
such	O
as	O
alpha-beta	O
pruning	O
extensive	O
use	O
of	O
a	O
supervised	B
learning	I
mode	O
called	O
book	O
learning	O
and	O
hierarchical	O
lookup	O
tables	O
called	O
signature	O
tables	O
to	O
represent	O
the	O
value	B
function	I
instead	O
of	O
linear	O
function	B
approximation	I
this	O
version	O
learned	O
to	O
play	O
much	O
better	O
than	O
the	O
program	O
though	O
still	O
not	O
at	O
a	O
master	O
level	O
samuel	O
s	O
checkers-playing	O
program	O
was	O
widely	O
recognized	O
as	O
a	O
significant	O
achievement	O
in	O
artificial	B
intelligence	I
and	O
machine	O
learning	O
watson	O
s	O
daily-double	O
wagering	O
ibm	O
is	O
the	O
system	O
developed	O
by	O
a	O
team	O
of	O
ibm	O
researchers	O
to	O
play	O
the	O
popular	O
tv	O
quiz	O
show	O
it	O
gained	O
fame	O
in	O
by	O
winning	O
first	O
prize	O
in	O
an	O
exhibition	O
match	O
against	O
human	O
champions	O
although	O
the	O
main	O
technical	O
achievement	O
demonstrated	O
by	O
watson	O
was	O
its	O
ability	O
to	O
quickly	O
and	O
accurately	O
answer	O
natural	O
language	O
questions	O
over	O
broad	O
areas	O
of	O
general	O
knowledge	O
its	O
winning	O
jeopardy	O
performance	O
also	O
relied	O
on	O
sophisticated	O
decision-making	O
strategies	O
for	O
critical	O
parts	O
of	O
the	O
game	O
tesauro	O
gondek	O
lechner	O
fan	O
and	O
prager	O
adapted	O
tesauro	O
s	O
td-gammon	B
system	O
described	O
above	O
to	O
create	O
the	O
strategy	O
used	O
by	O
watson	O
in	O
daily-double	O
wagering	O
in	O
its	O
celebrated	O
winning	O
performance	O
against	O
human	O
champions	O
these	O
authors	O
report	O
that	O
the	O
effectiveness	O
of	O
this	O
wagering	O
strategy	O
went	O
well	O
beyond	O
what	O
human	O
players	O
are	O
able	O
to	O
do	O
in	O
live	O
game	O
play	O
and	O
that	O
it	O
along	O
with	O
other	O
advanced	O
strategies	O
was	O
an	O
important	O
contributor	O
to	O
watson	O
s	O
impressive	O
winning	O
performance	O
here	O
we	O
focus	O
only	O
on	O
dd	O
wagering	O
because	O
it	O
is	O
the	O
component	O
of	O
watson	O
that	O
owes	O
the	O
most	O
to	O
reinforcement	B
learning	I
jeopardy	O
is	O
played	O
by	O
three	O
contestants	O
who	O
face	O
a	O
board	O
showing	O
squares	O
each	O
of	O
which	O
hides	O
a	O
clue	O
and	O
has	O
a	O
dollar	O
value	B
the	O
squares	O
are	O
arranged	O
in	O
six	O
columns	O
each	O
corresponding	O
to	O
a	O
different	O
category	O
a	O
contestant	O
selects	O
a	O
square	O
the	O
host	O
reads	O
the	O
square	O
s	O
clue	O
and	O
each	O
contestant	O
may	O
choose	O
to	O
respond	O
to	O
the	O
clue	O
by	O
sounding	O
a	O
buzzer	O
buzzing	O
in	O
the	O
first	O
contestant	O
to	O
buzz	O
in	O
gets	O
to	O
try	O
responding	O
to	O
the	O
clue	O
if	O
this	O
contestant	O
s	O
response	O
is	O
correct	O
their	O
score	O
increases	O
by	O
the	O
dollar	O
value	B
of	O
the	O
square	O
if	O
their	O
response	O
is	O
not	O
correct	O
or	O
if	O
they	O
do	O
not	O
respond	O
within	O
five	O
seconds	O
their	O
score	O
decreases	O
by	O
that	O
amount	O
and	O
the	O
other	O
contestants	O
get	O
a	O
chance	O
to	O
buzz	O
in	O
to	O
respond	O
to	O
the	O
same	O
clue	O
one	O
or	O
two	O
squares	O
on	O
the	O
game	O
s	O
current	O
round	O
are	O
special	O
dd	O
squares	O
a	O
contestant	O
who	O
selects	O
one	O
of	O
these	O
gets	O
an	O
exclusive	O
opportunity	O
to	O
respond	O
to	O
the	O
square	O
s	O
clue	O
and	O
has	O
to	O
decide	O
before	O
the	O
clue	O
is	O
revealed	O
on	O
how	O
much	O
to	O
wager	O
or	O
bet	O
the	O
bet	O
has	O
to	O
be	O
greater	O
than	O
five	O
dollars	O
but	O
not	O
greater	O
than	O
the	O
contestant	O
s	O
current	O
score	O
if	O
the	O
contestant	O
responds	O
correctly	O
to	O
the	O
dd	O
clue	O
their	O
score	O
increases	O
by	O
the	O
bet	O
amount	O
otherwise	O
it	O
decreases	O
by	O
the	O
bet	O
amount	O
at	O
the	O
end	O
of	O
each	O
game	O
is	O
a	O
final	O
jeopardy	O
round	O
in	O
which	O
each	O
contestant	O
writes	O
down	O
a	O
sealed	O
bet	O
and	O
then	O
writes	O
an	O
answer	O
after	O
the	O
clue	O
is	O
read	O
trademark	O
of	O
ibm	O
corp	O
trademark	O
of	O
jeopardy	O
productions	O
inc	O
chapter	O
applications	B
and	I
case	I
studies	I
the	O
contestant	O
with	O
the	O
highest	O
score	O
after	O
three	O
rounds	O
of	O
play	O
a	O
round	O
consists	O
of	O
revealing	O
all	O
clues	O
is	O
the	O
winner	O
the	O
game	O
has	O
many	O
other	O
details	O
but	O
these	O
are	O
enough	O
to	O
appreciate	O
the	O
importance	O
of	O
dd	O
wagering	O
winning	O
or	O
losing	O
often	O
depends	O
on	O
a	O
contestant	O
s	O
dd	O
wagering	O
strategy	O
whenever	O
watson	O
selected	O
a	O
dd	O
square	O
it	O
chose	O
its	O
bet	O
by	O
comparing	O
action	B
values	O
qs	O
bet	O
that	O
estimated	O
the	O
probability	O
of	O
a	O
win	O
from	O
the	O
current	O
game	O
state	B
s	O
for	O
each	O
round-dollar	O
legal	O
bet	O
except	O
for	O
some	O
risk-abatement	O
measures	O
described	O
below	O
watson	O
selected	O
the	O
bet	O
with	O
the	O
maximum	O
action	B
value	B
action	B
values	O
were	O
computed	O
whenever	O
a	O
betting	O
decision	O
was	O
needed	O
by	O
using	O
two	O
types	O
of	O
estimates	O
that	O
were	O
learned	O
before	O
any	O
live	O
game	O
play	O
took	O
place	O
the	O
first	O
were	O
estimated	O
values	O
of	O
the	O
afterstates	B
that	O
would	O
result	O
from	O
selecting	O
each	O
legal	O
bet	O
these	O
estimates	O
were	O
obtained	O
from	O
a	O
state-value	O
function	O
v	O
defined	O
by	O
parameters	O
w	O
that	O
gave	O
estimates	O
of	O
the	O
probability	O
of	O
a	O
win	O
for	O
watson	O
from	O
any	O
game	O
state	B
the	O
second	O
estimates	O
used	O
to	O
compute	O
action	B
values	O
gave	O
the	O
in-category	O
dd	O
confidence	O
pdd	O
which	O
estimated	O
the	O
likelihood	O
that	O
watson	O
would	O
respond	O
correctly	O
to	O
the	O
as-yet	O
unrevealed	O
dd	O
clue	O
tesauro	O
et	O
al	O
used	O
the	O
reinforcement	B
learning	I
approach	O
of	O
td-gammon	B
described	O
above	O
to	O
learn	O
v	O
a	O
straightforward	O
combination	O
of	O
nonlinear	O
td	B
using	O
a	O
multilayer	O
neural	B
network	O
with	O
weights	O
w	O
trained	O
by	O
backpropagating	O
td	B
errors	O
during	O
many	O
simulated	O
games	O
states	O
were	O
represented	O
to	O
the	O
network	O
by	O
feature	O
vectors	O
specifically	O
designed	O
for	O
jeopardy	O
features	O
included	O
the	O
current	O
scores	O
of	O
the	O
three	O
players	O
how	O
many	O
dds	O
remained	O
the	O
total	O
dollar	O
value	B
of	O
the	O
remaining	O
clues	O
and	O
other	O
information	O
related	O
to	O
the	O
amount	O
of	O
play	O
left	O
in	O
the	O
game	O
unlike	O
td-gammon	B
which	O
learned	O
by	O
self-play	O
watson	O
s	O
v	O
was	O
learned	O
over	O
millions	O
of	O
simulated	O
games	O
against	O
carefullycrafted	O
models	O
of	O
human	O
players	O
in-category	O
confidence	O
estimates	O
were	O
conditioned	O
on	O
the	O
number	O
of	O
right	O
responses	O
r	O
and	O
wrong	O
responses	O
w	O
that	O
watson	O
gave	O
in	O
previouslyplayed	O
clues	O
in	O
the	O
current	O
category	O
the	O
dependencies	O
on	O
w	O
were	O
estimated	O
from	O
watson	O
s	O
actual	O
accuracies	O
over	O
many	O
thousands	O
of	O
historical	O
categories	O
with	O
the	O
previously	O
learned	O
value	B
function	I
v	O
and	O
in-category	O
dd	O
confidence	O
pdd	O
watson	O
computed	O
qs	O
bet	O
for	O
each	O
legal	O
round-dollar	O
bet	O
as	O
follows	O
qs	O
bet	O
pdd	O
vsw	O
bet	O
pdd	O
vsw	O
bet	O
where	O
sw	O
is	O
watson	O
s	O
current	O
score	O
and	O
v	O
gives	O
the	O
estimated	O
value	B
for	O
the	O
game	O
state	B
after	O
watson	O
s	O
response	O
to	O
the	O
dd	O
clue	O
which	O
is	O
either	O
correct	O
or	O
incorrect	O
computing	O
an	O
action	B
value	B
this	O
way	O
corresponds	O
to	O
the	O
insight	O
from	O
exercise	O
that	O
an	O
action	B
value	B
is	O
the	O
expected	B
next	O
state	B
value	B
given	O
the	O
action	B
that	O
here	O
it	O
is	O
the	O
expected	B
next	O
afterstate	O
value	B
because	O
the	O
full	O
next	O
state	B
of	O
the	O
entire	O
game	O
depends	O
on	O
the	O
next	O
square	O
selection	O
tesauro	O
et	O
al	O
found	O
that	O
selecting	O
bets	O
by	O
maximizing	O
action	B
values	O
incurred	O
a	O
frightening	O
amount	O
of	O
risk	O
meaning	O
that	O
if	O
watson	O
s	O
response	O
to	O
the	O
clue	O
happened	O
to	O
be	O
wrong	O
the	O
loss	O
could	O
be	O
disastrous	O
for	O
its	O
chances	O
of	O
winning	O
to	O
decrease	O
the	O
downside	O
risk	O
of	O
a	O
wrong	O
answer	O
tesauro	O
et	O
al	O
adjusted	O
by	O
subtracting	O
a	O
small	O
fraction	O
of	O
the	O
standard	O
deviation	O
over	O
watson	O
s	O
correctincorrect	O
afterstate	O
evaluations	O
they	O
further	O
reduced	O
risk	O
by	O
prohibiting	O
bets	O
that	O
would	O
cause	O
the	O
wrong-answer	O
afterstate	O
value	B
to	O
decrease	O
below	O
a	O
certain	O
limit	O
these	O
measures	O
slightly	O
reduced	O
watson	O
s	O
expectation	O
of	O
winning	O
but	O
they	O
significantly	O
reduced	O
downside	O
risk	O
not	O
only	O
in	O
terms	O
of	O
watson	O
s	O
daily-double	O
wagering	O
average	O
risk	O
per	O
dd	O
bet	O
but	O
even	O
more	O
so	O
in	O
extreme-risk	O
scenarios	O
where	O
a	O
risk-neutral	O
watson	O
would	O
bet	O
most	O
or	O
all	O
of	O
its	O
bankroll	O
why	O
was	O
the	O
td-gammon	B
method	O
of	O
self-play	O
not	O
used	O
to	O
learn	O
the	O
critical	O
value	B
function	I
v	O
learning	O
from	O
self-play	O
in	O
jeopardy	O
would	O
not	O
have	O
worked	O
very	O
well	O
because	O
watson	O
was	O
so	O
different	O
from	O
any	O
human	O
contestant	O
self-play	O
would	O
have	O
led	O
to	O
exploration	O
of	O
state	B
space	O
regions	O
that	O
are	O
not	O
typical	O
for	O
play	O
against	O
human	O
opponents	O
particularly	O
human	O
champions	O
in	O
addition	O
unlike	O
backgammon	B
jeopardy	O
is	O
a	O
game	O
of	O
imperfect	O
information	O
because	O
contestants	O
do	O
not	O
have	O
access	O
to	O
all	O
the	O
information	O
influencing	O
their	O
opponents	O
play	O
in	O
particular	O
jeopardy	O
contestants	O
do	O
not	O
know	O
how	O
much	O
confidence	O
their	O
opponents	O
have	O
for	O
responding	O
to	O
clues	O
in	O
the	O
various	O
categories	O
self-play	O
would	O
have	O
been	O
something	O
like	O
playing	O
poker	O
with	O
someone	O
who	O
is	O
holding	O
the	O
same	O
cards	O
that	O
you	O
hold	O
as	O
a	O
result	O
of	O
these	O
complications	O
much	O
of	O
the	O
effort	O
in	O
developing	O
watson	O
s	O
ddwagering	O
strategy	O
was	O
devoted	O
to	O
creating	O
good	O
models	O
of	O
human	O
opponents	O
the	O
models	O
did	O
not	O
address	O
the	O
natural	O
language	O
aspect	O
of	O
the	O
game	O
but	O
were	O
instead	O
stochastic	O
process	O
models	O
of	O
events	O
that	O
can	O
occur	O
during	O
play	O
statistics	O
were	O
extracted	O
from	O
an	O
extensive	O
fan-created	O
archive	O
of	O
game	O
information	O
from	O
the	O
beginning	O
of	O
the	O
show	O
to	O
the	O
present	O
day	O
the	O
archive	O
includes	O
information	O
such	O
as	O
the	O
ordering	O
of	O
the	O
clues	O
right	O
and	O
wrong	O
contestant	O
answers	O
dd	O
locations	O
and	O
dd	O
and	O
fj	O
bets	O
for	O
nearly	O
clues	O
three	O
models	O
were	O
constructed	O
an	O
average	O
contestant	O
model	O
on	O
all	O
the	O
data	O
a	O
champion	O
model	O
on	O
statistics	O
from	O
games	O
with	O
the	O
best	O
players	O
and	O
a	O
grand	O
champion	O
model	O
on	O
statistics	O
from	O
games	O
with	O
the	O
best	O
players	O
in	O
addition	O
to	O
serving	O
as	O
opponents	O
during	O
learning	O
the	O
models	O
were	O
used	O
to	O
asses	O
the	O
benefits	O
produced	O
by	O
the	O
learned	O
dd-wagering	O
strategy	O
watson	O
s	O
win	O
rate	O
in	O
simulation	O
when	O
it	O
used	O
a	O
baseline	B
heuristic	O
dd-wagering	O
strategy	O
was	O
when	O
it	O
used	O
the	O
learned	O
values	O
and	O
a	O
default	O
confidence	O
value	B
its	O
win	O
rate	O
increased	O
to	O
and	O
with	O
live	O
in-category	O
confidence	O
it	O
was	O
tesauro	O
et	O
al	O
regarded	O
this	O
as	O
a	O
significant	O
improvement	O
given	O
that	O
the	O
dd	O
wagering	O
was	O
needed	O
only	O
about	O
to	O
times	O
in	O
each	O
game	O
because	O
watson	O
had	O
only	O
a	O
few	O
seconds	O
to	O
bet	O
as	O
well	O
as	O
to	O
select	O
squares	O
and	O
decide	O
whether	O
or	O
not	O
to	O
buzz	O
in	O
the	O
computation	O
time	O
needed	O
to	O
make	O
these	O
decisions	O
was	O
a	O
critical	O
factor	O
the	O
neural	B
network	O
implementation	O
of	O
v	O
allowed	O
dd	O
bets	O
to	O
be	O
made	O
quickly	O
enough	O
to	O
meet	O
the	O
time	O
constraints	O
of	O
live	O
play	O
however	O
once	O
games	O
could	O
be	O
simulated	O
fast	O
enough	O
through	O
improvements	O
in	O
the	O
simulation	O
software	O
near	O
the	O
end	O
of	O
a	O
game	O
it	O
was	O
feasible	O
to	O
estimate	O
the	O
value	B
of	O
bets	O
by	O
averaging	O
over	O
many	O
monte-carlo	O
trials	O
in	O
which	O
the	O
consequence	O
of	O
each	O
bet	O
was	O
determined	O
by	O
simulating	O
play	O
to	O
the	O
game	O
s	O
end	O
selecting	O
endgame	O
dd	O
bets	O
in	O
live	O
play	O
based	O
on	O
monte-carlo	O
trials	O
instead	O
of	O
the	O
neural	B
network	O
significantly	O
improved	O
watson	O
s	O
performance	O
because	O
errors	O
in	O
value	B
estimates	O
in	O
endgames	O
could	O
seriously	O
affect	O
its	O
chances	O
of	O
winning	O
making	O
all	O
the	O
decisions	O
via	O
monte-carlo	O
trials	O
might	O
have	O
led	O
to	O
better	O
wagering	O
decisions	O
but	O
this	O
was	O
simply	O
impossible	O
given	O
the	O
complexity	O
of	O
the	O
game	O
and	O
the	O
time	O
constraints	O
of	O
live	O
play	O
although	O
its	O
ability	O
to	O
quickly	O
and	O
accurately	O
answer	O
natural	O
language	O
questions	O
stands	O
out	O
as	O
watson	O
s	O
major	O
achievement	O
all	O
of	O
its	O
sophisticated	O
decision	O
strategies	O
chapter	O
applications	B
and	I
case	I
studies	I
contributed	O
to	O
its	O
impressive	O
defeat	O
of	O
human	O
champions	O
according	O
to	O
tesauro	O
et	O
al	O
it	O
is	O
plainly	O
evident	O
that	O
our	O
strategy	O
algorithms	O
achieve	O
a	O
level	O
of	O
quantitative	O
precision	O
and	O
real-time	O
performance	O
that	O
exceeds	O
human	O
capabilities	O
this	O
is	O
particularly	O
true	O
in	O
the	O
cases	O
of	O
dd	O
wagering	O
and	O
endgame	O
buzzing	O
where	O
humans	O
simply	O
cannot	O
come	O
close	O
to	O
matching	O
the	O
precise	O
equity	O
and	O
confidence	O
estimates	O
and	O
complex	O
decision	O
calculations	O
performed	O
by	O
watson	O
optimizing	B
memory	I
control	B
most	O
computers	O
use	O
dynamic	O
random	O
access	O
memory	O
as	O
their	O
main	O
memory	O
because	O
of	O
its	O
low	O
cost	O
and	O
high	O
capacity	O
the	O
job	O
of	O
a	O
dram	O
memory	O
controller	O
is	O
to	O
efficiently	O
use	O
the	O
interface	O
between	O
the	O
processor	O
chip	O
and	O
an	O
off-chip	O
dram	O
system	O
to	O
provide	O
the	O
high-bandwidth	O
and	O
low-latency	O
data	O
transfer	O
necessary	O
for	O
highspeed	O
program	O
execution	O
a	O
memory	O
controller	O
needs	O
to	O
deal	O
with	O
dynamically	O
changing	O
patterns	O
of	O
readwrite	O
requests	O
while	O
adhering	O
to	O
a	O
large	O
number	O
of	O
timing	O
and	O
resource	O
constraints	O
required	O
by	O
the	O
hardware	O
this	O
is	O
a	O
formidable	O
scheduling	O
problem	O
especially	O
with	O
modern	O
processors	O
with	O
multiple	O
cores	O
sharing	O
the	O
same	O
dram	O
ipek	O
mutlu	O
mart	O
nez	O
and	O
caruana	O
mart	O
nez	O
and	O
ipek	O
designed	O
a	O
reinforcement	B
learning	I
memory	O
controller	O
and	O
demonstrated	O
that	O
it	O
can	O
significantly	O
improve	O
the	O
speed	O
of	O
program	O
execution	O
over	O
what	O
was	O
possible	O
with	O
conventional	O
controllers	O
at	O
the	O
time	O
of	O
their	O
research	O
they	O
were	O
motivated	O
by	O
limitations	O
of	O
existing	O
state-of-the-art	O
controllers	O
that	O
used	O
policies	O
that	O
did	O
not	O
take	O
advantage	O
of	O
past	O
scheduling	O
experience	O
and	O
did	O
not	O
account	O
for	O
long-term	O
consequences	O
of	O
scheduling	O
decisions	O
ipek	O
et	O
al	O
s	O
project	O
was	O
carried	O
out	O
by	O
means	O
of	O
simulation	O
but	O
they	O
designed	O
the	O
controller	O
at	O
the	O
detailed	O
level	O
of	O
the	O
hardware	O
needed	O
to	O
implement	O
it	O
including	O
the	O
learning	O
algorithm	O
directly	O
on	O
a	O
processor	O
chip	O
accessing	O
dram	O
involves	O
a	O
number	O
of	O
steps	O
that	O
have	O
to	O
be	O
done	O
according	O
to	O
strict	O
time	O
constraints	O
dram	O
systems	O
consist	O
of	O
multiple	O
dram	O
chips	O
each	O
containing	O
multiple	O
rectangular	O
arrays	O
of	O
storage	O
cells	O
arranged	O
in	O
rows	O
and	O
columns	O
each	O
cell	O
stores	O
a	O
bit	O
as	O
the	O
charge	O
on	O
a	O
capacitor	O
because	O
the	O
charge	O
decreases	O
over	O
time	O
each	O
dram	O
cell	O
needs	O
to	O
be	O
recharged	O
refreshed	O
every	O
few	O
milliseconds	O
to	O
prevent	O
memory	O
content	O
from	O
being	O
lost	O
this	O
need	O
to	O
refresh	O
the	O
cells	O
is	O
why	O
dram	O
is	O
called	O
dynamic	O
each	O
cell	O
array	O
has	O
a	O
row	O
buffer	O
that	O
holds	O
a	O
row	O
of	O
bits	O
that	O
can	O
be	O
transferred	O
into	O
or	O
out	O
of	O
one	O
of	O
the	O
array	O
s	O
rows	O
an	O
activate	O
command	O
opens	O
a	O
row	O
which	O
means	O
moving	O
the	O
contents	O
of	O
the	O
row	O
whose	O
address	O
is	O
indicated	O
by	O
the	O
command	O
into	O
the	O
row	O
buffer	O
with	O
a	O
row	O
open	O
the	O
controller	O
can	O
issue	O
read	O
and	O
write	O
commands	O
to	O
the	O
cell	O
array	O
each	O
read	O
command	O
transfers	O
a	O
word	O
short	O
sequence	O
of	O
consecutive	O
bits	O
in	O
the	O
row	O
buffer	O
to	O
the	O
external	O
data	O
bus	O
and	O
each	O
write	O
command	O
transfers	O
a	O
word	O
in	O
the	O
external	O
data	O
bus	O
to	O
the	O
row	O
buffer	O
before	O
a	O
different	O
row	O
can	O
be	O
opened	O
a	O
precharge	O
command	O
must	O
be	O
issued	O
which	O
transfers	O
the	O
updated	O
data	O
in	O
the	O
row	O
buffer	O
back	O
into	O
the	O
addressed	O
row	O
of	O
the	O
cell	O
array	O
after	O
this	O
another	O
activate	O
command	O
can	O
open	O
a	O
new	O
row	O
to	O
be	O
accessed	O
read	O
and	O
write	O
commands	O
are	O
column	O
optimizing	B
memory	I
control	B
commands	O
because	O
they	O
sequentially	O
transfer	O
bits	O
into	O
or	O
out	O
of	O
columns	O
of	O
the	O
row	O
buffer	O
multiple	O
bits	O
can	O
be	O
transferred	O
without	O
re-opening	O
the	O
row	O
read	O
and	O
write	O
commands	O
to	O
the	O
currently-open	O
row	O
can	O
be	O
carried	O
out	O
more	O
quickly	O
than	O
accessing	O
a	O
different	O
row	O
which	O
would	O
involve	O
additional	O
row	O
commands	O
precharge	O
and	O
activate	O
this	O
is	O
sometimes	O
referred	O
to	O
as	O
row	O
locality	O
a	O
memory	O
controller	O
maintains	O
a	O
memory	O
transaction	O
queue	O
that	O
stores	O
memory-access	O
requests	O
from	O
the	O
processors	O
sharing	O
the	O
memory	O
system	O
the	O
controller	O
has	O
to	O
process	O
requests	O
by	O
issuing	O
commands	O
to	O
the	O
memory	O
system	O
while	O
adhering	O
to	O
a	O
large	O
number	O
of	O
timing	O
constraints	O
a	O
controller	O
s	O
policy	B
for	O
scheduling	O
access	O
requests	O
can	O
have	O
a	O
large	O
effect	O
on	O
the	O
performance	O
of	O
the	O
memory	O
system	O
such	O
as	O
the	O
average	O
latency	O
with	O
which	O
requests	O
can	O
be	O
satisfied	O
and	O
the	O
throughput	O
the	O
system	O
is	O
capable	O
of	O
achieving	O
the	O
simplest	O
scheduling	O
strategy	O
handles	O
access	O
requests	O
in	O
the	O
order	O
in	O
which	O
they	O
arrive	O
by	O
issuing	O
all	O
the	O
commands	O
required	O
by	O
the	O
request	O
before	O
beginning	O
to	O
service	O
the	O
next	O
one	O
but	O
if	O
the	O
system	O
is	O
not	O
ready	O
for	O
one	O
of	O
these	O
commands	O
or	O
executing	O
a	O
command	O
would	O
result	O
in	O
resources	O
being	O
underutilized	O
due	O
to	O
timing	O
constraints	O
arising	O
from	O
servicing	O
that	O
one	O
command	O
it	O
makes	O
sense	O
to	O
begin	O
servicing	O
a	O
newer	O
request	O
before	O
finishing	O
the	O
older	O
one	O
policies	O
can	O
gain	O
efficiency	O
by	O
reordering	O
requests	O
for	O
example	O
by	O
giving	O
priority	O
to	O
read	O
requests	O
over	O
write	O
requests	O
or	O
by	O
giving	O
priority	O
to	O
readwrite	O
commands	O
to	O
already	O
open	O
rows	O
the	O
policy	B
called	O
first-ready	O
firstcome-first-serve	O
gives	O
priority	O
to	O
column	O
commands	O
and	O
write	O
over	O
row	O
commands	O
and	O
precharge	O
and	O
in	O
case	O
of	O
a	O
tie	O
gives	O
priority	O
to	O
the	O
oldest	O
command	O
fr-fcfs	O
was	O
shown	O
to	O
outperform	O
other	O
scheduling	O
policies	O
in	O
terms	O
of	O
average	O
memory-access	O
latency	O
under	O
conditions	O
commonly	O
encountered	O
figure	O
is	O
a	O
high-level	O
view	O
of	O
ipek	O
et	O
al	O
s	O
reinforcement	B
learning	I
memory	O
controller	O
they	O
modeled	O
the	O
dram	O
access	O
process	O
as	O
an	O
mdp	O
whose	O
states	O
are	O
the	O
contents	O
of	O
the	O
transaction	O
queue	O
and	O
whose	O
actions	O
are	O
commands	O
to	O
the	O
dram	O
system	O
precharge	O
activate	O
read	O
write	O
and	O
noop	O
the	O
reward	B
signal	I
is	O
whenever	O
the	O
action	B
figure	O
high-level	O
view	O
of	O
the	O
reinforcement	B
learning	I
dram	O
controller	O
the	O
scheduler	O
is	O
the	O
reinforcement	B
learning	I
agent	O
its	O
environment	B
is	O
represented	O
by	O
features	O
of	O
the	O
transaction	O
ieee	O
reprinted	O
with	O
queue	O
and	O
its	O
actions	O
are	O
commands	O
to	O
the	O
dram	O
system	O
permission	O
from	O
j	O
f	O
mart	O
nez	O
and	O
e	O
ipek	O
dynamic	O
multicore	O
resource	O
management	O
a	O
machine	O
learning	O
approach	O
micro	O
ieee	O
p	O
chapter	O
applications	B
and	I
case	I
studies	I
is	O
read	O
or	O
write	O
and	O
otherwise	O
it	O
is	O
state	B
transitions	O
were	O
considered	O
to	O
be	O
stochastic	O
because	O
the	O
next	O
state	B
of	O
the	O
system	O
not	O
only	O
depends	O
on	O
the	O
scheduler	O
s	O
command	O
but	O
also	O
on	O
aspects	O
of	O
the	O
system	O
s	O
behavior	O
that	O
the	O
scheduler	O
cannot	O
control	B
such	O
as	O
the	O
workloads	O
of	O
the	O
processor	O
cores	O
accessing	O
the	O
dram	O
system	O
critical	O
to	O
this	O
mdp	O
are	O
constraints	O
on	O
the	O
actions	O
available	O
in	O
each	O
state	B
recall	O
from	O
chapter	O
that	O
the	O
set	O
of	O
available	O
actions	O
can	O
depend	O
on	O
the	O
state	B
at	O
ast	O
where	O
at	O
is	O
the	O
action	B
at	O
time	O
step	O
t	O
and	O
ast	O
is	O
the	O
set	O
of	O
actions	O
available	O
in	O
state	B
st	O
in	O
this	O
application	O
the	O
integrity	O
of	O
the	O
dram	O
system	O
was	O
assured	O
by	O
not	O
allowing	O
actions	O
that	O
would	O
violate	O
timing	O
or	O
resource	O
constraints	O
although	O
ipek	O
et	O
al	O
did	O
not	O
make	O
it	O
explicit	O
they	O
effectively	O
accomplished	O
this	O
by	O
pre-defining	O
the	O
sets	O
ast	O
for	O
all	O
possible	O
states	O
st	O
these	O
constraints	O
explain	O
why	O
the	O
mdp	O
has	O
a	O
noop	O
action	B
and	O
why	O
the	O
reward	B
signal	I
is	O
except	O
when	O
a	O
read	O
or	O
write	O
command	O
is	O
issued	O
noop	O
is	O
issued	O
when	O
it	O
is	O
the	O
sole	O
legal	O
action	B
in	O
a	O
state	B
to	O
maximize	O
utilization	O
of	O
the	O
memory	O
system	O
the	O
controller	O
s	O
task	O
is	O
to	O
drive	O
the	O
system	O
to	O
states	O
in	O
which	O
either	O
a	O
read	O
or	O
a	O
write	O
action	B
can	O
be	O
selected	O
only	O
these	O
actions	O
result	O
in	O
sending	O
data	O
over	O
the	O
external	O
data	O
bus	O
so	O
it	O
is	O
only	O
these	O
that	O
contribute	O
to	O
the	O
throughput	O
of	O
the	O
system	O
although	O
precharge	O
and	O
activate	O
produce	O
no	O
immediate	O
reward	O
the	O
agent	O
needs	O
to	O
select	O
these	O
actions	O
to	O
make	O
it	O
possible	O
to	O
later	O
select	O
the	O
rewarded	O
read	O
and	O
write	O
actions	O
the	O
scheduling	O
agent	O
used	O
sarsa	B
to	O
learn	O
an	O
action-value	O
function	O
states	O
were	O
represented	O
by	O
six	O
integer-valued	O
features	O
to	O
approximate	O
the	O
action-value	O
function	O
the	O
algorithm	O
used	O
linear	O
function	B
approximation	I
implemented	O
by	O
tile	B
coding	I
with	O
hashing	O
the	O
tile	B
coding	I
had	O
tilings	O
each	O
storing	O
action	B
values	O
as	O
fixed	O
point	O
numbers	O
exploration	O
was	O
with	O
state	B
features	O
included	O
the	O
number	O
of	O
read	O
requests	O
in	O
the	O
transaction	O
queue	O
the	O
number	O
of	O
write	O
requests	O
in	O
the	O
transaction	O
queue	O
the	O
number	O
of	O
write	O
requests	O
in	O
the	O
transaction	O
queue	O
waiting	O
for	O
their	O
row	O
to	O
be	O
opened	O
and	O
the	O
number	O
of	O
read	O
requests	O
in	O
the	O
transaction	O
queue	O
waiting	O
for	O
their	O
row	O
to	O
be	O
opened	O
that	O
are	O
the	O
oldest	O
issued	O
by	O
their	O
requesting	O
processors	O
other	O
features	O
depended	O
on	O
how	O
the	O
dram	O
interacts	O
with	O
cache	O
memory	O
details	O
we	O
omit	O
here	O
the	O
selection	O
of	O
the	O
state	B
features	O
was	O
based	O
on	O
ipek	O
et	O
al	O
s	O
understanding	O
of	O
factors	O
that	O
impact	O
dram	O
performance	O
for	O
example	O
balancing	O
the	O
rate	O
of	O
servicing	O
reads	O
and	O
writes	O
based	O
on	O
how	O
many	O
of	O
each	O
are	O
in	O
the	O
transaction	O
queue	O
can	O
help	O
avoid	O
stalling	O
the	O
dram	O
system	O
s	O
interaction	O
with	O
cache	O
memory	O
the	O
authors	O
in	O
fact	O
generated	O
a	O
relatively	O
long	O
list	O
of	O
potential	O
features	O
and	O
then	O
pared	O
them	O
down	O
to	O
a	O
handful	O
using	O
simulations	O
guided	O
by	O
stepwise	O
feature	O
selection	O
an	O
interesting	O
aspect	O
of	O
this	O
formulation	O
of	O
the	O
scheduling	O
problem	O
as	O
an	O
mdp	O
is	O
that	O
the	O
features	O
input	O
to	O
the	O
tile	B
coding	I
for	O
defining	O
the	O
action-value	O
function	O
were	O
different	O
from	O
the	O
features	O
used	O
to	O
specify	O
the	O
action-constraint	O
sets	O
ast	O
whereas	O
the	O
tile	B
coding	I
input	O
was	O
derived	O
from	O
the	O
contents	O
of	O
the	O
transaction	O
queue	O
the	O
constraint	O
sets	O
depended	O
on	O
a	O
host	O
of	O
other	O
features	O
related	O
to	O
timing	O
and	O
resource	O
constraints	O
that	O
had	O
to	O
be	O
satisfied	O
by	O
the	O
hardware	O
implementation	O
of	O
the	O
entire	O
system	O
in	O
this	O
way	O
the	O
action	B
constraints	O
ensured	O
that	O
the	O
learning	O
algorithm	O
s	O
exploration	O
could	O
not	O
endanger	O
the	O
integrity	O
of	O
the	O
physical	O
system	O
while	O
learning	O
was	O
effectively	O
limited	O
to	O
a	O
safe	O
region	O
of	O
the	O
much	O
larger	O
state	B
space	O
of	O
the	O
hardware	O
implementation	O
optimizing	B
memory	I
control	B
because	O
an	O
objective	O
of	O
this	O
work	O
was	O
that	O
the	O
learning	O
controller	O
could	O
be	O
implemented	O
on	O
a	O
chip	O
so	O
that	O
learning	O
could	O
occur	O
online	B
while	O
a	O
computer	O
is	O
running	O
hardware	O
implementation	O
details	O
were	O
important	O
considerations	O
the	O
design	O
included	O
two	O
five-stage	O
pipelines	O
to	O
calculate	O
and	O
compare	O
two	O
action	B
values	O
at	O
every	O
processor	O
clock	O
cycle	O
and	O
to	O
update	O
the	O
appropriate	O
action	B
value	B
this	O
included	O
accessing	O
the	O
tile	B
coding	I
which	O
was	O
stored	O
on-chip	O
in	O
static	O
ram	O
for	O
the	O
configuration	O
ipek	O
et	O
al	O
simulated	O
which	O
was	O
a	O
chip	O
typical	O
of	O
high-end	O
workstations	O
at	O
the	O
time	O
of	O
their	O
research	O
there	O
were	O
processor	O
cycles	O
for	O
every	O
dram	O
cycle	O
considering	O
the	O
cycles	O
needed	O
to	O
fill	O
the	O
pipes	O
up	O
to	O
actions	O
could	O
be	O
evaluated	O
in	O
each	O
dram	O
cycle	O
ipek	O
et	O
al	O
found	O
that	O
the	O
number	O
of	O
legal	O
commands	O
for	O
any	O
state	B
was	O
rarely	O
greater	O
than	O
this	O
and	O
that	O
performance	O
loss	O
was	O
negligible	O
if	O
enough	O
time	O
was	O
not	O
always	O
available	O
to	O
consider	O
all	O
legal	O
commands	O
these	O
and	O
other	O
clever	O
design	O
details	O
made	O
it	O
feasible	O
to	O
implement	O
the	O
complete	O
controller	O
and	O
learning	O
algorithm	O
on	O
a	O
multi-processor	O
chip	O
ipek	O
et	O
al	O
evaluated	O
their	O
learning	O
controller	O
in	O
simulation	O
by	O
comparing	O
it	O
with	O
three	O
other	O
controllers	O
the	O
fr-fcfs	O
controller	O
mentioned	O
above	O
that	O
produces	O
the	O
best	O
on-average	O
performance	O
a	O
conventional	O
controller	O
that	O
processes	O
each	O
request	O
in	O
order	O
and	O
an	O
unrealizable	O
ideal	O
controller	O
called	O
the	O
optimistic	O
controller	O
able	O
to	O
sustain	O
dram	O
throughput	O
if	O
given	O
enough	O
demand	O
by	O
ignoring	O
all	O
timing	O
and	O
resource	O
constraints	O
but	O
otherwise	O
modeling	O
dram	O
latency	O
row	O
buffer	O
hits	O
and	O
bandwidth	O
they	O
simulated	O
nine	O
memory-intensive	O
parallel	O
workloads	O
consisting	O
of	O
scientific	O
and	O
data-mining	O
applications	O
figure	O
shows	O
the	O
performance	O
inverse	O
of	O
execution	O
time	O
normalized	O
to	O
the	O
performance	O
of	O
fr-fcfs	O
of	O
each	O
controller	O
for	O
the	O
nine	O
applications	O
together	O
with	O
the	O
geometric	O
mean	O
of	O
their	O
performances	O
over	O
the	O
applications	O
the	O
learning	O
controller	O
labeled	O
rl	O
in	O
the	O
figure	O
improved	O
over	O
that	O
of	O
fr-fcfs	O
by	O
from	O
to	O
over	O
the	O
nine	O
applications	O
with	O
an	O
average	O
improvement	O
of	O
of	O
course	O
no	O
realizable	O
controller	O
can	O
match	O
the	O
performance	O
of	O
optimistic	O
which	O
ignores	O
all	O
timing	O
and	O
resource	O
constraints	O
but	O
the	O
learning	O
controller	O
s	O
performance	O
figure	O
performances	O
of	O
four	O
controllers	O
over	O
a	O
suite	O
of	O
simulated	O
benchmark	O
applications	O
the	O
controllers	O
are	O
the	O
simplest	O
in-order	O
controller	O
fr-fcfs	O
the	O
learning	O
controller	O
rl	O
and	O
the	O
unrealizable	O
optimistic	O
controller	O
which	O
ignores	O
all	O
timing	O
and	O
resource	O
constraints	O
to	O
provide	O
a	O
performance	O
upper	O
bound	O
performance	O
normalized	O
to	O
that	O
of	O
fr-fcfs	O
is	O
the	O
inverse	O
of	O
execution	O
time	O
at	O
far	O
right	O
is	O
the	O
geometric	O
mean	O
of	O
performances	O
over	O
the	O
benchmark	O
applications	O
for	O
each	O
controller	O
controller	O
rl	O
comes	O
closest	O
to	O
the	O
ideal	O
performance	O
ieee	O
reprinted	O
with	O
permission	O
from	O
j	O
f	O
mart	O
nez	O
and	O
e	O
ipek	O
dynamic	O
multicore	O
resource	O
management	O
a	O
machine	O
learning	O
approach	O
micro	O
ieee	O
p	O
chapter	O
applications	B
and	I
case	I
studies	I
closed	O
the	O
gap	O
with	O
optimistic	O
s	O
upper	O
bound	O
by	O
an	O
impressive	O
because	O
the	O
rationale	O
for	O
on-chip	O
implementation	O
of	O
the	O
learning	O
algorithm	O
was	O
to	O
allow	O
the	O
scheduling	O
policy	B
to	O
adapt	O
online	B
to	O
changing	O
workloads	O
ipek	O
et	O
al	O
analyzed	O
the	O
impact	O
of	O
online	B
learning	O
compared	O
to	O
a	O
previously-learned	O
fixed	O
policy	B
they	O
trained	O
their	O
controller	O
with	O
data	O
from	O
all	O
nine	O
benchmark	O
applications	O
and	O
then	O
held	O
the	O
resulting	O
action	B
values	O
fixed	O
throughout	O
the	O
simulated	O
execution	O
of	O
the	O
applications	O
they	O
found	O
that	O
the	O
average	O
performance	O
of	O
the	O
controller	O
that	O
learned	O
online	B
was	O
better	O
than	O
that	O
of	O
the	O
controller	O
using	O
the	O
fixed	O
policy	B
leading	O
them	O
to	O
conclude	O
that	O
online	B
learning	O
is	O
an	O
important	O
feature	O
of	O
their	O
approach	O
this	O
learning	O
memory	O
controller	O
was	O
never	O
committed	O
to	O
physical	O
hardware	O
because	O
of	O
the	O
large	O
cost	O
of	O
fabrication	O
nevertheless	O
ipek	O
et	O
al	O
could	O
convincingly	O
argue	O
on	O
the	O
basis	O
of	O
their	O
simulation	O
results	O
that	O
a	O
memory	O
controller	O
that	O
learns	O
online	B
via	O
reinforcement	B
learning	I
has	O
the	O
potential	O
to	O
improve	O
performance	O
to	O
levels	O
that	O
would	O
otherwise	O
require	O
more	O
complex	O
and	O
more	O
expensive	O
memory	O
systems	O
while	O
removing	O
from	O
human	O
designers	O
some	O
of	O
the	O
burden	O
required	O
to	O
manually	O
design	O
efficient	O
scheduling	O
policies	O
mukundan	O
and	O
mart	O
nez	O
took	O
this	O
project	O
forward	O
by	O
investigating	O
learning	O
controllers	O
with	O
additional	O
actions	O
other	O
performance	O
criteria	O
and	O
more	O
complex	O
reward	O
functions	O
derived	O
using	O
genetic	B
algorithms	I
they	O
considered	O
additional	O
performance	O
criteria	O
related	O
to	O
energy	O
efficiency	O
the	O
results	O
of	O
these	O
studies	O
surpassed	O
the	O
earlier	O
results	O
described	O
above	O
and	O
significantly	O
surpassed	O
the	O
state-of-the-art	O
for	O
all	O
of	O
the	O
performance	O
criteria	O
they	O
considered	O
the	O
approach	O
is	O
especially	O
promising	O
for	O
developing	O
sophisticated	O
power-aware	O
dram	O
interfaces	O
human-level	O
video	O
game	O
play	O
one	O
of	O
the	O
greatest	O
challenges	O
in	O
applying	O
reinforcement	B
learning	I
to	O
real-world	O
problems	O
is	O
deciding	O
how	O
to	O
represent	O
and	O
store	O
value	B
functions	O
andor	O
policies	O
unless	O
the	O
state	B
set	O
is	O
finite	O
and	O
small	O
enough	O
to	O
allow	O
exhaustive	O
representation	O
by	O
a	O
lookup	O
table	O
as	O
in	O
many	O
of	O
our	O
illustrative	O
examples	O
one	O
must	O
use	O
a	O
parameterized	O
function	B
approximation	I
scheme	O
whether	O
linear	O
or	O
nonlinear	O
function	B
approximation	I
relies	O
on	O
features	O
that	O
have	O
to	O
be	O
readily	O
accessible	O
to	O
the	O
learning	O
system	O
and	O
able	O
to	O
convey	O
the	O
information	O
necessary	O
for	O
skilled	O
performance	O
most	O
successful	O
applications	O
of	O
reinforcement	B
learning	I
owe	O
much	O
to	O
sets	O
of	O
features	O
carefully	O
handcrafted	O
based	O
on	O
human	O
knowledge	O
and	O
intuition	O
about	O
the	O
specific	O
problem	O
to	O
be	O
tackled	O
a	O
team	O
of	O
researchers	O
at	O
google	O
deepmind	O
developed	O
an	O
impressive	O
demonstration	O
that	O
a	O
deep	O
multi-layer	O
artificial	O
neural	B
network	O
can	O
automate	O
the	O
feature	O
design	O
process	O
et	O
al	O
multi-layer	O
anns	O
have	O
been	O
used	O
for	O
function	B
approximation	I
in	O
reinforcement	B
learning	I
ever	O
since	O
the	O
popularization	O
of	O
the	O
backpropagation	B
algorithm	O
as	O
a	O
method	O
for	O
learning	O
internal	O
representations	O
hinton	O
and	O
williams	O
see	O
section	O
striking	O
results	O
have	O
been	O
obtained	O
by	O
coupling	O
reinforcement	B
learning	I
with	O
backpropagation	B
the	O
results	O
obtained	O
by	O
tesauro	O
and	O
colleages	O
with	O
td-gammon	B
and	O
watson	O
discussed	O
above	O
are	O
notable	O
examples	O
these	O
and	O
other	O
applications	O
benefited	O
from	O
the	O
ability	O
of	O
multi-layer	O
anns	O
to	O
learn	O
task-relevant	O
features	O
however	O
in	O
all	O
the	O
examples	O
of	O
which	O
we	O
are	O
aware	O
the	O
most	O
human-level	O
video	O
game	O
play	O
impressive	O
demonstrations	O
required	O
the	O
network	O
s	O
input	O
to	O
be	O
represented	O
in	O
terms	O
of	O
specialized	O
features	O
handcrafted	O
for	O
the	O
given	O
problem	O
this	O
is	O
vividly	O
apparent	O
in	O
the	O
td-gammon	B
results	O
td-gammon	B
whose	O
network	O
input	O
was	O
essentially	O
a	O
raw	O
representation	O
of	O
he	O
backgammon	B
board	O
meaning	O
that	O
it	O
involved	O
very	O
little	O
knowledge	O
of	O
backgammon	B
learned	O
to	O
play	O
approximately	O
as	O
well	O
as	O
the	O
best	O
previous	O
backgammon	B
computer	O
programs	O
adding	O
specialized	O
backgammon	B
features	O
produced	O
td-gammon	B
which	O
was	O
substantially	O
better	O
than	O
all	O
previous	O
backgammon	B
programs	O
and	O
competed	O
well	O
against	O
human	O
experts	O
mnih	O
et	O
al	O
developed	O
a	O
reinforcement	B
learning	I
agent	O
called	O
deep	O
q-network	O
that	O
combined	O
q-learning	B
with	O
a	O
deep	O
convolutional	O
ann	O
a	O
many-layered	O
or	O
deep	O
ann	O
specialized	O
for	O
processing	O
spatial	O
arrays	O
of	O
data	O
such	O
as	O
images	O
we	O
describe	O
deep	O
convolutional	O
anns	O
in	O
section	O
by	O
the	O
time	O
of	O
mnih	O
et	O
al	O
s	O
work	O
with	O
dqn	O
deep	O
anns	O
including	O
deep	O
convolutional	O
anns	O
had	O
produced	O
impressive	O
results	O
in	O
many	O
applications	O
but	O
they	O
had	O
not	O
been	O
widely	O
used	O
in	O
reinforcement	B
learning	I
mnih	O
et	O
al	O
used	O
dqn	O
to	O
show	O
how	O
a	O
single	O
reinforcement	B
learning	I
agent	O
can	O
achieve	O
high	O
levels	O
of	O
performance	O
in	O
many	O
different	O
problems	O
without	O
relying	O
on	O
different	O
problemspecific	O
feature	O
sets	O
to	O
demonstrate	O
this	O
they	O
let	O
dqn	O
learn	O
to	O
play	O
different	O
atari	O
video	O
games	O
by	O
interacting	O
with	O
a	O
game	O
emulator	O
for	O
learning	O
each	O
game	O
dqn	O
used	O
the	O
same	O
raw	O
input	O
the	O
same	O
network	O
architecture	O
and	O
the	O
same	O
parameter	O
values	O
step	O
size	O
discount	O
rate	O
exploration	O
parameters	O
and	O
many	O
more	O
specific	O
to	O
the	O
implementation	O
dqn	O
achieved	O
levels	O
of	O
play	O
at	O
or	O
beyond	O
human	O
level	O
on	O
a	O
large	O
fraction	O
of	O
these	O
games	O
although	O
the	O
games	O
were	O
alike	O
in	O
being	O
played	O
by	O
watching	O
streams	O
of	O
video	O
images	O
they	O
varied	O
widely	O
in	O
other	O
respects	O
their	O
actions	O
had	O
different	O
effects	O
they	O
had	O
different	O
state-transition	O
dynamics	O
and	O
they	O
needed	O
different	O
policies	O
for	O
earning	O
high	O
scores	O
the	O
deep	O
convolutional	O
ann	O
learned	O
to	O
transform	O
the	O
raw	O
input	O
common	O
to	O
all	O
the	O
games	O
into	O
features	O
specialized	O
for	O
representing	O
the	O
action	B
values	O
required	O
for	O
playing	O
at	O
the	O
high	O
level	O
dqn	O
achieved	O
for	O
most	O
of	O
the	O
games	O
the	O
atari	O
is	O
a	O
home	O
video	O
game	O
console	O
that	O
was	O
sold	O
in	O
various	O
versions	O
by	O
atari	O
inc	O
from	O
to	O
it	O
introduced	O
or	O
popularized	O
many	O
arcade	O
video	O
games	O
that	O
are	O
now	O
considered	O
classics	O
such	O
as	O
pong	O
breakout	O
space	O
invaders	O
and	O
asteroids	O
although	O
much	O
simpler	O
than	O
modern	O
video	O
games	O
atari	O
games	O
are	O
still	O
entertaining	O
and	O
challenging	O
for	O
human	O
players	O
and	O
they	O
have	O
been	O
attractive	O
as	O
testbeds	O
for	O
developing	O
and	O
evaluating	O
reinforcement	B
learning	I
methods	O
cohen	O
littman	O
naddaf	O
cobo	O
zang	O
isbell	O
and	O
thomaz	O
bellemare	O
veness	O
and	O
bowling	O
bellemare	O
naddaf	O
veness	O
and	O
bowling	O
developed	O
the	O
publicly	O
available	O
arcade	O
learning	O
environment	B
to	O
encourage	O
and	O
simplify	O
using	O
atari	O
games	O
to	O
study	O
learning	O
and	O
planning	B
algorithms	O
these	O
previous	O
studies	O
and	O
the	O
availability	O
of	O
ale	O
made	O
the	O
atari	O
game	O
collection	O
a	O
good	O
choice	O
for	O
mnih	O
et	O
al	O
s	O
demonstration	O
which	O
was	O
also	O
influenced	O
by	O
the	O
impressive	O
human-level	O
performance	O
that	O
td-gammon	B
was	O
able	O
to	O
achieve	O
in	O
backgammon	B
dqn	O
is	O
similar	O
to	O
td-gammon	B
in	O
using	O
a	O
multi-layer	O
ann	O
as	O
the	O
function	B
approximation	I
method	O
for	O
a	O
semi-gradient	O
form	O
of	O
a	O
td	B
algorithm	O
with	O
the	O
gradients	O
computed	O
by	O
the	O
backpropagation	B
algorithm	O
however	O
instead	O
of	O
using	O
td	B
as	O
td-gammon	B
did	O
dqn	O
used	O
the	O
semi-gradient	O
form	O
of	O
q-learning	B
td-gammon	B
estimated	O
the	O
values	O
of	O
chapter	O
applications	B
and	I
case	I
studies	I
afterstates	B
which	O
were	O
easily	O
obtained	O
from	O
the	O
rules	O
for	O
making	O
backgammon	B
moves	O
to	O
use	O
the	O
same	O
algorithm	O
for	O
the	O
atari	O
games	O
would	O
have	O
required	O
generating	O
the	O
next	O
states	O
for	O
each	O
possible	O
action	B
would	O
not	O
have	O
been	O
afterstates	B
in	O
that	O
case	O
this	O
could	O
have	O
been	O
done	O
by	O
using	O
the	O
game	O
emulator	O
to	O
run	O
single-step	O
simulations	O
for	O
all	O
the	O
possible	O
actions	O
ale	O
makes	O
possible	O
or	O
a	O
model	O
of	O
each	O
game	O
s	O
state-transition	O
function	O
could	O
have	O
been	O
learned	O
and	O
used	O
to	O
predict	O
next	O
states	O
guo	O
lee	O
lewis	O
and	O
singh	O
while	O
these	O
methods	O
might	O
have	O
produced	O
results	O
comparable	O
to	O
dqn	O
s	O
they	O
would	O
have	O
been	O
more	O
complicated	O
to	O
implement	O
and	O
would	O
have	O
significantly	O
increased	O
the	O
time	O
needed	O
for	O
learning	O
another	O
motivation	B
for	O
using	O
q-learning	B
was	O
that	O
dqn	O
used	O
the	O
experience	B
replay	I
method	O
described	O
below	O
which	O
requires	O
an	O
off-policy	B
algorithm	O
being	O
model-free	O
and	O
off-policy	B
made	O
q-learning	B
a	O
natural	O
choice	O
before	O
describing	O
the	O
details	O
of	O
dqn	O
and	O
how	O
the	O
experiments	O
were	O
conducted	O
we	O
look	O
at	O
the	O
skill	O
levels	O
dqn	O
was	O
able	O
to	O
achieve	O
mnih	O
et	O
al	O
compared	O
the	O
scores	O
of	O
dqn	O
with	O
the	O
scores	O
of	O
the	O
best	O
performing	O
learning	O
system	O
in	O
the	O
literature	O
at	O
the	O
time	O
the	O
scores	O
of	O
a	O
professional	O
human	O
games	O
tester	O
and	O
the	O
scores	O
of	O
an	O
agent	O
that	O
selected	O
actions	O
at	O
random	O
the	O
best	O
system	O
from	O
the	O
literature	O
used	O
linear	O
function	B
approximation	I
with	O
features	O
hand	O
designed	O
using	O
some	O
knowledge	O
about	O
atari	O
games	O
naddaf	O
veness	O
and	O
bowling	O
dqn	O
learned	O
on	O
each	O
game	O
by	O
interacting	O
with	O
the	O
game	O
emulator	O
for	O
million	O
frames	O
which	O
corresponds	O
to	O
about	O
days	O
of	O
experience	O
with	O
the	O
game	O
at	O
the	O
start	O
of	O
learning	O
on	O
each	O
game	O
the	O
weights	O
of	O
dqn	O
s	O
network	O
were	O
reset	O
to	O
random	O
values	O
to	O
evaluate	O
dqn	O
s	O
skill	O
level	O
after	O
learning	O
its	O
score	O
was	O
averaged	O
over	O
sessions	O
on	O
each	O
game	O
each	O
lasting	O
up	O
to	O
minutes	O
and	O
beginning	O
with	O
a	O
random	O
initial	O
game	O
state	B
the	O
professional	O
human	O
tester	O
played	O
using	O
the	O
same	O
emulator	O
the	O
sound	O
turned	O
off	O
to	O
remove	O
any	O
possible	O
advantage	O
over	O
dqn	O
which	O
did	O
not	O
process	O
audio	O
after	O
hours	O
of	O
practice	O
the	O
human	O
played	O
about	O
episodes	B
of	O
each	O
game	O
for	O
up	O
to	O
minutes	O
each	O
and	O
was	O
not	O
allowed	O
to	O
take	O
any	O
break	O
during	O
this	O
time	O
dqn	O
learned	O
to	O
play	O
better	O
than	O
the	O
best	O
previous	O
reinforcement	B
learning	I
systems	O
on	O
all	O
but	O
of	O
the	O
games	O
and	O
played	O
better	O
than	O
the	O
human	O
player	O
on	O
of	O
the	O
games	O
by	O
considering	O
any	O
performance	O
that	O
scored	O
at	O
or	O
above	O
of	O
the	O
human	O
score	O
to	O
be	O
comparable	O
to	O
or	O
better	O
than	O
human-level	O
play	O
mnih	O
et	O
al	O
concluded	O
that	O
the	O
levels	O
of	O
play	O
dqn	O
learned	O
reached	O
or	O
exceeded	O
human	O
level	O
on	O
of	O
the	O
games	O
see	O
mnih	O
et	O
al	O
for	O
a	O
more	O
detailed	O
account	O
of	O
these	O
results	O
for	O
an	O
artificial	O
learning	O
system	O
to	O
achieve	O
these	O
levels	O
of	O
play	O
would	O
be	O
impressive	O
enough	O
but	O
what	O
makes	O
these	O
results	O
remarkable	O
and	O
what	O
many	O
at	O
the	O
time	O
considered	O
to	O
be	O
breakthrough	O
results	O
for	O
artificial	B
intelligence	I
is	O
that	O
the	O
very	O
same	O
learning	O
system	O
achieved	O
these	O
levels	O
of	O
play	O
on	O
widely	O
varying	O
games	O
without	O
relying	O
on	O
any	O
game-specific	O
modifications	O
a	O
human	O
playing	O
any	O
of	O
these	O
atari	O
games	O
sees	O
pixel	O
image	O
frames	O
with	O
colors	O
at	O
in	O
principle	O
exactly	O
these	O
images	O
could	O
have	O
formed	O
the	O
raw	O
input	O
to	O
dqn	O
but	O
to	O
reduce	O
memory	O
and	O
processing	O
requirements	O
mnih	O
et	O
al	O
preprocessed	O
each	O
frame	O
to	O
produce	O
an	O
array	O
of	O
luminance	O
values	O
because	O
the	O
full	O
states	O
of	O
many	O
of	O
the	O
atari	O
games	O
are	O
not	O
completely	O
observable	O
from	O
the	O
image	O
frames	O
mnih	O
et	O
al	O
stacked	O
the	O
four	O
most	O
recent	O
frames	O
so	O
that	O
the	O
inputs	O
to	O
the	O
network	O
had	O
dimension	O
human-level	O
video	O
game	O
play	O
this	O
did	O
not	O
eliminate	O
partial	O
observability	O
for	O
all	O
of	O
the	O
games	O
but	O
it	O
was	O
helpful	O
in	O
making	O
many	O
of	O
them	O
more	O
markovian	O
an	O
essential	O
point	O
here	O
is	O
that	O
these	O
preprocessing	O
steps	O
were	O
exactly	O
the	O
same	O
for	O
all	O
games	O
no	O
game-specific	O
prior	B
knowledge	I
was	O
involved	O
beyond	O
the	O
general	O
understanding	O
that	O
it	O
should	O
still	O
be	O
possible	O
to	O
learn	O
good	O
policies	O
with	O
this	O
reduced	O
dimension	O
and	O
that	O
stacking	O
adjacent	O
frames	O
should	O
help	O
with	O
the	O
partial	O
observability	O
of	O
some	O
of	O
the	O
games	O
because	O
no	O
game-specific	O
prior	B
knowledge	I
beyond	O
this	O
minimal	O
amount	O
was	O
used	O
in	O
preprocessing	O
the	O
image	O
frames	O
we	O
can	O
think	O
of	O
the	O
input	O
vectors	O
as	O
being	O
raw	O
input	O
to	O
dqn	O
the	O
basic	O
architecture	O
of	O
dqn	O
is	O
similar	O
to	O
the	O
deep	O
convolutional	O
ann	O
illustrated	O
in	O
figure	O
unlike	O
that	O
network	O
subsampling	O
in	B
dqn	I
is	O
treated	O
as	O
part	O
of	O
each	O
convolutional	O
layer	O
with	O
feature	O
maps	O
consisting	O
of	O
units	O
having	O
only	O
a	O
selection	O
of	O
the	O
possible	O
receptive	O
fields	O
dqn	O
has	O
three	O
hidden	O
convolutional	O
layers	O
followed	O
by	O
one	O
fully	O
connected	O
hidden	O
layer	O
followed	O
by	O
the	O
output	O
layer	O
the	O
three	O
successive	O
hidden	O
convolutional	O
layers	O
of	O
dqn	O
produce	O
feature	O
maps	O
feature	O
maps	O
and	O
feature	O
maps	O
the	O
activation	O
function	O
of	O
the	O
units	O
of	O
each	O
feature	O
map	O
is	O
a	O
rectifier	O
nonlinearity	O
x	O
the	O
units	O
in	O
this	O
third	O
convolutional	O
layer	O
all	O
connect	O
to	O
each	O
of	O
units	O
in	O
the	O
fully	O
connected	O
hidden	O
layer	O
which	O
then	O
each	O
connect	O
to	O
all	O
units	O
in	O
the	O
output	O
layer	O
one	O
for	O
each	O
possible	O
action	B
in	O
an	O
atari	O
game	O
the	O
activation	O
levels	O
of	O
dqn	O
s	O
output	O
units	O
were	O
the	O
estimated	O
optimal	O
action	B
values	O
q-values	O
of	O
the	O
corresponding	O
state	B
action	B
pairs	O
for	O
the	O
state	B
represented	O
by	O
the	O
network	O
s	O
input	O
the	O
assignment	O
of	O
output	O
units	O
to	O
a	O
game	O
s	O
actions	O
varied	O
from	O
game	O
to	O
game	O
and	O
because	O
the	O
number	O
of	O
valid	O
actions	O
varied	O
between	O
and	O
for	O
the	O
games	O
not	O
all	O
output	O
units	O
had	O
functional	O
roles	O
in	O
all	O
of	O
the	O
games	O
it	O
helps	O
to	O
think	O
of	O
the	O
network	O
as	O
if	O
it	O
were	O
separate	O
networks	O
one	O
for	O
estimating	O
the	O
optimal	O
action	B
value	B
of	O
each	O
possible	O
action	B
in	O
reality	O
these	O
networks	O
shared	O
their	O
initial	O
layers	O
but	O
the	O
output	O
units	O
learned	O
to	O
use	O
the	O
features	O
extracted	O
by	O
these	O
layers	O
in	O
different	O
ways	O
dqn	O
s	O
reward	B
signal	I
indicated	O
how	O
a	O
games	O
s	O
score	O
changed	O
from	O
one	O
time	O
step	O
to	O
the	O
next	O
whenever	O
it	O
increased	O
whenever	O
it	O
decreased	O
and	O
otherwise	O
this	O
standardized	O
the	O
reward	B
signal	I
across	O
the	O
games	O
and	O
made	O
a	O
single	O
step-size	B
parameter	I
work	O
well	O
for	O
all	O
the	O
games	O
despite	O
their	O
varying	O
ranges	O
of	O
scores	O
dqn	O
used	O
an	O
policy	B
with	O
decreasing	O
linearly	O
over	O
the	O
first	O
million	O
frames	O
and	O
remaining	O
at	O
a	O
low	O
value	B
for	O
the	O
rest	O
of	O
the	O
learning	O
session	O
the	O
values	O
of	O
the	O
various	O
other	O
parameters	O
such	O
as	O
the	O
learning	O
step	O
size	O
discount	O
rate	O
and	O
others	O
specific	O
to	O
the	O
implementation	O
were	O
selected	O
by	O
performing	O
informal	O
searches	O
to	O
see	O
which	O
values	O
worked	O
best	O
for	O
a	O
small	O
selection	O
of	O
the	O
games	O
these	O
values	O
were	O
then	O
held	O
fixed	O
for	O
all	O
of	O
the	O
games	O
after	O
dqn	O
selected	O
an	O
action	B
the	O
action	B
was	O
executed	O
by	O
the	O
game	O
emulator	O
which	O
returned	O
a	O
reward	O
and	O
the	O
next	O
video	O
frame	O
the	O
frame	O
was	O
preprocessed	O
and	O
added	O
to	O
the	O
four-frame	O
stack	O
that	O
became	O
the	O
next	O
input	O
to	O
the	O
network	O
skipping	O
for	O
the	O
moment	O
the	O
changes	O
to	O
the	O
basic	O
q-learning	B
procedure	O
made	O
by	O
mnih	O
et	O
al	O
dqn	O
used	O
the	O
following	O
semi-gradient	O
form	O
of	O
q-learning	B
to	O
update	O
the	O
network	O
s	O
weights	O
wt	O
max	O
a	O
a	O
wt	O
qst	O
at	O
qst	O
at	O
wt	O
chapter	O
applications	B
and	I
case	I
studies	I
where	O
wt	O
is	O
the	O
vector	B
of	O
the	O
network	O
s	O
weights	O
at	O
is	O
the	O
action	B
selected	O
at	O
time	O
step	O
t	O
and	O
st	O
and	O
are	O
respectively	O
the	O
preprocessed	O
image	O
stacks	O
input	O
to	O
the	O
network	O
at	O
time	O
steps	O
t	O
and	O
t	O
the	O
gradient	B
in	O
was	O
computed	O
by	O
backpropagation	B
imagining	O
again	O
that	O
there	O
was	O
a	O
separate	O
network	O
for	O
each	O
action	B
for	O
the	O
update	O
at	O
time	O
step	O
t	O
backpropagation	B
was	O
applied	O
only	O
to	O
the	O
network	O
corresponding	O
to	O
at	O
mnih	O
et	O
al	O
took	O
advantage	O
of	O
techniques	O
shown	O
to	O
improve	O
the	O
basic	O
backpropagation	B
algorithm	O
when	O
applied	O
to	O
large	O
networks	O
they	O
used	O
a	O
mini-batch	O
method	O
that	O
updated	O
weights	O
only	O
after	O
accumulating	B
gradient	B
information	O
over	O
a	O
small	O
batch	O
of	O
images	O
after	O
images	O
this	O
yielded	O
smoother	O
sample	O
gradients	O
compared	O
to	O
the	O
usual	O
procedure	O
that	O
updates	O
weights	O
after	O
each	O
action	B
they	O
also	O
used	O
a	O
gradient-ascent	O
algorithm	O
called	O
rmsprop	O
and	O
hinton	O
that	O
accelerates	O
learning	O
by	O
adjusting	O
the	O
step-size	B
parameter	I
for	O
each	O
weight	O
based	O
on	O
a	O
running	O
average	O
of	O
the	O
magnitudes	O
of	O
recent	O
gradients	O
for	O
that	O
weight	O
mnih	O
et	O
al	O
modified	O
the	O
basic	O
q-learning	B
procedure	O
in	O
three	O
ways	O
first	O
they	O
used	O
a	O
method	O
called	O
experience	B
replay	I
first	O
studied	O
by	O
lin	O
this	O
method	O
stores	O
the	O
agent	O
s	O
experience	O
at	O
each	O
time	O
step	O
in	O
a	O
replay	O
memory	O
that	O
is	O
accessed	O
to	O
perform	O
the	O
weight	O
updates	O
it	O
worked	O
like	O
this	O
in	B
dqn	I
after	O
the	O
game	O
emulator	O
executed	O
action	B
at	O
in	O
a	O
state	B
represented	O
by	O
the	O
image	O
stack	O
st	O
and	O
returned	O
reward	O
and	O
image	O
stack	O
it	O
added	O
the	O
tuple	O
at	O
to	O
the	O
replay	O
memory	O
this	O
memory	O
accumulated	O
experiences	O
over	O
many	O
plays	O
of	O
the	O
same	O
game	O
at	O
each	O
time	O
step	O
multiple	O
q-learning	B
updates	O
a	O
mini-batch	O
were	O
performed	O
based	O
on	O
experiences	O
sampled	O
uniformly	O
at	O
random	O
from	O
the	O
replay	O
memory	O
instead	O
of	O
becoming	O
the	O
new	O
st	O
for	O
the	O
next	O
update	O
as	O
it	O
would	O
in	O
the	O
usual	O
form	O
of	O
q-learning	B
a	O
new	O
unconnected	O
experience	O
was	O
drawn	O
from	O
the	O
replay	O
memory	O
to	O
supply	O
data	O
for	O
the	O
next	O
update	O
because	O
q-learning	B
is	O
an	O
off-policy	B
algorithm	O
it	O
does	O
not	O
need	O
to	O
be	O
applied	O
along	O
connected	O
trajectories	O
q-learning	B
with	O
experience	B
replay	I
provided	O
several	O
advantages	O
over	O
the	O
usual	O
form	O
of	O
q-learning	B
the	O
ability	O
to	O
use	O
each	O
stored	O
experience	O
for	O
many	O
updates	O
allowed	O
dqn	O
to	O
learn	O
more	O
efficiently	O
from	O
its	O
experiences	O
experience	B
replay	I
reduced	O
the	O
variance	O
of	O
the	O
updates	O
because	O
successive	O
updates	O
were	O
not	O
correlated	O
with	O
one	O
another	O
as	O
they	O
would	O
be	O
with	O
standard	O
q-learning	B
and	O
by	O
removing	O
the	O
dependence	O
of	O
successive	O
experiences	O
on	O
the	O
current	O
weights	O
experience	B
replay	I
eliminated	O
one	O
source	O
of	O
instability	O
mnih	O
et	O
al	O
modified	O
standard	O
q-learning	B
in	O
a	O
second	O
way	O
to	O
improve	O
its	O
stability	O
as	O
in	O
other	O
methods	O
that	O
bootstrap	O
the	O
target	O
for	O
a	O
q-learning	B
update	O
depends	O
on	O
the	O
current	O
action-value	O
function	O
estimate	O
when	O
a	O
parameterized	O
function	B
approximation	I
method	O
is	O
used	O
to	O
represent	O
action	B
values	O
the	O
target	O
is	O
a	O
function	O
of	O
the	O
same	O
parameters	O
that	O
are	O
being	O
updated	O
for	O
example	O
the	O
target	O
in	O
the	O
update	O
given	O
by	O
is	O
maxa	O
a	O
wt	O
its	O
dependence	O
on	O
wt	O
complicates	O
the	O
process	O
compared	O
to	O
the	O
simpler	O
supervised-learning	O
situation	O
in	O
which	O
the	O
targets	O
do	O
not	O
depend	O
on	O
the	O
parameters	O
being	O
updated	O
as	O
discussed	O
in	O
chapter	O
this	O
can	O
lead	O
to	O
oscillations	O
andor	O
divergence	O
to	O
address	O
this	O
problem	O
mnih	O
et	O
al	O
used	O
a	O
technique	O
that	O
brought	O
q-learning	B
closer	O
to	O
the	O
simpler	O
supervised-learning	O
case	O
while	O
still	O
allowing	O
it	O
to	O
bootstrap	O
whenever	O
a	O
certain	O
number	O
c	O
of	O
updates	O
had	O
been	O
done	O
to	O
the	O
weights	O
w	O
of	O
the	O
action-value	O
mastering	O
the	O
game	O
of	O
go	O
network	O
they	O
inserted	O
the	O
network	O
s	O
current	O
weights	O
into	O
another	O
network	O
and	O
held	O
these	O
duplicate	O
weights	O
fixed	O
for	O
the	O
next	O
c	O
updates	O
of	O
w	O
the	O
outputs	O
of	O
this	O
duplicate	O
network	O
over	O
the	O
next	O
c	O
updates	O
of	O
w	O
were	O
used	O
as	O
the	O
q-learning	B
targets	O
letting	O
q	O
denote	O
the	O
output	O
of	O
this	O
duplicate	O
network	O
then	O
instead	O
of	O
the	O
update	O
rule	O
was	O
wt	O
max	O
a	O
a	O
wt	O
qst	O
at	O
qst	O
at	O
wt	O
a	O
final	O
modification	O
of	O
standard	O
q-learning	B
was	O
also	O
found	O
to	O
improve	O
stability	O
they	O
clipped	O
the	O
error	O
term	O
maxa	O
a	O
wt	O
qst	O
at	O
wt	O
so	O
that	O
it	O
remained	O
in	O
the	O
interval	O
mnih	O
et	O
al	O
conducted	O
a	O
large	O
number	O
of	O
learning	O
runs	O
on	O
of	O
the	O
games	O
to	O
gain	O
insight	O
into	O
the	O
effect	O
that	O
various	O
of	O
dqn	O
s	O
design	O
features	O
had	O
on	O
its	O
performance	O
they	O
ran	O
dqn	O
with	O
the	O
four	O
combinations	O
of	O
experience	B
replay	I
and	O
the	O
duplicate	O
target	O
network	O
being	O
included	O
or	O
not	O
included	O
although	O
the	O
results	O
varied	O
from	O
game	O
to	O
game	O
each	O
of	O
these	O
features	O
alone	O
significantly	O
improved	O
performance	O
and	O
very	O
dramatically	O
improved	O
performance	O
when	O
used	O
together	O
mnih	O
et	O
al	O
also	O
studied	O
the	O
role	O
played	O
by	O
the	O
deep	O
convolutional	O
ann	O
in	B
dqn	I
s	O
learning	O
ability	O
by	O
comparing	O
the	O
deep	O
convolutional	O
version	O
of	O
dqn	O
with	O
a	O
version	O
having	O
a	O
network	O
of	O
just	O
one	O
linear	O
layer	O
both	O
receiving	O
the	O
same	O
stacked	O
preprocessed	O
video	O
frames	O
here	O
the	O
improvement	O
of	O
the	O
deep	O
convolutional	O
version	O
over	O
the	O
linear	O
version	O
was	O
particularly	O
striking	O
across	O
all	O
of	O
the	O
test	O
games	O
creating	O
artificial	O
agents	O
that	O
excel	O
over	O
a	O
diverse	O
collection	O
of	O
challenging	O
tasks	O
has	O
been	O
an	O
enduring	O
goal	O
of	O
artificial	B
intelligence	I
the	O
promise	O
of	O
machine	O
learning	O
as	O
a	O
means	O
for	O
achieving	O
this	O
has	O
been	O
frustrated	O
by	O
the	O
need	O
to	O
craft	O
problem-specific	O
representations	O
deepmind	O
s	O
dqn	O
stands	O
as	O
a	O
major	O
step	O
forward	O
by	O
demonstrating	O
that	O
a	O
single	O
agent	O
can	O
learn	O
problem-specific	O
features	O
enabling	O
it	O
to	O
acquire	O
humancompetitive	O
skills	O
over	O
a	O
range	O
of	O
tasks	O
but	O
as	O
mnih	O
et	O
al	O
point	O
out	O
dqn	O
is	O
not	O
a	O
complete	O
solution	O
to	O
the	O
problem	O
of	O
task-independent	O
learning	O
although	O
the	O
skills	O
needed	O
to	O
excel	O
on	O
the	O
atari	O
games	O
were	O
markedly	O
diverse	O
all	O
the	O
games	O
were	O
played	O
by	O
observing	O
video	O
images	O
which	O
made	O
a	O
deep	O
convolutional	O
ann	O
a	O
natural	O
choice	O
for	O
this	O
collection	O
of	O
tasks	O
in	O
addition	O
dqn	O
s	O
performance	O
on	O
some	O
of	O
the	O
atari	O
games	O
fell	O
considerably	O
short	O
of	O
human	O
skill	O
levels	O
on	O
these	O
games	O
the	O
games	O
most	O
difficult	O
for	O
dqn	O
especially	O
montezuma	O
s	O
revenge	O
on	O
which	O
dqn	O
learned	O
to	O
perform	O
about	O
as	O
well	O
as	O
the	O
random	O
player	O
require	O
deep	O
planning	B
beyond	O
what	O
dqn	O
was	O
designed	O
to	O
do	O
further	O
learning	O
control	B
skills	O
through	O
extensive	O
practice	O
like	O
dqn	O
learned	O
how	O
to	O
play	O
the	O
atari	O
games	O
is	O
just	O
one	O
of	O
the	O
types	O
of	O
learning	O
humans	O
routinely	O
accomplish	O
despite	O
these	O
limitations	O
dqn	O
advanced	O
the	O
state-of-the-art	O
in	O
machine	O
learning	O
by	O
impressively	O
demonstrating	O
the	O
promise	O
of	O
combining	O
reinforcement	B
learning	I
with	O
modern	O
methods	O
of	O
deep	B
learning	I
mastering	O
the	O
game	O
of	O
go	O
the	O
ancient	O
chinese	O
game	O
of	O
go	O
has	O
challenged	O
artificial	B
intelligence	I
researchers	O
for	O
many	O
decades	O
methods	O
that	O
achieve	O
human-level	O
skill	O
or	O
even	O
superhuman-level	O
skill	O
chapter	O
applications	B
and	I
case	I
studies	I
in	O
other	O
games	O
have	O
not	O
been	O
successful	O
in	O
producing	O
strong	O
go	O
programs	O
thanks	O
to	O
a	O
very	O
active	O
community	O
of	O
go	O
programmers	O
and	O
international	O
competitions	O
the	O
level	O
of	O
go	O
program	O
play	O
has	O
improved	O
significantly	O
over	O
the	O
years	O
until	O
recently	O
however	O
no	O
go	O
program	O
had	O
been	O
able	O
to	O
play	O
anywhere	O
near	O
the	O
level	O
of	O
a	O
human	O
go	O
master	O
a	O
team	O
at	O
deepmind	O
et	O
al	O
developed	O
the	O
program	O
alphago	B
that	O
broke	O
this	O
barrier	O
by	O
combining	O
deep	O
artificial	B
neural	B
networks	I
anns	O
section	O
supervised	B
learning	I
monte	B
carlo	I
tree	O
search	O
section	O
and	B
reinforcement	B
learning	I
by	O
the	O
time	O
of	O
silver	O
et	O
al	O
s	O
publication	O
alphago	B
had	O
been	O
shown	O
to	O
be	O
decisively	O
stronger	O
than	O
other	O
current	O
go	O
programs	O
and	O
it	O
had	O
defeated	O
the	O
european	O
go	O
champion	O
fan	O
hui	O
games	O
to	O
these	O
were	O
the	O
first	O
victories	O
of	O
a	O
go	O
program	O
over	O
a	O
professional	O
human	O
go	O
player	O
without	O
handicap	O
in	O
full	O
go	O
games	O
shortly	O
thereafter	O
a	O
similar	O
version	O
of	O
alphago	B
won	O
stunning	O
victories	O
over	O
the	O
world	O
champion	O
lee	O
sedol	O
winning	O
out	O
of	O
a	O
games	O
in	O
a	O
challenge	O
match	O
making	O
worldwide	O
headline	O
news	O
artificial	B
intelligence	I
researchers	O
thought	O
that	O
it	O
would	O
be	O
many	O
more	O
years	O
perhaps	O
decades	O
before	O
a	O
program	O
reached	O
this	O
level	O
of	O
play	O
here	O
we	O
describe	O
alphago	B
and	O
a	O
successor	O
program	O
called	O
alphago	B
zero	O
et	O
al	O
where	O
in	O
addition	O
to	O
reinforcement	B
learning	I
alphago	B
relied	O
on	O
supervised	B
learning	I
from	O
a	O
large	O
database	O
of	O
expert	O
human	O
moves	O
alphago	B
zero	O
used	O
only	O
reinforcement	B
learning	I
and	O
no	O
human	O
data	O
or	O
guidance	O
beyond	O
the	O
basic	O
rules	O
of	O
the	O
game	O
the	O
zero	O
in	O
its	O
name	O
we	O
first	O
describe	O
alphago	B
in	O
some	O
detail	O
in	O
order	O
to	O
highlight	O
the	O
relative	O
simplicity	O
of	O
alphago	B
zero	O
which	O
is	O
both	O
higher-performing	O
and	O
more	O
of	O
a	O
pure	O
reinforcement	B
learning	I
program	O
in	O
many	O
ways	O
both	O
alphago	B
and	O
alphago	B
zero	O
are	O
descendants	O
of	O
tesauo	O
s	O
tdgammon	O
itself	O
a	O
descendant	O
of	O
samuel	O
s	O
checkers	O
player	O
all	O
these	O
programs	O
included	O
reinforcement	B
learning	I
over	O
simulated	O
games	O
of	O
self-play	O
alphago	B
and	O
alphago	B
zero	O
also	O
built	O
upon	O
the	O
progress	O
made	O
by	O
deepmind	O
on	O
playing	O
atari	O
games	O
with	O
the	O
program	O
dqn	O
that	O
used	O
deep	O
convolutional	O
anns	O
to	O
approximate	O
optimal	O
value	B
functions	O
go	O
is	O
a	O
game	O
between	O
two	O
players	O
who	O
alternately	O
place	O
black	O
and	O
white	O
stones	O
on	O
unoccupied	O
intersections	O
or	O
points	O
on	O
a	O
board	O
with	O
a	O
grid	O
of	O
horizontal	O
and	O
vertical	O
lines	O
to	O
produce	O
positions	O
like	O
that	O
shown	O
to	O
the	O
right	O
the	O
game	O
s	O
goal	O
is	O
to	O
capture	O
an	O
area	O
of	O
the	O
board	O
larger	O
than	O
that	O
captured	O
by	O
the	O
opponent	O
stones	O
are	O
captured	O
according	O
to	O
simple	O
rules	O
a	O
player	O
s	O
stones	O
are	O
captured	O
if	O
they	O
are	O
completely	O
surrounded	O
by	O
the	O
other	O
player	O
s	O
stones	O
meaning	O
that	O
there	O
is	O
no	O
horizontally	O
or	O
vertically	O
adjacent	O
point	O
that	O
is	O
unoccupied	O
for	O
example	O
figure	O
shows	O
on	O
the	O
left	O
three	O
white	O
stones	O
with	O
an	O
unoccupied	O
adjacent	O
point	O
x	O
if	O
player	O
black	O
places	O
a	O
stone	O
on	O
x	O
the	O
three	O
white	O
stones	O
are	O
captured	O
and	O
taken	O
a	O
go	O
board	O
configuration	O
mastering	O
the	O
game	O
of	O
go	O
off	O
the	O
board	O
middle	O
however	O
if	O
player	O
white	O
were	O
to	O
place	O
a	O
stone	O
on	O
point	O
x	O
first	O
than	O
the	O
possibility	O
of	O
this	O
capture	O
would	O
be	O
blocked	O
right	O
other	O
rules	O
are	O
needed	O
to	O
prevent	O
infinite	O
capturingre-capturing	O
loops	O
the	O
game	O
ends	O
when	O
neither	O
player	O
wishes	O
to	O
place	O
another	O
stone	O
these	O
rules	O
are	O
simple	O
but	O
they	O
produce	O
a	O
very	O
complex	O
game	O
that	O
has	O
had	O
wide	O
appeal	O
for	O
thousands	O
of	O
years	O
figure	O
go	O
capturing	O
rule	O
left	O
the	O
three	O
white	O
stones	O
are	O
not	O
surrounded	O
because	O
point	O
x	O
is	O
unoccupied	O
middle	O
if	O
black	O
places	O
a	O
stone	O
on	O
x	O
the	O
three	O
white	O
stones	O
are	O
captured	O
and	O
removed	O
from	O
the	O
board	O
right	O
if	O
white	O
places	O
a	O
stone	O
on	O
point	O
x	O
first	O
the	O
capture	O
is	O
blocked	O
methods	O
that	O
produce	O
strong	O
play	O
for	O
other	O
games	O
such	O
as	O
chess	B
have	O
not	O
worked	O
as	O
well	O
for	O
go	O
the	O
search	O
space	O
for	O
go	O
is	O
significantly	O
larger	O
than	O
that	O
of	O
chess	B
because	O
go	O
has	O
a	O
larger	O
number	O
of	O
legal	O
moves	O
per	O
position	O
than	O
chess	B
versus	O
and	O
go	O
games	O
tend	O
to	O
involve	O
more	O
moves	O
than	O
chess	B
games	O
versus	O
but	O
the	O
size	O
of	O
the	O
search	O
space	O
is	O
not	O
the	O
major	O
factor	O
that	O
makes	O
go	O
so	O
difficult	O
exhaustive	O
search	O
is	O
infeasible	O
for	O
both	O
chess	B
and	O
go	O
and	O
go	O
on	O
smaller	O
boards	O
e	O
g	O
has	O
proven	O
to	O
be	O
exceedingly	O
difficult	O
as	O
well	O
experts	O
agree	O
that	O
the	O
major	O
stumbling	O
block	O
to	O
creating	O
stronger-than-amateur	O
go	O
programs	O
is	O
the	O
difficulty	O
of	O
defining	O
an	O
adequate	O
position	O
evaluation	O
function	O
a	O
good	O
evaluation	O
function	O
allows	O
search	O
to	O
be	O
truncated	B
at	O
a	O
feasible	O
depth	O
by	O
providing	O
relatively	O
easy-to-compute	O
predictions	O
of	O
what	O
deeper	O
search	O
would	O
likely	O
yield	O
according	O
to	O
m	O
uller	O
no	O
simple	O
yet	O
reasonable	O
evaluation	O
function	O
will	O
ever	O
be	O
found	O
for	O
go	O
a	O
major	O
step	O
forward	O
was	O
the	O
introduction	O
of	O
mcts	O
to	O
go	O
programs	O
the	O
strongest	O
programs	O
at	O
the	O
time	O
of	O
alphago	B
s	O
development	O
all	O
included	O
mcts	O
but	O
master-level	O
skill	O
remained	O
elusive	O
recall	O
from	O
section	O
that	O
mcts	O
is	O
a	O
decision-time	O
planning	B
procedure	O
that	O
does	O
not	O
attempt	O
to	O
learn	O
and	O
store	O
a	O
global	O
evaluation	O
function	O
like	O
a	O
rollout	O
algorithm	O
it	O
runs	O
many	O
monte	B
carlo	I
simulations	O
of	O
entire	O
episodes	B
entire	O
go	O
games	O
to	O
select	O
each	O
action	B
each	O
go	O
move	O
where	O
to	O
place	O
a	O
stone	O
or	O
to	O
resign	O
unlike	O
a	O
simple	O
rollout	O
algorithm	O
however	O
mcts	O
is	O
an	O
iterative	B
procedure	O
that	O
incrementally	O
extends	O
a	O
search	O
tree	O
whose	O
root	O
node	O
represents	O
the	O
current	O
environment	B
state	B
as	O
illustrated	O
in	O
figure	O
each	O
iteration	O
traverses	O
the	O
tree	O
by	O
simulating	O
actions	O
guided	O
by	O
statistics	O
associated	O
with	O
the	O
tree	O
s	O
edges	O
in	O
its	O
basic	O
version	O
when	O
a	O
simulation	O
reaches	O
a	O
leaf	O
node	O
of	O
the	O
search	O
tree	O
mcts	O
expands	O
the	O
tree	O
by	O
adding	O
some	O
or	O
all	O
of	O
the	O
leaf	O
node	O
s	O
children	O
to	O
the	O
tree	O
from	O
the	O
leaf	O
node	O
or	O
one	O
of	O
its	O
newly	O
added	O
child	O
notes	O
a	O
rollout	O
is	O
executed	O
a	O
simulation	O
that	O
typically	O
proceeds	O
all	O
the	O
way	O
to	O
a	O
terminal	O
state	B
with	O
actions	O
selected	O
by	O
a	O
rollout	O
policy	B
when	O
the	O
rollout	O
completes	O
the	O
statistics	O
associated	O
with	O
the	O
search	O
tree	O
s	O
edges	O
that	O
were	O
traversed	O
in	O
this	O
iteration	O
are	O
updated	O
by	O
backing	O
up	O
the	O
return	B
produced	O
by	O
the	O
rollout	O
mcts	O
continues	O
this	O
process	O
starting	O
each	O
time	O
at	O
the	O
search	O
tree	O
s	O
root	O
at	O
the	O
current	O
state	B
for	O
as	O
many	O
iterations	O
as	O
possible	O
given	O
the	O
time	O
constraints	O
then	O
finally	O
an	O
action	B
from	O
the	O
root	O
node	O
still	O
represents	O
the	O
current	O
environment	B
state	B
is	O
selected	O
x	O
chapter	O
applications	B
and	I
case	I
studies	I
according	O
to	O
statistics	O
accumulated	O
in	O
the	O
root	O
node	O
s	O
outgoing	O
edges	O
this	O
is	O
the	O
action	B
the	O
agent	O
takes	O
after	O
the	O
environment	B
transitions	O
to	O
its	O
next	O
state	B
mcts	O
is	O
executed	O
again	O
with	O
the	O
root	O
node	O
set	O
to	O
represent	O
the	O
new	O
current	O
state	B
the	O
search	O
tree	O
at	O
the	O
start	O
of	O
this	O
next	O
execution	O
might	O
be	O
just	O
this	O
new	O
root	O
node	O
or	O
it	O
might	O
include	O
descendants	O
of	O
this	O
node	O
left	O
over	O
from	O
mcts	O
s	O
previous	O
execution	O
the	O
remainder	O
of	O
the	O
tree	O
is	O
discarded	O
alphago	B
the	O
main	O
innovation	O
that	O
made	O
alphago	B
such	O
a	O
strong	O
player	O
is	O
that	O
it	O
selected	O
moves	O
by	O
a	O
novel	O
version	O
of	O
mcts	O
that	O
was	O
guided	O
by	O
both	O
a	O
policy	B
and	O
a	O
value	B
function	I
learned	O
by	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
provided	O
by	O
deep	O
convolutional	O
anns	O
another	O
key	O
feature	O
is	O
that	O
instead	O
of	O
reinforcement	B
learning	I
starting	O
from	O
random	O
network	O
weights	O
it	O
started	O
from	O
weights	O
that	O
were	O
the	O
result	O
of	O
previous	O
supervised	B
learning	I
from	O
a	O
large	O
collection	O
of	O
human	O
expert	O
moves	O
the	O
deepmind	O
team	O
called	O
alphago	B
s	O
modification	O
of	O
basic	O
mcts	O
asynchronous	O
policy	B
and	O
value	B
mcts	O
or	O
apv-mcts	O
it	O
selected	O
actions	O
via	O
basic	O
mcts	O
as	O
described	O
above	O
but	O
with	O
some	O
twists	O
in	O
how	O
it	O
extended	O
its	O
search	O
tree	O
and	O
how	O
it	O
evaluated	O
action	B
edges	O
in	O
contrast	O
to	O
basic	O
mcts	O
which	O
expands	O
its	O
current	O
search	O
tree	O
by	O
using	O
stored	O
action	B
values	O
to	O
select	O
an	O
unexplored	O
edge	O
from	O
a	O
leaf	O
node	O
apv-mcts	O
as	O
implemented	O
in	O
alphago	B
expanded	O
its	O
tree	O
by	O
choosing	O
an	O
edge	O
according	O
to	O
probabilities	O
supplied	O
by	O
a	O
deep	O
convolutional	O
ann	O
called	O
the	O
sl-policy	O
network	O
trained	O
previously	O
by	O
supervised	B
learning	I
to	O
predict	O
moves	O
contained	O
in	O
a	O
database	O
of	O
nearly	O
million	O
human	O
expert	O
moves	O
then	O
also	O
in	O
contrast	O
to	O
basic	O
mcts	O
which	O
evaluates	O
the	O
newly-added	O
state	B
node	O
solely	O
by	O
the	O
return	B
of	O
a	O
rollout	O
initiated	O
from	O
it	O
apv-mcts	O
evaluated	O
the	O
node	O
in	O
two	O
ways	O
by	O
this	O
return	B
of	O
the	O
rollout	O
but	O
also	O
by	O
a	O
value	B
function	I
v	O
learned	O
previously	O
by	O
a	O
reinforcement	B
learning	I
method	O
if	O
s	O
was	O
the	O
newly-added	O
node	O
its	O
value	B
became	O
vs	O
g	O
where	O
g	O
was	O
the	O
return	B
of	O
the	O
rollout	O
and	O
controlled	O
the	O
mixing	O
of	O
the	O
values	O
resulting	O
from	O
these	O
two	O
evaluation	O
methods	O
in	O
alphago	B
these	O
values	O
were	O
supplied	O
by	O
the	O
value	B
network	O
another	O
deep	O
convolutional	O
ann	O
that	O
was	O
trained	O
as	O
we	O
describe	O
below	O
to	O
output	O
estimated	O
values	O
of	O
board	O
positions	O
apv-mcts	O
s	O
rollouts	O
in	O
alphago	B
were	O
simulated	O
games	O
with	O
both	O
players	O
using	O
a	O
fast	O
rollout	O
policy	B
provided	O
by	O
a	O
simple	O
linear	O
network	O
also	O
trained	O
by	O
supervised	B
learning	I
before	O
play	O
throughout	O
its	O
execution	O
apv-mcts	O
kept	O
track	O
of	O
how	O
many	O
simulations	O
passed	O
through	O
each	O
edge	O
of	O
the	O
search	O
tree	O
and	O
when	O
its	O
execution	O
completed	O
the	O
most-visited	O
edge	O
from	O
the	O
root	O
node	O
was	O
selected	O
as	O
the	O
action	B
to	O
take	O
here	O
the	O
move	O
alphago	B
actually	O
made	O
in	O
a	O
game	O
the	O
value	B
network	O
had	O
the	O
same	O
structure	O
as	O
the	O
deep	O
convolutional	O
sl	O
policy	B
network	O
except	O
that	O
it	O
had	O
a	O
single	O
output	O
unit	O
that	O
gave	O
estimated	O
values	O
of	O
game	O
positions	O
instead	O
of	O
the	O
sl	O
policy	B
network	O
s	O
probability	O
distributions	O
over	O
legal	O
actions	O
ideally	O
the	O
value	B
network	O
would	O
output	O
optimal	O
state	B
values	O
and	O
it	O
might	O
have	O
been	O
possible	O
to	O
approximate	O
the	O
optimal	O
value	B
function	I
along	O
the	O
lines	O
of	O
td-gammon	B
described	O
above	O
mastering	O
the	O
game	O
of	O
go	O
self-play	O
with	O
nonlinear	O
td	B
coupled	O
to	O
a	O
deep	O
convolutional	O
ann	O
but	O
the	O
deepmind	O
team	O
took	O
a	O
different	O
approach	O
that	O
held	O
more	O
promise	O
for	O
a	O
game	O
as	O
complex	O
as	O
go	O
they	O
divided	O
the	O
process	O
of	O
training	O
the	O
value	B
network	O
into	O
two	O
stages	O
in	O
the	O
first	O
stage	O
they	O
created	O
the	O
best	O
policy	B
they	O
could	O
by	O
using	O
reinforcement	B
learning	I
to	O
train	O
an	O
rl	O
policy	B
network	O
this	O
was	O
a	O
deep	O
convolutional	O
ann	O
with	O
the	O
same	O
structure	O
as	O
the	O
sl	O
policy	B
network	O
it	O
was	O
initialized	O
with	O
the	O
final	O
weights	O
of	O
the	O
sl	O
policy	B
network	O
that	O
were	O
learned	O
via	O
supervised	B
learning	I
and	O
then	O
policy-gradient	O
reinforcement	B
learning	I
was	O
used	O
to	O
improve	O
upon	O
the	O
sl	O
policy	B
in	O
the	O
second	O
stage	O
of	O
training	O
the	O
value	B
network	O
the	O
team	O
used	O
monte	B
carlo	I
policy	B
evaluation	O
on	O
data	O
obtained	O
from	O
a	O
large	O
number	O
of	O
simulated	O
self-play	O
games	O
with	O
moves	O
selected	O
by	O
the	O
rl	O
policy	B
network	O
figure	O
illustrates	O
the	O
networks	O
used	O
by	O
alphago	B
and	O
the	O
steps	O
taken	O
to	O
train	O
them	O
in	O
what	O
the	O
deepmind	O
team	O
called	O
the	O
alphago	B
pipeline	O
all	O
these	O
networks	O
were	O
trained	O
before	O
any	O
live	O
game	O
play	O
took	O
place	O
and	O
their	O
weights	O
remained	O
fixed	O
throughout	O
live	O
play	O
figure	O
alphago	B
pipeline	O
adapted	O
with	O
permission	O
from	O
macmillan	O
publishers	O
ltd	O
nature	O
vol	O
p	O
copyright	O
here	O
is	O
some	O
more	O
detail	O
about	O
alphago	B
s	O
anns	O
and	O
their	O
training	O
the	O
identicallystructured	O
sl	O
and	O
rl	O
policy	B
networks	O
were	O
similar	O
to	O
dqn	O
s	O
deep	O
convolutional	O
network	O
described	O
in	O
section	O
for	O
playing	O
atari	O
games	O
except	O
that	O
they	O
had	O
convolutional	O
layers	O
with	O
the	O
final	O
layer	O
consisting	O
of	O
a	O
soft-max	B
unit	O
for	O
each	O
point	O
on	O
the	O
go	O
board	O
the	O
networks	O
input	O
was	O
a	O
image	O
stack	O
in	O
which	O
each	O
point	O
on	O
the	O
go	O
board	O
was	O
represented	O
by	O
the	O
values	O
of	O
binary	O
or	O
integer-valued	O
features	O
for	O
example	O
for	O
each	O
point	O
one	O
feature	O
indicated	O
if	O
the	O
point	O
was	O
occupied	O
by	O
one	O
of	O
alphago	B
s	O
stones	O
one	O
of	O
its	O
opponent	O
s	O
stones	O
or	O
was	O
unoccupied	O
thus	O
providing	O
the	O
raw	O
representation	O
of	O
the	O
board	O
configuration	O
other	O
features	O
were	O
based	O
on	O
the	O
rules	O
of	O
go	O
such	O
as	O
the	O
number	O
of	O
adjacent	O
points	O
that	O
were	O
empty	O
the	O
number	O
of	O
opponent	O
stones	O
that	O
would	O
be	O
captured	O
by	O
placing	O
a	O
stone	O
there	O
the	O
number	O
of	O
turns	O
since	O
a	O
stone	O
rollout	O
policy	B
sl	O
policy	B
network	O
rl	O
policy	B
network	O
value	B
networkpolicy	O
gradientsupervised	O
learningmc	O
policy	B
evaluationself	O
playsupervised	O
learningnetworksdata	O
chapter	O
applications	B
and	I
case	I
studies	I
was	O
placed	O
there	O
and	O
other	O
features	O
that	O
the	O
design	O
team	O
considered	O
to	O
be	O
important	O
training	O
the	O
sl	O
policy	B
network	O
took	O
approximately	O
weeks	O
using	O
a	O
distributed	O
implementation	O
of	O
stochastic	O
gradient	B
ascent	O
on	O
processors	O
the	O
network	O
achieved	O
accuracy	O
where	O
the	O
best	O
accuracy	O
achieved	O
by	O
other	O
groups	O
at	O
the	O
time	O
of	O
publication	O
was	O
training	O
the	O
rl	O
policy	B
network	O
was	O
done	O
by	O
policy	B
gradient	B
reinforcement	B
learning	I
over	O
simulated	O
games	O
between	O
the	O
rl	O
policy	B
network	O
s	O
current	O
policy	B
and	O
opponents	O
using	O
policies	O
randomly	O
selected	O
from	O
policies	O
produced	O
by	O
earlier	O
iterations	O
of	O
the	O
learning	O
algorithm	O
playing	O
against	O
a	O
randomly	O
selected	O
collection	O
of	O
opponents	O
prevented	O
overfitting	O
to	O
the	O
current	O
policy	B
the	O
reward	B
signal	I
was	O
if	O
the	O
current	O
policy	B
won	O
if	O
it	O
lost	O
and	O
zero	O
otherwise	O
these	O
games	O
directly	O
pitted	O
the	O
two	O
policies	O
against	O
one	O
another	O
without	O
involving	O
mcts	O
by	O
simulating	O
many	O
games	O
in	O
parallel	O
on	O
processors	O
the	O
deepmind	O
team	O
trained	O
the	O
rl	O
policy	B
network	O
on	O
a	O
million	O
games	O
in	O
a	O
single	O
day	O
in	O
testing	O
the	O
final	O
rl	O
policy	B
they	O
found	O
that	O
it	O
won	O
more	O
than	O
of	O
games	O
played	O
against	O
the	O
sl	O
policy	B
and	O
it	O
won	O
of	O
games	O
played	O
against	O
a	O
go	O
program	O
using	O
mcts	O
that	O
simulated	O
games	O
per	O
move	O
the	O
value	B
network	O
whose	O
structure	O
was	O
similar	O
to	O
that	O
of	O
the	O
sl	O
and	O
rl	O
policy	B
networks	O
except	O
for	O
its	O
single	O
output	O
unit	O
received	O
the	O
same	O
input	O
as	O
the	O
sl	O
and	O
rl	O
policy	B
networks	O
with	O
the	O
exception	O
that	O
there	O
was	O
an	O
additional	O
binary	O
feature	O
giving	O
the	O
current	O
color	O
to	O
play	O
monte	B
carlo	I
policy	B
evaluation	O
was	O
used	O
to	O
train	O
the	O
network	O
from	O
data	O
obtained	O
from	O
a	O
large	O
number	O
of	O
self-play	O
games	O
played	O
using	O
the	O
rl	O
policy	B
to	O
avoid	O
overfitting	O
and	O
instability	O
due	O
to	O
the	O
strong	O
correlations	O
between	O
positions	O
encountered	O
in	O
self-play	O
the	O
deepmind	O
team	O
constructed	O
a	O
data	O
set	O
of	O
million	O
positions	O
each	O
chosen	O
randomly	O
from	O
a	O
unique	O
self-play	O
game	O
then	O
training	O
was	O
done	O
using	O
million	O
mini-batches	O
each	O
of	O
positions	O
drawn	O
from	O
this	O
data	O
set	O
training	O
took	O
one	O
week	O
on	O
gpus	O
the	O
rollout	O
policy	B
was	O
learned	O
prior	O
to	O
play	O
by	O
a	O
simple	O
linear	O
network	O
trained	O
by	O
supervised	B
learning	I
from	O
a	O
corpus	O
of	O
million	O
human	O
moves	O
the	O
rollout	O
policy	B
network	O
had	O
to	O
output	O
actions	O
quickly	O
while	O
still	O
being	O
reasonably	O
accurate	O
in	O
principle	O
the	O
sl	O
or	O
rl	O
policy	B
networks	O
could	O
have	O
been	O
used	O
in	O
the	O
rollouts	O
but	O
the	O
forward	O
propagation	O
through	O
these	O
deep	O
networks	O
took	O
too	O
much	O
time	O
for	O
either	O
of	O
them	O
to	O
be	O
used	O
in	O
rollout	O
simulations	O
a	O
great	O
many	O
of	O
which	O
had	O
to	O
be	O
carried	O
out	O
for	O
each	O
move	O
decision	O
during	O
live	O
play	O
for	O
this	O
reason	O
the	O
rollout	O
policy	B
network	O
was	O
less	O
complex	O
than	O
the	O
other	O
policy	B
networks	O
and	O
its	O
input	O
features	O
could	O
be	O
computed	O
more	O
quickly	O
than	O
the	O
features	O
used	O
for	O
the	O
policy	B
networks	O
the	O
rollout	O
policy	B
network	O
allowed	O
approximately	O
complete	O
game	O
simulations	O
per	O
second	O
to	O
be	O
run	O
on	O
each	O
of	O
the	O
processing	O
threads	O
that	O
alphago	B
used	O
one	O
may	O
wonder	O
why	O
the	O
sl	O
policy	B
was	O
used	O
instead	O
of	O
the	O
better	O
rl	O
policy	B
to	O
select	O
actions	O
in	O
the	O
expansion	O
phase	O
of	O
apv-mcts	O
these	O
policies	O
took	O
the	O
same	O
amount	O
of	O
time	O
to	O
compute	O
because	O
they	O
used	O
the	O
same	O
network	O
architecture	O
the	O
team	O
actually	O
found	O
that	O
alphago	B
played	O
better	O
against	O
human	O
opponents	O
when	O
apv-mcts	O
used	O
as	O
the	O
sl	O
policy	B
instead	O
of	O
the	O
rl	O
policy	B
they	O
conjectured	O
that	O
the	O
reason	O
for	O
this	O
was	O
that	O
the	O
latter	O
was	O
tuned	O
to	O
respond	O
to	O
optimal	O
moves	O
rather	O
than	O
to	O
the	O
broader	O
set	O
of	O
moves	O
characteristic	O
of	O
human	O
play	O
interestingly	O
the	O
situation	O
was	O
reversed	O
for	O
the	O
value	B
function	I
used	O
by	O
apv-mcts	O
they	O
found	O
that	O
when	O
apv-mcts	O
used	O
the	O
value	B
mastering	O
the	O
game	O
of	O
go	O
function	O
derived	O
from	O
the	O
rl	O
policy	B
it	O
performed	O
better	O
than	O
if	O
it	O
used	O
the	O
value	B
function	I
derived	O
from	O
the	O
sl	O
policy	B
several	O
methods	O
worked	O
together	O
to	O
produce	O
alphago	B
s	O
impressive	O
playing	O
skill	O
the	O
deepmind	O
team	O
evaluated	O
different	O
versions	O
of	O
alphago	B
in	O
order	O
to	O
assess	O
the	O
contributions	O
made	O
by	O
these	O
various	O
components	O
the	O
parameter	O
in	O
controlled	O
the	O
mixing	O
of	O
game	O
state	B
evaluations	O
produced	O
by	O
the	O
value	B
network	O
and	O
by	O
rollouts	O
with	O
alphago	B
used	O
just	O
the	O
value	B
network	O
without	O
rollouts	O
and	O
with	O
evaluation	O
relied	O
just	O
on	O
rollouts	O
they	O
found	O
that	O
alphago	B
using	O
just	O
the	O
value	B
network	O
played	O
better	O
than	O
the	O
rollout-only	O
alphago	B
and	O
in	O
fact	O
played	O
better	O
than	O
the	O
strongest	O
of	O
all	O
other	O
go	O
programs	O
existing	O
at	O
the	O
time	O
the	O
best	O
play	O
resulted	O
from	O
setting	O
indicating	O
that	O
combining	O
the	O
value	B
network	O
with	O
rollouts	O
was	O
particularly	O
important	O
to	O
alphago	B
s	O
success	O
these	O
evaluation	O
methods	O
complemented	O
one	O
another	O
the	O
value	B
network	O
evaluated	O
the	O
high-performance	O
rl	O
policy	B
that	O
was	O
too	O
slow	O
to	O
be	O
used	O
in	O
live	O
play	O
while	O
rollouts	O
using	O
the	O
weaker	O
but	O
much	O
faster	O
rollout	O
policy	B
were	O
able	O
to	O
add	O
precision	O
to	O
the	O
value	B
network	O
s	O
evaluations	O
for	O
specific	O
states	O
that	O
occurred	O
during	O
games	O
overall	O
alphago	B
s	O
remarkable	O
success	O
fueled	O
a	O
new	O
round	O
of	O
enthusiasm	O
for	O
the	O
promise	O
of	O
artificial	B
intelligence	I
specifically	O
for	O
systems	O
combining	O
reinforcement	B
learning	I
with	O
deep	O
anns	O
to	O
address	O
problems	O
in	O
other	O
challenging	O
domains	O
alphago	B
zero	O
building	O
upon	O
the	O
experience	O
with	O
alphago	B
a	O
deepmind	O
team	O
developed	O
alphago	B
zero	O
et	O
al	O
in	O
contrast	O
to	O
alphago	B
this	O
program	O
used	O
no	O
human	O
data	O
or	O
guidance	O
beyond	O
the	O
basic	O
rules	O
of	O
the	O
game	O
the	O
zero	O
in	O
its	O
name	O
it	O
learned	O
exclusively	O
from	O
self-play	O
reinforcement	B
learning	I
with	O
input	O
giving	O
just	O
raw	O
descriptions	O
of	O
the	O
placements	O
of	O
stones	O
on	O
the	O
go	O
board	O
alphago	B
zero	O
implemented	O
a	O
form	O
of	O
policy	B
iteration	I
interleaving	O
policy	B
evaluation	O
with	O
policy	B
improvement	I
figure	O
is	O
an	O
overview	O
of	O
alphago	B
zero	O
s	O
algorithm	O
a	O
significant	O
difference	O
between	O
alphago	B
zero	O
and	O
alphago	B
is	O
that	O
alphago	B
zero	O
used	O
mcts	O
to	O
select	O
moves	O
throughout	O
self-play	O
reinforcement	B
learning	I
whereas	O
alphago	B
used	O
mcts	O
for	O
live	O
play	O
after	O
but	O
not	O
during	O
learning	O
other	O
differences	O
besides	O
not	O
using	O
any	O
human	O
data	O
or	O
human-crafted	O
features	O
are	O
that	O
alphago	B
zero	O
used	O
only	O
one	O
deep	O
convolutional	O
ann	O
and	O
used	O
a	O
simpler	O
version	O
of	O
mcts	O
alphago	B
zero	O
s	O
mcts	O
was	O
simpler	O
than	O
the	O
version	O
used	O
by	O
alphago	B
in	O
that	O
it	O
did	O
not	O
include	O
rollouts	O
of	O
complete	O
games	O
and	O
therefore	O
did	O
not	O
need	O
a	O
rollout	O
policy	B
each	O
iteration	O
of	O
alphago	B
zero	O
s	O
mcts	O
ran	O
a	O
simulation	O
that	O
ended	O
at	O
a	O
leaf	O
node	O
of	O
the	O
current	O
search	O
tree	O
instead	O
of	O
at	O
the	O
terminal	O
position	O
of	O
a	O
complete	O
game	O
simulation	O
but	O
as	O
in	O
alphago	B
each	O
iteration	O
of	O
mcts	O
in	O
alphago	B
zero	O
was	O
guided	O
by	O
the	O
output	O
of	O
a	O
deep	O
convolutional	O
network	O
labeled	O
f	O
in	O
figure	O
were	O
is	O
the	O
network	O
s	O
weight	O
vector	B
the	O
input	O
to	O
the	O
network	O
whose	O
architecture	O
we	O
describe	O
below	O
consisted	O
of	O
raw	O
representations	O
of	O
board	O
positions	O
and	O
its	O
output	O
had	O
two	O
parts	O
a	O
scalar	O
value	B
v	O
an	O
estimate	O
of	O
the	O
probability	O
that	O
the	O
current	O
player	O
will	O
win	O
from	O
from	O
the	O
current	O
board	O
position	O
and	O
a	O
vector	B
p	O
of	O
move	O
probabilities	O
one	O
for	O
each	O
possible	O
stone	O
placement	O
on	O
the	O
current	O
board	O
plus	O
the	O
pass	O
or	O
resign	O
move	O
chapter	O
applications	B
and	I
case	I
studies	I
figure	O
alphago	B
zero	O
self-play	O
reinforcement	B
learning	I
a	O
the	O
program	O
played	O
many	O
games	O
against	O
itself	O
one	O
shown	O
here	O
as	O
a	O
sequence	O
of	O
board	O
positions	O
si	O
i	O
t	O
with	O
moves	O
ai	O
i	O
t	O
and	O
winner	O
z	O
each	O
move	O
ai	O
was	O
determined	O
by	O
action	B
probabilities	O
i	O
returned	O
by	O
mcts	O
executed	O
from	O
root	O
node	O
si	O
and	O
guided	O
by	O
a	O
deep	O
convolutional	O
network	O
here	O
labeled	O
f	O
with	O
latest	O
weights	O
shown	O
here	O
for	O
just	O
one	O
position	O
s	O
but	O
repeated	O
for	O
all	O
si	O
the	O
network	O
s	O
inputs	O
were	O
raw	O
representations	O
of	O
board	O
positions	O
si	O
with	O
several	O
past	O
positions	O
though	O
not	O
shown	O
here	O
and	O
its	O
outputs	O
were	O
vectors	O
p	O
of	O
move	O
probabilities	O
that	O
guided	O
mcts	O
s	O
forward	O
searches	O
and	O
scalar	O
values	O
v	O
that	O
estimated	O
the	O
probability	O
of	O
the	O
current	O
player	O
winning	O
from	O
each	O
position	O
si	O
b	O
deep	O
convolutional	O
network	O
training	O
training	O
examples	O
were	O
randomly	O
sampled	O
steps	O
from	O
recent	O
self-play	O
games	O
weights	O
were	O
updated	O
to	O
move	O
the	O
policy	B
vector	B
p	O
toward	O
the	O
probabilities	O
returned	O
by	O
mcts	O
and	O
to	O
include	O
the	O
winners	O
z	O
in	O
the	O
estimated	O
win	O
probability	O
v	O
reprinted	O
from	O
draft	O
of	O
silver	O
et	O
al	O
with	O
permission	O
of	O
the	O
authors	O
and	O
deepmind	O
instead	O
of	O
selecting	O
self-play	O
actions	O
according	O
to	O
the	O
probabilities	O
p	O
however	O
alphago	B
zero	O
used	O
these	O
probabilities	O
together	O
with	O
the	O
network	O
s	O
value	B
output	O
to	O
direct	O
each	O
execution	O
of	O
mcts	O
which	O
returned	O
new	O
move	O
probabilities	O
shown	O
in	O
figure	O
as	O
the	O
policies	O
i	O
these	O
policies	O
benefitted	O
from	O
the	O
many	O
simulations	O
that	O
mcts	O
conducted	O
each	O
time	O
it	O
executed	O
the	O
result	O
was	O
that	O
the	O
policy	B
actually	O
followed	O
by	O
alphago	B
zero	O
was	O
an	O
improvement	O
over	O
the	O
policy	B
given	O
by	O
the	O
network	O
s	O
outputs	O
p	O
silver	O
et	O
al	O
wrote	O
that	O
mcts	O
may	O
therefore	O
be	O
viewed	O
as	O
a	O
powerful	O
policy	B
improvement	I
operator	O
here	O
is	O
more	O
detail	O
about	O
alphago	B
zero	O
s	O
ann	O
and	O
how	O
it	O
was	O
trained	O
the	O
network	O
took	O
as	O
input	O
a	O
image	O
stack	O
consisting	O
of	O
binary	O
feature	O
planes	O
the	O
first	O
feature	O
planes	O
were	O
raw	O
representations	O
of	O
the	O
positions	O
of	O
the	O
current	O
player	O
s	O
stones	O
in	O
the	O
current	O
and	O
seven	O
past	O
board	O
configurations	O
a	O
feature	O
value	B
was	O
if	O
a	O
player	O
s	O
stone	O
was	O
on	O
the	O
corresponding	O
point	O
and	O
was	O
otherwise	O
the	O
next	O
feature	O
planes	O
t	O
theterminalpositionstisscoredtocomputethegamewinnerz	O
bneuralnetworktraininginalphagozero	O
theneuralnetworktakestherawboardpositionsasitsinputpassesitthroughmanyconvolutionallayerswithparameters	O
areupdatedsoastomaximisethesimilarityofthepolicyvectorptothesearchprobabilities	O
mastering	O
the	O
game	O
of	O
go	O
similarly	O
coded	O
the	O
positions	O
of	O
the	O
opponent	O
s	O
stones	O
a	O
final	O
input	O
feature	O
plane	O
had	O
a	O
constant	O
value	B
indicating	O
the	O
color	O
of	O
the	O
current	O
play	O
for	O
black	O
for	O
white	O
because	O
repetition	O
is	O
not	O
allowed	O
in	O
go	O
and	O
one	O
player	O
is	O
given	O
some	O
number	O
of	O
compensation	O
points	O
for	O
not	O
getting	O
the	O
first	O
move	O
the	O
current	O
board	O
position	O
is	O
not	O
a	O
markov	O
state	B
of	O
go	O
this	O
is	O
why	O
features	O
describing	O
past	O
board	O
positions	O
and	O
the	O
color	O
feature	O
were	O
needed	O
the	O
network	O
was	O
two-headed	O
meaning	O
that	O
after	O
a	O
number	O
of	O
initial	O
layers	O
the	O
network	O
split	O
into	O
two	O
separate	O
heads	O
of	O
additional	O
layers	O
that	O
separately	O
fed	O
into	O
two	O
sets	O
of	O
output	O
units	O
in	O
this	O
case	O
one	O
head	O
fed	O
output	O
units	O
producing	O
move	O
probabilities	O
p	O
one	O
for	O
each	O
possible	O
stone	O
placement	O
plus	O
pass	O
the	O
other	O
head	O
fed	O
just	O
one	O
output	O
unit	O
producing	O
the	O
scalar	O
v	O
an	O
estimate	O
of	O
the	O
probability	O
that	O
the	O
current	O
player	O
will	O
win	O
from	O
the	O
current	O
board	O
position	O
the	O
network	O
before	O
the	O
split	O
consisted	O
of	O
convolutional	O
layers	O
each	O
followed	O
by	O
batch	O
normalization	O
and	O
with	O
skip	O
connections	O
added	O
to	O
implement	O
residual	O
learning	O
by	O
pairs	O
of	O
layers	O
section	O
overall	O
move	O
probabilities	O
and	O
values	O
were	O
computed	O
by	O
and	O
layers	O
respectively	O
starting	O
with	O
random	O
weights	O
the	O
network	O
was	O
trained	O
by	O
stochastic	O
gradient	B
descent	O
momentum	O
regularization	O
and	O
step-size	B
parameter	I
decreasing	O
as	O
training	O
continues	O
using	O
batches	O
of	O
examples	O
sampled	O
uniformly	O
at	O
random	O
from	O
all	O
the	O
steps	O
of	O
the	O
most	O
recent	O
games	O
of	O
self-play	O
with	O
the	O
current	O
best	O
policy	B
extra	O
noise	O
was	O
added	O
to	O
the	O
network	O
s	O
output	O
p	O
to	O
encourage	O
exploration	O
of	O
all	O
possible	O
moves	O
at	O
periodic	O
checkpoints	O
during	O
training	O
which	O
silver	O
et	O
al	O
chose	O
to	O
be	O
at	O
every	O
training	O
steps	O
the	O
policy	B
output	O
by	O
the	O
ann	O
with	O
the	O
latest	O
weights	O
was	O
evaluated	O
by	O
simulating	O
games	O
mcts	O
with	O
iterations	O
to	O
select	O
each	O
move	O
against	O
the	O
current	O
best	O
policy	B
if	O
the	O
new	O
policy	B
won	O
a	O
margin	O
set	O
to	O
reduce	O
noise	O
in	O
the	O
outcome	O
then	O
it	O
became	O
the	O
best	O
policy	B
to	O
be	O
used	O
in	O
subsequent	O
self-play	O
the	O
network	O
s	O
weights	O
were	O
updated	O
to	O
make	O
the	O
network	O
s	O
policy	B
output	O
p	O
more	O
closely	O
match	O
the	O
policy	B
returned	O
by	O
mcts	O
and	O
to	O
make	O
its	O
value	B
output	O
v	O
more	O
closely	O
match	O
the	O
probability	O
that	O
the	O
current	O
best	O
policy	B
wins	O
from	O
the	O
board	O
position	O
represented	O
by	O
the	O
network	O
s	O
input	O
the	O
deepmind	O
team	O
trained	O
alphago	B
zero	O
over	O
million	O
games	O
of	O
self-play	O
which	O
took	O
about	O
days	O
each	O
move	O
of	O
each	O
game	O
was	O
selected	O
by	O
running	O
mcts	O
for	O
iterations	O
taking	O
approximately	O
second	O
per	O
move	O
network	O
weights	O
were	O
updated	O
over	O
batches	O
each	O
consisting	O
of	O
board	O
configurations	O
they	O
then	O
ran	O
tournaments	O
with	O
the	O
trained	O
alphago	B
zero	O
playing	O
against	O
the	O
version	O
of	O
alphago	B
that	O
defeated	O
fan	O
hui	O
by	O
games	O
to	O
and	O
against	O
the	O
version	O
that	O
defeated	O
lee	O
sedol	O
by	O
games	O
to	O
they	O
used	O
the	O
elo	O
rating	O
system	O
to	O
evaluate	O
the	O
relative	O
performances	O
of	O
the	O
programs	O
the	O
difference	O
between	O
two	O
elo	O
ratings	O
is	O
meant	O
to	O
predict	O
the	O
outcome	O
of	O
games	O
between	O
the	O
players	O
the	O
elo	O
ratings	O
of	O
alphago	B
zero	O
the	O
version	O
of	O
alphago	B
that	O
played	O
against	O
fan	O
hui	O
and	O
the	O
version	O
that	O
played	O
against	O
lee	O
sedol	O
were	O
respectively	O
and	O
the	O
gaps	O
in	O
these	O
elo	O
ratings	O
translate	O
into	O
predictions	O
that	O
alphago	B
zero	O
would	O
defeat	O
these	O
other	O
programs	O
with	O
probabilities	O
very	O
close	O
to	O
one	O
in	O
a	O
match	O
of	O
games	O
between	O
alphago	B
zero	O
trained	O
as	O
described	O
and	O
the	O
exact	O
version	O
of	O
alphago	B
that	O
defeated	O
lee	O
sedol	O
held	O
under	O
the	O
same	O
conditions	O
that	O
were	O
used	O
in	O
that	O
match	O
alphago	B
zero	O
defeated	O
alphago	B
in	O
all	O
games	O
chapter	O
applications	B
and	I
case	I
studies	I
the	O
deepmind	O
team	O
also	O
compared	O
alphago	B
zero	O
with	O
a	O
program	O
using	O
an	O
ann	O
with	O
the	O
same	O
architecture	O
but	O
trained	O
by	O
supervised	B
learning	I
to	O
predict	O
human	O
moves	O
in	O
a	O
data	O
set	O
containing	O
nearly	O
million	O
positions	O
from	O
games	O
they	O
found	O
that	O
the	O
supervised-learning	O
player	O
initially	O
played	O
better	O
than	O
alphago	B
zero	O
and	O
was	O
better	O
at	O
predicting	O
human	O
expert	O
moves	O
but	O
played	O
less	O
well	O
after	O
alphago	B
zero	O
was	O
trained	O
for	O
a	O
day	O
this	O
suggested	O
that	O
alphago	B
zero	O
had	O
discovered	O
a	O
strategy	O
for	O
playing	O
that	O
was	O
different	O
from	O
how	O
humans	O
play	O
in	O
fact	O
alphago	B
zero	O
discovered	O
and	O
came	O
to	O
prefer	O
some	O
novel	O
variations	O
of	O
classical	O
move	O
sequences	O
final	O
tests	O
of	O
alphago	B
zero	O
s	O
algorithm	O
were	O
conducted	O
with	O
a	O
version	O
having	O
a	O
larger	O
ann	O
and	O
trained	O
over	O
million	O
self-play	O
games	O
which	O
took	O
about	O
days	O
again	O
starting	O
with	O
random	O
weights	O
this	O
version	O
achieved	O
an	O
elo	O
rating	O
of	O
the	O
team	O
pitted	O
this	O
version	O
of	O
alphago	B
zero	O
against	O
a	O
program	O
called	O
alphago	B
master	O
the	O
strongest	O
program	O
at	O
the	O
time	O
that	O
was	O
identical	O
to	O
alphago	B
zero	O
but	O
like	O
alphago	B
used	O
human	O
data	O
and	O
features	O
alphago	B
master	O
s	O
elo	O
rating	O
was	O
and	O
it	O
had	O
defeated	O
the	O
strongest	O
human	O
professional	O
players	O
to	O
in	O
online	B
games	O
in	O
a	O
game	O
match	O
alphago	B
zero	O
with	O
the	O
larger	O
network	O
and	O
more	O
extensive	O
learning	O
defeated	O
alphago	B
master	O
games	O
to	O
thus	O
providing	O
a	O
convincing	O
demonstration	O
of	O
the	O
problem-solving	O
power	O
of	O
alphago	B
zero	O
s	O
algorithm	O
alphago	B
zero	O
soundly	O
demonstrated	O
that	O
superhuman	O
performance	O
can	O
be	O
achieved	O
by	O
pure	O
reinforcement	B
learning	I
augmented	O
by	O
a	O
simple	O
version	O
of	O
mcts	O
and	O
deep	O
anns	O
with	O
very	O
minimal	O
knowledge	O
of	O
the	O
domain	O
and	O
no	O
reliance	O
on	O
human	O
data	O
or	O
guidance	O
we	O
will	O
surely	O
see	O
systems	O
inspired	O
by	O
the	O
deepmind	O
accomplishments	O
of	O
both	O
alphago	B
and	O
alphago	B
zero	O
applied	O
to	O
challenging	O
problems	O
in	O
other	O
domains	O
recently	O
yet	O
a	O
better	O
program	O
alphazero	O
was	O
described	O
by	O
silver	O
et	O
al	O
that	O
does	O
not	O
even	O
incorporate	O
knowledge	O
of	O
go	O
alphazero	O
is	O
a	O
general	O
reinforcement	B
learning	I
algorithm	O
that	O
improves	O
over	O
the	O
world	O
s	O
hitherto	O
best	O
programs	O
in	O
the	O
diverse	O
games	O
of	O
go	O
chess	B
and	O
shogi	O
personalized	O
web	O
services	O
personalizing	B
web	I
services	I
such	O
as	O
the	O
delivery	O
of	O
news	O
articles	O
or	O
advertisements	O
is	O
one	O
approach	O
to	O
increasing	O
users	O
satisfaction	O
with	O
a	O
website	O
or	O
to	O
increase	O
the	O
yield	O
of	O
a	O
marketing	O
campaign	O
a	O
policy	B
can	O
recommend	O
content	O
considered	O
to	O
be	O
the	O
best	O
for	O
each	O
particular	O
user	O
based	O
on	O
a	O
profile	O
of	O
that	O
user	O
s	O
interests	O
and	O
preferences	O
inferred	O
from	O
their	O
history	B
of	I
online	B
activity	O
this	O
is	O
a	O
natural	O
domain	O
for	O
machine	O
learning	O
and	O
in	O
particular	O
for	O
reinforcement	B
learning	I
a	O
reinforcement	B
learning	I
system	O
can	O
improve	O
a	O
recommendation	O
policy	B
by	O
making	O
adjustments	O
in	O
response	O
to	O
user	O
feedback	O
one	O
way	O
to	O
obtain	O
user	O
feedback	O
is	O
by	O
means	O
of	O
website	O
satisfaction	O
surveys	O
but	O
for	O
acquiring	O
feedback	O
in	O
real	O
time	O
it	O
is	O
common	O
to	O
monitor	O
user	O
clicks	O
as	O
indicators	O
of	O
interest	O
in	O
a	O
link	O
a	O
method	O
long	O
used	O
in	O
marketing	O
called	O
ab	O
testing	O
is	O
a	O
simple	O
type	O
of	O
reinforcement	B
learning	I
used	O
to	O
decide	O
which	O
of	O
two	O
versions	O
a	O
or	O
b	O
of	O
a	O
website	O
users	O
prefer	O
because	O
it	O
is	O
non-associative	O
like	O
a	O
two-armed	O
bandit	O
problem	O
this	O
approach	O
does	O
not	O
personalize	O
content	O
delivery	O
adding	O
context	O
consisting	O
of	O
features	O
describing	O
individual	O
users	O
and	O
personalized	O
web	O
services	O
the	O
content	O
to	O
be	O
delivered	O
allows	O
personalizing	O
service	O
this	O
has	O
been	O
formalized	O
as	O
a	O
contextual	O
bandit	O
problem	O
an	O
associative	B
reinforcement	B
learning	I
problem	O
section	O
with	O
the	O
objective	O
of	O
maximizing	O
the	O
total	O
number	O
of	O
user	O
clicks	O
li	O
chu	O
langford	O
and	O
schapire	O
applied	O
a	O
contextual	O
bandit	B
algorithm	I
to	O
the	O
problem	O
of	O
personalizing	O
the	O
yahoo	O
front	O
page	O
today	O
webpage	O
of	O
the	O
most	O
visited	O
pages	O
on	O
the	O
internet	O
at	O
the	O
time	O
of	O
their	O
research	O
by	O
selecting	O
the	O
news	O
story	O
to	O
feature	O
their	O
objective	O
was	O
to	O
maximize	O
the	O
click-through	O
rate	O
which	O
is	O
the	O
ratio	B
of	O
the	O
total	O
number	O
of	O
clicks	O
all	O
users	O
make	O
on	O
a	O
webpage	O
to	O
the	O
total	O
number	O
of	O
visits	O
to	O
the	O
page	O
their	O
contextual	O
bandit	B
algorithm	I
improved	O
over	O
a	O
standard	O
non-associative	O
bandit	B
algorithm	I
by	O
theocharous	O
thomas	O
and	O
ghavamzadeh	O
argued	O
that	O
better	O
results	O
are	O
possible	O
by	O
formulating	O
personalized	O
recommendation	O
as	O
a	O
markov	O
decision	O
problem	O
with	O
the	O
objective	O
of	O
maximizing	O
the	O
total	O
number	O
of	O
clicks	O
users	O
make	O
over	O
repeated	O
visits	O
to	O
a	O
website	O
policies	O
derived	O
from	O
the	O
contextual	O
bandit	O
formulation	O
are	O
greedy	O
in	O
the	O
sense	O
that	O
they	O
do	O
not	O
take	O
long-term	O
effects	O
of	O
actions	O
into	O
account	O
these	O
policies	O
effectively	O
treat	O
each	O
visit	O
to	O
a	O
website	O
as	O
if	O
it	O
were	O
made	O
by	O
a	O
new	O
visitor	O
uniformly	O
sampled	O
from	O
the	O
population	O
of	O
the	O
website	O
s	O
visitors	O
by	O
not	O
using	O
the	O
fact	O
that	O
many	O
users	O
repeatedly	O
visit	O
the	O
same	O
websites	O
greedy	O
policies	O
do	O
not	O
take	O
advantage	O
of	O
possibilities	O
provided	O
by	O
long-term	O
interactions	O
with	O
individual	O
users	O
as	O
an	O
example	O
of	O
how	O
a	O
marketing	O
strategy	O
might	O
take	O
advantage	O
of	O
long-term	O
user	O
interaction	O
theocharous	O
et	O
al	O
contrasted	O
a	O
greedy	O
policy	B
with	O
a	O
longer-term	O
policy	B
for	O
displaying	O
ads	O
for	O
buying	O
a	O
product	O
say	O
a	O
car	O
the	O
ad	O
displayed	O
by	O
the	O
greedy	O
policy	B
might	O
offer	O
a	O
discount	O
if	O
the	O
user	O
buys	O
the	O
car	O
immediately	O
a	O
user	O
either	O
takes	O
the	O
offer	O
or	O
leaves	O
the	O
website	O
and	O
if	O
they	O
ever	O
return	B
to	O
the	O
site	O
they	O
would	O
likely	O
see	O
the	O
same	O
offer	O
a	O
longer-term	O
policy	B
on	O
the	O
other	O
hand	O
can	O
transition	O
the	O
user	O
down	O
a	O
sales	O
funnel	O
before	O
presenting	O
the	O
final	O
deal	O
it	O
might	O
start	O
by	O
describing	O
the	O
availability	O
of	O
favorable	O
financing	O
terms	O
then	O
praise	O
an	O
excellent	O
service	O
department	O
and	O
then	O
on	O
the	O
next	O
visit	O
offer	O
the	O
final	O
discount	O
this	O
type	O
of	O
policy	B
can	O
result	O
in	O
more	O
clicks	O
by	O
a	O
user	O
over	O
repeated	O
visits	O
to	O
the	O
site	O
and	O
if	O
the	O
policy	B
is	O
suitably	O
designed	O
more	O
eventual	O
sales	O
working	O
at	O
adobe	O
systems	O
incorporated	O
theocharous	O
et	O
al	O
conducted	O
experiments	O
to	O
see	O
if	O
policies	O
designed	O
to	O
maximize	O
clicks	O
over	O
the	O
long	O
term	O
could	O
in	O
fact	O
improve	O
over	O
short-term	O
greedy	O
policies	O
the	O
adobe	O
marketing	O
cloud	O
a	O
set	O
of	O
tools	O
that	O
many	O
companies	O
use	O
to	O
run	O
digital	O
marketing	O
campaigns	O
provides	O
infrastructure	O
for	O
automating	O
user-targed	O
advertising	O
and	O
fund-raising	O
campaigns	O
actually	O
deploying	O
novel	O
policies	O
using	O
these	O
tools	O
entails	O
significant	O
risk	O
because	O
a	O
new	O
policy	B
may	O
end	O
up	O
performing	O
poorly	O
for	O
this	O
reason	O
the	O
research	O
team	O
needed	O
to	O
assess	O
what	O
a	O
policy	B
s	O
performance	O
would	O
be	O
if	O
it	O
were	O
to	O
be	O
actually	O
deployed	O
but	O
to	O
do	O
so	O
on	O
the	O
basis	O
of	O
data	O
collected	O
under	O
the	O
execution	O
of	O
other	O
policies	O
a	O
critical	O
aspect	O
of	O
this	O
research	O
then	O
was	O
offpolicy	O
evaluation	O
further	O
the	O
team	O
wanted	O
to	O
do	O
this	O
with	O
high	O
confidence	O
to	O
reduce	O
the	O
risk	O
of	O
deploying	O
a	O
new	O
policy	B
although	O
high	O
confidence	O
off-policy	B
evaluation	O
was	O
a	O
central	O
component	O
of	O
this	O
research	O
also	O
thomas	O
thomas	O
theocharous	O
and	O
ghavamzadeh	O
here	O
we	O
focus	O
only	O
on	O
the	O
algorithms	O
and	O
their	O
results	O
theocharous	O
et	O
al	O
compared	O
the	O
results	O
of	O
two	O
algorithms	O
for	O
learning	O
ad	O
recommendation	O
policies	O
the	O
first	O
algorithm	O
which	O
they	O
called	O
greedy	O
optimization	O
had	O
the	O
goal	O
chapter	O
applications	B
and	I
case	I
studies	I
of	O
maximizing	O
only	O
the	O
probability	O
of	O
immediate	O
clicks	O
as	O
in	O
the	O
standard	O
contextual	O
bandit	O
formulation	O
this	O
algorithm	O
did	O
not	O
take	O
the	O
long-term	O
effects	O
of	O
recommendations	O
into	O
account	O
the	O
other	O
algorithm	O
a	O
reinforcement	B
learning	I
algorithm	O
based	O
on	O
an	O
mdp	O
formulation	O
aimed	O
at	O
improving	O
the	O
number	O
of	O
clicks	O
users	O
made	O
over	O
multiple	O
visits	O
to	O
a	O
website	O
they	O
called	O
this	O
latter	O
algorithm	O
life-time	O
value	B
optimization	O
both	O
algorithms	O
faced	O
challenging	O
problems	O
because	O
the	O
reward	B
signal	I
in	O
this	O
domain	O
is	O
very	O
sparse	B
because	O
users	O
usually	O
do	O
not	O
click	O
on	O
ads	O
and	O
user	O
clicking	O
is	O
very	O
random	O
so	O
that	O
returns	O
have	O
high	O
variance	O
data	O
sets	O
from	O
the	O
banking	O
industry	O
were	O
used	O
for	O
training	O
and	O
testing	O
these	O
algorithms	O
the	O
data	O
sets	O
consisted	O
of	O
many	O
complete	O
trajectories	O
of	O
customer	O
interaction	O
with	O
a	O
bank	O
s	O
website	O
that	O
showed	O
each	O
customer	O
one	O
out	O
of	O
a	O
collection	O
of	O
possible	O
offers	O
if	O
a	O
customer	O
clicked	O
the	O
reward	O
was	O
and	O
otherwise	O
it	O
was	O
one	O
data	O
set	O
contained	O
approximately	O
interactions	O
from	O
a	O
month	O
of	O
a	O
bank	O
s	O
campaign	O
that	O
randomly	O
offered	O
one	O
of	O
offers	O
the	O
other	O
data	O
set	O
from	O
another	O
bank	O
s	O
campaign	O
contained	O
interactions	O
involving	O
possible	O
offers	O
all	O
interactions	O
included	O
customer	O
features	O
such	O
as	O
the	O
time	O
since	O
the	O
customer	O
s	O
last	O
visit	O
to	O
the	O
website	O
the	O
number	O
of	O
their	O
visits	O
so	O
far	O
the	O
last	O
time	O
the	O
customer	O
clicked	O
geographic	O
location	O
one	O
of	O
a	O
collection	O
of	O
interests	O
and	O
features	O
giving	O
demographic	O
information	O
greedy	O
optimization	O
was	O
based	O
on	O
a	O
mapping	O
estimating	O
the	O
probability	O
of	O
a	O
click	O
as	O
a	O
function	O
of	O
user	O
features	O
the	O
mapping	O
was	O
learned	O
via	O
supervised	B
learning	I
from	O
one	O
of	O
the	O
data	O
sets	O
by	O
means	O
of	O
a	O
random	O
forest	O
algorithm	O
rf	O
algorithms	O
have	O
been	O
widely	O
used	O
for	O
large-scale	O
applications	O
in	O
industry	O
because	O
they	O
are	O
effective	O
predictive	O
tools	O
that	O
tend	O
not	O
to	O
overfit	O
and	O
are	O
relatively	O
insensitive	O
to	O
outliers	O
and	O
noise	O
theocharous	O
et	O
al	O
then	O
used	O
the	O
mapping	O
to	O
define	O
an	O
policy	B
that	O
selected	O
with	O
probability	O
the	O
offer	O
predicted	O
by	O
the	O
rf	O
algorithm	O
to	O
have	O
the	O
highest	O
probability	O
of	O
producing	O
a	O
click	O
and	O
otherwise	O
selected	O
from	O
the	O
other	O
offers	O
uniformly	O
at	O
random	O
ltv	O
optimization	O
used	O
a	O
batch-mode	O
reinforcement	B
learning	I
algorithm	O
called	O
fitted	O
q	O
iteration	O
it	O
is	O
a	O
variant	O
of	O
fitted	O
value	B
iteration	I
adapted	O
to	O
q-learning	B
batch	O
mode	O
means	O
that	O
the	O
entire	O
data	O
set	O
for	O
learning	O
is	O
available	O
from	O
the	O
start	O
as	O
opposed	O
to	O
the	O
online	B
mode	O
of	O
the	O
algorithms	O
we	O
focus	O
on	O
in	O
this	O
book	O
in	O
which	O
data	O
are	O
acquired	O
sequentially	O
while	O
the	O
learning	O
algorithm	O
executes	O
batchmode	O
reinforcement	B
learning	I
algorithms	O
are	O
sometimes	O
necessary	O
when	O
online	B
learning	O
is	O
not	O
practical	O
and	O
they	O
can	O
use	O
any	O
batch-mode	O
supervised	B
learning	I
regression	O
algorithm	O
including	O
algorithms	O
known	O
to	O
scale	O
well	O
to	O
high-dimensional	O
spaces	O
the	O
convergence	O
of	O
fqi	O
depends	O
on	O
properties	O
of	O
the	O
function	B
approximation	I
algorithm	O
for	O
their	O
application	O
to	O
ltv	O
optimization	O
theocharous	O
et	O
al	O
used	O
the	O
same	O
rf	O
algorithm	O
they	O
used	O
for	O
the	O
greedy	O
optimization	O
approach	O
because	O
in	O
this	O
case	O
fqi	O
convergence	O
is	O
not	O
monotonic	O
theocharous	O
et	O
al	O
kept	O
track	O
of	O
the	O
best	O
fqi	O
policy	B
by	O
off-policy	B
evaluation	O
using	O
a	O
validation	O
training	O
set	O
the	O
final	O
policy	B
for	O
testing	O
the	O
ltv	O
approach	O
was	O
the	O
policy	B
based	O
on	O
the	O
best	O
policy	B
produced	O
by	O
fqi	O
with	O
the	O
initial	O
actionvalue	O
function	O
set	O
to	O
the	O
mapping	O
produced	O
by	O
the	O
rf	O
for	O
the	O
greedy	O
optimization	O
approach	O
to	O
measure	O
the	O
performance	O
of	O
the	O
policies	O
produced	O
by	O
the	O
greedy	O
and	O
ltv	O
ap	O
personalized	O
web	O
services	O
proaches	O
theocharous	O
et	O
al	O
used	O
the	O
ctr	O
metric	O
and	O
a	O
metric	O
they	O
called	O
the	O
ltv	O
metric	O
these	O
metrics	O
are	O
similar	O
except	O
that	O
the	O
ltv	O
metric	O
critically	O
distinguishes	O
between	O
individual	O
website	O
visitors	O
ctr	O
total	O
of	O
clicks	O
total	O
of	O
visits	O
ltv	O
total	O
of	O
clicks	O
total	O
of	O
visitors	O
figure	O
illustrates	O
how	O
these	O
metrics	O
differ	O
each	O
circle	O
represents	O
a	O
user	O
visit	O
to	O
the	O
site	O
black	O
circles	O
are	O
visits	O
at	O
which	O
the	O
user	O
clicks	O
each	O
row	O
represents	O
visits	O
by	O
a	O
particular	O
user	O
by	O
not	O
distinguishing	O
between	O
visitors	O
the	O
ctr	O
for	O
these	O
sequences	O
is	O
whereas	O
the	O
ltv	O
is	O
because	O
ltv	O
is	O
larger	O
than	O
ctr	O
to	O
the	O
extent	O
that	O
individual	O
users	O
revisit	O
the	O
site	O
it	O
is	O
an	O
indicator	O
of	O
how	O
successful	O
a	O
policy	B
is	O
in	O
encouraging	O
users	O
to	O
engage	O
in	O
extended	O
interactions	O
with	O
the	O
site	O
figure	O
click	O
through	O
rate	O
versus	O
life-time	O
value	B
each	O
circle	O
represents	O
a	O
user	O
visit	O
black	O
circles	O
are	O
visits	O
at	O
which	O
the	O
user	O
clicks	O
adapted	O
from	O
theocharous	O
et	O
al	O
testing	O
the	O
policies	O
produced	O
by	O
the	O
greedy	O
and	O
ltv	O
approaches	O
was	O
done	O
using	O
a	O
high	O
confidence	O
off-policy	B
evaluation	O
method	O
on	O
a	O
test	O
data	O
set	O
consisting	O
of	O
real-world	O
interactions	O
with	O
a	O
bank	O
website	O
served	O
by	O
a	O
random	O
policy	B
as	O
expected	B
results	O
showed	O
that	O
greedy	O
optimization	O
performed	O
best	O
as	O
measured	O
by	O
the	O
ctr	O
metric	O
while	O
ltv	O
optimization	O
performed	O
best	O
as	O
measured	O
by	O
the	O
ltv	O
metric	O
furthermore	O
although	O
we	O
have	O
omitted	O
its	O
details	O
the	O
high	O
confidence	O
off-policy	B
evaluation	O
method	O
provided	O
probabilistic	O
guarantees	O
that	O
the	O
ltv	O
optimization	O
method	O
would	O
with	O
high	O
probability	O
produce	O
policies	O
that	O
improve	O
upon	O
policies	O
currently	O
deployed	O
assured	O
by	O
these	O
probabilistic	O
guarantees	O
adobe	O
announced	O
in	O
that	O
the	O
new	O
ltv	O
algorithm	O
would	O
be	O
a	O
standard	O
component	O
of	O
the	O
adobe	O
marketing	O
cloud	O
so	O
that	O
a	O
retailer	O
could	O
issue	O
a	O
sequence	O
of	O
offers	O
following	O
a	O
policy	B
likely	O
to	O
yield	O
higher	O
return	B
than	O
a	O
policy	B
that	O
is	O
insensitive	O
to	O
long-term	O
results	O
chapter	O
applications	B
and	I
case	I
studies	I
thermal	O
soaring	O
birds	O
and	O
gliders	O
take	O
advantage	O
of	O
upward	O
air	O
currents	O
thermals	O
to	O
gain	O
altitude	O
in	O
order	O
to	O
maintain	O
flight	O
while	O
expending	O
little	O
or	O
no	O
energy	O
thermal	O
soaring	O
as	O
this	O
behavior	O
is	O
called	O
is	O
a	O
complex	O
skill	O
requiring	O
responding	O
to	O
subtle	O
environmental	O
cues	O
to	O
increase	O
altitude	O
by	O
exploiting	O
a	O
rising	O
column	O
of	O
air	O
for	O
as	O
long	O
as	O
possible	O
reddy	O
celani	O
sejnowski	O
and	O
vergassola	O
used	O
reinforcement	B
learning	I
to	O
investigate	O
thermal	O
soaring	O
policies	O
that	O
are	O
effective	O
in	O
the	O
strong	O
atmospheric	O
turbulence	O
usually	O
accompanying	O
rising	O
air	O
currents	O
their	O
primary	O
goal	O
was	O
to	O
provide	O
insight	O
into	O
the	O
cues	O
birds	O
sense	O
and	O
how	O
they	O
use	O
them	O
to	O
achieve	O
their	O
impressive	O
thermal	O
soaring	O
performance	O
but	O
the	O
results	O
also	O
contribute	O
to	O
technology	O
relevant	O
to	O
autonomous	O
gliders	O
reinforcement	B
learning	I
had	O
previously	O
been	O
applied	O
to	O
the	O
problem	O
of	O
navigating	O
efficiently	O
to	O
the	O
vicinity	O
of	O
a	O
thermal	O
updraft	O
dunn	O
and	O
valasek	O
but	O
not	O
to	O
the	O
more	O
challenging	O
problem	O
of	O
soaring	O
within	O
the	O
turbulence	O
of	O
the	O
updraft	O
itself	O
reddy	O
et	O
al	O
modeled	O
the	O
soaring	O
problem	O
as	O
a	O
continuing	O
mdp	O
with	O
discounting	B
the	O
agent	O
interacted	O
with	O
a	O
detailed	O
model	O
of	O
a	O
glider	O
flying	O
in	O
turbulent	O
air	O
they	O
devoted	O
significant	O
effort	O
toward	O
making	O
the	O
model	O
generate	O
realistic	O
thermal	O
soaring	O
conditions	O
including	O
investigating	O
several	O
different	O
approaches	O
to	O
atmospheric	O
modeling	O
for	O
the	O
learning	O
experiments	O
air	O
flow	O
in	O
a	O
three-dimensional	O
box	O
with	O
one	O
kilometer	O
sides	O
one	O
of	O
which	O
was	O
at	O
ground	O
level	O
was	O
modeled	O
by	O
a	O
sophisticated	O
physics-based	O
set	O
of	O
partial	O
differential	B
equations	O
involving	O
air	O
velocity	O
temperature	O
and	O
pressure	O
introducing	O
small	O
random	O
perturbations	O
into	O
the	O
numerical	O
simulation	O
caused	O
the	O
model	O
to	O
produce	O
analogs	O
of	O
thermal	O
updrafts	O
and	O
accompanying	O
turbulence	O
left	O
glider	O
flight	O
was	O
modeled	O
by	O
aerodynamic	O
equations	O
involving	O
velocity	O
lift	O
drag	O
and	O
figure	O
thermal	O
soaring	O
model	O
left	O
snapshot	O
of	O
the	O
vertical	O
velocity	O
field	O
of	O
the	O
simulated	O
cube	O
of	O
air	O
in	O
red	O
is	O
a	O
region	O
of	O
large	O
upward	O
flow	O
right	O
diagram	O
of	O
powerless	O
flight	O
showing	O
bank	O
angle	O
and	O
angle	O
of	O
attack	O
adapted	O
with	O
permission	O
from	O
pnas	O
vol	O
p	O
reddy	O
celani	O
sejnowski	O
and	O
vergassola	O
learning	O
to	O
soar	O
in	O
turbulent	O
environments	O
contributesignificantlyandmoreexploratorystrategiesarepreferred	O
thesarsaalgorithmfindstheoptimalpolicybyestimatingforeverystate	O
actionpairitsqfunctiondefinedastheexpectedsumoffuturerewardsgiventhecurrentstatesandtheactiona	O
ateachsteptheqfunctionisupdatedasfollowsq	O
sa	O
q	O
sa	O
r	O
q	O
s	O
q	O
sa	O
isthelearningrate	O
theupdateismadeonlineanddoesnotrequireanypriormodeloftheflowortheflight	O
thisfeatureisparticularlyrelevantinmodelingdecision-makingprocessesinanimals	O
whenthealgo-rithmisclosetoconvergencetheqfunctionapproachesthesolutiontobellman	O
aswhichencodestheprobabilityofchoosingactionaatstatesapproachestheoptimalone	O
pandisobtainedfromtheqfunctionviaaboltzmann-likeexpression	O
as	O
exp	O
sa	O
sa	O
q	O
sa	O
q	O
sa	O
maxa	O
q	O
sa	O
mina	O
q	O
sa	O
tempisaneffective	O
temperature	O
temp	O
approximately	O
lzxlift	O
ldrag	O
dvelocity	O
directionwing	O
directionbank	O
angleglide	O
angleangle	O
of	O
b	O
nardconvection	O
fortheverticalvelocityfieldtheredandbluecolorsindicateregionsoflargeupwardanddownwardflowrespectively	O
forthetemperaturefieldtheredandbluecolorsindicateregionsofhighandlowtemperaturerespectively	O
noticethatthehotandcoldregionsdrivetheupwardanddownwardbranchesoftheconvectivecellinagreementwiththebasicphysicsofconvection	O
ctheforce-bodydiagramofflightwithnothrustthatiswithoutanyengineorflappingofwings	O
thefigurealsoshowsthebankangle	O
actionpairitsqfunctiondefinedastheexpectedsumoffuturerewardsgiventhecurrentstatesandtheactiona	O
ateachsteptheqfunctionisupdatedasfollowsq	O
sa	O
q	O
sa	O
r	O
q	O
s	O
q	O
sa	O
isthelearningrate	O
theupdateismadeonlineanddoesnotrequireanypriormodeloftheflowortheflight	O
thisfeatureisparticularlyrelevantinmodelingdecision-makingprocessesinanimals	O
whenthealgo-rithmisclosetoconvergencetheqfunctionapproachesthesolutiontobellman	O
aswhichencodestheprobabilityofchoosingactionaatstatesapproachestheoptimalone	O
pandisobtainedfromtheqfunctionviaaboltzmann-likeexpression	O
as	O
exp	O
sa	O
sa	O
q	O
sa	O
q	O
sa	O
maxa	O
q	O
sa	O
mina	O
q	O
sa	O
tempisaneffective	O
temperature	O
temp	O
approximately	O
lzxlift	O
ldrag	O
dvelocity	O
directionwing	O
directionbank	O
angleglide	O
angleangle	O
of	O
b	O
nardconvection	O
fortheverticalvelocityfieldtheredandbluecolorsindicateregionsoflargeupwardanddownwardflowrespectively	O
forthetemperaturefieldtheredandbluecolorsindicateregionsofhighandlowtemperaturerespectively	O
noticethatthehotandcoldregionsdrivetheupwardanddownwardbranchesoftheconvectivecellinagreementwiththebasicphysicsofconvection	O
ctheforce-bodydiagramofflightwithnothrustthatiswithoutanyengineorflappingofwings	O
thefigurealsoshowsthebankangle	O
thermal	O
soaring	O
other	O
factors	O
governing	O
powerless	O
flight	O
of	O
a	O
fixed-wing	O
aircraft	O
maneuvering	O
the	O
glider	O
involved	O
changing	O
its	O
angle	O
of	O
attack	O
angle	O
between	O
the	O
glider	O
s	O
wing	O
and	O
the	O
direction	O
of	O
air	O
flow	O
and	O
its	O
bank	O
angle	O
right	O
the	O
interface	O
between	O
the	O
agent	O
and	O
the	O
environment	B
required	O
defining	O
the	O
agent	O
s	O
actions	O
the	O
state	B
information	O
the	O
agent	O
receives	O
from	O
the	O
environment	B
and	O
the	O
reward	B
signal	I
by	O
experimenting	O
with	O
various	O
possibilities	O
reddy	O
et	O
al	O
decided	O
that	O
three	O
actions	O
each	O
for	O
the	O
angle	O
of	O
attack	O
and	O
the	O
bank	O
angle	O
were	O
enough	O
for	O
their	O
purposes	O
increment	O
or	O
decrement	O
the	O
current	O
bank	O
angle	O
and	O
angle	O
of	O
attack	O
by	O
and	O
respectively	O
or	O
leave	O
them	O
unchanged	O
this	O
resulted	O
in	O
possible	O
actions	O
the	O
bank	O
angle	O
was	O
bounded	O
to	O
remain	O
between	O
and	O
because	O
a	O
goal	O
of	O
their	O
study	O
was	O
to	O
try	O
to	O
determine	O
what	O
minimal	O
set	O
of	O
sensory	O
cues	O
are	O
necessary	O
for	O
effective	O
soaring	O
both	O
to	O
shed	O
light	O
on	O
the	O
cues	O
birds	O
might	O
use	O
for	O
soaring	O
and	O
to	O
minimize	O
the	O
sensing	O
complexity	O
required	O
for	O
automated	O
glider	O
soaring	O
the	O
authors	O
tried	O
various	O
sets	O
of	O
signals	O
as	O
input	O
to	O
the	O
reinforcement	B
learning	I
agent	O
they	O
started	O
by	O
using	O
state	B
aggregation	I
of	O
a	O
four-dimensional	O
state	B
space	O
with	O
dimensions	O
giving	O
local	O
vertical	O
wind	O
speed	O
local	O
vertical	O
wind	O
acceleration	O
torque	O
depending	O
on	O
the	O
difference	O
between	O
the	O
vertical	O
wind	O
velocities	O
at	O
the	O
left	O
and	O
right	O
wing	O
tips	O
and	O
the	O
local	O
temperature	O
each	O
dimension	O
was	O
discretized	O
into	O
three	O
bins	O
positive	O
high	O
negative	O
high	O
and	O
small	O
results	O
described	O
below	O
showed	O
that	O
only	O
two	O
of	O
these	O
dimensions	O
were	O
critical	O
for	O
effective	O
soaring	O
behavior	O
the	O
overall	O
objective	O
of	O
thermal	O
soaring	O
is	O
to	O
gain	O
as	O
much	O
altitude	O
as	O
possible	O
from	O
each	O
rising	O
column	O
of	O
air	O
reddy	O
et	O
al	O
tried	O
a	O
straightforward	O
reward	B
signal	I
that	O
rewarded	O
the	O
agent	O
at	O
the	O
end	O
of	O
each	O
episode	O
based	O
on	O
the	O
altitude	O
gained	O
over	O
the	O
episode	O
a	O
large	O
negative	O
reward	B
signal	I
if	O
the	O
glider	O
touched	O
the	O
ground	O
and	O
zero	O
otherwise	O
they	O
found	O
that	O
learning	O
was	O
not	O
successful	O
with	O
this	O
reward	B
signal	I
for	O
episodes	B
of	O
realistic	O
duration	O
and	O
that	O
eligibility	B
traces	I
did	O
not	O
help	O
by	O
experimenting	O
with	O
various	O
reward	O
signals	O
they	O
found	O
that	O
learning	O
was	O
best	O
with	O
a	O
reward	B
signal	I
that	O
at	O
each	O
time	O
step	O
linearly	O
combined	O
the	O
vertical	O
wind	O
velocity	O
and	O
vertical	O
wind	O
acceleration	O
observed	O
on	O
the	O
previous	O
time	O
step	O
learning	O
was	O
by	O
one-step	O
sarsa	B
with	O
actions	O
selected	O
according	O
to	O
a	O
soft-max	B
distribution	O
based	O
on	O
normalized	O
action	B
values	O
specifically	O
the	O
action	B
probabilities	O
were	O
computed	O
according	O
to	O
with	O
action	B
preferences	I
hs	O
a	O
qs	O
a	O
minb	O
qs	O
b	O
qs	O
b	O
minb	O
qs	O
b	O
where	O
is	O
a	O
parameter	O
vector	B
with	O
one	O
component	O
for	O
each	O
action	B
and	O
aggregated	O
group	O
of	O
states	O
and	O
qs	O
a	O
merely	O
returned	O
the	O
component	O
corresponding	O
to	O
s	O
a	O
in	O
the	O
usual	O
way	O
for	O
state	B
aggregation	I
methods	O
the	O
above	O
equation	O
forms	O
the	O
action	B
preferences	I
by	O
normalizing	O
the	O
approximate	O
action	B
values	O
to	O
the	O
interval	O
then	O
dividing	O
by	O
a	O
positive	O
temperature	O
parameter	O
as	O
increases	O
the	O
probability	O
of	O
selecting	O
an	O
action	B
becomes	O
less	O
dependent	O
on	O
its	O
preference	O
as	O
decreases	O
toward	O
zero	O
the	O
probability	O
of	O
selecting	O
the	O
most	O
highly-preferred	O
action	B
approaches	O
one	O
making	O
the	O
policy	B
approach	O
the	O
greedy	O
policy	B
the	O
temperature	O
parameter	O
was	O
initialized	O
to	O
and	O
incrementally	O
et	O
al	O
described	O
this	O
slightly	O
differently	O
but	O
our	O
version	O
is	O
equivalent	O
to	O
theirs	O
chapter	O
applications	B
and	I
case	I
studies	I
decreased	O
to	O
during	O
learning	O
action	B
preferences	I
were	O
computed	O
from	O
the	O
current	O
estimates	O
of	O
the	O
action	B
values	O
the	O
action	B
with	O
the	O
maximum	O
estimated	O
action	B
value	B
was	O
given	O
preference	O
the	O
action	B
with	O
the	O
minimum	O
estimated	O
action	B
value	B
was	O
given	O
preference	O
and	O
the	O
preferences	O
of	O
the	O
other	O
actions	O
were	O
scaled	O
between	O
these	O
extremes	O
the	O
step-size	O
and	O
discount-rate	O
parameters	O
were	O
fixed	O
at	O
and	O
respectively	O
each	O
learning	O
episode	O
took	O
place	O
with	O
the	O
agent	O
controlling	O
simulated	O
flight	O
in	O
an	O
independently	O
generated	O
period	O
of	O
simulated	O
turbulent	O
air	O
currents	O
each	O
episode	O
lasted	O
minutes	O
simulated	O
with	O
a	O
second	O
time	O
step	O
learning	O
effectively	O
converged	O
after	O
a	O
few	O
hundred	O
episodes	B
the	O
left	O
panel	O
of	O
figure	O
shows	O
a	O
sample	O
trajectory	O
before	O
learning	O
where	O
the	O
agent	O
selects	O
actions	O
randomly	O
starting	O
at	O
the	O
top	O
of	O
the	O
volume	O
shown	O
the	O
glider	O
s	O
trajectory	O
is	O
in	O
the	O
direction	O
indicated	O
by	O
the	O
arrow	O
and	O
quickly	O
loses	O
altitude	O
figure	O
s	O
right	O
panel	O
is	O
a	O
trajectory	O
after	O
learning	O
the	O
glider	O
starts	O
at	O
the	O
same	O
place	O
appearing	O
at	O
the	O
bottom	O
of	O
the	O
volume	O
and	O
gains	O
altitude	O
by	O
spiraling	O
within	O
the	O
rising	O
column	O
of	O
air	O
although	O
reddy	O
at	O
al	O
found	O
that	O
performance	O
varied	O
widely	O
over	O
different	O
simulated	O
periods	O
of	O
air	O
flow	O
the	O
number	O
of	O
times	O
the	O
glider	O
touched	O
the	O
ground	O
consistently	O
decreased	O
to	O
nearly	O
zero	O
as	O
learning	O
progressed	O
after	O
experimenting	O
with	O
different	O
sets	O
of	O
features	O
available	O
to	O
the	O
learning	O
agent	O
it	O
turned	O
out	O
that	O
the	O
combination	O
of	O
just	O
vertical	O
wind	O
acceleration	O
and	O
torques	O
worked	O
best	O
the	O
authors	O
conjectured	O
that	O
because	O
these	O
features	O
give	O
information	O
about	O
the	O
gradient	B
of	O
vertical	O
wind	O
velocity	O
in	O
two	O
different	O
directions	O
they	O
allow	O
the	O
controller	O
to	O
select	O
between	O
turning	O
by	O
changing	O
the	O
bank	O
angle	O
or	O
continuing	O
along	O
the	O
same	O
course	O
by	O
leaving	O
the	O
bank	O
angle	O
alone	O
this	O
allows	O
the	O
glider	O
to	O
stay	O
within	O
a	O
rising	O
column	O
of	O
figure	O
sample	O
thermal	O
soaring	O
trajectories	O
with	O
arrows	O
showing	O
the	O
direction	O
of	O
flight	O
from	O
the	O
same	O
starting	O
point	O
that	O
the	O
altitude	O
scales	O
are	O
shifted	O
left	O
before	O
learning	O
the	O
agent	O
selects	O
actions	O
randomly	O
and	O
the	O
glider	O
descends	O
right	O
after	O
learning	O
the	O
glider	O
gains	O
altitude	O
by	O
following	O
a	O
spiral	O
trajectory	O
adapted	O
with	O
permission	O
from	O
pnas	O
vol	O
p	O
reddy	O
celani	O
sejnowski	O
and	O
vergassola	O
learning	O
to	O
soar	O
in	O
turbulent	O
environments	O
thermal	O
soaring	O
air	O
vertical	O
wind	O
velocity	O
is	O
indicative	O
of	O
the	O
strength	O
of	O
the	O
thermal	O
but	O
does	O
not	O
help	O
in	O
staying	O
within	O
the	O
flow	O
they	O
found	O
that	O
sensitivity	O
to	O
temperature	O
was	O
of	O
little	O
help	O
they	O
also	O
found	O
that	O
controlling	O
the	O
angle	O
of	O
attack	O
is	O
not	O
helpful	O
in	O
staying	O
within	O
a	O
particular	O
thermal	O
being	O
useful	O
instead	O
for	O
traveling	O
between	O
thermals	O
when	O
covering	O
large	O
distances	O
as	O
in	O
cross-country	O
gliding	O
and	O
bird	O
migration	O
due	O
to	O
the	O
fact	O
that	O
soaring	O
in	O
different	O
levels	O
of	O
turbulence	O
requires	O
different	O
policies	O
training	O
was	O
done	O
in	O
conditions	O
ranging	O
from	O
weak	O
to	O
strong	O
turbulence	O
in	O
strong	O
turbulence	O
the	O
rapidly	O
changing	O
wind	O
and	O
glider	O
velocities	O
allowed	O
less	O
time	O
for	O
the	O
controller	O
to	O
react	O
this	O
reduced	O
the	O
amount	O
of	O
control	B
possible	O
compared	O
to	O
what	O
was	O
possible	O
for	O
maneuvering	O
when	O
fluctuations	O
were	O
weak	O
reddy	O
at	O
al	O
examined	O
the	O
policies	O
sarsa	B
learned	O
under	O
these	O
different	O
conditions	O
common	O
to	O
policies	O
learned	O
in	O
all	O
regimes	O
were	O
these	O
features	O
when	O
sensing	O
negative	O
wind	O
acceleration	O
bank	O
sharply	O
in	O
the	O
direction	O
of	O
the	O
wing	O
with	O
the	O
higher	O
lift	O
when	O
sensing	O
large	O
positive	O
wind	O
acceleration	O
and	O
no	O
torque	O
do	O
nothing	O
however	O
different	O
levels	O
of	O
turbulence	O
led	O
to	O
policy	B
differences	O
policies	O
learned	O
in	O
strong	O
turbulence	O
were	O
more	O
conservative	O
in	O
that	O
they	O
preferred	O
small	O
bank	O
angles	O
whereas	O
in	O
weak	O
turbulence	O
the	O
best	O
action	B
was	O
to	O
turn	O
as	O
much	O
as	O
possible	O
by	O
banking	O
sharply	O
systematic	O
study	O
of	O
the	O
bank	O
angles	O
preferred	O
by	O
the	O
policies	O
learned	O
under	O
the	O
different	O
conditions	O
led	O
the	O
authors	O
to	O
suggest	O
that	O
by	O
detecting	O
when	O
vertical	O
wind	O
acceleration	O
crosses	O
a	O
certain	O
threshold	O
the	O
controller	O
can	O
adjust	O
its	O
policy	B
to	O
cope	O
with	O
different	O
turbulence	O
regimes	O
reddy	O
et	O
al	O
also	O
conducted	O
experiments	O
to	O
investigate	O
the	O
effect	O
of	O
the	O
discount-rate	O
parameter	O
on	O
the	O
performance	O
of	O
the	O
learned	O
policies	O
they	O
found	O
that	O
the	O
altitude	O
gained	O
in	O
an	O
episode	O
increased	O
as	O
increased	O
reaching	O
a	O
maximum	O
for	O
suggesting	O
that	O
effective	O
thermal	O
soaring	O
requires	O
taking	O
into	O
account	O
long-term	O
effects	O
of	O
control	B
decisions	O
this	O
computational	O
study	O
of	O
thermal	O
soaring	O
illustrates	O
how	O
reinforcement	B
learning	I
can	O
further	O
progress	O
toward	O
different	O
kinds	O
of	O
objectives	O
learning	O
policies	O
having	O
access	O
to	O
different	O
sets	O
of	O
environmental	O
cues	O
and	B
control	B
actions	O
contributes	O
to	O
both	O
the	O
engineering	O
objective	O
of	O
designing	O
autonomous	O
gliders	O
and	O
the	O
scientific	O
objective	O
of	O
improving	O
understanding	O
of	O
the	O
soaring	O
skills	O
of	O
birds	O
in	O
both	O
cases	O
hypotheses	O
resulting	O
from	O
the	O
learning	O
experiments	O
can	O
be	O
tested	O
in	O
the	O
field	O
by	O
instrumenting	O
real	O
gliders	O
and	O
by	O
comparing	O
predictions	O
with	O
observed	O
bird	O
soaring	O
behavior	O
chapter	O
frontiers	O
in	O
this	O
final	O
chapter	O
we	O
touch	O
on	O
some	O
topics	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
but	O
that	O
we	O
see	O
as	O
particularly	O
important	O
for	O
the	O
future	O
of	O
reinforcement	B
learning	I
many	O
of	O
these	O
topics	O
bring	O
us	O
beyond	O
what	O
is	O
reliably	O
known	O
and	O
some	O
bring	O
us	O
beyond	O
the	O
mdp	O
framework	O
general	O
value	B
functions	O
and	O
auxiliary	B
tasks	I
over	O
the	O
course	O
of	O
this	O
book	O
our	O
notion	O
of	O
value	B
function	I
has	O
become	O
quite	O
general	O
with	O
off-policy	B
learning	O
we	O
allowed	O
a	O
value	B
function	I
to	O
be	O
conditional	O
on	O
an	O
arbitrary	O
target	O
policy	B
then	O
in	O
section	O
we	O
generalized	O
discounting	B
to	O
a	O
termination	O
function	O
s	O
so	O
that	O
a	O
different	O
discount	O
rate	O
could	O
be	O
applied	O
at	O
each	O
time	O
step	O
in	O
determining	O
the	O
return	B
this	O
allowed	O
us	O
to	O
express	O
predictions	O
about	O
how	O
much	O
reward	O
we	O
will	O
get	O
over	O
an	O
arbitrary	O
state-dependent	O
horizon	O
the	O
next	O
and	O
perhaps	O
final	O
step	O
is	O
to	O
generalize	O
beyond	O
rewards	O
to	O
permit	O
predictions	O
about	O
arbitrary	O
signals	O
rather	O
than	O
predicting	O
the	O
sum	O
of	O
future	O
rewards	O
we	O
might	O
predict	O
the	O
sum	O
of	O
the	O
future	O
values	O
of	O
a	O
sound	O
or	O
color	O
sensation	O
or	O
of	O
an	O
internal	O
highly	O
processed	O
signal	O
such	O
as	O
another	O
prediction	B
whatever	O
signal	O
is	O
added	O
up	O
in	O
this	O
way	O
in	O
a	O
value-function-like	O
prediction	B
we	O
call	O
it	O
the	O
cumulant	O
of	O
that	O
prediction	B
we	O
formalize	O
it	O
in	O
a	O
cumulant	O
signal	O
ct	O
r	O
using	O
this	O
a	O
general	O
value	B
function	I
or	O
gvf	O
is	O
written	O
v	O
st	O
s	O
at	O
as	O
with	O
conventional	O
value	B
functions	O
as	O
v	O
or	O
q	O
this	O
is	O
an	O
ideal	O
function	O
that	O
we	O
seek	O
to	O
approximate	O
with	O
a	O
parameterized	O
form	O
which	O
we	O
might	O
continue	O
to	O
denote	O
vsw	O
although	O
of	O
course	O
there	O
would	O
have	O
to	O
be	O
a	O
different	O
w	O
for	O
each	O
prediction	B
that	O
is	O
for	O
each	O
choice	O
of	O
and	O
ct	O
because	O
a	O
gvf	O
has	O
no	O
necessary	O
connection	O
to	O
reward	O
it	O
is	O
perhaps	O
a	O
misnomer	O
to	O
call	O
it	O
a	O
value	B
function	I
one	O
could	O
simply	O
call	O
it	O
a	O
prediction	B
or	O
to	O
make	O
it	O
more	O
distinctive	O
a	O
forecast	O
in	O
preparation	O
whatever	O
it	O
is	O
called	O
chapter	O
frontiers	O
it	O
is	O
in	O
the	O
form	O
of	O
a	O
value	B
function	I
and	O
thus	O
can	O
be	O
learned	O
in	O
the	O
usual	O
ways	O
using	O
the	O
methods	O
developed	O
in	O
this	O
book	O
for	O
learning	O
approximate	O
value	B
functions	O
along	O
with	O
the	O
learned	O
predictions	O
we	O
might	O
also	O
learn	O
policies	O
to	O
maximize	O
the	O
predictions	O
in	O
the	O
usual	O
ways	O
by	O
generalized	O
policy	B
iteration	I
or	O
by	O
actor	O
critic	O
methods	O
in	O
this	O
way	O
an	O
agent	O
could	O
learn	O
to	O
predict	O
and	B
control	B
great	O
numbers	O
of	O
signals	O
not	O
just	O
long-term	O
reward	O
why	O
might	O
it	O
be	O
useful	O
to	O
predict	O
and	B
control	B
signals	O
other	O
than	O
long-term	O
reward	O
these	O
are	O
auxiliary	B
tasks	I
in	O
that	O
they	O
are	O
extra	O
in-addition-to	O
the	O
main	O
task	O
of	O
maximizing	O
reward	O
one	O
answer	O
is	O
that	O
the	O
ability	O
to	O
predict	O
and	B
control	B
a	O
diverse	O
multitude	O
of	O
signals	O
can	O
constitute	O
a	O
powerful	O
kind	O
of	O
environmental	O
model	O
as	O
we	O
saw	O
in	O
chapter	O
a	O
good	O
model	O
can	O
enable	O
the	O
agent	O
to	O
get	O
reward	O
more	O
efficiently	O
it	O
takes	O
a	O
couple	O
of	O
further	O
concepts	O
to	O
develop	O
this	O
answer	O
clearly	O
so	O
we	O
postpone	O
it	O
to	O
the	O
next	O
section	O
first	O
let	O
s	O
consider	O
two	O
simpler	O
ways	O
in	O
which	O
a	O
multitude	O
of	O
diverse	O
predictions	O
can	O
be	O
helpful	O
to	O
a	O
reinforcement	B
learning	I
agent	O
one	O
simple	O
way	O
in	O
which	O
auxiliary	B
tasks	I
can	O
help	O
on	O
the	O
main	O
task	O
is	O
that	O
they	O
may	O
require	O
some	O
of	O
the	O
same	O
representations	O
as	O
are	O
needed	O
on	O
the	O
main	O
task	O
some	O
of	O
the	O
auxiliary	B
tasks	I
may	O
be	O
easier	O
with	O
less	O
delay	O
and	O
a	O
clearer	O
connection	O
between	O
actions	O
and	O
outcomes	O
if	O
good	O
features	O
can	O
be	O
found	O
early	O
on	O
easy	O
auxilary	O
tasks	O
then	O
those	O
features	O
may	O
significantly	O
speed	O
learning	O
on	O
the	O
main	O
task	O
there	O
is	O
no	O
necessary	O
reason	O
why	O
this	O
has	O
to	O
be	O
true	O
but	O
in	O
many	O
cases	O
it	O
seems	O
plausible	O
for	O
example	O
if	O
you	O
learn	O
to	O
predict	O
and	B
control	B
your	O
sensors	O
over	O
short	O
time	O
scales	O
say	O
seconds	O
then	O
you	O
might	O
plausibly	O
come	O
up	O
with	O
part	O
of	O
the	O
idea	O
of	O
objects	O
which	O
would	O
then	O
greatly	O
help	O
with	O
the	O
prediction	B
and	B
control	B
of	O
long-term	O
reward	O
we	O
might	O
imagine	O
an	O
artificial	O
neural	B
network	O
in	O
which	O
the	O
last	O
layer	O
is	O
split	O
into	O
multiple	O
parts	O
or	O
heads	O
each	O
working	O
on	O
a	O
different	O
task	O
one	O
head	O
might	O
produce	O
the	O
approximate	O
value	B
function	I
for	O
the	O
main	O
task	O
reward	O
as	O
its	O
cumulant	O
whereas	O
the	O
others	O
would	O
produce	O
solutions	O
to	O
various	O
auxilary	O
tasks	O
all	O
heads	O
could	O
propagate	O
errors	O
by	O
stochastic	O
gradient	B
descent	O
into	O
the	O
same	O
body	O
the	O
shared	O
preceding	O
part	O
of	O
the	O
network	O
which	O
would	O
then	O
try	O
to	O
form	O
representations	O
in	O
its	O
next-to-last	O
layer	O
to	O
support	O
all	O
the	O
heads	O
researchers	O
have	O
experimented	O
with	O
auxiliary	B
tasks	I
such	O
as	O
predicting	O
change	O
in	O
pixels	O
predicting	O
the	O
next	O
time	O
step	O
s	O
reward	O
and	O
predicting	O
the	O
distribution	O
of	O
the	O
return	B
in	O
many	O
cases	O
this	O
approach	O
has	O
been	O
shown	O
to	O
greatly	O
accelerate	O
learning	O
on	O
the	O
main	O
task	O
et	O
al	O
multiple	O
predictions	O
have	O
similarly	O
been	O
repeatedly	O
proposed	O
as	O
a	O
way	O
of	O
directing	O
the	O
construction	O
of	O
state	B
estimates	O
section	O
another	O
simple	O
way	O
in	O
which	O
the	O
learning	O
of	O
auxiliary	B
tasks	I
can	O
improve	O
performance	O
is	O
best	O
explained	O
by	O
analogy	O
to	O
the	O
psychological	O
phenomena	O
of	O
classical	B
conditioning	I
one	O
way	O
of	O
understanding	O
classical	B
conditioning	I
is	O
that	O
evolution	B
has	O
built	O
in	O
a	O
reflexive	O
association	O
to	O
a	O
particular	O
action	B
from	O
the	O
prediction	B
of	O
a	O
particular	O
signal	O
for	O
example	O
humans	O
and	O
many	O
other	O
animals	O
appear	O
to	O
have	O
a	O
built-in	O
reflex	O
to	O
blink	O
whenever	O
their	O
prediction	B
of	O
being	O
poked	O
in	O
the	O
eye	O
exceeds	O
some	O
threshold	O
the	O
prediction	B
is	O
learned	O
but	O
the	O
association	O
from	O
prediction	B
to	O
eye	O
closure	O
is	O
built	O
in	O
and	O
thus	O
the	O
animal	O
is	O
saved	O
many	O
unprotected	O
pokes	O
in	O
its	O
eye	O
similarly	O
the	O
association	O
from	O
fear	O
to	O
increased	O
heart	O
rate	O
or	O
to	O
freezing	O
can	O
be	O
built	O
in	O
agent	O
temporal	B
abstraction	I
via	O
options	B
designers	O
can	O
do	O
something	O
similar	O
connecting	O
by	O
design	O
learning	O
predictions	O
of	O
specific	O
events	O
to	O
predetermined	O
actions	O
for	O
example	O
a	O
self-driving	O
car	O
that	O
learns	O
to	O
predict	O
whether	O
going	O
forward	O
will	O
produce	O
a	O
collision	O
could	O
be	O
given	O
a	O
built-in	O
reflex	O
to	O
stop	O
or	O
to	O
turn	O
away	O
whenever	O
the	O
prediction	B
is	O
above	O
some	O
threshold	O
or	O
consider	O
a	O
vacuum-cleaning	O
robot	O
that	O
learned	O
to	O
predict	O
whether	O
it	O
might	O
run	O
out	O
of	O
battery	O
power	O
before	O
returning	O
to	O
the	O
charger	O
and	O
that	O
reflexively	O
headed	O
back	O
to	O
the	O
charger	O
whenever	O
the	O
prediction	B
became	O
non-zero	O
the	O
correct	O
prediction	B
would	O
depend	O
on	O
the	O
size	O
of	O
the	O
house	O
the	O
room	O
the	O
robot	O
was	O
in	O
and	O
the	O
age	O
of	O
the	O
battery	O
all	O
of	O
which	O
would	O
be	O
hard	O
for	O
the	O
robot	O
designer	O
to	O
know	O
it	O
would	O
be	O
difficult	O
for	O
the	O
designer	O
to	O
build	O
in	O
a	O
reliable	O
algorithm	O
for	O
deciding	O
whether	O
to	O
head	O
back	O
to	O
the	O
charger	O
in	O
sensory	O
terms	O
but	O
it	O
might	O
be	O
easy	O
to	O
do	O
this	O
in	O
terms	O
of	O
the	O
learned	O
prediction	B
we	O
foresee	O
many	O
possible	O
ways	O
like	O
this	O
in	O
which	O
learned	O
predictions	O
might	O
combine	O
usefully	O
with	O
built-in	O
algorithms	O
for	O
controlling	O
behavior	O
finally	O
perhaps	O
the	O
most	O
important	O
role	O
for	O
auxiliary	B
tasks	I
is	O
in	O
moving	O
beyond	O
the	O
assumption	O
we	O
have	O
made	O
throughout	O
this	O
book	O
that	O
the	O
state	B
representation	O
is	O
fixed	O
and	O
given	O
to	O
the	O
agent	O
to	O
explain	O
this	O
role	O
we	O
first	O
have	O
to	O
take	O
a	O
few	O
steps	O
back	O
to	O
appreciate	O
the	O
magnitude	O
of	O
this	O
assumption	O
and	O
the	O
implications	O
of	O
removing	O
it	O
we	O
do	O
that	O
in	O
section	O
temporal	B
abstraction	I
via	O
options	B
an	O
appealing	O
aspect	O
of	O
the	O
mdp	O
formalism	O
is	O
that	O
it	O
can	O
be	O
applied	O
usefully	O
to	O
tasks	O
at	O
many	O
different	O
time	O
scales	O
one	O
can	O
use	O
it	O
to	O
formalize	O
the	O
task	O
of	O
deciding	O
which	O
muscles	O
to	O
twitch	O
to	O
grasp	O
an	O
object	O
which	O
airplane	O
flight	O
to	O
take	O
to	O
arrive	O
conveniently	O
at	O
a	O
distant	O
city	O
and	O
which	O
job	O
to	O
take	O
to	O
lead	O
a	O
satisfying	O
life	O
these	O
tasks	O
differ	O
greatly	O
in	O
their	O
time	O
scales	O
yet	O
each	O
can	O
be	O
usefully	O
formulated	O
as	O
an	O
mdp	O
that	O
can	O
be	O
solved	O
by	O
planning	B
or	O
learning	O
processes	O
as	O
described	O
in	O
this	O
book	O
all	O
involve	O
interaction	O
with	O
the	O
world	O
sequential	O
decision	O
making	O
and	O
a	O
goal	O
usefully	O
conceived	O
of	O
as	O
accumulating	B
rewards	O
over	O
time	O
and	O
so	O
all	O
can	O
be	O
formulated	O
as	O
mdps	O
although	O
all	O
these	O
tasks	O
can	O
be	O
formulated	O
as	O
mdps	O
one	O
might	O
think	O
that	O
they	O
cannot	O
be	O
formulated	O
as	O
a	O
single	O
mdp	O
they	O
involve	O
such	O
different	O
time	O
scales	O
such	O
different	O
notions	O
of	O
choice	O
and	O
action	B
it	O
would	O
be	O
no	O
good	O
for	O
example	O
to	O
plan	O
a	O
flight	O
across	O
a	O
continent	O
at	O
the	O
level	O
of	O
muscle	O
twitches	O
yet	O
for	O
other	O
tasks	O
grasping	O
throwing	O
darts	O
or	O
hitting	O
a	O
baseball	O
low-level	O
muscle	O
twitches	O
may	O
be	O
just	O
the	O
right	O
level	O
people	O
do	O
all	O
these	O
things	O
seamlessly	O
without	O
appearing	O
to	O
switch	O
between	O
levels	O
can	O
the	O
mdp	O
framework	O
be	O
stretched	O
to	O
cover	O
all	O
the	O
levels	O
simultaneously	O
perhaps	O
it	O
can	O
one	O
popular	O
idea	O
is	O
to	O
formalize	O
an	O
mdp	O
at	O
a	O
detailed	O
level	O
with	O
a	O
small	O
time	O
step	O
yet	O
enable	O
planning	B
at	O
higher	O
levels	O
using	O
extended	O
courses	O
of	O
action	B
that	O
correspond	O
to	O
many	O
base-level	O
time	O
steps	O
to	O
do	O
this	O
we	O
need	O
a	O
notion	O
of	O
course	O
of	O
action	B
that	O
extends	O
over	O
many	O
time	O
steps	O
and	O
includes	O
a	O
notion	O
of	O
termination	O
a	O
general	O
way	O
to	O
formulate	O
these	O
two	O
ideas	O
is	O
as	O
a	O
policy	B
and	O
a	O
state-dependent	O
termination	O
function	O
as	O
in	O
gvfs	O
we	O
define	O
a	O
pair	O
of	O
these	O
as	O
a	O
generalized	O
notion	O
of	O
action	B
termed	O
an	O
option	O
to	O
execute	O
an	O
option	O
at	O
time	O
t	O
is	O
to	O
obtain	O
the	O
action	B
to	O
take	O
at	O
from	O
then	O
terminate	O
at	O
time	O
t	O
with	O
probability	O
if	O
the	O
option	O
does	O
chapter	O
frontiers	O
not	O
terminate	O
at	O
t	O
then	O
is	O
selected	O
from	O
and	O
the	O
option	O
terminates	O
at	O
t	O
with	O
probability	O
and	O
so	O
on	O
until	O
eventual	O
termination	O
it	O
is	O
convenient	O
to	O
consider	O
low-level	O
actions	O
to	O
be	O
special	O
cases	O
of	O
options	B
each	O
action	B
a	O
corresponds	O
to	O
an	O
option	O
whose	O
policy	B
picks	O
the	O
action	B
a	O
for	O
all	O
s	O
s	O
and	O
whose	O
termination	O
function	O
is	O
zero	O
for	O
all	O
s	O
s	O
options	B
effectively	O
extend	O
the	O
action	B
space	O
the	O
agent	O
can	O
either	O
select	O
a	O
low-level	O
actionoption	O
terminating	O
after	O
one	O
time	O
step	O
or	O
select	O
an	O
extended	O
option	O
that	O
might	O
execute	O
for	O
many	O
time	O
steps	O
before	O
terminating	O
options	B
are	O
designed	O
so	O
that	O
they	O
are	O
interchangable	O
with	O
low-level	O
actions	O
for	O
example	O
the	O
notion	O
of	O
an	O
action-value	O
function	O
q	O
naturally	O
generalizes	O
to	O
an	O
optionvalue	O
function	O
that	O
takes	O
a	O
state	B
and	O
option	O
as	O
input	O
and	O
returns	O
the	O
expected	B
return	B
starting	O
from	O
that	O
state	B
executing	O
that	O
option	O
to	O
termination	O
and	O
thereafter	O
following	O
the	O
policy	B
we	O
can	O
also	O
generalize	O
the	O
notion	O
of	O
policy	B
to	O
a	O
hierarchical	B
policy	B
that	O
selects	O
from	O
options	B
rather	O
than	O
actions	O
where	O
options	B
when	O
selected	O
execute	O
until	O
termination	O
with	O
these	O
ideas	O
many	O
of	O
the	O
algorithms	O
in	O
this	O
book	O
can	O
be	O
generalized	O
to	O
learn	O
approximate	O
option-value	O
functions	O
and	O
hierarchical	O
policies	O
in	O
the	O
simplest	O
case	O
the	O
learning	O
process	O
jumps	O
from	O
option	O
initiation	O
to	O
option	O
termination	O
with	O
an	O
update	O
only	O
occurring	O
when	O
an	O
option	O
terminates	O
more	O
subtly	O
updates	O
can	O
be	O
made	O
on	O
each	O
time	O
step	O
using	O
intra-option	O
learning	O
algorithms	O
which	O
in	O
general	O
require	O
off-policy	B
learning	O
perhaps	O
the	O
most	O
important	O
generalization	O
made	O
possible	O
by	O
option	O
ideas	O
is	O
that	O
of	O
the	O
environmental	O
model	O
as	O
developed	O
in	O
chapters	O
and	O
the	O
conventional	O
model	O
of	O
an	O
action	B
is	O
the	O
state-transition	O
probabilities	O
and	O
the	O
expected	B
immediate	O
reward	O
for	O
taking	O
the	O
action	B
in	O
each	O
state	B
how	O
do	O
conventional	O
action	B
models	O
generalize	O
to	O
option	B
models	I
for	B
options	B
the	O
appropriate	O
model	O
is	O
again	O
of	O
two	O
parts	O
one	O
corresponding	O
to	O
the	O
state	B
transition	O
resulting	O
from	O
executing	O
the	O
option	O
and	O
one	O
corresponding	O
to	O
the	O
expected	B
cumulative	O
reward	O
along	O
the	O
way	O
the	O
reward	O
part	O
of	O
an	O
option	O
model	O
analogous	O
to	O
the	O
expected	B
reward	O
for	O
state	B
action	B
pairs	O
is	O
rs	O
s	O
for	O
all	O
options	B
and	O
all	O
states	O
s	O
s	O
where	O
is	O
the	O
random	O
time	O
step	O
at	O
which	O
the	O
option	O
terminates	O
according	O
to	O
note	O
the	O
role	O
of	O
the	O
overall	O
discounting	B
parameter	O
in	O
this	O
equation	O
discounting	B
is	O
according	O
to	O
but	O
termination	O
of	O
the	O
option	O
is	O
according	O
to	O
the	O
state-transition	O
part	O
of	O
an	O
option	O
model	O
is	O
a	O
little	O
more	O
subtle	O
this	O
part	O
of	O
the	O
model	O
characterizes	O
the	O
probability	O
of	O
each	O
possible	O
resulting	O
state	B
in	O
but	O
now	O
this	O
state	B
may	O
result	O
after	O
various	O
numbers	O
of	O
time	O
steps	O
each	O
of	O
which	O
must	O
be	O
discounted	O
differently	O
the	O
model	O
for	O
option	O
specifies	O
for	O
each	O
state	B
s	O
that	O
might	O
start	O
executing	O
in	O
and	O
for	O
each	O
state	B
that	O
might	O
terminate	O
in	O
k	O
prsk	O
k	O
s	O
note	O
that	O
because	O
of	O
the	O
factor	O
of	O
k	O
this	O
is	O
no	O
longer	O
a	O
transition	O
probability	O
and	O
no	O
longer	O
sums	O
to	O
one	O
over	O
all	O
values	O
of	O
we	O
continue	O
to	O
use	O
the	O
notation	O
in	O
p	O
temporal	B
abstraction	I
via	O
options	B
the	O
above	O
definition	O
of	O
the	O
transition	O
part	O
of	O
an	O
option	O
model	O
allows	O
us	O
to	O
formulate	O
bellman	B
equations	O
and	B
dynamic	B
programming	I
algorithms	O
that	O
apply	O
to	O
all	O
options	B
including	O
low-level	O
actions	O
as	O
a	O
special	O
case	O
for	O
example	O
the	O
general	O
bellman	B
equation	I
for	O
the	O
state	B
values	O
of	O
a	O
hierarchical	B
policy	B
is	O
v	O
where	O
denotes	O
the	O
set	O
of	O
options	B
available	O
in	O
state	B
s	O
if	O
includes	O
only	O
the	O
low-level	O
actions	O
then	O
this	O
equation	O
reduces	O
to	O
a	O
version	O
of	O
the	O
usual	O
bellman	B
equation	I
except	O
of	O
course	O
is	O
included	O
in	O
the	O
new	O
p	O
and	O
thus	O
does	O
not	O
appear	O
similarly	O
the	O
corresponding	O
planning	B
algorithms	O
also	O
have	O
no	O
for	O
example	O
the	O
value	B
iteration	I
algorithm	O
with	B
options	B
analogous	O
to	O
is	O
max	O
for	O
all	O
s	O
s	O
if	O
includes	O
all	O
the	O
low-level	O
actions	O
available	O
in	O
each	O
s	O
then	O
this	O
algorithm	O
converges	O
to	O
the	O
conventional	O
v	O
from	O
which	O
the	O
optimal	O
policy	B
can	O
be	O
computed	O
however	O
it	O
is	O
particularly	O
useful	O
to	O
plan	O
with	B
options	B
when	O
only	O
a	O
subset	O
of	O
the	O
possible	O
options	B
are	O
considered	O
in	O
each	O
state	B
value	B
iteration	I
will	O
then	O
converge	O
to	O
the	O
best	O
hierarchical	B
policy	B
limited	O
to	O
the	O
restricted	O
set	O
of	O
options	B
although	O
this	O
policy	B
may	O
be	O
sub-optimal	O
convergence	O
can	O
be	O
much	O
faster	O
because	O
fewer	O
options	B
are	O
considered	O
and	O
because	O
each	O
option	O
can	O
jump	O
over	O
many	O
time	O
steps	O
to	O
plan	O
with	B
options	B
one	O
must	O
either	O
be	O
given	O
the	O
option	B
models	I
or	O
learn	O
them	O
one	O
natural	O
way	O
to	O
learn	O
an	O
option	O
model	O
is	O
to	O
formulate	O
it	O
as	O
a	O
collection	O
of	O
gvfs	O
defined	O
in	O
the	O
preceding	O
section	O
and	O
then	O
learn	O
the	O
gvfs	O
using	O
the	O
methods	O
presented	O
in	O
this	O
book	O
it	O
is	O
not	O
difficult	O
to	O
see	O
how	O
this	O
could	O
be	O
done	O
for	O
the	O
reward	O
part	O
of	O
the	O
option	O
model	O
one	O
merely	O
chooses	O
one	O
gvf	O
s	O
cumulant	O
to	O
be	O
the	O
reward	O
rt	O
its	O
policy	B
to	O
be	O
the	O
the	O
option	O
s	O
policy	B
and	O
its	O
termination	O
function	O
to	O
be	O
the	O
discount	O
rate	O
times	O
the	O
option	O
s	O
termination	O
function	O
the	O
true	O
gvf	O
then	O
equals	O
the	O
reward	O
part	O
of	O
the	O
option	O
model	O
v	O
rs	O
and	O
the	O
learning	O
methods	O
described	O
in	O
this	O
book	O
can	O
be	O
used	O
to	O
approximate	O
it	O
the	O
state-transition	O
part	O
of	O
the	O
option	O
model	O
is	O
only	O
a	O
little	O
more	O
complicated	O
one	O
needs	O
to	O
allocate	O
one	O
gvf	O
for	O
each	O
state	B
that	O
the	O
option	O
might	O
terminate	O
in	O
we	O
don	O
t	O
want	O
these	O
gvfs	O
to	O
accumulate	O
anything	O
except	O
when	O
the	O
option	O
terminates	O
and	O
then	O
only	O
when	O
the	O
termination	O
is	O
in	O
the	O
appropriate	O
state	B
this	O
can	O
be	O
achieved	O
by	O
choosing	O
the	O
cumulant	O
of	O
the	O
gvf	O
that	O
predicts	O
transition	O
to	O
state	B
to	O
be	O
ct	O
the	O
gvf	O
s	O
policy	B
and	O
termination	O
functions	O
are	O
chosen	O
the	O
same	O
as	O
for	O
the	O
reward	O
part	O
of	O
the	O
option	O
model	O
the	O
true	O
gvf	O
then	O
equals	O
the	O
portion	O
of	O
the	O
option	O
s	O
state-transition	O
model	O
v	O
and	O
again	O
this	O
book	O
s	O
methods	O
could	O
be	O
employed	O
to	O
learn	O
it	O
although	O
each	O
of	O
these	O
steps	O
is	O
seemingly	O
natural	O
putting	O
them	O
all	O
together	O
function	B
approximation	I
and	O
other	O
essential	O
components	O
is	O
quite	O
challenging	O
and	O
beyond	O
the	O
current	O
state	B
of	O
the	O
art	O
chapter	O
frontiers	O
exercise	O
this	O
section	O
has	O
presented	O
options	B
for	O
the	O
discounted	O
case	O
but	O
discounting	B
is	O
arguably	O
inappropriate	O
for	O
control	B
when	O
using	O
function	B
approximation	I
what	O
is	O
the	O
natural	O
bellman	B
equation	I
for	O
a	O
hierarchical	B
policy	B
analogous	O
to	O
but	O
for	O
the	O
average	B
reward	I
setting	I
what	O
are	O
the	O
two	O
parts	O
of	O
the	O
option	O
model	O
analogous	O
to	O
and	O
for	O
the	O
average	B
reward	I
setting	I
observations	O
and	O
state	B
throughout	O
this	O
book	O
we	O
have	O
written	O
the	O
learned	O
approximate	O
value	B
functions	O
the	O
policies	O
in	O
chapter	O
as	O
functions	O
of	O
the	O
environment	B
s	O
state	B
this	O
is	O
a	O
significant	O
limitation	O
of	O
the	O
methods	O
presented	O
in	O
part	O
i	O
in	O
which	O
the	O
learned	O
value	B
function	I
was	O
implemented	O
as	O
a	O
table	O
such	O
that	O
any	O
value	B
function	I
could	O
be	O
exactly	O
approximated	O
that	O
case	O
is	O
tantamount	O
to	O
assuming	O
that	O
the	O
state	B
of	O
the	O
environment	B
is	O
completely	O
observed	O
by	O
the	O
agent	O
but	O
in	O
many	O
cases	O
of	O
interest	O
and	O
certainly	O
in	O
the	O
lives	O
of	O
all	O
natural	O
intelligences	O
the	O
sensory	O
input	O
gives	O
only	O
partial	O
information	O
about	O
the	O
state	B
of	O
the	O
world	O
some	O
objects	O
may	O
be	O
occluded	O
by	O
others	O
or	O
behind	O
the	O
agent	O
or	O
miles	O
away	O
in	O
these	O
cases	O
potentially	O
important	O
aspects	O
of	O
the	O
environment	B
s	O
state	B
are	O
not	O
directly	O
observable	O
and	O
it	O
is	O
a	O
strong	O
unrealistic	O
and	O
limiting	O
assumption	O
to	O
assume	O
that	O
the	O
learned	O
value	B
function	I
is	O
implemented	O
as	O
a	O
table	O
over	O
the	O
environment	B
s	O
state	B
space	O
the	O
framework	O
of	O
parametric	O
function	B
approximation	I
that	O
we	O
developed	O
in	O
part	O
ii	O
is	O
far	O
less	O
restrictive	O
and	O
arguably	O
no	O
limitation	O
at	O
all	O
in	O
part	O
ii	O
we	O
retained	O
the	O
assumption	O
that	O
the	O
learned	O
value	B
functions	O
policies	O
are	O
functions	O
of	O
the	O
environment	B
s	O
state	B
but	O
allowed	O
these	O
functions	O
to	O
be	O
arbitrarily	O
restricted	O
by	O
the	O
parameterization	O
it	O
is	O
somewhat	O
surprising	O
and	O
not	O
widely	O
recognized	O
that	O
function	B
approximation	I
includes	O
important	O
aspects	O
of	O
partial	O
observability	O
for	O
example	O
if	O
there	O
is	O
some	O
state	B
variable	O
that	O
is	O
not	O
observable	O
then	O
the	O
parameterization	O
can	O
be	O
chosen	O
such	O
that	O
the	O
approximate	O
value	B
does	O
not	O
depend	O
on	O
that	O
state	B
variable	O
the	O
effect	O
is	O
just	O
as	O
if	O
that	O
state	B
variable	O
were	O
not	O
observable	O
because	O
of	O
this	O
all	O
the	O
results	O
obtained	O
for	O
the	O
parameterized	O
case	O
apply	O
to	O
partial	O
observability	O
without	O
change	O
in	O
this	O
sense	O
the	O
case	O
of	O
parameterized	O
function	B
approximation	I
includes	O
the	O
case	O
of	O
partial	O
observability	O
nevertheless	O
there	O
are	O
many	O
issues	O
that	O
cannot	O
be	O
investigated	O
without	O
a	O
more	O
explicit	O
treatment	O
of	O
partial	O
observability	O
although	O
we	O
cannot	O
give	O
them	O
a	O
full	O
treatment	O
here	O
we	O
can	O
outline	O
the	O
changes	O
that	O
would	O
be	O
needed	O
to	O
do	O
so	O
there	O
are	O
four	O
steps	O
first	O
we	O
would	O
change	O
the	O
problem	O
the	O
environment	B
would	O
emit	O
not	O
its	O
states	O
but	O
only	O
observations	O
signals	O
that	O
depend	O
on	O
its	O
state	B
but	O
like	O
a	O
robot	O
s	O
sensors	O
provide	O
only	O
partial	O
information	O
about	O
it	O
for	O
convenience	O
without	O
loss	O
of	O
generality	O
we	O
assume	O
that	O
the	O
reward	O
is	O
a	O
direct	O
known	O
function	O
of	O
the	O
observation	O
the	O
observation	O
is	O
a	O
vector	B
and	O
the	O
reward	O
is	O
one	O
of	O
is	O
components	O
the	O
environmental	O
interaction	O
would	O
then	O
have	O
no	O
explicit	O
states	O
or	O
rewards	O
but	O
would	O
simply	O
be	O
an	O
alternating	O
sequence	O
of	O
actions	O
at	O
a	O
and	B
observations	I
ot	O
o	O
going	O
on	O
forever	O
equation	O
or	O
forming	O
episodes	B
each	O
ending	O
with	O
a	O
special	O
terminal	O
observation	O
observations	O
and	O
state	B
second	O
we	O
can	O
recover	O
the	O
idea	O
of	O
state	B
as	O
used	O
in	O
this	O
book	O
from	O
the	O
sequence	O
of	O
observations	O
and	O
actions	O
let	O
us	O
use	O
the	O
word	O
history	O
and	O
the	O
notation	O
ht	O
for	O
an	O
initial	O
portion	O
of	O
the	O
trajectory	O
up	O
to	O
an	O
observation	O
ht	O
at	O
ot	O
the	O
history	O
represents	O
the	O
most	O
that	O
we	O
can	O
know	O
about	O
the	O
past	O
without	O
looking	O
outside	O
of	O
the	O
data	O
stream	O
the	O
history	O
is	O
the	O
whole	O
past	O
data	O
stream	O
of	O
course	O
the	O
history	O
grows	O
with	O
t	O
and	O
can	O
become	O
large	O
and	O
unwieldy	O
the	O
idea	O
of	O
state	B
is	O
that	O
of	O
some	O
compact	O
summary	O
of	O
the	O
history	O
that	O
is	O
as	O
useful	O
as	O
the	O
actual	O
history	O
for	O
predicting	O
the	O
future	O
let	O
us	O
be	O
clear	O
about	O
exactly	O
what	O
this	O
means	O
to	O
be	O
a	O
summary	O
of	O
the	O
history	O
the	O
state	B
must	O
be	O
a	O
function	O
of	O
history	O
st	O
f	O
and	O
to	O
be	O
as	O
useful	O
for	O
predicting	O
the	O
future	O
as	O
the	O
whole	O
history	O
it	O
must	O
have	O
what	O
is	O
known	O
as	O
the	O
markov	B
property	I
formally	O
this	O
is	O
a	O
property	O
of	O
the	O
function	O
f	O
a	O
function	O
f	O
has	O
the	O
markov	B
property	I
if	O
and	O
only	O
if	O
any	O
two	O
histories	O
h	O
and	O
that	O
are	O
mapped	O
by	O
f	O
to	O
the	O
same	O
state	B
f	O
also	O
have	O
the	O
same	O
probabilities	O
for	O
their	O
next	O
observation	O
f	O
f	O
oht	O
h	O
at	O
a	O
oht	O
at	O
a	O
for	O
all	O
o	O
o	O
and	O
a	O
a	O
if	O
f	O
is	O
markov	O
then	O
st	O
f	O
is	O
a	O
state	B
as	O
we	O
have	O
used	O
the	O
term	O
in	O
this	O
book	O
let	O
us	O
henceforth	O
call	O
it	O
a	O
markov	O
state	B
to	O
distinguish	O
it	O
from	O
states	O
that	O
are	O
summaries	O
of	O
the	O
history	O
but	O
fall	O
short	O
of	O
the	O
markov	B
property	I
we	O
will	O
consider	O
shortly	O
a	O
markov	O
state	B
is	O
a	O
good	O
basis	O
for	O
predicting	O
the	O
next	O
observation	O
but	O
more	O
importantly	O
it	O
is	O
also	O
a	O
good	O
basis	O
for	O
predicting	O
or	O
controlling	O
anything	O
for	O
example	O
let	O
a	O
test	O
be	O
any	O
specific	O
sequence	O
of	O
alternating	O
actions	O
and	B
observations	I
that	O
might	O
occur	O
in	O
the	O
future	O
for	O
example	O
a	O
three-step	O
test	O
is	O
denoted	O
the	O
probability	O
of	O
this	O
test	O
given	O
a	O
specific	O
history	O
h	O
is	O
defined	O
as	O
p	O
ht	O
h	O
at	O
if	O
f	O
is	O
markov	O
and	O
h	O
and	O
are	O
any	O
two	O
histories	O
that	O
map	O
to	O
the	O
same	O
state	B
under	O
f	O
then	O
for	O
any	O
test	O
of	O
any	O
length	O
its	O
probabilities	O
given	O
the	O
two	O
histories	O
must	O
also	O
be	O
the	O
same	O
f	O
f	O
p	O
p	O
in	O
other	O
words	O
a	O
markov	O
state	B
summarizes	O
all	O
the	O
information	O
in	O
the	O
history	O
necessary	O
for	O
determining	O
any	O
test	O
s	O
probability	O
in	O
fact	O
it	O
summarizes	O
all	O
that	O
is	O
necessary	O
for	O
making	O
any	O
prediction	B
including	O
any	O
gvf	O
and	O
for	O
behaving	O
optimally	O
f	O
is	O
markov	O
then	O
there	O
is	O
always	O
a	O
deterministic	O
function	O
such	O
that	O
choosing	O
at	O
is	O
optimal	O
the	O
third	O
step	O
in	O
extending	O
reinforcement	B
learning	I
to	O
partial	O
observability	O
is	O
to	O
deal	O
with	O
certain	O
computational	O
considerations	O
in	O
particular	O
we	O
want	O
the	O
state	B
to	O
be	O
a	O
compact	O
summary	O
of	O
the	O
history	O
for	O
example	O
the	O
identity	O
function	O
completely	O
satisfies	O
the	O
conditions	O
for	O
a	O
markov	O
f	O
but	O
would	O
nevertheless	O
be	O
of	O
little	O
use	O
because	O
the	O
corresponding	O
state	B
st	O
ht	O
would	O
grow	O
with	O
time	O
and	O
become	O
unwieldy	O
as	O
mentioned	O
earlier	O
but	O
more	O
fundamentally	O
because	O
it	O
would	O
never	O
recur	O
the	O
agent	O
would	O
never	O
chapter	O
frontiers	O
encounter	O
the	O
same	O
state	B
twice	O
a	O
continuing	O
task	O
and	O
thus	O
could	O
never	O
benefit	O
from	O
a	O
tabular	O
learning	O
method	O
we	O
want	O
our	O
states	O
to	O
be	O
compact	O
as	O
well	O
as	O
markov	O
there	O
is	O
a	O
similar	O
issue	O
regarding	O
how	O
state	B
is	O
obtained	O
and	O
updated	O
we	O
don	O
t	O
really	O
want	O
a	O
function	O
f	O
that	O
takes	O
whole	O
histories	O
instead	O
for	O
computational	O
reasons	O
we	O
prefer	O
to	O
obtain	O
the	O
same	O
effect	O
as	O
f	O
with	O
an	O
incremental	O
recursive	O
update	O
that	O
computes	O
from	O
st	O
incorporating	O
the	O
next	O
increment	O
of	O
data	O
at	O
and	O
ust	O
at	O
for	O
all	O
t	O
with	O
the	O
first	O
state	B
given	O
the	O
function	O
u	O
is	O
called	O
the	O
state-update	B
function	I
for	O
example	O
if	O
f	O
were	O
the	O
identity	O
ht	O
then	O
u	O
would	O
merely	O
extend	O
st	O
by	O
appending	O
at	O
and	O
to	O
it	O
given	O
f	O
it	O
is	O
always	O
possible	O
to	O
construct	O
a	O
corresponding	O
u	O
but	O
it	O
may	O
not	O
be	O
computationally	O
convenient	O
and	O
as	O
in	O
the	O
identity	O
example	O
it	O
may	O
not	O
produce	O
a	O
compact	O
state	B
the	O
state-update	B
function	I
is	O
a	O
central	O
part	O
of	O
any	O
agent	O
architecture	O
that	O
handles	O
partial	O
observability	O
it	O
must	O
be	O
efficiently	O
computatible	O
as	O
no	O
actions	O
or	O
predictions	O
can	O
be	O
made	O
until	O
the	O
state	B
is	O
available	O
an	O
example	O
of	O
obtaining	O
markov	O
states	O
through	O
a	O
state-update	B
function	I
is	O
provided	O
by	O
the	O
popular	O
bayesian	O
approach	O
known	O
as	O
partially	B
observable	I
mdps	I
or	O
pomdps	O
in	O
this	O
approach	O
the	O
environment	B
is	O
assumed	O
to	O
have	O
a	O
well	O
defined	O
latent	B
state	B
xt	O
that	O
underlies	O
and	O
produces	O
the	O
environment	B
s	O
observations	O
but	O
is	O
never	O
available	O
to	O
the	O
agent	O
is	O
not	O
to	O
be	O
confused	O
with	O
the	O
state	B
st	O
used	O
by	O
the	O
agent	O
to	O
make	O
predictions	O
and	O
decisions	O
the	O
natural	O
markov	O
state	B
st	O
for	O
a	O
pomdp	O
is	O
the	O
distribution	O
over	O
the	O
latent	O
states	O
given	O
the	O
history	O
called	O
the	O
belief	B
state	B
for	O
concreteness	O
assume	O
the	O
usual	O
case	O
in	O
which	O
there	O
are	O
a	O
finite	O
number	O
of	O
hidden	O
states	O
xt	O
d	O
then	O
the	O
belief	B
state	B
is	O
the	O
vector	B
st	O
st	O
rd	O
with	O
components	O
sti	O
prxt	O
i	O
ht	O
for	O
all	O
possible	O
latent	O
states	O
i	O
d	O
the	O
belief	B
state	B
remains	O
the	O
same	O
size	O
number	O
of	O
components	O
however	O
t	O
grows	O
it	O
can	O
also	O
be	O
incrementally	O
updated	O
by	O
bayes	O
rule	O
assuming	O
one	O
has	O
complete	O
knowledge	O
of	O
the	O
internal	O
workings	O
of	O
the	O
environment	B
specifically	O
the	O
ith	O
component	O
of	O
the	O
belief-state	O
update	O
function	O
is	O
sxpi	O
ox	O
a	O
ox	O
a	O
us	O
a	O
oi	O
for	O
all	O
a	O
a	O
o	O
o	O
and	O
belief	O
states	O
s	O
rd	O
with	O
components	O
sx	O
where	O
the	O
fourargument	O
p	O
function	O
here	O
is	O
not	O
the	O
usual	O
one	O
for	O
mdps	O
in	O
chapter	O
but	O
the	O
anal	O
ogous	O
one	O
for	O
pomdps	O
in	O
terms	O
of	O
the	O
latent	B
state	B
ox	O
a	O
prxt	O
ot	O
o	O
x	O
at	O
a	O
this	O
approach	O
is	O
popular	O
in	O
theoretical	O
work	O
and	O
has	O
many	O
significant	O
applications	O
but	O
its	O
assumptions	O
and	O
computational	O
complexity	O
scale	O
poorly	O
and	O
we	O
do	O
not	O
recommend	O
it	O
as	O
an	O
approach	O
to	O
artificial	B
intelligence	I
another	O
example	O
of	O
markov	O
states	O
is	O
provided	O
by	O
predictive	B
state	B
representations	I
or	O
psrs	O
psrs	O
address	O
the	O
weakness	O
of	O
the	O
pomdp	O
approach	O
that	O
the	O
semantics	O
of	O
its	O
agent	O
state	B
st	O
are	O
grounded	O
in	O
the	O
environment	B
state	B
xt	O
which	O
is	O
never	O
observed	O
and	O
thus	O
is	O
difficult	O
to	O
learn	O
about	O
in	O
psrs	O
and	O
related	O
approaches	O
the	O
semantics	O
of	O
observations	O
and	O
state	B
the	O
agent	O
state	B
is	O
instead	O
grounded	O
in	O
predictions	O
about	O
future	O
observations	O
and	O
actions	O
which	O
are	O
readily	O
observable	O
in	O
psrs	O
a	O
markov	O
state	B
is	O
defined	O
as	O
a	O
d-vector	O
of	O
the	O
probabilities	O
of	O
d	O
specially	O
chosen	O
core	O
tests	O
as	O
defined	O
above	O
the	O
vector	B
is	O
then	O
updated	O
by	O
a	O
state-update	B
function	I
u	O
that	O
is	O
analogous	O
to	O
bayes	O
rule	O
but	O
with	O
a	O
semantics	O
grounded	O
in	O
observable	O
data	O
which	O
arguably	O
makes	O
it	O
easier	O
to	O
learn	O
this	O
approach	O
has	O
been	O
extended	O
in	O
many	O
ways	O
including	O
end-tests	O
compositional	O
tests	O
powerful	O
spectral	O
methods	O
and	O
closed-loop	O
and	O
temporally	O
abstract	O
tests	O
learned	O
by	O
td	B
methods	O
some	O
of	O
the	O
best	O
theoretical	O
developments	O
are	O
for	O
systems	O
known	O
as	O
observable	O
operator	O
models	O
and	O
sequential	O
systems	O
the	O
fourth	O
and	O
final	O
step	O
in	O
our	O
brief	O
outline	O
of	O
how	O
to	O
handle	O
partial	O
observability	O
in	O
reinforcement	B
learning	I
is	O
to	O
re-introduce	O
approximation	O
as	O
discussed	O
in	O
the	O
introduction	O
to	O
part	O
ii	O
to	O
approach	O
artificial	B
intelligence	I
ambitiously	O
one	O
must	O
embrace	O
approximation	O
this	O
is	O
just	O
as	O
true	O
for	O
states	O
as	O
it	O
is	O
for	O
value	B
functions	O
we	O
must	O
accept	O
and	O
work	O
with	O
an	O
approximate	O
notion	O
of	O
state	B
the	O
approximate	O
state	B
will	O
play	O
the	O
same	O
role	O
in	O
our	O
algorithms	O
as	O
before	O
so	O
we	O
continue	O
to	O
use	O
the	O
notation	O
st	O
for	O
the	O
state	B
used	O
by	O
the	O
agent	O
even	O
though	O
it	O
may	O
not	O
be	O
markov	O
perhaps	O
the	O
simplest	O
example	O
of	O
an	O
approximate	O
state	B
is	O
just	O
the	O
latest	O
observation	O
st	O
ot	O
of	O
course	O
this	O
approach	O
cannot	O
handle	O
any	O
hidden	O
state	B
information	O
it	O
would	O
be	O
better	O
to	O
use	O
the	O
last	O
k	O
observations	O
and	O
actions	O
st	O
ot	O
at	O
ot	O
at	O
k	O
for	O
some	O
k	O
which	O
can	O
be	O
achieved	O
by	O
a	O
state-update	B
function	I
that	O
just	O
shifts	O
the	O
new	O
data	O
in	O
and	O
the	O
oldest	O
data	O
out	O
this	O
kth-order	B
history	I
approach	I
is	O
still	O
very	O
simple	O
but	O
can	O
greatly	O
increase	O
the	O
agent	O
s	O
capabilities	O
compared	O
to	O
trying	O
to	O
use	O
the	O
single	O
immediate	O
observation	O
directly	O
as	O
the	O
state	B
what	O
happens	O
when	O
the	O
markov	B
property	I
is	O
only	O
approximately	O
satisfied	O
unfortunately	O
long-term	O
prediction	B
performance	O
can	O
degrade	O
dramatically	O
when	O
the	O
onestep	O
predictions	O
defining	O
the	O
markov	B
property	I
become	O
even	O
slightly	O
inaccurate	O
longerterm	O
tests	O
gvfs	O
and	O
state-update	O
functions	O
may	O
all	O
approximate	O
poorly	O
the	O
shortterm	O
and	O
long-term	O
approximation	O
objectives	O
are	O
just	O
different	O
and	O
there	O
are	O
no	O
useful	O
theoretical	O
guarantees	O
at	O
present	O
nevertheless	O
there	O
are	O
still	O
reasons	O
to	O
think	O
that	O
the	O
general	O
idea	O
outlined	O
in	O
this	O
section	O
applies	O
to	O
the	O
approximate	O
case	O
the	O
general	O
idea	O
is	O
that	O
a	O
state	B
that	O
is	O
good	O
for	O
some	O
predictions	O
is	O
also	O
good	O
for	O
others	O
particular	O
that	O
a	O
markov	O
state	B
sufficient	O
for	O
one-step	O
predictions	O
is	O
also	O
sufficient	O
for	O
all	O
others	O
if	O
we	O
step	O
back	O
from	O
that	O
specific	O
result	O
for	O
the	O
markov	O
case	O
the	O
general	O
idea	O
is	O
similar	O
to	O
what	O
we	O
discussed	O
in	O
section	O
with	O
multi-headed	O
learning	O
and	O
auxiliary	B
tasks	I
we	O
discussed	O
how	O
representations	O
that	O
were	O
good	O
for	O
the	O
auxiliary	B
tasks	I
were	O
often	O
also	O
good	O
for	O
the	O
main	O
task	O
taken	O
together	O
these	O
suggest	O
an	O
approach	O
to	O
both	O
partial	O
observability	O
and	O
representation	B
learning	I
in	O
which	O
multiple	O
predictions	O
are	O
pursued	O
and	O
used	O
to	O
direct	O
the	O
construction	O
of	O
state	B
features	O
the	O
guarantee	O
provided	O
by	O
the	O
perfect-but-impractical	O
markov	B
property	I
is	O
replaced	O
by	O
the	O
heuristic	O
that	O
what	O
s	O
good	O
for	O
some	O
predictions	O
may	O
be	O
good	O
for	O
others	O
this	O
approach	O
scales	O
well	O
with	O
computational	O
resources	O
with	O
a	O
large	O
machine	O
one	O
could	O
experiment	O
with	O
large	O
numbers	O
of	O
predictions	O
perhaps	O
favoring	O
those	O
that	O
are	O
most	O
similar	O
to	O
the	O
ones	O
of	O
ultimate	O
interest	O
or	O
that	O
are	O
easiest	O
to	O
learn	O
reliably	O
or	O
according	O
to	O
some	O
other	O
criteria	O
it	O
is	O
important	O
here	O
to	O
move	O
beyond	O
selecting	O
the	O
predictions	O
chapter	O
frontiers	O
manually	O
the	O
agent	O
should	O
do	O
it	O
this	O
would	O
require	O
a	O
general	O
language	O
for	O
predictions	O
so	O
that	O
the	O
agent	O
can	O
systematically	O
explore	O
a	O
large	O
space	O
of	O
possible	O
predictions	O
sifting	O
through	O
them	O
for	O
the	O
ones	O
that	O
are	O
most	O
useful	O
in	O
particular	O
both	O
pomdp	O
and	O
psr	O
approaches	O
can	O
be	O
applied	O
with	O
approximate	O
states	O
the	O
semantics	O
of	O
the	O
state	B
is	O
useful	O
in	O
forming	O
the	O
state-update	B
function	I
as	O
it	O
is	O
in	O
these	O
two	O
approaches	O
and	O
in	O
the	O
k-order	O
approach	O
there	O
is	O
not	O
a	O
strong	O
need	O
for	O
the	O
semantics	O
to	O
be	O
correct	O
in	O
order	O
to	O
retain	O
useful	O
information	O
in	O
the	O
state	B
some	O
approaches	O
to	O
state	B
augmentation	O
such	O
as	O
echo	O
state	B
networks	O
keep	O
almost	O
arbitrary	O
information	O
about	O
the	O
history	O
and	O
can	O
nevertheless	O
perform	O
well	O
there	O
are	O
many	O
possibilities	O
and	O
we	O
expect	O
more	O
work	O
and	O
ideas	O
in	O
this	O
area	O
learning	O
the	O
state-update	B
function	I
for	O
an	O
approximate	O
state	B
is	O
a	O
major	O
part	O
of	O
the	O
representation	B
learning	I
problem	O
as	O
it	O
arises	O
in	O
reinforcement	B
learning	I
designing	O
reward	O
signals	O
a	O
major	O
advantage	O
of	O
reinforcement	B
learning	I
over	O
supervised	B
learning	I
is	O
that	O
reinforcement	B
learning	I
does	O
not	O
rely	O
on	O
detailed	O
instructional	O
information	O
generating	O
a	O
reward	B
signal	I
does	O
not	O
depend	O
on	O
knowledge	O
of	O
what	O
the	O
agent	O
s	O
correct	O
actions	O
should	O
be	O
but	O
the	O
success	O
of	O
a	O
reinforcement	B
learning	I
application	O
strongly	O
depends	O
on	O
how	O
well	O
the	O
reward	B
signal	I
frames	O
the	O
goal	O
of	O
the	O
application	O
s	O
designer	O
and	O
how	O
well	O
the	O
signal	O
assesses	O
progress	O
in	O
reaching	O
that	O
goal	O
for	O
these	O
reasons	O
designing	O
a	O
reward	B
signal	I
is	O
a	O
critical	O
part	O
of	O
any	O
application	O
of	O
reinforcement	B
learning	I
by	O
designing	O
a	O
reward	B
signal	I
we	O
mean	O
designing	O
the	O
part	O
of	O
an	O
agent	O
s	O
environment	B
that	O
is	O
responsible	O
for	O
computing	O
each	O
scalar	O
reward	O
rt	O
and	O
sending	O
it	O
to	O
the	O
agent	O
at	O
each	O
time	O
t	O
in	O
our	O
discussion	O
of	O
terminology	O
at	O
the	O
end	O
of	O
chapter	O
we	O
said	O
that	O
rt	O
is	O
more	O
like	O
a	O
signal	O
generated	O
inside	O
an	O
animal	O
s	O
brain	O
than	O
it	O
is	O
like	O
an	O
object	O
or	O
event	O
in	O
the	O
animal	O
s	O
external	O
environment	B
the	O
parts	O
of	O
our	O
brains	O
that	O
generate	O
these	O
signals	O
for	O
us	O
evolved	O
over	O
millions	O
of	O
years	O
to	O
be	O
well	O
suited	O
to	O
the	O
challenges	O
our	O
ancestors	O
had	O
to	O
face	O
in	O
their	O
struggles	O
to	O
propagate	O
their	O
genes	O
to	O
future	O
generations	O
we	O
should	O
therefore	O
not	O
think	O
that	O
designing	O
a	O
good	O
reward	B
signal	I
is	O
always	O
an	O
easy	O
thing	O
to	O
do	O
one	O
challenge	O
is	O
to	O
design	O
a	O
reward	B
signal	I
so	O
that	O
as	O
an	O
agent	O
learns	O
its	O
behavior	O
approaches	O
and	O
ideally	O
eventually	O
achieves	O
what	O
the	O
application	O
s	O
designer	O
actually	O
desires	O
this	O
can	O
be	O
easy	O
if	O
the	O
designer	O
s	O
goal	O
is	O
simple	O
and	O
easy	O
to	O
identify	O
such	O
as	O
finding	O
the	O
solution	O
to	O
a	O
well-defined	O
problem	O
or	O
earning	O
a	O
high	O
score	O
in	O
a	O
well-defined	O
game	O
in	O
cases	O
like	O
these	O
it	O
is	O
usual	O
to	O
reward	O
the	O
agent	O
according	O
to	O
its	O
success	O
in	O
solving	O
the	O
problem	O
or	O
its	O
success	O
in	O
improving	O
its	O
score	O
but	O
some	O
problems	O
involve	O
goals	O
that	O
are	O
difficult	O
to	O
translate	O
into	O
reward	O
signals	O
this	O
is	O
especially	O
true	O
when	O
the	O
problem	O
requires	O
the	O
agent	O
to	O
skillfully	O
perform	O
a	O
complex	O
task	O
or	O
set	O
of	O
tasks	O
such	O
as	O
would	O
be	O
required	O
of	O
a	O
useful	O
household	O
robotic	O
assistant	O
further	O
reinforcement	B
learning	I
agents	O
can	O
discover	O
unexpected	O
ways	O
to	O
make	O
their	O
environments	O
deliver	O
reward	O
some	O
of	O
which	O
might	O
be	O
undesirable	O
or	O
even	O
dangerous	O
this	O
is	O
a	O
longstanding	O
and	O
critical	O
challenge	O
for	O
any	O
method	O
like	O
reinforcement	B
learning	I
that	O
is	O
based	O
on	O
optimization	O
we	O
discuss	O
this	O
issue	O
more	O
in	O
section	O
the	O
final	O
section	O
of	O
this	O
book	O
even	O
when	O
there	O
is	O
a	O
simple	O
and	O
easily	O
identifiable	O
goal	O
the	O
problem	O
of	O
sparse	B
reward	O
designing	O
reward	O
signals	O
often	O
arises	O
delivering	O
non-zero	O
reward	O
frequently	O
enough	O
to	O
allow	O
the	O
agent	O
to	O
achieve	O
the	O
goal	O
once	O
let	O
alone	O
to	O
learn	O
to	O
achieve	O
it	O
efficiently	O
from	O
multiple	O
initial	O
conditions	O
can	O
be	O
a	O
daunting	O
challenge	O
state	B
action	B
pairs	O
that	O
clearly	O
deserve	O
to	O
trigger	O
reward	O
may	O
be	O
few	O
and	O
far	O
between	O
and	O
rewards	O
that	O
mark	O
progress	O
toward	O
a	O
goal	O
can	O
be	O
infrequent	O
because	O
progress	O
is	O
difficult	O
or	O
even	O
impossible	O
to	O
detect	O
the	O
agent	O
may	O
wander	O
aimlessly	O
for	O
long	O
periods	O
of	O
time	O
minsky	B
called	O
the	O
plateau	O
problem	O
in	O
practice	O
designing	O
a	O
reward	B
signal	I
is	O
often	O
left	O
to	O
an	O
informal	O
trial-and-error	O
search	O
for	O
a	O
signal	O
that	O
produces	O
acceptable	O
results	O
if	O
the	O
agent	O
fails	O
to	O
learn	O
learns	O
too	O
slowly	O
or	O
learns	O
the	O
wrong	O
thing	O
then	O
the	O
designer	O
of	O
the	O
application	O
tweaks	O
the	O
reward	B
signal	I
and	O
tries	O
again	O
to	O
do	O
this	O
the	O
designer	O
judges	O
the	O
agent	O
s	O
performance	O
by	O
criteria	O
that	O
he	O
or	O
she	O
is	O
attempting	O
to	O
translate	O
into	O
a	O
reward	B
signal	I
so	O
that	O
the	O
agent	O
s	O
goal	O
matches	O
his	O
or	O
her	O
own	O
and	O
if	O
learning	O
is	O
too	O
slow	O
the	O
designer	O
may	O
try	O
to	O
design	O
a	O
non-sparse	O
reward	B
signal	I
that	O
effectively	O
guides	O
learning	O
throughout	O
the	O
agent	O
s	O
interaction	O
with	O
its	O
environment	B
it	O
is	O
tempting	O
to	O
address	O
the	O
sparse	B
reward	O
problem	O
by	O
rewarding	O
the	O
agent	O
for	O
achieving	O
subgoals	O
that	O
the	O
designer	O
thinks	O
are	O
important	O
way	O
stations	O
to	O
the	O
overall	O
goal	O
but	O
augmenting	O
the	O
reward	B
signal	I
with	O
well-intentioned	O
supplemental	O
rewards	O
may	O
lead	O
the	O
agent	O
to	O
behave	O
very	O
differently	O
from	O
what	O
is	O
intended	O
the	O
agent	O
may	O
end	O
up	O
not	O
achieving	O
the	O
overall	O
goal	O
at	O
all	O
a	O
better	O
way	O
to	O
provide	O
such	O
guidance	O
is	O
to	O
leave	O
the	O
reward	B
signal	I
alone	O
and	O
instead	O
augment	O
the	O
value-function	B
approximation	I
with	O
an	O
initial	O
guess	O
of	O
what	O
it	O
should	O
ultimately	O
be	O
or	O
augment	O
it	O
with	O
initial	O
guesses	O
as	O
to	O
what	O
certain	O
parts	O
of	O
it	O
should	O
be	O
for	O
example	O
suppose	O
one	O
wants	O
to	O
offer	O
s	O
r	O
as	O
an	O
initial	O
guess	O
at	O
the	O
true	O
optimal	O
value	B
function	I
v	O
and	O
that	O
one	O
is	O
using	O
linear	O
function	B
approximation	I
with	O
features	O
x	O
s	O
rd	O
then	O
one	O
would	O
define	O
the	O
initial	O
value	B
function	B
approximation	I
to	O
be	O
vsw	O
and	O
update	O
the	O
weights	O
w	O
as	O
usual	O
if	O
the	O
initial	O
weight	O
vector	B
is	O
then	O
the	O
initial	O
value	B
function	I
will	O
be	O
but	O
the	O
asymptotic	O
solution	O
quality	O
will	O
be	O
determined	O
by	O
the	O
feature	O
vectors	O
as	O
usual	O
this	O
initialization	O
can	O
be	O
done	O
for	O
arbitrary	O
nonlinear	O
approximators	O
and	O
arbitrary	O
forms	O
of	O
though	O
it	O
is	O
not	O
guaranteed	O
to	O
always	O
accelerate	O
learning	O
a	O
particularly	O
effective	O
approach	O
to	O
the	O
sparse	B
reward	O
problem	O
is	O
the	O
shaping	B
technique	O
introduced	O
by	O
the	O
psychologist	O
b	O
f	O
skinner	B
and	O
described	O
in	O
section	O
the	O
effectiveness	O
of	O
this	O
technique	O
relies	O
on	O
the	O
fact	O
that	O
sparse	B
reward	O
problems	O
are	O
not	O
just	O
problems	O
with	O
the	O
reward	B
signal	I
they	O
are	O
also	O
problems	O
with	O
an	O
agent	O
s	O
policy	B
in	O
preventing	O
the	O
agent	O
from	O
frequently	O
encountering	O
rewarding	O
states	O
shaping	B
involves	O
changing	O
the	O
reward	B
signal	I
as	O
learning	O
proceeds	O
starting	O
from	O
a	O
reward	B
signal	I
that	O
is	O
not	O
sparse	B
given	O
the	O
agent	O
s	O
initial	O
behavior	O
and	O
gradually	O
modifying	O
it	O
toward	O
the	O
reward	B
signal	I
suited	O
to	O
problem	O
of	O
original	O
interest	O
each	O
modification	O
is	O
made	O
so	O
that	O
the	O
agent	O
is	O
frequently	O
rewarded	O
given	O
its	O
current	O
behavior	O
the	O
agent	O
faces	O
a	O
sequence	O
of	O
increasingly-difficult	O
reinforcement	B
learning	I
problems	O
where	O
what	O
is	O
learned	O
at	O
each	O
stage	O
makes	O
the	O
next-harder	O
problem	O
relatively	O
easy	O
because	O
the	O
agent	O
now	O
encounters	O
reward	O
more	O
frequently	O
than	O
it	O
would	O
if	O
it	O
did	O
not	O
have	O
prior	O
experience	O
with	O
easier	O
problems	O
this	O
kind	O
of	O
shaping	B
is	O
an	O
essential	O
technique	O
in	O
training	O
animals	O
and	O
it	O
is	O
effective	O
in	O
computational	O
reinforcement	B
learning	I
as	O
well	O
chapter	O
frontiers	O
what	O
if	O
one	O
has	O
no	O
idea	O
what	O
the	O
rewards	O
should	O
be	O
but	O
there	O
is	O
another	O
agent	O
perhaps	O
a	O
person	O
who	O
is	O
already	O
expert	O
at	O
the	O
task	O
and	O
whose	O
behavior	O
can	O
be	O
observed	O
in	O
this	O
case	O
one	O
can	O
use	O
methods	O
known	O
variously	O
as	O
imitation	O
learning	O
learning	O
from	O
demonstration	O
and	O
apprenticeship	O
learning	O
the	O
idea	O
here	O
is	O
to	O
benefit	O
from	O
the	O
expert	O
agent	O
but	O
leave	O
open	O
the	O
possibility	O
of	O
eventually	O
performing	O
better	O
learning	O
from	O
an	O
expert	O
s	O
behavior	O
can	O
be	O
done	O
either	O
by	O
learning	O
directly	O
by	O
supervised	B
learning	I
or	O
by	O
extracting	O
a	O
reward	B
signal	I
using	O
what	O
is	O
known	O
as	O
inverse	B
reinforcement	B
learning	I
and	O
then	O
using	O
a	O
reinforcement	B
learning	I
algorithm	O
with	O
that	O
reward	B
signal	I
to	O
learn	O
a	O
policy	B
the	O
task	O
of	O
inverse	B
reinforcement	B
learning	I
as	O
explored	O
by	O
ng	O
and	O
russell	O
is	O
to	O
try	O
to	O
recover	O
the	O
expert	O
s	O
reward	B
signal	I
from	O
the	O
expert	O
s	O
behavior	O
alone	O
this	O
cannot	O
be	O
done	O
exactly	O
because	O
a	O
policy	B
can	O
be	O
optimal	O
with	O
respect	O
to	O
many	O
different	O
reward	O
signals	O
example	O
any	O
reward	B
signal	I
that	O
gives	O
the	O
same	O
reward	O
for	O
all	O
states	O
and	O
actions	O
but	O
it	O
is	O
possible	O
to	O
find	O
plausible	O
reward	B
signal	I
candidates	O
unfortunately	O
strong	O
assumptions	O
are	O
required	O
including	O
knowledge	O
of	O
the	O
environment	B
s	O
dynamics	O
and	O
of	O
the	O
feature	O
vectors	O
in	O
which	O
the	O
reward	B
signal	I
is	O
linear	O
the	O
method	O
also	O
requires	O
completely	O
solving	O
the	O
problem	O
by	O
dynamic	B
programming	I
methods	O
multiple	O
times	O
these	O
difficulties	O
notwithstanding	O
abbeel	O
and	O
ng	O
argue	O
that	O
the	O
inverse	B
reinforcement	B
learning	I
approach	O
can	O
sometimes	O
be	O
more	O
effective	O
than	O
supervised	B
learning	I
for	O
benefiting	O
from	O
the	O
behavior	O
of	O
an	O
expert	O
another	O
approach	O
to	O
finding	O
a	O
good	O
reward	B
signal	I
is	O
to	O
automate	O
the	O
trial-and-error	O
search	O
for	O
a	O
good	O
signal	O
that	O
we	O
mentioned	O
above	O
from	O
an	O
application	O
perspective	O
the	O
reward	B
signal	I
is	O
a	O
parameter	O
of	O
the	O
learning	O
algorithm	O
as	O
is	O
true	O
for	O
other	O
algorithm	O
parameters	O
the	O
search	O
for	O
a	O
good	O
reward	B
signal	I
can	O
be	O
automated	O
by	O
defining	O
a	O
space	O
of	O
feasible	O
candidates	O
and	O
applying	O
an	O
optimization	O
algorithm	O
the	O
optimization	O
algorithm	O
evaluates	O
each	O
candidate	O
reward	B
signal	I
by	O
running	O
the	O
reinforcement	B
learning	I
system	O
with	O
that	O
signal	O
for	O
some	O
number	O
of	O
steps	O
and	O
then	O
scoring	O
the	O
overall	O
result	O
by	O
a	O
high-level	O
objective	O
function	O
intended	O
to	O
encode	O
the	O
designer	O
s	O
true	O
goal	O
ignoring	O
the	O
limitations	O
of	O
the	O
agent	O
reward	O
signals	O
can	O
even	O
be	O
improved	O
via	O
online	B
gradient	B
ascent	O
where	O
the	O
gradient	B
is	O
that	O
of	O
the	O
high-level	O
objective	O
function	O
lewis	O
and	O
singh	O
relating	O
this	O
approach	O
to	O
the	O
natural	O
world	O
the	O
algorithm	O
for	O
optimizing	O
the	O
high-level	O
objective	O
function	O
is	O
analogous	O
to	O
evolution	B
where	O
the	O
high-level	O
objective	O
function	O
is	O
an	O
animal	O
s	O
evolutionary	O
fitness	O
determined	O
by	O
the	O
number	O
of	O
its	O
offspring	O
that	O
survive	O
to	O
reproductive	O
age	O
computational	O
experiments	O
with	O
this	O
bilevel	O
optimization	O
approach	O
one	O
level	O
analogous	O
to	O
evolution	B
and	O
the	O
other	O
due	O
to	O
reinforcement	B
learning	I
by	O
individual	O
agents	O
have	O
confirmed	O
that	O
intuition	O
alone	O
is	O
not	O
always	O
adequate	O
to	O
devise	O
a	O
good	O
reward	B
signal	I
lewis	O
and	O
barto	O
the	O
performance	O
of	O
a	O
reinforcement	B
learning	I
agent	O
as	O
evaluated	O
by	O
the	O
high-level	O
objective	O
function	O
can	O
be	O
very	O
sensitive	O
to	O
details	O
of	O
the	O
agent	O
s	O
reward	B
signal	I
in	O
subtle	O
ways	O
determined	O
by	O
the	O
agent	O
s	O
limitations	O
and	O
the	O
environments	O
in	O
which	O
it	O
acts	O
and	O
learns	O
these	O
experiments	O
also	O
demonstrated	O
that	O
an	O
agent	O
s	O
goal	O
should	O
not	O
always	O
be	O
the	O
same	O
as	O
the	O
goal	O
of	O
the	O
agent	O
s	O
designer	O
at	O
first	O
this	O
seems	O
counterintuitive	O
but	O
it	O
may	O
be	O
impossible	O
for	O
the	O
agent	O
to	O
achieve	O
the	O
designer	O
s	O
goal	O
no	O
matter	O
what	O
its	O
reward	B
signal	I
is	O
the	O
agent	O
has	O
to	O
learn	O
under	O
various	O
kinds	O
of	O
constraints	O
such	O
as	O
limited	O
computational	O
power	O
limited	O
access	O
to	O
infor	O
remaining	O
issues	O
mation	O
about	O
its	O
environment	B
or	O
limited	O
time	O
to	O
learn	O
when	O
there	O
are	O
constraints	O
like	O
these	O
learning	O
to	O
achieve	O
a	O
goal	O
that	O
is	O
different	O
from	O
the	O
designer	O
s	O
goal	O
can	O
sometimes	O
end	O
up	O
getting	O
closer	O
to	O
the	O
designer	O
s	O
goal	O
than	O
if	O
that	O
goal	O
were	O
pursued	O
directly	O
singh	O
and	O
lewis	O
sorg	O
examples	O
of	O
this	O
in	O
the	O
natural	O
world	O
are	O
easy	O
to	O
find	O
because	O
we	O
cannot	O
directly	O
assess	O
the	O
nutritional	O
value	B
of	O
most	O
foods	O
evolution	B
the	O
designer	O
of	O
our	O
reward	B
signal	I
gave	O
us	O
a	O
reward	B
signal	I
that	O
makes	O
us	O
seek	O
certain	O
tastes	O
though	O
certainly	O
not	O
infallible	O
possibly	O
detrimental	O
in	O
environments	O
that	O
differ	O
in	O
certain	O
ways	O
from	O
ancestral	O
environments	O
this	O
compensates	O
for	O
many	O
of	O
our	O
limitations	O
our	O
limited	O
sensory	O
abilities	O
the	O
limited	O
time	O
over	O
which	O
we	O
can	O
learn	O
and	O
the	O
risks	O
involved	O
in	O
finding	O
a	O
healthy	O
diet	O
through	O
personal	O
experimentation	O
similarly	O
because	O
an	O
animal	O
cannot	O
observe	O
its	O
own	O
evolutionary	O
fitness	O
that	O
objective	O
function	O
does	O
not	O
work	O
as	O
a	O
reward	B
signal	I
for	O
learning	O
evolution	B
instead	O
provides	O
reward	O
signals	O
that	O
are	O
sensitive	O
to	O
observable	O
predictors	O
of	O
evolutionary	O
fitness	O
finally	O
remember	O
that	O
a	O
reinforcement	B
learning	I
agent	O
is	O
not	O
necessarily	O
like	O
a	O
complete	O
organism	O
or	O
robot	O
it	O
can	O
be	O
a	O
component	O
of	O
a	O
larger	O
behaving	O
system	O
this	O
means	O
that	O
reward	O
signals	O
may	O
be	O
influenced	O
by	O
things	O
inside	O
the	O
larger	O
behaving	O
agent	O
such	O
as	O
motivational	O
states	O
memories	O
ideas	O
or	O
even	O
hallucinations	O
reward	O
signals	O
may	O
also	O
depend	O
on	O
properties	O
of	O
the	O
learning	O
process	O
itself	O
such	O
as	O
measures	O
of	O
how	O
much	O
progress	O
learning	O
is	O
making	O
making	O
reward	O
signals	O
sensitive	O
to	O
information	O
about	O
internal	O
factors	O
such	O
as	O
these	O
makes	O
it	O
possible	O
for	O
an	O
agent	O
to	O
learn	O
how	O
to	O
control	B
the	O
cognitive	O
architecture	O
of	O
which	O
it	O
is	O
a	O
part	O
as	O
well	O
as	O
to	O
acquire	O
knowledge	O
and	O
skills	O
that	O
would	O
be	O
difficult	O
to	O
learn	O
from	O
a	O
reward	B
signal	I
that	O
depended	O
only	O
on	O
external	O
events	O
possibilities	O
like	O
these	O
led	O
to	O
the	O
idea	O
of	O
intrinsically-motivated	O
reinforcement	B
learning	I
that	O
we	O
briefly	O
discuss	O
further	O
at	O
the	O
end	O
of	O
the	O
following	O
section	O
remaining	O
issues	O
in	O
this	O
book	O
we	O
have	O
presented	O
the	O
foundations	O
of	O
a	O
reinforcement	B
learning	I
approach	O
to	O
artificial	B
intelligence	I
roughly	O
speaking	O
that	O
approach	O
is	O
based	O
on	O
model-free	O
and	O
model-based	O
methods	O
working	O
together	O
as	O
in	O
the	O
dyna	B
architecture	I
of	O
chapter	O
combined	O
with	B
function	B
approximation	I
as	O
developed	O
in	O
part	O
ii	O
the	O
focus	O
has	O
been	O
on	O
online	B
and	O
incremental	O
algorithms	O
which	O
we	O
see	O
as	O
fundamental	O
even	O
to	O
model-based	O
methods	O
and	O
on	O
how	O
these	O
can	O
be	O
applied	O
in	O
off-policy	B
training	O
situations	O
the	O
full	O
rationale	O
for	O
the	O
latter	O
has	O
been	O
presented	O
only	O
in	O
this	O
last	O
chapter	O
that	O
is	O
we	O
have	O
all	O
along	O
presented	O
off-policy	B
learning	O
as	O
an	O
appealing	O
way	O
to	O
deal	O
with	O
the	O
exploreexploit	O
dilemma	O
but	O
only	O
in	O
this	O
chapter	O
have	O
we	O
discussed	O
learning	O
about	O
many	O
diverse	O
auxiliary	B
tasks	I
simultaneously	O
with	O
gvfs	O
and	O
learning	O
about	O
the	O
world	O
hierarchically	O
in	O
terms	O
of	O
temporally-abstract	O
option	B
models	I
both	O
of	O
which	O
involve	O
off-policy	B
learning	O
much	O
remains	O
to	O
be	O
worked	O
out	O
as	O
we	O
have	O
indicated	O
throughout	O
the	O
book	O
and	O
as	O
evidenced	O
by	O
the	O
directions	O
for	O
additional	O
research	O
discussed	O
in	O
this	O
chapter	O
but	O
suppose	O
we	O
are	O
generous	O
and	O
grant	O
the	O
broad	O
outlines	O
of	O
everything	O
that	O
we	O
have	O
done	O
in	O
the	O
book	O
and	O
everything	O
that	O
has	O
been	O
outlined	O
so	O
far	O
in	O
this	O
chapter	O
what	O
would	O
remain	O
even	O
after	O
that	O
of	O
course	O
we	O
can	O
t	O
know	O
for	O
sure	O
what	O
will	O
be	O
required	O
but	O
we	O
can	O
make	O
some	O
guesses	O
in	O
this	O
section	O
we	O
highlight	O
six	O
further	O
issues	O
which	O
it	O
seems	O
to	O
us	O
chapter	O
frontiers	O
will	O
still	O
need	O
to	O
be	O
addressed	O
by	O
future	O
research	O
first	O
we	O
still	O
need	O
powerful	O
parametric	O
function	B
approximation	I
methods	O
that	O
work	O
well	O
in	O
fully	O
incremental	O
and	O
online	B
settings	O
methods	O
based	O
on	O
deep	B
learning	I
and	O
artificial	B
neural	B
networks	I
are	O
a	O
major	O
step	O
in	O
this	O
direction	O
but	O
still	O
only	O
work	O
well	O
with	O
batch	O
training	O
on	O
large	O
data	O
sets	O
with	O
training	O
from	O
extensive	O
off-line	B
self	O
play	O
or	O
with	O
learning	O
from	O
the	O
interleaved	O
experience	O
of	O
multiple	O
agents	O
on	O
the	O
same	O
task	O
these	O
and	O
other	O
settings	O
are	O
ways	O
of	O
working	O
around	O
a	O
basic	O
limitation	O
of	O
today	O
s	O
deep	B
learning	I
methods	O
which	O
struggle	O
to	O
learn	O
rapidly	O
in	O
the	O
incremental	O
online	B
settings	O
that	O
are	O
most	O
natural	O
for	O
the	O
reinforcement	B
learning	I
algorithms	O
emphasized	O
in	O
this	O
book	O
the	O
problem	O
is	O
sometimes	O
described	O
as	O
one	O
of	O
catastrophic	B
interference	I
or	O
correlated	O
data	O
when	O
something	O
new	O
is	O
learned	O
it	O
tends	O
to	O
replace	O
what	O
has	O
previously	O
been	O
learned	O
rather	O
than	O
adding	O
to	O
it	O
with	O
the	O
result	O
that	O
the	O
benefit	O
of	O
the	O
older	O
learning	O
is	O
lost	O
techniques	O
such	O
as	O
replay	O
buffers	O
are	O
often	O
used	O
to	O
retain	O
and	O
replay	O
old	O
data	O
so	O
that	O
its	O
benefits	O
are	O
not	O
permanently	O
lost	O
an	O
honest	O
assessment	O
has	O
to	O
be	O
that	O
current	O
deep	B
learning	I
methods	O
are	O
not	O
well	O
suited	O
to	O
online	B
learning	O
we	O
see	O
no	O
reason	O
that	O
this	O
limitation	O
is	O
insurmountable	O
but	O
algorithms	O
that	O
address	O
it	O
while	O
at	O
the	O
same	O
time	O
retaining	O
the	O
advantages	B
of	I
deep	B
learning	I
have	O
not	O
yet	O
been	O
devised	O
most	O
current	O
deep	B
learning	I
research	O
is	O
directed	O
toward	O
working	O
around	O
this	O
limitation	O
rather	O
than	O
removing	O
it	O
second	O
perhaps	O
closely	O
related	O
we	O
still	O
need	O
methods	O
for	O
learning	O
features	O
such	O
that	O
subsequent	O
learning	O
generalizes	O
well	O
this	O
issue	O
is	O
an	O
instance	O
of	O
a	O
general	O
problem	O
variously	O
called	O
representation	B
learning	I
constructive	O
induction	O
and	O
metalearning	O
how	O
can	O
we	O
use	O
experience	O
not	O
just	O
to	O
learn	O
a	O
given	O
desired	O
function	O
but	O
to	O
learn	O
inductive	O
biases	O
such	O
that	O
future	O
learning	O
generalizes	O
better	O
and	O
is	O
thus	O
faster	O
this	O
is	O
an	O
old	O
problem	O
dating	O
back	O
to	O
the	O
origins	O
of	O
artificial	B
intelligence	I
and	O
pattern	O
recognition	O
in	O
the	O
and	O
such	O
age	O
should	O
give	O
one	O
pause	O
perhaps	O
there	O
is	O
no	O
solution	O
but	O
it	O
is	O
equally	O
likely	O
that	O
the	O
time	O
for	O
finding	O
a	O
solution	O
and	O
demonstrating	O
its	O
effectiveness	O
has	O
not	O
yet	O
arrived	O
today	O
machine	O
learning	O
is	O
conducted	O
at	O
a	O
far	O
larger	O
scale	O
than	O
it	O
has	O
been	O
in	O
the	O
past	O
and	O
the	O
potential	O
benefits	O
of	O
a	O
good	O
representation	B
learning	I
method	O
have	O
become	O
much	O
more	O
apparent	O
we	O
note	O
that	O
a	O
new	O
annual	O
conference	O
the	O
international	O
conference	O
on	O
learning	O
representations	O
has	O
been	O
exploring	O
this	O
and	O
related	O
topics	O
every	O
year	O
since	O
it	O
is	O
also	O
less	O
common	O
to	O
explore	O
representation	B
learning	I
within	O
a	O
reinforcement	B
learning	I
context	O
reinforcement	B
learning	I
brings	O
some	O
new	O
possibilities	O
to	O
this	O
old	O
issue	O
such	O
as	O
the	O
auxiliary	B
tasks	I
discussed	O
in	O
section	O
in	O
reinforcement	B
learning	I
the	O
problem	O
of	O
representation	B
learning	I
can	O
be	O
identified	O
with	O
the	O
problem	O
of	O
learning	O
the	O
state-update	B
function	I
discussed	O
in	O
section	O
third	O
we	O
still	O
need	O
scalable	O
methods	O
for	O
planning	B
with	O
learned	O
environment	B
models	O
planning	B
methods	O
have	O
proven	O
extremely	O
effective	O
in	O
applications	O
such	O
as	O
alphago	B
zero	O
and	O
computer	O
chess	B
in	O
which	O
the	O
model	B
of	I
the	I
environment	B
is	O
known	O
from	O
the	O
rules	O
of	O
the	O
game	O
or	O
can	O
otherwise	O
be	O
supplied	O
by	O
human	O
designers	O
but	O
cases	O
of	O
full	O
model-based	B
reinforcement	B
learning	I
in	O
which	O
the	O
environment	B
model	O
is	O
learned	O
from	O
data	O
and	O
then	O
used	O
for	O
planning	B
are	O
rare	O
the	O
dyna	O
system	O
described	O
in	O
chapter	O
is	O
one	O
example	O
but	O
would	O
claim	O
that	O
deep	B
learning	I
solves	O
this	O
problem	O
for	O
example	O
that	O
dqn	O
as	O
described	O
in	O
section	O
illustrates	O
a	O
solution	O
but	O
we	O
are	O
unconvinced	O
there	O
is	O
as	O
yet	O
little	O
evidence	O
that	O
deep	B
learning	I
alone	O
solves	O
the	O
representation	B
learning	I
problem	O
in	O
a	O
general	O
and	O
efficient	O
way	O
remaining	O
issues	O
as	O
described	O
there	O
and	O
in	O
most	O
subsequent	O
work	O
it	O
uses	O
a	O
tabular	O
model	O
without	O
function	B
approximation	I
which	O
greatly	O
limits	O
its	O
applicability	O
only	O
a	O
few	O
studies	O
have	O
included	O
learned	O
linear	O
models	O
and	O
even	O
fewer	O
have	O
also	O
explored	O
including	O
temporally-abstract	O
models	O
using	O
options	B
as	O
discussed	O
in	O
section	O
more	O
work	O
is	O
needed	O
before	O
planning	B
with	B
learned	I
models	I
can	O
be	O
effective	O
for	O
example	O
the	O
learning	O
of	O
the	O
model	O
needs	O
to	O
be	O
selective	O
because	O
the	O
scope	O
of	O
a	O
model	O
strongly	O
affects	O
planning	B
efficiency	O
if	O
a	O
model	O
focuses	O
on	O
the	O
key	O
consequences	O
of	O
the	O
most	O
important	O
options	B
then	O
planning	B
can	O
be	O
efficient	O
and	O
rapid	O
but	O
if	O
a	O
model	O
includes	O
details	O
of	O
unimportant	O
consequences	O
of	O
options	B
that	O
are	O
unlikely	O
to	O
be	O
selected	O
then	O
planning	B
may	O
be	O
almost	O
useless	O
environment	B
models	O
should	O
be	O
constructed	O
judiciously	O
with	O
regard	O
to	O
both	O
their	O
states	O
and	O
dynamics	O
with	O
the	O
goal	O
of	O
optimizing	O
the	O
planning	B
process	O
the	O
various	O
parts	O
of	O
the	O
model	O
should	O
be	O
continually	O
monitored	O
as	O
to	O
the	O
degree	O
to	O
which	O
they	O
contribute	O
to	O
or	O
detract	O
from	O
planning	B
efficiency	O
the	O
field	O
has	O
not	O
yet	O
addressed	O
this	O
complex	O
of	O
issues	O
or	O
designed	O
model-learning	O
methods	O
that	O
take	O
into	O
account	O
their	O
implications	O
a	O
fourth	O
issue	O
that	O
needs	O
to	O
be	O
addressed	O
in	O
future	O
research	O
is	O
that	O
of	O
automating	O
the	O
choice	O
of	O
tasks	O
on	O
which	O
an	O
agent	O
works	O
and	O
uses	O
to	O
structure	O
its	O
developing	O
competence	O
it	O
is	O
usual	O
in	O
machine	O
learning	O
for	O
human	O
designers	O
to	O
set	O
the	O
tasks	O
that	O
the	O
learning	O
agent	O
is	O
expected	B
to	O
master	O
because	O
these	O
tasks	O
are	O
known	O
in	O
advance	O
and	O
remain	O
fixed	O
they	O
can	O
be	O
built	O
into	O
the	O
learning	O
algorithm	O
code	O
however	O
looking	O
ahead	O
we	O
will	O
want	O
the	O
agent	O
to	O
make	O
its	O
own	O
choices	O
about	O
what	O
tasks	O
it	O
should	O
try	O
to	O
master	O
these	O
might	O
be	O
subtasks	O
of	O
a	O
specific	O
overall	O
task	O
that	O
is	O
already	O
known	O
or	O
they	O
might	O
be	O
intended	O
to	O
create	O
building	O
blocks	O
that	O
permit	O
more	O
efficient	O
learning	O
of	O
many	O
different	O
tasks	O
that	O
the	O
agent	O
is	O
likely	O
to	O
face	O
in	O
the	O
future	O
but	O
which	O
are	O
currently	O
unknown	O
these	O
tasks	O
may	O
be	O
like	O
the	O
auxiliary	B
tasks	I
or	O
the	O
gvfs	O
discussed	O
in	O
section	O
or	O
tasks	O
solved	O
by	O
options	B
as	O
discussed	O
in	O
section	O
in	O
forming	O
a	O
gvf	O
for	O
example	O
what	O
should	O
the	O
cumulant	O
the	O
policy	B
and	O
the	O
termination	O
function	O
be	O
the	O
current	O
state	B
of	O
the	O
art	O
is	O
to	O
select	O
these	O
manually	O
but	O
far	O
greater	O
power	O
and	O
generality	O
would	O
come	O
from	O
making	O
these	O
task	O
choices	O
automatically	O
particularly	O
when	O
they	O
derive	O
from	O
what	O
the	O
agent	O
has	O
previously	O
constructed	O
as	O
a	O
result	O
of	O
representation	B
learning	I
or	O
experience	O
with	O
previous	O
subproblems	O
if	O
gvf	O
design	O
is	O
automated	O
then	O
the	O
design	O
choices	O
themselves	O
will	O
have	O
to	O
be	O
explicitly	O
represented	O
rather	O
than	O
the	O
task	O
choices	O
being	O
in	O
the	O
mind	O
of	O
the	O
designer	O
and	O
built	O
into	O
the	O
code	O
they	O
will	O
have	O
to	O
be	O
in	O
the	O
machine	O
itself	O
in	O
such	O
a	O
way	O
that	O
they	O
can	O
be	O
set	O
and	O
changed	O
monitored	O
filtered	O
and	O
searched	O
among	O
automatically	O
tasks	O
could	O
then	O
be	O
built	O
hierarchically	O
upon	O
others	O
much	O
like	O
features	O
are	O
in	O
an	O
artificial	O
neural	B
network	O
the	O
tasks	O
are	O
the	O
questions	O
and	O
the	O
contents	O
of	O
the	O
neural	B
network	O
are	O
the	O
answers	O
to	O
those	O
questions	O
we	O
expect	O
there	O
will	O
need	O
to	O
be	O
a	O
full	O
hierarchy	O
of	O
questions	O
to	O
match	O
the	O
hierarchy	O
of	O
answers	O
provided	O
by	O
modern	O
deep	B
learning	I
methods	O
the	O
fifth	O
issue	O
that	O
we	O
would	O
like	O
to	O
highlight	O
for	O
future	O
research	O
is	O
that	O
of	O
the	O
interaction	O
between	O
behavior	O
and	O
learning	O
via	O
some	O
computational	O
analog	O
of	O
curiosity	B
in	O
this	O
chapter	O
we	O
have	O
been	O
imagining	O
a	O
setting	O
in	O
which	O
many	O
tasks	O
are	O
being	O
learned	O
simultaneously	O
using	O
off-policy	B
methods	I
from	O
the	O
same	O
stream	O
of	O
experience	O
the	O
actions	O
taken	O
will	O
of	O
course	O
influence	O
this	O
stream	O
of	O
experience	O
which	O
in	O
turn	O
will	O
determine	O
chapter	O
frontiers	O
how	O
much	O
learning	O
occurs	O
and	O
which	O
tasks	O
are	O
learned	O
when	O
reward	O
is	O
not	O
available	O
or	O
not	O
strongly	O
influenced	O
by	O
behavior	O
the	O
agent	O
is	O
free	O
to	O
choose	O
actions	O
that	O
maximize	O
in	O
some	O
sense	O
the	O
learning	O
on	O
the	O
tasks	O
that	O
is	O
to	O
use	O
some	O
measure	O
of	O
learning	O
progress	O
as	O
an	O
internal	O
or	O
intrinsic	B
reward	O
implementing	O
a	O
computational	O
form	O
of	O
curiosity	B
in	O
addition	O
to	O
measuring	O
learning	O
progress	O
intrinsic	B
reward	O
can	O
among	O
other	O
possibilities	O
signal	O
the	O
receipt	O
of	O
unexpected	O
novel	O
or	O
otherwise	O
interesting	O
input	O
or	O
can	O
assess	O
the	O
agent	O
s	O
ability	O
to	O
cause	O
changes	O
in	O
its	O
environment	B
intrinsic	B
reward	O
signals	O
generated	O
in	O
these	O
ways	O
can	O
be	O
used	O
by	O
an	O
agent	O
to	O
pose	O
tasks	O
for	O
itself	O
by	O
defining	O
auxiliary	B
tasks	I
gvfs	O
or	O
options	B
as	O
discussed	O
above	O
so	O
that	O
skills	O
learned	O
in	O
this	O
way	O
can	O
contribute	O
to	O
the	O
agent	O
s	O
ability	O
to	O
master	O
future	O
tasks	O
the	O
result	O
is	O
a	O
computational	O
analog	O
of	O
something	O
like	O
play	O
many	O
preliminary	O
studies	O
of	O
such	O
uses	O
of	O
intrinsic	B
reward	O
signals	O
have	O
been	O
conducted	O
and	O
exciting	O
topics	O
for	O
future	O
research	O
remain	O
in	O
this	O
general	O
area	O
a	O
final	O
issue	O
that	O
demands	O
attention	O
in	O
future	O
research	O
is	O
that	O
of	O
developing	O
methods	O
to	O
make	O
it	O
acceptably	O
safe	O
to	O
embed	O
reinforcement	B
learning	I
agents	O
into	O
physical	O
environments	O
thereby	O
helping	O
to	O
ensure	O
that	O
the	O
benefits	O
of	O
reinforcement	B
learning	I
outweigh	O
harm	O
it	O
can	O
cause	O
this	O
is	O
one	O
of	O
the	O
most	O
pressing	O
areas	O
for	O
future	O
research	O
and	O
we	O
discuss	O
it	O
further	O
in	O
the	O
following	O
section	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
arti	O
ficial	O
intelligence	O
when	O
we	O
were	O
writing	O
the	O
first	O
edition	O
of	O
this	O
book	O
in	O
the	O
artificial	B
intelligence	I
was	O
making	O
significant	O
progress	O
and	O
was	O
having	O
an	O
impact	O
on	O
society	O
though	O
it	O
was	O
mostly	O
still	O
the	O
promise	O
of	O
artificial	B
intelligence	I
that	O
was	O
inspiring	O
developments	O
machine	O
learning	O
was	O
part	O
of	O
that	O
outlook	O
but	O
it	O
had	O
not	O
yet	O
become	O
indispensable	O
to	O
artificial	B
intelligence	I
by	O
today	O
that	O
promise	O
has	O
transitioned	O
to	O
applications	O
that	O
are	O
changing	O
the	O
lives	O
of	O
millions	O
of	O
people	O
and	O
machine	O
learning	O
has	O
come	O
into	O
its	O
own	O
as	O
a	O
key	O
technology	O
as	O
we	O
write	O
this	O
second	O
edition	O
some	O
of	O
the	O
most	O
remarkable	O
developments	O
in	O
artificial	B
intelligence	I
have	O
involved	O
reinforcement	B
learning	I
most	O
notably	O
deep	B
reinforcement	B
learning	I
reinforcement	B
learning	I
with	B
function	B
approximation	I
by	O
deep	O
neural	B
networks	O
we	O
are	O
at	O
the	O
beginning	O
of	O
a	O
wave	O
of	O
real-world	O
applications	O
of	O
artificial	B
intelligence	I
many	O
of	O
which	O
will	O
include	O
reinforcement	B
learning	I
deep	O
and	O
otherwise	O
that	O
will	O
impact	O
our	O
lives	O
in	O
ways	O
that	O
are	O
hard	O
to	O
predict	O
but	O
an	O
abundance	O
of	O
successful	O
real-world	O
applications	O
does	O
not	O
mean	O
that	O
true	O
artificial	B
intelligence	I
has	O
arrived	O
despite	O
great	O
progress	O
in	O
many	O
areas	O
the	O
gulf	O
between	O
artificial	B
intelligence	I
and	O
the	O
intelligence	O
of	O
humans	O
and	O
even	O
of	O
other	O
animals	O
remains	O
great	O
superhuman	O
performance	O
can	O
be	O
achieved	O
in	O
some	O
domains	O
even	O
formidable	O
domains	O
like	O
go	O
but	O
it	O
remains	O
a	O
significant	O
challenge	O
to	O
develop	O
systems	O
that	O
are	O
like	O
us	O
in	O
being	O
complete	O
interactive	O
agents	O
having	O
general	O
adaptability	O
and	O
problem-solving	O
skills	O
emotional	O
sophistication	O
creativity	O
and	O
the	O
ability	O
to	O
learn	O
quickly	O
from	O
experience	O
with	O
its	O
focus	O
on	O
learning	O
by	O
interacting	O
with	O
dynamic	O
environments	O
reinforcement	B
learning	I
as	O
it	O
develops	O
over	O
the	O
future	O
will	O
be	O
a	O
critical	O
component	O
of	O
agents	O
with	O
these	O
abilities	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artificial	B
intelligence	I
reinforcement	B
learning	I
s	O
connections	O
to	O
psychology	B
and	O
neuroscience	B
and	O
underscore	O
its	O
relevance	O
to	O
another	O
longstanding	O
goal	O
of	O
artificial	B
intelligence	I
shedding	O
light	O
on	O
fundamental	O
questions	O
about	O
the	O
mind	O
and	O
how	O
it	O
emerges	O
from	O
the	O
brain	O
reinforcement	B
learning	I
theory	O
is	O
already	O
contributing	O
to	O
our	O
understanding	O
of	O
the	O
brain	O
s	O
reward	O
motivation	B
and	O
decision-making	O
processes	O
and	O
there	O
is	O
good	O
reason	O
to	O
believe	O
that	O
through	O
its	O
links	O
to	O
computational	O
psychiatry	O
reinforcement	B
learning	I
theory	O
will	O
contribute	O
to	O
methods	O
for	O
treating	O
mental	O
disorders	O
including	O
drug	O
abuse	O
and	B
addiction	B
another	O
contribution	O
that	O
reinforcement	B
learning	I
can	O
make	O
over	O
the	O
future	O
is	O
as	O
an	O
aid	O
to	O
human	O
decision	O
making	O
policies	O
derived	O
by	O
reinforcement	B
learning	I
in	O
simulated	O
environments	O
can	O
advise	O
human	O
decision	O
makers	O
in	O
such	O
areas	O
as	O
education	O
healthcare	O
transportation	O
energy	O
and	O
public-sector	O
resource	O
allocation	O
particularly	O
relevant	O
is	O
the	O
key	O
feature	O
of	O
reinforcement	B
learning	I
that	O
it	O
takes	O
long-term	O
consequences	O
of	O
decisions	O
into	O
account	O
this	O
is	O
very	O
clear	O
in	O
games	O
like	O
backgammon	B
and	O
go	O
where	O
some	O
of	O
the	O
most	O
impressive	O
results	O
of	O
reinforcement	B
learning	I
have	O
been	O
demonstrated	O
but	O
it	O
is	O
also	O
a	O
property	O
of	O
many	O
high-stakes	O
decisions	O
that	O
affect	O
our	O
lives	O
and	O
our	O
planet	O
reinforcement	B
learning	I
follows	O
related	O
methods	O
for	O
advising	O
human	O
decision	O
making	O
that	O
have	O
been	O
developed	O
in	O
the	O
past	O
by	O
decision	O
analysts	O
in	O
many	O
disciplines	O
with	O
advanced	O
function	B
approximation	I
methods	O
and	O
massive	O
computational	O
power	O
reinforcement	B
learning	I
methods	O
have	O
the	O
potential	O
to	O
overcome	O
some	O
of	O
the	O
difficulties	O
of	O
scaling	O
up	O
traditional	O
decision-support	O
methods	O
to	O
larger	O
and	O
more	O
complex	O
problems	O
the	O
rapid	O
pace	O
of	O
advances	O
in	O
artificial	B
intelligence	I
has	O
led	O
to	O
warnings	O
that	O
artificial	B
intelligence	I
poses	O
serious	O
threats	O
to	O
our	O
societies	O
even	O
to	O
humanity	O
itself	O
the	O
renowned	O
scientist	O
and	B
artificial	B
intelligence	I
pioneer	O
herbert	O
simon	O
anticipated	O
the	O
warnings	O
we	O
are	O
hearing	O
today	O
in	O
a	O
presentation	O
at	O
the	O
earthware	O
symposium	O
at	O
cmu	O
in	O
he	O
spoke	O
of	O
the	O
eternal	O
conflict	O
between	O
the	O
promise	O
and	O
perils	O
of	O
any	O
new	O
knowledge	O
reminding	O
us	O
of	O
the	O
greek	O
myths	O
of	O
prometheus	O
the	O
hero	O
of	O
modern	O
science	O
who	O
stole	O
fire	O
from	O
the	O
gods	O
for	O
the	O
benefit	O
of	O
mankind	O
and	O
pandora	O
whose	O
box	O
could	O
be	O
opened	O
by	O
a	O
small	O
and	O
innocent	O
action	B
to	O
release	O
untold	O
perils	O
on	O
the	O
world	O
while	O
accepting	O
that	O
this	O
conflict	O
is	O
inevitable	O
simon	O
urged	O
us	O
to	O
recognize	O
that	O
as	O
designers	O
of	O
our	O
future	O
and	O
not	O
mere	O
spectators	O
the	O
decisions	O
we	O
make	O
can	O
tilt	O
the	O
scale	O
in	O
prometheus	O
favor	O
this	O
is	O
certainly	O
true	O
for	O
reinforcement	B
learning	I
which	O
can	O
benefit	O
society	O
but	O
can	O
also	O
produce	O
undesirable	O
outcomes	O
if	O
it	O
is	O
carelessly	O
deployed	O
thus	O
the	O
safety	O
of	O
artificial	B
intelligence	I
applications	O
involving	O
reinforcement	B
learning	I
is	O
a	O
topic	O
that	O
deserves	O
careful	O
attention	O
a	O
reinforcement	B
learning	I
agent	O
can	O
learn	O
by	O
interacting	O
with	O
either	O
the	O
real	O
world	O
or	O
with	O
a	O
simulation	O
of	O
some	O
piece	O
of	O
the	O
real	O
world	O
or	O
by	O
a	O
mixture	O
of	O
these	O
two	O
sources	O
of	O
experience	O
simulators	O
provide	O
safe	O
environments	O
in	O
which	O
an	O
agent	O
can	O
explore	O
and	O
learn	O
without	O
risking	O
real	O
damage	O
to	O
itself	O
or	O
to	O
its	O
environment	B
in	O
most	O
current	O
applications	O
policies	O
are	O
learned	O
from	O
simulated	O
experience	O
instead	O
of	O
direct	O
interaction	O
with	O
the	O
real	O
world	O
in	O
addition	O
to	O
avoiding	O
undesirable	O
real-world	O
consequences	O
learning	O
from	O
simulated	O
experience	O
can	O
make	O
virtually	O
unlimited	O
data	O
available	O
for	O
learning	O
generally	O
at	O
less	O
cost	O
than	O
needed	O
to	O
obtain	O
real	O
experience	O
and	O
because	O
simulations	O
typically	O
run	O
much	O
faster	O
than	O
real	O
time	O
learning	O
can	O
often	O
occur	O
more	O
quickly	O
than	O
if	O
it	O
relied	O
on	O
real	O
experience	O
chapter	O
frontiers	O
nevertheless	O
the	O
full	O
potential	O
of	O
reinforcement	B
learning	I
requires	O
reinforcement	B
learning	I
agents	O
to	O
be	O
embedded	O
into	O
the	O
flow	O
of	O
real-world	O
experience	O
where	O
they	O
act	O
explore	O
and	O
learn	O
in	O
our	O
world	O
and	O
not	O
just	O
in	O
their	O
worlds	O
after	O
all	O
reinforcement	B
learning	I
algorithms	O
at	O
least	O
those	O
upon	O
which	O
we	O
focus	O
in	O
this	O
book	O
are	O
designed	O
to	O
learn	O
online	B
and	O
they	O
emulate	O
many	O
aspects	O
of	O
how	O
animals	O
are	O
able	O
to	O
survive	O
in	O
nonstationary	O
and	O
hostile	O
environments	O
embedding	O
reinforcement	B
learning	I
agents	O
in	O
the	O
real	O
world	O
can	O
be	O
transformative	O
in	O
realizing	O
the	O
promises	O
of	O
artificial	B
intelligence	I
to	O
amplify	O
and	O
extend	O
human	O
abilities	O
a	O
major	O
reason	O
for	O
wanting	O
a	O
reinforcement	B
learning	I
agent	O
to	O
act	O
and	O
learn	O
in	O
the	O
real	O
world	O
is	O
that	O
it	O
is	O
often	O
difficult	O
sometimes	O
impossible	O
to	O
simulate	O
real-world	O
experience	O
with	O
enough	O
fidelity	O
to	O
make	O
the	O
resulting	O
policies	O
whether	O
derived	O
by	O
reinforcement	B
learning	I
or	O
by	O
other	O
methods	O
work	O
well	O
and	O
safely	O
when	O
directing	O
real	O
actions	O
this	O
is	O
especially	O
true	O
for	O
environments	O
whose	O
dynamics	O
depend	O
on	O
the	O
behavior	O
of	O
humans	O
such	O
as	O
in	O
education	O
healthcare	O
transportation	O
and	O
public	O
policy	B
domains	O
that	O
can	O
surely	O
benefit	O
from	O
improved	O
decision	O
making	O
however	O
it	O
is	O
for	O
real-world	O
embedded	O
agents	O
that	O
warnings	O
about	O
potential	O
dangers	O
of	O
artificial	B
intelligence	I
need	O
to	O
be	O
heeded	O
some	O
of	O
these	O
warnings	O
are	O
particularly	O
relevant	O
to	O
reinforcement	B
learning	I
because	O
reinforcement	B
learning	I
is	O
based	O
on	O
optimization	O
it	O
inherits	O
the	O
pluses	O
and	O
minuses	O
of	O
all	O
optimization	O
methods	O
on	O
the	O
minus	O
side	O
is	O
the	O
problem	O
of	O
devising	O
objective	O
functions	O
or	O
reward	O
signals	O
in	O
the	O
case	O
of	O
reinforcement	B
learning	I
so	O
that	O
optimization	O
produces	O
the	O
desired	O
results	O
while	O
avoiding	O
undesirable	O
results	O
we	O
said	O
in	O
section	O
that	O
reinforcement	B
learning	I
agents	O
can	O
discover	O
unexpected	O
ways	O
to	O
make	O
their	O
environments	O
deliver	O
reward	O
some	O
of	O
which	O
might	O
be	O
undesirable	O
or	O
even	O
dangerous	O
when	O
we	O
specify	O
what	O
we	O
want	O
a	O
system	O
to	O
learn	O
only	O
indirectly	O
as	O
we	O
do	O
in	O
designing	O
a	O
reinforcement	B
learning	I
system	O
s	O
reward	B
signal	I
we	O
will	O
not	O
know	O
how	O
closely	O
the	O
agent	O
will	O
fulfill	O
our	O
desire	O
until	O
its	O
learning	O
is	O
complete	O
this	O
is	O
hardly	O
a	O
new	O
problem	O
with	O
reinforcement	B
learning	I
recognition	O
of	O
it	O
has	O
a	O
long	O
history	O
in	O
both	O
literature	O
and	O
engineering	O
for	O
example	O
in	O
goethe	O
s	O
poem	O
the	O
sorcerer	O
s	O
apprentice	O
the	O
apprentice	O
uses	O
magic	O
to	O
enchant	O
a	O
broom	O
to	O
do	O
his	O
job	O
of	O
fetching	O
water	O
but	O
the	O
result	O
is	O
an	O
unintended	O
flood	O
due	O
to	O
the	O
apprentice	O
s	O
inadequate	O
knowledge	O
of	O
magic	O
in	O
the	O
engineering	O
context	O
norbert	O
wiener	O
the	O
founder	O
of	O
cybernetics	B
warned	O
of	O
this	O
problem	O
more	O
than	O
half	O
a	O
century	O
ago	O
by	O
relating	O
the	O
supernatural	O
story	O
of	O
the	O
monkey	O
s	O
paw	O
it	O
grants	O
what	O
you	O
ask	O
for	O
not	O
what	O
you	O
should	O
have	O
asked	O
for	O
or	O
what	O
you	O
intend	O
the	O
problem	O
has	O
also	O
been	O
discussed	O
at	O
length	O
in	O
a	O
modern	O
context	O
by	O
nick	O
bostrom	O
anyone	O
having	O
experience	O
with	O
reinforcement	B
learning	I
has	O
likely	O
seen	O
their	O
systems	O
discover	O
unexpected	O
ways	O
to	O
obtain	O
a	O
lot	O
of	O
reward	O
sometimes	O
the	O
unexpected	O
behavior	O
is	O
good	O
it	O
solves	O
a	O
problem	O
in	O
a	O
nice	O
new	O
way	O
in	O
other	O
instances	O
what	O
the	O
agent	O
learns	O
violates	O
considerations	O
that	O
the	O
system	O
designer	O
may	O
never	O
have	O
thought	O
about	O
careful	O
design	B
of	I
reward	O
signals	O
is	O
essential	O
if	O
an	O
agent	O
is	O
to	O
act	O
in	O
the	O
real	O
world	O
with	O
no	O
opportunity	O
for	O
human	O
vetting	O
of	O
its	O
actions	O
or	O
means	O
to	O
easily	O
interrupt	O
its	O
behavior	O
despite	O
the	O
possibility	O
of	O
unintended	O
negative	O
consequences	O
optimization	O
has	O
been	O
used	O
for	O
hundreds	O
of	O
years	O
by	O
engineers	O
architects	O
and	O
others	O
whose	O
designs	O
have	O
posi	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artificial	B
intelligence	I
tively	O
impacted	O
the	O
world	O
we	O
owe	O
much	O
that	O
is	O
good	O
in	O
our	O
environment	B
to	O
the	O
application	O
of	O
optimization	O
methods	O
many	O
approaches	O
have	O
been	O
developed	O
to	O
mitigate	O
the	O
risk	O
of	O
optimization	O
such	O
as	O
adding	O
hard	O
and	O
soft	O
constraints	O
restricting	O
optimization	O
to	O
robust	O
and	O
risk-sensitive	O
policies	O
and	O
optimizing	O
with	O
multiple	O
objective	O
functions	O
some	O
of	O
these	O
approaches	O
have	O
been	O
adapted	O
to	O
reinforcement	B
learning	I
and	O
more	O
research	O
is	O
needed	O
to	O
address	O
these	O
concerns	O
the	O
problem	O
of	O
ensuring	O
that	O
a	O
reinforcement	B
learning	I
agent	O
s	O
goal	O
is	O
attuned	O
to	O
our	O
own	O
remains	O
a	O
challenge	O
another	O
challenge	O
if	O
reinforcement	B
learning	I
agents	O
are	O
to	O
act	O
and	O
learn	O
in	O
the	O
real	O
world	O
is	O
not	O
just	O
about	O
what	O
they	O
might	O
learn	O
eventually	O
but	O
about	O
how	O
they	O
will	O
behave	O
while	O
they	O
are	O
learning	O
how	O
do	O
you	O
make	O
sure	O
that	O
an	O
agent	O
gets	O
enough	O
experience	O
to	O
learn	O
a	O
high-performing	O
policy	B
all	O
the	O
while	O
not	O
harming	O
its	O
environment	B
other	O
agents	O
or	O
itself	O
more	O
realistically	O
while	O
keeping	O
the	O
probability	O
of	O
harm	O
acceptably	O
low	O
this	O
problem	O
is	O
also	O
not	O
novel	O
or	O
unique	O
to	O
reinforcement	B
learning	I
risk	O
management	O
and	O
mitigation	O
for	O
embedded	O
reinforcement	B
learning	I
is	O
similar	O
to	O
what	O
control	B
engineers	O
have	O
had	O
to	O
confront	O
from	O
the	O
beginning	O
of	O
using	O
automatic	O
control	B
in	O
situations	O
where	O
a	O
controller	O
s	O
behavior	O
can	O
have	O
unacceptable	O
possibly	O
catastrophic	O
consequences	O
as	O
in	O
the	O
control	B
of	O
an	O
aircraft	O
or	O
a	O
delicate	O
chemical	O
process	O
control	B
applications	O
rely	O
on	O
careful	O
system	O
modeling	O
model	O
validation	O
and	O
extensive	O
testing	O
and	O
there	O
is	O
a	O
highly-developed	O
body	O
of	O
theory	O
aimed	O
at	O
ensuring	O
convergence	O
and	B
stability	I
of	O
adaptive	O
controllers	O
designed	O
for	O
use	O
when	O
the	O
dynamics	O
of	O
the	O
system	O
to	O
be	O
controlled	O
are	O
not	O
fully	O
known	O
theoretical	O
guarantees	O
are	O
never	O
iron-clad	O
because	O
they	O
depend	O
on	O
the	O
validity	O
of	O
the	O
assumptions	O
underlying	O
the	O
mathematics	O
but	O
without	O
this	O
theory	O
combined	O
with	O
risk-management	O
and	O
mitigation	O
practices	O
automatic	O
control	B
adaptive	O
and	O
otherwise	O
would	O
not	O
be	O
as	O
beneficial	O
as	O
it	O
is	O
today	O
in	O
improving	O
the	O
quality	O
efficiency	O
and	O
cost-effectiveness	O
of	O
processes	O
on	O
which	O
we	O
have	O
come	O
to	O
rely	O
one	O
of	O
the	O
most	O
pressing	O
areas	O
for	O
future	O
reinforcement	B
learning	I
research	O
is	O
to	O
adapt	O
and	O
extend	O
methods	O
developed	O
in	O
control	B
engineering	O
with	O
the	O
goal	O
of	O
making	O
it	O
acceptably	O
safe	O
to	O
fully	O
embed	O
reinforcement	B
learning	I
agents	O
into	O
physical	O
environments	O
in	O
closing	O
we	O
return	B
to	O
simon	O
s	O
call	O
for	O
us	O
to	O
recognize	O
that	O
we	O
are	O
designers	O
of	O
our	O
future	O
and	O
not	O
simply	O
spectators	O
by	O
decisions	O
we	O
make	O
as	O
individuals	O
and	O
by	O
the	O
influence	O
we	O
can	O
exert	O
on	O
how	O
our	O
societies	O
are	O
governed	O
we	O
can	O
work	O
toward	O
ensuring	O
that	O
the	O
benefits	O
made	O
possible	O
by	O
a	O
new	O
technology	O
outweigh	O
the	O
harm	O
it	O
can	O
cause	O
there	O
is	O
ample	O
opportunity	O
to	O
do	O
this	O
in	O
the	O
case	O
of	O
reinforcement	B
learning	I
which	O
can	O
help	O
improve	O
the	O
quality	O
fairness	O
and	O
sustainability	O
of	O
life	O
on	O
our	O
planet	O
but	O
which	O
can	O
also	O
release	O
new	O
perils	O
a	O
threat	O
already	O
here	O
is	O
the	O
displacement	O
of	O
jobs	O
caused	O
by	O
applications	O
of	O
artificial	B
intelligence	I
still	O
there	O
are	O
good	O
reasons	O
to	O
believe	O
that	O
the	O
benefits	O
of	O
artificial	B
intelligence	I
can	O
outweigh	O
the	O
disruption	O
it	O
causes	O
as	O
to	O
safety	O
hazards	O
possible	O
with	O
reinforcement	B
learning	I
are	O
not	O
completely	O
different	O
from	O
those	O
that	O
have	O
been	O
managed	O
successfully	O
for	O
related	O
applications	O
of	O
optimization	O
and	B
control	B
methods	O
as	O
reinforcement	B
learning	I
moves	O
out	O
into	O
the	O
real	O
world	O
in	O
future	O
applications	O
developers	O
have	O
an	O
obligation	O
to	O
follow	O
best	O
practices	O
that	O
have	O
evolved	O
for	O
similar	O
technologies	O
while	O
at	O
the	O
same	O
time	O
extending	O
them	O
to	O
make	O
sure	O
that	O
prometheus	O
keeps	O
the	O
upper	O
hand	O
chapter	O
frontiers	O
bibliographical	O
and	O
historical	O
remarks	O
general	O
value	B
functions	O
were	O
first	O
explicitly	O
identified	O
by	O
sutton	O
and	O
colleagues	O
sutton	O
et	O
al	O
modayil	O
white	O
and	O
sutton	O
ring	O
preparation	O
developed	O
an	O
extensive	O
thought	O
experiment	O
with	O
gvfs	O
forecasts	O
that	O
has	O
been	O
influential	O
despite	O
not	O
yet	O
having	O
been	O
published	O
the	O
first	O
demonstrations	O
of	O
multi-headed	O
learning	O
in	O
reinforcement	B
learning	I
were	O
by	O
jaderberg	O
et	O
al	O
bellemare	O
dabney	O
and	O
munos	O
showed	O
that	O
predicting	O
more	O
things	O
about	O
the	O
distribution	O
of	O
reward	O
could	O
significantly	O
speed	O
learning	O
to	O
optimize	O
its	O
expectation	O
an	O
instance	O
of	O
auxiliary	B
tasks	I
many	O
others	O
have	O
since	O
taken	O
up	O
this	O
line	O
of	O
research	O
the	O
general	O
theory	O
of	O
classical	B
conditioning	I
as	O
learned	O
predictions	O
together	O
with	O
built-in	O
reflexive	O
reactions	O
to	O
the	O
predictions	O
has	O
not	O
to	O
our	O
knowledge	O
been	O
clearly	O
articulated	O
in	O
the	O
psychological	O
literature	O
modayil	O
and	O
sutton	O
describe	O
it	O
as	O
an	O
approach	O
to	O
the	O
engineering	O
of	O
robots	O
and	O
other	O
agents	O
calling	O
it	O
pavlovian	O
control	B
to	O
allude	O
to	O
its	O
roots	O
in	O
classical	B
conditioning	I
the	O
formalization	O
of	O
temporally	O
abstract	O
courses	O
of	O
action	B
as	O
options	B
was	O
introduced	O
by	O
sutton	O
precup	O
and	O
singh	O
building	O
on	O
prior	O
work	O
by	O
parr	O
and	O
sutton	O
and	O
on	O
classical	O
work	O
on	O
semi-mdps	O
see	O
puterman	O
precup	O
s	O
phd	O
thesis	O
developed	O
option	O
ideas	O
fully	O
an	O
important	O
limitation	O
of	O
these	O
early	O
works	O
is	O
that	O
they	O
did	O
not	O
treat	O
the	O
off-policy	B
case	O
with	B
function	B
approximation	I
intra-option	O
learning	O
in	O
general	O
requires	O
offpolicy	O
learning	O
which	O
could	O
not	O
be	O
done	O
reliably	O
with	B
function	B
approximation	I
at	O
that	O
time	O
although	O
now	O
we	O
have	O
a	O
variety	O
of	O
stable	O
off-policy	B
learning	O
methods	O
using	O
function	B
approximation	I
their	O
combination	O
with	O
option	O
ideas	O
had	O
not	O
been	O
significantly	O
explored	O
at	O
the	O
time	O
of	O
publication	O
of	O
this	O
book	O
barto	O
and	O
mahadevan	O
and	O
hengst	O
review	O
the	O
options	B
formalism	O
and	O
other	O
approaches	O
to	O
temporal	B
abstraction	I
using	O
gvfs	O
to	O
implement	O
option	B
models	I
has	O
not	O
previously	O
been	O
described	O
our	O
presentation	O
uses	O
the	O
trick	O
introduced	O
by	O
modayil	O
white	O
and	O
sutton	O
for	O
predicting	O
signals	O
at	O
the	O
termination	O
of	O
policies	O
among	O
the	O
few	O
works	O
that	O
have	O
learned	O
option	B
models	I
with	B
function	B
approximation	I
are	O
those	O
by	O
bacon	O
harb	O
and	O
precup	O
the	O
extension	O
of	O
options	B
and	O
option	B
models	I
to	O
the	O
average-reward	O
setting	O
has	O
not	O
yet	O
been	O
developed	O
in	O
the	O
literature	O
a	O
good	O
presentation	O
of	O
the	O
pomdp	O
approach	O
is	O
given	O
by	O
monahan	O
psrs	O
and	O
tests	O
were	O
introduced	O
by	O
littman	O
sutton	O
and	O
singh	O
ooms	O
were	O
introduced	O
by	O
jaeger	O
sequential	O
systems	O
which	O
unify	O
psrs	O
ooms	O
and	O
many	O
other	O
works	O
were	O
introduced	O
in	O
the	O
phd	O
thesis	O
of	O
michael	O
thon	O
thon	O
and	O
jaeger	O
the	O
theory	O
of	O
reinforcement	B
learning	I
with	O
a	O
non-markov	O
state	B
representation	O
was	O
developed	O
explicitly	O
by	O
singh	O
jaakkola	O
and	O
jordan	O
jaakkola	O
singh	O
and	B
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artificial	B
intelligence	I
jordan	O
early	O
reinforcement	B
learning	I
approaches	O
to	O
partial	O
observability	O
were	O
developed	O
by	O
chrisman	O
mccallum	O
parr	O
and	O
russell	O
littman	O
cassandra	O
and	O
kaelbling	O
and	O
by	O
lin	O
and	O
mitchell	O
early	O
efforts	O
to	O
include	O
advice	O
and	O
teaching	O
in	O
reinforcement	B
learning	I
include	O
those	O
by	O
lin	O
maclin	O
and	O
shavlik	O
clouse	O
and	O
clouse	O
and	O
utgoff	O
skinner	B
s	O
shaping	B
should	O
not	O
be	O
confused	O
with	O
the	O
potential-based	O
shaping	B
technique	O
introduced	O
by	O
ng	O
harada	O
and	O
russell	O
their	O
technique	O
has	O
been	O
shown	O
by	O
wiewiora	O
to	O
be	O
equivalent	O
to	O
the	O
simpler	O
idea	O
of	O
providing	O
an	O
initial	O
approximation	O
to	O
the	O
value	B
function	I
as	O
in	O
we	O
recommend	O
the	O
book	O
by	O
goodfellow	O
bengio	O
and	O
courville	O
for	O
discussion	O
of	O
today	O
s	O
deep	B
learning	I
techniques	O
the	O
problem	O
of	O
catastrophic	B
interference	I
in	O
artificial	B
neural	B
networks	I
was	O
developed	O
by	O
mccloskey	O
and	O
cohen	O
ratcliff	O
and	O
french	O
the	O
idea	O
of	O
a	O
replay	O
buffer	O
was	O
introduced	O
by	O
lin	O
and	O
used	O
prominently	O
in	O
deep	B
learning	I
in	O
the	O
atari	O
game	O
playing	O
system	O
mnih	O
et	O
al	O
minsky	B
was	O
one	O
of	O
the	O
first	O
to	O
identify	O
the	O
problem	O
of	O
representation	B
learning	I
among	O
the	O
few	O
works	O
to	O
consider	O
planning	B
with	O
learned	O
approximate	O
models	O
are	O
those	O
by	O
kuvayev	O
and	O
sutton	O
sutton	O
szepesvari	O
geramifard	O
and	O
bowling	O
nouri	O
and	O
littman	O
and	O
hester	O
and	O
stone	O
the	O
need	O
to	O
be	O
selective	O
in	O
model	O
construction	O
to	O
avoid	O
slowing	O
planning	B
is	O
well	O
known	O
in	O
artificial	B
intelligence	I
some	O
of	O
the	O
classic	O
work	O
is	O
by	O
minton	O
and	O
tambe	O
newell	O
and	O
rosenbloom	O
hauskrecht	O
meuleau	O
kaelbling	O
dean	O
and	O
boutilier	O
showed	O
this	O
effect	O
in	O
mdps	O
with	O
deterministic	O
options	B
schmidhuber	O
b	O
proposed	O
how	O
something	O
like	O
curiosity	B
would	O
result	O
if	O
reward	O
signals	O
were	O
a	O
function	O
of	O
how	O
quickly	O
an	O
agent	O
s	O
environment	B
model	O
is	O
improving	O
the	O
empowerment	O
function	O
proposed	O
by	O
klyubin	O
polani	O
and	O
nehaniv	O
is	O
an	O
information-theoretic	O
measure	O
of	O
an	O
agent	O
s	O
ability	O
to	O
control	B
its	O
environment	B
that	O
can	O
function	O
as	O
an	O
intrinsic	B
reward	B
signal	I
baldassarre	O
and	O
mirolli	O
is	O
a	O
collection	O
of	O
contributions	O
by	O
researchers	O
studying	O
intrinsic	B
reward	O
and	O
motivation	B
from	O
both	O
biological	O
and	O
computational	O
perspectives	O
including	O
a	O
perspective	O
on	O
intrinsically-motivated	O
reinforcement	B
learning	I
to	O
use	O
the	O
term	O
introduced	O
by	O
singh	O
barto	O
and	O
chentenez	O
see	O
also	O
oudeyer	O
and	O
kaplan	O
oudeyer	O
kaplan	O
and	O
hafner	O
and	O
barto	O
references	O
abbeel	O
p	O
ng	O
a	O
y	O
apprenticeship	O
learning	O
via	O
inverse	B
reinforcement	B
learning	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
acm	O
new	O
york	O
abramson	O
b	O
expected-outcome	O
a	O
general	O
model	O
of	O
static	O
evaluation	O
ieee	O
trans	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
adams	O
c	O
d	O
variations	O
in	O
the	O
sensitivity	O
of	O
instrumental	O
responding	O
to	O
reinforcer	O
devaluation	O
the	O
quarterly	O
journal	O
of	O
experimental	O
psychology	B
adams	O
c	O
d	O
dickinson	O
a	O
instrumental	O
responding	O
following	O
reinforcer	O
devaluation	O
the	O
quarterly	O
journal	O
of	O
experimental	O
psychology	B
adams	O
r	O
a	O
huys	O
q	O
j	O
m	O
roiser	O
j	O
p	O
computational	O
psychiatry	O
towards	O
a	O
mathematically	O
informed	O
understanding	O
of	O
mental	O
illness	O
journal	O
of	O
neurology	O
neurosurgery	O
psychiatry	O
agrawal	O
r	O
sample	O
mean	O
based	O
index	O
policies	O
with	O
ologn	O
regret	O
for	O
the	O
multi-armed	O
bandit	O
problem	O
advances	O
in	O
applied	O
probability	O
agre	O
p	O
e	O
the	O
dynamic	O
structure	O
of	O
everyday	O
life	O
ph	O
d	O
thesis	O
massachusetts	O
institute	O
of	O
technology	O
cambridge	O
ma	O
ai-tr	O
mit	O
artificial	B
intelligence	I
laboratory	O
agre	O
p	O
e	O
chapman	O
d	O
what	O
are	O
plans	O
for	O
robotics	O
and	O
autonomous	O
systems	O
aizerman	O
m	O
a	O
braverman	O
e	O
i	O
rozonoer	O
l	O
i	O
probability	O
problem	O
of	O
pattern	O
recognition	O
learning	O
and	O
potential	O
functions	O
method	O
avtomat	O
i	O
telemekh	O
albus	O
j	O
s	O
a	O
theory	O
of	O
cerebellar	O
function	O
mathematical	O
biosciences	O
albus	O
j	O
s	O
brain	O
behavior	O
and	O
robotics	O
byte	O
books	O
peterborough	O
nh	O
aleksandrov	O
v	O
m	O
sysoev	O
v	O
i	O
shemeneva	O
v	O
v	O
stochastic	O
optimization	O
of	O
systems	O
izv	O
akad	O
nauk	O
sssr	O
tekh	O
amari	O
s	O
i	O
natural	O
gradient	B
works	O
efficiently	O
in	O
learning	O
neural	B
computation	O
an	O
p	O
c	O
e	O
an	O
improved	O
multi-dimensional	O
cmac	O
neural	B
network	O
receptive	O
field	O
function	O
and	O
placement	O
ph	O
d	O
thesis	O
university	O
of	O
new	O
hampshire	O
durham	O
an	O
p	O
c	O
e	O
miller	O
w	O
t	O
parks	O
p	O
c	O
design	O
improvements	O
in	O
associative	O
memories	O
for	O
cerebellar	O
model	O
articulation	O
controllers	O
artificial	B
neural	B
networks	I
pp	O
elsevier	O
north-holland	O
anderson	O
c	O
w	O
learning	O
and	O
problem	O
solving	O
with	O
multilayer	O
connectionist	O
systems	O
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
anderson	O
c	O
w	O
strategy	O
learning	O
with	O
multilayer	O
connectionist	O
representations	O
in	O
references	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
anderson	O
c	O
w	O
learning	O
to	O
control	B
an	O
inverted	O
pendulum	O
using	O
neural	B
networks	O
ieee	O
control	B
systems	O
magazine	O
anderson	O
j	O
a	O
silverstein	O
j	O
w	O
ritz	O
s	O
a	O
jones	O
r	O
s	O
distinctive	O
features	O
categorical	O
perception	O
and	O
probability	O
learning	O
some	O
applications	O
of	O
a	O
neural	B
model	O
psychological	O
review	O
andreae	B
j	O
h	O
stella	O
a	O
scheme	O
for	O
a	O
learning	O
machine	O
in	O
proceedings	O
of	O
the	O
ifac	O
congress	O
basle	O
pp	O
butterworths	O
london	O
andreae	B
j	O
h	O
a	O
learning	O
machine	O
with	O
monologue	O
international	O
journal	O
of	O
man	O
machine	O
studies	O
andreae	B
j	O
h	O
learning	O
machines	O
a	O
unified	O
view	O
in	O
a	O
r	O
meetham	O
and	O
r	O
a	O
hudson	O
encyclopedia	O
of	O
information	O
linguistics	O
and	B
control	B
pp	O
pergamon	O
oxford	O
andreae	B
j	O
h	O
thinking	O
with	O
the	O
teachable	O
machine	O
academic	O
press	O
london	O
arthur	O
w	O
b	O
designing	O
economic	O
agents	O
that	O
act	O
like	O
human	O
agents	O
a	O
behavioral	O
approach	O
to	O
bounded	O
rationality	O
the	O
american	O
economic	O
review	O
atkeson	O
c	O
g	O
memory-based	O
approaches	O
to	O
approximating	O
continuous	O
functions	O
in	O
sante	O
fe	O
institute	O
studies	O
in	O
the	O
sciences	O
of	O
complexity	O
proceedings	O
vol	O
pp	O
addison-wesley	O
atkeson	O
c	O
g	O
moore	O
a	O
w	O
schaal	O
s	O
locally	O
weighted	O
learning	O
artificial	O
intelli	O
gence	O
review	O
auer	O
p	O
cesa-bianchi	O
n	O
fischer	O
p	O
finite-time	O
analysis	O
of	O
the	O
multiarmed	O
bandit	O
problem	O
machine	O
learning	O
bacon	O
p	O
l	O
harb	O
j	O
precup	O
d	O
the	O
option-critic	O
architecture	O
in	O
proceedings	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artificial	B
intelligence	I
pp	O
baird	O
l	O
c	O
residual	O
algorithms	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
baird	O
l	O
c	O
reinforcement	B
learning	I
through	O
gradient	B
descent	O
ph	O
d	O
thesis	O
carnegie	O
mellon	O
university	O
pittsburgh	O
pa	O
baird	O
l	O
c	O
klopf	B
a	O
h	O
reinforcement	B
learning	I
with	O
high-dimensional	O
continuous	O
actions	O
wright	O
laboratory	O
wright-patterson	O
air	O
force	O
base	O
tech	O
rep	O
baird	O
l	O
moore	O
a	O
w	O
gradient	B
descent	O
for	O
general	O
reinforcement	B
learning	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
baldassarre	O
g	O
mirolli	O
m	O
intrinsically	O
motivated	O
learning	O
in	O
natural	O
and	O
artificial	O
systems	O
springer-verlag	O
berlin	O
heidelberg	O
balke	O
a	O
pearl	O
j	O
counterfactual	O
probabilities	O
computational	O
methods	O
bounds	O
and	O
applications	O
in	O
proceedings	O
of	O
the	O
tenth	O
international	O
conference	O
on	O
uncertainty	O
in	O
artificial	B
intelligence	I
pp	O
morgan	O
kaufmann	O
baras	O
d	O
meir	O
r	O
reinforcement	B
learning	I
spike-time-dependent	O
plasticity	O
and	O
the	O
bcm	O
rule	O
neural	B
computation	O
barnard	O
e	O
temporal-difference	O
methods	O
and	O
markov	O
models	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
barreto	O
a	O
s	O
precup	O
d	O
pineau	O
j	O
reinforcement	B
learning	I
using	O
kernel-based	O
stochastic	O
factorization	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
references	O
pp	O
curran	O
associates	O
inc	O
bartlett	O
p	O
l	O
baxter	O
j	O
hebbian	B
synaptic	O
modifications	O
in	O
spiking	O
neurons	O
that	O
learn	O
technical	O
report	O
research	O
school	O
of	O
information	O
sciences	O
and	O
engineering	O
australian	O
national	O
university	O
bartlett	O
p	O
l	O
baxter	O
j	O
a	O
biologically	O
plausible	O
and	O
locally	O
optimal	O
learning	O
algorithm	O
for	O
spiking	O
neurons	O
rapport	O
technique	O
australian	O
national	O
university	O
barto	O
a	O
g	O
learning	O
by	O
statistical	O
cooperation	O
of	O
self-interested	O
neuron-like	O
computing	O
elements	O
human	O
neurobiology	O
barto	O
a	O
g	O
game-theoretic	O
cooperativity	O
in	O
networks	O
of	O
self-interested	O
units	O
in	O
j	O
s	O
denker	O
neural	B
networks	O
for	O
computing	O
pp	O
american	O
institute	O
of	O
physics	O
new	O
york	O
barto	O
a	O
g	O
from	O
chemotaxis	O
to	O
cooperativity	O
abstract	O
exercises	O
in	O
neuronal	O
learning	O
strategies	O
in	O
r	O
durbin	O
r	O
maill	O
and	O
g	O
mitchison	O
the	O
computing	O
neuron	O
pp	O
addison-wesley	O
reading	O
ma	O
barto	O
a	O
g	O
connectionist	O
learning	O
for	O
control	B
an	O
overview	O
in	O
t	O
miller	O
r	O
s	O
sutton	O
and	O
p	O
j	O
werbos	B
neural	B
networks	O
for	O
control	B
pp	O
mit	O
press	O
cambridge	O
ma	O
barto	O
a	O
g	O
some	O
learning	O
tasks	O
from	O
a	O
control	B
perspective	O
in	O
l	O
nadel	O
and	O
d	O
l	O
stein	O
lectures	O
in	O
complex	O
systems	O
pp	O
addison-wesley	O
redwood	O
city	O
ca	O
barto	O
a	O
g	O
reinforcement	B
learning	I
and	O
adaptive	O
critic	O
methods	O
in	O
d	O
a	O
white	O
and	O
d	O
a	O
sofge	O
handbook	O
of	O
intelligent	O
control	B
neural	B
fuzzy	O
and	O
adaptive	O
approaches	O
pp	O
van	O
nostrand	O
reinhold	O
new	O
york	O
barto	O
a	O
g	O
adaptive	O
critics	O
and	O
the	O
basal	B
ganglia	I
in	O
j	O
c	O
houk	O
j	O
l	O
davis	O
and	O
d	O
g	O
beiser	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	B
ganglia	I
pp	O
mit	O
press	O
cambridge	O
ma	O
barto	O
a	O
g	O
reinforcement	B
learning	I
in	O
m	O
a	O
arbib	O
handbook	O
of	O
brain	O
theory	O
and	O
neural	B
networks	O
pp	O
mit	O
press	O
cambridge	O
ma	O
barto	O
a	O
g	O
adaptive	O
real-time	B
dynamic	B
programming	I
in	O
c	O
sammut	O
and	O
g	O
i	O
webb	O
encyclopedia	O
of	O
machine	O
learning	O
pp	O
springer	O
science	O
and	O
business	O
media	O
barto	O
a	O
g	O
intrinsic	B
motivation	B
and	B
reinforcement	B
learning	I
in	O
g	O
baldassarre	O
and	O
m	O
mirolli	O
intrinsically	O
motivated	O
learning	O
in	O
natural	O
and	O
artificial	O
systems	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
barto	O
a	O
g	O
anandan	O
p	O
pattern	O
recognizing	O
stochastic	O
learning	B
automata	I
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
barto	O
a	O
g	O
anderson	O
c	O
w	O
structural	B
learning	O
in	O
connectionist	O
systems	O
in	O
program	O
of	O
the	O
seventh	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
pp	O
barto	O
a	O
g	O
anderson	O
c	O
w	O
sutton	O
r	O
s	O
synthesis	O
of	O
nonlinear	O
control	B
surfaces	O
by	O
a	O
layered	O
associative	B
search	I
network	O
biological	O
cybernetics	B
barto	O
a	O
g	O
bradtke	O
s	O
j	O
singh	O
s	O
p	O
real-time	O
learning	O
and	B
control	B
using	O
asynchronous	B
dynamic	B
programming	I
technical	O
report	O
department	O
of	O
computer	O
and	O
information	O
science	O
university	O
of	O
massachusetts	O
amherst	O
barto	O
a	O
g	O
bradtke	O
s	O
j	O
singh	O
s	O
p	O
learning	O
to	O
act	O
using	O
real-time	B
dynamic	B
programming	I
artificial	B
intelligence	I
barto	O
a	O
g	O
duff	O
m	O
monte	B
carlo	I
matrix	O
inversion	O
and	B
reinforcement	B
learning	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
morgan	O
kaufmann	O
san	O
francisco	O
references	O
barto	O
a	O
g	O
jordan	O
m	O
i	O
gradient	B
following	O
without	O
back-propagation	O
in	O
layered	O
in	O
m	O
caudill	O
and	O
c	O
butler	O
proceedings	O
of	O
the	O
ieee	O
first	O
annual	O
networks	O
conference	O
on	O
neural	B
networks	O
pp	O
sos	O
printing	O
san	O
diego	O
barto	O
a	O
g	O
mahadevan	O
s	O
recent	O
advances	O
in	O
hierarchical	O
reinforcement	B
learning	I
discrete	O
event	O
dynamic	O
systems	O
barto	O
a	O
g	O
singh	O
s	O
p	O
on	O
the	O
computational	O
economics	O
of	O
reinforcement	B
learning	I
in	O
connectionist	O
models	O
proceedings	O
of	O
the	O
summer	O
school	O
morgan	O
kaufmann	O
barto	O
a	O
g	O
sutton	O
r	O
s	O
goal	O
seeking	O
components	O
for	O
adaptive	O
intelligence	O
an	O
initial	O
assessment	O
technical	O
report	O
air	O
force	O
wright	O
aeronautical	O
laboratoriesavionics	O
laboratory	O
wright-patterson	O
afb	O
oh	O
barto	O
a	O
g	O
sutton	O
r	O
s	O
landmark	O
learning	O
an	O
illustration	O
of	O
associative	B
search	I
biological	O
cybernetics	B
barto	O
a	O
g	O
sutton	O
r	O
s	O
simulation	O
of	O
anticipatory	O
responses	O
in	O
classical	B
conditioning	I
by	O
a	O
neuron-like	O
adaptive	O
element	O
behavioural	O
brain	O
research	O
barto	O
a	O
g	O
sutton	O
r	O
s	O
anderson	O
c	O
w	O
neuronlike	O
elements	O
that	O
can	O
solve	O
difficult	O
learning	O
control	B
problems	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
reprinted	O
in	O
j	O
a	O
anderson	O
and	O
e	O
rosenfeld	O
neurocomputing	O
foundations	O
of	O
research	O
pp	O
mit	O
press	O
cambridge	O
ma	O
barto	O
a	O
g	O
sutton	O
r	O
s	O
brouwer	O
p	O
s	O
associative	B
search	I
network	O
a	O
reinforcement	B
learning	I
associative	O
memory	O
biological	O
cybernetics	B
barto	O
a	O
g	O
sutton	O
r	O
s	O
watkins	B
c	O
j	O
c	O
h	O
learning	O
and	O
sequential	O
decision	O
in	O
m	O
gabriel	O
and	O
j	O
moore	O
learning	O
and	O
computational	O
neuroscience	B
making	O
foundations	O
of	O
adaptive	O
networks	O
pp	O
mit	O
press	O
cambridge	O
ma	O
baxter	O
j	O
bartlett	O
p	O
l	O
infinite-horizon	O
policy-gradient	O
estimation	O
journal	O
of	O
arti	O
ficial	O
intelligence	O
research	O
baxter	O
j	O
bartlett	O
p	O
l	O
weaver	O
l	O
experiments	O
with	O
infinite-horizon	O
policy-gradient	O
estimation	O
journal	O
of	O
artificial	B
intelligence	I
research	O
bellemare	O
m	O
g	O
dabney	O
w	O
munos	O
r	O
a	O
distributional	O
perspective	O
on	O
reinforcement	B
learning	I
arxiv	O
preprint	O
bellemare	O
m	O
g	O
naddaf	O
y	O
veness	O
j	O
bowling	O
m	O
the	O
arcade	O
learning	O
environment	B
journal	O
of	O
artificial	B
intelligence	I
research	O
an	O
evaluation	O
platform	O
for	O
general	O
agents	O
bellemare	O
m	O
g	O
veness	O
j	O
bowling	O
m	O
investigating	O
contingency	O
awareness	O
using	O
in	O
proceedings	O
of	O
the	O
twenty-sixth	O
aaai	O
conference	O
on	O
artificial	O
atari	O
games	O
intelligence	O
pp	O
aaai	O
press	O
menlo	O
park	O
ca	O
bellman	B
r	O
e	O
a	O
problem	O
in	O
the	O
sequential	O
design	B
of	I
experiments	O
sankhya	O
bellman	B
r	O
e	O
dynamic	B
programming	I
princeton	O
university	O
press	O
princeton	O
bellman	B
r	O
e	O
a	O
markov	O
decision	O
process	O
journal	O
of	O
mathematics	O
and	O
mechanics	O
bellman	B
r	O
e	O
dreyfus	O
s	O
e	O
functional	O
approximations	O
and	B
dynamic	B
programming	I
mathematical	O
tables	O
and	O
other	O
aids	O
to	O
computation	O
bellman	B
r	O
e	O
kalaba	O
r	O
kotkin	O
b	O
polynomial	O
approximation	O
a	O
new	O
computational	O
technique	O
in	O
dynamic	B
programming	I
allocation	O
processes	O
mathematical	O
computation	O
bengio	O
y	O
learning	O
deep	O
architectures	O
for	O
ai	O
foundations	O
and	O
trends	O
in	O
machine	O
learning	O
bengio	O
y	O
courville	O
a	O
c	O
vincent	O
p	O
unsupervised	O
feature	O
learning	O
and	O
deep	O
learn	O
references	O
ing	O
a	O
review	O
and	O
new	O
perspectives	O
corr	O
arxiv	O
bentley	O
j	O
l	O
multidimensional	O
binary	O
search	O
trees	O
used	O
for	O
associative	O
searching	O
communications	O
of	O
the	O
acm	O
berg	O
h	O
c	O
chemotaxis	O
in	O
bacteria	O
annual	O
review	O
of	O
biophysics	O
and	O
bioengineering	O
berns	O
g	O
s	O
mcclure	O
s	O
m	O
pagnoni	O
g	O
montague	O
p	O
r	O
predictability	O
modulates	O
human	O
brain	O
response	O
to	O
reward	O
the	O
journal	O
of	O
neuroscience	B
berridge	O
k	O
c	O
kringelbach	O
m	O
l	O
affective	O
neuroscience	B
of	O
pleasure	O
reward	O
in	O
humans	O
and	O
animals	O
psychopharmacology	O
berridge	O
k	O
c	O
robinson	O
t	O
e	O
what	O
is	O
the	O
role	O
of	O
dopamine	B
in	O
reward	O
hedonic	O
impact	O
reward	O
learning	O
or	O
incentive	O
salience	O
brain	O
research	O
reviews	O
berry	O
d	O
a	O
fristedt	O
b	O
bandit	B
problems	I
chapman	O
and	O
hall	O
london	O
bertsekas	O
d	O
p	O
distributed	O
dynamic	B
programming	I
ieee	O
transactions	O
on	O
automatic	O
control	B
bertsekas	O
d	O
p	O
distributed	O
asynchronous	O
computation	O
of	O
fixed	O
points	O
mathematical	O
programming	O
bertsekas	O
d	O
p	O
dynamic	B
programming	I
deterministic	O
and	O
stochastic	O
models	O
prentice	O
hall	O
englewood	O
cliffs	O
nj	O
bertsekas	O
d	O
p	O
dynamic	B
programming	I
and	O
optimal	B
control	B
volume	O
third	O
edition	O
athena	O
scientific	O
belmont	O
ma	O
bertsekas	O
d	O
p	O
dynamic	B
programming	I
and	O
optimal	B
control	B
volume	O
approximate	B
dynamic	B
programming	I
fourth	O
edition	O
athena	O
scientific	O
belmont	O
ma	O
bertsekas	O
d	O
p	O
rollout	B
algorithms	I
for	O
discrete	O
optimization	O
a	O
survey	O
in	O
handbook	O
of	O
combinatorial	O
optimization	O
pp	O
springer	O
new	O
york	O
bertsekas	O
d	O
p	O
tsitsiklis	O
j	O
n	O
parallel	O
and	O
distributed	O
computation	O
numerical	O
methods	O
prentice-hall	O
englewood	O
cliffs	O
nj	O
bertsekas	O
d	O
p	O
tsitsiklis	O
j	O
n	O
neuro-dynamic	O
programming	O
athena	O
scientific	O
belmont	O
ma	O
bertsekas	O
d	O
p	O
tsitsiklis	O
j	O
n	O
wu	O
c	O
rollout	B
algorithms	I
for	O
combinatorial	O
opti	O
mization	O
journal	O
of	O
heuristics	O
bertsekas	O
d	O
p	O
yu	O
h	O
projected	O
equation	O
methods	O
for	O
approximate	O
solution	O
of	O
large	O
linear	O
systems	O
journal	O
of	O
computational	O
and	O
applied	O
mathematics	O
bhat	O
n	O
farias	O
v	O
moallemi	O
c	O
c	O
non-parametric	O
approximate	O
dynamic	O
programin	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
ming	O
via	O
the	O
kernel	O
method	O
pp	O
curran	O
associates	O
inc	O
bhatnagar	O
s	O
sutton	O
r	O
ghavamzadeh	O
m	O
lee	O
m	O
natural	O
actor	O
critic	O
algorithms	O
automatica	O
biermann	O
a	O
w	O
fairfield	O
j	O
r	O
c	O
beres	O
t	O
r	O
signature	O
table	O
systems	O
and	O
learning	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
bishop	O
c	O
m	O
neural	B
networks	O
for	O
pattern	O
recognition	O
clarendon	O
oxford	O
bishop	O
c	O
m	O
pattern	O
recognition	O
and	O
machine	O
learning	O
springer	O
science	O
business	O
media	O
new	O
york	O
llc	O
blodgett	O
h	O
c	O
the	O
effect	O
of	O
the	O
introduction	O
of	O
reward	O
upon	O
the	O
maze	O
performance	O
of	O
rats	O
university	O
of	O
california	O
publications	O
in	B
psychology	B
boakes	O
r	O
a	O
costa	O
d	O
s	O
j	O
temporal	O
contiguity	O
in	O
associative	O
learning	O
iinterference	O
journal	O
of	O
experimental	O
psychology	B
animal	O
and	O
decay	O
from	O
an	O
historical	O
perspective	O
learning	O
and	O
cognition	O
references	O
booker	O
l	O
b	O
intelligent	O
behavior	O
as	O
an	O
adaptation	O
to	O
the	O
task	O
environment	B
ph	O
d	O
the	O
sis	O
university	O
of	O
michigan	O
ann	O
arbor	O
bostrom	O
n	O
superintelligence	O
paths	O
dangers	O
strategies	O
oxford	O
university	O
press	O
oxford	O
bottou	O
l	O
vapnik	O
v	O
local	O
learning	O
algorithms	O
neural	B
computation	O
boyan	O
j	O
a	O
least-squares	O
temporal	O
difference	O
learning	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
boyan	O
j	O
a	O
technical	O
update	O
least-squares	O
temporal	O
difference	O
learning	O
machine	O
learning	O
boyan	O
j	O
a	O
moore	O
a	O
w	O
generalization	O
in	O
reinforcement	B
learning	I
safely	O
approximating	O
the	O
value	B
function	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
bradtke	O
s	O
j	O
reinforcement	B
learning	I
applied	O
to	O
linear	O
quadratic	O
regulation	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
morgan	O
kaufmann	O
bradtke	O
s	O
j	O
incremental	O
dynamic	B
programming	I
for	O
on-line	O
adaptive	O
optimal	B
control	B
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
appeared	O
as	O
cmpsci	O
technical	O
report	O
bradtke	O
s	O
j	O
barto	O
a	O
g	O
linear	O
least	O
squares	O
algorithms	O
for	O
temporal	O
difference	O
learning	O
machine	O
learning	O
bradtke	O
s	O
j	O
ydstie	O
b	O
e	O
barto	O
a	O
g	O
adaptive	O
linear	O
quadratic	O
control	B
using	O
policy	B
in	O
proceedings	O
of	O
the	O
american	O
control	B
conference	O
pp	O
american	O
iteration	O
automatic	O
control	B
council	O
evanston	O
il	O
brafman	O
r	O
i	O
tennenholtz	O
m	O
r-max	O
a	O
general	O
polynomial	O
time	O
algorithm	O
for	O
near-optimal	O
reinforcement	B
learning	I
journal	O
of	O
machine	O
learning	O
research	O
breiman	O
l	O
random	O
forests	O
machine	O
learning	O
breiter	O
h	O
c	O
aharon	O
i	O
kahneman	O
d	O
dale	O
a	O
shizgal	O
p	O
functional	O
imaging	O
of	O
neural	B
responses	O
to	O
expectancy	O
and	O
experience	O
of	O
monetary	O
gains	O
and	O
losses	O
neuron	O
breland	O
k	O
breland	O
m	O
the	O
misbehavior	O
of	O
organisms	O
american	O
psychologist	O
bridle	O
j	O
s	O
training	O
stochastic	O
model	O
recognition	O
algorithms	O
as	O
networks	O
can	O
lead	O
to	O
maximum	O
mutual	O
information	O
estimates	O
of	O
parameters	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
broomhead	O
d	O
s	O
lowe	O
d	O
multivariable	O
functional	O
interpolation	O
and	O
adaptive	O
net	O
works	O
complex	O
systems	O
bromberg-martin	O
e	O
s	O
matsumoto	O
m	O
hong	O
s	O
hikosaka	O
o	O
a	O
pallidus-habenuladopamine	O
pathway	O
signals	O
inferred	O
stimulus	O
values	O
journal	O
of	O
neurophysiology	O
browne	O
c	O
b	O
powley	O
e	O
whitehouse	O
d	O
lucas	O
s	O
m	O
cowling	O
p	O
i	O
rohlfshagen	O
p	O
tavener	O
s	O
perez	O
d	O
samothrakis	O
s	O
colton	O
s	O
a	O
survey	O
of	O
monte	B
carlo	I
tree	O
search	O
methods	O
ieee	O
transactions	O
on	O
computational	O
intelligence	O
and	O
ai	O
in	O
games	O
brown	O
j	O
bullock	O
d	O
grossberg	O
s	O
how	O
the	O
basal	B
ganglia	I
use	O
parallel	O
excitatory	O
and	O
inhibitory	O
learning	O
pathways	O
to	O
selectively	O
respond	O
to	O
unexpected	O
rewarding	O
cues	O
the	O
journal	O
of	O
neuroscience	B
bryson	O
a	O
e	O
jr	O
optimal	B
control	B
to	O
ieee	O
control	B
systems	O
buchanan	O
b	O
g	O
mitchell	O
t	O
smith	O
r	O
g	O
johnson	O
c	O
r	O
jr	O
models	O
of	O
learning	O
references	O
systems	O
encyclopedia	O
of	O
computer	O
science	O
and	O
technology	O
buhusi	O
c	O
v	O
schmajuk	O
n	O
a	O
timing	O
in	O
simple	O
conditioning	O
and	O
occasion	O
setting	O
a	O
neural	B
network	O
approach	O
behavioural	O
processes	O
bu	O
soniu	O
l	O
lazaric	O
a	O
ghavamzadeh	O
m	O
munos	O
r	O
babu	O
ska	O
r	O
de	O
schutter	O
b	O
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
least-squares	O
methods	O
for	O
policy	B
iteration	I
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
bush	O
r	O
r	O
mosteller	O
f	O
stochastic	O
models	O
for	O
learning	O
wiley	O
new	O
york	O
byrne	O
j	O
h	O
gingrich	O
k	O
j	O
baxter	O
d	O
a	O
computational	O
capabilities	O
of	O
single	O
neurons	O
relationship	O
to	O
simple	O
forms	O
of	O
associative	O
and	O
nonassociative	O
learning	O
in	O
aplysia	O
in	O
r	O
d	O
hawkins	O
and	O
g	O
h	O
bower	O
computational	O
models	O
of	O
learning	O
pp	O
academic	O
press	O
new	O
york	O
calabresi	O
p	O
picconi	O
b	O
tozzi	O
a	O
filippo	O
m	O
d	O
dopamine-mediated	O
regulation	O
of	O
corticostriatal	O
synaptic	B
plasticity	I
trends	O
in	B
neuroscience	B
camerer	O
c	O
behavioral	O
game	B
theory	I
experiments	O
in	O
strategic	O
interaction	O
princeton	O
university	O
press	O
campbell	O
d	O
t	O
knowledge-processes	O
pp	O
pergamon	O
new	O
york	O
blind	O
variation	O
and	O
selective	O
survival	O
as	O
a	O
general	O
strategy	O
in	O
in	O
m	O
c	O
yovits	O
and	O
s	O
cameron	O
self-organizing	O
systems	O
cao	O
x	O
r	O
stochastic	O
learning	O
and	O
optimization	O
a	O
sensitivity-based	O
approach	O
annual	O
reviews	O
in	O
control	B
cao	O
x	O
r	O
chen	O
h	O
f	O
perturbation	O
realization	O
potentials	O
and	O
sensitivity	O
analysis	O
of	O
markov	O
processes	O
ieee	O
transactions	O
on	O
automatic	O
control	B
carlstr	O
om	O
j	O
nordstr	O
om	O
e	O
control	B
of	O
self-similar	O
atm	O
call	O
traffic	O
by	O
reinforcement	B
learning	I
in	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
applications	O
of	O
neural	B
networks	O
to	O
telecommunications	O
pp	O
erlbaum	O
hillsdale	O
nj	O
chapman	O
d	O
kaelbling	O
l	O
p	O
input	O
generalization	O
in	O
delayed	B
reinforcement	B
learning	I
an	O
algorithm	O
and	O
performance	O
comparisons	O
in	O
proceedings	O
of	O
the	O
twelfth	O
international	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
chaslot	O
g	O
bakkes	O
s	O
szita	O
i	O
spronck	O
p	O
monte-carlo	O
tree	O
search	O
a	O
new	O
framework	O
for	O
game	O
ai	O
in	O
proceedings	O
of	O
the	O
fourth	O
aaai	O
conference	O
on	O
artificial	B
intelligence	I
and	O
interactive	O
digital	O
entertainment	O
pp	O
aaai	O
press	O
menlo	O
park	O
ca	O
chow	O
c	O
-s	O
tsitsiklis	O
j	O
n	O
an	O
optimal	O
one-way	O
multigrid	O
algorithm	O
for	O
discrete-time	O
stochastic	O
control	B
ieee	O
transactions	O
on	O
automatic	O
control	B
chrisman	O
l	O
reinforcement	B
learning	I
with	O
perceptual	O
aliasing	O
the	O
perceptual	O
distinctions	O
approach	O
in	O
proceedings	O
of	O
the	O
tenth	O
national	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
aaaimit	O
press	O
menlo	O
park	O
ca	O
christensen	O
j	O
korf	O
r	O
e	O
a	O
unified	O
theory	O
of	O
heuristic	O
evaluation	O
functions	O
and	O
in	O
proceedings	O
of	O
the	O
fifth	O
national	O
conference	O
on	O
artificial	O
its	O
application	O
to	O
learning	O
intelligence	O
pp	O
morgan	O
kaufmann	O
cichosz	O
p	O
truncating	O
temporal	O
differences	O
on	O
the	O
efficient	O
implementation	O
of	O
td	B
for	O
reinforcement	B
learning	I
journal	O
of	O
artificial	B
intelligence	I
research	O
claridge-chang	O
a	O
roorda	O
r	O
d	O
vrontou	O
e	O
sjulson	O
l	O
li	O
h	O
hirsh	O
j	O
miesenb	O
ock	O
g	O
writing	O
memories	O
with	O
light-addressable	O
reinforcement	O
circuitry	O
cell	O
clark	O
r	O
e	O
squire	O
l	O
r	O
classical	B
conditioning	I
and	O
brain	O
systems	O
the	O
role	O
of	O
awareness	O
science	O
references	O
clark	O
w	O
a	O
farley	O
b	O
g	O
generalization	O
of	O
pattern	O
recognition	O
in	O
a	O
self-organizing	O
system	O
in	O
proceedings	O
of	O
the	O
western	O
joint	O
computer	O
conference	O
pp	O
clouse	O
j	O
on	O
integrating	O
apprentice	O
learning	O
and	B
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
appeared	O
as	O
cmpsci	O
technical	O
report	O
clouse	O
j	O
utgoff	O
p	O
a	O
teaching	O
method	O
for	O
reinforcement	B
learning	I
systems	O
in	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
cobo	O
l	O
c	O
zang	O
p	O
isbell	O
c	O
l	O
thomaz	O
a	O
l	O
automatic	O
state	B
abstraction	O
from	O
in	O
proceedings	O
of	O
the	O
twenty-second	O
international	O
joint	O
conference	O
on	O
demonstration	O
artificial	B
intelligence	I
pp	O
aaai	O
press	O
connell	O
j	O
a	O
colony	O
architecture	O
for	O
an	O
artificial	O
creature	O
technical	O
report	O
ai-tr	O
mit	O
artificial	B
intelligence	I
laboratory	O
cambridge	O
ma	O
connell	O
m	O
e	O
utgoff	O
p	O
e	O
learning	O
to	O
control	B
a	O
dynamic	O
physical	O
system	O
compu	O
tational	O
intelligence	O
contreras-vidal	O
j	O
l	O
schultz	B
w	O
a	O
predictive	O
reinforcement	O
model	O
of	O
dopamine	B
neurons	O
for	O
learning	O
approach	O
behavior	O
journal	O
of	O
computational	O
neuroscience	B
coulom	O
r	O
efficient	O
selectivity	O
and	O
backup	O
operators	O
in	O
monte-carlo	O
tree	O
search	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
computers	O
and	O
games	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
courville	O
a	O
c	O
daw	O
n	O
d	O
touretzky	O
d	O
s	O
bayesian	O
theories	O
of	O
conditioning	O
in	O
a	O
changing	O
world	O
trends	O
in	O
cognitive	O
science	O
craik	O
k	O
j	O
w	O
the	O
nature	O
of	O
explanation	O
cambridge	O
university	O
press	O
cambridge	O
cross	O
j	O
g	O
a	O
stochastic	O
learning	O
model	O
of	O
economic	O
behavior	O
the	O
quarterly	O
journal	O
of	O
economics	O
crow	O
t	O
j	O
cortical	O
synapses	O
and	B
reinforcement	I
a	O
hypothesis	O
nature	O
curtiss	O
j	O
h	O
a	O
theoretical	O
comparison	O
of	O
the	O
efficiencies	O
of	O
two	O
classical	O
methods	O
and	O
a	O
monte	B
carlo	I
method	O
for	O
computing	O
one	O
component	O
of	O
the	O
solution	O
of	O
a	O
set	O
of	O
linear	O
algebraic	O
equations	O
in	O
h	O
a	O
meyer	O
symposium	O
on	O
monte	B
carlo	I
methods	I
pp	O
wiley	O
new	O
york	O
cybenko	O
g	O
approximation	O
by	O
superpositions	O
of	O
a	O
sigmoidal	O
function	O
mathematics	O
of	O
control	B
signals	O
and	O
systems	O
cziko	O
g	O
without	O
miracles	O
universal	O
selection	O
theory	O
and	O
the	O
second	O
darvinian	O
revolution	O
mit	O
press	O
cambridge	O
ma	O
dabney	O
w	O
adaptive	O
step-sizes	O
for	O
reinforcement	B
learning	I
phd	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
dabney	O
w	O
barto	O
a	O
g	O
adaptive	O
step-size	O
for	O
online	B
temporal	O
difference	O
learning	O
in	O
proceedings	O
of	O
the	O
annual	O
conference	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artificial	B
intelligence	I
daniel	O
j	O
w	O
splines	O
and	O
efficiency	O
in	O
dynamic	B
programming	I
journal	O
of	O
mathematical	O
analysis	O
and	O
applications	O
dann	O
c	O
neumann	O
g	O
peters	O
j	O
policy	B
evaluation	O
with	O
temporal	O
differences	O
a	O
survey	O
and	O
comparison	O
journal	O
of	O
machine	O
learning	O
research	O
daw	O
n	O
d	O
courville	O
a	O
c	O
touretzky	O
d	O
s	O
timing	O
and	O
partial	O
observability	O
in	O
the	O
dopamine	B
system	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
references	O
pp	O
mit	O
press	O
cambridge	O
ma	O
daw	O
n	O
d	O
courville	O
a	O
c	O
touretzky	O
d	O
s	O
representation	O
and	O
timing	O
in	O
theories	O
of	O
the	O
dopamine	B
system	O
neural	B
computation	O
daw	O
n	O
d	O
niv	O
y	O
dayan	O
p	O
uncertainty	O
based	O
competition	O
between	O
prefrontal	O
and	O
dorsolateral	O
striatal	O
systems	O
for	O
behavioral	O
control	B
nature	O
neuroscience	B
daw	O
n	O
d	O
shohamy	O
d	O
the	O
cognitive	O
neuroscience	B
of	O
motivation	B
and	O
learning	O
social	O
cognition	O
dayan	O
p	O
reinforcement	O
comparison	O
in	O
d	O
s	O
touretzky	O
j	O
l	O
elman	O
t	O
j	O
sejnowski	O
and	O
g	O
e	O
hinton	O
connectionist	O
models	O
proceedings	O
of	O
the	O
summer	O
school	O
pp	O
morgan	O
kaufmann	O
dayan	O
p	O
the	O
convergence	O
of	O
td	B
for	O
general	O
machine	O
learning	O
dayan	O
p	O
matters	O
temporal	O
trends	O
in	O
cognitive	O
sciences	O
dayan	O
p	O
abbott	O
l	O
f	O
theoretical	O
neuroscience	B
computational	O
and	O
mathematical	O
modeling	O
of	O
neural	B
systems	O
mit	O
press	O
cambridge	O
ma	O
dayan	O
p	O
berridge	O
k	O
c	O
model-based	O
and	O
model-free	O
pavlovian	O
reward	O
learning	O
revaluation	O
revision	O
and	O
revaluation	O
cognitive	O
affective	O
behavioral	O
neuroscience	B
dayan	O
p	O
niv	O
y	O
reinforcement	B
learning	I
the	O
good	O
the	O
bad	O
and	O
the	O
ugly	O
current	O
opinion	O
in	O
neurobiology	O
dayan	O
p	O
niv	O
y	O
seymour	O
b	O
daw	O
n	O
d	O
the	O
misbehavior	O
of	O
value	B
and	O
the	O
discipline	O
of	O
the	O
will	O
neural	B
networks	O
dayan	O
p	O
sejnowski	O
t	O
td	B
converges	O
with	O
probability	O
machine	O
learning	O
de	O
asis	O
k	O
hernandez-garcia	O
j	O
f	O
holland	B
g	O
z	O
sutton	O
r	O
s	O
multi-step	O
rein	O
forcement	O
learning	O
a	O
unifying	O
algorithm	O
arxiv	O
preprint	O
dean	O
t	O
lin	O
s	O
-h	O
decomposition	O
techniques	O
for	O
planning	B
in	O
stochastic	O
domains	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
international	O
joint	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
morgan	O
kaufmann	O
see	O
also	O
technical	O
report	O
brown	O
university	O
department	O
of	O
computer	O
science	O
degris	O
t	O
white	O
m	O
sutton	O
r	O
s	O
off-policy	B
actor	O
critic	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
arxiv	O
preprint	O
denardo	O
e	O
v	O
contraction	O
mappings	O
in	O
the	O
theory	O
underlying	O
dynamic	B
programming	I
siam	O
review	O
dennett	O
d	O
c	O
why	O
the	O
law	B
of	I
effect	I
will	O
not	O
go	O
away	O
brainstorms	O
pp	O
bradfordmit	O
press	O
cambridge	O
ma	O
derthick	O
m	O
variations	O
on	O
the	O
boltzmann	O
machine	O
learning	O
algorithm	O
carnegie-mellon	O
university	O
department	O
of	O
computer	O
science	O
technical	O
report	O
no	O
deutsch	O
j	O
a	O
a	O
new	O
type	O
of	O
behaviour	O
theory	O
british	O
journal	O
of	O
psychology	B
general	O
section	O
deutsch	O
j	O
a	O
a	O
machine	O
with	O
insight	O
quarterly	O
journal	O
of	O
experimental	O
psychology	B
dick	O
t	O
policy	B
gradient	B
reinforcement	B
learning	I
without	O
regret	O
m	O
sc	O
thesis	O
uni	O
versity	O
of	O
alberta	O
edmonton	O
dickinson	O
a	O
contemporary	O
animal	O
learning	O
theory	O
cambridge	O
university	O
press	O
cambridge	O
dickinson	O
a	O
actions	O
and	O
habits	O
the	O
development	O
of	O
behavioral	O
autonomy	O
phil	O
references	O
trans	O
r	O
soc	O
lond	O
b	O
dickinson	O
a	O
balleine	O
b	O
w	O
the	O
role	O
of	O
learning	O
in	O
motivation	B
in	O
c	O
r	O
gallistel	O
stevens	O
handbook	O
of	O
experimental	O
psychology	B
volume	O
pp	O
wiley	O
ny	O
dietterich	O
t	O
g	O
buchanan	O
b	O
g	O
the	O
role	O
of	O
the	O
critic	O
in	O
learning	O
systems	O
in	O
o	O
g	O
selfridge	O
e	O
l	O
rissland	O
and	O
m	O
a	O
arbib	O
adaptive	O
control	B
of	O
ill-defined	O
systems	O
pp	O
plenum	O
press	O
ny	O
proceedings	O
of	O
the	O
nato	O
advanced	O
research	O
institute	O
on	O
adaptive	O
control	B
of	O
ill-defined	O
systems	O
nato	O
conference	O
series	O
ii	O
systems	O
science	O
vol	O
dietterich	O
t	O
g	O
flann	O
n	O
s	O
explanation-based	O
learning	O
and	B
reinforcement	B
learning	I
a	O
unified	O
view	O
in	O
a	O
prieditis	O
and	O
s	O
russell	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
dietterich	O
t	O
g	O
wang	O
x	O
batch	O
value	B
function	B
approximation	I
via	O
support	O
vectors	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
diuk	O
c	O
cohen	O
a	O
littman	O
m	O
l	O
an	O
object-oriented	O
representation	O
for	O
efficient	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
reinforcement	B
learning	I
learning	O
pp	O
acm	O
new	O
york	O
dolan	O
r	O
j	O
dayan	O
p	O
goals	O
and	O
habits	O
in	O
the	O
brain	O
neuron	O
doll	O
b	O
b	O
simon	O
d	O
a	O
daw	O
n	O
d	O
the	O
ubiquity	O
of	O
model-based	B
reinforcement	B
learning	I
current	O
opinion	O
in	O
neurobiology	O
donahoe	O
j	O
w	O
burgos	O
j	O
e	O
behavior	O
analysis	O
and	O
revaluation	O
journal	O
of	O
the	O
experimental	O
analysis	O
of	O
behavior	O
dorigo	O
m	O
colombetti	O
m	O
robot	O
shaping	B
developing	O
autonomous	O
agents	O
through	O
learning	O
artificial	B
intelligence	I
doya	O
k	O
temporal	O
difference	O
learning	O
in	O
continuous	B
time	I
and	O
space	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
doya	O
k	O
sejnowski	O
t	O
j	O
a	O
novel	O
reinforcement	O
model	O
of	O
birdsong	O
vocalization	O
learning	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
doya	O
k	O
sejnowski	O
t	O
j	O
a	O
computational	O
model	O
of	O
birdsong	O
learning	O
by	O
auditory	O
in	O
p	O
w	O
f	O
poon	O
and	O
j	O
f	O
brugge	O
central	O
experience	O
and	O
auditory	O
feedback	O
auditory	O
processing	O
and	O
neural	B
modeling	O
pp	O
springer	O
boston	O
ma	O
doyle	O
p	O
g	O
snell	O
j	O
l	O
random	O
walks	O
and	O
electric	O
networks	O
the	O
mathematical	O
association	O
of	O
america	O
carus	O
mathematical	O
monograph	O
dreyfus	O
s	O
e	O
law	O
a	O
m	O
the	O
art	O
and	O
theory	O
of	O
dynamic	B
programming	I
academic	O
press	O
new	O
york	O
duda	O
r	O
o	O
hart	O
p	O
e	O
pattern	O
classification	O
and	O
scene	O
analysis	O
wiley	O
new	O
york	O
duff	O
m	O
o	O
q-learning	B
for	B
bandit	B
problems	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
egger	O
d	O
m	O
miller	O
n	O
e	O
secondary	B
reinforcement	I
in	O
rats	O
as	O
a	O
function	O
of	O
information	O
value	B
and	O
reliability	O
of	O
the	O
stimulus	O
journal	O
of	O
experimental	O
psychology	B
eshel	O
n	O
tian	O
j	O
bukwich	O
m	O
uchida	O
n	O
dopamine	B
neurons	O
share	O
common	O
response	O
function	O
for	O
reward	O
prediction	B
error	O
nature	O
neuroscience	B
estes	O
w	O
k	O
discriminative	O
conditioning	O
i	O
a	O
discriminative	O
property	O
of	O
conditioned	O
anticipation	O
journal	O
of	O
experimental	O
psychology	B
estes	O
w	O
k	O
discriminative	O
conditioning	O
ii	O
effects	O
of	O
a	O
pavlovian	O
conditioned	O
stimulus	O
references	O
upon	O
a	O
subsequently	O
established	O
operant	O
response	O
journal	O
of	O
experimental	O
psychology	B
estes	O
w	O
k	O
toward	O
a	O
statistical	O
theory	O
of	O
learning	O
psychololgical	O
review	O
farley	O
b	O
g	O
clark	O
w	O
a	O
simulation	O
of	O
self-organizing	O
systems	O
by	O
digital	O
computer	O
ire	O
transactions	O
on	O
information	O
theory	O
farries	O
m	O
a	O
fairhall	O
a	O
l	O
reinforcement	B
learning	I
with	O
modulated	O
spike	O
timingde	O
pendent	O
synaptic	B
plasticity	I
journal	O
of	O
neurophysiology	O
feldbaum	O
a	O
a	O
optimal	B
control	B
systems	O
academic	O
press	O
new	O
york	O
finch	O
g	O
culler	O
e	O
higher	O
order	O
conditioning	O
with	O
constant	O
motivation	B
the	O
american	O
journal	O
of	O
finnsson	O
h	O
bj	O
ornsson	O
y	O
simulation-based	O
approach	O
to	O
general	O
game	O
playing	O
in	O
proceedings	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artificial	B
intelligence	I
pp	O
fiorillo	O
c	O
d	O
yun	O
s	O
r	O
song	O
m	O
r	O
diversity	O
and	O
homogeneity	O
in	O
responses	O
of	O
midbrain	O
dopamine	B
neurons	O
the	O
journal	O
of	O
neuroscience	B
florian	O
r	O
v	O
reinforcement	B
learning	I
through	O
modulation	O
of	O
spike-timing-dependent	O
synaptic	B
plasticity	I
neural	B
computation	O
fogel	O
l	O
j	O
owens	O
a	O
j	O
walsh	O
m	O
j	O
artificial	B
intelligence	I
through	O
simulated	O
evo	O
lution	O
john	O
wiley	O
and	O
sons	O
french	O
r	O
m	O
catastrophic	O
forgetting	O
in	O
connectionist	O
networks	O
trends	O
in	O
cognitive	O
sciences	O
frey	O
u	O
morris	O
r	O
g	O
m	O
synaptic	O
tagging	O
and	O
long-term	O
potentiation	O
nature	O
fr	O
emaux	O
n	O
sprekeler	O
h	O
gerstner	O
w	O
functional	O
requirements	O
for	O
reward-modulated	O
spike-timing-dependent	O
plasticity	O
the	O
journal	O
of	O
neuroscience	B
friedman	O
j	O
h	O
bentley	O
j	O
l	O
finkel	O
r	O
a	O
an	O
algorithm	O
for	O
finding	O
best	O
matches	O
in	O
logarithmic	O
expected	B
time	O
acm	O
transactions	O
on	O
mathematical	O
software	O
friston	O
k	O
j	O
tononi	O
g	O
reeke	O
g	O
n	O
sporns	O
o	O
edelman	O
g	O
m	O
value-dependent	O
selection	O
in	O
the	O
brain	O
simulation	O
in	O
a	O
synthetic	O
neural	B
model	O
neuroscience	B
ieee	O
transactions	O
on	O
fu	O
k	O
s	O
learning	O
control	B
systems	O
review	O
and	O
outlook	O
automatic	O
control	B
galanter	O
e	O
gerstenhaber	O
m	O
on	O
thought	O
the	O
extrinsic	O
theory	O
psychological	O
review	O
gallistel	O
c	O
r	O
deconstructing	O
the	O
law	B
of	I
effect	I
games	O
and	O
economic	O
behavior	O
gardner	O
m	O
mathematical	O
games	O
scientific	O
american	O
geist	O
m	O
scherrer	O
b	O
off-policy	B
learning	O
with	B
eligibility	B
traces	I
a	O
survey	O
journal	O
of	O
machine	O
learning	O
research	O
gelly	O
s	O
silver	O
d	O
combining	O
online	B
and	O
o	O
ine	O
knowledge	O
in	O
uct	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
gelperin	O
a	O
hopfield	O
j	O
j	O
tank	O
d	O
w	O
the	O
logic	O
of	O
limax	O
learning	O
in	O
a	O
selverston	O
model	O
neural	B
networks	O
and	O
behavior	O
pp	O
plenum	O
press	O
new	O
york	O
genesereth	O
m	O
thielscher	O
m	O
general	O
game	O
playing	O
synthesis	O
lectures	O
on	O
artificial	B
intelligence	I
and	O
machine	O
learning	O
gershman	O
s	O
j	O
moustafa	O
a	O
a	O
ludvig	O
e	O
a	O
time	O
representation	O
in	O
reinforcement	B
learning	I
models	O
of	O
the	O
basal	B
ganglia	I
frontiers	O
in	O
computational	O
neuroscience	B
references	O
gershman	O
s	O
j	O
pesaran	O
b	O
daw	O
n	O
d	O
human	O
reinforcement	B
learning	I
subdivides	O
structured	O
action	B
spaces	O
by	O
learning	O
effector-specific	O
values	O
the	O
journal	O
of	O
neuroscience	B
ghiassian	O
s	O
rafiee	O
b	O
sutton	O
r	O
s	O
a	O
first	O
empirical	O
study	O
of	O
emphatic	O
temporal	O
difference	O
learning	O
workshop	O
on	O
continual	O
learning	O
and	O
deep	B
learning	I
at	O
the	O
conference	O
on	O
neural	B
information	O
processing	O
systems	O
gibbs	O
c	O
m	O
cool	O
v	O
land	O
t	O
kehoe	O
e	O
j	O
gormezano	O
i	O
second-order	O
conditioning	O
of	O
the	O
rabbits	O
nictitating	O
membrane	O
response	O
integrative	O
physiological	O
and	O
behavioral	O
science	O
gittins	O
j	O
c	O
jones	O
d	O
m	O
a	O
dynamic	O
allocation	O
index	O
for	O
the	O
sequential	O
design	B
of	I
experiments	O
in	O
j	O
gani	O
k	O
sarkadi	O
and	O
i	O
vincze	O
progress	O
in	O
statistics	O
pp	O
north-holland	O
amsterdam	O
london	O
glimcher	O
p	O
w	O
understanding	O
dopamine	B
and	B
reinforcement	B
learning	I
the	O
dopamine	B
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
reward	B
prediction	B
error	I
hypothesis	I
glimcher	O
p	O
w	O
decisions	O
uncertainty	O
and	O
the	O
brain	O
the	O
science	O
of	O
neuroeconomics	B
mit	O
press	O
cambridge	O
ma	O
glimcher	O
p	O
w	O
fehr	O
e	O
neuroeconomics	B
decision	O
making	O
and	O
the	O
brain	O
second	O
edition	O
academic	O
press	O
goethe	O
j	O
w	O
v	O
the	O
sorcerers	O
apprentice	O
in	O
the	O
permanent	O
goethe	O
p	O
the	O
dial	O
press	O
inc	O
new	O
york	O
goldstein	O
h	O
classical	O
mechanics	O
addison-wesley	O
reading	O
ma	O
goodfellow	O
i	O
bengio	O
y	O
courville	O
a	O
deep	B
learning	I
mit	O
press	O
cambridge	O
ma	O
goodwin	O
g	O
c	O
sin	O
k	O
s	O
adaptive	O
filtering	O
prediction	B
and	B
control	B
prentice-hall	O
englewood	O
cliffs	O
nj	O
gopnik	O
a	O
glymour	O
c	O
sobel	O
d	O
schulz	O
l	O
e	O
kushnir	O
t	O
danks	O
d	O
a	O
theory	O
of	O
causal	O
learning	O
in	O
children	O
causal	O
maps	O
and	O
bayes	O
nets	O
psychological	O
review	O
gordon	O
g	O
j	O
stable	O
function	B
approximation	I
in	O
dynamic	B
programming	I
in	O
a	O
prieditis	O
and	O
s	O
russell	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
an	O
expanded	O
version	O
was	O
published	O
as	O
technical	O
report	O
carnegie	O
mellon	O
university	O
pittsburgh	O
pa	O
gordon	O
g	O
j	O
chattering	O
in	O
sarsa	B
cmu	O
learning	O
lab	O
internal	O
report	O
gordon	O
g	O
j	O
stable	O
fitted	O
reinforcement	B
learning	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
gordon	O
g	O
j	O
approximate	O
solutions	O
to	O
markov	O
decision	O
processes	O
ph	O
d	O
thesis	O
carnegie	O
mellon	O
university	O
pittsburgh	O
pa	O
pittsburgh	O
pa	O
gordon	O
g	O
j	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
converges	O
to	O
a	O
region	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
graybiel	O
a	O
m	O
the	O
basal	B
ganglia	I
current	O
biology	O
greensmith	O
e	O
bartlett	O
p	O
l	O
baxter	O
j	O
variance	O
reduction	O
techniques	O
for	O
gradient	B
estimates	O
in	O
reinforcement	B
learning	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
greensmith	O
e	O
bartlett	O
p	O
l	O
baxter	O
j	O
variance	O
reduction	O
techniques	O
for	O
gradient	B
estimates	O
in	O
reinforcement	B
learning	I
journal	O
of	O
machine	O
learning	O
research	O
griffith	O
a	O
k	O
a	O
new	O
machine	O
learning	O
technique	O
applied	O
to	O
the	O
game	O
of	O
checkers	O
references	O
technical	O
report	O
project	O
mac	O
artificial	B
intelligence	I
memo	O
massachusetts	O
institute	O
of	O
technology	O
cambridge	O
ma	O
griffith	O
a	O
k	O
a	O
comparison	O
and	O
evaluation	O
of	O
three	O
machine	O
learning	O
procedures	O
as	O
applied	O
to	O
the	O
game	O
of	O
checkers	O
artificial	B
intelligence	I
grondman	O
i	O
busoniu	O
l	O
lopes	O
g	O
a	O
babuska	O
r	O
a	O
survey	O
of	O
actor	O
critic	O
reinforcement	B
learning	I
standard	O
and	O
natural	O
policy	B
gradients	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
part	O
c	O
and	O
reviews	O
grossberg	O
s	O
a	O
neural	B
model	O
of	O
attention	O
reinforcement	O
and	O
discrimination	O
learning	O
international	O
review	O
of	O
neurobiology	O
grossberg	O
s	O
schmajuk	O
n	O
a	O
neural	B
dynamics	O
of	O
adaptive	O
timing	O
and	O
temporal	O
discrimination	O
during	O
associative	O
learning	O
neural	B
networks	O
gullapalli	O
v	O
a	O
stochastic	O
reinforcement	O
algorithm	O
for	O
learning	O
real-valued	O
functions	O
neural	B
networks	O
gullapalli	O
v	O
barto	O
a	O
g	O
shaping	B
as	O
a	O
method	O
for	O
accelerating	O
reinforcement	B
learning	I
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
symposium	O
on	O
intelligent	O
control	B
pp	O
ieee	O
gurvits	O
l	O
lin	O
l	O
-j	O
hanson	O
s	O
j	O
incremental	O
learning	O
of	O
evaluation	O
functions	O
for	O
absorbing	O
markov	O
chains	O
new	O
methods	O
and	O
theorems	O
siemans	O
corporate	O
research	O
princeton	O
nj	O
hackman	O
l	O
faster	O
gradient-td	O
algorithms	O
m	O
sc	O
thesis	O
university	O
of	O
alberta	O
edmonton	O
hallak	O
a	O
tamar	O
a	O
mannor	O
s	O
emphatic	O
td	B
bellman	B
operator	I
is	O
a	O
contraction	O
hallak	O
a	O
tamar	O
a	O
munos	O
r	O
mannor	O
s	O
generalized	O
emphatic	O
temporal	O
difference	O
learning	O
bias-variance	O
analysis	O
in	O
proceedings	O
of	O
the	O
thirtieth	O
aaai	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
aaai	O
press	O
menlo	O
park	O
ca	O
hammer	O
m	O
the	O
neural	B
basis	O
of	O
associative	O
reward	O
learning	O
in	O
honeybees	O
trends	O
in	B
neuroscience	B
hammer	O
m	O
menzel	O
r	O
learning	O
and	O
memory	O
in	O
the	O
honeybee	O
the	O
journal	O
of	O
neuroscience	B
hampson	O
s	O
e	O
a	O
neural	B
model	O
of	O
adaptive	O
behavior	O
ph	O
d	O
thesis	O
university	O
of	O
california	O
irvine	O
hampson	O
s	O
e	O
connectionist	O
problem	O
solving	O
computational	O
aspects	O
of	O
biological	O
learning	O
birkhauser	O
boston	O
hare	O
t	O
a	O
o	O
doherty	O
j	O
camerer	O
c	O
f	O
schultz	B
w	O
rangel	O
a	O
dissociating	O
the	O
role	O
of	O
the	O
orbitofrontal	O
cortex	O
and	O
the	O
striatum	O
in	O
the	O
computation	O
of	O
goal	O
values	O
and	O
prediction	B
errors	O
the	O
journal	O
of	O
neuroscience	B
harth	O
e	O
tzanakou	O
e	O
alopex	O
a	O
stochastic	O
method	O
for	O
determining	O
visual	O
receptive	O
fields	O
vision	O
research	O
hassabis	O
d	O
maguire	O
e	O
a	O
deconstructing	O
episodic	O
memory	O
with	O
construction	O
trends	O
in	O
cognitive	O
sciences	O
hauskrecht	O
m	O
meuleau	O
n	O
kaelbling	O
l	O
p	O
dean	O
t	O
boutilier	O
c	O
hierarchical	O
solution	O
of	O
markov	O
decision	O
processes	O
using	O
macro-actions	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
conference	O
on	O
uncertainty	O
in	O
artificial	B
intelligence	I
pp	O
morgan	O
kaufmann	O
hawkins	O
r	O
d	O
kandel	O
e	O
r	O
is	O
there	O
a	O
cell-biological	O
alphabet	O
for	O
simple	O
forms	O
of	O
learning	O
psychological	O
review	O
haykin	O
s	O
neural	B
networks	O
a	O
comprehensive	O
foundation	O
macmillan	O
college	O
publish	O
references	O
ing	O
company	O
new	O
york	O
he	O
k	O
huertas	O
m	O
hong	O
s	O
z	O
tie	O
x	O
hell	O
j	O
w	O
shouval	O
h	O
kirkwood	O
a	O
distinct	O
eligibility	B
traces	I
for	O
ltp	O
and	O
ltd	O
in	O
cortical	O
synapses	O
neuron	O
he	O
k	O
zhang	O
x	O
ren	O
s	O
sun	O
j	O
deep	B
residual	I
learning	I
for	O
image	O
recognition	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	O
pp	O
hebb	O
d	O
o	O
the	O
organization	O
of	O
behavior	O
a	O
neuropsychological	O
theory	O
john	O
wiley	O
and	O
sons	O
inc	O
new	O
york	O
reissued	O
by	O
lawrence	O
erlbaum	O
associates	O
inc	O
mahwah	O
nj	O
hengst	O
b	O
hierarchical	O
approaches	O
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
rein	O
forcement	O
learning	O
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
herrnstein	O
r	O
j	O
on	O
the	O
law	B
of	I
effect	I
journal	O
of	O
the	O
experimental	O
analysis	O
of	O
behavior	O
hersh	O
r	O
griego	O
r	O
j	O
brownian	O
motion	O
and	O
potential	O
theory	O
scientific	O
american	O
hester	O
t	O
stone	O
p	O
learning	O
and	O
using	O
models	O
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
hesterberg	O
t	O
c	O
advances	O
in	O
importance	B
sampling	I
ph	O
d	O
thesis	O
statistics	O
depart	O
ment	O
stanford	O
university	O
hilgard	O
e	O
r	O
theories	O
of	O
learning	O
second	O
edition	O
appleton-century-cofts	O
inc	O
new	O
york	O
hilgard	O
e	O
r	O
bower	O
g	O
h	O
theories	O
of	O
learning	O
prentice-hall	O
englewood	O
cliffs	O
nj	O
hinton	O
g	O
e	O
distributed	O
representations	O
technical	O
report	O
depart	O
ment	O
of	O
computer	O
science	O
carnegie-mellon	O
university	O
pittsburgh	O
pa	O
hinton	O
g	O
e	O
osindero	O
s	O
teh	O
y	O
a	O
fast	O
learning	O
algorithm	O
for	O
deep	O
belief	O
nets	O
neural	B
computation	O
hochreiter	O
s	O
schmidhuber	O
j	O
ltsm	O
can	O
solve	O
hard	O
time	O
lag	O
problems	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
holland	B
j	O
h	O
adaptation	O
in	O
natural	O
and	O
artificial	O
systems	O
university	O
of	O
michigan	O
press	O
ann	O
arbor	O
holland	B
j	O
h	O
adaptation	O
in	O
r	O
rosen	O
and	O
f	O
m	O
snell	O
progress	O
in	O
theoretical	O
biology	O
vol	O
pp	O
academic	O
press	O
new	O
york	O
holland	B
j	O
h	O
escaping	O
brittleness	O
the	O
possibility	O
of	O
general-purpose	O
learning	O
algorithms	O
applied	O
to	O
rule-based	O
systems	O
in	O
r	O
s	O
michalski	O
j	O
g	O
carbonell	O
and	O
t	O
m	O
mitchell	O
machine	O
learning	O
an	O
artificial	B
intelligence	I
approach	O
vol	O
pp	O
morgan	O
kaufmann	O
hollerman	O
j	O
r	O
schultz	B
w	O
dopmine	O
neurons	O
report	O
an	O
error	O
in	O
the	O
temporal	O
prediction	B
of	O
reward	O
during	O
learning	O
nature	O
neuroscience	B
houk	O
j	O
c	O
adams	O
j	O
l	O
barto	O
a	O
g	O
a	O
model	O
of	O
how	O
the	O
basal	B
ganglia	I
generates	O
and	O
uses	O
neural	B
signals	O
that	O
predict	O
reinforcement	O
in	O
j	O
c	O
houk	O
j	O
l	O
davis	O
and	O
d	O
g	O
beiser	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	B
ganglia	I
pp	O
mit	O
press	O
cambridge	O
ma	O
howard	O
r	O
dynamic	B
programming	I
and	O
markov	O
processes	O
mit	O
press	O
cambridge	O
ma	O
references	O
hull	B
c	O
l	O
the	O
goal-gradient	O
hypothesis	O
and	O
maze	O
learning	O
psychological	O
review	O
hull	B
c	O
l	O
principles	O
of	O
behavior	O
appleton-century	O
new	O
york	O
hull	B
c	O
l	O
a	O
behavior	O
system	O
wiley	O
new	O
york	O
ioffe	O
s	O
szegedy	O
c	O
batch	O
normalization	O
accelerating	O
deep	O
network	O
training	O
by	O
reducing	O
internal	O
covariate	O
shift	O
ipek	O
e	O
mutlu	O
o	O
mart	O
nez	O
j	O
f	O
caruana	O
r	O
self-optimizing	O
memory	O
controllers	O
a	O
reinforcement	B
learning	I
approach	O
in	O
isca	O
of	O
the	O
annual	O
international	O
symposium	O
on	O
computer	O
architecture	O
pp	O
ieee	O
computer	O
society	O
washington	O
dc	O
usa	O
izhikevich	O
e	O
m	O
solving	O
the	O
distal	O
reward	O
problem	O
through	O
linkage	O
of	O
stdp	O
and	O
dopamine	B
signaling	O
cerebral	O
cortex	O
jaakkola	O
t	O
jordan	O
m	O
i	O
singh	O
s	O
p	O
on	O
the	O
convergence	O
of	O
stochastic	O
iterative	B
dynamic	B
programming	I
algorithms	O
neural	B
computation	O
jaakkola	O
t	O
singh	O
s	O
p	O
jordan	O
m	O
i	O
reinforcement	B
learning	I
algorithm	O
for	O
partially	O
observable	O
markov	O
decision	O
problems	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
jacobs	O
r	O
a	O
increased	O
rates	O
of	O
convergence	O
through	O
learning	O
rate	O
adaptation	O
neural	B
networks	O
jaderberg	O
m	O
mnih	O
v	O
czarnecki	O
w	O
m	O
schaul	O
t	O
leibo	O
j	O
z	O
silver	O
d	O
kavukcuoglu	O
k	O
reinforcement	B
learning	I
with	O
unsupervised	O
auxiliary	B
tasks	I
arxiv	O
preprint	O
jaeger	O
h	O
observable	O
operator	O
models	O
and	O
conditioned	O
continuation	O
representations	O
arbeitspapiere	O
der	O
gmd	O
gmd	O
forschungszentrum	O
informationstechnik	O
sankt	O
augustin	O
germany	O
jaeger	O
h	O
discrete	O
time	O
discrete	O
valued	O
observable	O
operator	O
models	O
a	O
tutorial	O
gmd-forschungszentrum	O
informationstechnik	O
jaeger	O
h	O
observable	O
operator	O
models	O
for	O
discrete	O
stochastic	O
time	O
series	O
neural	B
computation	O
jaeger	O
h	O
tutorial	O
on	O
training	O
recurrent	O
neural	B
networks	O
covering	O
bppt	O
rtrl	O
ekf	O
and	O
the	O
echo	O
state	B
network	O
approach	O
german	O
national	O
research	O
center	O
for	O
information	O
technology	O
technical	O
report	O
gmd	O
report	O
joel	O
d	O
niv	O
y	O
ruppin	O
e	O
actor	O
critic	O
models	O
of	O
the	O
basal	B
ganglia	I
new	O
anatomical	O
and	O
computational	O
perspectives	O
neural	B
networks	O
johnson	O
a	O
redish	O
a	O
d	O
neural	B
ensembles	O
in	O
transiently	O
encode	O
paths	O
forward	O
of	O
the	O
animal	O
at	O
a	O
decision	O
point	O
the	O
journal	O
of	O
neuroscience	B
kaelbling	O
l	O
p	O
hierarchical	O
learning	O
in	O
stochastic	O
domains	O
preliminary	O
results	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
kaelbling	O
l	O
p	O
learning	O
in	O
embedded	O
systems	O
mit	O
press	O
cambridge	O
ma	O
kaelbling	O
l	O
p	O
special	O
triple	O
issue	O
on	O
reinforcement	B
learning	I
machine	O
learning	O
kaelbling	O
l	O
p	O
littman	O
m	O
l	O
moore	O
a	O
w	O
reinforcement	B
learning	I
a	O
survey	O
journal	O
of	O
artificial	B
intelligence	I
research	O
kakade	O
s	O
m	O
a	O
natural	O
policy	B
gradient	B
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
kakade	O
s	O
m	O
on	O
the	O
sample	O
complexity	O
of	O
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
london	O
references	O
kakutani	O
s	O
markov	O
processes	O
and	O
the	O
dirichlet	O
problem	O
proceedings	O
of	O
the	O
japan	O
academy	O
kalos	O
m	O
h	O
whitlock	O
p	O
a	O
monte	B
carlo	I
methods	I
wiley	O
new	O
york	O
kamin	O
l	O
j	O
attention-like	O
processes	O
in	O
classical	B
conditioning	I
in	O
m	O
r	O
jones	O
miami	O
symposium	O
on	O
the	O
prediction	B
of	O
behavior	O
aversive	O
stimulation	O
pp	O
university	O
of	O
miami	O
press	O
coral	O
gables	O
florida	O
kamin	O
l	O
j	O
predictability	O
surprise	O
attention	O
and	O
conditioning	O
in	O
b	O
a	O
campbell	O
and	O
r	O
m	O
church	O
punishment	O
and	O
aversive	O
behavior	O
pp	O
appletoncentury-crofts	O
new	O
york	O
kandel	O
e	O
r	O
schwartz	O
j	O
h	O
jessell	O
t	O
m	O
siegelbaum	O
s	O
a	O
hudspeth	O
a	O
j	O
principles	O
of	O
neural	B
science	O
fifth	O
edition	O
mcgraw-hill	O
companies	O
inc	O
karampatziakis	O
n	O
langford	O
j	O
online	B
importance	O
weight	O
aware	O
updates	O
kashyap	O
r	O
l	O
blaydon	O
c	O
c	O
fu	O
k	O
s	O
stochastic	O
approximation	O
in	O
j	O
m	O
mendel	O
and	O
k	O
s	O
fu	O
adaptive	O
learning	O
and	O
pattern	O
recognition	O
systems	O
theory	O
and	O
applications	O
pp	O
academic	O
press	O
new	O
york	O
kearney	O
a	O
veeriah	O
v	O
travnik	O
j	O
sutton	O
r	O
s	O
pilarski	O
p	O
m	O
preparation	O
tidbd	O
adapting	O
temporal-difference	O
step-sizes	O
through	O
stochastic	O
meta-descent	O
kearns	O
m	O
singh	O
s	O
near-optimal	O
reinforcement	B
learning	I
in	O
polynomial	O
time	O
machine	O
learning	O
keerthi	O
s	O
s	O
ravindran	O
b	O
reinforcement	B
learning	I
in	O
e	O
fieslerm	O
and	O
r	O
beale	O
handbook	O
of	O
neural	B
computation	O
oxford	O
university	O
press	O
new	O
york	O
kehoe	O
e	O
j	O
conditioning	O
with	O
serial	O
compound	B
stimuli	O
theoretical	O
and	O
empirical	O
issues	O
experimental	O
animal	O
behavior	O
kehoe	O
e	O
j	O
schreurs	O
b	O
g	O
graham	O
p	O
temporal	O
primacy	O
overrides	O
prior	O
training	O
in	O
serial	O
compound	B
conditioning	O
of	O
the	O
rabbits	O
nictitating	O
membrane	O
response	O
animal	O
learning	O
behavior	O
keiflin	O
r	O
janak	O
p	O
h	O
dopamine	B
prediction	B
errors	O
in	O
reward	O
learning	O
and	B
addiction	B
ffrom	O
theory	O
to	O
neural	B
circuitry	O
neuron	O
kimble	O
g	O
a	O
hilgard	O
and	O
marquis	O
conditioning	O
and	O
learning	O
appleton-century	O
crofts	O
new	O
york	O
kimble	O
g	O
a	O
foundations	O
of	O
conditioning	O
and	O
learning	O
appleton-century-crofts	O
new	O
york	O
kingma	O
d	O
ba	O
j	O
adam	O
a	O
method	O
for	O
stochastic	O
optimization	O
klopf	B
a	O
h	O
brain	O
function	O
and	O
adaptive	O
systems	O
a	O
heterostatic	O
theory	O
technical	O
report	O
air	O
force	O
cambridge	O
research	O
laboratories	O
bedford	O
ma	O
a	O
summary	O
appears	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
systems	O
man	O
and	O
cybernetics	B
ieee	O
systems	O
man	O
and	O
cybernetics	B
society	O
dallas	O
tx	O
klopf	B
a	O
h	O
a	O
comparison	O
of	O
natural	O
and	B
artificial	B
intelligence	I
sigart	O
newsletter	O
klopf	B
a	O
h	O
the	O
hedonistic	O
neuron	O
a	O
theory	O
of	O
memory	O
learning	O
and	O
intelligence	O
hemisphere	O
washington	O
dc	O
klopf	B
a	O
h	O
a	O
neuronal	O
model	O
of	O
classical	B
conditioning	I
psychobiology	O
klyubin	O
a	O
s	O
polani	O
d	O
nehaniv	O
c	O
l	O
empowerment	O
a	O
universal	O
agent-centric	O
measure	O
of	O
control	B
in	O
proceedings	O
of	O
the	O
ieee	O
congress	O
on	O
evolutionary	O
computation	O
pp	O
ieee	O
kober	O
j	O
peters	O
j	O
reinforcement	B
learning	I
in	O
robotics	O
a	O
survey	O
in	O
m	O
wiering	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
references	O
berlin	O
heidelberg	O
kocsis	O
l	O
szepesv	O
ari	O
cs	O
bandit	O
based	O
monte-carlo	O
planning	B
in	O
proceedings	O
of	O
the	O
european	O
conference	O
on	O
machine	O
learning	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
kohonen	O
t	O
associative	O
memory	O
a	O
system	O
theoretic	O
approach	O
springer-verlag	O
berlin	O
koller	O
d	O
friedman	O
n	O
probabilistic	O
graphical	O
models	O
principles	O
and	O
techniques	O
mit	O
press	O
kolodziejski	O
c	O
porr	O
b	O
w	O
org	O
otter	O
f	O
on	O
the	O
asymptotic	O
equivalence	O
between	O
differential	B
hebbian	B
and	O
temporal	O
difference	O
learning	O
neural	B
computation	O
kolter	O
j	O
z	O
the	O
fixed	O
points	O
of	O
off-policy	B
td	B
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
konda	O
v	O
r	O
tsitsiklis	O
j	O
n	O
actor-critic	O
algorithms	O
in	O
advances	O
in	O
neural	B
informa	O
tion	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
konda	O
v	O
r	O
tsitsiklis	O
j	O
n	O
on	O
actor-critic	O
algorithms	O
siam	O
journal	O
on	O
control	B
and	O
optimization	O
konidaris	O
g	O
d	O
osentoski	O
s	O
thomas	O
p	O
s	O
value	B
function	B
approximation	I
in	O
reinforcement	B
learning	I
using	O
the	O
fourier	B
basis	I
in	O
proceedings	O
of	O
the	O
twenty-fifth	O
conference	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artificial	B
intelligence	I
pp	O
korf	O
r	O
e	O
optimal	O
path	O
finding	O
algorithms	O
in	O
l	O
n	O
kanal	O
and	O
v	O
kumar	O
search	O
in	O
artificial	B
intelligence	I
pp	O
springer-verlag	O
berlin	O
korf	O
r	O
e	O
real-time	O
heuristic	B
search	I
artificial	B
intelligence	I
koshland	O
d	O
e	O
bacterial	O
chemotaxis	O
as	O
a	O
model	O
behavioral	O
system	O
raven	O
press	O
new	O
york	O
koza	O
j	O
r	O
genetic	O
programming	O
on	O
the	O
programming	O
of	O
computers	O
by	O
means	O
of	O
natural	O
selection	O
mit	O
press	O
cambridge	O
ma	O
kraft	O
l	O
g	O
campagna	O
d	O
p	O
a	O
summary	O
comparison	O
of	O
cmac	O
neural	B
network	O
and	O
traditional	O
adaptive	O
control	B
systems	O
in	O
t	O
miller	O
r	O
s	O
sutton	O
and	O
p	O
j	O
werbos	B
neural	B
networks	O
for	O
control	B
pp	O
mit	O
press	O
cambridge	O
ma	O
kraft	O
l	O
g	O
miller	O
w	O
t	O
dietz	O
d	O
development	O
and	O
application	O
of	O
cmac	O
neural	B
network-based	O
control	B
in	O
d	O
a	O
white	O
and	O
d	O
a	O
sofge	O
handbook	O
of	O
intelligent	O
control	B
neural	B
fuzzy	O
and	O
adaptive	O
approaches	O
pp	O
van	O
nostrand	O
reinhold	O
new	O
york	O
kumar	O
p	O
r	O
varaiya	O
p	O
stochastic	O
systems	O
estimation	O
identification	O
and	O
adaptive	O
control	B
prentice-hall	O
englewood	O
cliffs	O
nj	O
kumar	O
p	O
r	O
a	O
survey	O
of	O
some	O
results	O
in	O
stochastic	O
adaptive	O
control	B
siam	O
journal	O
of	O
control	B
and	O
optimization	O
kumar	O
v	O
kanal	O
l	O
n	O
the	O
cdp	O
a	O
unifying	O
formulation	O
for	O
heuristic	B
search	I
dynamic	O
in	O
l	O
n	O
kanal	O
and	O
v	O
kumar	O
search	O
in	O
programming	O
and	O
branch-and-bound	O
artificial	B
intelligence	I
pp	O
springer-verlag	O
berlin	O
kushner	O
h	O
j	O
dupuis	O
p	O
numerical	O
methods	O
for	O
stochastic	O
control	B
problems	O
in	O
continuous	B
time	I
springer-verlag	O
new	O
york	O
kuvayev	O
l	O
sutton	O
r	O
s	O
model-based	B
reinforcement	B
learning	I
with	O
an	O
approximate	O
learned	O
model	O
proceedings	O
of	O
the	O
ninth	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
pp	O
yale	O
university	O
new	O
haven	O
ct	O
lagoudakis	O
m	O
parr	O
r	O
least	O
squares	O
policy	B
iteration	I
journal	O
of	O
machine	O
learning	O
research	O
references	O
lai	O
t	O
l	O
robbins	O
h	O
asymptotically	O
efficient	O
adaptive	O
allocation	O
rules	O
advances	O
in	O
applied	O
mathematics	O
lakshmivarahan	O
s	O
narendra	O
k	O
s	O
learning	O
algorithms	O
for	O
two-person	O
zero-sum	O
siam	O
journal	O
of	O
stochastic	O
games	O
with	O
incomplete	O
information	O
a	O
unified	O
approach	O
control	B
and	O
optimization	O
lammel	O
s	O
lim	O
b	O
k	O
malenka	O
r	O
c	O
reward	O
and	O
aversion	O
in	O
a	O
heterogeneous	O
midbrain	O
dopamine	B
system	O
neuropharmacology	O
lane	O
s	O
h	O
handelman	O
d	O
a	O
gelfand	O
j	O
j	O
theory	O
and	O
development	O
of	O
higher-order	O
cmac	O
neural	B
networks	O
ieee	O
control	B
systems	O
lecun	O
y	O
une	O
procdure	O
d	O
apprentissage	O
pour	O
rseau	O
a	O
seuil	O
asymmetrique	O
learning	O
scheme	O
for	O
asymmetric	O
threshold	O
networks	O
in	O
proceedings	O
of	O
cognitiva	O
paris	O
france	O
lecun	O
y	O
bottou	O
l	O
bengio	O
y	O
haffner	O
p	O
gradient-based	O
learning	O
applied	O
to	O
document	O
recognition	O
proceedings	O
of	O
the	O
ieee	O
legenstein	O
r	O
w	O
maass	O
d	O
p	O
a	O
learning	O
theory	O
for	O
reward-modulated	O
spike-timingdependent	O
plasticity	O
with	O
application	O
to	O
biofeedback	O
plos	O
computational	O
biology	O
levy	O
w	O
b	O
steward	O
d	O
temporal	O
contiguity	O
requirements	O
for	O
long-term	O
associative	O
potentiationdepression	O
in	O
the	O
hippocampus	O
neuroscience	B
lewis	O
f	O
l	O
liu	O
d	O
reinforcement	B
learning	I
and	O
approximate	O
dynamic	O
pro	O
gramming	O
for	O
feedback	O
control	B
john	O
wiley	O
and	O
sons	O
lewis	O
r	O
l	O
howes	O
a	O
singh	O
s	O
computational	O
rationality	O
linking	O
mechanism	O
and	O
behavior	O
through	O
utility	O
maximization	O
topics	O
in	O
cognitive	O
science	O
li	O
l	O
sample	O
complexity	O
bounds	O
of	O
exploration	O
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
li	O
l	O
chu	O
w	O
langford	O
j	O
schapire	O
r	O
e	O
a	O
contextual-bandit	O
approach	O
to	O
personin	O
proceedings	O
of	O
the	O
international	O
conference	O
alized	O
news	O
article	O
recommendation	O
on	O
world	O
wide	O
web	O
pp	O
acm	O
new	O
york	O
lin	O
c	O
-s	O
kim	O
h	O
cmac-based	O
adaptive	O
critic	O
self-learning	O
control	B
ieee	O
transac	O
tions	O
on	O
neural	B
networks	O
lin	O
l	O
-j	O
self-improving	O
reactive	O
agents	O
based	O
on	O
reinforcement	B
learning	I
planning	B
and	O
teaching	O
machine	O
learning	O
lin	O
l	O
-j	O
mitchell	O
t	O
reinforcement	B
learning	I
with	O
hidden	O
states	O
in	O
proceedings	O
of	O
the	O
second	O
international	O
conference	O
on	O
simulation	O
of	O
adaptive	O
behavior	O
from	O
animals	O
to	O
animats	O
pp	O
mit	O
press	O
cambridge	O
ma	O
littman	O
m	O
l	O
cassandra	O
a	O
r	O
kaelbling	O
l	O
p	O
learning	O
policies	O
for	O
partially	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
observable	O
environments	O
scaling	O
up	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
littman	O
m	O
l	O
dean	O
t	O
l	O
kaelbling	O
l	O
p	O
on	O
the	O
complexity	O
of	O
solving	O
markov	O
in	O
proceedings	O
of	O
the	O
eleventh	O
annual	O
conference	O
on	O
uncertainty	O
in	O
decision	O
problems	O
artificial	B
intelligence	I
pp	O
littman	O
m	O
l	O
sutton	O
r	O
s	O
singh	O
predictive	O
representations	O
of	O
state	B
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
liu	O
j	O
s	O
monte	B
carlo	I
strategies	O
in	O
scientific	O
computing	O
springer-verlag	O
berlin	O
ljung	O
l	O
system	B
identification	I
in	O
a	O
proch	O
azka	O
j	O
uhl	O
r	O
p	O
w	O
j	O
rayner	O
and	O
n	O
g	O
kingsbury	O
signal	O
analysis	O
and	O
prediction	B
pp	O
springer	O
science	O
business	O
media	O
new	O
york	O
llc	O
references	O
ljung	O
l	O
s	O
oderstrom	O
t	O
theory	O
and	O
practice	O
of	O
recursive	O
identification	O
mit	O
press	O
cambridge	O
ma	O
ljungberg	O
t	O
apicella	O
p	O
schultz	B
w	O
responses	O
of	O
monkey	O
dopamine	B
neurons	O
during	O
learning	O
of	O
behavioral	O
reactions	O
journal	O
of	O
neurophysiology	O
lovejoy	O
w	O
s	O
a	O
survey	O
of	O
algorithmic	O
methods	O
for	O
partially	O
observed	O
markov	O
decision	O
processes	O
annals	O
of	O
operations	O
research	O
luce	O
d	O
individual	O
choice	O
behavior	O
wiley	O
new	O
york	O
ludvig	O
e	O
a	O
bellemare	O
m	O
g	O
pearson	O
k	O
g	O
a	O
primer	O
on	O
reinforcement	B
learning	I
in	O
the	O
brain	O
psychological	O
computational	O
and	O
neural	B
perspectives	O
in	O
e	O
alonso	O
and	O
e	O
mondrag	O
on	O
computational	O
neuroscience	B
for	O
advancing	O
artificial	B
intelligence	I
models	O
methods	O
and	O
applications	O
pp	O
medical	O
information	O
science	O
reference	O
hershey	O
pa	O
ludvig	O
e	O
a	O
sutton	O
r	O
s	O
kehoe	O
e	O
j	O
stimulus	O
representation	O
and	O
the	O
timing	O
of	O
reward-prediction	O
errors	O
in	O
models	O
of	O
the	O
dopamine	B
system	O
neural	B
computation	O
ludvig	O
e	O
a	O
sutton	O
r	O
s	O
kehoe	O
e	O
j	O
evaluating	O
the	O
td	B
model	I
of	O
classical	B
conditioning	I
learning	O
behavior	O
machado	O
a	O
learning	O
the	O
temporal	O
dynamics	O
of	O
behavior	O
psychological	O
review	O
mackintosh	O
n	O
j	O
a	O
theory	O
of	O
attention	O
variations	O
in	O
the	O
associability	O
of	O
stimuli	O
with	O
reinforcement	O
psychological	O
review	O
mackintosh	O
n	O
j	O
conditioning	O
and	O
associative	O
learning	O
clarendon	O
press	O
oxford	O
maclin	O
r	O
shavlik	O
j	O
w	O
incorporating	O
advice	O
into	O
agents	O
that	O
learn	O
from	O
reinin	O
proceedings	O
of	O
the	O
twelfth	O
national	O
conference	O
on	O
artificial	B
intelligence	I
forcements	O
pp	O
aaai	O
press	O
menlo	O
park	O
ca	O
maei	O
h	O
r	O
gradient	B
temporal-difference	B
learning	I
algorithms	O
ph	O
d	O
thesis	O
univer	O
sity	O
of	O
alberta	O
edmonton	O
maei	O
h	O
r	O
sutton	O
r	O
s	O
gq	O
a	O
general	O
gradient	B
algorithm	O
for	O
temporal-difference	O
prediction	B
learning	O
with	B
eligibility	B
traces	I
in	O
proceedings	O
of	O
the	O
third	O
conference	O
on	O
artificial	O
general	O
intelligence	O
pp	O
maei	O
h	O
r	O
szepesv	O
ari	O
cs	O
bhatnagar	O
s	O
precup	O
d	O
silver	O
d	O
sutton	O
r	O
s	O
convergent	O
temporal-difference	B
learning	I
with	O
arbitrary	O
smooth	O
function	B
approximation	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
maei	O
h	O
r	O
szepesv	O
ari	O
cs	O
bhatnagar	O
s	O
sutton	O
r	O
s	O
toward	O
off-policy	B
learning	O
control	B
with	B
function	B
approximation	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
mahadevan	O
s	O
average	O
reward	O
reinforcement	B
learning	I
foundations	O
algorithms	O
and	O
empirical	O
results	O
machine	O
learning	O
mahadevan	O
s	O
liu	O
b	O
thomas	O
p	O
dabney	O
w	O
giguere	O
s	O
jacek	O
n	O
gemp	O
i	O
liu	O
j	O
proximal	O
reinforcement	B
learning	I
a	O
new	O
theory	O
of	O
sequential	O
decision	O
making	O
in	O
primal-dual	O
spaces	O
arxiv	O
preprint	O
mahadevan	O
s	O
connell	O
j	O
automatic	O
programming	O
of	O
behavior-based	O
robots	O
using	O
reinforcement	B
learning	I
artificial	B
intelligence	I
mahmood	O
a	O
r	O
incremental	O
off-policy	B
reinforcement	B
learning	I
algorithms	O
ph	O
d	O
thesis	O
university	O
of	O
alberta	O
edmonton	O
mahmood	O
a	O
r	O
sutton	O
r	O
s	O
off-policy	B
learning	O
based	O
on	O
weighted	O
importance	O
references	O
sampling	O
with	O
linear	O
computational	O
complexity	O
in	O
proceedings	O
of	O
the	O
conference	O
on	O
uncertainty	O
in	O
artificial	B
intelligence	I
pp	O
auai	O
press	O
corvallis	O
oregon	O
mahmood	O
a	O
r	O
sutton	O
r	O
s	O
degris	O
t	O
pilarski	O
p	O
m	O
tuning-free	O
step-size	O
adaptation	O
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
speech	O
and	O
signal	O
processing	O
proceedings	O
pp	O
ieee	O
mahmood	O
a	O
r	O
yu	O
h	O
sutton	O
r	O
s	O
multi-step	O
off-policy	B
learning	O
without	O
importance	B
sampling	I
ratios	O
arxiv	O
mahmood	O
a	O
r	O
van	O
hasselt	O
h	O
sutton	O
r	O
s	O
weighted	O
importance	B
sampling	I
for	O
off-policy	B
learning	O
with	O
linear	O
function	B
approximation	I
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
marbach	O
p	O
tsitsiklis	O
j	O
n	O
simulation-based	O
optimization	O
of	O
markov	O
reward	O
processes	O
mit	O
technical	O
report	O
marbach	O
p	O
tsitsiklis	O
j	O
n	O
simulation-based	O
optimization	O
of	O
markov	O
reward	O
processes	O
ieee	O
transactions	O
on	O
automatic	O
control	B
markram	O
h	O
l	O
ubke	O
j	O
frotscher	O
m	O
sakmann	O
b	O
regulation	O
of	O
synaptic	O
efficacy	O
by	O
coincidence	O
of	O
postsynaptic	O
aps	O
and	O
epsps	O
science	O
mart	O
nez	O
j	O
f	O
ipek	O
e	O
dynamic	O
multicore	O
resource	O
management	O
a	O
machine	O
learning	O
approach	O
micro	O
ieee	O
mataric	O
m	O
j	O
reward	O
functions	O
for	O
accelerated	O
learning	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
matsuda	O
w	O
furuta	O
t	O
nakamura	O
k	O
c	O
hioki	O
h	O
fujiyama	O
f	O
arai	O
r	O
kaneko	O
t	O
single	O
nigrostriatal	O
dopaminergic	O
neurons	O
form	O
widely	O
spread	O
and	O
highly	O
dense	O
axonal	O
arborizations	O
in	O
the	O
neostriatum	O
the	O
journal	O
of	O
neuroscience	B
mazur	O
j	O
e	O
learning	O
and	O
behavior	O
ed	O
prentice-hall	O
englewood	O
cliffs	O
nj	O
mccallum	O
a	O
k	O
overcoming	O
incomplete	O
perception	O
with	O
utile	O
distinction	O
memory	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
mccallum	O
a	O
k	O
reinforcement	B
learning	I
with	O
selective	O
perception	O
and	O
hidden	O
state	B
ph	O
d	O
thesis	O
university	O
of	O
rochester	O
rochester	O
ny	O
mccloskey	O
m	O
cohen	O
n	O
j	O
catastrophic	B
interference	I
in	O
connectionist	O
networks	O
the	O
sequential	O
learning	O
problem	O
psychology	B
of	O
learning	O
and	O
motivation	B
mcclure	O
s	O
m	O
daw	O
n	O
d	O
montague	O
p	O
r	O
a	O
computational	O
substrate	O
for	O
incentive	O
salience	O
trends	O
in	O
neurosciences	O
mcculloch	O
w	O
s	O
pitts	O
w	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
bulletin	O
of	O
mathematical	O
biophysics	O
mcmahan	O
h	O
b	O
gordon	O
g	O
j	O
fast	O
exact	O
planning	B
in	O
markov	O
decision	O
processes	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
automated	O
planning	B
and	O
scheduling	O
pp	O
melo	O
f	O
s	O
meyn	O
s	O
p	O
ribeiro	O
m	O
i	O
an	O
analysis	O
of	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
mendel	O
j	O
m	O
a	O
survey	O
of	O
learning	O
control	B
systems	O
isa	O
transactions	O
mendel	O
j	O
m	O
mclaren	O
r	O
w	O
reinforcement	B
learning	I
control	B
and	O
pattern	O
recognition	O
systems	O
in	O
j	O
m	O
mendel	O
and	O
k	O
s	O
fu	O
adaptive	O
learning	O
and	O
pattern	O
recognition	O
systems	O
theory	O
and	O
applications	O
pp	O
academic	O
press	O
new	O
york	O
references	O
michie	B
d	O
trial	O
and	O
error	O
in	O
s	O
a	O
barnett	O
and	O
a	O
mclaren	O
science	O
survey	O
part	O
pp	O
penguin	O
harmondsworth	O
michie	B
d	O
experiments	O
on	O
the	O
mechanisation	O
of	O
game	O
learning	O
characterization	O
of	O
the	O
model	O
and	O
its	O
parameters	O
the	O
computer	O
journal	O
michie	B
d	O
on	O
machine	O
intelligence	O
edinburgh	O
university	O
press	O
edinburgh	O
michie	B
d	O
chambers	O
r	O
a	O
boxes	B
an	O
experiment	O
in	O
adaptive	O
control	B
in	O
e	O
dale	O
and	O
d	O
michie	B
machine	O
intelligence	O
pp	O
oliver	O
and	O
boyd	O
edinburgh	O
miller	O
r	O
meaning	O
and	O
purpose	O
in	O
the	O
intact	O
brain	O
a	O
philosophical	O
psychological	O
and	O
biological	O
account	O
of	O
conscious	O
process	O
clarendon	O
press	O
oxford	O
miller	O
w	O
t	O
an	O
e	O
glanz	O
f	O
carter	O
m	O
the	O
design	B
of	I
cmac	O
neural	B
networks	O
for	O
control	B
adaptive	O
and	O
learning	O
systems	O
miller	O
w	O
t	O
glanz	O
f	O
h	O
unh	O
cmac	O
verison	O
the	O
university	O
of	O
new	O
hampshire	O
implementation	O
of	O
the	O
cerebellar	O
model	O
arithmetic	O
computer	O
cmac	O
robotics	O
laboratory	O
technical	O
report	O
university	O
of	O
new	O
hampshire	O
durham	O
miller	O
s	O
williams	O
r	O
j	O
learning	O
to	O
control	B
a	O
bioreactor	O
using	O
a	O
neural	B
net	O
dyna-q	O
system	O
in	O
proceedings	O
of	O
the	O
seventh	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
pp	O
center	O
for	O
systems	O
science	O
dunham	O
laboratory	O
yale	O
university	O
new	O
haven	O
miller	O
w	O
t	O
scalera	O
s	O
m	O
kim	O
a	O
neural	B
network	O
control	B
of	O
dynamic	O
balance	O
for	O
a	O
biped	O
walking	O
robot	O
in	O
proceedings	O
of	O
the	O
eighth	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
pp	O
center	O
for	O
systems	O
science	O
dunham	O
laboratory	O
yale	O
university	O
new	O
haven	O
minton	O
s	O
quantitative	O
results	O
concerning	O
the	O
utility	O
of	O
explanation-based	O
learning	O
artificial	B
intelligence	I
minsky	B
m	O
l	O
theory	O
of	O
neural-analog	O
reinforcement	O
systems	O
and	O
its	O
application	O
to	O
the	O
brain-model	O
problem	O
ph	O
d	O
thesis	O
princeton	O
university	O
minsky	B
m	O
l	O
steps	O
toward	O
artificial	B
intelligence	I
proceedings	O
of	O
the	O
institute	O
of	O
radio	O
engineers	O
reprinted	O
in	O
e	O
a	O
feigenbaum	O
and	O
j	O
feldman	O
computers	O
and	O
thought	O
pp	O
mcgraw-hill	O
new	O
york	O
minsky	B
m	O
l	O
computation	O
finite	O
and	O
infinite	O
machines	O
prentice-hall	O
englewood	O
cliffs	O
nj	O
mnih	O
v	O
kavukcuoglu	O
k	O
silver	O
d	O
graves	O
a	O
antonoglou	O
i	O
wierstra	O
d	O
riedmiller	O
m	O
playing	O
atari	O
with	O
deep	B
reinforcement	B
learning	I
arxiv	O
preprint	O
mnih	O
v	O
kavukcuoglu	O
k	O
silver	O
d	O
rusu	O
a	O
a	O
veness	O
j	O
bellemare	O
m	O
g	O
graves	O
a	O
riedmiller	O
m	O
fidjeland	O
a	O
k	O
ostrovski	O
g	O
petersen	O
s	O
beattie	O
c	O
sadik	O
a	O
antonoglou	O
i	O
king	O
h	O
kumaran	O
d	O
wierstra	O
d	O
legg	O
s	O
hassabis	O
d	O
humanlevel	O
control	B
through	O
deep	B
reinforcement	B
learning	I
nature	O
modayil	O
j	O
sutton	O
r	O
s	O
prediction	B
driven	O
behavior	O
learning	O
predictions	O
that	O
drive	O
fixed	O
responses	O
in	O
workshop	O
on	O
artificial	B
intelligence	I
and	O
robotics	O
quebec	O
city	O
canada	O
modayil	O
j	O
white	O
a	O
sutton	O
r	O
s	O
multi-timescale	O
nexting	O
in	O
a	O
reinforcement	B
learning	I
robot	O
adaptive	O
behavior	O
monahan	O
g	O
e	O
state	B
of	O
the	O
art	O
a	O
survey	O
of	O
partially	O
observable	O
markov	O
decision	O
processes	O
theory	O
models	O
and	O
algorithms	O
management	O
science	O
montague	O
p	O
r	O
dayan	O
p	O
nowlan	O
s	O
j	O
pouget	O
a	O
sejnowski	O
t	O
j	O
using	O
aperiodic	O
in	O
advances	O
in	O
neural	B
reinforcement	O
for	O
directed	O
self-organization	O
during	O
development	O
information	O
processing	O
systems	O
pp	O
morgan	O
kaufmann	O
montague	O
p	O
r	O
dayan	O
p	O
person	O
c	O
sejnowski	O
t	O
j	O
bee	O
foraging	O
in	O
uncertain	O
references	O
environments	O
using	O
predictive	O
hebbian	B
learning	O
nature	O
montague	O
p	O
r	O
dayan	O
p	O
sejnowski	O
t	O
j	O
a	O
framework	O
for	O
mesencephalic	O
dopamine	B
systems	O
based	O
on	O
predictive	O
hebbian	B
learning	O
the	O
journal	O
of	O
neuroscience	B
montague	O
p	O
r	O
dolan	O
r	O
j	O
friston	O
k	O
j	O
dayan	O
p	O
computational	O
psychiatry	O
trends	O
in	O
cognitive	O
sciences	O
montague	O
p	O
r	O
sejnowski	O
t	O
j	O
the	O
predictive	O
brain	O
temporal	O
coincidence	O
and	O
temporal	O
order	O
in	O
synaptic	O
learningmechanisms	O
learning	O
memory	O
moore	O
a	O
w	O
efficient	O
memory-based	O
learning	O
for	O
robot	O
control	B
ph	O
d	O
thesis	O
university	O
of	O
cambridge	O
moore	O
a	O
w	O
atkeson	O
c	O
g	O
prioritized	B
sweeping	I
reinforcement	B
learning	I
with	O
less	O
data	O
and	O
less	O
real	O
time	O
machine	O
learning	O
moore	O
a	O
w	O
schneider	O
j	O
deng	O
k	O
efficient	O
locally	O
weighted	O
polynomial	O
regression	O
predictions	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
morgan	O
kaufmann	O
moore	O
j	O
w	O
blazis	O
d	O
e	O
j	O
simulation	O
of	O
a	O
classically	O
conditioned	O
response	O
a	O
in	O
j	O
h	O
byrne	O
and	O
w	O
o	O
cerebellar	O
implementation	O
of	O
the	O
sutton-barto-desmond	O
model	O
berry	O
neural	B
models	O
of	O
plasticity	O
pp	O
academic	O
press	O
san	O
diego	O
ca	O
moore	O
j	O
w	O
choi	O
j	O
-s	O
brunzell	O
d	O
h	O
predictive	O
timing	O
under	O
temporal	O
uncertainty	O
in	O
d	O
a	O
rosenbaum	O
and	O
c	O
e	O
the	O
time	O
derivative	O
model	O
of	O
the	O
conditioned	O
response	O
collyer	O
timing	O
of	O
behavior	O
pp	O
mit	O
press	O
cambridge	O
ma	O
moore	O
j	O
w	O
desmond	O
j	O
e	O
berthier	O
n	O
e	O
blazis	O
e	O
j	O
sutton	O
r	O
s	O
barto	O
a	O
g	O
simulation	O
of	O
the	O
classically	O
conditioned	O
nictitating	O
membrane	O
response	O
by	O
a	O
neuronlike	O
adaptive	O
element	O
i	O
response	O
topography	O
neuronal	O
firing	O
and	O
interstimulus	O
intervals	O
behavioural	O
brain	O
research	O
moore	O
j	O
w	O
marks	O
j	O
s	O
castagna	O
v	O
e	O
polewan	O
r	O
j	O
parameter	O
stability	O
in	O
the	O
td	B
model	I
of	O
complex	O
cr	O
topographies	O
in	O
society	O
for	O
neuroscience	B
abstracts	O
moore	O
j	O
w	O
schmajuk	O
n	O
a	O
kamin	O
blocking	B
scholarpedia	O
moore	O
j	O
w	O
stickney	O
k	O
j	O
formation	O
of	O
attentional-associative	O
networks	O
in	O
real	O
timerole	O
of	O
the	O
hippocampus	O
and	O
implications	O
for	O
conditioning	O
physiological	O
psychology	B
mukundan	O
j	O
mart	O
nez	O
j	O
f	O
morse	O
multi-objective	O
reconfigurable	O
self-optimizing	O
memory	O
scheduler	O
in	O
ieee	O
international	O
symposium	O
on	O
high	O
performance	O
computer	O
architecture	O
pp	O
m	O
uller	O
m	O
computer	O
go	O
artificial	B
intelligence	I
munos	O
r	O
stepleton	O
t	O
harutyunyan	O
a	O
bellemare	O
m	O
safe	O
and	O
efficient	O
off-policy	B
reinforcement	B
learning	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
naddaf	O
y	O
game-independent	O
ai	O
agents	O
for	O
playing	O
atari	O
console	O
games	O
ph	O
d	O
thesis	O
university	O
of	O
alberta	O
edmonton	O
narendra	O
k	O
s	O
thathachar	O
m	O
a	O
l	O
learning	B
automata	I
a	O
survey	O
ieee	O
transac	O
tions	O
on	O
systems	O
man	O
and	O
cybernetics	B
narendra	O
k	O
s	O
thathachar	O
m	O
a	O
l	O
learning	B
automata	I
an	O
introduction	O
prentice	O
hall	O
englewood	O
cliffs	O
nj	O
narendra	O
k	O
s	O
wheeler	O
r	O
m	O
an	O
n-player	O
sequential	O
stochastic	O
game	O
with	O
identical	O
payoffs	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
narendra	O
k	O
s	O
wheeler	O
r	O
m	O
decentralized	O
learning	O
in	O
finite	O
markov	O
chains	O
ieee	O
references	O
transactions	O
on	O
automatic	O
control	B
nedi	O
c	O
a	O
bertsekas	O
d	O
p	O
least	O
squares	O
policy	B
evaluation	O
algorithms	O
with	O
linear	O
function	B
approximation	I
discrete	O
event	O
dynamic	O
systems	O
shaping	B
and	O
policy	B
search	O
in	O
reinforcement	B
learning	I
ng	O
a	O
y	O
ph	O
d	O
thesis	O
university	O
of	O
california	O
berkeley	O
ng	O
a	O
y	O
harada	O
d	O
russell	O
s	O
policy	B
invariance	O
under	O
reward	O
transformations	O
theory	O
and	O
application	O
to	O
reward	O
shaping	B
in	O
i	O
bratko	O
and	O
s	O
dzeroski	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
ng	O
a	O
y	O
russell	O
s	O
j	O
algorithms	O
for	O
inverse	B
reinforcement	B
learning	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
niv	O
y	O
reinforcement	B
learning	I
in	O
the	O
brain	O
journal	O
of	O
mathematical	O
psychology	B
niv	O
y	O
daw	O
n	O
d	O
dayan	O
p	O
how	O
fast	O
to	O
work	O
response	O
vigor	O
motivation	B
and	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
tonic	O
dopamine	B
pp	O
mit	O
press	O
cambridge	O
ma	O
niv	O
y	O
daw	O
n	O
d	O
joel	O
d	O
dayan	O
p	O
tonic	O
dopamine	B
opportunity	O
costs	O
and	O
the	O
control	B
of	O
response	O
vigor	O
psychopharmacology	O
niv	O
y	O
joel	O
d	O
dayan	O
p	O
a	O
normative	O
perspective	O
on	O
motivation	B
trends	O
in	O
cognitive	O
sciences	O
nouri	O
a	O
littman	O
m	O
l	O
multi-resolution	O
exploration	O
in	O
continuous	O
spaces	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
now	O
e	O
a	O
vrancx	O
p	O
hauwere	O
y	O
-m	O
d	O
game	B
theory	I
and	O
multi-agent	O
reinforcement	B
learning	I
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-theart	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
nutt	O
d	O
j	O
lingford-hughes	O
a	O
erritzoe	O
d	O
stokes	O
p	O
r	O
a	O
the	O
dopamine	B
theory	O
of	O
addiction	B
years	O
of	O
highs	O
and	O
lows	O
nature	O
reviews	O
neuroscience	B
o	O
doherty	O
j	O
p	O
dayan	O
p	O
friston	O
k	O
critchley	O
h	O
dolan	O
r	O
j	O
temporal	O
difference	O
models	O
and	O
reward-related	O
learning	O
in	O
the	O
human	O
brain	O
neuron	O
o	O
doherty	O
j	O
p	O
dayan	O
p	O
schultz	B
j	O
deichmann	O
r	O
friston	O
k	O
dolan	O
r	O
j	O
science	O
dissociable	O
roles	O
of	O
ventral	O
and	O
dorsal	O
striatum	O
in	O
instrumental	O
conditioning	O
olafsd	O
ottir	O
h	O
f	O
barry	O
c	O
saleem	O
a	O
b	O
hassabis	O
d	O
spiers	O
h	O
j	O
hippocampal	O
place	O
cells	O
construct	O
reward	O
related	O
sequences	O
through	O
unexplored	O
space	O
elife	O
oh	O
j	O
guo	O
x	O
lee	O
h	O
lewis	O
r	O
l	O
singh	O
s	O
action-conditional	O
video	O
prediction	B
using	O
deep	O
networks	O
in	O
atari	O
games	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
olds	O
j	O
milner	O
p	O
positive	O
reinforcement	O
produced	O
by	O
electrical	O
stimulation	O
of	O
the	O
septal	O
area	O
and	O
other	O
regions	O
of	O
rat	O
brain	O
journal	O
of	O
comparative	O
and	O
physiological	O
psychology	B
o	O
reilly	O
r	O
c	O
frank	O
m	O
j	O
making	O
working	O
memory	O
work	O
a	O
computational	O
model	O
of	O
learning	O
in	O
the	O
prefrontal	O
cortex	O
and	O
basal	B
ganglia	I
neural	B
computation	O
o	O
reilly	O
r	O
c	O
frank	O
m	O
j	O
hazy	O
t	O
e	O
watz	O
b	O
pvlv	O
the	O
primary	O
value	B
and	O
learned	O
value	B
pavlovian	O
learning	O
algorithm	O
behavioral	O
neuroscience	B
omohundro	O
s	O
m	O
efficient	O
algorithms	O
with	O
neural	B
network	O
behavior	O
technical	O
report	O
department	O
of	O
computer	O
science	O
university	O
of	O
illinois	O
at	O
urbana-champaign	O
ormoneit	O
d	O
sen	O
s	O
kernel-based	O
reinforcement	B
learning	I
machine	O
learning	O
references	O
oudeyer	O
p	O
-y	O
kaplan	O
f	O
what	O
is	O
intrinsic	B
motivation	B
a	O
typology	O
of	O
computational	O
approaches	O
frontiers	O
in	O
neurorobotics	O
oudeyer	O
p	O
-y	O
kaplan	O
f	O
hafner	O
v	O
v	O
intrinsic	B
motivation	B
systems	O
for	O
autonomous	O
mental	O
development	O
ieee	O
transactions	O
on	O
evolutionary	O
computation	O
padoa-schioppa	O
c	O
assad	O
j	O
a	O
neurons	O
in	O
the	O
orbitofrontal	O
cortex	O
encode	O
economic	O
value	B
nature	O
page	O
c	O
v	O
heuristics	O
for	O
signature	O
table	O
analysis	O
as	O
a	O
pattern	O
recognition	O
technique	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
pagnoni	O
g	O
zink	O
c	O
f	O
montague	O
p	O
r	O
berns	O
g	O
s	O
activity	O
in	O
human	O
ventral	O
striatum	O
locked	O
to	O
errors	O
of	O
reward	O
prediction	B
nature	O
neuroscience	B
pan	O
w	O
-x	O
schmidt	O
r	O
wickens	O
j	O
r	O
hyland	O
b	O
i	O
dopamine	B
cells	O
respond	O
to	O
predicted	O
events	O
during	O
classical	B
conditioning	I
evidence	O
for	O
eligibility	B
traces	I
in	O
the	O
rewardlearning	O
network	O
the	O
journal	O
of	O
neuroscience	B
park	O
j	O
kim	O
j	O
kang	O
d	O
an	O
rls-based	O
natural	O
actor	O
critic	O
algorithm	O
for	O
locomotion	O
of	O
a	O
two-linked	O
robot	O
arm	O
computational	O
intelligence	O
and	O
parks	O
p	O
c	O
militzer	O
j	O
improved	O
allocation	O
of	O
weights	O
for	O
associative	O
memory	O
storage	O
in	O
learning	O
control	B
systems	O
in	O
ifac	O
design	O
methods	O
of	O
control	B
systems	O
zurich	O
switzerland	O
pp	O
parr	O
r	O
hierarchical	O
control	B
and	O
learning	O
for	O
markov	O
decision	O
processes	O
ph	O
d	O
thesis	O
university	O
of	O
california	O
berkeley	O
parr	O
r	O
russell	O
s	O
approximating	O
optimal	O
policies	O
for	O
partially	O
observable	O
stochastic	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
international	O
joint	O
conference	O
on	O
artificial	O
domains	O
intelligence	O
pp	O
morgan	O
kaufmann	O
pavlov	B
p	O
i	O
conditioned	O
reflexes	O
oxford	O
university	O
press	O
london	O
pawlak	O
v	O
kerr	O
j	O
n	O
d	O
dopamine	B
receptor	O
activation	O
is	O
required	O
for	O
corticostriatal	O
spike-timing-dependent	O
plasticity	O
the	O
journal	O
of	O
neuroscience	B
pawlak	O
v	O
wickens	O
j	O
r	O
kirkwood	O
a	O
kerr	O
j	O
n	O
d	O
timing	O
is	O
not	O
everything	O
neuromodulation	O
opens	O
the	O
stdp	O
gate	O
frontiers	O
in	O
synaptic	O
neuroscience	B
pearce	O
j	O
m	O
hall	O
g	O
a	O
model	O
for	O
pavlovian	O
learning	O
variation	O
in	O
the	O
effectiveness	O
of	O
conditioning	O
but	O
not	O
unconditioned	O
stimuli	O
psychological	O
review	O
pearl	O
j	O
heuristics	O
intelligent	O
search	O
strategies	O
for	O
computer	O
problem	O
solving	O
addison-wesley	O
reading	O
ma	O
pearl	O
j	O
causal	O
diagrams	O
for	O
empirical	O
research	O
biometrika	O
pecevski	O
d	O
maass	O
w	O
legenstein	O
r	O
a	O
theoretical	O
analysis	O
of	O
learning	O
with	O
rewardmodulated	O
spike-timing-dependent	O
plasticity	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
peng	O
j	O
efficient	O
dynamic	O
programming-based	O
learning	O
for	O
control	B
ph	O
d	O
thesis	O
northeastern	O
university	O
boston	O
ma	O
peng	O
j	O
efficient	O
memory-based	O
dynamic	B
programming	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
peng	O
j	O
williams	O
r	O
j	O
efficient	O
learning	O
and	O
planning	B
within	O
the	O
dyna	O
framework	O
adaptive	O
behavior	O
peng	O
j	O
williams	O
r	O
j	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
san	O
francisco	O
incremental	O
multi-step	O
q-learning	B
references	O
peng	O
j	O
williams	O
r	O
j	O
incremental	O
multi-step	O
q-learning	B
machine	O
learning	O
perkins	O
t	O
j	O
pendrith	O
m	O
d	O
on	O
the	O
existence	O
of	O
fixed	O
points	O
for	B
q-learning	B
and	O
sarsa	B
in	O
partially	O
observable	O
domains	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
perkins	O
t	O
j	O
precup	O
d	O
a	O
convergent	O
form	O
of	O
approximate	O
policy	B
iteration	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
peters	O
j	O
b	O
uchel	O
c	O
neural	B
representations	O
of	O
subjective	O
reward	O
value	B
behavioral	O
brain	O
research	O
peters	O
j	O
schaal	O
s	O
natural	O
actor	O
critic	O
neurocomputing	O
peters	O
j	O
vijayakumar	O
s	O
schaal	O
s	O
natural	O
actor	O
critic	O
in	O
european	O
conference	O
on	O
machine	O
learning	O
pp	O
springer	O
berlin	O
heidelberg	O
pezzulo	O
g	O
van	O
der	O
meer	O
m	O
a	O
a	O
lansink	O
c	O
s	O
pennartz	O
c	O
m	O
a	O
internally	O
generated	O
sequences	O
in	O
learning	O
and	O
executing	O
goal-directed	O
behavior	O
trends	O
in	O
cognitive	O
science	O
pfeiffer	O
b	O
e	O
foster	O
d	O
j	O
hippocampal	O
place-cell	O
sequences	O
depict	O
future	O
paths	O
to	O
remembered	O
goals	O
nature	O
phansalkar	O
v	O
v	O
thathachar	O
m	O
a	O
l	O
local	O
and	O
global	O
optimization	O
algorithms	O
for	O
generalized	O
learning	B
automata	I
neural	B
computation	O
poggio	O
t	O
girosi	O
f	O
a	O
theory	O
of	O
networks	O
for	O
approximation	O
and	O
learning	O
a	O
i	O
memo	O
artificial	B
intelligence	I
laboratory	O
massachusetts	O
institute	O
of	O
technology	O
cambridge	O
ma	O
poggio	O
t	O
girosi	O
f	O
regularization	O
algorithms	O
for	O
learning	O
that	O
are	O
equivalent	O
to	O
multilayer	O
networks	O
science	O
polyak	O
b	O
t	O
new	O
stochastic	O
approximation	O
type	O
procedures	O
automat	O
i	O
telemekh	O
russian	O
polyak	O
b	O
t	O
juditsky	O
a	O
b	O
acceleration	O
of	O
stochastic	O
approximation	O
by	O
averaging	O
siam	O
journal	O
on	O
control	B
and	O
optimization	O
powell	O
m	O
j	O
d	O
radial	O
basis	O
functions	O
for	O
multivariate	O
interpolation	O
a	O
review	O
in	O
j	O
c	O
mason	O
and	O
m	O
g	O
cox	O
algorithms	O
for	O
approximation	O
pp	O
clarendon	O
press	O
oxford	O
powell	O
w	O
b	O
approximate	B
dynamic	B
programming	I
solving	O
the	O
curses	O
of	O
dimension	O
ality	O
second	O
edition	O
john	O
wiley	O
and	O
sons	O
powers	O
w	O
t	O
behavior	O
the	O
control	B
of	O
perception	O
aldine	O
de	O
gruyter	O
chicago	O
expanded	O
edition	O
precup	O
d	O
temporal	B
abstraction	I
in	O
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
precup	O
d	O
sutton	O
r	O
s	O
dasgupta	O
s	O
off-policy	B
temporal-difference	B
learning	I
with	B
function	B
approximation	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
precup	O
d	O
sutton	O
r	O
s	O
paduraru	O
c	O
koop	O
a	O
singh	O
s	O
off-policy	B
learning	O
with	B
options	B
and	O
recognizers	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
precup	O
d	O
sutton	O
r	O
s	O
singh	O
s	O
eligibility	B
traces	I
for	O
off-policy	B
policy	B
evaluation	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
references	O
puterman	O
m	O
l	O
markov	O
decision	O
problems	O
wiley	O
new	O
york	O
puterman	O
m	O
l	O
shin	O
m	O
c	O
modified	O
policy	B
iteration	I
algorithms	O
for	O
discounted	O
markov	O
decision	O
problems	O
management	O
science	O
quartz	O
s	O
dayan	O
p	O
montague	O
p	O
r	O
sejnowski	O
t	O
j	O
expectation	O
learning	O
in	O
the	O
brain	O
using	O
diffuse	O
ascending	O
connections	O
in	O
society	O
for	O
neuroscience	B
abstracts	O
randl	O
v	O
j	O
alstr	O
m	O
p	O
learning	O
to	O
drive	O
a	O
bicycle	O
using	O
reinforcement	B
learning	I
and	O
shaping	B
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
rangel	O
a	O
camerer	O
c	O
montague	O
p	O
r	O
a	O
framework	O
for	O
studying	O
the	O
neurobiology	O
of	O
value-based	O
decision	O
making	O
nature	O
reviews	O
neuroscience	B
rangel	O
a	O
hare	O
t	O
neural	B
computations	O
associated	O
with	O
goal-directed	O
choice	O
current	O
opinion	O
in	O
neurobiology	O
rao	O
r	O
p	O
sejnowski	O
t	O
j	O
spike-timing-dependent	O
hebbian	B
plasticity	O
as	O
temporal	O
difference	O
learning	O
neural	B
computation	O
ratcliff	O
r	O
connectionist	O
models	O
of	O
recognition	O
memory	O
constraints	O
imposed	O
by	O
learning	O
and	O
forgetting	O
functions	O
psychological	O
review	O
reddy	O
g	O
celani	O
a	O
sejnowski	O
t	O
j	O
vergassola	O
m	O
learning	O
to	O
soar	O
in	O
turbulent	O
environments	O
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
redish	O
d	O
a	O
addiction	B
as	O
a	O
computational	O
process	O
gone	O
awry	O
science	O
reetz	O
d	O
approximate	O
solutions	O
of	O
a	O
discounted	O
markovian	O
decision	O
process	O
bonner	O
mathematische	O
schriften	O
rescorla	O
r	O
a	O
wagner	O
a	O
r	O
a	O
theory	O
of	O
pavlovian	O
conditioning	O
variations	O
in	O
the	O
in	O
a	O
h	O
black	O
and	O
w	O
f	O
prokasy	O
effectiveness	O
of	O
reinforcement	O
and	O
nonreinforcement	O
classical	B
conditioning	I
ii	O
pp	O
appleton-century-crofts	O
new	O
york	O
revusky	O
s	O
garcia	O
j	O
learned	O
associations	O
over	O
long	O
delays	O
in	O
g	O
bower	O
the	O
psychology	B
of	O
learning	O
and	O
motivation	B
v	O
pp	O
academic	O
press	O
inc	O
new	O
york	O
reynolds	O
j	O
n	O
j	O
wickens	O
j	O
r	O
dopamine-dependent	O
plasticity	O
of	O
corticostriatal	O
synapses	O
neural	B
networks	O
ring	O
m	O
b	O
preparation	O
representing	O
knowledge	O
as	O
forecasts	O
state	B
as	O
knowledge	O
ripley	O
b	O
d	O
pattern	O
recognition	O
and	O
neural	B
networks	O
cambridge	O
university	O
press	O
rixner	O
s	O
memory	O
controller	O
optimizations	O
for	O
web	O
servers	O
in	O
proceedings	O
of	O
the	O
annual	O
ieeeacm	O
international	O
symposium	O
on	O
microarchitecture	O
p	O
ieee	O
computer	O
society	O
robbins	O
h	O
some	O
aspects	O
of	O
the	O
sequential	O
design	B
of	I
experiments	O
bulletin	O
of	O
the	O
american	O
mathematical	O
society	O
robertie	O
b	O
carbon	O
versus	O
silicon	O
matching	O
wits	O
with	O
td-gammon	B
inside	O
backgam	O
mon	O
romo	O
r	O
schultz	B
w	O
dopamine	B
neurons	O
of	O
the	O
monkey	O
midbrain	O
contingencies	O
of	O
responses	O
to	O
active	O
touch	O
during	O
self-initiated	O
arm	O
movements	O
journal	O
of	O
neurophysiology	O
rosenblatt	O
f	O
principles	O
of	O
neurodynamics	O
perceptrons	O
and	O
the	O
theory	O
of	O
brain	O
mechanisms	O
spartan	O
books	O
washington	O
dc	O
ross	O
s	O
introduction	O
to	O
stochastic	O
dynamic	B
programming	I
academic	O
press	O
new	O
york	O
ross	O
t	O
machines	O
that	O
think	O
scientific	O
american	O
rubinstein	O
r	O
y	O
simulation	O
and	O
the	O
monte	B
carlo	I
method	O
wiley	O
new	O
york	O
rumelhart	O
d	O
e	O
hinton	O
g	O
e	O
williams	O
r	O
j	O
learning	O
internal	O
representations	O
references	O
by	O
error	O
propagation	O
in	O
d	O
e	O
rumelhart	O
and	O
j	O
l	O
mcclelland	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
vol	O
i	O
foundations	O
bradfordmit	O
press	O
cambridge	O
ma	O
rummery	O
g	O
a	O
problem	O
solving	O
with	O
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
cambridge	O
rummery	O
g	O
a	O
niranjan	O
m	O
on-line	O
q-learning	B
using	O
connectionist	O
systems	O
technical	O
report	O
cuedf-infengtr	O
engineering	O
department	O
cambridge	O
university	O
ruppert	O
d	O
efficient	O
estimations	O
from	O
a	O
slowly	O
convergent	O
robbins-monro	O
process	O
cornell	O
university	O
operations	O
research	O
and	O
industrial	O
engineering	O
technical	O
report	O
no	O
russell	O
s	O
norvig	O
p	O
artificial	B
intelligence	I
a	O
modern	O
approach	O
edition	O
prentice	O
hall	O
englewood	O
cliffs	O
nj	O
rust	O
j	O
numerical	O
dynamic	B
programming	I
in	O
economics	O
in	O
h	O
amman	O
d	O
kendrick	O
and	O
j	O
rust	O
handbook	O
of	O
computational	O
economics	O
pp	O
elsevier	O
amsterdam	O
saddoris	O
m	O
p	O
cacciapaglia	O
f	O
wightmman	O
r	O
m	O
carelli	O
r	O
m	O
differential	B
dopamine	B
release	O
dynamics	O
in	O
the	O
nucleus	O
accumbens	O
core	O
and	O
shell	O
reveal	O
complementary	O
signals	O
for	O
error	O
prediction	B
and	O
incentive	O
motivation	B
the	O
journal	O
of	O
neuroscience	B
saksida	O
l	O
m	O
raymond	O
s	O
m	O
touretzky	O
d	O
s	O
shaping	B
robot	O
behavior	O
using	O
principles	O
from	O
instrumental	O
conditioning	O
robotics	O
and	O
autonomous	O
systems	O
samuel	O
a	O
l	O
some	O
studies	O
in	O
machine	O
learning	O
using	O
the	O
game	O
of	O
checkers	O
ibm	O
journal	O
on	O
research	O
and	O
development	O
reprinted	O
in	O
e	O
a	O
feigenbaum	O
and	O
j	O
feldman	O
computers	O
and	O
thought	O
pp	O
mcgraw-hill	O
new	O
york	O
samuel	O
a	O
l	O
some	O
studies	O
in	O
machine	O
learning	O
using	O
the	O
game	O
of	O
checkers	O
ii	O
recent	O
progress	O
ibm	O
journal	O
on	O
research	O
and	O
development	O
schaal	O
s	O
atkeson	O
c	O
g	O
robot	O
juggling	O
implementation	O
of	O
memory-based	O
learning	O
ieee	O
control	B
systems	O
schmajuk	O
n	O
a	O
computational	O
models	O
of	O
classical	B
conditioning	I
scholarpedia	O
schmidhuber	O
j	O
curious	O
model-building	O
control	B
systems	O
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
joint	O
conference	O
on	O
neural	B
networks	O
pp	O
ieee	O
schmidhuber	O
j	O
a	O
possibility	O
for	O
implementing	O
curiosity	B
and	O
boredom	O
in	O
modelin	O
from	O
animals	O
to	O
animats	O
proceedings	O
of	O
the	O
first	O
mit	O
press	O
building	O
neural	B
controllers	O
ternational	O
conference	O
on	O
simulation	O
of	O
adaptive	O
behavior	O
pp	O
cambridge	O
ma	O
schmidhuber	O
j	O
deep	B
learning	I
in	O
neural	B
networks	O
an	O
overview	O
neural	B
networks	O
schmidhuber	O
j	O
storck	O
j	O
hochreiter	O
s	O
reinforcement	O
driven	O
information	O
acquisition	O
in	O
nondeterministic	O
environments	O
technical	O
report	O
fakult	O
at	O
f	O
ur	O
informatik	O
technische	O
universit	O
at	O
m	O
unchen	O
m	O
unchen	O
germany	O
schraudolph	O
n	O
n	O
local	O
gain	O
adaptation	O
in	O
stochastic	O
gradient	B
descent	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
artificial	B
neural	B
networks	I
pp	O
ieee	O
london	O
schraudolph	O
n	O
n	O
fast	O
curvature	O
matrix-vector	O
products	O
for	O
second-order	O
gradient	B
descent	O
neural	B
computation	O
schraudolph	O
n	O
n	O
yu	O
j	O
aberdeen	O
d	O
fast	O
online	B
policy	B
gradient	B
learning	O
with	O
smd	O
gain	O
vector	B
adaptation	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
references	O
schultz	B
d	O
g	O
melsa	O
j	O
l	O
state	B
functions	O
and	O
linear	O
control	B
systems	O
mcgraw-hill	O
new	O
york	O
schultz	B
w	O
predictive	O
reward	B
signal	I
of	O
dopamine	B
neurons	O
journal	O
of	O
neurophysiology	O
schultz	B
w	O
apicella	O
p	O
ljungberg	O
t	O
responses	O
of	O
monkey	O
dopamine	B
neurons	O
to	O
reward	O
and	O
conditioned	O
stimuli	O
during	O
successive	O
steps	O
of	O
learning	O
a	O
delayed	O
response	O
task	O
the	O
journal	O
of	O
neuroscience	B
schultz	B
w	O
dayan	O
p	O
montague	O
p	O
r	O
a	O
neural	B
substrate	O
of	O
prediction	B
and	O
reward	O
science	O
schultz	B
w	O
romo	O
r	O
dopamine	B
neurons	O
of	O
the	O
monkey	O
midbrain	O
contingencies	O
of	O
responses	O
to	O
stimuli	O
eliciting	O
immediate	O
behavioral	O
reactions	O
journal	O
of	O
neurophysiology	O
schultz	B
w	O
romo	O
r	O
ljungberg	O
t	O
mirenowicz	O
j	O
hollerman	O
j	O
r	O
dickinson	O
a	O
reward-related	O
signals	O
carried	O
by	O
dopamine	B
neurons	O
in	O
j	O
c	O
houk	O
j	O
l	O
davis	O
and	O
d	O
g	O
beiser	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	B
ganglia	I
pp	O
mit	O
press	O
cambridge	O
ma	O
schwartz	O
a	O
a	O
reinforcement	B
learning	I
method	O
for	O
maximizing	O
undiscounted	O
rewards	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
schweitzer	O
p	O
j	O
seidmann	O
a	O
generalized	O
polynomial	O
approximations	O
in	O
markovian	O
decision	O
processes	O
journal	O
of	O
mathematical	O
analysis	O
and	O
applications	O
selfridge	O
o	O
g	O
tracking	O
and	O
trailing	O
adaptation	O
in	O
movement	O
strategies	O
technical	O
report	O
bolt	O
beranek	O
and	O
newman	O
inc	O
unpublished	O
report	O
selfridge	O
o	O
g	O
some	O
themes	O
and	O
primitives	O
in	O
ill-defined	O
systems	O
in	O
o	O
g	O
selfridge	O
e	O
l	O
rissland	O
and	O
m	O
a	O
arbib	O
adaptive	O
control	B
of	O
ill-defined	O
systems	O
pp	O
plenum	O
press	O
ny	O
proceedings	O
of	O
the	O
nato	O
advanced	O
research	O
institute	O
on	O
adaptive	O
control	B
of	O
ill-defined	O
systems	O
nato	O
conference	O
series	O
ii	O
systems	O
science	O
vol	O
selfridge	O
o	O
j	O
sutton	O
r	O
s	O
barto	O
a	O
g	O
training	O
and	O
tracking	O
in	O
robotics	O
in	O
a	O
joshi	O
proceedings	O
of	O
the	O
ninth	O
international	O
joint	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
morgan	O
kaufmann	O
seo	O
h	O
barraclough	O
d	O
lee	O
d	O
dynamic	O
signals	O
related	O
to	O
choices	O
and	O
outcomes	O
in	O
the	O
dorsolateral	O
prefrontal	O
cortex	O
cerebral	O
cortex	O
seung	O
h	O
s	O
learning	O
in	O
spiking	O
neural	B
networks	O
by	O
reinforcement	O
of	O
stochastic	O
synaptic	O
transmission	O
neuron	O
shah	O
a	O
psychological	O
and	O
neuroscientific	O
connections	O
with	O
reinforcement	B
learning	I
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
shannon	B
c	O
e	O
programming	O
a	O
computer	O
for	O
playing	O
chess	B
philosophical	O
magazine	O
and	O
journal	O
of	O
science	O
shannon	B
c	O
e	O
presentation	O
of	O
a	O
maze-solving	O
machine	O
in	O
h	O
v	O
forester	O
cyber	O
netics	O
transactions	O
of	O
the	O
eighth	O
conference	O
pp	O
josiah	O
macy	O
jr	O
foundation	O
shannon	B
c	O
e	O
theseus	O
maze-solving	O
mouse	O
shelton	O
c	O
r	O
importance	B
sampling	I
for	O
reinforcement	B
learning	I
with	O
multiple	O
objec	O
tives	O
ph	O
d	O
thesis	O
massachusetts	O
institute	O
of	O
technology	O
cambridge	O
ma	O
shepard	O
d	O
a	O
two-dimensional	O
interpolation	O
function	O
for	O
irregularly-spaced	O
data	O
in	O
references	O
proceedings	O
of	O
the	O
acm	O
national	O
conference	O
pp	O
acm	O
new	O
york	O
sherman	O
j	O
morrison	O
w	O
j	O
adjustment	O
of	O
an	O
inverse	O
matrix	O
corresponding	O
to	O
changes	O
in	O
the	O
elements	O
of	O
a	O
given	O
column	O
or	O
a	O
given	O
row	O
of	O
the	O
original	O
matrix	O
annals	O
of	O
mathematical	O
statistics	O
shewchuk	O
j	O
dean	O
t	O
towards	O
learning	O
time-varying	O
functions	O
with	O
high	O
input	O
in	O
proceedings	O
of	O
the	O
fifth	O
ieee	O
international	O
symposium	O
on	O
intelligent	O
dimensionality	O
control	B
pp	O
ieee	O
computer	O
society	O
press	O
los	O
alamitos	O
ca	O
shimansky	O
y	O
p	O
biologically	O
plausible	O
learning	O
in	O
neural	B
networks	O
a	O
lesson	O
from	O
bacterial	O
chemotaxis	O
biological	O
cybernetics	B
si	O
j	O
barto	O
a	O
powell	O
w	O
wunsch	O
d	O
handbook	O
of	O
learning	O
and	O
approximate	B
dynamic	B
programming	I
john	O
wiley	O
and	O
sons	O
silver	O
d	O
reinforcement	B
learning	I
and	O
simulation	O
based	O
search	O
in	O
the	O
game	O
of	O
go	O
ph	O
d	O
thesis	O
university	O
of	O
alberta	O
edmonton	O
silver	O
d	O
huang	O
a	O
maddison	O
c	O
j	O
guez	O
a	O
sifre	O
l	O
van	O
den	O
driessche	O
g	O
schrittwieser	O
j	O
antonoglou	O
i	O
panneershelvam	O
v	O
lanctot	O
m	O
dieleman	O
s	O
grewe	O
d	O
nham	O
j	O
kalchbrenner	O
n	O
sutskever	O
i	O
lillicrap	O
t	O
leach	O
m	O
kavukcuoglu	O
k	O
graepel	O
t	O
hassabis	O
d	O
mastering	O
the	O
game	O
of	O
go	O
with	O
deep	O
neural	B
networks	O
and	O
tree	O
search	O
nature	O
silver	O
d	O
lever	O
g	O
heess	O
n	O
degris	O
t	O
wierstra	O
d	O
riedmiller	O
m	O
deterministic	O
policy	B
gradient	B
algorithms	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
silver	O
d	O
schrittwieser	O
j	O
simonyan	O
k	O
antonoglou	O
i	O
huang	O
a	O
guez	O
a	O
hubert	O
t	O
baker	O
l	O
lai	O
m	O
bolton	O
a	O
chen	O
y	O
lillicrap	O
l	O
hui	O
f	O
sifre	O
l	O
van	O
den	O
driessche	O
g	O
graepel	O
t	O
hassibis	O
d	O
mastering	O
the	O
game	O
of	O
go	O
without	O
human	O
knowledge	O
nature	O
silver	O
d	O
hubert	O
t	O
schrittwieser	O
j	O
antonoglou	O
i	O
lai	O
m	O
guez	O
a	O
lanctot	O
m	O
sifre	O
l	O
kumaran	O
d	O
graepel	O
t	O
lillicrap	O
t	O
simoyan	O
k	O
hassibis	O
d	O
mastering	O
chess	B
and	O
shogi	O
by	O
self-play	O
with	O
a	O
general	O
reinforcement	B
learning	I
algorithm	O
s	O
im	O
sek	O
o	O
alg	O
orta	O
s	O
kothiyal	O
a	O
why	O
most	O
decisions	O
are	O
easy	O
in	O
tetris	O
and	O
perhaps	O
in	O
other	O
sequential	O
decision	O
problems	O
as	O
well	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
simon	O
h	O
lecture	O
at	O
the	O
earthware	O
symposium	O
carnegie	O
mellon	O
university	O
httpswww	O
youtube	O
comwatch	O
vezhyi	O
singh	O
s	O
p	O
reinforcement	B
learning	I
with	O
a	O
hierarchy	O
of	O
abstract	O
models	O
in	O
proceedings	O
of	O
the	O
tenth	O
national	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
aaaimit	O
press	O
menlo	O
park	O
ca	O
singh	O
s	O
p	O
scaling	O
reinforcement	B
learning	I
algorithms	O
by	O
learning	O
variable	O
temporal	O
resolution	O
models	O
in	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
singh	O
s	O
p	O
learning	O
to	O
solve	O
markovian	O
decision	O
processes	O
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
singh	O
s	O
p	O
special	O
double	B
issue	O
on	O
reinforcement	B
learning	I
machine	O
learning	O
singh	O
s	O
barto	O
a	O
g	O
chentanez	O
n	O
intrinsically	O
motivated	O
reinforcement	B
learning	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
singh	O
s	O
p	O
bertsekas	O
d	O
reinforcement	B
learning	I
for	O
dynamic	O
channel	O
allocation	O
in	O
references	O
cellular	O
telephone	O
systems	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
singh	O
s	O
p	O
jaakkola	O
t	O
jordan	O
m	O
i	O
learning	O
without	O
state-estimation	O
in	O
parin	O
proceedings	O
of	O
the	O
international	O
tially	O
observable	O
markovian	O
decision	O
problems	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
singh	O
s	O
jaakkola	O
t	O
littman	O
m	O
l	O
szepesvri	O
c	O
convergence	O
results	O
for	O
single-step	O
on-policy	O
reinforcement-learning	O
algorithms	O
machine	O
learning	O
singh	O
s	O
p	O
jaakkola	O
t	O
jordan	O
m	O
i	O
reinforcement	B
learning	I
with	O
soft	O
state	B
aggregation	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
singh	O
s	O
lewis	O
r	O
l	O
barto	O
a	O
g	O
where	O
do	O
rewards	O
come	O
from	O
in	O
n	O
taatgen	O
and	O
h	O
van	O
rijn	O
proceedings	O
of	O
the	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
pp	O
cognitive	O
science	O
society	O
singh	O
s	O
lewis	O
r	O
l	O
barto	O
a	O
g	O
sorg	O
j	O
intrinsically	O
motivated	O
reinforcement	B
learning	I
an	O
evolutionary	O
perspective	O
ieee	O
transactions	O
on	O
autonomous	O
mental	O
development	O
special	O
issue	O
on	O
active	O
learning	O
and	O
intrinsically	O
motivated	O
exploration	O
in	O
robots	O
advances	O
and	O
challenges	O
singh	O
s	O
p	O
sutton	O
r	O
s	O
reinforcement	B
learning	I
with	O
replacing	B
eligibility	B
traces	I
machine	O
learning	O
skinner	B
b	O
f	O
the	O
behavior	O
of	O
organisms	O
an	O
experimental	O
analysis	O
appleton	O
century	O
new	O
york	O
skinner	B
b	O
f	O
reinforcement	O
today	O
american	O
psychologist	O
skinner	B
b	O
f	O
operant	O
behavior	O
american	O
psychologist	O
sofge	O
d	O
a	O
white	O
d	O
a	O
applied	O
learning	O
optimal	B
control	B
for	O
manufacturing	O
in	O
d	O
a	O
white	O
and	O
d	O
a	O
sofge	O
handbook	O
of	O
intelligent	O
control	B
neural	B
fuzzy	O
and	O
adaptive	O
approaches	O
pp	O
van	O
nostrand	O
reinhold	O
new	O
york	O
sorg	O
j	O
d	O
the	O
optimal	O
reward	O
problemdesigning	O
effective	O
reward	O
for	O
bounded	O
agents	O
ph	O
d	O
thesis	O
university	O
of	O
michigan	O
ann	O
arbor	O
sorg	O
j	O
lewis	O
r	O
l	O
singh	O
s	O
p	O
reward	O
design	O
via	O
online	B
gradient	B
ascent	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
sorg	O
j	O
singh	O
s	O
lewis	O
r	O
internal	O
rewards	O
mitigate	O
agent	O
boundedness	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
spence	O
k	O
w	O
the	O
role	O
of	O
secondary	B
reinforcement	I
in	O
delayed	B
reward	I
learning	O
psy	O
chological	O
review	O
srivastava	O
n	O
hinton	O
g	O
krizhevsky	O
a	O
sutskever	O
i	O
salakhutdinov	O
r	O
dropout	O
a	O
simple	O
way	O
to	O
prevent	O
neural	B
networks	O
from	O
overfitting	O
journal	O
of	O
machine	O
learning	O
research	O
staddon	O
j	O
e	O
r	O
adaptive	O
behavior	O
and	O
learning	O
cambridge	O
university	O
press	O
stanfill	O
c	O
waltz	O
d	O
toward	O
memory-based	O
reasoning	O
communications	O
of	O
the	O
acm	O
steinberg	O
e	O
e	O
keiflin	O
r	O
boivin	O
j	O
r	O
witten	B
i	O
b	O
deisseroth	O
k	O
janak	O
p	O
h	O
a	O
causal	O
link	O
between	O
prediction	B
errors	O
dopamine	B
neurons	O
and	O
learning	O
nature	O
neuroscience	B
sterling	O
p	O
laughlin	O
s	O
principles	O
of	O
neural	B
design	O
mit	O
press	O
cambridge	O
ma	O
sugiyama	O
m	O
hachiya	O
h	O
morimura	O
t	O
statistical	O
reinforcement	B
learning	I
modern	O
machine	O
learning	O
approaches	O
chapman	O
hallcrc	O
references	O
suri	O
r	O
e	O
bargas	O
j	O
arbib	O
m	O
a	O
modeling	O
functions	O
of	O
striatal	O
dopamine	B
modulation	O
in	O
learning	O
and	O
planning	B
neuroscience	B
suri	O
r	O
e	O
schultz	B
w	O
learning	O
of	O
sequential	O
movements	O
by	O
neural	B
network	O
model	O
with	O
dopamine-like	O
reinforcement	B
signal	I
experimental	O
brain	O
research	O
suri	O
r	O
e	O
schultz	B
w	O
a	O
neural	B
network	O
model	O
with	O
dopamine-like	O
reinforcement	B
signal	I
that	O
learns	O
a	O
spatial	O
delayed	O
response	O
task	O
neuroscience	B
sutton	O
r	O
s	O
unpublished	O
report	O
learning	O
theory	O
support	O
for	O
a	O
single	O
channel	O
theory	O
of	O
the	O
brain	O
sutton	O
r	O
s	O
single	O
channel	O
theory	O
a	O
neuronal	O
theory	O
of	O
learning	O
brain	O
theory	O
newsletter	O
center	O
for	O
systems	O
neuroscience	B
university	O
of	O
massachusetts	O
amherst	O
ma	O
sutton	O
r	O
s	O
a	O
unified	O
theory	O
of	O
expectation	O
in	O
classical	O
and	O
instrumental	O
conditioning	O
bachelors	O
thesis	O
stanford	O
university	O
sutton	O
r	O
s	O
temporal	O
credit	B
assignment	I
in	O
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
sutton	O
r	O
s	O
learning	O
to	O
predict	O
by	O
the	O
method	O
of	O
temporal	O
differences	O
machine	O
learning	O
erratum	O
p	O
sutton	O
r	O
s	O
integrated	O
architectures	O
for	O
learning	O
planning	B
and	O
reacting	O
based	O
on	O
approximating	O
dynamic	B
programming	I
in	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
sutton	O
r	O
s	O
dyna	O
an	O
integrated	O
architecture	O
for	O
learning	O
planning	B
and	O
reacting	O
sigart	O
bulletin	O
acm	O
new	O
york	O
sutton	O
r	O
s	O
planning	B
by	O
incremental	O
dynamic	B
programming	I
in	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
sutton	O
r	O
s	O
reinforcement	B
learning	I
kluwer	O
academic	O
press	O
reprinting	O
of	O
a	O
special	O
double	B
issue	O
on	O
reinforcement	B
learning	I
machine	O
learning	O
sutton	O
r	O
s	O
adapting	O
bias	O
by	O
gradient	B
descent	O
an	O
incremental	O
version	O
of	O
delta-bardelta	O
proceedings	O
of	O
the	O
tenth	O
national	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
mit	O
press	O
sutton	O
r	O
s	O
gain	O
adaptation	O
beats	O
least	O
squares	O
proceedings	O
of	O
the	O
seventh	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
pp	O
yale	O
university	O
new	O
haven	O
ct	O
sutton	O
r	O
s	O
td	B
models	O
modeling	O
the	O
world	O
at	O
a	O
mixture	O
of	O
time	O
scales	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
sutton	O
r	O
s	O
on	O
the	O
virtues	O
of	O
linear	O
learning	O
and	O
trajectory	O
distributions	O
in	O
proceedings	O
of	O
the	O
workshop	O
on	O
value	B
function	B
approximation	I
at	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
sutton	O
r	O
s	O
generalization	O
in	O
reinforcement	B
learning	I
successful	O
examples	O
using	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
sparse	B
coarse	B
coding	I
pp	O
mit	O
press	O
cambridge	O
ma	O
sutton	O
r	O
s	O
the	O
grand	O
challenge	O
of	O
predictive	O
empirical	O
abstract	O
knowledge	O
working	O
notes	O
of	O
the	O
workshop	O
on	O
grand	O
challenges	O
for	O
reasoning	O
from	O
experiences	O
sutton	O
r	O
s	O
introduction	O
to	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
tutorial	O
at	O
the	O
conference	O
on	O
neural	B
information	O
processing	O
systems	O
montreal	O
december	O
sutton	O
r	O
s	O
true	B
online	B
emphatic	O
td	B
quick	O
reference	O
and	O
implementation	O
guide	O
references	O
code	O
is	O
available	O
in	O
python	O
and	O
c	O
by	O
downloading	O
the	O
source	O
files	O
of	O
this	O
arxiv	O
paper	O
as	O
a	O
zip	O
archive	O
sutton	O
r	O
s	O
barto	O
a	O
g	O
toward	O
a	O
modern	O
theory	O
of	O
adaptive	O
networks	O
expectation	O
and	O
prediction	B
psychological	O
review	O
sutton	O
r	O
s	O
barto	O
a	O
g	O
an	O
adaptive	O
network	O
that	O
constructs	O
and	O
uses	O
an	O
internal	O
model	O
of	O
its	O
world	O
cognition	O
and	O
brain	O
theory	O
sutton	O
r	O
s	O
barto	O
a	O
g	O
a	O
temporal-difference	O
model	O
of	O
classical	B
conditioning	I
in	O
proceedings	O
of	O
the	O
ninth	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
pp	O
erlbaum	O
hillsdale	O
nj	O
sutton	O
r	O
s	O
barto	O
a	O
g	O
time-derivative	O
models	O
of	O
pavlovian	O
reinforcement	O
in	O
m	O
gabriel	O
and	O
j	O
moore	O
learning	O
and	O
computational	O
neuroscience	B
foundations	O
of	O
adaptive	O
networks	O
pp	O
mit	O
press	O
cambridge	O
ma	O
sutton	O
r	O
s	O
maei	O
h	O
r	O
precup	O
d	O
bhatnagar	O
s	O
silver	O
d	O
szepesv	O
ari	O
cs	O
wiewiora	O
e	O
fast	O
gradient-descent	O
methods	O
for	O
temporal-difference	B
learning	I
with	O
linear	O
function	B
approximation	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
acm	O
new	O
york	O
sutton	O
r	O
s	O
szepesv	O
ari	O
cs	O
maei	O
h	O
r	O
a	O
convergent	O
temporal-difference	O
algorithm	O
for	O
off-policy	B
learning	O
with	O
linear	O
function	B
approximation	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
curran	O
associates	O
inc	O
sutton	O
r	O
s	O
mahmood	O
a	O
r	O
precup	O
d	O
van	O
hasselt	O
h	O
a	O
new	O
q	O
with	O
interim	O
forward	O
view	O
and	O
monte	B
carlo	I
equivalence	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
jmlr	O
wcp	O
sutton	O
r	O
s	O
mahmood	O
a	O
r	O
white	O
m	O
an	O
emphatic	O
approach	O
to	O
the	O
problem	O
of	O
off-policy	B
temporal-difference	B
learning	I
journal	O
of	O
machine	O
learning	O
research	O
sutton	O
r	O
s	O
mcallester	O
d	O
a	O
singh	O
s	O
p	O
mansour	O
y	O
policy	B
gradient	B
methods	I
for	O
reinforcement	B
learning	I
with	B
function	B
approximation	I
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
sutton	O
r	O
s	O
modayil	O
j	O
delp	O
m	O
degris	O
t	O
pilarski	O
p	O
m	O
white	O
a	O
precup	O
d	O
horde	O
a	O
scalable	O
real-time	O
architecture	O
for	O
learning	O
knowledge	O
from	O
unsupervised	O
sensorimotor	O
interaction	O
in	O
proceedings	O
of	O
the	O
tenth	O
international	O
conference	O
on	O
autonomous	O
agents	O
and	O
multiagent	O
systems	O
pp	O
taipei	O
taiwan	O
sutton	O
r	O
s	O
pinette	O
b	O
the	O
learning	O
of	O
world	O
models	O
by	O
connectionist	O
networks	O
in	O
proceedings	O
of	O
the	O
seventh	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
pp	O
sutton	O
r	O
s	O
precup	O
d	O
singh	O
s	O
between	O
mdps	O
and	O
semi-mdps	O
a	O
framework	O
for	O
temporal	B
abstraction	I
in	O
reinforcement	B
learning	I
artificial	B
intelligence	I
sutton	O
r	O
s	O
singh	O
s	O
p	O
mcallester	O
d	O
a	O
comparing	O
policy-gradient	O
algorithms	O
unpublished	O
manuscript	O
sutton	O
r	O
s	O
szepesv	O
ari	O
cs	O
geramifard	O
a	O
bowling	O
m	O
dyna-style	O
planning	B
in	O
proceedings	O
of	O
the	O
with	O
linear	O
function	B
approximation	I
and	O
prioritized	B
sweeping	I
conference	O
on	O
uncertainty	O
in	O
artificial	B
intelligence	I
pp	O
szepesv	O
ari	O
cs	O
algorithms	O
for	O
reinforcement	B
learning	I
in	O
synthesis	O
lectures	O
on	O
artifi	O
cial	O
intelligence	O
and	O
machine	O
learning	O
morgan	O
and	O
claypool	O
szita	O
i	O
reinforcement	B
learning	I
in	O
games	O
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
tadepalli	O
p	O
ok	O
d	O
h-learning	O
a	O
reinforcement	B
learning	I
method	O
to	O
optimize	O
undiscounted	O
average	O
reward	O
technical	O
report	O
oregon	O
state	B
university	O
computer	O
science	O
department	O
corvallis	O
references	O
tadepalli	O
p	O
ok	O
d	O
scaling	O
up	O
average	O
reward	O
reinforcement	B
learning	I
by	O
approximating	O
the	O
domain	O
models	O
and	O
the	O
value	B
function	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
takahashi	O
y	O
schoenbaum	O
g	O
and	O
niv	O
y	O
silencing	O
the	O
critics	O
understanding	O
the	O
effects	O
of	O
cocaine	O
sensitization	O
on	O
dorsolateral	O
and	O
ventral	O
striatum	O
in	O
the	O
context	O
of	O
an	O
actorcritic	O
model	O
frontiers	O
in	B
neuroscience	B
tambe	O
m	O
newell	O
a	O
rosenbloom	O
p	O
s	O
the	O
problem	O
of	O
expensive	O
chunks	O
and	O
its	O
solution	O
by	O
restricting	O
expressiveness	O
machine	O
learning	O
tan	O
m	O
learning	O
a	O
cost-sensitive	O
internal	O
representation	O
for	O
reinforcement	B
learning	I
in	O
l	O
a	O
birnbaum	O
and	O
g	O
c	O
collins	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pp	O
morgan	O
kaufmann	O
taylor	O
g	O
parr	O
r	O
kernelized	O
value	B
function	B
approximation	I
for	O
reinforcement	B
learning	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
acm	O
new	O
york	O
taylor	O
m	O
e	O
stone	O
p	O
transfer	O
learning	O
for	O
reinforcement	B
learning	I
domains	O
a	O
survey	O
journal	O
of	O
machine	O
learning	O
research	O
tesauro	O
g	O
simple	O
neural	B
models	O
of	O
classical	B
conditioning	I
biological	O
cybernetics	B
tesauro	O
g	O
practical	O
issues	O
in	O
temporal	O
difference	O
learning	O
machine	O
learning	O
tesauro	O
g	O
td-gammon	B
a	O
self-teaching	O
backgammon	B
program	O
achieves	O
master-level	O
play	O
neural	B
computation	O
tesauro	O
g	O
temporal	O
difference	O
learning	O
and	O
td-gammon	B
communications	O
of	O
the	O
acm	O
tesauro	O
g	O
programming	O
backgammon	B
using	O
self-teaching	O
neural	B
nets	O
artificial	B
intelligence	I
tesauro	O
g	O
galperin	O
g	O
r	O
on-line	O
policy	B
improvement	I
using	O
monte-carlo	O
search	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
pp	O
mit	O
press	O
cambridge	O
ma	O
tesauro	O
g	O
gondek	O
d	O
c	O
lechner	O
j	O
fan	O
j	O
prager	O
j	O
m	O
simulation	O
learning	O
ibm	O
journal	O
of	O
research	O
and	O
and	O
optimization	O
techniques	O
in	O
watson	O
s	O
game	O
strategies	O
development	O
tesauro	O
g	O
gondek	O
d	O
c	O
lenchner	O
j	O
fan	O
j	O
prager	O
j	O
m	O
analysis	O
of	O
watson	O
s	O
strategies	O
for	O
playing	O
jeopardy	O
journal	O
of	O
artificial	B
intelligence	I
research	O
tham	O
c	O
k	O
modular	O
on-line	O
function	B
approximation	I
for	O
scaling	O
up	O
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
cambridge	O
thathachar	O
m	O
a	O
l	O
sastry	O
p	O
s	O
a	O
new	O
approach	O
to	O
the	O
design	B
of	I
reinforcement	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
schemes	O
for	O
learning	B
automata	I
thathachar	O
m	O
sastry	O
p	O
s	O
varieties	O
of	O
learning	B
automata	I
an	O
overview	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
part	O
b	O
cybernetics	B
thathachar	O
m	O
sastry	O
p	O
s	O
networks	O
of	O
learning	B
automata	I
techniques	O
for	O
online	B
stochastic	O
optimization	O
springer	O
science	O
business	O
media	O
theocharous	O
g	O
thomas	O
p	O
s	O
ghavamzadeh	O
m	O
personalized	O
ad	O
recommendation	O
for	O
life-time	O
value	B
optimization	O
guarantees	O
in	O
proceedings	O
of	O
the	O
twenty-fourth	O
international	O
joint	O
conference	O
on	O
artificial	B
intelligence	I
aaai	O
press	O
palo	O
alto	O
ca	O
thistlethwaite	O
d	O
a	O
critical	O
review	O
of	O
latent	B
learning	I
and	O
related	O
experiments	O
psy	O
references	O
chological	O
bulletin	O
thomas	O
p	O
s	O
bias	O
in	O
natural	O
actor	O
critic	O
algorithms	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
jmlr	O
wcp	O
pp	O
thomas	O
p	O
s	O
safe	O
reinforcement	B
learning	I
ph	O
d	O
thesis	O
university	O
of	O
massachusetts	O
amherst	O
thomas	O
p	O
s	O
theocharous	O
g	O
ghavamzadeh	O
m	O
high-confidence	O
off-policy	B
evalin	O
proceedings	O
of	O
the	O
twenty-ninth	O
aaai	O
conference	O
on	O
artificial	B
intelligence	I
uation	O
pp	O
aaai	O
press	O
menlo	O
park	O
ca	O
thompson	O
w	O
r	O
on	O
the	O
likelihood	O
that	O
one	O
unknown	O
probability	O
exceeds	O
another	O
in	O
view	O
of	O
the	O
evidence	O
of	O
two	O
samples	O
biometrika	O
thompson	O
w	O
r	O
on	O
the	O
theory	O
of	O
apportionment	O
american	O
journal	O
of	O
mathematics	O
thon	O
m	O
spectral	O
learning	O
of	O
sequential	O
systems	O
ph	O
d	O
thesis	O
jacobs	O
university	O
bremen	O
thon	O
m	O
jaeger	O
h	O
links	O
between	O
multiplicity	O
automata	O
observable	O
operator	O
models	O
and	O
predictive	B
state	B
representations	I
a	O
unified	O
learning	O
framework	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
thorndike	O
e	O
l	O
animal	O
intelligence	O
an	O
experimental	O
study	O
of	O
the	O
associative	O
processes	O
in	O
animals	O
the	O
psychological	O
review	O
series	O
of	O
monograph	O
supplements	O
thorndike	O
e	O
l	O
animal	O
intelligence	O
hafner	O
darien	O
ct	O
thorp	O
e	O
o	O
beat	O
the	O
dealer	O
a	O
winning	O
strategy	O
for	O
the	O
game	O
of	O
twenty-one	O
random	O
house	O
new	O
york	O
tian	O
t	O
preparation	O
an	O
empirical	O
study	O
of	O
sliding-step	O
methods	O
in	O
temporal	O
difference	O
learning	O
m	O
sc	O
thesis	O
university	O
of	O
alberta	O
edmonton	O
tieleman	O
t	O
hinton	O
g	O
lecture	O
rmsprop	O
coursera	O
neural	B
networks	O
for	O
machine	O
learning	O
tolman	B
e	O
c	O
purposive	O
behavior	O
in	O
animals	O
and	O
men	O
century	O
new	O
york	O
tolman	B
e	O
c	O
cognitive	B
maps	I
in	O
rats	O
and	O
men	O
psychological	O
review	O
tsai	O
h	O
-s	O
zhang	O
f	O
adamantidis	O
a	O
stuber	O
g	O
d	O
bonci	O
a	O
de	O
lecea	O
l	O
deisseroth	O
k	O
phasic	O
firing	O
in	O
dopaminergic	O
neurons	O
is	O
sufficient	O
for	O
behavioral	O
conditioning	O
science	O
tsetlin	O
m	O
l	O
automaton	O
theory	O
and	O
modeling	O
of	O
biological	O
systems	O
academic	O
press	O
new	O
york	O
tsitsiklis	O
j	O
n	O
asynchronous	O
stochastic	O
approximation	O
and	O
q-learning	B
machine	O
learning	O
tsitsiklis	O
j	O
n	O
on	O
the	O
convergence	O
of	O
optimistic	O
policy	B
iteration	I
journal	O
of	O
machine	O
learning	O
research	O
tsitsiklis	O
j	O
n	O
van	O
roy	O
b	O
feature-based	O
methods	O
for	O
large	O
scale	O
dynamic	O
program	O
ming	O
machine	O
learning	O
tsitsiklis	O
j	O
n	O
van	O
roy	O
b	O
an	O
analysis	O
of	O
temporal-difference	B
learning	I
with	B
function	B
approximation	I
ieee	O
transactions	O
on	O
automatic	O
control	B
tsitsiklis	O
j	O
n	O
van	O
roy	O
b	O
average	O
cost	O
temporal-difference	B
learning	I
automatica	O
turing	O
a	O
m	O
intelligent	O
machinery	O
a	O
heretical	O
theory	O
the	O
turing	O
test	O
verbal	O
behavior	O
as	O
the	O
hallmark	O
of	O
intelligence	O
p	O
ungar	O
l	O
h	O
a	O
bioreactor	O
benchmark	O
for	O
adaptive	O
network-based	O
process	O
control	B
in	O
references	O
w	O
t	O
miller	O
r	O
s	O
sutton	O
and	O
p	O
j	O
werbos	B
neural	B
networks	O
for	O
control	B
pp	O
mit	O
press	O
cambridge	O
ma	O
unnikrishnan	O
k	O
p	O
venugopal	O
k	O
p	O
alopex	O
a	O
correlation-based	O
learning	O
algorithm	O
for	O
feedforward	O
and	O
recurrent	O
neural	B
networks	O
n	O
eural	O
computation	O
urbanczik	O
r	O
senn	O
w	O
reinforcement	B
learning	I
in	O
populations	O
of	O
spiking	O
neurons	O
nature	O
neuroscience	B
urbanowicz	O
r	O
j	O
moore	O
j	O
h	O
learning	O
classifier	B
systems	I
a	O
complete	O
introduction	O
review	O
and	O
roadmap	O
journal	O
of	O
artificial	O
evolution	B
and	O
applications	O
valentin	O
v	O
v	O
dickinson	O
a	O
o	O
doherty	O
j	O
p	O
determining	O
the	O
neural	B
substrates	O
of	O
goal-directed	O
learning	O
in	O
the	O
human	O
brain	O
the	O
journal	O
of	O
neuroscience	B
in	O
advances	O
in	O
neural	B
information	O
processing	O
van	O
hasselt	O
h	O
double	B
q-learning	B
systems	O
pp	O
curran	O
associates	O
inc	O
van	O
hasselt	O
h	O
insights	O
in	O
reinforcement	B
learning	I
formal	O
analysis	O
and	O
empirical	O
evaluation	O
of	O
temporal-difference	B
learning	I
siks	O
dissertation	O
series	O
number	O
van	O
hasselt	O
h	O
reinforcement	B
learning	I
in	O
continuous	B
state	B
and	O
action	B
spaces	O
in	O
m	O
wiering	O
and	O
m	O
van	O
otterlo	O
reinforcement	B
learning	I
state-of-the-art	O
pp	O
springer-verlag	O
berlin	O
heidelberg	O
van	O
hasselt	O
h	O
sutton	O
r	O
s	O
learning	O
to	O
predict	O
independent	O
of	O
span	O
arxiv	O
van	O
roy	O
b	O
bertsekas	O
d	O
p	O
lee	O
y	O
tsitsiklis	O
j	O
n	O
a	O
neuro-dynamic	O
programming	O
approach	O
to	O
retailer	O
inventory	O
management	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
decision	O
and	B
control	B
vol	O
pp	O
van	O
seijen	O
h	O
effective	O
multi-step	O
temporal-difference	B
learning	I
for	O
non-linear	O
function	B
approximation	I
arxiv	O
preprint	O
van	O
seijen	O
h	O
sutton	O
r	O
s	O
efficient	O
planning	B
in	O
mdps	O
by	O
small	O
backups	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
van	O
seijen	O
h	O
sutton	O
r	O
s	O
true	B
online	B
td	B
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
jmlr	O
wcp	O
van	O
seijen	O
h	O
mahmood	O
a	O
r	O
pilarski	O
p	O
m	O
machado	O
m	O
c	O
sutton	O
r	O
s	O
true	B
online	B
temporal-difference	B
learning	I
journal	O
of	O
machine	O
learning	O
research	O
van	O
seijen	O
h	O
van	O
hasselt	O
h	O
whiteson	O
s	O
wiering	O
m	O
a	O
theoretical	O
and	O
empirical	O
analysis	O
of	O
expected	B
sarsa	B
in	O
ieee	O
symposium	O
on	O
adaptive	O
dynamic	B
programming	I
and	B
reinforcement	B
learning	I
pp	O
varga	O
r	O
s	O
matrix	O
iterative	B
analysis	O
englewood	O
cliffs	O
nj	O
prentice-hall	O
vasilaki	O
e	O
fr	O
emaux	O
n	O
urbanczik	O
r	O
senn	O
w	O
gerstner	O
w	O
spike-based	O
reinforcement	B
learning	I
in	O
continuous	B
state	B
and	O
action	B
space	O
when	O
policy	B
gradient	B
methods	I
fail	O
plos	O
computational	O
biology	O
viswanathan	O
r	O
narendra	O
k	O
s	O
games	O
of	O
stochastic	O
automata	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
walter	O
w	O
g	O
an	O
imitation	O
of	O
life	O
scientific	O
american	O
walter	O
w	O
g	O
a	O
machine	O
that	O
learns	O
scientific	O
american	O
waltz	O
m	O
d	O
fu	O
k	O
s	O
a	O
heuristic	O
approach	O
to	O
reinforcement	B
learning	I
control	B
systems	O
ieee	O
transactions	O
on	O
automatic	O
control	B
watkins	B
c	O
j	O
c	O
h	O
learning	O
from	O
delayed	O
rewards	O
ph	O
d	O
thesis	O
university	O
of	O
cambridge	O
watkins	B
c	O
j	O
c	O
h	O
dayan	O
p	O
q-learning	B
machine	O
learning	O
werbos	B
p	O
j	O
advanced	O
forecasting	O
methods	O
for	O
global	O
crisis	O
warning	O
and	O
models	O
of	O
references	O
intelligence	O
general	O
systems	O
yearbook	O
werbos	B
p	O
j	O
applications	O
of	O
advances	O
in	O
nonlinear	O
sensitivity	O
analysis	O
in	O
r	O
f	O
drenick	O
and	O
f	O
kozin	O
system	O
modeling	O
and	O
optimization	O
pp	O
springer-verlag	O
berlin	O
werbos	B
p	O
j	O
building	O
and	O
understanding	O
adaptive	O
systems	O
a	O
statisticalnumerical	O
approach	O
to	O
factory	O
automation	O
and	O
brain	O
research	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
werbos	B
p	O
j	O
generalization	O
of	O
back	O
propagation	O
with	O
applications	O
to	O
a	O
recurrent	O
gas	O
market	O
model	O
neural	B
networks	O
werbos	B
p	O
j	O
neural	B
networks	O
for	O
control	B
and	O
system	B
identification	I
in	O
proceedings	O
of	O
the	O
conference	O
on	O
decision	O
and	B
control	B
pp	O
ieee	O
control	B
systems	O
society	O
werbos	B
p	O
j	O
approximate	B
dynamic	B
programming	I
for	O
real-time	O
control	B
and	O
neural	B
modeling	O
in	O
d	O
a	O
white	O
and	O
d	O
a	O
sofge	O
handbook	O
of	O
intelligent	O
control	B
neural	B
fuzzy	O
and	O
adaptive	O
approaches	O
pp	O
van	O
nostrand	O
reinhold	O
new	O
york	O
werbos	B
p	O
j	O
the	O
roots	O
of	O
backpropagation	B
from	O
ordered	O
derivatives	O
to	O
neural	B
networks	O
and	O
political	O
forecasting	O
john	O
wiley	O
and	O
sons	O
wiering	O
m	O
van	O
otterlo	O
m	O
reinforcement	B
learning	I
state-of-the-art	O
springer	O
verlag	O
berlin	O
heidelberg	O
white	O
a	O
developing	O
a	O
predictive	O
approach	O
to	O
knowledge	O
ph	O
d	O
thesis	O
university	O
of	O
alberta	O
edmonton	O
white	O
d	O
j	O
dynamic	B
programming	I
holden-day	O
san	O
francisco	O
white	O
d	O
j	O
real	O
applications	O
of	O
markov	O
decision	O
processes	O
interfaces	O
white	O
d	O
j	O
interfaces	O
further	O
real	O
applications	O
of	O
markov	O
decision	O
processes	O
white	O
d	O
j	O
a	O
survey	O
of	O
applications	O
of	O
markov	O
decision	O
processes	O
journal	O
of	O
the	O
operational	O
research	O
society	O
white	O
a	O
white	O
m	O
investigating	O
practical	O
linear	O
temporal	O
difference	O
learning	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
autonomous	O
agents	O
and	O
multiagent	O
systems	O
pp	O
whitehead	O
s	O
d	O
ballard	O
d	O
h	O
learning	O
to	O
perceive	O
and	O
act	O
by	O
trial	O
and	O
error	O
machine	O
learning	O
whitt	O
w	O
approximations	O
of	O
dynamic	O
programs	O
i	O
mathematics	O
of	O
operations	O
re	O
search	O
whittle	O
p	O
optimization	O
over	O
time	O
vol	O
wiley	O
new	O
york	O
whittle	O
p	O
optimization	O
over	O
time	O
vol	O
wiley	O
new	O
york	O
wickens	O
j	O
k	O
otter	O
r	O
cellular	O
models	O
of	O
reinforcement	O
in	O
j	O
c	O
houk	O
j	O
l	O
davis	O
and	O
d	O
g	O
beiser	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	B
ganglia	I
pp	O
mit	O
press	O
cambridge	O
ma	O
widrow	O
b	O
gupta	O
n	O
k	O
maitra	O
s	O
punishreward	O
learning	O
with	O
a	O
critic	O
in	O
adaptive	O
threshold	O
systems	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	B
widrow	O
b	O
hoff	O
m	O
e	O
adaptive	O
switching	O
circuits	O
in	O
wescon	O
convention	O
record	O
part	O
iv	O
pp	O
institute	O
of	O
radio	O
engineers	O
new	O
york	O
reprinted	O
in	O
j	O
a	O
anderson	O
and	O
e	O
rosenfeld	O
neurocomputing	O
foundations	O
of	O
research	O
pp	O
mit	O
press	O
cambridge	O
ma	O
widrow	O
b	O
smith	O
f	O
w	O
pattern-recognizing	O
control	B
systems	O
in	O
j	O
t	O
tou	O
and	O
r	O
h	O
wilcox	O
computer	O
and	O
information	O
sciences	O
pp	O
spartan	O
washington	O
dc	O
references	O
widrow	O
b	O
stearns	O
s	O
d	O
adaptive	O
signal	O
processing	O
prentice-hall	O
englewood	O
cliffs	O
nj	O
wiener	O
n	O
god	O
and	O
golem	O
inc	O
a	O
comment	O
on	O
certain	O
points	O
where	O
cybernetics	B
impinges	O
on	O
religion	O
mit	O
press	O
cambridge	O
ma	O
wiewiora	O
e	O
potential-based	O
shaping	B
and	O
q-value	O
initialization	O
are	O
equivalent	O
journal	O
of	O
artificial	B
intelligence	I
research	O
williams	O
r	O
j	O
reinforcement	B
learning	I
in	O
connectionist	O
networks	O
a	O
mathematical	O
analysis	O
technical	O
report	O
ics	O
institute	O
for	O
cognitive	O
science	O
university	O
of	O
california	O
at	O
san	O
diego	O
la	O
jolla	O
williams	O
r	O
j	O
reinforcement-learning	O
connectionist	O
systems	O
technical	O
report	O
nu	O
college	O
of	O
computer	O
science	O
northeastern	O
university	O
boston	O
williams	O
r	O
j	O
on	O
the	O
use	O
of	O
backpropagation	B
in	O
associative	B
reinforcement	B
learning	I
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
conference	O
on	O
neural	B
networks	O
pp	O
ieee	O
san	O
diego	O
section	O
and	O
ieee	O
tab	O
neural	B
network	O
committee	O
williams	O
r	O
j	O
simple	O
statistical	O
gradient-following	O
algorithms	O
for	O
connectionist	O
rein	O
forcement	O
learning	O
machine	O
learning	O
williams	O
r	O
j	O
baird	O
l	O
c	O
a	O
mathematical	O
analysis	O
of	O
actor	O
critic	O
architectures	O
for	O
learning	O
optimal	O
controls	O
through	O
incremental	O
dynamic	B
programming	I
in	O
proceedings	O
of	O
the	O
sixth	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
pp	O
center	O
for	O
systems	O
science	O
dunham	O
laboratory	O
yale	O
university	O
new	O
haven	O
wilson	O
r	O
c	O
takahashi	O
y	O
k	O
schoenbaum	O
g	O
niv	O
y	O
orbitofrontal	O
cortex	O
as	O
a	O
cognitive	O
map	O
of	O
task	O
space	O
neuron	O
wilson	O
s	O
w	O
zcs	O
a	O
zeroth	O
order	O
classifier	O
system	O
evolutionary	O
computation	O
wise	O
r	O
a	O
dopamine	B
learning	O
and	O
motivation	B
nature	O
reviews	O
neuroscience	B
witten	B
i	O
h	O
the	O
apparent	O
conflict	O
between	O
estimation	O
and	B
control	B
a	O
survey	O
of	O
the	O
two-armed	O
problem	O
journal	O
of	O
the	O
franklin	O
institute	O
witten	B
i	O
h	O
an	O
adaptive	O
optimal	O
controller	O
for	O
discrete-time	O
markov	O
environments	O
information	O
and	B
control	B
witten	B
i	O
h	O
corbin	O
m	O
j	O
human	O
operators	O
and	O
automatic	O
adaptive	O
controllers	O
a	O
comparative	O
study	O
on	O
a	O
particular	O
control	B
task	O
international	O
journal	O
of	O
man	O
machine	O
studies	O
woodbury	O
t	O
dunn	O
c	O
and	O
valasek	O
j	O
autonomous	O
soaring	O
using	O
reinforcement	B
learning	I
for	O
trajectory	O
generation	O
in	O
aerospace	O
sciences	O
meeting	O
p	O
woodworth	O
r	O
s	O
experimental	O
psychology	B
new	O
york	O
henry	O
holt	O
and	O
company	O
xie	O
x	O
seung	O
h	O
s	O
learning	O
in	O
neural	B
networks	O
by	O
reinforcement	O
of	O
irregular	O
spiking	O
physical	O
review	O
e	O
xu	O
x	O
xie	O
t	O
hu	O
d	O
lu	O
x	O
kernel	O
least-squares	O
temporal	O
difference	O
learning	O
international	O
journal	O
of	O
information	O
technology	O
yagishita	O
s	O
hayashi-takagi	O
a	O
ellis-davies	O
g	O
c	O
r	O
urakubo	O
h	O
ishii	O
s	O
kasai	O
h	O
a	O
critical	O
time	O
window	O
for	O
dopamine	B
actions	O
on	O
the	O
structural	B
plasticity	O
of	O
dendritic	O
spines	O
science	O
yee	O
r	O
c	O
saxena	O
s	O
utgoff	O
p	O
e	O
barto	O
a	O
g	O
explaining	O
temporal	O
differences	O
to	O
create	O
useful	O
concepts	O
for	O
evaluating	O
states	O
in	O
proceedings	O
of	O
the	O
eighth	O
national	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
aaai	O
press	O
menlo	O
park	O
ca	O
yin	O
h	O
h	O
knowlton	O
b	O
j	O
the	O
role	O
of	O
the	O
basal	B
ganglia	I
in	O
habit	O
formation	O
nature	O
references	O
reviews	O
neuroscience	B
young	O
p	O
recursive	O
estimation	O
and	O
time-series	O
analysis	O
springer-verlag	O
berlin	O
yu	O
h	O
convergence	O
of	O
least	O
squares	O
temporal	O
difference	O
methods	O
under	O
general	O
condi	O
tions	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
yu	O
h	O
least	O
squares	O
temporal	O
difference	O
methods	O
an	O
analysis	O
under	O
general	O
conditions	O
siam	O
journal	O
on	O
control	B
and	O
optimization	O
yu	O
h	O
on	O
convergence	O
of	O
emphatic	O
temporal-difference	B
learning	I
in	O
proceedings	O
of	O
the	O
annual	O
conference	O
on	O
learning	O
theory	O
jmlr	O
wcp	O
also	O
yu	O
h	O
weak	O
convergence	O
properties	O
of	O
constrained	O
emphatic	O
temporal-difference	B
learning	I
with	O
constant	O
and	O
slowly	O
diminishing	O
stepsize	O
journal	O
of	O
machine	O
learning	O
research	O
yu	O
h	O
on	O
convergence	O
of	O
some	O
gradient-based	O
temporal-differences	O
algorithms	O
for	O
off	O
policy	B
learning	O
yu	O
h	O
mahmood	O
a	O
r	O
sutton	O
r	O
s	O
on	O
generalized	O
bellman	B
equations	O
and	O
temporaldifference	O
learning	O
a	O
summary	O
appeared	O
in	O
proceedings	O
of	O
the	O
canadian	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
springer	O
zhang	O
m	O
yum	O
t	O
p	O
comparisons	O
of	O
channel-assignment	O
strategies	O
in	O
cellular	O
mobile	O
telephone	O
systems	O
ieee	O
transactions	O
on	O
vehicular	O
technology	O
zhang	O
w	O
reinforcement	B
learning	I
for	O
job-shop	O
scheduling	O
ph	O
d	O
thesis	O
oregon	O
state	B
university	O
corvallis	O
technical	O
report	O
zhang	O
w	O
dietterich	O
t	O
g	O
a	O
reinforcement	B
learning	I
approach	O
to	O
job-shop	O
scheduling	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
international	O
joint	O
conference	O
on	O
artificial	B
intelligence	I
pp	O
morgan	O
kaufmann	O
zhang	O
w	O
dietterich	O
t	O
g	O
high-performance	O
job-shop	O
scheduling	O
with	O
a	O
time	O
delay	O
in	O
advances	O
in	O
neural	B
information	O
processing	O
systems	O
td	B
network	O
pp	O
mit	O
press	O
cambridge	O
ma	O
zweben	O
m	O
daun	O
b	O
deale	O
m	O
scheduling	O
and	O
rescheduling	O
with	O
iterative	B
repair	O
in	O
m	O
zweben	O
and	O
m	O
s	O
fox	O
intelligent	O
scheduling	O
pp	O
morgan	O
kaufmann	O
index	O
page	O
numbers	O
in	O
italics	O
are	O
recommended	O
to	O
be	O
consulted	O
first	O
page	O
numbers	O
in	O
bold	O
contain	O
boxed	O
algorithms	O
k-armed	B
bandits	I
absorbing	B
state	B
access-control	B
queuing	B
example	I
action	B
preferences	I
in	B
bandit	B
problems	I
action-value	O
function	O
see	O
value	B
function	I
ac	O
tion	O
action-value	B
methods	I
for	B
bandit	B
problems	I
actor	O
critic	O
one-step	O
with	B
eligibility	B
traces	I
with	B
eligibility	B
traces	I
neural	B
addiction	B
afterstates	B
agent	O
environment	B
interface	O
alphago	B
alphago	B
zero	O
alphazero	O
andreae	B
john	O
applications	B
and	I
case	I
studies	I
approximate	B
dynamic	B
programming	I
artificial	B
intelligence	I
xv	O
artificial	B
neural	B
networks	I
associative	B
reinforcement	B
learning	I
associative	B
search	I
asynchronous	B
dynamic	B
programming	I
atari	B
video	I
game	I
play	I
auxiliary	B
tasks	I
average	B
reward	I
setting	I
averagers	B
backgammon	B
backpropagation	B
backup	B
diagram	I
for	B
dynamic	B
programming	I
for	B
monte	B
carlo	I
methods	I
for	B
q-learning	B
for	O
for	B
sarsa	B
for	B
expected	B
sarsa	B
for	B
sarsa	B
for	O
td	B
for	O
q	O
for	B
tree	B
backup	I
for	O
truncated	B
td	B
for	O
n-step	B
q	O
for	B
n-step	B
expected	B
sarsa	B
for	B
n-step	B
sarsa	B
for	B
n-step	B
td	B
for	B
n-step	B
tree	B
backup	I
for	O
samuel	O
s	O
checker	O
player	O
compound	B
half	B
backups	I
backward	B
view	I
of	I
eligibility	B
traces	I
baird	O
s	O
counterexample	O
bandit	B
algorithm	I
simple	O
bandit	B
problems	I
basal	B
ganglia	I
baseline	B
behavior	O
policy	B
see	O
off-policy	B
learn	O
ing	O
bellman	B
equation	I
for	O
v	O
for	O
q	O
for	O
optimal	O
value	B
functions	O
v	O
and	O
q	O
differential	B
for	B
options	B
bellman	B
error	I
learnability	B
of	I
vector	B
bellman	B
operator	I
bellman	B
residual	O
see	O
bellman	B
error	I
bellman	B
richard	O
binary	B
features	I
bioreactor	B
example	I
blackjack	B
example	I
blocking	B
maze	I
example	I
bootstrapping	B
n-step	B
and	B
dynamic	B
programming	I
and	B
function	B
approximation	I
index	O
and	B
monte	B
carlo	I
methods	I
and	B
stability	I
and	B
td	B
learning	I
assessment	B
of	I
in	B
psychology	B
parameter	O
or	O
n	O
boxes	B
branching	B
factor	I
breakfast	B
example	I
bucket-brigade	B
algorithm	I
catastrophic	B
interference	I
certainty-equivalence	B
estimate	I
chess	B
classical	B
conditioning	I
blocking	B
and	B
higher-order	I
conditioning	I
delay	B
and	I
trace	I
conditioning	I
rescorla-wagner	B
model	I
td	B
model	I
classifier	B
systems	I
cliff	B
walking	I
example	I
cmac	O
see	O
tile	B
coding	I
coarse	B
coding	I
cognitive	B
maps	I
collective	B
reinforcement	B
learning	I
complex	B
backup	I
complex	O
backups	O
see	O
compound	B
update	O
compound	B
stimulus	I
compound	B
updatebackup	O
conditioned	O
stimulus	O
unconditioned	O
stimulus	O
conditioned	O
response	O
us	O
cr	O
constant-	O
mc	O
contextual	B
bandits	I
continuing	B
tasks	I
continuous	B
action	B
continuous	B
state	B
continuous	B
time	I
control	B
and	I
prediction	B
control	B
theory	I
control	B
variates	I
and	B
eligibility	B
traces	I
credit	B
assignment	I
in	B
psychology	B
structural	B
critic	O
see	O
actor	O
critic	O
curiosity	B
curse	B
of	I
dimensionality	I
cybernetics	B
xv	O
deadly	B
triad	I
deep	B
learning	I
deep	B
reinforcement	B
learning	I
deep	B
residual	I
learning	I
delayed	B
reinforcement	I
delayed	B
reward	I
dimensions	O
of	O
reinforcement	B
learning	I
methods	O
direct	B
and	I
indirect	I
rl	I
discounting	B
in	B
pole	I
balancing	I
rate	O
parameter	O
state	B
dependent	I
deprecated	B
distribution	B
models	I
dopamine	B
and	B
addiction	B
double	B
learning	I
dp	O
see	O
dynamic	B
programming	I
driving-home	B
example	I
dyna	B
architecture	I
dynamic	B
programming	I
and	B
artificial	B
intelligence	I
and	B
function	B
approximation	I
and	B
options	B
and	B
the	I
deadly	B
triad	I
computational	O
efficiency	O
of	O
eligibility	B
traces	I
accumulating	B
replacing	B
dutch	B
contingent	B
and	I
non-contingent	I
off-policy	B
with	O
state-dependent	O
and	O
emphatic-td	B
methods	I
off-policy	B
environment	B
episodes	B
episodic	O
tasks	O
error	B
reduction	I
property	I
evaluative	B
feedback	I
evolution	B
evolutionary	B
methods	I
expected	B
approximate	I
value	B
expected	B
sarsa	B
see	O
also	O
sarsa	B
expected	B
index	O
expected	B
update	I
experience	B
replay	I
exploreexploit	O
dilemma	O
exploring	B
starts	I
feature	B
construction	I
final	O
time	O
step	O
fourier	B
basis	I
function	B
approximation	I
gambler	O
s	O
example	O
game	B
theory	I
gazelle	B
calf	I
example	I
general	O
value	B
functions	O
generalized	O
policy	B
iteration	I
genetic	B
algorithms	I
gittins	B
index	I
glidingsoaring	O
case	O
study	O
goal	O
see	O
reward	B
signal	I
golf	B
example	I
gradient	B
gradient	B
descent	O
see	O
stochastic	O
gradient	B
de	O
and	B
eligibility	B
traces	I
and	B
infinite	I
variance	I
discounting	B
aware	I
incremental	B
implementation	I
per-decision	B
n-step	B
incremental	B
implementation	I
of	B
averages	I
of	B
weighted	I
averages	I
instrumental	O
conditioning	O
see	O
also	O
law	B
of	I
effect	I
motivation	B
thorndike	O
s	O
puzzle	O
boxes	B
interest	B
and	I
emphasis	I
inverse	B
reinforcement	B
learning	I
jack	O
s	O
car	O
rental	O
example	O
kernel-based	B
function	B
approximation	I
klopf	B
a	O
harry	O
xi	O
xv	O
latent	B
learning	I
law	B
of	I
effect	I
scent	O
gradient-td	B
methods	I
greedy	O
or	O
learning	B
automata	I
least	O
mean	O
square	O
algorithm	O
as	B
exploiting	I
as	B
shortsighted	I
policies	O
least-squares	O
td	B
linear	O
function	B
approximation	I
gridworld	B
examples	I
cliff	B
walking	I
dyna	B
blocking	B
maze	I
dyna	B
maze	I
dyna	B
shortcut	I
maze	I
windy	B
habitual	B
and	I
goal-directed	I
control	B
hedonistic	B
neurons	I
heuristic	B
search	I
as	B
sequences	I
of	I
backups	I
in	O
samuel	O
s	O
checkers	O
player	O
in	B
td-gammon	B
history	B
of	I
reinforcement	B
learning	I
holland	B
john	O
hull	B
clark	O
importance	B
sampling	I
ratio	B
weighted	B
and	I
ordinary	I
local	B
and	I
global	I
optima	I
markov	O
decision	O
process	O
markov	B
property	I
markov	O
reward	O
process	O
maximization	B
bias	I
maximum-likelihood	B
estimate	I
mc	O
see	O
monte	B
carlo	I
methods	I
mean	O
squared	O
bellman	B
error	I
be	O
projected	B
bellman	B
error	I
pbe	O
return	B
error	I
re	O
td	B
error	I
tde	O
value	B
error	I
ve	O
memory-based	O
function	O
approx	O
michie	B
donald	O
minsky	B
marvin	O
model	B
of	I
the	I
environment	B
model-based	B
and	I
model-free	I
methods	I
index	O
in	B
animal	I
learning	I
reducing	B
variance	I
model-based	B
reinforcement	B
learning	I
on-policy	B
distribution	I
in	B
neuroscience	B
monte	B
carlo	I
methods	I
first-	O
and	O
every-visit	O
mc	O
first-visit	O
mc	O
control	B
first-visit	O
mc	O
prediction	B
gradient	B
method	O
for	O
v	O
monte	B
carlo	I
es	O
starts	O
off-policy	B
control	B
off-policy	B
prediction	B
monte	B
carlo	I
tree	O
search	O
motivation	B
mountain	B
car	I
example	I
multi-armed	B
bandits	I
n-step	B
methods	I
q	O
sarsa	B
differential	B
off-policy	B
td	B
tree	B
backup	I
truncated	B
naughts	O
and	O
crosses	O
see	O
tic-tac-toe	B
neural	B
networks	O
see	O
artificial	B
neural	B
networks	I
neurodynamic	B
programming	I
neuroeconomics	B
neuroscience	B
nonstationarity	B
inherent	B
notation	O
ix	O
xvii	O
off-policy	B
methods	I
vs	B
on-policy	B
methods	I
monte	B
carlo	I
q-learning	B
expected	B
sarsa	B
n-step	B
n-step	B
q	O
n-step	B
sarsa	B
n-step	B
tree	B
backup	I
and	B
eligibility	B
traces	I
emphatic-td	O
gq	O
gtd	O
htd	O
q	O
tree	B
backup	I
vs	B
uniform	I
distribution	I
on-policy	B
methods	I
actor	O
critic	O
approximate	O
control	B
prediction	B
monte	B
carlo	I
n-step	B
sarsa	B
with	B
eligibility	B
traces	I
operant	O
conditioning	O
see	O
instrumental	O
learn	O
ing	O
optimal	B
control	B
optimistic	B
initial	I
values	I
optimizing	B
memory	I
control	B
pain	B
and	I
pleasure	I
partially	B
observable	I
mdps	I
pavlov	B
ivan	O
pavlovian	O
conditioning	O
see	O
classical	B
conditioning	I
control	B
personalizing	B
web	I
services	I
planning	B
with	B
learned	I
models	I
with	B
options	B
policy	B
soft	O
and	O
policy	B
approximation	I
policy	B
evaluation	O
see	O
also	O
prediction	B
iterative	B
policy	B
gradient	B
methods	I
reinforce	B
actor	O
critic	O
policy	B
gradient	B
theorem	B
proof	B
episodic	O
case	O
proof	B
continuing	O
case	O
policy	B
improvement	I
theorem	B
policy	B
iteration	I
polynomial	B
basis	I
prediction	B
see	O
also	O
policy	B
evaluation	O
and	B
control	B
monte	B
carlo	I
off-policy	B
index	O
td	B
with	B
approximation	I
prior	B
knowledge	I
prioritized	B
sweeping	I
projected	B
bellman	B
error	I
vector	B
pseudo	B
termination	I
psychology	B
q	O
watkins	B
s	O
q-function	O
see	O
action-value	O
function	O
q-learning	B
double	B
q-planning	B
q	O
queuing	B
example	I
r-learning	B
racetrack	B
exercise	I
radial	O
basis	O
functions	O
random	B
walk	I
td	B
results	O
on	O
fourier	B
and	I
polynomial	I
bases	I
real-time	B
dynamic	B
programming	I
recycling	B
robot	I
example	I
reinforce	B
with	B
baseline	B
reinforcement	B
learning	I
reinforcement	B
signal	I
representation	B
learning	I
residual-gradient	B
algorithm	I
naive	B
return	B
n-step	B
for	O
q	O
for	B
action	B
values	I
for	B
expected	B
sarsa	B
for	B
tree	B
backup	I
with	B
control	B
variates	I
with	B
function	B
approximation	I
differential	B
flat	O
partial	O
with	B
state-dependent	I
termination	I
truncated	B
reward	B
prediction	B
error	I
hypothesis	I
reward	B
signal	I
and	B
reinforcement	I
design	B
of	I
intrinsic	B
sparse	B
rod	B
maneuvering	I
example	I
rollout	B
algorithms	I
root	O
mean-squared	O
error	O
sample	B
and	I
expected	B
updates	I
sample	B
or	I
simulation	I
model	I
sample-average	B
method	I
samuel	O
s	O
checkers	O
player	O
sarsa	B
vs	B
q-learning	B
differential	B
one-step	O
expected	B
n-step	B
n-step	B
off-policy	B
double	B
n-step	B
differential	B
off-policy	B
sarsa	B
true	B
online	B
schultz	B
wolfram	O
search	B
control	B
secondary	B
reinforcement	I
selective	B
bootstrap	I
adaptation	I
semi-gradient	B
methods	I
shannon	B
claude	O
shaping	B
skinner	B
b	O
f	O
soap	B
bubble	I
example	I
soft	O
and	O
policies	O
soft-max	B
for	B
bandits	I
spike-timing-dependent	O
plasticity	O
state	B
and	B
observations	I
kth-order	B
history	I
approach	I
belief	B
state	B
latent	B
state	B
markov	B
property	I
observable	O
operator	O
models	O
partially	B
observable	I
mdps	I
predictive	B
state	B
representations	I
index	O
truncated	B
n-step	B
thorndike	O
edward	O
see	O
law	B
of	I
effect	I
tic-tac-toe	B
tile	B
coding	I
tolman	B
edward	O
trace-decay	O
parameter	O
state	B
dependent	I
trajectory	B
sampling	I
transition	B
probabilities	I
tree	B
backup	I
n-step	B
tree-backup	O
trial-and-error	O
see	O
also	O
instrumental	O
conditioning	O
true	B
online	B
td	B
tsitsiklis	O
and	O
van	O
roy	O
s	O
counterexample	O
undiscounted	O
continuing	B
tasks	I
see	O
average	O
re	O
ward	O
setting	O
unsupervised	B
learning	I
value	B
value	B
function	I
for	O
a	O
given	O
policy	B
v	O
and	O
q	O
for	O
an	O
optimal	O
policy	B
v	O
and	O
q	O
action	B
approximate	O
action	B
values	O
qs	O
a	O
w	O
approximate	O
state	B
values	O
vsw	O
differential	B
vs	B
evolutionary	B
methods	I
value	B
iteration	I
value-function	B
approximation	I
watkins	B
chris	O
watson	O
player	O
werbos	B
paul	O
witten	B
ian	O
state-update	B
function	I
state	B
aggregation	I
step-size	B
parameter	I
automatic	B
adaptation	I
in	B
dqn	I
in	B
psychological	I
models	I
selecting	B
manually	I
with	B
coarse	B
coding	I
with	B
fourier	I
features	I
with	B
tile	B
coding	I
stochastic	O
approx	O
convergence	O
conditions	O
stochastic	O
gradient-descent	O
in	B
the	I
bellman	B
error	I
strong	B
and	I
weak	I
methods	I
supervised	B
learning	I
xv	O
sweeps	O
see	O
also	O
prioritized	B
sweeping	I
synaptic	B
plasticity	I
hebbian	B
two-factor	B
and	I
three	I
factor	I
system	B
identification	I
tabular	B
solution	I
methods	I
target	O
policy	B
of	B
update	I
td	B
see	O
temporal-difference	B
learning	I
td	B
error	I
n-step	B
differential	B
with	B
function	B
approximation	I
td	B
true	B
online	B
td-gammon	B
temporal	B
abstraction	I
hierarchical	B
policy	B
option	B
models	I
options	B
planning	B
with	B
options	B
temporal-difference	B
learning	I
history	B
of	I
advantages	B
of	I
optimality	B
of	I
td	B
true	B
online	B
methods	O
off-line	B
online	B
