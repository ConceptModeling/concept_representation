bayesian	B
reasoning	O
and	O
machine	O
learning	B
david	O
barber	O
notation	O
list	O
v	O
domx	O
x	O
x	O
px	O
tr	O
px	O
fa	O
px	O
y	O
px	O
y	O
px	O
y	O
pxy	O
x	O
fx	O
i	O
y	O
pa	O
ch	O
ne	O
x	O
yz	O
dim	O
x	O
b	O
dim	O
x	O
s	O
y	O
t	O
d	O
n	O
n	O
y	O
s	O
erfx	O
i	O
j	O
im	O
ii	O
a	O
calligraphic	O
symbol	O
typically	O
denotes	O
a	O
set	O
of	O
random	O
variables	O
domain	B
of	O
a	O
variable	O
the	O
variable	O
x	O
is	O
in	O
the	O
state	O
x	O
probability	O
of	O
eventvariable	O
x	O
being	O
in	O
the	O
state	O
true	O
probability	O
of	O
eventvariable	O
x	O
being	O
in	O
the	O
state	O
false	O
probability	O
of	O
x	O
and	O
y	O
probability	O
of	O
x	O
and	O
y	O
probability	O
of	O
x	O
or	O
y	O
the	O
probability	O
of	O
x	O
conditioned	O
on	O
y	O
for	O
continuous	B
variables	O
this	O
is	O
shorthand	O
fxdx	O
and	O
for	O
discrete	B
variables	O
means	O
summation	O
over	O
the	O
states	O
of	O
x	O
fx	O
indicator	O
has	O
value	B
if	O
x	O
y	O
otherwise	O
the	O
parents	B
of	O
node	O
x	O
the	O
children	B
of	O
node	O
x	O
neighbours	O
of	O
node	O
x	O
variables	O
x	O
are	O
independent	O
of	O
variables	O
y	O
conditioned	O
on	O
variables	O
z	O
variables	O
x	O
are	O
dependent	O
on	O
variables	O
y	O
conditioned	O
variables	O
z	O
for	O
a	O
discrete	B
variable	O
x	O
this	O
denotes	O
the	O
number	O
of	O
states	O
x	O
can	O
take	O
the	O
average	B
of	O
the	O
function	B
fx	O
with	O
respect	O
to	O
the	O
distribution	B
px	O
delta	O
function	B
for	O
discrete	B
a	O
b	O
this	O
is	O
the	O
kronecker	B
delta	I
ab	O
and	O
for	O
continuous	B
a	O
b	O
the	O
dirac	B
delta	I
function	B
b	O
the	O
dimension	O
of	O
the	O
vectormatrix	O
x	O
the	O
number	O
of	O
times	O
variable	O
x	O
is	O
in	O
state	O
s	O
and	O
y	O
in	O
state	O
t	O
simultaneously	O
dataset	O
data	O
index	O
number	O
of	O
dataset	O
training	B
points	O
the	O
number	O
of	O
times	O
variable	O
x	O
is	O
in	O
state	O
y	O
sample	O
covariance	B
matrix	B
the	O
logistic	B
sigmoid	B
exp	O
x	O
the	O
error	B
function	B
the	O
set	O
of	O
unique	O
neighbouring	O
edges	O
on	O
a	O
graph	B
the	O
m	O
m	O
identity	B
matrix	B
draft	O
march	O
preface	O
machine	O
learning	B
the	O
last	O
decade	O
has	O
seen	O
considerable	O
growth	O
in	O
interest	O
in	O
artificial	O
intelligence	O
and	O
machine	O
learning	B
in	O
the	O
broadest	O
sense	O
these	O
fields	O
aim	O
to	O
learn	O
something	O
useful	O
about	O
the	O
environment	O
within	O
which	O
the	O
organism	O
operates	O
how	O
gathered	O
information	O
is	O
processed	O
leads	O
to	O
the	O
development	O
of	O
algorithms	O
how	O
to	O
process	O
high	B
dimensional	I
data	I
and	O
deal	O
with	O
uncertainty	B
in	O
the	O
early	O
stages	O
of	O
research	O
in	O
machine	O
learning	B
and	O
related	O
areas	O
similar	O
techniques	O
were	O
discovered	O
in	O
relatively	O
isolated	O
research	O
communities	O
whilst	O
not	O
all	O
techniques	O
have	O
a	O
natural	B
description	O
in	O
terms	O
of	O
probability	O
theory	O
many	O
do	O
and	O
it	O
is	O
the	O
framework	O
of	O
graphical	O
models	O
marriage	O
between	O
graph	B
and	O
probability	O
theory	O
that	O
has	O
enabled	O
the	O
understanding	O
and	O
transference	O
of	O
ideas	O
from	O
statistical	O
physics	O
statistics	O
machine	O
learning	B
and	O
information	O
theory	O
to	O
this	O
extent	O
it	O
is	O
now	O
reasonable	O
to	O
expect	O
that	O
machine	O
learning	B
researchers	O
are	O
familiar	O
with	O
the	O
basics	O
of	O
statistical	O
modelling	B
techniques	O
this	O
book	O
concentrates	O
on	O
the	O
probabilistic	B
aspects	O
of	O
information	O
processing	O
and	O
machine	O
learning	B
certainly	O
no	O
claim	O
is	O
made	O
as	O
to	O
the	O
correctness	O
or	O
that	O
this	O
is	O
the	O
only	O
useful	O
approach	B
indeed	O
one	O
might	O
counter	O
that	O
this	O
is	O
unnecessary	O
since	O
biological	O
organisms	O
don	O
t	O
use	O
probability	O
theory	O
whether	O
this	O
is	O
the	O
case	O
or	O
not	O
it	O
is	O
undeniable	O
that	O
the	O
framework	O
of	O
graphical	O
models	O
and	O
probability	O
has	O
helped	O
with	O
the	O
explosion	O
of	O
new	O
algorithms	O
and	O
models	O
in	O
the	O
machine	O
learning	B
community	O
one	O
should	O
also	O
be	O
clear	O
that	O
bayesian	B
viewpoint	O
is	O
not	O
the	O
only	O
way	O
to	O
go	O
about	O
describing	O
machine	O
learning	B
and	O
information	O
processing	O
bayesian	B
and	O
probabilistic	B
techniques	O
really	O
come	O
into	O
their	O
own	O
in	O
domains	O
where	O
uncertainty	B
is	O
a	O
necessary	O
consideration	O
the	O
structure	B
of	O
the	O
book	O
one	O
aim	O
of	O
part	O
i	O
of	O
the	O
book	O
is	O
to	O
encourage	O
computer	O
science	O
students	O
into	O
this	O
area	O
a	O
particular	O
difficulty	O
that	O
many	O
modern	O
students	O
face	O
is	O
a	O
limited	O
formal	O
training	B
in	O
calculus	B
and	O
linear	B
algebra	I
meaning	O
that	O
minutiae	O
of	O
continuous	B
and	O
high-dimensional	O
distributions	O
can	O
turn	O
them	O
away	O
in	O
beginning	O
with	O
probability	O
as	O
a	O
form	O
of	O
reasoning	O
system	O
we	O
hope	O
to	O
show	O
the	O
reader	O
how	O
ideas	O
from	O
logical	O
inference	B
and	O
dynamical	O
programming	O
that	O
they	O
may	O
be	O
more	O
familiar	O
with	O
have	O
natural	B
parallels	O
in	O
a	O
probabilistic	B
context	O
in	O
particular	O
computer	O
science	O
students	O
are	O
familiar	O
with	O
the	O
concept	O
of	O
algorithms	O
as	O
core	O
however	O
it	O
is	O
more	O
common	O
in	O
machine	O
learning	B
to	O
view	O
the	O
model	B
as	O
core	O
and	O
how	O
this	O
is	O
implemented	O
is	O
secondary	O
from	O
this	O
perspective	O
understanding	O
how	O
to	O
translate	O
a	O
mathematical	O
model	B
into	O
a	O
piece	O
of	O
computer	O
code	O
is	O
central	O
part	O
ii	O
introduces	O
the	O
statistical	O
background	O
needed	O
to	O
understand	O
continuous	B
distributions	O
and	O
how	O
learning	B
can	O
be	O
viewed	O
from	O
a	O
probabilistic	B
framework	O
part	O
iii	O
discusses	O
machine	O
learning	B
topics	O
certainly	O
some	O
readers	O
will	O
raise	O
an	O
eyebrow	O
to	O
see	O
their	O
favourite	O
statistical	O
topic	O
listed	O
under	O
machine	O
learning	B
a	O
difference	O
viewpoint	O
between	O
statistics	O
and	O
machine	O
learning	B
is	O
what	O
kinds	O
of	O
systems	O
we	O
would	O
ultimately	O
iii	O
like	O
to	O
construct	O
capable	O
of	O
humanbiological	O
information	O
processing	O
tasks	O
rather	O
than	O
in	O
some	O
of	O
the	O
techniques	O
this	O
section	O
of	O
the	O
book	O
is	O
therefore	O
what	O
i	O
feel	O
would	O
be	O
useful	O
for	O
machine	O
learners	O
to	O
know	O
part	O
iv	O
discusses	O
dynamical	O
models	O
in	O
which	O
time	O
is	O
explicitly	O
considered	O
in	O
particular	O
the	O
kalman	B
filter	I
is	O
treated	O
as	O
a	O
form	O
of	O
graphical	O
model	B
which	O
helps	O
emphasise	O
what	O
the	O
model	B
is	O
rather	O
than	O
focusing	O
on	O
it	O
as	O
a	O
filter	O
as	O
is	O
more	O
traditional	O
in	O
the	O
engineering	O
literature	O
part	O
v	O
contains	O
a	O
brief	O
introduction	O
to	O
approximate	B
inference	B
techniques	O
including	O
both	O
stochastic	O
carlo	O
and	O
deterministic	B
techniques	O
the	O
references	O
in	O
the	O
book	O
are	O
not	O
generally	O
intended	O
as	O
crediting	O
authors	O
with	O
ideas	O
nor	O
are	O
they	O
always	O
to	O
the	O
most	O
authoritative	O
works	O
rather	O
the	O
references	O
are	O
largely	O
to	O
works	O
which	O
are	O
at	O
a	O
level	O
reasonably	O
consistent	B
with	O
the	O
book	O
and	O
which	O
are	O
readily	O
available	O
whom	O
this	O
book	O
is	O
for	O
my	O
primary	O
aim	O
was	O
to	O
write	O
a	O
book	O
for	O
final	O
year	O
undergraduates	O
and	O
graduates	O
without	O
significant	O
experience	O
in	O
calculus	B
and	O
mathematics	O
that	O
gave	O
an	O
inroad	O
into	O
machine	O
learning	B
much	O
of	O
which	O
is	O
currently	O
phrased	O
in	O
terms	O
of	O
probabilities	O
and	O
multi-variate	B
distributions	O
the	O
aim	O
was	O
to	O
encourage	O
students	O
that	O
apparently	O
unexciting	O
statistical	O
concepts	O
are	O
actually	O
highly	O
relevant	O
for	O
research	O
in	O
making	O
intelligent	O
systems	O
that	O
interact	O
with	O
humans	O
in	O
a	O
natural	B
manner	O
such	O
a	O
research	O
programme	O
inevitably	O
requires	O
dealing	O
with	O
high-dimensional	O
data	O
time-series	O
networks	O
logical	O
reasoning	O
modelling	B
and	O
uncertainty	B
other	O
books	O
in	O
this	O
area	O
whilst	O
there	O
are	O
several	O
excellent	O
textbooks	O
in	O
this	O
area	O
none	O
currently	O
meets	O
the	O
requirements	O
that	O
i	O
personally	O
need	O
for	O
teaching	O
namely	O
one	O
that	O
contains	O
demonstration	O
code	O
and	O
gently	O
introduces	O
probability	O
and	O
statistics	O
before	O
leading	O
on	O
to	O
more	O
advanced	O
topics	O
in	O
machine	O
learning	B
this	O
lead	O
me	O
to	O
build	O
on	O
my	O
lecture	O
material	O
from	O
courses	O
given	O
at	O
aston	O
edinburgh	O
epfl	O
and	O
ucl	O
and	O
expand	O
the	O
demonstration	O
software	O
considerably	O
the	O
book	O
is	O
due	O
for	O
publication	O
by	O
cambridge	O
university	O
press	O
in	O
the	O
literature	O
on	O
machine	O
learning	B
is	O
vast	O
as	O
is	O
the	O
overlap	O
with	O
the	O
relevant	O
areas	O
of	O
statistics	O
engineering	O
and	O
other	O
physical	O
sciences	O
in	O
this	O
respect	O
it	O
is	O
difficult	O
to	O
isolate	O
particular	O
areas	O
and	O
this	O
book	O
is	O
an	O
attempt	O
to	O
integrate	O
parts	O
of	O
the	O
machine	O
learning	B
and	O
statistics	O
literature	O
the	O
book	O
is	O
written	O
in	O
an	O
informal	O
style	O
at	O
the	O
expense	O
of	O
rigour	O
and	O
detailed	O
proofs	O
as	O
an	O
introductory	O
textbook	O
topics	O
are	O
naturally	O
covered	O
to	O
a	O
somewhat	O
shallow	O
level	O
and	O
the	O
reader	O
is	O
referred	O
to	O
more	O
specialised	O
books	O
for	O
deeper	O
treatments	O
amongst	O
my	O
favourites	O
are	O
graphical	O
models	O
graphical	O
models	O
by	O
s	O
lauritzen	O
oxford	O
university	O
press	O
bayesian	B
networks	O
and	O
decision	O
graphs	O
by	O
f	O
jensen	O
and	O
t	O
d	O
nielsen	O
springer	O
verlag	O
probabilistic	B
networks	O
and	O
expert	O
systems	O
by	O
r	O
g	O
cowell	O
a	O
p	O
dawid	O
s	O
l	O
lauritzen	O
and	O
d	O
j	O
spiegelhalter	O
springer	O
verlag	O
probabilistic	B
reasoning	O
in	O
intelligent	O
systems	O
by	O
j	O
pearl	O
morgan	O
kaufmann	O
graphical	O
models	O
in	O
applied	O
multivariate	B
statistics	O
by	O
j	O
whittaker	O
wiley	O
probabilistic	B
graphical	O
models	O
principles	O
and	O
techniques	O
by	O
d	O
koller	O
and	O
n	O
friedman	O
mit	O
press	O
machine	O
learning	B
and	O
information	O
processing	O
information	O
theory	O
inference	B
and	O
learning	B
algorithms	O
by	O
d	O
j	O
c	O
mackay	O
cambridge	O
uni	O
versity	O
press	O
iv	O
draft	O
march	O
pattern	O
recognition	O
and	O
machine	O
learning	B
by	O
c	O
m	O
bishop	O
springer	O
verlag	O
an	O
introduction	O
to	O
support	O
vector	O
machines	O
n	O
cristianini	O
and	O
j	O
shawe-taylor	O
cambridge	O
university	O
press	O
gaussian	B
processes	O
for	O
machine	O
learning	B
by	O
c	O
e	O
rasmussen	O
and	O
c	O
k	O
i	O
williams	O
mit	O
press	O
how	O
to	O
use	O
this	O
book	O
part	O
i	O
would	O
be	O
suitable	O
for	O
an	O
introductory	O
course	O
on	O
graphical	O
models	O
with	O
a	O
focus	O
on	O
inference	B
part	O
ii	O
contains	O
enough	O
material	O
for	O
a	O
short	O
lecture	O
course	O
on	O
learning	B
in	O
probabilistic	B
models	O
part	O
iii	O
is	O
reasonably	O
self-contained	O
and	O
would	O
be	O
suitable	O
for	O
a	O
course	O
on	O
machine	O
learning	B
from	O
a	O
probabilistic	B
perspective	O
particularly	O
combined	O
with	O
the	O
dynamical	O
models	O
material	O
in	O
part	O
iv	O
part	O
v	O
would	O
be	O
suitable	O
for	O
a	O
short	O
course	O
on	O
approximate	B
inference	B
accompanying	O
code	O
the	O
matlab	O
code	O
is	O
provided	O
to	O
help	O
readers	O
see	O
how	O
mathematical	O
models	O
translate	O
into	O
actual	O
code	O
the	O
code	O
is	O
not	O
meant	O
to	O
be	O
an	O
industrial	O
strength	O
research	O
tool	O
rather	O
a	O
reasonably	O
lightweight	O
toolbox	O
that	O
enables	O
the	O
reader	O
to	O
play	O
with	O
concepts	O
in	O
graph	B
theory	O
probability	O
theory	O
and	O
machine	O
learning	B
in	O
an	O
attempt	O
to	O
retain	O
readability	O
no	O
extensive	O
error	O
andor	O
exception	O
handling	O
has	O
been	O
included	O
the	O
code	O
contains	O
at	O
the	O
moment	O
basic	O
routines	O
for	O
manipulating	O
discrete	B
variable	O
distributions	O
along	O
with	O
a	O
set	O
of	O
routines	O
that	O
are	O
more	O
concerned	O
with	O
continuous	B
variable	O
machine	O
learning	B
one	O
could	O
in	O
principle	O
extend	O
the	O
graphical	O
models	O
part	O
of	O
the	O
code	O
considerably	O
to	O
support	O
continuous	B
variables	O
limited	O
support	O
for	O
continuous	B
variables	O
is	O
currently	O
provided	O
so	O
that	O
for	O
example	O
inference	B
in	O
the	O
linear	B
dynamical	I
system	I
may	O
be	O
written	O
in	O
conducted	O
of	O
operations	O
on	O
gaussian	B
potentials	O
however	O
in	O
general	O
potentials	O
on	O
continuous	B
variables	O
need	O
to	O
be	O
manipulated	O
with	O
care	O
and	O
often	O
specialised	O
routines	O
are	O
required	O
to	O
ensure	O
numerical	B
stability	I
acknowledgements	O
many	O
people	O
have	O
helped	O
this	O
book	O
along	O
the	O
way	O
either	O
in	O
terms	O
of	O
reading	O
feedback	O
general	O
insights	O
allowing	O
me	O
to	O
present	O
their	O
work	O
or	O
just	O
plain	O
motivation	O
amongst	O
these	O
i	O
would	O
like	O
to	O
thank	O
massimiliano	O
pontil	O
mark	O
herbster	O
john	O
shawe-taylor	O
vladimir	O
kolmogorov	O
yuri	O
boykov	O
tom	O
minka	O
simon	O
prince	O
silvia	O
chiappa	O
bertrand	O
mesot	O
robert	O
cowell	O
ali	O
taylan	O
cemgil	O
david	O
blei	O
jeff	O
bilmes	O
david	O
cohn	O
david	O
page	O
peter	O
sollich	O
chris	O
williams	O
marc	O
toussaint	O
amos	O
storkey	O
zakria	O
hussain	O
seraf	O
n	O
moral	O
milan	O
studen	O
y	O
tristan	O
fletcher	O
tom	O
furmston	O
ed	O
challis	O
and	O
chris	O
bracegirdle	O
i	O
would	O
also	O
like	O
to	O
thank	O
the	O
many	O
students	O
that	O
have	O
helped	O
improve	O
the	O
material	O
during	O
lectures	O
over	O
the	O
years	O
i	O
m	O
particularly	O
grateful	O
to	O
tom	O
minka	O
for	O
allowing	O
parts	O
of	O
his	O
lightspeed	O
toolbox	O
to	O
be	O
bundled	O
with	O
the	O
brmltoolbox	O
and	O
am	O
similarly	O
indebted	O
to	O
taylan	O
cemgil	O
for	O
his	O
graphlayout	O
package	O
a	O
final	O
thankyou	O
to	O
my	O
family	B
and	O
friends	O
website	B
the	O
code	O
along	O
with	O
an	O
electronic	O
version	O
of	O
the	O
book	O
is	O
available	O
from	O
httpwww	O
cs	O
ucl	O
ac	O
ukstaffd	O
barberbrml	O
instructors	O
seeking	O
solutions	O
to	O
the	O
exercises	O
can	O
find	O
information	O
at	O
the	O
website	B
along	O
with	O
additional	O
teaching	O
material	O
the	O
website	B
also	O
contains	O
a	O
feedback	O
form	O
and	O
errata	O
list	O
draft	O
march	O
v	O
vi	O
draft	O
march	O
contents	O
i	O
inference	B
in	O
probabilistic	B
models	O
probabilistic	B
reasoning	O
probability	O
refresher	O
probability	O
tables	O
interpreting	O
conditional	B
probability	I
probabilistic	B
reasoning	O
prior	B
likelihood	B
and	O
posterior	B
two	O
dice	O
what	O
were	O
the	O
individual	O
scores	O
further	O
worked	O
examples	O
code	O
basic	O
probability	O
code	O
general	O
utilities	O
an	O
example	O
notes	O
exercises	O
basic	O
graph	B
concepts	O
graphs	O
spanning	B
tree	B
numerically	O
encoding	O
graphs	O
edge	B
list	I
adjacency	B
matrix	B
clique	B
matrix	B
code	O
utility	B
routines	O
exercises	O
belief	B
networks	I
probabilistic	B
inference	B
in	O
structured	B
distributions	O
graphically	O
representing	O
distributions	O
constructing	O
a	O
simple	O
belief	B
network	I
wet	O
grass	O
uncertain	B
evidence	O
belief	B
networks	I
conditional	B
independence	B
the	O
impact	O
of	O
collisions	O
d-separation	O
d-connection	O
and	O
dependence	O
markov	B
equivalence	I
in	O
belief	B
networks	I
belief	B
networks	I
have	O
limited	O
expressibility	O
vii	O
contents	O
contents	O
learning	B
the	O
direction	O
of	O
arrows	O
causality	B
simpson	O
s	O
paradox	O
influence	O
diagrams	O
and	O
the	O
do-calculus	O
parameterising	O
belief	B
networks	I
further	O
reading	O
code	O
naive	O
inference	B
demo	O
conditional	B
independence	B
demo	O
utility	B
routines	O
exercises	O
graphical	O
models	O
graphical	O
models	O
markov	O
networks	O
markov	O
properties	B
gibbs	B
networks	O
markov	O
random	O
fields	O
conditional	B
independence	B
using	O
markov	O
networks	O
lattice	O
models	O
chain	B
graphical	O
models	O
expressiveness	O
of	O
graphical	O
models	O
factor	B
graphs	O
conditional	B
independence	B
in	O
factor	B
graphs	O
notes	O
code	O
exercises	O
efficient	O
inference	B
in	O
trees	O
other	O
forms	O
of	O
inference	B
marginal	B
inference	B
variable	B
elimination	I
in	O
a	O
markov	B
chain	B
and	O
message	B
passing	B
the	O
sum-product	B
algorithm	B
on	O
factor	B
graphs	O
computing	O
the	O
marginal	B
likelihood	B
the	O
problem	B
with	O
loops	O
max-product	B
finding	O
the	O
n	B
most	I
probable	I
states	I
most	O
probable	O
path	B
and	O
shortest	B
path	B
mixed	B
inference	B
inference	B
in	O
multiply-connected	B
graphs	O
bucket	B
elimination	I
loop-cut	O
conditioning	B
message	B
passing	B
for	O
continuous	B
distributions	O
notes	O
code	O
factor	B
graph	B
examples	O
most	O
probable	O
and	O
shortest	B
path	B
bucket	B
elimination	I
message	B
passing	B
on	O
gaussians	O
exercises	O
viii	O
draft	O
march	O
contents	O
contents	O
the	O
junction	B
tree	B
algorithm	B
clique	B
graphs	O
junction	O
trees	O
clustering	B
variables	O
reparameterisation	B
absorption	B
absorption	B
schedule	B
on	O
clique	B
trees	O
the	O
running	B
intersection	I
property	I
constructing	O
a	O
junction	B
tree	B
for	O
singly-connected	B
distributions	O
moralisation	B
forming	O
the	O
clique	B
graph	B
forming	O
a	O
junction	B
tree	B
from	O
a	O
clique	B
graph	B
assigning	O
potentials	O
to	O
cliques	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
triangulation	B
algorithms	O
the	O
junction	B
tree	B
algorithm	B
remarks	O
on	O
the	O
jta	O
computing	O
the	O
normalisation	B
constant	I
of	O
a	O
distribution	B
the	O
marginal	B
likelihood	B
finding	O
the	O
most	B
likely	I
state	I
reabsorption	B
converting	O
a	O
junction	B
tree	B
to	O
a	O
directed	B
network	O
the	O
need	O
for	O
approximations	O
bounded	O
width	O
junction	O
trees	O
code	O
utility	B
routines	O
exercises	O
making	O
decisions	O
syntax	O
of	O
influence	O
diagrams	O
solving	B
influence	O
diagrams	O
temporally	B
unbounded	I
mdps	O
expected	O
utility	B
utility	B
of	O
money	B
decision	O
trees	O
extending	O
bayesian	B
networks	O
for	O
decisions	O
efficient	O
inference	B
using	O
a	O
junction	B
tree	B
markov	O
decision	O
processes	O
maximising	O
expected	O
utility	B
by	O
message	B
passing	B
bellman	O
s	O
equation	B
value	B
iteration	B
policy	B
iteration	B
a	O
curse	B
of	I
dimensionality	I
probabilistic	B
inference	B
and	O
planning	B
non-stationary	B
markov	B
decision	I
process	I
non-stationary	B
probabilistic	B
inference	B
planner	O
stationary	B
planner	I
utilities	O
at	O
each	O
timestep	O
code	O
summax	O
under	O
a	O
partial	B
order	I
junction	O
trees	O
for	O
influence	O
diagrams	O
partially	B
observable	I
mdps	O
restricted	B
utility	B
functions	O
reinforcement	B
learning	B
further	O
topics	O
draft	O
march	O
ix	O
contents	O
contents	O
party-friend	O
example	O
chest	B
clinic	I
with	B
decisions	I
markov	O
decision	O
processes	O
exercises	O
ii	O
learning	B
in	O
probabilistic	B
models	O
statistics	O
for	O
machine	O
learning	B
estimator	O
bias	B
discrete	B
distributions	O
continuous	B
distributions	O
distributions	O
summarising	O
distributions	O
bounded	O
distributions	O
unbounded	O
distributions	O
multivariate	B
distributions	O
multivariate	B
gaussian	B
conditioning	B
as	O
system	B
reversal	I
completing	O
the	O
square	O
gaussian	B
propagation	B
whitening	B
and	O
centering	B
maximum	B
likelihood	B
training	B
bayesian	B
inference	B
of	O
the	O
mean	B
and	O
variance	B
gauss-gamma	B
distribution	B
exponential	B
family	B
conjugate	B
priors	O
the	O
kullback-leibler	B
divergence	B
klqp	O
entropy	B
code	O
exercises	O
learning	B
as	O
inference	B
learning	B
as	O
inference	B
learning	B
the	O
bias	B
of	O
a	O
coin	O
making	O
decisions	O
a	O
continuum	O
of	O
parameters	O
decisions	O
based	O
on	O
continuous	B
intervals	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
summarising	O
the	O
posterior	B
maximum	B
likelihood	B
and	O
the	O
empirical	B
distribution	B
maximum	B
likelihood	B
training	B
of	O
belief	B
networks	I
bayesian	B
belief	B
network	I
training	B
global	B
and	O
local	B
parameter	B
independence	B
learning	B
binary	O
variable	O
tables	O
using	O
a	O
beta	B
prior	B
learning	B
multivariate	B
discrete	B
tables	O
using	O
a	O
dirichlet	B
prior	B
parents	B
structure	B
learning	B
empirical	B
independence	B
network	B
scoring	I
maximum	B
likelihood	B
for	O
undirected	B
models	O
the	O
likelihood	B
gradient	B
decomposable	B
markov	O
networks	O
non-decomposable	O
markov	O
networks	O
constrained	O
decomposable	B
markov	O
networks	O
x	O
draft	O
march	O
contents	O
contents	O
iterative	B
scaling	I
conditional	B
random	O
fields	O
pseudo	B
likelihood	B
learning	B
the	O
structure	B
properties	B
of	O
maximum	B
likelihood	B
training	B
assuming	O
the	O
correct	O
model	B
class	O
training	B
when	O
the	O
assumed	O
model	B
is	O
incorrect	O
code	O
pc	B
algorithm	B
using	O
an	O
oracle	O
demo	O
of	O
empirical	B
conditional	B
independence	B
bayes	O
dirichlet	B
structure	B
learning	B
exercises	O
naive	B
bayes	I
naive	B
bayes	I
and	O
conditional	B
independence	B
estimation	O
using	O
maximum	B
likelihood	B
binary	O
attributes	O
multi-state	O
variables	O
text	O
classification	B
bayesian	B
naive	B
bayes	I
tree	B
augmented	B
naive	B
bayes	I
chow-liu	B
trees	O
learning	B
tree	B
augmented	B
naive	B
bayes	I
networks	O
code	O
exercises	O
learning	B
with	O
hidden	B
variables	I
hidden	B
variables	I
and	O
missing	B
data	I
why	O
hiddenmissing	O
variables	O
can	O
complicate	O
proceedings	O
the	O
missing	B
at	I
random	I
assumption	O
maximum	B
likelihood	B
identifiability	B
issues	O
expectation	B
maximisation	B
variational	O
em	B
classical	O
em	B
application	O
to	O
belief	B
networks	I
application	O
to	O
markov	O
networks	O
convergence	O
extensions	O
of	O
em	B
partial	O
m	O
step	O
partial	O
e	O
step	O
a	O
failure	B
case	I
for	O
em	B
variational	B
bayes	I
em	B
is	O
a	O
special	O
case	O
of	O
variational	B
bayes	I
factorising	O
the	O
parameter	B
posterior	B
bayesian	B
methods	O
and	O
ml-ii	B
optimising	O
the	O
likelihood	B
by	O
gradient	B
methods	O
directed	B
models	O
undirected	B
models	O
code	O
exercises	O
draft	O
march	O
xi	O
contents	O
contents	O
bayesian	B
model	B
selection	I
comparing	O
models	O
the	O
bayesian	B
way	O
illustrations	O
coin	O
tossing	O
a	O
discrete	B
parameter	B
space	O
a	O
continuous	B
parameter	B
space	O
occam	O
s	O
razor	O
and	O
bayesian	B
complexity	O
penalisation	O
a	O
continuous	B
example	O
curve	O
fitting	O
approximating	O
the	O
model	B
likelihood	B
laplace	B
s	O
method	O
bayes	B
information	I
criterion	I
exercises	O
iii	O
machine	O
learning	B
machine	O
learning	B
concepts	O
styles	O
of	O
learning	B
supervised	B
learning	B
unsupervised	B
learning	B
anomaly	B
detection	I
online	B
learning	B
interacting	O
with	O
the	O
environment	O
semi-supervised	B
learning	B
supervised	B
learning	B
utility	B
and	O
loss	O
what	O
s	O
the	O
catch	O
using	O
the	O
empirical	B
distribution	B
bayesian	B
decision	O
approach	B
learning	B
lower-dimensional	O
representations	O
in	O
semi-supervised	B
learning	B
features	O
and	O
preprocessing	O
bayes	O
versus	O
empirical	B
decisions	O
representing	O
data	O
categorical	B
ordinal	B
numerical	B
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
outcome	B
analysis	B
hdiff	O
model	B
likelihood	B
hsame	O
model	B
likelihood	B
dependent	O
outcome	B
analysis	B
is	O
classifier	B
a	O
better	O
than	O
b	O
code	O
notes	O
exercises	O
nearest	B
neighbour	B
classification	B
do	O
as	O
your	O
neighbour	B
does	O
k-nearest	O
neighbours	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
when	O
your	O
nearest	B
neighbour	B
is	O
far	O
away	O
code	O
utility	B
routines	O
demonstration	O
exercises	O
xii	O
draft	O
march	O
contents	O
contents	O
unsupervised	B
linear	B
dimension	I
reduction	I
latent	B
semantic	I
analysis	B
lsa	O
for	O
information	B
retrieval	I
high-dimensional	O
spaces	O
low	B
dimensional	I
manifolds	O
principal	B
components	I
analysis	B
deriving	O
the	O
optimal	O
linear	B
reconstruction	O
maximum	O
variance	B
criterion	O
pca	B
algorithm	B
pca	B
and	O
nearest	O
neighbours	O
comments	O
on	O
pca	B
high	B
dimensional	I
data	I
eigen-decomposition	O
for	O
n	O
d	O
pca	B
via	O
singular	B
value	B
decomposition	B
pca	B
with	O
missing	B
data	I
finding	O
the	O
principal	B
directions	I
collaborative	B
filtering	I
using	O
pca	B
with	O
missing	B
data	I
matrix	B
decomposition	B
methods	O
probabilistic	B
latent	B
semantic	I
analysis	B
extensions	O
and	O
variations	O
applications	O
of	O
plsanmf	O
kernel	B
pca	B
canonical	B
correlation	I
analysis	B
svd	B
formulation	O
notes	O
supervised	B
linear	B
dimension	I
reduction	I
supervised	B
linear	B
projections	O
fisher	O
s	O
linear	B
discriminant	O
canonical	B
variates	I
dealing	O
with	O
the	O
nullspace	O
using	O
non-gaussian	O
data	O
distributions	O
code	O
exercises	O
linear	B
models	O
the	O
dual	B
representation	I
and	O
kernels	O
introduction	O
fitting	O
a	O
straight	O
line	O
linear	B
parameter	B
models	O
for	O
regression	B
vector	O
outputs	O
regularisation	B
radial	B
basis	I
functions	I
regression	B
in	O
the	O
dual-space	O
positive	B
definite	I
kernels	O
functions	O
linear	B
parameter	B
models	O
for	O
classification	B
logistic	B
regression	B
maximum	B
likelihood	B
training	B
beyond	O
first	O
order	O
gradient	B
ascent	O
avoiding	O
overconfident	O
classification	B
multiple	B
classes	I
the	O
kernel	B
trick	O
for	O
classification	B
support	O
vector	O
machines	O
maximum	O
margin	B
linear	B
classifier	B
using	O
kernels	O
draft	O
march	O
xiii	O
contents	O
contents	O
performing	O
the	O
optimisation	B
probabilistic	B
interpretation	O
soft	B
zero-one	B
loss	I
for	O
outlier	B
robustness	O
notes	O
code	O
bayesian	B
linear	B
models	O
regression	B
with	O
additive	O
gaussian	B
noise	O
bayesian	B
linear	B
parameter	B
models	O
determining	O
hyperparameters	O
ml-ii	B
learning	B
the	O
hyperparameters	O
using	O
em	B
hyperparameter	B
optimisation	B
using	O
the	O
gradient	B
validation	B
likelihood	B
prediction	O
the	O
relevance	B
vector	I
machine	I
classification	B
hyperparameter	B
optimisation	B
laplace	B
approximation	B
making	O
predictions	O
relevance	B
vector	I
machine	I
for	O
classification	B
multi-class	O
case	O
code	O
exercises	O
gaussian	B
processes	O
non-parametric	B
prediction	O
from	O
parametric	O
to	O
non-parametric	B
from	O
bayesian	B
linear	B
models	O
to	O
gaussian	B
processes	O
a	O
prior	B
on	O
functions	O
gaussian	B
process	I
prediction	O
regression	B
with	O
noisy	O
training	B
outputs	O
covariance	B
functions	O
making	O
new	O
covariance	B
functions	O
from	O
old	O
stationary	B
covariance	B
functions	O
non-stationary	B
covariance	B
functions	O
analysis	B
of	O
covariance	B
functions	O
smoothness	B
of	O
the	O
functions	O
mercer	O
kernels	O
fourier	O
analysis	B
for	O
stationary	B
kernels	O
gaussian	B
processes	O
for	O
classification	B
binary	O
classification	B
laplace	B
s	O
approximation	B
hyperparameter	B
optimisation	B
multiple	B
classes	I
further	O
reading	O
code	O
exercises	O
mixture	B
models	O
density	B
estimation	I
using	O
mixtures	O
expectation	B
maximisation	B
for	O
mixture	B
models	O
unconstrained	O
discrete	B
tables	O
mixture	B
of	O
product	O
of	O
bernoulli	B
distributions	O
the	O
gaussian	B
mixture	B
model	B
xiv	O
draft	O
march	O
contents	O
contents	O
em	B
algorithm	B
practical	O
issues	O
classification	B
using	O
gaussian	B
mixture	B
models	O
the	O
parzen	B
estimator	I
k-means	B
bayesian	B
mixture	B
models	O
semi-supervised	B
learning	B
mixture	B
of	I
experts	I
indicator	O
models	O
joint	B
indicator	B
approach	B
factorised	B
prior	B
joint	B
indicator	B
approach	B
polya	B
prior	B
latent	B
dirichlet	B
allocation	I
graph	B
based	O
representations	O
of	O
data	O
dyadic	B
data	I
monadic	B
data	I
cliques	O
and	O
adjacency	B
matrices	O
for	O
monadic	B
binary	O
data	O
further	O
reading	O
code	O
exercises	O
mixed	B
membership	I
models	O
latent	B
linear	B
models	O
factor	B
analysis	B
finding	O
the	O
optimal	O
bias	B
factor	B
analysis	B
maximum	B
likelihood	B
direct	O
likelihood	B
optimisation	B
expectation	B
maximisation	B
interlude	O
modelling	B
faces	B
probabilistic	B
principal	B
components	I
analysis	B
canonical	B
correlation	I
analysis	B
and	O
factor	B
analysis	B
independent	B
components	I
analysis	B
code	O
exercises	O
latent	B
ability	O
models	O
the	O
rasch	B
model	B
maximum	B
likelihood	B
training	B
bayesian	B
rasch	B
models	O
competition	B
models	I
code	O
exercises	O
bradly-terry-luce	B
model	B
elo	B
ranking	O
model	B
glicko	B
and	O
trueskill	B
iv	O
dynamical	O
models	O
discrete-state	O
markov	O
models	O
markov	O
models	O
equilibrium	O
and	O
stationary	B
distribution	B
of	O
a	O
markov	B
chain	B
fitting	O
markov	O
models	O
mixture	B
of	O
markov	O
models	O
the	O
classical	O
inference	B
problems	O
hidden	B
markov	O
models	O
draft	O
march	O
xv	O
contents	O
contents	O
learning	B
hmms	O
filtering	O
parallel	B
smoothing	B
correction	O
smoothing	B
most	O
likely	O
joint	B
state	O
self	O
localisation	B
and	O
kidnapped	O
robots	O
natural	B
language	O
models	O
em	B
algorithm	B
mixture	B
emission	O
the	O
hmm-gmm	O
discriminative	B
training	B
related	O
models	O
explicit	O
duration	B
model	B
input-output	B
hmm	B
linear	B
chain	B
crfs	O
dynamic	B
bayesian	B
networks	O
applications	O
object	O
tracking	O
automatic	O
speech	B
recognition	I
bioinformatics	B
part-of-speech	B
tagging	B
code	O
exercises	O
continuous-state	O
markov	O
models	O
stationary	B
distribution	B
with	O
noise	O
observed	O
linear	B
dynamical	O
systems	O
auto-regressive	B
models	O
training	B
an	O
ar	O
model	B
ar	O
model	B
as	O
an	O
olds	O
time-varying	B
ar	O
model	B
latent	B
linear	B
dynamical	O
systems	O
inference	B
filtering	O
smoothing	B
rauch-tung-striebel	B
correction	O
method	O
the	O
likelihood	B
most	B
likely	I
state	I
time	O
independence	B
and	O
riccati	B
equations	I
identifiability	B
issues	O
em	B
algorithm	B
subspace	O
methods	O
structured	B
ldss	O
bayesian	B
ldss	O
inference	B
maximum	B
likelihood	B
learning	B
using	O
em	B
code	O
autoregressive	O
models	O
learning	B
linear	B
dynamical	O
systems	O
switching	B
auto-regressive	B
models	O
exercises	O
xvi	O
draft	O
march	O
contents	O
contents	O
switching	B
linear	B
dynamical	O
systems	O
introduction	O
the	O
switching	B
lds	O
exact	O
inference	B
is	O
computationally	O
intractable	O
gaussian	B
sum	I
filtering	I
continuous	B
filtering	O
discrete	B
filtering	O
the	O
likelihood	B
collapsing	O
gaussians	O
relation	O
to	O
other	O
methods	O
gaussian	B
sum	I
smoothing	B
continuous	B
smoothing	B
discrete	B
smoothing	B
collapsing	B
the	I
mixture	B
using	O
mixtures	O
in	O
smoothing	B
relation	O
to	O
other	O
methods	O
reset	O
models	O
a	O
poisson	B
reset	B
model	B
hmm-reset	O
code	O
exercises	O
distributed	B
computation	I
introduction	O
stochastic	O
hopfield	O
networks	O
learning	B
sequences	B
a	O
single	O
sequence	O
multiple	O
sequences	B
boolean	O
networks	O
sequence	O
disambiguation	O
tractable	O
continuous	B
latent	B
variable	I
models	O
deterministic	B
latent	B
variables	O
an	O
augmented	B
hopfield	B
network	I
stochastically	O
spiking	O
neurons	O
hopfield	O
membrane	O
potential	B
dynamic	B
synapses	I
leaky	B
integrate	I
and	I
fire	I
models	O
code	O
exercises	O
neural	O
models	O
v	O
approximate	B
inference	B
sampling	B
introduction	O
univariate	B
sampling	B
multi-variate	B
sampling	B
ancestral	B
sampling	B
dealing	O
with	O
evidence	O
perfect	B
sampling	B
for	O
a	O
markov	B
network	I
gibbs	B
sampling	B
gibbs	B
sampling	B
as	O
a	O
markov	B
chain	B
structured	B
gibbs	B
sampling	B
remarks	O
draft	O
march	O
xvii	O
contents	O
contents	O
markov	B
chain	B
monte	I
carlo	I
markov	O
chains	O
metropolis-hastings	B
sampling	B
auxiliary	B
variable	I
methods	O
hybrid	B
monte	I
carlo	I
swendson-wang	B
slice	B
sampling	B
importance	B
sampling	B
sequential	B
importance	B
sampling	B
particle	O
filtering	O
as	O
an	O
approximate	B
forward	O
pass	O
code	O
exercises	O
deterministic	B
approximate	B
inference	B
introduction	O
the	O
laplace	B
approximation	B
properties	B
of	O
kullback-leibler	O
variational	B
inference	B
bounding	O
the	O
normalisation	B
constant	I
bounding	O
the	O
marginal	B
likelihood	B
gaussian	B
approximations	O
using	O
kl	O
divergence	B
moment	O
matching	O
properties	B
of	O
minimising	O
klpq	O
variational	O
bounding	O
using	O
klqp	O
pairwise	B
markov	B
random	B
field	I
general	O
mean	B
field	O
equations	O
asynchronous	B
updating	I
guarantees	O
approximation	B
improvement	O
intractable	B
energy	B
structured	B
variational	O
approximation	B
mutual	B
information	B
maximisation	B
a	O
kl	O
variational	B
approach	B
the	O
information	B
maximisation	B
algorithm	B
linear	B
gaussian	B
decoder	O
loopy	B
belief	B
propagation	B
classical	O
bp	O
on	O
an	O
undirected	B
graph	B
loopy	B
bp	O
as	O
a	O
variational	O
procedure	O
expectation	B
propagation	B
map	B
for	O
mrfs	O
map	B
assignment	O
attractive	B
binary	I
mrfs	O
potts	B
model	B
further	O
reading	O
a	O
background	O
mathematics	O
linear	B
algebra	I
vector	B
algebra	I
the	O
scalar	B
product	I
as	O
a	O
projection	B
lines	O
in	O
space	O
planes	O
and	O
hyperplanes	O
matrices	O
linear	B
transformations	O
determinants	O
matrix	B
inversion	B
computing	O
the	O
matrix	B
inverse	O
eigenvalues	O
and	O
eigenvectors	O
matrix	B
decompositions	O
xviii	O
draft	O
march	O
contents	O
contents	O
critical	O
points	O
matrix	B
identities	O
multivariate	B
calculus	B
interpreting	O
the	O
gradient	B
vector	O
higher	O
derivatives	O
chain	B
rule	I
matrix	B
calculus	B
inequalities	O
convexity	O
jensen	O
s	O
inequality	O
optimisation	B
gradient	B
descent	B
gradient	B
descent	B
with	O
fixed	O
stepsize	O
gradient	B
descent	B
with	O
momentum	B
gradient	B
descent	B
with	O
line	O
searches	O
exact	O
line	B
search	I
condition	O
multivariate	B
minimization	O
quadratic	O
functions	O
minimising	O
quadratic	O
functions	O
using	O
line	B
search	I
gram-schmidt	O
construction	B
of	O
conjugate	B
vectors	O
the	O
conjugate	B
vectors	I
algorithm	B
the	O
conjugate	B
gradients	I
algorithm	B
newton	O
s	O
method	O
quasi-newton	O
methods	O
constrained	B
optimisation	B
using	O
lagrange	O
multipliers	O
draft	O
march	O
xix	O
contents	O
contents	O
xx	O
draft	O
march	O
part	O
i	O
inference	B
in	O
probabilistic	B
models	O
chapter	O
probabilistic	B
reasoning	O
probability	O
refresher	O
variables	O
states	O
and	O
notational	O
shortcuts	O
variables	O
will	O
be	O
denoted	O
using	O
either	O
upper	O
case	O
x	O
or	O
lower	O
case	O
x	O
and	O
a	O
set	O
of	O
variables	O
will	O
typically	O
be	O
denoted	O
by	O
a	O
calligraphic	O
symbol	O
for	O
example	O
v	O
b	O
c	O
the	O
domain	B
of	O
a	O
variable	O
x	O
is	O
written	O
domx	O
and	O
denotes	O
the	O
states	O
x	O
can	O
take	O
states	O
will	O
typically	O
be	O
represented	O
using	O
sans-serif	O
font	O
for	O
example	O
for	O
a	O
coin	O
c	O
we	O
might	O
have	O
domc	O
tails	O
and	O
pc	O
heads	O
represents	O
the	O
probability	O
that	O
variable	O
c	O
is	O
in	O
state	O
heads	O
the	O
meaning	O
of	O
pstate	O
will	O
often	O
be	O
clear	O
without	O
specific	O
reference	O
to	O
a	O
variable	O
for	O
example	O
if	O
we	O
are	O
discussing	O
an	O
experiment	O
about	O
a	O
coin	O
c	O
the	O
meaning	O
of	O
pheads	O
is	O
clear	O
from	O
the	O
context	O
being	O
shortx	O
fx	O
the	O
hand	O
for	O
pc	O
heads	O
when	O
summing	O
performing	O
some	O
other	O
operation	O
over	O
a	O
interpretation	O
is	O
that	O
all	O
states	O
of	O
x	O
are	O
included	O
i	O
e	O
x	O
fx	O
s	O
domx	O
fx	O
s	O
for	O
our	O
purposes	O
events	O
are	O
expressions	O
about	O
random	O
variables	O
such	O
as	O
two	O
heads	O
in	O
coin	O
tosses	O
two	O
events	O
are	O
mutually	O
exclusive	O
if	O
they	O
cannot	O
both	O
simultaneously	O
occur	O
for	O
example	O
the	O
events	O
the	O
coin	O
is	O
heads	O
and	O
the	O
coin	O
is	O
tails	O
are	O
mutually	O
exclusive	O
one	O
can	O
think	O
of	O
defining	O
a	O
new	O
variable	O
named	O
by	O
the	O
event	O
so	O
for	O
example	O
pthe	O
coin	O
is	O
tails	O
can	O
be	O
interpreted	O
as	O
pthe	O
coin	O
is	O
tails	O
true	O
we	O
use	O
px	O
tr	O
for	O
the	O
probability	O
of	O
eventvariable	O
x	O
being	O
in	O
the	O
state	O
true	O
and	O
px	O
fa	O
for	O
the	O
probability	O
of	O
eventvariable	O
x	O
being	O
in	O
the	O
state	O
false	O
the	O
rules	O
of	O
probability	O
definition	O
of	O
probability	O
variables	O
the	O
probability	O
of	O
an	O
event	O
x	O
occurring	O
is	O
represented	O
by	O
a	O
value	B
between	O
and	O
px	O
means	O
that	O
we	O
are	O
certain	O
that	O
the	O
event	O
does	O
occur	O
conversely	O
px	O
means	O
that	O
we	O
are	O
certain	O
that	O
the	O
event	O
does	O
not	O
occur	O
the	O
summation	O
of	O
the	O
probability	O
over	O
all	O
the	O
states	O
is	O
px	O
x	O
x	O
such	O
probabilities	O
are	O
normalised	O
we	O
will	O
usually	O
more	O
conveniently	O
x	O
px	O
probability	O
refresher	O
two	O
events	O
x	O
and	O
y	O
can	O
interact	O
through	O
px	O
or	O
y	O
px	O
py	O
px	O
and	O
y	O
we	O
will	O
use	O
the	O
shorthand	O
px	O
y	O
for	O
px	O
and	O
y	O
note	O
that	O
py	O
x	O
px	O
y	O
and	O
px	O
or	O
y	O
py	O
or	O
x	O
definition	O
notation	O
an	O
alternative	O
notation	O
in	O
terms	O
of	O
set	O
theory	O
is	O
to	O
write	O
px	O
or	O
y	O
px	O
y	O
px	O
y	O
px	O
y	O
definition	O
given	O
a	O
joint	B
distribution	B
px	O
y	O
the	O
distribution	B
of	O
a	O
single	O
variable	O
is	O
given	O
by	O
px	O
y	O
px	O
y	O
here	O
px	O
is	O
termed	O
a	O
marginal	B
of	O
the	O
joint	B
probability	O
distribution	B
px	O
y	O
the	O
process	O
of	O
computing	O
a	O
marginal	B
from	O
a	O
joint	B
distribution	B
is	O
called	O
marginalisation	B
more	O
generally	O
one	O
has	O
xi	O
xn	O
xn	O
xi	O
an	O
important	O
definition	O
that	O
will	O
play	O
a	O
central	O
role	O
in	O
this	O
book	O
is	O
conditional	B
probability	I
definition	O
probability	O
bayes	O
rule	O
the	O
probability	O
of	O
event	O
x	O
conditioned	O
on	O
knowing	O
event	O
y	O
more	O
shortly	O
the	O
probability	O
of	O
x	O
given	O
y	O
is	O
defined	O
as	O
pxy	O
px	O
y	O
py	O
if	O
py	O
then	O
pxy	O
is	O
not	O
defined	O
probability	O
density	B
functions	O
definition	O
density	B
functions	O
for	O
a	O
single	O
continuous	B
variable	O
x	O
the	O
probability	O
density	B
px	O
is	O
defined	O
such	O
that	O
px	O
pxdx	O
draft	O
march	O
probability	O
refresher	O
b	O
a	O
pa	O
x	O
b	O
as	O
shorthand	O
we	O
will	O
sometimes	O
b	O
pxdx	O
xa	O
px	O
particularly	O
when	O
we	O
want	O
an	O
expression	O
to	O
be	O
valid	O
for	O
either	O
continuous	B
or	O
discrete	B
variables	O
the	O
multivariate	B
case	O
is	O
analogous	O
with	O
integration	O
over	O
all	O
real	O
space	O
and	O
the	O
probability	O
that	O
x	O
belongs	O
to	O
a	O
region	O
of	O
the	O
space	O
defined	O
accordingly	O
for	O
continuous	B
variables	O
formally	O
speaking	O
events	O
are	O
defined	O
for	O
the	O
variable	O
occurring	O
within	O
a	O
defined	O
region	O
for	O
example	O
px	O
fxdx	O
where	O
here	O
fx	O
is	O
the	O
probability	O
density	B
function	B
of	O
the	O
continuous	B
random	O
variable	O
x	O
unlike	O
probabilities	O
probability	O
densities	O
can	O
take	O
positive	O
values	O
greater	O
than	O
appear	O
strange	O
the	O
nervous	O
reader	O
may	O
simply	O
replace	O
our	O
px	O
x	O
notation	O
formally	O
speaking	O
for	O
a	O
continuous	B
variable	O
one	O
should	O
not	O
speak	O
of	O
the	O
probability	O
that	O
x	O
since	O
the	O
probability	O
of	O
a	O
single	O
value	B
is	O
always	O
zero	O
however	O
we	O
shall	O
often	O
write	O
px	O
for	O
continuous	B
variables	O
thus	O
not	O
distinguishing	O
between	O
probabilities	O
and	O
probability	O
density	B
function	B
values	O
whilst	O
this	O
may	O
x	O
fxdx	O
where	O
is	O
a	O
small	O
region	O
centred	O
on	O
x	O
this	O
is	O
well	O
defined	O
in	O
a	O
probabilistic	B
sense	O
and	O
in	O
the	O
limit	O
being	O
very	O
small	O
this	O
would	O
give	O
approximately	O
fx	O
if	O
we	O
consistently	O
use	O
the	O
same	O
for	O
all	O
occurrences	O
of	O
pdfs	O
then	O
we	O
will	O
simply	O
have	O
a	O
common	O
prefactor	O
in	O
all	O
expressions	O
our	O
strategy	O
is	O
to	O
simply	O
ignore	O
these	O
values	O
in	O
the	O
end	O
only	O
relative	O
probabilities	O
will	O
be	O
relevant	O
and	O
write	O
px	O
in	O
this	O
way	O
all	O
the	O
standard	O
rules	O
of	O
probability	O
carry	O
over	O
including	O
bayes	O
rule	O
interpreting	O
conditional	B
probability	I
imagine	O
a	O
circular	O
dart	O
board	O
split	O
into	O
equal	O
sections	O
labelled	B
from	O
to	O
and	O
randy	O
a	O
dart	O
thrower	O
who	O
hits	O
any	O
one	O
of	O
the	O
sections	O
uniformly	O
at	O
random	O
hence	O
the	O
probability	O
that	O
a	O
dart	O
thrown	O
by	O
randy	O
occurs	O
in	O
any	O
one	O
of	O
the	O
regions	O
is	O
pregion	O
i	O
a	O
friend	O
of	O
randy	O
tells	O
him	O
that	O
he	O
hasn	O
t	O
hit	O
the	O
region	O
what	O
is	O
the	O
probability	O
that	O
randy	O
has	O
hit	O
the	O
region	O
conditioned	O
on	O
this	O
information	O
only	O
regions	O
to	O
remain	O
possible	O
and	O
since	O
there	O
is	O
no	O
preference	O
for	O
randy	O
to	O
hit	O
any	O
of	O
these	O
regions	O
the	O
probability	O
is	O
the	O
conditioning	B
means	O
that	O
certain	O
states	O
are	O
now	O
inaccessible	O
and	O
the	O
original	O
probability	O
is	O
subsequently	O
distributed	O
over	O
the	O
remaining	O
accessible	O
states	O
from	O
the	O
rules	O
of	O
probability	O
pregion	O
region	O
pregion	O
not	O
region	O
pnot	O
region	O
pregion	O
pnot	O
region	O
giving	O
the	O
intuitive	O
result	O
pregion	O
in	O
the	O
above	O
pregion	O
not	O
region	O
pregion	O
an	O
important	O
point	O
to	O
clarify	O
is	O
that	O
pa	O
ab	O
b	O
should	O
not	O
be	O
interpreted	O
as	O
given	O
the	O
event	O
b	O
b	O
has	O
occurred	O
pa	O
ab	O
b	O
is	O
the	O
probability	O
of	O
the	O
event	O
a	O
a	O
occurring	O
in	O
most	O
contexts	O
no	O
such	O
explicit	O
temporal	O
causality	B
is	O
and	O
the	O
correct	O
interpretation	O
should	O
be	O
pa	O
ab	O
b	O
is	O
the	O
probability	O
of	O
a	O
being	O
in	O
state	O
a	O
under	O
the	O
constraint	O
that	O
b	O
is	O
in	O
state	O
b	O
constant	O
since	O
pa	O
a	O
b	O
b	O
is	O
not	O
a	O
distribution	B
in	O
a	O
in	O
other	O
make	O
it	O
a	O
distribution	B
we	O
need	O
to	O
divide	O
pa	O
a	O
b	O
the	O
relation	O
between	O
the	O
conditional	B
pa	O
ab	O
b	O
and	O
the	O
joint	B
pa	O
a	O
b	O
b	O
is	O
just	O
a	O
normalisation	B
a	O
pa	O
a	O
b	O
b	O
to	O
a	O
pa	O
a	O
b	O
b	O
which	O
when	O
summed	O
over	O
a	O
does	O
sum	O
to	O
indeed	O
this	O
is	O
just	O
the	O
definition	O
of	O
pa	O
ab	O
b	O
will	O
discuss	O
issues	O
related	O
to	O
causality	B
further	O
in	O
draft	O
march	O
probability	O
refresher	O
definition	O
events	O
x	O
and	O
y	O
are	O
independent	O
if	O
knowing	O
one	O
event	O
gives	O
no	O
extra	O
information	O
about	O
the	O
other	O
event	O
mathematically	O
this	O
is	O
expressed	O
by	O
px	O
y	O
pxpy	O
provided	O
that	O
px	O
and	O
py	O
independence	B
of	O
x	O
and	O
y	O
is	O
equivalent	B
to	O
pxy	O
px	O
pyx	O
py	O
if	O
pxy	O
px	O
for	O
all	O
states	O
of	O
x	O
and	O
y	O
then	O
the	O
variables	O
x	O
and	O
y	O
are	O
said	O
to	O
be	O
independent	O
if	O
px	O
y	O
kfxgy	O
for	O
some	O
constant	O
k	O
and	O
positive	O
functions	O
f	O
and	O
g	O
then	O
x	O
and	O
y	O
are	O
independent	O
deterministic	B
dependencies	O
sometimes	O
the	O
concept	O
of	O
independence	B
is	O
perhaps	O
a	O
little	O
strange	O
consider	O
the	O
following	O
variables	O
x	O
and	O
y	O
are	O
both	O
binary	O
domains	O
consist	O
of	O
two	O
states	O
we	O
define	O
the	O
distribution	B
such	O
that	O
x	O
and	O
y	O
are	O
always	O
both	O
in	O
a	O
certain	O
joint	B
state	O
px	O
a	O
y	O
px	O
a	O
y	O
px	O
b	O
y	O
px	O
b	O
y	O
are	O
x	O
and	O
y	O
dependent	O
the	O
reader	O
may	O
show	O
that	O
px	O
a	O
px	O
b	O
and	O
py	O
py	O
hence	O
pxpy	O
px	O
y	O
for	O
all	O
states	O
of	O
x	O
and	O
y	O
and	O
x	O
and	O
y	O
are	O
therefore	O
independent	O
this	O
may	O
seem	O
strange	O
we	O
know	O
for	O
sure	O
the	O
relation	O
between	O
x	O
and	O
y	O
namely	O
that	O
they	O
are	O
always	O
in	O
the	O
same	O
joint	B
state	O
yet	O
they	O
are	O
independent	O
since	O
the	O
distribution	B
is	O
trivially	O
concentrated	O
in	O
a	O
single	O
joint	B
state	O
knowing	O
the	O
state	O
of	O
x	O
tells	O
you	O
nothing	O
that	O
you	O
didn	O
t	O
anyway	O
know	O
about	O
the	O
state	O
of	O
y	O
and	O
vice	O
versa	O
this	O
potential	B
confusion	O
comes	O
from	O
using	O
the	O
term	O
independent	O
which	O
in	O
english	O
suggests	O
that	O
there	O
is	O
no	O
influence	O
or	O
relation	O
between	O
objects	O
discussed	O
the	O
best	O
way	O
to	O
think	O
about	O
statistical	O
independence	B
is	O
to	O
ask	O
whether	O
or	O
not	O
knowing	O
the	O
state	O
of	O
variable	O
y	O
tells	O
you	O
something	O
more	O
than	O
you	O
knew	O
before	O
about	O
variable	O
x	O
where	O
knew	O
before	O
means	O
working	O
with	O
the	O
joint	B
distribution	B
of	O
px	O
y	O
to	O
figure	O
out	O
what	O
we	O
can	O
know	O
about	O
x	O
namely	O
px	O
probability	O
tables	O
based	O
on	O
the	O
populations	O
and	O
of	O
england	O
scotland	O
and	O
wales	O
the	O
a	O
priori	O
probability	O
that	O
a	O
randomly	O
selected	O
person	O
from	O
these	O
three	O
countries	O
would	O
live	O
in	O
england	O
scotland	O
or	O
wales	O
would	O
be	O
approximately	O
and	O
respectively	O
we	O
can	O
write	O
this	O
as	O
a	O
vector	O
probability	O
table	O
pcnt	O
e	O
pcnt	O
s	O
pcnt	O
w	O
whose	O
component	O
values	O
sum	O
to	O
the	O
ordering	O
of	O
the	O
components	O
in	O
this	O
vector	O
is	O
arbitrary	O
as	O
long	O
as	O
it	O
is	O
consistently	O
applied	O
draft	O
march	O
probability	O
refresher	O
for	O
the	O
sake	O
of	O
simplicity	O
let	O
s	O
assume	O
that	O
only	O
three	O
mother	O
tongue	O
languages	O
exist	O
english	O
scottish	O
and	O
welsh	O
with	O
conditional	B
probabilities	O
given	O
the	O
country	O
of	O
residence	O
england	O
scotland	O
and	O
wales	O
we	O
write	O
a	O
conditional	B
probability	I
table	O
pm	O
t	O
engcnt	O
e	O
pm	O
t	O
scotcnt	O
e	O
pm	O
t	O
welcnt	O
e	O
pm	O
t	O
engcnt	O
s	O
pm	O
t	O
welcnt	O
s	O
pm	O
t	O
welcnt	O
w	O
pm	O
t	O
engcnt	O
w	O
pm	O
t	O
scotcnt	O
s	O
pm	O
t	O
scotcnt	O
w	O
from	O
this	O
we	O
can	O
form	O
a	O
joint	B
distribution	B
pcnt	O
m	O
t	O
pm	O
tcntpcnt	O
this	O
could	O
be	O
written	O
as	O
a	O
matrix	B
with	O
rows	O
indexed	O
by	O
country	O
and	O
columns	O
indexed	O
by	O
mother	O
tongue	O
the	O
joint	B
distribution	B
contains	O
all	O
the	O
information	O
about	O
the	O
model	B
of	O
this	O
environment	O
by	O
summing	O
a	O
column	O
of	O
this	O
table	O
we	O
have	O
the	O
marginal	B
pcnt	O
summing	O
the	O
row	O
gives	O
the	O
marginal	B
pm	O
t	O
similarly	O
one	O
could	O
easily	O
infer	O
pcntm	O
t	O
pcntm	O
t	O
t	O
from	O
this	O
joint	B
distribution	B
for	O
joint	B
distributions	O
over	O
a	O
larger	O
number	O
of	O
variables	O
xi	O
i	O
d	O
with	O
each	O
variable	O
xi	O
taking	O
ki	O
entries	O
explicitly	O
storing	O
tables	O
therefore	O
requires	O
space	O
exponential	B
in	O
the	O
number	O
of	O
variables	O
which	O
rapidly	O
becomes	O
impractical	O
for	O
a	O
large	O
number	O
of	O
variables	O
ki	O
states	O
the	O
table	O
describing	O
the	O
joint	B
distribution	B
is	O
an	O
array	O
a	O
probability	O
distribution	B
assigns	O
a	O
value	B
to	O
each	O
of	O
the	O
joint	B
states	O
of	O
the	O
variables	O
for	O
this	O
reason	O
pt	O
j	O
r	O
s	O
is	O
considered	O
equivalent	B
to	O
pj	O
s	O
r	O
t	O
any	O
such	O
reordering	O
of	O
the	O
variables	O
since	O
in	O
each	O
case	O
the	O
joint	B
setting	O
of	O
the	O
variables	O
is	O
simply	O
a	O
different	O
index	O
to	O
the	O
same	O
probability	O
this	O
situation	O
is	O
more	O
clear	O
in	O
the	O
set	O
theoretic	O
notation	O
pj	O
s	O
t	O
r	O
we	O
abbreviate	O
this	O
set	O
theoretic	O
notation	O
by	O
using	O
the	O
commas	O
however	O
one	O
should	O
be	O
careful	O
not	O
to	O
confuse	O
the	O
use	O
of	O
this	O
indexing	O
type	O
notation	O
with	O
functions	O
fx	O
y	O
which	O
are	O
in	O
general	O
dependent	O
on	O
the	O
variable	O
order	O
whilst	O
the	O
variables	O
to	O
the	O
left	O
of	O
the	O
conditioning	B
bar	O
may	O
be	O
written	O
in	O
any	O
order	O
and	O
equally	O
those	O
to	O
the	O
right	O
of	O
the	O
conditioning	B
bar	O
may	O
be	O
written	O
in	O
any	O
order	O
moving	O
variables	O
across	O
the	O
bar	O
is	O
not	O
generally	O
equivalent	B
so	O
that	O
interpreting	O
conditional	B
probability	I
together	O
with	O
the	O
rules	O
of	O
probability	O
conditional	B
probability	I
enables	O
one	O
to	O
reason	O
in	O
a	O
rational	O
logical	O
and	O
consistent	B
way	O
one	O
could	O
argue	O
that	O
much	O
of	O
science	O
deals	O
with	O
problems	O
of	O
the	O
form	O
tell	O
me	O
something	O
about	O
the	O
parameters	O
given	O
that	O
i	O
have	O
observed	O
data	O
d	O
and	O
have	O
some	O
knowledge	O
of	O
the	O
underlying	O
data	O
generating	O
mechanism	O
from	O
a	O
modelling	B
perspective	O
this	O
requires	O
p	O
pd	O
pd	O
pd	O
pd	O
this	O
shows	O
how	O
from	O
a	O
forward	O
or	O
generative	B
model	B
pd	O
of	O
the	O
dataset	O
and	O
coupled	B
with	O
a	O
prior	B
belief	O
p	O
about	O
which	O
parameter	B
values	O
are	O
appropriate	O
we	O
can	O
infer	O
the	O
posterior	B
distribution	B
p	O
of	O
parameters	O
in	O
light	O
of	O
the	O
observed	O
data	O
this	O
use	O
of	O
a	O
generative	B
model	B
sits	O
well	O
with	O
physical	O
models	O
of	O
the	O
world	O
which	O
typically	O
postulate	O
how	O
to	O
generate	O
observed	O
phenomena	O
assuming	O
we	O
know	O
the	O
correct	O
parameters	O
of	O
the	O
model	B
for	O
example	O
one	O
might	O
postulate	O
how	O
to	O
generate	O
a	O
time-series	O
of	O
displacements	O
for	O
a	O
swinging	O
pendulum	O
but	O
with	O
unknown	O
mass	O
length	O
and	O
damping	O
constant	O
using	O
this	O
generative	B
model	B
and	O
given	O
only	O
the	O
displacements	O
we	O
could	O
infer	O
the	O
unknown	O
physical	O
properties	B
of	O
the	O
pendulum	O
such	O
as	O
its	O
mass	O
length	O
and	O
friction	O
damping	O
constant	O
draft	O
march	O
probabilistic	B
reasoning	O
subjective	B
probability	O
probability	O
is	O
a	O
contentious	O
topic	O
and	O
we	O
do	O
not	O
wish	O
to	O
get	O
bogged	O
down	O
by	O
the	O
debate	O
here	O
apart	O
from	O
pointing	O
out	O
that	O
it	O
is	O
not	O
necessarily	O
the	O
axioms	O
of	O
probability	O
that	O
are	O
contentious	O
rather	O
what	O
interpretation	O
we	O
should	O
place	O
on	O
them	O
in	O
some	O
cases	O
potential	B
repetitions	O
of	O
an	O
experiment	O
can	O
be	O
envisaged	O
so	O
that	O
the	O
long	O
run	O
frequentist	B
definition	O
of	O
probability	O
in	O
which	O
probabilities	O
are	O
defined	O
with	O
respect	O
to	O
a	O
potentially	O
infinite	O
repetition	O
of	O
experiments	O
makes	O
sense	O
for	O
example	O
in	O
coin	O
tossing	O
the	O
probability	O
of	O
heads	O
might	O
be	O
interpreted	O
as	O
if	O
i	O
were	O
to	O
repeat	O
the	O
experiment	O
of	O
flipping	O
a	O
coin	O
random	O
the	O
limit	O
of	O
the	O
number	O
of	O
heads	O
that	O
occurred	O
over	O
the	O
number	O
of	O
tosses	O
is	O
defined	O
as	O
the	O
probability	O
of	O
a	O
head	O
occurring	O
here	O
s	O
another	O
problem	B
that	O
is	O
typical	O
of	O
the	O
kind	O
of	O
scenario	O
one	O
might	O
face	O
in	O
a	O
machine	O
learning	B
situation	O
a	O
film	O
enthusiast	O
joins	O
a	O
new	O
online	B
film	O
service	O
based	O
on	O
expressing	O
a	O
few	O
films	O
a	O
user	O
likes	O
and	O
dislikes	O
the	O
online	B
company	O
tries	O
to	O
estimate	O
the	O
probability	O
that	O
the	O
user	O
will	O
like	O
each	O
of	O
the	O
films	O
in	O
their	O
database	O
if	O
we	O
were	O
to	O
define	O
probability	O
as	O
a	O
limiting	O
case	O
of	O
infinite	O
repetitions	O
of	O
the	O
same	O
experiment	O
this	O
wouldn	O
t	O
make	O
much	O
sense	O
in	O
this	O
case	O
since	O
we	O
can	O
t	O
repeat	O
the	O
experiment	O
however	O
if	O
we	O
assume	O
that	O
the	O
user	O
behaves	O
in	O
a	O
manner	O
consistent	B
with	O
other	O
users	O
we	O
should	O
be	O
able	O
to	O
exploit	O
the	O
large	O
amount	O
of	O
data	O
from	O
other	O
users	O
ratings	O
to	O
make	O
a	O
reasonable	O
guess	O
as	O
to	O
what	O
this	O
consumer	O
likes	O
this	O
degree	B
of	I
belief	I
or	O
bayesian	B
subjective	B
interpretation	O
of	O
probability	O
sidesteps	O
non-repeatability	O
issues	O
it	O
s	O
just	O
a	O
consistent	B
framework	O
for	O
manipulating	O
real	O
values	O
consistent	B
with	O
our	O
intuition	O
about	O
probabilistic	B
reasoning	O
the	O
axioms	O
of	O
probability	O
combined	O
with	O
bayes	O
rule	O
make	O
for	O
a	O
complete	O
reasoning	O
system	O
one	O
which	O
includes	O
traditional	O
deductive	O
logic	O
as	O
a	O
special	O
remark	O
the	O
central	O
paradigm	O
of	O
probabilistic	B
reasoning	O
is	O
to	O
identify	O
all	O
relevant	O
variables	O
xn	O
in	O
the	O
environment	O
and	O
make	O
a	O
probabilistic	B
model	B
xn	O
of	O
their	O
interaction	O
reasoning	O
is	O
then	O
performed	O
by	O
introducing	O
evidence	O
that	O
sets	O
variables	O
in	O
known	O
states	O
and	O
subsequently	O
computing	O
probabilities	O
of	O
interest	O
conditioned	O
on	O
this	O
evidence	O
example	O
consider	O
the	O
following	O
fictitious	O
scientific	O
information	O
doctors	O
find	O
that	O
people	O
with	O
kreuzfeld-jacob	O
disease	O
almost	O
invariably	O
ate	O
hamburgers	O
thus	O
phamburger	O
eaterkj	O
the	O
probability	O
of	O
an	O
individual	O
having	O
kj	O
is	O
currently	O
rather	O
low	O
about	O
one	O
in	O
assuming	O
eating	O
lots	O
of	O
hamburgers	O
is	O
rather	O
widespread	O
say	O
phamburger	O
eater	O
what	O
is	O
the	O
probability	O
that	O
a	O
hamburger	O
eater	O
will	O
have	O
kreuzfeld-jacob	O
disease	O
this	O
may	O
be	O
computed	O
as	O
pkj	O
eater	O
phamburger	O
eater	O
kj	O
phamburger	O
eater	O
phamburger	O
eaterkj	O
phamburger	O
eater	O
if	O
the	O
fraction	O
of	O
people	O
eating	O
hamburgers	O
was	O
rather	O
small	O
phamburger	O
eater	O
what	O
is	O
the	O
probability	O
that	O
a	O
regular	O
hamburger	O
eater	O
will	O
have	O
kreuzfeld-jacob	O
disease	O
repeating	O
the	O
above	O
calculation	O
this	O
is	O
given	O
by	O
draft	O
march	O
probabilistic	B
reasoning	O
intuitively	O
this	O
is	O
much	O
higher	O
than	O
in	O
scenario	O
since	O
here	O
we	O
can	O
be	O
more	O
sure	O
that	O
eating	O
hamburgers	O
is	O
related	O
to	O
the	O
illness	O
in	O
this	O
case	O
only	O
a	O
small	O
number	O
of	O
people	O
in	O
the	O
population	O
eat	O
hamburgers	O
and	O
most	O
of	O
them	O
get	O
ill	O
example	O
clouseau	O
inspector	O
clouseau	O
arrives	O
at	O
the	O
scene	O
of	O
a	O
crime	O
the	O
victim	O
lies	O
dead	O
in	O
the	O
room	O
and	O
the	O
inspector	O
quickly	O
finds	O
the	O
murder	O
weapon	O
a	O
knife	O
the	O
butler	O
and	O
maid	O
are	O
his	O
main	O
suspects	O
the	O
inspector	O
has	O
a	O
prior	B
belief	O
of	O
that	O
the	O
butler	O
is	O
the	O
murderer	O
and	O
a	O
prior	B
belief	O
of	O
that	O
the	O
maid	O
is	O
the	O
murderer	O
these	O
probabilities	O
are	O
independent	O
in	O
the	O
sense	O
that	O
pb	O
m	O
pbpm	O
is	O
possible	O
that	O
both	O
the	O
butler	O
and	O
the	O
maid	O
murdered	O
the	O
victim	O
or	O
neither	O
the	O
inspector	O
s	O
prior	B
criminal	O
knowledge	O
can	O
be	O
formulated	O
mathematically	O
as	O
follows	O
domb	O
domm	O
not	O
murderer	O
domk	O
used	O
knife	O
not	O
used	O
pb	O
murderer	O
pknife	O
usedb	O
not	O
murderer	O
m	O
not	O
murderer	O
pknife	O
usedb	O
not	O
murderer	O
m	O
murderer	O
m	O
not	O
murderer	O
pknife	O
usedb	O
murderer	O
pknife	O
usedb	O
murderer	O
m	O
murderer	O
pm	O
murderer	O
what	O
is	O
the	O
probability	O
that	O
the	O
butler	O
is	O
the	O
murderer	O
that	O
it	O
might	O
be	O
that	O
neither	O
is	O
the	O
murderer	O
using	O
b	O
for	O
the	O
two	O
states	O
of	O
b	O
and	O
m	O
for	O
the	O
two	O
states	O
of	O
m	O
pbk	O
pb	O
mk	O
m	O
m	O
plugging	O
in	O
the	O
values	O
we	O
have	O
pb	O
murdererknife	O
used	O
b	O
m	O
pkb	O
mpm	B
m	O
pkb	O
mpm	B
pb	O
m	O
k	O
pk	O
the	O
role	O
of	O
pknife	O
used	O
in	O
the	O
inspector	O
clouseau	O
example	O
can	O
cause	O
some	O
confusion	O
in	O
the	O
above	O
pknife	O
usedb	O
mpm	B
pknife	O
used	O
b	O
m	O
is	O
computed	O
to	O
be	O
but	O
surely	O
pknife	O
used	O
since	O
this	O
is	O
given	O
in	O
the	O
question	O
note	O
that	O
the	O
quantity	O
pknife	O
used	O
relates	O
to	O
the	O
prior	B
probability	O
the	O
model	B
assigns	O
to	O
the	O
knife	O
being	O
used	O
the	O
absence	O
of	O
any	O
other	O
information	O
if	O
we	O
know	O
that	O
the	O
knife	O
is	O
used	O
then	O
the	O
posterior	B
pknife	O
usedknife	O
used	O
pknife	O
used	O
knife	O
used	O
pknife	O
used	O
pknife	O
used	O
pknife	O
used	O
which	O
naturally	O
must	O
be	O
the	O
case	O
another	O
potential	B
confusion	O
is	O
the	O
choice	O
pb	O
murderer	O
pm	O
murderer	O
which	O
means	O
that	O
pb	O
not	O
murderer	O
pm	O
not	O
murderer	O
these	O
events	O
are	O
not	O
exclusive	O
and	O
it	O
s	O
just	O
coincidence	O
that	O
the	O
numerical	B
values	O
are	O
chosen	O
this	O
way	O
for	O
example	O
we	O
could	O
have	O
also	O
chosen	O
pb	O
murderer	O
pm	O
murderer	O
which	O
means	O
that	O
pb	O
not	O
murderer	O
pm	O
not	O
murderer	O
draft	O
march	O
p	O
pd	O
pd	O
prior	B
likelihood	B
evidence	O
p	O
posterior	B
the	O
evidence	O
is	O
also	O
called	O
the	O
marginal	B
likelihood	B
the	O
term	O
likelihood	B
is	O
used	O
for	O
the	O
probability	O
that	O
a	O
model	B
generates	O
observed	O
data	O
more	O
fully	O
if	O
we	O
condition	O
on	O
the	O
model	B
m	O
we	O
have	O
p	O
m	O
pd	O
mp	O
pdm	O
where	O
we	O
see	O
the	O
role	O
of	O
the	O
likelihood	B
pd	O
m	O
and	O
marginal	B
likelihood	B
pdm	O
the	O
marginal	B
likelihood	B
is	O
also	O
called	O
the	O
model	B
likelihood	B
the	O
most	B
probable	I
a	I
posteriori	I
argmax	O
p	O
m	O
setting	O
is	O
that	O
which	O
maximises	O
the	O
posterior	B
bayes	O
rule	O
tells	O
us	O
how	O
to	O
update	O
our	O
prior	B
knowledge	O
with	O
the	O
data	O
generating	O
mechanism	O
the	O
prior	B
distribution	B
p	O
describes	O
the	O
information	O
we	O
have	O
about	O
the	O
variable	O
before	O
seeing	O
any	O
data	O
after	O
data	O
d	O
arrives	O
we	O
update	O
the	O
prior	B
distribution	B
to	O
the	O
posterior	B
p	O
pd	O
two	O
dice	O
what	O
were	O
the	O
individual	O
scores	O
two	O
fair	O
dice	O
are	O
rolled	O
someone	O
tells	O
you	O
that	O
the	O
sum	O
of	O
the	O
two	O
scores	O
is	O
what	O
is	O
the	O
probability	O
distribution	B
of	O
the	O
two	O
dice	O
the	O
score	O
of	O
die	O
a	O
is	O
denoted	O
sa	O
with	O
domsa	O
and	O
similarly	O
for	O
sb	O
the	O
three	O
variables	O
involved	O
are	O
then	O
sa	O
sb	O
and	O
the	O
total	O
score	O
t	O
sa	O
sb	O
a	O
model	B
of	O
these	O
three	O
variables	O
naturally	O
takes	O
the	O
form	O
pt	O
sa	O
sb	O
ptsa	O
sb	O
likelihood	B
psa	O
sb	O
prior	B
prior	B
likelihood	B
and	O
posterior	B
the	O
prior	B
likelihood	B
and	O
posterior	B
are	O
all	O
probabilities	O
they	O
are	O
assigned	O
these	O
names	O
due	O
to	O
their	O
role	O
in	O
bayes	O
rule	O
described	O
below	O
prior	B
likelihood	B
and	O
posterior	B
definition	O
prior	B
likelihood	B
and	O
posterior	B
for	O
data	O
d	O
and	O
variable	O
bayes	O
rule	O
tells	O
us	O
how	O
to	O
update	O
our	O
prior	B
beliefs	O
about	O
the	O
variable	O
in	O
light	O
of	O
the	O
data	O
to	O
a	O
posterior	B
belief	O
the	O
prior	B
psa	O
sb	O
is	O
the	O
joint	B
probability	O
of	O
score	O
sa	O
and	O
score	O
sb	O
without	O
knowing	O
anything	O
else	O
assuming	O
no	O
dependency	O
in	O
the	O
rolling	O
mechanism	O
psa	O
sb	O
psapsb	O
since	O
the	O
dice	O
are	O
fair	O
both	O
psa	O
and	O
psb	O
are	O
uniform	B
distributions	O
psa	O
s	O
example	O
is	O
due	O
to	O
taylan	O
cemgil	O
sa	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
psapsb	O
sa	O
sa	O
sa	O
draft	O
march	O
sa	O
sa	O
further	O
worked	O
examples	O
here	O
the	O
likelihood	B
term	O
is	O
ptsa	O
sb	O
i	O
sa	O
sb	O
which	O
states	O
that	O
the	O
total	O
score	O
is	O
given	O
by	O
sa	O
sb	O
here	O
i	O
y	O
is	O
the	O
indicator	B
function	B
defined	O
as	O
i	O
y	O
if	O
x	O
y	O
and	O
otherwise	O
sa	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
pt	O
sb	O
sa	O
sa	O
sa	O
sa	O
sa	O
hence	O
our	O
complete	O
model	B
is	O
pt	O
sa	O
sb	O
ptsa	O
sbpsapsb	O
where	O
the	O
terms	O
on	O
the	O
right	O
are	O
explicitly	O
defined	O
our	O
interest	O
is	O
then	O
obtainable	O
using	O
bayes	O
rule	O
where	O
pt	O
psa	O
sbt	O
pt	O
sbpsapsb	O
pt	O
the	O
term	O
pt	O
sasb	O
pt	O
sbpsapsb	O
pt	O
sbpsapsb	O
sa	O
sa	O
sa	O
sa	O
sa	O
sa	O
psa	O
sbt	O
sa	O
sa	O
sa	O
sa	O
sa	O
sa	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
sb	O
equal	O
mass	O
in	O
only	O
non-zero	O
elements	O
as	O
shown	O
sasb	O
pt	O
sbpsapsb	O
hence	O
the	O
posterior	B
is	O
given	O
by	O
further	O
worked	O
examples	O
example	O
s	O
in	O
the	O
bathroom	O
consider	O
a	O
household	O
of	O
three	O
people	O
alice	O
bob	O
and	O
cecil	O
cecil	O
wants	O
to	O
go	O
to	O
the	O
bathroom	O
but	O
finds	O
it	O
occupied	O
he	O
then	O
goes	O
to	O
alice	O
s	O
room	O
and	O
sees	O
she	O
is	O
there	O
since	O
cecil	O
knows	O
that	O
only	O
either	O
alice	O
or	O
bob	O
can	O
be	O
in	O
the	O
bathroom	O
from	O
this	O
he	O
infers	O
that	O
bob	O
must	O
be	O
in	O
the	O
bathroom	O
to	O
arrive	O
at	O
the	O
same	O
conclusion	O
in	O
a	O
mathematical	O
framework	O
let	O
s	O
define	O
the	O
following	O
events	O
a	O
alice	O
is	O
in	O
her	O
bedroom	O
b	O
bob	O
is	O
in	O
his	O
bedroom	O
o	O
bathroom	O
occupied	O
we	O
can	O
encode	O
the	O
information	O
that	O
if	O
either	O
alice	O
or	O
bob	O
are	O
not	O
in	O
their	O
bedrooms	O
then	O
they	O
must	O
be	O
in	O
the	O
bathroom	O
might	O
both	O
be	O
in	O
the	O
bathroom	O
as	O
po	O
tra	O
fa	O
b	O
po	O
tra	O
b	O
fa	O
the	O
first	O
term	O
expresses	O
that	O
the	O
bathroom	O
is	O
occupied	O
if	O
alice	O
is	O
not	O
in	O
her	O
bedroom	O
wherever	O
bob	O
is	O
similarly	O
the	O
second	O
term	O
expresses	O
bathroom	O
occupancy	O
as	O
long	O
as	O
bob	O
is	O
not	O
in	O
his	O
bedroom	O
then	O
pb	O
fao	O
tr	O
a	O
tr	O
pb	O
fa	O
o	O
tr	O
a	O
tr	O
po	O
tr	O
a	O
tr	O
where	O
po	O
tra	O
tr	O
b	O
fapa	O
tr	O
b	O
fa	O
po	O
tr	O
a	O
tr	O
po	O
tr	O
a	O
tr	O
po	O
tra	O
tr	O
b	O
fapa	O
tr	O
b	O
fa	O
po	O
tra	O
tr	O
b	O
trpa	O
tr	O
b	O
tr	O
draft	O
march	O
further	O
worked	O
examples	O
using	O
the	O
fact	O
po	O
tra	O
tr	O
b	O
fa	O
and	O
po	O
tra	O
tr	O
b	O
tr	O
which	O
encodes	O
that	O
if	O
if	O
alice	O
is	O
in	O
her	O
room	O
and	O
bob	O
is	O
not	O
the	O
bathroom	O
must	O
be	O
occupied	O
and	O
similarly	O
if	O
both	O
alice	O
and	O
bob	O
are	O
in	O
their	O
rooms	O
the	O
bathroom	O
cannot	O
be	O
occupied	O
pb	O
fao	O
tr	O
a	O
tr	O
pa	O
tr	O
b	O
fa	O
pa	O
tr	O
b	O
fa	O
this	O
example	O
is	O
interesting	O
since	O
we	O
are	O
not	O
required	O
to	O
make	O
a	O
full	O
probabilistic	B
model	B
in	O
this	O
case	O
thanks	O
to	O
the	O
limiting	O
nature	O
of	O
the	O
probabilities	O
don	O
t	O
need	O
to	O
specify	O
pa	O
b	O
the	O
situation	O
is	O
common	O
in	O
limiting	O
situations	O
of	O
probabilities	O
being	O
either	O
or	O
corresponding	O
to	O
traditional	O
logic	O
systems	O
example	O
resolution	B
we	O
can	O
represent	O
the	O
statement	O
all	O
apples	O
are	O
fruit	O
by	O
pf	O
tra	O
tr	O
similarly	O
all	O
fruits	O
grow	O
on	O
trees	O
may	O
be	O
represented	O
by	O
pt	O
trf	O
tr	O
additionally	O
we	O
assume	O
that	O
whether	O
or	O
not	O
something	O
grows	O
on	O
a	O
tree	B
depends	O
only	O
on	O
whether	O
or	O
not	O
it	O
is	O
a	O
fruit	O
pta	O
f	O
p	O
from	O
these	O
we	O
can	O
compute	O
pt	O
tra	O
tr	O
pt	O
trf	O
fa	O
pf	O
faa	O
tr	O
pt	O
trf	O
a	O
trpfa	O
tr	O
pf	O
tra	O
tr	O
pt	O
trf	O
tr	O
pt	O
trf	O
tr	O
f	O
f	O
in	O
other	O
words	O
we	O
have	O
deduced	O
that	O
all	O
apples	O
grow	O
on	O
trees	O
is	O
a	O
true	O
statement	O
based	O
on	O
the	O
information	O
presented	O
kind	O
of	O
reasoning	O
is	O
called	O
resolution	B
and	O
is	O
a	O
form	O
of	O
transitivity	O
from	O
the	O
statements	O
a	O
f	O
and	O
f	O
t	O
we	O
can	O
infer	O
a	O
t	O
example	O
inverse	B
modus	I
ponens	I
according	O
to	O
logic	O
from	O
the	O
statement	O
if	O
a	O
is	O
true	O
then	O
b	O
is	O
true	O
one	O
may	O
deduce	O
that	O
if	O
b	O
is	O
false	O
then	O
a	O
is	O
false	O
let	O
s	O
see	O
how	O
this	O
fits	O
in	O
with	O
a	O
probabilistic	B
reasoning	O
system	O
we	O
can	O
express	O
the	O
statement	O
if	O
a	O
is	O
true	O
then	O
b	O
is	O
true	O
as	O
pb	O
tra	O
tr	O
then	O
we	O
may	O
infer	O
pa	O
fab	O
fa	O
pa	O
trb	O
fa	O
pb	O
faa	O
trpa	O
tr	O
pb	O
faa	O
trpa	O
tr	O
pb	O
faa	O
fapa	O
fa	O
this	O
follows	O
since	O
pb	O
faa	O
tr	O
pb	O
tra	O
tr	O
annihilating	O
the	O
second	O
term	O
both	O
the	O
above	O
examples	O
are	O
intuitive	O
expressions	O
of	O
deductive	O
logic	O
the	O
standard	O
rules	O
of	O
aristotelian	O
logic	O
are	O
therefore	O
seen	O
to	O
be	O
limiting	O
cases	O
of	O
probabilistic	B
reasoning	O
example	O
xor	O
gate	O
a	O
standard	O
xor	O
logic	O
gate	O
is	O
given	O
by	O
the	O
table	O
on	O
the	O
right	O
if	O
we	O
observe	O
that	O
the	O
output	O
of	O
the	O
xor	O
gate	O
is	O
what	O
can	O
we	O
say	O
about	O
a	O
and	O
b	O
in	O
this	O
case	O
either	O
a	O
and	O
b	O
were	O
both	O
or	O
a	O
and	O
b	O
were	O
both	O
this	O
means	O
we	O
don	O
t	O
know	O
which	O
state	O
a	O
was	O
in	O
it	O
could	O
equally	O
likely	O
have	O
been	O
or	O
a	O
b	O
a	O
xor	O
b	O
draft	O
march	O
further	O
worked	O
examples	O
consider	O
a	O
soft	B
version	O
of	O
the	O
xor	O
gate	O
given	O
on	O
the	O
right	O
with	O
additionally	O
pa	O
pb	O
what	O
is	O
pa	O
pa	O
c	O
pa	O
b	O
c	O
b	O
b	O
pc	O
bpa	O
a	O
b	O
pc	O
b	O
pa	O
b	O
pc	O
b	O
pa	O
c	O
pa	O
b	O
c	O
b	O
b	O
pc	O
bpa	O
pa	O
b	O
pc	O
b	O
then	O
pa	O
pa	O
c	O
pa	O
c	O
pa	O
c	O
example	O
larry	O
is	O
typically	O
late	O
for	O
school	O
if	O
larry	O
is	O
late	O
we	O
denote	O
this	O
with	O
l	O
late	O
otherwise	O
l	O
not	O
late	O
when	O
his	O
mother	O
asks	O
whether	O
or	O
not	O
he	O
was	O
late	O
for	O
school	O
he	O
never	O
admits	O
to	O
being	O
late	O
the	O
response	O
larry	O
gives	O
rl	O
is	O
represented	O
as	O
follows	O
prl	O
not	O
latel	O
not	O
late	O
prl	O
latel	O
late	O
the	O
remaining	O
two	O
values	O
are	O
determined	O
by	O
normalisation	B
and	O
are	O
prl	O
latel	O
not	O
late	O
prl	O
not	O
latel	O
late	O
given	O
that	O
rl	O
not	O
late	O
what	O
is	O
the	O
probability	O
that	O
larry	O
was	O
late	O
i	O
e	O
pl	O
laterl	O
not	O
late	O
using	O
bayes	O
we	O
have	O
pl	O
laterl	O
not	O
late	O
pl	O
late	O
rl	O
not	O
late	O
prl	O
not	O
late	O
pl	O
late	O
rl	O
not	O
late	O
pl	O
late	O
rl	O
not	O
late	O
pl	O
not	O
late	O
rl	O
not	O
late	O
in	O
the	O
above	O
pl	O
late	O
rl	O
not	O
late	O
prl	O
not	O
latel	O
late	O
pl	O
late	O
and	O
pl	O
not	O
late	O
rl	O
not	O
late	O
prl	O
not	O
latel	O
not	O
late	O
pl	O
not	O
late	O
hence	O
pl	O
laterl	O
not	O
late	O
pl	O
late	O
pl	O
late	O
pl	O
not	O
late	O
pl	O
late	O
draft	O
march	O
where	O
we	O
used	O
normalisation	B
in	O
the	O
last	O
step	O
pl	O
late	O
pl	O
not	O
late	O
this	O
result	O
is	O
intuitive	O
larry	O
s	O
mother	O
knows	O
that	O
he	O
never	O
admits	O
to	O
being	O
late	O
so	O
her	O
belief	O
about	O
whether	O
or	O
not	O
he	O
really	O
was	O
late	O
is	O
unchanged	O
regardless	O
of	O
what	O
larry	O
actually	O
says	O
further	O
worked	O
examples	O
example	O
and	O
sue	O
continuing	O
the	O
example	O
above	O
larry	O
s	O
sister	O
sue	O
always	O
tells	O
the	O
truth	O
to	O
her	O
mother	O
as	O
to	O
whether	O
or	O
not	O
larry	O
was	O
late	O
for	O
school	O
prs	O
not	O
latel	O
not	O
late	O
prs	O
latel	O
late	O
the	O
remaining	O
two	O
values	O
are	O
determined	O
by	O
normalisation	B
and	O
are	O
prs	O
latel	O
not	O
late	O
prs	O
not	O
latel	O
late	O
we	O
also	O
assume	O
prs	O
rll	O
prslprll	O
we	O
can	O
then	O
write	O
prl	O
rs	O
l	O
prllprslpl	O
given	O
that	O
rs	O
late	O
what	O
is	O
the	O
probability	O
that	O
larry	O
was	O
late	O
using	O
bayes	O
rule	O
we	O
have	O
pl	O
laterl	O
not	O
late	O
rs	O
late	O
z	O
prs	O
latel	O
lateprl	O
not	O
latel	O
latepl	O
late	O
where	O
the	O
normalisation	B
z	O
is	O
given	O
by	O
prs	O
latel	O
lateprl	O
not	O
latel	O
latepl	O
late	O
prs	O
latel	O
not	O
lateprl	O
not	O
latel	O
not	O
latepl	O
not	O
late	O
hence	O
pl	O
laterl	O
not	O
late	O
rs	O
late	O
pl	O
late	O
pl	O
late	O
pl	O
not	O
late	O
this	O
result	O
is	O
also	O
intuitive	O
since	O
larry	O
s	O
mother	O
knows	O
that	O
sue	O
always	O
tells	O
the	O
truth	O
no	O
matter	O
what	O
larry	O
says	O
she	O
knows	O
he	O
was	O
late	O
example	O
luke	O
has	O
been	O
told	O
he	O
s	O
lucky	O
and	O
has	O
won	O
a	O
prize	O
in	O
the	O
lottery	O
there	O
are	O
prizes	O
available	O
of	O
value	B
the	O
prior	B
probabilities	O
of	O
winning	O
these	O
prizes	O
are	O
with	O
being	O
the	O
prior	B
probability	O
of	O
winning	O
no	O
prize	O
luke	O
asks	O
eagerly	O
did	O
i	O
win	O
i	O
m	O
afraid	O
not	O
sir	O
is	O
the	O
response	O
of	O
the	O
lottery	O
phone	O
operator	O
did	O
i	O
win	O
asks	O
luke	O
again	O
i	O
m	O
afraid	O
not	O
sir	O
what	O
is	O
the	O
probability	O
that	O
luke	O
has	O
won	O
note	O
first	O
that	O
we	O
denote	O
w	O
for	O
the	O
first	O
prize	O
of	O
and	O
w	O
for	O
the	O
remaining	O
prizes	O
and	O
w	O
for	O
no	O
prize	O
we	O
need	O
to	O
compute	O
pw	O
w	O
w	O
pw	O
w	O
w	O
w	O
pw	O
w	O
w	O
pw	O
or	O
w	O
or	O
w	O
pw	O
draft	O
march	O
code	O
where	O
the	O
term	O
in	O
the	O
denominator	O
is	O
computed	O
using	O
the	O
fact	O
that	O
the	O
events	O
w	O
are	O
mutually	O
exclusive	O
can	O
only	O
win	O
one	O
prize	O
this	O
result	O
makes	O
intuitive	O
sense	O
once	O
we	O
have	O
removed	O
the	O
impossible	O
states	O
of	O
w	O
the	O
probability	O
that	O
luke	O
wins	O
the	O
prize	O
is	O
proportional	O
to	O
the	O
prior	B
probability	O
of	O
that	O
prize	O
with	O
the	O
normalisation	B
being	O
simply	O
the	O
total	O
set	O
of	O
possible	O
probability	O
remaining	O
code	O
the	O
brmltoolbox	O
code	O
accompanying	O
this	O
book	O
is	O
intended	O
to	O
give	O
the	O
reader	O
some	O
insight	O
into	O
representing	O
discrete	B
probability	O
tables	O
and	O
performing	O
simple	O
inference	B
the	O
matlab	O
code	O
is	O
written	O
with	O
only	O
minimal	O
error	O
trapping	O
to	O
keep	O
the	O
code	O
as	O
short	O
and	O
hopefully	O
reasonably	O
basic	O
probability	O
code	O
at	O
the	O
simplest	O
level	O
we	O
only	O
need	O
two	O
basic	O
routines	O
one	O
for	O
multiplying	O
probability	O
tables	O
together	O
potentials	O
in	O
the	O
code	O
and	O
one	O
for	O
summing	O
a	O
probability	O
table	O
potentials	O
are	O
represented	O
using	O
a	O
structure	B
for	O
example	O
in	O
the	O
code	O
corresponding	O
to	O
the	O
inspector	O
clouseau	O
example	O
democlouseau	O
m	O
we	O
define	O
a	O
probability	O
table	O
as	O
ans	O
variables	O
table	O
double	O
this	O
says	O
that	O
the	O
potential	B
depends	O
on	O
the	O
variables	O
and	O
the	O
entries	O
are	O
stored	O
in	O
the	O
array	O
given	O
by	O
the	O
table	O
field	O
the	O
size	O
of	O
the	O
array	O
informs	O
how	O
many	O
states	O
each	O
variable	O
takes	O
in	O
the	O
order	O
given	O
by	O
variables	O
the	O
order	O
in	O
which	O
the	O
variables	O
are	O
defined	O
in	O
a	O
potential	B
is	O
irrelevant	O
provided	O
that	O
one	O
indexes	O
the	O
array	O
consistently	O
a	O
routine	O
that	O
can	O
help	O
with	O
setting	O
table	O
entries	O
is	O
setstate	O
m	O
for	O
example	O
means	O
that	O
for	O
potential	B
the	O
table	O
entry	O
for	O
variable	O
being	O
in	O
state	O
variable	O
being	O
in	O
state	O
and	O
variable	O
being	O
in	O
state	O
should	O
be	O
set	O
to	O
value	B
the	O
philosophy	O
of	O
the	O
code	O
is	O
to	O
keep	O
the	O
information	O
required	O
to	O
perform	O
computations	O
to	O
a	O
minimum	O
additional	O
information	O
about	O
the	O
labels	O
of	O
variables	O
and	O
their	O
domains	O
can	O
be	O
useful	O
to	O
check	B
results	O
but	O
is	O
not	O
actually	O
required	O
to	O
carry	O
out	O
computations	O
one	O
may	O
also	O
specify	O
the	O
name	O
and	O
domain	B
of	O
each	O
variable	O
for	O
example	O
ans	O
domain	B
murderer	O
not	O
murderer	O
name	O
butler	O
the	O
variable	O
name	O
and	O
domain	B
information	O
in	O
the	O
clouseau	O
example	O
is	O
stored	O
in	O
the	O
structure	B
variable	O
which	O
can	O
be	O
helpful	O
to	O
display	O
the	O
potential	B
table	O
knife	O
knife	O
knife	O
knife	O
knife	O
knife	O
knife	O
knife	O
used	O
not	O
used	O
used	O
not	O
used	O
used	O
not	O
used	O
used	O
not	O
used	O
maid	O
maid	O
maid	O
maid	O
maid	O
maid	O
maid	O
maid	O
murderer	O
murderer	O
not	O
murderer	O
not	O
murderer	O
murderer	O
murderer	O
not	O
murderer	O
not	O
murderer	O
butler	O
butler	O
butler	O
butler	O
butler	O
butler	O
butler	O
butler	O
murderer	O
murderer	O
murderer	O
murderer	O
not	O
murderer	O
not	O
murderer	O
not	O
murderer	O
not	O
murderer	O
the	O
time	O
of	O
writing	O
some	O
versions	O
of	O
matlab	O
suffer	O
from	O
serious	O
memory	O
indexing	O
bugs	O
some	O
of	O
which	O
may	O
appear	O
in	O
the	O
array	O
structures	O
used	O
in	O
the	O
code	O
provided	O
to	O
deal	O
with	O
this	O
turn	O
off	O
the	O
jit	O
accelerator	O
by	O
typing	O
feature	O
accel	O
off	O
draft	O
march	O
multiplying	O
potentials	O
in	O
order	O
to	O
multiply	O
potentials	O
for	O
arrays	O
the	O
tables	O
of	O
each	O
potential	B
must	O
be	O
dimensionally	O
consistent	B
that	O
is	O
the	O
number	O
of	O
states	O
of	O
variable	O
i	O
in	O
potential	B
must	O
match	O
the	O
number	O
of	O
states	O
of	O
variable	O
i	O
in	O
any	O
other	O
potential	B
this	O
can	O
be	O
checked	O
using	O
potvariables	O
m	O
this	O
consistency	O
is	O
also	O
required	O
for	O
other	O
basic	O
operations	O
such	O
as	O
summing	O
potentials	O
multpots	O
m	O
multiplying	O
two	O
or	O
more	O
potentials	O
divpots	O
m	O
dividing	O
a	O
potential	B
by	O
another	O
code	O
summing	O
a	O
potential	B
sumpot	O
m	O
sum	O
a	O
potential	B
over	O
a	O
set	O
of	O
variables	O
sumpots	O
m	O
sum	O
a	O
set	O
of	O
potentials	O
together	O
making	O
a	O
conditional	B
potential	B
condpot	O
m	O
make	O
a	O
potential	B
conditioned	O
on	O
variables	O
setting	O
a	O
potential	B
setpot	O
m	O
set	O
variables	O
in	O
a	O
potential	B
to	O
given	O
states	O
setevpot	O
m	O
set	O
variables	O
in	O
a	O
potential	B
to	O
given	O
states	O
and	O
return	O
also	O
an	O
identity	O
potential	B
on	O
the	O
given	O
states	O
the	O
philosophy	O
of	O
brmltoolbox	O
is	O
that	O
all	O
information	O
about	O
variables	O
is	O
local	B
and	O
is	O
read	O
off	O
from	O
a	O
potential	B
using	O
setevpot	O
m	O
enables	O
one	O
to	O
set	O
variables	O
in	O
a	O
state	O
whilst	O
maintaining	O
information	O
about	O
the	O
number	O
of	O
states	O
of	O
a	O
variable	O
maximising	O
a	O
potential	B
maxpot	O
m	O
maximise	O
a	O
potential	B
over	O
a	O
set	O
of	O
variables	O
see	O
also	O
maxnarray	O
m	O
and	O
maxnpot	O
m	O
which	O
return	O
the	O
n-highest	O
values	O
and	O
associated	O
states	O
other	O
potential	B
utilities	O
setstate	O
m	O
set	O
the	O
a	O
potential	B
state	O
to	O
a	O
given	O
value	B
table	O
m	O
return	O
a	O
table	O
from	O
a	O
potential	B
whichpot	O
m	O
return	O
potentials	O
which	O
contain	O
a	O
set	O
of	O
variables	O
potvariables	O
m	O
variables	O
and	O
their	O
number	O
of	O
states	O
in	O
a	O
set	O
of	O
potentials	O
orderpotfields	O
m	O
order	O
the	O
fields	O
of	O
a	O
potential	B
structure	B
uniquepots	O
m	O
merge	O
redundant	O
potentials	O
and	O
return	O
only	O
unique	O
ones	O
numstates	O
m	O
number	O
of	O
states	O
of	O
a	O
variable	O
in	O
a	O
domain	B
squeezepots	O
m	O
remove	O
redundant	O
potentials	O
by	O
merging	O
normpot	O
m	O
normalise	O
a	O
potential	B
to	O
form	O
a	O
distribution	B
general	O
utilities	O
condp	O
m	O
return	O
a	O
table	O
pxy	O
from	O
px	O
y	O
condexp	O
m	O
form	O
a	O
conditional	B
distribution	B
from	O
a	O
log	O
value	B
logsumexp	O
m	O
compute	O
the	O
log	O
of	O
a	O
sum	O
of	O
exponentials	O
in	O
a	O
numerically	O
precise	O
way	O
normp	O
m	O
return	O
a	O
normalised	O
table	O
from	O
an	O
unnormalised	O
table	O
draft	O
march	O
exercises	O
assign	O
m	O
assign	O
values	O
to	O
multiple	O
variables	O
maxarray	O
m	O
maximize	O
a	O
multi-dimensional	O
array	O
over	O
a	O
subset	O
an	O
example	O
the	O
following	O
code	O
highlights	O
the	O
use	O
of	O
the	O
above	O
routines	O
in	O
solving	B
the	O
inspector	O
clouseau	O
democlouseau	O
m	O
solving	B
the	O
inspector	O
clouseau	O
example	O
notes	O
the	O
interpretation	O
of	O
probability	O
is	O
contentious	O
and	O
we	O
refer	O
the	O
reader	O
to	O
for	O
detailed	O
discussions	O
a	O
useful	O
website	B
that	O
relates	O
to	O
understanding	O
probability	O
and	O
bayesian	B
reasoning	O
is	O
understandinguncertainty	O
org	O
exercises	O
exercise	O
prove	O
px	O
yz	O
pxzpyx	O
z	O
and	O
also	O
pxy	O
z	O
pyx	O
zpxz	O
pyz	O
exercise	O
prove	O
the	O
bonferroni	B
inequality	I
pa	O
b	O
pa	O
pb	O
exercise	O
from	O
there	O
are	O
two	O
boxes	O
box	O
contains	O
three	O
red	O
and	O
five	O
white	O
balls	O
and	O
box	O
contains	O
two	O
red	O
and	O
five	O
white	O
balls	O
a	O
box	O
is	O
chosen	O
at	O
random	O
pbox	O
pbox	O
and	O
a	O
ball	O
chosen	O
at	O
random	O
from	O
this	O
box	O
turns	O
out	O
to	O
be	O
red	O
what	O
is	O
the	O
posterior	B
probability	O
that	O
the	O
red	O
ball	O
came	O
from	O
box	O
exercise	O
from	O
two	O
balls	O
are	O
placed	O
in	O
a	O
box	O
as	O
follows	O
a	O
fair	O
coin	O
is	O
tossed	O
and	O
a	O
white	O
ball	O
is	O
placed	O
in	O
the	O
box	O
if	O
a	O
head	O
occurs	O
otherwise	O
a	O
red	O
ball	O
is	O
placed	O
in	O
the	O
box	O
the	O
coin	O
is	O
tossed	O
again	O
and	O
a	O
red	O
ball	O
is	O
placed	O
in	O
the	O
box	O
if	O
a	O
tail	O
occurs	O
otherwise	O
a	O
white	O
ball	O
is	O
placed	O
in	O
the	O
box	O
balls	O
are	O
drawn	O
from	O
the	O
box	O
three	O
times	O
in	O
succession	O
with	O
replacing	O
the	O
drawn	O
ball	O
back	O
in	O
the	O
box	O
it	O
is	O
found	O
that	O
on	O
all	O
three	O
occasions	O
a	O
red	O
ball	O
is	O
drawn	O
what	O
is	O
the	O
probability	O
that	O
both	O
balls	O
in	O
the	O
box	O
are	O
red	O
exercise	O
david	O
spiegelhalter	O
understandinguncertainty	O
org	O
a	O
secret	O
government	O
agency	O
has	O
developed	O
a	O
scanner	O
which	O
determines	O
whether	O
a	O
person	O
is	O
a	O
terrorist	O
the	O
scanner	O
is	O
fairly	O
reliable	O
of	O
all	O
scanned	O
terrorists	O
are	O
identified	O
as	O
terrorists	O
and	O
of	O
all	O
upstanding	O
citizens	O
are	O
identified	O
as	O
such	O
an	O
informant	O
tells	O
the	O
agency	O
that	O
exactly	O
one	O
passenger	O
of	O
aboard	O
an	O
aeroplane	O
in	O
which	O
you	O
are	O
seated	O
is	O
a	O
terrorist	O
the	O
agency	O
decide	O
to	O
scan	O
each	O
passenger	O
and	O
the	O
shifty	O
looking	O
man	O
sitting	O
next	O
to	O
you	O
is	O
the	O
first	O
to	O
test	O
positive	O
what	O
are	O
the	O
chances	O
that	O
this	O
man	O
is	O
a	O
terrorist	O
exercise	O
monty	O
hall	O
problem	B
on	O
a	O
gameshow	O
there	O
are	O
three	O
doors	O
behind	O
one	O
door	O
is	O
a	O
prize	O
the	O
gameshow	O
host	O
asks	O
you	O
to	O
pick	O
a	O
door	O
he	O
then	O
opens	O
a	O
different	O
door	O
to	O
the	O
one	O
you	O
chose	O
and	O
shows	O
that	O
there	O
is	O
no	O
prize	O
behind	O
it	O
is	O
is	O
better	O
to	O
stick	O
with	O
your	O
original	O
guess	O
as	O
to	O
where	O
the	O
prize	O
is	O
or	O
better	O
to	O
change	O
your	O
mind	O
exercise	O
consider	O
three	O
variable	O
distributions	O
which	O
admit	O
the	O
factorisation	O
pa	O
b	O
c	O
pabpbcpc	O
where	O
all	O
variables	O
are	O
binary	O
how	O
many	O
parameters	O
are	O
needed	O
to	O
specify	O
distributions	O
of	O
this	O
form	O
draft	O
march	O
exercise	O
repeat	O
the	O
inspector	O
clouseau	O
scenario	O
but	O
with	O
the	O
restriction	O
that	O
either	O
the	O
maid	O
or	O
the	O
butler	O
is	O
the	O
murderer	O
but	O
not	O
both	O
explicitly	O
the	O
probability	O
of	O
the	O
maid	O
being	O
the	O
murder	O
and	O
not	O
the	O
butler	O
is	O
the	O
probability	O
of	O
the	O
butler	O
being	O
the	O
murder	O
and	O
not	O
the	O
maid	O
is	O
modify	O
democlouseau	O
m	O
to	O
implement	O
this	O
exercise	O
prove	O
exercises	O
pa	O
or	O
c	O
pa	O
b	O
pa	O
c	O
pa	O
b	O
c	O
exercise	O
prove	O
pxz	O
pxy	O
zpyz	O
y	O
yw	O
pxw	O
y	O
zpwy	O
zpyz	O
exercise	O
as	O
a	O
young	O
man	O
mr	O
gott	O
visits	O
berlin	O
in	O
he	O
s	O
surprised	O
that	O
he	O
cannot	O
cross	B
into	O
east	O
berlin	O
since	O
there	O
is	O
a	O
wall	O
separating	O
the	O
two	O
halves	O
of	O
the	O
city	O
he	O
s	O
told	O
that	O
the	O
wall	O
was	O
erected	O
years	O
previously	O
he	O
reasons	O
that	O
the	O
wall	O
will	O
have	O
a	O
finite	O
lifespan	O
his	O
ignorance	O
means	O
that	O
he	O
arrives	O
uniformly	O
at	O
random	O
at	O
some	O
time	O
in	O
the	O
lifespan	O
of	O
the	O
wall	O
since	O
only	O
of	O
the	O
time	O
one	O
would	O
arrive	O
in	O
the	O
first	O
or	O
last	O
of	O
the	O
lifespan	O
of	O
the	O
wall	O
he	O
asserts	O
that	O
with	O
confidence	O
the	O
wall	O
will	O
survive	O
between	O
and	O
years	O
in	O
the	O
now	O
professor	O
gott	O
is	O
pleased	O
to	O
find	O
that	O
his	O
prediction	O
was	O
correct	O
and	O
promotes	O
his	O
prediction	O
method	O
in	O
elite	O
journals	O
this	O
delta-t	O
method	O
is	O
widely	O
adopted	O
and	O
used	O
to	O
form	O
predictions	O
in	O
a	O
range	O
of	O
scenarios	O
about	O
which	O
researchers	O
are	O
totally	O
ignorant	O
would	O
you	O
buy	O
a	O
prediction	O
from	O
prof	O
gott	O
explain	O
carefully	O
your	O
reasoning	O
exercise	O
implement	O
the	O
soft	B
xor	O
gate	O
using	O
brmltoolbox	O
you	O
may	O
find	O
condpot	O
m	O
of	O
use	O
exercise	O
implement	O
the	O
hamburgers	O
scenarios	O
using	O
brmltoolbox	O
to	O
do	O
so	O
you	O
will	O
need	O
to	O
define	O
the	O
joint	B
distribution	B
phamburgers	O
kj	O
in	O
which	O
domhamburgers	O
domkj	O
fa	O
exercise	O
implement	O
the	O
two-dice	O
example	O
using	O
brmltoolbox	O
exercise	O
a	O
redistribution	O
lottery	O
involves	O
picking	O
the	O
correct	O
four	O
numbers	O
from	O
to	O
replacement	O
so	O
for	O
example	O
is	O
not	O
possible	O
the	O
order	O
of	O
the	O
picked	O
numbers	O
is	O
irrelevant	O
every	O
week	O
a	O
million	O
people	O
play	O
this	O
game	O
each	O
paying	O
to	O
enter	O
with	O
the	O
numbers	O
being	O
the	O
most	O
popular	O
in	O
every	O
people	O
chooses	O
these	O
numbers	O
given	O
that	O
the	O
million	O
pounds	O
prize	O
money	B
is	O
split	O
equally	O
between	O
winners	O
and	O
that	O
any	O
four	O
numbers	O
come	O
up	O
at	O
random	O
what	O
is	O
the	O
expected	O
amount	O
of	O
money	B
each	O
of	O
the	O
players	O
choosing	O
will	O
win	O
each	O
week	O
the	O
least	O
popular	O
set	O
of	O
numbers	O
is	O
with	O
only	O
in	O
people	O
choosing	O
this	O
how	O
much	O
do	O
they	O
profit	O
each	O
week	O
on	O
average	B
do	O
you	O
think	O
there	O
is	O
any	O
skill	O
involved	O
in	O
playing	O
this	O
lottery	O
exercise	O
in	O
a	O
test	O
of	O
psychometry	O
the	O
car	O
keys	O
and	O
wrist	O
watches	O
of	O
people	O
are	O
given	O
to	O
a	O
medium	O
the	O
medium	O
then	O
attempts	O
to	O
match	O
the	O
wrist	O
watch	O
with	O
the	O
car	O
key	O
of	O
each	O
person	O
what	O
is	O
the	O
expected	O
number	O
of	O
correct	O
matches	O
that	O
the	O
medium	O
will	O
make	O
chance	O
what	O
is	O
the	O
probability	O
that	O
the	O
medium	O
will	O
obtain	O
at	O
least	O
correct	O
match	O
draft	O
march	O
chapter	O
basic	O
graph	B
concepts	O
graphs	O
definition	O
a	O
graph	B
g	O
consists	O
of	O
vertices	O
and	O
edges	O
between	O
the	O
vertices	O
edges	O
may	O
be	O
directed	B
have	O
an	O
arrow	O
in	O
a	O
single	O
direction	O
or	O
undirected	B
a	O
graph	B
with	O
all	O
edges	O
directed	B
is	O
called	O
a	O
directed	B
graph	B
and	O
one	O
with	O
all	O
edges	O
undirected	B
is	O
called	O
an	O
undirected	B
graph	B
definition	O
ancestors	O
descendants	O
a	O
path	B
a	O
b	O
from	O
node	O
a	O
to	O
node	O
b	O
is	O
a	O
sequence	O
of	O
vertices	O
a	O
an	O
an	O
b	O
with	O
an	O
edge	O
in	O
the	O
graph	B
thereby	O
connecting	O
a	O
to	O
b	O
for	O
a	O
directed	B
graph	B
this	O
means	O
that	O
a	O
path	B
is	O
a	O
sequence	O
of	O
nodes	O
which	O
when	O
we	O
follow	O
the	O
direction	O
of	O
the	O
arrows	O
leads	O
us	O
from	O
a	O
to	O
b	O
the	O
vertices	O
a	O
such	O
that	O
a	O
b	O
and	O
b	O
a	O
are	O
the	O
descendants	O
of	O
and	O
b	O
a	O
are	O
the	O
ancestors	O
of	O
b	O
the	O
vertices	O
b	O
such	O
that	O
a	O
b	O
definition	O
acyclic	O
graph	B
a	O
dag	O
is	O
a	O
graph	B
g	O
with	O
directed	B
edges	O
on	O
each	O
link	O
between	O
the	O
vertices	O
such	O
that	O
by	O
following	O
a	O
path	B
of	O
vertices	O
from	O
one	O
node	O
to	O
another	O
along	O
the	O
direction	O
of	O
each	O
edge	O
no	O
path	B
will	O
revisit	O
a	O
vertex	O
in	O
a	O
dag	O
the	O
ancestors	O
of	O
b	O
are	O
those	O
nodes	O
who	O
have	O
a	O
directed	B
path	B
ending	O
at	O
b	O
conversely	O
the	O
descendants	O
of	O
a	O
are	O
those	O
nodes	O
who	O
have	O
a	O
directed	B
path	B
starting	O
at	O
a	O
definition	O
in	O
a	O
dag	O
the	O
children	B
of	O
the	O
parents	B
of	O
are	O
pa	O
the	O
family	B
of	O
a	O
node	O
is	O
itself	O
and	O
its	O
are	O
ch	O
parents	B
the	O
markov	B
blanket	I
of	O
a	O
node	O
is	O
itself	O
its	O
parents	B
children	B
and	O
the	O
parents	B
of	O
its	O
children	B
in	O
this	O
case	O
the	O
markov	B
blanket	I
of	O
is	O
c	O
a	O
f	O
b	O
g	O
d	O
e	O
c	O
b	O
g	O
e	O
a	O
f	O
d	O
graphs	O
figure	O
multiply-connected	B
graph	B
singly-connected	B
graph	B
definition	O
graph	B
a	O
d	O
b	O
c	O
e	O
an	O
undirected	B
graph	B
g	O
consists	O
of	O
undirected	B
edges	O
between	O
nodes	O
definition	O
for	O
an	O
undirected	B
graph	B
g	O
the	O
neighbours	O
of	O
x	O
ne	O
are	O
those	O
nodes	O
directly	O
connected	B
to	O
x	O
definition	O
graph	B
an	O
undirected	B
graph	B
is	O
connected	B
if	O
there	O
is	O
a	O
path	B
between	O
every	O
set	O
of	O
vertices	O
there	O
are	O
no	O
isolated	O
islands	O
for	O
a	O
graph	B
which	O
is	O
not	O
connected	B
the	O
connected	B
components	I
are	O
those	O
subgraphs	O
which	O
are	O
connected	B
definition	O
a	O
d	O
b	O
c	O
e	O
given	O
an	O
undirected	B
graph	B
a	O
clique	B
is	O
a	O
maximally	O
connected	B
subset	O
of	O
vertices	O
all	O
the	O
members	O
of	O
the	O
clique	B
are	O
connected	B
to	O
each	O
other	O
furthermore	O
there	O
is	O
no	O
larger	O
clique	B
that	O
can	O
be	O
made	O
from	O
a	O
clique	B
for	O
example	O
this	O
graph	B
has	O
two	O
cliques	O
b	O
c	O
d	O
and	O
c	O
e	O
whilst	O
a	O
b	O
c	O
are	O
fully	O
connected	B
this	O
is	O
a	O
non-maximal	O
clique	B
since	O
there	O
is	O
a	O
larger	O
fully	O
connected	B
set	O
a	O
b	O
c	O
d	O
that	O
contains	O
this	O
a	O
non-maximal	O
clique	B
is	O
sometimes	O
called	O
a	O
cliquo	B
definition	O
graph	B
a	O
graph	B
is	O
singly-connected	B
if	O
there	O
is	O
only	O
one	O
path	B
from	O
a	O
vertex	O
a	O
to	O
another	O
vertex	O
b	O
otherwise	O
the	O
graph	B
is	O
multiply-connected	B
this	O
definition	O
applies	O
regardless	O
of	O
whether	O
or	O
not	O
the	O
edges	O
in	O
the	O
graph	B
are	O
directed	B
an	O
alternative	O
name	O
for	O
a	O
singly-connected	B
graph	B
is	O
a	O
tree	B
a	O
multiply-connected	B
graph	B
is	O
also	O
called	O
loopy	B
draft	O
march	O
numerically	O
encoding	O
graphs	O
spanning	B
tree	B
definition	O
tree	B
a	O
spanning	B
tree	B
of	O
an	O
undirected	B
graph	B
g	O
is	O
a	O
singly-connected	B
subset	O
of	O
the	O
existing	O
edges	O
such	O
that	O
the	O
resulting	O
singlyconnected	O
graph	B
covers	O
all	O
vertices	O
of	O
g	O
on	O
the	O
right	O
is	O
a	O
graph	B
and	O
an	O
associated	O
spanning	B
tree	B
a	O
maximum	O
weight	B
spanning	B
tree	B
is	O
a	O
spanning	B
tree	B
such	O
that	O
the	O
sum	O
of	O
all	O
weights	O
on	O
the	O
edges	O
of	O
the	O
tree	B
is	O
larger	O
than	O
for	O
any	O
other	O
spanning	B
tree	B
of	O
g	O
finding	O
the	O
maximal	O
weight	B
spanning	B
tree	B
a	O
simple	O
algorithm	B
to	O
find	O
a	O
spanning	B
tree	B
with	O
maximal	O
weight	B
is	O
as	O
follows	O
start	O
by	O
picking	O
the	O
edge	O
with	O
the	O
largest	O
weight	B
and	O
add	O
this	O
to	O
the	O
edge	O
set	O
then	O
pick	O
the	O
next	O
candidate	O
edge	O
which	O
has	O
the	O
largest	O
weight	B
and	O
add	O
this	O
to	O
the	O
edge	O
set	O
if	O
this	O
results	O
in	O
an	O
edge	O
set	O
with	O
cycles	O
then	O
reject	O
the	O
candidate	O
edge	O
and	O
find	O
the	O
next	O
largest	O
edge	O
weight	B
note	O
that	O
there	O
may	O
be	O
more	O
than	O
one	O
maximal	O
weight	B
spanning	B
tree	B
numerically	O
encoding	O
graphs	O
to	O
express	O
the	O
structure	B
of	O
gms	O
we	O
need	O
to	O
numerically	O
encode	O
the	O
links	O
on	O
the	O
graphs	O
for	O
a	O
graph	B
of	O
n	O
vertices	O
we	O
can	O
describe	O
the	O
graph	B
structure	B
in	O
various	O
equivalent	B
ways	O
edge	B
list	I
as	O
the	O
name	O
suggests	O
an	O
edge	B
list	I
simply	O
lists	O
which	O
vertex-vertex	O
pairs	O
are	O
in	O
the	O
graph	B
for	O
an	O
edge	B
list	I
is	O
l	O
where	O
an	O
undirected	B
edge	O
is	O
represented	O
by	O
a	O
bidirectional	O
edge	O
adjacency	B
matrix	B
an	O
alternative	O
is	O
to	O
use	O
an	O
adjacency	B
matrix	B
a	O
where	O
aij	O
if	O
there	O
is	O
an	O
edge	O
from	O
variable	O
i	O
to	O
j	O
in	O
the	O
graph	B
and	O
otherwise	O
some	O
authors	O
include	O
self-connections	O
in	O
this	O
definition	O
an	O
undirected	B
graph	B
has	O
a	O
symmetric	O
adjacency	B
matrix	B
provided	O
that	O
the	O
vertices	O
are	O
labelled	B
in	O
ancestral	B
order	I
always	O
come	O
before	O
children	B
a	O
directed	B
graph	B
can	O
be	O
represented	O
as	O
a	O
triangular	O
adjacency	B
matrix	B
t	O
draft	O
march	O
numerically	O
encoding	O
graphs	O
figure	O
an	O
undirected	B
graph	B
can	O
be	O
represented	O
as	O
a	O
symmetric	O
adjacency	B
matrix	B
a	O
directed	B
graph	B
with	O
vertices	O
labelled	B
in	O
ancestral	B
order	I
corresponds	O
to	O
a	O
triangular	O
adjacency	B
matrix	B
adjacency	B
matrix	B
powers	O
for	O
an	O
n	O
n	O
adjacency	B
matrix	B
a	O
powers	O
of	O
the	O
adjacency	B
if	O
we	O
include	O
s	O
on	O
the	O
diagonal	O
of	O
a	O
then	O
in	O
the	O
graph	B
if	O
a	O
corresponds	O
to	O
a	O
dag	O
the	O
non-zero	O
entries	O
of	O
the	O
jth	O
row	O
correspond	O
to	O
the	O
ij	O
is	O
non-zero	O
when	O
there	O
is	O
a	O
path	B
connecting	O
j	O
to	O
i	O
are	O
from	O
node	O
i	O
to	O
node	O
j	O
in	O
k	O
edge	O
hops	O
ij	O
specify	O
how	O
many	O
paths	O
there	O
descendants	O
of	O
node	O
j	O
clique	B
matrix	B
for	O
an	O
undirected	B
graph	B
with	O
n	O
vertices	O
and	O
maximal	O
cliques	O
ck	O
a	O
clique	B
matrix	B
is	O
an	O
n	O
k	O
matrix	B
in	O
which	O
each	O
column	O
ck	O
has	O
zeros	O
expect	O
for	O
ones	O
on	O
entries	O
describing	O
the	O
clique	B
a	O
cliquo	B
matrix	B
relaxes	O
the	O
constraint	O
that	O
cliques	O
are	O
required	O
to	O
be	O
for	O
example	O
c	O
cinc	O
is	O
a	O
clique	B
matrix	B
for	O
a	O
cliquo	B
matrix	B
containing	O
only	O
two-dimensional	O
maximal	O
cliques	O
is	O
called	O
an	O
incidence	B
matrix	B
for	O
example	O
is	O
an	O
incidence	B
matrix	B
for	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
cincct	O
inc	O
is	O
equal	O
to	O
the	O
adjacency	B
matrix	B
except	O
that	O
the	O
diagonals	O
now	O
contain	O
the	O
degree	B
of	O
each	O
vertex	O
number	O
of	O
edges	O
it	O
touches	O
similarly	O
for	O
any	O
cliquo	B
matrix	B
the	O
diagonal	O
entry	O
of	O
expresses	O
the	O
number	O
of	O
cliquos	O
that	O
vertex	O
i	O
occurs	O
in	O
off	O
diagonal	O
elements	O
contain	O
the	O
number	O
of	O
cliquos	O
that	O
vertices	O
i	O
and	O
j	O
jointly	O
inhabit	O
remark	O
confusions	O
graphs	O
are	O
widely	O
used	O
but	O
differ	O
markedly	O
in	O
what	O
they	O
represent	O
two	O
potential	B
pitfalls	O
are	O
described	O
below	O
state	O
transition	O
diagrams	O
such	O
graphical	O
representations	O
are	O
common	O
in	O
markov	O
chains	O
and	O
finite	O
state	O
automata	O
a	O
set	O
of	O
states	O
is	O
written	O
as	O
set	O
of	O
nodesvertices	O
of	O
a	O
graph	B
and	O
a	O
directed	B
edge	O
between	O
node	O
i	O
and	O
node	O
j	O
an	O
associated	O
weight	B
pij	O
represents	O
that	O
a	O
transition	O
from	O
state	O
i	O
to	O
state	O
j	O
can	O
occur	O
with	O
probability	O
pij	O
from	O
the	O
graphical	O
models	O
perspective	O
we	O
would	O
simply	O
write	O
down	O
a	O
directed	B
graph	B
xt	O
xt	O
to	O
represent	O
this	O
markov	B
chain	B
the	O
state-transition	O
diagram	O
simply	O
provides	O
a	O
graphical	O
description	O
of	O
the	O
conditional	B
probability	I
table	O
pxt	O
term	O
cliquo	B
for	O
a	O
non-maximal	O
clique	B
is	O
attributed	O
to	O
julian	O
besag	O
draft	O
march	O
exercises	O
neural	O
networks	O
neural	O
networks	O
also	O
have	O
vertices	O
and	O
edges	O
in	O
general	O
however	O
neural	O
networks	O
are	O
graphical	O
representations	O
of	O
functions	O
whereas	O
as	O
graphical	O
models	O
are	O
representations	O
of	O
distributions	O
richer	O
formalism	O
neural	O
networks	O
any	O
other	O
parametric	O
description	O
may	O
be	O
used	O
to	O
represent	O
the	O
conditional	B
probability	I
tables	O
as	O
in	O
sigmoid	B
belief	O
code	O
utility	B
routines	O
ancestors	O
m	O
find	O
the	O
ancestors	O
of	O
a	O
node	O
in	O
a	O
dag	O
edges	O
m	O
edge	B
list	I
from	O
an	O
adjacency	B
matrix	B
ancestralorder	O
m	O
ancestral	B
order	I
from	O
a	O
dag	O
connectedcomponents	O
m	O
connected	B
components	I
parents	B
m	O
parents	B
of	O
a	O
node	O
given	O
an	O
adjacency	B
matrix	B
children	B
m	O
children	B
of	O
a	O
node	O
given	O
an	O
adjacency	B
matrix	B
neigh	O
m	O
neighbours	O
of	O
a	O
node	O
given	O
an	O
adjacency	B
matrix	B
a	O
connected	B
graph	B
is	O
a	O
tree	B
if	O
the	O
number	O
of	O
edges	O
plus	O
is	O
equal	O
to	O
the	O
number	O
of	O
nodes	O
however	O
for	O
a	O
possibly	O
disconnected	B
graph	B
this	O
is	O
not	O
the	O
case	O
the	O
code	O
below	O
deals	O
with	O
the	O
possibly	O
disconnected	B
case	O
the	O
routine	O
is	O
based	O
on	O
the	O
observation	O
that	O
any	O
singly-connected	B
graph	B
must	O
always	O
possess	O
a	O
simplical	B
node	O
leaf	O
node	O
which	O
can	O
be	O
eliminated	O
to	O
reveal	O
a	O
smaller	O
singly-connected	B
graph	B
istree	O
m	O
if	O
graph	B
is	O
singly	O
connected	B
return	O
and	O
elimination	O
sequence	O
spantree	O
m	O
return	O
a	O
spanning	B
tree	B
from	O
an	O
ordered	O
edge	B
list	I
singleparenttree	O
m	O
find	O
a	O
directed	B
tree	B
with	O
at	O
most	O
one	O
parent	O
from	O
an	O
undirected	B
tree	B
additional	O
routines	O
for	O
basic	O
manipulations	O
in	O
graphs	O
are	O
given	O
at	O
the	O
end	O
of	O
exercises	O
j	O
in	O
one	O
timestep	O
and	O
otherwise	O
show	O
that	O
the	O
exercise	O
consider	O
an	O
adjacency	B
matrix	B
a	O
with	O
elements	O
if	O
one	O
can	O
reach	O
state	O
i	O
from	O
state	O
ij	O
represents	O
the	O
number	O
of	O
paths	O
that	O
lead	O
from	O
state	O
j	O
to	O
i	O
in	O
k	O
timesteps	O
hence	O
derive	O
an	O
algorithm	B
that	O
will	O
find	O
the	O
minimum	O
number	O
of	O
steps	O
to	O
get	O
from	O
state	O
j	O
to	O
state	O
i	O
exercise	O
for	O
an	O
n	O
n	O
symmetric	O
adjacency	B
matrix	B
a	O
describe	O
an	O
algorithm	B
to	O
find	O
the	O
connected	B
components	I
you	O
may	O
wish	O
to	O
examine	O
connectedcomponents	O
m	O
exercise	O
show	O
that	O
for	O
a	O
connected	B
graph	B
that	O
is	O
singly-connected	B
the	O
number	O
of	O
edges	O
e	O
must	O
be	O
equal	O
to	O
the	O
number	O
of	O
vertices	O
minus	O
e	O
v	O
give	O
an	O
example	O
graph	B
with	O
e	O
v	O
that	O
is	O
not	O
singly-connected	B
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
belief	B
networks	I
probabilistic	B
inference	B
in	O
structured	B
distributions	O
consider	O
an	O
environment	O
composed	O
of	O
n	O
variables	O
with	O
a	O
corresponding	O
distribution	B
xn	O
writing	O
e	O
as	O
the	O
set	O
of	O
evidential	O
variables	O
and	O
using	O
evidence	O
xe	O
e	O
e	O
to	O
denote	O
all	O
available	O
evidence	O
then	O
inference	B
and	O
reasoning	O
can	O
be	O
carried	O
out	O
automatically	O
by	O
the	O
brute	O
force	O
xei	O
pxe	O
xe	O
e	O
e	O
xi	O
xi	O
xei	O
xe	O
pxe	O
xe	O
e	O
e	O
xe	O
pxi	O
xievidence	O
if	O
all	O
variables	O
are	O
binary	O
two	O
states	O
these	O
summations	O
require	O
operations	O
such	O
exponential	B
computation	O
is	O
impractical	O
and	O
techniques	O
that	O
reduce	O
this	O
burden	O
by	O
exploiting	O
any	O
structure	B
in	O
the	O
joint	B
probability	O
table	O
are	O
the	O
topic	O
of	O
our	O
discussions	O
on	O
efficient	O
inference	B
naively	O
specifying	O
all	O
the	O
entries	O
of	O
a	O
table	O
xn	O
over	O
binary	O
variables	O
xi	O
takes	O
space	O
we	O
will	O
need	O
to	O
deal	O
with	O
large	O
numbers	O
of	O
variables	O
in	O
machine	O
learning	B
and	O
related	O
application	O
areas	O
with	O
distributions	O
on	O
potentially	O
hundreds	O
if	O
not	O
millions	O
of	O
variables	O
the	O
only	O
way	O
to	O
deal	O
with	O
such	O
large	O
distributions	O
is	O
to	O
constrain	O
the	O
nature	O
of	O
the	O
variable	O
interactions	O
in	O
some	O
manner	O
both	O
to	O
render	O
specification	O
and	O
ultimately	O
inference	B
in	O
such	O
systems	O
tractable	O
the	O
key	O
idea	O
is	O
to	O
specify	O
which	O
variables	O
are	O
independent	O
of	O
others	O
leading	O
to	O
a	O
structured	B
factorisation	O
of	O
the	O
joint	B
probability	O
distribution	B
belief	B
networks	I
are	O
a	O
convenient	O
framework	O
for	O
representing	O
such	O
factorisations	O
into	O
local	B
conditional	B
distributions	O
we	O
will	O
discuss	O
belief	B
networks	I
more	O
formally	O
in	O
first	O
discussing	O
their	O
natural	B
graphical	O
representations	O
of	O
distributions	O
definition	O
network	O
a	O
belief	B
network	I
is	O
a	O
distribution	B
of	O
the	O
form	O
xd	O
pxipa	O
where	O
pa	O
represent	O
the	O
parental	O
variables	O
of	O
variable	O
xi	O
written	O
as	O
a	O
directed	B
graph	B
with	O
an	O
arrow	O
pointing	O
from	O
a	O
parent	O
variable	O
to	O
child	O
variable	O
a	O
belief	B
network	I
is	O
a	O
directed	B
acyclic	I
graph	B
with	O
the	O
ith	O
vertex	O
in	O
the	O
graph	B
corresponding	O
to	O
the	O
factor	B
pxipa	O
extension	O
to	O
continuous	B
variables	O
is	O
straightforward	O
replacing	O
summation	O
with	O
integration	O
over	O
pdfs	O
we	O
defer	O
treatment	O
of	O
this	O
to	O
later	O
chapters	O
since	O
our	O
aim	O
is	O
to	O
here	O
outline	O
more	O
the	O
intuitions	O
without	O
needing	O
to	O
deal	O
with	O
integration	O
of	O
high	O
dimensional	O
distributions	O
graphically	O
representing	O
distributions	O
graphically	O
representing	O
distributions	O
belief	B
networks	I
called	O
bayes	O
networks	O
or	O
bayesian	B
belief	B
networks	I
are	O
a	O
way	O
to	O
depict	O
the	O
independence	B
assumptions	O
made	O
in	O
a	O
distribution	B
their	O
application	O
domain	B
is	O
widespread	O
ranging	O
from	O
and	O
expert	O
reasoning	O
under	O
uncertainty	B
to	O
machine	O
learning	B
before	O
we	O
more	O
formally	O
define	O
a	O
bn	O
an	O
example	O
will	O
help	O
motivate	O
the	O
constructing	O
a	O
simple	O
belief	B
network	I
wet	O
grass	O
one	O
morning	O
tracey	O
leaves	O
her	O
house	O
and	O
realises	O
that	O
her	O
grass	O
is	O
wet	O
is	O
it	O
due	O
to	O
overnight	O
rain	O
or	O
did	O
she	O
forget	O
to	O
turn	O
off	O
the	O
sprinkler	O
last	O
night	O
next	O
she	O
notices	O
that	O
the	O
grass	O
of	O
her	O
neighbour	B
jack	O
is	O
also	O
wet	O
this	O
explains	O
away	O
to	O
some	O
extent	O
the	O
possibility	O
that	O
her	O
sprinkler	O
was	O
left	O
on	O
and	O
she	O
concludes	O
therefore	O
that	O
it	O
has	O
probably	O
been	O
raining	O
making	O
a	O
model	B
we	O
can	O
model	B
the	O
above	O
situation	O
using	O
probability	O
by	O
following	O
a	O
general	O
modelling	B
approach	B
first	O
we	O
define	O
the	O
variables	O
we	O
wish	O
to	O
include	O
in	O
our	O
model	B
in	O
the	O
above	O
situation	O
the	O
natural	B
variables	O
are	O
r	O
means	O
that	O
it	O
has	O
been	O
raining	O
and	O
otherwise	O
s	O
means	O
that	O
tracey	O
has	O
forgotten	O
to	O
turn	O
off	O
the	O
sprinkler	O
and	O
otherwise	O
j	O
means	O
that	O
jack	O
s	O
grass	O
is	O
wet	O
and	O
otherwise	O
t	O
means	O
that	O
tracey	O
s	O
grass	O
is	O
wet	O
and	O
otherwise	O
a	O
model	B
of	O
tracey	O
s	O
world	O
then	O
corresponds	O
to	O
a	O
probability	O
distribution	B
on	O
the	O
joint	B
set	O
of	O
the	O
variables	O
of	O
interest	O
pt	O
j	O
r	O
s	O
order	O
of	O
the	O
variables	O
is	O
irrelevant	O
since	O
each	O
of	O
the	O
variables	O
in	O
this	O
example	O
can	O
take	O
one	O
of	O
two	O
states	O
it	O
would	O
appear	O
that	O
we	O
naively	O
have	O
to	O
specify	O
the	O
values	O
for	O
each	O
of	O
the	O
states	O
e	O
g	O
pt	O
j	O
r	O
s	O
etc	O
however	O
since	O
there	O
are	O
normalisation	B
conditions	O
for	O
probabilities	O
we	O
do	O
not	O
need	O
to	O
specify	O
all	O
the	O
state	O
probabilities	O
to	O
see	O
how	O
many	O
states	O
need	O
to	O
be	O
specified	O
consider	O
the	O
following	O
decomposition	B
without	O
loss	O
of	O
generality	O
and	O
repeatedly	O
using	O
bayes	O
rule	O
we	O
may	O
write	O
pt	O
j	O
r	O
s	O
ptj	O
r	O
spj	O
r	O
s	O
ptj	O
r	O
spjr	O
spr	O
s	O
ptj	O
r	O
spjr	O
sprsps	O
that	O
is	O
we	O
may	O
write	O
the	O
joint	B
distribution	B
as	O
a	O
product	O
of	O
conditional	B
distributions	O
the	O
first	O
term	O
ptj	O
r	O
s	O
requires	O
us	O
to	O
specify	O
values	O
we	O
need	O
pt	O
r	O
s	O
for	O
the	O
joint	B
states	O
of	O
j	O
r	O
s	O
the	O
other	O
value	B
pt	O
r	O
s	O
is	O
given	O
by	O
normalisation	B
pt	O
r	O
s	O
pt	O
r	O
s	O
similarly	O
we	O
need	O
values	O
for	O
the	O
other	O
factors	O
making	O
a	O
total	O
of	O
values	O
in	O
all	O
in	O
general	O
for	O
a	O
distribution	B
on	O
n	O
binary	O
variables	O
we	O
need	O
to	O
specify	O
values	O
in	O
the	O
range	O
the	O
important	O
point	O
here	O
is	O
that	O
the	O
number	O
of	O
values	O
that	O
need	O
to	O
be	O
specified	O
in	O
general	O
scales	O
exponentially	O
with	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	B
this	O
is	O
impractical	O
in	O
general	O
and	O
motivates	O
simplifications	O
conditional	B
independence	B
the	O
modeler	O
often	O
knows	O
constraints	O
on	O
the	O
system	O
for	O
example	O
in	O
the	O
scenario	O
above	O
we	O
may	O
assume	O
that	O
tracey	O
s	O
grass	O
is	O
wet	O
depends	O
only	O
directly	O
on	O
whether	O
or	O
not	O
is	O
has	O
been	O
raining	O
and	O
whether	O
or	O
not	O
her	O
sprinkler	O
was	O
on	O
that	O
is	O
we	O
make	O
a	O
conditional	B
independence	B
assumption	O
ptj	O
r	O
s	O
ptr	O
s	O
scenario	O
is	O
adapted	O
from	O
draft	O
march	O
graphically	O
representing	O
distributions	O
r	O
s	O
b	O
e	O
j	O
t	O
r	O
a	O
figure	O
belief	B
network	I
structure	B
for	O
the	O
wet	O
grass	O
example	O
each	O
node	O
in	O
the	O
graph	B
represents	O
a	O
variable	O
in	O
the	O
joint	B
distribution	B
and	O
the	O
variables	O
which	O
feed	O
in	O
parents	B
to	O
another	O
variable	O
represent	O
which	O
variables	O
are	O
bn	O
to	O
the	O
right	O
of	O
the	O
conditioning	B
bar	O
for	O
the	O
burglar	O
model	B
similarly	O
since	O
whether	O
or	O
not	O
jack	O
s	O
grass	O
is	O
wet	O
is	O
influenced	O
only	O
directly	O
by	O
whether	O
or	O
not	O
it	O
has	O
been	O
raining	O
we	O
write	O
pjr	O
s	O
pjr	O
and	O
since	O
the	O
rain	O
is	O
not	O
directly	O
influenced	O
by	O
the	O
sprinkler	O
prs	O
pr	O
which	O
means	O
that	O
our	O
model	B
now	O
becomes	O
pt	O
j	O
r	O
s	O
ptr	O
spjrprps	O
we	O
can	O
represent	O
these	O
conditional	B
independencies	O
graphically	O
as	O
in	O
this	O
reduces	O
the	O
number	O
of	O
values	O
that	O
we	O
need	O
to	O
specify	O
to	O
a	O
saving	O
over	O
the	O
previous	O
values	O
in	O
the	O
case	O
where	O
no	O
conditional	B
independencies	O
had	O
been	O
assumed	O
to	O
complete	O
the	O
model	B
we	O
need	O
to	O
numerically	O
specify	O
the	O
values	O
of	O
each	O
conditional	B
probability	I
table	O
let	O
the	O
prior	B
probabilities	O
for	O
r	O
and	O
s	O
be	O
pr	O
and	O
ps	O
we	O
set	O
the	O
remaining	O
probabilities	O
to	O
pj	O
pj	O
jack	O
s	O
grass	O
is	O
wet	O
due	O
to	O
unknown	O
effects	O
other	O
than	O
rain	O
pt	O
s	O
pt	O
s	O
s	O
a	O
small	O
chance	O
that	O
even	O
though	O
the	O
sprinkler	O
was	O
left	O
on	O
it	O
didn	O
t	O
wet	O
the	O
grass	O
noticeably	O
pt	O
s	O
inference	B
now	O
that	O
we	O
ve	O
made	O
a	O
model	B
of	O
an	O
environment	O
we	O
can	O
perform	O
inference	B
let	O
s	O
calculate	O
the	O
probability	O
that	O
the	O
sprinkler	O
was	O
on	O
overnight	O
given	O
that	O
tracey	O
s	O
grass	O
is	O
wet	O
ps	O
to	O
do	O
this	O
we	O
use	O
bayes	O
rule	O
ps	O
ps	O
t	O
jr	O
pt	O
j	O
r	O
s	O
jrs	O
pt	O
j	O
r	O
s	O
pt	O
jr	O
pjrpt	O
s	O
jrs	O
pjrpt	O
sprps	O
r	O
pt	O
s	O
rs	O
pt	O
sprps	O
so	O
the	O
belief	O
that	O
the	O
sprinkler	O
is	O
on	O
increases	O
above	O
the	O
prior	B
probability	O
due	O
to	O
the	O
fact	O
that	O
the	O
grass	O
is	O
wet	O
let	O
us	O
now	O
calculate	O
the	O
probability	O
that	O
tracey	O
s	O
sprinkler	O
was	O
on	O
overnight	O
given	O
that	O
her	O
grass	O
is	O
wet	O
and	O
that	O
jack	O
s	O
grass	O
is	O
also	O
wet	O
ps	O
j	O
we	O
use	O
bayes	O
rule	O
again	O
draft	O
march	O
graphically	O
representing	O
distributions	O
ps	O
j	O
ps	O
t	O
j	O
pt	O
j	O
r	O
pt	O
j	O
r	O
s	O
rs	O
pt	O
j	O
r	O
s	O
r	O
pj	O
s	O
rs	O
pj	O
sprps	O
the	O
probability	O
that	O
the	O
sprinkler	O
is	O
on	O
given	O
the	O
extra	O
evidence	O
that	O
jack	O
s	O
grass	O
is	O
wet	O
is	O
lower	O
than	O
the	O
probability	O
that	O
the	O
grass	O
is	O
wet	O
given	O
only	O
that	O
tracey	O
s	O
grass	O
is	O
wet	O
that	O
is	O
that	O
the	O
grass	O
is	O
wet	O
due	O
to	O
the	O
sprinkler	O
is	O
explained	O
away	O
by	O
the	O
fact	O
that	O
jack	O
s	O
grass	O
is	O
also	O
wet	O
this	O
increases	O
the	O
chance	O
that	O
the	O
rain	O
has	O
played	O
a	O
role	O
in	O
making	O
tracey	O
s	O
grass	O
wet	O
naturally	O
we	O
don	O
t	O
wish	O
to	O
carry	O
out	O
such	O
inference	B
calculations	O
by	O
hand	O
all	O
the	O
time	O
general	O
purpose	O
algorithms	O
exist	O
for	O
this	O
such	O
as	O
the	O
junction	B
tree	B
algorithm	B
and	O
we	O
shall	O
introduce	O
these	O
in	O
later	O
chapters	O
example	O
it	O
the	O
burglar	O
here	O
s	O
another	O
example	O
using	O
binary	O
variables	O
adapted	O
from	O
sally	O
comes	O
home	O
to	O
find	O
that	O
the	O
burglar	O
alarm	O
is	O
sounding	O
has	O
she	O
been	O
burgled	O
or	O
was	O
the	O
alarm	O
triggered	O
by	O
an	O
earthquake	O
she	O
turns	O
the	O
car	O
radio	O
on	O
for	O
news	O
of	O
earthquakes	O
and	O
finds	O
that	O
the	O
radio	O
broadcasts	O
an	O
earthquake	O
alert	O
using	O
bayes	O
rule	O
we	O
can	O
write	O
without	O
loss	O
of	O
generality	O
pb	O
e	O
a	O
r	O
pab	O
e	O
rpb	O
e	O
r	O
we	O
can	O
repeat	O
this	O
for	O
pb	O
e	O
r	O
and	O
continue	O
pb	O
e	O
a	O
r	O
pab	O
e	O
rprb	O
epebpb	O
however	O
the	O
alarm	O
is	O
surely	O
not	O
directly	O
influenced	O
by	O
any	O
report	O
on	O
the	O
radio	O
that	O
is	O
pab	O
e	O
r	O
pab	O
e	O
similarly	O
we	O
can	O
make	O
other	O
conditional	B
independence	B
assumptions	O
such	O
that	O
pb	O
e	O
a	O
r	O
pab	O
eprepepb	O
specifying	O
conditional	B
probability	I
tables	O
alarm	O
burglar	O
earthquake	O
radio	O
earthquake	O
the	O
remaining	O
tables	O
are	O
pb	O
and	O
pe	O
the	O
tables	O
and	O
graphical	O
structure	B
fully	O
specify	O
the	O
distribution	B
now	O
consider	O
what	O
happens	O
as	O
we	O
observe	O
evidence	O
initial	O
evidence	O
the	O
alarm	O
is	O
sounding	O
pb	O
er	O
pb	O
e	O
a	O
r	O
ber	O
pb	O
e	O
a	O
r	O
er	O
pa	O
epb	O
ber	O
pa	O
epbpepre	O
draft	O
march	O
graphically	O
representing	O
distributions	O
additional	O
evidence	O
the	O
radio	O
broadcasts	O
an	O
earthquake	O
warning	O
a	O
similar	O
calculation	O
gives	O
pb	O
r	O
thus	O
initially	O
because	O
the	O
alarm	O
sounds	O
sally	O
thinks	O
that	O
she	O
s	O
been	O
burgled	O
however	O
this	O
probability	O
drops	O
dramatically	O
when	O
she	O
hears	O
that	O
there	O
has	O
been	O
an	O
earthquake	O
that	O
is	O
the	O
earthquake	O
explains	O
away	O
to	O
an	O
extent	O
the	O
fact	O
that	O
the	O
alarm	O
is	O
ringing	O
see	O
demoburglar	O
m	O
uncertain	B
evidence	O
in	O
soft	B
or	O
uncertain	B
evidence	O
the	O
variable	O
is	O
in	O
more	O
than	O
one	O
state	O
with	O
the	O
strength	O
of	O
our	O
belief	O
about	O
each	O
state	O
being	O
given	O
by	O
probabilities	O
for	O
example	O
if	O
x	O
has	O
the	O
states	O
domx	O
blue	O
green	O
the	O
vector	O
represents	O
the	O
probabilities	O
of	O
the	O
respective	O
states	O
in	O
contrast	O
for	O
hard	B
evidence	O
we	O
are	O
certain	O
that	O
a	O
variable	O
is	O
in	O
a	O
particular	O
state	O
in	O
this	O
case	O
all	O
the	O
probability	O
mass	O
is	O
in	O
one	O
of	O
the	O
vector	O
components	O
for	O
example	O
performing	O
inference	B
with	O
soft-evidence	O
is	O
straightforward	O
and	O
can	O
be	O
achieved	O
using	O
bayes	O
rule	O
writing	O
the	O
soft	B
evidence	O
as	O
y	O
we	O
have	O
pxypy	O
y	O
px	O
y	O
y	O
where	O
py	O
i	O
y	O
represents	O
the	O
probability	O
that	O
y	O
is	O
in	O
state	O
i	O
under	O
the	O
soft-evidence	O
this	O
is	O
a	O
generalisation	B
of	O
hard-evidence	O
in	O
which	O
the	O
vector	O
py	O
y	O
has	O
all	O
zero	O
component	O
values	O
except	O
for	O
all	O
but	O
a	O
single	O
component	O
note	O
that	O
the	O
soft	B
evidence	O
py	O
i	O
y	O
does	O
not	O
correspond	O
to	O
the	O
marginal	B
py	O
i	O
in	O
the	O
original	O
joint	B
distribution	B
px	O
y	O
a	O
procedure	O
to	O
form	O
a	O
joint	B
distribution	B
known	O
as	O
jeffrey	O
s	O
rule	O
is	O
to	O
begin	O
with	O
an	O
original	O
distribution	B
y	O
from	O
which	O
we	O
can	O
define	O
y	O
x	O
y	O
using	O
the	O
soft	B
evidence	O
py	O
y	O
we	O
then	O
define	O
a	O
new	O
joint	B
distribution	B
y	O
y	O
y	O
in	O
the	O
bn	O
we	O
use	O
a	O
dashed	O
circle	O
to	O
represent	O
that	O
a	O
variable	O
is	O
in	O
a	O
soft-evidence	O
state	O
example	O
revisiting	O
the	O
earthquake	O
scenario	O
imagine	O
that	O
we	O
think	O
we	O
hear	O
the	O
burglar	O
alarm	O
sounding	O
but	O
are	O
not	O
sure	O
specifically	O
we	O
are	O
only	O
sure	O
we	O
heard	O
the	O
alarm	O
for	O
this	O
binary	O
variable	O
case	O
we	O
represent	O
this	O
soft-evidence	O
for	O
the	O
states	O
as	O
a	O
what	O
is	O
the	O
probability	O
of	O
a	O
burglary	O
under	O
this	O
soft-evidence	O
pb	O
a	O
pb	O
pb	O
pb	O
a	O
a	O
the	O
probabilities	O
pb	O
and	O
pb	O
are	O
calculated	O
using	O
bayes	O
rule	O
as	O
before	O
to	O
give	O
pb	O
a	O
uncertain	B
evidence	O
versus	O
unreliable	O
modelling	B
an	O
entertaining	O
example	O
of	O
uncertain	B
evidence	O
is	O
given	O
by	O
draft	O
march	O
graphically	O
representing	O
distributions	O
b	O
a	O
g	O
w	O
b	O
a	O
h	O
w	O
b	O
a	O
g	O
w	O
j	O
n	O
b	O
a	O
g	O
w	O
figure	O
mr	O
holmes	O
burglary	O
worries	O
as	O
given	O
in	O
mrs	O
modified	O
problem	B
mrs	O
gibbon	O
is	O
not	O
drinking	O
but	O
somewhat	O
deaf	O
we	O
represent	O
such	O
uncertain	B
by	O
a	O
circle	O
holmes	O
gets	O
additional	O
information	O
from	O
his	O
neighbour	B
mrs	O
and	O
informant	O
dodgy	O
virtual	B
evidence	O
can	O
be	O
represented	O
by	O
a	O
dashed	O
line	O
mr	O
holmes	O
receives	O
a	O
telephone	O
call	O
from	O
his	O
neighbour	B
dr	O
watson	O
who	O
states	O
that	O
he	O
hears	O
the	O
sound	O
of	O
a	O
burglar	O
alarm	O
from	O
the	O
direction	O
of	O
mr	O
holmes	O
house	O
while	O
preparing	O
to	O
rush	O
home	O
mr	O
holmes	O
recalls	O
that	O
dr	O
watson	O
is	O
known	O
to	O
be	O
a	O
tasteless	O
practical	O
joker	O
and	O
he	O
decides	O
to	O
first	O
call	O
another	O
neighbour	B
mrs	O
gibbon	O
who	O
despite	O
occasional	O
drinking	O
problems	O
is	O
far	O
more	O
reliable	O
when	O
mr	O
holmes	O
calls	O
mrs	O
gibbon	O
he	O
soon	O
realises	O
that	O
she	O
is	O
somewhat	O
tipsy	O
instead	O
of	O
answering	O
his	O
question	O
directly	O
she	O
goes	O
on	O
and	O
on	O
about	O
her	O
latest	O
back	O
operation	O
and	O
about	O
how	O
terribly	O
noisy	O
and	O
crime-ridden	O
the	O
neighbourhood	O
has	O
become	O
when	O
he	O
finally	O
hangs	O
up	O
all	O
mr	O
holmes	O
can	O
glean	O
from	O
the	O
conversation	O
is	O
that	O
there	O
is	O
probably	O
an	O
chance	O
that	O
mrs	O
gibbon	O
did	O
hear	O
an	O
alarm	O
sound	O
from	O
her	O
window	O
a	O
bn	O
for	O
this	O
scenario	O
is	O
depicted	O
in	O
which	O
deals	O
with	O
four	O
binary	O
variables	O
house	O
is	O
has	O
sounded	O
hears	O
alarm	O
and	O
mrs	O
hears	O
pb	O
a	O
g	O
w	O
pabpbpwapga	O
holmes	O
is	O
interested	O
in	O
the	O
likelihood	B
that	O
his	O
house	O
has	O
been	O
burgled	O
naively	O
holmes	O
might	O
pb	O
trw	O
tr	O
g	O
tr	O
however	O
after	O
finding	O
out	O
about	O
mrs	O
gibbon	O
s	O
state	O
mr	O
holmes	O
no	O
longer	O
finds	O
the	O
above	O
model	B
reliable	O
he	O
wants	O
to	O
ignore	O
the	O
effect	O
that	O
mrs	O
gibbon	O
s	O
evidence	O
has	O
on	O
the	O
inference	B
and	O
replace	O
it	O
with	O
his	O
own	O
belief	O
as	O
to	O
what	O
mrs	O
gibbon	O
observed	O
mr	O
holmes	O
can	O
achieve	O
this	O
by	O
replacing	O
the	O
term	O
pg	O
tra	O
by	O
a	O
so-called	O
virtual	B
evidence	O
term	O
pg	O
tra	O
pha	O
where	O
pha	O
a	O
tr	O
a	O
fa	O
here	O
the	O
state	O
h	O
is	O
arbitrary	O
and	O
fixed	O
this	O
is	O
used	O
to	O
modify	O
the	O
joint	B
distribution	B
to	O
pb	O
a	O
h	O
w	O
pabpbpwapha	O
see	O
when	O
we	O
then	O
compute	O
pb	O
trw	O
tr	O
h	O
the	O
effect	O
of	O
mr	O
holmes	O
judgement	O
will	O
count	O
for	O
a	O
factor	B
of	O
times	O
more	O
in	O
favour	O
of	O
the	O
alarm	O
sounding	O
than	O
not	O
the	O
values	O
of	O
the	O
table	O
entries	O
are	O
irrelevant	O
up	O
to	O
normalisation	B
since	O
any	O
constants	O
can	O
be	O
absorbed	O
into	O
the	O
proportionality	O
constant	O
note	O
also	O
that	O
pha	O
is	O
not	O
a	O
distribution	B
in	O
a	O
and	O
hence	O
no	O
normalisation	B
is	O
required	O
this	O
form	O
of	O
evidence	O
is	O
also	O
called	O
likelihood	B
evidence	O
a	O
twist	O
on	O
pearl	O
s	O
scenario	O
is	O
that	O
mrs	O
gibbon	O
has	O
not	O
been	O
drinking	O
however	O
she	O
is	O
a	O
little	O
deaf	O
and	O
cannot	O
be	O
sure	O
herself	O
that	O
she	O
heard	O
the	O
alarm	O
she	O
is	O
sure	O
she	O
heard	O
it	O
in	O
this	O
case	O
holmes	O
would	O
might	O
be	O
tempted	O
to	O
include	O
an	O
additional	O
variable	O
as	O
a	O
parent	O
of	O
g	O
this	O
would	O
then	O
require	O
us	O
to	O
specify	O
the	O
joint	B
distribution	B
pgt	O
a	O
for	O
the	O
parental	O
joint	B
states	O
of	O
t	O
and	O
a	O
here	O
we	O
assume	O
that	O
we	O
do	O
not	O
have	O
access	O
to	O
such	O
information	O
notation	O
tr	O
is	O
equivalent	B
to	O
and	O
fa	O
to	O
from	O
draft	O
march	O
trust	O
the	O
model	B
however	O
the	O
observation	O
itself	O
is	O
now	O
uncertain	B
this	O
can	O
be	O
dealt	O
with	O
using	O
the	O
soft	B
evidence	O
technique	O
from	O
jeffrey	O
s	O
rule	O
one	O
uses	O
the	O
original	O
model	B
equation	B
to	O
compute	O
a	O
pgapw	O
trapab	O
tr	O
ba	O
pgapw	O
trapab	O
graphically	O
representing	O
distributions	O
pb	O
trw	O
tr	O
g	O
pb	O
tr	O
w	O
tr	O
g	O
pw	O
tr	O
g	O
and	O
then	O
uses	O
the	O
soft-evidence	O
g	O
tr	O
g	O
fa	O
pg	O
g	O
to	O
compute	O
pb	O
trw	O
tr	O
g	O
pb	O
trw	O
tr	O
g	O
trpg	O
tr	O
g	O
pb	O
trw	O
tr	O
g	O
fapg	O
fa	O
g	O
the	O
reader	O
may	O
show	O
that	O
an	O
alternative	O
way	O
to	O
represent	O
an	O
uncertain	B
observation	O
such	O
as	O
mrs	O
gibbon	O
being	O
non-tipsy	O
but	O
hard-of-hearing	O
above	O
is	O
to	O
use	O
a	O
virtual	B
evidence	O
child	O
from	O
g	O
uncertain	B
evidence	O
within	O
an	O
unreliable	O
model	B
to	O
highlight	O
uncertain	B
evidence	O
in	O
an	O
unreliable	O
model	B
we	O
introduce	O
two	O
additional	O
characters	O
mrs	O
nosy	O
lives	O
next	O
door	O
to	O
mr	O
holmes	O
and	O
is	O
completely	B
deaf	O
but	O
nevertheless	O
an	O
incorrigible	O
curtain-peeker	O
who	O
seems	O
to	O
notice	O
most	O
things	O
unfortunately	O
she	O
s	O
also	O
rather	O
prone	O
to	O
imagining	O
things	O
based	O
on	O
his	O
conversation	O
with	O
her	O
mr	O
holmes	O
counts	O
her	O
story	O
as	O
times	O
in	O
favour	O
of	O
there	O
not	O
being	O
a	O
burglary	O
to	O
there	O
being	O
a	O
burglary	O
and	O
therefore	O
uses	O
a	O
virtual	B
evidence	O
term	O
b	O
tr	O
b	O
fa	O
b	O
tr	O
b	O
fa	O
pnosyb	O
pjoeb	O
mr	O
holmes	O
also	O
telephones	O
dodgy	O
joe	O
his	O
contact	O
in	O
the	O
criminal	O
underworld	O
to	O
see	O
if	O
he	O
s	O
heard	O
of	O
any	O
planned	O
burglary	O
on	O
mr	O
holmes	O
home	O
he	O
summarises	O
this	O
information	O
using	O
a	O
virtual	B
evidence	O
term	O
when	O
all	O
this	O
information	O
is	O
combined	O
mrs	O
gibbon	O
is	O
not	O
tipsy	O
but	O
somewhat	O
hard	B
of	O
hearing	O
mrs	O
nosy	O
and	O
dodgy	O
joe	O
we	O
first	O
deal	O
with	O
the	O
unreliable	O
model	B
pb	O
a	O
w	O
tr	O
g	O
nosy	O
joe	O
pbpnosybpjoebpabpw	O
trapga	O
from	O
which	O
we	O
can	O
compute	O
pb	O
trw	O
tr	O
g	O
nosy	O
joe	O
finally	O
we	O
perform	O
inference	B
with	O
the	O
soft-evidence	O
pb	O
trw	O
tr	O
g	O
nosy	O
joe	O
pb	O
trw	O
tr	O
g	O
nosy	O
joepg	O
g	O
g	O
an	O
important	O
consideration	O
above	O
is	O
that	O
the	O
virtual	B
evidence	O
does	O
not	O
replace	O
the	O
prior	B
pb	O
with	O
another	O
prior	B
distribution	B
rather	O
the	O
virtual	B
evidence	O
terms	O
modify	O
the	O
prior	B
through	O
the	O
inclusion	O
of	O
extra	O
factors	O
the	O
usual	O
assumption	O
is	O
that	O
each	O
virtual	B
evidence	O
acts	O
independently	O
although	O
one	O
can	O
consider	O
dependent	O
scenarios	O
if	O
required	O
draft	O
march	O
belief	B
networks	I
figure	O
two	O
bns	O
for	O
a	O
variable	O
distribution	B
both	O
graphs	O
and	O
represent	O
the	O
same	O
distribution	B
strictly	O
speaking	O
they	O
represent	O
the	O
same	O
of	O
independence	B
assumptions	O
the	O
graphs	O
say	O
nothing	O
about	O
the	O
content	O
of	O
the	O
cpts	O
the	O
extension	O
of	O
this	O
cascade	B
to	O
many	O
variables	O
is	O
clear	O
and	O
always	O
results	O
in	O
a	O
directed	B
acyclic	I
graph	B
belief	B
networks	I
in	O
the	O
wet	O
grass	O
and	O
burglar	O
examples	O
we	O
had	O
a	O
choice	O
as	O
to	O
how	O
we	O
recursively	O
used	O
bayes	O
rule	O
in	O
a	O
general	O
variable	O
case	O
we	O
could	O
choose	O
the	O
factorisation	O
an	O
equally	O
valid	O
choice	O
is	O
in	O
general	O
two	O
different	O
graphs	O
may	O
represent	O
the	O
same	O
independence	B
assumptions	O
as	O
we	O
will	O
discuss	O
further	O
in	O
if	O
one	O
wishes	O
to	O
make	O
independence	B
assumptions	O
then	O
the	O
choice	O
of	O
factorisation	O
becomes	O
significant	O
the	O
observation	O
that	O
any	O
distribution	B
may	O
be	O
written	O
in	O
the	O
cascade	B
form	O
gives	O
an	O
algorithm	B
for	O
constructing	O
a	O
bn	O
on	O
variables	O
xn	O
write	O
down	O
the	O
n	O
variable	O
cascade	B
graph	B
assign	O
any	O
ordering	O
of	O
the	O
variables	O
to	O
the	O
nodes	O
you	O
may	O
then	O
delete	O
any	O
of	O
the	O
directed	B
connections	O
more	O
formally	O
this	O
corresponds	O
to	O
an	O
ordering	O
of	O
the	O
variables	O
which	O
without	O
loss	O
of	O
generality	O
we	O
may	O
write	O
as	O
xn	O
then	O
from	O
bayes	O
rule	O
we	O
have	O
xn	O
xn	O
xn	O
pxn	O
xn	O
n	O
the	O
representation	O
of	O
any	O
bn	O
is	O
therefore	O
a	O
directed	B
acyclic	I
graph	B
every	O
probability	O
distribution	B
can	O
be	O
written	O
as	O
a	O
bn	O
even	O
though	O
it	O
may	O
correspond	O
to	O
a	O
fully	O
connected	B
cascade	B
dag	O
the	O
particular	O
role	O
of	O
a	O
bn	O
is	O
that	O
the	O
structure	B
of	O
the	O
dag	O
corresponds	O
to	O
a	O
set	O
of	O
conditional	B
independence	B
assumptions	O
namely	O
which	O
parental	O
variables	O
are	O
sufficient	O
to	O
specify	O
each	O
conditional	B
probability	I
table	O
note	O
that	O
this	O
does	O
not	O
mean	B
that	O
non-parental	O
variables	O
have	O
no	O
influence	O
for	O
example	O
for	O
distribution	B
with	O
dag	O
this	O
does	O
not	O
imply	O
the	O
dag	O
specifies	O
conditional	B
independence	B
statements	O
of	O
variables	O
on	O
their	O
ancestors	O
namely	O
which	O
ancestors	O
are	O
causes	O
for	O
the	O
variable	O
the	O
dag	O
corresponds	O
to	O
a	O
statement	O
of	O
conditional	B
independencies	O
in	O
the	O
model	B
to	O
complete	O
the	O
specification	O
of	O
the	O
bn	O
we	O
need	O
to	O
define	O
all	O
elements	O
of	O
the	O
conditional	B
probability	I
tables	O
pxipa	O
once	O
the	O
graphical	O
structure	B
is	O
defined	O
the	O
entries	O
of	O
the	O
conditional	B
probability	I
tables	O
pxipa	O
can	O
be	O
expressed	O
for	O
every	O
possible	O
state	O
of	O
the	O
parental	O
variables	O
pa	O
a	O
value	B
for	O
each	O
of	O
the	O
states	O
of	O
xi	O
needs	O
to	O
be	O
specified	O
one	O
since	O
this	O
is	O
determined	O
by	O
normalisation	B
for	O
a	O
large	O
number	O
of	O
parents	B
writing	O
out	O
a	O
table	O
of	O
values	O
is	O
intractable	O
and	O
the	O
tables	O
are	O
usually	O
parameterised	O
in	O
a	O
low	B
dimensional	I
manner	O
this	O
will	O
be	O
a	O
central	O
topic	O
of	O
our	O
discussion	O
on	O
the	O
application	O
of	O
bns	O
in	O
machine	O
learning	B
draft	O
march	O
belief	B
networks	I
conditional	B
independence	B
whilst	O
a	O
bn	O
corresponds	O
to	O
a	O
set	O
of	O
conditional	B
independence	B
assumptions	O
it	O
is	O
not	O
always	O
immediately	O
clear	O
from	O
the	O
dag	O
whether	O
a	O
set	O
of	O
variables	O
is	O
conditionally	O
independent	O
of	O
a	O
set	O
of	O
other	O
variables	O
for	O
example	O
in	O
are	O
and	O
independent	O
given	O
the	O
state	O
of	O
the	O
answer	O
is	O
yes	O
since	O
we	O
have	O
now	O
combining	O
the	O
two	O
results	O
above	O
we	O
have	O
so	O
that	O
and	O
are	O
indeed	O
independent	O
conditioned	O
on	O
definition	O
independence	B
x	O
yz	O
denotes	O
that	O
the	O
two	O
sets	O
of	O
variables	O
x	O
and	O
y	O
are	O
independent	O
of	O
each	O
other	O
provided	O
we	O
know	O
the	O
state	O
of	O
the	O
set	O
of	O
variables	O
z	O
for	O
full	O
conditional	B
independence	B
x	O
and	O
y	O
must	O
be	O
independent	O
given	O
all	O
states	O
of	O
z	O
formally	O
this	O
means	O
that	O
px	O
pxzpyz	O
for	O
all	O
states	O
of	O
x	O
in	O
case	O
the	O
conditioning	B
set	O
is	O
empty	O
we	O
may	O
also	O
write	O
x	O
y	O
for	O
x	O
y	O
in	O
which	O
case	O
x	O
is	O
independent	O
of	O
y	O
if	O
x	O
and	O
y	O
are	O
not	O
conditionally	O
independent	O
they	O
are	O
conditionally	O
dependent	O
this	O
is	O
written	O
to	O
develop	O
intuition	O
about	O
conditional	B
independence	B
consider	O
the	O
three	O
variable	O
distribution	B
we	O
may	O
write	O
this	O
in	O
any	O
of	O
the	O
ways	O
where	O
is	O
any	O
of	O
the	O
permutations	O
of	O
whilst	O
all	O
different	O
dags	O
they	O
represent	O
the	O
same	O
distribution	B
namely	O
that	O
which	O
makes	O
no	O
conditional	B
independence	B
statements	O
to	O
make	O
an	O
independence	B
statement	O
we	O
need	O
to	O
drop	O
one	O
of	O
the	O
links	O
this	O
gives	O
rise	O
to	O
the	O
dags	O
in	O
are	O
any	O
of	O
these	O
graphs	O
equivalent	B
in	O
the	O
sense	O
that	O
they	O
represent	O
the	O
same	O
distribution	B
figure	O
draft	O
march	O
belief	B
networks	I
figure	O
by	O
dropping	O
say	O
the	O
connection	O
between	O
variables	O
and	O
we	O
reduce	O
the	O
possible	O
bn	O
graphs	O
amongst	O
three	O
variables	O
to	O
fully	O
connected	B
cascade	B
graphs	O
correspond	O
to	O
with	O
with	O
with	O
with	O
with	O
and	O
with	O
any	O
other	O
graphs	O
would	O
be	O
cyclic	O
and	O
therefore	O
not	O
distributions	O
x	O
y	O
x	O
y	O
x	O
y	O
z	O
z	O
z	O
x	O
y	O
w	O
z	O
variable	O
z	O
is	O
a	O
collider	B
graphs	O
figure	O
in	O
graphs	O
and	O
variable	O
z	O
is	O
not	O
a	O
collider	B
and	O
represent	O
conditional	B
independence	B
x	O
y	O
z	O
in	O
graphs	O
and	O
x	O
and	O
y	O
are	O
graphically	O
conditionally	O
dependent	O
given	O
variable	O
z	O
applying	O
bayes	O
rule	O
gives	O
graphc	O
graphd	O
graphb	O
so	O
that	O
dags	O
and	O
represent	O
the	O
same	O
ci	O
assumptions	O
namely	O
that	O
given	O
the	O
state	O
of	O
variable	O
variables	O
and	O
are	O
independent	O
however	O
graph	B
represents	O
something	O
fundamentally	O
different	O
namely	O
there	O
is	O
no	O
way	O
to	O
transform	O
the	O
distribution	B
into	O
any	O
of	O
the	O
others	O
remark	O
dependence	O
belief	B
networks	I
are	O
good	O
for	O
encoding	O
conditional	B
independence	B
but	O
are	O
not	O
well	O
suited	O
to	O
describing	O
dependence	O
for	O
example	O
consider	O
the	O
trivial	O
network	O
pa	O
b	O
pbapa	O
which	O
has	O
the	O
dag	O
representation	O
a	O
b	O
this	O
may	O
appear	O
to	O
encode	O
that	O
a	O
and	O
b	O
are	O
dependent	O
however	O
there	O
are	O
certainly	O
instances	O
when	O
this	O
is	O
not	O
the	O
case	O
for	O
example	O
it	O
may	O
be	O
that	O
the	O
conditional	B
is	O
such	O
that	O
pba	O
pb	O
so	O
that	O
pa	O
b	O
papb	O
in	O
this	O
case	O
although	O
generally	O
there	O
may	O
appear	O
to	O
be	O
a	O
graphical	O
dependence	O
from	O
the	O
dag	O
there	O
can	O
be	O
instances	O
of	O
the	O
distributions	O
for	O
which	O
dependence	O
does	O
not	O
follow	O
the	O
same	O
holds	O
for	O
markov	O
networks	O
in	O
which	O
pa	O
b	O
b	O
whilst	O
this	O
suggests	O
graphical	O
dependence	O
between	O
a	O
and	O
b	O
for	O
b	O
the	O
variables	O
are	O
independent	O
the	O
impact	O
of	O
collisions	O
in	O
a	O
general	O
bn	O
how	O
could	O
we	O
check	B
if	O
x	O
y	O
z	O
in	O
x	O
and	O
y	O
are	O
independent	O
when	O
conditioned	O
on	O
z	O
in	O
they	O
are	O
dependent	O
in	O
this	O
situation	O
variable	O
z	O
is	O
called	O
a	O
collider	B
the	O
arrows	O
of	O
its	O
draft	O
march	O
belief	B
networks	I
a	O
b	O
e	O
c	O
d	O
figure	O
the	O
variable	O
d	O
is	O
a	O
collider	B
along	O
the	O
path	B
a	O
b	O
d	O
c	O
but	O
not	O
along	O
the	O
path	B
a	O
b	O
d	O
e	O
is	O
a	O
e	O
b	O
a	O
and	O
b	O
are	O
not	O
d-connected	O
since	O
there	O
are	O
no	O
colliders	O
on	O
the	O
only	O
path	B
between	O
a	O
and	O
e	O
and	O
since	O
there	O
is	O
a	O
non-collider	O
b	O
which	O
is	O
in	O
the	O
conditioning	B
set	O
hence	O
a	O
and	O
b	O
are	O
d-separated	O
i	O
e	O
a	O
e	O
b	O
neighbours	O
are	O
pointing	O
towards	O
it	O
what	O
about	O
in	O
when	O
we	O
condition	O
on	O
z	O
x	O
and	O
y	O
will	O
be	O
graphically	O
dependent	O
since	O
w	O
px	O
yz	O
px	O
y	O
z	O
pz	O
pz	O
pzwpwx	O
ypxpy	O
pxzpyz	O
intuitively	O
variable	O
w	O
becomes	O
dependent	O
on	O
the	O
value	B
of	O
z	O
and	O
since	O
x	O
and	O
y	O
are	O
conditionally	O
dependent	O
on	O
w	O
they	O
are	O
also	O
conditionally	O
dependent	O
on	O
z	O
roughly	O
speaking	O
if	O
there	O
is	O
a	O
non-collider	O
z	O
which	O
is	O
conditioned	O
on	O
along	O
the	O
path	B
between	O
x	O
and	O
y	O
in	O
then	O
this	O
path	B
does	O
not	O
make	O
x	O
and	O
y	O
dependent	O
similarly	O
if	O
there	O
is	O
a	O
path	B
between	O
x	O
and	O
y	O
which	O
contains	O
a	O
collider	B
provided	O
that	O
this	O
collider	B
is	O
not	O
in	O
the	O
conditioning	B
set	O
neither	O
are	O
any	O
of	O
its	O
descendants	O
then	O
this	O
path	B
does	O
not	O
make	O
x	O
and	O
y	O
dependent	O
if	O
there	O
is	O
a	O
path	B
between	O
x	O
and	O
y	O
which	O
contains	O
no	O
colliders	O
and	O
no	O
conditioning	B
variables	O
then	O
this	O
path	B
d-connects	O
x	O
and	O
y	O
note	O
that	O
a	O
collider	B
is	O
defined	O
relative	O
to	O
a	O
path	B
in	O
the	O
variable	O
d	O
is	O
a	O
collider	B
along	O
the	O
path	B
a	O
b	O
d	O
c	O
but	O
not	O
along	O
the	O
path	B
a	O
b	O
d	O
e	O
relative	O
to	O
this	O
path	B
the	O
two	O
arrows	O
do	O
not	O
point	O
inwards	O
to	O
d	O
consider	O
the	O
bn	O
a	O
b	O
c	O
here	O
a	O
and	O
c	O
are	O
independent	O
however	O
conditioning	B
of	O
b	O
makes	O
them	O
graphically	O
dependent	O
intuitively	O
whilst	O
we	O
believe	O
the	O
root	O
causes	O
are	O
independent	O
given	O
the	O
value	B
of	O
the	O
observation	O
this	O
tells	O
us	O
something	O
about	O
the	O
state	O
of	O
both	O
the	O
causes	O
coupling	O
them	O
and	O
making	O
them	O
dependent	O
d-separation	O
the	O
dag	O
concepts	O
of	O
d-separation	O
and	O
d-connection	O
are	O
central	O
to	O
determining	O
conditional	B
independence	B
in	O
any	O
bn	O
with	O
structure	B
given	O
by	O
the	O
definition	O
d-separation	O
if	O
g	O
is	O
a	O
directed	B
graph	B
in	O
which	O
x	O
y	O
and	O
z	O
are	O
disjoint	O
sets	O
of	O
vertices	O
then	O
x	O
and	O
y	O
are	O
d-connected	O
by	O
z	O
in	O
g	O
if	O
and	O
only	O
if	O
there	O
exists	O
an	O
undirected	B
path	B
u	O
between	O
some	O
vertex	O
in	O
x	O
and	O
some	O
vertex	O
in	O
y	O
such	O
that	O
for	O
every	O
collider	B
c	O
on	O
u	O
either	O
c	O
or	O
a	O
descendent	O
of	O
c	O
is	O
in	O
z	O
and	O
no	O
non-collider	O
on	O
u	O
is	O
in	O
z	O
x	O
and	O
y	O
are	O
d-separated	O
by	O
z	O
in	O
g	O
if	O
and	O
only	O
if	O
they	O
are	O
not	O
d-connected	O
by	O
z	O
in	O
g	O
one	O
may	O
also	O
phrase	O
this	O
as	O
follows	O
for	O
every	O
variable	O
x	O
x	O
and	O
y	O
y	O
check	B
every	O
path	B
u	O
between	O
x	O
and	O
y	O
a	O
path	B
u	O
is	O
said	O
to	O
be	O
blocked	B
if	O
there	O
is	O
a	O
node	O
w	O
on	O
u	O
such	O
that	O
either	O
w	O
is	O
a	O
collider	B
and	O
neither	O
w	O
nor	O
any	O
of	O
its	O
descendants	O
is	O
in	O
z	O
w	O
is	O
not	O
a	O
collider	B
on	O
u	O
and	O
w	O
is	O
in	O
z	O
draft	O
march	O
belief	B
networks	I
if	O
all	O
such	O
paths	O
are	O
blocked	B
then	O
x	O
and	O
y	O
are	O
d-separated	O
by	O
z	O
if	O
the	O
variable	O
sets	O
x	O
and	O
y	O
are	O
d-separated	O
by	O
z	O
they	O
are	O
independent	O
conditional	B
on	O
z	O
in	O
all	O
probability	O
distributions	O
such	O
a	O
graph	B
can	O
represent	O
the	O
bayes	O
ball	O
provides	O
a	O
linear	B
time	O
complexity	O
algorithm	B
which	O
given	O
a	O
set	O
of	O
nodes	O
x	O
and	O
z	O
determines	O
the	O
set	O
of	O
nodes	O
y	O
such	O
that	O
x	O
y	O
z	O
y	O
is	O
called	O
the	O
set	O
of	O
irrelevant	O
nodes	O
for	O
x	O
given	O
z	O
d-connection	O
and	O
dependence	O
given	O
a	O
dag	O
we	O
can	O
imply	O
with	O
certainty	O
that	O
two	O
variables	O
are	O
independent	O
provided	O
they	O
are	O
d-separated	O
can	O
we	O
infer	O
that	O
they	O
are	O
dependent	O
provided	O
they	O
are	O
d-connected	O
consider	O
the	O
following	O
situation	O
pa	O
b	O
c	O
pca	B
bpapb	O
for	O
which	O
we	O
note	O
that	O
a	O
and	O
b	O
are	O
d-connected	O
by	O
c	O
for	O
concreteness	O
we	O
assume	O
c	O
is	O
binary	O
with	O
states	O
the	O
question	O
is	O
whether	O
a	O
and	O
b	O
are	O
dependent	O
conditioned	O
on	O
c	O
a	O
b	O
c	O
to	O
answer	O
this	O
consider	O
pa	O
bc	O
pc	O
bpapb	O
ab	O
pc	O
bpapb	O
in	O
general	O
the	O
first	O
term	O
pc	O
b	O
does	O
not	O
need	O
to	O
be	O
a	O
factored	O
function	B
of	O
a	O
and	O
b	O
and	O
therefore	O
a	O
and	O
b	O
are	O
conditionally	O
graphically	O
dependent	O
however	O
we	O
can	O
construct	O
cases	O
where	O
this	O
is	O
not	O
so	O
for	O
example	O
let	O
pc	O
b	O
and	O
pc	O
b	O
pc	O
b	O
where	O
and	O
are	O
arbitrary	O
potentials	O
between	O
and	O
then	O
z	O
a	O
b	O
pa	O
bc	O
z	O
which	O
shows	O
that	O
pa	O
bc	O
is	O
a	O
product	O
of	O
a	O
function	B
in	O
a	O
and	O
function	B
in	O
b	O
so	O
that	O
a	O
and	O
b	O
are	O
independent	O
conditioned	O
on	O
c	O
a	O
second	O
example	O
is	O
given	O
by	O
the	O
distribution	B
pa	O
b	O
c	O
pcbpbapa	O
in	O
which	O
a	O
and	O
c	O
are	O
d-connected	O
by	O
b	O
the	O
question	O
is	O
are	O
a	O
and	O
c	O
dependent	O
a	O
c	O
for	O
simplicity	O
we	O
assume	O
b	O
takes	O
the	O
two	O
states	O
then	O
pa	O
c	O
b	O
pa	O
c	O
pa	O
papb	O
pcbpba	O
pa	O
pcb	O
pcb	O
pcb	O
pb	O
pcb	O
pcb	O
for	O
the	O
setting	O
pb	O
for	O
some	O
constant	O
for	O
all	O
states	O
of	O
a	O
then	O
which	O
is	O
a	O
product	O
of	O
a	O
function	B
of	O
a	O
and	O
a	O
function	B
of	O
c	O
hence	O
a	O
and	O
c	O
are	O
independent	O
draft	O
march	O
belief	B
networks	I
c	O
e	O
a	O
c	O
e	O
a	O
b	O
t	O
d	O
f	O
s	O
b	O
g	O
b	O
d	O
f	O
s	O
b	O
t	O
g	O
u	O
figure	O
examples	O
for	O
d-separation	O
b	O
d-separates	O
a	O
from	O
e	O
the	O
joint	B
variables	O
d	O
d-connect	O
a	O
and	O
e	O
c	O
and	O
e	O
are	O
d-connected	O
b	O
dconnects	O
a	O
and	O
e	O
figure	O
t	O
and	O
f	O
are	O
d-connected	O
by	O
g	O
b	O
and	O
f	O
are	O
d-separated	O
by	O
u	O
the	O
moral	O
of	O
the	O
story	O
is	O
that	O
d-separation	O
necessarily	O
implies	O
independence	B
however	O
d-connection	O
does	O
not	O
necessarily	O
imply	O
dependence	O
it	O
might	O
be	O
that	O
there	O
are	O
numerical	B
settings	O
for	O
which	O
variables	O
are	O
independent	O
even	O
though	O
they	O
are	O
d-connected	O
for	O
this	O
reason	O
we	O
use	O
the	O
term	O
graphical	O
dependence	O
when	O
the	O
graph	B
would	O
suggest	O
that	O
variables	O
are	O
dependent	O
even	O
though	O
there	O
may	O
be	O
numerical	B
instantiations	O
were	O
dependence	O
does	O
not	O
hold	O
see	O
example	O
consider	O
is	O
a	O
e	O
b	O
if	O
we	O
sum	O
out	O
variable	O
d	O
then	O
we	O
see	O
that	O
a	O
and	O
e	O
are	O
independent	O
given	O
b	O
since	O
the	O
variable	O
e	O
will	O
appear	O
as	O
an	O
isolated	O
factor	B
independent	O
of	O
all	O
other	O
variables	O
hence	O
indeed	O
a	O
e	O
b	O
whilst	O
b	O
is	O
a	O
collider	B
which	O
is	O
in	O
the	O
conditioning	B
set	O
we	O
need	O
all	O
colliders	O
on	O
the	O
path	B
to	O
be	O
in	O
the	O
conditioning	B
set	O
their	O
descendants	O
for	O
d-connectedness	O
in	O
if	O
we	O
sum	O
out	O
variable	O
d	O
then	O
c	O
and	O
e	O
become	O
intrinsically	O
linked	O
and	O
pa	O
b	O
c	O
e	O
will	O
not	O
factorise	O
into	O
a	O
function	B
of	O
a	O
multiplied	O
by	O
a	O
function	B
of	O
e	O
hence	O
they	O
are	O
dependent	O
example	O
consider	O
the	O
graph	B
in	O
are	O
the	O
variables	O
t	O
and	O
f	O
unconditionally	O
independent	O
i	O
e	O
t	O
f	O
here	O
there	O
are	O
two	O
colliders	O
namely	O
g	O
and	O
s	O
however	O
these	O
are	O
not	O
in	O
the	O
conditioning	B
set	O
is	O
empty	O
and	O
hence	O
they	O
are	O
d-separated	O
and	O
therefore	O
unconditionally	O
independent	O
what	O
about	O
t	O
f	O
g	O
there	O
is	O
a	O
collider	B
on	O
the	O
path	B
between	O
t	O
and	O
f	O
which	O
is	O
in	O
the	O
conditioning	B
set	O
hence	O
t	O
and	O
f	O
are	O
d-connected	O
by	O
g	O
and	O
therefore	O
t	O
and	O
f	O
are	O
not	O
independent	O
conditioned	O
on	O
g	O
what	O
about	O
b	O
f	O
s	O
since	O
there	O
is	O
a	O
collider	B
s	O
in	O
the	O
conditioning	B
set	O
on	O
the	O
path	B
between	O
t	O
and	O
f	O
then	O
b	O
and	O
f	O
are	O
graphically	O
conditionally	O
dependent	O
given	O
s	O
example	O
is	O
f	O
u	O
in	O
since	O
the	O
conditioning	B
set	O
is	O
empty	O
and	O
every	O
path	B
from	O
either	O
b	O
or	O
f	O
to	O
u	O
contains	O
a	O
collider	B
b	O
f	O
are	O
unconditionally	O
independent	O
of	O
u	O
markov	B
equivalence	I
in	O
belief	B
networks	I
draft	O
march	O
definition	O
properties	B
of	O
belief	B
networks	I
belief	B
networks	I
b	O
b	O
b	O
b	O
b	O
b	O
b	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
c	O
c	O
c	O
c	O
c	O
c	O
c	O
pa	O
b	O
c	O
pca	B
bpapb	O
a	O
and	O
b	O
are	O
independent	O
pa	O
b	O
papb	O
a	O
and	O
b	O
are	O
conditionally	O
dependent	O
on	O
c	O
pa	O
bc	O
pacpbc	O
a	O
b	O
marginalising	O
over	O
c	O
makes	O
a	O
and	O
b	O
independent	O
a	O
b	O
conditioning	B
on	O
c	O
makes	O
a	O
and	O
b	O
dependent	O
pa	O
b	O
c	O
pacpbcpc	O
a	O
and	O
b	O
are	O
dependent	O
pa	O
b	O
papb	O
a	O
and	O
b	O
are	O
conditionally	O
independent	O
on	O
c	O
pa	O
bc	O
pacpbc	O
a	O
marginalising	O
over	O
c	O
makes	O
a	O
and	O
b	O
dependent	O
b	O
a	O
a	O
b	O
b	O
conditioning	B
on	O
c	O
makes	O
a	O
and	O
b	O
independent	O
a	O
b	O
c	O
c	O
definition	O
equivalence	O
two	O
graphs	O
are	O
markov	B
equivalent	B
if	O
they	O
both	O
represent	O
the	O
same	O
set	O
of	O
conditional	B
independence	B
statements	O
define	O
the	O
skeleton	B
of	O
a	O
graph	B
by	O
removing	O
the	O
directions	O
on	O
the	O
arrows	O
define	O
an	O
immorality	B
in	O
a	O
dag	O
as	O
a	O
configuration	O
of	O
three	O
nodes	O
abc	O
such	O
that	O
c	O
is	O
a	O
child	O
of	O
both	O
a	O
and	O
b	O
with	O
a	O
and	O
b	O
not	O
directly	O
connected	B
two	O
dags	O
represent	O
the	O
same	O
set	O
of	O
independence	B
assumptions	O
are	O
markov	B
equivalent	B
if	O
and	O
only	O
if	O
they	O
have	O
the	O
same	O
skeleton	B
and	O
the	O
same	O
set	O
of	O
immoralities	O
using	O
this	O
rule	O
we	O
see	O
that	O
in	O
bns	O
have	O
the	O
same	O
skeleton	B
with	O
no	O
immoralities	O
and	O
are	O
therefore	O
equivalent	B
however	O
bn	O
has	O
an	O
immorality	B
and	O
is	O
therefore	O
not	O
equivalent	B
to	O
dags	O
draft	O
march	O
causality	B
belief	B
networks	I
have	O
limited	O
expressibility	O
h	O
figure	O
two	O
treatments	O
and	O
corresponding	O
outcomes	O
the	O
health	O
of	O
a	O
patient	O
is	O
represented	O
by	O
h	O
this	O
dag	O
embodies	O
the	O
conditional	B
independence	B
statements	O
namely	O
that	O
the	O
treatments	O
have	O
no	O
effect	O
on	O
each	O
other	O
one	O
could	O
represent	O
the	O
marginalised	O
latent	B
variable	I
using	O
a	O
bi-directional	O
edge	O
consider	O
the	O
dag	O
in	O
this	O
dag	O
could	O
be	O
used	O
to	O
represent	O
two	O
successive	O
experiments	O
where	O
and	O
are	O
two	O
treatments	O
and	O
and	O
represent	O
two	O
outcomes	O
of	O
interest	O
h	O
is	O
the	O
underlying	O
health	O
status	O
of	O
the	O
patient	O
the	O
first	O
treatment	O
has	O
no	O
effect	O
on	O
the	O
second	O
outcome	O
hence	O
there	O
is	O
no	O
edge	O
from	O
to	O
now	O
consider	O
the	O
implied	O
independencies	O
in	O
the	O
marginal	B
distribution	B
obtained	O
by	O
marginalising	O
the	O
full	O
distribution	B
over	O
h	O
there	O
is	O
no	O
dag	O
containing	O
only	O
the	O
vertices	O
which	O
represents	O
the	O
independence	B
relations	O
and	O
does	O
not	O
also	O
imply	O
some	O
other	O
independence	B
relation	O
that	O
is	O
not	O
implied	O
by	O
consequently	O
any	O
dag	O
on	O
vertices	O
alone	O
will	O
either	O
fail	O
to	O
represent	O
an	O
independence	B
relation	O
of	O
or	O
will	O
impose	O
some	O
additional	O
independence	B
restriction	O
that	O
is	O
not	O
implied	O
by	O
the	O
dag	O
in	O
the	O
above	O
example	O
h	O
hph	O
cannot	O
in	O
general	O
be	O
expressed	O
as	O
a	O
product	O
of	O
functions	O
defined	O
on	O
a	O
limited	O
set	O
of	O
the	O
variables	O
however	O
it	O
is	O
the	O
case	O
that	O
the	O
conditional	B
independence	B
conditions	O
hold	O
in	O
they	O
are	O
there	O
encoded	O
in	O
the	O
form	O
of	O
the	O
conditional	B
probability	I
tables	O
it	O
is	O
just	O
that	O
we	O
cannot	O
see	O
this	O
independence	B
since	O
it	O
is	O
not	O
present	O
in	O
the	O
structure	B
of	O
the	O
marginalised	O
graph	B
one	O
can	O
naturally	O
infer	O
this	O
in	O
the	O
larger	O
graph	B
h	O
this	O
example	O
demonstrates	O
that	O
bns	O
cannot	O
express	O
all	O
the	O
conditional	B
independence	B
statements	O
that	O
could	O
be	O
made	O
on	O
that	O
set	O
of	O
variables	O
set	O
of	O
conditional	B
independence	B
statements	O
can	O
be	O
increased	O
by	O
considering	O
extra	O
latent	B
variables	O
however	O
this	O
situation	O
is	O
rather	O
general	O
in	O
the	O
sense	O
that	O
any	O
graphical	O
model	B
has	O
limited	O
expressibility	O
in	O
terms	O
of	O
independence	B
it	O
is	O
worth	O
bearing	O
in	O
mind	O
that	O
belief	B
networks	I
may	O
not	O
always	O
be	O
the	O
most	O
appropriate	O
framework	O
to	O
express	O
one	O
s	O
independence	B
assumptions	O
and	O
intuitions	O
a	O
natural	B
consideration	O
is	O
to	O
use	O
a	O
bi-directional	O
arrow	O
when	O
a	O
latent	B
variable	I
is	O
marginalised	O
for	O
one	O
could	O
depict	O
the	O
marginal	B
distribution	B
using	O
a	O
bi-directional	O
edge	O
similarly	O
a	O
bn	O
with	O
a	O
latent	B
conditioned	O
variable	O
can	O
be	O
represented	O
using	O
an	O
undirected	B
edge	O
for	O
a	O
discussion	O
of	O
these	O
and	O
related	O
issues	O
see	O
causality	B
causality	B
is	O
a	O
contentious	O
topic	O
and	O
the	O
purpose	O
of	O
this	O
section	O
is	O
make	O
the	O
reader	O
aware	O
of	O
some	O
pitfalls	O
that	O
can	O
occur	O
and	O
which	O
may	O
give	O
rise	O
to	O
erroneous	O
inferences	O
the	O
reader	O
is	O
referred	O
to	O
and	O
for	O
further	O
details	O
the	O
word	O
causal	B
is	O
contentious	O
particularly	O
in	O
cases	O
where	O
the	O
model	B
of	O
the	O
data	O
contains	O
no	O
explicit	O
temporal	O
information	O
so	O
that	O
formally	O
only	O
correlations	O
or	O
dependencies	O
can	O
be	O
inferred	O
for	O
a	O
distribution	B
pa	O
b	O
we	O
could	O
write	O
this	O
as	O
either	O
pabpb	O
or	O
pbapa	O
in	O
we	O
might	O
think	O
that	O
b	O
causes	O
a	O
and	O
in	O
a	O
causes	O
b	O
clearly	O
this	O
is	O
not	O
very	O
meaningful	O
since	O
they	O
both	O
represent	O
exactly	O
the	O
same	O
distribution	B
formally	O
belief	B
networks	I
only	O
make	O
independence	B
statements	O
not	O
causal	B
ones	O
nevertheless	O
in	O
constructing	O
bns	O
it	O
can	O
be	O
helpful	O
to	O
think	O
about	O
dependencies	O
in	O
terms	O
of	O
causation	O
draft	O
march	O
causality	B
a	O
b	O
a	O
b	O
r	O
w	O
r	O
w	O
figure	O
both	O
and	O
represent	O
the	O
same	O
distribution	B
pa	O
b	O
pabpb	O
pbapa	O
the	O
graph	B
represents	O
prain	O
grasswet	O
pgrasswetrainprain	O
we	O
could	O
equally	O
have	O
written	O
praingrasswetpgrasswet	O
although	O
this	O
appears	O
to	O
be	O
causally	O
non-sensical	O
g	O
d	O
r	O
fd	O
g	O
d	O
a	O
dag	O
for	O
the	O
relation	O
befigure	O
tween	O
gender	O
drug	O
and	O
recovery	O
see	O
influence	O
diagram	O
no	O
decision	O
variable	O
is	O
required	O
for	O
g	O
since	O
g	O
has	O
no	O
parents	B
r	O
since	O
our	O
intuitive	O
understanding	O
is	O
usually	O
framed	O
in	O
how	O
one	O
variable	O
influences	O
another	O
first	O
we	O
discuss	O
a	O
classic	O
conundrum	O
that	O
highlights	O
potential	B
pitfalls	O
that	O
can	O
arise	O
simpson	O
s	O
paradox	O
simpson	O
s	O
paradox	O
is	O
a	O
cautionary	O
tale	O
in	O
causal	B
reasoning	O
in	O
bns	O
consider	O
a	O
medical	O
trial	O
in	O
which	O
patient	O
treatment	O
and	O
outcome	O
are	O
recovered	O
two	O
trials	O
were	O
conducted	O
one	O
with	O
females	O
and	O
one	O
with	O
males	O
the	O
data	O
is	O
summarised	O
in	O
the	O
question	O
is	O
does	O
the	O
drug	O
cause	O
increased	O
recovery	O
according	O
to	O
the	O
table	O
for	O
males	O
the	O
answer	O
is	O
no	O
since	O
more	O
males	O
recovered	O
when	O
they	O
were	O
not	O
given	O
the	O
drug	O
than	O
when	O
they	O
were	O
similarly	O
more	O
females	O
recovered	O
when	O
not	O
given	O
the	O
drug	O
than	O
recovered	O
when	O
given	O
the	O
drug	O
the	O
conclusion	O
appears	O
that	O
the	O
drug	O
cannot	O
be	O
beneficial	O
since	O
it	O
aids	O
neither	O
subpopulation	O
however	O
ignoring	O
the	O
gender	O
information	O
and	O
collating	O
both	O
the	O
male	O
and	O
female	O
data	O
into	O
one	O
combined	O
table	O
we	O
find	O
that	O
more	O
people	O
recovered	O
when	O
given	O
the	O
drug	O
than	O
when	O
not	O
hence	O
even	O
though	O
the	O
drug	O
doesn	O
t	O
seem	O
to	O
work	O
for	O
either	O
males	O
or	O
females	O
it	O
does	O
seem	O
to	O
work	O
overall	O
should	O
we	O
therefore	O
recommend	O
the	O
drug	O
or	O
not	O
resolution	B
of	O
the	O
paradox	O
the	O
paradox	O
occurs	O
since	O
we	O
are	O
asking	O
a	O
causal	B
interventional	O
question	O
the	O
question	O
we	O
are	O
intuitively	O
asking	O
is	O
if	O
we	O
give	O
someone	O
the	O
drug	O
what	O
happens	O
however	O
the	O
calculation	O
we	O
performed	O
above	O
was	O
only	O
an	O
observational	O
calculation	O
the	O
calculation	O
we	O
really	O
want	O
is	O
to	O
first	O
intervene	O
setting	O
males	O
recovered	O
not	O
recovered	O
rec	O
rate	O
given	O
drug	O
not	O
given	O
drug	O
females	O
given	O
drug	O
not	O
given	O
drug	O
combined	O
given	O
drug	O
not	O
given	O
drug	O
recovered	O
not	O
recovered	O
rec	O
rate	O
recovered	O
not	O
recovered	O
rec	O
rate	O
table	O
table	O
for	O
simpson	O
s	O
paradox	O
draft	O
march	O
causality	B
the	O
drug	O
state	O
and	O
then	O
observe	O
what	O
effect	O
this	O
has	O
on	O
recovery	O
describes	O
this	O
as	O
the	O
difference	O
between	O
given	O
that	O
we	O
see	O
evidence	O
versus	O
given	O
that	O
we	O
do	O
evidence	O
a	O
model	B
of	O
the	O
gender	O
drug	O
and	O
recovery	O
data	O
makes	O
no	O
conditional	B
independence	B
assumptions	O
is	O
pg	O
d	O
r	O
prg	O
dpdgpg	O
an	O
observational	O
calculation	O
concerns	O
computing	O
prg	O
d	O
and	O
prd	O
interpretation	O
however	O
if	O
we	O
intervene	O
and	O
give	O
the	O
drug	O
then	O
the	O
term	O
pdg	O
in	O
equation	B
should	O
play	O
no	O
role	O
in	O
the	O
experiment	O
the	O
distribution	B
models	O
that	O
given	O
the	O
gender	O
we	O
select	O
a	O
drug	O
with	O
probability	O
pdg	O
which	O
is	O
not	O
the	O
case	O
we	O
decide	O
to	O
give	O
the	O
drug	O
or	O
not	O
independent	O
of	O
gender	O
in	O
the	O
causal	B
case	O
we	O
are	O
modelling	B
the	O
causal	B
experiment	O
in	O
this	O
case	O
the	O
term	O
pdg	O
needs	O
to	O
be	O
replaced	O
by	O
a	O
term	O
that	O
reflects	O
the	O
setup	O
of	O
the	O
experiment	O
in	O
an	O
atomic	O
intervention	O
a	O
single	O
variable	O
is	O
set	O
in	O
a	O
particular	O
in	O
our	O
atomic	O
causal	B
intervention	O
in	O
setting	O
d	O
we	O
are	O
dealing	O
with	O
the	O
modified	O
distribution	B
in	O
a	O
causal	B
pg	O
rd	O
prg	O
dpg	O
where	O
the	O
terms	O
on	O
the	O
right	O
hand	O
side	O
of	O
this	O
equation	B
are	O
taken	O
from	O
the	O
original	O
bn	O
of	O
the	O
data	O
to	O
denote	O
an	O
intervention	O
we	O
use	O
prg	O
d	O
prg	O
d	O
prg	O
dpg	O
r	O
prg	O
dpg	O
prg	O
d	O
can	O
also	O
consider	O
here	O
g	O
as	O
being	O
interventional	O
in	O
this	O
case	O
it	O
doesn	O
t	O
matter	O
since	O
the	O
fact	O
that	O
the	O
variable	O
g	O
has	O
no	O
parents	B
means	O
that	O
for	O
any	O
distribution	B
conditional	B
on	O
g	O
the	O
prior	B
factor	B
pg	O
will	O
not	O
be	O
present	O
using	O
equation	B
for	O
the	O
males	O
given	O
the	O
drug	O
recover	O
versus	O
recovery	O
when	O
not	O
given	O
the	O
drug	O
for	O
the	O
females	O
given	O
the	O
drug	O
recover	O
versus	O
recovery	O
when	O
not	O
given	O
the	O
drug	O
similarly	O
prd	O
prd	O
g	O
prg	O
dpg	O
rg	O
prg	O
dpg	O
g	O
prg	O
dpg	O
using	O
the	O
above	O
post	B
intervention	I
distribution	B
we	O
have	O
precoverydrug	O
and	O
precoveryno	O
drug	O
hence	O
we	O
correctly	O
infer	O
that	O
the	O
drug	O
is	O
overall	O
not	O
helpful	O
as	O
we	O
intuitively	O
expect	O
and	O
is	O
consistent	B
with	O
the	O
results	O
from	O
both	O
subpopulations	O
here	O
prd	O
means	O
that	O
we	O
first	O
choose	O
either	O
a	O
male	O
or	O
female	O
patient	O
at	O
random	O
and	O
then	O
give	O
them	O
the	O
drug	O
or	O
not	O
depending	O
on	O
the	O
state	O
of	O
d	O
the	O
point	O
is	O
that	O
we	O
do	O
not	O
randomly	O
decide	O
whether	O
or	O
not	O
to	O
give	O
the	O
drug	O
hence	O
the	O
absence	O
of	O
the	O
term	O
pdg	O
from	O
the	O
joint	B
distribution	B
one	O
way	O
to	O
think	O
about	O
such	O
models	O
is	O
to	O
consider	O
how	O
to	O
draw	O
a	O
sample	O
from	O
the	O
joint	B
distribution	B
of	O
the	O
random	O
variables	O
in	O
most	O
cases	O
this	O
should	O
clarify	O
the	O
role	O
of	O
causality	B
in	O
the	O
experiment	O
in	O
contrast	O
to	O
the	O
interventional	O
calculation	O
the	O
observational	O
calculation	O
makes	O
no	O
conditional	B
independence	B
assumptions	O
this	O
means	O
that	O
for	O
example	O
the	O
term	O
pdg	O
plays	O
a	O
role	O
in	O
the	O
calculation	O
reader	O
might	O
wish	O
to	O
verify	O
that	O
the	O
result	O
given	O
in	O
the	O
combined	O
data	O
in	O
is	O
equivalent	B
to	O
inferring	O
with	O
the	O
full	O
distribution	B
equation	B
general	O
experimental	O
conditions	O
can	O
be	O
modelled	O
by	O
replacing	O
pdg	O
by	O
an	O
intervention	O
distribution	B
draft	O
march	O
definition	O
s	O
do	O
operator	O
causality	B
in	O
a	O
causal	B
inference	B
in	O
which	O
the	O
effect	O
of	O
setting	O
variables	O
xck	O
ck	O
c	O
in	O
states	O
xck	O
is	O
to	O
be	O
inferred	O
this	O
is	O
equivalent	B
to	O
standard	O
evidential	O
inference	B
in	O
the	O
post	B
intervention	I
distribution	B
doxck	O
xck	O
xck	O
pxcipa	O
p	O
c	O
where	O
any	O
parental	O
states	O
of	O
pa	O
of	O
xj	O
are	O
set	O
in	O
their	O
evidential	O
states	O
an	O
alternative	O
notation	O
is	O
xck	O
in	O
words	O
for	O
those	O
variables	O
for	O
which	O
we	O
causally	O
intervene	O
and	O
set	O
in	O
a	O
particular	O
state	O
the	O
corresponding	O
terms	O
pxcipa	O
are	O
removed	O
from	O
the	O
original	O
belief	B
network	I
for	O
variables	O
which	O
are	O
evidential	O
but	O
non-causal	O
the	O
corresponding	O
factors	O
are	O
not	O
removed	O
from	O
the	O
distribution	B
the	O
interpretation	O
is	O
that	O
the	O
post	B
intervention	I
distribution	B
corresponds	O
to	O
an	O
experiment	O
in	O
which	O
the	O
causal	B
variables	O
are	O
first	O
set	O
and	O
non-causal	O
variables	O
are	O
subsequently	O
observed	O
influence	O
diagrams	O
and	O
the	O
do-calculus	O
in	O
making	O
causal	B
inferences	O
we	O
must	O
adjust	O
the	O
model	B
to	O
reflect	O
any	O
causal	B
experimental	O
conditions	O
in	O
setting	O
any	O
variable	O
into	O
a	O
particular	O
state	O
we	O
need	O
to	O
surgically	O
remove	O
all	O
parental	O
links	O
of	O
that	O
variable	O
pearl	O
calls	O
this	O
the	O
do	O
operator	O
and	O
contrasts	O
an	O
observational	O
see	O
inference	B
pxy	O
with	O
a	O
causal	B
make	O
or	O
do	O
inference	B
pxdoy	O
a	O
useful	O
alternative	O
representation	O
is	O
to	O
append	O
variables	O
x	O
upon	O
which	O
an	O
intervention	O
can	O
possibly	O
be	O
made	O
with	O
a	O
parental	O
decision	O
variable	O
for	O
pd	O
g	O
r	O
fd	O
pdfd	O
gpgprg	O
dpfd	O
where	O
pdfd	O
g	O
pdpa	O
pdfd	O
d	O
g	O
for	O
d	O
d	O
and	O
otherwise	O
hence	O
if	O
the	O
decision	O
variable	O
fd	O
is	O
set	O
to	O
the	O
empty	O
state	O
the	O
variable	O
d	O
is	O
determined	O
by	O
the	O
standard	O
observational	O
term	O
pdpa	O
if	O
the	O
decision	O
variable	O
is	O
set	O
to	O
a	O
state	O
of	O
d	O
then	O
the	O
variable	O
puts	O
all	O
its	O
probability	O
in	O
that	O
single	O
state	O
of	O
d	O
d	O
this	O
has	O
the	O
effect	O
of	O
replacing	O
the	O
conditional	B
probability	I
term	O
a	O
unit	O
factor	B
and	O
any	O
instances	O
of	O
d	O
set	O
to	O
the	O
variable	O
in	O
its	O
interventional	O
a	O
potential	B
advantage	O
of	O
the	O
influence	O
diagram	O
approach	B
over	O
the	O
do-calculus	O
is	O
that	O
deriving	O
conditional	B
independence	B
statements	O
can	O
be	O
made	O
based	O
on	O
standard	O
techniques	O
for	O
the	O
augmented	B
bn	O
additionally	O
for	O
parameter	B
learning	B
standard	O
techniques	O
apply	O
in	O
which	O
the	O
decision	O
variables	O
are	O
set	O
to	O
the	O
condition	O
under	O
which	O
each	O
data	O
sample	O
was	O
collected	O
causal	B
or	O
non-causal	O
sample	O
example	O
and	O
accidents	O
a	O
causal	B
belief	B
network	I
the	O
influence	O
diagram	O
is	O
a	O
distribution	B
over	O
variables	O
in	O
including	O
decision	O
variables	O
in	O
contrast	O
to	O
the	O
application	O
of	O
ids	O
in	O
general	O
cases	O
can	O
be	O
considered	O
in	O
which	O
the	O
variables	O
are	O
placed	O
in	O
a	O
distribution	B
of	O
states	O
draft	O
march	O
parameterising	O
belief	B
networks	I
y	O
y	O
y	O
figure	O
if	O
all	O
variables	O
are	O
binary	O
states	O
are	O
required	O
to	O
specify	O
here	O
only	O
states	O
are	O
required	O
noisy	O
logic	O
gates	O
fd	O
fa	O
d	O
a	O
consider	O
the	O
following	O
cpt	O
entries	O
pd	O
bad	O
pa	O
trd	O
bad	O
if	O
we	O
intervene	O
and	O
use	O
a	O
bad	O
driver	O
what	O
is	O
the	O
probability	O
of	O
an	O
accident	O
pa	O
trd	O
bad	O
fd	O
bad	O
fa	O
pa	O
trd	O
bad	O
on	O
the	O
other	O
hand	O
if	O
we	O
intervene	O
and	O
make	O
an	O
accident	O
what	O
is	O
the	O
probability	O
the	O
driver	O
involved	O
is	O
bad	O
this	O
is	O
pd	O
bada	O
tr	O
fd	O
fa	O
tr	O
pd	O
bad	O
learning	B
the	O
direction	O
of	O
arrows	O
in	O
the	O
absence	O
of	O
data	O
from	O
causal	B
experiments	O
one	O
should	O
be	O
justifiably	O
sceptical	O
about	O
learning	B
causal	B
networks	O
nevertheless	O
one	O
might	O
prefer	O
a	O
certain	O
direction	O
of	O
a	O
link	O
based	O
on	O
assumptions	O
of	O
the	O
simplicity	O
of	O
the	O
cpts	O
this	O
preference	O
may	O
come	O
from	O
a	O
physical	O
intuition	O
that	O
whilst	O
root	O
causes	O
may	O
be	O
uncertain	B
the	O
relationship	O
from	O
cause	O
to	O
effect	O
is	O
clear	O
in	O
this	O
sense	O
a	O
measure	O
of	O
the	O
complexity	O
of	O
a	O
cpt	O
is	O
required	O
such	O
as	O
entropy	B
such	O
heuristics	O
can	O
be	O
numerically	O
encoded	O
and	O
the	O
directions	O
learned	O
in	O
an	O
otherwise	O
markov	B
equivalent	B
graph	B
parameterising	O
belief	B
networks	I
consider	O
a	O
variable	O
y	O
with	O
many	O
parental	O
variables	O
xn	O
formally	O
the	O
structure	B
of	O
the	O
graph	B
implies	O
nothing	O
about	O
the	O
form	O
of	O
the	O
parameterisation	B
of	O
the	O
table	O
xn	O
if	O
each	O
parent	O
xi	O
has	O
dim	O
states	O
and	O
there	O
is	O
no	O
constraint	O
on	O
the	O
table	O
then	O
the	O
table	O
xn	O
contains	O
i	O
dim	O
entries	O
if	O
stored	O
explicitly	O
for	O
each	O
state	O
this	O
would	O
require	O
potentially	O
huge	O
storage	O
an	O
alternative	O
is	O
to	O
constrain	O
the	O
table	O
to	O
have	O
a	O
simpler	O
parametric	O
form	O
for	O
example	O
one	O
might	O
write	O
a	O
decomposition	B
in	O
which	O
only	O
a	O
limited	O
number	O
of	O
parental	O
interactions	O
are	O
required	O
is	O
called	O
divorcing	B
parents	B
in	O
for	O
example	O
in	O
assuming	O
all	O
variables	O
are	O
binary	O
the	O
number	O
of	O
states	O
requiring	O
specification	O
is	O
compared	O
to	O
the	O
states	O
in	O
the	O
unconstrained	O
case	O
the	O
distribution	B
can	O
be	O
stored	O
using	O
only	O
independent	O
parameters	O
draft	O
march	O
logic	O
gates	O
exercises	O
another	O
technique	O
to	O
constrain	O
cpts	O
uses	O
simple	O
classes	O
of	O
conditional	B
tables	O
for	O
example	O
in	O
one	O
could	O
use	O
a	O
logical	O
or	O
gate	O
on	O
binary	O
zi	O
say	O
if	O
at	O
least	O
one	O
of	O
the	O
zi	O
is	O
in	O
state	O
otherwise	O
we	O
can	O
then	O
make	O
a	O
cpt	O
by	O
including	O
the	O
additional	O
terms	O
pzi	O
when	O
each	O
xi	O
is	O
binary	O
there	O
are	O
in	O
total	O
only	O
quantities	O
required	O
for	O
specifying	O
pyx	O
in	O
this	O
case	O
can	O
be	O
used	O
to	O
represent	O
any	O
noisy	B
logic	I
gate	I
such	O
as	O
the	O
noisy	O
or	O
or	O
noisy	O
and	O
where	O
the	O
number	O
of	O
parameters	O
required	O
to	O
specify	O
the	O
noisy	O
gate	O
is	O
linear	B
in	O
the	O
number	O
of	O
parents	B
x	O
the	O
noisy-or	O
is	O
particularly	O
common	O
in	O
disease-symptom	O
networks	O
in	O
which	O
many	O
diseases	O
x	O
can	O
give	O
rise	O
to	O
the	O
same	O
symptom	O
y	O
provided	O
that	O
at	O
least	O
one	O
of	O
the	O
diseases	O
is	O
present	O
the	O
probability	O
that	O
the	O
symptom	O
will	O
be	O
present	O
is	O
high	O
further	O
reading	O
an	O
introduction	O
to	O
bayesian	B
networks	O
and	O
graphical	O
models	O
in	O
expert	O
systems	O
is	O
to	O
be	O
found	O
in	O
which	O
also	O
discusses	O
general	O
inference	B
techniques	O
which	O
will	O
be	O
discussed	O
during	O
later	O
chapters	O
code	O
naive	O
inference	B
demo	O
demoburglar	O
m	O
was	O
it	O
the	O
burglar	O
demo	O
demochestclinic	O
m	O
naive	O
inference	B
on	O
chest	B
clinic	I
conditional	B
independence	B
demo	O
the	O
following	O
demo	O
determines	O
whether	O
x	O
y	O
z	O
for	O
the	O
chest	B
clinic	I
network	O
and	O
checks	O
the	O
result	O
the	O
independence	B
test	O
is	O
based	O
on	O
the	O
markov	O
method	O
of	O
this	O
is	O
preferred	O
over	O
the	O
d-separation	O
method	O
since	O
it	O
is	O
arguably	O
simpler	O
to	O
code	O
and	O
also	O
more	O
general	O
in	O
that	O
it	O
deals	O
also	O
with	O
conditional	B
independence	B
in	O
markov	O
networks	O
as	O
well	O
as	O
belief	B
networks	I
running	O
the	O
demo	O
code	O
below	O
it	O
may	O
happen	O
that	O
the	O
numerical	B
dependence	O
is	O
very	O
low	O
that	O
is	O
px	O
pxzpyz	O
even	O
though	O
this	O
highlights	O
the	O
difference	O
between	O
structural	O
and	O
numerical	B
independence	B
condindeppot	O
m	O
numerical	B
measure	O
of	O
conditional	B
independence	B
democondindep	O
m	O
demo	O
of	O
conditional	B
independence	B
markov	O
method	O
utility	B
routines	O
dag	O
m	O
find	O
the	O
dag	O
structure	B
for	O
a	O
belief	B
network	I
exercises	O
exercise	O
animal	O
the	O
party	O
animal	O
problem	B
corresponds	O
to	O
the	O
network	O
in	O
the	O
boss	O
is	O
angry	O
and	O
the	O
worker	O
has	O
a	O
headache	O
what	O
is	O
the	O
probability	O
the	O
worker	O
has	O
been	O
to	O
a	O
party	O
to	O
code	O
for	O
conditional	B
independence	B
is	O
given	O
in	O
draft	O
march	O
exercises	O
d	O
u	O
h	O
p	O
a	O
figure	O
party	O
animal	O
here	O
all	O
variables	O
are	O
binary	O
when	O
set	O
to	O
the	O
statements	O
are	O
true	O
p	O
been	O
to	O
party	O
h	O
got	O
a	O
headache	O
d	O
demotivated	O
at	O
work	O
u	O
underperform	O
at	O
work	O
a	O
angry	O
shaded	O
variables	O
are	O
observed	O
in	O
the	O
true	O
state	O
a	O
t	O
x	O
l	O
e	O
b	O
s	O
d	O
x	O
positive	O
x-ray	O
d	O
dyspnea	O
of	O
breath	O
e	O
either	O
tuberculosis	O
or	O
lung	O
cancer	O
t	O
tuberculosis	O
l	O
lung	O
cancer	O
b	O
bronchitis	O
a	O
visited	O
asia	O
s	O
smoker	O
figure	O
belief	B
network	I
structure	B
for	O
the	O
chest	B
clinic	I
example	O
complete	O
the	O
specifications	O
the	O
probabilities	O
are	O
given	O
as	O
follows	O
pu	O
trp	O
tr	O
d	O
tr	O
pu	O
trp	O
fa	O
d	O
tr	O
pu	O
trp	O
fa	O
d	O
fa	O
pu	O
trp	O
tr	O
d	O
fa	O
exercise	O
consider	O
the	O
distribution	B
pa	O
b	O
c	O
pca	B
bpapb	O
is	O
a	O
b	O
is	O
a	O
b	O
c	O
exercise	O
the	O
chest	B
clinic	I
network	O
concerns	O
the	O
diagnosis	O
of	O
lung	O
disease	O
lung	O
cancer	O
or	O
both	O
or	O
neither	O
in	O
this	O
model	B
a	O
visit	O
to	O
asia	O
is	O
assumed	O
to	O
increase	O
the	O
probability	O
of	O
tuberculosis	O
state	O
if	O
the	O
following	O
conditional	B
independence	B
relationships	O
are	O
true	O
or	O
false	O
tuberculosis	O
smoking	O
shortness	O
of	O
breath	O
lung	O
cancer	O
bronchitis	O
smoking	O
visit	O
to	O
asia	O
smoking	O
lung	O
cancer	O
visit	O
to	O
asia	O
smoking	O
lung	O
cancer	O
shortness	O
of	O
breath	O
exercise	O
consider	O
the	O
network	O
in	O
which	O
concerns	O
the	O
probability	O
of	O
a	O
car	O
starting	O
pf	O
empty	O
pb	O
bad	O
pg	O
emptyb	O
good	O
f	O
not	O
empty	O
pg	O
emptyb	O
good	O
f	O
empty	O
pg	O
emptyb	O
bad	O
f	O
not	O
empty	O
pg	O
emptyb	O
bad	O
f	O
empty	O
pt	O
fab	O
good	O
pt	O
fab	O
bad	O
ps	O
fat	O
tr	O
f	O
empty	O
ps	O
fat	O
tr	O
f	O
not	O
empty	O
ps	O
fat	O
fa	O
f	O
not	O
empty	O
ps	O
fat	O
fa	O
f	O
empty	O
calculate	O
p	O
emptys	O
no	O
the	O
probability	O
of	O
the	O
fuel	O
tank	O
being	O
empty	O
conditioned	O
on	O
the	O
observation	O
that	O
the	O
car	O
does	O
not	O
start	O
exercise	O
consider	O
the	O
chest	B
clinic	I
bayesian	B
network	O
in	O
calculate	O
by	O
hand	O
the	O
values	O
for	O
pd	O
pds	O
tr	O
pds	O
fa	O
the	O
table	O
values	O
are	O
pa	O
tr	O
pt	O
tra	O
tr	O
pl	O
trs	O
tr	O
pb	O
trs	O
tr	O
px	O
tre	O
tr	O
pd	O
tre	O
tr	O
b	O
tr	O
pd	O
tre	O
fa	O
b	O
tr	O
ps	O
tr	O
pt	O
tra	O
fa	O
pl	O
trs	O
fa	O
pb	O
trs	O
fa	O
px	O
tre	O
fa	O
pd	O
tre	O
tr	O
b	O
fa	O
pd	O
tre	O
fa	O
b	O
fa	O
pe	O
trt	O
l	O
only	O
if	O
both	O
t	O
and	O
l	O
are	O
fa	O
otherwise	O
draft	O
march	O
exercises	O
battery	O
fuel	O
gauge	O
figure	O
belief	B
network	I
of	O
car	O
not	O
see	O
turn	O
over	O
start	O
exercise	O
if	O
we	O
interpret	O
the	O
chest	B
clinic	I
network	O
causally	O
how	O
can	O
we	O
help	O
a	O
doctor	O
answer	O
the	O
question	O
if	O
i	O
could	O
cure	O
my	O
patients	O
of	O
bronchitis	O
how	O
would	O
this	O
affect	O
my	O
patients	O
s	O
chance	O
of	O
being	O
short	O
of	O
breath	O
how	O
does	O
this	O
compare	O
with	O
pd	O
trb	O
fa	O
in	O
a	O
non-causal	O
interpretation	O
and	O
what	O
does	O
this	O
mean	B
exercise	O
there	O
is	O
a	O
synergistic	O
relationship	O
between	O
asbestos	O
exposure	O
smoking	O
and	O
cancer	O
a	O
model	B
describing	O
this	O
relationship	O
is	O
given	O
by	O
pa	O
s	O
c	O
pca	B
spaps	O
is	O
a	O
s	O
is	O
a	O
s	O
c	O
how	O
could	O
you	O
adjust	O
the	O
model	B
to	O
account	O
for	O
the	O
fact	O
that	O
people	O
who	O
work	O
in	O
the	O
building	O
industry	O
have	O
a	O
higher	O
likelihood	B
to	O
also	O
be	O
smokers	O
and	O
also	O
a	O
higher	O
likelihood	B
to	O
asbestos	O
exposure	O
exercise	O
consider	O
the	O
three	O
variable	O
distribution	B
pa	O
b	O
c	O
pabpbcpc	O
where	O
all	O
variables	O
are	O
binary	O
how	O
many	O
parameters	O
are	O
needed	O
to	O
specify	O
distributions	O
of	O
this	O
form	O
exercise	O
consider	O
the	O
belief	B
network	I
on	O
the	O
right	O
which	O
represents	O
mr	O
holmes	O
burglary	O
worries	O
as	O
given	O
in	O
mrs	O
all	O
variables	O
take	O
the	O
two	O
states	O
fa	O
the	O
table	O
entries	O
are	O
pb	O
tr	O
pa	O
trb	O
tr	O
pa	O
trb	O
fa	O
pw	O
tra	O
tr	O
pw	O
tra	O
fa	O
pg	O
tra	O
tr	O
pg	O
tra	O
fa	O
compute	O
by	O
hand	O
show	O
your	O
working	O
pb	O
trw	O
tr	O
pb	O
trw	O
tr	O
g	O
fa	O
b	O
a	O
g	O
w	O
consider	O
the	O
same	O
situation	O
as	O
above	O
except	O
that	O
now	O
the	O
evidence	O
is	O
uncertain	B
mrs	O
gibbon	O
thinks	O
that	O
the	O
state	O
is	O
g	O
fa	O
with	O
probability	O
similarly	O
dr	O
watson	O
believes	O
in	O
the	O
state	O
w	O
fa	O
with	O
value	B
compute	O
by	O
hand	O
the	O
posteriors	O
under	O
these	O
uncertain	B
evidences	O
pb	O
tr	O
w	O
pb	O
tr	O
w	O
g	O
exercise	O
a	O
doctor	O
gives	O
a	O
patient	O
a	O
or	O
no	O
drug	O
dependent	O
on	O
their	O
or	O
young	O
and	O
or	O
female	O
whether	O
or	O
not	O
the	O
patient	O
or	O
doesn	O
t	O
recover	O
depends	O
on	O
all	O
d	O
a	O
g	O
in	O
addition	O
a	O
g	O
write	O
down	O
the	O
belief	B
network	I
for	O
the	O
above	O
situation	O
draft	O
march	O
exercises	O
explain	O
how	O
to	O
compute	O
precoverdrug	O
explain	O
how	O
to	O
compute	O
precoverdodrug	O
young	O
exercise	O
implement	O
the	O
wet	O
grass	O
scenario	O
numerically	O
using	O
the	O
brmltoolbox	O
exercise	O
burglar	O
consider	O
the	O
burglar	O
scenario	O
we	O
now	O
wish	O
to	O
model	B
the	O
fact	O
that	O
in	O
los	O
angeles	O
the	O
probability	O
of	O
being	O
burgled	O
increases	O
if	O
there	O
is	O
an	O
earthquake	O
explain	O
how	O
to	O
include	O
this	O
effect	O
in	O
the	O
model	B
exercise	O
given	O
two	O
belief	B
networks	I
represented	O
as	O
dags	O
with	O
associated	O
adjacency	B
matrices	O
a	O
and	O
b	O
write	O
a	O
matlab	O
function	B
markovequivab	O
m	O
that	O
returns	O
if	O
a	O
and	O
b	O
are	O
markov	B
equivalent	B
and	O
zero	O
otherwise	O
exercise	O
the	O
adjacency	B
matrices	O
of	O
two	O
belief	B
networks	I
are	O
given	O
below	O
abmatrices	O
mat	O
state	O
if	O
they	O
are	O
markov	B
equivalent	B
a	O
b	O
exercise	O
there	O
are	O
three	O
computers	O
indexed	O
by	O
i	O
computer	O
i	O
can	O
send	O
a	O
message	B
in	O
one	O
timestep	O
to	O
computer	O
j	O
if	O
cij	O
otherwise	O
cij	O
there	O
is	O
a	O
fault	O
in	O
the	O
network	O
and	O
the	O
task	O
is	O
to	O
find	O
out	O
some	O
information	O
about	O
the	O
communication	O
matrix	B
c	O
is	O
not	O
necessarily	O
symmetric	O
to	O
do	O
this	O
thomas	O
the	O
engineer	O
will	O
run	O
some	O
tests	O
that	O
reveal	O
whether	O
or	O
not	O
computer	O
i	O
can	O
send	O
a	O
message	B
to	O
computer	O
j	O
in	O
t	O
timesteps	O
t	O
this	O
is	O
expressed	O
as	O
cijt	O
with	O
cij	O
for	O
example	O
he	O
might	O
know	O
that	O
meaning	O
that	O
according	O
to	O
his	O
test	O
a	O
message	B
sent	O
from	O
computer	O
will	O
arrive	O
at	O
computer	O
in	O
at	O
most	O
timesteps	O
note	O
that	O
this	O
message	B
could	O
go	O
via	O
different	O
routes	O
it	O
might	O
go	O
directly	O
from	O
to	O
in	O
one	O
timestep	O
or	O
indirectly	O
from	O
to	O
and	O
then	O
from	O
to	O
or	O
both	O
you	O
may	O
assume	O
cii	O
a	O
priori	O
thomas	O
thinks	O
there	O
is	O
a	O
probability	O
that	O
cij	O
given	O
the	O
test	O
information	O
c	O
compute	O
the	O
a	O
posteriori	O
probability	O
vector	O
exercise	O
a	O
belief	B
network	I
models	O
the	O
relation	O
between	O
the	O
variables	O
oil	O
inf	O
eh	O
bp	O
rt	O
which	O
stand	O
for	O
the	O
price	O
of	O
oil	O
inflation	O
rate	O
economy	O
health	O
british	O
petroleum	O
stock	O
price	O
retailer	O
stock	O
price	O
each	O
variable	O
takes	O
the	O
states	O
low	O
high	O
except	O
for	O
bp	O
which	O
has	O
states	O
low	O
high	O
normal	B
the	O
belief	B
network	I
model	B
for	O
these	O
variables	O
has	O
tables	O
draw	O
a	O
belief	B
network	I
for	O
this	O
distribution	B
given	O
that	O
bp	O
stock	O
price	O
is	O
normal	B
and	O
the	O
retailer	O
stock	O
price	O
is	O
high	O
what	O
is	O
the	O
probability	O
that	O
inflation	O
is	O
high	O
exercise	O
there	O
are	O
a	O
set	O
of	O
c	O
potentials	O
with	O
potential	B
c	O
defined	O
on	O
a	O
subset	O
of	O
variables	O
xc	O
if	O
xc	O
xd	O
then	O
can	O
merge	O
potentials	O
c	O
and	O
d	O
since	O
c	O
is	O
contained	O
within	O
d	O
with	O
reference	O
to	O
suitable	O
graph	B
structures	O
describe	O
an	O
efficient	O
algorithm	B
to	O
merge	O
a	O
set	O
of	O
potentials	O
so	O
that	O
for	O
the	O
new	O
set	O
of	O
potentials	O
no	O
potential	B
is	O
contained	O
within	O
the	O
other	O
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
graphical	O
models	O
graphical	O
models	O
graphical	O
models	O
are	O
depictions	O
of	O
independencedependence	O
relationships	O
for	O
distributions	O
each	O
form	O
of	O
gm	O
is	O
a	O
particular	O
union	O
of	O
graph	B
and	O
probability	O
constructs	O
and	O
details	O
the	O
form	O
of	O
independence	B
assumptions	O
represented	O
gms	O
are	O
useful	O
since	O
they	O
provide	O
a	O
framework	O
for	O
studying	O
a	O
wide	O
class	O
of	O
probabilistic	B
models	O
and	O
associated	O
algorithms	O
in	O
particular	O
they	O
help	O
to	O
clarify	O
modelling	B
assumptions	O
and	O
provide	O
a	O
unified	O
framework	O
under	O
which	O
inference	B
algorithms	O
in	O
different	O
communities	O
can	O
be	O
related	O
it	O
needs	O
to	O
be	O
emphasised	O
that	O
all	O
forms	O
of	O
gm	O
have	O
a	O
limited	O
ability	O
to	O
graphically	O
expresses	O
conditional	B
as	O
we	O
ve	O
seen	O
belief	B
networks	I
are	O
useful	O
for	O
modelling	B
ancestral	B
conditional	B
independence	B
in	O
this	O
chapter	O
we	O
ll	O
introduce	O
other	O
types	O
of	O
gm	O
that	O
are	O
more	O
suited	O
to	O
representing	O
different	O
assumptions	O
markov	O
networks	O
for	O
example	O
are	O
particularly	O
suited	O
to	O
modelling	B
marginal	B
dependence	O
and	O
conditional	B
independence	B
here	O
we	O
ll	O
focus	O
on	O
markov	O
networks	O
chain	B
graphs	O
marry	O
belief	O
and	O
markov	O
networks	O
and	O
factor	B
graphs	O
there	O
are	O
many	O
more	O
inhabitants	O
of	O
the	O
zoo	O
of	O
graphical	O
models	O
see	O
the	O
general	O
viewpoint	O
we	O
adopt	O
is	O
to	O
describe	O
the	O
problem	B
environment	O
using	O
a	O
probabilistic	B
model	B
after	O
which	O
reasoning	O
corresponds	O
to	O
performing	O
probabilistic	B
inference	B
this	O
is	O
therefore	O
a	O
two	O
part	O
process	O
modelling	B
after	O
identifying	O
all	O
potentially	O
relevant	O
variables	O
of	O
a	O
problem	B
environment	O
our	O
task	O
is	O
to	O
describe	O
how	O
these	O
variables	O
can	O
interact	O
this	O
is	O
achieved	O
using	O
structural	O
assumptions	O
as	O
to	O
the	O
form	O
of	O
the	O
joint	B
probability	O
distribution	B
of	O
all	O
the	O
variables	O
typically	O
corresponding	O
to	O
assumptions	O
of	O
independence	B
of	O
variables	O
each	O
class	O
of	O
graphical	O
model	B
corresponds	O
to	O
a	O
factorisation	O
property	O
of	O
the	O
joint	B
distribution	B
inference	B
once	O
the	O
basic	O
assumptions	O
as	O
to	O
how	O
variables	O
interact	O
with	O
each	O
other	O
is	O
formed	O
the	O
probabilistic	B
model	B
is	O
constructed	O
all	O
questions	O
of	O
interest	O
are	O
answered	O
by	O
performing	O
inference	B
on	O
the	O
distribution	B
this	O
can	O
be	O
a	O
computationally	O
non-trivial	O
step	O
so	O
that	O
coupling	O
gms	O
with	O
accurate	O
inference	B
algorithms	O
is	O
central	O
to	O
successful	O
graphical	O
modelling	B
whilst	O
not	O
a	O
strict	O
separation	B
gms	O
tend	O
to	O
fall	O
into	O
two	O
broad	O
classes	O
those	O
useful	O
in	O
modelling	B
and	O
those	O
useful	O
in	O
representing	O
inference	B
algorithms	O
for	O
modelling	B
belief	B
networks	I
markov	O
networks	O
chain	B
graphs	O
and	O
influence	O
diagrams	O
are	O
some	O
of	O
the	O
most	O
popular	O
for	O
inference	B
one	O
typically	O
compiles	O
a	O
model	B
into	O
a	O
suitable	O
gm	O
for	O
which	O
an	O
algorithm	B
can	O
be	O
readily	O
applied	O
such	O
inference	B
gms	O
include	O
factor	B
graphs	O
junction	O
trees	O
and	O
region	B
graphs	I
markov	O
networks	O
markov	O
networks	O
figure	O
pa	O
pb	O
pc	O
belief	B
networks	I
correspond	O
to	O
a	O
special	O
kind	O
of	O
factorisation	O
of	O
the	O
joint	B
probability	O
distribution	B
in	O
which	O
each	O
of	O
the	O
factors	O
is	O
itself	O
a	O
distribution	B
an	O
alternative	O
factorisation	O
is	O
for	O
example	O
and	O
z	O
is	O
a	O
constant	O
which	O
ensures	O
normalisation	B
called	O
the	O
pa	O
b	O
c	O
z	O
b	O
c	O
where	O
b	O
and	O
c	O
are	O
potentials	O
partition	B
function	B
z	O
b	O
c	O
abc	O
we	O
will	O
typically	O
use	O
the	O
convention	O
that	O
the	O
ordering	O
of	O
the	O
variables	O
in	O
the	O
potential	B
is	O
not	O
relevant	O
for	O
a	O
distribution	B
the	O
joint	B
variables	O
simply	O
index	O
an	O
element	O
of	O
the	O
potential	B
table	O
markov	O
networks	O
are	O
defined	O
as	O
products	O
of	O
non-negative	O
functions	O
defined	O
on	O
maximal	O
cliques	O
of	O
an	O
undirected	B
graph	B
see	O
case	O
of	O
a	O
potential	B
satisfying	O
normalisation	B
definition	O
a	O
potential	B
is	O
a	O
non-negative	O
function	B
of	O
the	O
variable	O
x	O
a	O
joint	B
potential	B
xn	O
is	O
a	O
non-negative	O
function	B
of	O
the	O
set	O
of	O
variables	O
a	O
distribution	B
is	O
a	O
special	O
x	O
this	O
holds	O
similarly	O
for	O
continuous	B
variables	O
with	O
summation	O
replaced	O
by	O
integration	O
xn	O
z	O
definition	O
network	O
for	O
a	O
set	O
of	O
variables	O
x	O
xn	O
a	O
markov	B
network	I
is	O
defined	O
as	O
a	O
product	O
of	O
potentials	O
on	O
subsets	O
of	O
the	O
variables	O
xc	O
x	O
cxc	O
graphically	O
this	O
is	O
represented	O
by	O
an	O
undirected	B
graph	B
g	O
with	O
xc	O
c	O
c	O
being	O
the	O
maximal	O
cliques	O
of	O
g	O
the	O
constant	O
z	O
ensures	O
the	O
distribution	B
is	O
normalised	O
the	O
graph	B
is	O
said	O
to	O
satisfy	O
the	O
factorisation	O
property	O
in	O
the	O
special	O
case	O
that	O
the	O
graph	B
contains	O
cliques	O
of	O
only	O
size	O
the	O
distribution	B
is	O
called	O
a	O
pairwise	B
markov	B
network	I
with	O
potentials	O
defined	O
on	O
each	O
link	O
between	O
two	O
variables	O
for	O
the	O
case	O
in	O
which	O
clique	B
potentials	O
are	O
strictly	O
positive	O
this	O
is	O
called	O
a	O
gibbs	B
distribution	B
remark	O
markov	B
network	I
whilst	O
a	O
markov	B
network	I
is	O
formally	O
defined	O
on	O
maximal	O
cliques	O
in	O
practice	O
authors	O
often	O
use	O
the	O
term	O
to	O
refer	O
to	O
non-maximal	O
cliques	O
for	O
example	O
in	O
the	O
graph	B
on	O
the	O
right	O
the	O
maximal	O
cliques	O
are	O
and	O
so	O
that	O
the	O
graph	B
describes	O
a	O
distribution	B
in	O
a	O
pairwise	B
network	O
though	O
the	O
poten	O
tials	O
are	O
assumed	O
to	O
be	O
over	O
two-cliques	O
giving	O
draft	O
march	O
markov	O
networks	O
definition	O
of	O
markov	O
networks	O
b	O
b	O
b	O
a	O
a	O
a	O
c	O
c	O
c	O
pa	O
b	O
c	O
aca	O
c	O
bcb	O
cz	O
a	O
and	O
b	O
are	O
unconditionally	O
dependent	O
pa	O
b	O
papb	O
a	O
and	O
b	O
are	O
conditionally	O
independent	O
on	O
c	O
pa	O
bc	O
pacpbc	O
a	O
marginalising	O
over	O
c	O
makes	O
a	O
and	O
b	O
dependent	O
b	O
a	O
b	O
conditioning	B
on	O
c	O
makes	O
a	O
and	O
b	O
independent	O
markov	O
properties	B
we	O
here	O
state	O
some	O
of	O
the	O
most	O
useful	O
results	O
the	O
reader	O
is	O
referred	O
to	O
for	O
proofs	O
and	O
more	O
detailed	O
discussion	O
consider	O
the	O
markov	B
network	I
in	O
here	O
we	O
use	O
the	O
shorthand	O
etc	O
we	O
will	O
use	O
this	O
undirected	B
graph	B
to	O
demonstrate	O
conditional	B
independence	B
properties	B
local	B
markov	O
property	O
definition	O
markov	O
property	O
pxxx	O
pxne	O
when	O
conditioned	O
on	O
its	O
neighbours	O
x	O
is	O
independent	O
of	O
the	O
remaining	O
variables	O
of	O
the	O
graph	B
the	O
conditional	B
distribution	B
is	O
the	O
last	O
line	O
above	O
follows	O
since	O
the	O
variable	O
only	O
appears	O
in	O
the	O
cliques	O
that	O
border	O
the	O
generalisation	B
of	O
the	O
above	O
example	O
is	O
clear	O
a	O
mn	O
with	O
positive	O
clique	B
potentials	O
defined	O
with	O
respect	O
to	O
an	O
undirected	B
graph	B
g	O
pxixi	O
pxine	O
pairwise	B
markov	O
property	O
definition	O
markov	O
property	O
for	O
any	O
non-adjacent	O
vertices	O
x	O
and	O
y	O
x	O
yxx	O
y	O
notation	O
xi	O
is	O
shorthand	O
for	O
the	O
set	O
of	O
all	O
variables	O
x	O
excluding	O
variable	O
xi	O
namely	O
xxi	O
in	O
set	O
notation	O
draft	O
march	O
markov	O
networks	O
figure	O
by	O
the	O
global	B
markov	O
property	O
since	O
every	O
path	B
from	O
to	O
passes	O
through	O
then	O
where	O
the	O
last	O
line	O
follows	O
since	O
for	O
fixed	O
the	O
function	B
is	O
a	O
product	O
of	O
a	O
function	B
on	O
and	O
a	O
function	B
on	O
implying	O
independence	B
global	B
markov	O
property	O
definition	O
a	O
subset	O
s	O
separates	O
a	O
subset	O
a	O
from	O
a	O
subset	O
b	O
if	O
every	O
path	B
from	O
any	O
member	O
of	O
a	O
to	O
any	O
member	O
of	O
b	O
passes	O
though	O
s	O
definition	O
markov	O
property	O
for	O
a	O
disjoint	O
subset	O
of	O
variables	O
where	O
s	O
separates	O
a	O
from	O
b	O
in	O
g	O
then	O
a	O
bs	O
this	O
implies	O
that	O
example	O
machine	O
a	O
boltzmann	B
machine	I
is	O
a	O
mn	O
on	O
binary	O
variables	O
domxi	O
of	O
the	O
form	O
i	O
bixi	O
ij	O
wij	O
xixj	O
px	O
zw	O
b	O
e	O
where	O
the	O
interactions	O
wij	O
are	O
the	O
weights	O
and	O
the	O
bi	O
the	O
biases	O
this	O
model	B
has	O
been	O
studied	O
in	O
the	O
machine	O
learning	B
community	O
as	O
a	O
basic	O
model	B
of	O
distributed	O
memory	O
and	O
the	O
graphical	O
model	B
of	O
the	O
bm	O
is	O
an	O
undirected	B
graph	B
with	O
a	O
link	O
been	O
nodes	O
i	O
and	O
j	O
for	O
wij	O
consequently	O
for	O
all	O
but	O
specially	O
constrained	O
w	O
the	O
graph	B
is	O
multiply-connected	B
and	O
inference	B
will	O
be	O
typically	O
intractable	O
gibbs	B
networks	O
for	O
simplicity	O
we	O
assume	O
that	O
the	O
potentials	O
are	O
strictly	O
positive	O
in	O
which	O
case	O
mns	O
are	O
also	O
termed	O
gibbs	B
networks	O
in	O
this	O
case	O
a	O
gn	O
satisfies	O
the	O
following	O
independence	B
relations	O
draft	O
march	O
markov	O
networks	O
figure	O
local	B
distributions	O
the	O
markov	B
network	I
consistent	B
with	O
the	O
local	B
distributions	O
if	O
the	O
local	B
distributions	O
are	O
positive	O
by	O
the	O
hammersley-clifford	O
theorem	B
the	O
only	O
joint	B
distribution	B
that	O
can	O
be	O
consistent	B
with	O
the	O
local	B
distributions	O
must	O
be	O
a	O
gibbs	B
distribution	B
with	O
structure	B
given	O
by	O
markov	O
random	O
fields	O
definition	O
random	B
field	I
a	O
mrf	O
is	O
defined	O
by	O
a	O
set	O
of	O
distributions	O
pxine	O
where	O
i	O
n	O
indexes	O
the	O
distributions	O
and	O
ne	O
are	O
the	O
neighbours	O
of	O
variable	O
xi	O
namely	O
that	O
subset	O
of	O
the	O
variables	O
xn	O
that	O
the	O
distribution	B
of	O
variable	O
xi	O
depends	O
on	O
the	O
term	O
markov	O
indicates	O
that	O
this	O
is	O
a	O
proper	O
subset	O
of	O
the	O
variables	O
a	O
distribution	B
is	O
an	O
mrf	O
with	O
respect	O
to	O
an	O
undirected	B
graph	B
g	O
if	O
pxixi	O
pxine	O
where	O
ne	O
are	O
the	O
neighbouring	O
variables	O
of	O
variable	O
xi	O
according	O
to	O
the	O
undirected	B
graph	B
g	O
hammersley	B
clifford	I
theorem	B
the	O
hammersley-clifford	O
theorem	B
helps	O
resolve	O
questions	O
as	O
to	O
when	O
a	O
set	O
of	O
positive	O
local	B
distributions	O
pxine	O
could	O
ever	O
form	O
a	O
consistent	B
joint	B
distribution	B
xn	O
local	B
distributions	O
pxine	O
can	O
form	O
a	O
consistent	B
joint	B
distribution	B
if	O
and	O
only	O
if	O
xn	O
factorises	O
according	O
to	O
xn	O
exp	O
vcxc	O
z	O
indexed	O
by	O
c	O
equation	B
is	O
equivalent	B
c	O
where	O
the	O
sum	O
is	O
over	O
all	O
cliques	O
and	O
vcxc	O
is	O
a	O
real	O
function	B
defined	O
over	O
the	O
variables	O
in	O
the	O
clique	B
c	O
namely	O
a	O
mn	O
on	O
positive	O
clique	B
potentials	O
the	O
graph	B
over	O
which	O
the	O
cliques	O
are	O
defined	O
is	O
an	O
undirected	B
graph	B
with	O
a	O
link	O
between	O
xi	O
and	O
xj	O
if	O
pxixi	O
pxixij	O
that	O
is	O
if	O
xj	O
has	O
an	O
effect	O
on	O
the	O
conditional	B
distribution	B
of	O
xi	O
then	O
add	O
an	O
undirected	B
link	O
between	O
xi	O
and	O
xj	O
this	O
is	O
then	O
repeated	O
over	O
all	O
the	O
variables	O
see	O
note	O
that	O
the	O
hc	O
theorem	B
does	O
not	O
mean	B
that	O
given	O
a	O
set	O
of	O
conditional	B
distributions	O
we	O
can	O
always	O
form	O
a	O
consistent	B
joint	B
distribution	B
from	O
them	O
rather	O
it	O
states	O
what	O
the	O
functional	O
form	O
of	O
a	O
joint	B
distribution	B
for	O
the	O
conditionals	O
to	O
be	O
consistent	B
with	O
the	O
joint	B
see	O
conditional	B
independence	B
using	O
markov	O
networks	O
for	O
x	O
each	O
being	O
collections	O
of	O
variables	O
in	O
we	O
discussed	O
an	O
algorithm	B
to	O
determine	O
x	O
yz	O
an	O
alternative	O
and	O
more	O
general	O
method	O
it	O
handles	O
directed	B
and	O
undirected	B
graphs	O
uses	O
the	O
following	O
steps	O
draft	O
march	O
a	O
c	O
e	O
h	O
i	O
j	O
b	O
d	O
f	O
k	O
g	O
b	O
d	O
f	O
a	O
c	O
e	O
i	O
markov	O
networks	O
figure	O
belief	B
network	I
for	O
which	O
we	O
are	O
interested	O
in	O
checking	O
conditional	B
independence	B
a	O
b	O
i	O
ancestral	B
moralised	O
graph	B
for	O
a	O
b	O
i	O
every	O
path	B
from	O
a	O
red	O
to	O
green	O
node	O
passes	O
through	O
a	O
yellow	O
node	O
so	O
a	O
and	O
b	O
are	O
independent	O
given	O
d	O
i	O
alternatively	O
if	O
we	O
consider	O
a	O
b	O
i	O
the	O
variable	O
d	O
is	O
uncoloured	O
and	O
we	O
can	O
travel	O
from	O
the	O
red	O
to	O
the	O
green	O
without	O
encountering	O
in	O
this	O
case	O
a	O
is	O
a	O
yellow	O
node	O
the	O
e	O
f	O
path	B
dependent	O
on	O
b	O
conditioned	O
on	O
i	O
ancestral	B
graph	B
remove	O
from	O
the	O
dag	O
any	O
node	O
which	O
is	O
neither	O
in	O
x	O
y	O
z	O
nor	O
an	O
ancestor	B
of	O
a	O
node	O
in	O
this	O
set	O
together	O
with	O
any	O
edges	O
in	O
or	O
out	O
of	O
such	O
nodes	O
moralisation	B
add	O
a	O
line	O
between	O
any	O
two	O
remaining	O
nodes	O
which	O
have	O
a	O
common	O
child	O
but	O
are	O
not	O
already	O
connected	B
by	O
an	O
arrow	O
then	O
remove	O
remaining	O
arrowheads	O
separation	B
in	O
the	O
undirected	B
graph	B
so	O
constructed	O
look	O
for	O
a	O
path	B
which	O
joins	O
a	O
node	O
in	O
x	O
to	O
one	O
in	O
y	O
but	O
does	O
not	O
intersect	O
z	O
if	O
there	O
is	O
no	O
such	O
path	B
deduce	O
that	O
x	O
yz	O
for	O
markov	O
networks	O
only	O
the	O
final	O
separation	B
criterion	O
needs	O
to	O
be	O
applied	O
see	O
for	O
an	O
example	O
lattice	O
models	O
undirected	B
models	O
have	O
a	O
long	O
history	O
in	O
different	O
branches	O
of	O
science	O
especially	O
statistical	O
mechanics	O
on	O
lattices	O
and	O
more	O
recently	O
as	O
models	O
in	O
visual	O
processing	O
in	O
which	O
the	O
models	O
encourage	O
neighbouring	O
variables	O
to	O
be	O
in	O
the	O
same	O
consider	O
a	O
model	B
in	O
which	O
our	O
desire	O
is	O
that	O
states	O
of	O
the	O
binary	O
valued	O
variables	O
arranged	O
on	O
a	O
lattice	O
should	O
prefer	O
their	O
neighbouring	O
variables	O
to	O
be	O
in	O
the	O
same	O
state	O
i	O
j	O
z	O
ijxi	O
xj	O
where	O
i	O
j	O
denotes	O
the	O
set	O
of	O
indices	O
where	O
i	O
and	O
j	O
are	O
neighbours	O
in	O
the	O
undirected	B
graph	B
the	O
ising	B
model	B
a	O
set	O
of	O
potentials	O
for	O
equation	B
that	O
encourages	O
neighbouring	O
variables	O
to	O
have	O
the	O
same	O
state	O
is	O
ijxi	O
xj	O
e	O
xj	O
this	O
corresponds	O
to	O
a	O
well-known	O
model	B
of	O
the	O
physics	O
of	O
magnetic	O
systems	O
called	O
the	O
ising	B
model	B
which	O
consists	O
of	O
mini-magnets	O
which	O
prefer	O
to	O
be	O
aligned	O
in	O
the	O
same	O
state	O
depending	O
on	O
the	O
temperature	O
figure	O
onsagar	O
magnetisation	O
as	O
the	O
temperature	O
t	O
decreases	O
towards	O
the	O
critical	O
temperature	O
tc	O
a	O
phase	O
transition	O
occurs	O
in	O
which	O
a	O
large	O
fraction	O
of	O
the	O
variables	O
become	O
aligned	O
in	O
the	O
same	O
state	O
draft	O
march	O
chain	B
graphical	O
models	O
a	O
c	O
b	O
d	O
a	O
b	O
a	O
cd	O
b	O
e	O
g	O
d	O
f	O
c	O
c	O
aedf	O
bg	O
figure	O
chain	B
graphs	O
the	O
chain	B
components	O
are	O
identified	O
by	O
deleting	O
the	O
directed	B
edges	O
and	O
idena	O
chain	B
components	O
are	O
d	O
which	O
can	O
be	O
tifying	O
the	O
remaining	O
connected	B
components	I
written	O
as	O
a	O
bn	O
on	O
the	O
cluster	O
variables	O
in	O
chain	B
components	O
are	O
e	O
d	O
f	O
g	O
which	O
has	O
the	O
cluster	O
bn	O
representation	O
t	O
for	O
high	O
t	O
the	O
variables	O
behave	O
independently	O
so	O
that	O
no	O
global	B
magnetisation	O
appears	O
for	O
low	O
t	O
there	O
is	O
a	O
strong	B
preference	O
for	O
neighbouring	O
mini-magnets	O
to	O
become	O
aligned	O
generating	O
a	O
strong	B
macromagnet	O
remarkably	O
one	O
can	O
show	O
that	O
in	O
a	O
very	O
large	O
two-dimensional	O
lattice	O
below	O
the	O
so-called	O
curie	O
temperature	O
tc	O
variables	O
the	O
system	O
admits	O
a	O
phase	O
change	O
in	O
that	O
a	O
large	O
fraction	O
of	O
the	O
variables	O
become	O
aligned	O
above	O
tc	O
on	O
average	B
the	O
variables	O
are	O
unaligned	O
this	O
is	O
depicted	O
in	O
xin	O
is	O
the	O
average	B
alignment	O
of	O
the	O
variables	O
that	O
this	O
phase	O
change	O
happens	O
where	O
m	O
for	O
non-zero	O
temperature	O
has	O
driven	O
considerable	O
research	O
in	O
this	O
and	O
related	O
global	B
coherence	O
effects	O
such	O
as	O
this	O
that	O
arise	O
from	O
weak	O
local	B
constraints	O
are	O
present	O
in	O
systems	O
that	O
admit	O
emergent	O
behaviour	O
similar	O
local	B
constraints	O
are	O
popular	O
in	O
image	O
restoration	O
algorithms	O
to	O
clean	O
up	O
noise	O
under	O
the	O
assumption	O
that	O
noise	O
will	O
not	O
show	O
any	O
local	B
spatial	O
coherence	O
whilst	O
signal	O
will	O
an	O
example	O
is	O
given	O
in	O
where	O
we	O
discuss	O
algorithms	O
for	O
inference	B
under	O
special	O
constraints	O
on	O
the	O
mrf	O
chain	B
graphical	O
models	O
definition	O
component	O
the	O
chain	B
components	O
of	O
a	O
graph	B
g	O
are	O
obtained	O
by	O
forming	O
a	O
graph	B
with	O
directed	B
edges	O
removed	O
from	O
g	O
then	O
each	O
connected	B
component	O
in	O
constitutes	O
a	O
chain	B
component	I
chain	B
graphs	O
contain	O
both	O
directed	B
and	O
undirected	B
links	O
to	O
develop	O
the	O
intuition	O
consider	O
the	O
only	O
terms	O
that	O
we	O
can	O
unambiguously	O
specify	O
from	O
this	O
depiction	O
are	O
pa	O
and	O
pb	O
since	O
there	O
is	O
no	O
mixed	B
interaction	O
of	O
directed	B
and	O
undirected	B
edges	O
at	O
the	O
a	O
and	O
b	O
vertices	O
by	O
probability	O
therefore	O
we	O
must	O
have	O
pa	O
b	O
c	O
d	O
papbpc	O
da	O
b	O
looking	O
at	O
the	O
graph	B
we	O
might	O
expect	O
the	O
interpretation	O
to	O
be	O
pc	O
da	O
b	O
dpcapdb	O
cd	O
however	O
to	O
ensure	O
normalisation	B
and	O
also	O
to	O
retain	O
generality	O
we	O
interpret	O
this	O
chain	B
component	I
as	O
pc	O
da	O
b	O
dpcapdb	O
b	O
with	O
b	O
dpcapdb	O
this	O
leads	O
to	O
the	O
interpretation	O
of	O
a	O
cg	O
as	O
a	O
dag	O
over	O
the	O
chain	B
components	O
each	O
chain	B
component	I
represents	O
a	O
distribution	B
over	O
the	O
variables	O
of	O
the	O
component	O
conditioned	O
on	O
the	O
parental	O
components	O
the	O
conditional	B
distribution	B
is	O
itself	O
a	O
product	O
over	O
the	O
cliques	O
of	O
the	O
undirected	B
component	O
and	O
moralised	O
parental	O
components	O
including	O
also	O
a	O
factor	B
to	O
ensure	O
normalisation	B
over	O
the	O
chain	B
component	I
draft	O
march	O
expressiveness	O
of	O
graphical	O
models	O
definition	O
graph	B
distribution	B
the	O
distribution	B
associated	O
with	O
a	O
chain	B
graph	B
g	O
is	O
found	O
by	O
first	O
identifying	O
the	O
chain	B
components	O
then	O
px	O
and	O
p	O
p	O
c	O
c	O
where	O
c	O
denotes	O
the	O
union	O
of	O
the	O
cliques	O
in	O
component	O
together	O
with	O
the	O
moralised	O
parental	O
components	O
of	O
with	O
being	O
the	O
associated	O
functions	O
defined	O
on	O
each	O
clique	B
the	O
proportionality	O
factor	B
is	O
determined	O
implicitly	O
by	O
the	O
constraint	O
that	O
the	O
distribution	B
sums	O
to	O
bns	O
are	O
cgs	O
in	O
which	O
the	O
connected	B
components	I
are	O
singletons	O
mns	O
are	O
cgs	O
in	O
which	O
the	O
chain	B
components	O
are	O
simply	O
the	O
connected	B
components	I
of	O
the	O
undirected	B
graph	B
cgs	O
can	O
be	O
useful	O
since	O
they	O
are	O
more	O
expressive	O
of	O
ci	O
statements	O
than	O
either	O
belief	B
networks	I
or	O
markov	O
networks	O
alone	O
the	O
reader	O
is	O
referred	O
to	O
and	O
for	O
further	O
details	O
example	O
graphs	O
are	O
more	O
expressive	O
than	O
belief	O
or	O
markov	O
networks	O
consider	O
the	O
chain	B
graph	B
in	O
which	O
has	O
chain	B
component	I
decomposition	B
pa	O
b	O
c	O
d	O
e	O
f	O
papbpc	O
d	O
e	O
fa	O
b	O
where	O
pc	O
d	O
e	O
fa	O
b	O
c	O
e	O
f	O
f	O
b	O
b	O
with	O
the	O
normalisation	B
requirement	O
b	O
c	O
e	O
f	O
f	O
b	O
the	O
marginal	B
pc	O
d	O
e	O
f	O
is	O
given	O
by	O
e	O
f	O
ab	O
bpapb	O
c	O
b	O
cdef	O
since	O
the	O
marginal	B
distribution	B
of	O
pc	O
d	O
e	O
f	O
is	O
an	O
undirected	B
no	O
dag	O
can	O
express	O
the	O
ci	O
statements	O
contained	O
in	O
the	O
marginal	B
pc	O
d	O
e	O
f	O
similarly	O
no	O
undirected	B
distribution	B
on	O
the	O
same	O
skeleton	B
as	O
could	O
express	O
that	O
a	O
and	O
b	O
are	O
independent	O
i	O
e	O
pa	O
b	O
papb	O
expressiveness	O
of	O
graphical	O
models	O
it	O
is	O
clear	O
that	O
directed	B
distributions	O
can	O
be	O
represented	O
as	O
undirected	B
distributions	O
since	O
one	O
can	O
associate	O
each	O
factor	B
in	O
a	O
directed	B
distribution	B
with	O
a	O
potential	B
for	O
example	O
the	O
distribution	B
pabpbcpc	O
can	O
be	O
factored	O
as	O
b	O
c	O
where	O
b	O
pab	O
and	O
c	O
pbcpc	O
with	O
z	O
hence	O
every	O
belief	B
network	I
can	O
be	O
represented	O
as	O
some	O
mn	O
by	O
simple	O
identification	O
of	O
the	O
factors	O
in	O
the	O
distributions	O
however	O
in	O
general	O
the	O
associated	O
undirected	B
graph	B
corresponds	O
to	O
the	O
draft	O
march	O
expressiveness	O
of	O
graphical	O
models	O
a	O
c	O
e	O
b	O
d	O
f	O
c	O
e	O
d	O
f	O
c	O
e	O
d	O
f	O
figure	O
the	O
cg	O
expresses	O
a	O
b	O
and	O
d	O
e	O
f	O
no	O
directed	B
graph	B
could	O
express	O
both	O
these	O
conditions	O
since	O
the	O
marginal	B
distribution	B
pc	O
d	O
e	O
f	O
is	O
an	O
undirected	B
four	O
cycle	O
any	O
dag	O
on	O
a	O
cycle	O
must	O
contain	O
a	O
collider	B
as	O
in	O
and	O
therefore	O
express	O
a	O
different	O
set	O
of	O
ci	O
statements	O
than	O
similarly	O
no	O
connected	B
markov	B
network	I
can	O
express	O
unconditional	O
independence	B
and	O
hence	O
expresses	O
ci	O
statements	O
that	O
no	O
belief	B
network	I
or	O
markov	B
network	I
alone	O
can	O
express	O
moralised	O
directed	B
graph	B
will	O
contain	O
additional	O
links	O
and	O
independence	B
information	O
can	O
be	O
lost	O
for	O
example	O
the	O
mn	O
of	O
pca	B
bpapb	O
if	O
a	O
single	O
clique	B
b	O
c	O
from	O
which	O
one	O
cannot	O
graphically	O
infer	O
that	O
a	O
b	O
the	O
converse	O
question	O
is	O
whether	O
every	O
undirected	B
model	B
can	O
be	O
represented	O
by	O
a	O
bn	O
with	O
a	O
readily	O
derived	O
link	O
structure	B
consider	O
the	O
example	O
in	O
in	O
this	O
case	O
there	O
is	O
no	O
directed	B
model	B
with	O
the	O
same	O
link	O
structure	B
that	O
can	O
express	O
the	O
in	O
the	O
undirected	B
graph	B
naturally	O
every	O
probability	O
distribution	B
can	O
be	O
represented	O
by	O
some	O
bn	O
though	O
it	O
may	O
not	O
necessarily	O
have	O
a	O
simple	O
structure	B
and	O
be	O
simply	O
a	O
fully	O
connected	B
cascade	B
style	O
graph	B
in	O
this	O
sense	O
the	O
dag	O
cannot	O
graphically	O
represent	O
the	O
independencedependence	O
relations	O
true	O
in	O
the	O
distribution	B
definition	O
maps	O
a	O
graph	B
is	O
an	O
independence	B
map	B
of	O
a	O
given	O
distribution	B
p	O
if	O
every	O
conditional	B
independence	B
statement	O
that	O
one	O
can	O
derive	O
from	O
the	O
graph	B
g	O
is	O
true	O
in	O
the	O
distribution	B
p	O
that	O
is	O
x	O
yz	O
g	O
x	O
yz	O
p	O
for	O
all	O
disjoint	O
sets	O
x	O
similarly	O
a	O
graph	B
is	O
a	O
dependence	O
map	B
of	O
a	O
given	O
distribution	B
p	O
if	O
every	O
conditional	B
independence	B
statement	O
that	O
one	O
can	O
derive	O
from	O
p	O
is	O
true	O
in	O
the	O
graph	B
g	O
that	O
is	O
x	O
yz	O
g	O
x	O
yz	O
p	O
for	O
all	O
disjoint	O
sets	O
x	O
a	O
graph	B
g	O
which	O
is	O
both	O
an	O
i-map	O
and	O
a	O
d-map	O
for	O
p	O
is	O
called	O
a	O
perfect	B
map	B
and	O
x	O
yz	O
g	O
x	O
yz	O
p	O
for	O
all	O
disjoint	O
sets	O
x	O
in	O
this	O
case	O
the	O
set	O
of	O
all	O
conditional	B
independence	B
and	O
dependence	O
statements	O
expressible	O
in	O
the	O
graph	B
g	O
are	O
consistent	B
with	O
p	O
and	O
vice	O
versa	O
due	O
to	O
inverse	B
modus	I
ponens	I
a	O
dependence	O
map	B
is	O
equivalent	B
to	O
g	O
p	O
although	O
this	O
is	O
less	O
useful	O
since	O
standard	O
graphical	O
model	B
representations	O
cannot	O
express	O
dependence	O
note	O
that	O
the	O
above	O
definitions	O
are	O
not	O
dependent	O
on	O
the	O
graph	B
being	O
directed	B
or	O
undirected	B
indeed	O
some	O
distributions	O
may	O
have	O
a	O
perfect	O
directed	B
map	B
but	O
no	O
perfect	O
undirected	B
map	B
for	O
example	O
px	O
y	O
z	O
pzx	O
ypxpy	O
draft	O
march	O
factor	B
graphs	O
b	O
b	O
a	O
c	O
a	O
c	O
d	O
d	O
figure	O
an	O
undirected	B
model	B
for	O
which	O
we	O
wish	O
to	O
find	O
a	O
directed	B
equivalent	B
every	O
dag	O
with	O
the	O
same	O
structure	B
as	O
the	O
undirected	B
model	B
must	O
have	O
a	O
situation	O
where	O
two	O
arrows	O
will	O
point	O
to	O
a	O
node	O
such	O
as	O
node	O
d	O
summing	O
over	O
the	O
states	O
of	O
variable	O
d	O
will	O
leave	O
a	O
dag	O
on	O
the	O
variables	O
a	O
b	O
c	O
with	O
no	O
link	O
between	O
a	O
and	O
c	O
this	O
cannot	O
represent	O
the	O
undirected	B
model	B
since	O
when	O
one	O
marginals	O
over	O
d	O
in	O
the	O
undirected	B
this	O
adds	O
a	O
link	O
between	O
a	O
and	O
c	O
has	O
a	O
directed	B
perfect	B
map	B
x	O
z	O
y	O
that	O
pzx	O
y	O
xx	O
yy	O
but	O
no	O
perfect	O
undirected	B
map	B
example	O
consider	O
the	O
distribution	B
defined	O
on	O
variables	O
h	O
the	O
bn	O
hph	O
is	O
an	O
i-map	O
for	O
distribution	B
since	O
every	O
independence	B
statement	O
in	O
the	O
bn	O
is	O
true	O
for	O
the	O
corresponding	O
graph	B
however	O
it	O
is	O
not	O
a	O
d-map	O
since	O
cannot	O
be	O
inferred	O
from	O
the	O
bn	O
similarly	O
no	O
undirected	B
graph	B
can	O
represent	O
all	O
independence	B
statements	O
true	O
in	O
in	O
this	O
case	O
no	O
perfect	B
map	B
bn	O
or	O
a	O
mn	O
can	O
represent	O
factor	B
graphs	O
factor	B
graphs	O
are	O
mainly	O
used	O
as	O
part	O
of	O
inference	B
definition	O
graph	B
given	O
a	O
function	B
xn	O
x	O
i	O
i	O
the	O
fg	O
has	O
a	O
node	O
by	O
a	O
square	O
for	O
each	O
factor	B
i	O
and	O
a	O
variable	O
node	O
by	O
a	O
circle	O
for	O
each	O
variable	O
xj	O
for	O
each	O
xj	O
x	O
i	O
an	O
undirected	B
link	O
is	O
made	O
between	O
factor	B
i	O
and	O
variable	O
xj	O
for	O
a	O
factor	B
i	O
parents	B
to	O
the	O
factor	B
node	O
and	O
a	O
directed	B
link	O
from	O
the	O
factor	B
node	O
to	O
the	O
child	O
this	O
has	O
the	O
same	O
structure	B
as	O
an	O
fg	O
but	O
preserves	O
the	O
information	O
that	O
the	O
factors	O
are	O
distributions	O
x	O
which	O
is	O
a	O
conditional	B
distribution	B
pxipa	O
we	O
may	O
use	O
directed	B
links	O
from	O
the	O
factor	B
graphs	O
are	O
useful	O
since	O
they	O
can	O
preserve	O
more	O
information	O
about	O
the	O
form	O
of	O
the	O
distribution	B
than	O
either	O
a	O
belief	B
network	I
or	O
a	O
markov	B
network	I
chain	B
graph	B
can	O
do	O
alone	O
consider	O
the	O
distribution	B
pa	O
b	O
c	O
b	O
c	O
c	O
a	O
fg	O
is	O
an	O
alternative	O
graphical	O
depiction	O
of	O
a	O
in	O
which	O
the	O
vertices	O
represent	O
variables	O
and	O
a	O
hyperedge	O
a	O
factor	B
as	O
a	O
function	B
of	O
the	O
variables	O
associated	O
with	O
the	O
hyperedge	O
a	O
fg	O
is	O
therefore	O
a	O
hypergraph	O
with	O
the	O
additional	O
interpretation	O
that	O
the	O
graph	B
represents	O
a	O
function	B
defined	O
as	O
products	O
over	O
the	O
associated	O
hyperedges	O
many	O
thanks	O
to	O
robert	O
cowell	O
for	O
this	O
observation	O
draft	O
march	O
exercises	O
a	O
a	O
a	O
a	O
a	O
c	O
b	O
c	O
b	O
c	O
b	O
c	O
b	O
c	O
b	O
a	O
c	O
b	O
d	O
b	O
c	O
a	O
figure	O
b	O
c	O
b	O
c	O
both	O
and	O
have	O
the	O
same	O
undirected	B
graphical	O
model	B
directed	B
fg	O
of	O
the	O
bn	O
in	O
is	O
an	O
undirected	B
fg	O
of	O
the	O
advantage	O
of	O
over	O
is	O
that	O
information	O
regarding	O
the	O
marginal	B
independence	B
of	O
variables	O
b	O
and	O
c	O
is	O
clear	O
from	O
graph	B
whereas	O
one	O
could	O
only	O
ascertain	O
this	O
by	O
examination	O
of	O
the	O
numerical	B
entries	O
of	O
the	O
a	O
partially	O
directed	B
fg	O
of	O
pab	O
c	O
c	O
d	O
no	O
directed	B
undirected	B
or	O
factors	O
in	O
graph	B
chain	B
graph	B
can	O
represent	O
both	O
the	O
conditional	B
and	O
marginal	B
independence	B
statements	O
expressed	O
by	O
this	O
graph	B
and	O
also	O
the	O
factored	O
structure	B
of	O
the	O
undirected	B
terms	O
the	O
mn	O
representation	O
is	O
given	O
in	O
however	O
could	O
equally	O
represent	O
some	O
unfactored	O
clique	B
potential	B
b	O
c	O
in	O
this	O
sense	O
the	O
fg	O
representation	O
in	O
more	O
precisely	O
conveys	O
the	O
form	O
of	O
distribution	B
equation	B
an	O
unfactored	O
clique	B
potential	B
b	O
c	O
is	O
represented	O
by	O
the	O
fg	O
hence	O
different	O
fgs	O
can	O
have	O
the	O
same	O
mn	O
since	O
information	O
regarding	O
the	O
structure	B
of	O
the	O
clique	B
potential	B
is	O
lost	O
in	O
the	O
mn	O
conditional	B
independence	B
in	O
factor	B
graphs	O
a	O
rule	O
which	O
works	O
with	O
both	O
directed	B
and	O
undirected	B
partially	O
directed	B
fgs	O
is	O
as	O
to	O
determine	O
whether	O
two	O
variables	O
are	O
independent	O
given	O
a	O
set	O
of	O
conditioned	O
variables	O
consider	O
all	O
paths	O
connecting	O
the	O
two	O
variables	O
if	O
all	O
paths	O
are	O
blocked	B
the	O
variables	O
are	O
conditionally	O
independent	O
a	O
path	B
is	O
blocked	B
if	O
any	O
one	O
or	O
more	O
of	O
the	O
following	O
conditions	O
are	O
satisfied	O
one	O
of	O
the	O
variables	O
in	O
the	O
path	B
is	O
in	O
the	O
conditioning	B
set	O
one	O
of	O
the	O
variables	O
or	O
factors	O
in	O
the	O
path	B
has	O
two	O
incoming	O
edges	O
that	O
are	O
part	O
of	O
the	O
path	B
and	O
neither	O
the	O
variable	O
or	O
factor	B
nor	O
any	O
of	O
its	O
descendants	O
are	O
in	O
the	O
conditioning	B
set	O
notes	O
a	O
detailed	O
discussion	O
of	O
the	O
axiomatic	O
and	O
logical	O
basis	O
of	O
conditional	B
independence	B
is	O
given	O
in	O
and	O
code	O
condindep	O
m	O
conditional	B
independence	B
test	O
px	O
y	O
pxzpy	O
exercises	O
exercise	O
consider	O
the	O
pairwise	B
markov	B
network	I
px	O
express	O
in	O
terms	O
of	O
the	O
following	O
draft	O
march	O
exercises	O
for	O
a	O
set	O
of	O
local	B
distributions	O
defined	O
as	O
is	O
it	O
always	O
possible	O
to	O
find	O
a	O
joint	B
distribution	B
consistent	B
with	O
these	O
local	B
conditional	B
distributions	O
exercise	O
consider	O
the	O
markov	B
network	I
pa	O
b	O
c	O
aba	O
b	O
bcb	O
c	O
nominally	O
by	O
summing	O
over	O
b	O
the	O
variables	O
a	O
and	O
c	O
are	O
dependent	O
for	O
binary	O
b	O
explain	O
a	O
situation	O
in	O
which	O
this	O
is	O
not	O
the	O
case	O
so	O
that	O
marginally	O
a	O
and	O
c	O
are	O
independent	O
exercise	O
show	O
that	O
for	O
the	O
boltzmann	B
machine	I
px	O
zw	O
b	O
extwxxtb	O
one	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
w	O
wt	O
exercise	O
the	O
restricted	B
boltzmann	B
machine	I
is	O
a	O
specially	O
constrained	O
boltzmann	B
machine	I
on	O
a	O
bipartite	O
graph	B
consisting	O
of	O
a	O
layer	O
of	O
visible	B
variables	O
v	O
vv	O
and	O
hidden	B
variables	I
h	O
hh	O
pv	O
h	O
zw	O
a	O
b	O
evtwhatvbth	O
all	O
variables	O
are	O
binary	O
taking	O
states	O
show	O
that	O
the	O
distribution	B
of	O
hidden	B
units	O
conditional	B
on	O
the	O
visible	B
units	O
factorises	O
as	O
bi	O
j	O
wjivj	O
phv	O
i	O
phiv	O
with	O
phiv	O
where	O
ex	O
by	O
symmetry	O
arguments	O
write	O
down	O
the	O
form	O
of	O
the	O
conditional	B
pvh	O
is	O
ph	O
factorised	B
can	O
the	O
partition	B
function	B
zw	O
a	O
b	O
be	O
computed	O
efficiently	O
for	O
the	O
rbm	O
exercise	O
consider	O
px	O
is	O
it	O
possible	O
to	O
compute	O
argmax	O
px	O
efficiently	O
exercise	O
you	O
are	O
given	O
that	O
x	O
yz	O
u	O
u	O
z	O
derive	O
the	O
most	O
general	O
form	O
of	O
probability	O
distribution	B
px	O
y	O
z	O
u	O
consistent	B
with	O
these	O
statements	O
does	O
this	O
distribution	B
have	O
a	O
simple	O
graphical	O
model	B
draft	O
march	O
exercises	O
exercise	O
the	O
undirected	B
graph	B
represents	O
a	O
markov	B
network	I
with	O
nodes	O
counting	B
clockwise	O
around	O
the	O
pentagon	O
with	O
potentials	O
show	O
that	O
the	O
joint	B
distribution	B
can	O
be	O
written	O
as	O
and	O
express	O
the	O
marginal	B
probability	O
tables	O
explicitly	O
as	O
functions	O
of	O
the	O
potentials	O
xj	O
exercise	O
consider	O
the	O
belief	B
network	I
on	O
the	O
right	O
write	O
down	O
a	O
markov	B
network	I
of	O
is	O
your	O
markov	B
network	I
a	O
perfect	B
map	B
of	O
exercise	O
two	O
research	O
labs	O
work	O
independently	O
on	O
the	O
relationship	O
between	O
discrete	B
variables	O
x	O
and	O
y	O
lab	O
a	O
proudly	O
announces	O
that	O
they	O
have	O
ascertained	O
distribution	B
paxy	O
from	O
data	O
lab	O
b	O
proudly	O
announces	O
that	O
they	O
have	O
ascertained	O
pbyx	O
from	O
data	O
is	O
it	O
always	O
possible	O
to	O
find	O
a	O
joint	B
distribution	B
px	O
y	O
consistent	B
with	O
the	O
results	O
of	O
both	O
labs	O
is	O
it	O
possible	O
to	O
define	O
consistent	B
marginals	O
px	O
and	O
py	O
in	O
the	O
sense	O
that	O
px	O
and	O
py	O
y	O
paxypy	O
x	O
pbyxpx	O
if	O
so	O
explain	O
how	O
to	O
find	O
such	O
marginals	O
if	O
not	O
explain	O
why	O
not	O
exercise	O
research	O
lab	O
a	O
states	O
its	O
findings	O
about	O
a	O
set	O
of	O
variables	O
xn	O
as	O
a	O
list	O
la	O
of	O
conditional	B
independence	B
statements	O
lab	O
b	O
similarly	O
provides	O
a	O
list	O
of	O
conditional	B
independence	B
statements	O
lb	O
is	O
it	O
possible	O
to	O
find	O
a	O
distribution	B
which	O
is	O
consistent	B
with	O
la	O
and	O
lb	O
if	O
the	O
lists	O
also	O
contain	O
dependence	O
statements	O
how	O
could	O
one	O
attempt	O
to	O
find	O
a	O
distribution	B
that	O
is	O
consistent	B
with	O
both	O
lists	O
exercise	O
consider	O
the	O
distribution	B
px	O
y	O
w	O
z	O
pzwpwx	O
ypxpy	O
write	O
pxz	O
using	O
a	O
formula	O
involving	O
or	O
some	O
of	O
pzw	O
pwx	O
y	O
px	O
py	O
write	O
pyz	O
using	O
a	O
formula	O
involving	O
or	O
some	O
of	O
pzw	O
pwx	O
y	O
px	O
py	O
using	O
the	O
above	O
results	O
derive	O
an	O
explicit	O
condition	O
for	O
x	O
y	O
z	O
and	O
explain	O
if	O
this	O
is	O
satisfied	O
for	O
this	O
distribution	B
exercise	O
consider	O
the	O
distribution	B
h	O
draw	O
a	O
belief	B
network	I
for	O
this	O
distribution	B
can	O
the	O
distribution	B
h	O
be	O
written	O
as	O
a	O
non-complete	O
belief	B
network	I
show	O
that	O
for	O
as	O
defined	O
above	O
draft	O
march	O
exercise	O
consider	O
the	O
distribution	B
pa	O
b	O
c	O
d	O
aba	O
b	O
bcb	O
c	O
cdc	O
d	O
dad	O
a	O
where	O
the	O
are	O
potentials	O
draw	O
a	O
markov	B
network	I
for	O
this	O
distribution	B
exercises	O
explain	O
if	O
the	O
distribution	B
can	O
be	O
represented	O
as	O
a	O
non-complete	O
belief	B
network	I
derive	O
explicitly	O
if	O
a	O
c	O
exercise	O
show	O
how	O
for	O
any	O
singly-connected	B
markov	B
network	I
one	O
may	O
construct	O
a	O
markov	B
equivalent	B
belief	B
network	I
exercise	O
consider	O
a	O
pairwise	B
binary	O
markov	B
network	I
defined	O
on	O
variables	O
si	O
i	O
n	O
ij	O
e	O
ijsi	O
sj	O
where	O
e	O
is	O
a	O
given	O
edge	O
set	O
and	O
the	O
potentials	O
ij	O
are	O
arbitrary	O
explain	O
how	O
to	O
translate	O
such	O
a	O
markov	B
network	I
into	O
a	O
boltzmann	B
machine	I
with	O
ps	O
draft	O
march	O
chapter	O
efficient	O
inference	B
in	O
trees	O
marginal	B
inference	B
given	O
a	O
distribution	B
xn	O
inference	B
is	O
the	O
process	O
of	O
computing	O
functions	O
of	O
the	O
distribution	B
for	O
example	O
computing	O
a	O
marginal	B
conditioned	O
on	O
a	O
subset	O
of	O
variables	O
being	O
in	O
a	O
particular	O
state	O
would	O
be	O
an	O
inference	B
task	O
similarly	O
computing	O
the	O
mean	B
of	O
a	O
variable	O
can	O
be	O
considered	O
an	O
inference	B
task	O
the	O
main	O
focus	O
of	O
this	O
chapter	O
is	O
on	O
efficient	O
inference	B
algorithms	O
for	O
marginal	B
inference	B
in	O
singly-connected	B
structures	O
an	O
efficient	O
algorithm	B
for	O
multiply-connected	B
graphs	O
will	O
be	O
considered	O
in	O
marginal	B
inference	B
is	O
concerned	O
with	O
the	O
computation	O
of	O
the	O
distribution	B
of	O
a	O
subset	O
of	O
variables	O
possibly	O
conditioned	O
on	O
another	O
subset	O
for	O
example	O
given	O
a	O
joint	B
distribution	B
a	O
marginal	B
inference	B
given	O
evidence	O
calculation	O
is	O
tr	O
tr	O
marginal	B
inference	B
for	O
discrete	B
models	O
involves	O
summation	O
and	O
will	O
be	O
the	O
focus	O
of	O
our	O
development	O
in	O
principle	O
the	O
algorithms	O
carry	O
over	O
to	O
continuous	B
variable	O
models	O
although	O
the	O
lack	O
of	O
closure	O
of	O
most	O
continuous	B
distributions	O
under	O
marginalisation	B
gaussian	B
being	O
a	O
notable	O
exception	O
can	O
make	O
the	O
direct	O
transference	O
of	O
these	O
algorithms	O
to	O
the	O
continuous	B
domain	B
problematic	O
variable	B
elimination	I
in	O
a	O
markov	B
chain	B
and	O
message	B
passing	B
a	O
key	O
concept	O
in	O
efficient	O
inference	B
is	O
message	B
passing	B
in	O
which	O
information	O
from	O
the	O
graph	B
is	O
summarised	O
by	O
local	B
edge	O
information	O
to	O
develop	O
this	O
idea	O
consider	O
the	O
four	O
variable	O
markov	B
chain	B
chains	O
are	O
discussed	O
in	O
more	O
depth	O
in	O
pa	O
b	O
c	O
d	O
pabpbcpcdpd	O
as	O
given	O
in	O
for	O
which	O
our	O
task	O
is	O
to	O
calculate	O
the	O
marginal	B
pa	O
for	O
simplicity	O
we	O
assume	O
that	O
each	O
of	O
the	O
variables	O
has	O
domain	B
then	O
pa	O
pa	O
b	O
c	O
d	O
b	O
b	O
pa	O
a	O
b	O
c	O
d	O
figure	O
a	O
markov	B
chain	B
is	O
of	O
the	O
form	O
pxt	O
for	O
some	O
assignment	O
of	O
the	O
variables	O
to	O
labels	O
xt	O
variable	B
elimination	I
can	O
be	O
carried	O
out	O
in	O
time	O
linear	B
in	O
the	O
number	O
of	O
variables	O
in	O
the	O
chain	B
we	O
could	O
carry	O
out	O
this	O
computation	O
by	O
simply	O
summing	O
each	O
of	O
the	O
probabilities	O
for	O
the	O
states	O
of	O
the	O
variables	O
bc	O
and	O
d	O
marginal	B
inference	B
a	O
more	O
efficient	O
approach	B
is	O
to	O
push	O
the	O
summation	O
over	O
d	O
as	O
far	O
to	O
the	O
right	O
as	O
possible	O
pa	O
b	O
pa	O
d	O
pcdpd	O
dc	O
where	O
d	O
is	O
a	O
state	O
potential	B
similarly	O
we	O
can	O
distribute	O
the	O
summation	O
over	O
c	O
as	O
far	O
to	O
the	O
right	O
as	O
possible	O
pa	O
b	O
pa	O
c	O
then	O
finally	O
pa	O
b	O
pa	O
c	O
pbc	O
d	O
cb	O
by	O
distributing	O
the	O
summations	O
we	O
have	O
made	O
additions	O
compared	O
to	O
from	O
the	O
naive	O
approach	B
whilst	O
this	O
saving	O
may	O
not	O
appear	O
much	O
the	O
important	O
point	O
is	O
that	O
the	O
number	O
of	O
computations	O
for	O
a	O
chain	B
of	O
length	O
t	O
would	O
scale	O
linearly	O
with	O
t	O
as	O
opposed	O
to	O
exponentially	O
for	O
the	O
naive	O
approach	B
this	O
procedure	O
is	O
naturally	O
enough	O
called	O
variable	B
elimination	I
since	O
each	O
time	O
we	O
sum	O
over	O
the	O
states	O
of	O
a	O
variable	O
we	O
eliminate	O
it	O
from	O
the	O
distribution	B
we	O
can	O
always	O
perform	O
variable	B
elimination	I
in	O
a	O
chain	B
efficiently	O
since	O
there	O
is	O
a	O
natural	B
way	O
to	O
distribute	O
the	O
summations	O
working	O
inwards	O
from	O
the	O
edges	O
note	O
that	O
in	O
the	O
above	O
case	O
the	O
potentials	O
are	O
in	O
fact	O
always	O
distributions	O
we	O
are	O
just	O
recursively	O
computing	O
the	O
marginal	B
distribution	B
of	O
the	O
right	O
leaf	O
of	O
the	O
chain	B
one	O
can	O
view	O
the	O
elimination	O
of	O
a	O
variable	O
as	O
passing	B
a	O
message	B
to	O
a	O
neighbouring	O
vertex	O
on	O
the	O
graph	B
we	O
can	O
calculate	O
a	O
univariate-marginal	O
of	O
any	O
singly-connected	B
graph	B
by	O
starting	O
at	O
a	O
leaf	O
of	O
the	O
tree	B
eliminating	O
the	O
variable	O
there	O
and	O
then	O
working	O
inwards	O
nibbling	O
off	O
each	O
time	O
a	O
leaf	O
of	O
the	O
remaining	O
tree	B
provided	O
we	O
perform	O
elimination	O
from	O
the	O
leaves	O
inwards	O
then	O
the	O
structure	B
of	O
the	O
remaining	O
graph	B
is	O
simply	O
a	O
subtree	O
of	O
the	O
original	O
tree	B
albeit	O
with	O
the	O
conditional	B
probability	I
table	O
entries	O
modified	O
to	O
potentials	O
which	O
update	O
under	O
recursion	O
this	O
is	O
guaranteed	O
to	O
enable	O
us	O
to	O
calculate	O
any	O
marginal	B
pxi	O
using	O
a	O
number	O
of	O
summations	O
which	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
variables	O
in	O
the	O
graph	B
finding	O
conditional	B
marginals	O
for	O
a	O
chain	B
consider	O
the	O
following	O
inference	B
problem	B
given	O
pa	O
b	O
c	O
d	O
pabpbcpcdpd	O
find	O
pda	O
this	O
can	O
be	O
computed	O
using	O
bc	O
bc	O
pda	O
pa	O
b	O
c	O
d	O
pabpbcpcdpd	O
pcdpd	O
c	O
c	O
b	O
pabpbc	O
bc	O
the	O
fact	O
the	O
missing	B
proportionality	O
constant	O
is	O
found	O
by	O
repeating	O
the	O
computation	O
for	O
all	O
states	O
of	O
variable	O
d	O
since	O
we	O
know	O
that	O
pda	O
k	O
c	O
where	O
c	O
is	O
the	O
unnormalised	O
result	O
of	O
the	O
summation	O
we	O
can	O
use	O
d	O
pda	O
to	O
infer	O
that	O
k	O
d	O
c	O
draft	O
march	O
marginal	B
inference	B
in	O
this	O
example	O
the	O
potential	B
b	O
is	O
not	O
a	O
distribution	B
in	O
c	O
nor	O
is	O
c	O
in	O
general	O
one	O
may	O
view	O
variable	B
elimination	I
as	O
the	O
passing	B
of	O
messages	O
in	O
the	O
form	O
of	O
potentials	O
from	O
nodes	O
to	O
their	O
neighbours	O
for	O
belief	B
networks	I
variable	B
elimination	I
passes	O
messages	O
that	O
are	O
distributions	O
when	O
following	O
the	O
direction	O
of	O
the	O
edge	O
and	O
non-normalised	O
potentials	O
when	O
passing	B
messages	O
against	O
the	O
direction	O
of	O
the	O
edge	O
remark	O
variable	B
elimination	I
in	O
trees	O
as	O
matrix	B
multiplication	O
variable	B
elimination	I
is	O
related	O
to	O
the	O
associativity	O
of	O
matrix	B
multiplication	O
for	O
equation	B
above	O
we	O
can	O
define	O
matrices	O
pa	O
ib	O
j	O
pc	O
id	O
j	O
pb	O
ic	O
j	O
pd	O
i	O
pa	O
i	O
then	O
the	O
marginal	B
ma	O
can	O
be	O
written	O
ma	O
mabmbcmcdmd	O
mabmbcmcdmd	O
since	O
matrix	B
multiplication	O
is	O
associative	O
this	O
matrix	B
formulation	O
of	O
calculating	O
marginals	O
is	O
called	O
the	O
transfer	B
matrix	B
method	O
and	O
is	O
particularly	O
popular	O
in	O
the	O
physics	O
example	O
will	O
the	O
fly	O
be	O
you	O
live	O
in	O
a	O
house	O
with	O
three	O
rooms	O
labelled	B
there	O
is	O
a	O
door	O
between	O
rooms	O
and	O
and	O
another	O
between	O
rooms	O
and	O
one	O
cannot	O
directly	O
pass	O
between	O
rooms	O
and	O
in	O
one	O
time-step	O
an	O
annoying	O
fly	O
is	O
buzzing	O
from	O
one	O
room	O
to	O
another	O
and	O
there	O
is	O
some	O
smelly	O
cheese	O
in	O
room	O
which	O
seems	O
to	O
attract	O
the	O
fly	O
more	O
using	O
xt	O
for	O
which	O
room	O
the	O
fly	O
is	O
in	O
at	O
time	O
t	O
with	O
domxt	O
the	O
movement	O
of	O
the	O
fly	O
can	O
be	O
described	O
by	O
a	O
transition	O
ixt	O
j	O
mij	O
where	O
m	O
is	O
a	O
transition	B
matrix	B
m	O
t	O
the	O
transition	B
matrix	B
is	O
stochastic	O
in	O
the	O
sense	O
that	O
as	O
required	O
of	O
a	O
conditional	B
probability	I
distribution	B
mij	O
given	O
that	O
the	O
fly	O
is	O
in	O
room	O
at	O
time	O
what	O
is	O
the	O
probability	O
of	O
room	O
occupancy	O
at	O
time	O
t	O
assume	O
a	O
markov	B
chain	B
which	O
is	O
defined	O
by	O
the	O
joint	B
distribution	B
xt	O
we	O
are	O
asked	O
to	O
compute	O
which	O
is	O
given	O
by	O
since	O
the	O
graph	B
of	O
the	O
distribution	B
is	O
a	O
markov	B
chain	B
we	O
can	O
easily	O
distribute	O
the	O
summation	O
over	O
the	O
terms	O
this	O
is	O
most	O
easily	O
done	O
using	O
the	O
transfer	B
matrix	B
method	O
giving	O
draft	O
march	O
where	O
v	O
is	O
a	O
vector	O
with	O
components	O
reflecting	O
the	O
evidence	O
that	O
at	O
time	O
the	O
fly	O
is	O
in	O
room	O
computing	O
this	O
we	O
have	O
decimal	O
places	O
of	O
accuracy	O
marginal	B
inference	B
xt	O
similarly	O
after	O
time-steps	O
the	O
occupancy	O
probabilities	O
are	O
the	O
room	O
occupancy	O
probability	O
is	O
converging	O
to	O
a	O
particular	O
distribution	B
the	O
stationary	B
distribution	B
of	O
the	O
markov	B
chain	B
one	O
might	O
ask	O
where	O
the	O
fly	O
is	O
after	O
an	O
infinite	O
number	O
of	O
time-steps	O
that	O
is	O
we	O
are	O
interested	O
in	O
the	O
large	O
t	O
behaviour	O
of	O
at	O
convergence	O
pxt	O
writing	O
p	O
for	O
the	O
vector	O
describing	O
the	O
stationary	B
distribution	B
this	O
means	O
p	O
mp	O
in	O
other	O
words	O
p	O
is	O
the	O
eigenvector	O
of	O
m	O
with	O
eigenvalue	O
computing	O
this	O
numerically	O
the	O
stationary	B
distribution	B
is	O
note	O
that	O
software	O
packages	O
usually	O
return	O
eigenvectors	O
with	O
ete	O
the	O
unit	O
eigenvector	O
therefore	O
will	O
usually	O
require	O
normalisation	B
to	O
make	O
this	O
a	O
probability	O
the	O
sum-product	B
algorithm	B
on	O
factor	B
graphs	O
both	O
markov	O
and	O
belief	B
networks	I
can	O
be	O
represented	O
using	O
factor	B
graphs	O
for	O
this	O
reason	O
it	O
is	O
convenient	O
to	O
derive	O
a	O
marginal	B
inference	B
algorithm	B
for	O
the	O
fg	O
since	O
this	O
then	O
applies	O
to	O
both	O
markov	O
and	O
belief	B
networks	I
this	O
is	O
termed	O
the	O
sum-product	B
algorithm	B
since	O
to	O
compute	O
marginals	O
we	O
need	O
to	O
distribute	O
the	O
sum	O
over	O
variable	O
states	O
over	O
the	O
product	O
of	O
factors	O
in	O
older	O
texts	O
this	O
is	O
referred	O
to	O
as	O
belief	B
propagation	B
non-branching	O
graphs	O
variable	O
to	O
variable	O
messages	O
consider	O
the	O
distribution	B
pa	O
b	O
c	O
d	O
b	O
c	O
d	O
which	O
has	O
the	O
factor	B
graph	B
represented	O
in	O
with	O
factors	O
as	O
defined	O
by	O
the	O
f	O
above	O
to	O
compute	O
the	O
marginal	B
pa	O
b	O
c	O
since	O
the	O
variable	O
d	O
only	O
occurs	O
locally	O
we	O
use	O
pa	O
b	O
c	O
pa	O
b	O
c	O
d	O
d	O
d	O
b	O
c	O
d	O
b	O
d	O
similarly	O
pa	O
b	O
c	O
pa	O
b	O
c	O
c	O
hence	O
c	O
b	O
c	O
c	O
d	O
c	O
c	O
d	O
c	O
c	O
bb	O
it	O
is	O
clear	O
how	O
one	O
can	O
recurse	O
this	O
definition	O
of	O
messages	O
so	O
that	O
for	O
a	O
chain	B
of	O
length	O
n	O
variables	O
the	O
marginal	B
of	O
the	O
first	O
node	O
can	O
be	O
computed	O
in	O
time	O
linear	B
in	O
n	O
the	O
term	O
c	O
b	O
can	O
be	O
interpreted	O
as	O
draft	O
march	O
d	O
d	O
cc	O
marginal	B
inference	B
a	O
b	O
c	O
d	O
figure	O
for	O
singly-connected	B
structures	O
without	O
branches	O
simple	O
messages	O
from	O
one	O
variable	O
to	O
its	O
neighbour	B
may	O
be	O
defined	O
to	O
form	O
an	O
efficient	O
marginal	B
inference	B
scheme	O
carrying	O
marginal	B
information	O
from	O
the	O
graph	B
beyond	O
c	O
for	O
any	O
singly-connected	B
structure	B
the	O
factors	O
at	O
the	O
edge	O
of	O
the	O
graph	B
can	O
be	O
replaced	O
with	O
messages	O
that	O
reflect	O
marginal	B
information	O
from	O
the	O
graph	B
beyond	O
that	O
factor	B
for	O
simple	O
linear	B
structures	O
with	O
no	O
branching	O
messages	O
from	O
variables	O
to	O
variables	O
are	O
sufficient	O
however	O
as	O
we	O
will	O
see	O
below	O
it	O
is	O
useful	O
in	O
more	O
general	O
structures	O
with	O
branching	O
to	O
consider	O
two	O
types	O
of	O
messages	O
namely	O
those	O
from	O
variables	O
to	O
factors	O
and	O
vice	O
versa	O
general	O
singly-connected	B
factor	B
graphs	O
the	O
slightly	O
more	O
complex	O
example	O
pabpbc	O
dpcpdped	O
has	O
the	O
factor	B
graph	B
b	O
c	O
d	O
e	O
if	O
the	O
marginal	B
pa	O
b	O
is	O
to	O
be	O
represented	O
by	O
the	O
amputated	O
graph	B
with	O
messages	O
on	O
the	O
edges	O
then	O
e	O
e	O
in	O
this	O
case	O
it	O
is	O
natural	B
to	O
consider	O
messages	O
from	O
factors	O
to	O
variables	O
similarly	O
we	O
can	O
break	O
the	O
message	B
from	O
the	O
factor	B
into	O
messages	O
arriving	O
from	O
the	O
two	O
branches	O
through	O
c	O
and	O
d	O
namely	O
pa	O
b	O
cd	O
c	O
d	O
bb	O
b	O
c	O
d	O
c	O
cd	O
e	O
d	O
e	O
similarly	O
we	O
can	O
interpret	O
d	O
dd	O
e	O
e	O
dd	O
to	O
complete	O
the	O
interpretation	O
we	O
identify	O
c	O
c	O
in	O
a	O
non-branching	O
link	O
one	O
can	O
more	O
simply	O
use	O
a	O
variable	O
to	O
variable	O
message	B
a	O
b	O
e	O
c	O
d	O
figure	O
for	O
a	O
branching	O
singly-connected	B
graph	B
it	O
is	O
useful	O
to	O
define	O
messages	O
from	O
both	O
factors	O
to	O
variables	O
and	O
variables	O
to	O
factors	O
draft	O
march	O
for	O
consistency	O
of	O
interpretation	O
one	O
also	O
can	O
view	O
the	O
above	O
as	O
a	O
convenience	O
of	O
this	O
approach	B
is	O
that	O
the	O
messages	O
can	O
be	O
reused	O
to	O
evaluate	O
other	O
marginal	B
inferences	O
for	O
example	O
it	O
is	O
clear	O
that	O
pb	O
is	O
given	O
by	O
marginal	B
inference	B
to	O
compute	O
the	O
marginal	B
pa	O
we	O
then	O
have	O
b	O
b	O
aa	O
b	O
b	O
b	O
b	O
b	O
b	O
pa	O
b	O
a	O
a	O
pb	O
c	O
bb	O
if	O
we	O
additionally	O
desire	O
pc	O
we	O
need	O
to	O
define	O
the	O
message	B
from	O
to	O
c	O
c	O
d	O
b	O
d	O
bd	O
where	O
b	O
b	O
this	O
demonstrates	O
the	O
reuse	O
of	O
already	O
computed	O
message	B
from	O
d	O
to	O
to	O
compute	O
the	O
marginal	B
pc	O
definition	O
schedule	B
a	O
message	B
schedule	B
is	O
a	O
specified	O
sequence	O
of	O
message	B
updates	O
a	O
valid	O
schedule	B
is	O
that	O
a	O
message	B
can	O
be	O
sent	O
from	O
a	O
node	O
only	O
when	O
that	O
node	O
has	O
received	O
all	O
requisite	O
messages	O
from	O
its	O
neighbours	O
in	O
general	O
there	O
is	O
more	O
than	O
one	O
valid	O
updating	O
schedule	B
sum-product	B
algorithm	B
the	O
sum-product	B
algorithm	B
is	O
described	O
below	O
in	O
which	O
messages	O
are	O
updated	O
as	O
a	O
function	B
of	O
incoming	O
messages	O
one	O
then	O
proceeds	O
by	O
computing	O
the	O
messages	O
in	O
a	O
schedule	B
that	O
allows	O
the	O
computation	O
of	O
a	O
new	O
message	B
based	O
on	O
previously	O
computed	O
messages	O
until	O
all	O
messages	O
from	O
all	O
factors	O
to	O
variables	O
and	O
vice-versa	O
have	O
been	O
computed	O
definition	O
messages	O
on	O
factor	B
graphs	O
x	O
provided	O
the	O
given	O
a	O
distribution	B
defined	O
as	O
a	O
product	O
on	O
subsets	O
of	O
the	O
variables	O
px	O
factor	B
graph	B
is	O
singly-connected	B
we	O
can	O
carry	O
out	O
summation	O
over	O
the	O
variables	O
efficiently	O
f	O
f	O
z	O
initialisation	O
messages	O
from	O
extremal	B
node	O
factors	O
are	O
initialised	O
to	O
the	O
factor	B
messages	O
from	O
extremal	B
variable	O
nodes	O
are	O
set	O
to	O
unity	O
variable	O
to	O
factor	B
message	B
x	O
f	O
g	O
g	O
x	O
f	O
xx	O
x	O
x	O
x	O
f	O
x	O
f	O
f	O
x	O
draft	O
march	O
marginal	B
inference	B
factor	B
to	O
variable	O
message	B
f	O
x	O
f	O
f	O
y	O
x	O
fx	O
y	O
y	O
f	O
we	O
y	O
x	O
fx	O
to	O
emphasise	O
that	O
we	O
sum	O
over	O
all	O
states	O
in	O
the	O
set	O
of	O
variables	O
x	O
fx	O
marginal	B
f	O
nex	O
px	O
f	O
x	O
y	O
fy	O
f	O
y	O
f	O
y	O
xx	O
x	O
f	O
x	O
f	O
f	O
x	O
x	O
x	O
x	O
for	O
marginal	B
inference	B
the	O
important	O
information	O
is	O
the	O
relative	O
size	O
of	O
the	O
message	B
states	O
so	O
that	O
we	O
may	O
renormalise	O
messages	O
as	O
we	O
wish	O
since	O
the	O
marginal	B
will	O
be	O
proportional	O
to	O
the	O
incoming	O
messages	O
for	O
that	O
variable	O
the	O
normalisation	B
constant	I
is	O
trivially	O
obtained	O
using	O
the	O
fact	O
that	O
the	O
marginal	B
must	O
sum	O
to	O
however	O
if	O
we	O
wish	O
to	O
also	O
compute	O
any	O
normalisation	B
constant	I
using	O
these	O
messages	O
we	O
cannot	O
normalise	O
the	O
messages	O
since	O
this	O
global	B
information	O
will	O
then	O
be	O
lost	O
to	O
resolve	O
this	O
one	O
may	O
work	O
with	O
log	O
messages	O
to	O
avoid	O
numerical	B
underoverflow	O
problems	O
the	O
sum-product	B
algorithm	B
is	O
able	O
to	O
perform	O
efficient	O
marginal	B
inference	B
in	O
both	O
belief	O
and	O
markov	O
networks	O
since	O
both	O
are	O
expressible	O
as	O
factor	B
graphs	O
this	O
is	O
the	O
reason	O
for	O
the	O
preferred	O
use	O
of	O
the	O
factor	B
graph	B
since	O
it	O
requires	O
only	O
a	O
single	O
algorithm	B
and	O
is	O
agnostic	O
to	O
whether	O
or	O
not	O
the	O
graph	B
is	O
a	O
locally	O
or	O
globally	O
normalised	O
distribution	B
computing	O
the	O
marginal	B
likelihood	B
for	O
a	O
distribution	B
defined	O
as	O
products	O
over	O
potentials	O
f	O
f	O
f	O
f	O
x	O
x	O
px	O
z	O
z	O
x	O
f	O
the	O
normalisation	B
is	O
given	O
by	O
x	O
to	O
compute	O
this	O
summation	O
efficiently	O
we	O
take	O
the	O
product	O
of	O
all	O
incoming	O
messages	O
to	O
an	O
arbitrarily	O
chosen	O
variable	O
x	O
and	O
then	O
sum	O
over	O
the	O
states	O
of	O
that	O
variable	O
z	O
x	O
f	O
nex	O
f	O
x	O
if	O
the	O
factor	B
graph	B
is	O
derived	O
from	O
setting	O
a	O
subset	O
of	O
variables	O
of	O
a	O
bn	O
in	O
evidential	O
states	O
px	O
pxv	O
pv	O
then	O
the	O
summation	O
over	O
all	O
non-evidential	O
variables	O
will	O
yield	O
the	O
marginal	B
on	O
the	O
visible	B
variables	O
pv	O
for	O
this	O
method	O
to	O
work	O
the	O
absolute	O
relative	O
values	O
of	O
the	O
messages	O
are	O
required	O
which	O
prohibits	O
renormalisation	O
at	O
each	O
stage	O
of	O
the	O
message	B
passing	B
procedure	O
however	O
without	O
normalisation	B
the	O
draft	O
march	O
marginal	B
inference	B
a	O
d	O
b	O
c	O
a	O
b	O
c	O
figure	O
factor	B
graph	B
with	O
a	O
loop	O
eliminating	O
the	O
variable	O
d	O
adds	O
an	O
edge	O
between	O
a	O
and	O
c	O
demonstrating	O
that	O
in	O
general	O
one	O
cannot	O
perform	O
marginal	B
inference	B
in	O
loopy	B
graphs	O
by	O
simply	O
passing	B
messages	O
along	O
existing	O
edges	O
in	O
the	O
original	O
graph	B
numerical	B
value	B
of	O
messages	O
can	O
become	O
very	O
small	O
particularly	O
for	O
large	O
graphs	O
and	O
numerical	B
precision	B
issues	O
can	O
occur	O
a	O
remedy	O
in	O
this	O
situation	O
is	O
to	O
work	O
with	O
log	O
messages	O
for	O
this	O
the	O
variable	O
to	O
factor	B
messages	O
become	O
simply	O
log	O
g	O
x	O
g	O
x	O
g	O
g	O
x	O
f	O
x	O
f	O
f	O
x	O
f	O
f	O
f	O
x	O
log	O
f	O
f	O
y	O
x	O
fx	O
naively	O
one	O
may	O
write	O
more	O
care	O
is	O
required	O
for	O
the	O
factors	O
to	O
variable	O
messages	O
which	O
are	O
defined	O
by	O
y	O
x	O
fx	O
y	O
y	O
f	O
y	O
y	O
f	O
however	O
the	O
exponentiation	O
of	O
the	O
log	O
messages	O
will	O
cause	O
potential	B
numerical	B
precision	B
problems	O
a	O
solution	O
to	O
this	O
numerical	B
difficulty	O
is	O
obtained	O
by	O
finding	O
the	O
largest	O
value	B
of	O
the	O
incoming	O
log	O
messages	O
y	O
f	O
max	O
y	O
y	O
f	O
then	O
f	O
x	O
y	O
x	O
fx	O
y	O
f	O
log	O
f	O
f	O
y	O
y	O
f	O
y	O
f	O
by	O
construction	B
the	O
terms	O
e	O
contributions	O
to	O
the	O
summation	O
are	O
computed	O
accurately	O
y	O
y	O
f	O
y	O
f	O
will	O
be	O
this	O
ensures	O
that	O
the	O
dominant	O
numerical	B
log	O
marginals	O
are	O
readily	O
found	O
using	O
log	O
px	O
f	O
f	O
x	O
draft	O
march	O
other	O
forms	O
of	O
inference	B
the	O
problem	B
with	O
loops	O
loops	O
cause	O
a	O
problem	B
with	O
variable	B
elimination	I
message	B
passing	B
techniques	O
since	O
once	O
a	O
variable	O
is	O
eliminated	O
the	O
structure	B
of	O
the	O
amputated	O
graph	B
in	O
general	O
changes	O
for	O
example	O
consider	O
the	O
fg	O
pa	O
b	O
c	O
d	O
b	O
c	O
d	O
d	O
the	O
marginal	B
pa	O
b	O
c	O
is	O
given	O
by	O
pa	O
b	O
c	O
b	O
d	O
d	O
d	O
which	O
adds	O
a	O
link	O
ac	O
in	O
the	O
amputated	O
graph	B
see	O
this	O
means	O
that	O
one	O
cannot	O
account	O
for	O
information	O
from	O
variable	O
d	O
by	O
simply	O
updating	O
potentials	O
on	O
links	O
in	O
the	O
original	O
graph	B
one	O
needs	O
to	O
account	O
for	O
the	O
fact	O
that	O
the	O
structure	B
of	O
the	O
graph	B
changes	O
the	O
junction	B
tree	B
algorithm	B
is	O
a	O
widely	O
used	O
technique	O
to	O
deal	O
with	O
this	O
and	O
essentially	O
combines	O
variables	O
together	O
in	O
order	O
to	O
make	O
a	O
new	O
singly-connected	B
graph	B
for	O
which	O
the	O
graph	B
structure	B
remains	O
singly-connected	B
under	O
variable	B
elimination	I
other	O
forms	O
of	O
inference	B
max-product	B
a	O
common	O
interest	O
is	O
the	O
most	B
likely	I
state	I
of	O
distribution	B
that	O
is	O
argmax	O
p	O
xn	O
to	O
compute	O
this	O
efficiently	O
we	O
exploit	O
any	O
factorisation	O
structure	B
of	O
the	O
distribution	B
analogous	O
to	O
the	O
sum-product	B
algorithm	B
that	O
is	O
we	O
aim	O
to	O
distribute	O
the	O
maximization	O
so	O
that	O
only	O
local	B
computations	O
are	O
required	O
to	O
develop	O
the	O
algorithm	B
consider	O
a	O
function	B
which	O
can	O
be	O
represented	O
as	O
an	O
undirected	B
chain	B
for	O
which	O
we	O
wish	O
to	O
find	O
the	O
joint	B
state	O
x	O
which	O
maximises	O
f	O
firstly	O
we	O
calculate	O
the	O
maximum	O
value	B
of	O
f	O
since	O
potentials	O
are	O
non-negative	O
we	O
may	O
write	O
max	O
x	O
fx	O
max	O
max	O
max	O
max	O
max	O
max	O
max	O
max	O
the	O
final	O
equation	B
corresponds	O
to	O
solving	B
a	O
single	O
variable	O
optimisation	B
and	O
determines	O
both	O
the	O
optimal	O
value	B
of	O
the	O
function	B
f	O
and	O
also	O
the	O
optimal	O
state	O
x	O
given	O
x	O
the	O
optimal	O
is	O
given	O
by	O
x	O
and	O
so	O
on	O
this	O
procedure	O
is	O
called	O
backtracking	B
note	O
that	O
we	O
could	O
have	O
equally	O
started	O
at	O
the	O
other	O
end	O
of	O
the	O
chain	B
by	O
defining	O
messages	O
that	O
pass	O
information	O
from	O
xi	O
to	O
and	O
similarly	O
x	O
argmax	O
argmax	O
argmax	O
the	O
chain	B
structure	B
of	O
the	O
function	B
ensures	O
that	O
the	O
maximal	O
value	B
its	O
state	O
can	O
be	O
computed	O
in	O
time	O
which	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
factors	O
in	O
the	O
function	B
there	O
is	O
no	O
requirement	O
here	O
that	O
the	O
function	B
f	O
corresponds	O
to	O
a	O
probability	O
distribution	B
the	O
factors	O
must	O
be	O
non-negative	O
draft	O
march	O
other	O
forms	O
of	O
inference	B
example	O
consider	O
a	O
distribution	B
defined	O
over	O
binary	O
variables	O
pa	O
b	O
c	O
pabpbcpc	O
with	O
pa	O
trb	O
tr	O
pa	O
trb	O
fa	O
pb	O
trc	O
tr	O
pb	O
trc	O
fa	O
pc	O
tr	O
what	O
is	O
the	O
most	O
likely	O
joint	B
configuration	O
argmax	O
pa	O
b	O
c	O
abc	O
naively	O
we	O
could	O
evaluate	O
pa	O
b	O
c	O
over	O
all	O
the	O
joint	B
states	O
of	O
a	O
b	O
c	O
and	O
select	O
that	O
states	O
with	O
highest	O
probability	O
a	O
message	B
passing	B
approach	B
is	O
to	O
define	O
max	O
c	O
pbcpc	O
for	O
the	O
state	O
b	O
tr	O
pb	O
trc	O
trpc	O
tr	O
pb	O
trc	O
fapc	O
fa	O
hence	O
tr	O
similarly	O
for	O
b	O
fa	O
pb	O
fac	O
trpc	O
tr	O
pb	O
fac	O
fapc	O
fa	O
hence	O
fa	O
we	O
now	O
consider	O
max	O
b	O
pab	O
for	O
a	O
tr	O
the	O
state	O
b	O
tr	O
has	O
value	B
pa	O
trb	O
tr	O
tr	O
and	O
state	O
b	O
fa	O
has	O
value	B
pa	O
trb	O
fa	O
fa	O
hence	O
tr	O
similarly	O
for	O
a	O
fa	O
the	O
state	O
b	O
tr	O
has	O
value	B
pa	O
fab	O
tr	O
tr	O
and	O
state	O
b	O
fa	O
has	O
value	B
pa	O
fab	O
fa	O
fa	O
giving	O
fa	O
now	O
we	O
can	O
compute	O
the	O
optimal	O
state	O
argmax	O
a	O
a	O
fa	O
given	O
this	O
optimal	O
state	O
we	O
can	O
backtrack	O
giving	O
argmax	O
b	O
b	O
pa	O
fab	O
fa	O
c	O
argmax	O
c	O
pb	O
facpc	O
fa	O
note	O
that	O
in	O
the	O
backtracking	B
process	O
we	O
already	O
have	O
all	O
the	O
information	O
required	O
from	O
the	O
computation	O
of	O
the	O
messages	O
draft	O
march	O
other	O
forms	O
of	O
inference	B
using	O
a	O
factor	B
graph	B
one	O
can	O
also	O
use	O
the	O
factor	B
graph	B
to	O
compute	O
the	O
joint	B
most	O
probable	O
state	O
provided	O
that	O
a	O
full	O
schedule	B
of	O
message	B
passing	B
has	O
occurred	O
the	O
product	O
of	O
messages	O
into	O
a	O
variable	O
equals	O
the	O
maximum	O
value	B
of	O
the	O
joint	B
function	B
with	O
respect	O
to	O
all	O
other	O
variables	O
one	O
can	O
then	O
simply	O
read	O
off	O
the	O
most	O
probable	O
state	O
by	O
maximising	O
this	O
local	B
potential	B
one	O
then	O
proceeds	O
in	O
computing	O
the	O
messages	O
in	O
a	O
schedule	B
that	O
allows	O
the	O
computation	O
of	O
a	O
new	O
message	B
based	O
on	O
previously	O
computed	O
messages	O
until	O
all	O
messages	O
from	O
all	O
factors	O
to	O
variables	O
and	O
vice-versa	O
have	O
been	O
computed	O
the	O
message	B
updates	O
are	O
given	O
below	O
definition	O
messages	O
on	O
factor	B
graphs	O
x	O
provided	O
the	O
given	O
a	O
distribution	B
defined	O
as	O
a	O
product	O
on	O
subsets	O
of	O
the	O
variables	O
px	O
factor	B
graph	B
is	O
singly-connected	B
we	O
can	O
carry	O
out	O
maximisation	B
over	O
the	O
variables	O
efficiently	O
f	O
f	O
z	O
initialisation	O
messages	O
from	O
extremal	B
node	O
factors	O
are	O
initialised	O
to	O
the	O
factor	B
messages	O
from	O
extremal	B
variable	O
nodes	O
are	O
set	O
to	O
unity	O
variable	O
to	O
factor	B
message	B
x	O
f	O
g	O
g	O
x	O
f	O
xx	O
x	O
x	O
f	O
f	O
x	O
x	O
x	O
f	O
factor	B
to	O
variable	O
message	B
f	O
x	O
max	O
y	O
x	O
fx	O
f	O
f	O
y	O
maximal	O
state	O
x	O
argmax	O
x	O
f	O
nex	O
f	O
x	O
y	O
f	O
f	O
y	O
f	O
y	O
xx	O
x	O
f	O
x	O
y	O
fy	O
f	O
f	O
x	O
x	O
x	O
x	O
in	O
earlier	O
literature	O
this	O
algorithm	B
is	O
called	O
belief	O
revision	O
finding	O
the	O
n	B
most	I
probable	I
states	I
it	O
is	O
often	O
of	O
interest	O
to	O
calculate	O
not	O
just	O
the	O
most	O
likely	O
joint	B
state	O
but	O
the	O
n	B
most	I
probable	I
states	I
particularly	O
in	O
cases	O
where	O
the	O
optimal	O
state	O
is	O
only	O
slightly	O
more	O
probable	O
than	O
other	O
states	O
this	O
is	O
an	O
interesting	O
problem	B
in	O
itself	O
and	O
can	O
be	O
tackled	O
with	O
a	O
variety	O
of	O
methods	O
a	O
general	O
technique	O
is	O
given	O
by	O
which	O
is	O
based	O
on	O
the	O
junction	B
tree	B
formalism	O
and	O
the	O
construction	B
of	O
candidate	O
lists	O
see	O
for	O
example	O
draft	O
march	O
other	O
forms	O
of	O
inference	B
figure	O
state	O
transition	O
diagram	O
not	O
shown	O
the	O
shortest	B
path	B
from	O
state	O
to	O
state	O
is	O
considered	O
as	O
a	O
markov	B
chain	B
walk	O
the	O
most	O
probable	O
path	B
from	O
state	O
to	O
state	O
is	O
the	O
latter	O
path	B
is	O
longer	O
but	O
more	O
probable	O
since	O
for	O
the	O
path	B
the	O
probability	O
of	O
exiting	O
from	O
state	O
into	O
state	O
is	O
each	O
transition	O
is	O
equally	O
likely	O
see	O
demomostprobablepath	O
m	O
for	O
singly-connected	B
structures	O
several	O
approaches	O
have	O
been	O
developed	O
for	O
the	O
hidden	B
markov	I
model	B
a	O
simple	O
algorithm	B
is	O
the	O
n-viterbi	O
approach	B
which	O
stores	O
the	O
n-most	O
probable	O
messages	O
at	O
each	O
stage	O
of	O
the	O
propagation	B
see	O
for	O
example	O
a	O
special	O
case	O
of	O
nilsson	O
s	O
approach	B
is	O
available	O
for	O
hidden	B
markov	O
which	O
is	O
the	O
particularly	O
efficient	O
for	O
large	O
state	O
spaces	O
for	O
more	O
general	O
singly-connected	B
graphs	O
one	O
can	O
extend	O
the	O
max-product	B
algorithm	B
to	O
an	O
n-max-product	B
algorithm	B
by	O
retaining	O
at	O
each	O
stage	O
the	O
n	B
most	I
probable	I
messages	O
see	O
below	O
these	O
techniques	O
require	O
n	O
to	O
be	O
specified	O
a-priori	O
compared	O
to	O
anytime	O
alternatives	O
an	O
alternative	O
approach	B
for	O
singlyconnected	O
networks	O
was	O
developed	O
in	O
of	O
particular	O
interest	O
is	O
the	O
application	O
of	O
the	O
singly-connected	B
algorithms	O
as	O
an	O
approximation	B
when	O
for	O
example	O
nilsson	O
s	O
approach	B
on	O
a	O
multiply-connected	B
graph	B
is	O
n-max-product	B
the	O
algorithm	B
for	O
n-max-product	B
is	O
a	O
straightforward	O
modification	O
of	O
the	O
standard	O
algorithms	O
computationally	O
a	O
straightforward	O
way	O
to	O
accomplish	O
this	O
is	O
to	O
introduce	O
an	O
additional	O
variable	O
for	O
each	O
message	B
that	O
is	O
used	O
to	O
index	O
the	O
most	O
likely	O
messages	O
for	O
example	O
consider	O
the	O
distribution	B
pa	O
b	O
c	O
d	O
b	O
c	O
d	O
for	O
which	O
we	O
wish	O
to	O
find	O
the	O
two	O
most	O
probable	O
values	O
using	O
the	O
notation	O
imax	O
x	O
fx	O
for	O
the	O
ith	O
highest	O
value	B
of	O
fx	O
the	O
maximisation	B
over	O
d	O
can	O
be	O
expressed	O
using	O
the	O
message	B
db	O
d	O
d	O
db	O
d	O
d	O
using	O
a	O
similar	O
message	B
for	O
the	O
maximisation	B
over	O
c	O
the	O
most	O
likely	O
states	O
of	O
pa	O
b	O
c	O
d	O
can	O
be	O
computed	O
using	O
abcd	O
abmbmd	O
b	O
db	O
md	O
cb	O
mc	O
where	O
mc	O
and	O
md	O
index	O
the	O
highest	O
values	O
at	O
the	O
final	O
stage	O
we	O
now	O
have	O
a	O
table	O
with	O
dim	O
a	O
dim	O
b	O
entries	O
from	O
which	O
we	O
compute	O
the	O
highest	O
two	O
states	O
the	O
generalisation	B
of	O
this	O
to	O
the	O
factor	B
graph	B
formalism	O
is	O
straightforward	O
and	O
contained	O
in	O
maxnprodfg	O
m	O
essentially	O
the	O
only	O
modification	O
required	O
is	O
to	O
define	O
extended	O
messages	O
which	O
contain	O
the	O
n-most	O
likely	O
messages	O
computed	O
at	O
each	O
stage	O
at	O
a	O
junction	O
of	O
the	O
factor	B
graph	B
all	O
the	O
messages	O
from	O
the	O
neighbours	O
along	O
with	O
their	O
n-most	O
probable	O
tables	O
are	O
multiplied	O
together	O
into	O
a	O
large	O
table	O
for	O
a	O
factor	B
to	O
variable	O
message	B
the	O
n-most	O
probable	O
messages	O
are	O
retained	O
see	O
maxnprodfg	O
m	O
the	O
n-most	O
probable	O
states	O
for	O
each	O
variable	O
can	O
then	O
be	O
read	O
off	O
by	O
finding	O
the	O
variable	O
state	O
that	O
maximises	O
the	O
product	O
of	O
incoming	O
extended	O
messages	O
draft	O
march	O
other	O
forms	O
of	O
inference	B
most	O
probable	O
path	B
and	O
shortest	B
path	B
what	O
is	O
the	O
most	O
likely	O
path	B
from	O
state	O
a	O
to	O
state	O
b	O
for	O
an	O
n	O
state	O
markov	B
chain	B
note	O
that	O
this	O
is	O
not	O
necessarily	O
the	O
same	O
as	O
the	O
shortest	B
path	B
as	O
explained	O
in	O
if	O
assume	O
that	O
a	O
length	O
t	O
path	B
exists	O
this	O
has	O
probability	O
pst	O
bst	O
finding	O
the	O
most	O
probable	O
path	B
can	O
then	O
be	O
readily	O
solved	O
using	O
the	O
max-product	B
max-sum	B
algorithm	B
for	O
the	O
log-transitions	O
on	O
a	O
simple	O
serial	O
factor	B
graph	B
to	O
deal	O
with	O
the	O
issue	O
that	O
we	O
don	O
t	O
know	O
the	O
optimal	O
t	O
one	O
approach	B
is	O
to	O
redefine	O
the	O
probability	O
transitions	O
such	O
that	O
the	O
desired	O
state	O
b	O
is	O
an	O
absorbing	B
state	I
of	O
the	O
chain	B
is	O
one	O
can	O
enter	O
this	O
state	O
but	O
not	O
leave	O
it	O
with	O
this	O
redefinition	O
the	O
most	O
probable	O
joint	B
state	O
will	O
correspond	O
to	O
the	O
most	O
probable	O
state	O
on	O
the	O
product	O
of	O
n	O
transitions	O
once	O
the	O
absorbing	B
state	I
is	O
reached	O
the	O
chain	B
will	O
stay	O
in	O
this	O
state	O
and	O
hence	O
the	O
most	O
probable	O
path	B
can	O
be	O
read	O
off	O
from	O
the	O
sequence	O
of	O
states	O
up	O
to	O
the	O
first	O
time	O
the	O
chain	B
hits	O
the	O
absorbing	B
state	I
this	O
approach	B
is	O
demonstrated	O
in	O
demomostprobablepath	O
m	O
along	O
with	O
the	O
more	O
direct	O
approaches	O
described	O
below	O
an	O
alternative	O
cleaner	O
approach	B
is	O
as	O
follows	O
for	O
the	O
markov	B
chain	B
we	O
can	O
dispense	O
with	O
variable-to-factor	O
and	O
factor-to-variable	O
messages	O
and	O
use	O
only	O
variable-to-variable	O
messages	O
if	O
we	O
want	O
to	O
find	O
the	O
most	O
likely	O
set	O
of	O
states	O
a	O
st	O
b	O
to	O
get	O
us	O
there	O
then	O
this	O
can	O
be	O
computed	O
by	O
defining	O
the	O
maximal	O
path	B
probability	O
e	O
b	O
t	O
to	O
get	O
from	O
a	O
to	O
b	O
in	O
t	O
e	O
b	O
t	O
max	O
max	O
pst	O
bst	O
max	O
pst	O
bst	O
to	O
compute	O
this	O
efficiently	O
we	O
define	O
messages	O
t	O
t	O
t	O
max	O
st	O
t	O
a	O
until	O
the	O
point	O
e	O
b	O
t	O
max	O
st	O
t	O
t	O
pst	O
bst	O
t	O
t	O
b	O
we	O
can	O
now	O
proceed	O
to	O
find	O
the	O
maximal	O
path	B
probability	O
for	O
timestep	O
t	O
since	O
the	O
messages	O
up	O
to	O
time	O
t	O
will	O
be	O
the	O
same	O
as	O
before	O
we	O
need	O
only	O
compute	O
one	O
additional	O
message	B
t	O
t	O
from	O
which	O
e	O
b	O
t	O
max	O
st	O
t	O
t	O
pst	O
bst	O
t	O
t	O
b	O
we	O
can	O
proceed	O
in	O
this	O
manner	O
until	O
we	O
reach	O
e	O
b	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
nodes	O
in	O
the	O
graph	B
we	O
don	O
t	O
need	O
to	O
go	O
beyond	O
this	O
number	O
of	O
steps	O
since	O
those	O
that	O
do	O
must	O
necessarily	O
contain	O
non-simple	O
paths	O
simple	B
path	B
is	O
one	O
that	O
does	O
not	O
include	O
the	O
same	O
state	O
more	O
than	O
once	O
the	O
optimal	O
time	O
t	O
is	O
then	O
given	O
by	O
which	O
of	O
e	O
b	O
e	O
b	O
n	O
is	O
maximal	O
given	O
t	O
one	O
can	O
begin	O
to	O
since	O
e	O
b	O
t	O
max	O
st	O
t	O
t	O
pst	O
bst	O
we	O
know	O
the	O
optimal	O
state	O
s	O
t	O
argmax	O
st	O
t	O
t	O
pst	O
bst	O
alternative	O
to	O
finding	O
t	O
is	O
to	O
define	O
self-transitions	O
with	O
probability	O
and	O
then	O
use	O
a	O
fixed	O
time	O
t	O
n	O
once	O
the	O
desired	O
state	O
b	O
is	O
reached	O
the	O
self-transition	O
then	O
preserves	O
the	O
chain	B
in	O
state	O
b	O
for	O
the	O
remaining	O
timesteps	O
this	O
procedure	O
is	O
used	O
in	O
mostprobablepathmult	O
m	O
draft	O
march	O
we	O
can	O
then	O
continue	O
to	O
backtrack	O
s	O
t	O
argmax	O
st	O
t	O
t	O
ps	O
t	O
and	O
so	O
on	O
see	O
mostprobablepath	O
m	O
other	O
forms	O
of	O
inference	B
in	O
the	O
above	O
derivation	O
we	O
do	O
not	O
use	O
any	O
properties	B
of	O
probability	O
except	O
that	O
p	O
must	O
be	O
nonnegative	O
sign	O
changes	O
can	O
flip	O
a	O
whole	O
sequence	O
probability	O
and	O
the	O
local	B
message	B
recursion	O
no	O
longer	O
applies	O
one	O
can	O
consider	O
the	O
algorithm	B
as	O
finding	O
the	O
optimal	O
product	O
path	B
from	O
a	O
to	O
b	O
it	O
is	O
straightforward	O
to	O
modify	O
the	O
algorithm	B
to	O
solve	O
the	O
single-sink	O
shortest	B
weighted	I
path	B
problem	B
one	O
way	O
to	O
do	O
this	O
is	O
to	O
replace	O
the	O
markov	O
transition	O
probabilities	O
with	O
edge	O
weights	O
exp	O
ustst	O
where	O
ustst	O
is	O
infinite	O
if	O
there	O
is	O
no	O
edge	O
from	O
st	O
to	O
st	O
this	O
approach	B
is	O
taken	O
in	O
shortestpath	O
m	O
which	O
is	O
able	O
to	O
deal	O
with	O
either	O
positive	O
or	O
negative	O
edge	O
weights	O
this	O
method	O
is	O
therefore	O
more	O
general	O
than	O
the	O
well-known	O
dijkstra	O
s	O
algorithm	B
which	O
requires	O
weights	O
to	O
be	O
positive	O
if	O
a	O
negative	O
edge	O
cycle	O
exists	O
the	O
code	O
returns	O
the	O
shortest	O
weighted	O
length	O
n	O
path	B
where	O
n	O
is	O
the	O
number	O
of	O
nodes	O
in	O
the	O
graph	B
see	O
demoshortestpath	O
m	O
the	O
above	O
algorithm	B
is	O
efficient	O
for	O
the	O
single-source	O
single-sink	O
scenario	O
since	O
the	O
messages	O
contain	O
only	O
n	O
states	O
meaning	O
that	O
the	O
overall	O
storage	O
is	O
on	O
as	O
it	O
stands	O
the	O
algorithm	B
is	O
numerically	O
impractical	O
since	O
the	O
messages	O
are	O
recursively	O
multiplied	O
by	O
values	O
usually	O
less	O
than	O
least	O
for	O
the	O
case	O
of	O
probabilities	O
one	O
will	O
therefore	O
quickly	O
run	O
into	O
numerical	B
underflow	O
possibly	O
overflow	O
in	O
the	O
case	O
of	O
non-probabilities	O
with	O
this	O
method	O
to	O
fix	O
the	O
final	O
point	O
above	O
it	O
is	O
best	O
to	O
work	O
by	O
defining	O
the	O
logarithm	O
of	O
e	O
since	O
this	O
is	O
a	O
monotonic	O
transformation	O
the	O
most	O
probable	O
path	B
defined	O
through	O
log	O
e	O
is	O
the	O
same	O
as	O
that	O
obtained	O
from	O
e	O
in	O
this	O
case	O
l	O
b	O
t	O
max	O
max	O
log	O
pst	O
bst	O
log	O
a	O
log	O
pstst	O
log	O
pst	O
bst	O
t	O
we	O
can	O
therefore	O
define	O
new	O
messages	O
t	O
max	O
st	O
t	O
t	O
log	O
one	O
then	O
proceeds	O
as	O
before	O
by	O
finding	O
the	O
most	O
probable	O
t	O
defined	O
on	O
l	O
and	O
backtracks	O
graphical	O
model	B
corresponding	O
to	O
this	O
simple	O
markov	B
chain	B
is	O
the	O
belief	B
network	I
remark	O
a	O
possible	O
confusion	O
is	O
that	O
optimal	O
paths	O
can	O
be	O
efficiently	O
found	O
when	O
the	O
graph	B
is	O
loopy	B
note	O
that	O
the	O
graph	B
in	O
is	O
a	O
state-transition	O
diagram	O
not	O
a	O
graphical	O
model	B
the	O
t	O
pstst	O
a	O
linear	B
serial	O
structure	B
hence	O
the	O
underlying	O
graphical	O
model	B
is	O
a	O
simple	O
chain	B
which	O
explains	O
why	O
computation	O
is	O
efficient	O
most	O
probable	O
path	B
multiple-sink	O
if	O
we	O
need	O
the	O
most	O
probable	O
path	B
between	O
all	O
states	O
a	O
and	O
b	O
one	O
could	O
re-run	O
the	O
above	O
single-sourcesingle-sink	O
algorithm	B
for	O
all	O
a	O
and	O
b	O
a	O
computationally	O
more	O
efficient	O
approach	B
is	O
to	O
observe	O
that	O
one	O
can	O
define	O
a	O
message	B
for	O
each	O
starting	O
state	O
a	O
t	O
max	O
st	O
t	O
t	O
draft	O
march	O
other	O
forms	O
of	O
inference	B
algorithm	B
compute	O
marginal	B
from	O
distribution	B
px	O
procedure	O
bucket	O
eliminationpx	O
evidential	O
variables	O
are	O
ordered	O
xn	O
f	O
f	O
initialize	O
all	O
bucket	O
potentials	O
to	O
unity	O
while	O
there	O
are	O
potentials	O
left	O
in	O
the	O
distribution	B
do	O
for	O
each	O
potential	B
f	O
its	O
highest	O
variable	O
xj	O
to	O
the	O
ordering	O
multiply	O
f	O
with	O
the	O
potential	B
in	O
bucket	O
j	O
and	O
remove	O
f	O
the	O
distribution	B
end	O
while	O
for	O
i	O
bucket	O
n	O
to	O
do	O
for	O
bucket	O
i	O
sum	O
over	O
the	O
states	O
of	O
variable	O
xi	O
and	O
call	O
this	O
potential	B
i	O
identify	O
the	O
highest	O
variable	O
xh	O
of	O
potential	B
i	O
multiply	O
the	O
existing	O
potential	B
in	O
bucket	O
h	O
by	O
i	O
end	O
for	O
the	O
marginal	B
is	O
proportional	O
to	O
return	O
end	O
procedure	O
f	O
f	O
assumes	O
non	O
fill	O
buckets	O
empty	O
buckets	O
the	O
conditional	B
marginal	B
and	O
continue	O
until	O
we	O
find	O
the	O
maximal	O
path	B
probability	O
matrix	B
for	O
getting	O
from	O
any	O
state	O
a	O
to	O
any	O
state	O
b	O
in	O
t	O
timesteps	O
e	O
b	O
t	O
max	O
st	O
t	O
t	O
pst	O
bst	O
since	O
we	O
know	O
the	O
message	B
t	O
t	O
for	O
all	O
states	O
a	O
we	O
can	O
readily	O
compute	O
the	O
most	O
probable	O
path	B
from	O
all	O
starting	O
states	O
a	O
to	O
all	O
states	O
b	O
after	O
t	O
steps	O
this	O
requires	O
passing	B
an	O
n	O
n	O
matrix	B
message	B
we	O
can	O
then	O
proceed	O
to	O
the	O
next	O
timestep	O
t	O
since	O
the	O
messages	O
up	O
to	O
time	O
t	O
will	O
be	O
the	O
same	O
as	O
before	O
we	O
need	O
only	O
compute	O
one	O
additional	O
message	B
t	O
t	O
from	O
which	O
e	O
b	O
t	O
max	O
st	O
t	O
t	O
pst	O
bst	O
in	O
this	O
way	O
one	O
can	O
then	O
efficiently	O
compute	O
the	O
optimal	O
path	B
probabilities	O
for	O
any	O
starting	O
state	O
a	O
and	O
end	O
state	O
b	O
after	O
t	O
timesteps	O
to	O
find	O
the	O
optimal	O
corresponding	O
path	B
backtracking	B
proceeds	O
as	O
before	O
see	O
mostprobablepathmult	O
m	O
one	O
can	O
also	O
use	O
the	O
same	O
algorithm	B
to	O
solve	O
the	O
multiple-source	O
multiple	O
sink	O
shortest	B
path	B
problem	B
this	O
algorithm	B
is	O
a	O
variant	O
of	O
the	O
floyd-warshall-roy	O
for	O
finding	O
shortest	O
weighted	O
summed	O
paths	O
on	O
a	O
directed	B
graph	B
above	O
algorithm	B
enumerates	O
through	O
time	O
whereas	O
the	O
fwr	O
algorithm	B
enumerates	O
through	O
states	O
mixed	B
inference	B
an	O
often	O
encountered	O
situation	O
is	O
to	O
infer	O
the	O
most	B
likely	I
state	I
of	O
a	O
joint	B
marginal	B
possibly	O
given	O
some	O
evidence	O
for	O
example	O
given	O
a	O
distribution	B
xn	O
find	O
argmax	O
xm	O
argmax	O
xn	O
in	O
general	O
even	O
for	O
tree	B
structured	B
xn	O
the	O
optimal	O
marginal	B
state	O
cannot	O
be	O
computed	O
efficiently	O
one	O
way	O
to	O
see	O
this	O
is	O
that	O
due	O
to	O
the	O
summation	O
the	O
resulting	O
joint	B
marginal	B
does	O
not	O
have	O
a	O
structured	B
factored	O
form	O
as	O
products	O
of	O
simpler	O
functions	O
of	O
the	O
marginal	B
variables	O
finding	O
the	O
most	O
probable	O
joint	B
marginal	B
then	O
requires	O
a	O
search	O
over	O
all	O
the	O
joint	B
marginal	B
states	O
a	O
task	O
exponential	B
in	O
m	O
an	O
approximate	B
solution	O
is	O
provided	O
by	O
the	O
em	B
algorithm	B
and	O
draft	O
march	O
inference	B
in	O
multiply-connected	B
graphs	O
e	O
c	O
b	O
g	O
a	O
d	O
f	O
pepgd	O
e	O
pca	B
pbpda	O
b	O
pbpda	O
b	O
e	O
g	O
e	O
g	O
pa	O
pfd	O
pa	O
pa	O
b	O
a	O
pa	O
b	O
a	O
pfd	O
pfd	O
pfd	O
g	O
pfd	O
g	O
a	O
d	O
node	O
is	O
eliminated	O
from	O
the	O
graph	B
the	O
second	O
stage	O
of	O
eliminating	O
c	O
is	O
trivial	O
since	O
figure	O
the	O
bucket	B
elimination	I
algorithm	B
applied	O
to	O
the	O
graph	B
at	O
each	O
stage	O
at	O
least	O
one	O
c	O
pca	B
and	O
has	O
therefore	O
been	O
skipped	O
over	O
since	O
this	O
bucket	O
does	O
not	O
send	O
any	O
message	B
inference	B
in	O
multiply-connected	B
graphs	O
bucket	B
elimination	I
we	O
consider	O
here	O
a	O
general	O
conditional	B
marginal	B
variable	B
elimination	I
method	O
that	O
works	O
for	O
any	O
distribution	B
multiply	O
connected	B
graphs	O
the	O
algorithm	B
assumes	O
the	O
distribution	B
is	O
in	O
the	O
form	O
f	O
xn	O
and	O
that	O
the	O
task	O
is	O
to	O
compute	O
for	O
example	O
for	O
we	O
could	O
use	O
the	O
sets	O
of	O
variables	O
here	O
are	O
in	O
general	O
the	O
construction	B
of	O
potentials	O
for	O
a	O
distribution	B
is	O
not	O
unique	O
the	O
task	O
of	O
computing	O
a	O
marginal	B
in	O
which	O
a	O
set	O
of	O
variables	O
are	O
clamped	O
to	O
their	O
evidential	O
states	O
is	O
evidence	O
f	O
f	O
the	O
algorithm	B
is	O
given	O
in	O
and	O
can	O
be	O
considered	O
a	O
way	O
to	O
organise	O
the	O
distributed	O
the	O
algorithm	B
is	O
best	O
explained	O
by	O
a	O
simple	O
example	O
as	O
given	O
below	O
example	O
elimination	O
consider	O
the	O
problem	B
of	O
calculating	O
the	O
marginal	B
pf	O
of	O
pa	O
b	O
c	O
d	O
e	O
f	O
g	O
pfdpgd	O
epcapda	O
bpapbpe	O
see	O
pf	O
pa	O
b	O
c	O
d	O
e	O
f	O
g	O
abcdeg	O
abcdeg	O
pfdpgd	O
epcapda	O
bpapbpe	O
draft	O
march	O
inference	B
in	O
multiply-connected	B
graphs	O
pf	O
terms	O
we	O
can	O
write	O
pf	O
adg	O
pf	O
pf	O
d	O
d	O
we	O
can	O
distribute	O
the	O
summation	O
over	O
the	O
various	O
terms	O
as	O
follows	O
eb	O
and	O
c	O
are	O
end	O
nodes	O
so	O
that	O
we	O
can	O
sum	O
over	O
their	O
values	O
b	O
pgd	O
epe	O
b	O
pda	O
bpb	O
b	O
d	O
e	O
pfdpa	O
adg	O
for	O
convenience	O
lets	O
write	O
the	O
terms	O
in	O
the	O
brackets	O
as	O
e	O
g	O
the	O
c	O
pda	O
bpb	O
pca	B
e	O
pgd	O
epe	O
c	O
pca	B
is	O
equal	O
to	O
unity	O
and	O
we	O
therefore	O
eliminate	O
this	O
node	O
directly	O
rearranging	O
pfdpa	O
b	O
d	O
e	O
g	O
if	O
we	O
think	O
of	O
this	O
graphically	O
the	O
effect	O
of	O
summing	O
over	O
b	O
c	O
e	O
is	O
effectively	O
to	O
remove	O
or	O
eliminate	O
those	O
variables	O
we	O
can	O
now	O
carry	O
on	O
summing	O
over	O
a	O
and	O
g	O
since	O
these	O
are	O
end	O
points	O
of	O
the	O
new	O
graph	B
pfd	O
pa	O
b	O
d	O
e	O
g	O
a	O
g	O
again	O
this	O
defines	O
new	O
functions	O
a	O
g	O
so	O
that	O
the	O
final	O
answer	O
can	O
be	O
found	O
from	O
pfd	O
a	O
g	O
we	O
illustrate	O
this	O
in	O
initially	O
we	O
define	O
an	O
ordering	O
of	O
the	O
variables	O
beginning	O
with	O
the	O
one	O
that	O
we	O
wish	O
to	O
find	O
the	O
marginal	B
for	O
a	O
suitable	O
ordering	O
is	O
therefore	O
f	O
d	O
a	O
g	O
b	O
c	O
e	O
then	O
starting	O
with	O
the	O
highest	O
bucket	O
e	O
to	O
our	O
ordering	O
f	O
d	O
a	O
g	O
b	O
c	O
e	O
we	O
put	O
all	O
the	O
functions	O
that	O
mention	O
e	O
in	O
the	O
e	O
bucket	O
continuing	O
with	O
the	O
next	O
highest	O
bucket	O
c	O
we	O
put	O
all	O
the	O
remaining	O
functions	O
that	O
mention	O
c	O
in	O
this	O
c	O
bucket	O
etc	O
the	O
result	O
of	O
this	O
initialisation	O
procedure	O
is	O
that	O
terms	O
distributions	O
in	O
the	O
dag	O
are	O
distributed	O
over	O
the	O
buckets	O
as	O
shown	O
in	O
the	O
left	O
most	O
column	O
of	O
eliminating	O
then	O
the	O
highest	O
bucket	O
e	O
we	O
pass	O
a	O
message	B
to	O
node	O
g	O
immediately	O
we	O
can	O
also	O
eliminate	O
bucket	O
c	O
since	O
this	O
sums	O
to	O
unity	O
in	O
the	O
next	O
column	O
we	O
have	O
now	O
two	O
less	O
buckets	O
and	O
we	O
eliminate	O
the	O
highest	O
remaining	O
bucket	O
this	O
time	O
b	O
passing	B
a	O
message	B
to	O
bucket	O
a	O
there	O
are	O
some	O
important	O
observations	O
we	O
can	O
make	O
about	O
bucket	B
elimination	I
to	O
compute	O
say	O
we	O
need	O
to	O
re-order	O
the	O
variables	O
that	O
the	O
required	O
marginal	B
variable	O
is	O
labelled	B
and	O
repeat	O
bucket	B
elimination	I
hence	O
each	O
query	B
of	O
a	O
marginal	B
in	O
this	O
case	O
requires	O
re-running	O
the	O
algorithm	B
it	O
would	O
be	O
more	O
efficient	O
to	O
reuse	O
messages	O
rather	O
than	O
recalculating	O
them	O
each	O
time	O
in	O
general	O
bucket	B
elimination	I
constructs	O
multi-variable	O
messages	O
from	O
bucket	O
to	O
bucket	O
the	O
storage	O
requirements	O
of	O
a	O
multi-variable	O
message	B
are	O
exponential	B
in	O
the	O
number	O
of	O
variables	O
of	O
the	O
message	B
for	O
trees	O
we	O
can	O
always	O
choose	O
a	O
variable	O
ordering	O
to	O
render	O
the	O
computational	B
complexity	I
to	O
be	O
linear	B
in	O
the	O
number	O
of	O
variables	O
such	O
an	O
ordering	O
is	O
called	O
perfect	O
and	O
indeed	O
it	O
can	O
be	O
shown	O
that	O
a	O
perfect	O
ordering	O
can	O
always	O
easily	O
be	O
found	O
for	O
singly-connected	B
graphs	O
however	O
orderings	O
exist	O
for	O
which	O
bucket	B
elimination	I
will	O
be	O
extremely	O
inefficient	O
loop-cut	O
conditioning	B
for	O
distributions	O
which	O
contain	O
a	O
loop	O
is	O
more	O
than	O
one	O
path	B
between	O
two	O
nodes	O
in	O
the	O
graph	B
when	O
the	O
directions	O
are	O
removed	O
we	O
run	O
into	O
some	O
difficulty	O
with	O
the	O
message	B
passing	B
routines	O
such	O
as	O
the	O
sum-product	B
algorithm	B
which	O
are	O
designed	O
to	O
work	O
on	O
singly-connected	B
graphs	O
only	O
one	O
way	O
to	O
solve	O
draft	O
march	O
c	O
a	O
f	O
b	O
g	O
d	O
e	O
c	O
b	O
g	O
e	O
a	O
f	O
d	O
notes	O
figure	O
a	O
multiplyconnected	O
graph	B
reduced	O
to	O
a	O
singly-connected	B
graph	B
by	O
conditioning	B
on	O
the	O
variable	O
c	O
the	O
difficulties	O
of	O
multiply	O
connected	B
graphs	O
is	O
to	O
identify	O
nodes	O
that	O
when	O
removed	O
would	O
reveal	O
a	O
singly-connected	B
consider	O
the	O
example	O
if	O
imagine	O
that	O
we	O
wish	O
to	O
calculate	O
a	O
marginal	B
say	O
pd	O
then	O
pd	O
c	O
abefg	O
pcapa	O
p	O
pda	O
bpb	O
pfc	O
d	O
p	O
pgd	O
e	O
where	O
the	O
p	O
definitions	O
are	O
not	O
necessarily	O
distributions	O
for	O
each	O
state	O
of	O
c	O
the	O
form	O
of	O
the	O
products	O
of	O
factors	O
remaining	O
as	O
a	O
function	B
of	O
a	O
b	O
e	O
f	O
g	O
is	O
singly-connected	B
so	O
that	O
standard	O
singly-connected	B
message	B
passing	B
can	O
be	O
used	O
to	O
perform	O
inference	B
we	O
will	O
need	O
to	O
do	O
perform	O
inference	B
for	O
each	O
state	O
of	O
variable	O
c	O
each	O
state	O
defining	O
a	O
new	O
singly-connected	B
graph	B
the	O
same	O
structure	B
but	O
with	O
modified	O
potentials	O
more	O
generally	O
we	O
can	O
define	O
a	O
set	O
of	O
variables	O
c	O
called	O
the	O
loop	B
cut	B
set	I
and	O
run	O
singly-connected	B
inference	B
for	O
each	O
joint	B
state	O
of	O
the	O
cut-set	O
variables	O
c	O
this	O
can	O
also	O
be	O
used	O
for	O
finding	O
the	O
most	B
likely	I
state	I
of	O
a	O
multiply-connected	B
joint	B
distribution	B
as	O
well	O
hence	O
for	O
a	O
computational	O
price	O
exponential	B
in	O
the	O
loopcut	O
size	O
we	O
can	O
calculate	O
the	O
marginals	O
the	O
most	B
likely	I
state	I
for	O
a	O
multiply-connected	B
distribution	B
however	O
determining	O
a	O
small	O
cut	B
set	O
is	O
in	O
general	O
difficult	O
and	O
there	O
is	O
no	O
guarantee	O
that	O
this	O
will	O
anyway	O
be	O
small	O
for	O
a	O
given	O
graph	B
whilst	O
this	O
method	O
is	O
able	O
to	O
handle	O
loops	O
in	O
a	O
general	O
manner	O
it	O
is	O
not	O
particularly	O
elegant	O
since	O
the	O
concept	O
of	O
messages	O
now	O
only	O
applies	O
conditioned	O
on	O
the	O
cut	B
set	O
variables	O
and	O
how	O
to	O
re-use	O
messages	O
for	O
inference	B
of	O
additional	O
quantities	O
of	O
interest	O
becomes	O
unclear	O
we	O
will	O
discuss	O
an	O
alternative	O
method	O
for	O
handling	O
multiply	O
connected	B
distributions	O
in	O
message	B
passing	B
for	O
continuous	B
distributions	O
for	O
parametric	O
continuous	B
distributions	O
px	O
x	O
message	B
passing	B
corresponds	O
to	O
passing	B
parameters	O
of	O
the	O
distributions	O
for	O
the	O
sum-product	B
algorithm	B
this	O
requires	O
that	O
the	O
operations	O
of	O
multiplication	O
and	O
integration	O
over	O
the	O
variables	O
are	O
closed	O
with	O
respect	O
to	O
the	O
family	B
of	O
distributions	O
this	O
is	O
the	O
case	O
for	O
example	O
for	O
the	O
gaussian	B
distribution	B
the	O
marginal	B
of	O
a	O
gaussian	B
is	O
another	O
gaussian	B
and	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
a	O
gaussian	B
see	O
this	O
means	O
that	O
we	O
can	O
then	O
implement	O
the	O
sum-product	B
algorithm	B
based	O
on	O
passing	B
mean	B
and	O
covariance	B
parameters	O
to	O
implement	O
this	O
requires	O
some	O
tedious	O
algebra	O
to	O
compute	O
the	O
appropriate	O
message	B
parameter	B
updates	O
at	O
this	O
stage	O
the	O
complexities	O
from	O
performing	O
such	O
calculations	O
are	O
a	O
potential	B
distraction	O
though	O
the	O
interested	O
reader	O
may	O
refer	O
to	O
demosumprodgaussmoment	O
m	O
demosumprodgausscanon	O
m	O
and	O
demosumprodgausscanonlds	O
m	O
and	O
also	O
for	O
examples	O
of	O
message	B
passing	B
with	O
gaussians	O
for	O
more	O
general	O
exponential	B
family	B
distributions	O
message	B
passing	B
is	O
essentially	O
straightforward	O
though	O
again	O
the	O
specifics	O
of	O
the	O
updates	O
may	O
be	O
tedious	O
to	O
work	O
out	O
in	O
cases	O
where	O
the	O
operations	O
of	O
marginalisation	B
and	O
products	O
are	O
not	O
closed	O
within	O
the	O
family	B
the	O
distributions	O
need	O
to	O
be	O
projected	O
back	O
to	O
the	O
chosen	O
message	B
family	B
expectation	B
propagation	B
is	O
relevant	O
in	O
this	O
case	O
notes	O
a	O
take-home	O
message	B
from	O
this	O
chapter	O
is	O
that	O
inference	B
in	O
singly-connected	B
structures	O
is	O
usually	O
computationally	O
tractable	O
notable	O
exceptions	O
are	O
when	O
the	O
message	B
passing	B
operations	O
are	O
notclosed	O
within	O
the	O
message	B
family	B
or	O
representing	O
messages	O
explicitly	O
requires	O
an	O
exponential	B
amount	O
of	O
space	O
this	O
happens	O
for	O
example	O
when	O
the	O
distribution	B
can	O
contain	O
both	O
discrete	B
and	O
continuous	B
variables	O
draft	O
march	O
code	O
such	O
as	O
the	O
switching	B
linear	B
dynamical	I
system	I
which	O
we	O
discuss	O
in	O
broadly	O
speaking	O
inference	B
in	O
multiply-connected	B
structures	O
is	O
more	O
complex	O
and	O
may	O
be	O
intractable	O
however	O
we	O
do	O
not	O
want	O
to	O
give	O
the	O
impression	O
that	O
this	O
is	O
always	O
the	O
case	O
notable	O
exceptions	O
are	O
finding	O
the	O
map	B
state	O
in	O
an	O
attractive	O
pairwise	B
mrf	O
finding	O
the	O
map	B
and	O
mpm	B
state	O
in	O
a	O
binary	O
planar	O
mrf	O
with	O
pure	O
interactions	O
see	O
for	O
example	O
for	O
n	O
variables	O
in	O
the	O
graph	B
a	O
naive	O
use	O
of	O
the	O
junction	B
tree	B
algorithm	B
for	O
these	O
inferences	O
would	O
result	O
in	O
an	O
computation	O
whereas	O
clever	O
algorithms	O
are	O
able	O
to	O
return	O
the	O
exact	O
results	O
in	O
operations	O
of	O
interest	O
is	O
bond	O
which	O
is	O
an	O
intuitive	O
node	O
elimination	O
method	O
to	O
arrive	O
at	O
the	O
mpm	B
inference	B
in	O
pureinteraction	O
ising	O
models	O
code	O
the	O
code	O
below	O
implements	O
message	B
passing	B
on	O
a	O
tree	B
structured	B
factor	B
graph	B
the	O
fg	O
is	O
stored	O
as	O
an	O
adjacency	B
matrix	B
with	O
the	O
message	B
between	O
fg	O
node	O
i	O
and	O
fg	O
node	O
j	O
given	O
in	O
aij	O
factorgraph	O
m	O
return	O
a	O
factor	B
graph	B
adjacency	B
matrix	B
and	O
message	B
numbers	O
sumprodfg	O
m	O
sum-product	B
algorithm	B
on	O
a	O
factor	B
graph	B
in	O
general	O
it	O
is	O
recommended	O
to	O
work	O
in	O
log-space	O
in	O
the	O
max-product	B
case	O
particularly	O
for	O
large	O
graphs	O
since	O
the	O
produce	O
of	O
messages	O
can	O
become	O
very	O
small	O
the	O
code	O
provided	O
does	O
not	O
work	O
in	O
log	O
space	O
and	O
as	O
such	O
may	O
not	O
work	O
on	O
large	O
graphs	O
writing	O
this	O
using	O
log-messages	O
is	O
straightforward	O
but	O
leads	O
to	O
less	O
readable	O
code	O
an	O
implementation	O
based	O
on	O
log-messages	O
is	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
maxprodfg	O
m	O
max-product	B
algorithm	B
on	O
a	O
factor	B
graph	B
maxnprodfg	O
m	O
n-max-product	B
algorithm	B
on	O
a	O
factor	B
graph	B
factor	B
graph	B
examples	O
for	O
the	O
distribution	B
from	O
the	O
following	O
code	O
finds	O
the	O
marginals	O
and	O
most	O
likely	O
joint	B
states	O
the	O
number	O
of	O
states	O
of	O
each	O
variable	O
is	O
chosen	O
at	O
random	O
demosumprod	O
m	O
test	O
the	O
sum-product	B
algorithm	B
demomaxprod	O
m	O
test	O
the	O
max-product	B
algorithm	B
demomaxnprod	O
m	O
test	O
the	O
max-n-product	O
algorithm	B
most	O
probable	O
and	O
shortest	B
path	B
mostprobablepath	O
m	O
most	O
probable	O
path	B
demomostprobablepath	O
m	O
most	O
probable	O
versus	O
shortest	B
path	B
demo	O
the	O
shortest	B
path	B
demo	O
works	O
for	O
both	O
positive	O
and	O
negative	O
edge	O
weights	O
if	O
negative	O
weight	B
cycles	O
exist	O
the	O
code	O
finds	O
the	O
best	O
length	O
n	O
shortest	B
path	B
demoshortestpath	O
m	O
shortest	B
path	B
demo	O
mostprobablepathmult	O
m	O
most	O
probable	O
path	B
multi-source	O
multi-sink	O
demomostprobablepathmult	O
m	O
demo	O
of	O
most	O
probable	O
path	B
multi-source	O
multi-sink	O
bucket	B
elimination	I
the	O
efficacy	O
of	O
bucket	B
elimination	I
depends	O
critically	O
on	O
the	O
elimination	O
sequence	O
chosen	O
in	O
the	O
demonstration	O
below	O
we	O
find	O
the	O
marginal	B
of	O
a	O
variable	O
in	O
the	O
chest	B
clinic	I
exercise	O
using	O
a	O
randomly	O
chosen	O
elimination	O
order	O
the	O
desired	O
marginal	B
variable	O
is	O
specified	O
as	O
the	O
last	O
to	O
be	O
eliminated	O
for	O
comparison	O
we	O
use	O
an	O
elimination	O
sequence	O
based	O
on	O
decimating	O
a	O
triangulated	B
graph	B
of	O
the	O
model	B
as	O
discussed	O
in	O
again	O
under	O
the	O
constraint	O
that	O
the	O
last	O
variable	O
to	O
be	O
decimated	O
is	O
the	O
marginal	B
variable	O
of	O
draft	O
march	O
exercises	O
interest	O
for	O
this	O
smarter	O
choice	O
of	O
elimination	O
sequence	O
the	O
complexity	O
of	O
computing	O
this	O
single	O
marginal	B
is	O
roughly	O
the	O
same	O
as	O
that	O
for	O
the	O
junction	B
tree	B
algorithm	B
using	O
the	O
same	O
triangulation	B
bucketelim	O
m	O
bucket	B
elimination	I
demobucketelim	O
m	O
demo	O
bucket	B
elimination	I
message	B
passing	B
on	O
gaussians	O
the	O
following	O
code	O
hints	O
at	O
how	O
message	B
passing	B
may	O
be	O
implemented	O
for	O
continuous	B
distributions	O
the	O
reader	O
is	O
referred	O
to	O
the	O
brmltoolbox	O
for	O
further	O
details	O
and	O
also	O
for	O
the	O
algebraic	O
manipulations	O
required	O
to	O
perform	O
marginalisation	B
and	O
products	O
of	O
gaussians	O
the	O
same	O
principal	O
holds	O
for	O
any	O
family	B
of	O
distributions	O
which	O
is	O
closed	O
under	O
products	O
and	O
marginalisation	B
and	O
the	O
reader	O
may	O
wish	O
to	O
implement	O
specific	O
families	O
following	O
the	O
method	O
outlined	O
for	O
gaussians	O
demosumprodgaussmoment	O
m	O
sum-product	B
message	B
passing	B
based	O
on	O
gaussian	B
moment	O
parameterisation	B
exercises	O
exercise	O
given	O
a	O
pairwise	B
singly	O
connected	B
markov	B
network	I
of	O
the	O
form	O
xj	O
i	O
j	O
px	O
z	O
explain	O
how	O
to	O
efficiently	O
compute	O
the	O
normalisation	B
factor	B
called	O
the	O
partition	B
function	B
z	O
as	O
a	O
function	B
of	O
the	O
potentials	O
exercise	O
you	O
are	O
employed	O
by	O
a	O
web	O
start	O
up	O
company	O
that	O
designs	O
virtual	B
environments	O
in	O
which	O
players	O
can	O
move	O
between	O
rooms	O
the	O
rooms	O
which	O
are	O
accessible	O
from	O
another	O
in	O
one	O
time	O
step	O
is	O
given	O
by	O
the	O
matrix	B
m	O
stored	O
in	O
virtualworlds	O
mat	O
where	O
mij	O
means	O
that	O
there	O
is	O
a	O
door	O
between	O
rooms	O
i	O
and	O
j	O
mji	O
mij	O
means	O
that	O
there	O
is	O
no	O
door	O
between	O
rooms	O
i	O
and	O
j	O
mii	O
meaning	O
that	O
in	O
one	O
time	O
step	O
one	O
can	O
stay	O
in	O
the	O
same	O
room	O
you	O
can	O
visualise	O
this	O
matrix	B
by	O
typing	O
imagescm	O
write	O
a	O
list	O
of	O
rooms	O
which	O
cannot	O
be	O
reached	O
from	O
room	O
after	O
time	O
steps	O
the	O
manager	O
complains	O
that	O
takes	O
at	O
least	O
time	O
steps	O
to	O
get	O
from	O
room	O
to	O
room	O
is	O
this	O
true	O
find	O
the	O
most	O
likely	O
path	B
of	O
rooms	O
to	O
get	O
from	O
room	O
to	O
room	O
if	O
a	O
single	O
player	O
were	O
to	O
jump	O
randomly	O
from	O
one	O
room	O
to	O
another	O
stay	O
in	O
the	O
same	O
room	O
with	O
no	O
preference	O
between	O
rooms	O
what	O
is	O
the	O
probability	O
at	O
time	O
t	O
the	O
player	O
will	O
be	O
in	O
room	O
assume	O
that	O
effectively	O
an	O
infinite	O
amount	O
of	O
time	O
has	O
passed	O
and	O
the	O
player	O
began	O
in	O
room	O
at	O
t	O
if	O
two	O
players	O
are	O
jumping	O
randomly	O
between	O
rooms	O
staying	O
in	O
the	O
same	O
room	O
explain	O
how	O
to	O
compute	O
the	O
probability	O
that	O
after	O
an	O
infinite	O
amount	O
of	O
time	O
at	O
least	O
one	O
of	O
them	O
will	O
be	O
in	O
room	O
assume	O
that	O
both	O
players	O
begin	O
in	O
room	O
exercise	O
consider	O
the	O
hidden	B
markov	I
model	B
vt	O
ht	O
pvthtphtht	O
in	O
which	O
domht	O
h	O
and	O
domvt	O
v	O
for	O
all	O
t	O
t	O
draw	O
a	O
belief	B
network	I
representation	O
of	O
the	O
above	O
distribution	B
draw	O
a	O
factor	B
graph	B
representation	O
of	O
the	O
above	O
distribution	B
draft	O
march	O
exercises	O
use	O
the	O
factor	B
graph	B
to	O
derive	O
a	O
sum-product	B
algorithm	B
to	O
compute	O
marginals	O
vt	O
explain	O
the	O
sequence	O
order	O
of	O
messages	O
passed	O
on	O
your	O
factor	B
graph	B
explain	O
how	O
to	O
compute	O
pht	O
vt	O
exercise	O
for	O
a	O
singly	O
connected	B
markov	B
network	I
px	O
xn	O
the	O
computation	O
of	O
a	O
marginal	B
pxi	O
can	O
be	O
carried	O
out	O
efficiently	O
similarly	O
the	O
most	O
likely	O
joint	B
state	O
x	O
arg	O
px	O
can	O
be	O
computed	O
efficiently	O
explain	O
when	O
the	O
most	O
likely	O
joint	B
state	O
of	O
a	O
marginal	B
can	O
be	O
computed	O
efficiently	O
i	O
e	O
under	O
what	O
circumstances	O
could	O
one	O
efficiently	O
o	O
time	O
compute	O
argmax	O
xm	O
for	O
m	O
n	O
exercise	O
consider	O
the	O
internet	O
with	O
webpages	O
labelled	B
n	O
if	O
webpage	O
j	O
has	O
a	O
link	O
to	O
webpage	O
i	O
then	O
we	O
place	O
an	O
element	O
of	O
the	O
matrix	B
lij	O
otherwise	O
lij	O
by	O
considering	O
a	O
random	O
jump	O
from	O
webpage	O
j	O
to	O
webpage	O
i	O
to	O
be	O
given	O
by	O
the	O
transition	O
probability	O
mij	O
i	O
lij	O
what	O
is	O
the	O
probability	O
that	O
after	O
an	O
infinite	O
amount	O
of	O
random	O
surfing	O
one	O
ends	O
up	O
on	O
webpage	O
i	O
how	O
could	O
you	O
relate	O
this	O
to	O
the	O
potential	B
relevance	O
of	O
a	O
webpage	O
in	O
terms	O
of	O
a	O
search	B
engine	I
exercise	O
a	O
special	O
time-homogeneous	O
hidden	B
markov	I
model	B
is	O
given	O
by	O
xt	O
yt	O
ht	O
phtht	O
the	O
variable	O
xt	O
has	O
states	O
domxt	O
c	O
g	O
t	O
labelled	B
as	O
states	O
the	O
variable	O
yt	O
has	O
states	O
domyt	O
c	O
g	O
t	O
the	O
hidden	B
or	O
latent	B
variable	I
ht	O
has	O
states	O
domht	O
the	O
hmm	B
models	O
the	O
following	O
process	O
in	O
humans	O
z-factor	O
proteins	O
are	O
a	O
sequence	O
on	O
states	O
of	O
the	O
variables	O
xt	O
in	O
bananas	O
zfactor	O
proteins	O
are	O
also	O
present	O
but	O
represented	O
by	O
a	O
different	O
sequence	O
yt	O
given	O
a	O
sequence	O
xt	O
from	O
a	O
human	O
the	O
task	O
is	O
to	O
find	O
the	O
corresponding	O
sequence	O
yt	O
in	O
the	O
banana	O
by	O
first	O
finding	O
the	O
most	O
likely	O
joint	B
latent	B
sequence	O
and	O
then	O
the	O
most	O
likely	O
banana	O
sequence	O
given	O
this	O
optimal	O
latent	B
sequence	O
that	O
is	O
we	O
require	O
t	O
h	O
argmax	O
yth	O
where	O
h	O
h	O
t	O
argmax	O
xt	O
the	O
file	O
banana	O
mat	O
contains	O
the	O
emission	O
distributions	O
pxgh	O
pygh	O
and	O
transition	O
phtghtm	O
the	O
initial	O
hidden	B
distribution	B
is	O
given	O
in	O
the	O
observed	O
x	O
sequence	O
is	O
given	O
in	O
x	O
explain	O
mathematically	O
and	O
in	O
detail	O
how	O
to	O
compute	O
the	O
optimal	O
y-sequence	O
using	O
the	O
two-stage	O
procedure	O
as	O
stated	O
above	O
write	O
a	O
matlab	O
routine	O
that	O
computes	O
and	O
displays	O
the	O
optimal	O
y-sequence	O
given	O
the	O
observed	O
x-sequence	O
your	O
routine	O
must	O
make	O
use	O
of	O
the	O
factor	B
graph	B
formalism	O
explain	O
whether	O
or	O
not	O
it	O
is	O
computationally	O
tractable	O
to	O
compute	O
arg	O
max	O
xt	O
bonus	O
question	O
by	O
considering	O
yt	O
as	O
parameters	O
explain	O
how	O
the	O
em	B
algorithm	B
may	O
be	O
used	O
to	O
find	O
most	O
likely	O
marginal	B
states	O
implement	O
this	O
approach	B
with	O
a	O
suitable	O
initialisation	O
for	O
the	O
optimal	O
parameters	O
yt	O
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
the	O
junction	B
tree	B
algorithm	B
clustering	B
variables	O
in	O
we	O
discussed	O
efficient	O
inference	B
for	O
singly-connected	B
graphs	O
for	O
which	O
variable	B
elimination	I
and	O
message	B
passing	B
schemes	O
are	O
appropriate	O
in	O
the	O
multiply-connected	B
case	O
however	O
one	O
cannot	O
in	O
general	O
perform	O
inference	B
by	O
passing	B
messages	O
only	O
along	O
existing	O
links	O
in	O
the	O
graph	B
the	O
idea	O
behind	O
the	O
junction	B
tree	B
algorithm	B
is	O
to	O
form	O
a	O
new	O
representation	O
of	O
the	O
graph	B
in	O
which	O
variables	O
are	O
clustered	O
together	O
resulting	O
in	O
a	O
singly-connected	B
graph	B
in	O
the	O
cluster	O
variables	O
on	O
a	O
different	O
graph	B
the	O
main	O
focus	O
of	O
the	O
development	O
will	O
be	O
on	O
marginal	B
inference	B
though	O
similar	O
techniques	O
apply	O
to	O
difference	O
inferences	O
such	O
as	O
finding	O
the	O
maximal	O
state	O
of	O
the	O
distribution	B
at	O
this	O
stage	O
it	O
is	O
important	O
to	O
point	O
out	O
that	O
the	O
junction	B
tree	B
algorithm	B
is	O
not	O
a	O
magic	O
method	O
to	O
deal	O
with	O
intractabilities	O
resulting	O
from	O
multiply	O
connected	B
graphs	O
it	O
is	O
simply	O
a	O
way	O
to	O
perform	O
correct	O
inference	B
on	O
a	O
multiply	O
connected	B
graph	B
by	O
transforming	O
to	O
a	O
singly	O
connected	B
structure	B
carrying	O
out	O
the	O
inference	B
on	O
the	O
resulting	O
junction	B
tree	B
may	O
still	O
be	O
computationally	O
intractable	O
for	O
example	O
the	O
junction	B
tree	B
representation	O
of	O
a	O
general	O
two-dimensional	O
ising	B
model	B
is	O
a	O
single	O
supernode	O
containing	O
all	O
the	O
variables	O
inference	B
in	O
this	O
case	O
is	O
exponentially	O
complex	O
in	O
the	O
number	O
of	O
variables	O
nevertheless	O
even	O
in	O
cases	O
where	O
implementing	O
the	O
jta	O
any	O
other	O
exact	O
inference	B
algorithm	B
may	O
be	O
intractable	O
the	O
jta	O
provides	O
useful	O
insight	O
into	O
the	O
representation	O
of	O
distributions	O
that	O
can	O
form	O
the	O
basis	O
for	O
approximate	B
inference	B
in	O
this	O
sense	O
the	O
jta	O
is	O
key	O
to	O
understanding	O
issues	O
related	O
to	O
representations	O
and	O
complexity	O
of	O
inference	B
and	O
is	O
central	O
to	O
the	O
development	O
of	O
efficient	O
inference	B
algorithms	O
reparameterisation	B
consider	O
the	O
chain	B
pa	O
b	O
c	O
d	O
pabpbcpcdpd	O
using	O
bayes	O
rule	O
we	O
can	O
reexpress	O
this	O
as	O
pa	O
b	O
c	O
d	O
pa	O
b	O
pb	O
pb	O
c	O
pc	O
pc	O
d	O
pd	O
pd	O
pa	O
bpb	O
cpc	O
d	O
pbpc	O
a	O
useful	O
insight	O
is	O
that	O
the	O
distribution	B
can	O
therefore	O
be	O
written	O
as	O
a	O
product	O
of	O
marginal	B
distributions	O
divided	O
by	O
a	O
product	O
of	O
the	O
intersection	O
of	O
the	O
marginal	B
distributions	O
looking	O
at	O
the	O
numerator	O
pa	O
bpb	O
cpc	O
d	O
this	O
cannot	O
be	O
a	O
distribution	B
over	O
a	O
b	O
c	O
d	O
since	O
we	O
are	O
overcounting	B
b	O
and	O
c	O
where	O
this	O
overcounting	B
of	O
b	O
arises	O
from	O
the	O
overlap	O
between	O
the	O
sets	O
a	O
b	O
and	O
b	O
c	O
which	O
have	O
b	O
as	O
their	O
intersection	O
similarly	O
the	O
overcounting	B
of	O
c	O
arises	O
from	O
the	O
overlap	O
between	O
the	O
sets	O
b	O
c	O
and	O
c	O
d	O
roughly	O
speaking	O
we	O
need	O
to	O
correct	O
for	O
this	O
overcounting	B
by	O
dividing	O
by	O
the	O
distribution	B
on	O
the	O
intersections	O
given	O
the	O
transformed	O
representation	O
a	O
marginal	B
such	O
as	O
pa	O
b	O
can	O
be	O
read	O
off	O
directly	O
from	O
the	O
factors	O
in	O
the	O
new	O
clique	B
graphs	O
a	O
b	O
c	O
d	O
expression	O
abc	O
bc	O
bcd	O
figure	O
b	O
c	O
c	O
d	O
graph	B
of	O
markov	B
network	I
equivalent	B
clique	B
the	O
aim	O
of	O
the	O
junction	B
tree	B
algorithm	B
is	O
to	O
form	O
a	O
representation	O
of	O
the	O
distribution	B
which	O
contains	O
the	O
marginals	O
explicitly	O
we	O
want	O
to	O
do	O
this	O
in	O
a	O
way	O
that	O
works	O
for	O
belief	O
and	O
markov	O
networks	O
and	O
also	O
deals	O
with	O
the	O
multiply-connected	B
case	O
in	O
order	O
to	O
do	O
so	O
an	O
appropriate	O
way	O
to	O
parameterise	O
the	O
distribution	B
is	O
in	O
terms	O
of	O
a	O
clique	B
graph	B
as	O
described	O
in	O
the	O
next	O
section	O
clique	B
graphs	O
definition	O
graph	B
a	O
clique	B
graph	B
consists	O
of	O
a	O
set	O
of	O
potentials	O
nx	O
n	O
each	O
defined	O
on	O
a	O
set	O
of	O
variables	O
x	O
i	O
for	O
neighbouring	O
cliques	O
on	O
the	O
graph	B
defined	O
on	O
sets	O
of	O
variables	O
x	O
i	O
and	O
x	O
j	O
the	O
intersection	O
x	O
s	O
x	O
i	O
x	O
j	O
is	O
called	O
the	O
separator	B
and	O
has	O
a	O
corresponding	O
potential	B
sx	O
s	O
a	O
clique	B
graph	B
represents	O
the	O
function	B
c	O
cx	O
c	O
s	O
sx	O
s	O
for	O
notational	O
simplicity	O
we	O
will	O
usually	O
drop	O
the	O
clique	B
potential	B
index	O
c	O
graphically	O
clique	B
potentials	O
are	O
represented	O
by	O
circlesovals	O
and	O
separator	B
potentials	O
by	O
squares	O
x	O
x	O
x	O
x	O
the	O
graph	B
on	O
the	O
left	O
represents	O
x	O
clique	B
graphs	O
translate	O
markov	O
networks	O
into	O
structures	O
convenient	O
for	O
carrying	O
out	O
inference	B
consider	O
the	O
markov	B
network	I
in	O
pa	O
b	O
c	O
d	O
b	O
c	O
c	O
d	O
z	O
which	O
contains	O
two	O
clique	B
potentials	O
sharing	O
the	O
variables	O
b	O
c	O
an	O
equivalent	B
representation	O
is	O
given	O
by	O
the	O
clique	B
graph	B
in	O
defined	O
as	O
the	O
product	O
of	O
the	O
numerator	O
clique	B
potentials	O
divided	O
by	O
the	O
product	O
of	O
the	O
separator	B
potentials	O
in	O
this	O
case	O
the	O
separator	B
potential	B
may	O
be	O
set	O
to	O
the	O
normalisation	B
constant	I
z	O
by	O
summing	O
we	O
have	O
zpa	O
b	O
c	O
b	O
b	O
c	O
d	O
b	O
cpb	O
c	O
d	O
multiplying	O
the	O
two	O
expressions	O
we	O
have	O
d	O
zpb	O
c	O
d	O
c	O
c	O
a	O
b	O
c	O
c	O
d	O
d	O
a	O
in	O
other	O
words	O
pa	O
b	O
c	O
d	O
pa	O
b	O
cpb	O
c	O
d	O
pc	O
b	O
b	O
c	O
b	O
c	O
ad	O
pa	O
b	O
c	O
d	O
draft	O
march	O
clique	B
graphs	O
the	O
important	O
observation	O
is	O
that	O
the	O
distribution	B
can	O
be	O
written	O
in	O
terms	O
of	O
its	O
marginals	O
on	O
the	O
variables	O
in	O
the	O
original	O
cliques	O
and	O
that	O
as	O
a	O
clique	B
graph	B
it	O
has	O
the	O
same	O
structure	B
as	O
before	O
all	O
that	O
has	O
changed	O
is	O
that	O
the	O
original	O
clique	B
potentials	O
have	O
been	O
replaced	O
by	O
the	O
marginals	O
of	O
the	O
distribution	B
and	O
the	O
separator	B
by	O
the	O
marginal	B
defined	O
on	O
the	O
separator	B
variables	O
b	O
c	O
pa	O
b	O
c	O
c	O
d	O
pb	O
c	O
d	O
z	O
pc	O
b	O
the	O
usefulness	O
of	O
this	O
representation	O
is	O
that	O
if	O
we	O
are	O
interested	O
in	O
the	O
marginal	B
pa	O
b	O
c	O
this	O
can	O
be	O
read	O
off	O
from	O
the	O
transformed	O
clique	B
potential	B
to	O
make	O
use	O
of	O
this	O
representation	O
we	O
require	O
a	O
systematic	O
way	O
of	O
transforming	O
the	O
clique	B
graph	B
potentials	O
so	O
that	O
at	O
the	O
end	O
of	O
the	O
transformation	O
the	O
new	O
potentials	O
contain	O
the	O
marginals	O
of	O
the	O
distribution	B
remark	O
note	O
that	O
whilst	O
visually	O
similar	O
a	O
factor	B
graph	B
and	O
a	O
clique	B
graph	B
are	O
different	O
representations	O
in	O
a	O
clique	B
graph	B
the	O
nodes	O
contain	O
sets	O
of	O
variables	O
which	O
may	O
share	O
variables	O
with	O
other	O
nodes	O
absorption	B
consider	O
neighbouring	O
cliques	O
v	O
and	O
w	O
sharing	O
the	O
variables	O
s	O
in	O
common	O
in	O
this	O
case	O
the	O
distribution	B
on	O
the	O
variables	O
x	O
v	O
w	O
is	O
px	O
and	O
our	O
aim	O
is	O
to	O
find	O
a	O
new	O
representation	O
in	O
which	O
the	O
potentials	O
are	O
given	O
by	O
pw	O
ps	O
in	O
this	O
example	O
we	O
can	O
explicitly	O
work	O
out	O
the	O
new	O
potentials	O
as	O
function	B
of	O
the	O
old	O
potentials	O
by	O
computing	O
the	O
marginals	O
as	O
follows	O
px	O
pv	O
pw	O
pv	O
vs	O
ws	O
px	O
px	O
vs	O
ws	O
vs	O
and	O
and	O
the	O
refine	O
the	O
w	O
potential	B
using	O
there	O
is	O
a	O
symmetry	O
present	O
in	O
the	O
two	O
equations	O
above	O
they	O
are	O
the	O
same	O
under	O
interchanging	O
v	O
and	O
w	O
one	O
way	O
to	O
describe	O
these	O
equations	O
is	O
through	O
absorption	B
we	O
say	O
that	O
the	O
cluster	O
w	O
absorbs	O
information	O
from	O
cluster	O
v	O
by	O
the	O
following	O
updating	O
procedure	O
first	O
we	O
define	O
a	O
new	O
separator	B
the	O
advantage	O
of	O
this	O
interpretation	O
is	O
that	O
the	O
new	O
representation	O
is	O
still	O
a	O
valid	O
clique	B
graph	B
representation	O
of	O
the	O
distribution	B
since	O
draft	O
march	O
px	O
a	O
c	O
e	O
d	O
junction	O
trees	O
figure	O
an	O
example	O
absorption	B
schedule	B
on	O
a	O
clique	B
tree	B
many	O
valid	O
schedules	O
exist	O
under	O
the	O
constraint	O
that	O
messages	O
can	O
only	O
be	O
passed	O
to	O
a	O
neighbour	B
when	O
all	O
other	O
messages	O
have	O
been	O
received	O
b	O
f	O
after	O
w	O
absorbs	O
information	O
from	O
v	O
then	O
contains	O
the	O
marginal	B
pw	O
similarly	O
after	O
v	O
absorbs	O
information	O
from	O
w	O
then	O
contains	O
the	O
marginal	B
pv	O
after	O
the	O
separator	B
s	O
has	O
participated	O
in	O
absorption	B
along	O
both	O
directions	O
then	O
the	O
separator	B
potential	B
will	O
contain	O
ps	O
is	O
not	O
the	O
case	O
after	O
only	O
a	O
single	O
absorption	B
to	O
see	O
this	O
consider	O
continuing	O
we	O
have	O
the	O
new	O
potential	B
given	O
by	O
vs	O
ws	O
ws	O
ps	O
pv	O
definition	O
let	O
v	O
and	O
w	O
be	O
neighbours	O
in	O
a	O
clique	B
graph	B
let	O
s	O
be	O
their	O
separator	B
and	O
let	O
and	O
be	O
their	O
potentials	O
absorption	B
from	O
v	O
to	O
w	O
through	O
s	O
replaces	O
the	O
tables	O
and	O
with	O
vs	O
we	O
say	O
that	O
clique	B
w	O
absorbs	O
information	O
from	O
clique	B
v	O
absorption	B
schedule	B
on	O
clique	B
trees	O
having	O
defined	O
the	O
local	B
message	B
propagation	B
approach	B
we	O
need	O
to	O
define	O
an	O
update	O
ordering	O
for	O
absorption	B
in	O
general	O
a	O
node	O
v	O
can	O
send	O
exactly	O
one	O
message	B
to	O
a	O
neighbour	B
w	O
and	O
it	O
may	O
only	O
be	O
sent	O
when	O
v	O
has	O
received	O
a	O
message	B
from	O
each	O
of	O
its	O
other	O
neighbours	O
we	O
continue	O
this	O
sequence	O
of	O
absorptions	O
until	O
a	O
message	B
has	O
been	O
passed	O
in	O
both	O
directions	O
along	O
every	O
link	O
see	O
for	O
example	O
note	O
that	O
the	O
message	B
passing	B
scheme	O
is	O
not	O
unique	O
definition	O
schedule	B
a	O
clique	B
can	O
send	O
a	O
message	B
to	O
a	O
neighbour	B
provided	O
it	O
has	O
already	O
received	O
messages	O
from	O
all	O
other	O
neighbours	O
junction	O
trees	O
there	O
are	O
a	O
few	O
stages	O
we	O
need	O
to	O
go	O
through	O
in	O
order	O
to	O
transform	O
a	O
distribution	B
into	O
an	O
appropriate	O
structure	B
for	O
inference	B
initially	O
we	O
explain	O
how	O
to	O
do	O
this	O
for	O
singly-connected	B
structures	O
before	O
moving	O
draft	O
march	O
junction	O
trees	O
figure	O
singly-connected	B
markov	B
network	I
clique	B
graph	B
clique	B
tree	B
onto	O
the	O
multiply-connected	B
case	O
consider	O
the	O
singly-connected	B
markov	B
network	I
the	O
clique	B
graph	B
of	O
this	O
singly-connected	B
markov	B
network	I
is	O
multiply-connected	B
where	O
the	O
separator	B
potentials	O
are	O
all	O
set	O
to	O
unity	O
nevertheless	O
let	O
s	O
try	O
to	O
reexpress	O
the	O
markov	B
network	I
in	O
terms	O
of	O
marginals	O
first	O
we	O
have	O
the	O
relations	O
taking	O
the	O
product	O
of	O
the	O
three	O
marginals	O
we	O
have	O
this	O
means	O
that	O
the	O
markov	B
network	I
can	O
be	O
expressed	O
in	O
terms	O
of	O
marginals	O
as	O
hence	O
a	O
valid	O
clique	B
graph	B
is	O
also	O
given	O
by	O
the	O
representation	O
indeed	O
if	O
a	O
variable	O
occurs	O
on	O
every	O
separator	B
in	O
a	O
clique	B
graph	B
loop	O
one	O
can	O
remove	O
that	O
variable	O
from	O
an	O
arbitrarily	O
chosen	O
separator	B
in	O
the	O
loop	O
this	O
leaves	O
an	O
empty	O
separator	B
which	O
we	O
can	O
simply	O
remove	O
this	O
shows	O
that	O
in	O
such	O
cases	O
we	O
can	O
transform	O
the	O
clique	B
graph	B
into	O
a	O
clique	B
tree	B
a	O
singly-connected	B
clique	B
graph	B
provided	O
that	O
the	O
original	O
markov	B
network	I
is	O
singly-connected	B
one	O
can	O
always	O
form	O
a	O
clique	B
tree	B
in	O
this	O
manner	O
the	O
running	B
intersection	I
property	I
sticking	O
with	O
the	O
above	O
example	O
consider	O
the	O
clique	B
tree	B
in	O
draft	O
march	O
as	O
a	O
representation	O
of	O
the	O
distribution	B
where	O
we	O
set	O
to	O
make	O
this	O
match	O
now	O
perform	O
absorption	B
on	O
this	O
clique	B
tree	B
we	O
absorb	O
the	O
new	O
separator	B
is	O
junction	O
trees	O
and	O
the	O
new	O
potential	B
is	O
now	O
the	O
new	O
separator	B
is	O
and	O
the	O
new	O
potential	B
is	O
since	O
we	O
ve	O
hit	O
the	O
buffers	O
in	O
terms	O
of	O
message	B
passing	B
the	O
potential	B
cannot	O
be	O
updated	O
further	O
let	O
s	O
examine	O
more	O
carefully	O
the	O
value	B
of	O
this	O
new	O
potential	B
hence	O
the	O
new	O
potential	B
contains	O
the	O
marginal	B
to	O
complete	O
a	O
full	O
round	O
of	O
message	B
passing	B
we	O
need	O
to	O
have	O
passed	O
messages	O
in	O
a	O
valid	O
schedule	B
along	O
both	O
directions	O
of	O
each	O
separator	B
to	O
do	O
so	O
we	O
continue	O
as	O
follows	O
we	O
absorb	O
the	O
new	O
separator	B
is	O
and	O
and	O
note	O
that	O
so	O
that	O
now	O
after	O
absorbing	O
through	O
both	O
directions	O
the	O
separator	B
contains	O
the	O
marginal	B
the	O
reader	O
may	O
show	O
that	O
finally	O
we	O
absorb	O
the	O
new	O
separator	B
is	O
hence	O
after	O
a	O
full	O
round	O
of	O
message	B
passing	B
the	O
new	O
potentials	O
all	O
contain	O
the	O
correct	O
marginals	O
draft	O
march	O
junction	O
trees	O
vi	O
wi	O
the	O
new	O
representation	O
is	O
consistent	B
in	O
the	O
sense	O
that	O
for	O
any	O
necessarily	O
neighbouring	O
cliques	O
v	O
and	O
w	O
with	O
intersection	O
i	O
and	O
corresponding	O
potentials	O
and	O
note	O
that	O
bidirectional	O
absorption	B
guarantees	O
consistency	O
for	O
neighbouring	O
cliques	O
as	O
in	O
the	O
example	O
above	O
provided	O
that	O
we	O
started	O
with	O
a	O
clique	B
tree	B
which	O
is	O
a	O
correct	O
representation	O
of	O
the	O
distribution	B
in	O
general	O
the	O
only	O
possible	O
source	O
of	O
non-consistency	O
is	O
if	O
a	O
variable	O
occurs	O
in	O
two	O
non-neighbouring	O
cliques	O
and	O
is	O
not	O
present	O
in	O
all	O
cliques	O
on	O
any	O
path	B
connection	O
them	O
an	O
extreme	O
example	O
would	O
be	O
if	O
we	O
removed	O
the	O
link	O
between	O
cliques	O
and	O
in	O
this	O
case	O
this	O
is	O
still	O
a	O
clique	B
tree	B
however	O
global	B
consistency	O
could	O
not	O
be	O
guaranteed	O
since	O
the	O
information	O
required	O
to	O
make	O
clique	B
consistent	B
with	O
the	O
rest	O
of	O
the	O
graph	B
cannot	O
reach	O
this	O
clique	B
formally	O
the	O
requirement	O
for	O
the	O
propagation	B
of	O
local	B
to	O
global	B
consistency	O
is	O
that	O
the	O
clique	B
tree	B
is	O
a	O
junction	B
tree	B
as	O
defined	O
below	O
definition	O
tree	B
a	O
clique	B
tree	B
is	O
a	O
junction	B
tree	B
if	O
for	O
each	O
pair	O
of	O
nodes	O
v	O
and	O
w	O
all	O
nodes	O
on	O
the	O
path	B
between	O
v	O
and	O
w	O
contain	O
the	O
intersection	O
v	O
w	O
this	O
is	O
also	O
called	O
the	O
running	B
intersection	I
property	I
from	O
this	O
definition	O
local	B
consistency	O
will	O
be	O
passed	O
on	O
to	O
any	O
neighbours	O
and	O
the	O
distribution	B
will	O
be	O
globally	O
consistent	B
proofs	O
for	O
these	O
results	O
are	O
contained	O
in	O
example	O
consistent	B
junction	B
tree	B
to	O
gain	O
some	O
intuition	O
about	O
the	O
meaning	O
of	O
consistency	O
consider	O
the	O
junction	B
tree	B
in	O
after	O
a	O
full	O
round	O
of	O
message	B
passing	B
on	O
this	O
tree	B
each	O
link	O
is	O
consistent	B
and	O
the	O
product	O
of	O
the	O
potentials	O
divided	O
by	O
the	O
product	O
of	O
the	O
separator	B
potentials	O
is	O
just	O
the	O
original	O
distribution	B
itself	O
imagine	O
that	O
we	O
are	O
interested	O
in	O
calculating	O
the	O
marginal	B
for	O
the	O
node	O
abc	O
that	O
requires	O
summing	O
over	O
all	O
the	O
other	O
variables	O
def	O
gh	O
if	O
we	O
consider	O
summing	O
over	O
h	O
then	O
because	O
the	O
link	O
is	O
consistent	B
h	O
h	O
so	O
that	O
the	O
ratio	O
c	O
e	O
h	O
is	O
unity	O
and	O
the	O
effect	O
of	O
summing	O
over	O
node	O
h	O
is	O
that	O
the	O
link	O
between	O
eh	O
and	O
dce	O
can	O
be	O
removed	O
along	O
with	O
the	O
separator	B
the	O
same	O
happens	O
for	O
the	O
link	O
between	O
node	O
eg	O
and	O
dce	O
and	O
also	O
for	O
cf	O
to	O
abc	O
the	O
only	O
nodes	O
remaining	O
are	O
now	O
dce	O
and	O
abc	O
and	O
their	O
separator	B
c	O
which	O
have	O
so	O
far	O
been	O
unaffected	O
by	O
the	O
summations	O
we	O
still	O
need	O
to	O
sum	O
out	O
over	O
d	O
and	O
e	O
again	O
because	O
the	O
link	O
is	O
consistent	B
de	O
so	O
that	O
the	O
ratio	O
summation	O
over	O
the	O
other	O
variables	O
in	O
that	O
potential	B
for	O
example	O
pf	O
the	O
result	O
of	O
the	O
summation	O
of	O
all	O
variables	O
not	O
in	O
abc	O
therefore	O
produces	O
unity	O
for	O
the	O
cliques	O
and	O
their	O
separators	O
and	O
the	O
summed	O
potential	B
representation	O
reduces	O
simply	O
to	O
the	O
potential	B
b	O
c	O
which	O
is	O
the	O
marginal	B
pa	O
b	O
c	O
it	O
is	O
clear	O
that	O
a	O
similar	O
effect	O
will	O
happen	O
for	O
other	O
nodes	O
we	O
can	O
then	O
obtain	O
the	O
marginals	O
for	O
individual	O
variables	O
by	O
simple	O
brute	O
force	O
de	O
c	O
f	O
draft	O
march	O
constructing	O
a	O
junction	B
tree	B
for	O
singly-connected	B
distributions	O
constructing	O
a	O
junction	B
tree	B
for	O
singly-connected	B
distributions	O
moralisation	B
for	O
belief	B
networks	I
an	O
initial	O
step	O
is	O
required	O
which	O
is	O
not	O
required	O
in	O
the	O
case	O
of	O
undirected	B
graphs	O
definition	O
for	O
each	O
variable	O
x	O
add	O
an	O
undirected	B
link	O
between	O
all	O
parents	B
of	O
x	O
and	O
replace	O
the	O
directed	B
link	O
from	O
x	O
to	O
its	O
parents	B
by	O
undirected	B
links	O
this	O
creates	O
a	O
moralised	O
markov	B
network	I
forming	O
the	O
clique	B
graph	B
the	O
clique	B
graph	B
is	O
formed	O
by	O
identifying	O
the	O
cliques	O
in	O
the	O
markov	B
network	I
and	O
adding	O
a	O
link	O
between	O
cliques	O
that	O
have	O
a	O
non-empty	O
intersection	O
add	O
a	O
separator	B
between	O
the	O
intersecting	O
cliques	O
forming	O
a	O
junction	B
tree	B
from	O
a	O
clique	B
graph	B
for	O
a	O
singly-connected	B
distribution	B
any	O
maximal	O
weight	B
spanning	B
tree	B
of	O
a	O
clique	B
graph	B
is	O
a	O
junction	B
tree	B
definition	O
tree	B
a	O
junction	B
tree	B
is	O
obtained	O
by	O
finding	O
a	O
maximal	O
weight	B
spanning	B
tree	B
of	O
the	O
clique	B
graph	B
the	O
weight	B
of	O
the	O
tree	B
is	O
defined	O
as	O
the	O
sum	O
of	O
all	O
the	O
separator	B
weights	O
of	O
the	O
tree	B
where	O
the	O
separator	B
weight	B
is	O
the	O
number	O
of	O
variables	O
in	O
the	O
separator	B
if	O
the	O
clique	B
graph	B
contains	O
loops	O
then	O
all	O
separators	O
on	O
the	O
loop	O
contain	O
the	O
same	O
variable	O
by	O
continuing	O
to	O
remove	O
loop	O
links	O
until	O
you	O
have	O
a	O
tree	B
is	O
revealed	O
we	O
obtain	O
a	O
junction	B
tree	B
example	O
a	O
junction	B
tree	B
consider	O
the	O
belief	B
network	I
in	O
the	O
moralisation	B
procedure	O
gives	O
identifying	O
the	O
cliques	O
in	O
this	O
graph	B
and	O
linking	O
them	O
together	O
gives	O
the	O
clique	B
graph	B
in	O
there	O
are	O
several	O
possible	O
junction	O
trees	O
one	O
could	O
obtain	O
from	O
this	O
clique	B
graph	B
and	O
one	O
is	O
given	O
in	O
assigning	O
potentials	O
to	O
cliques	O
of	O
a	O
set	O
of	O
potentials	O
definition	O
potential	B
assignment	O
given	O
a	O
junction	B
tree	B
and	O
a	O
function	B
defined	O
as	O
the	O
product	O
x	O
n	O
a	O
valid	O
clique	B
potential	B
assignment	O
places	O
potentials	O
in	O
jt	O
cliques	O
whose	O
variables	O
can	O
contain	O
them	O
such	O
that	O
the	O
product	O
of	O
the	O
jt	O
clique	B
potentials	O
divided	O
by	O
the	O
jt	O
separator	B
potentials	O
is	O
equal	O
to	O
the	O
function	B
a	O
simple	O
way	O
to	O
achieve	O
this	O
assignment	O
is	O
to	O
list	O
all	O
the	O
potentials	O
and	O
order	O
the	O
jt	O
cliques	O
arbitrarily	O
then	O
for	O
each	O
potential	B
search	O
through	O
the	O
jt	O
cliques	O
until	O
the	O
first	O
is	O
encountered	O
for	O
which	O
the	O
potential	B
variables	O
are	O
a	O
subset	O
of	O
the	O
jt	O
clique	B
variables	O
subsequently	O
the	O
potential	B
on	O
each	O
jt	O
clique	B
is	O
taken	O
as	O
the	O
product	O
of	O
all	O
clique	B
potentials	O
assigned	O
to	O
the	O
jt	O
clique	B
lastly	O
we	O
assign	O
all	O
jt	O
separators	O
to	O
unity	O
this	O
approach	B
is	O
taken	O
in	O
jtassignpot	O
m	O
draft	O
march	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
c	O
h	O
c	O
a	O
e	O
abc	O
c	O
eh	O
d	O
g	O
c	O
e	O
b	O
f	O
d	O
g	O
a	O
e	O
c	O
h	O
cf	O
dce	O
e	O
eg	O
b	O
f	O
c	O
e	O
abc	O
c	O
cf	O
eh	O
dce	O
e	O
eg	O
e	O
figure	O
belief	B
network	I
a	O
junction	B
tree	B
this	O
satisfies	O
the	O
running	B
intersection	I
property	I
that	O
for	O
any	O
two	O
nodes	O
which	O
contain	O
a	O
variable	O
in	O
common	O
any	O
clique	B
on	O
the	O
path	B
linking	O
the	O
two	O
nodes	O
also	O
contains	O
that	O
variable	O
moralised	O
version	O
of	O
clique	B
graph	B
of	O
example	O
for	O
the	O
belief	B
network	I
of	O
we	O
wish	O
to	O
assign	O
its	O
potentials	O
to	O
the	O
junction	B
tree	B
in	O
this	O
case	O
the	O
assignment	O
is	O
unique	O
and	O
is	O
given	O
by	O
papbpca	O
b	O
pdped	O
c	O
pfc	O
pge	O
phe	O
all	O
separator	B
potentials	O
are	O
initialised	O
to	O
unity	O
note	O
that	O
in	O
some	O
instances	O
it	O
can	O
be	O
that	O
a	O
junction	B
tree	B
clique	B
is	O
assigned	O
to	O
unity	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
when	O
the	O
distribution	B
contains	O
loops	O
the	O
construction	B
outlined	O
in	O
does	O
not	O
result	O
in	O
a	O
junction	B
tree	B
the	O
reason	O
is	O
that	O
due	O
to	O
the	O
loops	O
variable	B
elimination	I
changes	O
the	O
structure	B
of	O
the	O
remaining	O
graph	B
to	O
see	O
this	O
consider	O
the	O
following	O
distribution	B
pa	O
b	O
c	O
d	O
b	O
c	O
d	O
a	O
as	O
shown	O
in	O
let	O
s	O
first	O
try	O
to	O
make	O
a	O
clique	B
graph	B
we	O
have	O
a	O
choice	O
about	O
which	O
variable	O
first	O
to	O
marginalise	O
over	O
let	O
s	O
choose	O
d	O
pa	O
b	O
c	O
b	O
draft	O
march	O
d	O
d	O
a	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
a	O
d	O
a	O
b	O
c	O
b	O
c	O
a	O
d	O
b	O
c	O
a	O
d	O
b	O
c	O
abc	O
ac	O
acd	O
figure	O
an	O
undirected	B
graph	B
with	O
a	O
loop	O
c	O
in	O
the	O
subgraph	O
representation	O
junction	B
tree	B
for	O
the	O
induced	B
representation	I
for	O
the	O
graph	B
in	O
eliminating	O
node	O
d	O
adds	O
a	O
link	O
between	O
a	O
and	O
equivalent	B
induced	O
the	O
remaining	O
subgraph	O
therefore	O
has	O
an	O
extra	O
connection	O
between	O
a	O
and	O
c	O
see	O
we	O
can	O
express	O
the	O
joint	B
in	O
terms	O
of	O
the	O
marginals	O
using	O
to	O
continue	O
the	O
transformation	O
into	O
marginal	B
form	O
let	O
s	O
try	O
to	O
replace	O
the	O
numerator	O
terms	O
with	O
probabilities	O
we	O
can	O
do	O
this	O
by	O
considering	O
pa	O
b	O
c	O
d	O
pa	O
b	O
c	O
d	O
d	O
a	O
d	O
a	O
pa	O
c	O
d	O
d	O
b	O
c	O
b	O
plugging	O
this	O
into	O
the	O
above	O
equation	B
we	O
have	O
pa	O
b	O
c	O
d	O
d	O
d	O
pa	O
b	O
cpa	O
c	O
d	O
b	O
b	O
c	O
we	O
recognise	O
that	O
the	O
denominator	O
is	O
simply	O
pa	O
c	O
hence	O
pa	O
b	O
c	O
d	O
pa	O
b	O
cpa	O
c	O
d	O
pa	O
c	O
this	O
means	O
that	O
a	O
valid	O
clique	B
graph	B
for	O
the	O
distribution	B
must	O
contain	O
cliques	O
larger	O
than	O
those	O
in	O
the	O
original	O
distribution	B
to	O
form	O
a	O
jt	O
based	O
on	O
products	O
of	O
cliques	O
divided	O
by	O
products	O
of	O
separators	O
we	O
could	O
start	O
from	O
the	O
induced	B
representation	I
alternatively	O
we	O
could	O
have	O
marginalised	O
over	O
variables	O
a	O
and	O
c	O
and	O
ended	O
up	O
with	O
the	O
equivalent	B
representation	O
generally	O
the	O
result	O
from	O
variable	B
elimination	I
and	O
re-representation	O
in	O
terms	O
of	O
the	O
induced	O
graph	B
is	O
that	O
a	O
link	O
is	O
added	O
between	O
any	O
two	O
variables	O
on	O
a	O
loop	O
length	O
or	O
more	O
which	O
does	O
not	O
have	O
a	O
chord	B
this	O
is	O
called	O
triangulation	B
a	O
markov	B
network	I
on	O
a	O
triangulated	B
graph	B
can	O
always	O
be	O
written	O
in	O
terms	O
of	O
the	O
product	O
of	O
marginals	O
divided	O
by	O
the	O
product	O
of	O
separators	O
armed	O
with	O
this	O
new	O
induced	B
representation	I
we	O
can	O
form	O
a	O
junction	B
tree	B
a	O
f	O
b	O
e	O
c	O
a	O
d	O
f	O
b	O
e	O
c	O
d	O
figure	O
markov	B
network	I
resentation	O
loopy	B
ladder	O
induced	O
rep	O
draft	O
march	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
a	O
a	O
f	O
f	O
b	O
j	O
b	O
j	O
c	O
d	O
e	O
a	O
g	O
h	O
i	O
e	O
a	O
k	O
d	O
l	O
c	O
g	O
h	O
i	O
k	O
l	O
f	O
f	O
b	O
j	O
b	O
j	O
c	O
d	O
e	O
a	O
g	O
h	O
i	O
e	O
a	O
a	O
k	O
d	O
l	O
c	O
g	O
h	O
i	O
k	O
l	O
f	O
f	O
f	O
b	O
j	O
b	O
b	O
j	O
j	O
c	O
d	O
e	O
g	O
h	O
i	O
k	O
d	O
d	O
e	O
e	O
l	O
c	O
c	O
g	O
g	O
h	O
h	O
i	O
i	O
k	O
k	O
l	O
l	O
figure	O
markov	B
network	I
for	O
which	O
we	O
seek	O
a	O
triangulation	B
via	O
greedy	O
variable	B
elimination	I
we	O
we	O
then	O
eliminate	O
variables	O
b	O
d	O
since	O
these	O
only	O
add	O
first	O
eliminate	O
the	O
simplical	B
nodes	I
a	O
e	O
l	O
f	O
and	O
i	O
are	O
now	O
simplical	B
and	O
are	O
eliminated	O
a	O
single	O
extra	O
link	O
to	O
the	O
induced	O
graph	B
the	O
remaining	O
variables	O
j	O
k	O
we	O
eliminate	O
g	O
and	O
h	O
since	O
this	O
adds	O
only	O
single	O
extra	O
links	O
final	O
triangulation	B
the	O
variable	B
elimination	I
order	O
is	O
may	O
be	O
eliminated	O
in	O
any	O
order	O
e	O
l	O
d	O
i	O
h	O
j	O
k	O
where	O
the	O
brackets	O
indicate	O
that	O
the	O
order	O
in	O
which	O
the	O
variables	O
inside	O
the	O
bracket	O
are	O
eliminated	O
is	O
irrelevant	O
compared	O
with	O
the	O
triangulation	B
produced	O
by	O
the	O
maxcardinality	O
checking	O
approach	B
in	O
this	O
triangulation	B
is	O
more	O
parsimonious	O
example	O
a	O
slightly	O
more	O
complex	O
loopy	B
distribution	B
is	O
depicted	O
in	O
pa	O
b	O
c	O
d	O
e	O
f	O
b	O
c	O
d	O
e	O
f	O
f	O
e	O
there	O
are	O
different	O
induced	O
representations	O
depending	O
on	O
which	O
variables	O
we	O
decide	O
to	O
eliminate	O
the	O
reader	O
may	O
convince	O
herself	O
that	O
one	O
such	O
induced	B
representation	I
is	O
given	O
by	O
definition	O
this	O
is	O
a	O
link	O
joining	O
two	O
non-consecutive	O
vertices	O
of	O
a	O
loop	O
definition	O
graph	B
an	O
undirected	B
graph	B
is	O
triangulated	B
if	O
every	O
loop	O
of	O
length	O
or	O
more	O
has	O
a	O
chord	B
an	O
equivalent	B
term	O
is	O
that	O
the	O
graph	B
is	O
decomposable	B
or	O
chordal	B
an	O
undirected	B
graph	B
is	O
triangulated	B
if	O
and	O
only	O
if	O
its	O
clique	B
graph	B
has	O
a	O
junction	B
tree	B
triangulation	B
algorithms	O
when	O
a	O
variable	O
is	O
eliminated	O
from	O
a	O
graph	B
links	O
are	O
added	O
between	O
all	O
the	O
neighbours	O
of	O
the	O
eliminated	O
variable	O
a	O
triangulation	B
algorithm	B
is	O
one	O
that	O
produces	O
a	O
graph	B
for	O
which	O
there	O
exists	O
a	O
variable	B
elimination	I
order	O
that	O
introduces	O
no	O
extra	O
links	O
in	O
the	O
graph	B
draft	O
march	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
figure	O
junction	B
tree	B
formed	O
from	O
the	O
triangulation	B
one	O
verify	O
that	O
this	O
satisfies	O
the	O
running	B
intersection	I
property	I
abf	O
bf	O
bcf	O
g	O
cdhi	O
di	O
dei	O
cf	O
g	O
cf	O
gj	O
chi	O
chik	O
cj	O
ck	O
cjk	O
jk	O
jkl	O
for	O
discrete	B
variables	O
the	O
complexity	O
of	O
inference	B
scales	O
exponentially	O
with	O
clique	B
sizes	O
in	O
the	O
triangulated	B
graph	B
since	O
absorption	B
requires	O
computing	O
tables	O
on	O
the	O
cliques	O
it	O
is	O
therefore	O
of	O
some	O
interest	O
to	O
find	O
a	O
triangulated	B
graph	B
with	O
small	O
clique	B
sizes	O
however	O
finding	O
the	O
triangulated	B
graph	B
with	O
the	O
smallest	O
maximal	O
clique	B
is	O
an	O
np-hard	O
problem	B
for	O
a	O
general	O
graph	B
and	O
heuristics	O
are	O
unavoidable	O
below	O
we	O
describe	O
two	O
simple	O
algorithms	O
that	O
are	O
generically	O
reasonable	O
although	O
there	O
may	O
be	O
cases	O
where	O
an	O
alternative	O
algorithm	B
may	O
be	O
considerably	O
more	O
remark	O
does	O
not	O
mean	B
putting	O
triangles	O
on	O
the	O
original	O
graph	B
note	O
that	O
a	O
triangulated	B
graph	B
is	O
not	O
one	O
in	O
which	O
squares	O
in	O
the	O
original	O
graph	B
have	O
triangles	O
within	O
them	O
in	O
the	O
triangulated	B
graph	B
whilst	O
this	O
is	O
the	O
case	O
for	O
this	O
is	O
not	O
true	O
for	O
the	O
term	O
triangulation	B
refers	O
to	O
the	O
fact	O
that	O
every	O
square	O
loop	O
of	O
length	O
must	O
have	O
a	O
triangle	O
with	O
edges	O
added	O
until	O
this	O
criterion	O
is	O
satisfied	O
greedy	O
variable	B
elimination	I
an	O
intuitive	O
way	O
to	O
think	O
of	O
triangulation	B
is	O
to	O
first	O
start	O
with	O
simplical	B
nodes	I
namely	O
those	O
which	O
when	O
eliminated	O
do	O
not	O
introduce	O
any	O
extra	O
links	O
in	O
the	O
remaining	O
graph	B
next	O
consider	O
a	O
non-simplical	O
node	O
of	O
the	O
remaining	O
graph	B
that	O
has	O
the	O
minimal	O
number	O
of	O
neighbours	O
then	O
add	O
a	O
link	O
between	O
all	O
neighbours	O
of	O
this	O
node	O
and	O
finally	O
eliminate	O
this	O
node	O
from	O
the	O
graph	B
continue	O
until	O
all	O
nodes	O
have	O
been	O
eliminated	O
procedure	O
corresponds	O
to	O
rose-tarjan	O
with	O
a	O
particular	O
node	O
elimination	O
choice	O
by	O
labelling	O
the	O
nodes	O
eliminated	O
in	O
sequence	O
we	O
obtain	O
a	O
perfect	O
ordering	O
below	O
in	O
reverse	O
in	O
the	O
case	O
that	O
variables	O
have	O
different	O
numbers	O
of	O
states	O
a	O
more	O
refined	O
version	O
is	O
to	O
choose	O
the	O
non-simplical	O
node	O
i	O
which	O
when	O
eliminated	O
leaves	O
the	O
smallest	O
clique	B
table	O
size	O
product	O
of	O
the	O
size	O
of	O
all	O
the	O
state	O
dimensions	O
of	O
the	O
neighbours	O
of	O
node	O
i	O
see	O
for	O
an	O
example	O
definition	O
elimination	O
in	O
variable	B
elimination	I
one	O
simply	O
picks	O
any	O
non-deleted	O
node	O
x	O
in	O
the	O
graph	B
and	O
then	O
adds	O
links	O
to	O
all	O
the	O
neighbours	O
of	O
x	O
node	O
x	O
is	O
then	O
deleted	O
one	O
repeats	O
this	O
until	O
all	O
nodes	O
have	O
been	O
whilst	O
this	O
procedure	O
guarantees	O
a	O
triangulated	B
graph	B
its	O
efficiency	O
depends	O
heavily	O
on	O
the	O
sequence	O
of	O
nodes	O
chosen	O
to	O
be	O
eliminated	O
several	O
heuristics	O
for	O
this	O
have	O
been	O
proposed	O
including	O
the	O
one	O
below	O
which	O
corresponds	O
to	O
choosing	O
x	O
to	O
be	O
the	O
node	O
with	O
the	O
minimal	O
number	O
of	O
neighbours	O
maximum	B
cardinality	I
checking	I
terminates	O
with	O
success	O
if	O
the	O
graph	B
is	O
triangulated	B
not	O
only	O
is	O
this	O
a	O
sufficient	O
condition	O
for	O
a	O
graph	B
to	O
be	O
triangulated	B
but	O
is	O
also	O
necessary	O
it	O
processes	O
each	O
node	O
and	O
the	O
time	O
to	O
process	O
a	O
node	O
is	O
quadratic	O
in	O
the	O
number	O
of	O
adjacent	O
nodes	O
this	O
triangulation	B
checking	O
algorithm	B
also	O
suggests	O
draft	O
march	O
the	O
junction	B
tree	B
algorithm	B
figure	O
starting	O
with	O
the	O
markov	B
network	I
in	O
the	O
maximum	B
cardinality	I
check	B
algorithm	B
proceeds	O
until	O
where	O
an	O
additional	O
link	O
is	O
required	O
one	O
continues	O
the	O
algorithm	B
until	O
the	O
fully	O
triangulated	B
graph	B
is	O
found	O
a	O
triangulation	B
construction	B
algorithm	B
we	O
simply	O
add	O
a	O
link	O
between	O
the	O
two	O
neighbours	O
that	O
caused	O
the	O
algorithm	B
to	O
fail	O
and	O
then	O
restart	O
the	O
algorithm	B
the	O
algorithm	B
is	O
restarted	O
from	O
the	O
beginning	O
not	O
just	O
continued	O
from	O
the	O
current	O
node	O
this	O
is	O
important	O
since	O
the	O
new	O
link	O
may	O
change	O
the	O
connectivity	O
between	O
previously	O
labelled	B
nodes	O
see	O
for	O
an	O
definition	O
elimination	O
order	O
let	O
the	O
n	O
variables	O
in	O
a	O
markov	B
network	I
be	O
ordered	O
from	O
to	O
n	O
the	O
ordering	O
is	O
perfect	O
if	O
for	O
each	O
node	O
i	O
the	O
neighbours	O
of	O
i	O
that	O
are	O
later	O
in	O
the	O
ordering	O
and	O
i	O
itself	O
form	O
a	O
clique	B
this	O
means	O
that	O
when	O
we	O
eliminate	O
the	O
variables	O
in	O
sequence	O
from	O
to	O
n	O
no	O
additional	O
links	O
are	O
induced	O
in	O
the	O
remaining	O
marginal	B
graph	B
a	O
graph	B
which	O
admits	O
a	O
perfect	B
elimination	I
order	I
is	O
decomposable	B
and	O
vice	O
versa	O
algorithm	B
a	O
check	B
if	O
a	O
graph	B
is	O
decomposable	B
the	O
graph	B
is	O
triangulated	B
if	O
after	O
cycling	O
through	O
all	O
the	O
n	O
nodes	O
in	O
the	O
graph	B
the	O
fail	O
criterion	O
is	O
not	O
encountered	O
choose	O
any	O
node	O
in	O
the	O
graph	B
and	O
label	O
it	O
for	O
i	O
to	O
n	O
do	O
end	O
for	O
where	O
there	O
is	O
more	O
than	O
one	O
node	O
with	O
the	O
most	O
labeled	O
neighbours	O
the	O
tie	O
may	O
be	O
broken	O
arbitrarily	O
choose	O
the	O
node	O
with	O
the	O
most	O
labeled	O
neighbours	O
and	O
label	O
it	O
i	O
if	O
any	O
two	O
labeled	O
neighbours	O
of	O
i	O
are	O
not	O
adjacent	O
to	O
each	O
other	O
fail	O
the	O
junction	B
tree	B
algorithm	B
we	O
now	O
have	O
all	O
the	O
steps	O
required	O
for	O
inference	B
in	O
multiply-connected	B
graphs	O
moralisation	B
marry	O
the	O
parents	B
this	O
is	O
required	O
only	O
for	O
directed	B
distributions	O
triangulation	B
ensure	O
that	O
every	O
loop	O
of	O
length	O
or	O
more	O
has	O
a	O
chord	B
junction	B
tree	B
form	O
a	O
junction	B
tree	B
from	O
cliques	O
of	O
the	O
triangulated	B
graph	B
removing	O
any	O
unnecessary	O
links	O
in	O
a	O
loop	O
on	O
the	O
cluster	O
graph	B
algorithmically	O
this	O
can	O
be	O
achieved	O
by	O
finding	O
a	O
tree	B
with	O
maximal	O
spanning	O
weight	B
with	O
weight	B
wij	O
given	O
by	O
the	O
number	O
of	O
variables	O
in	O
the	O
separator	B
between	O
cliques	O
i	O
and	O
j	O
alternatively	O
given	O
a	O
clique	B
elimination	O
order	O
the	O
lowest	O
cliques	O
eliminated	O
first	O
one	O
may	O
connect	O
each	O
clique	B
i	O
to	O
the	O
single	O
neighbouring	O
clique	B
j	O
i	O
with	O
greatest	O
edge	O
weight	B
wij	O
example	O
is	O
due	O
to	O
david	O
page	O
www	O
cs	O
wisc	O
edu	O
draft	O
march	O
the	O
junction	B
tree	B
algorithm	B
a	O
c	O
f	O
b	O
e	O
d	O
g	O
b	O
e	O
a	O
c	O
f	O
d	O
g	O
h	O
i	O
h	O
i	O
original	O
figure	O
loopy	B
belief	B
network	I
the	O
moralisation	B
links	O
are	O
between	O
nodes	O
e	O
and	O
f	O
and	O
between	O
nodes	O
f	O
and	O
g	O
the	O
other	O
additional	O
links	O
come	O
from	O
triangulation	B
the	O
clique	B
size	O
of	O
the	O
resulting	O
clique	B
tree	B
shown	O
is	O
four	O
from	O
potential	B
assignment	O
assign	O
potentials	O
to	O
junction	B
tree	B
cliques	O
and	O
set	O
the	O
separator	B
potentials	O
to	O
unity	O
message	B
propagation	B
carry	O
out	O
absorption	B
until	O
updates	O
have	O
been	O
passed	O
along	O
both	O
directions	O
of	O
every	O
link	O
on	O
the	O
jt	O
the	O
clique	B
marginals	O
can	O
then	O
be	O
read	O
off	O
from	O
the	O
jt	O
an	O
example	O
is	O
given	O
in	O
remarks	O
on	O
the	O
jta	O
the	O
algorithm	B
provides	O
an	O
upper	O
bound	B
on	O
the	O
computation	O
required	O
to	O
calculate	O
marginals	O
in	O
the	O
graph	B
there	O
may	O
exist	O
more	O
efficient	O
algorithms	O
in	O
particular	O
cases	O
although	O
generally	O
it	O
is	O
believed	O
that	O
there	O
cannot	O
be	O
much	O
more	O
efficient	O
approaches	O
than	O
the	O
jta	O
since	O
every	O
other	O
approach	B
must	O
perform	O
a	O
one	O
particular	O
special	O
case	O
is	O
that	O
of	O
marginal	B
inference	B
for	O
a	O
binary	O
variable	O
mrf	O
on	O
a	O
two-dimensional	O
lattice	O
containing	O
only	O
pure	O
quadratic	O
interactions	O
in	O
this	O
case	O
the	O
complexity	O
of	O
computing	O
a	O
marginal	B
inference	B
is	O
where	O
n	O
is	O
the	O
number	O
of	O
variables	O
in	O
the	O
distribution	B
this	O
is	O
in	O
contrast	O
to	O
the	O
pessimistic	O
exponential	B
complexity	O
suggested	O
by	O
the	O
jta	O
such	O
cases	O
are	O
highly	O
specialised	O
and	O
it	O
is	O
unlikely	O
that	O
a	O
general	O
purpose	O
algorithm	B
that	O
could	O
consistently	O
outperform	O
the	O
jta	O
exists	O
one	O
might	O
think	O
that	O
the	O
only	O
class	O
of	O
distributions	O
for	O
which	O
essentially	O
a	O
linear	B
time	O
algorithm	B
is	O
available	O
are	O
singly-connected	B
distributions	O
however	O
there	O
are	O
decomposable	B
graphs	O
for	O
which	O
the	O
cliques	O
have	O
limited	O
size	O
meaning	O
that	O
inference	B
is	O
tractable	O
for	O
example	O
an	O
extended	O
version	O
of	O
the	O
ladder	O
in	O
has	O
a	O
simple	O
induced	O
decomposable	B
representation	O
for	O
which	O
marginal	B
inference	B
would	O
be	O
linear	B
in	O
the	O
number	O
of	O
rungs	O
in	O
the	O
ladder	O
effectively	O
these	O
structures	O
are	O
hyper	B
trees	O
in	O
which	O
the	O
complexity	O
is	O
then	O
related	O
to	O
the	O
tree	B
width	I
of	O
the	O
ideally	O
we	O
would	O
like	O
to	O
find	O
a	O
triangulated	B
graph	B
which	O
has	O
minimal	O
clique	B
size	O
however	O
it	O
can	O
be	O
shown	O
to	O
be	O
a	O
hard-computation	O
problem	B
p	O
to	O
find	O
the	O
most	O
efficient	O
triangulation	B
in	O
practice	O
most	O
general	O
purpose	O
triangulation	B
algorithms	O
are	O
somewhat	O
heuristic	O
and	O
chosen	O
to	O
provide	O
reasonable	O
but	O
clearly	O
not	O
optimal	O
generic	O
performance	B
numerical	B
overunder	O
flow	O
issues	O
can	O
occur	O
in	O
large	O
cliques	O
where	O
many	O
probability	O
values	O
are	O
multiplied	O
together	O
similarly	O
in	O
long	O
chains	O
since	O
absorption	B
will	O
tend	O
to	O
reduce	O
the	O
numerical	B
size	O
of	O
potential	B
entries	O
in	O
a	O
clique	B
if	O
we	O
only	O
care	O
about	O
marginals	O
we	O
can	O
avoid	O
numerical	B
difficulties	O
by	O
normalising	O
potentials	O
at	O
each	O
step	O
these	O
missing	B
normalisation	B
constants	O
can	O
always	O
be	O
found	O
under	O
the	O
normalisation	B
constraint	O
if	O
required	O
one	O
can	O
always	O
store	O
the	O
values	O
of	O
these	O
local	B
renormalisations	O
should	O
for	O
example	O
the	O
global	B
normalisation	B
constant	I
of	O
a	O
distribution	B
be	O
required	O
see	O
after	O
clamping	O
variables	O
in	O
evidential	O
states	O
running	O
the	O
jta	O
returns	O
the	O
joint	B
distribution	B
on	O
the	O
non-evidential	O
variables	O
in	O
a	O
clique	B
with	O
all	O
the	O
evidential	O
variables	O
clamped	O
in	O
their	O
evidential	O
states	O
from	O
this	O
conditionals	O
are	O
straightforward	O
to	O
calculate	O
imagine	O
that	O
we	O
have	O
run	O
the	O
jt	O
algorithm	B
and	O
want	O
to	O
afterwards	O
find	O
the	O
marginal	B
pxevidence	O
we	O
could	O
do	O
so	O
by	O
clamping	O
the	O
evidential	O
variables	O
however	O
if	O
both	O
x	O
and	O
the	O
set	O
of	O
evidential	O
draft	O
march	O
the	O
junction	B
tree	B
algorithm	B
variables	O
are	O
all	O
contained	O
within	O
a	O
single	O
clique	B
of	O
the	O
jt	O
then	O
we	O
may	O
use	O
the	O
consistent	B
jt	O
cliques	O
to	O
compute	O
pxevidence	O
the	O
reason	O
is	O
that	O
since	O
the	O
jt	O
clique	B
contains	O
the	O
marginal	B
on	O
the	O
set	O
of	O
variables	O
which	O
includes	O
x	O
and	O
the	O
evidential	O
variables	O
we	O
can	O
obtain	O
the	O
required	O
marginal	B
by	O
considering	O
the	O
single	O
jt	O
clique	B
alone	O
representing	O
the	O
marginal	B
distribution	B
of	O
a	O
set	O
of	O
variables	O
x	O
which	O
are	O
not	O
contained	O
within	O
a	O
single	O
clique	B
is	O
in	O
general	O
computationally	O
difficult	O
whilst	O
the	O
probability	O
of	O
any	O
state	O
of	O
px	O
may	O
be	O
computed	O
efficiently	O
there	O
are	O
in	O
general	O
an	O
exponential	B
number	O
of	O
such	O
states	O
a	O
classical	O
example	O
in	O
this	O
regard	O
is	O
the	O
hmm	B
with	O
singly-connected	B
joint	B
distribution	B
pvh	O
however	O
the	O
marginal	B
distribution	B
ph	O
is	O
fully	O
connected	B
this	O
means	O
that	O
for	O
example	O
whilst	O
the	O
entropy	B
of	O
pvh	O
is	O
straightforward	O
to	O
compute	O
the	O
entropy	B
of	O
the	O
marginal	B
ph	O
is	O
intractable	O
computing	O
the	O
normalisation	B
constant	I
of	O
a	O
distribution	B
for	O
a	O
markov	B
network	I
px	O
z	O
how	O
can	O
we	O
find	O
z	O
efficiently	O
if	O
we	O
used	O
the	O
jta	O
on	O
the	O
unnormalised	O
i	O
have	O
the	O
equivalent	B
representation	O
i	O
we	O
would	O
z	O
c	O
s	O
c	O
s	O
px	O
z	O
x	O
z	O
xc	O
since	O
the	O
distribution	B
must	O
normalise	O
we	O
can	O
obtain	O
z	O
from	O
for	O
a	O
consistent	B
jt	O
summing	O
first	O
over	O
the	O
variables	O
of	O
a	O
simplical	B
jt	O
clique	B
including	O
the	O
separator	B
variables	O
the	O
marginal	B
clique	B
will	O
cancel	O
with	O
the	O
corresponding	O
separator	B
to	O
give	O
a	O
unity	O
term	O
so	O
that	O
the	O
clique	B
and	O
separator	B
can	O
be	O
removed	O
this	O
forms	O
a	O
new	O
jt	O
for	O
which	O
we	O
then	O
eliminate	O
another	O
simplical	B
clique	B
continuing	O
in	O
this	O
manner	O
we	O
will	O
be	O
left	O
with	O
a	O
single	O
numerator	O
potential	B
so	O
that	O
this	O
is	O
true	O
for	O
any	O
clique	B
c	O
so	O
it	O
makes	O
sense	O
to	O
choose	O
one	O
with	O
a	O
small	O
number	O
of	O
states	O
so	O
that	O
the	O
resulting	O
raw	O
summation	O
is	O
efficient	O
hence	O
in	O
order	O
to	O
compute	O
the	O
normalisation	B
constant	I
of	O
a	O
distribution	B
one	O
runs	O
the	O
jt	O
algorithm	B
on	O
an	O
unnormalised	O
distribution	B
and	O
the	O
global	B
normalisation	B
is	O
then	O
given	O
by	O
the	O
local	B
normalisation	B
of	O
any	O
clique	B
note	O
that	O
if	O
the	O
graph	B
is	O
disconnected	B
are	O
isolated	O
cliques	O
the	O
normalisation	B
is	O
the	O
product	O
of	O
the	O
connected	B
component	O
normalisation	B
constants	O
a	O
computationally	O
convenient	O
way	O
to	O
find	O
this	O
is	O
to	O
compute	O
the	O
product	O
of	O
all	O
clique	B
normalisations	O
divided	O
by	O
the	O
product	O
of	O
all	O
separator	B
normalisations	O
the	O
marginal	B
likelihood	B
our	O
interest	O
here	O
is	O
the	O
computation	O
of	O
pv	O
where	O
v	O
is	O
a	O
subset	O
of	O
the	O
full	O
variable	O
set	O
naively	O
one	O
could	O
carry	O
out	O
this	O
computation	O
by	O
summing	O
over	O
all	O
the	O
non-evidential	O
variables	O
variables	O
h	O
xv	O
explicitly	O
in	O
cases	O
where	O
this	O
is	O
computationally	O
impractical	O
an	O
alternative	O
is	O
to	O
use	O
phv	O
pvh	O
pv	O
one	O
can	O
view	O
this	O
as	O
a	O
product	O
of	O
clique	B
potentials	O
divided	O
by	O
the	O
normalisation	B
pv	O
for	O
which	O
the	O
general	O
method	O
of	O
may	O
be	O
directly	O
applied	O
see	O
demojtree	O
m	O
draft	O
march	O
the	O
junction	B
tree	B
algorithm	B
example	O
simple	O
example	O
of	O
the	O
jta	O
consider	O
running	O
the	O
jta	O
on	O
the	O
simple	O
graph	B
pa	O
b	O
c	O
pabpbcpc	O
the	O
moralisation	B
and	O
triangulation	B
steps	O
are	O
trivial	O
and	O
the	O
jta	O
is	O
given	O
immediately	O
by	O
the	O
figure	O
on	O
the	O
right	O
a	O
valid	O
assignment	O
is	O
a	O
ab	O
b	O
b	O
c	O
bc	O
to	O
find	O
a	O
marginal	B
pb	O
we	O
first	O
run	O
the	O
jta	O
b	O
pab	O
c	O
pbcpc	O
absorbing	O
from	O
ab	O
through	O
b	O
the	O
new	O
separator	B
is	O
a	O
b	O
a	O
pab	O
the	O
new	O
potential	B
on	O
c	O
is	O
given	O
by	O
pbcpc	O
c	O
c	O
c	O
c	O
c	O
absorbing	O
from	O
bc	O
through	O
b	O
the	O
new	O
separator	B
is	O
pbcpc	O
the	O
new	O
potential	B
on	O
b	O
is	O
given	O
by	O
b	O
b	O
c	O
pbcpc	O
this	O
is	O
therefore	O
indeed	O
equal	O
to	O
the	O
marginal	B
the	O
new	O
separator	B
contains	O
the	O
marginal	B
pb	O
since	O
pbcpc	O
pb	O
c	O
pb	O
c	O
c	O
c	O
pa	O
b	O
c	O
pa	O
b	O
example	O
a	O
conditional	B
marginal	B
continuing	O
with	O
the	O
distribution	B
in	O
we	O
consider	O
how	O
to	O
compute	O
pba	O
c	O
first	O
we	O
clamp	O
the	O
evidential	O
variables	O
in	O
their	O
states	O
then	O
we	O
claim	O
that	O
the	O
effect	O
of	O
running	O
the	O
jta	O
is	O
to	O
produce	O
on	O
a	O
set	O
of	O
clique	B
variables	O
x	O
the	O
marginals	O
on	O
the	O
cliques	O
px	O
we	O
demonstrate	O
this	O
below	O
a	O
pab	O
however	O
since	O
a	O
is	O
clamped	O
in	O
state	O
a	O
then	O
the	O
summation	O
is	O
not	O
carried	O
out	O
over	O
a	O
and	O
we	O
have	O
instead	O
in	O
general	O
the	O
new	O
separator	B
is	O
given	O
by	O
pa	O
the	O
new	O
potential	B
on	O
the	O
c	O
clique	B
is	O
given	O
by	O
a	O
b	O
c	O
c	O
c	O
c	O
c	O
the	O
new	O
separator	B
is	O
normally	O
given	O
by	O
pbcpc	O
pbc	O
however	O
since	O
c	O
is	O
clamped	O
in	O
state	O
we	O
have	O
instead	O
pbc	O
draft	O
march	O
finding	O
the	O
most	B
likely	I
state	I
the	O
new	O
potential	B
on	O
b	O
is	O
given	O
by	O
b	O
b	O
pa	O
pa	O
pa	O
the	O
effect	O
of	O
clamping	O
a	O
set	O
of	O
variables	O
v	O
in	O
their	O
evidential	O
states	O
and	O
running	O
the	O
jta	O
is	O
that	O
for	O
a	O
clique	B
i	O
which	O
contains	O
the	O
set	O
of	O
non-evidential	O
variables	O
hi	O
the	O
consistent	B
potential	B
from	O
the	O
jta	O
contains	O
the	O
marginal	B
phiv	O
finding	O
a	O
conditional	B
marginal	B
is	O
then	O
straightforward	O
by	O
ensuring	O
normalisation	B
example	O
the	O
likelihood	B
pa	O
c	O
the	O
effect	O
of	O
clamping	O
the	O
variables	O
in	O
their	O
evidential	O
states	O
and	O
running	O
the	O
jta	O
produces	O
the	O
joint	B
marginals	O
such	O
as	O
b	O
pa	O
b	O
c	O
then	O
calculating	O
the	O
likelihood	B
is	O
easy	O
since	O
we	O
just	O
sum	O
out	O
over	O
the	O
non-evidential	O
variables	O
of	O
any	O
converged	O
potential	B
pa	O
c	O
b	O
b	O
b	O
pa	O
b	O
c	O
finding	O
the	O
most	B
likely	I
state	I
a	O
quantity	O
of	O
interest	O
is	O
the	O
most	O
likely	O
joint	B
state	O
of	O
a	O
distribution	B
argmax	O
px	O
and	O
it	O
is	O
natural	B
to	O
wonder	O
how	O
this	O
can	O
be	O
efficiently	O
computed	O
in	O
the	O
case	O
of	O
a	O
loopy	B
distribution	B
since	O
the	O
development	O
of	O
the	O
jta	O
is	O
based	O
around	O
a	O
variable	B
elimination	I
procedure	O
and	O
the	O
max	O
operator	O
distributes	O
over	O
the	O
distribution	B
as	O
well	O
eliminating	O
a	O
variable	O
by	O
maximising	O
over	O
that	O
variable	O
will	O
have	O
the	O
same	O
effect	O
on	O
the	O
graph	B
structure	B
as	O
summation	O
did	O
this	O
means	O
that	O
a	O
junction	B
tree	B
is	O
again	O
an	O
appropriate	O
structure	B
on	O
which	O
to	O
perform	O
max	O
operations	O
once	O
a	O
jt	O
has	O
been	O
constructed	O
one	O
then	O
uses	O
the	O
max	O
absorption	B
procedure	O
below	O
to	O
perform	O
maximisation	B
over	O
the	O
variables	O
after	O
a	O
full	O
round	O
of	O
absorption	B
has	O
been	O
carried	O
out	O
the	O
cliques	O
contain	O
the	O
distribution	B
on	O
the	O
variables	O
of	O
the	O
clique	B
with	O
all	O
remaining	O
variables	O
set	O
to	O
their	O
optimal	O
states	O
the	O
optimal	O
local	B
states	O
can	O
be	O
found	O
by	O
explicit	O
optimisation	B
of	O
each	O
clique	B
potential	B
separately	O
note	O
that	O
this	O
procedure	O
holds	O
also	O
for	O
non-distributions	O
in	O
this	O
sense	O
this	O
is	O
an	O
example	O
of	O
a	O
more	O
general	O
dynamic	B
programming	O
procedure	O
applied	O
in	O
a	O
case	O
where	O
the	O
underlying	O
graph	B
is	O
multiply-connected	B
this	O
demonstrates	O
how	O
to	O
efficiently	O
compute	O
the	O
optimum	O
of	O
a	O
multiply-connected	B
function	B
defined	O
as	O
the	O
product	O
on	O
potentials	O
definition	O
absorption	B
let	O
v	O
and	O
w	O
be	O
neighbours	O
in	O
a	O
clique	B
graph	B
let	O
s	O
be	O
their	O
separator	B
and	O
let	O
and	O
be	O
their	O
potentials	O
absorption	B
replaces	O
the	O
tables	O
and	O
with	O
maxvs	O
once	O
messages	O
have	O
been	O
passed	O
in	O
both	O
directions	O
over	O
all	O
separators	O
according	O
to	O
a	O
valid	O
schedule	B
the	O
most-likely	O
joint	B
state	O
can	O
be	O
read	O
off	O
from	O
maximising	O
the	O
state	O
of	O
the	O
clique	B
potentials	O
this	O
is	O
implemented	O
in	O
absorb	O
m	O
and	O
absorption	B
m	O
where	O
a	O
flag	O
is	O
used	O
to	O
switch	O
between	O
either	O
sum	O
or	O
max	O
absorption	B
draft	O
march	O
reabsorption	B
converting	O
a	O
junction	B
tree	B
to	O
a	O
directed	B
network	O
abc	O
c	O
c	O
e	O
dce	O
e	O
abc	O
c	O
c	O
e	O
cf	O
dce	O
e	O
cf	O
a	O
b	O
c	O
e	O
d	O
f	O
eg	O
eh	O
eg	O
eh	O
g	O
h	O
figure	O
junction	B
tree	B
directed	B
junction	B
tree	B
in	O
which	O
all	O
edges	O
are	O
consistently	O
oriented	O
away	O
from	O
the	O
clique	B
a	O
set	B
chain	B
formed	O
from	O
the	O
junction	B
tree	B
by	O
reabsorbing	O
each	O
separator	B
into	O
its	O
child	O
clique	B
reabsorption	B
converting	O
a	O
junction	B
tree	B
to	O
a	O
directed	B
network	O
it	O
is	O
sometimes	O
useful	O
to	O
be	O
able	O
to	O
convert	O
the	O
jt	O
back	O
to	O
a	O
bn	O
of	O
a	O
desired	O
form	O
for	O
example	O
if	O
one	O
wishes	O
to	O
draw	O
samples	O
from	O
a	O
markov	B
network	I
this	O
can	O
be	O
achieved	O
by	O
ancestral	B
sampling	B
on	O
an	O
equivalent	B
directed	B
structure	B
see	O
revisiting	O
the	O
example	O
from	O
we	O
have	O
the	O
jt	O
given	O
in	O
to	O
find	O
a	O
valid	O
directed	B
representation	O
we	O
first	O
orient	O
the	O
jt	O
edges	O
consistently	O
away	O
from	O
a	O
chosen	O
root	O
node	O
singleparenttree	O
m	O
thereby	O
forming	O
a	O
directed	B
jt	O
which	O
has	O
the	O
property	O
that	O
each	O
clique	B
has	O
at	O
most	O
one	O
parent	O
clique	B
definition	O
v	O
s	O
w	O
v	O
w	O
let	O
v	O
and	O
w	O
be	O
neighbouring	O
cliques	O
in	O
a	O
directed	B
jt	O
in	O
which	O
each	O
clique	B
in	O
the	O
tree	B
has	O
at	O
most	O
one	O
parent	O
furthermore	O
let	O
s	O
be	O
their	O
separator	B
and	O
and	O
be	O
the	O
potentials	O
reabsorption	B
into	O
w	O
removes	O
the	O
separator	B
and	O
forms	O
a	O
conditional	B
distribution	B
pwv	O
we	O
say	O
that	O
clique	B
w	O
reabsorbs	O
the	O
separator	B
s	O
in	O
where	O
one	O
amongst	O
many	O
possible	O
directed	B
representations	O
is	O
formed	O
from	O
the	O
jt	O
specifically	O
represents	O
pa	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
pe	O
gpd	O
c	O
epa	O
b	O
cpc	O
fpe	O
h	O
pepcpcpe	O
we	O
now	O
have	O
many	O
choices	O
as	O
to	O
which	O
clique	B
re-absorbs	O
a	O
separator	B
one	O
such	O
choice	O
would	O
give	O
pa	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
pgepd	O
ecpa	O
b	O
c	O
this	O
can	O
be	O
represented	O
using	O
a	O
so-called	O
set	O
in	O
chains	O
generalise	O
belief	B
networks	I
to	O
a	O
product	O
of	O
clusters	O
of	O
variables	O
conditioned	O
on	O
parents	B
by	O
writing	O
each	O
of	O
the	O
set	O
conditional	B
probabilities	O
as	O
local	B
conditional	B
bns	O
one	O
may	O
also	O
write	O
full	O
bn	O
for	O
example	O
one	O
such	O
would	O
be	O
given	O
from	O
the	O
decomposition	B
pca	B
bpbapapgepfcphepde	O
cpec	O
draft	O
march	O
code	O
figure	O
diseases	O
giving	O
rise	O
to	O
symptoms	O
assuming	O
the	O
symptoms	O
are	O
all	O
instantiated	O
the	O
triangulated	B
graph	B
of	O
the	O
diseases	O
is	O
a	O
clique	B
the	O
need	O
for	O
approximations	O
the	O
jta	O
provides	O
an	O
upper	O
bound	B
on	O
the	O
complexity	O
of	O
inference	B
and	O
attempts	O
to	O
exploit	O
the	O
structure	B
of	O
the	O
graph	B
to	O
reduce	O
computations	O
however	O
in	O
a	O
great	O
deal	O
of	O
interesting	O
applications	O
the	O
use	O
of	O
the	O
jta	O
algorithm	B
would	O
result	O
in	O
clique-sizes	O
in	O
the	O
triangulated	B
graph	B
that	O
are	O
prohibitively	O
large	O
a	O
classical	O
situation	O
in	O
which	O
this	O
can	O
arise	O
are	O
disease-symptom	O
networks	O
for	O
example	O
for	O
the	O
graph	B
in	O
the	O
triangulated	B
graph	B
of	O
the	O
diseases	O
is	O
fully	O
coupled	B
meaning	O
that	O
no	O
simplification	O
can	O
occur	O
in	O
general	O
this	O
situation	O
is	O
common	O
in	O
such	O
bipartite	O
networks	O
even	O
when	O
the	O
children	B
only	O
have	O
a	O
small	O
number	O
of	O
parents	B
intuitively	O
as	O
one	O
eliminates	O
each	O
parent	O
links	O
are	O
added	O
between	O
other	O
parents	B
mediated	O
via	O
the	O
common	O
children	B
unless	O
the	O
graph	B
is	O
highly	O
regular	O
analogous	O
to	O
a	O
form	O
of	O
hidden	B
markov	I
model	B
this	O
fill-in	O
effect	O
rapidly	O
results	O
in	O
large	O
cliques	O
and	O
intractable	O
computations	O
dealing	O
with	O
large	O
clique	B
in	O
the	O
triangulated	B
graph	B
is	O
an	O
active	B
research	O
topic	O
and	O
we	O
ll	O
discuss	O
strategies	O
to	O
approximate	B
the	O
computations	O
in	O
bounded	O
width	O
junction	O
trees	O
in	O
some	O
applications	O
we	O
may	O
be	O
at	O
liberty	O
to	O
choose	O
the	O
structure	B
of	O
the	O
markov	B
network	I
for	O
example	O
if	O
we	O
wish	O
to	O
fit	O
a	O
markov	B
network	I
to	O
data	O
we	O
may	O
wish	O
to	O
use	O
as	O
complex	O
a	O
markov	B
network	I
as	O
we	O
can	O
computationally	O
afford	O
in	O
such	O
cases	O
we	O
desire	O
that	O
the	O
clique	B
sizes	O
of	O
the	O
resulting	O
triangulated	B
markov	B
network	I
are	O
smaller	O
than	O
a	O
specified	O
tree	B
width	I
the	O
corresponding	O
junction	B
tree	B
as	O
a	O
hypertree	O
constructing	O
such	O
bounded	O
width	O
or	O
thin	B
junction	O
trees	O
is	O
an	O
active	B
research	O
topic	O
a	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
start	O
with	O
a	O
graph	B
and	O
include	O
a	O
randomly	O
chosen	O
edge	O
provided	O
that	O
the	O
size	O
of	O
all	O
cliques	O
in	O
the	O
resulting	O
triangulated	B
graph	B
is	O
below	O
a	O
specified	O
maximal	O
width	O
see	O
demothinjt	O
m	O
and	O
makethinjt	O
m	O
which	O
assumes	O
an	O
initial	O
graph	B
g	O
and	O
a	O
graph	B
of	O
candidate	O
edges	O
c	O
iteratively	O
expanding	O
g	O
until	O
a	O
maximal	O
tree	B
width	I
limit	O
is	O
reached	O
see	O
also	O
for	O
a	O
discussion	O
on	O
learning	B
an	O
appropriate	O
markov	O
structure	B
based	O
on	O
data	O
code	O
absorb	O
m	O
absorption	B
update	O
v	O
s	O
w	O
absorption	B
m	O
full	O
absorption	B
schedule	B
over	O
tree	B
jtree	O
m	O
form	O
a	O
junction	B
tree	B
triangulate	O
m	O
triangulation	B
based	O
on	O
simple	O
node	O
elimination	O
utility	B
routines	O
knowing	O
if	O
an	O
undirected	B
graph	B
is	O
a	O
tree	B
and	O
returning	O
a	O
valid	O
elimination	O
sequence	O
is	O
useful	O
a	O
connected	B
graph	B
is	O
a	O
tree	B
if	O
the	O
number	O
of	O
edges	O
plus	O
is	O
equal	O
to	O
the	O
number	O
of	O
nodes	O
however	O
for	O
a	O
possibly	O
disconnected	B
graph	B
this	O
is	O
not	O
the	O
case	O
the	O
code	O
deals	O
with	O
the	O
possibly	O
disconnected	B
case	O
returning	O
a	O
valid	O
elimination	O
sequence	O
if	O
the	O
graph	B
is	O
singly-connected	B
the	O
routine	O
is	O
based	O
on	O
the	O
observation	O
that	O
any	O
singly-connected	B
graph	B
must	O
always	O
possess	O
a	O
simplical	B
node	O
which	O
can	O
be	O
eliminated	O
to	O
reveal	O
a	O
smaller	O
singly-connected	B
graph	B
istree	O
m	O
if	O
graph	B
is	O
singly	O
connected	B
return	O
and	O
elimination	O
sequence	O
elimtri	O
m	O
vertexnode	O
elimination	O
on	O
a	O
triangulated	B
graph	B
with	O
given	O
end	O
node	O
demojtree	O
m	O
junction	B
tree	B
chest	B
clinic	I
draft	O
march	O
exercises	O
exercise	O
show	O
that	O
the	O
markov	B
network	I
elimination	O
labelling	O
for	O
this	O
graph	B
exercise	O
consider	O
the	O
following	O
distribution	B
exercises	O
is	O
not	O
perfect	O
elimination	O
ordered	O
and	O
give	O
a	O
perfect	O
draw	O
a	O
clique	B
graph	B
that	O
represents	O
this	O
distribution	B
and	O
indicate	O
the	O
separators	O
on	O
the	O
graph	B
write	O
down	O
an	O
alternative	O
formula	O
for	O
the	O
distribution	B
in	O
terms	O
of	O
the	O
marginal	B
probabilities	O
exercise	O
consider	O
the	O
distribution	B
write	O
down	O
a	O
junction	B
tree	B
for	O
the	O
above	O
graph	B
carry	O
out	O
the	O
absorption	B
procedure	O
and	O
demonstrate	O
that	O
this	O
gives	O
the	O
correct	O
result	O
for	O
the	O
marginal	B
exercise	O
consider	O
the	O
distribution	B
pa	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
papbapcapdapebpfcpgdphe	O
fpif	O
g	O
draw	O
the	O
belief	B
network	I
for	O
this	O
distribution	B
draw	O
the	O
moralised	O
graph	B
draw	O
the	O
triangulated	B
graph	B
your	O
triangulated	B
graph	B
should	O
contain	O
cliques	O
of	O
the	O
smallest	O
size	O
possible	O
draw	O
a	O
junction	B
tree	B
for	O
the	O
above	O
graph	B
and	O
verify	O
that	O
it	O
satisfies	O
the	O
running	B
intersection	I
property	I
describe	O
a	O
suitable	O
initialisation	O
of	O
clique	B
potentials	O
describe	O
the	O
absorption	B
procedure	O
and	O
write	O
down	O
an	O
appropriate	O
message	B
updating	O
schedule	B
exercise	O
this	O
question	O
concerns	O
the	O
distribution	B
pa	O
b	O
c	O
d	O
e	O
f	O
papbapcbpdcpedpfa	O
e	O
draw	O
the	O
belief	B
network	I
for	O
this	O
distribution	B
draw	O
the	O
moralised	O
graph	B
draw	O
the	O
triangulated	B
graph	B
your	O
triangulated	B
graph	B
should	O
contain	O
cliques	O
of	O
the	O
smallest	O
size	O
possible	O
draw	O
a	O
junction	B
tree	B
for	O
the	O
above	O
graph	B
and	O
verify	O
that	O
it	O
satisfies	O
the	O
running	B
intersection	I
property	I
describe	O
a	O
suitable	O
initialisation	O
of	O
clique	B
potentials	O
describe	O
the	O
absorption	B
procedure	O
and	O
an	O
appropriate	O
message	B
updating	O
schedule	B
show	O
that	O
the	O
distribution	B
can	O
be	O
expressed	O
in	O
the	O
form	O
pafpba	O
cpca	O
dpda	O
epea	O
fpf	O
draft	O
march	O
exercises	O
exercise	O
for	O
the	O
undirected	B
graph	B
on	O
the	O
square	O
lattice	O
as	O
shown	O
draw	O
a	O
triangulated	B
graph	B
with	O
the	O
smallest	O
clique	B
sizes	O
possible	O
exercise	O
consider	O
a	O
binary	O
variable	O
markov	B
random	B
field	I
px	O
z	O
ij	O
xj	O
defined	O
ixixj	O
for	O
i	O
a	O
neighbour	B
of	O
j	O
on	O
the	O
lattice	O
on	O
the	O
n	O
n	O
lattice	O
with	O
xj	O
e	O
and	O
i	O
j	O
a	O
naive	O
way	O
to	O
perform	O
inference	B
is	O
to	O
first	O
stack	O
all	O
the	O
variables	O
in	O
the	O
tth	O
column	O
and	O
call	O
this	O
cluster	O
variable	O
xt	O
as	O
shown	O
the	O
resulting	O
graph	B
is	O
then	O
singly-connected	B
what	O
is	O
the	O
complexity	O
of	O
computing	O
the	O
normalisation	B
constant	I
based	O
on	O
this	O
cluster	O
representation	O
compute	O
log	O
z	O
for	O
n	O
exercise	O
given	O
a	O
consistent	B
junction	B
tree	B
on	O
which	O
a	O
full	O
round	O
of	O
message	B
passing	B
has	O
occurred	O
explain	O
how	O
to	O
form	O
a	O
belief	B
network	I
from	O
the	O
junction	B
tree	B
exercise	O
the	O
file	O
diseasenet	O
mat	O
contains	O
the	O
potentials	O
for	O
a	O
disease	O
bi-partite	O
belief	B
network	I
with	O
diseases	O
and	O
symptoms	O
each	O
disease	O
and	O
symptom	O
is	O
a	O
binary	O
variable	O
and	O
each	O
symptom	O
connects	O
to	O
parent	O
diseases	O
using	O
the	O
brmltoolbox	O
construct	O
a	O
junction	B
tree	B
for	O
this	O
distribution	B
and	O
use	O
it	O
to	O
compute	O
all	O
the	O
marginals	O
of	O
the	O
symptoms	O
psi	O
on	O
most	O
standard	O
computers	O
computing	O
the	O
marginal	B
psi	O
by	O
raw	O
summation	O
of	O
the	O
joint	B
distribution	B
is	O
computationally	O
infeasible	O
explain	O
how	O
to	O
compute	O
the	O
marginals	O
psi	O
in	O
a	O
tractable	O
way	O
without	O
using	O
the	O
junction	B
tree	B
formalism	O
by	O
implementing	O
this	O
method	O
compare	O
it	O
with	O
the	O
results	O
from	O
the	O
junction	B
tree	B
algorithm	B
consider	O
the	O
scenario	O
in	O
which	O
all	O
the	O
symptom	O
variables	O
are	O
instantiated	O
using	O
the	O
junction	B
tree	B
estimate	O
an	O
upper	O
bound	B
on	O
the	O
number	O
of	O
seconds	O
that	O
computing	O
a	O
marginal	B
takes	O
assuming	O
that	O
for	O
a	O
two	O
clique	B
table	O
containing	O
s	O
states	O
absorption	B
takes	O
o	O
seconds	O
for	O
an	O
unspecified	O
compare	O
this	O
estimate	O
with	O
the	O
time	O
required	O
to	O
compute	O
the	O
marginal	B
by	O
raw	O
summation	O
of	O
the	O
instantiated	O
belief	B
network	I
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
making	O
decisions	O
expected	O
utility	B
this	O
chapter	O
concerns	O
situations	O
in	O
which	O
decisions	O
need	O
to	O
be	O
taken	O
under	O
uncertainty	B
consider	O
the	O
following	O
scenario	O
you	O
are	O
asked	O
if	O
you	O
wish	O
to	O
take	O
a	O
bet	O
on	O
the	O
outcome	O
of	O
tossing	O
a	O
fair	O
coin	O
if	O
you	O
bet	O
and	O
win	O
you	O
gain	O
if	O
you	O
bet	O
and	O
lose	O
you	O
lose	O
if	O
you	O
don	O
t	O
bet	O
the	O
cost	O
to	O
you	O
is	O
zero	O
we	O
can	O
set	O
this	O
up	O
using	O
a	O
two	O
state	O
variable	O
x	O
with	O
domx	O
lose	O
a	O
decision	O
variable	O
d	O
with	O
domd	O
no	O
bet	O
and	O
utilities	O
as	O
follows	O
uwin	O
bet	O
ulose	O
bet	O
uwin	O
no	O
bet	O
ulose	O
no	O
bet	O
since	O
we	O
don	O
t	O
know	O
the	O
state	O
of	O
x	O
in	O
order	O
to	O
make	O
a	O
decision	O
about	O
whether	O
or	O
not	O
to	O
bet	O
arguably	O
the	O
best	O
we	O
can	O
do	O
is	O
work	O
out	O
our	O
expected	O
winningslosses	O
under	O
the	O
situations	O
of	O
betting	O
and	O
not	O
if	O
we	O
bet	O
we	O
would	O
expect	O
to	O
gain	O
ubet	O
pwin	O
uwin	O
bet	O
plose	O
ulose	O
bet	O
if	O
we	O
don	O
t	O
bet	O
the	O
expected	O
gain	O
is	O
zero	O
uno	O
bet	O
based	O
on	O
taking	O
the	O
decision	O
which	O
maximises	O
expected	O
utility	B
we	O
would	O
therefore	O
be	O
advised	O
not	O
to	O
bet	O
definition	O
expected	O
utility	B
the	O
utility	B
of	O
a	O
decision	O
is	O
ud	O
where	O
px	O
is	O
the	O
distribution	B
of	O
the	O
outcome	O
x	O
and	O
d	O
represents	O
the	O
decision	O
utility	B
of	O
money	B
you	O
are	O
a	O
wealthy	O
individual	O
with	O
in	O
your	O
bank	O
account	O
you	O
are	O
asked	O
if	O
you	O
would	O
like	O
to	O
participate	O
in	O
a	O
fair	O
coin	O
tossing	O
bet	O
in	O
which	O
if	O
you	O
win	O
your	O
bank	O
account	O
will	O
become	O
however	O
if	O
you	O
lose	O
your	O
bank	O
account	O
will	O
contain	O
only	O
assuming	O
the	O
coin	O
is	O
fair	O
should	O
you	O
take	O
the	O
bet	O
if	O
we	O
take	O
the	O
bet	O
our	O
expected	O
bank	O
balance	O
would	O
be	O
ubet	O
if	O
we	O
don	O
t	O
bet	O
our	O
bank	O
balance	O
will	O
remain	O
at	O
based	O
on	O
expected	O
utility	B
we	O
are	O
therefore	O
advised	O
to	O
take	O
the	O
bet	O
that	O
if	O
one	O
considers	O
instead	O
the	O
amount	O
one	O
will	O
win	O
or	O
lose	O
one	O
may	O
show	O
decision	O
trees	O
that	O
the	O
difference	O
in	O
expected	O
utility	B
between	O
betting	O
and	O
not	O
betting	O
is	O
the	O
same	O
whilst	O
the	O
above	O
is	O
a	O
correct	O
mathematical	O
derivation	O
few	O
people	O
who	O
are	O
millionaires	O
are	O
likely	O
to	O
be	O
willing	O
to	O
risk	B
losing	O
almost	O
everything	O
in	O
order	O
to	O
become	O
a	O
billionaire	O
this	O
means	O
that	O
the	O
subjective	B
utility	B
of	O
money	B
is	O
not	O
simply	O
the	O
quantity	O
of	O
money	B
in	O
order	O
to	O
better	O
reflect	O
the	O
situation	O
the	O
utility	B
of	O
money	B
would	O
need	O
to	O
be	O
a	O
non-linear	B
function	B
of	O
money	B
growing	O
slowly	O
for	O
large	O
quantities	O
of	O
money	B
and	O
decreasing	O
rapidly	O
for	O
small	O
quantities	O
of	O
money	B
decision	O
trees	O
decision	O
trees	O
are	O
a	O
way	O
to	O
graphically	O
organise	O
a	O
sequential	B
decision	O
process	O
a	O
decision	B
tree	B
contains	O
decision	O
nodes	O
each	O
with	O
branches	O
for	O
each	O
of	O
the	O
alternative	O
decisions	O
chance	O
nodes	O
variables	O
also	O
appear	O
in	O
the	O
tree	B
with	O
the	O
utility	B
of	O
each	O
branch	O
computed	O
at	O
the	O
leaf	O
of	O
each	O
branch	O
the	O
expected	O
utility	B
of	O
any	O
decision	O
can	O
then	O
be	O
computed	O
on	O
the	O
basis	O
of	O
the	O
weighted	O
summation	O
of	O
all	O
branches	O
from	O
the	O
decision	O
to	O
all	O
leaves	O
from	O
that	O
branch	O
example	O
consider	O
the	O
decision	O
problem	B
as	O
to	O
whether	O
or	O
not	O
to	O
go	O
ahead	O
with	O
a	O
fund-raising	O
garden	O
party	O
if	O
we	O
go	O
ahead	O
with	O
the	O
party	O
and	O
it	O
subsequently	O
rains	O
then	O
we	O
will	O
lose	O
money	B
very	O
few	O
people	O
will	O
show	O
up	O
on	O
the	O
other	O
hand	O
if	O
we	O
don	O
t	O
go	O
ahead	O
with	O
the	O
party	O
and	O
it	O
doesn	O
t	O
rain	O
we	O
re	O
free	O
to	O
go	O
and	O
do	O
something	O
else	O
fun	O
to	O
characterise	O
this	O
numerically	O
we	O
use	O
prain	O
rain	O
prain	O
no	O
rain	O
the	O
utility	B
is	O
defined	O
as	O
u	O
rain	O
u	O
no	O
rain	O
u	O
party	O
rain	O
u	O
party	O
no	O
rain	O
we	O
represent	O
this	O
situation	O
in	O
the	O
question	O
is	O
should	O
we	O
go	O
ahead	O
with	O
the	O
party	O
since	O
we	O
don	O
t	O
know	O
what	O
will	O
actually	O
happen	O
to	O
the	O
weather	O
we	O
compute	O
the	O
expected	O
utility	B
of	O
each	O
decision	O
uparty	O
rainprain	O
uno	O
party	O
rainprain	O
u	O
u	O
party	O
rain	O
rain	O
rain	O
max	O
p	O
arty	O
based	O
on	O
expected	O
utility	B
we	O
are	O
therefore	O
advised	O
to	O
go	O
ahead	O
with	O
the	O
party	O
the	O
maximal	O
expected	O
utility	B
is	O
given	O
by	O
demodecparty	O
m	O
prainup	O
arty	O
rain	O
example	O
an	O
extension	O
of	O
the	O
party	O
problem	B
is	O
that	O
if	O
we	O
decide	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
have	O
the	O
opportunity	O
to	O
visit	O
a	O
friend	O
however	O
we	O
re	O
not	O
sure	O
if	O
this	O
friend	O
will	O
be	O
in	O
the	O
question	O
is	O
should	O
we	O
still	O
go	O
ahead	O
with	O
the	O
party	O
we	O
need	O
to	O
quantify	O
all	O
the	O
uncertainties	O
and	O
utilities	O
if	O
we	O
go	O
ahead	O
with	O
the	O
party	O
the	O
utilities	O
are	O
as	O
before	O
uparty	O
rain	O
uparty	O
no	O
rain	O
draft	O
march	O
decision	O
trees	O
y	O
e	O
s	O
rain	O
yes	O
p	O
arty	O
n	O
o	O
y	O
e	O
s	O
rain	O
with	O
figure	O
a	O
decision	B
tree	B
containing	O
chance	O
nodes	O
with	O
ovals	O
decision	O
nodes	O
with	O
squares	O
and	O
utility	B
nodes	O
with	O
diamonds	O
note	O
that	O
a	O
decision	B
tree	B
is	O
not	O
a	O
graphical	O
representation	O
of	O
a	O
belief	B
network	I
with	O
additional	O
nodes	O
rather	O
a	O
decision	B
tree	B
is	O
an	O
explicit	O
enumeration	O
of	O
the	O
possible	O
choices	O
that	O
can	O
be	O
made	O
beginning	O
with	O
the	O
leftmost	O
decision	O
node	O
with	O
probabilities	O
on	O
the	O
links	O
out	O
of	O
chance	O
nodes	O
prain	O
rain	O
prain	O
no	O
rain	O
if	O
we	O
decide	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
will	O
consider	O
going	O
to	O
visit	O
a	O
friend	O
in	O
making	O
the	O
decision	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
have	O
utilities	O
uparty	O
party	O
rain	O
uparty	O
party	O
no	O
rain	O
the	O
probability	O
that	O
the	O
friend	O
is	O
in	O
depends	O
on	O
the	O
weather	O
according	O
to	O
pf	O
riend	O
inrain	O
pf	O
riend	O
inno	O
rain	O
the	O
other	O
probabilities	O
are	O
determined	O
by	O
normalisation	B
we	O
additionally	O
have	O
uvisit	O
in	O
visit	O
uvisit	O
out	O
visit	O
with	O
the	O
remaining	O
utilities	O
zero	O
the	O
two	O
sets	O
of	O
utilities	O
add	O
up	O
so	O
that	O
the	O
overall	O
utility	B
of	O
any	O
decision	O
sequence	O
is	O
uparty	O
uvisit	O
the	O
decision	B
tree	B
for	O
the	O
party-friend	O
problem	B
is	O
shown	O
is	O
for	O
each	O
decision	O
sequence	O
the	O
utility	B
of	O
that	O
sequence	O
is	O
given	O
at	O
the	O
corresponding	O
leaf	O
of	O
the	O
dt	O
note	O
that	O
the	O
leaves	O
contain	O
the	O
total	O
utility	B
uparty	O
uvisit	O
solving	B
the	O
dt	O
corresponds	O
to	O
finding	O
for	O
each	O
decision	O
node	O
the	O
maximal	O
expected	O
utility	B
possible	O
optimising	O
over	O
future	O
decisions	O
at	O
any	O
point	O
in	O
the	O
tree	B
choosing	O
that	O
action	O
which	O
leads	O
to	O
the	O
child	O
with	O
highest	O
expected	O
utility	B
will	O
lead	O
to	O
the	O
optimal	O
strategy	O
using	O
this	O
we	O
find	O
that	O
the	O
optimal	O
expected	O
utility	B
has	O
value	B
and	O
is	O
given	O
by	O
going	O
ahead	O
with	O
the	O
party	O
see	O
demodecpartyfriend	O
m	O
in	O
dts	O
the	O
same	O
nodes	O
are	O
often	O
repeated	O
throughout	O
the	O
tree	B
for	O
a	O
longer	O
sequence	O
of	O
decisions	O
the	O
number	O
of	O
branches	O
in	O
the	O
tree	B
can	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
decisions	O
making	O
this	O
representation	O
impractical	O
in	O
this	O
example	O
the	O
dt	O
is	O
asymmetric	O
since	O
if	O
we	O
decide	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
will	O
not	O
visit	O
the	O
friend	O
curtailing	O
the	O
further	O
decisions	O
present	O
in	O
the	O
lower	O
half	O
of	O
the	O
tree	B
rain	O
f	O
riend	O
mathematically	O
we	O
can	O
express	O
the	O
optimal	O
expected	O
utility	B
u	O
for	O
the	O
party-visit	O
example	O
by	O
summing	O
over	O
un-revealed	O
variables	O
and	O
optimising	O
over	O
future	O
decisions	O
max	O
p	O
arty	O
prain	O
max	O
v	O
isit	O
pf	O
riendrain	O
arty	O
rain	O
uvisitv	O
isit	O
f	O
riendi	O
arty	O
no	O
where	O
the	O
term	O
i	O
arty	O
no	O
has	O
the	O
effect	O
of	O
curtailing	O
the	O
dt	O
if	O
the	O
party	O
goes	O
ahead	O
to	O
answer	O
the	O
question	O
as	O
to	O
whether	O
or	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
take	O
that	O
state	O
of	O
p	O
arty	O
that	O
corresponds	O
to	O
draft	O
march	O
s	O
e	O
y	O
p	O
arty	O
s	O
e	O
y	O
rain	O
n	O
o	O
yes	O
v	O
isit	O
n	O
o	O
s	O
e	O
y	O
rain	O
f	O
riend	O
o	O
u	O
t	O
n	O
o	O
f	O
riend	O
o	O
u	O
t	O
n	O
o	O
f	O
riend	O
o	O
u	O
t	O
yes	O
v	O
isit	O
n	O
o	O
f	O
riend	O
o	O
u	O
t	O
s	O
e	O
y	O
s	O
e	O
y	O
n	O
o	O
in	O
p	O
f	O
o	O
u	O
t	O
yes	O
v	O
n	O
o	O
n	O
o	O
s	O
e	O
y	O
in	O
f	O
o	O
u	O
t	O
in	O
n	O
o	O
f	O
riend	O
o	O
u	O
t	O
yes	O
v	O
n	O
o	O
in	O
f	O
o	O
u	O
t	O
decision	O
trees	O
figure	O
solving	B
a	O
decision	B
tree	B
decision	B
tree	B
for	O
the	O
party-visit	O
solving	B
the	O
dt	O
responds	O
to	O
making	O
the	O
decision	O
with	O
the	O
highest	O
expected	O
future	O
utility	B
this	O
can	O
be	O
achieved	O
by	O
starting	O
at	O
the	O
leaves	O
for	O
a	O
chance	O
parent	O
node	O
x	O
the	O
utility	B
of	O
the	O
parent	O
is	O
the	O
expected	O
utility	B
of	O
that	O
variable	O
for	O
example	O
at	O
the	O
top	O
of	O
the	O
dt	O
we	O
have	O
the	O
rain	O
variable	O
with	O
the	O
children	B
and	O
hence	O
the	O
expected	O
utility	B
of	O
the	O
rain	O
node	O
is	O
for	O
a	O
decision	O
node	O
the	O
value	B
of	O
the	O
node	O
is	O
the	O
optimum	O
of	O
its	O
child	O
values	O
one	O
recurses	O
thus	O
backwards	O
from	O
the	O
leaves	O
to	O
the	O
root	O
for	O
example	O
the	O
value	B
of	O
the	O
rain	O
chance	O
node	O
in	O
the	O
lower	O
branch	O
is	O
given	O
by	O
the	O
optimal	O
decision	O
sequence	O
is	O
then	O
given	O
at	O
each	O
decision	O
node	O
by	O
finding	O
which	O
child	O
node	O
has	O
the	O
maximal	O
value	B
hence	O
the	O
overall	O
best	O
decision	O
is	O
to	O
decide	O
to	O
go	O
ahead	O
with	O
the	O
party	O
if	O
we	O
decided	O
not	O
to	O
do	O
so	O
and	O
it	O
does	O
not	O
rain	O
then	O
the	O
best	O
decision	O
we	O
could	O
take	O
would	O
be	O
to	O
not	O
visit	O
the	O
friend	O
has	O
an	O
expected	O
utility	B
of	O
a	O
more	O
compact	O
description	O
of	O
this	O
problem	B
is	O
given	O
by	O
the	O
influence	O
diagram	O
see	O
also	O
demodecpartyfriend	O
m	O
the	O
maximal	O
expected	O
utility	B
above	O
the	O
way	O
to	O
read	O
equation	B
is	O
to	O
start	O
from	O
the	O
last	O
decision	O
that	O
needs	O
to	O
be	O
taken	O
in	O
this	O
case	O
v	O
isit	O
when	O
we	O
are	O
at	O
the	O
v	O
isit	O
stage	O
we	O
assume	O
that	O
we	O
will	O
have	O
previously	O
made	O
a	O
decision	O
about	O
p	O
arty	O
and	O
also	O
will	O
have	O
observed	O
whether	O
or	O
not	O
is	O
is	O
raining	O
however	O
we	O
don	O
t	O
know	O
whether	O
or	O
not	O
our	O
friend	O
will	O
be	O
in	O
so	O
we	O
compute	O
the	O
expected	O
utility	B
by	O
averaging	O
over	O
this	O
unknown	O
we	O
then	O
take	O
the	O
optimal	O
decision	O
by	O
maximising	O
over	O
v	O
isit	O
subsequently	O
we	O
move	O
to	O
the	O
next-to-last	O
decision	O
assuming	O
that	O
what	O
we	O
will	O
do	O
in	O
the	O
future	O
is	O
optimal	O
since	O
in	O
the	O
future	O
we	O
will	O
have	O
taken	O
a	O
decision	O
under	O
the	O
uncertain	B
f	O
riend	O
variable	O
the	O
current	O
decision	O
can	O
then	O
be	O
taken	O
under	O
uncertainty	B
about	O
rain	O
and	O
maximising	O
this	O
expected	O
optimal	O
utility	B
over	O
p	O
arty	O
note	O
that	O
the	O
sequence	O
of	O
maximisations	O
and	O
summations	O
matters	O
changing	O
the	O
order	O
will	O
in	O
general	O
result	O
in	O
a	O
different	O
problem	B
with	O
a	O
different	O
expected	O
one	O
only	O
had	O
a	O
sequence	O
of	O
summations	O
the	O
order	O
of	O
the	O
summations	O
is	O
irrelevant	O
likewise	O
for	O
the	O
case	O
of	O
all	O
maximisations	O
however	O
summation	O
and	O
maximisation	B
operators	O
do	O
not	O
in	O
general	O
commute	B
draft	O
march	O
extending	O
bayesian	B
networks	O
for	O
decisions	O
p	O
arty	O
rain	O
u	O
tility	O
figure	O
an	O
influence	O
diagram	O
which	O
contains	O
random	O
variables	O
with	O
ovalscircles	O
decision	O
nodes	O
with	O
squares	O
and	O
utility	B
nodes	O
with	O
diamonds	O
contrasted	O
with	O
this	O
is	O
a	O
more	O
compact	O
representation	O
of	O
the	O
structure	B
of	O
the	O
problem	B
the	O
diagram	O
represents	O
the	O
expression	O
prainuparty	O
rain	O
in	O
addition	O
the	O
diagram	O
denotes	O
an	O
ordering	O
of	O
the	O
variables	O
with	O
party	O
rain	O
to	O
the	O
convention	O
given	O
by	O
equation	B
extending	O
bayesian	B
networks	O
for	O
decisions	O
an	O
influence	O
diagram	O
is	O
a	O
bayesian	B
network	O
with	O
additional	O
decision	O
nodes	O
and	O
utility	B
nodes	O
the	O
decision	O
nodes	O
have	O
no	O
associated	O
distribution	B
the	O
utility	B
nodes	O
are	O
deterministic	B
functions	O
of	O
their	O
parents	B
the	O
utility	B
and	O
decision	O
nodes	O
can	O
be	O
either	O
continuous	B
or	O
discrete	B
for	O
simplicity	O
in	O
the	O
examples	O
here	O
the	O
decisions	O
will	O
be	O
discrete	B
a	O
benefit	O
of	O
decision	O
trees	O
is	O
that	O
they	O
are	O
general	O
and	O
explicitly	O
encode	O
the	O
utilities	O
and	O
probabilities	O
associated	O
with	O
each	O
decision	O
and	O
event	O
in	O
addition	O
we	O
can	O
readily	O
solve	O
small	O
decision	O
problems	O
using	O
decision	O
trees	O
however	O
when	O
the	O
sequence	O
of	O
decisions	O
increases	O
the	O
number	O
of	O
leaves	O
in	O
the	O
decision	B
tree	B
grows	O
and	O
representing	O
the	O
tree	B
can	O
become	O
an	O
exponentially	O
complex	O
problem	B
in	O
such	O
cases	O
it	O
can	O
be	O
useful	O
to	O
use	O
an	O
influence	O
diagram	O
an	O
id	O
states	O
which	O
information	O
is	O
required	O
in	O
order	O
to	O
make	O
each	O
decision	O
and	O
the	O
order	O
in	O
which	O
these	O
decisions	O
are	O
to	O
be	O
made	O
the	O
details	O
of	O
the	O
probabilities	O
and	O
rewards	O
are	O
not	O
specified	O
in	O
the	O
id	O
and	O
this	O
can	O
enable	O
a	O
more	O
compact	O
description	O
of	O
the	O
decision	O
problem	B
syntax	O
of	O
influence	O
diagrams	O
information	O
links	O
an	O
information	B
link	I
from	O
a	O
random	O
variable	O
into	O
a	O
decision	O
node	O
x	O
d	O
indicates	O
that	O
the	O
state	O
of	O
the	O
variable	O
x	O
will	O
be	O
known	O
before	O
decision	O
d	O
is	O
taken	O
information	O
links	O
from	O
another	O
decision	O
node	O
d	O
in	O
to	O
d	O
similarly	O
indicate	O
that	O
decision	O
d	O
is	O
known	O
before	O
decision	O
d	O
is	O
taken	O
we	O
use	O
a	O
dashed	O
link	O
to	O
denote	O
that	O
decision	O
d	O
is	O
not	O
functionally	O
related	O
to	O
its	O
parents	B
random	O
variables	O
random	O
variables	O
may	O
depend	O
on	O
the	O
states	O
of	O
parental	O
random	O
variables	O
in	O
belief	B
networks	I
but	O
also	O
decision	O
node	O
states	O
d	O
x	O
y	O
as	O
decisions	O
are	O
taken	O
the	O
states	O
of	O
some	O
random	O
variables	O
will	O
be	O
revealed	O
to	O
emphasise	O
this	O
we	O
typically	O
shade	O
a	O
node	O
to	O
denote	O
that	O
its	O
state	O
will	O
be	O
revealed	O
during	O
the	O
sequential	B
decision	O
process	O
utilities	O
a	O
utility	B
node	O
is	O
a	O
deterministic	B
function	B
of	O
its	O
parents	B
the	O
parents	B
can	O
be	O
either	O
random	O
variables	O
or	O
decision	O
nodes	O
in	O
the	O
party	O
example	O
the	O
bn	O
trivially	O
consists	O
of	O
a	O
single	O
node	O
and	O
the	O
influence	O
diagram	O
is	O
given	O
in	O
the	O
more	O
complex	O
party-friend	O
problem	B
is	O
depicted	O
in	O
the	O
id	O
generally	O
provides	O
a	O
more	O
compact	O
representation	O
of	O
the	O
structure	B
of	O
problem	B
than	O
a	O
dt	O
although	O
details	O
about	O
the	O
specific	O
probabilities	O
and	O
utilities	O
are	O
not	O
present	O
in	O
the	O
id	O
draft	O
march	O
extending	O
bayesian	B
networks	O
for	O
decisions	O
p	O
arty	O
v	O
isit	O
rain	O
f	O
riend	O
uparty	O
uvisit	O
figure	O
an	O
influence	O
diagram	O
for	O
the	O
party-visit	O
problem	B
the	O
partial	O
ordering	O
is	O
p	O
arty	O
rain	O
v	O
isit	O
f	O
riend	O
the	O
dashed-link	O
from	O
party	O
to	O
visit	O
is	O
not	O
strictly	O
necessary	O
but	O
retained	O
in	O
order	O
to	O
satisfy	O
the	O
convention	O
that	O
there	O
is	O
a	O
directed	B
path	B
connecting	O
all	O
decision	O
nodes	O
partial	O
ordering	O
an	O
id	O
defines	O
a	O
partial	O
ordering	O
of	O
the	O
nodes	O
we	O
begin	O
by	O
writing	O
those	O
variables	O
whose	O
states	O
are	O
known	O
variables	O
before	O
the	O
first	O
decision	O
we	O
then	O
find	O
that	O
set	O
of	O
variables	O
whose	O
states	O
are	O
revealed	O
before	O
the	O
second	O
decision	O
subsequently	O
the	O
set	O
of	O
variables	O
xt	O
is	O
revealed	O
before	O
decision	O
the	O
remaining	O
fully-unobserved	O
variables	O
are	O
placed	O
at	O
the	O
end	O
of	O
the	O
ordering	O
xn	O
dn	O
xn	O
with	O
xk	O
being	O
the	O
variables	O
revealed	O
between	O
decision	O
dk	O
and	O
the	O
term	O
partial	O
refers	O
to	O
the	O
fact	O
that	O
there	O
is	O
no	O
order	O
implied	O
amongst	O
the	O
variables	O
within	O
the	O
set	O
xn	O
for	O
notational	O
clarity	O
at	O
points	O
below	O
we	O
will	O
indicate	O
decision	O
variables	O
with	O
to	O
reinforce	O
that	O
we	O
maximise	O
over	O
these	O
variables	O
and	O
sum	O
over	O
the	O
non-starred	O
variables	O
where	O
the	O
sets	O
are	O
empty	O
we	O
omit	O
writing	O
them	O
for	O
example	O
in	O
the	O
ordering	O
is	O
t	O
est	O
oil	O
the	O
optimal	O
first	O
decision	O
is	O
determined	O
by	O
computing	O
seismic	O
drill	O
uj	O
xn	O
xn	O
i	O
i	O
p	O
j	O
j	O
max	O
dn	O
max	O
for	O
each	O
state	O
of	O
the	O
decision	O
given	O
in	O
equation	B
above	O
i	O
denotes	O
the	O
set	O
of	O
indices	O
for	O
the	O
random	O
variables	O
and	O
j	O
the	O
indices	O
for	O
the	O
utility	B
nodes	O
for	O
each	O
state	O
of	O
the	O
conditioning	B
variables	O
the	O
optimal	O
decision	O
is	O
found	O
using	O
argmax	O
remark	O
off	O
the	O
partial	O
ordering	O
sometimes	O
it	O
can	O
be	O
tricky	O
to	O
read	O
the	O
partial	O
ordering	O
from	O
the	O
id	O
a	O
method	O
is	O
to	O
identify	O
the	O
first	O
decision	O
and	O
then	O
any	O
variables	O
that	O
need	O
to	O
be	O
observed	O
to	O
make	O
that	O
decision	O
then	O
identify	O
the	O
next	O
decision	O
and	O
the	O
variables	O
that	O
are	O
revealed	O
after	O
decision	O
is	O
taken	O
and	O
before	O
decision	O
is	O
taken	O
etc	O
this	O
gives	O
the	O
partial	O
ordering	O
place	O
any	O
unrevealed	O
variables	O
at	O
the	O
end	O
of	O
the	O
ordering	O
implicit	O
and	O
explicit	O
information	O
links	O
the	O
information	O
links	O
are	O
a	O
potential	B
source	O
of	O
confusion	O
an	O
information	B
link	I
specifies	O
explicitly	O
which	O
quantities	O
are	O
known	O
before	O
that	O
decision	O
is	O
we	O
also	O
implicitly	O
assume	O
the	O
no	B
forgetting	I
principle	I
that	O
all	O
past	O
decisions	O
and	O
revealed	O
variables	O
are	O
available	O
at	O
the	O
current	O
decision	O
revealed	O
variables	O
are	O
necessarily	O
the	O
parents	B
of	O
all	O
past	O
decision	O
nodes	O
if	O
we	O
were	O
to	O
include	O
all	O
such	O
information	O
links	O
ids	O
would	O
get	O
potentially	O
rather	O
messy	O
in	O
both	O
explicit	O
and	O
implicit	O
information	O
links	O
are	O
demonstrated	O
we	O
call	O
an	O
information	B
link	I
fundamental	O
if	O
its	O
removal	O
would	O
alter	O
the	O
partial	O
ordering	O
authors	O
prefer	O
to	O
write	O
all	O
information	O
links	O
where	O
possible	O
and	O
others	O
prefer	O
to	O
leave	O
them	O
implicit	O
here	O
we	O
largely	O
take	O
the	O
implicit	O
approach	B
for	O
the	O
purposes	O
of	O
computation	O
all	O
that	O
is	O
required	O
is	O
a	O
partial	O
ordering	O
one	O
can	O
therefore	O
view	O
this	O
as	O
basic	O
and	O
the	O
information	O
links	O
as	O
superficial	O
draft	O
march	O
extending	O
bayesian	B
networks	O
for	O
decisions	O
t	O
est	O
oil	O
t	O
est	O
oil	O
seismic	O
drill	O
seismic	O
drill	O
figure	O
the	O
partial	O
ordering	O
is	O
t	O
est	O
oil	O
the	O
explicit	O
information	O
links	O
from	O
t	O
est	O
to	O
seismic	O
and	O
from	O
seismic	O
to	O
drill	O
are	O
both	O
fundamental	O
in	O
the	O
sense	O
that	O
removing	O
either	O
results	O
in	O
a	O
different	O
partial	O
ordering	O
the	O
shaded	O
node	O
emphasises	O
that	O
the	O
state	O
of	O
this	O
variable	O
will	O
be	O
revealed	O
during	O
the	O
sequential	B
decision	O
process	O
conversely	O
the	O
non-shaded	O
node	O
will	O
never	O
be	O
observed	O
based	O
on	O
the	O
id	O
in	O
there	O
is	O
an	O
implicit	O
link	O
from	O
t	O
est	O
to	O
drill	O
since	O
the	O
decision	O
about	O
t	O
est	O
is	O
taken	O
before	O
seismic	O
is	O
revealed	O
seismic	O
drill	O
causal	B
consistency	I
for	O
an	O
influence	O
diagram	O
to	O
be	O
consistent	B
a	O
current	O
decision	O
cannot	O
affect	O
the	O
past	O
this	O
means	O
that	O
any	O
random	O
variable	O
descendants	O
of	O
a	O
decision	O
d	O
in	O
the	O
id	O
must	O
come	O
later	O
in	O
the	O
partial	O
ordering	O
assuming	O
the	O
no-forgetting	O
principle	O
this	O
means	O
that	O
for	O
any	O
valid	O
id	O
there	O
must	O
be	O
a	O
directed	B
path	B
connecting	O
all	O
decisions	O
this	O
can	O
be	O
a	O
useful	O
check	B
on	O
the	O
consistency	O
of	O
an	O
id	O
asymmetry	B
ids	O
are	O
most	O
convenient	O
when	O
the	O
corresponding	O
dt	O
is	O
symmetric	O
however	O
some	O
forms	O
of	O
asymmetry	B
are	O
relatively	O
straightforward	O
to	O
deal	O
with	O
in	O
the	O
id	O
framework	O
for	O
our	O
party-visit	O
example	O
the	O
dt	O
is	O
asymmetric	O
however	O
this	O
is	O
easily	O
dealt	O
with	O
in	O
the	O
id	O
by	O
using	O
a	O
link	O
from	O
p	O
arty	O
to	O
uvisit	O
which	O
removes	O
the	O
contribution	O
from	O
uvisit	O
when	O
the	O
party	O
goes	O
ahead	O
more	O
complex	O
issues	O
arise	O
when	O
the	O
set	O
of	O
variables	O
than	O
can	O
be	O
observed	O
depends	O
on	O
the	O
decision	O
sequence	O
taken	O
in	O
this	O
case	O
the	O
dt	O
is	O
asymmetric	O
in	O
general	O
influence	O
diagrams	O
are	O
not	O
well	O
suited	O
to	O
modelling	B
such	O
asymmetries	O
although	O
some	O
effects	O
can	O
be	O
mediated	O
either	O
by	O
careful	O
use	O
of	O
additional	O
variables	O
or	O
extending	O
the	O
id	O
notation	O
see	O
and	O
for	O
further	O
details	O
of	O
these	O
issues	O
and	O
possible	O
resolutions	O
example	O
i	O
do	O
a	O
phd	O
consider	O
a	O
decision	O
whether	O
or	O
not	O
to	O
do	O
phd	O
as	O
part	O
of	O
our	O
education	O
taking	O
a	O
phd	O
incurs	O
costs	O
uc	O
both	O
in	O
terms	O
of	O
fees	O
but	O
also	O
in	O
terms	O
of	O
lost	O
income	O
however	O
if	O
we	O
have	O
a	O
phd	O
we	O
are	O
more	O
likely	O
to	O
win	O
a	O
nobel	O
prize	O
which	O
would	O
certainly	O
be	O
likely	O
to	O
boost	O
our	O
income	O
subsequently	O
benefitting	O
our	O
finances	O
this	O
setup	O
is	O
depicted	O
in	O
the	O
ordering	O
is	O
empty	O
sets	O
e	O
p	O
and	O
dome	O
phd	O
no	O
phd	O
domi	O
average	B
high	O
domp	O
no	O
prize	O
the	O
probabilities	O
are	O
pwin	O
nobel	O
prizeno	O
phd	O
pwin	O
nobel	O
prizedo	O
phd	O
plowdo	O
phd	O
no	O
prize	O
paveragedo	O
phd	O
no	O
prize	O
phighdo	O
phd	O
no	O
prize	O
plowno	O
phd	O
no	O
prize	O
paverageno	O
phd	O
no	O
prize	O
phighno	O
phd	O
no	O
prize	O
plowdo	O
phd	O
prize	O
phighdo	O
phd	O
prize	O
plowno	O
phd	O
prize	O
phighno	O
phd	O
prize	O
paveragedo	O
phd	O
prize	O
paverageno	O
phd	O
prize	O
draft	O
march	O
s	O
us	O
p	O
i	O
ub	O
e	O
uc	O
e	O
uc	O
p	O
i	O
ub	O
extending	O
bayesian	B
networks	O
for	O
decisions	O
figure	O
education	O
e	O
incurs	O
some	O
cost	O
but	O
also	O
gives	O
a	O
chance	O
to	O
win	O
a	O
prestigious	O
science	O
prize	O
both	O
of	O
these	O
affect	O
our	O
likely	O
incomes	O
with	O
corresponding	O
long	O
term	O
financial	O
benefits	O
the	O
start-up	O
scenario	O
the	O
utilities	O
are	O
uc	O
phd	O
uc	O
phd	O
ub	O
ub	O
ub	O
the	O
expected	O
utility	B
of	O
education	O
is	O
ue	O
ip	O
pie	O
p	O
ubi	O
so	O
that	O
udo	O
phd	O
whilst	O
not	O
taking	O
a	O
phd	O
is	O
uno	O
phd	O
making	O
it	O
on	O
average	B
beneficial	O
to	O
do	O
a	O
phd	O
see	O
demodecphd	O
m	O
example	O
and	O
start-up	O
companies	O
influence	O
diagrams	O
are	O
particularly	O
useful	O
when	O
a	O
sequence	O
of	O
decisions	O
is	O
taken	O
for	O
example	O
in	O
we	O
model	B
a	O
new	O
situation	O
in	O
which	O
someone	O
has	O
first	O
decided	O
whether	O
or	O
not	O
to	O
take	O
a	O
phd	O
ten	O
years	O
later	O
in	O
their	O
career	O
they	O
decide	O
whether	O
or	O
not	O
to	O
make	O
a	O
start-up	O
company	O
this	O
decision	O
is	O
based	O
on	O
whether	O
or	O
not	O
they	O
won	O
the	O
nobel	O
prize	O
the	O
start-up	O
decision	O
is	O
modelled	O
by	O
s	O
with	O
doms	O
fa	O
if	O
we	O
make	O
a	O
start-up	O
this	O
will	O
cost	O
some	O
money	B
in	O
terms	O
of	O
investment	O
however	O
the	O
potential	B
benefit	O
in	O
terms	O
of	O
our	O
income	O
could	O
be	O
high	O
we	O
model	B
this	O
with	O
other	O
required	O
table	O
entries	O
being	O
taken	O
from	O
plowstart	O
up	O
no	O
prize	O
plowno	O
start	O
up	O
no	O
prize	O
paverageno	O
start	O
up	O
no	O
prize	O
phighno	O
start	O
up	O
no	O
prize	O
plowstart	O
up	O
prize	O
plowno	O
start	O
up	O
prize	O
paveragestart	O
up	O
no	O
prize	O
paveragestart	O
up	O
prize	O
paverageno	O
start	O
up	O
prize	O
phighstart	O
up	O
no	O
prize	O
phighstart	O
up	O
prize	O
phighno	O
start	O
up	O
prize	O
and	O
us	O
up	O
us	O
start	O
up	O
our	O
interest	O
is	O
to	O
advise	O
whether	O
or	O
not	O
it	O
is	O
desirable	O
terms	O
of	O
expected	O
utility	B
to	O
take	O
a	O
phd	O
now	O
bearing	O
in	O
mind	O
that	O
later	O
one	O
may	O
or	O
may	O
not	O
win	O
the	O
nobel	O
prize	O
and	O
may	O
or	O
may	O
not	O
make	O
a	O
start-up	O
company	O
the	O
ordering	O
is	O
empty	O
sets	O
e	O
p	O
s	O
i	O
draft	O
march	O
solving	B
influence	O
diagrams	O
figure	O
markov	B
decision	I
process	I
these	O
can	O
be	O
used	O
to	O
model	B
planning	B
problems	O
of	O
the	O
form	O
how	O
do	O
i	O
get	O
to	O
where	O
i	O
want	O
to	O
be	O
incurring	O
the	O
lowest	O
total	O
cost	O
they	O
are	O
readily	O
solvable	O
using	O
a	O
message	B
passing	B
algorithm	B
the	O
expected	O
optimal	O
utility	B
for	O
any	O
state	O
of	O
e	O
is	O
ue	O
max	O
s	O
p	O
i	O
pis	O
p	O
uce	O
ubi	O
where	O
we	O
assume	O
that	O
the	O
optimal	O
decisions	O
are	O
taken	O
in	O
the	O
future	O
computing	O
the	O
above	O
we	O
find	O
udo	O
phd	O
uno	O
phd	O
hence	O
we	O
are	O
better	O
off	O
not	O
doing	O
a	O
phd	O
see	O
demodecphd	O
m	O
solving	B
influence	O
diagrams	O
solving	B
an	O
influence	O
diagram	O
means	O
computing	O
the	O
optimal	O
decision	O
or	O
sequence	O
of	O
decisions	O
here	O
we	O
focus	O
on	O
finding	O
the	O
optimal	O
first	O
decision	O
the	O
direct	O
approach	B
is	O
to	O
take	O
equation	B
and	O
perform	O
the	O
required	O
sequence	O
of	O
summations	O
and	O
maximisations	O
explicitly	O
however	O
we	O
may	O
be	O
able	O
to	O
exploit	O
the	O
structure	B
of	O
the	O
problem	B
to	O
for	O
computational	O
efficiency	O
to	O
develop	O
this	O
we	O
first	O
derive	O
an	O
efficient	O
algorithm	B
for	O
a	O
highly	O
structured	B
id	O
the	O
markov	B
decision	I
process	I
which	O
we	O
will	O
discuss	O
further	O
in	O
efficient	O
inference	B
consider	O
the	O
following	O
function	B
from	O
the	O
id	O
of	O
where	O
the	O
represent	O
conditional	B
probabilities	O
and	O
the	O
u	O
are	O
utilities	O
we	O
write	O
this	O
in	O
terms	O
of	O
potentials	O
since	O
this	O
will	O
facilitate	O
the	O
generalisation	B
to	O
other	O
cases	O
our	O
task	O
is	O
to	O
take	O
the	O
optimal	O
first	O
decision	O
based	O
on	O
the	O
expected	O
optimal	O
utility	B
max	O
max	O
whilst	O
we	O
could	O
carry	O
out	O
the	O
sequence	O
of	O
maximisations	O
and	O
summations	O
naively	O
our	O
interest	O
is	O
to	O
derive	O
a	O
computationally	O
efficient	O
approach	B
let	O
s	O
see	O
how	O
to	O
distribute	O
these	O
operations	O
by	O
hand	O
since	O
only	O
depends	O
on	O
explicitly	O
we	O
can	O
write	O
max	O
max	O
max	O
max	O
max	O
max	O
draft	O
march	O
solving	B
influence	O
diagrams	O
starting	O
with	O
the	O
first	O
line	O
and	O
carrying	O
out	O
the	O
summation	O
over	O
and	O
max	O
over	O
this	O
gives	O
a	O
new	O
function	B
of	O
max	O
in	O
addition	O
we	O
define	O
the	O
message	B
in	O
our	O
particular	O
example	O
will	O
be	O
unity	O
max	O
using	O
this	O
we	O
can	O
write	O
max	O
max	O
now	O
we	O
carry	O
out	O
the	O
sum	O
over	O
and	O
max	O
over	O
for	O
the	O
first	O
row	O
above	O
and	O
define	O
a	O
utility	B
message	B
max	O
and	O
probability	O
max	O
the	O
optimal	O
decision	O
for	O
can	O
be	O
obtained	O
from	O
since	O
the	O
probability	O
message	B
represents	O
information	O
about	O
the	O
distribution	B
passed	O
to	O
via	O
it	O
is	O
more	O
intuitive	O
to	O
write	O
which	O
has	O
the	O
interpretation	O
of	O
the	O
average	B
of	O
a	O
utility	B
with	O
respect	O
to	O
a	O
distribution	B
it	O
is	O
intuitively	O
clear	O
that	O
we	O
can	O
continue	O
along	O
this	O
line	O
for	O
richer	O
structures	O
than	O
chains	O
indeed	O
provided	O
we	O
have	O
formed	O
an	O
appropriate	O
junction	B
tree	B
we	O
can	O
pass	O
potential	B
and	O
utility	B
messages	O
from	O
clique	B
to	O
neighbouring	O
clique	B
as	O
described	O
in	O
the	O
following	O
section	O
using	O
a	O
junction	B
tree	B
in	O
complex	O
ids	O
computational	O
efficiency	O
in	O
carrying	O
out	O
the	O
series	O
of	O
summations	O
and	O
maximisations	O
may	O
be	O
an	O
issue	O
and	O
one	O
therefore	O
seeks	O
to	O
exploit	O
structure	B
in	O
the	O
id	O
it	O
is	O
intuitive	O
that	O
some	O
form	O
of	O
junction	B
tree	B
style	O
algorithm	B
is	O
applicable	O
we	O
can	O
first	O
represent	O
an	O
id	O
using	O
decision	O
potentials	O
which	O
consist	O
of	O
two	O
parts	O
as	O
defined	O
below	O
definition	O
potential	B
a	O
decision	B
potential	B
on	O
a	O
clique	B
c	O
contains	O
two	O
potentials	O
a	O
probability	B
potential	B
c	O
and	O
a	O
utility	B
potential	B
c	O
the	O
joint	B
potentials	O
for	O
the	O
junction	B
tree	B
are	O
defined	O
as	O
c	O
c	O
c	O
c	O
c	O
c	O
with	O
the	O
junction	B
tree	B
representing	O
the	O
term	O
our	O
mdp	O
example	O
all	O
these	O
probability	O
messages	O
are	O
unity	O
draft	O
march	O
solving	B
influence	O
diagrams	O
in	O
this	O
case	O
there	O
are	O
constraints	O
on	O
the	O
triangulation	B
imposed	O
by	O
the	O
partial	O
ordering	O
which	O
restricts	O
the	O
variables	O
elimination	O
sequence	O
this	O
results	O
in	O
a	O
so-called	O
strong	B
junction	B
tree	B
the	O
treatment	O
here	O
is	O
inspired	O
by	O
a	O
related	O
approach	B
which	O
deals	O
with	O
more	O
general	O
chain	B
graphs	O
is	O
given	O
in	O
the	O
sequence	O
of	O
steps	O
required	O
to	O
construct	O
a	O
jt	O
for	O
an	O
id	O
is	O
as	O
follows	O
remove	O
information	O
edges	O
parental	O
links	O
of	O
decision	O
nodes	O
are	O
moralization	O
marry	O
all	O
parents	B
of	O
the	O
remaining	O
nodes	O
remove	O
utility	B
nodes	O
remove	O
the	O
utility	B
nodes	O
and	O
their	O
parental	O
links	O
strong	B
triangulation	B
form	O
a	O
triangulation	B
based	O
on	O
an	O
elimination	O
order	O
which	O
obeys	O
the	O
partial	O
or	O
dering	O
of	O
the	O
variables	O
strong	B
junction	B
tree	B
from	O
the	O
strongly	O
triangulated	B
graph	B
form	O
a	O
junction	B
tree	B
and	O
orient	O
the	O
edges	O
towards	O
the	O
strong	B
root	O
clique	B
that	O
appears	O
last	O
in	O
the	O
elimination	O
sequence	O
the	O
cliques	O
are	O
ordered	O
according	O
to	O
the	O
sequence	O
in	O
which	O
they	O
are	O
eliminated	O
the	O
separator	B
probability	O
cliques	O
are	O
initialised	O
to	O
the	O
identity	O
with	O
the	O
separator	B
utilities	O
initialised	O
to	O
zero	O
the	O
probability	O
cliques	O
are	O
then	O
initialised	O
by	O
placing	O
conditional	B
probability	I
factors	O
into	O
the	O
lowest	O
available	O
clique	B
to	O
the	O
elimination	O
order	O
that	O
can	O
contain	O
them	O
and	O
similarly	O
for	O
the	O
utilities	O
remaining	O
probability	O
cliques	O
are	O
set	O
to	O
the	O
identity	O
and	O
utility	B
cliques	O
to	O
zero	O
example	O
tree	B
an	O
example	O
of	O
a	O
junction	B
tree	B
for	O
an	O
id	O
is	O
given	O
in	O
the	O
moralisation	B
and	O
triangulation	B
links	O
are	O
given	O
in	O
the	O
orientation	O
of	O
the	O
edges	O
follows	O
the	O
partial	O
ordering	O
with	O
the	O
leaf	O
cliques	O
being	O
the	O
first	O
to	O
disappear	O
under	O
the	O
sequence	O
of	O
summations	O
and	O
maximisations	O
a	O
by-product	O
of	O
the	O
above	O
steps	O
is	O
that	O
the	O
cliques	O
describe	O
the	O
fundamental	O
dependencies	O
on	O
previous	O
decisions	O
and	O
observations	O
in	O
for	O
example	O
the	O
information	B
link	I
from	O
f	O
to	O
is	O
not	O
present	O
in	O
the	O
moralised-triangulated	O
graph	B
nor	O
in	O
the	O
associated	O
cliques	O
of	O
this	O
is	O
because	O
once	O
e	O
is	O
revealed	O
the	O
utility	B
is	O
independent	O
of	O
f	O
giving	O
rise	O
to	O
the	O
two-branch	O
structure	B
in	O
nevertheless	O
the	O
information	B
link	I
from	O
f	O
to	O
is	O
fundamental	O
since	O
it	O
specifies	O
that	O
f	O
will	O
be	O
revealed	O
removing	O
this	O
link	O
would	O
therefore	O
change	O
the	O
partial	O
ordering	O
absorption	B
by	O
analogy	O
with	O
the	O
definition	O
of	O
messages	O
in	O
for	O
two	O
neighbouring	O
cliques	O
and	O
where	O
is	O
closer	O
to	O
the	O
strong	B
root	O
of	O
the	O
jt	O
last	O
clique	B
defined	O
through	O
the	O
elimination	O
order	O
we	O
define	O
s	O
s	O
new	O
s	O
in	O
the	O
above	O
s	O
new	O
s	O
c	O
is	O
a	O
generalised	B
marginalisation	B
operation	O
it	O
sums	O
over	O
those	O
elements	O
of	O
clique	B
c	O
which	O
are	O
random	O
variables	O
and	O
maximises	O
over	O
the	O
decision	O
variables	O
in	O
the	O
clique	B
the	O
order	O
of	O
this	O
sequence	O
of	O
sums	O
and	O
maximisations	O
follows	O
the	O
partial	O
ordering	O
defined	O
by	O
absorption	B
is	O
then	O
computed	O
from	O
the	O
leaves	O
inwards	O
to	O
the	O
root	O
of	O
the	O
strong	B
junction	B
tree	B
the	O
optimal	O
setting	O
of	O
a	O
decision	O
can	O
then	O
be	O
computed	O
from	O
the	O
root	O
clique	B
subsequently	O
backtracking	B
may	O
be	O
that	O
for	O
the	O
case	O
in	O
which	O
the	O
domain	B
is	O
dependent	O
on	O
the	O
parental	O
variables	O
such	O
links	O
must	O
remain	O
draft	O
march	O
l	O
a	O
b	O
c	O
d	O
i	O
h	O
j	O
k	O
g	O
a	O
b	O
c	O
d	O
e	O
f	O
b	O
c	O
a	O
b	O
c	O
solving	B
influence	O
diagrams	O
i	O
h	O
l	O
j	O
k	O
g	O
e	O
f	O
b	O
e	O
d	O
c	O
e	O
g	O
g	O
g	O
i	O
i	O
i	O
l	O
b	O
e	O
d	O
b	O
e	O
f	O
d	O
e	O
f	O
f	O
h	O
h	O
h	O
k	O
h	O
k	O
h	O
k	O
j	O
figure	O
influence	O
diagram	O
adapted	O
from	O
causal	B
consistency	I
is	O
satisfied	O
since	O
there	O
is	O
a	O
directed	B
path	B
linking	O
the	O
all	O
decisions	O
in	O
sequence	O
the	O
partial	O
ordering	O
is	O
b	O
f	O
g	O
c	O
d	O
h	O
i	O
j	O
k	O
l	O
moralised	O
and	O
strongly	O
triangulated	B
graph	B
moralisation	B
links	O
are	O
in	O
green	O
strong	B
triangulation	B
links	O
are	O
in	O
red	O
strong	B
junction	B
tree	B
absorption	B
passes	O
information	O
from	O
the	O
leaves	O
of	O
the	O
tree	B
towards	O
the	O
root	O
applied	O
to	O
infer	O
the	O
optimal	O
decision	O
trajectory	O
the	O
optimal	O
decision	O
for	O
d	O
can	O
be	O
obtained	O
by	O
working	O
with	O
the	O
clique	B
containing	O
d	O
which	O
is	O
closest	O
to	O
the	O
strong	B
root	O
and	O
setting	O
any	O
previously	O
taken	O
decisions	O
and	O
revealed	O
observations	O
into	O
their	O
evidential	O
states	O
see	O
demodecasia	O
m	O
for	O
an	O
example	O
example	O
on	O
a	O
chain	B
for	O
the	O
id	O
of	O
the	O
moralisation	B
and	O
triangulation	B
steps	O
are	O
trivial	O
and	O
give	O
the	O
jt	O
where	O
the	O
cliques	O
are	O
indexed	O
according	O
the	O
elimination	O
order	O
the	O
probability	O
and	O
utility	B
cliques	O
are	O
initialised	O
to	O
draft	O
march	O
updating	O
the	O
separator	B
we	O
have	O
the	O
new	O
probability	B
potential	B
solving	B
influence	O
diagrams	O
with	O
the	O
separator	B
cliques	O
initialised	O
to	O
max	O
and	O
utility	B
potential	B
max	O
max	O
max	O
at	O
the	O
next	O
step	O
we	O
update	O
the	O
probability	B
potential	B
and	O
utility	B
potential	B
max	O
the	O
next	O
separator	B
decision	B
potential	B
is	O
max	O
max	O
max	O
max	O
finally	O
we	O
end	O
up	O
with	O
the	O
root	O
decision	B
potential	B
and	O
max	O
max	O
from	O
the	O
final	O
decision	B
potential	B
we	O
have	O
the	O
expression	O
which	O
is	O
equivalent	B
to	O
that	O
which	O
would	O
be	O
obtained	O
by	O
simply	O
distributing	O
the	O
summations	O
and	O
maximisations	O
over	O
the	O
original	O
id	O
at	O
least	O
for	O
this	O
special	O
case	O
we	O
therefore	O
have	O
verified	O
that	O
the	O
jt	O
approach	B
yields	O
the	O
correct	O
root	O
clique	B
potentials	O
draft	O
march	O
markov	O
decision	O
processes	O
markov	O
decision	O
processes	O
consider	O
a	O
markov	B
chain	B
with	O
transition	O
probabilities	O
jxt	O
i	O
at	O
each	O
time	O
t	O
we	O
consider	O
an	O
action	O
which	O
affects	O
the	O
state	O
at	O
time	O
t	O
we	O
describe	O
this	O
by	O
ixt	O
j	O
dt	O
k	O
associated	O
with	O
each	O
state	O
xt	O
i	O
is	O
a	O
utility	B
uxt	O
i	O
and	O
is	O
schematically	O
depicted	O
in	O
one	O
use	O
of	O
such	O
an	O
environment	O
model	B
would	O
be	O
to	O
help	O
plan	O
a	O
sequence	O
of	O
actions	O
required	O
to	O
reach	O
a	O
goal	O
state	O
in	O
minimal	O
total	O
summed	O
cost	O
more	O
generally	O
one	O
could	O
consider	O
utilities	O
that	O
depend	O
on	O
transitions	O
and	O
decisions	O
i	O
xt	O
j	O
dt	O
k	O
and	O
also	O
time	O
dependent	O
versions	O
of	O
all	O
of	O
these	O
ixt	O
j	O
dt	O
k	O
i	O
xt	O
j	O
dt	O
k	O
we	O
ll	O
stick	O
with	O
the	O
time-independent	O
case	O
here	O
since	O
the	O
generalisations	O
are	O
conceptually	O
straightforward	O
at	O
the	O
expense	O
of	O
notational	O
complexity	O
mdps	O
can	O
be	O
used	O
to	O
solve	O
planning	B
tasks	O
such	O
as	O
how	O
can	O
one	O
get	O
to	O
a	O
desired	O
goal	O
state	O
as	O
quickly	O
as	O
possible	O
by	O
defining	O
the	O
utility	B
of	O
being	O
in	O
the	O
goal	O
state	O
as	O
high	O
and	O
being	O
in	O
the	O
non-goal	O
state	O
as	O
a	O
low	O
value	B
at	O
each	O
time	O
t	O
we	O
have	O
a	O
utility	B
uxt	O
of	O
being	O
in	O
state	O
xt	O
for	O
positive	O
utilities	O
the	O
total	O
utility	B
of	O
any	O
state-decision	O
path	B
is	O
defined	O
as	O
we	O
know	O
the	O
initial	O
state	O
and	O
the	O
probability	O
with	O
which	O
this	O
happens	O
is	O
given	O
by	O
uxt	O
t	O
max	O
max	O
dt	O
max	O
dt	O
xt	O
at	O
time	O
t	O
we	O
want	O
to	O
make	O
that	O
decision	O
that	O
will	O
lead	O
to	O
maximal	O
expected	O
total	O
utility	B
our	O
task	O
is	O
to	O
compute	O
for	O
each	O
state	O
of	O
and	O
then	O
choose	O
that	O
state	O
with	O
maximal	O
expected	O
total	O
utility	B
to	O
carry	O
out	O
the	O
summations	O
and	O
maximisations	O
efficiently	O
we	O
could	O
use	O
the	O
junction	B
tree	B
approach	B
as	O
described	O
in	O
the	O
previous	O
section	O
however	O
in	O
this	O
case	O
the	O
id	O
is	O
sufficiently	O
simple	O
that	O
a	O
direct	O
message	B
passing	B
approach	B
can	O
be	O
used	O
to	O
compute	O
the	O
expected	O
utility	B
maximising	O
expected	O
utility	B
by	O
message	B
passing	B
consider	O
the	O
mdp	O
t	O
dt	O
uxt	O
for	O
the	O
specific	O
example	O
in	O
the	O
joint	B
model	B
of	O
the	O
bn	O
and	O
utility	B
is	O
to	O
decide	O
on	O
how	O
to	O
take	O
the	O
first	O
optimal	O
decision	O
we	O
need	O
to	O
compute	O
max	O
max	O
since	O
only	O
depends	O
on	O
explicitly	O
we	O
can	O
write	O
max	O
max	O
max	O
draft	O
march	O
max	O
we	O
now	O
start	O
with	O
the	O
first	O
line	O
and	O
carry	O
out	O
the	O
summation	O
over	O
and	O
maximisation	B
over	O
this	O
gives	O
a	O
new	O
function	B
of	O
markov	O
decision	O
processes	O
for	O
each	O
line	O
we	O
distribute	O
the	O
operations	O
max	O
max	O
max	O
which	O
we	O
can	O
incorporate	O
in	O
the	O
next	O
line	O
max	O
similarly	O
we	O
can	O
now	O
carry	O
out	O
the	O
sum	O
over	O
and	O
max	O
over	O
to	O
define	O
a	O
new	O
function	B
max	O
to	O
give	O
given	O
above	O
we	O
can	O
then	O
find	O
the	O
optimal	O
decision	O
by	O
bear	O
in	O
mind	O
that	O
when	O
we	O
come	O
to	O
make	O
decision	O
we	O
will	O
have	O
observed	O
and	O
in	O
general	O
the	O
optimal	O
decision	O
is	O
given	O
by	O
argmax	O
d	O
what	O
about	O
d	O
we	O
can	O
then	O
find	O
d	O
by	O
argmax	O
subsequently	O
we	O
can	O
backtrack	O
further	O
to	O
find	O
d	O
t	O
argmax	O
d	O
pxtxt	O
dt	O
ut	O
dt	O
xt	O
bellman	O
s	O
equation	B
xt	O
ut	O
txt	O
max	O
dt	O
pxtxt	O
dt	O
ut	O
it	O
is	O
more	O
common	O
to	O
define	O
the	O
value	B
of	O
being	O
in	O
state	O
xt	O
as	O
vtxt	O
uxt	O
ut	O
vt	O
uxt	O
and	O
write	O
then	O
the	O
equivalent	B
recursion	O
vt	O
uxt	O
max	O
dt	O
draft	O
march	O
pxtxt	O
dt	O
xt	O
in	O
a	O
markov	B
decision	I
process	I
as	O
above	O
we	O
can	O
define	O
utility	B
messages	O
recursively	O
as	O
temporally	B
unbounded	I
mdps	O
the	O
optimal	O
decision	O
d	O
t	O
is	O
then	O
given	O
by	O
t	O
argmax	O
d	O
dt	O
is	O
called	O
bellman	O
s	O
temporally	B
unbounded	I
mdps	O
in	O
the	O
previous	O
discussion	O
about	O
mdps	O
we	O
assumed	O
a	O
given	O
end	O
time	O
t	O
from	O
which	O
one	O
can	O
propagate	O
messages	O
back	O
from	O
the	O
end	O
of	O
the	O
chain	B
the	O
infinite	O
t	O
case	O
would	O
appear	O
to	O
be	O
ill-defined	O
since	O
the	O
sum	O
of	O
utilities	O
uxt	O
will	O
in	O
general	O
be	O
unbounded	O
there	O
is	O
a	O
simple	O
way	O
to	O
avoid	O
this	O
difficulty	O
if	O
we	O
let	O
u	O
maxs	O
us	O
be	O
the	O
largest	O
value	B
of	O
the	O
utility	B
and	O
consider	O
the	O
sum	O
of	O
modified	O
utilities	O
for	O
a	O
chosen	O
discount	B
factor	B
tuxt	O
u	O
t	O
u	O
t	O
where	O
we	O
used	O
the	O
result	O
for	O
a	O
geometric	O
series	O
in	O
the	O
limit	O
t	O
this	O
means	O
that	O
the	O
summed	O
modified	O
utility	B
tuxt	O
is	O
finite	O
the	O
only	O
modification	O
required	O
to	O
our	O
previous	O
discussion	O
is	O
to	O
include	O
a	O
factor	B
in	O
the	O
message	B
definition	O
assuming	O
that	O
we	O
are	O
at	O
convergence	O
we	O
define	O
a	O
value	B
vxt	O
s	O
dependent	O
only	O
on	O
the	O
state	O
s	O
and	O
not	O
the	O
time	O
this	O
means	O
we	O
replace	O
the	O
time-dependent	O
bellman	O
s	O
value	B
recursion	O
equation	B
with	O
the	O
stationary	B
equation	B
vs	O
us	O
max	O
d	O
pxt	O
s	O
dt	O
we	O
then	O
need	O
to	O
solve	O
equation	B
for	O
the	O
value	B
vs	O
for	O
all	O
states	O
s	O
the	O
optimal	O
decision	O
policy	B
when	O
one	O
is	O
in	O
state	O
xt	O
s	O
is	O
then	O
given	O
by	O
d	O
argmax	O
s	O
dt	O
d	O
for	O
a	O
deterministic	B
transition	O
p	O
for	O
each	O
decision	O
d	O
only	O
one	O
state	O
is	O
available	O
this	O
means	O
that	O
the	O
best	O
decision	O
is	O
the	O
one	O
that	O
takes	O
us	O
to	O
the	O
accessible	O
state	O
with	O
highest	O
value	B
seems	O
straightforward	O
to	O
solve	O
however	O
the	O
max	O
operation	O
means	O
that	O
the	O
equations	O
are	O
non-linear	B
in	O
the	O
value	B
v	O
and	O
no	O
closed	O
form	O
solution	O
is	O
available	O
two	O
popular	O
techniques	O
for	O
solving	B
equation	B
are	O
value	B
and	O
policy	B
iteration	B
which	O
we	O
describe	O
below	O
when	O
the	O
number	O
of	O
states	O
s	O
is	O
very	O
large	O
approximate	B
solutions	O
are	O
required	O
sampling	B
and	O
state-dimension	O
reduction	O
techniques	O
are	O
described	O
in	O
value	B
iteration	B
a	O
naive	O
procedure	O
is	O
to	O
iterate	O
equation	B
until	O
convergence	O
assuming	O
some	O
initial	O
guess	O
for	O
the	O
values	O
uniform	B
one	O
can	O
show	O
that	O
this	O
value	B
iteration	B
procedure	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
unique	O
the	O
convergence	O
rate	O
depends	O
somewhat	O
on	O
the	O
discount	O
the	O
smaller	O
is	O
the	O
faster	O
is	O
the	O
convergence	O
an	O
example	O
of	O
value	B
iteration	B
is	O
given	O
in	O
continuous-time	O
analog	O
has	O
a	O
long	O
history	O
in	O
physics	O
and	O
is	O
called	O
the	O
hamilton-jacobi	B
equation	B
and	O
enables	O
one	O
to	O
solve	O
mdps	O
by	O
message	B
passing	B
this	O
being	O
a	O
special	O
case	O
of	O
the	O
more	O
general	O
junction	B
tree	B
approach	B
described	O
earlier	O
in	O
draft	O
march	O
temporally	B
unbounded	I
mdps	O
figure	O
states	O
defined	O
on	O
a	O
two	O
dimensional	O
grid	O
in	O
each	O
square	O
the	O
top	O
left	O
value	B
is	O
the	O
state	O
number	O
and	O
the	O
bottom	O
right	O
is	O
the	O
utility	B
of	O
being	O
in	O
that	O
state	O
an	O
agent	O
can	O
move	O
from	O
a	O
state	O
to	O
a	O
neighbouring	O
state	O
as	O
indicated	O
the	O
task	O
is	O
to	O
solve	O
this	O
problem	B
such	O
that	O
for	O
any	O
position	O
one	O
knows	O
how	O
to	O
move	O
optimally	O
to	O
maximise	O
the	O
expected	O
utility	B
this	O
means	O
that	O
we	O
need	O
to	O
move	O
towards	O
the	O
goal	O
states	O
with	O
nonzero	O
utility	B
see	O
demomdp	O
figure	O
value	B
iteration	B
on	O
a	O
set	O
of	O
states	O
corresponding	O
to	O
a	O
two	O
dimensional	O
grid	O
deterministic	B
transitions	O
are	O
allowed	O
to	O
neighbours	O
on	O
the	O
grid	O
left	O
right	O
up	O
down	O
there	O
are	O
three	O
goal	O
states	O
each	O
with	O
utility	B
all	O
other	O
states	O
have	O
utility	B
plotted	O
is	O
the	O
value	B
vs	O
for	O
after	O
updates	O
of	O
value	B
iteration	B
where	O
the	O
states	O
index	O
a	O
point	O
on	O
the	O
x	O
y	O
grid	O
the	O
optimal	O
decision	O
for	O
any	O
state	O
on	O
the	O
grid	O
is	O
to	O
go	O
to	O
the	O
neighbouring	O
state	O
with	O
highest	O
value	B
see	O
demomdp	O
policy	B
iteration	B
in	O
policy	B
iteration	B
we	O
first	O
assume	O
we	O
know	O
the	O
optimal	O
decision	O
d	O
for	O
any	O
state	O
s	O
we	O
may	O
use	O
this	O
in	O
equation	B
to	O
give	O
vs	O
us	O
pxt	O
s	O
d	O
the	O
maximisation	B
over	O
d	O
has	O
disappeared	O
since	O
we	O
have	O
assumed	O
we	O
already	O
know	O
the	O
optimal	O
decision	O
for	O
each	O
state	O
s	O
for	O
fixed	O
d	O
equation	B
is	O
now	O
linear	B
in	O
the	O
value	B
defining	O
the	O
value	B
v	O
and	O
utility	B
u	O
vectors	O
and	O
transition	B
matrix	B
p	O
us	O
vs	O
ps	O
d	O
in	O
matrix	B
notation	O
equation	B
becomes	O
i	O
i	O
v	O
u	O
ptv	O
v	O
u	O
v	O
u	O
these	O
linear	B
equations	O
are	O
readily	O
solved	O
with	O
gaussian	B
elimination	O
using	O
this	O
the	O
optimal	O
policy	B
is	O
recomputed	O
using	O
equation	B
the	O
two	O
steps	O
of	O
solving	B
for	O
the	O
value	B
and	O
recomputing	O
the	O
policy	B
are	O
iterated	O
until	O
convergence	O
in	O
policy	B
iteration	B
we	O
guess	O
an	O
initial	O
d	O
then	O
solve	O
the	O
linear	B
equations	O
for	O
the	O
value	B
and	O
then	O
recompute	O
the	O
optimal	O
decision	O
see	O
demomdp	O
m	O
for	O
a	O
comparison	O
of	O
value	B
and	O
policy	B
iteration	B
and	O
also	O
an	O
em	B
style	O
approach	B
which	O
we	O
discuss	O
in	O
the	O
next	O
section	O
example	O
grid-world	O
mdp	O
a	O
set	O
of	O
states	O
defined	O
on	O
a	O
grid	O
utilities	O
for	O
being	O
in	O
a	O
grid	O
state	O
is	O
given	O
in	O
for	O
which	O
the	O
agent	O
deterministically	O
moves	O
to	O
a	O
neighbouring	O
grid	O
state	O
at	O
each	O
time	O
step	O
after	O
initialising	O
the	O
value	B
of	O
each	O
grid	O
state	O
to	O
unity	O
the	O
converged	O
value	B
for	O
each	O
state	O
is	O
given	O
in	O
the	O
optimal	O
policy	B
is	O
then	O
given	O
by	O
moving	O
to	O
the	O
neighbouring	O
grid	O
state	O
with	O
highest	O
value	B
draft	O
march	O
probabilistic	B
inference	B
and	O
planning	B
figure	O
a	O
markov	O
decision	O
prob	O
the	O
corresponding	O
probabiliscess	O
tic	O
inference	B
planner	O
a	O
curse	B
of	I
dimensionality	I
consider	O
the	O
following	O
tower	B
of	I
hanoi	I
problem	B
there	O
are	O
pegs	O
a	O
b	O
c	O
d	O
and	O
disks	O
numbered	O
from	O
to	O
you	O
may	O
move	O
a	O
single	O
disk	O
from	O
one	O
peg	O
to	O
another	O
however	O
you	O
are	O
not	O
allowed	O
to	O
put	O
a	O
bigger	O
numbered	O
disk	O
on	O
top	O
of	O
a	O
smaller	O
numbered	O
disk	O
starting	O
with	O
all	O
disks	O
on	O
peg	O
a	O
how	O
can	O
you	O
move	O
them	O
all	O
to	O
peg	O
d	O
in	O
the	O
minimal	O
number	O
of	O
moves	O
this	O
would	O
appear	O
to	O
be	O
a	O
straightforward	O
markov	B
decision	I
process	I
in	O
which	O
the	O
transitions	O
are	O
allowed	O
disk	O
moves	O
if	O
we	O
use	O
x	O
to	O
represent	O
the	O
state	O
of	O
the	O
disks	O
on	O
the	O
pegs	O
this	O
has	O
states	O
are	O
equivalent	B
up	O
to	O
permutation	O
of	O
the	O
pegs	O
which	O
reduces	O
this	O
by	O
a	O
factor	B
of	O
this	O
large	O
number	O
of	O
states	O
renders	O
this	O
naive	O
approach	B
computationally	O
problematic	O
many	O
interesting	O
real-world	O
problems	O
suffer	O
from	O
this	O
large	O
number	O
of	O
states	O
issue	O
so	O
that	O
a	O
naive	O
approach	B
based	O
as	O
we	O
ve	O
described	O
is	O
computationally	O
infeasible	O
finding	O
efficient	O
exact	O
and	O
also	O
approximate	B
state	O
representations	O
is	O
a	O
key	O
aspect	O
to	O
solving	B
large	O
scale	O
mdps	O
see	O
for	O
example	O
probabilistic	B
inference	B
and	O
planning	B
an	O
alternative	O
to	O
the	O
classical	O
mdp	O
solution	O
methods	O
is	O
to	O
make	O
use	O
of	O
the	O
standard	O
methods	O
for	O
training	B
probabilistic	B
models	O
such	O
as	O
the	O
expectation-maximisation	O
algorithm	B
in	O
order	O
to	O
do	O
so	O
we	O
first	O
need	O
to	O
write	O
the	O
problem	B
of	O
maximising	O
expected	O
utility	B
in	O
a	O
form	O
that	O
is	O
suitable	O
to	O
do	O
this	O
we	O
first	O
discuss	O
how	O
a	O
mdp	O
can	O
be	O
expressed	O
as	O
the	O
maximisation	B
of	O
a	O
form	O
of	O
belief	B
network	I
in	O
which	O
the	O
parameters	O
to	O
be	O
found	O
relate	O
to	O
the	O
policy	B
non-stationary	B
markov	B
decision	I
process	I
consider	O
the	O
mdp	O
in	O
in	O
which	O
for	O
simplicity	O
we	O
assume	O
we	O
know	O
the	O
initial	O
state	O
our	O
task	O
is	O
then	O
to	O
find	O
the	O
decisions	O
that	O
maximise	O
the	O
expected	O
utility	B
based	O
on	O
a	O
sequential	B
decision	O
process	O
the	O
first	O
decision	O
is	O
given	O
by	O
maximising	O
the	O
expected	O
utility	B
max	O
more	O
generally	O
this	O
utility	B
can	O
be	O
computed	O
efficiently	O
using	O
a	O
standard	O
message	B
passing	B
routine	O
ut	O
max	O
dt	O
where	O
ut	O
t	O
ut	O
draft	O
march	O
probabilistic	B
inference	B
and	O
planning	B
non-stationary	B
probabilistic	B
inference	B
planner	O
as	O
an	O
alternative	O
to	O
the	O
above	O
mdp	O
description	O
consider	O
the	O
belief	B
network	I
in	O
which	O
we	O
have	O
a	O
utility	B
associated	O
with	O
the	O
last	O
then	O
the	O
expected	O
utility	B
is	O
given	O
by	O
u	O
here	O
the	O
terms	O
pdtxt	O
t	O
are	O
the	O
policy	B
distributions	O
that	O
we	O
wish	O
to	O
learn	O
and	O
t	O
are	O
the	O
parameters	O
of	O
the	O
tth	O
policy	B
distribution	B
let	O
s	O
assume	O
that	O
we	O
have	O
one	O
per	O
time	O
so	O
that	O
t	O
is	O
a	O
function	B
that	O
maps	O
a	O
state	O
x	O
to	O
a	O
probability	O
distribution	B
over	O
decisions	O
our	O
interest	O
is	O
to	O
find	O
the	O
policy	B
distributions	O
that	O
maximise	O
the	O
expected	O
utility	B
since	O
each	O
time-step	O
has	O
its	O
own	O
t	O
and	O
for	O
each	O
state	O
we	O
have	O
a	O
separate	O
unconstrained	O
distribution	B
to	O
optimise	O
over	O
and	O
we	O
can	O
write	O
max	O
u	O
max	O
max	O
this	O
shows	O
that	O
provided	O
there	O
are	O
no	O
constraints	O
on	O
the	O
policy	B
distributions	O
is	O
a	O
separate	O
one	O
for	O
each	O
timepoint	O
we	O
are	O
allowed	O
to	O
distribute	O
the	O
maximisations	O
over	O
the	O
individual	O
policies	O
inside	O
the	O
summation	O
more	O
generally	O
for	O
a	O
finite	O
time	O
t	O
one	O
can	O
define	O
messages	O
to	O
solve	O
for	O
the	O
optimal	O
policy	B
distributions	O
ut	O
max	O
t	O
dt	O
pdtxt	O
with	O
ut	O
t	O
ut	O
deterministic	B
policy	B
for	O
a	O
deterministic	B
policy	B
only	O
a	O
single	O
state	O
is	O
allowed	O
so	O
that	O
where	O
d	O
function	B
for	O
each	O
time	O
t	O
equation	B
reduces	O
to	O
pdtxt	O
t	O
d	O
t	O
t	O
is	O
a	O
policy	B
function	B
that	O
maps	O
a	O
state	O
x	O
to	O
a	O
single	O
decision	O
d	O
since	O
we	O
have	O
a	O
separate	O
policy	B
ut	O
max	O
d	O
t	O
t	O
d	O
which	O
is	O
equivalent	B
to	O
equation	B
this	O
shows	O
that	O
solving	B
the	O
mdp	O
is	O
equivalent	B
to	O
maximising	O
a	O
standard	O
expected	O
utility	B
defined	O
in	O
terms	O
of	O
a	O
belief	B
network	I
under	O
the	O
assumption	O
that	O
each	O
time	O
point	O
has	O
its	O
own	O
policy	B
distribution	B
and	O
that	O
this	O
is	O
deterministic	B
stationary	B
planner	I
if	O
we	O
reconsider	O
our	O
simple	O
example	O
but	O
now	O
constrain	O
the	O
policy	B
distributions	O
to	O
be	O
the	O
same	O
for	O
all	O
time	O
pdtxt	O
t	O
pdtxt	O
more	O
succinctly	O
t	O
then	O
equation	B
becomes	O
u	O
in	O
this	O
case	O
we	O
cannot	O
distribute	O
the	O
maximisation	B
over	O
the	O
policy	B
over	O
the	O
individual	O
terms	O
of	O
the	O
product	O
however	O
computing	O
the	O
expected	O
utility	B
for	O
any	O
given	O
policy	B
is	O
straightforward	O
using	O
message	B
passing	B
one	O
may	O
thus	O
optimise	O
the	O
expected	O
utility	B
using	O
standard	O
numerical	B
optimisation	B
procedures	O
or	O
alternatively	O
an	O
em	B
style	O
approach	B
as	O
we	O
discuss	O
below	O
draft	O
march	O
a	O
variational	O
training	B
approach	B
probabilistic	B
inference	B
and	O
planning	B
without	O
loss	O
of	O
generality	O
we	O
assume	O
that	O
the	O
utility	B
is	O
positive	O
and	O
define	O
a	O
distribution	B
then	O
for	O
any	O
variational	O
distribution	B
using	O
the	O
definition	O
of	O
and	O
the	O
fact	O
that	O
the	O
denominator	O
in	O
equation	B
is	O
equal	O
to	O
u	O
we	O
obtain	O
the	O
bound	B
log	O
u	O
this	O
then	O
gives	O
a	O
two-stage	O
em	B
style	O
procedure	O
m-step	B
isolating	O
the	O
dependencies	O
on	O
for	O
a	O
given	O
variational	O
distribution	B
qold	O
maximising	O
the	O
bound	B
equation	B
is	O
equivalent	B
to	O
maximising	O
e	O
one	O
then	O
finds	O
a	O
policy	B
new	O
which	O
maximises	O
e	O
new	O
argmax	O
e	O
e-step	B
for	O
fixed	O
the	O
best	O
q	O
is	O
given	O
by	O
the	O
update	O
qnew	O
from	O
this	O
joint	B
distribution	B
in	O
order	O
to	O
determine	O
the	O
m-step	B
updates	O
we	O
only	O
require	O
the	O
marginals	O
and	O
both	O
of	O
which	O
are	O
straightforward	O
to	O
obtain	O
since	O
q	O
is	O
simply	O
a	O
first	O
order	O
markov	B
chain	B
in	O
the	O
joint	B
variables	O
xt	O
dt	O
for	O
example	O
one	O
may	O
write	O
the	O
q-distribution	O
as	O
a	O
simple	O
chain	B
factor	B
graph	B
for	O
which	O
marginal	B
inference	B
can	O
be	O
performed	O
readily	O
using	O
the	O
sum-product	B
algorithm	B
this	O
procedure	O
is	O
analogous	O
to	O
the	O
standard	O
em	B
procedure	O
the	O
usual	O
guarantees	O
therefore	O
carry	O
over	O
so	O
that	O
finding	O
a	O
policy	B
that	O
increases	O
e	O
is	O
guaranteed	O
to	O
improve	O
the	O
expected	O
utility	B
in	O
complex	O
situations	O
in	O
which	O
for	O
reasons	O
of	O
storage	O
the	O
optimal	O
q	O
cannot	O
be	O
used	O
a	O
structured	B
constrained	O
variational	O
approximation	B
may	O
be	O
applied	O
in	O
this	O
case	O
as	O
in	O
generalised	B
em	B
only	O
a	O
guaranteed	O
improvement	O
on	O
the	O
lower	O
bound	B
of	O
the	O
expected	O
utility	B
is	O
achieved	O
nevertheless	O
this	O
may	O
be	O
of	O
considerable	O
use	O
in	O
practical	O
situations	O
for	O
which	O
general	O
techniques	O
of	O
approximate	B
inference	B
may	O
be	O
applied	O
the	O
deterministic	B
case	O
for	O
the	O
special	O
case	O
that	O
the	O
policy	B
is	O
deterministic	B
simply	O
maps	O
each	O
state	O
x	O
to	O
single	O
decision	O
d	O
writing	O
this	O
policy	B
map	B
as	O
d	O
equation	B
reduces	O
to	O
ud	O
d	O
d	O
we	O
now	O
define	O
a	O
variational	O
distribution	B
only	O
over	O
d	O
d	O
draft	O
march	O
probabilistic	B
inference	B
and	O
planning	B
and	O
the	O
energy	B
term	O
becomes	O
ed	O
d	O
eds	O
t	O
and	O
d	O
argmax	O
eds	O
ds	O
d	O
for	O
a	O
more	O
general	O
problem	B
in	O
which	O
the	O
utility	B
is	O
at	O
the	O
last	O
time	O
point	O
t	O
and	O
no	O
starting	O
state	O
is	O
given	O
we	O
have	O
a	O
stationary	B
transition	O
dt	O
qxt	O
s	O
log	O
px	O
s	O
ds	O
d	O
this	O
shows	O
how	O
to	O
train	O
a	O
stationary	B
mdp	O
using	O
em	B
in	O
which	O
there	O
is	O
a	O
utility	B
defined	O
only	O
at	O
the	O
last	O
time-point	O
below	O
we	O
generalise	O
this	O
to	O
the	O
case	O
of	O
utilities	O
at	O
each	O
time	O
for	O
both	O
the	O
stationary	B
and	O
non-stationary	B
cases	O
utilities	O
at	O
each	O
timestep	O
consider	O
a	O
generalisation	B
in	O
which	O
we	O
have	O
an	O
additive	O
utility	B
associated	O
with	O
each	O
time-point	O
non-stationary	B
policy	B
to	O
help	O
develop	O
the	O
approach	B
let	O
s	O
look	O
at	O
simply	O
including	O
utilities	O
at	O
times	O
t	O
for	O
the	O
previous	O
example	O
the	O
expected	O
utility	B
is	O
given	O
by	O
u	O
v	O
v	O
defining	O
value	B
messages	O
and	O
u	O
v	O
for	O
a	O
more	O
general	O
case	O
defined	O
over	O
t	O
timesteps	O
we	O
have	O
analogously	O
an	O
expected	O
utility	B
u	O
and	O
our	O
interest	O
is	O
to	O
maximise	O
this	O
expected	O
utility	B
with	O
respect	O
to	O
all	O
the	O
policies	O
max	O
u	O
as	O
before	O
since	O
each	O
timestep	O
has	O
its	O
own	O
policy	B
distribution	B
for	O
each	O
state	O
we	O
may	O
distribute	O
the	O
maximisation	B
using	O
the	O
recursion	O
utxt	O
max	O
t	O
with	O
vtt	O
uxt	O
draft	O
march	O
pdtxt	O
stationary	B
deterministic	B
policy	B
probabilistic	B
inference	B
and	O
planning	B
for	O
an	O
mdp	O
the	O
optimal	O
policy	B
is	O
so	O
that	O
methods	O
which	O
explicitly	O
seek	O
for	O
deterministic	B
policies	O
are	O
of	O
interest	O
for	O
a	O
stationary	B
deterministic	B
policy	B
we	O
have	O
the	O
expected	O
utility	B
utxt	O
xt	O
u	O
px	O
dx	O
with	O
the	O
convention	O
viewed	O
as	O
a	O
factor	B
graph	B
this	O
is	O
simply	O
a	O
chain	B
so	O
that	O
for	O
any	O
policy	B
d	O
the	O
expected	O
utility	B
can	O
be	O
computed	O
easily	O
in	O
principle	O
one	O
could	O
then	O
attempt	O
to	O
optimise	O
u	O
with	O
respect	O
to	O
the	O
decisions	O
directly	O
an	O
alternative	O
is	O
to	O
use	O
an	O
em	B
style	O
to	O
do	O
this	O
we	O
need	O
to	O
define	O
a	O
distribution	B
t	O
utxt	O
zd	O
px	O
dx	O
the	O
normalisation	B
constant	I
zd	O
of	O
this	O
distribution	B
is	O
utxt	O
px	O
dx	O
utxt	O
px	O
dx	O
u	O
if	O
we	O
now	O
define	O
a	O
variational	O
distribution	B
t	O
and	O
consider	O
t	O
t	O
this	O
gives	O
the	O
lower	O
bound	B
log	O
u	O
t	O
log	O
utxt	O
px	O
dx	O
in	O
terms	O
of	O
an	O
em	B
algorithm	B
the	O
m-step	B
requires	O
the	O
dependency	O
on	O
d	O
alone	O
which	O
is	O
ed	O
px	O
dx	O
qx	O
x	O
s	O
t	O
log	O
px	O
s	O
dx	O
d	O
for	O
each	O
given	O
state	O
s	O
we	O
now	O
attempt	O
to	O
find	O
the	O
optimal	O
decision	O
d	O
which	O
corresponds	O
to	O
maximising	O
qx	O
x	O
s	O
t	O
eds	O
defining	O
qx	O
x	O
s	O
t	O
log	O
d	O
we	O
see	O
that	O
for	O
given	O
s	O
up	O
to	O
a	O
constant	O
eds	O
is	O
the	O
kullback-leibler	B
divergence	B
between	O
aligned	O
with	O
d	O
so	O
that	O
the	O
optimal	O
decision	O
d	O
is	O
given	O
by	O
the	O
index	O
of	O
the	O
distribution	B
d	O
argmin	O
and	O
d	O
most	O
closely	O
d	O
draft	O
march	O
further	O
topics	O
figure	O
an	O
example	O
partially	B
observable	I
markov	B
decision	I
process	I
the	O
hidden	B
variables	I
h	O
are	O
never	O
observed	O
in	O
solving	B
the	O
influence	O
diagram	O
we	O
are	O
required	O
to	O
first	O
sum	O
over	O
variables	O
that	O
are	O
never	O
observed	O
doing	O
so	O
will	O
couple	O
together	O
all	O
past	O
observed	O
variables	O
and	O
decisions	O
that	O
means	O
any	O
decision	O
at	O
time	O
t	O
will	O
depend	O
on	O
all	O
previous	O
decisions	O
note	O
that	O
the	O
no-forgetting	O
principle	O
means	O
that	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
write	O
that	O
each	O
decision	O
depends	O
on	O
all	O
previous	O
observations	O
this	O
is	O
implicitly	O
assumed	O
the	O
e-step	B
concerns	O
the	O
computation	O
of	O
the	O
marginal	B
distributions	O
required	O
in	O
the	O
m-step	B
the	O
optimal	O
q	O
distribution	B
is	O
proportional	O
to	O
p	O
evaluated	O
at	O
the	O
previous	O
decision	B
function	B
d	O
t	O
utxt	O
px	O
dx	O
for	O
a	O
constant	O
discount	B
factor	B
at	O
each	O
time-step	O
and	O
an	O
otherwise	O
stationary	B
utxt	O
tuxt	O
using	O
this	O
t	O
tuxt	O
px	O
dx	O
for	O
each	O
t	O
this	O
is	O
a	O
simple	O
markov	B
chain	B
for	O
which	O
the	O
pairwise	B
transition	O
marginals	O
required	O
for	O
the	O
m-step	B
equation	B
are	O
straightforward	O
this	O
requires	O
inference	B
in	O
a	O
series	O
of	O
markov	O
models	O
of	O
different	O
lengths	O
this	O
can	O
be	O
done	O
efficiently	O
using	O
a	O
single	O
forward	O
and	O
backward	O
see	O
mdpemdeterministicpolicy	O
m	O
which	O
also	O
deals	O
with	O
the	O
more	O
general	O
case	O
of	O
utilities	O
dependent	O
on	O
the	O
decisionaction	O
as	O
well	O
as	O
the	O
state	O
note	O
that	O
this	O
em	B
algorithm	B
formally	O
fails	O
in	O
the	O
case	O
of	O
a	O
deterministic	B
environment	O
transition	O
pxtxt	O
dt	O
is	O
deterministic	B
see	O
for	O
an	O
explanation	O
and	O
for	O
a	O
possible	O
resolution	B
further	O
topics	O
partially	B
observable	I
mdps	O
in	O
a	O
pomdp	O
there	O
are	O
states	O
that	O
are	O
not	O
observed	O
this	O
seemingly	O
innocuous	O
extension	O
of	O
the	O
mdp	O
case	O
can	O
lead	O
however	O
to	O
computational	O
difficulties	O
let	O
s	O
consider	O
the	O
situation	O
in	O
and	O
attempt	O
to	O
compute	O
the	O
optimal	O
expected	O
utility	B
based	O
on	O
the	O
sequence	O
of	O
summations	O
and	O
maximisations	O
u	O
max	O
max	O
max	O
the	O
sum	O
over	O
the	O
hidden	B
variables	I
couples	O
all	O
the	O
decisions	O
and	O
observations	O
meaning	O
that	O
we	O
no	O
longer	O
have	O
a	O
simple	O
chain	B
structure	B
for	O
the	O
remaining	O
maximisations	O
for	O
a	O
pomdp	O
of	O
length	O
t	O
this	O
leads	O
to	O
intractable	O
problem	B
with	O
complexity	O
exponential	B
in	O
t	O
an	O
alternative	O
view	O
is	O
to	O
recognise	O
that	O
all	O
past	O
decisions	O
and	O
observations	O
can	O
be	O
summarised	O
in	O
terms	O
of	O
a	O
belief	O
in	O
the	O
current	O
latent	B
state	O
this	O
suggests	O
that	O
instead	O
of	O
having	O
an	O
actual	O
state	O
as	O
in	O
the	O
mdp	O
case	O
we	O
need	O
the	O
standard	O
mdp	O
framework	O
it	O
is	O
more	O
common	O
to	O
define	O
utxt	O
t	O
so	O
that	O
for	O
comparison	O
with	O
the	O
standard	O
policyvalue	O
routines	O
one	O
needs	O
to	O
divide	O
the	O
expected	O
utility	B
by	O
draft	O
march	O
further	O
topics	O
to	O
use	O
a	O
distribution	B
over	O
states	O
to	O
represent	O
our	O
current	O
knowledge	O
one	O
can	O
therefore	O
write	O
down	O
an	O
effective	O
mdp	O
albeit	O
over	O
belief	O
distributions	O
as	O
opposed	O
to	O
finite	O
states	O
approximate	B
techniques	O
are	O
required	O
to	O
solve	O
the	O
resulting	O
infinite	O
state	O
mdps	O
and	O
the	O
reader	O
is	O
referred	O
to	O
more	O
specialised	O
texts	O
for	O
a	O
study	O
of	O
approximation	B
procedures	O
see	O
for	O
example	O
restricted	B
utility	B
functions	O
an	O
alternative	O
to	O
solving	B
mdps	O
is	O
to	O
consider	O
restricted	B
utilities	O
such	O
that	O
the	O
policy	B
can	O
be	O
found	O
easily	O
recently	O
efficient	O
solutions	O
have	O
been	O
developed	O
for	O
classes	O
of	O
mdps	O
with	O
utilities	O
restricted	B
to	O
kullbackleibler	O
divergences	O
reinforcement	B
learning	B
reinforcement	B
learning	B
deals	O
mainly	O
with	O
stationary	B
markov	O
decision	O
processes	O
the	O
added	O
twist	O
is	O
that	O
the	O
transition	O
d	O
possibly	O
the	O
utility	B
is	O
unknown	O
initially	O
an	O
agent	O
begins	O
to	O
explore	O
the	O
set	O
of	O
states	O
and	O
utilities	O
associated	O
with	O
taking	O
decisions	O
the	O
set	O
of	O
accessible	O
states	O
and	O
their	O
rewards	O
populates	O
as	O
the	O
agent	O
traverses	O
its	O
environment	O
consider	O
for	O
example	O
a	O
maze	O
problem	B
with	O
a	O
given	O
start	O
and	O
goal	O
state	O
though	O
with	O
an	O
unknown	O
maze	O
structure	B
the	O
task	O
is	O
to	O
get	O
from	O
the	O
start	O
to	O
the	O
goal	O
in	O
the	O
minimum	O
number	O
of	O
moves	O
on	O
the	O
maze	O
clearly	O
there	O
is	O
a	O
balance	O
required	O
between	O
curiosity	O
and	O
acting	O
to	O
maximise	O
the	O
expected	O
reward	O
if	O
we	O
are	O
too	O
curious	O
t	O
take	O
optimal	O
decisions	O
given	O
the	O
currently	O
available	O
information	O
about	O
the	O
maze	O
structure	B
and	O
continue	O
exploring	O
the	O
possible	O
maze	O
routes	O
this	O
may	O
be	O
bad	O
on	O
the	O
other	O
hand	O
if	O
we	O
don	O
t	O
explore	O
the	O
possible	O
maze	O
states	O
we	O
might	O
never	O
realise	O
that	O
there	O
is	O
a	O
much	O
more	O
optimal	O
short-cut	O
to	O
follow	O
than	O
that	O
based	O
on	O
our	O
current	O
knowledge	O
this	O
exploration-exploitation	O
tradeoff	O
is	O
central	O
to	O
the	O
difficulties	O
of	O
rl	O
see	O
for	O
an	O
extensive	O
discussion	O
of	O
reinforcement	B
learning	B
for	O
a	O
given	O
set	O
of	O
environment	O
data	O
x	O
transitions	O
and	O
utilities	O
one	O
aspect	O
of	O
rl	O
problem	B
can	O
be	O
considered	O
as	O
finding	O
the	O
policy	B
that	O
maximises	O
expected	O
reward	O
given	O
only	O
a	O
prior	B
belief	O
about	O
the	O
environment	O
and	O
observed	O
decisions	O
and	O
states	O
if	O
we	O
assume	O
we	O
know	O
the	O
utility	B
function	B
but	O
not	O
the	O
transition	O
we	O
may	O
write	O
u	O
where	O
represents	O
the	O
environment	O
state	O
transition	O
dt	O
given	O
a	O
set	O
of	O
observed	O
states	O
and	O
decisions	O
p	O
px	O
where	O
p	O
is	O
a	O
prior	B
on	O
the	O
transition	O
similar	O
techniques	O
to	O
the	O
em	B
style	O
training	B
can	O
be	O
carried	O
through	O
in	O
this	O
case	O
as	O
rather	O
than	O
the	O
policy	B
being	O
a	O
function	B
of	O
the	O
state	O
and	O
the	O
environment	O
optimally	O
one	O
needs	O
to	O
consider	O
a	O
policy	B
pdtxt	O
b	O
as	O
a	O
function	B
of	O
the	O
state	O
and	O
the	O
belief	O
in	O
the	O
environment	O
this	O
means	O
that	O
for	O
example	O
if	O
the	O
belief	O
in	O
the	O
environment	O
has	O
high	O
entropy	B
the	O
agent	O
can	O
recognise	O
this	O
and	O
explicitly	O
carry	O
out	O
decisionsactions	O
to	O
explore	O
the	O
environment	O
a	O
further	O
complication	O
in	O
rl	O
is	O
that	O
the	O
data	O
collected	O
x	O
depends	O
on	O
the	O
policy	B
if	O
we	O
write	O
t	O
for	O
an	O
episode	O
in	O
which	O
policy	B
t	O
is	O
followed	O
and	O
data	O
xt	O
collected	O
then	O
the	O
utility	B
of	O
the	O
policy	B
given	O
all	O
the	O
historical	O
information	O
is	O
u	O
depending	O
on	O
the	O
priors	O
on	O
the	O
environment	O
and	O
also	O
on	O
how	O
long	O
each	O
episode	O
is	O
we	O
will	O
have	O
different	O
posteriors	O
for	O
the	O
environment	O
parameters	O
if	O
we	O
then	O
set	O
argmax	O
u	O
this	O
affects	O
the	O
data	O
we	O
collect	O
at	O
the	O
next	O
episode	O
in	O
this	O
way	O
the	O
trajectory	O
of	O
policies	O
can	O
be	O
very	O
different	O
depending	O
on	O
these	O
episode	O
lengths	O
and	O
priors	O
draft	O
march	O
code	O
code	O
summax	O
under	O
a	O
partial	B
order	I
maxsumpot	O
m	O
generalised	B
elimination	O
operation	O
according	O
to	O
a	O
partial	O
ordering	O
sumpotid	O
m	O
summax	O
an	O
id	O
with	O
probability	O
and	O
decision	O
potentials	O
demodecparty	O
m	O
demo	O
of	O
summingmaxing	O
an	O
id	O
junction	O
trees	O
for	O
influence	O
diagrams	O
there	O
is	O
no	O
need	O
to	O
specify	O
the	O
information	O
links	O
provided	O
that	O
a	O
partial	O
ordering	O
is	O
given	O
in	O
the	O
code	O
jtreeid	O
m	O
no	O
check	B
is	O
made	O
that	O
the	O
partial	O
ordering	O
is	O
consistent	B
with	O
the	O
influence	O
diagram	O
in	O
this	O
case	O
the	O
first	O
step	O
of	O
the	O
junction	B
tree	B
formulation	O
in	O
is	O
not	O
required	O
also	O
the	O
moralisation	B
and	O
removal	O
of	O
utility	B
nodes	O
is	O
easily	O
dealt	O
with	O
by	O
defining	O
utility	B
potentials	O
and	O
including	O
them	O
in	O
the	O
moralisation	B
process	O
the	O
strong	B
triangulation	B
is	O
found	O
by	O
a	O
simple	O
variable	B
elimination	I
scheme	O
which	O
seeks	O
to	O
eliminate	O
a	O
variable	O
with	O
the	O
least	O
number	O
of	O
neighbours	O
provided	O
that	O
the	O
variable	O
may	O
be	O
eliminated	O
according	O
to	O
the	O
specified	O
partial	O
ordering	O
the	O
junction	B
tree	B
is	O
constructed	O
based	O
only	O
on	O
the	O
elimination	O
clique	B
sequence	O
obtained	O
from	O
the	O
triangulation	B
routine	O
the	O
junction	B
tree	B
is	O
then	O
obtained	O
by	O
connecting	O
a	O
clique	B
ci	O
to	O
the	O
first	O
clique	B
j	O
i	O
that	O
is	O
connected	B
to	O
this	O
clique	B
clique	B
ci	O
is	O
then	O
eliminated	O
from	O
the	O
graph	B
in	O
this	O
manner	O
a	O
junction	B
tree	B
of	O
connected	B
cliques	O
is	O
formed	O
we	O
do	O
not	O
require	O
the	O
separators	O
for	O
the	O
influence	O
diagram	O
absorption	B
since	O
these	O
can	O
be	O
computed	O
and	O
discarded	O
on	O
the	O
fly	O
note	O
that	O
the	O
code	O
only	O
computes	O
messages	O
from	O
the	O
leaves	O
to	O
the	O
root	O
of	O
the	O
junction	B
tree	B
which	O
is	O
sufficient	O
for	O
taking	O
decisions	O
at	O
the	O
root	O
if	O
one	O
desires	O
an	O
optimal	O
decision	O
at	O
a	O
non-root	O
one	O
would	O
need	O
to	O
absorb	O
probabilities	O
into	O
a	O
clique	B
which	O
contains	O
the	O
decision	O
required	O
these	O
extra	O
forward	O
probability	O
absorptions	O
are	O
required	O
because	O
information	O
about	O
any	O
unobserved	O
variables	O
can	O
be	O
affected	O
by	O
decisions	O
and	O
observations	O
in	O
the	O
past	O
this	O
extra	O
forward	O
probability	O
schedule	B
is	O
not	O
given	O
in	O
the	O
code	O
and	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
jtreeid	O
m	O
junction	B
tree	B
for	O
an	O
influence	O
diagram	O
absorptionid	O
m	O
absorption	B
on	O
an	O
influence	O
diagram	O
triangulateporder	O
m	O
triangulation	B
based	O
on	O
a	O
partial	O
ordering	O
demodecphd	O
m	O
demo	O
for	O
utility	B
of	O
doing	O
phd	O
and	O
startup	O
party-friend	O
example	O
the	O
code	O
below	O
implements	O
the	O
party-friend	O
example	O
in	O
the	O
text	O
to	O
deal	O
with	O
the	O
asymmetry	B
the	O
v	O
isit	O
utility	B
is	O
zero	O
if	O
p	O
arty	O
is	O
in	O
state	O
yes	O
demodecpartyfriend	O
m	O
demo	O
for	O
party-friend	O
chest	B
clinic	I
with	B
decisions	I
the	O
table	O
for	O
the	O
chest	B
clinic	I
decision	O
network	O
is	O
taken	O
from	O
see	O
there	O
if	O
an	O
x-ray	O
is	O
taken	O
then	O
information	O
about	O
x	O
is	O
is	O
a	O
slight	O
modification	O
however	O
to	O
the	O
pxe	O
table	O
available	O
however	O
if	O
the	O
decision	O
is	O
not	O
to	O
take	O
an	O
x-ray	O
no	O
information	O
about	O
x	O
is	O
available	O
this	O
is	O
a	O
form	O
of	O
asymmetry	B
a	O
straightforward	O
approach	B
in	O
this	O
case	O
is	O
to	O
make	O
dx	O
a	O
parent	O
of	O
the	O
x	O
variable	O
and	O
draft	O
march	O
a	O
t	O
x	O
dx	O
ux	O
l	O
e	O
s	O
d	O
b	O
dh	O
uh	O
code	O
s	O
smoking	O
x	O
positive	O
x-ray	O
d	O
dyspnea	O
of	O
breath	O
e	O
either	O
tuberculosis	O
or	O
lung	O
cancer	O
t	O
tuberculosis	O
l	O
lung	O
cancer	O
b	O
bronchitis	O
a	O
visited	O
asia	O
dh	O
hospitalise	O
dx	O
take	O
x-ray	O
figure	O
influence	O
diagram	O
for	O
the	O
chest	B
clinic	I
decision	O
example	O
set	O
the	O
distribution	B
of	O
x	O
to	O
be	O
uninformative	O
if	O
dx	O
fa	O
pa	O
tr	O
ps	O
tr	O
pt	O
tra	O
tr	O
pt	O
tra	O
fa	O
pl	O
trs	O
tr	O
pl	O
trs	O
fa	O
pb	O
trs	O
tr	O
pb	O
trs	O
fa	O
px	O
tre	O
tr	O
dx	O
tr	O
px	O
tre	O
fa	O
dx	O
tr	O
px	O
tre	O
fa	O
dx	O
fa	O
px	O
tre	O
tr	O
dx	O
fa	O
pd	O
tre	O
tr	O
b	O
tr	O
pd	O
tre	O
tr	O
b	O
fa	O
pd	O
tre	O
fa	O
b	O
tr	O
pd	O
tre	O
fa	O
b	O
fa	O
the	O
two	O
utilities	O
are	O
designed	O
to	O
reflect	O
the	O
costs	O
and	O
benefits	O
of	O
taking	O
an	O
x-ray	O
and	O
hospitalising	O
a	O
patient	O
l	O
tr	O
t	O
tr	O
dh	O
tr	O
l	O
fa	O
t	O
tr	O
dh	O
tr	O
t	O
fa	O
l	O
tr	O
dh	O
tr	O
t	O
fa	O
l	O
fa	O
dh	O
tr	O
l	O
tr	O
dh	O
fa	O
t	O
tr	O
l	O
fa	O
dh	O
fa	O
t	O
tr	O
dh	O
fa	O
t	O
fa	O
l	O
tr	O
dh	O
fa	O
t	O
fa	O
l	O
fa	O
t	O
tr	O
dx	O
tr	O
t	O
fa	O
dx	O
tr	O
dx	O
fa	O
t	O
tr	O
dx	O
fa	O
t	O
fa	O
we	O
assume	O
that	O
we	O
know	O
whether	O
or	O
not	O
the	O
patient	O
has	O
been	O
to	O
asia	O
before	O
deciding	O
on	O
taking	O
an	O
x-ray	O
the	O
partial	O
ordering	O
is	O
then	O
a	O
dx	O
x	O
dh	O
e	O
l	O
s	O
t	O
the	O
demo	O
demodecasia	O
m	O
produces	O
the	O
results	O
utility	B
table	O
asia	O
yes	O
takexray	O
yes	O
takexray	O
yes	O
asia	O
no	O
asia	O
yes	O
takexray	O
no	O
asia	O
no	O
takexray	O
no	O
which	O
shows	O
that	O
optimally	O
one	O
should	O
take	O
an	O
x-ray	O
only	O
if	O
the	O
patient	O
has	O
been	O
to	O
asia	O
demodecasia	O
m	O
junction	B
tree	B
influence	O
diagram	O
demo	O
draft	O
march	O
exercises	O
markov	O
decision	O
processes	O
in	O
demomdp	O
m	O
we	O
consider	O
a	O
simple	O
two	O
dimensional	O
grid	O
in	O
which	O
an	O
agent	O
can	O
move	O
to	O
a	O
grid	O
square	O
either	O
above	O
below	O
left	O
right	O
of	O
the	O
current	O
square	O
or	O
stay	O
in	O
the	O
current	O
square	O
we	O
defined	O
goal	O
states	O
squares	O
that	O
have	O
high	O
utility	B
with	O
others	O
having	O
zero	O
utility	B
demomdpclean	O
m	O
demo	O
of	O
value	B
and	O
policy	B
iteration	B
for	O
a	O
simple	O
mdp	O
mdpsolve	O
m	O
mdp	O
solver	O
using	O
value	B
or	O
policy	B
iteration	B
mdp	O
solver	O
using	O
em	B
and	O
assuming	O
a	O
deterministic	B
policy	B
the	O
following	O
is	O
not	O
fully	O
documented	O
in	O
the	O
text	O
although	O
the	O
method	O
is	O
reasonably	O
straightforward	O
and	O
follows	O
that	O
described	O
in	O
the	O
inference	B
is	O
carried	O
out	O
using	O
a	O
simple	O
style	O
recursion	O
this	O
could	O
also	O
be	O
implemented	O
using	O
the	O
general	O
factor	B
graph	B
code	O
but	O
was	O
coded	O
explicitly	O
for	O
reasons	O
of	O
speed	O
the	O
code	O
also	O
handles	O
the	O
more	O
general	O
case	O
of	O
utilities	O
as	O
a	O
function	B
of	O
both	O
the	O
state	O
and	O
the	O
action	O
uxt	O
dt	O
mdpemdeterministicpolicy	O
m	O
mdp	O
solver	O
using	O
em	B
and	O
assuming	O
a	O
deterministic	B
policy	B
emqtranmarginal	O
m	O
marginal	B
information	O
required	O
for	O
the	O
transition	O
term	O
of	O
the	O
energy	B
emqutilmarginal	O
m	O
marginal	B
information	O
required	O
for	O
the	O
utility	B
term	O
of	O
the	O
energy	B
emtotalbetamessage	O
m	O
backward	O
information	O
required	O
for	O
inference	B
in	O
the	O
mdp	O
emminimizekl	O
m	O
find	O
the	O
optimal	O
decision	O
emvaluetable	O
m	O
return	O
the	O
expected	O
value	B
of	O
the	O
policy	B
exercises	O
exercise	O
you	O
play	O
a	O
game	O
in	O
which	O
you	O
have	O
a	O
probability	O
p	O
of	O
winning	O
if	O
you	O
win	O
the	O
game	O
you	O
gain	O
an	O
amount	O
s	O
and	O
if	O
you	O
lose	O
the	O
game	O
you	O
lose	O
an	O
amount	O
s	O
show	O
that	O
the	O
expected	O
gain	O
from	O
playing	O
the	O
game	O
is	O
exercise	O
it	O
is	O
suggested	O
that	O
the	O
utility	B
of	O
money	B
is	O
based	O
not	O
on	O
the	O
amount	O
but	O
rather	O
how	O
much	O
we	O
have	O
relative	O
to	O
other	O
peoples	O
assume	O
a	O
distribution	B
pi	O
i	O
of	O
incomes	O
using	O
a	O
histogram	O
with	O
bins	O
each	O
bin	O
representing	O
an	O
income	O
range	O
use	O
a	O
histogram	O
to	O
roughly	O
reflect	O
the	O
distribution	B
of	O
incomes	O
in	O
society	O
namely	O
that	O
most	O
incomes	O
are	O
around	O
the	O
average	B
with	O
few	O
very	O
wealthy	O
and	O
few	O
extremely	O
poor	O
people	O
now	O
define	O
the	O
utility	B
of	O
an	O
income	O
x	O
as	O
the	O
chance	O
that	O
income	O
x	O
will	O
be	O
higher	O
than	O
a	O
randomly	O
chosen	O
income	O
y	O
the	O
distribution	B
you	O
defined	O
and	O
relate	O
this	O
to	O
the	O
cumulative	O
distribution	B
of	O
p	O
write	O
a	O
program	O
to	O
compute	O
this	O
probability	O
and	O
plot	O
the	O
resulting	O
utility	B
as	O
a	O
function	B
of	O
income	O
now	O
repeat	O
the	O
coin	O
tossing	O
bet	O
of	O
so	O
that	O
if	O
one	O
wins	O
the	O
bet	O
one	O
s	O
new	O
income	O
will	O
be	O
placed	O
in	O
the	O
top	O
histogram	O
bin	O
whilst	O
if	O
one	O
loses	O
one	O
s	O
new	O
income	O
is	O
in	O
the	O
lowest	O
bin	O
compare	O
the	O
optimal	O
expect	O
utility	B
decisions	O
under	O
the	O
situations	O
in	O
which	O
one	O
s	O
original	O
income	O
is	O
average	B
and	O
much	O
higher	O
than	O
average	B
exercise	O
derive	O
a	O
partial	O
ordering	O
for	O
the	O
id	O
on	O
the	O
right	O
and	O
explain	O
how	O
this	O
id	O
differs	O
from	O
that	O
of	O
t	O
est	O
oil	O
seismic	O
drill	O
exercise	O
this	O
question	O
follows	O
closely	O
demomdp	O
m	O
and	O
represents	O
a	O
problem	B
in	O
which	O
a	O
pilot	O
wishes	O
to	O
land	O
an	O
airplane	O
the	O
matrix	B
ux	O
y	O
in	O
the	O
file	O
airplane	O
mat	O
contains	O
the	O
utilities	O
of	O
being	O
in	O
position	O
x	O
y	O
and	O
is	O
a	O
very	O
crude	O
model	B
of	O
a	O
runway	O
and	O
taxiing	O
area	O
to	O
tom	O
furmston	O
for	O
coding	O
this	O
draft	O
march	O
exercises	O
the	O
airspace	O
is	O
represented	O
by	O
an	O
grid	O
gy	O
in	O
the	O
notation	O
employed	O
in	O
demomdp	O
m	O
the	O
matrix	B
represents	O
that	O
position	O
is	O
the	O
desired	O
parking	O
bay	O
of	O
the	O
airplane	O
vertical	O
height	O
of	O
the	O
airplane	O
is	O
not	O
taken	O
in	O
to	O
account	O
the	O
positive	O
values	O
in	O
u	O
represent	O
runway	O
and	O
areas	O
where	O
the	O
airplane	O
is	O
allowed	O
zero	O
utilities	O
represent	O
neutral	O
positions	O
the	O
negative	O
values	O
represent	O
unfavourable	O
positions	O
for	O
the	O
airplane	O
by	O
examining	O
the	O
matrix	B
u	O
you	O
will	O
see	O
that	O
the	O
airplane	O
should	O
preferably	O
not	O
veer	O
off	O
the	O
runway	O
and	O
also	O
should	O
avoid	O
two	O
small	O
villages	O
close	O
to	O
the	O
airport	O
at	O
each	O
timestep	O
the	O
plane	O
can	O
perform	O
one	O
of	O
the	O
following	O
actions	O
stay	O
up	O
down	O
left	O
right	O
for	O
stay	O
the	O
airplane	O
stays	O
in	O
the	O
same	O
x	O
y	O
position	O
for	O
up	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
y	O
position	O
for	O
down	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
y	O
position	O
for	O
left	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
y	O
position	O
for	O
right	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
y	O
position	O
a	O
move	O
that	O
takes	O
the	O
airplane	O
out	O
of	O
the	O
airspace	O
is	O
not	O
allowed	O
the	O
airplane	O
begins	O
in	O
at	O
point	O
x	O
y	O
assuming	O
that	O
an	O
action	O
deterministically	O
results	O
in	O
the	O
intended	O
grid	O
move	O
find	O
the	O
optimal	O
xt	O
yt	O
sequence	O
for	O
times	O
t	O
for	O
the	O
position	O
of	O
the	O
aircraft	O
the	O
pilot	O
tells	O
you	O
that	O
there	O
is	O
a	O
fault	O
with	O
the	O
airplane	O
when	O
the	O
pilot	O
instructs	O
the	O
plane	O
to	O
go	O
right	O
with	O
probability	O
it	O
actually	O
goes	O
up	O
this	O
remains	O
in	O
the	O
airspace	O
assuming	O
again	O
that	O
the	O
airplane	O
begins	O
at	O
point	O
x	O
y	O
return	O
the	O
optimal	O
xt	O
yt	O
sequence	O
for	O
times	O
t	O
for	O
the	O
position	O
of	O
the	O
aircraft	O
exercise	O
the	O
influence	O
diagram	O
depicted	O
describes	O
the	O
first	O
stage	O
of	O
a	O
game	O
the	O
decision	O
variable	O
not	O
play	O
indicates	O
the	O
decision	O
to	O
either	O
play	O
the	O
first	O
stage	O
or	O
not	O
if	O
you	O
decide	O
to	O
play	O
there	O
is	O
a	O
cost	O
but	O
no	O
cost	O
otherwise	O
play	O
the	O
variable	O
describes	O
if	O
you	O
win	O
or	O
lose	O
the	O
game	O
lose	O
with	O
probabilities	O
play	O
the	O
utility	B
of	O
winninglosing	O
is	O
no	O
play	O
win	O
lose	O
show	O
that	O
the	O
expected	O
utility	B
gain	O
of	O
playing	O
this	O
game	O
is	O
play	O
if	O
you	O
win	O
the	O
exercise	O
above	O
describes	O
the	O
first	O
stage	O
of	O
a	O
new	O
two-stage	O
game	O
first	O
stage	O
win	O
you	O
have	O
to	O
make	O
a	O
decision	O
as	O
to	O
whether	O
or	O
not	O
play	O
in	O
the	O
second	O
stage	O
not	O
play	O
if	O
you	O
do	O
not	O
win	O
the	O
first	O
stage	O
you	O
cannot	O
enter	O
the	O
second	O
stage	O
if	O
you	O
decide	O
to	O
play	O
the	O
second	O
stage	O
you	O
win	O
with	O
probability	O
win	O
play	O
if	O
you	O
decide	O
not	O
to	O
play	O
the	O
second	O
stage	O
there	O
is	O
no	O
chance	O
to	O
win	O
win	O
not	O
play	O
the	O
cost	O
of	O
playing	O
the	O
second	O
stage	O
is	O
play	O
no	O
play	O
draft	O
march	O
exercises	O
and	O
the	O
utility	B
of	O
winninglosing	O
the	O
second	O
stage	O
is	O
win	O
lose	O
draw	O
an	O
influence	O
diagram	O
that	O
describes	O
this	O
two-stage	O
game	O
a	O
gambler	O
needs	O
to	O
decide	O
if	O
he	O
should	O
even	O
enter	O
the	O
first	O
stage	O
of	O
this	O
two-stage	O
game	O
show	O
that	O
based	O
on	O
taking	O
the	O
optimal	O
future	O
decision	O
the	O
expected	O
utility	B
based	O
on	O
the	O
first	O
decision	O
is	O
play	O
if	O
if	O
exercise	O
you	O
have	O
b	O
in	O
your	O
bank	O
account	O
you	O
are	O
asked	O
if	O
you	O
would	O
like	O
to	O
participate	O
in	O
a	O
bet	O
in	O
which	O
if	O
you	O
win	O
your	O
bank	O
account	O
will	O
become	O
w	O
however	O
if	O
you	O
lose	O
your	O
bank	O
account	O
will	O
contain	O
only	O
l	O
you	O
win	O
the	O
bet	O
with	O
probability	O
pw	O
assuming	O
that	O
the	O
utility	B
is	O
given	O
by	O
the	O
number	O
of	O
pounds	O
in	O
your	O
bank	O
account	O
write	O
down	O
a	O
formula	O
for	O
the	O
expected	O
utility	B
of	O
taking	O
the	O
bet	O
ubet	O
and	O
also	O
the	O
expected	O
utility	B
of	O
not	O
taking	O
the	O
bet	O
uno	O
bet	O
the	O
above	O
situation	O
can	O
be	O
formulated	O
differently	O
if	O
you	O
win	O
the	O
bet	O
you	O
gain	O
b	O
if	O
you	O
lose	O
the	O
bet	O
you	O
lose	O
l	O
compute	O
the	O
expected	O
amount	O
of	O
money	B
you	O
gain	O
if	O
you	O
bet	O
ugainbet	O
and	O
if	O
you	O
don	O
t	O
bet	O
ugainno	O
bet	O
show	O
that	O
ubet	O
uno	O
bet	O
ugainbet	O
ugainno	O
bet	O
exercise	O
consider	O
the	O
party-friend	O
scenario	O
an	O
alternative	O
is	O
to	O
replace	O
the	O
link	O
from	O
p	O
arty	O
to	O
uvisit	O
by	O
an	O
information	B
link	I
from	O
p	O
arty	O
to	O
v	O
isit	O
with	O
the	O
constraint	O
that	O
v	O
isit	O
can	O
be	O
in	O
state	O
yes	O
only	O
if	O
p	O
arty	O
is	O
in	O
state	O
no	O
explain	O
how	O
this	O
constraint	O
can	O
be	O
achieved	O
by	O
including	O
an	O
additional	O
additive	O
term	O
to	O
the	O
utilities	O
and	O
modify	O
demodecpartyfriend	O
m	O
accordingly	O
to	O
demonstrate	O
this	O
for	O
the	O
case	O
in	O
which	O
utilities	O
are	O
all	O
positive	O
explain	O
how	O
the	O
same	O
constraint	O
can	O
be	O
achieved	O
using	O
a	O
multiplicative	O
factor	B
exercise	O
consider	O
an	O
objective	O
f	O
x	O
uxpx	O
for	O
a	O
positive	O
function	B
ux	O
and	O
that	O
our	O
task	O
is	O
to	O
maximise	O
f	O
with	O
respect	O
to	O
an	O
expectationmaximisation	O
style	O
bounding	O
approach	B
can	O
be	O
derived	O
by	O
defining	O
the	O
auxiliary	O
distribution	B
px	O
uxpx	O
f	O
so	O
that	O
by	O
considering	O
klqx	O
px	O
for	O
some	O
variational	O
distribution	B
qx	O
we	O
obtain	O
the	O
bound	B
log	O
f	O
px	O
the	O
m-step	B
states	O
that	O
the	O
optimal	O
q	O
distribution	B
is	O
given	O
by	O
qx	O
px	O
old	O
at	O
the	O
e-step	B
of	O
the	O
algorithm	B
the	O
new	O
parameters	O
new	O
are	O
given	O
by	O
maximising	O
the	O
energy	B
term	O
new	O
argmax	O
px	O
px	O
old	O
show	O
that	O
for	O
a	O
deterministic	B
distribution	B
px	O
f	O
the	O
e-step	B
fails	O
giving	O
new	O
old	O
draft	O
march	O
exercise	O
consider	O
an	O
objective	O
f	O
uxpx	O
x	O
for	O
a	O
positive	O
function	B
ux	O
and	O
px	O
f	O
exercises	O
and	O
an	O
arbitrary	O
distribution	B
nx	O
our	O
task	O
is	O
to	O
maximise	O
f	O
with	O
respect	O
to	O
as	O
the	O
previous	O
exercise	O
showed	O
if	O
we	O
attempt	O
an	O
em	B
algorithm	B
in	O
the	O
limit	O
of	O
a	O
deterministic	B
model	B
then	O
no-updating	O
occurs	O
and	O
the	O
em	B
algorithm	B
fails	O
to	O
find	O
that	O
optimises	O
show	O
that	O
f	O
and	O
hence	O
x	O
nxux	O
f	O
new	O
f	O
old	O
new	O
old	O
show	O
that	O
if	O
for	O
we	O
can	O
find	O
a	O
new	O
such	O
that	O
f	O
new	O
f	O
old	O
then	O
necessarily	O
new	O
old	O
using	O
this	O
result	O
derive	O
an	O
em-style	O
algorithm	B
that	O
guarantees	O
to	O
increase	O
f	O
we	O
are	O
already	O
at	O
the	O
optimum	O
for	O
and	O
therefore	O
guarantees	O
to	O
increase	O
hint	O
use	O
px	O
uxpx	O
f	O
and	O
consider	O
klqx	O
px	O
for	O
some	O
variational	O
distribution	B
qx	O
exercise	O
the	O
file	O
idjensen	O
mat	O
contains	O
probability	O
and	O
utility	B
tables	O
for	O
the	O
influence	O
diagram	O
of	O
using	O
brmltoolbox	O
write	O
a	O
program	O
that	O
returns	O
the	O
maximal	O
expected	O
utility	B
for	O
this	O
id	O
using	O
a	O
strong	B
junction	B
tree	B
approach	B
and	O
check	B
the	O
result	O
by	O
explicit	O
summation	O
and	O
maximisation	B
similarly	O
your	O
program	O
should	O
output	O
the	O
maximal	O
expected	O
utility	B
for	O
both	O
states	O
of	O
and	O
check	B
that	O
the	O
computation	O
using	O
the	O
strong	B
junction	B
tree	B
agrees	O
with	O
the	O
result	O
from	O
explicit	O
elimination	O
summation	O
and	O
maximisation	B
exercise	O
for	O
a	O
pomdp	O
explain	O
the	O
structure	B
of	O
the	O
strong	B
junction	B
tree	B
and	O
relate	O
this	O
to	O
the	O
complexity	O
of	O
inference	B
in	O
the	O
pomdp	O
exercise	O
define	O
a	O
partial	B
order	I
for	O
the	O
id	O
diagram	O
depicted	O
draw	O
a	O
junction	B
tree	B
for	O
this	O
id	O
i	O
b	O
f	O
d	O
g	O
a	O
e	O
c	O
h	O
draft	O
march	O
part	O
ii	O
learning	B
in	O
probabilistic	B
models	O
chapter	O
statistics	O
for	O
machine	O
learning	B
distributions	O
definition	O
distribution	B
function	B
for	O
a	O
univariate	B
distribution	B
px	O
the	O
cdf	O
is	O
defined	O
as	O
cdfy	O
px	O
y	O
for	O
an	O
unbounded	O
domain	B
cdf	O
and	O
cdf	O
summarising	O
distributions	O
definition	O
the	O
mode	B
x	O
of	O
a	O
distribution	B
px	O
is	O
the	O
state	O
of	O
x	O
at	O
which	O
the	O
distribution	B
takes	O
its	O
highest	O
value	B
x	O
argmax	O
px	O
a	O
distribution	B
could	O
have	O
more	O
than	O
one	O
node	O
multi-modal	O
a	O
widespread	O
abuse	O
of	O
terminology	O
is	O
to	O
refer	O
to	O
any	O
isolated	O
local	B
maximum	O
of	O
px	O
to	O
be	O
a	O
mode	B
x	O
definition	O
and	O
expectation	B
denotes	O
the	O
average	B
or	O
expectation	B
of	O
fx	O
with	O
respect	O
to	O
the	O
distribution	B
px	O
a	O
common	O
alternative	O
notation	O
is	O
ex	O
when	O
the	O
context	O
is	O
clear	O
one	O
may	O
drop	O
the	O
notational	O
dependency	O
on	O
px	O
the	O
notation	O
is	O
shorthand	O
for	O
the	O
average	B
of	O
fx	O
conditioned	O
on	O
knowing	O
the	O
state	O
of	O
variable	O
y	O
i	O
e	O
the	O
average	B
of	O
fx	O
with	O
respect	O
to	O
the	O
distribution	B
pxy	O
an	O
advantage	O
of	O
the	O
expectation	B
notations	O
is	O
that	O
they	O
hold	O
whether	O
the	O
distribution	B
is	O
over	O
continuous	B
or	O
discrete	B
variables	O
in	O
the	O
discrete	B
case	O
summarising	O
distributions	O
x	O
fx	O
xpx	O
x	O
and	O
for	O
continuous	B
variables	O
fxpxdx	O
the	O
reader	O
might	O
wonder	O
what	O
means	O
when	O
x	O
is	O
discrete	B
if	O
domx	O
orange	O
pear	O
with	O
associated	O
probabilities	O
px	O
for	O
each	O
of	O
the	O
states	O
what	O
does	O
refer	O
to	O
clearly	O
makes	O
sense	O
if	O
fx	O
x	O
maps	O
the	O
state	O
x	O
to	O
a	O
numerical	B
value	B
for	O
example	O
fx	O
apple	O
fx	O
orange	O
fx	O
pear	O
for	O
which	O
is	O
meaningful	O
unless	O
the	O
states	O
of	O
the	O
discrete	B
variable	O
are	O
associated	O
with	O
a	O
numerical	B
value	B
then	O
has	O
no	O
meaning	O
for	O
example	O
definition	O
the	O
kth	O
moment	O
of	O
a	O
distribution	B
is	O
given	O
by	O
the	O
average	B
of	O
xk	O
under	O
the	O
distribution	B
for	O
k	O
we	O
have	O
the	O
mean	B
typically	O
denoted	O
by	O
definition	O
and	O
correlation	O
px	O
px	O
the	O
square	O
root	O
of	O
the	O
variance	B
is	O
called	O
the	O
standard	B
deviation	I
the	O
notation	O
varx	O
is	O
also	O
used	O
to	O
emphasise	O
for	O
which	O
variable	O
the	O
variance	B
is	O
computed	O
the	O
reader	O
may	O
show	O
that	O
an	O
equivalent	B
expression	O
is	O
for	O
a	O
multivariate	B
distribution	B
the	O
matrix	B
with	O
elements	O
ij	O
i	O
where	O
i	O
is	O
called	O
the	O
covariance	B
matrix	B
the	O
diagonal	O
entries	O
of	O
the	O
covariance	B
matrix	B
contain	O
the	O
variance	B
of	O
each	O
variable	O
an	O
equivalent	B
expression	O
is	O
the	O
correlation	O
matrix	B
has	O
elements	O
ij	O
i	O
i	O
ij	O
j	O
j	O
where	O
i	O
is	O
the	O
deviation	O
of	O
variable	O
xi	O
the	O
correlation	O
is	O
a	O
normalised	O
form	O
of	O
the	O
covariance	B
so	O
that	O
each	O
element	O
is	O
bounded	O
ij	O
draft	O
march	O
summarising	O
distributions	O
for	O
independent	O
variables	O
xi	O
and	O
xj	O
xi	O
xj	O
the	O
covariance	B
ij	O
is	O
zero	O
similarly	O
independent	O
variables	O
have	O
zero	O
correlation	O
they	O
are	O
uncorrelated	O
note	O
however	O
that	O
the	O
converse	O
is	O
not	O
generally	O
true	O
two	O
variables	O
can	O
be	O
uncorrelated	O
but	O
dependent	O
a	O
special	O
case	O
is	O
for	O
when	O
xi	O
and	O
xj	O
are	O
gaussian	B
distributed	O
then	O
independence	B
is	O
equivalent	B
to	O
being	O
uncorrelated	O
see	O
definition	O
and	O
kurtosis	B
the	O
skewness	B
is	O
a	O
measure	O
of	O
the	O
asymmetry	B
of	O
a	O
distribution	B
px	O
px	O
where	O
is	O
the	O
variance	B
of	O
x	O
with	O
respect	O
to	O
px	O
a	O
positive	O
skewness	B
means	O
the	O
distribution	B
has	O
a	O
heavy	O
tail	O
to	O
the	O
right	O
similarly	O
a	O
negative	O
skewness	B
means	O
the	O
distribution	B
has	O
a	O
heavy	O
tail	O
to	O
the	O
left	O
the	O
kurtosis	B
is	O
a	O
measure	O
of	O
how	O
peaked	O
around	O
the	O
mean	B
a	O
distribution	B
is	O
a	O
distribution	B
with	O
positive	O
kurtosis	B
has	O
more	O
mass	O
around	O
its	O
mean	B
than	O
would	O
a	O
gaussian	B
with	O
the	O
same	O
mean	B
and	O
variance	B
these	O
are	O
also	O
called	O
super	B
gaussian	B
similarly	O
a	O
negative	O
kurtosis	B
gaussian	B
distribution	B
has	O
less	O
mass	O
around	O
its	O
mean	B
than	O
the	O
corresponding	O
gaussian	B
the	O
kurtosis	B
is	O
defined	O
such	O
that	O
a	O
gaussian	B
has	O
zero	O
kurtosis	B
accounts	O
for	O
the	O
term	O
in	O
the	O
definition	O
definition	O
distribution	B
for	O
a	O
set	O
of	O
datapoints	O
xn	O
which	O
are	O
states	O
of	O
a	O
random	O
variable	O
x	O
the	O
empirical	B
distribution	B
has	O
probability	O
mass	O
distributed	O
evenly	O
over	O
the	O
datapoints	O
and	O
zero	O
elsewhere	O
for	O
a	O
discrete	B
variable	O
x	O
the	O
empirical	B
distribution	B
is	O
px	O
n	O
i	O
xn	O
where	O
n	O
is	O
the	O
number	O
of	O
datapoints	O
for	O
a	O
continuous	B
distribution	B
we	O
have	O
px	O
n	O
xn	O
where	O
is	O
the	O
dirac	B
delta	I
function	B
the	O
sample	O
mean	B
of	O
the	O
datapoints	O
is	O
given	O
by	O
the	O
and	O
the	O
sample	O
variance	B
is	O
given	O
by	O
the	O
n	O
xn	O
n	O
draft	O
march	O
summarising	O
distributions	O
for	O
vectors	O
the	O
sample	O
mean	B
vector	O
has	O
elements	O
xn	O
i	O
i	O
n	O
and	O
sample	O
covariance	B
matrix	B
has	O
elements	O
i	O
j	O
j	O
ij	O
n	O
definition	O
function	B
for	O
continuous	B
x	O
we	O
define	O
the	O
dirac	B
delta	I
function	B
which	O
is	O
zero	O
everywhere	O
expect	O
at	O
where	O
there	O
is	O
a	O
spike	O
and	O
one	O
can	O
view	O
the	O
dirac	B
delta	I
function	B
as	O
an	O
infinitely	O
narrow	O
gaussian	B
lim	O
n	O
the	O
kronecker	B
delta	I
is	O
similarly	O
zero	O
everywhere	O
except	O
for	O
the	O
kronecker	B
delta	I
is	O
equivalent	B
to	O
i	O
we	O
use	O
the	O
expression	O
to	O
denote	O
either	O
the	O
dirac	O
or	O
kronecker	B
delta	I
depending	O
on	O
the	O
context	O
estimator	O
bias	B
definition	O
estimator	O
given	O
data	O
x	O
xn	O
from	O
a	O
distribution	B
px	O
we	O
can	O
use	O
the	O
data	O
x	O
to	O
estimate	O
the	O
parameter	B
that	O
was	O
used	O
to	O
generate	O
the	O
data	O
the	O
estimator	O
is	O
a	O
function	B
of	O
the	O
data	O
which	O
we	O
write	O
for	O
an	O
unbiased	B
estimator	I
px	O
px	O
more	O
generally	O
one	O
can	O
consider	O
any	O
estimating	O
function	B
of	O
data	O
this	O
is	O
an	O
unbiased	B
estimator	I
of	O
a	O
quantity	O
if	O
figure	O
empirical	B
distribution	B
over	O
a	O
discrete	B
variable	O
with	O
states	O
the	O
empirical	B
samples	O
consist	O
of	O
n	O
samples	O
at	O
each	O
of	O
states	O
and	O
samples	O
at	O
state	O
where	O
n	O
on	O
normalising	O
this	O
gives	O
a	O
distribution	B
with	O
values	O
over	O
the	O
states	O
draft	O
march	O
discrete	B
distributions	O
a	O
classical	O
example	O
for	O
estimator	O
bias	B
are	O
those	O
of	O
the	O
mean	B
and	O
variance	B
let	O
xn	O
n	O
this	O
is	O
an	O
unbiased	B
estimator	I
of	O
the	O
mean	B
since	O
n	O
n	O
on	O
the	O
other	O
hand	O
consider	O
the	O
estimator	O
of	O
the	O
variance	B
n	O
n	O
px	O
n	O
this	O
is	O
biased	O
since	O
a	O
few	O
lines	O
of	O
algebra	O
n	O
n	O
discrete	B
distributions	O
definition	O
distribution	B
the	O
bernoulli	B
distribution	B
concerns	O
a	O
discrete	B
binary	O
variable	O
x	O
with	O
domx	O
the	O
states	O
are	O
not	O
merely	O
symbolic	O
but	O
real	O
values	O
and	O
px	O
from	O
normalisation	B
it	O
follows	O
that	O
px	O
from	O
this	O
px	O
px	O
the	O
variance	B
is	O
given	O
by	O
varx	O
definition	O
distribution	B
the	O
categorical	B
distribution	B
generalises	O
the	O
bernoulli	B
distribution	B
to	O
more	O
than	O
two	O
states	O
for	O
a	O
discrete	B
variable	O
x	O
with	O
symbolic	O
states	O
domx	O
c	O
px	O
c	O
c	O
c	O
the	O
dirichlet	B
is	O
conjugate	B
to	O
the	O
categorical	B
distribution	B
c	O
definition	O
distribution	B
the	O
binomial	B
describes	O
the	O
distribution	B
of	O
a	O
discrete	B
two-state	O
variable	O
x	O
with	O
domx	O
where	O
the	O
states	O
are	O
symbolic	O
the	O
probability	O
that	O
in	O
n	O
bernoulli	B
trials	O
samples	O
k	O
success	O
states	O
will	O
be	O
observed	O
is	O
k	O
k	O
k	O
py	O
k	O
draft	O
march	O
is	O
the	O
binomial	B
coefficient	O
the	O
mean	B
and	O
variance	B
are	O
k	O
n	O
varx	O
n	O
the	O
beta	B
distribution	B
is	O
the	O
conjugate	B
prior	B
for	O
the	O
binomial	B
distribution	B
continuous	B
distributions	O
definition	O
distribution	B
consider	O
a	O
multi-state	O
variable	O
x	O
with	O
domx	O
k	O
with	O
corresponding	O
state	O
probabilities	O
k	O
we	O
then	O
draw	O
n	O
samples	O
from	O
this	O
distribution	B
the	O
probability	O
of	O
observing	O
the	O
state	O
times	O
state	O
times	O
state	O
k	O
yk	O
times	O
in	O
the	O
n	O
samples	O
is	O
yi	O
i	O
n	O
yk	O
py	O
where	O
n	O
yi	O
n	O
i	O
varyi	O
n	O
i	O
i	O
n	O
i	O
j	O
j	O
the	O
dirichlet	B
distribution	B
is	O
the	O
conjugate	B
prior	B
for	O
the	O
multinomial	B
distribution	B
definition	O
distribution	B
the	O
poisson	B
distribution	B
can	O
be	O
used	O
to	O
model	B
situations	O
in	O
which	O
the	O
expected	O
number	O
of	O
events	O
scales	O
with	O
the	O
length	O
of	O
the	O
interval	O
within	O
which	O
the	O
events	O
can	O
occur	O
if	O
is	O
the	O
expected	O
number	O
of	O
events	O
per	O
unit	O
interval	O
then	O
the	O
distribution	B
of	O
the	O
number	O
of	O
events	O
x	O
within	O
an	O
interval	O
t	O
is	O
px	O
k	O
t	O
tk	O
k	O
e	O
k	O
for	O
a	O
unit	O
length	O
interval	O
varx	O
the	O
poisson	B
distribution	B
can	O
be	O
derived	O
as	O
a	O
limiting	O
case	O
of	O
a	O
binomial	B
distribution	B
in	O
which	O
the	O
success	O
probability	O
scales	O
as	O
in	O
the	O
limit	O
n	O
continuous	B
distributions	O
bounded	O
distributions	O
definition	O
distribution	B
for	O
a	O
variable	O
x	O
the	O
distribution	B
is	O
uniform	B
if	O
px	O
const	O
over	O
the	O
domain	B
of	O
the	O
variable	O
definition	O
distribution	B
for	O
x	O
x	O
px	O
e	O
one	O
can	O
show	O
that	O
for	O
rate	O
varx	O
draft	O
march	O
continuous	B
distributions	O
exponential	B
figure	O
laplace	B
distribution	B
exponential	B
distribution	B
the	O
alternative	O
parameterisation	B
b	O
is	O
called	O
the	O
scale	O
definition	O
distribution	B
x	O
gam	O
ta	O
tdt	O
x	O
e	O
x	O
is	O
called	O
the	O
shape	O
parameter	B
is	O
the	O
scale	O
parameter	B
and	O
the	O
parameters	O
are	O
related	O
to	O
the	O
mean	B
and	O
variance	B
through	O
s	O
where	O
is	O
the	O
mean	B
of	O
the	O
distribution	B
and	O
s	O
is	O
the	O
standard	B
deviation	I
the	O
mode	B
is	O
given	O
by	O
for	O
an	O
alternative	O
parameterisation	B
uses	O
the	O
inverse	O
scale	O
gamis	O
b	O
gam	O
x	O
bx	O
definition	O
gamma	B
distribution	B
invgam	O
x	O
e	O
this	O
has	O
mean	B
for	O
and	O
variance	B
for	O
definition	O
distribution	B
px	O
b	O
b	O
x	O
x	O
x	O
where	O
the	O
beta	B
function	B
is	O
defined	O
as	O
b	O
draft	O
march	O
and	O
is	O
the	O
gamma	B
function	B
note	O
that	O
the	O
distribution	B
can	O
be	O
flipped	O
by	O
interchanging	O
x	O
for	O
x	O
which	O
is	O
equivalent	B
to	O
interchanging	O
and	O
continuous	B
distributions	O
the	O
mean	B
is	O
given	O
by	O
varx	O
unbounded	O
distributions	O
definition	O
exponential	B
distribution	B
px	O
e	O
b	O
for	O
scale	O
b	O
varx	O
univariate	B
gaussian	B
distribution	B
the	O
gaussian	B
distribution	B
is	O
an	O
important	O
distribution	B
in	O
science	O
it	O
s	O
technical	O
description	O
is	O
given	O
in	O
definition	O
gaussian	B
distribution	B
px	O
n	O
e	O
where	O
is	O
the	O
mean	B
of	O
the	O
distribution	B
and	O
the	O
variance	B
this	O
is	O
also	O
called	O
the	O
normal	B
distribution	B
one	O
can	O
show	O
that	O
the	O
parameters	O
indeed	O
correspond	O
to	O
n	O
for	O
and	O
the	O
gaussian	B
is	O
called	O
the	O
standard	B
normal	B
distribution	B
definition	O
s	O
t-distribution	O
px	O
draft	O
march	O
figure	O
gamma	B
distribution	B
with	O
varying	O
for	O
fixed	O
gamma	B
distribution	B
with	O
varying	O
for	O
fixed	O
multivariate	B
distributions	O
figure	O
top	O
datapoints	O
drawn	O
from	O
a	O
gaussian	B
distribution	B
each	O
vertical	O
line	O
denotes	O
a	O
datapoint	O
at	O
the	O
corresponding	O
x	O
value	B
on	O
the	O
horizontal	O
axis	O
middle	O
histogram	O
using	O
equally	O
spaced	O
bins	O
of	O
the	O
datapoints	O
bottom	O
gaussian	B
distribution	B
n	O
from	O
which	O
the	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
amount	O
of	O
datapoints	O
were	O
drawn	O
data	O
and	O
limitingly	O
small	O
bin	O
size	O
the	O
normalised	O
histogram	O
tends	O
to	O
the	O
gaussian	B
probability	O
density	B
function	B
where	O
is	O
the	O
mean	B
the	O
degrees	O
of	O
freedom	O
and	O
scales	O
the	O
distribution	B
the	O
variance	B
is	O
given	O
by	O
varx	O
for	O
for	O
the	O
distribution	B
tends	O
to	O
a	O
gaussian	B
with	O
mean	B
and	O
variance	B
as	O
decreases	O
the	O
tails	O
of	O
the	O
distribution	B
become	O
fatter	O
the	O
t-distribution	O
can	O
be	O
derived	O
from	O
a	O
scaled	B
mixture	B
pxa	O
b	O
n	O
ba	O
e	O
gamis	O
b	O
d	O
b	O
bae	O
b	O
a	O
d	O
it	O
is	O
conventional	O
to	O
reparameterise	O
using	O
and	O
ab	O
multivariate	B
distributions	O
definition	O
distribution	B
the	O
dirichlet	B
distribution	B
is	O
a	O
distribution	B
on	O
probability	O
distributions	O
uq	O
q	O
p	O
where	O
zu	O
zu	O
uq	O
i	O
it	O
is	O
conventional	O
to	O
denote	O
the	O
distribution	B
as	O
dirichlet	B
draft	O
march	O
multivariate	B
gaussian	B
figure	O
dirichlet	B
distribution	B
with	O
parameter	B
displayed	O
on	O
the	O
simplex	O
black	O
denotes	O
low	O
probability	O
and	O
white	O
high	O
probability	O
the	O
parameter	B
u	O
controls	O
how	O
strongly	O
the	O
mass	O
of	O
the	O
distribution	B
is	O
pushed	O
to	O
the	O
corners	O
of	O
the	O
simplex	O
setting	O
uq	O
for	O
all	O
q	O
corresponds	O
to	O
a	O
uniform	B
distribution	B
in	O
the	O
binary	O
case	O
q	O
this	O
is	O
equivalent	B
to	O
a	O
beta	B
distribution	B
the	O
product	O
of	O
two	O
dirichlet	B
distributions	O
is	O
marginal	B
of	O
a	O
dirichlet	B
dirichlet	B
dirichlet	B
dirichlet	B
j	O
dirichlet	B
iui	O
uj	O
p	O
i	O
b	O
the	O
marginal	B
of	O
a	O
single	O
component	O
i	O
is	O
a	O
beta	B
distribution	B
multivariate	B
gaussian	B
the	O
multivariate	B
gaussian	B
plays	O
a	O
central	O
role	O
throughout	O
this	O
book	O
and	O
as	O
such	O
we	O
discuss	O
its	O
properties	B
in	O
some	O
detail	O
definition	O
gaussian	B
distribution	B
px	O
n	O
e	O
where	O
is	O
the	O
mean	B
vector	O
of	O
the	O
distribution	B
and	O
the	O
covariance	B
matrix	B
the	O
inverse	O
covariance	B
is	O
called	O
the	O
precision	B
one	O
may	O
show	O
n	O
draft	O
march	O
multivariate	B
gaussian	B
figure	O
bivariate	O
gaussian	B
with	O
mean	B
and	O
covariance	B
plotted	O
on	O
the	O
probability	O
density	B
contours	O
for	O
the	O
same	O
vertical	O
axis	O
is	O
the	O
probability	O
density	B
value	B
px	O
bivariate	O
gaussian	B
plotted	O
are	O
the	O
unit	O
eigenvectors	O
scaled	O
by	O
the	O
square	O
root	O
of	O
their	O
eigenvalues	O
i	O
the	O
multivariate	B
gaussian	B
is	O
given	O
in	O
note	O
that	O
det	O
m	O
ddet	O
where	O
m	O
is	O
a	O
d	O
d	O
matrix	B
which	O
explains	O
the	O
dimension	O
independent	O
notation	O
in	O
the	O
normalisation	B
constant	I
of	O
the	O
moment	B
representation	I
uses	O
and	O
to	O
parameterise	O
the	O
gaussian	B
the	O
alternative	O
canonical	B
representation	I
pxb	O
m	O
c	O
ce	O
xtmxxtb	O
is	O
related	O
to	O
the	O
moment	B
representation	I
via	O
m	O
m	O
btm	O
ce	O
the	O
multivariate	B
gaussian	B
is	O
widely	O
used	O
and	O
it	O
is	O
instructive	O
to	O
understand	O
the	O
geometric	O
picture	O
this	O
can	O
be	O
obtained	O
by	O
view	O
the	O
distribution	B
in	O
a	O
different	O
co-ordinate	O
system	O
first	O
we	O
use	O
that	O
every	O
real	O
symmetric	O
matrix	B
d	O
d	O
has	O
an	O
eigen-decomposition	O
e	O
et	O
where	O
ete	O
i	O
and	O
diag	O
d	O
in	O
the	O
case	O
of	O
a	O
covariance	B
matrix	B
all	O
the	O
eigenvalues	O
i	O
are	O
positive	O
this	O
means	O
that	O
one	O
can	O
use	O
the	O
transformation	O
y	O
et	O
so	O
that	O
e	O
et	O
yty	O
under	O
this	O
transformation	O
the	O
multivariate	B
gaussian	B
reduces	O
to	O
a	O
product	O
of	O
d	O
univariate	B
zero-mean	O
unit	O
variance	B
gaussians	O
the	O
jacobian	O
of	O
the	O
transformation	O
is	O
a	O
constant	O
this	O
means	O
that	O
we	O
can	O
view	O
a	O
multivariate	B
gaussian	B
as	O
a	O
shifted	O
scaled	O
and	O
rotated	O
version	O
of	O
an	O
isotropic	B
gaussian	B
in	O
which	O
the	O
centre	O
is	O
given	O
by	O
the	O
mean	B
the	O
rotation	O
by	O
the	O
eigenvectors	O
and	O
the	O
scaling	O
by	O
the	O
square	O
root	O
of	O
the	O
eigenvalues	O
as	O
depicted	O
in	O
isotropic	B
means	O
same	O
under	O
rotation	O
for	O
any	O
isotropic	B
distribution	B
contours	O
of	O
equal	O
probability	O
are	O
spherical	O
around	O
the	O
origin	O
some	O
useful	O
properties	B
of	O
the	O
gaussian	B
are	O
as	O
follows	O
draft	O
march	O
definition	O
gaussian	B
for	O
a	O
distribution	B
n	O
defined	O
jointly	O
over	O
two	O
vectors	O
x	O
and	O
y	O
of	O
potentially	O
differing	O
dimensions	O
multivariate	B
gaussian	B
x	O
x	O
y	O
y	O
z	O
with	O
corresponding	O
mean	B
and	O
partitioned	B
covariance	B
xx	O
xy	O
yx	O
yy	O
where	O
yx	O
t	O
xy	O
the	O
marginal	B
distribution	B
is	O
given	O
by	O
px	O
n	O
x	O
xx	O
and	O
conditional	B
pxy	O
n	O
x	O
xy	O
yy	O
y	O
xx	O
xy	O
yy	O
yx	O
definition	O
of	O
two	O
gaussians	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
another	O
gaussian	B
with	O
a	O
multiplicative	O
factor	B
n	O
n	O
exp	O
s	O
s	O
where	O
s	O
and	O
the	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
definition	O
transform	O
of	O
a	O
gaussian	B
for	O
the	O
linear	B
transformation	I
y	O
ax	O
b	O
where	O
x	O
n	O
y	O
a	O
b	O
a	O
y	O
n	O
definition	O
of	O
a	O
gaussian	B
the	O
differential	B
entropy	B
of	O
a	O
multivariate	B
gaussian	B
px	O
n	O
is	O
hx	O
log	O
det	O
d	O
where	O
d	O
dim	O
x	O
note	O
that	O
the	O
entropy	B
is	O
independent	O
of	O
the	O
mean	B
draft	O
march	O
multivariate	B
gaussian	B
figure	O
beta	B
distribution	B
the	O
parameters	O
and	O
can	O
also	O
be	O
witting	O
in	O
terms	O
of	O
the	O
mean	B
and	O
variance	B
leading	O
to	O
an	O
alternative	O
parameterisation	B
see	O
conditioning	B
as	O
system	B
reversal	I
for	O
a	O
joint	B
distribution	B
px	O
y	O
consider	O
the	O
conditional	B
pxy	O
the	O
statistics	O
of	O
pxy	O
can	O
be	O
obtained	O
using	O
a	O
linear	B
system	O
of	O
the	O
form	O
where	O
x	O
ay	O
n	O
and	O
this	O
reversed	O
noise	O
is	O
uncorrelated	O
with	O
y	O
to	O
show	O
this	O
we	O
need	O
to	O
make	O
the	O
statistics	O
of	O
x	O
under	O
this	O
linear	B
system	O
match	O
those	O
given	O
by	O
the	O
conditioning	B
operation	O
the	O
mean	B
of	O
the	O
linear	B
system	O
is	O
given	O
by	O
x	O
a	O
y	O
and	O
the	O
covariances	O
by	O
that	O
covariance	B
of	O
y	O
remains	O
unaffected	O
by	O
the	O
system	B
reversal	I
xx	O
a	O
yy	O
at	O
xy	O
a	O
yy	O
from	O
equation	B
we	O
have	O
a	O
xy	O
yy	O
which	O
using	O
in	O
equation	B
gives	O
using	O
equation	B
we	O
similarly	O
obtain	O
xx	O
a	O
yy	O
at	O
xx	O
xy	O
x	O
a	O
y	O
x	O
xy	O
yy	O
yx	O
this	O
means	O
that	O
we	O
can	O
write	O
an	O
explicit	O
linear	B
system	O
of	O
the	O
form	O
equation	B
where	O
the	O
parameters	O
are	O
given	O
in	O
terms	O
of	O
the	O
statistics	O
of	O
the	O
original	O
system	O
these	O
results	O
are	O
just	O
a	O
restatement	O
of	O
the	O
conditioning	B
results	O
but	O
shows	O
how	O
it	O
may	O
be	O
interpreted	O
as	O
a	O
linear	B
system	O
this	O
is	O
useful	O
in	O
deriving	O
results	O
in	O
inference	B
with	O
linear	B
dynamical	O
systems	O
yy	O
y	O
completing	O
the	O
square	O
a	O
useful	O
technique	O
in	O
manipulating	O
gaussians	O
is	O
completing	O
the	O
square	O
for	O
example	O
the	O
expression	O
e	O
xtaxbtx	O
can	O
be	O
transformed	O
as	O
follows	O
first	O
we	O
complete	O
the	O
square	O
hence	O
xtax	O
btx	O
xtax	O
btx	O
n	O
e	O
from	O
this	O
one	O
can	O
derive	O
e	O
xtaxbtxdx	O
a	O
a	O
a	O
a	O
a	O
a	O
bta	O
bta	O
bta	O
draft	O
march	O
gaussian	B
propagation	B
let	O
y	O
be	O
linearly	O
related	O
to	O
x	O
through	O
y	O
mx	O
where	O
n	O
and	O
x	O
n	O
x	O
x	O
then	O
the	O
marginal	B
py	O
x	O
pyxpx	O
is	O
a	O
gaussian	B
py	O
n	O
y	O
m	O
x	O
m	O
xmt	O
multivariate	B
gaussian	B
whitening	B
and	O
centering	B
for	O
a	O
set	O
of	O
data	O
xn	O
with	O
dim	O
xn	O
d	O
we	O
can	O
transform	O
this	O
data	O
to	O
yn	O
with	O
zero	O
mean	B
using	O
centering	B
where	O
the	O
mean	B
m	O
of	O
the	O
data	O
is	O
given	O
by	O
yn	O
xn	O
m	O
m	O
n	O
xn	O
furthermore	O
we	O
can	O
transform	O
to	O
a	O
values	O
zn	O
that	O
have	O
zero	O
mean	B
and	O
unit	O
covariance	B
using	O
whitening	B
where	O
the	O
covariance	B
s	O
of	O
the	O
data	O
is	O
given	O
by	O
zn	O
s	O
m	O
s	O
n	O
m	O
mt	O
y	O
usvt	O
y	O
then	O
an	O
equivalent	B
approach	B
is	O
to	O
compute	O
the	O
svd	B
decomposition	B
of	O
the	O
matrix	B
of	O
centered	O
datapoints	O
z	O
ndiag	O
uty	O
has	O
zero	O
mean	B
and	O
unit	O
covariance	B
see	O
maximum	B
likelihood	B
training	B
given	O
a	O
set	O
of	O
training	B
data	O
x	O
drawn	O
from	O
a	O
gaussian	B
n	O
with	O
unknown	O
mean	B
and	O
covariance	B
how	O
can	O
we	O
find	O
these	O
parameters	O
assuming	O
the	O
data	O
are	O
drawn	O
i	O
i	O
d	O
the	O
log	O
likelihood	B
is	O
log	O
px	O
l	O
n	O
log	O
det	O
draft	O
march	O
taking	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
vector	O
we	O
obtain	O
the	O
vector	O
derivative	O
equating	O
to	O
zero	O
gives	O
that	O
at	O
the	O
optimum	O
of	O
the	O
log	O
likelihood	B
the	O
derivative	O
of	O
l	O
with	O
respect	O
to	O
the	O
matrix	B
requires	O
more	O
work	O
dependence	O
on	O
the	O
covariance	B
and	O
also	O
parameterise	O
using	O
the	O
inverse	O
covariance	B
it	O
is	O
convenient	O
to	O
isolate	O
the	O
n	O
log	O
m	O
multivariate	B
gaussian	B
optimal	O
l	O
n	O
and	O
therefore	O
optimally	O
xn	O
n	O
optimal	O
using	O
m	O
mt	O
we	O
obtain	O
m	O
n	O
l	O
trace	O
l	O
n	O
equating	O
the	O
derivative	O
to	O
the	O
zero	O
matrix	B
and	O
solving	B
for	O
gives	O
equations	O
and	O
define	O
the	O
maximum	B
likelihood	B
solution	O
mean	B
and	O
covariance	B
for	O
training	B
data	O
x	O
consistent	B
with	O
our	O
previous	O
results	O
in	O
fact	O
these	O
equations	O
simply	O
set	O
the	O
parameters	O
to	O
their	O
sample	O
statistics	O
of	O
the	O
empirical	B
distribution	B
that	O
is	O
the	O
mean	B
is	O
set	O
to	O
the	O
sample	O
mean	B
of	O
the	O
data	O
and	O
the	O
covariance	B
to	O
the	O
sample	O
covariance	B
bayesian	B
inference	B
of	O
the	O
mean	B
and	O
variance	B
for	O
simplicity	O
here	O
we	O
deal	O
with	O
the	O
univariate	B
case	O
assuming	O
i	O
i	O
d	O
data	O
the	O
likelihood	B
is	O
px	O
e	O
for	O
a	O
bayesian	B
treatment	O
we	O
require	O
the	O
posterior	B
of	O
the	O
parameters	O
p	O
px	O
px	O
our	O
aim	O
is	O
to	O
find	O
conjugate	B
priors	O
for	O
the	O
mean	B
and	O
variance	B
a	O
convenient	O
choice	O
for	O
a	O
prior	B
on	O
the	O
mean	B
is	O
that	O
it	O
is	O
a	O
gaussian	B
centred	O
on	O
p	O
draft	O
march	O
e	O
the	O
posterior	B
is	O
then	O
p	O
e	O
nxn	O
p	O
it	O
is	O
convenient	O
to	O
write	O
this	O
in	O
the	O
form	O
p	O
p	O
multivariate	B
gaussian	B
since	O
equation	B
has	O
quadratic	O
contributions	O
in	O
in	O
the	O
exponent	O
the	O
conditional	B
posterior	B
p	O
is	O
gaussian	B
to	O
identify	O
this	O
gaussian	B
we	O
multiply	O
out	O
the	O
terms	O
in	O
the	O
exponent	O
to	O
arrive	O
at	O
we	O
encounter	O
a	O
difficulty	O
in	O
attempting	O
to	O
find	O
a	O
conjugate	B
prior	B
for	O
because	O
the	O
term	O
is	O
not	O
a	O
simple	O
expression	O
of	O
for	O
this	O
reason	O
we	O
constrain	O
if	O
we	O
therefore	O
use	O
an	O
inverse	B
gamma	B
distribution	B
we	O
will	O
have	O
a	O
conjugate	B
prior	B
for	O
for	O
a	O
gaussinverse-gamma	O
prior	B
a	O
c	O
draft	O
march	O
b	O
n	O
n	O
xn	O
b	O
a	O
with	O
exp	O
a	O
using	O
the	O
identity	O
we	O
can	O
write	O
a	O
c	O
a	O
p	O
ae	O
c	O
a	O
c	O
n	O
a	O
b	O
p	O
e	O
a	O
c	O
a	O
p	O
p	O
for	O
some	O
fixed	O
hyperparameter	B
defining	O
the	O
constants	O
n	O
xn	O
c	O
n	O
a	O
n	O
b	O
we	O
have	O
a	O
c	O
a	O
c	O
using	O
this	O
expression	O
in	O
equation	B
we	O
obtain	O
e	O
c	O
a	O
p	O
n	O
invgam	O
a	O
b	O
a	O
p	O
p	O
n	O
p	O
n	O
the	O
posterior	B
is	O
also	O
gauss-inverse-gamma	B
with	O
exponential	B
family	B
gauss-gamma	B
distribution	B
it	O
is	O
common	O
to	O
to	O
use	O
a	O
prior	B
on	O
the	O
precision	B
defined	O
as	O
the	O
inverse	O
variance	B
if	O
we	O
then	O
use	O
a	O
gamma	B
prior	B
p	O
gam	O
the	O
posterior	B
will	O
be	O
p	O
gam	O
c	O
a	O
where	O
the	O
gauss-gamma	B
prior	B
distribution	B
p	O
n	O
p	O
n	O
is	O
the	O
conjugate	B
prior	B
for	O
a	O
gaussian	B
with	O
unknown	O
mean	B
and	O
precision	B
the	O
posterior	B
for	O
this	O
prior	B
is	O
a	O
gauss-gamma	B
distribution	B
with	O
parameters	O
gam	O
gam	O
b	O
a	O
a	O
the	O
marginal	B
p	O
is	O
a	O
student	O
s	O
t-distribution	O
an	O
example	O
of	O
a	O
gauss-gamma	B
priorposterior	O
is	O
given	O
in	O
the	O
maximum	B
likelihood	B
solution	O
is	O
recovered	O
in	O
the	O
limit	O
of	O
a	O
flat	O
prior	B
see	O
the	O
unbiased	O
estimators	O
for	O
the	O
mean	B
and	O
variance	B
are	O
given	O
using	O
the	O
proper	O
prior	B
for	O
the	O
multivariate	B
case	O
the	O
extension	O
of	O
these	O
techniques	O
uses	O
a	O
multivariate	B
gaussian	B
distribution	B
for	O
the	O
conjugate	B
prior	B
on	O
the	O
mean	B
and	O
an	O
inverse	B
wishart	B
distribution	B
for	O
the	O
conjugate	B
prior	B
on	O
the	O
exponential	B
family	B
a	O
theoretically	O
convenient	O
class	O
of	O
distributions	O
are	O
the	O
exponential	B
family	B
which	O
contains	O
many	O
standard	O
distributions	O
including	O
the	O
gaussian	B
gamma	B
poisson	B
dirichlet	B
wishart	B
multinomial	B
markov	B
random	B
field	I
definition	O
family	B
for	O
a	O
distribution	B
on	O
a	O
multidimensional	O
variable	O
x	O
or	O
discrete	B
an	O
exponential	B
family	B
model	B
is	O
of	O
the	O
form	O
i	O
i	O
are	O
the	O
parameters	O
tix	O
the	O
test	O
statistics	O
and	O
is	O
the	O
log	O
partition	B
function	B
that	O
ensure	O
normalisation	B
px	O
hxe	O
log	O
hxe	O
x	O
draft	O
march	O
i	O
i	O
exponential	B
family	B
prior	B
posterior	B
figure	O
bayesian	B
approach	B
to	O
inferring	O
the	O
mean	B
and	O
precision	B
variance	B
of	O
a	O
gaussian	B
based	O
a	O
gauss-gamma	B
prior	B
with	O
on	O
n	O
randomly	O
drawn	O
datapoints	O
gauss-gamma	B
posterior	B
conditional	B
on	O
the	O
data	O
for	O
comparison	O
the	O
sample	O
mean	B
of	O
the	O
data	O
is	O
and	O
maximum	B
likelihood	B
optimal	O
variance	B
is	O
using	O
the	O
n	O
normalisation	B
the	O
datapoints	O
were	O
drawn	O
from	O
a	O
gaussian	B
with	O
mean	B
and	O
variance	B
see	O
demogaussbayes	O
m	O
one	O
can	O
always	O
transform	O
the	O
parameters	O
to	O
the	O
form	O
in	O
which	O
case	O
the	O
distribution	B
is	O
in	O
canonical	B
form	I
px	O
hxe	O
ttx	O
for	O
example	O
the	O
univariate	B
gaussian	B
can	O
be	O
written	O
e	O
e	O
x	O
log	O
defining	O
x	O
and	O
hx	O
then	O
log	O
note	O
that	O
the	O
parameterisation	B
is	O
not	O
necessarily	O
unique	O
we	O
can	O
for	O
example	O
rescale	O
the	O
functions	O
tix	O
and	O
inversely	O
scale	O
i	O
by	O
the	O
same	O
amount	O
to	O
arrive	O
at	O
an	O
equivalent	B
representation	O
conjugate	B
priors	O
in	O
principle	O
bayesian	B
learning	B
for	O
the	O
exponential	B
family	B
is	O
straightforward	O
in	O
canonical	B
form	I
px	O
hxe	O
ttx	O
for	O
a	O
prior	B
with	O
hyperparameters	O
p	O
e	O
t	O
the	O
posterior	B
is	O
p	O
px	O
hxe	O
ttx	O
t	O
e	O
ttx	O
so	O
that	O
the	O
prior	B
equation	B
is	O
conjugate	B
for	O
the	O
exponential	B
family	B
likelihood	B
equation	B
whilst	O
the	O
likelihood	B
is	O
in	O
the	O
exponential	B
family	B
the	O
conjugate	B
prior	B
is	O
not	O
necessarily	O
in	O
the	O
exponential	B
family	B
draft	O
march	O
the	O
kullback-leibler	B
divergence	B
klqp	O
the	O
kullback-leibler	B
divergence	B
klqp	O
the	O
kullback-leibler	B
divergence	B
klqp	O
measures	O
the	O
difference	O
between	O
distributions	O
q	O
and	O
definition	O
kl	O
divergence	B
for	O
two	O
distributions	O
qx	O
and	O
px	O
klqp	O
qx	O
log	O
where	O
denotes	O
average	B
of	O
the	O
function	B
fx	O
with	O
respect	O
to	O
the	O
distribution	B
rx	O
the	O
kl	O
divergence	B
is	O
the	O
kl	O
divergence	B
is	O
widely	O
used	O
and	O
it	O
is	O
therefore	O
important	O
to	O
understand	O
why	O
the	O
divergence	B
is	O
positive	O
to	O
see	O
this	O
consider	O
the	O
following	O
linear	B
bound	B
on	O
the	O
function	B
logx	O
logx	O
x	O
as	O
plotted	O
in	O
the	O
figure	O
on	O
the	O
right	O
replacing	O
x	O
by	O
pxqx	O
in	O
the	O
above	O
bound	B
px	O
qx	O
log	O
px	O
qx	O
since	O
probabilities	O
are	O
non-negative	O
we	O
can	O
multiply	O
both	O
sides	O
by	O
qx	O
to	O
obtain	O
we	O
now	O
integrate	O
sum	O
in	O
the	O
case	O
of	O
discrete	B
variables	O
both	O
sides	O
pxdx	O
qxdx	O
px	O
qx	O
qx	O
log	O
px	O
qx	O
log	O
qx	O
px	O
log	O
rearranging	O
gives	O
qx	O
log	O
klqp	O
the	O
kl	O
divergence	B
is	O
zero	O
if	O
and	O
only	O
if	O
the	O
two	O
distributions	O
are	O
exactly	O
the	O
same	O
entropy	B
for	O
both	O
discrete	B
and	O
continuous	B
variables	O
the	O
entropy	B
is	O
defined	O
as	O
hp	O
for	O
continuous	B
variables	O
this	O
is	O
also	O
called	O
the	O
differential	B
entropy	B
see	O
also	O
the	O
entropy	B
is	O
a	O
measure	O
of	O
the	O
uncertainty	B
in	O
a	O
distribution	B
one	O
way	O
to	O
see	O
this	O
is	O
that	O
hp	O
klpu	O
const	O
where	O
u	O
is	O
a	O
uniform	B
distribution	B
since	O
the	O
klpu	O
the	O
less	O
like	O
a	O
uniform	B
distribution	B
p	O
is	O
the	O
smaller	O
will	O
be	O
the	O
entropy	B
or	O
vice	O
versa	O
the	O
more	O
similar	O
p	O
is	O
to	O
a	O
uniform	B
distribution	B
the	O
greater	O
will	O
be	O
the	O
entropy	B
since	O
the	O
uniform	B
distribution	B
contains	O
the	O
least	O
information	O
a	O
prior	B
about	O
which	O
state	O
px	O
is	O
in	O
the	O
entropy	B
is	O
therefore	O
a	O
measure	O
of	O
the	O
a	O
priori	O
uncertainty	B
in	O
the	O
state	O
occupancy	O
for	O
a	O
discrete	B
distribution	B
we	O
can	O
permute	O
the	O
state	O
labels	O
without	O
changing	O
the	O
entropy	B
for	O
a	O
discrete	B
distribution	B
the	O
entropy	B
is	O
positive	O
whereas	O
the	O
differential	B
entropy	B
can	O
be	O
negative	O
draft	O
march	O
code	O
demogaussbayes	O
m	O
bayesian	B
fitting	O
of	O
a	O
univariate	B
gaussian	B
loggaussgamma	O
m	O
plotting	O
routine	O
for	O
a	O
gauss-gamma	B
distribution	B
exercises	O
exercises	O
exercise	O
in	O
a	O
public	O
lecture	O
the	O
following	O
phrase	O
was	O
uttered	O
by	O
a	O
professor	O
of	O
experimental	O
psychology	O
in	O
a	O
recent	O
data	O
survey	O
of	O
people	O
claim	O
to	O
have	O
above	O
average	B
intelligence	O
which	O
is	O
clearly	O
nonsense	O
laughs	O
is	O
it	O
theoretically	O
possible	O
for	O
of	O
people	O
to	O
have	O
above	O
average	B
intelligence	O
if	O
so	O
give	O
an	O
example	O
otherwise	O
explain	O
why	O
not	O
what	O
about	O
above	O
median	O
intelligence	O
exercise	O
consider	O
the	O
distribution	B
defined	O
on	O
real	O
variables	O
x	O
y	O
px	O
y	O
domx	O
domy	O
show	O
that	O
furthermore	O
show	O
that	O
x	O
and	O
y	O
are	O
uncorrelated	O
whilst	O
x	O
and	O
y	O
are	O
uncorrelated	O
show	O
that	O
they	O
are	O
nevertheless	O
dependent	O
exercise	O
for	O
a	O
variable	O
x	O
with	O
domx	O
and	O
px	O
show	O
that	O
in	O
n	O
independent	O
draws	O
xn	O
from	O
this	O
distribution	B
the	O
probability	O
of	O
observing	O
k	O
states	O
is	O
the	O
binomial	B
distribution	B
k	O
i	O
k	O
k	O
e	O
dx	O
e	O
i	O
dx	O
by	O
considering	O
exercise	O
constant	O
of	O
a	O
gaussian	B
the	O
normalisation	B
constant	I
of	O
a	O
gaussian	B
distribution	B
is	O
related	O
to	O
the	O
integral	O
e	O
dy	O
e	O
dxdy	O
and	O
transforming	O
to	O
polar	O
coordinates	O
show	O
that	O
i	O
e	O
dx	O
exercise	O
for	O
a	O
univariate	B
gaussian	B
distribution	B
show	O
that	O
n	O
dirichlet	B
b	O
k	O
b	O
j	O
exercise	O
for	O
a	O
beta	B
distribution	B
show	O
that	O
exercise	O
show	O
that	O
the	O
marginal	B
of	O
a	O
dirichlet	B
distribution	B
is	O
another	O
dirichlet	B
distribution	B
and	O
using	O
x	O
derive	O
an	O
explicit	O
expression	O
for	O
the	O
kth	O
moment	O
of	O
a	O
beta	B
distribution	B
draft	O
march	O
exercises	O
exercise	O
define	O
the	O
moment	B
generating	I
function	B
as	O
px	O
gt	O
show	O
that	O
px	O
lim	O
t	O
dk	O
dtk	O
gt	O
exercise	O
of	O
variables	O
consider	O
a	O
one	O
dimensional	O
continuous	B
random	O
variable	O
x	O
with	O
corresponding	O
px	O
for	O
a	O
variable	O
y	O
fx	O
where	O
fx	O
is	O
a	O
monotonic	O
function	B
show	O
that	O
the	O
distribution	B
of	O
y	O
is	O
df	O
dx	O
py	O
px	O
x	O
f	O
exercise	O
of	O
a	O
multivariate	B
gaussian	B
consider	O
i	O
e	O
by	O
using	O
the	O
transformation	O
show	O
that	O
z	O
i	O
a	O
b	O
m	O
c	O
d	O
exercise	O
consider	O
the	O
partitioned	B
matrix	B
for	O
which	O
we	O
wish	O
to	O
find	O
the	O
inverse	O
m	O
we	O
assume	O
that	O
a	O
is	O
m	O
m	O
and	O
invertible	O
and	O
d	O
is	O
n	O
n	O
and	O
invertible	O
by	O
definition	O
the	O
partitioned	B
inverse	O
im	O
in	O
where	O
in	O
the	O
above	O
im	O
is	O
the	O
m	O
m	O
identity	B
matrix	B
of	O
the	O
same	O
dimension	O
as	O
a	O
and	O
the	O
zero	O
matrix	B
of	O
the	O
same	O
dimension	O
as	O
d	O
using	O
the	O
above	O
derive	O
the	O
results	O
p	O
q	O
p	O
q	O
r	O
s	O
m	O
a	O
b	O
c	O
d	O
r	O
s	O
must	O
satisfy	O
p	O
bd	O
q	O
a	O
ca	O
r	O
d	O
bd	O
s	O
ca	O
the	O
skewness	B
and	O
kurtosis	B
are	O
both	O
exercise	O
show	O
that	O
for	O
gaussian	B
distribution	B
px	O
n	O
zero	O
exercise	O
consider	O
a	O
small	O
interval	O
of	O
time	O
t	O
and	O
let	O
the	O
probability	O
of	O
an	O
event	O
occurring	O
in	O
this	O
small	O
interval	O
be	O
t	O
derive	O
a	O
distribution	B
that	O
expresses	O
the	O
probability	O
of	O
at	O
least	O
one	O
event	O
in	O
an	O
interval	O
from	O
to	O
t	O
draft	O
march	O
exercise	O
consider	O
a	O
vector	O
variable	O
x	O
xn	O
and	O
set	O
of	O
functions	O
defined	O
on	O
each	O
component	O
of	O
x	O
ixi	O
for	O
example	O
for	O
x	O
we	O
might	O
have	O
exercises	O
consider	O
the	O
distribution	B
z	O
e	O
t	O
px	O
e	O
i	O
ixidxi	O
where	O
is	O
a	O
vector	O
function	B
with	O
ith	O
component	O
ixi	O
and	O
is	O
a	O
parameter	B
vector	O
each	O
component	O
is	O
tractably	O
integrable	O
in	O
the	O
sense	O
that	O
can	O
be	O
computed	O
either	O
analytically	O
or	O
to	O
an	O
acceptable	O
numerical	B
accuracy	O
show	O
that	O
xi	O
xj	O
the	O
normalisation	B
constant	I
z	O
can	O
be	O
tractably	O
computed	O
consider	O
the	O
transformation	O
x	O
my	O
for	O
an	O
invertible	O
matrix	B
m	O
show	O
that	O
the	O
distribution	B
pym	O
is	O
tractable	O
normalisation	B
constant	I
is	O
known	O
and	O
that	O
in	O
general	O
explain	O
the	O
significance	O
of	O
this	O
is	O
deriving	O
tractable	O
multivariate	B
distributions	O
exercise	O
show	O
that	O
we	O
may	O
reparameterise	O
the	O
beta	B
distribution	B
by	O
writing	O
the	O
parameters	O
and	O
as	O
functions	O
of	O
the	O
mean	B
m	O
and	O
variance	B
s	O
using	O
m	O
s	O
exercise	O
consider	O
the	O
function	B
f	O
show	O
that	O
lim	O
f	O
log	O
and	O
hence	O
that	O
log	O
d	O
lim	O
using	O
this	O
result	O
show	O
therefore	O
that	O
f	O
log	O
b	O
where	O
b	O
is	O
the	O
beta	B
function	B
show	O
additionally	O
that	O
log	O
b	O
using	O
the	O
fact	O
that	O
b	O
f	O
where	O
is	O
the	O
gamma	B
function	B
relate	O
the	O
above	O
averages	O
to	O
the	O
digamma	B
function	B
defined	O
as	O
d	O
dx	O
log	O
draft	O
march	O
exercises	O
exercise	O
using	O
a	O
similar	O
generating	O
function	B
approach	B
as	O
in	O
explain	O
how	O
to	O
compute	O
exercise	O
consider	O
the	O
function	B
ui	O
i	O
fx	O
i	O
x	O
e	O
sxfxdx	O
is	O
show	O
that	O
the	O
laplace	B
transform	O
of	O
fx	O
fs	O
d	O
d	O
n	O
i	O
s	O
i	O
ui	O
e	O
i	O
d	O
i	O
fs	O
i	O
ui	O
s	O
by	O
taking	O
the	O
inverse	O
laplace	B
transform	O
show	O
that	O
i	O
ui	O
i	O
ui	O
x	O
fx	O
i	O
ui	O
hence	O
show	O
that	O
the	O
normalisation	B
constant	I
of	O
a	O
dirichlet	B
distribution	B
with	O
parameters	O
u	O
is	O
given	O
by	O
exercise	O
by	O
using	O
the	O
laplace	B
transform	O
as	O
in	O
show	O
that	O
the	O
marginal	B
of	O
a	O
dirichlet	B
distribution	B
is	O
a	O
dirichlet	B
distribution	B
exercise	O
derive	O
the	O
formula	O
for	O
the	O
differential	B
entropy	B
of	O
a	O
multi-variate	B
gaussian	B
exercise	O
show	O
that	O
for	O
a	O
gamma	B
distribution	B
gam	O
the	O
mode	B
is	O
given	O
by	O
x	O
provided	O
that	O
exercise	O
consider	O
a	O
distribution	B
px	O
and	O
a	O
distribution	B
with	O
changed	O
by	O
a	O
small	O
amount	O
take	O
the	O
taylor	B
expansion	I
of	O
klpx	O
for	O
small	O
and	O
show	O
that	O
this	O
is	O
equal	O
to	O
log	O
px	O
p	O
more	O
generally	O
for	O
a	O
distribution	B
parameterised	O
by	O
a	O
vector	O
i	O
i	O
show	O
that	O
a	O
small	O
change	O
in	O
the	O
parameter	B
results	O
in	O
ij	O
i	O
j	O
fij	O
where	O
the	O
fisher	B
information	I
matrix	B
is	O
defined	O
as	O
fij	O
i	O
j	O
log	O
px	O
p	O
log	O
px	O
show	O
that	O
the	O
fisher	B
information	I
matrix	B
is	O
positive	B
definite	I
by	O
expressing	O
it	O
equivalently	O
as	O
fij	O
log	O
px	O
j	O
i	O
draft	O
march	O
p	O
exercise	O
consider	O
the	O
joint	B
prior	B
distribution	B
p	O
n	O
n	O
n	O
xn	O
n	O
gam	O
n	O
show	O
that	O
for	O
then	O
the	O
prior	B
distribution	B
becomes	O
flat	O
of	O
and	O
for	O
show	O
that	O
for	O
these	O
settings	O
the	O
mean	B
and	O
variance	B
that	O
jointly	O
maximise	O
the	O
posterior	B
equation	B
are	O
given	O
by	O
the	O
standard	O
maximum	B
likelihood	B
settings	O
exercises	O
exercise	O
show	O
that	O
in	O
the	O
limit	O
the	O
jointly	O
optimal	O
mean	B
and	O
variance	B
obtained	O
from	O
argmax	O
is	O
given	O
by	O
n	O
p	O
xn	O
n	O
n	O
n	O
where	O
note	O
that	O
these	O
correspond	O
to	O
the	O
standard	O
unbiased	O
estimators	O
of	O
the	O
mean	B
and	O
variance	B
exercise	O
for	O
the	O
gauss-gamma	B
posterior	B
p	O
given	O
in	O
equation	B
compute	O
the	O
marginal	B
posterior	B
p	O
what	O
is	O
the	O
mean	B
of	O
this	O
distribution	B
exercise	O
derive	O
equation	B
exercise	O
consider	O
the	O
multivariate	B
gaussian	B
distribution	B
px	O
n	O
on	O
the	O
vector	O
x	O
with	O
components	O
xn	O
px	O
e	O
and	O
pyix	O
n	O
calculate	O
xi	O
xn	O
exercise	O
observations	O
yn	O
are	O
noisy	O
i	O
i	O
d	O
measurements	O
of	O
an	O
underlying	O
variable	O
x	O
with	O
px	O
n	O
with	O
mean	B
x	O
for	O
i	O
n	O
show	O
that	O
yn	O
is	O
gaussian	B
n	O
n	O
y	O
where	O
y	O
yn	O
and	O
variance	B
n	O
such	O
that	O
n	O
n	O
exercise	O
consider	O
a	O
set	O
of	O
data	O
xn	O
drawn	O
from	O
a	O
gaussian	B
with	O
know	O
mean	B
and	O
unknown	O
variance	B
assume	O
a	O
gamma	B
distribution	B
prior	B
on	O
p	O
gamis	O
b	O
show	O
that	O
the	O
posterior	B
distribution	B
is	O
n	O
b	O
p	O
gamis	O
draft	O
march	O
exercises	O
show	O
that	O
the	O
distribution	B
for	O
x	O
is	O
pxx	O
px	O
student	O
x	O
a	O
b	O
exercise	O
the	O
poisson	B
distribution	B
is	O
a	O
discrete	B
distribution	B
on	O
the	O
non-negative	O
integers	O
with	O
p	O
e	O
x	O
x	O
x	O
you	O
are	O
given	O
a	O
sample	O
of	O
n	O
observations	O
xn	O
drawn	O
from	O
this	O
distribution	B
determine	O
the	O
maximum	B
likelihood	B
estimator	O
of	O
the	O
poisson	B
parameter	B
exercise	O
for	O
a	O
gaussian	B
mixture	B
model	B
pin	O
i	O
i	O
pi	O
i	O
show	O
that	O
px	O
has	O
mean	B
i	O
px	O
and	O
i	O
pi	O
i	O
pi	O
i	O
i	O
t	O
i	O
i	O
i	O
j	O
pi	O
i	O
pj	O
t	O
j	O
pi	O
exercise	O
show	O
that	O
for	O
the	O
whitened	O
data	O
matrix	B
given	O
in	O
equation	B
zzt	O
ni	O
exercise	O
consider	O
a	O
uniform	B
distribution	B
pi	O
defined	O
on	O
states	O
i	O
n	O
show	O
that	O
the	O
entropy	B
of	O
this	O
distribution	B
is	O
h	O
pi	O
log	O
pi	O
log	O
n	O
and	O
that	O
there	O
for	O
as	O
the	O
number	O
of	O
states	O
n	O
increases	O
to	O
infinity	O
the	O
entropy	B
diverges	O
to	O
infinity	O
exercise	O
consider	O
a	O
continuous	B
distribution	B
px	O
x	O
we	O
can	O
form	O
a	O
discrete	B
approximation	B
with	O
probabilities	O
pi	O
to	O
this	O
continuous	B
distribution	B
by	O
identifying	O
a	O
continuous	B
value	B
in	O
for	O
each	O
state	O
i	O
n	O
with	O
this	O
pi	O
pin	O
i	O
pin	O
show	O
that	O
the	O
entropy	B
h	O
i	O
pin	O
pin	O
log	O
pin	O
i	O
pi	O
log	O
pi	O
is	O
given	O
by	O
i	O
i	O
pin	O
since	O
for	O
a	O
continuous	B
distribution	B
pxdx	O
h	O
n	O
pin	O
draft	O
march	O
a	O
discrete	B
approximation	B
of	O
this	O
integral	O
into	O
bins	O
of	O
size	O
gives	O
hence	O
show	O
that	O
for	O
large	O
n	O
h	O
px	O
log	O
pxdx	O
const	O
exercises	O
where	O
the	O
constant	O
tends	O
to	O
infinity	O
as	O
n	O
note	O
that	O
this	O
result	O
says	O
that	O
as	O
a	O
continuous	B
distribution	B
has	O
essentially	O
an	O
infinite	O
number	O
of	O
states	O
the	O
amount	O
of	O
uncertainty	B
in	O
the	O
distribution	B
is	O
infinite	O
we	O
would	O
need	O
an	O
infinite	O
number	O
of	O
bits	O
to	O
specify	O
a	O
continuous	B
value	B
this	O
motivates	O
the	O
definition	O
of	O
the	O
differential	B
entropy	B
which	O
neglects	O
the	O
infinite	O
constant	O
of	O
the	O
limiting	O
case	O
of	O
the	O
discrete	B
entropy	B
exercise	O
consider	O
two	O
multivariate	B
gaussians	O
n	O
and	O
n	O
show	O
that	O
the	O
log	O
product	O
of	O
the	O
two	O
gaussians	O
is	O
given	O
by	O
t	O
t	O
log	O
det	O
det	O
a	O
a	O
defining	O
a	O
and	O
b	O
log	O
det	O
det	O
writing	O
a	O
and	O
a	O
show	O
that	O
the	O
product	O
of	O
gaussians	O
is	O
a	O
gaussian	B
with	O
covariance	B
t	O
bta	O
we	O
can	O
write	O
the	O
above	O
as	O
t	O
mean	B
and	O
log	O
prefactor	O
bta	O
t	O
t	O
show	O
that	O
this	O
can	O
be	O
written	O
as	O
n	O
n	O
exp	O
exercise	O
show	O
that	O
px	O
log	O
det	O
det	O
s	O
s	O
log	O
det	O
draft	O
march	O
chapter	O
learning	B
as	O
inference	B
learning	B
as	O
inference	B
in	O
previous	O
chapters	O
we	O
largely	O
assumed	O
that	O
all	O
distributions	O
are	O
fully	O
specified	O
for	O
the	O
inference	B
tasks	O
in	O
machine	O
learning	B
and	O
related	O
fields	O
however	O
the	O
distributions	O
need	O
to	O
be	O
learned	O
on	O
the	O
basis	O
of	O
data	O
learning	B
is	O
then	O
the	O
problem	B
of	O
integrating	O
data	O
with	O
domain	B
knowledge	O
of	O
the	O
model	B
environment	O
definition	O
and	O
posteriors	O
priors	O
and	O
posteriors	O
typically	O
refer	O
to	O
the	O
parameter	B
distributions	O
before	O
to	O
and	O
after	O
to	O
seeing	O
the	O
data	O
formally	O
bayes	O
rule	O
relates	O
these	O
via	O
p	O
pv	O
pv	O
where	O
is	O
the	O
parameter	B
of	O
interest	O
and	O
v	O
represents	O
the	O
observed	O
data	O
learning	B
the	O
bias	B
of	O
a	O
coin	O
consider	O
data	O
expressing	O
the	O
results	O
of	O
tossing	O
a	O
coin	O
we	O
write	O
vn	O
if	O
on	O
toss	O
n	O
the	O
coin	O
comes	O
up	O
heads	O
and	O
vn	O
if	O
it	O
is	O
tails	O
our	O
aim	O
is	O
to	O
estimate	O
the	O
probability	O
that	O
the	O
coin	O
will	O
be	O
a	O
head	O
pvn	O
called	O
the	O
bias	B
of	O
the	O
coin	O
for	O
a	O
fair	O
coin	O
the	O
variables	O
in	O
this	O
environment	O
are	O
vn	O
and	O
and	O
we	O
require	O
a	O
model	B
of	O
the	O
probabilistic	B
interaction	O
of	O
the	O
variables	O
vn	O
assuming	O
there	O
is	O
no	O
dependence	O
between	O
the	O
observed	O
tosses	O
except	O
through	O
we	O
have	O
the	O
belief	B
network	I
vn	O
p	O
pvn	O
which	O
is	O
depicted	O
in	O
the	O
assumption	O
that	O
each	O
observation	O
is	O
identically	B
and	I
independently	I
distributed	I
is	O
called	O
the	O
i	O
i	O
d	O
assumption	O
learning	B
refers	O
to	O
using	O
the	O
observations	O
vn	O
to	O
infer	O
in	O
this	O
context	O
our	O
interest	O
is	O
p	O
vn	O
vn	O
vn	O
vn	O
vn	O
we	O
still	O
need	O
to	O
fully	O
specify	O
the	O
prior	B
p	O
to	O
avoid	O
complexities	O
resulting	O
from	O
continuous	B
variables	O
we	O
ll	O
consider	O
a	O
discrete	B
with	O
only	O
three	O
possible	O
states	O
specifically	O
we	O
assume	O
p	O
p	O
p	O
vn	O
vn	O
n	O
learning	B
as	O
inference	B
figure	O
belief	B
network	I
for	O
coin	O
tossing	O
model	B
plate	B
notation	O
equivalent	B
of	O
a	O
plate	B
replicates	O
the	O
quantities	O
inside	O
the	O
plate	B
a	O
number	O
of	O
times	O
as	O
specified	O
in	O
the	O
plate	B
as	O
shown	O
in	O
this	O
prior	B
expresses	O
that	O
we	O
have	O
belief	O
that	O
the	O
coin	O
is	O
fair	O
belief	O
the	O
coin	O
is	O
biased	O
to	O
land	O
heads	O
and	O
belief	O
the	O
coin	O
is	O
biased	O
to	O
land	O
tails	O
the	O
distribution	B
of	O
given	O
the	O
data	O
and	O
our	O
beliefs	O
is	O
pvn	O
p	O
p	O
vn	O
p	O
p	O
i	O
is	O
the	O
number	O
of	O
occurrences	O
of	O
heads	O
which	O
we	O
more	O
conveniently	O
denote	O
in	O
the	O
as	O
nh	O
i	O
is	O
the	O
number	O
of	O
tails	O
nt	O
hence	O
p	O
vn	O
p	O
nh	O
for	O
an	O
experiment	O
with	O
nh	O
nt	O
the	O
posterior	B
distribution	B
is	O
p	O
k	O
k	O
p	O
k	O
k	O
p	O
k	O
k	O
where	O
v	O
is	O
shorthand	O
for	O
vn	O
from	O
the	O
normalisation	B
requirement	O
we	O
have	O
so	O
that	O
p	O
p	O
p	O
as	O
shown	O
in	O
these	O
are	O
the	O
posterior	B
parameter	B
beliefs	O
in	O
this	O
case	O
if	O
we	O
were	O
asked	O
to	O
choose	O
a	O
single	O
a	O
posteriori	O
most	O
likely	O
value	B
for	O
it	O
would	O
be	O
although	O
our	O
confidence	O
in	O
this	O
is	O
low	O
since	O
the	O
posterior	B
belief	O
that	O
is	O
also	O
appreciable	O
this	O
result	O
is	O
intuitive	O
since	O
even	O
though	O
we	O
observed	O
more	O
tails	O
than	O
heads	O
our	O
prior	B
belief	O
was	O
that	O
it	O
was	O
more	O
likely	O
the	O
coin	O
is	O
fair	O
repeating	O
the	O
above	O
with	O
nh	O
nt	O
the	O
posterior	B
changes	O
to	O
p	O
p	O
so	O
that	O
the	O
posterior	B
belief	O
in	O
dominates	O
this	O
is	O
reasonable	O
since	O
in	O
this	O
situation	O
there	O
are	O
so	O
many	O
more	O
tails	O
than	O
heads	O
that	O
this	O
is	O
unlikely	O
to	O
occur	O
from	O
a	O
fair	O
coin	O
even	O
though	O
we	O
a	O
priori	O
thought	O
that	O
the	O
coin	O
was	O
fair	O
a	O
posteriori	O
we	O
have	O
enough	O
evidence	O
to	O
change	O
our	O
minds	O
p	O
figure	O
prior	B
encoding	O
our	O
beliefs	O
about	O
the	O
amount	O
the	O
coin	O
is	O
biased	O
posterior	B
having	O
seen	O
to	O
heads	O
posterior	B
having	O
heads	O
and	O
tails	O
seen	O
heads	O
and	O
tails	O
draft	O
march	O
learning	B
as	O
inference	B
making	O
decisions	O
in	O
itself	O
the	O
bayesian	B
posterior	B
merely	O
represents	O
our	O
beliefs	O
and	O
says	O
nothing	O
about	O
how	O
best	O
to	O
summarise	O
these	O
beliefs	O
in	O
situations	O
in	O
which	O
decisions	O
need	O
to	O
be	O
taken	O
under	O
uncertainty	B
we	O
need	O
to	O
additionally	O
specify	O
what	O
the	O
utility	B
of	O
any	O
decision	O
is	O
as	O
in	O
in	O
the	O
coin	O
tossing	O
scenario	O
where	O
is	O
assumed	O
to	O
be	O
either	O
or	O
we	O
setup	O
a	O
decision	O
problem	B
as	O
follows	O
if	O
we	O
correctly	O
state	O
the	O
bias	B
of	O
the	O
coin	O
we	O
gain	O
points	O
being	O
incorrect	O
however	O
loses	O
points	O
we	O
can	O
write	O
this	O
using	O
u	O
where	O
is	O
the	O
true	O
value	B
for	O
the	O
bias	B
the	O
expected	O
utility	B
of	O
the	O
decision	O
that	O
the	O
coin	O
is	O
is	O
u	O
u	O
u	O
u	O
plugging	O
in	O
the	O
numbers	O
from	O
equation	B
we	O
obtain	O
u	O
similarly	O
u	O
and	O
u	O
so	O
that	O
the	O
best	O
decision	O
is	O
to	O
say	O
that	O
the	O
coin	O
is	O
unbiased	O
repeating	O
the	O
above	O
calculations	O
for	O
nh	O
nt	O
we	O
arrive	O
at	O
u	O
u	O
u	O
so	O
that	O
the	O
best	O
decision	O
in	O
this	O
case	O
is	O
to	O
choose	O
as	O
more	O
information	O
about	O
the	O
distribution	B
pv	O
becomes	O
available	O
the	O
posterior	B
p	O
becomes	O
increasingly	O
peaked	O
aiding	O
our	O
decision	O
making	O
process	O
a	O
continuum	O
of	O
parameters	O
in	O
we	O
considered	O
only	O
three	O
possible	O
values	O
for	O
here	O
we	O
discuss	O
a	O
continuum	O
of	O
parameters	O
using	O
a	O
flat	O
prior	B
we	O
first	O
examine	O
the	O
case	O
of	O
a	O
flat	O
or	O
uniform	B
prior	B
p	O
k	O
for	O
some	O
constant	O
k	O
for	O
continuous	B
variables	O
normalisation	B
requires	O
p	O
since	O
p	O
k	O
draft	O
march	O
learning	B
as	O
inference	B
figure	O
posterior	B
p	O
assuming	O
a	O
flat	O
prior	B
on	O
nh	O
nt	O
and	O
nh	O
nt	O
in	O
both	O
cases	O
the	O
most	O
probable	O
state	O
of	O
the	O
posterior	B
is	O
which	O
makes	O
intuitive	O
sense	O
since	O
the	O
fraction	O
of	O
heads	O
to	O
tails	O
in	O
both	O
cases	O
is	O
where	O
there	O
is	O
more	O
data	O
the	O
posterior	B
is	O
more	O
certain	O
and	O
sharpens	O
around	O
the	O
most	O
probable	O
value	B
the	O
maximum	O
a	O
posteriori	O
setting	O
is	O
in	O
both	O
cases	O
this	O
being	O
the	O
value	B
of	O
for	O
which	O
the	O
posterior	B
attains	O
its	O
highest	O
value	B
repeating	O
the	O
previous	O
calculations	O
with	O
this	O
flat	O
continuous	B
prior	B
we	O
have	O
c	O
nh	O
p	O
c	O
where	O
c	O
is	O
a	O
constant	O
to	O
be	O
determined	O
by	O
normalisation	B
nh	O
d	O
bnh	O
nt	O
where	O
b	O
is	O
the	O
beta	B
function	B
definition	O
if	O
the	O
posterior	B
is	O
of	O
the	O
same	O
parametric	O
form	O
as	O
the	O
prior	B
then	O
we	O
call	O
the	O
prior	B
the	O
conjugate	B
distribution	B
for	O
the	O
likelihood	B
distribution	B
using	O
a	O
conjugate	B
prior	B
determining	O
the	O
normalisation	B
constant	I
of	O
a	O
continuous	B
distribution	B
requires	O
that	O
the	O
integral	O
of	O
the	O
unnormalised	O
posterior	B
can	O
be	O
carried	O
out	O
for	O
the	O
coin	O
tossing	O
case	O
it	O
is	O
clear	O
that	O
if	O
the	O
prior	B
is	O
of	O
the	O
form	O
of	O
a	O
beta	B
distribution	B
then	O
the	O
posterior	B
will	O
be	O
of	O
the	O
same	O
parametric	O
form	O
p	O
b	O
the	O
posterior	B
is	O
p	O
nh	O
so	O
that	O
p	O
b	O
nh	O
nt	O
b	O
nh	O
nt	O
the	O
prior	B
and	O
posterior	B
are	O
of	O
the	O
same	O
form	O
beta	B
distributions	O
but	O
simply	O
with	O
different	O
parameters	O
hence	O
the	O
beta	B
distribution	B
is	O
conjugate	B
to	O
the	O
binomial	B
distribution	B
decisions	O
based	O
on	O
continuous	B
intervals	O
the	O
result	O
of	O
a	O
coin	O
tossing	O
experiment	O
is	O
nh	O
heads	O
and	O
nt	O
tails	O
you	O
now	O
need	O
to	O
make	O
a	O
decision	O
you	O
win	O
dollars	O
if	O
your	O
guess	O
that	O
the	O
coin	O
is	O
more	O
likely	O
to	O
come	O
up	O
heads	O
than	O
tails	O
is	O
correct	O
if	O
your	O
guess	O
is	O
incorrect	O
you	O
lose	O
a	O
million	O
dollars	O
what	O
is	O
your	O
decision	O
an	O
uninformative	O
prior	B
we	O
need	O
two	O
quantities	O
for	O
our	O
guess	O
and	O
for	O
the	O
truth	O
then	O
the	O
utility	B
of	O
saying	O
heads	O
is	O
u	O
u	O
draft	O
march	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
in	O
the	O
above	O
p	O
p	O
b	O
nh	O
nt	O
nh	O
nt	O
d	O
where	O
ixa	O
b	O
is	O
the	O
regularised	B
incomplete	O
beta	B
function	B
for	O
the	O
former	O
case	O
of	O
nh	O
nt	O
under	O
a	O
flat	O
prior	B
p	O
nt	O
since	O
the	O
events	O
are	O
exclusive	O
p	O
hence	O
the	O
expected	O
utility	B
of	O
saying	O
heads	O
is	O
more	O
likely	O
is	O
similarly	O
the	O
utility	B
of	O
saying	O
tails	O
is	O
more	O
likely	O
is	O
so	O
we	O
are	O
better	O
off	O
taking	O
the	O
decision	O
that	O
the	O
coin	O
is	O
more	O
likely	O
to	O
come	O
up	O
tails	O
if	O
we	O
modify	O
the	O
above	O
so	O
that	O
we	O
lose	O
million	O
dollars	O
if	O
we	O
guess	O
tails	O
when	O
in	O
fact	O
it	O
as	O
heads	O
the	O
expected	O
utility	B
of	O
saying	O
tails	O
would	O
be	O
in	O
which	O
case	O
we	O
would	O
be	O
better	O
of	O
saying	O
heads	O
in	O
this	O
case	O
even	O
though	O
we	O
are	O
more	O
confident	O
that	O
the	O
coin	O
is	O
likely	O
to	O
come	O
up	O
tails	O
we	O
would	O
pay	O
such	O
a	O
penalty	O
of	O
making	O
a	O
mistake	O
in	O
saying	O
tails	O
that	O
it	O
is	O
fact	O
better	O
to	O
say	O
heads	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
summarising	O
the	O
posterior	B
definition	O
likelihood	B
and	O
maximum	O
a	O
posteriori	O
maximum	B
likelihood	B
sets	O
parameter	B
given	O
data	O
v	O
using	O
m	O
l	O
argmax	O
pv	O
maximum	O
a	O
posteriori	O
uses	O
that	O
setting	O
that	O
maximises	O
the	O
posterior	B
distribution	B
of	O
the	O
parameter	B
m	O
ap	O
argmax	O
pv	O
where	O
p	O
is	O
the	O
prior	B
distribution	B
m	O
in	O
making	O
such	O
an	O
approximation	B
potentially	O
useful	O
information	O
concerning	O
the	O
reliability	O
a	O
crude	O
summary	O
of	O
the	O
posterior	B
is	O
given	O
by	O
a	O
distribution	B
with	O
all	O
its	O
mass	O
in	O
a	O
single	O
most	B
likely	I
state	I
of	O
the	O
parameter	B
estimate	O
is	O
lost	O
possibilities	O
and	O
their	O
associated	O
credibilities	O
in	O
contrast	O
the	O
full	O
posterior	B
reflects	O
our	O
beliefs	O
about	O
the	O
range	O
of	O
one	O
can	O
motivate	O
map	B
from	O
a	O
decision	O
theoretic	O
perspective	O
if	O
we	O
assume	O
a	O
utility	B
that	O
is	O
zero	O
for	O
all	O
but	O
the	O
correct	O
u	O
true	O
i	O
true	O
draft	O
march	O
a	O
an	O
s	O
sn	O
cn	O
n	O
n	O
c	O
a	O
s	O
c	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
figure	O
a	O
model	B
for	O
the	O
relationship	O
between	O
lung	O
cancer	O
asbestos	O
exposure	O
and	O
smoking	O
plate	B
notation	O
replicating	O
the	O
observed	O
n	O
datapoints	O
and	O
placing	O
priors	O
over	O
the	O
cpts	O
tied	O
across	O
all	O
datapoints	O
then	O
the	O
expected	O
utility	B
of	O
is	O
u	O
true	O
i	O
true	O
p	O
truev	O
p	O
this	O
means	O
that	O
the	O
maximum	O
utility	B
decision	O
is	O
to	O
return	O
that	O
with	O
the	O
highest	O
posterior	B
value	B
when	O
a	O
flat	O
prior	B
p	O
const	O
is	O
used	O
the	O
map	B
parameter	B
assignment	O
is	O
equivalent	B
to	O
the	O
maximum	B
likelihood	B
setting	O
m	O
l	O
argmax	O
pv	O
the	O
term	O
maximum	B
likelihood	B
refers	O
to	O
the	O
parameter	B
for	O
which	O
the	O
observed	O
data	O
is	O
most	O
likely	O
to	O
be	O
generated	O
by	O
the	O
model	B
since	O
the	O
logarithm	O
is	O
a	O
strictly	O
increasing	O
function	B
then	O
for	O
a	O
positive	O
function	B
f	O
opt	O
argmax	O
f	O
opt	O
argmax	O
log	O
f	O
so	O
that	O
the	O
map	B
parameters	O
can	O
be	O
found	O
either	O
by	O
optimising	O
the	O
map	B
objective	O
or	O
equivalently	O
its	O
logarithm	O
log	O
p	O
log	O
pv	O
log	O
p	O
log	O
pv	O
where	O
the	O
normalisation	B
constant	I
pv	O
is	O
not	O
a	O
function	B
of	O
the	O
log	O
likelihood	B
is	O
convenient	O
since	O
under	O
the	O
i	O
i	O
d	O
assumption	O
it	O
is	O
a	O
summation	O
of	O
data	O
terms	O
log	O
p	O
n	O
log	O
pvn	O
log	O
p	O
log	O
pv	O
so	O
that	O
quantities	O
such	O
as	O
derivatives	O
of	O
the	O
log-likelihood	O
w	O
r	O
t	O
are	O
straightforward	O
to	O
compute	O
example	O
in	O
the	O
coin-tossing	O
experiment	O
of	O
the	O
ml	O
setting	O
is	O
in	O
both	O
nh	O
nt	O
and	O
nh	O
nt	O
maximum	B
likelihood	B
and	O
the	O
empirical	B
distribution	B
given	O
a	O
dataset	O
of	O
discrete	B
variables	O
x	O
we	O
define	O
the	O
empirical	B
distribution	B
as	O
i	O
xn	O
draft	O
march	O
qx	O
n	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
a	O
s	O
c	O
figure	O
a	O
database	O
containing	O
information	O
about	O
the	O
asbestos	O
exposure	O
signifies	O
exposure	O
being	O
a	O
smoker	O
signifies	O
the	O
individual	O
is	O
a	O
smoker	O
and	O
lung	O
cancer	O
signifies	O
the	O
individual	O
has	O
lung	O
cancer	O
each	O
row	O
contains	O
the	O
information	O
for	O
an	O
individual	O
so	O
that	O
there	O
are	O
individuals	O
in	O
the	O
database	O
in	O
the	O
case	O
that	O
x	O
is	O
a	O
vector	O
of	O
variables	O
i	O
xn	O
i	O
xn	O
i	O
i	O
the	O
kullback-leibler	B
divergence	B
between	O
the	O
empirical	B
distribution	B
qx	O
and	O
a	O
distribution	B
px	O
is	O
klqp	O
our	O
interest	O
is	O
the	O
functional	O
dependence	O
of	O
klqp	O
on	O
p	O
since	O
the	O
entropic	O
term	O
is	O
independent	O
of	O
px	O
we	O
may	O
consider	O
this	O
constant	O
and	O
focus	O
on	O
the	O
second	O
term	O
alone	O
hence	O
n	O
log	O
pxn	O
const	O
klqp	O
const	O
we	O
log	O
pxn	O
as	O
the	O
log	O
likelihood	B
under	O
the	O
model	B
px	O
assuming	O
that	O
the	O
data	O
is	O
i	O
i	O
d	O
this	O
means	O
that	O
setting	O
parameters	O
by	O
maximum	B
likelihood	B
is	O
equivalent	B
to	O
setting	O
parameters	O
by	O
minimising	O
the	O
kullback-leibler	B
divergence	B
between	O
the	O
empirical	B
distribution	B
and	O
the	O
parameterised	O
distribution	B
in	O
the	O
case	O
that	O
px	O
is	O
unconstrained	O
the	O
optimal	O
choice	O
is	O
to	O
set	O
px	O
qx	O
namely	O
the	O
maximum	B
likelihood	B
optimal	O
distribution	B
corresponds	O
to	O
the	O
empirical	B
distribution	B
maximum	B
likelihood	B
training	B
of	O
belief	B
networks	I
consider	O
the	O
following	O
model	B
of	O
the	O
relationship	O
between	O
exposure	O
to	O
asbestos	O
being	O
a	O
smoker	O
and	O
the	O
incidence	B
of	O
lung	O
cancer	O
pa	O
s	O
c	O
pca	B
spaps	O
which	O
is	O
depicted	O
in	O
each	O
variable	O
is	O
binary	O
doma	O
doms	O
domc	O
we	O
assume	O
that	O
there	O
is	O
no	O
direct	O
relationship	O
between	O
smoking	O
and	O
exposure	O
to	O
asbestos	O
this	O
is	O
the	O
kind	O
of	O
assumption	O
that	O
we	O
may	O
be	O
able	O
to	O
elicit	O
from	O
medical	O
experts	O
furthermore	O
we	O
assume	O
that	O
we	O
have	O
a	O
list	O
of	O
patient	O
records	O
where	O
each	O
row	O
represents	O
a	O
patient	O
s	O
data	O
to	O
learn	O
the	O
table	O
entries	O
pca	B
s	O
we	O
can	O
do	O
so	O
by	O
counting	B
the	O
number	O
of	O
c	O
is	O
in	O
state	O
for	O
each	O
of	O
the	O
parental	O
states	O
of	O
a	O
and	O
s	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
similarly	O
based	O
on	O
counting	B
pa	O
and	O
ps	O
these	O
three	O
cpts	O
then	O
complete	O
the	O
full	O
distribution	B
specification	O
setting	O
the	O
cpt	O
entries	O
in	O
this	O
way	O
by	O
counting	B
the	O
relative	O
number	O
of	O
occurrences	O
corresponds	O
mathematically	O
to	O
maximum	B
likelihood	B
learning	B
under	O
the	O
i	O
i	O
d	O
assumption	O
as	O
we	O
show	O
below	O
maximum	B
likelihood	B
corresponds	O
to	O
counting	B
for	O
a	O
bn	O
there	O
is	O
a	O
constraint	O
on	O
the	O
form	O
of	O
px	O
namely	O
px	O
pxipa	O
draft	O
march	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
to	O
compute	O
the	O
maximum	B
likelihood	B
setting	O
of	O
each	O
term	O
pxipa	O
as	O
shown	O
in	O
we	O
can	O
equivalently	O
minimise	O
the	O
kullback-leibler	B
divergence	B
between	O
the	O
empirical	B
distribution	B
qx	O
and	O
px	O
for	O
the	O
bn	O
px	O
and	O
empirical	B
distribution	B
qx	O
we	O
have	O
log	O
p	O
const	O
qx	O
klqp	O
p	O
const	O
this	O
follows	O
using	O
the	O
general	O
result	O
which	O
says	O
that	O
if	O
the	O
function	B
f	O
only	O
depends	O
on	O
a	O
subset	O
of	O
the	O
variables	O
we	O
only	O
need	O
to	O
know	O
the	O
marginal	B
distribution	B
of	O
this	O
subset	O
of	O
variables	O
in	O
order	O
to	O
carry	O
out	O
the	O
average	B
since	O
qx	O
is	O
fixed	O
we	O
can	O
add	O
on	O
entropic	O
terms	O
in	O
q	O
and	O
equivalently	O
mimimize	O
klqp	O
qxipa	O
p	O
the	O
final	O
line	O
is	O
a	O
positive	O
weighted	O
sum	O
of	O
individual	O
kullback-leibler	O
divergences	O
the	O
minimal	O
kullbackleibler	O
setting	O
and	O
that	O
which	O
corresponds	O
to	O
maximum	B
likelihood	B
is	O
therefore	O
in	O
terms	O
of	O
the	O
original	O
data	O
this	O
is	O
pxipa	O
qxipa	O
pxi	O
spa	O
t	O
i	O
s	O
i	O
xj	O
paxi	O
j	O
this	O
expression	O
corresponds	O
to	O
the	O
intuition	O
that	O
the	O
table	O
entry	O
pxipa	O
can	O
be	O
set	O
by	O
counting	B
the	O
number	O
of	O
times	O
the	O
state	O
s	O
pa	O
t	O
occurs	O
in	O
the	O
dataset	O
t	O
is	O
a	O
vector	O
of	O
parental	O
states	O
the	O
table	O
is	O
then	O
given	O
by	O
the	O
relative	O
number	O
of	O
counts	O
of	O
being	O
in	O
state	O
s	O
compared	O
to	O
the	O
other	O
states	O
for	O
fixed	O
joint	B
parental	O
state	O
t	O
an	O
alternative	O
method	O
to	O
derive	O
this	O
intuitive	O
result	O
is	O
to	O
use	O
lagrange	O
multipliers	O
see	O
for	O
reader	O
less	O
comfortable	O
with	O
the	O
above	O
kullback-leibler	O
derivation	O
a	O
more	O
direct	O
example	O
is	O
given	O
below	O
which	O
makes	O
use	O
of	O
the	O
notation	O
to	O
denote	O
the	O
number	O
of	O
times	O
that	O
states	O
occur	O
together	O
in	O
the	O
training	B
data	O
example	O
we	O
wish	O
to	O
learn	O
the	O
table	O
entries	O
of	O
the	O
distribution	B
we	O
address	O
here	O
how	O
to	O
find	O
the	O
cpt	O
entry	O
using	O
maximum	B
likelihood	B
for	O
i	O
i	O
d	O
data	O
the	O
contribution	O
from	O
to	O
the	O
log	O
likelihood	B
is	O
n	O
log	O
pxn	O
xn	O
the	O
number	O
of	O
times	O
occurs	O
in	O
the	O
log	O
likelihood	B
is	O
the	O
number	O
of	O
such	O
occurrences	O
in	O
the	O
training	B
set	O
since	O
the	O
normalisation	B
constraint	O
draft	O
march	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	B
xn	O
xn	O
y	O
figure	O
a	O
variable	O
y	O
with	O
a	O
large	O
number	O
of	O
parents	B
xn	O
requires	O
the	O
specification	O
of	O
an	O
exponentially	O
large	O
number	O
of	O
entries	O
in	O
the	O
conditional	B
probability	I
xn	O
one	O
solution	O
to	O
this	O
difficulty	O
is	O
to	O
parameterise	O
the	O
conditional	B
xn	O
the	O
total	O
contribution	O
of	O
to	O
the	O
log	O
likelihood	B
is	O
log	O
log	O
using	O
we	O
have	O
log	O
log	O
differentiating	O
the	O
above	O
expression	O
w	O
r	O
t	O
and	O
equating	O
to	O
zero	O
gives	O
the	O
solution	O
for	O
optimal	O
is	O
then	O
corresponding	O
to	O
the	O
intuitive	O
counting	B
procedure	O
conditional	B
probability	I
functions	O
consider	O
a	O
binary	O
variable	O
y	O
with	O
n	O
binary	O
parental	O
variables	O
x	O
xn	O
there	O
are	O
entries	O
in	O
the	O
cpt	O
of	O
pyx	O
so	O
that	O
it	O
is	O
infeasible	O
to	O
explicitly	O
store	O
these	O
entries	O
for	O
even	O
moderate	O
values	O
of	O
n	O
to	O
reduce	O
the	O
complexity	O
of	O
this	O
cpt	O
we	O
may	O
constrain	O
the	O
form	O
of	O
the	O
table	O
for	O
example	O
one	O
could	O
use	O
a	O
function	B
py	O
w	O
e	O
wtx	O
where	O
we	O
only	O
need	O
to	O
specify	O
the	O
n-dimensional	O
parameter	B
vector	O
w	O
in	O
this	O
case	O
rather	O
than	O
using	O
maximum	B
likelihood	B
to	O
learn	O
the	O
entries	O
of	O
the	O
cpts	O
directly	O
we	O
instead	O
learn	O
the	O
value	B
of	O
the	O
parameter	B
w	O
since	O
the	O
number	O
of	O
parameters	O
in	O
w	O
is	O
small	O
compared	O
with	O
in	O
the	O
unconstrained	O
case	O
we	O
also	O
have	O
some	O
hope	O
that	O
with	O
a	O
small	O
number	O
of	O
training	B
examples	O
we	O
can	O
learn	O
a	O
reliable	O
value	B
for	O
w	O
example	O
consider	O
the	O
following	O
variable	O
model	B
where	O
xi	O
i	O
we	O
assume	O
that	O
the	O
cpt	O
is	O
parameterised	O
using	O
e	O
one	O
may	O
verify	O
that	O
the	O
above	O
probability	O
is	O
always	O
positive	O
and	O
lies	O
between	O
and	O
due	O
to	O
normalisation	B
we	O
must	O
have	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
for	O
unrestricted	O
and	O
the	O
maximum	B
likelihood	B
setting	O
is	O
and	O
the	O
contribution	O
to	O
the	O
log	O
likelihood	B
from	O
the	O
term	O
assuming	O
i	O
i	O
d	O
data	O
is	O
i	O
i	O
xn	O
log	O
e	O
xn	O
this	O
objective	O
function	B
needs	O
to	O
be	O
optimised	O
numerically	O
to	O
find	O
the	O
best	O
and	O
the	O
gradient	B
is	O
l	O
dl	O
d	O
dl	O
d	O
xn	O
xn	O
xn	O
e	O
xn	O
e	O
xn	O
xn	O
the	O
gradient	B
can	O
be	O
used	O
as	O
part	O
of	O
a	O
standard	O
optimisation	B
procedure	O
as	O
conjugate	B
gradients	O
see	O
appendix	O
to	O
aid	O
finding	O
the	O
maximum	B
likelihood	B
parameters	O
bayesian	B
belief	B
network	I
training	B
an	O
alternative	O
to	O
maximum	B
likelihood	B
training	B
of	O
a	O
bn	O
is	O
to	O
use	O
a	O
bayesian	B
approach	B
in	O
which	O
we	O
maintain	O
a	O
distribution	B
over	O
parameters	O
we	O
continue	O
with	O
the	O
asbestos	O
smoking	O
cancer	O
scenario	O
pa	O
c	O
s	O
pca	B
spaps	O
which	O
can	O
be	O
represented	O
as	O
a	O
belief	B
network	I
so	O
far	O
we	O
ve	O
only	O
specified	O
the	O
independence	B
structure	B
but	O
not	O
the	O
entries	O
of	O
the	O
tables	O
pca	B
s	O
pa	O
ps	O
given	O
a	O
set	O
of	O
visible	B
observations	O
v	O
sn	O
cn	O
n	O
n	O
we	O
would	O
like	O
to	O
learn	O
appropriate	O
distributions	O
for	O
the	O
table	O
entries	O
to	O
begin	O
we	O
need	O
a	O
notation	O
for	O
the	O
table	O
entries	O
with	O
all	O
variables	O
binary	O
we	O
have	O
parameters	O
such	O
as	O
pa	O
a	O
a	O
pc	O
s	O
c	O
c	O
and	O
similarly	O
for	O
the	O
remaining	O
parameters	O
c	O
c	O
c	O
a	O
s	O
c	O
c	O
c	O
c	O
c	O
for	O
our	O
example	O
the	O
parameters	O
are	O
global	B
and	O
local	B
parameter	B
independence	B
in	O
bayesian	B
learning	B
of	O
bns	O
we	O
need	O
to	O
specify	O
a	O
prior	B
on	O
the	O
joint	B
table	O
entries	O
since	O
in	O
general	O
dealing	O
with	O
multi-dimensional	O
continuous	B
distributions	O
is	O
computationally	O
problematic	O
it	O
is	O
useful	O
to	O
specify	O
only	O
uni-variate	O
distributions	O
in	O
the	O
prior	B
as	O
we	O
show	O
below	O
this	O
has	O
a	O
pleasing	O
consequence	O
that	O
for	O
i	O
i	O
d	O
data	O
the	O
posterior	B
also	O
factorises	O
into	O
uni-variate	O
distributions	O
global	B
parameter	B
independence	B
a	O
convenient	O
assumption	O
is	O
that	O
the	O
prior	B
factorises	O
over	O
parameters	O
for	O
our	O
asbestos	O
smoking	O
cancer	O
example	O
we	O
assume	O
assuming	O
the	O
data	O
is	O
i	O
i	O
d	O
we	O
then	O
have	O
the	O
joint	B
model	B
p	O
a	O
s	O
c	O
p	O
ap	O
sp	O
c	O
p	O
a	O
s	O
cv	O
p	O
ap	O
sp	O
n	O
pan	O
apsn	O
spcnsn	O
an	O
c	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
a	O
an	O
s	O
sn	O
cn	O
n	O
n	O
as	O
c	O
s	O
p	O
figure	O
a	O
bayesian	B
parameter	B
model	B
for	O
the	O
relationship	O
between	O
lung	O
cancer	O
asbestos	O
exposure	O
and	O
smoking	O
with	O
factorised	B
parameter	B
priors	O
the	O
global	B
parameter	B
independence	B
assumption	O
means	O
that	O
the	O
prior	B
over	O
tables	O
factorises	O
into	O
priors	O
over	O
each	O
conditional	B
probability	I
table	O
the	O
local	B
independence	B
assumption	O
which	O
in	O
this	O
case	O
comes	O
into	O
efc	O
where	O
fect	O
only	O
for	O
pca	B
s	O
means	O
that	O
p	O
c	O
factorises	O
as	O
p	O
p	O
as	O
p	O
the	O
belief	B
network	I
for	O
which	O
is	O
given	O
in	O
learning	B
then	O
corresponds	O
to	O
inference	B
of	O
a	O
convenience	O
of	O
the	O
factorised	B
prior	B
for	O
a	O
bn	O
is	O
that	O
the	O
posterior	B
also	O
factorises	O
since	O
pv	O
a	O
s	O
cp	O
ap	O
sp	O
c	O
pv	O
p	O
a	O
s	O
cv	O
pv	O
a	O
s	O
cp	O
a	O
s	O
c	O
p	O
p	O
a	O
s	O
cv	O
p	O
a	O
s	O
cv	O
p	O
pan	O
a	O
p	O
avap	O
svsp	O
cv	O
n	O
pv	O
p	O
psn	O
s	O
n	O
n	O
pcnsn	O
an	O
c	O
so	O
that	O
one	O
can	O
consider	O
each	O
parameter	B
posterior	B
separately	O
in	O
this	O
case	O
learning	B
involves	O
computing	O
the	O
posterior	B
distributions	O
p	O
ivi	O
where	O
vi	O
is	O
the	O
set	O
of	O
training	B
data	O
restricted	B
to	O
the	O
family	B
of	O
variable	O
i	O
the	O
global	B
independence	B
assumption	O
conveniently	O
results	O
in	O
a	O
posterior	B
distribution	B
that	O
factorises	O
over	O
the	O
conditional	B
tables	O
however	O
the	O
parameter	B
c	O
is	O
itself	O
dimensional	O
to	O
simplify	O
this	O
we	O
need	O
to	O
make	O
a	O
further	O
assumption	O
as	O
to	O
the	O
structure	B
of	O
each	O
local	B
table	O
local	B
parameter	B
independence	B
if	O
we	O
further	O
assume	O
that	O
the	O
prior	B
for	O
the	O
table	O
factorises	O
over	O
all	O
states	O
a	O
c	O
p	O
c	O
p	O
c	O
c	O
c	O
c	O
then	O
the	O
posterior	B
p	O
cv	O
pv	O
cp	O
c	O
c	O
c	O
c	O
p	O
c	O
c	O
c	O
p	O
c	O
p	O
c	O
c	O
p	O
c	O
p	O
c	O
c	O
p	O
c	O
c	O
p	O
p	O
c	O
so	O
that	O
the	O
posterior	B
also	O
factorises	O
over	O
the	O
parental	O
states	O
of	O
the	O
local	B
conditional	B
table	O
posterior	B
marginal	B
table	O
a	O
marginal	B
probability	O
table	O
is	O
given	O
by	O
for	O
example	O
pc	O
s	O
draft	O
march	O
c	O
pc	O
s	O
c	O
cv	O
bayesian	B
belief	B
network	I
training	B
the	O
integral	O
over	O
all	O
the	O
other	O
tables	O
in	O
equation	B
is	O
unity	O
and	O
we	O
are	O
left	O
with	O
pc	O
s	O
pc	O
s	O
c	O
c	O
c	O
learning	B
binary	O
variable	O
tables	O
using	O
a	O
beta	B
prior	B
we	O
continue	O
the	O
example	O
of	O
where	O
all	O
variables	O
are	O
binary	O
but	O
using	O
a	O
continuous	B
valued	O
table	O
prior	B
the	O
simplest	O
case	O
is	O
to	O
start	O
with	O
pa	O
a	O
since	O
this	O
requires	O
only	O
a	O
univariate	B
prior	B
distribution	B
p	O
a	O
the	O
likelihood	B
depends	O
on	O
the	O
table	O
variable	O
via	O
pa	O
a	O
a	O
so	O
that	O
the	O
total	O
likelihood	B
term	O
is	O
a	O
the	O
posterior	B
is	O
therefore	O
p	O
ava	O
p	O
a	O
a	O
a	O
then	O
conjugacy	O
will	O
hold	O
and	O
the	O
mathematics	O
this	O
means	O
that	O
if	O
the	O
prior	B
is	O
also	O
of	O
the	O
form	O
of	O
integration	O
will	O
be	O
straightforward	O
this	O
suggests	O
that	O
the	O
most	O
convenient	O
choice	O
is	O
a	O
beta	B
distribution	B
a	O
p	O
a	O
b	O
a	O
a	O
a	O
b	O
a	O
a	O
a	O
a	O
a	O
a	O
for	O
which	O
the	O
posterior	B
is	O
also	O
a	O
beta	B
distribution	B
p	O
ava	O
b	O
a	O
a	O
a	O
the	O
marginal	B
table	O
is	O
given	O
by	O
pa	O
a	O
p	O
ava	O
a	O
a	O
a	O
a	O
using	O
the	O
result	O
for	O
the	O
mean	B
of	O
a	O
beta	B
distribution	B
the	O
situation	O
for	O
the	O
table	O
pca	B
s	O
is	O
slightly	O
more	O
complex	O
since	O
we	O
need	O
to	O
specify	O
a	O
prior	B
for	O
each	O
of	O
the	O
parental	O
tables	O
as	O
above	O
this	O
is	O
most	O
convenient	O
if	O
we	O
specify	O
a	O
beta	B
prior	B
one	O
for	O
each	O
of	O
the	O
parental	O
states	O
let	O
s	O
look	O
at	O
a	O
specific	O
table	O
pc	O
s	O
assuming	O
the	O
local	B
independence	B
property	O
we	O
have	O
p	O
ca	O
s	O
a	O
s	O
ca	O
s	O
a	O
s	O
given	O
by	O
c	O
c	O
as	O
before	O
the	O
marginal	B
probability	O
table	O
is	O
then	O
given	O
by	O
pc	O
s	O
ca	O
s	O
a	O
s	O
ca	O
s	O
ca	O
s	O
s	O
since	O
s	O
a	O
s	O
a	O
s	O
the	O
prior	B
parameters	O
ca	O
s	O
are	O
called	O
hyperparameters	O
if	O
one	O
had	O
no	O
preference	O
one	O
could	O
set	O
all	O
of	O
the	O
ca	O
s	O
to	O
be	O
equal	O
to	O
the	O
same	O
value	B
and	O
similarly	O
for	O
a	O
complete	O
ignorance	O
prior	B
would	O
correspond	O
to	O
setting	O
see	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
no	O
data	O
limit	O
n	O
in	O
the	O
limit	O
of	O
no	O
data	O
the	O
marginal	B
probability	O
table	O
corresponds	O
to	O
the	O
prior	B
which	O
is	O
given	O
in	O
this	O
case	O
by	O
pc	O
s	O
ca	O
s	O
ca	O
s	O
ca	O
s	O
for	O
a	O
flat	O
prior	B
for	O
all	O
states	O
a	O
c	O
this	O
would	O
give	O
a	O
prior	B
probability	O
of	O
pc	O
s	O
infinite	O
data	O
limit	O
n	O
in	O
this	O
limit	O
the	O
marginal	B
probability	O
tables	O
are	O
dominated	O
by	O
the	O
data	O
counts	O
since	O
these	O
will	O
typically	O
grow	O
in	O
proportion	O
to	O
the	O
size	O
of	O
the	O
dataset	O
this	O
means	O
that	O
in	O
the	O
infinite	O
very	O
large	O
data	O
limit	O
pc	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
which	O
corresponds	O
to	O
the	O
maximum	B
likelihood	B
solution	O
this	O
effect	O
that	O
the	O
large	O
data	O
limit	O
of	O
a	O
bayesian	B
procedure	O
corresponds	O
to	O
the	O
maximum	B
likelihood	B
solution	O
is	O
general	O
unless	O
the	O
prior	B
has	O
a	O
pathologically	O
strong	B
effect	O
example	O
consider	O
the	O
binary	O
variable	O
network	O
pc	O
a	O
s	O
pca	B
spaps	O
the	O
data	O
v	O
is	O
given	O
in	O
using	O
a	O
flat	O
beta	B
prior	B
for	O
all	O
conditional	B
probability	I
tables	O
the	O
marginal	B
posterior	B
tables	O
are	O
given	O
by	O
pa	O
n	O
by	O
comparison	O
the	O
maximum	B
likelihood	B
setting	O
is	O
the	O
bayesian	B
result	O
is	O
a	O
little	O
more	O
cautious	O
than	O
the	O
maximum	B
likelihood	B
which	O
squares	O
with	O
our	O
prior	B
belief	O
that	O
any	O
setting	O
of	O
the	O
probability	O
is	O
equally	O
likely	O
pulling	O
the	O
posterior	B
towards	O
similarly	O
ps	O
n	O
and	O
pc	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
a	O
s	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
learning	B
multivariate	B
discrete	B
tables	O
using	O
a	O
dirichlet	B
prior	B
the	O
natural	B
generalisation	B
to	O
more	O
than	O
two-state	O
variables	O
is	O
given	O
by	O
using	O
a	O
dirichlet	B
prior	B
again	O
assuming	O
i	O
i	O
d	O
data	O
and	O
the	O
local	B
and	O
global	B
parameter	B
prior	B
independencies	O
since	O
under	O
the	O
global	B
parameter	B
independence	B
assumption	O
the	O
posterior	B
factorises	O
over	O
variables	O
in	O
equation	B
we	O
can	O
concentrate	O
on	O
the	O
posterior	B
of	O
a	O
single	O
variable	O
no	O
parents	B
let	O
s	O
consider	O
the	O
contribution	O
of	O
a	O
variable	O
v	O
with	O
domv	O
i	O
the	O
contribution	O
to	O
the	O
posterior	B
from	O
a	O
datapoint	O
vn	O
is	O
ivni	O
i	O
i	O
pvn	O
p	O
so	O
that	O
the	O
posterior	B
is	O
proportional	O
to	O
i	O
ivni	O
ivni	O
i	O
p	O
for	O
a	O
dirichlet	B
prior	B
distribution	B
with	O
hyperparameters	O
u	O
p	O
ui	O
i	O
using	O
this	O
prior	B
the	O
posterior	B
becomes	O
i	O
ui	O
i	O
ivni	O
ui	O
ivni	O
i	O
p	O
which	O
means	O
that	O
the	O
posterior	B
is	O
given	O
by	O
p	O
dirichlet	B
c	O
where	O
c	O
is	O
a	O
count	O
vector	O
with	O
components	O
ci	O
i	O
i	O
being	O
the	O
number	O
of	O
times	O
state	O
i	O
was	O
observed	O
in	O
the	O
training	B
data	O
the	O
marginal	B
table	O
is	O
given	O
by	O
integrating	O
pv	O
iv	O
pv	O
i	O
ip	O
iv	O
i	O
pv	O
iv	O
ui	O
ci	O
j	O
uj	O
cj	O
since	O
the	O
single-variable	O
marginal	B
distribution	B
of	O
a	O
dirichlet	B
is	O
a	O
beta	B
distribution	B
the	O
marginal	B
table	O
is	O
the	O
mean	B
of	O
a	O
beta	B
distribution	B
given	O
that	O
the	O
marginal	B
p	O
is	O
beta	B
distribution	B
with	O
parameters	O
ui	O
ci	O
uj	O
cj	O
the	O
marginal	B
table	O
is	O
given	O
by	O
which	O
generalises	O
the	O
binary	O
state	O
formula	O
equation	B
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
parents	B
to	O
deal	O
with	O
the	O
general	O
case	O
of	O
a	O
variable	O
v	O
with	O
parents	B
pa	O
we	O
denote	O
the	O
probability	O
of	O
v	O
being	O
in	O
state	O
i	O
conditioned	O
on	O
the	O
parents	B
being	O
in	O
state	O
j	O
as	O
pv	O
ipa	O
j	O
iv	O
j	O
the	O
number	O
of	O
states	O
j	O
will	O
be	O
exponential	B
in	O
k	O
i	O
iv	O
j	O
this	O
forms	O
the	O
components	O
of	O
a	O
vector	O
j	O
note	O
that	O
if	O
v	O
has	O
k	O
parents	B
then	O
local	B
independence	B
means	O
p	O
j	O
p	O
p	O
j	O
p	O
and	O
global	B
independence	B
means	O
v	O
where	O
v	O
v	O
represents	O
the	O
combined	O
table	O
of	O
all	O
the	O
variables	O
we	O
drop	O
the	O
explicit	O
sans-serif	O
font	O
on	O
the	O
states	O
from	O
here	O
on	O
in	O
parameter	B
posterior	B
thanks	O
to	O
the	O
global	B
parameter	B
independence	B
the	O
posterior	B
distribution	B
over	O
the	O
tables	O
factorises	O
with	O
one	O
posterior	B
table	O
per	O
variable	O
each	O
posterior	B
table	O
for	O
a	O
variable	O
v	O
depends	O
only	O
on	O
the	O
information	O
local	B
to	O
the	O
family	B
of	O
each	O
variable	O
dv	O
assuming	O
a	O
dirichlet	B
distribution	B
prior	B
p	O
j	O
dirichlet	B
juv	O
j	O
the	O
posterior	B
is	O
proportional	O
to	O
the	O
joint	B
distribution	B
p	O
p	O
zuv	O
j	O
i	O
zuv	O
j	O
i	O
j	O
j	O
iv	O
juivj	O
iv	O
jivnipavnj	O
n	O
j	O
i	O
iv	O
juivj	O
where	O
zu	O
is	O
the	O
normalisation	B
constant	I
of	O
a	O
dirichlet	B
distribution	B
hence	O
the	O
posterior	B
is	O
p	O
j	O
where	O
the	O
hyperparameter	B
prior	B
term	O
is	O
updated	O
by	O
the	O
observed	O
counts	O
iv	O
j	O
uiv	O
j	O
i	O
pa	O
j	O
u	O
by	O
analogy	O
with	O
the	O
no-parents	O
case	O
the	O
marginal	B
table	O
is	O
given	O
by	O
the	O
states	O
explicitly	O
iv	O
j	O
pv	O
ipa	O
jdv	O
u	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
a	O
s	O
c	O
figure	O
a	O
database	O
of	O
patient	O
records	O
about	O
the	O
asbestos	O
exposure	O
signifies	O
exposure	O
being	O
a	O
smoker	O
signifies	O
the	O
individual	O
is	O
a	O
smoker	O
and	O
lung	O
cancer	O
signifies	O
no	O
cancer	O
signifies	O
early	O
stage	O
cancer	O
signifies	O
late	O
state	O
cancer	O
each	O
row	O
contains	O
the	O
information	O
for	O
an	O
individual	O
so	O
that	O
there	O
are	O
individuals	O
in	O
the	O
database	O
example	O
consider	O
the	O
pca	B
spspa	O
asbestos	O
example	O
with	O
doma	O
doms	O
except	O
now	O
with	O
the	O
variable	O
c	O
taking	O
three	O
states	O
domc	O
accounting	O
for	O
different	O
kinds	O
of	O
cancer	O
the	O
marginal	B
table	O
under	O
a	O
dirichlet	B
prior	B
is	O
then	O
given	O
by	O
pc	O
s	O
s	O
a	O
s	O
i	O
uia	O
s	O
i	O
a	O
s	O
assuming	O
a	O
flat	O
dirichlet	B
prior	B
which	O
corresponds	O
to	O
setting	O
all	O
components	O
of	O
u	O
to	O
this	O
gives	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
and	O
similarly	O
for	O
the	O
other	O
three	O
tables	O
pca	B
s	O
pca	B
s	O
pca	B
s	O
model	B
likelihood	B
for	O
a	O
belief	B
network	I
m	O
the	O
joint	B
probability	O
of	O
all	O
variables	O
factorises	O
into	O
the	O
local	B
probabilities	O
of	O
each	O
variable	O
conditioned	O
on	O
its	O
parents	B
pvm	O
pdm	O
v	O
for	O
i	O
i	O
d	O
data	O
d	O
the	O
likelihood	B
under	O
the	O
network	O
m	O
is	O
pvpa	O
m	O
pvnpa	O
m	O
v	O
n	O
v	O
j	O
j	O
zuv	O
j	O
where	O
u	O
are	O
the	O
dirichlet	B
hyperparameters	O
and	O
is	O
given	O
by	O
equation	B
expression	O
can	O
be	O
written	O
explicitly	O
in	O
terms	O
of	O
gamma	B
functions	O
see	O
in	O
the	O
above	O
expression	O
in	O
general	O
the	O
number	O
of	O
parental	O
states	O
differs	O
for	O
each	O
variable	O
v	O
so	O
that	O
implicit	O
in	O
the	O
above	O
formula	O
is	O
that	O
the	O
state	O
product	O
over	O
j	O
goes	O
from	O
to	O
the	O
number	O
of	O
parental	O
states	O
of	O
variable	O
v	O
due	O
to	O
the	O
local	B
and	O
global	B
parameter	B
independence	B
assumptions	O
the	O
logarithm	O
of	O
the	O
model	B
likelihood	B
splits	O
into	O
terms	O
one	O
for	O
each	O
variable	O
v	O
and	O
parental	O
configuration	O
this	O
is	O
called	O
the	O
likelihood	B
decomposable	B
property	O
structure	B
learning	B
up	O
to	O
this	O
point	O
we	O
have	O
assumed	O
that	O
we	O
are	O
given	O
both	O
the	O
structure	B
of	O
the	O
distribution	B
and	O
a	O
dataset	O
d	O
a	O
more	O
complex	O
task	O
is	O
when	O
we	O
need	O
to	O
learn	O
the	O
structure	B
of	O
the	O
network	O
as	O
well	O
we	O
ll	O
consider	O
the	O
case	O
in	O
which	O
the	O
data	O
is	O
complete	O
there	O
are	O
no	O
missing	B
observations	O
since	O
for	O
d	O
variables	O
there	O
is	O
an	O
exponentially	O
large	O
number	O
d	O
of	O
bn	O
structures	O
it	O
s	O
clear	O
that	O
we	O
cannot	O
search	O
over	O
all	O
possible	O
structures	O
for	O
this	O
reason	O
structure	B
learning	B
is	O
a	O
computationally	O
challenging	O
problem	B
and	O
we	O
must	O
rely	O
on	O
constraints	O
and	O
heuristics	O
to	O
help	O
guide	O
the	O
search	O
furthermore	O
for	O
all	O
but	O
the	O
sparsest	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
algorithm	B
pc	B
algorithm	B
for	O
skeleton	B
learning	B
start	O
with	O
a	O
complete	O
undirected	B
graph	B
g	O
on	O
the	O
set	O
v	O
of	O
all	O
vertices	O
i	O
repeat	O
for	O
x	O
v	O
do	O
for	O
y	O
adj	O
do	O
determine	O
if	O
there	O
a	O
subset	O
s	O
of	O
size	O
i	O
of	O
the	O
neighbours	O
of	O
x	O
including	O
y	O
for	O
which	O
x	O
ys	O
if	O
this	O
set	O
exists	O
remove	O
the	O
x	O
y	O
link	O
from	O
the	O
graph	B
g	O
and	O
set	O
sxy	O
s	O
end	O
for	O
end	O
for	O
i	O
i	O
until	O
all	O
nodes	O
have	O
i	O
neighbours	O
networks	O
estimating	O
the	O
dependencies	O
to	O
any	O
accuracy	O
requires	O
a	O
large	O
amount	O
of	O
data	O
making	O
testing	O
of	O
dependencies	O
difficult	O
indeed	O
for	O
a	O
finite	O
amount	O
of	O
data	O
two	O
variables	O
will	O
always	O
have	O
non-zero	O
mutual	B
information	I
so	O
that	O
a	O
threshold	O
needs	O
to	O
be	O
set	O
to	O
decide	O
if	O
the	O
measured	O
dependence	O
is	O
significant	O
under	O
the	O
finite	O
sample	O
see	O
other	O
complexities	O
arise	O
from	O
the	O
concern	O
that	O
a	O
belief	O
or	O
markov	B
network	I
on	O
the	O
visible	B
variables	O
alone	O
may	O
not	O
be	O
a	O
parsimonious	O
way	O
to	O
represent	O
the	O
observed	O
data	O
if	O
for	O
example	O
there	O
may	O
be	O
latent	B
variables	O
which	O
are	O
driving	O
the	O
observed	O
dependencies	O
for	O
these	O
reasons	O
we	O
will	O
not	O
discuss	O
this	O
topic	O
in	O
detail	O
here	O
and	O
limit	O
the	O
discussion	O
to	O
two	O
central	O
approaches	O
a	O
special	O
case	O
that	O
is	O
computationally	O
tractable	O
is	O
when	O
the	O
network	O
is	O
constrained	O
to	O
have	O
at	O
most	O
one	O
parent	O
we	O
defer	O
discussion	O
of	O
this	O
to	O
pc	B
algorithm	B
the	O
pc	O
first	O
learns	O
the	O
skeleton	B
of	O
a	O
graph	B
after	O
which	O
edges	O
may	O
be	O
oriented	O
to	O
form	O
a	O
oriented	O
dag	O
the	O
pc	B
algorithm	B
begins	O
at	O
the	O
first	O
round	O
with	O
a	O
complete	O
skeleton	B
g	O
and	O
attempts	O
to	O
remove	O
as	O
many	O
links	O
as	O
possible	O
at	O
the	O
first	O
step	O
we	O
test	O
all	O
pairs	O
x	O
y	O
if	O
an	O
x	O
and	O
y	O
pair	O
are	O
deemed	O
independent	O
then	O
the	O
link	O
x	O
y	O
is	O
removed	O
from	O
the	O
complete	O
graph	B
one	O
repeats	O
this	O
for	O
all	O
the	O
pairwise	B
links	O
in	O
the	O
second	O
round	O
for	O
the	O
remaining	O
graph	B
one	O
examines	O
each	O
x	O
y	O
link	O
and	O
conditions	O
on	O
a	O
single	O
neighbour	B
z	O
of	O
x	O
if	O
x	O
y	O
z	O
then	O
remove	O
the	O
link	O
x	O
y	O
one	O
repeats	O
in	O
this	O
way	O
through	O
all	O
the	O
variables	O
at	O
each	O
round	O
the	O
number	O
of	O
neighbours	O
in	O
the	O
conditioning	B
set	O
is	O
increased	O
by	O
one	O
see	O
and	O
demopcoracle	O
m	O
a	O
refinement	O
of	O
this	O
algorithm	B
known	O
as	O
npc	O
for	O
necessary	O
path	B
attempts	O
to	O
limit	O
the	O
number	O
of	O
independence	B
checks	O
which	O
may	O
otherwise	O
result	O
in	O
inconsistencies	O
due	O
to	O
the	O
empirical	B
estimates	O
of	O
conditional	B
mutual	B
information	I
given	O
a	O
learned	O
skeleton	B
a	O
partial	O
dag	O
can	O
be	O
constructed	O
using	O
note	O
that	O
this	O
is	O
necessary	O
since	O
the	O
undirected	B
graph	B
g	O
is	O
a	O
skeleton	B
not	O
a	O
belief	B
network	I
of	O
the	O
independence	B
assumptions	O
discovered	O
for	O
example	O
we	O
may	O
have	O
a	O
graph	B
g	O
with	O
x	O
z	O
y	O
in	O
which	O
the	O
x	O
y	O
link	O
was	O
removed	O
on	O
the	O
basis	O
x	O
y	O
sxy	O
as	O
a	O
mn	O
the	O
graph	B
x	O
z	O
y	O
implies	O
although	O
this	O
is	O
inconsistent	O
with	O
the	O
discovery	O
in	O
the	O
first	O
round	O
x	O
y	O
this	O
is	O
the	O
reason	O
for	O
the	O
orientation	O
part	O
for	O
consistency	O
we	O
must	O
have	O
x	O
z	O
y	O
for	O
which	O
x	O
y	O
and	O
z	O
note	O
that	O
in	O
we	O
have	O
for	O
the	O
unmarried	O
collider	B
test	O
z	O
which	O
in	O
this	O
case	O
is	O
true	O
resulting	O
in	O
a	O
collider	B
forming	O
see	O
also	O
example	O
orienting	O
z	O
x	O
y	O
x	O
y	O
z	O
x	O
y	O
if	O
x	O
is	O
independent	O
of	O
y	O
it	O
must	O
be	O
that	O
z	O
is	O
a	O
collider	B
since	O
otherwise	O
marginalising	O
over	O
z	O
would	O
introduce	O
a	O
dependence	O
between	O
x	O
and	O
y	O
example	O
appears	O
in	O
and	O
thanks	O
also	O
to	O
seraf	O
n	O
moral	O
for	O
his	O
online	B
notes	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
x	O
z	O
x	O
z	O
t	O
t	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
t	O
t	O
t	O
t	O
t	O
t	O
t	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
t	O
t	O
t	O
t	O
t	O
t	O
t	O
y	O
w	O
y	O
w	O
t	O
t	O
the	O
bn	O
from	O
which	O
data	O
is	O
assumed	O
generated	O
and	O
against	O
which	O
figure	O
pc	B
algorithm	B
conditional	B
independence	B
tests	O
will	O
be	O
performed	O
the	O
initial	O
skeleton	B
is	O
fully	O
connected	B
in	O
the	O
first	O
round	O
all	O
the	O
pairwise	B
mutual	O
informations	O
x	O
y	O
are	O
checked	O
and	O
the	O
link	O
between	O
x	O
i	O
we	O
now	O
look	O
at	O
connected	B
subsets	O
on	O
and	O
y	O
removed	O
if	O
deemed	O
independent	O
line	O
the	O
three	O
variables	O
x	O
y	O
z	O
of	O
the	O
remaining	O
graph	B
removing	O
the	O
link	O
x	O
y	O
if	O
x	O
y	O
z	O
is	O
true	O
not	O
all	O
steps	O
i	O
we	O
now	O
examine	O
all	O
x	O
ya	O
b	O
the	O
algorithm	B
terminates	O
after	O
this	O
round	O
are	O
shown	O
final	O
skeleton	B
i	O
gets	O
incremented	O
to	O
since	O
there	O
are	O
no	O
nodes	O
with	O
or	O
more	O
neighbours	O
during	O
this	O
process	O
the	O
sets	O
sxy	O
sxw	O
szw	O
y	O
sxt	O
w	O
syt	O
w	O
were	O
found	O
see	O
also	O
demopcoracle	O
m	O
z	O
x	O
y	O
x	O
y	O
z	O
z	O
x	O
y	O
if	O
x	O
is	O
independent	O
of	O
y	O
conditioned	O
on	O
z	O
z	O
must	O
not	O
be	O
a	O
collider	B
any	O
other	O
orientation	O
is	O
appropriate	O
empirical	B
independence	B
given	O
a	O
data	O
set	O
d	O
containing	O
variables	O
x	O
y	O
z	O
our	O
interest	O
is	O
to	O
measure	O
if	O
x	O
y	O
z	O
one	O
approach	B
is	O
to	O
use	O
the	O
conditional	B
mutual	B
information	I
which	O
is	O
the	O
average	B
of	O
conditional	B
kullback-leibler	O
divergences	O
definition	O
information	O
mix	O
yz	O
where	O
this	O
expression	O
is	O
equally	O
valid	O
for	O
sets	O
of	O
variables	O
if	O
x	O
y	O
z	O
is	O
true	O
then	O
mix	O
yz	O
is	O
zero	O
and	O
vice	O
versa	O
when	O
z	O
the	O
average	B
over	O
pz	O
is	O
absent	O
and	O
one	O
writes	O
mix	O
y	O
given	O
data	O
we	O
can	O
obtain	O
an	O
estimate	O
of	O
the	O
conditional	B
mutual	B
information	I
by	O
using	O
the	O
empirical	B
distribution	B
px	O
y	O
z	O
estimated	O
by	O
simply	O
counting	B
occurrences	O
in	O
the	O
data	O
in	O
practice	O
however	O
we	O
only	O
have	O
a	O
finite	O
amount	O
of	O
data	O
to	O
estimate	O
the	O
empirical	B
distribution	B
so	O
that	O
for	O
data	O
sampled	O
from	O
distribution	B
for	O
which	O
the	O
variables	O
truly	O
are	O
independent	O
the	O
empirical	B
mutual	B
information	I
will	O
typically	O
be	O
greater	O
than	O
zero	O
an	O
issue	O
therefore	O
is	O
what	O
threshold	O
to	O
use	O
for	O
the	O
empirical	B
conditional	B
mutual	B
information	I
to	O
decide	O
if	O
this	O
is	O
sufficiently	O
far	O
from	O
zero	O
to	O
be	O
caused	O
by	O
dependence	O
a	O
frequentist	B
approach	B
is	O
to	O
compute	O
the	O
distribution	B
of	O
the	O
conditional	B
mutual	B
information	I
and	O
then	O
see	O
where	O
the	O
sample	O
value	B
is	O
compared	O
to	O
the	O
distribution	B
according	O
to	O
yz	O
is	O
chi-square	O
distributed	O
draft	O
march	O
bayesian	B
belief	B
network	I
training	B
algorithm	B
skeleton	B
orientation	O
algorithm	B
a	O
dag	O
unmarried	O
collider	B
examine	O
all	O
undirected	B
links	O
x	O
z	O
y	O
if	O
z	O
sxy	O
set	O
x	O
z	O
y	O
repeat	O
until	O
no	O
more	O
edges	O
can	O
be	O
oriented	O
the	O
remaining	O
edges	O
can	O
be	O
arbitrarily	O
oriented	O
provided	O
that	O
the	O
graph	B
remains	O
a	O
dag	O
and	O
no	O
additional	O
x	O
z	O
y	O
x	O
z	O
y	O
for	O
x	O
y	O
if	O
there	O
is	O
a	O
directed	B
path	B
from	O
x	O
to	O
y	O
orient	O
x	O
y	O
if	O
for	O
x	O
z	O
y	O
there	O
is	O
a	O
w	O
such	O
that	O
x	O
w	O
y	O
w	O
z	O
w	O
then	O
orient	O
z	O
w	O
colliders	O
are	O
introduced	O
with	O
degrees	O
of	O
freedom	O
although	O
this	O
test	O
does	O
not	O
work	O
well	O
in	O
the	O
case	O
of	O
small	O
amounts	O
of	O
data	O
an	O
alternative	O
pragmatic	O
approach	B
is	O
to	O
estimate	O
the	O
threshold	O
based	O
on	O
empirical	B
samples	O
of	O
the	O
mi	O
under	O
controlled	O
independentdependent	O
conditions	O
see	O
democondindepemp	O
m	O
for	O
a	O
comparison	O
of	O
these	O
approaches	O
bayesian	B
conditional	B
independence	B
test	O
a	O
bayesian	B
approach	B
to	O
testing	O
for	O
independence	B
can	O
be	O
made	O
by	O
comparing	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
the	O
independence	B
hypothesis	O
versus	O
the	O
likelihood	B
under	O
the	O
dependent	O
hypothesis	O
for	O
the	O
independence	B
hypothesis	O
we	O
have	O
a	O
joint	B
distribution	B
over	O
variables	O
and	O
parameters	O
px	O
y	O
z	O
pxz	O
xzpyz	O
yzpz	O
zp	O
xzp	O
yzp	O
z	O
for	O
categorical	B
distributions	O
it	O
is	O
convenient	O
to	O
use	O
a	O
prior	B
dirichlet	B
on	O
the	O
parameters	O
assuming	O
also	O
local	B
as	O
well	O
as	O
global	B
parameter	B
independence	B
for	O
a	O
set	O
of	O
assumed	O
i	O
i	O
d	O
data	O
yn	O
zn	O
n	O
n	O
the	O
likelihood	B
is	O
then	O
given	O
by	O
integratingover	O
the	O
parameters	O
px	O
zuz	O
zuz	O
zuxz	O
z	O
zuyz	O
z	O
zuxz	O
z	O
zuyz	O
where	O
uxz	O
is	O
a	O
hyperparameter	B
matrix	B
of	O
pseudo	B
counts	O
for	O
each	O
state	O
of	O
x	O
given	O
each	O
state	O
of	O
z	O
zv	O
is	O
the	O
normalisation	B
constant	I
of	O
a	O
dirichlet	B
distribution	B
with	O
vector	O
parameter	B
v	O
for	O
the	O
dependent	O
hypothesis	O
we	O
have	O
px	O
y	O
z	O
px	O
y	O
z	O
xyzp	O
xyz	O
the	O
likelihood	B
is	O
then	O
px	O
zuxyz	O
y	O
z	O
zuxyz	O
x	O
z	O
y	O
x	O
y	O
x	O
y	O
x	O
w	O
z	O
w	O
z	O
w	O
z	O
y	O
w	O
t	O
t	O
t	O
t	O
figure	O
skeleton	B
orientation	O
algorithm	B
the	O
skeleton	B
along	O
with	O
sxy	O
sxw	O
szw	O
z	O
sxy	O
y	O
sxt	O
w	O
syt	O
w	O
t	O
szw	O
so	O
form	O
collider	B
so	O
form	O
collider	B
final	O
partially	O
oriented	O
dag	O
the	O
remaining	O
edge	O
may	O
be	O
oriented	O
as	O
desired	O
without	O
violating	O
the	O
dag	O
condition	O
see	O
also	O
demopcoracle	O
m	O
draft	O
march	O
px	O
pxn	O
yn	O
zn	O
thanks	O
to	O
conjugacy	O
this	O
is	O
straightforward	O
and	O
gives	O
the	O
expression	O
n	O
bayesian	B
belief	B
network	I
training	B
z	O
zn	O
xn	O
yn	O
n	O
xz	O
yz	O
xyz	O
xn	O
yn	O
zn	O
n	O
figure	O
bayesian	B
conditional	B
independence	B
test	O
a	O
using	O
dirichlet	B
priors	O
on	O
the	O
tables	O
model	B
hindep	O
for	O
conditional	B
independence	B
x	O
y	O
a	O
model	B
hdep	O
for	O
conditional	B
depenz	O
dence	O
z	O
by	O
computing	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
each	O
model	B
a	O
numerical	B
score	O
for	O
the	O
whether	O
the	O
data	O
is	O
more	O
consistent	B
with	O
the	O
conditional	B
independence	B
assumption	O
can	O
be	O
formed	O
see	O
democondindepemp	O
m	O
figure	O
conditional	B
independence	B
test	O
of	O
x	O
y	O
with	O
x	O
y	O
having	O
states	O
respectively	O
from	O
the	O
oracle	O
belief	B
network	I
shown	O
in	O
each	O
experiment	O
the	O
tables	O
are	O
drawn	O
at	O
random	O
and	O
examples	O
are	O
sampled	O
to	O
form	O
a	O
dataset	O
for	O
each	O
dataset	O
a	O
test	O
is	O
carried	O
out	O
to	O
determine	O
if	O
x	O
and	O
y	O
are	O
independent	O
conditioned	O
on	O
correct	O
answer	O
being	O
that	O
they	O
are	O
independent	O
over	O
experiments	O
the	O
bayesian	B
conditional	B
independence	B
test	O
correctly	O
states	O
that	O
the	O
variables	O
are	O
conditionally	O
independent	O
of	O
the	O
time	O
compared	O
with	O
only	O
accuracy	O
using	O
the	O
chi-square	O
mutual	B
information	I
test	O
see	O
democondindepemp	O
m	O
assuming	O
each	O
hypothesis	O
is	O
equally	O
likely	O
for	O
a	O
bayes	O
factor	B
px	O
px	O
greater	O
than	O
we	O
assume	O
that	O
conditional	B
independence	B
holds	O
otherwise	O
we	O
assume	O
the	O
variables	O
are	O
conditionally	O
dependent	O
democondindepemp	O
m	O
suggests	O
that	O
the	O
bayesisan	O
hypothesis	O
test	O
tends	O
to	O
outperform	O
the	O
conditional	B
mutual	B
information	I
approach	B
particularly	O
in	O
the	O
small	O
sample	O
size	O
case	O
see	O
network	B
scoring	I
an	O
alternative	O
to	O
local	B
methods	O
such	O
as	O
the	O
pc	B
algorithm	B
is	O
to	O
evaluate	O
the	O
whole	O
network	O
structure	B
in	O
a	O
probabilistic	B
context	O
given	O
a	O
model	B
structure	B
m	O
we	O
wish	O
to	O
compute	O
pmd	O
pdmpm	O
some	O
care	O
is	O
needed	O
here	O
since	O
we	O
have	O
to	O
first	O
fit	O
each	O
model	B
with	O
parameters	O
pv	O
m	O
to	O
the	O
data	O
d	O
if	O
we	O
do	O
this	O
using	O
maximum	B
likelihood	B
alone	O
with	O
no	O
constraints	O
on	O
we	O
will	O
always	O
end	O
up	O
favouring	O
that	O
model	B
m	O
with	O
the	O
most	O
complex	O
structure	B
pm	O
const	O
this	O
can	O
be	O
remedied	O
by	O
using	O
the	O
bayesian	B
technique	O
pdm	O
pd	O
mp	O
in	O
the	O
case	O
of	O
directed	B
networks	O
however	O
as	O
we	O
saw	O
in	O
the	O
assumptions	O
of	O
local	B
and	O
global	B
parameter	B
independence	B
make	O
the	O
integrals	O
tractable	O
for	O
a	O
discrete	B
state	O
network	O
and	O
dirichlet	B
priors	O
we	O
have	O
pdm	O
given	O
explicitly	O
by	O
the	O
bayesian	B
dirichlet	B
score	I
equation	B
first	O
we	O
specify	O
the	O
hyperparameters	O
uv	O
j	O
and	O
then	O
search	O
over	O
structures	O
m	O
to	O
find	O
the	O
one	O
with	O
the	O
best	O
score	O
pdm	O
the	O
simplest	O
setting	O
for	O
the	O
hyperparameters	O
is	O
set	O
them	O
all	O
to	O
another	O
setting	O
is	O
the	O
uninformative	O
prior	B
uiv	O
j	O
dim	O
v	O
dim	O
pa	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
the	O
correct	O
structure	B
in	O
which	O
all	O
figure	O
learning	B
the	O
structure	B
of	O
a	O
bayesian	B
network	O
variables	O
are	O
binary	O
the	O
ancestral	B
order	I
is	O
the	O
dataset	O
is	O
formed	O
from	O
samples	O
the	O
learned	O
structure	B
based	O
on	O
the	O
pc	B
algorithm	B
using	O
the	O
bayesian	B
empirical	B
from	O
this	O
network	O
conditional	B
independence	B
test	O
undirected	B
edges	O
may	O
be	O
oriented	O
arbitrarily	O
the	O
learned	O
structure	B
based	O
on	O
the	O
bayes	O
dirichlet	B
network	B
scoring	I
method	O
see	O
demopcdata	O
m	O
and	O
demobdscore	O
m	O
where	O
dim	O
x	O
is	O
the	O
number	O
of	O
states	O
of	O
the	O
variables	O
x	O
giving	O
rise	O
to	O
the	O
bdeu	B
score	I
for	O
an	O
equivalent	B
sample	O
size	O
parameter	B
a	O
discussion	O
of	O
these	O
settings	O
is	O
given	O
in	O
under	O
the	O
concept	O
of	O
likelihood	B
equivalence	O
namely	O
that	O
two	O
networks	O
which	O
are	O
markov	B
equivalent	B
should	O
have	O
the	O
same	O
score	O
how	O
dense	O
the	O
resulting	O
network	O
is	O
can	O
be	O
sensitive	O
to	O
including	O
an	O
explicit	O
prior	B
pm	O
on	O
the	O
networks	O
to	O
favour	O
those	O
with	O
sparse	B
connections	O
is	O
also	O
a	O
sensible	O
idea	O
for	O
which	O
one	O
modified	O
the	O
score	O
to	O
pdmpm	O
searching	O
over	O
structures	O
is	O
a	O
computationally	O
demanding	O
task	O
however	O
since	O
the	O
log-score	O
decomposes	O
into	O
terms	O
involving	O
each	O
family	B
of	O
v	O
we	O
can	O
compare	O
two	O
networks	O
differing	O
in	O
a	O
single	O
arc	O
efficiently	O
search	O
heuristics	O
based	O
on	O
local	B
additionremovalreversal	O
of	O
links	O
that	O
increase	O
the	O
score	O
are	O
in	O
learnbayesnet	O
m	O
we	O
simplify	O
the	O
problem	B
for	O
demonstration	O
purposes	O
in	O
which	O
we	O
assume	O
we	O
know	O
the	O
ancestral	B
order	I
of	O
the	O
variables	O
and	O
also	O
the	O
maximal	O
number	O
of	O
parents	B
of	O
each	O
variable	O
example	O
algorithm	B
versus	O
network	B
scoring	I
in	O
we	O
compare	O
the	O
pc	B
algorithm	B
with	O
bd	O
network	B
scoring	I
based	O
dirichlet	B
hyperparameters	O
set	O
to	O
unity	O
on	O
samples	O
from	O
a	O
known	O
belief	B
network	I
the	O
pc	B
algorithm	B
conditional	B
independence	B
test	O
is	O
based	O
on	O
the	O
bayesian	B
factor	B
in	O
which	O
dirichlet	B
priors	O
with	O
were	O
used	O
throughout	O
in	O
the	O
network	B
scoring	I
technique	O
outperforms	O
the	O
pc	B
algorithm	B
this	O
is	O
partly	O
explained	O
by	O
the	O
network	B
scoring	I
technique	O
being	O
provided	O
with	O
the	O
correct	O
ancestral	B
order	I
and	O
the	O
constraint	O
that	O
each	O
variable	O
has	O
maximally	O
two	O
parents	B
maximum	B
likelihood	B
for	O
undirected	B
models	O
consider	O
a	O
markov	B
network	I
distribution	B
px	O
defined	O
on	O
necessarily	O
maximal	O
cliques	O
xc	O
x	O
px	O
z	O
cxc	O
c	O
c	O
where	O
z	O
x	O
c	O
cxc	O
c	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
ensures	O
normalisation	B
given	O
a	O
set	O
of	O
data	O
x	O
n	O
n	O
n	O
and	O
assuming	O
i	O
i	O
d	O
data	O
the	O
log	O
likelihood	B
is	O
log	O
cx	O
n	O
c	O
c	O
n	O
log	O
z	O
l	O
n	O
c	O
in	O
general	O
learning	B
the	O
optimal	O
parameters	O
c	O
c	O
c	O
is	O
awkward	O
since	O
they	O
are	O
coupled	B
via	O
z	O
unlike	O
the	O
bn	O
the	O
objective	O
function	B
does	O
not	O
split	O
into	O
a	O
set	O
of	O
isolated	O
parameter	B
terms	O
and	O
in	O
general	O
we	O
need	O
to	O
resort	O
to	O
numerical	B
methods	O
in	O
special	O
cases	O
however	O
exact	O
results	O
still	O
apply	O
in	O
particular	O
when	O
the	O
mn	O
is	O
decomposable	B
and	O
no	O
constraints	O
are	O
placed	O
on	O
the	O
form	O
of	O
the	O
clique	B
potentials	O
as	O
we	O
discuss	O
in	O
more	O
generally	O
however	O
gradient	B
based	O
techniques	O
may	O
be	O
used	O
and	O
also	O
give	O
insight	O
into	O
properties	B
of	O
the	O
maximum	B
likelihood	B
solution	O
the	O
likelihood	B
gradient	B
l	O
c	O
c	O
log	O
z	O
where	O
we	O
used	O
the	O
result	O
c	O
c	O
n	O
cxc	O
c	O
c	O
n	O
log	O
cx	O
n	O
c	O
z	O
x	O
log	O
cxc	O
c	O
pxc	O
log	O
cxc	O
c	O
c	O
pxc	O
the	O
gradient	B
can	O
then	O
be	O
used	O
as	O
part	O
of	O
a	O
standard	O
numerical	B
optimisation	B
package	O
exponential	B
form	O
potentials	O
a	O
common	O
form	O
of	O
parameterisation	B
is	O
to	O
use	O
an	O
exponential	B
form	O
cxc	O
e	O
t	O
cxc	O
where	O
are	O
the	O
parameters	O
and	O
cxc	O
is	O
a	O
fixed	O
feature	O
function	B
defined	O
on	O
the	O
variables	O
of	O
clique	B
c	O
differentiating	O
with	O
respect	O
to	O
and	O
equating	O
to	O
zero	O
we	O
obtain	O
that	O
the	O
maximum	B
likelihood	B
solution	O
satisfies	O
that	O
the	O
empirical	B
average	B
of	O
a	O
feature	O
function	B
matches	O
the	O
average	B
of	O
the	O
feature	O
function	B
with	O
respect	O
to	O
the	O
model	B
n	O
where	O
is	O
the	O
number	O
of	O
times	O
the	O
clique	B
state	O
xc	O
is	O
observed	O
in	O
the	O
dataset	O
an	O
intuitive	O
interpretation	O
is	O
to	O
sample	O
states	O
x	O
from	O
the	O
trained	O
model	B
px	O
and	O
use	O
these	O
to	O
compute	O
the	O
average	B
of	O
each	O
feature	O
function	B
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
of	O
samples	O
for	O
a	O
maximum	B
likelihood	B
optimal	O
model	B
these	O
sample	O
averages	O
will	O
match	O
those	O
based	O
on	O
the	O
empirical	B
average	B
unconstrained	O
potentials	O
for	O
unconstrained	O
potentials	O
we	O
have	O
a	O
separate	O
table	O
for	O
each	O
of	O
the	O
states	O
defined	O
on	O
the	O
clique	B
this	O
means	O
we	O
may	O
write	O
n	O
c	O
c	O
yc	O
cx	O
n	O
where	O
the	O
product	O
is	O
over	O
all	O
states	O
of	O
potential	B
c	O
this	O
expression	O
follows	O
since	O
the	O
indicator	O
is	O
zero	O
for	O
all	O
but	O
the	O
single	O
observed	O
state	O
x	O
n	O
i	O
x	O
n	O
c	O
log	O
cyc	O
n	O
log	O
z	O
l	O
c	O
the	O
log	O
likelihood	B
is	O
then	O
c	O
yc	O
n	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
figure	O
a	O
decomposable	B
markov	B
network	I
set	B
chain	B
for	O
formed	O
by	O
choosing	O
clique	B
as	O
root	O
and	O
orienting	O
edges	O
consistently	O
away	O
from	O
the	O
root	O
each	O
separator	B
is	O
absorbed	O
into	O
its	O
child	O
clique	B
to	O
form	O
the	O
set	B
chain	B
a	O
junction	B
tree	B
for	O
where	O
c	O
cyc	O
z	O
y	O
i	O
x	O
n	O
c	O
n	O
pyc	O
cyc	O
cyc	O
n	O
n	O
n	O
pyc	O
i	O
x	O
n	O
c	O
differentiating	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
a	O
specific	O
table	O
entry	O
we	O
obtain	O
equating	O
to	O
zero	O
the	O
maximum	B
likelihood	B
solution	O
is	O
obtained	O
when	O
that	O
is	O
the	O
unconstrained	O
optimal	O
maximum	B
likelihood	B
solution	O
is	O
given	O
by	O
setting	O
the	O
clique	B
potentials	O
such	O
that	O
the	O
marginal	B
distribution	B
on	O
each	O
clique	B
pyc	O
matches	O
the	O
empirical	B
distribution	B
on	O
each	O
clique	B
decomposable	B
markov	O
networks	O
in	O
the	O
case	O
that	O
there	O
is	O
no	O
constraint	O
placed	O
on	O
the	O
form	O
of	O
the	O
factors	O
c	O
and	O
if	O
the	O
mn	O
corresponding	O
to	O
these	O
potentials	O
is	O
decomposable	B
then	O
we	O
know	O
the	O
junction	B
tree	B
representation	O
that	O
we	O
can	O
express	O
the	O
distribution	B
in	O
the	O
form	O
of	O
a	O
product	O
of	O
local	B
marginals	O
divided	O
by	O
the	O
separator	B
distributions	O
px	O
c	O
pxc	O
s	O
pxs	O
px	O
c	O
pxcxs	O
by	O
reabsorbing	O
the	O
separators	O
into	O
the	O
numerator	O
terms	O
we	O
can	O
form	O
a	O
set	B
chain	B
distribution	B
since	O
this	O
is	O
directed	B
the	O
maximum	B
likelihood	B
solution	O
to	O
learning	B
the	O
tables	O
is	O
trivial	O
since	O
we	O
assign	O
each	O
set	B
chain	B
factor	B
pxcxs	O
by	O
counting	B
the	O
instances	O
in	O
the	O
see	O
learnmarkovdecom	O
m	O
the	O
procedure	O
is	O
perhaps	O
best	O
explained	O
by	O
an	O
example	O
as	O
given	O
below	O
see	O
for	O
a	O
general	O
description	O
example	O
given	O
a	O
dataset	O
v	O
n	O
n	O
n	O
we	O
wish	O
to	O
fit	O
by	O
maximum	B
likelihood	B
a	O
mn	O
of	O
the	O
form	O
z	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
algorithm	B
learning	B
of	O
an	O
unconstrained	O
decomposable	B
markov	B
network	I
using	O
maximum	B
likelihood	B
we	O
have	O
a	O
triangulated	B
markov	B
network	I
on	O
cliques	O
cxc	O
c	O
c	O
and	O
the	O
empirical	B
marginal	B
distributions	O
on	O
all	O
cliques	O
and	O
separators	O
form	O
a	O
junction	B
tree	B
from	O
the	O
cliques	O
initialise	O
each	O
clique	B
cxc	O
to	O
and	O
each	O
separator	B
sxs	O
to	O
choose	O
a	O
root	O
clique	B
on	O
the	O
junction	B
tree	B
and	O
orient	O
edges	O
consistently	O
away	O
from	O
this	O
root	O
for	O
this	O
oriented	O
junction	B
tree	B
divide	O
each	O
clique	B
by	O
its	O
parent	O
separator	B
return	O
the	O
new	O
potentials	O
on	O
each	O
clique	B
as	O
the	O
maximum	B
likelihood	B
solution	O
where	O
the	O
potentials	O
are	O
unconstrained	O
tables	O
see	O
since	O
the	O
graph	B
is	O
decomposable	B
we	O
know	O
it	O
admits	O
a	O
factorisation	O
of	O
clique	B
potentials	O
divided	O
by	O
the	O
separators	O
we	O
can	O
convert	O
this	O
to	O
a	O
set	B
chain	B
by	O
reabsorbing	O
the	O
denominators	O
into	O
numerator	O
terms	O
see	O
for	O
example	O
by	O
choosing	O
the	O
clique	B
as	O
rootwe	O
can	O
write	O
where	O
we	O
identified	O
the	O
factors	O
with	O
clique	B
potentials	O
and	O
the	O
normalisation	B
constant	I
z	O
is	O
unity	O
see	O
the	O
advantage	O
is	O
that	O
in	O
this	O
representation	O
the	O
clique	B
potentials	O
are	O
independent	O
since	O
the	O
distribution	B
is	O
a	O
bn	O
on	O
cluster	O
variables	O
the	O
log	O
likelihood	B
for	O
an	O
i	O
i	O
d	O
dataset	O
x	O
n	O
n	O
is	O
log	O
pxn	O
log	O
pxn	O
xn	O
xn	O
log	O
pxn	O
xn	O
log	O
pxn	O
l	O
n	O
where	O
each	O
of	O
the	O
terms	O
is	O
an	O
independent	O
parameter	B
of	O
the	O
model	B
the	O
maximum	B
likelihood	B
solution	O
then	O
corresponds	O
for	O
the	O
bn	O
case	O
to	O
simply	O
setting	O
each	O
factor	B
to	O
the	O
datacounts	O
for	O
example	O
n	O
non-decomposable	O
markov	O
networks	O
in	O
the	O
non-decomposable	O
or	O
constrained	O
case	O
no	O
closed	O
form	O
maximum	B
likelihood	B
solution	O
generally	O
exists	O
and	O
one	O
needs	O
to	O
resort	O
to	O
numerical	B
methods	O
according	O
to	O
equation	B
the	O
maximum	B
likelihood	B
solution	O
is	O
such	O
that	O
the	O
clique	B
marginals	O
match	O
the	O
empirical	B
marginals	O
assuming	O
that	O
we	O
can	O
absorb	O
the	O
normalisation	B
constant	I
into	O
an	O
arbitrarily	O
chosen	O
clique	B
we	O
can	O
drop	O
explicitly	O
representing	O
the	O
normalisation	B
constant	I
for	O
a	O
clique	B
c	O
the	O
requirement	O
that	O
the	O
marginal	B
of	O
p	O
matches	O
the	O
empirical	B
marginal	B
on	O
the	O
variables	O
in	O
the	O
clique	B
is	O
xc	O
given	O
an	O
initial	O
setting	O
for	O
the	O
potentials	O
we	O
can	O
then	O
update	O
to	O
satisfy	O
the	O
above	O
marginal	B
requirement	O
newxc	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
which	O
is	O
required	O
for	O
each	O
of	O
the	O
states	O
of	O
xc	O
by	O
multiplying	O
and	O
dividing	O
the	O
right	O
hand	O
side	O
by	O
this	O
is	O
equivalent	B
to	O
newxc	O
pxc	O
one	O
can	O
view	O
this	O
ipf	O
update	O
as	O
coordinate-wise	O
optimisation	B
of	O
the	O
log	O
likelihood	B
in	O
which	O
the	O
coordinate	O
corresponds	O
to	O
cxc	O
with	O
all	O
other	O
parameters	O
fixed	O
in	O
this	O
case	O
this	O
conditional	B
optimum	O
is	O
analytically	O
given	O
by	O
the	O
above	O
setting	O
one	O
proceeds	O
by	O
selecting	O
another	O
potential	B
to	O
update	O
note	O
that	O
in	O
general	O
with	O
each	O
update	O
the	O
marginal	B
pxc	O
need	O
to	O
be	O
recomputed	O
computing	O
these	O
marginals	O
may	O
be	O
expensive	O
unless	O
the	O
width	O
of	O
the	O
junction	B
tree	B
formed	O
from	O
the	O
graph	B
is	O
suitably	O
limited	O
example	O
machine	O
learning	B
we	O
define	O
the	O
bm	O
as	O
vtwv	O
pvw	O
zw	O
e	O
for	O
symmetric	O
w	O
and	O
binary	O
variables	O
domvi	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
e	O
v	O
vtwv	O
the	O
log	O
likelihood	B
is	O
zw	O
lw	O
wvn	O
n	O
log	O
zw	O
differentiating	O
w	O
r	O
t	O
wij	O
i	O
j	O
we	O
have	O
the	O
gradient	B
l	O
wij	O
vn	O
i	O
vn	O
j	O
a	O
simple	O
algorithm	B
to	O
optimise	O
the	O
weight	B
matrix	B
w	O
is	O
to	O
use	O
gradient	B
ascent	O
wnew	O
ij	O
wold	O
ij	O
vn	O
i	O
vn	O
j	O
the	O
second	O
order	O
statistics	O
of	O
the	O
model	B
match	O
those	O
of	O
the	O
empirical	B
for	O
a	O
learning	B
rate	I
the	O
intuitive	O
interpretation	O
is	O
that	O
learning	B
will	O
stop	O
gradient	B
is	O
zero	O
when	O
j	O
bm	O
learning	B
however	O
is	O
difficult	O
since	O
is	O
typically	O
computationally	O
intractable	O
for	O
an	O
arbitrary	O
interaction	O
matrix	B
w	O
and	O
therefore	O
needs	O
to	O
be	O
approximated	O
indeed	O
one	O
cannot	O
compute	O
the	O
likelihood	B
lw	O
exactly	O
so	O
that	O
monitoring	O
performance	B
is	O
also	O
difficult	O
n	O
vn	O
i	O
vn	O
constrained	O
decomposable	B
markov	O
networks	O
if	O
there	O
are	O
no	O
constraints	O
on	O
the	O
forms	O
of	O
the	O
maximal	O
clique	B
potentials	O
of	O
the	O
markov	B
network	I
as	O
we	O
ve	O
seen	O
learning	B
is	O
straightforward	O
here	O
our	O
interest	O
is	O
when	O
the	O
functional	O
form	O
of	O
the	O
maximal	O
clique	B
is	O
constrained	O
to	O
be	O
a	O
product	O
of	O
potentials	O
on	O
smaller	O
cx	O
i	O
i	O
c	O
cxc	O
i	O
with	O
no	O
constraint	O
being	O
placed	O
on	O
the	O
non-maximal	O
clique	B
potentials	O
i	O
in	O
general	O
in	O
this	O
case	O
one	O
cannot	O
write	O
down	O
directly	O
the	O
maximum	B
likelihood	B
solution	O
for	O
the	O
non-maximal	O
clique	B
potentials	O
c	O
cx	O
i	O
i	O
c	O
cx	O
i	O
boltzmann	B
machine	I
is	O
of	O
this	O
form	O
since	O
any	O
unconstrained	O
binary	O
pairwise	B
potentials	O
can	O
be	O
converted	O
into	O
a	O
bm	O
for	O
other	O
cases	O
in	O
which	O
the	O
i	O
c	O
are	O
constrained	O
then	O
iterative	B
scaling	I
may	O
be	O
used	O
in	O
place	O
of	O
ipf	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
interpreted	O
as	O
the	O
distrifigure	O
represents	O
bution	O
a	O
junction	B
tree	B
for	O
the	O
pair	O
wise	O
mn	O
in	O
we	O
have	O
a	O
choice	O
were	O
to	O
place	O
the	O
pairwise	B
cliques	O
and	O
this	O
is	O
one	O
valid	O
choice	O
using	O
the	O
shorthand	O
ab	O
abxa	O
xb	O
and	O
xab	O
xb	O
graph	B
represents	O
graph	B
a	O
markov	B
network	I
pairwise	B
mn	O
the	O
the	O
as	O
a	O
consider	O
the	O
graph	B
in	O
in	O
the	O
constrained	O
case	O
in	O
which	O
we	O
interpret	O
the	O
graph	B
as	O
a	O
pairwise	B
mn	O
ipf	O
may	O
be	O
used	O
to	O
learn	O
the	O
pairwise	B
tables	O
since	O
the	O
graph	B
is	O
decomposable	B
there	O
are	O
however	O
computational	O
savings	O
that	O
can	O
be	O
made	O
in	O
this	O
for	O
an	O
empirical	B
distribution	B
maximum	B
likelihood	B
requires	O
that	O
all	O
the	O
pairwise	B
marginals	O
of	O
the	O
mn	O
match	O
the	O
corresponding	O
marginals	O
obtained	O
from	O
as	O
explained	O
in	O
we	O
have	O
a	O
choice	O
as	O
to	O
which	O
junction	B
tree	B
clique	B
each	O
potential	B
is	O
assigned	O
to	O
with	O
one	O
valid	O
choice	O
being	O
given	O
in	O
keeping	O
the	O
potentials	O
of	O
the	O
cliques	O
and	O
fixed	O
we	O
can	O
update	O
the	O
potentials	O
of	O
clique	B
using	O
a	O
bar	O
to	O
denote	O
fixed	O
potentials	O
we	O
the	O
marginal	B
requirement	O
that	O
the	O
mn	O
matches	O
the	O
empirical	B
marginal	B
can	O
be	O
written	O
in	O
shorthand	O
as	O
which	O
can	O
be	O
expressed	O
as	O
the	O
messages	O
and	O
are	O
the	O
boundary	B
separator	B
tables	O
when	O
we	O
choose	O
the	O
central	O
clique	B
as	O
root	O
and	O
carry	O
out	O
absorption	B
towards	O
the	O
root	O
given	O
these	O
fixed	O
messages	O
we	O
can	O
then	O
perform	O
updates	O
of	O
the	O
root	O
clique	B
using	O
after	O
making	O
this	O
update	O
we	O
can	O
subsequently	O
update	O
similarly	O
using	O
the	O
constraint	O
new	O
so	O
that	O
new	O
given	O
converged	O
updates	O
for	O
this	O
clique	B
we	O
can	O
choose	O
another	O
clique	B
as	O
root	O
propagate	O
towards	O
the	O
root	O
and	O
compute	O
the	O
separator	B
cliques	O
on	O
the	O
boundary	B
of	O
the	O
root	O
given	O
these	O
fixed	O
boundary	B
clique	B
potentials	O
we	O
perform	O
ipf	O
within	O
the	O
clique	B
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
algorithm	B
efficient	O
iterative	B
proportional	I
fitting	I
given	O
a	O
set	O
of	O
i	O
i	O
i	O
and	O
a	O
corresponding	O
set	O
of	O
reference	O
marginal	B
distributions	O
on	O
the	O
variables	O
of	O
each	O
potential	B
we	O
aim	O
to	O
set	O
all	O
such	O
that	O
all	O
marginals	O
of	O
the	O
markov	B
network	I
match	O
the	O
given	O
empirical	B
marginals	O
given	O
a	O
markov	B
network	I
on	O
potentials	O
i	O
i	O
i	O
triangulate	O
the	O
graph	B
and	O
form	O
the	O
cliques	O
choose	O
a	O
clique	B
c	O
as	O
root	O
propagate	O
messages	O
towards	O
the	O
root	O
and	O
compute	O
the	O
separators	O
on	O
the	O
boundary	B
of	O
the	O
root	O
repeat	O
assign	O
potentials	O
to	O
cliques	O
thus	O
each	O
clique	B
has	O
a	O
set	O
of	O
associated	O
potentials	O
fc	O
initialise	O
all	O
potentials	O
example	O
to	O
unity	O
repeat	O
until	O
all	O
markov	B
network	I
marginals	O
converge	O
to	O
the	O
reference	O
marginals	O
choose	O
a	O
potential	B
i	O
in	O
clique	B
c	O
i	O
fc	O
perform	O
an	O
ipf	O
update	O
for	O
i	O
given	O
fixed	O
boundary	B
separators	O
and	O
other	O
potentials	O
in	O
c	O
until	O
potentials	O
in	O
clique	B
c	O
converge	O
this	O
efficient	O
ipf	O
procedure	O
is	O
described	O
more	O
generally	O
in	O
for	O
an	O
empirical	B
distribution	B
more	O
generally	O
ipf	O
minimises	O
the	O
kullback-leibler	B
divergence	B
between	O
a	O
given	O
reference	O
distribution	B
and	O
the	O
markov	B
network	I
see	O
demoipfeff	O
m	O
and	O
ipf	O
m	O
example	O
in	O
examples	O
of	O
binary	O
pixel	O
handwritten	O
twos	O
are	O
presented	O
forming	O
the	O
training	B
set	O
from	O
which	O
we	O
wish	O
to	O
fit	O
a	O
markov	B
network	I
first	O
all	O
pairwise	B
empirical	B
entropies	O
hxi	O
xj	O
i	O
j	O
were	O
computed	O
and	O
used	O
to	O
rank	B
edges	O
with	O
highest	O
entropy	B
edges	O
ranked	O
first	O
edges	O
were	O
included	O
in	O
a	O
graph	B
g	O
highest	O
ranked	O
first	O
provided	O
the	O
triangulated	B
g	O
had	O
all	O
cliques	O
less	O
than	O
size	O
this	O
resulted	O
in	O
unique	O
cliques	O
and	O
an	O
adjacency	B
matrix	B
for	O
the	O
triangulated	B
g	O
as	O
presented	O
in	O
in	O
the	O
number	O
of	O
times	O
that	O
a	O
pixel	O
appears	O
in	O
the	O
cliques	O
is	O
shown	O
and	O
indicates	O
the	O
degree	B
of	O
importance	B
of	O
each	O
pixel	O
in	O
distinguishing	O
between	O
the	O
examples	O
two	O
models	O
were	O
then	O
trained	O
and	O
used	O
to	O
compute	O
the	O
most	O
likely	O
reconstruction	O
based	O
on	O
missing	B
data	I
pxmissingxvisible	O
the	O
first	O
model	B
was	O
a	O
markov	B
network	I
on	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	B
for	O
which	O
essentially	O
no	O
training	B
is	O
required	O
and	O
the	O
settings	O
for	O
each	O
clique	B
potential	B
can	O
be	O
obtained	O
as	O
explained	O
in	O
the	O
model	B
makes	O
errors	O
in	O
reconstruction	O
of	O
the	O
missing	B
pixels	O
note	O
that	O
the	O
unfortunate	O
effect	O
of	O
reconstructing	O
a	O
white	O
pixel	O
surrounded	O
by	O
black	O
pixels	O
is	O
an	O
effect	O
of	O
the	O
limited	O
training	B
data	O
with	O
larger	O
amounts	O
of	O
data	O
the	O
model	B
would	O
recognise	O
that	O
such	O
effects	O
do	O
not	O
occur	O
in	O
the	O
second	O
model	B
the	O
same	O
maximal	O
cliques	O
were	O
used	O
but	O
the	O
maximal	O
clique	B
potentials	O
restricted	B
to	O
be	O
the	O
product	O
of	O
all	O
pairwise	B
two-cliques	O
within	O
the	O
maximal	O
clique	B
this	O
is	O
equivalent	B
to	O
using	O
a	O
boltzmann	B
machine	I
and	O
was	O
trained	O
using	O
the	O
efficient	O
ipf	O
approach	B
of	O
the	O
corresponding	O
reconstruction	O
error	O
is	O
this	O
performance	B
is	O
worse	O
than	O
the	O
unconstrained	O
network	O
since	O
the	O
boltzmann	B
machine	I
is	O
a	O
highly	O
constrained	O
markov	B
network	I
see	O
demolearndecmn	O
m	O
figure	O
based	O
on	O
the	O
pairwise	B
empirical	B
entropies	O
hxi	O
xj	O
edges	O
are	O
ordered	O
high	O
entropy	B
edges	O
first	O
shown	O
is	O
the	O
adjacency	B
matrix	B
of	O
the	O
resulting	O
markov	B
network	I
whose	O
junction	B
tree	B
has	O
cliques	O
in	O
size	O
represents	O
an	O
indicated	O
are	O
the	O
number	O
of	O
edge	O
cliques	O
that	O
each	O
pixel	O
is	O
a	O
member	O
of	O
indicating	O
a	O
degree	B
of	O
importance	B
note	O
that	O
the	O
lowest	O
clique	B
membership	O
value	B
is	O
so	O
that	O
each	O
pixel	O
is	O
a	O
member	O
of	O
at	O
least	O
one	O
clique	B
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
figure	O
learning	B
digits	O
simon	O
lucas	O
algoval	O
system	O
using	O
a	O
markov	B
network	I
top	O
row	O
the	O
training	B
examples	O
each	O
example	O
is	O
a	O
binary	O
image	O
on	O
pixels	O
second	O
row	O
the	O
training	B
data	O
with	O
missing	B
pixels	O
represents	O
a	O
missing	B
pixel	O
third	O
row	O
reconstructions	O
from	O
the	O
missing	B
data	I
using	O
a	O
thin-junction-tree	O
mn	O
with	O
maximum	O
clique	B
size	O
bottom	O
row	O
reconstructions	O
using	O
a	O
thin-junction-tree	O
boltzmann	B
machine	I
with	O
maximum	O
clique	B
size	O
trained	O
using	O
efficient	O
ipf	O
iterative	B
scaling	I
we	O
consider	O
markov	O
networks	O
of	O
the	O
exponential	B
form	O
e	O
cfcvc	O
where	O
fcvc	O
and	O
c	O
ranges	O
of	O
the	O
non-maximal	O
cliques	O
vc	O
v	O
the	O
normalisation	B
requirement	O
is	O
c	O
e	O
cfcvc	O
pv	O
z	O
z	O
v	O
c	O
a	O
maximum	B
likelihood	B
training	B
algorithm	B
for	O
a	O
markov	B
network	I
somewhat	O
analogous	O
to	O
the	O
em	B
approach	B
of	O
can	O
be	O
derived	O
as	O
consider	O
the	O
bound	B
for	O
positive	O
x	O
log	O
x	O
x	O
log	O
x	O
x	O
hence	O
z	O
z	O
old	O
log	O
z	O
log	O
z	O
old	O
z	O
z	O
old	O
log	O
z	O
z	O
old	O
n	O
l	O
n	O
cn	O
then	O
we	O
can	O
write	O
a	O
bound	B
on	O
the	O
log	O
likelihood	B
cfcv	O
n	O
c	O
log	O
z	O
old	O
z	O
z	O
old	O
as	O
it	O
stands	O
the	O
bound	B
is	O
in	O
general	O
not	O
straightforward	O
to	O
optimise	O
since	O
the	O
parameters	O
of	O
each	O
potential	B
are	O
coupled	B
through	O
the	O
z	O
term	O
for	O
convenience	O
it	O
is	O
useful	O
to	O
first	O
reparmameterise	O
and	O
write	O
then	O
c	O
c	O
c	O
old	O
c	O
c	O
old	O
z	O
c	O
fcvc	O
c	O
c	O
cfcvc	O
e	O
c	O
pc	O
c	O
v	O
e	O
v	O
e	O
d	O
fdvd	O
e	O
c	O
fcvc	O
old	O
c	O
e	O
c	O
fcvc	O
c	O
one	O
can	O
decouple	O
this	O
using	O
an	O
additional	O
bound	B
derived	O
by	O
first	O
considering	O
draft	O
march	O
c	O
pc	O
we	O
may	O
apply	O
jensen	O
s	O
inequality	O
to	O
give	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
e	O
where	O
pc	O
since	O
pc	O
d	O
fdvd	O
c	O
cfcvc	O
z	O
l	O
hence	O
n	O
v	O
c	O
n	O
d	O
fdvd	O
c	O
pce	O
c	O
c	O
fcv	O
n	O
c	O
c	O
c	O
n	O
plugging	O
this	O
bound	B
into	O
we	O
have	O
c	O
fcvc	O
old	O
e	O
f	O
fdvc	O
pce	O
c	O
d	O
pv	O
old	O
log	O
z	O
old	O
pce	O
c	O
lb	O
c	O
the	O
term	O
in	O
curly	O
brackets	O
contains	O
the	O
potential	B
parameters	O
c	O
in	O
an	O
uncoupled	O
fashion	O
differentiating	O
with	O
respect	O
to	O
c	O
the	O
gradient	B
of	O
each	O
lower	O
bound	B
is	O
given	O
by	O
lb	O
c	O
c	O
n	O
fcv	O
n	O
c	O
n	O
c	O
fcvce	O
c	O
old	O
d	O
pv	O
old	O
this	O
can	O
be	O
used	O
as	O
part	O
of	O
a	O
gradient	B
based	O
optimisation	B
procedure	O
to	O
learn	O
the	O
parameters	O
c	O
a	O
potential	B
advantage	O
over	O
ipf	O
is	O
that	O
all	O
the	O
parameters	O
may	O
be	O
updated	O
simultaneously	O
whereas	O
in	O
ipf	O
they	O
must	O
be	O
updated	O
sequentially	O
intuitively	O
the	O
parameters	O
converge	O
when	O
the	O
empirical	B
average	B
of	O
the	O
functions	O
f	O
match	O
the	O
average	B
of	O
the	O
functions	O
with	O
respect	O
to	O
samples	O
drawn	O
from	O
the	O
distribution	B
in	O
line	O
with	O
our	O
general	O
condition	O
for	O
maximum	B
likelihood	B
optimal	O
solution	O
c	O
fcvc	O
the	O
zero	O
of	O
the	O
gradient	B
can	O
be	O
found	O
in	O
the	O
special	O
case	O
that	O
the	O
functions	O
sum	O
to	O
analytically	O
giving	O
the	O
update	O
c	O
old	O
c	O
log	O
n	O
n	O
fcv	O
n	O
c	O
log	O
old	O
the	O
constraint	O
that	O
the	O
features	O
fc	O
need	O
to	O
be	O
non-negative	O
can	O
be	O
relaxed	O
at	O
the	O
expense	O
of	O
additional	O
variational	O
parameters	O
see	O
in	O
cases	O
where	O
the	O
zero	O
of	O
the	O
gradient	B
cannot	O
be	O
computed	O
analytically	O
there	O
may	O
be	O
little	O
advantage	O
in	O
general	O
in	O
using	O
is	O
over	O
standard	O
gradient	B
based	O
procedures	O
on	O
the	O
log	O
likelihood	B
directly	O
if	O
the	O
junction	B
tree	B
formed	O
from	O
this	O
exponential	B
form	O
markov	B
network	I
has	O
limited	O
tree	B
width	I
computational	O
savings	O
can	O
be	O
made	O
by	O
performing	O
ipf	O
over	O
the	O
cliques	O
of	O
the	O
junction	B
tree	B
and	O
updating	O
the	O
parameters	O
within	O
each	O
clique	B
using	O
this	O
is	O
a	O
modified	O
version	O
of	O
the	O
constrained	O
decomposable	B
case	O
see	O
also	O
for	O
a	O
unified	O
treatment	O
of	O
propagation	B
and	O
scaling	O
on	O
junction	O
trees	O
conditional	B
random	O
fields	O
for	O
an	O
input	O
x	O
and	O
output	O
y	O
a	O
crf	O
is	O
defined	O
by	O
a	O
conditional	B
distribution	B
ky	O
x	O
for	O
potentials	O
ky	O
x	O
to	O
make	O
learning	B
more	O
straightforward	O
the	O
potentials	O
are	O
usually	O
defined	O
as	O
e	O
kfkyx	O
for	O
fixed	O
functions	O
fy	O
x	O
and	O
parameters	O
k	O
in	O
this	O
case	O
the	O
distribution	B
of	O
the	O
output	O
conditioned	O
on	O
the	O
input	O
is	O
pyx	O
zx	O
k	O
pyx	O
zx	O
k	O
draft	O
march	O
e	O
kfkyx	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
for	O
an	O
i	O
i	O
d	O
dataset	O
of	O
input-outputs	O
d	O
yn	O
n	O
n	O
training	B
based	O
on	O
conditional	B
maximum	B
likelihood	B
requires	O
the	O
maximisation	B
of	O
k	O
l	O
log	O
pynxn	O
kfkyn	O
xn	O
log	O
zxn	O
in	O
general	O
no	O
closed	O
form	O
solution	O
for	O
the	O
optimal	O
exists	O
and	O
this	O
needs	O
to	O
be	O
determined	O
numerically	O
first	O
we	O
note	O
that	O
equation	B
is	O
equivalent	B
to	O
equation	B
where	O
the	O
parameters	O
are	O
here	O
denoted	O
by	O
and	O
the	O
variables	O
v	O
are	O
here	O
denoted	O
by	O
y	O
in	O
the	O
crf	O
case	O
the	O
inputs	O
simply	O
have	O
the	O
effect	O
of	O
determining	O
the	O
feature	O
fky	O
x	O
in	O
this	O
sense	O
iterative	B
scaling	I
or	O
any	O
related	O
method	O
for	O
maximum	B
likelihood	B
training	B
of	O
constrained	O
markov	O
networks	O
may	O
be	O
readily	O
adapted	O
taking	O
advantage	O
also	O
of	O
any	O
computational	O
savings	O
from	O
limited	O
width	O
junction	O
trees	O
as	O
an	O
alternative	O
here	O
we	O
briefly	O
describe	O
gradient	B
based	O
training	B
the	O
gradient	B
has	O
components	O
l	O
n	O
i	O
fiyn	O
xn	O
the	O
terms	O
can	O
be	O
problematic	O
and	O
their	O
tractability	O
depends	O
on	O
the	O
structure	B
of	O
the	O
potentials	O
for	O
a	O
multivariate	B
y	O
provided	O
the	O
structure	B
of	O
the	O
cliques	O
defined	O
on	O
subsets	O
of	O
y	O
is	O
singlyconnected	O
then	O
computing	O
the	O
average	B
is	O
generally	O
tractable	O
more	O
generally	O
provided	O
the	O
cliques	O
of	O
the	O
resulting	O
junction	B
tree	B
have	O
limited	O
width	O
then	O
exact	O
marginals	O
are	O
avaiable	O
an	O
example	O
of	O
this	O
is	O
given	O
for	O
a	O
linear-chain	O
crf	O
in	O
see	O
also	O
below	O
another	O
quantity	O
often	O
useful	O
for	O
numerical	B
optimisation	B
is	O
the	O
hessian	B
which	O
has	O
components	O
l	O
n	O
i	O
j	O
xnfjy	O
where	O
the	O
averages	O
above	O
are	O
with	O
respect	O
to	O
pyxn	O
this	O
expression	O
is	O
a	O
sum	O
of	O
covariance	B
elements	O
and	O
is	O
therefore	O
negative	O
definite	O
hence	O
the	O
function	B
l	O
is	O
concave	O
and	O
has	O
only	O
a	O
single	O
global	B
optimum	O
whilst	O
no	O
closed	O
form	O
solution	O
for	O
the	O
optimal	O
exists	O
the	O
optimal	O
solutions	O
can	O
be	O
found	O
easily	O
using	O
a	O
numerical	B
technique	O
such	O
as	O
conjugate	B
gradients	O
in	O
practice	O
regularisation	B
terms	O
are	O
often	O
added	O
to	O
prevent	O
overfitting	B
for	O
a	O
discussion	O
of	O
regularisation	B
using	O
a	O
term	O
k	O
k	O
k	O
for	O
positive	O
regularisation	B
constants	O
k	O
discourages	O
the	O
weights	O
from	O
being	O
too	O
large	O
this	O
term	O
is	O
also	O
negative	O
definite	O
and	O
hence	O
the	O
overall	O
objective	O
function	B
remains	O
concave	O
iterative	B
scaling	I
may	O
also	O
be	O
used	O
to	O
train	O
a	O
crf	O
though	O
in	O
practice	O
gradient	B
based	O
techniques	O
are	O
to	O
be	O
once	O
trained	O
a	O
crf	O
can	O
be	O
used	O
for	O
predicting	O
the	O
output	O
distribution	B
for	O
a	O
novel	O
input	O
x	O
the	O
most	O
likely	O
output	O
y	O
is	O
equivalently	O
given	O
by	O
argmax	O
argmax	O
kfky	O
x	O
y	O
log	O
zx	O
since	O
the	O
normalisation	B
term	O
is	O
independent	O
of	O
y	O
finding	O
the	O
most	O
likely	O
output	O
is	O
equivalent	B
to	O
y	O
y	O
k	O
log	O
pyx	O
k	O
kfky	O
x	O
y	O
argmax	O
y	O
draft	O
march	O
maximum	B
likelihood	B
for	O
undirected	B
models	O
figure	O
training	B
results	O
for	O
a	O
linear	B
chain	B
crf	O
there	O
are	O
training	B
sequences	B
one	O
per	O
subpanel	O
in	O
each	O
the	O
top	O
row	O
corresponds	O
to	O
the	O
input	O
sequence	O
xt	O
state	O
represented	O
by	O
a	O
different	O
colour	O
the	O
middle	O
row	O
the	O
correct	O
output	O
sequence	O
yt	O
state	O
represented	O
by	O
a	O
different	O
colour	O
together	O
the	O
input	O
and	O
output	O
sequences	B
make	O
the	O
training	B
data	O
d	O
the	O
bottom	O
row	O
contains	O
the	O
most	O
likely	O
output	O
sequence	O
given	O
the	O
trained	O
crf	O
arg	O
five	O
additional	O
test	O
sequences	B
along	O
with	O
the	O
correct	O
output	O
and	O
predicted	O
output	O
sequence	O
natural	B
language	O
processing	O
in	O
a	O
natural	B
language	O
processing	O
application	O
xt	O
might	O
represent	O
a	O
word	O
and	O
yt	O
a	O
corresponding	O
linguistic	O
tag	O
noun	O
verb	O
etc	O
a	O
more	O
suitable	O
form	O
in	O
this	O
case	O
is	O
to	O
constrain	O
the	O
crf	O
to	O
be	O
of	O
the	O
form	O
exp	O
kgkyt	O
yt	O
lhlyt	O
xt	O
k	O
l	O
for	O
binary	O
functions	O
gk	O
and	O
hl	O
and	O
parameters	O
k	O
and	O
l	O
the	O
grammatical	O
structure	B
of	O
tag-tag	O
transitions	O
is	O
encoded	O
in	O
gkyt	O
yt	O
and	O
linguistic	O
tag	O
information	O
in	O
hkyt	O
xt	O
with	O
the	O
importance	B
of	O
these	O
being	O
determined	O
by	O
the	O
corresponding	O
in	O
this	O
case	O
inference	B
of	O
the	O
marginals	O
is	O
straightforward	O
since	O
the	O
factor	B
graph	B
corresponding	O
to	O
the	O
inference	B
problem	B
is	O
a	O
linear	B
chain	B
variants	O
of	O
the	O
linear	B
chain	B
crf	O
are	O
used	O
heavily	O
in	O
natural	B
language	O
processing	O
including	O
part-of-speech	B
tagging	B
and	O
machine	O
translation	O
which	O
the	O
input	O
sequence	O
x	O
represents	O
a	O
sentence	O
say	O
in	O
english	O
and	O
the	O
output	O
sequence	O
y	O
the	O
corresponding	O
translation	O
into	O
french	O
see	O
for	O
example	O
example	O
chain	B
crf	O
we	O
consider	O
a	O
crf	O
with	O
x	O
input	O
states	O
and	O
y	O
output	O
states	O
of	O
the	O
form	O
k	O
kgkytyt	O
e	O
l	O
lhlytxt	O
here	O
the	O
binary	O
functions	O
gkyt	O
yt	O
i	O
ak	O
i	O
bk	O
k	O
model	B
the	O
transitions	O
between	O
two	O
consecutive	O
outputs	O
the	O
binary	O
functions	O
hlyt	O
xt	O
i	O
al	O
i	O
cl	O
l	O
model	B
the	O
translation	O
of	O
the	O
input	O
to	O
the	O
output	O
there	O
are	O
therefore	O
parameters	O
in	O
total	O
in	O
we	O
plot	O
the	O
training	B
and	O
test	O
results	O
based	O
on	O
a	O
small	O
set	O
of	O
data	O
the	O
training	B
of	O
the	O
crf	O
is	O
obtained	O
using	O
iterations	O
of	O
gradient	B
ascent	O
with	O
a	O
learning	B
rate	I
of	O
see	O
demolinearcrf	O
m	O
draft	O
march	O
properties	B
of	O
maximum	B
likelihood	B
pseudo	B
likelihood	B
consider	O
a	O
mn	O
on	O
variables	O
x	O
with	O
dim	O
x	O
d	O
of	O
the	O
form	O
cxc	O
c	O
for	O
all	O
but	O
specially	O
constrained	O
c	O
the	O
partition	B
function	B
z	O
will	O
be	O
intractable	O
and	O
the	O
likelihood	B
of	O
a	O
set	O
of	O
i	O
i	O
d	O
data	O
intractable	O
as	O
well	O
a	O
surrogate	O
is	O
to	O
use	O
the	O
pseudo	B
likelihood	B
of	O
the	O
probability	O
of	O
each	O
variable	O
conditioned	O
on	O
all	O
other	O
variables	O
is	O
equivalent	B
to	O
conditioning	B
on	O
only	O
the	O
variable	O
s	O
neighbours	O
for	O
a	O
mn	O
c	O
px	O
z	O
l	O
log	O
pxn	O
i	O
i	O
are	O
usually	O
straightforward	O
to	O
work	O
out	O
since	O
they	O
require	O
finding	O
the	O
normalisation	B
the	O
terms	O
pxn	O
of	O
a	O
univariate	B
distribution	B
only	O
in	O
this	O
case	O
the	O
gradient	B
can	O
be	O
computed	O
exactly	O
and	O
learning	B
of	O
the	O
parameters	O
carried	O
out	O
at	O
least	O
for	O
the	O
case	O
of	O
the	O
boltzmann	B
machine	I
this	O
forms	O
a	O
consistent	B
learning	B
the	O
structure	B
learning	B
the	O
structure	B
of	O
a	O
markov	B
network	I
can	O
also	O
be	O
based	O
on	O
independence	B
tests	O
as	O
for	O
belief	B
networks	I
a	O
criterion	O
for	O
finding	O
a	O
mn	O
on	O
a	O
set	O
of	O
nodes	O
v	O
is	O
to	O
use	O
the	O
fact	O
that	O
no	O
edge	O
exits	O
between	O
x	O
and	O
y	O
if	O
conditioned	O
on	O
all	O
other	O
nodes	O
x	O
and	O
y	O
are	O
deemed	O
independent	O
this	O
is	O
the	O
pairwise	B
markov	O
property	O
described	O
in	O
by	O
checking	O
x	O
yvx	O
y	O
for	O
every	O
pair	O
of	O
variables	O
x	O
and	O
y	O
this	O
edge	O
deletion	O
approach	B
in	O
principle	O
reveals	O
the	O
structure	B
of	O
the	O
for	O
learning	B
the	O
structure	B
from	O
an	O
oracle	O
this	O
method	O
is	O
sound	O
however	O
a	O
practical	O
difficulty	O
in	O
the	O
case	O
where	O
the	O
independencies	O
are	O
determined	O
from	O
data	O
is	O
that	O
checking	O
if	O
x	O
y	O
vx	O
y	O
requires	O
in	O
principle	O
enormous	O
amounts	O
of	O
data	O
the	O
reason	O
for	O
this	O
is	O
that	O
the	O
conditioning	B
selects	O
only	O
those	O
parts	O
of	O
the	O
dataset	O
consistent	B
with	O
the	O
conditioning	B
in	O
practice	O
this	O
will	O
result	O
in	O
very	O
small	O
numbers	O
of	O
remaining	O
datapoints	O
and	O
estimating	O
independencies	O
on	O
this	O
basis	O
is	O
unreliable	O
the	O
markov	O
boundary	B
uses	O
the	O
local	B
markov	O
property	O
namely	O
that	O
conditioned	O
on	O
its	O
neighbours	O
a	O
variable	O
is	O
independent	O
of	O
all	O
other	O
variables	O
in	O
the	O
graph	B
by	O
starting	O
with	O
a	O
variable	O
x	O
and	O
an	O
empty	O
neighbourhood	O
set	O
one	O
can	O
progressively	O
include	O
neighbours	O
testing	O
if	O
their	O
inclusion	O
renders	O
the	O
remaining	O
non-neighbours	O
independent	O
of	O
x	O
a	O
difficultly	O
with	O
this	O
is	O
that	O
if	O
one	O
doesn	O
t	O
have	O
the	O
correct	O
markov	O
boundary	B
then	O
including	O
a	O
variable	O
in	O
the	O
neighbourhood	O
set	O
may	O
be	O
deemed	O
necessary	O
to	O
see	O
this	O
consider	O
a	O
network	O
which	O
corresponds	O
to	O
a	O
linear	B
chain	B
and	O
that	O
x	O
is	O
at	O
the	O
edge	O
of	O
the	O
chain	B
in	O
this	O
case	O
only	O
the	O
nearest	B
neighbour	B
of	O
x	O
is	O
in	O
the	O
markov	O
boundary	B
of	O
x	O
however	O
if	O
this	O
nearest	B
neighbour	B
were	O
not	O
currently	O
in	O
the	O
set	O
then	O
any	O
other	O
non-nearest	O
neighbour	B
would	O
be	O
included	O
even	O
though	O
this	O
is	O
not	O
strictly	O
required	O
to	O
counter	O
this	O
the	O
neighbourhood	O
variables	O
included	O
in	O
the	O
neighbourhood	O
of	O
x	O
may	O
be	O
later	O
removed	O
if	O
they	O
are	O
deemed	O
superfluous	O
to	O
the	O
in	O
cases	O
where	O
specific	O
constraints	O
are	O
imposed	O
such	O
as	O
learning	B
structures	O
whose	O
resulting	O
triangulation	B
has	O
a	O
bounded	O
tree-width	O
whilst	O
still	O
formally	O
difficult	O
approximate	B
procedures	O
are	O
in	O
terms	O
of	O
network	B
scoring	I
methods	O
for	O
undirected	B
networks	O
computing	O
a	O
score	O
is	O
hampered	O
by	O
the	O
fact	O
that	O
the	O
parameters	O
of	O
each	O
clique	B
become	O
coupled	B
in	O
the	O
normalisation	B
constant	I
of	O
the	O
distribution	B
this	O
issue	O
can	O
be	O
addressed	O
using	O
hyper	B
markov	I
properties	B
of	O
maximum	B
likelihood	B
training	B
assuming	O
the	O
correct	O
model	B
class	O
consider	O
a	O
dataset	O
x	O
n	O
n	O
generated	O
from	O
an	O
underlying	O
parametric	O
model	B
px	O
our	O
interest	O
is	O
to	O
fit	O
a	O
model	B
px	O
of	O
the	O
same	O
form	O
as	O
the	O
correct	O
underlying	O
model	B
px	O
and	O
examine	O
draft	O
march	O
code	O
whether	O
if	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
data	O
the	O
parameter	B
learned	O
by	O
maximum	B
likelihood	B
matches	O
the	O
correct	O
parameter	B
our	O
derivation	O
below	O
is	O
non-rigorous	O
but	O
highlights	O
the	O
essence	O
of	O
the	O
argument	O
assuming	O
the	O
data	O
is	O
i	O
i	O
d	O
the	O
log	O
likelihood	B
l	O
log	O
px	O
is	O
l	O
n	O
log	O
pxn	O
in	O
the	O
limit	O
n	O
the	O
sample	O
average	B
can	O
be	O
replaced	O
by	O
an	O
average	B
with	O
respect	O
to	O
the	O
distribution	B
generating	O
the	O
data	O
px	O
px	O
l	O
px	O
up	O
to	O
a	O
negligible	O
constant	O
this	O
is	O
the	O
kullback-leibler	B
divergence	B
between	O
two	O
distributions	O
in	O
x	O
just	O
with	O
different	O
parameter	B
settings	O
the	O
that	O
maximises	O
l	O
is	O
that	O
which	O
minimises	O
the	O
kullbackleibler	O
divergence	B
namely	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
data	O
we	O
can	O
in	O
principle	O
learn	O
the	O
correct	O
parameters	O
we	O
know	O
the	O
correct	O
model	B
class	O
the	O
property	O
of	O
an	O
estimator	O
such	O
that	O
the	O
parameter	B
converges	O
to	O
the	O
true	O
model	B
parameter	B
as	O
the	O
sequence	O
of	O
data	O
increase	O
is	O
termed	O
a	O
consistency	O
training	B
when	O
the	O
assumed	O
model	B
is	O
incorrect	O
we	O
write	O
qx	O
for	O
the	O
assumed	O
model	B
and	O
px	O
for	O
the	O
correct	O
generating	O
model	B
repeating	O
the	O
above	O
calculations	O
in	O
the	O
case	O
of	O
the	O
assumed	O
model	B
being	O
correct	O
we	O
have	O
that	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
data	O
the	O
likelihood	B
is	O
l	O
qx	O
klpx	O
px	O
since	O
q	O
and	O
p	O
are	O
not	O
of	O
the	O
same	O
form	O
setting	O
to	O
does	O
not	O
necessarily	O
minimise	O
klpx	O
and	O
therefore	O
does	O
not	O
necessarily	O
optimize	O
l	O
code	O
condindepemp	O
m	O
bayes	O
test	O
and	O
mutual	B
information	I
for	O
empirical	B
conditional	B
independence	B
condmi	O
m	O
conditional	B
mutual	B
information	I
condmiemp	O
m	O
conditional	B
mutual	B
information	I
of	O
empirical	B
distribution	B
miemp	O
m	O
mutual	B
information	I
of	O
empirical	B
distribution	B
pc	B
algorithm	B
using	O
an	O
oracle	O
this	O
demo	O
uses	O
an	O
oracle	O
to	O
determine	O
x	O
y	O
z	O
rather	O
than	O
using	O
data	O
to	O
determine	O
the	O
empirical	B
dependence	O
the	O
oracle	O
is	O
itself	O
a	O
belief	B
network	I
for	O
the	O
partial	O
orientation	O
only	O
the	O
first	O
unmarried	O
collider	B
rule	O
is	O
implemented	O
demopcoracle	O
m	O
demo	O
of	O
pc	B
algorithm	B
with	O
an	O
oracle	O
pcskeletonoracle	O
m	O
pc	B
algorithm	B
using	O
an	O
oracle	O
pcorient	O
m	O
orient	O
a	O
skeleton	B
demo	O
of	O
empirical	B
conditional	B
independence	B
for	O
half	O
of	O
the	O
experiments	O
the	O
data	O
is	O
drawn	O
from	O
a	O
distribution	B
for	O
which	O
x	O
y	O
z	O
is	O
true	O
for	O
the	O
other	O
half	O
of	O
the	O
experiments	O
the	O
data	O
is	O
drawn	O
from	O
a	O
random	O
distribution	B
for	O
which	O
x	O
y	O
z	O
is	O
false	O
we	O
then	O
measure	O
the	O
fraction	O
of	O
experiments	O
for	O
which	O
the	O
bayes	O
test	O
correctly	O
decides	O
x	O
y	O
z	O
we	O
also	O
measure	O
the	O
fraction	O
of	O
experiments	O
for	O
which	O
the	O
mutual	B
information	I
test	O
correctly	O
decides	O
x	O
y	O
z	O
based	O
on	O
draft	O
march	O
fuse	O
drum	O
toner	O
paper	O
roller	O
burning	O
quality	O
wrinkled	O
mult	O
pages	O
paper	O
jam	O
exercises	O
figure	O
printer	B
nightmare	I
belief	B
network	I
all	O
variables	O
are	O
binary	O
the	O
upper	O
variables	O
without	O
parents	B
are	O
possible	O
problems	O
and	O
the	O
lower	O
variables	O
consequences	O
of	O
problems	O
setting	O
the	O
threshold	O
equal	O
to	O
the	O
median	O
of	O
all	O
the	O
empirical	B
conditional	B
mutual	B
information	I
values	O
a	O
similar	O
empirical	B
threshold	O
can	O
also	O
be	O
obtained	O
for	O
the	O
bayes	O
factor	B
this	O
is	O
not	O
strictly	O
kosher	O
in	O
the	O
pure	O
bayesian	B
spirit	O
since	O
one	O
should	O
in	O
principle	O
set	O
the	O
threshold	O
to	O
zero	O
the	O
test	O
based	O
on	O
the	O
assumed	O
chi-squared	O
distributed	O
mi	O
is	O
included	O
for	O
comparison	O
although	O
it	O
seems	O
to	O
be	O
impractical	O
in	O
these	O
small	O
data	O
cases	O
democondindepemp	O
m	O
demo	O
of	O
empirical	B
conditional	B
independence	B
based	O
on	O
data	O
bayes	O
dirichlet	B
structure	B
learning	B
it	O
is	O
interesting	O
to	O
compare	O
the	O
result	O
of	O
demopcdata	O
m	O
with	O
demobdscore	O
m	O
pcskeletondata	O
m	O
pc	B
algorithm	B
using	O
empirical	B
conditional	B
independence	B
demopcdata	O
m	O
demo	O
of	O
pc	B
algorithm	B
with	O
data	O
bdscore	O
m	O
bayes	O
dirichlet	B
score	O
for	O
a	O
node	O
given	O
parents	B
learnbayesnet	O
m	O
given	O
an	O
ancestral	B
order	I
and	O
maximal	O
parents	B
learn	O
the	O
network	O
demobdscore	O
m	O
demo	O
of	O
structure	B
learning	B
exercises	O
exercise	O
nightmare	O
cheapco	O
is	O
quite	O
honestly	O
a	O
pain	O
in	O
the	O
neck	O
not	O
only	O
did	O
they	O
buy	O
a	O
dodgy	O
old	O
laser	O
printer	O
from	O
stoppress	O
and	O
use	O
it	O
mercilessly	O
but	O
try	O
to	O
get	O
away	O
with	O
using	O
substandard	O
components	O
and	O
materials	O
unfortunately	O
for	O
stoppress	O
they	O
have	O
a	O
contract	O
to	O
maintain	O
cheapco	O
s	O
old	O
warhorse	O
and	O
end	O
up	O
frequently	O
sending	O
the	O
mechanic	O
out	O
to	O
repair	O
the	O
printer	O
after	O
the	O
visit	O
they	O
decide	O
to	O
make	O
a	O
statistical	O
model	B
of	O
cheapco	O
s	O
printer	O
so	O
that	O
they	O
will	O
have	O
a	O
reasonable	O
idea	O
of	O
the	O
fault	O
based	O
only	O
on	O
the	O
information	O
that	O
cheapco	O
s	O
secretary	O
tells	O
them	O
on	O
the	O
phone	O
in	O
that	O
way	O
stoppress	O
hopes	O
to	O
be	O
able	O
to	O
send	O
out	O
to	O
cheapco	O
only	O
a	O
junior	O
repair	O
mechanic	O
having	O
most	O
likely	O
diagnosed	O
the	O
fault	O
over	O
the	O
phone	O
based	O
on	O
the	O
manufacturer	O
s	O
information	O
stoppress	O
has	O
a	O
good	O
idea	O
of	O
the	O
dependencies	O
in	O
the	O
printer	O
and	O
what	O
is	O
likely	O
to	O
directly	O
affect	O
other	O
printer	O
components	O
the	O
belief	B
network	I
in	O
represents	O
these	O
assumptions	O
however	O
the	O
specific	O
way	O
that	O
cheapco	O
abuse	O
their	O
printer	O
is	O
a	O
mystery	O
so	O
that	O
the	O
exact	O
probabilistic	B
relationships	O
between	O
the	O
faults	O
and	O
problems	O
is	O
idiosyncratic	O
to	O
cheapco	O
stoppress	O
has	O
the	O
following	O
table	O
of	O
faults	O
for	O
each	O
of	O
the	O
visits	O
each	O
column	O
represents	O
a	O
visit	O
fuse	O
assembly	O
malfunction	O
drum	O
unit	O
toner	O
out	O
poor	O
paper	O
quality	O
worn	O
roller	O
burning	O
smell	O
poor	O
print	O
quality	O
wrinkled	O
pages	O
multiple	O
pages	O
fed	O
paper	O
jam	O
draft	O
march	O
exercises	O
the	O
above	O
table	O
is	O
contained	O
in	O
printer	O
mat	O
learn	O
all	O
table	O
entries	O
on	O
the	O
basis	O
of	O
maximum	B
likelihood	B
program	O
the	O
belief	B
network	I
using	O
the	O
tables	O
maximum	B
likelihood	B
tables	O
and	O
brmltoolbox	O
compute	O
the	O
probability	O
that	O
there	O
is	O
a	O
fuse	O
assembly	O
malfunction	O
given	O
that	O
the	O
secretary	O
complains	O
there	O
is	O
a	O
burning	O
smell	O
and	O
that	O
the	O
paper	O
is	O
jammed	O
and	O
that	O
there	O
are	O
no	O
other	O
problems	O
repeat	O
the	O
above	O
calculation	O
using	O
a	O
bayesian	B
method	O
in	O
which	O
a	O
flat	O
beta	B
prior	B
is	O
used	O
on	O
all	O
tables	O
given	O
the	O
above	O
information	O
from	O
the	O
secretary	O
what	O
is	O
the	O
most	O
likely	O
joint	B
diagnosis	O
over	O
the	O
diagnostic	O
variables	O
that	O
is	O
the	O
joint	B
most	O
likely	O
pf	O
use	O
drum	O
t	O
oner	O
p	O
aper	O
rollerevidence	O
use	O
the	O
max-absorption	O
method	O
on	O
the	O
associated	O
junction	B
tree	B
compute	O
the	O
joint	B
most	B
likely	I
state	I
of	O
the	O
distribution	B
pf	O
use	O
drum	O
t	O
oner	O
p	O
aper	O
rollerburning	O
smell	O
paper	O
jammed	O
explain	O
how	O
to	O
compute	O
this	O
efficiently	O
using	O
the	O
max-absorption	O
method	O
exercise	O
explain	O
how	O
to	O
use	O
a	O
factorised	B
beta	B
prior	B
in	O
the	O
case	O
of	O
learning	B
table	O
entries	O
in	O
belief	B
networks	I
in	O
which	O
each	O
variable	O
has	O
maximally	O
a	O
single	O
parent	O
consider	O
the	O
issues	O
around	O
bayesian	B
learning	B
of	O
binary	O
table	O
entries	O
when	O
the	O
number	O
of	O
parental	O
variables	O
is	O
not	O
restricted	B
exercise	O
consider	O
data	O
xn	O
n	O
n	O
show	O
that	O
for	O
a	O
gaussian	B
distribution	B
the	O
maximum	B
likelihood	B
estimator	O
of	O
the	O
mean	B
is	O
m	O
n	O
xn	O
and	O
variance	B
is	O
n	O
exercise	O
a	O
training	B
set	O
consists	O
of	O
one	O
dimensional	O
examples	O
from	O
two	O
classes	O
the	O
training	B
examples	O
from	O
class	O
are	O
and	O
from	O
class	O
are	O
fit	O
a	O
dimensional	O
gaussian	B
using	O
maximum	B
likelihood	B
to	O
each	O
of	O
these	O
two	O
classes	O
also	O
estimate	O
the	O
class	O
probabilities	O
and	O
using	O
maximum	B
likelihood	B
what	O
is	O
the	O
probability	O
that	O
the	O
test	O
point	O
x	O
belongs	O
to	O
class	O
exercise	O
for	O
a	O
set	O
of	O
n	O
observations	O
data	O
x	O
xn	O
and	O
independently	O
gathered	O
observations	O
the	O
log	O
likelihood	B
for	O
a	O
belief	B
network	I
to	O
generate	O
x	O
is	O
log	O
px	O
log	O
p	O
i	O
i	O
we	O
define	O
the	O
notation	O
meaning	O
variable	O
xi	O
is	O
in	O
state	O
s	O
and	O
the	O
parents	B
of	O
variable	O
xi	O
are	O
in	O
the	O
vector	O
of	O
states	O
t	O
using	O
a	O
lagrangian	B
i	O
st	O
pxi	O
spa	O
t	O
l	O
log	O
p	O
i	O
i	O
s	O
j	O
s	O
xn	O
pa	O
j	O
s	O
xn	O
pa	O
s	O
j	O
xn	O
j	O
ti	O
i	O
ti	O
xn	O
j	O
show	O
that	O
the	O
maximum	B
likelihood	B
setting	O
of	O
i	O
st	O
is	O
s	O
i	O
s	O
draft	O
march	O
exercises	O
cl	O
n	O
exercise	O
likelihood	B
training	B
consider	O
a	O
situation	O
in	O
which	O
we	O
partition	O
observable	O
variables	O
into	O
disjoint	O
sets	O
x	O
and	O
y	O
and	O
that	O
we	O
want	O
to	O
find	O
the	O
parameters	O
that	O
maximize	O
the	O
conditional	B
likelihood	B
pynxn	O
for	O
a	O
set	O
of	O
training	B
data	O
yn	O
n	O
n	O
all	O
data	O
is	O
assumed	O
generated	O
from	O
the	O
same	O
distribution	B
px	O
y	O
pyx	O
for	O
some	O
unknown	O
parameter	B
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
i	O
i	O
d	O
training	B
data	O
does	O
cl	O
have	O
an	O
optimum	O
at	O
exercise	O
matching	O
one	O
way	O
to	O
set	O
parameters	O
of	O
a	O
distribution	B
is	O
to	O
match	O
the	O
moments	O
of	O
the	O
distribution	B
to	O
the	O
empirical	B
moments	O
this	O
sometimes	O
corresponds	O
to	O
maximum	B
likelihood	B
the	O
gaussian	B
distribution	B
for	O
example	O
though	O
generally	O
this	O
is	O
not	O
consistent	B
with	O
maximum	B
likelihood	B
for	O
data	O
with	O
mean	B
m	O
and	O
variance	B
s	O
show	O
that	O
to	O
fit	O
a	O
beta	B
distribution	B
by	O
moment	O
matching	O
we	O
use	O
m	O
s	O
m	O
m	O
exercise	O
for	O
data	O
xn	O
n	O
n	O
generated	O
from	O
a	O
beta	B
distribution	B
b	O
b	O
show	O
that	O
the	O
log	O
likelihood	B
is	O
given	O
by	O
m	O
la	O
b	O
log	O
xn	O
xn	O
n	O
log	O
ba	O
b	O
where	O
ba	O
b	O
is	O
the	O
beta	B
function	B
show	O
that	O
the	O
derivatives	O
are	O
a	O
l	O
log	O
xn	O
b	O
b	O
l	O
xn	O
b	O
where	O
d	O
log	O
is	O
the	O
digamma	B
function	B
and	O
suggest	O
a	O
method	O
to	O
learn	O
the	O
parameters	O
ab	O
exercise	O
consider	O
the	O
boltzmann	B
machine	I
as	O
defined	O
in	O
derive	O
the	O
gradient	B
with	O
respect	O
to	O
the	O
biases	O
wii	O
write	O
down	O
the	O
pseudo	B
likelihood	B
for	O
a	O
set	O
of	O
i	O
i	O
d	O
data	O
vn	O
and	O
derive	O
the	O
gradient	B
of	O
this	O
exercise	O
show	O
that	O
the	O
model	B
likelihood	B
equation	B
can	O
be	O
written	O
explicitly	O
as	O
with	O
respect	O
to	O
wij	O
i	O
j	O
pdm	O
v	O
j	O
i	O
uiv	O
j	O
i	O
iv	O
j	O
i	O
iv	O
j	O
j	O
exercise	O
define	O
the	O
set	O
n	O
as	O
consisting	O
of	O
node	O
belief	B
networks	I
in	O
which	O
each	O
node	O
has	O
at	O
most	O
parents	B
for	O
a	O
given	O
ancestral	B
order	I
a	O
the	O
restricted	B
set	O
is	O
written	O
na	O
how	O
many	O
belief	B
networks	I
are	O
in	O
na	O
what	O
is	O
the	O
computational	O
time	O
to	O
find	O
the	O
optimal	O
member	O
of	O
na	O
using	O
the	O
bayesian	B
dirichlet	B
score	I
assuming	O
that	O
computing	O
the	O
bd	B
score	I
of	O
any	O
member	O
of	O
na	O
takes	O
second	O
and	O
bearing	O
in	O
mind	O
the	O
decomposability	O
of	O
the	O
bd	B
score	I
what	O
is	O
the	O
time	O
to	O
find	O
the	O
optimal	O
member	O
of	O
n	O
exercise	O
for	O
the	O
markov	B
network	I
px	O
y	O
z	O
z	O
y	O
z	O
derive	O
an	O
iterative	B
scaling	I
algorithm	B
to	O
learn	O
the	O
unconstrained	O
tables	O
y	O
and	O
y	O
based	O
on	O
a	O
set	O
of	O
i	O
i	O
d	O
data	O
x	O
draft	O
march	O
exercise	O
given	O
training	B
data	O
xn	O
derive	O
an	O
iterative	B
scaling	I
algorithm	B
for	O
maximum	B
likelihood	B
training	B
of	O
crfs	O
of	O
the	O
form	O
px	O
e	O
cfcx	O
z	O
where	O
z	O
cannot	O
all	O
be	O
zero	O
for	O
any	O
given	O
x	O
exercise	O
for	O
data	O
x	O
n	O
consider	O
maximum	B
likelihood	B
learning	B
of	O
a	O
markov	B
network	I
px	O
c	O
cxc	O
with	O
potentials	O
of	O
the	O
form	O
c	O
e	O
cfcx	O
and	O
non-negative	O
features	O
fcx	O
may	O
assume	O
that	O
the	O
features	O
x	O
with	O
fcxc	O
being	O
general	O
real	O
valued	O
functions	O
and	O
c	O
real	O
valued	O
parameters	O
by	O
considering	O
exercises	O
c	O
cxc	O
e	O
cfcxc	O
cfcxc	O
c	O
cfcxc	O
for	O
auxiliary	O
variables	O
pc	O
such	O
pc	O
pc	O
c	O
algorithm	B
in	O
which	O
each	O
parameter	B
c	O
can	O
be	O
learned	O
separately	O
c	O
pc	O
explain	O
how	O
to	O
derive	O
a	O
form	O
of	O
iterative	B
scaling	I
training	B
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
naive	B
bayes	I
naive	B
bayes	I
and	O
conditional	B
independence	B
naive	B
bayes	I
is	O
a	O
popular	O
classification	B
method	O
and	O
aids	O
our	O
discussion	O
of	O
conditional	B
independence	B
overfitting	B
and	O
bayesian	B
methods	O
in	O
nb	O
we	O
form	O
a	O
joint	B
model	B
of	O
observations	O
x	O
and	O
the	O
corresponding	O
class	O
label	O
c	O
using	O
a	O
belief	B
network	I
of	O
the	O
form	O
px	O
c	O
pc	O
pxic	O
whose	O
belief	B
network	I
is	O
depicted	O
in	O
coupled	B
with	O
a	O
suitable	O
choice	O
for	O
each	O
conditional	B
distri	O
bution	O
pxic	O
we	O
can	O
then	O
use	O
bayes	O
rule	O
to	O
form	O
a	O
classifier	B
for	O
a	O
novel	O
attribute	O
vector	O
x	O
pcx	O
px	O
px	O
px	O
c	O
px	O
in	O
practice	O
it	O
is	O
common	O
to	O
consider	O
only	O
two	O
classes	O
domc	O
the	O
theory	O
we	O
describe	O
below	O
is	O
valid	O
for	O
any	O
number	O
of	O
classes	O
c	O
though	O
our	O
examples	O
are	O
restricted	B
to	O
the	O
binary	O
class	O
case	O
also	O
the	O
attributes	O
xi	O
are	O
often	O
taken	O
to	O
be	O
binary	O
as	O
we	O
shall	O
do	O
initially	O
below	O
as	O
well	O
the	O
extension	O
to	O
more	O
than	O
two	O
attribute	O
states	O
or	O
continuous	B
attributes	O
is	O
straightforward	O
example	O
ezsurvey	O
org	O
considers	O
radio	O
station	O
listeners	O
conveniently	O
fall	O
into	O
two	O
groups	O
the	O
young	O
and	O
old	O
they	O
assume	O
that	O
given	O
the	O
knowledge	O
that	O
a	O
customer	O
is	O
either	O
young	O
or	O
old	O
this	O
is	O
sufficient	O
to	O
determine	O
whether	O
or	O
not	O
a	O
customer	O
will	O
like	O
a	O
particular	O
radio	O
station	O
independent	O
of	O
their	O
likes	O
or	O
dislikes	O
for	O
any	O
other	O
stations	O
where	O
each	O
of	O
the	O
variables	O
can	O
take	O
the	O
states	O
either	O
like	O
or	O
dislike	O
and	O
the	O
age	O
variable	O
can	O
take	O
the	O
value	B
either	O
young	O
or	O
old	O
thus	O
the	O
information	O
about	O
the	O
age	O
of	O
the	O
customer	O
determines	O
the	O
individual	O
product	O
preferences	O
without	O
needing	O
to	O
know	O
anything	O
else	O
to	O
complete	O
the	O
specification	O
given	O
that	O
a	O
customer	O
is	O
young	O
she	O
has	O
a	O
chance	O
to	O
like	O
a	O
chance	O
to	O
like	O
a	O
chance	O
to	O
like	O
and	O
a	O
chance	O
to	O
like	O
similarly	O
an	O
old	O
listener	O
has	O
a	O
chance	O
to	O
like	O
an	O
chance	O
to	O
like	O
a	O
chance	O
to	O
like	O
and	O
a	O
chance	O
to	O
like	O
they	O
know	O
that	O
of	O
the	O
listeners	O
are	O
old	O
estimation	O
using	O
maximum	B
likelihood	B
c	O
cn	O
xn	O
i	O
c	O
ic	O
i	O
d	O
n	O
n	O
figure	O
naive	B
bayes	I
classifer	O
the	O
central	O
assumption	O
is	O
that	O
given	O
the	O
class	O
c	O
the	O
attributes	O
xi	O
assuming	O
the	O
data	O
is	O
i	O
i	O
d	O
are	O
independent	O
maximum	B
likelihood	B
learns	O
the	O
optimal	O
parameters	O
of	O
the	O
distribution	B
pc	O
and	O
the	O
class-dependent	O
attribute	O
distributions	O
pxic	O
given	O
this	O
model	B
and	O
a	O
new	O
customer	O
that	O
likes	O
and	O
but	O
dislikes	O
and	O
what	O
is	O
the	O
probability	O
that	O
they	O
are	O
young	O
this	O
is	O
given	O
by	O
page	O
like	O
dislike	O
like	O
dislike	O
like	O
dislike	O
like	O
dislikeage	O
youngpage	O
young	O
age	O
like	O
dislike	O
like	O
dislikeagepage	O
using	O
the	O
naive	B
bayes	I
structure	B
the	O
numerator	O
above	O
is	O
given	O
by	O
likeage	O
dislikeage	O
young	O
likeage	O
dislikeage	O
youngpage	O
young	O
plugging	O
in	O
the	O
values	O
we	O
obtain	O
the	O
denominator	O
is	O
given	O
by	O
this	O
value	B
plus	O
the	O
corresponding	O
term	O
evaluated	O
under	O
assuming	O
the	O
customer	O
is	O
old	O
which	O
gives	O
page	O
like	O
dislike	O
like	O
dislike	O
estimation	O
using	O
maximum	B
likelihood	B
learning	B
the	O
table	O
entries	O
for	O
nb	O
is	O
a	O
straightforward	O
application	O
of	O
the	O
more	O
general	O
bn	O
learning	B
discussed	O
in	O
for	O
a	O
fully	O
observed	O
dataset	O
maximum	B
likelihood	B
learning	B
of	O
the	O
table	O
entries	O
corresponds	O
to	O
counting	B
the	O
number	O
of	O
occurrences	O
in	O
the	O
training	B
data	O
as	O
we	O
show	O
below	O
binary	O
attributes	O
consider	O
a	O
dataset	O
n	O
n	O
of	O
binary	O
attributes	O
xn	O
i	O
i	O
d	O
each	O
datapoint	O
xn	O
has	O
an	O
associated	O
class	O
label	O
cn	O
the	O
number	O
of	O
datapoints	O
from	O
class	O
c	O
is	O
and	O
the	O
number	O
from	O
class	O
c	O
denoted	O
is	O
for	O
each	O
attribute	O
of	O
the	O
two	O
classes	O
we	O
need	O
to	O
estimate	O
the	O
values	O
pxi	O
c	O
i	O
the	O
other	O
probability	O
pxi	O
is	O
given	O
by	O
the	O
normalisation	B
requirement	O
pxi	O
pxi	O
c	O
i	O
draft	O
march	O
based	O
on	O
the	O
nb	O
conditional	B
independence	B
assumption	O
the	O
probability	O
of	O
observing	O
a	O
vector	O
x	O
can	O
be	O
compactly	O
written	O
estimation	O
using	O
maximum	B
likelihood	B
pxc	O
pxic	O
c	O
i	O
c	O
i	O
xi	O
in	O
the	O
above	O
expression	O
xi	O
is	O
either	O
or	O
and	O
hence	O
each	O
i	O
term	O
contributes	O
a	O
factor	B
c	O
i	O
if	O
xi	O
or	O
c	O
i	O
if	O
xi	O
together	O
with	O
the	O
assumption	O
that	O
the	O
training	B
data	O
is	O
i	O
i	O
d	O
generated	O
the	O
log	O
likelihood	B
of	O
the	O
attributes	O
and	O
class	O
labels	O
is	O
log	O
pxn	O
cn	O
log	O
n	O
i	O
pxn	O
i	O
i	O
log	O
cn	O
xn	O
i	O
xn	O
i	O
cn	O
i	O
log	O
pc	O
log	O
pc	O
this	O
can	O
be	O
written	O
more	O
explicitly	O
in	O
terms	O
of	O
the	O
parameters	O
as	O
n	O
l	O
l	O
in	O
in	O
i	O
cn	O
log	O
i	O
i	O
i	O
cn	O
i	O
i	O
i	O
cn	O
log	O
i	O
i	O
log	O
pc	O
log	O
pc	O
we	O
can	O
find	O
the	O
maximum	B
likelihood	B
optimal	O
c	O
i	O
by	O
differentiating	O
w	O
r	O
t	O
c	O
i	O
and	O
equating	O
to	O
zero	O
giving	O
i	O
i	O
cn	O
i	O
cn	O
c	O
i	O
n	O
c	O
i	O
pxi	O
i	O
i	O
cn	O
c	O
i	O
n	O
i	O
cn	O
c	O
number	O
of	O
times	O
xi	O
for	O
class	O
c	O
number	O
of	O
datapoints	O
in	O
class	O
c	O
similarly	O
optimising	O
equation	B
with	O
respect	O
to	O
pc	O
gives	O
pc	O
number	O
of	O
times	O
class	O
c	O
occurs	O
total	O
number	O
of	O
data	O
points	O
classification	B
boundary	B
we	O
classify	O
a	O
novel	O
input	O
x	O
as	O
class	O
if	O
pc	O
pc	O
using	O
bayes	O
rule	O
and	O
writing	O
the	O
log	O
of	O
the	O
above	O
expression	O
this	O
is	O
equivalent	B
to	O
i	O
log	O
px	O
log	O
pc	O
log	O
px	O
log	O
px	O
log	O
pc	O
log	O
px	O
from	O
the	O
definition	O
of	O
the	O
classifier	B
this	O
is	O
equivalent	B
to	O
normalisation	B
constant	I
log	O
px	O
can	O
be	O
dropped	O
from	O
both	O
sides	O
ic	O
log	O
pc	O
log	O
px	O
log	O
px	O
i	O
ic	O
log	O
pc	O
i	O
log	O
using	O
the	O
binary	O
encoding	O
xi	O
we	O
classify	O
x	O
as	O
class	O
if	O
i	O
log	O
this	O
decision	O
rule	O
can	O
be	O
expressed	O
in	O
the	O
form	O
classify	O
x	O
as	O
class	O
i	O
pc	O
i	O
i	O
x	O
i	O
i	O
i	O
x	O
i	O
wix	O
i	O
a	O
for	O
some	O
suitable	O
choice	O
of	O
weights	O
wi	O
and	O
constant	O
a	O
see	O
the	O
interpretation	O
is	O
that	O
w	O
specifies	O
a	O
hyperplane	B
in	O
the	O
attribute	O
space	O
and	O
x	O
is	O
classified	O
as	O
if	O
it	O
lies	O
on	O
the	O
positive	O
side	O
of	O
the	O
hyperplane	B
i	O
pc	O
i	O
draft	O
march	O
estimation	O
using	O
maximum	B
likelihood	B
figure	O
english	O
tastes	O
over	O
attributes	O
lager	O
whiskey	O
porridge	O
f	O
ootball	O
each	O
column	O
represents	O
the	O
tastes	O
of	O
an	O
individual	O
scottish	O
tastes	O
example	O
they	O
scottish	O
consider	O
the	O
following	O
vector	O
of	O
attributes	O
shortbread	O
likes	O
lager	O
drinks	O
whiskey	O
eats	O
porridge	O
watched	O
england	O
play	O
football	O
a	O
vector	O
x	O
would	O
describe	O
that	O
a	O
person	O
likes	O
shortbread	O
does	O
not	O
like	O
lager	O
drinks	O
whiskey	O
eats	O
porridge	O
and	O
has	O
not	O
watched	O
england	O
play	O
football	O
together	O
with	O
each	O
vector	O
x	O
there	O
is	O
a	O
label	O
nat	O
describing	O
the	O
nationality	O
of	O
the	O
person	O
domnat	O
english	O
see	O
we	O
wish	O
to	O
classify	O
the	O
vector	O
x	O
as	O
either	O
scottish	O
or	O
english	O
we	O
can	O
use	O
bayes	O
rule	O
to	O
calculate	O
the	O
probability	O
that	O
x	O
is	O
scottish	O
or	O
english	O
pscottishx	O
pxscottishpscottish	O
px	O
pxscottishpscottish	O
pxscottishpscottish	O
pxenglishpenglish	O
by	O
maximum	B
likelihood	B
the	O
prior	B
class	O
probability	O
pscottish	O
is	O
given	O
by	O
the	O
fraction	O
of	O
people	O
in	O
the	O
database	O
that	O
are	O
scottish	O
and	O
similarly	O
penglish	O
is	O
given	O
as	O
the	O
fraction	O
of	O
people	O
in	O
the	O
database	O
that	O
are	O
english	O
this	O
gives	O
pscottish	O
and	O
penglish	O
for	O
pxnat	O
under	O
the	O
naive	B
bayes	I
assumption	O
pxnat	O
so	O
that	O
knowing	O
whether	O
not	O
someone	O
is	O
scottish	O
we	O
don	O
t	O
need	O
to	O
know	O
anything	O
else	O
to	O
calculate	O
the	O
probability	O
of	O
their	O
likes	O
and	O
dislikes	O
based	O
on	O
the	O
table	O
in	O
and	O
using	O
maximum	B
likelihood	B
we	O
have	O
for	O
x	O
we	O
get	O
pscottishx	O
since	O
this	O
is	O
greater	O
than	O
we	O
would	O
classify	O
this	O
person	O
as	O
being	O
scottish	O
small	O
data	O
counts	O
in	O
consider	O
trying	O
to	O
classify	O
the	O
vector	O
x	O
in	O
the	O
training	B
data	O
all	O
scottish	O
people	O
say	O
they	O
like	O
shortbread	O
this	O
means	O
that	O
for	O
this	O
particular	O
x	O
px	O
scottish	O
and	O
therefore	O
that	O
we	O
make	O
the	O
extremely	O
confident	O
classification	B
pscottishx	O
this	O
demonstrates	O
a	O
difficulty	O
using	O
maximum	B
likelihood	B
with	O
sparse	B
data	O
one	O
way	O
to	O
ameliorate	O
this	O
is	O
to	O
smooth	O
the	O
probabilities	O
for	O
example	O
by	O
adding	O
a	O
certain	O
small	O
number	O
to	O
the	O
frequency	O
counts	O
of	O
each	O
attribute	O
this	O
ensures	O
that	O
draft	O
march	O
estimation	O
using	O
maximum	B
likelihood	B
there	O
are	O
no	O
zero	O
probabilities	O
in	O
the	O
model	B
an	O
alternative	O
is	O
to	O
use	O
a	O
bayesian	B
approach	B
that	O
discourages	O
extreme	O
probabilities	O
as	O
discussed	O
in	O
potential	B
pitfalls	O
with	O
encoding	O
in	O
many	O
off-the-shelf	O
packages	O
implementing	O
naive	B
bayes	I
binary	O
attributes	O
are	O
assumed	O
in	O
practice	O
however	O
the	O
case	O
of	O
non-binary	O
attributes	O
often	O
occurs	O
consider	O
the	O
following	O
attribute	O
age	O
in	O
a	O
survey	O
a	O
person	O
s	O
age	O
is	O
marked	O
down	O
using	O
the	O
variable	O
a	O
a	O
means	O
the	O
person	O
is	O
between	O
and	O
years	O
old	O
a	O
means	O
the	O
person	O
is	O
between	O
and	O
years	O
old	O
a	O
means	O
the	O
person	O
is	O
older	O
than	O
one	O
way	O
to	O
transform	O
the	O
variable	O
a	O
into	O
a	O
binary	O
representation	O
would	O
be	O
to	O
use	O
three	O
binary	O
variables	O
with	O
representing	O
a	O
a	O
a	O
respectively	O
this	O
is	O
called	O
of	O
m	O
coding	O
since	O
only	O
of	O
the	O
binary	O
variables	O
is	O
active	B
in	O
encoding	O
the	O
m	O
states	O
by	O
construction	B
means	O
that	O
the	O
variables	O
are	O
dependent	O
for	O
example	O
if	O
we	O
know	O
that	O
we	O
know	O
that	O
and	O
regardless	O
of	O
any	O
class	O
conditioning	B
these	O
variables	O
will	O
always	O
be	O
dependent	O
contrary	O
to	O
the	O
assumption	O
of	O
naive	B
bayes	I
a	O
correct	O
approach	B
is	O
to	O
use	O
variables	O
with	O
more	O
than	O
two	O
states	O
as	O
explained	O
in	O
multi-state	O
variables	O
for	O
a	O
variable	O
xi	O
with	O
more	O
than	O
two	O
states	O
domxi	O
s	O
the	O
likelihood	B
of	O
observing	O
a	O
state	O
xi	O
s	O
is	O
denoted	O
sc	O
pxi	O
sc	O
i	O
s	O
pxi	O
sc	O
for	O
a	O
set	O
of	O
data	O
vectors	O
xnn	O
n	O
belonging	O
to	O
class	O
c	O
under	O
the	O
i	O
i	O
d	O
assumption	O
the	O
likelihood	B
of	O
the	O
nb	O
model	B
generating	O
data	O
from	O
class	O
c	O
is	O
ixn	O
i	O
i	O
sc	O
which	O
gives	O
the	O
class	O
conditional	B
log-likelihood	O
l	O
i	O
i	O
s	O
i	O
c	O
log	O
i	O
sc	O
pxncn	O
we	O
can	O
optimize	O
with	O
respect	O
to	O
the	O
parameters	O
using	O
a	O
lagrange	O
multiplier	B
for	O
each	O
of	O
the	O
attributes	O
i	O
and	O
classes	O
c	O
to	O
ensure	O
normalisation	B
l	O
i	O
i	O
s	O
i	O
c	O
log	O
i	O
sc	O
i	O
sc	O
to	O
find	O
the	O
optimum	O
of	O
this	O
function	B
we	O
may	O
differentiate	O
with	O
respect	O
to	O
i	O
the	O
resulting	O
equation	B
we	O
obtain	O
sc	O
and	O
equate	O
to	O
zero	O
solving	B
c	O
i	O
i	O
i	O
s	O
i	O
c	O
sc	O
i	O
c	O
i	O
hence	O
by	O
normalisation	B
i	O
sc	O
pxi	O
sc	O
i	O
n	O
i	O
s	O
i	O
c	O
i	O
i	O
c	O
the	O
maximum	B
likelihood	B
setting	O
for	O
the	O
parameter	B
pxi	O
sc	O
equals	O
the	O
relative	O
number	O
of	O
times	O
that	O
attribute	O
i	O
is	O
in	O
state	O
s	O
for	O
class	O
c	O
draft	O
march	O
bayesian	B
naive	B
bayes	I
figure	O
bayesian	B
naive	B
bayes	I
with	O
a	O
factorised	B
prior	B
on	O
the	O
class	O
conditional	B
attribute	O
probabilities	O
pxi	O
sc	O
for	O
simplicity	O
we	O
assume	O
that	O
the	O
class	O
probability	O
c	O
pc	O
is	O
learned	O
with	O
maximum	B
likelihood	B
so	O
that	O
no	O
distribution	B
is	O
placed	O
over	O
this	O
parameter	B
c	O
n	O
n	O
cn	O
xn	O
i	O
ic	O
c	O
c	O
i	O
d	O
text	O
classification	B
consider	O
a	O
set	O
of	O
documents	O
about	O
politics	O
and	O
another	O
set	O
about	O
sport	O
our	O
interest	O
is	O
to	O
make	O
a	O
method	O
that	O
can	O
automatically	O
classify	O
a	O
new	O
document	O
as	O
pertaining	O
to	O
either	O
sport	O
or	O
politics	O
we	O
search	O
through	O
both	O
sets	O
of	O
documents	O
to	O
find	O
the	O
most	O
commonly	O
occurring	O
words	O
each	O
document	O
is	O
then	O
represented	O
by	O
a	O
dimensional	O
vector	O
representing	O
the	O
number	O
of	O
times	O
that	O
each	O
of	O
the	O
words	O
occurs	O
in	O
that	O
document	O
the	O
so	O
called	O
bag	B
of	I
words	I
representation	O
is	O
a	O
crude	O
representation	O
of	O
the	O
document	O
since	O
it	O
discards	O
word	O
order	O
a	O
naive	B
bayes	I
model	B
specifies	O
a	O
distribution	B
of	O
these	O
number	O
of	O
occurrences	O
pxic	O
where	O
xi	O
is	O
the	O
count	O
of	O
the	O
number	O
of	O
times	O
word	O
i	O
appears	O
in	O
documents	O
of	O
type	O
c	O
one	O
can	O
achieve	O
this	O
using	O
either	O
a	O
multistate	O
representation	O
discussed	O
in	O
or	O
using	O
a	O
continuous	B
xi	O
to	O
represent	O
the	O
frequency	O
of	O
word	O
i	O
in	O
the	O
document	O
in	O
this	O
case	O
pxic	O
could	O
be	O
conveniently	O
modelled	O
using	O
for	O
example	O
a	O
beta	B
distribution	B
intuitively	O
a	O
despite	O
the	O
simplicity	O
of	O
naive	B
bayes	I
it	O
can	O
classify	O
documents	O
surprisingly	O
potential	B
justification	O
for	O
the	O
conditional	B
independence	B
assumption	O
is	O
that	O
if	O
we	O
know	O
a	O
document	O
is	O
about	O
politics	O
this	O
is	O
a	O
good	O
indication	O
of	O
the	O
kinds	O
of	O
other	O
words	O
we	O
will	O
find	O
in	O
the	O
document	O
because	O
naive	B
bayes	I
is	O
a	O
reasonable	O
classifier	B
in	O
this	O
sense	O
and	O
has	O
minimal	O
storage	O
and	O
fast	O
training	B
it	O
has	O
been	O
applied	O
to	O
time-storage	O
critical	O
applications	O
such	O
as	O
automatically	O
classifying	O
webpages	O
into	O
and	O
spam	O
bayesian	B
naive	B
bayes	I
to	O
predict	O
the	O
class	O
c	O
of	O
an	O
input	O
x	O
we	O
use	O
pcxd	O
pxd	O
cpcd	O
pxd	O
cpcd	O
for	O
convenience	O
we	O
will	O
simply	O
set	O
pcd	O
using	O
maximum	B
likelihood	B
n	O
pcd	O
n	O
i	O
c	O
however	O
as	O
we	O
ve	O
seen	O
setting	O
the	O
parameters	O
of	O
pxd	O
c	O
using	O
maximum	B
likelihood	B
training	B
can	O
yield	O
over-confident	O
predictions	O
in	O
the	O
case	O
of	O
sparse	B
data	O
a	O
bayesian	B
approach	B
that	O
addresses	O
this	O
difficulty	O
sc	O
that	O
discourage	O
extreme	O
values	O
the	O
model	B
is	O
is	O
to	O
use	O
priors	O
on	O
the	O
probabilities	O
pxi	O
sc	O
i	O
depicted	O
in	O
the	O
prior	B
p	O
ic	O
we	O
will	O
use	O
a	O
prior	B
on	O
the	O
table	O
entries	O
and	O
make	O
the	O
global	B
factorisation	O
assumption	O
p	O
ic	O
draft	O
march	O
bayesian	B
naive	B
bayes	I
we	O
consider	O
discrete	B
xi	O
each	O
of	O
which	O
take	O
states	O
from	O
s	O
in	O
this	O
case	O
pxi	O
sc	O
corresponds	O
to	O
a	O
multinomial	B
distribution	B
for	O
which	O
the	O
conjugate	B
prior	B
is	O
a	O
dirichlet	B
distribution	B
under	O
the	O
factorised	B
prior	B
assumption	O
we	O
define	O
a	O
prior	B
for	O
each	O
attribute	O
i	O
and	O
class	O
c	O
p	O
ic	O
where	O
uic	O
is	O
the	O
hyperparameter	B
vector	O
of	O
the	O
dirichlet	B
distribution	B
for	O
table	O
pxic	O
the	O
posterior	B
first	O
let	O
s	O
see	O
how	O
the	O
bayesian	B
approach	B
is	O
used	O
to	O
classify	O
a	O
novel	O
point	O
x	O
let	O
d	O
denote	O
the	O
training	B
data	O
cn	O
n	O
n	O
from	O
equation	B
the	O
term	O
px	O
is	O
computed	O
using	O
the	O
following	O
decomposition	B
px	O
px	O
px	O
c	O
px	O
hence	O
in	O
order	O
to	O
make	O
a	O
prediction	O
we	O
require	O
the	O
parameter	B
posterior	B
consistent	B
with	O
our	O
general	O
bayesian	B
bn	O
training	B
result	O
in	O
the	O
parameter	B
posterior	B
factorises	O
for	O
dirichlet	B
hyperparameters	O
uic	O
the	O
above	O
equation	B
updates	O
the	O
hyperparameter	B
by	O
the	O
number	O
of	O
times	O
variable	O
i	O
is	O
in	O
state	O
s	O
for	O
class	O
c	O
data	O
a	O
common	O
default	O
setting	O
is	O
to	O
take	O
all	O
components	O
of	O
u	O
to	O
be	O
px	O
id	O
c	O
px	O
i	O
s	O
i	O
sc	O
i	O
p	O
where	O
p	O
ic	O
p	O
ic	O
p	O
ic	O
ic	O
uic	O
uic	O
where	O
the	O
vector	O
uic	O
has	O
components	O
ncnc	O
p	O
ic	O
i	O
i	O
s	O
s	O
ui	O
sc	O
pxn	O
i	O
ic	O
ncnc	O
by	O
conjugacy	O
the	O
posterior	B
for	O
class	O
c	O
is	O
a	O
dirichlet	B
distribution	B
classification	B
the	O
class	O
distribution	B
is	O
given	O
by	O
pc	O
to	O
compute	O
px	O
c	O
pc	O
c	O
we	O
use	O
i	O
s	O
px	O
px	O
i	O
sd	O
c	O
using	O
the	O
general	O
identity	O
i	O
pc	O
c	O
sdirichlet	O
d	O
zu	O
zu	O
us	O
s	O
u	O
s	O
us	O
s	O
draft	O
march	O
where	O
zu	O
is	O
the	O
normalisation	B
constant	I
of	O
the	O
dirichlet	B
distribution	B
dirichlet	B
and	O
figure	O
a	O
chow-liu	B
tree	B
in	O
which	O
each	O
variable	O
xi	O
has	O
at	O
most	O
one	O
parent	O
the	O
variables	O
may	O
be	O
indexed	O
such	O
that	O
i	O
d	O
tree	B
augmented	B
naive	B
bayes	I
we	O
obtain	O
pc	O
pc	O
i	O
zu	O
ic	O
z	O
uic	O
where	O
i	O
s	O
u	O
ui	O
sc	O
i	O
i	O
s	O
example	O
naive	B
bayes	I
repeating	O
the	O
previous	O
analysis	B
for	O
the	O
are	O
they	O
scottish	O
data	O
from	O
the	O
probability	O
under	O
a	O
uniform	B
dirichlet	B
prior	B
for	O
all	O
the	O
tables	O
gives	O
a	O
value	B
of	O
for	O
the	O
probability	O
that	O
is	O
scottish	O
compared	O
with	O
a	O
value	B
of	O
under	O
the	O
standard	O
naive	B
bayes	I
assumption	O
tree	B
augmented	B
naive	B
bayes	I
a	O
natural	B
extension	O
of	O
naive	B
bayes	I
is	O
to	O
relax	O
the	O
assumption	O
that	O
the	O
attributes	O
are	O
independent	O
given	O
the	O
class	O
pxc	O
pxic	O
the	O
question	O
then	O
arises	O
which	O
structure	B
should	O
we	O
choose	O
for	O
pxc	O
as	O
we	O
saw	O
in	O
learning	B
a	O
structure	B
is	O
computationally	O
infeasible	O
for	O
all	O
but	O
very	O
small	O
numbers	O
of	O
attributes	O
a	O
practical	O
algorithm	B
requires	O
a	O
specific	O
form	O
of	O
constraint	O
on	O
the	O
structure	B
to	O
do	O
this	O
we	O
first	O
make	O
a	O
digression	O
into	O
the	O
maximum	B
likelihood	B
learning	B
of	O
trees	O
constrained	O
to	O
have	O
at	O
most	O
a	O
single	O
parent	O
chow-liu	B
trees	O
consider	O
a	O
multivariate	B
distribution	B
px	O
that	O
we	O
wish	O
to	O
approximate	B
with	O
a	O
distribution	B
qx	O
furthermore	O
we	O
constrain	O
the	O
approximation	B
qx	O
to	O
be	O
a	O
belief	B
network	I
in	O
which	O
each	O
node	O
has	O
at	O
most	O
one	O
parent	O
first	O
we	O
assume	O
that	O
we	O
have	O
chosen	O
a	O
particular	O
labelling	O
of	O
the	O
variables	O
i	O
d	O
for	O
which	O
the	O
dag	O
single	O
parent	O
constraint	O
means	O
qx	O
qxixpai	O
pai	O
i	O
or	O
pai	O
where	O
pai	O
is	O
the	O
single	O
parent	O
index	O
of	O
node	O
i	O
to	O
find	O
the	O
best	O
approximating	O
distribution	B
q	O
in	O
this	O
constrained	O
class	O
we	O
may	O
minimise	O
the	O
kullback-leibler	B
divergence	B
klpq	O
since	O
px	O
is	O
fixed	O
the	O
first	O
term	O
is	O
constant	O
by	O
adding	O
a	O
pxixpai	O
on	O
px	O
alone	O
we	O
can	O
write	O
pxixpai	O
that	O
depends	O
pxixpai	O
pxixpai	O
pxpai	O
klpq	O
const	O
draft	O
march	O
tree	B
augmented	B
naive	B
bayes	I
algorithm	B
chow-liu	B
trees	O
end	O
for	O
for	O
j	O
to	O
d	O
do	O
compute	O
the	O
mutual	B
information	I
for	O
the	O
pair	O
of	O
variables	O
xi	O
xj	O
wij	O
mixi	O
xj	O
for	O
i	O
to	O
d	O
do	O
end	O
for	O
for	O
the	O
undirected	B
graph	B
g	O
with	O
edge	O
weights	O
w	O
find	O
a	O
maximum	O
weight	B
undirected	B
spanning	B
tree	B
t	O
choose	O
an	O
arbitrary	O
variable	O
as	O
the	O
root	O
node	O
of	O
the	O
tree	B
t	O
form	O
a	O
directed	B
tree	B
by	O
orienting	O
all	O
edges	O
away	O
from	O
the	O
root	O
node	O
this	O
enables	O
us	O
to	O
recognise	O
that	O
up	O
to	O
a	O
negligible	O
constant	O
the	O
overall	O
kullback-leibler	B
divergence	B
is	O
a	O
positive	O
sum	O
of	O
individual	O
kullback-leibler	O
divergences	O
so	O
that	O
the	O
optimal	O
setting	O
is	O
therefore	O
plugging	O
this	O
solution	O
into	O
equation	B
and	O
using	O
log	O
pxixpai	O
log	O
pxi	O
xpai	O
log	O
pxpai	O
we	O
obtain	O
qxixpai	O
pxixpai	O
klpq	O
const	O
pxi	O
pxixpai	O
pxpai	O
we	O
still	O
need	O
to	O
find	O
the	O
optimal	O
parental	O
structure	B
pai	O
that	O
minimises	O
the	O
above	O
expression	O
if	O
we	O
add	O
and	O
subtract	O
an	O
entropy	B
term	O
we	O
can	O
write	O
pxi	O
klpq	O
pxixpai	O
pxpai	O
const	O
for	O
two	O
variables	O
xi	O
and	O
xj	O
and	O
distribution	B
pxi	O
xj	O
the	O
mutual	B
information	I
can	O
be	O
written	O
as	O
mixi	O
xj	O
log	O
pxi	O
xj	O
pxipxj	O
pxixj	O
which	O
can	O
be	O
seen	O
as	O
the	O
kullback-leibler	B
divergence	B
klpxi	O
xjpxipxj	O
and	O
is	O
therefore	O
nonnegative	O
using	O
this	O
equation	B
is	O
xpai	O
xpai	O
klpq	O
since	O
our	O
task	O
is	O
to	O
find	O
the	O
parental	O
indices	O
pai	O
and	O
the	O
entropic	O
const	O
i	O
is	O
independent	O
of	O
this	O
mapping	O
finding	O
the	O
optimal	O
mapping	O
is	O
equivalent	B
to	O
maximising	O
the	O
summed	O
mutual	O
informations	O
under	O
the	O
constraint	O
that	O
pai	O
i	O
since	O
we	O
also	O
need	O
to	O
choose	O
the	O
optimal	O
initial	O
labelling	O
of	O
the	O
variables	O
as	O
well	O
the	O
problem	B
is	O
equivalent	B
to	O
computing	O
all	O
the	O
pairwise	B
mutual	O
informations	O
wij	O
mixi	O
xj	O
and	O
then	O
finding	O
a	O
maximal	O
spanning	B
tree	B
for	O
the	O
graph	B
with	O
edge	O
weights	O
w	O
spantree	O
m	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
once	O
found	O
we	O
need	O
to	O
identify	O
a	O
directed	B
tree	B
with	O
at	O
most	O
one	O
parent	O
this	O
is	O
achieved	O
by	O
choosing	O
an	O
arbitrary	O
node	O
and	O
then	O
orienting	O
edges	O
consistently	O
away	O
from	O
this	O
node	O
draft	O
march	O
tree	B
augmented	B
naive	B
bayes	I
c	O
figure	O
tree	B
augmented	B
naive	B
bayes	I
each	O
variable	O
xi	O
has	O
at	O
most	O
one	O
parent	O
the	O
maximum	B
likelihood	B
optimal	O
tan	O
structure	B
is	O
computed	O
using	O
a	O
modified	O
chow-liu	B
algorithm	B
in	O
which	O
the	O
conditional	B
mutual	B
information	I
mixi	O
xjc	O
is	O
computed	O
for	O
all	O
i	O
j	O
a	O
maximum	O
weight	B
spanning	B
tree	B
is	O
then	O
found	O
and	O
turned	O
into	O
a	O
directed	B
graph	B
by	O
orienting	O
the	O
edges	O
outwards	O
from	O
a	O
chosen	O
root	O
node	O
the	O
table	O
entries	O
can	O
then	O
be	O
read	O
off	O
using	O
the	O
usual	O
maximum	B
likelihood	B
counting	B
argument	O
maximum	B
likelihood	B
chow-liu	B
trees	O
if	O
px	O
is	O
the	O
empirical	B
distribution	B
px	O
n	O
then	O
xn	O
klpq	O
const	O
n	O
n	O
log	O
qxn	O
hence	O
the	O
approximation	B
q	O
that	O
minimises	O
the	O
kullback-leibler	B
divergence	B
between	O
the	O
empirical	B
distribution	B
and	O
p	O
is	O
equivalent	B
to	O
that	O
which	O
maximises	O
the	O
likelihood	B
of	O
the	O
data	O
this	O
means	O
that	O
if	O
we	O
use	O
the	O
mutual	B
information	I
found	O
from	O
the	O
empirical	B
distribution	B
with	O
pxi	O
a	O
xj	O
b	O
a	O
xj	O
b	O
then	O
the	O
chow-liu	B
tree	B
produced	O
corresponds	O
to	O
the	O
maximum	B
likelihood	B
solution	O
amongst	O
all	O
singleparent	O
trees	O
an	O
outline	O
of	O
the	O
procedure	O
is	O
given	O
in	O
an	O
efficient	O
algorithm	B
for	O
sparse	B
data	O
is	O
also	O
remark	O
tree	B
structured	B
belief	B
networks	I
the	O
chow-liu	B
algorithm	B
pertains	O
to	O
the	O
discussion	O
in	O
on	O
learning	B
the	O
structure	B
of	O
belief	B
networks	I
from	O
data	O
under	O
the	O
special	O
constraint	O
that	O
each	O
variable	O
has	O
at	O
most	O
one	O
parent	O
the	O
chow-liu	B
algorithm	B
returns	O
the	O
maximum	B
likelihood	B
structure	B
to	O
fit	O
the	O
data	O
learning	B
tree	B
augmented	B
naive	B
bayes	I
networks	O
for	O
a	O
distribution	B
pxc	O
of	O
the	O
form	O
of	O
a	O
tree	B
structure	B
with	O
a	O
single-parent	O
constraint	O
we	O
can	O
readily	O
find	O
the	O
class	O
conditional	B
maximum	B
likelihood	B
solution	O
by	O
computing	O
the	O
chow-liu	B
tree	B
for	O
each	O
class	O
one	O
then	O
adds	O
links	O
from	O
the	O
class	O
node	O
c	O
to	O
each	O
variable	O
and	O
learns	O
the	O
class	O
conditional	B
probabilities	O
from	O
c	O
to	O
x	O
which	O
can	O
be	O
read	O
off	O
for	O
maximum	B
likelihood	B
using	O
the	O
usual	O
counting	B
argument	O
note	O
that	O
this	O
would	O
generally	O
result	O
in	O
a	O
different	O
chow-liu	B
tree	B
for	O
each	O
class	O
practitioners	O
typically	O
constrain	O
the	O
network	O
to	O
have	O
the	O
same	O
structure	B
for	O
all	O
classes	O
the	O
maximum	B
likelihood	B
objective	O
under	O
the	O
tan	O
constraint	O
then	O
corresponds	O
to	O
maximising	O
the	O
conditional	B
mutual	O
mixi	O
xjc	O
see	O
once	O
the	O
structure	B
is	O
learned	O
one	O
subsequently	O
sets	O
parameters	O
by	O
maximum	B
likelihood	B
counting	B
techniques	O
to	O
prevent	O
overfitting	B
are	O
discussed	O
in	O
and	O
can	O
be	O
addressed	O
using	O
dirichlet	B
priors	O
as	O
for	O
the	O
simpler	O
naive	B
bayes	I
structure	B
one	O
can	O
readily	O
consider	O
less	O
restrictive	O
structures	O
than	O
single-parent	O
belief	B
networks	I
however	O
the	O
complexity	O
of	O
finding	O
optimal	O
bn	O
structures	O
is	O
generally	O
computationally	O
infeasible	O
and	O
heuristics	O
are	O
required	O
to	O
limit	O
the	O
search	O
space	O
draft	O
march	O
exercises	O
code	O
naivebayestrain	O
m	O
naive	B
bayes	I
trained	O
with	O
maximum	B
likelihood	B
naivebayestest	O
m	O
naive	B
bayes	I
test	O
naivebayesdirichlettrain	O
m	O
naive	B
bayes	I
trained	O
with	O
bayesian	B
dirichlet	B
naivebayesdirichlettest	O
m	O
naive	B
bayes	I
testing	O
with	O
bayesian	B
dirichlet	B
demonaivebayes	O
m	O
demo	O
of	O
naive	B
bayes	I
exercises	O
exercise	O
a	O
local	B
supermarket	O
specializing	O
in	O
breakfast	O
cereals	O
decides	O
to	O
analyze	O
the	O
buying	O
patterns	O
of	O
its	O
customers	O
they	O
make	O
a	O
small	O
survey	O
asking	O
randomly	O
chosen	O
people	O
their	O
age	O
or	O
younger	O
than	O
years	O
and	O
which	O
of	O
the	O
breakfast	O
cereals	O
frosties	O
sugar	O
puffs	O
branflakes	O
they	O
like	O
each	O
respondent	O
provides	O
a	O
vector	O
with	O
entries	O
or	O
corresponding	O
to	O
whether	O
they	O
like	O
or	O
dislike	O
the	O
cereal	O
thus	O
a	O
respondent	O
with	O
would	O
like	O
cornflakes	O
frosties	O
and	O
branflakes	O
but	O
not	O
sugar	O
puffs	O
the	O
older	O
than	O
years	O
respondents	O
provide	O
the	O
following	O
data	O
the	O
younger	O
than	O
years	O
old	O
respondents	O
responded	O
a	O
novel	O
customer	O
comes	O
into	O
the	O
supermarket	O
and	O
says	O
she	O
only	O
likes	O
frosties	O
and	O
sugar	O
puffs	O
using	O
naive	B
bayes	I
trained	O
with	O
maximum	B
likelihood	B
what	O
is	O
the	O
probability	O
that	O
she	O
is	O
younger	O
than	O
exercise	O
a	O
psychologist	O
does	O
a	O
small	O
survey	O
on	O
happiness	O
each	O
respondent	O
provides	O
a	O
vector	O
with	O
entries	O
or	O
corresponding	O
to	O
whether	O
they	O
answer	O
yes	O
to	O
a	O
question	O
or	O
no	O
respectively	O
the	O
question	O
vector	O
has	O
attributes	O
x	O
married	O
healthy	O
thus	O
a	O
response	O
would	O
indicate	O
that	O
the	O
respondent	O
was	O
rich	O
unmarried	O
healthy	O
in	O
addition	O
each	O
respondent	O
gives	O
a	O
value	B
c	O
if	O
they	O
are	O
content	O
with	O
their	O
lifestyle	O
and	O
c	O
if	O
they	O
are	O
not	O
the	O
following	O
responses	O
were	O
obtained	O
from	O
people	O
who	O
claimed	O
also	O
to	O
be	O
content	O
and	O
for	O
not	O
content	O
using	O
naive	B
bayes	I
what	O
is	O
the	O
probability	O
that	O
a	O
person	O
who	O
is	O
not	O
rich	O
married	O
and	O
healthy	O
is	O
content	O
what	O
is	O
the	O
probability	O
that	O
a	O
person	O
who	O
is	O
not	O
rich	O
and	O
married	O
is	O
content	O
is	O
we	O
do	O
not	O
know	O
whether	O
or	O
not	O
they	O
are	O
healthy	O
consider	O
the	O
following	O
vector	O
of	O
attributes	O
if	O
customer	O
is	O
younger	O
than	O
otherwise	O
if	O
customer	O
is	O
between	O
and	O
years	O
old	O
otherwise	O
if	O
customer	O
is	O
older	O
than	O
otherwise	O
if	O
customer	O
walks	O
to	O
work	O
otherwise	O
each	O
vector	O
of	O
attributes	O
has	O
an	O
associated	O
class	O
label	O
rich	O
or	O
poor	O
point	O
out	O
any	O
potential	B
difficulties	O
with	O
using	O
your	O
previously	O
described	O
approach	B
to	O
training	B
using	O
naive	B
bayes	I
hence	O
describe	O
how	O
to	O
extend	O
your	O
previous	O
naive	B
bayes	I
method	O
to	O
deal	O
with	O
this	O
dataset	O
exercise	O
whizzco	O
decide	O
to	O
make	O
a	O
text	O
classifier	B
to	O
begin	O
with	O
they	O
attempt	O
to	O
classify	O
documents	O
as	O
either	O
sport	O
or	O
politics	O
they	O
decide	O
to	O
represent	O
each	O
document	O
as	O
a	O
vector	O
of	O
attributes	O
describing	O
the	O
presence	O
or	O
absence	O
of	O
words	O
x	O
football	O
golf	O
defence	O
offence	O
wicket	O
office	O
strategy	O
draft	O
march	O
exercises	O
training	B
data	O
from	O
sport	O
documents	O
and	O
from	O
politics	O
documents	O
is	O
represented	O
below	O
in	O
matlab	O
using	O
a	O
matrix	B
in	O
which	O
each	O
row	O
represents	O
the	O
attributes	O
politics	O
sport	O
using	O
a	O
naive	B
bayes	I
classifier	B
what	O
is	O
the	O
probability	O
that	O
the	O
document	O
x	O
is	O
about	O
politics	O
exercise	O
a	O
naive	B
bayes	I
classifier	B
for	O
binary	O
attributes	O
xi	O
is	O
parameterised	O
by	O
i	O
pxi	O
i	O
pxi	O
and	O
pclass	O
and	O
pclass	O
show	O
that	O
the	O
decision	B
boundary	B
to	O
classify	O
a	O
datapoint	O
x	O
can	O
be	O
written	O
as	O
wtx	O
b	O
and	O
state	O
explicitly	O
w	O
and	O
b	O
as	O
a	O
function	B
of	O
exercise	O
this	O
question	O
concerns	O
spam	B
filtering	I
each	O
email	O
is	O
represented	O
by	O
a	O
vector	O
x	O
xd	O
where	O
xi	O
each	O
entry	O
of	O
the	O
vector	O
indicates	O
if	O
a	O
particular	O
symbol	O
or	O
word	O
appears	O
in	O
the	O
email	O
the	O
symbolswords	O
are	O
money	B
cash	O
viagra	O
etc	O
so	O
that	O
if	O
the	O
word	O
cash	O
appears	O
in	O
the	O
email	O
the	O
training	B
dataset	O
consists	O
of	O
a	O
set	O
of	O
vectors	O
along	O
with	O
the	O
class	O
label	O
c	O
where	O
c	O
indicates	O
the	O
email	O
is	O
spam	O
and	O
c	O
not	O
spam	O
hence	O
the	O
training	B
set	O
consists	O
of	O
a	O
set	O
of	O
pairs	O
cn	O
n	O
n	O
the	O
naive	B
bayes	I
model	B
is	O
given	O
by	O
pc	O
x	O
pc	O
pxic	O
draw	O
a	O
belief	B
network	I
for	O
this	O
distribution	B
derive	O
expressions	O
for	O
the	O
parameters	O
of	O
this	O
model	B
in	O
terms	O
of	O
the	O
training	B
data	O
using	O
maximum	B
likelihood	B
assume	O
that	O
the	O
data	O
is	O
independent	O
and	O
identically	O
distributed	O
cn	O
xn	O
pcn	O
xn	O
explicitly	O
the	O
parameters	O
are	O
pc	O
pxi	O
pxi	O
i	O
d	O
given	O
a	O
trained	O
model	B
px	O
c	O
explain	O
how	O
to	O
form	O
a	O
classifier	B
pcx	O
if	O
viagra	O
never	O
appears	O
in	O
the	O
spam	O
training	B
data	O
discuss	O
what	O
effect	O
this	O
will	O
have	O
on	O
the	O
classifi	O
cation	O
for	O
a	O
new	O
email	O
that	O
contains	O
the	O
word	O
viagra	O
write	O
down	O
an	O
expression	O
for	O
the	O
decision	B
boundary	B
and	O
show	O
that	O
it	O
can	O
be	O
written	O
in	O
the	O
form	O
pc	O
pc	O
udxd	O
b	O
for	O
suitably	O
defined	O
u	O
and	O
b	O
draft	O
march	O
exercises	O
exercise	O
for	O
a	O
distribution	B
px	O
c	O
and	O
an	O
approximation	B
qx	O
c	O
show	O
that	O
when	O
px	O
c	O
corresponds	O
to	O
the	O
empirical	B
distribution	B
finding	O
qx	O
c	O
that	O
minimises	O
the	O
kullback-leibler	B
divergence	B
klpx	O
cqx	O
c	O
corresponds	O
to	O
maximum	B
likelihood	B
training	B
of	O
qx	O
c	O
exercise	O
consider	O
a	O
distribution	B
px	O
c	O
and	O
a	O
tree	B
augmented	B
approximation	B
qx	O
c	O
i	O
qxixpai	O
c	O
pai	O
i	O
or	O
pai	O
show	O
that	O
for	O
the	O
optimal	O
qx	O
c	O
constrained	O
as	O
above	O
the	O
solution	O
qx	O
c	O
that	O
minimises	O
klpx	O
cqx	O
c	O
when	O
plugged	O
back	O
into	O
the	O
kullback-leibler	O
expression	O
gives	O
as	O
a	O
function	B
of	O
the	O
parental	O
structure	B
klpx	O
cqx	O
c	O
i	O
log	O
pxi	O
xpaic	O
pxpaicpxic	O
const	O
pxixpaic	O
this	O
shows	O
that	O
under	O
the	O
single-parent	O
constraint	O
and	O
that	O
each	O
tree	B
qxc	O
has	O
the	O
same	O
structure	B
minimising	O
the	O
kullback-leibler	B
divergence	B
is	O
equivalent	B
to	O
maximising	O
the	O
sum	O
of	O
conditional	B
mutual	B
information	I
terms	O
exercise	O
write	O
a	O
matlab	O
routine	O
a	O
chowliux	O
where	O
x	O
is	O
a	O
d	O
n	O
data	O
matrix	B
containing	O
a	O
multivariate	B
datapoint	O
on	O
each	O
column	O
that	O
returns	O
a	O
chow-liu	B
maximum	B
likelihood	B
tree	B
for	O
x	O
the	O
tree	B
structure	B
is	O
to	O
be	O
returned	O
in	O
the	O
sparse	B
matrix	B
a	O
you	O
may	O
find	O
the	O
routine	O
spantree	O
m	O
useful	O
the	O
file	O
chowliudata	O
mat	O
contains	O
a	O
data	O
matrix	B
for	O
variables	O
use	O
your	O
routine	O
to	O
find	O
the	O
maximum	B
likelihood	B
chow	O
liu	O
tree	B
and	O
draw	O
a	O
picture	O
of	O
the	O
resulting	O
dag	O
with	O
edges	O
oriented	O
away	O
from	O
variable	O
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
learning	B
with	O
hidden	B
variables	I
hidden	B
variables	I
and	O
missing	B
data	I
in	O
practice	O
data	O
entries	O
are	O
often	O
missing	B
resulting	O
in	O
incomplete	O
information	O
to	O
specify	O
a	O
likelihood	B
observational	O
variables	O
may	O
be	O
split	O
into	O
visible	B
for	O
which	O
we	O
actually	O
know	O
the	O
state	O
and	O
missing	B
whose	O
states	O
would	O
nominally	O
be	O
known	O
but	O
are	O
missing	B
for	O
a	O
particular	O
datapoint	O
another	O
scenario	O
in	O
which	O
not	O
all	O
variables	O
in	O
the	O
model	B
are	O
observed	O
are	O
the	O
so-called	O
hidden	B
or	O
latent	B
variable	I
models	O
in	O
this	O
case	O
there	O
are	O
variables	O
which	O
are	O
essential	O
for	O
the	O
model	B
description	O
but	O
never	O
observed	O
for	O
example	O
the	O
underlying	O
physics	O
of	O
a	O
model	B
may	O
contain	O
latent	B
processes	O
which	O
are	O
essential	O
to	O
describe	O
the	O
model	B
but	O
cannot	O
be	O
directly	O
measured	O
why	O
hiddenmissing	O
variables	O
can	O
complicate	O
proceedings	O
in	O
learning	B
the	O
parameters	O
of	O
models	O
as	O
previously	O
described	O
in	O
we	O
assumed	O
we	O
have	O
complete	O
information	O
to	O
define	O
all	O
variables	O
of	O
the	O
joint	B
model	B
of	O
the	O
data	O
pv	O
consider	O
the	O
asbestos-smokingcancer	O
network	O
of	O
in	O
the	O
case	O
of	O
complete	O
data	O
the	O
likelihood	B
is	O
pvn	O
pan	O
sn	O
cn	O
pcnan	O
sn	O
cpan	O
apsn	O
s	O
which	O
is	O
factorised	B
in	O
terms	O
of	O
the	O
table	O
entry	O
parameters	O
we	O
exploited	O
this	O
property	O
to	O
show	O
that	O
table	O
entries	O
can	O
be	O
learned	O
by	O
considering	O
only	O
local	B
information	O
both	O
in	O
the	O
maximum	B
likelihood	B
and	O
bayesian	B
frameworks	O
now	O
consider	O
the	O
case	O
that	O
for	O
some	O
of	O
the	O
patients	O
only	O
partial	O
information	O
is	O
available	O
for	O
example	O
for	O
patient	O
n	O
with	O
record	O
vn	O
s	O
it	O
is	O
known	O
that	O
the	O
patient	O
has	O
cancer	O
and	O
is	O
a	O
smoker	O
but	O
whether	O
or	O
not	O
they	O
had	O
exposure	O
to	O
asbestos	O
is	O
unknown	O
since	O
we	O
can	O
only	O
use	O
the	O
visible	B
available	O
information	O
is	O
it	O
would	O
seem	O
reasonable	O
to	O
assess	O
parameters	O
using	O
the	O
marginal	B
likelihood	B
pcna	O
sn	O
cpa	O
apsn	O
s	O
pvn	O
pa	O
sn	O
cn	O
a	O
a	O
we	O
will	O
discuss	O
when	O
this	O
approach	B
is	O
valid	O
in	O
using	O
the	O
marginal	B
likelihood	B
may	O
result	O
in	O
computational	O
difficulties	O
since	O
equation	B
is	O
not	O
factorised	B
over	O
the	O
tables	O
this	O
means	O
that	O
the	O
likelihood	B
function	B
cannot	O
be	O
written	O
as	O
a	O
product	O
of	O
functions	O
one	O
for	O
each	O
separate	O
parameter	B
in	O
this	O
case	O
the	O
maximisation	B
of	O
the	O
likelihood	B
is	O
more	O
complex	O
since	O
the	O
parameters	O
of	O
different	O
tables	O
are	O
coupled	B
a	O
similar	O
complication	O
holds	O
for	O
bayesian	B
learning	B
as	O
we	O
saw	O
in	O
under	O
a	O
prior	B
factorised	B
over	O
each	O
cpt	O
the	O
posterior	B
is	O
also	O
factorised	B
however	O
in	O
the	O
case	O
of	O
unknown	O
asbestos	O
exposure	O
a	O
term	O
hidden	B
variables	I
and	O
missing	B
data	I
xvis	O
xinv	O
xvis	O
xinv	O
minv	O
minv	O
figure	O
missing	B
at	I
random	I
assumption	O
missing	B
completely	B
at	O
random	O
assumption	O
is	O
introduced	O
of	O
the	O
form	O
pvn	O
pcna	O
sn	O
cpa	O
apsn	O
s	O
psn	O
a	O
pcna	O
sn	O
cpa	O
a	O
a	O
which	O
cannot	O
be	O
written	O
as	O
a	O
product	O
of	O
a	O
functions	O
of	O
fs	O
sfa	O
afc	O
c	O
the	O
missing	B
variable	O
therefore	O
introduces	O
dependencies	O
in	O
the	O
posterior	B
parameter	B
distribution	B
making	O
the	O
posterior	B
more	O
complex	O
in	O
both	O
the	O
maximum	B
likelihood	B
and	O
bayesian	B
cases	O
one	O
has	O
a	O
well	O
defined	O
likelihood	B
function	B
of	O
the	O
table	O
parametersposterior	O
the	O
difficulty	O
is	O
therefore	O
not	O
conceptual	O
but	O
rather	O
computational	O
how	O
are	O
we	O
to	O
find	O
the	O
optimum	O
of	O
the	O
likelihoodsummarise	O
the	O
posterior	B
note	O
that	O
missing	B
data	I
does	O
not	O
always	O
make	O
the	O
parameter	B
posterior	B
non-factorised	O
for	O
example	O
if	O
the	O
cancer	O
state	O
is	O
unobserved	O
above	O
because	O
cancer	O
is	O
a	O
collider	B
with	O
no	O
descendants	O
the	O
conditional	B
distribution	B
simply	O
sums	O
to	O
and	O
one	O
is	O
left	O
with	O
a	O
factor	B
dependent	O
on	O
a	O
and	O
another	O
on	O
s	O
the	O
missing	B
at	I
random	I
assumption	O
under	O
what	O
circumstances	O
is	O
it	O
valid	O
to	O
use	O
the	O
marginal	B
likelihood	B
to	O
assess	O
parameters	O
we	O
partition	O
the	O
variables	O
x	O
into	O
those	O
that	O
are	O
visible	B
xvis	O
and	O
invisible	O
xinv	O
so	O
that	O
the	O
set	O
of	O
all	O
variables	O
can	O
be	O
written	O
x	O
xinv	O
for	O
the	O
visible	B
variables	O
we	O
have	O
an	O
observed	O
state	O
xvis	O
v	O
whereas	O
the	O
state	O
of	O
the	O
invisible	O
variables	O
is	O
unknown	O
we	O
use	O
an	O
indicator	O
minv	O
to	O
denote	O
that	O
the	O
state	O
of	O
the	O
invisible	O
variables	O
is	O
unknown	O
then	O
for	O
a	O
datapoint	O
which	O
contains	O
both	O
visible	B
and	O
invisible	O
information	O
pxvis	O
v	O
minv	O
xinv	O
xinv	O
pxvis	O
v	O
xinv	O
minv	O
pminv	O
v	O
xinv	O
v	O
xinv	O
if	O
we	O
assume	O
that	O
the	O
mechanism	O
which	O
generates	O
invisible	O
data	O
has	O
the	O
form	O
then	O
pminv	O
v	O
xinv	O
pminv	O
v	O
pxvis	O
v	O
minv	O
pminv	O
pxvis	O
v	O
xinv	O
xinv	O
pminv	O
vpxvis	O
v	O
only	O
the	O
term	O
pxvis	O
v	O
conveys	O
information	O
about	O
the	O
model	B
therefore	O
provided	O
the	O
mechanism	O
by	O
which	O
the	O
data	O
is	O
missing	B
depends	O
only	O
on	O
the	O
visible	B
states	O
we	O
may	O
simply	O
use	O
the	O
marginal	B
likelihood	B
to	O
assess	O
parameters	O
this	O
is	O
called	O
the	O
missing	B
at	I
random	I
assumption	O
example	O
missing	B
at	I
random	I
ezsurvey	O
org	O
stop	O
men	O
on	O
the	O
street	O
and	O
ask	O
them	O
their	O
favourite	O
colour	O
all	O
men	O
whose	O
favourite	O
colour	O
is	O
pink	O
decline	O
to	O
respond	O
to	O
the	O
question	O
for	O
any	O
other	O
colour	O
all	O
men	O
respond	O
to	O
the	O
question	O
based	O
on	O
the	O
data	O
ezsurvey	O
org	O
produce	O
a	O
histogram	O
of	O
men	O
s	O
favourite	O
colour	O
based	O
on	O
the	O
likelihood	B
of	O
the	O
visible	B
data	O
alone	O
confidently	O
stating	O
that	O
none	O
of	O
them	O
likes	O
pink	O
draft	O
march	O
hidden	B
variables	I
and	O
missing	B
data	I
for	O
simplicity	O
assume	O
there	O
are	O
only	O
three	O
colours	O
blue	O
green	O
and	O
pink	O
ezsurvey	O
org	O
attempts	O
to	O
find	O
the	O
histogram	O
with	O
probabilities	O
b	O
g	O
p	O
with	O
b	O
g	O
p	O
each	O
respondent	O
produces	O
a	O
visible	B
response	O
xc	O
with	O
domxc	O
green	O
pink	O
otherwise	O
mc	O
if	O
there	O
is	O
no	O
response	O
three	O
men	O
are	O
asked	O
their	O
favourite	O
colour	O
giving	O
data	O
c	O
c	O
c	O
missing	B
green	O
based	O
on	O
the	O
likelihood	B
of	O
the	O
visible	B
data	O
alone	O
we	O
have	O
the	O
log	O
likelihood	B
for	O
i	O
i	O
d	O
data	O
l	O
b	O
g	O
p	O
log	O
b	O
log	O
g	O
b	O
g	O
p	O
where	O
the	O
last	O
lagrange	O
term	O
ensures	O
normalisation	B
maximising	O
the	O
expression	O
we	O
arrive	O
at	O
b	O
g	O
p	O
the	O
unreasonable	O
result	O
that	O
ezsurvey	O
org	O
produce	O
is	O
due	O
to	O
not	O
accounting	O
correctly	O
for	O
the	O
mechanism	O
which	O
produces	O
the	O
data	O
the	O
correct	O
mechanism	O
that	O
generates	O
the	O
data	O
the	O
missing	B
data	I
is	O
blue	O
c	O
green	O
b	O
p	O
g	O
b	O
b	O
g	O
g	O
where	O
we	O
used	O
probability	O
that	O
the	O
favourite	O
colour	O
is	O
pink	O
maximising	O
the	O
likelihood	B
we	O
arrive	O
at	O
c	O
p	O
since	O
the	O
probability	O
that	O
a	O
datapoint	O
is	O
missing	B
is	O
the	O
same	O
as	O
the	O
b	O
g	O
p	O
as	O
we	O
would	O
expect	O
on	O
the	O
other	O
hand	O
if	O
there	O
is	O
another	O
visible	B
variable	O
t	O
denoting	O
the	O
time	O
of	O
day	O
and	O
the	O
probability	O
that	O
men	O
respond	O
to	O
the	O
question	O
depends	O
only	O
on	O
the	O
time	O
t	O
alone	O
example	O
the	O
missing	B
probability	O
is	O
high	O
during	O
rush	O
hour	O
then	O
we	O
may	O
indeed	O
treat	O
the	O
missing	B
data	I
as	O
missing	B
at	I
random	I
a	O
stronger	O
assumption	O
than	O
mar	O
is	O
pminv	O
v	O
xinv	O
pminv	O
which	O
is	O
called	O
missing	B
completely	B
at	O
random	O
this	O
applies	O
for	O
example	O
to	O
latent	B
variable	I
models	O
in	O
which	O
the	O
variable	O
state	O
is	O
always	O
missing	B
independent	O
of	O
anything	O
else	O
maximum	B
likelihood	B
throughout	O
the	O
remaining	O
discussion	O
we	O
will	O
assume	O
any	O
missing	B
data	I
is	O
mar	O
or	O
missing	B
completely	B
at	O
random	O
this	O
means	O
that	O
we	O
can	O
treat	O
any	O
unobserved	O
variables	O
by	O
summing	O
integrating	O
over	O
their	O
states	O
for	O
maximum	B
likelihood	B
we	O
learn	O
model	B
parameters	O
by	O
optimising	O
the	O
marginal	B
likelihood	B
pv	O
h	O
pv	O
h	O
with	O
respect	O
to	O
identifiability	B
issues	O
the	O
marginal	B
likelihood	B
objective	O
function	B
depends	O
on	O
the	O
parameters	O
only	O
through	O
pv	O
so	O
that	O
equivalent	B
parameter	B
solutions	O
may	O
exist	O
for	O
example	O
consider	O
a	O
latent	B
variable	I
problem	B
with	O
distribution	B
draft	O
march	O
expectation	B
maximisation	B
in	O
which	O
variable	O
is	O
never	O
observed	O
this	O
means	O
that	O
the	O
marginal	B
likelihood	B
only	O
depends	O
on	O
the	O
entry	O
given	O
a	O
maximum	B
likelihood	B
solution	O
we	O
can	O
then	O
always	O
find	O
an	O
equivalent	B
maximum	B
likelihood	B
solution	O
provided	O
in	O
other	O
cases	O
there	O
is	O
an	O
inherent	O
symmetry	O
in	O
the	O
parameter	B
space	O
of	O
the	O
marginal	B
likelihood	B
for	O
example	O
consider	O
the	O
network	O
over	O
binary	O
variables	O
pc	O
a	O
s	O
pca	B
spaps	O
our	O
aim	O
is	O
to	O
learn	O
the	O
table	O
pa	O
and	O
the	O
four	O
tables	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
where	O
we	O
used	O
a	O
to	O
denote	O
that	O
these	O
are	O
parameter	B
estimates	O
we	O
assume	O
that	O
we	O
have	O
missing	B
data	I
such	O
that	O
the	O
states	O
of	O
variable	O
a	O
are	O
never	O
observed	O
in	O
this	O
case	O
an	O
equivalent	B
solution	O
the	O
sense	O
that	O
it	O
has	O
the	O
same	O
marginal	B
likelihood	B
is	O
given	O
by	O
interchanging	O
the	O
states	O
of	O
a	O
pa	O
p	O
and	O
the	O
four	O
tables	O
p	O
p	O
s	O
pc	O
s	O
s	O
pc	O
s	O
p	O
p	O
s	O
pc	O
s	O
s	O
pc	O
s	O
a	O
similar	O
situation	O
occurs	O
in	O
a	O
more	O
general	O
setting	O
in	O
which	O
the	O
state	O
of	O
a	O
variable	O
is	O
consistently	O
unobserved	O
models	O
are	O
a	O
case	O
in	O
point	O
yielding	O
an	O
inherent	O
symmetry	O
in	O
the	O
solution	O
space	O
a	O
well	O
known	O
characteristic	O
of	O
maximum	B
likelihood	B
algorithms	O
is	O
that	O
jostling	O
occurs	O
in	O
the	O
initial	O
stages	O
of	O
training	B
in	O
which	O
these	O
symmetric	O
solutions	O
compete	O
expectation	B
maximisation	B
the	O
em	B
algorithm	B
is	O
a	O
convenient	O
and	O
general	O
purpose	O
iterative	O
approach	B
to	O
maximising	O
the	O
likelihood	B
under	O
missing	B
datahidden	O
it	O
is	O
generally	O
straightforward	O
to	O
implement	O
and	O
can	O
achieve	O
large	O
jumps	O
in	O
parameter	B
space	O
particularly	O
in	O
the	O
initial	O
iterations	O
variational	O
em	B
the	O
key	O
feature	O
of	O
the	O
em	B
algorithm	B
is	O
to	O
form	O
an	O
alternative	O
objective	O
function	B
for	O
which	O
the	O
parameter	B
coupling	O
effect	O
discussed	O
in	O
is	O
removed	O
meaning	O
that	O
individual	O
parameter	B
updates	O
can	O
be	O
achieved	O
akin	O
to	O
the	O
case	O
of	O
fully	O
observed	O
data	O
the	O
way	O
this	O
works	O
is	O
to	O
replace	O
the	O
marginal	B
likelihood	B
with	O
a	O
lower	O
bound	B
it	O
is	O
this	O
lower	O
bound	B
that	O
has	O
the	O
decoupled	O
form	O
we	O
first	O
consider	O
a	O
single	O
variable	O
pair	O
h	O
where	O
v	O
stands	O
for	O
visible	B
and	O
h	O
for	O
hidden	B
to	O
derive	O
the	O
bound	B
on	O
the	O
marginal	B
likelihood	B
consider	O
the	O
kullback-leibler	B
divergence	B
between	O
a	O
variational	O
distribution	B
qhv	O
and	O
the	O
parametric	O
model	B
phv	O
klqhvphv	O
qhv	O
log	O
phv	O
draft	O
march	O
expectation	B
maximisation	B
the	O
term	O
variational	O
refers	O
to	O
the	O
fact	O
that	O
this	O
distribution	B
will	O
be	O
a	O
parameter	B
of	O
an	O
optimisation	B
problem	B
using	O
bayes	O
rule	O
phv	O
ph	O
v	O
and	O
the	O
fact	O
that	O
pv	O
does	O
not	O
depend	O
on	O
h	O
ph	O
v	O
log	O
pv	O
klqhvphv	O
rearranging	O
we	O
obtain	O
a	O
bound	B
on	O
the	O
marginal	B
log	O
pv	O
ph	O
v	O
v	O
is	O
the	O
sum	O
of	O
the	O
individual	O
log	O
likelihoods	O
entropy	B
energy	B
the	O
bound	B
is	O
potentially	O
useful	O
since	O
it	O
is	O
similar	O
in	O
form	O
to	O
the	O
fully	O
observed	O
case	O
except	O
that	O
terms	O
with	O
missing	B
data	I
have	O
their	O
log	O
likelihood	B
weighted	O
by	O
a	O
prefactor	O
is	O
a	O
marginal	B
likelihood	B
bound	B
for	O
a	O
single	O
training	B
example	O
under	O
the	O
i	O
i	O
d	O
assumption	O
the	O
log	O
likelihood	B
of	O
all	O
training	B
data	O
log	O
pv	O
log	O
pvn	O
summing	O
over	O
the	O
training	B
data	O
we	O
obtain	O
a	O
bound	B
on	O
the	O
log	O
likelihood	B
log	O
pv	O
phn	O
vn	O
note	O
that	O
the	O
bound	B
is	O
exact	O
is	O
the	O
right	O
hand	O
side	O
is	O
equal	O
to	O
the	O
log	O
likelihood	B
when	O
we	O
set	O
qhnvn	O
phnvn	O
n	O
n	O
the	O
bound	B
suggests	O
an	O
iterative	O
procedure	O
to	O
optimise	O
e-step	B
for	O
fixed	O
find	O
the	O
distributions	O
qhnvn	O
that	O
maximise	O
equation	B
m-step	B
for	O
fixed	O
n	O
n	O
find	O
the	O
parameters	O
that	O
maximise	O
equation	B
classical	O
em	B
in	O
the	O
variational	O
e-step	B
above	O
the	O
fully	O
optimal	O
setting	O
is	O
qhnvn	O
phnvn	O
since	O
q	O
does	O
not	O
depend	O
on	O
new	O
the	O
m-step	B
is	O
equivalent	B
to	O
maximising	O
the	O
energy	B
term	O
alone	O
see	O
example	O
one-parameter	O
one-state	O
example	O
we	O
consider	O
a	O
model	B
small	O
enough	O
that	O
we	O
can	O
plot	O
fully	O
the	O
evolution	O
of	O
the	O
em	B
algorithm	B
the	O
model	B
is	O
on	O
a	O
single	O
visible	B
variable	O
v	O
and	O
single	O
two-state	O
hidden	B
variable	I
h	O
we	O
define	O
a	O
model	B
pv	O
h	O
pvhph	O
with	O
pvh	O
e	O
and	O
ph	O
ph	O
for	O
an	O
observation	O
v	O
and	O
our	O
interest	O
is	O
to	O
find	O
the	O
parameter	B
that	O
optimises	O
the	O
likelihood	B
e	O
pv	O
is	O
analogous	O
to	O
a	O
standard	O
partition	B
function	B
bound	B
in	O
statistical	O
physics	O
from	O
where	O
the	O
terminology	O
energy	B
and	O
entropy	B
hails	O
draft	O
march	O
expectation	B
maximisation	B
algorithm	B
expectation	B
maximisation	B
compute	O
maximum	B
likelihood	B
value	B
for	O
data	O
with	O
hidden	B
variables	I
input	O
a	O
distribution	B
px	O
and	O
dataset	O
v	O
returns	O
ml	O
setting	O
of	O
t	O
choose	O
an	O
initial	O
setting	O
for	O
the	O
parameters	O
while	O
not	O
converged	O
likelihood	B
not	O
converged	O
do	O
end	O
while	O
return	O
t	O
phn	O
vn	O
t	O
phnvn	O
t	O
run	O
over	O
all	O
datapoints	O
e	O
step	O
t	O
t	O
for	O
n	O
to	O
n	O
do	O
qn	O
end	O
for	O
t	O
arg	O
max	O
the	O
max	O
likelihood	B
parameter	B
estimate	O
t	O
m	O
step	O
iteration	B
counter	O
initialisation	O
figure	O
the	O
log	O
likelihood	B
for	O
the	O
model	B
described	O
in	O
contours	O
of	O
the	O
lower	O
bound	B
lbqh	O
for	O
an	O
initial	O
choice	O
qh	O
and	O
successive	O
updates	O
of	O
the	O
e	O
starting	O
at	O
the	O
em	B
algorithm	B
converges	O
and	O
m	O
steps	O
are	O
plotted	O
to	O
a	O
local	B
optimum	O
the	O
log	O
likelihood	B
is	O
plotted	O
in	O
with	O
optimum	O
at	O
the	O
em	B
procedure	O
iteratively	O
optimises	O
the	O
lower	O
bound	B
log	O
pv	O
lbqh	O
qh	O
log	O
qh	O
qh	O
log	O
qh	O
qh	O
log	O
where	O
qh	O
qh	O
from	O
an	O
initial	O
starting	O
the	O
em	B
algorithm	B
finds	O
the	O
q	O
distribution	B
that	O
optimises	O
lq	O
and	O
then	O
updates	O
depending	O
on	O
the	O
initial	O
the	O
solution	O
found	O
is	O
either	O
a	O
global	B
or	O
local	B
optimum	O
of	O
the	O
likelihood	B
see	O
the	O
m-step	B
is	O
easy	O
to	O
work	O
out	O
analytically	O
in	O
this	O
case	O
with	O
new	O
v	O
e-step	B
sets	O
qnewh	O
phv	O
so	O
that	O
qnewh	O
pv	O
pv	O
where	O
we	O
used	O
e	O
e	O
e	O
pv	O
pv	O
pv	O
qh	O
similarly	O
the	O
draft	O
march	O
logpv	O
expectation	B
maximisation	B
example	O
consider	O
a	O
simple	O
model	B
where	O
assuming	O
an	O
unconstrained	O
distribution	B
our	O
aim	O
is	O
to	O
learn	O
from	O
the	O
data	O
the	O
energy	B
term	O
for	O
the	O
classical	O
em	B
is	O
log	O
old	O
old	O
writing	O
out	O
fully	O
each	O
of	O
the	O
above	O
terms	O
on	O
a	O
separate	O
line	O
gives	O
the	O
energy	B
log	O
old	O
log	O
old	O
log	O
old	O
log	O
old	O
log	O
this	O
expression	O
resembles	O
the	O
standard	O
log	O
likelihood	B
of	O
fully	O
observed	O
data	O
except	O
that	O
terms	O
with	O
missing	B
data	I
have	O
their	O
weighted	O
log	O
parameters	O
the	O
parameters	O
are	O
conveniently	O
decoupled	O
in	O
this	O
bound	B
from	O
the	O
trivial	O
normalisation	B
constraint	O
so	O
that	O
finding	O
the	O
optimal	O
parameters	O
is	O
straightforward	O
this	O
is	O
achieved	O
by	O
the	O
m-step	B
update	O
which	O
gives	O
old	O
old	O
old	O
old	O
where	O
old	O
old	O
etc	O
the	O
e	O
and	O
m-steps	O
are	O
iterated	O
till	O
convergence	O
the	O
em	B
algorithm	B
increases	O
the	O
likelihood	B
whilst	O
by	O
construction	B
the	O
em	B
algorithm	B
cannot	O
decrease	O
the	O
bound	B
on	O
the	O
likelihood	B
an	O
important	O
question	O
is	O
whether	O
or	O
not	O
the	O
log	O
likelihood	B
itself	O
is	O
necessarily	O
increased	O
by	O
this	O
procedure	O
we	O
use	O
for	O
the	O
new	O
parameters	O
and	O
for	O
the	O
previous	O
parameters	O
in	O
two	O
consecutive	O
iterations	O
using	O
qhnvn	O
phnvn	O
we	O
see	O
that	O
as	O
a	O
function	B
of	O
the	O
parameters	O
the	O
lower	O
bound	B
for	O
a	O
single	O
variable	O
pair	O
h	O
depends	O
on	O
and	O
phv	O
ph	O
v	O
phv	O
lb	O
and	O
that	O
is	O
the	O
kullback-leibler	B
divergence	B
is	O
the	O
difference	O
between	O
the	O
lower	O
bound	B
and	O
the	O
true	O
likelihood	B
we	O
may	O
write	O
lb	O
log	O
pv	O
klphv	O
log	O
pv	O
lb	O
klphv	O
hence	O
log	O
pv	O
log	O
pv	O
lb	O
draft	O
march	O
lb	O
klphv	O
expectation	B
maximisation	B
s	O
c	O
figure	O
a	O
database	O
containing	O
information	O
about	O
being	O
a	O
smoker	O
signifies	O
the	O
individual	O
is	O
a	O
smoker	O
and	O
lung	O
cancer	O
signifies	O
the	O
individual	O
has	O
lung	O
cancer	O
each	O
row	O
contains	O
the	O
information	O
for	O
an	O
individual	O
so	O
that	O
there	O
are	O
individuals	O
in	O
the	O
database	O
the	O
first	O
assertion	O
is	O
true	O
since	O
by	O
definition	O
of	O
the	O
m-step	B
we	O
search	O
for	O
a	O
which	O
has	O
a	O
higher	O
value	B
for	O
the	O
bound	B
than	O
our	O
starting	O
value	B
the	O
second	O
assertion	O
is	O
true	O
by	O
the	O
property	O
of	O
the	O
kullback-leibler	B
divergence	B
for	O
more	O
than	O
a	O
single	O
datapoint	O
we	O
simply	O
sum	O
each	O
individual	O
bound	B
for	O
log	O
pvn	O
hence	O
we	O
reach	O
the	O
important	O
conclusion	O
that	O
the	O
em	B
algorithm	B
increases	O
not	O
only	O
the	O
lower	O
bound	B
on	O
the	O
marginal	B
likelihood	B
but	O
the	O
marginal	B
likelihood	B
itself	O
correctly	O
the	O
em	B
cannot	O
decrease	O
these	O
quantities	O
shared	O
parameters	O
and	O
tables	O
the	O
case	O
of	O
tables	O
sharing	O
parameters	O
is	O
essentially	O
straightforward	O
according	O
to	O
the	O
energy	B
term	O
we	O
need	O
to	O
identify	O
all	O
those	O
terms	O
in	O
which	O
the	O
shared	O
parameter	B
occurs	O
the	O
objective	O
for	O
the	O
shared	O
parameter	B
is	O
then	O
the	O
sum	O
over	O
all	O
energy	B
terms	O
containing	O
the	O
shared	O
parameter	B
application	O
to	O
belief	B
networks	I
conceptually	O
the	O
application	O
of	O
em	B
to	O
training	B
belief	B
networks	I
with	O
missing	B
data	I
is	O
straightforward	O
the	O
battle	O
is	O
more	O
notational	O
than	O
conceptual	O
we	O
begin	O
the	O
development	O
with	O
an	O
example	O
from	O
which	O
intuition	O
about	O
the	O
general	O
case	O
can	O
be	O
gleaned	O
example	O
consider	O
the	O
network	O
pa	O
c	O
s	O
pca	B
spaps	O
for	O
which	O
we	O
have	O
a	O
set	O
of	O
data	O
but	O
that	O
the	O
states	O
of	O
variable	O
a	O
are	O
never	O
observed	O
see	O
our	O
goal	O
is	O
to	O
learn	O
the	O
cpts	O
pca	B
s	O
and	O
pa	O
and	O
ps	O
to	O
apply	O
em	B
to	O
this	O
case	O
we	O
first	O
assume	O
initial	O
parameters	O
a	O
s	O
c	O
the	O
first	O
e-step	B
for	O
iteration	B
t	O
then	O
defines	O
a	O
set	O
of	O
distributions	O
on	O
the	O
hidden	B
variables	I
the	O
hidden	B
variable	I
is	O
a	O
pac	O
s	O
pac	O
s	O
and	O
so	O
on	O
for	O
the	O
training	B
examples	O
n	O
for	O
notational	O
convenience	O
we	O
write	O
qn	O
of	O
qn	O
t	O
in	O
place	O
t	O
we	O
now	O
move	O
to	O
the	O
first	O
m-step	B
the	O
energy	B
term	O
for	O
any	O
iteration	B
t	O
is	O
e	O
pcnan	O
sn	O
log	O
pan	O
log	O
pcnan	O
t	O
t	O
t	O
log	O
psn	O
the	O
final	O
term	O
is	O
the	O
log	O
likelihood	B
of	O
the	O
variable	O
s	O
and	O
ps	O
appears	O
explicitly	O
only	O
in	O
this	O
term	O
hence	O
the	O
usual	O
maximum	B
likelihood	B
rule	O
applies	O
and	O
ps	O
is	O
simply	O
given	O
by	O
the	O
relative	O
number	O
of	O
times	O
draft	O
march	O
expectation	B
maximisation	B
that	O
s	O
occurs	O
in	O
the	O
database	O
giving	O
ps	O
ps	O
n	O
log	O
pa	O
the	O
parameter	B
pa	O
occurs	O
in	O
the	O
terms	O
t	O
log	O
pa	O
qn	O
t	O
log	O
pa	O
which	O
using	O
the	O
normalisation	B
constraint	O
is	O
qn	O
t	O
pa	O
t	O
t	O
n	O
qn	O
t	O
n	O
qn	O
n	O
n	O
n	O
n	O
qn	O
n	O
qn	O
t	O
pa	O
qn	O
t	O
differentiating	O
with	O
respect	O
to	O
pa	O
and	O
solving	B
for	O
the	O
zero	O
derivative	O
we	O
get	O
that	O
is	O
whereas	O
in	O
the	O
standard	O
maximum	B
likelihood	B
estimate	O
we	O
would	O
have	O
the	O
real	O
counts	O
of	O
the	O
t	O
data	O
in	O
the	O
above	O
formula	O
here	O
they	O
have	O
been	O
replaced	O
with	O
our	O
guessed	O
values	O
qn	O
t	O
and	O
qn	O
a	O
similar	O
story	O
holds	O
for	O
pc	O
s	O
the	O
contribution	O
of	O
this	O
term	O
to	O
the	O
energy	B
is	O
t	O
log	O
pc	O
s	O
qn	O
qn	O
t	O
log	O
pc	O
s	O
which	O
is	O
log	O
pc	O
s	O
pc	O
s	O
pc	O
s	O
n	O
t	O
pc	O
s	O
qn	O
qn	O
t	O
n	O
i	O
i	O
qn	O
t	O
i	O
i	O
i	O
n	O
n	O
i	O
i	O
qn	O
t	O
i	O
i	O
qn	O
t	O
n	O
i	O
i	O
i	O
i	O
i	O
i	O
n	O
optimising	O
with	O
respect	O
to	O
pc	O
s	O
gives	O
for	O
comparison	O
the	O
setting	O
in	O
the	O
complete	O
data	O
case	O
is	O
there	O
is	O
an	O
intuitive	O
relationship	O
between	O
these	O
updates	O
in	O
the	O
missing	B
data	I
case	O
we	O
replace	O
the	O
indicators	O
by	O
the	O
assumed	O
distributions	O
q	O
iterating	O
the	O
e	O
and	O
m	O
steps	O
these	O
equations	O
will	O
converge	O
to	O
a	O
local	B
likelihood	B
optimum	O
to	O
minimise	O
the	O
notational	O
burden	O
we	O
assume	O
that	O
the	O
structure	B
of	O
the	O
missing	B
variables	O
is	O
fixed	O
throughout	O
this	O
being	O
equivalent	B
therefore	O
to	O
a	O
latent	B
variable	I
model	B
the	O
form	O
of	O
the	O
energy	B
term	O
for	O
belief	B
networks	I
n	O
n	O
i	O
it	O
is	O
useful	O
to	O
define	O
the	O
following	O
notation	O
t	O
qthvn	O
vn	O
qn	O
pxn	O
i	O
i	O
t	O
sets	O
the	O
visible	B
where	O
x	O
h	O
represents	O
all	O
the	O
variables	O
in	O
the	O
distribution	B
this	O
means	O
that	O
qn	O
variables	O
in	O
the	O
observed	O
state	O
and	O
defines	O
a	O
conditional	B
distribution	B
on	O
the	O
unobserved	O
variables	O
we	O
draft	O
march	O
expectation	B
maximisation	B
algorithm	B
em	B
for	O
belief	B
networks	I
tables	O
and	O
dataset	O
on	O
the	O
visible	B
variables	O
v	O
returns	O
the	O
maximum	B
likelihood	B
setting	O
of	O
tables	O
t	O
set	O
pt	O
to	O
initial	O
values	O
while	O
p	O
not	O
converged	O
likelihood	B
not	O
converged	O
do	O
input	O
a	O
bn	O
structure	B
pxipa	O
i	O
k	O
with	O
empty	O
iteration	B
counter	O
initialisation	O
t	O
pt	O
vn	O
qn	O
n	O
qn	O
t	O
t	O
t	O
for	O
n	O
to	O
n	O
do	O
end	O
for	O
for	O
i	O
to	O
k	O
do	O
end	O
for	O
end	O
while	O
return	O
ptxipa	O
qtx	O
qn	O
t	O
n	O
then	O
define	O
the	O
mixture	B
distribution	B
run	O
over	O
all	O
datapoints	O
e	O
step	O
run	O
over	O
all	O
variables	O
m	O
step	O
the	O
max	O
likelihood	B
parameter	B
estimate	O
the	O
energy	B
term	O
in	O
equation	B
can	O
be	O
written	O
more	O
compactly	O
as	O
n	O
n	O
n	O
n	O
to	O
see	O
this	O
consider	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
qthvn	O
vn	O
x	O
n	O
px	O
pxipa	O
n	O
using	O
the	O
structure	B
of	O
the	O
belief	B
network	I
we	O
have	O
qtxipa	O
pxipa	O
this	O
means	O
that	O
maximising	O
the	O
energy	B
is	O
equivalent	B
to	O
minimising	O
i	O
i	O
i	O
qtpaxi	O
n	O
pxipa	O
qtpaxi	O
where	O
we	O
added	O
the	O
constant	O
first	O
term	O
to	O
make	O
this	O
into	O
the	O
form	O
of	O
a	O
kullback-leibler	B
divergence	B
since	O
this	O
is	O
a	O
sum	O
of	O
independent	O
kullback-leibler	O
divergences	O
optimally	O
the	O
m-step	B
is	O
given	O
by	O
setting	O
pxipa	O
qtxipa	O
in	O
practice	O
storing	O
the	O
qtx	O
over	O
the	O
states	O
of	O
all	O
variables	O
x	O
is	O
prohibitively	O
expense	O
fortunately	O
since	O
the	O
m-step	B
only	O
requires	O
the	O
distribution	B
on	O
the	O
family	B
of	O
each	O
variable	O
xi	O
one	O
only	O
requires	O
the	O
local	B
distributions	O
qn	O
oldxipa	O
we	O
may	O
therefore	O
dispense	O
with	O
the	O
global	B
qoldx	O
and	O
equivalently	O
use	O
pnewxipa	O
oldxi	O
pa	O
n	O
qn	O
oldpa	O
using	O
the	O
em	B
algorithm	B
the	O
optimal	O
setting	O
for	O
the	O
e-step	B
is	O
to	O
use	O
qthnvn	O
poldhnvn	O
with	O
this	O
notation	O
the	O
em	B
algorithm	B
can	O
be	O
compactly	O
stated	O
as	O
in	O
see	O
also	O
embeliefnet	O
m	O
draft	O
march	O
expectation	B
maximisation	B
example	O
general	O
belief	B
networks	I
consider	O
a	O
five	O
variable	O
distribution	B
with	O
discrete	B
variables	O
in	O
which	O
the	O
variables	O
and	O
are	O
consistently	O
hidden	B
in	O
the	O
training	B
data	O
and	O
training	B
data	O
for	O
are	O
always	O
present	O
the	O
distribution	B
can	O
be	O
represented	O
as	O
a	O
belief	B
network	I
in	O
this	O
case	O
the	O
contributions	O
to	O
the	O
energy	B
have	O
the	O
form	O
which	O
may	O
be	O
written	O
as	O
n	O
pxn	O
n	O
pxn	O
n	O
pxn	O
n	O
n	O
n	O
log	O
pxn	O
a	O
useful	O
property	O
can	O
now	O
be	O
exploited	O
namely	O
that	O
each	O
term	O
depends	O
on	O
only	O
those	O
hidden	B
variables	I
in	O
the	O
family	B
that	O
that	O
term	O
represents	O
thus	O
we	O
may	O
write	O
n	O
pxn	O
n	O
pxn	O
n	O
n	O
n	O
log	O
pxn	O
the	O
final	O
term	O
can	O
be	O
set	O
using	O
maximum	B
likelihood	B
let	O
us	O
consider	O
therefore	O
a	O
more	O
difficult	O
table	O
when	O
will	O
the	O
table	O
entry	O
j	O
occur	O
in	O
the	O
energy	B
this	O
happens	O
whenever	O
xn	O
is	O
in	O
state	O
i	O
since	O
there	O
is	O
a	O
summation	O
over	O
all	O
the	O
states	O
of	O
variables	O
to	O
the	O
average	B
there	O
is	O
also	O
a	O
term	O
with	O
variable	O
in	O
state	O
j	O
hence	O
the	O
contribution	O
to	O
the	O
energy	B
from	O
terms	O
of	O
the	O
form	O
j	O
is	O
i	O
i	O
log	O
j	O
where	O
the	O
indicator	B
function	B
i	O
isation	O
of	O
the	O
table	O
we	O
add	O
a	O
lagrange	O
term	O
i	O
equals	O
if	O
xn	O
is	O
in	O
state	O
i	O
and	O
is	O
zero	O
otherwise	O
to	O
ensure	O
normal	B
k	O
j	O
n	O
n	O
i	O
i	O
log	O
j	O
differentiating	O
with	O
respect	O
to	O
j	O
we	O
get	O
n	O
or	O
hence	O
i	O
i	O
j	O
i	O
i	O
i	O
n	O
n	O
nk	O
j	O
j	O
i	O
i	O
k	O
draft	O
march	O
expectation	B
maximisation	B
figure	O
evolution	O
of	O
the	O
log-likelihood	O
versus	O
iterations	O
under	O
the	O
em	B
training	B
procedure	O
solving	B
the	O
printer	B
nightmare	I
with	O
missing	B
data	I
note	O
how	O
rapid	O
progress	O
is	O
made	O
at	O
the	O
beginning	O
but	O
convergence	O
can	O
be	O
slow	O
using	O
the	O
em	B
algorithm	B
we	O
have	O
jxn	O
xn	O
xn	O
this	O
optimal	O
distribution	B
is	O
easy	O
to	O
compute	O
since	O
this	O
is	O
the	O
marginal	B
on	O
the	O
family	B
given	O
some	O
evidential	O
variables	O
hence	O
the	O
m-step	B
update	O
for	O
the	O
table	O
is	O
what	O
about	O
the	O
table	O
j	O
to	O
ensure	O
normalisation	B
of	O
the	O
table	O
we	O
add	O
a	O
lagrange	O
term	O
i	O
i	O
n	O
nk	O
i	O
jxn	O
k	O
jxn	O
xn	O
xn	O
xn	O
xn	O
k	O
j	O
j	O
i	O
n	O
j	O
log	O
j	O
as	O
before	O
differentiating	O
and	O
using	O
the	O
em	B
settings	O
we	O
have	O
j	O
i	O
i	O
j	O
ixn	O
j	O
kxn	O
xn	O
xn	O
xn	O
xn	O
n	O
nk	O
j	O
i	O
i	O
i	O
j	O
and	O
equation	B
would	O
be	O
j	O
i	O
j	O
i	O
i	O
n	O
n	O
there	O
is	O
a	O
simple	O
intuitive	O
pattern	O
to	O
equation	B
and	O
equation	B
if	O
there	O
were	O
no	O
hidden	B
data	O
equation	B
would	O
read	O
all	O
that	O
we	O
do	O
therefore	O
in	O
the	O
general	O
em	B
case	O
is	O
to	O
replace	O
those	O
deterministic	B
functions	O
such	O
as	O
i	O
this	O
is	O
merely	O
a	O
restatement	O
of	O
the	O
general	O
update	O
given	O
in	O
equation	B
under	O
the	O
definition	O
i	O
by	O
their	O
missing	B
variable	O
equivalents	O
ixn	O
xn	O
xn	O
application	O
to	O
markov	O
networks	O
for	O
a	O
mn	O
defined	O
over	O
visible	B
and	O
hidden	B
variables	I
pv	O
h	O
is	O
z	O
log	O
pv	O
hqh	O
c	O
ch	O
v	O
log	O
z	O
c	O
ch	O
v	O
the	O
em	B
variational	O
bound	B
whilst	O
the	O
bound	B
decouples	O
the	O
parameters	O
in	O
the	O
second	O
term	O
the	O
parameters	O
are	O
nevertheless	O
coupled	B
in	O
the	O
normalisation	B
z	O
because	O
of	O
this	O
we	O
cannot	O
optimise	O
the	O
above	O
bound	B
on	O
a	O
parameter	B
by	O
parameter	B
basis	O
one	O
approach	B
is	O
to	O
use	O
an	O
additional	O
bound	B
log	O
z	O
from	O
above	O
as	O
for	O
iterative	B
scaling	I
draft	O
march	O
likelihood	B
extensions	O
of	O
em	B
convergence	O
convergence	O
of	O
em	B
can	O
be	O
slow	O
particularly	O
when	O
the	O
number	O
of	O
missing	B
observations	O
is	O
greater	O
than	O
the	O
number	O
of	O
visible	B
observations	O
in	O
practice	O
one	O
often	O
combines	O
the	O
em	B
with	O
gradient	B
based	O
procedures	O
to	O
improve	O
convergence	O
see	O
note	O
also	O
that	O
the	O
log	O
likelihood	B
is	O
typically	O
a	O
non-convex	O
function	B
of	O
the	O
parameters	O
this	O
means	O
that	O
there	O
may	O
be	O
multiple	O
local	B
optima	O
and	O
the	O
solution	O
found	O
often	O
depends	O
on	O
the	O
initialisation	O
extensions	O
of	O
em	B
partial	O
m	O
step	O
it	O
is	O
not	O
necessary	O
to	O
find	O
the	O
full	O
optimum	O
of	O
the	O
energy	B
term	O
at	O
each	O
iteration	B
as	O
long	O
as	O
one	O
finds	O
a	O
parameter	B
which	O
has	O
a	O
higher	O
energy	B
than	O
that	O
of	O
the	O
current	O
parameter	B
then	O
the	O
conditions	O
required	O
in	O
still	O
hold	O
and	O
the	O
likelihood	B
cannot	O
decrease	O
at	O
each	O
iteration	B
partial	O
e	O
step	O
phn	O
vn	O
the	O
e-step	B
requires	O
us	O
to	O
find	O
the	O
optimum	O
of	O
log	O
pv	O
with	O
respect	O
to	O
qhnvn	O
the	O
fully	O
optimal	O
setting	O
is	O
qhnvn	O
phnvn	O
for	O
a	O
guaranteed	O
increase	O
in	O
likelihood	B
at	O
each	O
iteration	B
from	O
we	O
required	O
that	O
the	O
fully	O
optimal	O
setting	O
of	O
q	O
is	O
used	O
unfortunately	O
therefore	O
one	O
cannot	O
in	O
general	O
guarantee	O
that	O
such	O
a	O
partial	O
e	O
step	O
would	O
always	O
increase	O
the	O
likelihood	B
of	O
course	O
it	O
is	O
guaranteed	O
to	O
increase	O
the	O
lower	O
bound	B
on	O
the	O
likelihood	B
though	O
not	O
the	O
likelihood	B
itself	O
intractable	B
energy	B
the	O
em	B
algorithm	B
assumes	O
that	O
we	O
can	O
calculate	O
ph	O
v	O
of	O
distributions	O
for	O
example	O
factorised	B
distributions	O
qhv	O
simpler	O
class	O
of	O
distributions	O
q	O
e	O
g	O
q	O
factorised	B
qhv	O
however	O
in	O
general	O
it	O
may	O
be	O
that	O
we	O
can	O
only	O
carry	O
out	O
the	O
average	B
over	O
q	O
for	O
a	O
very	O
restricted	B
class	O
j	O
qhjv	O
in	O
such	O
cases	O
one	O
may	O
use	O
a	O
i	O
qhiv	O
for	O
which	O
the	O
averaging	O
required	O
for	O
the	O
energy	B
may	O
be	O
simpler	O
we	O
can	O
find	O
the	O
best	O
distribution	B
in	O
class	O
q	O
by	O
minimising	O
the	O
kl	O
divergence	B
between	O
qhv	O
q	O
and	O
phv	O
numerically	O
using	O
a	O
non-linear	B
optimisation	B
routine	O
qopt	O
argmin	O
q	O
q	O
klqhphv	O
alternatively	O
one	O
can	O
assume	O
a	O
certain	O
structured	B
form	O
for	O
the	O
q	O
distribution	B
and	O
learn	O
the	O
optimal	O
factors	O
of	O
the	O
distribution	B
by	O
free	O
form	O
functional	O
calculus	B
viterbi	B
training	B
an	O
extreme	O
case	O
is	O
to	O
restrict	O
qhnvn	O
to	O
a	O
delta-function	O
in	O
this	O
case	O
the	O
entropic	O
term	O
is	O
constant	O
so	O
that	O
the	O
optimal	O
delta	O
function	B
q	O
is	O
to	O
set	O
qhnvn	O
hn	O
draft	O
march	O
where	O
hn	O
argmax	O
h	O
ph	O
vn	O
a	O
failure	B
case	I
for	O
em	B
this	O
is	O
called	O
viterbi	B
training	B
and	O
is	O
common	O
in	O
training	B
hmms	O
see	O
em	B
training	B
with	O
this	O
restricted	B
class	O
of	O
q	O
distribution	B
is	O
therefore	O
guaranteed	O
to	O
increase	O
the	O
lower	O
bound	B
on	O
the	O
log	O
likelihood	B
though	O
not	O
the	O
likelihood	B
itself	O
a	O
practical	O
advantage	O
of	O
viterbi	B
training	B
is	O
that	O
the	O
energy	B
is	O
always	O
tractable	O
to	O
compute	O
becoming	O
simply	O
log	O
phn	O
vn	O
which	O
is	O
amenable	O
to	O
optimisation	B
provided	O
there	O
is	O
sufficient	O
data	O
one	O
might	O
hope	O
that	O
the	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
parameter	B
will	O
be	O
sharply	O
peaked	O
around	O
the	O
optimum	O
value	B
this	O
means	O
that	O
at	O
convergence	O
the	O
approximation	B
of	O
the	O
posterior	B
phv	O
opt	O
by	O
a	O
delta	O
function	B
will	O
be	O
reasonable	O
and	O
an	O
update	O
of	O
em	B
using	O
viterbi	B
training	B
will	O
produce	O
a	O
new	O
approximately	O
the	O
same	O
as	O
opt	O
for	O
any	O
highly	O
suboptimal	O
however	O
phv	O
will	O
be	O
far	O
from	O
a	O
delta	O
function	B
and	O
therefore	O
a	O
viterbi	B
update	O
is	O
less	O
reliable	O
in	O
terms	O
of	O
leading	O
to	O
an	O
increase	O
in	O
the	O
likelihood	B
itself	O
this	O
suggests	O
that	O
the	O
initialisation	O
of	O
for	O
viterbi	B
training	B
is	O
more	O
critical	O
than	O
for	O
the	O
standard	O
em	B
stochastic	O
training	B
another	O
approximate	B
qhnvn	O
distribution	B
would	O
be	O
to	O
use	O
an	O
empirical	B
distribution	B
formed	O
by	O
samples	O
from	O
the	O
fully	O
optimal	O
distribution	B
phnvn	O
that	O
is	O
one	O
draws	O
samples	O
for	O
a	O
discussion	O
on	O
sampling	B
hn	O
hn	O
l	O
from	O
phnvn	O
and	O
forms	O
a	O
q	O
distribution	B
l	O
hn	O
l	O
the	O
energy	B
becomes	O
then	O
proportional	O
to	O
qhnvn	O
log	O
phn	O
l	O
vn	O
so	O
that	O
as	O
in	O
viterbi	B
training	B
the	O
energy	B
is	O
always	O
computationally	O
tractable	O
for	O
this	O
restricted	B
q	O
class	O
provided	O
that	O
the	O
samples	O
from	O
phnvn	O
are	O
reliable	O
stochastic	O
training	B
will	O
produce	O
an	O
energy	B
function	B
with	O
average	B
the	O
same	O
characteristics	O
as	O
the	O
true	O
energy	B
under	O
the	O
classical	O
em	B
algorithm	B
this	O
means	O
that	O
the	O
solution	O
obtained	O
from	O
stochastic	O
training	B
should	O
tend	O
to	O
that	O
from	O
the	O
classical	O
em	B
as	O
the	O
number	O
of	O
samples	O
increases	O
pv	O
h	O
fh	O
ph	O
if	O
we	O
attempt	O
an	O
em	B
approach	B
for	O
this	O
this	O
will	O
fail	O
also	O
for	O
a	O
more	O
general	O
model	B
of	O
the	O
form	O
a	O
failure	B
case	I
for	O
em	B
consider	O
a	O
likelihood	B
of	O
the	O
form	O
pv	O
the	O
e-step	B
is	O
p	O
ph	O
h	O
qh	O
old	O
p	O
old	O
ph	O
draft	O
march	O
variational	B
bayes	I
and	O
the	O
m-step	B
sets	O
new	O
argmax	O
pv	O
h	O
old	O
argmax	O
pvh	O
old	O
where	O
we	O
used	O
the	O
fact	O
that	O
for	O
this	O
model	B
ph	O
is	O
independent	O
of	O
in	O
the	O
case	O
that	O
p	O
fh	O
then	O
ph	O
old	O
fh	O
ph	O
so	O
that	O
optimising	O
the	O
energy	B
requires	O
new	O
argmax	O
fh	O
old	O
since	O
ph	O
old	O
is	O
zero	O
everywhere	O
expect	O
that	O
h	O
for	O
which	O
v	O
fh	O
then	O
the	O
energy	B
is	O
effectively	O
negative	O
infinity	O
if	O
old	O
however	O
when	O
old	O
the	O
energy	B
is	O
this	O
is	O
therefore	O
the	O
optimum	O
of	O
the	O
energy	B
and	O
represents	O
therefore	O
a	O
failure	O
in	O
updating	O
for	O
em	B
this	O
situation	O
occurs	O
in	O
practice	O
and	O
has	O
been	O
noted	O
in	O
particular	O
in	O
the	O
context	O
of	O
independent	O
component	O
one	O
can	O
attempt	O
to	O
heal	O
this	O
behaviour	O
by	O
deriving	O
an	O
em	B
algorithm	B
based	O
on	O
the	O
distribution	B
pvh	O
fh	O
where	O
nh	O
is	O
an	O
arbitrary	O
distribution	B
on	O
the	O
hidden	B
variable	I
h	O
the	O
original	O
deterministic	B
model	B
corresponds	O
to	O
defining	O
pvh	O
pv	O
h	O
we	O
have	O
pv	O
an	O
em	B
algorithm	B
for	O
pv	O
satisfies	O
pv	O
new	O
pv	O
old	O
new	O
old	O
which	O
implies	O
new	O
old	O
this	O
means	O
that	O
the	O
em	B
algorithm	B
for	O
the	O
non-deterministic	O
case	O
is	O
guaranteed	O
to	O
increase	O
the	O
likelihood	B
under	O
the	O
deterministic	B
model	B
at	O
each	O
iteration	B
we	O
are	O
at	O
convergence	O
see	O
for	O
an	O
application	O
of	O
this	O
antifreeze	B
technique	O
to	O
learning	B
markov	O
decision	O
processes	O
with	O
em	B
variational	B
bayes	I
as	O
discussed	O
in	O
maximum	B
likelihood	B
corresponds	O
to	O
a	O
form	O
of	O
bayesian	B
approach	B
in	O
which	O
the	O
parameter	B
posterior	B
distribution	B
a	O
flat	O
prior	B
is	O
approximated	O
with	O
a	O
delta	O
function	B
p	O
opt	O
variational	B
bayes	I
is	O
analogous	O
to	O
em	B
in	O
that	O
it	O
attempts	O
to	O
deal	O
with	O
hidden	B
variables	I
but	O
using	O
a	O
distribution	B
that	O
better	O
represents	O
the	O
posterior	B
distribution	B
than	O
given	O
by	O
maximum	B
likelihood	B
to	O
keep	O
the	O
notation	O
simple	O
we	O
ll	O
initially	O
assume	O
only	O
a	O
single	O
datapoint	O
with	O
observation	O
v	O
our	O
interest	O
is	O
then	O
the	O
parameter	B
posterior	B
p	O
pv	O
pv	O
h	O
h	O
the	O
vb	O
approach	B
assumes	O
a	O
factorised	B
approximation	B
of	O
the	O
joint	B
hidden	B
and	O
parameter	B
posterior	B
ph	O
qhq	O
discrete	B
variables	O
and	O
the	O
kronecker	B
delta	I
the	O
energy	B
attains	O
the	O
maximal	O
value	B
of	O
zero	O
when	O
old	O
in	O
the	O
case	O
of	O
continuous	B
variables	O
however	O
the	O
log	O
of	O
the	O
dirac	B
delta	I
function	B
is	O
not	O
well	O
defined	O
considering	O
the	O
delta	O
function	B
as	O
the	O
limit	O
of	O
a	O
narrow	O
width	O
gaussian	B
for	O
any	O
small	O
but	O
finite	O
width	O
the	O
energy	B
is	O
largest	O
when	O
old	O
draft	O
march	O
a	O
bound	B
on	O
the	O
marginal	B
likelihood	B
by	O
minimising	O
the	O
kl	O
divergence	B
klqhq	O
q	O
ph	O
we	O
arrive	O
at	O
the	O
bound	B
log	O
pv	O
q	O
pv	O
h	O
for	O
fixed	O
q	O
if	O
we	O
minimize	O
the	O
kullback-leibler	B
divergence	B
we	O
get	O
the	O
tightest	O
lower	O
bound	B
on	O
log	O
pv	O
if	O
then	O
for	O
fixed	O
qh	O
we	O
minimise	O
the	O
kullback-leibler	B
divergence	B
w	O
r	O
t	O
q	O
we	O
are	O
maximising	O
the	O
term	O
q	O
pv	O
h	O
and	O
hence	O
pushing	O
up	O
the	O
bound	B
on	O
the	O
marginal	B
likelihood	B
this	O
simple	O
co-ordinate	O
wise	O
procedure	O
in	O
which	O
we	O
first	O
fix	O
the	O
q	O
and	O
solve	O
for	O
qh	O
and	O
then	O
vice	O
versa	O
is	O
analogous	O
to	O
the	O
e	O
and	O
m	O
step	O
of	O
the	O
em	B
algorithm	B
e-step	B
qhqold	O
qnewh	O
argmin	O
qh	O
m-step	B
kl	O
qnew	O
argmin	O
q	O
klqnewhq	O
variational	B
bayes	I
in	O
full	O
generality	O
for	O
a	O
set	O
of	O
observations	O
v	O
and	O
hidden	B
variables	I
h	O
for	O
distributions	O
qh	O
and	O
q	O
which	O
are	O
parametersisedconstrained	O
the	O
best	O
distributions	O
in	O
the	O
minimal	O
kl	O
sense	O
are	O
returned	O
in	O
general	O
each	O
iteration	B
of	O
vb	O
is	O
guaranteed	O
to	O
increase	O
the	O
bound	B
on	O
the	O
marginal	B
likelihood	B
but	O
not	O
the	O
marginal	B
likelihood	B
itself	O
like	O
the	O
em	B
algorithm	B
vb	O
can	O
often	O
does	O
suffer	O
from	O
local	B
maxima	O
issues	O
this	O
means	O
that	O
the	O
converged	O
solution	O
can	O
be	O
dependent	O
on	O
the	O
initialisation	O
unconstrained	O
approximations	O
for	O
fixed	O
q	O
the	O
contribution	O
to	O
the	O
kl	O
divergence	B
is	O
pv	O
h	O
klqh	O
ph	O
const	O
where	O
ph	O
z	O
pvh	O
e	O
where	O
z	O
is	O
a	O
normalising	O
constant	O
hence	O
for	O
fixed	O
q	O
the	O
optimal	O
qh	O
is	O
given	O
by	O
p	O
pvh	O
qh	O
e	O
similarly	O
for	O
fixed	O
qh	O
optimally	O
pvh	O
q	O
e	O
log	O
pv	O
hn	O
i	O
i	O
d	O
data	O
under	O
the	O
i	O
i	O
d	O
assumption	O
we	O
obtain	O
a	O
bound	B
on	O
the	O
marginal	B
likelihood	B
for	O
the	O
whole	O
dataset	O
n	O
q	O
pvn	O
hn	O
the	O
bound	B
holds	O
for	O
any	O
qhn	O
and	O
q	O
but	O
is	O
tightest	O
for	O
the	O
converged	O
estimates	O
from	O
the	O
vb	O
procedure	O
for	O
an	O
i	O
i	O
d	O
dataset	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
without	O
loss	O
of	O
generality	O
we	O
may	O
assume	O
qhn	O
under	O
this	O
we	O
arrive	O
at	O
n	O
draft	O
march	O
variational	B
bayes	I
algorithm	B
variational	B
bayes	I
t	O
choose	O
an	O
initial	O
distribution	B
while	O
not	O
converged	O
likelihood	B
bound	B
not	O
converged	O
do	O
end	O
while	O
return	O
qn	O
t	O
t	O
t	O
arg	O
minqh	O
klqhqt	O
qn	O
t	O
arg	O
minq	O
klqn	O
qn	O
t	O
t	O
iteration	B
counter	O
initialisation	O
e	O
step	O
m	O
step	O
the	O
posterior	B
parameter	B
approximation	B
hn	O
vn	O
n	O
hn	O
n	O
figure	O
generic	O
form	O
of	O
a	O
model	B
a	O
facwith	O
hidden	B
variables	I
torised	O
posterior	B
approximation	B
uses	O
in	O
variational	B
bayes	I
em	B
is	O
a	O
special	O
case	O
of	O
variational	B
bayes	I
if	O
we	O
wish	O
to	O
find	O
a	O
summary	O
of	O
the	O
parameter	B
distribution	B
corresponding	O
to	O
only	O
the	O
most	O
likely	O
point	O
then	O
q	O
where	O
is	O
the	O
single	O
optimal	O
value	B
of	O
the	O
parameter	B
if	O
we	O
plug	O
this	O
assumption	O
into	O
equation	B
we	O
obtain	O
the	O
bound	B
the	O
m-step	B
is	O
then	O
given	O
by	O
log	O
pv	O
pv	O
h	O
const	O
pvh	O
log	O
p	O
argmax	O
for	O
a	O
flat	O
prior	B
p	O
const	O
this	O
is	O
therefore	O
equivalent	B
to	O
energy	B
maximisation	B
in	O
the	O
em	B
algorithm	B
using	O
this	O
single	O
optimal	O
value	B
in	O
the	O
vb	O
update	O
for	O
qhn	O
we	O
have	O
qn	O
t	O
pv	O
h	O
phv	O
which	O
is	O
the	O
standard	O
e-step	B
of	O
em	B
hence	O
em	B
is	O
a	O
special	O
case	O
of	O
vb	O
under	O
a	O
flat	O
prior	B
p	O
const	O
and	O
a	O
delta	O
function	B
approximation	B
of	O
the	O
parameter	B
posterior	B
factorising	O
the	O
parameter	B
posterior	B
let	O
s	O
reconsider	O
bayesian	B
learning	B
in	O
the	O
binary	O
variable	O
network	O
pa	O
c	O
s	O
pca	B
spaps	O
in	O
which	O
we	O
use	O
a	O
factorised	B
parameter	B
prior	B
p	O
c	O
ap	O
s	O
when	O
all	O
the	O
data	O
is	O
observed	O
the	O
parameter	B
posterior	B
factorises	O
as	O
we	O
discussed	O
in	O
if	O
the	O
state	O
of	O
a	O
is	O
not	O
observed	O
the	O
parameter	B
posterior	B
no	O
longer	O
factorises	O
p	O
a	O
s	O
cv	O
p	O
ap	O
sp	O
cpv	O
a	O
s	O
c	O
p	O
ap	O
sp	O
p	O
ap	O
sp	O
n	O
n	O
pvn	O
a	O
s	O
c	O
psn	O
an	O
pcnsn	O
an	O
cpan	O
a	O
draft	O
march	O
algorithm	B
variational	B
bayes	I
data	O
t	O
choose	O
an	O
initial	O
distribution	B
while	O
not	O
converged	O
likelihood	B
bound	B
not	O
converged	O
do	O
t	O
t	O
for	O
n	O
to	O
n	O
do	O
pvnhn	O
t	O
e	O
qn	O
end	O
for	O
qt	O
p	O
pvnhn	O
t	O
end	O
while	O
return	O
qn	O
t	O
s	O
s	O
n	O
a	O
a	O
c	O
c	O
s	O
a	O
a	O
n	O
c	O
variational	B
bayes	I
iteration	B
counter	O
initialisation	O
run	O
over	O
all	O
datapoints	O
e	O
step	O
m	O
step	O
the	O
posterior	B
parameter	B
approximation	B
figure	O
a	O
model	B
for	O
the	O
relationship	O
between	O
lung	O
cancer	O
asbestos	O
exposure	O
and	O
smoking	O
with	O
factorised	B
parameter	B
priors	O
variables	O
c	O
and	O
s	O
are	O
observed	O
but	O
variable	O
a	O
is	O
consistently	O
missb	O
a	O
factorised	B
parameter	B
posteing	O
rior	O
approximation	B
where	O
the	O
summation	O
over	O
a	O
prevents	O
the	O
factorisation	O
into	O
a	O
product	O
of	O
the	O
individual	O
table	O
parameters	O
since	O
it	O
is	O
convenient	O
in	O
terms	O
of	O
representations	O
to	O
work	O
with	O
factorised	B
posteriors	O
we	O
can	O
apply	O
vb	O
but	O
with	O
a	O
factorised	B
constraint	O
on	O
the	O
form	O
of	O
the	O
q	O
in	O
vb	O
we	O
define	O
a	O
distribution	B
over	O
the	O
visible	B
and	O
hidden	B
variables	I
in	O
this	O
case	O
the	O
hidden	B
variables	I
are	O
the	O
an	O
and	O
the	O
visible	B
are	O
sn	O
cn	O
the	O
joint	B
posterior	B
over	O
all	O
unobserved	O
variables	O
and	O
missing	B
observations	O
is	O
p	O
a	O
s	O
c	O
anv	O
p	O
ap	O
sp	O
p	O
a	O
s	O
c	O
anv	O
q	O
aq	O
cq	O
n	O
to	O
make	O
a	O
factorised	B
posterior	B
approximation	B
we	O
use	O
n	O
pcnsn	O
an	O
cpsn	O
span	O
a	O
qan	O
and	O
minimise	O
the	O
kullback-leibler	B
divergence	B
between	O
the	O
left	O
and	O
right	O
of	O
the	O
above	O
m-step	B
hence	O
q	O
a	O
p	O
n	O
pan	O
e	O
pan	O
qan	O
log	O
a	O
qan	O
log	O
a	O
hence	O
e	O
pan	O
it	O
is	O
convenient	O
to	O
use	O
a	O
beta	B
distribution	B
prior	B
a	O
p	O
a	O
a	O
a	O
draft	O
march	O
variational	B
bayes	I
a	O
s	O
n	O
q	O
a	O
b	O
n	O
a	O
similar	O
calculation	O
gives	O
q	O
s	O
b	O
q	O
ca	O
s	O
b	O
qan	O
i	O
n	O
qan	O
i	O
c	O
n	O
n	O
i	O
qan	O
n	O
i	O
qan	O
since	O
the	O
posterior	B
approximation	B
is	O
then	O
also	O
a	O
beta	B
distribution	B
and	O
four	O
tables	O
one	O
for	O
each	O
of	O
the	O
parental	O
states	O
of	O
c	O
for	O
example	O
these	O
are	O
reminiscent	O
of	O
the	O
standard	O
bayesian	B
equations	O
equation	B
except	O
that	O
the	O
counts	O
have	O
been	O
replaced	O
by	O
q	O
s	O
e-step	B
we	O
still	O
need	O
to	O
determine	O
qan	O
the	O
optimal	O
value	B
is	O
given	O
by	O
minimising	O
the	O
kullback-leibler	B
divergence	B
with	O
respect	O
to	O
qan	O
this	O
gives	O
the	O
solution	O
that	O
optimally	O
pcnsnan	O
pan	O
a	O
qan	O
e	O
for	O
example	O
if	O
assume	O
that	O
for	O
datapoint	O
n	O
s	O
is	O
in	O
state	O
and	O
c	O
in	O
state	O
then	O
a	O
qan	O
e	O
and	O
a	O
qan	O
e	O
to	O
compute	O
such	O
quantities	O
explicitly	O
we	O
need	O
the	O
values	O
and	O
for	O
a	O
beta	B
distribution	B
these	O
are	O
straightforward	O
to	O
compute	O
see	O
the	O
complete	O
vb	O
procedure	O
is	O
then	O
given	O
by	O
iterating	O
equations	O
and	O
until	O
convergence	O
given	O
a	O
converged	O
factorised	B
approximation	B
computing	O
a	O
marginal	B
table	O
pa	O
is	O
then	O
straightforward	O
under	O
the	O
approximation	B
pa	O
qa	O
aq	O
av	O
a	O
a	O
aq	O
av	O
since	O
q	O
av	O
is	O
a	O
beta	B
distribution	B
b	O
a	O
the	O
mean	B
is	O
straightforward	O
using	O
this	O
for	O
both	O
states	O
of	O
a	O
leads	O
to	O
n	O
qan	O
n	O
qan	O
n	O
qan	O
pa	O
the	O
application	O
of	O
vb	O
to	O
learning	B
the	O
tables	O
in	O
arbitrarily	O
structured	B
bns	O
is	O
a	O
straightforward	O
extension	O
of	O
the	O
technique	O
outlined	O
here	O
under	O
the	O
factorised	B
approximation	B
qh	O
qhq	O
one	O
will	O
always	O
obtain	O
a	O
simple	O
updating	O
equation	B
analogous	O
to	O
the	O
full	O
data	O
case	O
but	O
with	O
the	O
missing	B
data	I
replaced	O
by	O
variational	O
approximations	O
nevertheless	O
if	O
a	O
variable	O
has	O
many	O
missing	B
parents	B
the	O
number	O
of	O
states	O
in	O
the	O
average	B
with	O
respect	O
to	O
the	O
q	O
distribution	B
can	O
become	O
intractable	O
and	O
further	O
constraints	O
on	O
the	O
form	O
of	O
the	O
approximation	B
or	O
additional	O
bounds	O
are	O
required	O
one	O
may	O
readily	O
extend	O
the	O
above	O
to	O
the	O
case	O
of	O
dirichlet	B
distributions	O
on	O
multinomial	B
variables	O
see	O
indeed	O
the	O
extension	O
to	O
the	O
exponential	B
family	B
is	O
straightforward	O
draft	O
march	O
optimising	O
the	O
likelihood	B
by	O
gradient	B
methods	O
figure	O
standard	O
ml	O
learning	B
the	O
best	O
parameter	B
is	O
found	O
by	O
maximising	O
the	O
probability	O
that	O
the	O
model	B
generates	O
the	O
observed	O
data	O
ml-ii	B
learning	B
in	O
cases	O
where	O
we	O
have	O
a	O
opt	O
arg	O
max	O
pv	O
prior	B
preference	O
for	O
the	O
parameters	O
but	O
with	O
unspecified	O
hyperparameter	B
we	O
can	O
find	O
by	O
opt	O
arg	O
max	O
pv	O
arg	O
max	O
v	O
v	O
bayesian	B
methods	O
and	O
ml-ii	B
consider	O
a	O
parameterised	O
distribution	B
pv	O
for	O
which	O
we	O
wish	O
to	O
the	O
learn	O
the	O
optimal	O
parameters	O
given	O
some	O
data	O
the	O
model	B
pv	O
is	O
depicted	O
in	O
where	O
a	O
dot	O
indicates	O
that	O
no	O
distribution	B
is	O
present	O
on	O
that	O
variable	O
for	O
a	O
single	O
observed	O
datapoint	O
v	O
setting	O
by	O
maximum	B
likelihood	B
corresponds	O
to	O
finding	O
the	O
parameter	B
that	O
maximises	O
pv	O
in	O
some	O
cases	O
we	O
may	O
have	O
an	O
idea	O
about	O
which	O
parameters	O
are	O
more	O
appropriate	O
and	O
can	O
express	O
this	O
prior	B
preference	O
using	O
a	O
distribution	B
p	O
if	O
the	O
prior	B
were	O
fully	O
specified	O
then	O
there	O
is	O
nothing	O
to	O
learn	O
since	O
p	O
is	O
now	O
fully	O
known	O
however	O
in	O
many	O
cases	O
in	O
practice	O
we	O
are	O
unsure	O
of	O
the	O
exact	O
parameter	B
settings	O
of	O
the	O
prior	B
and	O
hence	O
specify	O
a	O
parametersised	O
prior	B
using	O
a	O
distribution	B
p	O
with	O
hyperparameter	B
this	O
is	O
depicted	O
in	O
the	O
learning	B
corresponds	O
to	O
finding	O
the	O
optimal	O
pv	O
this	O
is	O
known	O
as	O
an	O
ml-ii	B
procedure	O
since	O
it	O
corresponds	O
to	O
maximum	B
likelihood	B
but	O
at	O
the	O
higher	O
hyperparameter	B
this	O
is	O
a	O
form	O
of	O
approximate	B
bayesian	B
analysis	B
since	O
although	O
is	O
set	O
using	O
maximum	B
likelihood	B
after	O
training	B
we	O
have	O
a	O
distribution	B
over	O
parameters	O
p	O
optimising	O
the	O
likelihood	B
by	O
gradient	B
methods	O
that	O
maximises	O
the	O
likelihood	B
pv	O
directed	B
models	O
the	O
em	B
algorithm	B
typically	O
works	O
well	O
when	O
the	O
amount	O
of	O
missing	B
information	O
is	O
small	O
compared	O
to	O
the	O
complete	O
information	O
in	O
this	O
case	O
em	B
exhibits	O
approximately	O
the	O
same	O
convergence	O
as	O
newton	O
based	O
gradient	B
however	O
if	O
the	O
fraction	O
of	O
missing	B
information	O
approaches	O
unity	O
em	B
can	O
converge	O
very	O
slowly	O
in	O
the	O
case	O
of	O
continuous	B
parameters	O
an	O
alternative	O
is	O
to	O
compute	O
the	O
gradient	B
of	O
the	O
likelihood	B
directly	O
and	O
use	O
this	O
as	O
part	O
of	O
a	O
standard	O
continuous	B
variable	O
optimisation	B
routine	O
the	O
gradient	B
is	O
straightforward	O
to	O
compute	O
using	O
the	O
following	O
identity	O
consider	O
the	O
log	O
likelihood	B
l	O
log	O
pv	O
the	O
derivative	O
can	O
be	O
written	O
l	O
pv	O
h	O
at	O
this	O
point	O
we	O
take	O
the	O
derivative	O
inside	O
the	O
integral	O
pv	O
pv	O
h	O
pv	O
l	O
pv	O
h	O
pv	O
h	O
h	O
phv	O
log	O
pv	O
h	O
log	O
pv	O
h	O
where	O
we	O
used	O
log	O
fx	O
fx	O
the	O
right	O
hand	O
side	O
is	O
the	O
average	B
of	O
the	O
derivative	O
of	O
the	O
log	O
complete	O
likelihood	B
this	O
is	O
closely	O
related	O
to	O
the	O
derivative	O
of	O
the	O
energy	B
term	O
in	O
the	O
em	B
algorithm	B
though	O
note	O
that	O
the	O
average	B
here	O
is	O
performed	O
with	O
respect	O
the	O
current	O
distribution	B
parameters	O
and	O
not	O
old	O
as	O
in	O
the	O
em	B
case	O
used	O
in	O
this	O
way	O
computing	O
the	O
derivatives	O
of	O
latent	B
variable	I
models	O
is	O
relatively	O
straightforward	O
these	O
derivatives	O
may	O
then	O
be	O
used	O
as	O
part	O
of	O
a	O
standard	O
optimisation	B
routine	O
such	O
as	O
conjugate	B
draft	O
march	O
exercises	O
undirected	B
models	O
consider	O
an	O
undirected	B
model	B
which	O
contains	O
both	O
hidden	B
and	O
visible	B
variables	O
pv	O
h	O
z	O
e	O
for	O
i	O
i	O
d	O
data	O
the	O
log	O
likelihood	B
on	O
the	O
visible	B
variables	O
is	O
discrete	B
v	O
and	O
h	O
which	O
has	O
gradient	B
l	O
n	O
l	O
n	O
h	O
e	O
hv	O
e	O
h	O
clamped	O
average	B
phvn	O
h	O
free	O
average	B
phv	O
for	O
a	O
markov	B
network	I
that	O
is	O
intractable	O
partition	B
function	B
z	O
cannot	O
be	O
computed	O
efficiently	O
the	O
gradient	B
is	O
particularly	O
difficult	O
to	O
estimate	O
since	O
it	O
is	O
the	O
difference	O
of	O
two	O
quantities	O
each	O
of	O
which	O
needs	O
to	O
be	O
estimated	O
even	O
getting	O
the	O
sign	O
of	O
the	O
gradient	B
correct	O
can	O
therefore	O
be	O
computationally	O
difficult	O
for	O
this	O
reason	O
learning	B
in	O
models	O
such	O
as	O
the	O
boltzmann	B
machine	I
with	O
hidden	B
units	O
is	O
particularly	O
difficult	O
code	O
in	O
the	O
demo	O
code	O
we	O
take	O
the	O
original	O
chest	B
clinic	I
network	O
and	O
draw	O
data	O
samples	O
from	O
this	O
network	O
our	O
interest	O
is	O
then	O
to	O
see	O
if	O
we	O
can	O
use	O
the	O
em	B
algorithm	B
to	O
estimate	O
the	O
tables	O
based	O
on	O
the	O
data	O
some	O
parts	O
of	O
the	O
data	O
missing	B
at	I
random	I
we	O
assume	O
that	O
we	O
know	O
the	O
correct	O
bn	O
structure	B
only	O
that	O
the	O
cpts	O
are	O
unknown	O
we	O
assume	O
the	O
logic	O
gate	O
table	O
is	O
known	O
so	O
we	O
do	O
not	O
need	O
to	O
learn	O
this	O
demoemchestclinic	O
m	O
demo	O
of	O
em	B
in	O
learning	B
the	O
chest	B
clinic	I
tables	O
the	O
following	O
code	O
implements	O
maximum	B
likelihood	B
learning	B
of	O
bn	O
tables	O
based	O
on	O
data	O
with	O
possibly	O
missing	B
values	O
embeliefnet	O
m	O
em	B
training	B
of	O
a	O
belief	B
network	I
exercises	O
exercise	O
nightmare	O
continued	O
continuing	O
with	O
the	O
bn	O
given	O
in	O
the	O
following	O
table	O
represents	O
data	O
gathered	O
on	O
the	O
printer	O
where	O
indicates	O
that	O
the	O
entry	O
is	O
missing	B
each	O
column	O
represents	O
a	O
datapoint	O
use	O
the	O
em	B
algorithm	B
to	O
learn	O
all	O
cpts	O
of	O
the	O
network	O
fuse	O
assembly	O
malfunction	O
drum	O
unit	O
toner	O
out	O
poor	O
paper	O
quality	O
worn	O
roller	O
burning	O
smell	O
poor	O
print	O
quality	O
wrinkled	O
pages	O
multiple	O
pages	O
fed	O
paper	O
jam	O
draft	O
march	O
the	O
table	O
is	O
contained	O
in	O
emprinter	O
mat	O
using	O
states	O
nan	O
in	O
place	O
of	O
brmltoolbox	O
requires	O
states	O
to	O
be	O
numbered	O
given	O
no	O
wrinkled	O
pages	O
no	O
burning	O
smell	O
and	O
poor	O
print	O
quality	O
what	O
is	O
the	O
probability	O
there	O
is	O
a	O
drum	O
unit	O
problem	B
exercise	O
consider	O
the	O
following	O
distribution	B
over	O
discrete	B
variables	O
exercises	O
in	O
which	O
the	O
variables	O
and	O
are	O
consistently	O
hidden	B
in	O
the	O
training	B
data	O
and	O
training	B
data	O
for	O
are	O
always	O
present	O
show	O
that	O
the	O
em	B
update	O
for	O
the	O
table	O
is	O
given	O
by	O
j	O
i	O
i	O
n	O
nk	O
i	O
jxn	O
k	O
jxn	O
xn	O
xn	O
xn	O
xn	O
exercise	O
consider	O
a	O
simple	O
two	O
variable	O
bn	O
py	O
x	O
pyxpx	O
where	O
both	O
y	O
and	O
x	O
are	O
binary	O
variables	O
domx	O
domy	O
you	O
have	O
a	O
set	O
of	O
training	B
data	O
xn	O
n	O
n	O
in	O
which	O
for	O
some	O
cases	O
xn	O
may	O
be	O
missing	B
we	O
are	O
specifically	O
interested	O
in	O
learning	B
the	O
table	O
px	O
from	O
this	O
data	O
a	O
colleague	O
suggests	O
that	O
one	O
can	O
set	O
px	O
by	O
simply	O
looking	O
at	O
datapoints	O
where	O
x	O
is	O
observed	O
and	O
then	O
setting	O
px	O
to	O
be	O
the	O
fraction	O
of	O
observed	O
x	O
that	O
is	O
in	O
state	O
explain	O
how	O
this	O
suggested	O
procedure	O
relates	O
to	O
maximum	B
likelihood	B
and	O
em	B
exercise	O
assume	O
that	O
a	O
sequence	O
is	O
generated	O
by	O
a	O
markov	B
chain	B
for	O
a	O
single	O
chain	B
of	O
length	O
t	O
we	O
have	O
t	O
vt	O
for	O
simplicity	O
we	O
denote	O
the	O
sequence	O
of	O
visible	B
variables	O
as	O
v	O
vt	O
for	O
a	O
single	O
markov	B
chain	B
labelled	B
by	O
h	O
t	O
pvh	O
h	O
in	O
total	O
there	O
are	O
a	O
set	O
of	O
h	O
such	O
markov	O
chains	O
h	O
the	O
distribution	B
on	O
the	O
visible	B
variables	O
is	O
therefore	O
pv	O
pvhph	O
there	O
are	O
a	O
set	O
of	O
training	B
sequences	B
vn	O
n	O
n	O
assuming	O
that	O
each	O
sequence	O
vn	O
is	O
independently	O
and	O
identically	O
drawn	O
from	O
a	O
markov	B
chain	B
mixture	B
model	B
with	O
h	O
components	O
derive	O
the	O
expectation	B
maximisation	B
algorithm	B
for	O
training	B
this	O
model	B
write	O
a	O
general	O
matlab	O
function	B
in	O
the	O
form	O
function	B
to	O
perform	O
em	B
learning	B
for	O
any	O
set	O
of	O
same	O
length	O
sequences	B
of	O
integers	O
vn	O
t	O
v	O
t	O
t	O
v	O
is	O
a	O
cell	O
array	O
of	O
the	O
training	B
data	O
is	O
the	O
time	O
element	O
of	O
the	O
second	O
training	B
sequence	O
each	O
element	O
say	O
must	O
be	O
an	O
integer	O
from	O
to	O
v	O
v	O
is	O
the	O
number	O
of	O
states	O
of	O
the	O
visible	B
variables	O
the	O
bio-sequence	O
case	O
below	O
this	O
will	O
be	O
h	O
is	O
the	O
draft	O
march	O
exercises	O
number	O
of	O
mixture	B
components	O
num	O
em	B
loops	O
is	O
the	O
number	O
of	O
em	B
iterations	O
a	O
is	O
the	O
transition	B
matrix	B
pv	O
is	O
the	O
prior	B
state	O
of	O
the	O
first	O
visible	B
variable	O
ph	O
is	O
a	O
vector	O
of	O
prior	B
probabilities	O
for	O
the	O
mixture	B
state	O
phhph	O
q	O
is	O
the	O
cell	O
array	O
of	O
posterior	B
probabilities	O
qmuhphvmu	O
your	O
routine	O
must	O
also	O
display	O
for	O
each	O
em	B
iteration	B
the	O
value	B
of	O
the	O
log	O
likelihood	B
as	O
a	O
check	B
on	O
your	O
routine	O
the	O
log	O
likelihood	B
must	O
increase	O
at	O
each	O
iteration	B
the	O
file	O
sequences	B
mat	O
contains	O
a	O
set	O
of	O
fictitious	O
bio-sequence	O
in	O
a	O
cell	O
array	O
sequencesmut	O
thus	O
is	O
the	O
third	O
sequence	O
gtctcctgccctctctgaac	O
which	O
consists	O
of	O
timesteps	O
there	O
are	O
such	O
sequences	B
in	O
total	O
your	O
task	O
is	O
to	O
cluster	O
these	O
sequences	B
into	O
two	O
clusters	O
assuming	O
that	O
each	O
cluster	O
is	O
modelled	O
by	O
a	O
markov	B
chain	B
state	O
which	O
of	O
the	O
sequences	B
belong	O
together	O
by	O
assigning	O
a	O
sequence	O
vn	O
to	O
that	O
state	O
for	O
which	O
phvn	O
is	O
highest	O
exercise	O
write	O
a	O
general	O
purpose	O
routine	O
vbbeliefnetpotxpars	O
along	O
the	O
lines	O
of	O
embeliefnet	O
m	O
that	O
performs	O
variational	B
bayes	I
under	O
a	O
dirichlet	B
prior	B
using	O
a	O
factorised	B
parameter	B
approximation	B
assume	O
both	O
global	B
and	O
local	B
parameter	B
independence	B
for	O
the	O
prior	B
and	O
the	O
approximation	B
q	O
exercise	O
consider	O
a	O
layered	O
boltzmann	B
machine	I
which	O
has	O
the	O
form	O
pv	O
z	O
where	O
dim	O
v	O
dim	O
dim	O
dim	O
v	O
y	O
e	O
all	O
variables	O
are	O
binary	O
with	O
states	O
and	O
the	O
parameters	O
for	O
each	O
layer	O
l	O
are	O
l	O
al	O
wij	O
xiyj	O
xixj	O
yiyj	O
in	O
terms	O
of	O
fitting	O
the	O
model	B
to	O
visible	B
data	O
vn	O
is	O
the	O
layered	O
model	B
above	O
any	O
more	O
powerful	O
than	O
fitting	O
a	O
two-layered	O
model	B
factor	B
is	O
not	O
present	O
in	O
the	O
two-layer	O
case	O
if	O
we	O
use	O
a	O
restricted	B
potential	B
y	O
e	O
ij	O
wij	O
xiyj	O
is	O
the	O
three	O
layered	O
model	B
more	O
powerful	O
in	O
being	O
able	O
to	O
fit	O
the	O
visible	B
data	O
than	O
the	O
two-layered	O
model	B
exercise	O
the	O
sigmoid	B
belief	B
network	I
is	O
defined	O
by	O
the	O
layered	O
network	O
pxl	O
pxl	O
where	O
vector	O
variables	O
have	O
binary	O
components	O
xl	O
and	O
the	O
width	O
of	O
layer	O
l	O
is	O
given	O
by	O
wl	O
in	O
addition	O
pxl	O
pxl	O
i	O
and	O
pxl	O
i	O
wt	O
e	O
x	O
for	O
a	O
weight	B
vector	O
wil	O
describing	O
the	O
interaction	O
from	O
the	O
parental	O
layer	O
the	O
top	O
layer	O
pxl	O
describes	O
a	O
factorised	B
distribution	B
pxl	O
pxl	O
wl	O
draw	O
the	O
belief	B
network	I
structure	B
of	O
this	O
distribution	B
for	O
the	O
layer	O
what	O
is	O
the	O
computational	B
complexity	I
of	O
computing	O
the	O
likelihood	B
assuming	O
that	O
all	O
layers	O
have	O
equal	O
width	O
w	O
draft	O
march	O
assuming	O
a	O
fully	O
factorised	B
approximation	B
for	O
an	O
equal	O
width	O
network	O
qxl	O
i	O
exercises	O
write	O
down	O
the	O
energy	B
term	O
of	O
the	O
variational	O
em	B
procedure	O
for	O
a	O
single	O
data	O
observation	O
and	O
discuss	O
the	O
tractability	O
of	O
computing	O
the	O
energy	B
exercise	O
a	O
probability	O
table	O
i	O
j	O
ij	O
with	O
ij	O
exercise	O
show	O
how	O
to	O
find	O
the	O
components	O
b	O
g	O
p	O
that	O
maximise	O
equation	B
ij	O
is	O
learned	O
using	O
maximal	O
marginal	B
likelihood	B
in	O
which	O
is	O
never	O
observed	O
show	O
that	O
if	O
is	O
given	O
as	O
a	O
maximal	O
marginal	B
likelihood	B
solution	O
then	O
has	O
the	O
same	O
marginal	B
likelihood	B
score	O
draft	O
march	O
chapter	O
bayesian	B
model	B
selection	I
comparing	O
models	O
the	O
bayesian	B
way	O
given	O
two	O
models	O
and	O
with	O
parameters	O
and	O
associated	O
parameter	B
priors	O
px	O
px	O
px	O
px	O
how	O
can	O
we	O
compare	O
the	O
performance	B
of	O
the	O
models	O
in	O
fitting	O
a	O
set	O
of	O
data	O
d	O
xn	O
the	O
application	O
of	O
bayes	O
rule	O
to	O
models	O
gives	O
a	O
framework	O
for	O
answering	O
questions	O
like	O
this	O
a	O
form	O
of	O
bayesian	B
hypothesis	B
testing	I
applied	O
at	O
the	O
model	B
level	O
more	O
generally	O
given	O
an	O
indexed	O
set	O
of	O
models	O
mm	O
and	O
associated	O
prior	B
beliefs	O
in	O
the	O
appropriateness	O
of	O
each	O
model	B
pmi	O
our	O
interest	O
is	O
the	O
model	B
posterior	B
probability	O
pmid	O
pdmipmi	O
pd	O
where	O
pd	O
pdmipmi	O
model	B
mi	O
is	O
parameterised	O
by	O
i	O
and	O
the	O
model	B
likelihood	B
is	O
given	O
by	O
pdmi	O
pd	O
i	O
mip	O
imid	O
i	O
in	O
discrete	B
parameter	B
spaces	O
the	O
integral	O
is	O
replaced	O
with	O
summation	O
note	O
that	O
the	O
number	O
of	O
parameters	O
dim	O
i	O
need	O
not	O
be	O
the	O
same	O
for	O
each	O
model	B
a	O
point	O
of	O
caution	O
here	O
is	O
that	O
pmid	O
only	O
refers	O
to	O
the	O
probability	O
relative	O
to	O
the	O
set	O
of	O
models	O
specified	O
mm	O
this	O
is	O
not	O
the	O
absolute	O
probability	O
that	O
model	B
m	O
fits	O
well	O
to	O
compute	O
such	O
a	O
quantity	O
would	O
require	O
one	O
to	O
specify	O
all	O
possible	O
models	O
whilst	O
interpreting	O
the	O
posterior	B
pmid	O
requires	O
some	O
care	O
comparing	O
two	O
competing	O
model	B
hypotheses	O
mi	O
and	O
mj	O
is	O
straightforward	O
and	O
only	O
requires	O
the	O
bayes	O
factor	B
pmid	O
pmjd	O
pdmi	O
pdmj	O
bayes	O
factor	B
pmi	O
pmj	O
which	O
does	O
not	O
require	O
integrationsummation	O
over	O
all	O
possible	O
models	O
illustrations	O
coin	O
tossing	O
figure	O
discrete	B
prior	B
model	B
of	O
a	O
fair	O
coin	O
prior	B
for	O
a	O
biased	O
unfair	O
coin	O
in	O
both	O
cases	O
we	O
are	O
making	O
explicit	O
choices	O
here	O
about	O
what	O
we	O
consider	O
to	O
be	O
a	O
fair	O
and	O
and	O
unfair	O
illustrations	O
coin	O
tossing	O
we	O
ll	O
consider	O
two	O
illustrations	O
the	O
first	O
uses	O
a	O
discrete	B
parameter	B
space	O
to	O
keep	O
the	O
mathematics	O
simple	O
in	O
the	O
second	O
we	O
use	O
a	O
continuous	B
parameter	B
space	O
a	O
discrete	B
parameter	B
space	O
a	O
simple	O
choice	O
would	O
be	O
to	O
consider	O
two	O
competing	O
models	O
one	O
corresponding	O
to	O
a	O
fair	O
coin	O
and	O
the	O
other	O
a	O
biased	O
coin	O
the	O
bias	B
of	O
the	O
coin	O
namely	O
the	O
probability	O
that	O
the	O
coin	O
will	O
land	O
heads	O
is	O
specified	O
by	O
so	O
that	O
a	O
truly	O
fair	O
coin	O
has	O
for	O
simplicity	O
we	O
assume	O
dom	O
for	O
the	O
fair	O
coin	O
we	O
use	O
the	O
distribution	B
p	O
air	O
in	O
and	O
for	O
the	O
biased	O
coin	O
the	O
distribution	B
p	O
in	O
for	O
each	O
model	B
m	O
the	O
likelihood	B
is	O
given	O
by	O
pdm	O
pd	O
mp	O
nh	O
p	O
p	O
p	O
assuming	O
that	O
pmf	O
air	O
pmbiased	O
the	O
bayes	O
factor	B
is	O
given	O
by	O
the	O
ratio	O
of	O
the	O
two	O
model	B
likelihoods	O
example	O
parameter	B
space	O
heads	O
and	O
tails	O
here	O
pdmf	O
air	O
and	O
pdmbiased	O
the	O
bayes	O
factor	B
is	O
pmf	O
aird	O
pmbiasedd	O
indicating	O
that	O
there	O
is	O
little	O
to	O
choose	O
between	O
the	O
two	O
models	O
heads	O
and	O
tails	O
here	O
pdmf	O
air	O
and	O
pdmbiased	O
the	O
bayes	O
factor	B
is	O
pmf	O
aird	O
pmbiasedd	O
indicating	O
that	O
have	O
around	O
times	O
the	O
belief	O
in	O
the	O
biased	O
model	B
as	O
opposed	O
to	O
the	O
fair	O
model	B
a	O
continuous	B
parameter	B
space	O
here	O
we	O
repeat	O
the	O
above	O
calculation	O
but	O
for	O
continuous	B
parameter	B
spaces	O
draft	O
march	O
illustrations	O
coin	O
tossing	O
figure	O
probability	O
density	B
priors	O
on	O
the	O
probability	O
of	O
a	O
head	O
p	O
for	O
a	O
fair	O
coin	O
p	O
air	O
for	O
an	O
biased	O
coin	O
p	O
b	O
note	O
the	O
different	O
b	O
vertical	O
scales	O
in	O
the	O
two	O
cases	O
fair	O
coin	O
for	O
the	O
fair	O
coin	O
a	O
uni-modal	O
prior	B
is	O
appropriate	O
we	O
use	O
beta	B
distribution	B
p	O
b	O
b	O
b	O
b	O
ba	O
b	O
a	O
for	O
convenience	O
since	O
as	O
this	O
is	O
conjugate	B
to	O
the	O
binomial	B
distribution	B
the	O
required	O
integrations	O
are	O
trivial	O
a	O
reasonable	O
choice	O
for	O
a	O
fair	O
coin	O
is	O
a	O
b	O
as	O
shown	O
in	O
ba	O
b	O
a	O
nh	O
p	O
nh	O
ba	O
b	O
nh	O
bnh	O
a	O
nt	O
b	O
ba	O
b	O
in	O
general	O
pdmf	O
air	O
biased	O
coin	O
for	O
the	O
biased	O
coin	O
we	O
use	O
a	O
bimodal	O
distribution	B
formed	O
for	O
convenience	O
as	O
a	O
mixture	B
of	O
two	O
beta	B
distributions	O
b	O
as	O
shown	O
in	O
the	O
model	B
likelihood	B
pdmbiased	O
is	O
given	O
by	O
p	O
p	O
nh	O
bnh	O
nt	O
nh	O
nh	O
bnh	O
nt	O
assuming	O
no	O
prior	B
preference	O
for	O
either	O
a	O
fair	O
or	O
biased	O
coin	O
pm	O
const	O
and	O
repeating	O
the	O
above	O
scenario	O
in	O
the	O
discrete	B
parameter	B
case	O
example	O
parameter	B
space	O
draft	O
march	O
occam	O
s	O
razor	O
and	O
bayesian	B
complexity	O
penalisation	O
figure	O
the	O
likelihood	B
of	O
the	O
total	O
dice	O
score	O
ptn	O
for	O
n	O
to	O
n	O
die	O
plotted	O
along	O
the	O
horizontal	O
axis	O
is	O
the	O
total	O
score	O
t	O
the	O
vertical	O
line	O
marks	O
the	O
comparison	O
for	O
pt	O
for	O
the	O
different	O
number	O
of	O
die	O
the	O
more	O
complex	O
models	O
which	O
can	O
reach	O
more	O
states	O
have	O
lower	O
likelihood	B
due	O
to	O
normalisation	B
over	O
t	O
heads	O
and	O
tails	O
here	O
pdmf	O
air	O
and	O
pdmbiased	O
the	O
bayes	O
factor	B
is	O
pmf	O
aird	O
pmbiasedd	O
indicating	O
that	O
there	O
is	O
little	O
to	O
choose	O
between	O
the	O
two	O
models	O
heads	O
and	O
tails	O
here	O
pdmf	O
air	O
and	O
pdmbiased	O
the	O
bayes	O
factor	B
is	O
pmf	O
aird	O
pmbiasedd	O
indicating	O
that	O
have	O
around	O
times	O
the	O
belief	O
in	O
the	O
biased	O
model	B
as	O
opposed	O
to	O
the	O
fair	O
model	B
occam	O
s	O
razor	O
and	O
bayesian	B
complexity	O
penalisation	O
we	O
return	O
to	O
the	O
dice	O
scenario	O
of	O
there	O
we	O
assumed	O
there	O
are	O
two	O
die	O
whose	O
scores	O
and	O
are	O
not	O
known	O
only	O
the	O
sum	O
of	O
the	O
two	O
scores	O
t	O
is	O
known	O
we	O
then	O
computed	O
the	O
posterior	B
joint	B
score	O
distribution	B
for	O
the	O
two	O
die	O
we	O
repeat	O
the	O
calculation	O
but	O
now	O
for	O
multiple	O
dice	O
and	O
with	O
the	O
twist	O
that	O
we	O
don	O
t	O
know	O
how	O
many	O
dice	O
there	O
only	O
that	O
the	O
sum	O
of	O
the	O
scores	O
is	O
si	O
and	O
are	O
given	O
the	O
value	B
t	O
however	O
we	O
are	O
not	O
told	O
the	O
number	O
of	O
die	O
that	O
is	O
we	O
know	O
t	O
involved	O
n	O
assuming	O
that	O
any	O
number	O
n	O
is	O
equally	O
likely	O
what	O
is	O
the	O
posterior	B
distribution	B
over	O
n	O
from	O
bayes	O
rule	O
we	O
need	O
to	O
compute	O
the	O
posterior	B
distribution	B
over	O
models	O
in	O
the	O
above	O
pt	O
pnt	O
ptnpn	O
ptn	O
pt	O
snn	O
i	O
psi	O
i	O
t	O
i	O
si	O
psi	O
description	O
of	O
occam	O
s	O
razor	O
is	O
due	O
to	O
taylan	O
cemgil	O
draft	O
march	O
a	O
continuous	B
example	O
curve	O
fitting	O
n	O
figure	O
the	O
posterior	B
distribution	B
pnt	O
of	O
the	O
number	O
of	O
die	O
given	O
the	O
observed	O
summed	O
score	O
of	O
where	O
psi	O
for	O
all	O
scores	O
si	O
by	O
enumerating	O
all	O
states	O
we	O
can	O
explicitly	O
compute	O
ptn	O
as	O
displayed	O
in	O
the	O
important	O
observation	O
is	O
that	O
as	O
the	O
models	O
explaining	O
the	O
data	O
become	O
more	O
complex	O
increases	O
more	O
states	O
become	O
accessible	O
and	O
the	O
probability	O
mass	O
typically	O
reduces	O
we	O
see	O
this	O
effect	O
at	O
pt	O
where	O
apart	O
from	O
n	O
the	O
value	B
of	O
pt	O
decreases	O
with	O
increasing	O
n	O
since	O
the	O
higher	O
n	O
have	O
mass	O
in	O
more	O
states	O
becoming	O
more	O
spread	O
out	O
assuming	O
pn	O
const	O
the	O
posterior	B
pnt	O
is	O
plotted	O
in	O
a	O
posteriori	O
there	O
are	O
only	O
plausible	O
models	O
namely	O
n	O
since	O
the	O
rest	O
are	O
either	O
too	O
complex	O
or	O
impossible	O
this	O
demonstrates	O
the	O
occam	O
s	O
razor	O
effect	O
which	O
penalises	O
models	O
which	O
are	O
over	O
complex	O
a	O
continuous	B
example	O
curve	O
fitting	O
consider	O
an	O
additive	O
set	O
of	O
periodic	B
functions	O
cosx	O
wk	O
coskx	O
this	O
can	O
be	O
conveniently	O
written	O
in	O
vector	O
form	O
wt	O
where	O
is	O
a	O
k	O
dimensional	O
vector	O
with	O
elements	O
cosx	O
coskxt	O
and	O
the	O
vector	O
w	O
contains	O
the	O
weights	O
of	O
the	O
additive	O
function	B
we	O
are	O
given	O
a	O
set	O
of	O
data	O
d	O
yn	O
n	O
n	O
drawn	O
from	O
this	O
distribution	B
where	O
y	O
is	O
the	O
clean	O
corrupted	O
with	O
additive	O
zero	O
mean	B
gaussian	B
noise	O
with	O
variance	B
yn	O
n	O
see	O
assuming	O
i	O
i	O
d	O
data	O
we	O
are	O
interested	O
in	O
the	O
posterior	B
probability	O
of	O
the	O
number	O
of	O
coefficients	O
given	O
the	O
observed	O
data	O
pd	O
we	O
will	O
assume	O
pk	O
const	O
the	O
likelihood	B
term	O
above	O
is	O
given	O
by	O
the	O
integral	O
pkd	O
pdkpk	O
pd	O
n	O
pxn	O
xn	O
k	O
pwk	O
w	O
xn	O
k	O
pynxn	O
w	O
k	O
for	O
pwk	O
n	O
ik	O
the	O
integrand	O
is	O
a	O
gaussian	B
in	O
w	O
for	O
which	O
it	O
is	O
straightforward	O
to	O
evaluate	O
the	O
integral	O
and	O
log	O
yn	O
k	O
n	O
bta	O
log	O
det	O
a	O
k	O
log	O
k	O
w	O
xn	O
yn	O
yn	O
n	O
for	O
regression	B
under	O
the	O
i	O
i	O
d	O
figure	O
belief	B
network	I
representation	O
of	O
a	O
hierarchical	O
bayesian	B
model	B
data	O
assumption	O
note	O
that	O
are	O
included	O
to	O
highlight	O
the	O
role	O
of	O
the	O
the	O
intermediate	O
nodes	O
on	O
yn	O
x	O
clean	O
underlying	O
model	B
n	O
with	O
the	O
intermediate	O
node	O
and	O
place	O
directly	O
arrows	O
from	O
w	O
and	O
xn	O
to	O
yn	O
since	O
pyw	O
x	O
wtx	O
we	O
can	O
if	O
desired	O
do	O
away	O
n	O
draft	O
march	O
approximating	O
the	O
model	B
likelihood	B
figure	O
the	O
data	O
generated	O
with	O
additive	O
gaussian	B
noise	O
from	O
a	O
k	O
component	O
model	B
the	O
posterior	B
pkd	O
the	O
reconstruction	O
of	O
the	O
data	O
using	O
where	O
is	O
the	O
mean	B
posterior	B
vector	O
of	O
the	O
optimal	O
dimensional	O
model	B
pwd	O
k	O
plotted	O
in	O
the	O
continuous	B
line	O
is	O
the	O
reconstruction	O
plotted	O
in	O
dots	O
is	O
the	O
true	O
underlying	O
clean	O
data	O
where	O
a	O
i	O
txn	O
b	O
yn	O
assuming	O
and	O
we	O
sampled	O
some	O
data	O
from	O
a	O
model	B
with	O
k	O
components	O
we	O
assume	O
that	O
we	O
know	O
the	O
correct	O
noise	O
level	O
the	O
posterior	B
pkd	O
plotted	O
in	O
is	O
sharply	O
peaked	O
at	O
k	O
which	O
is	O
the	O
correct	O
value	B
used	O
to	O
generate	O
the	O
data	O
the	O
clean	O
reconstructions	O
for	O
k	O
are	O
plotted	O
in	O
approximating	O
the	O
model	B
likelihood	B
for	O
a	O
model	B
with	O
continuous	B
parameter	B
vector	O
dim	O
k	O
and	O
data	O
d	O
the	O
model	B
likelihood	B
is	O
pdm	O
pd	O
mp	O
for	O
a	O
generic	O
expression	O
pd	O
mp	O
e	O
f	O
unless	O
f	O
is	O
of	O
a	O
particularly	O
simple	O
form	O
in	O
for	O
example	O
one	O
cannot	O
compute	O
the	O
integral	O
in	O
and	O
approximations	O
are	O
required	O
laplace	B
s	O
method	O
a	O
simple	O
approximation	B
of	O
is	O
given	O
by	O
laplace	B
s	O
method	O
log	O
h	O
log	O
pdm	O
f	O
where	O
is	O
the	O
map	B
solution	O
argmax	O
pd	O
mp	O
and	O
h	O
is	O
the	O
hessian	B
of	O
f	O
at	O
draft	O
march	O
exercises	O
for	O
data	O
d	O
that	O
is	O
i	O
i	O
d	O
generated	O
the	O
above	O
specialises	O
to	O
pdm	O
p	O
pxn	O
md	O
f	O
log	O
p	O
log	O
pxn	O
m	O
in	O
this	O
case	O
laplace	B
s	O
method	O
computes	O
the	O
optimum	O
of	O
the	O
function	B
bayes	B
information	I
criterion	I
for	O
i	O
i	O
d	O
data	O
the	O
hessian	B
scales	O
with	O
the	O
number	O
of	O
training	B
examples	O
n	O
and	O
a	O
crude	O
approximation	B
is	O
to	O
set	O
h	O
nik	O
where	O
k	O
dim	O
in	O
this	O
case	O
one	O
may	O
take	O
as	O
a	O
model	B
comparison	O
procedure	O
the	O
function	B
log	O
pdm	O
log	O
pd	O
m	O
log	O
p	O
k	O
log	O
k	O
log	O
n	O
for	O
a	O
simple	O
prior	B
that	O
penalises	O
the	O
length	O
of	O
the	O
parameter	B
vector	O
p	O
n	O
i	O
the	O
above	O
reduces	O
to	O
log	O
pdm	O
log	O
pd	O
m	O
k	O
log	O
n	O
the	O
bayes	O
information	O
approximates	O
by	O
ignoring	O
the	O
penalty	O
term	O
giving	O
bic	O
log	O
pd	O
m	O
k	O
log	O
n	O
the	O
bic	O
criterion	O
may	O
be	O
used	O
as	O
an	O
approximate	B
way	O
to	O
compare	O
models	O
where	O
the	O
term	O
k	O
log	O
n	O
penalises	O
model	B
complexity	O
in	O
general	O
the	O
laplace	B
approximation	B
equation	B
is	O
to	O
be	O
preferred	O
to	O
the	O
bic	O
criterion	O
since	O
it	O
more	O
correctly	O
accounts	O
for	O
the	O
uncertainty	B
in	O
the	O
posterior	B
parameter	B
estimate	O
other	O
techniques	O
that	O
aim	O
to	O
improve	O
on	O
the	O
laplace	B
method	O
are	O
discussed	O
in	O
and	O
exercises	O
exercise	O
write	O
a	O
program	O
to	O
implement	O
the	O
fairbiased	O
coin	O
tossing	O
model	B
selection	I
example	O
of	O
using	O
a	O
discrete	B
domain	B
for	O
explain	O
how	O
to	O
overcome	O
potential	B
numerical	B
issues	O
in	O
dealing	O
with	O
large	O
nh	O
and	O
nt	O
the	O
order	O
of	O
exercise	O
you	O
work	O
at	O
dodder	O
s	O
hedge	B
fund	I
and	O
the	O
manager	O
wants	O
to	O
model	B
next	O
day	O
returns	O
based	O
on	O
current	O
day	O
information	O
xt	O
the	O
vector	O
of	O
factors	O
each	O
day	O
xt	O
captures	O
essential	O
aspects	O
of	O
the	O
market	O
he	O
argues	O
that	O
a	O
simple	O
linear	B
model	B
wkxkt	O
should	O
be	O
reasonable	O
and	O
asks	O
you	O
to	O
find	O
the	O
weight	B
vector	O
w	O
based	O
on	O
historical	O
information	O
d	O
t	O
t	O
in	O
addition	O
he	O
also	O
gives	O
you	O
a	O
measure	O
of	O
the	O
volatility	O
t	O
for	O
each	O
day	O
under	O
the	O
assumption	O
that	O
the	O
returns	O
are	O
i	O
i	O
d	O
gaussian	B
distributed	O
n	O
w	O
pytxt	O
w	O
yt	O
wtxt	O
t	O
explain	O
how	O
to	O
set	O
the	O
weight	B
vector	O
w	O
by	O
maximum	B
likelihood	B
draft	O
march	O
exercises	O
your	O
hedge	B
fund	I
manager	O
is	O
however	O
convinced	O
that	O
some	O
of	O
the	O
factors	O
are	O
useless	O
for	O
prediction	O
and	O
wishes	O
to	O
remove	O
as	O
many	O
as	O
possible	O
to	O
do	O
this	O
you	O
decide	O
to	O
use	O
a	O
bayesian	B
model	B
selection	I
method	O
in	O
which	O
you	O
use	O
a	O
prior	B
pwm	O
n	O
i	O
where	O
m	O
indexes	O
the	O
model	B
each	O
model	B
uses	O
only	O
a	O
subset	O
of	O
the	O
factors	O
by	O
translating	O
the	O
integer	O
m	O
into	O
a	O
binary	O
vector	O
representation	O
the	O
model	B
describes	O
which	O
factors	O
are	O
to	O
be	O
used	O
for	O
example	O
if	O
k	O
there	O
would	O
be	O
models	O
where	O
the	O
first	O
model	B
is	O
yt	O
with	O
weight	B
prior	B
n	O
similarly	O
model	B
would	O
be	O
yt	O
with	O
n	O
you	O
decide	O
to	O
use	O
a	O
flat	O
prior	B
pm	O
const	O
draw	O
the	O
hierarchical	O
bayesian	B
network	O
for	O
this	O
model	B
and	O
explain	O
how	O
to	O
find	O
the	O
best	O
model	B
for	O
the	O
data	O
using	O
bayesian	B
model	B
selection	I
by	O
suitably	O
adapting	O
equation	B
using	O
the	O
data	O
dodder	O
mat	O
perform	O
bayesian	B
model	B
selection	I
as	O
above	O
for	O
k	O
and	O
find	O
which	O
of	O
the	O
factors	O
are	O
most	O
likely	O
to	O
explain	O
the	O
data	O
exercise	O
here	O
we	O
will	O
derive	O
the	O
expression	O
and	O
also	O
an	O
alternative	O
form	O
starting	O
from	O
pynw	O
xn	O
k	O
n	O
i	O
pw	O
yn	O
wt	O
nyn	O
wt	O
e	O
n	O
n	O
wtw	O
e	O
show	O
that	O
this	O
can	O
be	O
expressed	O
as	O
e	O
where	O
e	O
n	O
a	O
i	O
txn	O
wtawbtw	O
n	O
b	O
yn	O
by	O
completing	O
the	O
square	O
derive	O
since	O
each	O
yn	O
n	O
n	O
is	O
linearly	O
related	O
through	O
w	O
and	O
w	O
is	O
gaussian	B
distributed	O
the	O
joint	B
vector	O
yn	O
is	O
gaussian	B
distributed	O
using	O
the	O
gaussian	B
propagation	B
results	O
derive	O
an	O
alternative	O
expression	O
for	O
log	O
xn	O
draft	O
march	O
part	O
iii	O
machine	O
learning	B
chapter	O
machine	O
learning	B
concepts	O
styles	O
of	O
learning	B
broadly	O
speaking	O
the	O
main	O
two	O
subfields	O
of	O
machine	O
learning	B
are	O
supervised	B
learning	B
and	O
unsupervised	B
learning	B
in	O
supervised	B
learning	B
the	O
focus	O
is	O
on	O
accurate	O
prediction	O
whereas	O
in	O
unsupervised	B
learning	B
the	O
aim	O
is	O
to	O
find	O
accurate	O
compact	O
descriptions	O
of	O
the	O
data	O
particularly	O
in	O
supervised	B
learning	B
one	O
is	O
interested	O
in	O
methods	O
that	O
perform	O
well	O
on	O
previously	O
unseen	O
data	O
that	O
is	O
the	O
method	O
generalises	O
to	O
unseen	O
data	O
in	O
this	O
sense	O
one	O
distinguishes	O
between	O
data	O
that	O
is	O
used	O
to	O
train	O
a	O
model	B
and	O
data	O
that	O
is	O
used	O
to	O
test	O
the	O
performance	B
of	O
the	O
trained	O
model	B
see	O
supervised	B
learning	B
consider	O
a	O
database	O
of	O
face	O
images	O
each	O
represented	O
by	O
a	O
x	O
along	O
with	O
each	O
image	O
x	O
is	O
an	O
output	O
class	O
y	O
female	O
that	O
states	O
if	O
the	O
image	O
is	O
of	O
a	O
male	O
or	O
female	O
a	O
database	O
of	O
such	O
image-class	O
pairs	O
is	O
available	O
d	O
yn	O
n	O
the	O
task	O
is	O
to	O
make	O
an	O
accurate	O
predictor	O
yx	O
of	O
the	O
sex	O
of	O
a	O
novel	O
image	O
x	O
this	O
is	O
an	O
example	O
application	O
that	O
would	O
be	O
hard	B
to	O
program	O
in	O
a	O
traditional	O
programming	O
manner	O
since	O
formally	O
specifying	O
how	O
male	O
faces	B
differ	O
from	O
female	O
faces	B
is	O
difficult	O
an	O
alternative	O
is	O
to	O
give	O
examples	O
faces	B
and	O
their	O
gender	O
labels	O
and	O
let	O
a	O
machine	O
automatically	O
learn	O
a	O
rule	O
to	O
differentiate	O
male	O
from	O
female	O
faces	B
definition	O
learning	B
given	O
a	O
set	O
of	O
data	O
d	O
yn	O
n	O
n	O
the	O
task	O
is	O
to	O
learn	O
the	O
relationship	O
between	O
the	O
input	O
x	O
and	O
output	O
y	O
such	O
that	O
when	O
given	O
a	O
new	O
input	O
x	O
the	O
predicted	O
output	O
y	O
is	O
accurate	O
to	O
specify	O
explicitly	O
what	O
accuracy	O
means	O
one	O
defines	O
a	O
loss	B
function	B
lypred	O
ytrue	O
or	O
conversely	O
a	O
utility	B
function	B
u	O
l	O
in	O
supervised	B
learning	B
our	O
interest	O
is	O
describing	O
y	O
conditioned	O
on	O
knowing	O
x	O
from	O
a	O
probabilistic	B
modelling	B
perspective	O
we	O
are	O
therefore	O
concerned	O
primarily	O
with	O
the	O
conditional	B
distribution	B
pyxd	O
the	O
term	O
supervised	B
indicates	O
that	O
there	O
is	O
a	O
supervisor	O
specifying	O
the	O
output	O
y	O
for	O
each	O
input	O
x	O
in	O
the	O
available	O
data	O
d	O
the	O
output	O
is	O
also	O
called	O
a	O
label	O
particularly	O
when	O
discussing	O
classification	B
predicting	O
tomorrow	O
s	O
stock	O
price	O
yt	O
based	O
on	O
past	O
observations	O
yt	O
is	O
a	O
form	O
of	O
supervised	B
learning	B
we	O
have	O
a	O
collection	O
of	O
times	O
and	O
prices	O
d	O
yt	O
t	O
t	O
where	O
time	O
t	O
is	O
the	O
input	O
and	O
the	O
price	O
yt	O
is	O
the	O
output	O
an	O
m	O
n	O
face	O
image	O
with	O
elements	O
fmn	O
we	O
can	O
form	O
a	O
vector	O
by	O
stacking	O
the	O
entries	O
of	O
the	O
matrix	B
in	O
matlab	O
one	O
may	O
achieve	O
this	O
using	O
xf	O
train	O
test	O
styles	O
of	O
learning	B
figure	O
in	O
training	B
and	O
evaluating	O
a	O
model	B
conceptually	O
there	O
are	O
two	O
sources	O
of	O
data	O
the	O
parameters	O
of	O
the	O
model	B
are	O
set	O
on	O
the	O
basis	O
of	O
the	O
train	O
data	O
only	O
if	O
the	O
test	O
data	O
is	O
generated	O
from	O
the	O
same	O
underlying	O
process	O
that	O
generated	O
the	O
train	O
data	O
an	O
unbiased	O
estimate	O
of	O
the	O
generalisation	B
performance	B
can	O
be	O
obtained	O
by	O
measuring	O
the	O
test	O
data	O
performance	B
of	O
the	O
trained	O
model	B
importantly	O
the	O
test	O
performance	B
should	O
not	O
be	O
used	O
to	O
adjust	O
the	O
model	B
parameters	O
since	O
we	O
would	O
then	O
no	O
longer	O
have	O
an	O
independent	O
measure	O
of	O
the	O
performance	B
of	O
the	O
model	B
example	O
a	O
father	O
decides	O
to	O
teach	O
his	O
young	O
son	O
what	O
a	O
sports	O
car	O
is	O
finding	O
it	O
difficult	O
to	O
explain	O
in	O
words	O
he	O
decides	O
to	O
give	O
some	O
examples	O
they	O
stand	O
on	O
a	O
motorway	O
bridge	O
and	O
as	O
each	O
car	O
passes	O
underneath	O
the	O
father	O
cries	O
out	O
that	O
s	O
a	O
sports	O
car	O
when	O
a	O
sports	O
car	O
passes	O
by	O
after	O
ten	O
minutes	O
the	O
father	O
asks	O
his	O
son	O
if	O
he	O
s	O
understood	O
what	O
a	O
sports	O
car	O
is	O
the	O
son	O
says	O
sure	O
it	O
s	O
easy	O
an	O
old	O
red	O
vw	O
beetle	O
passes	O
by	O
and	O
the	O
son	O
shouts	O
that	O
s	O
a	O
sports	O
car	O
dejected	O
the	O
father	O
asks	O
why	O
do	O
you	O
say	O
that	O
because	O
all	O
sports	O
cars	O
are	O
red	O
replies	O
the	O
son	O
this	O
is	O
an	O
example	O
scenario	O
for	O
supervised	B
learning	B
here	O
the	O
father	O
plays	O
the	O
role	O
of	O
the	O
supervisor	O
and	O
his	O
son	O
is	O
the	O
student	O
learner	O
it	O
s	O
indicative	O
of	O
the	O
kinds	O
of	O
problems	O
encountered	O
in	O
machine	O
learning	B
in	O
that	O
it	O
is	O
not	O
really	O
clear	O
anyway	O
what	O
a	O
sports	O
car	O
is	O
if	O
we	O
knew	O
that	O
then	O
we	O
wouldn	O
t	O
need	O
to	O
go	O
through	O
the	O
process	O
of	O
learning	B
this	O
example	O
also	O
highlights	O
the	O
issue	O
that	O
there	O
is	O
a	O
difference	O
between	O
performing	O
well	O
on	O
training	B
data	O
and	O
performing	O
well	O
on	O
novel	O
test	O
data	O
the	O
main	O
interest	O
in	O
supervised	B
learning	B
is	O
to	O
discover	O
an	O
underlying	O
rule	O
that	O
will	O
generalise	O
well	O
leading	O
to	O
accurate	O
prediction	O
on	O
new	O
inputs	O
for	O
an	O
input	O
x	O
if	O
the	O
output	O
is	O
one	O
of	O
a	O
discrete	B
number	O
of	O
possible	O
classes	O
this	O
is	O
called	O
a	O
classification	B
problem	B
in	O
classification	B
problems	O
we	O
will	O
generally	O
use	O
c	O
for	O
the	O
output	O
for	O
an	O
input	O
x	O
if	O
the	O
output	O
is	O
continuous	B
this	O
is	O
called	O
a	O
regression	B
problem	B
for	O
example	O
based	O
on	O
historical	O
information	O
of	O
demand	O
for	O
sun-cream	O
in	O
your	O
supermarket	O
you	O
are	O
asked	O
to	O
predict	O
the	O
demand	O
for	O
the	O
next	O
month	O
in	O
some	O
cases	O
it	O
is	O
possible	O
to	O
discretise	O
a	O
continuous	B
output	O
and	O
then	O
consider	O
a	O
corresponding	O
classification	B
problem	B
however	O
in	O
other	O
cases	O
it	O
is	O
impractical	O
or	O
unnatural	O
to	O
do	O
this	O
for	O
example	O
if	O
the	O
output	O
y	O
is	O
a	O
high	O
dimensional	O
continuous	B
valued	O
vector	O
or	O
if	O
the	O
ordering	O
of	O
states	O
of	O
the	O
variable	O
is	O
meaningful	O
unsupervised	B
learning	B
definition	O
learning	B
given	O
a	O
set	O
of	O
data	O
d	O
n	O
n	O
in	O
unsupervised	B
learning	B
we	O
aim	O
to	O
to	O
learn	O
a	O
plausible	O
compact	O
description	O
of	O
the	O
data	O
an	O
objective	O
is	O
used	O
to	O
quantify	O
the	O
accuracy	O
of	O
the	O
description	O
in	O
unsupervised	B
learning	B
there	O
is	O
no	O
special	O
prediction	O
variable	O
from	O
a	O
probabilistic	B
perspective	O
we	O
are	O
interested	O
in	O
modelling	B
the	O
distribution	B
px	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
the	O
i	O
i	O
d	O
assumption	O
for	O
example	O
would	O
be	O
one	O
objective	O
measure	O
of	O
the	O
accuracy	O
of	O
the	O
description	O
draft	O
march	O
styles	O
of	O
learning	B
example	O
a	O
supermarket	O
chain	B
wishes	O
to	O
discover	O
how	O
many	O
different	O
basic	O
consumer	O
buying	O
behaviours	O
there	O
are	O
based	O
on	O
a	O
large	O
database	O
of	O
supermarket	O
checkout	O
data	O
items	O
brought	O
by	O
a	O
customer	O
on	O
a	O
visit	O
to	O
a	O
checkout	O
are	O
represented	O
by	O
a	O
sparse	B
dimensional	O
vector	O
x	O
which	O
contains	O
a	O
in	O
the	O
ith	O
element	O
if	O
the	O
customer	O
bought	O
product	O
i	O
and	O
otherwise	O
based	O
on	O
million	O
such	O
checkout	O
vectors	O
from	O
stores	O
across	O
the	O
country	O
d	O
n	O
the	O
supermarket	O
chain	B
wishes	O
to	O
discover	O
patterns	O
of	O
buying	O
behaviour	O
in	O
the	O
table	O
each	O
column	O
represents	O
the	O
buying	O
patterns	O
of	O
a	O
customer	O
customer	O
records	O
and	O
just	O
the	O
first	O
of	O
the	O
products	O
are	O
shown	O
a	O
indicates	O
that	O
the	O
customer	O
bought	O
that	O
item	O
we	O
wish	O
to	O
find	O
common	O
patterns	O
in	O
the	O
data	O
such	O
as	O
if	O
someone	O
buys	O
coffee	O
they	O
are	O
also	O
likely	O
to	O
buy	O
milk	O
coffee	O
tea	O
milk	O
beer	O
diapers	O
aspirin	O
example	O
the	O
table	O
on	O
the	O
right	O
represents	O
a	O
collection	O
of	O
unlabelled	B
two-dimensional	O
points	O
we	O
can	O
visualise	O
this	O
data	O
by	O
plotting	O
it	O
in	O
dimensions	O
by	O
simply	O
eye-balling	O
the	O
data	O
we	O
can	O
see	O
that	O
there	O
are	O
two	O
apparent	O
clusters	O
here	O
one	O
centred	O
around	O
and	O
the	O
other	O
around	O
a	O
reasonable	O
model	B
to	O
describe	O
this	O
data	O
might	O
therefore	O
be	O
to	O
describe	O
it	O
as	O
two	O
clusters	O
centred	O
at	O
and	O
each	O
with	O
a	O
standard	B
deviation	I
of	O
around	O
anomaly	B
detection	I
a	O
baby	O
processes	O
a	O
mass	O
of	O
initially	O
confusing	O
sensory	O
data	O
after	O
a	O
while	O
the	O
baby	O
begins	O
to	O
understand	O
her	O
environment	O
in	O
the	O
sense	O
that	O
novel	O
sensory	O
data	O
from	O
the	O
same	O
environment	O
is	O
familiar	O
or	O
expected	O
when	O
a	O
strange	O
face	O
presents	O
itself	O
the	O
baby	O
recognises	O
that	O
this	O
is	O
not	O
familiar	O
and	O
may	O
be	O
upset	O
the	O
baby	O
has	O
learned	O
a	O
representation	O
of	O
the	O
familiar	O
and	O
can	O
distinguish	O
the	O
expected	O
from	O
the	O
unexpected	O
this	O
is	O
an	O
example	O
of	O
unsupervised	B
learning	B
models	O
that	O
can	O
detect	O
irregular	O
events	O
are	O
used	O
in	O
plant	B
monitoring	I
and	O
require	O
a	O
model	B
of	O
normality	O
which	O
will	O
in	O
most	O
cases	O
be	O
based	O
on	O
unlabelled	B
data	I
online	B
learning	B
in	O
the	O
above	O
situations	O
we	O
assumed	O
that	O
the	O
data	O
d	O
was	O
given	O
beforehand	O
in	O
online	B
learning	B
data	O
arrives	O
sequentially	O
and	O
we	O
want	O
to	O
continually	O
update	O
our	O
model	B
as	O
new	O
data	O
becomes	O
available	O
online	B
learning	B
may	O
occur	O
in	O
either	O
a	O
supervised	B
or	O
unsupervised	B
context	O
interacting	O
with	O
the	O
environment	O
in	O
many	O
real-world	O
situations	O
an	O
agent	O
is	O
able	O
to	O
interact	O
in	O
some	O
manner	O
with	O
its	O
environment	O
query	B
learning	B
here	O
the	O
agent	O
has	O
the	O
ability	O
to	O
request	O
data	O
from	O
the	O
environment	O
for	O
example	O
a	O
predictor	O
might	O
recognise	O
that	O
it	O
is	O
less	O
confidently	O
able	O
to	O
predict	O
in	O
certain	O
regions	O
of	O
the	O
space	O
x	O
and	O
therefore	O
requests	O
more	O
training	B
data	O
in	O
this	O
region	O
active	B
learning	B
can	O
also	O
be	O
considered	O
in	O
an	O
unsupervised	B
context	O
in	O
which	O
the	O
agent	O
might	O
request	O
information	O
in	O
regions	O
where	O
px	O
looks	O
uninformative	O
or	O
flat	O
draft	O
march	O
supervised	B
learning	B
reinforcement	B
learning	B
one	O
might	O
term	O
this	O
also	O
survival	O
learning	B
one	O
has	O
in	O
mind	O
scenarios	O
such	O
as	O
encountered	O
in	O
real-life	O
where	O
an	O
organism	O
needs	O
to	O
learn	O
the	O
best	O
actions	O
to	O
take	O
in	O
its	O
environment	O
in	O
order	O
to	O
survive	O
as	O
long	O
as	O
possible	O
in	O
each	O
situation	O
in	O
which	O
the	O
agent	O
finds	O
itself	O
it	O
needs	O
to	O
take	O
an	O
action	O
some	O
actions	O
may	O
eventually	O
be	O
beneficial	O
to	O
food	O
for	O
example	O
whilst	O
others	O
may	O
be	O
disastrous	O
to	O
being	O
eaten	O
for	O
example	O
based	O
on	O
accumulated	O
experience	O
the	O
agent	O
needs	O
to	O
learn	O
which	O
action	O
to	O
take	O
in	O
a	O
given	O
situation	O
in	O
order	O
to	O
obtain	O
a	O
desired	O
long	O
term	O
goal	O
essentially	O
actions	O
that	O
lead	O
to	O
long	O
term	O
rewards	O
need	O
to	O
reinforced	O
reinforcement	B
learning	B
has	O
connections	O
with	O
control	O
theory	O
markov	O
decision	O
processes	O
and	O
game	O
theory	O
whilst	O
we	O
discussed	O
mdps	O
and	O
briefly	O
mentioned	O
how	O
an	O
environment	O
can	O
be	O
learned	O
based	O
on	O
delayed	O
rewards	O
in	O
we	O
will	O
not	O
discuss	O
this	O
topic	O
further	O
in	O
this	O
book	O
semi-supervised	B
learning	B
in	O
machine	O
learning	B
a	O
common	O
scenario	O
is	O
to	O
have	O
a	O
small	O
amount	O
of	O
labelled	B
and	O
a	O
large	O
amount	O
of	O
unlabelled	B
data	I
for	O
example	O
it	O
may	O
be	O
that	O
we	O
have	O
access	O
to	O
many	O
images	O
of	O
faces	B
however	O
only	O
a	O
small	O
number	O
of	O
them	O
may	O
have	O
been	O
labelled	B
as	O
instances	O
of	O
known	O
faces	B
in	O
semi-supervised	B
learning	B
one	O
tries	O
to	O
use	O
the	O
unlabelled	B
data	I
to	O
make	O
a	O
better	O
classifier	B
than	O
that	O
based	O
on	O
the	O
labelled	B
data	I
alone	O
supervised	B
learning	B
supervised	B
and	O
unsupervised	B
learning	B
are	O
mature	O
fields	O
with	O
a	O
wide	O
range	O
of	O
practical	O
tools	O
and	O
associated	O
theoretical	O
analyses	O
our	O
aim	O
here	O
is	O
to	O
give	O
a	O
brief	O
introduction	O
to	O
the	O
issues	O
and	O
philosophies	O
behind	O
the	O
approaches	O
we	O
focus	O
here	O
mainly	O
on	O
supervised	B
learning	B
and	O
classification	B
in	O
particular	O
utility	B
and	O
loss	O
to	O
more	O
fully	O
specify	O
a	O
supervised	B
problem	B
we	O
need	O
to	O
be	O
clear	O
what	O
cost	O
is	O
involved	O
in	O
making	O
a	O
correct	O
or	O
incorrect	O
prediction	O
in	O
a	O
two	O
class	O
problem	B
domc	O
we	O
assume	O
here	O
that	O
everything	O
we	O
know	O
about	O
the	O
environment	O
is	O
contained	O
in	O
a	O
model	B
px	O
c	O
given	O
a	O
new	O
input	O
x	O
the	O
optimal	O
prediction	O
also	O
depends	O
on	O
how	O
costly	O
making	O
an	O
error	O
is	O
this	O
can	O
be	O
quantified	O
using	O
a	O
loss	B
function	B
conversely	O
a	O
utility	B
in	O
forming	O
a	O
decision	B
function	B
cx	O
that	O
will	O
produce	O
a	O
class	O
label	O
for	O
the	O
new	O
input	O
x	O
we	O
don	O
t	O
know	O
the	O
true	O
class	O
only	O
our	O
presumed	O
distribution	B
pcx	O
the	O
expected	O
utility	B
for	O
the	O
decision	B
function	B
is	O
ctrue	O
ucx	O
uctrue	O
cx	O
and	O
the	O
optimal	O
decision	O
is	O
that	O
which	O
maximises	O
the	O
expected	O
utility	B
zero-one	B
loss	I
if	O
c	O
uctrue	O
c	O
ctrue	O
for	O
the	O
two	O
class	O
case	O
we	O
then	O
have	O
a	O
count	O
the	O
correct	O
predictions	O
measure	O
of	O
prediction	O
performance	B
is	O
based	O
on	O
the	O
zero-one	B
utility	B
conversely	O
the	O
zero-one	B
loss	I
if	O
c	O
ctrue	O
pctrue	O
for	O
cx	O
pctrue	O
for	O
cx	O
hence	O
in	O
order	O
to	O
have	O
the	O
highest	O
expected	O
utility	B
the	O
decision	B
function	B
cx	O
should	O
correspond	O
to	O
if	O
pc	O
selecting	O
the	O
highest	O
class	O
probability	O
pcx	O
if	O
pc	O
cx	O
ucx	O
in	O
the	O
case	O
of	O
a	O
tie	O
either	O
class	O
is	O
selected	O
at	O
random	O
with	O
equal	O
probability	O
draft	O
march	O
supervised	B
learning	B
general	O
loss	O
functions	O
in	O
general	O
for	O
a	O
two-class	O
problem	B
we	O
have	O
uctrue	O
c	O
uctrue	O
c	O
for	O
cx	O
uctrue	O
c	O
uctrue	O
c	O
for	O
cx	O
ucx	O
and	O
the	O
optimal	O
decision	B
function	B
cx	O
chooses	O
that	O
class	O
with	O
highest	O
expected	O
utility	B
one	O
can	O
readily	O
generalise	O
this	O
to	O
multiple-class	O
situations	O
using	O
a	O
utility	B
matrix	B
with	O
elements	O
uij	O
uctrue	O
i	O
cpred	O
j	O
where	O
the	O
i	O
j	O
element	O
of	O
the	O
matrix	B
contains	O
the	O
utility	B
of	O
predicting	O
class	O
j	O
when	O
the	O
true	O
class	O
is	O
i	O
conversely	O
one	O
could	O
think	O
of	O
a	O
loss-matrix	O
with	O
entries	O
lij	O
uij	O
the	O
expected	O
loss	O
with	O
respect	O
to	O
pcx	O
is	O
then	O
termed	O
the	O
risk	B
in	O
some	O
applications	O
the	O
utility	B
matrix	B
is	O
highly	O
non-symmetric	O
consider	O
a	O
medical	O
scenario	O
in	O
which	O
we	O
are	O
asked	O
to	O
predict	O
whether	O
or	O
not	O
the	O
patient	O
has	O
cancer	O
domc	O
benign	O
if	O
the	O
true	O
class	O
is	O
cancer	O
yet	O
we	O
predict	O
benign	O
this	O
could	O
have	O
terrible	O
consequences	O
for	O
the	O
patient	O
on	O
the	O
other	O
hand	O
if	O
the	O
class	O
is	O
benign	O
yet	O
we	O
predict	O
cancer	O
this	O
may	O
be	O
less	O
disastrous	O
for	O
the	O
patient	O
such	O
asymmetric	O
utilities	O
can	O
bias	B
the	O
predictions	O
in	O
favour	O
of	O
conservative	O
decisions	O
in	O
the	O
cancer	O
case	O
we	O
would	O
be	O
more	O
inclined	O
to	O
decide	O
the	O
sample	O
is	O
cancerous	O
than	O
benign	O
even	O
if	O
the	O
predictive	O
probability	O
of	O
the	O
two	O
classes	O
is	O
equal	O
what	O
s	O
the	O
catch	O
in	O
solving	B
for	O
the	O
optimal	O
decision	B
function	B
cx	O
above	O
we	O
are	O
assuming	O
that	O
the	O
model	B
pcx	O
is	O
correct	O
the	O
catch	O
is	O
therefore	O
that	O
in	O
practice	O
we	O
typically	O
don	O
t	O
know	O
the	O
correct	O
model	B
underlying	O
the	O
data	O
all	O
we	O
have	O
is	O
a	O
dataset	O
of	O
examples	O
d	O
yn	O
n	O
n	O
and	O
our	O
domain	B
knowledge	O
we	O
want	O
our	O
method	O
to	O
perform	O
well	O
not	O
just	O
on	O
a	O
specifically	O
chosen	O
x	O
but	O
any	O
new	O
input	O
that	O
could	O
come	O
along	O
that	O
is	O
we	O
want	O
it	O
to	O
generalise	O
to	O
novel	O
inputs	O
this	O
means	O
we	O
also	O
need	O
a	O
model	B
for	O
px	O
in	O
order	O
to	O
measure	O
what	O
the	O
expected	O
performance	B
of	O
our	O
decision	B
function	B
would	O
be	O
hence	O
we	O
require	O
knowledge	O
of	O
the	O
joint	B
distribution	B
pc	O
x	O
pcxpx	O
we	O
therefore	O
need	O
to	O
form	O
a	O
distribution	B
px	O
cd	O
which	O
should	O
ideally	O
be	O
close	O
to	O
the	O
true	O
but	O
unknown	O
joint	B
data	O
distribution	B
communities	O
of	O
researchers	O
in	O
machine	O
learning	B
form	O
around	O
different	O
strategies	O
to	O
address	O
the	O
lack	O
of	O
knowledge	O
about	O
the	O
true	O
pc	O
x	O
using	O
the	O
empirical	B
distribution	B
a	O
direct	O
approach	B
to	O
not	O
knowing	O
the	O
correct	O
model	B
ptruec	O
x	O
is	O
to	O
replace	O
it	O
with	O
the	O
empirical	B
distribution	B
px	O
cd	O
n	O
xn	O
cn	O
n	O
that	O
is	O
we	O
assume	O
that	O
the	O
underlying	O
distribution	B
is	O
approximated	O
by	O
placing	O
equal	O
mass	O
on	O
each	O
of	O
the	O
points	O
cn	O
in	O
the	O
dataset	O
using	O
this	O
gives	O
the	O
empirical	B
utility	B
n	O
or	O
conversely	O
the	O
empirical	B
risk	B
n	O
r	O
n	O
lcn	O
cxn	O
draft	O
march	O
ucn	O
cxn	O
supervised	B
learning	B
train	O
validate	O
test	O
figure	O
models	O
can	O
be	O
trained	O
using	O
the	O
train	O
data	O
based	O
on	O
different	O
regularisation	B
parameters	O
the	O
optimal	O
regularisation	B
parameter	B
is	O
determined	O
by	O
the	O
empirical	B
performance	B
on	O
the	O
validation	B
data	O
an	O
independent	O
measure	O
of	O
the	O
generalisation	B
performance	B
is	O
obtained	O
by	O
using	O
a	O
separate	O
test	B
set	I
assuming	O
the	O
loss	O
is	O
minimal	O
when	O
the	O
correct	O
class	O
is	O
predicted	O
the	O
optimal	O
decision	O
cx	O
for	O
any	O
input	O
in	O
the	O
train	B
set	I
is	O
trivially	O
given	O
by	O
cxn	O
cn	O
however	O
for	O
any	O
new	O
x	O
not	O
contained	O
in	O
d	O
then	O
cx	O
is	O
undefined	O
in	O
order	O
to	O
define	O
the	O
class	O
of	O
a	O
novel	O
input	O
one	O
may	O
use	O
a	O
parametric	O
function	B
for	O
example	O
for	O
a	O
two	O
class	O
problem	B
domc	O
a	O
linear	B
decision	B
function	B
is	O
given	O
by	O
cx	O
fx	O
if	O
tx	O
if	O
tx	O
fx	O
if	O
the	O
vector	O
input	O
x	O
is	O
on	O
the	O
positive	O
side	O
of	O
a	O
hyperplane	B
defined	O
by	O
the	O
vector	O
and	O
bias	B
we	O
assign	O
it	O
to	O
class	O
otherwise	O
to	O
class	O
return	O
to	O
the	O
geometric	O
interpretation	O
of	O
this	O
in	O
the	O
empirical	B
risk	B
then	O
becomes	O
a	O
function	B
of	O
the	O
parameters	O
n	O
r	O
n	O
lcn	O
fxn	O
the	O
optimal	O
parameters	O
are	O
given	O
by	O
minimising	O
the	O
empirical	B
risk	B
with	O
respect	O
to	O
opt	O
argmin	O
r	O
the	O
decision	O
for	O
a	O
new	O
datapoint	O
x	O
is	O
then	O
given	O
by	O
fx	O
opt	O
in	O
this	O
empirical	B
risk	B
minimisation	I
approach	B
as	O
we	O
make	O
the	O
decision	B
function	B
fx	O
more	O
complex	O
the	O
empirical	B
risk	B
goes	O
down	O
if	O
we	O
make	O
fx	O
too	O
complex	O
we	O
will	O
have	O
no	O
confidence	O
fx	O
will	O
perform	O
well	O
on	O
a	O
novel	O
input	O
x	O
to	O
constrain	O
the	O
complexity	O
of	O
fx	O
we	O
may	O
minimise	O
the	O
penalised	B
empirical	B
risk	B
r	O
r	O
p	O
for	O
the	O
linear	B
decision	B
function	B
above	O
it	O
is	O
reasonable	O
to	O
penalise	O
wildly	O
changing	O
classifications	O
in	O
the	O
sense	O
that	O
if	O
we	O
change	O
the	O
input	O
x	O
by	O
only	O
a	O
small	O
amount	O
we	O
expect	O
average	B
minimal	O
change	O
in	O
the	O
class	O
label	O
the	O
squared	O
difference	O
in	O
tx	O
for	O
two	O
inputs	O
and	O
t	O
where	O
x	O
by	O
constraining	O
the	O
length	O
of	O
to	O
be	O
small	O
we	O
would	O
then	O
limit	O
the	O
ability	O
of	O
the	O
classifier	B
to	O
change	O
class	O
for	O
only	O
a	O
small	O
change	O
in	O
input	O
this	O
motivates	O
a	O
penalised	B
risk	B
of	O
the	O
form	O
r	O
r	O
t	O
where	O
is	O
a	O
regularising	O
constant	O
we	O
subsequently	O
minimise	O
this	O
penalised	B
empirical	B
risk	B
with	O
respect	O
to	O
we	O
discuss	O
how	O
to	O
find	O
an	O
appropriate	O
setting	O
for	O
the	O
regularisation	B
constant	O
below	O
validation	B
in	O
penalised	B
empirical	B
risk	B
minimisation	I
we	O
need	O
to	O
set	O
the	O
regularisation	B
parameter	B
this	O
can	O
be	O
achieved	O
by	O
evaluating	O
the	O
performance	B
of	O
the	O
learned	O
classifier	B
fx	O
on	O
validation	B
data	O
dvalidate	O
for	O
several	O
different	O
values	O
and	O
choosing	O
the	O
one	O
with	O
the	O
best	O
performance	B
it	O
s	O
important	O
that	O
the	O
validation	B
data	O
is	O
not	O
the	O
data	O
on	O
which	O
the	O
model	B
was	O
trained	O
since	O
we	O
know	O
that	O
the	O
optimal	O
setting	O
for	O
in	O
that	O
case	O
is	O
zero	O
and	O
again	O
we	O
will	O
have	O
no	O
confidence	O
in	O
the	O
generalisation	B
ability	O
the	O
distance	O
between	O
two	O
datapoints	O
is	O
distributed	O
according	O
to	O
an	O
isotropic	B
multivariate	B
gaussian	B
with	O
zero	O
t	O
motivating	O
the	O
choice	O
of	O
the	O
euclidean	O
squared	O
mean	B
and	O
covariance	B
the	O
average	B
squared	O
change	O
is	O
length	O
of	O
the	O
parameter	B
as	O
the	O
penalty	O
term	O
t	O
draft	O
march	O
supervised	B
learning	B
algorithm	B
setting	O
regularisation	B
parameters	O
using	O
cross-validation	B
choose	O
a	O
set	O
of	O
training	B
and	O
validation	B
set	O
choose	O
a	O
set	O
of	O
regularisation	B
parameters	O
a	O
di	O
traindi	O
i	O
i	O
validate	O
for	O
a	O
to	O
a	O
do	O
for	O
i	O
to	O
i	O
do	O
a	O
argmin	O
i	O
train	O
ap	O
l	O
a	O
end	O
for	O
l	O
a	O
i	O
end	O
for	O
opt	O
argmin	O
a	O
r	O
i	O
validate	O
adi	O
train	O
and	O
validation	B
di	O
given	O
an	O
original	O
dataset	O
d	O
we	O
split	O
this	O
into	O
disjoint	O
parts	O
dtraindvalidate	O
where	O
the	O
size	O
of	O
the	O
validation	B
set	O
is	O
usually	O
chosen	O
to	O
be	O
smaller	O
than	O
the	O
train	B
set	I
for	O
each	O
parameter	B
a	O
one	O
then	O
finds	O
the	O
minimal	O
empirical	B
risk	B
parameter	B
a	O
this	O
splitting	O
procedure	O
is	O
repeated	O
each	O
time	O
producing	O
a	O
separate	O
training	B
di	O
validation	B
set	O
along	O
with	O
an	O
optimal	O
penalised	B
empirical	B
risk	B
parameter	B
i	O
a	O
and	O
associated	O
validation	B
performance	B
r	O
i	O
validate	O
the	O
performance	B
of	O
regularisation	B
parameter	B
a	O
is	O
taken	O
as	O
the	O
average	B
of	O
the	O
validation	B
performances	O
over	O
i	O
the	O
best	O
regularisation	B
parameter	B
is	O
then	O
given	O
as	O
that	O
with	O
the	O
minimal	O
average	B
validation	B
error	O
see	O
and	O
using	O
the	O
optimal	O
regularisation	B
parameter	B
many	O
practitioners	O
retrain	O
on	O
the	O
basis	O
of	O
the	O
whole	O
dataset	O
d	O
in	O
cross-validation	B
a	O
dataset	O
is	O
partitioned	B
into	O
training	B
and	O
validation	B
sets	O
multiple	O
times	O
with	O
validation	B
results	O
obtained	O
for	O
each	O
partition	O
more	O
specifically	O
in	O
k-fold	O
cross	B
validation	B
the	O
data	O
d	O
is	O
split	O
into	O
k	O
validate	O
this	O
gives	O
a	O
total	O
of	O
equal	O
sized	O
disjoint	O
parts	O
then	O
di	O
k	O
different	O
training-validation	O
sets	O
over	O
which	O
performance	B
is	O
averaged	O
see	O
in	O
practice	O
cross	B
validation	B
is	O
popular	O
as	O
is	O
leave-one-out	O
cross	B
validation	B
in	O
which	O
the	O
validation	B
sets	O
consist	O
of	O
only	O
a	O
single	O
example	O
validate	O
di	O
and	O
di	O
train	O
ddi	O
adi	O
benefits	O
of	O
the	O
empirical	B
risk	B
approach	B
for	O
a	O
utility	B
uctrue	O
cpred	O
and	O
penalty	O
p	O
the	O
empirical	B
risk	B
approach	B
is	O
summarised	O
in	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
training	B
data	O
the	O
empirical	B
distribution	B
will	O
tend	O
towards	O
the	O
correct	O
distribution	B
the	O
discriminant	O
function	B
is	O
chosen	O
on	O
the	O
basis	O
of	O
minimal	O
risk	B
which	O
is	O
the	O
quantity	O
we	O
are	O
ultimately	O
interested	O
in	O
the	O
procedure	O
is	O
conceptually	O
straightforward	O
train	O
validate	O
train	O
validate	O
train	O
validate	O
train	O
test	O
draft	O
march	O
figure	O
in	O
cross-validation	B
the	O
original	O
dataset	O
is	O
split	O
into	O
several	O
train-validation	O
sets	O
depicted	O
is	O
cross-validation	B
for	O
a	O
range	O
of	O
regularisation	B
parameters	O
the	O
optimal	O
regularisation	B
parameter	B
is	O
found	O
based	O
on	O
the	O
empirical	B
validation	B
performance	B
averaged	O
across	O
the	O
different	O
splits	O
x	O
px	O
c	O
x	O
c	O
fx	O
uc	O
fx	O
p	O
supervised	B
learning	B
figure	O
empirical	B
risk	B
approach	B
given	O
the	O
dataset	O
x	O
a	O
model	B
of	O
the	O
data	O
px	O
c	O
is	O
made	O
usually	O
using	O
the	O
empirical	B
distribution	B
for	O
a	O
classifier	B
fx	O
the	O
parameter	B
is	O
learned	O
by	O
maximising	O
the	O
penalised	B
empirical	B
utility	B
minimising	O
empirical	B
risk	B
with	O
respect	O
to	O
the	O
penalty	O
parameter	B
is	O
set	O
by	O
validation	B
a	O
novel	O
input	O
x	O
is	O
then	O
assigned	O
to	O
class	O
fx	O
given	O
this	O
optimal	O
figure	O
the	O
unregularised	O
fit	O
to	O
training	B
given	O
by	O
whilst	O
the	O
training	B
data	O
is	O
well	O
the	O
regularised	B
fit	O
whilst	O
the	O
fitted	O
the	O
error	O
on	O
the	O
validation	B
examples	O
is	O
high	O
train	O
error	O
is	O
high	O
the	O
validation	B
error	O
is	O
all	O
important	O
is	O
low	O
the	O
true	O
function	B
which	O
generated	O
this	O
noisy	O
data	O
is	O
the	O
dashed	O
line	O
the	O
function	B
learned	O
from	O
the	O
data	O
is	O
given	O
by	O
the	O
solid	O
line	O
drawbacks	O
of	O
the	O
empirical	B
risk	B
approach	B
it	O
seems	O
extreme	O
to	O
assume	O
that	O
the	O
data	O
follows	O
the	O
empirical	B
distribution	B
particularly	O
for	O
small	O
amounts	O
of	O
training	B
data	O
to	O
generalise	O
well	O
we	O
need	O
to	O
make	O
sensible	O
assumptions	O
as	O
to	O
px	O
that	O
is	O
the	O
distribution	B
for	O
all	O
x	O
that	O
could	O
arise	O
if	O
the	O
utility	B
loss	B
function	B
changes	O
the	O
discriminant	O
function	B
needs	O
to	O
be	O
retrained	O
some	O
problems	O
require	O
an	O
estimate	O
of	O
the	O
confidence	O
of	O
the	O
prediction	O
whilst	O
there	O
may	O
be	O
heuristic	O
ways	O
to	O
evaluating	O
confidence	O
in	O
the	O
prediction	O
this	O
is	O
not	O
inherent	O
in	O
the	O
framework	O
when	O
there	O
are	O
multiple	O
penalty	O
parameters	O
performing	O
cross	B
validation	B
in	O
a	O
discretised	O
grid	O
of	O
the	O
parameters	O
becomes	O
infeasible	O
it	O
seems	O
a	O
shame	O
to	O
discard	O
all	O
those	O
trained	O
models	O
in	O
cross-validation	B
can	O
t	O
they	O
be	O
combined	O
in	O
some	O
manner	O
and	O
used	O
to	O
make	O
a	O
better	O
predictor	O
example	O
a	O
good	O
regularisation	B
parameter	B
in	O
we	O
fit	O
the	O
function	B
a	O
sinwx	O
to	O
data	O
learning	B
the	O
parameters	O
a	O
and	O
w	O
the	O
unregularised	O
solution	O
badly	O
overfits	O
the	O
data	O
and	O
has	O
a	O
high	O
validation	B
error	O
to	O
encourage	O
a	O
smoother	O
solution	O
a	O
regularisation	B
term	O
ereg	O
is	O
used	O
the	O
validation	B
error	O
based	O
on	O
several	O
different	O
values	O
of	O
the	O
regularisation	B
parameter	B
was	O
computed	O
finding	O
that	O
gave	O
a	O
low	O
validation	B
error	O
the	O
resulting	O
fit	O
to	O
novel	O
data	O
is	O
reasonable	O
bayesian	B
decision	O
approach	B
an	O
alternative	O
to	O
using	O
the	O
empirical	B
distribution	B
is	O
to	O
fit	O
a	O
model	B
pc	O
x	O
to	O
the	O
train	O
data	O
d	O
given	O
this	O
model	B
the	O
decision	B
function	B
cx	O
is	O
automatically	O
determined	O
from	O
the	O
maximal	O
expected	O
utility	B
draft	O
march	O
supervised	B
learning	B
x	O
px	O
c	O
x	O
pcx	O
uc	O
c	O
c	O
figure	O
bayesian	B
decision	O
approach	B
a	O
model	B
px	O
c	O
is	O
fitted	O
to	O
the	O
data	O
after	O
leaning	O
this	O
model	B
is	O
used	O
to	O
compute	O
pcx	O
for	O
a	O
novel	O
x	O
we	O
then	O
find	O
the	O
distribution	B
of	O
the	O
assumed	O
truth	O
pcx	O
the	O
prediction	O
is	O
then	O
given	O
by	O
that	O
c	O
which	O
maximises	O
the	O
expected	O
utility	B
c	O
minimal	O
risk	B
with	O
respect	O
to	O
this	O
model	B
as	O
in	O
equation	B
in	O
which	O
the	O
unknown	O
pctruex	O
is	O
replaced	O
with	O
pcx	O
this	O
approach	B
therefore	O
divorces	O
learning	B
the	O
parameters	O
of	O
pc	O
x	O
from	O
the	O
utility	B
loss	O
benefits	O
of	O
the	O
bayesian	B
decision	O
approach	B
this	O
is	O
a	O
conceptually	O
clean	O
approach	B
in	O
which	O
one	O
tries	O
ones	O
best	O
to	O
model	B
the	O
environment	O
independent	O
of	O
the	O
subsequent	O
decision	O
process	O
in	O
this	O
case	O
learning	B
the	O
environment	O
is	O
separated	O
from	O
the	O
ultimate	O
effect	O
this	O
will	O
have	O
on	O
the	O
expected	O
utility	B
the	O
ultimate	O
decision	O
c	O
for	O
a	O
novel	O
input	O
x	O
can	O
be	O
a	O
highly	O
complex	O
function	B
of	O
x	O
due	O
to	O
the	O
maximisation	B
operation	O
drawbacks	O
of	O
the	O
bayesian	B
decision	O
approach	B
if	O
the	O
environment	O
model	B
pc	O
x	O
is	O
poor	O
the	O
prediction	O
c	O
could	O
be	O
highly	O
inaccurate	O
since	O
mod	O
elling	O
the	O
environment	O
is	O
divorced	O
from	O
prediction	O
to	O
avoid	O
fully	O
divorcing	O
the	O
learning	B
of	O
the	O
model	B
pc	O
x	O
from	O
its	O
effect	O
on	O
decisions	O
in	O
practice	O
one	O
often	O
includes	O
regularisation	B
terms	O
in	O
the	O
environment	O
model	B
pc	O
x	O
which	O
are	O
set	O
by	O
validation	B
based	O
on	O
an	O
empirical	B
utility	B
there	O
are	O
two	O
main	O
approaches	O
to	O
fitting	O
pc	O
x	O
to	O
data	O
d	O
we	O
could	O
parameterise	O
the	O
joint	B
distribution	B
using	O
pc	O
x	O
pcx	O
cxpx	O
x	O
discriminative	B
approach	B
or	O
pc	O
x	O
pxc	O
xcpc	O
c	O
generative	B
approach	B
we	O
ll	O
consider	O
these	O
two	O
approaches	O
below	O
in	O
the	O
context	O
of	O
trying	O
to	O
make	O
a	O
system	O
that	O
can	O
distinguish	O
between	O
a	O
male	O
and	O
female	O
face	O
we	O
have	O
a	O
database	O
of	O
face	O
images	O
in	O
which	O
each	O
image	O
is	O
represented	O
as	O
a	O
real-valued	O
vector	O
xn	O
n	O
n	O
along	O
with	O
a	O
label	O
cn	O
stating	O
if	O
the	O
image	O
is	O
male	O
or	O
female	O
generative	B
approach	B
px	O
c	O
pxc	O
xcpc	O
c	O
for	O
simplicity	O
we	O
use	O
maximum	B
likelihood	B
training	B
for	O
the	O
parameters	O
assuming	O
the	O
data	O
d	O
is	O
i	O
i	O
d	O
we	O
have	O
a	O
log	O
likelihood	B
log	O
pd	O
log	O
pxncn	O
xc	O
n	O
n	O
log	O
pcn	O
c	O
as	O
we	O
see	O
the	O
dependence	O
on	O
xc	O
occurs	O
only	O
in	O
the	O
first	O
term	O
and	O
c	O
only	O
occurs	O
in	O
the	O
second	O
this	O
means	O
that	O
learning	B
the	O
optimal	O
parameters	O
is	O
equivalent	B
to	O
isolating	O
the	O
data	O
for	O
the	O
male-class	O
and	O
draft	O
march	O
supervised	B
learning	B
c	O
cn	O
xc	O
cx	O
x	O
xn	O
n	O
cn	O
xn	O
n	O
figure	O
two	O
generic	O
strategies	O
for	O
probabilisa	O
class	O
dependent	O
generative	B
tic	O
classification	B
model	B
of	O
x	O
after	O
learning	B
parameters	O
classification	B
is	O
obtained	O
by	O
making	O
x	O
evidential	O
and	O
inferring	O
a	O
discriminative	B
classification	B
method	O
pcx	O
pcx	O
fitting	O
a	O
model	B
pxc	O
male	O
xmale	O
we	O
similarly	O
isolate	O
the	O
female	O
data	O
and	O
fit	O
a	O
separate	O
model	B
pxc	O
female	O
xfemale	O
the	O
class	O
distribution	B
pc	O
c	O
can	O
be	O
easily	O
set	O
according	O
to	O
the	O
ratio	O
of	O
malesfemales	O
in	O
the	O
set	O
of	O
training	B
data	O
to	O
make	O
a	O
classification	B
of	O
a	O
new	O
image	O
x	O
as	O
either	O
male	O
or	O
female	O
we	O
may	O
use	O
pc	O
malex	O
px	O
c	O
male	O
xmale	O
px	O
c	O
male	O
xmale	O
px	O
c	O
female	O
xfemale	O
based	O
on	O
zero-one	B
loss	I
if	O
this	O
probability	O
is	O
greater	O
than	O
we	O
classify	O
x	O
as	O
male	O
otherwise	O
female	O
more	O
generally	O
we	O
may	O
use	O
this	O
probability	O
as	O
part	O
of	O
a	O
decision	O
process	O
as	O
in	O
equation	B
advantages	O
prior	B
information	O
about	O
the	O
structure	B
of	O
the	O
data	O
is	O
often	O
most	O
naturally	O
specified	O
through	O
a	O
generative	B
model	B
pxc	O
for	O
example	O
for	O
male	O
faces	B
we	O
would	O
expect	O
to	O
see	O
heavier	O
eyebrows	O
a	O
squarer	O
jaw	O
etc	O
disadvantages	O
the	O
generative	B
approach	B
does	O
not	O
directly	O
target	O
the	O
classification	B
model	B
pcx	O
since	O
the	O
goal	O
of	O
generative	B
training	B
is	O
rather	O
to	O
model	B
pxc	O
if	O
the	O
data	O
x	O
is	O
complex	O
finding	O
a	O
suitable	O
generative	B
data	O
model	B
pxc	O
is	O
a	O
difficult	O
task	O
on	O
the	O
other	O
hand	O
it	O
might	O
be	O
that	O
making	O
a	O
model	B
of	O
pcx	O
is	O
simpler	O
particularly	O
if	O
the	O
decision	B
boundary	B
between	O
the	O
classes	O
has	O
a	O
simple	O
form	O
even	O
if	O
the	O
data	O
distribution	B
of	O
each	O
class	O
is	O
complex	O
see	O
furthermore	O
since	O
each	O
generative	B
model	B
is	O
separately	O
trained	O
for	O
each	O
class	O
there	O
is	O
no	O
competition	O
amongst	O
the	O
models	O
to	O
explain	O
the	O
data	O
discriminative	B
approach	B
pc	O
x	O
pcx	O
cxpx	O
x	O
assuming	O
i	O
i	O
d	O
data	O
the	O
log	O
likelihood	B
is	O
log	O
pd	O
log	O
pcnxn	O
cx	O
n	O
n	O
n	O
log	O
pxn	O
x	O
the	O
parameters	O
are	O
isolated	O
in	O
the	O
two	O
terms	O
so	O
that	O
maximum	B
likelihood	B
training	B
is	O
equivalent	B
to	O
finding	O
the	O
parameters	O
of	O
cx	O
that	O
will	O
best	O
predict	O
the	O
class	O
c	O
for	O
a	O
given	O
training	B
input	O
x	O
the	O
parameters	O
x	O
for	O
modelling	B
the	O
data	O
occur	O
separately	O
in	O
the	O
second	O
term	O
above	O
and	O
setting	O
them	O
can	O
therefore	O
be	O
treated	O
as	O
a	O
separate	O
unsupervised	B
learning	B
problem	B
this	O
approach	B
therefore	O
isolates	O
modelling	B
the	O
decision	B
boundary	B
from	O
modelling	B
the	O
input	O
distribution	B
see	O
classification	B
of	O
a	O
new	O
point	O
x	O
is	O
based	O
on	O
pcx	O
opt	O
cx	O
as	O
for	O
the	O
generative	B
case	O
this	O
approach	B
still	O
learns	O
a	O
joint	B
distribution	B
pc	O
x	O
pcxpx	O
which	O
can	O
be	O
used	O
as	O
part	O
of	O
a	O
decision	O
process	O
if	O
required	O
advantages	O
the	O
discriminative	B
approach	B
directly	O
addresses	O
making	O
an	O
accurate	O
classifier	B
based	O
on	O
pcx	O
modelling	B
the	O
decision	B
boundary	B
as	O
opposed	O
to	O
the	O
class	O
conditional	B
data	O
distribution	B
in	O
the	O
generative	B
approach	B
whilst	O
the	O
data	O
from	O
each	O
class	O
may	O
be	O
distributed	O
in	O
a	O
complex	O
way	O
it	O
could	O
be	O
that	O
the	O
decision	B
boundary	B
between	O
them	O
is	O
relatively	O
easy	O
to	O
model	B
draft	O
march	O
supervised	B
learning	B
figure	O
each	O
point	O
represents	O
a	O
high	O
dimensional	O
vector	O
with	O
an	O
associated	O
class	O
label	O
either	O
male	O
or	O
female	O
the	O
point	O
x	O
is	O
a	O
new	O
point	O
for	O
which	O
we	O
would	O
like	O
to	O
predict	O
whether	O
this	O
should	O
be	O
male	O
or	O
female	O
in	O
the	O
generative	B
approach	B
a	O
male	O
model	B
pxmale	O
generates	O
data	O
similar	O
to	O
the	O
m	O
points	O
similarly	O
the	O
female	O
model	B
pxfemale	O
generates	O
points	O
that	O
are	O
similar	O
to	O
the	O
f	O
points	O
above	O
we	O
then	O
use	O
bayes	O
rule	O
to	O
calculate	O
the	O
probability	O
pmalex	O
using	O
the	O
two	O
we	O
directly	O
make	O
a	O
model	B
of	O
pmalex	O
which	O
cares	O
less	O
about	O
how	O
the	O
points	O
m	O
or	O
f	O
are	O
distributed	O
but	O
more	O
about	O
describing	O
the	O
boundary	B
which	O
can	O
separate	O
the	O
two	O
classes	O
as	O
given	O
by	O
the	O
line	O
fitted	O
models	O
as	O
given	O
in	O
the	O
text	O
in	O
the	O
discriminative	B
approach	B
ch	O
h	O
xh	O
cn	O
hn	O
xn	O
n	O
figure	O
a	O
strategy	O
for	O
semi-supervised	B
learning	B
when	O
cn	O
is	O
missing	B
the	O
term	O
pcnhn	O
is	O
absent	O
the	O
large	O
amount	O
of	O
training	B
data	O
helps	O
the	O
model	B
learn	O
a	O
good	O
lower	O
dimensioncompressed	O
representation	O
h	O
of	O
the	O
data	O
x	O
fitting	O
then	O
a	O
classification	B
model	B
pch	O
using	O
this	O
lower	O
dimensional	O
representation	O
may	O
be	O
much	O
easier	O
than	O
fitting	O
a	O
model	B
directly	O
from	O
the	O
complex	O
data	O
to	O
the	O
class	O
pcx	O
disadvantages	O
discriminative	B
approaches	O
are	O
usually	O
trained	O
as	O
black-box	B
classifiers	O
with	O
little	O
prior	B
knowledge	O
built	O
in	O
as	O
to	O
how	O
data	O
for	O
a	O
given	O
class	O
might	O
look	O
domain	B
knowledge	O
is	O
often	O
more	O
easily	O
expressed	O
using	O
the	O
generative	B
framework	O
hybrid	O
generative-discriminative	B
approaches	O
one	O
could	O
use	O
a	O
generative	B
description	O
pxc	O
building	O
in	O
prior	B
information	O
and	O
use	O
this	O
to	O
form	O
a	O
joint	B
distribution	B
px	O
c	O
from	O
which	O
a	O
discriminative	B
model	B
pcx	O
may	O
be	O
formed	O
using	O
bayes	O
rule	O
specifically	O
we	O
can	O
use	O
pxc	O
xcpc	O
c	O
c	O
pxc	O
xcpc	O
c	O
pcx	O
and	O
use	O
a	O
separate	O
model	B
for	O
px	O
x	O
subsequently	O
the	O
parameters	O
xc	O
c	O
for	O
this	O
hybrid	O
model	B
can	O
be	O
found	O
by	O
maximising	O
the	O
probability	O
of	O
being	O
in	O
the	O
correct	O
class	O
this	O
approach	B
would	O
appear	O
to	O
leverage	O
the	O
advantages	O
of	O
both	O
the	O
discriminative	B
and	O
generative	B
frameworks	O
since	O
we	O
can	O
more	O
readily	O
incorporate	O
domain	B
knowledge	O
in	O
the	O
generative	B
model	B
pxc	O
xc	O
yet	O
train	O
this	O
in	O
a	O
discriminative	B
way	O
this	O
approach	B
is	O
rarely	O
taken	O
in	O
practice	O
since	O
the	O
resulting	O
functional	O
form	O
of	O
the	O
likelihood	B
depends	O
in	O
a	O
complex	O
manner	O
on	O
the	O
parameters	O
in	O
this	O
case	O
no	O
separation	B
occurs	O
was	O
previously	O
the	O
case	O
for	O
the	O
generative	B
and	O
discriminative	B
approaches	O
learning	B
lower-dimensional	O
representations	O
in	O
semi-supervised	B
learning	B
one	O
way	O
to	O
exploit	O
a	O
large	O
amount	O
of	O
unlabelled	B
training	B
data	O
to	O
improve	O
classification	B
modelling	B
is	O
to	O
try	O
to	O
find	O
a	O
lower	O
dimensional	O
representation	O
h	O
of	O
the	O
data	O
x	O
based	O
on	O
this	O
the	O
mapping	O
from	O
h	O
to	O
c	O
may	O
be	O
rather	O
simpler	O
to	O
learn	O
than	O
a	O
mapping	O
from	O
x	O
to	O
c	O
directly	O
to	O
do	O
so	O
we	O
can	O
form	O
the	O
likelihood	B
using	O
see	O
pcx	O
n	O
opt	O
argmax	O
pcx	O
h	O
draft	O
march	O
and	O
then	O
set	O
any	O
parameters	O
for	O
example	O
by	O
using	O
maximum	B
likelihood	B
pxnhn	O
xhph	O
h	O
bayes	O
versus	O
empirical	B
decisions	O
features	O
and	O
preprocessing	O
it	O
is	O
often	O
the	O
case	O
that	O
when	O
attempting	O
to	O
make	O
a	O
predictive	O
model	B
transforming	O
the	O
raw	O
input	O
x	O
into	O
a	O
form	O
that	O
more	O
directly	O
captures	O
the	O
relevant	O
label	O
information	O
can	O
greatly	O
improve	O
performance	B
for	O
example	O
in	O
the	O
male-female	O
classification	B
case	O
it	O
might	O
be	O
that	O
building	O
a	O
classifier	B
directly	O
in	O
terms	O
of	O
the	O
elements	O
of	O
the	O
face	O
vector	O
x	O
is	O
difficult	O
however	O
using	O
features	O
which	O
contain	O
geometric	O
information	O
such	O
as	O
the	O
distance	O
between	O
eyes	O
width	O
of	O
mouth	O
etc	O
may	O
make	O
finding	O
a	O
classifier	B
easier	O
in	O
practice	O
data	O
is	O
often	O
preprocessed	O
to	O
remove	O
noise	O
centre	O
an	O
image	O
etc	O
bayes	O
versus	O
empirical	B
decisions	O
the	O
empirical	B
risk	B
and	O
bayesian	B
approaches	O
are	O
at	O
the	O
extremes	O
of	O
the	O
philosophical	O
spectrum	B
in	O
the	O
empirical	B
risk	B
approach	B
one	O
makes	O
a	O
seemingly	O
over-simplistic	O
data	O
generating	O
assumption	O
however	O
decision	B
function	B
parameters	O
are	O
set	O
based	O
on	O
the	O
task	O
of	O
making	O
decisions	O
on	O
the	O
other	O
hand	O
the	O
bayesian	B
approach	B
attempts	O
to	O
learn	O
pc	O
x	O
without	O
regard	O
to	O
its	O
ultimate	O
use	O
as	O
part	O
of	O
a	O
larger	O
decision	O
process	O
what	O
objective	O
criterion	O
can	O
we	O
use	O
to	O
learn	O
pc	O
x	O
particularly	O
if	O
we	O
are	O
only	O
interested	O
in	O
classification	B
with	O
a	O
low	O
test-risk	O
the	O
following	O
example	O
is	O
intended	O
to	O
recapitulate	O
the	O
two	O
generic	O
bayes	O
and	O
empirical	B
risk	B
approaches	O
we	O
ve	O
been	O
considering	O
example	O
two	O
generic	O
decision	O
strategies	O
consider	O
a	O
situation	O
in	O
which	O
based	O
on	O
patient	O
information	O
x	O
we	O
need	O
to	O
take	O
a	O
decision	O
d	O
as	O
whether	O
or	O
not	O
to	O
operate	O
the	O
utility	B
of	O
operating	O
ud	O
c	O
depends	O
on	O
whether	O
or	O
not	O
the	O
patient	O
has	O
cancer	O
for	O
example	O
uoperate	O
cancer	O
udon	O
t	O
operate	O
cancer	O
udon	O
t	O
operate	O
benign	O
uoperate	O
benign	O
we	O
have	O
independent	O
true	O
assessments	O
of	O
whether	O
or	O
not	O
a	O
patient	O
had	O
cancer	O
giving	O
rise	O
to	O
a	O
set	O
of	O
historical	O
records	O
d	O
cn	O
n	O
n	O
faced	O
with	O
a	O
new	O
patient	O
with	O
information	O
x	O
we	O
need	O
to	O
make	O
a	O
decision	O
whether	O
or	O
not	O
to	O
operate	O
in	O
the	O
bayesian	B
decision	O
approach	B
one	O
would	O
first	O
make	O
a	O
model	B
pcxd	O
example	O
logistic	B
regression	B
using	O
this	O
model	B
the	O
decision	O
is	O
given	O
by	O
that	O
which	O
maximises	O
the	O
expected	O
utility	B
d	O
argmax	O
d	O
cancer	O
pbenignxdud	O
benign	O
in	O
this	O
approach	B
learning	B
the	O
model	B
pcxd	O
is	O
divorced	O
from	O
the	O
ultimate	O
use	O
of	O
the	O
model	B
in	O
the	O
decision	O
making	O
process	O
an	O
advantage	O
of	O
this	O
approach	B
is	O
that	O
from	O
the	O
viewpoint	O
of	O
expected	O
utility	B
it	O
is	O
optimal	O
provided	O
the	O
model	B
pcxd	O
is	O
correct	O
unfortunately	O
this	O
is	O
rarely	O
the	O
case	O
given	O
the	O
limited	O
model	B
resources	O
it	O
might	O
make	O
sense	O
to	O
focus	O
on	O
ensuring	O
the	O
prediction	O
of	O
cancer	O
is	O
correct	O
since	O
this	O
has	O
a	O
more	O
significant	O
effect	O
on	O
the	O
utility	B
however	O
formally	O
this	O
is	O
not	O
possible	O
in	O
this	O
framework	O
the	O
alternative	O
empirical	B
utility	B
approach	B
recognises	O
that	O
the	O
task	O
can	O
be	O
stated	O
as	O
to	O
translate	O
patient	O
information	O
x	O
into	O
an	O
operation	O
decision	O
d	O
to	O
do	O
so	O
one	O
could	O
parameterise	O
this	O
as	O
dx	O
fx	O
and	O
then	O
learn	O
under	O
maximising	O
the	O
empirical	B
utility	B
ufxn	O
cn	O
for	O
example	O
if	O
x	O
is	O
a	O
vector	O
representing	O
the	O
patient	O
information	O
and	O
the	O
parameter	B
we	O
might	O
use	O
a	O
linear	B
decision	B
function	B
such	O
as	O
tx	O
d	O
operate	O
tx	O
d	O
don	O
t	O
operate	O
u	O
n	O
fx	O
the	O
advantage	O
of	O
this	O
approach	B
is	O
that	O
the	O
parameters	O
of	O
the	O
decision	O
are	O
directly	O
related	O
to	O
the	O
utility	B
of	O
making	O
the	O
decision	O
a	O
disadvantage	O
is	O
that	O
we	O
cannot	O
easily	O
incorporate	O
domain	B
knowledge	O
into	O
the	O
decision	B
function	B
it	O
may	O
be	O
that	O
we	O
have	O
a	O
good	O
model	B
of	O
pcx	O
and	O
would	O
wish	O
to	O
make	O
use	O
of	O
this	O
draft	O
march	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
both	O
approaches	O
are	O
heavily	O
used	O
in	O
practice	O
and	O
which	O
is	O
to	O
be	O
preferred	O
depends	O
very	O
much	O
on	O
the	O
problem	B
whilst	O
the	O
bayesian	B
approach	B
appears	O
formally	O
optimal	O
it	O
is	O
prone	O
to	O
model	B
mis-specification	O
a	O
pragmatic	O
alternative	O
bayesian	B
approach	B
is	O
to	O
fit	O
a	O
parameterised	O
distribution	B
pc	O
x	O
to	O
the	O
data	O
d	O
where	O
penalises	O
complexity	O
of	O
the	O
fitted	O
distribution	B
setting	O
using	O
validation	B
on	O
the	O
risk	B
this	O
has	O
the	O
potential	B
advantage	O
of	O
allowing	O
one	O
to	O
incorporate	O
sensible	O
prior	B
information	O
about	O
pc	O
x	O
whilst	O
assessing	O
competing	O
models	O
in	O
the	O
light	O
of	O
their	O
actual	O
predictive	O
risk	B
similarly	O
for	O
the	O
empirical	B
risk	B
approach	B
one	O
can	O
modify	O
the	O
extreme	O
empirical	B
distribution	B
assumption	O
by	O
using	O
a	O
more	O
plausible	O
model	B
px	O
c	O
of	O
the	O
data	O
representing	O
data	O
the	O
numeric	O
encoding	O
of	O
data	O
can	O
have	O
a	O
significant	O
effect	O
on	O
performance	B
and	O
an	O
understanding	O
of	O
the	O
options	O
for	O
representing	O
data	O
is	O
therefore	O
of	O
considerable	O
importance	B
categorical	B
for	O
categorical	B
nominal	O
data	O
the	O
observed	O
value	B
belongs	O
to	O
one	O
of	O
a	O
number	O
of	O
classes	O
with	O
no	O
intrinsic	O
ordering	O
of	O
the	O
classes	O
an	O
example	O
of	O
a	O
categorical	B
variable	O
would	O
be	O
the	O
description	O
of	O
the	O
type	O
of	O
job	O
that	O
someone	O
does	O
e	O
g	O
healthcare	O
education	O
financial	O
services	O
transport	O
homeworker	O
unemployed	O
engineering	O
etc	O
one	O
way	O
to	O
transform	O
this	O
data	O
into	O
numerical	B
values	O
would	O
be	O
to	O
use	O
encoding	O
here	O
s	O
an	O
example	O
there	O
are	O
kinds	O
of	O
jobs	O
soldier	O
sailor	O
tinker	O
spy	O
a	O
soldier	O
is	O
represented	O
as	O
a	O
sailer	O
as	O
a	O
tinker	O
as	O
and	O
a	O
spy	O
as	O
in	O
this	O
encoding	O
the	O
distance	O
between	O
the	O
vectors	O
representing	O
two	O
different	O
professions	O
is	O
constant	O
it	O
is	O
clear	O
that	O
encoding	O
induces	O
dependencies	O
in	O
the	O
profession	O
attributes	O
since	O
if	O
one	O
of	O
the	O
profession	O
attributes	O
is	O
the	O
others	O
must	O
be	O
zero	O
ordinal	B
an	O
ordinal	B
variable	O
consists	O
of	O
categories	O
with	O
an	O
ordering	O
or	O
ranking	O
of	O
the	O
categories	O
e	O
g	O
cold	O
cool	O
warm	O
hot	O
in	O
this	O
case	O
to	O
preserve	O
the	O
ordering	O
we	O
could	O
perhaps	O
use	O
for	O
cold	O
for	O
cool	O
for	O
warm	O
and	O
for	O
hot	O
this	O
choice	O
is	O
somewhat	O
arbitrary	O
and	O
one	O
should	O
bear	O
in	O
mind	O
that	O
results	O
will	O
generally	O
be	O
dependent	O
on	O
the	O
numerical	B
coding	O
used	O
numerical	B
numerical	B
data	O
takes	O
on	O
values	O
that	O
are	O
real	O
numbers	O
e	O
g	O
a	O
temperature	O
measured	O
by	O
a	O
thermometer	O
or	O
the	O
salary	O
that	O
someone	O
earns	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
how	O
can	O
we	O
assess	O
whether	O
two	O
classifiers	O
are	O
performing	O
differently	O
for	O
techniques	O
which	O
are	O
based	O
on	O
bayesian	B
classifiers	O
pc	O
m	O
there	O
will	O
always	O
be	O
in	O
principle	O
a	O
direct	O
way	O
to	O
estimate	O
the	O
suitability	O
of	O
the	O
model	B
m	O
by	O
computing	O
pmd	O
we	O
consider	O
here	O
the	O
less	O
fortunate	O
situation	O
where	O
the	O
only	O
information	O
presumed	O
available	O
is	O
the	O
test	O
performance	B
of	O
the	O
two	O
classifiers	O
to	O
outline	O
the	O
basic	O
issue	O
let	O
s	O
consider	O
two	O
classifiers	O
a	O
and	O
b	O
which	O
predict	O
the	O
class	O
of	O
test	O
examples	O
classifier	B
a	O
makes	O
errors	O
and	O
correct	O
classifications	O
whereas	O
classifier	B
b	O
makes	O
errors	O
and	O
correct	O
classifications	O
is	O
classifier	B
a	O
better	O
than	O
classifier	B
b	O
our	O
lack	O
of	O
confidence	O
in	O
pronouncing	O
that	O
a	O
is	O
better	O
than	O
b	O
results	O
from	O
the	O
small	O
number	O
of	O
test	O
examples	O
on	O
the	O
other	O
hand	O
if	O
classifier	B
a	O
makes	O
errors	O
and	O
correct	O
classifications	O
whilst	O
classifier	B
b	O
makes	O
errors	O
and	O
correct	O
classifications	O
intuitively	O
we	O
would	O
be	O
more	O
confident	O
that	O
classifier	B
a	O
is	O
better	O
than	O
classifier	B
b	O
perhaps	O
the	O
most	O
practically	O
relevant	O
question	O
from	O
a	O
machine	O
learning	B
perspective	O
is	O
the	O
probability	O
that	O
classifier	B
a	O
outperforms	O
classifier	B
b	O
given	O
the	O
available	O
test	O
information	O
whilst	O
this	O
question	O
can	O
draft	O
march	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
be	O
addressed	O
using	O
a	O
bayesian	B
procedure	O
we	O
first	O
focus	O
on	O
a	O
simpler	O
question	O
namely	O
whether	O
classifier	B
a	O
and	O
b	O
are	O
the	O
outcome	B
analysis	B
the	O
treatment	O
in	O
this	O
section	O
refers	O
to	O
outcomes	O
and	O
quantifies	O
if	O
data	O
is	O
likely	O
to	O
come	O
from	O
the	O
same	O
multinomial	B
distribution	B
in	O
the	O
main	O
we	O
will	O
apply	O
this	O
to	O
assessing	O
if	O
two	O
classifiers	O
are	O
essentially	O
performing	O
the	O
same	O
although	O
one	O
should	O
bear	O
in	O
mind	O
that	O
the	O
method	O
applies	O
more	O
generally	O
to	O
assessing	O
if	O
outcomes	O
are	O
likely	O
to	O
have	O
been	O
generated	O
from	O
the	O
same	O
or	O
different	O
underlying	O
processes	O
consider	O
a	O
situation	O
where	O
two	O
classifiers	O
a	O
and	O
b	O
have	O
been	O
tested	O
on	O
some	O
data	O
so	O
that	O
we	O
have	O
for	O
each	O
example	O
in	O
the	O
test	B
set	I
an	O
outcome	O
pair	O
obn	O
n	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
test	O
data	O
points	O
and	O
oa	O
q	O
similarly	O
for	O
ob	O
that	O
is	O
there	O
are	O
q	O
possible	O
types	O
of	O
outcomes	O
that	O
can	O
occur	O
for	O
example	O
for	O
binary	O
classification	B
we	O
will	O
typically	O
have	O
the	O
four	O
cases	O
domo	O
falsepositive	O
truenegative	O
falsenegative	O
if	O
the	O
classifier	B
predicts	O
class	O
c	O
false	O
and	O
the	O
truth	O
is	O
class	O
t	O
false	O
these	O
are	O
defined	O
as	O
truepositive	O
falsepositive	O
truenegative	O
falsenegative	O
c	O
true	O
c	O
true	O
c	O
false	O
c	O
false	O
t	O
true	O
t	O
false	O
t	O
false	O
t	O
true	O
we	O
call	O
oa	O
n	O
n	O
the	O
outcomes	O
for	O
classifier	B
a	O
and	O
similarly	O
for	O
ob	O
n	O
n	O
for	O
classifier	B
b	O
to	O
be	O
specific	O
we	O
have	O
two	O
hypotheses	O
we	O
wish	O
to	O
test	O
hdiff	O
oa	O
and	O
ob	O
are	O
from	O
different	O
categorical	B
distributions	O
hsame	O
oa	O
and	O
ob	O
are	O
from	O
the	O
same	O
categorical	B
distribution	B
in	O
both	O
cases	O
we	O
will	O
use	O
categorical	B
models	O
poc	O
q	O
h	O
c	O
q	O
with	O
unknown	O
parameters	O
will	O
correspond	O
to	O
using	O
the	O
same	O
parameters	O
a	O
b	O
for	O
both	O
classifiers	O
and	O
hypothesis	O
to	O
using	O
different	O
parameters	O
as	O
we	O
will	O
discuss	O
below	O
in	O
the	O
bayesian	B
framework	O
we	O
want	O
to	O
find	O
how	O
likely	O
it	O
is	O
that	O
a	O
modelhypothesis	O
is	O
responsible	O
for	O
generating	O
the	O
data	O
for	O
any	O
hypothesis	O
h	O
calculate	O
phoa	O
ob	O
poa	O
obhph	O
poa	O
ob	O
where	O
ph	O
is	O
our	O
prior	B
belief	O
that	O
h	O
is	O
the	O
correct	O
hypothesis	O
note	O
that	O
the	O
normalising	O
constant	O
poa	O
ob	O
does	O
not	O
depend	O
on	O
the	O
hypothesis	O
under	O
all	O
hypotheses	O
we	O
will	O
make	O
the	O
independence	B
of	O
trials	O
assumption	O
poa	O
obh	O
poan	O
obnh	O
to	O
make	O
further	O
progress	O
we	O
need	O
to	O
state	O
what	O
the	O
specific	O
hypotheses	O
mean	B
draft	O
march	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
oa	O
ob	O
oa	O
ob	O
p	O
oa	O
ob	O
figure	O
hdiff	O
corresponds	O
to	O
the	O
outcomes	O
for	O
the	O
two	O
classifiers	O
being	O
independently	O
genb	O
hsame	O
both	O
outcomes	O
are	O
generated	O
erated	O
hdep	O
the	O
outfrom	O
the	O
same	O
distribution	B
comes	O
are	O
dependent	O
correlated	O
hdiff	O
model	B
likelihood	B
we	O
now	O
use	O
the	O
above	O
assumptions	O
to	O
compute	O
the	O
hypothesis	O
likelihood	B
phdiffoa	O
ob	O
poa	O
obhdiffphdiff	O
poa	O
ob	O
the	O
outcome	O
model	B
for	O
classifier	B
a	O
is	O
specified	O
using	O
parameters	O
giving	O
poa	O
hdiff	O
and	O
similarly	O
we	O
use	O
for	O
classifier	B
b	O
the	O
finite	O
amount	O
of	O
data	O
means	O
that	O
we	O
are	O
uncertain	B
as	O
to	O
these	O
parameter	B
values	O
and	O
therefore	O
the	O
joint	B
term	O
in	O
the	O
numerator	O
above	O
is	O
poa	O
obphdiffoa	O
ob	O
poa	O
ob	O
hdiffp	O
d	O
phdiff	O
where	O
we	O
assumed	O
poa	O
hdiffp	O
pob	O
hdiffp	O
p	O
p	O
and	O
poa	O
ob	O
hdiff	O
poa	O
hdiffpob	O
hdiff	O
note	O
that	O
one	O
might	O
expect	O
there	O
to	O
be	O
a	O
specific	O
constraint	O
that	O
the	O
two	O
models	O
a	O
and	O
b	O
are	O
different	O
however	O
since	O
the	O
models	O
are	O
assumed	O
independent	O
and	O
each	O
has	O
parameters	O
sampled	O
from	O
an	O
effectively	O
infinite	O
set	O
and	O
are	O
continuous	B
the	O
probability	O
that	O
sampled	O
parameters	O
and	O
of	O
the	O
two	O
models	O
are	O
the	O
same	O
is	O
zero	O
since	O
we	O
are	O
dealing	O
with	O
categorical	B
distributions	O
it	O
is	O
convenient	O
to	O
use	O
the	O
dirichlet	B
prior	B
which	O
is	O
conjugate	B
to	O
the	O
categorical	B
distribution	B
zu	O
q	O
p	O
uq	O
q	O
zu	O
uq	O
the	O
prior	B
hyperparameter	B
u	O
controls	O
how	O
strongly	O
the	O
mass	O
of	O
the	O
distribution	B
is	O
pushed	O
to	O
the	O
corners	O
of	O
the	O
simplex	O
see	O
setting	O
uq	O
for	O
all	O
q	O
corresponds	O
to	O
a	O
uniform	B
prior	B
the	O
likelihood	B
of	O
oa	O
is	O
given	O
poa	O
hdiffp	O
q	O
q	O
zu	O
q	O
q	O
uq	O
q	O
d	O
zu	O
zu	O
where	O
is	O
a	O
vector	O
with	O
components	O
hence	O
q	O
being	O
the	O
number	O
of	O
times	O
that	O
variable	O
a	O
is	O
is	O
state	O
q	O
in	O
the	O
data	O
phdiffoa	O
ob	O
phdiff	O
zu	O
zu	O
zu	O
zu	O
where	O
zu	O
is	O
given	O
by	O
equation	B
hsame	O
model	B
likelihood	B
in	O
hsame	O
the	O
hypothesis	O
is	O
that	O
the	O
outcomes	O
for	O
the	O
two	O
classifiers	O
are	O
generated	O
from	O
the	O
same	O
categorical	B
distribution	B
hence	O
poa	O
obphsameoa	O
ob	O
phsame	O
poa	O
hsamepob	O
hsamep	O
phsame	O
zu	O
zu	O
draft	O
march	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
bayes	O
factor	B
if	O
we	O
assume	O
that	O
we	O
have	O
no	O
prior	B
preference	O
for	O
either	O
hypothesis	O
phsame	O
then	O
phdiffoa	O
ob	O
phsameoa	O
ob	O
zu	O
zuzu	O
this	O
is	O
the	O
evidence	O
to	O
suggest	O
that	O
the	O
data	O
were	O
generated	O
by	O
two	O
different	O
categorical	B
distributions	O
example	O
two	O
people	O
classify	O
the	O
expression	O
of	O
each	O
image	O
into	O
happy	O
sad	O
or	O
normal	B
using	O
states	O
respectively	O
each	O
column	O
of	O
the	O
data	O
below	O
represents	O
an	O
image	O
classed	O
by	O
the	O
two	O
people	O
is	O
the	O
top	O
row	O
and	O
person	O
the	O
second	O
row	O
are	O
the	O
two	O
people	O
essentially	O
in	O
agreement	O
to	O
help	O
answer	O
this	O
question	O
we	O
perform	O
a	O
hdiff	O
versus	O
hsame	O
test	O
from	O
this	O
data	O
the	O
count	O
vector	O
for	O
person	O
is	O
and	O
for	O
person	O
based	O
on	O
a	O
flat	O
prior	B
for	O
the	O
categorical	B
distribution	B
and	O
assuming	O
no	O
prior	B
preference	O
for	O
either	O
hypothesis	O
we	O
have	O
the	O
bayes	O
factor	B
ppersons	O
and	O
classify	O
differently	O
ppersons	O
and	O
classify	O
the	O
same	O
where	O
the	O
z	O
function	B
is	O
given	O
in	O
equation	B
this	O
is	O
strong	B
evidence	O
the	O
two	O
people	O
are	O
classifying	O
the	O
images	O
differently	O
below	O
we	O
discuss	O
some	O
further	O
examples	O
for	O
the	O
hdiff	O
versus	O
hsame	O
test	O
as	O
above	O
the	O
only	O
quantities	O
we	O
need	O
for	O
this	O
test	O
are	O
the	O
vector	O
counts	O
from	O
the	O
data	O
let	O
s	O
assume	O
that	O
there	O
are	O
three	O
kinds	O
of	O
outcomes	O
q	O
for	O
example	O
domo	O
bad	O
ugly	O
are	O
our	O
set	O
of	O
outcomes	O
and	O
we	O
want	O
to	O
test	O
if	O
two	O
classifiers	O
are	O
essentially	O
producing	O
the	O
same	O
outcome	O
distributions	O
or	O
different	O
example	O
versus	O
hsame	O
we	O
have	O
the	O
two	O
outcome	O
counts	O
and	O
then	O
the	O
bayes	O
factor	B
equation	B
is	O
strong	B
evidence	O
in	O
favour	O
of	O
the	O
two	O
classifiers	O
being	O
different	O
alternatively	O
consider	O
the	O
two	O
outcome	O
counts	O
and	O
then	O
the	O
bayes	O
factor	B
equation	B
is	O
weak	O
evidence	O
against	O
the	O
two	O
classifiers	O
being	O
different	O
as	O
a	O
final	O
example	O
consider	O
counts	O
and	O
this	O
gives	O
a	O
bayes	O
factor	B
equation	B
of	O
strong	B
evidence	O
that	O
the	O
two	O
classifiers	O
are	O
statistically	O
the	O
same	O
in	O
all	O
cases	O
the	O
results	O
are	O
consistent	B
with	O
the	O
model	B
in	O
fact	O
used	O
to	O
generate	O
the	O
count	O
data	O
the	O
two	O
outcomes	O
for	O
a	O
and	O
b	O
were	O
indeed	O
from	O
different	O
categorical	B
distributions	O
the	O
more	O
test	O
data	O
we	O
have	O
the	O
more	O
confident	O
we	O
are	O
in	O
our	O
statements	O
dependent	O
outcome	B
analysis	B
here	O
we	O
consider	O
the	O
more	O
common	O
case	O
that	O
outcomes	O
are	O
dependent	O
for	O
example	O
it	O
is	O
often	O
the	O
case	O
that	O
if	O
classifier	B
a	O
works	O
well	O
then	O
classifier	B
b	O
will	O
also	O
work	O
well	O
thus	O
we	O
want	O
to	O
evaluate	O
the	O
draft	O
march	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
hypothesis	O
hdep	O
the	O
outcomes	O
that	O
the	O
two	O
classifiers	O
make	O
are	O
dependent	O
to	O
do	O
so	O
we	O
assume	O
a	O
categorical	B
distribution	B
over	O
the	O
joint	B
states	O
poan	O
obnp	O
hdep	O
poa	O
i	O
ob	O
j	O
here	O
p	O
is	O
a	O
q	O
q	O
matrix	B
of	O
probabilities	O
so	O
is	O
the	O
probability	O
that	O
a	O
makes	O
outcome	O
i	O
and	O
b	O
makes	O
outcome	O
j	O
then	O
pohdep	O
po	O
phdepdp	O
pop	O
hdeppphdepdp	O
where	O
for	O
convenience	O
we	O
write	O
o	O
ob	O
assuming	O
a	O
dirichlet	B
prior	B
on	O
p	O
with	O
hyperparameters	O
u	O
we	O
have	O
pophdepo	O
phdep	O
zvec	O
zvecu	O
where	O
vecd	O
is	O
a	O
vector	O
formed	O
from	O
concatenating	O
the	O
rows	O
of	O
the	O
matrix	B
d	O
here	O
is	O
the	O
count	O
matrix	B
with	O
equal	O
to	O
the	O
number	O
of	O
times	O
that	O
joint	B
outcome	O
i	O
ob	O
j	O
occurred	O
in	O
the	O
n	O
datapoints	O
as	O
before	O
we	O
can	O
then	O
use	O
this	O
in	O
a	O
bayes	O
factor	B
calculation	O
for	O
the	O
uniform	B
prior	B
i	O
j	O
testing	O
for	O
dependencies	O
in	O
the	O
outcomes	O
hdep	O
versus	O
hdiff	O
to	O
test	O
whether	O
or	O
not	O
the	O
outcomes	O
of	O
the	O
classifiers	O
are	O
dependent	O
hdep	O
against	O
the	O
hypothesis	O
that	O
they	O
are	O
independent	O
hdiff	O
we	O
may	O
use	O
assuming	O
phdiff	O
phdep	O
phdiffo	O
phdepo	O
zu	O
zu	O
zu	O
zvecu	O
zu	O
zvec	O
example	O
versus	O
hdiff	O
consider	O
the	O
outcome	O
count	O
matrix	B
so	O
that	O
and	O
then	O
phdiffo	O
phdepo	O
strong	B
evidence	O
that	O
the	O
classifiers	O
perform	O
independently	O
consider	O
the	O
outcome	O
count	O
matrix	B
so	O
that	O
and	O
then	O
phdiffo	O
phdepo	O
strong	B
evidence	O
that	O
the	O
classifiers	O
perform	O
dependently	O
these	O
results	O
are	O
in	O
fact	O
consistent	B
with	O
the	O
way	O
the	O
data	O
was	O
generated	O
in	O
each	O
case	O
draft	O
march	O
bayesian	B
hypothesis	B
testing	I
for	O
outcome	B
analysis	B
testing	O
for	O
dependencies	O
in	O
the	O
outcomes	O
hdep	O
versus	O
hsame	O
in	O
practice	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
dependencies	O
are	O
quite	O
likely	O
in	O
the	O
outcomes	O
that	O
classifiers	O
for	O
example	O
two	O
classifiers	O
will	O
often	O
do	O
well	O
on	O
easy	O
test	O
examples	O
and	O
badly	O
on	O
difficult	O
examples	O
are	O
these	O
dependencies	O
strong	B
enough	O
to	O
make	O
us	O
believe	O
that	O
the	O
outcomes	O
are	O
coming	O
from	O
the	O
same	O
process	O
in	O
this	O
sense	O
we	O
want	O
to	O
test	O
phsameo	O
phdepo	O
zu	O
zu	O
zvecu	O
zvec	O
example	O
versus	O
hsame	O
consider	O
an	O
experiment	O
which	O
gives	O
the	O
test	O
outcome	O
count	O
matrix	B
so	O
that	O
and	O
then	O
phsameo	O
phdepo	O
strong	B
evidence	O
that	O
the	O
classifiers	O
are	O
performing	O
differently	O
consider	O
an	O
experiment	O
which	O
gives	O
the	O
test	O
outcome	O
count	O
matrix	B
so	O
that	O
and	O
then	O
phsameo	O
phdepo	O
strong	B
evidence	O
that	O
the	O
classifiers	O
are	O
performing	O
the	O
same	O
these	O
results	O
are	O
in	O
fact	O
consistent	B
with	O
the	O
way	O
the	O
data	O
was	O
generated	O
is	O
classifier	B
a	O
better	O
than	O
b	O
we	O
return	O
to	O
the	O
question	O
with	O
which	O
we	O
began	O
this	O
outcome	B
analysis	B
given	O
the	O
common	O
scenario	O
of	O
observing	O
a	O
number	O
of	O
errors	O
for	O
classifier	B
a	O
on	O
a	O
test	B
set	I
and	O
a	O
number	O
for	O
b	O
can	O
we	O
say	O
which	O
classifier	B
is	O
better	O
this	O
corresponds	O
to	O
the	O
special	O
case	O
of	O
binary	O
classes	O
q	O
with	O
dome	O
incorrect	O
under	O
the	O
hdiff	O
for	O
this	O
special	O
case	O
it	O
makes	O
sense	O
to	O
use	O
a	O
beta	B
distribution	B
corresponds	O
to	O
the	O
dirichlet	B
when	O
q	O
then	O
for	O
a	O
being	O
the	O
probability	O
that	O
classifier	B
a	O
generates	O
a	O
correct	O
label	O
we	O
have	O
poa	O
a	O
similarly	O
correct	O
a	O
incorrect	O
pob	O
b	O
correct	O
b	O
incorrect	O
we	O
assume	O
independent	O
identical	O
beta	B
distribution	B
priors	O
p	O
a	O
b	O
p	O
b	O
b	O
draft	O
march	O
code	O
figure	O
two	O
classifiers	O
a	O
and	O
b	O
and	O
their	O
posterior	B
distributions	O
of	O
the	O
probability	O
that	O
they	O
for	O
a	O
with	O
correct	O
and	O
incorrect	O
labels	O
classify	O
correctly	O
a	O
uniform	B
beta	B
prior	B
b	O
curve	O
b	O
with	O
correct	O
incorrect	O
b	O
curve	O
px	O
y	O
for	O
a	O
with	O
correct	O
and	O
incorrect	O
labels	O
curve	O
b	O
b	O
with	O
correct	O
incorrect	O
b	O
curve	O
px	O
y	O
as	O
the	O
amount	O
of	O
data	O
increases	O
the	O
overlap	O
between	O
the	O
distributions	O
decreases	O
and	O
the	O
certainty	O
that	O
one	O
classifier	B
is	O
better	O
than	O
the	O
other	O
correspondingly	O
increases	O
where	O
a	O
flat	O
prior	B
corresponds	O
to	O
using	O
the	O
hyperparameter	B
setting	O
the	O
posterior	B
distributions	O
for	O
a	O
and	O
b	O
are	O
independent	O
p	O
aoa	O
b	O
correct	O
correct	O
incorrect	O
the	O
question	O
of	O
whether	O
a	O
is	O
better	O
than	O
b	O
can	O
then	O
be	O
addressed	O
by	O
computing	O
incorrect	O
p	O
bob	O
b	O
xa	O
xb	O
ba	O
bbc	O
d	O
yc	O
yd	O
dydx	O
x	O
p	O
a	O
boa	O
ob	O
where	O
a	O
correct	O
b	O
incorrect	O
c	O
correct	O
d	O
incorrect	O
example	O
classifier	B
a	O
makes	O
errors	O
and	O
correct	O
classifications	O
whereas	O
classifier	B
b	O
makes	O
errors	O
and	O
correct	O
classifications	O
using	O
a	O
flat	O
prior	B
this	O
gives	O
p	O
a	O
boa	O
ob	O
on	O
the	O
other	O
hand	O
if	O
classifier	B
a	O
makes	O
errors	O
and	O
correct	O
classifications	O
whilst	O
classifier	B
b	O
makes	O
errors	O
and	O
correct	O
classifications	O
we	O
have	O
p	O
a	O
boa	O
ob	O
this	O
demonstrates	O
the	O
intuitive	O
effect	O
that	O
even	O
though	O
the	O
proportion	O
of	O
correctincorrect	O
classifications	O
doesn	O
t	O
change	O
for	O
the	O
two	O
scenarios	O
as	O
we	O
have	O
more	O
data	O
our	O
confidence	O
in	O
determining	O
the	O
better	O
classifier	B
increases	O
code	O
demobayeserroranalysis	O
m	O
demo	O
for	O
bayesian	B
error	B
analysis	B
betaxbiggery	O
m	O
px	O
y	O
for	O
x	O
b	O
b	O
y	O
b	O
d	O
draft	O
march	O
notes	O
a	O
general	O
introduction	O
to	O
machine	O
learning	B
is	O
given	O
in	O
an	O
excellent	O
reference	O
for	O
bayesian	B
decision	B
theory	I
is	O
approaches	O
based	O
on	O
empirical	B
risk	B
are	O
discussed	O
in	O
exercises	O
with	O
exercises	O
exercise	O
given	O
the	O
distributions	O
n	O
corresponding	O
prior	B
occurrence	O
of	O
classes	O
and	O
calculate	O
the	O
decision	B
boundary	B
explicitly	O
as	O
a	O
function	B
of	O
how	O
many	O
solutions	O
are	O
there	O
to	O
the	O
decision	B
boundary	B
and	O
are	O
they	O
all	O
reasonable	O
exercise	O
suppose	O
that	O
instead	O
of	O
using	O
the	O
bayes	O
decision	O
rule	O
to	O
choose	O
class	O
k	O
if	O
pckx	O
pcjx	O
for	O
all	O
j	O
k	O
we	O
use	O
a	O
randomized	O
decision	O
rule	O
choosing	O
class	O
j	O
with	O
probability	O
qcjx	O
calculate	O
the	O
error	O
for	O
this	O
decision	O
rule	O
and	O
show	O
that	O
the	O
error	O
is	O
minimized	O
by	O
using	O
bayes	O
decision	O
rule	O
and	O
n	O
and	O
class	O
has	O
the	O
distribution	B
pxc	O
n	O
exercise	O
consider	O
datapoints	O
generated	O
from	O
two	O
different	O
classes	O
class	O
has	O
the	O
distribution	B
pxc	O
n	O
probabilities	O
of	O
each	O
class	O
are	O
pc	O
pc	O
show	O
that	O
the	O
posterior	B
probability	O
pc	O
is	O
of	O
the	O
form	O
the	O
prior	B
pc	O
exp	O
b	O
and	O
determine	O
a	O
and	O
b	O
in	O
terms	O
of	O
and	O
exercise	O
wowco	O
com	O
is	O
a	O
new	O
startup	O
prediction	O
company	O
after	O
years	O
of	O
failures	O
they	O
eventually	O
find	O
a	O
neural	B
network	I
with	O
a	O
trillion	O
hidden	B
units	O
that	O
achieves	O
zero	O
test	O
error	O
on	O
every	O
learning	B
problem	B
posted	O
on	O
the	O
internet	O
up	O
last	O
week	O
each	O
learning	B
problem	B
included	O
a	O
training	B
and	O
test	B
set	I
proud	O
of	O
their	O
achievement	O
they	O
market	O
their	O
product	O
aggressively	O
with	O
the	O
claim	O
that	O
it	O
predicts	O
perfectly	O
on	O
all	O
known	O
problems	O
would	O
you	O
buy	O
this	O
product	O
justify	O
your	O
answer	O
exercise	O
three	O
people	O
classify	O
images	O
into	O
of	O
three	O
categories	O
each	O
column	O
in	O
the	O
table	O
below	O
represents	O
the	O
classifications	O
of	O
each	O
image	O
with	O
the	O
top	O
row	O
being	O
the	O
class	O
from	O
person	O
the	O
middle	O
from	O
person	O
and	O
the	O
bottom	O
from	O
person	O
assuming	O
no	O
prior	B
preference	O
amongst	O
hypotheses	O
and	O
a	O
uniform	B
prior	B
on	O
counts	O
compute	O
ppersons	O
and	O
classify	O
differently	O
ppersons	O
and	O
classify	O
the	O
same	O
exercise	O
than	O
random	B
guessing	I
consider	O
a	O
classifier	B
that	O
makes	O
r	O
correct	O
classifications	O
and	O
w	O
wrong	O
classifications	O
is	O
the	O
classifier	B
better	O
than	O
random	B
guessing	I
let	O
d	O
be	O
the	O
fact	O
that	O
there	O
are	O
r	O
right	O
and	O
w	O
wrong	O
answers	O
assume	O
also	O
that	O
the	O
classifications	O
are	O
i	O
i	O
d	O
show	O
that	O
under	O
the	O
hypothesis	O
the	O
data	O
is	O
generated	O
purely	O
at	O
random	O
pdhrandom	O
define	O
to	O
be	O
the	O
probability	O
that	O
the	O
classifier	B
makes	O
an	O
error	O
then	O
pd	O
r	O
draft	O
march	O
exercises	O
then	O
consider	O
pdhnon	O
random	O
pd	O
show	O
that	O
for	O
a	O
beta	B
prior	B
p	O
b	O
b	O
pdhnon	O
random	O
br	O
a	O
w	O
b	O
ba	O
b	O
where	O
ba	O
b	O
is	O
the	O
beta-function	O
considering	O
the	O
random	O
and	O
non-random	O
hypotheses	O
as	O
a	O
priori	O
equally	O
likely	O
show	O
that	O
phrandomd	O
braw	O
bab	O
for	O
a	O
flat	O
prior	B
a	O
b	O
compute	O
the	O
probability	O
that	O
for	O
correct	O
and	O
incorrect	O
classifications	O
the	O
data	O
is	O
from	O
a	O
purely	O
random	O
distribution	B
to	O
equation	B
repeat	O
this	O
for	O
correct	O
and	O
incorrect	O
classifications	O
show	O
that	O
the	O
standard	B
deviation	I
in	O
the	O
number	O
of	O
errors	O
of	O
a	O
random	O
classifier	B
is	O
r	O
w	O
and	O
relate	O
this	O
to	O
the	O
above	O
computation	O
exercise	O
for	O
a	O
prediction	O
model	B
pyx	O
and	O
true	O
data	O
generating	O
distribution	B
px	O
y	O
we	O
define	O
the	O
accuracy	O
as	O
a	O
px	O
y	O
pyx	O
xy	O
you	O
are	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
yn	O
n	O
n	O
by	O
taking	O
qx	O
y	O
to	O
be	O
the	O
empirical	B
this	O
shows	O
that	O
the	O
prediction	O
accuracy	O
is	O
lower	O
bounded	O
by	O
the	O
training	B
accuracy	O
and	O
the	O
gap	O
between	O
the	O
empirical	B
distribution	B
and	O
the	O
unknown	O
true	O
data	O
generating	O
mechanism	O
in	O
theories	O
such	O
as	O
pac	O
bayes	O
one	O
may	O
bound	B
this	O
gap	O
resulting	O
in	O
a	O
bound	B
on	O
the	O
predictive	O
accuracy	O
according	O
to	O
this	O
naive	O
bound	B
the	O
best	O
thing	O
to	O
do	O
to	O
increase	O
the	O
prediction	O
accuracy	O
is	O
to	O
increase	O
the	O
training	B
accuracy	O
the	O
first	O
kullback-leibler	O
term	O
is	O
independent	O
of	O
the	O
predictor	O
as	O
n	O
increases	O
the	O
first	O
term	O
kullback-leibler	O
term	O
becomes	O
small	O
and	O
minimising	O
the	O
training	B
error	O
is	O
justifiable	O
draft	O
march	O
by	O
defining	O
px	O
y	O
pyx	O
a	O
px	O
y	O
and	O
considering	O
klqx	O
y	O
px	O
y	O
show	O
that	O
for	O
any	O
distribution	B
qx	O
y	O
log	O
a	O
klqx	O
ypx	O
y	O
distribution	B
qx	O
y	O
n	O
show	O
that	O
xn	O
yn	O
log	O
a	O
klqx	O
ypx	O
y	O
n	O
log	O
pynxn	O
assuming	O
that	O
the	O
training	B
data	O
are	O
drawn	O
from	O
a	O
distribution	B
pyx	O
which	O
is	O
deterministic	B
show	O
that	O
exercises	O
log	O
a	O
klqxpx	O
n	O
log	O
pynxn	O
and	O
hence	O
that	O
provided	O
the	O
training	B
data	O
is	O
correctly	O
predicted	O
pynxn	O
the	O
accuracy	O
can	O
be	O
related	O
to	O
the	O
empirical	B
input	O
distribution	B
and	O
true	O
input	O
distribution	B
by	O
klqxpx	O
a	O
e	O
draft	O
march	O
chapter	O
nearest	B
neighbour	B
classification	B
do	O
as	O
your	O
neighbour	B
does	O
successful	O
prediction	O
typically	O
relies	O
on	O
smoothness	B
in	O
the	O
data	O
if	O
the	O
class	O
label	O
can	O
change	O
arbitrarily	O
as	O
we	O
move	O
a	O
small	O
amount	O
in	O
the	O
input	O
space	O
the	O
problem	B
is	O
essentially	O
random	O
and	O
no	O
algorithm	B
will	O
generalise	O
well	O
machine	O
learning	B
researchers	O
therefore	O
construct	O
appropriate	O
measures	O
of	O
smoothness	B
for	O
the	O
problem	B
they	O
have	O
at	O
hand	O
nearest	B
neighbour	B
methods	O
are	O
a	O
good	O
starting	O
point	O
since	O
they	O
readily	O
encode	O
basic	O
smoothness	B
intuitions	O
and	O
are	O
easy	O
to	O
program	O
forming	O
a	O
useful	O
baseline	O
method	O
in	O
a	O
classification	B
problem	B
each	O
input	O
vector	O
x	O
has	O
a	O
corresponding	O
class	O
label	O
cn	O
c	O
given	O
a	O
dataset	O
of	O
n	O
such	O
training	B
examples	O
d	O
cn	O
n	O
n	O
and	O
a	O
novel	O
x	O
we	O
aim	O
to	O
return	O
the	O
correct	O
class	O
cx	O
a	O
simple	O
but	O
often	O
effective	O
strategy	O
for	O
this	O
supervised	B
learning	B
problem	B
can	O
be	O
stated	O
as	O
for	O
novel	O
x	O
find	O
the	O
nearest	O
input	O
in	O
the	O
training	B
set	O
and	O
use	O
the	O
class	O
of	O
this	O
nearest	O
input	O
for	O
vectors	O
x	O
and	O
representing	O
two	O
different	O
datapoints	O
we	O
measure	O
nearness	O
using	O
a	O
dissimilarity	B
function	B
dx	O
a	O
common	O
dissimilarity	O
is	O
the	O
squared	B
euclidean	I
distance	I
dx	O
which	O
can	O
be	O
more	O
conveniently	O
written	O
based	O
on	O
the	O
squared	B
euclidean	I
distance	I
the	O
decision	B
boundary	B
is	O
determined	O
by	O
the	O
lines	O
which	O
are	O
the	O
perpendicular	O
bisectors	O
of	O
the	O
closest	O
training	B
points	O
with	O
different	O
training	B
labels	O
see	O
this	O
is	O
called	O
a	O
voronoi	B
tessellation	I
the	O
nearest	B
neighbour	B
algorithm	B
is	O
simple	O
and	O
intuitive	O
there	O
are	O
however	O
some	O
issues	O
how	O
should	O
we	O
measure	O
the	O
distance	O
between	O
points	O
whilst	O
the	O
euclidean	O
square	O
distance	O
is	O
popular	O
this	O
may	O
not	O
always	O
be	O
appropriate	O
a	O
fundamental	O
limitation	O
of	O
the	O
euclidean	O
distance	O
is	O
that	O
it	O
does	O
not	O
take	O
into	O
account	O
how	O
the	O
data	O
is	O
distributed	O
for	O
example	O
if	O
the	O
length	O
scales	O
of	O
x	O
vary	O
greatly	O
the	O
largest	O
length	O
scale	O
will	O
dominate	O
the	O
squared	O
distance	O
with	O
potentially	O
useful	O
class-specific	O
information	O
in	O
other	O
components	O
of	O
x	O
lost	O
the	O
mahalanobis	B
distance	I
dx	O
where	O
is	O
the	O
covariance	B
matrix	B
of	O
the	O
inputs	O
all	O
classes	O
can	O
overcome	O
some	O
of	O
these	O
problems	O
since	O
it	O
rescales	O
all	O
length	O
scales	O
to	O
be	O
essentially	O
equal	O
the	O
whole	O
dataset	O
needs	O
to	O
be	O
stored	O
to	O
make	O
a	O
classification	B
this	O
can	O
be	O
addressed	O
by	O
a	O
method	O
called	O
data	O
editing	O
in	O
which	O
datapoints	O
which	O
have	O
little	O
or	O
no	O
effect	O
on	O
the	O
decision	B
boundary	B
are	O
removed	O
from	O
the	O
training	B
dataset	O
k-nearest	O
neighbours	O
figure	O
in	O
nearest	B
neighbour	B
classification	B
a	O
new	O
vector	O
is	O
assigned	O
the	O
label	O
of	O
the	O
nearest	O
vector	O
in	O
the	O
training	B
set	O
here	O
there	O
are	O
three	O
classes	O
with	O
training	B
points	O
given	O
by	O
the	O
circles	O
along	O
with	O
their	O
class	O
the	O
dots	O
indicate	O
the	O
class	O
of	O
the	O
nearest	O
training	B
vector	O
the	O
decision	B
boundary	B
is	O
piecewise	O
linear	B
with	O
each	O
segment	O
corresponding	O
to	O
the	O
perpendicular	O
bisector	O
between	O
two	O
datapoints	O
belonging	O
to	O
different	O
classes	O
giving	O
rise	O
to	O
a	O
voronoi	B
tessellation	I
of	O
the	O
input	O
space	O
algorithm	B
nearest	B
neighbour	B
algorithm	B
to	O
classify	O
a	O
new	O
vector	O
x	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
cn	O
n	O
n	O
calculate	O
the	O
dissimilarity	O
of	O
the	O
test	O
point	O
x	O
to	O
each	O
of	O
the	O
stored	O
points	O
dn	O
d	O
xn	O
n	O
n	O
find	O
the	O
training	B
point	O
xn	O
which	O
is	O
nearest	O
to	O
x	O
n	O
argmin	O
n	O
d	O
xn	O
assign	O
the	O
class	O
label	O
cx	O
cn	O
in	O
the	O
case	O
that	O
there	O
are	O
two	O
or	O
more	O
equidistant	O
neighbours	O
with	O
different	O
class	O
labels	O
the	O
most	O
numerous	O
if	O
there	O
is	O
no	O
one	O
single	O
most	O
numerous	O
class	O
we	O
use	O
the	O
k-nearest-neighbours	O
case	O
class	O
is	O
chosen	O
described	O
in	O
the	O
next	O
section	O
each	O
distance	O
calculation	O
can	O
be	O
expensive	O
if	O
the	O
datapoints	O
are	O
high	O
dimensional	O
principal	B
components	I
analysis	B
see	O
is	O
one	O
way	O
to	O
address	O
this	O
and	O
replaces	O
xn	O
with	O
a	O
low	B
dimensional	I
projection	B
p	O
the	O
euclidean	O
distance	O
of	O
two	O
is	O
then	O
approximately	O
given	O
by	O
see	O
this	O
is	O
both	O
faster	O
to	O
compute	O
and	O
can	O
also	O
improve	O
classification	B
accuracy	O
since	O
only	O
the	O
large	O
scale	O
characteristics	O
of	O
the	O
data	O
are	O
retained	O
in	O
the	O
pca	B
projections	O
it	O
is	O
not	O
clear	O
how	O
to	O
deal	O
with	O
missing	B
data	I
or	O
incorporate	O
prior	B
beliefs	O
and	O
domain	B
knowledge	O
for	O
large	O
databases	O
computing	O
the	O
nearest	B
neighbour	B
of	O
a	O
novel	O
point	O
x	O
can	O
be	O
very	O
time-consuming	O
since	O
x	O
needs	O
to	O
be	O
compared	O
to	O
each	O
of	O
the	O
training	B
points	O
depending	O
on	O
the	O
geometry	O
of	O
the	O
training	B
points	O
finding	O
the	O
nearest	B
neighbour	B
can	O
accelerated	O
by	O
examining	O
the	O
values	O
of	O
each	O
of	O
the	O
components	O
xi	O
of	O
x	O
in	O
turn	O
such	O
an	O
axis-aligned	O
space-split	O
is	O
called	O
a	O
and	O
can	O
reduce	O
the	O
possible	O
set	O
of	O
candidate	O
nearest	O
neighbours	O
in	O
the	O
training	B
set	O
to	O
the	O
novel	O
x	O
particularly	O
in	O
low	O
dimensions	O
k-nearest	O
neighbours	O
basing	O
the	O
classification	B
on	O
only	O
the	O
single	O
nearest	B
neighbour	B
can	O
lead	O
to	O
inaccuracies	O
if	O
your	O
neighbour	B
is	O
simply	O
mistaken	O
an	O
incorrect	O
training	B
class	O
label	O
or	O
is	O
not	O
a	O
particularly	O
representative	O
example	O
of	O
his	O
class	O
then	O
these	O
situations	O
will	O
typically	O
result	O
in	O
an	O
incorrect	O
classification	B
by	O
including	O
more	O
than	O
the	O
single	O
nearest	B
neighbour	B
we	O
hope	O
to	O
make	O
a	O
more	O
robust	O
classifier	B
with	O
a	O
smoother	O
decision	B
boundary	B
swayed	O
by	O
single	O
neighbour	B
opinions	O
for	O
datapoints	O
which	O
are	O
somewhat	O
anomalous	O
compared	O
with	O
draft	O
march	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
figure	O
in	O
k-nearest	O
neighbours	O
we	O
centre	O
a	O
hypersphere	O
around	O
the	O
point	O
we	O
wish	O
to	O
classify	O
the	O
central	O
dot	O
the	O
inner	O
circle	O
corresponds	O
to	O
the	O
nearest	B
neighbour	B
however	O
using	O
the	O
nearest	O
neighbours	O
we	O
find	O
that	O
there	O
are	O
two	O
blue	O
classes	O
and	O
one	O
red	O
and	O
we	O
would	O
therefore	O
class	O
the	O
point	O
as	O
blue	O
class	O
in	O
the	O
case	O
of	O
a	O
tie	O
one	O
can	O
extend	O
k	O
until	O
the	O
tie	O
is	O
broken	O
neighbours	O
from	O
the	O
same	O
class	O
their	O
influence	O
will	O
be	O
outvoted	O
if	O
we	O
assume	O
the	O
euclidean	O
distance	O
as	O
the	O
dissimilarity	O
measure	O
the	O
k-nearest	O
neighbour	B
algorithm	B
considers	O
a	O
hypersphere	O
centred	O
on	O
the	O
test	O
point	O
x	O
we	O
increase	O
the	O
radius	O
r	O
until	O
the	O
hypersphere	O
contains	O
exactly	O
k	O
points	O
in	O
the	O
training	B
data	O
the	O
class	O
label	O
cx	O
is	O
then	O
given	O
by	O
the	O
most	O
numerous	O
class	O
within	O
the	O
hypersphere	O
choosing	O
k	O
whilst	O
there	O
is	O
some	O
sense	O
in	O
making	O
k	O
there	O
is	O
certainly	O
little	O
sense	O
in	O
making	O
k	O
n	O
being	O
the	O
number	O
of	O
training	B
points	O
for	O
k	O
very	O
large	O
all	O
classifications	O
will	O
become	O
the	O
same	O
simply	O
assign	O
each	O
novel	O
x	O
to	O
the	O
most	O
numerous	O
class	O
in	O
the	O
training	B
data	O
this	O
suggests	O
that	O
there	O
is	O
an	O
optimal	O
intermediate	O
setting	O
of	O
k	O
which	O
gives	O
the	O
best	O
generalisation	B
performance	B
this	O
can	O
be	O
determined	O
using	O
cross-validation	B
as	O
described	O
in	O
example	O
digit	O
example	O
consider	O
two	O
classes	O
of	O
handwritten	B
digits	I
zeros	O
and	O
ones	O
each	O
digit	O
contains	O
pixels	O
the	O
training	B
data	O
consists	O
of	O
zeros	O
and	O
ones	O
a	O
subset	O
of	O
which	O
are	O
plotted	O
in	O
to	O
test	O
the	O
performance	B
of	O
the	O
nearest	B
neighbour	B
method	O
on	O
euclidean	O
distance	O
we	O
use	O
an	O
independent	O
test	B
set	I
containing	O
a	O
further	O
digits	O
the	O
nearest	B
neighbour	B
method	O
applied	O
to	O
this	O
data	O
correctly	O
predicts	O
the	O
class	O
label	O
of	O
all	O
test	O
points	O
the	O
reason	O
for	O
the	O
high	O
success	O
rate	O
is	O
that	O
examples	O
of	O
zeros	O
and	O
ones	O
are	O
sufficiently	O
different	O
that	O
they	O
can	O
be	O
easily	O
distinguished	O
a	O
more	O
difficult	O
task	O
is	O
to	O
distinguish	O
between	O
ones	O
and	O
sevens	O
we	O
repeat	O
the	O
above	O
experiment	O
now	O
using	O
training	B
examples	O
of	O
ones	O
and	O
training	B
examples	O
of	O
sevens	O
again	O
new	O
test	O
examples	O
ones	O
and	O
sevens	O
were	O
used	O
to	O
assess	O
the	O
performance	B
this	O
time	O
errors	O
are	O
found	O
using	O
nearest	B
neighbour	B
classification	B
a	O
error	O
rate	O
for	O
this	O
two	O
class	O
problem	B
the	O
test	O
points	O
on	O
which	O
the	O
nearest	B
neighbour	B
method	O
makes	O
errors	O
are	O
plotted	O
in	O
if	O
we	O
use	O
k	O
nearest	O
neighbours	O
the	O
classification	B
error	O
reduces	O
to	O
a	O
slight	O
improvement	O
as	O
an	O
aside	O
the	O
best	O
machine	O
learning	B
methods	O
classify	O
real	O
world	O
digits	O
all	O
classes	O
to	O
an	O
error	O
of	O
less	O
than	O
better	O
than	O
the	O
performance	B
of	O
an	O
average	B
human	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
consider	O
the	O
situation	O
where	O
we	O
have	O
simplicity	O
data	O
from	O
two	O
classes	O
class	O
and	O
class	O
we	O
make	O
the	O
following	O
mixture	B
model	B
for	O
data	O
from	O
class	O
e	O
xn	O
pxc	O
n	O
n	O
class	O
n	O
class	O
where	O
d	O
is	O
the	O
dimension	O
of	O
a	O
datapoint	O
x	O
and	O
are	O
the	O
number	O
of	O
training	B
datapoints	O
of	O
class	O
and	O
is	O
the	O
variance	B
this	O
is	O
a	O
parzen	B
estimator	I
which	O
models	O
the	O
data	O
distribution	B
as	O
a	O
uniform	B
weighted	O
sum	O
of	O
distributions	O
centred	O
on	O
the	O
training	B
points	O
draft	O
march	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
figure	O
some	O
of	O
the	O
training	B
examples	O
of	O
the	O
digit	O
zero	O
and	O
one	O
and	O
seven	O
there	O
are	O
training	B
examples	O
of	O
each	O
of	O
these	O
three	O
digit	O
classes	O
figure	O
versus	O
classification	B
using	O
the	O
nn	O
method	O
the	O
out	O
of	O
test	O
examples	O
that	O
are	O
incorrectly	O
classified	O
the	O
nearest	O
neighbours	O
in	O
the	O
training	B
set	O
corresponding	O
to	O
each	O
testpoint	O
above	O
n	O
class	O
e	O
similarly	O
for	O
data	O
from	O
class	O
xn	O
pxc	O
to	O
classify	O
a	O
new	O
datapoint	O
x	O
we	O
use	O
bayes	O
rule	O
n	O
class	O
px	O
n	O
pc	O
px	O
px	O
the	O
maximum	B
likelihood	B
setting	O
of	O
pc	O
is	O
and	O
pc	O
an	O
analogous	O
expression	O
to	O
equation	B
holds	O
for	O
pc	O
to	O
see	O
which	O
class	O
is	O
most	O
likely	O
we	O
may	O
use	O
the	O
ratio	O
pc	O
pc	O
px	O
px	O
if	O
this	O
ratio	O
is	O
greater	O
than	O
one	O
we	O
classify	O
x	O
as	O
otherwise	O
is	O
a	O
complicated	O
function	B
of	O
x	O
however	O
if	O
is	O
very	O
small	O
the	O
numerator	O
which	O
is	O
a	O
sum	O
of	O
exponential	B
terms	O
will	O
be	O
dominated	O
by	O
that	O
term	O
for	O
which	O
datapoint	O
in	O
class	O
is	O
closest	O
to	O
the	O
point	O
x	O
similarly	O
the	O
denominator	O
will	O
be	O
dominated	O
by	O
that	O
datapoint	O
in	O
class	O
which	O
is	O
closest	O
to	O
x	O
in	O
this	O
case	O
therefore	O
pc	O
pc	O
e	O
e	O
e	O
e	O
taking	O
the	O
limit	O
with	O
certainty	O
we	O
classify	O
x	O
as	O
class	O
if	O
x	O
has	O
a	O
point	O
in	O
the	O
class	O
data	O
which	O
is	O
closer	O
than	O
the	O
closest	O
point	O
in	O
the	O
class	O
data	O
the	O
nearest	B
neighbour	B
method	O
is	O
therefore	O
recovered	O
as	O
the	O
limiting	O
case	O
of	O
a	O
probabilistic	B
generative	B
model	B
see	O
the	O
motivation	O
of	O
using	O
k	O
nearest	O
neighbours	O
is	O
to	O
produce	O
a	O
result	O
that	O
is	O
robust	O
against	O
unrepresentative	O
nearest	O
neighbours	O
to	O
ensure	O
a	O
similar	O
kind	O
of	O
robustness	O
in	O
the	O
probabilistic	B
interpretation	O
we	O
may	O
use	O
a	O
finite	O
value	B
this	O
smoothes	O
the	O
extreme	O
probabilities	O
of	O
classification	B
and	O
means	O
that	O
more	O
points	O
just	O
the	O
nearest	O
will	O
have	O
an	O
effective	O
contribution	O
in	O
equation	B
the	O
extension	O
to	O
more	O
than	O
draft	O
march	O
exercises	O
figure	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
for	O
each	O
class	O
we	O
use	O
a	O
mixture	B
of	O
gaussians	O
to	O
model	B
the	O
data	O
from	O
that	O
class	O
pxc	O
placing	O
at	O
each	O
training	B
point	O
an	O
isotropic	B
gaussian	B
of	O
width	O
in	O
the	O
limit	O
the	O
width	O
of	O
each	O
gaussian	B
is	O
represented	O
by	O
the	O
circle	O
a	O
novel	O
point	O
is	O
assigned	O
the	O
class	O
of	O
its	O
nearest	B
neighbour	B
for	O
finite	O
the	O
influence	O
of	O
non-nearest	O
neighbours	O
has	O
an	O
effect	O
resulting	O
in	O
a	O
soft	B
version	O
of	O
nearest	O
neighbours	O
two	O
classes	O
is	O
straightforward	O
requiring	O
a	O
class	O
conditional	B
generative	B
model	B
for	O
each	O
class	O
to	O
go	O
beyond	O
nearest	B
neighbour	B
methods	O
we	O
can	O
relax	O
the	O
assumption	O
of	O
using	O
a	O
parzen	B
estimator	I
and	O
use	O
a	O
richer	O
generative	B
model	B
we	O
will	O
examine	O
such	O
cases	O
in	O
some	O
detail	O
in	O
later	O
chapters	O
in	O
particular	O
when	O
your	O
nearest	B
neighbour	B
is	O
far	O
away	O
for	O
a	O
novel	O
input	O
x	O
that	O
is	O
far	O
from	O
all	O
training	B
points	O
nearest	O
neighbours	O
and	O
its	O
soft	B
probabilistic	B
variant	O
will	O
confidently	O
classify	O
x	O
as	O
belonging	O
to	O
the	O
class	O
of	O
the	O
nearest	O
training	B
point	O
this	O
is	O
arguably	O
opposite	O
to	O
what	O
we	O
would	O
like	O
namely	O
that	O
the	O
classification	B
should	O
tend	O
to	O
the	O
prior	B
probabilities	O
of	O
the	O
class	O
based	O
on	O
the	O
number	O
of	O
training	B
data	O
per	O
class	O
a	O
way	O
to	O
avoid	O
this	O
problem	B
is	O
for	O
each	O
class	O
to	O
include	O
a	O
fictitious	O
mixture	B
component	O
at	O
the	O
mean	B
of	O
all	O
the	O
data	O
with	O
large	O
variance	B
equal	O
for	O
each	O
class	O
for	O
novel	O
inputs	O
close	O
to	O
the	O
training	B
data	O
this	O
extra	O
fictitious	O
datapoint	O
will	O
have	O
no	O
appreciable	O
effect	O
however	O
as	O
we	O
move	O
away	O
from	O
the	O
high	O
density	B
regions	O
of	O
the	O
training	B
data	O
this	O
additional	O
fictitious	O
component	O
will	O
dominate	O
since	O
the	O
distance	O
from	O
x	O
to	O
each	O
fictitious	O
class	O
point	O
is	O
the	O
same	O
in	O
the	O
limit	O
that	O
x	O
is	O
far	O
from	O
the	O
training	B
data	O
the	O
effect	O
is	O
that	O
no	O
class	O
information	O
from	O
the	O
position	O
of	O
x	O
occurs	O
see	O
for	O
an	O
example	O
code	O
nearneigh	O
m	O
k	O
nearest	B
neighbour	B
utility	B
routines	O
majority	O
m	O
find	O
the	O
majority	O
entry	O
in	O
each	O
column	O
of	O
a	O
matrix	B
demonstration	O
demonearneigh	O
m	O
k	O
nearest	B
neighbour	B
demo	O
exercises	O
exercise	O
the	O
file	O
nndata	O
mat	O
contains	O
training	B
and	O
test	O
data	O
for	O
the	O
handwritten	B
digits	I
and	O
using	O
leave	O
one	O
out	O
cross-validation	B
find	O
the	O
optimal	O
k	O
in	O
k-nearest	O
neighours	O
and	O
use	O
this	O
to	O
compute	O
the	O
classification	B
accuracy	O
of	O
the	O
method	O
on	O
the	O
test	O
data	O
exercise	O
write	O
a	O
routine	O
softnearneighxtrainxtesttrainlabelssigma	O
to	O
implement	O
soft	B
nearest	O
neighbours	O
analogous	O
to	O
nearneigh	O
m	O
here	O
sigma	O
is	O
the	O
variance	B
in	O
equation	B
as	O
above	O
the	O
file	O
nndata	O
mat	O
contains	O
training	B
and	O
test	O
data	O
for	O
the	O
handwritten	B
digits	I
and	O
using	O
leave	O
one	O
out	O
cross-validation	B
find	O
the	O
optimal	O
and	O
use	O
this	O
to	O
compute	O
the	O
classification	B
accuracy	O
of	O
the	O
method	O
on	O
the	O
test	O
data	O
hint	O
you	O
may	O
have	O
numerical	B
difficulty	O
with	O
this	O
method	O
to	O
avoid	O
this	O
consider	O
using	O
the	O
logarithm	O
and	O
how	O
to	O
numerically	O
compute	O
for	O
large	O
a	O
and	O
b	O
see	O
also	O
logsumexp	O
m	O
draft	O
march	O
exercise	O
in	O
the	O
text	O
we	O
suggested	O
the	O
use	O
of	O
the	O
mahalanobis	B
distance	I
dx	O
y	O
yt	O
y	O
exercises	O
as	O
a	O
way	O
to	O
improve	O
on	O
the	O
euclidean	O
distance	O
with	O
the	O
covariance	B
matrix	B
of	O
the	O
combined	O
data	O
from	O
both	O
classes	O
consider	O
a	O
modification	O
based	O
on	O
using	O
a	O
mixture	B
model	B
n	O
class	O
n	O
class	O
and	O
pxc	O
pxc	O
n	O
xn	O
n	O
xn	O
explain	O
how	O
the	O
soft	B
nearest	O
neighbours	O
algorithm	B
can	O
deal	O
with	O
the	O
issue	O
that	O
the	O
distribution	B
of	O
data	O
from	O
the	O
different	O
classes	O
can	O
be	O
very	O
different	O
for	O
the	O
case	O
pc	O
pc	O
log	O
and	O
and	O
small	O
derive	O
a	O
simple	O
expression	O
that	O
approximates	O
exercise	O
the	O
editor	O
at	O
yoman	O
mens	O
magazine	O
has	O
just	O
had	O
a	O
great	O
idea	O
based	O
on	O
the	O
success	O
of	O
a	O
recent	O
national	O
poll	O
to	O
test	O
iq	O
she	O
decides	O
to	O
make	O
a	O
beauty	O
quotient	O
test	O
she	O
collects	O
as	O
many	O
images	O
of	O
male	O
faces	B
as	O
she	O
can	O
taking	O
care	O
to	O
make	O
sure	O
that	O
all	O
the	O
images	O
are	O
scaled	O
to	O
roughly	O
the	O
same	O
size	O
and	O
under	O
the	O
same	O
lighting	O
conditions	O
she	O
then	O
gives	O
each	O
male	O
face	O
a	O
bq	O
score	O
from	O
severely	O
aesthetically	O
challenged	O
to	O
generously	O
aesthetically	O
gifted	O
thus	O
for	O
each	O
image	O
x	O
there	O
is	O
an	O
associated	O
value	B
b	O
in	O
the	O
range	O
to	O
in	O
total	O
she	O
collects	O
n	O
images	O
and	O
associated	O
scores	O
bn	O
n	O
n	O
where	O
each	O
image	O
is	O
represented	O
by	O
a	O
d-dimensional	O
real-valued	O
vector	O
x	O
one	O
morning	O
she	O
bounces	O
into	O
your	O
office	O
and	O
tells	O
you	O
the	O
good	O
news	O
it	O
is	O
your	O
task	O
to	O
make	O
a	O
test	O
for	O
the	O
male	O
nation	O
to	O
determine	O
their	O
beauty	O
quotient	O
the	O
idea	O
she	O
explains	O
is	O
that	O
a	O
man	O
can	O
send	O
online	B
an	O
image	O
of	O
their	O
face	O
x	O
to	O
yoman	O
and	O
will	O
instantly	O
receive	O
an	O
automatic	O
bq	O
response	O
b	O
as	O
a	O
first	O
step	O
you	O
decide	O
to	O
use	O
the	O
k	O
nearest	B
neighbour	B
method	O
to	O
assign	O
a	O
bq	O
score	O
b	O
to	O
a	O
novel	O
test	O
image	O
x	O
describe	O
how	O
to	O
determine	O
the	O
optimal	O
number	O
of	O
neighbours	O
k	O
to	O
use	O
your	O
line	O
manager	O
is	O
pleased	O
with	O
your	O
algorithm	B
but	O
is	O
disappointed	O
that	O
it	O
does	O
not	O
provide	O
any	O
simple	O
explanation	O
of	O
beauty	O
that	O
she	O
can	O
present	O
in	O
a	O
future	O
version	O
of	O
yoman	O
magazine	O
to	O
address	O
this	O
you	O
decide	O
to	O
make	O
a	O
model	B
based	O
on	O
linear	B
regression	B
that	O
is	O
b	O
wtx	O
where	O
w	O
is	O
a	O
parameter	B
vector	O
chosen	O
to	O
minimise	O
ew	O
n	O
bn	O
after	O
training	B
a	O
suitable	O
w	O
how	O
can	O
yoman	O
explain	O
to	O
its	O
readership	O
in	O
a	O
simple	O
way	O
what	O
facial	O
features	O
are	O
important	O
for	O
determining	O
one	O
s	O
bq	O
describe	O
fully	O
and	O
mathematically	O
a	O
method	O
to	O
train	O
this	O
linear	B
regression	B
model	B
your	O
expla	O
nation	O
must	O
be	O
detailed	O
enough	O
so	O
that	O
a	O
programmer	O
can	O
directly	O
implement	O
it	O
discuss	O
any	O
implications	O
of	O
the	O
situation	O
d	O
n	O
discuss	O
any	O
advantagesdisadvantages	O
of	O
using	O
the	O
linear	B
regression	B
model	B
compared	O
with	O
using	O
the	O
knn	O
approach	B
draft	O
march	O
chapter	O
unsupervised	B
linear	B
dimension	I
reduction	I
high-dimensional	O
spaces	O
low	B
dimensional	I
manifolds	O
in	O
machine	O
learning	B
problems	O
data	O
is	O
often	O
high	O
dimensional	O
images	O
bag-of-word	O
descriptions	O
geneexpresssions	O
etc	O
in	O
such	O
cases	O
we	O
cannot	O
expect	O
the	O
training	B
data	O
to	O
densely	O
populate	O
the	O
space	O
meaning	O
that	O
there	O
will	O
be	O
large	O
parts	O
in	O
which	O
little	O
is	O
known	O
about	O
the	O
data	O
for	O
the	O
hand-written	O
digits	O
from	O
the	O
data	O
is	O
dimensional	O
for	O
binary	O
valued	O
pixels	O
the	O
possible	O
number	O
of	O
images	O
that	O
could	O
ever	O
exist	O
is	O
nevertheless	O
we	O
would	O
expect	O
that	O
only	O
a	O
handful	O
of	O
examples	O
of	O
a	O
digit	O
should	O
be	O
sufficient	O
a	O
human	O
to	O
understand	O
how	O
to	O
recognise	O
a	O
digit-like	O
images	O
must	O
therefore	O
occupy	O
a	O
highly	O
constrained	O
subspace	O
of	O
the	O
dimensions	O
and	O
we	O
expect	O
only	O
a	O
small	O
number	O
of	O
directions	O
to	O
be	O
relevant	O
for	O
describing	O
the	O
data	O
to	O
a	O
reasonable	O
accuracy	O
whilst	O
the	O
data	O
vectors	O
may	O
be	O
very	O
high	O
dimensional	O
they	O
will	O
therefore	O
typically	O
lie	O
close	O
to	O
a	O
much	O
lower	O
dimensional	O
manifold	O
a	O
two-dimensional	O
manifold	O
corresponds	O
to	O
a	O
warped	O
sheet	O
of	O
paper	O
embedded	O
in	O
a	O
high	O
dimensional	O
space	O
meaning	O
that	O
the	O
distribution	B
of	O
the	O
data	O
is	O
heavily	O
constrained	O
here	O
we	O
concentrate	O
on	O
linear	B
dimension	I
reduction	I
techniques	O
for	O
which	O
there	O
exist	O
computationally	O
efficient	O
approaches	O
in	O
this	O
approach	B
a	O
high	O
dimensional	O
datapoint	O
x	O
is	O
projected	O
down	O
to	O
a	O
lower	O
dimensional	O
vector	O
y	O
by	O
y	O
fx	O
const	O
where	O
the	O
non-square	O
matrix	B
f	O
has	O
dimensions	O
dim	O
dim	O
with	O
dim	O
dim	O
the	O
methods	O
in	O
this	O
chapter	O
are	O
largely	O
non-probabilistic	O
although	O
many	O
have	O
natural	B
probabilistic	B
interpretations	O
for	O
example	O
pca	B
is	O
closely	O
related	O
to	O
factor	B
analysis	B
described	O
in	O
principal	B
components	I
analysis	B
if	O
data	O
lies	O
close	O
to	O
a	O
hyperplane	B
as	O
in	O
we	O
can	O
accurately	O
approximate	B
each	O
data	O
point	O
by	O
using	O
vectors	O
that	O
span	O
the	O
hyperplane	B
alone	O
effectively	O
we	O
are	O
trying	O
to	O
discover	O
a	O
low	B
dimensional	I
co-ordinate	O
system	O
in	O
which	O
we	O
can	O
approximately	O
represent	O
the	O
data	O
we	O
express	O
the	O
approximation	B
for	O
datapoint	O
xn	O
as	O
xn	O
c	O
i	O
bi	O
xn	O
yn	O
here	O
the	O
vector	O
c	O
is	O
a	O
constant	O
and	O
defines	O
a	O
point	O
in	O
the	O
hyperplane	B
and	O
the	O
bi	O
define	O
vectors	O
in	O
the	O
hyperplane	B
known	O
as	O
principal	O
component	O
coefficients	O
or	O
loadings	O
the	O
yn	O
i	O
are	O
the	O
low	B
dimensional	I
co-ordinates	O
of	O
the	O
data	O
expresses	O
how	O
to	O
find	O
the	O
reconstruction	O
xn	O
given	O
the	O
lower	O
dimensional	O
representation	O
yn	O
has	O
components	O
yn	O
i	O
i	O
m	O
for	O
a	O
data	O
space	O
of	O
dimension	O
principal	B
components	I
analysis	B
figure	O
in	O
linear	B
dimension	I
reduction	I
a	O
hyperplane	B
is	O
fitted	O
such	O
that	O
the	O
average	B
distance	O
between	O
datapoints	O
rings	O
and	O
their	O
projections	O
onto	O
the	O
plane	O
dots	O
is	O
minimal	O
dim	O
d	O
we	O
hope	O
to	O
accurately	O
describe	O
the	O
data	O
using	O
only	O
a	O
small	O
number	O
m	O
d	O
of	O
co-ordinates	O
y	O
to	O
determine	O
the	O
best	O
lower	O
dimensional	O
representation	O
it	O
is	O
convenient	O
to	O
use	O
the	O
square	O
distance	O
error	O
between	O
x	O
and	O
its	O
reconstruction	O
x	O
eb	O
y	O
c	O
i	O
i	O
xn	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
optimal	O
bias	B
c	O
is	O
given	O
by	O
the	O
mean	B
of	O
the	O
data	O
therefore	O
assume	O
that	O
the	O
data	O
has	O
been	O
centred	O
zero	O
n	O
xnn	O
we	O
n	O
xn	O
so	O
that	O
we	O
can	O
set	O
c	O
to	O
zero	O
and	O
concentrate	O
on	O
finding	O
the	O
optimal	O
basis	O
b	O
below	O
eb	O
y	O
deriving	O
the	O
optimal	O
linear	B
reconstruction	O
to	O
find	O
the	O
best	O
basis	O
vectors	O
b	O
bj	O
coordinates	O
y	O
we	O
may	O
minimize	O
the	O
sum	O
of	O
squared	O
differences	O
between	O
each	O
vector	O
x	O
i	O
and	O
corresponding	O
low	B
dimensional	I
and	O
its	O
reconstruction	O
x	O
xn	O
where	O
x	O
consider	O
a	O
transformation	O
q	O
of	O
the	O
basis	O
b	O
so	O
that	O
b	O
bq	O
is	O
an	O
orthonormal	B
matrix	B
bt	O
b	O
i	O
since	O
q	O
is	O
invertible	O
we	O
may	O
write	O
by	O
bq	O
b	O
y	O
which	O
is	O
of	O
then	O
same	O
form	O
as	O
by	O
albeit	O
with	O
an	O
orthonormality	O
constraint	O
on	O
b	O
hence	O
without	O
loss	O
of	O
generality	O
we	O
may	O
consider	O
equation	B
under	O
the	O
orthonormality	O
constraint	O
btb	O
i	O
namely	O
that	O
the	O
basis	O
vectors	O
are	O
mutually	O
orthogonal	B
and	O
of	O
unit	O
length	O
byt	O
by	O
trace	O
j	O
bj	O
yn	O
i	O
i	O
by	O
differentiating	O
equation	B
with	O
respect	O
to	O
yn	O
k	O
we	O
obtain	O
the	O
orthonormality	O
constraint	O
eb	O
y	O
xn	O
i	O
bk	O
i	O
j	O
bj	O
yn	O
i	O
i	O
j	O
i	O
yn	O
k	O
j	O
i	O
bj	O
i	O
bk	O
i	O
i	O
jk	O
xn	O
i	O
bk	O
i	O
yn	O
j	O
the	O
squared	O
error	O
eb	O
y	O
therefore	O
has	O
zero	O
derivative	O
when	O
k	O
yn	O
bk	O
i	O
xn	O
i	O
i	O
i	O
bk	O
xn	O
i	O
yn	O
k	O
draft	O
march	O
we	O
now	O
substitute	O
this	O
solution	O
into	O
equation	B
to	O
write	O
the	O
squared	O
error	O
only	O
as	O
a	O
function	B
of	O
b	O
bijbkjxn	O
k	O
principal	B
components	I
analysis	B
j	O
j	O
bj	O
yn	O
i	O
eb	O
jk	O
the	O
objective	O
eb	O
becomes	O
jk	O
kxn	O
bj	O
i	O
bj	O
k	O
i	O
i	O
n	O
eb	O
i	O
bbt	O
btb	O
i	O
hence	O
the	O
objective	O
becomes	O
xn	O
trace	O
n	O
n	O
eb	O
where	O
s	O
is	O
the	O
sample	O
covariance	B
matrix	B
of	O
the	O
trace	O
trace	O
m	O
n	O
xn	O
s	O
mxn	O
mt	O
n	O
l	O
trace	O
trace	O
btb	O
i	O
i	O
to	O
minimise	O
equation	B
under	O
the	O
constraint	O
btb	O
i	O
we	O
use	O
a	O
set	O
of	O
lagrange	O
multipliers	O
l	O
so	O
that	O
the	O
objective	O
is	O
to	O
minimize	O
the	O
constant	O
prefactor	O
n	O
since	O
the	O
constraint	O
is	O
symmetric	O
we	O
can	O
assume	O
that	O
l	O
is	O
also	O
symmetric	O
differentiating	O
with	O
respect	O
to	O
b	O
and	O
equating	O
to	O
zero	O
we	O
obtain	O
that	O
at	O
the	O
optimum	O
sb	O
bl	O
whose	O
columns	O
are	O
the	O
corresponding	O
eigenvectors	O
of	O
s	O
in	O
this	O
case	O
trace	O
which	O
is	O
this	O
is	O
a	O
form	O
of	O
eigen-equation	O
so	O
that	O
a	O
solution	O
is	O
given	O
by	O
taking	O
l	O
to	O
be	O
diagonal	O
and	O
b	O
as	O
the	O
matrix	B
the	O
sum	O
of	O
the	O
eigenvalues	O
corresponding	O
to	O
the	O
eigenvectors	O
forming	O
b	O
since	O
we	O
wish	O
to	O
minimise	O
eb	O
we	O
take	O
the	O
eigenvectors	O
with	O
largest	O
corresponding	O
eigenvalues	O
if	O
we	O
order	O
the	O
eigenvalues	O
the	O
squared	O
error	O
is	O
given	O
by	O
from	O
equation	B
n	O
eb	O
trace	O
trace	O
i	O
im	O
i	O
i	O
whilst	O
the	O
solution	O
to	O
this	O
eigen-problem	O
is	O
unique	O
this	O
only	O
serves	O
to	O
define	O
the	O
solution	O
subspace	O
since	O
one	O
may	O
rotate	O
and	O
scale	O
b	O
and	O
y	O
such	O
that	O
the	O
value	B
of	O
the	O
squared	O
loss	O
is	O
exactly	O
the	O
same	O
the	O
justification	O
for	O
choosing	O
the	O
non-rotated	O
eigen	O
solution	O
is	O
given	O
by	O
the	O
additional	O
requirement	O
that	O
the	O
principal	O
components	O
corresponds	O
to	O
directions	O
of	O
maximal	O
variance	B
as	O
explained	O
in	O
we	O
use	O
the	O
unbiased	O
sample	O
covariance	B
simply	O
because	O
this	O
is	O
standard	O
in	O
the	O
literature	O
if	O
we	O
were	O
to	O
replace	O
this	O
with	O
the	O
sample	O
covariance	B
as	O
defined	O
in	O
the	O
only	O
change	O
required	O
is	O
to	O
replace	O
n	O
by	O
n	O
throughout	O
which	O
has	O
no	O
effect	O
on	O
the	O
form	O
of	O
the	O
solutions	O
found	O
by	O
pca	B
draft	O
march	O
yn	O
i	O
principal	B
components	I
analysis	B
figure	O
projection	B
of	O
two	O
dimensional	O
data	O
using	O
one	O
dimensional	O
pca	B
plotted	O
are	O
the	O
original	O
datapoints	O
x	O
rings	O
and	O
their	O
reconstructions	O
x	O
dots	O
using	O
dimensional	O
pca	B
the	O
lines	O
represent	O
the	O
orthogonal	B
projection	B
of	O
the	O
original	O
datapoint	O
onto	O
the	O
first	O
eigenvector	O
the	O
arrows	O
are	O
the	O
two	O
eigenvectors	O
scaled	O
by	O
the	O
square	O
root	O
of	O
their	O
corresponding	O
eigenvalues	O
the	O
data	O
has	O
been	O
centred	O
to	O
have	O
zero	O
mean	B
for	O
each	O
high	O
dimensional	O
datapoint	O
x	O
the	O
low	B
dimensional	I
representation	O
y	O
is	O
given	O
in	O
this	O
case	O
by	O
the	O
distance	O
negative	O
from	O
the	O
origin	O
along	O
the	O
first	O
eigenvector	O
direction	O
to	O
the	O
corresponding	O
orthogonal	B
projection	B
point	O
maximum	O
variance	B
criterion	O
we	O
search	O
first	O
for	O
the	O
single	O
direction	O
b	O
such	O
that	O
when	O
the	O
data	O
is	O
projected	O
onto	O
this	O
direction	O
the	O
variance	B
of	O
this	O
projection	B
is	O
maximal	O
amongst	O
all	O
possible	O
such	O
projections	O
using	O
equation	B
for	O
a	O
single	O
vector	O
b	O
we	O
have	O
bixn	O
i	O
the	O
projection	B
of	O
a	O
datapoint	O
onto	O
a	O
direction	O
b	O
is	O
btxn	O
for	O
a	O
unit	O
length	O
vector	O
b	O
hence	O
the	O
sum	O
of	O
squared	O
projections	O
is	O
bt	O
xn	O
b	O
n	O
n	O
which	O
ignoring	O
constants	O
is	O
simply	O
the	O
negative	O
of	O
equation	B
for	O
a	O
single	O
retained	O
eigenvector	O
b	O
sb	O
b	O
hence	O
the	O
optimal	O
single	O
b	O
which	O
maximises	O
the	O
projection	B
variance	B
is	O
given	O
by	O
the	O
eigenvector	O
corresponding	O
to	O
the	O
largest	O
eigenvalue	O
of	O
s	O
under	O
the	O
criterion	O
that	O
the	O
next	O
optimal	O
direction	O
should	O
be	O
orthonormal	B
to	O
the	O
first	O
one	O
can	O
readily	O
show	O
that	O
is	O
given	O
by	O
the	O
second	O
largest	O
eigenvector	O
and	O
so	O
on	O
this	O
explains	O
why	O
despite	O
the	O
squared	O
loss	O
equation	B
being	O
invariant	O
with	O
respect	O
to	O
arbitrary	O
rotation	O
scaling	O
of	O
the	O
basis	O
vectors	O
the	O
ones	O
given	O
by	O
the	O
eigen-decomposition	O
have	O
the	O
additional	O
property	O
that	O
they	O
correspond	O
to	O
directions	O
of	O
maximal	O
variance	B
these	O
maximal	O
variance	B
directions	O
found	O
by	O
pca	B
are	O
called	O
the	O
principal	B
directions	I
pca	B
algorithm	B
the	O
routine	O
for	O
pca	B
is	O
presented	O
in	O
in	O
the	O
notation	O
of	O
y	O
fx	O
the	O
projection	B
matrix	B
f	O
corresponds	O
to	O
et	O
similarly	O
for	O
the	O
reconstruction	O
equation	B
the	O
coordinate	O
yn	O
corresponds	O
to	O
etxn	O
and	O
bi	O
corresponds	O
to	O
ei	O
the	O
pca	B
reconstructions	O
are	O
orthogonal	B
projections	O
of	O
the	O
data	O
onto	O
the	O
subspace	O
spanned	O
by	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
of	O
the	O
covariance	B
matrix	B
see	O
figure	O
top	O
row	O
a	O
selection	O
of	O
the	O
digit	O
taken	O
from	O
the	O
database	O
of	O
examples	O
plotted	O
beneath	O
each	O
digit	O
is	O
the	O
reconstruction	O
using	O
and	O
eigenvectors	O
top	O
to	O
bottom	O
note	O
how	O
the	O
reconstructions	O
for	O
fewer	O
eigenvectors	O
express	O
less	O
variability	O
from	O
each	O
other	O
and	O
resemble	O
more	O
a	O
mean	B
digit	O
draft	O
march	O
principal	B
components	I
analysis	B
algorithm	B
principal	B
components	I
analysis	B
to	O
form	O
an	O
m-dimensional	O
approximation	B
of	O
a	O
dataset	O
n	O
n	O
with	O
dim	O
xn	O
d	O
find	O
the	O
d	O
sample	O
mean	B
vector	O
and	O
d	O
d	O
covariance	B
matrix	B
m	O
n	O
xn	O
s	O
n	O
mxn	O
mt	O
find	O
the	O
eigenvectors	O
em	B
of	O
the	O
covariance	B
matrix	B
s	O
sorted	O
so	O
that	O
the	O
eigenvalue	O
of	O
ei	O
is	O
larger	O
than	O
ej	O
for	O
i	O
j	O
form	O
the	O
matrix	B
e	O
em	B
the	O
lower	O
dimensional	O
representation	O
of	O
each	O
data	O
point	O
xn	O
is	O
given	O
by	O
yn	O
etxn	O
m	O
the	O
approximate	B
reconstruction	O
of	O
the	O
original	O
datapoint	O
xn	O
is	O
the	O
total	O
squared	O
error	O
over	O
all	O
the	O
training	B
data	O
made	O
by	O
the	O
approximation	B
is	O
xn	O
m	O
eyn	O
j	O
jm	O
where	O
m	O
n	O
are	O
the	O
eigenvalues	O
discarded	O
in	O
the	O
projection	B
example	O
the	O
dimension	O
of	O
digits	O
we	O
have	O
examples	O
of	O
handwritten	O
s	O
where	O
each	O
image	O
consists	O
of	O
real-values	O
pixels	O
see	O
each	O
image	O
matrix	B
is	O
stacked	O
to	O
form	O
a	O
dimensional	O
vector	O
giving	O
a	O
dimensional	O
data	O
matrix	B
x	O
the	O
covariance	B
matrix	B
of	O
this	O
data	O
has	O
eigenvalue	O
spectrum	B
as	O
plotted	O
in	O
where	O
we	O
plot	O
only	O
the	O
largest	O
eigenvalues	O
note	O
how	O
after	O
around	O
components	O
the	O
mean	B
squared	O
reconstruction	O
error	O
is	O
small	O
indicating	O
that	O
the	O
data	O
indeed	O
lie	O
close	O
to	O
a	O
dimensional	O
hyperplane	B
the	O
eigenvalues	O
are	O
computed	O
using	O
pca	B
m	O
the	O
reconstructions	O
using	O
different	O
numbers	O
of	O
eigenvectors	O
and	O
are	O
plotted	O
in	O
note	O
how	O
using	O
only	O
a	O
small	O
number	O
of	O
eigenvectors	O
the	O
reconstruction	O
more	O
closely	O
resembles	O
the	O
mean	B
image	O
example	O
in	O
we	O
present	O
example	O
images	O
for	O
which	O
we	O
wish	O
to	O
find	O
a	O
lower	O
dimensional	O
representation	O
using	O
pca	B
the	O
first	O
eigenfaces	O
are	O
presented	O
along	O
with	O
reconstructions	O
of	O
the	O
original	O
data	O
using	O
these	O
eigenfaces	O
see	O
figure	O
for	O
the	O
digits	O
data	O
consisting	O
of	O
examples	O
of	O
the	O
digit	O
each	O
image	O
being	O
represented	O
by	O
a	O
dimensional	O
vector	O
plotted	O
as	O
the	O
largest	O
eigenvalues	O
so	O
that	O
the	O
largest	O
eigenvalue	O
is	O
of	O
the	O
sample	O
covariance	B
matrix	B
draft	O
march	O
numbereigenvalue	O
principal	B
components	I
analysis	B
figure	O
of	O
the	O
training	B
images	O
people	O
with	O
images	O
of	O
each	O
person	O
each	O
image	O
consists	O
of	O
greyscale	O
pixels	O
the	O
training	B
data	O
is	O
scaled	O
so	O
that	O
represented	O
as	O
an	O
image	O
the	O
components	O
of	O
each	O
image	O
sum	O
to	O
the	O
average	B
value	B
of	O
each	O
pixel	O
across	O
all	O
images	O
is	O
this	O
is	O
a	O
subset	O
of	O
the	O
images	O
in	O
the	O
full	O
olivetti	O
research	O
face	O
database	O
figure	O
svd	B
reconstruction	O
of	O
the	O
images	O
in	O
using	O
a	O
combination	O
of	O
the	O
eigen-images	O
the	O
eigen-images	O
are	O
found	O
using	O
svd	B
of	O
the	O
and	O
taking	O
the	O
eigenvectors	O
with	O
largest	O
eigenvalue	O
the	O
images	O
corresponding	O
to	O
the	O
largest	O
eigenvalues	O
are	O
contained	O
in	O
the	O
first	O
row	O
and	O
the	O
next	O
in	O
the	O
row	O
below	O
etc	O
the	O
root	O
mean	B
square	O
reconstruction	O
error	O
is	O
a	O
small	O
improvement	O
over	O
plsa	O
pca	B
and	O
nearest	O
neighbours	O
for	O
high-dimensional	O
data	O
computing	O
the	O
squared	B
euclidean	I
distance	I
between	O
vectors	O
can	O
be	O
expensive	O
and	O
also	O
sensitive	O
to	O
noise	O
it	O
is	O
therefore	O
often	O
useful	O
to	O
project	O
the	O
data	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
first	O
for	O
example	O
in	O
making	O
a	O
classifier	B
to	O
distinguish	O
between	O
the	O
digit	O
and	O
the	O
digit	O
we	O
can	O
form	O
a	O
lower	O
dimensional	O
representation	O
first	O
by	O
ignoring	O
the	O
class	O
label	O
make	O
a	O
dataset	O
of	O
training	B
points	O
each	O
of	O
the	O
training	B
points	O
xn	O
is	O
then	O
projected	O
to	O
a	O
lower	O
dimensional	O
pca	B
representation	O
yn	O
subsequently	O
any	O
distance	O
calculations	O
are	O
replaced	O
by	O
to	O
justify	O
this	O
consider	O
xbtxa	O
xb	O
m	O
eyb	O
mteya	O
m	O
eyb	O
m	O
ybteteya	O
yb	O
ybtya	O
yb	O
where	O
the	O
last	O
equality	O
is	O
due	O
to	O
the	O
orthonormality	O
of	O
eigenvectors	O
ete	O
i	O
using	O
principal	O
components	O
why	O
this	O
number	O
was	O
chosen	O
and	O
the	O
nearest	B
neighbour	B
rule	O
to	O
classify	O
ones	O
and	O
sevens	O
gave	O
a	O
test-set	O
error	O
of	O
in	O
examples	O
compared	O
to	O
from	O
the	O
standard	O
method	O
on	O
the	O
non-projected	O
data	O
how	O
can	O
it	O
be	O
that	O
the	O
classification	B
performance	B
has	O
improved	O
a	O
plausible	O
explanation	O
is	O
that	O
the	O
new	O
pca	B
representation	O
of	O
the	O
data	O
is	O
more	O
robust	O
since	O
only	O
the	O
large	O
scale	O
change	O
directions	O
in	O
the	O
space	O
are	O
retained	O
with	O
low	O
variance	B
directions	O
discarded	O
draft	O
march	O
high	B
dimensional	I
data	I
figure	O
finding	O
the	O
optimal	O
pca	B
dimension	O
to	O
use	O
for	O
classifying	O
hand-written	O
digits	O
using	O
nearest	O
neighbours	O
training	B
examples	O
are	O
used	O
and	O
the	O
validation	B
error	O
plotted	O
on	O
further	O
examples	O
based	O
on	O
the	O
validation	B
error	O
we	O
see	O
that	O
a	O
dimension	O
of	O
is	O
reasonable	O
example	O
the	O
best	O
pca	B
dimension	O
there	O
are	O
examples	O
of	O
the	O
digit	O
and	O
examples	O
of	O
the	O
digit	O
we	O
will	O
use	O
half	O
the	O
data	O
for	O
training	B
and	O
the	O
other	O
half	O
for	O
testing	O
the	O
training	B
examples	O
were	O
further	O
split	O
into	O
a	O
training	B
set	O
of	O
examples	O
and	O
a	O
separate	O
validation	B
set	O
of	O
examples	O
pca	B
was	O
used	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
inputs	O
and	O
then	O
nearest	O
neighbours	O
used	O
to	O
classify	O
the	O
validation	B
examples	O
different	O
reduced	O
dimensions	O
were	O
in	O
vestigated	O
and	O
based	O
on	O
the	O
validation	B
results	O
was	O
selected	O
as	O
the	O
optimal	O
number	O
of	O
pca	B
components	O
retained	O
see	O
the	O
independent	O
test	O
error	O
on	O
independent	O
examples	O
using	O
dimensions	O
is	O
comments	O
on	O
pca	B
the	O
intrinsic	O
dimension	O
of	O
data	O
how	O
many	O
dimensions	O
should	O
the	O
linear	B
subspace	O
have	O
from	O
equation	B
the	O
reconstruction	O
error	O
is	O
proportional	O
to	O
the	O
sum	O
of	O
the	O
discarded	O
eigenvalues	O
if	O
we	O
plot	O
the	O
eigenvalue	O
spectrum	B
set	O
of	O
eigenvalues	O
ordered	O
by	O
decreasing	O
value	B
we	O
might	O
hope	O
to	O
see	O
a	O
few	O
large	O
values	O
and	O
many	O
small	O
values	O
if	O
the	O
data	O
does	O
lie	O
close	O
to	O
an	O
m	O
dimensional	O
hyperplane	B
we	O
would	O
see	O
m	O
large	O
eigenvalues	O
with	O
the	O
rest	O
being	O
very	O
small	O
this	O
would	O
give	O
an	O
indication	O
of	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
in	O
the	O
data	O
or	O
the	O
intrinsic	O
dimensionality	O
directions	O
corresponding	O
to	O
the	O
small	O
eigenvalues	O
are	O
then	O
interpreted	O
as	O
noise	O
non-linear	B
dimension	O
reduction	O
in	O
pca	B
we	O
are	O
presupposing	O
that	O
the	O
data	O
lies	O
close	O
to	O
a	O
hyperplane	B
is	O
this	O
really	O
a	O
good	O
description	O
more	O
generally	O
we	O
would	O
expect	O
data	O
to	O
lie	O
on	O
low	B
dimensional	I
curved	O
manifolds	O
also	O
data	O
is	O
often	O
clustered	O
examples	O
of	O
handwritten	O
s	O
look	O
similar	O
to	O
each	O
other	O
and	O
form	O
a	O
cluster	O
separate	O
from	O
the	O
s	O
cluster	O
nevertheless	O
since	O
linear	B
dimension	I
reduction	I
is	O
computationally	O
relatively	O
straightforward	O
this	O
is	O
one	O
of	O
the	O
most	O
common	O
dimensionality	O
reduction	O
techniques	O
high	B
dimensional	I
data	I
the	O
computational	B
complexity	I
of	O
computing	O
an	O
eigen-decomposition	O
of	O
a	O
d	O
d	O
matrix	B
is	O
you	O
exceed	O
one	O
can	O
exploit	O
this	O
fact	O
to	O
bound	B
the	O
complexity	O
by	O
as	O
described	O
below	O
might	O
be	O
wondering	O
therefore	O
how	O
it	O
is	O
possible	O
to	O
perform	O
pca	B
on	O
high	B
dimensional	I
data	I
for	O
example	O
if	O
we	O
have	O
images	O
each	O
of	O
pixels	O
the	O
covariance	B
matrix	B
will	O
be	O
dimensional	O
it	O
would	O
appear	O
a	O
significant	O
computational	O
challenge	O
to	O
compute	O
the	O
eigen-decomposition	O
of	O
this	O
matrix	B
in	O
this	O
case	O
however	O
since	O
there	O
are	O
only	O
such	O
vectors	O
the	O
number	O
of	O
non-zero	O
eigenvalues	O
cannot	O
draft	O
march	O
of	O
eigenvaluesnumber	O
of	O
errors	O
eigen-decomposition	O
for	O
n	O
d	O
high	B
dimensional	I
data	I
first	O
note	O
that	O
for	O
zero	O
mean	B
data	O
the	O
sample	O
covariance	B
matrix	B
can	O
be	O
expressed	O
as	O
xn	O
i	O
xn	O
j	O
n	O
in	O
matrix	B
notation	O
this	O
can	O
be	O
written	O
s	O
n	O
xxt	O
where	O
the	O
d	O
n	O
matrix	B
x	O
contains	O
all	O
the	O
data	O
vectors	O
x	O
since	O
the	O
eigenvectors	O
of	O
a	O
matrix	B
m	O
are	O
equal	O
to	O
those	O
of	O
m	O
for	O
scalar	O
one	O
can	O
consider	O
more	O
simply	O
the	O
eigenvectors	O
of	O
xxt	O
writing	O
the	O
d	O
n	O
matrix	B
of	O
eigenvectors	O
as	O
e	O
is	O
a	O
non-square	O
thin	B
matrix	B
since	O
there	O
will	O
be	O
fewer	O
eigenvalues	O
than	O
data	O
dimensions	O
and	O
the	O
eigenvalues	O
as	O
an	O
n	O
n	O
diagonal	O
matrix	B
the	O
eigen-decomposition	O
of	O
the	O
covariance	B
s	O
is	O
xxte	O
e	O
xtxxte	O
xte	O
xtx	O
e	O
e	O
where	O
we	O
defined	O
e	O
xte	O
the	O
final	O
expression	O
above	O
represents	O
the	O
eigenvector	O
equation	B
for	O
xtx	O
this	O
is	O
a	O
matrix	B
of	O
dimensions	O
n	O
n	O
so	O
that	O
calculating	O
the	O
eigen-decomposition	O
takes	O
operations	O
compared	O
with	O
operations	O
in	O
the	O
original	O
high-dimensional	O
space	O
we	O
then	O
can	O
therefore	O
calculate	O
the	O
eigenvectors	O
e	O
and	O
eigenvalues	O
of	O
this	O
matrix	B
more	O
easily	O
once	O
found	O
we	O
use	O
the	O
fact	O
that	O
the	O
eigenvalues	O
of	O
s	O
are	O
given	O
by	O
the	O
diagonal	O
entries	O
of	O
and	O
the	O
eigenvectors	O
by	O
e	O
x	O
e	O
pca	B
via	O
singular	B
value	B
decomposition	B
an	O
alternative	O
to	O
using	O
an	O
eigen-decomposition	O
routine	O
to	O
find	O
the	O
pca	B
solution	O
is	O
to	O
make	O
use	O
of	O
the	O
singular	B
value	B
decomposition	B
of	O
an	O
d	O
n	O
dimensional	O
matrix	B
x	O
this	O
is	O
given	O
by	O
x	O
udvt	O
where	O
utu	O
id	O
and	O
vtv	O
in	O
and	O
d	O
is	O
a	O
diagonal	O
matrix	B
of	O
the	O
singular	B
values	O
we	O
assume	O
that	O
the	O
decomposition	B
has	O
ordered	O
the	O
singular	B
values	O
so	O
that	O
the	O
upper	O
left	O
diagonal	O
element	O
of	O
d	O
contains	O
the	O
largest	O
singular	B
value	B
the	O
matrix	B
xxt	O
can	O
then	O
be	O
written	O
as	O
xxt	O
udvtvdut	O
since	O
is	O
in	O
the	O
form	O
of	O
an	O
eigen-decomposition	O
the	O
pca	B
solution	O
is	O
equivalently	O
given	O
by	O
performing	O
the	O
svd	B
decomposition	B
of	O
x	O
for	O
which	O
the	O
eigenvectors	O
are	O
then	O
given	O
by	O
u	O
and	O
corresponding	O
eigenvalues	O
by	O
the	O
square	O
of	O
the	O
singular	B
values	O
shows	O
that	O
pca	B
is	O
a	O
form	O
of	O
matrix	B
decomposition	B
method	O
x	O
udvt	O
um	O
dm	O
vt	O
m	O
where	O
um	O
dm	O
vm	O
correspond	O
to	O
taking	O
only	O
the	O
first	O
m	O
singular	B
values	O
of	O
the	O
full	O
matrices	O
draft	O
march	O
latent	B
semantic	I
analysis	B
figure	O
document	O
data	O
for	O
a	O
dictionary	O
containing	O
words	O
and	O
documents	O
black	O
indicates	O
that	O
a	O
word	O
was	O
present	O
in	O
a	O
document	O
the	O
data	O
consists	O
of	O
two	O
similar	O
topics	O
only	O
in	O
their	O
usage	O
of	O
the	O
first	O
two	O
words	O
and	O
a	O
random	O
background	O
topic	O
latent	B
semantic	I
analysis	B
in	O
the	O
document	O
analysis	B
literature	O
pca	B
is	O
also	O
called	O
latent	B
semantic	I
analysis	B
and	O
is	O
concerned	O
with	O
analysing	O
a	O
set	O
of	O
n	O
documents	O
where	O
each	O
document	O
dn	O
n	O
n	O
is	O
represented	O
by	O
a	O
vector	O
xn	O
xn	O
d	O
might	O
count	O
how	O
many	O
times	O
the	O
word	O
cat	O
appears	O
of	O
word	O
occurrences	O
for	O
example	O
the	O
first	O
element	O
xn	O
the	O
number	O
of	O
occurrences	O
of	O
dog	O
etc	O
this	O
bag	B
of	I
words	I
is	O
formed	O
by	O
first	O
choosing	O
in	O
document	O
n	O
xn	O
is	O
the	O
normalised	O
number	O
of	O
occurrences	O
of	O
the	O
a	O
dictionary	O
of	O
d	O
words	O
the	O
vector	O
element	O
xn	O
i	O
word	O
i	O
in	O
the	O
document	O
n	O
typically	O
d	O
will	O
be	O
large	O
of	O
the	O
order	O
and	O
x	O
will	O
be	O
very	O
sparse	B
since	O
any	O
document	O
contains	O
only	O
a	O
small	O
fraction	O
of	O
the	O
available	O
words	O
in	O
the	O
dictionary	O
given	O
a	O
set	O
of	O
documents	O
d	O
the	O
aim	O
in	O
lsa	O
is	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
of	O
each	O
document	O
the	O
whole	O
document	O
database	O
is	O
represented	O
by	O
the	O
so-called	O
term-document	B
matrix	B
x	O
which	O
has	O
dimension	O
d	O
n	O
see	O
for	O
example	O
in	O
this	O
small	O
example	O
the	O
term-document	B
matrix	B
is	O
short	O
and	O
fat	O
whereas	O
in	O
practice	O
most	O
often	O
the	O
matrix	B
will	O
be	O
tall	O
and	O
thin	B
example	O
topic	O
we	O
have	O
a	O
small	O
dictionary	O
containing	O
the	O
words	O
influenza	O
flu	O
headache	O
nose	O
temperature	O
bed	O
cat	O
tree	B
car	O
foot	O
the	O
database	O
contains	O
a	O
large	O
number	O
of	O
articles	O
that	O
discuss	O
ailments	O
and	O
articles	O
which	O
seem	O
to	O
talk	O
about	O
the	O
effects	O
of	O
influenza	O
in	O
addition	O
to	O
some	O
background	O
documents	O
that	O
are	O
not	O
specific	O
to	O
ailments	O
some	O
of	O
the	O
more	O
formal	O
documents	O
exclusively	O
use	O
the	O
term	O
influenza	O
whereas	O
the	O
other	O
more	O
tabloid	O
documents	O
use	O
the	O
informal	O
term	O
flu	O
each	O
document	O
is	O
represented	O
by	O
a	O
simple	O
bag-of-words	O
style	O
description	O
namely	O
a	O
vector	O
in	O
which	O
element	O
i	O
of	O
that	O
vector	O
is	O
set	O
to	O
if	O
word	O
i	O
occurs	O
in	O
the	O
document	O
and	O
otherwise	O
the	O
data	O
is	O
represented	O
in	O
the	O
data	O
is	O
generated	O
using	O
the	O
artificial	O
mechanism	O
described	O
in	O
demolsi	O
m	O
the	O
result	O
of	O
using	O
pca	B
on	O
this	O
data	O
is	O
represented	O
in	O
where	O
we	O
plot	O
the	O
eigenvectors	O
scaled	O
by	O
their	O
eigenvalue	O
the	O
first	O
eigenvector	O
groups	O
all	O
the	O
influenza	O
words	O
together	O
and	O
the	O
second	O
deals	O
with	O
the	O
different	O
usage	O
of	O
the	O
terms	O
influenza	O
and	O
flu	O
rescaling	O
in	O
lsa	O
it	O
is	O
common	O
to	O
scale	O
the	O
transformation	O
so	O
that	O
the	O
projected	O
vectors	O
have	O
approximately	O
unit	O
covariance	B
centred	O
data	O
using	O
the	O
covariance	B
of	O
the	O
projections	O
is	O
obtained	O
from	O
y	O
n	O
m	O
ut	O
m	O
x	O
n	O
n	O
n	O
yn	O
d	O
m	O
ut	O
m	O
um	O
d	O
m	O
d	O
m	O
ut	O
m	O
d	O
m	O
i	O
xn	O
xxt	O
generally	O
one	O
can	O
consider	O
term-counts	O
in	O
which	O
terms	O
can	O
can	O
be	O
single	O
words	O
or	O
sets	O
of	O
words	O
or	O
even	O
sub-words	O
draft	O
march	O
latent	B
semantic	I
analysis	B
figure	O
hinton	O
diagram	O
of	O
the	O
eigenvector	O
matrix	B
e	O
where	O
each	O
eigenvector	O
column	O
is	O
scaled	O
by	O
the	O
corresponding	O
eigenvalue	O
red	O
indicates	O
positive	O
and	O
green	O
negative	O
area	O
of	O
each	O
square	O
corresponds	O
to	O
the	O
magnitude	O
showing	O
that	O
there	O
are	O
only	O
a	O
few	O
large	O
eigenvalues	O
note	O
that	O
the	O
overall	O
sign	O
of	O
any	O
eigenvector	O
is	O
irrelevant	O
the	O
first	O
eigenvector	O
corresponds	O
to	O
a	O
topic	O
in	O
which	O
the	O
words	O
influenza	O
flu	O
headache	O
nose	O
temperature	O
bed	O
are	O
prevalent	O
the	O
second	O
eigenvector	O
denotes	O
that	O
there	O
is	O
negative	O
correlation	O
between	O
the	O
occurrence	O
of	O
influenza	O
and	O
flu	O
given	O
y	O
the	O
approximate	B
reconstruction	O
x	O
is	O
the	O
euclidean	O
distance	O
between	O
two	O
points	O
xa	O
and	O
xb	O
is	O
then	O
approximately	O
ya	O
m	O
ya	O
ya	O
n	O
x	O
um	O
dm	O
y	O
n	O
xa	O
d	O
ya	O
n	O
dm	O
ut	O
m	O
um	O
dm	O
it	O
is	O
common	O
to	O
ignore	O
the	O
the	O
projected	O
space	O
just	O
to	O
be	O
the	O
euclidean	O
distance	O
between	O
the	O
y	O
vectors	O
m	O
term	O
factor	B
and	O
consider	O
a	O
measure	O
of	O
dissimilarity	O
in	O
lsa	O
for	O
information	B
retrieval	I
consider	O
a	O
large	O
collection	O
of	O
documents	O
from	O
the	O
web	O
creating	O
a	O
database	O
d	O
our	O
interest	O
it	O
to	O
find	O
the	O
most	O
similar	O
document	O
to	O
a	O
specified	O
query	B
document	O
using	O
a	O
bag-of-words	O
style	O
representation	O
for	O
document	O
n	O
xn	O
and	O
similarly	O
for	O
the	O
query	B
document	O
x	O
we	O
address	O
this	O
task	O
by	O
first	O
defining	O
a	O
measure	O
of	O
dissimilarity	O
between	O
documents	O
for	O
example	O
dxn	O
xm	O
xmt	O
xm	O
one	O
then	O
searches	O
for	O
the	O
document	O
that	O
minimises	O
this	O
dissimilarity	O
nopt	O
argmin	O
n	O
dxn	O
x	O
and	O
returns	O
document	O
xnopt	O
as	O
the	O
result	O
of	O
the	O
search	O
query	B
a	O
difficulty	O
with	O
this	O
approach	B
is	O
that	O
the	O
bag-of-words	O
representation	O
will	O
have	O
mostly	O
zeros	O
be	O
very	O
sparse	B
hence	O
differences	O
may	O
be	O
due	O
to	O
noise	O
rather	O
than	O
any	O
real	O
similarity	O
between	O
the	O
query	B
and	O
database	O
document	O
lsa	O
alleviates	O
this	O
problem	B
by	O
using	O
a	O
lower	O
dimensional	O
representation	O
y	O
of	O
the	O
high-dimensional	O
x	O
the	O
y	O
capture	O
the	O
main	O
variations	O
in	O
the	O
data	O
and	O
are	O
less	O
sensitive	O
to	O
random	O
uncorrelated	O
noise	O
using	O
the	O
dissimilarity	O
defined	O
in	O
terms	O
of	O
the	O
lower	O
dimensional	O
y	O
is	O
therefore	O
more	O
robust	O
and	O
likely	O
to	O
retrieve	O
more	O
useful	O
documents	O
the	O
squared	O
difference	O
between	O
two	O
documents	O
can	O
also	O
be	O
written	O
xtx	O
if	O
as	O
is	O
commonly	O
done	O
the	O
bag-of-words	O
representations	O
are	O
scaled	O
to	O
have	O
unit	O
length	O
x	O
x	O
xtx	O
so	O
that	O
xt	O
x	O
the	O
distance	O
is	O
x	O
x	O
xt	O
draft	O
march	O
pca	B
with	O
missing	B
data	I
figure	O
two	O
bag-of-word	O
vectors	O
the	O
euclidean	O
distance	O
between	O
the	O
two	O
is	O
large	O
normalised	O
vectors	O
the	O
euclidean	O
distance	O
is	O
now	O
related	O
directly	O
to	O
the	O
angle	O
between	O
the	O
vectors	O
in	O
this	O
case	O
two	O
documents	O
which	O
have	O
the	O
same	O
relative	O
frequency	O
of	O
words	O
will	O
both	O
have	O
the	O
same	O
dissimilarly	O
even	O
though	O
the	O
number	O
of	O
occurrences	O
of	O
the	O
words	O
is	O
different	O
figure	O
top	O
original	O
data	O
matrix	B
x	O
black	O
is	O
missing	B
white	O
present	O
the	O
data	O
is	O
constructed	O
from	O
a	O
set	O
of	O
only	O
basis	O
vectors	O
middle	O
x	O
with	O
missing	B
data	I
sparsity	O
bottom	O
reconstruction	O
found	O
using	O
svdm	O
m	O
svd	B
for	O
missing	B
data	I
this	O
problem	B
is	O
essentially	O
easy	O
since	O
despite	O
there	O
being	O
many	O
missing	B
elements	O
the	O
data	O
is	O
indeed	O
constructed	O
from	O
a	O
model	B
for	O
which	O
svd	B
is	O
appropriate	O
such	O
techniques	O
have	O
application	O
in	O
collaborative	B
filtering	I
and	O
recommender	O
systems	O
where	O
one	O
wishes	O
to	O
fill	O
in	O
missing	B
values	O
in	O
a	O
matrix	B
and	O
one	O
may	O
equivalently	O
consider	O
the	O
cosine	B
similarity	I
s	O
x	O
xt	O
cos	O
where	O
is	O
the	O
angle	O
between	O
the	O
unit	O
vectors	O
x	O
and	O
pca	B
is	O
arguably	O
suboptimal	O
for	O
document	O
analysis	B
since	O
we	O
would	O
expect	O
the	O
presence	O
of	O
a	O
latent	B
topic	I
to	O
contribute	O
only	O
positive	O
counts	O
to	O
the	O
data	O
a	O
related	O
version	O
of	O
pca	B
in	O
which	O
the	O
decomposition	B
is	O
constrained	O
to	O
have	O
positive	O
elements	O
is	O
called	O
plsa	O
and	O
discussed	O
in	O
example	O
continuing	O
the	O
influenza	O
example	O
someone	O
who	O
uploads	O
a	O
query	B
document	O
which	O
uses	O
the	O
term	O
flu	O
might	O
also	O
be	O
interested	O
in	O
documents	O
about	O
influenza	O
however	O
the	O
search	O
query	B
term	O
flu	O
does	O
not	O
contain	O
the	O
word	O
influenza	O
so	O
how	O
can	O
one	O
retrieve	O
such	O
documents	O
since	O
the	O
first	O
component	O
using	O
pca	B
groups	O
all	O
influenza	O
terms	O
together	O
if	O
we	O
use	O
only	O
the	O
first	O
component	O
of	O
the	O
representation	O
y	O
to	O
compare	O
documents	O
this	O
will	O
retrieve	O
documents	O
independent	O
of	O
whether	O
the	O
term	O
flu	O
or	O
influenza	O
is	O
used	O
pca	B
with	O
missing	B
data	I
when	O
values	O
of	O
the	O
data	O
matrix	B
x	O
are	O
missing	B
the	O
standard	O
pca	B
algorithm	B
as	O
described	O
cannot	O
be	O
implemented	O
unfortunately	O
there	O
is	O
no	O
quick	O
fix	O
pca	B
solution	O
when	O
some	O
of	O
the	O
xn	O
i	O
are	O
missing	B
and	O
more	O
complex	O
numerical	B
procedures	O
need	O
to	O
invoked	O
a	O
naive	O
approach	B
in	O
this	O
case	O
is	O
to	O
require	O
the	O
draft	O
march	O
squared	O
reconstruction	O
error	O
to	O
be	O
small	O
only	O
for	O
the	O
existing	O
elements	O
of	O
x	O
that	O
is	O
xn	O
j	O
eb	O
y	O
n	O
i	O
i	O
j	O
bj	O
yn	O
i	O
pca	B
with	O
missing	B
data	I
where	O
n	O
we	O
find	O
that	O
the	O
optimal	O
weights	O
satisfy	O
btb	O
i	O
i	O
if	O
the	O
ith	O
entry	O
of	O
the	O
nth	O
vector	O
is	O
available	O
and	O
is	O
zero	O
otherwise	O
differentiating	O
as	O
before	O
n	O
i	O
xn	O
i	O
bk	O
i	O
n	O
yk	O
i	O
one	O
then	O
substitutes	O
this	O
expression	O
into	O
the	O
squared	O
error	O
and	O
minimises	O
the	O
error	O
with	O
respect	O
to	O
b	O
under	O
the	O
orthonormality	O
constraint	O
an	O
alternative	O
iterative	O
optimisation	B
procedure	O
is	O
as	O
follows	O
first	O
select	O
a	O
random	O
d	O
m	O
matrix	B
b	O
then	O
iterate	O
until	O
convergence	O
the	O
following	O
two	O
steps	O
optimize	O
y	O
for	O
fixed	O
b	O
for	O
fixed	O
b	O
the	O
above	O
e	O
b	O
y	O
is	O
a	O
quadratic	O
function	B
of	O
the	O
matrix	B
y	O
which	O
can	O
be	O
optimised	O
directly	O
by	O
differentiating	O
and	O
equating	O
to	O
zero	O
one	O
obtains	O
the	O
fixed	O
point	O
condition	O
j	O
i	O
yn	O
j	O
bj	O
i	O
l	O
n	O
i	O
xn	O
bl	O
i	O
xn	O
i	O
yn	O
l	O
bk	O
i	O
e	O
b	O
y	O
n	O
i	O
i	O
yl	O
n	O
l	O
i	O
kl	O
bl	O
i	O
bk	O
i	O
n	O
i	O
i	O
n	O
i	O
xn	O
i	O
bk	O
i	O
in	O
matrix	B
notation	O
we	O
then	O
have	O
a	O
set	O
of	O
linear	B
systems	O
n	O
n	O
cn	O
mnyn	O
one	O
may	O
solve	O
each	O
linear	B
system	O
using	O
gaussian	B
elimination	O
can	O
avoid	O
explicit	O
matrix	B
inversion	B
by	O
using	O
the	O
operator	O
in	O
matlab	O
it	O
can	O
be	O
that	O
one	O
or	O
more	O
of	O
the	O
above	O
linear	B
systems	O
is	O
underdetermined	O
this	O
can	O
occur	O
when	O
there	O
are	O
less	O
observed	O
values	O
in	O
the	O
nth	O
data	O
column	O
of	O
x	O
than	O
there	O
are	O
components	O
m	O
in	O
this	O
case	O
one	O
may	O
use	O
the	O
pseudo-inverse	O
to	O
provide	O
a	O
minimal	O
length	O
solution	O
optimize	O
b	O
for	O
fixed	O
y	O
one	O
now	O
freezes	O
y	O
and	O
considers	O
the	O
function	B
xn	O
j	O
n	O
i	O
i	O
j	O
bj	O
yn	O
i	O
eb	O
y	O
for	O
fixed	O
y	O
the	O
above	O
expression	O
is	O
quadratic	O
in	O
the	O
matrix	B
b	O
which	O
can	O
again	O
be	O
optimised	O
using	O
linear	B
algebra	I
this	O
corresponds	O
to	O
solving	B
a	O
set	O
of	O
linear	B
systems	O
for	O
the	O
ith	O
row	O
of	O
b	O
mi	O
fibi	O
mathematically	O
this	O
is	O
bi	O
fi	O
n	O
i	O
xn	O
i	O
yn	O
k	O
n	O
k	O
kj	O
n	O
n	O
i	O
yn	O
j	O
yn	O
k	O
in	O
this	O
manner	O
one	O
is	O
guaranteed	O
to	O
iteratively	O
decrease	O
the	O
value	B
of	O
the	O
squared	O
error	O
loss	O
until	O
a	O
minimum	O
is	O
reached	O
this	O
technique	O
is	O
implemented	O
in	O
svdm	O
m	O
note	O
that	O
efficient	O
techniques	O
based	O
on	O
updating	O
the	O
solution	O
as	O
a	O
new	O
column	O
of	O
x	O
arrives	O
one	O
at	O
a	O
time	O
online	B
updating	O
are	O
available	O
see	O
for	O
example	O
draft	O
march	O
pca	B
with	O
missing	B
data	I
finding	O
the	O
principal	B
directions	I
for	O
the	O
missing	B
data	I
case	O
the	O
basis	O
b	O
found	O
using	O
the	O
above	O
technique	O
is	O
based	O
only	O
on	O
minimising	O
the	O
squared	O
reconstruction	O
error	O
and	O
therefore	O
does	O
not	O
necessarily	O
satisfy	O
the	O
maximal	O
variance	B
principal	B
directions	I
criterion	O
namely	O
that	O
the	O
columns	O
of	O
b	O
point	O
along	O
the	O
eigen-directions	O
for	O
a	O
given	O
b	O
y	O
with	O
approximate	B
decomposition	B
x	O
by	O
we	O
can	O
return	O
a	O
new	O
orthonormal	B
basis	O
u	O
by	O
performing	O
svd	B
on	O
the	O
completed	O
data	O
by	O
usvt	O
to	O
return	O
an	O
orthonormal	B
basis	O
b	O
u	O
in	O
general	O
however	O
this	O
is	O
potentially	O
computationally	O
expensive	O
if	O
only	O
the	O
principal	B
directions	I
are	O
required	O
an	O
alternative	O
is	O
to	O
explicitly	O
transform	O
the	O
solution	O
b	O
using	O
an	O
invertible	O
matrix	B
q	O
x	O
bqq	O
calling	O
the	O
new	O
basis	O
b	O
bq	O
for	O
a	O
solution	O
to	O
be	O
aligned	O
with	O
the	O
principal	B
directions	I
we	O
need	O
bt	O
b	O
i	O
in	O
other	O
words	O
qtbtbq	O
i	O
forming	O
the	O
svd	B
of	O
b	O
b	O
udvt	O
and	O
substituting	O
in	O
equation	B
we	O
have	O
the	O
requirement	O
qtvdutudvtq	O
i	O
since	O
utu	O
i	O
and	O
vtv	O
vvt	O
i	O
we	O
can	O
use	O
q	O
vtd	O
hence	O
given	O
a	O
solution	O
b	O
we	O
can	O
find	O
the	O
principal	B
directions	I
from	O
the	O
svd	B
of	O
b	O
using	O
b	O
utd	O
if	O
the	O
d	O
m	O
matrix	B
b	O
is	O
non-square	O
m	O
d	O
then	O
the	O
matrix	B
d	O
will	O
be	O
non-square	O
and	O
non-invertible	O
to	O
make	O
the	O
above	O
well	O
defined	O
one	O
may	O
append	O
d	O
with	O
the	O
columns	O
of	O
the	O
identity	O
im	O
id	O
where	O
ik	O
is	O
the	O
kth	O
column	O
of	O
the	O
identity	B
matrix	B
and	O
use	O
in	O
place	O
of	O
d	O
above	O
collaborative	B
filtering	I
using	O
pca	B
with	O
missing	B
data	I
entry	O
in	O
the	O
vector	O
x	O
specifies	O
the	O
rating	O
the	O
user	O
gives	O
to	O
the	O
ith	O
film	O
the	O
matrix	B
x	O
for	O
all	O
a	O
database	O
contains	O
a	O
set	O
of	O
vectors	O
each	O
describing	O
the	O
film	O
ratings	O
for	O
a	O
user	O
in	O
the	O
database	O
the	O
ith	O
the	O
n	O
users	O
has	O
many	O
missing	B
values	O
since	O
any	O
single	O
user	O
will	O
only	O
have	O
given	O
a	O
rating	O
for	O
a	O
small	O
selection	O
of	O
the	O
possible	O
d	O
films	O
in	O
a	O
practical	O
example	O
one	O
might	O
have	O
d	O
films	O
and	O
n	O
users	O
for	O
any	O
user	O
n	O
the	O
task	O
is	O
to	O
predict	O
reasonable	O
values	O
for	O
the	O
missing	B
entries	O
of	O
their	O
rating	O
vector	O
xn	O
thereby	O
providing	O
a	O
suggestion	O
as	O
to	O
which	O
films	O
they	O
might	O
like	O
to	O
view	O
viewed	O
as	O
a	O
missing	B
data	I
problem	B
one	O
can	O
fit	O
b	O
and	O
y	O
using	O
svdm	O
m	O
as	O
above	O
given	O
b	O
and	O
y	O
we	O
can	O
form	O
a	O
reconstruction	O
on	O
all	O
the	O
entries	O
of	O
x	O
by	O
using	O
x	O
by	O
giving	O
therefore	O
a	O
prediction	O
for	O
the	O
missing	B
values	O
draft	O
march	O
matrix	B
decomposition	B
methods	O
figure	O
under-complete	B
representation	I
there	O
are	O
too	O
few	O
basis	O
over-complete	B
representation	I
vectors	O
to	O
represent	O
the	O
datapoints	O
there	O
are	O
too	O
many	O
basis	O
vectors	O
to	O
form	O
a	O
unique	O
representation	O
of	O
a	O
datapoint	O
in	O
terms	O
of	O
a	O
linear	B
combination	O
of	O
the	O
basis	O
vectors	O
z	O
x	O
y	O
x	O
y	O
z	O
figure	O
joint	B
plsa	O
conditional	B
plsa	O
whilst	O
written	O
as	O
a	O
graphical	O
model	B
some	O
care	O
is	O
required	O
in	O
the	O
interpretation	O
see	O
text	O
matrix	B
decomposition	B
methods	O
given	O
a	O
data	O
matrix	B
x	O
for	O
which	O
each	O
column	O
represents	O
a	O
datapoint	O
an	O
approximate	B
matrix	B
decomposition	B
is	O
of	O
the	O
form	O
x	O
by	O
into	O
a	O
basis	O
matrix	B
b	O
and	O
weight	B
coordinate	O
matrix	B
y	O
symbolically	O
matrix	B
decompositions	O
are	O
of	O
the	O
form	O
x	O
data	O
d	O
n	O
b	O
basis	O
d	O
m	O
y	O
weightscomponents	O
m	O
n	O
in	O
this	O
section	O
we	O
will	O
consider	O
some	O
common	O
matrix	B
decomposition	B
methods	O
under-complete	B
decompositions	O
when	O
m	O
d	O
there	O
are	O
fewer	O
basis	O
vectors	O
than	O
dimensions	O
the	O
matrix	B
b	O
is	O
then	O
called	O
tall	O
or	O
thin	B
in	O
this	O
case	O
the	O
matrix	B
y	O
forms	O
a	O
lower	O
dimensional	O
approximate	B
representation	O
of	O
the	O
data	O
x	O
pca	B
being	O
a	O
classic	O
example	O
over-complete	B
decompositions	O
for	O
m	O
d	O
the	O
basis	O
is	O
over-complete	B
there	O
being	O
more	O
basis	O
vectors	O
than	O
dimensions	O
in	O
such	O
cases	O
additional	O
constraints	O
are	O
placed	O
on	O
either	O
the	O
basis	O
or	O
components	O
for	O
example	O
one	O
might	O
require	O
that	O
only	O
a	O
small	O
number	O
of	O
the	O
large	O
number	O
of	O
available	O
basis	O
vectors	O
is	O
used	O
to	O
form	O
the	O
representation	O
for	O
any	O
given	O
x	O
such	O
sparse-representations	O
are	O
common	O
in	O
theoretical	O
neurobiology	O
where	O
issues	O
of	O
energy	B
efficiency	O
rapidity	O
of	O
processing	O
and	O
robustness	O
are	O
of	O
probabilistic	B
latent	B
semantic	I
analysis	B
consider	O
two	O
objects	O
x	O
and	O
y	O
where	O
domx	O
i	O
and	O
domy	O
j	O
we	O
have	O
a	O
count	O
matrix	B
with	O
elements	O
cij	O
which	O
describes	O
the	O
number	O
of	O
times	O
that	O
x	O
i	O
y	O
j	O
was	O
observed	O
we	O
can	O
transform	O
this	O
count	O
matrix	B
into	O
a	O
frequency	O
matrix	B
p	O
with	O
elements	O
px	O
i	O
y	O
j	O
px	O
i	O
y	O
j	O
ij	O
cij	O
xij	O
our	O
interest	O
is	O
to	O
find	O
a	O
decomposition	B
of	O
this	O
frequency	O
matrix	B
of	O
the	O
form	O
in	O
k	O
px	O
iz	O
k	O
py	O
jz	O
k	O
pz	O
k	O
bik	O
ykj	O
px	O
i	O
y	O
j	O
draft	O
march	O
matrix	B
decomposition	B
methods	O
which	O
is	O
a	O
form	O
of	O
matrix	B
decomposition	B
into	O
basis	O
b	O
and	O
coordinates	O
y	O
this	O
has	O
the	O
interpretation	O
of	O
discovering	O
latent	B
topics	O
z	O
that	O
describe	O
the	O
joint	B
behaviour	O
of	O
x	O
and	O
y	O
an	O
em	B
style	O
training	B
algorithm	B
in	O
order	O
to	O
find	O
the	O
approximate	B
decomposition	B
we	O
first	O
need	O
a	O
measure	O
of	O
difference	O
between	O
the	O
matrix	B
with	O
elements	O
pij	O
and	O
the	O
approximation	B
with	O
elements	O
pij	O
since	O
all	O
elements	O
are	O
bounded	O
between	O
and	O
and	O
sum	O
to	O
we	O
may	O
interpret	O
p	O
as	O
a	O
joint	B
probability	O
and	O
p	O
as	O
an	O
approximation	B
to	O
this	O
for	O
probabilities	O
a	O
useful	O
measure	O
of	O
discrepancy	O
is	O
the	O
kullback-leibler	B
divergence	B
since	O
p	O
is	O
fixed	O
minimising	O
the	O
kullback-leibler	B
divergence	B
with	O
respect	O
to	O
the	O
approximation	B
p	O
is	O
equivalent	B
to	O
maximising	O
the	O
likelihood	B
term	O
this	O
is	O
px	O
y	O
log	O
px	O
y	O
klp	O
p	O
klqzx	O
y	O
pzx	O
y	O
xy	O
z	O
it	O
s	O
convenient	O
to	O
derive	O
an	O
em	B
style	O
algorithm	B
to	O
learn	O
pxz	O
pyz	O
and	O
pz	O
to	O
do	O
this	O
consider	O
qzx	O
y	O
log	O
qzx	O
y	O
qzx	O
y	O
log	O
pzx	O
y	O
z	O
z	O
implies	O
summation	O
over	O
all	O
states	O
of	O
the	O
variable	O
z	O
rearranging	O
this	O
gives	O
the	O
bound	B
qzx	O
y	O
log	O
pz	O
x	O
y	O
log	O
px	O
y	O
plugging	O
this	O
into	O
the	O
likelihood	B
term	O
above	O
we	O
have	O
the	O
bound	B
px	O
y	O
log	O
px	O
y	O
xy	O
qzx	O
y	O
log	O
qzx	O
y	O
z	O
qzx	O
y	O
log	O
qzx	O
y	O
px	O
px	O
xy	O
z	O
z	O
xy	O
z	O
m-step	B
for	O
fixed	O
pxz	O
pyz	O
the	O
contribution	O
to	O
the	O
bound	B
from	O
pz	O
is	O
qzx	O
y	O
pxz	O
log	O
pyz	O
log	O
pz	O
similarly	O
for	O
fixed	O
pyz	O
pz	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
the	O
optimal	O
setting	O
of	O
pz	O
is	O
xy	O
qzx	O
ypx	O
y	O
pz	O
qzx	O
y	O
log	O
pz	O
qzx	O
ypx	O
y	O
xy	O
px	O
pz	O
xy	O
z	O
xy	O
z	O
px	O
y	O
since	O
equation	B
is	O
up	O
to	O
a	O
constant	O
kl	O
the	O
contribution	O
to	O
the	O
bound	B
from	O
pxz	O
is	O
qzx	O
y	O
log	O
pyz	O
therefore	O
optimally	O
pxz	O
px	O
yqzx	O
y	O
and	O
similarly	O
pyz	O
px	O
yqzx	O
y	O
x	O
draft	O
march	O
matrix	B
decomposition	B
methods	O
the	O
images	O
figure	O
conditional	B
plsa	O
reconstruction	O
of	O
in	O
using	O
a	O
positive	O
combination	O
of	O
the	O
positive	O
base	O
images	O
in	O
the	O
root	O
mean	B
square	O
reconstruction	O
error	O
is	O
the	O
base	O
images	O
tend	O
to	O
be	O
more	O
localised	O
than	O
the	O
corresponding	O
eigen-images	O
here	O
one	O
sees	O
local	B
structure	B
such	O
as	O
foreheads	O
chins	O
etc	O
e-step	B
the	O
optimal	O
setting	O
for	O
the	O
q	O
distribution	B
at	O
each	O
iteration	B
is	O
qzx	O
y	O
pzx	O
y	O
which	O
is	O
fixed	O
throughout	O
the	O
m-step	B
the	O
procedure	O
is	O
given	O
in	O
and	O
a	O
demonstration	O
is	O
in	O
demoplsa	O
m	O
the	O
likelihood	B
equation	B
is	O
guaranteed	O
to	O
increase	O
the	O
kullback-leibler	B
divergence	B
equation	B
decrease	O
under	O
iterating	O
between	O
the	O
e	O
and	O
m-steps	O
since	O
the	O
method	O
is	O
analogous	O
to	O
an	O
em	B
procedure	O
generalisations	O
such	O
as	O
using	O
simpler	O
q	O
distributions	O
to	O
generalised	B
em	B
procedures	O
are	O
immediate	O
based	O
on	O
modifying	O
the	O
above	O
derivation	O
a	O
related	O
probabilistic	B
model	B
for	O
variables	O
x	O
y	O
z	O
with	O
z	O
hidden	B
and	O
domx	O
i	O
domy	O
j	O
domz	O
k	O
consider	O
a	O
distribution	B
px	O
y	O
z	O
pxz	O
pyz	O
pz	O
and	O
data	O
d	O
yn	O
n	O
n	O
assuming	O
the	O
data	O
are	O
i	O
i	O
d	O
draws	O
from	O
equation	B
the	O
log	O
likelihood	B
is	O
log	O
pd	O
pxn	O
yn	O
where	O
log	O
pxn	O
yn	O
pxnzn	O
pynzn	O
pzn	O
zn	O
if	O
xn	O
yn	O
are	O
sampled	O
from	O
a	O
distribution	B
px	O
y	O
then	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
of	O
samples	O
n	O
equation	B
becomes	O
log	O
pd	O
px	O
which	O
is	O
equation	B
from	O
this	O
viewpoint	O
even	O
though	O
we	O
started	O
out	O
with	O
a	O
set	O
of	O
samples	O
in	O
the	O
limit	O
only	O
the	O
distribution	B
of	O
the	O
observed	O
data	O
px	O
y	O
is	O
relevant	O
the	O
generic	O
code	O
for	O
the	O
finite	O
sample	O
case	O
trained	O
with	O
em	B
is	O
given	O
in	O
demomultinomialpxygz	O
m	O
see	O
also	O
a	O
fully	O
probabilistic	B
interpretation	O
of	O
plsa	O
can	O
be	O
made	O
via	O
poisson	B
a	O
related	O
probabilistic	B
model	B
is	O
latent	B
dirichlet	B
allocation	I
which	O
is	O
described	O
in	O
draft	O
march	O
matrix	B
decomposition	B
methods	O
algorithm	B
plsa	O
given	O
a	O
frequency	O
matrix	B
px	O
i	O
y	O
j	O
return	O
a	O
k	O
py	O
jz	O
k	O
pz	O
k	O
see	O
plsa	O
m	O
initialise	O
pz	O
pxz	O
pyz	O
while	O
not	O
converged	O
do	O
set	O
qzx	O
y	O
pzx	O
y	O
set	O
pxz	O
set	O
pyz	O
end	O
while	O
y	O
px	O
yqzx	O
y	O
x	O
px	O
yqzx	O
y	O
xy	O
px	O
yqzx	O
y	O
k	O
px	O
iz	O
e-step	B
m-steps	O
set	O
pz	O
algorithm	B
conditional	B
plsa	O
given	O
a	O
frequency	O
matrix	B
px	O
iy	O
j	O
return	O
a	O
decomposition	B
k	O
px	O
iz	O
k	O
pz	O
ky	O
j	O
see	O
plsacond	O
m	O
initialise	O
pxz	O
pzy	O
while	O
not	O
converged	O
do	O
y	O
pxyqzx	O
y	O
x	O
pxyqzx	O
y	O
end	O
while	O
set	O
qzx	O
y	O
pzx	O
y	O
set	O
pxz	O
set	O
pzy	O
e-step	B
m-steps	O
conditional	B
plsa	O
in	O
some	O
cases	O
it	O
is	O
more	O
natural	B
to	O
consider	O
a	O
conditional	B
frequency	O
matrix	B
px	O
iy	O
j	O
px	O
iy	O
j	O
and	O
seek	O
an	O
approximate	B
decomposition	B
px	O
iz	O
k	O
k	O
xij	O
bik	O
pz	O
ky	O
j	O
ykj	O
as	O
depicted	O
in	O
deriving	O
an	O
em	B
style	O
algorithm	B
for	O
this	O
is	O
straightforward	O
and	O
is	O
presented	O
in	O
being	O
equivalent	B
to	O
the	O
non-negative	B
matrix	B
factorisation	I
algorithm	B
of	O
is	O
unity	O
example	O
the	O
basis	O
a	O
set	O
of	O
images	O
is	O
give	O
in	O
these	O
were	O
created	O
by	O
first	O
defining	O
base	O
images	O
each	O
base	O
image	O
is	O
positive	O
and	O
scaled	O
so	O
that	O
the	O
sum	O
of	O
the	O
pixels	O
i	O
px	O
iz	O
k	O
where	O
k	O
and	O
x	O
indexes	O
the	O
pixels	O
see	O
we	O
then	O
sum	O
each	O
of	O
these	O
images	O
using	O
a	O
randomly	O
chosen	O
positive	O
set	O
of	O
weights	O
the	O
constraint	O
that	O
the	O
weights	O
sum	O
to	O
to	O
generate	O
a	O
training	B
image	O
with	O
elements	O
px	O
iy	O
j	O
and	O
j	O
indexes	O
the	O
training	B
image	O
this	O
is	O
repeated	O
times	O
to	O
form	O
the	O
full	O
training	B
set	O
the	O
task	O
is	O
given	O
only	O
the	O
training	B
set	O
images	O
to	O
reconstruct	O
the	O
basis	O
from	O
which	O
the	O
images	O
were	O
formed	O
we	O
assume	O
that	O
we	O
know	O
the	O
correct	O
number	O
of	O
base	O
images	O
namely	O
the	O
results	O
of	O
using	O
conditional	B
plsa	O
on	O
this	O
task	O
are	O
presented	O
in	O
and	O
using	O
svd	B
in	O
in	O
this	O
case	O
plsa	O
finds	O
the	O
correct	O
natural	B
basis	O
corresponding	O
to	O
the	O
way	O
the	O
images	O
were	O
generated	O
the	O
eigenbasis	O
is	O
just	O
as	O
good	O
in	O
terms	O
of	O
being	O
able	O
to	O
represent	O
any	O
of	O
the	O
training	B
images	O
but	O
in	O
this	O
case	O
does	O
not	O
correspond	O
to	O
the	O
constraints	O
under	O
which	O
the	O
data	O
was	O
generated	O
extensions	O
and	O
variations	O
non-negative	B
matrix	B
factorisation	I
non-negative	B
matrix	B
factorisation	I
considers	O
a	O
decomposition	B
in	O
which	O
both	O
the	O
basis	O
and	O
weight	B
matrices	O
have	O
non-negative	O
entries	O
an	O
early	O
example	O
of	O
this	O
work	O
is	O
as	O
a	O
form	O
of	O
constrained	O
factor	B
draft	O
march	O
matrix	B
decomposition	B
methods	O
figure	O
training	B
data	O
consisting	O
of	O
a	O
positive	O
combination	O
of	O
the	O
base	O
images	O
basis	O
learned	O
using	O
conditional	B
the	O
chosen	O
base	O
images	O
from	O
which	O
the	O
training	B
data	O
is	O
derived	O
eigenbasis	O
plsa	O
on	O
the	O
training	B
data	O
this	O
is	O
virtually	O
indistinguishable	O
from	O
the	O
true	O
basis	O
called	O
eigenfaces	O
closely	O
related	O
works	O
are	O
which	O
is	O
a	O
generalisation	B
of	O
plsa	O
there	O
is	O
no	O
requirement	O
that	O
the	O
basis	O
or	O
components	O
sum	O
to	O
unity	O
in	O
all	O
cases	O
em-style	O
training	B
algorithms	O
exist	O
although	O
their	O
convergence	O
can	O
be	O
slow	O
a	O
natural	B
relaxation	O
is	O
when	O
only	O
one	O
of	O
the	O
factors	O
in	O
the	O
decomposition	B
is	O
constrained	O
to	O
be	O
non-negative	O
we	O
will	O
encounter	O
similar	O
models	O
in	O
the	O
discussion	O
on	O
independent	O
component	O
analysis	B
gradient	B
based	O
training	B
em	B
style	O
algorithms	O
are	O
easy	O
to	O
derive	O
and	O
implement	O
but	O
can	O
exhibit	O
poor	O
convergence	O
gradient	B
based	O
methods	O
to	O
simultaneously	O
optimize	O
with	O
respect	O
to	O
the	O
basis	O
and	O
the	O
components	O
have	O
been	O
developed	O
but	O
require	O
a	O
parameterisation	B
that	O
ensures	O
positivity	O
of	O
the	O
array	O
decompositions	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
method	O
to	O
the	O
decomposition	B
of	O
multidimensional	O
arrays	O
based	O
also	O
on	O
more	O
than	O
one	O
basis	O
for	O
example	O
ps	O
tu	O
v	O
puw	O
pv	O
pw	O
vw	O
ps	O
t	O
u	O
ps	O
t	O
uv	O
w	O
pv	O
w	O
vw	O
such	O
extensions	O
require	O
only	O
additional	O
bookkeeping	O
applications	O
of	O
plsanmf	O
physical	O
models	O
non-negative	O
decompositions	O
can	O
arise	O
naturally	O
in	O
certain	O
physical	O
situations	O
for	O
example	O
in	O
acoustics	O
positive	O
amounts	O
of	O
energy	B
combine	O
linearly	O
from	O
different	O
signal	O
sources	O
to	O
form	O
the	O
observed	O
signal	O
let	O
s	O
imagine	O
that	O
two	O
kinds	O
of	O
signals	O
are	O
present	O
in	O
an	O
acoustic	O
signal	O
say	O
a	O
piano	O
and	O
a	O
singer	O
using	O
nmf	O
one	O
can	O
learn	O
two	O
separate	O
bases	O
for	O
these	O
cases	O
and	O
then	O
reconstruct	O
a	O
given	O
signal	O
using	O
only	O
one	O
of	O
the	O
bases	O
this	O
means	O
that	O
one	O
could	O
potentially	O
remove	O
the	O
singer	O
from	O
a	O
recording	O
leaving	O
only	O
the	O
piano	O
see	O
also	O
for	O
a	O
more	O
standard	O
probabilistic	B
model	B
in	O
acoustics	O
this	O
would	O
be	O
analogous	O
to	O
reconstructing	O
the	O
images	O
in	O
using	O
say	O
only	O
one	O
of	O
the	O
learned	O
basis	O
images	O
see	O
modelling	B
citations	O
we	O
have	O
a	O
collection	O
of	O
research	O
documents	O
which	O
cite	O
other	O
documents	O
for	O
example	O
document	O
might	O
cite	O
documents	O
etc	O
given	O
only	O
the	O
list	O
of	O
citations	O
for	O
each	O
document	O
can	O
we	O
identify	O
key	O
research	O
papers	O
and	O
the	O
communities	O
that	O
cite	O
them	O
note	O
that	O
this	O
is	O
not	O
the	O
same	O
question	O
as	O
finding	O
the	O
most	O
cited	O
documents	O
rather	O
we	O
want	O
to	O
identify	O
documents	O
with	O
communities	O
and	O
find	O
their	O
relevance	O
for	O
a	O
draft	O
march	O
matrix	B
decomposition	B
methods	O
learning	B
learning	B
to	O
predict	O
by	O
the	O
methods	O
of	O
temporal	O
differences	O
sutton	O
neuronlike	O
adaptive	O
elements	O
that	O
can	O
solve	O
difficult	O
learning	B
control	O
problems	O
barto	O
et	O
al	O
practical	O
issues	O
in	O
temporal	O
difference	O
learning	B
tesauro	O
learning	B
explanation-based	O
generalization	O
a	O
unifying	O
view	O
mitchell	O
et	O
al	O
learning	B
internal	O
representations	O
by	O
error	O
propagation	B
rumelhart	O
et	O
al	O
explanation-based	O
learning	B
an	O
alternative	O
view	O
dejong	O
et	O
al	O
networks	O
learning	B
internal	O
representations	O
by	O
error	O
propagation	B
rumelhart	O
et	O
al	O
neural	O
networks	O
and	O
the	O
bias-variance	O
dilemma	O
geman	O
et	O
al	O
the	O
cascade-correlation	O
learning	B
architecture	O
fahlman	O
et	O
al	O
classification	B
and	O
regression	B
trees	O
breiman	O
et	O
al	O
learnability	O
and	O
the	O
vapnik-chervonenkis	O
dimension	O
blumer	O
et	O
al	O
learning	B
quickly	O
when	O
irrelevant	O
attributes	O
abound	O
littlestone	O
reasoning	O
probabilistic	B
reasoning	O
in	O
intelligent	O
systems	O
networks	O
of	O
plausible	O
inference	B
pearl	O
factor	B
factor	B
factor	B
factor	B
factor	B
maximum	B
likelihood	B
from	O
incomplete	O
data	O
via	O
the	O
em	B
algorithm	B
dempster	O
et	O
al	O
factor	B
factor	B
local	B
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
lauritzen	O
et	O
al	O
algorithms	O
genetic	O
algorithms	O
in	O
search	O
optimization	O
and	O
machine	O
learning	B
goldberg	O
adaptation	O
in	O
natural	B
and	O
artificial	O
systems	O
holland	O
genetic	O
programming	O
on	O
the	O
programming	O
of	O
computers	O
by	O
means	O
of	O
natural	B
selection	O
koza	O
efficient	O
induction	O
of	O
logic	O
programs	O
muggleton	O
et	O
al	O
learning	B
logical	O
definitions	O
from	O
relations	O
quinlan	O
inductive	O
logic	O
programming	O
techniques	O
and	O
applications	O
lavrac	O
et	O
al	O
table	O
highest	O
ranked	O
documents	O
according	O
to	O
pcz	O
the	O
factor	B
topic	O
labels	O
are	O
manual	O
assignments	O
based	O
on	O
similarities	O
to	O
the	O
cora	O
topics	O
reproduced	O
from	O
community	O
we	O
use	O
the	O
variable	O
d	O
d	O
to	O
index	O
documents	O
and	O
c	O
d	O
to	O
index	O
citations	O
d	O
and	O
c	O
have	O
the	O
same	O
domain	B
namely	O
the	O
index	O
of	O
a	O
research	O
article	O
if	O
document	O
d	O
i	O
cites	O
article	O
c	O
j	O
then	O
we	O
set	O
the	O
entry	O
of	O
the	O
matrix	B
cij	O
if	O
there	O
is	O
no	O
citation	O
cij	O
is	O
set	O
to	O
zero	O
we	O
can	O
form	O
a	O
distribution	B
over	O
documents	O
and	O
citations	O
using	O
pd	O
i	O
c	O
j	O
ij	O
cij	O
example	O
citations	O
the	O
cora	O
corpus	O
mccallum	O
contains	O
an	O
archive	O
of	O
around	O
computer	O
science	O
research	O
papers	O
from	O
this	O
archive	O
the	O
authors	O
in	O
extracted	O
the	O
papers	O
in	O
the	O
machine	O
learning	B
category	O
consisting	O
of	O
documents	O
and	O
citations	O
using	O
these	O
the	O
distribution	B
equation	B
was	O
formed	O
the	O
documents	O
have	O
additionally	O
been	O
categorised	O
by	O
hand	O
into	O
topics	O
case-based	O
reasoning	O
genetic	O
algorithms	O
neural	O
networks	O
probabilistic	B
methods	O
reinforcement	B
learning	B
rule	O
learning	B
and	O
theory	O
in	O
the	O
joint	B
plsa	O
method	O
is	O
fitted	O
to	O
the	O
data	O
using	O
z	O
topics	O
from	O
the	O
trained	O
model	B
the	O
expression	O
pc	O
jz	O
k	O
defines	O
how	O
authoritative	O
paper	O
j	O
is	O
according	O
to	O
community	O
z	O
k	O
the	O
results	O
are	O
presented	O
in	O
and	O
show	O
how	O
the	O
method	O
discovers	O
intuitively	O
meaningful	O
topics	O
modelling	B
the	O
web	O
consider	O
a	O
collection	O
of	O
websites	O
indexed	O
by	O
i	O
if	O
website	B
j	O
points	O
to	O
website	B
i	O
one	O
sets	O
cij	O
giving	O
a	O
directed	B
graph	B
of	O
website-to-website	O
links	O
since	O
a	O
website	B
will	O
discuss	O
usually	O
only	O
of	O
a	O
small	O
number	O
of	O
draft	O
march	O
topics	O
we	O
might	O
be	O
able	O
to	O
explain	O
why	O
there	O
is	O
a	O
link	O
between	O
two	O
websites	O
using	O
a	O
plsa	O
decomposition	B
these	O
algorithms	O
have	O
proved	O
useful	O
for	O
internet	O
search	O
for	O
example	O
to	O
determine	O
the	O
latent	B
topics	O
of	O
websites	O
and	O
identify	O
the	O
most	O
authoritative	O
websites	O
see	O
for	O
a	O
discussion	O
kernel	B
pca	B
kernel	B
pca	B
kernel	B
pca	B
is	O
a	O
non-linear	B
extension	O
of	O
pca	B
designed	O
to	O
discover	O
non-linear	B
manifolds	O
here	O
we	O
only	O
briefly	O
describe	O
the	O
approach	B
and	O
refer	O
the	O
reader	O
to	O
for	O
details	O
in	O
kernel	B
pca	B
we	O
replace	O
each	O
x	O
by	O
a	O
feature	O
vector	O
x	O
note	O
that	O
the	O
use	O
of	O
x	O
here	O
does	O
not	O
have	O
the	O
interpretation	O
we	O
used	O
before	O
as	O
the	O
approximate	B
reconstruction	O
the	O
feature	B
map	B
takes	O
a	O
vector	O
x	O
and	O
produces	O
a	O
higher	O
dimensional	O
vector	O
x	O
for	O
example	O
we	O
could	O
map	B
a	O
two	O
dimensional	O
vector	O
x	O
using	O
the	O
idea	O
is	O
then	O
to	O
perform	O
pca	B
on	O
these	O
higher	O
dimensional	O
feature	O
vectors	O
subsequently	O
mapping	O
back	O
the	O
eigenvectors	O
to	O
the	O
original	O
space	O
x	O
the	O
main	O
challenge	O
is	O
to	O
write	O
this	O
without	O
explicitly	O
computing	O
pca	B
in	O
the	O
potentially	O
very	O
high	O
dimensional	O
feature	O
vector	O
space	O
as	O
a	O
reminder	O
in	O
standard	O
pca	B
for	O
zero	O
mean	B
data	O
one	O
forms	O
an	O
eigen-decomposition	O
of	O
the	O
sample	O
s	O
n	O
x	O
xt	O
for	O
simplicity	O
we	O
concentrate	O
here	O
on	O
finding	O
the	O
first	O
principal	O
component	O
e	O
which	O
satisfies	O
x	O
xt	O
e	O
e	O
for	O
corresponding	O
eigenvalue	O
n	O
the	O
dual	B
representation	I
is	O
obtained	O
by	O
pre-multiplying	O
by	O
xt	O
so	O
that	O
in	O
terms	O
of	O
f	O
xt	O
e	O
the	O
standard	O
pca	B
eigen-problem	O
reduces	O
to	O
solving	B
xt	O
x	O
f	O
f	O
e	O
x	O
f	O
xt	O
x	O
xt	O
x	O
yi	O
n	O
i	O
the	O
feature	O
eigenvector	O
e	O
is	O
then	O
recovered	O
using	O
we	O
note	O
that	O
matrix	B
xt	O
x	O
has	O
elements	O
mn	O
and	O
recognise	O
this	O
as	O
the	O
scalar	B
product	I
between	O
vectors	O
this	O
means	O
that	O
the	O
matrix	B
is	O
positive	B
definite	I
and	O
we	O
may	O
equivalently	O
use	O
a	O
positive	B
definite	I
kernel	B
see	O
kxm	O
xn	O
kmn	O
mn	O
then	O
equation	B
can	O
be	O
written	O
as	O
k	O
f	O
f	O
one	O
then	O
solves	O
this	O
eigen-equation	O
to	O
find	O
the	O
n	O
dimensional	O
principal	O
dual	B
feature	O
vector	O
f	O
the	O
projection	B
of	O
the	O
feature	O
x	O
is	O
given	O
by	O
y	O
xt	O
e	O
xt	O
x	O
f	O
more	O
generally	O
for	O
a	O
larger	O
number	O
of	O
components	O
the	O
ith	O
kernel	B
pca	B
projection	B
yi	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
kernel	B
directly	O
as	O
kx	O
xn	O
f	O
i	O
n	O
use	O
the	O
normalisation	B
n	O
as	O
opposed	O
to	O
n	O
just	O
for	O
notational	O
convenience	O
in	O
practice	O
there	O
is	O
little	O
difference	O
draft	O
march	O
kernel	B
pca	B
figure	O
canonical	B
correlation	I
analysis	B
training	B
data	O
the	O
top	O
panel	O
contains	O
the	O
x	O
matrix	B
of	O
dimensional	O
points	O
and	O
the	O
bottom	O
the	O
corresponding	O
dimensional	O
y	O
matrix	B
the	O
data	O
in	O
was	O
produced	O
using	O
x	O
ah	O
y	O
bh	O
where	O
a	O
is	O
a	O
matrix	B
and	O
b	O
is	O
a	O
matrix	B
matrices	O
a	O
and	O
b	O
learned	O
by	O
cca	O
note	O
that	O
they	O
are	O
close	O
to	O
the	O
true	O
a	O
and	O
b	O
up	O
to	O
rescaling	O
and	O
sign	O
changes	O
see	O
democca	O
m	O
where	O
i	O
is	O
the	O
eigenvalue	O
label	O
the	O
above	O
derivation	O
implicitly	O
assumed	O
zero	O
mean	B
features	O
x	O
even	O
if	O
the	O
original	O
data	O
x	O
is	O
zero	O
mean	B
due	O
to	O
the	O
non-linear	B
mapping	O
the	O
features	O
may	O
not	O
be	O
zero	O
mean	B
to	O
correct	O
for	O
this	O
one	O
may	O
show	O
that	O
the	O
only	O
modification	O
required	O
is	O
to	O
replace	O
the	O
matrix	B
k	O
in	O
equation	B
above	O
with	O
mn	O
kxm	O
xn	O
k	O
n	O
kxd	O
xn	O
n	O
kxm	O
xd	O
n	O
xd	O
finding	O
the	O
reconstructions	O
the	O
above	O
gives	O
a	O
procedure	O
for	O
finding	O
the	O
kpca	O
projection	B
y	O
however	O
in	O
many	O
cases	O
we	O
would	O
also	O
like	O
to	O
have	O
an	O
approximate	B
reconstruction	O
using	O
the	O
lower	O
dimensional	O
y	O
this	O
is	O
not	O
straightforward	O
since	O
the	O
mapping	O
from	O
y	O
to	O
x	O
is	O
in	O
general	O
highly	O
non-linear	B
here	O
we	O
outline	O
a	O
procedure	O
for	O
achieving	O
this	O
first	O
we	O
find	O
the	O
reconstruction	O
x	O
of	O
the	O
feature	O
space	O
x	O
now	O
f	O
n	O
i	O
x	O
yi	O
ei	O
i	O
i	O
yi	O
i	O
n	O
given	O
x	O
we	O
try	O
to	O
find	O
that	O
point	O
in	O
the	O
original	O
data	O
space	O
that	O
maps	O
to	O
x	O
this	O
can	O
be	O
found	O
by	O
minimising	O
x	O
yi	O
i	O
i	O
n	O
up	O
to	O
negligable	O
constants	O
this	O
is	O
i	O
kxn	O
f	O
n	O
one	O
then	O
finds	O
by	O
minimising	O
numerically	O
draft	O
march	O
training	B
canonical	B
correlation	I
analysis	B
canonical	B
correlation	I
analysis	B
consider	O
x	O
and	O
y	O
which	O
have	O
dimensions	O
dim	O
and	O
dim	O
respectively	O
for	O
example	O
x	O
might	O
represent	O
a	O
segment	O
of	O
video	O
and	O
y	O
the	O
corresponding	O
audio	O
given	O
then	O
a	O
collection	O
yn	O
n	O
n	O
an	O
interesting	O
challenge	O
is	O
to	O
identify	O
which	O
parts	O
of	O
the	O
audio	O
and	O
video	O
files	O
are	O
strongly	O
correlated	O
one	O
might	O
expect	O
for	O
example	O
that	O
the	O
mouth	O
region	O
of	O
the	O
video	O
is	O
strongly	O
correlated	O
with	O
the	O
audio	O
one	O
way	O
to	O
achieve	O
this	O
is	O
to	O
project	O
each	O
x	O
and	O
y	O
to	O
one	O
dimension	O
using	O
atx	O
and	O
bty	O
such	O
that	O
the	O
correlation	O
between	O
the	O
projections	O
is	O
maximal	O
the	O
unnormalised	O
correlation	O
between	O
the	O
projections	O
atx	O
and	O
bty	O
atxnbtyn	O
at	O
xnynt	O
b	O
n	O
n	O
and	O
the	O
normalised	O
correlation	O
is	O
atsxyb	O
where	O
sxy	O
is	O
the	O
sample	O
x	O
y	O
cross	B
correlation	O
matrix	B
when	O
the	O
joint	B
covariance	B
of	O
the	O
stacked	O
vectors	O
zn	O
yn	O
is	O
considered	O
sxx	O
sxy	O
syx	O
syy	O
are	O
the	O
blocks	O
of	O
the	O
joint	B
covariance	B
matrix	B
since	O
equation	B
is	O
invariant	O
with	O
respect	O
to	O
length	O
scaling	O
of	O
a	O
and	O
also	O
b	O
we	O
can	O
consider	O
the	O
equivalent	B
objective	O
ea	O
b	O
atsxyb	O
subject	O
to	O
atsxxa	O
and	O
btsyyb	O
to	O
find	O
the	O
optimal	O
projections	O
a	O
b	O
under	O
the	O
constraints	O
we	O
use	O
the	O
lagrangian	B
l	O
b	O
a	O
b	O
atsxyb	O
a	O
atsxxa	O
from	O
which	O
we	O
obtain	O
the	O
zero	O
derivative	O
criteria	O
btsyyb	O
b	O
sxyb	O
asxxa	O
syxa	O
bsyyb	O
hence	O
atsxyb	O
aatsxxa	O
a	O
btsyxa	O
bbtsyyb	O
b	O
since	O
atsxyb	O
btsyxa	O
we	O
must	O
have	O
a	O
b	O
at	O
the	O
optimum	O
if	O
we	O
assume	O
that	O
syy	O
is	O
invertible	O
b	O
s	O
yy	O
syxa	O
using	O
this	O
to	O
eliminate	O
b	O
in	O
equation	B
we	O
have	O
sxys	O
yy	O
syxa	O
which	O
is	O
a	O
generalised	B
eigen-problem	O
assuming	O
that	O
sxx	O
is	O
invertible	O
we	O
can	O
equivalently	O
write	O
xx	O
sxys	O
s	O
yy	O
syxa	O
which	O
is	O
a	O
standard	O
eigen-problem	O
with	O
as	O
the	O
eigenvalue	O
once	O
this	O
is	O
solved	O
we	O
can	O
find	O
b	O
using	O
equation	B
draft	O
march	O
exercises	O
svd	B
formulation	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
we	O
can	O
find	O
a	O
by	O
first	O
computing	O
the	O
svd	B
of	O
xx	O
sxys	O
yy	O
s	O
in	O
the	O
form	O
udvt	O
and	O
extracting	O
the	O
maximal	O
singular	B
vector	O
of	O
u	O
first	O
column	O
on	O
u	O
then	O
a	O
yy	O
where	O
is	O
the	O
first	O
column	O
of	O
v	O
in	O
this	O
way	O
is	O
optimally	O
s	O
the	O
extension	O
to	O
finding	O
m	O
multiple	O
directions	O
a	O
and	O
b	O
is	O
clear	O
one	O
xx	O
and	O
similarly	O
b	O
is	O
optimally	O
s	O
takes	O
the	O
corresponding	O
first	O
m	O
singular	B
values	O
accordingly	O
doing	O
so	O
maximises	O
the	O
criterion	O
this	O
approach	B
is	O
taken	O
in	O
cca	O
m	O
see	O
for	O
a	O
demonstration	O
one	O
can	O
also	O
show	O
that	O
cca	O
corresponds	O
to	O
the	O
probabilistic	B
factor	B
analysis	B
model	B
under	O
a	O
block	O
restriction	O
on	O
the	O
form	O
of	O
the	O
factor	B
loadings	O
see	O
cca	O
and	O
related	O
kernel	B
extensions	O
have	O
been	O
applied	O
in	O
machine	O
learning	B
contexts	O
for	O
example	O
to	O
model	B
the	O
correlation	O
between	O
images	O
and	O
text	O
in	O
order	O
to	O
improve	O
image	O
retrieval	O
from	O
text	O
queries	O
see	O
notes	O
pca	B
is	O
also	O
known	O
as	O
the	O
karhunen-loeve	O
decomposition	B
particularly	O
in	O
the	O
engineering	O
literature	O
code	O
pca	B
m	O
principal	B
components	I
analysis	B
demolsi	O
m	O
demo	O
of	O
latent	B
semantic	O
indexinganalysis	O
svdm	O
m	O
singular	B
value	B
decomposition	B
with	O
missing	B
data	I
demosvdmissing	O
m	O
demo	O
svd	B
with	O
missing	B
data	I
plsa	O
m	O
probabilistic	B
latent	B
semantic	I
analysis	B
plsacond	O
m	O
conditional	B
probabilistic	B
latent	B
semantic	I
analysis	B
demoplsa	O
m	O
demo	O
of	O
plsa	O
demomultnomialpxygz	O
m	O
demo	O
of	O
finite	O
sample	O
plsa	O
cca	O
m	O
canonical	B
correlation	I
analysis	B
democca	O
m	O
demo	O
of	O
canonical	B
correlation	I
analysis	B
exercises	O
exercise	O
consider	O
a	O
dataset	O
in	O
two	O
dimensions	O
where	O
the	O
data	O
lies	O
on	O
the	O
circumference	O
of	O
a	O
circle	O
of	O
unit	O
radius	O
what	O
would	O
be	O
the	O
effect	O
of	O
using	O
pca	B
on	O
this	O
dataset	O
in	O
which	O
we	O
attempt	O
to	O
reduce	O
the	O
dimensionality	O
to	O
suggest	O
an	O
alternative	O
one	O
dimensional	O
representation	O
of	O
the	O
data	O
exercise	O
consider	O
two	O
vectors	O
xa	O
and	O
xb	O
and	O
their	O
corresponding	O
pca	B
approximations	O
and	O
c	O
aiei	O
biei	O
where	O
the	O
eigenvectors	O
ei	O
i	O
m	O
are	O
mutually	O
orthogonal	B
and	O
have	O
unit	O
length	O
the	O
eigenvector	O
ei	O
has	O
corresponding	O
eigenvalue	O
i	O
approximate	B
by	O
using	O
the	O
pca	B
representations	O
of	O
the	O
data	O
and	O
show	O
that	O
this	O
is	O
equal	O
to	O
exercise	O
show	O
how	O
the	O
solution	O
for	O
a	O
to	O
the	O
cca	O
problem	B
in	O
equation	B
can	O
be	O
transformed	O
into	O
the	O
form	O
expressed	O
by	O
equation	B
as	O
claimed	O
in	O
the	O
text	O
draft	O
march	O
exercises	O
xa	O
s	O
xa	O
exercise	O
let	O
s	O
be	O
the	O
covariance	B
matrix	B
of	O
the	O
data	O
the	O
mahalanobis	B
distance	I
between	O
xa	O
and	O
xb	O
is	O
defined	O
as	O
explain	O
how	O
to	O
approximate	B
this	O
distance	O
using	O
m-dimensional	O
pca	B
approximations	O
exercise	O
with	O
external	O
inputs	O
in	O
some	O
applications	O
one	O
may	O
suspect	O
that	O
certain	O
external	O
variables	O
v	O
have	O
a	O
strong	B
influence	O
on	O
how	O
the	O
data	O
x	O
is	O
distributed	O
for	O
example	O
if	O
x	O
represents	O
an	O
image	O
it	O
might	O
be	O
that	O
we	O
know	O
the	O
lighting	O
condition	O
v	O
under	O
which	O
the	O
image	O
was	O
made	O
this	O
will	O
have	O
a	O
large	O
effect	O
on	O
the	O
image	O
it	O
would	O
make	O
sense	O
therefore	O
to	O
include	O
the	O
known	O
lighting	O
condition	O
in	O
forming	O
a	O
lower	O
dimensional	O
representation	O
of	O
the	O
image	O
note	O
that	O
we	O
don	O
t	O
want	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
of	O
the	O
joint	B
x	O
v	O
rather	O
we	O
want	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
of	O
x	O
alone	O
bearing	O
in	O
mind	O
that	O
some	O
of	O
the	O
variability	O
observed	O
may	O
be	O
due	O
to	O
v	O
we	O
therefore	O
assume	O
an	O
approximation	B
k	O
ck	O
vn	O
j	O
k	O
where	O
the	O
coefficients	O
yn	O
i	O
i	O
n	O
n	O
n	O
and	O
basis	O
vectors	O
bj	O
j	O
j	O
and	O
ck	O
k	O
k	O
are	O
to	O
be	O
determined	O
the	O
external	O
inputs	O
vn	O
are	O
given	O
the	O
sum	O
squared	O
error	O
loss	O
between	O
the	O
xn	O
and	O
their	O
linear	B
reconstruction	O
equation	B
is	O
j	O
bj	O
yn	O
xn	O
xn	O
e	O
ni	O
j	O
k	O
i	O
j	O
bj	O
yn	O
i	O
k	O
ck	O
vn	O
i	O
find	O
the	O
parameters	O
that	O
minimise	O
e	O
exercise	O
consider	O
the	O
following	O
datapoints	O
perform	O
principal	B
components	I
analysis	B
by	O
calculating	O
the	O
mean	B
c	O
of	O
the	O
data	O
calculating	O
the	O
covariance	B
matrix	B
s	O
finding	O
the	O
eigenvalues	O
and	O
eigenvectors	O
ei	O
of	O
the	O
covariance	B
matrix	B
xnxnt	O
cct	O
of	O
the	O
data	O
you	O
should	O
find	O
that	O
only	O
two	O
eigenvalues	O
are	O
large	O
and	O
therefore	O
that	O
the	O
data	O
can	O
be	O
well	O
represented	O
using	O
two	O
components	O
only	O
let	O
and	O
be	O
the	O
two	O
eigenvectors	O
with	O
largest	O
eigenvalues	O
calculate	O
the	O
two	O
dimensional	O
representation	O
of	O
each	O
datapoint	O
c	O
c	O
n	O
calculate	O
the	O
reconstruction	O
of	O
each	O
datapoint	O
c	O
n	O
exercise	O
consider	O
a	O
conditional	B
frequency	O
matrix	B
px	O
iy	O
j	O
k	O
show	O
how	O
to	O
derive	O
an	O
em	B
style	O
algorithm	B
for	O
an	O
approximate	B
decomposition	B
of	O
this	O
matrix	B
in	O
the	O
form	O
px	O
iy	O
j	O
px	O
iz	O
k	O
pz	O
ky	O
j	O
where	O
k	O
z	O
i	O
x	O
j	O
y	O
exercise	O
for	O
the	O
multinomial	B
model	B
px	O
y	O
z	O
described	O
in	O
equation	B
derive	O
explicitly	O
the	O
em	B
algorithm	B
and	O
implement	O
this	O
in	O
matlab	O
for	O
randomly	O
chosen	O
values	O
for	O
the	O
conditional	B
probabilities	O
draw	O
samples	O
from	O
this	O
model	B
for	O
x	O
y	O
z	O
and	O
compute	O
from	O
this	O
the	O
matrix	B
with	O
elements	O
pij	O
i	O
y	O
j	O
i	O
y	O
j	O
now	O
run	O
plsa	O
plsa	O
m	O
with	O
the	O
settings	O
x	O
y	O
z	O
to	O
learn	O
and	O
compare	O
your	O
results	O
with	O
those	O
obtained	O
from	O
the	O
finite	O
sample	O
model	B
equation	B
draft	O
march	O
chapter	O
supervised	B
linear	B
dimension	I
reduction	I
supervised	B
linear	B
projections	O
in	O
we	O
discussed	O
dimension	O
reduction	O
using	O
an	O
unsupervised	B
procedure	O
in	O
cases	O
where	O
class	O
information	O
is	O
available	O
and	O
our	O
ultimate	O
interest	O
is	O
to	O
reduce	O
dimensionality	O
for	O
improved	O
classification	B
it	O
makes	O
sense	O
to	O
use	O
the	O
available	O
class	O
information	O
in	O
forming	O
the	O
projections	O
exploiting	O
the	O
class	O
label	O
information	O
to	O
improve	O
the	O
projection	B
is	O
a	O
form	O
of	O
supervised	B
dimension	O
reduction	O
let	O
s	O
consider	O
data	O
from	O
two	O
different	O
classes	O
for	O
class	O
we	O
have	O
a	O
set	O
of	O
data	O
datapoints	O
and	O
similarly	O
for	O
class	O
we	O
have	O
a	O
set	O
of	O
datapoints	O
our	O
interest	O
is	O
then	O
to	O
find	O
a	O
linear	B
projection	B
y	O
wtx	O
where	O
dim	O
w	O
d	O
l	O
l	O
d	O
such	O
that	O
for	O
datapoints	O
xi	O
xj	O
in	O
the	O
same	O
class	O
the	O
distance	O
between	O
their	O
projections	O
yi	O
yj	O
should	O
be	O
small	O
conversely	O
for	O
datapoints	O
in	O
different	O
classes	O
the	O
distance	O
between	O
their	O
projections	O
should	O
be	O
large	O
this	O
may	O
be	O
useful	O
for	O
classification	B
purposes	O
since	O
for	O
a	O
novel	O
point	O
x	O
if	O
its	O
projection	B
y	O
wtx	O
is	O
close	O
to	O
class	O
projected	O
data	O
we	O
would	O
expect	O
x	O
to	O
belong	O
to	O
class	O
in	O
forming	O
the	O
supervised	B
projection	B
only	O
the	O
class	O
discriminative	B
parts	O
of	O
the	O
data	O
are	O
retained	O
so	O
that	O
the	O
procedure	O
can	O
be	O
considered	O
a	O
form	O
of	O
supervised	B
feature	O
extraction	O
fisher	O
s	O
linear	B
discriminant	O
we	O
restrict	O
attention	O
to	O
binary	O
class	O
data	O
also	O
for	O
simplicity	O
we	O
project	O
the	O
data	O
down	O
to	O
one	O
dimension	O
the	O
canonical	B
variates	I
algorithm	B
of	O
deals	O
with	O
the	O
generalisations	O
gaussian	B
assumption	O
we	O
model	B
the	O
data	O
from	O
each	O
class	O
with	O
a	O
gaussian	B
that	O
is	O
n	O
n	O
fisher	O
s	O
linear	B
discriminant	O
figure	O
the	O
large	O
crosses	O
represent	O
data	O
from	O
class	O
and	O
the	O
large	O
circles	O
from	O
class	O
their	O
projections	O
onto	O
dimension	O
are	O
represented	O
by	O
their	O
small	O
counterparts	O
fisher	O
s	O
linear	B
discriminant	O
unsupervised	B
dimension	O
reduction	O
analysis	B
here	O
there	O
is	O
little	O
class	O
overlap	O
in	O
the	O
projections	O
using	O
principal	B
components	I
analysis	B
for	O
comparison	O
there	O
is	O
considerable	O
class	O
overlap	O
in	O
the	O
projection	B
in	O
both	O
and	O
the	O
one	O
dimensional	O
projection	B
is	O
the	O
distance	O
along	O
the	O
line	O
measured	O
from	O
an	O
arbitrary	O
chosen	O
fixed	O
point	O
on	O
the	O
line	O
where	O
is	O
the	O
sample	O
mean	B
of	O
class	O
data	O
and	O
the	O
sample	O
covariance	B
similarly	O
for	O
class	O
the	O
projections	O
of	O
the	O
points	O
from	O
the	O
two	O
classes	O
are	O
then	O
given	O
by	O
because	O
the	O
projections	O
are	O
linear	B
the	O
projected	O
distributions	O
are	O
also	O
gaussian	B
wtxn	O
yn	O
wtxn	O
yn	O
n	O
n	O
we	O
search	O
for	O
a	O
projection	B
w	O
such	O
that	O
the	O
projected	O
distributions	O
have	O
minimal	O
overlap	O
this	O
can	O
be	O
achieved	O
if	O
the	O
projected	O
gaussian	B
means	O
are	O
maximally	O
separated	O
is	O
large	O
however	O
if	O
the	O
variances	O
are	O
also	O
large	O
there	O
could	O
be	O
a	O
large	O
overlap	O
still	O
in	O
the	O
classes	O
a	O
useful	O
objective	O
function	B
therefore	O
is	O
where	O
i	O
represents	O
the	O
fraction	O
of	O
the	O
dataset	O
in	O
class	O
i	O
equation	B
is	O
in	O
terms	O
of	O
the	O
projection	B
w	O
the	O
objective	O
f	O
wt	O
w	O
wt	O
w	O
wtaw	O
wtbw	O
where	O
the	O
optimal	O
w	O
can	O
be	O
found	O
by	O
differentiating	O
equation	B
with	O
respect	O
to	O
w	O
this	O
gives	O
a	O
wtaw	O
wtbw	O
w	O
aw	O
and	O
therefore	O
the	O
zero	O
derivative	O
requirement	O
is	O
wtbw	O
wtbw	O
aw	O
wtaw	O
bw	O
b	O
wtaw	O
bw	O
draft	O
march	O
canonical	B
variates	I
multiplying	O
by	O
the	O
inverse	O
of	O
b	O
we	O
have	O
b	O
w	O
wtaw	O
wtbw	O
w	O
this	O
means	O
that	O
the	O
optimal	O
projection	B
is	O
explicitly	O
given	O
by	O
w	O
b	O
although	O
the	O
proportionality	O
factor	B
depends	O
on	O
w	O
we	O
may	O
take	O
it	O
to	O
be	O
constant	O
since	O
the	O
objective	O
function	B
f	O
of	O
equation	B
is	O
invariant	O
to	O
rescaling	O
of	O
w	O
we	O
may	O
therefore	O
take	O
w	O
kb	O
it	O
is	O
common	O
to	O
rescale	O
w	O
to	O
have	O
unit	O
length	O
wtw	O
such	O
that	O
k	O
b	O
an	O
illustration	O
of	O
the	O
method	O
is	O
given	O
in	O
which	O
demonstrates	O
how	O
supervised	B
dimension	O
reduction	O
can	O
produce	O
lower	B
dimensional	I
representations	I
more	O
suitable	O
for	O
subsequent	O
classification	B
than	O
an	O
unsupervised	B
method	O
such	O
as	O
pca	B
one	O
can	O
also	O
arrive	O
at	O
the	O
equation	B
from	O
a	O
different	O
starting	O
objective	O
by	O
treating	O
the	O
projection	B
as	O
a	O
regression	B
problem	B
y	O
wtxb	O
in	O
which	O
the	O
outputs	O
y	O
are	O
defined	O
as	O
and	O
for	O
classes	O
and	O
class	O
respectively	O
one	O
may	O
show	O
that	O
for	O
suitably	O
chosen	O
and	O
the	O
solution	O
using	O
a	O
least	O
squares	O
criterion	O
is	O
given	O
by	O
equation	B
this	O
also	O
suggests	O
a	O
way	O
to	O
regularise	O
lda	O
see	O
kernel	B
extensions	O
of	O
lda	O
are	O
possible	O
see	O
for	O
example	O
when	O
the	O
naive	O
method	O
breaks	O
down	O
the	O
above	O
derivation	O
relied	O
on	O
the	O
existence	O
of	O
the	O
inverse	O
of	O
b	O
in	O
practice	O
however	O
b	O
may	O
not	O
be	O
invertible	O
and	O
the	O
above	O
procedure	O
requires	O
modification	O
a	O
case	O
where	O
b	O
is	O
not	O
invertible	O
is	O
when	O
there	O
are	O
fewer	O
datapoints	O
than	O
dimensions	O
d	O
another	O
case	O
is	O
when	O
there	O
are	O
elements	O
of	O
the	O
input	O
vectors	O
that	O
never	O
vary	O
for	O
example	O
in	O
the	O
hand-written	O
digits	O
case	O
the	O
pixels	O
at	O
the	O
corner	O
edges	O
are	O
actually	O
always	O
zero	O
let	O
s	O
call	O
this	O
corner	O
pixel	O
z	O
the	O
matrix	B
b	O
will	O
then	O
have	O
a	O
zero	O
entry	O
for	O
the	O
whole	O
zth	O
row	O
and	O
column	O
will	O
be	O
zero	O
so	O
that	O
for	O
any	O
vector	O
w	O
wz	O
wtbw	O
this	O
shows	O
that	O
the	O
denominator	O
of	O
fisher	O
s	O
objective	O
can	O
become	O
zero	O
and	O
the	O
objective	O
ill	O
defined	O
we	O
will	O
deal	O
with	O
these	O
issues	O
canonical	B
variates	I
canonical	B
variates	I
generalises	O
fisher	O
s	O
method	O
to	O
projections	O
in	O
more	O
than	O
one	O
dimension	O
and	O
more	O
than	O
two	O
classes	O
the	O
projection	B
of	O
any	O
point	O
is	O
given	O
by	O
y	O
wtx	O
where	O
w	O
is	O
a	O
d	O
l	O
matrix	B
assuming	O
that	O
the	O
data	O
x	O
from	O
class	O
c	O
is	O
gaussian	B
distributed	O
px	O
n	O
mc	O
sc	O
the	O
projections	O
y	O
are	O
also	O
gaussian	B
py	O
n	O
y	O
wtmc	O
wtscw	O
to	O
extend	O
to	O
more	O
than	O
two	O
classes	O
we	O
define	O
the	O
following	O
matrices	O
draft	O
march	O
between	O
class	O
scatter	O
find	O
m	O
the	O
mean	B
of	O
the	O
whole	O
dataset	O
and	O
mc	O
the	O
mean	B
of	O
the	O
each	O
class	O
c	O
canonical	B
variates	I
where	O
nc	O
is	O
the	O
number	O
of	O
datapoints	O
in	O
class	O
c	O
c	O
c	O
within	O
class	O
scatter	O
for	O
each	O
class	O
c	O
form	O
a	O
covariance	B
matrix	B
sc	O
and	O
mean	B
mc	O
define	O
form	O
a	O
nc	O
m	O
mt	O
b	O
ncsc	O
this	O
naturally	O
gives	O
rise	O
to	O
a	O
raleigh	B
quotient	I
objective	O
trace	O
f	O
bt	O
b	O
b	O
then	O
defining	O
the	O
objective	O
can	O
be	O
written	O
in	O
terms	O
of	O
w	O
trace	O
wt	O
b	O
ta	O
b	O
w	O
w	O
bw	O
w	O
b	O
w	O
wt	O
w	O
wtc	O
w	O
f	O
w	O
trace	O
f	O
w	O
trace	O
subject	O
to	O
wt	O
w	O
i	O
assuming	O
b	O
is	O
invertible	O
otherwise	O
we	O
can	O
define	O
the	O
cholesky	B
factor	B
b	O
with	O
if	O
we	O
assume	O
an	O
orthonormality	O
constraint	O
on	O
w	O
then	O
we	O
equivalently	O
require	O
the	O
maximisation	B
of	O
where	O
c	O
b	O
ta	O
b	O
since	O
c	O
is	O
symmetric	O
and	O
positive	O
semi-definite	O
it	O
has	O
a	O
real	O
eigen-decomposition	O
c	O
e	O
et	O
where	O
diag	O
d	O
is	O
diagonal	O
with	O
non-negative	O
entries	O
containing	O
the	O
eigenvalues	O
sorted	O
by	O
decreasing	O
order	O
and	O
ete	O
i	O
hence	O
wte	O
et	O
w	O
f	O
w	O
trace	O
by	O
setting	O
w	O
el	O
where	O
el	O
is	O
the	O
lth	O
eigenvector	O
the	O
objective	O
becomes	O
the	O
sum	O
of	O
the	O
first	O
l	O
eigenvalues	O
this	O
setting	O
maximises	O
the	O
objective	O
function	B
since	O
forming	O
w	O
from	O
any	O
other	O
columns	O
of	O
e	O
would	O
give	O
a	O
lower	O
sum	O
we	O
then	O
return	O
w	O
b	O
w	O
as	O
the	O
projection	B
matrix	B
the	O
procedure	O
is	O
outlined	O
in	O
note	O
that	O
since	O
a	O
has	O
rank	B
c	O
there	O
can	O
be	O
no	O
more	O
than	O
c	O
non-zero	O
eigenvalues	O
and	O
corresponding	O
directions	O
draft	O
march	O
canonical	B
variates	I
algorithm	B
canonical	B
variates	I
compute	O
the	O
between	O
and	O
within	O
class	O
scatter	O
matrices	O
a	O
equation	B
and	O
b	O
equation	B
compute	O
the	O
cholesky	B
factor	B
b	O
of	O
b	O
compute	O
the	O
l	O
principal	O
eigenvectors	O
el	O
of	O
b	O
ta	O
b	O
w	O
el	O
return	O
w	O
b	O
w	O
as	O
the	O
projection	B
matrix	B
dealing	O
with	O
the	O
nullspace	O
the	O
above	O
derivation	O
of	O
canonical	B
variates	I
also	O
fisher	O
s	O
lda	O
requires	O
the	O
invertibility	O
of	O
the	O
matrix	B
b	O
however	O
as	O
we	O
discussed	O
in	O
one	O
may	O
encounter	O
situations	O
where	O
b	O
is	O
not	O
invertible	O
a	O
solution	O
is	O
to	O
require	O
that	O
w	O
lies	O
only	O
in	O
the	O
subspace	O
spanned	O
by	O
the	O
data	O
is	O
there	O
can	O
be	O
no	O
contribution	O
from	O
the	O
nullspace	O
to	O
do	O
this	O
we	O
first	O
concatenate	O
the	O
training	B
data	O
from	O
all	O
classes	O
into	O
one	O
large	O
matrix	B
x	O
a	O
basis	O
for	O
x	O
can	O
be	O
found	O
using	O
for	O
example	O
the	O
thin-svd	O
technique	O
which	O
returns	O
an	O
orthonormal	B
basis	O
q	O
we	O
then	O
require	O
the	O
solution	O
w	O
to	O
be	O
expressed	O
in	O
this	O
basis	O
w	O
for	O
some	O
matrix	B
substituting	O
this	O
in	O
the	O
canonical	B
variates	I
objective	O
equation	B
we	O
obtain	O
trace	O
f	O
this	O
is	O
of	O
the	O
same	O
form	O
as	O
the	O
standard	O
quotient	O
equation	B
on	O
replacing	O
the	O
between-scatter	O
a	O
with	O
qtaq	O
and	O
the	O
within-scatter	O
b	O
with	O
qtbq	O
in	O
this	O
case	O
is	O
guaranteed	O
invertible	O
and	O
one	O
may	O
carry	O
out	O
canonical	B
variates	I
as	O
in	O
above	O
this	O
will	O
return	O
a	O
matrix	B
we	O
then	O
return	O
w	O
see	O
also	O
canonvar	O
m	O
example	O
canonical	B
variates	I
on	O
the	O
digits	O
data	O
we	O
apply	O
canonical	B
variates	I
to	O
project	O
the	O
digit	B
data	I
onto	O
two	O
dimensions	O
see	O
there	O
are	O
examples	O
of	O
a	O
three	O
examples	O
of	O
a	O
five	O
and	O
examples	O
of	O
a	O
seven	O
thus	O
overall	O
there	O
are	O
examples	O
lying	O
in	O
a	O
pixels	O
dimensional	O
space	O
note	O
how	O
the	O
canonical	B
variates	I
projected	O
data	O
onto	O
two	O
dimensions	O
has	O
very	O
little	O
class	O
overlap	O
see	O
in	O
comparison	O
the	O
projections	O
formed	O
from	O
pca	B
which	O
discards	O
the	O
class	O
information	O
displays	O
a	O
high	O
degree	B
of	O
class	O
overlap	O
the	O
different	O
scales	O
of	O
the	O
canonical	B
variates	I
and	O
pca	B
projections	O
figure	O
each	O
three	O
dimensional	O
datapoint	O
lies	O
in	O
a	O
two-dimensional	O
plane	O
meaning	O
that	O
the	O
matrix	B
b	O
is	O
not	O
full	O
rank	B
and	O
therefore	O
not	O
invertible	O
a	O
solution	O
is	O
given	O
by	O
finding	O
vectors	O
that	O
span	O
the	O
plane	O
and	O
expressing	O
the	O
canonical	B
variates	I
solution	O
in	O
terms	O
of	O
these	O
vectors	O
alone	O
draft	O
march	O
exercises	O
figure	O
canonical	B
variates	I
projection	B
of	O
examples	O
of	O
handwritten	B
digits	I
o	O
and	O
there	O
are	O
examples	O
from	O
each	O
digit	O
class	O
plotted	O
are	O
the	O
projections	O
down	O
to	O
dimensions	O
pca	B
projections	O
for	O
comparison	O
in	O
pca	B
w	O
is	O
unitary	O
in	O
canonical	B
is	O
due	O
to	O
the	O
different	O
constraints	O
on	O
the	O
projection	B
matrices	O
w	O
variates	O
wtbw	O
i	O
meaning	O
that	O
w	O
will	O
scale	O
with	O
the	O
inverse	O
square	O
root	O
of	O
the	O
largest	O
eigenvalues	O
of	O
the	O
within	O
class	O
scatter	O
matrix	B
since	O
the	O
canonical	B
variates	I
objective	O
is	O
independent	O
of	O
linear	B
scaling	O
w	O
can	O
be	O
rescaled	O
with	O
an	O
arbitrary	O
scalar	O
prefactor	O
w	O
as	O
desired	O
using	O
non-gaussian	O
data	O
distributions	O
the	O
applicability	O
of	O
canonical	B
variates	I
depends	O
on	O
our	O
assumption	O
that	O
a	O
gaussian	B
is	O
a	O
good	O
description	O
of	O
the	O
data	O
clearly	O
if	O
the	O
data	O
is	O
multimodal	O
using	O
a	O
single	O
gaussian	B
to	O
model	B
the	O
data	O
in	O
each	O
class	O
is	O
a	O
poor	O
assumption	O
this	O
may	O
result	O
in	O
projections	O
with	O
a	O
large	O
class	O
overlap	O
in	O
principle	O
there	O
is	O
no	O
conceptual	O
difficulty	O
in	O
using	O
more	O
complex	O
distributions	O
with	O
say	O
more	O
general	O
criteria	O
such	O
as	O
kullbackleibler	O
divergence	B
between	O
projected	O
distributions	O
used	O
as	O
the	O
objective	O
however	O
such	O
criteria	O
typically	O
result	O
in	O
difficult	O
optimisation	B
problems	O
canonical	B
variates	I
is	O
popular	O
due	O
to	O
its	O
simplicity	O
and	O
lack	O
of	O
local	B
optima	O
issues	O
in	O
constructing	O
the	O
projection	B
code	O
canonvar	O
m	O
canonical	B
variates	I
democanonvardigits	O
m	O
demo	O
for	O
canonical	B
variates	I
exercises	O
exercise	O
what	O
happens	O
to	O
fisher	O
s	O
linear	B
discriminant	O
if	O
there	O
are	O
less	O
datapoints	O
than	O
dimensions	O
exercise	O
modify	O
democanonvardigits	O
m	O
to	O
project	O
and	O
visualise	O
the	O
digits	O
data	O
in	O
dimensions	O
exercise	O
consider	O
class	O
datapoints	O
and	O
class	O
datapoints	O
we	O
will	O
make	O
a	O
linear	B
predictor	O
for	O
the	O
data	O
y	O
wtx	O
b	O
draft	O
march	O
exercises	O
with	O
the	O
aim	O
to	O
predict	O
value	B
for	O
data	O
from	O
class	O
and	O
for	O
data	O
from	O
class	O
two	O
a	O
measure	O
of	O
the	O
fit	O
is	O
given	O
by	O
b	O
b	O
show	O
that	O
by	O
setting	O
and	O
the	O
w	O
which	O
minimises	O
e	O
corresponds	O
to	O
fisher	O
s	O
lda	O
solution	O
hint	O
first	O
show	O
that	O
the	O
two	O
zero	O
derivative	O
conditions	O
are	O
ew	O
and	O
b	O
b	O
xt	O
b	O
b	O
xt	O
which	O
can	O
be	O
reduced	O
to	O
the	O
single	O
equation	B
n	O
where	O
b	O
is	O
as	O
defined	O
for	O
lda	O
in	O
the	O
text	O
equation	B
n	O
nb	O
w	O
note	O
that	O
this	O
suggests	O
a	O
way	O
to	O
regularise	O
lda	O
namely	O
by	O
adding	O
on	O
a	O
term	O
wtw	O
to	O
ew	O
this	O
can	O
be	O
absorbed	O
into	O
redefining	O
equation	B
as	O
b	O
i	O
in	O
other	O
words	O
one	O
can	O
increase	O
the	O
covariance	B
b	O
by	O
an	O
additive	O
amount	O
i	O
the	O
optimal	O
regularising	O
constant	O
may	O
be	O
set	O
by	O
cross-validation	B
more	O
generally	O
one	O
can	O
consider	O
the	O
use	O
of	O
a	O
regularising	O
matrix	B
r	O
where	O
r	O
is	O
positive	B
definite	I
exercise	O
consider	O
the	O
digit	B
data	I
of	O
fives	O
and	O
sevens	O
make	O
a	O
training	B
set	O
which	O
consists	O
of	O
the	O
first	O
examples	O
from	O
each	O
digit	O
class	O
use	O
canonical	B
variates	I
to	O
first	O
project	O
the	O
data	O
down	O
to	O
dimensions	O
and	O
compute	O
the	O
nearest	B
neighbour	B
performance	B
on	O
the	O
remaining	O
digits	O
compare	O
the	O
classification	B
accuracy	O
to	O
using	O
nearest	O
neighbours	O
the	O
projections	O
from	O
pca	B
using	O
components	O
exercise	O
consider	O
an	O
objective	O
function	B
of	O
the	O
form	O
f	O
aw	O
bw	O
where	O
aw	O
and	O
bw	O
are	O
positive	O
functions	O
and	O
our	O
task	O
is	O
to	O
maximise	O
f	O
with	O
respect	O
to	O
w	O
it	O
may	O
be	O
that	O
this	O
objective	O
does	O
not	O
have	O
a	O
simple	O
algebraic	O
solution	O
even	O
though	O
aw	O
and	O
bw	O
are	O
simple	O
functions	O
we	O
can	O
consider	O
an	O
alternative	O
objective	O
namely	O
jw	O
aw	O
bw	O
where	O
is	O
a	O
constant	O
scalar	O
choose	O
an	O
initial	O
point	O
wold	O
at	O
random	O
and	O
set	O
old	O
awoldbwold	O
in	O
that	O
case	O
jwold	O
old	O
now	O
choose	O
a	O
w	O
such	O
that	O
jw	O
old	O
aw	O
oldbw	O
this	O
is	O
certainly	O
possible	O
since	O
jwold	O
old	O
if	O
we	O
can	O
find	O
a	O
w	O
such	O
that	O
jw	O
old	O
then	O
aw	O
oldbw	O
show	O
that	O
for	O
such	O
a	O
w	O
f	O
f	O
and	O
suggest	O
an	O
iterative	O
optimisation	B
procedure	O
for	O
objective	O
functions	O
of	O
the	O
form	O
f	O
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
linear	B
models	O
introduction	O
fitting	O
a	O
straight	O
line	O
given	O
training	B
data	O
yn	O
n	O
n	O
for	O
scalar	O
input	O
xn	O
and	O
scalar	O
output	O
yn	O
a	O
linear	B
regression	B
fit	O
is	O
yx	O
a	O
bx	O
to	O
determine	O
the	O
best	O
parameters	O
a	O
b	O
we	O
use	O
a	O
measure	O
of	O
the	O
discrepancy	O
between	O
the	O
observed	O
outputs	O
and	O
the	O
linear	B
regression	B
fit	O
such	O
as	O
the	O
sum	O
squared	O
training	B
error	O
this	O
is	O
also	O
called	O
ordinary	B
least	I
squares	I
and	O
minimises	O
the	O
average	B
vertical	O
projection	B
of	O
the	O
points	O
y	O
to	O
fitted	O
line	O
ea	O
b	O
a	O
our	O
task	O
is	O
to	O
find	O
the	O
parameters	O
a	O
and	O
b	O
that	O
minimise	O
ea	O
b	O
differentiating	O
with	O
respect	O
to	O
a	O
and	O
b	O
we	O
obtain	O
a	O
ea	O
b	O
a	O
bxn	O
b	O
ea	O
b	O
a	O
bxnxn	O
dividing	O
by	O
n	O
and	O
equating	O
to	O
zero	O
the	O
optimal	O
parameters	O
are	O
given	O
from	O
the	O
solution	O
to	O
the	O
two	O
linear	B
equations	O
a	O
where	O
we	O
used	O
the	O
notation	O
to	O
denote	O
to	O
determine	O
a	O
and	O
b	O
a	O
b	O
n	O
hence	O
b	O
and	O
a	O
is	O
found	O
by	O
substituting	O
this	O
value	B
for	O
b	O
into	O
equation	B
fxn	O
yn	O
we	O
can	O
readily	O
solve	O
the	O
in	O
contrast	O
to	O
ordinary	B
least	I
squares	I
regression	B
pca	B
from	O
minimises	O
the	O
orthogonal	B
projection	B
of	O
y	O
to	O
the	O
line	O
and	O
is	O
known	O
as	O
orthogonal	B
least	I
squares	I
see	O
linear	B
parameter	B
models	O
for	O
regression	B
figure	O
data	O
from	O
crickets	O
the	O
number	O
of	O
chirps	O
per	O
second	O
versus	O
the	O
temperature	O
in	O
fahrenheit	O
example	O
consider	O
the	O
data	O
in	O
in	O
which	O
we	O
plot	O
the	O
number	O
of	O
chirps	O
c	O
per	O
second	O
for	O
crickets	O
versus	O
the	O
temperature	O
t	O
in	O
degrees	O
fahrenheit	O
a	O
biologist	O
believes	O
that	O
there	O
is	O
a	O
simple	O
relation	O
between	O
the	O
number	O
of	O
chirps	O
and	O
the	O
temperature	O
of	O
the	O
form	O
c	O
a	O
bt	O
where	O
she	O
needs	O
to	O
determine	O
the	O
parameters	O
a	O
and	O
b	O
for	O
the	O
cricket	O
data	O
the	O
fit	O
is	O
plotted	O
in	O
for	O
comparison	O
we	O
plot	O
the	O
fit	O
from	O
the	O
pca	B
which	O
minimises	O
the	O
sum	O
of	O
the	O
squared	O
orthogonal	B
projections	O
from	O
the	O
data	O
to	O
the	O
line	O
in	O
this	O
case	O
there	O
is	O
little	O
numerical	B
difference	O
between	O
the	O
two	O
fits	O
linear	B
parameter	B
models	O
for	O
regression	B
we	O
can	O
generalise	O
on	O
the	O
idea	O
of	O
fitting	O
straight	O
lines	O
to	O
fitting	O
linear	B
functions	O
of	O
vector	O
inputs	O
for	O
a	O
dataset	O
yn	O
n	O
n	O
a	O
linear	B
parameter	B
regression	B
model	B
is	O
defined	O
yx	O
wt	O
where	O
is	O
a	O
vector	O
valued	O
function	B
of	O
the	O
input	O
vector	O
x	O
for	O
example	O
in	O
the	O
case	O
of	O
a	O
straight	O
line	O
fit	O
with	O
a	O
scalar	O
input	O
and	O
output	O
we	O
have	O
xt	O
w	O
bt	O
we	O
define	O
the	O
train	O
error	O
as	O
the	O
sum	O
of	O
squared	O
differences	O
between	O
the	O
observed	O
outputs	O
and	O
the	O
predictions	O
under	O
the	O
linear	B
model	B
ew	O
wt	O
where	O
n	O
we	O
now	O
wish	O
to	O
determine	O
the	O
parameter	B
vector	O
w	O
that	O
minimises	O
ew	O
writing	O
out	O
the	O
error	O
in	O
terms	O
of	O
the	O
components	O
of	O
w	O
ew	O
wi	O
n	O
i	O
wj	O
n	O
j	O
differentiating	O
with	O
respect	O
to	O
wk	O
and	O
equating	O
to	O
zero	O
gives	O
j	O
i	O
n	O
yn	O
n	O
wi	O
i	O
n	O
n	O
k	O
or	O
in	O
matrix	B
notation	O
k	O
i	O
yn	O
n	O
n	O
ntw	O
that	O
the	O
model	B
is	O
linear	B
in	O
the	O
parameter	B
w	O
not	O
necessarily	O
linear	B
in	O
x	O
draft	O
march	O
per	O
sectemperature	O
linear	B
parameter	B
models	O
for	O
regression	B
figure	O
straight	O
line	O
regression	B
fit	O
to	O
the	O
cricket	O
data	O
pca	B
fit	O
to	O
the	O
data	O
in	O
regression	B
we	O
minimize	O
the	O
residuals	B
the	O
vertical	O
distances	O
from	O
datapoints	O
to	O
the	O
line	O
in	O
pca	B
the	O
fit	O
minimizes	O
the	O
orthogonal	B
projections	O
to	O
the	O
line	O
in	O
this	O
case	O
there	O
is	O
little	O
difference	O
in	O
the	O
fitted	O
lines	O
both	O
go	O
through	O
the	O
mean	B
of	O
the	O
data	O
the	O
linear	B
regression	B
fit	O
has	O
slope	O
and	O
the	O
pca	B
fit	O
has	O
slope	O
these	O
are	O
called	O
the	O
normal	B
equations	I
for	O
which	O
the	O
solution	O
is	O
w	O
n	O
nt	O
yn	O
n	O
although	O
we	O
write	O
the	O
solution	O
using	O
matrix	B
inversion	B
in	O
practice	O
one	O
finds	O
the	O
numerical	B
solution	O
using	O
gaussian	B
since	O
this	O
is	O
faster	O
and	O
more	O
numerically	O
stable	O
example	O
a	O
cubic	O
polynomial	O
fit	O
a	O
cubic	O
polynomial	O
is	O
given	O
by	O
yx	O
as	O
a	O
lpm	O
this	O
can	O
be	O
expressed	O
using	O
x	O
the	O
ordinary	B
least	I
squares	I
solution	O
has	O
the	O
form	O
given	O
in	O
equation	B
the	O
fitted	O
cubic	O
polynomial	O
is	O
plotted	O
in	O
see	O
also	O
democubicpoly	O
m	O
example	O
return	O
in	O
we	O
present	O
fitting	O
an	O
lpm	O
with	O
vector	O
inputs	O
x	O
to	O
a	O
scalar	O
output	O
y	O
the	O
vector	O
x	O
represents	O
factors	O
that	O
are	O
believed	O
to	O
affect	O
the	O
stock	O
price	O
of	O
a	O
company	O
with	O
the	O
stock	O
price	O
return	O
given	O
by	O
the	O
scalar	O
y	O
a	O
hedge	B
fund	I
manager	O
believes	O
that	O
the	O
returns	O
may	O
be	O
linearly	O
related	O
to	O
the	O
factors	O
yt	O
wixit	O
and	O
wishes	O
to	O
fit	O
the	O
parameters	O
w	O
in	O
order	O
to	O
use	O
the	O
model	B
to	O
predict	O
future	O
stock	O
returns	O
this	O
is	O
straightforward	O
using	O
ordinary	B
least	I
squares	I
this	O
being	O
simply	O
an	O
lpm	O
with	O
a	O
linear	B
function	B
see	O
figure	O
cubic	O
polynomial	O
fit	O
to	O
the	O
cricket	O
data	O
draft	O
march	O
per	O
sectemperature	O
per	O
sectemperature	O
per	O
sectemperature	O
linear	B
parameter	B
models	O
for	O
regression	B
on	O
yt	O
figure	O
predicting	O
stock	O
return	O
using	O
a	O
linear	B
lpm	O
the	O
top	O
five	O
panels	O
present	O
the	O
inputs	O
for	O
train	O
days	O
and	O
test	O
days	O
the	O
corresponding	O
train	O
output	O
return	O
y	O
for	O
each	O
day	O
is	O
given	O
in	O
the	O
bottom	O
panel	O
the	O
predictions	O
are	O
the	O
predictions	O
based	O
i	O
wixit	O
with	O
w	O
trained	O
using	O
ordinary	B
least	I
squares	I
with	O
a	O
regularisation	B
term	O
the	O
ols	O
learned	O
w	O
is	O
despite	O
the	O
simplicity	O
of	O
these	O
models	O
their	O
application	O
in	O
the	O
finance	O
industry	O
is	O
widespread	O
with	O
significant	O
investment	O
made	O
on	O
collating	O
factors	O
x	O
that	O
may	O
be	O
indicative	O
of	O
future	O
return	O
see	O
demolpmhedge	O
m	O
for	O
an	O
example	O
such	O
models	O
also	O
form	O
the	O
basis	O
for	O
more	O
complex	O
models	O
in	O
finance	O
see	O
for	O
example	O
vector	O
outputs	O
it	O
is	O
straightforward	O
to	O
generalise	O
the	O
above	O
framework	O
to	O
vector	O
outputs	O
y	O
using	O
a	O
separate	O
weight	B
vector	O
wi	O
for	O
each	O
output	O
component	O
yi	O
we	O
have	O
the	O
mathematics	O
follows	O
similarly	O
to	O
before	O
and	O
we	O
may	O
define	O
a	O
training	B
error	O
per	O
output	O
as	O
yix	O
wt	O
i	O
ew	O
ewi	O
i	O
i	O
n	O
i	O
i	O
wt	O
yn	O
since	O
the	O
training	B
error	O
decomposes	O
into	O
individual	O
terms	O
one	O
for	O
each	O
output	O
the	O
weights	O
for	O
each	O
output	O
can	O
be	O
trained	O
separately	O
in	O
other	O
words	O
the	O
problem	B
decomposes	O
into	O
a	O
set	O
of	O
independent	O
scalar	O
output	O
problems	O
in	O
case	O
the	O
parameters	O
w	O
are	O
tied	O
or	O
shared	O
amongst	O
the	O
outputs	O
the	O
training	B
is	O
still	O
straightforward	O
since	O
the	O
objective	O
function	B
remains	O
linear	B
in	O
the	O
parameters	O
and	O
this	O
is	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
regularisation	B
for	O
most	O
purposes	O
our	O
interest	O
is	O
not	O
just	O
to	O
find	O
the	O
function	B
that	O
best	O
fits	O
the	O
training	B
data	O
but	O
one	O
that	O
that	O
will	O
generalise	O
well	O
to	O
control	O
the	O
complexity	O
of	O
the	O
fitted	O
function	B
we	O
may	O
add	O
an	O
extra	O
regularising	O
penalty	O
term	O
to	O
the	O
training	B
error	O
to	O
penalise	O
rapid	O
changes	O
in	O
the	O
output	O
for	O
example	O
a	O
regularising	O
term	O
that	O
can	O
be	O
added	O
to	O
equation	B
is	O
yxn	O
e	O
xn	O
yxn	O
xn	O
the	O
factor	B
factor	B
e	O
regularising	O
term	O
penalises	O
large	O
differences	O
in	O
the	O
outputs	O
corresponding	O
to	O
two	O
inputs	O
the	O
has	O
the	O
effect	O
of	O
weighting	O
more	O
heavily	O
terms	O
for	O
which	O
two	O
input	O
vectors	O
xn	O
and	O
are	O
close	O
together	O
is	O
a	O
fixed	O
length-scale	O
parameter	B
and	O
determines	O
the	O
overall	O
strength	O
of	O
the	O
draft	O
march	O
linear	B
parameter	B
models	O
for	O
regression	B
figure	O
a	O
set	O
of	O
fixed-width	O
radial	B
basis	I
functions	I
with	O
the	O
centres	O
mi	O
evenly	O
spaced	O
by	O
taking	O
a	O
linear	B
combie	O
nation	O
of	O
these	O
functions	O
we	O
can	O
form	O
a	O
flexible	O
function	B
class	O
since	O
y	O
wt	O
expression	O
can	O
be	O
written	O
as	O
wtrw	O
where	O
r	O
xn	O
n	O
n	O
e	O
the	O
regularised	B
train	O
error	O
is	O
then	O
e	O
wt	O
wtrw	O
n	O
by	O
differentiating	O
the	O
regularised	B
training	B
error	O
and	O
equating	O
to	O
zero	O
we	O
find	O
the	O
optimal	O
w	O
is	O
given	O
by	O
w	O
n	O
nt	O
r	O
yn	O
n	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
a	O
regulariser	O
that	O
penalises	O
the	O
sum	O
square	O
length	O
of	O
the	O
weights	O
wtw	O
i	O
i	O
which	O
corresponds	O
to	O
setting	O
r	O
i	O
regularising	O
pararameters	O
such	O
as	O
may	O
be	O
determined	O
using	O
a	O
validation	B
set	O
a	O
popular	O
lpm	O
is	O
given	O
by	O
the	O
non-linear	B
function	B
with	O
components	O
radial	B
basis	I
functions	I
ix	O
exp	O
these	O
basis	O
functions	O
are	O
bump	O
shaped	O
with	O
the	O
center	O
of	O
the	O
bump	O
i	O
being	O
given	O
by	O
mi	O
and	O
the	O
width	O
by	O
an	O
example	O
is	O
given	O
in	O
in	O
which	O
several	O
rbfs	O
are	O
plotted	O
with	O
different	O
centres	O
in	O
lpm	O
regression	B
we	O
can	O
then	O
use	O
a	O
linear	B
combination	O
of	O
these	O
bumps	O
to	O
fit	O
the	O
data	O
one	O
can	O
apply	O
the	O
same	O
approach	B
using	O
vector	O
inputs	O
for	O
vector	O
x	O
and	O
centre	O
m	O
the	O
radial	O
basis	O
function	B
depends	O
on	O
the	O
distance	O
between	O
x	O
and	O
the	O
centre	O
m	O
giving	O
a	O
bump	O
in	O
input	O
space	O
example	O
consider	O
fitting	O
the	O
data	O
in	O
using	O
radial	B
basis	I
functions	I
uniformly	O
spread	O
over	O
the	O
input	O
space	O
with	O
width	O
parameter	B
and	O
regularising	O
term	O
wtw	O
the	O
generalisation	B
performance	B
on	O
the	O
test	O
data	O
depends	O
heavily	O
on	O
the	O
width	O
and	O
regularising	O
parameter	B
in	O
order	O
to	O
find	O
reasonable	O
values	O
for	O
these	O
parameters	O
we	O
may	O
use	O
a	O
validation	B
set	O
for	O
simplicity	O
we	O
set	O
the	O
regularisation	B
parameter	B
to	O
and	O
use	O
the	O
validation	B
set	O
to	O
determine	O
a	O
suitable	O
in	O
we	O
plot	O
the	O
validation	B
error	O
as	O
a	O
function	B
of	O
based	O
on	O
this	O
graph	B
we	O
can	O
find	O
the	O
best	O
value	B
of	O
that	O
which	O
minimises	O
the	O
validation	B
error	O
the	O
predictions	O
are	O
also	O
given	O
in	O
draft	O
march	O
the	O
dual	B
representation	I
and	O
kernels	O
figure	O
the	O
are	O
the	O
training	B
points	O
and	O
the	O
are	O
the	O
validation	B
points	O
the	O
solid	O
line	O
is	O
the	O
correct	O
underlying	O
function	B
which	O
is	O
corrupted	O
with	O
a	O
small	O
amount	O
of	O
additive	O
noise	O
to	O
form	O
the	O
train	O
data	O
the	O
dashed	O
line	O
is	O
the	O
best	O
predictor	O
based	O
on	O
the	O
validation	B
set	O
a	O
curse	B
of	I
dimensionality	I
if	O
the	O
data	O
has	O
non-trivial	O
behaviour	O
over	O
some	O
region	O
in	O
x	O
then	O
we	O
need	O
to	O
cover	O
the	O
region	O
of	O
x	O
space	O
fairly	O
densely	O
with	O
bump	O
type	O
functions	O
in	O
the	O
above	O
case	O
we	O
used	O
basis	O
functions	O
for	O
this	O
one	O
dimensional	O
space	O
in	O
dimensions	O
if	O
we	O
wish	O
to	O
cover	O
each	O
dimension	O
to	O
the	O
same	O
discretisation	O
level	O
we	O
would	O
need	O
basis	O
functions	O
similarly	O
for	O
dimensions	O
we	O
would	O
need	O
functions	O
to	O
fit	O
such	O
an	O
lpm	O
would	O
require	O
solving	B
a	O
linear	B
system	O
in	O
more	O
than	O
variables	O
this	O
explosion	O
in	O
the	O
number	O
of	O
basis	O
functions	O
with	O
the	O
input	O
dimension	O
is	O
a	O
curse	B
of	I
dimensionality	I
a	O
possible	O
remedy	O
is	O
to	O
make	O
the	O
basis	O
functions	O
very	O
broad	O
so	O
that	O
each	O
covers	O
more	O
of	O
the	O
high	O
dimensional	O
space	O
however	O
this	O
will	O
mean	B
a	O
lack	O
of	O
flexibility	O
of	O
the	O
fitted	O
function	B
since	O
it	O
is	O
constrained	O
to	O
be	O
smooth	O
another	O
approach	B
is	O
to	O
place	O
basis	O
functions	O
centred	O
on	O
the	O
training	B
input	O
points	O
and	O
add	O
some	O
more	O
basis	O
functions	O
randomly	O
placed	O
close	O
to	O
the	O
training	B
inputs	O
the	O
rational	O
behind	O
this	O
is	O
that	O
when	O
we	O
come	O
to	O
do	O
prediction	O
we	O
will	O
most	O
likely	O
see	O
novel	O
x	O
that	O
are	O
close	O
to	O
the	O
training	B
points	O
we	O
do	O
not	O
need	O
to	O
make	O
accurate	O
predictions	O
over	O
all	O
the	O
space	O
a	O
further	O
approach	B
is	O
to	O
make	O
the	O
positions	O
of	O
the	O
basis	O
functions	O
adaptive	O
allowing	O
them	O
to	O
be	O
moved	O
around	O
in	O
the	O
space	O
to	O
minimise	O
the	O
error	O
this	O
approach	B
motivates	O
the	O
neural	B
network	I
an	O
alternative	O
is	O
to	O
reexpress	O
the	O
problem	B
of	O
fitting	O
an	O
lpm	O
by	O
reparameterising	O
the	O
problem	B
as	O
discussed	O
below	O
the	O
dual	B
representation	I
and	O
kernels	O
consider	O
a	O
set	O
of	O
training	B
data	O
with	O
inputs	O
x	O
n	O
n	O
and	O
corresponding	O
outputs	O
yn	O
n	O
n	O
for	O
an	O
lpm	O
of	O
the	O
form	O
fx	O
wtx	O
our	O
interest	O
is	O
to	O
find	O
the	O
best	O
fit	O
parameters	O
w	O
we	O
assume	O
that	O
we	O
have	O
found	O
an	O
optimal	O
parameter	B
w	O
the	O
nullspace	O
of	O
x	O
are	O
those	O
x	O
which	O
are	O
orthogonal	B
to	O
all	O
the	O
inputs	O
in	O
x	O
that	O
is	O
xn	O
x	O
w	O
x	O
for	O
all	O
n	O
if	O
we	O
then	O
consider	O
the	O
vector	O
w	O
with	O
an	O
additional	O
component	O
in	O
the	O
direction	O
orthogonal	B
to	O
the	O
space	O
spanned	O
by	O
x	O
xn	O
wt	O
xn	O
figure	O
the	O
validation	B
error	O
as	O
a	O
function	B
of	O
the	O
basis	O
function	B
width	O
for	O
the	O
validation	B
data	O
in	O
and	O
rbfs	O
in	O
based	O
on	O
the	O
validation	B
error	O
the	O
optimal	O
setting	O
of	O
the	O
basis	O
function	B
width	O
parameter	B
is	O
draft	O
march	O
error	O
the	O
dual	B
representation	I
and	O
kernels	O
the	O
output	O
of	O
an	O
rbf	O
figure	O
here	O
function	B
exp	O
the	O
combined	O
and	O
output	O
for	O
two	O
rbfs	O
with	O
as	O
above	O
and	O
this	O
means	O
that	O
adding	O
a	O
contribution	O
to	O
w	O
outside	O
of	O
the	O
space	O
spanned	O
by	O
x	O
has	O
no	O
effect	O
on	O
the	O
predictions	O
on	O
the	O
train	O
data	O
if	O
the	O
training	B
criterion	O
depends	O
only	O
on	O
how	O
well	O
the	O
lpm	O
predicts	O
the	O
train	O
data	O
there	O
is	O
therefore	O
no	O
need	O
to	O
consider	O
contributions	O
to	O
w	O
from	O
outside	O
of	O
x	O
that	O
is	O
without	O
loss	O
of	O
generality	O
we	O
may	O
consider	O
the	O
representation	O
w	O
anxn	O
the	O
parameters	O
a	O
an	O
are	O
called	O
the	O
dual	B
parameters	I
we	O
can	O
then	O
write	O
the	O
output	O
of	O
the	O
lpm	O
directly	O
in	O
terms	O
of	O
the	O
dual	B
parameters	I
wtxn	O
am	O
xn	O
more	O
generally	O
for	O
a	O
vector	O
function	B
the	O
solution	O
will	O
lie	O
in	O
the	O
space	O
spanned	O
by	O
w	O
an	O
and	O
we	O
may	O
write	O
wt	O
am	O
amk	O
xn	O
where	O
we	O
have	O
defined	O
a	O
kernel	B
function	B
k	O
xn	O
in	O
matrix	B
form	O
the	O
output	O
of	O
the	O
lpm	O
on	O
a	O
training	B
input	O
x	O
is	O
then	O
wt	O
atkn	O
where	O
kn	O
is	O
the	O
nth	O
column	O
of	O
the	O
gram	B
matrix	B
k	O
regression	B
in	O
the	O
dual-space	O
for	O
ordinary	B
least	I
squares	I
regression	B
using	O
equation	B
we	O
have	O
a	O
train	O
error	O
yn	O
ea	O
is	O
analogous	O
to	O
the	O
standard	O
regression	B
equation	B
on	O
interchanging	O
a	O
for	O
w	O
and	O
kn	O
for	O
similarly	O
the	O
regularisation	B
term	O
can	O
be	O
expressed	O
as	O
draft	O
march	O
wtw	O
anam	O
atka	O
the	O
dual	B
representation	I
and	O
kernels	O
by	O
direct	O
analogy	O
the	O
optimal	O
solution	O
for	O
a	O
is	O
therefore	O
a	O
kn	O
k	O
ynkn	O
we	O
can	O
express	O
the	O
above	O
solution	O
more	O
conveniently	O
by	O
first	O
writing	O
a	O
k	O
i	O
ynk	O
since	O
kn	O
is	O
the	O
nth	O
column	O
of	O
k	O
then	O
k	O
is	O
the	O
nth	O
column	O
of	O
the	O
identity	B
matrix	B
with	O
a	O
little	O
thought	O
we	O
can	O
rewrite	O
equation	B
more	O
simply	O
as	O
a	O
i	O
y	O
where	O
y	O
is	O
the	O
vector	O
with	O
components	O
formed	O
from	O
the	O
training	B
inputs	O
yn	O
using	O
this	O
the	O
prediction	O
for	O
a	O
new	O
input	O
x	O
is	O
given	O
by	O
yx	O
kt	O
i	O
y	O
where	O
the	O
vector	O
k	O
has	O
components	O
k	O
xm	O
this	O
dual	B
space	O
solution	O
shows	O
that	O
predictions	O
can	O
be	O
expressed	O
purely	O
in	O
terms	O
of	O
the	O
kernel	B
k	O
this	O
means	O
that	O
we	O
may	O
dispense	O
with	O
defining	O
the	O
vector	O
functions	O
and	O
define	O
a	O
kernel	B
function	B
directly	O
this	O
approach	B
is	O
also	O
used	O
in	O
gaussian	B
processes	O
and	O
enables	O
us	O
to	O
use	O
effectively	O
very	O
large	O
infinite	O
dimensional	O
vectors	O
without	O
ever	O
explicitly	O
needing	O
to	O
compute	O
them	O
note	O
that	O
the	O
gram	B
matrix	B
k	O
has	O
dimension	O
n	O
n	O
which	O
means	O
that	O
the	O
computational	B
complexity	I
of	O
performing	O
the	O
be	O
prohibitively	O
expensive	O
and	O
numerical	B
approximations	O
are	O
required	O
this	O
is	O
in	O
contrast	O
to	O
the	O
computational	B
complexity	I
of	O
solving	B
the	O
normal	B
equations	I
in	O
the	O
original	O
weight	B
space	O
viewpoint	O
is	O
o	O
matrix	B
inversion	B
in	O
equation	B
is	O
for	O
moderate	O
to	O
large	O
n	O
than	O
this	O
will	O
dim	O
to	O
an	O
extent	O
the	O
dual	B
parameterisation	B
helps	O
us	O
with	O
the	O
curse	B
of	I
dimensionality	I
since	O
the	O
complexity	O
of	O
learning	B
in	O
the	O
dual	B
parameterisation	B
scales	O
cubically	O
with	O
the	O
number	O
of	O
training	B
points	O
not	O
cubically	O
with	O
the	O
dimension	O
of	O
the	O
vector	O
positive	B
definite	I
kernels	O
functions	O
the	O
kernel	B
k	O
in	O
was	O
defined	O
as	O
the	O
scalar	B
product	I
between	O
two	O
vectors	O
and	O
for	O
any	O
set	O
of	O
points	O
xm	O
the	O
resulting	O
matrix	B
zmkmnzn	O
ztkz	O
i	O
m	O
i	O
n	O
i	O
is	O
positive	O
semi-definite	O
since	O
for	O
any	O
z	O
zn	O
n	O
i	O
zm	O
m	O
i	O
zm	O
m	O
i	O
mn	O
i	O
m	O
n	O
i	O
m	O
instead	O
of	O
specifying	O
high-dimensional	O
vectors	O
we	O
may	O
instead	O
specify	O
a	O
function	B
kx	O
that	O
produces	O
a	O
positive	B
definite	I
matrix	B
k	O
for	O
any	O
inputs	O
x	O
such	O
a	O
function	B
is	O
called	O
a	O
covariance	B
function	B
or	O
a	O
positive	O
kernel	B
for	O
example	O
a	O
popular	O
choice	O
is	O
e	O
for	O
this	O
is	O
commonly	O
called	O
the	O
squared	B
exponential	B
kernel	B
for	O
this	O
is	O
known	O
as	O
the	O
ornstein-uhlenbeck	B
kernel	B
covariance	B
functions	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
draft	O
march	O
linear	B
parameter	B
models	O
for	O
classification	B
figure	O
the	O
logistic	B
sigmoid	B
function	B
x	O
the	O
parameter	B
determines	O
the	O
steepness	O
of	O
the	O
sigmoid	B
the	O
full	O
line	O
is	O
for	O
and	O
the	O
dashed	O
for	O
as	O
the	O
logistic	B
sigmoid	B
tends	O
to	O
a	O
heaviside	B
step	I
function	B
the	O
dotted	O
curve	O
is	O
the	O
error	B
function	B
erf	O
x	O
for	O
which	O
closely	O
matches	O
the	O
standard	O
logistic	B
sigmoid	B
with	O
linear	B
parameter	B
models	O
for	O
classification	B
in	O
a	O
binary	O
classification	B
problem	B
we	O
are	O
given	O
some	O
training	B
data	O
d	O
cn	O
n	O
n	O
where	O
the	O
targets	O
c	O
inspired	O
by	O
the	O
lpm	O
regression	B
model	B
we	O
can	O
assign	O
the	O
probability	O
that	O
a	O
novel	O
input	O
x	O
belongs	O
to	O
class	O
using	O
pc	O
fxtw	O
where	O
fx	O
in	O
the	O
statistics	O
literature	O
fx	O
is	O
termed	O
a	O
mean	B
function	B
the	O
inverse	O
function	B
f	O
is	O
the	O
link	O
function	B
two	O
popular	O
choices	O
for	O
the	O
function	B
fx	O
are	O
the	O
and	O
probit	B
functions	O
the	O
logit	B
is	O
given	O
by	O
fx	O
ex	O
ex	O
e	O
x	O
which	O
is	O
also	O
called	O
the	O
logistic	B
sigmoid	B
and	O
written	O
the	O
scaled	O
version	O
is	O
defined	O
as	O
x	O
a	O
closely	O
related	O
model	B
is	O
probit	B
regression	B
which	O
uses	O
in	O
place	O
of	O
the	O
logistic	B
sigmoid	B
the	O
error	B
function	B
the	O
cumulative	O
distribution	B
of	O
the	O
standard	B
normal	B
distribution	B
fx	O
e	O
dt	O
erfx	O
this	O
can	O
also	O
be	O
written	O
in	O
terms	O
of	O
the	O
standard	O
error	B
function	B
x	O
x	O
erfx	O
e	O
dt	O
the	O
shape	O
of	O
the	O
probit	B
and	O
logistic	B
functions	O
are	O
similar	O
under	O
rescaling	O
see	O
we	O
focus	O
below	O
on	O
the	O
logit	B
function	B
similar	O
derivations	O
carry	O
over	O
in	O
a	O
straightforward	O
manner	O
to	O
any	O
monotonic	O
mean	B
function	B
logistic	B
regression	B
logistic	B
regression	B
corresponds	O
to	O
the	O
model	B
pc	O
xtw	O
where	O
b	O
is	O
a	O
scalar	O
and	O
w	O
is	O
a	O
vector	O
as	O
the	O
argument	O
b	O
xtw	O
of	O
the	O
sigmoid	B
function	B
increases	O
the	O
probability	O
x	O
belongs	O
to	O
class	O
increases	O
the	O
decision	B
boundary	B
the	O
decision	B
boundary	B
is	O
defined	O
as	O
that	O
set	O
of	O
x	O
for	O
which	O
pc	O
pc	O
this	O
is	O
given	O
by	O
the	O
hyperplane	B
b	O
xtw	O
draft	O
march	O
linear	B
parameter	B
models	O
for	O
classification	B
w	O
figure	O
the	O
decision	B
boundary	B
pc	O
line	O
for	O
two	O
dimensional	O
data	O
the	O
decision	B
boundary	B
is	O
a	O
line	O
if	O
all	O
the	O
training	B
data	O
for	O
class	O
circles	O
lie	O
on	O
one	O
side	O
of	O
the	O
line	O
and	O
for	O
class	O
circles	O
on	O
the	O
other	O
the	O
data	O
is	O
said	O
to	O
be	O
linearly	B
separable	I
on	O
the	O
side	O
of	O
the	O
hyperplane	B
for	O
which	O
b	O
xtw	O
inputs	O
x	O
are	O
classified	O
as	O
s	O
and	O
on	O
the	O
other	O
side	O
they	O
are	O
classified	O
as	O
s	O
the	O
bias	B
parameter	B
b	O
simply	O
shifts	O
the	O
decision	B
boundary	B
by	O
a	O
constant	O
amount	O
the	O
orientation	O
of	O
the	O
decision	B
boundary	B
is	O
determined	O
by	O
w	O
the	O
normal	B
to	O
the	O
hyperplane	B
see	O
to	O
clarify	O
the	O
geometric	O
interpretation	O
let	O
x	O
be	O
a	O
point	O
on	O
the	O
decision	B
boundary	B
and	O
consider	O
a	O
new	O
point	O
x	O
x	O
w	O
where	O
w	O
is	O
a	O
vector	O
perpendicular	O
to	O
w	O
so	O
that	O
wtw	O
then	O
b	O
wtx	O
b	O
x	O
w	O
b	O
wtx	O
wtw	O
b	O
wtx	O
thus	O
if	O
x	O
is	O
on	O
the	O
decision	B
boundary	B
so	O
is	O
x	O
plus	O
any	O
vector	O
perpendicular	O
to	O
w	O
in	O
d	O
dimensions	O
the	O
space	O
of	O
vectors	O
that	O
are	O
perpendicular	O
to	O
w	O
occupy	O
a	O
d	O
dimensional	O
hyperplane	B
for	O
example	O
if	O
the	O
data	O
is	O
two	O
dimensional	O
the	O
decision	B
boundary	B
is	O
a	O
one	O
dimensional	O
hyperplane	B
a	O
line	O
as	O
depicted	O
in	O
linear	B
separability	I
and	O
linear	B
independence	B
definition	O
separability	O
if	O
all	O
the	O
training	B
data	O
for	O
class	O
lies	O
on	O
one	O
side	O
of	O
a	O
hyperplane	B
and	O
for	O
class	O
on	O
the	O
other	O
the	O
data	O
is	O
said	O
to	O
be	O
linearly	B
separable	I
for	O
d	O
dimensional	O
data	O
provided	O
there	O
are	O
no	O
more	O
than	O
d	O
training	B
points	O
then	O
these	O
are	O
linearly	B
separable	I
provided	O
they	O
are	O
linearly	B
independent	I
to	O
see	O
this	O
let	O
cn	O
if	O
xn	O
is	O
in	O
class	O
and	O
cn	O
if	O
xn	O
is	O
in	O
class	O
for	O
the	O
data	O
to	O
be	O
linearly	B
separable	I
we	O
require	O
wtxn	O
b	O
n	O
n	O
where	O
is	O
an	O
arbitrarily	O
small	O
positive	O
constant	O
the	O
above	O
equations	O
state	O
that	O
each	O
input	O
is	O
just	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	B
if	O
there	O
are	O
n	O
d	O
datapoints	O
the	O
above	O
can	O
be	O
written	O
in	O
matrix	B
form	O
as	O
xw	O
b	O
c	O
where	O
x	O
is	O
a	O
square	O
matrix	B
whose	O
nth	O
column	O
contains	O
xn	O
provided	O
that	O
x	O
is	O
invertible	O
the	O
solution	O
is	O
w	O
x	O
b	O
the	O
bias	B
b	O
can	O
be	O
set	O
arbitrarily	O
this	O
shows	O
that	O
provided	O
the	O
xn	O
are	O
linearly	B
independent	I
we	O
can	O
always	O
find	O
a	O
hyperplane	B
that	O
linearly	O
separates	O
the	O
data	O
provided	O
the	O
data	O
are	O
not-collinear	O
occupying	O
the	O
same	O
d	O
dimensional	O
subspace	O
the	O
bias	B
can	O
be	O
used	O
to	O
improve	O
this	O
to	O
enabling	O
d	O
arbitrarily	O
labelled	B
points	O
to	O
be	O
linearly	O
separated	O
in	O
d	O
dimensions	O
a	O
dataset	O
that	O
is	O
not	O
linearly	B
separable	I
is	O
given	O
by	O
the	O
following	O
four	O
training	B
points	O
and	O
class	O
labels	O
draft	O
march	O
linear	B
parameter	B
models	O
for	O
classification	B
figure	O
the	O
xor	O
problem	B
this	O
is	O
not	O
linearly	B
separable	I
this	O
data	O
represents	O
the	O
xor	B
function	B
and	O
is	O
plotted	O
in	O
this	O
function	B
is	O
not	O
linearly	B
separable	I
since	O
no	O
straight	O
line	O
has	O
all	O
inputs	O
from	O
one	O
class	O
on	O
one	O
side	O
and	O
the	O
other	O
class	O
on	O
the	O
other	O
classifying	O
data	O
which	O
is	O
not	O
linearly	B
separable	I
can	O
only	O
be	O
achieved	O
using	O
a	O
non-linear	B
decision	B
boundary	B
it	O
might	O
be	O
that	O
data	O
is	O
non-linearly	O
separable	O
in	O
the	O
original	O
data	O
space	O
however	O
by	O
mapping	O
to	O
a	O
higher	O
dimension	O
using	O
a	O
non-linear	B
vector	O
function	B
we	O
generate	O
a	O
set	O
of	O
non-linearly	O
dependent	O
highdimensional	O
vectors	O
which	O
can	O
then	O
be	O
separated	O
using	O
a	O
high-dimensional	O
hyperplane	B
we	O
will	O
discuss	O
this	O
in	O
the	O
perceptron	B
the	O
perceptron	B
assigns	O
x	O
to	O
class	O
if	O
b	O
wtx	O
and	O
to	O
class	O
otherwise	O
that	O
is	O
pc	O
xtw	O
where	O
the	O
step	O
function	B
is	O
defined	O
as	O
if	O
we	O
consider	O
the	O
logistic	B
regression	B
model	B
pc	O
b	O
xtw	O
x	O
x	O
and	O
take	O
the	O
limit	O
we	O
have	O
the	O
perceptron	B
like	O
classifier	B
pc	O
b	O
xtw	O
b	O
xtw	O
b	O
xtw	O
the	O
only	O
difference	O
between	O
this	O
probabilistic	B
perceptron	B
and	O
the	O
standard	O
perceptron	B
is	O
in	O
the	O
technical	O
definition	O
of	O
the	O
value	B
of	O
the	O
step	O
function	B
at	O
the	O
perceptron	B
may	O
therefore	O
essentially	O
be	O
viewed	O
as	O
a	O
limiting	O
case	O
of	O
logistic	B
regression	B
maximum	B
likelihood	B
training	B
given	O
a	O
data	O
set	O
d	O
how	O
can	O
we	O
learn	O
the	O
weights	O
to	O
obtain	O
good	O
classification	B
probabilistically	O
if	O
we	O
assume	O
that	O
each	O
data	O
point	O
has	O
been	O
drawn	O
independently	O
from	O
the	O
same	O
distribution	B
that	O
generates	O
the	O
data	O
standard	O
i	O
i	O
d	O
assumption	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
is	O
explicitly	O
the	O
conditional	B
dependence	O
on	O
the	O
parameters	O
b	O
w	O
pdb	O
w	O
pcnxn	O
b	O
wpxn	O
pc	O
b	O
wcn	O
pc	O
b	O
cn	O
pxn	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
cn	O
for	O
this	O
discriminative	B
model	B
we	O
do	O
not	O
model	B
the	O
input	O
distribution	B
px	O
so	O
that	O
we	O
may	O
equivalently	O
consider	O
the	O
log	O
likelihood	B
of	O
the	O
output	O
class	O
variables	O
conditioned	O
on	O
the	O
training	B
inputs	O
for	O
logistic	B
regression	B
this	O
gives	O
lw	O
b	O
cn	O
log	O
wtxn	O
cn	O
log	O
draft	O
march	O
wtxn	O
gradient	B
ascent	O
linear	B
parameter	B
models	O
for	O
classification	B
there	O
is	O
no	O
closed	O
form	O
solution	O
to	O
the	O
maximisation	B
of	O
lw	O
b	O
which	O
needs	O
to	O
be	O
carried	O
out	O
numerically	O
one	O
of	O
the	O
simplest	O
methods	O
is	O
gradient	B
ascent	O
for	O
which	O
the	O
gradient	B
is	O
given	O
by	O
wl	O
bxn	O
here	O
we	O
made	O
use	O
of	O
the	O
derivative	O
relation	O
d	O
dl	O
db	O
b	O
for	O
the	O
logistic	B
sigmoid	B
the	O
derivative	O
with	O
respect	O
to	O
the	O
bias	B
is	O
the	O
gradient	B
ascent	O
procedure	O
then	O
corresponds	O
to	O
updating	O
the	O
weights	O
and	O
bias	B
using	O
wnew	O
w	O
wl	O
bnew	O
b	O
dl	O
db	O
where	O
the	O
learning	B
rate	I
is	O
a	O
scalar	O
chosen	O
small	O
enough	O
to	O
ensure	O
convergence	O
the	O
application	O
of	O
the	O
above	O
rule	O
will	O
lead	O
to	O
a	O
gradual	O
increase	O
in	O
the	O
log	O
likelihood	B
batch	B
training	B
writing	O
the	O
updates	O
explicitly	O
gives	O
wnew	O
w	O
bxn	O
bnew	O
b	O
b	O
this	O
is	O
called	O
a	O
batch	B
update	I
since	O
the	O
parameters	O
w	O
and	O
b	O
are	O
updated	O
only	O
after	O
passing	B
through	O
the	O
whole	O
of	O
training	B
data	O
this	O
batch	B
version	O
converges	O
in	O
all	O
cases	O
since	O
the	O
error	O
surface	O
is	O
bowl	O
shaped	O
below	O
for	O
linearly	B
separable	I
data	O
however	O
the	O
optimal	O
setting	O
is	O
for	O
the	O
weights	O
to	O
become	O
infinitely	O
large	O
since	O
then	O
the	O
logistic	B
sigmoids	O
will	O
saturate	O
to	O
or	O
below	O
a	O
stopping	O
criterion	O
based	O
on	O
either	O
minimal	O
changes	O
to	O
the	O
log	O
likelihood	B
or	O
the	O
parameters	O
is	O
therefore	O
required	O
to	O
halt	O
the	O
optimisation	B
routine	O
for	O
non-linearly	O
separable	O
data	O
the	O
likelihood	B
has	O
a	O
maximum	O
at	O
finite	O
w	O
so	O
the	O
algorithm	B
converges	O
however	O
the	O
predictions	O
will	O
be	O
less	O
certain	O
reflected	O
in	O
a	O
broad	O
confidence	O
interval	O
see	O
in	O
batch	B
training	B
the	O
zero	O
gradient	B
criterion	O
is	O
bxn	O
ment	O
that	O
for	O
a	O
zero	O
gradient	B
cn	O
meaning	O
that	O
the	O
weights	O
must	O
tend	O
to	O
infinity	O
for	O
this	O
in	O
the	O
case	O
that	O
the	O
inputs	O
xn	O
n	O
n	O
are	O
linearly	B
independent	I
we	O
immediately	O
have	O
the	O
require	O
condition	O
to	O
hold	O
for	O
linearly	B
separable	I
data	O
we	O
can	O
also	O
show	O
that	O
the	O
weights	O
must	O
become	O
infinite	O
at	O
convergence	O
taking	O
the	O
scalar	B
product	I
of	O
equation	B
with	O
w	O
we	O
have	O
the	O
zero	O
gradient	B
requirement	O
n	O
wtxn	O
where	O
n	O
for	O
simplicity	O
we	O
assume	O
b	O
for	O
linearly	B
separable	I
data	O
we	O
have	O
c	O
c	O
wtxn	O
draft	O
march	O
linear	B
parameter	B
models	O
for	O
classification	B
then	O
using	O
the	O
fact	O
that	O
n	O
we	O
have	O
n	O
wtxn	O
c	O
c	O
figure	O
the	O
decision	B
boundary	B
pc	O
line	O
and	O
confidence	O
boundaries	O
pc	O
and	O
pc	O
and	O
iteraa	O
tions	O
of	O
batch	B
gradient	B
ascent	O
with	O
non-linearly	O
separalinearly	O
separable	O
data	O
ble	O
data	O
note	O
how	O
the	O
confidence	O
interval	O
remains	O
broad	O
see	O
demologreg	O
m	O
each	O
term	O
n	O
wtxn	O
is	O
non-negative	O
and	O
the	O
zero	O
gradient	B
condition	O
requires	O
the	O
sum	O
of	O
these	O
terms	O
to	O
be	O
zero	O
this	O
can	O
only	O
happen	O
if	O
all	O
the	O
terms	O
are	O
zero	O
implying	O
that	O
cn	O
n	O
requiring	O
the	O
sigmoid	B
to	O
saturate	O
for	O
which	O
the	O
weights	O
must	O
be	O
infinite	O
online	B
training	B
in	O
practice	O
it	O
is	O
common	O
to	O
update	O
the	O
parameters	O
after	O
each	O
training	B
example	O
has	O
been	O
considered	O
wnew	O
w	O
n	O
bxn	O
bnew	O
b	O
n	O
b	O
an	O
advantage	O
of	O
online	B
training	B
is	O
that	O
the	O
dataset	O
does	O
not	O
need	O
to	O
be	O
stored	O
since	O
only	O
the	O
performance	B
on	O
the	O
current	O
input	O
is	O
required	O
provided	O
that	O
the	O
data	O
is	O
linearly	B
separable	I
the	O
above	O
online	B
procedure	O
converges	O
is	O
not	O
too	O
large	O
however	O
if	O
the	O
data	O
is	O
not	O
linearly	B
separable	I
the	O
online	B
version	O
will	O
not	O
converge	O
since	O
the	O
opposing	O
class	O
labels	O
will	O
continually	O
pull	O
the	O
weights	O
one	O
way	O
and	O
then	O
the	O
other	O
as	O
each	O
conflicting	O
example	O
is	O
used	O
to	O
form	O
an	O
update	O
for	O
the	O
limiting	O
case	O
of	O
the	O
perceptron	B
with	O
and	O
linearly	B
separable	I
data	O
online	B
updating	O
converges	O
in	O
a	O
finite	O
number	O
of	O
but	O
does	O
not	O
converge	O
for	O
non-linearly	O
separable	O
data	O
geometry	O
of	O
the	O
error	O
surface	O
the	O
hessian	B
of	O
the	O
log	O
likelihood	B
lw	O
is	O
the	O
matrix	B
with	O
n	O
ijn	O
ij	O
hij	O
wiwj	O
xn	O
i	O
xn	O
j	O
n	O
this	O
is	O
negative	O
definite	O
since	O
for	O
any	O
z	O
zihijzj	O
zixn	O
i	O
zjxn	O
j	O
n	O
zixn	O
i	O
in	O
this	O
means	O
that	O
the	O
error	O
surface	O
is	O
concave	O
upside	O
down	O
bowl	O
and	O
gradient	B
ascent	O
is	O
guaranteed	O
to	O
converge	O
to	O
the	O
optimal	O
solution	O
provided	O
the	O
learning	B
rate	I
is	O
small	O
enough	O
example	O
handwritten	B
digits	I
we	O
apply	O
logistic	B
regression	B
to	O
the	O
handwritten	B
digits	I
of	O
in	O
which	O
there	O
are	O
ones	O
and	O
sevens	O
in	O
the	O
train	O
data	O
using	O
gradient	B
ascent	O
training	B
with	O
a	O
suitably	O
chosen	O
stopping	O
criterion	O
the	O
number	O
of	O
errors	O
made	O
on	O
the	O
test	O
points	O
is	O
compared	O
with	O
errors	O
using	O
nearest	B
neighbour	B
methods	O
see	O
for	O
a	O
visualisation	O
of	O
the	O
learned	O
w	O
simplicity	O
we	O
ignore	O
the	O
bias	B
b	O
this	O
can	O
readily	O
be	O
dealt	O
with	O
by	O
extending	O
x	O
to	O
a	O
d	O
dimensional	O
vector	O
x	O
with	O
a	O
in	O
the	O
d	O
component	O
then	O
for	O
a	O
d	O
dimensional	O
w	O
we	O
have	O
wt	O
x	O
wtx	O
draft	O
march	O
the	O
kernel	B
trick	O
for	O
classification	B
figure	O
logistic	B
regression	B
for	O
classifying	O
hand	O
written	O
digits	O
and	O
displayed	O
is	O
a	O
hinton	O
diagram	O
of	O
the	O
learned	O
weight	B
vector	O
w	O
plotted	O
as	O
a	O
image	O
for	O
visual	O
interpretation	O
green	O
are	O
positive	O
and	O
an	O
input	O
x	O
with	O
a	O
value	B
in	O
this	O
component	O
will	O
tend	O
to	O
increase	O
the	O
probability	O
that	O
the	O
input	O
is	O
classed	O
as	O
a	O
similarly	O
inputs	O
with	O
positive	O
contributions	O
in	O
the	O
red	O
regions	O
tend	O
to	O
increase	O
the	O
probability	O
as	O
being	O
classed	O
as	O
a	O
digit	O
note	O
that	O
the	O
elements	O
of	O
each	O
input	O
x	O
are	O
either	O
positive	O
or	O
zero	O
beyond	O
first	O
order	O
gradient	B
ascent	O
since	O
the	O
surface	O
has	O
a	O
single	O
optimum	O
a	O
newton	B
update	I
wnew	O
wold	O
h	O
where	O
h	O
is	O
the	O
hessian	B
matrix	B
as	O
above	O
and	O
will	O
typically	O
converge	O
much	O
faster	O
than	O
gradient	B
ascent	O
for	O
large	O
scale	O
problems	O
the	O
inversion	B
of	O
the	O
hessian	B
is	O
computationally	O
demanding	O
and	O
limited	O
memory	O
bfgs	O
or	O
conjugate	B
gradient	B
methods	O
may	O
be	O
considered	O
as	O
more	O
practical	O
alternatives	O
see	O
avoiding	O
overconfident	O
classification	B
provided	O
the	O
data	O
is	O
linearly	B
separable	I
the	O
weights	O
will	O
continue	O
to	O
increase	O
and	O
the	O
classifications	O
will	O
become	O
extreme	O
this	O
is	O
undesirable	O
since	O
the	O
classifications	O
will	O
be	O
over-confident	O
this	O
can	O
be	O
prevented	O
by	O
adding	O
a	O
penalty	O
term	O
to	O
the	O
objective	O
function	B
l	O
b	O
lw	O
b	O
wtw	O
the	O
scalar	O
constant	O
encourages	O
smaller	O
values	O
of	O
w	O
that	O
we	O
wish	O
to	O
maximise	O
the	O
log	O
likelihood	B
an	O
appropriate	O
value	B
for	O
can	O
be	O
determined	O
using	O
validation	B
data	O
multiple	B
classes	I
for	O
more	O
than	O
two	O
classes	O
one	O
may	O
use	O
the	O
softmax	B
function	B
pc	O
ix	O
ewt	O
i	O
xbi	O
ewt	O
j	O
xbj	O
where	O
c	O
is	O
the	O
number	O
of	O
classes	O
when	O
c	O
this	O
reduced	O
to	O
the	O
logistic	B
sigmoid	B
one	O
can	O
show	O
that	O
the	O
likelihood	B
for	O
this	O
case	O
is	O
also	O
concave	O
see	O
and	O
the	O
kernel	B
trick	O
for	O
classification	B
a	O
drawback	O
of	O
logistic	B
regression	B
as	O
described	O
above	O
is	O
the	O
simplicity	O
of	O
the	O
decision	O
surface	O
a	O
hyperplane	B
one	O
way	O
to	O
extend	O
the	O
method	O
to	O
more	O
complex	O
non-linear	B
decision	O
boundaries	O
is	O
to	O
consider	O
mapping	O
the	O
inputs	O
x	O
in	O
a	O
non-linear	B
way	O
to	O
pc	O
wt	O
b	O
for	O
example	O
the	O
one-dimensional	O
input	O
x	O
could	O
get	O
mapped	O
to	O
a	O
two	O
dimensional	O
vector	O
sinx	O
mapping	O
into	O
a	O
higher	O
dimensional	O
space	O
makes	O
it	O
easier	O
to	O
find	O
a	O
separating	O
hyperplane	B
since	O
any	O
set	O
of	O
points	O
that	O
are	O
linearly	B
independent	I
can	O
be	O
linearly	O
separated	O
provided	O
we	O
have	O
as	O
many	O
dimensions	O
as	O
datapoints	O
draft	O
march	O
support	O
vector	O
machines	O
figure	O
logistic	B
regression	B
pc	O
using	O
a	O
quadratic	O
function	B
iterations	O
of	O
gradient	B
ascent	O
training	B
were	O
performed	O
with	O
a	O
learning	B
rate	I
plotted	O
are	O
the	O
datapoints	O
for	O
the	O
two	O
classes	O
cross	B
and	O
circle	O
and	O
the	O
equal	O
probability	O
contours	O
the	O
decision	B
boundary	B
is	O
the	O
contour	O
see	O
demologregnonlinear	O
m	O
for	O
the	O
maximum	B
likelihood	B
criterion	O
we	O
can	O
use	O
exactly	O
the	O
same	O
algorithm	B
as	O
before	O
on	O
replacing	O
x	O
with	O
see	O
for	O
a	O
demonstration	O
using	O
a	O
quadratic	O
function	B
since	O
only	O
the	O
scalar	B
product	I
between	O
the	O
vectors	O
plays	O
a	O
role	O
the	O
dual	B
representation	I
may	O
be	O
used	O
in	O
which	O
we	O
assume	O
the	O
weight	B
can	O
be	O
expressed	O
in	O
the	O
form	O
n	O
n	O
w	O
n	O
we	O
then	O
subsequently	O
find	O
a	O
solution	O
in	O
terms	O
of	O
the	O
dual	B
parameters	I
n	O
this	O
is	O
potentially	O
advantageous	O
since	O
there	O
may	O
be	O
less	O
training	B
points	O
than	O
dimensions	O
of	O
the	O
classifier	B
depends	O
only	O
on	O
scalar	O
products	O
which	O
can	O
be	O
written	O
in	O
terms	O
of	O
a	O
positive	B
definite	I
kernel	B
n	O
pc	O
ankx	O
xn	O
for	O
convenience	O
we	O
can	O
write	O
the	O
above	O
as	O
pc	O
atkx	O
where	O
the	O
n	O
dimensional	O
vector	O
kx	O
has	O
elements	O
kx	O
xn	O
then	O
the	O
above	O
is	O
of	O
exactly	O
the	O
same	O
form	O
as	O
the	O
original	O
specification	O
of	O
logistic	B
regression	B
namely	O
as	O
a	O
function	B
of	O
a	O
linear	B
combination	O
of	O
vectors	O
hence	O
the	O
same	O
training	B
algorithm	B
to	O
maximise	O
the	O
likelihood	B
can	O
be	O
employed	O
simply	O
on	O
replacing	O
xn	O
with	O
kxn	O
the	O
details	O
are	O
left	O
to	O
the	O
interested	O
reader	O
and	O
follow	O
closely	O
the	O
treatment	O
of	O
gaussian	B
processes	O
for	O
classification	B
support	O
vector	O
machines	O
like	O
kernel	B
logistic	B
regression	B
svms	O
are	O
a	O
form	O
of	O
kernel	B
linear	B
classifier	B
however	O
the	O
svm	O
uses	O
an	O
objective	O
which	O
more	O
explicitly	O
encourages	O
good	O
generalisation	B
performance	B
svms	O
do	O
not	O
fit	O
comfortably	O
within	O
a	O
probabilistic	B
framework	O
and	O
as	O
such	O
we	O
describe	O
them	O
here	O
only	O
briefly	O
referring	O
the	O
reader	O
to	O
the	O
wealth	O
of	O
excellent	O
literature	O
on	O
this	O
the	O
description	O
here	O
is	O
inspired	O
largely	O
by	O
maximum	O
margin	B
linear	B
classifier	B
in	O
the	O
svm	O
literature	O
it	O
is	O
common	O
to	O
use	O
and	O
to	O
denote	O
the	O
two	O
classes	O
for	O
a	O
hyperplane	B
defined	O
by	O
weight	B
w	O
and	O
bias	B
b	O
a	O
linear	B
discriminant	O
is	O
given	O
by	O
wtx	O
b	O
wtx	O
b	O
class	O
class	O
draft	O
march	O
support	O
vector	O
machines	O
figure	O
svm	O
classification	B
of	O
data	O
from	O
two	O
classes	O
circles	O
and	O
filled	O
circles	O
the	O
decision	B
boundary	B
wtx	O
b	O
line	O
for	O
linearly	B
separable	I
data	O
the	O
maximum	O
margin	B
hyperplane	B
is	O
equidistant	O
from	O
the	O
closest	O
opposite	O
class	O
points	O
these	O
support	B
vectors	I
are	O
highlighted	O
in	O
blue	O
and	O
the	O
margin	B
in	O
red	O
the	O
distance	O
of	O
the	O
decision	B
boundary	B
from	O
the	O
origin	O
is	O
b	O
wtw	O
and	O
the	O
distance	O
of	O
a	O
general	O
point	O
x	O
from	O
the	O
origin	O
along	O
the	O
direction	O
w	O
is	O
xtw	O
wtw	O
w	O
origin	O
for	O
a	O
point	O
x	O
that	O
is	O
close	O
to	O
the	O
decision	B
boundary	B
at	O
wtx	O
b	O
a	O
small	O
change	O
in	O
x	O
can	O
lead	O
to	O
a	O
change	O
in	O
classification	B
to	O
make	O
the	O
classifier	B
more	O
robust	O
we	O
therefore	O
impose	O
that	O
for	O
the	O
training	B
data	O
at	O
least	O
the	O
decision	B
boundary	B
should	O
be	O
separated	O
from	O
the	O
data	O
by	O
some	O
finite	O
margin	B
in	O
the	O
first	O
instance	O
that	O
the	O
data	O
is	O
linearly	B
separable	I
wtx	O
b	O
wtx	O
b	O
class	O
class	O
since	O
w	O
b	O
and	O
can	O
all	O
be	O
rescaled	O
arbitrary	O
we	O
need	O
to	O
fix	O
the	O
scale	O
of	O
the	O
above	O
to	O
break	O
this	O
invariance	O
it	O
is	O
convenient	O
to	O
set	O
so	O
that	O
a	O
point	O
x	O
from	O
class	O
that	O
is	O
closest	O
to	O
the	O
decision	B
boundary	B
satisfies	O
wtx	O
b	O
and	O
a	O
point	O
x	O
from	O
class	O
that	O
is	O
closest	O
to	O
the	O
decision	B
boundary	B
satisfies	O
wtx	O
b	O
from	O
vector	B
algebra	I
the	O
distance	O
from	O
the	O
origin	O
along	O
the	O
direction	O
w	O
to	O
a	O
point	O
x	O
is	O
given	O
by	O
wtx	O
wtw	O
the	O
margin	B
between	O
the	O
hyperplanes	O
for	O
the	O
two	O
classes	O
is	O
then	O
the	O
difference	O
between	O
the	O
two	O
distances	O
along	O
the	O
direction	O
w	O
which	O
is	O
wt	O
wtw	O
x	O
wtw	O
to	O
set	O
the	O
distance	O
between	O
the	O
two	O
hyperplanes	O
to	O
be	O
maximal	O
we	O
need	O
to	O
minimise	O
the	O
length	O
wtw	O
given	O
that	O
for	O
each	O
xn	O
we	O
have	O
a	O
corresponding	O
label	O
yn	O
in	O
order	O
to	O
classify	O
the	O
training	B
labels	O
correctly	O
and	O
maximise	O
the	O
margin	B
the	O
optimisation	B
problem	B
is	O
therefore	O
equivalent	B
to	O
subject	O
to	O
minimise	O
wtw	O
this	O
is	O
a	O
quadratic	B
programming	I
problem	B
note	O
that	O
the	O
factor	B
is	O
just	O
for	O
convenience	O
wtxn	O
b	O
n	O
n	O
to	O
account	O
for	O
potentially	O
mislabelled	O
training	B
points	O
for	O
data	O
that	O
is	O
not	O
linearly	B
separable	I
we	O
relax	O
the	O
exact	O
classification	B
constraint	O
and	O
use	O
instead	O
wtxn	O
b	O
n	O
where	O
n	O
here	O
each	O
n	O
measures	O
how	O
far	O
xn	O
is	O
from	O
the	O
correct	O
margin	B
for	O
n	O
datapoint	O
xn	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	B
however	O
for	O
n	O
the	O
datapoint	O
is	O
assigned	O
the	O
opposite	O
class	O
to	O
its	O
training	B
label	O
ideally	O
we	O
want	O
to	O
limit	O
the	O
size	O
of	O
these	O
violations	O
n	O
here	O
we	O
briefly	O
describe	O
two	O
standard	O
approaches	O
draft	O
march	O
support	O
vector	O
machines	O
soft-margin	O
n	O
w	O
origin	O
figure	O
slack	O
margin	B
the	O
term	O
n	O
measures	O
how	O
far	O
a	O
variable	O
is	O
from	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
for	O
its	O
class	O
if	O
n	O
then	O
the	O
point	O
will	O
be	O
misclassified	O
treated	O
as	O
an	O
outlier	B
the	O
soft-margin	O
objective	O
is	O
minimise	O
wtw	O
c	O
n	O
subject	O
to	O
wtxn	O
b	O
n	O
n	O
n	O
where	O
c	O
controls	O
the	O
number	O
of	O
mislabellings	O
of	O
the	O
training	B
data	O
the	O
constant	O
c	O
needs	O
to	O
be	O
determined	O
empirically	O
using	O
a	O
validation	B
set	O
the	O
optimisation	B
problem	B
expressed	O
by	O
can	O
be	O
formulated	O
using	O
the	O
lagrangian	B
lw	O
b	O
for	O
points	O
xn	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	B
which	O
is	O
to	O
be	O
minimised	O
with	O
respect	O
to	O
x	O
b	O
and	O
maximised	O
with	O
respect	O
to	O
wtw	O
c	O
wtxn	O
b	O
n	O
n	O
n	O
so	O
that	O
maximising	O
l	O
with	O
respect	O
to	O
requires	O
the	O
corresponding	O
n	O
to	O
be	O
set	O
to	O
zero	O
only	O
training	B
points	O
that	O
are	O
support	B
vectors	I
lying	O
on	O
the	O
decision	B
boundary	B
will	O
have	O
non-zero	O
n	O
n	O
n	O
differentiating	O
the	O
lagrangian	B
and	O
equating	O
to	O
zero	O
we	O
have	O
the	O
conditions	O
lw	O
b	O
wi	O
lw	O
b	O
n	O
n	O
wi	O
b	O
nynxn	O
i	O
nyn	O
n	O
lw	O
b	O
c	O
n	O
n	O
from	O
this	O
we	O
see	O
that	O
the	O
solution	O
for	O
w	O
is	O
given	O
by	O
w	O
n	O
nynxn	O
since	O
only	O
the	O
support	B
vectors	I
have	O
non-zero	O
n	O
the	O
solution	O
for	O
w	O
will	O
typically	O
depend	O
on	O
only	O
a	O
small	O
number	O
of	O
the	O
training	B
data	O
using	O
these	O
conditions	O
and	O
substituting	O
back	O
into	O
the	O
original	O
problem	B
the	O
objective	O
is	O
equivalent	B
to	O
minimising	O
n	O
l	O
subject	O
to	O
n	O
nm	O
n	O
ynym	O
n	O
m	O
xm	O
n	O
yn	O
n	O
n	O
if	O
we	O
define	O
kxn	O
xm	O
xm	O
draft	O
march	O
support	O
vector	O
machines	O
figure	O
svm	O
training	B
the	O
solid	O
red	O
and	O
solid	O
blue	O
circles	O
represent	O
training	B
data	O
from	O
different	O
classes	O
the	O
support	B
vectors	I
are	O
highlighted	O
in	O
green	O
for	O
the	O
unfilled	O
test	O
points	O
the	O
class	O
assigned	O
to	O
them	O
by	O
the	O
svm	O
is	O
given	O
by	O
the	O
colour	O
see	O
demosvm	O
m	O
the	O
optimisation	B
problem	B
is	O
maximize	O
subject	O
to	O
n	O
n	O
nm	O
yn	O
n	O
n	O
kxn	O
xm	O
c	O
nm	O
ynym	O
n	O
m	O
n	O
optimising	O
this	O
objective	O
is	O
discussed	O
in	O
soft-margin	O
constraint	O
in	O
the	O
soft-margin	O
version	O
one	O
uses	O
a	O
penalty	O
n	O
c	O
n	O
to	O
give	O
the	O
optimisation	B
problem	B
minimise	O
wtwc	O
subject	O
to	O
wtxn	O
b	O
n	O
n	O
n	O
n	O
where	O
c	O
is	O
an	O
empirically	O
determined	O
penalty	O
factor	B
that	O
controls	O
the	O
number	O
of	O
mislabellings	O
of	O
the	O
training	B
data	O
to	O
reformulate	O
the	O
optimisation	B
problem	B
we	O
use	O
the	O
lagrangian	B
lw	O
b	O
wtwc	O
n	O
n	O
n	O
wtxn	O
b	O
n	O
n	O
n	O
rn	O
n	O
n	O
n	O
rn	O
the	O
variables	O
rn	O
are	O
introduced	O
in	O
order	O
to	O
give	O
a	O
non-trivial	O
solution	O
n	O
c	O
following	O
a	O
similar	O
argument	O
as	O
for	O
the	O
case	O
by	O
differentiating	O
the	O
lagrangian	B
and	O
equating	O
to	O
zero	O
we	O
arrive	O
at	O
the	O
optimisation	B
problem	B
maximize	O
subject	O
to	O
n	O
n	O
nm	O
yn	O
n	O
n	O
ynym	O
n	O
mkxn	O
xm	O
n	O
c	O
which	O
is	O
closely	O
related	O
to	O
the	O
problem	B
except	O
that	O
we	O
now	O
have	O
the	O
box-constraint	O
n	O
c	O
using	O
kernels	O
the	O
final	O
objectives	O
and	O
depend	O
on	O
the	O
inputs	O
xn	O
only	O
via	O
the	O
scalar	B
product	I
xn	O
if	O
we	O
map	B
x	O
to	O
a	O
vector	O
function	B
of	O
x	O
then	O
we	O
can	O
write	O
kxn	O
xm	O
this	O
means	O
that	O
we	O
can	O
use	O
any	O
positive	B
definite	I
kernel	B
k	O
and	O
make	O
a	O
non-linear	B
classifier	B
see	O
draft	O
march	O
soft	B
zero-one	B
loss	I
for	O
outlier	B
robustness	O
performing	O
the	O
optimisation	B
both	O
of	O
the	O
above	O
soft-margin	O
svm	O
optimisations	O
problems	O
and	O
are	O
quadratic	O
programs	O
for	O
which	O
the	O
exact	O
computational	O
cost	O
scales	O
as	O
whilst	O
these	O
can	O
be	O
solved	O
with	O
general	O
purpose	O
mal	O
optimisation	B
whose	O
practical	O
performance	B
is	O
typically	O
or	O
better	O
a	O
variant	O
of	O
routines	O
specifically	O
tailored	O
routines	O
that	O
exploit	O
the	O
structure	B
of	O
the	O
problem	B
are	O
preferred	O
in	O
practice	O
of	O
particular	O
practical	O
interest	O
are	O
chunking	B
techniques	O
that	O
optimise	O
over	O
a	O
subset	O
of	O
the	O
in	O
the	O
limit	O
of	O
updating	O
only	O
two	O
components	O
of	O
this	O
can	O
be	O
achieved	O
analytically	O
resulting	O
in	O
the	O
sequential	B
mini	O
this	O
algorithm	B
is	O
provided	O
in	O
svmtrain	O
m	O
once	O
the	O
optimal	O
solution	O
is	O
found	O
the	O
decision	B
function	B
for	O
a	O
new	O
point	O
x	O
is	O
n	O
ynkxn	O
x	O
b	O
assign	O
to	O
class	O
assign	O
to	O
class	O
the	O
optimal	O
b	O
is	O
determined	O
using	O
the	O
maximum	O
margin	B
condition	O
n	O
b	O
min	O
wt	O
xn	O
max	O
yn	O
wt	O
xn	O
probabilistic	B
interpretation	O
kernelised	O
logistic-regression	O
has	O
some	O
of	O
the	O
characteristics	O
of	O
the	O
svm	O
but	O
does	O
not	O
express	O
the	O
large	O
margin	B
requirement	O
also	O
the	O
sparse	B
data	O
usage	O
of	O
the	O
svm	O
is	O
similar	O
to	O
that	O
of	O
the	O
relevance	B
vector	I
machine	I
we	O
discuss	O
in	O
however	O
a	O
probabilistic	B
model	B
whose	O
map	B
assignment	O
matches	O
exactly	O
the	O
svm	O
is	O
hampered	O
by	O
the	O
normalisation	B
requirement	O
for	O
a	O
probability	O
distribution	B
whilst	O
arguably	O
no	O
fully	O
satisfactory	O
direct	O
match	O
between	O
the	O
svm	O
and	O
a	O
related	O
probabilistic	B
model	B
has	O
been	O
achieved	O
approximate	B
matches	O
have	O
been	O
soft	B
zero-one	B
loss	I
for	O
outlier	B
robustness	O
both	O
the	O
support	B
vector	I
machine	I
and	O
logistic	B
regression	B
are	O
potentially	O
mislead	O
by	O
outliers	O
for	O
the	O
svm	O
a	O
mislabelled	O
datapoint	O
that	O
is	O
far	O
from	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	B
would	O
require	O
a	O
large	O
slack	O
however	O
since	O
exactly	O
such	O
large	O
are	O
discouraged	O
it	O
is	O
unlikely	O
that	O
the	O
svm	O
would	O
admit	O
such	O
a	O
solution	O
for	O
logistic	B
regression	B
the	O
probability	O
of	O
generating	O
a	O
mislabelled	O
point	O
far	O
from	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	B
is	O
so	O
exponentially	O
small	O
that	O
this	O
will	O
never	O
happen	O
in	O
practice	O
this	O
means	O
that	O
the	O
model	B
trained	O
with	O
maximum	B
likelihood	B
will	O
never	O
present	O
such	O
a	O
solution	O
in	O
both	O
cases	O
therefore	O
mislabelled	O
points	O
outliers	O
have	O
a	O
significant	O
impact	O
on	O
the	O
location	O
of	O
the	O
decision	B
boundary	B
a	O
robust	O
technique	O
to	O
deal	O
with	O
outliers	O
is	O
to	O
use	O
the	O
zero-one	B
loss	I
in	O
which	O
a	O
mislabeled	O
point	O
contributes	O
only	O
a	O
relatively	O
small	O
loss	O
soft	B
variants	O
of	O
this	O
are	O
obtained	O
by	O
using	O
the	O
objective	O
wtxn	O
wtw	O
which	O
is	O
to	O
be	O
minimised	O
with	O
respect	O
to	O
w	O
and	O
b	O
for	O
the	O
first	O
term	O
above	O
tends	O
to	O
the	O
zero-one	B
loss	I
the	O
second	O
term	O
represents	O
a	O
penalty	O
on	O
the	O
length	O
of	O
w	O
and	O
prevents	O
overfitting	B
kernel	B
extensions	O
of	O
this	O
soft	B
zero-one	B
loss	I
are	O
straightforward	O
unfortunately	O
the	O
objective	O
is	O
highly	O
non-convex	O
and	O
finding	O
the	O
optimal	O
w	O
b	O
is	O
computationally	O
difficult	O
a	O
simple-minded	O
scheme	O
is	O
to	O
fix	O
all	O
components	O
of	O
w	O
except	O
one	O
wi	O
and	O
then	O
perform	O
a	O
numerical	B
one-dimensional	O
optimisation	B
over	O
this	O
single	O
parameter	B
wi	O
at	O
the	O
next	O
step	O
another	O
parameter	B
wj	O
is	O
chosen	O
and	O
the	O
procedure	O
repeated	O
until	O
convergence	O
as	O
usual	O
can	O
be	O
set	O
using	O
validation	B
the	O
practical	O
difficulties	O
of	O
minimising	O
non-convex	O
high-dimensional	O
objective	O
functions	O
means	O
that	O
these	O
approaches	O
are	O
rarely	O
used	O
in	O
practice	O
a	O
discussion	O
of	O
practical	O
attempts	O
in	O
this	O
area	O
is	O
given	O
in	O
draft	O
march	O
exercises	O
figure	O
soft	B
zero-one	B
loss	I
decision	B
boundary	B
line	O
versus	O
logistic	B
regression	B
line	O
the	O
number	O
of	O
mis-classified	O
training	B
points	O
using	O
the	O
soft	B
zero-one	B
loss	I
is	O
compared	O
to	O
for	O
logistic	B
regression	B
the	O
penalty	O
was	O
used	O
for	O
the	O
soft-loss	O
with	O
for	O
logistic	B
regression	B
no	O
penalty	O
term	O
was	O
used	O
the	O
outliers	O
have	O
a	O
significant	O
impact	O
on	O
the	O
decision	B
boundary	B
for	O
logistic	B
regression	B
whilst	O
the	O
soft	B
zero-one	B
loss	I
essentially	O
gives	O
up	O
on	O
the	O
outliers	O
and	O
fits	O
a	O
large	O
margin	B
classifier	B
between	O
the	O
remaining	O
points	O
see	O
demosoftloss	O
m	O
an	O
illustration	O
of	O
the	O
difference	O
between	O
logistic	B
regression	B
and	O
this	O
soft	B
zero-one	B
loss	I
is	O
given	O
in	O
which	O
demonstrates	O
how	O
logistic	B
regression	B
is	O
influenced	O
by	O
the	O
mass	O
of	O
the	O
data	O
points	O
whereas	O
the	O
zero-one	B
loss	I
attempts	O
to	O
mimimise	O
the	O
number	O
of	O
mis-classifications	O
whilst	O
maintaining	O
a	O
large	O
margin	B
notes	O
the	O
perceptron	B
has	O
a	O
long	O
history	O
in	O
artificial	O
intelligence	O
and	O
machine	O
learning	B
rosenblatt	O
discussed	O
the	O
perceptron	B
as	O
a	O
model	B
for	O
human	O
learning	B
arguing	O
that	O
its	O
distributive	O
nature	O
input-output	B
patterns	O
are	O
stored	O
in	O
the	O
weight	B
vector	O
is	O
closely	O
related	O
to	O
the	O
kind	O
of	O
information	O
storage	O
believed	O
to	O
be	O
present	O
in	O
biological	O
to	O
deal	O
with	O
non-linear	B
decision	O
boundaries	O
the	O
main	O
thrust	O
of	O
research	O
in	O
the	O
ensuing	O
neural	B
network	I
community	O
was	O
on	O
the	O
use	O
of	O
multilayered	O
structures	O
in	O
which	O
the	O
outputs	O
of	O
perceptrons	O
are	O
used	O
as	O
the	O
inputs	O
to	O
other	O
perceptrons	O
resulting	O
in	O
potentially	O
highly	O
non-linear	B
discriminant	O
functions	O
this	O
line	O
of	O
research	O
was	O
largely	O
inspired	O
by	O
analogies	O
to	O
biological	O
information	O
processing	O
in	O
which	O
layered	O
structures	O
are	O
prevalent	O
such	O
multilayered	O
artificial	O
neural	O
networks	O
are	O
fascinating	O
and	O
once	O
trained	O
are	O
extremely	O
fast	O
in	O
forming	O
their	O
decisions	O
however	O
reliably	O
training	B
these	O
systems	O
is	O
a	O
highly	O
complex	O
task	O
and	O
probabilistic	B
generalisations	O
in	O
which	O
priors	O
are	O
placed	O
on	O
the	O
parameters	O
lead	O
to	O
computational	O
difficulties	O
whilst	O
perhaps	O
less	O
inspiring	O
from	O
a	O
biological	O
viewpoint	O
the	O
alternative	O
route	O
of	O
using	O
the	O
kernel	B
trick	O
to	O
boost	O
the	O
power	O
of	O
a	O
linear	B
classifier	B
has	O
the	O
advantage	O
of	O
ease	O
of	O
training	B
and	O
generalisation	B
to	O
probabilistic	B
variants	O
more	O
recently	O
however	O
there	O
has	O
been	O
a	O
resurgence	O
of	O
interest	O
in	O
the	O
multilayer	O
systems	O
with	O
new	O
heuristics	O
aimed	O
at	O
improving	O
the	O
difficulties	O
in	O
training	B
see	O
for	O
example	O
code	O
democubicpoly	O
m	O
demo	O
of	O
fitting	O
a	O
cubic	O
polynomial	O
demologreg	O
m	O
demo	O
logistic	B
regression	B
logreg	O
m	O
logistic	B
regression	B
gradient	B
ascent	O
training	B
demologregnonlinear	O
m	O
demo	O
of	O
logistic	B
regression	B
with	O
a	O
non-linear	B
svmtrain	O
m	O
svm	O
training	B
using	O
the	O
smo	O
algorithm	B
demosvm	O
m	O
svm	O
demo	O
demosoftloss	O
m	O
softloss	O
demo	O
softloss	O
m	O
softloss	O
function	B
exercises	O
exercise	O
give	O
an	O
example	O
of	O
a	O
two-dimensional	O
dataset	O
for	O
which	O
the	O
data	O
are	O
linearly	B
separable	I
but	O
not	O
linearly	B
independent	I
can	O
you	O
find	O
a	O
dataset	O
which	O
is	O
linearly	B
independent	I
but	O
not	O
linearly	B
separable	I
draft	O
march	O
exercises	O
the	O
fitted	O
lines	O
go	O
through	O
the	O
ynn	O
exercise	O
show	O
that	O
for	O
both	O
ordinary	O
and	O
orthogonal	B
least	I
squares	I
regression	B
fits	O
to	O
data	O
yn	O
n	O
n	O
exercise	O
consider	O
the	O
softmax	B
function	B
for	O
classifying	O
an	O
input	O
vector	O
x	O
into	O
one	O
of	O
c	O
c	O
classes	O
using	O
c	O
ewt	O
ewt	O
x	O
pcx	O
a	O
set	O
of	O
input-class	O
examples	O
is	O
given	O
by	O
d	O
cn	O
n	O
n	O
write	O
down	O
the	O
log-likelihood	O
l	O
of	O
the	O
classes	O
conditional	B
on	O
the	O
inputs	O
assuming	O
that	O
the	O
data	O
is	O
i	O
i	O
d	O
compute	O
the	O
hessian	B
with	O
elements	O
hij	O
wiwj	O
where	O
w	O
is	O
is	O
the	O
stacked	O
vector	O
w	O
wt	O
wt	O
c	O
and	O
show	O
that	O
the	O
hessian	B
is	O
positive	O
semi-definite	O
that	O
is	O
zthz	O
for	O
any	O
z	O
exercise	O
derive	O
from	O
equation	B
the	O
dual	B
optimisation	B
problem	B
equation	B
exercise	O
a	O
datapoint	O
x	O
is	O
projected	O
to	O
a	O
lower	O
dimensional	O
vector	O
x	O
using	O
x	O
mx	O
where	O
m	O
is	O
a	O
fat	O
matrix	B
for	O
a	O
set	O
of	O
data	O
xn	O
n	O
n	O
and	O
corresponding	O
binary	O
class	O
labels	O
yn	O
using	O
logistic	B
regression	B
on	O
the	O
projected	O
datapoints	O
xn	O
corresponds	O
to	O
a	O
form	O
of	O
constrained	O
logistic	B
regression	B
in	O
the	O
original	O
higher	O
dimensional	O
space	O
x	O
explain	O
if	O
it	O
is	O
reasonable	O
to	O
use	O
an	O
algorithm	B
such	O
as	O
pca	B
to	O
first	O
reduce	O
the	O
data	O
dimensionality	O
before	O
using	O
logistic	B
regression	B
exercise	O
the	O
logistic	B
sigmoid	B
function	B
is	O
defined	O
as	O
what	O
is	O
the	O
inverse	O
function	B
exercise	O
given	O
a	O
dataset	O
d	O
cn	O
n	O
n	O
where	O
cn	O
logistic	B
regression	B
uses	O
the	O
model	B
pc	O
b	O
assuming	O
the	O
data	O
is	O
drawn	O
independently	O
and	O
identically	O
show	O
that	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
l	O
with	O
respect	O
to	O
w	O
is	O
wl	O
cn	O
wtxn	O
b	O
xn	O
exercise	O
consider	O
a	O
dataset	O
d	O
cn	O
n	O
n	O
where	O
cn	O
and	O
x	O
is	O
a	O
d	O
dimensional	O
vector	O
show	O
that	O
if	O
the	O
training	B
data	O
is	O
linearly	B
separable	I
with	O
the	O
hyperplane	B
wtx	O
b	O
the	O
data	O
is	O
also	O
separable	O
with	O
the	O
hyperplane	B
wtx	O
b	O
where	O
w	O
w	O
b	O
b	O
for	O
any	O
scalar	O
what	O
consequence	O
does	O
the	O
above	O
result	O
have	O
for	O
maximum	B
likelihood	B
training	B
of	O
linearly	B
separable	I
data	O
exercise	O
consider	O
a	O
dataset	O
d	O
cn	O
n	O
n	O
where	O
cn	O
and	O
x	O
is	O
a	O
n	O
dimensional	O
vector	O
we	O
have	O
n	O
datapoints	O
in	O
a	O
n	O
dimensional	O
space	O
in	O
the	O
text	O
we	O
showed	O
that	O
we	O
can	O
find	O
a	O
hyperplane	B
by	O
b	O
that	O
linearly	O
separates	O
this	O
data	O
we	O
need	O
for	O
each	O
datapoint	O
xn	O
wtxn	O
b	O
where	O
for	O
cn	O
and	O
for	O
cn	O
comment	O
on	O
the	O
relation	O
between	O
maximum	B
likelihood	B
training	B
and	O
the	O
algorithm	B
suggested	O
above	O
draft	O
march	O
exercise	O
given	O
training	B
data	O
d	O
yn	O
n	O
n	O
you	O
decide	O
to	O
fit	O
a	O
regression	B
model	B
y	O
mx	O
c	O
to	O
this	O
data	O
derive	O
an	O
expression	O
for	O
m	O
and	O
c	O
in	O
terms	O
of	O
d	O
using	O
the	O
minimum	O
sum	O
squared	O
error	O
criterion	O
exercise	O
given	O
training	B
data	O
d	O
cn	O
n	O
n	O
cn	O
where	O
x	O
are	O
vector	O
inputs	O
a	O
discriminative	B
model	B
is	O
exercises	O
pc	O
where	O
gx	O
exp	O
and	O
ex	O
is	O
a	O
neural	O
with	O
a	O
single	O
hidden	B
layer	O
and	O
two	O
hidden	B
units	O
x	O
x	O
write	O
down	O
the	O
log	O
likelihood	B
for	O
the	O
class	O
conditioned	O
on	O
the	O
inputs	O
based	O
on	O
the	O
usual	O
i	O
i	O
d	O
as	O
sumption	O
calculate	O
the	O
derivatives	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
network	O
parameters	O
v	O
comment	O
on	O
the	O
relationship	O
between	O
this	O
model	B
and	O
logistic	B
regression	B
comment	O
on	O
the	O
decision	B
boundary	B
of	O
this	O
model	B
draft	O
march	O
chapter	O
bayesian	B
linear	B
models	O
regression	B
with	O
additive	O
gaussian	B
noise	O
the	O
linear	B
models	O
in	O
were	O
trained	O
under	O
maximum	B
likelihood	B
and	O
do	O
not	O
deal	O
with	O
the	O
issue	O
that	O
from	O
a	O
probabilistic	B
perspective	O
parameter	B
estimates	O
are	O
inherently	O
uncertain	B
due	O
to	O
the	O
limited	O
available	O
training	B
data	O
regression	B
refers	O
to	O
inferring	O
a	O
mapping	O
on	O
the	O
basis	O
of	O
observed	O
data	O
d	O
yn	O
n	O
n	O
where	O
yn	O
represents	O
an	O
input-output	B
pair	O
we	O
discuss	O
here	O
the	O
scalar	O
output	O
case	O
vector	O
inputs	O
x	O
with	O
the	O
extension	O
to	O
the	O
vector	O
output	O
case	O
y	O
is	O
straightforward	O
we	O
assume	O
that	O
each	O
output	O
is	O
generated	O
from	O
a	O
model	B
f	O
w	O
where	O
the	O
parameters	O
w	O
of	O
the	O
function	B
f	O
are	O
unknown	O
an	O
output	O
y	O
is	O
generated	O
by	O
the	O
addition	O
of	O
noise	O
to	O
the	O
clean	O
model	B
output	O
the	O
model	B
generates	O
an	O
output	O
y	O
for	O
input	O
x	O
with	O
y	O
f	O
w	O
if	O
the	O
noise	O
is	O
gaussian	B
distributed	O
n	O
probability	O
fx	O
w	O
pyw	O
x	O
n	O
e	O
f	O
if	O
we	O
assume	O
that	O
each	O
data	O
input-output	B
pair	O
is	O
generated	O
identically	O
and	O
independently	O
the	O
likelihood	B
the	O
model	B
generates	O
the	O
data	O
is	O
pdw	O
pynw	O
xnpxn	O
we	O
may	O
use	O
a	O
prior	B
weight	B
distribution	B
pw	O
to	O
quantify	O
our	O
a	O
priori	O
belief	O
in	O
the	O
suitability	O
each	O
parameter	B
setting	O
writing	O
d	O
the	O
posterior	B
weight	B
distribution	B
is	O
then	O
given	O
by	O
pwd	O
pdwpw	O
pdywdxpw	O
using	O
the	O
gaussian	B
noise	O
assumption	O
and	O
for	O
convenience	O
defining	O
this	O
gives	O
log	O
pwd	O
fxn	O
log	O
p	O
n	O
log	O
const	O
note	O
the	O
similarity	O
between	O
equation	B
and	O
the	O
regularised	B
training	B
error	O
equation	B
in	O
the	O
probabilistic	B
framework	O
we	O
identify	O
the	O
choice	O
of	O
a	O
sum	O
square	O
error	O
with	O
the	O
assumption	O
of	O
additive	O
gaussian	B
noise	O
similarly	O
the	O
regularising	O
term	O
is	O
identified	O
with	O
log	O
pw	O
regression	B
with	O
additive	O
gaussian	B
noise	O
w	O
xn	O
yn	O
n	O
figure	O
belief	B
network	I
representation	O
of	O
a	O
bayesian	B
model	B
for	O
regression	B
under	O
the	O
i	O
i	O
d	O
data	O
assumption	O
the	O
hyperparameter	B
acts	O
as	O
a	O
form	O
of	O
regulariser	O
controlling	O
the	O
flexibility	O
of	O
the	O
prior	B
on	O
the	O
weights	O
w	O
the	O
hyperparameter	B
controls	O
the	O
level	O
of	O
noise	O
on	O
the	O
observations	O
bayesian	B
linear	B
parameter	B
models	O
linear	B
parameter	B
models	O
as	O
discussed	O
in	O
have	O
the	O
form	O
fx	O
w	O
wi	O
ix	O
wt	O
where	O
the	O
parameters	O
wi	O
are	O
also	O
called	O
weights	O
and	O
dim	O
w	O
b	O
such	O
models	O
have	O
a	O
linear	B
parameter	B
dependence	O
but	O
may	O
represent	O
a	O
non-linear	B
input-output	B
mapping	O
if	O
the	O
basis	O
functions	O
ix	O
are	O
nonlinear	O
in	O
x	O
since	O
the	O
output	O
scales	O
linearly	O
with	O
w	O
we	O
can	O
discourage	O
extreme	O
output	O
values	O
by	O
penalising	O
large	O
weight	B
values	O
a	O
natural	B
weight	B
prior	B
is	O
thus	O
pw	O
n	O
b	O
wtw	O
e	O
yn	O
wt	O
where	O
the	O
precision	B
is	O
the	O
inverse	O
variance	B
if	O
is	O
large	O
the	O
total	O
squared	O
length	O
of	O
the	O
weight	B
vector	O
w	O
is	O
encouraged	O
to	O
be	O
small	O
under	O
the	O
gaussian	B
noise	O
assumption	O
the	O
posterior	B
distribution	B
is	O
log	O
pw	O
wtw	O
const	O
where	O
represents	O
the	O
hyperparameter	B
set	O
parameters	O
that	O
determine	O
the	O
functions	O
may	O
also	O
be	O
included	O
in	O
the	O
hyperparameter	B
set	O
using	O
the	O
lpm	O
in	O
equation	B
with	O
a	O
gaussian	B
prior	B
equation	B
and	O
completing	O
the	O
square	O
the	O
weight	B
posterior	B
is	O
a	O
gaussian	B
distribution	B
pw	O
n	O
m	O
s	O
where	O
the	O
covariance	B
and	O
mean	B
are	O
given	O
by	O
s	O
i	O
t	O
m	O
s	O
yn	O
the	O
mean	B
prediction	O
for	O
an	O
input	O
x	O
is	O
then	O
given	O
by	O
fx	O
fx	O
wpwd	O
mt	O
similarly	O
the	O
variance	B
of	O
the	O
underlying	O
estimated	O
clean	O
function	B
is	O
varfx	O
wt	O
txs	O
the	O
output	O
variance	B
varfx	O
depends	O
only	O
on	O
the	O
input	O
variables	O
and	O
not	O
on	O
the	O
training	B
outputs	O
y	O
since	O
the	O
additive	O
noise	O
is	O
uncorrelated	O
with	O
the	O
model	B
outputs	O
the	O
predictive	B
variance	B
is	O
varyx	O
varfx	O
and	O
represents	O
the	O
variance	B
of	O
the	O
noisy	O
output	O
for	O
an	O
input	O
x	O
draft	O
march	O
regression	B
with	O
additive	O
gaussian	B
noise	O
figure	O
along	O
the	O
horizontal	O
axis	O
we	O
plot	O
the	O
input	O
x	O
and	O
along	O
the	O
vertical	O
axis	O
the	O
output	O
t	O
prediction	O
using	O
regularised	B
training	B
and	O
fixed	O
hyperparamethe	O
raw	O
input-output	B
training	B
data	O
prediction	O
using	O
ml-ii	B
optimised	O
hyperparameters	O
also	O
plotted	O
are	O
standard	O
error	O
bars	O
on	O
ters	O
the	O
clean	O
underlying	O
example	O
in	O
we	O
show	O
the	O
mean	B
prediction	O
on	O
the	O
data	O
in	O
using	O
gaussian	B
basis	O
functions	O
ix	O
with	O
width	O
and	O
centres	O
ci	O
spread	O
out	O
evenly	O
over	O
the	O
input	O
space	O
from	O
to	O
we	O
set	O
the	O
other	O
hyperparameters	O
by	O
hand	O
to	O
and	O
the	O
prediction	O
severely	O
overfits	O
the	O
data	O
a	O
result	O
of	O
a	O
poor	O
choice	O
of	O
hyperparameter	B
settings	O
this	O
is	O
resolved	O
in	O
using	O
the	O
ml-ii	B
parameters	O
as	O
described	O
below	O
determining	O
hyperparameters	O
ml-ii	B
the	O
hyperparameter	B
posterior	B
distribution	B
is	O
p	O
pd	O
a	O
simple	O
summarisation	O
of	O
the	O
posterior	B
is	O
given	O
by	O
the	O
map	B
assignment	O
which	O
takes	O
the	O
single	O
optimal	O
setting	O
argmax	O
p	O
if	O
the	O
prior	B
belief	O
about	O
the	O
hyperparameters	O
is	O
weak	O
const	O
this	O
is	O
equivalent	B
to	O
using	O
the	O
that	O
maximises	O
the	O
marginal	B
likelihood	B
pd	O
pd	O
wpw	O
this	O
approach	B
to	O
setting	O
hyperparameters	O
is	O
called	O
ml-ii	B
or	O
the	O
evidence	O
in	O
the	O
case	O
of	O
bayesian	B
linear	B
parameter	B
models	O
under	O
gaussian	B
additive	O
noise	O
computing	O
the	O
marginal	B
likelihood	B
equation	B
involves	O
only	O
gaussian	B
integration	O
a	O
direct	O
approach	B
to	O
deriving	O
an	O
expression	O
for	O
the	O
marginal	B
likelihood	B
is	O
to	O
consider	O
yn	O
wt	O
wtw	O
pd	O
wpw	O
exp	O
draft	O
march	O
regression	B
with	O
additive	O
gaussian	B
noise	O
where	O
d	O
by	O
collating	O
terms	O
in	O
w	O
the	O
square	O
the	O
above	O
represents	O
a	O
gaussian	B
in	O
w	O
with	O
additional	O
factors	O
after	O
integrating	O
over	O
this	O
gaussian	B
we	O
have	O
log	O
pd	O
dts	O
log	O
det	O
b	O
log	O
n	O
log	O
n	O
log	O
n	O
see	O
for	O
an	O
alternative	O
expression	O
example	O
using	O
the	O
hyperparameters	O
that	O
optimise	O
expression	O
gives	O
the	O
results	O
in	O
where	O
we	O
plot	O
both	O
the	O
mean	B
predictions	O
and	O
standard	O
predictive	O
error	O
bars	O
this	O
demonstrates	O
that	O
an	O
acceptable	O
setting	O
for	O
the	O
hyperparameters	O
can	O
be	O
obtained	O
by	O
maximising	O
the	O
marginal	B
likelihood	B
learning	B
the	O
hyperparameters	O
using	O
em	B
we	O
can	O
set	O
hyperparameters	O
such	O
as	O
and	O
by	O
maximising	O
the	O
marginal	B
likelihood	B
equation	B
a	O
convenient	O
computational	O
procedure	O
to	O
achieve	O
this	O
is	O
to	O
interpret	O
the	O
w	O
as	O
latent	B
variables	O
and	O
apply	O
the	O
em	B
algorithm	B
in	O
this	O
case	O
the	O
energy	B
term	O
is	O
according	O
to	O
the	O
general	O
em	B
procedure	O
we	O
need	O
to	O
maximise	O
the	O
energy	B
term	O
for	O
a	O
hyperparameter	B
the	O
derivative	O
of	O
the	O
energy	B
is	O
given	O
by	O
for	O
the	O
bayesian	B
lpm	O
with	O
gaussian	B
weight	B
and	O
noise	O
distributions	O
we	O
obtain	O
pwd	O
old	O
e	O
e	O
pdw	O
old	O
log	O
pdw	O
yn	O
mt	O
yn	O
mt	O
new	O
yn	O
wt	O
n	O
e	O
n	O
trace	O
n	O
s	O
s	O
solving	B
for	O
the	O
zero	O
derivatives	O
gives	O
the	O
m-step	B
update	O
pw	O
oldd	O
trace	O
s	O
txn	O
where	O
s	O
is	O
the	O
empirical	B
covariance	B
of	O
the	O
basis-function	O
vectors	O
n	O
n	O
similarly	O
for	O
e	O
b	O
wtw	O
b	O
trace	O
mtm	O
pw	O
oldd	O
which	O
on	O
equating	O
to	O
zero	O
gives	O
the	O
update	O
new	O
b	O
trace	O
mtm	O
where	O
s	O
and	O
m	O
are	O
given	O
in	O
equation	B
an	O
alternative	O
fixed	O
point	O
procedure	O
that	O
is	O
often	O
more	O
rapidly	O
convergent	O
than	O
em	B
is	O
given	O
in	O
equation	B
closed	O
form	O
updates	O
for	O
other	O
hyperparameters	O
such	O
as	O
the	O
width	O
of	O
the	O
basis	O
functions	O
are	O
generally	O
not	O
available	O
and	O
the	O
corresponding	O
energy	B
term	O
needs	O
to	O
be	O
optimised	O
numerically	O
draft	O
march	O
regression	B
with	O
additive	O
gaussian	B
noise	O
figure	O
predictions	O
for	O
an	O
rbf	O
for	O
different	O
widths	O
for	O
each	O
the	O
ml-ii	B
optimal	O
are	O
obtained	O
by	O
running	O
the	O
em	B
procedure	O
to	O
convergence	O
and	O
subsequently	O
used	O
to	O
form	O
the	O
predictions	O
in	O
each	O
panel	O
the	O
dots	O
represent	O
the	O
training	B
points	O
with	O
x	O
along	O
the	O
horizontal	O
axis	O
and	O
y	O
along	O
the	O
vertical	O
axis	O
mean	B
predictions	O
are	O
plotted	O
along	O
with	O
predictive	O
error	O
bars	O
of	O
one	O
standard	B
deviation	I
according	O
to	O
ml-ii	B
the	O
best	O
model	B
corresponds	O
to	O
see	O
the	O
smaller	O
values	O
of	O
overfit	O
the	O
data	O
giving	O
rise	O
to	O
too	O
rough	O
functions	O
the	O
largest	O
values	O
of	O
underfit	O
giving	O
too	O
smooth	O
functions	O
see	O
demobayeslinreg	O
m	O
hyperparameter	B
optimisation	B
using	O
the	O
gradient	B
hyperparameters	O
such	O
as	O
can	O
be	O
set	O
by	O
maximising	O
the	O
marginal	B
likelihood	B
pd	O
w	O
pdw	O
to	O
find	O
the	O
optimal	O
we	O
search	O
for	O
the	O
zero	O
derivative	O
of	O
log	O
pd	O
from	O
equation	B
we	O
can	O
use	O
the	O
general	O
derivative	O
identity	O
to	O
arrive	O
at	O
log	O
pw	O
pw	O
log	O
pd	O
since	O
log	O
pw	O
wtw	O
b	O
log	O
const	O
figure	O
the	O
log	O
marginal	B
likelihood	B
log	O
pd	O
having	O
found	O
the	O
optimal	O
values	O
of	O
the	O
hyperparameters	O
and	O
using	O
ml-ii	B
these	O
optimal	O
values	O
are	O
dependent	O
on	O
according	O
to	O
ml-ii	B
the	O
best	O
model	B
corresponds	O
to	O
draft	O
march	O
marginal	B
likelihoodlambda	O
wtw	O
b	O
pw	O
we	O
obtain	O
log	O
pd	O
wtw	O
pw	O
b	O
setting	O
the	O
derivative	O
to	O
zero	O
the	O
optimal	O
satisfies	O
one	O
may	O
now	O
form	O
a	O
fixed	O
point	O
equation	B
regression	B
with	O
additive	O
gaussian	B
noise	O
new	O
b	O
trace	O
which	O
is	O
in	O
fact	O
a	O
re-derivation	O
of	O
the	O
em	B
procedure	O
for	O
this	O
model	B
for	O
a	O
gaussian	B
posterior	B
pw	O
n	O
m	O
s	O
wtw	O
trace	O
mtm	O
new	O
b	O
trace	O
mtm	O
gull-mackay	O
fixed	O
point	O
iteration	B
from	O
equation	B
we	O
have	O
wtw	O
pw	O
b	O
s	O
mtwt	O
b	O
so	O
that	O
an	O
alternative	O
fixed	O
point	O
is	O
new	O
b	O
s	O
mtm	O
in	O
practice	O
this	O
update	O
converges	O
more	O
rapidly	O
than	O
equation	B
example	O
the	O
basis	O
function	B
widths	O
in	O
we	O
plot	O
the	O
training	B
data	O
for	O
a	O
regression	B
problem	B
using	O
a	O
bayesian	B
lpm	O
a	O
set	O
of	O
radial	B
basis	I
functions	I
are	O
used	O
ix	O
with	O
ci	O
i	O
spread	O
out	O
evenly	O
between	O
and	O
the	O
hyperparameters	O
and	O
are	O
learned	O
by	O
ml-ii	B
under	O
em	B
updating	O
for	O
a	O
fixed	O
width	O
we	O
then	O
present	O
the	O
predictions	O
each	O
time	O
finding	O
the	O
optimal	O
and	O
for	O
this	O
width	O
the	O
optimal	O
joint	B
hyperparameter	B
setting	O
is	O
obtained	O
as	O
described	O
in	O
which	O
shows	O
the	O
marginal	B
log	O
likelihood	B
for	O
a	O
range	O
of	O
widths	O
validation	B
likelihood	B
the	O
hyperparameters	O
found	O
by	O
ml-ii	B
are	O
those	O
which	O
are	O
best	O
at	O
explaining	O
the	O
training	B
data	O
in	O
principle	O
this	O
is	O
different	O
from	O
those	O
that	O
are	O
best	O
for	O
prediction	O
and	O
in	O
practice	O
therefore	O
it	O
is	O
reasonable	O
to	O
set	O
hyperparameters	O
also	O
by	O
validation	B
techniques	O
one	O
such	O
method	O
is	O
to	O
set	O
hyperparameters	O
by	O
minimal	O
prediction	O
error	O
on	O
a	O
validation	B
set	O
another	O
common	O
technique	O
is	O
to	O
set	O
hyperparameters	O
by	O
their	O
likelihood	B
on	O
a	O
validation	B
set	O
val	O
ym	O
w	O
val	O
m	O
m	O
pyvalw	O
pyval	O
draft	O
march	O
regression	B
with	O
additive	O
gaussian	B
noise	O
from	O
which	O
we	O
obtain	O
log	O
pyval	O
where	O
yval	O
val	O
cval	O
vals	O
t	O
and	O
the	O
design	B
matrix	B
t	O
val	O
ym	O
val	O
val	O
val	O
log	O
det	O
cval	O
valmt	O
c	O
val	O
valm	O
the	O
optimal	O
hyperparameters	O
can	O
then	O
be	O
found	O
by	O
maximising	O
with	O
respect	O
to	O
prediction	O
the	O
mean	B
function	B
predictor	O
based	O
on	O
hyperparameters	O
and	O
weights	O
w	O
is	O
given	O
by	O
fx	O
fx	O
wpw	O
fx	O
wpw	O
p	O
the	O
term	O
in	O
curly	O
brackets	O
is	O
the	O
mean	B
predictor	O
for	O
fixed	O
hyperparameters	O
then	O
weights	O
each	O
mean	B
predictor	O
by	O
the	O
posterior	B
probability	O
of	O
the	O
hyperparameter	B
p	O
this	O
is	O
a	O
general	O
recipe	O
for	O
combining	O
model	B
predictions	O
where	O
each	O
model	B
is	O
weighted	O
by	O
its	O
posterior	B
probability	O
however	O
computing	O
the	O
integral	O
over	O
the	O
hyperparameter	B
posterior	B
is	O
numerically	O
challenging	O
and	O
approximations	O
are	O
usually	O
required	O
provided	O
the	O
hyperparameters	O
are	O
well	O
determined	O
by	O
the	O
data	O
we	O
may	O
instead	O
approximate	B
the	O
above	O
hyperparameter	B
integral	O
by	O
finding	O
the	O
map	B
hyperparameters	O
and	O
use	O
fx	O
fx	O
wpw	O
the	O
relevance	B
vector	I
machine	I
the	O
relevance	B
vector	I
machine	I
assumes	O
that	O
only	O
a	O
small	O
number	O
of	O
components	O
of	O
the	O
basis	O
function	B
vector	O
are	O
relevant	O
in	O
determining	O
the	O
solution	O
for	O
w	O
for	O
a	O
predictor	O
fx	O
w	O
wi	O
ix	O
wt	O
it	O
is	O
often	O
the	O
case	O
that	O
some	O
basis	O
functions	O
will	O
be	O
redundant	O
in	O
the	O
sense	O
that	O
a	O
linear	B
combination	O
of	O
the	O
other	O
basis	O
functions	O
can	O
reproduce	O
the	O
training	B
outputs	O
with	O
insignificant	O
loss	O
in	O
accuracy	O
to	O
exploit	O
this	O
effect	O
and	O
seek	O
a	O
parsimonious	O
solution	O
we	O
may	O
use	O
a	O
more	O
refined	O
prior	B
that	O
encourages	O
each	O
wi	O
itself	O
to	O
be	O
small	O
the	O
modifications	O
required	O
to	O
the	O
description	O
of	O
are	O
to	O
replace	O
s	O
with	O
s	O
diag	O
t	O
the	O
marginal	B
likelihood	B
is	O
then	O
given	O
by	O
log	O
pd	O
dts	O
log	O
det	O
log	O
i	O
n	O
log	O
n	O
log	O
the	O
em	B
update	O
for	O
is	O
unchanged	O
and	O
the	O
em	B
update	O
for	O
each	O
i	O
is	O
new	O
i	O
i	O
draft	O
march	O
pw	O
i	O
pwi	O
i	O
n	O
where	O
the	O
prior	B
on	O
each	O
individual	O
weight	B
is	O
given	O
by	O
i	O
i	O
i	O
i	O
e	O
pwi	O
i	O
algorithm	B
evidence	B
procedure	I
for	O
bayesian	B
logistic	B
regression	B
initialise	O
w	O
and	O
while	O
not	O
converged	O
do	O
end	O
while	O
find	O
optimal	O
w	O
by	O
iterating	O
equation	B
equation	B
to	O
convergence	O
update	O
according	O
to	O
equation	B
classification	B
for	O
the	O
logistic	B
regression	B
model	B
i	O
pc	O
x	O
wi	O
ix	O
classification	B
e-step	B
m-step	B
the	O
maximum	B
likelihood	B
method	O
returns	O
only	O
a	O
single	O
optimal	O
w	O
to	O
deal	O
with	O
the	O
inevitable	O
uncertainty	B
in	O
estimating	O
w	O
we	O
need	O
to	O
determine	O
the	O
posterior	B
distribution	B
of	O
the	O
weights	O
w	O
to	O
do	O
so	O
we	O
first	O
define	O
a	O
prior	B
on	O
the	O
weights	O
pw	O
for	O
which	O
a	O
convenient	O
choice	O
is	O
a	O
gaussian	B
e	O
pw	O
n	O
where	O
is	O
the	O
inverse	O
variance	B
called	O
the	O
precision	B
given	O
a	O
dataset	O
of	O
input-class	O
labels	O
d	O
cn	O
n	O
n	O
the	O
parameter	B
posterior	B
is	O
pd	O
pw	O
pw	O
pdw	O
pcnxn	O
w	O
pd	O
unfortunately	O
this	O
distribution	B
is	O
not	O
of	O
any	O
standard	O
form	O
and	O
exactly	O
inferring	O
statistics	O
such	O
as	O
the	O
mean	B
or	O
the	O
most	O
probable	O
value	B
are	O
formally	O
computationally	O
intractable	O
hyperparameter	B
optimisation	B
hyperparameters	O
such	O
as	O
can	O
be	O
set	O
by	O
maximising	O
the	O
marginal	B
likelihood	B
pd	O
pdwpw	O
w	O
pcnxn	O
w	O
w	O
e	O
wtw	O
there	O
are	O
several	O
approaches	O
one	O
could	O
take	O
to	O
approximate	B
this	O
and	O
below	O
we	O
discuss	O
the	O
laplace	B
and	O
a	O
variational	O
technique	O
common	O
to	O
all	O
approaches	O
however	O
is	O
the	O
form	O
of	O
the	O
gradient	B
differing	O
only	O
in	O
the	O
statistics	O
under	O
an	O
approximation	B
to	O
the	O
posterior	B
for	O
this	O
reason	O
we	O
derive	O
first	O
generic	O
hyperparameter	B
update	O
formulae	O
that	O
apply	O
under	O
all	O
approximations	O
to	O
find	O
the	O
optimal	O
we	O
search	O
for	O
the	O
zero	O
derivative	O
of	O
log	O
pd	O
this	O
is	O
equivalent	B
to	O
the	O
linear	B
regression	B
case	O
and	O
we	O
immediately	O
obtain	O
setting	O
the	O
derivative	O
to	O
zero	O
an	O
exact	O
equation	B
is	O
that	O
the	O
optimal	O
satisfies	O
wtw	O
b	O
pw	O
log	O
pd	O
wtw	O
pw	O
b	O
one	O
may	O
now	O
form	O
a	O
fixed	O
point	O
equation	B
new	O
b	O
draft	O
march	O
classification	B
the	O
averages	O
in	O
the	O
above	O
expression	O
cannot	O
be	O
computed	O
exactly	O
and	O
are	O
replaced	O
by	O
averages	O
with	O
respect	O
an	O
approximation	B
of	O
the	O
posterior	B
qw	O
note	O
that	O
since	O
we	O
only	O
have	O
an	O
approximation	B
to	O
the	O
posterior	B
and	O
therefore	O
the	O
mean	B
and	O
covariance	B
statistics	O
we	O
cannot	O
guarantee	O
that	O
the	O
likelihood	B
will	O
always	O
increase	O
for	O
a	O
gaussian	B
approximation	B
of	O
the	O
posterior	B
qw	O
n	O
m	O
s	O
wtw	O
trace	O
trace	O
mtm	O
new	O
b	O
trace	O
mtm	O
in	O
this	O
case	O
the	O
gull-mackay	O
alternative	O
fixed	O
point	O
is	O
new	O
b	O
s	O
mtm	O
the	O
hyperparameter	B
updates	O
and	O
have	O
the	O
same	O
form	O
as	O
for	O
the	O
regression	B
model	B
the	O
mean	B
m	O
and	O
covariance	B
s	O
of	O
the	O
posterior	B
in	O
the	O
regression	B
and	O
classification	B
cases	O
are	O
however	O
different	O
in	O
the	O
classification	B
case	O
we	O
need	O
to	O
approximate	B
the	O
mean	B
and	O
covariance	B
as	O
discussed	O
below	O
laplace	B
approximation	B
the	O
weight	B
posterior	B
is	O
given	O
by	O
ew	O
pw	O
e	O
where	O
ew	O
wtw	O
log	O
hn	O
n	O
by	O
approximating	O
ew	O
by	O
a	O
quadratic	O
function	B
in	O
w	O
we	O
obtain	O
a	O
gaussian	B
approximation	B
qwd	O
to	O
pwd	O
to	O
do	O
so	O
we	O
first	O
find	O
the	O
minimum	O
of	O
ew	O
differentiating	O
we	O
obtain	O
it	O
is	O
convenient	O
to	O
use	O
a	O
newton	O
method	O
to	O
find	O
the	O
optimum	O
the	O
hessian	B
matrix	B
with	O
elements	O
e	O
w	O
nhn	O
n	O
wi	O
wj	O
ew	O
hij	O
is	O
given	O
by	O
h	O
i	O
n	O
n	O
nt	O
j	O
note	O
that	O
the	O
hessian	B
is	O
positive	O
semidefinite	O
so	O
that	O
the	O
function	B
ew	O
is	O
convex	O
shaped	O
and	O
finding	O
a	O
minimum	O
of	O
ew	O
is	O
numerically	O
unproblematic	O
a	O
newton	B
update	I
then	O
is	O
wnew	O
w	O
h	O
e	O
given	O
a	O
converged	O
w	O
the	O
posterior	B
approximation	B
is	O
given	O
by	O
qwd	O
n	O
m	O
s	O
where	O
m	O
w	O
is	O
the	O
converged	O
estimate	O
of	O
the	O
minimum	O
point	O
of	O
ew	O
and	O
h	O
is	O
the	O
hessian	B
of	O
ew	O
at	O
this	O
point	O
s	O
h	O
draft	O
march	O
classification	B
wtw	O
e	O
e	O
w	O
ew	O
approximating	O
the	O
marginal	B
likelihood	B
the	O
marginal	B
likelihood	B
is	O
given	O
by	O
pd	O
pdwpw	O
w	O
w	O
pcnxn	O
w	O
log	O
n	O
for	O
an	O
optimum	O
value	B
m	O
w	O
we	O
approximate	B
the	O
marginal	B
likelihood	B
using	O
log	O
pd	O
l	O
log	O
det	O
i	O
j	O
b	O
log	O
given	O
this	O
approximation	B
l	O
to	O
the	O
marginal	B
likelihood	B
an	O
alternative	O
strategy	O
for	O
hyperparameter	B
optimisation	B
is	O
to	O
optimises	O
l	O
with	O
respect	O
to	O
by	O
differentiating	O
l	O
directly	O
the	O
reader	O
may	O
show	O
that	O
the	O
resulting	O
updates	O
are	O
in	O
fact	O
equivalent	B
to	O
using	O
the	O
general	O
condition	O
equation	B
under	O
a	O
laplace	B
approximation	B
to	O
the	O
posterior	B
statistics	O
making	O
predictions	O
ultimately	O
our	O
interest	O
is	O
to	O
classify	O
in	O
novel	O
situations	O
averaging	O
over	O
posterior	B
weight	B
uncertainty	B
pc	O
w	O
pc	O
wpw	O
the	O
b	O
dimensional	O
integrals	O
over	O
w	O
cannot	O
be	O
computed	O
analytically	O
and	O
numerical	B
approximation	B
is	O
required	O
in	O
this	O
particular	O
case	O
the	O
relative	O
benign	O
nature	O
of	O
the	O
posterior	B
log	O
posterior	B
is	O
concave	O
see	O
below	O
suggests	O
that	O
a	O
simple	O
laplace	B
approximation	B
may	O
suffice	O
for	O
a	O
variational	O
approximation	B
to	O
make	O
a	O
class	O
prediction	O
for	O
a	O
novel	O
input	O
x	O
we	O
use	O
pc	O
pc	O
wpwd	O
xtw	O
pwd	O
however	O
since	O
the	O
term	O
depends	O
on	O
w	O
via	O
the	O
scalar	B
product	I
xtw	O
we	O
only	O
require	O
the	O
integral	O
to	O
compute	O
the	O
predictions	O
it	O
would	O
appear	O
that	O
we	O
need	O
to	O
carry	O
out	O
an	O
integral	O
in	O
b	O
dimensions	O
over	O
the	O
one-dimensional	O
projection	B
h	O
xtw	O
so	O
that	O
pc	O
phxd	O
h	O
xtm	O
xt	O
x	O
phxd	O
n	O
under	O
the	O
laplace	B
approximation	B
w	O
is	O
gaussian	B
pwd	O
n	O
m	O
s	O
since	O
h	O
is	O
a	O
projection	B
of	O
w	O
h	O
is	O
also	O
gaussian	B
distributed	O
predictions	O
may	O
then	O
be	O
made	O
by	O
numerically	O
evaluating	O
the	O
one-dimensional	O
integral	O
over	O
the	O
gaussian	B
distribution	B
in	O
h	O
equation	B
approximating	O
the	O
gaussian	B
average	B
of	O
a	O
logistic	B
sigmoid	B
predictions	O
under	O
a	O
gaussian	B
posterior	B
approximation	B
require	O
the	O
computation	O
of	O
i	O
draft	O
march	O
classification	B
figure	O
bayesian	B
logistic	B
regression	B
with	O
the	O
rbf	O
e	O
placing	O
basis	O
functions	O
centred	O
on	O
a	O
subset	O
of	O
the	O
training	B
points	O
the	O
green	O
points	O
are	O
training	B
data	O
from	O
class	O
and	O
the	O
red	O
points	O
are	O
training	B
data	O
from	O
class	O
the	O
contours	O
represent	O
the	O
probability	O
of	O
being	O
in	O
class	O
the	O
optimal	O
value	B
of	O
found	O
by	O
the	O
evidence	B
procedure	I
in	O
this	O
case	O
is	O
is	O
set	O
by	O
hand	O
to	O
see	O
demobayeslogregression	O
m	O
where	O
xt	O
xt	O
x	O
gaussian	B
quadrature	O
is	O
an	O
obvious	O
numerical	B
an	O
alternative	O
is	O
to	O
replace	O
the	O
logistic	B
sigmoid	B
by	O
a	O
suitably	O
transformed	O
erf	O
the	O
reason	O
being	O
that	O
the	O
gaussian	B
average	B
of	O
an	O
erf	O
function	B
is	O
another	O
erf	O
function	B
using	O
a	O
single	O
erf	O
an	O
approximation	B
erf	O
x	O
these	O
two	O
functions	O
agree	O
at	O
a	O
reasonable	O
criterion	O
is	O
that	O
the	O
derivatives	O
of	O
these	O
two	O
should	O
agree	O
at	O
x	O
since	O
then	O
they	O
have	O
locally	O
the	O
same	O
slope	O
around	O
the	O
origin	O
and	O
have	O
globally	O
similar	O
shape	O
using	O
and	O
that	O
the	O
derivative	O
is	O
this	O
requires	O
a	O
more	O
accurate	O
approximation	B
can	O
be	O
found	O
by	O
considering	O
ui	O
i	O
erf	O
ix	O
i	O
ui	O
suitable	O
values	O
for	O
ui	O
and	O
i	O
are	O
given	O
in	O
logsigapp	O
m	O
which	O
uses	O
a	O
linear	B
combination	O
of	O
erf	O
functions	O
to	O
approximate	B
the	O
logistic	B
sigmoid	B
to	O
compute	O
the	O
approximate	B
average	B
of	O
over	O
a	O
gaussian	B
one	O
may	O
then	O
make	O
use	O
of	O
the	O
result	O
erf	O
e	O
d	O
dx	O
erf	O
d	O
since	O
e	O
we	O
have	O
erf	O
x	O
dx	O
e	O
erf	O
x	O
dx	O
i	O
uierf	O
i	O
i	O
further	O
approximate	B
statistics	O
can	O
be	O
obtained	O
using	O
the	O
results	O
derived	O
in	O
x	O
that	O
the	O
definition	O
of	O
the	O
erf	O
function	B
used	O
here	O
is	O
taken	O
to	O
be	O
consistent	B
with	O
matlab	O
namely	O
that	O
erfx	O
e	O
dt	O
other	O
authors	O
define	O
it	O
to	O
be	O
the	O
cumulative	O
density	B
function	B
of	O
a	O
standard	O
gaussian	B
x	O
e	O
dt	O
draft	O
march	O
classification	B
figure	O
classification	B
using	O
the	O
rvm	O
with	O
rbf	O
e	O
placing	O
a	O
basis	O
function	B
on	O
a	O
subset	O
of	O
the	O
training	B
data	O
points	O
the	O
green	O
points	O
are	O
training	B
data	O
from	O
class	O
and	O
the	O
red	O
points	O
are	O
training	B
data	O
from	O
class	O
the	O
contours	O
represent	O
the	O
probability	O
of	O
being	O
in	O
class	O
training	B
points	O
the	O
training	B
points	O
weighted	O
by	O
their	O
relevance	O
value	B
n	O
nearly	O
all	O
the	O
points	O
have	O
a	O
value	B
so	O
small	O
that	O
they	O
effectively	O
vanish	O
see	O
demobayeslogregrvm	O
m	O
relevance	B
vector	I
machine	I
for	O
classification	B
in	O
adopting	O
the	O
rvm	O
prior	B
to	O
classification	B
we	O
encourage	O
individual	O
weights	O
to	O
be	O
small	O
pw	O
i	O
where	O
pwi	O
i	O
n	O
pwi	O
i	O
wi	O
i	O
n	O
the	O
only	O
alterations	O
in	O
the	O
previous	O
evidence	B
procedure	I
are	O
ei	O
iwi	O
nhn	O
i	O
h	O
diag	O
j	O
these	O
are	O
used	O
in	O
the	O
newton	B
update	I
formula	O
as	O
before	O
the	O
em	B
update	O
equation	B
for	O
the	O
s	O
is	O
given	O
by	O
new	O
i	O
i	O
sii	O
where	O
h	O
similarly	O
the	O
gull-mackay	O
update	O
is	O
given	O
by	O
new	O
i	O
isii	O
i	O
running	O
this	O
procedure	O
one	O
typically	O
finds	O
that	O
many	O
of	O
the	O
s	O
tend	O
to	O
infinity	O
and	O
the	O
corresponding	O
weights	O
are	O
pruned	O
from	O
the	O
system	O
the	O
remaining	O
weights	O
tend	O
correspond	O
to	O
basis	O
functions	O
the	O
rbf	O
case	O
in	O
the	O
centres	O
of	O
mass	O
of	O
clusters	O
of	O
datapoints	O
of	O
the	O
same	O
class	O
see	O
contrast	O
this	O
with	O
the	O
situation	O
in	O
svms	O
where	O
the	O
retained	O
datapoints	O
tend	O
to	O
be	O
on	O
the	O
decision	O
boundaries	O
the	O
number	O
of	O
training	B
points	O
retained	O
by	O
the	O
rvm	O
tends	O
to	O
be	O
very	O
small	O
smaller	O
indeed	O
that	O
the	O
number	O
retained	O
in	O
the	O
svm	O
framework	O
whilst	O
the	O
rvm	O
does	O
not	O
support	O
large	O
margins	O
and	O
hence	O
may	O
be	O
a	O
less	O
robust	O
classifier	B
it	O
does	O
retain	O
the	O
advantages	O
of	O
a	O
probabilistic	B
a	O
potential	B
critique	O
of	O
the	O
rvm	O
coupled	B
with	O
an	O
ml-ii	B
procedure	O
for	O
learning	B
the	O
i	O
is	O
that	O
it	O
is	O
overly	O
aggressive	O
in	O
terms	O
of	O
pruning	O
indeed	O
as	O
one	O
may	O
verify	O
running	O
demobayeslogregrvm	O
m	O
it	O
is	O
common	O
to	O
find	O
an	O
instance	O
of	O
a	O
problem	B
for	O
which	O
there	O
exists	O
a	O
set	O
of	O
i	O
such	O
that	O
the	O
training	B
data	O
can	O
be	O
classified	O
perfectly	O
however	O
after	O
using	O
ml-ii	B
so	O
many	O
of	O
the	O
i	O
are	O
set	O
to	O
zero	O
that	O
the	O
training	B
data	O
can	O
no	O
longer	O
be	O
classified	O
perfectly	O
draft	O
march	O
exercises	O
multi-class	O
case	O
we	O
briefly	O
note	O
that	O
the	O
multi-class	O
case	O
can	O
be	O
treated	O
by	O
using	O
the	O
softmax	B
function	B
under	O
a	O
one-of-m	O
class	O
coding	O
scheme	O
the	O
class	O
probabilities	O
are	O
pc	O
my	O
eym	O
which	O
automatically	O
enforces	O
the	O
constraint	O
classes	O
the	O
cost	O
of	O
the	O
laplace	B
approximation	B
scales	O
as	O
however	O
one	O
may	O
show	O
by	O
careful	O
implementation	O
that	O
the	O
cost	O
may	O
be	O
reduced	O
to	O
only	O
analogous	O
to	O
the	O
cost	O
savings	O
possible	O
in	O
m	O
pc	O
m	O
naively	O
it	O
would	O
appear	O
that	O
for	O
c	O
the	O
gaussian	B
process	I
classification	B
model	B
code	O
demobayeslinreg	O
m	O
demo	O
of	O
bayesian	B
linear	B
regression	B
bayeslinreg	O
m	O
bayesian	B
linear	B
regression	B
demobayeslogregrvm	O
m	O
demo	O
of	O
bayesian	B
logistic	B
regression	B
bayeslogregressionrvm	O
m	O
bayesian	B
logistic	B
regression	B
avsigmagauss	O
m	O
approximation	B
of	O
the	O
gaussian	B
average	B
of	O
a	O
logistic	B
sigmoid	B
logsigapp	O
m	O
approximation	B
of	O
the	O
logistic	B
sigmoid	B
using	O
mixture	B
of	O
erfs	O
exercises	O
exercise	O
the	O
exercise	O
concerns	O
bayesian	B
regression	B
show	O
that	O
for	O
f	O
wtx	O
and	O
pw	O
n	O
that	O
pfx	O
is	O
gaussian	B
distributed	O
furthermore	O
find	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
gaussian	B
what	O
is	O
pft	O
x	O
consider	O
a	O
target	O
point	O
t	O
f	O
where	O
n	O
exercise	O
a	O
bayesian	B
linear	B
parameter	B
regression	B
model	B
is	O
given	O
by	O
yn	O
wt	O
n	O
in	O
vector	O
notation	O
y	O
this	O
can	O
be	O
written	O
with	O
t	O
and	O
is	O
a	O
zero	O
mean	B
gaussian	B
distributed	O
vector	O
with	O
covariance	B
y	O
w	O
an	O
expression	O
for	O
the	O
marginal	B
likelihood	B
of	O
a	O
dataset	O
is	O
given	O
in	O
equation	B
a	O
more	O
compact	O
expression	O
can	O
be	O
obtained	O
by	O
considering	O
xn	O
since	O
yn	O
is	O
linearly	O
related	O
to	O
xn	O
through	O
w	O
then	O
y	O
is	O
gaussian	B
distributed	O
with	O
mean	B
and	O
covariance	B
matrix	B
for	O
pw	O
n	O
draft	O
march	O
w	O
w	O
show	O
that	O
the	O
covariance	B
matrix	B
can	O
be	O
expressed	O
as	O
c	O
i	O
t	O
hence	O
show	O
that	O
the	O
log	O
marginal	B
likelihood	B
can	O
we	O
written	O
as	O
log	O
xn	O
log	O
det	O
c	O
ytc	O
exercises	O
exercise	O
using	O
as	O
a	O
basis	O
derive	O
expression	O
for	O
the	O
log	O
likelihood	B
on	O
a	O
validation	B
set	O
exercise	O
consider	O
the	O
function	B
ew	O
as	O
defined	O
in	O
equation	B
compute	O
the	O
hessian	B
matrix	B
which	O
has	O
elements	O
hij	O
wi	O
wj	O
ew	O
ij	O
n	O
n	O
n	O
nt	O
show	O
that	O
the	O
hessian	B
is	O
positive	O
semidefinite	O
exercise	O
show	O
that	O
for	O
any	O
function	B
f	O
fxtwpwdw	O
fhphdh	O
where	O
ph	O
is	O
the	O
distribution	B
of	O
the	O
scalar	O
xtw	O
the	O
significance	O
of	O
this	O
result	O
is	O
that	O
any	O
high-dimensional	O
integral	O
of	O
the	O
above	O
form	O
can	O
be	O
reduced	O
to	O
a	O
one-dimensional	O
integral	O
over	O
the	O
distribution	B
of	O
the	O
field	O
exercise	O
this	O
exercise	O
concerns	O
bayesian	B
logistic	B
regression	B
our	O
interest	O
to	O
derive	O
a	O
formula	O
for	O
the	O
optimal	O
regularisation	B
parameter	B
based	O
on	O
the	O
laplace	B
approximation	B
to	O
the	O
marginal	B
log-likelihood	O
given	O
by	O
log	O
pd	O
l	O
log	O
n	O
log	O
det	O
i	O
j	O
b	O
log	O
n	O
log	O
as	O
in	O
equation	B
the	O
laplace	B
procedure	O
finds	O
first	O
an	O
optimal	O
w	O
that	O
minimises	O
which	O
will	O
depend	O
on	O
the	O
setting	O
of	O
formally	O
therefore	O
in	O
finding	O
the	O
that	O
optimises	O
l	O
we	O
should	O
make	O
use	O
of	O
the	O
total	O
derivative	O
formula	O
dl	O
d	O
l	O
l	O
wi	O
wi	O
however	O
when	O
evaluated	O
at	O
w	O
w	O
l	O
w	O
this	O
means	O
that	O
in	O
order	O
to	O
compute	O
the	O
derivative	O
with	O
respect	O
to	O
we	O
only	O
need	O
consider	O
the	O
terms	O
with	O
an	O
explicit	O
dependence	O
equating	O
the	O
derivative	O
to	O
zero	O
and	O
using	O
log	O
det	O
i	O
show	O
that	O
the	O
optimal	O
satisfies	O
the	O
fixed	O
point	O
equation	B
new	O
n	O
trace	O
i	O
j	O
draft	O
march	O
chapter	O
gaussian	B
processes	O
non-parametric	B
prediction	O
gaussian	B
processes	O
are	O
flexible	O
bayesian	B
models	O
that	O
fit	O
well	O
within	O
the	O
probabilistic	B
modelling	B
framework	O
in	O
developing	O
gps	O
it	O
is	O
useful	O
to	O
first	O
step	O
back	O
and	O
see	O
what	O
information	O
we	O
need	O
to	O
form	O
a	O
predictor	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
yn	O
n	O
n	O
x	O
y	O
where	O
xn	O
is	O
the	O
input	O
for	O
datapoint	O
n	O
and	O
yn	O
the	O
corresponding	O
output	O
continuous	B
variable	O
in	O
the	O
regression	B
case	O
and	O
a	O
discrete	B
variable	O
in	O
the	O
classification	B
case	O
our	O
aim	O
is	O
to	O
make	O
a	O
prediction	O
y	O
for	O
a	O
new	O
input	O
x	O
in	O
the	O
discriminative	B
framework	O
no	O
model	B
of	O
the	O
inputs	O
x	O
is	O
assumed	O
and	O
only	O
the	O
outputs	O
are	O
modelled	O
conditioned	O
on	O
the	O
inputs	O
given	O
a	O
joint	B
model	B
yn	O
y	O
xn	O
x	O
py	O
y	O
x	O
we	O
may	O
subsequently	O
use	O
conditioning	B
to	O
form	O
a	O
predictor	O
py	O
much	O
use	O
of	O
the	O
i	O
i	O
d	O
assumption	O
that	O
each	O
datapoint	O
is	O
independently	O
sampled	O
from	O
the	O
same	O
generating	O
distribution	B
in	O
this	O
context	O
this	O
might	O
appear	O
to	O
suggest	O
the	O
assumption	O
in	O
previous	O
chapters	O
we	O
ve	O
made	O
yn	O
y	O
xn	O
x	O
py	O
x	O
pynx	O
x	O
n	O
however	O
this	O
is	O
clearly	O
of	O
little	O
use	O
since	O
the	O
predictive	O
conditional	B
is	O
simply	O
py	O
meaning	O
the	O
predictions	O
make	O
no	O
use	O
of	O
the	O
training	B
outputs	O
for	O
a	O
non-trivial	O
predictor	O
we	O
therefore	O
need	O
to	O
specify	O
a	O
joint	B
non-factorised	O
distribution	B
over	O
outputs	O
x	O
py	O
x	O
from	O
parametric	O
to	O
non-parametric	B
if	O
we	O
revisit	O
our	O
i	O
i	O
d	O
assumptions	O
for	O
parametric	O
models	O
we	O
used	O
a	O
parameter	B
to	O
make	O
a	O
model	B
of	O
the	O
input-output	B
distribution	B
pyx	O
for	O
a	O
parametric	O
model	B
predictions	O
are	O
formed	O
using	O
x	O
x	O
py	O
x	O
x	O
under	O
the	O
assumption	O
that	O
given	O
the	O
data	O
is	O
i	O
i	O
d	O
we	O
obtain	O
py	O
py	O
py	O
p	O
p	O
n	O
where	O
py	O
py	O
pyn	O
xn	O
n	O
py	O
pyn	O
xn	O
yn	O
xn	O
y	O
x	O
y	O
x	O
yn	O
xn	O
non-parametric	B
prediction	O
figure	O
a	O
parametric	O
model	B
for	O
predicb	O
the	O
form	O
of	O
the	O
tion	O
assuming	O
i	O
i	O
d	O
data	O
model	B
after	O
integrating	O
out	O
the	O
parameters	O
our	O
non-parametric	B
model	B
will	O
have	O
this	O
structure	B
after	O
integrating	O
over	O
the	O
parameters	O
the	O
joint	B
data	O
distribution	B
is	O
given	O
by	O
py	O
py	O
pyn	O
xn	O
n	O
which	O
does	O
not	O
in	O
general	O
factorise	O
into	O
individual	O
datapoint	O
terms	O
see	O
the	O
idea	O
of	O
a	O
nonparametric	O
approach	B
is	O
to	O
specify	O
the	O
form	O
of	O
these	O
dependencies	O
without	O
reference	O
to	O
an	O
explicit	O
parametric	O
model	B
one	O
route	O
towards	O
a	O
non-parametric	B
model	B
is	O
to	O
start	O
with	O
a	O
parametric	O
model	B
and	O
integrate	O
out	O
the	O
parameters	O
in	O
order	O
to	O
make	O
this	O
tractable	O
we	O
use	O
a	O
simple	O
linear	B
parameter	B
predictor	O
with	O
a	O
gaussian	B
parameter	B
prior	B
for	O
regression	B
this	O
leads	O
to	O
closed	O
form	O
expressions	O
although	O
the	O
classification	B
case	O
will	O
require	O
numerical	B
approximation	B
from	O
bayesian	B
linear	B
models	O
to	O
gaussian	B
processes	O
to	O
develop	O
the	O
gp	O
we	O
briefly	O
revisit	O
the	O
bayesian	B
linear	B
parameter	B
model	B
of	O
for	O
parameters	O
w	O
and	O
basis	O
functions	O
ix	O
the	O
output	O
is	O
given	O
by	O
zero	O
output	O
noise	O
y	O
wi	O
ix	O
i	O
if	O
we	O
stack	O
all	O
the	O
yn	O
into	O
a	O
vector	O
y	O
then	O
y	O
w	O
where	O
is	O
the	O
design	B
matrix	B
assuming	O
a	O
gaussian	B
weight	B
prior	B
pw	O
n	O
w	O
and	O
since	O
y	O
is	O
linear	B
in	O
w	O
the	O
joint	B
output	O
yn	O
is	O
gaussian	B
distributed	O
a	O
gaussian	B
prior	B
on	O
w	O
induces	O
a	O
gaussian	B
on	O
the	O
joint	B
y	O
with	O
mean	B
and	O
covariance	B
pw	O
w	O
t	O
w	O
t	O
w	O
from	O
this	O
we	O
see	O
that	O
the	O
w	O
can	O
be	O
absorbed	O
into	O
using	O
its	O
cholesky	B
decomposition	B
in	O
other	O
words	O
without	O
loss	O
of	O
generality	O
we	O
may	O
assume	O
w	O
i	O
after	O
integrating	O
out	O
the	O
weights	O
the	O
bayesian	B
linear	B
regression	B
model	B
induces	O
a	O
gaussian	B
distribution	B
on	O
any	O
set	O
of	O
outputs	O
y	O
as	O
xn	O
n	O
k	O
where	O
the	O
covariance	B
matrix	B
k	O
depends	O
on	O
the	O
training	B
inputs	O
alone	O
via	O
n	O
n	O
n	O
since	O
the	O
matrix	B
k	O
is	O
formed	O
as	O
the	O
scalar	B
product	I
of	O
vectors	O
it	O
is	O
by	O
construction	B
positive	O
semidefinite	O
as	O
we	O
saw	O
in	O
after	O
integrating	O
out	O
the	O
weights	O
the	O
only	O
thing	O
the	O
model	B
directly	O
depends	O
draft	O
march	O
non-parametric	B
prediction	O
on	O
is	O
the	O
covariance	B
matrix	B
k	O
in	O
a	O
gaussian	B
process	I
we	O
directly	O
specify	O
the	O
joint	B
output	O
covariance	B
k	O
as	O
a	O
function	B
of	O
two	O
inputs	O
specifically	O
we	O
need	O
to	O
define	O
the	O
n	O
element	O
of	O
the	O
covariance	B
matrix	B
for	O
any	O
two	O
inputs	O
xn	O
and	O
kxn	O
this	O
is	O
achieved	O
using	O
a	O
covariance	B
function	B
kxn	O
the	O
required	O
form	O
of	O
the	O
function	B
kxn	O
is	O
very	O
special	O
when	O
applied	O
to	O
create	O
the	O
elements	O
of	O
the	O
matrix	B
k	O
it	O
must	O
produce	O
a	O
positive	B
definite	I
matrix	B
we	O
discuss	O
how	O
to	O
create	O
such	O
covariance	B
functions	O
in	O
one	O
explicit	O
straightforward	O
construction	B
is	O
to	O
form	O
the	O
covariance	B
function	B
from	O
the	O
scalar	B
product	I
of	O
the	O
basis	O
vector	O
and	O
for	O
finite-dimensional	O
this	O
is	O
known	O
as	O
a	O
finite	O
dimensional	O
gaussian	B
process	I
given	O
any	O
covariance	B
function	B
we	O
can	O
always	O
find	O
a	O
corresponding	O
basis	O
vector	O
representation	O
that	O
is	O
for	O
any	O
gp	O
we	O
can	O
always	O
relate	O
this	O
back	O
to	O
a	O
parametric	O
bayesian	B
lpm	O
however	O
for	O
many	O
commonly	O
used	O
covariance	B
functions	O
the	O
basis	O
functions	O
corresponds	O
to	O
infinite	O
dimensional	O
vectors	O
it	O
is	O
in	O
such	O
cases	O
that	O
the	O
advantages	O
of	O
using	O
the	O
gp	O
framework	O
are	O
particularly	O
evident	O
since	O
we	O
would	O
not	O
be	O
able	O
to	O
compute	O
efficiently	O
with	O
the	O
corresponding	O
infinite	O
dimensional	O
parametric	O
model	B
a	O
prior	B
on	O
functions	O
the	O
nature	O
of	O
many	O
machine	O
learning	B
applications	O
is	O
such	O
that	O
the	O
knowledge	O
about	O
the	O
true	O
underlying	O
mechanism	O
behind	O
the	O
data	O
generation	O
process	O
is	O
limited	O
instead	O
one	O
relies	O
on	O
generic	O
smoothness	B
assumptions	O
of	O
the	O
form	O
that	O
for	O
two	O
inputs	O
x	O
and	O
that	O
are	O
close	O
the	O
corresponding	O
outputs	O
y	O
and	O
should	O
be	O
similar	O
many	O
generic	O
techniques	O
in	O
machine	O
learning	B
can	O
be	O
viewed	O
as	O
different	O
characterisations	O
of	O
smoothness	B
an	O
advantage	O
of	O
the	O
gp	O
framework	O
in	O
this	O
respect	O
is	O
that	O
the	O
mathematical	O
smoothness	B
properties	B
of	O
the	O
functions	O
are	O
well	O
understood	O
giving	O
confidence	O
in	O
the	O
procedure	O
for	O
a	O
given	O
covariance	B
matrix	B
k	O
equation	B
specifies	O
a	O
distribution	B
on	O
in	O
the	O
following	O
sense	O
we	O
specify	O
a	O
set	O
of	O
input	O
points	O
x	O
xn	O
and	O
a	O
n	O
n	O
covariance	B
matrix	B
k	O
then	O
we	O
draw	O
a	O
vector	O
y	O
from	O
the	O
gaussian	B
defined	O
by	O
equation	B
we	O
can	O
then	O
plot	O
the	O
sampled	O
function	B
at	O
the	O
finite	O
set	O
of	O
points	O
yn	O
n	O
n	O
what	O
kind	O
of	O
functions	O
does	O
a	O
gp	O
correspond	O
to	O
consider	O
two	O
scalar	O
inputs	O
xi	O
and	O
xj	O
separated	O
by	O
a	O
distance	O
xj	O
the	O
corresponding	O
sampled	O
outputs	O
yi	O
and	O
yj	O
fluctuate	O
as	O
different	O
functions	O
are	O
drawn	O
for	O
a	O
covariance	B
function	B
that	O
has	O
a	O
high	O
value	B
for	O
xj	O
small	O
we	O
expect	O
yi	O
and	O
yj	O
to	O
be	O
very	O
similar	O
since	O
they	O
are	O
highly	O
correlated	O
conversely	O
for	O
a	O
covariance	B
function	B
that	O
has	O
low	O
value	B
for	O
a	O
given	O
small	O
separation	B
xj	O
we	O
expect	O
yi	O
and	O
yj	O
to	O
be	O
effectively	O
in	O
general	O
we	O
would	O
expect	O
the	O
correlation	O
between	O
yi	O
and	O
yj	O
to	O
decrease	O
the	O
further	O
apart	O
xi	O
and	O
xj	O
are	O
in	O
we	O
show	O
three	O
sample	O
functions	O
drawn	O
from	O
a	O
squared	B
exponential	B
covariance	B
function	B
defined	O
over	O
points	O
uniformly	O
spaced	O
from	O
to	O
each	O
sampled	O
function	B
looks	O
reasonably	O
smooth	O
conversely	O
for	O
the	O
ornstein	O
uhlenbeck	O
covariance	B
function	B
the	O
sampled	O
functions	O
look	O
locally	O
rough	O
these	O
smoothness	B
properties	B
are	O
related	O
to	O
the	O
form	O
of	O
the	O
covariance	B
function	B
as	O
discussed	O
in	O
the	O
zero	O
mean	B
assumption	O
implies	O
that	O
if	O
we	O
were	O
to	O
draw	O
a	O
large	O
number	O
of	O
such	O
functions	O
the	O
mean	B
across	O
these	O
functions	O
at	O
a	O
given	O
point	O
x	O
tends	O
to	O
zero	O
similarly	O
for	O
any	O
two	O
points	O
x	O
and	O
if	O
we	O
compute	O
the	O
sample	O
covariance	B
between	O
the	O
corresponding	O
y	O
and	O
for	O
all	O
such	O
sampled	O
functions	O
this	O
will	O
tend	O
to	O
the	O
covariance	B
function	B
value	B
kx	O
the	O
zero-mean	O
assumption	O
can	O
be	O
easily	O
relaxed	O
by	O
defining	O
a	O
mean	B
function	B
mx	O
to	O
give	O
pyx	O
n	O
m	O
k	O
in	O
many	O
practical	O
situations	O
one	O
typically	O
deals	O
with	O
detrended	O
data	O
in	O
which	O
such	O
mean	B
trends	O
have	O
been	O
already	O
removed	O
for	O
this	O
reason	O
much	O
of	O
the	O
development	O
of	O
gps	O
in	O
the	O
machine	O
learning	B
literature	O
is	O
for	O
the	O
zero	O
mean	B
case	O
term	O
function	B
is	O
potentially	O
confusing	O
since	O
we	O
do	O
not	O
have	O
an	O
explicit	O
functional	O
form	O
for	O
the	O
input	O
output-mapping	O
for	O
any	O
finite	O
set	O
of	O
inputs	O
xn	O
the	O
values	O
for	O
the	O
function	B
are	O
given	O
by	O
the	O
outputs	O
at	O
those	O
points	O
yn	O
periodic	B
functions	O
however	O
we	O
would	O
expect	O
high	O
correlation	O
at	O
separating	O
distances	O
corresponding	O
to	O
the	O
period	O
of	O
the	O
function	B
draft	O
march	O
gaussian	B
process	I
prediction	O
gaussian	B
process	I
prediction	O
for	O
a	O
dataset	O
d	O
and	O
novel	O
input	O
x	O
a	O
zero	O
mean	B
gp	O
makes	O
a	O
gaussian	B
model	B
of	O
the	O
joint	B
outputs	O
yn	O
y	O
given	O
the	O
joint	B
inputs	O
xn	O
x	O
for	O
convenience	O
we	O
write	O
this	O
as	O
y	O
py	O
y	O
x	O
n	O
where	O
is	O
a	O
n	O
dimensional	O
zero-vector	O
the	O
covariance	B
matrix	B
k	O
is	O
a	O
block	O
matrix	B
with	O
elements	O
k	O
kxx	O
kxx	O
kx	O
where	O
kxx	O
is	O
the	O
covariance	B
matrix	B
of	O
the	O
training	B
inputs	O
x	O
that	O
is	O
kx	O
kxn	O
n	O
n	O
n	O
the	O
n	O
vector	O
kxx	O
has	O
elements	O
kxn	O
x	O
n	O
n	O
kx	O
is	O
the	O
transpose	O
of	O
the	O
above	O
vector	O
the	O
scalar	O
covariance	B
is	O
given	O
by	O
kx	O
kx	O
x	O
the	O
predictive	O
distribution	B
py	O
giving	O
a	O
gaussian	B
distribution	B
is	O
obtained	O
by	O
gaussian	B
conditioning	B
using	O
the	O
results	O
in	O
xxkxx	O
py	O
n	O
kx	O
xxy	O
kx	O
kx	O
for	O
fixed	O
hyperparameters	O
gp	O
regression	B
is	O
an	O
exact	O
method	O
and	O
there	O
are	O
no	O
issues	O
with	O
local	B
minima	O
furthermore	O
gps	O
are	O
attractive	O
since	O
they	O
automatically	O
model	B
uncertainty	B
in	O
the	O
predictions	O
however	O
the	O
computational	B
complexity	I
for	O
making	O
a	O
prediction	O
is	O
due	O
to	O
the	O
requirement	O
of	O
performing	O
the	O
matrix	B
inversion	B
solving	B
the	O
corresponding	O
linear	B
system	O
by	O
gaussian	B
elimination	O
this	O
can	O
be	O
prohibitively	O
expensive	O
for	O
large	O
datasets	O
and	O
a	O
large	O
body	O
of	O
research	O
on	O
efficient	O
approximations	O
exists	O
a	O
discussion	O
of	O
these	O
techniques	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
and	O
the	O
reader	O
is	O
referred	O
to	O
regression	B
with	O
noisy	O
training	B
outputs	O
to	O
prevent	O
overfitting	B
to	O
noisy	O
data	O
it	O
is	O
useful	O
to	O
assume	O
that	O
a	O
training	B
output	O
yn	O
is	O
the	O
result	O
of	O
some	O
clean	O
process	O
f	O
n	O
corrupted	O
by	O
additive	O
gaussian	B
noise	O
yn	O
f	O
n	O
where	O
in	O
this	O
case	O
our	O
interest	O
is	O
to	O
predict	O
the	O
clean	O
signal	O
f	O
for	O
a	O
novel	O
input	O
x	O
then	O
the	O
distribution	B
py	O
f	O
kxx	O
kxx	O
x	O
is	O
a	O
zero	O
mean	B
gaussian	B
with	O
block	O
covariance	B
matrix	B
n	O
kx	O
kx	O
so	O
that	O
kxx	O
is	O
replaced	O
by	O
kxx	O
in	O
forming	O
the	O
prediction	O
equation	B
draft	O
march	O
covariance	B
functions	O
example	O
training	B
data	O
from	O
a	O
one-dimensional	O
input	O
x	O
and	O
one	O
dimensional	O
output	O
y	O
are	O
plotted	O
in	O
along	O
with	O
the	O
mean	B
regression	B
function	B
fit	O
based	O
on	O
two	O
different	O
covariance	B
functions	O
note	O
how	O
the	O
smoothness	B
of	O
the	O
prior	B
translates	O
into	O
smoothness	B
of	O
the	O
prediction	O
the	O
smoothness	B
of	O
the	O
function	B
space	O
prior	B
is	O
a	O
consequence	O
of	O
the	O
choice	O
of	O
covariance	B
function	B
naively	O
we	O
can	O
partially	O
understand	O
this	O
by	O
the	O
behaviour	O
of	O
the	O
covariance	B
function	B
at	O
the	O
origin	O
see	O
demogpreg	O
m	O
the	O
marginal	B
likelihood	B
and	O
hyperparameter	B
learning	B
for	O
a	O
set	O
of	O
n	O
one-dimensional	O
training	B
inputs	O
represented	O
by	O
the	O
n	O
dimensional	O
vector	O
y	O
and	O
a	O
covariance	B
matrix	B
k	O
defined	O
on	O
the	O
inputs	O
xn	O
the	O
log	O
marginal	B
likelihood	B
is	O
one	O
can	O
learn	O
any	O
free	O
parameters	O
of	O
the	O
covariance	B
function	B
by	O
maximising	O
the	O
marginal	B
likelihood	B
for	O
example	O
a	O
squared	B
exponential	B
covariance	B
function	B
may	O
have	O
parameters	O
log	O
pyx	O
log	O
det	O
k	O
ytk	O
x	O
kx	O
x	O
exp	O
the	O
parameter	B
in	O
equation	B
specifies	O
the	O
appropriate	O
length-scale	O
of	O
the	O
inputs	O
and	O
the	O
variance	B
of	O
the	O
function	B
the	O
dependence	O
of	O
the	O
marginal	B
likelihood	B
on	O
the	O
parameters	O
is	O
typically	O
complex	O
and	O
no	O
closed	O
form	O
expression	O
for	O
the	O
maximum	B
likelihood	B
optimum	O
exists	O
in	O
this	O
case	O
on	O
resorts	O
to	O
numerical	B
optimisation	B
techniques	O
such	O
as	O
conjugate	B
gradients	O
vector	O
inputs	O
for	O
regression	B
with	O
vector	O
inputs	O
and	O
scalar	O
outputs	O
we	O
need	O
to	O
define	O
a	O
covariance	B
as	O
a	O
function	B
of	O
the	O
two	O
vectors	O
kx	O
using	O
the	O
multiplicative	O
property	O
of	O
covariance	B
functions	O
a	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
define	O
i	O
kxi	O
x	O
kx	O
i	O
for	O
example	O
for	O
the	O
squared	B
exponential	B
covariance	B
function	B
this	O
gives	O
kx	O
e	O
though	O
correlated	O
forms	O
are	O
possible	O
as	O
well	O
see	O
we	O
can	O
generalise	O
the	O
above	O
using	O
parameters	O
kx	O
exp	O
x	O
l	O
l	O
where	O
xl	O
is	O
the	O
lth	O
component	O
of	O
x	O
and	O
d	O
are	O
parameters	O
the	O
l	O
in	O
equation	B
allow	O
a	O
different	O
length	O
scale	O
on	O
each	O
input	O
dimension	O
and	O
can	O
be	O
learned	O
by	O
numerically	O
maximising	O
the	O
marginal	B
likelihood	B
for	O
irrelevant	O
inputs	O
the	O
corresponding	O
l	O
will	O
become	O
small	O
and	O
the	O
model	B
will	O
ignore	O
the	O
lth	O
input	O
dimension	O
this	O
is	O
closely	O
related	O
to	O
automatic	B
relevance	I
determination	I
covariance	B
functions	O
covariance	B
functions	O
kx	O
are	O
special	O
in	O
that	O
they	O
define	O
elements	O
of	O
a	O
positive	B
definite	I
matrix	B
these	O
functions	O
are	O
also	O
referred	O
to	O
as	O
kernels	O
particulary	O
in	O
the	O
machine	O
learning	B
literature	O
draft	O
march	O
covariance	B
functions	O
figure	O
the	O
input	O
space	O
from	O
to	O
is	O
split	O
evenly	O
into	O
points	O
three	O
samples	O
from	O
a	O
gp	O
prior	B
with	O
squared	B
exponential	B
covariance	B
function	B
the	O
covariance	B
matrix	B
k	O
is	O
defined	O
using	O
the	O
se	O
kernel	B
from	O
which	O
the	O
samples	O
are	O
drawn	O
using	O
prediction	O
based	O
on	O
training	B
points	O
plotted	O
is	O
the	O
posterior	B
predicted	O
function	B
based	O
on	O
the	O
se	O
covariance	B
the	O
central	O
line	O
is	O
the	O
mean	B
prediction	O
with	O
standard	O
three	O
samples	O
from	O
the	O
ornsteinerrors	O
bars	O
on	O
either	O
side	O
the	O
log	O
marginal	B
likelihood	B
is	O
posterior	B
prediction	O
for	O
the	O
ou	O
covariance	B
the	O
log	O
marginal	B
uhlenbeck	O
gp	O
prior	B
with	O
likelihood	B
is	O
meaning	O
that	O
the	O
se	O
covariance	B
is	O
much	O
more	O
heavily	O
supported	O
by	O
the	O
data	O
than	O
the	O
rougher	O
ou	O
covariance	B
definition	O
function	B
given	O
any	O
collection	O
of	O
points	O
xm	O
a	O
covariance	B
function	B
kxi	O
xj	O
defines	O
the	O
elements	O
of	O
a	O
m	O
m	O
matrix	B
kxi	O
xj	O
such	O
that	O
c	O
is	O
positive	O
semidefinite	O
making	O
new	O
covariance	B
functions	O
from	O
old	O
the	O
following	O
rules	O
can	O
all	O
be	O
proved	O
directly	O
generate	O
new	O
covariance	B
functions	O
from	O
existing	O
covariance	B
functions	O
definition	O
kx	O
draft	O
march	O
covariance	B
functions	O
definition	O
kx	O
x	O
y	O
definition	O
spaces	O
for	O
z	O
kz	O
and	O
kz	O
definition	O
rescaling	O
kx	O
for	O
any	O
function	B
ax	O
definition	O
and	O
embedding	O
kx	O
for	O
any	O
mapping	O
x	O
ux	O
where	O
the	O
mapping	O
ux	O
has	O
arbitrary	O
dimension	O
a	O
small	O
collection	O
of	O
covariance	B
functions	O
commonly	O
used	O
in	O
machine	O
learning	B
is	O
given	O
below	O
we	O
refer	O
the	O
reader	O
to	O
and	O
for	O
further	O
popular	O
covariance	B
functions	O
stationary	B
covariance	B
functions	O
definition	O
kernel	B
a	O
kernel	B
kx	O
is	O
stationary	B
if	O
the	O
kernel	B
depends	O
only	O
on	O
the	O
separation	B
x	O
that	O
is	O
kx	O
kx	O
following	O
the	O
notation	O
in	O
for	O
a	O
stationary	B
covariance	B
function	B
we	O
may	O
write	O
kd	O
where	O
d	O
x	O
this	O
means	O
that	O
for	O
functions	O
drawn	O
from	O
the	O
gp	O
on	O
average	B
the	O
functions	O
depend	O
only	O
on	O
the	O
distance	O
between	O
inputs	O
and	O
not	O
on	O
the	O
absolute	O
position	O
of	O
an	O
input	O
in	O
other	O
words	O
the	O
functions	O
are	O
on	O
average	B
translation	O
invariant	O
for	O
isotropic	B
covariance	B
functions	I
the	O
covariance	B
is	O
defined	O
as	O
a	O
function	B
of	O
the	O
distance	O
kd	O
draft	O
march	O
definition	O
exponential	B
kd	O
e	O
covariance	B
functions	O
the	O
squared	B
exponential	B
is	O
one	O
of	O
the	O
most	O
common	O
covariance	B
functions	O
there	O
are	O
many	O
ways	O
to	O
show	O
that	O
this	O
is	O
a	O
covariance	B
function	B
an	O
elementary	O
technique	O
is	O
to	O
consider	O
e	O
e	O
xn	O
xn	O
e	O
the	O
first	O
two	O
factors	O
form	O
a	O
kernel	B
of	O
the	O
form	O
is	O
the	O
linear	B
kernel	B
taking	O
the	O
exponential	B
and	O
writing	O
the	O
power	O
series	O
expansion	O
of	O
the	O
exponential	B
we	O
have	O
in	O
the	O
final	O
term	O
i	O
ki	O
this	O
can	O
be	O
expressed	O
as	O
a	O
series	O
of	O
integer	O
powers	O
of	O
with	O
positive	O
coefficients	O
by	O
the	O
product	O
itself	O
and	O
sum	O
rules	O
above	O
this	O
is	O
therefore	O
a	O
kernel	B
as	O
well	O
we	O
then	O
use	O
the	O
fact	O
that	O
equation	B
is	O
the	O
product	O
of	O
two	O
kernels	O
and	O
hence	O
also	O
a	O
kernel	B
definition	O
kd	O
e	O
when	O
we	O
have	O
the	O
squared	B
exponential	B
covariance	B
function	B
when	O
this	O
is	O
the	O
ornsteinuhlenbeck	O
covariance	B
function	B
definition	O
ern	O
kd	O
k	O
where	O
k	O
is	O
a	O
modified	O
bessel	B
function	B
definition	O
quadratic	O
kd	O
definition	O
for	O
x	O
and	O
a	O
stationary	B
isotropic	B
covariance	B
function	B
can	O
be	O
obtained	O
by	O
first	O
mapping	O
x	O
to	O
the	O
two	O
dimensional	O
vector	O
ux	O
sinx	O
and	O
then	O
using	O
the	O
se	O
covariance	B
e	O
e	O
kx	O
x	O
see	O
and	O
draft	O
march	O
covariance	B
functions	O
figure	O
plots	O
of	O
the	O
gamma-exponential	O
covariance	B
e	O
versus	O
x	O
the	O
case	O
corresponds	O
to	O
the	O
se	O
covariance	B
function	B
the	O
drop	O
in	O
the	O
covariance	B
is	O
much	O
more	O
rapid	O
as	O
a	O
function	B
of	O
the	O
separation	B
x	O
for	O
small	O
suggesting	O
that	O
the	O
functions	O
corresponding	O
to	O
smaller	O
will	O
be	O
locally	O
rough	O
as	O
for	O
but	O
zoomed	O
in	O
towards	O
the	O
possess	O
relatively	O
higher	O
long	O
range	O
correlation	O
origin	O
for	O
the	O
se	O
case	O
the	O
derivative	O
of	O
the	O
covariance	B
function	B
is	O
zero	O
whereas	O
the	O
ou	O
covariance	B
has	O
a	O
first	O
order	O
contribution	O
to	O
the	O
drop	O
in	O
the	O
covariance	B
suggesting	O
that	O
locally	O
ou	O
sampled	O
functions	O
will	O
be	O
much	O
rougher	O
than	O
se	O
functions	O
non-stationary	B
covariance	B
functions	O
definition	O
kx	O
definition	O
network	O
kx	O
arcsin	O
the	O
functions	O
defined	O
by	O
this	O
covariance	B
always	O
go	O
through	O
the	O
origin	O
to	O
shift	O
this	O
one	O
may	O
use	O
the	O
embedding	O
x	O
x	O
where	O
the	O
has	O
the	O
effect	O
of	O
a	O
bias	B
from	O
the	O
origin	O
to	O
change	O
the	O
scale	O
of	O
the	O
bias	B
and	O
and	O
non-bias	O
contributions	O
one	O
may	O
use	O
additional	O
parameters	O
x	O
x	O
the	O
nn	O
covariance	B
function	B
can	O
be	O
derived	O
as	O
a	O
limiting	O
case	O
of	O
a	O
neural	B
network	I
with	O
infinite	O
hidden	B
and	O
making	O
use	O
of	O
exact	O
integral	O
results	O
in	O
definition	O
kx	O
i	O
i	O
i	O
for	O
functions	O
rix	O
draft	O
march	O
e	O
i	O
i	O
analysis	B
of	O
covariance	B
functions	O
figure	O
samples	O
from	O
a	O
gp	O
prior	B
for	O
x	O
points	O
uniformly	O
placed	O
from	O
to	O
from	O
the	O
periodic	B
covariance	B
function	B
with	O
bias	B
b	O
and	O
neural	B
network	I
covariance	B
function	B
sampled	O
analysis	B
of	O
covariance	B
functions	O
smoothness	B
of	O
the	O
functions	O
we	O
examine	O
local	B
smoothness	B
for	O
a	O
translation	O
invariant	O
kernel	B
kx	O
kx	O
for	O
two	O
onedimensional	O
points	O
x	O
and	O
separated	O
by	O
a	O
small	O
amount	O
x	O
the	O
covariance	B
between	O
the	O
outputs	O
y	O
and	O
is	O
by	O
taylor	B
expansion	I
kx	O
x	O
dk	O
so	O
that	O
the	O
change	O
in	O
the	O
covariance	B
at	O
the	O
local	B
level	O
is	O
dominated	O
by	O
the	O
first	O
derivative	O
of	O
the	O
covariance	B
function	B
for	O
the	O
se	O
covariance	B
kx	O
e	O
dk	O
dx	O
is	O
zero	O
at	O
x	O
this	O
means	O
that	O
for	O
the	O
se	O
covariance	B
function	B
the	O
first	O
order	O
change	O
in	O
the	O
covariance	B
is	O
zero	O
and	O
only	O
higher	O
order	O
terms	O
contribute	O
for	O
the	O
ornstein-uhlenbeck	B
covariance	B
kx	O
e	O
the	O
right	O
derivative	O
at	O
the	O
origin	O
is	O
lim	O
k	O
lim	O
e	O
where	O
this	O
result	O
is	O
obtained	O
using	O
l	O
h	O
opital	O
s	O
rule	O
hence	O
for	O
the	O
ou	O
covariance	B
function	B
there	O
is	O
a	O
first	O
order	O
negative	O
change	O
in	O
the	O
covariance	B
at	O
the	O
local	B
level	O
this	O
decrease	O
in	O
the	O
covariance	B
is	O
therefore	O
much	O
more	O
rapid	O
than	O
for	O
the	O
se	O
covariance	B
see	O
since	O
low	O
covariance	B
implies	O
low	O
dependence	O
gaussian	B
distributions	O
locally	O
the	O
functions	O
generated	O
from	O
the	O
ou	O
process	O
are	O
rough	O
whereas	O
they	O
are	O
smooth	O
in	O
the	O
se	O
case	O
a	O
more	O
formal	O
treatment	O
for	O
the	O
stationary	B
case	O
can	O
be	O
obtained	O
by	O
examining	O
the	O
eigenvalue-frequency	O
plot	O
of	O
the	O
covariance	B
function	B
density	B
for	O
rough	O
functions	O
the	O
density	B
of	O
eigenvalues	O
for	O
high	O
frequency	O
components	O
is	O
higher	O
than	O
for	O
smooth	O
functions	O
mercer	O
kernels	O
consider	O
the	O
function	B
kx	O
x	O
sx	O
sx	O
where	O
is	O
a	O
vector	O
with	O
component	O
functions	O
bx	O
then	O
for	O
a	O
set	O
of	O
points	O
xp	O
we	O
construct	O
the	O
matrix	B
k	O
with	O
elements	O
kxi	O
xj	O
sxi	O
sxj	O
draft	O
march	O
analysis	B
of	O
covariance	B
functions	O
we	O
claim	O
that	O
the	O
matrix	B
k	O
so	O
constructed	O
is	O
positive	O
semidefinite	O
and	O
hence	O
a	O
valid	O
covariance	B
matrix	B
recalling	O
that	O
a	O
matrix	B
is	O
positive	O
semidefinite	O
if	O
for	O
any	O
non	O
zero	O
vector	O
z	O
ztkz	O
using	O
the	O
definition	O
of	O
k	O
above	O
we	O
have	O
ztkz	O
zikijzj	O
zi	O
sxi	O
s	O
sxjzj	O
s	O
s	O
hence	O
any	O
function	B
of	O
the	O
form	O
equation	B
is	O
a	O
covariance	B
function	B
we	O
can	O
generalise	O
the	O
mercer	B
kernel	B
to	O
complex	O
functions	O
using	O
kx	O
x	O
where	O
represents	O
the	O
complex	O
conjugate	B
then	O
the	O
matrix	B
k	O
formed	O
from	O
inputs	O
xi	O
i	O
p	O
is	O
positive	O
semidefinite	O
since	O
for	O
any	O
real	O
vector	O
z	O
ztkz	O
zi	O
sxi	O
s	O
sxjzj	O
s	O
where	O
we	O
made	O
use	O
of	O
the	O
general	O
result	O
for	O
a	O
complex	O
variable	O
xx	O
a	O
further	O
generalisation	B
is	O
to	O
write	O
kx	O
x	O
fs	O
s	O
sds	O
for	O
fs	O
and	O
scalar	O
complex	O
functions	O
s	O
then	O
replacing	O
summations	O
with	O
integration	O
assuming	O
we	O
can	O
interchange	O
the	O
sum	O
over	O
the	O
components	O
of	O
z	O
with	O
the	O
integral	O
over	O
s	O
we	O
obtain	O
ztkz	O
fs	O
zi	O
s	O
ds	O
fs	O
szj	O
spectral	O
decomposition	B
is	O
a	O
generalisation	B
of	O
the	O
spectral	O
decomposition	B
of	O
a	O
kernel	B
kx	O
since	O
if	O
we	O
write	O
fs	O
as	O
a	O
sum	O
of	O
dirac	O
delta	O
functions	O
fs	O
k	O
k	O
and	O
using	O
k	O
kx	O
for	O
an	O
eigenfunction	O
kx	O
indexed	O
by	O
k	O
with	O
eigenvalue	O
k	O
we	O
obtain	O
the	O
spectral	O
decomposition	B
kx	O
x	O
k	O
kx	O
kx	O
if	O
all	O
the	O
eigenvalues	O
of	O
a	O
kernel	B
are	O
non-negative	O
the	O
kernel	B
is	O
a	O
covariance	B
function	B
consider	O
for	O
example	O
the	O
following	O
function	B
e	O
kx	O
x	O
we	O
claim	O
that	O
this	O
is	O
a	O
covariance	B
function	B
this	O
is	O
indeed	O
a	O
valid	O
covariance	B
function	B
in	O
the	O
sense	O
that	O
for	O
any	O
set	O
of	O
points	O
xd	O
the	O
d	O
d	O
matrix	B
formed	O
with	O
elements	O
kxd	O
is	O
positive	B
definite	I
as	O
discussed	O
after	O
the	O
solution	O
given	O
to	O
shows	O
that	O
there	O
do	O
indeed	O
exist	O
real-valued	O
vectors	O
such	O
that	O
one	O
can	O
represent	O
kx	O
x	O
where	O
the	O
vectors	O
are	O
infinite	O
dimensional	O
this	O
demonstrates	O
the	O
generalisation	B
of	O
the	O
finite-dimensional	O
weight	B
space	O
viewpoint	O
of	O
a	O
gp	O
to	O
the	O
potentially	O
implicit	O
infinite	O
dimensional	O
representation	O
draft	O
march	O
gaussian	B
processes	O
for	O
classification	B
cn	O
yn	O
xn	O
c	O
y	O
x	O
figure	O
gp	O
classification	B
the	O
gp	O
induces	O
a	O
gaussian	B
distribution	B
on	O
the	O
latent	B
activations	O
yn	O
y	O
given	O
the	O
observed	O
values	O
of	O
cn	O
the	O
classification	B
of	O
the	O
new	O
input	O
x	O
is	O
then	O
given	O
via	O
the	O
correlation	O
induced	O
by	O
the	O
training	B
points	O
on	O
the	O
latent	B
activation	O
y	O
fourier	O
analysis	B
for	O
stationary	B
kernels	O
gx	O
for	O
a	O
function	B
gx	O
with	O
fourier	O
transform	O
gs	O
we	O
may	O
use	O
the	O
inverse	O
fourier	O
transform	O
to	O
write	O
gse	O
ixsds	O
where	O
i	O
for	O
a	O
stationary	B
kernel	B
kx	O
with	O
fourier	O
transform	O
ks	O
we	O
can	O
therefore	O
write	O
kx	O
x	O
kse	O
ix	O
kse	O
which	O
is	O
of	O
the	O
same	O
form	O
as	O
equation	B
where	O
the	O
fourier	O
transform	O
ks	O
is	O
identified	O
with	O
fs	O
and	O
s	O
e	O
isx	O
hence	O
provided	O
the	O
fourier	O
transform	O
ks	O
is	O
positive	O
the	O
translation	O
invariant	O
kernel	B
kx	O
is	O
a	O
covariance	B
function	B
bochner	O
s	O
asserts	O
the	O
converse	O
that	O
any	O
translation	O
invariant	O
covariance	B
function	B
must	O
have	O
such	O
a	O
fourier	O
representation	O
application	O
to	O
the	O
squared	B
exponential	B
kernel	B
for	O
the	O
translation	O
invariant	O
squared	B
exponential	B
kernel	B
kx	O
e	O
dx	O
e	O
e	O
e	O
ks	O
e	O
its	O
fourier	O
transform	O
is	O
hence	O
the	O
fourier	O
transform	O
of	O
the	O
se	O
kernel	B
is	O
a	O
gaussian	B
since	O
this	O
is	O
positive	O
the	O
se	O
kernel	B
is	O
a	O
covariance	B
function	B
gaussian	B
processes	O
for	O
classification	B
adapting	O
the	O
gp	O
framework	O
to	O
classification	B
requires	O
replacing	O
the	O
gaussian	B
regression	B
term	O
pyx	O
with	O
a	O
corresponding	O
classification	B
term	O
pcx	O
for	O
a	O
discrete	B
label	O
c	O
to	O
do	O
so	O
we	O
will	O
use	O
the	O
gp	O
to	O
define	O
a	O
latent	B
continuous	B
space	O
which	O
will	O
then	O
be	O
mapped	O
to	O
a	O
class	O
probability	O
using	O
pcx	O
given	O
training	B
data	O
inputs	O
x	O
corresponding	O
class	O
labels	O
c	O
and	O
a	O
novel	O
pcyxpyxdy	O
pcypyxdy	O
input	O
x	O
then	O
pc	O
where	O
py	O
pc	O
py	O
py	O
pcypy	O
x	O
pcnyn	O
x	O
class	O
mapping	O
yn	O
y	O
xn	O
x	O
gaussian	B
process	I
dyn	O
draft	O
march	O
gaussian	B
processes	O
for	O
classification	B
the	O
graphical	O
structure	B
of	O
this	O
model	B
is	O
depicted	O
in	O
the	O
posterior	B
latent	B
y	O
is	O
then	O
formed	O
from	O
the	O
standard	O
regression	B
term	O
from	O
the	O
gaussian	B
process	I
multiplied	O
by	O
a	O
set	O
of	O
non-gaussian	O
maps	O
from	O
the	O
latent	B
activations	O
to	O
the	O
class	O
probabilities	O
we	O
can	O
reformulate	O
the	O
prediction	O
problem	B
more	O
conveniently	O
as	O
follows	O
py	O
py	O
py	O
x	O
where	O
pycx	O
xn	O
pcnyn	O
in	O
equation	B
the	O
term	O
py	O
x	O
does	O
not	O
contain	O
any	O
class	O
label	O
information	O
and	O
is	O
simply	O
a	O
conditional	B
gaussian	B
the	O
advantage	O
of	O
the	O
above	O
description	O
is	O
that	O
we	O
can	O
therefore	O
form	O
an	O
approximation	B
to	O
pycx	O
and	O
then	O
reuse	O
this	O
approximation	B
in	O
the	O
prediction	O
for	O
many	O
different	O
x	O
without	O
needing	O
to	O
rerun	O
the	O
binary	O
classification	B
for	O
the	O
binary	O
class	O
case	O
we	O
will	O
use	O
the	O
convention	O
that	O
c	O
we	O
therefore	O
need	O
to	O
specify	O
pc	O
for	O
a	O
real	O
valued	O
activation	O
y	O
a	O
convenient	O
choice	O
is	O
the	O
logistic	B
transfer	O
e	O
x	O
then	O
pcy	O
y	O
is	O
a	O
valid	O
distribution	B
since	O
x	O
ensuring	O
that	O
the	O
sum	O
over	O
the	O
class	O
states	O
is	O
a	O
difficulty	O
is	O
that	O
the	O
non-linear	B
class	O
mapping	O
term	O
makes	O
the	O
computation	O
of	O
the	O
posterior	B
distribution	B
equation	B
difficult	O
since	O
the	O
integrals	O
over	O
yn	O
cannot	O
be	O
carried	O
out	O
analytically	O
there	O
are	O
many	O
approximate	B
techniques	O
one	O
could	O
apply	O
in	O
this	O
case	O
including	O
variational	O
methods	O
analogous	O
to	O
that	O
described	O
in	O
section	O
below	O
we	O
describe	O
the	O
straightforward	O
laplace	B
method	O
leaving	O
the	O
more	O
sophisticated	O
methods	O
for	O
further	O
laplace	B
s	O
approximation	B
in	O
the	O
laplace	B
method	O
we	O
approximate	B
the	O
non-gaussian	O
distribution	B
by	O
a	O
qycx	O
pycx	O
qycx	O
predictions	O
can	O
be	O
formed	O
from	O
the	O
joint	B
gaussian	B
for	O
compactness	O
we	O
define	O
the	O
class	O
label	O
vector	O
and	O
outputs	O
x	O
y	O
py	O
py	O
c	O
and	O
notationally	O
drop	O
the	O
present	O
conditioning	B
on	O
the	O
inputs	O
x	O
also	O
for	O
convenience	O
we	O
define	O
will	O
also	O
refer	O
to	O
this	O
as	O
the	O
sigmoid	B
function	B
more	O
strictly	O
a	O
sigmoid	B
function	B
refers	O
to	O
any	O
s-shaped	O
function	B
the	O
greek	O
for	O
s	O
authors	O
use	O
the	O
term	O
laplace	B
approximation	B
solely	O
for	O
approximating	O
an	O
integral	O
here	O
we	O
use	O
the	O
term	O
to	O
refer	O
to	O
a	O
gaussian	B
approximation	B
of	O
a	O
non-gaussian	O
distribution	B
draft	O
march	O
finding	O
the	O
mode	B
the	O
laplace	B
approximation	B
corresponds	O
to	O
a	O
second	O
order	O
expansion	O
around	O
the	O
mode	B
of	O
the	O
distribution	B
our	O
task	O
is	O
therefore	O
to	O
find	O
the	O
maximum	O
of	O
gaussian	B
processes	O
for	O
classification	B
pyc	O
py	O
c	O
e	O
where	O
cty	O
eyn	O
ytk	O
xxy	O
log	O
det	O
n	O
log	O
the	O
maximum	O
needs	O
to	O
be	O
found	O
numerically	O
and	O
it	O
is	O
convenient	O
to	O
use	O
the	O
newton	O
method	O
ynew	O
y	O
differentiating	O
equation	B
with	O
respect	O
to	O
y	O
we	O
obtain	O
the	O
gradient	B
and	O
hessian	B
k	O
xxy	O
k	O
xx	O
d	O
where	O
the	O
noise	O
matrix	B
is	O
given	O
by	O
d	O
diag	O
n	O
n	O
using	O
these	O
expressions	O
in	O
the	O
newton	B
update	I
gives	O
to	O
avoid	O
unnecessary	O
inversions	O
one	O
may	O
rewrite	O
this	O
in	O
the	O
form	O
ynew	O
y	O
xx	O
k	O
ynew	O
kxx	O
dkxx	O
c	O
xx	O
and	O
qyx	O
x	O
in	O
equation	B
predictions	O
are	O
then	O
made	O
y	O
qyx	O
x	O
given	O
a	O
converged	O
solution	O
y	O
we	O
have	O
found	O
a	O
gaussian	B
approximation	B
we	O
now	O
have	O
gaussians	O
for	O
py	O
using	O
making	O
predictions	O
n	O
py	O
py	O
yqyx	O
x	O
where	O
by	O
conditioning	B
kx	O
we	O
can	O
also	O
write	O
this	O
as	O
a	O
linear	B
system	O
n	O
x	O
py	O
y	O
xxy	O
kx	O
kx	O
kx	O
kx	O
where	O
n	O
aging	O
over	O
y	O
and	O
the	O
noise	O
we	O
obtain	O
xxkxx	O
xxy	O
kx	O
kx	O
xxkxx	O
using	O
equation	B
and	O
equation	B
and	O
aver	O
similarly	O
the	O
variance	B
of	O
the	O
latent	B
prediction	O
is	O
xx	O
y	O
kx	O
y	O
xx	O
k	O
d	O
kxx	O
kx	O
xx	O
kx	O
kx	O
vary	O
xxkxx	O
kx	O
kx	O
xxkxx	O
draft	O
march	O
gaussian	B
processes	O
for	O
classification	B
figure	O
gaussian	B
process	I
classification	B
the	O
x-axis	O
are	O
the	O
inputs	O
and	O
the	O
class	O
is	O
the	O
y-axis	O
green	O
points	O
are	O
training	B
points	O
from	O
class	O
and	O
red	O
from	O
class	O
the	O
dots	O
are	O
the	O
predictions	O
pc	O
for	O
a	O
rand	O
of	O
points	O
x	O
ou	O
covariance	B
see	O
square	O
exponential	B
covariance	B
where	O
the	O
last	O
line	O
is	O
obtained	O
using	O
the	O
matrix	B
inversion	B
lemma	I
the	O
class	O
prediction	O
for	O
a	O
new	O
input	O
x	O
is	O
then	O
given	O
by	O
pc	O
in	O
order	O
to	O
calculate	O
the	O
gaussian	B
integral	O
over	O
the	O
logistic	B
sigmoid	B
function	B
we	O
use	O
an	O
approximation	B
of	O
the	O
sigmoid	B
function	B
based	O
on	O
the	O
error	B
function	B
erfx	O
see	O
and	O
avsigmagauss	O
m	O
example	O
an	O
example	O
of	O
binary	O
classification	B
is	O
given	O
in	O
in	O
which	O
one-dimensional	O
input	O
training	B
data	O
with	O
binary	O
class	O
labels	O
is	O
plotted	O
along	O
with	O
the	O
class	O
probability	O
predictions	O
on	O
a	O
range	O
of	O
input	O
points	O
in	O
both	O
cases	O
the	O
covariance	B
function	B
is	O
of	O
the	O
form	O
xj	O
ij	O
the	O
square	O
exponential	B
covariance	B
produces	O
a	O
smoother	O
class	O
prediction	O
than	O
the	O
ornstein-uhlenbeck	B
covariance	B
function	B
see	O
and	O
demogpclass	O
m	O
marginal	B
likelihood	B
the	O
marginal	B
likelihood	B
is	O
given	O
by	O
under	O
the	O
laplace	B
approximation	B
the	O
marginal	B
likelihood	B
is	O
approximated	O
by	O
pcx	O
pcypyx	O
y	O
pcx	O
y	O
e	O
ye	O
ytay	O
y	O
where	O
a	O
integrating	O
over	O
y	O
gives	O
log	O
pcx	O
log	O
qcx	O
where	O
log	O
det	O
a	O
log	O
log	O
qcx	O
y	O
y	O
ct	O
y	O
e	O
yn	O
xx	O
n	O
log	O
ytk	O
xx	O
y	O
log	O
det	O
kxxd	O
where	O
y	O
is	O
the	O
converged	O
iterate	O
of	O
equation	B
one	O
can	O
also	O
simplify	O
the	O
above	O
using	O
that	O
at	O
convergence	O
k	O
xx	O
y	O
c	O
draft	O
march	O
hyperparameter	B
optimisation	B
code	O
the	O
approximate	B
marginal	B
likelihood	B
can	O
be	O
used	O
to	O
assess	O
hyperparameters	O
of	O
the	O
kernel	B
a	O
little	O
care	O
is	O
required	O
in	O
computing	O
derivatives	O
of	O
the	O
approximate	B
marginal	B
likelihood	B
since	O
the	O
optimum	O
y	O
depends	O
on	O
we	O
use	O
the	O
total	O
derivative	O
formula	O
log	O
qcx	O
ytk	O
i	O
d	O
d	O
log	O
qcx	O
log	O
qcx	O
yi	O
log	O
qcx	O
d	O
d	O
yi	O
xxy	O
log	O
det	O
kxxd	O
which	O
can	O
be	O
evaluated	O
using	O
the	O
standard	O
results	O
for	O
the	O
derivative	O
of	O
a	O
matrix	B
determinant	B
and	O
inverse	O
since	O
the	O
derivative	O
of	O
is	O
zero	O
at	O
y	O
and	O
noting	O
that	O
d	O
depends	O
explicitly	O
on	O
y	O
yi	O
log	O
qcx	O
yi	O
log	O
det	O
kxxd	O
the	O
implicit	O
derivative	O
is	O
obtained	O
from	O
using	O
the	O
fact	O
that	O
at	O
convergence	O
y	O
kxx	O
to	O
give	O
d	O
d	O
y	O
kxxd	O
kxx	O
these	O
results	O
are	O
substituted	O
into	O
equation	B
to	O
find	O
an	O
explicit	O
expression	O
for	O
the	O
derivative	O
multiple	B
classes	I
the	O
extension	O
of	O
the	O
preceding	O
framework	O
to	O
multiple	B
classes	I
is	O
essentially	O
straightforward	O
and	O
may	O
be	O
achieved	O
using	O
the	O
softmax	B
function	B
pc	O
my	O
eym	O
which	O
automatically	O
enforces	O
the	O
constraint	O
classes	O
the	O
cost	O
of	O
implementing	O
the	O
laplace	B
approximation	B
for	O
the	O
multiclass	O
case	O
scales	O
as	O
however	O
one	O
may	O
show	O
by	O
careful	O
implementation	O
that	O
the	O
cost	O
is	O
only	O
and	O
we	O
refer	O
the	O
reader	O
m	O
pc	O
m	O
naively	O
it	O
would	O
appear	O
that	O
the	O
for	O
c	O
to	O
for	O
details	O
further	O
reading	O
gaussian	B
processes	O
have	O
been	O
heavily	O
developed	O
within	O
the	O
machine	O
learning	B
community	O
over	O
recent	O
years	O
for	O
which	O
efficient	O
approximations	O
for	O
both	O
regression	B
and	O
classification	B
remains	O
an	O
active	B
research	O
topic	O
we	O
direct	O
the	O
interested	O
reader	O
to	O
and	O
for	O
further	O
discussion	O
code	O
gpreg	O
m	O
gaussian	B
process	I
regression	B
demogpreg	O
m	O
demo	O
gp	O
regression	B
covfnge	O
m	O
gamma-exponential	O
covariance	B
function	B
gpclass	O
m	O
gaussian	B
process	I
classification	B
demogpclass	O
m	O
demo	O
gaussian	B
process	I
classification	B
draft	O
march	O
exercises	O
exercises	O
exercise	O
show	O
that	O
the	O
sample	O
covariance	B
matrix	B
with	O
elements	O
sij	O
xi	O
i	O
is	O
positive	O
semidefinite	O
xn	O
xn	O
i	O
xn	O
j	O
xi	O
xj	O
where	O
exercise	O
show	O
that	O
e	O
sinx	O
kx	O
x	O
is	O
a	O
covariance	B
function	B
exercise	O
consider	O
the	O
function	B
fxi	O
xj	O
e	O
xj	O
for	O
one	O
dimensional	O
inputs	O
xi	O
show	O
that	O
fxi	O
xj	O
e	O
i	O
exixj	O
e	O
j	O
xj	O
is	O
a	O
kernel	B
and	O
find	O
an	O
explicit	O
representation	O
by	O
taylor	O
expanding	O
the	O
central	O
term	O
show	O
that	O
e	O
for	O
the	O
kernel	B
fxi	O
xj	O
as	O
the	O
scalar	B
product	I
of	O
two	O
infinite	O
dimensional	O
vectors	O
exercise	O
show	O
that	O
for	O
a	O
covariance	B
function	B
then	O
kx	O
is	O
also	O
a	O
covariance	B
function	B
for	O
any	O
polynomial	O
fx	O
with	O
positive	O
coefficients	O
show	O
therefore	O
that	O
and	O
tan	O
are	O
covariance	B
functions	O
exercise	O
for	O
a	O
covariance	B
function	B
show	O
that	O
is	O
also	O
a	O
valid	O
covariance	B
function	B
for	O
a	O
positive	B
definite	I
symmetric	O
matrix	B
a	O
exercise	O
kernel	B
let	O
x	O
and	O
be	O
two	O
strings	O
of	O
characters	O
and	O
sx	O
be	O
the	O
number	O
of	O
times	O
that	O
substring	O
s	O
appears	O
in	O
string	O
x	O
then	O
ws	O
sx	O
sx	O
kx	O
x	O
s	O
is	O
a	O
kernel	B
covariance	B
function	B
provided	O
the	O
weight	B
of	O
each	O
substring	O
ws	O
is	O
positive	O
given	O
a	O
collection	O
of	O
strings	O
about	O
politics	O
and	O
another	O
collection	O
about	O
sport	O
explain	O
how	O
to	O
form	O
a	O
gp	O
classifier	B
using	O
a	O
string	O
kernel	B
explain	O
how	O
the	O
weights	O
ws	O
can	O
be	O
adjusted	O
to	O
improve	O
the	O
fit	O
of	O
the	O
classifier	B
to	O
the	O
data	O
and	O
give	O
an	O
explicit	O
formula	O
for	O
the	O
derivative	O
with	O
respect	O
to	O
ws	O
of	O
the	O
log	O
marginal	B
likelihood	B
under	O
the	O
laplace	B
approximation	B
exercise	O
regression	B
consider	O
predicting	O
a	O
vector	O
output	O
y	O
given	O
training	B
data	O
x	O
y	O
yn	O
n	O
n	O
to	O
make	O
a	O
gp	O
predictor	O
py	O
we	O
need	O
a	O
gaussian	B
model	B
yn	O
y	O
xn	O
x	O
a	O
gp	O
requires	O
then	O
a	O
specification	O
of	O
the	O
covariance	B
cym	O
two	O
different	O
input	O
vectors	O
show	O
that	O
under	O
the	O
dimension	O
independence	B
assumption	O
i	O
yn	O
j	O
xm	O
of	O
the	O
components	O
of	O
the	O
outputs	O
for	O
cym	O
j	O
xm	O
ciym	O
i	O
xm	O
is	O
a	O
covariance	B
function	B
for	O
the	O
ith	O
dimension	O
that	O
separate	O
gp	O
predictors	O
can	O
be	O
where	O
ciym	O
constructed	O
independently	O
one	O
for	O
each	O
output	O
dimension	O
i	O
i	O
xm	O
ij	O
i	O
yn	O
i	O
yn	O
i	O
yn	O
draft	O
march	O
exercise	O
consider	O
the	O
markov	O
update	O
of	O
a	O
linear	B
dynamical	I
system	I
exercises	O
xt	O
axt	O
t	O
where	O
a	O
is	O
a	O
given	O
matrix	B
and	O
t	O
is	O
zero	O
mean	B
gaussian	B
noise	O
with	O
it	O
ij	O
also	O
t	O
show	O
that	O
xt	O
is	O
gaussian	B
distributed	O
show	O
that	O
the	O
covariance	B
matrix	B
of	O
xt	O
has	O
elements	O
n	O
t	O
t	O
at	O
t	O
t	O
and	O
explain	O
why	O
a	O
linear	B
dynamical	I
system	I
is	O
a	O
gaussian	B
process	I
consider	O
yt	O
bxt	O
where	O
is	O
zero	O
mean	B
gaussian	B
noise	O
with	O
covariance	B
ij	O
the	O
vectors	O
are	O
uncorrelated	O
with	O
the	O
vectors	O
show	O
that	O
the	O
sequence	O
of	O
vectors	O
yt	O
is	O
a	O
gaussian	B
process	I
with	O
a	O
suitably	O
defined	O
covariance	B
function	B
exercise	O
a	O
form	O
of	O
independent	B
components	I
analysis	B
of	O
a	O
one-dimensional	O
signal	O
yt	O
is	O
obtained	O
from	O
the	O
joint	B
model	B
t	O
t	O
t	O
w	O
n	O
with	O
t	O
t	O
w	O
n	O
t	O
t	O
n	O
where	O
is	O
a	O
given	O
noise	O
variance	B
the	O
signal	O
can	O
be	O
viewed	O
as	O
the	O
linear	B
combination	O
of	O
two	O
independent	O
gaussian	B
processes	O
the	O
covariance	B
matrices	O
of	O
the	O
two	O
processes	O
have	O
elements	O
from	O
a	O
stationary	B
kernel	B
e	O
write	O
down	O
an	O
em	B
algorithm	B
for	O
learning	B
the	O
mixing	O
parameters	O
given	O
an	O
observation	O
se	O
quence	O
consider	O
an	O
extension	O
of	O
the	O
above	O
model	B
to	O
the	O
case	O
of	O
two	O
outputs	O
pyi	O
t	O
t	O
wi	O
n	O
t	O
with	O
t	O
t	O
t	O
t	O
t	O
w	O
n	O
t	O
w	O
n	O
t	O
t	O
t	O
t	O
t	O
t	O
n	O
show	O
that	O
for	O
t	O
the	O
likelihood	B
is	O
not	O
invariant	O
with	O
respect	O
to	O
an	O
orthogonal	B
rotation	O
wr	O
with	O
rrt	O
i	O
and	O
explain	O
the	O
significance	O
of	O
this	O
with	O
respect	O
to	O
identifying	O
independent	O
components	O
draft	O
march	O
chapter	O
mixture	B
models	O
density	B
estimation	I
using	O
mixtures	O
a	O
mixture	B
model	B
is	O
one	O
in	O
which	O
a	O
set	O
of	O
component	O
models	O
is	O
combined	O
to	O
produce	O
a	O
richer	O
model	B
pv	O
pvhph	O
the	O
variable	O
v	O
is	O
visible	B
or	O
observable	O
and	O
h	O
h	O
indexes	O
each	O
component	O
model	B
pvh	O
along	O
with	O
its	O
weight	B
ph	O
mixture	B
models	O
have	O
natural	B
application	O
in	O
clustering	B
data	O
where	O
h	O
indexes	O
the	O
cluster	O
this	O
interpretation	O
can	O
be	O
gained	O
from	O
considering	O
how	O
to	O
generate	O
a	O
sample	O
datapoint	O
v	O
from	O
the	O
model	B
equation	B
first	O
we	O
sample	O
a	O
cluster	O
h	O
from	O
ph	O
and	O
then	O
draw	O
a	O
visible	B
state	O
v	O
from	O
pvh	O
for	O
a	O
set	O
of	O
i	O
i	O
d	O
data	O
vn	O
a	O
mixture	B
model	B
is	O
of	O
the	O
form	O
hn	O
vn	O
pvnhnphn	O
clustering	B
is	O
achieved	O
by	O
inference	B
of	O
argmax	O
vn	O
which	O
thanks	O
to	O
the	O
factorised	B
form	O
of	O
the	O
distribution	B
is	O
equivalent	B
to	O
computing	O
arg	O
maxhn	O
phnvn	O
for	O
each	O
datapoint	O
in	O
this	O
way	O
we	O
can	O
cluster	O
many	O
kinds	O
of	O
data	O
for	O
which	O
a	O
distance	O
measure	O
in	O
the	O
sense	O
of	O
the	O
classical	O
k-means	B
algorithm	B
is	O
not	O
directly	O
apparent	O
explicitly	O
writing	O
the	O
dependence	O
on	O
the	O
parameters	O
the	O
model	B
is	O
pv	O
h	O
pvh	O
vhph	O
h	O
the	O
optimal	O
parameters	O
vh	O
h	O
of	O
a	O
mixture	B
model	B
are	O
then	O
most	O
commonly	O
set	O
by	O
maximum	B
likelihood	B
opt	O
argmax	O
vn	O
numerically	O
this	O
can	O
be	O
achieved	O
using	O
an	O
optimisation	B
procedure	O
such	O
as	O
gradient	B
based	O
approaches	O
alternatively	O
by	O
treating	O
the	O
component	O
indices	O
as	O
latent	B
variables	O
one	O
may	O
also	O
apply	O
the	O
em	B
algorithm	B
as	O
described	O
in	O
the	O
following	O
section	O
which	O
in	O
many	O
classical	O
models	O
produces	O
simple	O
update	O
formulae	O
expectation	B
maximisation	B
for	O
mixture	B
models	O
h	O
vh	O
hn	O
vn	O
n	O
figure	O
a	O
mixture	B
model	B
has	O
a	O
trivial	O
graphical	O
representation	O
as	O
a	O
dag	O
with	O
a	O
single	O
hidden	B
node	O
which	O
can	O
be	O
in	O
and	O
one	O
of	O
h	O
states	O
i	O
h	O
the	O
parameters	O
are	O
assumed	O
common	O
across	O
all	O
datapoints	O
example	O
the	O
data	O
in	O
naturally	O
has	O
two	O
clusters	O
and	O
can	O
be	O
modelled	O
with	O
a	O
mixture	B
of	O
two	O
two-dimensional	O
gaussians	O
each	O
gaussian	B
describing	O
one	O
of	O
the	O
clusters	O
here	O
there	O
is	O
a	O
clear	O
visual	O
interpretation	O
of	O
the	O
meaning	O
of	O
cluster	O
with	O
the	O
mixture	B
model	B
placing	O
two	O
datapoints	O
in	O
the	O
same	O
cluster	O
if	O
they	O
are	O
both	O
likely	O
to	O
be	O
generated	O
by	O
the	O
same	O
model	B
component	O
expectation	B
maximisation	B
for	O
mixture	B
models	O
by	O
treating	O
the	O
index	O
h	O
as	O
a	O
missing	B
variable	O
mixture	B
models	O
can	O
be	O
trained	O
using	O
the	O
em	B
algorithm	B
there	O
are	O
two	O
sets	O
of	O
parameters	O
vh	O
for	O
each	O
component	O
model	B
pvh	O
vh	O
and	O
h	O
for	O
the	O
mixture	B
weights	O
ph	O
h	O
according	O
to	O
the	O
general	O
approach	B
for	O
i	O
i	O
d	O
data	O
of	O
we	O
need	O
to	O
consider	O
the	O
energy	B
term	O
where	O
e	O
pvn	O
h	O
pvnh	O
poldhvn	O
ph	O
poldhvn	O
pvnh	O
old	O
vhph	O
old	O
h	O
and	O
maximise	O
with	O
respect	O
to	O
the	O
parameters	O
vh	O
h	O
h	O
h	O
unconstrained	O
discrete	B
tables	O
here	O
we	O
consider	O
training	B
a	O
simple	O
belief	O
networkpvh	O
vhph	O
h	O
v	O
v	O
h	O
h	O
in	O
which	O
the	O
tables	O
are	O
unconstrained	O
this	O
is	O
a	O
special	O
case	O
of	O
the	O
more	O
general	O
framework	O
discussed	O
in	O
figure	O
two	O
dimensional	O
data	O
which	O
displays	O
clusters	O
in	O
this	O
case	O
a	O
gaussian	B
mixture	B
model	B
would	O
fit	O
the	O
data	O
well	O
for	O
suitable	O
means	O
and	O
covariances	O
draft	O
march	O
expectation	B
maximisation	B
for	O
mixture	B
models	O
h	O
ph	O
isolating	O
the	O
dependence	O
of	O
equation	B
on	O
ph	O
we	O
obtain	O
m-step	B
ph	O
if	O
no	O
constraint	O
is	O
placed	O
on	O
ph	O
h	O
we	O
may	O
write	O
the	O
parameters	O
as	O
simply	O
ph	O
with	O
the	O
understanding	O
that	O
ph	O
we	O
now	O
wish	O
to	O
maximise	O
equation	B
with	O
respect	O
to	O
ph	O
under	O
the	O
constraint	O
j	O
ph	O
it	O
is	O
standard	O
to	O
treat	O
this	O
maximisation	B
problem	B
using	O
lagrange	O
multipliers	O
see	O
here	O
we	O
take	O
an	O
alternative	O
approach	B
based	O
on	O
recognising	O
the	O
similarity	O
of	O
the	O
above	O
to	O
a	O
form	O
of	O
kullback-leibler	B
divergence	B
first	O
we	O
define	O
the	O
distribution	B
poldhvn	O
log	O
ph	O
h	O
poldhvn	O
poldhvn	O
h	O
ph	O
then	O
maximising	O
equation	B
is	O
equivalent	B
to	O
maximising	O
ph	O
since	O
the	O
two	O
expressions	O
are	O
related	O
by	O
the	O
constant	O
factor	B
poldhvn	O
by	O
subtracting	O
the	O
independent	O
term	O
ph	O
from	O
equation	B
we	O
obtain	O
the	O
negative	O
kullback-leibler	B
divergence	B
kl	O
pp	O
this	O
means	O
that	O
the	O
optimal	O
ph	O
is	O
that	O
distribution	B
which	O
minimises	O
the	O
kullback-leibler	B
divergence	B
optimally	O
therefore	O
ph	O
ph	O
so	O
that	O
h	O
pvh	O
jpv	O
ih	O
j	O
v	O
l	O
differentiating	O
with	O
respect	O
to	O
pv	O
ih	O
j	O
and	O
equating	O
to	O
zero	O
i	O
v	O
poldhvn	O
log	O
p	O
pv	O
ih	O
j	O
i	O
i	O
poldh	O
jv	O
i	O
i	O
i	O
poldh	O
jv	O
i	O
i	O
i	O
poldh	O
jv	O
i	O
i	O
i	O
poldh	O
jv	O
i	O
l	O
pv	O
ih	O
j	O
hence	O
pnewv	O
ih	O
j	O
pnewv	O
ih	O
j	O
draft	O
march	O
which	O
using	O
the	O
normalisation	B
requirement	O
gives	O
pnewh	O
poldhvn	O
poldhvn	O
m-step	B
pvh	O
the	O
dependence	O
of	O
equation	B
on	O
pvh	O
is	O
n	O
h	O
poldhvn	O
vh	O
if	O
the	O
distributions	O
vh	O
poldhvn	O
are	O
not	O
constrained	O
we	O
can	O
apply	O
a	O
similar	O
kullback-leibler	O
method	O
as	O
we	O
did	O
in	O
for	O
compatibility	O
with	O
other	O
texts	O
here	O
we	O
demonstrate	O
the	O
more	O
standard	O
lagrange	O
procedure	O
we	O
need	O
to	O
ensure	O
that	O
pvh	O
is	O
a	O
distribution	B
for	O
each	O
of	O
the	O
mixture	B
states	O
h	O
h	O
this	O
can	O
be	O
achieved	O
using	O
a	O
set	O
of	O
lagrange	O
multipliers	O
giving	O
the	O
lagrangian	B
expectation	B
maximisation	B
for	O
mixture	B
models	O
h	O
vih	O
hn	O
vn	O
i	O
i	O
d	O
n	O
n	O
figure	O
mixture	B
of	O
a	O
product	O
of	O
bernoulli	B
distributions	O
in	O
a	O
bayesian	B
treatment	O
a	O
parameter	B
prior	B
is	O
used	O
in	O
the	O
text	O
we	O
simply	O
set	O
the	O
parameters	O
using	O
maximum	B
likelihood	B
e-step	B
according	O
to	O
the	O
general	O
em	B
procedure	O
optimally	O
we	O
set	O
pnewhvn	O
phvn	O
pnewhvn	O
pvnhph	O
h	O
pvnhph	O
equations	O
are	O
repeated	O
until	O
convergence	O
the	O
initialisation	O
of	O
the	O
tables	O
and	O
mixture	B
probabilities	O
can	O
severely	O
affect	O
the	O
quality	O
of	O
the	O
solution	O
found	O
since	O
the	O
likelihood	B
often	O
has	O
local	B
optima	O
if	O
random	O
initialisations	O
are	O
used	O
it	O
is	O
recommended	O
to	O
record	O
the	O
converged	O
value	B
of	O
the	O
likelihood	B
itself	O
to	O
see	O
which	O
parameters	O
have	O
the	O
higher	O
likelihood	B
the	O
solution	O
with	O
the	O
highest	O
likelihood	B
is	O
to	O
be	O
preferred	O
mixture	B
of	O
product	O
of	O
bernoulli	B
distributions	O
we	O
describe	O
a	O
simple	O
mixture	B
model	B
that	O
can	O
be	O
used	O
to	O
clustering	B
binary	O
vectors	O
v	O
vdt	O
vi	O
the	O
mixture	B
of	O
bernoulli	B
model	B
is	O
given	O
by	O
pv	O
ph	O
pvih	O
where	O
each	O
term	O
pvih	O
is	O
a	O
bernoulli	B
distribution	B
the	O
model	B
is	O
depicted	O
in	O
and	O
has	O
parameters	O
ph	O
and	O
pvi	O
h	O
h	O
em	B
training	B
to	O
train	O
the	O
model	B
under	O
maximum	B
likelihood	B
it	O
is	O
convenient	O
to	O
use	O
the	O
em	B
algorithm	B
which	O
as	O
usual	O
may	O
be	O
derived	O
by	O
writing	O
down	O
the	O
energy	B
n	O
pvn	O
n	O
i	O
i	O
n	O
and	O
then	O
performing	O
the	O
maximisation	B
over	O
the	O
table	O
entries	O
from	O
our	O
general	O
results	O
we	O
may	O
immediately	O
jump	O
to	O
the	O
updates	O
the	O
m-step	B
is	O
given	O
by	O
i	O
poldh	O
jvn	O
i	O
i	O
poldh	O
jvn	O
i	O
n	O
i	O
poldh	O
jvn	O
pvn	O
n	O
n	O
i	O
n	O
poldh	O
jvn	O
n	O
pvn	O
i	O
j	O
pnewvi	O
j	O
pnewh	O
j	O
and	O
the	O
e-step	B
by	O
poldh	O
jvn	O
ph	O
j	O
is	O
similar	O
to	O
the	O
naive	B
bayes	I
classifier	B
in	O
which	O
the	O
class	O
labels	O
are	O
always	O
hidden	B
draft	O
march	O
expectation	B
maximisation	B
for	O
mixture	B
models	O
figure	O
data	O
from	O
questionnaire	O
responses	O
people	O
were	O
each	O
asked	O
questions	O
with	O
yes	O
and	O
no	O
answers	O
black	O
denotes	O
that	O
the	O
absence	O
of	O
a	O
response	O
data	O
this	O
training	B
data	O
was	O
generated	O
by	O
two	O
component	O
binomial	B
mixture	B
missing	B
data	I
was	O
simulated	O
by	O
randomly	O
removing	O
values	O
from	O
the	O
dataset	O
equations	O
are	O
iterated	O
until	O
convergence	O
if	O
an	O
attribute	O
i	O
is	O
missing	B
for	O
datapoint	O
n	O
one	O
needs	O
to	O
sum	O
over	O
the	O
states	O
of	O
the	O
corresponding	O
vn	O
effect	O
of	O
performing	O
the	O
summation	O
for	O
this	O
model	B
is	O
simply	O
to	O
remove	O
the	O
corresponding	O
factor	B
pvn	O
from	O
the	O
algorithm	B
see	O
i	O
the	O
i	O
initialisation	O
the	O
em	B
algorithm	B
can	O
be	O
very	O
sensitive	O
to	O
initial	O
conditions	O
consider	O
the	O
following	O
initialisation	O
pvi	O
j	O
with	O
ph	O
set	O
arbitrarily	O
this	O
means	O
that	O
at	O
the	O
first	O
iteration	B
poldh	O
jvn	O
ph	O
j	O
the	O
subsequent	O
m-step	B
updates	O
are	O
pnewh	O
ph	O
pnewvih	O
j	O
pnewvih	O
j	O
for	O
any	O
j	O
this	O
means	O
that	O
the	O
parameters	O
pvh	O
immediately	O
become	O
independent	O
of	O
h	O
and	O
the	O
model	B
is	O
numerically	O
trapped	O
in	O
a	O
symmetric	O
solution	O
it	O
makes	O
sense	O
therefore	O
to	O
initialise	O
the	O
parameters	O
in	O
a	O
non-symmetric	O
fashion	O
example	O
a	O
company	O
sends	O
out	O
a	O
questionnaire	O
containing	O
a	O
set	O
of	O
d	O
yesno	O
questions	O
to	O
a	O
set	O
of	O
customers	O
the	O
binary	O
responses	O
of	O
a	O
customer	O
are	O
stored	O
in	O
a	O
vector	O
v	O
vdt	O
in	O
total	O
n	O
customers	O
send	O
back	O
their	O
questionnaires	O
vn	O
and	O
the	O
company	O
wishes	O
to	O
perform	O
an	O
analysis	B
to	O
find	O
what	O
kinds	O
of	O
customers	O
it	O
has	O
the	O
company	O
assumes	O
there	O
are	O
h	O
essential	O
types	O
of	O
customer	O
for	O
which	O
the	O
profile	O
of	O
responses	O
is	O
defined	O
by	O
only	O
the	O
customer	O
type	O
data	O
from	O
a	O
questionnaire	O
containing	O
questions	O
with	O
respondents	O
is	O
presented	O
in	O
the	O
data	O
has	O
a	O
large	O
number	O
of	O
missing	B
values	O
we	O
assume	O
there	O
are	O
h	O
kinds	O
of	O
respondents	O
and	O
attempt	O
to	O
assign	O
each	O
respondent	O
into	O
one	O
of	O
the	O
two	O
clusters	O
running	O
the	O
em	B
algorithm	B
on	O
this	O
data	O
with	O
random	O
initial	O
values	O
for	O
the	O
tables	O
produces	O
the	O
results	O
in	O
based	O
on	O
assigning	O
each	O
datapoint	O
vn	O
to	O
the	O
cluster	O
with	O
maximal	O
posterior	B
probability	O
hn	O
arg	O
maxh	O
phvn	O
given	O
a	O
trained	O
model	B
pvhph	O
the	O
model	B
assigns	O
of	O
the	O
data	O
to	O
the	O
correct	O
cluster	O
is	O
known	O
in	O
this	O
simulated	O
case	O
see	O
and	O
mixprodbern	O
m	O
draft	O
march	O
figure	O
em	B
learning	B
of	O
a	O
mixture	B
true	O
ph	O
of	O
bernoulli	B
products	O
and	O
learned	O
ph	O
for	O
h	O
true	O
pvh	O
and	O
learned	O
pvh	O
for	O
v	O
each	O
column	O
pair	O
corresponds	O
to	O
pvih	O
and	O
pvih	O
with	O
i	O
the	O
learned	O
probabilities	O
are	O
reasonably	O
close	O
to	O
the	O
true	O
values	O
the	O
gaussian	B
mixture	B
model	B
figure	O
top	O
a	O
selection	O
of	O
of	O
the	O
handwritten	B
digits	I
in	O
the	O
training	B
set	O
bottom	O
the	O
trained	O
cluster	O
outputs	O
pvi	O
for	O
h	O
mixtures	O
see	O
demomixbernoullidigits	O
m	O
example	O
digits	O
we	O
have	O
a	O
collection	O
of	O
handwritten	B
digits	I
which	O
we	O
wish	O
to	O
cluster	O
into	O
groups	O
each	O
digit	O
is	O
a	O
dimensional	O
binary	O
vector	O
using	O
a	O
mixture	B
of	O
bernoulli	B
products	O
trained	O
with	O
iterations	O
of	O
em	B
a	O
random	O
perturbation	O
of	O
the	O
mean	B
of	O
the	O
data	O
used	O
as	O
initialisation	O
the	O
clusters	O
are	O
presented	O
in	O
as	O
we	O
see	O
the	O
method	O
captures	O
natural	B
clusters	O
in	O
the	O
data	O
for	O
example	O
there	O
are	O
two	O
kinds	O
of	O
one	O
slightly	O
more	O
slanted	O
than	O
the	O
other	O
two	O
kinds	O
of	O
etc	O
the	O
gaussian	B
mixture	B
model	B
gaussians	O
are	O
particularly	O
convenient	O
continuous	B
mixture	B
components	O
since	O
they	O
constitute	O
bumps	O
of	O
probability	O
mass	O
aiding	O
an	O
intuitive	O
interpretation	O
of	O
the	O
model	B
as	O
a	O
reminder	O
a	O
d	O
dimensional	O
gaussian	B
distribution	B
for	O
a	O
continuous	B
variable	O
x	O
is	O
s	O
exp	O
mt	O
s	O
m	O
where	O
m	O
is	O
the	O
mean	B
and	O
s	O
is	O
the	O
covariance	B
matrix	B
a	O
mixture	B
of	O
gaussians	O
is	O
then	O
p	O
s	O
p	O
where	O
pi	O
is	O
the	O
mixture	B
weight	B
for	O
component	O
i	O
for	O
a	O
set	O
of	O
data	O
x	O
and	O
under	O
the	O
usual	O
exp	O
mit	O
s	O
i	O
mi	O
pxmi	O
sipi	O
i	O
i	O
d	O
assumption	O
the	O
log	O
likelihood	B
is	O
log	O
px	O
si	O
matrices	O
in	O
addition	O
to	O
pi	O
pi	O
log	O
where	O
the	O
parameters	O
are	O
si	O
pi	O
i	O
h	O
the	O
optimal	O
parameters	O
can	O
be	O
set	O
using	O
maximum	B
likelihood	B
bearing	O
in	O
mind	O
the	O
constraint	O
that	O
the	O
si	O
must	O
be	O
symmetric	O
positive	B
definite	I
i	O
pi	O
gradient	B
based	O
optimisation	B
approaches	O
are	O
feasible	O
under	O
a	O
parameterisation	B
of	O
the	O
si	O
cholesky	B
decomposition	B
and	O
pi	O
softmax	B
that	O
enforce	O
the	O
constraints	O
an	O
alternative	O
is	O
the	O
em	B
approach	B
which	O
in	O
this	O
case	O
is	O
particularly	O
convenient	O
since	O
it	O
automatically	O
provides	O
parameter	B
updates	O
that	O
ensure	O
these	O
constraints	O
em	B
algorithm	B
the	O
energy	B
term	O
is	O
pxn	O
draft	O
march	O
the	O
gaussian	B
mixture	B
model	B
plugging	O
in	O
the	O
definition	O
of	O
the	O
gaussian	B
components	O
we	O
have	O
poldixn	O
mit	O
s	O
i	O
mi	O
log	O
det	O
si	O
log	O
pi	O
the	O
m-step	B
requires	O
the	O
maximisation	B
of	O
the	O
above	O
with	O
respect	O
to	O
mi	O
si	O
pi	O
m-step	B
optimal	O
mi	O
maximising	O
equation	B
with	O
respect	O
to	O
mi	O
is	O
equivalent	B
to	O
minimising	O
poldixn	O
mit	O
s	O
i	O
mi	O
differentiating	O
with	O
respect	O
to	O
mi	O
and	O
equating	O
to	O
zero	O
we	O
have	O
mi	O
hence	O
optimally	O
mi	O
i	O
poldixns	O
poldixnxn	O
poldixn	O
poldixn	O
poldixn	O
poldni	O
mi	O
poldnixn	O
by	O
defining	O
the	O
membership	O
distribution	B
which	O
quantifies	O
the	O
membership	O
of	O
datapoints	O
to	O
cluster	O
i	O
we	O
can	O
write	O
equation	B
more	O
compactly	O
as	O
m-step	B
optimal	O
si	O
optimising	O
equation	B
with	O
respect	O
to	O
si	O
is	O
equivalent	B
to	O
minimising	O
i	O
log	O
i	O
poldixn	O
n	O
i	O
n	O
i	O
s	O
where	O
n	O
i	O
xn	O
mi	O
to	O
aid	O
the	O
matrix	B
calculus	B
we	O
isolate	O
the	O
dependency	O
on	O
si	O
to	O
give	O
log	O
i	O
poldixn	O
i	O
trace	O
poldixn	O
n	O
differentiating	O
with	O
respect	O
to	O
s	O
i	O
i	O
n	O
i	O
and	O
equating	O
to	O
zero	O
we	O
obtain	O
i	O
n	O
i	O
si	O
poldixn	O
poldixn	O
n	O
using	O
the	O
membership	O
poldni	O
the	O
optimal	O
si	O
is	O
given	O
by	O
si	O
poldni	O
mi	O
mit	O
draft	O
march	O
the	O
gaussian	B
mixture	B
model	B
figure	O
training	B
a	O
mixture	B
of	O
isotropic	B
gaussians	O
if	O
we	O
start	O
with	O
large	O
variances	O
for	O
the	O
gaussians	O
even	O
after	O
one	O
iteration	B
the	O
gaussians	O
are	O
centred	O
close	O
to	O
the	O
mean	B
of	O
the	O
the	O
gaussians	O
begin	O
to	O
separate	O
data	O
one	O
by	O
one	O
the	O
gaussians	O
move	O
towards	O
appropriate	O
parts	O
of	O
the	O
data	O
the	O
final	O
converged	O
solution	O
the	O
gaussians	O
are	O
constrained	O
to	O
have	O
variances	O
greater	O
than	O
a	O
set	O
amount	O
see	O
demogmmem	O
m	O
iteration	B
iterations	O
iterations	O
iterations	O
this	O
ensures	O
that	O
si	O
is	O
symmetric	O
positive	O
semi-definite	O
a	O
special	O
case	O
is	O
to	O
constrain	O
the	O
covariances	O
si	O
to	O
be	O
diagonal	O
for	O
which	O
the	O
update	O
is	O
see	O
si	O
poldnidiag	O
mi	O
where	O
above	O
diag	O
means	O
forming	O
a	O
new	O
matrix	B
from	O
the	O
matrix	B
m	O
with	O
zero	O
entries	O
except	O
for	O
the	O
diagonal	O
entries	O
of	O
m	O
a	O
more	O
extreme	O
case	O
is	O
that	O
of	O
isotropic	B
gaussians	O
si	O
i	O
i	O
the	O
reader	O
may	O
show	O
that	O
the	O
optimal	O
update	O
for	O
in	O
this	O
case	O
is	O
given	O
by	O
taking	O
the	O
average	B
of	O
the	O
diagonal	O
entries	O
of	O
the	O
i	O
diagonally	O
constrained	O
covariance	B
update	O
poldnixn	O
if	O
no	O
constraint	O
is	O
placed	O
on	O
the	O
weights	O
the	O
update	O
follows	O
the	O
general	O
formula	O
given	O
in	O
equation	B
i	O
dim	O
x	O
m-step	B
optimal	O
mixture	B
coefficients	O
poldixn	O
pi	O
n	O
e-step	B
explicitly	O
this	O
is	O
given	O
by	O
the	O
responsibility	B
poldixn	O
poldxnipi	O
mit	O
s	O
s	O
pi	O
exp	O
exp	O
poldixn	O
i	O
mi	O
the	O
above	O
equations	O
are	O
iterated	O
until	O
convergence	O
the	O
performance	B
of	O
em	B
for	O
gaussian	B
mixtures	O
can	O
be	O
strongly	O
dependent	O
on	O
the	O
initialisation	O
which	O
we	O
discuss	O
below	O
in	O
addition	O
constraints	O
on	O
the	O
covariance	B
matrix	B
are	O
required	O
in	O
order	O
to	O
find	O
sensible	O
solutions	O
draft	O
march	O
the	O
gaussian	B
mixture	B
model	B
practical	O
issues	O
infinite	O
troubles	O
a	O
difficulty	O
arises	O
with	O
using	O
maximum	B
likelihood	B
to	O
fit	O
a	O
gaussian	B
mixture	B
model	B
consider	O
placing	O
a	O
component	O
pxmi	O
si	O
with	O
mean	B
mi	O
set	O
to	O
one	O
of	O
the	O
datapoints	O
mi	O
xn	O
the	O
contribution	O
from	O
that	O
gaussian	B
will	O
be	O
pxnmi	O
si	O
si	O
xnts	O
e	O
i	O
xn	O
si	O
in	O
the	O
limit	O
that	O
the	O
width	O
of	O
the	O
covariance	B
goes	O
to	O
zero	O
eigenvalues	O
of	O
si	O
tend	O
to	O
zero	O
this	O
probability	O
density	B
becomes	O
infinite	O
this	O
means	O
that	O
one	O
can	O
obtain	O
a	O
maximum	B
likelihood	B
solution	O
by	O
placing	O
zero-width	O
gaussians	O
on	O
a	O
selection	O
of	O
the	O
datapoints	O
resulting	O
in	O
an	O
infinite	O
likelihood	B
this	O
is	O
clearly	O
undesirable	O
and	O
arises	O
because	O
in	O
this	O
case	O
the	O
maximum	B
likelihood	B
solution	O
does	O
not	O
constrain	O
the	O
parameters	O
in	O
a	O
sensible	O
way	O
note	O
that	O
this	O
is	O
not	O
related	O
to	O
the	O
em	B
algorithm	B
but	O
a	O
property	O
of	O
the	O
maximum	B
likelihood	B
method	O
itself	O
all	O
computational	O
methods	O
which	O
aim	O
to	O
fit	O
unconstrained	O
mixtures	O
of	O
gaussians	O
using	O
maximum	B
likelihood	B
therefore	O
succeed	O
in	O
finding	O
reasonable	O
solutions	O
merely	O
by	O
getting	O
trapped	O
in	O
favourable	O
local	B
maxima	O
a	O
remedy	O
is	O
to	O
include	O
an	O
additional	O
constraint	O
on	O
the	O
width	O
of	O
the	O
gaussians	O
ensuring	O
that	O
they	O
cannot	O
become	O
too	O
small	O
one	O
approach	B
is	O
to	O
monitor	O
the	O
eigenvalues	O
of	O
each	O
covariance	B
matrix	B
and	O
if	O
an	O
update	O
would	O
result	O
in	O
a	O
new	O
eigenvalue	O
smaller	O
than	O
a	O
desired	O
threshold	O
the	O
update	O
is	O
rejected	O
in	O
gmmem	O
m	O
we	O
use	O
a	O
similar	O
approach	B
in	O
which	O
we	O
constrain	O
the	O
determinant	B
product	O
of	O
the	O
eigenvalues	O
of	O
the	O
covariances	O
to	O
be	O
greater	O
than	O
a	O
desired	O
specified	O
minimum	O
value	B
one	O
can	O
view	O
the	O
formal	O
failure	O
of	O
maximum	B
likelihood	B
in	O
the	O
case	O
of	O
gaussian	B
mixtures	O
as	O
a	O
result	O
of	O
an	O
inappropriate	O
prior	B
maximum	B
likelihood	B
is	O
equivalent	B
to	O
map	B
in	O
which	O
a	O
flat	O
prior	B
is	O
placed	O
on	O
each	O
matrix	B
si	O
this	O
is	O
unreasonable	O
since	O
the	O
matrices	O
are	O
required	O
to	O
be	O
positive	B
definite	I
and	O
of	O
non-vanishing	O
width	O
a	O
bayesian	B
solution	O
to	O
this	O
problem	B
is	O
possible	O
placing	O
a	O
prior	B
on	O
covariance	B
matrices	O
the	O
natural	B
prior	B
in	O
this	O
case	O
is	O
the	O
wishart	B
distribution	B
or	O
a	O
gamma	B
distribution	B
in	O
the	O
case	O
of	O
a	O
diagonal	O
covariance	B
initialisation	O
a	O
useful	O
intialisation	O
strategy	O
is	O
to	O
set	O
the	O
covariances	O
to	O
be	O
diagonal	O
with	O
large	O
variances	O
this	O
gives	O
the	O
components	O
a	O
chance	O
to	O
sense	O
where	O
data	O
lies	O
an	O
illustration	O
of	O
the	O
performance	B
of	O
the	O
algorithm	B
is	O
given	O
in	O
symmetry	B
breaking	I
if	O
the	O
covariances	O
are	O
initialised	O
to	O
large	O
values	O
the	O
em	B
algorithm	B
appears	O
to	O
make	O
little	O
progress	O
in	O
the	O
first	O
iterations	O
as	O
each	O
component	O
jostles	O
with	O
the	O
others	O
to	O
try	O
to	O
explain	O
the	O
data	O
eventually	O
one	O
gaussian	B
component	O
breaks	O
away	O
and	O
takes	O
responsibility	B
for	O
explaining	O
the	O
data	O
in	O
its	O
vicinity	O
see	O
the	O
origin	O
of	O
this	O
jostling	O
is	O
an	O
inherent	O
symmetry	O
in	O
the	O
solution	O
it	O
makes	O
no	O
difference	O
to	O
the	O
likelihood	B
if	O
we	O
relabel	O
what	O
the	O
components	O
are	O
called	O
this	O
permutation	O
symmetry	O
causes	O
initial	O
confusion	O
as	O
to	O
which	O
model	B
should	O
explain	O
which	O
parts	O
of	O
the	O
data	O
eventually	O
this	O
symmetry	O
is	O
broken	O
and	O
a	O
local	B
solution	O
is	O
found	O
the	O
symmetries	O
can	O
severely	O
handicap	O
em	B
in	O
fitting	O
a	O
large	O
number	O
of	O
models	O
in	O
the	O
mixture	B
since	O
the	O
number	O
of	O
permutations	O
increases	O
dramatically	O
with	O
the	O
number	O
of	O
components	O
a	O
heuristic	O
is	O
to	O
begin	O
with	O
a	O
small	O
number	O
of	O
components	O
say	O
two	O
for	O
which	O
symmetry	B
breaking	I
is	O
less	O
problematic	O
once	O
a	O
local	B
broken	O
solution	O
has	O
been	O
found	O
more	O
models	O
are	O
included	O
into	O
the	O
mixture	B
initialised	O
close	O
to	O
the	O
currently	O
found	O
solutions	O
in	O
this	O
way	O
a	O
hierarchical	O
breaking	O
scheme	O
is	O
envisaged	O
another	O
popular	O
method	O
for	O
initialisation	O
is	O
to	O
center	O
the	O
means	O
to	O
those	O
found	O
by	O
the	O
k-means	B
algorithm	B
however	O
this	O
itself	O
requires	O
a	O
heuristic	O
initialisation	O
classification	B
using	O
gaussian	B
mixture	B
models	O
consider	O
data	O
drawn	O
from	O
two	O
classes	O
c	O
we	O
can	O
fit	O
a	O
gmm	O
pxc	O
to	O
the	O
data	O
from	O
class	O
and	O
another	O
gmm	O
pxc	O
to	O
the	O
data	O
from	O
class	O
this	O
gives	O
rise	O
to	O
two	O
class-conditional	O
gmms	O
pxcxc	O
picn	O
mc	O
i	O
sc	O
i	O
draft	O
march	O
the	O
gaussian	B
mixture	B
model	B
figure	O
a	O
gaussian	B
mixture	B
model	B
with	O
h	O
components	O
there	O
is	O
a	O
component	O
with	O
large	O
variance	B
and	O
small	O
weight	B
that	O
has	O
little	O
effect	O
on	O
the	O
distribution	B
close	O
to	O
where	O
the	O
other	O
three	O
components	O
have	O
appreciable	O
mass	O
as	O
we	O
move	O
further	O
away	O
this	O
additional	O
component	O
gains	O
in	O
influence	O
the	O
gmm	O
probability	O
density	B
function	B
from	O
plotted	O
on	O
a	O
log	O
scale	O
the	O
influence	O
of	O
each	O
gaussian	B
far	O
from	O
the	O
origin	O
becomes	O
clearer	O
for	O
a	O
novel	O
point	O
x	O
the	O
posterior	B
class	O
probability	O
is	O
pcx	O
px	O
where	O
pc	O
is	O
the	O
prior	B
class	O
probability	O
the	O
maximum	B
likelihood	B
setting	O
is	O
that	O
pc	O
is	O
proportional	O
to	O
the	O
number	O
of	O
training	B
points	O
in	O
class	O
c	O
consider	O
a	O
testpoint	O
x	O
a	O
long	O
way	O
from	O
the	O
training	B
data	O
for	O
both	O
classes	O
for	O
such	O
a	O
point	O
the	O
probability	O
that	O
either	O
of	O
the	O
two	O
class	O
models	O
generated	O
the	O
data	O
is	O
very	O
low	O
however	O
one	O
will	O
be	O
much	O
lower	O
than	O
the	O
other	O
gaussians	O
drop	O
exponentially	O
quickly	O
meaning	O
that	O
the	O
posterior	B
probability	O
will	O
be	O
confidently	O
close	O
to	O
for	O
that	O
class	O
which	O
has	O
a	O
component	O
closest	O
to	O
x	O
this	O
is	O
an	O
unfortunate	O
property	O
since	O
we	O
would	O
end	O
up	O
confidently	O
predicting	O
the	O
class	O
of	O
novel	O
data	O
that	O
is	O
not	O
similar	O
to	O
anything	O
we	O
ve	O
seen	O
before	O
we	O
would	O
prefer	O
the	O
opposite	O
effect	O
that	O
for	O
novel	O
data	O
far	O
from	O
the	O
training	B
data	O
the	O
classification	B
confidence	O
drops	O
and	O
all	O
classes	O
become	O
equally	O
likely	O
a	O
remedy	O
for	O
this	O
situation	O
is	O
to	O
include	O
an	O
additional	O
component	O
in	O
the	O
gaussian	B
mixture	B
for	O
each	O
class	O
that	O
is	O
very	O
broad	O
we	O
first	O
collect	O
the	O
input	O
data	O
from	O
all	O
classes	O
into	O
a	O
dataset	O
x	O
and	O
let	O
m	O
be	O
the	O
mean	B
of	O
all	O
this	O
data	O
and	O
s	O
the	O
covariance	B
then	O
for	O
the	O
model	B
of	O
each	O
class	O
c	O
data	O
we	O
include	O
an	O
additional	O
gaussian	B
the	O
notational	O
dependency	O
on	O
x	O
pxc	O
in	O
mc	O
pc	O
i	O
sc	O
i	O
pc	O
m	O
s	O
pc	O
i	O
where	O
pc	O
i	O
i	O
h	O
i	O
h	O
where	O
is	O
a	O
small	O
positive	O
value	B
and	O
inflates	O
the	O
covariance	B
take	O
and	O
in	O
demogmmclass	O
m	O
the	O
effect	O
of	O
the	O
additional	O
component	O
on	O
the	O
training	B
likelihood	B
is	O
negligible	O
since	O
it	O
has	O
small	O
weight	B
and	O
large	O
variance	B
compared	O
to	O
the	O
other	O
components	O
see	O
however	O
as	O
we	O
move	O
away	O
from	O
the	O
region	O
where	O
the	O
first	O
h	O
components	O
have	O
appreciable	O
mass	O
the	O
additional	O
component	O
gains	O
in	O
influence	O
since	O
it	O
has	O
a	O
higher	O
variance	B
if	O
we	O
include	O
the	O
same	O
additional	O
component	O
in	O
the	O
gmm	O
for	O
each	O
class	O
c	O
then	O
the	O
influence	O
of	O
this	O
additional	O
component	O
will	O
be	O
the	O
same	O
for	O
each	O
class	O
dominating	O
as	O
we	O
move	O
far	O
from	O
the	O
influence	O
of	O
the	O
other	O
components	O
for	O
a	O
point	O
far	O
from	O
the	O
training	B
data	O
the	O
likelihood	B
will	O
be	O
roughly	O
equal	O
for	O
each	O
class	O
since	O
in	O
this	O
region	O
the	O
additional	O
broad	O
component	O
dominates	O
with	O
equal	O
measure	O
the	O
posterior	B
distribution	B
will	O
then	O
tend	O
to	O
the	O
prior	B
class	O
probability	O
pc	O
mitigating	O
the	O
deleterious	O
effect	O
of	O
a	O
single	O
gmm	O
dominating	O
when	O
a	O
testpoint	O
is	O
far	O
from	O
the	O
training	B
data	O
draft	O
march	O
the	O
gaussian	B
mixture	B
model	B
figure	O
class	O
conditional	B
gmm	O
training	B
and	O
classification	B
data	O
from	O
two	O
different	O
classes	O
we	O
fit	O
a	O
gmm	O
with	O
two	O
components	O
to	O
the	O
data	O
from	O
each	O
class	O
the	O
diamond	O
is	O
a	O
test	O
point	O
far	O
from	O
the	O
training	B
data	O
we	O
will	O
clasb	O
upper	O
subpanel	O
are	O
the	O
class	O
sify	O
probabilities	O
pc	O
for	O
the	O
training	B
points	O
and	O
the	O
point	O
being	O
the	O
test	O
point	O
the	O
lower	O
subpanel	O
are	O
the	O
class	O
probabilities	O
but	O
including	O
the	O
additional	O
large	O
variance	B
gaussian	B
term	O
see	O
demogmmclass	O
m	O
example	O
the	O
data	O
in	O
has	O
a	O
cluster	O
structure	B
for	O
each	O
class	O
based	O
on	O
fitting	O
a	O
gmm	O
to	O
each	O
of	O
the	O
two	O
classes	O
a	O
test	O
point	O
far	O
from	O
the	O
training	B
data	O
is	O
confidently	O
classified	O
as	O
belonging	O
to	O
class	O
this	O
is	O
an	O
undesired	O
effect	O
since	O
we	O
would	O
prefer	O
that	O
points	O
far	O
from	O
the	O
training	B
data	O
are	O
not	O
classified	O
with	O
any	O
certainty	O
by	O
including	O
an	O
additional	O
large	O
variance	B
gaussian	B
component	O
for	O
each	O
class	O
this	O
has	O
little	O
effect	O
on	O
the	O
class	O
probabilities	O
of	O
the	O
training	B
data	O
yet	O
has	O
the	O
desired	O
effect	O
of	O
making	O
the	O
class	O
probability	O
for	O
the	O
test	O
point	O
maximally	O
uncertain	B
the	O
parzen	B
estimator	I
the	O
parzen	O
density	B
estimator	O
is	O
formed	O
by	O
placing	O
a	O
bump	O
of	O
mass	O
on	O
each	O
datapoint	O
a	O
popular	O
choice	O
is	O
a	O
d	O
dimensional	O
x	O
n	O
px	O
xn	O
n	O
px	O
n	O
giving	O
the	O
mixture	B
of	O
gaussians	O
e	O
there	O
is	O
no	O
training	B
required	O
for	O
a	O
parzen	B
estimator	I
only	O
the	O
positions	O
of	O
the	O
n	O
datapoints	O
need	O
storing	O
whilst	O
the	O
parzen	O
technique	O
is	O
a	O
reasonable	O
and	O
cheap	O
way	O
to	O
form	O
a	O
density	B
estimator	O
it	O
does	O
not	O
enable	O
us	O
to	O
form	O
any	O
simpler	O
description	O
of	O
the	O
data	O
in	O
particular	O
we	O
cannot	O
perform	O
clustering	B
since	O
there	O
is	O
no	O
lower	O
number	O
of	O
clusters	O
assumed	O
to	O
underly	O
the	O
data	O
generating	O
process	O
this	O
is	O
in	O
contrast	O
to	O
gmms	O
trained	O
using	O
maximum	B
likelihood	B
on	O
a	O
fixed	O
number	O
h	O
n	O
of	O
components	O
k-means	B
consider	O
a	O
mixture	B
of	O
k	O
isotropic	B
gaussians	O
in	O
which	O
each	O
covariance	B
is	O
constrained	O
to	O
be	O
equal	O
to	O
px	O
pin	O
mi	O
whilst	O
the	O
em	B
algorithm	B
breaks	O
down	O
if	O
a	O
gaussian	B
component	O
is	O
allowed	O
to	O
set	O
mi	O
equal	O
to	O
a	O
datapoint	O
with	O
by	O
constraining	O
all	O
components	O
to	O
have	O
the	O
same	O
variance	B
the	O
algorithm	B
has	O
a	O
well	O
draft	O
march	O
algorithm	B
k-means	B
the	O
gaussian	B
mixture	B
model	B
initialise	O
the	O
centres	O
mi	O
i	O
k	O
while	O
not	O
converged	O
do	O
for	O
each	O
centre	O
i	O
find	O
all	O
the	O
xn	O
for	O
which	O
i	O
is	O
the	O
nearest	O
euclidean	O
sense	O
centre	O
call	O
this	O
set	O
of	O
points	O
ni	O
let	O
ni	O
be	O
the	O
number	O
of	O
datapoints	O
in	O
set	O
ni	O
update	O
the	O
means	O
n	O
ni	O
xn	O
mnew	O
i	O
ni	O
end	O
while	O
figure	O
datapoints	O
clustered	O
using	O
k-means	B
with	O
components	O
the	O
means	O
are	O
given	O
by	O
the	O
evolution	O
of	O
the	O
red	O
crosses	O
mean	B
square	O
distance	O
to	O
nearest	O
centre	O
with	O
iterations	O
of	O
the	O
algorithm	B
the	O
means	O
were	O
initialised	O
to	O
close	O
to	O
the	O
overall	O
mean	B
of	O
the	O
data	O
see	O
demokmeans	O
m	O
defined	O
limit	O
as	O
the	O
reader	O
may	O
show	O
that	O
in	O
this	O
case	O
the	O
membership	O
distribution	B
equation	B
becomes	O
deterministic	B
poldni	O
if	O
mi	O
is	O
closest	O
to	O
xn	O
otherwise	O
in	O
this	O
limit	O
the	O
em	B
update	O
for	O
the	O
mean	B
mi	O
is	O
given	O
by	O
taking	O
the	O
average	B
of	O
the	O
points	O
closest	O
to	O
mi	O
this	O
limiting	O
and	O
constrained	O
gmm	O
then	O
reduces	O
to	O
the	O
so-called	O
k-means	B
algorithm	B
despite	O
its	O
simplicity	O
the	O
k-means	B
algorithm	B
converges	O
quickly	O
and	O
often	O
gives	O
a	O
reasonable	O
clustering	B
provided	O
the	O
centres	O
are	O
initialised	O
reasonably	O
see	O
k-means	B
is	O
often	O
used	O
as	O
a	O
simple	O
form	O
of	O
data	B
compression	I
rather	O
than	O
sending	O
the	O
datapoint	O
xn	O
one	O
sends	O
instead	O
the	O
index	O
of	O
the	O
centre	O
to	O
which	O
it	O
is	O
associated	O
this	O
is	O
called	O
vector	B
quantisation	I
and	O
is	O
a	O
form	O
of	O
lossy	O
compression	O
to	O
improve	O
the	O
quality	O
more	O
information	O
can	O
be	O
transmitted	O
such	O
as	O
an	O
approximation	B
of	O
the	O
difference	O
between	O
x	O
and	O
the	O
corresponding	O
mean	B
m	O
which	O
can	O
be	O
used	O
to	O
improve	O
the	O
reconstruction	O
of	O
the	O
compressed	O
datapoint	O
bayesian	B
mixture	B
models	O
bayesian	B
extensions	O
include	O
placing	O
priors	O
on	O
the	O
parameters	O
of	O
each	O
model	B
in	O
the	O
mixture	B
and	O
also	O
on	O
the	O
component	O
distribution	B
in	O
most	O
cases	O
this	O
will	O
give	O
rise	O
to	O
the	O
marginal	B
likelihood	B
being	O
an	O
intractable	O
integral	O
methods	O
that	O
approximate	B
the	O
integral	O
include	O
sampling	B
techniques	O
see	O
also	O
for	O
an	O
approximate	B
variational	O
treatment	O
focussed	O
on	O
bayesian	B
gaussian	B
mixture	B
models	O
semi-supervised	B
learning	B
in	O
some	O
cases	O
we	O
may	O
know	O
to	O
which	O
mixture	B
component	O
certain	O
datapoints	O
belong	O
given	O
this	O
information	O
we	O
want	O
to	O
fit	O
a	O
mixture	B
model	B
with	O
a	O
specified	O
number	O
of	O
components	O
h	O
and	O
parameters	O
we	O
write	O
hm	O
m	O
m	O
for	O
the	O
m	O
known	O
datapoints	O
and	O
corresponding	O
components	O
and	O
hn	O
n	O
n	O
for	O
the	O
remaining	O
datapoints	O
whose	O
components	O
hn	O
are	O
unknown	O
we	O
aim	O
then	O
to	O
maximise	O
the	O
draft	O
march	O
mixture	B
of	I
experts	I
n	O
n	O
xn	O
yn	O
hn	O
w	O
u	O
figure	O
mixture	B
of	I
experts	I
model	B
the	O
prediction	O
of	O
the	O
output	O
yn	O
or	O
continuous	B
given	O
the	O
input	O
xn	O
averages	O
over	O
individual	O
experts	O
pynxn	O
whn	O
the	O
expert	O
hn	O
is	O
selected	O
by	O
the	O
gating	O
mechanism	O
with	O
probability	O
phnxn	O
u	O
so	O
that	O
some	O
experts	O
will	O
be	O
more	O
able	O
to	O
predict	O
the	O
output	O
for	O
xn	O
in	O
their	O
part	O
of	O
the	O
space	O
the	O
parameters	O
w	O
u	O
can	O
be	O
learned	O
by	O
maximum	B
likelihood	B
after	O
marginalising	O
over	O
the	O
hidden	B
expert	O
variables	O
likelihood	B
m	O
pvm	O
n	O
hn	O
pvnhn	O
if	O
we	O
were	O
to	O
lump	O
all	O
the	O
datapoints	O
together	O
this	O
is	O
essentially	O
equivalent	B
to	O
the	O
standard	O
unsupervised	B
case	O
expect	O
that	O
some	O
of	O
the	O
h	O
are	O
fixed	O
into	O
known	O
states	O
the	O
only	O
effect	O
on	O
the	O
em	B
algorithm	B
is	O
therefore	O
in	O
the	O
terms	O
poldhv	O
which	O
are	O
delta	O
functions	O
in	O
the	O
known	O
state	O
resulting	O
in	O
a	O
minor	O
modification	O
of	O
the	O
standard	O
algorithm	B
mixture	B
of	I
experts	I
the	O
mixture	B
of	I
experts	I
is	O
related	O
to	O
discriminative	B
training	B
of	O
an	O
output	O
y	O
distribution	B
conditioned	O
on	O
an	O
input	O
x	O
this	O
can	O
be	O
used	O
in	O
either	O
the	O
regression	B
of	O
classification	B
contexts	O
and	O
has	O
the	O
general	O
form	O
see	O
pyx	O
w	O
u	O
pyx	O
whphx	O
u	O
here	O
h	O
indexes	O
the	O
mixture	B
component	O
each	O
expert	O
has	O
parameters	O
w	O
wh	O
and	O
corresponding	O
gating	O
parameters	O
u	O
uh	O
unlike	O
a	O
standard	O
mixture	B
model	B
the	O
component	O
distribution	B
phx	O
u	O
is	O
dependent	O
on	O
the	O
input	O
x	O
this	O
so-called	O
gating	O
distribution	B
is	O
conventionally	O
taken	O
to	O
be	O
of	O
the	O
softmax	B
form	O
phx	O
u	O
eut	O
h	O
eut	O
hx	O
the	O
idea	O
is	O
that	O
we	O
have	O
a	O
set	O
of	O
h	O
predictive	O
models	O
pyx	O
wh	O
each	O
with	O
a	O
different	O
parameter	B
wh	O
h	O
h	O
how	O
suitable	O
model	B
h	O
is	O
for	O
predicting	O
with	O
the	O
current	O
input	O
x	O
is	O
determined	O
by	O
the	O
alignment	O
of	O
input	O
x	O
with	O
the	O
weight	B
vector	O
uh	O
in	O
this	O
way	O
the	O
input	O
x	O
is	O
softly	O
assigned	O
to	O
the	O
appropriate	O
experts	O
maximum	B
likelihood	B
training	B
can	O
be	O
achieved	O
using	O
a	O
form	O
of	O
em	B
we	O
will	O
not	O
derive	O
the	O
em	B
algorithm	B
for	O
the	O
mixture	B
of	I
experts	I
model	B
in	O
full	O
merely	O
pointing	O
the	O
direction	O
along	O
which	O
the	O
derivation	O
would	O
continue	O
for	O
a	O
single	O
datapoint	O
x	O
the	O
em	B
energy	B
term	O
is	O
pyx	O
whphx	O
for	O
regression	B
a	O
simple	O
choice	O
is	O
y	O
xtwh	O
pyx	O
wh	O
n	O
and	O
for	O
classification	B
py	O
wh	O
draft	O
march	O
in	O
both	O
cases	O
computing	O
the	O
derivatives	O
of	O
the	O
energy	B
with	O
respect	O
to	O
the	O
parameters	O
w	O
is	O
straightforward	O
so	O
that	O
an	O
em	B
algorithm	B
is	O
readily	O
available	O
an	O
alternative	O
to	O
em	B
is	O
to	O
compute	O
the	O
gradient	B
of	O
the	O
likelihood	B
directly	O
using	O
the	O
standard	O
approach	B
discussed	O
in	O
indicator	O
models	O
a	O
bayesian	B
treatment	O
is	O
to	O
consider	O
pyx	O
where	O
it	O
is	O
conventional	O
to	O
assume	O
pw	O
pyx	O
whphx	O
upwpu	O
h	O
pwh	O
pu	O
wu	O
h	O
h	O
puh	O
the	O
integrals	O
are	O
generally	O
intractable	O
and	O
approximations	O
are	O
required	O
see	O
for	O
a	O
variational	O
treatment	O
for	O
regression	B
and	O
for	O
a	O
variational	O
treatment	O
of	O
classification	B
an	O
extension	O
to	O
bayesian	B
model	B
selection	I
in	O
which	O
the	O
number	O
of	O
experts	O
is	O
estimated	O
is	O
considered	O
in	O
indicator	O
models	O
in	O
the	O
indicator	B
approach	B
we	O
specify	O
a	O
distribution	B
over	O
the	O
cluster	O
assignments	O
for	O
consistency	O
with	O
the	O
literature	O
we	O
use	O
an	O
indicator	O
z	O
as	O
opposed	O
to	O
a	O
hidden	B
variable	I
h	O
although	O
they	O
play	O
the	O
same	O
role	O
a	O
clustering	B
model	B
with	O
parameters	O
on	O
the	O
component	O
models	O
and	O
joint	B
indicator	O
prior	B
takes	O
the	O
form	O
pvnzn	O
since	O
the	O
zn	O
indicate	O
cluster	O
membership	O
below	O
we	O
discuss	O
the	O
role	O
of	O
different	O
indicator	O
priors	O
in	O
clustering	B
joint	B
indicator	B
approach	B
factorised	B
prior	B
assuming	O
prior	B
independence	B
of	O
indicators	O
pzn	O
we	O
obtain	O
from	O
equation	B
zn	O
k	O
pvnzn	O
pvnzn	O
zn	O
which	O
recovers	O
the	O
standard	O
mixture	B
model	B
equation	B
as	O
we	O
discuss	O
below	O
more	O
sophisticated	O
joint	B
indicator	O
priors	O
can	O
be	O
used	O
to	O
explicitly	O
control	O
the	O
complexity	O
of	O
the	O
indicator	O
assignments	O
and	O
open	O
the	O
path	B
to	O
essentially	O
infinite	O
dimensional	O
models	O
joint	B
indicator	B
approach	B
polya	B
prior	B
for	O
a	O
large	O
number	O
of	O
available	O
clusters	O
components	O
k	O
using	O
a	O
factorised	B
joint	B
indicator	O
distribution	B
could	O
potentially	O
lead	O
to	O
overfitting	B
resulting	O
in	O
little	O
or	O
no	O
meaningful	O
clustering	B
one	O
way	O
to	O
control	O
the	O
effective	O
number	O
of	O
components	O
that	O
are	O
used	O
is	O
via	O
a	O
parameter	B
that	O
regulates	O
the	O
complexity	O
n	O
pzn	O
p	O
draft	O
march	O
indicator	O
models	O
zn	O
vn	O
zn	O
vn	O
zn	O
vn	O
n	O
figure	O
a	O
generic	O
mixture	B
model	B
for	O
data	O
each	O
zn	O
indicates	O
the	O
cluster	O
of	O
each	O
datapoint	O
is	O
a	O
set	O
of	O
parameters	O
and	O
zn	O
k	O
selects	O
parameter	B
k	O
for	O
datapoint	O
vn	O
for	O
a	O
potentially	O
large	O
number	O
of	O
clusters	O
one	O
way	O
to	O
control	O
complexity	O
is	O
to	O
constrain	O
the	O
joint	B
plate	B
notation	O
of	O
indicator	O
distribution	B
figure	O
the	O
number	O
of	O
unique	O
clusters	O
u	O
when	O
indicators	O
are	O
sampled	O
from	O
a	O
polya	B
distribution	B
k	O
k	O
k	O
equation	B
with	O
and	O
n	O
datapoints	O
even	O
though	O
the	O
number	O
of	O
available	O
clusters	O
k	O
is	O
larger	O
than	O
the	O
number	O
of	O
datapoints	O
the	O
effective	O
number	O
of	O
used	O
clusters	O
remains	O
constrained	O
see	O
demopolya	O
m	O
where	O
pz	O
is	O
a	O
categorical	B
distribution	B
pzn	O
k	O
k	O
a	O
convenient	O
choice	O
for	O
p	O
is	O
the	O
dirichlet	B
distribution	B
this	O
is	O
conjugate	B
to	O
the	O
categorical	B
distribution	B
k	O
p	O
dirichlet	B
the	O
number	O
of	O
unique	O
clusters	O
used	O
is	O
then	O
given	O
by	O
u	O
nk	O
n	O
the	O
integral	O
over	O
in	O
equation	B
can	O
be	O
performed	O
analytically	O
to	O
give	O
a	O
polya	B
distribution	B
i	O
k	O
i	O
the	O
distribution	B
over	O
likely	O
cluster	O
numbers	O
is	O
controlled	O
by	O
the	O
parameter	B
the	O
scaling	O
in	O
equation	B
ensures	O
a	O
sensible	O
limit	O
as	O
k	O
see	O
in	O
which	O
limit	O
the	O
models	O
are	O
known	O
as	O
dirichlet	B
process	I
mixture	B
models	I
this	O
approach	B
means	O
that	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
constrain	O
the	O
number	O
of	O
possible	O
components	O
k	O
since	O
the	O
number	O
of	O
active	B
components	O
u	O
remains	O
limited	O
even	O
for	O
very	O
large	O
k	O
k	O
clustering	B
is	O
achieved	O
by	O
considering	O
argmax	O
in	O
practice	O
it	O
is	O
common	O
to	O
consider	O
argmax	O
zn	O
unfortunately	O
posterior	B
inference	B
of	O
for	O
this	O
class	O
of	O
models	O
is	O
formally	O
computationally	O
intractable	O
and	O
approximate	B
inference	B
techniques	O
are	O
required	O
a	O
detailed	O
discussion	O
of	O
these	O
techniques	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
and	O
we	O
refer	O
the	O
reader	O
to	O
for	O
a	O
deterministic	B
approach	B
and	O
for	O
a	O
discussion	O
of	O
sampling	B
approaches	O
draft	O
march	O
mixed	B
membership	I
models	O
figure	O
latent	B
dirichlet	B
allocation	I
for	O
document	O
n	O
we	O
first	O
sample	O
a	O
distribution	B
of	O
topics	O
n	O
then	O
for	O
each	O
word	O
position	O
w	O
in	O
the	O
document	O
we	O
sample	O
a	O
topic	O
zn	O
w	O
from	O
the	O
topic	O
distribution	B
given	O
the	O
topic	O
we	O
then	O
sample	O
a	O
word	O
from	O
the	O
word	O
distribution	B
of	O
that	O
topic	O
the	O
parameters	O
of	O
the	O
model	B
are	O
the	O
word	O
distributions	O
for	O
each	O
topic	O
and	O
the	O
parameters	O
of	O
the	O
topic	O
distribution	B
n	O
zn	O
w	O
vn	O
w	O
wn	O
n	O
mixed	B
membership	I
models	O
unlike	O
standard	O
mixture	B
models	O
in	O
which	O
each	O
object	O
is	O
assumed	O
to	O
have	O
been	O
generated	O
from	O
a	O
single	O
cluster	O
in	O
mixed	B
membership	I
models	O
an	O
object	O
may	O
be	O
a	O
member	O
of	O
more	O
than	O
one	O
group	O
latent	B
dirichlet	B
allocation	I
discussed	O
below	O
is	O
an	O
example	O
of	O
such	O
a	O
mixed	B
membership	I
model	B
and	O
is	O
one	O
of	O
a	O
number	O
of	O
models	O
developed	O
in	O
recent	O
years	O
latent	B
dirichlet	B
allocation	I
so	O
far	O
we	O
ve	O
considered	O
clustering	B
in	O
the	O
sense	O
that	O
each	O
observation	O
is	O
assumed	O
to	O
have	O
been	O
generated	O
from	O
a	O
single	O
cluster	O
in	O
contrast	O
latent	B
dirichlet	B
and	O
related	O
methods	O
are	O
generative	B
mixed	B
membership	I
models	O
in	O
which	O
each	O
datapoint	O
may	O
belong	O
to	O
more	O
than	O
a	O
single	O
cluster	O
a	O
typical	O
application	O
is	O
to	O
identify	O
topic	O
clusters	O
in	O
a	O
collection	O
of	O
documents	O
a	O
single	O
document	O
contains	O
a	O
sequence	O
of	O
words	O
for	O
example	O
v	O
cat	O
sat	O
on	O
the	O
cat	O
mat	O
vn	O
if	O
each	O
word	O
in	O
the	O
available	O
dictionary	O
is	O
assigned	O
to	O
a	O
unique	O
state	O
dog	O
tree	B
cat	O
we	O
can	O
represent	O
then	O
the	O
nth	O
document	O
as	O
a	O
vector	O
vn	O
wn	O
vn	O
i	O
d	O
where	O
wn	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
nth	O
document	O
the	O
number	O
of	O
words	O
wn	O
in	O
each	O
document	O
can	O
vary	O
although	O
the	O
overall	O
dictionary	O
from	O
which	O
they	O
came	O
is	O
fixed	O
the	O
aim	O
is	O
to	O
find	O
common	O
topics	O
in	O
documents	O
assuming	O
that	O
any	O
document	O
could	O
potentially	O
contain	O
more	O
than	O
one	O
topic	O
it	O
is	O
useful	O
to	O
think	O
first	O
of	O
an	O
underlying	O
generative	B
model	B
of	O
words	O
including	O
latent	B
topics	O
we	O
will	O
later	O
integrate	O
out	O
we	O
first	O
sample	O
a	O
probability	O
distribution	B
that	O
represents	O
the	O
topics	O
likely	O
to	O
occur	O
for	O
this	O
document	O
then	O
for	O
each	O
word-position	O
in	O
the	O
document	O
sample	O
a	O
topic	O
and	O
subsequently	O
a	O
word	O
from	O
the	O
distribution	B
of	O
words	O
for	O
that	O
topic	O
mathematically	O
for	O
document	O
n	O
and	O
the	O
wth	O
word-position	O
in	O
the	O
document	O
vn	O
w	O
k	O
to	O
indicate	O
which	O
of	O
the	O
k	O
possible	O
topics	O
that	O
word	O
belongs	O
for	O
each	O
topic	O
k	O
one	O
then	O
has	O
a	O
categorical	B
distribution	B
over	O
all	O
the	O
words	O
i	O
d	O
in	O
the	O
dictionary	O
w	O
we	O
use	O
zn	O
pvn	O
w	O
izn	O
w	O
k	O
ik	O
distribution	B
of	O
topics	O
n	O
the	O
animal	O
topic	O
has	O
high	O
probability	O
to	O
emit	O
animal-like	O
words	O
etc	O
for	O
each	O
document	O
n	O
we	O
have	O
a	O
k	O
which	O
gives	O
a	O
latent	B
description	O
of	O
the	O
document	O
in	O
terms	O
of	O
its	O
topic	O
membership	O
for	O
example	O
document	O
n	O
discusses	O
issues	O
related	O
to	O
wildlife	O
conservation	O
might	O
have	O
a	O
topic	O
distribution	B
with	O
high	O
mass	O
on	O
the	O
latent	B
animals	O
and	O
environment	O
topics	O
note	O
that	O
the	O
topics	O
are	O
indeed	O
latent	B
the	O
name	O
animal	O
would	O
be	O
given	O
post-hoc	O
based	O
on	O
the	O
kinds	O
of	O
words	O
n	O
draft	O
march	O
mixed	B
membership	I
models	O
that	O
the	O
latent	B
topic	I
would	O
generate	O
ik	O
as	O
in	O
to	O
control	O
complexity	O
one	O
may	O
use	O
a	O
dirichlet	B
prior	B
to	O
limit	O
the	O
number	O
of	O
topics	O
active	B
in	O
any	O
particular	O
document	O
p	O
n	O
dirichlet	B
n	O
where	O
is	O
a	O
vector	O
of	O
length	O
the	O
number	O
of	O
topics	O
a	O
generative	B
model	B
for	O
sampling	B
a	O
document	O
vn	O
with	O
wn	O
word	O
positions	O
is	O
choose	O
n	O
dirichlet	B
n	O
for	O
each	O
of	O
word	O
position	O
vn	O
w	O
w	O
wn	O
choose	O
a	O
topic	O
zn	O
choose	O
a	O
word	O
vn	O
w	O
p	O
w	O
w	O
n	O
w	O
w	O
training	B
the	O
lda	O
model	B
corresponds	O
to	O
learning	B
the	O
parameters	O
which	O
relates	O
to	O
the	O
number	O
of	O
topics	O
and	O
which	O
describes	O
the	O
distribution	B
of	O
words	O
within	O
each	O
topic	O
unfortunately	O
finding	O
the	O
requisite	O
marginals	O
for	O
learning	B
from	O
the	O
posterior	B
is	O
formally	O
computationally	O
intractable	O
efficient	O
approximate	B
inference	B
for	O
this	O
class	O
of	O
models	O
is	O
a	O
topic	O
of	O
research	O
interest	O
and	O
both	O
variational	O
and	O
sampling	B
approaches	O
have	O
recently	O
been	O
there	O
are	O
close	O
similarities	O
between	O
lda	O
and	O
both	O
of	O
which	O
describe	O
a	O
document	O
in	O
terms	O
of	O
a	O
distribution	B
over	O
latent	B
topics	O
lda	O
is	O
a	O
probabilistic	B
model	B
for	O
which	O
issues	O
such	O
as	O
setting	O
hyperparameters	O
can	O
be	O
addressed	O
using	O
maximum	B
likelihood	B
plsa	O
on	O
the	O
other	O
hand	O
is	O
essentially	O
a	O
matrix	B
decomposition	B
technique	O
as	O
pca	B
issues	O
such	O
as	O
hyperparameters	O
setting	O
for	O
plsa	O
are	O
therefore	O
addressed	O
using	O
validation	B
data	O
whilst	O
plsa	O
is	O
a	O
description	O
only	O
of	O
the	O
training	B
data	O
lda	O
is	O
a	O
generative	B
data	O
model	B
and	O
can	O
in	O
principle	O
be	O
used	O
to	O
synthesise	O
new	O
documents	O
example	O
an	O
illustration	O
of	O
the	O
use	O
of	O
lda	O
is	O
given	O
in	O
the	O
documents	O
are	O
taken	O
from	O
the	O
trec	O
associated	O
press	O
corpus	O
containing	O
newswire	O
articles	O
with	O
unique	O
terms	O
after	O
removing	O
a	O
standard	O
list	O
of	O
stop	B
words	I
words	O
such	O
as	O
the	O
a	O
etc	O
that	O
would	O
otherwise	O
dominate	O
the	O
statistics	O
the	O
em	B
algorithm	B
variational	O
approximate	B
inference	B
was	O
used	O
to	O
find	O
the	O
dirichlet	B
and	O
conditional	B
categorical	B
parameters	O
for	O
a	O
lda	O
model	B
the	O
top	O
words	O
from	O
four	O
resulting	O
categorical	B
distributions	O
ik	O
are	O
illustrated	O
these	O
distributions	O
capture	O
some	O
of	O
the	O
underlying	O
topics	O
in	O
the	O
corpus	O
an	O
example	O
document	O
from	O
the	O
corpus	O
is	O
presented	O
along	O
with	O
the	O
words	O
coloured	O
by	O
the	O
most	O
probable	O
latent	B
topic	I
they	O
correspond	O
to	O
graph	B
based	O
representations	O
of	O
data	O
mixed	B
membership	I
models	O
are	O
used	O
in	O
a	O
variety	O
of	O
contexts	O
and	O
are	O
distinguished	O
also	O
by	O
the	O
form	O
of	O
data	O
available	O
here	O
we	O
focus	O
on	O
analysing	O
a	O
representation	O
of	O
the	O
interactions	O
amongst	O
a	O
collection	O
of	O
objects	O
in	O
particular	O
the	O
data	O
has	O
been	O
processed	O
such	O
that	O
all	O
the	O
information	O
of	O
interest	O
is	O
characterised	O
by	O
an	O
interaction	O
matrix	B
for	O
graph	B
based	O
representations	O
of	O
data	O
two	O
objects	O
are	O
similar	O
if	O
they	O
are	O
neighbours	O
on	O
a	O
graph	B
representing	O
the	O
data	O
objects	O
in	O
the	O
field	O
of	O
social-networks	O
for	O
example	O
each	O
individual	O
is	O
represented	O
as	O
a	O
node	O
in	O
a	O
graph	B
with	O
a	O
link	O
between	O
two	O
nodes	O
if	O
the	O
individuals	O
are	O
friends	O
given	O
a	O
graph	B
one	O
might	O
wish	O
to	O
identify	O
communities	O
of	O
closely	O
linked	O
friends	O
interpreted	O
as	O
a	O
social	O
network	O
in	O
individual	O
is	O
a	O
member	O
of	O
his	O
work	O
group	O
and	O
also	O
the	O
poker	O
group	O
these	O
two	O
groups	O
of	O
individuals	O
are	O
otherwise	O
disjoint	O
discovering	O
such	O
groupings	O
contrasts	O
with	O
graph	B
partitioning	I
in	O
which	O
each	O
node	O
is	O
assigned	O
to	O
only	O
one	O
of	O
a	O
set	O
of	O
subgraphs	O
for	O
which	O
a	O
typical	O
criterion	O
is	O
that	O
each	O
subgraph	O
should	O
be	O
roughly	O
of	O
the	O
same	O
size	O
and	O
that	O
there	O
are	O
few	O
connections	O
between	O
the	O
draft	O
march	O
mixed	B
membership	I
models	O
arts	O
new	O
film	O
show	O
music	O
movie	O
play	O
musical	O
best	O
actor	O
first	O
york	O
opera	O
theater	O
actress	O
love	O
budgets	O
million	O
tax	O
program	O
budget	O
billion	O
federal	O
year	O
spending	O
new	O
state	O
plan	O
money	B
programs	O
government	O
congress	O
children	B
education	O
children	B
women	O
people	O
child	O
years	O
families	O
work	O
parents	B
says	O
family	B
welfare	O
men	O
percent	O
care	O
life	O
school	O
students	O
schools	O
education	O
teachers	O
high	O
public	O
teacher	O
bennett	O
manigat	O
namphy	O
state	O
president	O
elementary	O
haiti	O
the	O
william	O
randolph	O
hearst	O
foundation	O
will	O
give	O
million	O
to	O
lincoln	O
center	O
metropolitan	O
opera	O
co	O
new	O
york	O
philharmonic	O
and	O
juilliard	O
school	O
our	O
board	O
felt	O
that	O
we	O
had	O
a	O
real	O
opportunity	O
to	O
make	O
a	O
mark	O
on	O
the	O
future	O
of	O
the	O
performing	O
arts	O
with	O
these	O
grants	O
an	O
act	O
every	O
bit	O
as	O
important	O
as	O
our	O
traditional	O
areas	O
of	O
support	O
in	O
health	O
medical	O
research	O
education	O
and	O
the	O
social	O
services	O
hearst	O
foundation	O
president	O
randolph	O
a	O
hearst	O
said	O
monday	O
in	O
announcing	O
the	O
grants	O
lincoln	O
centers	O
share	O
will	O
be	O
for	O
its	O
new	O
building	O
which	O
will	O
house	O
young	O
artists	O
and	O
provide	O
new	O
public	O
facilities	O
the	O
metropolitan	O
opera	O
co	O
and	O
new	O
york	O
philharmonic	O
will	O
receive	O
each	O
the	O
juilliard	O
school	O
where	O
music	O
and	O
the	O
performing	O
arts	O
are	O
taught	O
will	O
get	O
the	O
hearst	O
foundation	O
a	O
leading	O
supporter	O
of	O
the	O
lincoln	O
center	O
consolidated	O
corporate	O
fund	O
will	O
make	O
its	O
usual	O
annual	O
donation	O
too	O
figure	O
a	O
subset	O
of	O
the	O
latent	B
topics	O
discovered	O
by	O
lda	O
and	O
the	O
high	O
probability	O
words	O
associated	O
with	O
each	O
topic	O
each	O
column	O
represents	O
a	O
topic	O
with	O
the	O
topic	O
name	O
such	O
as	O
art	O
assigned	O
by	O
hand	O
after	O
viewing	O
the	O
most	O
likely	O
words	O
corresponding	O
to	O
the	O
topic	O
a	O
document	O
from	O
the	O
training	B
data	O
in	O
which	O
the	O
words	O
are	O
coloured	O
according	O
to	O
the	O
most	O
likely	O
latent	B
topic	I
this	O
demonstrates	O
the	O
mixed-membership	O
nature	O
of	O
the	O
model	B
assigning	O
the	O
datapoint	O
in	O
this	O
case	O
to	O
several	O
clusters	O
reproduced	O
from	O
figure	O
the	O
social	O
network	O
of	O
a	O
set	O
of	O
individuals	O
represented	O
as	O
an	O
undirected	B
graph	B
here	O
individual	O
belongs	O
to	O
the	O
group	O
and	O
also	O
by	O
contrast	O
in	O
graph	B
partitioning	I
one	O
breaks	O
the	O
graph	B
into	O
roughly	O
equally	O
sized	O
disjoint	O
partitions	O
such	O
that	O
each	O
node	O
is	O
a	O
member	O
of	O
only	O
a	O
single	O
partition	O
with	O
a	O
minimal	O
number	O
of	O
edges	O
between	O
partitions	O
another	O
example	O
is	O
that	O
nodes	O
in	O
the	O
graph	B
represent	O
products	O
and	O
a	O
link	O
between	O
nodes	O
i	O
and	O
j	O
indicates	O
that	O
customers	O
who	O
by	O
product	O
i	O
frequently	O
also	O
buy	O
product	O
j	O
the	O
aim	O
is	O
to	O
decompose	O
the	O
graph	B
into	O
groups	O
each	O
corresponding	O
to	O
products	O
that	O
are	O
commonly	O
co-bought	O
by	O
a	O
growing	O
area	O
of	O
application	O
of	O
graph	B
based	O
representations	O
is	O
in	O
bioinformatics	B
in	O
which	O
nodes	O
represent	O
genes	O
and	O
a	O
link	O
between	O
them	O
representing	O
that	O
the	O
two	O
genes	O
have	O
similar	O
activity	O
profiles	O
the	O
task	O
is	O
then	O
to	O
identify	O
groups	O
of	O
similarly	O
behaving	O
dyadic	B
data	I
consider	O
two	O
kinds	O
of	O
objects	O
for	O
example	O
films	O
and	O
customers	O
each	O
film	O
is	O
indexed	O
by	O
f	O
f	O
and	O
each	O
user	O
by	O
u	O
u	O
the	O
interaction	O
of	O
user	O
u	O
with	O
film	O
f	O
can	O
be	O
described	O
by	O
the	O
element	O
of	O
a	O
matrix	B
muf	O
representing	O
the	O
rating	O
a	O
user	O
gives	O
to	O
a	O
film	O
a	O
dyadic	B
dataset	O
consists	O
of	O
such	O
a	O
matrix	B
and	O
the	O
aim	O
is	O
to	O
decompose	O
this	O
matrix	B
to	O
explain	O
the	O
ratings	O
by	O
finding	O
types	O
of	O
films	O
and	O
types	O
of	O
user	O
another	O
example	O
is	O
to	O
consider	O
a	O
collection	O
of	O
documents	O
summarised	O
by	O
an	O
interaction	O
matrix	B
in	O
which	O
mwd	O
is	O
if	O
word	O
w	O
appears	O
in	O
document	O
d	O
and	O
zero	O
otherwise	O
this	O
matrix	B
can	O
be	O
represented	O
as	O
a	O
bipartite	O
graph	B
as	O
in	O
the	O
upper	O
nodes	O
represent	O
documents	O
and	O
the	O
lower	O
nodes	O
words	O
with	O
a	O
link	O
between	O
them	O
if	O
that	O
word	O
occurs	O
in	O
that	O
document	O
one	O
the	O
seeks	O
assignments	O
of	O
documents	O
to	O
groups	O
or	O
latent	B
topics	O
to	O
succinctly	O
explain	O
the	O
link	O
structure	B
of	O
the	O
bipartite	O
graph	B
via	O
a	O
small	O
number	O
of	O
latent	B
nodes	O
as	O
schematically	O
depicted	O
in	O
one	O
may	O
view	O
this	O
as	O
a	O
form	O
of	O
matrix	B
t	O
mwd	O
uwtv	O
t	O
td	O
where	O
t	O
indexes	O
the	O
topics	O
and	O
the	O
feature	O
matrices	O
u	O
and	O
v	O
control	O
the	O
word-to-topic	O
mapping	O
and	O
the	O
topic-to-document	O
mapping	O
this	O
differs	O
from	O
latent	B
dirichlet	B
allocation	I
which	O
has	O
a	O
probabilistic	B
interpretation	O
of	O
first	O
generating	O
a	O
topic	O
and	O
then	O
a	O
word	O
conditional	B
on	O
the	O
chosen	O
topic	O
here	O
the	O
interaction	O
between	O
document-topic	O
matrix	B
v	O
and	O
word-topic	O
matrix	B
u	O
is	O
non-probabilistic	O
more	O
generally	O
we	O
can	O
draft	O
march	O
there	O
are	O
figure	O
graphical	O
representation	O
of	O
dyadic	B
data	I
documents	O
and	O
words	O
a	O
link	O
represents	O
that	O
a	O
particular	O
worddocument	O
pair	O
occurs	O
in	O
the	O
dataset	O
a	O
latent	B
decomposition	B
of	O
using	O
topics	O
a	O
topic	O
corresponds	O
to	O
a	O
collection	O
of	O
words	O
and	O
each	O
document	O
a	O
collection	O
of	O
topics	O
the	O
open	O
nodes	O
indicate	O
latent	B
variables	O
where	O
u	O
and	O
v	O
are	O
feature	O
matrices	O
in	O
real-valued	O
data	O
is	O
modelled	O
using	O
pmu	O
w	O
v	O
n	O
m	O
uwvt	O
where	O
u	O
and	O
v	O
are	O
assumed	O
binary	O
and	O
the	O
real-valued	O
w	O
is	O
a	O
topic-interaction	O
matrix	B
in	O
this	O
viewpoint	O
learning	B
then	O
consists	O
of	O
inferring	O
uwv	O
given	O
the	O
dyadic	B
observation	O
matrix	B
m	O
assuming	O
factorised	B
priors	O
the	O
posterior	B
over	O
the	O
matrices	O
is	O
pu	O
w	O
vm	O
pmu	O
w	O
vpupwpv	O
a	O
convenient	O
choice	O
is	O
a	O
gaussian	B
prior	B
distribution	B
for	O
w	O
with	O
the	O
feature	O
matrices	O
u	O
and	O
v	O
sampled	O
from	O
beta-bernoulli	O
priors	O
the	O
resulting	O
posterior	B
distribution	B
is	O
formally	O
computationally	O
intractable	O
and	O
in	O
this	O
is	O
addressed	O
using	O
a	O
sampling	B
approximation	B
monadic	B
data	I
in	O
monadic	B
data	I
there	O
is	O
only	O
one	O
type	O
of	O
object	O
and	O
the	O
interaction	O
between	O
the	O
objects	O
is	O
represented	O
by	O
a	O
square	O
interaction	O
matrix	B
for	O
example	O
one	O
might	O
have	O
a	O
matrix	B
with	O
elements	O
aij	O
if	O
proteins	O
i	O
and	O
j	O
can	O
bind	O
to	O
each	O
other	O
and	O
otherwise	O
a	O
depiction	O
of	O
the	O
interaction	O
matrix	B
is	O
given	O
by	O
a	O
graph	B
in	O
which	O
an	O
edge	O
represents	O
an	O
interaction	O
for	O
example	O
in	O
the	O
following	O
section	O
we	O
discuss	O
a	O
particular	O
mixed	B
membership	I
model	B
and	O
highlight	O
potential	B
applications	O
the	O
method	O
is	O
based	O
on	O
clique	B
decompositions	O
of	O
graphs	O
and	O
as	O
such	O
we	O
require	O
a	O
short	O
digression	O
into	O
clique-based	O
graph	B
representations	O
cliques	O
and	O
adjacency	B
matrices	O
for	O
monadic	B
binary	O
data	O
a	O
symmetric	O
adjacency	B
matrix	B
has	O
elements	O
aij	O
with	O
a	O
indicating	O
a	O
link	O
between	O
nodes	O
i	O
and	O
j	O
for	O
the	O
graph	B
in	O
the	O
adjacency	B
matrix	B
is	O
mixed	B
membership	I
models	O
write	O
a	O
distribution	B
pmu	O
v	O
a	O
where	O
we	O
include	O
self	O
connections	O
on	O
the	O
diagonal	O
given	O
a	O
our	O
aim	O
is	O
to	O
find	O
a	O
simpler	O
description	O
that	O
reveals	O
the	O
underlying	O
cluster	O
structure	B
such	O
as	O
and	O
in	O
given	O
the	O
undirected	B
graph	B
in	O
the	O
incidence	B
matrix	B
finc	O
is	O
an	O
alternative	O
description	O
of	O
the	O
adjacency	B
draft	O
march	O
figure	O
the	O
minimal	O
clique	B
cover	O
is	O
mixed	B
membership	I
models	O
figure	O
bipartite	O
representations	O
of	O
the	O
decompositions	O
of	O
shaded	O
nodes	O
represent	O
observed	O
variables	O
and	O
open	O
nodes	O
latent	B
variables	O
incidence	B
matrix	B
representation	O
minimal	O
clique	B
decomposition	B
given	O
the	O
v	O
nodes	O
in	O
the	O
graph	B
we	O
construct	O
finc	O
as	O
follows	O
for	O
each	O
link	O
i	O
j	O
in	O
the	O
graph	B
form	O
a	O
column	O
of	O
the	O
matrix	B
finc	O
with	O
zero	O
entries	O
except	O
for	O
a	O
in	O
the	O
ith	O
and	O
jth	O
row	O
the	O
column	O
ordering	O
is	O
arbitrary	O
for	O
example	O
for	O
the	O
graph	B
in	O
an	O
incidence	B
matrix	B
is	O
the	O
incidence	B
matrix	B
has	O
the	O
property	O
that	O
the	O
adjacency	B
structure	B
of	O
the	O
original	O
graph	B
is	O
given	O
by	O
the	O
outer	O
product	O
of	O
the	O
incidence	B
matrix	B
with	O
itself	O
the	O
diagonal	O
entries	O
contain	O
the	O
degree	B
of	O
links	O
of	O
each	O
node	O
for	O
our	O
example	O
this	O
gives	O
finc	O
fincft	O
inc	O
fincft	O
inc	O
so	O
that	O
a	O
h	O
here	O
h	O
is	O
the	O
element-wise	O
heaviside	B
step	I
function	B
if	O
mij	O
and	O
is	O
otherwise	O
a	O
useful	O
viewpoint	O
of	O
the	O
incidence	B
matrix	B
is	O
that	O
it	O
identifies	O
two-cliques	O
in	O
the	O
graph	B
we	O
are	O
using	O
the	O
term	O
clique	B
in	O
the	O
non-maximal	O
sense	O
there	O
are	O
five	O
in	O
and	O
each	O
column	O
of	O
finc	O
specifies	O
which	O
elements	O
are	O
in	O
each	O
graphically	O
we	O
can	O
depict	O
this	O
incidence	B
decomposition	B
as	O
a	O
bipartite	O
graph	B
as	O
in	O
where	O
the	O
open	O
nodes	O
represent	O
the	O
five	O
the	O
incidence	B
matrix	B
can	O
be	O
generalised	B
to	O
describe	O
larger	O
cliques	O
consider	O
the	O
following	O
matrix	B
as	O
a	O
decomposition	B
for	O
and	O
its	O
outer-product	O
f	O
fft	O
the	O
interpretation	O
is	O
that	O
f	O
represents	O
a	O
decomposition	B
into	O
two	O
as	O
in	O
the	O
incidence	B
matrix	B
each	O
column	O
represents	O
a	O
clique	B
and	O
the	O
rows	O
containing	O
a	O
express	O
which	O
elements	O
are	O
in	O
the	O
clique	B
defined	O
by	O
that	O
column	O
this	O
decomposition	B
can	O
be	O
represented	O
as	O
the	O
bipartite	O
graph	B
of	O
for	O
the	O
graph	B
of	O
both	O
finc	O
and	O
f	O
satisfy	O
a	O
h	O
h	O
fincft	O
inc	O
one	O
can	O
view	O
equation	B
as	O
a	O
form	O
of	O
binary	O
matrix	B
factoristation	O
of	O
the	O
binary	O
square	O
matrix	B
a	O
into	O
non-square	O
binary	O
matrices	O
for	O
our	O
clustering	B
purposes	O
the	O
decomposition	B
using	O
f	O
is	O
to	O
be	O
preferred	O
to	O
the	O
incidence	B
decomposition	B
since	O
f	O
decomposes	O
the	O
graph	B
into	O
a	O
smaller	O
number	O
of	O
larger	O
cliques	O
a	O
formal	O
specification	O
of	O
the	O
problem	B
of	O
finding	O
a	O
minimum	O
number	O
of	O
maximal	O
fully-connected	O
subsets	O
is	O
the	O
computational	O
problem	B
min	O
clique	B
indeed	O
f	O
solves	O
min	O
clique	B
cover	O
for	O
draft	O
march	O
mixed	B
membership	I
models	O
figure	O
the	O
function	B
increases	O
this	O
sigmoid	B
function	B
tends	O
to	O
a	O
step	O
function	B
e	O
for	O
as	O
ii	O
express	O
the	O
number	O
of	O
cliquescolumns	O
that	O
node	O
i	O
occurs	O
in	O
off-diagonal	O
elements	O
definition	O
matrix	B
given	O
an	O
adjacency	B
matrix	B
i	O
j	O
v	O
a	O
clique	B
matrix	B
f	O
has	O
elements	O
fic	O
i	O
v	O
c	O
c	O
such	O
that	O
a	O
hfft	O
diagonal	O
elements	O
contain	O
the	O
number	O
of	O
cliquescolumns	O
that	O
nodes	O
i	O
and	O
j	O
jointly	O
inhabit	O
ij	O
whilst	O
finding	O
a	O
clique	B
decomposition	B
f	O
is	O
easy	O
the	O
incidence	B
matrix	B
for	O
example	O
finding	O
a	O
clique	B
with	O
the	O
minimal	O
number	O
of	O
columns	O
i	O
e	O
solving	B
min	O
clique	B
cover	O
is	O
a	O
generative	B
model	B
of	O
adjacency	B
matrices	O
solving	B
min	O
clique	B
cover	O
is	O
a	O
computationally	O
hard	B
problem	B
and	O
approximations	O
are	O
in	O
general	O
unavoidable	O
below	O
we	O
relax	O
the	O
strict	O
clique	B
requirement	O
and	O
assume	O
that	O
provided	O
only	O
a	O
small	O
number	O
of	O
links	O
in	O
an	O
almost	O
clique	B
are	O
missing	B
this	O
may	O
be	O
considered	O
a	O
sufficiently	O
well-connected	O
group	O
of	O
nodes	O
to	O
form	O
a	O
cluster	O
given	O
an	O
adjacency	B
matrix	B
a	O
and	O
a	O
prior	B
on	O
clique	B
matrices	O
f	O
our	O
interest	O
is	O
the	O
posterior	B
pfa	O
pafpf	O
we	O
first	O
concentrate	O
on	O
the	O
generative	B
term	O
paf	O
to	O
find	O
well-connected	O
clusters	O
we	O
relax	O
the	O
constraint	O
that	O
the	O
decomposition	B
is	O
in	O
the	O
form	O
of	O
cliques	O
in	O
the	O
original	O
graph	B
and	O
view	O
the	O
absence	O
of	O
links	O
as	O
statistical	O
fluctuations	O
away	O
from	O
a	O
perfect	O
clique	B
given	O
a	O
v	O
c	O
matrix	B
f	O
we	O
desire	O
that	O
the	O
higher	O
the	O
overlap	O
between	O
fi	O
and	O
fj	O
is	O
the	O
greater	O
the	O
probability	O
of	O
a	O
link	O
between	O
i	O
and	O
j	O
this	O
may	O
be	O
achieved	O
using	O
for	O
example	O
paij	O
e	O
fif	O
t	O
j	O
with	O
where	O
controls	O
the	O
steepness	O
of	O
the	O
function	B
see	O
the	O
shift	O
in	O
equation	B
ensures	O
that	O
approximates	O
the	O
step-function	O
since	O
the	O
argument	O
of	O
is	O
an	O
integer	O
under	O
equation	B
if	O
j	O
and	O
paij	O
is	O
high	O
absent	O
links	O
fi	O
and	O
fj	O
have	O
at	O
least	O
one	O
in	O
the	O
same	O
position	O
fif	O
t	O
contribute	O
paij	O
paij	O
the	O
parameter	B
controls	O
how	O
strictly	O
matches	O
a	O
for	O
large	O
very	O
little	O
flexibility	O
is	O
allowed	O
and	O
only	O
cliques	O
will	O
be	O
identified	O
for	O
small	O
subsets	O
that	O
would	O
be	O
cliques	O
if	O
it	O
were	O
not	O
for	O
a	O
small	O
number	O
of	O
missing	B
links	O
are	O
clustered	O
together	O
the	O
setting	O
of	O
is	O
user	O
and	O
problem	B
dependent	O
assuming	O
each	O
element	O
of	O
the	O
adjacency	B
matrix	B
is	O
sampled	O
independently	O
from	O
the	O
generating	O
process	O
the	O
joint	B
probability	O
of	O
observing	O
a	O
is	O
its	O
diagonal	O
elements	O
paf	O
i	O
j	O
j	O
fif	O
t	O
j	O
fif	O
t	O
j	O
the	O
ultimate	O
quantity	O
of	O
interest	O
is	O
the	O
posterior	B
distribution	B
of	O
clique	B
structure	B
equation	B
for	O
which	O
we	O
now	O
specify	O
a	O
prior	B
pf	O
over	O
clique	B
matrices	O
use	O
lower	O
indices	O
fi	O
to	O
denote	O
the	O
ith	O
row	O
of	O
f	O
draft	O
march	O
mixed	B
membership	I
models	O
figure	O
adjacency	B
matrix	B
of	O
political	O
books	O
clique	B
matrix	B
non-zero	O
entries	O
adjacency	B
reconstruction	O
using	O
an	O
approximate	B
clique	B
matrix	B
with	O
cliques	O
see	O
also	O
and	O
democliquedecomp	O
m	O
figure	O
political	O
books	O
dimensional	O
clique	B
matrix	B
broken	O
into	O
groups	O
by	O
a	O
politically	O
astute	O
reader	O
a	O
black	O
square	O
indicates	O
qfic	O
liberal	O
books	O
conservative	O
books	O
neutral	O
booksyellow	O
by	O
inspection	O
cliques	O
largely	O
correspond	O
to	O
conservative	O
books	O
clique	B
matrix	B
prior	B
pf	O
since	O
we	O
are	O
interested	O
in	O
clustering	B
ideally	O
we	O
want	O
to	O
place	O
as	O
many	O
nodes	O
in	O
the	O
graph	B
as	O
possible	O
in	O
a	O
cluster	O
this	O
means	O
that	O
we	O
wish	O
to	O
bias	B
the	O
contributions	O
to	O
the	O
adjacency	B
matrix	B
a	O
to	O
occur	O
from	O
a	O
small	O
number	O
of	O
columns	O
of	O
f	O
to	O
achieve	O
this	O
we	O
first	O
reparameterise	O
f	O
as	O
f	O
cmaxf	O
where	O
c	O
play	O
the	O
role	O
of	O
indicators	O
and	O
f	O
c	O
is	O
column	O
c	O
of	O
f	O
cmax	O
is	O
an	O
assumed	O
maximal	O
number	O
of	O
clusters	O
ideally	O
we	O
would	O
like	O
to	O
find	O
an	O
f	O
with	O
a	O
low	O
number	O
of	O
indicators	O
cmax	O
in	O
state	O
to	O
achieve	O
this	O
we	O
define	O
a	O
prior	B
distribution	B
on	O
the	O
binary	O
hypercube	O
cmax	O
c	O
c	O
to	O
encourage	O
a	O
small	O
number	O
of	O
the	O
ensure	O
that	O
is	O
less	O
than	O
this	O
gives	O
rise	O
to	O
a	O
beta-bernoulli	O
distribution	B
cs	O
to	O
be	O
we	O
use	O
a	O
beta	B
prior	B
p	O
with	O
suitable	O
parameters	O
to	O
p	O
c	O
p	O
p	O
ba	O
n	O
b	O
cmax	O
n	O
where	O
ba	O
b	O
is	O
the	O
beta	B
function	B
and	O
n	O
ba	O
b	O
c	O
is	O
the	O
number	O
of	O
indicators	O
in	O
state	O
to	O
encourage	O
that	O
only	O
a	O
small	O
number	O
of	O
components	O
should	O
be	O
active	B
we	O
set	O
a	O
b	O
corresponds	O
to	O
a	O
mean	B
of	O
and	O
variance	B
the	O
distribution	B
is	O
on	O
the	O
vertices	O
of	O
the	O
binary	O
hypercube	O
with	O
a	O
bias	B
towards	O
vertices	O
close	O
to	O
the	O
origin	O
through	O
equation	B
the	O
prior	B
on	O
induces	O
a	O
prior	B
on	O
f	O
the	O
resulting	O
distribution	B
pf	O
pf	O
is	O
formally	O
intractable	O
and	O
in	O
this	O
is	O
addressed	O
using	O
a	O
variational	O
technique	O
see	O
cliquedecomp	O
c	O
and	O
cliquedecomp	O
m	O
clique	B
matrices	O
also	O
play	O
a	O
natural	B
role	O
in	O
the	O
parameterisation	B
of	O
positive	B
definite	I
matrices	O
see	O
draft	O
march	O
years	O
for	O
revenge	O
bush	O
vs	O
the	O
beltway	O
charlie	O
wilson	O
s	O
war	O
losing	O
bin	O
laden	O
sleeping	O
with	O
the	O
devil	O
the	O
man	O
who	O
warned	O
america	O
why	O
america	O
slept	O
ghost	O
wars	O
a	O
national	O
party	O
no	O
more	O
bush	O
country	O
dereliction	O
of	O
duty	O
legacy	O
off	O
with	O
their	O
heads	O
persecution	O
rumsfeld	O
s	O
war	O
breakdown	O
betrayal	O
shut	O
up	O
and	O
sing	O
meant	O
to	O
be	O
the	O
right	O
man	O
ten	O
minutes	O
from	O
normal	B
hillary	O
s	O
scheme	O
the	O
french	O
betrayal	O
of	O
america	O
tales	O
from	O
the	O
left	O
coast	O
hating	O
america	O
the	O
third	O
terrorist	O
endgame	O
spin	O
sisters	O
all	O
the	O
shah	O
s	O
men	O
dangerous	O
dimplomacy	O
the	O
price	O
of	O
loyalty	O
house	O
of	O
bush	O
house	O
of	O
saud	O
the	O
death	O
of	O
right	O
and	O
wrong	O
useful	O
idiots	O
the	O
o	O
reilly	O
factor	B
let	O
freedom	O
ring	O
those	O
who	O
trespass	O
bias	B
slander	O
the	O
savage	O
nation	O
deliver	O
us	O
from	O
evil	O
give	O
me	O
a	O
break	O
the	O
enemy	O
within	O
the	O
real	O
america	O
who	O
s	O
looking	O
out	O
for	O
you	O
the	O
official	O
handbook	O
vast	O
right	O
wing	O
conspiracy	O
power	O
plays	O
arrogance	O
the	O
perfect	O
wife	O
the	O
bushes	O
things	O
worth	O
fighting	O
for	O
surprise	O
security	O
the	O
american	O
experience	O
allies	O
why	O
courage	O
matters	O
hollywood	O
interrupted	O
fighting	O
back	O
we	O
will	O
prevail	O
the	O
faith	O
of	O
george	O
w	O
bush	O
rise	O
of	O
the	O
vulcans	O
downsize	O
this	O
stupid	O
white	O
men	O
rush	O
limbaugh	O
is	O
a	O
big	O
fat	O
idiot	O
the	O
best	O
democracy	O
money	B
can	O
buy	O
the	O
culture	O
of	O
fear	O
america	O
unbound	O
the	O
choice	O
the	O
great	O
unraveling	O
rogue	O
nation	O
soft	B
power	O
colossus	O
the	O
sorrows	O
of	O
empire	O
against	O
all	O
enemies	O
american	O
dynasty	O
big	O
lies	O
the	O
lies	O
of	O
george	O
w	O
bush	O
worse	O
than	O
watergate	O
plan	O
of	O
attack	O
bush	O
at	O
war	O
the	O
new	O
pearl	O
harbor	O
bushwomen	O
the	O
bubble	O
of	O
american	O
supremacy	O
living	O
history	O
the	O
politics	O
of	O
truth	O
fanatics	O
and	O
fools	O
bushwhacked	O
disarming	O
iraq	O
lies	O
and	O
the	O
lying	O
liars	O
who	O
tell	O
them	O
moveon	O
s	O
ways	O
to	O
love	O
your	O
country	O
the	O
buying	O
of	O
the	O
president	O
perfectly	O
legal	O
hegemony	O
or	O
survival	O
the	O
exception	O
to	O
the	O
rulers	O
freethinkers	O
had	O
enough	O
it	O
s	O
still	O
the	O
economy	O
stupid	O
we	O
re	O
right	O
they	O
re	O
wrong	O
what	O
liberal	O
media	O
the	O
clinton	O
wars	O
weapons	O
of	O
mass	O
deception	O
dude	O
where	O
s	O
my	O
country	O
thieves	O
in	O
high	O
places	O
shrub	O
buck	O
up	O
suck	O
up	O
the	O
future	O
of	O
freedom	O
empire	O
exercises	O
example	O
books	O
clustering	B
the	O
data	O
consists	O
of	O
books	O
on	O
us	O
politics	O
sold	O
by	O
the	O
online	B
bookseller	O
amazon	O
the	O
adjacency	B
matrix	B
with	O
element	O
aij	O
represents	O
frequent	O
co-purchasing	O
of	O
books	O
i	O
and	O
j	O
krebs	O
www	O
orgnet	O
com	O
additionally	O
books	O
are	O
labelled	B
liberal	O
neutral	O
or	O
conservative	O
according	O
to	O
the	O
judgement	O
of	O
a	O
politically	O
astute	O
reader	O
mejnnetdata	O
the	O
interest	O
is	O
to	O
assign	O
books	O
to	O
clusters	O
using	O
a	O
alone	O
and	O
then	O
see	O
if	O
these	O
clusters	O
correspond	O
in	O
some	O
way	O
to	O
the	O
ascribed	O
political	O
leanings	O
of	O
each	O
book	O
note	O
that	O
the	O
information	O
here	O
is	O
minimal	O
all	O
that	O
is	O
known	O
to	O
the	O
clustering	B
algorithm	B
is	O
which	O
books	O
were	O
co-bought	O
a	O
no	O
other	O
information	O
on	O
the	O
content	O
or	O
title	O
of	O
the	O
books	O
are	O
exploited	O
by	O
the	O
algorithm	B
with	O
an	O
initial	O
cmax	O
cliques	O
beta	B
parameters	O
a	O
b	O
and	O
steepness	O
the	O
most	O
probably	O
posterior	B
marginal	B
solution	O
contains	O
cliques	O
giving	O
a	O
perfect	O
reconstruction	O
of	O
the	O
adjacency	B
a	O
for	O
comparison	O
the	O
incidence	B
matrix	B
has	O
however	O
this	O
clique	B
matrix	B
is	O
too	O
large	O
to	O
provide	O
a	O
compact	O
interpretation	O
of	O
the	O
data	O
indeed	O
there	O
are	O
more	O
clusters	O
than	O
books	O
to	O
cluster	O
the	O
data	O
more	O
aggressively	O
we	O
fix	O
cmax	O
and	O
re-run	O
the	O
algorithm	B
this	O
results	O
only	O
in	O
an	O
approximate	B
clique	B
decomposition	B
a	O
hfft	O
as	O
plotted	O
in	O
the	O
resulting	O
approximate	B
clique	B
matrix	B
is	O
plotted	O
in	O
and	O
demonstrates	O
how	O
individual	O
books	O
are	O
present	O
in	O
more	O
than	O
one	O
cluster	O
interestingly	O
the	O
clusters	O
found	O
only	O
on	O
the	O
basis	O
of	O
the	O
adjacency	B
matrix	B
have	O
some	O
correspondence	O
with	O
the	O
ascribed	O
political	O
leanings	O
of	O
each	O
book	O
cliques	O
correspond	O
to	O
largely	O
conservative	O
books	O
most	O
books	O
belong	O
to	O
more	O
than	O
a	O
single	O
cliquecluster	O
suggesting	O
that	O
they	O
are	O
not	O
single	O
topic	O
books	O
consistent	B
with	O
the	O
assumption	O
of	O
a	O
mixed	B
membership	I
model	B
further	O
reading	O
the	O
literature	O
on	O
mixture	B
modelling	B
is	O
extensive	O
and	O
a	O
good	O
overview	O
and	O
entrance	O
to	O
the	O
literature	O
is	O
contained	O
in	O
code	O
mixprodbern	O
m	O
em	B
training	B
of	O
a	O
mixture	B
of	O
product	O
bernoulli	B
distributions	O
demomixbernoulli	O
m	O
demo	O
of	O
a	O
mixture	B
of	O
product	O
bernoulli	B
distributions	O
gmmem	O
m	O
em	B
training	B
of	O
a	O
mixture	B
of	O
gaussians	O
gmmloglik	O
m	O
gmm	O
log	O
likelihood	B
demogmmem	O
m	O
demo	O
of	O
a	O
em	B
for	O
mixture	B
of	O
gaussians	O
demogmmclass	O
m	O
demo	O
gmm	O
for	O
classification	B
kmeans	O
m	O
k-means	B
demokmeans	O
m	O
demo	O
of	O
k-means	B
demopolya	O
m	O
demo	O
of	O
the	O
number	O
of	O
active	B
clusters	O
from	O
a	O
polya	B
distribution	B
dirrnd	O
m	O
dirichlet	B
random	O
distribution	B
generator	O
cliquedecomp	O
m	O
clique	B
matrix	B
decomposition	B
cliquedecomp	O
c	O
clique	B
matrix	B
decomposition	B
democliquedecomp	O
m	O
demo	O
clique	B
matrix	B
decomposition	B
exercises	O
pv	O
exercise	O
consider	O
a	O
mixture	B
of	O
factorised	B
models	O
h	O
i	O
draft	O
march	O
pvih	O
show	O
that	O
optimally	O
ph	O
n	O
for	O
assumed	O
i	O
i	O
d	O
data	O
vn	O
n	O
n	O
some	O
observation	O
components	O
may	O
be	O
missing	B
so	O
that	O
for	O
example	O
the	O
third	O
component	O
of	O
the	O
fifth	O
datapoint	O
is	O
unknown	O
show	O
that	O
maximum	B
likelihood	B
training	B
on	O
the	O
observed	O
data	O
corresponds	O
to	O
ignoring	O
components	O
vn	O
i	O
that	O
are	O
missing	B
exercise	O
derive	O
the	O
optimal	O
em	B
update	O
for	O
fitting	O
a	O
mixture	B
of	O
gaussians	O
under	O
the	O
constraint	O
that	O
the	O
covariances	O
are	O
diagonal	O
exercises	O
exercise	O
consider	O
a	O
mixture	B
of	O
k	O
isotropic	B
gaussians	O
each	O
with	O
the	O
same	O
covariance	B
si	O
in	O
the	O
limit	O
show	O
that	O
the	O
em	B
algorithm	B
tends	O
to	O
the	O
k-means	B
clustering	B
algorithm	B
exercise	O
consider	O
the	O
term	O
we	O
wish	O
to	O
optimise	O
the	O
above	O
with	O
respect	O
to	O
the	O
distribution	B
ph	O
this	O
can	O
be	O
achieved	O
by	O
defining	O
the	O
lagrangian	B
l	O
by	O
differentiating	O
the	O
lagrangian	B
with	O
respect	O
to	O
ph	O
and	O
using	O
the	O
normalisation	B
ph	O
h	O
h	O
ph	O
poldhvn	O
exercise	O
we	O
showed	O
that	O
fitting	O
an	O
unconstrained	O
mixture	B
of	O
gaussians	O
using	O
maximum	B
likelihood	B
is	O
problematic	O
since	O
by	O
placing	O
one	O
of	O
the	O
gaussians	O
over	O
a	O
datapoints	O
and	O
letting	O
the	O
covariance	B
determinant	B
go	O
to	O
zero	O
we	O
obtain	O
an	O
infinite	O
likelihood	B
in	O
contrast	O
when	O
fitting	O
a	O
single	O
gaussian	B
n	O
to	O
i	O
i	O
d	O
data	O
xn	O
show	O
that	O
the	O
maximum	B
likelihood	B
optimum	O
for	O
has	O
non-zero	O
determinant	B
and	O
that	O
the	O
optimal	O
likelihood	B
remains	O
finite	O
exercise	O
modify	O
gmmem	O
m	O
suitably	O
so	O
that	O
it	O
can	O
deal	O
with	O
the	O
semi-supervised	B
scenario	O
in	O
which	O
the	O
mixture	B
component	O
h	O
of	O
some	O
of	O
the	O
observations	O
v	O
is	O
known	O
exercise	O
you	O
wish	O
to	O
parameterise	O
covariance	B
matrices	O
s	O
under	O
the	O
constraint	O
that	O
specified	O
elements	O
are	O
zero	O
the	O
constraints	O
are	O
specified	O
using	O
a	O
matrix	B
a	O
with	O
elements	O
aij	O
if	O
sij	O
and	O
aij	O
otherwise	O
consider	O
a	O
clique	B
matrix	B
z	O
for	O
which	O
a	O
hzzt	O
and	O
matrix	B
s	O
z	O
zt	O
with	O
ij	O
if	O
zij	O
if	O
zij	O
for	O
parameters	O
show	O
that	O
for	O
any	O
s	O
is	O
positive	O
semidefinite	O
and	O
parameterises	O
covariance	B
matrices	O
under	O
the	O
zero	O
constraints	O
specified	O
by	O
a	O
draft	O
march	O
chapter	O
latent	B
linear	B
models	O
factor	B
analysis	B
in	O
we	O
discussed	O
principal	B
components	I
analysis	B
which	O
forms	O
lower	B
dimensional	I
representations	I
of	O
data	O
based	O
on	O
assuming	O
that	O
the	O
data	O
lies	O
close	O
to	O
a	O
hyperplane	B
here	O
we	O
describe	O
a	O
related	O
probabilistic	B
model	B
for	O
which	O
extensions	O
to	O
bayesian	B
methods	O
can	O
be	O
envisaged	O
any	O
probabilistic	B
model	B
may	O
also	O
be	O
used	O
as	O
a	O
component	O
of	O
a	O
larger	O
more	O
complex	O
model	B
such	O
as	O
a	O
mixture	B
model	B
enabling	O
natural	B
generalisations	O
we	O
use	O
v	O
to	O
describe	O
a	O
real	O
data	O
vector	O
to	O
emphasise	O
that	O
this	O
is	O
a	O
visible	B
quantity	O
the	O
dataset	O
is	O
then	O
given	O
by	O
a	O
set	O
of	O
vectors	O
v	O
where	O
dim	O
d	O
our	O
interest	O
is	O
to	O
find	O
a	O
lower	O
dimensional	O
probabilistic	B
description	O
of	O
this	O
data	O
if	O
data	O
lies	O
close	O
to	O
a	O
h-dimensional	O
hyperplane	B
we	O
may	O
accurately	O
approximate	B
each	O
datapoint	O
by	O
a	O
low	O
h-dimensional	O
coordinate	O
system	O
in	O
general	O
datapoints	O
will	O
not	O
lie	O
exactly	O
on	O
the	O
hyperplane	B
and	O
we	O
model	B
this	O
discrepancy	O
with	O
gaussian	B
noise	O
mathematically	O
the	O
fa	O
model	B
generates	O
an	O
observation	O
v	O
according	O
to	O
v	O
fh	O
c	O
where	O
the	O
noise	O
is	O
gaussian	B
distributed	O
n	O
the	O
constant	O
bias	B
c	O
sets	O
the	O
origin	O
of	O
the	O
coordinate	O
system	O
the	O
factor	B
loading	I
matrix	B
f	O
plays	O
a	O
similar	O
role	O
as	O
the	O
basis	O
matrix	B
in	O
pca	B
see	O
similarly	O
the	O
hidden	B
coordinates	O
h	O
plays	O
the	O
role	O
of	O
the	O
components	O
we	O
used	O
in	O
the	O
difference	O
between	O
pca	B
and	O
factor	B
analysis	B
is	O
in	O
the	O
choice	O
of	O
probabilistic	B
pca	B
factor	B
analysis	B
diag	O
d	O
factor	B
analysis	B
figure	O
factor	B
analysis	B
the	O
visible	B
vector	O
variable	O
v	O
is	O
related	O
to	O
the	O
vector	O
hidden	B
variable	I
h	O
by	O
a	O
linear	B
mapping	O
with	O
independent	O
additive	O
gaussian	B
noise	O
on	O
each	O
visible	B
variable	O
the	O
prior	B
on	O
the	O
hidden	B
variable	I
may	O
be	O
taken	O
to	O
be	O
an	O
isotropic	B
gaussian	B
thus	O
being	O
independent	O
across	O
its	O
components	O
a	O
probabilistic	B
description	O
from	O
equation	B
and	O
equation	B
given	O
h	O
the	O
data	O
is	O
gaussian	B
distributed	O
with	O
mean	B
fh	O
c	O
and	O
covariance	B
p	O
n	O
fh	O
c	O
e	O
fh	O
ct	O
fh	O
c	O
to	O
complete	O
the	O
model	B
we	O
need	O
to	O
specify	O
the	O
hidden	B
distribution	B
ph	O
a	O
convenient	O
choice	O
is	O
a	O
gaussian	B
p	O
n	O
i	O
e	O
under	O
this	O
prior	B
the	O
coordinates	O
h	O
will	O
be	O
preferentially	O
concentrated	O
around	O
values	O
close	O
to	O
if	O
we	O
sample	O
a	O
h	O
from	O
ph	O
and	O
then	O
draw	O
a	O
value	B
for	O
v	O
using	O
pvh	O
the	O
sampled	O
v	O
vectors	O
would	O
produce	O
a	O
saucer	O
or	O
pancake	O
of	O
points	O
in	O
the	O
v	O
space	O
using	O
a	O
correlated	O
gaussian	B
prior	B
ph	O
n	O
h	O
has	O
no	O
effect	O
on	O
the	O
complexity	O
of	O
the	O
model	B
since	O
h	O
can	O
be	O
absorbed	O
into	O
f	O
since	O
v	O
is	O
linearly	O
related	O
to	O
h	O
through	O
equation	B
and	O
both	O
and	O
h	O
are	O
gaussian	B
then	O
v	O
is	O
gaussian	B
distributed	O
the	O
mean	B
and	O
covariance	B
can	O
be	O
computed	O
using	O
the	O
propagation	B
results	O
in	O
p	O
p	O
p	O
dh	O
n	O
v	O
c	O
fft	O
invariance	O
of	O
the	O
likelihood	B
under	O
factor	B
rotation	I
since	O
the	O
matrix	B
f	O
only	O
appears	O
in	O
the	O
final	O
model	B
pv	O
through	O
fft	O
the	O
likelihood	B
is	O
unchanged	O
if	O
we	O
rotate	O
f	O
using	O
fr	O
with	O
rrt	O
i	O
frfrt	O
frrtft	O
fft	O
the	O
solution	O
space	O
for	O
f	O
is	O
therefore	O
not	O
unique	O
we	O
can	O
arbitrarily	O
rotate	O
the	O
matrix	B
f	O
and	O
produce	O
an	O
equally	O
likely	O
model	B
of	O
the	O
data	O
some	O
care	O
is	O
therefore	O
required	O
when	O
interpreting	O
the	O
entries	O
of	O
f	O
varimax	B
provides	O
a	O
more	O
interpretable	O
f	O
by	O
using	O
a	O
suitable	O
rotation	O
matrix	B
r	O
the	O
aim	O
is	O
to	O
produce	O
a	O
rotated	O
f	O
for	O
which	O
each	O
column	O
has	O
only	O
a	O
small	O
number	O
of	O
large	O
values	O
finding	O
a	O
suitable	O
rotation	O
results	O
in	O
a	O
non-linear	B
optimisation	B
problem	B
and	O
needs	O
to	O
be	O
solved	O
numerically	O
see	O
for	O
details	O
finding	O
the	O
optimal	O
bias	B
for	O
a	O
set	O
of	O
data	O
v	O
and	O
using	O
the	O
usual	O
i	O
i	O
d	O
assumption	O
the	O
log	O
likelihood	B
is	O
log	O
pvn	O
ct	O
d	O
c	O
n	O
log	O
det	O
d	O
log	O
pvf	O
where	O
d	O
fft	O
c	O
n	O
differentiating	O
equation	B
with	O
respect	O
to	O
c	O
and	O
equating	O
to	O
zero	O
we	O
arrive	O
at	O
the	O
maximum	B
likelihood	B
optimal	O
setting	O
that	O
the	O
bias	B
c	O
is	O
the	O
mean	B
of	O
the	O
data	O
vn	O
v	O
draft	O
march	O
factor	B
analysis	B
maximum	B
likelihood	B
latent	B
two-dimensional	O
figure	O
factor	B
analysis	B
points	O
generated	O
from	O
the	O
model	B
points	O
hn	O
sampled	O
from	O
n	O
i	O
these	O
are	O
transformed	O
to	O
a	O
point	O
on	O
the	O
three-dimensional	O
plane	O
by	O
xn	O
c	O
fhn	O
the	O
covariance	B
of	O
is	O
degenerate	O
with	O
covariance	B
matrix	B
fft	O
for	O
each	O
point	O
xn	O
on	O
the	O
plane	O
a	O
random	O
noise	O
vector	O
is	O
drawn	O
from	O
n	O
and	O
added	O
to	O
the	O
in-plane	O
vector	O
to	O
form	O
a	O
sample	O
xn	O
plotted	O
in	O
red	O
the	O
distribution	B
of	O
points	O
forms	O
a	O
pancake	O
in	O
space	O
points	O
underneath	O
the	O
plane	O
are	O
not	O
shown	O
we	O
will	O
use	O
this	O
setting	O
throughout	O
with	O
this	O
setting	O
the	O
log	O
likelihood	B
equation	B
can	O
be	O
written	O
log	O
pvf	O
n	O
d	O
log	O
det	O
where	O
s	O
is	O
the	O
sample	O
covariance	B
matrix	B
s	O
n	O
v	O
vt	O
factor	B
analysis	B
maximum	B
likelihood	B
we	O
now	O
specialise	O
to	O
the	O
assumption	O
that	O
diag	O
d	O
we	O
consider	O
two	O
methods	O
for	O
learning	B
the	O
factor	B
loadings	O
f	O
a	O
direct	O
and	O
an	O
em	B
approach	B
direct	O
likelihood	B
optimisation	B
optimal	O
f	O
for	O
fixed	O
to	O
find	O
the	O
maximum	B
likelihood	B
setting	O
of	O
f	O
we	O
differentiate	O
the	O
log	O
likelihood	B
equation	B
with	O
respect	O
to	O
f	O
and	O
equate	O
to	O
zero	O
this	O
gives	O
d	O
f	O
d	O
d	O
d	O
f	O
using	O
ffft	O
f	O
fft	O
ffft	O
a	O
stationary	B
point	O
is	O
given	O
when	O
d	O
f	O
d	O
s	O
d	O
f	O
since	O
d	O
is	O
invertible	O
the	O
optimal	O
f	O
satisfies	O
f	O
s	O
d	O
f	O
using	O
the	O
definition	O
of	O
d	O
equation	B
one	O
can	O
rewrite	O
d	O
f	O
as	O
d	O
f	O
i	O
ft	O
presentation	O
here	O
follows	O
closely	O
that	O
of	O
draft	O
march	O
factor	B
analysis	B
maximum	B
likelihood	B
plugging	O
this	O
into	O
the	O
zero	O
derivative	O
condition	O
equation	B
becomes	O
f	O
i	O
ft	O
s	O
using	O
the	O
reparameterisations	O
equation	B
can	O
be	O
written	O
in	O
the	O
isotropic	B
form	O
s	O
s	O
f	O
f	O
i	O
ft	O
f	O
f	O
s	O
f	O
we	O
assume	O
that	O
the	O
transformed	O
factor	B
matrix	B
f	O
has	O
a	O
thin	B
svd	B
decomposition	B
f	O
uhlvt	O
where	O
dim	O
uh	O
d	O
h	O
dim	O
l	O
h	O
h	O
dim	O
v	O
h	O
h	O
and	O
ut	O
huh	O
ih	O
and	O
l	O
diag	O
lh	O
are	O
the	O
singular	B
values	O
of	O
f	O
plugging	O
this	O
assumption	O
into	O
equation	B
we	O
obtain	O
vtv	O
ih	O
ih	O
suh	O
uh	O
which	O
gives	O
suhlvt	O
h	O
is	O
then	O
an	O
eigen-equation	O
for	O
uh	O
intuitively	O
it	O
s	O
clear	O
that	O
we	O
need	O
to	O
find	O
then	O
the	O
eigendecomposition	O
of	O
s	O
and	O
then	O
set	O
the	O
columns	O
of	O
uh	O
to	O
those	O
eigenvectors	O
corresponding	O
to	O
the	O
largest	O
eigenvalues	O
this	O
is	O
derived	O
more	O
formally	O
below	O
determining	O
the	O
appropriate	O
eigenvalues	O
we	O
can	O
relate	O
the	O
form	O
of	O
the	O
solution	O
to	O
the	O
eigen-decomposition	O
of	O
s	O
s	O
u	O
ut	O
u	O
where	O
ur	O
are	O
arbitrary	O
additional	O
columns	O
chosen	O
to	O
complete	O
uh	O
to	O
form	O
an	O
orthogonal	B
u	O
utu	O
i	O
i	O
or	O
li	O
i	O
given	O
uut	O
i	O
using	O
diag	O
d	O
equation	B
stipulates	O
the	O
solution	O
for	O
f	O
the	O
solution	O
for	O
f	O
is	O
found	O
from	O
equation	B
to	O
determine	O
the	O
optimal	O
i	O
we	O
write	O
the	O
log	O
likelihood	B
in	O
terms	O
of	O
the	O
i	O
as	O
follows	O
using	O
the	O
new	O
parameterisation	B
log	O
det	O
draft	O
march	O
d	O
f	O
ft	O
i	O
d	O
trace	O
we	O
have	O
and	O
s	O
s	O
n	O
log	O
pvf	O
trace	O
the	O
log	O
likelihood	B
equation	B
in	O
this	O
new	O
parameterisation	B
is	O
f	O
ft	O
id	O
s	O
id	O
f	O
s	O
id	O
f	O
log	O
det	O
using	O
i	O
i	O
and	O
equation	B
we	O
can	O
write	O
id	O
f	O
ft	O
id	O
h	O
udiag	O
h	O
ut	O
i	O
i	O
i	O
h	O
i	O
h	O
i	O
i	O
i	O
log	O
i	O
using	O
this	O
we	O
can	O
write	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
eigenvalues	O
fixed	O
as	O
to	O
maximise	O
the	O
likelihood	B
we	O
need	O
to	O
minimise	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
since	O
log	O
we	O
i	O
log	O
i	O
term	O
a	O
solution	O
for	O
fixed	O
is	O
therefore	O
i	O
log	O
det	O
factor	B
analysis	B
maximum	B
likelihood	B
so	O
that	O
trace	O
similarly	O
log	O
det	O
id	O
f	O
s	O
id	O
f	O
should	O
place	O
the	O
largest	O
h	O
eigenvalues	O
in	O
log	O
pvf	O
log	O
i	O
h	O
n	O
f	O
uh	O
h	O
ih	O
r	O
where	O
h	O
diag	O
h	O
are	O
the	O
h	O
largest	O
eigenvalues	O
of	O
r	O
is	O
an	O
arbitrary	O
orthogonal	B
matrix	B
s	O
with	O
uh	O
being	O
the	O
matrix	B
of	O
the	O
corresponding	O
eigenvectors	O
svd	B
based	O
approach	B
rather	O
than	O
finding	O
the	O
eigen-decomposition	O
of	O
considering	O
the	O
thin	B
svd	B
decomposition	B
of	O
s	O
we	O
can	O
avoid	O
forming	O
the	O
covariance	B
matrix	B
by	O
where	O
the	O
data	O
matrix	B
is	O
x	O
n	O
x	O
x	O
given	O
a	O
thin	B
decomposition	B
x	O
uh	O
vt	O
we	O
obtain	O
the	O
eigenvalues	O
i	O
using	O
this	O
svd	B
method	O
is	O
o	O
svd	B
methods	O
are	O
finding	O
the	O
optimal	O
s	O
s	O
diag	O
new	O
diag	O
min	O
ii	O
for	O
d	O
n	O
this	O
is	O
convenient	O
since	O
the	O
computational	B
complexity	I
when	O
the	O
matrix	B
x	O
is	O
too	O
large	O
to	O
store	O
in	O
memory	O
online	B
the	O
zero	O
derivative	O
of	O
the	O
log	O
likelihood	B
occurs	O
when	O
where	O
f	O
is	O
given	O
by	O
equation	B
there	O
is	O
no	O
closed	O
form	O
solution	O
to	O
a	O
simple	O
iterative	O
scheme	O
is	O
to	O
first	O
guess	O
values	O
for	O
the	O
diagonal	O
entries	O
of	O
and	O
then	O
find	O
the	O
optimal	O
f	O
using	O
equation	B
subsequently	O
is	O
updated	O
using	O
we	O
update	O
f	O
using	O
equation	B
and	O
using	O
equation	B
until	O
convergence	O
alternative	O
schemes	O
for	O
updating	O
the	O
noise	O
matrix	B
can	O
improve	O
convergence	O
considerably	O
for	O
example	O
updating	O
only	O
a	O
single	O
component	O
of	O
with	O
the	O
rest	O
fixed	O
can	O
be	O
achieved	O
using	O
a	O
closed	O
form	O
draft	O
march	O
expectation	B
maximisation	B
a	O
popular	O
way	O
to	O
train	O
factor	B
analysis	B
in	O
machine	O
learning	B
is	O
to	O
use	O
em	B
we	O
assume	O
that	O
the	O
bias	B
c	O
has	O
been	O
optimally	O
set	O
to	O
the	O
data	O
mean	B
v	O
factor	B
analysis	B
maximum	B
likelihood	B
m-step	B
as	O
usual	O
we	O
need	O
to	O
consider	O
the	O
energy	B
which	O
neglecting	O
constants	O
is	O
e	O
fht	O
fh	O
qhvn	O
n	O
log	O
det	O
where	O
dn	O
vn	O
v	O
the	O
optimal	O
variational	O
distribution	B
qhvn	O
is	O
determined	O
by	O
the	O
e-step	B
below	O
maximising	O
e	O
with	O
respect	O
to	O
f	O
gives	O
fnew	O
ah	O
where	O
a	O
n	O
n	O
finally	O
new	O
n	O
n	O
h	O
fh	O
n	O
qhvn	O
qhvn	O
dn	O
diag	O
n	O
qhvn	O
diag	O
n	O
n	O
dndnt	O
fhft	O
e-step	B
the	O
above	O
recursions	O
depend	O
on	O
the	O
statistics	O
for	O
the	O
e-step	B
we	O
have	O
qhvn	O
using	O
the	O
em	B
optimal	O
choice	O
with	O
qhvn	O
pvnhph	O
n	O
mn	O
i	O
ft	O
ft	O
i	O
ft	O
using	O
these	O
results	O
we	O
can	O
express	O
the	O
statistics	O
in	O
equation	B
as	O
mn	O
h	O
n	O
n	O
mnmnt	O
equations	O
are	O
iterated	O
till	O
convergence	O
as	O
for	O
any	O
em	B
algorithm	B
the	O
likelihood	B
equation	B
the	O
diagonal	O
constraint	O
on	O
increases	O
at	O
each	O
iteration	B
convergence	O
using	O
this	O
em	B
technique	O
can	O
be	O
slower	O
than	O
that	O
of	O
the	O
direct	O
eigen-approach	O
of	O
and	O
commercial	O
implementations	O
usually	O
avoid	O
em	B
for	O
this	O
reason	O
provided	O
however	O
that	O
a	O
reasonable	O
initialisation	O
is	O
used	O
the	O
performance	B
of	O
the	O
two	O
training	B
algorithms	O
can	O
be	O
similar	O
a	O
useful	O
initialisation	O
is	O
to	O
use	O
pca	B
and	O
then	O
set	O
f	O
to	O
the	O
principal	B
directions	I
mixtures	O
of	O
fa	O
an	O
advantage	O
of	O
probabilistic	B
models	O
is	O
that	O
they	O
may	O
be	O
used	O
as	O
components	O
in	O
more	O
complex	O
models	O
such	O
as	O
mixtures	O
of	O
training	B
can	O
then	O
be	O
achieved	O
using	O
em	B
or	O
gradient	B
based	O
approaches	O
bayesian	B
extensions	O
are	O
clearly	O
of	O
interest	O
whilst	O
formally	O
intractable	O
they	O
can	O
be	O
addressed	O
using	O
approximate	B
methods	O
for	O
example	O
draft	O
march	O
interlude	O
modelling	B
faces	B
g	O
g	O
origin	O
f	O
figure	O
latent	B
identity	O
model	B
the	O
mean	B
represents	O
the	O
mean	B
of	O
the	O
faces	B
the	O
subspace	O
f	O
represents	O
the	O
directions	O
of	O
variation	O
of	O
different	O
faces	B
so	O
that	O
is	O
a	O
mean	B
face	O
for	O
individual	O
and	O
similarly	O
for	O
the	O
subspace	O
g	O
denotes	O
the	O
directions	O
of	O
variability	O
for	O
any	O
individual	O
face	O
caused	O
by	O
pose	O
lighting	O
etc	O
this	O
variability	O
is	O
assumed	O
the	O
same	O
for	O
each	O
person	O
a	O
particular	O
mean	B
face	O
is	O
then	O
given	O
by	O
the	O
mean	B
face	O
of	O
the	O
person	O
plus	O
poseillumination	O
variation	O
for	O
example	O
a	O
sample	O
face	O
is	O
then	O
given	O
by	O
a	O
mean	B
face	O
xij	O
plus	O
gaussian	B
noise	O
from	O
n	O
wij	O
hi	O
xij	O
j	O
i	O
figure	O
the	O
jth	O
image	O
of	O
the	O
ith	O
person	O
xij	O
is	O
modelled	O
using	O
a	O
linear	B
latent	B
model	B
with	O
parameters	O
interlude	O
modelling	B
faces	B
factor	B
analysis	B
has	O
widespread	O
application	O
in	O
statistics	O
and	O
machine	O
learning	B
as	O
an	O
inventive	O
application	O
of	O
fa	O
highlighting	O
the	O
probabilistic	B
nature	O
of	O
the	O
model	B
we	O
describe	O
a	O
face	O
modelling	B
technique	O
that	O
has	O
as	O
its	O
heart	O
a	O
latent	B
linear	B
consider	O
a	O
gallery	O
of	O
face	O
images	O
x	O
i	O
i	O
j	O
j	O
so	O
that	O
the	O
vector	O
xij	O
represents	O
the	O
jth	O
image	O
of	O
the	O
ith	O
person	O
as	O
a	O
latent	B
linear	B
model	B
of	O
faces	B
we	O
consider	O
xij	O
fhi	O
gwij	O
here	O
f	O
f	O
d	O
f	O
is	O
used	O
to	O
model	B
variability	O
between	O
people	O
and	O
g	O
g	O
d	O
g	O
models	O
variability	O
related	O
to	O
pose	O
illumination	O
etc	O
within	O
the	O
different	O
images	O
of	O
each	O
person	O
the	O
contribution	O
fi	O
fhi	O
accounts	O
for	O
variability	O
between	O
different	O
people	O
being	O
constant	O
for	O
individual	O
i	O
for	O
fixed	O
i	O
the	O
contribution	O
gwij	O
accounts	O
for	O
the	O
variability	O
over	O
the	O
images	O
of	O
person	O
i	O
explaining	O
why	O
two	O
images	O
of	O
the	O
same	O
person	O
do	O
not	O
look	O
identical	O
see	O
for	O
a	O
graphical	O
representation	O
as	O
a	O
probabilistic	B
linear	B
latent	B
variable	I
model	B
we	O
have	O
for	O
an	O
image	O
xij	O
the	O
parameters	O
are	O
g	O
for	O
the	O
collection	O
of	O
images	O
assuming	O
i	O
i	O
d	O
data	O
pxijhi	O
wij	O
n	O
fhi	O
gwij	O
phi	O
n	O
i	O
pwij	O
n	O
i	O
pxijhi	O
wij	O
phi	O
for	O
which	O
the	O
graphical	O
model	B
is	O
depicted	O
in	O
the	O
task	O
of	O
learning	B
is	O
then	O
to	O
maximise	O
the	O
likelihood	B
px	O
w	O
h	O
px	O
px	O
w	O
h	O
wh	O
draft	O
march	O
interlude	O
modelling	B
faces	B
mean	B
variance	B
figure	O
latent	B
identity	O
model	B
of	O
face	O
images	O
each	O
image	O
is	O
represented	O
by	O
a	O
vector	O
comes	O
from	O
the	O
rgb	O
colour	O
coding	O
there	O
are	O
i	O
individuals	O
in	O
the	O
database	O
and	O
j	O
per	O
pixel	O
standard	B
deviation	I
black	O
is	O
low	O
white	O
is	O
images	O
per	O
person	O
three	O
samples	O
from	O
high	O
the	O
model	B
with	O
h	O
fixed	O
and	O
drawing	O
randomly	O
from	O
w	O
in	O
the	O
within-individual	O
subspace	O
g	O
reproduced	O
from	O
three	O
directions	O
from	O
the	O
between-individual	O
subspace	O
f	O
mean	B
of	O
the	O
data	O
this	O
model	B
can	O
be	O
seem	O
as	O
a	O
constrained	O
version	O
of	O
factor	B
analysis	B
by	O
using	O
stacked	O
vectors	O
for	O
only	O
a	O
single	O
individual	O
i	O
f	O
g	O
f	O
g	O
g	O
f	O
the	O
generalisation	B
to	O
multiple	O
individuals	O
i	O
is	O
straightforward	O
the	O
model	B
can	O
be	O
trained	O
using	O
either	O
a	O
constrained	O
form	O
of	O
the	O
direct	O
method	O
or	O
em	B
as	O
described	O
in	O
example	O
images	O
from	O
the	O
trained	O
model	B
are	O
presented	O
in	O
recognition	O
in	O
closed	O
set	O
face	O
recognition	O
a	O
new	O
probe	O
face	O
x	O
is	O
to	O
be	O
matched	O
to	O
a	O
person	O
n	O
in	O
the	O
gallery	O
of	O
training	B
faces	B
in	O
model	B
mn	O
the	O
nth	O
gallery	O
face	O
is	O
forced	O
to	O
share	O
its	O
latent	B
identity	O
variable	O
hn	O
with	O
the	O
test	O
face	O
indicating	O
that	O
these	O
faces	B
belong	O
to	O
the	O
same	O
assuming	O
a	O
single	O
exemplar	O
per	O
person	O
xi	O
x	O
pxn	O
x	O
pxi	O
bayes	O
rule	O
then	O
gives	O
the	O
posterior	B
class	O
assignment	O
xi	O
x	O
xi	O
x	O
for	O
a	O
uniform	B
prior	B
the	O
term	O
pmn	O
is	O
constant	O
and	O
can	O
be	O
neglected	O
the	O
quantities	O
we	O
require	O
above	O
are	O
given	O
by	O
pxn	O
pxn	O
hn	O
wn	O
px	O
px	O
h	O
w	O
hnwn	O
h	O
w	O
is	O
analogous	O
to	O
bayesian	B
outcome	B
analysis	B
in	O
in	O
which	O
the	O
hypotheses	O
assume	O
that	O
either	O
the	O
errors	O
were	O
generated	O
from	O
the	O
same	O
or	O
a	O
different	O
model	B
draft	O
march	O
probabilistic	B
principal	B
components	I
analysis	B
x	O
w	O
hnwnw	O
pxn	O
x	O
x	O
w	O
figure	O
face	O
recognition	O
model	B
only	O
for	O
a	O
single	O
examplar	O
per	O
person	O
j	O
in	O
model	B
the	O
test	O
image	O
probe	O
x	O
is	O
assumed	O
to	O
be	O
from	O
person	O
albeit	O
with	O
a	O
different	O
for	O
model	B
poseillumination	O
the	O
test	O
image	O
is	O
assumed	O
to	O
be	O
from	O
person	O
one	O
calculates	O
x	O
and	O
x	O
and	O
then	O
uses	O
bayes	O
rule	O
to	O
infer	O
which	O
person	O
the	O
test	O
image	O
x	O
most	O
likely	O
belongs	O
pxn	O
x	O
hn	O
wn	O
w	O
where	O
px	O
h	O
w	O
is	O
obtained	O
from	O
equation	B
we	O
assume	O
the	O
parameters	O
are	O
fixed	O
having	O
been	O
learned	O
using	O
maximum	B
likelihood	B
note	O
that	O
in	O
equation	B
we	O
do	O
not	O
introduce	O
h	O
since	O
both	O
xn	O
and	O
x	O
are	O
assumed	O
to	O
be	O
generated	O
from	O
the	O
same	O
person	O
with	O
the	O
latent	B
identity	O
hn	O
these	O
marginal	B
probabilities	O
are	O
straightforward	O
to	O
derive	O
since	O
they	O
are	O
marginals	O
of	O
gaussians	O
in	O
practice	O
the	O
best	O
results	O
are	O
obtained	O
using	O
a	O
between-individual	O
subspace	O
dimension	O
f	O
and	O
withinindividual	O
subspace	O
dimension	O
g	O
both	O
equal	O
to	O
this	O
model	B
then	O
has	O
performance	B
competitive	O
with	O
the	O
a	O
benefit	O
of	O
the	O
probabilistic	B
model	B
is	O
that	O
the	O
extension	O
to	O
mixtures	O
of	O
this	O
model	B
is	O
essentially	O
straightforward	O
which	O
boosts	O
performance	B
further	O
related	O
models	O
can	O
also	O
be	O
used	O
for	O
the	O
open	O
set	O
face	O
recognition	O
problem	B
in	O
which	O
the	O
probe	O
face	O
may	O
or	O
may	O
not	O
belong	O
to	O
one	O
of	O
the	O
individuals	O
in	O
the	O
probabilistic	B
principal	B
components	I
analysis	B
ppca	O
corresponds	O
to	O
factor	B
analysis	B
under	O
the	O
restriction	O
plugging	O
this	O
assumption	O
into	O
the	O
direct	O
optimisation	B
solution	O
equation	B
gives	O
f	O
uh	O
h	O
ih	O
r	O
where	O
the	O
eigenvalues	O
entries	O
of	O
h	O
and	O
corresponding	O
eigenvectors	O
of	O
uh	O
are	O
the	O
largest	O
eigenvalues	O
of	O
since	O
the	O
eigenvalues	O
of	O
are	O
those	O
of	O
s	O
simply	O
scaled	O
by	O
the	O
eigenvectors	O
are	O
unchanged	O
we	O
can	O
equivalently	O
write	O
h	O
r	O
f	O
uh	O
where	O
r	O
is	O
an	O
arbitrary	O
orthogonal	B
matrix	B
with	O
rtr	O
i	O
and	O
uh	O
h	O
are	O
the	O
eigenvectors	O
and	O
corresponding	O
eigenvalues	O
of	O
the	O
sample	O
covariance	B
s	O
classical	O
pca	B
is	O
recovered	O
in	O
the	O
limit	O
note	O
that	O
for	O
a	O
full	O
correspondence	O
with	O
pca	B
one	O
needs	O
to	O
set	O
r	O
i	O
which	O
points	O
f	O
along	O
the	O
principal	B
directions	I
optimal	O
a	O
particular	O
convenience	O
of	O
ppca	O
is	O
that	O
the	O
optimal	O
noise	O
can	O
be	O
found	O
immediately	O
we	O
order	O
the	O
eigenvalues	O
of	O
s	O
so	O
that	O
d	O
in	O
equation	B
an	O
expression	O
for	O
the	O
log	O
likelihood	B
is	O
given	O
in	O
which	O
the	O
eigenvalues	O
are	O
those	O
on	O
replacing	O
i	O
with	O
i	O
we	O
can	O
therefore	O
write	O
an	O
explicit	O
expression	O
for	O
the	O
log	O
likelihood	B
in	O
terms	O
of	O
and	O
the	O
eigenvalues	O
of	O
a	O
l	O
n	O
d	O
draft	O
march	O
i	O
h	O
log	O
h	O
log	O
i	O
canonical	B
correlation	I
analysis	B
and	O
factor	B
analysis	B
figure	O
for	O
a	O
hidden	B
unit	O
model	B
here	O
are	O
plotted	O
the	O
results	O
of	O
training	B
ppca	O
and	O
fa	O
on	O
examples	O
of	O
the	O
handwritten	O
digit	O
seven	O
the	O
top	O
row	O
contains	O
the	O
factor	B
analysis	B
factors	O
and	O
the	O
bottom	O
row	O
the	O
largest	O
eigenvectors	O
from	O
ppca	O
are	O
plotted	O
by	O
differentiating	O
l	O
and	O
equating	O
to	O
zero	O
the	O
maximum	B
likelihood	B
optimal	O
setting	O
for	O
is	O
d	O
h	O
j	O
in	O
summary	O
ppca	O
is	O
obtained	O
by	O
taking	O
the	O
principal	O
eigenvalues	O
and	O
corresponding	O
eigenvectors	O
of	O
the	O
sample	O
covariance	B
matrix	B
s	O
and	O
setting	O
the	O
variance	B
by	O
equation	B
the	O
single-shot	O
training	B
nature	O
of	O
ppca	O
makes	O
it	O
an	O
attractive	O
algorithm	B
and	O
also	O
gives	O
a	O
useful	O
initialisation	O
for	O
factor	B
analysis	B
example	O
comparison	O
of	O
fa	O
and	O
ppca	O
we	O
trained	O
both	O
ppca	O
and	O
fa	O
to	O
model	B
handwritten	B
digits	I
of	O
the	O
number	O
from	O
a	O
database	O
of	O
such	O
images	O
we	O
fitted	O
both	O
ppca	O
and	O
fa	O
iterations	O
of	O
em	B
in	O
each	O
case	O
from	O
the	O
same	O
random	O
initialisation	O
using	O
hidden	B
units	O
the	O
learned	O
factors	O
for	O
these	O
models	O
are	O
in	O
to	O
get	O
a	O
feeling	O
for	O
how	O
well	O
each	O
of	O
these	O
models	O
the	O
data	O
we	O
drew	O
samples	O
from	O
each	O
model	B
as	O
given	O
in	O
compared	O
with	O
ppca	O
in	O
fa	O
the	O
individual	O
noise	O
on	O
each	O
observation	O
pixel	O
enables	O
a	O
cleaner	O
representation	O
of	O
the	O
regions	O
of	O
zero	O
sample	O
variance	B
canonical	B
correlation	I
analysis	B
and	O
factor	B
analysis	B
we	O
outline	O
how	O
cca	O
as	O
discussed	O
in	O
is	O
related	O
to	O
a	O
constrained	O
form	O
of	O
fa	O
as	O
a	O
brief	O
reminder	O
cca	O
considers	O
two	O
spaces	O
x	O
and	O
y	O
where	O
for	O
example	O
x	O
might	O
represent	O
an	O
audio	O
sequence	O
of	O
a	O
person	O
speaking	O
and	O
y	O
the	O
corresponding	O
video	O
sequence	O
of	O
the	O
face	O
of	O
the	O
person	O
speaking	O
the	O
two	O
streams	O
of	O
data	O
are	O
dependent	O
since	O
we	O
would	O
expect	O
the	O
parts	O
around	O
the	O
mouth	O
region	O
to	O
be	O
correlated	O
with	O
the	O
speech	O
signal	O
the	O
aim	O
in	O
cca	O
is	O
to	O
find	O
a	O
low	B
dimensional	I
representation	O
that	O
explains	O
the	O
correlation	O
between	O
the	O
x	O
and	O
y	O
spaces	O
a	O
model	B
that	O
achieves	O
a	O
similar	O
effect	O
to	O
cca	O
is	O
to	O
use	O
a	O
latent	B
factor	B
h	O
to	O
underlie	O
the	O
data	O
in	O
both	O
the	O
x	O
and	O
y	O
spaces	O
that	O
is	O
px	O
y	O
where	O
pxhpyhphdh	O
pxh	O
n	O
ha	O
x	O
pyh	O
n	O
hb	O
y	O
ph	O
n	O
figure	O
samples	O
from	O
the	O
learned	O
fa	O
model	B
note	O
how	O
the	O
noise	O
variance	B
depends	O
on	O
the	O
pixel	O
being	O
zero	O
for	O
pixels	O
on	O
the	O
boundary	B
of	O
samples	O
the	O
image	O
from	O
the	O
learned	O
ppca	O
model	B
factor	B
analysis	B
ppca	O
draft	O
march	O
fafafafafapcapcapcapcapca	O
independent	B
components	I
analysis	B
h	O
x	O
y	O
canonical	B
correlation	I
analysis	B
cca	O
corresponds	O
to	O
the	O
latent	B
variable	I
model	B
in	O
which	O
a	O
common	O
latent	B
variable	I
generates	O
both	O
the	O
observed	O
x	O
and	O
y	O
variables	O
this	O
is	O
therefore	O
a	O
formed	O
of	O
constrained	B
factor	B
analysis	B
we	O
can	O
express	O
equation	B
as	O
a	O
form	O
of	O
factor	B
analysis	B
by	O
writing	O
n	O
x	O
n	O
y	O
x	O
y	O
z	O
x	O
y	O
a	O
b	O
h	O
a	O
b	O
f	O
by	O
using	O
the	O
stacked	O
vectors	O
and	O
integrating	O
out	O
the	O
latent	B
variable	I
h	O
we	O
obtain	O
ff	O
t	O
x	O
y	O
from	O
the	O
fa	O
results	O
equation	B
the	O
optimal	O
f	O
is	O
given	O
by	O
pz	O
n	O
f	O
t	O
f	O
s	O
f	O
s	O
so	O
that	O
optimally	O
f	O
is	O
given	O
by	O
the	O
principal	O
eigenvector	O
of	O
s	O
by	O
imposing	O
x	O
above	O
equation	B
can	O
be	O
expressed	O
as	O
the	O
coupled	B
equations	O
xi	O
y	O
yi	O
the	O
x	O
x	O
sxxa	O
syxa	O
y	O
y	O
sxyb	O
syyb	O
a	O
b	O
i	O
eliminating	O
b	O
we	O
have	O
for	O
an	O
arbitrary	O
proportionality	O
constant	O
x	O
sxx	O
a	O
x	O
y	O
sxy	O
y	O
syy	O
syxa	O
i	O
x	O
in	O
the	O
limit	O
y	O
this	O
tends	O
to	O
the	O
zero	O
derivative	O
condition	O
equation	B
so	O
that	O
cca	O
can	O
be	O
seen	O
as	O
in	O
fact	O
a	O
form	O
limiting	O
of	O
fa	O
for	O
a	O
more	O
thorough	O
correspondence	O
by	O
viewing	O
cca	O
in	O
this	O
manner	O
extensions	O
to	O
using	O
more	O
than	O
a	O
single	O
latent	B
dimension	O
h	O
become	O
clear	O
see	O
in	O
addition	O
to	O
the	O
benefits	O
of	O
a	O
probabilistic	B
interpretation	O
as	O
we	O
ve	O
indicated	O
cca	O
corresponds	O
to	O
training	B
a	O
form	O
of	O
fa	O
by	O
maximising	O
the	O
joint	B
likelihood	B
px	O
yw	O
u	O
alternatively	O
training	B
based	O
on	O
the	O
maximising	O
the	O
conditional	B
pyx	O
w	O
u	O
corresponds	O
to	O
a	O
special	O
case	O
of	O
a	O
technique	O
called	O
partial	B
least	I
squares	I
see	O
for	O
example	O
this	O
correspondence	O
is	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
extending	O
fa	O
to	O
kernel	B
variants	O
is	O
not	O
straightforward	O
since	O
under	O
replacing	O
x	O
with	O
a	O
non-linear	B
mapping	O
normalising	O
the	O
expression	O
e	O
is	O
in	O
general	O
intractable	O
independent	B
components	I
analysis	B
independent	B
components	I
analysis	B
is	O
a	O
linear	B
decomposition	B
of	O
the	O
data	O
v	O
in	O
which	O
the	O
components	O
of	O
underlying	O
latent	B
vector	O
variable	O
h	O
are	O
in	O
other	O
words	O
we	O
seek	O
a	O
linear	B
coordinate	O
system	O
in	O
which	O
the	O
coordinates	O
are	O
independent	O
such	O
independent	O
coordinate	O
systems	O
arguably	O
form	O
draft	O
march	O
independent	B
components	I
analysis	B
exp	O
figure	O
latent	B
data	O
is	O
sampled	O
from	O
the	O
prior	B
pxi	O
with	O
the	O
mixing	B
matrix	B
a	O
shown	O
in	O
green	O
to	O
create	O
the	O
observed	O
two	O
dimensional	O
vectors	O
y	O
ax	O
the	O
red	O
lines	O
are	O
the	O
mixing	B
matrix	B
estimated	O
by	O
ica	B
m	O
based	O
on	O
the	O
observations	O
for	O
comparison	O
pca	B
produces	O
the	O
blue	O
components	O
note	O
that	O
the	O
components	O
have	O
been	O
scaled	O
to	O
improve	O
visualisation	O
as	O
expected	O
pca	B
finds	O
the	O
orthogonal	B
directions	O
of	O
maximal	O
variation	O
ica	B
however	O
correctly	O
estimates	O
the	O
directions	O
in	O
which	O
the	O
components	O
were	O
independently	O
generated	O
see	O
demoica	O
m	O
a	O
natural	B
representation	O
of	O
the	O
data	O
and	O
can	O
give	O
rise	O
to	O
very	O
different	O
representations	O
than	O
pca	B
see	O
from	O
a	O
probabilistic	B
viewpoint	O
the	O
model	B
is	O
described	O
by	O
phi	O
pv	O
ha	O
pvh	O
i	O
in	O
ica	B
it	O
is	O
common	O
to	O
assume	O
that	O
the	O
observations	O
are	O
linearly	O
related	O
to	O
the	O
latent	B
variables	O
h	O
for	O
technical	O
reasons	O
the	O
most	O
convenient	O
practical	O
choice	O
is	O
to	O
where	O
a	O
is	O
a	O
square	O
mixing	B
matrix	B
so	O
that	O
the	O
likelihood	B
of	O
an	O
observation	O
v	O
is	O
v	O
ah	O
pvh	O
i	O
pv	O
phidh	O
i	O
phidh	O
i	O
i	O
the	O
underlying	O
independence	B
assumptions	O
are	O
then	O
the	O
same	O
as	O
for	O
ppca	O
the	O
limit	O
of	O
zero	O
output	O
noise	O
below	O
however	O
we	O
will	O
choose	O
a	O
non-gaussian	O
prior	B
phi	O
for	O
a	O
given	O
set	O
of	O
data	O
v	O
and	O
prior	B
ph	O
our	O
aim	O
is	O
to	O
find	O
a	O
for	O
i	O
i	O
d	O
data	O
the	O
log	O
likelihood	B
is	O
conveniently	O
written	O
in	O
terms	O
of	O
b	O
a	O
lb	O
n	O
log	O
det	O
log	O
pbvni	O
n	O
i	O
note	O
that	O
for	O
a	O
gaussian	B
prior	B
ph	O
e	O
the	O
log	O
likelihood	B
becomes	O
lb	O
n	O
log	O
det	O
n	O
btbvn	O
const	O
lb	O
n	O
aba	O
bab	O
where	O
d	O
dx	O
log	O
px	O
b	O
n	O
px	O
d	O
dx	O
px	O
which	O
is	O
invariant	O
with	O
respect	O
to	O
an	O
orthogonal	B
rotation	O
b	O
rb	O
with	O
rtr	O
i	O
this	O
means	O
that	O
for	O
a	O
gaussian	B
prior	B
ph	O
we	O
cannot	O
estimate	O
uniquely	O
the	O
mixing	B
matrix	B
to	O
break	O
this	O
rotational	O
invariance	O
we	O
therefore	O
need	O
to	O
use	O
a	O
non-gaussian	O
prior	B
assuming	O
we	O
have	O
a	O
non-gaussian	O
prior	B
ph	O
taking	O
the	O
derivative	O
w	O
r	O
t	O
bab	O
we	O
obtain	O
treatment	O
follows	O
that	O
presented	O
in	O
draft	O
march	O
exercises	O
a	O
simple	O
gradient	B
ascent	O
learning	B
rule	O
for	O
b	O
is	O
then	O
n	O
b	O
t	O
n	O
n	O
n	O
bnew	O
b	O
an	O
alternative	O
natural	B
gradient	B
that	O
approximates	O
a	O
newton	B
update	I
is	O
given	O
by	O
multiplying	O
the	O
gradient	B
by	O
btb	O
on	O
the	O
right	O
to	O
give	O
the	O
update	O
bnew	O
b	O
i	O
b	O
here	O
is	O
a	O
learning	B
rate	I
which	O
in	O
the	O
code	O
ica	B
m	O
we	O
nominally	O
set	O
to	O
a	O
natural	B
extension	O
is	O
to	O
consider	O
noise	O
on	O
the	O
outputs	O
for	O
which	O
an	O
em	B
algorithm	B
is	O
readily	O
available	O
however	O
in	O
the	O
limit	O
of	O
low	O
output	O
noise	O
the	O
em	B
formally	O
fails	O
an	O
effect	O
which	O
is	O
related	O
to	O
the	O
general	O
discussion	O
in	O
a	O
popular	O
alternative	O
estimation	O
method	O
is	O
and	O
can	O
be	O
related	O
to	O
an	O
iterative	O
maximum	B
likelihood	B
optimisation	B
procedure	O
ica	B
can	O
also	O
be	O
motivated	O
from	O
several	O
alternative	O
directions	O
including	O
information	O
we	O
refer	O
the	O
reader	O
to	O
for	O
an	O
in-depth	O
discussion	O
of	O
ica	B
and	O
related	O
extensions	O
code	O
fa	O
m	O
factor	B
analysis	B
demofa	O
m	O
demo	O
of	O
factor	B
analysis	B
ica	B
m	O
independent	B
components	I
analysis	B
demoica	O
m	O
demo	O
ica	B
exercises	O
exercise	O
factor	B
analysis	B
and	O
scaling	O
assume	O
that	O
a	O
h-factor	O
model	B
holds	O
for	O
x	O
now	O
consider	O
the	O
the	O
transformation	O
y	O
cx	O
where	O
c	O
is	O
a	O
non-singular	O
square	O
diagonal	O
matrix	B
show	O
that	O
factor	B
analysis	B
is	O
scale	O
invariant	O
i	O
e	O
that	O
the	O
h-factor	O
model	B
also	O
holds	O
for	O
y	O
with	O
the	O
factor	B
loadings	O
appropriately	O
scaled	O
how	O
must	O
the	O
specific	O
factors	O
be	O
scaled	O
exercise	O
for	O
the	O
constrained	B
factor	B
analysis	B
model	B
h	O
n	O
diag	O
n	O
h	O
n	O
i	O
derive	O
a	O
maximum	B
likelihood	B
em	B
algorithm	B
for	O
the	O
matrices	O
a	O
and	O
b	O
assuming	O
the	O
datapoints	O
xn	O
are	O
i	O
i	O
d	O
exercise	O
an	O
apparent	O
extension	O
of	O
fa	O
analysis	B
is	O
to	O
consider	O
a	O
correlated	O
prior	B
ph	O
n	O
h	O
show	O
that	O
provided	O
no	O
constraints	O
are	O
placed	O
on	O
the	O
factor	B
loading	I
matrix	B
f	O
using	O
a	O
correlated	O
prior	B
ph	O
is	O
an	O
equivalent	B
model	B
to	O
the	O
original	O
uncorrelated	O
fa	O
model	B
exercise	O
using	O
the	O
woodbury	O
identity	O
and	O
the	O
definition	O
of	O
d	O
in	O
equation	B
show	O
that	O
one	O
can	O
rewrite	O
d	O
a	O
b	O
x	O
as	O
d	O
i	O
ft	O
www	O
cis	O
hut	O
fiprojectsicafastica	O
draft	O
march	O
exercise	O
consider	O
an	O
ica	B
model	B
show	O
l	O
is	O
maximal	O
for	O
j	O
d	O
h	O
pyjx	O
py	O
xw	O
j	O
x	O
pyjx	O
w	O
n	O
yj	O
wt	O
j	O
i	O
with	O
pxi	O
w	O
wj	O
exercises	O
exercise	O
for	O
the	O
log	O
likelihood	B
function	B
l	O
n	O
d	O
log	O
i	O
i	O
h	O
log	O
h	O
for	O
the	O
above	O
model	B
derive	O
an	O
em	B
algorithm	B
for	O
a	O
set	O
of	O
i	O
i	O
d	O
data	O
yn	O
and	O
show	O
that	O
the	O
required	O
statistics	O
for	O
the	O
m-step	B
are	O
pxynw	O
show	O
that	O
for	O
a	O
non-gaussian	O
prior	B
pxi	O
the	O
posterior	B
pxy	O
w	O
is	O
non-factorised	O
non-gaussian	O
and	O
generally	O
intractable	O
normalisation	B
constant	I
cannot	O
be	O
computed	O
efficiently	O
show	O
that	O
in	O
the	O
limit	O
the	O
em	B
algorithm	B
fails	O
draft	O
march	O
chapter	O
latent	B
ability	O
models	O
the	O
rasch	B
model	B
consider	O
an	O
exam	O
in	O
which	O
student	O
s	O
answers	O
question	O
q	O
either	O
correctly	O
xqs	O
or	O
incorrectly	O
xqs	O
for	O
a	O
set	O
of	O
n	O
students	O
and	O
q	O
questions	O
the	O
performance	B
of	O
all	O
students	O
is	O
given	O
in	O
the	O
q	O
n	O
binary	O
matrix	B
x	O
based	O
on	O
this	O
data	O
alone	O
we	O
wish	O
to	O
evaluate	O
the	O
ability	O
of	O
each	O
student	O
one	O
approach	B
is	O
to	O
define	O
the	O
ability	O
as	O
as	O
the	O
fraction	O
of	O
questions	O
student	O
s	O
answered	O
correctly	O
a	O
more	O
subtle	O
analysis	B
is	O
to	O
accept	O
that	O
some	O
questions	O
are	O
more	O
difficult	O
than	O
others	O
so	O
that	O
a	O
student	O
who	O
answered	O
difficult	O
questions	O
should	O
be	O
awarded	O
more	O
highly	O
than	O
a	O
student	O
who	O
answered	O
the	O
same	O
number	O
of	O
easy	O
questions	O
a	O
priori	O
we	O
do	O
not	O
know	O
which	O
are	O
the	O
difficult	O
questions	O
and	O
this	O
needs	O
to	O
be	O
estimated	O
based	O
on	O
x	O
to	O
account	O
for	O
inherent	O
differences	O
in	O
question	O
difficulty	O
we	O
may	O
model	B
the	O
probability	O
that	O
a	O
student	O
s	O
gets	O
a	O
question	O
q	O
correct	O
based	O
on	O
the	O
student	O
s	O
latent	B
ability	O
s	O
and	O
the	O
latent	B
difficulty	O
of	O
the	O
question	O
q	O
a	O
simple	O
generative	B
model	B
of	O
the	O
response	O
is	O
pxqs	O
s	O
q	O
where	O
e	O
x	O
under	O
this	O
model	B
the	O
higher	O
the	O
latent	B
ability	O
is	O
above	O
the	O
latent	B
difficulty	O
of	O
the	O
question	O
the	O
more	O
likely	O
it	O
is	O
that	O
the	O
student	O
will	O
answer	O
the	O
question	O
correctly	O
maximum	B
likelihood	B
training	B
making	O
the	O
i	O
i	O
d	O
assumption	O
the	O
likelihood	B
of	O
the	O
data	O
x	O
under	O
this	O
model	B
is	O
px	O
l	O
log	O
px	O
the	O
log	O
likelihood	B
is	O
s	O
qxqs	O
s	O
xqs	O
xqs	O
log	O
s	O
q	O
xqs	O
log	O
s	O
q	O
qs	O
q	O
q	O
xqs	O
s	O
s	O
figure	O
the	O
rasch	B
model	B
for	O
analysing	O
questions	O
each	O
element	O
of	O
the	O
binary	O
matrix	B
x	O
with	O
xqs	O
if	O
student	O
s	O
gets	O
question	O
q	O
correct	O
is	O
generated	O
using	O
the	O
latent	B
ability	O
of	O
the	O
student	O
s	O
and	O
the	O
latent	B
difficulty	O
of	O
the	O
question	O
q	O
with	O
derivatives	O
competition	B
models	I
s	O
q	O
l	O
q	O
s	O
q	O
l	O
s	O
a	O
simple	O
way	O
to	O
learn	O
the	O
parameters	O
is	O
to	O
use	O
gradient	B
ascent	O
see	O
demorasch	O
m	O
with	O
extensions	O
to	O
newton	O
methods	O
being	O
straightforward	O
the	O
generalisation	B
to	O
more	O
than	O
two	O
responses	O
xqs	O
can	O
be	O
achieved	O
using	O
a	O
softmax-style	O
function	B
more	O
generally	O
the	O
rasch	B
model	B
is	O
an	O
example	O
of	O
an	O
item	B
response	I
theory	I
a	O
subject	O
dealing	O
with	O
the	O
analysis	B
of	O
missing	B
data	I
assuming	O
the	O
data	O
is	O
missing	B
at	I
random	I
missing	B
data	I
can	O
be	O
treated	O
by	O
computing	O
the	O
likelihood	B
of	O
only	O
the	O
observed	O
elements	O
of	O
x	O
in	O
rasch	B
m	O
missing	B
data	I
is	O
assumed	O
to	O
be	O
coded	O
as	O
nan	O
so	O
that	O
the	O
likelihood	B
and	O
gradients	O
are	O
straightforward	O
to	O
compute	O
based	O
on	O
summing	O
only	O
over	O
terms	O
containing	O
non	O
nan	O
entries	O
example	O
we	O
display	O
an	O
example	O
of	O
the	O
use	O
of	O
the	O
rasch	B
model	B
in	O
estimating	O
the	O
latent	B
abilities	O
of	O
students	O
based	O
on	O
a	O
set	O
of	O
questions	O
based	O
on	O
using	O
the	O
number	O
of	O
questions	O
each	O
student	O
answered	O
correctly	O
the	O
best	O
students	O
are	O
from	O
first	O
alternatively	O
ranking	O
students	O
according	O
to	O
the	O
latent	B
ability	O
gives	O
this	O
differs	O
slightly	O
in	O
this	O
case	O
from	O
the	O
number-correct	O
ranking	O
since	O
the	O
rasch	B
model	B
takes	O
into	O
account	O
the	O
fact	O
that	O
some	O
students	O
answered	O
difficult	O
questions	O
correctly	O
for	O
example	O
student	O
answered	O
some	O
difficult	O
questions	O
correctly	O
bayesian	B
rasch	B
models	O
the	O
rasch	B
model	B
will	O
potentially	O
overfit	O
the	O
data	O
especially	O
when	O
there	O
is	O
only	O
a	O
small	O
amount	O
of	O
data	O
for	O
this	O
case	O
a	O
natural	B
extension	O
is	O
to	O
use	O
a	O
bayesian	B
technique	O
placing	O
independent	O
priors	O
on	O
the	O
ability	O
and	O
question	O
difficulty	O
so	O
that	O
the	O
posterior	B
ability	O
and	O
question	O
difficulty	O
is	O
given	O
by	O
p	O
px	O
natural	B
priors	O
are	O
p	O
s	O
s	O
n	O
p	O
q	O
n	O
q	O
where	O
and	O
are	O
hyperparameters	O
that	O
can	O
be	O
learned	O
by	O
maximising	O
px	O
even	O
using	O
gaussian	B
priors	O
the	O
posterior	B
distribution	B
p	O
is	O
not	O
of	O
a	O
standard	O
form	O
and	O
approximations	O
are	O
required	O
in	O
this	O
case	O
however	O
the	O
posterior	B
is	O
log	O
concave	O
so	O
that	O
approximation	B
methods	O
based	O
on	O
variational	O
or	O
laplace	B
techniques	O
are	O
potentially	O
adequate	O
or	O
alternatively	O
one	O
may	O
use	O
sampling	B
approximations	O
competition	B
models	I
bradly-terry-luce	B
model	B
the	O
bradly-terry-luce	B
model	B
assesses	O
the	O
ability	O
of	O
players	O
based	O
on	O
one-on-one	O
matches	O
here	O
we	O
describe	O
games	O
in	O
which	O
only	O
winlose	O
outcomes	O
arise	O
leaving	O
aside	O
the	O
complicating	O
possibility	O
of	O
draws	O
for	O
this	O
draft	O
march	O
competition	B
models	I
figure	O
rasch	B
model	B
the	O
data	O
of	O
correct	O
answers	O
and	O
incorrect	O
answers	O
the	O
fraction	O
of	O
questions	O
each	O
student	O
answered	O
the	O
estimated	O
latent	B
difficulty	O
of	O
each	O
question	O
correctly	O
the	O
estimated	O
latent	B
ability	O
winlose	O
scenario	O
the	O
btl	O
model	B
is	O
a	O
straightforward	O
modification	O
of	O
the	O
rasch	B
model	B
so	O
that	O
for	O
latent	B
ability	O
i	O
of	O
player	O
i	O
and	O
latent	B
ability	O
j	O
of	O
player	O
j	O
the	O
probability	O
that	O
i	O
beats	O
j	O
is	O
given	O
by	O
where	O
i	O
j	O
stands	O
for	O
player	O
i	O
beats	O
player	O
j	O
based	O
on	O
a	O
matrix	B
of	O
games	O
data	O
x	O
with	O
pi	O
j	O
i	O
j	O
if	O
i	O
j	O
in	O
game	O
n	O
otherwise	O
xn	O
ij	O
px	O
where	O
mij	O
the	O
likelihood	B
of	O
the	O
model	B
is	O
given	O
by	O
i	O
jxn	O
n	O
ij	O
ij	O
ij	O
i	O
jmij	O
or	O
a	O
bayesian	B
technique	O
can	O
then	O
proceed	O
as	O
for	O
the	O
rasch	B
model	B
n	O
xn	O
ij	O
is	O
the	O
number	O
of	O
times	O
player	O
i	O
beat	O
player	O
j	O
training	B
using	O
maximum	B
likelihood	B
for	O
the	O
case	O
of	O
only	O
two	O
objects	O
interacting	O
these	O
models	O
are	O
called	O
pairwise	B
comparison	I
models	I
thurstone	O
in	O
the	O
s	O
applied	O
such	O
models	O
to	O
a	O
wide	O
range	O
of	O
data	O
and	O
the	O
bradley-terry-luce	O
model	B
is	O
in	O
fact	O
a	O
special	O
case	O
of	O
his	O
example	O
an	O
example	O
application	O
of	O
the	O
btl	O
model	B
is	O
given	O
in	O
in	O
which	O
a	O
matrix	B
x	O
containing	O
the	O
number	O
of	O
times	O
that	O
competitor	O
i	O
beat	O
competitor	O
j	O
is	O
given	O
the	O
matrix	B
entries	O
x	O
were	O
drawn	O
from	O
a	O
btl	O
model	B
based	O
on	O
true	O
abilities	O
using	O
x	O
alone	O
the	O
maximum	B
likelihood	B
estimate	O
of	O
these	O
latent	B
abilities	O
is	O
in	O
close	O
agreement	O
with	O
the	O
true	O
abilities	O
draft	O
march	O
of	O
questions	O
ability	O
competition	B
models	I
the	O
data	O
x	O
with	O
xij	O
being	O
the	O
number	O
of	O
times	O
that	O
competitor	O
i	O
beat	O
figure	O
btl	O
model	B
competitor	O
j	O
the	O
true	O
versus	O
estimated	O
ability	O
even	O
though	O
the	O
data	O
is	O
quite	O
sparse	B
a	O
reasonable	O
estimate	O
of	O
the	O
latent	B
ability	O
of	O
each	O
competitor	O
is	O
found	O
elo	B
ranking	O
model	B
the	O
elo	B
system	O
used	O
in	O
chess	O
ranking	O
is	O
closely	O
related	O
to	O
the	O
btl	O
model	B
above	O
though	O
there	O
is	O
the	O
added	O
complication	O
of	O
the	O
possibility	O
of	O
draws	O
in	O
addition	O
the	O
elo	B
system	O
takes	O
into	O
account	O
a	O
measure	O
of	O
the	O
variability	O
in	O
performance	B
for	O
a	O
given	O
ability	O
i	O
the	O
actual	O
performance	B
i	O
of	O
player	O
i	O
in	O
a	O
game	O
is	O
given	O
by	O
i	O
i	O
where	O
n	O
variability	O
in	O
the	O
performance	B
more	O
formally	O
the	O
elo	B
model	B
modifies	O
the	O
btl	O
model	B
to	O
give	O
the	O
variance	B
is	O
fixed	O
across	O
all	O
players	O
and	O
thus	O
takes	O
into	O
account	O
intrinsic	O
px	O
px	O
p	O
n	O
where	O
px	O
is	O
given	O
by	O
equation	B
on	O
replacing	O
with	O
glicko	B
and	O
trueskill	B
and	O
are	O
essentially	O
bayesian	B
versions	O
of	O
the	O
elo	B
model	B
with	O
the	O
refinement	O
that	O
the	O
latent	B
ability	O
is	O
modelled	O
not	O
by	O
a	O
single	O
number	O
but	O
by	O
a	O
gaussian	B
distribution	B
this	O
can	O
capture	O
the	O
fact	O
that	O
a	O
player	O
may	O
be	O
consistently	O
reasonable	O
high	O
i	O
and	O
low	O
erratic	O
genius	O
i	O
but	O
with	O
large	O
i	O
the	O
parameters	O
of	O
the	O
model	B
are	O
then	O
i	O
or	O
an	O
for	O
a	O
set	O
of	O
s	O
players	O
the	O
interaction	O
model	B
px	O
is	O
as	O
for	O
the	O
winlose	O
elo	B
model	B
equation	B
the	O
likelihood	B
for	O
the	O
model	B
given	O
the	O
parameters	O
is	O
i	O
p	O
i	O
i	O
n	O
i	O
i	O
i	O
i	O
i	O
px	O
px	O
this	O
integral	O
is	O
formally	O
intractable	O
and	O
numerical	B
approximations	O
are	O
required	O
in	O
this	O
context	O
expectation	B
propagation	B
has	O
proven	O
to	O
be	O
a	O
useful	O
the	O
trueskill	B
system	O
is	O
used	O
for	O
example	O
to	O
assess	O
the	O
abilities	O
of	O
players	O
in	O
online	B
gaming	O
also	O
taking	O
into	O
account	O
the	O
abilities	O
of	O
teams	O
of	O
individuals	O
in	O
tournaments	O
a	O
temporal	O
extension	O
has	O
recently	O
been	O
used	O
to	O
reevaluate	O
the	O
change	O
in	O
ability	O
of	O
chess	O
players	O
with	O
draft	O
march	O
competitorcompetitor	O
abilityestimated	O
ability	O
exercises	O
code	O
rasch	B
m	O
rasch	B
model	B
training	B
demorasch	O
m	O
demo	O
for	O
the	O
rasch	B
model	B
exercises	O
exercise	O
bronco	O
bronco	O
mat	O
contains	O
information	O
about	O
a	O
bucking	O
bronco	O
competition	O
there	O
are	O
competitors	O
and	O
bucking	O
broncos	O
a	O
competitor	O
j	O
attempts	O
to	O
stay	O
on	O
a	O
bucking	O
bronco	O
i	O
for	O
a	O
minute	O
if	O
the	O
competitor	O
succeeds	O
the	O
entry	O
xij	O
is	O
otherwise	O
each	O
competitor	O
gets	O
to	O
ride	O
three	O
bucking	O
broncos	O
only	O
missing	B
data	I
is	O
coded	O
as	O
nan	O
having	O
viewed	O
all	O
the	O
amateurs	O
desperate	O
dan	O
enters	O
the	O
competition	O
and	O
bribes	O
the	O
organisers	O
into	O
letting	O
him	O
avoid	O
having	O
to	O
ride	O
the	O
difficult	O
broncos	O
based	O
on	O
using	O
a	O
rasch	B
model	B
which	O
are	O
the	O
top	O
most	O
difficult	O
broncos	O
in	O
order	O
of	O
the	O
most	O
difficult	O
first	O
exercise	O
training	B
show	O
that	O
the	O
log	O
likelihood	B
for	O
the	O
bradly-terry-luce	B
model	B
is	O
given	O
by	O
xij	O
log	O
i	O
j	O
l	O
ij	O
where	O
xij	O
is	O
the	O
number	O
of	O
times	O
that	O
player	O
i	O
beats	O
player	O
j	O
in	O
a	O
set	O
of	O
games	O
compute	O
the	O
gradient	B
of	O
l	O
compute	O
the	O
hessian	B
of	O
the	O
btl	O
model	B
and	O
verify	O
that	O
it	O
is	O
negative	O
semidefinite	O
exercise	O
reine	O
program	O
a	O
simple	O
gradient	B
ascent	O
routine	O
to	O
learn	O
the	O
latent	B
abilities	O
of	O
competitors	O
based	O
on	O
a	O
series	O
of	O
winlose	O
outcomes	O
in	O
a	O
modified	O
form	O
of	O
swiss	O
cow	O
fighting	O
a	O
set	O
of	O
cows	O
compete	O
by	O
pushing	O
each	O
other	O
until	O
submission	O
at	O
the	O
end	O
of	O
the	O
competition	O
one	O
cow	O
is	O
deemed	O
to	O
be	O
la	O
reine	O
based	O
on	O
the	O
data	O
in	O
btl	O
mat	O
which	O
xij	O
contains	O
the	O
number	O
of	O
times	O
cow	O
i	O
beat	O
cow	O
j	O
fit	O
a	O
btl	O
model	B
and	O
return	O
a	O
ranked	O
list	O
of	O
the	O
top	O
ten	O
best	O
fighting	O
cows	O
la	O
reine	O
first	O
exercise	O
an	O
extension	O
of	O
the	O
btl	O
model	B
is	O
to	O
consider	O
additional	O
factors	O
that	O
describe	O
the	O
state	O
of	O
the	O
competitors	O
when	O
they	O
play	O
for	O
example	O
we	O
have	O
a	O
set	O
of	O
s	O
football	O
teams	O
and	O
a	O
set	O
of	O
matrices	O
ij	O
if	O
team	O
i	O
beat	O
team	O
j	O
in	O
match	O
n	O
in	O
addition	O
we	O
have	O
for	O
each	O
match	O
and	O
team	O
xn	O
with	O
x	O
n	O
i	O
that	O
describes	O
the	O
team	O
for	O
example	O
for	O
the	O
team	O
i	O
a	O
vector	O
of	O
binary	O
factors	O
f	O
n	O
united	O
the	O
factor	B
if	O
bozo	O
is	O
playing	O
if	O
not	O
it	O
is	O
suggested	O
that	O
the	O
ability	O
of	O
team	O
i	O
in	O
game	O
n	O
is	O
measured	O
by	O
n	O
i	O
di	O
whif	O
n	O
hi	O
hi	O
if	O
factor	B
h	O
is	O
present	O
in	O
team	O
i	O
in	O
game	O
n	O
di	O
is	O
a	O
default	O
latent	B
ability	O
of	O
the	O
team	O
which	O
where	O
f	O
n	O
is	O
assumed	O
constant	O
across	O
all	O
games	O
we	O
have	O
such	O
a	O
set	O
of	O
factors	O
for	O
each	O
match	O
giving	O
f	O
n	O
hi	O
using	O
the	O
above	O
definition	O
of	O
the	O
latent	B
ability	O
in	O
the	O
btl	O
model	B
our	O
interest	O
is	O
to	O
find	O
the	O
weights	O
w	O
and	O
abilities	O
d	O
that	O
best	O
predict	O
the	O
ability	O
of	O
the	O
team	O
given	O
that	O
we	O
have	O
a	O
set	O
of	O
historical	O
plays	O
fn	O
n	O
n	O
write	O
down	O
the	O
likelihood	B
for	O
the	O
btl	O
model	B
as	O
a	O
function	B
of	O
the	O
set	O
of	O
all	O
team	O
weights	O
w	O
d	O
draft	O
march	O
exercises	O
compute	O
the	O
gradient	B
of	O
the	O
log	O
likelihood	B
of	O
this	O
model	B
explain	O
how	O
this	O
model	B
can	O
be	O
used	O
to	O
assess	O
the	O
importance	B
of	O
bozo	O
s	O
contribution	O
to	O
madchester	O
united	O
s	O
ability	O
given	O
learned	O
w	O
d	O
and	O
the	O
knowledge	O
that	O
madchester	O
united	O
will	O
play	O
chelski	O
tomorrow	O
explain	O
how	O
given	O
the	O
list	O
of	O
factors	O
f	O
for	O
chelski	O
includes	O
issues	O
such	O
as	O
who	O
will	O
be	O
playing	O
in	O
the	O
team	O
one	O
can	O
select	O
the	O
best	O
madchester	O
united	O
team	O
to	O
maximise	O
the	O
probability	O
of	O
winning	O
the	O
game	O
draft	O
march	O
part	O
iv	O
dynamical	O
models	O
chapter	O
discrete-state	O
markov	O
models	O
markov	O
models	O
time-series	O
are	O
datasets	O
for	O
which	O
the	O
constituent	O
datapoints	O
can	O
be	O
naturally	O
ordered	O
this	O
order	O
often	O
corresponds	O
to	O
an	O
underlying	O
single	O
physical	O
dimension	O
typically	O
time	O
though	O
any	O
other	O
single	O
dimension	O
may	O
be	O
used	O
the	O
time-series	O
models	O
we	O
consider	O
are	O
probability	O
models	O
over	O
a	O
collection	O
of	O
random	O
variables	O
vt	O
with	O
individual	O
variables	O
vt	O
indexed	O
by	O
a	O
time	O
index	O
t	O
these	O
indices	O
are	O
elements	O
of	O
the	O
index	O
set	O
t	O
for	O
nonnegative	O
indices	O
t	O
n	O
the	O
model	B
is	O
a	O
discrete-time	O
process	O
continuoustime	O
processes	O
t	O
r	O
are	O
natural	B
in	O
particular	O
application	O
domains	O
yet	O
require	O
additional	O
notation	O
and	O
concepts	O
we	O
therefore	O
focus	O
exclusively	O
on	O
discrete-time	O
models	O
a	O
probabilistic	B
time-series	O
model	B
requires	O
a	O
specification	O
of	O
the	O
joint	B
distribution	B
vt	O
for	O
the	O
case	O
in	O
which	O
the	O
observed	O
data	O
vt	O
is	O
discrete	B
the	O
joint	B
probability	O
table	O
for	O
vt	O
has	O
exponentially	O
many	O
entries	O
we	O
cannot	O
expect	O
to	O
independently	O
specify	O
all	O
the	O
exponentially	O
many	O
entries	O
and	O
are	O
therefore	O
forced	O
to	O
make	O
simplified	O
models	O
under	O
which	O
these	O
entries	O
can	O
be	O
parameterised	O
in	O
a	O
lower	O
dimensional	O
manner	O
such	O
simplifications	O
are	O
at	O
the	O
heart	O
of	O
time-series	O
modelling	B
and	O
we	O
will	O
discuss	O
some	O
classical	O
models	O
in	O
the	O
following	O
sections	O
definition	O
notation	O
xab	O
xa	O
xb	O
with	O
xab	O
xa	O
for	O
b	O
a	O
for	O
timeseries	O
data	O
vt	O
we	O
need	O
a	O
model	B
for	O
consistency	O
with	O
the	O
causal	B
nature	O
of	O
time	O
it	O
is	O
meaningful	O
to	O
consider	O
the	O
decomposition	B
with	O
the	O
convention	O
for	O
t	O
it	O
is	O
often	O
natural	B
to	O
assume	O
that	O
the	O
influence	O
of	O
the	O
immediate	O
past	O
is	O
more	O
relevant	O
than	O
the	O
remote	O
past	O
and	O
in	O
markov	O
models	O
only	O
a	O
limited	O
number	O
of	O
previous	O
observations	O
are	O
required	O
to	O
predict	O
the	O
future	O
definition	O
chain	B
a	O
markov	B
chain	B
defined	O
on	O
either	O
discrete	B
or	O
continuous	B
variables	O
is	O
one	O
in	O
which	O
the	O
following	O
conditional	B
independence	B
assumption	O
holds	O
vt	O
pvtvt	O
l	O
vt	O
markov	O
models	O
figure	O
first	O
order	O
markov	B
chain	B
second	O
order	O
markov	B
chain	B
where	O
l	O
is	O
the	O
order	O
of	O
the	O
markov	B
chain	B
and	O
vt	O
for	O
t	O
for	O
a	O
first	O
order	O
markov	B
chain	B
pvtvt	O
for	O
a	O
stationary	B
markov	B
chain	B
the	O
transitions	O
pvt	O
wise	O
the	O
chain	B
is	O
non-stationary	B
pvt	O
s	O
s	O
t	O
s	O
s	O
are	O
time-independent	O
other	O
equilibrium	O
and	O
stationary	B
distribution	B
of	O
a	O
markov	B
chain	B
the	O
stationary	B
distribution	B
p	O
of	O
a	O
markov	B
chain	B
with	O
transition	B
matrix	B
m	O
is	O
defined	O
by	O
the	O
condition	O
p	O
pxt	O
ixt	O
j	O
mij	O
j	O
p	O
in	O
matrix	B
notation	O
this	O
can	O
be	O
written	O
as	O
the	O
vector	O
equation	B
p	O
mp	O
so	O
that	O
the	O
stationary	B
distribution	B
is	O
proportional	O
to	O
the	O
eigenvector	O
with	O
unit	O
eigenvalue	O
of	O
the	O
transition	B
matrix	B
note	O
that	O
there	O
may	O
be	O
more	O
than	O
one	O
stationary	B
distribution	B
see	O
and	O
given	O
a	O
state	O
we	O
can	O
iteratively	O
draw	O
samples	O
xt	O
from	O
the	O
markov	B
chain	B
drawing	O
a	O
sample	O
from	O
and	O
then	O
from	O
etc	O
as	O
we	O
repeatedly	O
sample	O
a	O
new	O
state	O
from	O
the	O
chain	B
the	O
distribution	B
at	O
time	O
t	O
for	O
an	O
initial	O
distribution	B
is	O
pt	O
if	O
for	O
t	O
p	O
is	O
independent	O
of	O
the	O
initial	O
distribution	B
then	O
p	O
is	O
called	O
the	O
equilibrium	O
distribution	B
of	O
the	O
chain	B
see	O
for	O
an	O
example	O
of	O
a	O
markov	B
chain	B
which	O
does	O
not	O
have	O
an	O
equilibrium	O
distribution	B
example	O
despite	O
their	O
apparent	O
simplicity	O
markov	O
chains	O
have	O
been	O
put	O
to	O
interesting	O
use	O
in	O
information	B
retrieval	I
and	O
search-engines	O
define	O
the	O
matrix	B
aij	O
if	O
website	B
j	O
has	O
a	O
hyperlink	O
to	O
website	B
i	O
otherwise	O
from	O
this	O
we	O
can	O
define	O
a	O
markov	O
transition	B
matrix	B
with	O
elements	O
mij	O
i	O
aij	O
the	O
equilibrium	O
distribution	B
of	O
this	O
markov	B
chain	B
has	O
the	O
interpretation	O
if	O
follow	O
links	O
at	O
random	O
jumping	O
from	O
website	B
to	O
website	B
the	O
equilibrium	O
distribution	B
component	O
p	O
is	O
the	O
relative	O
number	O
of	O
times	O
we	O
will	O
visit	O
website	B
i	O
this	O
has	O
a	O
natural	B
interpretation	O
as	O
the	O
importance	B
of	O
website	B
i	O
if	O
a	O
website	B
draft	O
march	O
markov	O
models	O
is	O
isolated	O
in	O
the	O
web	O
it	O
will	O
be	O
visited	O
infrequently	O
by	O
random	O
hopping	O
if	O
a	O
website	B
is	O
linked	O
by	O
many	O
others	O
it	O
will	O
be	O
visited	O
more	O
frequently	O
a	O
crude	O
search	B
engine	I
works	O
then	O
as	O
follows	O
for	O
each	O
website	B
i	O
a	O
list	O
of	O
words	O
associated	O
with	O
that	O
website	B
is	O
collected	O
after	O
doing	O
this	O
for	O
all	O
websites	O
one	O
can	O
make	O
an	O
inverse	O
list	O
of	O
which	O
websites	O
contain	O
word	O
w	O
when	O
a	O
user	O
searches	O
for	O
word	O
w	O
the	O
list	O
of	O
websites	O
that	O
that	O
word	O
is	O
then	O
returned	O
ranked	O
according	O
to	O
the	O
importance	B
of	O
the	O
site	O
defined	O
by	O
the	O
equilibrium	O
distribution	B
this	O
is	O
a	O
crude	O
summary	O
as	O
how	O
early	O
search	O
engines	O
worked	O
infolab	O
stanford	O
edu	O
backrubgoogle	O
html	O
figure	O
a	O
state	O
transition	O
diagram	O
for	O
a	O
three	O
state	O
markov	B
chain	B
note	O
that	O
a	O
state	O
transition	O
diagram	O
is	O
not	O
a	O
graphical	O
model	B
it	O
simply	O
displays	O
the	O
non-zero	O
entries	O
of	O
the	O
transition	B
matrix	B
pij	O
the	O
absence	O
of	O
link	O
from	O
j	O
to	O
i	O
indicates	O
that	O
pij	O
fitting	O
markov	O
models	O
given	O
a	O
sequence	O
fitting	O
a	O
stationary	B
markov	B
chain	B
by	O
maximum	B
likelihood	B
corresponds	O
to	O
setting	O
the	O
transitions	O
by	O
counting	B
the	O
number	O
of	O
observed	O
transitions	O
in	O
the	O
sequence	O
pv	O
iv	O
j	O
i	O
i	O
vt	O
j	O
to	O
show	O
this	O
for	O
convenience	O
we	O
write	O
pv	O
iv	O
j	O
ij	O
so	O
that	O
the	O
likelihood	B
is	O
is	O
known	O
ivtivt	O
ij	O
vtvt	O
ij	O
ij	O
l	O
taking	O
logs	O
and	O
adding	O
the	O
lagrange	O
constraint	O
for	O
the	O
normalisation	B
i	O
i	O
vt	O
j	O
log	O
ij	O
j	O
ij	O
j	O
i	O
ij	O
differentiating	O
with	O
respect	O
to	O
ij	O
and	O
equating	O
to	O
zero	O
we	O
immediately	O
arrive	O
at	O
the	O
intuitive	O
setting	O
equation	B
for	O
a	O
set	O
of	O
timeseries	O
vn	O
n	O
n	O
the	O
transition	O
is	O
given	O
by	O
counting	B
all	O
transitions	O
across	O
time	O
and	O
datapoints	O
the	O
maximum	B
likelihood	B
setting	O
for	O
the	O
initial	O
first	O
timestep	O
distribution	B
is	O
i	O
bayesian	B
fitting	O
i	O
i	O
n	O
for	O
simplicity	O
we	O
assume	O
a	O
factorised	B
prior	B
on	O
the	O
transition	O
p	O
a	O
convenient	O
choice	O
for	O
each	O
conditional	B
transition	O
is	O
a	O
dirichlet	B
distribution	B
with	O
hyperparameters	O
uj	O
p	O
j	O
p	O
where	O
uj	O
p	O
draft	O
march	O
since	O
this	O
is	O
conjugate	B
to	O
the	O
categorical	B
transition	O
and	O
uj	O
ivtivt	O
ij	O
uij	O
ij	O
t	O
ij	O
j	O
i	O
i	O
vt	O
j	O
being	O
the	O
number	O
of	O
j	O
i	O
transitions	O
in	O
the	O
dataset	O
markov	O
models	O
h	O
figure	O
mixture	B
of	O
first	O
order	O
markov	O
chains	O
the	O
discrete	B
hidden	B
variable	I
domh	O
h	O
indexes	O
the	O
markov	B
chain	B
pvtvt	O
h	O
such	O
models	O
can	O
be	O
useful	O
as	O
simple	O
sequence	O
clustering	B
tools	O
mixture	B
of	O
markov	O
models	O
given	O
a	O
set	O
of	O
sequences	B
v	O
n	O
n	O
how	O
might	O
we	O
cluster	O
them	O
to	O
keep	O
the	O
notation	O
less	O
cluttered	O
we	O
assume	O
that	O
all	O
sequences	B
are	O
of	O
the	O
same	O
length	O
t	O
with	O
the	O
extension	O
to	O
differing	O
lengths	O
being	O
straightforward	O
one	O
simple	O
approach	B
is	O
to	O
fit	O
a	O
mixture	B
of	O
markov	O
models	O
assuming	O
the	O
data	O
we	O
define	O
a	O
mixture	B
model	B
for	O
a	O
single	O
datapoint	O
here	O
we	O
assume	O
each	O
n	O
pvn	O
component	O
model	B
is	O
first	O
order	O
markov	O
is	O
i	O
i	O
d	O
pv	O
ph	O
pvtvt	O
h	O
the	O
graphical	O
model	B
is	O
depicted	O
in	O
clustering	B
can	O
then	O
be	O
achieved	O
by	O
finding	O
the	O
maximum	B
likelihood	B
parameters	O
ph	O
pvtvt	O
h	O
and	O
subsequently	O
assigning	O
the	O
clusters	O
according	O
to	O
phvn	O
below	O
we	O
discuss	O
the	O
application	O
of	O
the	O
em	B
algorithm	B
to	O
this	O
model	B
to	O
learn	O
the	O
maximum	B
likelihood	B
parameters	O
em	B
algorithm	B
under	O
the	O
i	O
i	O
d	O
data	O
assumption	O
the	O
log	O
likelihood	B
is	O
log	O
ph	O
pvn	O
t	O
h	O
for	O
the	O
m-step	B
our	O
task	O
is	O
to	O
maximise	O
the	O
energy	B
pvn	O
the	O
contribution	O
to	O
the	O
energy	B
from	O
the	O
parameter	B
ph	O
is	O
t	O
pvtvt	O
log	O
pv	O
e	O
by	O
defining	O
poldh	O
kl	O
poldhvn	O
poldhph	O
pnewh	O
poldhvn	O
one	O
can	O
view	O
maximising	O
as	O
equivalent	B
to	O
minimising	O
so	O
that	O
the	O
optimal	O
choice	O
from	O
the	O
m-step	B
is	O
to	O
set	O
pnew	O
pold	O
namely	O
for	O
those	O
less	O
comfortable	O
with	O
this	O
argument	O
a	O
direct	O
maximisation	B
including	O
a	O
lagrange	O
term	O
to	O
ensure	O
normalisation	B
of	O
ph	O
can	O
be	O
used	O
to	O
derive	O
the	O
same	O
result	O
draft	O
march	O
markov	O
models	O
similarly	O
the	O
m-step	B
for	O
pvtvt	O
h	O
is	O
pnewvt	O
ivt	O
j	O
h	O
k	O
poldh	O
kvn	O
t	O
i	O
t	O
i	O
the	O
initial	O
term	O
is	O
updated	O
using	O
ih	O
k	O
the	O
e-step	B
sets	O
i	O
poldh	O
kvn	O
poldhvn	O
phpvn	O
ph	O
pvn	O
t	O
t	O
h	O
given	O
an	O
initialisation	O
the	O
em	B
algorithm	B
then	O
iterates	O
and	O
until	O
convergence	O
for	O
long	O
sequences	B
explicitly	O
computing	O
the	O
product	O
of	O
many	O
terms	O
may	O
lead	O
to	O
numerical	B
underflow	O
issues	O
in	O
practice	O
it	O
is	O
therefore	O
best	O
to	O
work	O
with	O
logs	O
log	O
poldhvn	O
log	O
ph	O
log	O
pvn	O
t	O
t	O
h	O
const	O
in	O
this	O
way	O
any	O
large	O
constants	O
common	O
to	O
all	O
h	O
can	O
be	O
removed	O
and	O
the	O
distribution	B
may	O
be	O
computed	O
accurately	O
see	O
mixmarkov	O
m	O
example	O
clustering	B
consider	O
the	O
fictitious	O
gene	O
sequences	B
below	O
presented	O
in	O
an	O
arbitrarily	O
chosen	O
order	O
each	O
sequence	O
consists	O
of	O
symbols	O
from	O
the	O
set	O
c	O
g	O
t	O
the	O
task	O
is	O
to	O
try	O
to	O
cluster	O
these	O
sequences	B
into	O
two	O
groups	O
based	O
on	O
the	O
biologically	O
unrealistic	O
assumption	O
that	O
gene	O
sequences	B
in	O
the	O
same	O
cluster	O
follow	O
a	O
stationary	B
markov	B
chain	B
cataggcattctatgtgctg	O
gtgcctggacctgaaaagcc	O
gttggtcagcacacggactg	O
taagtgtcctctgctcctaa	O
gccaagcagggtctcaactt	O
ccagttacggacgccgaaag	O
cggccgcgcctccgggaacg	O
cctcccctcccctttcctgc	O
caccatcacccttgctaagg	O
catggactgctccacaaagg	O
tggaaccttaaaaaaaaaaa	O
aaagtgctctgaaaactcac	O
cactacggctacctgggcaa	O
aaagaactcccctccctgcc	O
aaaaaaacgaaaaacctaag	O
gtctcctgccctctctgaac	O
acatgaactacatagtataa	O
cggtccgtccgaggcactc	O
caaatgcctcacgcgtctca	O
gcgtaaaaaaagtcctgggt	O
a	O
simple	O
approach	B
is	O
to	O
assume	O
that	O
the	O
sequences	B
are	O
generated	O
from	O
a	O
two-component	O
h	O
mixture	B
of	O
markov	O
models	O
and	O
train	O
the	O
model	B
using	O
maximum	B
likelihood	B
the	O
likelihood	B
has	O
local	B
optima	O
so	O
that	O
the	O
procedure	O
needs	O
to	O
be	O
run	O
several	O
times	O
and	O
the	O
solution	O
with	O
the	O
highest	O
likelihood	B
chosen	O
one	O
can	O
if	O
this	O
posterior	B
probability	O
is	O
greater	O
than	O
then	O
assign	O
each	O
of	O
the	O
sequences	B
by	O
examining	O
ph	O
we	O
assign	O
it	O
to	O
class	O
otherwise	O
to	O
class	O
using	O
this	O
procedure	O
we	O
find	O
the	O
following	O
clusters	O
cataggcattctatgtgctg	O
ccagttacggacgccgaaag	O
cggccgcgcctccgggaacg	O
acatgaactacatagtataa	O
gttggtcagcacacggactg	O
cactacggctacctgggcaa	O
cggtccgtccgaggcactcg	O
caccatcacccttgctaagg	O
caaatgcctcacgcgtctca	O
gccaagcagggtctcaactt	O
catggactgctccacaaagg	O
tggaaccttaaaaaaaaaaa	O
gtctcctgccctctctgaac	O
gtgcctggacctgaaaagcc	O
aaagtgctctgaaaactcac	O
cctcccctcccctttcctgc	O
taagtgtcctctgctcctaa	O
aaagaactcccctccctgcc	O
aaaaaaacgaaaaacctaag	O
gcgtaaaaaaagtcctgggt	O
where	O
sequences	B
in	O
the	O
first	O
column	O
are	O
assigned	O
to	O
cluster	O
and	O
sequences	B
in	O
the	O
second	O
column	O
to	O
cluster	O
in	O
this	O
case	O
the	O
data	O
in	O
was	O
in	O
fact	O
generated	O
by	O
a	O
two-component	O
markov	O
mixture	B
and	O
the	O
posterior	B
assignment	O
is	O
in	O
agreement	O
with	O
the	O
known	O
clusters	O
see	O
demomixmarkov	O
m	O
draft	O
march	O
hidden	B
markov	O
models	O
figure	O
a	O
first	O
order	O
hidden	B
markov	I
model	B
with	O
hidden	B
variables	I
domht	O
h	O
t	O
t	O
the	O
visible	B
variables	O
vt	O
can	O
be	O
either	O
discrete	B
or	O
continuous	B
hidden	B
markov	O
models	O
the	O
hidden	B
markov	I
model	B
defines	O
a	O
markov	B
chain	B
on	O
hidden	B
latent	B
variables	O
the	O
observed	O
visible	B
variables	O
are	O
dependent	O
on	O
the	O
hidden	B
variables	I
through	O
an	O
emission	O
pvtht	O
this	O
defines	O
a	O
joint	B
distribution	B
pvthtphtht	O
for	O
which	O
the	O
graphical	O
model	B
is	O
depicted	O
in	O
for	O
a	O
stationary	B
hmm	B
the	O
transition	O
phtht	O
and	O
emission	O
pvtht	O
distributions	O
are	O
constant	O
through	O
time	O
the	O
use	O
of	O
the	O
hmm	B
is	O
widespread	O
and	O
a	O
subset	O
of	O
the	O
many	O
applications	O
of	O
hmms	O
is	O
given	O
in	O
definition	O
distribution	B
for	O
a	O
stationary	B
hmm	B
the	O
transition	B
distribution	B
is	O
defined	O
by	O
the	O
h	O
h	O
transition	B
matrix	B
i	O
i	O
and	O
an	O
initial	O
distribution	B
ai	O
i	O
definition	O
distribution	B
for	O
a	O
stationary	B
hmm	B
and	O
emission	B
distribution	B
pvtht	O
with	O
discrete	B
states	O
vt	O
v	O
we	O
define	O
a	O
v	O
h	O
emission	B
matrix	B
bij	O
pvt	O
iht	O
j	O
for	O
continuous	B
outputs	O
ht	O
selects	O
one	O
of	O
h	O
possible	O
output	O
distributions	O
pvtht	O
ht	O
h	O
in	O
the	O
engineering	O
and	O
machine	O
learning	B
communities	O
the	O
term	O
hmm	B
typically	O
refers	O
to	O
the	O
case	O
of	O
discrete	B
variables	O
ht	O
a	O
convention	O
that	O
we	O
adopt	O
here	O
in	O
statistics	O
the	O
term	O
hmm	B
often	O
refers	O
to	O
any	O
model	B
with	O
the	O
independence	B
structure	B
in	O
equation	B
regardless	O
of	O
the	O
form	O
of	O
the	O
variables	O
ht	O
for	O
example	O
the	O
classical	O
inference	B
problems	O
filtering	O
prediction	O
smoothing	B
likelihood	B
most	O
likely	O
hidden	B
path	B
alignment	O
the	O
present	O
the	O
future	O
the	O
past	O
argmax	O
t	O
s	O
t	O
u	O
the	O
most	O
likely	O
hidden	B
path	B
problem	B
is	O
termed	O
viterbi	B
alignment	I
in	O
the	O
engineering	O
literature	O
all	O
these	O
classical	O
inference	B
problems	O
are	O
straightforward	O
since	O
the	O
distribution	B
is	O
singly-connected	B
so	O
that	O
any	O
standard	O
inference	B
method	O
can	O
be	O
adopted	O
for	O
these	O
problems	O
the	O
factor	B
graph	B
and	O
junction	O
trees	O
for	O
draft	O
march	O
hidden	B
markov	O
models	O
figure	O
factor	B
graph	B
for	O
the	O
first	O
order	O
hmm	B
of	O
junction	B
tree	B
for	O
the	O
first	O
order	O
hmm	B
are	O
given	O
in	O
in	O
both	O
cases	O
after	O
suitable	O
setting	O
of	O
the	O
factors	O
and	O
clique	B
potentials	O
filtering	O
corresponds	O
to	O
passing	B
messages	O
from	O
left	O
to	O
right	O
and	O
upwards	O
smoothing	B
corresponds	O
to	O
a	O
valid	O
schedule	B
of	O
message	B
passingabsorption	O
both	O
forwards	O
and	O
backwards	O
along	O
all	O
edges	O
it	O
is	O
also	O
straightforward	O
to	O
derive	O
appropriate	O
recursions	O
directly	O
this	O
is	O
instructive	O
and	O
also	O
useful	O
in	O
constructing	O
compact	O
and	O
numerically	O
stable	O
algorithms	O
filtering	O
we	O
first	O
compute	O
the	O
joint	B
marginal	B
pht	O
from	O
which	O
the	O
conditional	B
marginal	B
can	O
subsequently	O
be	O
obtained	O
by	O
normalisation	B
a	O
recursion	O
for	O
pht	O
is	O
obtained	O
by	O
considering	O
pht	O
ht	O
ht	O
ht	O
pht	O
ht	O
vt	O
ht	O
ht	O
ht	O
ht	O
pvthtphtht	O
the	O
cancellations	O
follow	O
from	O
the	O
conditional	B
independence	B
assumptions	O
of	O
the	O
model	B
hence	O
if	O
we	O
define	O
pht	O
pvtht	O
corrector	O
ht	O
equation	B
above	O
gives	O
the	O
phtht	O
predictor	O
with	O
t	O
this	O
recursion	O
has	O
the	O
interpretation	O
that	O
the	O
filtered	O
distribution	B
is	O
propagated	O
forwards	O
by	O
the	O
dynamics	O
for	O
one	O
timestep	O
to	O
reveal	O
a	O
new	O
prior	B
distribution	B
at	O
time	O
t	O
this	O
distribution	B
is	O
then	O
modulated	O
by	O
the	O
observation	O
vt	O
incorporating	O
the	O
new	O
evidence	O
into	O
the	O
filtered	O
distribution	B
is	O
also	O
referred	O
to	O
as	O
a	O
predictor-corrector	B
method	O
since	O
each	O
is	O
smaller	O
than	O
and	O
the	O
recursion	O
involves	O
multiplication	O
by	O
terms	O
less	O
than	O
the	O
s	O
can	O
become	O
very	O
small	O
to	O
avoid	O
numerical	B
problems	O
it	O
is	O
therefore	O
advisable	O
to	O
work	O
with	O
log	O
see	O
hmmforward	O
m	O
normalisation	B
gives	O
the	O
filtered	O
posterior	B
if	O
we	O
only	O
require	O
the	O
filtered	O
posterior	B
we	O
are	O
free	O
to	O
rescale	O
the	O
s	O
as	O
we	O
wish	O
in	O
this	O
case	O
an	O
alternative	O
to	O
working	O
with	O
log	O
messages	O
is	O
to	O
work	O
with	O
normalised	O
messages	O
so	O
that	O
the	O
sum	O
of	O
the	O
components	O
is	O
always	O
draft	O
march	O
hidden	B
markov	O
models	O
we	O
can	O
write	O
equation	B
above	O
directly	O
as	O
a	O
recursion	O
for	O
the	O
filtered	O
distribution	B
pvthtphtht	O
t	O
ht	O
intuitively	O
the	O
term	O
pht	O
has	O
the	O
effect	O
of	O
removing	O
all	O
nodes	O
in	O
the	O
graph	B
before	O
time	O
t	O
and	O
replacing	O
their	O
influence	O
by	O
a	O
modified	O
prior	B
distribution	B
on	O
ht	O
one	O
may	O
interpret	O
pvthtphtht	O
as	O
a	O
likelihood	B
giving	O
rise	O
to	O
the	O
joint	B
posterior	B
pht	O
ht	O
under	O
bayesian	B
updating	O
at	O
the	O
next	O
timestep	O
the	O
previous	O
posterior	B
becomes	O
the	O
new	O
prior	B
parallel	B
smoothing	B
there	O
are	O
two	O
main	O
approaches	O
to	O
computing	O
perhaps	O
the	O
most	O
common	O
in	O
the	O
hmm	B
literature	O
is	O
the	O
parallel	B
method	O
which	O
is	O
equivalent	B
to	O
message	B
passing	B
on	O
factor	B
graphs	O
in	O
this	O
one	O
separates	O
the	O
smoothed	O
posterior	B
into	O
contributions	O
from	O
the	O
past	O
and	O
future	O
pht	O
pht	O
pht	O
past	O
future	O
the	O
term	O
is	O
obtained	O
from	O
the	O
forward	O
recursion	O
the	O
term	O
may	O
be	O
obtained	O
using	O
a	O
backward	O
recursion	O
as	O
we	O
show	O
below	O
the	O
forward	O
and	O
backward	O
recursions	O
are	O
independent	O
and	O
may	O
therefore	O
be	O
run	O
in	O
parallel	B
with	O
their	O
results	O
combined	O
to	O
obtain	O
the	O
smoothed	O
posterior	B
this	O
approach	B
is	O
also	O
sometimes	O
termed	O
the	O
two-filter	B
smoother	I
the	O
recursion	O
pvttht	O
ht	O
ht	O
ht	O
defining	O
pvt	O
htht	O
ht	O
ht	O
htht	O
ht	O
ht	O
equation	B
above	O
gives	O
the	O
pvthtphtht	O
t	O
t	O
with	O
as	O
for	O
the	O
forward	O
pass	O
working	O
in	O
log	O
space	O
is	O
recommended	O
to	O
avoid	O
numerical	B
difficulties	O
if	O
one	O
only	O
desires	O
posterior	B
distributions	O
one	O
can	O
also	O
perform	O
local	B
normalisation	B
at	O
each	O
stage	O
since	O
only	O
the	O
relative	O
magnitude	O
of	O
the	O
components	O
of	O
are	O
of	O
importance	B
the	O
smoothed	O
posterior	B
is	O
then	O
given	O
by	O
ht	O
together	O
the	O
recursions	O
are	O
called	O
the	O
forward-backward	B
algorithm	B
correction	O
smoothing	B
an	O
alternative	O
to	O
the	O
parallel	B
method	O
is	O
to	O
form	O
a	O
recursion	O
directly	O
for	O
the	O
smoothed	O
posterior	B
this	O
can	O
be	O
achieved	O
by	O
recognising	O
that	O
conditioning	B
on	O
the	O
present	O
makes	O
the	O
future	O
redundant	O
pht	O
draft	O
march	O
hidden	B
markov	O
models	O
this	O
gives	O
a	O
recursion	O
for	O
with	O
the	O
term	O
may	O
be	O
computed	O
using	O
the	O
filtered	O
results	O
where	O
the	O
proportionality	O
constant	O
is	O
found	O
by	O
normalisation	B
this	O
is	O
a	O
form	O
of	O
dynamics	B
reversal	I
as	O
if	O
we	O
are	O
reversing	O
the	O
direction	O
of	O
the	O
hidden	B
to	O
hidden	B
arrow	O
in	O
the	O
hmm	B
this	O
procedure	O
also	O
termed	O
the	O
rauch-tung-striebel	B
is	O
sequential	B
since	O
we	O
need	O
to	O
first	O
complete	O
the	O
recursions	O
after	O
which	O
the	O
recursion	O
may	O
begin	O
this	O
is	O
a	O
so-called	O
correction	B
smoother	I
since	O
it	O
corrects	O
the	O
filtered	O
result	O
interestingly	O
once	O
filtering	O
has	O
been	O
carried	O
out	O
the	O
evidential	O
states	O
are	O
not	O
needed	O
during	O
the	O
subsequent	O
recursion	O
the	O
and	O
recursions	O
are	O
related	O
through	O
computing	O
the	O
pairwise	B
marginal	B
pht	O
ht	O
to	O
implement	O
the	O
em	B
algorithm	B
for	O
learning	B
we	O
require	O
terms	O
such	O
as	O
pht	O
ht	O
these	O
can	O
be	O
obtained	O
by	O
message	B
passing	B
on	O
either	O
a	O
factor	B
graph	B
or	O
junction	B
tree	B
which	O
the	O
pairwise	B
marginals	O
are	O
contained	O
in	O
the	O
cliques	O
see	O
alternatively	O
an	O
explicit	O
recursion	O
is	O
as	O
follows	O
pht	O
ht	O
ht	O
ht	O
ht	O
ht	O
ht	O
rearranging	O
we	O
therefore	O
have	O
pht	O
see	O
hmmsmooth	O
m	O
the	O
likelihood	B
the	O
likelihood	B
of	O
a	O
sequence	O
of	O
observations	O
can	O
be	O
computed	O
from	O
pht	O
ht	O
an	O
alternative	O
computation	O
can	O
be	O
found	O
by	O
making	O
use	O
of	O
the	O
decomposition	B
ht	O
each	O
factor	B
can	O
be	O
computed	O
using	O
pvt	O
ht	O
ht	O
ht	O
ht	O
phtht	O
is	O
most	O
common	O
to	O
use	O
this	O
terminology	O
for	O
the	O
continuous	B
variable	O
case	O
though	O
we	O
adopt	O
it	O
here	O
also	O
for	O
the	O
discrete	B
variable	O
case	O
draft	O
march	O
hidden	B
markov	O
models	O
figure	O
localising	O
the	O
burglar	O
the	O
latent	B
variable	I
ht	O
denotes	O
the	O
positions	O
defined	O
over	O
the	O
grid	O
of	O
the	O
ground	O
floor	O
of	O
the	O
house	O
a	O
representation	O
of	O
the	O
probability	O
that	O
the	O
floor	O
will	O
creak	O
at	O
each	O
of	O
the	O
positions	O
pvcreakh	O
light	O
squares	O
represent	O
probability	O
and	O
dark	O
square	O
a	O
representation	O
of	O
the	O
probability	O
pvbumph	O
that	O
the	O
burglar	O
will	O
bump	O
into	O
something	O
in	O
each	O
of	O
the	O
positions	O
creaks	O
bumps	O
where	O
the	O
final	O
term	O
pht	O
is	O
the	O
filtered	O
result	O
in	O
both	O
approaches	O
the	O
likelihood	B
of	O
an	O
output	O
sequence	O
requires	O
only	O
a	O
forward	O
computation	O
if	O
required	O
one	O
can	O
also	O
compute	O
the	O
likelihood	B
using	O
ht	O
which	O
is	O
valid	O
for	O
any	O
t	O
t	O
most	O
likely	O
joint	B
state	O
the	O
most	O
likely	O
path	B
of	O
is	O
the	O
same	O
as	O
the	O
most	B
likely	I
state	I
fixed	O
of	O
pvthtphtht	O
t	O
the	O
most	O
likely	O
path	B
can	O
be	O
found	O
using	O
the	O
max-product	B
version	O
of	O
the	O
factor	B
graph	B
or	O
max-absorption	O
on	O
the	O
junction	B
tree	B
alternatively	O
an	O
explicit	O
derivation	O
can	O
be	O
obtained	O
by	O
considering	O
max	O
ht	O
pvthtphtht	O
pvthtphtht	O
max	O
ht	O
pvtht	O
the	O
message	B
conveys	O
information	O
from	O
the	O
end	O
of	O
the	O
chain	B
to	O
the	O
penultimate	O
timestep	O
we	O
can	O
continue	O
in	O
this	O
manner	O
defining	O
the	O
recursion	O
max	O
ht	O
pvthtphtht	O
t	O
t	O
with	O
this	O
means	O
that	O
the	O
effect	O
of	O
maximising	O
over	O
ht	O
is	O
compressed	O
into	O
a	O
message	B
so	O
that	O
the	O
most	B
likely	I
state	I
h	O
argmax	O
h	O
is	O
given	O
by	O
once	O
computed	O
backtracking	B
gives	O
t	O
argmax	O
h	O
ht	O
pvthtphth	O
t	O
this	O
special	O
case	O
of	O
the	O
max-product	B
algorithm	B
is	O
called	O
the	O
viterbi	B
algorithm	B
similarly	O
one	O
may	O
use	O
the	O
n-max-product	B
algorithm	B
to	O
obtain	O
the	O
n-most	O
likely	O
hidden	B
paths	O
example	O
localisation	B
example	O
you	O
re	O
asleep	O
upstairs	O
in	O
your	O
house	O
and	O
awoken	O
by	O
noises	O
from	O
downstairs	O
you	O
realise	O
that	O
a	O
burglar	O
is	O
on	O
the	O
ground	O
floor	O
and	O
attempt	O
to	O
understand	O
where	O
he	O
his	O
from	O
listening	O
to	O
his	O
movements	O
you	O
mentally	O
partition	O
the	O
ground	O
floor	O
into	O
a	O
grid	O
for	O
each	O
grid	O
position	O
you	O
know	O
the	O
probability	O
that	O
if	O
someone	O
is	O
in	O
that	O
position	O
the	O
floorboard	O
will	O
creak	O
draft	O
march	O
hidden	B
markov	O
models	O
creaks	O
and	O
bumps	O
filtering	O
smoothing	B
viterbi	B
true	O
burglar	O
position	O
t	O
t	O
vcreak	O
t	O
vbump	O
where	O
vcreak	O
t	O
t	O
otherwise	O
and	O
vbump	O
each	O
panel	O
represents	O
the	O
means	O
that	O
there	O
was	O
a	O
creak	O
in	O
the	O
floorboard	O
meaning	O
bumped	O
into	O
something	O
is	O
in	O
state	O
otherwise	O
t	O
and	O
the	O
right	O
t	O
the	O
lighter	O
colour	O
represents	O
the	O
occurrence	O
of	O
a	O
creak	O
or	O
bump	O
the	O
darker	O
colour	O
the	O
absence	O
the	O
smoothed	O
the	O
most	O
likely	O
figure	O
localising	O
the	O
burglar	O
through	O
time	O
for	O
time	O
steps	O
visible	B
information	O
vt	O
there	O
are	O
panels	O
one	O
for	O
each	O
time	O
t	O
the	O
left	O
half	O
of	O
the	O
panel	O
represents	O
half	O
the	O
filtered	O
distribution	B
representing	O
where	O
we	O
think	O
the	O
burglar	O
is	O
distribution	B
so	O
that	O
we	O
can	O
figure	O
out	O
where	O
we	O
think	O
the	O
burglar	O
went	O
burglar	O
path	B
arg	O
the	O
actual	O
path	B
of	O
the	O
burglar	O
similarly	O
you	O
know	O
for	O
each	O
position	O
the	O
probability	O
that	O
someone	O
will	O
bump	O
into	O
something	O
in	O
the	O
dark	O
the	O
floorboard	O
creaking	O
and	O
bumping	O
into	O
objects	O
can	O
occur	O
independently	O
in	O
addition	O
you	O
assume	O
that	O
the	O
burglar	O
will	O
move	O
only	O
one	O
grid	O
square	O
forwards	O
backwards	O
left	O
or	O
right	O
in	O
a	O
single	O
timestep	O
based	O
on	O
a	O
series	O
of	O
bumpno	O
bump	O
and	O
creakno	O
creak	O
information	O
you	O
try	O
to	O
figure	O
out	O
based	O
on	O
your	O
knowledge	O
of	O
the	O
ground	O
floor	O
where	O
the	O
burglar	O
might	O
be	O
we	O
can	O
represent	O
the	O
scenario	O
using	O
a	O
hmm	B
where	O
h	O
denotes	O
the	O
grid	O
square	O
the	O
visible	B
variable	O
has	O
a	O
factorised	B
form	O
v	O
vcreak	O
vbump	O
and	O
to	O
use	O
our	O
standard	O
code	O
we	O
form	O
a	O
new	O
visible	B
variable	O
with	O
states	O
using	O
pvh	O
pvcreakhpvbumph	O
based	O
on	O
the	O
past	O
information	O
our	O
belief	O
as	O
to	O
where	O
the	O
burglar	O
might	O
be	O
is	O
represented	O
by	O
the	O
filtered	O
distribution	B
after	O
the	O
burglar	O
has	O
left	O
at	O
t	O
the	O
police	O
arrive	O
and	O
try	O
to	O
piece	O
together	O
where	O
the	O
burglar	O
went	O
based	O
on	O
the	O
sequence	O
of	O
creaks	O
and	O
bumps	O
you	O
provide	O
at	O
any	O
time	O
t	O
the	O
information	O
as	O
to	O
where	O
the	O
burglar	O
could	O
have	O
been	O
is	O
represented	O
by	O
the	O
smoothed	O
distribution	B
the	O
police	O
s	O
single	O
best-guess	O
for	O
the	O
sequence	O
of	O
burglar	O
positions	O
is	O
provided	O
by	O
the	O
most	O
likely	O
joint	B
hidden	B
state	O
arg	O
self	O
localisation	B
and	O
kidnapped	O
robots	O
a	O
robot	O
has	O
an	O
internal	O
grid-based	O
map	B
of	O
its	O
environment	O
and	O
for	O
each	O
location	O
h	O
h	O
knows	O
the	O
likely	O
sensor	O
readings	O
he	O
would	O
expect	O
in	O
that	O
location	O
the	O
robot	O
is	O
kidnapped	O
and	O
placed	O
somewhere	O
in	O
the	O
environment	O
the	O
robot	O
then	O
starts	O
to	O
move	O
gathering	O
sensor	O
information	O
based	O
on	O
these	O
readings	O
and	O
intended	O
movements	O
the	O
robot	O
attempts	O
to	O
figure	O
out	O
his	O
location	O
due	O
to	O
wheel	O
slippage	O
on	O
the	O
floor	O
an	O
intended	O
action	O
by	O
the	O
robot	O
such	O
as	O
move	O
forwards	O
might	O
not	O
be	O
successful	O
given	O
all	O
the	O
information	O
the	O
robot	O
has	O
he	O
would	O
like	O
to	O
infer	O
this	O
problem	B
differs	O
from	O
the	O
burglar	O
scenario	O
in	O
that	O
the	O
robot	O
now	O
has	O
knowledge	O
of	O
the	O
intended	O
movements	O
he	O
makes	O
this	O
should	O
give	O
more	O
draft	O
march	O
learning	B
hmms	O
figure	O
a	O
model	B
for	O
robot	O
self-localisation	O
at	O
each	O
time	O
the	O
robot	O
makes	O
an	O
intended	O
movement	O
mt	O
as	O
a	O
generative	B
model	B
knowing	O
the	O
intended	O
movement	O
mt	O
and	O
the	O
current	O
grid	O
position	O
ht	O
the	O
robot	O
has	O
an	O
idea	O
of	O
where	O
he	O
should	O
be	O
at	O
the	O
next	O
time-step	O
and	O
what	O
sensor	O
reading	O
he	O
would	O
expect	O
there	O
based	O
on	O
only	O
the	O
sensor	O
information	O
and	O
the	O
intended	O
movements	O
the	O
task	O
is	O
to	O
infer	O
a	O
distribution	B
over	O
robot	O
locations	O
information	O
as	O
to	O
where	O
he	O
could	O
be	O
one	O
can	O
view	O
this	O
as	O
extra	O
visible	B
information	O
though	O
it	O
is	O
more	O
natural	B
to	O
think	O
of	O
this	O
as	O
additional	O
input	O
information	O
a	O
model	B
of	O
this	O
scenario	O
is	O
see	O
pvthtphtht	O
mt	O
the	O
visible	B
variables	O
are	O
known	O
as	O
are	O
the	O
intended	O
movements	O
the	O
model	B
expresses	O
that	O
the	O
movements	O
selected	O
by	O
the	O
robot	O
are	O
random	O
no	O
decision	O
making	O
in	O
terms	O
of	O
where	O
to	O
go	O
next	O
we	O
assume	O
that	O
the	O
robot	O
has	O
full	O
knowledge	O
of	O
the	O
conditional	B
distributions	O
defining	O
the	O
model	B
knows	O
the	O
map	B
of	O
his	O
environment	O
and	O
all	O
state	O
transition	O
and	O
emission	O
probabilities	O
if	O
our	O
interest	O
is	O
only	O
in	O
localising	O
the	O
robot	O
since	O
the	O
inputs	O
m	O
are	O
known	O
this	O
model	B
is	O
in	O
fact	O
a	O
form	O
of	O
time-dependent	O
hmm	B
pvthtphtht	O
t	O
for	O
a	O
time-dependent	O
transition	O
phtht	O
t	O
defined	O
by	O
the	O
intended	O
movement	O
mt	O
any	O
inference	B
task	O
required	O
then	O
follows	O
the	O
standard	O
stationary	B
hmm	B
algorithms	O
albeit	O
on	O
replacing	O
the	O
time-independent	O
transitions	O
phtht	O
with	O
the	O
known	O
time-dependent	O
transitions	O
in	O
self	B
localisation	B
and	I
mapping	I
the	O
robot	O
does	O
not	O
know	O
the	O
map	B
of	O
his	O
environment	O
this	O
corresponds	O
to	O
having	O
to	O
learn	O
the	O
transition	O
and	O
emission	O
distributions	O
on-the-fly	O
as	O
he	O
explores	O
the	O
environment	O
natural	B
language	O
models	O
a	O
simple	O
generative	B
model	B
of	O
language	O
can	O
be	O
obtained	O
from	O
the	O
letter-to-letter	O
transitions	O
so-called	O
bigram	B
in	O
the	O
example	O
below	O
we	O
use	O
this	O
in	O
a	O
hmm	B
to	O
clean	O
up	O
the	O
mis-typings	O
example	O
fingers	O
a	O
stubby	O
fingers	O
typist	O
has	O
the	O
tendency	O
to	O
hit	O
either	O
the	O
correct	O
key	O
or	O
a	O
neighbouring	O
key	O
for	O
simplicity	O
we	O
assume	O
that	O
there	O
are	O
keys	O
lower	O
case	O
a	O
to	O
lower	O
case	O
z	O
and	O
the	O
space	O
bar	O
to	O
model	B
this	O
we	O
use	O
an	O
emission	B
distribution	B
bij	O
pv	O
ih	O
j	O
where	O
i	O
j	O
as	O
depicted	O
in	O
a	O
database	O
of	O
letter-to-next-letter	O
frequencies	O
yields	O
the	O
transition	B
matrix	B
aij	O
ih	O
j	O
in	O
english	O
for	O
simplicity	O
we	O
assume	O
that	O
is	O
uniform	B
also	O
we	O
assume	O
that	O
each	O
intended	O
key	O
press	O
results	O
in	O
a	O
single	O
press	O
given	O
a	O
typed	O
sequence	O
kezrninh	O
what	O
is	O
the	O
most	O
likely	O
word	O
that	O
this	O
corresponds	O
to	O
by	O
listing	O
the	O
most	O
likely	O
hidden	B
sequences	B
the	O
n-max-product	B
algorithm	B
and	O
discarding	O
those	O
that	O
are	O
not	O
in	O
a	O
standard	O
english	O
dictionary	O
the	O
most	O
likely	O
word	O
that	O
was	O
intended	O
is	O
learning	B
see	O
demohmmbigram	O
m	O
learning	B
hmms	O
given	O
a	O
set	O
of	O
data	O
v	O
of	O
n	O
sequences	B
where	O
sequence	O
vn	O
vn	O
is	O
of	O
length	O
tn	O
we	O
seek	O
the	O
hmm	B
transition	B
matrix	B
a	O
emission	B
matrix	B
b	O
and	O
initial	O
vector	O
a	O
most	O
likely	O
to	O
have	O
have	O
generated	O
draft	O
march	O
learning	B
hmms	O
figure	O
the	O
letter-to-letter	O
transition	B
matrix	B
for	O
english	O
ih	O
j	O
the	O
letter	O
emission	B
matrix	B
for	O
a	O
typist	O
with	O
stubby	O
fingers	O
in	O
which	O
the	O
key	O
or	O
its	O
neighbours	O
on	O
the	O
keyboard	O
are	O
likely	O
to	O
be	O
hit	O
v	O
we	O
make	O
the	O
i	O
i	O
d	O
assumption	O
so	O
that	O
each	O
sequence	O
is	O
independently	O
generated	O
and	O
assume	O
that	O
we	O
know	O
the	O
number	O
of	O
hidden	B
states	O
h	O
for	O
simplicity	O
we	O
concentrate	O
here	O
on	O
the	O
case	O
of	O
discrete	B
visible	B
variables	O
assuming	O
also	O
we	O
know	O
the	O
number	O
of	O
states	O
v	O
em	B
algorithm	B
the	O
application	O
of	O
em	B
to	O
the	O
hmm	B
model	B
is	O
called	O
the	O
baum-welch	B
algorithm	B
and	O
follows	O
the	O
general	O
strategy	O
outlined	O
in	O
m-step	B
assuming	O
i	O
i	O
d	O
data	O
the	O
m-step	B
is	O
given	O
by	O
maximising	O
the	O
energy	B
t	O
n	O
hn	O
vn	O
vn	O
pvn	O
tn	O
hn	O
hn	O
t	O
with	O
respect	O
to	O
the	O
parameters	O
a	O
b	O
a	O
hn	O
denotes	O
using	O
the	O
form	O
of	O
the	O
hmm	B
we	O
obtain	O
pvn	O
t	O
where	O
for	O
compactness	O
we	O
drop	O
the	O
sequence	O
index	O
from	O
the	O
h	O
variables	O
to	O
avoid	O
potential	B
confusion	O
we	O
write	O
i	O
to	O
denote	O
the	O
table	O
entry	O
for	O
the	O
probability	O
that	O
the	O
initial	O
hidden	B
variable	I
is	O
in	O
state	O
i	O
optimising	O
equation	B
with	O
respect	O
to	O
enforcing	O
to	O
be	O
a	O
distribution	B
we	O
obtain	O
anew	O
i	O
i	O
n	O
ivn	O
tn	O
which	O
is	O
the	O
average	B
number	O
of	O
times	O
that	O
the	O
first	O
hidden	B
variable	I
is	O
in	O
state	O
i	O
similarly	O
which	O
is	O
the	O
number	O
of	O
times	O
that	O
a	O
transition	O
from	O
hidden	B
state	O
i	O
to	O
hidden	B
state	O
occurs	O
averaged	O
over	O
all	O
times	O
we	O
assumed	O
stationarity	O
and	O
training	B
sequences	B
normalising	O
we	O
obtain	O
poldht	O
i	O
i	O
i	O
anew	O
i	O
poldht	O
i	O
poldht	O
i	O
anew	O
draft	O
march	O
abcdefghijklmnopqrstuvwxyz	O
abcdefghijklmnopqrstuvwxyz	O
abcdefghijklmnopqrstuvwxyz	O
abcdefghijklmnopqrstuvwxyz	O
finally	O
bnew	O
ji	O
pnewvt	O
jht	O
i	O
i	O
t	O
j	O
poldht	O
ivn	O
learning	B
hmms	O
which	O
is	O
the	O
expected	O
number	O
of	O
times	O
that	O
for	O
the	O
observation	O
being	O
in	O
state	O
j	O
the	O
hidden	B
state	O
is	O
i	O
the	O
proportionality	O
constant	O
is	O
determined	O
by	O
the	O
normalisation	B
requirement	O
e-step	B
in	O
computing	O
the	O
m-step	B
above	O
the	O
quantities	O
ivn	O
poldht	O
i	O
are	O
obtained	O
by	O
inference	B
using	O
the	O
techniques	O
described	O
in	O
and	O
poldht	O
ivn	O
equations	O
are	O
repeated	O
until	O
convergence	O
see	O
hmmem	O
m	O
and	O
demohmmlearn	O
m	O
parameter	B
initialisation	O
the	O
em	B
algorithm	B
converges	O
to	O
a	O
local	B
maxima	O
of	O
the	O
likelihood	B
and	O
in	O
general	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
algorithm	B
will	O
find	O
the	O
global	B
maximum	O
how	O
best	O
to	O
initialise	O
the	O
parameters	O
is	O
a	O
thorny	O
issue	O
with	O
a	O
suitable	O
initialisation	O
of	O
the	O
emission	B
distribution	B
often	O
being	O
critical	O
for	O
a	O
practical	O
strategy	O
is	O
to	O
initialise	O
the	O
emission	O
pvh	O
based	O
on	O
first	O
fitting	O
a	O
simpler	O
non-temporal	O
mixture	B
model	B
h	O
pvhph	O
to	O
the	O
data	O
continuous	B
observations	O
for	O
a	O
continuous	B
vector	O
observation	O
vt	O
with	O
dim	O
vt	O
d	O
we	O
require	O
a	O
model	B
pvtht	O
mapping	O
the	O
discrete	B
state	O
ht	O
to	O
a	O
distribution	B
over	O
outputs	O
using	O
a	O
continuous	B
output	O
does	O
not	O
change	O
any	O
of	O
the	O
standard	O
inference	B
message	B
passing	B
equations	O
so	O
that	O
inference	B
can	O
be	O
carried	O
out	O
for	O
essentially	O
arbitrarily	O
complex	O
emission	O
distributions	O
indeed	O
filtering	O
smoothing	B
and	O
viterbi	B
inference	B
the	O
normalisation	B
z	O
of	O
the	O
emission	O
pvh	O
hz	O
is	O
not	O
required	O
for	O
learning	B
however	O
the	O
emission	O
normalisation	B
constant	I
is	O
required	O
since	O
this	O
is	O
a	O
dependent	O
on	O
the	O
parameters	O
of	O
the	O
model	B
mixture	B
emission	O
to	O
make	O
a	O
richer	O
emission	O
model	B
for	O
continuous	B
observations	O
one	O
approach	B
is	O
use	O
a	O
mixture	B
pvtht	O
kt	O
n	O
pvtkt	O
htpktht	O
where	O
kt	O
is	O
a	O
discrete	B
summation	O
variable	O
for	O
learning	B
it	O
is	O
useful	O
to	O
consider	O
the	O
kt	O
as	O
additional	O
latent	B
variables	O
so	O
that	O
updates	O
for	O
each	O
component	O
of	O
the	O
emission	O
model	B
can	O
be	O
derived	O
to	O
achieve	O
this	O
consider	O
the	O
contribution	O
to	O
the	O
energy	B
from	O
the	O
emission	O
equal	O
length	O
sequences	B
ev	O
pvn	O
t	O
as	O
it	O
stands	O
the	O
parameters	O
of	O
each	O
component	O
pvtkt	O
ht	O
are	O
coupled	B
in	O
the	O
above	O
expression	O
one	O
approach	B
is	O
to	O
consider	O
klqkthtpktht	O
vt	O
from	O
which	O
we	O
immediately	O
obtain	O
the	O
bound	B
log	O
pvt	O
ht	O
pvt	O
kt	O
and	O
log	O
pvn	O
t	O
t	O
qkthn	O
t	O
t	O
pvn	O
t	O
hn	O
t	O
t	O
pkthn	O
t	O
t	O
draft	O
march	O
learning	B
hmms	O
n	O
n	O
using	O
this	O
in	O
the	O
energy	B
contribution	O
we	O
have	O
the	O
bound	B
on	O
the	O
energy	B
contribution	O
ev	O
qkthn	O
t	O
t	O
pvn	O
t	O
hn	O
t	O
t	O
pkthn	O
t	O
t	O
qhn	O
t	O
we	O
may	O
now	O
maximise	O
this	O
lower	O
bound	B
on	O
the	O
energy	B
of	O
the	O
energy	B
itself	O
the	O
contribution	O
from	O
each	O
emission	O
component	O
pv	O
vh	O
h	O
k	O
k	O
is	O
qkt	O
khn	O
t	O
hqhn	O
t	O
hvn	O
log	O
pvn	O
t	O
h	O
k	O
k	O
the	O
above	O
can	O
then	O
be	O
optimised	O
for	O
fixed	O
qkt	O
khn	O
using	O
t	O
h	O
with	O
these	O
distributions	O
updated	O
the	O
contribution	O
to	O
the	O
energy	B
bound	B
from	O
the	O
mixture	B
weights	O
is	O
given	O
by	O
t	O
ktpktht	O
qnewkthn	O
t	O
pvnhn	O
log	O
pk	O
kh	O
n	O
pk	O
kh	O
h	O
n	O
qkt	O
khn	O
t	O
hqhn	O
t	O
hvn	O
qkt	O
khn	O
t	O
hqhn	O
t	O
hvn	O
so	O
that	O
the	O
m-step	B
update	O
for	O
the	O
mixture	B
weights	O
is	O
is	O
fixed	O
during	O
which	O
the	O
emissions	O
pvh	O
k	O
are	O
learned	O
along	O
with	O
updating	O
qkt	O
khn	O
t	O
in	O
this	O
case	O
the	O
em	B
algorithm	B
is	O
composed	O
of	O
an	O
emission	O
em	B
loop	O
in	O
which	O
the	O
transitions	O
and	O
qhn	O
t	O
h	O
hvn	O
the	O
transition	O
em	B
loop	O
fixes	O
the	O
emission	B
distribution	B
pvh	O
and	O
learns	O
the	O
best	O
transition	O
phtht	O
an	O
alternative	O
to	O
the	O
above	O
derivation	O
is	O
to	O
consider	O
the	O
k	O
as	O
hidden	B
variables	I
and	O
then	O
use	O
standard	O
em	B
algorithm	B
on	O
the	O
joint	B
latent	B
variables	O
kt	O
the	O
reader	O
may	O
show	O
that	O
the	O
two	O
approaches	O
are	O
equivalent	B
the	O
hmm-gmm	O
ktht	O
ktht	O
a	O
common	O
continuous	B
observation	O
mixture	B
emission	O
model	B
component	O
is	O
a	O
gaussian	B
pvtkt	O
ht	O
n	O
so	O
that	O
kt	O
ht	O
indexes	O
the	O
k	O
h	O
mean	B
vectors	O
and	O
covariance	B
matrices	O
em	B
updates	O
for	O
these	O
means	O
and	O
covariances	O
are	O
straightforward	O
to	O
derive	O
from	O
equation	B
see	O
these	O
models	O
are	O
common	O
in	O
tracking	O
applications	O
in	O
particular	O
in	O
speech	B
recognition	I
under	O
the	O
constraint	O
that	O
the	O
covariances	O
are	O
diagonal	O
discriminative	B
training	B
hmms	O
can	O
be	O
used	O
for	O
supervised	B
learning	B
of	O
sequences	B
that	O
is	O
for	O
each	O
sequence	O
vn	O
we	O
have	O
a	O
corresponding	O
class	O
label	O
cn	O
for	O
example	O
we	O
might	O
associated	O
a	O
particular	O
composer	O
c	O
with	O
a	O
sequence	O
and	O
wish	O
to	O
make	O
a	O
model	B
that	O
will	O
predict	O
the	O
composer	O
for	O
a	O
novel	O
music	O
sequence	O
a	O
generative	B
approach	B
to	O
using	O
hmms	O
for	O
classification	B
is	O
to	O
train	O
a	O
separate	O
hmm	B
for	O
each	O
class	O
and	O
subsequently	O
use	O
bayes	O
rule	O
to	O
form	O
the	O
classification	B
for	O
a	O
novel	O
sequence	O
v	O
using	O
pc	O
pv	O
pv	O
if	O
the	O
data	O
is	O
noisy	O
and	O
difficult	O
to	O
model	B
however	O
this	O
generative	B
approach	B
may	O
not	O
work	O
well	O
since	O
much	O
of	O
the	O
expressive	O
power	O
of	O
each	O
model	B
is	O
used	O
to	O
model	B
the	O
complex	O
data	O
rather	O
than	O
focussing	O
on	O
draft	O
march	O
related	O
models	O
figure	O
an	O
explicit	O
duration	O
hmm	B
the	O
counter	O
variables	O
ct	O
deterministically	O
count	O
down	O
to	O
zero	O
when	O
they	O
reach	O
one	O
a	O
h	O
transition	O
is	O
allowed	O
and	O
the	O
new	O
value	B
for	O
ct	O
is	O
sampled	O
the	O
decision	B
boundary	B
in	O
applications	O
such	O
as	O
speech	B
recognition	I
improvements	O
in	O
performance	B
are	O
often	O
reported	O
when	O
the	O
models	O
are	O
trained	O
in	O
a	O
discriminative	B
way	O
in	O
discriminative	B
training	B
see	O
for	O
example	O
one	O
defines	O
a	O
new	O
single	O
discriminative	B
model	B
formed	O
from	O
the	O
c	O
hmms	O
using	O
log	O
pcnvn	O
log	O
pvn	O
log	O
pc	O
log	O
generative	B
likelihood	B
and	O
then	O
maximises	O
the	O
likelihood	B
of	O
a	O
set	O
of	O
observed	O
classes	O
and	O
corresponding	O
observations	O
for	O
a	O
single	O
data	O
pair	O
vn	O
the	O
log	O
likelihood	B
is	O
pvn	O
t	O
the	O
first	O
term	O
above	O
represents	O
the	O
generative	B
likelihood	B
term	O
with	O
the	O
last	O
term	O
accounting	O
for	O
the	O
discrimination	O
whilst	O
deriving	O
em	B
style	O
updates	O
is	O
hampered	O
by	O
the	O
discriminative	B
terms	O
computing	O
the	O
gradient	B
is	O
straightforward	O
using	O
the	O
technique	O
described	O
in	O
in	O
some	O
applications	O
a	O
class	O
label	O
ct	O
is	O
available	O
at	O
each	O
timestep	O
together	O
with	O
an	O
observation	O
vt	O
given	O
a	O
training	B
sequence	O
more	O
generally	O
a	O
set	O
of	O
sequences	B
the	O
aim	O
is	O
to	O
find	O
the	O
optimal	O
class	O
sequence	O
c	O
for	O
a	O
novel	O
observation	O
sequence	O
v	O
one	O
approach	B
is	O
to	O
train	O
a	O
generative	B
model	B
pvtctpctct	O
and	O
subsequently	O
use	O
viterbi	B
to	O
form	O
the	O
class	O
c	O
arg	O
however	O
this	O
approach	B
may	O
not	O
be	O
optimal	O
in	O
terms	O
of	O
class	O
discrimination	O
a	O
cheap	O
surrogate	O
is	O
to	O
train	O
a	O
discriminative	B
classification	B
model	B
pctvt	O
separately	O
with	O
this	O
one	O
can	O
form	O
the	O
emission	O
written	O
for	O
continuous	B
vt	O
pctvt	O
pvt	O
pctvt	O
pvt	O
vt	O
pvtct	O
where	O
pvt	O
is	O
user	O
defined	O
whilst	O
computing	O
the	O
local	B
if	O
the	O
only	O
use	O
of	O
pvtct	O
is	O
to	O
find	O
the	O
optimal	O
class	O
sequence	O
for	O
a	O
novel	O
observation	O
sequence	O
v	O
pctvt	O
pvt	O
may	O
be	O
problematic	O
argmax	O
c	O
vt	O
then	O
the	O
local	B
normalisations	O
play	O
no	O
role	O
since	O
they	O
are	O
independent	O
of	O
c	O
hence	O
during	O
viterbi	B
decoding	O
we	O
may	O
replace	O
the	O
term	O
pvtht	O
with	O
pctvt	O
without	O
affecting	O
the	O
optimal	O
sequence	O
using	O
a	O
model	B
in	O
this	O
way	O
is	O
a	O
special	O
case	O
of	O
the	O
general	O
hybrid	O
procedure	O
described	O
in	O
the	O
approach	B
is	O
suboptimal	O
since	O
learning	B
the	O
classifier	B
is	O
divorced	O
from	O
learning	B
the	O
transition	O
model	B
nevertheless	O
this	O
heuristic	O
historically	O
has	O
some	O
support	O
in	O
the	O
speech	B
recognition	I
community	O
related	O
models	O
explicit	O
duration	B
model	B
for	O
a	O
hmm	B
with	O
self-transition	O
pht	O
iht	O
i	O
i	O
the	O
probability	O
that	O
the	O
latent	B
dynamics	O
stays	O
i	O
which	O
decays	O
exponentially	O
with	O
time	O
in	O
practice	O
however	O
we	O
would	O
in	O
state	O
i	O
for	O
timesteps	O
is	O
draft	O
march	O
related	O
models	O
figure	O
a	O
first	O
order	O
input-output	B
hidden	B
markov	I
model	B
the	O
input	O
x	O
and	O
output	O
v	O
nodes	O
are	O
shaded	O
to	O
emphasise	O
that	O
their	O
states	O
are	O
known	O
during	O
training	B
during	O
testing	O
the	O
inputs	O
are	O
known	O
and	O
the	O
outputs	O
are	O
predicted	O
often	O
like	O
to	O
constrain	O
the	O
dynamics	O
to	O
remain	O
in	O
the	O
same	O
state	O
for	O
a	O
minimum	O
number	O
of	O
timesteps	O
or	O
to	O
have	O
a	O
specified	O
duration	O
distribution	B
a	O
way	O
to	O
enforce	O
this	O
is	O
to	O
use	O
a	O
latent	B
counter	O
variable	O
ct	O
which	O
at	O
the	O
beginning	O
is	O
initialised	O
to	O
a	O
duration	O
sampled	O
from	O
the	O
duration	O
distribution	B
pdurct	O
with	O
maximal	O
duration	O
dmax	O
then	O
at	O
each	O
timestep	O
the	O
counter	O
decrements	O
by	O
until	O
it	O
reaches	O
after	O
which	O
a	O
new	O
duration	O
is	O
sampled	O
pctct	O
the	O
state	O
ht	O
can	O
transition	O
only	O
when	O
ct	O
ct	O
ht	O
pdurct	O
ct	O
ct	O
ct	O
ct	O
phtht	O
ct	O
ptranhtht	O
including	O
the	O
counter	O
variable	O
c	O
defines	O
a	O
joint	B
latent	B
variable	I
distribution	B
that	O
ensures	O
h	O
remains	O
in	O
a	O
desired	O
minimal	O
number	O
of	O
timesteps	O
see	O
since	O
dim	O
ct	O
ht	O
dmaxh	O
naively	O
the	O
the	O
forward	O
and	O
backward	O
recursions	O
the	O
deterministic	B
nature	O
of	O
the	O
transitions	O
means	O
that	O
this	O
can	O
be	O
computational	B
complexity	I
of	O
inference	B
in	O
this	O
model	B
scales	O
as	O
h	O
reduced	O
to	O
h	O
see	O
also	O
however	O
when	O
one	O
runs	O
max	O
the	O
hidden	B
semi-markov	O
model	B
generalises	O
the	O
explicit	O
duration	B
model	B
in	O
that	O
once	O
a	O
new	O
duration	O
ct	O
is	O
sampled	O
the	O
model	B
emits	O
a	O
distribution	B
pvttct	O
defined	O
on	O
a	O
segment	O
of	O
the	O
next	O
ct	O
input-output	B
hmm	B
the	O
is	O
a	O
hmm	B
with	O
additional	O
input	O
variables	O
see	O
each	O
input	O
can	O
be	O
continuous	B
or	O
discrete	B
and	O
modulates	O
the	O
transitions	O
pvtht	O
xtphtht	O
xt	O
t	O
the	O
iohmm	O
may	O
be	O
used	O
as	O
a	O
conditional	B
predictor	O
where	O
the	O
outputs	O
vt	O
represent	O
the	O
prediction	O
at	O
time	O
t	O
in	O
the	O
case	O
of	O
continuous	B
inputs	O
and	O
discrete	B
outputs	O
the	O
tables	O
pvtht	O
xt	O
and	O
phtht	O
xt	O
are	O
usually	O
parameterised	O
using	O
a	O
non-linear	B
function	B
for	O
example	O
pvt	O
yht	O
h	O
xt	O
x	O
w	O
ewt	O
hyx	O
inference	B
then	O
follows	O
in	O
a	O
similar	O
manner	O
as	O
for	O
the	O
standard	O
hmm	B
defining	O
pht	O
the	O
forward	O
pass	O
is	O
given	O
by	O
pvtxt	O
ht	O
ht	O
ht	O
draft	O
march	O
pht	O
ht	O
ht	O
ht	O
ht	O
ht	O
phtht	O
xt	O
x	O
figure	O
linear	B
chain	B
crf	O
since	O
the	O
input	O
x	O
is	O
observed	O
the	O
distribution	B
is	O
just	O
a	O
linear	B
chain	B
factor	B
graph	B
the	O
inference	B
of	O
pairwise	B
marginals	O
pyt	O
yt	O
is	O
therefore	O
straightforward	O
using	O
message	B
passing	B
related	O
models	O
the	O
backward	O
pass	O
is	O
pht	O
for	O
which	O
we	O
need	O
the	O
likelihood	B
can	O
be	O
found	O
ht	O
ht	O
direction	B
bias	B
consider	O
predicting	O
the	O
output	O
distribution	B
given	O
both	O
past	O
and	O
future	O
input	O
information	O
because	O
the	O
hidden	B
states	O
are	O
unobserved	O
we	O
have	O
thus	O
the	O
prediction	O
uses	O
only	O
past	O
information	O
and	O
discards	O
any	O
future	O
contextual	O
information	O
this	O
direction	B
bias	B
is	O
sometimes	O
considered	O
problematic	O
in	O
natural	B
language	O
modelling	B
and	O
motivates	O
the	O
use	O
of	O
undirected	B
models	O
such	O
as	O
conditional	B
random	O
fields	O
linear	B
chain	B
crfs	O
linear	B
chain	B
conditional	B
random	O
fields	O
are	O
an	O
extension	O
of	O
the	O
unstructured	O
crfs	O
we	O
briefly	O
discussed	O
in	O
and	O
have	O
application	O
to	O
modelling	B
the	O
distribution	B
of	O
a	O
set	O
of	O
outputs	O
given	O
an	O
input	O
vector	O
x	O
for	O
example	O
x	O
might	O
represent	O
a	O
sentence	O
in	O
english	O
and	O
should	O
represent	O
the	O
translation	O
into	O
french	O
note	O
that	O
the	O
vector	O
x	O
does	O
not	O
have	O
to	O
have	O
dimension	O
t	O
a	O
first	O
order	O
linear	B
chain	B
crf	O
has	O
the	O
form	O
zx	O
tyt	O
yt	O
x	O
where	O
are	O
the	O
free	O
parameters	O
of	O
the	O
potentials	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
potentials	O
of	O
the	O
form	O
exp	O
kfktyt	O
yt	O
x	O
where	O
fktyt	O
yt	O
x	O
are	O
features	O
see	O
also	O
given	O
a	O
set	O
of	O
input-output	B
sequence	O
pairs	O
xn	O
yn	O
n	O
n	O
all	O
sequenced	O
have	O
equal	O
length	O
t	O
for	O
simplicity	O
we	O
can	O
learn	O
the	O
parameters	O
by	O
maximum	B
likelihood	B
under	O
the	O
standard	O
i	O
i	O
d	O
data	O
assumption	O
the	O
log	O
likelihood	B
is	O
kfkyn	O
t	O
yn	O
t	O
xn	O
log	O
zxn	O
n	O
the	O
reader	O
may	O
readily	O
check	B
that	O
the	O
log	O
likelihood	B
is	O
concave	O
so	O
that	O
the	O
objective	O
function	B
has	O
no	O
local	B
optima	O
the	O
gradient	B
is	O
given	O
by	O
fiyn	O
t	O
yn	O
t	O
xn	O
yt	O
learning	B
therefore	O
requires	O
inference	B
of	O
the	O
marginal	B
terms	O
pyt	O
yt	O
since	O
equation	B
corresponds	O
to	O
a	O
linear	B
chain	B
factor	B
graph	B
see	O
inference	B
of	O
pairwise	B
marginals	O
is	O
straightforward	O
draft	O
march	O
l	O
tn	O
k	O
l	O
nt	O
i	O
related	O
models	O
figure	O
using	O
a	O
linear	B
chain	B
crf	O
to	O
learn	O
the	O
sequences	B
in	O
log	O
likelihood	B
under	O
gradient	B
ascent	O
the	O
learned	O
parameter	B
vector	O
at	O
the	O
end	O
of	O
training	B
the	O
evolution	O
of	O
the	O
using	O
message	B
passing	B
this	O
can	O
be	O
achieved	O
using	O
either	O
the	O
standard	O
factor	B
graph	B
message	B
passing	B
or	O
by	O
deriving	O
an	O
explicit	O
algorithm	B
see	O
finding	O
the	O
most	O
likely	O
output	O
sequence	O
for	O
a	O
novel	O
input	O
x	O
is	O
straightforward	O
since	O
t	O
argmax	O
y	O
tyt	O
yt	O
x	O
corresponds	O
again	O
to	O
a	O
simple	O
linear	B
chain	B
for	O
which	O
max-product	B
inference	B
yields	O
the	O
required	O
result	O
see	O
also	O
in	O
some	O
applications	O
particularly	O
in	O
natural	B
language	O
processing	O
the	O
dimension	O
k	O
of	O
the	O
vector	O
of	O
features	O
fk	O
may	O
be	O
many	O
hundreds	O
of	O
thousands	O
this	O
means	O
that	O
the	O
storage	O
of	O
the	O
hessian	B
is	O
not	O
feasible	O
for	O
newton	O
based	O
training	B
and	O
either	O
limited	O
memory	O
methods	O
or	O
conjugate	B
gradient	B
techniques	O
are	O
typically	O
table	O
a	O
subset	O
of	O
the	O
training	B
input-output	B
sequences	B
each	O
row	O
contains	O
an	O
input	O
xt	O
entry	O
and	O
output	O
yt	O
entry	O
there	O
are	O
input	O
states	O
and	O
output	O
states	O
potentials	O
yt	O
xt	O
exp	O
example	O
chain	B
crf	O
as	O
a	O
model	B
for	O
the	O
data	O
in	O
a	O
linear	B
crf	O
model	B
has	O
i	O
ifiyt	O
yt	O
xt	O
where	O
we	O
set	O
the	O
binary	O
feature	O
functions	O
by	O
first	O
mapping	O
each	O
of	O
the	O
dim	O
dim	O
states	O
to	O
a	O
unique	O
integer	O
ia	O
b	O
c	O
from	O
to	O
dim	O
dim	O
fiabcyt	O
yt	O
xt	O
i	O
a	O
i	O
b	O
i	O
c	O
that	O
is	O
each	O
joint	B
configuration	O
of	O
yt	O
yt	O
xt	O
is	O
mapped	O
to	O
an	O
index	O
and	O
in	O
this	O
case	O
the	O
feature	O
vector	O
f	O
will	O
trivially	O
have	O
only	O
a	O
single	O
non-zero	O
entry	O
the	O
evolution	O
of	O
the	O
gradient	B
ascent	O
training	B
algorithm	B
is	O
plotted	O
in	O
in	O
practice	O
one	O
would	O
use	O
richer	O
feature	O
functions	O
defined	O
to	O
seek	O
features	O
of	O
the	O
input	O
sequence	O
x	O
and	O
also	O
to	O
produce	O
a	O
feature	O
vector	O
with	O
more	O
than	O
one	O
non-zero	O
entry	O
see	O
demolinearcrf	O
m	O
draft	O
march	O
applications	O
figure	O
a	O
dynamic	B
bayesian	B
network	I
possible	O
transitions	O
between	O
variables	O
at	O
the	O
same	O
timeslice	O
have	O
not	O
been	O
shown	O
figure	O
a	O
coupled	B
hmm	B
for	O
example	O
the	O
upper	O
hmm	B
might	O
model	B
speech	O
and	O
the	O
lower	O
the	O
corresponding	O
video	O
sequence	O
the	O
upper	O
hidden	B
units	O
then	O
correspond	O
to	O
phonemes	O
and	O
the	O
lower	O
to	O
mouth	O
positions	O
this	O
model	B
therefore	O
captures	O
the	O
expected	O
coupling	O
between	O
mouth	O
positions	O
and	O
phonemes	O
dynamic	B
bayesian	B
networks	O
a	O
dbn	O
is	O
defined	O
as	O
a	O
belief	O
networkreplicated	O
through	O
time	O
for	O
a	O
multivariate	B
xt	O
with	O
dim	O
xt	O
d	O
the	O
dbn	O
defines	O
a	O
joint	B
model	B
xt	O
pxitxit	O
xt	O
where	O
xit	O
denotes	O
the	O
set	O
of	O
variables	O
at	O
time	O
t	O
except	O
for	O
xit	O
the	O
form	O
of	O
each	O
pxitxit	O
xt	O
is	O
chosen	O
such	O
that	O
the	O
overall	O
distribution	B
remains	O
acyclic	O
at	O
each	O
time-step	O
t	O
there	O
is	O
a	O
set	O
of	O
variables	O
xit	O
i	O
x	O
some	O
of	O
which	O
may	O
be	O
observed	O
in	O
a	O
first	O
order	O
dbn	O
each	O
variable	O
xit	O
has	O
parental	O
variables	O
taken	O
from	O
the	O
set	O
of	O
variables	O
in	O
the	O
previous	O
time-slice	O
xt	O
or	O
from	O
the	O
present	O
time-slice	O
in	O
most	O
applications	O
the	O
model	B
is	O
temporally	O
homogeneous	O
so	O
that	O
one	O
may	O
fully	O
describe	O
the	O
distribution	B
in	O
terms	O
of	O
a	O
two-time-slice	O
model	B
the	O
generalisation	B
to	O
higher-order	O
models	O
is	O
straightforward	O
a	O
coupled	B
hmm	B
is	O
a	O
special	O
dbn	O
that	O
may	O
be	O
used	O
to	O
model	B
coupled	B
streams	O
of	O
information	O
for	O
example	O
video	O
and	O
audio	O
see	O
applications	O
object	O
tracking	O
hmms	O
are	O
used	O
to	O
track	O
moving	O
objects	O
based	O
on	O
an	O
understanding	O
of	O
the	O
dynamics	O
of	O
the	O
object	O
in	O
the	O
transition	B
distribution	B
and	O
an	O
understanding	O
of	O
how	O
an	O
object	O
with	O
a	O
known	O
position	O
would	O
be	O
observed	O
in	O
the	O
emission	B
distribution	B
given	O
an	O
observed	O
sequence	O
the	O
hidden	B
position	O
can	O
then	O
be	O
inferred	O
the	O
burglar	O
is	O
a	O
case	O
in	O
point	O
hmms	O
have	O
been	O
applied	O
in	O
a	O
many	O
tracking	O
contexts	O
including	O
tracking	O
people	O
in	O
videos	O
musical	O
pitch	O
and	O
many	O
automatic	O
speech	B
recognition	I
many	O
speech	B
recognition	I
systems	O
make	O
use	O
of	O
roughly	O
speaking	O
a	O
continuous	B
output	O
vector	O
vt	O
at	O
time	O
t	O
represents	O
which	O
frequencies	O
are	O
present	O
in	O
the	O
speech	O
signal	O
in	O
a	O
small	O
window	O
around	O
time	O
t	O
these	O
acoustic	O
vectors	O
are	O
typically	O
formed	O
from	O
taking	O
a	O
discrete	B
fourier	O
transform	O
of	O
the	O
speech	O
signal	O
over	O
a	O
small	O
window	O
around	O
time	O
t	O
with	O
additional	O
transformations	O
to	O
mimic	O
human	O
auditory	O
processing	O
alternatively	O
related	O
forms	O
of	O
linear	B
coding	O
of	O
the	O
observed	O
acoustic	O
waveform	O
may	O
be	O
the	O
corresponding	O
discrete	B
latent	B
state	O
ht	O
represents	O
a	O
phoneme	O
a	O
basic	O
unit	O
of	O
human	O
speech	O
which	O
there	O
are	O
in	O
standard	O
english	O
training	B
data	O
is	O
painstakingly	O
constructed	O
by	O
a	O
human	O
linguist	O
who	O
draft	O
march	O
applications	O
determines	O
the	O
phoneme	O
ht	O
for	O
each	O
time	O
t	O
and	O
many	O
different	O
observed	O
sequences	B
vt	O
given	O
then	O
each	O
acoustic	O
vector	O
vt	O
and	O
an	O
associated	O
phoneme	O
ht	O
one	O
may	O
use	O
maximum	B
likelihood	B
to	O
fit	O
a	O
mixture	B
of	O
isotropic	B
gaussians	O
pvtht	O
to	O
vt	O
this	O
forms	O
the	O
emission	B
distribution	B
for	O
a	O
hmm	B
using	O
the	O
database	O
of	O
labelled	B
phonemes	O
the	O
phoneme	O
transition	O
phtht	O
can	O
be	O
learned	O
simple	O
counting	B
and	O
forms	O
the	O
transition	B
distribution	B
for	O
a	O
hmm	B
note	O
that	O
in	O
this	O
case	O
since	O
the	O
hidden	B
variable	I
h	O
and	O
observation	O
v	O
are	O
known	O
during	O
training	B
training	B
the	O
hmm	B
is	O
straightforward	O
and	O
boils	O
down	O
to	O
training	B
the	O
emission	O
and	O
transition	O
distributions	O
independently	O
for	O
a	O
new	O
sequence	O
of	O
acoustic	O
vectors	O
we	O
can	O
then	O
use	O
the	O
hmm	B
to	O
infer	O
the	O
most	O
likely	O
phoneme	O
sequence	O
through	O
time	O
arg	O
which	O
takes	O
into	O
account	O
both	O
the	O
way	O
that	O
phonemes	O
appear	O
as	O
acoustic	O
vectors	O
and	O
also	O
the	O
prior	B
language	O
constraints	O
of	O
likely	O
phoneme	O
to	O
phoneme	O
transitions	O
the	O
fact	O
that	O
people	O
speak	O
at	O
different	O
speeds	O
can	O
be	O
addressed	O
using	O
time-warping	O
in	O
which	O
the	O
latent	B
phoneme	O
remains	O
in	O
the	O
same	O
state	O
for	O
a	O
number	O
of	O
timesteps	O
hmm	B
models	O
are	O
typically	O
trained	O
on	O
the	O
assumption	O
of	O
clean	O
underlying	O
speech	O
in	O
practice	O
noise	O
corrupts	O
the	O
speech	O
signal	O
in	O
a	O
complex	O
way	O
so	O
that	O
the	O
resulting	O
model	B
is	O
inappropriate	O
and	O
performance	B
degrades	O
significantly	O
to	O
account	O
for	O
this	O
it	O
is	O
traditional	O
to	O
attempt	O
to	O
denoise	O
the	O
signal	O
before	O
sending	O
this	O
to	O
a	O
standard	O
hmm	B
recogniser	O
if	O
the	O
hmm	B
is	O
used	O
to	O
model	B
a	O
single	O
word	O
it	O
is	O
natural	B
to	O
constrain	O
the	O
hidden	B
state	O
sequence	O
to	O
go	O
forwards	O
through	O
time	O
visiting	O
a	O
set	O
of	O
states	O
in	O
sequence	O
the	O
phoneme	O
order	O
for	O
the	O
word	O
is	O
known	O
in	O
this	O
case	O
the	O
structure	B
of	O
the	O
transition	O
matrices	O
is	O
upper	O
triangular	O
lower	O
depending	O
on	O
your	O
definition	O
or	O
even	O
a	O
banded	O
triangular	O
matrix	B
such	O
forward	O
constraints	O
describe	O
a	O
so-called	O
left-to-right	O
transition	B
matrix	B
bioinformatics	B
in	O
the	O
field	O
of	O
bioinformatics	B
hmms	O
have	O
been	O
widely	O
applied	O
to	O
modelling	B
genetic	O
sequences	B
multiple	O
sequence	O
alignment	O
using	O
forms	O
of	O
constrained	O
hmms	O
have	O
been	O
particularly	O
successful	O
other	O
applications	O
involve	O
gene	O
finding	O
and	O
protein	O
family	B
part-of-speech	B
tagging	B
consider	O
the	O
sentence	O
below	O
in	O
which	O
each	O
word	O
has	O
been	O
linguistically	O
tagged	O
hospitality	O
nn	O
is	O
bez	O
an	O
at	O
excellent	O
jj	O
virtue	O
nn	O
but	O
cc	O
not	O
xnot	O
when	O
wrb	O
the	O
ati	O
guests	O
nns	O
have	O
hv	O
to	O
to	O
sleep	O
vb	O
in	O
in	O
rows	O
nns	O
in	O
in	O
the	O
ati	O
cellar	O
nn	O
the	O
subscripts	O
denote	O
a	O
linguistic	O
tag	O
for	O
example	O
nn	O
is	O
the	O
singular	B
common	O
noun	O
tag	O
ati	O
is	O
the	O
article	O
tag	O
etc	O
given	O
a	O
training	B
set	O
of	O
such	O
tagged	O
sequences	B
the	O
task	O
is	O
to	O
tag	O
a	O
novel	O
word	O
sequence	O
one	O
approach	B
is	O
to	O
use	O
ht	O
to	O
be	O
a	O
tag	O
and	O
vt	O
to	O
be	O
a	O
word	O
and	O
fit	O
a	O
hmm	B
to	O
this	O
data	O
for	O
the	O
training	B
data	O
both	O
the	O
tags	O
and	O
words	O
are	O
observed	O
so	O
that	O
maximum	B
likelihood	B
training	B
of	O
the	O
transition	O
and	O
emission	B
distribution	B
can	O
be	O
achieved	O
by	O
simple	O
counting	B
given	O
a	O
new	O
sequence	O
of	O
words	O
the	O
most	O
likely	O
tag	O
sequence	O
can	O
be	O
inferred	O
using	O
the	O
viterbi	B
algorithm	B
more	O
recent	O
part-of-speech	O
taggers	O
tend	O
to	O
use	O
conditional	B
random	O
fields	O
in	O
which	O
the	O
input	O
sequence	O
is	O
the	O
sentence	O
and	O
the	O
output	O
sequence	O
is	O
the	O
tag	O
sequence	O
one	O
possible	O
parameterisation	B
of	O
for	O
a	O
linear	B
chain	B
crf	O
is	O
to	O
use	O
a	O
potential	B
of	O
the	O
form	O
yt	O
x	O
in	O
which	O
the	O
first	O
factor	B
encodes	O
the	O
grammatical	O
structure	B
of	O
the	O
language	O
and	O
the	O
second	O
the	O
a	O
priori	O
likely	O
tag	O
draft	O
march	O
exercises	O
code	O
demomixmarkov	O
m	O
demo	O
for	O
mixture	B
of	O
markov	O
models	O
mixmarkov	O
m	O
mixture	B
of	O
markov	O
models	O
demohmminference	O
m	O
demo	O
of	O
hmm	B
inference	B
hmmforward	O
m	O
forward	O
recursion	O
hmmbackward	O
m	O
forward	O
recursion	O
hmmgamma	O
m	O
rts	O
correction	O
recursion	O
hmmsmooth	O
m	O
single	O
and	O
pairwise	B
smoothing	B
hmmviterbi	O
m	O
most	B
likely	I
state	I
algorithm	B
demohmmburglar	O
m	O
demo	O
of	O
burglar	O
localisation	B
demohmmbigram	O
m	O
demo	O
of	O
stubby	O
fingers	O
typing	O
hmmem	O
m	O
em	B
algorithm	B
for	O
hmm	B
demohmmlearn	O
m	O
demo	O
of	O
em	B
algorithm	B
for	O
hmm	B
demolinearcrf	O
m	O
demo	O
of	O
learning	B
a	O
linear	B
chain	B
crf	O
the	O
following	O
linear	B
chain	B
crf	O
potential	B
is	O
particularly	O
simple	O
and	O
in	O
practice	O
one	O
would	O
use	O
a	O
more	O
complex	O
one	O
linearcrfpotential	O
m	O
linear	B
crf	O
potential	B
the	O
following	O
likelihood	B
and	O
gradient	B
routines	O
are	O
valid	O
for	O
any	O
linear	B
crf	O
potential	B
yt	O
x	O
linearcrfgrad	O
m	O
linear	B
crf	O
gradient	B
linearcrfloglik	O
m	O
linear	B
crf	O
log	O
likelihood	B
exercises	O
exercise	O
a	O
stochastic	O
matrix	B
mij	O
as	O
non-negative	O
entries	O
and	O
eigenvector	O
e	O
j	O
mijej	O
ei	O
by	O
summing	O
over	O
i	O
show	O
that	O
i	O
mij	O
consider	O
an	O
eigenvalue	O
i	O
ei	O
then	O
must	O
exercise	O
consider	O
the	O
markov	B
chain	B
with	O
transition	B
matrix	B
be	O
equal	O
to	O
m	O
show	O
that	O
this	O
markov	B
chain	B
does	O
not	O
have	O
an	O
equilibrium	O
distribution	B
and	O
state	O
a	O
stationary	B
distribution	B
for	O
this	O
chain	B
exercise	O
consider	O
a	O
hmm	B
with	O
states	O
and	O
output	O
symbols	O
with	O
a	O
left-to-right	O
state	O
transition	B
matrix	B
a	O
b	O
where	O
aij	O
pht	O
iht	O
j	O
emission	B
matrix	B
bij	O
pvt	O
iht	O
j	O
and	O
initial	O
state	O
probability	O
vector	O
a	O
given	O
the	O
observed	O
symbol	O
sequence	O
is	O
compute	O
compute	O
find	O
the	O
most	O
probable	O
hidden	B
state	O
sequence	O
arg	O
draft	O
march	O
exercises	O
exercise	O
this	O
exercise	O
follows	O
from	O
given	O
the	O
long	O
character	O
string	O
rgenmonleunosbpnntje	O
vrancg	O
typed	O
with	O
stubby	O
fingers	O
what	O
is	O
the	O
most	O
likely	O
correct	O
english	O
sentence	O
intended	O
in	O
the	O
list	O
of	O
decoded	O
sequences	B
what	O
value	B
is	O
log	O
for	O
this	O
sequence	O
you	O
will	O
need	O
to	O
modify	O
demohmmbigram	O
m	O
suitably	O
exercise	O
show	O
that	O
if	O
a	O
transition	O
probability	O
aij	O
pht	O
iht	O
j	O
in	O
a	O
hmm	B
is	O
initialised	O
to	O
zero	O
for	O
em	B
training	B
then	O
it	O
will	O
remain	O
at	O
zero	O
throughout	O
training	B
exercise	O
consider	O
the	O
problem	B
find	O
the	O
most	O
likely	O
joint	B
output	O
sequence	O
for	O
a	O
hmm	B
that	O
is	O
v	O
argmax	O
where	O
pvthtphtht	O
explain	O
why	O
a	O
local	B
message	B
passing	B
algorithm	B
cannot	O
in	O
general	O
be	O
found	O
for	O
this	O
problem	B
and	O
discuss	O
the	O
computational	B
complexity	I
of	O
finding	O
an	O
exact	O
solution	O
explain	O
how	O
to	O
adapt	O
the	O
expectation-maximisation	O
algorithm	B
to	O
form	O
a	O
recursive	O
algorithm	B
for	O
explain	O
which	O
it	O
guarantees	O
an	O
improved	O
solution	O
at	O
each	O
iteration	B
finding	O
an	O
approximate	B
v	O
additionally	O
explain	O
how	O
the	O
algorithm	B
can	O
be	O
implemented	O
using	O
local	B
message	B
passing	B
exercise	O
explain	O
how	O
to	O
train	O
a	O
hmm	B
using	O
em	B
but	O
with	O
a	O
constrained	O
transition	B
matrix	B
particular	O
explain	O
how	O
to	O
learn	O
a	O
transition	B
matrix	B
with	O
an	O
upper	O
triangular	O
structure	B
in	O
exercise	O
write	O
a	O
program	O
to	O
fit	O
a	O
mixture	B
of	O
lth	O
order	O
markov	O
models	O
exercise	O
using	O
the	O
correspondence	O
a	O
c	O
g	O
t	O
define	O
a	O
transition	B
matrix	B
p	O
that	O
produces	O
sequences	B
of	O
the	O
form	O
a	O
c	O
g	O
t	O
a	O
c	O
g	O
t	O
a	O
c	O
g	O
t	O
a	O
c	O
g	O
t	O
now	O
define	O
a	O
new	O
transition	B
matrix	B
pnew	O
define	O
a	O
transition	B
matrix	B
q	O
that	O
produces	O
sequences	B
of	O
the	O
form	O
t	O
g	O
c	O
a	O
t	O
g	O
c	O
a	O
t	O
g	O
c	O
a	O
t	O
g	O
c	O
a	O
now	O
define	O
a	O
new	O
transition	B
matrix	B
qnew	O
assume	O
that	O
the	O
probability	O
of	O
being	O
in	O
the	O
initial	O
state	O
of	O
the	O
markov	B
chain	B
is	O
constant	O
for	O
all	O
four	O
states	O
a	O
c	O
g	O
t	O
what	O
is	O
the	O
probability	O
that	O
the	O
markov	B
chain	B
pnew	O
generated	O
the	O
sequence	O
s	O
given	O
by	O
s	O
a	O
a	O
g	O
t	O
a	O
c	O
t	O
t	O
a	O
c	O
c	O
t	O
a	O
c	O
g	O
c	O
similarly	O
what	O
is	O
the	O
probability	O
that	O
s	O
was	O
generated	O
by	O
qnew	O
does	O
it	O
make	O
sense	O
that	O
s	O
has	O
a	O
higher	O
likelihood	B
under	O
pnew	O
compared	O
with	O
qnew	O
draft	O
march	O
using	O
the	O
function	B
randgen	O
m	O
generate	O
sequences	B
of	O
length	O
from	O
the	O
markov	B
chain	B
defined	O
by	O
pnew	O
similarly	O
generate	O
sequences	B
each	O
of	O
length	O
from	O
the	O
markov	B
chain	B
defined	O
by	O
qnew	O
concatenate	O
all	O
these	O
sequences	B
into	O
a	O
cell	O
array	O
v	O
so	O
that	O
contains	O
the	O
first	O
sequence	O
and	O
the	O
last	O
sequence	O
use	O
mixmarkov	O
m	O
to	O
learn	O
the	O
optimum	O
maximum	B
likelihood	B
parameters	O
that	O
generated	O
these	O
sequences	B
assume	O
that	O
there	O
are	O
h	O
kinds	O
of	O
markov	B
chain	B
the	O
result	O
returned	O
in	O
phgvn	O
indicate	O
the	O
posterior	B
probability	O
of	O
sequence	O
assignment	O
do	O
you	O
agree	O
with	O
the	O
solution	O
found	O
exercises	O
take	O
the	O
sequence	O
s	O
as	O
defined	O
in	O
equation	B
define	O
an	O
emission	B
distribution	B
that	O
has	O
output	O
states	O
such	O
that	O
pv	O
ih	O
j	O
i	O
j	O
i	O
j	O
using	O
this	O
emission	B
distribution	B
and	O
the	O
transition	O
given	O
by	O
pnew	O
defined	O
in	O
equation	B
adapt	O
demohmminferencesimple	O
m	O
suitably	O
to	O
find	O
the	O
most	O
likely	O
hidden	B
sequence	O
hp	O
that	O
generated	O
the	O
observed	O
sequence	O
s	O
repeat	O
this	O
computation	O
but	O
for	O
the	O
transition	O
qnew	O
to	O
give	O
hq	O
which	O
hidden	B
sequence	O
hp	O
is	O
to	O
be	O
preferred	O
justify	O
your	O
answer	O
or	O
hq	O
exercise	O
derive	O
an	O
algorithm	B
that	O
will	O
find	O
the	O
most	O
likely	O
joint	B
state	O
argmax	O
tht	O
ht	O
for	O
arbitrarily	O
defined	O
potentials	O
tht	O
ht	O
first	O
consider	O
max	O
tht	O
ht	O
t	O
t	O
derive	O
the	O
recursion	O
t	O
tht	O
max	O
ht	O
tht	O
ht	O
t	O
explain	O
how	O
the	O
above	O
recursion	O
enables	O
the	O
computation	O
of	O
argmax	O
tht	O
ht	O
show	O
that	O
how	O
the	O
maximisation	B
over	O
ht	O
may	O
be	O
pushed	O
inside	O
the	O
product	O
and	O
that	O
the	O
result	O
of	O
the	O
maximisation	B
can	O
be	O
interpreted	O
as	O
a	O
message	B
explain	O
how	O
once	O
the	O
most	B
likely	I
state	I
for	O
is	O
computed	O
one	O
may	O
efficiently	O
compute	O
the	O
remaining	O
optimal	O
states	O
ht	O
exercise	O
derive	O
an	O
algorithm	B
that	O
will	O
compute	O
pairwise	B
marginals	O
pht	O
ht	O
from	O
the	O
joint	B
distribution	B
tht	O
ht	O
for	O
arbitrarily	O
defined	O
potentials	O
tht	O
ht	O
draft	O
march	O
exercises	O
first	O
tht	O
ht	O
show	O
that	O
how	O
the	O
summation	O
over	O
may	O
be	O
pushed	O
inside	O
the	O
product	O
and	O
that	O
the	O
result	O
of	O
the	O
maximisation	B
can	O
be	O
interpreted	O
as	O
a	O
message	B
derive	O
the	O
recursion	O
tht	O
ht	O
t	O
t	O
similarly	O
show	O
that	O
one	O
can	O
push	O
the	O
summation	O
of	O
ht	O
inside	O
the	O
product	O
to	O
define	O
ht	O
t	O
tht	O
t	O
t	O
t	O
ht	O
pht	O
ht	O
t	O
ht	O
and	O
that	O
by	O
pushing	O
in	O
ht	O
etc	O
one	O
can	O
define	O
messages	O
show	O
that	O
t	O
t	O
ht	O
t	O
exercise	O
a	O
second	O
order	O
hmm	B
is	O
defined	O
as	O
phm	O
m	O
phtht	O
ht	O
following	O
a	O
similar	O
approach	B
to	O
the	O
first	O
order	O
hmm	B
derive	O
explicitly	O
a	O
message	B
passing	B
algorithm	B
to	O
compute	O
the	O
most	O
likely	O
joint	B
state	O
argmax	O
phm	O
m	O
exercise	O
since	O
the	O
likelihood	B
of	O
the	O
hmm	B
can	O
be	O
computed	O
using	O
filtering	O
only	O
in	O
principle	O
we	O
do	O
not	O
need	O
smoothing	B
to	O
maximise	O
the	O
likelihood	B
to	O
the	O
em	B
approach	B
explain	O
how	O
to	O
compute	O
the	O
likelihood	B
gradient	B
by	O
the	O
use	O
of	O
filtered	O
information	O
alone	O
using	O
only	O
a	O
forward	O
pass	O
exercise	O
derive	O
the	O
em	B
updates	O
for	O
fitting	O
a	O
hmm	B
with	O
an	O
emission	B
distribution	B
given	O
by	O
a	O
mixture	B
of	O
multi-variate	B
gaussians	O
exercise	O
consider	O
the	O
hmm	B
defined	O
on	O
hidden	B
variables	I
h	O
ht	O
and	O
observations	O
v	O
vt	O
pvh	O
phtht	O
show	O
that	O
the	O
posterior	B
phv	O
is	O
a	O
markov	B
chain	B
phv	O
phtht	O
where	O
phtht	O
and	O
are	O
suitably	O
defined	O
distributions	O
draft	O
march	O
exercise	O
for	O
training	B
a	O
hmm	B
with	O
a	O
gaussian	B
mixture	B
emission	O
hmm-gmm	O
model	B
in	O
derive	O
the	O
following	O
em	B
update	O
formulae	O
for	O
the	O
means	O
and	O
covariances	O
new	O
kh	O
new	O
kh	O
kht	O
n	O
n	O
and	O
where	O
kht	O
nvn	O
t	O
kht	O
qkt	O
khn	O
t	O
qkt	O
khn	O
t	O
kh	O
t	O
kh	O
t	O
hqhn	O
t	O
hqhn	O
t	O
hvn	O
t	O
hvn	O
exercises	O
exercise	O
consider	O
the	O
hmm	B
duration	B
model	B
defined	O
by	O
equation	B
and	O
equation	B
with	O
emission	B
distribution	B
pvtht	O
our	O
interest	O
is	O
to	O
derive	O
a	O
recursion	O
for	O
the	O
filtered	O
distribution	B
tht	O
ct	O
pht	O
ct	O
show	O
that	O
tht	O
ct	O
pvtht	O
ht	O
phtht	O
ctpctct	O
t	O
ct	O
using	O
this	O
derive	O
ht	O
tht	O
ct	O
pvtht	O
phtht	O
cpctct	O
t	O
ct	O
phtht	O
c	O
ht	O
ct	O
pcct	O
t	O
ct	O
show	O
that	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
can	O
be	O
written	O
as	O
ht	O
phtht	O
ct	O
cpct	O
cct	O
t	O
i	O
ht	O
phtht	O
c	O
t	O
c	O
show	O
that	O
the	O
recursion	O
for	O
is	O
then	O
given	O
by	O
th	O
pvtht	O
ht	O
ptranhht	O
t	O
i	O
pvtht	O
ht	O
ptranhtht	O
t	O
and	O
for	O
c	O
th	O
c	O
pvtht	O
hpdurc	O
t	O
i	O
dmax	O
t	O
c	O
explain	O
why	O
the	O
computational	B
complexity	I
of	O
filtered	O
inference	B
in	O
the	O
duration	B
model	B
is	O
h	O
derive	O
an	O
efficient	O
smoothing	B
algorithm	B
for	O
this	O
duration	B
model	B
draft	O
march	O
chapter	O
continuous-state	O
markov	O
models	O
observed	O
linear	B
dynamical	O
systems	O
in	O
many	O
practical	O
timeseries	O
applications	O
the	O
data	O
is	O
naturally	O
continuous	B
particularly	O
for	O
models	O
of	O
the	O
physical	O
environment	O
in	O
contrast	O
to	O
discrete-state	O
markov	O
models	O
continuous	B
state	O
distributions	O
are	O
not	O
automatically	O
closed	O
under	O
operations	O
such	O
as	O
products	O
and	O
marginalisation	B
to	O
make	O
practical	O
algorithms	O
for	O
which	O
inference	B
and	O
learning	B
can	O
be	O
carried	O
efficiently	O
we	O
therefore	O
are	O
heavily	O
restricted	B
in	O
the	O
form	O
of	O
the	O
continuous	B
transition	O
pvtvt	O
a	O
simple	O
yet	O
powerful	O
class	O
of	O
such	O
transitions	O
are	O
the	O
linear	B
dynamical	O
systems	O
a	O
deterministic	B
observed	O
linear	B
dynamical	O
defines	O
the	O
temporal	O
evolution	O
of	O
a	O
vector	O
vt	O
according	O
to	O
the	O
discrete-time	O
update	O
equation	B
vt	O
atvt	O
where	O
at	O
is	O
the	O
transition	B
matrix	B
at	O
time	O
t	O
for	O
the	O
case	O
that	O
at	O
is	O
invariant	O
with	O
t	O
the	O
process	O
is	O
called	O
time-invariant	O
which	O
we	O
assume	O
throughout	O
unless	O
explicitly	O
stated	O
otherwise	O
a	O
motivation	O
for	O
studying	O
oldss	O
is	O
that	O
many	O
equations	O
that	O
describe	O
the	O
physical	O
world	O
can	O
be	O
written	O
as	O
an	O
olds	O
oldss	O
are	O
interesting	O
since	O
they	O
may	O
be	O
used	O
as	O
simple	O
prediction	O
models	O
if	O
vt	O
describes	O
the	O
state	O
of	O
the	O
environment	O
at	O
time	O
t	O
then	O
avt	O
predicts	O
the	O
environment	O
at	O
time	O
as	O
such	O
these	O
models	O
have	O
widespread	O
application	O
in	O
many	O
branches	O
of	O
science	O
from	O
engineering	O
and	O
physics	O
to	O
economics	O
the	O
olds	O
equation	B
is	O
deterministic	B
so	O
that	O
if	O
we	O
specify	O
all	O
future	O
values	O
are	O
defined	O
for	O
a	O
dim	O
v	O
v	O
dimensional	O
vector	O
its	O
evolution	O
is	O
described	O
by	O
vt	O
at	O
p	O
t	O
where	O
diag	O
v	O
is	O
the	O
diagonal	O
eigenvalue	O
matrix	B
and	O
p	O
is	O
the	O
corresponding	O
eigenvector	O
matrix	B
of	O
a	O
if	O
i	O
then	O
for	O
large	O
t	O
vt	O
will	O
explode	O
on	O
the	O
other	O
hand	O
if	O
i	O
then	O
t	O
i	O
will	O
tend	O
to	O
zero	O
for	O
stable	O
systems	O
we	O
require	O
therefore	O
no	O
eigenvalues	O
of	O
magnitude	O
greater	O
than	O
and	O
only	O
unit	O
eigenvalues	O
will	O
contribute	O
in	O
long	O
term	O
note	O
that	O
the	O
eigenvalues	O
may	O
be	O
complex	O
which	O
corresponds	O
to	O
rotational	O
behaviour	O
see	O
more	O
generally	O
we	O
may	O
consider	O
additive	O
noise	O
on	O
v	O
and	O
define	O
a	O
stochastic	O
olds	O
definition	O
linear	B
dynamical	I
system	I
vt	O
atvt	O
t	O
use	O
the	O
terminology	O
observed	O
lds	O
to	O
differentiate	O
from	O
the	O
more	O
general	O
lds	O
state-space	O
model	B
in	O
some	O
texts	O
however	O
the	O
term	O
lds	O
is	O
applied	O
to	O
the	O
models	O
under	O
discussion	O
in	O
this	O
chapter	O
where	O
t	O
is	O
a	O
noise	O
vector	O
sampled	O
from	O
a	O
gaussian	B
distribution	B
n	O
t	O
t	O
t	O
this	O
is	O
equivalent	B
to	O
a	O
first	O
order	O
markov	O
model	B
pvtvt	O
n	O
atvt	O
t	O
t	O
auto-regressive	B
models	O
at	O
t	O
we	O
have	O
an	O
initial	O
distribution	B
n	O
for	O
t	O
if	O
the	O
parameters	O
are	O
timeindependent	O
t	O
at	O
a	O
t	O
the	O
process	O
is	O
called	O
time-invariant	O
stationary	B
distribution	B
with	O
noise	O
consider	O
the	O
one-dimensional	O
linear	B
system	O
t	O
v	O
vt	O
avt	O
t	O
t	O
n	O
if	O
we	O
start	O
at	O
some	O
state	O
and	O
then	O
for	O
t	O
recursively	O
sample	O
according	O
to	O
vt	O
avt	O
t	O
does	O
the	O
distribution	B
of	O
the	O
vt	O
t	O
tend	O
to	O
a	O
steady	O
fixed	O
distribution	B
assuming	O
that	O
we	O
can	O
represent	O
the	O
distribution	B
of	O
vt	O
as	O
a	O
gaussian	B
with	O
mean	B
t	O
and	O
variance	B
using	O
we	O
have	O
t	O
vt	O
n	O
t	O
then	O
t	O
t	O
t	O
t	O
a	O
t	O
a	O
t	O
t	O
t	O
v	O
t	O
v	O
t	O
vt	O
n	O
so	O
that	O
assuming	O
there	O
is	O
a	O
fixed	O
variance	B
for	O
the	O
infinite	O
time	O
case	O
the	O
stationary	B
distribution	B
satisfies	O
v	O
v	O
similarly	O
the	O
mean	B
is	O
given	O
by	O
a	O
if	O
a	O
the	O
variance	B
mean	B
increases	O
indefinitely	O
with	O
t	O
for	O
a	O
the	O
mean	B
tends	O
to	O
zero	O
yet	O
the	O
variance	B
remains	O
finite	O
even	O
though	O
the	O
magnitude	O
of	O
vt	O
is	O
decreased	O
by	O
a	O
factor	B
of	O
a	O
at	O
each	O
iteration	B
the	O
additive	O
noise	O
on	O
average	B
boosts	O
the	O
magnitude	O
so	O
that	O
it	O
remains	O
steady	O
in	O
the	O
long	O
run	O
more	O
generally	O
for	O
a	O
system	O
updating	O
a	O
vector	O
vt	O
according	O
to	O
vt	O
avt	O
t	O
for	O
the	O
existence	O
of	O
a	O
steady	O
state	O
we	O
require	O
that	O
all	O
eigenvalues	O
of	O
a	O
must	O
be	O
auto-regressive	B
models	O
a	O
scalar	O
time-invariant	O
auto-regressive	B
model	B
is	O
defined	O
by	O
vt	O
alvt	O
l	O
t	O
t	O
n	O
t	O
where	O
a	O
alt	O
are	O
called	O
the	O
ar	O
coefficients	O
and	O
is	O
called	O
the	O
innovation	B
noise	I
the	O
model	B
predicts	O
the	O
future	O
based	O
on	O
a	O
linear	B
combination	O
of	O
the	O
previous	O
l	O
observations	O
as	O
a	O
belief	B
network	I
the	O
ar	O
model	B
can	O
be	O
written	O
as	O
an	O
lth	O
order	O
markov	O
model	B
pvtvt	O
vt	O
l	O
with	O
vi	O
for	O
i	O
draft	O
march	O
auto-regressive	B
models	O
figure	O
fitting	O
an	O
order	O
ar	O
model	B
to	O
the	O
training	B
points	O
the	O
x	O
axis	O
represents	O
time	O
and	O
the	O
y	O
axis	O
the	O
value	B
of	O
the	O
timeseries	O
the	O
solid	O
line	O
is	O
the	O
mean	B
prediction	O
and	O
the	O
dashed	O
lines	O
one	O
standard	B
deviation	I
see	O
demoartrain	O
m	O
with	O
pvtvt	O
vt	O
l	O
n	O
vt	O
alvt	O
l	O
introducing	O
the	O
vector	O
of	O
the	O
l	O
previous	O
observations	O
vt	O
vt	O
vt	O
lt	O
we	O
can	O
write	O
more	O
compactly	O
pvtvt	O
vt	O
l	O
n	O
vt	O
at	O
vt	O
ar	O
models	O
are	O
heavily	O
used	O
in	O
financial	O
time-series	O
prediction	O
for	O
example	O
being	O
able	O
to	O
capture	O
simple	O
trends	O
in	O
the	O
data	O
another	O
common	O
application	O
area	O
is	O
in	O
speech	O
processing	O
whereby	O
for	O
a	O
onedimensional	O
speech	O
signal	O
partitioned	B
into	O
windows	O
of	O
length	O
t	O
the	O
ar	O
coefficients	O
best	O
able	O
to	O
describe	O
the	O
signal	O
in	O
each	O
window	O
are	O
these	O
ar	O
coefficients	O
then	O
form	O
a	O
compressed	O
representation	O
of	O
the	O
signal	O
and	O
subsequently	O
transmitted	O
for	O
each	O
window	O
rather	O
than	O
the	O
original	O
signal	O
itself	O
the	O
signal	O
can	O
then	O
be	O
approximately	O
reconstructed	O
based	O
on	O
the	O
ar	O
coefficients	O
such	O
a	O
representation	O
is	O
used	O
for	O
example	O
in	O
telephones	O
and	O
known	O
as	O
a	O
linear	B
predictive	O
training	B
an	O
ar	O
model	B
maximum	B
likelihood	B
training	B
of	O
the	O
ar	O
coefficients	O
is	O
straightforward	O
based	O
on	O
log	O
vt	O
vt	O
differentiating	O
w	O
r	O
t	O
a	O
and	O
equating	O
to	O
zero	O
we	O
arrive	O
at	O
log	O
pvt	O
vt	O
t	O
t	O
t	O
t	O
t	O
vt	O
vt	O
vt	O
so	O
that	O
optimally	O
t	O
a	O
vt	O
vt	O
t	O
vt	O
vt	O
t	O
vt	O
vt	O
t	O
these	O
equations	O
can	O
be	O
solved	O
by	O
gaussian	B
elimination	O
similarly	O
optimally	O
above	O
we	O
assume	O
that	O
negative	O
timepoints	O
are	O
available	O
in	O
order	O
to	O
keep	O
the	O
notation	O
simple	O
if	O
times	O
before	O
the	O
window	O
over	O
which	O
we	O
learn	O
the	O
coefficients	O
are	O
not	O
available	O
a	O
minor	O
adjustment	O
is	O
required	O
to	O
start	O
the	O
summations	O
from	O
t	O
l	O
given	O
a	O
trained	O
a	O
future	O
predictions	O
can	O
be	O
made	O
using	O
vt	O
capturing	O
the	O
trend	O
in	O
the	O
data	O
t	O
a	O
as	O
we	O
see	O
the	O
model	B
is	O
capable	O
of	O
draft	O
march	O
auto-regressive	B
models	O
figure	O
a	O
time-varying	B
ar	O
model	B
as	O
a	O
latent	B
lds	O
since	O
the	O
observations	O
are	O
known	O
this	O
model	B
is	O
a	O
time-varying	B
latent	B
lds	O
for	O
which	O
smoothed	O
inference	B
determines	O
the	O
time-varying	B
ar	O
coefficients	O
ar	O
model	B
as	O
an	O
olds	O
we	O
can	O
write	O
equation	B
as	O
an	O
olds	O
using	O
vt	O
vt	O
vt	O
vt	O
vt	O
vt	O
l	O
t	O
al	O
we	O
can	O
write	O
equation	B
as	O
the	O
olds	O
t	O
n	O
t	O
vt	O
a	O
vt	O
t	O
where	O
we	O
define	O
the	O
block	O
matrices	O
al	O
i	O
a	O
in	O
this	O
representation	O
the	O
first	O
component	O
of	O
the	O
vector	O
is	O
updated	O
according	O
to	O
the	O
standard	O
ar	O
model	B
with	O
the	O
remaining	O
components	O
being	O
copies	O
of	O
the	O
previous	O
values	O
time-varying	B
ar	O
model	B
an	O
alternative	O
to	O
maximum	B
likelihood	B
is	O
to	O
view	O
learning	B
the	O
ar	O
coefficients	O
as	O
a	O
problem	B
in	O
inference	B
in	O
a	O
latent	B
lds	O
a	O
model	B
which	O
is	O
discussed	O
in	O
detail	O
in	O
if	O
at	O
are	O
the	O
latent	B
ar	O
coefficients	O
the	O
term	O
vt	O
vt	O
t	O
t	O
t	O
n	O
can	O
be	O
viewed	O
as	O
the	O
emission	B
distribution	B
of	O
a	O
latent	B
lds	O
in	O
which	O
the	O
hidden	B
variable	I
is	O
at	O
and	O
the	O
time-dependent	O
emission	B
matrix	B
is	O
given	O
by	O
vt	O
t	O
by	O
placing	O
a	O
simple	O
latent	B
transition	O
t	O
a	O
t	O
we	O
encourage	O
the	O
ar	O
coefficients	O
to	O
change	O
slowly	O
with	O
time	O
this	O
defines	O
a	O
model	B
a	O
t	O
n	O
at	O
at	O
a	O
t	O
pvtat	O
vt	O
t	O
our	O
interest	O
is	O
then	O
in	O
the	O
conditional	B
from	O
which	O
we	O
can	O
compute	O
the	O
a-posteriori	O
most	O
likely	O
sequence	O
of	O
ar	O
coefficients	O
standard	O
smoothing	B
algorithms	O
can	O
then	O
be	O
applied	O
to	O
yield	O
the	O
time-varying	B
ar	O
coefficients	O
see	O
demoarlds	O
m	O
n	O
definition	O
fourier	O
transform	O
for	O
a	O
sequence	O
the	O
dft	O
is	O
defined	O
as	O
fk	O
i	O
n	O
kn	O
xne	O
k	O
n	O
fk	O
is	O
a	O
representation	O
as	O
to	O
how	O
much	O
frequency	O
k	O
is	O
present	O
in	O
the	O
sequence	O
the	O
power	O
of	O
component	O
k	O
is	O
defined	O
as	O
the	O
absolute	O
length	O
of	O
the	O
complex	O
fk	O
draft	O
march	O
auto-regressive	B
models	O
spectrogram	B
of	O
up	O
to	O
hz	O
figure	O
the	O
raw	O
recording	O
of	O
seconds	O
of	O
a	O
nightingale	O
song	O
additional	O
background	O
clustering	B
of	O
the	O
results	O
in	O
panel	O
using	O
birdsong	O
an	O
component	O
gaussian	B
mixture	B
model	B
the	O
index	O
to	O
of	O
the	O
component	O
most	O
probably	O
responsible	O
for	O
the	O
observation	O
is	O
indicated	O
vertically	O
in	O
black	O
the	O
ar	O
coefficients	O
learned	O
using	O
clustering	B
the	O
results	O
in	O
panel	O
using	O
a	O
gaussian	B
mixture	B
v	O
model	B
with	O
components	O
the	O
ar	O
components	O
group	O
roughly	O
according	O
to	O
the	O
different	O
song	O
regimes	O
h	O
see	O
arlds	O
m	O
definition	O
given	O
a	O
timeseries	O
the	O
spectrogram	B
at	O
time	O
t	O
is	O
a	O
representation	O
of	O
the	O
frequencies	O
present	O
in	O
a	O
window	O
localised	O
around	O
t	O
for	O
each	O
window	O
one	O
computes	O
the	O
discrete	B
fourier	O
transform	O
from	O
which	O
we	O
obtain	O
a	O
vector	O
of	O
log	O
power	O
in	O
each	O
frequency	O
the	O
window	O
is	O
then	O
moved	O
one	O
step	O
forward	O
and	O
the	O
dft	O
recomputed	O
note	O
that	O
by	O
taking	O
the	O
logarithm	O
small	O
values	O
in	O
the	O
original	O
signal	O
can	O
translate	O
to	O
visibly	O
appreciable	O
values	O
in	O
the	O
spectrogram	B
example	O
in	O
we	O
plot	O
the	O
raw	O
acoustic	O
recording	O
for	O
a	O
second	O
fragment	O
of	O
a	O
nightingale	O
song	O
the	O
spectrogram	B
is	O
also	O
plotted	O
and	O
gives	O
an	O
indication	O
of	O
which	O
frequencies	O
are	O
present	O
in	O
the	O
signal	O
as	O
a	O
function	B
of	O
time	O
the	O
nightingale	O
song	O
is	O
very	O
complicated	O
but	O
at	O
least	O
locally	O
can	O
be	O
very	O
repetitive	O
a	O
crude	O
way	O
to	O
find	O
which	O
segments	O
repeat	O
is	O
to	O
form	O
a	O
cluster	O
analysis	B
of	O
the	O
spectrogram	B
in	O
we	O
show	O
the	O
results	O
of	O
fitting	O
a	O
gaussian	B
mixture	B
model	B
with	O
components	O
from	O
which	O
we	O
see	O
there	O
is	O
some	O
repetition	O
of	O
components	O
locally	O
in	O
time	O
an	O
alternative	O
representation	O
of	O
the	O
signal	O
is	O
given	O
by	O
the	O
time-varying	B
ar	O
coefficients	O
as	O
plotted	O
in	O
a	O
gmm	O
clustering	B
with	O
components	O
draft	O
march	O
latent	B
linear	B
dynamical	O
systems	O
figure	O
a	O
lds	O
both	O
hidden	B
and	O
visible	B
variables	O
are	O
gaussian	B
distributed	O
in	O
this	O
case	O
produces	O
a	O
somewhat	O
clearer	O
depiction	O
of	O
the	O
different	O
phases	O
of	O
the	O
nightingale	O
singing	O
than	O
that	O
afforded	O
by	O
the	O
spectrogram	B
latent	B
linear	B
dynamical	O
systems	O
the	O
latent	B
lds	O
defines	O
a	O
stochastic	O
linear	B
dynamical	I
system	I
in	O
a	O
latent	B
hidden	B
space	O
on	O
a	O
sequence	O
of	O
vectors	O
each	O
observation	O
vt	O
is	O
as	O
linear	B
function	B
of	O
the	O
latent	B
vector	O
ht	O
this	O
model	B
is	O
also	O
called	O
a	O
linear	B
gaussian	B
state	I
space	I
model	B
the	O
model	B
can	O
also	O
be	O
considered	O
a	O
form	O
of	O
lds	O
on	O
the	O
joint	B
variables	O
xt	O
ht	O
with	O
parts	O
of	O
the	O
vector	O
xt	O
missing	B
for	O
this	O
reason	O
we	O
will	O
also	O
refer	O
to	O
this	O
model	B
as	O
a	O
linear	B
dynamical	I
system	I
the	O
latent	B
prefix	O
t	O
are	O
noise	O
vectors	O
at	O
is	O
called	O
the	O
transition	B
matrix	B
and	O
bt	O
the	O
emission	B
matrix	B
the	O
where	O
h	O
terms	O
ht	O
and	O
vt	O
are	O
the	O
hidden	B
and	O
output	O
bias	B
respectively	O
the	O
transition	O
and	O
emission	O
models	O
define	O
a	O
first	O
order	O
markov	O
model	B
definition	O
linear	B
dynamical	I
system	I
h	O
v	O
ht	O
h	O
t	O
t	O
t	O
vt	O
v	O
t	O
transition	O
model	B
emission	O
model	B
ht	O
atht	O
h	O
vt	O
btht	O
v	O
t	O
t	O
and	O
v	O
t	O
h	O
t	O
n	O
v	O
t	O
n	O
atht	O
ht	O
h	O
btht	O
vt	O
v	O
t	O
t	O
phtht	O
with	O
the	O
transitions	O
and	O
emissions	O
given	O
by	O
gaussian	B
distributions	O
phtht	O
n	O
pvtht	O
n	O
n	O
this	O
lds	O
can	O
be	O
represented	O
as	O
a	O
belief	O
networkin	O
with	O
the	O
extension	O
to	O
higher	O
orders	O
being	O
intuitive	O
one	O
may	O
also	O
include	O
an	O
external	O
input	O
ot	O
at	O
each	O
time	O
which	O
will	O
add	O
cot	O
to	O
the	O
mean	B
of	O
the	O
hidden	B
variable	I
and	O
dot	O
to	O
the	O
mean	B
of	O
the	O
observation	O
h	O
explicit	O
expressions	O
for	O
the	O
transition	O
and	O
emission	O
distributions	O
are	O
given	O
below	O
for	O
the	O
time-invariant	O
case	O
with	O
vt	O
ht	O
each	O
hidden	B
variable	I
is	O
a	O
multidimensional	O
gaussian	B
distributed	O
vector	O
ht	O
with	O
phtht	O
exp	O
aht	O
h	O
aht	O
which	O
states	O
that	O
has	O
a	O
mean	B
equal	O
to	O
aht	O
with	O
gaussian	B
fluctuations	O
described	O
by	O
the	O
covariance	B
matrix	B
h	O
similarly	O
pvtht	O
v	O
exp	O
bhtt	O
v	O
bht	O
describes	O
an	O
output	O
vt	O
with	O
mean	B
bht	O
and	O
covariance	B
v	O
models	O
are	O
also	O
often	O
called	O
kalman	O
filters	O
we	O
avoid	O
this	O
terminology	O
here	O
since	O
the	O
word	O
filter	O
refers	O
to	O
a	O
specific	O
kind	O
of	O
inference	B
and	O
runs	O
the	O
risk	B
of	O
confusing	O
a	O
filtering	O
algorithm	B
with	O
the	O
model	B
itself	O
draft	O
march	O
inference	B
figure	O
a	O
single	O
phasor	O
plotted	O
as	O
a	O
damped	O
two	O
dimensional	O
rotation	O
r	O
ht	O
with	O
a	O
damping	O
factor	B
by	O
taking	O
a	O
projection	B
onto	O
the	O
y	O
axis	O
the	O
phasor	O
generates	O
a	O
damped	O
sinusoid	O
example	O
consider	O
a	O
dynamical	O
system	O
defined	O
on	O
two	O
dimensional	O
vectors	O
ht	O
cos	O
sin	O
cos	O
sin	O
r	O
ht	O
with	O
r	O
r	O
rotates	O
the	O
vector	O
ht	O
through	O
angle	O
in	O
one	O
timestep	O
under	O
this	O
lds	O
h	O
will	O
trace	O
out	O
points	O
on	O
a	O
circle	O
through	O
time	O
by	O
taking	O
a	O
scalar	O
projection	B
of	O
ht	O
for	O
example	O
vt	O
the	O
elements	O
vt	O
t	O
t	O
describe	O
a	O
sinusoid	O
through	O
time	O
see	O
by	O
using	O
a	O
block	O
diagonal	O
r	O
blkdiag	O
r	O
m	O
and	O
taking	O
a	O
scalar	O
projection	B
of	O
the	O
extended	O
m	O
dimensional	O
h	O
vector	O
one	O
can	O
construct	O
a	O
representation	O
of	O
a	O
signal	O
in	O
terms	O
of	O
m	O
sinusoidal	O
components	O
inference	B
given	O
an	O
observation	O
sequence	O
we	O
wish	O
to	O
consider	O
filtering	O
and	O
smoothing	B
as	O
we	O
did	O
for	O
the	O
hmm	B
for	O
the	O
hmm	B
in	O
deriving	O
the	O
various	O
message	B
passing	B
recursions	O
we	O
used	O
only	O
the	O
independence	B
structure	B
encoded	O
by	O
the	O
belief	B
network	I
since	O
the	O
lds	O
has	O
the	O
same	O
independence	B
structure	B
as	O
the	O
hmm	B
we	O
can	O
use	O
the	O
same	O
independence	B
assumptions	O
in	O
deriving	O
the	O
updates	O
for	O
the	O
lds	O
however	O
in	O
implementing	O
them	O
we	O
need	O
to	O
deal	O
with	O
the	O
issue	O
that	O
we	O
now	O
have	O
continuous	B
hidden	B
variables	I
rather	O
than	O
discrete	B
states	O
the	O
fact	O
that	O
the	O
distributions	O
are	O
gaussian	B
means	O
that	O
we	O
can	O
deal	O
with	O
continuous	B
messages	O
exactly	O
in	O
translating	O
the	O
hmm	B
message	B
passing	B
equations	O
we	O
first	O
replace	O
summation	O
with	O
integration	O
for	O
example	O
the	O
filtering	O
recursion	O
becomes	O
ht	O
pvthtphtht	O
t	O
since	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
another	O
gaussian	B
and	O
the	O
integral	O
of	O
a	O
gaussian	B
is	O
another	O
gaussian	B
the	O
resulting	O
is	O
also	O
gaussian	B
this	O
closure	O
property	O
of	O
gaussians	O
means	O
that	O
we	O
may	O
represent	O
pht	O
n	O
ft	O
ft	O
with	O
mean	B
ft	O
and	O
covariance	B
ft	O
the	O
effect	O
of	O
equation	B
is	O
equivalent	B
to	O
updating	O
the	O
mean	B
ft	O
and	O
covariance	B
ft	O
into	O
a	O
mean	B
ft	O
and	O
covariance	B
ft	O
for	O
our	O
task	O
below	O
is	O
to	O
find	O
explicit	O
algebraic	O
formulae	O
for	O
these	O
updates	O
numerical	B
stability	I
translating	O
the	O
message	B
passing	B
inference	B
techniques	O
we	O
developed	O
for	O
the	O
hmm	B
into	O
the	O
lds	O
is	O
largely	O
straightforward	O
indeed	O
one	O
could	O
simply	O
run	O
a	O
standard	O
sum-product	B
algorithm	B
for	O
continuous	B
variables	O
see	O
demosumprodgausscanonlds	O
m	O
in	O
long	O
timeseries	O
numerical	B
instabilities	O
can	O
build	O
up	O
and	O
may	O
result	O
in	O
grossly	O
inaccurate	O
results	O
depending	O
on	O
the	O
transition	O
and	O
emission	B
distribution	B
parameters	O
and	O
the	O
method	O
of	O
implementing	O
the	O
message	B
updates	O
for	O
this	O
reason	O
specialised	O
routines	O
have	O
been	O
developed	O
that	O
are	O
reasonably	O
numerically	O
stable	O
under	O
certain	O
parameter	B
for	O
the	O
hmm	B
in	O
we	O
discussed	O
two	O
alternative	O
methods	O
for	O
smoothing	B
the	O
parallel	B
approach	B
and	O
the	O
sequential	B
approach	B
the	O
recursion	O
is	O
suitable	O
when	O
the	O
emission	O
and	O
transition	O
covariance	B
entries	O
are	O
small	O
and	O
the	O
recursion	O
usually	O
preferable	O
in	O
the	O
more	O
standard	O
case	O
of	O
small	O
covariance	B
values	O
draft	O
march	O
inference	B
analytical	O
shortcuts	O
in	O
deriving	O
the	O
inference	B
recursions	O
we	O
need	O
to	O
frequently	O
multiply	O
and	O
integrate	O
gaussians	O
whilst	O
in	O
principle	O
straightforward	O
this	O
can	O
be	O
algebraically	O
tedious	O
and	O
wherever	O
possible	O
it	O
is	O
useful	O
to	O
appeal	O
to	O
known	O
shortcuts	O
for	O
example	O
one	O
can	O
exploit	O
the	O
general	O
result	O
that	O
the	O
linear	B
transform	O
of	O
a	O
gaussian	B
random	O
variable	O
is	O
another	O
gaussian	B
random	O
variable	O
similarly	O
it	O
is	O
convenient	O
to	O
make	O
use	O
of	O
the	O
conditioning	B
formulae	O
as	O
well	O
as	O
the	O
dynamics	B
reversal	I
intuition	O
these	O
results	O
are	O
stated	O
in	O
and	O
below	O
we	O
derive	O
the	O
most	O
useful	O
for	O
our	O
purposes	O
here	O
consider	O
a	O
linear	B
transformation	I
of	O
a	O
gaussian	B
random	O
variable	O
x	O
n	O
x	O
x	O
n	O
y	O
mx	O
where	O
x	O
and	O
are	O
assumed	O
to	O
be	O
generated	O
from	O
independent	O
processes	O
to	O
find	O
the	O
distribution	B
py	O
one	O
approach	B
would	O
be	O
to	O
write	O
this	O
formally	O
as	O
py	O
n	O
mx	O
x	O
x	O
dx	O
and	O
carry	O
out	O
the	O
integral	O
completing	O
the	O
square	O
however	O
since	O
a	O
gaussian	B
variable	O
under	O
linear	B
transformation	I
is	O
another	O
gaussian	B
we	O
can	O
take	O
a	O
shortcut	O
and	O
just	O
find	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
transformed	O
variable	O
its	O
mean	B
is	O
given	O
by	O
m	O
x	O
to	O
find	O
the	O
covariance	B
consider	O
the	O
displacement	O
of	O
a	O
variable	O
h	O
from	O
its	O
mean	B
which	O
we	O
write	O
as	O
the	O
covariance	B
is	O
by	O
h	O
for	O
y	O
the	O
displacement	O
is	O
h	O
h	O
y	O
m	O
x	O
y	O
so	O
that	O
the	O
covariance	B
is	O
x	O
x	O
x	O
x	O
since	O
the	O
noises	O
and	O
x	O
are	O
assumed	O
we	O
have	O
mt	O
m	O
mt	O
m	O
y	O
m	O
xmt	O
filtering	O
we	O
represent	O
the	O
filtered	O
distribution	B
as	O
a	O
gaussian	B
with	O
mean	B
ft	O
and	O
covariance	B
ft	O
n	O
ft	O
ft	O
this	O
is	O
called	O
the	O
moment	B
representation	I
our	O
task	O
is	O
then	O
to	O
find	O
a	O
recursion	O
for	O
ft	O
ft	O
in	O
terms	O
of	O
ft	O
ft	O
a	O
convenient	O
approach	B
is	O
to	O
first	O
find	O
the	O
joint	B
distribution	B
pht	O
and	O
then	O
condition	O
on	O
vt	O
to	O
find	O
the	O
distribution	B
the	O
term	O
pht	O
is	O
a	O
gaussian	B
whose	O
statistics	O
can	O
be	O
found	O
from	O
the	O
relations	O
vt	O
bht	O
v	O
t	O
ht	O
aht	O
h	O
t	O
using	O
the	O
above	O
and	O
assuming	O
time-invariance	O
and	O
zero	O
biases	O
we	O
readily	O
find	O
ht	O
ht	O
t	O
a	O
ht	O
ht	O
t	O
at	O
h	O
aft	O
h	O
draft	O
march	O
inference	B
parameters	O
t	O
b	O
h	O
v	O
h	O
algorithm	B
lds	O
forward	O
pass	O
compute	O
the	O
filtered	O
posteriors	O
n	O
ft	O
for	O
a	O
lds	O
with	O
t	O
the	O
log-likelihood	O
l	O
log	O
is	O
also	O
returned	O
t	O
l	O
log	O
for	O
t	O
t	O
do	O
ft	O
pt	O
ldsforwardft	O
ft	O
vt	O
l	O
l	O
log	O
pt	O
end	O
for	O
function	B
ldsforwardf	O
f	O
v	O
mean	B
of	O
pht	O
covariance	B
of	O
pht	O
find	O
by	O
conditioning	B
compute	O
bt	O
v	O
aft	O
h	O
vv	O
vh	O
vh	O
vv	O
v	O
h	O
af	O
h	O
v	O
b	O
h	O
v	O
hh	O
afat	O
h	O
vv	O
b	O
hhbt	O
v	O
vh	O
b	O
hh	O
vh	O
vv	O
v	O
h	O
t	O
vt	O
exp	O
return	O
end	O
function	B
vv	O
hh	O
t	O
in	O
the	O
above	O
using	O
our	O
moment	B
representation	I
of	O
the	O
forward	O
messages	O
vt	O
vt	O
ht	O
ht	O
t	O
t	O
t	O
t	O
aft	O
h	O
bt	O
v	O
b	O
ht	O
ht	O
vt	O
ht	O
b	O
ft	O
then	O
using	O
phtvt	O
will	O
have	O
mean	B
t	O
t	O
ft	O
t	O
and	O
covariance	B
ht	O
ht	O
ht	O
vt	O
vt	O
vt	O
ft	O
b	O
b	O
ht	O
vt	O
vt	O
vt	O
t	O
vt	O
ht	O
t	O
writing	O
out	O
the	O
above	O
explicitly	O
we	O
have	O
for	O
the	O
mean	B
bpbt	O
v	O
t	O
baft	O
bpbt	O
v	O
bp	O
ft	O
ht	O
ht	O
t	O
ft	O
aft	O
ft	O
p	O
h	O
and	O
covariance	B
where	O
p	O
aft	O
h	O
the	O
filtering	O
procedure	O
is	O
presented	O
in	O
with	O
a	O
single	O
update	O
in	O
ldsforwardupdate	O
m	O
one	O
can	O
write	O
the	O
covariance	B
update	O
as	O
where	O
we	O
define	O
the	O
kalman	B
gain	I
matrix	B
ft	O
kb	O
p	O
k	O
v	O
we	O
present	O
in	O
algorithm	B
the	O
recursion	O
in	O
standard	O
engineering	O
notation	O
see	O
also	O
ldssmooth	O
m	O
the	O
iteration	B
is	O
expected	O
to	O
be	O
numerically	O
stable	O
when	O
the	O
noise	O
covariances	O
are	O
small	O
y	O
and	O
covariance	B
xx	O
xy	O
yy	O
yx	O
is	O
a	O
gaussian	B
with	O
mean	B
x	O
xy	O
yy	O
draft	O
march	O
symmetrising	O
the	O
updates	O
inference	B
a	O
potential	B
numerical	B
issue	O
with	O
the	O
covariance	B
update	O
is	O
that	O
it	O
is	O
the	O
difference	O
of	O
two	O
positive	B
definite	I
matrices	O
if	O
there	O
are	O
numerical	B
errors	O
the	O
ft	O
may	O
not	O
be	O
positive	B
definite	I
nor	O
symmetric	O
using	O
the	O
woodbury	O
identity	O
equation	B
can	O
be	O
written	O
more	O
compactly	O
as	O
ft	O
p	O
bt	O
v	O
b	O
whilst	O
this	O
is	O
positive	O
semidefinite	O
this	O
is	O
numerically	O
expensive	O
since	O
it	O
involves	O
two	O
matrix	B
inversions	O
an	O
alternative	O
is	O
to	O
use	O
the	O
definition	O
of	O
k	O
from	O
which	O
we	O
can	O
write	O
k	O
v	O
kt	O
kb	O
pbtkt	O
hence	O
we	O
arrive	O
at	O
joseph	O
s	O
symmetrized	O
p	O
kbt	O
kb	O
p	O
kbt	O
k	O
v	O
kt	O
kb	O
kb	O
p	O
the	O
left	O
hand	O
side	O
is	O
the	O
addition	O
of	O
two	O
positive	B
definite	I
matrices	O
so	O
that	O
the	O
resulting	O
update	O
for	O
the	O
covariance	B
is	O
more	O
numerically	O
stable	O
a	O
similar	O
method	O
can	O
be	O
used	O
in	O
the	O
backward	O
pass	O
below	O
an	O
alternative	O
is	O
to	O
avoid	O
using	O
covariance	B
matrices	O
directly	O
and	O
use	O
their	O
square	O
root	O
as	O
the	O
parameter	B
deriving	O
updates	O
for	O
these	O
smoothing	B
rauch-tung-striebel	B
correction	O
method	O
the	O
smoothed	O
posterior	B
is	O
necessarily	O
gaussian	B
since	O
it	O
is	O
the	O
conditional	B
marginal	B
of	O
a	O
larger	O
gaussian	B
by	O
representing	O
the	O
posterior	B
as	O
a	O
gaussian	B
with	O
mean	B
gt	O
and	O
covariance	B
gt	O
n	O
gt	O
gt	O
we	O
can	O
form	O
a	O
recursion	O
for	O
gt	O
and	O
gt	O
as	O
follows	O
pht	O
the	O
term	O
can	O
be	O
found	O
by	O
conditioning	B
the	O
joint	B
distribution	B
pht	O
which	O
is	O
obtained	O
in	O
the	O
usual	O
manner	O
by	O
finding	O
its	O
mean	B
and	O
covariance	B
the	O
term	O
is	O
a	O
known	O
gaussian	B
from	O
filtering	O
with	O
mean	B
ft	O
and	O
covariance	B
ft	O
hence	O
the	O
joint	B
distribution	B
pht	O
has	O
means	O
aft	O
and	O
covariance	B
elements	O
ft	O
ht	O
ht	O
t	O
ftat	O
ft	O
ht	O
ht	O
ht	O
aftat	O
h	O
to	O
find	O
we	O
may	O
use	O
the	O
conditioned	O
gaussian	B
results	O
it	O
is	O
useful	O
to	O
use	O
the	O
system	B
reversal	I
result	O
which	O
interprets	O
as	O
an	O
equivalent	B
linear	B
system	O
going	O
backwards	O
in	O
time	O
ht	O
mt	O
t	O
where	O
at	O
ht	O
ht	O
ht	O
draft	O
march	O
inference	B
algorithm	B
lds	O
backward	O
pass	O
compute	O
the	O
smoothed	O
posteriors	O
this	O
requires	O
the	O
filtered	O
results	O
from	O
gt	O
ft	O
ft	O
gt	O
ft	O
gt	O
ft	O
for	O
t	O
t	O
do	O
end	O
for	O
function	B
ldsbackwardg	O
g	O
f	O
f	O
afat	O
h	O
a	O
t	O
ag	O
at	O
h	O
af	O
h	O
f	O
t	O
ag	O
m	O
return	O
mt	O
t	O
t	O
end	O
function	B
ht	O
ht	O
with	O
and	O
t	O
n	O
t	O
ht	O
af	O
m	O
f	O
a	O
h	O
statistics	O
of	O
pht	O
dynamics	B
reversal	I
backward	O
propagation	B
ht	O
ht	O
t	O
ht	O
ht	O
ht	O
ht	O
t	O
using	O
dynamics	B
reversal	I
equation	B
and	O
assuming	O
that	O
is	O
gaussian	B
distributed	O
it	O
is	O
then	O
straightforward	O
to	O
work	O
out	O
the	O
statistics	O
of	O
the	O
mean	B
is	O
given	O
by	O
and	O
covariance	B
gt	O
at	O
mt	O
mt	O
at	O
t	O
t	O
at	O
ht	O
ht	O
ht	O
t	O
gt	O
at	O
t	O
t	O
this	O
procedure	O
is	O
the	O
rauch-tung-striebel	B
kalman	O
this	O
is	O
called	O
a	O
correction	O
method	O
since	O
it	O
takes	O
the	O
filtered	O
estimate	O
and	O
corrects	O
it	O
to	O
form	O
a	O
smoothed	O
estimate	O
the	O
procedure	O
is	O
outlined	O
in	O
and	O
is	O
detailed	O
in	O
ldsbackwardupdate	O
m	O
see	O
also	O
ldssmooth	O
m	O
the	O
cross	B
moment	I
an	O
advantage	O
of	O
the	O
dynamics	B
reversal	I
interpretation	O
given	O
above	O
is	O
that	O
the	O
cross	B
moment	I
is	O
required	O
for	O
learning	B
is	O
immediately	O
obtained	O
from	O
gtgt	O
htht	O
ht	O
the	O
likelihood	B
we	O
can	O
compute	O
the	O
likelihood	B
using	O
the	O
decomposition	B
in	O
which	O
each	O
conditional	B
is	O
a	O
gaussian	B
in	O
vt	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
term	O
has	O
mean	B
and	O
covariance	B
b	O
t	O
baft	O
t	O
h	O
tt	O
b	O
bt	O
v	O
log	O
t	O
the	O
log	O
likelihood	B
is	O
then	O
given	O
by	O
bt	O
v	O
t	O
t	O
t	O
log	O
det	O
t	O
draft	O
march	O
most	B
likely	I
state	I
since	O
the	O
mode	B
of	O
a	O
gaussian	B
is	O
equal	O
to	O
its	O
mean	B
there	O
is	O
no	O
difference	O
between	O
the	O
most	O
probable	O
joint	B
posterior	B
state	O
inference	B
argmax	O
and	O
the	O
set	O
of	O
most	O
probable	O
marginal	B
states	O
ht	O
argmax	O
htt	O
t	O
t	O
hence	O
the	O
most	O
likely	O
hidden	B
state	O
sequence	O
is	O
equivalent	B
to	O
the	O
smoothed	O
mean	B
sequence	O
time	O
independence	B
and	O
riccati	B
equations	I
both	O
the	O
filtered	O
ft	O
and	O
smoothed	O
gt	O
covariance	B
recursions	O
are	O
independent	O
of	O
the	O
observations	O
depending	O
only	O
on	O
the	O
parameters	O
of	O
the	O
model	B
this	O
is	O
a	O
general	O
characteristic	O
of	O
linear	B
gaussian	B
systems	O
typically	O
the	O
covariance	B
recursions	O
converge	O
quickly	O
to	O
values	O
that	O
are	O
reasonably	O
constant	O
throughout	O
the	O
dynamics	O
with	O
only	O
appreciable	O
differences	O
at	O
the	O
boundaries	O
t	O
and	O
t	O
t	O
in	O
practice	O
one	O
often	O
drops	O
the	O
time-dependence	O
of	O
the	O
covariances	O
and	O
approximates	O
them	O
with	O
a	O
single	O
time-independent	O
covariance	B
this	O
approximation	B
dramatically	O
reduces	O
storage	O
requirements	O
the	O
converged	O
filtered	O
f	O
satisfies	O
the	O
recursion	O
afat	O
h	O
b	O
afat	O
h	O
bt	O
v	O
b	O
afat	O
h	O
f	O
afat	O
h	O
which	O
can	O
be	O
related	O
to	O
a	O
form	O
of	O
algebraic	B
riccati	B
equation	B
a	O
technique	O
to	O
solve	O
these	O
equations	O
is	O
to	O
being	O
with	O
setting	O
the	O
covariance	B
to	O
with	O
this	O
a	O
new	O
f	O
is	O
found	O
using	O
the	O
right	O
hand	O
side	O
of	O
and	O
subsequently	O
recursively	O
updated	O
alternatively	O
using	O
the	O
woodbury	O
identity	O
the	O
converged	O
covariance	B
satisfies	O
f	O
afat	O
h	O
bt	O
v	O
b	O
although	O
this	O
form	O
is	O
less	O
numerically	O
convenient	O
in	O
forming	O
an	O
iterative	O
solver	O
for	O
f	O
since	O
it	O
requires	O
two	O
matrix	B
inversions	O
example	O
trajectory	O
analysis	B
a	O
toy	O
rocket	O
with	O
unknown	O
mass	O
and	O
initial	O
velocity	O
is	O
launched	O
in	O
the	O
air	O
in	O
addition	O
the	O
constant	O
accelerations	O
from	O
the	O
rocket	O
s	O
propulsion	O
system	O
are	O
unknown	O
it	O
is	O
known	O
is	O
that	O
newton	O
s	O
laws	O
apply	O
and	O
an	O
instrument	O
can	O
measure	O
the	O
vertical	O
height	O
and	O
horizontal	O
distance	O
of	O
the	O
rocket	O
at	O
each	O
time	O
xt	O
yt	O
from	O
the	O
origin	O
based	O
on	O
noisy	O
measurements	O
of	O
xt	O
and	O
yt	O
our	O
task	O
is	O
to	O
infer	O
the	O
position	O
of	O
the	O
rocket	O
at	O
each	O
time	O
although	O
this	O
is	O
perhaps	O
most	O
appropriately	O
considered	O
from	O
the	O
using	O
continuous	B
time	O
dynamics	O
we	O
will	O
translate	O
this	O
into	O
a	O
discrete	B
time	O
approximation	B
newton	O
s	O
law	O
states	O
that	O
x	O
fxt	O
m	O
y	O
fyt	O
m	O
where	O
m	O
is	O
the	O
mass	O
of	O
the	O
object	O
and	O
fxt	O
fyt	O
are	O
the	O
horizontal	O
and	O
vertical	O
forces	O
respectively	O
hence	O
as	O
they	O
stand	O
these	O
equations	O
are	O
not	O
in	O
a	O
form	O
directly	O
usable	O
in	O
the	O
lds	O
framework	O
a	O
naive	O
approach	B
is	O
to	O
reparameterise	O
time	O
to	O
use	O
the	O
variable	O
t	O
such	O
that	O
t	O
t	O
where	O
t	O
is	O
integer	O
and	O
is	O
a	O
unit	O
of	O
time	O
the	O
dynamics	O
is	O
then	O
x	O
t	O
x	O
t	O
x	O
y	O
t	O
y	O
t	O
y	O
t	O
t	O
draft	O
march	O
learning	B
linear	B
dynamical	O
systems	O
figure	O
estimate	O
of	O
the	O
trajectory	O
of	O
a	O
newtonian	O
ballistic	O
object	O
based	O
on	O
noisy	O
observations	O
circles	O
all	O
time	O
labels	O
are	O
known	O
but	O
omitted	O
in	O
the	O
plot	O
the	O
x	O
points	O
are	O
the	O
true	O
positions	O
of	O
the	O
object	O
and	O
the	O
crosses	O
are	O
the	O
estimated	O
smoothed	O
mean	B
positions	O
of	O
the	O
object	O
plotted	O
every	O
several	O
time	O
steps	O
see	O
demoldstracking	O
m	O
where	O
dy	O
t	O
x	O
x	O
dt	O
we	O
can	O
write	O
an	O
update	O
equation	B
for	O
the	O
and	O
as	O
t	O
fy	O
t	O
fx	O
t	O
y	O
y	O
these	O
are	O
discrete	B
time	O
difference	O
equations	O
indexed	O
by	O
t	O
the	O
instrument	O
which	O
measures	O
xt	O
and	O
yt	O
is	O
not	O
completely	B
accurate	O
for	O
simplicity	O
we	O
relabel	O
axt	O
fxtmt	O
ayt	O
fytmt	O
these	O
accelerations	O
will	O
be	O
assumed	O
to	O
be	O
roughly	O
constant	O
but	O
unknown	O
ax	O
t	O
ax	O
t	O
x	O
ay	O
t	O
ay	O
t	O
y	O
where	O
x	O
and	O
y	O
are	O
small	O
noise	O
terms	O
the	O
initial	O
distributions	O
for	O
the	O
accelerations	O
are	O
assumed	O
vague	O
using	O
a	O
zero	O
mean	B
gaussian	B
with	O
large	O
variance	B
we	O
describe	O
the	O
above	O
model	B
by	O
considering	O
xt	O
yt	O
axt	O
ayt	O
as	O
hidden	B
variables	I
giving	O
rise	O
to	O
a	O
h	O
dimensional	O
lds	O
with	O
transition	O
and	O
emission	O
matrices	O
as	O
below	O
b	O
a	O
we	O
place	O
a	O
large	O
variance	B
on	O
their	O
initial	O
values	O
and	O
attempt	O
to	O
infer	O
the	O
unknown	O
trajectory	O
a	O
demonstration	O
is	O
given	O
in	O
despite	O
the	O
significant	O
observation	O
noise	O
the	O
object	O
trajectory	O
can	O
be	O
accurately	O
inferred	O
learning	B
linear	B
dynamical	O
systems	O
whilst	O
in	O
many	O
applications	O
particularly	O
of	O
underlying	O
known	O
physical	O
processes	O
the	O
paremeters	O
of	O
the	O
lds	O
are	O
known	O
in	O
many	O
machine	O
learning	B
tasks	O
we	O
need	O
to	O
learn	O
the	O
parameters	O
of	O
the	O
lds	O
based	O
on	O
for	O
simplicity	O
we	O
assume	O
that	O
we	O
know	O
the	O
dimensionality	O
h	O
of	O
the	O
lds	O
identifiability	B
issues	O
an	O
interesting	O
question	O
is	O
whether	O
we	O
can	O
uniquely	O
identify	O
the	O
parameters	O
of	O
an	O
lds	O
there	O
are	O
always	O
trivial	O
redundancies	O
in	O
the	O
solution	O
obtained	O
by	O
permuting	O
the	O
hidden	B
variables	I
arbitrarily	O
and	O
flipping	O
their	O
signs	O
to	O
show	O
that	O
there	O
are	O
potentially	O
many	O
more	O
equivalent	B
solutions	O
consider	O
the	O
following	O
lds	O
vt	O
bht	O
v	O
t	O
ht	O
aht	O
h	O
t	O
draft	O
march	O
we	O
now	O
attempt	O
to	O
transform	O
this	O
original	O
system	O
to	O
a	O
new	O
form	O
which	O
will	O
produce	O
exactly	O
the	O
same	O
outputs	O
for	O
an	O
invertible	O
matrix	B
r	O
we	O
consider	O
learning	B
linear	B
dynamical	O
systems	O
rht	O
rar	O
r	O
h	O
t	O
which	O
is	O
representable	O
as	O
a	O
new	O
latent	B
dynamics	O
ht	O
a	O
ht	O
h	O
t	O
where	O
a	O
rar	O
ht	O
rht	O
h	O
the	O
transformed	O
h	O
t	O
r	O
h	O
t	O
in	O
addition	O
we	O
can	O
reexpress	O
the	O
outputs	O
to	O
be	O
a	O
function	B
of	O
vt	O
br	O
v	O
t	O
b	O
ht	O
v	O
t	O
hence	O
provided	O
we	O
place	O
no	O
constraints	O
on	O
a	O
b	O
and	O
h	O
there	O
exists	O
an	O
infinite	O
space	O
of	O
equivalent	B
solutions	O
a	O
ra	O
r	O
b	O
br	O
h	O
r	O
hrt	O
all	O
with	O
the	O
same	O
likelihood	B
value	B
this	O
means	O
that	O
directly	O
interpreting	O
the	O
learned	O
parameters	O
needs	O
to	O
be	O
done	O
with	O
some	O
care	O
this	O
redundancy	O
can	O
be	O
mitigated	O
by	O
imposing	O
constraints	O
on	O
the	O
parameters	O
em	B
algorithm	B
for	O
simplicity	O
we	O
assume	O
we	O
have	O
a	O
single	O
sequence	O
to	O
which	O
we	O
wish	O
to	O
fit	O
a	O
lds	O
using	O
maximum	B
likelihood	B
since	O
the	O
lds	O
contains	O
latent	B
variables	O
one	O
approach	B
is	O
to	O
use	O
the	O
em	B
algorithm	B
as	O
usual	O
the	O
m-step	B
of	O
the	O
em	B
algorithm	B
requires	O
us	O
to	O
maximise	O
the	O
energy	B
with	O
respect	O
to	O
the	O
parameters	O
a	O
b	O
a	O
v	O
h	O
thanks	O
to	O
the	O
form	O
of	O
the	O
lds	O
the	O
energy	B
decomposes	O
as	O
phtht	O
it	O
is	O
straightforward	O
to	O
derive	O
that	O
the	O
m-step	B
for	O
the	O
parameters	O
is	O
given	O
by	O
brackets	O
denote	O
expectation	B
with	O
respect	O
to	O
the	O
smoothed	O
posterior	B
htht	O
t	O
t	O
vt	O
bt	O
vt	O
at	O
a	O
new	O
v	O
t	O
b	O
htht	O
vtvt	O
t	O
t	O
t	O
htht	O
t	O
a	O
t	O
t	O
new	O
h	O
anew	O
t	O
new	O
t	O
new	O
vt	O
bnew	O
new	O
v	O
t	O
t	O
vtvt	O
t	O
t	O
new	O
h	O
t	O
htht	O
t	O
htht	O
t	O
t	O
vt	O
a	O
htht	O
if	O
b	O
is	O
updated	O
according	O
to	O
the	O
above	O
the	O
first	O
equation	B
can	O
be	O
simplified	O
to	O
similarly	O
if	O
a	O
is	O
updated	O
according	O
to	O
em	B
algorithm	B
then	O
the	O
second	O
equation	B
can	O
be	O
simplified	O
to	O
draft	O
march	O
learning	B
linear	B
dynamical	O
systems	O
the	O
statistics	O
required	O
therefore	O
include	O
smoothed	O
means	O
covariances	O
and	O
cross	B
moments	O
the	O
extension	O
to	O
learning	B
multiple	O
timeseries	O
is	O
straightforward	O
since	O
the	O
energy	B
is	O
simply	O
summed	O
over	O
the	O
individual	O
sequences	B
the	O
performance	B
of	O
the	O
em	B
algorithm	B
for	O
the	O
lds	O
often	O
depends	O
heavily	O
on	O
a	O
the	O
initialisation	O
if	O
we	O
remove	O
the	O
hidden	B
to	O
hidden	B
links	O
the	O
model	B
is	O
closely	O
related	O
to	O
factor	B
analysis	B
lds	O
can	O
be	O
considered	O
a	O
temporal	O
extension	O
of	O
factor	B
analysis	B
one	O
initialisation	O
technique	O
is	O
therefore	O
to	O
learn	O
the	O
b	O
matrix	B
using	O
factor	B
analysis	B
by	O
treating	O
the	O
observations	O
as	O
temporally	O
independent	O
subspace	O
methods	O
an	O
alternative	O
to	O
em	B
and	O
maximum	B
likelihood	B
training	B
of	O
an	O
lds	O
is	O
to	O
use	O
a	O
subspace	O
the	O
chief	O
benefit	O
of	O
these	O
techniques	O
is	O
that	O
they	O
avoid	O
the	O
convergence	O
difficulties	O
of	O
em	B
to	O
motivate	O
subspace	O
techniques	O
consider	O
a	O
deterministic	B
lds	O
vt	O
bht	O
ht	O
aht	O
under	O
this	O
assumption	O
vt	O
bht	O
baht	O
and	O
more	O
generally	O
vt	O
this	O
means	O
that	O
a	O
low	B
dimensional	I
system	O
underlies	O
all	O
visible	B
information	O
since	O
all	O
points	O
lie	O
in	O
a	O
h-dimensional	O
subspace	O
which	O
is	O
then	O
projected	O
to	O
form	O
the	O
observation	O
this	O
suggests	O
that	O
some	O
form	O
of	O
subspace	O
identification	O
technique	O
will	O
enable	O
us	O
to	O
learn	O
a	O
and	O
b	O
given	O
a	O
set	O
of	O
observation	O
vectors	O
vt	O
consider	O
the	O
block	O
hankel	B
matrix	B
formed	O
from	O
stacking	O
the	O
vectors	O
for	O
an	O
order	O
l	O
matrix	B
this	O
is	O
a	O
v	O
l	O
t	O
l	O
matrix	B
for	O
example	O
for	O
t	O
and	O
l	O
this	O
is	O
b	O
ba	O
m	O
m	O
w	O
we	O
now	O
find	O
the	O
svd	B
of	O
m	O
m	O
u	O
s	O
vt	O
if	O
the	O
v	O
are	O
generated	O
from	O
a	O
free	O
lds	O
we	O
can	O
write	O
where	O
w	O
is	O
termed	O
the	O
extended	B
observability	I
matrix	B
the	O
matrix	B
s	O
will	O
contain	O
the	O
singular	B
values	O
up	O
to	O
the	O
dimension	O
of	O
the	O
hidden	B
variables	I
h	O
with	O
the	O
remaining	O
singular	B
values	O
from	O
equation	B
this	O
means	O
that	O
the	O
emission	B
matrix	B
b	O
is	O
contained	O
in	O
the	O
estimated	O
hidden	B
variables	I
are	O
then	O
contained	O
in	O
the	O
submatrix	O
based	O
on	O
the	O
relation	O
ht	O
aht	O
one	O
can	O
then	O
find	O
the	O
best	O
least	O
squares	O
estimate	O
for	O
a	O
by	O
minimising	O
aht	O
for	O
which	O
the	O
optimal	O
solution	O
is	O
ht	O
a	O
ht	O
where	O
denotes	O
the	O
pseudo	B
inverse	I
see	O
ldssubspace	O
m	O
estimates	O
for	O
the	O
covariance	B
matrices	O
can	O
also	O
be	O
obtained	O
from	O
the	O
residual	O
errors	O
in	O
fitting	O
the	O
block	O
hankel	B
matrix	B
v	O
and	O
extended	B
observability	I
matrix	B
h	O
whilst	O
this	O
derivation	O
formally	O
holds	O
only	O
for	O
the	O
noise	O
free	O
case	O
one	O
can	O
nevertheless	O
apply	O
this	O
in	O
the	O
case	O
of	O
non-zero	O
noise	O
and	O
hope	O
to	O
gain	O
an	O
estimate	O
for	O
a	O
and	O
b	O
that	O
is	O
correct	O
in	O
the	O
mean	B
in	O
addition	O
to	O
forming	O
a	O
solution	O
in	O
its	O
own	O
right	O
the	O
subspace	B
method	I
forms	O
a	O
potentially	O
useful	O
way	O
to	O
initialise	O
the	O
em	B
algorithm	B
draft	O
march	O
switching	B
auto-regressive	B
models	O
figure	O
a	O
first	O
order	O
switching	B
ar	I
model	B
in	O
terms	O
of	O
inference	B
conditioned	O
on	O
this	O
is	O
a	O
hmm	B
structured	B
ldss	O
many	O
physical	O
equations	O
are	O
local	B
both	O
in	O
time	O
and	O
space	O
for	O
example	O
in	O
weather	O
models	O
the	O
atmosphere	O
is	O
partitioned	B
into	O
cells	O
hit	O
each	O
containing	O
the	O
pressure	O
at	O
that	O
location	O
the	O
equations	O
describing	O
how	O
the	O
pressure	O
updates	O
only	O
depend	O
on	O
the	O
pressure	O
at	O
the	O
current	O
cell	O
and	O
small	O
number	O
of	O
neighbouring	O
cells	O
at	O
the	O
previous	O
time	O
t	O
if	O
we	O
use	O
a	O
linear	B
model	B
and	O
measure	O
some	O
aspects	O
of	O
the	O
cells	O
at	O
each	O
time	O
then	O
the	O
weather	O
is	O
describable	O
by	O
a	O
lds	O
with	O
a	O
highly	O
structured	B
sparse	B
transition	B
matrix	B
a	O
in	O
practice	O
the	O
weather	O
models	O
are	O
non-linear	B
but	O
local	B
linear	B
approximations	O
are	O
often	O
a	O
similar	O
situation	O
arises	O
in	O
brain	O
imaging	O
in	O
which	O
voxels	O
cubes	O
of	O
activity	O
depend	O
only	O
on	O
their	O
neighbours	O
from	O
the	O
previous	O
another	O
application	O
of	O
structured	B
ldss	O
is	O
in	O
temporal	O
independent	O
component	O
analysis	B
this	O
is	O
defined	O
as	O
the	O
discovery	O
of	O
a	O
set	O
of	O
independent	O
latent	B
dynamical	O
processes	O
from	O
which	O
the	O
data	O
is	O
a	O
projected	O
observation	O
if	O
each	O
independent	O
dynamical	O
process	O
can	O
itself	O
be	O
described	O
by	O
a	O
lds	O
this	O
gives	O
rise	O
to	O
a	O
structured	B
lds	O
with	O
a	O
block	O
diagonal	O
transition	B
matrix	B
a	O
such	O
models	O
can	O
be	O
used	O
to	O
extract	O
independent	O
components	O
under	O
prior	B
knowledge	O
of	O
the	O
likely	O
underlying	O
frequencies	O
in	O
each	O
of	O
the	O
temporal	O
see	O
also	O
bayesian	B
ldss	O
the	O
extension	O
to	O
placing	O
priors	O
on	O
the	O
transition	O
and	O
emission	O
parameters	O
of	O
the	O
lds	O
leads	O
in	O
general	O
to	O
computational	O
difficulties	O
in	O
computing	O
the	O
likelihood	B
for	O
example	O
for	O
a	O
prior	B
on	O
a	O
the	O
likelihood	B
is	O
a	O
which	O
is	O
difficult	O
to	O
evaluate	O
since	O
the	O
dependence	O
of	O
the	O
likelihood	B
on	O
the	O
matrix	B
a	O
is	O
a	O
complicated	O
function	B
approximate	B
treatments	O
of	O
this	O
case	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
although	O
we	O
briefly	O
note	O
that	O
sampling	B
are	O
popular	O
in	O
this	O
context	O
in	O
addition	O
to	O
deterministic	B
variational	O
switching	B
auto-regressive	B
models	O
for	O
a	O
time-series	O
of	O
scalar	O
values	O
an	O
lth	O
order	O
switching	B
ar	I
model	B
can	O
be	O
written	O
as	O
vt	O
vt	O
t	O
t	O
t	O
where	O
we	O
now	O
have	O
a	O
set	O
of	O
ar	O
coefficients	O
s	O
s	O
themselves	O
have	O
a	O
markov	O
transition	O
pvtvt	O
vt	O
l	O
st	O
t	O
n	O
t	O
t	O
pstst	O
so	O
that	O
the	O
full	O
model	B
is	O
the	O
discrete	B
switch	O
variables	O
given	O
an	O
observed	O
sequence	O
and	O
parameters	O
inference	B
is	O
straightforward	O
since	O
this	O
is	O
a	O
form	O
of	O
hmm	B
to	O
make	O
this	O
more	O
apparent	O
we	O
may	O
write	O
inference	B
t	O
where	O
pvtstpstst	O
pvtst	O
pvtvt	O
vt	O
l	O
st	O
n	O
vt	O
vt	O
t	O
draft	O
march	O
switching	B
auto-regressive	B
models	O
figure	O
learning	B
a	O
switching	B
ar	I
model	B
the	O
upper	O
plot	O
shows	O
the	O
training	B
data	O
the	O
colour	O
indicates	O
which	O
of	O
the	O
two	O
ar	O
models	O
is	O
active	B
at	O
that	O
time	O
whilst	O
this	O
information	O
is	O
plotted	O
here	O
this	O
is	O
assumed	O
unknown	O
to	O
the	O
learning	B
algorithm	B
as	O
are	O
the	O
coefficients	O
as	O
we	O
assume	O
that	O
the	O
order	O
l	O
and	O
number	O
of	O
switches	O
s	O
however	O
is	O
known	O
in	O
the	O
bottom	O
plot	O
we	O
show	O
the	O
time	O
series	O
again	O
after	O
training	B
in	O
which	O
we	O
colour	O
the	O
points	O
according	O
to	O
the	O
most	O
likely	O
smoothed	O
ar	O
model	B
at	O
each	O
timestep	O
see	O
demosarlearn	O
m	O
note	O
that	O
the	O
emission	B
distribution	B
pvtst	O
is	O
time-dependent	O
the	O
filtering	O
recursion	O
is	O
then	O
pvtstpstst	O
st	O
smoothing	B
can	O
be	O
achieved	O
using	O
the	O
standard	O
recursions	O
modified	O
to	O
use	O
the	O
time-dependent	O
emissions	O
see	O
demosarinference	O
m	O
maximum	B
likelihood	B
learning	B
using	O
em	B
to	O
fit	O
the	O
set	O
of	O
ar	O
coefficients	O
and	O
innovation	O
variances	O
as	O
s	O
s	O
using	O
maximum	B
likelihood	B
training	B
for	O
a	O
set	O
of	O
data	O
we	O
may	O
make	O
use	O
of	O
the	O
em	B
algorithm	B
which	O
we	O
need	O
to	O
maximise	O
with	O
respect	O
to	O
the	O
parameters	O
using	O
the	O
definition	O
of	O
the	O
emission	O
and	O
isolating	O
the	O
dependency	O
on	O
a	O
we	O
have	O
t	O
pstst	O
m-step	B
up	O
to	O
negligible	O
constants	O
the	O
energy	B
is	O
given	O
by	O
t	O
pvt	O
vt	O
vt	O
vt	O
t	O
e	O
t	O
log	O
const	O
on	O
differentiating	O
with	O
respect	O
to	O
as	O
and	O
equating	O
to	O
zero	O
the	O
optimal	O
as	O
satisfies	O
the	O
linear	B
equation	B
which	O
may	O
be	O
solved	O
using	O
gaussian	B
elimination	O
similarly	O
one	O
may	O
show	O
that	O
updates	O
that	O
maximise	O
the	O
energy	B
with	O
respect	O
to	O
are	O
poldst	O
vt	O
vt	O
t	O
t	O
t	O
vt	O
vt	O
t	O
poldst	O
vt	O
vt	O
poldst	O
t	O
t	O
as	O
the	O
update	O
for	O
pstst	O
follows	O
the	O
standard	O
em	B
for	O
hmm	B
rule	O
equation	B
see	O
sarlearn	O
m	O
here	O
we	O
don	O
t	O
include	O
an	O
update	O
for	O
the	O
prior	B
since	O
there	O
is	O
insufficient	O
information	O
at	O
the	O
start	O
of	O
the	O
sequence	O
and	O
assume	O
is	O
flat	O
with	O
high	O
frequency	O
data	O
it	O
is	O
unlikely	O
that	O
a	O
change	O
in	O
the	O
switch	O
variable	O
is	O
reasonable	O
at	O
each	O
time	O
t	O
a	O
simple	O
constraint	O
to	O
account	O
for	O
this	O
is	O
to	O
use	O
a	O
modified	O
transition	O
pstst	O
pstst	O
st	O
otherwise	O
mod	O
tskip	O
draft	O
march	O
switches	O
code	O
figure	O
a	O
latent	B
switching	B
order	O
ar	O
model	B
here	O
the	O
st	O
indicates	O
which	O
of	O
a	O
set	O
of	O
available	O
ar	O
models	O
is	O
active	B
at	O
time	O
t	O
the	O
square	O
nodes	O
emphasise	O
that	O
these	O
are	O
discrete	B
variables	O
the	O
clean	O
ar	O
signal	O
vt	O
which	O
is	O
not	O
observed	O
is	O
corrupted	O
by	O
additive	O
noise	O
to	O
form	O
the	O
noisy	O
observations	O
vt	O
in	O
terms	O
of	O
inference	B
conditioned	O
on	O
this	O
can	O
be	O
expressed	O
as	O
a	O
switching	B
lds	O
signal	O
reconstruction	O
using	O
the	O
latent	B
switching	B
ar	I
model	B
in	O
top	O
noisy	O
signal	O
bottom	O
reconstructed	O
clean	O
signal	O
the	O
dashed	O
lines	O
and	O
the	O
numbers	O
show	O
the	O
most-likely	O
state	O
segmentation	O
arg	O
e-step	B
the	O
m-step	B
requires	O
the	O
smoothed	O
statistics	O
poldst	O
and	O
poldst	O
s	O
st	O
obtained	O
from	O
hmm	B
inference	B
which	O
can	O
be	O
example	O
a	O
switching	B
ar	I
model	B
in	O
the	O
training	B
data	O
is	O
generated	O
by	O
an	O
switching	B
ar	I
model	B
so	O
that	O
we	O
know	O
the	O
ground	O
truth	O
as	O
to	O
which	O
model	B
generated	O
which	O
parts	O
of	O
the	O
data	O
based	O
on	O
the	O
training	B
data	O
the	O
labels	O
st	O
are	O
unknown	O
a	O
switching	B
ar	I
model	B
is	O
fitted	O
using	O
em	B
in	O
this	O
case	O
the	O
problem	B
is	O
straightforward	O
so	O
that	O
a	O
good	O
estimate	O
is	O
obtained	O
of	O
both	O
the	O
sets	O
of	O
ar	O
parameters	O
and	O
which	O
switches	O
were	O
used	O
at	O
which	O
time	O
example	O
parts	O
of	O
speech	O
in	O
a	O
segment	O
of	O
a	O
speech	O
signal	O
is	O
shown	O
described	O
by	O
a	O
switching	B
ar	I
model	B
each	O
of	O
the	O
available	O
ar	O
models	O
is	O
responsible	O
for	O
modelling	B
the	O
dynamics	O
of	O
a	O
basic	O
subunit	O
of	O
the	O
model	B
was	O
trained	O
on	O
many	O
example	O
sequences	B
using	O
s	O
states	O
with	O
a	O
left-to-right	O
transition	B
matrix	B
the	O
interest	O
is	O
to	O
determine	O
when	O
each	O
subunit	O
is	O
most	O
likely	O
to	O
be	O
active	B
this	O
corresponds	O
to	O
the	O
computation	O
of	O
the	O
most-likely	O
switch	O
path	B
given	O
the	O
observed	O
signal	O
code	O
in	O
the	O
linear	B
dynamical	I
system	I
code	O
below	O
only	O
the	O
simplest	O
form	O
of	O
the	O
recursions	O
is	O
given	O
no	O
attempt	O
has	O
been	O
made	O
to	O
ensure	O
numerical	B
stability	I
ldsforwardupdate	O
m	O
lds	O
forward	O
ldsbackwardupdate	O
m	O
lds	O
backward	O
ldssmooth	O
m	O
linear	B
dynamical	I
system	I
filtering	O
and	O
smoothing	B
ldsforward	O
m	O
alternative	O
lds	O
forward	O
algorithm	B
slds	O
chapter	O
ldsbackward	O
m	O
alternative	O
lds	O
backward	O
algorithm	B
slds	O
chapter	O
demosumprodgausscanonlds	O
m	O
sum-product	B
algorithm	B
for	O
smoothed	O
inference	B
demoldstracking	O
m	O
demo	O
of	O
tracking	O
in	O
a	O
newtonian	O
system	O
draft	O
march	O
exercises	O
ldssubspace	O
m	O
subspace	O
learning	B
matrix	B
method	O
demoldssubspace	O
m	O
demo	O
of	O
subspace	O
learning	B
method	O
autoregressive	O
models	O
note	O
that	O
in	O
the	O
code	O
the	O
autoregressive	O
vector	O
a	O
has	O
as	O
its	O
last	O
entry	O
the	O
first	O
ar	O
coefficient	O
reverse	O
order	O
to	O
that	O
presented	O
in	O
the	O
text	O
artrain	O
m	O
learn	O
ar	O
coefficients	O
elimination	O
demoartrain	O
m	O
demo	O
of	O
fitting	O
an	O
ar	O
model	B
to	O
data	O
arlds	O
m	O
learn	O
ar	O
coefficients	O
using	O
a	O
lds	O
demoarlds	O
m	O
demo	O
of	O
learning	B
ar	O
coefficients	O
using	O
an	O
lds	O
demosarinference	O
m	O
demo	O
for	O
inference	B
in	O
a	O
switching	B
autoregressive	O
model	B
in	O
in	O
sarlearn	O
m	O
a	O
slight	O
fudge	O
is	O
used	O
since	O
we	O
do	O
not	O
deal	O
fully	O
with	O
the	O
case	O
at	O
the	O
start	O
where	O
there	O
is	O
insufficient	O
information	O
to	O
define	O
the	O
ar	O
model	B
for	O
long	O
timeseries	O
this	O
will	O
have	O
a	O
negligible	O
effect	O
although	O
it	O
might	O
lead	O
to	O
small	O
decreases	O
in	O
the	O
log	O
likelihood	B
under	O
the	O
em	B
algorithm	B
sarlearn	O
m	O
learning	B
of	O
a	O
sar	O
using	O
em	B
demosarlearn	O
m	O
demo	O
of	O
sar	O
learning	B
hmmforwardsar	O
m	O
switching	B
autoregressive	O
hmm	B
forward	O
pass	O
hmmbackwardsar	O
m	O
switching	B
autoregressive	O
hmm	B
backward	O
pass	O
exercises	O
exercise	O
consider	O
the	O
two-dimension	O
linear	B
model	B
ht	O
r	O
ht	O
where	O
r	O
r	O
is	O
rotation	O
matrix	B
which	O
rotates	O
the	O
vector	O
ht	O
through	O
angle	O
in	O
one	O
timestep	O
sin	O
cos	O
sin	O
cos	O
by	O
xt	O
xt	O
yt	O
yt	O
eliminate	O
yt	O
to	O
write	O
an	O
equation	B
for	O
in	O
terms	O
of	O
xt	O
and	O
xt	O
explain	O
why	O
the	O
eigenvalues	O
of	O
a	O
rotation	O
matrix	B
are	O
general	O
imaginary	O
explain	O
how	O
to	O
model	B
a	O
sinusoid	O
rotating	O
with	O
angular	O
velocity	O
using	O
a	O
two-dimensional	O
lds	O
explain	O
how	O
to	O
model	B
a	O
sinusoid	O
using	O
an	O
ar	O
model	B
explain	O
the	O
relationship	O
between	O
the	O
second	O
order	O
differential	B
equation	B
x	O
x	O
which	O
describes	O
a	O
harmonic	O
oscillator	O
and	O
the	O
second	O
order	O
difference	O
equation	B
which	O
approximates	O
this	O
differential	B
equation	B
is	O
it	O
possible	O
to	O
find	O
a	O
difference	O
equation	B
which	O
exactly	O
matches	O
the	O
solution	O
of	O
the	O
differential	B
equation	B
at	O
chosen	O
points	O
exercise	O
show	O
that	O
for	O
any	O
anti-symmetric	O
matrix	B
m	O
m	O
mt	O
the	O
matrix	B
exponential	B
matlab	O
this	O
is	O
expm	O
a	O
em	B
draft	O
march	O
is	O
orthogonal	B
namely	O
ata	O
i	O
exercises	O
explain	O
how	O
one	O
may	O
then	O
construct	O
random	O
orthogonal	B
matrices	O
with	O
some	O
control	O
over	O
the	O
angles	O
of	O
the	O
complex	O
eigenvalues	O
discuss	O
how	O
this	O
relates	O
to	O
the	O
frequencies	O
encountered	O
in	O
a	O
lds	O
where	O
a	O
is	O
the	O
transition	B
matrix	B
exercise	O
run	O
the	O
demo	O
demoldstracking	O
m	O
which	O
tracks	O
a	O
ballistic	O
object	O
using	O
a	O
linear	B
dynamical	I
system	I
see	O
modify	O
demoldstracking	O
m	O
so	O
that	O
in	O
addition	O
to	O
the	O
x	O
and	O
y	O
positions	O
the	O
x	O
speed	O
is	O
also	O
observed	O
compare	O
and	O
contrast	O
the	O
accuracy	O
of	O
the	O
tracking	O
with	O
and	O
without	O
this	O
extra	O
information	O
exercise	O
nightsong	O
mat	O
contains	O
a	O
small	O
stereo	O
segment	O
nightingale	O
song	O
sampled	O
at	O
hertz	O
plot	O
the	O
original	O
waveform	O
using	O
download	O
the	O
program	O
myspecgram	O
m	O
from	O
labrosa	O
ee	O
columbia	O
edumatlabsgrammyspecgram	O
m	O
and	O
plot	O
the	O
spectrogram	B
imagesclogabsy	O
the	O
routine	O
demogmmem	O
m	O
demonstrates	O
fitting	O
a	O
mixture	B
of	O
gaussians	O
to	O
data	O
the	O
mixture	B
assignment	O
probabilities	O
are	O
contained	O
in	O
phgn	O
write	O
a	O
routine	O
to	O
cluster	O
the	O
data	O
vlogabsy	O
using	O
gaussian	B
components	O
and	O
explain	O
how	O
one	O
might	O
segment	O
the	O
series	O
x	O
into	O
different	O
regions	O
examine	O
the	O
routine	O
demoarlds	O
m	O
which	O
fits	O
autoregressive	O
coefficients	O
using	O
an	O
interpretation	O
as	O
a	O
linear	B
dynamical	I
system	I
adapt	O
the	O
routine	O
demoarlds	O
m	O
to	O
learn	O
the	O
ar	O
coefficients	O
of	O
the	O
data	O
x	O
you	O
will	O
almost	O
certainly	O
need	O
to	O
subsample	O
the	O
data	O
x	O
for	O
example	O
by	O
taking	O
every	O
datapoint	O
with	O
the	O
learned	O
ar	O
coefficients	O
the	O
smoothed	O
results	O
fit	O
a	O
gaussian	B
mixture	B
with	O
components	O
compare	O
and	O
contrast	O
your	O
results	O
with	O
those	O
obtained	O
from	O
the	O
gaussian	B
mixture	B
model	B
fit	O
to	O
the	O
spectrogram	B
exercise	O
consider	O
a	O
supervised	B
learning	B
problem	B
in	O
which	O
we	O
make	O
a	O
linear	B
model	B
of	O
the	O
scaler	O
output	O
yt	O
based	O
on	O
vector	O
input	O
xt	O
t	O
where	O
y	O
t	O
xt	O
y	O
yt	O
wt	O
t	O
is	O
zero	O
mean	B
gaussian	B
noise	O
training	B
data	O
d	O
yt	O
t	O
t	O
is	O
available	O
for	O
a	O
time-invariant	O
weight	B
vector	O
wt	O
w	O
explain	O
how	O
to	O
find	O
the	O
single	O
weight	B
vector	O
w	O
and	O
the	O
noise	O
variance	B
by	O
maximum	B
likelihood	B
extend	O
the	O
above	O
model	B
to	O
include	O
a	O
transition	O
wt	O
wt	O
w	O
t	O
where	O
w	O
is	O
zero	O
mean	B
gaussian	B
noise	O
with	O
a	O
given	O
covariance	B
has	O
zero	O
mean	B
explain	O
t	O
how	O
to	O
cast	O
finding	O
as	O
smoothing	B
in	O
a	O
related	O
linear	B
dynamical	I
system	I
write	O
a	O
routine	O
w	O
linpredarxysigmawsigmay	O
that	O
takes	O
an	O
input	O
data	O
matrix	B
x	O
xt	O
where	O
each	O
column	O
contains	O
an	O
input	O
and	O
vector	O
y	O
yt	O
sigmaw	O
is	O
the	O
additive	O
weight	B
noise	O
and	O
sigmay	O
is	O
an	O
assumed	O
known	O
time-invariant	O
output	O
noise	O
the	O
returned	O
w	O
contains	O
the	O
smoothed	O
mean	B
weights	O
draft	O
march	O
chapter	O
switching	B
linear	B
dynamical	O
systems	O
introduction	O
complex	O
timeseries	O
which	O
are	O
not	O
well	O
described	O
globally	O
by	O
a	O
single	O
linear	B
dynamical	I
system	I
may	O
be	O
divided	O
into	O
segments	O
each	O
modelled	O
by	O
a	O
potentially	O
different	O
lds	O
such	O
models	O
can	O
handle	O
situations	O
in	O
which	O
the	O
underlying	O
model	B
jumps	O
from	O
one	O
parameter	B
setting	O
to	O
another	O
for	O
example	O
a	O
single	O
lds	O
might	O
well	O
represent	O
the	O
normal	B
flows	O
in	O
a	O
chemical	O
plant	O
when	O
a	O
break	O
in	O
a	O
pipeline	O
occurs	O
the	O
dynamics	O
of	O
the	O
system	O
changes	O
from	O
one	O
set	O
of	O
linear	B
flow	O
equations	O
to	O
another	O
this	O
scenario	O
can	O
be	O
modelled	O
suing	O
a	O
sets	O
of	O
two	O
linear	B
systems	O
each	O
with	O
different	O
parameters	O
the	O
discrete	B
latent	B
variable	I
at	O
each	O
time	O
st	O
pipe	O
broken	O
indicates	O
which	O
of	O
the	O
ldss	O
is	O
most	O
appropriate	O
at	O
the	O
current	O
time	O
this	O
is	O
called	O
a	O
switching	B
lds	O
and	O
used	O
in	O
many	O
disciplines	O
from	O
econometrics	O
to	O
machine	O
learning	B
the	O
switching	B
lds	O
at	O
each	O
time	O
t	O
a	O
switch	O
variable	O
st	O
s	O
describes	O
which	O
of	O
a	O
set	O
of	O
ldss	O
is	O
to	O
be	O
used	O
the	O
observation	O
visible	B
variable	O
vt	O
rv	O
is	O
linearly	O
related	O
to	O
the	O
hidden	B
state	O
ht	O
rh	O
by	O
vt	O
bstht	O
vst	O
vst	O
n	O
vst	O
vst	O
vst	O
here	O
st	O
describes	O
which	O
of	O
the	O
set	O
of	O
emission	O
matrices	O
bs	O
is	O
active	B
at	O
time	O
t	O
the	O
observation	O
noise	O
vst	O
is	O
drawn	O
from	O
a	O
gaussian	B
with	O
mean	B
vst	O
and	O
covariance	B
vst	O
the	O
transition	O
dynamics	O
of	O
the	O
continuous	B
hidden	B
state	O
ht	O
is	O
linear	B
ht	O
astht	O
hst	O
hst	O
n	O
hst	O
hst	O
hst	O
and	O
the	O
switch	O
variable	O
st	O
selects	O
a	O
single	O
transition	B
matrix	B
from	O
the	O
available	O
set	O
as	O
the	O
gaussian	B
transition	O
noise	O
hst	O
also	O
depends	O
on	O
the	O
switch	O
variable	O
the	O
dynamics	O
of	O
st	O
itself	O
is	O
markovian	O
with	O
transition	O
pstst	O
for	O
the	O
more	O
general	O
augmented	B
aslds	O
model	B
the	O
switch	O
st	O
is	O
dependent	O
on	O
both	O
the	O
previous	O
st	O
and	O
ht	O
the	O
model	B
defines	O
a	O
joint	B
distribution	B
pvtht	O
stphtht	O
stpstht	O
st	O
with	O
pvtht	O
st	O
n	O
vst	O
bstht	O
vst	O
phtht	O
st	O
n	O
ht	O
hst	O
astht	O
hst	O
gaussian	B
sum	I
filtering	I
figure	O
the	O
independence	B
structure	B
of	O
the	O
aslds	O
square	O
nodes	O
st	O
denote	O
discrete	B
switch	O
variables	O
ht	O
are	O
continuous	B
latenthidden	O
variables	O
and	O
vt	O
continuous	B
observedvisible	O
variables	O
the	O
discrete	B
state	O
st	O
determines	O
which	O
linear	B
dynamical	I
system	I
from	O
a	O
finite	O
set	O
of	O
linear	B
dynamical	O
systems	O
is	O
operational	O
at	O
time	O
t	O
in	O
the	O
slds	O
links	O
from	O
h	O
to	O
s	O
are	O
not	O
normally	O
considered	O
at	O
time	O
t	O
denotes	O
the	O
prior	B
and	O
denotes	O
the	O
slds	O
can	O
be	O
thought	O
of	O
as	O
a	O
marriage	O
between	O
a	O
hidden	B
markov	I
model	B
and	O
a	O
linear	B
dynamical	I
system	I
the	O
slds	O
is	O
also	O
called	O
a	O
jump	O
markov	O
modelprocess	O
switching	B
kalman	B
filter	I
switching	B
linear	B
gaussian	B
state	I
space	I
model	B
conditional	B
linear	B
gaussian	B
model	B
exact	O
inference	B
is	O
computationally	O
intractable	O
both	O
exact	O
filtered	O
and	O
smoothed	O
inference	B
in	O
the	O
slds	O
is	O
intractable	O
scaling	O
exponentially	O
with	O
time	O
as	O
an	O
informal	O
explanation	O
consider	O
filtered	O
posterior	B
inference	B
for	O
which	O
by	O
analogy	O
with	O
equation	B
the	O
forward	O
pass	O
is	O
ht	O
st	O
ht	O
at	O
timestep	O
is	O
an	O
indexed	O
set	O
of	O
gaussians	O
at	O
timestep	O
due	O
to	O
the	O
summation	O
over	O
the	O
states	O
will	O
be	O
an	O
indexed	O
set	O
of	O
s	O
gaussians	O
similarly	O
at	O
timestep	O
it	O
will	O
be	O
and	O
in	O
general	O
gives	O
rise	O
to	O
st	O
gaussians	O
at	O
time	O
t	O
even	O
for	O
small	O
t	O
the	O
number	O
of	O
components	O
required	O
to	O
exactly	O
represent	O
the	O
filtered	O
distribution	B
is	O
therefore	O
computationally	O
intractable	O
analogously	O
smoothing	B
is	O
also	O
intractable	O
the	O
origin	O
of	O
the	O
intractability	O
of	O
the	O
slds	O
differs	O
from	O
structural	O
intractability	O
that	O
we	O
ve	O
previously	O
encountered	O
in	O
the	O
slds	O
in	O
terms	O
of	O
the	O
cluster	O
variables	O
with	O
xt	O
ht	O
and	O
visible	B
variables	O
the	O
graph	B
of	O
the	O
distribution	B
is	O
singly-connected	B
from	O
a	O
purely	O
graph	B
theoretic	O
viewpoint	O
one	O
would	O
therefore	O
envisage	O
little	O
difficulty	O
in	O
carrying	O
out	O
inference	B
indeed	O
as	O
we	O
saw	O
above	O
the	O
derivation	O
of	O
the	O
filtering	O
algorithm	B
is	O
straightforward	O
since	O
the	O
graph	B
is	O
singly-connected	B
however	O
the	O
numerical	B
implementation	O
of	O
the	O
algorithm	B
is	O
intractable	O
since	O
the	O
description	O
of	O
the	O
messages	O
requires	O
an	O
exponentially	O
increasing	O
number	O
of	O
terms	O
in	O
order	O
to	O
deal	O
with	O
this	O
intractability	O
several	O
approximation	B
schemes	O
have	O
been	O
introduced	O
here	O
we	O
focus	O
on	O
techniques	O
which	O
approximate	B
the	O
switch	O
conditional	B
posteriors	O
using	O
a	O
limited	O
mixture	B
of	O
gaussians	O
since	O
the	O
exact	O
posterior	B
distributions	O
are	O
mixtures	O
of	O
gaussians	O
albeit	O
with	O
an	O
exponentially	O
large	O
number	O
of	O
components	O
the	O
aim	O
is	O
to	O
drop	O
low	O
weight	B
components	O
such	O
that	O
the	O
resulting	O
approximation	B
accurately	O
represents	O
the	O
posterior	B
gaussian	B
sum	I
filtering	I
describes	O
the	O
exact	O
filtering	O
recursion	O
with	O
an	O
exponentially	O
increasing	O
number	O
of	O
components	O
with	O
time	O
in	O
general	O
the	O
influence	O
of	O
ancient	O
observations	O
will	O
be	O
much	O
less	O
relevant	O
than	O
that	O
of	O
recent	O
observations	O
this	O
suggests	O
that	O
the	O
effective	O
time	O
is	O
limited	O
and	O
that	O
therefore	O
a	O
corresponding	O
limited	O
number	O
of	O
components	O
in	O
the	O
gaussian	B
mixture	B
should	O
suffice	O
to	O
accurately	O
represent	O
the	O
filtered	O
our	O
aim	O
is	O
to	O
form	O
a	O
recursion	O
for	O
pst	O
based	O
on	O
a	O
gaussian	B
mixture	B
approximation	B
of	O
phtst	O
given	O
an	O
approximation	B
of	O
the	O
filtered	O
distribution	B
pst	O
qst	O
the	O
exact	O
recursion	O
equation	B
is	O
approximated	O
by	O
ht	O
st	O
ht	O
draft	O
march	O
gaussian	B
sum	I
filtering	I
this	O
approximation	B
to	O
the	O
filtered	O
posterior	B
at	O
the	O
next	O
timestep	O
will	O
contain	O
s	O
times	O
more	O
components	O
than	O
in	O
the	O
previous	O
timestep	O
and	O
to	O
prevent	O
an	O
exponential	B
explosion	O
in	O
mixture	B
components	O
we	O
will	O
need	O
to	O
subsequently	O
collapse	O
the	O
mixture	B
in	O
a	O
suitable	O
way	O
we	O
will	O
deal	O
with	O
this	O
issue	O
once	O
has	O
been	O
computed	O
to	O
derive	O
the	O
updates	O
it	O
is	O
useful	O
to	O
break	O
the	O
new	O
filtered	O
approximation	B
from	O
equation	B
into	O
continuous	B
and	O
discrete	B
parts	O
qht	O
qhtst	O
and	O
derive	O
separate	O
filtered	O
update	O
formulae	O
as	O
described	O
below	O
continuous	B
filtering	O
the	O
exact	O
representation	O
of	O
phtst	O
is	O
a	O
mixture	B
with	O
components	O
to	O
retain	O
computational	O
feasibility	O
we	O
therefore	O
approximate	B
this	O
with	O
a	O
limited	O
i-component	O
mixture	B
qhtst	O
qhtit	O
st	O
where	O
qhtit	O
st	O
is	O
a	O
gaussian	B
parameterised	O
with	O
mean	B
fit	O
st	O
and	O
covariance	B
fit	O
st	O
strictly	O
speaking	O
we	O
should	O
use	O
the	O
notation	O
ftit	O
st	O
since	O
for	O
each	O
time	O
t	O
we	O
have	O
a	O
set	O
of	O
means	O
indexed	O
by	O
it	O
st	O
although	O
we	O
drop	O
these	O
dependencies	O
in	O
the	O
notation	O
used	O
here	O
urally	O
this	O
gives	O
rise	O
to	O
a	O
mixture	B
of	O
gaussians	O
for	O
an	O
important	O
remark	O
is	O
that	O
many	O
techniques	O
approximate	B
phtst	O
using	O
a	O
single	O
gaussian	B
natst	O
phtst	O
however	O
in	O
making	O
a	O
single	O
gaussian	B
approximation	B
to	O
phtst	O
the	O
representation	O
of	O
the	O
posterior	B
may	O
be	O
poor	O
our	O
aim	O
here	O
is	O
to	O
maintain	O
an	O
accurate	O
approximation	B
to	O
phtst	O
by	O
using	O
a	O
mixture	B
of	O
gaussians	O
to	O
find	O
a	O
recursion	O
for	O
the	O
approximating	O
distribution	B
we	O
first	O
assume	O
that	O
we	O
know	O
the	O
filtered	O
approximation	B
qht	O
and	O
then	O
propagate	O
this	O
forwards	O
using	O
the	O
exact	O
dynamics	O
to	O
do	O
so	O
consider	O
first	O
the	O
relation	O
stit	O
stit	O
st	O
it	O
wherever	O
possible	O
we	O
now	O
substitute	O
the	O
exact	O
dynamics	O
and	O
evaluate	O
each	O
of	O
the	O
two	O
factors	O
above	O
the	O
usefulness	O
of	O
decomposing	O
the	O
update	O
in	O
this	O
way	O
is	O
that	O
the	O
new	O
filtered	O
approximation	B
is	O
of	O
the	O
form	O
of	O
a	O
gaussian	B
mixture	B
where	O
it	O
is	O
gaussian	B
and	O
qst	O
are	O
the	O
weights	O
or	O
mixing	O
proportions	O
of	O
the	O
components	O
we	O
describe	O
below	O
how	O
to	O
compute	O
these	O
terms	O
explicitly	O
produces	O
a	O
new	O
gaussian	B
mixture	B
with	O
i	O
s	O
components	O
which	O
we	O
will	O
collapse	O
back	O
to	O
i	O
components	O
at	O
the	O
end	O
of	O
the	O
computation	O
evaluating	O
it	O
we	O
aim	O
to	O
find	O
a	O
filtering	O
recursion	O
for	O
it	O
since	O
this	O
is	O
conditional	B
on	O
switch	O
states	O
and	O
components	O
this	O
corresponds	O
to	O
a	O
single	O
lds	O
forward	O
step	O
which	O
can	O
be	O
evaluated	O
by	O
considering	O
first	O
the	O
joint	B
distribution	B
it	O
ht	O
st	O
it	O
and	O
subsequently	O
conditioning	B
on	O
in	O
the	O
above	O
we	O
used	O
the	O
exact	O
dynamics	O
where	O
possible	O
states	O
that	O
we	O
know	O
the	O
filtered	O
information	O
up	O
to	O
time	O
t	O
in	O
addition	O
to	O
knowing	O
the	O
switch	O
states	O
st	O
draft	O
march	O
gaussian	B
sum	I
filtering	I
and	O
the	O
mixture	B
component	O
index	O
it	O
to	O
ease	O
the	O
burden	O
on	O
notation	O
we	O
derive	O
this	O
for	O
ht	O
vt	O
for	O
all	O
t	O
the	O
exact	O
forward	O
dynamics	O
is	O
then	O
given	O
by	O
given	O
the	O
mixture	B
component	O
index	O
it	O
it	O
st	O
n	O
fit	O
st	O
fit	O
st	O
we	O
propagate	O
this	O
gaussian	B
with	O
the	O
exact	O
dynamics	O
equation	B
then	O
it	O
is	O
a	O
gaussian	B
with	O
covariance	B
and	O
mean	B
elements	O
hh	O
vv	O
vh	O
hh	O
t	O
these	O
results	O
are	O
obtained	O
from	O
integrating	O
the	O
forward	O
dynamics	O
equations	O
over	O
ht	O
using	O
the	O
results	O
in	O
hv	O
v	O
st	O
h	O
st	O
to	O
find	O
it	O
we	O
condition	O
it	O
on	O
using	O
the	O
standard	O
gaussian	B
conditioning	B
formulae	O
to	O
obtain	O
it	O
n	O
hv	O
hv	O
with	O
hv	O
h	O
hv	O
vv	O
v	O
hv	O
hh	O
hv	O
vv	O
vh	O
where	O
the	O
quantities	O
required	O
are	O
defined	O
in	O
equation	B
evaluating	O
the	O
mixture	B
weights	O
qst	O
up	O
to	O
a	O
normalisation	B
constant	I
the	O
mixture	B
weight	B
in	O
equation	B
can	O
be	O
found	O
from	O
qst	O
st	O
st	O
the	O
first	O
factor	B
in	O
equation	B
st	O
is	O
gaussian	B
with	O
mean	B
v	O
and	O
covariance	B
vv	O
as	O
given	O
in	O
equation	B
the	O
last	O
two	O
factors	O
qitst	O
and	O
are	O
given	O
from	O
the	O
previous	O
filtered	O
iteration	B
finally	O
st	O
is	O
found	O
from	O
st	O
augmented	B
slds	O
standard	O
slds	O
where	O
the	O
result	O
above	O
for	O
the	O
standard	O
slds	O
follows	O
from	O
the	O
independence	B
assumptions	O
present	O
in	O
the	O
standard	O
slds	O
in	O
the	O
aslds	O
the	O
term	O
in	O
equation	B
will	O
generally	O
need	O
to	O
be	O
computed	O
numerically	O
a	O
simple	O
approximation	B
is	O
to	O
evaluate	O
equation	B
at	O
the	O
mean	B
value	B
of	O
the	O
distribution	B
qhtit	O
st	O
to	O
take	O
covariance	B
information	O
into	O
account	O
an	O
alternative	O
would	O
be	O
to	O
draw	O
samples	O
from	O
the	O
gaussian	B
qhtit	O
st	O
and	O
thus	O
approximate	B
the	O
average	B
of	O
st	O
by	O
sampling	B
note	O
that	O
this	O
does	O
not	O
equate	O
gaussian	B
sum	I
filtering	I
with	O
a	O
sequential	B
sampling	B
procedure	O
such	O
as	O
particle	O
filtering	O
the	O
sampling	B
here	O
is	O
exact	O
for	O
which	O
no	O
convergence	O
issues	O
arise	O
closing	O
the	O
recursion	O
we	O
are	O
now	O
in	O
a	O
position	O
to	O
calculate	O
equation	B
for	O
each	O
setting	O
of	O
the	O
variable	O
we	O
have	O
a	O
mixture	B
of	O
i	O
s	O
gaussians	O
to	O
prevent	O
the	O
number	O
of	O
components	O
increasing	O
exponentially	O
with	O
time	O
we	O
numerically	O
collapse	O
back	O
to	O
i	O
gaussians	O
to	O
form	O
any	O
method	O
of	O
choice	O
may	O
be	O
supplied	O
to	O
collapse	O
a	O
mixture	B
to	O
a	O
smaller	O
mixture	B
a	O
straightforward	O
approach	B
is	O
to	O
repeatedly	O
merge	O
low-weight	O
components	O
as	O
explained	O
in	O
in	O
this	O
way	O
the	O
new	O
mixture	B
coefficients	O
i	O
are	O
defined	O
this	O
completes	O
the	O
description	O
of	O
how	O
to	O
form	O
a	O
recursion	O
for	O
the	O
continuous	B
filtered	O
posterior	B
approximation	B
in	O
equation	B
draft	O
march	O
gaussian	B
sum	I
filtering	I
figure	O
gaussian	B
sum	I
filtering	I
the	O
leftmost	O
column	O
depicts	O
the	O
previous	O
gaussian	B
mixture	B
approximation	B
qht	O
for	O
two	O
states	O
s	O
and	O
blue	O
and	O
three	O
mixture	B
components	O
i	O
with	O
the	O
mixture	B
weight	B
represented	O
by	O
the	O
area	O
of	O
each	O
oval	O
there	O
are	O
s	O
different	O
linear	B
systems	O
which	O
take	O
each	O
of	O
the	O
components	O
of	O
the	O
mixture	B
into	O
a	O
new	O
filtered	O
state	O
the	O
colour	O
of	O
the	O
arrow	O
indicating	O
which	O
dynamic	B
system	O
is	O
used	O
after	O
one	O
time-step	O
each	O
mixture	B
component	O
branches	O
into	O
a	O
further	O
s	O
components	O
so	O
that	O
the	O
joint	B
approximation	B
contains	O
components	O
column	O
to	O
keep	O
the	O
representation	O
computationally	O
tractable	O
the	O
mixture	B
of	O
gaussians	O
for	O
each	O
state	O
is	O
collapsed	O
back	O
to	O
i	O
components	O
this	O
means	O
that	O
each	O
coloured	O
state	O
needs	O
to	O
be	O
approximated	O
by	O
a	O
smaller	O
i	O
component	O
mixture	B
of	O
gaussians	O
there	O
are	O
many	O
ways	O
to	O
achieve	O
this	O
a	O
naive	O
but	O
computationally	O
efficient	O
approach	B
is	O
to	O
simply	O
ignore	O
the	O
lowest	O
weight	B
components	O
as	O
depicted	O
on	O
the	O
right	O
column	O
see	O
discrete	B
filtering	O
a	O
recursion	O
for	O
the	O
switch	O
variable	O
distribution	B
in	O
equation	B
is	O
it	O
st	O
itst	O
stit	O
the	O
r	O
h	O
s	O
of	O
the	O
above	O
equation	B
is	O
proportional	O
to	O
it	O
st	O
st	O
for	O
which	O
all	O
terms	O
have	O
been	O
computed	O
during	O
the	O
recursion	O
for	O
we	O
now	O
have	O
all	O
the	O
quantities	O
required	O
to	O
compute	O
the	O
gaussian	B
sum	O
approximation	B
of	O
the	O
filtering	O
forward	O
pass	O
a	O
schematic	O
representation	O
of	O
gaussian	B
sum	I
filtering	I
is	O
given	O
in	O
and	O
the	O
pseudo	B
code	O
is	O
presented	O
in	O
see	O
also	O
sldsforward	O
m	O
the	O
likelihood	B
the	O
likelihood	B
may	O
be	O
found	O
from	O
t	O
where	O
st	O
st	O
in	O
the	O
above	O
expression	O
all	O
terms	O
have	O
been	O
computed	O
in	O
forming	O
the	O
recursion	O
for	O
the	O
filtered	O
posterior	B
collapsing	O
gaussians	O
given	O
a	O
mixture	B
of	O
n	O
gaussians	O
px	O
pin	O
i	O
i	O
we	O
wish	O
to	O
collapse	O
this	O
to	O
a	O
smaller	O
k	O
n	O
mixture	B
of	O
gaussians	O
we	O
describe	O
a	O
simple	O
method	O
which	O
has	O
the	O
advantage	O
of	O
computational	O
efficiency	O
but	O
the	O
disadvantage	O
that	O
no	O
spatial	O
information	O
about	O
draft	O
march	O
gaussian	B
sum	I
smoothing	B
algorithm	B
aslds	O
forward	O
pass	O
approximate	B
the	O
filtered	O
posterior	B
t	O
phtst	O
it	O
wtit	O
stn	O
ftit	O
st	O
ftit	O
st	O
also	O
return	O
the	O
approximate	B
log-likelihood	O
l	O
log	O
it	O
are	O
the	O
number	O
of	O
components	O
in	O
each	O
gaussian	B
mixture	B
approximation	B
we	O
require	O
s	O
it	O
s	O
it	O
as	O
bs	O
hs	O
vs	O
hs	O
vs	O
for	O
to	O
s	O
do	O
p	O
p	O
end	O
for	O
for	O
t	O
to	O
t	O
do	O
for	O
st	O
to	O
s	O
do	O
for	O
i	O
to	O
it	O
and	O
s	O
to	O
s	O
do	O
xyi	O
s	O
xyi	O
s	O
p	O
ldsforwardft	O
s	O
ft	O
s	O
vt	O
p	O
s	O
st	O
i	O
s	O
wt	O
sp	O
s	O
t	O
p	O
end	O
for	O
collapse	O
the	O
it	O
s	O
mixture	B
of	O
gaussians	O
defined	O
by	O
xy	O
xy	O
and	O
weights	O
pi	O
sst	O
i	O
s	O
phtst	O
pitst	O
it	O
this	O
defines	O
the	O
new	O
means	O
ftit	O
st	O
covariances	O
ftit	O
st	O
and	O
mixture	B
weights	O
wtit	O
st	O
pitst	O
compute	O
tst	O
l	O
l	O
is	O
i	O
s	O
end	O
for	O
normalise	O
t	O
to	O
a	O
gaussian	B
with	O
it	O
stis	O
i	O
s	O
components	O
end	O
for	O
the	O
mixture	B
is	O
first	O
we	O
describe	O
how	O
to	O
collapse	O
a	O
mixture	B
to	O
a	O
single	O
gaussian	B
this	O
can	O
be	O
achieved	O
by	O
finding	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
mixture	B
distribution	B
these	O
are	O
pi	O
i	O
i	O
i	O
pi	O
i	O
i	O
t	O
i	O
t	O
to	O
collapse	O
a	O
mixture	B
then	O
to	O
a	O
k-component	O
mixture	B
we	O
may	O
first	O
retain	O
the	O
k	O
gaussians	O
with	O
the	O
largest	O
mixture	B
weights	O
the	O
remaining	O
n	O
k	O
gaussians	O
are	O
simply	O
merged	O
to	O
a	O
single	O
gaussian	B
using	O
the	O
above	O
method	O
alternative	O
heuristics	O
such	O
as	O
recursively	O
merging	O
the	O
two	O
gaussians	O
with	O
the	O
lowest	O
mixture	B
weights	O
are	O
also	O
reasonable	O
more	O
sophisticated	O
methods	O
which	O
retain	O
some	O
spatial	O
information	O
would	O
clearly	O
be	O
potentially	O
useful	O
the	O
method	O
presented	O
in	O
is	O
a	O
suitable	O
approach	B
which	O
considers	O
removing	O
gaussians	O
which	O
are	O
spatially	O
similar	O
not	O
just	O
low-weight	O
components	O
thereby	O
retaining	O
a	O
sense	O
of	O
diversity	O
over	O
the	O
possible	O
solutions	O
in	O
applications	O
with	O
many	O
thousands	O
of	O
timesteps	O
speed	O
can	O
be	O
a	O
factor	B
in	O
determining	O
which	O
method	O
of	O
collapsing	O
gaussians	O
is	O
to	O
be	O
preferred	O
relation	O
to	O
other	O
methods	O
gaussian	B
sum	I
filtering	I
can	O
be	O
considered	O
a	O
form	O
of	O
analytical	O
particle	O
filtering	O
in	O
which	O
instead	O
of	O
point	O
distributions	O
functions	O
being	O
propagated	O
gaussians	O
are	O
propagated	O
the	O
collapse	O
operation	O
to	O
a	O
smaller	O
number	O
of	O
gaussians	O
is	O
analogous	O
to	O
resampling	B
in	O
particle	O
filtering	O
since	O
a	O
gaussian	B
is	O
more	O
expressive	O
than	O
a	O
delta-function	O
the	O
gaussian	B
sum	O
filter	O
is	O
generally	O
an	O
improved	O
approximation	B
technique	O
over	O
using	O
point	O
particles	O
see	O
for	O
a	O
numerical	B
comparison	O
gaussian	B
sum	I
smoothing	B
approximating	O
the	O
smoothed	O
posterior	B
pht	O
is	O
more	O
involved	O
than	O
filtering	O
and	O
requires	O
additional	O
approximations	O
for	O
this	O
reason	O
smoothing	B
is	O
more	O
prone	O
to	O
failure	O
since	O
there	O
are	O
more	O
assumptions	O
that	O
need	O
to	O
be	O
satisfied	O
for	O
the	O
approximations	O
to	O
hold	O
the	O
route	O
we	O
take	O
here	O
is	O
to	O
assume	O
that	O
a	O
gaussian	B
draft	O
march	O
gaussian	B
sum	I
smoothing	B
pht	O
pst	O
pst	O
sum	O
filtered	O
approximation	B
has	O
been	O
carried	O
out	O
and	O
then	O
approximate	B
the	O
backward	O
pass	O
analogous	O
to	O
that	O
of	O
by	O
analogy	O
with	O
the	O
rts	O
smoothing	B
recursion	O
equation	B
the	O
exact	O
backward	O
pass	O
for	O
the	O
slds	O
reads	O
pht	O
where	O
is	O
composed	O
of	O
the	O
discrete	B
and	O
continuous	B
components	O
of	O
the	O
smoothed	O
posterior	B
at	O
the	O
next	O
time	O
step	O
the	O
recursion	O
runs	O
backwards	O
in	O
time	O
beginning	O
with	O
the	O
initialisation	O
pht	O
set	O
by	O
the	O
filtered	O
result	O
time	O
t	O
t	O
the	O
filtered	O
and	O
smoothed	O
posteriors	O
coincide	O
apart	O
from	O
the	O
fact	O
that	O
the	O
number	O
of	O
mixture	B
components	O
will	O
increase	O
at	O
each	O
step	O
computing	O
the	O
integral	O
over	O
in	O
equation	B
is	O
problematic	O
since	O
the	O
conditional	B
distribution	B
term	O
is	O
non-gaussian	O
in	O
for	O
this	O
reason	O
it	O
is	O
more	O
useful	O
derive	O
an	O
approximate	B
recursion	O
by	O
beginning	O
with	O
the	O
exact	O
relation	O
which	O
can	O
be	O
expressed	O
more	O
directly	O
in	O
terms	O
of	O
the	O
slds	O
dynamics	O
as	O
st	O
in	O
forming	O
the	O
recursion	O
we	O
assume	O
access	O
to	O
the	O
distribution	B
from	O
the	O
future	O
timestep	O
however	O
we	O
also	O
require	O
the	O
distribution	B
which	O
is	O
not	O
directly	O
known	O
and	O
needs	O
to	O
be	O
inferred	O
in	O
itself	O
a	O
computationally	O
challenging	O
task	O
in	O
the	O
expectation	B
correction	I
approach	B
one	O
assumes	O
the	O
approximation	B
resulting	O
in	O
an	O
approximate	B
recursion	O
for	O
the	O
smoothed	O
posterior	B
pst	O
st	O
where	O
represents	O
averaging	O
with	O
respect	O
to	O
the	O
distribution	B
in	O
carrying	O
out	O
the	O
approximate	B
recursion	O
we	O
will	O
end	O
up	O
with	O
a	O
mixture	B
of	O
gaussians	O
that	O
grows	O
at	O
each	O
timestep	O
to	O
avoid	O
the	O
exponential	B
explosion	O
problem	B
we	O
use	O
a	O
finite	O
mixture	B
approximation	B
and	O
plug	O
this	O
into	O
the	O
approximate	B
recursion	O
above	O
from	O
equation	B
a	O
recursion	O
for	O
the	O
approximation	B
is	O
given	O
by	O
st	O
qht	O
as	O
for	O
filtering	O
wherever	O
possible	O
we	O
replace	O
approximate	B
terms	O
by	O
their	O
exact	O
counterparts	O
and	O
parameterise	O
the	O
posterior	B
using	O
to	O
reduce	O
the	O
notational	O
burden	O
here	O
we	O
outline	O
the	O
method	O
only	O
for	O
the	O
case	O
of	O
using	O
a	O
single	O
component	O
approximation	B
in	O
both	O
the	O
forward	O
and	O
backward	O
passes	O
the	O
extension	O
to	O
using	O
a	O
mixture	B
to	O
approximate	B
each	O
is	O
conceptually	O
straightforward	O
and	O
deferred	O
to	O
in	O
the	O
single	O
gaussian	B
case	O
we	O
assume	O
we	O
have	O
a	O
gaussian	B
approximation	B
available	O
for	O
n	O
draft	O
march	O
gaussian	B
sum	I
smoothing	B
figure	O
the	O
ec	O
backpass	O
approximates	O
st	O
by	O
the	O
motivation	O
for	O
this	O
is	O
that	O
st	O
influences	O
only	O
indirectly	O
through	O
ht	O
however	O
ht	O
will	O
most	O
likely	O
be	O
heavily	O
influenced	O
by	O
so	O
that	O
not	O
knowing	O
the	O
state	O
of	O
st	O
is	O
likely	O
to	O
be	O
of	O
secondary	O
importance	B
the	O
green	O
shaded	O
node	O
is	O
the	O
variable	O
we	O
wish	O
to	O
find	O
the	O
posterior	B
for	O
the	O
values	O
of	O
the	O
blue	O
shaded	O
nodes	O
are	O
known	O
and	O
the	O
red	O
shaded	O
node	O
indicates	O
a	O
known	O
variable	O
which	O
is	O
assumed	O
unknown	O
in	O
forming	O
the	O
approximation	B
st	O
ht	O
vt	O
st	O
ht	O
vt	O
continuous	B
smoothing	B
for	O
given	O
st	O
an	O
rts	O
style	O
recursion	O
for	O
the	O
smoothed	O
continuous	B
is	O
obtained	O
from	O
equation	B
giving	O
qhtst	O
st	O
to	O
compute	O
equation	B
we	O
then	O
perform	O
a	O
single	O
update	O
of	O
the	O
lds	O
backward	O
recursion	O
discrete	B
smoothing	B
the	O
second	O
average	B
in	O
equation	B
corresponds	O
to	O
a	O
recursion	O
for	O
the	O
discrete	B
variable	O
and	O
is	O
given	O
by	O
the	O
average	B
of	O
with	O
respect	O
to	O
cannot	O
be	O
achieved	O
in	O
closed	O
form	O
a	O
simple	O
approach	B
is	O
to	O
approximate	B
the	O
average	B
by	O
evaluation	O
at	O
the	O
where	O
is	O
the	O
mean	B
of	O
with	O
respect	O
to	O
replacing	O
by	O
its	O
mean	B
gives	O
the	O
approximation	B
where	O
e	O
z	O
zt	O
and	O
z	O
ensures	O
normalisation	B
over	O
st	O
is	O
the	O
filtered	O
covariance	B
of	O
given	O
st	O
and	O
the	O
observations	O
which	O
may	O
be	O
taken	O
from	O
hh	O
in	O
equation	B
approximations	O
which	O
take	O
covariance	B
information	O
into	O
account	O
can	O
also	O
be	O
considered	O
although	O
the	O
above	O
simple	O
fast	O
method	O
may	O
suffice	O
in	O
practice	O
collapsing	B
the	I
mixture	B
from	O
and	O
we	O
now	O
have	O
all	O
the	O
terms	O
in	O
equation	B
to	O
compute	O
the	O
approximation	B
to	O
equation	B
due	O
to	O
the	O
summatino	O
over	O
in	O
equation	B
the	O
number	O
of	O
mixture	B
components	O
is	O
multiplied	O
by	O
s	O
at	O
each	O
iteration	B
to	O
prevent	O
an	O
exponential	B
explosion	O
of	O
components	O
the	O
mixture	B
equation	B
is	O
then	O
collapsed	O
to	O
a	O
single	O
gaussian	B
qht	O
qhtst	O
the	O
collapse	O
to	O
a	O
mixture	B
is	O
discussed	O
in	O
general	O
this	O
approximation	B
has	O
the	O
form	O
f	O
draft	O
march	O
gaussian	B
sum	I
smoothing	B
routine	O
needs	O
the	O
results	O
from	O
algorithm	B
aslds	O
ec	O
backward	O
pass	O
approximates	O
and	O
phtst	O
utjt	O
stn	O
st	O
gtjt	O
st	O
using	O
a	O
mixture	B
of	O
gaussians	O
jt	O
it	O
jt	O
s	O
it	O
this	O
gt	O
ft	O
gt	O
ft	O
ut	O
wt	O
for	O
t	O
t	O
to	O
do	O
for	O
s	O
to	O
s	O
s	O
fti	O
s	O
fti	O
s	O
pit	O
s	O
it	O
pi	O
s	O
to	O
s	O
i	O
to	O
it	O
to	O
do	O
end	O
for	O
for	O
st	O
to	O
s	O
do	O
collapse	O
the	O
mixture	B
defined	O
by	O
weights	O
pit	O
i	O
pi	O
st	O
means	O
st	O
and	O
covariances	O
st	O
to	O
a	O
ture	O
with	O
jt	O
components	O
this	O
defines	O
the	O
new	O
means	O
gtjt	O
st	O
covariances	O
gtjt	O
st	O
and	O
mixture	B
weights	O
utjt	O
st	O
pit	O
st	O
end	O
for	O
end	O
for	O
using	O
mixtures	O
in	O
smoothing	B
the	O
extension	O
to	O
the	O
mixture	B
case	O
is	O
straightforward	O
based	O
on	O
the	O
representation	O
pit	O
st	O
this	O
mixture	B
can	O
then	O
be	O
collapsed	O
to	O
a	O
smaller	O
mixture	B
using	O
any	O
method	O
of	O
choice	O
to	O
give	O
phtst	O
phtst	O
jt	O
qjtst	O
the	O
resulting	O
procedure	O
is	O
sketched	O
in	O
including	O
using	O
mixtures	O
in	O
both	O
the	O
forward	O
and	O
backward	O
passes	O
draft	O
march	O
phtst	O
qht	O
qjtst	O
analogously	O
to	O
the	O
case	O
with	O
a	O
single	O
component	O
it	O
st	O
the	O
average	B
in	O
the	O
last	O
line	O
of	O
the	O
above	O
equation	B
can	O
be	O
tackled	O
using	O
the	O
same	O
techniques	O
as	O
outlined	O
in	O
the	O
single	O
gaussian	B
case	O
to	O
approximate	B
it	O
st	O
we	O
consider	O
this	O
as	O
the	O
marginal	B
of	O
the	O
joint	B
distribution	B
qht	O
st	O
it	O
st	O
st	O
as	O
in	O
the	O
case	O
of	O
a	O
single	O
mixture	B
the	O
problematic	O
term	O
is	O
st	O
analogously	O
to	O
equation	B
we	O
make	O
the	O
assumption	O
st	O
meaning	O
that	O
information	O
about	O
the	O
current	O
switch	O
state	O
st	O
it	O
is	O
ignored	O
we	O
can	O
then	O
form	O
relation	O
to	O
other	O
methods	O
gaussian	B
sum	I
smoothing	B
a	O
classical	O
smoothing	B
approximation	B
for	O
the	O
slds	O
is	O
generalised	B
pseudo	B
bayes	I
in	O
gpb	O
one	O
starts	O
from	O
the	O
exact	O
recursion	O
pst	O
the	O
quantity	O
is	O
difficult	O
to	O
obtain	O
and	O
gpb	O
makes	O
the	O
approximation	B
plugging	O
this	O
into	O
equation	B
we	O
have	O
st	O
the	O
recursion	O
is	O
initialised	O
with	O
the	O
approximate	B
filtered	O
computing	O
the	O
smoothed	O
recursion	O
for	O
the	O
switch	O
states	O
in	O
gpb	O
is	O
then	O
equivalent	B
to	O
running	O
the	O
rts	O
backward	O
pass	O
on	O
a	O
hidden	B
markov	I
model	B
independently	O
of	O
the	O
backward	O
recursion	O
for	O
the	O
continuous	B
variables	O
the	O
only	O
information	O
the	O
gpb	O
method	O
uses	O
to	O
form	O
the	O
smoothed	O
distribution	B
from	O
the	O
filtered	O
distribution	B
is	O
the	O
markov	O
switch	O
transition	O
this	O
approximation	B
drops	O
information	O
from	O
the	O
future	O
since	O
information	O
passed	O
via	O
the	O
continuous	B
variables	O
is	O
not	O
taken	O
into	O
account	O
in	O
contrast	O
to	O
gpb	O
the	O
ec	O
gaussian	B
smoothing	B
technique	O
preserves	O
future	O
information	O
passing	B
through	O
the	O
continuous	B
variables	O
as	O
for	O
ec	O
gpb	O
forms	O
an	O
approximation	B
for	O
phtst	O
by	O
using	O
the	O
recursion	O
where	O
is	O
given	O
by	O
in	O
sldsbackward	O
m	O
one	O
may	O
choose	O
to	O
use	O
either	O
ec	O
or	O
gbp	O
example	O
flow	O
a	O
illustration	O
of	O
modelling	B
and	O
inference	B
with	O
a	O
slds	O
is	O
to	O
consider	O
a	O
simple	O
network	O
of	O
traffic	O
flow	O
here	O
there	O
are	O
junctions	O
a	O
b	O
c	O
d	O
and	O
traffic	O
flows	O
along	O
the	O
roads	O
in	O
the	O
direction	O
indicated	O
traffic	O
flows	O
into	O
the	O
junction	O
at	O
a	O
and	O
then	O
goes	O
via	O
different	O
routes	O
to	O
d	O
flow	O
out	O
of	O
a	O
junction	O
must	O
match	O
the	O
flow	O
in	O
to	O
a	O
junction	O
to	O
noise	O
there	O
are	O
traffic	O
light	O
switches	O
at	O
junctions	O
a	O
and	O
b	O
which	O
depending	O
on	O
their	O
state	O
route	O
traffic	O
differently	O
along	O
the	O
roads	O
using	O
to	O
denote	O
the	O
clean	O
free	O
flow	O
we	O
model	B
the	O
flows	O
using	O
the	O
switching	B
linear	B
system	O
at	O
at	O
i	O
i	O
i	O
at	O
i	O
i	O
i	O
a	O
bt	O
i	O
i	O
a	O
bt	O
i	O
i	O
b	O
ct	O
by	O
identifying	O
the	O
flows	O
at	O
time	O
t	O
with	O
a	O
dimensional	O
vector	O
hidden	B
variable	I
ht	O
we	O
can	O
write	O
the	O
above	O
flow	O
equations	O
as	O
at	O
a	O
dt	O
a	O
bt	O
b	O
dt	O
b	O
ct	O
c	O
dt	O
ht	O
asht	O
h	O
t	O
sb	O
which	O
takes	O
for	O
a	O
set	O
of	O
suitably	O
defined	O
matrices	O
as	O
indexed	O
by	O
the	O
switch	O
variable	O
s	O
sa	O
states	O
we	O
additionally	O
include	O
noise	O
terms	O
to	O
model	B
cars	O
parking	O
or	O
de-parking	O
during	O
a	O
single	O
timestep	O
the	O
covariance	B
h	O
is	O
diagonal	O
with	O
a	O
larger	O
variance	B
at	O
the	O
inflow	O
point	O
a	O
to	O
model	B
that	O
the	O
total	O
volume	O
of	O
traffic	O
entering	O
the	O
system	O
can	O
vary	O
noisy	O
measurements	O
of	O
the	O
flow	O
into	O
the	O
network	O
are	O
taken	O
at	O
a	O
at	O
v	O
draft	O
march	O
gaussian	B
sum	I
smoothing	B
a	O
d	O
b	O
c	O
figure	O
a	O
representation	O
of	O
the	O
traffic	O
flow	O
between	O
junctions	O
at	O
abcd	O
with	O
traffic	O
lights	O
at	O
a	O
and	O
b	O
if	O
sa	O
a	O
d	O
and	O
a	O
b	O
carry	O
and	O
of	O
the	O
flow	O
out	O
of	O
a	O
respectively	O
if	O
sa	O
all	O
the	O
flow	O
from	O
a	O
goes	O
through	O
a	O
d	O
for	O
sa	O
all	O
the	O
flow	O
goes	O
through	O
a	O
b	O
for	O
sb	O
the	O
flow	O
out	O
of	O
b	O
is	O
split	O
equally	O
between	O
b	O
d	O
and	O
b	O
c	O
for	O
sb	O
all	O
flow	O
out	O
of	O
b	O
goes	O
along	O
b	O
c	O
figure	O
time	O
evolution	O
of	O
the	O
traffic	O
flow	O
measured	O
at	O
two	O
points	O
in	O
the	O
network	O
sensors	O
measure	O
the	O
total	O
flow	O
into	O
the	O
network	O
at	O
and	O
the	O
total	O
flow	O
out	O
of	O
the	O
network	O
dt	O
a	O
dt	O
b	O
dt	O
c	O
dt	O
the	O
total	O
inflow	O
at	O
a	O
undergoes	O
a	O
random	O
walk	O
note	O
that	O
the	O
flow	O
measured	O
at	O
d	O
can	O
momentarily	O
drop	O
to	O
zero	O
if	O
all	O
traffic	O
is	O
routed	O
through	O
a	O
b	O
c	O
in	O
two	O
consecutive	O
time	O
steps	O
along	O
with	O
a	O
noisy	O
measurement	O
of	O
the	O
total	O
flow	O
out	O
of	O
the	O
system	O
at	O
d	O
dt	O
a	O
dt	O
b	O
dt	O
c	O
dt	O
v	O
the	O
observation	O
model	B
can	O
be	O
represented	O
by	O
vt	O
bht	O
v	O
t	O
using	O
a	O
constant	O
projection	B
matrix	B
b	O
the	O
switch	O
variables	O
follow	O
a	O
simple	O
markov	O
transition	O
pstst	O
which	O
biases	O
the	O
switches	O
to	O
remain	O
in	O
the	O
same	O
state	O
in	O
preference	O
to	O
jumping	O
to	O
another	O
state	O
see	O
demosldstraffic	O
m	O
for	O
details	O
given	O
the	O
above	O
system	O
and	O
a	O
prior	B
which	O
initialises	O
all	O
flow	O
at	O
a	O
we	O
draw	O
samples	O
from	O
the	O
model	B
using	O
forward	B
sampling	B
which	O
form	O
the	O
observations	O
using	O
only	O
the	O
observations	O
and	O
the	O
known	O
model	B
structure	B
we	O
then	O
attempt	O
to	O
infer	O
the	O
latent	B
switch	O
variables	O
and	O
traffic	O
flows	O
using	O
gaussian	B
sum	I
filtering	I
and	O
smoothing	B
method	O
with	O
mixture	B
components	O
per	O
switch	O
state	O
we	O
note	O
that	O
a	O
naive	O
hmm	B
approximation	B
based	O
on	O
discretising	O
each	O
continuous	B
flow	O
into	O
bins	O
would	O
contain	O
or	O
million	O
states	O
even	O
for	O
modest	O
size	O
problems	O
a	O
naive	O
approximation	B
based	O
on	O
discretisation	O
is	O
therefore	O
impractical	O
example	O
the	O
price	O
trend	O
the	O
following	O
is	O
a	O
simple	O
model	B
of	O
the	O
price	O
trend	O
of	O
a	O
stock	O
which	O
assumes	O
that	O
the	O
price	O
tends	O
to	O
continue	O
going	O
up	O
down	O
for	O
a	O
while	O
before	O
it	O
reverses	O
direction	O
h	O
i	O
h	O
vt	O
vst	O
here	O
represents	O
the	O
price	O
and	O
the	O
direction	O
there	O
is	O
only	O
a	O
single	O
observation	O
variable	O
at	O
each	O
time	O
which	O
is	O
the	O
price	O
plus	O
a	O
small	O
amount	O
of	O
noise	O
there	O
are	O
two	O
switch	O
states	O
domst	O
when	O
st	O
the	O
model	B
functions	O
normally	O
with	O
the	O
direction	O
being	O
equal	O
to	O
the	O
previous	O
direction	O
plus	O
a	O
small	O
amount	O
of	O
noise	O
h	O
when	O
st	O
however	O
the	O
direction	O
is	O
sampled	O
from	O
a	O
gaussian	B
with	O
a	O
large	O
variance	B
the	O
transition	O
pstst	O
is	O
set	O
so	O
that	O
normal	B
dynamics	O
is	O
more	O
likely	O
and	O
when	O
st	O
it	O
is	O
likely	O
to	O
go	O
back	O
to	O
normal	B
dynamics	O
the	O
next	O
timestep	O
full	O
details	O
are	O
in	O
sldspricemodel	O
mat	O
in	O
we	O
plot	O
some	O
samples	O
from	O
the	O
model	B
and	O
also	O
smoothed	O
inference	B
of	O
the	O
switch	O
distribution	B
showing	O
how	O
we	O
can	O
a	O
posteriori	O
infer	O
the	O
likely	O
changes	O
in	O
the	O
stock	O
price	O
direction	O
see	O
also	O
draft	O
march	O
reset	O
models	O
figure	O
given	O
the	O
observations	O
from	O
we	O
infer	O
the	O
flows	O
and	O
switch	O
states	O
of	O
all	O
the	O
latent	B
the	O
correct	O
latent	B
flows	O
through	O
time	O
along	O
with	O
the	O
switch	O
variable	O
state	O
used	O
to	O
variables	O
generate	O
the	O
data	O
the	O
colours	O
corresponds	O
to	O
the	O
flows	O
at	O
the	O
corresponding	O
coloured	O
edgesnodes	O
in	O
filtered	O
flows	O
based	O
on	O
a	O
i	O
gaussian	B
sum	O
forward	O
pass	O
approximation	B
plotted	O
are	O
the	O
components	O
of	O
the	O
vector	O
with	O
the	O
posterior	B
distribution	B
of	O
the	O
sa	O
and	O
sb	O
traffic	O
light	O
states	O
smoothed	O
flows	O
and	O
corresponding	O
smoothed	O
switch	O
psa	O
states	O
using	O
a	O
gaussian	B
sum	I
smoothing	B
approximation	B
with	O
j	O
plotted	O
below	O
t	O
figure	O
the	O
top	O
panel	O
is	O
a	O
time	O
series	O
of	O
prices	O
the	O
prices	O
tend	O
to	O
keep	O
going	O
up	O
or	O
down	O
with	O
infrequent	O
changes	O
in	O
the	O
direction	O
based	O
on	O
fitting	O
a	O
simple	O
slds	O
model	B
to	O
capture	O
this	O
kind	O
of	O
behaviour	O
the	O
probability	O
of	O
a	O
significant	O
change	O
in	O
the	O
price	O
direction	O
is	O
given	O
in	O
the	O
panel	O
below	O
based	O
on	O
the	O
smoothed	O
distribution	B
pst	O
reset	O
models	O
reset	O
models	O
are	O
special	O
switching	B
models	O
in	O
which	O
the	O
switch	O
state	O
isolates	O
the	O
present	O
from	O
the	O
past	O
resetting	O
the	O
position	O
of	O
the	O
latent	B
dynamics	O
are	O
also	O
known	O
as	O
changepoint	B
models	O
whilst	O
these	O
models	O
are	O
rather	O
general	O
it	O
can	O
be	O
helpful	O
to	O
consider	O
a	O
specific	O
model	B
and	O
here	O
we	O
consider	O
the	O
slds	O
changepoint	B
model	B
with	O
two	O
states	O
we	O
use	O
the	O
state	O
st	O
to	O
denote	O
that	O
the	O
lds	O
continues	O
with	O
the	O
standard	O
dynamics	O
with	O
st	O
however	O
the	O
continuous	B
dynamics	O
is	O
reset	O
to	O
a	O
prior	B
n	O
st	O
st	O
aht	O
st	O
st	O
phtht	O
st	O
where	O
n	O
similarly	O
we	O
write	O
pvtht	O
st	O
for	O
simplicity	O
we	O
assume	O
the	O
switch	O
dynamics	O
are	O
first	O
order	O
markov	O
with	O
transition	O
pstst	O
under	O
this	O
model	B
the	O
dynamics	O
follows	O
a	O
standard	O
lds	O
but	O
when	O
st	O
ht	O
is	O
reset	O
to	O
a	O
value	B
drawn	O
from	O
draft	O
march	O
reset	O
models	O
figure	O
the	O
independence	B
structure	B
of	O
a	O
reset	B
model	B
square	O
nodes	O
ct	O
denote	O
the	O
binary	O
reset	O
variables	O
and	O
st	O
the	O
state	O
dynamics	O
the	O
ht	O
are	O
continuous	B
variables	O
and	O
vt	O
continuous	B
observations	O
if	O
the	O
dynamics	O
resets	O
the	O
dependence	O
of	O
the	O
continuous	B
ht	O
on	O
the	O
past	O
is	O
cut	B
a	O
gaussian	B
distribution	B
independent	O
of	O
the	O
past	O
such	O
models	O
might	O
be	O
of	O
interest	O
in	O
prediction	O
where	O
the	O
time-series	O
is	O
following	O
a	O
trend	O
but	O
suddenly	O
changes	O
and	O
the	O
past	O
is	O
forgotten	O
whilst	O
this	O
may	O
not	O
seem	O
like	O
a	O
big	O
change	O
to	O
the	O
model	B
this	O
model	B
is	O
computationally	O
more	O
tractable	O
scaling	O
with	O
compared	O
to	O
in	O
the	O
general	O
two-state	O
slds	O
to	O
see	O
this	O
consider	O
the	O
filtering	O
recursion	O
st	O
ht	O
st	O
pvtht	O
stphtht	O
stpstst	O
st	O
st	O
we	O
now	O
consider	O
the	O
two	O
cases	O
st	O
st	O
pst	O
st	O
pst	O
st	O
ht	O
ht	O
st	O
st	O
shows	O
that	O
pht	O
st	O
is	O
not	O
a	O
mixture	B
model	B
in	O
ht	O
but	O
contains	O
only	O
a	O
single	O
component	O
proportional	O
to	O
if	O
we	O
use	O
this	O
information	O
in	O
equation	B
we	O
have	O
st	O
ht	O
st	O
ht	O
st	O
assuming	O
st	O
is	O
a	O
mixture	B
distribution	B
with	O
k	O
components	O
then	O
st	O
will	O
be	O
a	O
in	O
general	O
therefore	O
st	O
will	O
contain	O
t	O
components	O
and	O
mixture	B
with	O
k	O
components	O
st	O
a	O
single	O
component	O
as	O
opposed	O
to	O
the	O
full	O
slds	O
case	O
the	O
number	O
of	O
components	O
therefore	O
grows	O
only	O
linearly	O
with	O
time	O
as	O
opposed	O
to	O
exponentially	O
this	O
means	O
that	O
the	O
computational	O
effort	O
to	O
perform	O
exact	O
filtering	O
scales	O
as	O
run-length	O
formalism	O
one	O
may	O
also	O
describe	O
reset	O
models	O
using	O
a	O
run-length	O
formalism	O
using	O
at	O
each	O
time	O
t	O
a	O
latent	B
variable	I
rt	O
which	O
describes	O
the	O
length	O
of	O
the	O
current	O
if	O
there	O
is	O
a	O
change	O
the	O
run-length	O
variable	O
is	O
reset	O
to	O
zero	O
otherwise	O
it	O
is	O
increased	O
by	O
rt	O
rt	O
rt	O
pcp	O
prtrt	O
pcp	O
t	O
draft	O
march	O
where	O
pcp	O
is	O
the	O
probability	O
of	O
a	O
reset	O
changepoint	B
the	O
joint	B
distribution	B
is	O
given	O
by	O
prtrt	O
rt	O
rt	O
pvtvt	O
rtt	O
with	O
the	O
understanding	O
that	O
if	O
rt	O
then	O
pvtvt	O
rtt	O
pvt	O
the	O
graphical	O
model	B
of	O
this	O
distribution	B
is	O
awkward	O
to	O
draw	O
since	O
the	O
number	O
of	O
links	O
depends	O
on	O
the	O
run-length	O
rt	O
predictions	O
can	O
be	O
made	O
using	O
reset	O
models	O
rt	O
prt	O
rt	O
rt	O
where	O
the	O
filtered	O
run-length	O
is	O
given	O
by	O
the	O
forward	O
recursion	O
prt	O
rt	O
vt	O
pvtrtrt	O
prtrt	O
rtt	O
rt	O
prt	O
vtrt	O
which	O
shows	O
that	O
filtered	O
inference	B
scales	O
with	O
rt	O
a	O
poisson	B
reset	B
model	B
the	O
changepoint	B
structure	B
is	O
not	O
limited	O
to	O
conditionally	O
gaussian	B
cases	O
only	O
to	O
illustrate	O
this	O
we	O
consider	O
the	O
following	O
at	O
each	O
time	O
t	O
we	O
observe	O
a	O
count	O
yt	O
which	O
we	O
assume	O
is	O
poisson	B
distributed	O
with	O
an	O
unknown	O
positive	O
intensity	O
h	O
the	O
intensity	O
is	O
constant	O
but	O
at	O
certain	O
unknown	O
times	O
t	O
it	O
jumps	O
to	O
a	O
new	O
value	B
the	O
indicator	O
variable	O
ct	O
denotes	O
whether	O
time	O
t	O
is	O
such	O
a	O
changepoint	B
or	O
not	O
mathematically	O
the	O
model	B
is	O
pct	O
bect	O
pvtht	O
povt	O
ht	O
phtht	O
ct	O
i	O
ht	O
i	O
b	O
the	O
symbols	O
g	O
be	O
and	O
po	O
denote	O
the	O
gamma	B
bernoulli	B
and	O
the	O
poisson	B
distributions	O
respectively	O
gh	O
a	O
b	O
exp	O
log	O
h	O
bh	O
log	O
a	O
log	O
b	O
bec	O
exp	O
log	O
c	O
pov	O
h	O
exp	O
log	O
h	O
h	O
log	O
given	O
observed	O
counts	O
the	O
task	O
is	O
to	O
find	O
the	O
posterior	B
probability	O
of	O
a	O
change	O
and	O
the	O
associated	O
intensity	O
levels	O
for	O
each	O
region	O
between	O
two	O
consecutive	O
changepoints	O
plugging	O
the	O
above	O
definitions	O
in	O
the	O
generic	O
updates	O
equation	B
and	O
equation	B
we	O
see	O
that	O
ct	O
is	O
a	O
gamma	B
potential	B
and	O
that	O
ct	O
is	O
a	O
mixture	B
of	O
gamma	B
potentials	O
where	O
a	O
gamma	B
potential	B
is	O
defined	O
as	O
elgh	O
a	O
b	O
via	O
the	O
triple	O
b	O
l	O
for	O
the	O
corrector	O
update	O
step	O
we	O
need	O
to	O
calculate	O
the	O
product	O
of	O
a	O
poisson	B
term	O
with	O
the	O
observation	O
model	B
pvtht	O
povt	O
ht	O
a	O
useful	O
property	O
of	O
the	O
poisson	B
distribution	B
is	O
that	O
given	O
the	O
observation	O
the	O
latent	B
variable	I
is	O
gamma	B
distributed	O
pov	O
h	O
v	O
log	O
h	O
h	O
log	O
log	O
h	O
h	O
log	O
gh	O
v	O
hence	O
the	O
update	O
equation	B
requires	O
multiplication	O
of	O
two	O
gamma	B
potentials	O
a	O
nice	O
property	O
of	O
the	O
gamma	B
density	B
is	O
that	O
the	O
product	O
of	O
two	O
gamma	B
densities	O
is	O
also	O
a	O
gamma	B
potential	B
example	O
is	O
due	O
to	O
taylan	O
cemgil	O
draft	O
march	O
reset	O
models	O
where	O
log	O
the	O
recursions	O
for	O
this	O
reset	B
model	B
are	O
therefore	O
closed	O
in	O
the	O
space	O
of	O
a	O
mixture	B
of	O
gamma	B
potentials	O
with	O
an	O
additional	O
gamma	B
potential	B
in	O
the	O
mixture	B
at	O
each	O
timestep	O
a	O
similar	O
approach	B
can	O
be	O
used	O
to	O
form	O
the	O
smoothing	B
recursions	O
example	O
mining	O
disasters	O
we	O
illustrate	O
the	O
algorithm	B
on	O
the	O
coal	O
mining	O
disaster	O
dataset	O
the	O
data	O
set	O
consists	O
of	O
the	O
number	O
of	O
deadly	O
coal-mining	O
disasters	O
in	O
england	O
per	O
year	O
over	O
a	O
time	O
span	O
of	O
years	O
from	O
to	O
it	O
is	O
widely	O
agreed	O
in	O
the	O
statistical	O
literature	O
that	O
a	O
change	O
in	O
the	O
intensity	O
expected	O
value	B
of	O
the	O
number	O
of	O
disasters	O
occurs	O
around	O
the	O
year	O
after	O
new	O
health	O
and	O
safety	O
regulations	O
were	O
introduced	O
in	O
we	O
show	O
the	O
marginals	O
along	O
with	O
the	O
filtering	O
density	B
note	O
that	O
we	O
are	O
not	O
constraining	O
the	O
number	O
of	O
changepoints	O
and	O
in	O
principle	O
allow	O
any	O
number	O
the	O
smoothed	O
density	B
suggests	O
a	O
sharp	O
decrease	O
around	O
t	O
figure	O
estimation	O
of	O
change	O
points	O
coal	O
mining	O
disaster	O
dataset	O
filtered	O
estimate	O
of	O
the	O
marginal	B
intensity	O
and	O
smoothed	O
estimate	O
we	O
evaluate	O
these	O
mixture	B
of	O
gamma	B
distributions	O
on	O
a	O
fixed	O
grid	O
of	O
h	O
and	O
show	O
the	O
density	B
as	O
a	O
function	B
of	O
t	O
here	O
darker	O
color	O
means	O
higher	O
probability	O
hmm-reset	O
the	O
reset	B
model	B
defined	O
by	O
equations	O
above	O
is	O
useful	O
in	O
many	O
applications	O
but	O
is	O
limited	O
since	O
only	O
a	O
single	O
dynamical	O
model	B
is	O
considered	O
an	O
important	O
extension	O
is	O
to	O
consider	O
a	O
set	O
of	O
available	O
dynamical	O
models	O
indexed	O
by	O
st	O
s	O
with	O
a	O
reset	O
that	O
cuts	O
dependency	O
of	O
the	O
continuous	B
variable	O
on	O
the	O
st	O
phtht	O
st	O
ct	O
ct	O
ct	O
with	O
the	O
reset	O
recursions	O
on	O
replacing	O
ht	O
by	O
st	O
to	O
see	O
this	O
we	O
consider	O
the	O
filtering	O
recursion	O
for	O
the	O
two	O
cases	O
the	O
computational	B
complexity	I
of	O
filtering	O
for	O
this	O
model	B
is	O
which	O
can	O
be	O
understood	O
by	O
analogy	O
the	O
states	O
st	O
follow	O
a	O
markovian	O
dynamics	O
pstst	O
ct	O
see	O
a	O
reset	O
occurs	O
if	O
the	O
state	O
st	O
changes	O
otherwise	O
no	O
reset	O
occurs	O
pct	O
st	O
i	O
st	O
stpstst	O
ct	O
st	O
ct	O
st	O
ct	O
ht	O
st	O
st	O
ct	O
ctpct	O
st	O
st	O
ct	O
st	O
ht	O
st	O
draft	O
march	O
pct	O
st	O
ct	O
ct	O
of	O
accidentsfiltered	O
exercises	O
figure	O
the	O
independence	B
structure	B
of	O
a	O
hmm-reset	O
model	B
square	O
nodes	O
ct	O
denote	O
discrete	B
switch	O
variables	O
ht	O
are	O
continuous	B
latent	B
variables	O
and	O
vt	O
continuous	B
observations	O
the	O
discrete	B
state	O
ct	O
determines	O
which	O
linear	B
dynamical	I
system	I
from	O
a	O
finite	O
set	O
of	O
linear	B
dynamical	O
systems	O
is	O
operational	O
at	O
time	O
t	O
from	O
equation	B
we	O
see	O
that	O
st	O
ct	O
contains	O
only	O
a	O
single	O
component	O
proportional	O
to	O
this	O
is	O
therefore	O
exactly	O
analogous	O
to	O
the	O
standard	O
reset	B
model	B
except	O
that	O
we	O
need	O
now	O
to	O
index	O
a	O
set	O
of	O
messages	O
with	O
st	O
therefore	O
each	O
message	B
taking	O
o	O
steps	O
to	O
compute	O
the	O
computational	O
effort	O
to	O
perform	O
exact	O
filtering	O
scales	O
as	O
code	O
sldsforward	O
m	O
slds	O
forward	O
sldsbackward	O
m	O
slds	O
backward	O
correction	O
collapse	O
a	O
mixture	B
of	O
gaussians	O
to	O
a	O
smaller	O
mixture	B
of	O
gaussians	O
sldsmarggauss	O
m	O
marginalise	O
an	O
slds	O
gaussian	B
mixture	B
logeps	O
m	O
logarithm	O
with	O
offset	O
to	O
deal	O
with	O
demosldstraffic	O
m	O
demo	O
of	O
traffic	O
flow	O
using	O
a	O
switching	B
linear	B
dynamical	I
system	I
exercises	O
exercise	O
consider	O
the	O
setup	O
described	O
in	O
for	O
which	O
the	O
full	O
slds	O
model	B
is	O
given	O
in	O
sldspricemodel	O
m	O
following	O
the	O
notation	O
used	O
in	O
demosldstraffic	O
m	O
given	O
the	O
data	O
in	O
the	O
vector	O
v	O
your	O
task	O
is	O
to	O
fit	O
a	O
prediction	O
model	B
to	O
the	O
data	O
to	O
do	O
so	O
approximate	B
the	O
filtered	O
distribution	B
pht	O
using	O
a	O
mixture	B
of	O
i	O
components	O
the	O
prediction	O
of	O
the	O
price	O
at	O
the	O
next	O
day	O
is	O
then	O
vpredt	O
where	O
st	O
pht	O
compute	O
the	O
mean	B
prediction	O
error	O
compute	O
the	O
mean	B
naive	O
prediction	O
error	O
which	O
corresponds	O
to	O
saying	O
that	O
tomorrow	O
s	O
price	O
will	O
be	O
the	O
same	O
as	O
today	O
s	O
you	O
might	O
find	O
sldsmarggauss	O
m	O
of	O
interest	O
exercise	O
the	O
data	O
in	O
are	O
observed	O
prices	O
from	O
an	O
intermittent	O
mean-reverting	O
process	O
contained	O
in	O
meanrev	O
mat	O
there	O
are	O
two	O
states	O
s	O
there	O
is	O
a	O
true	O
price	O
pt	O
and	O
an	O
observed	O
price	O
vt	O
is	O
plotted	O
when	O
s	O
the	O
true	O
underlying	O
price	O
reverts	O
back	O
towards	O
the	O
mean	B
m	O
with	O
rate	O
r	O
otherwise	O
the	O
true	O
price	O
follows	O
a	O
random	O
walk	O
rpt	O
m	O
m	O
p	O
t	O
pt	O
p	O
t	O
pt	O
st	O
st	O
draft	O
march	O
exercises	O
figure	O
data	O
from	O
an	O
intermittent	O
mean-reverting	O
process	O
see	O
where	O
p	O
t	O
n	O
p	O
n	O
p	O
t	O
st	O
st	O
t	O
the	O
observed	O
price	O
vt	O
is	O
related	O
to	O
the	O
unknown	O
price	O
pt	O
by	O
vt	O
n	O
pt	O
it	O
is	O
known	O
that	O
of	O
the	O
time	O
is	O
in	O
the	O
same	O
state	O
as	O
at	O
time	O
t	O
and	O
that	O
at	O
time	O
t	O
either	O
state	O
of	O
s	O
is	O
equally	O
likely	O
also	O
at	O
t	O
n	O
m	O
based	O
on	O
this	O
information	O
and	O
using	O
gaussian	B
sum	I
filtering	I
with	O
i	O
components	O
sldsforward	O
m	O
what	O
is	O
the	O
probability	O
at	O
time	O
t	O
that	O
the	O
dynamics	O
is	O
following	O
a	O
random	O
walk	O
repeat	O
this	O
computation	O
for	O
smoothing	B
based	O
on	O
using	O
expectation	B
correction	I
with	O
i	O
j	O
components	O
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
distributed	B
computation	I
introduction	O
how	O
natural	B
organisms	O
process	O
information	O
is	O
a	O
fascinating	O
subject	O
and	O
one	O
of	O
the	O
grand	O
challenges	O
of	O
science	O
whilst	O
this	O
subject	O
is	O
still	O
in	O
its	O
early	O
stages	O
loosely	O
speaking	O
there	O
are	O
some	O
generic	O
properties	B
that	O
most	O
such	O
systems	O
are	O
believed	O
to	O
possess	O
patterns	O
are	O
stored	O
in	O
a	O
set	O
of	O
neurons	O
recall	O
of	O
patterns	O
is	O
robust	O
to	O
noise	O
transmission	O
between	O
neurons	O
is	O
of	O
a	O
binary	O
nature	O
and	O
is	O
stochastic	O
information	O
processing	O
is	O
distributed	O
and	O
highly	O
modular	O
in	O
this	O
chapter	O
we	O
discuss	O
some	O
of	O
the	O
classical	O
toy	O
models	O
that	O
have	O
been	O
developed	O
as	O
a	O
test	O
bed	O
for	O
analysing	O
such	O
in	O
particular	O
we	O
discuss	O
some	O
classical	O
models	O
from	O
a	O
probabilistic	B
viewpoint	O
stochastic	O
hopfield	O
networks	O
hopfield	O
networks	O
are	O
models	O
of	O
biological	O
memory	O
in	O
which	O
a	O
pattern	O
is	O
represented	O
by	O
the	O
activity	O
of	O
a	O
set	O
of	O
v	O
interconnected	O
neurons	O
the	O
term	O
network	O
here	O
refers	O
to	O
the	O
set	O
of	O
neurons	O
see	O
and	O
not	O
the	O
belief	B
network	I
representation	O
of	O
distribution	B
of	O
neural	O
states	O
unrolled	O
through	O
time	O
at	O
time	O
t	O
neuron	O
i	O
fires	O
vit	O
or	O
is	O
quiescent	O
vit	O
firing	O
depending	O
on	O
the	O
states	O
of	O
the	O
neurons	O
at	O
the	O
preceding	O
time	O
t	O
explicitly	O
neuron	O
i	O
fires	O
depending	O
on	O
the	O
potential	B
ait	O
i	O
wijvjt	O
where	O
wij	O
characterizes	O
the	O
efficacy	O
with	O
which	O
neuron	O
j	O
transmits	O
a	O
binary	O
signal	O
to	O
neuron	O
i	O
the	O
threshold	O
i	O
relates	O
to	O
the	O
neuron	O
s	O
predisposition	O
to	O
firing	O
writing	O
the	O
state	O
of	O
the	O
network	O
at	O
time	O
t	O
as	O
vt	O
vv	O
the	O
probability	O
that	O
neuron	O
i	O
fires	O
at	O
time	O
t	O
is	O
modelled	O
as	O
pvit	O
where	O
e	O
x	O
and	O
controls	O
the	O
level	O
of	O
stochastic	O
behaviour	O
of	O
the	O
neuron	O
the	O
probability	O
of	O
being	O
in	O
the	O
quiescent	O
state	O
is	O
given	O
by	O
normalization	O
pvit	O
pvit	O
these	O
two	O
rules	O
can	O
be	O
compactly	O
written	O
as	O
pvit	O
which	O
follows	O
directly	O
from	O
x	O
learning	B
sequences	B
figure	O
a	O
depiction	O
of	O
a	O
hopfield	B
network	I
neurons	O
the	O
connectivity	O
of	O
the	O
neurons	O
is	O
described	O
by	O
a	O
weight	B
matrix	B
with	O
elements	O
wij	O
the	O
graph	B
represents	O
a	O
snapshot	O
of	O
the	O
state	O
of	O
all	O
neurons	O
at	O
time	O
t	O
which	O
simultaneously	O
update	O
as	O
function	B
of	O
the	O
network	O
at	O
the	O
previous	O
time	O
t	O
in	O
the	O
limit	O
the	O
neuron	O
updates	O
deterministically	O
vit	O
sgn	O
in	O
a	O
synchronous	O
hopfield	B
network	I
all	O
neurons	O
update	O
independently	O
and	O
simultaneously	O
so	O
that	O
we	O
can	O
represent	O
the	O
temporal	O
evolution	O
of	O
the	O
neurons	O
as	O
a	O
dynamic	B
bayes	O
network	O
pvt	O
pvit	O
given	O
this	O
toy	O
description	O
of	O
how	O
neurons	O
update	O
how	O
can	O
we	O
use	O
the	O
network	O
to	O
do	O
interesting	O
things	O
for	O
example	O
to	O
store	O
a	O
set	O
of	O
patterns	O
and	O
recall	O
them	O
under	O
some	O
cue	O
the	O
patterns	O
will	O
be	O
stored	O
in	O
the	O
weights	O
and	O
in	O
the	O
following	O
section	O
we	O
address	O
how	O
to	O
learn	O
suitable	O
parameters	O
wij	O
and	O
i	O
to	O
learn	O
temporal	O
sequences	B
based	O
on	O
a	O
simple	O
local	B
learning	B
rule	O
learning	B
sequences	B
a	O
single	O
sequence	O
given	O
a	O
sequence	O
of	O
network	O
states	O
v	O
vt	O
we	O
would	O
like	O
the	O
network	O
to	O
store	O
this	O
sequence	O
such	O
that	O
it	O
can	O
be	O
recalled	O
under	O
some	O
cue	O
that	O
is	O
if	O
the	O
network	O
is	O
initialized	O
in	O
the	O
correct	O
starting	O
state	O
of	O
the	O
training	B
sequence	O
vt	O
the	O
remainder	O
of	O
the	O
training	B
sequence	O
for	O
t	O
should	O
be	O
reproduced	O
under	O
the	O
deterministic	B
dynamics	O
equation	B
without	O
error	O
two	O
classical	O
approaches	O
to	O
learning	B
a	O
temporal	O
sequence	O
are	O
the	O
and	O
pseudo	B
inverse	I
in	O
both	O
the	O
standard	O
hebb	B
and	O
pi	O
cases	O
the	O
thresholds	O
i	O
are	O
usually	O
set	O
to	O
zero	O
standard	O
hebb	B
rule	I
t	O
wij	O
v	O
vit	O
hebb	B
a	O
neurobiologist	O
actually	O
let	O
us	O
assume	O
that	O
the	O
persistence	O
or	O
repetition	O
of	O
a	O
reverberatory	O
activity	O
trace	O
tends	O
to	O
induce	O
lasting	O
cellular	O
changes	O
that	O
add	O
to	O
its	O
stability	O
when	O
an	O
axon	O
of	O
cell	O
a	O
is	O
near	O
enough	O
to	O
excite	O
a	O
cell	O
b	O
and	O
repeatedly	O
or	O
persistently	O
takes	O
part	O
in	O
firing	O
it	O
some	O
growth	O
process	O
or	O
metabolic	O
change	O
takes	O
place	O
in	O
one	O
or	O
both	O
cells	O
such	O
that	O
a	O
s	O
efficiency	O
as	O
one	O
of	O
the	O
cells	O
firing	O
b	O
is	O
increased	O
this	O
statement	O
is	O
sometimes	O
interpreted	O
to	O
mean	B
that	O
weights	O
are	O
exclusively	O
of	O
the	O
correlation	O
form	O
equation	B
for	O
a	O
discussion	O
this	O
can	O
severely	O
limit	O
the	O
performance	B
and	O
introduce	O
adverse	O
storage	O
artifacts	O
including	O
local	B
draft	O
march	O
learning	B
sequences	B
figure	O
a	O
dynamic	B
bayesian	B
network	I
representation	O
of	O
a	O
hopfield	B
network	I
the	O
network	O
operates	O
by	O
simultaneously	O
generating	O
a	O
new	O
set	O
of	O
neuron	O
states	O
according	O
to	O
equation	B
defines	O
a	O
markov	O
transition	B
matrix	B
modelling	B
the	O
transition	O
probability	O
vt	O
and	O
furthermore	O
imposes	O
the	O
constraint	O
that	O
the	O
neurons	O
are	O
conditionally	O
independent	O
given	O
the	O
previous	O
state	O
of	O
the	O
network	O
the	O
hebb	B
rule	I
can	O
be	O
motivated	O
mathematically	O
by	O
considering	O
j	O
wijvjt	O
v	O
v	O
vi	O
t	O
vit	O
t	O
j	O
j	O
v	O
vit	O
j	O
vj	O
t	O
vi	O
v	O
j	O
vi	O
j	O
vj	O
if	O
the	O
patterns	O
are	O
uncorrelated	O
then	O
the	O
interference	O
term	O
t	O
vi	O
j	O
vj	O
vj	O
will	O
be	O
relatively	O
small	O
to	O
see	O
this	O
we	O
first	O
note	O
that	O
for	O
randomly	O
drawn	O
patterns	O
the	O
mean	B
of	O
is	O
zero	O
since	O
t	O
and	O
the	O
patterns	O
are	O
randomly	O
the	O
variance	B
is	O
therefore	O
given	O
by	O
for	O
j	O
k	O
all	O
the	O
terms	O
are	O
independent	O
and	O
contribute	O
zero	O
on	O
average	B
therefore	O
j	O
v	O
jk	O
v	O
t	O
t	O
meaning	O
that	O
the	O
sign	O
i	O
j	O
v	O
j	O
j	O
t	O
v	O
j	O
when	O
all	O
the	O
terms	O
are	O
independent	O
zero	O
mean	B
and	O
contribute	O
zero	O
hence	O
provided	O
that	O
the	O
number	O
of	O
neurons	O
v	O
is	O
significantly	O
larger	O
than	O
the	O
length	O
of	O
the	O
sequence	O
t	O
then	O
the	O
average	B
size	O
of	O
the	O
interference	O
will	O
be	O
small	O
in	O
this	O
case	O
the	O
term	O
in	O
equation	B
dominates	O
j	O
wijvjt	O
will	O
be	O
that	O
of	O
vit	O
and	O
the	O
correct	O
pattern	O
sequence	O
recalled	O
the	O
hebb	B
rule	I
is	O
capable	O
of	O
storing	O
a	O
random	O
temporal	O
sequence	O
of	O
length	O
time	O
however	O
the	O
hebb	B
rule	I
performs	O
poorly	O
for	O
the	O
case	O
of	O
correlated	O
patterns	O
since	O
interference	O
from	O
the	O
other	O
patterns	O
becomes	O
pseudo	B
inverse	I
rule	I
the	O
pi	O
rule	O
finds	O
a	O
matrix	B
wij	O
that	O
solves	O
the	O
linear	B
equations	O
wijvjt	O
vit	O
t	O
t	O
j	O
draft	O
march	O
under	O
this	O
condition	O
sgn	O
j	O
wijvjt	O
recalled	O
in	O
matrix	B
notation	O
we	O
require	O
wv	O
v	O
where	O
vit	O
t	O
t	O
w	O
v	O
vtv	O
vt	O
sgn	O
vit	O
so	O
that	O
patterns	O
will	O
be	O
correctly	O
learning	B
sequences	B
v	O
it	O
vit	O
t	O
t	O
for	O
t	O
v	O
the	O
problem	B
is	O
under-determined	O
one	O
solution	O
is	O
given	O
by	O
the	O
pseudo	B
inverse	I
the	O
pseudo	B
inverse	I
rule	I
can	O
store	O
any	O
sequence	O
of	O
v	O
linearly	B
independent	I
patterns	O
whilst	O
attractive	O
compared	O
to	O
the	O
standard	O
hebb	B
in	O
terms	O
of	O
its	O
ability	O
to	O
store	O
longer	O
correlated	O
sequences	B
this	O
rule	O
suffers	O
from	O
very	O
small	O
basins	O
of	O
attraction	O
for	O
temporally	O
correlated	O
patterns	O
see	O
the	O
maximum	B
likelihood	B
hebb	B
rule	I
an	O
alternative	O
to	O
the	O
above	O
classical	O
algorithms	O
is	O
to	O
view	O
this	O
as	O
a	O
problem	B
of	O
pattern	O
storage	O
in	O
the	O
dbn	O
equation	B
first	O
we	O
need	O
to	O
clarify	O
what	O
we	O
mean	B
by	O
store	O
given	O
that	O
we	O
initialize	O
the	O
network	O
in	O
a	O
state	O
vt	O
we	O
wish	O
that	O
the	O
remaining	O
sequence	O
will	O
be	O
generated	O
with	O
high	O
probability	O
that	O
is	O
we	O
wish	O
to	O
adjust	O
the	O
network	O
parameters	O
such	O
that	O
the	O
probability	O
pvt	O
vt	O
is	O
furthermore	O
we	O
might	O
hope	O
that	O
the	O
sequence	O
will	O
be	O
recalled	O
with	O
high	O
probability	O
not	O
just	O
when	O
initialized	O
in	O
the	O
correct	O
state	O
but	O
also	O
for	O
states	O
close	O
hamming	O
distance	O
to	O
the	O
correct	O
initial	O
state	O
due	O
to	O
the	O
markov	O
nature	O
of	O
the	O
dynamics	O
the	O
conditional	B
likelihood	B
is	O
pvt	O
vt	O
pvt	O
this	O
is	O
a	O
product	O
of	O
transitions	O
from	O
given	O
states	O
to	O
given	O
states	O
since	O
these	O
transition	O
probabilities	O
are	O
known	O
the	O
conditional	B
likelihood	B
can	O
be	O
easily	O
evaluated	O
the	O
sequence	O
log	O
likelihood	B
is	O
t	O
pvt	O
t	O
lw	O
log	O
t	O
log	O
pvt	O
log	O
our	O
task	O
is	O
then	O
to	O
find	O
weights	O
w	O
and	O
thresholds	O
that	O
maximisise	O
lw	O
there	O
is	O
no	O
closed	O
form	O
solution	O
and	O
the	O
weights	O
therefore	O
need	O
to	O
be	O
determined	O
numerically	O
this	O
corresponds	O
to	O
a	O
straightforward	O
computational	O
problem	B
since	O
the	O
log	O
likelihood	B
is	O
a	O
convex	B
function	B
to	O
show	O
this	O
we	O
compute	O
the	O
hessian	B
for	O
expositional	O
clarity	O
this	O
does	O
not	O
affect	O
the	O
conclusions	O
t	O
t	O
dwijdwkl	O
where	O
we	O
defined	O
itvkt	O
ik	O
it	O
patterns	O
can	O
also	O
be	O
considered	O
in	O
this	O
framework	O
as	O
a	O
set	O
of	O
patterns	O
that	O
map	B
to	O
each	O
other	O
draft	O
march	O
learning	B
sequences	B
figure	O
leftmost	O
panel	O
the	O
temporally	O
highlycorrelated	O
training	B
sequence	O
we	O
desire	O
to	O
store	O
the	O
other	O
panels	O
show	O
the	O
temporal	O
evolution	O
of	O
the	O
network	O
after	O
initialization	O
in	O
the	O
correct	O
starting	O
state	O
but	O
corrupted	O
with	O
noise	O
during	O
recall	O
deterministic	B
updates	O
were	O
used	O
the	O
maximum	B
likelihood	B
rule	O
was	O
trained	O
using	O
batch	B
epochs	O
with	O
see	O
also	O
demohopfield	O
m	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
hessian	B
is	O
negative	O
definite	O
and	O
hence	O
the	O
likelihood	B
has	O
a	O
single	O
global	B
maximum	O
to	O
increase	O
the	O
likelihood	B
of	O
the	O
sequence	O
we	O
can	O
use	O
a	O
simple	O
method	O
such	O
as	O
gradient	B
wnew	O
ij	O
wij	O
dl	O
dwij	O
new	O
i	O
i	O
dl	O
d	O
i	O
where	O
t	O
dl	O
dwij	O
itvit	O
t	O
dl	O
d	O
i	O
itvit	O
the	O
learning	B
rate	I
is	O
chosen	O
empirically	O
to	O
be	O
sufficiently	O
small	O
to	O
ensure	O
convergence	O
the	O
learning	B
rule	O
equation	B
can	O
be	O
seen	O
as	O
a	O
modified	O
hebb	B
learning	B
rule	O
the	O
basic	O
hebb	B
rule	I
being	O
given	O
when	O
it	O
as	O
learning	B
progresses	O
the	O
it	O
will	O
typically	O
tend	O
to	O
values	O
close	O
to	O
either	O
or	O
and	O
hence	O
the	O
learning	B
rule	O
can	O
be	O
seen	O
as	O
asymptotically	O
equivalent	B
to	O
making	O
an	O
update	O
only	O
in	O
the	O
case	O
of	O
disagreement	O
and	O
vit	O
are	O
of	O
different	O
signs	O
this	O
batch	B
training	B
procedure	O
can	O
be	O
readily	O
converted	O
to	O
an	O
online	B
in	O
which	O
an	O
update	O
occurs	O
immediately	O
after	O
the	O
presentation	O
of	O
two	O
consecutive	O
patterns	O
storage	O
capacity	B
of	O
the	O
ml	O
hebb	B
rule	I
the	O
ml	O
hebb	B
rule	I
is	O
capable	O
of	O
storing	O
a	O
sequence	O
of	O
v	O
linearly	B
independent	I
patterns	O
to	O
see	O
this	O
we	O
can	O
form	O
an	O
input-output	B
training	B
set	O
for	O
each	O
neuron	O
i	O
vit	O
t	O
t	O
each	O
neuron	O
has	O
an	O
associated	O
weight	B
vector	O
wi	O
wij	O
j	O
v	O
which	O
forms	O
a	O
logistic	B
regressor	O
or	O
in	O
the	O
limit	O
a	O
for	O
perfect	O
recall	O
of	O
the	O
patterns	O
we	O
therefore	O
need	O
only	O
that	O
the	O
patterns	O
on	O
the	O
sequence	O
be	O
linearly	B
separable	I
this	O
will	O
be	O
the	O
case	O
if	O
the	O
patterns	O
are	O
linearly	B
independent	I
regardless	O
of	O
the	O
outputs	O
vit	O
t	O
t	O
relation	O
to	O
the	O
perceptron	B
rule	O
in	O
the	O
limit	O
that	O
the	O
activation	O
is	O
large	O
vit	O
i	O
vit	O
provided	O
the	O
activation	O
and	O
desired	O
next	O
output	O
are	O
the	O
same	O
sign	O
no	O
update	O
is	O
made	O
for	O
neuron	O
i	O
in	O
this	O
limit	O
equation	B
is	O
called	O
the	O
perceptron	B
for	O
an	O
activation	O
a	O
that	O
is	O
close	O
to	O
the	O
one	O
can	O
use	O
more	O
sophisticated	O
methods	O
such	O
as	O
the	O
newton	O
method	O
or	O
conjugate	B
gradients	O
in	O
theoretical	O
neurobiology	O
the	O
emphasis	O
is	O
small	O
gradient	B
style	O
updates	O
since	O
these	O
are	O
deemed	O
to	O
be	O
biologically	O
more	O
plausible	O
draft	O
march	O
training	B
sequencetimeneuron	O
learning	B
sequences	B
figure	O
the	O
fraction	O
of	O
neurons	O
correct	O
for	O
the	O
final	O
state	O
of	O
the	O
network	O
t	O
for	O
a	O
neuron	O
hopfield	B
network	I
trained	O
to	O
store	O
a	O
length	O
sequence	O
patterns	O
after	O
initialization	O
in	O
the	O
correct	O
initial	O
state	O
at	O
t	O
the	O
hopfield	B
network	I
is	O
updated	O
deterministically	O
with	O
a	O
randomly	O
chosen	O
percentage	O
of	O
the	O
neurons	O
flipped	O
post	O
updating	O
the	O
correlated	O
sequence	O
of	O
length	O
t	O
was	O
produced	O
by	O
flipping	O
with	O
probability	O
of	O
the	O
previous	O
state	O
of	O
the	O
network	O
a	O
fraction	O
correct	O
value	B
of	O
indicates	O
perfect	O
recall	O
of	O
the	O
final	O
state	O
and	O
a	O
value	B
of	O
indicates	O
a	O
performance	B
no	O
better	O
than	O
random	B
guessing	I
of	O
the	O
final	O
state	O
for	O
maximum	B
likelihood	B
epochs	O
of	O
training	B
were	O
used	O
with	O
during	O
recall	O
deterministic	B
updates	O
were	O
used	O
the	O
results	O
presented	O
are	O
averages	O
over	O
simulations	O
resulting	O
in	O
standard	O
errors	O
of	O
the	O
order	O
of	O
the	O
symbol	O
sizes	O
decision	B
boundary	B
a	O
small	O
change	O
can	O
lead	O
to	O
a	O
different	O
sign	O
of	O
the	O
neural	O
firing	O
to	O
guard	O
against	O
this	O
it	O
is	O
common	O
to	O
include	O
a	O
stability	O
criterion	O
vit	O
m	O
vit	O
m	O
i	O
where	O
m	O
is	O
an	O
empirically	O
chosen	O
positive	O
threshold	O
example	O
a	O
correlated	O
sequence	O
in	O
we	O
consider	O
storage	O
of	O
a	O
highly-correlated	O
temporal	O
sequence	O
of	O
length	O
t	O
of	O
neurons	O
using	O
the	O
three	O
learning	B
rules	O
hebb	B
maximum	B
likelihood	B
and	O
pseudo	B
inverse	I
the	O
sequence	O
is	O
chosen	O
to	O
be	O
highly	O
correlated	O
which	O
constitutes	O
a	O
difficult	O
learning	B
task	O
the	O
thresholds	O
i	O
are	O
set	O
to	O
zero	O
throughout	O
to	O
facilitate	O
comparison	O
the	O
initial	O
state	O
of	O
the	O
training	B
sequence	O
corrupted	O
by	O
noise	O
is	O
presented	O
to	O
the	O
trained	O
networks	O
and	O
we	O
desire	O
that	O
the	O
training	B
sequence	O
will	O
be	O
generated	O
from	O
this	O
initial	O
noisy	O
state	O
whilst	O
the	O
hebb	B
rule	I
is	O
operating	O
in	O
a	O
feasible	O
limit	O
for	O
uncorrelated	O
patterns	O
the	O
strong	B
correlations	O
in	O
this	O
training	B
sequence	O
entails	O
poor	O
results	O
the	O
pi	O
rule	O
is	O
capable	O
of	O
storing	O
a	O
sequence	O
of	O
length	O
yet	O
is	O
not	O
robust	O
to	O
perturbations	O
from	O
the	O
correct	O
initial	O
state	O
the	O
maximum	B
likelihood	B
rule	O
performs	O
well	O
after	O
a	O
small	O
amount	O
of	O
training	B
stochastic	O
interpretation	O
by	O
straightforward	O
manipulations	O
the	O
weight	B
update	O
rule	O
in	O
equation	B
can	O
be	O
written	O
as	O
t	O
dl	O
dwij	O
vit	O
a	O
stochastic	O
online	B
learning	B
rule	O
is	O
therefore	O
wijt	O
vit	O
vjt	O
vjt	O
where	O
vit	O
is	O
with	O
probability	O
and	O
otherwise	O
provided	O
that	O
the	O
learning	B
rate	I
is	O
small	O
this	O
stochastic	O
updating	O
will	O
approximate	B
the	O
learning	B
rule	O
example	O
sequences	B
under	O
perpetual	O
noise	O
we	O
compare	O
the	O
performance	B
of	O
the	O
maximum	B
likelihood	B
learning	B
rule	O
zero	O
thresholds	O
with	O
the	O
standard	O
hebb	B
pseudo	B
inverse	I
and	O
perceptron	B
rule	O
for	O
learning	B
a	O
single	O
temporal	O
sequence	O
the	O
network	O
is	O
initialized	O
to	O
a	O
noise	O
corrupted	O
draft	O
march	O
probabilityfraction	O
correctsequence	O
likelihoodnoise	O
trained	O
max	O
likelihoodperceptron	O
inverse	O
learning	B
sequences	B
the	O
figure	O
original	O
t	O
binary	O
video	O
sequence	O
on	O
a	O
set	O
of	O
neurons	O
reconstructions	O
beginning	O
from	O
a	O
noise	O
perturbed	O
initial	O
state	O
every	O
odd	O
time	O
reconstruction	O
is	O
also	O
randomly	O
perturbed	O
despite	O
the	O
high	O
level	O
of	O
noise	O
the	O
basis	O
of	O
attraction	O
of	O
the	O
pattern	O
sequence	O
is	O
very	O
broad	O
and	O
the	O
patterns	O
immediately	O
fall	O
back	O
close	O
to	O
the	O
pattern	O
sequence	O
even	O
after	O
a	O
single	O
timestep	O
version	O
of	O
the	O
correct	O
initial	O
state	O
vt	O
from	O
the	O
training	B
sequence	O
the	O
dynamics	O
is	O
then	O
run	O
for	O
the	O
same	O
number	O
of	O
steps	O
as	O
the	O
length	O
of	O
the	O
training	B
sequence	O
and	O
the	O
fraction	O
of	O
bits	O
of	O
the	O
recalled	O
final	O
state	O
which	O
are	O
the	O
same	O
as	O
the	O
training	B
sequence	O
final	O
state	O
vt	O
is	O
measured	O
at	O
each	O
stage	O
in	O
the	O
dynamics	O
the	O
last	O
the	O
state	O
of	O
the	O
network	O
was	O
corrupted	O
with	O
noise	O
by	O
flipping	O
each	O
neuron	O
state	O
with	O
the	O
specified	O
flip	O
probability	O
the	O
training	B
sequences	B
are	O
produced	O
by	O
starting	O
from	O
a	O
random	O
initial	O
state	O
and	O
then	O
choosing	O
at	O
random	O
percent	O
of	O
the	O
neurons	O
to	O
flip	O
each	O
of	O
the	O
chosen	O
neurons	O
being	O
flipped	O
with	O
probability	O
giving	O
a	O
random	O
training	B
sequence	O
with	O
a	O
high	O
degree	B
of	O
temporal	O
correlation	O
the	O
standard	O
hebb	B
rule	I
performs	O
relatively	O
poorly	O
particularly	O
for	O
small	O
flip	O
rates	O
whilst	O
the	O
other	O
methods	O
perform	O
relatively	O
well	O
being	O
robust	O
at	O
small	O
flip	O
rates	O
as	O
the	O
flip	O
rate	O
increases	O
the	O
pseudo	B
inverse	I
rule	I
becomes	O
unstable	O
especially	O
for	O
the	O
longer	O
temporal	O
sequence	O
which	O
places	O
more	O
demands	O
on	O
the	O
network	O
the	O
perceptron	B
rule	O
can	O
perform	O
as	O
well	O
as	O
the	O
maximum	B
likelihood	B
rule	O
although	O
its	O
performance	B
is	O
critically	O
dependent	O
on	O
an	O
appropriate	O
choice	O
of	O
the	O
threshold	O
m	O
the	O
results	O
for	O
m	O
perceptron	B
training	B
are	O
poor	O
for	O
small	O
flip	O
rates	O
an	O
advantage	O
of	O
the	O
maximum	B
likelihood	B
rule	O
is	O
that	O
it	O
performs	O
well	O
without	O
the	O
need	O
for	O
fine	O
tuning	O
of	O
parameters	O
in	O
all	O
cases	O
batch	B
training	B
was	O
used	O
an	O
example	O
for	O
a	O
larger	O
network	O
is	O
given	O
in	O
which	O
consists	O
of	O
highly	O
correlated	O
sequences	B
for	O
such	O
short	O
sequences	B
the	O
basin	O
of	O
attraction	O
is	O
very	O
large	O
and	O
the	O
video	O
sequence	O
can	O
be	O
stored	O
robustly	O
multiple	O
sequences	B
the	O
previous	O
section	O
detailed	O
how	O
to	O
train	O
a	O
hopfield	B
network	I
for	O
a	O
single	O
temporal	O
sequence	O
we	O
now	O
address	O
the	O
learning	B
a	O
set	O
of	O
sequences	B
n	O
n	O
n	O
if	O
we	O
assume	O
that	O
the	O
sequences	B
are	O
independent	O
the	O
log	O
likelihood	B
of	O
a	O
set	O
of	O
sequences	B
is	O
the	O
sum	O
of	O
the	O
individual	O
sequences	B
the	O
gradient	B
is	O
given	O
by	O
n	O
i	O
i	O
j	O
n	O
i	O
i	O
t	O
dl	O
dwij	O
where	O
t	O
dl	O
d	O
i	O
i	O
i	O
an	O
wijvn	O
j	O
j	O
n	O
i	O
i	O
i	O
the	O
log	O
likelihood	B
remains	O
convex	O
since	O
it	O
is	O
the	O
sum	O
of	O
convex	O
functions	O
so	O
that	O
the	O
standard	O
gradient	B
based	O
learning	B
algorithms	O
can	O
be	O
used	O
here	O
as	O
well	O
draft	O
march	O
tractable	O
continuous	B
latent	B
variable	I
models	O
boolean	O
networks	O
the	O
hopfield	B
network	I
is	O
one	O
particular	O
parameterisation	B
of	O
the	O
table	O
pvit	O
however	O
less	O
constrained	O
parameters	O
may	O
be	O
considered	O
indeed	O
one	O
could	O
consider	O
the	O
fully	O
unconstrained	O
case	O
in	O
which	O
each	O
neuron	O
i	O
would	O
have	O
an	O
associated	O
parental	O
states	O
this	O
exponentially	O
large	O
number	O
of	O
states	O
is	O
impractical	O
and	O
an	O
interesting	O
restriction	O
is	O
to	O
consider	O
that	O
each	O
neuron	O
has	O
only	O
k	O
parents	B
so	O
that	O
each	O
table	O
contains	O
entries	O
learning	B
the	O
table	O
parameters	O
by	O
maximum	B
likelihood	B
is	O
straightforward	O
since	O
the	O
log	O
likelihood	B
is	O
a	O
convex	B
function	B
of	O
the	O
table	O
entries	O
hence	O
for	O
given	O
any	O
sequence	O
set	O
of	O
sequences	B
one	O
may	O
readily	O
find	O
parameters	O
that	O
maximise	O
the	O
sequence	O
reconstruction	O
probability	O
the	O
maximum	B
likelihood	B
method	O
also	O
produces	O
large	O
basins	O
of	O
attraction	O
for	O
the	O
associated	O
stochastic	O
dynamical	O
system	O
such	O
models	O
are	O
of	O
potential	B
interest	O
in	O
artificial	B
life	I
and	O
random	B
boolean	I
networks	I
in	O
which	O
emergent	O
macroscopic	O
behaviour	O
appears	O
from	O
local	B
update	O
sequence	O
disambiguation	O
a	O
limitation	O
of	O
first	O
order	O
networks	O
defined	O
on	O
visible	B
variables	O
alone	O
as	O
the	O
hopfield	B
network	I
is	O
that	O
the	O
observation	O
transition	O
v	O
is	O
the	O
same	O
every	O
time	O
the	O
joint	B
state	O
v	O
is	O
encountered	O
this	O
means	O
that	O
if	O
the	O
sequence	O
contains	O
a	O
subsequence	O
such	O
as	O
a	O
b	O
a	O
c	O
this	O
cannot	O
be	O
recalled	O
with	O
high	O
probability	O
since	O
a	O
transitions	O
to	O
different	O
states	O
depending	O
on	O
time	O
whilst	O
one	O
could	O
attempt	O
to	O
resolve	O
this	O
sequence	O
disambiguation	O
problem	B
using	O
a	O
higher	O
order	O
markov	O
model	B
to	O
account	O
for	O
a	O
longer	O
temporal	O
context	O
we	O
would	O
lose	O
biological	O
plausibility	O
using	O
latent	B
variables	O
is	O
an	O
alternative	O
way	O
to	O
sequence	O
disambiguation	O
in	O
the	O
hopfield	O
model	B
the	O
recall	O
capacity	B
can	O
be	O
increased	O
using	O
latent	B
variables	O
by	O
make	O
a	O
sequencing	O
in	O
the	O
joint	B
latent-visible	O
space	O
that	O
is	O
linearly	B
independent	I
even	O
if	O
the	O
visible	B
variable	O
sequence	O
alone	O
is	O
not	O
in	O
we	O
discuss	O
a	O
general	O
method	O
that	O
extends	O
dynamic	B
bayes	O
networks	O
defined	O
on	O
visible	B
variables	O
alone	O
such	O
as	O
the	O
hopfield	B
network	I
to	O
include	O
continuous	B
non-linearly	O
updating	O
latent	B
variables	O
without	O
requiring	O
additional	O
approximations	O
tractable	O
continuous	B
latent	B
variable	I
models	O
a	O
dynamic	B
bayes	O
network	O
with	O
latent	B
variables	O
takes	O
the	O
form	O
t	O
t	O
t	O
ht	O
as	O
we	O
saw	O
in	O
provided	O
all	O
hidden	B
variables	I
are	O
discrete	B
inference	B
in	O
these	O
models	O
is	O
straightforward	O
however	O
in	O
many	O
physical	O
systems	O
it	O
is	O
more	O
natural	B
to	O
assume	O
continuous	B
ht	O
in	O
we	O
saw	O
that	O
one	O
such	O
tractable	O
continuous	B
ht	O
model	B
is	O
given	O
by	O
linear	B
gaussian	B
transitions	O
and	O
emissions	O
the	O
lds	O
whilst	O
this	O
is	O
useful	O
we	O
cannot	O
represent	O
non-linear	B
changes	O
in	O
the	O
latent	B
process	O
using	O
an	O
lds	O
alone	O
the	O
switching	B
lds	O
of	O
is	O
able	O
to	O
model	B
non-linear	B
continuous	B
dynamics	O
switching	B
although	O
we	O
saw	O
that	O
this	O
leads	O
to	O
computational	O
difficulties	O
for	O
computational	O
reasons	O
we	O
therefore	O
seem	O
limited	O
to	O
either	O
purely	O
discrete	B
h	O
no	O
limitation	O
on	O
the	O
discrete	B
transitions	O
or	O
purely	O
continuous	B
h	O
be	O
forced	O
to	O
use	O
simple	O
linear	B
dynamics	O
is	O
there	O
a	O
way	O
to	O
have	O
a	O
continuous	B
state	O
with	O
non-linear	B
dynamics	O
for	O
which	O
posterior	B
inference	B
remains	O
tractable	O
the	O
answer	O
is	O
yes	O
provided	O
that	O
we	O
assume	O
the	O
hidden	B
transitions	O
are	O
when	O
conditioned	O
on	O
the	O
visible	B
variables	O
this	O
renders	O
the	O
hidden	B
unit	O
distribution	B
trivial	O
this	O
allows	O
the	O
consideration	O
of	O
rich	O
non-linear	B
dynamics	O
in	O
the	O
hidden	B
space	O
if	O
required	O
deterministic	B
latent	B
variables	O
consider	O
a	O
belief	B
network	I
defined	O
on	O
a	O
sequence	O
of	O
visible	B
variables	O
to	O
enrich	O
the	O
model	B
we	O
include	O
additional	O
continuous	B
latent	B
variables	O
that	O
will	O
follow	O
a	O
non-linear	B
markov	O
transition	O
to	O
retain	O
tractability	O
of	O
inference	B
we	O
constrain	O
the	O
latent	B
dynamics	O
to	O
be	O
deterministic	B
described	O
by	O
pht	O
vt	O
ht	O
f	O
vt	O
ht	O
h	O
here	O
represents	O
the	O
dirac	B
delta	I
function	B
for	O
continuous	B
hidden	B
variables	I
the	O
non-linear	B
function	B
f	O
parameterises	O
the	O
cpt	O
whilst	O
the	O
restriction	O
to	O
deterministic	B
cpts	O
appears	O
severe	O
the	O
model	B
draft	O
march	O
tractable	O
continuous	B
latent	B
variable	I
models	O
ht	O
vt	O
ht	O
vt	O
figure	O
a	O
first	O
order	O
dynamic	B
bayesian	B
network	I
with	O
deterministic	B
hidden	B
cpts	O
by	O
diamonds	O
that	O
is	O
the	O
hidden	B
node	O
is	O
certainly	O
in	O
a	O
single	O
state	O
determined	O
by	O
its	O
parents	B
conditioning	B
on	O
the	O
visible	B
variables	O
forms	O
a	O
directed	B
chain	B
in	O
the	O
hidden	B
space	O
which	O
is	O
deterministic	B
hidden	B
unit	O
inference	B
can	O
be	O
achieved	O
by	O
forward	O
propagation	B
alone	O
integrating	O
out	O
hidden	B
variables	I
gives	O
a	O
cascade	B
style	O
directed	B
visible	B
graph	B
which	O
so	O
that	O
each	O
vt	O
depends	O
on	O
all	O
t	O
retains	O
some	O
attractive	O
features	O
the	O
marginal	B
t	O
is	O
non-markovian	O
coupling	O
all	O
the	O
variables	O
in	O
the	O
sequence	O
see	O
whilst	O
hidden	B
unit	O
inference	B
t	O
t	O
is	O
deterministic	B
as	O
illustrated	O
in	O
the	O
adjustable	O
parameters	O
of	O
the	O
hidden	B
and	O
visible	B
cpts	O
are	O
represented	O
by	O
h	O
and	O
v	O
respectively	O
for	O
learning	B
the	O
log	O
likelihood	B
of	O
a	O
single	O
training	B
sequence	O
v	O
is	O
to	O
maximise	O
the	O
log	O
likelihood	B
using	O
gradient	B
techniques	O
we	O
need	O
to	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
model	B
parameters	O
these	O
can	O
be	O
calculated	O
as	O
follows	O
ft	O
fvt	O
vt	O
ht	O
h	O
hence	O
the	O
derivatives	O
can	O
be	O
calculated	O
by	O
deterministic	B
forward	O
propagation	B
of	O
errors	O
alone	O
the	O
case	O
of	O
training	B
multiple	O
independently	O
generated	O
sequences	B
v	O
n	O
n	O
n	O
is	O
a	O
straightforward	O
extension	O
an	O
augmented	B
hopfield	B
network	I
to	O
make	O
the	O
deterministic	B
latent	B
variable	I
model	B
more	O
explicit	O
we	O
consider	O
the	O
case	O
of	O
continuous	B
hidden	B
units	O
and	O
discrete	B
binary	O
visible	B
units	O
vit	O
in	O
particular	O
we	O
restrict	O
attention	O
to	O
the	O
hopfield	O
model	B
augmented	B
with	O
latent	B
variables	O
that	O
have	O
a	O
simple	O
linear	B
dynamics	O
for	O
a	O
nonlinear	O
extension	O
ht	O
bvt	O
deterministic	B
latent	B
transition	O
it	O
cht	O
dvt	O
pvt	O
ht	O
draft	O
march	O
t	O
t	O
l	O
v	O
hv	O
log	O
v	O
log	O
pvt	O
ht	O
v	O
where	O
the	O
hidden	B
unit	O
values	O
are	O
calculated	O
recursively	O
using	O
ht	O
f	O
vt	O
ht	O
h	O
log	O
v	O
v	O
log	O
pvt	O
ht	O
v	O
v	O
t	O
dl	O
d	O
v	O
dl	O
d	O
h	O
dht	O
d	O
h	O
where	O
ht	O
ft	O
h	O
log	O
pvt	O
ht	O
v	O
dht	O
ft	O
ht	O
dht	O
d	O
h	O
d	O
h	O
this	O
model	B
generalises	O
a	O
recurrent	O
stochastic	O
heteroassociative	B
hopfield	O
to	O
include	O
deterministic	B
hidden	B
units	O
dependent	O
on	O
past	O
network	O
states	O
the	O
parameters	O
of	O
the	O
model	B
are	O
a	O
b	O
c	O
d	O
for	O
gradient	B
based	O
training	B
we	O
require	O
the	O
derivatives	O
with	O
respect	O
to	O
each	O
of	O
these	O
parameters	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
for	O
a	O
generic	O
parameter	B
is	O
this	O
gives	O
all	O
indices	O
are	O
summed	O
over	O
the	O
dimensions	O
of	O
the	O
quantities	O
they	O
relate	O
to	O
neural	O
models	O
vit	O
it	O
d	O
d	O
where	O
it	O
i	O
it	O
it	O
d	O
d	O
l	O
it	O
it	O
da	O
cij	O
cij	O
d	O
d	O
j	O
d	O
d	O
j	O
j	O
hjt	O
hjt	O
db	O
da	O
db	O
d	O
dc	O
d	O
dd	O
d	O
da	O
d	O
it	O
i	O
h	O
it	O
i	O
v	O
hit	O
it	O
it	O
j	O
hit	O
db	O
it	O
j	O
aij	O
d	O
da	O
aij	O
d	O
db	O
hjt	O
i	O
h	O
hjt	O
i	O
v	O
if	O
we	O
assume	O
that	O
is	O
a	O
given	O
fixed	O
value	B
we	O
can	O
compute	O
the	O
derivatives	O
recursively	O
by	O
forward	O
propagation	B
gradient	B
based	O
training	B
for	O
this	O
augmented	B
hopfield	B
network	I
is	O
therefore	O
straightforward	O
to	O
implement	O
this	O
model	B
extends	O
the	O
power	O
of	O
the	O
original	O
hopfield	O
model	B
being	O
capable	O
of	O
resolving	O
ambiguous	O
transitions	O
in	O
sequences	B
such	O
as	O
a	O
b	O
a	O
c	O
see	O
and	O
demohopfieldlatent	O
m	O
in	O
terms	O
of	O
a	O
dynamic	B
system	O
the	O
learned	O
network	O
is	O
an	O
attractor	O
with	O
the	O
training	B
sequence	O
as	O
a	O
stable	O
point	O
and	O
demonstrates	O
that	O
such	O
models	O
are	O
capable	O
of	O
learning	B
attractor	O
recurrent	O
networks	O
more	O
powerful	O
than	O
those	O
without	O
hidden	B
units	O
example	O
disambiguation	O
the	O
sequence	O
in	O
contains	O
repeated	O
patterns	O
and	O
therefore	O
cannot	O
be	O
reliably	O
recalled	O
with	O
a	O
first	O
order	O
model	B
containing	O
visible	B
variables	O
alone	O
to	O
deal	O
with	O
this	O
we	O
consider	O
a	O
hopfield	B
network	I
with	O
visible	B
units	O
and	O
additional	O
hidden	B
units	O
with	O
deterministic	B
latent	B
dynamics	O
the	O
model	B
was	O
trained	O
with	O
gradient	B
ascent	O
to	O
maximise	O
the	O
likelihood	B
of	O
the	O
binary	O
sequence	O
in	O
as	O
shown	O
in	O
the	O
learned	O
network	O
is	O
capable	O
of	O
recalling	O
the	O
sequence	O
correctly	O
even	O
when	O
initialised	O
in	O
an	O
incorrect	O
state	O
having	O
no	O
difficulty	O
with	O
the	O
fact	O
that	O
the	O
sequence	O
transitions	O
are	O
ambiguous	O
neural	O
models	O
the	O
tractable	O
deterministic	B
latent	B
variable	I
model	B
introduced	O
in	O
presents	O
an	O
opportunity	O
to	O
extend	O
models	O
such	O
as	O
the	O
hopfield	B
network	I
to	O
include	O
more	O
biologically	O
realistic	O
processes	O
without	O
losing	O
computational	O
tractability	O
first	O
we	O
discuss	O
a	O
general	O
framework	O
for	O
learning	B
in	O
a	O
class	O
of	O
neural	O
this	O
being	O
a	O
special	O
case	O
of	O
the	O
deterministic	B
latent	B
variable	I
and	O
a	O
generalisation	B
of	O
the	O
spike-response	O
model	B
of	O
theoretical	O
draft	O
march	O
neural	O
models	O
figure	O
the	O
training	B
sequence	O
consists	O
of	O
a	O
random	O
set	O
of	O
vectors	O
the	O
reconstruction	O
using	O
h	O
hidden	B
units	O
the	O
over	O
t	O
time	O
steps	O
initial	O
state	O
vt	O
for	O
the	O
recalled	O
sequence	O
was	O
set	O
to	O
the	O
correct	O
initial	O
training	B
value	B
albeit	O
with	O
one	O
of	O
the	O
values	O
flipped	O
note	O
that	O
the	O
method	O
is	O
capable	O
of	O
sequence	O
disambiguation	O
in	O
the	O
sense	O
that	O
the	O
transitions	O
of	O
the	O
form	O
a	O
b	O
a	O
c	O
can	O
be	O
recalled	O
stochastically	O
spiking	O
neurons	O
we	O
assume	O
that	O
neuron	O
i	O
fires	O
depending	O
on	O
the	O
membrane	O
potential	B
ait	O
through	O
pvit	O
ht	O
pvit	O
to	O
be	O
specific	O
we	O
take	O
throughout	O
pvit	O
here	O
we	O
to	O
define	O
the	O
quiescent	O
state	O
as	O
vit	O
so	O
that	O
pvit	O
the	O
choice	O
of	O
the	O
sigmoid	B
function	B
is	O
not	O
fundamental	O
and	O
is	O
chosen	O
merely	O
for	O
analytical	O
convenience	O
the	O
log-likelihood	O
of	O
a	O
sequence	O
of	O
visible	B
states	O
v	O
is	O
l	O
log	O
and	O
the	O
gradient	B
of	O
the	O
log-likelihood	O
is	O
then	O
dl	O
dwij	O
dait	O
dwij	O
t	O
t	O
where	O
we	O
used	O
the	O
fact	O
that	O
vi	O
here	O
wij	O
are	O
parameters	O
of	O
the	O
membrane	O
potential	B
below	O
we	O
take	O
equation	B
as	O
common	O
in	O
the	O
following	O
models	O
in	O
which	O
the	O
membrane	O
potential	B
ait	O
is	O
described	O
with	O
increasing	O
sophistication	O
hopfield	O
membrane	O
potential	B
as	O
a	O
first	O
step	O
we	O
show	O
how	O
the	O
hopfield	B
network	I
training	B
as	O
described	O
in	O
can	O
be	O
recovered	O
as	O
a	O
special	O
case	O
of	O
the	O
above	O
framework	O
the	O
hopfield	O
membrane	O
potential	B
is	O
ait	O
wijvjt	O
bi	O
where	O
wij	O
characterizes	O
the	O
efficacy	O
of	O
information	O
transmission	O
from	O
neuron	O
j	O
to	O
neuron	O
i	O
and	O
bi	O
is	O
a	O
threshold	O
applying	O
the	O
maximum	B
likelihood	B
framework	O
to	O
this	O
model	B
to	O
learn	O
a	O
temporal	O
sequence	O
v	O
by	O
adjustment	O
of	O
the	O
parameters	O
wij	O
bi	O
are	O
fixed	O
for	O
simplicity	O
we	O
obtain	O
the	O
learning	B
rule	O
daidwij	O
vjt	O
in	O
equation	B
t	O
wnew	O
ij	O
wij	O
dl	O
dwij	O
dl	O
dwij	O
vjt	O
where	O
the	O
learning	B
rate	I
is	O
chosen	O
empirically	O
to	O
be	O
sufficiently	O
small	O
to	O
ensure	O
convergence	O
matches	O
equation	B
uses	O
the	O
encoding	O
draft	O
march	O
neural	O
models	O
figure	O
learning	B
with	O
depression	B
u	O
t	O
despite	O
the	O
apparent	O
complexity	O
of	O
the	O
dynamics	O
learning	B
appropriate	O
neural	O
connection	O
weights	O
is	O
straightforward	O
using	O
maximum	B
likelihood	B
the	O
reconstruction	O
using	O
the	O
standard	O
hebb	B
rule	I
by	O
contrast	O
is	O
dynamic	B
synapses	I
in	O
more	O
realistic	O
synaptic	O
models	O
neurotransmitter	O
generation	O
depends	O
on	O
a	O
finite	O
rate	O
of	O
cell	O
subcomponent	O
production	O
and	O
the	O
quantity	O
of	O
vesicles	O
released	O
is	O
affected	O
by	O
the	O
history	O
of	O
loosely	O
speaking	O
when	O
a	O
neuron	O
fires	O
it	O
releases	O
a	O
chemical	O
substance	O
from	O
a	O
local	B
reservoir	O
this	O
reservoir	O
being	O
refilled	O
at	O
a	O
lower	O
rate	O
than	O
the	O
neuron	O
can	O
fire	O
if	O
the	O
neuron	O
fires	O
continually	O
its	O
ability	O
to	O
continue	O
firing	O
weakens	O
since	O
the	O
reservoir	O
of	O
release	O
chemical	O
is	O
depleted	O
this	O
can	O
be	O
accounted	O
for	O
by	O
using	O
a	O
depression	B
mechanism	O
that	O
affects	O
the	O
membrane	O
potential	B
for	O
depression	B
factors	O
xjt	O
a	O
simple	O
dynamics	O
for	O
these	O
depression	B
factors	O
ait	O
wijxjtvjt	O
xjt	O
xjt	O
t	O
xjt	O
u	O
xjtvjt	O
where	O
t	O
and	O
u	O
represent	O
time	O
scales	O
recovery	O
times	O
and	O
spiking	O
effect	O
parameters	O
respectively	O
note	O
that	O
these	O
depression	B
factor	B
dynamics	O
are	O
exactly	O
of	O
the	O
form	O
of	O
deterministic	B
hidden	B
variables	I
it	O
is	O
straightforward	O
to	O
include	O
these	O
dynamic	B
synapses	I
in	O
a	O
principled	O
way	O
using	O
the	O
maximum	B
likelihood	B
learning	B
framework	O
for	O
the	O
hopfield	O
potential	B
the	O
learning	B
dynamics	O
is	O
simply	O
given	O
by	O
equations	O
with	O
dait	O
dwij	O
xjtvjt	O
example	O
with	O
depression	B
in	O
we	O
demonstrate	O
learning	B
a	O
random	O
temporal	O
sequence	O
of	O
time	O
steps	O
for	O
an	O
assembly	O
of	O
neurons	O
with	O
dynamic	B
depressive	O
synapses	O
after	O
learning	B
wij	O
the	O
trained	O
network	O
is	O
initialised	O
in	O
the	O
first	O
state	O
of	O
the	O
training	B
sequence	O
the	O
remaining	O
states	O
of	O
the	O
sequence	O
were	O
then	O
correctly	O
recalled	O
by	O
iteration	B
of	O
the	O
learned	O
model	B
the	O
corresponding	O
generated	O
factors	O
xit	O
are	O
also	O
plotted	O
for	O
comparison	O
we	O
plot	O
the	O
results	O
of	O
using	O
the	O
dynamics	O
having	O
set	O
the	O
wij	O
using	O
the	O
temporal	O
hebb	B
rule	I
equation	B
the	O
poor	O
performance	B
of	O
the	O
correlation	O
based	O
hebb	B
rule	I
demonstrates	O
the	O
necessity	O
in	O
general	O
to	O
couple	O
a	O
dynamical	O
system	O
with	O
an	O
appropriate	O
learning	B
mechanism	O
leaky	B
integrate	I
and	I
fire	I
models	O
leaky	B
integrate	I
and	I
fire	I
models	O
move	O
a	O
step	O
further	O
towards	O
biological	O
realism	O
in	O
which	O
the	O
membrane	O
potential	B
increments	O
if	O
it	O
receives	O
an	O
excitatory	O
stimulus	O
and	O
decrements	O
if	O
it	O
receives	O
an	O
inhibitory	O
stimulus	O
after	O
firing	O
the	O
membrane	O
potential	B
is	O
reset	O
to	O
a	O
low	O
value	B
below	O
the	O
firing	O
threshold	O
and	O
thereafter	O
steadily	O
increases	O
to	O
a	O
resting	O
level	O
for	O
example	O
a	O
model	B
that	O
incorporates	O
such	O
effects	O
is	O
ait	O
j	O
vit	O
vit	O
f	O
ired	O
wijvjt	O
rest	O
ait	O
draft	O
march	O
neuron	O
exercises	O
since	O
vi	O
if	O
neuron	O
i	O
fires	O
at	O
time	O
t	O
the	O
potential	B
is	O
reset	O
to	O
f	O
ired	O
at	O
time	O
t	O
similarly	O
with	O
no	O
synaptic	O
input	O
the	O
potential	B
equilibrates	O
to	O
rest	O
with	O
time	O
constant	O
log	O
despite	O
the	O
increase	O
in	O
complexity	O
of	O
the	O
membrane	O
potential	B
over	O
the	O
hopfield	O
case	O
deriving	O
appropriate	O
learning	B
dynamics	O
for	O
this	O
new	O
system	O
is	O
straightforward	O
since	O
as	O
before	O
the	O
hidden	B
variables	I
the	O
membrane	O
potentials	O
update	O
in	O
a	O
deterministic	B
fashion	O
the	O
potential	B
derivatives	O
are	O
dait	O
dwij	O
vit	O
dait	O
dwij	O
vjt	O
by	O
initialising	O
the	O
derivative	O
equations	O
define	O
a	O
first	O
order	O
recursion	O
for	O
the	O
gradient	B
which	O
can	O
be	O
used	O
to	O
adapt	O
wij	O
in	O
the	O
usual	O
manner	O
wij	O
wij	O
dldwij	O
we	O
could	O
also	O
apply	O
synaptic	O
dynamics	O
to	O
this	O
case	O
by	O
replacing	O
the	O
term	O
vjt	O
in	O
equation	B
by	O
xjtvjt	O
dwij	O
although	O
a	O
detailed	O
discussion	O
of	O
the	O
properties	B
of	O
the	O
neuronal	O
responses	O
for	O
networks	O
trained	O
in	O
this	O
way	O
is	O
beyond	O
the	O
scope	O
of	O
these	O
notes	O
an	O
interesting	O
consequence	O
of	O
the	O
learning	B
rule	O
equation	B
is	O
a	O
spike	O
time	O
dependent	O
learning	B
window	O
in	O
qualitative	O
agreement	O
with	O
experimental	O
in	O
summary	O
provided	O
one	O
deals	O
with	O
deterministic	B
latent	B
dynamics	O
essentially	O
arbitrarily	O
complex	O
spatiotemporal	O
patterns	O
may	O
potentially	O
be	O
learned	O
and	O
generated	O
under	O
cued	O
retrieval	O
for	O
very	O
complex	O
neural	O
dynamics	O
the	O
spike-response	O
model	B
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
the	O
deterministic	B
latent	B
variable	I
model	B
in	O
which	O
the	O
latent	B
variables	O
have	O
been	O
explicitly	O
integrated	O
out	O
code	O
demohopfield	O
m	O
demo	O
of	O
hopfield	O
sequence	B
learning	B
hebbml	O
m	O
gradient	B
ascent	O
training	B
of	O
a	O
set	O
of	O
sequences	B
using	O
max	O
likelihood	B
hopfieldhiddennl	O
m	O
hopfield	B
network	I
with	O
additional	O
non-linear	B
latent	B
variables	O
demohopfieldlatent	O
m	O
demo	O
of	O
hopfield	O
net	O
with	O
deterministic	B
latent	B
variables	O
hopfieldhiddenliknl	O
m	O
hopfield	O
net	O
with	O
hidden	B
variables	I
sequence	O
likelihood	B
t	O
exercises	O
exercise	O
consider	O
a	O
very	O
large	O
hopfield	B
network	I
v	O
used	O
to	O
store	O
a	O
single	O
temporal	O
sequence	O
of	O
length	O
t	O
t	O
v	O
in	O
this	O
case	O
the	O
weight	B
matrix	B
w	O
may	O
be	O
difficult	O
to	O
store	O
explain	O
how	O
to	O
justify	O
the	O
assumption	O
wij	O
uitvit	O
where	O
uit	O
are	O
the	O
dual	B
parameters	I
and	O
derive	O
an	O
update	O
rule	O
for	O
the	O
dual	B
parameters	I
u	O
exercise	O
a	O
hopfield	B
network	I
is	O
used	O
to	O
store	O
a	O
raw	O
uncompressed	O
binary	O
video	O
sequence	O
each	O
image	O
in	O
the	O
sequence	O
contains	O
binary	O
pixels	O
at	O
a	O
rate	O
of	O
frames	O
per	O
second	O
how	O
many	O
hours	O
of	O
video	O
can	O
neurons	O
store	O
exercise	O
derive	O
the	O
update	O
equation	B
exercise	O
show	O
that	O
the	O
hessian	B
equation	B
is	O
negative	O
definite	O
that	O
is	O
ijkl	O
xijxkl	O
dwijdwkl	O
for	O
any	O
x	O
draft	O
march	O
exercise	O
for	O
the	O
augmented	B
hopfield	B
network	I
of	O
latent	B
dynamics	O
hit	O
aijhjt	O
bijvjt	O
j	O
exercises	O
derive	O
the	O
derivative	O
recursions	O
described	O
in	O
draft	O
march	O
part	O
v	O
approximate	B
inference	B
chapter	O
sampling	B
introduction	O
sampling	B
concerns	O
drawing	O
realisations	O
xl	O
of	O
a	O
variable	O
x	O
from	O
a	O
distribution	B
px	O
for	O
a	O
discrete	B
variable	O
x	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
samples	O
the	O
fraction	O
of	O
samples	O
in	O
state	O
x	O
tends	O
to	O
px	O
x	O
that	O
is	O
xl	O
x	O
px	O
x	O
in	O
the	O
continuous	B
case	O
one	O
can	O
consider	O
a	O
small	O
region	O
such	O
that	O
the	O
probability	O
that	O
the	O
samples	O
occupy	O
tends	O
to	O
the	O
integral	O
of	O
px	O
over	O
in	O
other	O
words	O
the	O
relative	O
frequency	O
x	O
tends	O
to	O
x	O
px	O
given	O
a	O
finite	O
set	O
of	O
samples	O
one	O
can	O
then	O
approximate	B
expectations	O
using	O
lim	O
l	O
l	O
l	O
f	O
f	O
l	O
f	O
fxl	O
f	O
this	O
approximation	B
holds	O
for	O
both	O
discrete	B
and	O
continuous	B
variables	O
provided	O
the	O
samples	O
are	O
indeed	O
from	O
px	O
then	O
the	O
average	B
of	O
the	O
approximation	B
is	O
the	O
variance	B
of	O
the	O
approximation	B
is	O
fxl	O
pxlxl	O
l	O
px	O
px	O
hence	O
the	O
mean	B
of	O
the	O
approximation	B
is	O
the	O
exact	O
mean	B
of	O
f	O
and	O
the	O
variance	B
of	O
the	O
approximation	B
scales	O
inversely	O
with	O
the	O
number	O
of	O
samples	O
in	O
principle	O
therefore	O
provided	O
the	O
samples	O
are	O
independently	O
drawn	O
from	O
px	O
only	O
a	O
small	O
number	O
of	O
samples	O
is	O
required	O
to	O
accurately	O
estimate	O
this	O
mean	B
importantly	O
this	O
result	O
is	O
independent	O
of	O
the	O
dimension	O
of	O
x	O
however	O
the	O
critical	O
difficulty	O
is	O
in	O
actually	O
generating	O
independent	O
samples	O
from	O
px	O
drawing	O
samples	O
from	O
high-dimensional	O
distributions	O
is	O
generally	O
difficult	O
and	O
few	O
guarantees	O
exist	O
to	O
ensure	O
that	O
in	O
a	O
practical	O
timeframe	O
the	O
samples	O
produced	O
are	O
representative	O
enough	O
such	O
that	O
expectations	O
can	O
be	O
approximated	O
accurately	O
there	O
are	O
many	O
different	O
sampling	B
algorithms	O
all	O
of	O
which	O
work	O
in	O
principle	O
but	O
each	O
working	O
in	O
practice	O
only	O
when	O
the	O
distribution	B
satisfies	O
particular	O
before	O
we	O
develop	O
schemes	O
for	O
multi-variate	B
distributions	O
we	O
consider	O
the	O
univariate	B
case	O
introduction	O
figure	O
a	O
representation	O
of	O
the	O
discrete	B
distribution	B
equation	B
the	O
unit	O
interval	O
from	O
to	O
is	O
partitioned	B
in	O
parts	O
whose	O
lengths	O
are	O
equal	O
to	O
and	O
univariate	B
sampling	B
in	O
the	O
following	O
we	O
assume	O
that	O
a	O
random	O
number	O
generator	O
exists	O
which	O
is	O
able	O
to	O
produce	O
a	O
value	B
uniformly	O
at	O
random	O
from	O
the	O
unit	O
interval	O
we	O
will	O
make	O
use	O
of	O
this	O
uniform	B
random	O
number	O
generator	O
to	O
draw	O
samples	O
from	O
non-uniform	O
distributions	O
discrete	B
case	O
consider	O
the	O
one	O
dimensional	O
discrete	B
distribution	B
px	O
where	O
domx	O
with	O
x	O
x	O
x	O
px	O
this	O
represents	O
a	O
partitioning	O
of	O
the	O
unit	O
interval	O
in	O
which	O
the	O
interval	O
has	O
been	O
labelled	B
as	O
state	O
as	O
state	O
and	O
as	O
state	O
if	O
we	O
were	O
to	O
drop	O
a	O
point	O
anywhere	O
at	O
random	O
uniformly	O
in	O
the	O
interval	O
the	O
chance	O
that	O
would	O
land	O
in	O
interval	O
is	O
and	O
the	O
chance	O
that	O
it	O
would	O
be	O
in	O
interval	O
is	O
and	O
similarly	O
for	O
interval	O
this	O
therefore	O
defines	O
for	O
us	O
a	O
valid	O
sampling	B
procedure	O
for	O
discrete	B
one-dimensional	O
distributions	O
as	O
described	O
in	O
in	O
our	O
example	O
we	O
have	O
we	O
then	O
draw	O
a	O
sample	O
uniformly	O
from	O
say	O
u	O
then	O
the	O
sampled	O
state	O
would	O
be	O
state	O
since	O
this	O
is	O
in	O
the	O
interval	O
sampling	B
from	O
a	O
discrete	B
univariate	B
distribution	B
is	O
straightforward	O
since	O
computing	O
the	O
cumulant	B
takes	O
only	O
o	O
steps	O
for	O
a	O
k	O
state	O
discrete	B
variable	O
continuous	B
case	O
in	O
the	O
following	O
we	O
assume	O
that	O
a	O
method	O
exists	O
to	O
generate	O
samples	O
from	O
the	O
uniform	B
distribution	B
intuitively	O
the	O
generalisation	B
of	O
the	O
discrete	B
case	O
to	O
the	O
continuous	B
case	O
is	O
clear	O
first	O
u	O
we	O
calculate	O
the	O
cumulant	B
density	B
function	B
cy	O
pxdx	O
y	O
then	O
we	O
sample	O
u	O
uniformly	O
from	O
and	O
obtain	O
the	O
corresponding	O
sample	O
x	O
by	O
solving	B
cx	O
u	O
x	O
c	O
formally	O
therefore	O
sampling	B
of	O
a	O
continuous	B
univariate	B
variable	O
is	O
straightforward	O
provided	O
we	O
can	O
compute	O
the	O
integral	O
of	O
the	O
corresponding	O
probability	O
density	B
function	B
algorithm	B
sampling	B
from	O
a	O
univariate	B
discrete	B
distribution	B
p	O
with	O
k	O
states	O
label	O
the	O
k	O
states	O
as	O
i	O
k	O
with	O
associated	O
probabilities	O
pi	O
calculate	O
the	O
cumulant	B
ci	O
j	O
i	O
pj	O
and	O
set	O
draw	O
a	O
value	B
u	O
uniformly	O
at	O
random	O
from	O
the	O
unit	O
interval	O
find	O
that	O
i	O
for	O
which	O
ci	O
u	O
ci	O
return	O
state	O
i	O
as	O
a	O
sample	O
from	O
p	O
draft	O
march	O
introduction	O
figure	O
histograms	O
of	O
the	O
samples	O
from	O
the	O
three	O
state	O
distribution	B
px	O
samples	O
samples	O
as	O
the	O
number	O
of	O
samples	O
increases	O
the	O
relative	O
frequency	O
of	O
the	O
samples	O
tends	O
to	O
the	O
distribution	B
px	O
for	O
special	O
distributions	O
such	O
as	O
gaussians	O
numerically	O
efficient	O
alternative	O
procedures	O
exist	O
usually	O
based	O
on	O
co-ordinate	O
transformations	O
see	O
multi-variate	B
sampling	B
one	O
way	O
to	O
generalise	O
the	O
one	O
dimensional	O
discrete	B
case	O
to	O
a	O
higher	O
dimensional	O
distribution	B
xn	O
is	O
to	O
translate	O
this	O
into	O
an	O
equivalent	B
one-dimensional	O
distribution	B
this	O
can	O
be	O
achieved	O
by	O
enumerating	O
all	O
the	O
possible	O
joint	B
states	O
xn	O
giving	O
each	O
a	O
unique	O
integer	O
i	O
from	O
to	O
the	O
total	O
number	O
of	O
states	O
and	O
constructing	O
a	O
univariate	B
distribution	B
with	O
probability	O
pi	O
px	O
for	O
i	O
corresponding	O
to	O
the	O
multivariate	B
state	O
x	O
this	O
then	O
transforms	O
the	O
multi-dimensional	O
distribution	B
into	O
an	O
equivalent	B
one-dimensional	O
distribution	B
and	O
sampling	B
can	O
be	O
achieved	O
as	O
before	O
in	O
general	O
of	O
course	O
this	O
procedure	O
is	O
impractical	O
since	O
the	O
number	O
of	O
states	O
will	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
variables	O
xn	O
an	O
alternative	O
exact	O
approach	B
would	O
be	O
to	O
capitalise	O
on	O
the	O
relation	O
we	O
can	O
sample	O
from	O
the	O
joint	B
distribution	B
by	O
first	O
sampling	B
a	O
state	O
for	O
from	O
the	O
one-dimensional	O
and	O
then	O
with	O
clamped	O
to	O
this	O
state	O
sampling	B
a	O
state	O
for	O
from	O
the	O
one-dimensional	O
it	O
is	O
clear	O
how	O
to	O
generalise	O
this	O
to	O
more	O
variables	O
by	O
using	O
a	O
cascade	B
decomposition	B
xn	O
pxnxn	O
however	O
in	O
order	O
to	O
apply	O
this	O
technique	O
we	O
need	O
to	O
know	O
the	O
conditionals	O
pxixi	O
unless	O
these	O
are	O
explicitly	O
given	O
we	O
need	O
to	O
compute	O
these	O
from	O
the	O
joint	B
distribution	B
xn	O
such	O
conditionals	O
will	O
in	O
general	O
require	O
the	O
summation	O
over	O
an	O
exponential	B
number	O
of	O
states	O
and	O
except	O
for	O
small	O
n	O
generally	O
also	O
be	O
impractical	O
for	O
belief	B
networks	I
however	O
by	O
construction	B
the	O
conditionals	O
are	O
specified	O
so	O
that	O
this	O
technique	O
becomes	O
practical	O
as	O
we	O
discuss	O
in	O
drawing	O
samples	O
from	O
a	O
multi-variate	B
distribution	B
is	O
in	O
general	O
therefore	O
a	O
complex	O
task	O
and	O
one	O
seeks	O
to	O
exploit	O
any	O
structural	O
properties	B
of	O
the	O
distribution	B
to	O
make	O
this	O
computationally	O
more	O
feasible	O
a	O
common	O
approach	B
is	O
to	O
seek	O
to	O
transform	O
the	O
distribution	B
into	O
a	O
product	O
of	O
lower	O
dimensional	O
distributions	O
a	O
classic	O
example	O
of	O
this	O
is	O
sampling	B
from	O
a	O
multi-variate	B
gaussian	B
which	O
can	O
be	O
reduced	O
to	O
sampling	B
from	O
a	O
set	O
of	O
univariate	B
gaussians	O
by	O
a	O
suitable	O
coordinate	O
transformation	O
as	O
discussed	O
in	O
example	O
from	O
a	O
multi-variate	B
gaussian	B
our	O
interest	O
is	O
to	O
draw	O
a	O
sample	O
from	O
the	O
multi-variate	B
gaussian	B
px	O
n	O
m	O
s	O
for	O
a	O
general	O
covariance	B
matrix	B
s	O
px	O
does	O
not	O
factorise	O
into	O
a	O
product	O
of	O
univariate	B
distributions	O
however	O
consider	O
the	O
transformation	O
where	O
c	O
is	O
chosen	O
so	O
that	O
cct	O
s	O
since	O
this	O
is	O
a	O
linear	B
transformation	I
y	O
is	O
also	O
gaussian	B
distributed	O
with	O
mean	B
y	O
c	O
m	O
c	O
px	O
m	O
px	O
c	O
m	O
since	O
the	O
mean	B
of	O
y	O
is	O
zero	O
the	O
covariance	B
is	O
given	O
by	O
c	O
m	O
c	O
t	O
c	O
t	O
c	O
t	O
i	O
px	O
draft	O
march	O
ancestral	B
sampling	B
figure	O
an	O
ancestral	B
belief	B
network	I
without	O
any	O
evidential	O
variables	O
to	O
sample	O
from	O
this	O
distribution	B
we	O
draw	O
a	O
sample	O
from	O
variable	O
and	O
then	O
variables	O
in	O
order	O
hence	O
py	O
n	O
i	O
i	O
n	O
hence	O
a	O
sample	O
from	O
y	O
can	O
be	O
obtained	O
by	O
independently	O
drawing	O
a	O
sample	O
from	O
each	O
of	O
the	O
univariate	B
zero	O
mean	B
unit	O
variance	B
gaussians	O
given	O
a	O
sample	O
for	O
y	O
a	O
sample	O
for	O
x	O
is	O
obtained	O
using	O
x	O
cy	O
m	O
drawing	O
samples	O
from	O
a	O
univariate	B
gaussian	B
is	O
a	O
well-studied	O
topic	O
with	O
a	O
popular	O
method	O
being	O
the	O
box-muller	O
technique	O
ancestral	B
sampling	B
belief	B
networks	I
take	O
the	O
general	O
form	O
px	O
i	O
pxipa	O
where	O
each	O
of	O
the	O
conditional	B
distributions	O
pxipa	O
is	O
known	O
provided	O
that	O
no	O
variables	O
are	O
evidential	O
we	O
can	O
sample	O
from	O
this	O
distribution	B
in	O
a	O
straightforward	O
manner	O
for	O
convenience	O
we	O
first	O
rename	O
the	O
variable	O
indices	O
so	O
that	O
parent	O
variables	O
always	O
come	O
before	O
their	O
children	B
ordering	O
for	O
example	O
one	O
can	O
sample	O
first	O
from	O
those	O
nodes	O
that	O
do	O
not	O
have	O
any	O
parents	B
and	O
given	O
these	O
values	O
one	O
can	O
then	O
sample	O
and	O
then	O
and	O
and	O
finally	O
despite	O
the	O
presence	O
of	O
loops	O
in	O
the	O
graph	B
such	O
a	O
forward	B
sampling	B
procedure	O
is	O
straightforward	O
this	O
procedure	O
holds	O
for	O
both	O
discrete	B
and	O
continuous	B
variables	O
if	O
one	O
attempted	O
to	O
carry	O
out	O
an	O
exact	O
inference	B
scheme	O
using	O
moralisation	B
and	O
triangulation	B
in	O
more	O
complex	O
multiply	O
connected	B
graphs	O
cliques	O
can	O
become	O
very	O
large	O
however	O
regardless	O
of	O
the	O
loop	O
structure	B
ancestral	B
sampling	B
is	O
straightforward	O
ancestral	B
or	O
forward	B
sampling	B
is	O
a	O
case	O
of	O
perfect	B
sampling	B
termed	O
exact	B
sampling	B
since	O
each	O
sample	O
is	O
indeed	O
drawn	O
from	O
the	O
required	O
distribution	B
this	O
is	O
in	O
contrast	O
to	O
markov	B
chain	B
monte	I
carlo	I
methods	O
for	O
which	O
the	O
samples	O
are	O
from	O
px	O
only	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
iterations	O
dealing	O
with	O
evidence	O
how	O
can	O
we	O
sample	O
from	O
a	O
distribution	B
in	O
which	O
certain	O
variables	O
xe	O
are	O
clamped	O
to	O
evidential	O
states	O
formally	O
we	O
need	O
to	O
sample	O
from	O
pxexe	O
pxe	O
xe	O
pxe	O
draft	O
march	O
gibbs	B
sampling	B
figure	O
the	O
markov	B
blanket	I
of	O
to	O
draw	O
a	O
sample	O
from	O
we	O
clamp	O
into	O
their	O
evidential	O
states	O
and	O
draw	O
a	O
sample	O
from	O
where	O
z	O
is	O
a	O
normalisation	B
constant	I
if	O
an	O
evidential	O
variable	O
xi	O
has	O
no	O
parents	B
then	O
one	O
can	O
simply	O
set	O
the	O
variable	O
into	O
this	O
state	O
and	O
continue	O
forward	B
sampling	B
as	O
before	O
for	O
example	O
to	O
compute	O
a	O
sample	O
from	O
defined	O
in	O
equation	B
one	O
simply	O
clamps	O
the	O
into	O
its	O
evidential	O
state	O
and	O
continues	O
forward	B
sampling	B
the	O
reason	O
this	O
is	O
straightforward	O
is	O
that	O
conditioning	B
on	O
merely	O
defines	O
a	O
new	O
distribution	B
on	O
a	O
subset	O
of	O
the	O
variables	O
for	O
which	O
the	O
distribution	B
is	O
immediately	O
known	O
on	O
the	O
other	O
hand	O
consider	O
sampling	B
from	O
using	O
bayes	O
rule	O
we	O
have	O
the	O
conditioning	B
on	O
means	O
that	O
the	O
structure	B
of	O
the	O
distribution	B
on	O
the	O
non-evidential	O
variables	O
changes	O
for	O
example	O
and	O
become	O
coupled	B
one	O
could	O
attempt	O
to	O
work	O
out	O
an	O
equivalent	B
new	O
forward	B
sampling	B
structure	B
although	O
generally	O
this	O
will	O
be	O
as	O
complex	O
as	O
running	O
an	O
exact	O
inference	B
approach	B
probability	O
that	O
a	O
sample	O
from	O
px	O
will	O
be	O
consistent	B
with	O
the	O
evidence	O
is	O
roughly	O
o	O
an	O
alternative	O
is	O
to	O
proceed	O
with	O
forward	B
sampling	B
from	O
the	O
non-evidential	O
distribution	B
and	O
then	O
discard	O
any	O
samples	O
which	O
do	O
not	O
match	O
the	O
evidential	O
states	O
this	O
is	O
generally	O
not	O
recommended	O
since	O
the	O
i	O
where	O
i	O
is	O
the	O
number	O
of	O
states	O
of	O
evidential	O
variable	O
i	O
in	O
principle	O
one	O
can	O
ease	O
this	O
effect	O
by	O
discarding	O
dim	O
xe	O
the	O
sample	O
as	O
soon	O
as	O
any	O
variable	O
state	O
is	O
inconsistent	O
with	O
the	O
evidence	O
nevertheless	O
the	O
number	O
of	O
re-starts	O
required	O
to	O
obtain	O
a	O
valid	O
sample	O
would	O
on	O
average	B
be	O
very	O
large	O
i	O
dim	O
xe	O
perfect	B
sampling	B
for	O
a	O
markov	B
network	I
for	O
a	O
markov	B
network	I
we	O
can	O
draw	O
exact	O
samples	O
by	O
forming	O
an	O
equivalent	B
directed	B
representation	O
of	O
the	O
graph	B
see	O
and	O
subsequently	O
using	O
ancestral	B
sampling	B
on	O
this	O
directed	B
graph	B
this	O
is	O
achieved	O
by	O
first	O
choosing	O
a	O
root	O
clique	B
and	O
then	O
consistently	O
orienting	O
edges	O
away	O
from	O
this	O
clique	B
an	O
exact	O
sample	O
can	O
then	O
be	O
drawn	O
from	O
the	O
markov	B
network	I
by	O
first	O
sampling	B
from	O
the	O
root	O
clique	B
and	O
then	O
recursively	O
from	O
the	O
children	B
of	O
this	O
clique	B
see	O
potsample	O
m	O
jtsample	O
m	O
and	O
demojtreesample	O
m	O
gibbs	B
sampling	B
the	O
inefficiency	O
of	O
methods	O
such	O
as	O
ancestral	B
sampling	B
under	O
evidence	O
motivates	O
alternative	O
techniques	O
an	O
important	O
and	O
widespread	O
technique	O
is	O
gibbs	B
sampling	B
which	O
is	O
generally	O
straightforward	O
to	O
implement	O
no	O
evidence	O
assume	O
we	O
have	O
a	O
joint	B
sample	O
state	O
from	O
the	O
multivariate	B
distribution	B
px	O
we	O
then	O
consider	O
a	O
particular	O
variable	O
xi	O
using	O
bayes	O
rule	O
we	O
may	O
write	O
px	O
xi	O
xi	O
xn	O
given	O
a	O
joint	B
initial	O
state	O
from	O
which	O
we	O
can	O
read	O
off	O
the	O
parental	O
state	O
can	O
then	O
draw	O
a	O
sample	O
i	O
from	O
i	O
n	O
we	O
i	O
n	O
pxixi	O
draft	O
march	O
gibbs	B
sampling	B
figure	O
a	O
toy	O
intractable	O
distribution	B
gibbs	B
sampling	B
by	O
conditioning	B
on	O
all	O
variables	O
except	O
one	O
leads	O
to	O
a	O
simple	O
univariate	B
conditional	B
disb	O
conditioning	B
on	O
yields	O
a	O
new	O
tribution	O
distribution	B
that	O
is	O
singly-connected	B
for	O
which	O
exact	B
sampling	B
is	O
straightforward	O
one	O
then	O
selects	O
another	O
variable	O
xj	O
which	O
only	O
xi	O
has	O
been	O
updated	O
i	O
i	O
n	O
we	O
assume	O
this	O
distribution	B
is	O
easy	O
to	O
sample	O
from	O
since	O
it	O
is	O
univariate	B
we	O
call	O
this	O
new	O
joint	B
sample	O
to	O
sample	O
and	O
by	O
continuing	O
this	O
procedure	O
generates	O
a	O
set	O
xl	O
of	O
samples	O
in	O
which	O
each	O
differs	O
from	O
xl	O
in	O
only	O
a	O
single	O
component	O
the	O
reason	O
this	O
is	O
valid	O
sampling	B
scheme	O
is	O
outlined	O
in	O
for	O
a	O
belief	B
network	I
the	O
conditional	B
pxixi	O
is	O
defined	O
by	O
the	O
markov	B
blanket	I
of	O
xi	O
pxjpa	O
pxixi	O
z	O
pxipa	O
j	O
chi	O
z	O
pxipa	O
xi	O
j	O
chi	O
see	O
for	O
example	O
this	O
means	O
that	O
only	O
the	O
parent	O
and	O
parents	B
of	O
children	B
states	O
are	O
required	O
in	O
forming	O
the	O
sample	O
update	O
the	O
normalisation	B
constant	I
for	O
this	O
univariate	B
distribution	B
is	O
straightforward	O
to	O
work	O
out	O
from	O
the	O
requirement	O
pxjpa	O
in	O
the	O
case	O
of	O
a	O
continuous	B
variable	O
xi	O
the	O
summation	O
above	O
is	O
replaced	O
with	O
integration	O
evidence	O
evidence	O
is	O
readily	O
dealt	O
with	O
by	O
clamping	O
for	O
all	O
samples	O
the	O
evidential	O
variables	O
into	O
their	O
evidential	O
states	O
there	O
is	O
also	O
no	O
need	O
to	O
sample	O
for	O
these	O
variables	O
since	O
their	O
states	O
are	O
known	O
gibbs	B
sampling	B
as	O
a	O
markov	B
chain	B
in	O
gibbs	B
sampling	B
we	O
have	O
a	O
sample	O
of	O
the	O
joint	B
variables	O
xl	O
at	O
stage	O
l	O
based	O
on	O
this	O
we	O
produce	O
a	O
new	O
joint	B
sample	O
this	O
means	O
that	O
we	O
can	O
write	O
gibbs	B
sampling	B
as	O
a	O
procedure	O
that	O
draws	O
from	O
for	O
some	O
distribution	B
if	O
we	O
choose	O
the	O
variable	O
to	O
update	O
xi	O
at	O
random	O
from	O
a	O
distribution	B
qi	O
then	O
gibbs	B
sampling	B
corresponds	O
to	O
drawing	O
samples	O
using	O
the	O
markov	O
transition	O
iqi	O
i	O
i	O
j	O
xl	O
j	O
with	O
qi	O
our	O
interest	O
is	O
to	O
show	O
that	O
the	O
stationary	B
distribution	B
of	O
out	O
assuming	O
x	O
is	O
continuous	B
the	O
discrete	B
case	O
is	O
analogous	O
qi	O
qi	O
qi	O
xi	O
qipx	O
qx	O
x	O
x	O
j	O
xj	O
px	O
x	O
px	O
ix	O
ix	O
ixipxi	O
xi	O
qipx	O
px	O
is	O
px	O
we	O
carry	O
this	O
i	O
qx	O
x	O
i	O
i	O
i	O
i	O
i	O
draft	O
march	O
gibbs	B
sampling	B
figure	O
a	O
two	O
dimensional	O
distribution	B
for	O
which	O
gibbs	B
sampling	B
fails	O
the	O
distribution	B
has	O
mass	O
only	O
in	O
the	O
shaded	O
quadrants	O
gibbs	B
sampling	B
proceeds	O
from	O
the	O
lth	O
sample	O
state	O
and	O
then	O
sampling	B
from	O
which	O
we	O
write	O
one	O
then	O
continues	O
with	O
a	O
etc	O
if	O
we	O
start	O
in	O
the	O
lower	O
left	O
quadrant	O
sample	O
from	O
and	O
proceed	O
this	O
way	O
the	O
upper	O
right	O
region	O
is	O
never	O
explored	O
xl	O
where	O
xl	O
hence	O
as	O
long	O
as	O
we	O
continue	O
to	O
draw	O
samples	O
according	O
to	O
the	O
distribution	B
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
samples	O
we	O
will	O
ultimately	O
tend	O
to	O
draw	O
samples	O
from	O
px	O
any	O
distribution	B
qi	O
suffices	O
so	O
visiting	O
all	O
variables	O
equally	O
often	O
is	O
also	O
a	O
valid	O
choice	O
technically	O
we	O
also	O
require	O
that	O
has	O
px	O
as	O
its	O
equilibrium	O
distribution	B
so	O
that	O
no	O
matter	O
in	O
which	O
state	O
we	O
start	O
we	O
always	O
converge	O
to	O
px	O
see	O
for	O
a	O
discussion	O
of	O
this	O
issue	O
structured	B
gibbs	B
sampling	B
one	O
can	O
extend	O
gibbs	B
sampling	B
by	O
using	O
conditioning	B
to	O
reveal	O
a	O
tractable	O
distribution	B
on	O
the	O
remaining	O
variables	O
for	O
example	O
consider	O
the	O
simple	O
distribution	B
in	O
single-site	O
gibbs	B
sampling	B
we	O
would	O
condition	O
on	O
three	O
of	O
the	O
four	O
variables	O
and	O
sample	O
from	O
the	O
remaining	O
variable	O
for	O
example	O
however	O
we	O
may	O
use	O
more	O
limited	O
conditioning	B
as	O
long	O
as	O
the	O
conditioned	O
distribution	B
is	O
easy	O
to	O
sample	O
from	O
in	O
the	O
case	O
of	O
equation	B
we	O
can	O
condition	O
on	O
alone	O
to	O
give	O
this	O
can	O
be	O
written	O
as	O
a	O
modified	O
distribution	B
as	O
a	O
distribution	B
on	O
this	O
is	O
a	O
singly-connected	B
linear	B
chain	B
from	O
which	O
samples	O
can	O
be	O
drawn	O
exactly	O
a	O
simple	O
approach	B
is	O
compute	O
the	O
normalisation	B
constant	I
by	O
any	O
of	O
the	O
standard	O
techniques	O
for	O
example	O
using	O
the	O
factor	B
graph	B
method	O
one	O
may	O
then	O
convert	O
this	O
undirected	B
linear	B
chain	B
to	O
a	O
directed	B
graph	B
and	O
use	O
ancestral	B
sampling	B
these	O
operations	O
are	O
linear	B
in	O
the	O
number	O
of	O
variables	O
in	O
the	O
conditioned	O
distribution	B
alternatively	O
one	O
may	O
form	O
a	O
junction	B
tree	B
from	O
a	O
set	O
of	O
potentials	O
choose	O
a	O
root	O
and	O
then	O
form	O
a	O
set	B
chain	B
by	O
reabsorption	B
on	O
the	O
junction	B
tree	B
ancestral	B
sampling	B
can	O
then	O
be	O
performed	O
on	O
the	O
resulting	O
oriented	O
clique	B
tree	B
this	O
is	O
the	O
approach	B
taken	O
in	O
gibbssample	O
m	O
in	O
the	O
above	O
example	O
one	O
can	O
also	O
reveal	O
a	O
tractable	O
distribution	B
by	O
conditioning	B
on	O
and	O
then	O
draw	O
a	O
sample	O
of	O
from	O
this	O
distribution	B
a	O
valid	O
sampling	B
procedure	O
is	O
then	O
to	O
draw	O
a	O
sample	O
from	O
equation	B
and	O
then	O
a	O
sample	O
from	O
equation	B
these	O
two	O
steps	O
are	O
then	O
iterated	O
note	O
that	O
and	O
are	O
not	O
constrained	O
to	O
be	O
equal	O
to	O
their	O
values	O
in	O
the	O
previous	O
sample	O
this	O
procedure	O
is	O
generally	O
to	O
be	O
preferred	O
to	O
the	O
single-site	O
gibbs	B
updating	O
since	O
the	O
samples	O
are	O
less	O
correlated	O
from	O
one	O
sample	O
to	O
the	O
next	O
see	O
demogibbssample	O
m	O
for	O
a	O
comparison	O
of	O
unstructured	O
and	O
structured	B
sampling	B
from	O
a	O
set	O
of	O
potentials	O
draft	O
march	O
gibbs	B
sampling	B
figure	O
two	O
hundred	O
gibbs	B
samples	O
for	O
a	O
two	O
dimensional	O
gaussian	B
at	O
each	O
stage	O
only	O
a	O
single	O
component	O
is	O
updated	O
for	O
a	O
gaussian	B
with	O
low	O
correlation	O
gibbs	B
sampling	B
can	O
move	O
through	O
the	O
for	O
a	O
strongly	O
correlated	O
gaussian	B
gibbs	B
sampling	B
is	O
less	O
effective	O
and	O
likely	O
regions	O
effectively	O
does	O
not	O
rapidly	O
explore	O
the	O
likely	O
regions	O
see	O
demogibbsgauss	O
m	O
remarks	O
if	O
the	O
initial	O
sample	O
is	O
in	O
a	O
part	O
of	O
the	O
state	O
space	O
that	O
is	O
very	O
unlikely	O
then	O
it	O
may	O
take	O
some	O
time	O
for	O
the	O
samples	O
to	O
become	O
representative	O
as	O
only	O
a	O
single	O
component	O
of	O
x	O
is	O
updated	O
at	O
each	O
iteration	B
this	O
motivates	O
a	O
so-called	O
burn	B
in	I
stage	O
in	O
which	O
the	O
initial	O
samples	O
are	O
discarded	O
in	O
single	O
site	O
gibbs	B
sampling	B
there	O
will	O
be	O
a	O
high	O
degree	B
of	O
correlation	O
in	O
any	O
two	O
successive	O
samples	O
since	O
only	O
one	O
variable	O
the	O
single-site	O
updating	O
version	O
is	O
updated	O
at	O
each	O
stage	O
an	O
ideal	O
perfect	B
sampling	B
scheme	O
would	O
draw	O
each	O
x	O
at	O
random	O
from	O
px	O
clearly	O
in	O
general	O
two	O
such	O
perfect	O
samples	O
will	O
not	O
possess	O
the	O
same	O
degree	B
of	O
correlation	O
as	O
those	O
from	O
gibbs	B
sampling	B
this	O
motivates	O
subsampling	B
in	O
which	O
say	O
every	O
sample	O
xk	O
is	O
taken	O
and	O
the	O
rest	O
discarded	O
due	O
to	O
its	O
simplicity	O
gibbs	B
sampling	B
is	O
one	O
of	O
the	O
most	O
popular	O
sampling	B
methods	O
and	O
is	O
particularly	O
convenient	O
when	O
applied	O
to	O
belief	B
networks	I
due	O
to	O
the	O
markov	B
blanket	I
gibbs	B
sampling	B
is	O
a	O
special	O
case	O
of	O
the	O
mcmc	O
framework	O
and	O
as	O
with	O
all	O
mcmc	O
methods	O
one	O
should	O
bear	O
in	O
mind	O
that	O
convergence	O
can	O
be	O
a	O
major	O
issue	O
that	O
is	O
answering	O
questions	O
such	O
as	O
how	O
many	O
samples	O
are	O
needed	O
to	O
be	O
reasonably	O
sure	O
that	O
my	O
sample	O
estimate	O
is	O
accurate	O
is	O
difficult	O
despite	O
mathematical	O
results	O
for	O
special	O
cases	O
general	O
rules	O
of	O
thumb	O
and	O
awareness	O
on	O
behalf	O
of	O
the	O
user	O
are	O
required	O
to	O
monitor	O
the	O
efficiency	O
of	O
the	O
sampling	B
gibbs	B
sampling	B
assumes	O
that	O
we	O
can	O
move	O
throughout	O
the	O
space	O
effectively	O
by	O
only	O
single	O
co-ordinate	O
updates	O
we	O
also	O
require	O
that	O
every	O
state	O
can	O
be	O
visited	O
infinitely	O
often	O
in	O
we	O
show	O
a	O
case	O
in	O
which	O
the	O
two	O
dimensional	O
continuous	B
distribution	B
has	O
mass	O
only	O
in	O
the	O
lower	O
left	O
and	O
upper	O
right	O
regions	O
in	O
that	O
case	O
if	O
we	O
start	O
in	O
the	O
lower	O
left	O
region	O
we	O
will	O
always	O
remain	O
there	O
and	O
never	O
explore	O
the	O
upper	O
right	O
region	O
this	O
problem	B
occurs	O
when	O
two	O
regions	O
which	O
are	O
not	O
connected	B
by	O
a	O
probable	O
gibbs	B
path	B
gibbs	B
sampling	B
becomes	O
a	O
perfect	O
sampler	O
when	O
the	O
distribution	B
is	O
factorised	B
that	O
is	O
the	O
variables	O
are	O
independent	O
this	O
suggests	O
that	O
in	O
general	O
gibbs	B
sampling	B
will	O
be	O
less	O
effective	O
when	O
variables	O
are	O
strongly	O
correlated	O
for	O
example	O
if	O
we	O
consider	O
gibbs	B
sampling	B
from	O
a	O
strongly	O
correlated	O
two	O
variable	O
gaussian	B
distribution	B
then	O
updates	O
will	O
move	O
very	O
slowly	O
in	O
space	O
bugs	O
package	O
www	O
mrc-bsu	O
cam	O
ac	O
ukbugs	O
is	O
general	O
purpose	O
software	O
for	O
sampling	B
from	O
belief	B
networks	I
draft	O
march	O
markov	B
chain	B
monte	I
carlo	I
markov	B
chain	B
monte	I
carlo	I
we	O
assume	O
we	O
have	O
a	O
multivariate	B
distribution	B
in	O
the	O
form	O
px	O
p	O
z	O
assume	O
we	O
are	O
able	O
to	O
evaluate	O
p	O
x	O
for	O
any	O
state	O
x	O
but	O
not	O
z	O
since	O
z	O
where	O
z	O
is	O
the	O
normalisation	B
constant	I
of	O
the	O
distribution	B
and	O
p	O
is	O
the	O
unnormalised	O
distribution	B
we	O
x	O
p	O
is	O
an	O
intractable	O
high	O
dimensional	O
summationintegration	O
the	O
idea	O
in	O
mcmc	O
sampling	B
is	O
to	O
sample	O
not	O
directly	O
from	O
px	O
but	O
from	O
a	O
different	O
distribution	B
such	O
that	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
samples	O
effectively	O
the	O
samples	O
will	O
be	O
from	O
px	O
to	O
achieve	O
this	O
we	O
forward	O
sample	O
from	O
a	O
markov	O
transition	O
whose	O
stationary	B
distribution	B
is	O
equal	O
to	O
px	O
markov	O
chains	O
consider	O
the	O
conditional	B
distribution	B
if	O
we	O
are	O
given	O
an	O
initial	O
sample	O
then	O
we	O
can	O
recursively	O
generate	O
samples	O
xl	O
after	O
a	O
long	O
time	O
l	O
provided	O
the	O
markov	B
chain	B
is	O
irreducible	O
meaning	O
that	O
we	O
can	O
eventually	O
get	O
from	O
any	O
state	O
to	O
any	O
other	O
state	O
the	O
samples	O
are	O
from	O
the	O
stationary	B
distribution	B
q	O
which	O
is	O
defined	O
as	O
a	O
continuous	B
variable	O
x	O
q	O
qx	O
the	O
condition	O
for	O
a	O
discrete	B
variable	O
is	O
analogous	O
on	O
replacing	O
integration	O
with	O
summation	O
the	O
idea	O
in	O
mcmc	O
is	O
for	O
a	O
given	O
distribution	B
px	O
to	O
find	O
a	O
transition	O
which	O
has	O
px	O
as	O
its	O
stationary	B
distribution	B
if	O
we	O
can	O
do	O
so	O
then	O
we	O
can	O
draw	O
samples	O
from	O
the	O
markov	B
chain	B
by	O
forward	B
sampling	B
and	O
take	O
these	O
as	O
samples	O
from	O
px	O
note	O
that	O
for	O
every	O
distribution	B
px	O
there	O
will	O
be	O
more	O
than	O
one	O
transition	O
with	O
px	O
as	O
its	O
stationary	B
distribution	B
this	O
is	O
why	O
there	O
are	O
very	O
many	O
different	O
mcmc	O
sampling	B
methods	O
each	O
with	O
different	O
characteristics	O
and	O
varying	O
suitability	O
for	O
the	O
particular	O
distribution	B
at	O
hand	O
detailed	B
balance	I
how	O
do	O
we	O
construct	O
a	O
transition	O
with	O
given	O
px	O
as	O
its	O
stationary	B
distribution	B
this	O
problem	B
can	O
be	O
simplified	O
if	O
we	O
consider	O
special	O
transitions	O
that	O
satisfy	O
the	O
detailed	B
balance	I
condition	O
if	O
we	O
are	O
given	O
the	O
marginal	B
distribution	B
px	O
the	O
detailed	B
balance	I
condition	O
for	O
a	O
transition	O
q	O
is	O
under	O
this	O
we	O
see	O
qx	O
x	O
px	O
x	O
x	O
px	O
qxx	O
x	O
so	O
that	O
px	O
is	O
the	O
stationary	B
distribution	B
of	O
the	O
detailed	B
balance	I
requirement	O
can	O
make	O
the	O
process	O
of	O
constructing	O
a	O
suitable	O
transition	O
easier	O
since	O
only	O
the	O
relative	O
value	B
of	O
to	O
px	O
is	O
required	O
in	O
equation	B
and	O
not	O
the	O
absolute	O
value	B
of	O
px	O
or	O
metropolis-hastings	B
sampling	B
consider	O
the	O
following	O
transition	O
qx	O
qx	O
x	O
x	O
qx	O
x	O
draft	O
march	O
algorithm	B
metropolis-hastings	B
mcmc	O
sampling	B
markov	B
chain	B
monte	I
carlo	I
choose	O
a	O
starting	O
point	O
for	O
i	O
to	O
l	O
do	O
draw	O
a	O
candidate	O
sample	O
xcand	O
from	O
the	O
proposal	O
let	O
a	O
qxl	O
qxcandxl	O
if	O
a	O
then	O
xl	O
xcand	O
else	O
draw	O
a	O
random	O
value	B
u	O
uniformly	O
from	O
the	O
unit	O
interval	O
if	O
u	O
a	O
then	O
xl	O
xcand	O
else	O
end	O
for	O
xl	O
xl	O
end	O
if	O
end	O
if	O
accept	O
the	O
candidate	O
accept	O
the	O
candidate	O
reject	O
the	O
candidate	O
where	O
valid	O
distribution	B
is	O
a	O
so-called	O
proposal	B
distribution	B
and	O
x	O
a	O
positive	O
function	B
this	O
defines	O
a	O
since	O
it	O
is	O
non-negative	O
and	O
qx	O
qx	O
x	O
x	O
our	O
interest	O
is	O
to	O
set	O
fx	O
such	O
that	O
the	O
stationary	B
distribution	B
of	O
proposal	O
is	O
equal	O
to	O
px	O
for	O
any	O
qx	O
px	O
that	O
is	O
qx	O
x	O
qx	O
x	O
qx	O
x	O
xpx	O
px	O
in	O
order	O
that	O
this	O
holds	O
we	O
require	O
the	O
integral	O
variable	O
from	O
to	O
x	O
now	O
consider	O
the	O
metropolis-hastings	B
acceptance	B
function	B
x	O
qx	O
x	O
fx	O
x	O
min	O
fx	O
x	O
qx	O
xpx	O
qxx	O
qx	O
qxx	O
x	O
min	O
qxx	O
qx	O
fx	O
x	O
which	O
is	O
defined	O
for	O
all	O
x	O
and	O
has	O
the	O
detailed	B
balance	I
property	O
qxx	O
hence	O
the	O
function	B
x	O
as	O
defined	O
above	O
ensures	O
equation	B
holds	O
and	O
that	O
its	O
stationary	B
distribution	B
how	O
do	O
we	O
sample	O
from	O
can	O
be	O
interpreted	O
as	O
a	O
mixture	B
of	O
two	O
distributions	O
x	O
one	O
proportional	O
to	O
to	O
draw	O
a	O
sample	O
from	O
this	O
we	O
draw	O
a	O
sample	O
from	O
and	O
accept	O
this	O
with	O
probability	O
x	O
since	O
drawing	O
from	O
and	O
accepting	O
are	O
performed	O
independently	O
the	O
probability	O
of	O
accepting	O
the	O
drawn	O
candidate	O
is	O
the	O
product	O
of	O
these	O
probabilities	O
namely	O
x	O
otherwise	O
the	O
candidate	O
is	O
rejected	O
and	O
we	O
take	O
the	O
sample	O
x	O
using	O
the	O
properties	B
of	O
the	O
acceptance	B
function	B
equation	B
the	O
following	O
is	O
equivalent	B
to	O
deciding	O
on	O
acceptingrejecting	O
the	O
candidate	O
if	O
x	O
and	O
the	O
other	O
x	O
with	O
mixture	B
coefficient	O
has	O
px	O
as	O
qxx	O
qx	O
draft	O
march	O
auxiliary	B
variable	I
methods	O
figure	O
metropolis-hastings	B
samples	O
from	O
a	O
bivariate	O
distribution	B
using	O
a	O
proposal	O
n	O
x	O
i	O
we	O
also	O
plot	O
the	O
iso-probability	O
contours	O
of	O
p	O
although	O
px	O
is	O
multimodal	O
the	O
dimensionality	O
is	O
low	O
enough	O
and	O
the	O
modes	O
sufficiently	O
close	O
such	O
that	O
a	O
simple	O
gaussian	B
proposal	B
distribution	B
is	O
able	O
to	O
bridge	O
the	O
two	O
modes	O
in	O
higher	O
dimensions	O
such	O
multi-modality	O
is	O
more	O
problematic	O
see	O
demometropolis	O
m	O
if	O
we	O
reject	O
the	O
candidate	O
we	O
take	O
x	O
otherwise	O
we	O
accept	O
the	O
sample	O
from	O
we	O
accept	O
the	O
sample	O
from	O
note	O
that	O
if	O
the	O
candidate	O
is	O
rejected	O
we	O
take	O
the	O
original	O
x	O
as	O
the	O
new	O
sample	O
hence	O
at	O
each	O
iteration	B
of	O
the	O
algorithm	B
produces	O
a	O
sample	O
either	O
a	O
copy	O
of	O
the	O
current	O
sample	O
or	O
the	O
candidate	O
sample	O
a	O
rough	O
rule	O
of	O
thumb	O
is	O
to	O
choose	O
a	O
proposal	B
distribution	B
for	O
which	O
the	O
acceptance	O
rate	O
is	O
between	O
and	O
with	O
probability	O
gaussian	B
proposal	B
distribution	B
a	O
common	O
proposal	B
distribution	B
for	O
multivariate	B
x	O
explicitly	O
as	O
a	O
vector	O
is	O
n	O
for	O
which	O
x	O
and	O
the	O
acceptance	O
criterion	O
becomes	O
e	O
x	O
min	O
p	O
p	O
if	O
the	O
unnormalised	O
probability	O
of	O
the	O
candidate	O
state	O
is	O
higher	O
than	O
the	O
current	O
state	O
we	O
therefore	O
accept	O
the	O
candidate	O
otherwise	O
if	O
the	O
unnormalised	O
probability	O
of	O
the	O
candidate	O
state	O
is	O
lower	O
than	O
the	O
current	O
state	O
we	O
accept	O
the	O
candidate	O
only	O
with	O
probability	O
p	O
if	O
the	O
candidate	O
is	O
rejected	O
the	O
new	O
sample	O
is	O
taken	O
to	O
be	O
a	O
copy	O
of	O
the	O
previous	O
sample	O
x	O
see	O
for	O
a	O
demonstration	O
in	O
high	O
dimensions	O
it	O
is	O
unlikely	O
that	O
a	O
random	O
candidate	O
sampled	O
from	O
a	O
gaussian	B
will	O
result	O
in	O
a	O
candidate	O
probability	O
higher	O
than	O
the	O
current	O
value	B
because	O
of	O
this	O
only	O
very	O
small	O
jumps	O
small	O
are	O
likely	O
to	O
be	O
accepted	O
this	O
limits	O
the	O
speed	O
at	O
which	O
we	O
explore	O
the	O
space	O
x	O
this	O
acceptance	B
function	B
above	O
highlights	O
that	O
sampling	B
is	O
different	O
from	O
finding	O
the	O
optimum	O
provided	O
has	O
a	O
higher	O
probability	O
than	O
x	O
we	O
accept	O
however	O
we	O
also	O
accept	O
a	O
specified	O
acceptance	O
probability	O
candidates	O
that	O
have	O
also	O
a	O
lower	O
probability	O
than	O
the	O
current	O
sample	O
auxiliary	B
variable	I
methods	O
a	O
practical	O
concern	O
in	O
mcmc	O
methods	O
is	O
ensuring	O
that	O
one	O
moves	O
effectively	O
through	O
the	O
significant	O
probability	O
regions	O
of	O
the	O
distribution	B
for	O
methods	O
such	O
as	O
metropolis-hastings	B
with	O
local	B
proposal	O
distributions	O
in	O
the	O
sense	O
they	O
are	O
unlikely	O
to	O
propose	O
a	O
candidate	O
far	O
from	O
the	O
current	O
sample	O
if	O
the	O
target	O
distribution	B
has	O
isolated	O
islands	O
of	O
high	O
density	B
then	O
the	O
likelihood	B
that	O
we	O
would	O
be	O
able	O
to	O
move	O
from	O
one	O
island	O
to	O
the	O
other	O
is	O
very	O
small	O
if	O
we	O
attempt	O
to	O
make	O
the	O
proposal	O
less	O
local	B
by	O
using	O
one	O
with	O
a	O
high	O
variance	B
the	O
chance	O
then	O
of	O
landing	O
at	O
random	O
on	O
a	O
high	O
density	B
island	O
is	O
remote	O
auxiliary	B
variable	I
methods	O
use	O
additional	O
dimensions	O
to	O
exploration	O
and	O
in	O
certain	O
cases	O
to	O
provide	O
a	O
bridge	O
between	O
draft	O
march	O
auxiliary	B
variable	I
methods	O
figure	O
hybrid	B
monte	I
carlo	I
multi-modal	O
distribution	B
px	O
for	O
which	O
we	O
desire	O
samples	O
hmc	O
forms	O
the	O
joint	B
distibution	O
pxpy	O
where	O
py	O
is	O
gaussian	B
starting	O
from	O
the	O
point	O
x	O
we	O
first	O
draw	O
a	O
y	O
from	O
the	O
gaussian	B
py	O
giving	O
a	O
point	O
y	O
green	O
line	O
then	O
we	O
use	O
hamiltonian	B
dynamics	I
line	O
to	O
traverse	O
the	O
distribution	B
at	O
roughly	O
constant	O
energy	B
for	O
a	O
fixed	O
number	O
of	O
steps	O
giving	O
we	O
accept	O
this	O
point	O
if	O
hx	O
and	O
make	O
the	O
new	O
sample	O
line	O
otherwise	O
this	O
candidate	O
is	O
accepted	O
with	O
probability	O
hx	O
if	O
rejected	O
the	O
new	O
sample	O
is	O
taken	O
as	O
a	O
copy	O
of	O
x	O
isolated	O
high	O
density	B
islands	O
consider	O
drawing	O
samples	O
from	O
px	O
where	O
x	O
is	O
a	O
high-dimensional	O
vector	O
for	O
an	O
auxiliary	B
variable	I
y	O
we	O
introduce	O
a	O
distribution	B
pyx	O
to	O
form	O
the	O
joint	B
distribution	B
px	O
y	O
pyxpx	O
if	O
we	O
draw	O
samples	O
yl	O
from	O
this	O
joint	B
distribution	B
then	O
a	O
valid	O
set	O
of	O
samples	O
from	O
px	O
is	O
given	O
by	O
taking	O
the	O
xl	O
alone	O
if	O
we	O
sampled	O
x	O
directly	O
from	O
px	O
and	O
then	O
y	O
from	O
pyx	O
introducing	O
y	O
is	O
pointless	O
since	O
there	O
is	O
no	O
effect	O
on	O
the	O
x	O
sampling	B
procedure	O
in	O
order	O
for	O
this	O
to	O
be	O
useful	O
therefore	O
the	O
auxiliary	B
variable	I
must	O
influence	O
how	O
we	O
sample	O
x	O
below	O
we	O
discuss	O
some	O
of	O
the	O
common	O
auxiliary	B
variable	I
schemes	O
hybrid	B
monte	I
carlo	I
hybrid	O
mc	O
is	O
a	O
method	O
for	O
continuous	B
variables	O
that	O
aims	O
to	O
make	O
non-local	O
jumps	O
in	O
the	O
samples	O
and	O
in	O
so	O
doing	O
to	O
jump	O
potentially	O
from	O
one	O
mode	B
to	O
another	O
we	O
define	O
the	O
distribution	B
from	O
which	O
we	O
wish	O
to	O
sample	O
as	O
px	O
zx	O
ehxx	O
for	O
some	O
given	O
hamiltonian	O
hxx	O
is	O
just	O
a	O
potential	B
we	O
then	O
define	O
another	O
easy	O
distribution	B
from	O
which	O
we	O
can	O
readily	O
generate	O
samples	O
py	O
zy	O
ehyy	O
so	O
that	O
the	O
joint	B
distribution	B
is	O
given	O
by	O
px	O
y	O
pxpy	O
z	O
ehxxhyy	O
z	O
ehxy	O
in	O
the	O
standard	O
form	O
of	O
the	O
algorithm	B
a	O
multi-dimensional	O
gaussian	B
is	O
chosen	O
for	O
the	O
auxiliary	O
distribution	B
with	O
dim	O
y	O
dim	O
x	O
so	O
that	O
hyy	O
yty	O
the	O
hmc	O
algorithm	B
first	O
draws	O
from	O
py	O
and	O
subsequently	O
from	O
px	O
y	O
for	O
a	O
gaussian	B
py	O
sampling	B
from	O
is	O
straightforward	O
in	O
the	O
next	O
dynamic	B
step	O
a	O
sample	O
is	O
drawn	O
from	O
px	O
y	O
using	O
a	O
metropolis	O
draft	O
march	O
auxiliary	B
variable	I
methods	O
mcmc	O
sampler	O
the	O
idea	O
is	O
to	O
go	O
from	O
one	O
point	O
of	O
the	O
space	O
x	O
y	O
to	O
a	O
new	O
point	O
that	O
is	O
a	O
non-trivial	O
distance	O
from	O
x	O
y	O
and	O
which	O
will	O
be	O
accepted	O
with	O
a	O
high	O
probability	O
the	O
candidate	O
will	O
have	O
a	O
good	O
chance	O
to	O
be	O
accepted	O
if	O
is	O
close	O
to	O
hx	O
y	O
this	O
can	O
be	O
achieved	O
by	O
following	O
a	O
contour	O
of	O
equal	O
energy	B
h	O
as	O
described	O
in	O
the	O
next	O
section	O
hamiltonian	B
dynamics	I
we	O
wish	O
to	O
make	O
an	O
update	O
x	O
x	O
y	O
y	O
for	O
small	O
x	O
and	O
y	O
such	O
that	O
the	O
hamiltonian	O
hx	O
y	O
hxx	O
hyy	O
is	O
conserved	O
we	O
can	O
satisfy	O
this	O
to	O
first	O
order	O
by	O
considering	O
the	O
taylor	B
expansion	I
hx	O
y	O
hx	O
x	O
y	O
y	O
hx	O
xt	O
xhx	O
y	O
hy	O
yt	O
yhx	O
y	O
conservation	O
up	O
to	O
first	O
order	O
therefore	O
requires	O
xt	O
xhx	O
y	O
yt	O
yhx	O
y	O
this	O
is	O
a	O
single	O
scalar	O
requirement	O
and	O
there	O
are	O
therefore	O
many	O
different	O
solutions	O
for	O
x	O
and	O
y	O
that	O
satisfy	O
this	O
single	O
condition	O
it	O
is	O
customary	O
to	O
use	O
hamiltonian	B
dynamics	I
which	O
correspond	O
to	O
the	O
setting	O
x	O
yhx	O
y	O
y	O
xhx	O
y	O
where	O
is	O
a	O
small	O
value	B
to	O
ensure	O
that	O
the	O
taylor	B
expansion	I
is	O
accurate	O
hence	O
xt	O
xt	O
yhyy	O
yt	O
yt	O
xhxx	O
for	O
the	O
hmc	O
method	O
hx	O
y	O
hxx	O
hyy	O
so	O
that	O
xhx	O
y	O
xhxx	O
and	O
yhx	O
y	O
yhyy	O
for	O
the	O
gaussian	B
case	O
yhyy	O
y	O
so	O
that	O
yt	O
yt	O
xhx	O
xt	O
xt	O
there	O
are	O
specific	O
ways	O
to	O
implement	O
the	O
hamiltonian	B
dynamics	I
called	O
leapfrog	B
discretisation	I
that	O
are	O
more	O
accurate	O
than	O
the	O
simple	O
time-discretisation	O
used	O
above	O
and	O
we	O
refer	O
the	O
reader	O
to	O
for	O
details	O
in	O
order	O
to	O
make	O
a	O
symmetric	O
proposal	B
distribution	B
at	O
the	O
start	O
of	O
the	O
dynamic	B
step	O
we	O
choose	O
or	O
uniformly	O
this	O
means	O
that	O
there	O
is	O
the	O
same	O
chance	O
that	O
we	O
go	O
back	O
to	O
the	O
point	O
x	O
y	O
starting	O
from	O
as	O
vice	O
versa	O
we	O
then	O
follow	O
the	O
hamiltonian	B
dynamics	I
for	O
many	O
time	O
steps	O
of	O
the	O
order	O
of	O
several	O
hundred	O
to	O
reach	O
a	O
candidate	O
point	O
if	O
the	O
hamiltonian	B
dynamics	I
is	O
numerically	O
accurate	O
will	O
have	O
roughly	O
the	O
same	O
value	B
as	O
hx	O
y	O
we	O
then	O
do	O
a	O
metropolis	O
step	O
and	O
accept	O
the	O
point	O
if	O
hx	O
y	O
and	O
otherwise	O
accept	O
it	O
with	O
probability	O
hx	O
y	O
if	O
rejected	O
we	O
take	O
the	O
initial	O
point	O
x	O
y	O
as	O
the	O
sample	O
combined	O
with	O
the	O
py	O
sample	O
step	O
we	O
then	O
have	O
the	O
general	O
procedure	O
as	O
described	O
in	O
in	O
hmc	O
we	O
use	O
not	O
just	O
the	O
potential	B
hxx	O
to	O
define	O
candidate	O
samples	O
but	O
the	O
gradient	B
of	O
hxx	O
as	O
well	O
an	O
intuitive	O
explanation	O
for	O
the	O
success	O
of	O
the	O
algorithm	B
is	O
that	O
it	O
is	O
less	O
myopic	O
than	O
straightforward	O
metropolis	O
since	O
the	O
gradient	B
enables	O
the	O
algorithm	B
to	O
feel	O
its	O
way	O
to	O
other	O
regions	O
of	O
high	O
probability	O
by	O
contouring	O
paths	O
in	O
the	O
augmented	B
space	O
one	O
can	O
also	O
view	O
the	O
auxiliary	O
variables	O
as	O
momentum	B
variables	O
it	O
is	O
as	O
if	O
the	O
sample	O
has	O
now	O
a	O
momentum	B
which	O
can	O
carry	O
it	O
through	O
the	O
low-density	O
xregions	O
provided	O
this	O
momentum	B
is	O
high	O
enough	O
we	O
can	O
escape	O
local	B
regions	O
of	O
significant	O
probability	O
see	O
draft	O
march	O
auxiliary	B
variable	I
methods	O
algorithm	B
hybrid	B
monte	I
carlo	I
sampling	B
start	O
from	O
x	O
for	O
i	O
to	O
l	O
do	O
end	O
for	O
draw	O
a	O
new	O
sample	O
y	O
from	O
py	O
choose	O
a	O
random	O
or	O
backwards	O
trajectory	O
direction	O
starting	O
from	O
x	O
y	O
follow	O
hamiltonian	B
dynamics	I
for	O
a	O
fixed	O
number	O
of	O
steps	O
giving	O
a	O
candidate	O
accept	O
if	O
hx	O
y	O
otherwise	O
accept	O
it	O
with	O
probability	O
hx	O
y	O
if	O
rejected	O
we	O
take	O
the	O
sample	O
as	O
x	O
y	O
current	O
sample	O
of	O
states	O
figure	O
swendson-wang	B
updating	O
for	O
px	O
on	O
a	O
nearest	B
neighbour	B
lattice	O
like	O
coloured	O
neighbours	O
are	O
bonded	O
together	O
with	O
probability	O
e	O
forming	O
clusters	O
of	O
variables	O
each	O
cluster	O
is	O
given	O
a	O
random	O
colour	O
forming	O
the	O
new	O
sample	O
i	O
j	O
exp	O
i	O
xj	O
swendson-wang	B
originally	O
the	O
sw	O
method	O
was	O
introduced	O
to	O
alleviate	O
the	O
problems	O
encountered	O
in	O
sampling	B
from	O
ising	O
models	O
close	O
to	O
their	O
critical	O
at	O
this	O
point	O
large	O
islands	O
of	O
same-state	O
variables	O
form	O
so	O
that	O
strong	B
correlations	O
appear	O
in	O
the	O
distribution	B
the	O
scenario	O
under	O
which	O
for	O
example	O
gibbs	B
sampling	B
is	O
not	O
well	O
suited	O
the	O
method	O
has	O
been	O
generalised	B
to	O
other	O
although	O
here	O
we	O
outline	O
the	O
procedure	O
for	O
the	O
ising	B
model	B
only	O
referring	O
the	O
reader	O
to	O
more	O
specialised	O
text	O
for	O
the	O
extensions	O
see	O
also	O
for	O
the	O
use	O
of	O
auxiliary	O
variables	O
in	O
perfect	B
sampling	B
the	O
ising	B
model	B
with	O
no	O
external	O
fields	O
is	O
defined	O
on	O
variables	O
x	O
xn	O
xi	O
and	O
takes	O
the	O
form	O
e	O
ixixj	O
i	O
j	O
px	O
z	O
which	O
means	O
that	O
this	O
is	O
a	O
pairwise	B
markov	B
network	I
with	O
a	O
potential	B
contribution	O
e	O
if	O
neighbouring	O
nodes	O
i	O
and	O
j	O
on	O
a	O
square	O
lattice	O
are	O
in	O
the	O
same	O
state	O
and	O
a	O
contribution	O
otherwise	O
we	O
assume	O
that	O
which	O
encourages	O
neighbours	O
to	O
be	O
in	O
the	O
same	O
state	O
the	O
lattice	O
based	O
neighbourhood	O
structure	B
makes	O
this	O
difficult	O
to	O
sample	O
from	O
and	O
especially	O
when	O
which	O
encourages	O
large	O
scale	O
islands	O
of	O
same-state	O
variables	O
to	O
form	O
the	O
aim	O
is	O
to	O
remove	O
the	O
problematic	O
terms	O
e	O
ixixj	O
by	O
the	O
use	O
of	O
the	O
auxiliary	O
bond	O
variables	O
yij	O
one	O
for	O
each	O
edge	O
on	O
the	O
lattice	O
making	O
the	O
conditional	B
pxy	O
easy	O
to	O
sample	O
from	O
this	O
is	O
given	O
by	O
pxy	O
pyxpx	O
yij	O
e	O
ixixj	O
using	O
pyx	O
we	O
can	O
cancel	O
the	O
terms	O
e	O
ixixj	O
by	O
setting	O
pyijxi	O
xj	O
pyx	O
where	O
yij	O
e	O
ixixj	O
denotes	O
a	O
uniform	B
distribution	B
between	O
and	O
e	O
ixixj	O
zij	O
is	O
the	O
normalisa	O
e	O
ixixj	O
i	O
j	O
i	O
j	O
zij	O
i	O
j	O
draft	O
march	O
auxiliary	B
variable	I
methods	O
with	O
figure	O
ten	O
successive	O
samples	O
from	O
a	O
ising	B
model	B
px	O
exp	O
close	O
to	O
the	O
critical	O
temperature	O
the	O
swendson-wang	B
procedure	O
is	O
used	O
starting	O
in	O
a	O
random	O
initial	O
configuration	O
the	O
samples	O
quickly	O
move	O
away	O
from	O
this	O
initial	O
state	O
with	O
the	O
characteristic	O
longrange	O
correlations	O
of	O
the	O
variables	O
seen	O
close	O
to	O
the	O
critical	O
temperature	O
i	O
j	O
i	O
xj	O
tion	O
constant	O
zij	O
e	O
ixixj	O
hence	O
yij	O
e	O
ixixj	O
pxy	O
pyxpx	O
e	O
ixixj	O
yij	O
e	O
ixixj	O
i	O
j	O
i	O
j	O
e	O
ixixj	O
let	O
s	O
assume	O
that	O
we	O
have	O
a	O
sample	O
if	O
yij	O
then	O
to	O
draw	O
a	O
sample	O
from	O
pxy	O
we	O
must	O
have	O
e	O
ixixj	O
which	O
means	O
that	O
xi	O
and	O
xj	O
are	O
constrained	O
to	O
be	O
in	O
the	O
same	O
state	O
otherwise	O
if	O
yij	O
then	O
this	O
introduces	O
no	O
extra	O
constraint	O
on	O
xi	O
and	O
xj	O
hence	O
wherever	O
yij	O
we	O
bond	O
xi	O
and	O
xj	O
to	O
be	O
in	O
the	O
same	O
state	O
same	O
state	O
then	O
pyijxi	O
xj	O
to	O
sample	O
from	O
the	O
bond	O
variables	O
pyijxi	O
xj	O
consider	O
first	O
the	O
situation	O
that	O
xi	O
and	O
xj	O
are	O
in	O
the	O
pyijxi	O
xj	O
u	O
a	O
bond	O
will	O
occur	O
if	O
yij	O
which	O
occurs	O
with	O
probability	O
e	O
similarly	O
when	O
xi	O
and	O
xj	O
are	O
in	O
different	O
states	O
zij	O
yij	O
yij	O
e	O
pyij	O
xj	O
e	O
e	O
e	O
hence	O
if	O
xi	O
xj	O
we	O
bind	O
xi	O
and	O
xj	O
to	O
be	O
in	O
the	O
same	O
state	O
with	O
probability	O
e	O
on	O
the	O
other	O
hand	O
if	O
xi	O
and	O
xj	O
are	O
in	O
different	O
states	O
yij	O
is	O
uniformly	O
distributed	O
between	O
and	O
after	O
doing	O
this	O
for	O
all	O
the	O
xi	O
and	O
xj	O
pairs	O
we	O
will	O
end	O
up	O
with	O
a	O
graph	B
in	O
which	O
we	O
have	O
clusters	O
of	O
like-state	O
bonded	O
variables	O
the	O
algorithm	B
simply	O
chooses	O
a	O
random	O
state	O
for	O
each	O
cluster	O
that	O
is	O
with	O
probability	O
all	O
variables	O
in	O
the	O
cluster	O
are	O
in	O
state	O
the	O
algorithm	B
is	O
described	O
below	O
see	O
algorithm	B
swendson-wang	B
sampling	B
n	O
for	O
i	O
j	O
in	O
the	O
edge	O
set	O
do	O
start	O
from	O
a	O
random	O
configuration	O
of	O
all	O
for	O
l	O
to	O
l	O
do	O
end	O
for	O
if	O
xi	O
xj	O
we	O
bond	O
variables	O
xi	O
and	O
xj	O
with	O
probability	O
e	O
end	O
for	O
for	O
each	O
cluster	O
formed	O
from	O
the	O
above	O
set	O
the	O
state	O
of	O
the	O
cluster	O
uniformly	O
at	O
random	O
this	O
gives	O
a	O
new	O
joint	B
configuration	O
xl	O
xl	O
n	O
this	O
technique	O
has	O
found	O
application	O
in	O
spatial	O
statistics	O
particularly	O
image	O
slice	B
sampling	B
slice	O
is	O
an	O
auxiliary	B
variable	I
technique	O
that	O
aims	O
to	O
overcome	O
some	O
of	O
the	O
difficulties	O
in	O
choosing	O
an	O
appropriate	O
length	O
scale	O
in	O
methods	O
such	O
as	O
metropolis	O
sampling	B
the	O
brief	O
discussion	O
draft	O
march	O
importance	B
sampling	B
figure	O
the	O
full	O
slice	O
for	O
a	O
given	O
y	O
ideally	O
slice	B
sampling	B
would	O
draw	O
an	O
x	O
sample	O
from	O
anywhere	O
on	O
the	O
full	O
slice	O
in	O
general	O
this	O
is	O
intractable	O
for	O
a	O
complex	O
distribution	B
and	O
a	O
local	B
approximate	B
slice	O
is	O
formed	O
instead	O
see	O
z	O
p	O
where	O
the	O
here	O
follows	O
that	O
presented	O
in	O
and	O
we	O
want	O
to	O
draw	O
samples	O
from	O
px	O
normalisation	B
constant	I
z	O
is	O
unknown	O
by	O
introducing	O
the	O
auxiliary	B
variable	I
y	O
and	O
defining	O
the	O
distribution	B
for	O
y	O
p	O
p	O
otherwise	O
px	O
y	O
we	O
px	O
ydy	O
z	O
dy	O
z	O
px	O
p	O
which	O
shows	O
that	O
the	O
marginal	B
of	O
px	O
y	O
over	O
y	O
is	O
equal	O
to	O
the	O
distribution	B
we	O
wish	O
to	O
draw	O
samples	O
from	O
hence	O
if	O
we	O
draw	O
samples	O
from	O
px	O
y	O
we	O
can	O
ignore	O
the	O
y	O
samples	O
and	O
we	O
will	O
have	O
a	O
valid	O
sampling	B
scheme	O
for	O
px	O
to	O
draw	O
from	O
px	O
y	O
we	O
use	O
gibbs	B
sampling	B
first	O
drawing	O
from	O
pyx	O
and	O
then	O
from	O
pxy	O
drawing	O
a	O
sample	O
from	O
pyx	O
means	O
that	O
we	O
draw	O
a	O
value	B
y	O
from	O
the	O
uniform	B
distribution	B
u	O
p	O
given	O
a	O
sample	O
y	O
one	O
then	O
draws	O
a	O
sample	O
x	O
from	O
pxy	O
using	O
pxy	O
px	O
y	O
we	O
see	O
that	O
pxy	O
is	O
the	O
distribution	B
over	O
x	O
such	O
that	O
p	O
y	O
pxy	O
i	O
y	O
for	O
a	O
given	O
y	O
we	O
call	O
the	O
x	O
that	O
satisfy	O
this	O
a	O
slice	O
computing	O
the	O
normalisation	B
of	O
this	O
distribution	B
is	O
in	O
general	O
non-trivial	O
since	O
we	O
would	O
in	O
principle	O
need	O
to	O
search	O
over	O
all	O
x	O
to	O
find	O
those	O
for	O
which	O
p	O
y	O
ideally	O
we	O
would	O
like	O
to	O
get	O
as	O
much	O
of	O
the	O
slice	O
as	O
feasible	O
since	O
this	O
will	O
improve	O
the	O
mixing	O
of	O
the	O
chain	B
if	O
we	O
concentrate	O
on	O
the	O
part	O
of	O
the	O
slice	O
only	O
very	O
local	B
to	O
the	O
current	O
x	O
then	O
the	O
samples	O
move	O
through	O
the	O
space	O
very	O
slowly	O
if	O
we	O
attempt	O
to	O
guess	O
at	O
random	O
a	O
point	O
a	O
long	O
way	O
from	O
x	O
and	O
check	B
if	O
is	O
in	O
the	O
slice	O
this	O
will	O
be	O
mostly	O
unsuccessful	O
the	O
happy	O
compromise	O
presented	O
in	O
and	O
described	O
in	O
determines	O
an	O
appropriate	O
local	B
slice	O
by	O
adjusting	O
the	O
left	O
and	O
right	O
regions	O
the	O
technique	O
is	O
to	O
start	O
from	O
the	O
current	O
x	O
and	O
attempt	O
to	O
find	O
the	O
largest	O
local	B
slice	O
by	O
incrementally	O
widening	O
the	O
candidate	O
slice	O
once	O
we	O
have	O
the	O
largest	O
potential	B
slice	O
we	O
attempt	O
to	O
sample	O
from	O
this	O
if	O
the	O
sample	O
point	O
within	O
the	O
local	B
slice	O
is	O
in	O
fact	O
not	O
in	O
the	O
slice	O
this	O
is	O
rejected	O
and	O
the	O
slice	O
is	O
shrunk	O
this	O
describes	O
a	O
procedure	O
for	O
sampling	B
from	O
a	O
univariate	B
distribution	B
p	O
to	O
sample	O
from	O
a	O
multivariate	B
distribution	B
px	O
single	O
variable	O
gibbs	B
sampling	B
can	O
be	O
used	O
to	O
sample	O
from	O
pxjxj	O
repeatedly	O
choosing	O
a	O
new	O
variable	O
xj	O
to	O
sample	O
importance	B
sampling	B
z	O
where	O
p	O
can	O
be	O
evaluated	O
but	O
z	O
importance	B
sampling	B
is	O
a	O
technique	O
to	O
approximate	B
averages	O
with	O
respect	O
to	O
an	O
intractable	O
distribution	B
px	O
the	O
term	O
sampling	B
is	O
arguably	O
a	O
misnomer	O
since	O
the	O
method	O
does	O
not	O
attempt	O
to	O
draw	O
samples	O
from	O
px	O
rather	O
the	O
method	O
draws	O
samples	O
from	O
a	O
simpler	O
importance	B
distribution	B
qx	O
and	O
then	O
reweights	O
them	O
such	O
that	O
averages	O
with	O
respect	O
to	O
px	O
can	O
be	O
approximated	O
using	O
the	O
samples	O
from	O
qx	O
consider	O
px	O
p	O
x	O
p	O
is	O
an	O
intractable	O
normalisation	B
constant	I
the	O
average	B
of	O
fx	O
with	O
respect	O
to	O
px	O
is	O
given	O
by	O
x	O
fx	O
p	O
p	O
qx	O
qx	O
x	O
fxp	O
x	O
p	O
fxpx	O
qx	O
qx	O
x	O
x	O
draft	O
march	O
importance	B
sampling	B
figure	O
for	O
the	O
current	O
sample	O
x	O
a	O
point	O
y	O
is	O
sampled	O
between	O
and	O
p	O
giving	O
a	O
point	O
y	O
circle	O
then	O
an	O
interval	O
of	O
width	O
w	O
is	O
placed	O
around	O
x	O
the	O
blue	O
bar	O
the	O
ends	O
of	O
the	O
bar	O
the	O
interval	O
is	O
increased	O
until	O
it	O
denote	O
if	O
the	O
point	O
is	O
in	O
the	O
slice	O
or	O
out	O
of	O
the	O
slice	O
given	O
an	O
interval	O
a	O
sample	O
is	O
taken	O
uniformly	O
in	O
the	O
interval	O
if	O
the	O
hits	O
a	O
point	O
out	O
of	O
the	O
slice	O
candidate	O
is	O
not	O
in	O
the	O
slice	O
y	O
the	O
candidate	O
is	O
rejected	O
and	O
the	O
interval	O
is	O
shrunk	O
the	O
sampling	B
from	O
the	O
interval	O
is	O
repeated	O
until	O
a	O
candidate	O
is	O
in	O
the	O
slice	O
and	O
is	O
subsequently	O
accepted	O
let	O
xl	O
be	O
samples	O
from	O
qx	O
then	O
we	O
can	O
approximate	B
the	O
average	B
by	O
fxpx	O
x	O
fxl	O
p	O
qxl	O
p	O
qxl	O
fxlwl	O
where	O
we	O
define	O
the	O
normalised	B
importance	B
weights	I
p	O
p	O
wl	O
with	O
wl	O
in	O
principle	O
reweighing	O
the	O
samples	O
from	O
q	O
will	O
give	O
the	O
correct	O
result	O
for	O
the	O
average	B
with	O
respect	O
to	O
p	O
since	O
the	O
weight	B
is	O
a	O
measure	O
of	O
how	O
well	O
q	O
matches	O
p	O
there	O
will	O
typically	O
be	O
only	O
a	O
single	O
dominant	O
weight	B
this	O
is	O
particularly	O
evident	O
in	O
high	O
dimensions	O
there	O
will	O
typically	O
only	O
be	O
one	O
dominant	O
weight	B
with	O
value	B
close	O
to	O
and	O
the	O
rest	O
will	O
be	O
zero	O
particularly	O
if	O
the	O
sampling	B
distribution	B
q	O
is	O
not	O
well	O
matched	O
to	O
p	O
as	O
an	O
indication	O
of	O
this	O
effect	O
consider	O
a	O
d-dimensional	O
multivariate	B
x	O
with	O
two	O
samples	O
and	O
for	O
simplicity	O
we	O
assume	O
that	O
both	O
p	O
and	O
q	O
are	O
factorised	B
over	O
their	O
variables	O
the	O
associated	O
unnormalised	O
importance	B
weights	O
are	O
then	O
p	O
d	O
d	O
p	O
d	O
d	O
if	O
we	O
assume	O
the	O
the	O
match	O
between	O
q	O
and	O
p	O
better	O
is	O
worse	O
by	O
a	O
factor	B
in	O
each	O
of	O
the	O
dimensions	O
at	O
than	O
then	O
so	O
that	O
importance	B
weight	B
at	O
will	O
exponentially	O
dominate	O
a	O
method	O
that	O
can	O
help	O
address	O
this	O
weight	B
dominance	O
is	O
resampling	B
given	O
the	O
weight	B
distribution	B
wl	O
one	O
draws	O
a	O
set	O
of	O
l	O
sample	O
indices	O
this	O
new	O
set	O
of	O
indices	O
will	O
almost	O
certainly	O
contain	O
repeats	O
since	O
any	O
of	O
the	O
original	O
low-weight	O
samples	O
will	O
most	O
likely	O
not	O
be	O
included	O
the	O
weight	B
of	O
each	O
of	O
these	O
new	O
samples	O
is	O
set	O
uniformly	O
to	O
this	O
procedure	O
helps	O
select	O
only	O
the	O
fittest	O
of	O
the	O
samples	O
and	O
is	O
known	O
as	O
sampling	B
importance	B
draft	O
march	O
draw	O
a	O
vertical	O
coordinate	O
y	O
uniformly	O
from	O
the	O
p	O
create	O
a	O
horizontal	O
interval	O
t	O
xright	O
that	O
contains	O
xi	O
as	O
follows	O
draw	O
r	O
u	O
xlef	O
t	O
xi	O
rw	O
xright	O
xi	O
rw	O
while	O
p	O
t	O
y	O
do	O
xlef	O
t	O
xlef	O
t	O
w	O
algorithm	B
slice	B
sampling	B
end	O
while	O
while	O
p	O
y	O
do	O
xright	O
xright	O
w	O
choose	O
a	O
starting	O
point	O
and	O
step	O
size	O
w	O
for	O
i	O
to	O
l	O
do	O
end	O
for	O
end	O
if	O
end	O
while	O
end	O
if	O
else	O
end	O
while	O
accept	O
false	O
while	O
accept	O
false	O
do	O
draw	O
a	O
random	O
value	B
uniformly	O
from	O
the	O
unit	O
interval	O
t	O
xright	O
if	O
p	O
y	O
then	O
accept	O
true	O
modify	O
the	O
interval	O
t	O
xright	O
as	O
follows	O
if	O
xi	O
then	O
xright	O
xlef	O
t	O
else	O
importance	B
sampling	B
create	O
an	O
initial	O
interval	O
step	O
out	O
left	O
step	O
out	O
right	O
found	O
a	O
valid	O
sample	O
shrinking	O
sequential	B
importance	B
sampling	B
one	O
can	O
apply	O
importance	B
sampling	B
to	O
temporal	O
distributions	O
for	O
which	O
the	O
importance	B
distribution	B
samples	O
from	O
are	O
paths	O
in	O
many	O
applications	O
such	O
as	O
tracking	O
one	O
wishes	O
to	O
update	O
ones	O
beliefs	O
as	O
time	O
increases	O
and	O
as	O
such	O
is	O
required	O
to	O
resample	O
and	O
then	O
reweight	O
the	O
whole	O
path	B
for	O
distributions	O
with	O
a	O
markov	O
structure	B
one	O
would	O
expect	O
that	O
a	O
local	B
update	O
is	O
possible	O
without	O
needing	O
to	O
deal	O
with	O
the	O
previous	O
path	B
to	O
show	O
this	O
consider	O
the	O
unnormalised	O
importance	B
weights	O
for	O
a	O
sample	O
path	B
xl	O
t	O
p	O
qxl	O
wl	O
p	O
qxl	O
p	O
txl	O
p	O
p	O
qxl	O
wl	O
we	O
can	O
recursively	O
define	O
the	O
un-normalised	O
weights	O
using	O
wl	O
t	O
wl	O
t	O
l	O
t	O
t	O
where	O
l	O
t	O
p	O
txl	O
p	O
this	O
means	O
that	O
in	O
sis	O
we	O
need	O
only	O
define	O
the	O
conditional	B
importance	B
distribution	B
the	O
ideal	O
setting	O
of	O
the	O
sequential	B
importance	B
distribution	B
is	O
q	O
p	O
and	O
although	O
this	O
choice	O
is	O
impractical	O
in	O
most	O
cases	O
draft	O
march	O
importance	B
sampling	B
figure	O
a	O
dynamic	B
bayesian	B
network	I
in	O
many	O
applications	O
of	O
interest	O
the	O
emission	B
distribution	B
pvtht	O
is	O
non-gaussian	O
leading	O
to	O
the	O
formal	O
intractability	O
of	O
filteringsmoothing	O
for	O
dynamic	B
bayes	O
networks	O
equation	B
will	O
simplify	O
considerably	O
for	O
example	O
consider	O
distributions	O
with	O
a	O
hidden	B
markov	O
independence	B
structure	B
pvthtphtht	O
where	O
are	O
observations	O
and	O
are	O
the	O
random	O
variables	O
a	O
cancelation	O
of	O
terms	O
in	O
the	O
numerator	O
and	O
denominator	O
occurs	O
leaving	O
simply	O
l	O
t	O
t	O
pvthl	O
qhl	O
tphl	O
thl	O
thl	O
sequential	B
importance	B
sampling	B
is	O
also	O
known	O
as	O
particle	O
filtering	O
particularly	O
in	O
cases	O
where	O
the	O
transition	O
is	O
easy	O
to	O
sample	O
from	O
a	O
common	O
sequential	B
importance	B
distribution	B
is	O
phtht	O
in	O
which	O
case	O
from	O
equation	B
l	O
by	O
t	O
pvtht	O
and	O
the	O
unnormalised	O
weights	O
are	O
recursively	O
defined	O
wl	O
t	O
wl	O
t	O
t	O
a	O
drawback	O
of	O
this	O
procedure	O
is	O
that	O
after	O
a	O
small	O
number	O
of	O
iterations	O
only	O
very	O
few	O
particle	O
weights	O
will	O
be	O
significantly	O
non-zero	O
due	O
to	O
the	O
mismatch	O
between	O
the	O
importance	B
distribution	B
q	O
and	O
the	O
target	O
distribution	B
p	O
this	O
can	O
be	O
addressed	O
using	O
resampling	B
as	O
described	O
in	O
particle	O
filtering	O
as	O
an	O
approximate	B
forward	O
pass	O
particle	O
filtering	O
can	O
be	O
viewed	O
as	O
an	O
approximation	B
to	O
the	O
exact	O
filtering	O
recursion	O
using	O
to	O
represent	O
the	O
filtered	O
distribution	B
the	O
exact	O
filtering	O
recursion	O
is	O
pvtht	O
phtht	O
ht	O
a	O
pf	O
can	O
be	O
viewed	O
as	O
an	O
approximation	B
of	O
equation	B
in	O
which	O
the	O
message	B
is	O
approximated	O
by	O
a	O
sum	O
of	O
wl	O
t	O
ht	O
hl	O
t	O
t	O
are	O
the	O
normalised	O
importance	B
where	O
wl	O
t	O
are	O
the	O
particles	O
in	O
other	O
words	O
the	O
message	B
is	O
represented	O
as	O
a	O
weighted	O
mixture	B
of	O
delta-spikes	O
where	O
the	O
weight	B
and	O
position	O
of	O
the	O
spikes	O
are	O
the	O
parameters	O
of	O
the	O
distribution	B
using	O
equation	B
in	O
equation	B
we	O
have	O
t	O
and	O
hl	O
wl	O
z	O
pvtht	O
draft	O
march	O
phthl	O
t	O
t	O
importance	B
sampling	B
the	O
constant	O
z	O
is	O
used	O
to	O
normalise	O
the	O
distribution	B
although	O
was	O
a	O
simple	O
sum	O
of	O
delta	O
peaks	O
in	O
general	O
will	O
not	O
be	O
the	O
delta-peaks	O
get	O
broadened	O
by	O
the	O
hidden-to-hidden	O
and	O
hiddento-observation	O
factors	O
our	O
task	O
is	O
then	O
to	O
approximate	B
as	O
a	O
new	O
sum	O
of	O
delta-peaks	O
below	O
we	O
discuss	O
a	O
method	O
to	O
achieve	O
this	O
for	O
which	O
explicit	O
knowledge	O
of	O
the	O
normalisation	B
z	O
is	O
not	O
required	O
this	O
is	O
useful	O
since	O
in	O
many	O
tracking	O
applications	O
the	O
normalisation	B
of	O
the	O
emission	O
pvtht	O
is	O
unknown	O
a	O
monte-carlo	O
sampling	B
approximation	B
a	O
simple	O
approach	B
to	O
forming	O
an	O
approximate	B
mixture-of-delta	O
functions	O
representation	O
of	O
equation	B
is	O
to	O
generate	O
a	O
set	O
of	O
sample	O
points	O
using	O
importance	B
sampling	B
that	O
is	O
we	O
generate	O
a	O
set	O
from	O
some	O
importance	B
distribution	B
qht	O
which	O
gives	O
the	O
unnormalised	O
importance	B
of	O
samples	O
weights	O
t	O
hl	O
t	O
pvthl	O
wl	O
t	O
t	O
t	O
phl	O
qhl	O
t	O
defining	O
the	O
normalised	O
weights	O
we	O
obtain	O
an	O
approximation	B
wl	O
t	O
t	O
wl	O
wl	O
t	O
ht	O
hl	O
t	O
ideally	O
one	O
would	O
use	O
the	O
importance	B
distribution	B
that	O
makes	O
the	O
importance	B
weights	O
unity	O
namely	O
qht	O
pvtht	O
phthl	O
t	O
t	O
however	O
this	O
is	O
often	O
difficult	O
to	O
sample	O
from	O
directly	O
due	O
to	O
the	O
unknown	O
normalisation	B
of	O
the	O
emission	O
pvtht	O
a	O
simpler	O
alternative	O
is	O
to	O
sample	O
from	O
the	O
transition	O
mixture	B
qht	O
phthl	O
t	O
t	O
to	O
do	O
so	O
one	O
first	O
samples	O
a	O
component	O
l	O
from	O
the	O
histogram	O
with	O
weights	O
from	O
this	O
sample	O
index	O
say	O
l	O
one	O
then	O
draws	O
a	O
sample	O
from	O
phthl	O
t	O
given	O
t	O
in	O
this	O
case	O
the	O
un-normalised	O
weights	O
t	O
wl	O
become	O
simply	O
t	O
pvthl	O
wl	O
t	O
this	O
forward-sampling-resampling	B
procedure	O
is	O
used	O
in	O
demoparticlefilter	O
m	O
and	O
in	O
the	O
following	O
toy	O
example	O
example	O
toy	O
face-tracking	O
example	O
at	O
time	O
t	O
a	O
binary	O
face	O
template	O
is	O
in	O
a	O
location	O
ht	O
which	O
describes	O
the	O
upper-left	O
corner	O
of	O
the	O
template	O
using	O
a	O
two-dimensional	O
vector	O
at	O
time	O
t	O
the	O
position	O
of	O
the	O
face	O
is	O
known	O
see	O
the	O
face	O
template	O
is	O
known	O
in	O
subsequent	O
times	O
the	O
face	O
moves	O
randomly	O
according	O
to	O
ht	O
ht	O
t	O
where	O
t	O
n	O
t	O
i	O
is	O
a	O
two	O
dimensional	O
zero	O
mean	B
unit	O
covariance	B
noise	O
vector	O
in	O
addition	O
a	O
fraction	O
of	O
the	O
binary	O
pixels	O
in	O
the	O
whole	O
image	O
are	O
selected	O
at	O
random	O
and	O
their	O
states	O
flipped	O
the	O
aim	O
draft	O
march	O
importance	B
sampling	B
figure	O
tracking	O
an	O
object	O
with	O
a	O
particle	B
filter	I
containing	O
particles	O
the	O
small	O
circles	O
are	O
the	O
particles	O
scaled	O
by	O
their	O
weights	O
the	O
correct	O
corner	O
position	O
of	O
the	O
face	O
is	O
given	O
by	O
the	O
the	O
filtered	O
average	B
by	O
the	O
large	O
circle	O
o	O
and	O
the	O
most	O
likely	O
particle	O
by	O
initial	O
position	O
of	O
the	O
face	O
without	O
noise	O
and	O
corresponding	O
weights	O
of	O
the	O
particles	O
face	O
with	O
noisy	O
background	O
and	O
the	O
tracked	O
corner	O
position	O
after	O
timesteps	O
the	O
forwardsampling-resampling	O
pf	O
method	O
is	O
used	O
to	O
maintain	O
a	O
healthy	O
proportion	O
of	O
non-zero	O
weights	O
see	O
demoparticlefilter	O
m	O
is	O
to	O
try	O
to	O
track	O
the	O
upper-left	O
corner	O
of	O
the	O
face	O
through	O
time	O
we	O
need	O
to	O
define	O
the	O
emission	B
distribution	B
pvtht	O
on	O
the	O
binary	O
pixels	O
with	O
vi	O
consider	O
the	O
following	O
compatibility	B
function	B
ht	O
vt	O
t	O
vht	O
where	O
vht	O
is	O
the	O
vector	O
representing	O
the	O
image	O
with	O
a	O
clean	O
face	O
placed	O
at	O
position	O
ht	O
this	O
measures	O
the	O
overlap	O
between	O
the	O
face	O
template	O
and	O
the	O
noisy	O
image	O
restricted	B
to	O
the	O
template	O
pixels	O
the	O
compatibility	B
function	B
is	O
maximal	O
when	O
the	O
observed	O
image	O
vt	O
has	O
the	O
face	O
placed	O
at	O
position	O
ht	O
we	O
can	O
therefore	O
tentatively	O
define	O
pvtht	O
ht	O
a	O
subtlety	O
is	O
that	O
ht	O
is	O
continuous	B
and	O
in	O
the	O
compatibility	B
function	B
we	O
first	O
map	B
ht	O
to	O
the	O
nearest	O
integer	O
pixel	O
representation	O
we	O
have	O
not	O
specified	O
the	O
normalisation	B
constant	I
of	O
this	O
distribution	B
which	O
fortunately	O
this	O
is	O
not	O
required	O
by	O
the	O
particle	O
filtering	O
algorithm	B
in	O
particles	O
are	O
used	O
to	O
track	O
the	O
face	O
the	O
particles	O
are	O
plotted	O
along	O
with	O
their	O
corresponding	O
weights	O
for	O
each	O
t	O
of	O
the	O
pixels	O
are	O
selected	O
at	O
random	O
in	O
the	O
image	O
and	O
their	O
states	O
flipped	O
using	O
the	O
forward-samplingresampling	O
method	O
we	O
can	O
successfully	O
track	O
the	O
face	O
despite	O
the	O
presence	O
of	O
the	O
background	O
clutter	O
real	O
tracking	O
applications	O
involve	O
complex	O
issues	O
including	O
tracking	O
multiple	O
objects	O
transformations	O
of	O
the	O
object	O
rotation	O
morphology	O
changes	O
nevertheless	O
the	O
principles	O
are	O
largely	O
the	O
same	O
and	O
many	O
tracking	O
applications	O
work	O
by	O
seeking	O
simple	O
compatibility	O
functions	O
often	O
based	O
on	O
the	O
colour	O
histogram	O
in	O
a	O
template	O
indeed	O
tracking	O
objects	O
in	O
complex	O
environments	O
was	O
one	O
of	O
the	O
original	O
applications	O
of	O
particle	O
filters	O
draft	O
march	O
weights	O
code	O
potsample	O
m	O
exact	O
sample	O
from	O
a	O
set	O
of	O
potentials	O
ancestralsample	O
m	O
ancestral	B
sample	O
from	O
a	O
belief	O
net	O
jtsample	O
m	O
sampling	B
from	O
a	O
consistent	B
junction	B
tree	B
gibbssample	O
m	O
gibbs	B
sampling	B
from	O
a	O
set	O
of	O
potentials	O
demometropolis	O
m	O
demo	O
of	O
metropolis	O
sampling	B
for	O
a	O
bimodal	O
distribution	B
metropolis	O
m	O
metropolis	O
sample	O
logp	O
m	O
log	O
of	O
a	O
bimodal	O
distribution	B
demoparticlefilter	O
m	O
demo	O
particle	O
filtering	O
method	O
placeobject	O
m	O
place	O
an	O
object	O
in	O
a	O
grid	O
compat	O
m	O
compatibility	B
function	B
demosamplehmm	O
m	O
naive	O
gibbs	B
sampling	B
for	O
a	O
hmm	B
exercises	O
exercise	O
method	O
let	O
u	O
u	O
and	O
log	O
sin	O
show	O
that	O
log	O
cos	O
n	O
exercises	O
and	O
suggest	O
an	O
algorithm	B
to	O
sample	O
from	O
a	O
univariate	B
normal	B
distribution	B
exercise	O
consider	O
the	O
distribution	B
for	O
fixed	O
in	O
a	O
given	O
state	O
write	O
down	O
a	O
distribution	B
on	O
the	O
remaining	O
variables	O
and	O
explain	O
how	O
forward	B
sampling	B
can	O
be	O
carried	O
out	O
for	O
this	O
new	O
distribution	B
exercise	O
consider	O
an	O
ising	B
model	B
on	O
an	O
m	O
m	O
square	O
lattice	O
with	O
nearest	B
neighbour	B
interactions	O
i	O
xj	O
i	O
j	O
px	O
exp	O
now	O
consider	O
the	O
m	O
m	O
grid	O
as	O
a	O
checkerboard	B
and	O
give	O
each	O
white	O
square	O
a	O
label	O
wi	O
and	O
each	O
black	O
square	O
a	O
label	O
bj	O
so	O
that	O
each	O
square	O
is	O
associated	O
with	O
a	O
particular	O
variable	O
show	O
that	O
that	O
is	O
conditioned	O
on	O
the	O
white	O
variables	O
the	O
black	O
variables	O
are	O
independent	O
the	O
converse	O
is	O
also	O
true	O
that	O
conditioned	O
on	O
the	O
black	O
variables	O
the	O
white	O
variables	O
are	O
independent	O
explain	O
how	O
this	O
can	O
be	O
exploited	O
by	O
a	O
gibbs	B
sampling	B
procedure	O
this	O
procedure	O
is	O
known	O
as	O
checkerboard	B
or	O
black	B
and	I
white	I
sampling	B
exercise	O
consider	O
the	O
symmetric	O
gaussian	B
proposal	B
distribution	B
and	O
the	O
target	O
distribution	B
q	O
x	O
n	O
q	O
p	O
n	O
px	O
n	O
log	O
px	O
where	O
dim	O
x	O
n	O
show	O
that	O
discuss	O
how	O
this	O
result	O
relates	O
to	O
the	O
probability	O
of	O
accepting	O
a	O
metropolis-hastings	B
update	O
under	O
a	O
gaussian	B
proposal	B
distribution	B
in	O
high-dimensions	O
draft	O
march	O
exercises	O
exercise	O
the	O
file	O
demosamplehmm	O
m	O
performs	O
naive	O
gibbs	B
sampling	B
of	O
the	O
posterior	B
for	O
a	O
hmm	B
at	O
each	O
gibbs	B
update	O
a	O
single	O
variable	O
ht	O
is	O
chosen	O
with	O
the	O
remaining	O
h	O
variables	O
clamped	O
the	O
procedure	O
starts	O
from	O
t	O
and	O
sweeps	O
forwards	O
through	O
time	O
when	O
the	O
end	O
time	O
t	O
t	O
is	O
reached	O
the	O
joint	B
state	O
is	O
taken	O
as	O
a	O
sample	O
from	O
the	O
posterior	B
the	O
parameter	B
controls	O
how	O
deterministic	B
the	O
hidden	B
transition	B
matrix	B
phtht	O
will	O
be	O
adjust	O
demosamplehmm	O
m	O
to	O
run	O
times	O
each	O
time	O
for	O
the	O
same	O
computing	O
a	O
mean	B
absolute	O
error	O
over	O
these	O
runs	O
then	O
repeat	O
this	O
for	O
discuss	O
why	O
the	O
performance	B
of	O
this	O
gibbs	B
sampling	B
routine	O
deteriorates	O
with	O
increasing	O
draft	O
march	O
exercises	O
draft	O
march	O
chapter	O
deterministic	B
approximate	B
inference	B
introduction	O
deterministic	B
approximate	B
inference	B
methods	O
are	O
an	O
alternative	O
to	O
the	O
stochastic	O
techniques	O
discussed	O
in	O
whilst	O
stochastic	O
methods	O
are	O
powerful	O
and	O
often	O
generally	O
applicable	O
they	O
nevertheless	O
produce	O
sample	O
estimates	O
of	O
a	O
quantity	O
even	O
if	O
we	O
are	O
able	O
to	O
perform	O
perfect	B
sampling	B
we	O
would	O
still	O
only	O
obtain	O
an	O
approximate	B
result	O
due	O
to	O
the	O
inherent	O
uncertainty	B
introduced	O
by	O
sampling	B
furthermore	O
in	O
practice	O
drawing	O
exact	O
samples	O
is	O
typically	O
computationally	O
intractable	O
and	O
assessing	O
the	O
quality	O
of	O
the	O
sample	O
estimates	O
is	O
difficult	O
in	O
this	O
chapter	O
we	O
discuss	O
some	O
alternative	O
deterministic	B
approximate	B
inference	B
schemes	O
the	O
first	O
laplace	B
s	O
method	O
is	O
a	O
simple	O
perturbation	O
technique	O
the	O
second	O
class	O
of	O
methods	O
are	O
those	O
that	O
produce	O
rigorous	O
bounds	O
on	O
quantities	O
of	O
interest	O
such	O
methods	O
are	O
interesting	O
since	O
they	O
provide	O
certain	O
knowledge	O
it	O
may	O
be	O
sufficient	O
for	O
example	O
to	O
show	O
that	O
a	O
marginal	B
probability	O
is	O
greater	O
than	O
in	O
order	O
to	O
make	O
an	O
informed	O
decision	O
a	O
further	O
class	O
of	O
methods	O
are	O
the	O
consistency	O
methods	O
such	O
as	O
loopy	B
belief	B
propagation	B
such	O
methods	O
have	O
revolutionised	O
certain	O
fields	O
including	O
error	O
providing	O
performance	B
unobtainable	O
from	O
sampling	B
based	O
procedures	O
it	O
is	O
important	O
to	O
bear	O
in	O
mind	O
that	O
no	O
single	O
approximation	B
technique	O
deterministic	B
or	O
stochastic	O
is	O
going	O
to	O
beat	O
all	O
others	O
on	O
all	O
problems	O
given	O
the	O
same	O
computational	O
resources	O
in	O
this	O
sense	O
insight	O
as	O
to	O
the	O
properties	B
of	O
the	O
approximation	B
method	O
used	O
is	O
useful	O
in	O
matching	O
an	O
approximation	B
method	O
to	O
the	O
problem	B
at	O
hand	O
the	O
laplace	B
approximation	B
consider	O
a	O
distribution	B
on	O
a	O
continuous	B
variable	O
of	O
the	O
form	O
px	O
ex	O
z	O
e	O
the	O
laplace	B
method	O
makes	O
a	O
gaussian	B
approximation	B
of	O
px	O
based	O
on	O
a	O
local	B
perturbation	O
expansion	O
around	O
a	O
mode	B
x	O
first	O
we	O
find	O
the	O
mode	B
numerically	O
giving	O
x	O
argmin	O
x	O
ex	O
then	O
a	O
taylor	B
expansion	I
up	O
to	O
second	O
order	O
around	O
this	O
mode	B
gives	O
ex	O
ex	O
x	O
ex	O
where	O
h	O
exx	O
is	O
the	O
hessian	B
evaluated	O
at	O
the	O
mode	B
at	O
the	O
mode	B
ex	O
and	O
an	O
approximation	B
of	O
the	O
distribution	B
is	O
given	O
by	O
the	O
gaussian	B
x	O
h	O
x	O
x	O
h	O
p	O
z	O
e	O
x	O
x	O
n	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	B
figure	O
fitting	O
a	O
mixture	B
of	O
gaussians	O
px	O
with	O
a	O
single	O
gaussian	B
the	O
green	O
curve	O
minimises	O
klqp	O
corresponding	O
to	O
fitting	O
a	O
local	B
model	B
the	O
red	O
curve	O
minimises	O
klpq	O
corresponding	O
to	O
moment	O
matching	O
which	O
has	O
mean	B
x	O
and	O
covariance	B
h	O
with	O
z	O
h	O
similarly	O
we	O
can	O
use	O
the	O
above	O
ex	O
h	O
expansion	O
to	O
estimate	O
the	O
integral	O
e	O
ex	O
x	O
x	O
ex	O
e	O
x	O
x	O
e	O
although	O
the	O
laplace	B
approximation	B
fits	O
a	O
gaussian	B
to	O
a	O
distribution	B
it	O
is	O
not	O
necessarily	O
the	O
best	O
gaussian	B
approximation	B
as	O
we	O
ll	O
see	O
below	O
other	O
criteria	O
such	O
as	O
based	O
on	O
minimal	O
kl	O
divergence	B
between	O
px	O
and	O
a	O
gaussian	B
approximation	B
may	O
be	O
more	O
appropriate	O
depending	O
on	O
the	O
context	O
a	O
benefit	O
of	O
laplace	B
s	O
method	O
is	O
its	O
relative	O
speed	O
and	O
simplicity	O
compared	O
with	O
other	O
approximate	B
inference	B
techniques	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	B
variational	O
methods	O
can	O
be	O
used	O
to	O
approximate	B
a	O
complex	O
distribution	B
px	O
by	O
a	O
simpler	O
distribution	B
qx	O
given	O
a	O
definition	O
of	O
discrepancy	O
between	O
an	O
approximation	B
qx	O
to	O
px	O
any	O
free	O
parameters	O
of	O
qx	O
are	O
then	O
set	O
by	O
minimising	O
the	O
discrepancy	O
a	O
particularly	O
popular	O
measure	O
of	O
the	O
discrepancy	O
between	O
an	O
approximation	B
qx	O
and	O
the	O
intractable	O
distribution	B
px	O
is	O
the	O
kullback-leibler	B
divergence	B
klqp	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
klqp	O
and	O
is	O
zero	O
if	O
and	O
only	O
if	O
the	O
distributions	O
p	O
and	O
q	O
are	O
identical	O
see	O
note	O
that	O
whilst	O
the	O
kl	O
divergence	B
cannot	O
be	O
negative	O
there	O
is	O
no	O
upper	O
bound	B
on	O
the	O
value	B
it	O
can	O
potentially	O
take	O
so	O
that	O
the	O
discrepancy	O
can	O
be	O
infinitely	O
bad	O
bounding	O
the	O
normalisation	B
constant	I
for	O
a	O
distribution	B
of	O
the	O
form	O
px	O
z	O
e	O
we	O
have	O
klqp	O
log	O
z	O
since	O
klqp	O
this	O
immediately	O
gives	O
the	O
bound	B
log	O
z	O
entropy	B
energy	B
which	O
is	O
called	O
the	O
free	O
energy	B
bound	B
in	O
the	O
physics	O
using	O
the	O
notation	O
hq	O
for	O
the	O
entropy	B
of	O
q	O
we	O
can	O
write	O
the	O
bound	B
more	O
compactly	O
as	O
log	O
z	O
hq	O
the	O
klqp	O
method	O
provides	O
therefore	O
a	O
lower	O
bound	B
on	O
the	O
normalisation	B
constant	I
for	O
some	O
models	O
it	O
is	O
possible	O
alternative	O
methods	O
see	O
for	O
example	O
and	O
to	O
also	O
form	O
an	O
upper	O
draft	O
march	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	B
bound	B
on	O
the	O
normalisation	B
constant	I
with	O
both	O
an	O
upper	O
and	O
lower	O
bound	B
on	O
the	O
normalisation	B
terms	O
we	O
are	O
able	O
to	O
bracket	O
marginals	O
l	O
pxi	O
u	O
see	O
the	O
tightness	O
of	O
the	O
resulting	O
bracket	O
gives	O
an	O
indication	O
as	O
to	O
how	O
tight	O
the	O
bounding	O
procedures	O
are	O
even	O
in	O
cases	O
where	O
the	O
resulting	O
bracket	O
is	O
weak	O
for	O
example	O
it	O
might	O
be	O
that	O
the	O
result	O
is	O
that	O
pcancer	O
true	O
this	O
may	O
be	O
sufficient	O
for	O
decision	O
making	O
purposes	O
since	O
the	O
probability	O
of	O
cancer	O
is	O
sufficiently	O
large	O
to	O
merit	O
action	O
bounding	O
the	O
marginal	B
likelihood	B
in	O
bayesian	B
modelling	B
the	O
likelihood	B
of	O
the	O
model	B
m	O
with	O
parameters	O
generating	O
data	O
d	O
is	O
given	O
by	O
pdm	O
pd	O
likelihood	B
p	O
prior	B
this	O
quantity	O
is	O
fundamental	O
to	O
model	B
comparison	O
however	O
in	O
cases	O
where	O
is	O
high-dimensional	O
the	O
integral	O
over	O
is	O
difficult	O
to	O
perform	O
using	O
bayes	O
rule	O
p	O
pd	O
pdm	O
and	O
considering	O
klq	O
q	O
p	O
q	O
pd	O
log	O
pdm	O
the	O
non-negativity	O
of	O
the	O
kullback-leibler	B
divergence	B
gives	O
the	O
bound	B
log	O
pdm	O
q	O
pd	O
this	O
bound	B
holds	O
for	O
any	O
distribution	B
q	O
and	O
saturates	O
when	O
q	O
p	O
since	O
using	O
the	O
optimal	O
setting	O
is	O
assumed	O
computationally	O
intractable	O
the	O
idea	O
in	O
variational	O
bounding	O
is	O
to	O
choose	O
a	O
distribution	B
family	B
for	O
q	O
for	O
which	O
the	O
bound	B
is	O
computationally	O
tractable	O
example	O
factorised	B
or	O
gaussian	B
and	O
then	O
maximise	O
the	O
bound	B
with	O
respect	O
to	O
any	O
free	O
parameters	O
of	O
q	O
the	O
resulting	O
bound	B
then	O
can	O
be	O
used	O
as	O
a	O
surrogate	O
for	O
the	O
exact	O
marginal	B
likelihood	B
in	O
model	B
comparison	O
gaussian	B
approximations	O
using	O
kl	O
divergence	B
minimising	O
klqp	O
using	O
a	O
simple	O
approximation	B
qx	O
of	O
a	O
more	O
complex	O
distribution	B
px	O
by	O
minimising	O
klqp	O
tends	O
to	O
give	O
a	O
solution	O
for	O
qx	O
that	O
focuses	O
on	O
a	O
local	B
mode	B
of	O
px	O
thereby	O
underestimating	O
the	O
variance	B
of	O
px	O
to	O
show	O
this	O
consider	O
approximating	O
a	O
mixture	B
of	O
two	O
gaussians	O
with	O
equal	O
variance	B
n	O
n	O
m	O
px	O
qx	O
n	O
see	O
with	O
a	O
single	O
gaussian	B
we	O
wish	O
to	O
find	O
the	O
optimal	O
m	O
that	O
minimise	O
klqp	O
if	O
we	O
consider	O
the	O
case	O
that	O
the	O
two	O
gaussian	B
components	O
of	O
px	O
are	O
well	O
separated	O
then	O
setting	O
qx	O
to	O
be	O
centred	O
on	O
the	O
left	O
mode	B
at	O
the	O
gaussian	B
qx	O
only	O
has	O
appreciable	O
mass	O
close	O
to	O
so	O
that	O
the	O
second	O
mode	B
at	O
has	O
negligible	O
contribution	O
to	O
the	O
kullback-leibler	B
divergence	B
in	O
this	O
sense	O
one	O
can	O
approximate	B
px	O
qx	O
so	O
that	O
klqp	O
log	O
draft	O
march	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	B
representing	O
a	O
distribution	B
of	O
the	O
form	O
figure	O
a	O
planar	O
pairwise	B
markov	B
random	B
field	I
on	O
a	O
set	O
of	O
variables	O
in	O
statistical	O
physics	O
such	O
lattice	O
models	O
include	O
the	O
ising	B
model	B
on	O
binary	O
spin	O
variables	O
xi	O
with	O
xj	O
ewij	O
xixj	O
i	O
j	O
xj	O
on	O
the	O
other	O
hand	O
setting	O
m	O
which	O
is	O
the	O
correct	O
mean	B
of	O
the	O
distribution	B
px	O
very	O
little	O
of	O
the	O
mass	O
of	O
the	O
mixture	B
is	O
captured	O
unless	O
is	O
large	O
giving	O
a	O
poor	O
fit	O
and	O
large	O
kl	O
divergence	B
another	O
way	O
to	O
view	O
this	O
is	O
to	O
consider	O
klqp	O
provided	O
q	O
is	O
close	O
to	O
p	O
around	O
where	O
q	O
has	O
significant	O
mass	O
the	O
ratio	O
qxpx	O
will	O
be	O
order	O
and	O
the	O
kl	O
divergence	B
small	O
setting	O
m	O
means	O
that	O
qxpx	O
is	O
large	O
where	O
q	O
has	O
significant	O
mass	O
and	O
is	O
therefore	O
a	O
poor	O
fit	O
the	O
optimal	O
solution	O
in	O
this	O
case	O
is	O
to	O
place	O
the	O
gaussian	B
close	O
to	O
a	O
single	O
mode	B
note	O
however	O
that	O
for	O
two	O
modes	O
that	O
are	O
less	O
well-separated	O
the	O
optimal	O
solution	O
will	O
not	O
necessarily	O
be	O
to	O
place	O
the	O
gaussian	B
around	O
a	O
local	B
mode	B
in	O
general	O
the	O
optimal	O
gaussian	B
fit	O
needs	O
to	O
be	O
determined	O
numerically	O
that	O
is	O
there	O
is	O
no	O
closed	O
form	O
solution	O
to	O
finding	O
the	O
optimal	O
mean	B
and	O
parameters	O
minimising	O
klpq	O
for	O
fitting	O
a	O
gaussian	B
q	O
to	O
p	O
based	O
on	O
klpq	O
we	O
have	O
log	O
const	O
klpq	O
px	O
px	O
minimising	O
this	O
with	O
respect	O
to	O
m	O
and	O
we	O
obtain	O
m	O
so	O
that	O
the	O
optimal	O
gaussian	B
fit	O
matches	O
the	O
first	O
and	O
second	O
moments	O
of	O
px	O
in	O
the	O
case	O
of	O
the	O
mean	B
of	O
px	O
is	O
a	O
zero	O
and	O
the	O
variance	B
of	O
px	O
is	O
large	O
this	O
solution	O
is	O
therefore	O
dramatically	O
different	O
from	O
that	O
produced	O
by	O
fitting	O
the	O
gaussian	B
using	O
klqp	O
the	O
fit	O
found	O
using	O
klqp	O
focusses	O
on	O
making	O
q	O
fit	O
p	O
locally	O
well	O
whereas	O
klpq	O
focusses	O
on	O
making	O
q	O
fit	O
p	O
well	O
to	O
the	O
global	B
statistics	O
of	O
the	O
distribution	B
at	O
the	O
expense	O
of	O
a	O
good	O
local	B
match	O
for	O
simplicity	O
consider	O
a	O
factorised	B
approximation	B
qx	O
moment	O
matching	O
properties	B
of	O
minimising	O
klpq	O
i	O
qxi	O
then	O
the	O
first	O
entropic	O
term	O
is	O
independent	O
of	O
qx	O
so	O
that	O
up	O
to	O
a	O
constant	O
independent	O
of	O
qx	O
the	O
above	O
is	O
i	O
klpq	O
klpxiqxi	O
i	O
so	O
that	O
optimally	O
qxi	O
pxi	O
that	O
is	O
the	O
optimal	O
factorised	B
approximation	B
is	O
to	O
set	O
the	O
factors	O
of	O
qxi	O
to	O
the	O
marginals	O
of	O
pxi	O
for	O
any	O
approximating	O
distribution	B
in	O
the	O
exponential	B
family	B
minimising	O
klpq	O
corresponds	O
to	O
moment	O
matching	O
see	O
in	O
practice	O
one	O
generally	O
cannot	O
compute	O
the	O
moments	O
of	O
px	O
the	O
distribution	B
px	O
is	O
considered	O
intractable	O
so	O
that	O
fitting	O
q	O
to	O
p	O
based	O
only	O
on	O
klpq	O
does	O
not	O
itself	O
lead	O
to	O
a	O
practical	O
algorithm	B
for	O
approximate	B
inference	B
nevertheless	O
as	O
we	O
will	O
see	O
it	O
is	O
a	O
useful	O
subroutine	O
for	O
local	B
approximations	O
in	O
particular	O
expectation	B
propagation	B
draft	O
march	O
variational	O
bounding	O
using	O
klqp	O
variational	O
bounding	O
using	O
klqp	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
fit	O
a	O
distribution	B
qx	O
from	O
some	O
assumed	O
family	B
to	O
an	O
intractable	O
distribution	B
px	O
as	O
we	O
saw	O
above	O
for	O
the	O
case	O
of	O
fitting	O
gaussians	O
the	O
optimal	O
q	O
needs	O
to	O
be	O
found	O
numerically	O
this	O
itself	O
can	O
be	O
a	O
complex	O
task	O
formally	O
this	O
can	O
be	O
just	O
as	O
difficult	O
as	O
performing	O
inference	B
directly	O
with	O
the	O
intractable	O
p	O
and	O
the	O
reader	O
may	O
wonder	O
why	O
we	O
trade	O
a	O
difficult	O
inference	B
task	O
for	O
a	O
potentially	O
difficult	O
optimisation	B
problem	B
the	O
general	O
idea	O
is	O
that	O
the	O
optimisation	B
problem	B
has	O
some	O
local	B
smoothness	B
properties	B
that	O
enable	O
one	O
to	O
rapidly	O
find	O
a	O
reasonable	O
optimum	O
based	O
on	O
generic	O
optimisation	B
methods	O
to	O
make	O
these	O
ideas	O
more	O
concrete	O
we	O
discuss	O
a	O
particular	O
case	O
of	O
fitting	O
q	O
to	O
a	O
formally	O
intractable	O
p	O
in	O
below	O
pairwise	B
markov	B
random	B
field	I
a	O
canonical	B
intractable	O
distribution	B
is	O
the	O
pairwise	B
markov	B
random	B
field	I
defined	O
on	O
binary	O
variables	O
xi	O
i	O
d	O
px	O
ij	O
wij	O
xixj	O
zw	O
b	O
e	O
ij	O
wij	O
xixj	O
zw	O
b	O
e	O
i	O
bixi	O
i	O
bixi	O
here	O
the	O
partition	B
function	B
zw	O
b	O
ensures	O
normalisation	B
x	O
i	O
the	O
terms	O
i	O
are	O
constant	O
and	O
without	O
loss	O
of	O
generality	O
we	O
may	O
set	O
wii	O
to	O
this	O
since	O
gives	O
an	O
undirected	B
distribution	B
with	O
connection	O
geometry	O
defined	O
by	O
the	O
weights	O
w	O
in	O
practice	O
the	O
weights	O
often	O
define	O
local	B
interactions	O
on	O
a	O
lattice	O
see	O
a	O
case	O
for	O
which	O
inference	B
in	O
this	O
model	B
is	O
required	O
is	O
given	O
in	O
pyx	O
i	O
px	O
e	O
example	O
image	B
denoising	I
consider	O
a	O
binary	O
image	O
where	O
x	O
describes	O
the	O
state	O
of	O
the	O
clean	O
pixels	O
encoding	O
we	O
assume	O
a	O
noisy	O
pixel	O
generating	O
process	O
that	O
takes	O
each	O
clean	O
pixel	O
xi	O
and	O
flips	O
its	O
binary	O
state	O
pyixi	O
pyixi	O
e	O
yixi	O
the	O
probability	O
that	O
yi	O
and	O
xi	O
are	O
in	O
the	O
same	O
state	O
is	O
e	O
e	O
our	O
interest	O
is	O
to	O
the	O
posterior	B
distribution	B
on	O
clean	O
pixels	O
pxy	O
in	O
order	O
to	O
do	O
this	O
we	O
need	O
to	O
make	O
an	O
assumption	O
as	O
to	O
what	O
clean	O
images	O
look	O
like	O
we	O
do	O
this	O
using	O
a	O
mrf	O
i	O
j	O
wij	O
xixj	O
for	O
some	O
settings	O
of	O
wij	O
with	O
j	O
indicating	O
that	O
i	O
and	O
j	O
are	O
neighbours	O
this	O
encodes	O
the	O
assumption	O
that	O
clean	O
images	O
tend	O
to	O
have	O
neighbouring	O
pixels	O
in	O
the	O
same	O
state	O
an	O
isolated	O
pixel	O
in	O
a	O
different	O
state	O
to	O
its	O
neighbours	O
is	O
unlikely	O
under	O
this	O
prior	B
we	O
now	O
have	O
the	O
joint	B
distribution	B
px	O
y	O
pyixi	O
i	O
see	O
from	O
which	O
the	O
posterior	B
is	O
given	O
by	O
pxy	O
pyxpx	O
x	O
pyxpx	O
e	O
i	O
j	O
wij	O
xixj	O
i	O
yixi	O
quantities	O
such	O
as	O
the	O
map	B
state	O
a	O
posteriori	O
probable	O
image	O
marginals	O
pxiy	O
and	O
the	O
normalisation	B
constant	I
are	O
of	O
interest	O
inference	B
with	O
a	O
general	O
mrf	O
is	O
formally	O
computationally	O
intractable	O
exact	O
polynomial	O
time	O
methods	O
are	O
known	O
two	O
celebrated	O
results	O
that	O
we	O
mention	O
in	O
passing	B
are	O
that	O
for	O
the	O
planar	O
mrf	O
model	B
with	O
pure	O
interactions	O
the	O
partition	B
function	B
is	O
computable	O
in	O
polynomial	O
as	O
is	O
the	O
map	B
state	O
for	O
attractive	O
planar	O
ising	O
models	O
w	O
see	O
draft	O
march	O
variational	O
bounding	O
using	O
klqp	O
figure	O
a	O
distribution	B
on	O
pixels	O
the	O
filled	O
nodes	O
indicate	O
observed	O
noisy	O
pixels	O
the	O
unshaded	O
nodes	O
a	O
markov	B
random	B
field	I
on	O
latent	B
clean	O
pixels	O
the	O
task	O
is	O
to	O
infer	O
the	O
clean	O
pixels	O
given	O
the	O
noisy	O
pixels	O
the	O
mrf	O
encourages	O
the	O
posterior	B
distribution	B
on	O
the	O
clean	O
pixels	O
to	O
contain	O
neighbouring	O
pixels	O
in	O
the	O
same	O
state	O
kullback-leibler	O
based	O
methods	O
for	O
the	O
mrf	O
we	O
have	O
klqp	O
bi	O
log	O
z	O
rewriting	O
this	O
gives	O
a	O
bound	B
on	O
the	O
log-partition	O
function	B
ij	O
wij	O
wij	O
ij	O
i	O
i	O
energy	B
bi	O
log	O
z	O
entropy	B
the	O
bound	B
saturates	O
when	O
q	O
p	O
however	O
this	O
is	O
of	O
little	O
help	O
since	O
we	O
cannot	O
compute	O
the	O
averages	O
of	O
variables	O
with	O
respect	O
to	O
this	O
intractable	O
distribution	B
the	O
idea	O
of	O
a	O
variational	O
method	O
is	O
to	O
assume	O
a	O
simpler	O
tractable	O
distribution	B
q	O
for	O
which	O
these	O
averages	O
can	O
be	O
computed	O
along	O
with	O
the	O
entropy	B
of	O
q	O
minimising	O
the	O
kl	O
divergence	B
with	O
respect	O
to	O
any	O
free	O
parameters	O
of	O
qx	O
is	O
then	O
equivalent	B
to	O
maximising	O
the	O
lower	O
bound	B
on	O
the	O
log	O
partition	B
function	B
factorised	B
approximation	B
a	O
simple	O
naive	O
assumption	O
is	O
the	O
fully	O
factorised	B
distribution	B
qx	O
qixi	O
i	O
the	O
graphical	O
model	B
of	O
this	O
approximation	B
is	O
given	O
in	O
in	O
this	O
case	O
wij	O
ij	O
bi	O
i	O
log	O
z	O
for	O
a	O
factorised	B
distribution	B
and	O
bearing	O
in	O
mind	O
that	O
xi	O
i	O
i	O
j	O
i	O
j	O
for	O
a	O
binary	O
variable	O
one	O
may	O
use	O
the	O
convenient	O
parametrization	O
qixi	O
e	O
i	O
e	O
i	O
e	O
i	O
so	O
that	O
qxi	O
qxi	O
tanh	O
i	O
qx	O
tion	O
tion	O
figure	O
naive	B
mean	B
field	I
approximation	B
a	O
spanning	B
tree	B
approximac	O
a	O
decomposable	B
approxima	O
i	O
qixi	O
draft	O
march	O
variational	O
bounding	O
using	O
klqp	O
this	O
gives	O
the	O
following	O
lower	O
bound	B
on	O
the	O
log	O
partition	B
function	B
wij	O
tanh	O
i	O
tanh	O
j	O
i	O
i	O
h	O
i	O
i	O
e	O
i	O
e	O
i	O
tanh	O
i	O
log	O
z	O
b	O
h	O
i	O
log	O
bi	O
tanh	O
i	O
where	O
h	O
i	O
is	O
the	O
binary	B
entropy	B
of	O
a	O
distribution	B
parameterised	O
according	O
to	O
equation	B
finding	O
the	O
best	O
factorised	B
approximation	B
in	O
the	O
minimal	O
kullback-liebler	O
divergence	B
sense	O
then	O
corresponds	O
to	O
maximising	O
the	O
bound	B
b	O
with	O
respect	O
to	O
the	O
variational	O
parameters	O
the	O
bound	B
b	O
equation	B
is	O
generally	O
non-convex	O
in	O
and	O
riddled	O
with	O
local	B
optima	O
finding	O
the	O
globally	O
optimal	O
is	O
therefore	O
typically	O
a	O
computationally	O
hard	B
problem	B
it	O
seems	O
that	O
we	O
have	O
simply	O
replaced	O
a	O
computationally	O
hard	B
problem	B
of	O
computing	O
log	O
z	O
by	O
an	O
equally	O
hard	B
computational	O
problem	B
of	O
maximising	O
b	O
indeed	O
the	O
graphical	O
structure	B
of	O
this	O
optimisation	B
problem	B
matches	O
exactly	O
that	O
of	O
the	O
original	O
mrf	O
however	O
the	O
hope	O
is	O
that	O
by	O
transforming	O
a	O
difficult	O
discrete	B
summation	O
into	O
a	O
continuous	B
optimisation	B
problem	B
we	O
will	O
be	O
able	O
to	O
bring	O
to	O
the	O
table	O
techniques	O
of	O
continuous	B
variable	O
numerical	B
optimisation	B
to	O
find	O
a	O
good	O
approximation	B
a	O
particularly	O
simple	O
optimisation	B
technique	O
is	O
to	O
differentiate	O
the	O
bound	B
equation	B
and	O
equate	O
to	O
zero	O
straightforward	O
algebra	O
leads	O
to	O
requirement	O
that	O
the	O
optimal	O
solution	O
satisfies	O
the	O
equations	O
wij	O
tanh	O
j	O
i	O
i	O
bi	O
one	O
may	O
show	O
that	O
updating	O
any	O
i	O
according	O
to	O
equation	B
increases	O
b	O
this	O
is	O
called	O
asychronous	B
updating	I
and	O
is	O
guaranteed	O
to	O
lead	O
to	O
a	O
minimum	O
of	O
the	O
kl	O
divergence	B
once	O
a	O
converged	O
solution	O
has	O
been	O
identified	O
in	O
addition	O
to	O
a	O
bound	B
on	O
log	O
z	O
we	O
can	O
approximate	B
tanh	O
i	O
validity	O
of	O
the	O
factorised	B
approximation	B
when	O
might	O
one	O
expect	O
such	O
a	O
naive	O
factorised	B
approximation	B
to	O
work	O
well	O
clearly	O
if	O
wij	O
is	O
very	O
small	O
for	O
i	O
j	O
the	O
distribution	B
p	O
will	O
be	O
effectively	O
factorised	B
however	O
a	O
more	O
interesting	O
case	O
is	O
when	O
each	O
variable	O
xi	O
has	O
many	O
neighbours	O
it	O
is	O
useful	O
to	O
write	O
the	O
mrf	O
as	O
px	O
ij	O
wij	O
xixj	O
z	O
i	O
xi	O
d	O
j	O
wij	O
xj	O
z	O
i	O
xizi	O
where	O
the	O
local	B
fields	O
are	O
defined	O
as	O
d	O
zi	O
wijxj	O
j	O
zi	O
each	O
of	O
the	O
terms	O
xj	O
in	O
the	O
summation	O
we	O
now	O
invoke	O
a	O
circular	O
self-consistent	O
argument	O
let	O
s	O
assume	O
that	O
px	O
is	O
factorised	B
then	O
for	O
j	O
wijxj	O
is	O
independent	O
provided	O
the	O
wij	O
are	O
not	O
strongly	O
correlated	O
the	O
conditions	O
of	O
validity	O
of	O
the	O
central	O
limit	O
theorem	B
and	O
each	O
zi	O
will	O
be	O
gaussian	B
distributed	O
assuming	O
that	O
each	O
wij	O
is	O
o	O
the	O
mean	B
of	O
zi	O
is	O
the	O
variance	B
is	O
i	O
wij	O
o	O
ik	O
draft	O
march	O
o	O
e	O
z	O
j	O
d	O
variational	O
bounding	O
using	O
klqp	O
hence	O
the	O
variance	B
of	O
the	O
field	O
zi	O
is	O
much	O
smaller	O
than	O
its	O
mean	B
value	B
as	O
d	O
increases	O
the	O
fluctuations	O
around	O
the	O
mean	B
diminish	O
and	O
we	O
may	O
write	O
pxi	O
px	O
z	O
i	O
i	O
the	O
assumption	O
that	O
p	O
is	O
approximately	O
factorised	B
is	O
therefore	O
self-consistent	O
in	O
the	O
limit	O
of	O
mrfs	O
with	O
a	O
large	O
number	O
of	O
neighbours	O
hence	O
the	O
factorised	B
approximation	B
would	O
appear	O
to	O
be	O
reasonable	O
in	O
the	O
extreme	O
limits	O
a	O
very	O
weakly	O
connected	B
system	O
wij	O
or	O
a	O
large	O
densely	O
connected	B
system	O
the	O
fully	O
factorised	B
approximation	B
is	O
also	O
called	O
the	O
naive	B
mean	B
field	I
theory	I
since	O
for	O
the	O
mrf	O
case	O
it	O
assumes	O
that	O
we	O
can	O
replace	O
the	O
effect	O
of	O
the	O
neighbours	O
by	O
a	O
mean	B
of	O
the	O
field	O
at	O
each	O
site	O
general	O
mean	B
field	O
equations	O
for	O
a	O
general	O
intractable	O
distribution	B
px	O
on	O
discrete	B
or	O
continuous	B
x	O
the	O
kl	O
divergence	B
between	O
a	O
factorised	B
approximation	B
qx	O
and	O
px	O
is	O
isolating	O
the	O
dependency	O
of	O
the	O
above	O
on	O
a	O
single	O
factor	B
qxi	O
we	O
have	O
i	O
qxi	O
i	O
kl	O
i	O
qxipx	O
qxi	O
exp	O
qxj	O
qxj	O
qxj	O
qxi	O
up	O
to	O
a	O
normalisation	B
constant	I
this	O
is	O
therefore	O
the	O
kl	O
divergence	B
between	O
qxi	O
and	O
a	O
distribution	B
proportional	O
to	O
exp	O
so	O
that	O
the	O
optimal	O
setting	O
for	O
qxi	O
satisfies	O
these	O
are	O
known	O
as	O
the	O
mean-field	O
equations	O
and	O
define	O
a	O
new	O
approximation	B
factor	B
in	O
terms	O
of	O
the	O
previous	O
approximation	B
factors	O
note	O
that	O
if	O
the	O
normalisation	B
constant	I
of	O
px	O
is	O
unknown	O
this	O
presents	O
no	O
problem	B
since	O
this	O
constant	O
is	O
simply	O
absorbed	O
into	O
the	O
normalisation	B
of	O
the	O
factors	O
qxi	O
in	O
other	O
words	O
one	O
may	O
replace	O
px	O
with	O
the	O
unnormalised	O
p	O
in	O
equation	B
beginning	O
with	O
an	O
initial	O
randomly	O
chosen	O
set	O
of	O
distributions	O
qxi	O
the	O
mean-field	O
equations	O
are	O
iterated	O
until	O
convergence	O
asynchronous	B
updating	I
is	O
guaranteed	O
to	O
decrease	O
the	O
kl	O
divergence	B
at	O
each	O
stage	O
asynchronous	B
updating	I
guarantees	O
approximation	B
improvement	O
for	O
a	O
factorised	B
variational	O
approximation	B
equation	B
we	O
claim	O
that	O
each	O
update	O
equation	B
reduces	O
the	O
kullback-leibler	O
approximation	B
error	O
to	O
show	O
this	O
we	O
write	O
a	O
single	O
updated	O
distribution	B
as	O
the	O
joint	B
distribution	B
under	O
this	O
single	O
update	O
is	O
qnew	O
i	O
zi	O
qold	O
j	O
qnew	O
qnew	O
i	O
qold	O
j	O
klqnewp	O
kl	O
qoldp	O
using	O
klqnewp	O
qnew	O
i	O
i	O
our	O
interest	O
is	O
the	O
change	O
in	O
the	O
approximation	B
error	O
under	O
this	O
single	O
mean-field	O
update	O
log	O
qold	O
j	O
j	O
qold	O
qold	O
j	O
qnew	O
i	O
draft	O
march	O
variational	O
bounding	O
using	O
klqp	O
figure	O
a	O
toy	O
intractable	O
distribution	B
a	O
structured	B
singly-connected	B
approximation	B
qold	O
j	O
ziqnew	O
qold	O
i	O
log	O
qold	O
i	O
i	O
i	O
q	O
qold	O
i	O
i	O
q	O
qold	O
i	O
qold	O
j	O
qnew	O
i	O
i	O
i	O
q	O
i	O
i	O
and	O
defining	O
the	O
un-normalised	O
distribution	B
q	O
then	O
i	O
qnew	O
log	O
qold	O
i	O
i	O
i	O
log	O
zi	O
log	O
qold	O
i	O
i	O
q	O
i	O
log	O
zi	O
qold	O
kl	O
i	O
qnewp	O
i	O
kl	O
qoldp	O
hence	O
kl	O
qold	O
j	O
qold	O
i	O
so	O
that	O
updating	O
a	O
single	O
component	O
of	O
q	O
at	O
a	O
time	O
is	O
guaranteed	O
to	O
improve	O
the	O
approximation	B
note	O
that	O
this	O
result	O
is	O
quite	O
general	O
holding	O
for	O
any	O
distribution	B
px	O
in	O
the	O
case	O
of	O
a	O
markov	B
network	I
the	O
guaranteed	O
approximation	B
improvement	O
is	O
equivalent	B
to	O
a	O
guaranteed	O
increase	O
speaking	O
a	O
non-decrease	O
in	O
the	O
lower	O
bound	B
on	O
the	O
partition	B
function	B
intractable	B
energy	B
whilst	O
the	O
fully	O
factorised	B
approximation	B
is	O
rather	O
severe	O
even	O
this	O
may	O
not	O
be	O
enough	O
to	O
render	O
the	O
qxj	O
for	O
field	O
equations	O
tractably	O
implementable	O
to	O
do	O
so	O
we	O
need	O
to	O
be	O
able	O
to	O
compute	O
p	O
some	O
models	O
of	O
interest	O
this	O
is	O
still	O
not	O
possible	O
and	O
additional	O
approximations	O
are	O
required	O
example	O
intractable	O
mean-field	O
approximation	B
consider	O
the	O
posterior	B
distribution	B
from	O
a	O
relevance	B
vector	I
machine	I
classification	B
problem	B
pwd	O
n	O
the	O
terms	O
render	O
pwd	O
non-gaussian	O
we	O
can	O
find	O
a	O
gaussian	B
approximation	B
qw	O
n	O
n	O
by	O
minimising	O
the	O
kullback-leibler	B
divergence	B
klqwpwd	O
log	O
n	O
the	O
entropic	O
term	O
is	O
straightforward	O
since	O
this	O
is	O
the	O
negative	O
entropy	B
of	O
a	O
gaussian	B
however	O
we	O
also	O
require	O
the	O
energy	B
which	O
includes	O
a	O
contribution	O
there	O
is	O
no	O
closed	O
form	O
expression	O
for	O
this	O
one	O
approach	B
is	O
to	O
use	O
additional	O
variational	O
approximations	O
another	O
approach	B
is	O
to	O
recognise	O
that	O
the	O
multi-variate	B
average	B
can	O
be	O
reduced	O
to	O
a	O
uni-variate	O
gaussian	B
average	B
n	O
m	O
m	O
txn	O
xn	O
log	O
draft	O
march	O
mutual	B
information	B
maximisation	B
a	O
kl	O
variational	B
approach	B
and	O
the	O
uni-variate	O
gaussian	B
average	B
can	O
be	O
carried	O
out	O
using	O
quadrature	O
this	O
approach	B
was	O
used	O
in	O
to	O
approximate	B
the	O
posterior	B
distribution	B
of	O
bayesian	B
neural	O
networks	O
structured	B
variational	O
approximation	B
one	O
can	O
extend	O
the	O
factorised	B
kl	O
variational	O
approximation	B
by	O
using	O
non-factorised	O
those	O
for	O
which	O
averages	O
of	O
the	O
variables	O
can	O
be	O
computed	O
in	O
linear	B
time	O
include	O
spanning	O
trees	O
and	O
decomposable	B
graphs	O
for	O
example	O
for	O
the	O
distribution	B
z	O
a	O
tractable	O
q	O
distribution	B
would	O
be	O
z	O
in	O
this	O
case	O
we	O
have	O
klqp	O
i	O
j	O
since	O
q	O
is	O
singly-connected	B
computing	O
the	O
marginals	O
and	O
entropy	B
is	O
straightforward	O
the	O
entropy	B
requires	O
only	O
pairwise	B
marginals	O
on	O
graph	B
neighbours	O
more	O
generally	O
one	O
can	O
exploit	O
any	O
structural	O
approximation	B
with	O
an	O
arbitrary	O
hypertree	O
width	O
w	O
by	O
use	O
of	O
the	O
junction	B
tree	B
algorithm	B
in	O
combination	O
with	O
the	O
kl	O
divergence	B
however	O
the	O
computational	O
expense	O
increases	O
exponentially	O
with	O
the	O
hypertree	O
mutual	B
information	B
maximisation	B
a	O
kl	O
variational	B
approach	B
here	O
we	O
take	O
a	O
short	O
interlude	O
here	O
to	O
demonstrate	O
an	O
application	O
of	O
the	O
kullback-leibler	O
variational	B
approach	B
in	O
information	O
theory	O
a	O
fundamental	O
goal	O
is	O
to	O
maximise	O
information	O
transfer	O
measured	O
by	O
the	O
also	O
ix	O
y	O
hx	O
hxy	O
where	O
the	O
and	O
are	O
defined	O
hx	O
hxy	O
here	O
we	O
are	O
interested	O
in	O
the	O
situation	O
in	O
which	O
px	O
is	O
fixed	O
but	O
pyx	O
has	O
adjustable	O
parameters	O
that	O
we	O
wish	O
to	O
set	O
in	O
order	O
to	O
maximise	O
ix	O
y	O
in	O
this	O
case	O
hx	O
is	O
constant	O
and	O
the	O
optimisation	B
problem	B
is	O
equivalent	B
to	O
minimising	O
the	O
conditional	B
entropy	B
hxy	O
unfortunately	O
in	O
many	O
cases	O
of	O
practical	O
interest	O
hxy	O
is	O
computationally	O
intractable	O
see	O
below	O
for	O
a	O
motivating	O
example	O
we	O
discuss	O
in	O
a	O
general	O
procedure	O
based	O
on	O
the	O
kullback-leibler	B
divergence	B
to	O
approximately	O
maximise	O
the	O
mutual	B
information	I
example	O
consider	O
a	O
neural	O
transmission	O
system	O
in	O
which	O
xi	O
denotes	O
an	O
emitting	O
neuron	O
in	O
a	O
non-firing	O
state	O
or	O
firing	O
state	O
and	O
yj	O
a	O
receiving	O
neuron	O
if	O
each	O
receiving	O
neuron	O
fires	O
independently	O
depending	O
only	O
on	O
the	O
emitting	O
neurons	O
we	O
have	O
pyix	O
draft	O
march	O
pyx	O
i	O
mutual	B
information	B
maximisation	B
a	O
kl	O
variational	B
approach	B
algorithm	B
im	O
algorithm	B
bution	O
px	O
i	O
find	O
the	O
optimal	O
parameters	O
wi	O
that	O
maximise	O
the	O
mutual	O
figure	O
an	O
information	O
transfer	O
problem	B
for	O
a	O
fixed	O
distrii	O
pxi	O
and	O
parameterised	O
distributions	O
pyjx	O
information	O
between	O
the	O
variables	O
x	O
and	O
y	O
such	O
considerations	O
are	O
popular	O
in	O
theoretical	O
neuroscience	O
and	O
aim	O
to	O
understand	O
how	O
the	O
receptive	O
fields	O
wi	O
of	O
a	O
neuron	O
relate	O
to	O
the	O
statistics	O
of	O
the	O
environment	O
px	O
choose	O
a	O
class	O
of	O
approximating	O
distributions	O
q	O
example	O
factorised	B
initialise	O
the	O
parameters	O
repeat	O
for	O
fixed	O
qxy	O
find	O
new	O
argmax	O
ix	O
y	O
for	O
fixed	O
qnewxy	O
argmax	O
qxy	O
q	O
ix	O
y	O
where	O
q	O
is	O
a	O
chosen	O
class	O
of	O
distributions	O
until	O
converged	O
where	O
for	O
example	O
we	O
could	O
use	O
pyi	O
wt	O
i	O
x	O
px	O
pxi	O
if	O
we	O
make	O
the	O
simple	O
assumption	O
that	O
emitting	O
neurons	O
fire	O
independently	O
i	O
then	O
for	O
pxy	O
all	O
components	O
of	O
the	O
x	O
variable	O
are	O
dependent	O
see	O
this	O
defines	O
a	O
complex	O
high-dimensional	O
px	O
y	O
for	O
which	O
the	O
conditional	B
entropy	B
is	O
typically	O
intractable	O
the	O
information	B
maximisation	B
algorithm	B
consider	O
this	O
immediately	O
gives	O
a	O
bound	B
klpxyqxy	O
pxy	O
log	O
pxy	O
x	O
x	O
multiplying	O
both	O
sides	O
by	O
py	O
we	O
obtain	O
pxy	O
log	O
qxy	O
xy	O
pypxy	O
log	O
pxy	O
px	O
y	O
log	O
qxy	O
xy	O
from	O
the	O
definition	O
the	O
left	O
of	O
the	O
above	O
is	O
hxy	O
hence	O
ix	O
y	O
hx	O
ix	O
y	O
draft	O
march	O
loopy	B
belief	B
propagation	B
c	O
a	O
e	O
d	O
b	O
f	O
figure	O
classical	O
belief	B
propagation	B
can	O
be	O
derived	O
by	O
considering	O
how	O
to	O
compute	O
the	O
marginal	B
of	O
a	O
variable	O
on	O
a	O
mrf	O
in	O
this	O
case	O
the	O
marginal	B
pd	O
depends	O
on	O
messages	O
transmitted	O
via	O
the	O
neighbours	O
of	O
d	O
by	O
defining	O
local	B
messages	O
on	O
the	O
links	O
of	O
the	O
graph	B
a	O
recursive	O
algorithm	B
for	O
computing	O
all	O
marginals	O
can	O
be	O
derived	O
see	O
text	O
from	O
this	O
lower	O
bound	B
on	O
the	O
mutual	B
information	I
we	O
arrive	O
at	O
the	O
information	B
maximisation	B
given	O
a	O
distribution	B
px	O
and	O
a	O
parameterised	O
distribution	B
pyx	O
we	O
seek	O
to	O
maximise	O
ix	O
y	O
with	O
respect	O
to	O
a	O
co-ordinate	O
wise	O
optimisation	B
procedure	O
is	O
presented	O
in	O
the	O
blahut-arimoto	B
algorithm	B
in	O
information	O
theory	O
for	O
example	O
is	O
a	O
special	O
case	O
in	O
which	O
the	O
optimal	O
decoder	O
qxy	O
pyx	O
is	O
used	O
in	O
applications	O
where	O
the	O
blahut-arimoto	B
algorithm	B
is	O
intractable	O
to	O
implement	O
the	O
im	O
algorithm	B
can	O
provide	O
an	O
alternative	O
by	O
restricting	O
q	O
to	O
a	O
tractable	O
family	B
of	O
distributions	O
in	O
the	O
sense	O
that	O
the	O
lower	O
bound	B
can	O
be	O
computed	O
the	O
blahut-arimoto	B
algorithm	B
is	O
analogous	O
to	O
the	O
em	B
algorithm	B
for	O
maximum	B
likelihood	B
and	O
guarantees	O
a	O
non-decrease	O
of	O
the	O
mutual	B
information	I
at	O
each	O
stage	O
of	O
the	O
update	O
similarly	O
the	O
im	O
procedure	O
is	O
analogous	O
to	O
a	O
generalised	B
em	B
procedure	O
and	O
each	O
step	O
of	O
the	O
procedure	O
cannot	O
decrease	O
the	O
lower	O
bound	B
on	O
the	O
mutual	B
information	I
linear	B
gaussian	B
decoder	O
a	O
special	O
case	O
of	O
the	O
im	O
framework	O
is	O
to	O
use	O
a	O
linear	B
gaussian	B
decoder	O
qxy	O
n	O
uy	O
log	O
qxy	O
uyt	O
uy	O
plugging	O
this	O
into	O
the	O
mi	O
bound	B
equation	B
and	O
optimising	O
with	O
respect	O
to	O
and	O
u	O
we	O
obtain	O
uy	O
ix	O
y	O
hx	O
log	O
det	O
u	O
where	O
using	O
this	O
in	O
the	O
mi	O
bound	B
we	O
obtain	O
const	O
up	O
to	O
irrelevant	O
constants	O
this	O
is	O
equivalent	B
to	O
linsker	O
s	O
as-if-gaussian	O
approximation	B
to	O
the	O
mutual	B
information	I
one	O
can	O
therefore	O
view	O
linsker	O
s	O
approach	B
as	O
a	O
special	O
case	O
of	O
the	O
im	O
algorithm	B
restricted	B
to	O
linear-gaussian	O
decoders	O
in	O
principle	O
one	O
can	O
therefore	O
improve	O
on	O
linsker	O
s	O
method	O
by	O
considering	O
more	O
powerful	O
non-linear-gaussian	O
decoders	O
applications	O
of	O
this	O
technique	O
to	O
neural	O
systems	O
are	O
discussed	O
in	O
loopy	B
belief	B
propagation	B
belief	B
propagation	B
is	O
a	O
technique	O
for	O
exact	O
inference	B
of	O
marginals	O
pxi	O
for	O
singly-connected	B
distributions	O
px	O
there	O
are	O
different	O
formulations	O
of	O
bp	O
the	O
most	O
modern	O
treatment	O
being	O
the	O
sum-product	B
algorithm	B
on	O
the	O
corresponding	O
factor	B
graph	B
as	O
described	O
in	O
an	O
important	O
observation	O
is	O
that	O
the	O
algorithm	B
is	O
purely	O
local	B
the	O
updates	O
are	O
unaware	O
of	O
the	O
global	B
structure	B
of	O
the	O
graph	B
depending	O
only	O
on	O
the	O
local	B
neighbourhood	O
structure	B
this	O
means	O
that	O
even	O
if	O
the	O
graph	B
is	O
multiply-connected	B
is	O
loopy	B
one	O
can	O
still	O
apply	O
the	O
algorithm	B
and	O
see	O
what	O
happens	O
provided	O
the	O
loops	O
in	O
the	O
graph	B
are	O
relatively	O
long	O
one	O
may	O
hope	O
that	O
running	O
loopy	B
bp	O
will	O
converge	O
to	O
a	O
good	O
approximation	B
of	O
the	O
true	O
marginals	O
in	O
general	O
this	O
cannot	O
be	O
guaranteed	O
however	O
when	O
the	O
method	O
converges	O
the	O
results	O
can	O
be	O
surprisingly	O
accurate	O
in	O
the	O
following	O
we	O
will	O
show	O
how	O
loopy	B
bp	O
can	O
also	O
be	O
motivated	O
by	O
a	O
variational	O
objective	O
to	O
do	O
so	O
the	O
most	O
natural	B
connection	O
is	O
with	O
the	O
classical	O
bp	O
algorithm	B
than	O
the	O
factor	B
graph	B
sum-product	B
algorithm	B
for	O
this	O
reason	O
we	O
briefly	O
describe	O
below	O
the	O
classical	O
bp	O
approach	B
draft	O
march	O
loopy	B
belief	B
propagation	B
x	O
x	O
i	O
x	O
i	O
xi	O
xi	O
xj	O
xj	O
xi	O
i	O
x	O
i	O
x	O
figure	O
loopy	B
belief	B
propagation	B
once	O
a	O
node	O
has	O
received	O
incoming	O
messages	O
from	O
all	O
neighbours	O
the	O
one	O
it	O
wants	O
to	O
send	O
a	O
message	B
to	O
it	O
may	O
send	O
an	O
outgoing	O
message	B
to	O
a	O
neighbour	B
xj	O
xi	O
xi	O
xi	O
xi	O
xj	O
xi	O
classical	O
bp	O
on	O
an	O
undirected	B
graph	B
graph	B
consider	O
calculating	O
the	O
marginal	B
pd	O
work	O
in	O
we	O
denote	O
both	O
a	O
node	O
and	O
its	O
state	O
by	O
the	O
same	O
symbol	O
so	O
that	O
bp	O
can	O
be	O
derived	O
by	O
considering	O
how	O
to	O
calculate	O
a	O
marginal	B
in	O
terms	O
of	O
messages	O
on	O
an	O
undirected	B
abcef	O
pa	O
b	O
c	O
d	O
e	O
f	O
for	O
the	O
pairwise	B
markov	O
netb	O
b	O
denotes	O
summation	O
over	O
the	O
states	O
of	O
the	O
variable	O
b	O
carrying	O
out	O
such	O
a	O
summation	O
results	O
in	O
a	O
message	B
b	O
d	O
containing	O
information	O
from	O
node	O
b	O
to	O
node	O
d	O
in	O
order	O
to	O
compute	O
the	O
summation	O
efficiently	O
we	O
distribute	O
summations	O
as	O
follows	O
pd	O
z	O
b	O
a	O
f	O
d	O
d	O
f	O
b	O
dd	O
a	O
dd	O
f	O
dd	O
e	O
c	O
e	O
dd	O
e	O
c	O
ee	O
where	O
we	O
define	O
messages	O
sending	O
information	O
from	O
node	O
to	O
node	O
as	O
a	O
function	B
of	O
the	O
state	O
of	O
node	O
in	O
general	O
a	O
node	O
xi	O
passes	O
a	O
message	B
to	O
node	O
xj	O
via	O
xi	O
xj	O
xj	O
xi	O
k	O
xk	O
xi	O
this	O
algorithm	B
is	O
equivalent	B
to	O
the	O
sum-product	B
algorithm	B
provided	O
the	O
graph	B
is	O
singly-connected	B
loopy	B
bp	O
as	O
a	O
variational	O
procedure	O
a	O
variational	O
procedure	O
that	O
corresponds	O
to	O
loopy	B
bp	O
can	O
be	O
derived	O
by	O
considering	O
the	O
terms	O
of	O
a	O
standard	O
variational	O
approximation	B
based	O
on	O
the	O
kullback-leibler	B
divergence	B
for	O
a	O
pairwise	B
mrf	O
defined	O
on	O
potentials	O
xj	O
and	O
approximating	O
distribution	B
qx	O
the	O
kullback-leibler	O
bound	B
is	O
i	O
j	O
px	O
z	O
xj	O
log	O
z	O
i	O
j	O
energy	B
since	O
each	O
contribution	O
to	O
the	O
energy	B
depends	O
on	O
qx	O
only	O
via	O
the	O
pairwise	B
marginals	O
qxi	O
xj	O
this	O
suggests	O
that	O
these	O
marginals	O
should	O
form	O
the	O
natural	B
parameters	O
of	O
any	O
approximation	B
can	O
we	O
then	O
find	O
an	O
expression	O
for	O
the	O
entropy	B
in	O
terms	O
of	O
these	O
pairwise	B
marginals	O
consider	O
a	O
case	O
in	O
which	O
the	O
required	O
marginals	O
are	O
draft	O
march	O
loopy	B
belief	B
propagation	B
given	O
these	O
marginals	O
the	O
energy	B
term	O
is	O
straightforward	O
to	O
compute	O
and	O
we	O
are	O
left	O
with	O
requiring	O
only	O
the	O
entropy	B
of	O
q	O
either	O
by	O
appealing	O
to	O
the	O
junction	B
tree	B
representation	O
or	O
by	O
straightforward	O
algebra	O
one	O
can	O
show	O
that	O
we	O
can	O
uniquely	O
express	O
q	O
in	O
terms	O
of	O
the	O
marginals	O
as	O
qx	O
an	O
intuitive	O
way	O
to	O
arrive	O
at	O
this	O
result	O
is	O
by	O
examining	O
the	O
numerator	O
of	O
equation	B
the	O
variable	O
appears	O
twice	O
as	O
does	O
the	O
variable	O
and	O
since	O
any	O
joint	B
distribution	B
cannot	O
have	O
replicated	O
variables	O
on	O
the	O
left	O
of	O
any	O
conditioning	B
we	O
must	O
compensate	O
for	O
the	O
additional	O
and	O
variables	O
by	O
dividing	O
by	O
these	O
marginals	O
in	O
this	O
case	O
the	O
entropy	B
of	O
qx	O
can	O
be	O
written	O
as	O
hqx	O
more	O
generally	O
any	O
decomposable	B
graph	B
can	O
be	O
represented	O
as	O
c	O
qxc	O
s	O
qxs	O
qx	O
where	O
the	O
qxc	O
are	O
the	O
marginals	O
defined	O
on	O
cliques	O
of	O
the	O
graph	B
with	O
xc	O
being	O
the	O
variables	O
of	O
the	O
clique	B
and	O
the	O
qxs	O
are	O
defined	O
on	O
the	O
separators	O
of	O
neighbouring	O
cliques	O
the	O
expression	O
for	O
the	O
entropy	B
of	O
the	O
distribution	B
is	O
then	O
given	O
by	O
a	O
sum	O
of	O
marginal	B
entropies	O
minus	O
the	O
entropy	B
of	O
the	O
marginals	O
defined	O
on	O
the	O
separators	O
bethe	B
free	I
energy	B
consider	O
now	O
a	O
mrf	O
corresponding	O
to	O
a	O
non-decomposable	O
graph	B
for	O
example	O
the	O
px	O
z	O
the	O
energy	B
requires	O
therefore	O
that	O
we	O
know	O
assuming	O
that	O
these	O
marginals	O
are	O
given	O
can	O
we	O
find	O
an	O
expression	O
for	O
the	O
entropy	B
of	O
the	O
joint	B
distribution	B
in	O
terms	O
of	O
its	O
pairwise	B
marginals	O
qxi	O
xj	O
in	O
general	O
this	O
is	O
not	O
possible	O
since	O
the	O
graph	B
corresponding	O
to	O
the	O
marginals	O
contains	O
loops	O
that	O
the	O
junction	B
tree	B
representation	O
would	O
result	O
in	O
cliques	O
greater	O
than	O
size	O
however	O
a	O
simple	O
no	O
overcounting	B
approximation	B
is	O
to	O
write	O
qx	O
subject	O
to	O
the	O
constraints	O
qxi	O
xj	O
qxj	O
xi	O
an	O
entropy	B
approximation	B
using	O
this	O
representation	O
is	O
therefore	O
hqx	O
hqxi	O
with	O
this	O
approximation	B
the	O
log	O
partition	B
function	B
is	O
known	O
in	O
statistical	O
physics	O
as	O
the	O
bethe	B
free	I
energy	B
our	O
interest	O
is	O
then	O
to	O
maximise	O
this	O
expression	O
with	O
respect	O
to	O
the	O
parameters	O
qxi	O
xj	O
xi	O
qxi	O
xj	O
qxj	O
these	O
may	O
be	O
enforced	O
using	O
lagrange	O
i	O
j	O
qxi	O
xi	O
ij	O
qxi	O
xj	O
draft	O
march	O
multipliers	O
one	O
can	O
write	O
the	O
bethe	B
free	I
energy	B
as	O
subject	O
to	O
marginal	B
consistency	O
hqxi	O
hqxi	O
xj	O
fq	O
i	O
j	O
i	O
i	O
j	O
loopy	B
belief	B
propagation	B
where	O
i	O
j	O
denotes	O
the	O
unique	O
neighbouring	O
edges	O
on	O
the	O
graph	B
edge	O
is	O
counted	O
only	O
once	O
this	O
is	O
no	O
longer	O
a	O
bound	B
on	O
the	O
log	O
partition	B
function	B
since	O
the	O
entropy	B
approximation	B
is	O
not	O
a	O
lower	O
bound	B
on	O
the	O
true	O
entropy	B
the	O
task	O
is	O
now	O
to	O
maximise	O
this	O
approximate	B
bound	B
with	O
respect	O
to	O
the	O
parameters	O
namely	O
all	O
the	O
pairwise	B
marginals	O
qxi	O
xj	O
and	O
the	O
lagrange	O
multipliers	O
a	O
simple	O
scheme	O
to	O
maximise	O
equation	B
is	O
to	O
use	O
a	O
fixed	O
point	O
iteration	B
by	O
equating	O
the	O
derivatives	O
of	O
the	O
bethe	B
free	I
energy	B
with	O
respect	O
to	O
the	O
parameters	O
qxi	O
xj	O
to	O
zero	O
and	O
likewise	O
for	O
the	O
lagrange	O
multipliers	O
one	O
may	O
show	O
that	O
the	O
resulting	O
set	O
of	O
fixed	O
point	O
equations	O
on	O
eliminating	O
q	O
is	O
equivalent	B
to	O
belief	O
for	O
which	O
in	O
general	O
a	O
node	O
xi	O
passes	O
a	O
message	B
to	O
node	O
xj	O
using	O
xi	O
xj	O
xj	O
xi	O
k	O
xk	O
xi	O
at	O
convergence	O
the	O
marginal	B
pxi	O
is	O
then	O
approximated	O
by	O
qxi	O
i	O
nej	O
xj	O
xi	O
the	O
prefactor	O
being	O
determined	O
by	O
normalisation	B
for	O
a	O
singly-connected	B
distribution	B
p	O
this	O
message	B
passing	B
scheme	O
converges	O
and	O
the	O
marginal	B
corresponds	O
to	O
the	O
exact	O
result	O
for	O
multiply	O
connected	B
structures	O
running	O
this	O
loopy	B
belief	B
propagation	B
will	O
generally	O
result	O
in	O
an	O
approximation	B
naturally	O
we	O
can	O
dispense	O
with	O
the	O
bethe	B
free	I
energy	B
if	O
desired	O
and	O
run	O
the	O
associated	O
loopy	B
belief	B
propagation	B
algorithm	B
directly	O
on	O
the	O
undirected	B
graph	B
the	O
convergence	O
of	O
loopy	B
belief	B
propagation	B
which	O
can	O
be	O
heavily	O
dependent	O
on	O
the	O
topology	O
of	O
the	O
graph	B
and	O
also	O
the	O
message	B
updating	O
the	O
potential	B
benefit	O
of	O
the	O
bethe	B
free	I
energy	B
viewpoint	O
is	O
that	O
it	O
gives	O
an	O
objective	O
that	O
is	O
required	O
to	O
be	O
optimised	O
opening	O
up	O
the	O
possibility	O
of	O
more	O
general	O
optimisation	B
techniques	O
than	O
bp	O
the	O
so-called	O
double-loop	O
techniques	O
iteratively	O
isolate	O
convex	O
contributions	O
to	O
the	O
bethe	B
free	I
energy	B
interleaved	O
with	O
concave	O
contributions	O
at	O
each	O
stage	O
the	O
resulting	O
optimisiations	O
can	O
be	O
carried	O
out	O
validity	O
of	O
loopy	B
belief	B
propagation	B
for	O
a	O
mrf	O
which	O
has	O
a	O
loop	O
computationally	O
this	O
means	O
that	O
a	O
perturbation	O
in	O
a	O
variable	O
on	O
the	O
loop	O
eventually	O
reverberates	O
to	O
the	O
same	O
variable	O
however	O
if	O
there	O
are	O
a	O
large	O
number	O
of	O
variables	O
in	O
the	O
loop	O
and	O
the	O
individual	O
neighbouring	O
links	O
are	O
not	O
all	O
extremely	O
strong	B
the	O
numerical	B
effect	O
of	O
the	O
loop	O
is	O
small	O
in	O
the	O
sense	O
that	O
influence	O
of	O
the	O
variable	O
on	O
itself	O
is	O
negligible	O
in	O
such	O
cases	O
one	O
would	O
expect	O
the	O
loopy	B
bp	O
approximation	B
to	O
be	O
accurate	O
an	O
area	O
of	O
particular	O
success	O
for	O
loopy	B
belief	B
propagation	B
inference	B
is	O
in	O
error	O
correction	O
based	O
on	O
low	O
density	B
parity	O
check	B
codes	O
which	O
are	O
designed	O
to	O
have	O
this	O
longloop	O
in	O
many	O
examples	O
of	O
practical	O
interest	O
example	O
an	O
mrf	O
with	O
nearest	B
neighbour	B
interactions	O
on	O
a	O
lattice	O
however	O
loops	O
can	O
be	O
very	O
short	O
in	O
such	O
cases	O
a	O
naive	O
implementation	O
of	O
loopy	B
bp	O
will	O
fail	O
a	O
natural	B
extension	O
is	O
to	O
cluster	O
variables	O
to	O
alleviate	O
some	O
of	O
the	O
issues	O
arising	O
from	O
strong	B
local	B
dependencies	O
this	O
technique	O
is	O
called	O
the	O
kikuchi	B
or	O
cluster	O
variation	O
more	O
elaborate	O
ways	O
of	O
clustering	B
variables	O
can	O
be	O
considered	O
using	O
region	O
example	O
the	O
file	O
demomfbpgibbs	O
m	O
compares	O
the	O
performance	B
of	O
naive	B
mean	B
field	I
theory	I
belief	B
propagation	B
and	O
unstructured	O
gibbs	B
sampling	B
on	O
marginal	B
inference	B
in	O
a	O
pairwise	B
markov	B
network	I
pw	O
x	O
y	O
z	O
wxw	O
x	O
wyw	O
y	O
wzw	O
z	O
xyx	O
y	O
xzx	O
z	O
yzy	O
z	O
in	O
which	O
all	O
variables	O
take	O
states	O
in	O
the	O
experiment	O
the	O
tables	O
are	O
selected	O
from	O
a	O
uniform	B
distribution	B
raised	O
to	O
a	O
power	O
for	O
close	O
to	O
zero	O
all	O
the	O
tables	O
are	O
essentially	O
flat	O
and	O
therefore	O
the	O
variables	O
become	O
independent	O
a	O
situation	O
for	O
which	O
mf	O
bp	O
and	O
gibbs	B
sampling	B
are	O
ideally	O
suited	O
as	O
is	O
increased	O
to	O
draft	O
march	O
expectation	B
propagation	B
the	O
markov	B
network	I
figure	O
that	O
we	O
wish	O
to	O
approximate	B
the	O
marginals	O
pw	O
px	O
py	O
pz	O
for	O
all	O
tables	O
are	O
drawn	O
from	O
a	O
uniform	B
distribution	B
raised	O
to	O
a	O
power	O
on	O
there	O
are	O
the	O
right	O
is	O
shown	O
the	O
naive	B
mean	B
field	I
approximation	B
factorised	B
structure	B
states	O
of	O
the	O
distribution	B
shown	O
is	O
a	O
randomly	O
sampled	O
distribution	B
for	O
which	O
has	O
many	O
isolated	O
peaks	O
suggesting	O
the	O
distribution	B
is	O
far	O
from	O
factorised	B
in	O
this	O
case	O
the	O
mf	O
and	O
gibbs	B
sampling	B
approxc	O
as	O
is	O
increased	O
to	O
typically	O
only	O
one	O
state	O
of	O
the	O
distribution	B
imations	O
may	O
perform	O
poorly	O
dominates	O
see	O
demomfbpgibbs	O
m	O
figure	O
multiply-connected	B
factor	B
expectation	B
graph	B
representing	O
px	O
propagation	B
approximates	O
in	O
terms	O
of	O
a	O
tractable	O
factor	B
graph	B
the	O
open	O
squares	O
indicate	O
that	O
the	O
factors	O
are	O
parameters	O
of	O
the	O
approximation	B
the	O
basic	O
ep	O
approximation	B
is	O
to	O
replace	O
all	O
factors	O
in	O
px	O
by	O
product	O
factors	O
tree	B
structured	B
ep	O
the	O
dependencies	O
amongst	O
the	O
variables	O
increase	O
and	O
the	O
methods	O
perform	O
worse	O
especially	O
mf	O
and	O
gibbs	B
as	O
is	O
increased	O
to	O
the	O
distribution	B
becomes	O
sharply	O
peaked	O
around	O
a	O
single	O
state	O
such	O
that	O
the	O
posterior	B
is	O
effectively	O
factorised	B
see	O
this	O
suggests	O
that	O
a	O
mf	O
approximation	B
also	O
gibbs	B
sampling	B
should	O
work	O
well	O
however	O
finding	O
this	O
state	O
is	O
computationally	O
difficult	O
and	O
both	O
methods	O
often	O
get	O
stuck	O
in	O
local	B
minima	O
belief	B
propagation	B
seems	O
less	O
susceptible	O
to	O
being	O
trapped	O
in	O
local	B
minima	O
in	O
this	O
regime	O
and	O
tends	O
to	O
outperform	O
both	O
mf	O
and	O
gibbs	B
sampling	B
expectation	B
propagation	B
the	O
messages	O
in	O
schemes	O
such	O
as	O
belief	B
propagation	B
are	O
not	O
always	O
representable	O
in	O
a	O
compact	O
form	O
the	O
switching	B
linear	B
dynamical	I
system	I
is	O
such	O
an	O
instance	O
in	O
which	O
the	O
messages	O
require	O
an	O
exponential	B
amount	O
of	O
storage	O
this	O
limits	O
bp	O
to	O
cases	O
such	O
as	O
discrete	B
networks	O
or	O
more	O
generally	O
exponential	B
family	B
messages	O
expectation	B
propagation	B
extends	O
the	O
applicability	O
of	O
bp	O
to	O
cases	O
in	O
which	O
the	O
messages	O
are	O
not	O
in	O
the	O
exponential	B
family	B
by	O
projecting	O
the	O
messages	O
back	O
to	O
the	O
exponential	B
family	B
at	O
each	O
stage	O
this	O
projection	B
is	O
obtained	O
by	O
using	O
a	O
kullback-leibler	O
consider	O
a	O
distribution	B
of	O
the	O
form	O
i	O
px	O
z	O
ix	O
in	O
ep	O
one	O
identifies	O
those	O
factors	O
ix	O
which	O
if	O
replaced	O
by	O
simpler	O
factors	O
ix	O
would	O
render	O
the	O
distribution	B
px	O
tractable	O
one	O
then	O
sets	O
any	O
free	O
parameters	O
of	O
ix	O
by	O
minimising	O
the	O
draft	O
march	O
wxyzoriginal	O
graphwxyzfactorised	O
distribution	B
expectation	B
propagation	B
leibler	O
divergence	B
klp	O
p	O
for	O
example	O
consider	O
a	O
pairwise	B
mrf	O
px	O
z	O
with	O
factor	B
graph	B
as	O
depicted	O
in	O
if	O
we	O
replace	O
all	O
terms	O
ijxi	O
xj	O
by	O
approximate	B
factors	O
ijxi	O
ijxj	O
then	O
the	O
resulting	O
joint	B
distribution	B
p	O
would	O
be	O
factorised	B
and	O
hence	O
tractable	O
since	O
the	O
variable	O
xi	O
appears	O
in	O
more	O
than	O
one	O
term	O
from	O
px	O
we	O
need	O
to	O
index	O
the	O
approximation	B
factors	O
a	O
convenient	O
way	O
to	O
do	O
this	O
is	O
p	O
z	O
which	O
is	O
represented	O
in	O
the	O
idea	O
in	O
ep	O
is	O
now	O
to	O
determine	O
the	O
optimal	O
approximation	B
term	O
by	O
the	O
self-consistent	O
requirement	O
that	O
on	O
replacing	O
it	O
with	O
its	O
exact	O
form	O
there	O
is	O
no	O
difference	O
to	O
the	O
marginal	B
of	O
p	O
consider	O
the	O
approximation	B
parameters	O
and	O
to	O
set	O
these	O
we	O
first	O
replace	O
the	O
contribution	O
by	O
this	O
gives	O
p	O
z	O
now	O
consider	O
the	O
kullback-leibler	B
divergence	B
between	O
this	O
distribution	B
and	O
our	O
approximation	B
kl	O
p	O
p	O
p	O
p	O
p	O
since	O
our	O
interest	O
is	O
in	O
updating	O
and	O
we	O
isolate	O
the	O
contribution	O
from	O
these	O
parameters	O
in	O
the	O
kullback-leibler	B
divergence	B
which	O
gives	O
kl	O
p	O
p	O
log	O
z	O
log	O
p	O
const	O
also	O
since	O
p	O
is	O
factorised	B
up	O
to	O
a	O
constant	O
proportionality	O
factor	B
the	O
dependence	O
of	O
z	O
on	O
and	O
is	O
z	O
differentiating	O
the	O
kullback-leibler	B
divergence	B
equation	B
with	O
respect	O
to	O
and	O
equating	O
to	O
zero	O
we	O
obtain	O
p	O
similarly	O
optimising	O
w	O
r	O
t	O
gives	O
p	O
these	O
equations	O
only	O
determine	O
the	O
approximation	B
factors	O
up	O
to	O
a	O
proportionality	O
constant	O
we	O
can	O
therefore	O
write	O
the	O
optimal	O
updates	O
as	O
p	O
and	O
p	O
draft	O
march	O
where	O
and	O
are	O
proportionality	O
terms	O
we	O
can	O
determine	O
the	O
proportionalities	O
by	O
the	O
requirement	O
that	O
the	O
term	O
approximation	B
has	O
the	O
same	O
effect	O
on	O
the	O
normalisation	B
of	O
p	O
as	O
it	O
has	O
expectation	B
propagation	B
on	O
p	O
that	O
which	O
on	O
substituting	O
in	O
the	O
updates	O
equation	B
and	O
equation	B
reduces	O
to	O
z	O
where	O
and	O
z	O
p	O
p	O
any	O
choice	O
of	O
local	B
normalisations	O
that	O
satisfies	O
equation	B
suffices	O
to	O
ensure	O
that	O
the	O
scale	O
of	O
the	O
term	O
approximation	B
matches	O
for	O
example	O
one	O
may	O
set	O
z	O
once	O
set	O
an	O
approximation	B
for	O
the	O
global	B
normalisation	B
constant	I
of	O
p	O
is	O
z	O
z	O
the	O
above	O
gives	O
a	O
procedure	O
for	O
updating	O
the	O
terms	O
and	O
one	O
then	O
chooses	O
another	O
term	O
and	O
replaces	O
it	O
with	O
its	O
approximation	B
until	O
the	O
parameters	O
of	O
the	O
approximation	B
converge	O
the	O
generic	O
procedure	O
is	O
outlined	O
in	O
comments	O
on	O
ep	O
for	O
the	O
mrf	O
example	O
above	O
ep	O
corresponds	O
to	O
belief	B
propagation	B
sum-product	B
form	O
on	O
the	O
factor	B
graph	B
this	O
is	O
intuitively	O
clear	O
since	O
in	O
both	O
ep	O
and	O
bp	O
the	O
product	O
of	O
messages	O
incoming	O
to	O
a	O
variable	O
is	O
proportional	O
to	O
the	O
approximation	B
of	O
the	O
marginal	B
of	O
that	O
variable	O
a	O
difference	O
however	O
is	O
the	O
schedule	B
in	O
ep	O
all	O
messages	O
corresponding	O
to	O
a	O
term	O
approximation	B
are	O
updated	O
simultaneously	O
the	O
above	O
and	O
whereas	O
in	O
bp	O
they	O
are	O
updated	O
in	O
arbitrary	O
order	O
ep	O
is	O
a	O
useful	O
extension	O
of	O
bp	O
to	O
cases	O
in	O
which	O
the	O
bp	O
messages	O
cannot	O
be	O
easily	O
represented	O
in	O
the	O
case	O
that	O
the	O
approximating	O
distribution	B
p	O
is	O
in	O
the	O
exponential	B
family	B
the	O
minimal	O
kullback-leibler	O
criterion	O
equates	O
to	O
matching	O
moments	O
of	O
the	O
approximating	O
distribution	B
to	O
p	O
see	O
for	O
a	O
more	O
detailed	O
discussion	O
in	O
general	O
there	O
is	O
no	O
need	O
to	O
replace	O
all	O
terms	O
in	O
the	O
joint	B
distribution	B
with	O
factorised	B
approximations	O
one	O
only	O
needs	O
that	O
the	O
resulting	O
approximating	O
distribution	B
is	O
tractable	O
this	O
results	O
in	O
a	O
structured	B
expectation	B
propagation	B
algorithm	B
see	O
ep	O
and	O
its	O
extensions	O
are	O
closely	O
related	O
to	O
other	O
variational	O
procedures	O
such	O
as	O
and	O
fractional	O
designed	O
to	O
compensate	O
for	O
message	B
overcounting	B
effects	O
draft	O
march	O
algorithm	B
expectation	B
propagation	B
approximation	B
of	O
px	O
z	O
decide	O
on	O
a	O
set	O
of	O
terms	O
ixi	O
to	O
replace	O
with	O
ixi	O
in	O
order	O
to	O
reveal	O
a	O
tractable	O
distribution	B
i	O
ixi	O
map	B
for	O
mrfs	O
i	O
px	O
z	O
ixi	O
initialise	O
the	O
all	O
parameters	O
ixi	O
repeat	O
select	O
a	O
term	O
ixi	O
from	O
p	O
to	O
update	O
replace	O
the	O
term	O
ixi	O
by	O
the	O
tractable	O
term	O
ixi	O
to	O
form	O
ixi	O
jxj	O
ixi	O
jxj	O
j	O
find	O
the	O
parameters	O
of	O
ixi	O
by	O
ixi	O
argmin	O
ixi	O
px	O
kl	O
p	O
p	O
jxj	O
x	O
p	O
set	O
any	O
proportionality	O
terms	O
of	O
ixi	O
by	O
requiring	O
ixi	O
i	O
where	O
x	O
z	O
i	O
until	O
converged	O
return	O
j	O
jxj	O
z	O
x	O
i	O
px	O
ixi	O
ixi	O
as	O
an	O
approximation	B
to	O
px	O
where	O
z	O
approximates	O
the	O
normalisation	B
constant	I
z	O
map	B
for	O
mrfs	O
consider	O
a	O
pairwise	B
mrf	O
px	O
eex	O
with	O
i	O
j	O
f	O
xj	O
i	O
ex	O
gxi	O
i	O
where	O
i	O
j	O
denotes	O
neighbouring	O
variables	O
here	O
the	O
terms	O
f	O
xj	O
represent	O
pairwise	B
interactions	O
the	O
i	O
represent	O
unary	O
interactions	O
written	O
for	O
convenience	O
in	O
terms	O
of	O
a	O
pairwise	B
interaction	O
with	O
terms	O
gxi	O
a	O
fixed	O
typically	O
the	O
term	O
fxi	O
xj	O
is	O
used	O
to	O
ensure	O
that	O
neighbouring	O
variables	O
xi	O
and	O
xj	O
are	O
in	O
similar	O
states	O
the	O
term	O
gxi	O
i	O
such	O
models	O
have	O
application	O
in	O
areas	O
such	O
as	O
computer	O
vision	O
and	O
image	O
restoration	O
in	O
which	O
an	O
observed	O
noisy	O
image	O
is	O
to	O
be	O
cleaned	O
to	O
do	O
so	O
we	O
seek	O
a	O
clean	O
image	O
x	O
for	O
which	O
each	O
clean	O
pixel	O
value	B
xi	O
is	O
close	O
to	O
the	O
observed	O
noisy	O
pixel	O
value	B
i	O
is	O
used	O
to	O
bias	B
xi	O
to	O
be	O
close	O
to	O
a	O
desired	O
state	O
i	O
whilst	O
being	O
in	O
a	O
similar	O
state	O
to	O
its	O
clean	O
neighbours	O
map	B
assignment	O
the	O
map	B
assignment	O
of	O
a	O
set	O
of	O
variables	O
xd	O
corresponds	O
to	O
that	O
joint	B
x	O
that	O
maximises	O
ex	O
for	O
a	O
general	O
graph	B
connectivity	O
we	O
cannot	O
naively	O
exploit	O
dynamic	B
programming	O
intuitions	O
to	O
find	O
an	O
draft	O
march	O
map	B
for	O
mrfs	O
b	O
c	O
f	O
a	O
e	O
d	O
a	O
e	O
b	O
c	O
f	O
figure	O
a	O
graph	B
with	O
bidirectional	O
weights	O
a	O
graph	B
cut	B
partitions	O
the	O
nodes	O
wij	O
wji	O
into	O
two	O
groups	O
s	O
and	O
t	O
the	O
weight	B
of	O
the	O
cut	B
is	O
the	O
sum	O
of	O
the	O
edge	O
weights	O
from	O
s	O
to	O
t	O
intuitively	O
it	O
is	O
clear	O
that	O
after	O
assigning	O
nodes	O
to	O
state	O
blue	O
and	O
that	O
the	O
weight	B
of	O
the	O
cut	B
corresponds	O
to	O
the	O
summed	O
weights	O
of	O
neighbours	O
in	O
different	O
states	O
here	O
we	O
highlight	O
those	O
weight	B
contributions	O
the	O
non-highlighted	O
edges	O
do	O
not	O
contribute	O
to	O
the	O
cut	B
weight	B
note	O
that	O
only	O
one	O
of	O
the	O
edge	O
directions	O
contributes	O
to	O
the	O
cut	B
d	O
exact	O
solution	O
since	O
the	O
graph	B
is	O
loopy	B
as	O
we	O
see	O
below	O
in	O
special	O
cases	O
even	O
though	O
the	O
graph	B
is	O
loopy	B
this	O
is	O
possible	O
in	O
general	O
however	O
approximate	B
algorithms	O
are	O
required	O
a	O
simple	O
general	O
approximate	B
solution	O
can	O
be	O
found	O
as	O
follows	O
first	O
initialise	O
all	O
x	O
at	O
random	O
then	O
select	O
a	O
variable	O
xi	O
and	O
find	O
the	O
state	O
of	O
xi	O
that	O
maximally	O
improves	O
ex	O
keeping	O
all	O
other	O
variables	O
fixed	O
one	O
then	O
repeats	O
this	O
selection	O
and	O
local	B
maximal	O
state	O
computation	O
until	O
convergence	O
this	O
is	O
called	O
iterated	O
conditional	B
due	O
to	O
the	O
markov	O
properties	B
its	O
clear	O
that	O
we	O
can	O
improve	O
on	O
this	O
icm	O
method	O
by	O
simultaneously	O
optimising	O
all	O
variables	O
conditioned	O
on	O
their	O
respective	O
markov	O
blankets	O
to	O
the	O
approach	B
used	O
in	O
black-white	O
sampling	B
another	O
improvement	O
is	O
to	O
update	O
only	O
a	O
subset	O
of	O
the	O
variables	O
where	O
the	O
subset	O
has	O
the	O
form	O
of	O
singly-connected	B
structure	B
by	O
recursively	O
clamping	O
variables	O
to	O
reveal	O
a	O
singly-connected	B
structure	B
on	O
un-clamped	O
variables	O
one	O
may	O
find	O
an	O
approximate	B
solution	O
by	O
solving	B
a	O
sequence	O
of	O
tractable	O
problems	O
remarkably	O
in	O
the	O
special	O
case	O
of	O
binary	O
variables	O
and	O
positive	O
w	O
discussed	O
below	O
an	O
efficient	O
exact	O
algorithm	B
exists	O
for	O
finding	O
the	O
map	B
state	O
regardless	O
of	O
the	O
topology	O
attractive	B
binary	I
mrfs	O
consider	O
finding	O
the	O
map	B
of	O
a	O
mrf	O
with	O
binary	O
variables	O
domxi	O
and	O
positive	O
connections	O
wij	O
in	O
this	O
case	O
our	O
task	O
is	O
to	O
find	O
the	O
assignment	O
x	O
that	O
maximises	O
cixi	O
i	O
j	O
wiji	O
xj	O
i	O
ex	O
where	O
i	O
j	O
denotes	O
neighbouring	O
variables	O
and	O
real	O
ci	O
note	O
that	O
for	O
binary	O
variables	O
xi	O
for	O
this	O
particular	O
case	O
an	O
efficient	O
map	B
algorithm	B
exists	O
for	O
arbitrary	O
topology	O
of	O
the	O
algorithm	B
first	O
translates	O
the	O
map	B
assignment	O
problem	B
into	O
an	O
equivalent	B
min	O
s-t-cut	O
for	O
which	O
efficient	O
algorithms	O
exist	O
in	O
min	O
s-t-cut	O
we	O
need	O
a	O
graph	B
with	O
positive	O
weights	O
on	O
the	O
edges	O
this	O
is	O
clearly	O
satisfied	O
i	O
cixi	O
need	O
to	O
be	O
addressed	O
draft	O
march	O
to	O
translate	O
the	O
map	B
assignment	O
problem	B
to	O
a	O
min-cut	O
problem	B
we	O
need	O
to	O
deal	O
with	O
the	O
additional	O
linear	B
i	O
cixi	O
first	O
consider	O
the	O
effect	O
of	O
including	O
a	O
new	O
node	O
x	O
and	O
connecting	O
this	O
to	O
each	O
existing	O
dealing	O
with	O
the	O
bias	B
terms	O
i	O
xj	O
xixj	O
xj	O
if	O
wij	O
although	O
the	O
bias	B
terms	O
cii	O
x	O
i	O
i	O
cixi	O
node	O
i	O
with	O
weight	B
ci	O
this	O
adds	O
then	O
a	O
term	O
if	O
we	O
set	O
x	O
in	O
state	O
this	O
will	O
then	O
add	O
terms	O
ci	O
xi	O
x	O
i	O
map	B
for	O
mrfs	O
s	O
s	O
a	O
e	O
a	O
e	O
b	O
c	O
b	O
c	O
f	O
f	O
d	O
d	O
t	O
t	O
a	O
graph	B
with	O
bidirectional	O
figure	O
weights	O
wij	O
wji	O
augmented	B
with	O
a	O
source	O
node	O
s	O
and	O
sink	O
node	O
t	O
each	O
node	O
has	O
a	O
corresponding	O
bias	B
whose	O
sign	O
is	O
indicated	O
the	O
source	O
node	O
is	O
linked	O
to	O
the	O
nodes	O
corresponding	O
to	O
positive	O
bias	B
and	O
the	O
a	O
graph	B
nodes	O
with	O
negative	O
bias	B
to	O
the	O
sink	O
cut	B
partitions	O
the	O
nodes	O
into	O
two	O
groups	O
s	O
and	O
t	O
where	O
s	O
is	O
the	O
union	O
of	O
the	O
source	O
node	O
and	O
nodes	O
in	O
state	O
t	O
is	O
the	O
union	O
of	O
the	O
sink	O
node	O
and	O
nodes	O
in	O
state	O
the	O
weight	B
of	O
the	O
cut	B
is	O
the	O
sum	O
of	O
the	O
edge	O
weights	O
from	O
s	O
to	O
t	O
the	O
red	O
lines	O
indicate	O
contributions	O
to	O
the	O
cut	B
and	O
can	O
be	O
considered	O
penalties	O
since	O
we	O
wish	O
to	O
find	O
the	O
minimal	O
cut	B
for	O
example	O
a	O
being	O
in	O
state	O
does	O
not	O
incur	O
a	O
penalty	O
since	O
ca	O
on	O
the	O
other	O
hand	O
variable	O
f	O
being	O
in	O
state	O
incurs	O
a	O
penalty	O
since	O
cf	O
otherwise	O
if	O
we	O
set	O
x	O
in	O
state	O
we	O
obtain	O
i	O
ci	O
xi	O
i	O
cixi	O
const	O
since	O
our	O
requirement	O
is	O
that	O
we	O
need	O
the	O
weights	O
to	O
be	O
positive	O
we	O
see	O
that	O
we	O
can	O
achieve	O
this	O
by	O
defining	O
two	O
additional	O
nodes	O
we	O
define	O
a	O
source	O
node	O
xs	O
set	O
to	O
state	O
and	O
connect	O
it	O
to	O
those	O
xi	O
which	O
have	O
positive	O
ci	O
defining	O
wsi	O
ci	O
in	O
addition	O
we	O
define	O
a	O
sink	O
node	O
xt	O
and	O
connect	O
all	O
nodes	O
with	O
negative	O
ci	O
to	O
xt	O
using	O
weight	B
wit	O
ci	O
is	O
therefore	O
positive	O
for	O
the	O
source	O
node	O
clamped	O
to	O
xs	O
and	O
the	O
sink	O
node	O
to	O
xt	O
then	O
including	O
the	O
source	O
and	O
sink	O
we	O
have	O
wiji	O
xj	O
const	O
ex	O
i	O
j	O
is	O
equal	O
to	O
the	O
energy	B
function	B
equation	B
definition	O
cut	B
for	O
a	O
graph	B
g	O
with	O
vertices	O
vd	O
and	O
weights	O
wij	O
a	O
cut	B
is	O
a	O
partition	O
of	O
the	O
vertices	O
into	O
two	O
disjoint	O
groups	O
called	O
s	O
and	O
t	O
the	O
weight	B
of	O
a	O
cut	B
is	O
then	O
defined	O
as	O
the	O
sum	O
of	O
the	O
weights	O
that	O
leave	O
s	O
and	O
land	O
in	O
t	O
see	O
the	O
weight	B
of	O
a	O
cut	B
corresponds	O
to	O
the	O
sum	O
of	O
weights	O
between	O
mismatched	O
neighbours	O
see	O
that	O
is	O
wiji	O
xj	O
cutx	O
cutx	O
i	O
j	O
i	O
j	O
since	O
i	O
xj	O
i	O
xj	O
we	O
can	O
define	O
the	O
weight	B
of	O
the	O
cut	B
equivalently	O
as	O
wij	O
i	O
xj	O
wiji	O
xj	O
const	O
i	O
j	O
draft	O
march	O
map	B
for	O
mrfs	O
clean	O
imfigure	O
age	O
restored	O
image	O
using	O
icm	O
see	O
demomrfclean	O
m	O
noisy	O
image	O
so	O
that	O
the	O
minimal	O
cut	B
assignment	O
will	O
correspond	O
to	O
which	O
take	O
operations	O
or	O
less	O
this	O
means	O
that	O
one	O
can	O
find	O
the	O
exact	O
map	B
assignment	O
of	O
an	O
attractive	B
binary	I
mrf	O
efficiently	O
in	O
operations	O
in	O
maxflow	O
m	O
we	O
implement	O
the	O
ford-fulkerson	O
i	O
j	O
wiji	O
xj	O
in	O
the	O
mrf	O
case	O
our	O
translation	O
into	O
a	O
weighted	O
graph	B
with	O
positive	O
interactions	O
then	O
requires	O
that	O
we	O
identify	O
the	O
source	O
and	O
all	O
other	O
variables	O
assigned	O
to	O
state	O
with	O
s	O
and	O
the	O
sink	O
and	O
all	O
variables	O
in	O
state	O
with	O
t	O
see	O
a	O
fundamental	O
result	O
is	O
that	O
the	O
min	O
s-t-cut	O
solution	O
corresponds	O
to	O
the	O
max-flow	O
solution	O
from	O
the	O
source	O
s	O
to	O
the	O
sink	O
t	O
there	O
are	O
efficient	O
algorithms	O
for	O
max-flow	O
see	O
for	O
example	O
breadth	O
first	O
search	O
see	O
also	O
ex	O
wiji	O
xj	O
example	O
dirty	O
pictures	O
in	O
we	O
present	O
a	O
noisy	O
binary	O
y	O
image	O
that	O
we	O
wish	O
to	O
clean	O
to	O
do	O
so	O
we	O
use	O
an	O
objective	O
i	O
yi	O
ij	O
i	O
the	O
variables	O
xi	O
i	O
are	O
defined	O
on	O
a	O
grid	O
and	O
where	O
wij	O
if	O
xi	O
and	O
xj	O
are	O
neighbours	O
on	O
the	O
grid	O
using	O
i	O
yi	O
xi	O
const	O
we	O
have	O
a	O
standard	O
binary	O
mrf	O
map	B
problem	B
with	O
bias	B
b	O
once	O
can	O
then	O
find	O
the	O
exact	O
optimal	O
x	O
by	O
the	O
min-cut	O
procedure	O
however	O
our	O
implementation	O
of	O
this	O
is	O
slow	O
and	O
instead	O
we	O
use	O
the	O
simpler	O
icm	O
algorithm	B
with	O
results	O
as	O
shown	O
in	O
potts	B
model	B
an	O
extension	O
of	O
the	O
previous	O
model	B
is	O
to	O
the	O
case	O
when	O
the	O
variables	O
are	O
non-binary	O
which	O
is	O
termed	O
the	O
potts	B
model	B
ex	O
wiji	O
xj	O
i	O
i	O
j	O
i	O
i	O
are	O
known	O
this	O
model	B
has	O
immediate	O
application	O
in	O
non-binary	O
image	O
restorawhere	O
wij	O
and	O
the	O
tion	O
and	O
also	O
in	O
clustering	B
based	O
on	O
a	O
similarity	O
score	O
whilst	O
no	O
efficient	O
exact	O
algorithm	B
is	O
known	O
a	O
useful	O
approach	B
is	O
to	O
approximate	B
the	O
problem	B
as	O
a	O
sequence	O
of	O
binary	O
problems	O
as	O
we	O
describe	O
below	O
potts	O
to	O
binary	O
mrf	O
translation	O
consider	O
the	O
representation	O
xi	O
si	O
sixold	O
i	O
or	O
where	O
si	O
and	O
for	O
a	O
given	O
n	O
this	O
restricts	O
xi	O
to	O
be	O
either	O
the	O
state	O
xold	O
depending	O
on	O
the	O
binary	O
variable	O
si	O
using	O
a	O
new	O
binary	O
variable	O
s	O
we	O
can	O
therefore	O
restrict	O
x	O
to	O
a	O
subpart	O
i	O
draft	O
march	O
map	B
for	O
mrfs	O
figure	O
noisy	O
image	O
restored	O
image	O
the	O
method	O
was	O
used	O
with	O
suitable	O
interactions	O
w	O
and	O
bias	B
c	O
to	O
ensure	O
reasonable	O
results	O
from	O
of	O
the	O
full	O
space	O
and	O
write	O
a	O
new	O
objective	O
function	B
in	O
terms	O
of	O
s	O
alone	O
es	O
i	O
j	O
i	O
sj	O
ij	O
w	O
i	O
isi	O
const	O
c	O
for	O
ij	O
this	O
new	O
problem	B
is	O
of	O
the	O
form	O
of	O
an	O
attractive	B
binary	I
mrf	O
which	O
can	O
be	O
solved	O
exactly	O
using	O
the	O
graph	B
cuts	O
procedure	O
the	O
idea	O
is	O
then	O
to	O
choose	O
another	O
value	B
random	O
and	O
then	O
find	O
the	O
optimal	O
s	O
for	O
the	O
new	O
in	O
this	O
way	O
we	O
are	O
guaranteed	O
to	O
iteratively	O
increase	O
e	O
for	O
a	O
given	O
and	O
xold	O
the	O
transformation	O
of	O
the	O
potts	B
model	B
objective	O
is	O
given	O
by	O
using	O
si	O
and	O
considering	O
i	O
xj	O
i	O
sj	O
sjxold	O
si	O
sixold	O
i	O
xold	O
xold	O
sisjuij	O
aisi	O
bjsj	O
const	O
j	O
xold	O
i	O
j	O
with	O
uij	O
xold	O
j	O
xold	O
i	O
xold	O
j	O
xold	O
j	O
xold	O
i	O
sisj	O
and	O
similarly	O
defined	O
ai	O
bi	O
by	O
enumeration	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
uij	O
is	O
either	O
or	O
using	O
the	O
mathematical	O
identity	O
sisj	O
sj	O
si	O
sj	O
we	O
can	O
write	O
i	O
xj	O
uij	O
sj	O
si	O
sj	O
aisi	O
bjsj	O
const	O
hence	O
terms	O
wiji	O
xj	O
translate	O
to	O
positive	O
interaction	O
terms	O
i	O
sj	O
all	O
the	O
unary	O
terms	O
are	O
easily	O
exactly	O
mapped	O
into	O
corresponding	O
unary	O
terms	O
i	O
defined	O
as	O
the	O
sum	O
of	O
all	O
unary	O
terms	O
in	O
si	O
this	O
shows	O
that	O
the	O
positive	O
interaction	O
wij	O
in	O
terms	O
of	O
the	O
original	O
variables	O
x	O
maps	O
to	O
a	O
positive	O
interaction	O
in	O
the	O
new	O
variables	O
s	O
hence	O
we	O
can	O
find	O
the	O
maximal	O
state	O
of	O
s	O
using	O
a	O
graph	B
cut	B
algorithm	B
a	O
related	O
different	O
procedure	O
is	O
outlined	O
in	O
isi	O
for	O
draft	O
march	O
exercises	O
example	O
model	B
for	O
image	O
reconstruction	O
an	O
example	O
image	O
restoration	O
problem	B
for	O
nearest	B
neighbour	B
interactions	O
on	O
a	O
pixel	O
lattice	O
and	O
suitably	O
chosen	O
w	O
c	O
is	O
given	O
in	O
the	O
images	O
are	O
non-binary	O
and	O
therefore	O
the	O
optimal	O
map	B
assignment	O
cannot	O
be	O
computed	O
exactly	O
in	O
an	O
efficient	O
way	O
the	O
alpha-expansion	B
technique	O
was	O
used	O
here	O
combined	O
with	O
an	O
efficient	O
min-cut	O
approach	B
see	O
for	O
details	O
further	O
reading	O
approximate	B
inference	B
is	O
a	O
highly	O
active	B
research	O
area	O
and	O
increasingly	O
links	O
to	O
convex	O
are	O
being	O
developed	O
see	O
for	O
a	O
general	O
overview	O
and	O
for	O
recent	O
application	O
of	O
convex	O
optimisation	B
to	O
approximate	B
inference	B
in	O
a	O
practical	O
machine	O
learning	B
application	O
code	O
loopybp	O
m	O
loopy	B
belief	B
propagation	B
graph	B
formalism	O
demoloopybp	O
m	O
demo	O
of	O
loopy	B
belief	B
propagation	B
demomfbpgibbs	O
m	O
comparison	O
of	O
mean	B
field	O
belief	B
propagation	B
and	O
gibbs	B
sampling	B
demomrfclean	O
m	O
demo	O
of	O
analysing	O
a	O
dirty	O
picture	O
maxflow	O
m	O
max-flow	O
min-cut	O
algorithm	B
binarymrfmap	O
m	O
optimising	O
a	O
binary	O
mrf	O
exercises	O
exercise	O
for	O
the	O
max-flow-min-cut	O
problem	B
under	O
the	O
convention	O
that	O
the	O
source	O
node	O
xs	O
is	O
clamped	O
to	O
state	O
and	O
the	O
sink	O
node	O
xt	O
to	O
state	O
a	O
standard	O
min-cut	O
algorithm	B
returns	O
that	O
joint	B
x	O
which	O
wiji	O
i	O
ij	O
explain	O
how	O
this	O
can	O
be	O
written	O
in	O
the	O
form	O
wiji	O
xj	O
ij	O
exercise	O
using	O
brmltoolbox	O
write	O
a	O
routine	O
kldivqp	O
that	O
returns	O
the	O
kullback-leibler	B
divergence	B
between	O
two	O
discrete	B
distributions	O
q	O
and	O
p	O
defined	O
as	O
potentials	O
q	O
and	O
p	O
exercise	O
the	O
file	O
p	O
mat	O
contains	O
a	O
distribution	B
px	O
y	O
z	O
on	O
ternary	O
state	O
variables	O
using	O
brmltoolbox	O
find	O
the	O
best	O
approximation	B
qx	O
yqz	O
that	O
minimises	O
the	O
kullback-leibler	B
divergence	B
klqp	O
and	O
state	O
the	O
value	B
of	O
the	O
minimal	O
kullback-leibler	B
divergence	B
for	O
the	O
optimal	O
q	O
exercise	O
consider	O
the	O
pairwise	B
mrf	O
defined	O
on	O
a	O
lattice	O
as	O
given	O
in	O
pmrf	O
mat	O
using	O
brmltoolbox	O
find	O
the	O
optimal	O
fully	O
factorised	B
find	O
the	O
optimal	O
fully	O
factorised	B
approximation	B
factor	B
graph	B
formalism	O
equations	O
qbp	O
i	O
by	O
loopy	B
belief	B
propagation	B
based	O
on	O
the	O
qm	O
f	O
i	O
by	O
solving	B
the	O
variational	O
mean	B
field	O
by	O
pure	O
enumeration	O
compute	O
the	O
exact	O
marginals	O
pi	O
draft	O
march	O
exercises	O
averaged	O
over	O
all	O
variables	O
compute	O
the	O
mean	B
expected	O
deviation	O
in	O
the	O
marginals	O
j	O
pix	O
j	O
for	O
both	O
the	O
bp	O
and	O
mf	O
approximations	O
and	O
comment	O
on	O
your	O
results	O
exercise	O
in	O
loopybp	O
m	O
the	O
message	B
schedule	B
is	O
chosen	O
at	O
random	O
modify	O
the	O
routine	O
to	O
choose	O
a	O
schedule	B
using	O
a	O
forward-reverse	O
elimination	O
sequence	O
on	O
a	O
random	O
spanning	B
tree	B
exercise	O
integration	O
bounds	O
consider	O
a	O
bound	B
then	O
for	O
fx	O
gx	O
x	O
fx	O
show	O
that	O
a	O
fxdx	O
x	O
a	O
gx	O
gxdx	O
where	O
fx	O
gx	O
for	O
x	O
a	O
fx	O
gx	O
x	O
fx	O
a	O
for	O
all	O
x	O
fxdx	O
gx	O
x	O
a	O
gxdx	O
the	O
significance	O
is	O
that	O
this	O
double	O
integration	O
summation	O
in	O
the	O
case	O
of	O
discrete	B
variables	O
is	O
a	O
general	O
procedure	O
for	O
generating	O
a	O
new	O
bound	B
from	O
an	O
existing	O
bound	B
exercise	O
starting	O
from	O
ex	O
and	O
using	O
the	O
double	O
integration	O
procedure	O
show	O
that	O
ex	O
x	O
a	O
z	O
by	O
replacing	O
x	O
stws	O
for	O
s	O
and	O
a	O
hts	O
derive	O
a	O
bound	B
on	O
the	O
partition	B
function	B
of	O
a	O
boltzmann	O
distribution	B
estws	O
s	O
show	O
that	O
this	O
bound	B
is	O
equivalent	B
to	O
the	O
mean	B
field	O
bound	B
on	O
the	O
partition	B
function	B
discuss	O
how	O
one	O
can	O
generate	O
tighter	O
bounds	O
on	O
the	O
partition	B
function	B
of	O
a	O
boltzmann	O
distribution	B
by	O
further	O
application	O
of	O
the	O
double	O
integration	O
procedure	O
exercise	O
consider	O
a	O
pairwise	B
mrf	O
for	O
symmetric	O
w	O
consider	O
the	O
decomposition	B
extwxbtx	O
z	O
px	O
w	O
qiwi	O
where	O
qi	O
i	O
i	O
i	O
bound	B
on	O
the	O
normalisation	B
z	O
and	O
discuss	O
a	O
naive	O
method	O
to	O
find	O
the	O
tightest	O
upper	O
bound	B
i	O
qi	O
and	O
the	O
graph	B
of	O
each	O
matrix	B
wi	O
is	O
a	O
tree	B
explain	O
how	O
to	O
form	O
an	O
upper	O
draft	O
march	O
exercise	O
derive	O
linkser	O
s	O
bound	B
on	O
the	O
mutual	B
information	I
equation	B
exercises	O
exercise	O
consider	O
the	O
average	B
of	O
a	O
positive	O
function	B
fx	O
with	O
respect	O
to	O
a	O
distribution	B
px	O
x	O
j	O
log	O
pxfx	O
j	O
x	O
px	O
log	O
fx	O
where	O
fx	O
the	O
simplest	O
version	O
of	O
jensen	O
s	O
inequality	O
states	O
that	O
by	O
considering	O
a	O
distribution	B
rx	O
pxfx	O
and	O
klqr	O
for	O
some	O
variational	O
distribution	B
qx	O
show	O
that	O
j	O
klqxpx	O
the	O
bound	B
saturates	O
when	O
qx	O
pxfx	O
this	O
shows	O
that	O
if	O
we	O
wish	O
to	O
approximate	B
the	O
average	B
j	O
the	O
optimal	O
choice	O
for	O
the	O
approximating	O
distribution	B
depends	O
on	O
both	O
the	O
distribution	B
px	O
and	O
integrand	O
fx	O
furthermore	O
show	O
that	O
j	O
klqxpx	O
klqxfx	O
hqx	O
where	O
hqx	O
is	O
the	O
entropy	B
of	O
qx	O
the	O
first	O
term	O
encourages	O
q	O
to	O
be	O
close	O
to	O
p	O
the	O
second	O
encourages	O
q	O
to	O
be	O
close	O
to	O
f	O
and	O
the	O
third	O
encourages	O
q	O
to	O
be	O
sharply	O
peaked	O
exercise	O
for	O
a	O
markov	B
random	B
field	I
over	O
d	O
binary	O
variables	O
xi	O
i	O
d	O
we	O
define	O
extwx	O
px	O
z	O
show	O
that	O
pxi	O
zi	O
z	O
where	O
zi	O
extwx	O
and	O
explain	O
why	O
a	O
bound	B
on	O
the	O
marginal	B
pxi	O
requires	O
both	O
upper	O
and	O
lower	O
bounds	O
on	O
partition	O
functions	O
exercise	O
consider	O
a	O
directed	B
graph	B
such	O
that	O
the	O
capacity	B
of	O
an	O
edge	O
x	O
y	O
is	O
cx	O
y	O
the	O
flow	O
on	O
an	O
edge	O
fx	O
y	O
must	O
not	O
exceed	O
the	O
capacity	B
of	O
the	O
edge	O
the	O
aim	O
is	O
to	O
maximise	O
the	O
flow	O
from	O
a	O
defined	O
source	O
node	O
s	O
to	O
a	O
defined	O
sink	O
node	O
t	O
in	O
addition	O
flow	O
must	O
be	O
conserved	O
such	O
that	O
for	O
any	O
node	O
other	O
than	O
the	O
source	O
or	O
sink	O
s	O
t	O
fy	O
x	O
fx	O
y	O
x	O
x	O
a	O
cut	B
is	O
defined	O
as	O
a	O
partition	O
of	O
the	O
nodes	O
into	O
two	O
non-overlapping	O
sets	O
s	O
and	O
t	O
such	O
that	O
s	O
is	O
in	O
s	O
and	O
t	O
in	O
t	O
show	O
that	O
the	O
net	O
flow	O
from	O
s	O
to	O
t	O
valf	O
is	O
the	O
same	O
as	O
the	O
net	O
flow	O
from	O
s	O
to	O
t	O
fy	O
x	O
valf	O
x	O
sy	O
t	O
fx	O
y	O
y	O
t	O
s	O
valf	O
x	O
sy	O
t	O
fx	O
y	O
namely	O
that	O
the	O
flow	O
is	O
upper	O
bounded	O
by	O
the	O
capacity	B
of	O
the	O
cut	B
draft	O
march	O
exercises	O
the	O
max-flow-min-cut	O
theorem	B
further	O
states	O
that	O
the	O
maximal	O
flow	O
is	O
actually	O
equal	O
to	O
the	O
capacity	B
of	O
the	O
cut	B
exercise	O
to	O
ising	O
translation	O
consider	O
the	O
function	B
ex	O
defined	O
on	O
a	O
set	O
of	O
multistate	O
variables	O
domxi	O
n	O
ex	O
wiji	O
xj	O
i	O
i	O
j	O
i	O
where	O
wij	O
and	O
observed	O
pixel	O
states	O
maximisation	B
of	O
ex	O
using	O
the	O
restricted	B
parameteristion	O
i	O
are	O
known	O
as	O
are	O
ci	O
our	O
interest	O
is	O
to	O
find	O
an	O
approximate	B
xi	O
si	O
sixold	O
i	O
where	O
si	O
and	O
for	O
a	O
given	O
n	O
show	O
how	O
to	O
write	O
ex	O
as	O
a	O
function	B
of	O
the	O
binary	O
variables	O
isi	O
const	O
c	O
es	O
i	O
j	O
i	O
sj	O
ij	O
w	O
i	O
for	O
using	O
the	O
graph	B
cuts	O
procedure	O
ij	O
this	O
new	O
problem	B
is	O
of	O
the	O
form	O
of	O
an	O
attractive	B
binary	I
mrf	O
which	O
can	O
be	O
solved	O
exactly	O
exercise	O
consider	O
an	O
approximating	O
distribution	B
in	O
the	O
exponential	B
family	B
qx	O
z	O
e	O
tgx	O
we	O
wish	O
to	O
use	O
qx	O
to	O
approximate	B
a	O
distribution	B
px	O
using	O
the	O
kl	O
divergence	B
klpq	O
show	O
that	O
optimally	O
show	O
that	O
a	O
gaussian	B
can	O
be	O
written	O
in	O
the	O
exponential	B
form	O
n	O
z	O
e	O
tgx	O
where	O
x	O
and	O
suitably	O
chosen	O
hence	O
show	O
that	O
the	O
optimal	O
gaussian	B
fit	O
n	O
sense	O
matches	O
the	O
moments	O
px	O
px	O
to	O
any	O
distribution	B
in	O
the	O
minimal	O
klpq	O
m	O
to	O
a	O
distribution	B
px	O
exercise	O
we	O
wish	O
to	O
find	O
a	O
gaussian	B
approximation	B
qx	O
n	O
show	O
that	O
klpq	O
const	O
write	O
the	O
kl	O
divergence	B
explicitly	O
as	O
a	O
function	B
of	O
m	O
and	O
and	O
confirm	O
the	O
general	O
result	O
that	O
the	O
optimal	O
m	O
and	O
that	O
minimise	O
klpq	O
are	O
given	O
by	O
setting	O
the	O
mean	B
and	O
variance	B
of	O
q	O
to	O
those	O
of	O
p	O
draft	O
march	O
exercise	O
for	O
a	O
pairwise	B
binary	O
markov	B
random	B
field	I
p	O
with	O
partition	B
function	B
zw	O
b	O
e	O
x	O
bi	O
log	O
zw	O
b	O
i	O
bixi	O
ij	O
wij	O
xixj	O
zw	O
b	O
x	O
show	O
that	O
the	O
means	O
can	O
be	O
computed	O
using	O
i	O
j	O
wij	O
xixj	O
xie	O
i	O
bixi	O
and	O
that	O
similarly	O
the	O
covariance	B
is	O
given	O
by	O
bi	O
bj	O
log	O
zw	O
b	O
exercises	O
exercise	O
the	O
naive	B
mean	B
field	I
theory	I
applied	O
to	O
a	O
pairwise	B
mrf	O
px	O
e	O
ij	O
wij	O
xixj	O
domxi	O
gives	O
a	O
factorised	B
approximation	B
qx	O
i	O
bixi	O
this	O
we	O
can	O
approximate	B
i	O
qxi	O
based	O
on	O
minimising	O
klqp	O
using	O
to	O
produce	O
a	O
better	O
non-factorised	O
approximation	B
to	O
we	O
could	O
fit	O
a	O
non-factorised	O
q	O
the	O
linearresponse	O
may	O
also	O
be	O
used	O
based	O
on	O
a	O
perturbation	O
expansion	O
of	O
the	O
free	O
energy	B
alternatively	O
consider	O
the	O
relation	O
i	O
j	O
pxi	O
xj	O
pxixjpxj	O
show	O
that	O
pxi	O
xj	O
pxi	O
explain	O
how	O
to	O
use	O
a	O
modified	O
naive	B
mean	B
field	I
method	O
to	O
find	O
a	O
non-factorised	O
approximation	B
to	O
exercise	O
derive	O
the	O
ep	O
updates	O
equation	B
and	O
equation	B
exercise	O
you	O
are	O
given	O
a	O
set	O
of	O
datapoints	O
labelled	B
to	O
n	O
and	O
a	O
similarity	O
metric	O
wij	O
i	O
j	O
n	O
which	O
denotes	O
the	O
similarity	O
of	O
the	O
points	O
i	O
and	O
j	O
you	O
want	O
to	O
assign	O
each	O
datapoint	O
to	O
a	O
cluster	O
index	O
cn	O
k	O
for	O
a	O
subset	O
of	O
the	O
datapoints	O
you	O
have	O
a	O
preference	O
for	O
the	O
cluster	O
index	O
explain	O
how	O
to	O
use	O
a	O
potts	B
model	B
to	O
formulate	O
an	O
objective	O
function	B
for	O
this	O
semi-supervised	B
clustering	B
problem	B
draft	O
march	O
appendix	O
a	O
background	O
mathematics	O
linear	B
algebra	I
vector	B
algebra	I
let	O
x	O
denote	O
the	O
n-dimensional	O
column	O
vector	O
with	O
components	O
xn	O
definition	O
product	O
the	O
scalar	B
product	I
w	O
x	O
is	O
defined	O
as	O
w	O
x	O
wixi	O
wtx	O
and	O
has	O
a	O
natural	B
geometric	O
interpretation	O
as	O
w	O
x	O
cos	O
where	O
is	O
the	O
angle	O
between	O
the	O
two	O
vectors	O
thus	O
if	O
the	O
lengths	O
of	O
two	O
vectors	O
are	O
fixed	O
their	O
inner	O
product	O
is	O
largest	O
when	O
whereupon	O
one	O
vector	O
is	O
a	O
constant	O
multiple	O
of	O
the	O
other	O
if	O
the	O
scalar	B
product	I
xty	O
then	O
x	O
and	O
y	O
are	O
orthogonal	B
are	O
a	O
right	O
angles	O
to	O
each	O
other	O
the	O
length	O
of	O
a	O
vector	O
is	O
denoted	O
the	O
squared	O
length	O
is	O
given	O
by	O
xtx	O
a	O
unit	B
vector	I
x	O
has	O
xtx	O
n	O
definition	O
dependence	O
a	O
set	O
of	O
vectors	O
xn	O
is	O
linearly	O
dependent	O
if	O
there	O
exists	O
a	O
vector	O
xj	O
that	O
can	O
be	O
expressed	O
as	O
a	O
linear	B
combination	O
of	O
the	O
other	O
vectors	O
vice-versa	O
if	O
the	O
only	O
solution	O
to	O
ixi	O
linear	B
algebra	I
figure	O
resolving	O
a	O
vector	O
a	O
into	O
components	O
along	O
the	O
orthogonal	B
directions	O
e	O
and	O
e	O
the	O
projection	B
of	O
a	O
onto	O
these	O
two	O
directions	O
are	O
lengths	O
and	O
along	O
the	O
directions	O
e	O
and	O
e	O
is	O
for	O
all	O
i	O
i	O
n	O
the	O
vectors	O
xn	O
are	O
linearly	B
independent	I
the	O
scalar	B
product	I
as	O
a	O
projection	B
suppose	O
that	O
we	O
wish	O
to	O
resolve	O
the	O
vector	O
a	O
into	O
its	O
components	O
along	O
the	O
orthogonal	B
directions	O
specified	O
by	O
the	O
unit	O
vectors	O
e	O
and	O
e	O
that	O
is	O
and	O
e	O
e	O
this	O
is	O
depicted	O
in	O
we	O
are	O
required	O
to	O
find	O
the	O
scalar	O
values	O
and	O
such	O
that	O
a	O
e	O
e	O
from	O
this	O
we	O
obtain	O
a	O
e	O
e	O
e	O
e	O
e	O
a	O
e	O
e	O
e	O
e	O
e	O
from	O
the	O
orthogonality	O
and	O
unit	O
lengths	O
of	O
the	O
vectors	O
e	O
and	O
e	O
this	O
becomes	O
simply	O
a	O
e	O
a	O
e	O
a	O
set	O
of	O
vectors	O
is	O
orthonormal	B
if	O
they	O
are	O
mutually	O
orthogonal	B
and	O
have	O
unit	O
length	O
this	O
means	O
that	O
we	O
can	O
write	O
the	O
vector	O
a	O
in	O
terms	O
of	O
the	O
orthonormal	B
components	O
e	O
and	O
e	O
as	O
a	O
e	O
e	O
e	O
e	O
one	O
can	O
see	O
therefore	O
that	O
the	O
scalar	B
product	I
between	O
a	O
and	O
e	O
projects	O
the	O
vector	O
a	O
onto	O
the	O
direction	O
e	O
the	O
projection	B
of	O
a	O
vector	O
a	O
onto	O
a	O
direction	O
specified	O
by	O
f	O
is	O
therefore	O
a	O
f	O
f	O
lines	O
in	O
space	O
a	O
line	O
in	O
more	O
dimensions	O
can	O
be	O
specified	O
as	O
follows	O
the	O
vector	O
of	O
any	O
point	O
along	O
the	O
line	O
is	O
given	O
for	O
some	O
s	O
by	O
the	O
equation	B
where	O
u	O
is	O
parallel	B
to	O
the	O
line	O
and	O
the	O
line	O
passes	O
through	O
the	O
point	O
a	O
see	O
this	O
is	O
called	O
the	O
parametric	O
representation	O
of	O
the	O
line	O
an	O
alternative	O
specification	O
can	O
be	O
given	O
by	O
realising	O
that	O
all	O
vectors	O
along	O
the	O
line	O
are	O
orthogonal	B
to	O
the	O
normal	B
of	O
the	O
line	O
n	O
and	O
n	O
are	O
orthonormal	B
that	O
is	O
p	O
a	O
su	O
s	O
r	O
a	O
n	O
p	O
n	O
a	O
n	O
if	O
the	O
vector	O
n	O
is	O
of	O
unit	O
length	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
represents	O
the	O
shortest	O
distance	O
from	O
the	O
origin	O
to	O
the	O
line	O
drawn	O
by	O
the	O
dashed	O
line	O
in	O
this	O
is	O
the	O
projection	B
of	O
a	O
onto	O
the	O
normal	B
direction	O
figure	O
a	O
line	O
can	O
be	O
specified	O
by	O
some	O
position	O
vector	O
on	O
the	O
line	O
a	O
and	O
a	O
unit	B
vector	I
along	O
the	O
direction	O
of	O
the	O
line	O
u	O
in	O
dimensions	O
there	O
is	O
a	O
unique	O
direction	O
n	O
perpendicular	O
to	O
the	O
line	O
in	O
three	O
dimensions	O
the	O
vectors	O
perpendicular	O
to	O
the	O
direction	O
of	O
the	O
line	O
lie	O
in	O
a	O
plane	O
whose	O
normal	B
vector	O
is	O
in	O
the	O
direction	O
of	O
the	O
line	O
u	O
draft	O
march	O
ee	O
aeeaapnu	O
linear	B
algebra	I
figure	O
a	O
plane	O
can	O
be	O
specified	O
by	O
a	O
point	O
in	O
the	O
plane	O
a	O
and	O
two	O
non-parallel	O
directions	O
in	O
the	O
plane	O
u	O
and	O
v	O
the	O
normal	B
to	O
the	O
plane	O
is	O
unique	O
and	O
in	O
the	O
same	O
direction	O
as	O
the	O
directed	B
line	O
from	O
the	O
origin	O
to	O
the	O
nearest	O
point	O
on	O
the	O
plane	O
planes	O
and	O
hyperplanes	O
a	O
line	O
is	O
a	O
one	O
dimensional	O
hyperplane	B
to	O
define	O
a	O
two-dimensional	O
plane	O
arbitrary	O
dimensional	O
space	O
one	O
may	O
specify	O
two	O
vectors	O
u	O
and	O
v	O
that	O
lie	O
in	O
the	O
plane	O
need	O
not	O
be	O
mutually	O
orthogonal	B
and	O
a	O
position	O
vector	O
a	O
in	O
the	O
plane	O
see	O
any	O
vector	O
p	O
in	O
the	O
plane	O
can	O
then	O
be	O
written	O
as	O
p	O
a	O
su	O
tv	O
t	O
r	O
an	O
alternative	O
definition	O
is	O
given	O
by	O
considering	O
that	O
any	O
vector	O
within	O
the	O
plane	O
must	O
be	O
orthogonal	B
to	O
the	O
normal	B
of	O
the	O
plane	O
n	O
a	O
n	O
p	O
n	O
a	O
n	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
represents	O
the	O
shortest	O
distance	O
from	O
the	O
origin	O
to	O
the	O
plane	O
drawn	O
by	O
the	O
dashed	O
line	O
in	O
the	O
advantage	O
of	O
this	O
representation	O
is	O
that	O
it	O
has	O
the	O
same	O
form	O
as	O
a	O
line	O
indeed	O
this	O
representation	O
of	O
is	O
independent	O
of	O
the	O
dimension	O
of	O
the	O
space	O
in	O
addition	O
only	O
two	O
vectors	O
need	O
to	O
be	O
defined	O
a	O
point	O
in	O
the	O
plane	O
a	O
and	O
the	O
normal	B
to	O
the	O
plane	O
n	O
matrices	O
an	O
m	O
n	O
matrix	B
a	O
is	O
a	O
collection	O
of	O
scalar	O
m	O
n	O
values	O
arranged	O
in	O
a	O
rectangle	O
of	O
m	O
rows	O
and	O
n	O
columns	O
a	O
vector	O
can	O
be	O
considered	O
a	O
n	O
matrix	B
if	O
the	O
element	O
of	O
the	O
i-th	O
row	O
and	O
j-th	O
column	O
is	O
aij	O
then	O
at	O
denotes	O
the	O
matrix	B
that	O
has	O
aji	O
there	O
instead	O
the	O
transpose	O
of	O
a	O
for	O
example	O
a	O
and	O
its	O
transpose	O
are	O
a	O
ij	O
at	O
the	O
i	O
j	O
element	O
of	O
matrix	B
a	O
can	O
be	O
written	O
aij	O
or	O
in	O
cases	O
where	O
more	O
clarity	O
is	O
required	O
definition	O
the	O
transpose	O
bt	O
of	O
the	O
n	O
by	O
m	O
matrix	B
b	O
is	O
the	O
m	O
by	O
n	O
matrix	B
d	O
with	O
components	O
bjk	O
b	O
b	O
and	O
btat	O
if	O
the	O
shapes	O
of	O
the	O
matrices	O
ab	O
and	O
c	O
are	O
such	O
that	O
it	O
makes	O
k	O
m	O
j	O
n	O
kj	O
sense	O
to	O
calculate	O
the	O
product	O
abc	O
then	O
ctbtat	O
a	O
square	O
matrix	B
a	O
is	O
symmetric	O
if	O
at	O
a	O
a	O
square	O
matrix	B
is	O
called	O
hermitian	B
if	O
a	O
at	O
where	O
denotes	O
the	O
complex	O
conjugate	B
operator	O
for	O
hermitian	B
matrices	O
the	O
eigenvectors	O
form	O
an	O
orthogonal	B
set	O
with	O
real	O
eigenvalues	O
draft	O
march	O
anuvp	O
definition	O
addition	O
for	O
two	O
matrix	B
a	O
and	O
b	O
of	O
the	O
same	O
size	O
bij	O
linear	B
algebra	I
definition	O
multiplication	O
for	O
an	O
l	O
by	O
n	O
matrix	B
a	O
and	O
an	O
n	O
by	O
m	O
matrix	B
b	O
the	O
product	O
ab	O
is	O
the	O
l	O
by	O
m	O
matrix	B
with	O
elements	O
i	O
l	O
k	O
m	O
for	O
example	O
note	O
that	O
even	O
if	O
ba	O
is	O
defined	O
as	O
well	O
that	O
is	O
if	O
l	O
n	O
generally	O
ba	O
is	O
not	O
equal	O
to	O
ab	O
they	O
do	O
we	O
say	O
they	O
commute	B
the	O
matrix	B
i	O
is	O
the	O
identity	B
matrix	B
necessarily	O
square	O
with	O
s	O
on	O
the	O
diagonal	O
and	O
s	O
everywhere	O
else	O
for	O
clarity	O
we	O
may	O
also	O
write	O
im	O
for	O
an	O
square	O
m	O
m	O
identity	B
matrix	B
then	O
for	O
an	O
m	O
n	O
matrix	B
a	O
ima	O
ain	O
a	O
the	O
identity	B
matrix	B
has	O
elements	O
ij	O
given	O
by	O
the	O
kronecker	B
delta	I
i	O
j	O
i	O
j	O
ij	O
i	O
definition	O
trace	O
aii	O
i	O
i	O
where	O
i	O
are	O
the	O
eigenvalues	O
of	O
a	O
linear	B
transformations	O
rotations	O
if	O
we	O
assume	O
that	O
rotation	O
of	O
a	O
two-dimensional	O
vector	O
x	O
yt	O
can	O
be	O
accomplished	O
by	O
matrix	B
multiplication	O
rx	O
then	O
since	O
matrix	B
multiplication	O
is	O
distributive	O
we	O
only	O
need	O
to	O
work	O
out	O
how	O
the	O
axes	O
unit	O
vectors	O
i	O
and	O
j	O
transform	O
since	O
rx	O
xri	O
yrj	O
the	O
unit	O
vectors	O
i	O
and	O
j	O
under	O
rotation	O
by	O
degrees	O
transform	O
to	O
vectors	O
cos	O
sin	O
rj	O
sin	O
cos	O
ri	O
draft	O
march	O
linear	B
algebra	I
from	O
this	O
one	O
can	O
simply	O
read	O
off	O
the	O
values	O
for	O
the	O
elements	O
cos	O
sin	O
cos	O
sin	O
r	O
determinants	O
definition	O
for	O
a	O
square	O
matrix	B
a	O
the	O
determinant	B
is	O
the	O
volume	O
of	O
the	O
transformation	O
of	O
the	O
matrix	B
a	O
to	O
a	O
sign	O
change	O
that	O
is	O
we	O
take	O
a	O
hypercube	O
of	O
unit	O
volume	O
and	O
map	B
each	O
vertex	O
under	O
the	O
transformation	O
and	O
the	O
volume	O
of	O
the	O
resulting	O
object	O
is	O
defined	O
as	O
the	O
determinant	B
writing	O
aij	O
det	O
det	O
the	O
determinant	B
in	O
the	O
case	O
has	O
the	O
form	O
the	O
determinant	B
of	O
the	O
matrix	B
a	O
is	O
given	O
by	O
the	O
sum	O
of	O
terms	O
where	O
ai	O
is	O
the	O
matrix	B
formed	O
from	O
a	O
by	O
removing	O
the	O
ith	O
row	O
and	O
column	O
this	O
form	O
of	O
the	O
determinant	B
generalises	O
to	O
any	O
dimension	O
that	O
is	O
we	O
can	O
define	O
the	O
determinant	B
recursively	O
as	O
an	O
expansion	O
along	O
the	O
top	O
row	O
of	O
determinants	O
of	O
reduced	O
matrices	O
the	O
absolute	O
value	B
of	O
the	O
determinant	B
is	O
the	O
volume	O
of	O
the	O
transformation	O
det	O
det	O
for	O
square	O
matrices	O
a	O
and	O
b	O
of	O
equal	O
dimensions	O
det	O
det	O
det	O
det	O
for	O
any	O
matrix	B
a	O
which	O
collapses	O
dimensions	O
then	O
the	O
volume	O
of	O
the	O
transformation	O
is	O
zero	O
and	O
so	O
is	O
the	O
determinant	B
if	O
the	O
determinant	B
is	O
zero	O
the	O
matrix	B
cannot	O
be	O
invertible	O
since	O
given	O
any	O
vector	O
x	O
given	O
a	O
projection	B
y	O
ax	O
we	O
cannot	O
uniquely	O
compute	O
which	O
vector	O
x	O
was	O
projected	O
to	O
y	O
there	O
will	O
in	O
general	O
be	O
an	O
infinite	O
number	O
of	O
solutions	O
definition	O
matrix	B
a	O
square	O
matrix	B
a	O
is	O
orthogonal	B
if	O
aat	O
i	O
ata	O
from	O
the	O
properties	B
of	O
the	O
determinant	B
we	O
see	O
therefore	O
that	O
an	O
orthogonal	B
matrix	B
has	O
determinant	B
and	O
hence	O
corresponds	O
to	O
a	O
volume	O
preserving	O
transformation	O
i	O
e	O
a	O
rotation	O
x	O
definition	O
rank	B
for	O
an	O
m	O
n	O
matrix	B
x	O
with	O
n	O
columns	O
each	O
written	O
as	O
an	O
m-vector	O
the	O
rank	B
of	O
x	O
is	O
the	O
maximum	O
number	O
of	O
linearly	B
independent	I
columns	O
equivalently	O
rows	O
a	O
n	O
n	O
square	O
matrix	B
is	O
full	O
rank	B
if	O
the	O
rank	B
is	O
n	O
and	O
the	O
matrix	B
is	O
non-singular	O
otherwise	O
the	O
matrix	B
is	O
reduced	O
rank	B
and	O
is	O
singular	B
draft	O
march	O
matrix	B
inversion	B
linear	B
algebra	I
definition	O
inversion	B
for	O
a	O
square	O
matrix	B
a	O
its	O
inverse	O
satisfies	O
a	O
i	O
aa	O
it	O
is	O
not	O
always	O
possible	O
to	O
find	O
a	O
matrix	B
a	O
such	O
that	O
a	O
i	O
in	O
that	O
case	O
we	O
call	O
the	O
matrix	B
a	O
singular	B
geometrically	O
singular	B
matrices	O
correspond	O
to	O
projections	O
if	O
we	O
were	O
to	O
take	O
the	O
transform	O
of	O
each	O
of	O
the	O
vertices	O
v	O
of	O
a	O
binary	O
hypercube	O
av	O
the	O
volume	O
of	O
the	O
transformed	O
hypercube	O
would	O
be	O
zero	O
if	O
you	O
are	O
given	O
a	O
vector	O
y	O
and	O
a	O
singular	B
transformation	O
a	O
one	O
cannot	O
uniquely	O
identify	O
a	O
vector	O
x	O
for	O
which	O
y	O
ax	O
typically	O
there	O
will	O
be	O
a	O
whole	O
space	O
of	O
possibilities	O
provided	O
the	O
inverses	O
matrices	O
exist	O
b	O
for	O
a	O
non-square	O
matrix	B
a	O
such	O
that	O
aat	O
is	O
invertible	O
then	O
the	O
pseudo	B
inverse	I
defined	O
as	O
a	O
satisfies	O
aa	O
i	O
computing	O
the	O
matrix	B
inverse	O
for	O
a	O
matrix	B
it	O
is	O
straightforward	O
to	O
work	O
out	O
for	O
a	O
general	O
matrix	B
the	O
explicit	O
form	O
of	O
the	O
inverse	O
if	O
the	O
matrix	B
whose	O
inverse	O
we	O
wish	O
to	O
find	O
is	O
a	O
then	O
the	O
condition	O
for	O
the	O
inverse	O
is	O
b	O
c	O
d	O
c	O
d	O
g	O
h	O
f	O
b	O
bg	O
af	O
bh	O
f	O
d	O
c	O
cf	O
dh	O
ce	O
dg	O
ad	O
bc	O
b	O
a	O
g	O
h	O
a	O
multiplying	O
out	O
the	O
left	O
hand	O
side	O
we	O
obtain	O
the	O
four	O
conditions	O
it	O
is	O
readily	O
verified	O
that	O
the	O
solution	O
to	O
this	O
set	O
of	O
four	O
linear	B
equations	O
is	O
given	O
by	O
the	O
quantity	O
ad	O
bc	O
is	O
the	O
determinant	B
of	O
a	O
there	O
are	O
many	O
ways	O
to	O
compute	O
the	O
inverse	O
of	O
a	O
general	O
matrix	B
and	O
we	O
refer	O
the	O
reader	O
to	O
more	O
specialised	O
texts	O
note	O
that	O
if	O
one	O
wants	O
to	O
solve	O
only	O
a	O
linear	B
system	O
although	O
the	O
solution	O
can	O
be	O
obtained	O
through	O
matrix	B
inversion	B
this	O
should	O
not	O
be	O
use	O
often	O
one	O
needs	O
to	O
solve	O
huge	O
dimensional	O
linear	B
systems	O
of	O
equations	O
and	O
speed	O
becomes	O
an	O
issue	O
these	O
equations	O
can	O
be	O
solved	O
much	O
more	O
accurately	O
and	O
quickly	O
using	O
elimination	O
techniques	O
such	O
as	O
gaussian	B
elimination	O
eigenvalues	O
and	O
eigenvectors	O
the	O
eigenvectors	O
of	O
a	O
matrix	B
correspond	O
to	O
the	O
natural	B
coordinate	O
system	O
in	O
which	O
the	O
geometric	O
transformation	O
represented	O
by	O
a	O
can	O
be	O
most	O
easily	O
understood	O
draft	O
march	O
linear	B
algebra	I
definition	O
and	O
eigenvectors	O
for	O
a	O
square	O
matrix	B
a	O
e	O
is	O
an	O
eigenvector	O
of	O
a	O
with	O
eigenvalue	O
if	O
ae	O
e	O
det	O
trace	O
i	O
i	O
hence	O
a	O
matrix	B
is	O
singular	B
if	O
it	O
has	O
a	O
zero	O
eigenvalue	O
the	O
trace	O
of	O
a	O
matrix	B
can	O
be	O
expressed	O
as	O
i	O
for	O
an	O
n	O
dimensional	O
matrix	B
there	O
are	O
repetitions	O
n	O
eigenvalues	O
each	O
with	O
a	O
corresponding	O
eigenvector	O
we	O
can	O
reform	O
equation	B
as	O
i	O
e	O
this	O
is	O
a	O
linear	B
equation	B
for	O
which	O
the	O
eigenvector	O
e	O
and	O
eigenvalue	O
is	O
a	O
solution	O
we	O
can	O
write	O
equation	B
as	O
be	O
where	O
b	O
a	O
i	O
if	O
b	O
has	O
an	O
inverse	O
then	O
a	O
solution	O
is	O
e	O
b	O
which	O
trivially	O
satisfies	O
the	O
eigen-equation	O
for	O
any	O
non-trivial	O
solution	O
to	O
the	O
problem	B
be	O
we	O
therefore	O
need	O
b	O
to	O
be	O
non-invertible	O
this	O
is	O
equivalent	B
to	O
the	O
condition	O
that	O
b	O
has	O
zero	O
determinant	B
hence	O
is	O
an	O
eigenvalue	O
of	O
a	O
if	O
det	O
i	O
this	O
is	O
known	O
as	O
the	O
characteristic	O
equation	B
this	O
determinant	B
equation	B
will	O
be	O
a	O
polynomial	O
of	O
degree	B
n	O
and	O
the	O
resulting	O
equation	B
is	O
known	O
as	O
the	O
characteristic	O
polynomial	O
once	O
we	O
have	O
found	O
an	O
eigenvalue	O
the	O
corresponding	O
eigenvector	O
can	O
be	O
found	O
by	O
substituting	O
this	O
value	B
for	O
in	O
equation	B
and	O
solving	B
the	O
linear	B
equations	O
for	O
e	O
it	O
may	O
be	O
that	O
the	O
for	O
an	O
eigenvalue	O
the	O
eigenvector	O
is	O
not	O
unique	O
and	O
there	O
is	O
a	O
space	O
of	O
corresponding	O
vectors	O
geometrically	O
the	O
eigenvectors	O
are	O
special	O
directions	O
such	O
that	O
the	O
effect	O
of	O
the	O
transformation	O
a	O
along	O
a	O
direction	O
e	O
is	O
simply	O
to	O
scale	O
the	O
vector	O
e	O
for	O
a	O
rotation	O
matric	O
r	O
in	O
general	O
there	O
will	O
be	O
no	O
direction	O
preserved	O
under	O
the	O
rotation	O
so	O
that	O
the	O
eigenvalues	O
and	O
eigenvectors	O
are	O
complex	O
valued	O
is	O
why	O
the	O
fourier	O
representation	O
which	O
corresponds	O
to	O
representation	O
in	O
a	O
rotated	O
basis	O
is	O
necessarily	O
complex	O
remark	O
of	O
eigenvectors	O
of	O
symmetric	O
matrices	O
for	O
a	O
real	O
symmetric	O
matric	O
a	O
at	O
and	O
two	O
of	O
its	O
eigenvectors	O
ei	O
and	O
ej	O
of	O
a	O
are	O
orthogonal	B
if	O
the	O
eigenvalues	O
i	O
and	O
j	O
are	O
different	O
the	O
above	O
can	O
be	O
shown	O
by	O
considering	O
aei	O
iei	O
iejtei	O
since	O
a	O
is	O
symmetric	O
the	O
left	O
hand	O
side	O
is	O
equivalent	B
to	O
jejtei	O
iejtei	O
jejtei	O
if	O
i	O
j	O
this	O
condition	O
can	O
be	O
satisfied	O
only	O
if	O
namely	O
that	O
the	O
eigenvectors	O
are	O
orthogonal	B
draft	O
march	O
matrix	B
decompositions	O
the	O
observation	O
that	O
the	O
eigenvectors	O
of	O
a	O
symmetric	O
matrix	B
are	O
orthogonal	B
leads	O
directly	O
to	O
the	O
spectral	O
decomposition	B
formula	O
below	O
linear	B
algebra	I
definition	O
decomposition	B
a	O
symmetric	O
matrix	B
a	O
has	O
an	O
eigen-decomposition	O
a	O
ieiet	O
i	O
where	O
i	O
is	O
the	O
eigenvalue	O
of	O
eigenvector	O
ei	O
and	O
the	O
eigenvectors	O
form	O
an	O
orthogonal	B
set	O
ej	O
ij	O
ei	O
in	O
matrix	B
notation	O
a	O
e	O
et	O
where	O
e	O
is	O
the	O
matrix	B
of	O
eigenvectors	O
and	O
the	O
corresponding	O
diagonal	O
eigenvalue	O
matrix	B
more	O
generally	O
for	O
a	O
square	O
non-symmetric	O
non-singular	O
a	O
we	O
can	O
write	O
a	O
e	O
e	O
definition	O
value	B
decomposition	B
the	O
svd	B
decomposition	B
of	O
a	O
n	O
p	O
matrix	B
x	O
is	O
x	O
usvt	O
where	O
dim	O
u	O
n	O
n	O
with	O
utu	O
in	O
also	O
dim	O
v	O
p	O
p	O
with	O
vtv	O
ip	O
the	O
matrix	B
s	O
has	O
dim	O
s	O
n	O
p	O
with	O
zeros	O
everywhere	O
except	O
on	O
the	O
diagonal	O
entries	O
the	O
singular	B
values	O
are	O
the	O
diagonal	O
entries	O
and	O
are	O
positive	O
the	O
singular	B
values	O
are	O
ordered	O
so	O
that	O
the	O
upper	O
left	O
diagonal	O
element	O
of	O
s	O
contains	O
the	O
largest	O
singular	B
value	B
quadratic	O
forms	O
definition	O
form	O
xtax	O
xtb	O
definition	O
definite	O
matrix	B
a	O
symmetric	O
matrix	B
a	O
with	O
the	O
property	O
that	O
xtax	O
for	O
any	O
vector	O
x	O
is	O
called	O
nonnegative	O
definite	O
a	O
symmetric	O
matrix	B
a	O
with	O
the	O
property	O
that	O
xtax	O
for	O
any	O
vector	O
x	O
is	O
called	O
positive	B
definite	I
a	O
positive	B
definite	I
matrix	B
has	O
full	O
rank	B
and	O
is	O
thus	O
invertible	O
using	O
the	O
eigen-decomposition	O
of	O
a	O
xtax	O
iyteieitx	O
i	O
i	O
i	O
which	O
is	O
greater	O
than	O
zero	O
if	O
and	O
only	O
if	O
all	O
the	O
eigenvalues	O
are	O
positive	O
hence	O
a	O
is	O
positive	B
definite	I
if	O
and	O
only	O
if	O
all	O
its	O
eigenvalues	O
are	O
positive	O
draft	O
march	O
multivariate	B
calculus	B
matrix	B
identities	O
definition	O
formula	O
for	O
a	O
positive	B
definite	I
matrix	B
a	O
trace	O
a	O
log	O
det	O
note	O
that	O
the	O
above	O
logarithm	O
of	O
a	O
matrix	B
is	O
not	O
the	O
element-wise	O
logarithm	O
in	O
matlab	O
the	O
required	O
function	B
is	O
logm	O
in	O
general	O
for	O
an	O
analytic	O
function	B
fx	O
fm	O
is	O
defined	O
via	O
the	O
power-series	O
expansion	O
of	O
the	O
function	B
on	O
the	O
right	O
since	O
det	O
is	O
a	O
scalar	O
the	O
logarithm	O
is	O
the	O
standard	O
logarithm	O
of	O
a	O
scalar	O
definition	O
inversion	B
lemma	I
formula	O
provided	O
the	O
appropriate	O
inverses	O
exist	O
a	O
a	O
a	O
i	O
vta	O
vta	O
eigenfunctions	O
kx	O
x	O
ax	O
a	O
ax	O
x	O
x	O
by	O
an	O
analogous	O
argument	O
that	O
proves	O
the	O
theorem	B
of	O
linear	B
algebra	I
above	O
the	O
eigenfunctions	O
are	O
orthogonal	B
of	O
a	O
real	O
symmetric	O
kernel	B
x	O
kx	O
are	O
orthogonal	B
ax	O
bx	O
ab	O
where	O
is	O
the	O
complex	O
conjugate	B
of	O
from	O
the	O
previous	O
results	O
we	O
know	O
that	O
a	O
symmetric	O
real	O
matrix	B
k	O
must	O
have	O
a	O
decomposition	B
in	O
terms	O
eigenvectors	O
with	O
positive	O
real	O
eigenvalues	O
since	O
this	O
is	O
to	O
be	O
true	O
for	O
any	O
dimension	O
of	O
matrix	B
it	O
suggests	O
that	O
we	O
need	O
the	O
symmetric	O
kernel	B
function	B
itself	O
to	O
have	O
a	O
decomposition	B
the	O
eigenvalues	O
are	O
countable	O
kxi	O
xj	O
yikxi	O
xjyj	O
ij	O
ij	O
since	O
yi	O
i	O
i	O
yi	O
zi	O
yi	O
z	O
i	O
which	O
is	O
greater	O
than	O
zero	O
if	O
the	O
eigenvalues	O
are	O
all	O
positive	O
for	O
complex	O
z	O
zz	O
if	O
the	O
eigenvalues	O
are	O
uncountable	O
happens	O
when	O
the	O
domain	B
of	O
the	O
kernel	B
is	O
unbounded	O
the	O
appropriate	O
decomposition	B
is	O
kxi	O
xj	O
s	O
sds	O
multivariate	B
calculus	B
definition	O
of	O
the	O
inner	O
product	O
is	O
useful	O
and	O
particularly	O
natural	B
in	O
the	O
context	O
of	O
translation	O
invariant	O
kernels	O
we	O
are	O
free	O
to	O
define	O
the	O
inner	O
product	O
but	O
this	O
conjugate	B
form	O
is	O
often	O
the	O
most	O
useful	O
draft	O
march	O
multivariate	B
calculus	B
figure	O
interpreting	O
the	O
gradient	B
the	O
ellipses	O
are	O
contours	O
of	O
constant	O
function	B
value	B
f	O
const	O
at	O
any	O
point	O
x	O
the	O
gradient	B
vector	O
fx	O
points	O
along	O
the	O
direction	O
of	O
maximal	O
increase	O
of	O
the	O
function	B
definition	O
derivative	O
consider	O
a	O
function	B
of	O
n	O
variables	O
xn	O
or	O
fx	O
the	O
partial	O
derivative	O
of	O
f	O
w	O
r	O
t	O
at	O
x	O
is	O
defined	O
as	O
the	O
following	O
limit	O
it	O
exists	O
f	O
fx	O
h	O
x	O
x	O
n	O
fx	O
h	O
lim	O
h	O
the	O
gradient	B
vector	O
of	O
f	O
will	O
be	O
denoted	O
by	O
f	O
or	O
g	O
f	O
f	O
xn	O
fx	O
gx	O
interpreting	O
the	O
gradient	B
vector	O
consider	O
a	O
function	B
fx	O
that	O
depends	O
on	O
a	O
vector	O
x	O
we	O
are	O
interested	O
in	O
how	O
the	O
function	B
changes	O
when	O
the	O
vector	O
x	O
changes	O
by	O
a	O
small	O
amount	O
x	O
x	O
where	O
is	O
a	O
vector	O
whose	O
length	O
is	O
very	O
small	O
according	O
to	O
a	O
taylor	B
expansion	I
the	O
function	B
will	O
change	O
to	O
i	O
f	O
xi	O
f	O
fx	O
f	O
fx	O
ft	O
xi	O
i	O
we	O
can	O
interpret	O
the	O
summation	O
above	O
as	O
the	O
scalar	B
product	I
between	O
the	O
vector	O
f	O
with	O
components	O
fi	O
f	O
and	O
the	O
gradient	B
points	O
along	O
the	O
direction	O
in	O
which	O
the	O
function	B
increases	O
most	O
rapidly	O
why	O
consider	O
a	O
direction	O
p	O
unit	O
length	O
vector	O
then	O
a	O
displacement	O
units	O
along	O
this	O
direction	O
changes	O
the	O
function	B
value	B
to	O
fx	O
p	O
fx	O
fx	O
p	O
the	O
direction	O
p	O
for	O
which	O
the	O
function	B
has	O
the	O
largest	O
change	O
is	O
that	O
which	O
maximises	O
the	O
overlap	O
fx	O
p	O
fx	O
p	O
cos	O
fx	O
cos	O
where	O
is	O
the	O
angle	O
between	O
p	O
and	O
fx	O
the	O
overlap	O
is	O
maximised	O
when	O
giving	O
p	O
fx	O
fx	O
hence	O
the	O
direction	O
along	O
which	O
the	O
function	B
changes	O
the	O
most	O
rapidly	O
is	O
along	O
fx	O
higher	O
derivatives	O
the	O
first	O
derivative	O
of	O
a	O
function	B
of	O
n	O
variables	O
is	O
an	O
n-vector	O
the	O
second-derivative	O
of	O
an	O
n-variable	O
function	B
is	O
defined	O
by	O
the	O
partial	O
derivatives	O
of	O
the	O
n	O
first	O
partial	O
derivatives	O
w	O
r	O
t	O
the	O
n	O
variables	O
i	O
n	O
j	O
n	O
draft	O
march	O
f	O
xj	O
xi	O
multivariate	B
calculus	B
which	O
is	O
usually	O
written	O
xi	O
xj	O
i	O
j	O
xi	O
i	O
j	O
if	O
the	O
partial	O
derivatives	O
f	O
xi	O
f	O
xj	O
and	O
xi	O
xj	O
are	O
continuous	B
then	O
xi	O
xj	O
exists	O
and	O
xi	O
xj	O
xj	O
xi	O
these	O
second	O
partial	O
derivatives	O
are	O
represented	O
by	O
a	O
square	O
symmetric	O
matrix	B
called	O
the	O
hessian	B
matrix	B
of	O
fx	O
xn	O
hf	O
xn	O
xn	O
chain	B
rule	I
consider	O
xn	O
now	O
let	O
each	O
xj	O
be	O
parameterized	O
by	O
um	O
i	O
e	O
xj	O
um	O
what	O
is	O
f	O
u	O
f	O
xn	O
xn	O
xn	O
xj	O
u	O
higher	O
order	O
terms	O
f	O
xj	O
xj	O
u	O
u	O
higher	O
order	O
terms	O
xj	O
u	O
so	O
f	O
therefore	O
definition	O
rule	O
f	O
u	O
f	O
xj	O
xj	O
u	O
or	O
in	O
vector	O
notation	O
u	O
fxu	O
f	O
txu	O
xu	O
u	O
f	O
xj	O
xj	O
higher	O
order	O
terms	O
j	O
vj	O
f	O
xj	O
definition	O
derivative	O
assume	O
f	O
is	O
differentiable	O
we	O
define	O
the	O
scalar	O
directional	B
derivative	I
of	O
f	O
in	O
a	O
direction	O
v	O
at	O
a	O
point	O
x	O
let	O
x	O
x	O
hv	O
then	O
d	O
dh	O
fx	O
hv	O
matrix	B
calculus	B
draft	O
march	O
f	O
tv	O
definition	O
of	O
a	O
matrix	B
trace	O
for	O
matrices	O
a	O
and	O
b	O
a	O
trace	O
bt	O
definition	O
of	O
log	O
det	O
log	O
det	O
trace	O
a	O
so	O
that	O
log	O
det	O
a	O
t	O
a	O
definition	O
of	O
a	O
matrix	B
inverse	O
for	O
an	O
invertible	O
matrix	B
a	O
a	O
a	O
t	O
aa	O
inequalities	O
convexity	O
inequalities	O
definition	O
function	B
a	O
function	B
fx	O
is	O
defined	O
as	O
convex	O
if	O
for	O
any	O
x	O
y	O
and	O
f	O
x	O
fx	O
if	O
fx	O
is	O
convex	O
fx	O
is	O
called	O
concave	O
an	O
intuitive	O
picture	O
of	O
a	O
convex	B
function	B
is	O
to	O
consider	O
first	O
the	O
quantity	O
x	O
as	O
we	O
vary	O
from	O
to	O
this	O
traces	O
points	O
between	O
x	O
and	O
y	O
hence	O
for	O
we	O
start	O
at	O
the	O
point	O
x	O
fx	O
and	O
as	O
increase	O
trace	O
a	O
straight	O
line	O
towards	O
the	O
point	O
y	O
fy	O
at	O
convexity	O
states	O
that	O
the	O
function	B
f	O
always	O
lies	O
below	O
this	O
straight	O
line	O
geometrically	O
this	O
means	O
that	O
the	O
function	B
fx	O
is	O
always	O
always	O
increasing	O
non-decreasing	O
hence	O
if	O
the	O
function	B
is	O
convex	O
as	O
an	O
example	O
the	O
function	B
log	O
x	O
is	O
concave	O
since	O
its	O
second	O
derivative	O
is	O
negative	O
d	O
dx	O
log	O
x	O
x	O
log	O
x	O
jensen	O
s	O
inequality	O
for	O
a	O
convex	B
function	B
fx	O
it	O
follows	O
directly	O
from	O
the	O
definition	O
of	O
convexity	O
that	O
for	O
any	O
distribution	B
px	O
draft	O
march	O
gradient	B
descent	B
optimisation	B
critical	O
points	O
when	O
all	O
first-order	O
partial	O
derivatives	O
at	O
a	O
point	O
are	O
zero	O
f	O
then	O
the	O
point	O
is	O
said	O
to	O
be	O
a	O
stationary	B
or	O
critical	B
point	I
can	O
be	O
a	O
minimum	O
maximum	O
or	O
saddle	O
point	O
necessary	O
first-order	O
condition	O
for	O
a	O
minimum	O
there	O
is	O
a	O
minimum	O
of	O
f	O
at	O
x	O
if	O
fx	O
fx	O
for	O
all	O
x	O
sufficiently	O
close	O
to	O
x	O
let	O
x	O
x	O
hv	O
for	O
small	O
h	O
and	O
some	O
direction	O
v	O
then	O
by	O
a	O
taylor	B
expansion	I
for	O
small	O
h	O
fx	O
hv	O
fx	O
h	O
f	O
tv	O
and	O
thus	O
for	O
a	O
minimum	O
h	O
f	O
tv	O
choosing	O
v	O
to	O
be	O
f	O
the	O
condition	O
becomes	O
h	O
f	O
t	O
f	O
and	O
is	O
violated	O
for	O
small	O
positive	O
h	O
unless	O
f	O
t	O
f	O
so	O
x	O
can	O
only	O
be	O
a	O
local	B
minimum	O
if	O
fx	O
i	O
e	O
if	O
fx	O
necessary	O
second-order	O
condition	O
for	O
a	O
minimum	O
at	O
a	O
stationary	B
point	O
f	O
hence	O
the	O
taylor	B
expansion	I
is	O
given	O
by	O
fx	O
hv	O
fx	O
v	O
thus	O
the	O
minimum	O
condition	O
requires	O
that	O
vthf	O
v	O
i	O
e	O
the	O
hessian	B
is	O
non-negative	O
definite	O
definition	O
for	O
a	O
minimum	O
sufficient	O
conditions	O
for	O
a	O
minimum	O
at	O
x	O
are	O
fx	O
and	O
hf	O
is	O
positive	B
definite	I
for	O
a	O
quadratic	O
function	B
fx	O
reads	O
btx	O
c	O
with	O
symmetric	O
a	O
the	O
necessary	O
condition	O
fx	O
ax	O
b	O
if	O
a	O
is	O
invertible	O
this	O
equation	B
has	O
the	O
unique	O
solution	O
x	O
a	O
minimum	O
if	O
a	O
is	O
positive	B
definite	I
x	O
is	O
a	O
gradient	B
descent	B
almost	O
all	O
of	O
the	O
search	O
techniques	O
that	O
we	O
consider	O
are	O
iterative	O
i	O
e	O
we	O
proceed	O
towards	O
the	O
minimum	O
x	O
by	O
a	O
sequence	O
of	O
steps	O
on	O
the	O
kth	O
step	O
we	O
take	O
a	O
step	O
of	O
length	O
k	O
in	O
the	O
direction	O
pk	O
xk	O
kpk	O
the	O
length	O
of	O
the	O
step	O
can	O
either	O
be	O
chosen	O
using	O
prior	B
knowledge	O
or	O
by	O
carrying	O
out	O
a	O
line	B
search	I
in	O
the	O
direction	O
pk	O
it	O
is	O
the	O
way	O
that	O
pk	O
is	O
chosen	O
that	O
tends	O
to	O
distinguish	O
the	O
different	O
methods	O
of	O
multivariate	B
optimization	O
that	O
we	O
will	O
discuss	O
draft	O
march	O
gradient	B
descent	B
we	O
shall	O
assume	O
that	O
we	O
can	O
analytically	O
evaluate	O
the	O
gradient	B
of	O
f	O
and	O
will	O
often	O
use	O
the	O
shorthand	O
notation	O
gk	O
fxk	O
typically	O
we	O
will	O
want	O
to	O
choose	O
pk	O
using	O
only	O
gradient	B
information	O
for	O
large	O
problems	O
it	O
can	O
be	O
very	O
expensive	O
to	O
compute	O
the	O
hessian	B
and	O
this	O
can	O
also	O
require	O
a	O
large	O
amount	O
of	O
storage	O
consider	O
the	O
change	B
of	I
variables	I
x	O
my	O
then	O
gy	O
fx	O
fmy	O
and	O
j	O
g	O
yi	O
f	O
xi	O
xi	O
yj	O
so	O
that	O
yg	O
m	O
xf	O
then	O
the	O
change	O
in	O
g	O
is	O
different	O
from	O
the	O
change	O
in	O
f	O
even	O
though	O
the	O
only	O
difference	O
between	O
the	O
two	O
functions	O
is	O
the	O
coordinate	O
system	O
this	O
unfortunate	O
sensitivity	O
to	O
the	O
parameterisation	B
of	O
the	O
function	B
is	O
partially	O
addressed	O
in	O
first	O
order	O
methods	O
such	O
as	O
gradient	B
descent	B
by	O
the	O
natural	B
gradient	B
which	O
uses	O
a	O
prefactor	O
designed	O
to	O
compensate	O
for	O
some	O
of	O
the	O
lost	O
invariance	O
we	O
refer	O
the	O
reader	O
to	O
for	O
a	O
description	O
of	O
this	O
method	O
gradient	B
descent	B
with	O
fixed	O
stepsize	O
locally	O
if	O
we	O
are	O
at	O
point	O
xk	O
we	O
can	O
decrease	O
fx	O
by	O
taking	O
a	O
step	O
in	O
the	O
direction	O
gx	O
if	O
we	O
make	O
the	O
update	O
equation	B
xk	O
gk	O
then	O
we	O
are	O
doing	O
gradient	B
descent	B
with	O
fixed	O
stepsize	O
if	O
is	O
non-infinitesimal	O
it	O
is	O
always	O
possible	O
that	O
we	O
will	O
step	O
over	O
the	O
true	O
minimum	O
making	O
very	O
small	O
guards	O
against	O
this	O
but	O
means	O
that	O
the	O
optimization	O
process	O
will	O
take	O
a	O
very	O
long	O
time	O
to	O
reach	O
a	O
minimum	O
to	O
see	O
why	O
gradient	B
descent	B
works	O
consider	O
the	O
general	O
update	O
xk	O
pk	O
for	O
small	O
we	O
can	O
expand	O
f	O
around	O
xk	O
using	O
taylor	O
s	O
theorem	B
fxk	O
kpk	O
fxk	O
kgt	O
k	O
pk	O
with	O
pk	O
gk	O
and	O
for	O
small	O
positive	O
k	O
we	O
see	O
a	O
guaranteed	O
reduction	O
fxk	O
kpk	O
fxk	O
gradient	B
descent	B
with	O
momentum	B
a	O
simple	O
idea	O
that	O
can	O
improve	O
convergence	O
of	O
gradient	B
descent	B
is	O
to	O
include	O
at	O
each	O
iteration	B
a	O
proportion	O
of	O
the	O
change	O
from	O
the	O
previous	O
iteration	B
one	O
uses	O
e	O
x	O
xk	O
where	O
is	O
the	O
momentum	B
coefficient	O
draft	O
march	O
multivariate	B
minimization	O
quadratic	O
functions	O
figure	O
optimisation	B
using	O
line	B
search	I
along	O
steepest	O
descent	B
directions	O
rushing	O
off	O
following	O
the	O
steepest	O
way	O
downhill	O
from	O
a	O
point	O
continuing	O
for	O
a	O
finite	O
time	O
in	O
that	O
direction	O
doesn	O
t	O
always	O
result	O
in	O
the	O
fastest	O
way	O
to	O
get	O
to	O
the	O
bottom	O
gradient	B
descent	B
with	O
line	O
searches	O
an	O
extension	O
to	O
the	O
idea	O
of	O
gradient	B
descent	B
is	O
to	O
choose	O
the	O
direction	O
of	O
steepest	O
descent	B
as	O
indicated	O
by	O
the	O
gradient	B
g	O
but	O
to	O
calculate	O
the	O
value	B
of	O
the	O
step	O
to	O
take	O
which	O
most	O
reduces	O
the	O
value	B
of	O
e	O
when	O
moving	O
in	O
that	O
direction	O
this	O
involves	O
solving	B
the	O
one-dimensional	O
problem	B
of	O
minimizing	O
exk	O
gk	O
with	O
respect	O
to	O
and	O
is	O
known	O
as	O
a	O
line	B
search	I
that	O
step	O
is	O
then	O
taken	O
and	O
the	O
process	O
repeated	O
again	O
finding	O
the	O
size	O
of	O
the	O
step	O
takes	O
a	O
little	O
work	O
for	O
example	O
you	O
might	O
find	O
three	O
points	O
along	O
the	O
line	O
such	O
that	O
the	O
error	O
at	O
the	O
intermediate	O
point	O
is	O
less	O
than	O
at	O
the	O
other	O
two	O
so	O
that	O
there	O
is	O
some	O
minimum	O
along	O
the	O
line	O
lies	O
between	O
the	O
first	O
and	O
second	O
or	O
between	O
the	O
second	O
and	O
third	O
and	O
some	O
kind	O
of	O
intervalhalving	O
approach	B
can	O
then	O
be	O
used	O
to	O
find	O
it	O
minimum	O
found	O
in	O
this	O
way	O
just	O
as	O
with	O
any	O
sort	O
of	O
gradient-descent	O
algorithm	B
may	O
not	O
be	O
a	O
global	B
minimum	O
of	O
course	O
there	O
are	O
several	O
variants	O
of	O
this	O
theme	O
notice	O
that	O
if	O
the	O
step	O
size	O
is	O
chosen	O
to	O
reduce	O
e	O
as	O
much	O
as	O
it	O
can	O
in	O
that	O
direction	O
then	O
no	O
further	O
improvement	O
in	O
e	O
can	O
be	O
made	O
by	O
moving	O
in	O
that	O
direction	O
for	O
the	O
moment	O
thus	O
the	O
next	O
step	O
will	O
have	O
no	O
component	O
in	O
that	O
direction	O
that	O
is	O
the	O
next	O
step	O
will	O
be	O
at	O
right	O
angles	O
to	O
the	O
one	O
just	O
taken	O
this	O
can	O
lead	O
to	O
zig-zag	O
type	O
behaviour	O
in	O
the	O
optimisation	B
see	O
exact	O
line	B
search	I
condition	O
at	O
the	O
k-th	O
step	O
we	O
chose	O
k	O
to	O
minimize	O
fxk	O
kpk	O
so	O
setting	O
f	O
fxk	O
pk	O
at	O
this	O
step	O
we	O
solve	O
the	O
one-dimensional	O
minimization	O
problem	B
for	O
f	O
thus	O
our	O
choice	O
of	O
k	O
will	O
satisfy	O
f	O
k	O
now	O
f	O
k	O
d	O
dh	O
fxk	O
kpk	O
f	O
k	O
d	O
dh	O
d	O
dh	O
so	O
f	O
k	O
means	O
the	O
directional	B
derivative	I
in	O
the	O
search	O
direction	O
must	O
vanish	O
at	O
the	O
new	O
point	O
and	O
this	O
gives	O
the	O
exact	O
line	B
search	I
condition	O
f	O
gt	O
for	O
a	O
quadratic	O
function	B
fx	O
calculate	O
k	O
since	O
axk	O
kapk	O
b	O
fxk	O
kapk	O
we	O
find	O
btxc	O
with	O
symmetric	O
we	O
can	O
use	O
the	O
condition	O
to	O
analytically	O
k	O
pt	O
k	O
gk	O
pt	O
k	O
apk	O
multivariate	B
minimization	O
quadratic	O
functions	O
the	O
goal	O
of	O
this	O
section	O
is	O
to	O
derive	O
efficient	O
algorithms	O
for	O
minimizing	O
multivariate	B
quadratic	O
functions	O
we	O
shall	O
begin	O
by	O
summarizing	O
some	O
properties	B
of	O
quadratic	O
functions	O
and	O
as	O
byproduct	O
obtain	O
an	O
efficient	O
method	O
for	O
checking	O
whether	O
a	O
symmetric	O
matrix	B
is	O
positive	B
definite	I
minimising	O
quadratic	O
functions	O
using	O
line	B
search	I
consider	O
minimising	O
the	O
quadratic	O
function	B
fx	O
xtax	O
btx	O
c	O
draft	O
march	O
multivariate	B
minimization	O
quadratic	O
functions	O
ptap	O
where	O
a	O
is	O
positive	B
definite	I
and	O
symmetric	O
a	O
is	O
not	O
symmetric	O
consider	O
instead	O
the	O
symmetrised	O
matrix	B
which	O
gives	O
the	O
same	O
function	B
f	O
although	O
we	O
know	O
where	O
the	O
minimum	O
of	O
this	O
function	B
is	O
just	O
using	O
linear	B
algebra	I
we	O
wish	O
to	O
use	O
this	O
function	B
as	O
a	O
toy	O
model	B
for	O
more	O
complex	O
functions	O
which	O
however	O
locally	O
look	O
approximately	O
quadratic	O
one	O
approach	B
is	O
to	O
search	O
along	O
a	O
particular	O
direction	O
p	O
and	O
find	O
a	O
minimum	O
along	O
this	O
direction	O
we	O
can	O
then	O
search	O
for	O
a	O
deeper	O
minima	O
by	O
looking	O
in	O
different	O
directions	O
that	O
is	O
we	O
can	O
search	O
along	O
a	O
line	O
p	O
such	O
that	O
the	O
function	B
attains	O
a	O
minimum	O
that	O
is	O
the	O
directional	B
derivative	I
is	O
zero	O
along	O
this	O
line	O
this	O
has	O
solution	O
p	O
p	O
ptap	O
now	O
we	O
ve	O
found	O
the	O
minimum	O
along	O
the	O
line	O
through	O
with	O
direction	O
p	O
but	O
how	O
should	O
we	O
choose	O
the	O
line	B
search	I
direction	O
p	O
it	O
would	O
seem	O
sensible	O
to	O
choose	O
successive	O
line	B
search	I
directions	O
p	O
according	O
to	O
pnew	O
fx	O
so	O
that	O
each	O
time	O
we	O
minimise	O
the	O
function	B
along	O
the	O
line	O
of	O
steepest	O
descent	B
however	O
this	O
is	O
far	O
from	O
the	O
optimal	O
choice	O
in	O
the	O
case	O
of	O
minimising	O
quadratic	O
functions	O
a	O
much	O
better	O
set	O
of	O
search	O
directions	O
are	O
those	O
defined	O
by	O
the	O
vectors	O
conjugate	B
to	O
a	O
if	O
the	O
matrix	B
a	O
were	O
diagonal	O
then	O
the	O
minimisation	O
is	O
straightforward	O
and	O
can	O
be	O
carried	O
out	O
independently	O
for	O
each	O
dimension	O
if	O
we	O
could	O
find	O
an	O
invertible	O
matrix	B
p	O
with	O
the	O
property	O
that	O
ptap	O
is	O
diagonal	O
then	O
the	O
solution	O
is	O
easy	O
since	O
for	O
f	O
x	O
xtptap	O
x	O
btp	O
x	O
c	O
with	O
x	O
p	O
x	O
we	O
can	O
compute	O
the	O
minimum	O
for	O
each	O
dimension	O
of	O
x	O
separately	O
and	O
then	O
retransform	O
to	O
find	O
x	O
p	O
x	O
definition	O
vectors	O
the	O
vectors	O
pi	O
i	O
k	O
are	O
called	O
conjugate	B
to	O
the	O
matrix	B
a	O
if	O
and	O
only	O
if	O
for	O
i	O
j	O
k	O
and	O
i	O
j	O
i	O
api	O
pt	O
i	O
apj	O
and	O
pt	O
the	O
two	O
conditions	O
guarantee	O
that	O
conjugate	B
vectors	O
are	O
linearly	B
independent	I
assume	O
that	O
i	O
jpj	O
jpj	O
ipi	O
jpj	O
now	O
multiplying	O
from	O
the	O
left	O
with	O
pt	O
as	O
we	O
can	O
make	O
this	O
argument	O
for	O
any	O
i	O
k	O
all	O
of	O
the	O
i	O
must	O
be	O
zero	O
i	O
a	O
yields	O
ipt	O
i	O
api	O
so	O
i	O
is	O
zero	O
since	O
we	O
know	O
that	O
pt	O
i	O
api	O
gram-schmidt	O
construction	B
of	O
conjugate	B
vectors	O
let	O
p	O
pk	O
where	O
the	O
columns	O
are	O
formed	O
from	O
a-conjugate	O
vectors	O
and	O
note	O
that	O
we	O
start	O
with	O
an	O
n	O
by	O
k	O
matrix	B
k	O
n	O
the	O
reason	O
for	O
this	O
is	O
that	O
we	O
are	O
aiming	O
at	O
an	O
incremental	O
procedure	O
where	O
columns	O
are	O
successively	O
added	O
to	O
p	O
since	O
pt	O
i	O
apj	O
the	O
matrix	B
ptap	O
will	O
be	O
diagonal	O
i	O
apj	O
for	O
i	O
j	O
assume	O
we	O
already	O
have	O
k	O
conjugate	B
vectors	O
pk	O
and	O
let	O
v	O
be	O
a	O
vector	O
if	O
pt	O
which	O
is	O
linearly	B
independent	I
of	O
pk	O
we	O
then	O
set	O
v	O
pt	O
j	O
av	O
j	O
apj	O
pt	O
pj	O
for	O
which	O
it	O
is	O
clear	O
that	O
the	O
vectors	O
are	O
conjugate	B
if	O
a	O
is	O
positive	B
definite	I
using	O
the	O
gramschmidt	O
procedure	O
we	O
can	O
construct	O
n	O
conjugate	B
vectors	O
for	O
a	O
positive	B
definite	I
matrix	B
in	O
the	O
following	O
way	O
we	O
start	O
with	O
n	O
linearly	B
independent	I
vectors	O
un	O
we	O
might	O
chose	O
ui	O
ei	O
the	O
unit	B
vector	I
in	O
draft	O
march	O
multivariate	B
minimization	O
quadratic	O
functions	O
the	O
ith	O
direction	O
we	O
then	O
set	O
and	O
use	O
to	O
compute	O
from	O
and	O
v	O
next	O
we	O
set	O
v	O
and	O
compute	O
from	O
and	O
v	O
continuing	O
in	O
this	O
manner	O
we	O
obtain	O
n	O
conjugate	B
vectors	O
note	O
that	O
at	O
each	O
stage	O
of	O
the	O
procedure	O
the	O
vectors	O
uk	O
span	O
the	O
same	O
subspace	O
as	O
the	O
vectors	O
pk	O
what	O
is	O
going	O
to	O
happen	O
if	O
a	O
is	O
not	O
positive	B
definite	I
if	O
we	O
could	O
find	O
n	O
conjugate	B
vectors	O
a	O
would	O
be	O
positive	B
definite	I
and	O
so	O
at	O
some	O
point	O
k	O
the	O
gram-schmidt	B
procedure	I
must	O
break	O
down	O
this	O
will	O
happen	O
if	O
pt	O
k	O
apk	O
so	O
by	O
trying	O
out	O
the	O
gram-schmidt	B
procedure	I
we	O
can	O
in	O
fact	O
find	O
out	O
whether	O
a	O
matrix	B
is	O
positive	B
definite	I
the	O
conjugate	B
vectors	I
algorithm	B
let	O
us	O
assume	O
that	O
when	O
minimising	O
fx	O
conjugate	B
to	O
a	O
which	O
we	O
use	O
as	O
our	O
search	O
directions	O
so	O
btx	O
c	O
we	O
first	O
construct	O
n	O
vectors	O
pn	O
xk	O
kpk	O
at	O
each	O
step	O
we	O
chose	O
k	O
by	O
an	O
exact	O
line	B
search	I
thus	O
k	O
pt	O
k	O
gk	O
k	O
apk	O
pt	O
this	O
conjugate	B
vectors	I
algorithm	B
has	O
the	O
geometrical	O
interpretation	O
that	O
not	O
only	O
is	O
the	O
directional	B
derivative	I
zero	O
at	O
the	O
new	O
point	O
along	O
the	O
direction	O
pk	O
it	O
is	O
zero	O
along	O
all	O
the	O
previous	O
search	O
directions	O
pk	O
theorem	B
expanding	O
subspace	O
theorem	B
be	O
a	O
sequence	O
of	O
vectors	O
in	O
rn	O
conjugate	B
to	O
the	O
definite	O
matrix	B
a	O
and	O
let	O
fx	O
btx	O
c	O
then	O
for	O
any	O
the	O
sequence	O
generated	O
according	O
to	O
and	O
has	O
the	O
property	O
that	O
the	O
directional	B
derivative	I
of	O
f	O
in	O
the	O
direction	O
pi	O
vanishes	O
at	O
the	O
point	O
if	O
i	O
k	O
i	O
e	O
proof	O
for	O
i	O
k	O
we	O
can	O
write	O
as	O
jpj	O
since	O
fx	O
ax	O
b	O
we	O
have	O
b	O
b	O
a	O
jpj	O
japj	O
so	O
pt	O
i	O
pt	O
i	O
jpt	O
i	O
apj	O
jpt	O
i	O
apj	O
now	O
since	O
the	O
point	O
was	O
obtained	O
by	O
an	O
exact	O
line	B
search	I
in	O
the	O
direction	O
pi	O
but	O
all	O
i	O
apj	O
by	O
conjugacy	O
so	O
of	O
the	O
terms	O
in	O
the	O
sum	O
over	O
j	O
also	O
vanish	O
since	O
j	O
i	O
and	O
pt	O
the	O
subspace	O
theorem	B
shows	O
that	O
because	O
we	O
use	O
conjugate	B
vectors	O
optimizing	O
in	O
the	O
direction	O
pk	O
does	O
not	O
spoil	O
the	O
optimality	O
w	O
r	O
t	O
to	O
the	O
previous	O
search	O
directions	O
in	O
particular	O
after	O
having	O
carried	O
out	O
n	O
steps	O
of	O
the	O
algorithm	B
we	O
have	O
f	O
for	O
i	O
n	O
the	O
n	O
equations	O
can	O
be	O
written	O
in	O
a	O
more	O
compact	O
form	O
as	O
f	O
pn	O
draft	O
march	O
multivariate	B
minimization	O
quadratic	O
functions	O
the	O
square	O
matrix	B
p	O
pn	O
is	O
invertible	O
since	O
the	O
pi	O
are	O
conjugate	B
so	O
the	O
point	O
is	O
the	O
minimum	O
x	O
of	O
the	O
quadratic	O
function	B
f	O
so	O
in	O
contrast	O
to	O
gradient	B
descent	B
for	O
a	O
quadratic	O
function	B
the	O
conjugate	B
vectors	I
algorithm	B
converges	O
in	O
a	O
finite	O
number	O
of	O
steps	O
the	O
conjugate	B
gradients	I
algorithm	B
the	O
conjugate	B
gradients	I
algorithm	B
is	O
a	O
special	O
case	O
of	O
the	O
conjugate	B
vectors	I
algorithm	B
in	O
which	O
the	O
gramschmidt	O
procedure	O
becomes	O
very	O
simple	O
we	O
do	O
not	O
use	O
a	O
predetermined	O
set	O
of	O
conjugate	B
vectors	O
but	O
construct	O
these	O
on-the-fly	O
after	O
k-steps	O
of	O
the	O
conjugate	B
vectors	I
algorithm	B
we	O
need	O
to	O
construct	O
a	O
vector	O
which	O
is	O
conjugate	B
to	O
pk	O
this	O
could	O
be	O
done	O
by	O
applying	O
the	O
gram-schmidt	B
procedure	I
to	O
any	O
vector	O
v	O
which	O
is	O
linearly	B
independent	I
of	O
the	O
vectors	O
pk	O
in	O
the	O
conjugate	B
gradients	I
algorithm	B
one	O
makes	O
the	O
special	O
choice	O
v	O
by	O
the	O
subspace	O
theorem	B
the	O
gradient	B
at	O
the	O
new	O
point	O
is	O
orthogonal	B
to	O
pi	O
i	O
k	O
so	O
is	O
linearly	B
independent	I
of	O
pk	O
and	O
a	O
valid	O
choice	O
for	O
v	O
unless	O
in	O
the	O
latter	O
case	O
is	O
our	O
minimum	O
and	O
we	O
are	O
done	O
and	O
from	O
now	O
on	O
we	O
assume	O
that	O
using	O
the	O
notation	O
gk	O
fxk	O
the	O
equation	B
for	O
the	O
new	O
search	O
direction	O
given	O
by	O
the	O
gram-schmidt	B
procedure	I
is	O
pt	O
i	O
pt	O
i	O
api	O
pi	O
since	O
is	O
orthogonal	B
to	O
pi	O
i	O
k	O
by	O
the	O
subspace	O
theorem	B
we	O
have	O
pt	O
can	O
be	O
written	O
as	O
gt	O
so	O
gt	O
pt	O
and	O
in	O
particular	O
we	O
now	O
want	O
to	O
show	O
that	O
because	O
we	O
have	O
been	O
using	O
the	O
conjugate	B
gradients	I
algorithm	B
at	O
the	O
previous	O
steps	O
as	O
well	O
in	O
equation	B
all	O
terms	O
but	O
the	O
last	O
in	O
the	O
sum	O
over	O
i	O
vanish	O
we	O
shall	O
assume	O
that	O
k	O
since	O
in	O
the	O
first	O
step	O
we	O
just	O
set	O
first	O
note	O
that	O
gi	O
b	O
b	O
xi	O
iapi	O
and	O
since	O
i	O
api	O
gi	O
i	O
so	O
in	O
equation	B
i	O
gt	O
pt	O
gt	O
gi	O
i	O
gt	O
i	O
since	O
the	O
pi	O
where	O
obtained	O
by	O
applying	O
the	O
gram-schmidt	B
procedure	I
to	O
the	O
gradients	O
gi	O
the	O
subspace	O
theorem	B
gt	O
for	O
i	O
k	O
this	O
shows	O
that	O
implies	O
also	O
gt	O
pt	O
i	O
gt	O
i	O
gt	O
k	O
if	O
i	O
k	O
if	O
i	O
k	O
hence	O
equation	B
simplifies	O
to	O
gt	O
k	O
pt	O
k	O
apk	O
pk	O
this	O
can	O
be	O
brought	O
into	O
an	O
even	O
simpler	O
form	O
by	O
applying	O
equation	B
to	O
k	O
gt	O
pt	O
k	O
apk	O
we	O
shall	O
write	O
this	O
in	O
the	O
form	O
k	O
apk	O
pt	O
gt	O
k	O
gk	O
pk	O
gt	O
gt	O
k	O
gk	O
pk	O
kpk	O
where	O
k	O
gt	O
gt	O
k	O
gk	O
draft	O
march	O
multivariate	B
minimization	O
quadratic	O
functions	O
algorithm	B
conjugate	B
gradients	O
for	O
minimising	O
a	O
function	B
fx	O
k	O
choose	O
while	O
gk	O
do	O
k	O
argmin	O
xk	O
kpk	O
k	O
gt	O
k	O
gk	O
kpk	O
k	O
k	O
end	O
while	O
fxk	O
kpk	O
k	O
line	B
search	I
the	O
formula	O
for	O
k	O
is	O
due	O
to	O
fletcher	O
and	O
reeves	O
since	O
the	O
gradients	O
are	O
orthogonal	B
k	O
can	O
also	O
be	O
written	O
as	O
k	O
gt	O
gk	O
k	O
gk	O
gt	O
this	O
is	O
the	O
polak-ribiere	O
formula	O
the	O
choice	O
between	O
the	O
two	O
expression	O
for	O
k	O
can	O
be	O
of	O
some	O
importance	B
if	O
f	O
is	O
not	O
quadratic	O
newton	O
s	O
method	O
consider	O
a	O
function	B
fx	O
that	O
we	O
wish	O
to	O
find	O
the	O
minimum	O
of	O
a	O
taylor	B
expansion	I
up	O
to	O
second	O
order	O
gives	O
fx	O
fx	O
t	O
f	O
th	O
o	O
the	O
matrix	B
h	O
is	O
the	O
hessian	B
differentiating	O
the	O
right	O
hand	O
side	O
with	O
respect	O
to	O
equivalently	O
completing	O
the	O
square	O
we	O
find	O
that	O
the	O
right	O
hand	O
side	O
has	O
its	O
lowest	O
value	B
when	O
f	O
h	O
h	O
f	O
hence	O
an	O
optimisation	B
routine	O
to	O
minimise	O
e	O
is	O
given	O
by	O
the	O
newton	B
update	I
xk	O
h	O
f	O
a	O
benefit	O
of	O
newton	O
method	O
over	O
gradient	B
descent	B
is	O
that	O
the	O
decrease	O
in	O
the	O
objective	O
function	B
is	O
invariant	O
under	O
a	O
linear	B
change	O
of	O
co-ordinates	O
y	O
mx	O
quasi-newton	O
methods	O
for	O
large-scale	O
problems	O
the	O
inversion	B
of	O
the	O
hessian	B
is	O
computationally	O
demanding	O
especially	O
if	O
the	O
matrix	B
is	O
close	O
to	O
singular	B
an	O
alternative	O
is	O
to	O
set	O
up	O
the	O
iteration	B
xk	O
kskgk	O
this	O
is	O
a	O
very	O
general	O
form	O
if	O
sk	O
a	O
then	O
we	O
have	O
newton	O
s	O
method	O
while	O
if	O
sk	O
i	O
we	O
have	O
steepest	O
descent	B
in	O
general	O
it	O
would	O
seem	O
to	O
be	O
a	O
good	O
idea	O
to	O
choose	O
sk	O
to	O
be	O
an	O
approximation	B
to	O
the	O
inverse	O
hessian	B
also	O
note	O
that	O
it	O
is	O
important	O
that	O
sk	O
be	O
positive	B
definite	I
so	O
that	O
for	O
small	O
k	O
we	O
obtain	O
a	O
descent	B
method	O
the	O
idea	O
behind	O
most	O
quasi-newton	O
methods	O
is	O
to	O
try	O
to	O
construct	O
an	O
approximate	B
inverse	O
hessian	B
hk	O
using	O
information	O
gathered	O
as	O
the	O
descent	B
progresses	O
and	O
to	O
set	O
sk	O
hk	O
as	O
we	O
have	O
seen	O
for	O
a	O
quadratic	O
optimization	O
problem	B
we	O
have	O
the	O
relationship	O
gk	O
xk	O
draft	O
march	O
algorithm	B
quasi-newton	O
for	O
minimising	O
a	O
function	B
fx	O
constrained	B
optimisation	B
using	O
lagrange	O
multipliers	O
k	O
choose	O
i	O
while	O
gk	O
do	O
pk	O
hkgk	O
k	O
argmin	O
xk	O
kpk	O
sk	O
xk	O
yk	O
gk	O
and	O
update	O
k	O
k	O
fxk	O
kpk	O
k	O
end	O
while	O
defining	O
sk	O
xk	O
and	O
yk	O
gk	O
we	O
see	O
that	O
equation	B
becomes	O
yk	O
ask	O
line	B
search	I
it	O
is	O
reasonable	O
to	O
demand	O
that	O
i	O
k	O
si	O
after	O
n	O
linearly	B
independent	I
steps	O
we	O
would	O
then	O
have	O
a	O
for	O
k	O
n	O
there	O
are	O
an	O
infinity	O
of	O
solutions	O
for	O
satisfying	O
equation	B
a	O
popular	O
choice	O
is	O
the	O
broyden-fletcher-goldfarb-shanno	B
bfgs	O
update	O
given	O
by	O
hk	O
hkyk	O
yt	O
k	O
k	O
sk	O
yt	O
skyt	O
k	O
skst	O
k	O
k	O
yk	O
st	O
hk	O
hkykst	O
k	O
k	O
yk	O
st	O
this	O
is	O
a	O
correction	O
to	O
hk	O
constructed	O
from	O
the	O
vectors	O
sk	O
and	O
hkyk	O
the	O
direction	O
vectors	O
pk	O
pk	O
hkgk	O
produced	O
by	O
the	O
algorithm	B
obey	O
pt	O
i	O
apj	O
pi	O
i	O
j	O
k	O
i	O
k	O
equation	B
is	O
called	O
the	O
hereditary	O
property	O
in	O
our	O
notation	O
sk	O
kpk	O
and	O
as	O
the	O
s	O
are	O
non-zero	O
equation	B
can	O
also	O
be	O
written	O
as	O
st	O
i	O
asj	O
i	O
j	O
k	O
since	O
the	O
pk	O
s	O
are	O
a-conjugate	O
and	O
since	O
we	O
successively	O
minimize	O
f	O
in	O
these	O
directions	O
we	O
see	O
that	O
the	O
bfgs	O
algorithm	B
is	O
a	O
conjugate	B
direction	O
method	O
with	O
the	O
choice	O
of	O
i	O
it	O
is	O
in	O
fact	O
the	O
conjugate	B
gradient	B
method	O
note	O
that	O
the	O
storage	O
requirements	O
for	O
quasi	O
newton	O
methods	O
scale	O
quadratically	O
with	O
the	O
number	O
of	O
variables	O
and	O
hence	O
tends	O
to	O
be	O
used	O
for	O
smaller	O
problems	O
limited	O
memory	O
bfgs	O
reduces	O
the	O
storage	O
by	O
only	O
using	O
the	O
l	O
latest	O
updates	O
in	O
computing	O
the	O
approximate	B
hessian	B
inverse	O
equation	B
in	O
contrast	O
the	O
memory	O
requirements	O
for	O
pure	O
conjugate	B
gradient	B
methods	O
scale	O
only	O
linearly	O
with	O
the	O
dimension	O
of	O
x	O
constrained	B
optimisation	B
using	O
lagrange	O
multipliers	O
single	O
constraint	O
consider	O
first	O
the	O
problem	B
of	O
minimising	O
fx	O
subject	O
to	O
a	O
single	O
constraint	O
cx	O
imagine	O
that	O
we	O
have	O
already	O
identified	O
an	O
x	O
that	O
satisfies	O
the	O
constraint	O
that	O
is	O
cx	O
how	O
can	O
we	O
tell	O
if	O
this	O
x	O
minimises	O
draft	O
march	O
constrained	B
optimisation	B
using	O
lagrange	O
multipliers	O
the	O
function	B
f	O
we	O
are	O
only	O
allowed	O
to	O
search	O
for	O
lower	O
function	B
values	O
around	O
this	O
x	O
in	O
directions	O
which	O
are	O
consistent	B
with	O
the	O
constraint	O
for	O
a	O
small	O
change	O
the	O
change	O
in	O
the	O
constraint	O
is	O
cx	O
cx	O
cx	O
hence	O
in	O
order	O
that	O
the	O
constraint	O
remains	O
satisfied	O
we	O
can	O
only	O
search	O
in	O
a	O
direction	O
such	O
that	O
cx	O
that	O
is	O
in	O
directions	O
that	O
are	O
orthogonal	B
to	O
cx	O
so	O
let	O
us	O
explore	O
the	O
change	O
in	O
f	O
along	O
a	O
direction	O
where	O
cx	O
fx	O
fx	O
fx	O
since	O
we	O
are	O
looking	O
for	O
a	O
point	O
x	O
that	O
minimises	O
the	O
function	B
f	O
we	O
require	O
x	O
to	O
be	O
a	O
stationary	B
point	O
fx	O
thus	O
must	O
be	O
orthogonal	B
to	O
both	O
fx	O
and	O
cx	O
since	O
we	O
wish	O
to	O
constrain	O
as	O
little	O
as	O
possible	O
the	O
most	O
freedom	O
is	O
given	O
by	O
enforcing	O
fx	O
to	O
be	O
parallel	B
to	O
cx	O
so	O
that	O
fx	O
cx	O
for	O
some	O
r	O
to	O
solve	O
the	O
optimisation	B
problem	B
therefore	O
we	O
look	O
for	O
a	O
point	O
x	O
such	O
that	O
fx	O
cx	O
for	O
some	O
and	O
for	O
which	O
cx	O
an	O
alternative	O
formulation	O
of	O
this	O
dual	B
requirement	O
is	O
to	O
look	O
for	O
x	O
and	O
that	O
jointly	O
minimise	O
the	O
lagrangian	B
lx	O
fx	O
cx	O
differentiating	O
with	O
respect	O
to	O
x	O
we	O
get	O
the	O
requirement	O
fx	O
cx	O
and	O
differentiating	O
with	O
respect	O
to	O
we	O
get	O
that	O
cx	O
multiple	O
constraints	O
consider	O
the	O
problem	B
of	O
optimising	O
fx	O
subject	O
to	O
the	O
constraints	O
cix	O
i	O
r	O
n	O
where	O
n	O
is	O
the	O
dimensionality	O
of	O
the	O
space	O
denote	O
by	O
s	O
the	O
n	O
r	O
dimensional	O
subspace	O
of	O
x	O
which	O
obeys	O
the	O
constraints	O
assume	O
that	O
x	O
is	O
such	O
an	O
optimum	O
as	O
in	O
the	O
unconstrained	O
case	O
we	O
consider	O
perturbations	O
v	O
to	O
x	O
but	O
now	O
such	O
that	O
v	O
lies	O
in	O
s	O
a	O
cix	O
hv	O
cix	O
vt	O
cix	O
i	O
cix	O
thus	O
for	O
the	O
perturbation	O
to	O
stay	O
within	O
s	O
we	O
require	O
that	O
vta	O
let	O
a	O
i	O
for	O
all	O
i	O
r	O
r	O
then	O
this	O
condition	O
can	O
be	O
rewritten	O
as	O
a	O
v	O
a	O
let	O
a	O
be	O
the	O
matrix	B
whose	O
columns	O
are	O
a	O
we	O
also	O
require	O
for	O
a	O
local	B
optimum	O
that	O
vt	O
f	O
for	O
all	O
v	O
in	O
s	O
we	O
see	O
that	O
f	O
must	O
be	O
orthogonal	B
to	O
v	O
and	O
that	O
v	O
must	O
be	O
orthogonal	B
to	O
the	O
a	O
i	O
s	O
this	O
can	O
be	O
achieved	O
by	O
forcing	O
f	O
to	O
be	O
a	O
linear	B
combination	O
of	O
the	O
a	O
i	O
a	O
geometrically	O
this	O
says	O
that	O
the	O
gradient	B
vector	O
is	O
normal	B
to	O
the	O
tangent	O
plane	O
to	O
s	O
at	O
x	O
these	O
conditions	O
give	O
rise	O
to	O
the	O
method	O
of	O
lagrange	O
multipliers	O
for	O
optimisation	B
problems	O
with	O
equality	O
constraints	O
the	O
method	O
requires	O
finding	O
x	O
and	O
which	O
solve	O
the	O
equations	O
f	O
i	O
s	O
i	O
e	O
i	O
f	O
i	O
aix	O
i	O
cix	O
for	O
i	O
r	O
there	O
are	O
n	O
r	O
equations	O
and	O
n	O
r	O
unknowns	O
so	O
the	O
system	O
is	O
well-determined	O
however	O
the	O
system	O
is	O
nonlinear	O
x	O
in	O
general	O
and	O
so	O
may	O
not	O
be	O
easy	O
to	O
solve	O
we	O
can	O
restate	O
these	O
conditions	O
by	O
introducing	O
the	O
lagrangian	B
function	B
lx	O
fx	O
icix	O
i	O
the	O
partial	O
derivatives	O
of	O
l	O
with	O
respect	O
to	O
x	O
and	O
reproduce	O
equations	O
and	O
hence	O
a	O
necessary	O
condition	O
for	O
a	O
local	B
minimizer	O
is	O
that	O
x	O
is	O
a	O
stationary	B
point	O
of	O
the	O
lagrangian	B
function	B
note	O
that	O
this	O
stationary	B
point	O
is	O
not	O
a	O
minimum	O
but	O
a	O
saddle	O
point	O
as	O
l	O
depends	O
linearly	O
on	O
we	O
have	O
given	O
first-order	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
a	O
local	B
optimum	O
to	O
show	O
that	O
this	O
optimum	O
is	O
a	O
local	B
minimum	O
we	O
would	O
need	O
to	O
consider	O
second-order	O
conditions	O
analogous	O
to	O
the	O
positive	O
definiteness	O
of	O
the	O
hessian	B
in	O
the	O
unconstrained	O
case	O
this	O
can	O
be	O
done	O
but	O
will	O
not	O
be	O
considered	O
here	O
draft	O
march	O
constrained	B
optimisation	B
using	O
lagrange	O
multipliers	O
draft	O
march	O
bibliography	O
l	O
f	O
abbott	O
j	O
a	O
varela	O
k	O
sen	O
and	O
s	O
b	O
nelson	O
synaptic	O
depression	B
and	O
cortical	O
gain	O
control	O
science	O
d	O
h	O
ackley	O
g	O
e	O
hinton	O
and	O
t	O
j	O
sejnowski	O
a	O
learning	B
algorithm	B
for	O
boltzmann	O
machines	O
cognitive	O
science	O
r	O
p	O
adams	O
and	O
d	O
j	O
c	O
mackay	O
bayesian	B
online	B
changepoint	B
detection	O
cavendish	O
laboratory	O
department	O
of	O
physics	O
university	O
of	O
cambridge	O
cambridge	O
uk	O
e	O
airoldi	O
d	O
blei	O
e	O
xing	O
and	O
s	O
fienberg	O
a	O
latent	B
mixed	B
membership	I
model	B
for	O
relational	O
data	O
in	O
linkkdd	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
link	O
discovery	O
pages	O
new	O
york	O
ny	O
usa	O
acm	O
e	O
m	O
airoldi	O
d	O
m	O
blei	O
s	O
e	O
fienberg	O
and	O
e	O
p	O
xing	O
mixed	B
membership	I
stochastic	O
blockmodels	O
journal	O
of	O
machine	O
learning	B
research	O
d	O
l	O
alspach	O
and	O
h	O
w	O
sorenson	O
nonlinear	O
bayesian	B
estimation	O
using	O
gaussian	B
sum	O
approximations	O
ieee	O
transactions	O
on	O
automatic	O
control	O
s-i	O
amari	O
natural	B
gradient	B
works	O
efficiently	O
in	O
learning	B
neural	B
computation	I
s-i	O
amari	O
natural	B
gradient	B
learning	B
for	O
over	O
and	O
under-complete	B
bases	O
in	O
ica	B
neural	B
computation	I
i	O
androutsopoulos	O
j	O
koutsias	O
k	O
v	O
chandrinos	O
and	O
c	O
d	O
spyropoulos	O
an	O
experimental	O
comparison	O
in	O
proceedings	O
of	O
of	O
naive	O
bayesian	B
and	O
keyword-based	O
anti-spam	O
filtering	O
with	O
personal	O
e-mail	O
messages	O
the	O
annual	O
international	O
acm	O
sigir	O
conference	O
on	O
research	O
and	O
development	O
in	O
information	B
retrieval	I
pages	O
new	O
york	O
ny	O
usa	O
acm	O
s	O
arora	O
and	O
c	O
lund	O
hardness	O
of	O
approximations	O
in	O
approximation	B
algorithms	O
for	O
np-hard	O
problems	O
pages	O
pws	O
publishing	O
co	O
boston	O
ma	O
usa	O
f	O
r	O
bach	O
and	O
m	O
i	O
jordan	O
thin	B
junction	O
trees	O
in	O
t	O
g	O
dietterich	O
s	O
becker	O
and	O
z	O
ghahramani	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
f	O
r	O
bach	O
and	O
m	O
i	O
jordan	O
a	O
probabilistic	B
interpretation	O
of	O
canonical	B
correlation	I
analysis	B
computer	O
science	O
division	O
and	O
department	O
of	O
statistics	O
university	O
of	O
california	O
berkeley	O
berkeley	O
usa	O
y	O
bar-shalom	O
and	O
xiao-rong	O
li	O
estimation	O
and	O
tracking	O
principles	O
techniques	O
and	O
software	O
artech	O
house	O
norwood	O
ma	O
d	O
barber	O
dynamic	B
bayesian	B
networks	O
with	O
deterministic	B
tables	O
in	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
d	O
barber	O
learning	B
in	O
spiking	O
neural	O
assemblies	O
in	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
bibliography	O
bibliography	O
d	O
barber	O
are	O
two	O
classifiers	O
performing	O
equally	O
a	O
treatment	O
using	O
bayesian	B
hypothesis	B
testing	I
idiap	O
rr	O
idiap	O
rue	O
de	O
simplon	O
martigny	O
switerland	O
may	O
idiap-rr	O
d	O
barber	O
expectation	B
correction	I
for	O
smoothing	B
in	O
switching	B
linear	B
gaussian	B
state	O
space	O
models	O
journal	O
of	O
machine	O
learning	B
research	O
d	O
barber	O
clique	B
matrices	O
for	O
statistical	O
graph	B
decomposition	B
and	O
parameterising	O
restricted	B
positive	O
in	O
d	O
a	O
mcallester	O
and	O
p	O
myllymaki	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
definite	O
matrices	O
number	O
pages	O
corvallis	O
oregon	O
usa	O
auai	O
press	O
d	O
barber	O
and	O
f	O
v	O
agakov	O
correlated	O
sequence	B
learning	B
in	O
a	O
network	O
of	O
spiking	O
neurons	O
using	O
maximum	B
likelihood	B
informatics	O
research	O
reports	O
edinburgh	O
university	O
d	O
barber	O
and	O
f	O
v	O
agakov	O
the	O
im	O
algorithm	B
a	O
variational	B
approach	B
to	O
information	O
maximization	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
d	O
barber	O
and	O
c	O
m	O
bishop	O
bayesian	B
model	B
comparison	O
by	O
monte	O
carlo	O
chaining	O
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petsche	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
d	O
barber	O
and	O
c	O
m	O
bishop	O
ensemble	O
learning	B
in	O
bayesian	B
neural	O
networks	O
machine	O
learning	B
pages	O
springer	O
in	O
neural	O
networks	O
and	O
d	O
barber	O
and	O
s	O
chiappa	O
unified	O
inference	B
for	O
variational	O
bayesian	B
linear	B
gaussian	B
state-space	O
models	O
in	O
b	O
sch	O
olkopf	O
j	O
platt	O
and	O
t	O
hoffman	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
d	O
barber	O
and	O
w	O
wiegerinck	O
tractable	O
variational	O
structures	O
for	O
approximating	O
graphical	O
models	O
in	O
m	O
s	O
kearns	O
s	O
a	O
solla	O
and	O
d	O
a	O
cohn	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
d	O
barber	O
and	O
c	O
k	O
i	O
williams	O
gaussian	B
processes	O
for	O
bayesian	B
classification	B
via	O
hybrid	B
monte	I
carlo	I
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petsche	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
nips	O
pages	O
cambridge	O
ma	O
mit	O
press	O
r	O
j	O
baxter	O
exactly	O
solved	O
models	O
in	O
statistical	O
mechanics	O
academic	O
press	O
m	O
j	O
beal	O
f	O
falciani	O
z	O
ghahramani	O
c	O
rangel	O
and	O
d	O
l	O
wild	O
a	O
bayesian	B
approach	B
to	O
reconstructing	O
genetic	O
regulatory	O
networks	O
with	O
hidden	B
factors	O
bioinformatics	B
a	O
becker	O
and	O
d	O
geiger	O
a	O
sufficiently	O
fast	O
algorithm	B
for	O
finding	O
close	O
to	O
optimal	O
clique	B
trees	O
artificial	O
intelligence	O
a	O
j	O
bell	O
and	O
t	O
j	O
sejnowski	O
an	O
information-maximization	O
approach	B
to	O
blind	O
separation	B
and	O
blind	O
deconvolution	O
neural	B
computation	I
r	O
e	O
bellman	O
dynamic	B
programming	O
princeton	O
university	O
press	O
princeton	O
nj	O
paperback	O
edition	O
by	O
dover	O
publications	O
y	O
bengio	O
and	O
p	O
frasconi	O
input-output	B
hmms	O
for	O
sequence	O
processing	O
ieee	O
trans	O
neural	O
networks	O
a	O
l	O
berger	O
s	O
d	O
della	O
pietra	O
and	O
v	O
j	O
d	O
della	O
pietra	O
a	O
maximum	O
entropy	B
approach	B
to	O
natural	B
language	O
processing	O
computational	O
linguistics	O
j	O
o	O
berger	O
statistical	O
decision	B
theory	I
and	O
bayesian	B
analysis	B
springer	O
second	O
edition	O
d	O
p	O
bertsekas	O
dynamic	B
programming	O
and	O
optimal	O
control	O
athena	O
scientific	O
second	O
edition	O
j	O
besag	O
spatial	O
interactions	O
and	O
the	O
statistical	O
analysis	B
of	O
lattice	O
systems	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
j	O
besag	O
on	O
the	O
statistical	O
analysis	B
of	O
dirty	O
pictures	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
j	O
besag	O
and	O
p	O
green	O
spatial	O
statistics	O
and	O
bayesian	B
computation	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
draft	O
march	O
bibliography	O
bibliography	O
g	O
j	O
bierman	O
measurement	O
updating	O
using	O
the	O
u-d	O
factorization	O
automatica	O
n	O
l	O
biggs	O
discrete	B
mathematics	O
oxford	O
university	O
press	O
k	O
binder	O
and	O
a	O
p	O
young	O
spin	O
glasses	O
experimental	O
facts	O
theoretical	O
concepts	O
and	O
open	O
questions	O
rev	O
mod	O
phys	O
oct	O
c	O
m	O
bishop	O
neural	O
networks	O
for	O
pattern	O
recognition	O
oxford	O
university	O
press	O
c	O
m	O
bishop	O
pattern	O
recognition	O
and	O
machine	O
learning	B
springer	O
c	O
m	O
bishop	O
and	O
m	O
svens	O
en	O
bayesian	B
hierarchical	O
mixtures	O
of	O
experts	O
in	O
u	O
kjaerulff	O
and	O
c	O
meek	O
editors	O
proceedings	O
nineteenth	O
conference	O
on	O
uncertainty	B
in	O
artificial	O
intelligence	O
pages	O
morgan	O
kaufmann	O
d	O
blei	O
a	O
ng	O
and	O
m	O
jordan	O
latent	B
dirichlet	B
allocation	I
journal	O
of	O
machine	O
learning	B
research	O
r	O
r	O
bouckaert	O
bayesian	B
belief	B
networks	I
from	O
construction	B
to	O
inference	B
phd	O
thesis	O
university	O
of	O
utrecht	O
s	O
boyd	O
and	O
l	O
vandenberghe	O
convex	O
optimization	O
cambridge	O
university	O
press	O
y	O
boykov	O
and	O
v	O
kolmogorov	O
an	O
experimental	O
comparison	O
of	O
min-cutmax-flow	O
algorithms	O
for	O
energy	B
minimization	O
in	O
vision	O
ieee	O
trans	O
pattern	O
anal	O
mach	O
intell	O
y	O
boykov	O
o	O
veksler	O
and	O
r	O
zabih	O
fast	O
approximate	B
energy	B
minimization	O
via	O
graph	B
cuts	O
ieee	O
trans	O
pattern	O
anal	O
mach	O
intell	O
m	O
brand	O
incremental	O
singular	B
value	B
decomposition	B
of	O
uncertain	B
data	O
with	O
missing	B
values	O
conference	O
on	O
computer	O
vision	O
pages	O
in	O
european	O
j	O
breese	O
and	O
d	O
heckerman	O
decision-theoretic	O
troubleshooting	O
a	O
framework	O
for	O
repair	O
and	O
experiment	O
in	O
e	O
horvitz	O
and	O
f	O
jensen	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
h	O
bunke	O
and	O
t	O
caelli	O
hidden	B
markov	O
models	O
applications	O
in	O
computer	O
vision	O
machine	O
perception	O
and	O
artificial	O
intelligence	O
world	O
scientific	O
publishing	O
co	O
inc	O
river	O
edge	O
nj	O
usa	O
w	O
buntine	O
theory	O
refinement	O
on	O
bayesian	B
networks	O
in	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
a	O
cano	O
and	O
s	O
moral	O
advances	O
in	O
intelligent	O
computing	O
ipmu	O
chapter	O
heuristic	O
algorithms	O
for	O
the	O
triangulation	B
of	O
graphs	O
pages	O
number	O
in	O
lectures	O
notes	O
in	O
computer	O
sciences	O
springer-verlag	O
o	O
capp	O
e	O
e	O
moulines	O
and	O
t	O
ryden	O
inference	B
in	O
hidden	B
markov	O
models	O
springer	O
new	O
york	O
e	O
castillo	O
j	O
m	O
gutierrez	O
and	O
a	O
s	O
hadi	O
expert	O
systems	O
and	O
probabilistic	B
network	O
models	O
springer	O
a	O
t	O
cemgil	O
bayesian	B
inference	B
in	O
non-negative	B
matrix	B
factorisation	I
models	O
technical	O
report	O
cuedf	O
university	O
of	O
cambridge	O
july	O
a	O
t	O
cemgil	O
b	O
kappen	O
and	O
d	O
barber	O
a	O
generative	B
model	B
for	O
music	O
transcription	O
ieee	O
transactions	O
on	O
audio	O
speech	O
and	O
language	O
processing	O
h	O
s	O
chang	O
m	O
c	O
fu	O
j	O
hu	O
and	O
s	O
i	O
marcus	O
simulation-based	O
algorithms	O
for	O
markov	O
decision	O
processes	O
springer	O
s	O
chiappa	O
and	O
d	O
barber	O
bayesian	B
linear	B
gaussian	B
state	O
space	O
models	O
for	O
biosignal	O
decomposition	B
signal	O
processing	O
letters	O
s	O
chib	O
and	O
m	O
dueker	O
non-markovian	O
regime	O
switching	B
with	O
endogenous	O
states	O
and	O
time-varying	B
state	O
strengths	O
econometric	O
society	O
north	O
american	O
summer	O
meetings	O
econometric	O
society	O
august	O
c	O
k	O
chow	O
and	O
c	O
n	O
liu	O
approximating	O
discrete	B
probability	O
distributions	O
with	O
dependence	O
trees	O
ieee	O
transactions	O
on	O
information	O
theory	O
draft	O
march	O
bibliography	O
bibliography	O
p	O
s	O
churchland	O
and	O
t	O
j	O
sejnowski	O
the	O
computational	O
brain	O
mit	O
press	O
cambridge	O
ma	O
usa	O
d	O
cohn	O
and	O
h	O
chang	O
learning	B
to	O
probabilistically	O
identify	O
authoritative	O
documents	O
in	O
p	O
langley	O
editor	O
international	O
conference	O
on	O
machine	O
learning	B
number	O
pages	O
morgan	O
kaufmann	O
d	O
cohn	O
and	O
t	O
hofmann	O
the	O
missing	B
link	O
a	O
probabilistic	B
model	B
of	O
document	O
content	O
and	O
hypertext	O
connectivity	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
a	O
c	O
c	O
coolen	O
r	O
k	O
uhn	O
and	O
p	O
sollich	O
theory	O
of	O
neural	O
information	O
processing	O
systems	O
oxford	O
university	O
press	O
g	O
f	O
cooper	O
and	O
e	O
herskovits	O
a	O
bayesian	B
method	O
for	O
the	O
induction	O
of	O
probabilistic	B
networks	O
from	O
data	O
machine	O
learning	B
a	O
corduneanu	O
and	O
c	O
m	O
bishop	O
variational	O
bayesian	B
model	B
selection	I
for	O
mixture	B
distributions	O
in	O
t	O
jaakkola	O
and	O
t	O
richardson	O
editors	O
artifcial	O
intelligence	O
and	O
statistics	O
pages	O
morgan	O
kaufmann	O
m	O
t	O
cover	O
and	O
j	O
a	O
thomas	O
elements	O
of	O
information	O
theory	O
wiley	O
r	O
g	O
cowell	O
a	O
p	O
dawid	O
s	O
l	O
lauritzen	O
and	O
d	O
j	O
spiegelhalter	O
probabilistic	B
networks	O
and	O
expert	O
systems	O
springer	O
d	O
r	O
cox	O
and	O
n	O
wermuth	O
multivariate	B
dependencies	O
chapman	O
and	O
hall	O
n	O
cristianini	O
and	O
j	O
shawe-taylor	O
an	O
introduction	O
to	O
support	O
vector	O
machines	O
cambridge	O
university	O
press	O
p	O
dangauthier	O
r	O
herbrich	O
t	O
minka	O
and	O
t	O
graepel	O
trueskill	B
through	O
time	O
revisiting	O
the	O
history	O
of	O
chess	O
in	O
b	O
sch	O
olkopf	O
j	O
platt	O
and	O
t	O
hoffman	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
h	O
a	O
david	O
the	O
method	O
of	O
paired	O
comparisons	O
oxford	O
university	O
press	O
new	O
york	O
a	O
p	O
dawid	O
influence	O
diagrams	O
for	O
causal	B
modelling	B
and	O
inference	B
international	O
statistical	O
review	O
a	O
p	O
dawid	O
and	O
s	O
l	O
lauritzen	O
hyper	B
markov	I
laws	O
in	O
the	O
statistical	O
analysis	B
of	O
decomposable	B
graphical	O
models	O
annals	O
of	O
statistics	O
p	O
dayan	O
and	O
l	O
f	O
abbott	O
theoretical	O
neuroscience	O
mit	O
press	O
p	O
dayan	O
and	O
g	O
e	O
hinton	O
using	O
expectation-maximization	O
for	O
reinforcement	B
learning	B
neural	O
computa	O
tion	O
t	O
de	O
bie	O
n	O
cristianini	O
and	O
r	O
rosipal	O
handbook	O
of	O
geometric	O
computing	O
applications	O
in	O
pattern	O
recognition	O
computer	O
vision	O
neuralcomputing	O
and	O
robotics	O
chapter	O
eigenproblems	O
in	O
pattern	O
recognition	O
springer-verlag	O
r	O
dechter	O
bucket	B
elimination	I
a	O
unifying	O
framework	O
for	O
probabilistic	B
inference	B
algorithms	O
in	O
e	O
horvitz	O
and	O
f	O
jensen	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
s	O
diederich	O
and	O
m	O
opper	O
learning	B
of	O
correlated	O
patterns	O
in	O
spin-glass	O
networks	O
by	O
local	B
learning	B
rules	O
physical	O
review	O
letters	O
r	O
diestel	O
graph	B
theory	O
springer	O
a	O
doucet	O
and	O
a	O
m	O
johansen	O
a	O
tutorial	O
on	O
particle	O
filtering	O
and	O
smoothing	B
fifteen	O
years	O
later	O
in	O
d	O
crisan	O
and	O
b	O
rozovsky	O
editors	O
oxford	O
handbook	O
of	O
nonlinear	O
filtering	O
oxford	O
university	O
press	O
r	O
o	O
duda	O
p	O
e	O
hart	O
and	O
d	O
g	O
stork	O
pattern	O
classification	B
wiley-interscience	O
publication	O
r	O
durbin	O
s	O
r	O
eddy	O
a	O
krogh	O
and	O
g	O
mitchison	O
biological	O
sequence	O
analysis	B
probabilistic	B
models	O
of	O
proteins	O
and	O
nucleic	O
acids	O
cambridge	O
university	O
press	O
a	O
d	O
uring	O
a	O
c	O
c	O
coolen	O
and	O
d	O
sherrington	O
phase	O
diagram	O
and	O
storage	O
capacity	B
of	O
sequence	O
processing	O
neural	O
networks	O
journal	O
of	O
physics	O
a	O
draft	O
march	O
bibliography	O
bibliography	O
j	O
m	O
gutierrez	O
e	O
castillo	O
and	O
a	O
s	O
hadi	O
expert	O
systems	O
and	O
probabilistic	B
network	O
models	O
springer	O
verlag	O
j	O
edmonds	O
and	O
r	O
m	O
karp	O
theoretical	O
improvements	O
in	O
algorithmic	O
efficiency	O
for	O
network	O
flow	O
problems	O
journal	O
of	O
the	O
acm	O
r	O
edwards	O
and	O
a	O
sokal	O
generalization	O
of	O
the	O
fortium-kasteleyn-swendson-wang	O
representation	O
and	O
monte	O
carlo	O
algorithm	B
physical	O
review	O
d	O
a	O
e	O
elo	B
the	O
rating	O
of	O
chess	O
players	O
past	O
and	O
present	O
arco	O
new	O
york	O
second	O
edition	O
y	O
ephraim	O
and	O
w	O
j	O
j	O
roberts	O
revisiting	O
autoregressive	O
hidden	B
markov	O
modeling	O
of	O
speech	O
signals	O
ieee	O
signal	O
processing	O
letters	O
february	O
e	O
erosheva	O
s	O
fienberg	O
and	O
j	O
lafferty	O
mixed	B
membership	I
models	O
of	O
scientific	O
publications	O
in	O
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
volume	O
pages	O
r-e	O
fan	O
p-h	O
chen	O
and	O
c-j	O
lin	O
working	O
set	O
selection	O
using	O
second	O
order	O
information	O
for	O
training	B
support	O
vector	O
machines	O
journal	O
of	O
machine	O
learning	B
research	O
p	O
fearnhead	O
exact	O
and	O
efficient	O
bayesian	B
inference	B
for	O
multiple	O
changepoint	B
problems	O
technical	O
report	O
deptartment	O
of	O
mathematics	O
and	O
statistics	O
lancaster	O
university	O
g	O
h	O
fischer	O
and	O
i	O
w	O
molenaar	O
rasch	B
models	O
foundations	O
recent	O
developments	O
and	O
applications	O
springer	O
new	O
york	O
m	O
e	O
fisher	O
statistical	O
mechanics	O
of	O
dimers	O
on	O
a	O
plane	O
lattice	O
physical	O
review	O
b	O
frey	O
extending	O
factor	B
graphs	O
as	O
to	O
unify	O
directed	B
and	O
undirected	B
graphical	O
models	O
in	O
c	O
meek	O
and	O
u	O
kj	O
rulff	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
morgan	O
kaufmann	O
n	O
friedman	O
d	O
geiger	O
and	O
m	O
goldszmidt	O
bayesian	B
network	O
classifiers	O
machine	O
learning	B
s	O
fr	O
uhwirth-schnatter	O
finite	O
mixture	B
and	O
markov	O
switching	B
models	O
springer	O
m	O
frydenberg	O
the	O
chain	B
graph	B
markov	O
property	O
scandanavian	O
journal	O
of	O
statistics	O
t	O
furmston	O
and	O
d	O
barber	O
solving	B
deterministic	B
policy	B
using	O
expectation-maximisation	O
and	O
antifreeze	B
in	O
e	O
suzuki	O
and	O
m	O
sebag	O
editors	O
european	O
conference	O
on	O
machine	O
learning	B
and	O
principles	O
and	O
practice	O
of	O
knowledge	O
discovery	O
in	O
databases	O
september	O
workshop	O
on	O
learning	B
and	O
data	O
mining	O
for	O
robotics	O
a	O
galka	O
o	O
yamashita	O
t	O
ozaki	O
r	O
biscay	O
and	O
p	O
valdes-sosa	O
a	O
solution	O
to	O
the	O
dynamical	O
inverse	O
problem	B
of	O
eeg	O
generation	O
using	O
spatiotemporal	O
kalman	O
filtering	O
neuroimage	O
p	O
gandhi	O
f	O
bromberg	O
and	O
d	O
margaritis	O
learning	B
markov	B
network	I
structure	B
using	O
few	O
independence	B
tests	O
in	O
proceedings	O
of	O
the	O
siam	O
international	O
conference	O
on	O
data	O
mining	O
pages	O
m	O
r	O
garey	O
and	O
d	O
s	O
johnson	O
computers	O
and	O
intractability	O
a	O
guide	O
to	O
the	O
theory	O
of	O
np-completeness	O
w	O
h	O
freeman	O
and	O
company	O
new	O
york	O
a	O
gelb	O
applied	O
optimal	O
estimation	O
mit	O
press	O
a	O
gelman	O
g	O
o	O
roberts	O
and	O
w	O
r	O
gilks	O
efficient	O
metropolis	O
jumping	O
rules	O
in	O
j	O
o	O
bernardo	O
j	O
m	O
berger	O
a	O
p	O
dawid	O
and	O
a	O
f	O
m	O
smith	O
editors	O
bayesian	B
statistics	O
volume	O
pages	O
oxford	O
university	O
press	O
s	O
geman	O
and	O
d	O
geman	O
stochastic	O
relaxation	O
gibbs	B
distributions	O
and	O
the	O
bayesian	B
restoration	O
of	O
images	O
in	O
readings	O
in	O
uncertain	B
reasoning	O
pages	O
san	O
francisco	O
ca	O
usa	O
morgan	O
kaufmann	O
publishers	O
inc	O
m	O
g	O
genton	O
classes	O
of	O
kernels	O
for	O
machine	O
learning	B
a	O
statistics	O
perspective	O
journal	O
of	O
machine	O
learning	B
research	O
w	O
gerstner	O
and	O
w	O
m	O
kistler	O
spiking	O
neuron	O
models	O
cambridge	O
university	O
press	O
draft	O
march	O
bibliography	O
bibliography	O
z	O
ghahramani	O
and	O
m	O
j	O
beal	O
variational	B
inference	B
for	O
bayesian	B
mixtures	O
of	O
factor	B
analysers	O
in	O
s	O
a	O
solla	O
t	O
k	O
leen	O
and	O
k-r	O
m	O
uller	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
z	O
ghahramani	O
and	O
g	O
e	O
hinton	O
variational	O
learning	B
for	O
switching	B
state-space	O
models	O
neural	B
computation	I
a	O
gibbons	O
algorithmic	O
graph	B
theory	O
cambridge	O
university	O
press	O
w	O
r	O
gilks	O
s	O
richardson	O
and	O
d	O
j	O
spiegelhalter	O
markov	B
chain	B
monte	I
carlo	I
in	O
practice	O
chapman	O
hall	O
m	O
girolami	O
and	O
a	O
kaban	O
on	O
an	O
equivalence	O
between	O
plsi	O
and	O
lda	O
in	O
proceedings	O
of	O
the	O
annual	O
international	O
acm	O
sigir	O
conference	O
on	O
research	O
and	O
development	O
in	O
information	B
retrieval	I
pages	O
new	O
york	O
ny	O
usa	O
acm	O
press	O
m	O
e	O
glickman	O
parameter	B
estimation	O
in	O
large	O
dynamic	B
paired	O
comparison	O
experiments	O
applied	O
statistics	O
a	O
globerson	O
and	O
t	O
jaakkola	O
approximate	B
inference	B
using	O
planar	O
graph	B
decomposition	B
in	O
b	O
sch	O
olkopf	O
j	O
platt	O
and	O
t	O
hoffman	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
d	O
goldberg	O
d	O
nichols	O
b	O
m	O
oki	O
and	O
d	O
terry	O
using	O
collaborative	B
filtering	I
to	O
weave	O
an	O
information	O
tapestry	O
communications	O
acm	O
g	O
h	O
golub	O
and	O
c	O
f	O
van	O
loan	O
matrix	B
computations	O
johns	O
hopkins	O
university	O
press	O
edition	O
m	O
c	O
golumbic	O
and	O
i	O
ben-arroyo	O
hartman	O
graph	B
theory	O
combinatorics	O
and	O
algorithms	O
springer-verlag	O
c	O
goutis	O
a	O
graphical	O
method	O
for	O
solving	B
a	O
decision	O
analysis	B
problem	B
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
p	O
j	O
green	O
and	O
b	O
w	O
silverman	O
nonparametric	O
regression	B
and	O
generalized	O
linear	B
models	O
volume	O
of	O
monographs	O
on	O
statistics	O
and	O
applied	O
probability	O
chapman	O
and	O
hall	O
d	O
m	O
greig	O
b	O
t	O
porteous	O
and	O
a	O
h	O
seheult	O
exact	O
maximum	O
a	O
posteriori	O
estimation	O
for	O
binary	O
images	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
g	O
grimmett	O
and	O
d	O
stirzaker	O
probability	O
and	O
random	O
processes	O
oxford	O
university	O
press	O
second	O
edition	O
s	O
f	O
gull	O
bayesian	B
data	O
analysis	B
straight-line	O
fitting	O
in	O
j	O
skilling	O
editor	O
maximum	O
entropy	B
and	O
bayesian	B
methods	O
pages	O
kluwer	O
a	O
k	O
gupta	O
and	O
d	O
k	O
nagar	O
matrix	B
variate	O
distributions	O
chapman	O
and	O
hallcrc	O
boca	O
raton	O
florida	O
usa	O
d	O
j	O
hand	O
and	O
k	O
yu	O
idiot	O
s	O
bayes	O
not	O
so	O
stupid	O
after	O
all	O
international	O
statistical	O
review	O
d	O
r	O
hardoon	O
s	O
szedmak	O
and	O
j	O
shawe-taylor	O
canonical	B
correlation	I
analysis	B
an	O
overview	O
with	O
appli	O
cation	O
to	O
learning	B
methods	O
neural	B
computation	I
d	O
o	O
hebb	B
the	O
organization	O
of	O
behavior	O
wiley	O
new	O
york	O
d	O
heckerman	O
a	O
tutorial	O
on	O
learning	B
with	O
bayesian	B
networks	O
technical	O
report	O
microsoft	O
research	O
redmond	O
wa	O
march	O
revised	O
november	O
d	O
heckerman	O
d	O
geiger	O
and	O
d	O
chickering	O
learning	B
bayesian	B
networks	O
the	O
combination	O
of	O
knowledge	O
and	O
statistical	O
data	O
machine	O
learning	B
r	O
herbrich	O
t	O
minka	O
and	O
t	O
graepel	O
trueskilltm	O
a	O
bayesian	B
skill	O
rating	O
system	O
in	O
b	O
sch	O
olkopf	O
j	O
platt	O
and	O
t	O
hoffman	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
h	O
hermansky	O
should	O
recognizers	O
have	O
ears	O
speech	O
communication	O
draft	O
march	O
bibliography	O
bibliography	O
j	O
hertz	O
a	O
krogh	O
and	O
r	O
palmer	O
introduction	O
to	O
the	O
theory	O
of	O
neural	B
computation	I
addison-wesley	O
t	O
heskes	O
convexity	O
arguments	O
for	O
efficient	O
minimization	O
of	O
the	O
bethe	O
and	O
kikuchi	B
free	O
energies	O
journal	O
of	O
artificial	O
intelligence	O
research	O
d	O
m	O
higdon	O
auxiliary	B
variable	I
methods	O
for	O
markov	B
chain	B
monte	I
carlo	I
with	O
applications	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
g	O
e	O
hinton	O
and	O
r	O
r	O
salakhutdinov	O
reducing	O
the	O
dimensionality	O
of	O
data	O
with	O
neural	O
networks	O
science	O
t	O
hofmann	O
j	O
puzicha	O
and	O
m	O
i	O
jordan	O
learning	B
from	O
dyadic	B
data	I
in	O
m	O
s	O
kearns	O
s	O
a	O
solla	O
and	O
d	O
a	O
cohn	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
cambridge	O
ma	O
mit	O
press	O
r	O
a	O
howard	O
and	O
j	O
e	O
matheson	O
influence	O
diagrams	O
decision	O
analysis	B
republished	O
version	O
of	O
the	O
original	O
report	O
a	O
hyv	O
arinen	O
j	O
karhunen	O
and	O
e	O
oja	O
independent	O
component	O
analysis	B
wiley	O
aapo	O
hyv	O
arinen	O
consistency	O
of	O
pseudolikelihood	O
estimation	O
of	O
fully	O
visible	B
boltzmann	O
machines	O
neural	B
computation	I
m	O
isard	O
and	O
a	O
blake	O
condensation	O
conditional	B
density	B
propagation	B
for	O
visual	O
tracking	O
international	O
journal	O
of	O
computer	O
vision	O
t	O
s	O
jaakkola	O
and	O
m	O
i	O
jordan	O
variational	O
probabilistic	B
inference	B
and	O
the	O
qmr-dt	O
network	O
journal	O
of	O
artificial	O
intelligence	O
research	O
t	O
s	O
jaakkola	O
and	O
m	O
i	O
jordan	O
bayesian	B
parameter	B
estimation	O
via	O
variational	O
methods	O
statistics	O
and	O
computing	O
r	O
a	O
jacobs	O
f	O
peng	O
and	O
m	O
a	O
tanner	O
a	O
bayesian	B
approach	B
to	O
model	B
selection	I
in	O
hierarchical	O
mixtures	O
of-experts	O
architectures	O
neural	O
networks	O
r	O
g	O
jarrett	O
a	O
note	O
on	O
the	O
intervals	O
between	O
coal-mining	O
disasters	O
biometrika	O
e	O
t	O
jaynes	O
probability	O
theory	O
the	O
logic	O
of	O
science	O
cambridge	O
university	O
press	O
f	O
jensen	O
f	O
v	O
jensen	O
and	O
d	O
dittmer	O
from	O
influence	O
diagrams	O
to	O
junction	O
trees	O
in	O
proceedings	O
of	O
the	O
annual	O
conference	O
on	O
uncertainty	B
in	O
artificial	O
intelligence	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
f	O
v	O
jensen	O
and	O
f	O
jensen	O
optimal	O
junction	O
trees	O
in	O
r	O
lopez	O
de	O
mantaras	O
and	O
d	O
poole	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
f	O
v	O
jensen	O
and	O
t	O
d	O
nielson	O
bayesian	B
networks	O
and	O
decision	O
graphs	O
springer	O
verlag	O
second	O
edition	O
m	O
i	O
jordan	O
and	O
r	O
a	O
jacobs	O
hierarchical	O
mixtures	O
of	O
experts	O
and	O
the	O
em	B
algorithm	B
neural	B
computation	I
b	O
h	O
juang	O
w	O
chou	O
and	O
c	O
h	O
lee	O
minimum	O
classification	B
error	O
rate	O
methods	O
for	O
speech	B
recognition	I
ieee	O
transactions	O
on	O
speech	O
and	O
audio	O
processing	O
l	O
p	O
kaelbling	O
m	O
l	O
littman	O
and	O
a	O
r	O
cassandra	O
planning	B
and	O
acting	O
in	O
partially	B
observable	I
stochastic	O
domains	O
artificial	O
intelligence	O
h	O
j	O
kappen	O
an	O
introduction	O
to	O
stochastic	O
control	O
theory	O
path	B
integrals	O
and	O
reinforcement	B
learning	B
in	O
proceedings	O
granada	O
seminar	O
on	O
computational	O
physics	O
computational	O
and	O
mathematical	O
modeling	O
of	O
cooperative	O
behavior	O
in	O
neural	O
systems	O
volume	O
pages	O
american	O
institute	O
of	O
physics	O
h	O
j	O
kappen	O
and	O
f	O
b	O
rodr	O
guez	O
efficient	O
learning	B
in	O
boltzmann	O
machines	O
using	O
linear	B
response	O
theory	O
neural	O
compution	O
h	O
j	O
kappen	O
and	O
w	O
wiegerinck	O
novel	O
iteration	B
schemes	O
for	O
the	O
cluster	B
variation	I
method	I
in	O
t	O
g	O
dietterich	O
s	O
becker	O
and	O
z	O
ghahramani	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
draft	O
march	O
bibliography	O
bibliography	O
y	O
karklin	O
and	O
m	O
s	O
lewicki	O
emergence	O
of	O
complex	O
cell	O
properties	B
by	O
learning	B
to	O
generalize	O
in	O
natural	B
scenes	O
nature	O
november	O
g	O
karypis	O
and	O
v	O
kumar	O
a	O
fast	O
and	O
high	O
quality	O
multilevel	O
scheme	O
for	O
partitioning	O
irregular	O
graphs	O
siam	O
journal	O
on	O
scientific	O
computing	O
p	O
w	O
kasteleyn	O
dimer	O
statistics	O
and	O
phase	O
transitions	O
journal	O
of	O
mathematical	O
physics	O
s	O
a	O
kauffman	O
at	O
home	O
in	O
the	O
universe	O
the	O
search	O
for	O
laws	O
of	O
self-organization	O
and	O
complexity	O
oxford	O
university	O
press	O
oxford	O
uk	O
c-j	O
kim	O
dynamic	B
linear	B
models	O
with	O
markov-switching	O
journal	O
of	O
econometrics	O
c-j	O
kim	O
and	O
c	O
r	O
nelson	O
state-space	O
models	O
with	O
regime	O
switching	B
mit	O
press	O
g	O
kitagawa	O
the	O
two-filter	O
formula	O
for	O
smoothing	B
and	O
an	O
implementation	O
of	O
the	O
gaussian-sum	O
smoother	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
u	O
b	O
kjaerulff	O
and	O
a	O
l	O
madsen	O
bayesian	B
networks	O
and	O
influence	O
diagrams	O
a	O
guide	O
to	O
construction	B
and	O
analysis	B
springer	O
a	O
krogh	O
m	O
brown	O
i	O
mian	O
k	O
sjolander	O
and	O
d	O
haussler	O
hidden	B
markov	O
models	O
in	O
computational	O
biology	O
applications	O
to	O
protein	O
modeling	O
journal	O
of	O
molecular	O
biology	O
s	O
kullback	O
information	O
theory	O
and	O
statistics	O
dover	O
k	O
kurihara	O
m	O
welling	O
and	O
y	O
w	O
teh	O
collapsed	O
variational	O
dirichlet	B
process	I
mixture	B
models	I
in	O
proceedings	O
of	O
the	O
international	O
joint	B
conference	O
on	O
artificial	O
intelligence	O
volume	O
pages	O
j	O
lafferty	O
a	O
mccallum	O
and	O
f	O
pereira	O
conditional	B
random	O
fields	O
probabilistic	B
models	O
for	O
segmenting	O
and	O
labeling	O
sequence	O
data	O
in	O
c	O
e	O
brodley	O
and	O
a	O
p	O
danyluk	O
editors	O
international	O
conference	O
on	O
machine	O
learning	B
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
h	O
lass	O
elements	O
of	O
pure	O
and	O
applied	O
mathematics	O
mcgraw-hill	O
by	O
dover	O
s	O
l	O
lauritzen	O
graphical	O
models	O
oxford	O
university	O
press	O
s	O
l	O
lauritzen	O
a	O
p	O
dawid	O
b	O
n	O
larsen	O
and	O
h-g	O
leimer	O
independence	B
properties	B
of	O
directed	B
markov	O
fields	O
networks	O
s	O
l	O
lauritzen	O
and	O
d	O
j	O
spiegelhalter	O
local	B
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
journal	O
of	O
royal	O
statistical	O
society	O
b	O
d	O
d	O
lee	O
and	O
h	O
s	O
seung	O
algorithms	O
for	O
non-negative	O
matrix	B
factorization	O
in	O
t	O
k	O
leen	O
t	O
g	O
dietterich	O
and	O
v	O
tresp	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
m	O
a	O
r	O
leisink	O
and	O
h	O
j	O
kappen	O
a	O
tighter	O
bound	B
for	O
graphical	O
models	O
in	O
neural	B
computation	I
volume	O
pages	O
mit	O
press	O
v	O
lepar	O
and	O
p	O
p	O
shenoy	O
a	O
comparison	O
of	O
lauritzen-spiegelhalter	O
hugin	O
and	O
shenoy-shafer	O
architectures	O
for	O
computing	O
marginals	O
of	O
probability	O
distributions	O
in	O
g	O
cooper	O
and	O
s	O
moral	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
u	O
lerner	O
r	O
parr	O
d	O
koller	O
and	O
g	O
biswas	O
bayesian	B
fault	O
detection	O
and	O
diagnosis	O
in	O
dynamic	B
systems	O
in	O
proceedings	O
of	O
the	O
seventeenth	O
national	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
u	O
n	O
lerner	O
hybrid	O
bayesian	B
networks	O
for	O
reasoning	O
about	O
complex	O
systems	O
computer	O
science	O
department	O
stanford	O
university	O
r	O
linsker	O
improved	O
local	B
learning	B
rule	O
for	O
information	O
maximization	O
and	O
related	O
applications	O
neural	O
networks	O
y	O
l	O
loh	O
e	O
w	O
carlson	O
and	O
m	O
y	O
j	O
tan	O
bond-propagation	O
algorithm	B
for	O
thermodynamic	O
functions	O
in	O
general	O
two-dimensional	O
ising	O
models	O
physical	O
review	O
b	O
h	O
lopes	O
and	O
m	O
west	O
bayesian	B
model	B
assessment	O
in	O
factor	B
analysis	B
statistica	O
sinica	O
draft	O
march	O
bibliography	O
bibliography	O
t	O
j	O
loredo	O
from	O
laplace	B
to	O
supernova	O
sn	O
bayesian	B
inference	B
in	O
astrophysics	O
in	O
p	O
f	O
fougere	O
editor	O
maximum	O
entropy	B
and	O
bayesian	B
methods	O
pages	O
kluwer	O
d	O
j	O
c	O
mackay	O
bayesian	B
interpolation	O
neural	B
computation	I
d	O
j	O
c	O
mackay	O
probable	O
networks	O
and	O
plausisble	O
predictions	O
a	O
review	O
of	O
practical	O
bayesian	B
methods	O
for	O
supervised	B
neural	O
networks	O
network	O
computation	O
in	O
neural	O
systems	O
d	O
j	O
c	O
mackay	O
introduction	O
to	O
gaussian	B
processes	O
in	O
neural	O
networks	O
and	O
machine	O
learning	B
volume	O
of	O
nato	O
advanced	O
study	O
institute	O
on	O
generalization	O
in	O
neural	O
networks	O
and	O
machine	O
learning	B
pages	O
springer	O
august	O
d	O
j	O
c	O
mackay	O
information	O
theory	O
inference	B
and	O
learning	B
algorithms	O
cambridge	O
university	O
press	O
u	O
madhow	O
fundamentals	O
of	O
digital	O
communication	O
cambridge	O
university	O
press	O
k	O
v	O
mardia	O
j	O
t	O
kent	O
and	O
j	O
m	O
bibby	O
multivariate	B
analysis	B
academic	O
press	O
h	O
markram	O
j	O
lubke	O
m	O
frotscher	O
and	O
b	O
sakmann	O
regulation	O
of	O
synaptic	O
efficacy	O
by	O
coincidence	O
of	O
postsynaptic	O
aps	O
and	O
epsps	O
science	O
g	O
mclachlan	O
and	O
t	O
krishnan	O
the	O
em	B
algorithm	B
and	O
extensions	O
john	O
wiley	O
and	O
sons	O
g	O
mclachlan	O
and	O
d	O
peel	O
finite	O
mixture	B
models	O
wiley	O
series	O
in	O
probability	O
and	O
statistics	O
wiley	O
interscience	O
e	O
meeds	O
z	O
ghahramani	O
r	O
m	O
neal	O
and	O
s	O
t	O
roweis	O
modeling	O
dyadic	B
data	I
with	O
binary	O
latent	B
factors	O
in	O
b	O
sch	O
olkopf	O
j	O
platt	O
and	O
t	O
hoffman	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pages	O
cambridge	O
ma	O
mit	O
press	O
m	O
meila	O
an	O
accelerated	O
chow	O
and	O
liu	O
algorithm	B
fitting	O
tree	B
distributions	O
to	O
high-dimensional	O
sparse	B
data	O
in	O
i	O
bratko	O
editor	O
international	O
conference	O
on	O
machine	O
learning	B
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
m	O
meila	O
and	O
m	O
i	O
jordan	O
triangulation	B
by	O
continuous	B
embedding	O
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petsche	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
b	O
mesot	O
and	O
d	O
barber	O
switching	B
linear	B
dynamical	O
systems	O
for	O
noise	O
robust	O
speech	B
recognition	I
ieee	O
transactions	O
of	O
audio	O
speech	O
and	O
language	O
processing	O
n	O
meuleau	O
m	O
hauskrecht	O
k-e	O
kim	O
l	O
peshkin	O
kaelbling	O
l	O
p	O
t	O
dean	O
and	O
c	O
boutilier	O
solving	B
very	O
large	O
weakly	O
coupled	B
markov	O
decision	O
processes	O
in	O
proceedings	O
of	O
the	O
fifteenth	O
national	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
t	O
mills	O
the	O
econometric	O
modelling	B
of	O
financial	O
time	O
series	O
cambridge	O
university	O
press	O
t	O
minka	O
expectation	B
propagation	B
for	O
approximate	B
bayesian	B
inference	B
in	O
j	O
breese	O
and	O
d	O
koller	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
t	O
minka	O
a	O
comparison	O
of	O
numerical	B
optimizers	O
for	O
logistic	B
regression	B
technical	O
report	O
microsoft	O
research	O
research	O
microsoft	O
com	O
minkapaperslogreg	O
t	O
minka	O
divergence	B
measures	O
and	O
message	B
passing	B
technical	O
report	O
microsoft	O
research	O
ltd	O
cambridge	O
uk	O
december	O
a	O
mira	O
j	O
m	O
ller	O
and	O
g	O
o	O
roberts	O
perfect	O
slice	O
samplers	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
methodology	O
c	O
mitchell	O
m	O
harper	O
and	O
l	O
jamieson	O
on	O
the	O
complexity	O
of	O
explicit	O
duration	O
hmm	B
s	O
speech	O
and	O
audio	O
processing	O
ieee	O
transactions	O
on	O
may	O
t	O
mitchell	O
machine	O
learning	B
mcgraw-hill	O
j	O
mooij	O
and	O
h	O
j	O
kappen	O
sufficient	O
conditions	O
for	O
convergence	O
of	O
loopy	B
belief	B
propagation	B
ieee	O
infor	O
mation	O
theory	O
a	O
moore	O
a	O
tutorial	O
httpwww	O
cs	O
cmu	O
edu	O
awmpapers	O
html	O
on	O
kd-trees	O
technical	O
report	O
available	O
from	O
draft	O
march	O
bibliography	O
bibliography	O
j	O
moussouris	O
gibbs	B
and	O
markov	O
random	O
systems	O
with	O
constraints	O
journal	O
of	O
statistical	O
physics	O
r	O
m	O
neal	O
connectionist	O
learning	B
of	O
belief	B
networks	I
artificial	O
intelligence	O
r	O
m	O
neal	O
probabilistic	B
inference	B
using	O
markov	B
chain	B
monte	I
carlo	I
methods	O
dept	O
of	O
computer	O
science	O
university	O
of	O
toronto	O
r	O
m	O
neal	O
markov	B
chain	B
sampling	B
methods	O
for	O
dirichlet	B
process	I
mixture	B
models	I
journal	O
of	O
computational	O
and	O
graphical	O
statistics	O
r	O
m	O
neal	O
slice	B
sampling	B
annals	O
of	O
statistics	O
r	O
e	O
neapolitan	O
learning	B
bayesian	B
networks	O
prentice	O
hall	O
a	O
v	O
nefian	O
luhong	O
l	O
xiaobo	O
p	O
liu	O
x	O
c	O
mao	O
and	O
k	O
murphy	O
a	O
coupled	B
hmm	B
for	O
audio-visual	O
speech	B
recognition	I
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
speech	O
and	O
signal	O
processing	O
volume	O
pages	O
d	O
nilsson	O
an	O
efficient	O
algorithm	B
for	O
finding	O
the	O
m	O
most	O
probable	O
configurations	O
in	O
a	O
probabilistic	B
expert	O
system	O
statistics	O
and	O
computing	O
d	O
nilsson	O
and	O
j	O
goldberger	O
sequentially	O
finnding	O
the	O
n-best	O
list	O
in	O
hidden	B
markov	O
models	O
internation	O
joint	B
conference	O
on	O
artificial	O
intelligence	O
a	O
b	O
novikoff	O
on	O
convergence	O
proofs	O
on	O
perceptrons	O
in	O
symposium	O
on	O
the	O
mathematical	O
theory	O
of	O
automata	O
york	O
volume	O
pages	O
brooklyn	O
n	O
y	O
polytechnic	O
press	O
of	O
polytechnic	O
institute	O
of	O
brooklyn	O
f	O
j	O
och	O
and	O
h	O
ney	O
discriminative	B
training	B
and	O
maximum	O
entropy	B
models	O
for	O
statistical	O
machine	O
translation	O
in	O
proceedings	O
of	O
the	O
annual	O
meeting	O
of	O
the	O
association	O
for	O
computational	O
linguistics	O
pages	O
philadelphia	O
july	O
b	O
a	O
olshausen	O
and	O
d	O
j	O
field	O
sparse	B
coding	O
with	O
an	O
overcomplete	O
basis	O
set	O
a	O
strategy	O
employed	O
by	O
vision	O
research	O
a	O
v	O
oppenheim	O
r	O
w	O
shafer	O
m	O
t	O
yoder	O
and	O
w	O
t	O
padgett	O
discrete-time	O
signal	O
processing	O
prentice	O
hall	O
third	O
edition	O
m	O
ostendorf	O
v	O
digalakis	O
and	O
o	O
a	O
kimball	O
from	O
hmms	O
to	O
segment	O
models	O
a	O
unified	O
view	O
of	O
stochastic	O
modeling	O
for	O
speech	B
recognition	I
ieee	O
transactions	O
on	O
speech	O
and	O
audio	O
processing	O
p	O
paatero	O
and	O
u	O
tapper	O
positive	O
matrix	B
factorization	O
a	O
non-negative	O
factor	B
model	B
with	O
optimal	O
utilization	O
of	O
error	O
estimates	O
of	O
data	O
values	O
environmetrics	O
v	O
pavlovic	O
j	O
m	O
rehg	O
and	O
j	O
maccormick	O
learning	B
switching	B
linear	B
models	O
of	O
human	O
motion	O
in	O
t	O
k	O
leen	O
t	O
g	O
dietterich	O
and	O
v	O
tresp	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
j	O
pearl	O
probabilistic	B
reasoning	O
in	O
intelligent	O
systems	O
networks	O
of	O
plausible	O
inference	B
morgan	O
kaufmann	O
j	O
pearl	O
causality	B
models	O
reasoning	O
and	O
inference	B
cambridge	O
university	O
press	O
b	O
a	O
pearlmutter	O
and	O
l	O
c	O
parra	O
maximum	B
likelihood	B
blind	O
source	O
separation	B
a	O
context-sensitive	O
generalization	O
of	O
ica	B
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petsche	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
k	O
b	O
petersen	O
and	O
o	O
winther	O
the	O
em	B
algorithm	B
in	O
independent	O
component	O
analysis	B
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
speech	O
and	O
signal	O
processing	O
volume	O
pages	O
j-p	O
pfister	O
t	O
toyiozumi	O
d	O
barber	O
and	O
w	O
gerstner	O
optimal	O
spike-timing	O
dependent	O
plasticity	O
for	O
precise	O
action	O
potential	B
firing	O
in	O
supervised	B
learning	B
neural	B
computation	I
j	O
platt	O
fast	O
training	B
of	O
support	O
vector	O
machines	O
using	O
sequential	B
minimal	O
optimization	O
in	O
b	O
sch	O
olkopf	O
c	O
j	O
c	O
burges	O
and	O
a	O
j	O
smola	O
editors	O
advances	O
in	O
kernel	B
methods	O
support	O
vector	O
learning	B
pages	O
mit	O
press	O
draft	O
march	O
bibliography	O
bibliography	O
i	O
porteous	O
d	O
newman	O
a	O
ihler	O
a	O
asuncion	O
p	O
smyth	O
and	O
m	O
welling	O
fast	O
collapsed	O
gibbs	B
sampling	B
for	O
latent	B
dirichlet	B
allocation	I
in	O
kdd	O
proceeding	O
of	O
the	O
acm	O
sigkdd	O
international	O
conference	O
on	O
knowledge	O
discovery	O
and	O
data	O
mining	O
pages	O
new	O
york	O
ny	O
usa	O
acm	O
j	O
e	O
potter	O
and	O
r	O
g	O
stern	O
statistical	O
filtering	O
of	O
space	O
navigation	O
measurements	O
in	O
american	O
institute	O
of	O
aeronautics	O
and	O
astronautics	O
guidance	O
and	O
control	O
conference	O
volume	O
pages	O
cambridge	O
mass	O
august	O
w	O
press	O
w	O
vettering	O
s	O
teukolsky	O
and	O
b	O
flannery	O
numerical	B
recipes	O
in	O
fortran	O
cambridge	O
university	O
press	O
s	O
j	O
d	O
prince	O
and	O
j	O
h	O
elder	O
probabilistic	B
linear	B
discriminant	I
analysis	B
for	O
inferences	O
about	O
identity	O
in	O
ieee	O
international	O
conference	O
on	O
computer	O
vision	O
iccv	O
pages	O
l	O
r	O
rabiner	O
a	O
tutorial	O
on	O
hidden	B
markov	O
models	O
and	O
selected	O
applications	O
in	O
speech	B
recognition	I
proc	O
of	O
the	O
ieee	O
c	O
e	O
rasmussen	O
and	O
c	O
k	O
i	O
williams	O
gaussian	B
processes	O
for	O
machine	O
learning	B
mit	O
press	O
h	O
e	O
rauch	O
g	O
tung	O
and	O
c	O
t	O
striebel	O
maximum	B
likelihood	B
estimates	O
of	O
linear	B
dynamic	B
systems	O
american	O
institute	O
of	O
aeronautics	O
and	O
astronautics	O
journal	O
t	O
richardson	O
and	O
p	O
spirtes	O
ancestral	B
graph	B
markov	O
models	O
annals	O
of	O
statistics	O
d	O
rose	O
r	O
e	O
tarjan	O
and	O
e	O
s	O
lueker	O
algorithmic	O
aspects	O
of	O
vertex	O
elimination	O
of	O
graphs	O
siam	O
journal	O
on	O
computing	O
f	O
rosenblatt	O
the	O
perceptron	B
a	O
probabilistic	B
model	B
for	O
information	O
storage	O
and	O
organization	O
in	O
the	O
brain	O
psychological	O
review	O
d	O
b	O
rubin	O
using	O
the	O
sir	O
algorithm	B
to	O
simulate	O
posterior	B
distributions	O
in	O
m	O
h	O
bernardo	O
k	O
m	O
degroot	O
d	O
v	O
lindley	O
and	O
a	O
f	O
m	O
smith	O
editors	O
bayesian	B
statistics	O
oxford	O
university	O
press	O
d	O
saad	O
and	O
m	O
opper	O
advanced	O
mean	B
field	O
methods	O
theory	O
and	O
practice	O
mit	O
press	O
r	O
salakhutdinov	O
s	O
roweis	O
and	O
z	O
ghahramani	O
optimization	O
with	O
em	B
and	O
expectation-conjugategradient	O
in	O
t	O
fawcett	O
and	O
n	O
mishra	O
editors	O
international	O
conference	O
on	O
machine	O
learning	B
number	O
pages	O
menlo	O
park	O
ca	O
aaai	O
press	O
l	O
k	O
saul	O
t	O
s	O
jaakkola	O
and	O
m	O
j	O
jordan	O
mean	B
field	I
theory	I
for	O
sigmoid	B
belief	B
networks	I
journal	O
of	O
artificial	O
intelligence	O
research	O
l	O
k	O
saul	O
and	O
m	O
i	O
jordan	O
exploiting	O
tractable	O
substructures	O
in	O
intractable	O
networks	O
in	O
d	O
s	O
touretzky	O
m	O
mozer	O
and	O
m	O
e	O
hasselmo	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
l	O
savage	O
the	O
foundations	O
of	O
statistics	O
wiley	O
r	O
d	O
schachter	O
bayes-ball	O
the	O
rational	O
pastime	O
determining	O
irrelevance	O
and	O
requisite	O
information	O
in	O
g	O
cooper	O
and	O
s	O
moral	O
editors	O
uncertainty	B
in	O
artificial	O
in	O
belief	B
networks	I
and	O
influence	O
diagrams	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
b	O
sch	O
olkopf	O
a	O
smola	O
and	O
k	O
r	O
m	O
uller	O
nonlinear	O
component	O
analysis	B
as	O
a	O
kernel	B
eigenvalue	O
problem	B
neural	B
computation	I
n	O
n	O
schraudolph	O
and	O
d	O
kamenetsky	O
efficient	O
exact	O
inference	B
in	O
planar	O
ising	O
models	O
in	O
d	O
koller	O
d	O
schuurmans	O
y	O
bengio	O
and	O
l	O
bottou	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
e	O
schwarz	O
estimating	O
the	O
dimension	O
of	O
a	O
model	B
annals	O
of	O
statistics	O
m	O
seeger	O
gaussian	B
processes	O
for	O
machine	O
learning	B
international	O
journal	O
of	O
neural	O
systems	O
m	O
seeger	O
expectation	B
propagation	B
for	O
exponential	B
families	O
technical	O
report	O
department	O
of	O
eecs	O
berkeley	O
www	O
kyb	O
tuebingen	O
mpg	O
debspeopleseeger	O
draft	O
march	O
bibliography	O
bibliography	O
m	O
seeger	O
and	O
h	O
nickisch	O
large	O
scale	O
variational	B
inference	B
and	O
experimental	O
design	O
for	O
sparse	B
generalized	O
linear	B
models	O
technical	O
report	O
max	O
planck	O
institute	O
for	O
biological	O
cybernetics	O
september	O
j	O
shawe-taylor	O
and	O
n	O
cristianini	O
kernel	B
methods	O
for	O
pattern	O
analysis	B
cambridge	O
university	O
press	O
s	O
siddiqi	O
b	O
boots	O
and	O
g	O
gordon	O
a	O
constraint	O
generation	O
approach	B
to	O
learning	B
stable	O
linear	B
dynamical	O
systems	O
in	O
j	O
c	O
platt	O
d	O
koller	O
y	O
singer	O
and	O
s	O
roweis	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
t	O
silander	O
p	O
kontkanen	O
and	O
p	O
myllym	O
aki	O
on	O
sensitivity	O
of	O
the	O
map	B
bayesian	B
network	O
structure	B
to	O
the	O
equivalent	B
sample	O
size	O
parameter	B
in	O
r	O
parr	O
and	O
l	O
van	O
der	O
gaag	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
corvallis	O
oregon	O
usa	O
auai	O
press	O
s	O
s	O
skiena	O
the	O
algorithm	B
design	O
manual	O
springer-verlag	O
new	O
york	O
usa	O
e	O
smith	O
and	O
m	O
s	O
lewicki	O
efficient	O
auditory	O
coding	O
nature	O
p	O
smolensky	O
parallel	B
distributed	O
processing	O
volume	O
foundations	O
chapter	O
information	O
processing	O
in	O
dynamical	O
systems	O
foundations	O
of	O
harmony	O
theory	O
pages	O
mit	O
press	O
cambridge	O
ma	O
g	O
sneddon	O
studies	O
in	O
the	O
atmospheric	O
sciences	O
chapter	O
a	O
statistical	O
perspective	O
on	O
data	O
assimilation	O
in	O
numerical	B
models	O
number	O
in	O
lecture	O
notes	O
in	O
statistics	O
springer-verlag	O
p	O
sollich	O
bayesian	B
methods	O
for	O
support	O
vector	O
machines	O
evidence	O
and	O
predictive	O
class	O
probabilities	O
machine	O
learning	B
d	O
x	O
song	O
d	O
wagner	O
and	O
x	O
tian	O
timing	O
analysis	B
of	O
keystrokes	O
and	O
timing	O
attacks	O
on	O
ssh	O
proceedings	O
of	O
the	O
conference	O
on	O
usenix	O
security	O
symposium	O
usenix	O
association	O
in	O
a	O
s	O
spanias	O
speech	O
coding	O
a	O
tutorial	O
review	O
proceedings	O
of	O
the	O
ieee	O
oct	O
d	O
j	O
spiegelhalter	O
a	O
p	O
dawid	O
s	O
l	O
lauritzen	O
and	O
r	O
g	O
cowell	O
bayesian	B
analysis	B
in	O
expert	O
systems	O
statistical	O
science	O
p	O
spirtes	O
c	O
glymour	O
and	O
r	O
scheines	O
causation	O
prediction	O
and	O
search	O
mit	O
press	O
edition	O
n	O
srebro	O
maximum	B
likelihood	B
bounded	O
tree-width	O
markov	O
networks	O
in	O
j	O
breese	O
and	O
d	O
koller	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
h	O
steck	O
constraint-based	O
structural	O
learning	B
in	O
bayesian	B
networks	O
using	O
finite	O
data	O
sets	O
phd	O
thesis	O
technical	O
university	O
munich	O
h	O
steck	O
learning	B
the	O
bayesian	B
network	O
structure	B
dirichlet	B
prior	B
vs	O
data	O
in	O
d	O
a	O
mcallester	O
and	O
p	O
myllymaki	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
corvallis	O
oregon	O
usa	O
auai	O
press	O
h	O
steck	O
and	O
t	O
jaakkola	O
on	O
the	O
dirichlet	B
prior	B
and	O
bayesian	B
regularization	O
in	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
editors	O
nips	O
pages	O
mit	O
press	O
m	O
studen	O
y	O
on	O
mathematical	O
description	O
of	O
probabilistic	B
conditional	B
independence	B
structures	O
phd	O
thesis	O
academy	O
of	O
sciences	O
of	O
the	O
czech	O
republic	O
m	O
studen	O
y	O
on	O
non-graphical	O
description	O
of	O
models	O
of	O
conditional	B
independence	B
structure	B
in	O
hsss	O
workshop	O
on	O
stochastic	O
systems	O
for	O
individual	O
behaviours	O
louvain	O
la	O
neueve	O
belgium	O
january	O
c	O
sutton	O
and	O
a	O
mccallum	O
an	O
introduction	O
to	O
conditional	B
random	O
fields	O
for	O
relational	O
learning	B
in	O
l	O
getoor	O
and	O
b	O
taskar	O
editors	O
introduction	O
to	O
statistical	O
relational	O
learning	B
mit	O
press	O
r	O
s	O
sutton	O
and	O
a	O
g	O
barto	O
reinforcement	B
learning	B
an	O
introduction	O
mit	O
press	O
r	O
j	O
swendsen	O
and	O
j-s	O
wang	O
nonuniversal	O
critical	O
dynamics	O
in	O
monte	O
carlo	O
simulations	O
physical	O
review	O
letters	O
b	O
k	O
sy	O
a	O
recurrence	O
local	B
computation	O
approach	B
towards	O
ordering	O
composite	O
beliefs	O
in	O
bayesian	B
belief	B
networks	I
international	O
journal	O
of	O
approximate	B
reasoning	O
t	O
sejnowski	O
the	O
book	O
of	O
hebb	B
neuron	O
draft	O
march	O
bibliography	O
bibliography	O
r	O
e	O
tarjan	O
and	O
m	O
yannakakis	O
simple	O
linear-time	O
algorithms	O
to	O
test	O
chordality	O
of	O
graphs	O
test	O
acyclicity	O
of	O
hypergraphs	O
and	O
selectively	O
reduce	O
acyclic	O
hypergraphs	O
siam	O
journal	O
on	O
computing	O
s	O
j	O
taylor	O
modelling	B
financial	O
time	O
series	O
world	O
scientific	O
second	O
edition	O
y	O
w	O
teh	O
d	O
newman	O
and	O
m	O
welling	O
a	O
collapsed	O
variational	O
bayesian	B
inference	B
algorithm	B
for	O
latent	B
dirichlet	B
allocation	I
in	O
j	O
c	O
platt	O
d	O
koller	O
y	O
singer	O
and	O
s	O
roweis	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
y	O
w	O
teh	O
and	O
m	O
welling	O
the	O
unified	O
propagation	B
and	O
scaling	O
algorithm	B
in	O
t	O
g	O
dietterich	O
s	O
becker	O
and	O
z	O
ghahramani	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
m	O
tipping	O
and	O
c	O
m	O
bishop	O
mixtures	O
of	O
probabilistic	B
principal	O
component	O
analysers	O
neural	B
computation	I
m	O
e	O
tipping	O
sparse	B
bayesian	B
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
journal	O
of	O
machine	O
learning	B
research	O
d	O
m	O
titterington	O
a	O
f	O
m	O
smith	O
and	O
u	O
e	O
makov	O
statistical	O
analysis	B
of	O
finite	O
mixture	B
distributions	O
wiley	O
e	O
todorov	O
efficient	O
computation	O
of	O
optimal	O
actions	O
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
of	O
the	O
united	O
states	O
of	O
america	O
m	O
toussaint	O
s	O
harmeling	O
and	O
a	O
storkey	O
probabilistic	B
inference	B
for	O
solving	B
research	O
report	O
university	O
of	O
edinburgh	O
school	O
of	O
informatics	O
m	O
tsodyks	O
k	O
pawelzik	O
and	O
h	O
markram	O
neural	O
networks	O
with	O
dynamic	B
synapses	I
neural	B
computation	I
p	O
van	O
overschee	O
and	O
b	O
de	O
moor	O
subspace	O
identification	O
for	O
linear	B
systems	O
theory	O
implementations	O
applications	O
kluwer	O
v	O
vapnik	O
the	O
nature	O
of	O
statistical	O
learning	B
theory	O
springer	O
new	O
york	O
m	O
verhaegen	O
and	O
p	O
van	O
dooren	O
numerical	B
aspects	O
of	O
different	O
kalman	B
filter	I
implementations	O
ieee	O
transactions	O
of	O
automatic	O
control	O
t	O
verma	O
and	O
j	O
pearl	O
causal	B
networks	O
semantics	O
and	O
expressiveness	O
in	O
r	O
d	O
schacter	O
t	O
s	O
levitt	O
l	O
n	O
kanal	O
and	O
j	O
f	O
lemmer	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
volume	O
pages	O
amsterdam	O
north-holland	O
t	O
o	O
virtanen	O
a	O
t	O
cemgil	O
and	O
s	O
j	O
godsill	O
bayesian	B
extensions	O
to	O
nonnegative	O
matrix	B
factorisation	I
for	O
audio	O
signal	O
modelling	B
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
speech	O
and	O
signal	O
processing	O
pages	O
g	O
wahba	O
support	O
vector	O
machines	O
repreducing	O
kernel	B
hilbert	O
spaces	O
and	O
randomized	O
gacv	O
pages	O
mit	O
press	O
m	O
j	O
wainwright	O
and	O
m	O
i	O
jordan	O
graphical	O
models	O
exponential	B
families	O
and	O
variational	B
inference	B
foun	O
dations	O
and	O
trends	O
in	O
machine	O
learning	B
h	O
wallach	O
efficient	O
training	B
of	O
conditional	B
random	O
fields	O
master	O
s	O
thesis	O
division	O
of	O
informatics	O
university	O
of	O
edinburgh	O
y	O
wang	O
j	O
hodges	O
and	O
b	O
tang	O
classification	B
of	O
web	O
documents	O
using	O
a	O
naive	B
bayes	I
method	O
ieee	O
international	O
conference	O
on	O
tools	O
with	O
artificial	O
intelligence	O
pages	O
s	O
waterhouse	O
d	O
mackay	O
and	O
t	O
robinson	O
bayesian	B
methods	O
for	O
mixtures	O
of	O
experts	O
in	O
d	O
s	O
touretzky	O
m	O
mozer	O
and	O
m	O
e	O
hasselmo	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
y	O
weiss	O
and	O
w	O
t	O
freeman	O
correctness	O
of	O
belief	B
propagation	B
in	O
gaussian	B
graphical	O
models	O
of	O
arbitrary	O
topology	O
neural	B
computation	I
draft	O
march	O
bibliography	O
bibliography	O
m	O
welling	O
t	O
p	O
minka	O
and	O
y	O
w	O
teh	O
structured	B
region	B
graphs	I
morphing	O
ep	O
into	O
gbp	O
in	O
f	O
bacchus	O
and	O
t	O
jaakkola	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
corvallis	O
oregon	O
usa	O
auai	O
press	O
j	O
whittaker	O
graphical	O
models	O
in	O
applied	O
multivariate	B
statistics	O
john	O
wiley	O
sons	O
w	O
wiegerinck	O
variational	O
approximations	O
between	O
mean	B
field	I
theory	I
and	O
the	O
junction	B
tree	B
algorithm	B
in	O
c	O
boutilier	O
and	O
m	O
goldszmidt	O
editors	O
uncertainty	B
in	O
artificial	O
intelligence	O
number	O
pages	O
san	O
francisco	O
ca	O
morgan	O
kaufmann	O
w	O
wiegerinck	O
and	O
t	O
heskes	O
fractional	O
belief	B
propagation	B
in	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
c	O
k	O
i	O
williams	O
computing	O
with	O
infinite	O
networks	O
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petsche	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
nips	O
pages	O
cambridge	O
ma	O
mit	O
press	O
c	O
k	O
i	O
williams	O
and	O
d	O
barber	O
bayesian	B
classification	B
with	O
gaussian	B
processes	O
ieee	O
trans	O
pattern	O
analysis	B
and	O
machine	O
intelligence	O
c	O
yanover	O
and	O
y	O
weiss	O
finding	O
the	O
m	O
most	O
probable	O
configurations	O
using	O
loopy	B
belief	B
propagation	B
in	O
s	O
thrun	O
l	O
saul	O
and	O
b	O
sch	O
olkopf	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
pages	O
cambridge	O
ma	O
mit	O
press	O
j	O
s	O
yedidia	O
w	O
t	O
freeman	O
and	O
y	O
weiss	O
constructing	O
free-energy	O
approximations	O
and	O
generalized	O
belief	B
propagation	B
algorithms	O
information	O
theory	O
ieee	O
transactions	O
on	O
july	O
s	O
young	O
d	O
kershaw	O
j	O
odell	O
d	O
ollason	O
v	O
valtchev	O
and	O
p	O
woodland	O
the	O
htk	O
book	O
version	O
cambridge	O
university	O
press	O
a	O
l	O
yuille	O
and	O
a	O
rangarajan	O
the	O
concave-convex	O
procedure	O
neural	B
computation	I
j	O
-h	O
zhao	O
p	O
l	O
h	O
yu	O
and	O
q	O
jiang	O
ml	O
estimation	O
for	O
factor	B
analysis	B
em	B
or	O
non-em	O
statistics	O
and	O
computing	O
o	O
zoeter	O
monitoring	O
non-linear	B
and	O
switching	B
dynamical	O
systems	O
phd	O
thesis	O
radboud	O
university	O
nijmegen	O
draft	O
march	O
index	O
of	O
m	O
coding	O
n-max-product	B
absorbing	B
state	I
absorption	B
influence	O
diagram	O
acceptance	B
function	B
active	B
learning	B
adjacency	B
matrix	B
algebraic	B
riccati	B
equation	B
ancestor	B
ancestral	B
ordering	I
ancestral	B
sampling	B
antifreeze	B
approximate	B
inference	B
belief	B
propagation	B
bethe	B
free	I
energy	B
double	B
integration	I
bound	B
expectation	B
propagation	B
graph	B
cut	B
laplace	B
approximation	B
switching	B
linear	B
dynamical	I
system	I
variational	B
approach	B
variational	B
inference	B
ar	O
model	B
see	O
auto-regressive	B
model	B
artificial	B
life	I
asychronous	B
updating	I
asymmetry	B
auto-regressive	B
model	B
switching	B
time-varying	B
automatic	B
relevance	I
determination	I
auxiliary	B
variable	I
sampling	B
average	B
backtracking	B
bag	B
of	I
words	I
batch	B
update	I
baum-welch	B
bayes	B
information	I
criterion	I
bayes	O
factor	B
model	B
selection	I
theorem	B
bayes	O
rule	O
see	O
bayes	O
theorem	B
bayesian	B
decision	B
theory	I
hypothesis	B
testing	I
image	B
denoising	I
linear	B
model	B
mixture	B
model	B
model	B
selection	I
occam	O
s	O
razor	O
outcome	B
analysis	B
bayesian	B
dirichlet	B
score	I
bayesian	B
linear	B
model	B
bd	B
score	I
bdeu	B
score	I
bdeu	B
score	I
belief	B
network	I
asbestos-smoking-cancer	B
cascade	B
chest	B
clinic	I
divorcing	B
parents	B
dynamic	B
noisy	B
and	I
gate	I
noisy	B
logic	I
gate	I
noisy	B
or	I
gate	I
sigmoid	B
structure	B
learning	B
training	B
bayesian	B
belief	B
propagation	B
loopy	B
belief	O
revision	O
see	O
max-product	B
bellman	O
s	O
equation	B
bessel	B
function	B
beta	B
distribution	B
function	B
bethe	B
free	I
energy	B
index	O
bias	B
unbiased	B
estimator	I
bigram	B
binary	B
entropy	B
bioinformatics	B
black	B
and	I
white	I
sampling	B
black-box	B
blahut-arimoto	B
algorithm	B
boltzmann	B
machine	I
restricted	B
bond	B
propagation	B
bonferroni	B
inequality	I
boolean	B
network	I
bradly-terry-luce	B
model	B
bucket	B
elimination	I
burn	B
in	I
calculus	B
canonical	B
correlation	I
analysis	B
constrained	B
factor	B
analysis	B
canonical	B
variates	I
causal	B
consistency	I
causality	B
do	B
calculus	B
influence	O
diagrams	O
post	B
intervention	I
distribution	B
cca	O
see	B
canonical	B
correlation	I
analysis	B
centering	B
chain	B
graph	B
chain	B
component	I
chain	B
rule	I
chain	B
structure	B
changepoint	B
model	B
checkerboard	B
chest	B
clinic	I
missing	B
data	I
with	B
decisions	I
children	B
see	O
directed	B
acyclic	I
graph	B
cholesky	B
chord	B
chordal	B
chow-liu	B
tree	B
classification	B
bayesian	B
boundary	B
error	B
analysis	B
linear	B
parameter	B
model	B
multiple	B
classes	I
performance	B
random	B
guessing	I
softmax	B
clique	B
decomposition	B
graph	B
index	O
matrix	B
cliquo	B
cluster	B
variation	I
method	I
clustering	B
collaborative	B
filtering	I
collider	B
see	O
directed	B
acyclic	I
graph	B
commute	B
compatibility	B
function	B
competition	O
model	B
bradly-terry-luce	B
elo	B
trueskill	B
competition	B
models	I
concave	B
function	B
condindep	O
m	O
conditional	B
entropy	B
conditional	B
likelihood	B
conditional	B
mutual	B
information	I
conditional	B
probability	I
conditional	B
random	B
field	I
conditioning	B
loop	B
cut	B
set	I
conjugate	B
distribution	B
exponential	B
family	B
gaussian	B
prior	B
conjugate	B
gradient	B
conjugate	B
gradients	I
algorithm	B
conjugate	B
vector	I
conjugate	B
vectors	I
algorithm	B
connected	B
components	I
connected	B
graph	B
consistent	B
consistent	B
estimator	I
convex	B
function	B
correction	B
smoother	I
correlation	O
matrix	B
cosine	B
similarity	I
coupled	B
hmm	B
covariance	B
matrix	B
covariance	B
function	B
construction	B
gibbs	B
isotropic	B
mat	O
ern	O
mercer	B
kernel	B
neural	B
network	I
non-stationary	B
ornstein-uhlenbeck	B
periodic	B
rational	B
quadratic	I
smoothness	B
draft	O
march	O
index	O
index	O
squared	B
exponential	B
stationary	B
cpt	O
see	O
conditional	B
probability	I
table	O
crf	O
see	O
conditional	B
random	B
field	I
critical	B
point	I
cross-validation	B
cumulant	B
curse	B
of	I
dimensionality	I
cut	B
set	I
conditioning	B
d-map	O
see	O
dependence	O
map	B
dag	O
see	O
directed	B
acyclic	I
graph	B
data	O
anomaly	B
detection	I
catagorical	B
dyadic	B
handwritten	B
digits	I
labelled	B
monadic	B
numerical	B
ordinal	B
unlabelled	B
data	B
compression	I
vector	B
quantisation	I
decision	B
boundary	B
decision	B
function	B
decision	B
theory	I
decision	B
tree	B
decomposable	B
degree	B
degree	B
of	I
belief	I
delta	O
function	B
see	O
dirac	B
delta	I
function	B
kronecker	B
density	B
estimation	I
parzen	B
estimator	I
dependence	O
map	B
descendant	B
design	B
matrix	B
detailed	B
balance	I
determinant	B
deterministic	B
latent	B
variable	I
model	B
differentiation	B
digamma	B
function	B
digit	B
data	I
dijkstra	O
s	O
algorithm	B
dimension	O
reduction	O
linear	B
supervised	B
dimensionality	O
reduction	O
linear	B
non-linear	B
dirac	B
delta	I
function	B
directed	B
acyclic	I
graph	B
ancestor	B
draft	O
march	O
ancestral	B
order	I
cascade	B
children	B
collider	B
descendant	B
family	B
immorality	B
markov	B
blanket	I
moralisation	B
parents	B
direction	B
bias	B
directional	B
derivative	I
dirichlet	B
distribution	B
dirichlet	B
process	I
mixture	B
models	I
discount	B
factor	B
discriminative	B
approach	B
training	B
discriminative	B
approach	B
discriminative	B
training	B
dissimilarity	B
function	B
distributed	B
computation	I
distribution	B
bernoulli	B
beta	B
binomial	B
categorical	B
change	B
of	I
variables	I
conjugate	B
continuous	B
density	B
dirichlet	B
discrete	B
divergence	B
double	B
exponential	B
empirical	B
average	B
expectation	B
exponential	B
exponential	B
family	B
canonical	B
gamma	B
mode	B
gauss-gamma	B
gauss-inverse-gamma	B
gaussian	B
canonical	B
exponential	B
form	I
conditioning	B
conjugate	B
entropy	B
isotropic	B
mixture	B
multivariate	B
normalisation	B
index	O
index	O
partitioned	B
propagation	B
system	B
reversal	I
univariate	B
inverse	B
gamma	B
inverse	B
wishart	B
joint	B
kurtosis	B
laplace	B
marginal	B
mode	B
multinomial	B
normal	B
poisson	B
polya	B
scaled	B
mixture	B
skewness	B
student	O
s	O
t	O
uniform	B
wishart	B
domain	B
double	B
integration	I
bound	B
dual	B
parameters	I
dual	B
representation	I
dyadic	B
data	I
dynamic	B
bayesian	B
network	I
dynamic	B
synapses	I
dynamical	O
system	O
linear	B
non-linear	B
dynamics	B
reversal	I
edge	B
list	I
efficient	O
ipf	O
eigen	O
decomposition	B
equation	B
function	B
problem	B
spectrum	B
value	B
elo	B
model	B
emission	B
distribution	B
emission	B
matrix	B
empirical	B
independence	B
empirical	B
distribution	B
empirical	B
risk	B
penalised	B
empirical	B
risk	B
minimisation	I
energy	B
entropy	B
differential	B
gaussian	B
ep	O
see	O
expectation	B
propagation	B
error	B
function	B
estimator	O
consistent	B
evidence	O
see	O
marginal	B
likelihood	B
hard	B
likelihood	B
soft	B
uncertain	B
virtual	B
evidence	B
procedure	I
exact	B
sampling	B
expectation	B
see	O
average	B
expectation	B
correction	I
expectation	B
maximisation	B
algorithm	B
antifreeze	B
belief	B
networks	I
e-step	B
energy	B
entropy	B
failure	B
case	I
generalised	B
intractable	B
energy	B
m-step	B
markov	B
decision	I
process	I
mixture	B
model	B
partial	B
e-step	B
partial	B
m-step	B
variational	B
bayes	I
viterbi	B
training	B
expectation	B
propagation	B
exponential	B
family	B
canonical	B
form	I
conjugate	B
extended	B
observability	I
matrix	B
face	B
model	B
factor	B
analysis	B
factor	B
rotation	I
probabilistic	B
pca	B
training	B
em	B
svd	B
factor	B
graph	B
factor	B
loading	I
family	B
see	O
directed	B
acyclic	I
graph	B
feature	B
map	B
filtering	O
finite	O
dimensional	O
gaussian	B
process	I
fisher	B
information	I
fisher	O
s	O
linear	B
discriminant	O
floyd-warshall-roy	B
algorithm	B
forward	B
sampling	B
forward-backward	B
draft	O
march	O
index	O
index	O
forward-sampling-resampling	B
gamma	B
digamma	B
distribution	B
function	B
gaussian	B
canonical	B
representation	I
distribution	B
moment	B
representation	I
sub	B
super	B
gaussian	B
mixture	B
model	B
bayesian	B
collapsing	B
the	I
mixture	B
em	B
algorithm	B
infinite	B
problems	I
k-means	B
parzen	B
estimator	I
symmetry	B
breaking	I
gaussian	B
process	I
classification	B
laplace	B
approximation	B
multiple	B
classes	I
regression	B
smoothness	B
weight	B
space	I
view	I
gaussian	B
sum	I
filtering	I
gaussian	B
sum	I
smoothing	B
generalisation	B
generalised	B
pseudo	B
bayes	I
generative	B
approach	B
model	B
training	B
generative	B
approach	B
gibbs	B
sampling	B
glicko	B
gmm	O
see	O
gaussian	B
mixture	B
model	B
google	B
gradient	B
descent	B
natural	B
gram	B
matrix	B
gram-schmidt	B
procedure	I
gramm	B
matrix	B
graph	B
adjacency	B
matrix	B
chain	B
chain	B
structured	B
chordal	B
clique	B
clique	B
matrix	B
cliquo	B
connected	B
draft	O
march	O
cut	B
decomposable	B
descendant	B
directed	B
disconnected	B
edge	B
list	I
factor	B
loopy	B
multiply-connected	B
neighbour	B
path	B
separation	B
set	B
chain	B
singly-connected	B
skeleton	B
spanning	B
tree	B
tree	B
triangulated	B
undirected	B
vertex	O
degree	B
graph	B
cut	B
algorithm	B
graph	B
partitioning	I
gull-mackay	B
iteration	B
hamilton-jacobi	B
equation	B
hamiltonian	B
dynamics	I
hammersley	B
clifford	I
theorem	B
handwritten	B
digits	I
hankel	B
matrix	B
harmonium	O
see	O
restricted	B
boltzmann	B
machine	I
heaviside	B
step	I
function	B
hebb	B
hebb	B
rule	I
hedge	B
fund	I
hermitian	B
hessian	B
hidden	B
markov	I
model	B
recursion	O
recursion	O
coupled	B
direction	B
bias	B
discriminative	B
training	B
duration	B
model	B
entropy	B
filtering	O
input-output	B
likelihood	B
most	B
likely	I
state	I
pairwise	B
marginal	B
rauch	B
tung	I
striebel	I
smoother	I
smoothing	B
parallel	B
sequential	B
viterbi	B
index	O
viterbi	B
algorithm	B
hidden	B
variables	I
hmm	B
see	O
hidden	B
markov	I
model	B
hopfield	B
network	I
augmented	B
capacity	B
hebb	B
rule	I
heteroassociative	B
maximum	B
likelihood	B
perceptron	B
pseudo	B
inverse	I
rule	I
sequence	B
learning	B
hybrid	B
monte	I
carlo	I
hyper	B
markov	I
hyper	B
tree	B
hyperparameter	B
hyperplane	B
hypothesis	B
testing	I
bayesian	B
error	B
analysis	B
i-map	O
see	O
independence	B
map	B
ica	B
identically	B
and	I
independently	I
distributed	I
identifiability	B
identity	B
matrix	B
iid	O
see	O
identically	B
and	I
independently	I
distributed	I
im	O
algorithm	B
see	O
information-maximisation	O
algo	O
rithm	O
immorality	B
importance	B
distribution	B
sampling	B
particle	B
filter	I
resampling	B
sequential	B
weight	B
incidence	B
matrix	B
independence	B
bayesian	B
conditional	B
empirical	B
map	B
markov	B
equivalent	B
mutual	B
information	I
naive	B
bayes	I
parameter	B
perfect	B
map	B
independent	B
components	I
analysis	B
indicator	B
function	B
indicator	B
model	B
induced	B
representation	I
inference	B
bond	B
propagation	B
bucket	B
elimination	I
causal	B
index	O
cut	B
set	I
conditioning	B
hmm	B
linear	B
dynamical	I
system	I
map	B
marginal	B
markov	B
decision	I
process	I
max-product	B
message	B
passing	B
mixed	B
mpm	B
sum-product	B
algorithm	B
transfer	B
matrix	B
variable	B
elimination	I
influence	O
diagram	O
absorption	B
asymmetry	B
causal	B
consistency	I
chest	B
clinic	I
decision	B
potential	B
fundamental	B
link	I
information	B
link	I
junction	B
tree	B
no	B
forgetting	I
principle	I
partial	B
order	I
probability	B
potential	B
solving	B
utility	B
utility	B
potential	B
information	B
link	I
information	B
maximisation	B
information	B
retrieval	I
information-maximisation	B
algorithm	B
innovation	B
noise	I
input-output	B
hmm	B
inverse	B
modus	I
ponens	I
ipf	O
see	O
iterative	B
proportional	I
fitting	I
efficient	O
ising	B
model	B
see	O
markov	B
network	I
approximate	B
inference	B
isotropic	B
isotropic	B
covariance	B
functions	I
item	B
response	I
theory	I
iterated	B
conditional	B
modes	I
iterative	B
proportional	I
fitting	I
iterative	B
scaling	I
jeffrey	O
s	O
rule	O
jensen	O
s	O
inequality	O
joseph	O
s	O
symmetrized	O
update	O
jump	O
markov	O
model	B
see	O
switching	B
linear	B
dynamical	I
system	I
junction	B
tree	B
absorption	B
algorithm	B
clique	B
graph	B
draft	O
march	O
index	O
index	O
computational	B
complexity	I
conditional	B
marginal	B
consistent	B
hyper	B
tree	B
influence	O
diagram	O
marginal	B
likelihood	B
most	B
likely	I
state	I
normalisation	B
constant	I
potential	B
running	B
intersection	I
property	I
separator	B
strong	B
strong	B
triangulation	B
tree	B
width	I
triangulation	B
k-means	B
kalman	B
filter	I
kalman	B
gain	I
kd-tree	B
kernel	B
see	O
covariance	B
function	B
classifier	B
kidnapped	B
robot	I
kikuchi	B
kl	O
divergence	B
see	O
kullback-leibler	B
divergence	B
knn	O
see	O
nearest	B
neighbour	B
kronecker	B
delta	I
kullback-leibler	B
divergence	B
kurtosis	B
labelled	B
data	I
lagrange	O
multiplier	B
lagrangian	B
laplace	B
approximation	B
latent	B
ability	I
model	B
latent	B
dirichlet	B
allocation	I
latent	B
linear	B
model	B
latent	B
semantic	I
analysis	B
latent	B
topic	I
latent	B
variable	I
deterministic	B
model	B
lattice	B
model	B
lda	B
regularised	B
lds	O
see	O
linear	B
dynamical	I
system	I
leaky	B
integrate	I
and	I
fire	I
model	B
leapfrog	B
discretisation	I
learning	B
active	B
anomaly	B
detection	I
bayesian	B
belief	B
network	I
belief	B
networks	I
em	B
draft	O
march	O
dirichlet	B
prior	B
inference	B
nearest	B
neighbour	B
online	B
query	B
reinforcement	B
semi-supervised	B
sequences	B
sequential	B
structure	B
supervised	B
unsupervised	B
learning	B
rate	I
likelihood	B
bound	B
marginal	B
model	B
approximate	B
pseudo	B
likelihood	B
decomposable	B
line	B
search	I
linear	B
algebra	I
linear	B
dimension	I
reduction	I
canonical	B
correlation	I
analysis	B
latent	B
semantic	I
analysis	B
non-negative	B
matrix	B
factorisation	I
probabilistic	B
latent	B
semantic	I
analysis	B
supervised	B
unsupervised	B
linear	B
discriminant	I
analysis	B
linear	B
discriminant	I
analysis	B
as	B
regression	B
penalised	B
regularised	B
linear	B
dynamical	I
system	I
cross	B
moment	I
dynamics	B
reversal	I
filtering	O
identifiability	B
inference	B
learning	B
likelihood	B
most	B
likely	I
state	I
numerical	B
stability	I
riccati	B
equations	I
smoothing	B
subspace	B
method	I
switching	B
symmetrising	B
updates	I
linear	B
gaussian	B
state	I
space	I
model	B
linear	B
model	B
bayesian	B
classification	B
factor	B
analysis	B
latent	B
index	O
index	O
regression	B
linear	B
parameter	B
model	B
bayesian	B
linear	B
perceptron	B
linear	B
separability	I
linear	B
transformation	I
linearly	B
independent	I
linearly	B
separable	I
linsker	O
s	O
as-if-gaussian	O
approximation	B
localisation	B
logic	O
aristotle	B
logistic	B
regression	B
logistic	B
sigmoid	B
logit	B
loop	B
cut	B
set	I
loopy	B
loss	B
function	B
zero-one	B
loss	B
matrix	B
luenberger	B
expanding	I
subspace	I
theorem	B
mahalanobis	B
distance	I
manifold	O
linear	B
low	B
dimensional	I
map	B
see	O
most	B
probable	I
a	I
posteriori	I
mar	O
see	O
missing	B
at	I
random	I
margin	B
soft	B
marginal	B
generalised	B
marginal	B
likelihood	B
approximate	B
marginalisation	B
markov	B
chain	B
first	O
order	O
stationary	B
distribution	B
equivalent	B
global	B
hyper	B
local	B
model	B
pairwise	B
random	B
field	I
approximation	B
markov	B
blanket	I
see	O
directed	B
acyclic	I
graph	B
markov	B
chain	B
absorbing	B
state	I
detailed	B
balance	I
pagerank	B
markov	B
chain	B
monte	I
carlo	I
auxiliary	B
variable	I
hybrid	B
monte	I
carlo	I
slice	B
sampling	B
swendson-wang	B
gibbs	B
sampling	B
metropolis-hastings	B
proposal	B
distribution	B
structured	B
gibbs	B
sampling	B
markov	B
decision	I
process	I
bellman	O
s	O
equation	B
discount	B
factor	B
non-stationary	B
policy	B
partially	B
observable	I
planning	B
policy	B
iteration	B
reinforcement	B
learning	B
stationary	B
stationary	B
deterministic	B
policy	B
temporally	B
unbounded	I
value	B
iteration	B
variational	B
inference	B
markov	B
equivalence	I
markov	B
network	I
boltzmann	B
machine	I
continuous-state	B
temporal	I
discrete-state	B
temporal	I
gibbs	B
distribution	B
gibbs	B
network	I
hammersley	B
clifford	I
theorem	B
pairwise	B
potential	B
markov	B
random	B
field	I
alpha-expansion	B
attractive	B
binary	I
graph	B
cut	B
map	B
potts	B
model	B
matrix	B
adjacency	B
cholesky	B
clique	B
gramm	B
hankel	B
incidence	B
inversion	B
inversion	B
lemma	I
orthogonal	B
positive	B
definite	I
pseudo	B
inverse	I
rank	B
matrix	B
factorisation	I
max-product	B
n	B
most	I
probable	I
states	I
max-sum	B
maximum	B
cardinality	I
checking	I
maximum	B
likelihood	B
belief	B
network	I
draft	O
march	O
index	O
index	O
chow-liu	B
tree	B
counting	B
empirical	B
distribution	B
factor	B
analysis	B
gaussian	B
gradient	B
optimisation	B
markov	B
network	I
ml-ii	B
naive	B
bayes	I
properties	B
maximum	B
likelihood	B
hopfield	B
network	I
mcmc	O
see	O
markov	B
chain	B
monte	I
carlo	I
mdp	O
see	O
markov	B
decision	I
process	I
mean	B
field	I
theory	I
asynchronous	B
updating	I
mercer	B
kernel	B
message	B
passing	B
schedule	B
message	B
passing	B
metropolis-hastings	B
acceptance	B
function	B
metropolis-hastings	B
sampling	B
minimum	B
clique	B
cover	I
missing	B
at	I
random	I
completely	B
missing	B
data	I
mixed	B
inference	B
mixed	B
membership	I
model	B
mixing	B
matrix	B
mixture	B
gaussian	B
mixture	B
model	B
bernoulli	B
product	I
dirichlet	B
process	I
mixture	B
expectation	B
maximisation	B
factor	B
analysis	B
gaussian	B
indicator	B
approach	B
markov	B
chain	B
pca	B
mixture	B
of	I
experts	I
mn	O
see	O
markov	B
network	I
mode	B
model	B
auto-regressive	B
changepoint	B
deterministic	B
latent	B
variable	I
faces	B
leaky	B
integrate	I
and	I
fire	I
linear	B
mixed	B
membership	I
mixture	B
rasch	B
model	B
selection	I
draft	O
march	O
approximate	B
moment	B
generating	I
function	B
moment	B
representation	I
momentum	B
monadic	B
data	I
money	B
financial	O
prediction	O
loadsa	B
moralisation	B
most	B
probable	I
a	I
posteriori	I
most	O
probable	O
path	B
multiple-source	B
multiple-sink	I
most	O
probable	O
state	O
n	B
most	I
probable	I
mrf	O
see	O
markov	B
random	B
field	I
multiply-connected	B
multiply-connected-distributions	B
multpots	O
m	O
mutual	B
information	I
approximation	B
conditional	B
maximisation	B
naive	B
bayes	I
bayesian	B
tree	B
augmented	B
naive	B
mean	B
field	I
naive	B
mean	B
field	I
theory	I
natural	B
gradient	B
nearest	B
neighbour	B
probabilistic	B
network	O
flow	O
network	B
modelling	B
neural	B
computation	I
neural	B
network	I
depression	B
dynamic	B
synapses	I
leaky	B
integrate	I
and	I
fire	I
newton	B
update	I
newton	O
s	O
method	O
no	B
forgetting	I
principle	I
node	O
extremal	B
simplical	B
non-negative	B
matrix	B
factorisation	I
normal	B
distribution	B
normal	B
equations	I
normalised	B
importance	B
weights	I
observed	B
linear	B
dynamical	I
system	I
occam	O
s	O
razor	O
one	B
of	I
m	I
encoding	I
online	B
learning	B
optimisation	B
broyden-fletcher-goldfarb-shanno	B
index	O
index	O
conjugate	B
gradients	I
algorithm	B
conjugate	B
vectors	I
algorithm	B
constrained	B
optimisation	B
critical	B
point	I
gradient	B
descent	B
luenberger	B
expanding	I
subspace	I
theorem	B
newton	O
s	O
method	O
quasi	B
newton	I
method	I
ordinary	B
least	I
squares	I
ornstein-uhlenbeck	B
orthogonal	B
orthogonal	B
least	I
squares	I
orthonormal	B
outcome	B
analysis	B
outlier	B
over-complete	B
representation	I
over-complete	B
representations	I
overcounting	B
overfitting	B
pagerank	B
pairwise	B
comparison	I
models	I
pairwise	B
markov	B
network	I
parents	B
see	O
directed	B
acyclic	I
graph	B
part-of-speech	B
tagging	B
partial	B
least	I
squares	I
partial	B
order	I
partially	B
observable	I
mdp	I
particle	B
filter	I
partition	B
function	B
partitioned	B
matrix	B
inversion	B
parzen	B
estimator	I
path	B
blocked	B
pc	B
algorithm	B
pca	B
see	O
principal	B
components	I
analysis	B
perceptron	B
logistic	B
regression	B
perfect	B
elimination	I
order	I
perfect	B
map	B
see	O
independence	B
perfect	B
sampling	B
planning	B
plant	B
monitoring	I
plate	B
poisson	B
distribution	B
policy	B
iteration	B
non-stationary	B
stationary	B
deterministic	B
polya	B
distribution	B
pomdp	O
see	O
partially	B
observable	I
mdp	I
positive	B
definite	I
kernel	B
matrix	B
parameterisation	B
posterior	B
dirichlet	B
potential	B
potts	B
model	B
precision	B
prediction	O
auto-regression	B
financial	O
non-parametric	B
parameteric	B
predictive	B
variance	B
predictor-corrector	B
principal	B
components	I
analysis	B
algorithm	B
high	B
dimensional	I
data	I
kernel	B
latent	B
semantic	I
analysis	B
missing	B
data	I
probabilistic	B
principal	B
directions	I
printer	B
nightmare	I
missing	B
data	I
prior	B
probabilistic	B
latent	B
semantic	I
analysis	B
conditional	B
em	B
algorithm	B
probabilistic	B
pca	B
probability	O
conditional	B
function	B
density	B
frequentist	B
posterior	B
potential	B
prior	B
subjective	B
probit	B
probit	B
regression	B
projection	B
proposal	B
distribution	B
pseudo	B
inverse	I
pseudo	B
inverse	I
hopfield	B
network	I
pseudo	B
likelihood	B
quadratic	B
form	I
quadratic	B
programming	I
query	B
learning	B
questionnaire	B
analysis	B
radial	B
basis	I
functions	I
raleigh	B
quotient	I
random	B
boolean	I
networks	I
rasch	B
model	B
draft	O
march	O
index	O
bayesian	B
rauch-tung-striebel	B
reabsorption	B
region	B
graphs	I
regresion	O
linear	B
parameter	B
model	B
regression	B
logisitic	B
regularisation	B
reinforcement	B
learning	B
relevance	B
vector	I
machine	I
relevance	B
vector	I
machine	I
reparameterisation	B
representation	O
dual	B
over-complete	B
sparse	B
under-complete	B
resampling	B
reset	B
model	B
residuals	B
resolution	B
responsibility	B
restricted	B
boltzmann	B
machine	I
riccati	B
equation	B
risk	B
robust	B
classification	B
rose-tarjan-lueker	B
elimination	I
running	B
intersection	I
property	I
sample	O
mean	B
variance	B
sampling	B
ancestral	B
gibbs	B
importance	B
multi-variate	B
particle	B
filter	I
univariate	B
sampling	B
importance	B
resampling	B
scalar	B
product	I
scaled	B
mixture	B
search	B
engine	I
self	B
localisation	B
and	I
mapping	I
semi-supervised	B
learning	B
lower	B
dimensional	I
representations	I
separator	B
sequential	B
importance	B
sampling	B
sequential	B
minimal	I
optimisation	B
set	B
chain	B
shortest	B
path	B
shortest	B
weighted	I
path	B
sigmoid	B
logistic	B
draft	O
march	O
sigmoid	B
belief	B
network	I
sigmoid	B
function	B
approximate	B
average	B
simple	B
path	B
simplical	B
nodes	I
simpson	O
s	O
paradox	O
singly-connected	B
singular	B
singular	B
value	B
decomposition	B
thin	B
skeleton	B
skewness	B
slice	B
sampling	B
smoothing	B
softmax	B
function	B
spam	B
filtering	I
spanning	B
tree	B
sparse	B
representation	I
spectrogram	B
speech	B
recognition	I
spike	B
response	I
model	B
squared	B
euclidean	I
distance	I
squared	B
exponential	B
standard	B
deviation	I
standard	B
normal	B
distribution	B
stationary	B
distribution	B
stationary	B
markov	B
chain	B
stationary	B
planner	I
stop	B
words	I
strong	B
junction	B
tree	B
strong	B
triangulation	B
structure	B
learning	B
bayesian	B
network	B
scoring	I
pc	B
algorithm	B
undirected	B
structured	B
expectation	B
propagation	B
subsampling	B
subspace	B
method	I
sum-product	B
sum-product	B
algorithm	B
supervised	B
learning	B
classification	B
regression	B
support	B
vector	I
machine	I
chunking	B
training	B
support	B
vectors	I
svd	B
see	O
singular	B
value	B
decomposition	B
svm	O
see	O
support	B
vector	I
machine	I
swendson-wang	B
sampling	B
switching	B
ar	I
model	B
index	O
index	O
index	O
switching	B
kalman	B
filter	I
see	O
switching	B
linear	B
dynam	O
ical	O
system	O
switching	B
linear	B
dynamical	I
system	I
under-complete	B
representation	I
undirected	B
graph	B
undirected	B
model	B
changepoint	B
model	B
expectation	B
correction	I
filtering	O
gaussian	B
sum	I
smoothing	B
generalised	B
pseudo	B
bayes	I
inference	B
computational	B
complexity	I
likelihood	B
smoothing	B
switching	B
linear	B
dynamical	O
systemcollapsing	O
gaus	O
sians	B
symmetry	B
breaking	I
system	B
reversal	I
tagging	B
tall	B
matrix	B
taylor	B
expansion	I
term-document	B
matrix	B
test	B
set	I
text	B
analysis	B
latent	B
semantic	I
analysis	B
latent	B
topic	I
probabilistic	B
latent	B
semantic	I
analysis	B
time-invariant	B
lds	I
tower	B
of	I
hanoi	I
trace-log	B
formula	I
train	B
set	I
training	B
batch	B
discriminative	B
generative	B
generative-discriminative	B
hmm	B
linear	B
dynamical	I
system	I
online	B
transfer	B
matrix	B
transition	B
distribution	B
transition	B
matrix	B
tree	B
chow-liu	B
tree	B
augmented	B
network	I
tree	B
width	I
triangulation	B
check	B
greedy	B
elimination	I
maximum	B
cardinality	I
strong	B
variable	B
elimination	I
trueskill	B
two-filter	B
smoother	I
uncertainty	B
learning	B
hidden	B
variable	I
latent	B
variable	I
uniform	B
distribution	B
unit	B
vector	I
unlabelled	B
data	I
unsupervised	B
learning	B
utility	B
matrix	B
message	B
money	B
potential	B
zero-one	B
loss	I
validation	B
cross	B
value	B
value	B
iteration	B
variable	O
hidden	B
missing	B
visible	B
variable	B
elimination	I
variance	B
variational	O
approximation	B
factorised	B
structured	B
variational	B
bayes	I
expectation	B
maximisation	B
variational	B
inference	B
varimax	B
vector	B
algebra	I
vector	B
quantisation	I
viterbi	B
viterbi	B
algorithm	B
viterbi	B
alignment	I
voronoi	B
tessellation	I
web	O
modelling	B
website	B
analysis	B
whitening	B
woodbury	B
formula	I
xor	B
function	B
zero-one	B
loss	I
draft	O
march	O
