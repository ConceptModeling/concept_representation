c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
processes	O
for	O
machine	O
learning	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
adaptive	O
computation	O
and	O
machine	O
learning	B
thomas	O
dietterich	O
editor	O
christopher	O
bishop	O
david	O
heckerman	O
michael	O
jordan	O
and	O
michael	O
kearns	O
associate	O
editors	O
bioinformatics	O
the	O
machine	O
learning	B
approach	O
pierre	O
baldi	O
and	O
s	O
ren	O
brunak	O
reinforcement	O
learning	B
an	O
introduction	O
richard	O
s	O
sutton	O
and	O
andrew	O
g	O
barto	O
graphical	O
models	O
for	O
machine	O
learning	B
and	O
digital	O
communication	O
brendan	O
j	O
frey	O
learning	B
in	O
graphical	O
models	O
michael	O
i	O
jordan	O
causation	O
prediction	O
and	O
search	O
second	O
edition	O
peter	O
spirtes	O
clark	O
glymour	O
and	O
richard	O
scheines	O
principles	O
of	O
data	O
mining	O
david	O
hand	O
heikki	O
mannila	O
and	O
padhraic	O
smyth	O
bioinformatics	O
the	O
machine	O
learning	B
approach	O
second	O
edition	O
pierre	O
baldi	O
and	O
s	O
ren	O
brunak	O
learning	B
kernel	B
classifiers	O
theory	O
and	O
algorithms	O
ralf	O
herbrich	O
learning	B
with	O
kernels	O
support	B
vector	I
machines	O
regularization	B
optimization	O
and	O
beyond	O
bernhard	O
sch	O
olkopf	O
and	O
alexander	O
j	O
smola	O
introduction	O
to	O
machine	O
learning	B
ethem	O
alpaydin	O
gaussian	O
processes	O
for	O
machine	O
learning	B
carl	O
edward	O
rasmussen	O
and	O
christopher	O
k	O
i	O
williams	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
processes	O
for	O
machine	O
learning	B
carl	O
edward	O
rasmussen	O
christopher	O
k	O
i	O
williams	O
the	O
mit	O
press	O
cambridge	O
massachusetts	O
london	O
england	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
massachusetts	O
institute	O
of	O
technology	O
all	O
rights	O
reserved	O
no	O
part	O
of	O
this	O
book	O
may	O
be	O
reproduced	O
in	O
any	O
form	O
by	O
any	O
electronic	O
or	O
mechanical	O
means	O
photocopying	O
recording	O
or	O
information	O
storage	O
and	O
retrieval	O
without	O
permission	O
in	O
writing	O
from	O
the	O
publisher	O
mit	O
press	O
books	O
may	O
be	O
purchased	O
at	O
special	O
quantity	O
discounts	O
for	O
business	O
or	O
sales	O
promotional	O
use	O
for	O
information	O
please	O
email	O
special	O
salesmitpress	O
mit	O
edu	O
or	O
write	O
to	O
special	O
sales	O
department	O
the	O
mit	O
press	O
hayward	O
street	O
cambridge	O
ma	O
typeset	O
by	O
the	O
authors	O
using	O
latex	O
this	O
book	O
was	O
printed	O
and	O
bound	O
in	O
the	O
united	O
states	O
of	O
america	O
library	O
of	O
congress	O
cataloging-in-publication	O
data	O
rasmussen	O
carl	O
edward	O
gaussian	O
processes	O
for	O
machine	O
learning	B
carl	O
edward	O
rasmussen	O
christopher	O
k	O
i	O
williams	O
p	O
cm	O
computation	O
and	O
machine	O
learning	B
includes	O
bibliographical	O
references	O
and	O
indexes	O
isbn	O
gaussian	O
processes	O
data	O
processing	O
machine	O
learning	B
mathematical	O
models	O
i	O
williams	O
christopher	O
k	O
i	O
ii	O
title	O
iii	O
series	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
actual	O
science	O
of	O
logic	O
is	O
conversant	O
at	O
present	O
only	O
with	O
things	O
either	O
certain	O
impossible	O
or	O
entirely	O
doubtful	O
none	O
of	O
which	O
we	O
have	O
to	O
reason	O
on	O
therefore	O
the	O
true	O
logic	O
for	O
this	O
world	O
is	O
the	O
calculus	O
of	O
probabilities	O
which	O
takes	O
account	O
of	O
the	O
magnitude	O
of	O
the	O
probability	O
which	O
is	O
or	O
ought	O
to	O
be	O
in	O
a	O
reasonable	O
man	O
s	O
mind	O
james	O
clerk	O
maxwell	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
contents	O
series	O
foreword	O
xi	O
preface	O
xiii	O
symbols	O
and	O
notation	O
xvii	O
introduction	O
a	O
pictorial	O
introduction	O
to	O
bayesian	O
modelling	O
roadmap	O
regression	O
weight-space	O
view	O
the	O
standard	O
linear	B
model	O
projections	O
of	O
inputs	O
into	O
feature	B
space	I
function-space	O
view	O
varying	O
the	O
hyperparameters	B
decision	O
theory	O
for	O
regression	O
an	O
example	O
application	O
smoothing	O
weight	O
functions	O
and	O
equivalent	B
kernels	O
incorporating	O
explicit	O
basis	O
functions	O
marginal	B
likelihood	B
history	O
and	O
related	O
work	O
exercises	O
classification	B
classification	B
problems	O
decision	O
theory	O
for	O
classification	B
linear	B
models	O
for	O
classification	B
gaussian	B
process	I
classification	B
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
gp	O
classifier	O
posterior	O
predictions	O
implementation	O
marginal	B
likelihood	B
multi-class	B
laplace	B
approximation	I
implementation	O
expectation	B
propagation	I
predictions	O
marginal	B
likelihood	B
implementation	O
experiments	O
a	O
toy	O
problem	O
one-dimensional	O
example	O
binary	B
handwritten	O
digit	O
classification	B
example	O
handwritten	O
digit	O
classification	B
example	O
discussion	O
sections	O
marked	O
by	O
an	O
asterisk	O
contain	O
advanced	O
material	O
that	O
may	O
be	O
omitted	O
on	O
a	O
first	O
reading	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
viii	O
contents	O
appendix	O
moment	O
derivations	O
exercises	O
covariance	B
functions	O
preliminaries	O
examples	O
of	O
covariance	B
functions	O
mean	B
square	I
continuity	I
and	O
differentiability	O
stationary	O
covariance	B
functions	O
dot	B
product	I
covariance	B
functions	O
other	O
non-stationary	O
covariance	B
functions	O
making	O
new	O
kernels	O
from	O
old	O
eigenfunction	B
analysis	O
of	O
kernels	O
an	O
analytic	O
example	O
numerical	O
approximation	O
of	O
eigenfunctions	O
string	B
kernels	O
fisher	B
kernels	O
kernels	O
for	O
non-vectorial	O
inputs	O
exercises	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
the	O
model	O
selection	O
problem	O
bayesian	O
model	O
selection	O
cross-validation	B
model	O
selection	O
for	O
gp	O
regression	O
marginal	B
likelihood	B
cross-validation	B
examples	O
and	O
discussion	O
model	O
selection	O
for	O
gp	O
classification	B
derivatives	O
of	O
the	O
marginal	B
likelihood	B
for	O
laplace	O
s	O
approximation	O
derivatives	O
of	O
the	O
marginal	B
likelihood	B
for	O
ep	O
cross-validation	B
example	O
exercises	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
reproducing	O
kernel	B
hilbert	O
spaces	O
regularization	B
regularization	B
defined	O
by	O
differential	O
operators	O
obtaining	O
the	O
regularized	O
solution	O
the	O
relationship	O
of	O
the	O
regularization	B
view	O
to	O
gaussian	B
process	I
support	B
vector	I
machines	O
prediction	O
spline	O
models	O
a	O
gaussian	B
process	I
spline	O
construction	O
support	B
vector	I
classification	B
support	B
vector	I
regression	I
least-squares	B
classification	B
probabilistic	B
least-squares	B
classification	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
contents	O
ix	O
relevance	O
vector	O
machines	O
exercises	O
some	O
specific	O
examples	O
of	O
equivalent	B
kernels	O
theoretical	O
perspectives	O
the	O
equivalent	B
kernel	B
asymptotic	O
analysis	O
consistency	B
equivalence	O
and	O
orthogonality	O
average-case	O
learning	B
curves	O
pac-bayesian	O
analysis	O
the	O
pac	O
framework	O
pac-bayesian	O
analysis	O
pac-bayesian	O
analysis	O
of	O
gp	O
classification	B
comparison	O
with	O
other	O
supervised	B
learning	B
methods	O
appendix	O
learning	B
curve	I
for	O
the	O
ornstein-uhlenbeck	B
process	O
exercises	O
approximation	O
methods	O
for	O
large	O
datasets	O
reduced-rank	O
approximations	O
of	O
the	O
gram	B
matrix	I
greedy	O
approximation	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
subset	B
of	I
regressors	I
the	O
nystr	O
om	O
method	O
projected	B
process	I
approximation	I
bayesian	B
committee	I
machine	I
comparison	O
of	O
approximate	O
gpr	O
methods	O
approximations	O
for	O
gpc	O
with	O
fixed	O
hyperparameters	B
approximating	O
the	O
marginal	B
likelihood	B
and	O
its	O
derivatives	O
appendix	O
equivalence	O
of	O
sr	O
and	O
gpr	O
using	O
the	O
nystr	O
om	O
approximate	O
iterative	O
solution	O
of	O
linear	B
systems	O
subset	B
of	I
datapoints	I
kernel	B
exercises	O
further	O
issues	O
and	O
conclusions	O
multiple	B
outputs	B
noise	O
models	O
with	O
dependencies	O
non-gaussian	B
likelihoods	O
derivative	B
observations	I
prediction	O
with	O
uncertain	B
inputs	I
mixtures	O
of	O
gaussian	O
processes	O
global	O
optimization	O
evaluation	O
of	O
integrals	B
student	O
s	O
t	O
process	O
invariances	B
latent	O
variable	O
models	O
conclusions	O
and	O
future	O
directions	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
x	O
contents	O
appendix	O
a	O
mathematical	O
background	O
joint	B
marginal	B
and	O
conditional	B
probability	O
gaussian	O
identities	O
matrix	O
identities	O
matrix	O
derivatives	O
matrix	O
norms	O
cholesky	B
decomposition	I
entropy	B
and	O
kullback-leibler	B
divergence	I
limits	O
measure	B
and	O
integration	O
lp	O
spaces	O
fourier	O
transforms	O
convexity	O
appendix	O
b	O
gaussian	O
markov	O
processes	O
fourier	O
analysis	O
continuous-time	O
gaussian	O
markov	O
processes	O
sampling	O
and	O
periodization	O
continuous-time	O
gmps	O
on	O
r	O
the	O
solution	O
of	O
the	O
corresponding	O
sde	O
on	O
the	O
circle	O
discrete-time	O
gmps	O
on	O
z	O
the	O
solution	O
of	O
the	O
corresponding	O
difference	O
equation	O
on	O
pn	O
discrete-time	O
gaussian	O
markov	O
processes	O
the	O
relationship	O
between	O
discrete-time	O
and	O
sampled	O
continuous-time	O
gmps	O
markov	O
processes	O
in	O
higher	O
dimensions	O
appendix	O
c	O
datasets	O
and	O
code	O
bibliography	O
author	O
index	O
subject	O
index	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
series	O
foreword	O
the	O
goal	O
of	O
building	O
systems	O
that	O
can	O
adapt	O
to	O
their	O
environments	O
and	O
learn	O
from	O
their	O
experience	O
has	O
attracted	O
researchers	O
from	O
many	O
fields	O
including	O
computer	O
science	O
engineering	O
mathematics	O
physics	O
neuroscience	O
and	O
cognitive	O
science	O
out	O
of	O
this	O
research	O
has	O
come	O
a	O
wide	O
variety	O
of	O
learning	B
techniques	O
that	O
have	O
the	O
potential	O
to	O
transform	O
many	O
scientific	O
and	O
industrial	O
fields	O
recently	O
several	O
research	O
communities	O
have	O
converged	O
on	O
a	O
common	O
set	B
of	O
issues	O
surrounding	O
supervised	O
unsupervised	O
and	O
reinforcement	O
learning	B
problems	O
the	O
mit	O
press	O
series	O
on	O
adaptive	O
computation	O
and	O
machine	O
learning	B
seeks	O
to	O
unify	O
the	O
many	O
diverse	O
strands	O
of	O
machine	O
learning	B
research	O
and	O
to	O
foster	O
high	O
quality	O
research	O
and	O
innovative	O
applications	O
one	O
of	O
the	O
most	O
active	O
directions	O
in	O
machine	O
learning	B
has	O
been	O
the	O
development	O
of	O
practical	O
bayesian	O
methods	O
for	O
challenging	O
learning	B
problems	O
gaussian	O
processes	O
for	O
machine	O
learning	B
presents	O
one	O
of	O
the	O
most	O
important	O
bayesian	O
machine	O
learning	B
approaches	O
based	O
on	O
a	O
particularly	O
effective	O
method	O
for	O
placing	O
a	O
prior	O
distribution	O
over	O
the	O
space	O
of	O
functions	O
carl	O
edward	O
rasmussen	O
and	O
chris	O
williams	O
are	O
two	O
of	O
the	O
pioneers	O
in	O
this	O
area	O
and	O
their	O
book	O
describes	O
the	O
mathematical	O
foundations	O
and	O
practical	O
application	O
of	O
gaussian	O
processes	O
in	O
regression	O
and	O
classification	B
tasks	O
they	O
also	O
show	O
how	O
gaussian	O
processes	O
can	O
be	O
interpreted	O
as	O
a	O
bayesian	O
version	O
of	O
the	O
well-known	O
support	B
vector	I
machine	I
methods	O
students	O
and	O
researchers	O
who	O
study	O
this	O
book	O
will	O
be	O
able	O
to	O
apply	O
gaussian	B
process	I
methods	O
in	O
creative	O
ways	O
to	O
solve	O
a	O
wide	O
range	O
of	O
problems	O
in	O
science	O
and	O
engineering	O
thomas	O
dietterich	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
preface	O
over	O
the	O
last	O
decade	O
there	O
has	O
been	O
an	O
explosion	O
of	O
work	O
in	O
the	O
kernel	B
machines	O
area	O
of	O
machine	O
learning	B
probably	O
the	O
best	O
known	O
example	O
of	O
this	O
is	O
work	O
on	O
support	B
vector	I
machines	O
but	O
during	O
this	O
period	O
there	O
has	O
also	O
been	O
much	O
activity	O
concerning	O
the	O
application	O
of	O
gaussian	B
process	I
models	O
to	O
machine	O
learning	B
tasks	O
the	O
goal	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
systematic	O
and	O
unified	O
treatment	O
of	O
this	O
area	O
gaussian	O
processes	O
provide	O
a	O
principled	O
practical	O
probabilistic	B
approach	O
to	O
learning	B
in	O
kernel	B
machines	O
this	O
gives	O
advantages	O
with	O
respect	O
to	O
the	O
interpretation	O
of	O
model	O
predictions	O
and	O
provides	O
a	O
wellfounded	O
framework	O
for	O
learning	B
and	O
model	O
selection	O
theoretical	O
and	O
practical	O
developments	O
of	O
over	O
the	O
last	O
decade	O
have	O
made	O
gaussian	O
processes	O
a	O
serious	O
competitor	O
for	O
real	O
supervised	B
learning	B
applications	O
roughly	O
speaking	O
a	O
stochastic	O
process	O
is	O
a	O
generalization	B
of	O
a	O
probability	O
distribution	O
describes	O
a	O
finite-dimensional	O
random	O
variable	O
to	O
functions	O
by	O
focussing	O
on	O
processes	O
which	O
are	O
gaussian	O
it	O
turns	O
out	O
that	O
the	O
computations	O
required	O
for	O
inference	O
and	O
learning	B
become	O
relatively	O
easy	O
thus	O
the	O
supervised	B
learning	B
problems	O
in	O
machine	O
learning	B
which	O
can	O
be	O
thought	O
of	O
as	O
learning	B
a	O
function	B
from	O
examples	O
can	O
be	O
cast	O
directly	O
into	O
the	O
gaussian	B
process	I
framework	O
our	O
interest	O
in	O
gaussian	B
process	I
models	O
in	O
the	O
context	O
of	O
machine	O
learning	B
was	O
aroused	O
in	O
while	O
we	O
were	O
both	O
graduate	O
students	O
in	O
geoff	O
hinton	O
s	O
neural	O
networks	O
lab	O
at	O
the	O
university	O
of	O
toronto	O
this	O
was	O
a	O
time	O
when	O
the	O
field	O
of	O
neural	O
networks	O
was	O
becoming	O
mature	O
and	O
the	O
many	O
connections	O
to	O
statistical	O
physics	O
probabilistic	B
models	O
and	O
statistics	O
became	O
well	O
known	O
and	O
the	O
first	O
kernel-based	O
learning	B
algorithms	O
were	O
becoming	O
popular	O
in	O
retrospect	O
it	O
is	O
clear	O
that	O
the	O
time	O
was	O
ripe	O
for	O
the	O
application	O
of	O
gaussian	O
processes	O
to	O
machine	O
learning	B
problems	O
many	O
researchers	O
were	O
realizing	O
that	O
neural	O
networks	O
were	O
not	O
so	O
easy	O
to	O
apply	O
in	O
practice	O
due	O
to	O
the	O
many	O
decisions	O
which	O
needed	O
to	O
be	O
made	O
what	O
architecture	O
what	O
activation	O
functions	O
what	O
learning	B
rate	O
etc	O
and	O
the	O
lack	O
of	O
a	O
principled	O
framework	O
to	O
answer	O
these	O
questions	O
the	O
probabilistic	B
framework	O
was	O
pursued	O
using	O
approximations	O
by	O
mackay	O
and	O
using	O
markov	B
chain	I
monte	I
carlo	I
methods	O
by	O
neal	O
neal	O
was	O
also	O
a	O
graduate	O
student	O
in	O
the	O
same	O
lab	O
and	O
in	O
his	O
thesis	O
he	O
sought	O
to	O
demonstrate	O
that	O
using	O
the	O
bayesian	O
formalism	O
one	O
does	O
not	O
necessarily	O
have	O
problems	O
with	O
overfitting	O
when	O
the	O
models	O
get	O
large	O
and	O
one	O
should	O
pursue	O
the	O
limit	O
of	O
large	O
models	O
while	O
his	O
own	O
work	O
was	O
focused	O
on	O
sophisticated	O
markov	O
chain	O
methods	O
for	O
inference	O
in	O
large	O
finite	O
networks	O
he	O
did	O
point	O
out	O
that	O
some	O
of	O
his	O
networks	O
became	O
gaussian	O
processes	O
in	O
the	O
limit	O
of	O
infinite	O
size	O
and	O
there	O
may	O
be	O
simpler	O
ways	O
to	O
do	O
inference	O
in	O
this	O
case	O
it	O
is	O
perhaps	O
interesting	O
to	O
mention	O
a	O
slightly	O
wider	O
historical	O
perspective	O
the	O
main	O
reason	O
why	O
neural	O
networks	O
became	O
popular	O
was	O
that	O
they	O
allowed	O
the	O
use	O
of	O
adaptive	O
basis	O
functions	O
as	O
opposed	O
to	O
the	O
well	O
known	O
linear	B
models	O
the	O
adaptive	O
basis	O
functions	O
or	O
hidden	O
units	O
could	O
learn	O
hidden	O
features	O
kernel	B
machines	O
gaussian	B
process	I
gaussian	O
processes	O
in	O
machine	O
learning	B
neural	O
networks	O
large	O
neural	O
networks	O
gaussian	O
processes	O
adaptive	O
basis	O
functions	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
xiv	O
many	O
fixed	O
basis	O
functions	O
useful	O
representations	O
supervised	B
learning	B
in	O
statistics	O
statistics	O
and	O
machine	O
learning	B
data	O
and	O
models	O
algorithms	O
and	O
predictions	O
bridging	O
the	O
gap	O
preface	O
useful	O
for	O
the	O
modelling	O
problem	O
at	O
hand	O
however	O
this	O
adaptivity	O
came	O
at	O
the	O
cost	O
of	O
a	O
lot	O
of	O
practical	O
problems	O
later	O
with	O
the	O
advancement	O
of	O
the	O
kernel	B
era	O
it	O
was	O
realized	O
that	O
the	O
limitation	O
of	O
fixed	O
basis	O
functions	O
is	O
not	O
a	O
big	O
restriction	O
if	O
only	O
one	O
has	O
enough	O
of	O
them	O
i	O
e	O
typically	O
infinitely	O
many	O
and	O
one	O
is	O
careful	O
to	O
control	O
problems	O
of	O
overfitting	O
by	O
using	O
priors	O
or	O
regularization	B
the	O
resulting	O
models	O
are	O
much	O
easier	O
to	O
handle	O
than	O
the	O
adaptive	O
basis	O
function	B
models	O
but	O
have	O
similar	O
expressive	O
power	O
thus	O
one	O
could	O
claim	O
that	O
far	O
a	O
machine	O
learning	B
is	O
concerned	O
the	O
adaptive	O
basis	O
functions	O
were	O
merely	O
a	O
decade-long	O
digression	O
and	O
we	O
are	O
now	O
back	O
to	O
where	O
we	O
came	O
from	O
this	O
view	O
is	O
perhaps	O
reasonable	O
if	O
we	O
think	O
of	O
models	O
for	O
solving	O
practical	O
learning	B
problems	O
although	O
mackay	O
ch	O
for	O
example	O
raises	O
concerns	O
by	O
asking	O
did	O
we	O
throw	O
out	O
the	O
baby	O
with	O
the	O
bath	O
water	O
as	O
the	O
kernel	B
view	O
does	O
not	O
give	O
us	O
any	O
hidden	O
representations	O
telling	O
us	O
what	O
the	O
useful	O
features	O
are	O
for	O
solving	O
a	O
particular	O
problem	O
as	O
we	O
will	O
argue	O
in	O
the	O
book	O
one	O
answer	O
may	O
be	O
to	O
learn	O
more	O
sophisticated	O
covariance	B
functions	O
and	O
the	O
hidden	O
properties	O
of	O
the	O
problem	O
are	O
to	O
be	O
found	O
here	O
an	O
important	O
area	O
of	O
future	O
developments	O
for	O
gp	O
models	O
is	O
the	O
use	O
of	O
more	O
expressive	O
covariance	B
functions	O
supervised	B
learning	B
problems	O
have	O
been	O
studied	O
for	O
more	O
than	O
a	O
century	O
in	O
statistics	O
and	O
a	O
large	O
body	O
of	O
well-established	O
theory	O
has	O
been	O
developed	O
more	O
recently	O
with	O
the	O
advance	O
of	O
affordable	O
fast	O
computation	O
the	O
machine	O
learning	B
community	O
has	O
addressed	O
increasingly	O
large	O
and	O
complex	O
problems	O
much	O
of	O
the	O
basic	O
theory	O
and	O
many	O
algorithms	O
are	O
shared	O
between	O
the	O
statistics	O
and	O
machine	O
learning	B
community	O
the	O
primary	O
differences	O
are	O
perhaps	O
the	O
types	O
of	O
the	O
problems	O
attacked	O
and	O
the	O
goal	O
of	O
learning	B
at	O
the	O
risk	B
of	O
oversimplification	O
one	O
could	O
say	O
that	O
in	O
statistics	O
a	O
prime	O
focus	O
is	O
often	O
in	O
understanding	O
the	O
data	O
and	O
relationships	O
in	O
terms	O
of	O
models	O
giving	O
approximate	O
summaries	O
such	O
as	O
linear	B
relations	O
or	O
independencies	O
in	O
contrast	O
the	O
goals	O
in	O
machine	O
learning	B
are	O
primarily	O
to	O
make	O
predictions	O
as	O
accurately	O
as	O
possible	O
and	O
to	O
understand	O
the	O
behaviour	O
of	O
learning	B
algorithms	O
these	O
differing	O
objectives	O
have	O
led	O
to	O
different	O
developments	O
in	O
the	O
two	O
fields	O
for	O
example	O
neural	B
network	I
algorithms	O
have	O
been	O
used	O
extensively	O
as	O
black-box	O
function	B
approximators	O
in	O
machine	O
learning	B
but	O
to	O
many	O
statisticians	O
they	O
are	O
less	O
than	O
satisfactory	O
because	O
of	O
the	O
difficulties	O
in	O
interpreting	O
such	O
models	O
gaussian	B
process	I
models	O
in	O
some	O
sense	O
bring	O
together	O
work	O
in	O
the	O
two	O
communities	O
as	O
we	O
will	O
see	O
gaussian	O
processes	O
are	O
mathematically	O
equivalent	B
to	O
many	O
well	O
known	O
models	O
including	O
bayesian	O
linear	B
models	O
spline	O
models	O
large	O
neural	O
networks	O
suitable	O
conditions	O
and	O
are	O
closely	O
related	O
to	O
others	O
such	O
as	O
support	B
vector	I
machines	O
under	O
the	O
gaussian	B
process	I
viewpoint	O
the	O
models	O
may	O
be	O
easier	O
to	O
handle	O
and	O
interpret	O
than	O
their	O
conventional	O
counterparts	O
such	O
as	O
e	O
g	O
neural	O
networks	O
in	O
the	O
statistics	O
community	O
gaussian	O
processes	O
have	O
also	O
been	O
discussed	O
many	O
times	O
although	O
it	O
would	O
probably	O
be	O
excessive	O
to	O
claim	O
that	O
their	O
use	O
is	O
widespread	O
except	O
for	O
certain	O
specific	O
applications	O
such	O
as	O
spatial	O
models	O
in	O
meteorology	O
and	O
geology	O
and	O
the	O
analysis	O
of	O
computer	O
experiments	O
a	O
rich	O
theory	O
also	O
exists	O
for	O
gaussian	B
process	I
models	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
preface	O
in	O
the	O
time	O
series	O
analysis	O
literature	O
some	O
pointers	O
to	O
this	O
literature	O
are	O
given	O
in	O
appendix	O
b	O
the	O
book	O
is	O
primarily	O
intended	O
for	O
graduate	O
students	O
and	O
researchers	O
in	O
machine	O
learning	B
at	O
departments	O
of	O
computer	O
science	O
statistics	O
and	O
applied	O
mathematics	O
as	O
prerequisites	O
we	O
require	O
a	O
good	O
basic	O
grounding	O
in	O
calculus	O
linear	B
algebra	O
and	O
probability	O
theory	O
as	O
would	O
be	O
obtained	O
by	O
graduates	O
in	O
numerate	O
disciplines	O
such	O
as	O
electrical	O
engineering	O
physics	O
and	O
computer	O
science	O
for	O
preparation	O
in	O
calculus	O
and	O
linear	B
algebra	O
any	O
good	O
university-level	O
textbook	O
on	O
mathematics	O
for	O
physics	O
or	O
engineering	O
such	O
as	O
arfken	O
would	O
be	O
fine	O
for	O
probability	O
theory	O
some	O
familiarity	O
with	O
multivariate	O
distributions	O
the	O
gaussian	O
and	O
conditional	B
probability	O
is	O
required	O
some	O
background	O
mathematical	O
material	O
is	O
also	O
provided	O
in	O
appendix	O
a	O
the	O
main	O
focus	O
of	O
the	O
book	O
is	O
to	O
present	O
clearly	O
and	O
concisely	O
an	O
overview	O
of	O
the	O
main	O
ideas	O
of	O
gaussian	O
processes	O
in	O
a	O
machine	O
learning	B
context	O
we	O
have	O
also	O
covered	O
a	O
wide	O
range	O
of	O
connections	O
to	O
existing	O
models	O
in	O
the	O
literature	O
and	O
cover	O
approximate	O
inference	O
for	O
faster	O
practical	O
algorithms	O
we	O
have	O
presented	O
detailed	O
algorithms	O
for	O
many	O
methods	O
to	O
aid	O
the	O
practitioner	O
software	O
implementations	O
are	O
available	O
from	O
the	O
website	O
for	O
the	O
book	O
see	O
appendix	O
c	O
we	O
have	O
also	O
included	O
a	O
small	O
set	B
of	O
exercises	O
in	O
each	O
chapter	O
we	O
hope	O
these	O
will	O
help	O
in	O
gaining	O
a	O
deeper	O
understanding	O
of	O
the	O
material	O
in	O
order	O
limit	O
the	O
size	O
of	O
the	O
volume	O
we	O
have	O
had	O
to	O
omit	O
some	O
topics	O
such	O
as	O
for	O
example	O
markov	B
chain	I
monte	I
carlo	I
methods	O
for	O
inference	O
one	O
of	O
the	O
most	O
difficult	O
things	O
to	O
decide	O
when	O
writing	O
a	O
book	O
is	O
what	O
sections	O
not	O
to	O
write	O
within	O
sections	O
we	O
have	O
often	O
chosen	O
to	O
describe	O
one	O
algorithm	O
in	O
particular	O
in	O
depth	O
and	O
mention	O
related	O
work	O
only	O
in	O
passing	O
although	O
this	O
causes	O
the	O
omission	O
of	O
some	O
material	O
we	O
feel	O
it	O
is	O
the	O
best	O
approach	O
for	O
a	O
monograph	O
and	O
hope	O
that	O
the	O
reader	O
will	O
gain	O
a	O
general	O
understanding	O
so	O
as	O
to	O
be	O
able	O
to	O
push	O
further	O
into	O
the	O
growing	O
literature	O
of	O
gp	O
models	O
the	O
book	O
has	O
a	O
natural	O
split	O
into	O
two	O
parts	O
with	O
the	O
chapters	O
up	O
to	O
and	O
including	O
chapter	O
covering	O
core	O
material	O
and	O
the	O
remaining	O
sections	O
covering	O
the	O
connections	O
to	O
other	O
methods	O
fast	O
approximations	O
and	O
more	O
specialized	O
properties	O
some	O
sections	O
are	O
marked	O
by	O
an	O
asterisk	O
these	O
sections	O
may	O
be	O
omitted	O
on	O
a	O
first	O
reading	O
and	O
are	O
not	O
pre-requisites	O
for	O
later	O
material	O
we	O
wish	O
to	O
express	O
our	O
considerable	O
gratitude	O
to	O
the	O
many	O
people	O
with	O
whom	O
we	O
have	O
interacted	O
during	O
the	O
writing	O
of	O
this	O
book	O
in	O
particular	O
moray	O
allan	O
david	O
barber	O
peter	O
bartlett	O
miguel	O
carreira-perpi	O
n	O
an	O
marcus	O
gallagher	O
manfred	O
opper	O
anton	O
schwaighofer	O
matthias	O
seeger	B
hanna	O
wallach	O
joe	O
whittaker	O
and	O
andrew	O
zisserman	O
all	O
read	O
parts	O
of	O
the	O
book	O
and	O
provided	O
valuable	O
feedback	O
dilan	O
g	O
or	O
ur	O
malte	O
kuss	O
iain	O
murray	O
joaquin	O
qui	O
nonerocandela	O
leif	O
rasmussen	O
and	O
sam	O
roweis	O
were	O
especially	O
heroic	O
and	O
provided	O
comments	O
on	O
the	O
whole	O
manuscript	O
we	O
thank	O
chris	O
bishop	O
miguel	O
carreiraperpi	O
n	O
an	O
nando	O
de	O
freitas	O
zoubin	O
ghahramani	O
peter	O
gr	O
unwald	O
mike	O
jordan	O
john	O
kent	O
radford	O
neal	O
joaquin	O
qui	O
nonero-candela	O
ryan	O
rifkin	O
stefan	O
schaal	O
anton	O
schwaighofer	O
matthias	O
seeger	B
peter	O
sollich	O
ingo	O
steinwart	O
xv	O
intended	O
audience	O
focus	O
scope	O
book	O
organization	O
acknowledgements	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
xvi	O
preface	O
errata	O
looking	O
ahead	O
amos	O
storkey	O
volker	O
tresp	O
sethu	O
vijayakumar	O
grace	O
wahba	O
joe	O
whittaker	O
and	O
tong	O
zhang	O
for	O
valuable	O
discussions	O
on	O
specific	O
issues	O
we	O
also	O
thank	O
bob	O
prior	O
and	O
the	O
staff	O
at	O
mit	O
press	O
for	O
their	O
support	O
during	O
the	O
writing	O
of	O
the	O
book	O
we	O
thank	O
the	O
gatsby	O
computational	O
neuroscience	O
unit	O
and	O
neil	O
lawrence	O
at	O
the	O
department	O
of	O
computer	O
science	O
university	O
of	O
sheffield	O
for	O
hosting	O
our	O
visits	O
and	O
kindly	O
providing	O
space	O
for	O
us	O
to	O
work	O
and	O
the	O
department	O
of	O
computer	O
science	O
at	O
the	O
university	O
of	O
toronto	O
for	O
computer	O
support	O
thanks	O
to	O
john	O
and	O
fiona	O
for	O
their	O
hospitality	O
on	O
numerous	O
occasions	O
some	O
of	O
the	O
diagrams	O
in	O
this	O
book	O
have	O
been	O
inspired	O
by	O
similar	O
diagrams	O
appearing	O
in	O
published	O
work	O
as	O
follows	O
figure	O
sch	O
olkopf	O
and	O
smola	O
figure	O
mackay	O
cer	O
gratefully	O
acknowledges	O
financial	O
support	O
from	O
the	O
german	O
research	O
foundation	O
ckiw	O
thanks	O
the	O
school	O
of	O
informatics	O
university	O
of	O
edinburgh	O
for	O
granting	O
him	O
sabbatical	O
leave	O
for	O
the	O
period	O
october	O
finally	O
we	O
reserve	O
our	O
deepest	O
appreciation	O
for	O
our	O
wives	O
agnes	O
and	O
barbara	O
and	O
children	O
ezra	O
kate	O
miro	O
and	O
ruth	O
for	O
their	O
patience	O
and	O
understanding	O
while	O
the	O
book	O
was	O
being	O
written	O
despite	O
our	O
best	O
efforts	O
it	O
is	O
inevitable	O
that	O
some	O
errors	O
will	O
make	O
it	O
through	O
to	O
the	O
printed	O
version	O
of	O
the	O
book	O
errata	O
will	O
be	O
made	O
available	O
via	O
the	O
book	O
s	O
website	O
at	O
httpwww	O
gaussianprocess	O
orggpml	O
we	O
have	O
found	O
the	O
joint	B
writing	O
of	O
this	O
book	O
an	O
excellent	O
experience	O
although	O
hard	O
at	O
times	O
we	O
are	O
confident	O
that	O
the	O
end	O
result	O
is	O
much	O
better	O
than	O
either	O
one	O
of	O
us	O
could	O
have	O
written	O
alone	O
now	O
ten	O
years	O
after	O
their	O
first	O
introduction	O
into	O
the	O
machine	O
learning	B
community	O
gaussian	O
processes	O
are	O
receiving	O
growing	O
attention	O
although	O
gps	O
have	O
been	O
known	O
for	O
a	O
long	O
time	O
in	O
the	O
statistics	O
and	O
geostatistics	B
fields	O
and	O
their	O
use	O
can	O
perhaps	O
be	O
traced	O
back	O
as	O
far	O
as	O
the	O
end	O
of	O
the	O
century	O
their	O
application	O
to	O
real	O
problems	O
is	O
still	O
in	O
its	O
early	O
phases	O
this	O
contrasts	O
somewhat	O
the	O
application	O
of	O
the	O
non-probabilistic	O
analogue	O
of	O
the	O
gp	O
the	O
support	B
vector	I
machine	I
which	O
was	O
taken	O
up	O
more	O
quickly	O
by	O
practitioners	O
perhaps	O
this	O
has	O
to	O
do	O
with	O
the	O
probabilistic	B
mind-set	O
needed	O
to	O
understand	O
gps	O
which	O
is	O
not	O
so	O
generally	O
appreciated	O
perhaps	O
it	O
is	O
due	O
to	O
the	O
need	O
for	O
computational	O
short-cuts	O
to	O
implement	O
inference	O
for	O
large	O
datasets	O
or	O
it	O
could	O
be	O
due	O
to	O
the	O
lack	O
of	O
a	O
self-contained	O
introduction	O
to	O
this	O
exciting	O
field	O
with	O
this	O
volume	O
we	O
hope	O
to	O
contribute	O
to	O
the	O
momentum	O
gained	O
by	O
gaussian	O
processes	O
in	O
machine	O
learning	B
carl	O
edward	O
rasmussen	O
and	O
chris	O
williams	O
t	O
ubingen	O
and	O
edinburgh	O
summer	O
second	O
printing	O
we	O
thank	O
baback	O
moghaddam	O
mikhail	O
parakhin	O
leif	O
rasmussen	O
benjamin	O
sobotta	O
kevin	O
s	O
van	O
horn	O
and	O
aki	O
vehtari	O
for	O
reporting	O
errors	O
in	O
the	O
first	O
printing	O
which	O
have	O
now	O
been	O
corrected	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
symbols	O
and	O
notation	O
matrices	O
are	O
capitalized	O
and	O
vectors	O
are	O
in	O
bold	O
type	O
we	O
do	O
not	O
generally	O
distinguish	O
between	O
probabilities	O
and	O
probability	O
densities	O
a	O
subscript	O
asterisk	O
such	O
as	O
in	O
x	O
indicates	O
reference	O
to	O
a	O
test	O
set	B
quantity	O
a	O
superscript	O
asterisk	O
denotes	O
complex	O
conjugate	O
symbol	O
c	O
hf	O
gih	O
kfkh	O
y	O
or	O
f	O
or	O
or	O
c	O
choleskya	O
covf	O
d	O
d	O
diagw	O
diagw	O
pq	O
e	O
or	O
eqxzx	O
fx	O
or	O
f	O
f	O
f	O
gp	O
hx	O
or	O
hx	O
h	O
or	O
hx	O
i	O
or	O
in	O
j	O
kx	O
k	O
or	O
kx	O
x	O
k	O
kx	O
or	O
k	O
kf	O
or	O
k	O
i	O
i	O
meaning	O
left	O
matrix	O
divide	O
ab	O
is	O
the	O
vector	O
x	O
which	O
solves	O
ax	O
b	O
an	O
equality	O
which	O
acts	O
as	O
a	O
definition	O
equality	O
up	O
to	O
an	O
additive	O
constant	O
determinant	O
of	O
k	O
matrix	O
euclidean	O
length	O
of	O
vector	O
y	O
rkhs	O
inner	O
product	O
rkhs	O
norm	O
the	O
transpose	O
of	O
vector	O
y	O
proportional	O
to	O
e	O
g	O
pxy	O
fx	O
y	O
means	O
that	O
pxy	O
is	O
equal	O
to	O
fx	O
y	O
times	O
a	O
factor	O
which	O
is	O
independent	O
of	O
x	O
distributed	O
according	O
to	O
example	O
x	O
n	O
partial	O
derivatives	O
f	O
the	O
matrix	O
of	O
second	O
derivatives	O
vector	O
of	O
all	O
s	O
length	O
n	O
vector	O
of	O
all	O
s	O
length	O
n	O
number	O
of	O
classes	O
in	O
a	O
classification	B
problem	O
cholesky	B
decomposition	I
l	O
is	O
a	O
lower	O
triangular	O
matrix	O
such	O
that	O
ll	O
a	O
gaussian	B
process	I
posterior	O
covariance	B
dimension	O
of	O
input	O
space	O
x	O
data	O
set	B
d	O
yii	O
n	O
argument	O
a	O
diagonal	O
matrix	O
containing	O
the	O
elements	O
of	O
vector	O
w	O
argument	O
a	O
vector	O
containing	O
the	O
diagonal	O
elements	O
of	O
matrix	O
w	O
kronecker	O
delta	O
pq	O
iff	O
p	O
q	O
and	O
otherwise	O
expectation	O
expectation	O
of	O
zx	O
when	O
x	O
qx	O
gaussian	B
process	I
vector	O
of	O
latent	O
function	B
values	O
f	O
fxn	O
gaussian	B
process	I
prediction	O
variable	O
gaussian	B
process	I
posterior	O
mean	O
gaussian	B
process	I
with	O
mean	B
function	B
mx	O
and	O
covariance	B
function	B
kx	O
either	O
fixed	O
basis	O
function	B
set	B
of	O
basis	O
functions	O
or	O
weight	B
function	B
set	B
of	O
basis	O
functions	O
evaluated	O
at	O
all	O
training	O
points	O
the	O
identity	O
matrix	O
size	O
n	O
bessel	O
function	B
of	O
the	O
first	O
kind	O
covariance	B
kernel	B
function	B
evaluated	O
at	O
x	O
and	O
n	O
n	O
covariance	B
gram	B
matrix	I
n	O
n	O
matrix	O
kx	O
x	O
the	O
covariance	B
between	O
training	O
and	O
test	O
cases	O
vector	O
short	O
for	O
kx	O
x	O
when	O
there	O
is	O
only	O
a	O
single	O
test	O
case	O
covariance	B
matrix	I
for	O
the	O
free	O
f	O
values	O
gaussian	B
process	I
f	O
kx	O
the	O
function	B
f	O
is	O
distributed	O
as	O
a	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
xviii	O
symbol	O
ky	O
k	O
la	O
b	O
logz	O
or	O
d	O
mx	O
n	O
or	O
n	O
n	O
n	O
and	O
n	O
n	O
nh	O
n	O
o	O
o	O
yx	O
and	O
pyx	O
pn	O
or	O
r	O
rlf	O
or	O
rlc	O
rllx	O
rc	O
ss	O
f	O
n	O
tra	O
tl	O
v	O
or	O
vqxzx	O
x	O
x	O
x	O
xi	O
xdi	O
z	O
symbols	O
and	O
notation	O
meaning	O
covariance	B
matrix	I
for	O
the	O
y	O
values	O
for	O
independent	O
homoscedastic	O
noise	O
ky	O
kf	O
ni	O
modified	O
bessel	O
function	B
loss	B
function	B
the	O
loss	B
of	O
predicting	O
b	O
when	O
a	O
is	O
true	O
note	O
argument	O
order	O
natural	O
logarithm	O
e	O
logarithm	O
to	O
the	O
base	O
characteristic	O
length-scale	B
input	O
dimension	O
d	O
logistic	B
function	B
exp	O
the	O
mean	B
function	B
of	O
a	O
gaussian	B
process	I
a	O
measure	B
section	O
variable	O
x	O
has	O
a	O
gaussian	B
distribution	I
with	O
mean	O
vector	O
and	O
covariance	B
matrix	I
short	O
for	O
unit	O
gaussian	O
x	O
n	O
i	O
number	O
of	O
training	O
test	O
cases	O
dimension	O
of	O
feature	B
space	I
number	O
of	O
hidden	O
units	O
in	O
a	O
neural	B
network	I
the	O
natural	O
numbers	O
the	O
positive	O
integers	O
big	O
oh	O
for	O
functions	O
f	O
and	O
g	O
on	O
n	O
we	O
write	O
fn	O
ogn	O
if	O
the	O
ratio	O
fngn	O
remains	O
bounded	O
as	O
n	O
either	O
matrix	O
of	O
all	O
zeros	O
or	O
differential	O
operator	B
conditional	B
random	O
variable	O
y	O
given	O
x	O
and	O
its	O
probability	O
the	O
regular	O
n-polygon	O
feature	O
map	B
of	O
input	O
xi	O
input	O
set	B
x	O
cumulative	O
unit	O
gaussian	O
z	O
exp	O
the	O
sigmoid	O
of	O
the	O
latent	O
value	O
if	O
fx	O
is	O
stochastic	O
map	B
prediction	O
evaluated	O
at	O
fx	O
mean	O
prediction	O
expected	O
value	O
of	O
note	O
in	O
general	O
that	O
the	O
real	O
numbers	O
the	O
risk	B
or	O
expected	O
loss	B
for	O
f	O
or	O
classifier	O
c	O
w	O
r	O
t	O
inputs	O
and	O
outputs	B
expected	O
loss	B
for	O
predicting	O
l	O
averaged	B
w	O
r	O
t	O
the	O
model	O
s	O
pred	O
distr	O
at	O
x	O
decision	B
region	I
for	O
class	O
c	O
power	O
spectrum	O
any	O
sigmoid	O
function	B
e	O
g	O
logistic	B
cumulative	O
gaussian	O
etc	O
variance	O
of	O
the	O
free	O
signal	O
noise	O
variance	O
vector	O
of	O
hyperparameters	B
of	O
the	O
covariance	B
function	B
trace	O
of	O
matrix	O
a	O
the	O
circle	O
with	O
circumference	O
l	O
variance	O
variance	O
of	O
zx	O
when	O
x	O
qx	O
input	O
space	O
and	O
also	O
the	O
index	B
set	B
for	O
the	O
stochastic	O
process	O
d	O
n	O
matrix	O
of	O
the	O
training	O
inputs	O
matrix	O
of	O
test	O
inputs	O
the	O
ith	O
training	O
input	O
the	O
dth	O
coordinate	O
of	O
the	O
ith	O
training	O
input	O
xi	O
the	O
integers	O
the	O
design	O
matrix	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
introduction	O
in	O
this	O
book	O
we	O
will	O
be	O
concerned	O
with	O
supervised	B
learning	B
which	O
is	O
the	O
problem	O
of	O
learning	B
input-output	O
mappings	O
from	O
empirical	O
data	O
training	O
dataset	O
depending	O
on	O
the	O
characteristics	O
of	O
the	O
output	O
this	O
problem	O
is	O
known	O
as	O
either	O
regression	O
for	O
continuous	O
outputs	B
or	O
classification	B
when	O
outputs	B
are	O
discrete	O
a	O
well	O
known	O
example	O
is	O
the	O
classification	B
of	O
images	O
of	O
handwritten	O
digits	O
the	O
training	O
set	B
consists	O
of	O
small	O
digitized	O
images	O
together	O
with	O
a	O
classification	B
from	O
normally	O
provided	O
by	O
a	O
human	O
the	O
goal	O
is	O
to	O
learn	O
a	O
mapping	O
from	O
image	O
to	O
classification	B
label	O
which	O
can	O
then	O
be	O
used	O
on	O
new	O
unseen	O
images	O
supervised	B
learning	B
is	O
an	O
attractive	O
way	O
to	O
attempt	O
to	O
tackle	O
this	O
problem	O
since	O
it	O
is	O
not	O
easy	O
to	O
specify	O
accurately	O
the	O
characteristics	O
of	O
say	O
the	O
handwritten	O
digit	O
an	O
example	O
of	O
a	O
regression	O
problem	O
can	O
be	O
found	O
in	O
robotics	O
where	O
we	O
wish	O
to	O
learn	O
the	O
inverse	O
dynamics	O
of	O
a	O
robot	B
arm	O
here	O
the	O
task	O
is	O
to	O
map	B
from	O
the	O
state	O
of	O
the	O
arm	O
by	O
the	O
positions	O
velocities	O
and	O
accelerations	O
of	O
the	O
joints	O
to	O
the	O
corresponding	O
torques	O
on	O
the	O
joints	O
such	O
a	O
model	O
can	O
then	O
be	O
used	O
to	O
compute	O
the	O
torques	O
needed	O
to	O
move	O
the	O
arm	O
along	O
a	O
given	O
trajectory	O
another	O
example	O
would	O
be	O
in	O
a	O
chemical	O
plant	O
where	O
we	O
might	O
wish	O
to	O
predict	O
the	O
yield	O
as	O
a	O
function	B
of	O
process	O
parameters	O
such	O
as	O
temperature	O
pressure	O
amount	O
of	O
catalyst	O
etc	O
in	O
general	O
we	O
denote	O
the	O
input	O
as	O
x	O
and	O
the	O
output	O
target	O
as	O
y	O
the	O
input	O
is	O
usually	O
represented	O
as	O
a	O
vector	O
x	O
as	O
there	O
are	O
in	O
general	O
many	O
input	O
variables	O
in	O
the	O
handwritten	O
digit	O
recognition	O
example	O
one	O
may	O
have	O
a	O
input	O
obtained	O
from	O
a	O
raster	O
scan	O
of	O
a	O
image	O
and	O
in	O
the	O
robot	B
arm	O
example	O
there	O
are	O
three	O
input	O
measurements	O
for	O
each	O
joint	B
in	O
the	O
arm	O
the	O
target	O
y	O
may	O
either	O
be	O
continuous	O
in	O
the	O
regression	O
case	O
or	O
discrete	O
in	O
the	O
classification	B
case	O
we	O
have	O
a	O
dataset	O
d	O
of	O
n	O
observations	O
d	O
yii	O
n	O
given	O
this	O
training	O
data	O
we	O
wish	O
to	O
make	O
predictions	O
for	O
new	O
inputs	O
x	O
that	O
we	O
have	O
not	O
seen	O
in	O
the	O
training	O
set	B
thus	O
it	O
is	O
clear	O
that	O
the	O
problem	O
at	O
hand	O
is	O
inductive	B
we	O
need	O
to	O
move	O
from	O
the	O
finite	O
training	O
data	O
d	O
to	O
a	O
digit	O
classification	B
robotic	O
control	O
the	O
dataset	O
training	O
is	O
inductive	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
introduction	O
two	O
approaches	O
gaussian	B
process	I
consistency	B
tractability	O
function	B
f	O
that	O
makes	O
predictions	O
for	O
all	O
possible	O
input	O
values	O
to	O
do	O
this	O
we	O
must	O
make	O
assumptions	O
about	O
the	O
characteristics	O
of	O
the	O
underlying	O
function	B
as	O
otherwise	O
any	O
function	B
which	O
is	O
consistent	O
with	O
the	O
training	O
data	O
would	O
be	O
equally	O
valid	O
a	O
wide	O
variety	O
of	O
methods	O
have	O
been	O
proposed	O
to	O
deal	O
with	O
the	O
supervised	B
learning	B
problem	O
here	O
we	O
describe	O
two	O
common	O
approaches	O
the	O
first	O
is	O
to	O
restrict	O
the	O
class	O
of	O
functions	O
that	O
we	O
consider	O
for	O
example	O
by	O
only	O
considering	O
linear	B
functions	O
of	O
the	O
input	O
the	O
second	O
approach	O
is	O
rather	O
loosely	O
to	O
give	O
a	O
prior	O
probability	O
to	O
every	O
possible	O
function	B
where	O
higher	O
probabilities	O
are	O
given	O
to	O
functions	O
that	O
we	O
consider	O
to	O
be	O
more	O
likely	O
for	O
example	O
because	O
they	O
are	O
smoother	O
than	O
other	O
the	O
first	O
approach	O
has	O
an	O
obvious	O
problem	O
in	O
that	O
we	O
have	O
to	O
decide	O
upon	O
the	O
richness	O
of	O
the	O
class	O
of	O
functions	O
considered	O
if	O
we	O
are	O
using	O
a	O
model	O
based	O
on	O
a	O
certain	O
class	O
of	O
functions	O
linear	B
functions	O
and	O
the	O
target	O
function	B
is	O
not	O
well	O
modelled	O
by	O
this	O
class	O
then	O
the	O
predictions	O
will	O
be	O
poor	O
one	O
may	O
be	O
tempted	O
to	O
increase	O
the	O
flexibility	O
of	O
the	O
class	O
of	O
functions	O
but	O
this	O
runs	O
into	O
the	O
danger	O
of	O
overfitting	O
where	O
we	O
can	O
obtain	O
a	O
good	O
fit	O
to	O
the	O
training	O
data	O
but	O
perform	O
badly	O
when	O
making	O
test	O
predictions	O
the	O
second	O
approach	O
appears	O
to	O
have	O
a	O
serious	O
problem	O
in	O
that	O
surely	O
there	O
are	O
an	O
uncountably	O
infinite	O
set	B
of	O
possible	O
functions	O
and	O
how	O
are	O
we	O
going	O
to	O
compute	O
with	O
this	O
set	B
in	O
finite	O
time	O
this	O
is	O
where	O
the	O
gaussian	B
process	I
comes	O
to	O
our	O
rescue	O
a	O
gaussian	B
process	I
is	O
a	O
generalization	B
of	O
the	O
gaussian	O
probability	O
distribution	O
whereas	O
a	O
probability	O
distribution	O
describes	O
random	O
variables	O
which	O
are	O
scalars	O
or	O
vectors	O
multivariate	O
distributions	O
a	O
stochastic	O
process	O
governs	O
the	O
properties	O
of	O
functions	O
leaving	O
mathematical	O
sophistication	O
aside	O
one	O
can	O
loosely	O
think	O
of	O
a	O
function	B
as	O
a	O
very	O
long	O
vector	O
each	O
entry	O
in	O
the	O
vector	O
specifying	O
the	O
function	B
value	O
fx	O
at	O
a	O
particular	O
input	O
x	O
it	O
turns	O
out	O
that	O
although	O
this	O
idea	O
is	O
a	O
little	O
na	O
ve	O
it	O
is	O
surprisingly	O
close	O
what	O
we	O
need	O
indeed	O
the	O
question	O
of	O
how	O
we	O
deal	O
computationally	O
with	O
these	O
infinite	O
dimensional	O
objects	O
has	O
the	O
most	O
pleasant	O
resolution	O
imaginable	O
if	O
you	O
ask	O
only	O
for	O
the	O
properties	O
of	O
the	O
function	B
at	O
a	O
finite	O
number	O
of	O
points	O
then	O
inference	O
in	O
the	O
gaussian	B
process	I
will	O
give	O
you	O
the	O
same	O
answer	O
if	O
you	O
ignore	O
the	O
infinitely	O
many	O
other	O
points	O
as	O
if	O
you	O
would	O
have	O
taken	O
them	O
all	O
into	O
account	O
and	O
these	O
answers	O
are	O
consistent	O
with	O
answers	O
to	O
any	O
other	O
finite	O
queries	O
you	O
may	O
have	O
one	O
of	O
the	O
main	O
attractions	O
of	O
the	O
gaussian	B
process	I
framework	O
is	O
precisely	O
that	O
it	O
unites	O
a	O
sophisticated	O
and	O
consistent	O
view	O
with	O
computational	O
tractability	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
these	O
ideas	O
have	O
been	O
around	O
for	O
some	O
time	O
although	O
they	O
are	O
perhaps	O
not	O
as	O
well	O
known	O
as	O
they	O
might	O
be	O
indeed	O
many	O
models	O
that	O
are	O
commonly	O
employed	O
in	O
both	O
machine	O
learning	B
and	O
statistics	O
are	O
in	O
fact	O
special	O
cases	O
of	O
or	O
restricted	O
kinds	O
of	O
gaussian	O
processes	O
in	O
this	O
volume	O
we	O
aim	O
to	O
give	O
a	O
systematic	O
and	O
unified	O
treatment	O
of	O
the	O
area	O
showing	O
connections	O
to	O
related	O
models	O
two	O
approaches	O
may	O
be	O
regarded	O
as	O
imposing	O
a	O
restriction	O
bias	O
and	O
a	O
preference	O
bias	O
respectively	O
see	O
e	O
g	O
mitchell	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
a	O
pictorial	O
introduction	O
to	O
bayesian	O
modelling	O
prior	O
posterior	O
figure	O
panel	O
shows	O
four	O
samples	O
drawn	O
from	O
the	O
prior	O
distribution	O
panel	O
shows	O
the	O
situation	O
after	O
two	O
datapoints	O
have	O
been	O
observed	O
the	O
mean	O
prediction	O
is	O
shown	O
as	O
the	O
solid	O
line	O
and	O
four	O
samples	O
from	O
the	O
posterior	O
are	O
shown	O
as	O
dashed	O
lines	O
in	O
both	O
plots	O
the	O
shaded	O
region	O
denotes	O
twice	O
the	O
standard	O
deviation	O
at	O
each	O
input	O
value	O
x	O
a	O
pictorial	O
introduction	O
to	O
bayesian	O
mod	O
elling	O
in	O
this	O
section	O
we	O
give	O
graphical	O
illustrations	O
of	O
how	O
the	O
second	O
method	O
works	O
on	O
some	O
simple	O
regression	O
and	O
classification	B
examples	O
we	O
first	O
consider	O
a	O
simple	O
regression	O
problem	O
mapping	O
from	O
an	O
input	O
x	O
to	O
an	O
output	O
fx	O
in	O
figure	O
we	O
show	O
a	O
number	O
of	O
sample	O
functions	O
drawn	O
at	O
random	O
from	O
the	O
prior	O
distribution	O
over	O
functions	O
specified	O
by	O
a	O
particular	O
gaussian	B
process	I
which	O
favours	O
smooth	O
functions	O
this	O
prior	O
is	O
taken	O
to	O
represent	O
our	O
prior	O
beliefs	O
over	O
the	O
kinds	O
of	O
functions	O
we	O
expect	O
to	O
observe	O
before	O
seeing	O
any	O
data	O
in	O
the	O
absence	O
of	O
knowledge	O
to	O
the	O
contrary	O
we	O
have	O
assumed	O
that	O
the	O
average	O
value	O
over	O
the	O
sample	O
functions	O
at	O
each	O
x	O
is	O
zero	O
although	O
the	O
specific	O
random	O
functions	O
drawn	O
in	O
figure	O
do	O
not	O
have	O
a	O
mean	O
of	O
zero	O
the	O
mean	O
of	O
fx	O
values	O
for	O
any	O
fixed	O
x	O
would	O
become	O
zero	O
independent	O
of	O
x	O
as	O
we	O
kept	O
on	O
drawing	O
more	O
functions	O
at	O
any	O
value	O
of	O
x	O
we	O
can	O
also	O
characterize	O
the	O
variability	O
of	O
the	O
sample	O
functions	O
by	O
computing	O
the	O
variance	O
at	O
that	O
point	O
the	O
shaded	O
region	O
denotes	O
twice	O
the	O
pointwise	O
standard	O
deviation	O
in	O
this	O
case	O
we	O
used	O
a	O
gaussian	B
process	I
which	O
specifies	O
that	O
the	O
prior	O
variance	O
does	O
not	O
depend	O
on	O
x	O
suppose	O
that	O
we	O
are	O
then	O
given	O
a	O
dataset	O
d	O
consisting	O
of	O
two	O
observations	O
and	O
we	O
wish	O
now	O
to	O
only	O
consider	O
functions	O
that	O
pass	O
though	O
these	O
two	O
data	O
points	O
exactly	O
is	O
also	O
possible	O
to	O
give	O
higher	O
preference	O
to	O
functions	O
that	O
merely	O
pass	O
close	O
to	O
the	O
datapoints	O
this	O
situation	O
is	O
illustrated	O
in	O
figure	O
the	O
dashed	O
lines	O
show	O
sample	O
functions	O
which	O
are	O
consistent	O
with	O
d	O
and	O
the	O
solid	O
line	O
depicts	O
the	O
mean	O
value	O
of	O
such	O
functions	O
notice	O
how	O
the	O
uncertainty	O
is	O
reduced	O
close	O
to	O
the	O
observations	O
the	O
combination	O
of	O
the	O
prior	O
and	O
the	O
data	O
leads	O
to	O
the	O
posterior	O
distribution	O
over	O
functions	O
regression	O
random	O
functions	O
mean	B
function	B
pointwise	O
variance	O
functions	O
that	O
agree	O
with	O
observations	O
posterior	O
over	O
functions	O
xfx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
non-parametric	B
inference	O
prior	O
specification	O
covariance	B
function	B
modelling	O
and	O
interpreting	O
classification	B
squashing	O
function	B
introduction	O
if	O
more	O
datapoints	O
were	O
added	O
one	O
would	O
see	O
the	O
mean	B
function	B
adjust	O
itself	O
to	O
pass	O
through	O
these	O
points	O
and	O
that	O
the	O
posterior	O
uncertainty	O
would	O
reduce	O
close	O
to	O
the	O
observations	O
notice	O
that	O
since	O
the	O
gaussian	B
process	I
is	O
not	O
a	O
parametric	B
model	O
we	O
do	O
not	O
have	O
to	O
worry	O
about	O
whether	O
it	O
is	O
possible	O
for	O
the	O
model	O
to	O
fit	O
the	O
data	O
would	O
be	O
the	O
case	O
if	O
e	O
g	O
you	O
tried	O
a	O
linear	B
model	O
on	O
strongly	O
non-linear	O
data	O
even	O
when	O
a	O
lot	O
of	O
observations	O
have	O
been	O
added	O
there	O
may	O
still	O
be	O
some	O
flexibility	O
left	O
in	O
the	O
functions	O
one	O
way	O
to	O
imagine	O
the	O
reduction	O
of	O
flexibility	O
in	O
the	O
distribution	O
of	O
functions	O
as	O
the	O
data	O
arrives	O
is	O
to	O
draw	O
many	O
random	O
functions	O
from	O
the	O
prior	O
and	O
reject	O
the	O
ones	O
which	O
do	O
not	O
agree	O
with	O
the	O
observations	O
while	O
this	O
is	O
a	O
perfectly	O
valid	O
way	O
to	O
do	O
inference	O
it	O
is	O
impractical	O
for	O
most	O
purposes	O
the	O
exact	O
analytical	O
computations	O
required	O
to	O
quantify	O
these	O
properties	O
will	O
be	O
detailed	O
in	O
the	O
next	O
chapter	O
the	O
specification	O
of	O
the	O
prior	O
is	O
important	O
because	O
it	O
fixes	O
the	O
properties	O
of	O
the	O
functions	O
considered	O
for	O
inference	O
above	O
we	O
briefly	O
touched	O
on	O
the	O
mean	O
and	O
pointwise	O
variance	O
of	O
the	O
functions	O
however	O
other	O
characteristics	O
can	O
also	O
be	O
specified	O
and	O
manipulated	O
note	O
that	O
the	O
functions	O
in	O
figure	O
are	O
smooth	O
and	O
stationary	O
stationarity	B
means	O
that	O
the	O
functions	O
look	O
similar	O
at	O
all	O
x	O
locations	O
these	O
are	O
properties	O
which	O
are	O
induced	O
by	O
the	O
covariance	B
function	B
of	O
the	O
gaussian	B
process	I
many	O
other	O
covariance	B
functions	O
are	O
possible	O
suppose	O
that	O
for	O
a	O
particular	O
application	O
we	O
think	O
that	O
the	O
functions	O
in	O
figure	O
vary	O
too	O
rapidly	O
that	O
their	O
characteristic	O
length-scale	B
is	O
too	O
short	O
slower	O
variation	O
is	O
achieved	O
by	O
simply	O
adjusting	O
parameters	O
of	O
the	O
covariance	B
function	B
the	O
problem	O
of	O
learning	B
in	O
gaussian	O
processes	O
is	O
exactly	O
the	O
problem	O
of	O
finding	O
suitable	O
properties	O
for	O
the	O
covariance	B
function	B
note	O
that	O
this	O
gives	O
us	O
a	O
model	O
of	O
the	O
data	O
and	O
characteristics	O
a	O
smoothness	O
characteristic	O
length-scale	B
etc	O
which	O
we	O
can	O
interpret	O
we	O
now	O
turn	O
to	O
the	O
classification	B
case	O
and	O
consider	O
the	O
binary	B
twoclass	O
classification	B
problem	O
an	O
example	O
of	O
this	O
is	O
classifying	O
objects	O
detected	O
in	O
astronomical	O
sky	O
surveys	O
into	O
stars	O
or	O
galaxies	O
our	O
data	O
has	O
the	O
label	O
for	O
stars	O
and	O
for	O
galaxies	O
and	O
our	O
task	O
will	O
be	O
to	O
predict	O
the	O
probability	O
that	O
an	O
example	O
with	O
input	O
vector	O
x	O
is	O
a	O
star	O
using	O
as	O
inputs	O
some	O
features	O
that	O
describe	O
each	O
object	O
obviously	O
should	O
lie	O
in	O
the	O
interval	O
a	O
gaussian	B
process	I
prior	O
over	O
functions	O
does	O
not	O
restrict	O
the	O
output	O
to	O
lie	O
in	O
this	O
interval	O
as	O
can	O
be	O
seen	O
from	O
figure	O
the	O
approach	O
that	O
we	O
shall	O
adopt	O
is	O
to	O
squash	O
the	O
prior	O
function	B
f	O
pointwise	O
through	O
a	O
response	B
function	B
which	O
restricts	O
the	O
output	O
to	O
lie	O
in	O
a	O
common	O
choice	O
for	O
this	O
function	B
is	O
the	O
logistic	B
function	B
exp	O
z	O
illustrated	O
in	O
figure	O
thus	O
the	O
prior	O
over	O
f	O
induces	O
a	O
prior	O
over	O
probabilistic	B
classifications	O
this	O
set	B
up	O
is	O
illustrated	O
in	O
figure	O
for	O
a	O
input	O
space	O
in	O
panel	O
we	O
see	O
a	O
sample	O
drawn	O
from	O
the	O
prior	O
over	O
functions	O
f	O
which	O
is	O
squashed	O
through	O
the	O
logistic	B
function	B
a	O
dataset	O
is	O
shown	O
in	O
panel	O
where	O
the	O
white	O
and	O
black	O
circles	O
denote	O
classes	O
and	O
respectively	O
as	O
in	O
the	O
regression	O
case	O
the	O
effect	O
of	O
the	O
data	O
is	O
to	O
downweight	O
in	O
the	O
posterior	O
those	O
functions	O
that	O
are	O
incompatible	O
with	O
the	O
data	O
a	O
contour	O
plot	O
of	O
the	O
posterior	O
mean	O
for	O
is	O
shown	O
in	O
panel	O
in	O
this	O
example	O
we	O
have	O
chosen	O
a	O
short	O
characteristic	O
length-scale	B
for	O
the	O
process	O
so	O
that	O
it	O
can	O
vary	O
fairly	O
rapidly	O
in	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
roadmap	O
figure	O
panel	O
shows	O
a	O
sample	O
from	O
prior	O
distribution	O
on	O
f	O
in	O
a	O
input	O
space	O
panel	O
is	O
a	O
plot	O
of	O
the	O
logistic	B
function	B
panel	O
shows	O
the	O
location	O
of	O
the	O
data	O
points	O
where	O
the	O
open	O
circles	O
denote	O
the	O
class	O
label	O
and	O
closed	O
circles	O
denote	O
the	O
class	O
label	O
panel	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
mean	O
predictive	B
probability	O
as	O
a	O
function	B
of	O
x	O
the	O
decision	O
boundaries	O
between	O
the	O
two	O
classes	O
are	O
shown	O
by	O
the	O
thicker	O
lines	O
this	O
case	O
notice	O
that	O
all	O
of	O
the	O
training	O
points	O
are	O
correctly	O
classified	O
including	O
the	O
two	O
outliers	O
in	O
the	O
ne	O
and	O
sw	O
corners	O
by	O
choosing	O
a	O
different	O
lengthscale	O
we	O
can	O
change	O
this	O
behaviour	O
as	O
illustrated	O
in	O
section	O
roadmap	O
the	O
book	O
has	O
a	O
natural	O
split	O
into	O
two	O
parts	O
with	O
the	O
chapters	O
up	O
to	O
and	O
including	O
chapter	O
covering	O
core	O
material	O
and	O
the	O
remaining	O
chapters	O
covering	O
the	O
connections	O
to	O
other	O
methods	O
fast	O
approximations	O
and	O
more	O
specialized	O
properties	O
some	O
sections	O
are	O
marked	O
by	O
an	O
asterisk	O
these	O
sections	O
may	O
be	O
omitted	O
on	O
a	O
first	O
reading	O
and	O
are	O
not	O
pre-requisites	O
for	O
later	O
material	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
classification	B
covariance	B
functions	O
learning	B
connections	O
theory	O
fast	O
approximations	O
introduction	O
chapter	O
contains	O
the	O
definition	O
of	O
gaussian	O
processes	O
in	O
particular	O
for	O
the	O
use	O
in	O
regression	O
it	O
also	O
discusses	O
the	O
computations	O
needed	O
to	O
make	O
predictions	O
for	O
regression	O
under	O
the	O
assumption	O
of	O
gaussian	O
observation	O
noise	O
the	O
computations	O
needed	O
to	O
make	O
predictions	O
are	O
tractable	O
and	O
are	O
dominated	O
by	O
the	O
inversion	O
of	O
a	O
n	O
n	O
matrix	O
in	O
a	O
short	O
experimental	O
section	O
the	O
gaussian	B
process	I
model	O
is	O
applied	O
to	O
a	O
robotics	O
task	O
chapter	O
considers	O
the	O
classification	B
problem	O
for	O
both	O
binary	B
and	O
multiclass	O
cases	O
the	O
use	O
of	O
a	O
non-linear	O
response	B
function	B
means	O
that	O
exact	O
computation	O
of	O
the	O
predictions	O
is	O
no	O
longer	O
possible	O
analytically	O
we	O
discuss	O
a	O
number	O
of	O
approximation	O
schemes	O
include	O
detailed	O
algorithms	O
for	O
their	O
implementation	O
and	O
discuss	O
some	O
experimental	O
comparisons	O
as	O
discussed	O
above	O
the	O
key	O
factor	O
that	O
controls	O
the	O
properties	O
of	O
a	O
gaussian	B
process	I
is	O
the	O
covariance	B
function	B
much	O
of	O
the	O
work	O
on	O
machine	O
learning	B
so	O
far	O
has	O
used	O
a	O
very	O
limited	O
set	B
of	O
covariance	B
functions	O
possibly	O
limiting	O
the	O
power	O
of	O
the	O
resulting	O
models	O
in	O
chapter	O
we	O
discuss	O
a	O
number	O
of	O
valid	O
covariance	B
functions	O
and	O
their	O
properties	O
and	O
provide	O
some	O
guidelines	O
on	O
how	O
to	O
combine	O
covariance	B
functions	O
into	O
new	O
ones	O
tailored	O
to	O
specific	O
needs	O
many	O
covariance	B
functions	O
have	O
adjustable	O
parameters	O
such	O
as	O
the	O
characteristic	O
length-scale	B
and	O
variance	O
illustrated	O
in	O
figure	O
chapter	O
describes	O
how	O
such	O
parameters	O
can	O
be	O
inferred	O
or	O
learned	O
from	O
the	O
data	O
based	O
on	O
either	O
bayesian	O
methods	O
the	O
marginal	B
likelihood	B
or	O
methods	O
of	O
crossvalidation	O
explicit	O
algorithms	O
are	O
provided	O
for	O
some	O
schemes	O
and	O
some	O
simple	O
practical	O
examples	O
are	O
demonstrated	O
gaussian	B
process	I
predictors	O
are	O
an	O
example	O
of	O
a	O
class	O
of	O
methods	O
known	O
as	O
kernel	B
machines	O
they	O
are	O
distinguished	O
by	O
the	O
probabilistic	B
viewpoint	O
taken	O
in	O
chapter	O
we	O
discuss	O
other	O
kernel	B
machines	O
such	O
as	O
support	B
vector	I
machines	O
splines	B
least-squares	B
classifiers	O
and	O
relevance	O
vector	O
machines	O
and	O
their	O
relationships	O
to	O
gaussian	B
process	I
prediction	O
in	O
chapter	O
we	O
discuss	O
a	O
number	O
of	O
more	O
theoretical	O
issues	O
relating	O
to	O
gaussian	B
process	I
methods	O
including	O
asymptotic	O
analysis	O
average-case	O
learning	B
curves	O
and	O
the	O
pac-bayesian	O
framework	O
one	O
issue	O
with	O
gaussian	B
process	I
prediction	O
methods	O
is	O
that	O
their	O
basic	O
complexity	O
is	O
due	O
to	O
the	O
inversion	O
of	O
a	O
n	O
n	O
matrix	O
for	O
large	O
datasets	O
this	O
is	O
prohibitive	O
both	O
time	O
and	O
space	O
and	O
so	O
a	O
number	O
of	O
approximation	O
methods	O
have	O
been	O
developed	O
as	O
described	O
in	O
chapter	O
the	O
main	O
focus	O
of	O
the	O
book	O
is	O
on	O
the	O
core	O
supervised	B
learning	B
problems	O
of	O
regression	O
and	O
classification	B
in	O
chapter	O
we	O
discuss	O
some	O
rather	O
less	O
standard	O
settings	O
that	O
gps	O
have	O
been	O
used	O
in	O
and	O
complete	O
the	O
main	O
part	O
of	O
the	O
book	O
with	O
some	O
conclusions	O
appendix	O
a	O
gives	O
some	O
mathematical	O
background	O
while	O
appendix	O
b	O
deals	O
specifically	O
with	O
gaussian	O
markov	O
processes	O
appendix	O
c	O
gives	O
details	O
of	O
how	O
to	O
access	O
the	O
data	O
and	O
programs	O
that	O
were	O
used	O
to	O
make	O
the	O
some	O
of	O
the	O
figures	O
and	O
run	O
the	O
experiments	O
described	O
in	O
the	O
book	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
regression	O
supervised	B
learning	B
can	O
be	O
divided	O
into	O
regression	O
and	O
classification	B
problems	O
whereas	O
the	O
outputs	B
for	O
classification	B
are	O
discrete	O
class	O
labels	O
regression	O
is	O
concerned	O
with	O
the	O
prediction	O
of	O
continuous	O
quantities	O
for	O
example	O
in	O
a	O
financial	O
application	O
one	O
may	O
attempt	O
to	O
predict	O
the	O
price	O
of	O
a	O
commodity	O
as	O
a	O
function	B
of	O
interest	O
rates	O
currency	O
exchange	O
rates	O
availability	O
and	O
demand	O
in	O
this	O
chapter	O
we	O
describe	O
gaussian	B
process	I
methods	O
for	O
regression	O
problems	O
classification	B
problems	O
are	O
discussed	O
in	O
chapter	O
there	O
are	O
several	O
ways	O
to	O
interpret	O
gaussian	B
process	I
regression	I
models	O
one	O
can	O
think	O
of	O
a	O
gaussian	B
process	I
as	O
defining	O
a	O
distribution	O
over	O
functions	O
and	O
inference	O
taking	O
place	O
directly	O
in	O
the	O
space	O
of	O
functions	O
the	O
function-space	O
view	O
although	O
this	O
view	O
is	O
appealing	O
it	O
may	O
initially	O
be	O
difficult	O
to	O
grasp	O
so	O
we	O
start	O
our	O
exposition	O
in	O
section	O
with	O
the	O
equivalent	B
weight-space	O
view	O
which	O
may	O
be	O
more	O
familiar	O
and	O
accessible	O
to	O
many	O
and	O
continue	O
in	O
section	O
with	O
the	O
function-space	O
view	O
gaussian	O
processes	O
often	O
have	O
characteristics	O
that	O
can	O
be	O
changed	O
by	O
setting	O
certain	O
parameters	O
and	O
in	O
section	O
we	O
discuss	O
how	O
the	O
properties	O
change	O
as	O
these	O
parameters	O
are	O
varied	O
the	O
predictions	O
from	O
a	O
gp	O
model	O
take	O
the	O
form	O
of	O
a	O
full	O
predictive	B
distribution	O
in	O
section	O
we	O
discuss	O
how	O
to	O
combine	O
a	O
loss	B
function	B
with	O
the	O
predictive	B
distributions	O
using	O
decision	O
theory	O
to	O
make	O
point	O
predictions	O
in	O
an	O
optimal	B
way	O
a	O
practical	O
comparative	O
example	O
involving	O
the	O
learning	B
of	O
the	O
inverse	O
dynamics	O
of	O
a	O
robot	B
arm	O
is	O
presented	O
in	O
section	O
we	O
give	O
some	O
theoretical	O
analysis	O
of	O
gaussian	B
process	I
regression	I
in	O
section	O
and	O
discuss	O
how	O
to	O
incorporate	O
explicit	O
basis	O
functions	O
into	O
the	O
models	O
in	O
section	O
as	O
much	O
of	O
the	O
material	O
in	O
this	O
chapter	O
can	O
be	O
considered	O
fairly	O
standard	O
we	O
postpone	O
most	O
references	O
to	O
the	O
historical	O
overview	O
in	O
section	O
weight-space	O
view	O
the	O
simple	O
linear	B
regression	I
model	O
where	O
the	O
output	O
is	O
a	O
linear	B
combination	O
of	O
the	O
inputs	O
has	O
been	O
studied	O
and	O
used	O
extensively	O
its	O
main	O
virtues	O
are	O
simplic	O
two	O
equivalent	B
views	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
ity	O
of	O
implementation	O
and	O
interpretability	O
its	O
main	O
drawback	O
is	O
that	O
it	O
only	O
allows	O
a	O
limited	O
flexibility	O
if	O
the	O
relationship	O
between	O
input	O
and	O
output	O
cannot	O
reasonably	O
be	O
approximated	O
by	O
a	O
linear	B
function	B
the	O
model	O
will	O
give	O
poor	O
predictions	O
in	O
this	O
section	O
we	O
first	O
discuss	O
the	O
bayesian	O
treatment	O
of	O
the	O
linear	B
model	O
we	O
then	O
make	O
a	O
simple	O
enhancement	O
to	O
this	O
class	O
of	O
models	O
by	O
projecting	O
the	O
inputs	O
into	O
a	O
high-dimensional	O
feature	B
space	I
and	O
applying	O
the	O
linear	B
model	O
there	O
we	O
show	O
that	O
in	O
some	O
feature	O
spaces	O
one	O
can	O
apply	O
the	O
kernel	B
trick	I
to	O
carry	O
out	O
computations	O
implicitly	O
in	O
the	O
high	O
dimensional	O
space	O
this	O
last	O
step	O
leads	O
to	O
computational	O
savings	O
when	O
the	O
dimensionality	O
of	O
the	O
feature	B
space	I
is	O
large	O
compared	O
to	O
the	O
number	O
of	O
data	O
points	O
we	O
have	O
a	O
training	O
set	B
d	O
of	O
n	O
observations	O
d	O
yi	O
i	O
n	O
where	O
x	O
denotes	O
an	O
input	O
vector	O
of	O
dimension	O
d	O
and	O
y	O
denotes	O
a	O
scalar	O
output	O
or	O
target	O
variable	O
the	O
column	O
vector	O
inputs	O
for	O
all	O
n	O
cases	O
are	O
aggregated	O
in	O
the	O
d	O
n	O
design	O
matrix	O
x	O
and	O
the	O
targets	O
are	O
collected	O
in	O
the	O
vector	O
y	O
so	O
we	O
can	O
write	O
d	O
y	O
in	O
the	O
regression	O
setting	O
the	O
targets	O
are	O
real	O
values	O
we	O
are	O
interested	O
in	O
making	O
inferences	O
about	O
the	O
relationship	O
between	O
inputs	O
and	O
targets	O
i	O
e	O
the	O
conditional	B
distribution	O
of	O
the	O
targets	O
given	O
the	O
inputs	O
we	O
are	O
not	O
interested	O
in	O
modelling	O
the	O
input	O
distribution	O
itself	O
the	O
standard	O
linear	B
model	O
we	O
will	O
review	O
the	O
bayesian	O
analysis	O
of	O
the	O
standard	O
linear	B
regression	I
model	O
with	O
gaussian	O
noise	O
fx	O
xw	O
y	O
fx	O
where	O
x	O
is	O
the	O
input	O
vector	O
w	O
is	O
a	O
vector	O
of	O
weights	O
of	O
the	O
linear	B
model	O
f	O
is	O
the	O
function	B
value	O
and	O
y	O
is	O
the	O
observed	O
target	O
value	O
often	O
a	O
bias	O
weight	O
or	O
offset	O
is	O
included	O
but	O
as	O
this	O
can	O
be	O
implemented	O
by	O
augmenting	O
the	O
input	O
vector	O
x	O
with	O
an	O
additional	O
element	O
whose	O
value	O
is	O
always	O
one	O
we	O
do	O
not	O
explicitly	O
include	O
it	O
in	O
our	O
notation	O
we	O
have	O
assumed	O
that	O
the	O
observed	O
values	O
y	O
differ	O
from	O
the	O
function	B
values	O
fx	O
by	O
additive	O
noise	O
and	O
we	O
will	O
further	O
assume	O
that	O
this	O
noise	O
follows	O
an	O
independent	O
identically	O
distributed	O
gaussian	B
distribution	I
with	O
zero	O
mean	O
and	O
variance	O
n	O
n	O
n	O
training	O
set	B
design	O
matrix	O
bias	O
offset	O
likelihood	B
this	O
noise	O
assumption	O
together	O
with	O
the	O
model	O
directly	O
gives	O
rise	O
to	O
the	O
likelihood	B
the	O
probability	O
density	O
of	O
the	O
observations	O
given	O
the	O
parameters	O
which	O
is	O
statistics	O
texts	O
the	O
design	O
matrix	O
is	O
usually	O
taken	O
to	O
be	O
the	O
transpose	O
of	O
our	O
definition	O
but	O
our	O
choice	O
is	O
deliberate	O
and	O
has	O
the	O
advantage	O
that	O
a	O
data	O
point	O
is	O
a	O
standard	O
vector	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
weight-space	O
view	O
factored	O
over	O
cases	O
in	O
the	O
training	O
set	B
of	O
the	O
independence	O
assumption	O
to	O
give	O
ny	O
pyx	O
w	O
pyixi	O
w	O
ny	O
n	O
x	O
n	O
i	O
n	O
n	O
ni	O
where	O
denotes	O
the	O
euclidean	O
length	O
of	O
vector	O
z	O
in	O
the	O
bayesian	O
formalism	O
we	O
need	O
to	O
specify	O
a	O
prior	O
over	O
the	O
parameters	O
expressing	O
our	O
beliefs	O
about	O
the	O
parameters	O
before	O
we	O
look	O
at	O
the	O
observations	O
we	O
put	O
a	O
zero	O
mean	O
gaussian	O
prior	O
with	O
covariance	B
matrix	I
p	O
on	O
the	O
weights	O
w	O
n	O
p	O
the	O
r	O
ole	O
and	O
properties	O
of	O
this	O
prior	O
will	O
be	O
discussed	O
in	O
section	O
for	O
now	O
we	O
will	O
continue	O
the	O
derivation	O
with	O
the	O
prior	O
as	O
specified	O
inference	O
in	O
the	O
bayesian	O
linear	B
model	O
is	O
based	O
on	O
the	O
posterior	O
distribution	O
over	O
the	O
weights	O
computed	O
by	O
bayes	O
rule	O
eq	O
posterior	O
likelihood	B
prior	O
marginal	B
likelihood	B
pwy	O
x	O
pyx	O
wpw	O
pyx	O
where	O
the	O
normalizing	O
constant	O
also	O
known	O
as	O
the	O
marginal	B
likelihood	B
page	O
is	O
independent	O
of	O
the	O
weights	O
and	O
given	O
by	O
pyx	O
pyx	O
wpw	O
dw	O
z	O
prior	O
posterior	O
marginal	B
likelihood	B
the	O
posterior	O
in	O
eq	O
combines	O
the	O
likelihood	B
and	O
the	O
prior	O
and	O
captures	O
everything	O
we	O
know	O
about	O
the	O
parameters	O
writing	O
only	O
the	O
terms	O
from	O
the	O
likelihood	B
and	O
prior	O
which	O
depend	O
on	O
the	O
weights	O
and	O
completing	O
the	O
square	O
we	O
obtain	O
pwx	O
y	O
xwy	O
xx	O
p	O
w	O
n	O
p	O
n	O
where	O
w	O
posterior	O
distribution	O
as	O
gaussian	O
with	O
mean	O
w	O
and	O
covariance	B
matrix	I
a	O
p	O
and	O
we	O
recognize	O
the	O
form	O
of	O
the	O
n	O
xx	O
n	O
pwx	O
y	O
n	O
w	O
a	O
a	O
n	O
n	O
xx	O
where	O
a	O
p	O
notice	O
that	O
for	O
this	O
model	O
indeed	O
for	O
any	O
gaussian	O
posterior	O
the	O
mean	O
of	O
the	O
posterior	O
distribution	O
pwy	O
x	O
is	O
also	O
its	O
mode	O
which	O
is	O
also	O
called	O
the	O
maximum	O
a	O
posteriori	O
estimate	O
of	O
bayes	O
rule	O
is	O
stated	O
as	O
pab	O
pbapapb	O
here	O
we	O
use	O
it	O
in	O
a	O
form	O
where	O
we	O
additionally	O
condition	O
everywhere	O
on	O
the	O
inputs	O
x	O
neglect	O
this	O
extra	O
conditioning	O
for	O
the	O
prior	O
which	O
is	O
independent	O
of	O
the	O
inputs	O
map	B
estimate	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
figure	O
example	O
of	O
bayesian	O
linear	B
model	O
f	O
with	O
intercept	O
and	O
slope	O
parameter	O
panel	O
shows	O
the	O
contours	O
of	O
the	O
prior	O
distribution	O
pw	O
n	O
i	O
eq	O
panel	O
shows	O
three	O
training	O
points	O
marked	O
by	O
crosses	O
panel	O
shows	O
contours	O
of	O
the	O
likelihood	B
pyx	O
w	O
eq	O
assuming	O
a	O
noise	O
level	O
of	O
n	O
note	O
that	O
the	O
slope	O
is	O
much	O
more	O
well	O
determined	O
than	O
the	O
intercept	O
panel	O
shows	O
the	O
posterior	O
pwx	O
y	O
eq	O
comparing	O
the	O
maximum	O
of	O
the	O
posterior	O
to	O
the	O
likelihood	B
we	O
see	O
that	O
the	O
intercept	O
has	O
been	O
shrunk	O
towards	O
zero	O
whereas	O
the	O
more	O
well	O
determined	O
slope	O
is	O
almost	O
unchanged	O
all	O
contour	O
plots	O
give	O
the	O
and	O
standard	O
deviation	O
equi-probability	O
contours	O
superimposed	O
on	O
the	O
data	O
in	O
panel	O
are	O
the	O
predictive	B
mean	O
plusminus	O
two	O
standard	O
deviations	O
of	O
the	O
predictive	B
distribution	O
pf	O
x	O
y	O
eq	O
w	O
in	O
a	O
non-bayesian	O
setting	O
the	O
negative	O
log	O
prior	O
is	O
sometimes	O
thought	O
of	O
as	O
a	O
penalty	O
term	O
and	O
the	O
map	B
point	O
is	O
known	O
as	O
the	O
penalized	B
maximum	I
likelihood	B
estimate	I
of	O
the	O
weights	O
and	O
this	O
may	O
cause	O
some	O
confusion	O
between	O
the	O
two	O
approaches	O
note	O
however	O
that	O
in	O
the	O
bayesian	O
setting	O
the	O
map	B
estimate	O
plays	O
no	O
special	O
r	O
the	O
penalized	B
maximum	O
likelihood	B
procedure	O
this	O
case	O
due	O
to	O
symmetries	O
in	O
the	O
model	O
and	O
posterior	O
it	O
happens	O
that	O
the	O
mean	O
of	O
the	O
predictive	B
distribution	O
is	O
the	O
same	O
as	O
the	O
prediction	O
at	O
the	O
mean	O
of	O
the	O
posterior	O
however	O
this	O
is	O
not	O
the	O
case	O
in	O
general	O
intercept	O
xoutput	O
yintercept	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
ridge	B
regression	O
predictive	B
distribution	O
feature	B
space	I
polynomial	B
regression	O
linear	B
in	O
the	O
parameters	O
weight-space	O
view	O
is	O
known	O
in	O
this	O
case	O
as	O
ridge	B
regression	O
and	O
kennard	O
because	O
of	O
the	O
effect	O
of	O
the	O
quadratic	O
penalty	O
term	O
p	O
w	O
from	O
the	O
log	O
prior	O
w	O
to	O
make	O
predictions	O
for	O
a	O
test	O
case	O
we	O
average	O
over	O
all	O
possible	O
parameter	O
values	O
weighted	O
by	O
their	O
posterior	O
probability	O
this	O
is	O
in	O
contrast	O
to	O
nonbayesian	O
schemes	O
where	O
a	O
single	O
parameter	O
is	O
typically	O
chosen	O
by	O
some	O
criterion	O
thus	O
the	O
predictive	B
distribution	O
for	O
f	O
fx	O
at	O
x	O
is	O
given	O
by	O
averaging	O
the	O
output	O
of	O
all	O
possible	O
linear	B
models	O
w	O
r	O
t	O
the	O
gaussian	O
posterior	O
pf	O
x	O
y	O
pf	O
wpwx	O
y	O
dw	O
z	O
n	O
a	O
a	O
x	O
x	O
the	O
predictive	B
distribution	O
is	O
again	O
gaussian	O
with	O
a	O
mean	O
given	O
by	O
the	O
posterior	O
mean	O
of	O
the	O
weights	O
from	O
eq	O
multiplied	O
by	O
the	O
test	O
input	O
as	O
one	O
would	O
expect	O
from	O
symmetry	O
considerations	O
the	O
predictive	B
variance	O
is	O
a	O
quadratic	B
form	I
of	O
the	O
test	O
input	O
with	O
the	O
posterior	O
covariance	B
matrix	I
showing	O
that	O
the	O
predictive	B
uncertainties	O
grow	O
with	O
the	O
magnitude	O
of	O
the	O
test	O
input	O
as	O
one	O
would	O
expect	O
for	O
a	O
linear	B
model	O
an	O
example	O
of	O
bayesian	O
linear	B
regression	I
is	O
given	O
in	O
figure	O
here	O
we	O
have	O
chosen	O
a	O
input	O
space	O
so	O
that	O
the	O
weight-space	O
is	O
two-dimensional	O
and	O
can	O
be	O
easily	O
visualized	O
contours	O
of	O
the	O
gaussian	O
prior	O
are	O
shown	O
in	O
panel	O
the	O
data	O
are	O
depicted	O
as	O
crosses	O
in	O
panel	O
this	O
gives	O
rise	O
to	O
the	O
likelihood	B
shown	O
in	O
panel	O
and	O
the	O
posterior	O
distribution	O
in	O
panel	O
the	O
predictive	B
distribution	O
and	O
its	O
error	O
bars	O
are	O
also	O
marked	O
in	O
panel	O
projections	O
of	O
inputs	O
into	O
feature	B
space	I
in	O
the	O
previous	O
section	O
we	O
reviewed	O
the	O
bayesian	O
linear	B
model	O
which	O
suffers	O
from	O
limited	O
expressiveness	O
a	O
very	O
simple	O
idea	O
to	O
overcome	O
this	O
problem	O
is	O
to	O
first	O
project	O
the	O
inputs	O
into	O
some	O
high	O
dimensional	O
space	O
using	O
a	O
set	B
of	O
basis	O
functions	O
and	O
then	O
apply	O
the	O
linear	B
model	O
in	O
this	O
space	O
instead	O
of	O
directly	O
on	O
the	O
inputs	O
themselves	O
for	O
example	O
a	O
scalar	O
input	O
x	O
could	O
be	O
projected	O
into	O
the	O
space	O
of	O
powers	O
of	O
x	O
x	O
to	O
implement	O
polynomial	B
regression	O
as	O
long	O
as	O
the	O
projections	O
are	O
fixed	O
functions	O
independent	O
of	O
the	O
parameters	O
w	O
the	O
model	O
is	O
still	O
linear	B
in	O
the	O
parameters	O
and	O
therefore	O
analytically	O
this	O
idea	O
is	O
also	O
used	O
in	O
classification	B
where	O
a	O
dataset	O
which	O
is	O
not	O
linearly	O
separable	O
in	O
the	O
original	O
data	O
space	O
may	O
become	O
linearly	O
separable	O
in	O
a	O
high	O
dimensional	O
feature	B
space	I
see	O
section	O
application	O
of	O
this	O
idea	O
begs	O
the	O
question	O
of	O
how	O
to	O
choose	O
the	O
basis	O
functions	O
as	O
we	O
shall	O
demonstrate	O
chapter	O
the	O
gaussian	B
process	I
formalism	O
allows	O
us	O
to	O
answer	O
this	O
question	O
for	O
now	O
we	O
assume	O
that	O
the	O
basis	O
functions	O
are	O
given	O
specifically	O
we	O
introduce	O
the	O
function	B
which	O
maps	O
a	O
d-dimensional	O
input	O
vector	O
x	O
into	O
an	O
n	O
dimensional	O
feature	B
space	I
further	O
let	O
the	O
matrix	O
with	O
adaptive	O
basis	O
functions	O
such	O
as	O
e	O
g	O
multilayer	O
perceptrons	O
may	O
at	O
first	O
seem	O
like	O
a	O
useful	O
extension	O
but	O
they	O
are	O
much	O
harder	O
to	O
treat	O
except	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
of	O
hidden	O
units	O
see	O
section	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
explicit	O
feature	B
space	I
formulation	O
alternative	O
formulation	O
computational	O
load	O
kernel	B
kernel	B
trick	I
be	O
the	O
aggregation	O
of	O
columns	O
for	O
all	O
cases	O
in	O
the	O
training	O
set	B
now	O
the	O
model	O
is	O
where	O
the	O
vector	O
of	O
parameters	O
now	O
has	O
length	O
n	O
the	O
analysis	O
for	O
this	O
model	O
is	O
analogous	O
to	O
the	O
standard	O
linear	B
model	O
except	O
that	O
everywhere	O
is	O
substituted	O
for	O
x	O
thus	O
the	O
predictive	B
distribution	O
becomes	O
fx	O
f	O
x	O
y	O
y	O
n	O
with	O
and	O
a	O
p	O
to	O
make	O
predictions	O
using	O
this	O
equation	O
we	O
need	O
to	O
invert	O
the	O
a	O
matrix	O
of	O
size	O
n	O
n	O
which	O
may	O
not	O
be	O
convenient	O
if	O
n	O
the	O
dimension	O
of	O
the	O
feature	B
space	I
is	O
large	O
however	O
we	O
can	O
rewrite	O
the	O
equation	O
in	O
the	O
following	O
way	O
n	O
f	O
x	O
y	O
ni	O
p	O
p	O
p	O
ni	O
p	O
ni	O
n	O
p	O
where	O
we	O
have	O
used	O
the	O
shorthand	O
and	O
defined	O
k	O
p	O
to	O
show	O
this	O
for	O
the	O
mean	O
first	O
note	O
that	O
using	O
the	O
definitions	O
of	O
a	O
and	O
k	O
we	O
have	O
n	O
ni	O
a	O
p	O
now	O
multiplying	O
through	O
by	O
a	O
from	O
left	O
and	O
n	O
a	O
ni	O
showing	O
the	O
equivalence	O
of	O
the	O
mean	O
expressions	O
in	O
eq	O
p	O
and	O
eq	O
for	O
the	O
variance	O
we	O
use	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
setting	O
z	O
p	O
w	O
in	O
eq	O
we	O
need	O
to	O
invert	O
matrices	O
of	O
size	O
n	O
n	O
which	O
is	O
more	O
convenient	O
when	O
n	O
n	O
geometrically	O
note	O
that	O
n	O
datapoints	O
can	O
span	O
at	O
most	O
n	O
dimensions	O
in	O
the	O
feature	B
space	I
ni	O
from	O
the	O
right	O
gives	O
ni	O
and	O
v	O
u	O
therein	O
p	O
or	O
notice	O
that	O
in	O
eq	O
the	O
feature	B
space	I
always	O
enters	O
in	O
the	O
form	O
of	O
p	O
p	O
thus	O
the	O
entries	O
of	O
these	O
matrices	O
are	O
invariably	O
of	O
the	O
form	O
p	O
where	O
x	O
and	O
are	O
in	O
either	O
the	O
training	O
or	O
the	O
test	O
sets	O
let	O
us	O
define	O
kx	O
p	O
for	O
reasons	O
that	O
will	O
become	O
clear	O
later	O
we	O
call	O
k	O
a	O
covariance	B
function	B
or	O
kernel	B
notice	O
that	O
p	O
is	O
an	O
inner	O
product	O
respect	O
to	O
p	O
as	O
p	O
is	O
positive	B
definite	I
we	O
can	O
define	O
so	O
that	O
p	O
for	O
example	O
if	O
the	O
svd	O
value	O
decomposition	O
of	O
p	O
u	O
du	O
where	O
d	O
is	O
diagonal	O
then	O
one	O
form	O
for	O
is	O
u	O
then	O
defining	O
p	O
we	O
obtain	O
a	O
simple	O
dot	B
product	I
representation	O
kx	O
p	O
p	O
p	O
if	O
an	O
algorithm	O
is	O
defined	O
solely	O
in	O
terms	O
of	O
inner	O
products	O
in	O
input	O
space	O
then	O
it	O
can	O
be	O
lifted	O
into	O
feature	B
space	I
by	O
replacing	O
occurrences	O
of	O
those	O
inner	O
products	O
by	O
kx	O
this	O
is	O
sometimes	O
called	O
the	O
kernel	B
trick	I
this	O
technique	O
is	O
particularly	O
valuable	O
in	O
situations	O
where	O
it	O
is	O
more	O
convenient	O
to	O
compute	O
the	O
kernel	B
than	O
the	O
feature	O
vectors	O
themselves	O
as	O
we	O
will	O
see	O
in	O
the	O
coming	O
sections	O
this	O
often	O
leads	O
to	O
considering	O
the	O
kernel	B
as	O
the	O
object	O
of	O
primary	O
interest	O
and	O
its	O
corresponding	O
feature	B
space	I
as	O
having	O
secondary	O
practical	O
importance	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
function-space	O
view	O
function-space	O
view	O
an	O
alternative	O
and	O
equivalent	B
way	O
of	O
reaching	O
identical	O
results	O
to	O
the	O
previous	O
section	O
is	O
possible	O
by	O
considering	O
inference	O
directly	O
in	O
function	B
space	O
we	O
use	O
a	O
gaussian	B
process	I
to	O
describe	O
a	O
distribution	O
over	O
functions	O
formally	O
definition	O
a	O
gaussian	B
process	I
is	O
a	O
collection	O
of	O
random	O
variables	O
any	O
finite	O
number	O
of	O
which	O
have	O
a	O
joint	B
gaussian	B
distribution	I
gaussian	B
process	I
a	O
gaussian	B
process	I
is	O
completely	O
specified	O
by	O
its	O
mean	B
function	B
and	O
covariance	B
function	B
we	O
define	O
mean	B
function	B
mx	O
and	O
the	O
covariance	B
function	B
kx	O
of	O
a	O
real	O
process	O
fx	O
as	O
covariance	B
and	O
mean	B
function	B
mx	O
efx	O
kx	O
efx	O
and	O
will	O
write	O
the	O
gaussian	B
process	I
as	O
fx	O
kx	O
usually	O
for	O
notational	O
simplicity	O
we	O
will	O
take	O
the	O
mean	B
function	B
to	O
be	O
zero	O
although	O
this	O
need	O
not	O
be	O
done	O
see	O
section	O
in	O
our	O
case	O
the	O
random	O
variables	O
represent	O
the	O
value	O
of	O
the	O
function	B
fx	O
at	O
location	O
x	O
often	O
gaussian	O
processes	O
are	O
defined	O
over	O
time	O
i	O
e	O
where	O
the	O
index	B
set	B
of	O
the	O
random	O
variables	O
is	O
time	O
this	O
is	O
not	O
the	O
case	O
in	O
our	O
use	O
of	O
gps	O
here	O
the	O
index	B
set	B
x	O
is	O
the	O
set	B
of	O
possible	O
inputs	O
which	O
could	O
be	O
more	O
general	O
e	O
g	O
rd	O
for	O
notational	O
convenience	O
we	O
use	O
the	O
enumeration	O
of	O
the	O
cases	O
in	O
the	O
training	O
set	B
to	O
identify	O
the	O
random	O
variables	O
such	O
that	O
fi	O
fxi	O
is	O
the	O
random	O
variable	O
corresponding	O
to	O
the	O
case	O
yi	O
as	O
would	O
be	O
expected	O
a	O
gaussian	B
process	I
is	O
defined	O
as	O
a	O
collection	O
of	O
random	O
variables	O
thus	O
the	O
definition	O
automatically	O
implies	O
a	O
consistency	B
requirement	O
which	O
is	O
also	O
sometimes	O
known	O
as	O
the	O
marginalization	B
property	I
this	O
property	O
simply	O
means	O
that	O
if	O
the	O
gp	O
e	O
g	O
specifies	O
n	O
then	O
it	O
must	O
also	O
specify	O
n	O
where	O
is	O
the	O
relevant	O
submatrix	O
of	O
see	O
eq	O
in	O
other	O
words	O
examination	O
of	O
a	O
larger	O
set	B
of	O
variables	O
does	O
not	O
change	O
the	O
distribution	O
of	O
the	O
smaller	O
set	B
notice	O
that	O
the	O
consistency	B
requirement	O
is	O
automatically	O
fulfilled	O
if	O
the	O
covariance	B
function	B
specifies	O
entries	O
of	O
the	O
covariance	B
the	O
definition	O
does	O
not	O
exclude	O
gaussian	O
processes	O
with	O
finite	O
index	O
sets	O
would	O
be	O
simply	O
gaussian	O
distributions	O
but	O
these	O
are	O
not	O
particularly	O
interesting	O
for	O
our	O
purposes	O
however	O
that	O
if	O
you	O
instead	O
specified	O
e	O
g	O
a	O
function	B
for	O
the	O
entries	O
of	O
the	O
inverse	O
covariance	B
matrix	I
then	O
the	O
marginalization	B
property	I
would	O
no	O
longer	O
be	O
fulfilled	O
and	O
one	O
could	O
not	O
think	O
of	O
this	O
as	O
a	O
consistent	O
collection	O
of	O
random	O
variables	O
this	O
would	O
not	O
qualify	O
as	O
a	O
gaussian	B
process	I
index	B
set	B
input	O
domain	O
marginalization	B
property	I
finite	O
index	B
set	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bayesian	O
linear	B
model	O
is	O
a	O
gaussian	B
process	I
basis	O
functions	O
smoothness	O
characteristic	O
length-scale	B
regression	O
a	O
simple	O
example	O
of	O
a	O
gaussian	B
process	I
can	O
be	O
obtained	O
from	O
our	O
bayesian	O
linear	B
regression	I
model	O
fx	O
with	O
prior	O
w	O
n	O
p	O
we	O
have	O
for	O
the	O
mean	O
and	O
covariance	B
efx	O
p	O
thus	O
fx	O
and	O
are	O
jointly	O
gaussian	O
with	O
zero	O
mean	O
and	O
covariance	B
given	O
by	O
p	O
indeed	O
the	O
function	B
values	O
fxn	O
corresponding	O
to	O
any	O
number	O
of	O
input	O
points	O
n	O
are	O
jointly	O
gaussian	O
although	O
if	O
n	O
n	O
then	O
this	O
gaussian	O
is	O
singular	O
the	O
joint	B
covariance	B
matrix	I
will	O
be	O
of	O
rank	O
n	O
in	O
this	O
chapter	O
our	O
running	O
example	O
of	O
a	O
covariance	B
function	B
will	O
be	O
the	O
squared	B
exponential	B
covariance	B
function	B
other	O
covariance	B
functions	O
are	O
discussed	O
in	O
chapter	O
the	O
covariance	B
function	B
specifies	O
the	O
covariance	B
between	O
pairs	O
of	O
random	O
variables	O
kxp	O
xq	O
note	O
that	O
the	O
covariance	B
between	O
the	O
outputs	B
is	O
written	O
as	O
a	O
function	B
of	O
the	O
inputs	O
for	O
this	O
particular	O
covariance	B
function	B
we	O
see	O
that	O
the	O
covariance	B
is	O
almost	O
unity	O
between	O
variables	O
whose	O
corresponding	O
inputs	O
are	O
very	O
close	O
and	O
decreases	O
as	O
their	O
distance	O
in	O
the	O
input	O
space	O
increases	O
it	O
can	O
be	O
shown	O
section	O
that	O
the	O
squared	B
exponential	B
covariance	B
function	B
corresponds	O
to	O
a	O
bayesian	O
linear	B
regression	I
model	O
with	O
an	O
infinite	O
number	O
of	O
basis	O
functions	O
indeed	O
for	O
every	O
positive	B
definite	I
covariance	B
function	B
k	O
there	O
exists	O
a	O
infinite	O
expansion	O
in	O
terms	O
of	O
basis	O
functions	O
mercer	O
s	O
theorem	O
in	O
section	O
we	O
can	O
also	O
obtain	O
the	O
se	O
covariance	B
function	B
from	O
the	O
linear	B
combination	O
of	O
an	O
infinite	O
number	O
of	O
gaussian-shaped	O
basis	O
functions	O
see	O
eq	O
and	O
eq	O
the	O
specification	O
of	O
the	O
covariance	B
function	B
implies	O
a	O
distribution	O
over	O
functions	O
to	O
see	O
this	O
we	O
can	O
draw	O
samples	O
from	O
the	O
distribution	O
of	O
functions	O
evaluated	O
at	O
any	O
number	O
of	O
points	O
in	O
detail	O
we	O
choose	O
a	O
number	O
of	O
input	O
x	O
and	O
write	O
out	O
the	O
corresponding	O
covariance	B
matrix	I
using	O
eq	O
elementwise	O
then	O
we	O
generate	O
a	O
random	O
gaussian	O
vector	O
with	O
this	O
covariance	B
matrix	I
f	O
kx	O
x	O
and	O
plot	O
the	O
generated	O
values	O
as	O
a	O
function	B
of	O
the	O
inputs	O
figure	O
shows	O
three	O
such	O
samples	O
the	O
generation	O
of	O
multivariate	O
gaussian	O
samples	O
is	O
described	O
in	O
section	O
in	O
the	O
example	O
in	O
figure	O
the	O
input	O
values	O
were	O
equidistant	O
but	O
this	O
need	O
not	O
be	O
the	O
case	O
notice	O
that	O
informally	O
the	O
functions	O
look	O
smooth	O
in	O
fact	O
the	O
squared	B
exponential	B
covariance	B
function	B
is	O
infinitely	O
differentiable	O
leading	O
to	O
the	O
process	O
being	O
infinitely	O
mean-square	O
differentiable	O
section	O
we	O
also	O
see	O
that	O
the	O
functions	O
seem	O
to	O
have	O
a	O
characteristic	O
length-scale	B
this	O
covariance	B
function	B
is	O
called	O
the	O
radial	O
basis	O
function	B
or	O
gaussian	O
here	O
we	O
prefer	O
squared	B
exponential	B
these	O
input	O
points	O
play	O
the	O
r	O
ole	O
of	O
test	O
inputs	O
and	O
therefore	O
carry	O
a	O
subscript	O
asterisk	O
this	O
will	O
become	O
clearer	O
later	O
when	O
both	O
training	O
and	O
test	O
points	O
are	O
involved	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
function-space	O
view	O
prior	O
posterior	O
figure	O
panel	O
shows	O
three	O
functions	O
drawn	O
at	O
random	O
from	O
a	O
gp	O
prior	O
the	O
dots	O
indicate	O
values	O
of	O
y	O
actually	O
generated	O
the	O
two	O
other	O
functions	O
have	O
correctly	O
been	O
drawn	O
as	O
lines	O
by	O
joining	O
a	O
large	O
number	O
of	O
evaluated	O
points	O
panel	O
shows	O
three	O
random	O
functions	O
drawn	O
from	O
the	O
posterior	O
i	O
e	O
the	O
prior	O
conditioned	O
on	O
the	O
five	O
noise	O
free	O
observations	O
indicated	O
in	O
both	O
plots	O
the	O
shaded	O
area	O
represents	O
the	O
pointwise	O
mean	O
plus	O
and	O
minus	O
two	O
times	O
the	O
standard	O
deviation	O
for	O
each	O
input	O
value	O
to	O
the	O
confidence	O
region	O
for	O
the	O
prior	O
and	O
posterior	O
respectively	O
which	O
informally	O
can	O
be	O
thought	O
of	O
as	O
roughly	O
the	O
distance	O
you	O
have	O
to	O
move	O
in	O
input	O
space	O
before	O
the	O
function	B
value	O
can	O
change	O
significantly	O
see	O
section	O
for	O
eq	O
the	O
characteristic	O
length-scale	B
is	O
around	O
one	O
unit	O
by	O
replacing	O
xq	O
by	O
xq	O
in	O
eq	O
for	O
some	O
positive	O
constant	O
we	O
could	O
change	O
the	O
characteristic	O
length-scale	B
of	O
the	O
process	O
also	O
the	O
overall	O
variance	O
of	O
the	O
random	O
function	B
can	O
be	O
controlled	O
by	O
a	O
positive	O
pre-factor	O
before	O
the	O
exp	O
in	O
eq	O
we	O
will	O
discuss	O
more	O
about	O
how	O
such	O
factors	O
affect	O
the	O
predictions	O
in	O
section	O
and	O
say	O
more	O
about	O
how	O
to	O
set	B
such	O
scale	O
parameters	O
in	O
chapter	O
prediction	O
with	O
noise-free	O
observations	O
we	O
are	O
usually	O
not	O
primarily	O
interested	O
in	O
drawing	O
random	O
functions	O
from	O
the	O
prior	O
but	O
want	O
to	O
incorporate	O
the	O
knowledge	O
that	O
the	O
training	O
data	O
provides	O
about	O
the	O
function	B
initially	O
we	O
will	O
consider	O
the	O
simple	O
special	O
case	O
where	O
the	O
observations	O
are	O
noise	O
free	O
that	O
is	O
we	O
know	O
fii	O
n	O
the	O
joint	B
distribution	O
of	O
the	O
training	O
outputs	B
f	O
and	O
the	O
test	O
outputs	B
f	O
according	O
to	O
the	O
prior	O
is	O
kx	O
x	O
kx	O
x	O
f	O
kx	O
x	O
kx	O
x	O
f	O
n	O
magnitude	O
joint	B
prior	O
if	O
there	O
are	O
n	O
training	O
points	O
and	O
n	O
test	O
points	O
then	O
kx	O
x	O
denotes	O
the	O
n	O
n	O
matrix	O
of	O
the	O
covariances	O
evaluated	O
at	O
all	O
pairs	O
of	O
training	O
and	O
test	O
points	O
and	O
similarly	O
for	O
the	O
other	O
entries	O
kx	O
x	O
kx	O
x	O
and	O
kx	O
x	O
to	O
get	O
the	O
posterior	O
distribution	O
over	O
functions	O
we	O
need	O
to	O
restrict	O
this	O
joint	B
prior	O
distribution	O
to	O
contain	O
only	O
those	O
functions	O
which	O
agree	O
with	O
the	O
observed	O
data	O
points	O
graphically	O
in	O
figure	O
you	O
may	O
think	O
of	O
generating	O
functions	O
from	O
the	O
prior	O
and	O
rejecting	O
the	O
ones	O
that	O
disagree	O
with	O
the	O
observations	O
al	O
graphical	O
rejection	O
xoutput	O
fx	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
noise-free	O
predictive	B
distribution	O
predictive	B
distribution	O
though	O
this	O
strategy	O
would	O
not	O
be	O
computationally	O
very	O
efficient	O
fortunately	O
in	O
probabilistic	B
terms	O
this	O
operation	O
is	O
extremely	O
simple	O
corresponding	O
to	O
conditioning	O
the	O
joint	B
gaussian	O
prior	O
distribution	O
on	O
the	O
observations	O
section	O
for	O
further	O
details	O
to	O
give	O
f	O
x	O
f	O
xkx	O
x	O
kx	O
x	O
kx	O
xkx	O
x	O
x	O
function	B
values	O
f	O
to	O
test	O
inputs	O
x	O
can	O
be	O
sampled	O
from	O
the	O
joint	B
posterior	O
distribution	O
by	O
evaluating	O
the	O
mean	O
and	O
covariance	B
matrix	I
from	O
eq	O
and	O
generating	O
samples	O
according	O
to	O
the	O
method	O
described	O
in	O
section	O
figure	O
shows	O
the	O
results	O
of	O
these	O
computations	O
given	O
the	O
five	O
datapoints	O
marked	O
with	O
symbols	O
notice	O
that	O
it	O
is	O
trivial	O
to	O
extend	O
these	O
computations	O
to	O
multidimensional	O
inputs	O
one	O
simply	O
needs	O
to	O
change	O
the	O
evaluation	O
of	O
the	O
covariance	B
function	B
in	O
accordance	O
with	O
eq	O
although	O
the	O
resulting	O
functions	O
may	O
be	O
harder	O
to	O
display	O
graphically	O
prediction	O
using	O
noisy	O
observations	O
it	O
is	O
typical	O
for	O
more	O
realistic	O
modelling	O
situations	O
that	O
we	O
do	O
not	O
have	O
access	O
to	O
function	B
values	O
themselves	O
but	O
only	O
noisy	O
versions	O
thereof	O
y	O
fx	O
assuming	O
additive	O
independent	O
identically	O
distributed	O
gaussian	O
noise	O
with	O
variance	O
n	O
the	O
prior	O
on	O
the	O
noisy	O
observations	O
becomes	O
covyp	O
yq	O
kxp	O
xq	O
n	O
pq	O
or	O
covy	O
kx	O
x	O
ni	O
where	O
pq	O
is	O
a	O
kronecker	O
delta	O
which	O
is	O
one	O
iff	O
p	O
q	O
and	O
zero	O
otherwise	O
it	O
follows	O
from	O
the	O
assumption	O
about	O
the	O
noise	O
that	O
a	O
diagonal	O
is	O
added	O
in	O
comparison	O
to	O
the	O
noise	O
free	O
case	O
eq	O
introducing	O
the	O
noise	O
term	O
in	O
eq	O
we	O
can	O
write	O
the	O
joint	B
distribution	O
of	O
the	O
observed	O
target	O
values	O
and	O
the	O
function	B
values	O
at	O
the	O
test	O
locations	O
under	O
the	O
prior	O
as	O
y	O
f	O
kx	O
x	O
kx	O
x	O
n	O
ni	O
kx	O
x	O
kx	O
x	O
deriving	O
the	O
conditional	B
distribution	O
corresponding	O
to	O
eq	O
we	O
arrive	O
at	O
the	O
key	O
predictive	B
equations	O
for	O
gaussian	B
process	I
regression	I
f	O
y	O
x	O
f	O
covf	O
where	O
f	O
ef	O
y	O
x	O
kx	O
xkx	O
x	O
covf	O
kx	O
x	O
kx	O
xkx	O
x	O
ni	O
kx	O
x	O
are	O
some	O
situations	O
where	O
it	O
is	O
reasonable	O
to	O
assume	O
that	O
the	O
observations	O
are	O
noise-free	O
for	O
example	O
for	O
computer	O
simulations	O
see	O
e	O
g	O
sacks	O
et	O
al	O
complicated	O
noise	O
models	O
with	O
non-trivial	O
covariance	B
structure	O
can	O
also	O
be	O
handled	O
see	O
section	O
that	O
the	O
kronecker	O
delta	O
is	O
on	O
the	O
index	O
of	O
the	O
cases	O
not	O
the	O
value	O
of	O
the	O
input	O
for	O
the	O
signal	O
part	O
of	O
the	O
covariance	B
function	B
the	O
input	O
value	O
is	O
the	O
index	B
set	B
to	O
the	O
random	O
variables	O
describing	O
the	O
function	B
for	O
the	O
noise	O
part	O
it	O
is	O
the	O
identity	O
of	O
the	O
point	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
function-space	O
view	O
f	O
y	O
observations	O
gaussian	O
field	O
yc	O
fc	O
correspondence	O
with	O
weight-space	O
view	O
compact	O
notation	O
predictive	B
distribution	O
linear	B
predictor	O
inputs	O
x	O
xc	O
figure	O
graphical	O
model	O
graph	O
for	O
a	O
gp	O
for	O
regression	O
squares	O
represent	O
observed	O
variables	O
and	O
circles	O
represent	O
unknowns	O
the	O
thick	O
horizontal	O
bar	O
represents	O
a	O
set	B
of	O
fully	O
connected	O
nodes	O
note	O
that	O
an	O
observation	O
yi	O
is	O
conditionally	O
independent	O
of	O
all	O
other	O
nodes	O
given	O
the	O
corresponding	O
latent	O
variable	O
fi	O
because	O
of	O
the	O
marginalization	B
property	I
of	O
gps	O
addition	O
of	O
further	O
inputs	O
x	O
latent	O
variables	O
f	O
and	O
unobserved	O
targets	O
y	O
does	O
not	O
change	O
the	O
distribution	O
of	O
any	O
other	O
variables	O
notice	O
that	O
we	O
now	O
have	O
exact	O
correspondence	O
with	O
the	O
weight	O
space	O
view	O
in	O
eq	O
when	O
identifying	O
kc	O
d	O
p	O
where	O
c	O
d	O
stand	O
for	O
either	O
x	O
or	O
x	O
for	O
any	O
set	B
of	O
basis	O
functions	O
we	O
can	O
compute	O
the	O
corresponding	O
covariance	B
function	B
as	O
kxp	O
xq	O
p	O
conversely	O
for	O
every	O
definite	O
covariance	B
function	B
k	O
there	O
exists	O
a	O
infinite	O
expansion	O
in	O
terms	O
of	O
basis	O
functions	O
see	O
section	O
the	O
expressions	O
involving	O
kx	O
x	O
kx	O
x	O
and	O
kx	O
x	O
etc	O
can	O
look	O
rather	O
unwieldy	O
so	O
we	O
now	O
introduce	O
a	O
compact	O
form	O
of	O
the	O
notation	O
setting	O
k	O
kx	O
x	O
and	O
k	O
kx	O
x	O
in	O
the	O
case	O
that	O
there	O
is	O
only	O
one	O
test	O
point	O
x	O
we	O
write	O
kx	O
k	O
to	O
denote	O
the	O
vector	O
of	O
covariances	O
between	O
the	O
test	O
point	O
and	O
the	O
n	O
training	O
points	O
using	O
this	O
compact	O
notation	O
and	O
for	O
a	O
single	O
test	O
point	O
x	O
equations	O
and	O
reduce	O
to	O
f	O
k	O
ni	O
vf	O
kx	O
x	O
k	O
ni	O
let	O
us	O
examine	O
the	O
predictive	B
distribution	O
as	O
given	O
by	O
equations	O
and	O
note	O
first	O
that	O
the	O
mean	O
prediction	O
eq	O
is	O
a	O
linear	B
combination	O
of	O
observations	O
y	O
this	O
is	O
sometimes	O
referred	O
to	O
as	O
a	O
linear	B
predictor	O
another	O
way	O
to	O
look	O
at	O
this	O
equation	O
is	O
to	O
see	O
it	O
as	O
a	O
linear	B
combination	O
of	O
n	O
kernel	B
functions	O
each	O
one	O
centered	O
on	O
a	O
training	O
point	O
by	O
writing	O
nx	O
fx	O
ikxi	O
x	O
ni	O
the	O
fact	O
that	O
the	O
mean	O
prediction	O
for	O
fx	O
can	O
be	O
where	O
written	O
as	O
eq	O
despite	O
the	O
fact	O
that	O
the	O
gp	O
can	O
be	O
represented	O
in	O
terms	O
of	O
a	O
infinite	O
number	O
of	O
basis	O
functions	O
is	O
one	O
manifestation	O
of	O
the	O
representer	B
theorem	I
see	O
section	O
for	O
more	O
on	O
this	O
point	O
we	O
can	O
understand	O
this	O
result	O
intuitively	O
because	O
although	O
the	O
gp	O
defines	O
a	O
joint	B
gaussian	B
distribution	I
over	O
all	O
of	O
the	O
y	O
variables	O
one	O
for	O
each	O
point	O
in	O
the	O
index	B
set	B
x	O
for	O
representer	B
theorem	I
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
posterior	O
posterior	O
covariance	B
figure	O
panel	O
is	O
identical	O
to	O
figure	O
showing	O
three	O
random	O
functions	O
drawn	O
from	O
the	O
posterior	O
panel	O
shows	O
the	O
posterior	O
co-variance	O
between	O
f	O
and	O
f	O
for	O
the	O
same	O
data	O
for	O
three	O
different	O
values	O
of	O
note	O
that	O
the	O
covariance	B
at	O
close	O
points	O
is	O
high	O
falling	O
to	O
zero	O
at	O
the	O
training	O
points	O
there	O
is	O
no	O
variance	O
since	O
it	O
is	O
a	O
noise-free	O
process	O
then	O
becomes	O
negative	O
etc	O
this	O
happens	O
because	O
if	O
the	O
smooth	O
function	B
happens	O
to	O
be	O
less	O
than	O
the	O
mean	O
on	O
one	O
side	O
of	O
the	O
data	O
point	O
it	O
tends	O
to	O
exceed	O
the	O
mean	O
on	O
the	O
other	O
side	O
causing	O
a	O
reversal	O
of	O
the	O
sign	O
of	O
the	O
covariance	B
at	O
the	O
data	O
points	O
note	O
for	O
contrast	O
that	O
the	O
prior	O
covariance	B
is	O
simply	O
of	O
gaussian	O
shape	O
and	O
never	O
negative	O
making	O
predictions	O
at	O
x	O
we	O
only	O
care	O
about	O
the	O
distribution	O
defined	O
by	O
the	O
n	O
training	O
points	O
and	O
the	O
test	O
point	O
as	O
a	O
gaussian	B
distribution	I
is	O
marginalized	O
by	O
just	O
taking	O
the	O
relevant	O
block	O
of	O
the	O
joint	B
covariance	B
matrix	I
section	O
it	O
is	O
clear	O
that	O
conditioning	O
this	O
distribution	O
on	O
the	O
observations	O
gives	O
us	O
the	O
desired	O
result	O
a	O
graphical	O
model	O
representation	O
of	O
a	O
gp	O
is	O
given	O
in	O
figure	O
note	O
also	O
that	O
the	O
variance	O
in	O
eq	O
does	O
not	O
depend	O
on	O
the	O
observed	O
targets	O
but	O
only	O
on	O
the	O
inputs	O
this	O
is	O
a	O
property	O
of	O
the	O
gaussian	B
distribution	I
the	O
variance	O
is	O
the	O
difference	O
between	O
two	O
terms	O
the	O
first	O
term	O
kx	O
x	O
is	O
simply	O
the	O
prior	O
covariance	B
from	O
that	O
is	O
subtracted	O
a	O
term	O
representing	O
the	O
information	O
the	O
observations	O
gives	O
us	O
about	O
the	O
function	B
we	O
can	O
very	O
simply	O
compute	O
the	O
predictive	B
distribution	O
of	O
test	O
targets	O
y	O
by	O
adding	O
ni	O
to	O
the	O
variance	O
in	O
the	O
expression	O
for	O
covf	O
the	O
predictive	B
distribution	O
for	O
the	O
gp	O
model	O
gives	O
more	O
than	O
just	O
pointwise	O
errorbars	O
of	O
the	O
simplified	O
eq	O
although	O
not	O
stated	O
explicitly	O
eq	O
holds	O
unchanged	O
when	O
x	O
denotes	O
multiple	O
test	O
inputs	O
in	O
this	O
case	O
the	O
covariance	B
of	O
the	O
test	O
targets	O
are	O
computed	O
diagonal	O
elements	O
are	O
the	O
pointwise	O
variances	O
in	O
fact	O
eq	O
is	O
the	O
mean	B
function	B
and	O
eq	O
the	O
covariance	B
function	B
of	O
the	O
posterior	B
process	I
recall	O
the	O
definition	O
of	O
gaussian	B
process	I
from	O
page	O
the	O
posterior	O
covariance	B
in	O
illustrated	O
in	O
figure	O
it	O
will	O
be	O
useful	O
for	O
chapter	O
to	O
introduce	O
the	O
marginal	B
likelihood	B
evidence	O
pyx	O
at	O
this	O
point	O
the	O
marginal	B
likelihood	B
is	O
the	O
integral	O
noisy	O
predictions	O
joint	B
predictions	O
posterior	B
process	I
marginal	B
likelihood	B
xoutput	O
fx	O
xpost	O
covariance	B
covfxfx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
varying	O
the	O
hyperparameters	B
o	O
input	O
x	O
y	O
k	O
function	B
n	O
level	O
x	O
input	O
ni	O
o	O
predictive	B
mean	O
eq	O
y	O
p	O
l	O
choleskyk	O
lly	O
f	O
k	O
v	O
lk	O
predictive	B
variance	O
eq	O
vf	O
kx	O
x	O
vv	O
log	O
pyx	O
eq	O
return	O
f	O
vf	O
log	O
pyx	O
marginal	B
likelihood	B
algorithm	O
predictions	O
and	O
log	O
marginal	B
likelihood	B
for	O
gaussian	B
process	I
regression	I
the	O
implementation	O
addresses	O
the	O
matrix	O
inversion	O
required	O
by	O
eq	O
and	O
using	O
cholesky	O
factorization	O
see	O
section	O
for	O
multiple	O
test	O
cases	O
lines	O
are	O
repeated	O
the	O
log	O
determinant	O
required	O
in	O
eq	O
is	O
computed	O
from	O
the	O
cholesky	O
factor	O
large	O
n	O
it	O
may	O
not	O
be	O
possible	O
to	O
represent	O
the	O
determinant	O
itself	O
the	O
computational	O
complexity	O
is	O
for	O
the	O
cholesky	B
decomposition	I
in	O
line	O
and	O
for	O
solving	O
triangular	O
systems	O
in	O
line	O
and	O
each	O
test	O
case	O
in	O
line	O
i	O
log	O
lii	O
n	O
log	O
of	O
the	O
likelihood	B
times	O
the	O
prior	O
pyx	O
z	O
pyf	O
xpfx	O
df	O
the	O
term	O
marginal	B
likelihood	B
refers	O
to	O
the	O
marginalization	O
over	O
the	O
function	B
values	O
f	O
under	O
the	O
gaussian	B
process	I
model	O
the	O
prior	O
is	O
gaussian	O
fx	O
n	O
k	O
or	O
log	O
pfx	O
log	O
n	O
log	O
and	O
the	O
likelihood	B
is	O
a	O
factorized	O
gaussian	O
yf	O
n	O
ni	O
so	O
we	O
can	O
make	O
use	O
of	O
equations	O
and	O
to	O
perform	O
the	O
integration	O
yielding	O
the	O
log	O
marginal	B
likelihood	B
fk	O
log	O
pyx	O
yk	O
ni	O
log	O
ni	O
n	O
log	O
this	O
result	O
can	O
also	O
be	O
obtained	O
directly	O
by	O
observing	O
that	O
y	O
n	O
k	O
ni	O
a	O
practical	O
implementation	O
of	O
gaussian	B
process	I
regression	I
is	O
shown	O
in	O
algorithm	O
the	O
algorithm	O
uses	O
cholesky	B
decomposition	I
instead	O
of	O
directly	O
inverting	O
the	O
matrix	O
since	O
it	O
is	O
faster	O
and	O
numerically	O
more	O
stable	O
see	O
section	O
the	O
algorithm	O
returns	O
the	O
predictive	B
mean	O
and	O
variance	O
for	O
noise	O
free	O
test	O
data	O
to	O
compute	O
the	O
predictive	B
distribution	O
for	O
noisy	O
test	O
data	O
y	O
simply	O
add	O
the	O
noise	O
variance	O
n	O
to	O
the	O
predictive	B
variance	O
of	O
f	O
varying	O
the	O
hyperparameters	B
typically	O
the	O
covariance	B
functions	O
that	O
we	O
use	O
will	O
have	O
some	O
free	O
parameters	O
for	O
example	O
the	O
squared-exponential	O
covariance	B
function	B
in	O
one	O
dimension	O
has	O
the	O
following	O
form	O
f	O
n	O
pq	O
kyxp	O
xq	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
hyperparameters	B
figure	O
data	O
is	O
generated	O
from	O
a	O
gp	O
with	O
hyperparameters	B
f	O
n	O
as	O
shown	O
by	O
the	O
symbols	O
using	O
gaussian	B
process	I
prediction	O
with	O
these	O
hyperparameters	B
we	O
obtain	O
a	O
confidence	O
region	O
for	O
the	O
underlying	O
function	B
f	O
in	O
grey	O
panels	O
and	O
again	O
show	O
the	O
confidence	O
region	O
but	O
this	O
time	O
for	O
hyperparameter	O
values	O
and	O
respectively	O
the	O
covariance	B
is	O
denoted	O
ky	O
as	O
it	O
is	O
for	O
the	O
noisy	O
targets	O
y	O
rather	O
than	O
for	O
the	O
underlying	O
function	B
f	O
observe	O
that	O
the	O
length-scale	B
the	O
signal	O
variance	O
f	O
and	O
the	O
noise	O
variance	O
n	O
can	O
be	O
varied	O
in	O
general	O
we	O
call	O
the	O
free	O
parameters	O
in	O
chapter	O
we	O
will	O
consider	O
various	O
methods	O
for	O
determining	O
the	O
hyperparameters	B
from	O
training	O
data	O
however	O
in	O
this	O
section	O
our	O
aim	O
is	O
more	O
simply	O
to	O
explore	O
the	O
effects	O
of	O
varying	O
the	O
hyperparameters	B
on	O
gp	O
prediction	O
consider	O
the	O
data	O
shown	O
by	O
signs	O
in	O
figure	O
this	O
was	O
generated	O
from	O
a	O
gp	O
with	O
the	O
se	O
kernel	B
with	O
f	O
n	O
the	O
figure	O
also	O
shows	O
the	O
standard-deviation	O
error	O
bars	O
for	O
the	O
predictions	O
obtained	O
using	O
these	O
values	O
of	O
the	O
hyperparameters	B
as	O
per	O
eq	O
notice	O
how	O
the	O
error	O
bars	O
get	O
larger	O
for	O
input	O
values	O
that	O
are	O
distant	O
from	O
any	O
training	O
points	O
indeed	O
if	O
the	O
x-axis	O
refer	O
to	O
the	O
parameters	O
of	O
the	O
covariance	B
function	B
as	O
hyperparameters	B
to	O
emphasize	O
that	O
they	O
are	O
parameters	O
of	O
a	O
non-parametric	B
model	O
in	O
accordance	O
with	O
the	O
weight-space	O
view	O
section	O
the	O
parameters	O
of	O
the	O
underlying	O
parametric	B
model	O
have	O
been	O
integrated	B
out	O
xoutput	O
y	O
xoutput	O
y	O
xoutput	O
y	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
decision	O
theory	O
for	O
regression	O
were	O
extended	O
one	O
would	O
see	O
the	O
error	O
bars	O
reflect	O
the	O
prior	O
standard	O
deviation	O
of	O
the	O
process	O
f	O
away	O
from	O
the	O
data	O
if	O
we	O
set	B
the	O
length-scale	B
shorter	O
so	O
that	O
and	O
kept	O
the	O
other	O
parameters	O
the	O
same	O
then	O
generating	O
from	O
this	O
process	O
we	O
would	O
expect	O
to	O
see	O
plots	O
like	O
those	O
in	O
figure	O
except	O
that	O
the	O
x-axis	O
should	O
be	O
rescaled	O
by	O
a	O
factor	O
of	O
equivalently	O
if	O
the	O
same	O
x-axis	O
was	O
kept	O
as	O
in	O
figure	O
then	O
a	O
sample	O
function	B
would	O
look	O
much	O
more	O
wiggly	O
if	O
we	O
make	O
predictions	O
with	O
a	O
process	O
with	O
on	O
the	O
data	O
generated	O
from	O
the	O
process	O
then	O
we	O
obtain	O
the	O
result	O
in	O
figure	O
the	O
remaining	O
two	O
parameters	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
as	O
explained	O
in	O
chapter	O
in	O
this	O
case	O
the	O
noise	O
parameter	O
is	O
reduced	O
to	O
n	O
as	O
the	O
greater	O
flexibility	O
of	O
the	O
signal	O
means	O
that	O
the	O
noise	O
level	O
can	O
be	O
reduced	O
this	O
can	O
be	O
observed	O
at	O
the	O
two	O
datapoints	O
near	O
x	O
in	O
the	O
plots	O
in	O
figure	O
these	O
are	O
essentially	O
explained	O
as	O
a	O
similar	O
function	B
value	O
with	O
differing	O
noise	O
however	O
in	O
figure	O
the	O
noise	O
level	O
is	O
very	O
low	O
so	O
these	O
two	O
points	O
have	O
to	O
be	O
explained	O
by	O
a	O
sharp	O
variation	O
in	O
the	O
value	O
of	O
the	O
underlying	O
function	B
f	O
notice	O
also	O
that	O
the	O
short	O
length-scale	B
means	O
that	O
the	O
error	O
bars	O
in	O
figure	O
grow	O
rapidly	O
away	O
from	O
the	O
datapoints	O
in	O
contrast	O
we	O
can	O
set	B
the	O
length-scale	B
longer	O
for	O
example	O
to	O
as	O
shown	O
in	O
figure	O
again	O
the	O
remaining	O
two	O
parameters	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
in	O
this	O
case	O
the	O
noise	O
level	O
has	O
been	O
increased	O
to	O
n	O
and	O
we	O
see	O
that	O
the	O
data	O
is	O
now	O
explained	O
by	O
a	O
slowly	O
varying	O
function	B
with	O
a	O
lot	O
of	O
noise	O
of	O
course	O
we	O
can	O
take	O
the	O
position	O
of	O
a	O
quickly-varying	O
signal	O
with	O
low	O
noise	O
or	O
a	O
slowly-varying	O
signal	O
with	O
high	O
noise	O
to	O
extremes	O
the	O
former	O
would	O
give	O
rise	O
to	O
a	O
white-noise	O
process	O
model	O
for	O
the	O
signal	O
while	O
the	O
latter	O
would	O
give	O
rise	O
to	O
a	O
constant	O
signal	O
with	O
added	O
white	O
noise	O
under	O
both	O
these	O
models	O
the	O
datapoints	O
produced	O
should	O
look	O
like	O
white	O
noise	O
however	O
studying	O
figure	O
we	O
see	O
that	O
white	O
noise	O
is	O
not	O
a	O
convincing	O
model	O
of	O
the	O
data	O
as	O
the	O
sequence	O
of	O
y	O
s	O
does	O
not	O
alternate	O
sufficiently	O
quickly	O
but	O
has	O
correlations	O
due	O
to	O
the	O
variability	O
of	O
the	O
underlying	O
function	B
of	O
course	O
this	O
is	O
relatively	O
easy	O
to	O
see	O
in	O
one	O
dimension	O
but	O
methods	O
such	O
as	O
the	O
marginal	B
likelihood	B
discussed	O
in	O
chapter	O
generalize	O
to	O
higher	O
dimensions	O
and	O
allow	O
us	O
to	O
score	O
the	O
various	O
models	O
in	O
this	O
case	O
the	O
marginal	B
likelihood	B
gives	O
a	O
clear	O
preference	O
for	O
f	O
n	O
over	O
the	O
other	O
two	O
alternatives	O
decision	O
theory	O
for	O
regression	O
in	O
the	O
previous	O
sections	O
we	O
have	O
shown	O
how	O
to	O
compute	O
predictive	B
distributions	O
for	O
the	O
outputs	B
y	O
corresponding	O
to	O
the	O
novel	O
test	O
input	O
x	O
the	O
predictive	B
distribution	O
is	O
gaussian	O
with	O
mean	O
and	O
variance	O
given	O
by	O
eq	O
and	O
eq	O
in	O
practical	O
applications	O
however	O
we	O
are	O
often	O
forced	O
to	O
make	O
a	O
decision	O
about	O
how	O
to	O
act	O
i	O
e	O
we	O
need	O
a	O
point-like	O
prediction	O
which	O
is	O
optimal	B
in	O
some	O
sense	O
to	O
this	O
end	O
we	O
need	O
a	O
loss	B
function	B
lytrue	O
yguess	O
which	O
specifies	O
the	O
loss	B
too	O
short	O
length-scale	B
too	O
long	O
length-scale	B
model	O
comparison	O
optimal	B
predictions	O
loss	B
function	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
non-bayesian	O
paradigm	O
bayesian	O
paradigm	O
penalty	O
incurred	O
by	O
guessing	O
the	O
value	O
yguess	O
when	O
the	O
true	O
value	O
is	O
ytrue	O
for	O
example	O
the	O
loss	B
function	B
could	O
equal	O
the	O
absolute	O
deviation	O
between	O
the	O
guess	O
and	O
the	O
truth	O
notice	O
that	O
we	O
computed	O
the	O
predictive	B
distribution	O
without	O
reference	O
to	O
the	O
loss	B
function	B
in	O
non-bayesian	O
paradigms	O
the	O
model	O
is	O
typically	O
trained	O
by	O
minimizing	O
the	O
empirical	O
risk	B
loss	B
in	O
contrast	O
in	O
the	O
bayesian	O
setting	O
there	O
is	O
a	O
clear	O
separation	O
between	O
the	O
likelihood	B
function	B
for	O
training	O
in	O
addition	O
to	O
the	O
prior	O
and	O
the	O
loss	B
function	B
the	O
likelihood	B
function	B
describes	O
how	O
the	O
noisy	O
measurements	O
are	O
assumed	O
to	O
deviate	O
from	O
the	O
underlying	O
noisefree	O
function	B
the	O
loss	B
function	B
on	O
the	O
other	O
hand	O
captures	O
the	O
consequences	O
of	O
making	O
a	O
specific	O
choice	O
given	O
an	O
actual	O
true	O
state	O
the	O
likelihood	B
and	O
loss	B
function	B
need	O
not	O
have	O
anything	O
in	O
expected	O
loss	B
risk	B
absolute	O
error	O
loss	B
squared	B
error	O
loss	B
robot	B
arm	O
our	O
goal	O
is	O
to	O
make	O
the	O
point	O
prediction	O
yguess	O
which	O
incurs	O
the	O
smallest	O
loss	B
but	O
how	O
can	O
we	O
achieve	O
that	O
when	O
we	O
don	O
t	O
know	O
ytrue	O
instead	O
we	O
minimize	O
the	O
expected	O
loss	B
or	O
risk	B
by	O
averaging	O
w	O
r	O
t	O
our	O
model	O
s	O
opinion	O
as	O
to	O
what	O
the	O
truth	O
might	O
be	O
z	O
rlyguessx	O
ly	O
yguesspy	O
dy	O
thus	O
our	O
best	O
guess	O
in	O
the	O
sense	O
that	O
it	O
minimizes	O
the	O
expected	O
loss	B
is	O
yoptimalx	O
argmin	O
yguess	O
rlyguessx	O
in	O
general	O
the	O
value	O
of	O
yguess	O
that	O
minimizes	O
the	O
risk	B
for	O
the	O
loss	B
function	B
y	O
is	O
the	O
median	O
of	O
py	O
while	O
for	O
the	O
squared	B
loss	B
y	O
it	O
is	O
the	O
mean	O
of	O
this	O
distribution	O
when	O
the	O
predictive	B
distribution	O
is	O
gaussian	O
the	O
mean	O
and	O
the	O
median	O
coincide	O
and	O
indeed	O
for	O
any	O
symmetric	O
loss	B
function	B
and	O
symmetric	O
predictive	B
distribution	O
we	O
always	O
get	O
yguess	O
as	O
the	O
mean	O
of	O
the	O
predictive	B
distribution	O
however	O
in	O
many	O
practical	O
problems	O
the	O
loss	B
functions	O
can	O
be	O
asymmetric	O
e	O
g	O
in	O
safety	O
critical	O
applications	O
and	O
point	O
predictions	O
may	O
be	O
computed	O
directly	O
from	O
eq	O
and	O
eq	O
a	O
comprehensive	O
treatment	O
of	O
decision	O
theory	O
can	O
be	O
found	O
in	O
berger	O
an	O
example	O
application	O
in	O
this	O
section	O
we	O
use	O
gaussian	B
process	I
regression	I
to	O
learn	O
the	O
inverse	O
dynamics	O
of	O
a	O
seven	O
degrees-of-freedom	O
sarcos	O
anthropomorphic	O
robot	B
arm	O
the	O
task	O
is	O
to	O
map	B
from	O
a	O
input	O
space	O
joint	B
positions	O
joint	B
velocities	O
joint	B
accelerations	O
to	O
the	O
corresponding	O
joint	B
torques	O
this	O
task	O
has	O
previously	O
been	O
used	O
to	O
study	O
regression	O
algorithms	O
by	O
vijayakumar	O
and	O
schaal	O
vijayakumar	O
et	O
al	O
and	O
vijayakumar	O
et	O
al	O
following	O
of	O
fallacious	O
arguments	O
like	O
a	O
gaussian	O
likelihood	B
implies	O
a	O
squared	B
error	O
loss	B
function	B
thank	O
sethu	O
vijayakumar	O
for	O
providing	O
us	O
with	O
the	O
data	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
an	O
example	O
application	O
this	O
previous	O
work	O
we	O
present	O
results	O
below	O
on	O
just	O
one	O
of	O
the	O
seven	O
mappings	O
from	O
the	O
input	O
variables	O
to	O
the	O
first	O
of	O
the	O
seven	O
torques	O
one	O
might	O
ask	O
why	O
it	O
is	O
necessary	O
to	O
learn	O
this	O
mapping	O
indeed	O
there	O
exist	O
physics-based	O
rigid-body-dynamics	O
models	O
which	O
allow	O
us	O
to	O
obtain	O
the	O
torques	O
from	O
the	O
position	O
velocity	O
and	O
acceleration	O
variables	O
however	O
the	O
real	O
robot	B
arm	O
is	O
actuated	O
hydraulically	O
and	O
is	O
rather	O
lightweight	O
and	O
compliant	O
so	O
the	O
assumptions	O
of	O
the	O
rigid-body-dynamics	O
model	O
are	O
violated	O
we	O
see	O
below	O
it	O
is	O
worth	O
noting	O
that	O
the	O
rigid-body-dynamics	O
model	O
is	O
nonlinear	O
involving	O
trigonometric	O
functions	O
and	O
squares	O
of	O
the	O
input	O
variables	O
an	O
inverse	O
dynamics	O
model	O
can	O
be	O
used	O
in	O
the	O
following	O
manner	O
a	O
planning	O
module	O
decides	O
on	O
a	O
trajectory	O
that	O
takes	O
the	O
robot	B
from	O
its	O
start	O
to	O
goal	O
states	O
and	O
this	O
specifies	O
the	O
desired	O
positions	O
velocities	O
and	O
accelerations	O
at	O
each	O
time	O
the	O
inverse	O
dynamics	O
model	O
is	O
used	O
to	O
compute	O
the	O
torques	O
needed	O
to	O
achieve	O
this	O
trajectory	O
and	O
errors	O
are	O
corrected	O
using	O
a	O
feedback	O
controller	O
the	O
dataset	O
consists	O
of	O
input-output	O
pairs	O
of	O
which	O
were	O
used	O
as	O
a	O
training	O
set	B
and	O
the	O
remaining	O
were	O
used	O
as	O
a	O
test	O
set	B
the	O
inputs	O
were	O
linearly	O
rescaled	O
to	O
have	O
zero	O
mean	O
and	O
unit	O
variance	O
on	O
the	O
training	O
set	B
the	O
outputs	B
were	O
centered	O
so	O
as	O
to	O
have	O
zero	O
mean	O
on	O
the	O
training	O
set	B
given	O
a	O
prediction	O
method	O
we	O
can	O
evaluate	O
the	O
quality	O
of	O
predictions	O
in	O
several	O
ways	O
perhaps	O
the	O
simplest	O
is	O
the	O
squared	B
error	O
loss	B
where	O
we	O
compute	O
the	O
squared	B
residual	O
fx	O
between	O
the	O
mean	O
prediction	O
and	O
the	O
target	O
at	O
each	O
test	O
point	O
this	O
can	O
be	O
summarized	O
by	O
the	O
mean	O
squared	B
error	O
by	O
averaging	O
over	O
the	O
test	O
set	B
however	O
this	O
quantity	O
is	O
sensitive	O
to	O
the	O
overall	O
scale	O
of	O
the	O
target	O
values	O
so	O
it	O
makes	O
sense	O
to	O
normalize	O
by	O
the	O
variance	O
of	O
the	O
targets	O
of	O
the	O
test	O
cases	O
to	O
obtain	O
the	O
standardized	B
mean	I
squared	B
error	I
this	O
causes	O
the	O
trivial	O
method	O
of	O
guessing	O
the	O
mean	O
of	O
the	O
training	O
targets	O
to	O
have	O
a	O
smse	O
of	O
approximately	O
additionally	O
if	O
we	O
produce	O
a	O
predictive	B
distribution	O
at	O
each	O
test	O
input	O
we	O
can	O
evaluate	O
the	O
negative	B
log	I
probability	I
of	O
the	O
target	O
under	O
the	O
as	O
gpr	O
produces	O
a	O
gaussian	O
predictive	B
density	O
one	O
obtains	O
log	O
py	O
x	O
fx	O
where	O
the	O
predictive	B
variance	O
for	O
gpr	O
is	O
computed	O
as	O
vf	O
n	O
where	O
vf	O
is	O
given	O
by	O
eq	O
we	O
must	O
include	O
the	O
noise	O
variance	O
n	O
as	O
we	O
are	O
predicting	O
the	O
noisy	O
target	O
y	O
this	O
loss	B
can	O
be	O
standardized	O
by	O
subtracting	O
the	O
loss	B
that	O
would	O
be	O
obtained	O
under	O
the	O
trivial	O
model	O
which	O
predicts	O
using	O
a	O
gaussian	O
with	O
the	O
mean	O
and	O
variance	O
of	O
the	O
training	O
data	O
we	O
denote	O
this	O
the	O
standardized	O
log	O
loss	B
the	O
mean	O
sll	O
is	O
denoted	O
msll	O
thus	O
the	O
msll	O
will	O
be	O
approximately	O
zero	O
for	O
simple	O
methods	O
and	O
negative	O
for	O
better	O
methods	O
a	O
number	O
of	O
models	O
were	O
tested	O
on	O
the	O
data	O
a	O
linear	B
regression	I
model	O
provides	O
a	O
simple	O
baseline	O
for	O
the	O
smse	O
by	O
estimating	O
the	O
noise	O
level	O
from	O
the	O
it	O
makes	O
sense	O
to	O
use	O
the	O
negative	B
log	I
probability	I
so	O
as	O
to	O
obtain	O
a	O
loss	B
not	O
a	O
utility	O
why	O
learning	B
mse	O
smse	O
msll	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
method	O
lr	O
rbd	O
lwpr	O
gpr	O
smse	O
msll	O
table	O
test	O
results	O
on	O
the	O
inverse	O
dynamics	O
problem	O
for	O
a	O
number	O
of	O
different	O
methods	O
the	O
denotes	O
a	O
missing	O
entry	O
caused	O
by	O
two	O
methods	O
not	O
producing	O
full	O
predictive	B
distributions	O
so	O
msll	O
could	O
not	O
be	O
evaluated	O
residuals	O
on	O
the	O
training	O
set	B
one	O
can	O
also	O
obtain	O
a	O
predictive	B
variance	O
and	O
thus	O
get	O
a	O
msll	O
value	O
for	O
lr	O
the	O
rigid-body-dynamics	O
model	O
has	O
a	O
number	O
of	O
free	O
parameters	O
these	O
were	O
estimated	O
by	O
vijayakumar	O
et	O
al	O
using	O
a	O
least-squares	B
fitting	O
procedure	O
we	O
also	O
give	O
results	O
for	O
the	O
locally	O
weighted	O
projection	O
regression	O
method	O
of	O
vijayakumar	O
et	O
al	O
which	O
is	O
an	O
on-line	O
method	O
that	O
cycles	O
through	O
the	O
dataset	O
multiple	O
times	O
for	O
the	O
gp	O
models	O
it	O
is	O
computationally	O
expensive	O
to	O
make	O
use	O
of	O
all	O
training	O
cases	O
due	O
to	O
the	O
scaling	O
of	O
the	O
basic	O
algorithm	O
in	O
chapter	O
we	O
present	O
several	O
different	O
approximate	O
gp	O
methods	O
for	O
large	O
datasets	O
the	O
result	O
given	O
in	O
table	O
was	O
obtained	O
with	O
the	O
subset	B
of	I
regressors	I
approximation	O
with	O
a	O
subset	O
size	O
of	O
this	O
result	O
is	O
taken	O
from	O
table	O
which	O
gives	O
full	O
results	O
of	O
the	O
various	O
approximation	O
methods	O
applied	O
to	O
the	O
inverse	O
dynamics	O
problem	O
the	O
squared	B
exponential	B
covariance	B
function	B
was	O
used	O
with	O
a	O
separate	O
length-scale	B
parameter	O
for	O
each	O
of	O
the	O
input	O
dimensions	O
plus	O
the	O
signal	O
and	O
noise	O
variance	O
n	O
these	O
parameters	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
parameters	O
likelihood	B
eq	O
on	O
a	O
subset	O
of	O
the	O
data	O
also	O
chapter	O
f	O
and	O
the	O
results	O
for	O
the	O
various	O
methods	O
are	O
presented	O
in	O
table	O
notice	O
that	O
the	O
problem	O
is	O
quite	O
non-linear	O
so	O
the	O
linear	B
regression	I
model	O
does	O
poorly	O
in	O
comparison	O
to	O
non-linear	O
the	O
non-linear	O
method	O
lwpr	O
improves	O
over	O
linear	B
regression	I
but	O
is	O
outperformed	O
by	O
gpr	O
smoothing	O
weight	O
functions	O
and	O
equiva	O
lent	O
kernels	O
gaussian	B
process	I
regression	I
aims	O
to	O
reconstruct	O
the	O
underlying	O
signal	O
f	O
by	O
removing	O
the	O
contaminating	O
noise	O
to	O
do	O
this	O
it	O
computes	O
a	O
weighted	O
average	O
of	O
the	O
noisy	O
observations	O
y	O
as	O
fx	O
kx	O
ni	O
as	O
fx	O
is	O
a	O
linear	B
combination	O
of	O
the	O
y	O
values	O
gaussian	B
process	I
regression	I
is	O
a	O
linear	B
smoother	O
hastie	O
and	O
tibshirani	O
sec	O
for	O
further	O
details	O
in	O
this	O
section	O
we	O
study	O
smoothing	O
first	O
in	O
terms	O
of	O
a	O
matrix	O
analysis	O
of	O
the	O
predictions	O
at	O
the	O
training	O
points	O
and	O
then	O
in	O
terms	O
of	O
the	O
equivalent	B
kernel	B
is	O
perhaps	O
surprising	O
that	O
rbd	O
does	O
worse	O
than	O
linear	B
regression	I
however	O
stefan	O
schaal	O
comm	O
states	O
that	O
the	O
rbd	O
parameters	O
were	O
optimized	O
on	O
a	O
very	O
large	O
dataset	O
of	O
which	O
the	O
training	O
data	O
used	O
here	O
is	O
subset	O
and	O
if	O
the	O
rbd	O
model	O
were	O
optimized	O
w	O
r	O
t	O
this	O
training	O
set	B
one	O
might	O
well	O
expect	O
it	O
to	O
outperform	O
linear	B
regression	I
linear	B
smoother	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
smoothing	O
weight	O
functions	O
and	O
equivalent	B
kernels	O
the	O
predicted	O
mean	O
values	O
f	O
at	O
the	O
training	O
points	O
are	O
given	O
by	O
ni	O
f	O
kk	O
let	O
k	O
have	O
the	O
eigendecomposition	O
k	O
pn	O
eigenvectors	O
are	O
mutually	O
orthogonal	O
let	O
y	O
i	O
where	O
i	O
is	O
the	O
ith	O
eigenvalue	B
and	O
ui	O
is	O
the	O
corresponding	O
eigenvector	O
as	O
k	O
is	O
real	O
and	O
symmetric	O
positive	B
semidefinite	I
its	O
eigenvalues	O
are	O
real	O
and	O
non-negative	O
and	O
its	O
iui	O
for	O
some	O
coefficients	O
i	O
u	O
iuiu	O
i	O
y	O
then	O
f	O
i	O
i	O
i	O
n	O
ui	O
nx	O
n	O
then	O
the	O
component	O
in	O
y	O
along	O
ui	O
is	O
effectively	O
notice	O
that	O
if	O
i	O
i	O
eliminated	O
for	O
most	O
covariance	B
functions	O
that	O
are	O
used	O
in	O
practice	O
the	O
eigenvalues	O
are	O
larger	O
for	O
more	O
slowly	O
varying	O
eigenvectors	O
fewer	O
zero-crossings	O
so	O
that	O
this	O
means	O
that	O
high-frequency	O
components	O
in	O
y	O
are	O
smoothed	O
out	O
the	O
effective	O
number	O
of	O
parameters	O
or	O
degrees	B
of	I
freedom	I
of	O
the	O
smoother	O
is	O
n	O
see	O
hastie	O
and	O
tibshirani	O
defined	O
as	O
trkk	O
sec	O
notice	O
that	O
this	O
counts	O
the	O
number	O
of	O
eigenvectors	O
which	O
are	O
not	O
eliminated	O
ni	O
i	O
i	O
we	O
can	O
define	O
a	O
vector	O
of	O
functions	O
hx	O
ni	O
thus	O
we	O
have	O
fx	O
hx	O
making	O
it	O
clear	O
that	O
the	O
mean	O
prediction	O
at	O
a	O
point	O
x	O
is	O
a	O
linear	B
combination	O
of	O
the	O
target	O
values	O
y	O
for	O
a	O
fixed	O
test	O
point	O
x	O
hx	O
gives	O
the	O
vector	O
of	O
weights	O
applied	O
to	O
targets	O
y	O
hx	O
is	O
called	O
the	O
weight	B
function	B
as	O
gaussian	B
process	I
regression	I
is	O
a	O
linear	B
smoother	O
the	O
weight	B
function	B
does	O
not	O
depend	O
on	O
y	O
note	O
the	O
difference	O
between	O
a	O
linear	B
model	O
where	O
the	O
prediction	O
is	O
a	O
linear	B
combination	O
of	O
the	O
inputs	O
and	O
a	O
linear	B
smoother	O
where	O
the	O
prediction	O
is	O
a	O
linear	B
combination	O
of	O
the	O
training	O
set	B
targets	O
understanding	O
the	O
form	O
of	O
the	O
weight	B
function	B
is	O
made	O
complicated	O
by	O
the	O
matrix	O
inversion	O
of	O
k	O
ni	O
and	O
the	O
fact	O
that	O
k	O
depends	O
on	O
the	O
specific	O
locations	O
of	O
the	O
n	O
datapoints	O
idealizing	O
the	O
situation	O
one	O
can	O
consider	O
the	O
observations	O
to	O
be	O
smeared	O
out	O
in	O
x-space	O
at	O
some	O
density	O
of	O
observations	O
in	O
this	O
case	O
analytic	O
tools	O
can	O
be	O
brought	O
to	O
bear	O
on	O
the	O
problem	O
as	O
shown	O
in	O
section	O
by	O
analogy	O
to	O
kernel	B
smoothing	O
silverman	O
called	O
the	O
idealized	O
weight	B
function	B
the	O
equivalent	B
kernel	B
see	O
also	O
girosi	O
et	O
al	O
sec	O
a	O
kernel	B
smoother	I
centres	O
a	O
kernel	B
on	O
x	O
and	O
then	O
computes	O
i	O
x	O
for	O
each	O
data	O
point	O
yi	O
where	O
is	O
a	O
length-scale	B
the	O
gaussian	O
is	O
a	O
commonly	O
used	O
kernel	B
function	B
the	O
prediction	O
for	O
fx	O
is	O
j	O
this	O
is	O
also	O
known	O
where	O
wi	O
ipn	O
computed	O
as	O
fx	O
as	O
the	O
nadaraya-watson	B
estimator	I
see	O
e	O
g	O
scott	O
sec	O
the	O
weight	B
function	B
and	O
equivalent	B
kernel	B
for	O
a	O
gaussian	B
process	I
are	O
illustrated	O
in	O
figure	O
for	O
a	O
one-dimensional	O
input	O
variable	O
x	O
we	O
have	O
used	O
the	O
squared	B
exponential	B
covariance	B
function	B
and	O
have	O
set	B
the	O
length-scale	B
that	O
there	O
are	O
n	O
training	O
points	O
spaced	O
randomly	O
along	O
that	O
this	O
kernel	B
function	B
does	O
not	O
need	O
to	O
be	O
a	O
valid	O
covariance	B
function	B
eigendecomposition	O
degrees	B
of	I
freedom	I
weight	B
function	B
equivalent	B
kernel	B
kernel	B
smoother	I
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regression	O
figure	O
panels	O
show	O
the	O
weight	B
function	B
hx	O
corresponding	O
to	O
the	O
n	O
training	O
points	O
the	O
equivalent	B
kernel	B
and	O
the	O
original	O
squared	B
exponential	B
kernel	B
panel	O
shows	O
the	O
equivalent	B
kernels	O
for	O
two	O
different	O
data	O
densities	O
see	O
text	O
for	O
further	O
details	O
the	O
small	O
cross	O
at	O
the	O
test	O
point	O
is	O
to	O
scale	O
in	O
all	O
four	O
plots	O
the	O
x-axis	O
figures	O
and	O
show	O
the	O
weight	B
function	B
and	O
equivalent	B
kernel	B
for	O
x	O
and	O
x	O
respectively	O
for	O
n	O
figure	O
is	O
also	O
for	O
x	O
but	O
uses	O
n	O
in	O
each	O
case	O
the	O
dots	O
correspond	O
to	O
the	O
weight	B
function	B
hx	O
and	O
the	O
solid	O
line	O
is	O
the	O
equivalent	B
kernel	B
whose	O
construction	O
is	O
explained	O
below	O
the	O
dashed	O
line	O
shows	O
a	O
squared	B
exponential	B
kernel	B
centered	O
on	O
the	O
test	O
point	O
scaled	O
to	O
have	O
the	O
same	O
height	O
as	O
the	O
maximum	O
value	O
in	O
the	O
equivalent	B
kernel	B
figure	O
shows	O
the	O
variation	O
in	O
the	O
equivalent	B
kernel	B
as	O
a	O
function	B
of	O
n	O
the	O
number	O
of	O
datapoints	O
in	O
the	O
unit	O
interval	O
many	O
interesting	O
observations	O
can	O
be	O
made	O
from	O
these	O
plots	O
observe	O
that	O
the	O
equivalent	B
kernel	B
has	O
general	O
a	O
shape	O
quite	O
different	O
to	O
the	O
original	O
se	O
kernel	B
in	O
figure	O
the	O
equivalent	B
kernel	B
is	O
clearly	O
oscillatory	O
negative	O
sidelobes	O
and	O
has	O
a	O
higher	O
spatial	O
frequency	O
than	O
the	O
original	O
kernel	B
figure	O
shows	O
similar	O
behaviour	O
although	O
due	O
to	O
edge	O
effects	O
the	O
equivalent	B
kernel	B
is	O
truncated	O
relative	O
to	O
that	O
in	O
figure	O
in	O
figure	O
we	O
see	O
that	O
at	O
higher	O
noise	O
levels	O
the	O
negative	O
sidelobes	O
are	O
reduced	O
and	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	B
is	O
similar	O
to	O
the	O
original	O
kernel	B
also	O
note	O
that	O
the	O
overall	O
height	O
of	O
the	O
equivalent	B
kernel	B
in	O
is	O
reduced	O
compared	O
to	O
that	O
in	O
and	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
incorporating	O
explicit	O
basis	O
functions	O
it	O
averages	O
over	O
a	O
wider	O
area	O
the	O
more	O
oscillatory	O
equivalent	B
kernel	B
for	O
lower	O
noise	O
levels	O
can	O
be	O
understood	O
in	O
terms	O
of	O
the	O
eigenanalysis	O
above	O
at	O
higher	O
noise	O
levels	O
only	O
the	O
large	O
varying	O
components	O
of	O
y	O
remain	O
while	O
for	O
smaller	O
noise	O
levels	O
the	O
more	O
oscillatory	O
components	O
are	O
also	O
retained	O
in	O
figure	O
we	O
have	O
plotted	O
the	O
equivalent	B
kernel	B
for	O
n	O
and	O
n	O
datapoints	O
in	O
notice	O
how	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	B
decreases	O
as	O
n	O
increases	O
we	O
discuss	O
this	O
behaviour	O
further	O
in	O
section	O
the	O
plots	O
of	O
equivalent	B
kernels	O
in	O
figure	O
were	O
made	O
by	O
using	O
a	O
dense	O
grid	O
of	O
ngrid	O
points	O
on	O
and	O
then	O
computing	O
the	O
smoother	O
matrix	O
kk	O
gridi	O
each	O
row	O
of	O
this	O
matrix	O
is	O
the	O
equivalent	B
kernel	B
at	O
the	O
appropriate	O
location	O
however	O
in	O
order	O
to	O
get	O
the	O
scaling	O
right	O
one	O
has	O
to	O
set	B
grid	O
nngridn	O
for	O
ngrid	O
n	O
this	O
means	O
that	O
the	O
effective	O
variance	O
at	O
each	O
of	O
the	O
ngrid	O
points	O
is	O
larger	O
but	O
as	O
there	O
are	O
correspondingly	O
more	O
points	O
this	O
effect	O
cancels	O
out	O
this	O
can	O
be	O
understood	O
by	O
imagining	O
the	O
situation	O
if	O
there	O
were	O
ngridn	O
independent	O
gaussian	O
observations	O
with	O
variance	O
grid	O
at	O
a	O
single	O
xposition	O
this	O
would	O
be	O
equivalent	B
to	O
one	O
gaussian	O
observation	O
with	O
variance	O
n	O
in	O
effect	O
the	O
n	O
observations	O
have	O
been	O
smoothed	O
out	O
uniformly	O
along	O
the	O
interval	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	B
can	O
be	O
obtained	O
analytically	O
if	O
we	O
go	O
to	O
the	O
continuum	O
limit	O
and	O
look	O
to	O
smooth	O
a	O
noisy	O
function	B
the	O
relevant	O
theory	O
and	O
some	O
example	O
equivalent	B
kernels	O
are	O
given	O
in	O
section	O
incorporating	O
explicit	O
basis	O
functions	O
it	O
is	O
common	O
but	O
by	O
no	O
means	O
necessary	O
to	O
consider	O
gps	O
with	O
a	O
zero	O
mean	B
function	B
note	O
that	O
this	O
is	O
not	O
necessarily	O
a	O
drastic	O
limitation	O
since	O
the	O
mean	O
of	O
the	O
posterior	B
process	I
is	O
not	O
confined	O
to	O
be	O
zero	O
yet	O
there	O
are	O
several	O
reasons	O
why	O
one	O
might	O
wish	O
to	O
explicitly	O
model	O
a	O
mean	B
function	B
including	O
interpretability	O
of	O
the	O
model	O
convenience	O
of	O
expressing	O
prior	O
information	O
and	O
a	O
number	O
of	O
analytical	O
limits	O
which	O
we	O
will	O
need	O
in	O
subsequent	O
chapters	O
the	O
use	O
of	O
explicit	O
basis	O
functions	O
is	O
a	O
way	O
to	O
specify	O
a	O
non-zero	O
mean	O
over	O
functions	O
but	O
as	O
we	O
will	O
see	O
in	O
this	O
section	O
one	O
can	O
also	O
use	O
them	O
to	O
achieve	O
other	O
interesting	O
effects	O
using	O
a	O
fixed	O
mean	B
function	B
mx	O
is	O
trivial	O
simply	O
apply	O
the	O
usual	O
zero	O
mean	O
gp	O
to	O
the	O
difference	O
between	O
the	O
observations	O
and	O
the	O
fixed	O
mean	B
function	B
with	O
fx	O
kx	O
the	O
predictive	B
mean	O
becomes	O
f	O
mx	O
kx	O
xk	O
y	O
mx	O
where	O
ky	O
k	O
eq	O
ni	O
and	O
the	O
predictive	B
variance	O
remains	O
unchanged	O
from	O
however	O
in	O
practice	O
it	O
can	O
often	O
be	O
difficult	O
to	O
specify	O
a	O
fixed	O
mean	B
function	B
in	O
many	O
cases	O
it	O
may	O
be	O
more	O
convenient	O
to	O
specify	O
a	O
few	O
fixed	O
basis	O
functions	O
fixed	O
mean	B
function	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
stochastic	O
mean	B
function	B
polynomial	B
regression	O
whose	O
coefficients	O
are	O
to	O
be	O
inferred	O
from	O
the	O
data	O
consider	O
gx	O
fx	O
hx	O
where	O
fx	O
kx	O
regression	O
here	O
fx	O
is	O
a	O
zero	O
mean	O
gp	O
hx	O
are	O
a	O
set	B
of	O
fixed	O
basis	O
functions	O
and	O
are	O
additional	O
parameters	O
this	O
formulation	O
expresses	O
that	O
the	O
data	O
is	O
close	O
to	O
a	O
global	O
linear	B
model	O
with	O
the	O
residuals	O
being	O
modelled	O
by	O
a	O
gp	O
this	O
idea	O
was	O
explored	O
explicitly	O
as	O
early	O
as	O
by	O
blight	O
and	O
ott	O
who	O
used	O
the	O
gp	O
to	O
model	O
the	O
residuals	O
from	O
a	O
polynomial	B
regression	O
i	O
e	O
hx	O
x	O
when	O
fitting	O
the	O
model	O
one	O
could	O
optimize	O
over	O
the	O
parameters	O
jointly	O
with	O
the	O
hyperparameters	B
of	O
the	O
covariance	B
function	B
alternatively	O
if	O
we	O
take	O
the	O
prior	O
on	O
to	O
be	O
gaussian	O
n	O
b	O
we	O
can	O
also	O
integrate	O
out	O
these	O
parameters	O
following	O
o	O
hagan	O
we	O
obtain	O
another	O
gp	O
gx	O
kx	O
now	O
with	O
an	O
added	O
contribution	O
in	O
the	O
covariance	B
function	B
caused	O
by	O
the	O
uncertainty	O
in	O
the	O
parameters	O
of	O
the	O
mean	O
predictions	O
are	O
made	O
by	O
plugging	O
the	O
mean	O
and	O
covariance	B
functions	O
of	O
gx	O
into	O
eq	O
and	O
eq	O
after	O
rearranging	O
we	O
obtain	O
gx	O
h	O
covg	O
covf	O
rb	O
hk	O
y	O
h	O
fx	O
r	O
k	O
k	O
y	O
h	O
y	O
h	O
where	O
the	O
h	O
matrix	O
collects	O
the	O
hx	O
vectors	O
for	O
all	O
training	O
h	O
all	O
test	O
cases	O
hk	O
y	O
k	O
notice	O
the	O
nice	O
interpretation	O
of	O
the	O
mean	O
expression	O
eq	O
top	O
line	O
is	O
the	O
mean	O
of	O
the	O
global	O
linear	B
model	O
parameters	O
being	O
a	O
compromise	O
between	O
the	O
data	O
term	O
and	O
prior	O
and	O
the	O
predictive	B
mean	O
is	O
simply	O
the	O
mean	O
linear	B
output	O
plus	O
what	O
the	O
gp	O
model	O
predicts	O
from	O
the	O
residuals	O
the	O
covariance	B
is	O
the	O
sum	O
of	O
the	O
usual	O
covariance	B
term	O
and	O
a	O
new	O
non-negative	O
contribution	O
y	O
y	O
b	O
and	O
r	O
h	O
hk	O
exploring	O
the	O
limit	O
of	O
the	O
above	O
expressions	O
as	O
the	O
prior	O
on	O
the	O
parameter	O
becomes	O
vague	O
b	O
o	O
o	O
is	O
the	O
matrix	O
of	O
zeros	O
we	O
obtain	O
a	O
predictive	B
distribution	O
which	O
is	O
independent	O
of	O
b	O
gx	O
fx	O
r	O
covg	O
covf	O
rhk	O
y	O
h	O
y	O
h	O
where	O
the	O
limiting	O
y	O
y	O
notice	O
that	O
predictions	O
under	O
the	O
limit	O
b	O
o	O
should	O
not	O
be	O
implemented	O
na	O
vely	O
by	O
plugging	O
the	O
modified	O
covariance	B
function	B
from	O
eq	O
into	O
the	O
standard	O
prediction	O
equations	O
since	O
the	O
entries	O
of	O
the	O
covariance	B
function	B
tend	O
to	O
infinity	O
thus	O
making	O
it	O
unsuitable	O
for	O
numerical	O
implementation	O
instead	O
eq	O
must	O
be	O
used	O
even	O
if	O
the	O
non-limiting	O
case	O
is	O
of	O
interest	O
eq	O
is	O
numerically	O
preferable	O
to	O
a	O
direct	O
implementation	O
based	O
on	O
eq	O
since	O
the	O
global	O
linear	B
part	O
will	O
often	O
add	O
some	O
very	O
large	O
eigenvalues	O
to	O
the	O
covariance	B
matrix	I
affecting	O
its	O
condition	O
number	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
history	O
and	O
related	O
work	O
marginal	B
likelihood	B
in	O
this	O
short	O
section	O
we	O
briefly	O
discuss	O
the	O
marginal	B
likelihood	B
for	O
the	O
model	O
with	O
a	O
gaussian	O
prior	O
n	O
b	O
on	O
the	O
explicit	O
parameters	O
from	O
eq	O
as	O
this	O
will	O
be	O
useful	O
later	O
particularly	O
in	O
section	O
we	O
can	O
express	O
the	O
marginal	B
likelihood	B
from	O
eq	O
as	O
log	O
pyx	O
b	O
b	O
yky	O
hbh	O
y	O
log	O
hbh	O
n	O
log	O
where	O
we	O
have	O
included	O
the	O
explicit	O
mean	O
we	O
are	O
interested	O
in	O
exploring	O
the	O
limit	O
where	O
b	O
o	O
i	O
e	O
when	O
the	O
prior	O
is	O
vague	O
in	O
this	O
limit	O
the	O
mean	O
of	O
the	O
prior	O
is	O
irrelevant	O
was	O
the	O
case	O
in	O
eq	O
so	O
without	O
loss	B
of	O
generality	O
the	O
limiting	O
case	O
we	O
assume	O
for	O
now	O
that	O
the	O
mean	O
is	O
zero	O
b	O
giving	O
log	O
pyx	O
b	O
y	O
h	O
and	O
c	O
k	O
yk	O
y	O
y	O
log	O
where	O
a	O
b	O
hk	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
and	O
eq	O
ycy	O
log	O
y	O
ha	O
y	O
and	O
we	O
have	O
used	O
log	O
n	O
log	O
we	O
now	O
explore	O
the	O
behaviour	O
of	O
the	O
log	O
marginal	B
likelihood	B
in	O
the	O
limit	O
of	O
vague	O
priors	O
on	O
in	O
this	O
limit	O
the	O
variances	O
of	O
the	O
gaussian	O
in	O
the	O
directions	O
spanned	O
by	O
columns	O
of	O
h	O
will	O
become	O
infinite	O
and	O
it	O
is	O
clear	O
that	O
this	O
will	O
require	O
special	O
treatment	O
the	O
log	O
marginal	B
likelihood	B
consists	O
of	O
three	O
terms	O
a	O
quadratic	B
form	I
in	O
y	O
a	O
log	O
determinant	O
term	O
and	O
a	O
term	O
involving	O
log	O
performing	O
an	O
eigendecomposition	O
of	O
the	O
covariance	B
matrix	I
we	O
see	O
that	O
the	O
contributions	O
to	O
quadratic	B
form	I
term	O
from	O
the	O
infinite-variance	O
directions	O
will	O
be	O
zero	O
however	O
the	O
log	O
determinant	O
term	O
will	O
tend	O
to	O
minus	O
infinity	O
the	O
standard	O
solution	O
ansley	O
and	O
kohn	O
in	O
this	O
case	O
is	O
to	O
project	O
y	O
onto	O
the	O
directions	O
orthogonal	O
to	O
the	O
span	O
of	O
h	O
and	O
compute	O
the	O
marginal	B
likelihood	B
in	O
this	O
subspace	O
let	O
the	O
rank	O
of	O
h	O
be	O
m	O
then	O
as	O
shown	O
in	O
ansley	O
and	O
kohn	O
this	O
means	O
that	O
we	O
must	O
discard	O
the	O
terms	O
log	O
m	O
log	O
pyx	O
where	O
a	O
hk	O
log	O
from	O
eq	O
to	O
give	O
ycy	O
y	O
ha	O
y	O
y	O
h	O
and	O
c	O
k	O
log	O
n	O
m	O
log	O
yk	O
y	O
y	O
log	O
history	O
and	O
related	O
work	O
prediction	O
with	O
gaussian	O
processes	O
is	O
certainly	O
not	O
a	O
very	O
recent	O
topic	O
especially	O
for	O
time	O
series	O
analysis	O
the	O
basic	O
theory	O
goes	O
back	O
at	O
least	O
as	O
far	O
as	O
the	O
work	O
of	O
wiener	O
and	O
kolmogorov	O
in	O
the	O
s	O
indeed	O
lauritzen	O
discusses	O
relevant	O
work	O
by	O
the	O
danish	O
astronomer	O
t	O
n	O
thiele	O
dating	O
from	O
time	O
series	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
geostatistics	B
kriging	B
computer	O
experiments	O
machine	O
learning	B
regression	O
gaussian	B
process	I
prediction	O
is	O
also	O
well	O
known	O
in	O
the	O
geostatistics	B
field	O
e	O
g	O
matheron	O
journel	O
and	O
huijbregts	O
where	O
it	O
is	O
known	O
as	O
and	O
in	O
meteorology	O
daley	O
although	O
this	O
literature	O
naturally	O
has	O
focussed	O
mostly	O
on	O
two-	O
and	O
three-dimensional	O
input	O
spaces	O
whittle	O
sec	O
also	O
suggests	O
the	O
use	O
of	O
such	O
methods	O
for	O
spatial	O
prediction	O
ripley	O
and	O
cressie	O
provide	O
useful	O
overviews	O
of	O
gaussian	B
process	I
prediction	O
in	O
spatial	O
statistics	O
gradually	O
it	O
was	O
realized	O
that	O
gaussian	B
process	I
prediction	O
could	O
be	O
used	O
in	O
a	O
general	O
regression	O
context	O
for	O
example	O
o	O
hagan	O
presents	O
the	O
general	O
theory	O
as	O
given	O
in	O
our	O
equations	O
and	O
and	O
applies	O
it	O
to	O
a	O
number	O
of	O
one-dimensional	O
regression	O
problems	O
sacks	O
et	O
al	O
describe	O
gpr	O
in	O
the	O
context	O
of	O
computer	O
experiments	O
the	O
observations	O
y	O
are	O
noise	O
free	O
and	O
discuss	O
a	O
number	O
of	O
interesting	O
directions	O
such	O
as	O
the	O
optimization	O
of	O
parameters	O
in	O
the	O
covariance	B
function	B
our	O
chapter	O
and	O
experimental	O
design	O
the	O
choice	O
of	O
x-points	O
that	O
provide	O
most	O
information	O
on	O
f	O
the	O
authors	O
describe	O
a	O
number	O
of	O
computer	O
simulations	O
that	O
were	O
modelled	O
including	O
an	O
example	O
where	O
the	O
response	O
variable	O
was	O
the	O
clock	O
asynchronization	O
in	O
a	O
circuit	O
and	O
the	O
inputs	O
were	O
six	O
transistor	O
widths	O
santner	O
et	O
al	O
is	O
a	O
recent	O
book	O
on	O
the	O
use	O
of	O
gps	O
for	O
the	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
williams	O
and	O
rasmussen	O
described	O
gaussian	B
process	I
regression	I
in	O
a	O
machine	O
learning	B
context	O
and	O
described	O
optimization	O
of	O
the	O
parameters	O
in	O
the	O
covariance	B
function	B
see	O
also	O
rasmussen	O
they	O
were	O
inspired	O
to	O
use	O
gaussian	B
process	I
by	O
the	O
connection	O
to	O
infinite	O
neural	O
networks	O
as	O
described	O
in	O
section	O
and	O
in	O
neal	O
the	O
kernelization	O
of	O
linear	B
ridge	B
regression	O
described	O
above	O
is	O
also	O
known	O
as	O
kernel	B
ridge	B
regression	O
see	O
e	O
g	O
saunders	O
et	O
al	O
relationships	O
between	O
gaussian	B
process	I
prediction	O
and	O
regularization	B
theory	O
splines	B
support	B
vector	I
machines	O
and	O
relevance	O
vector	O
machines	O
are	O
discussed	O
in	O
chapter	O
exercises	O
replicate	O
the	O
generation	O
of	O
random	O
functions	O
from	O
figure	O
use	O
a	O
regular	O
random	O
grid	O
of	O
scalar	O
inputs	O
and	O
the	O
covariance	B
function	B
from	O
eq	O
hints	O
on	O
how	O
to	O
generate	O
random	O
samples	O
from	O
multi-variate	O
gaussian	O
distributions	O
are	O
given	O
in	O
section	O
invent	O
some	O
training	O
data	O
points	O
and	O
make	O
random	O
draws	O
from	O
the	O
resulting	O
gp	O
posterior	O
using	O
eq	O
in	O
eq	O
we	O
saw	O
that	O
the	O
predictive	B
variance	O
at	O
x	O
under	O
the	O
feature	B
space	I
regression	O
model	O
was	O
varfx	O
show	O
that	O
covfx	O
check	O
that	O
this	O
is	O
compatible	O
with	O
the	O
expression	O
given	O
in	O
eq	O
named	O
the	O
method	O
after	O
the	O
south	O
african	O
mining	O
engineer	O
d	O
g	O
krige	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
exercises	O
the	O
wiener	B
process	I
is	O
defined	O
for	O
x	O
and	O
has	O
section	O
for	O
further	O
details	O
it	O
has	O
mean	O
zero	O
and	O
a	O
non-stationary	O
covariance	B
function	B
kx	O
minx	O
if	O
we	O
condition	O
on	O
the	O
wiener	B
process	I
passing	O
through	O
we	O
obtain	O
a	O
process	O
known	O
as	O
the	O
brownian	B
bridge	I
tied-down	B
wiener	B
process	I
show	O
that	O
this	O
process	O
has	O
covariance	B
kx	O
minx	O
for	O
x	O
and	O
mean	O
write	O
a	O
computer	O
program	O
to	O
draw	O
samples	O
from	O
this	O
process	O
at	O
a	O
finite	O
grid	O
of	O
x	O
points	O
in	O
let	O
varnfx	O
be	O
the	O
predictive	B
variance	O
of	O
a	O
gaussian	B
process	I
regression	I
model	O
at	O
x	O
given	O
a	O
dataset	O
of	O
size	O
n	O
the	O
corresponding	O
predictive	B
variance	O
using	O
a	O
dataset	O
of	O
only	O
the	O
first	O
n	O
training	O
points	O
is	O
denoted	O
varn	O
show	O
that	O
varnfx	O
varn	O
i	O
e	O
that	O
the	O
predictive	B
variance	O
at	O
x	O
cannot	O
increase	O
as	O
more	O
training	O
data	O
is	O
obtained	O
one	O
way	O
to	O
approach	O
this	O
problem	O
is	O
to	O
use	O
the	O
partitioned	O
matrix	O
equations	O
given	O
in	O
section	O
to	O
decompose	O
varnfx	O
kx	O
x	O
k	O
ni	O
an	O
alternative	O
information	O
theoretic	O
argument	O
is	O
given	O
in	O
williams	O
and	O
vivarelli	O
note	O
that	O
while	O
this	O
conclusion	O
is	O
true	O
for	O
gaussian	B
process	I
priors	O
and	O
gaussian	O
noise	O
models	O
it	O
does	O
not	O
hold	O
generally	O
see	O
barber	O
and	O
saad	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
classification	B
in	O
chapter	O
we	O
have	O
considered	O
regression	O
problems	O
where	O
the	O
targets	O
are	O
real	O
valued	O
another	O
important	O
class	O
of	O
problems	O
is	O
classification	B
problems	O
where	O
we	O
wish	O
to	O
assign	O
an	O
input	O
pattern	O
x	O
to	O
one	O
of	O
c	O
classes	O
practical	O
examples	O
of	O
classification	B
problems	O
are	O
handwritten	O
digit	O
recognition	O
we	O
wish	O
to	O
classify	O
a	O
digitized	O
image	O
of	O
a	O
handwritten	O
digit	O
into	O
one	O
of	O
ten	O
classes	O
and	O
the	O
classification	B
of	O
objects	O
detected	O
in	O
astronomical	O
sky	O
surveys	O
into	O
stars	O
or	O
galaxies	O
on	O
the	O
distribution	O
of	O
galaxies	O
in	O
the	O
universe	O
is	O
important	O
for	O
theories	O
of	O
the	O
early	O
universe	O
these	O
examples	O
nicely	O
illustrate	O
that	O
classification	B
problems	O
can	O
either	O
be	O
binary	B
two-class	O
c	O
or	O
multi-class	B
we	O
will	O
focus	O
attention	O
on	O
probabilistic	B
classification	B
where	O
test	O
predictions	O
take	O
the	O
form	O
of	O
class	O
probabilities	O
this	O
contrasts	O
with	O
methods	O
which	O
provide	O
only	O
a	O
guess	O
at	O
the	O
class	O
label	O
and	O
this	O
distinction	O
is	O
analogous	O
to	O
the	O
difference	O
between	O
predictive	B
distributions	O
and	O
point	O
predictions	O
in	O
the	O
regression	O
setting	O
since	O
generalization	B
to	O
test	O
cases	O
inherently	O
involves	O
some	O
level	O
of	O
uncertainty	O
it	O
seems	O
natural	O
to	O
attempt	O
to	O
make	O
predictions	O
in	O
a	O
way	O
that	O
reflects	O
these	O
uncertainties	O
in	O
a	O
practical	O
application	O
one	O
may	O
well	O
seek	O
a	O
class	O
guess	O
which	O
can	O
be	O
obtained	O
as	O
the	O
solution	O
to	O
a	O
decision	O
problem	O
involving	O
the	O
predictive	B
probabilities	O
as	O
well	O
as	O
a	O
specification	O
of	O
the	O
consequences	O
of	O
making	O
specific	O
predictions	O
loss	B
function	B
both	O
classification	B
and	O
regression	O
can	O
be	O
viewed	O
as	O
function	B
approximation	O
problems	O
unfortunately	O
the	O
solution	O
of	O
classification	B
problems	O
using	O
gaussian	O
processes	O
is	O
rather	O
more	O
demanding	O
than	O
for	O
the	O
regression	O
problems	O
considered	O
in	O
chapter	O
this	O
is	O
because	O
we	O
assumed	O
in	O
the	O
previous	O
chapter	O
that	O
the	O
likelihood	B
function	B
was	O
gaussian	O
a	O
gaussian	B
process	I
prior	O
combined	O
with	O
a	O
gaussian	O
likelihood	B
gives	O
rise	O
to	O
a	O
posterior	O
gaussian	B
process	I
over	O
functions	O
and	O
everything	O
remains	O
analytically	O
tractable	O
for	O
classification	B
models	O
where	O
the	O
targets	O
are	O
discrete	O
class	O
labels	O
the	O
gaussian	O
likelihood	B
is	O
the	O
statistics	O
literature	O
classification	B
is	O
often	O
called	O
discrimination	O
may	O
choose	O
to	O
ignore	O
the	O
discreteness	O
of	O
the	O
target	O
values	O
and	O
use	O
a	O
regression	O
treatment	O
where	O
all	O
targets	O
happen	O
to	O
be	O
say	O
for	O
binary	B
classification	B
this	O
is	O
known	O
as	O
binary	B
multi-class	B
probabilistic	B
classification	B
non-gaussian	B
likelihood	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
in	O
this	O
chapter	O
we	O
treat	O
methods	O
of	O
approximate	O
inference	O
for	O
classification	B
where	O
exact	O
inference	O
is	O
not	O
section	O
provides	O
a	O
general	O
discussion	O
of	O
classification	B
problems	O
and	O
describes	O
the	O
generative	O
and	O
discriminative	O
approaches	O
to	O
these	O
problems	O
in	O
section	O
we	O
saw	O
how	O
gaussian	B
process	I
regression	I
can	O
be	O
obtained	O
in	O
section	O
we	O
describe	O
an	O
analogue	O
of	O
by	O
generalizing	O
linear	B
regression	I
linear	B
regression	I
in	O
the	O
classification	B
case	O
logistic	B
regression	I
in	O
section	O
logistic	B
regression	I
is	O
generalized	B
to	O
yield	O
gaussian	B
process	I
classification	B
using	O
again	O
the	O
ideas	O
behind	O
the	O
generalization	B
of	O
linear	B
regression	I
to	O
gpr	O
for	O
gpr	O
the	O
combination	O
of	O
a	O
gp	O
prior	O
with	O
a	O
gaussian	O
likelihood	B
gives	O
rise	O
to	O
a	O
posterior	O
which	O
is	O
again	O
a	O
gaussian	B
process	I
in	O
the	O
classification	B
case	O
the	O
likelihood	B
is	O
non-gaussian	B
but	O
the	O
posterior	B
process	I
can	O
be	O
approximated	O
by	O
a	O
gp	O
the	O
laplace	B
approximation	I
for	O
gpc	O
is	O
described	O
in	O
section	O
binary	B
classification	B
and	O
in	O
section	O
multi-class	B
classification	B
and	O
the	O
expectation	B
propagation	I
algorithm	O
binary	B
classification	B
is	O
described	O
in	O
section	O
both	O
of	O
these	O
methods	O
make	O
use	O
of	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
experimental	O
results	O
for	O
gpc	O
are	O
given	O
in	O
section	O
and	O
a	O
discussion	O
of	O
these	O
results	O
is	O
provided	O
in	O
section	O
classification	B
problems	O
generative	B
approach	I
discriminative	B
approach	I
generative	O
model	O
example	O
the	O
natural	O
starting	O
point	O
for	O
discussing	O
approaches	O
to	O
classification	B
is	O
the	O
joint	B
probability	O
py	O
x	O
where	O
y	O
denotes	O
the	O
class	O
label	O
using	O
bayes	O
theorem	O
this	O
joint	B
probability	O
can	O
be	O
decomposed	O
either	O
as	O
pypxy	O
or	O
as	O
pxpyx	O
this	O
gives	O
rise	O
to	O
two	O
different	O
approaches	O
to	O
classification	B
problems	O
the	O
first	O
which	O
we	O
call	O
the	O
generative	B
approach	I
models	O
the	O
class-conditional	O
distributions	O
pxy	O
for	O
y	O
and	O
also	O
the	O
prior	O
probabilities	O
of	O
each	O
class	O
and	O
then	O
computes	O
the	O
posterior	O
probability	O
for	O
each	O
class	O
using	O
pyx	O
pc	O
pypxy	O
pccpxcc	O
the	O
alternative	O
approach	O
which	O
we	O
call	O
the	O
discriminative	B
approach	I
focusses	O
on	O
modelling	O
pyx	O
directly	O
dawid	O
calls	O
the	O
generative	O
and	O
discriminative	O
approaches	O
the	O
sampling	O
and	O
diagnostic	O
paradigms	O
respectively	O
to	O
turn	O
both	O
the	O
generative	O
and	O
discriminative	O
approaches	O
into	O
practical	O
methods	O
we	O
will	O
need	O
to	O
create	O
models	O
for	O
either	O
pxy	O
or	O
pyx	O
these	O
could	O
either	O
be	O
of	O
parametric	B
form	O
or	O
non-parametric	B
models	O
such	O
as	O
those	O
based	O
on	O
nearest	O
neighbours	O
for	O
the	O
generative	O
case	O
a	O
simple	O
com	O
least-squares	B
classification	B
see	O
section	O
that	O
the	O
important	O
distinction	O
is	O
between	O
gaussian	O
and	O
non-gaussian	B
likelihoods	O
regression	O
with	O
a	O
non-gaussian	B
likelihood	B
requires	O
a	O
similar	O
treatment	O
but	O
since	O
classification	B
defines	O
an	O
important	O
conceptual	O
and	O
application	O
area	O
we	O
have	O
chosen	O
to	O
treat	O
it	O
in	O
a	O
separate	O
chapter	O
for	O
non-gaussian	B
likelihoods	O
in	O
general	O
see	O
section	O
the	O
generative	B
approach	I
inference	O
for	O
py	O
is	O
generally	O
straightforward	O
being	O
estimation	O
of	O
a	O
binomial	O
probability	O
in	O
the	O
binary	B
case	O
or	O
a	O
multinomial	O
probability	O
in	O
the	O
multi-class	B
case	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
problems	O
discriminative	O
model	O
example	O
response	B
function	B
probit	B
regression	I
generative	O
or	O
discriminative	O
missing	O
values	O
mon	O
choice	O
would	O
be	O
to	O
model	O
the	O
class-conditional	O
densities	O
with	O
gaussians	O
pxcc	O
n	O
c	O
c	O
a	O
bayesian	O
treatment	O
can	O
be	O
obtained	O
by	O
placing	O
appropriate	O
priors	O
on	O
the	O
mean	O
and	O
covariance	B
of	O
each	O
of	O
the	O
gaussians	O
however	O
note	O
that	O
this	O
gaussian	O
model	O
makes	O
a	O
strong	O
assumption	O
on	O
the	O
form	O
of	O
classconditional	O
density	O
and	O
if	O
this	O
is	O
inappropriate	O
the	O
model	O
may	O
perform	O
poorly	O
for	O
the	O
binary	B
discriminative	O
case	O
one	O
simple	O
idea	O
is	O
to	O
turn	O
the	O
output	O
of	O
a	O
regression	O
model	O
into	O
a	O
class	O
probability	O
using	O
a	O
response	B
function	B
inverse	O
of	O
a	O
link	B
function	B
which	O
squashes	O
its	O
argument	O
which	O
can	O
lie	O
in	O
the	O
domain	O
into	O
the	O
range	O
guaranteeing	O
a	O
valid	O
probabilistic	B
interpretation	O
one	O
example	O
is	O
the	O
linear	B
logistic	B
regression	I
model	O
where	O
exp	O
z	O
standard	O
normal	O
distribution	O
z	O
which	O
combines	O
the	O
linear	B
model	O
with	O
the	O
logistic	B
response	B
function	B
another	O
common	O
choice	O
of	O
response	B
function	B
is	O
the	O
cumulative	O
density	O
function	B
of	O
a	O
n	O
this	O
approach	O
is	O
known	O
as	O
probit	B
regression	I
just	O
as	O
we	O
gave	O
a	O
bayesian	O
approach	O
to	O
linear	B
regression	I
in	O
chapter	O
we	O
can	O
take	O
a	O
parallel	O
approach	O
to	O
logistic	B
regression	I
as	O
discussed	O
in	O
section	O
as	O
in	O
the	O
regression	O
case	O
this	O
model	O
is	O
an	O
important	O
step	O
towards	O
the	O
gaussian	B
process	I
classifier	O
given	O
that	O
there	O
are	O
the	O
generative	O
and	O
discriminative	O
approaches	O
which	O
one	O
should	O
we	O
prefer	O
this	O
is	O
perhaps	O
the	O
biggest	O
question	O
in	O
classification	B
and	O
we	O
do	O
not	O
believe	O
that	O
there	O
is	O
a	O
right	O
answer	O
as	O
both	O
ways	O
of	O
writing	O
the	O
joint	B
py	O
x	O
are	O
correct	O
however	O
it	O
is	O
possible	O
to	O
identify	O
some	O
strengths	O
and	O
weaknesses	O
of	O
the	O
two	O
approaches	O
the	O
discriminative	B
approach	I
is	O
appealing	O
in	O
that	O
it	O
is	O
directly	O
modelling	O
what	O
we	O
want	O
pyx	O
also	O
density	O
estimation	O
for	O
the	O
class-conditional	O
distributions	O
is	O
a	O
hard	O
problem	O
particularly	O
when	O
x	O
is	O
high	O
dimensional	O
so	O
if	O
we	O
are	O
just	O
interested	O
in	O
classification	B
then	O
the	O
generative	B
approach	I
may	O
mean	O
that	O
we	O
are	O
trying	O
to	O
solve	O
a	O
harder	O
problem	O
than	O
we	O
need	O
to	O
however	O
to	O
deal	O
with	O
missing	O
input	O
values	O
outliers	O
and	O
unlabelled	O
data	O
points	O
in	O
a	O
principled	O
fashion	O
it	O
is	O
very	O
helpful	O
to	O
have	O
access	O
to	O
px	O
and	O
this	O
can	O
be	O
obtained	O
from	O
marginalizing	O
out	O
the	O
class	O
label	O
y	O
from	O
the	O
joint	B
y	O
pypxy	O
in	O
the	O
generative	B
approach	I
a	O
further	O
factor	O
in	O
the	O
choice	O
of	O
a	O
generative	O
or	O
discriminative	B
approach	I
could	O
also	O
be	O
which	O
one	O
is	O
most	O
conducive	O
to	O
the	O
incorporation	O
of	O
any	O
prior	O
information	O
which	O
is	O
available	O
see	O
ripley	O
sec	O
for	O
further	O
discussion	O
of	O
these	O
issues	O
the	O
gaussian	B
process	I
classifiers	O
developed	O
in	O
this	O
chapter	O
are	O
discriminative	O
as	O
px	O
p	O
decision	O
theory	O
for	O
classification	B
the	O
classifiers	O
described	O
above	O
provide	O
predictive	B
probabilities	O
py	O
for	O
a	O
test	O
input	O
x	O
however	O
sometimes	O
one	O
actually	O
needs	O
to	O
make	O
a	O
decision	O
and	O
to	O
do	O
this	O
we	O
need	O
to	O
consider	O
decision	O
theory	O
decision	O
theory	O
for	O
the	O
regression	O
problem	O
was	O
considered	O
in	O
section	O
here	O
we	O
discuss	O
decision	O
theory	O
for	O
classification	B
problems	O
a	O
comprehensive	O
treatment	O
of	O
decision	O
theory	O
can	O
be	O
found	O
in	O
berger	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
loss	B
risk	B
zero-one	B
loss	B
asymmetric	O
loss	B
bayes	B
classifier	I
decision	O
regions	O
reject	B
option	I
risk	B
minimization	O
classification	B
given	O
x	O
is	O
let	O
lc	O
be	O
the	O
loss	B
incurred	O
by	O
making	O
decision	O
if	O
the	O
true	O
class	O
is	O
cc	O
usually	O
lc	O
c	O
for	O
all	O
c	O
the	O
expected	O
risk	B
of	O
taking	O
decision	O
c	O
lc	O
and	O
the	O
optimal	B
decision	O
c	O
is	O
the	O
one	O
that	O
minimizes	O
one	O
common	O
choice	O
of	O
loss	B
function	B
is	O
the	O
zero-one	B
loss	B
where	O
a	O
penalty	O
of	O
one	O
unit	O
is	O
paid	O
for	O
an	O
incorrect	O
classification	B
and	O
for	O
a	O
correct	O
one	O
in	O
this	O
case	O
the	O
optimal	B
decision	O
rule	O
is	O
to	O
choose	O
the	O
class	O
cc	O
that	O
pccx	O
as	O
this	O
minimizes	O
the	O
expected	O
error	O
at	O
x	O
however	O
the	O
zero-one	B
loss	B
is	O
not	O
always	O
appropriate	O
a	O
classic	O
example	O
of	O
this	O
is	O
the	O
difference	O
in	O
loss	B
of	O
failing	O
to	O
spot	O
a	O
disease	O
when	O
carrying	O
out	O
a	O
medical	O
test	O
compared	O
to	O
the	O
cost	O
of	O
a	O
false	O
positive	O
on	O
the	O
test	O
so	O
that	O
lc	O
c	O
the	O
optimal	B
classifier	O
zero-one	B
loss	B
is	O
known	O
as	O
the	O
bayes	B
classifier	I
by	O
this	O
construction	O
the	O
feature	B
space	I
is	O
divided	O
into	O
decision	O
regions	O
such	O
that	O
a	O
pattern	O
falling	O
in	O
decision	B
region	I
rc	O
is	O
assigned	O
to	O
class	O
cc	O
can	O
be	O
more	O
than	O
one	O
decision	B
region	I
corresponding	O
to	O
a	O
single	O
class	O
the	O
boundaries	O
between	O
the	O
decision	O
regions	O
are	O
known	O
as	O
decision	O
surfaces	O
or	O
decision	O
boundaries	O
one	O
would	O
expect	O
misclassification	O
errors	O
to	O
occur	O
in	O
regions	O
where	O
the	O
maximum	O
class	O
probability	O
maxj	O
pcjx	O
is	O
relatively	O
low	O
this	O
could	O
be	O
due	O
to	O
either	O
a	O
region	O
of	O
strong	O
overlap	O
between	O
classes	O
or	O
lack	O
of	O
training	O
examples	O
within	O
this	O
region	O
thus	O
one	O
sensible	O
strategy	O
is	O
to	O
add	O
a	O
reject	B
option	I
so	O
that	O
if	O
maxj	O
pcjx	O
for	O
a	O
threshold	O
in	O
then	O
we	O
go	O
ahead	O
and	O
classify	O
the	O
pattern	O
otherwise	O
we	O
reject	O
it	O
and	O
leave	O
the	O
classification	B
task	O
to	O
a	O
more	O
sophisticated	O
system	O
for	O
multi-class	B
classification	B
we	O
could	O
alternatively	O
require	O
the	O
gap	O
between	O
the	O
most	O
probable	O
and	O
the	O
second	O
most	O
probable	O
class	O
to	O
exceed	O
and	O
otherwise	O
reject	O
as	O
is	O
varied	O
from	O
to	O
one	O
obtains	O
an	O
errorreject	O
curve	O
plotting	O
the	O
percentage	O
of	O
patterns	O
classified	O
incorrectly	O
against	O
the	O
percentage	O
rejected	O
typically	O
the	O
error	O
rate	O
will	O
fall	O
as	O
the	O
rejection	O
rate	O
increases	O
hansen	O
et	O
al	O
provide	O
an	O
analysis	O
of	O
the	O
error-reject	O
trade-off	O
we	O
have	O
focused	O
above	O
on	O
the	O
probabilistic	B
approach	O
to	O
classification	B
which	O
involves	O
a	O
two-stage	O
approach	O
of	O
first	O
computing	O
a	O
posterior	O
distribution	O
over	O
functions	O
and	O
then	O
combining	O
this	O
with	O
the	O
loss	B
function	B
to	O
produce	O
a	O
decision	O
however	O
it	O
is	O
worth	O
noting	O
that	O
some	O
authors	O
argue	O
that	O
if	O
our	O
goal	O
is	O
to	O
eventually	O
make	O
a	O
decision	O
then	O
we	O
should	O
aim	O
to	O
approximate	O
the	O
classification	B
function	B
that	O
minimizes	O
the	O
risk	B
loss	B
which	O
is	O
defined	O
as	O
z	O
x	O
dydx	O
rlc	O
where	O
py	O
x	O
is	O
the	O
joint	B
distribution	O
of	O
inputs	O
and	O
targets	O
and	O
cx	O
is	O
a	O
classification	B
function	B
that	O
assigns	O
an	O
input	O
pattern	O
x	O
to	O
one	O
of	O
c	O
classes	O
pn	O
e	O
g	O
vapnik	O
ch	O
as	O
py	O
x	O
is	O
unknown	O
in	O
this	O
approach	O
one	O
often	O
then	O
seeks	O
to	O
minimize	O
an	O
objective	O
function	B
which	O
includes	O
the	O
empirical	O
risk	B
lyi	O
cxi	O
as	O
well	O
as	O
a	O
regularization	B
term	O
while	O
this	O
is	O
a	O
reasonable	O
economics	O
one	O
usually	O
talks	O
of	O
maximizing	O
expected	O
utility	O
rather	O
than	O
minimizing	O
expected	O
loss	B
loss	B
is	O
negative	O
utility	O
this	O
suggests	O
that	O
statisticians	O
are	O
pessimists	O
while	O
economists	O
are	O
optimists	O
more	O
than	O
one	O
class	O
has	O
equal	O
posterior	O
probability	O
then	O
ties	O
can	O
be	O
broken	O
arbitrarily	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
linear	B
models	O
for	O
classification	B
method	O
we	O
note	O
that	O
the	O
probabilistic	B
approach	O
allows	O
the	O
same	O
inference	O
stage	O
to	O
be	O
re-used	O
with	O
different	O
loss	B
functions	O
it	O
can	O
help	O
us	O
to	O
incorporate	O
prior	O
knowledge	O
on	O
the	O
function	B
andor	O
noise	B
model	I
and	O
has	O
the	O
advantage	O
of	O
giving	O
probabilistic	B
predictions	O
which	O
can	O
be	O
helpful	O
e	O
g	O
for	O
the	O
reject	B
option	I
linear	B
models	O
for	O
classification	B
in	O
this	O
section	O
we	O
briefly	O
review	O
linear	B
models	O
for	O
binary	B
classification	B
which	O
form	O
the	O
foundation	O
of	O
gaussian	B
process	I
classification	B
models	O
in	O
the	O
next	O
section	O
we	O
follow	O
the	O
svm	O
literature	O
and	O
use	O
the	O
labels	O
y	O
and	O
y	O
to	O
distinguish	O
the	O
two	O
classes	O
although	O
for	O
the	O
multi-class	B
case	O
in	O
section	O
we	O
use	O
labels	O
the	O
likelihood	B
is	O
py	O
w	O
given	O
the	O
weight	B
vector	I
w	O
and	O
can	O
be	O
any	O
function	B
when	O
using	O
the	O
logistic	B
from	O
eq	O
the	O
model	O
is	O
usually	O
called	O
simply	O
logistic	B
regression	I
but	O
to	O
emphasize	O
the	O
parallels	O
to	O
linear	B
regression	I
we	O
prefer	O
the	O
term	O
linear	B
logistic	B
regression	I
when	O
using	O
the	O
cumulative	O
gaussian	O
we	O
call	O
the	O
model	O
linear	B
probit	B
regression	I
as	O
the	O
probability	O
of	O
the	O
two	O
classes	O
must	O
sum	O
to	O
we	O
have	O
py	O
w	O
py	O
w	O
thus	O
for	O
a	O
data	O
point	O
yi	O
the	O
likelihood	B
is	O
given	O
by	O
i	O
w	O
if	O
yi	O
for	O
symmetric	O
likelihood	B
i	O
w	O
if	O
yi	O
and	O
functions	O
such	O
as	O
the	O
logistic	B
or	O
probit	B
where	O
z	O
this	O
can	O
be	O
written	O
more	O
concisely	O
as	O
pyixi	O
w	O
we	O
see	O
that	O
the	O
logistic	B
regression	I
model	O
can	O
be	O
where	O
fi	O
fxi	O
x	O
i	O
w	O
defining	O
the	O
logit	O
transformation	O
as	O
logitx	O
written	O
as	O
logitx	O
xw	O
the	O
logitx	O
function	B
is	O
also	O
called	O
the	O
log	B
odds	I
ratio	I
generalized	B
linear	B
modelling	O
and	O
nelder	O
deals	O
with	O
the	O
issue	O
of	O
extending	O
linear	B
models	O
to	O
non-gaussian	B
data	O
scenarios	O
the	O
logit	O
transformation	O
is	O
the	O
canonical	B
link	B
function	B
for	O
binary	B
data	O
and	O
this	O
choice	O
simplifies	O
the	O
algebra	O
and	O
algorithms	O
given	O
a	O
dataset	O
d	O
yii	O
n	O
we	O
assume	O
that	O
the	O
labels	O
are	O
generated	O
independently	O
conditional	B
on	O
fx	O
using	O
the	O
same	O
gaussian	O
prior	O
w	O
n	O
p	O
as	O
for	O
regression	O
in	O
eq	O
we	O
then	O
obtain	O
the	O
un-normalized	O
log	O
posterior	O
nx	O
log	O
pwx	O
y	O
c	O
w	O
p	O
w	O
log	O
in	O
the	O
linear	B
regression	I
case	O
with	O
gaussian	O
noise	O
the	O
posterior	O
was	O
gaussian	O
with	O
mean	O
and	O
covariance	B
as	O
given	O
in	O
eq	O
for	O
classification	B
the	O
posterior	O
sigmoid	O
function	B
is	O
a	O
monotonically	O
increasing	O
function	B
mapping	O
from	O
r	O
to	O
it	O
derives	O
its	O
name	O
from	O
being	O
shaped	O
like	O
a	O
letter	O
s	O
linear	B
logistic	B
regression	I
linear	B
probit	B
regression	I
concise	O
notation	O
logit	O
log	B
odds	I
ratio	I
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
concavity	O
unique	O
maximum	O
irls	O
algorithm	O
properties	O
of	O
maximum	O
likelihood	B
classification	B
does	O
not	O
have	O
a	O
simple	O
analytic	O
form	O
however	O
it	O
is	O
easy	O
to	O
show	O
that	O
for	O
some	O
sigmoid	O
functions	O
such	O
as	O
the	O
logistic	B
and	O
cumulative	O
gaussian	O
the	O
log	O
likelihood	B
is	O
a	O
concave	O
function	B
of	O
w	O
for	O
fixed	O
d	O
as	O
the	O
quadratic	O
penalty	O
on	O
w	O
is	O
also	O
concave	O
then	O
the	O
log	O
posterior	O
is	O
a	O
concave	O
function	B
which	O
means	O
that	O
it	O
is	O
relatively	O
easy	O
to	O
find	O
its	O
unique	O
maximum	O
the	O
concavity	O
can	O
also	O
be	O
derived	O
from	O
the	O
fact	O
that	O
the	O
hessian	O
of	O
log	O
pwx	O
y	O
is	O
negative	O
definite	O
section	O
for	O
further	O
details	O
the	O
standard	O
algorithm	O
for	O
finding	O
the	O
maximum	O
is	O
newton	O
s	O
method	O
which	O
in	O
this	O
context	O
is	O
usually	O
called	O
the	O
iteratively	B
reweighted	I
least	I
squares	I
algorithm	O
as	O
described	O
e	O
g	O
in	O
mccullagh	O
and	O
nelder	O
however	O
note	O
that	O
minka	O
provides	O
evidence	O
that	O
other	O
optimization	O
methods	O
conjugate	O
gradient	O
ascent	O
may	O
be	O
faster	O
than	O
irls	O
notice	O
that	O
a	O
maximum	O
likelihood	B
treatment	O
to	O
an	O
unpenalized	O
version	O
of	O
eq	O
may	O
result	O
in	O
some	O
undesirable	O
outcomes	O
if	O
the	O
dataset	O
is	O
linearly	O
separable	O
if	O
there	O
exists	O
a	O
hyperplane	O
which	O
separates	O
the	O
positive	O
and	O
negative	O
examples	O
then	O
maximizing	O
the	O
likelihood	B
will	O
cause	O
to	O
tend	O
to	O
infinity	O
however	O
this	O
will	O
still	O
give	O
predictions	O
in	O
for	O
py	O
w	O
although	O
these	O
predictions	O
will	O
be	O
hard	O
zero	O
or	O
one	O
if	O
the	O
problem	O
is	O
ill-conditioned	O
e	O
g	O
due	O
to	O
duplicate	O
linearly	O
dependent	O
input	O
dimensions	O
there	O
will	O
be	O
no	O
unique	O
solution	O
predictions	O
softmax	B
multiple	O
logistic	B
as	O
an	O
example	O
consider	O
linear	B
logistic	B
regression	I
in	O
the	O
case	O
where	O
x-space	O
is	O
two	O
dimensional	O
and	O
there	O
is	O
no	O
bias	O
weight	O
so	O
that	O
w	O
is	O
also	O
two-dimensional	O
the	O
prior	O
in	O
weight	O
space	O
is	O
gaussian	O
and	O
for	O
simplicity	O
we	O
have	O
set	B
p	O
i	O
contours	O
of	O
the	O
prior	O
pw	O
are	O
illustrated	O
in	O
figure	O
if	O
we	O
have	O
a	O
data	O
set	B
d	O
as	O
shown	O
in	O
figure	O
then	O
this	O
induces	O
a	O
posterior	O
distribution	O
in	O
weight	O
space	O
as	O
shown	O
in	O
figure	O
notice	O
that	O
the	O
posterior	O
is	O
non-gaussian	B
and	O
unimodal	O
as	O
expected	O
the	O
dataset	O
is	O
not	O
linearly	O
separable	O
but	O
a	O
weight	B
vector	I
in	O
the	O
direction	O
is	O
clearly	O
a	O
reasonable	O
choice	O
as	O
the	O
posterior	O
distribution	O
shows	O
to	O
make	O
predictions	O
based	O
the	O
training	O
set	B
d	O
for	O
a	O
test	O
point	O
x	O
we	O
have	O
py	O
py	O
x	O
dw	O
z	O
integrating	O
the	O
prediction	O
py	O
x	O
w	O
over	O
the	O
posterior	O
distribution	O
of	O
weights	O
this	O
leads	O
to	O
contours	O
of	O
the	O
predictive	B
distribution	O
as	O
shown	O
in	O
figure	O
notice	O
how	O
the	O
contours	O
are	O
bent	O
reflecting	O
the	O
integration	O
of	O
many	O
different	O
but	O
plausible	O
w	O
s	O
in	O
the	O
multi-class	B
case	O
we	O
use	O
the	O
multiple	O
logistic	B
softmax	B
function	B
where	O
wc	O
is	O
the	O
weight	B
vector	I
for	O
class	O
c	O
and	O
all	O
weight	O
vectors	O
are	O
collected	O
into	O
the	O
matrix	O
w	O
the	O
corresponding	O
log	O
likelihood	B
is	O
of	O
the	O
form	O
i	O
as	O
in	O
the	O
binary	B
case	O
the	O
log	O
i	O
wc	O
logp	O
pc	O
cyix	O
expx	O
pn	O
likelihood	B
is	O
a	O
concave	O
function	B
of	O
w	O
it	O
is	O
interesting	O
to	O
note	O
that	O
in	O
a	O
generative	B
approach	I
where	O
the	O
classconditional	O
distributions	O
pxy	O
are	O
gaussian	O
with	O
the	O
same	O
covariance	B
matrix	I
py	O
ccx	O
w	O
p	O
expxwc	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	B
process	I
classification	B
figure	O
linear	B
logistic	B
regression	I
panel	O
shows	O
contours	O
of	O
the	O
prior	O
distribution	O
pw	O
n	O
i	O
panel	O
shows	O
the	O
dataset	O
with	O
circles	O
indicating	O
class	O
and	O
crosses	O
denoting	O
class	O
panel	O
shows	O
contours	O
of	O
the	O
posterior	O
distribution	O
pwd	O
panel	O
shows	O
contours	O
of	O
the	O
predictive	B
distribution	O
py	O
pyx	O
has	O
the	O
form	O
given	O
by	O
eq	O
and	O
eq	O
for	O
the	O
two-	O
and	O
multi-class	B
cases	O
respectively	O
the	O
constant	O
function	B
is	O
included	O
in	O
x	O
gaussian	B
process	I
classification	B
for	O
binary	B
classification	B
the	O
basic	O
idea	O
behind	O
gaussian	B
process	I
prediction	O
is	O
very	O
simple	O
we	O
place	O
a	O
gp	O
prior	O
over	O
the	O
latent	O
function	B
fx	O
and	O
then	O
squash	O
this	O
through	O
the	O
logistic	B
function	B
to	O
obtain	O
a	O
prior	O
on	O
py	O
note	O
that	O
is	O
a	O
deterministic	O
function	B
of	O
f	O
and	O
since	O
f	O
is	O
stochastic	O
so	O
is	O
this	O
construction	O
is	O
illustrated	O
in	O
figure	O
for	O
a	O
oneit	O
is	O
a	O
natural	O
generalization	B
of	O
the	O
linear	B
logistic	B
dimensional	O
input	O
space	O
latent	O
function	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
figure	O
panel	O
shows	O
a	O
sample	O
latent	O
function	B
f	O
drawn	O
from	O
a	O
gaussian	B
process	I
as	O
a	O
function	B
of	O
x	O
panel	O
shows	O
the	O
result	O
of	O
squashing	O
this	O
sample	O
function	B
through	O
the	O
logistic	B
logit	O
function	B
exp	O
z	O
to	O
obtain	O
the	O
class	O
probability	O
regression	O
model	O
and	O
parallels	O
the	O
development	O
from	O
linear	B
regression	I
to	O
gp	O
regression	O
that	O
we	O
explored	O
in	O
section	O
specifically	O
we	O
replace	O
the	O
linear	B
fx	O
function	B
from	O
the	O
linear	B
logistic	B
model	O
in	O
eq	O
by	O
a	O
gaussian	B
process	I
and	O
correspondingly	O
the	O
gaussian	O
prior	O
on	O
the	O
weights	O
by	O
a	O
gp	O
prior	O
the	O
latent	O
function	B
f	O
plays	O
the	O
r	O
ole	O
of	O
a	O
nuisance	O
function	B
we	O
do	O
not	O
observe	O
values	O
of	O
f	O
itself	O
observe	O
only	O
the	O
inputs	O
x	O
and	O
the	O
class	O
labels	O
y	O
and	O
we	O
are	O
not	O
particularly	O
interested	O
in	O
the	O
values	O
of	O
f	O
but	O
rather	O
in	O
in	O
particular	O
for	O
test	O
cases	O
the	O
purpose	O
of	O
f	O
is	O
solely	O
to	O
allow	O
a	O
convenient	O
formulation	O
of	O
the	O
model	O
and	O
the	O
computational	O
goal	O
pursued	O
in	O
the	O
coming	O
sections	O
will	O
be	O
to	O
remove	O
out	O
f	O
we	O
have	O
tacitly	O
assumed	O
that	O
the	O
latent	O
gaussian	B
process	I
is	O
noise-free	O
and	O
combined	O
it	O
with	O
smooth	O
likelihood	B
functions	O
such	O
as	O
the	O
logistic	B
or	O
probit	B
however	O
one	O
can	O
equivalently	O
think	O
of	O
adding	O
independent	O
noise	O
to	O
the	O
latent	O
process	O
in	O
combination	O
with	O
a	O
step-function	O
likelihood	B
in	O
particular	O
assuming	O
gaussian	O
noise	O
and	O
a	O
step-function	O
likelihood	B
is	O
exactly	O
equivalent	B
to	O
a	O
latent	O
process	O
and	O
probit	B
likelihood	B
see	O
exercise	O
nuisance	O
function	B
noise-free	O
latent	O
process	O
inference	O
is	O
naturally	O
divided	O
into	O
two	O
steps	O
first	O
computing	O
the	O
distribution	O
of	O
the	O
latent	O
variable	O
corresponding	O
to	O
a	O
test	O
case	O
pf	O
y	O
x	O
pf	O
x	O
fpfx	O
y	O
df	O
z	O
z	O
where	O
pfx	O
y	O
pyfpfxpyx	O
is	O
the	O
posterior	O
over	O
the	O
latent	O
variables	O
and	O
subsequently	O
using	O
this	O
distribution	O
over	O
the	O
latent	O
f	O
to	O
produce	O
a	O
probabilistic	B
prediction	O
py	O
y	O
x	O
y	O
x	O
df	O
equivalence	O
explains	O
why	O
no	O
numerical	O
problems	O
arise	O
from	O
considering	O
a	O
noise-free	O
process	O
if	O
care	O
is	O
taken	O
with	O
the	O
implementation	O
see	O
also	O
comment	O
at	O
the	O
end	O
of	O
section	O
xlatent	O
function	B
xclass	O
probability	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
gp	O
classifier	O
in	O
the	O
regression	O
case	O
gaussian	O
likelihood	B
computation	O
of	O
predictions	O
was	O
straightforward	O
as	O
the	O
relevant	O
integrals	B
were	O
gaussian	O
and	O
could	O
be	O
computed	O
analytically	O
in	O
classification	B
the	O
non-gaussian	B
likelihood	B
in	O
eq	O
makes	O
the	O
integral	O
analytically	O
intractable	O
similarly	O
eq	O
can	O
be	O
intractable	O
analytically	O
for	O
certain	O
sigmoid	O
functions	O
although	O
in	O
the	O
binary	B
case	O
it	O
is	O
only	O
a	O
one-dimensional	O
integral	O
so	O
simple	O
numerical	O
techniques	O
are	O
generally	O
adequate	O
thus	O
we	O
need	O
to	O
use	O
either	O
analytic	O
approximations	O
of	O
integrals	B
or	O
solutions	O
based	O
on	O
monte	O
carlo	O
sampling	O
in	O
the	O
coming	O
sections	O
we	O
describe	O
two	O
analytic	O
approximations	O
which	O
both	O
approximate	O
the	O
non-gaussian	B
joint	B
posterior	O
with	O
a	O
gaussian	O
one	O
the	O
first	O
is	O
the	O
straightforward	O
laplace	B
approximation	I
method	O
and	O
barber	O
and	O
the	O
second	O
is	O
the	O
more	O
sophisticated	O
expectation	B
propagation	I
method	O
due	O
to	O
minka	O
cavity	O
tap	O
approximation	O
of	O
opper	O
and	O
winther	O
is	O
closely	O
related	O
to	O
the	O
ep	O
method	O
a	O
number	O
of	O
other	O
approximations	O
have	O
also	O
been	O
suggested	O
see	O
e	O
g	O
gibbs	B
and	O
mackay	O
jaakkola	O
and	O
haussler	O
and	O
seeger	B
neal	O
describes	O
the	O
use	O
of	O
markov	B
chain	I
monte	I
carlo	I
approximations	O
all	O
of	O
these	O
methods	O
will	O
typically	O
scale	O
as	O
for	O
large	O
datasets	O
there	O
has	O
been	O
much	O
work	O
on	O
further	O
approximations	O
to	O
reduce	O
computation	O
time	O
as	O
discussed	O
in	O
chapter	O
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
case	O
is	O
described	O
in	O
section	O
and	O
for	O
the	O
multi-class	B
case	O
in	O
section	O
the	O
ep	O
method	O
for	O
binary	B
classification	B
is	O
described	O
in	O
section	O
relationships	O
between	O
gaussian	B
process	I
classifiers	O
and	O
other	O
techniques	O
such	O
as	O
spline	O
classifiers	O
support	B
vector	I
machines	O
and	O
least-squares	B
classification	B
are	O
discussed	O
in	O
sections	O
and	O
respectively	O
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
gp	O
classifier	O
laplace	O
s	O
method	O
utilizes	O
a	O
gaussian	O
approximation	O
qfx	O
y	O
to	O
the	O
posterior	O
pfx	O
y	O
in	O
the	O
integral	O
doing	O
a	O
second	O
order	O
taylor	O
expansion	O
of	O
log	O
pfx	O
y	O
around	O
the	O
maximum	O
of	O
the	O
posterior	O
we	O
obtain	O
a	O
gaussian	O
approximation	O
qfx	O
y	O
n	O
f	O
a	O
faf	O
where	O
f	O
argmaxf	O
pfx	O
y	O
and	O
a	O
log	O
pfx	O
yf	O
f	O
is	O
the	O
hessian	O
of	O
the	O
negative	O
log	O
posterior	O
at	O
that	O
point	O
the	O
structure	O
of	O
the	O
rest	O
of	O
this	O
section	O
is	O
as	O
follows	O
in	O
section	O
we	O
describe	O
how	O
to	O
find	O
f	O
and	O
a	O
section	O
explains	O
how	O
to	O
make	O
predictions	O
having	O
obtained	O
qfy	O
and	O
section	O
gives	O
more	O
implementation	O
details	O
for	O
the	O
laplace	O
gp	O
classifier	O
the	O
laplace	B
approximation	I
for	O
the	O
marginal	B
likelihood	B
is	O
described	O
in	O
section	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
logistic	B
probit	B
figure	O
the	O
log	O
likelihood	B
and	O
its	O
derivatives	O
for	O
a	O
single	O
case	O
as	O
a	O
function	B
of	O
zi	O
yifi	O
for	O
the	O
logistic	B
and	O
the	O
cumulative	O
gaussian	O
likelihood	B
the	O
two	O
likelihood	B
functions	O
are	O
fairly	O
similar	O
the	O
main	O
qualitative	O
difference	O
being	O
that	O
for	O
large	O
negative	O
arguments	O
the	O
log	O
logistic	B
behaves	O
linearly	O
whereas	O
the	O
log	O
cumulative	O
gaussian	O
has	O
a	O
quadratic	O
penalty	O
both	O
likelihoods	O
are	O
log	O
concave	O
posterior	O
by	O
bayes	O
rule	O
the	O
posterior	O
over	O
the	O
latent	O
variables	O
is	O
given	O
by	O
pfx	O
y	O
pyfpfxpyx	O
but	O
as	O
pyx	O
is	O
independent	O
of	O
f	O
we	O
need	O
only	O
consider	O
the	O
un-normalized	O
posterior	O
when	O
maximizing	O
w	O
r	O
t	O
f	O
taking	O
the	O
logarithm	O
and	O
introducing	O
expression	O
eq	O
for	O
the	O
gp	O
prior	O
gives	O
log	O
pyf	O
log	O
pfx	O
log	O
pyf	O
fk	O
log	O
n	O
log	O
differentiating	O
eq	O
w	O
r	O
t	O
f	O
we	O
obtain	O
log	O
pyf	O
k	O
log	O
pyf	O
k	O
w	O
k	O
where	O
w	O
log	O
pyf	O
is	O
diagonal	O
since	O
the	O
likelihood	B
factorizes	O
over	O
cases	O
distribution	O
for	O
yi	O
depends	O
only	O
on	O
fi	O
not	O
on	O
note	O
that	O
if	O
the	O
likelihood	B
pyf	O
is	O
log	O
concave	O
the	O
diagonal	O
elements	O
of	O
w	O
are	O
non-negative	O
and	O
the	O
hessian	O
in	O
eq	O
is	O
negative	O
definite	O
so	O
that	O
is	O
concave	O
and	O
has	O
a	O
unique	O
maximum	O
section	O
for	O
further	O
details	O
there	O
are	O
many	O
possible	O
functional	B
forms	O
of	O
the	O
likelihood	B
which	O
gives	O
the	O
target	O
class	O
probability	O
as	O
a	O
function	B
of	O
the	O
latent	O
variable	O
f	O
two	O
commonly	O
used	O
likelihood	B
functions	O
are	O
the	O
logistic	B
and	O
the	O
cumulative	O
gaussian	O
see	O
figure	O
the	O
expressions	O
for	O
the	O
log	O
likelihood	B
for	O
these	O
likelihood	B
functions	O
and	O
their	O
first	O
and	O
second	O
derivatives	O
w	O
r	O
t	O
the	O
latent	O
variable	O
are	O
given	O
in	O
the	O
un-normalized	O
posterior	O
log	O
likelihoods	O
and	O
their	O
derivatives	O
times	O
target	O
ziyifi	O
log	O
likelihood	B
log	O
pyifilog	O
derivative	O
times	O
target	O
ziyifi	O
log	O
likelihood	B
log	O
pyifilog	O
derivative	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
gp	O
classifier	O
following	O
table	O
log	O
pyifi	O
exp	O
fi	O
log	O
pyifi	O
ti	O
i	O
yin	O
log	O
pyifi	O
f	O
i	O
i	O
log	O
where	O
we	O
have	O
defined	O
i	O
pyi	O
and	O
t	O
at	O
the	O
maximum	O
of	O
we	O
have	O
n	O
yifin	O
f	O
log	O
py	O
as	O
a	O
self-consistent	O
equation	O
for	O
f	O
since	O
log	O
py	O
f	O
is	O
a	O
non-linear	O
function	B
of	O
f	O
eq	O
cannot	O
be	O
solved	O
directly	O
to	O
find	O
the	O
maximum	O
of	O
we	O
use	O
newton	O
s	O
method	O
with	O
the	O
iteration	O
f	O
new	O
f	O
f	O
w	O
log	O
pyf	O
k	O
w	O
f	O
log	O
to	O
gain	O
more	O
intuition	O
about	O
this	O
update	O
let	O
us	O
consider	O
what	O
happens	O
to	O
datapoints	O
that	O
are	O
well-explained	O
under	O
f	O
so	O
that	O
log	O
pyifi	O
fi	O
and	O
wii	O
are	O
close	O
to	O
zero	O
for	O
these	O
points	O
as	O
an	O
approximation	O
break	O
f	O
into	O
two	O
subvectors	O
that	O
corresponds	O
to	O
points	O
that	O
are	O
not	O
well-explained	O
and	O
to	O
those	O
that	O
are	O
then	O
it	O
is	O
easy	O
to	O
show	O
exercise	O
that	O
log	O
f	O
new	O
f	O
new	O
f	O
new	O
where	O
denotes	O
the	O
block	O
of	O
k	O
containing	O
the	O
covariance	B
between	O
is	O
computed	O
by	O
ignoring	O
the	O
two	O
groups	O
of	O
points	O
etc	O
this	O
means	O
that	O
f	O
new	O
using	O
the	O
entirely	O
the	O
well-explained	O
points	O
and	O
f	O
new	O
usual	O
gp	O
prediction	O
methods	O
treating	O
these	O
points	O
like	O
test	O
points	O
of	O
course	O
if	O
the	O
predictions	O
of	O
f	O
new	O
fail	O
to	O
match	O
the	O
targets	O
correctly	O
they	O
would	O
cease	O
to	O
be	O
well-explained	O
and	O
so	O
be	O
updated	O
on	O
the	O
next	O
iteration	O
is	O
predicted	O
from	O
f	O
new	O
having	O
found	O
the	O
maximum	O
posterior	O
f	O
we	O
can	O
now	O
specify	O
the	O
laplace	B
approximation	I
to	O
the	O
posterior	O
as	O
a	O
gaussian	O
with	O
mean	O
f	O
and	O
covariance	B
matrix	I
given	O
by	O
the	O
negative	O
inverse	O
hessian	O
of	O
from	O
eq	O
qfx	O
y	O
f	O
w	O
one	O
problem	O
with	O
the	O
laplace	B
approximation	I
is	O
that	O
it	O
is	O
essentially	O
uncontrolled	O
in	O
that	O
the	O
hessian	O
at	O
f	O
may	O
give	O
a	O
poor	O
approximation	O
to	O
the	O
true	O
shape	O
of	O
the	O
posterior	O
the	O
peak	O
could	O
be	O
much	O
broader	O
or	O
narrower	O
than	O
the	O
hessian	O
indicates	O
or	O
it	O
could	O
be	O
a	O
skew	O
peak	O
while	O
the	O
laplace	B
approximation	I
assumes	O
it	O
has	O
elliptical	O
contours	O
newton	O
s	O
method	O
intuition	O
on	O
influence	O
of	O
well-explained	O
points	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
predictions	O
latent	O
mean	O
the	O
posterior	O
mean	O
for	O
f	O
under	O
the	O
laplace	B
approximation	I
can	O
be	O
expressed	O
by	O
combining	O
the	O
gp	O
predictive	B
mean	O
eq	O
with	O
eq	O
into	O
eqf	O
y	O
x	O
kx	O
f	O
kx	O
log	O
py	O
f	O
compare	O
this	O
with	O
the	O
exact	O
mean	O
given	O
by	O
opper	O
and	O
winther	O
as	O
epf	O
y	O
x	O
ef	O
x	O
x	O
ydf	O
kx	O
pfx	O
ydf	O
kx	O
y	O
z	O
z	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
for	O
a	O
gp	O
ef	O
x	O
x	O
kx	O
and	O
have	O
let	O
efx	O
y	O
denote	O
the	O
posterior	O
mean	O
of	O
f	O
given	O
x	O
and	O
y	O
notice	O
the	O
similarity	O
between	O
the	O
middle	O
expression	O
of	O
eq	O
and	O
eq	O
where	O
the	O
exact	O
average	O
efx	O
y	O
has	O
been	O
replaced	O
with	O
the	O
modal	O
value	O
f	O
eqfx	O
y	O
a	O
simple	O
observation	O
from	O
eq	O
is	O
that	O
positive	O
training	O
examples	O
will	O
give	O
rise	O
to	O
a	O
positive	O
coefficient	O
for	O
their	O
kernel	B
function	B
i	O
log	O
pyifi	O
in	O
this	O
case	O
while	O
negative	O
examples	O
will	O
give	O
rise	O
to	O
a	O
negative	O
coefficient	O
this	O
is	O
analogous	O
to	O
the	O
solution	O
to	O
the	O
support	B
vector	I
machine	I
see	O
eq	O
also	O
note	O
that	O
training	O
points	O
which	O
have	O
i	O
log	O
pyifi	O
that	O
are	O
well-explained	O
under	O
f	O
do	O
not	O
contribute	O
strongly	O
to	O
predictions	O
at	O
novel	O
test	O
points	O
this	O
is	O
similar	O
to	O
the	O
behaviour	O
of	O
non-support	O
vectors	O
in	O
the	O
support	B
vector	I
machine	I
section	O
we	O
can	O
also	O
compute	O
vqf	O
y	O
the	O
variance	O
of	O
f	O
y	O
under	O
the	O
gaussian	O
approximation	O
this	O
comprises	O
of	O
two	O
terms	O
i	O
e	O
vqf	O
y	O
x	O
epf	O
ef	O
x	O
eqfxyef	O
x	O
f	O
ef	O
y	O
x	O
the	O
first	O
term	O
is	O
due	O
to	O
the	O
variance	O
of	O
f	O
if	O
we	O
condition	O
on	O
a	O
particular	O
value	O
of	O
f	O
and	O
is	O
given	O
by	O
kx	O
x	O
kx	O
cf	O
eq	O
the	O
second	O
term	O
in	O
eq	O
is	O
due	O
to	O
the	O
fact	O
that	O
ef	O
x	O
f	O
kx	O
depends	O
on	O
f	O
and	O
thus	O
there	O
is	O
an	O
additional	O
term	O
of	O
kx	O
covfx	O
yk	O
under	O
the	O
gaussian	O
approximation	O
covfx	O
y	O
w	O
and	O
thus	O
k	O
w	O
vqf	O
y	O
x	O
kx	O
x	O
k	O
kx	O
x	O
k	O
k	O
k	O
w	O
where	O
the	O
last	O
line	O
is	O
obtained	O
using	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
given	O
the	O
mean	O
and	O
variance	O
of	O
f	O
we	O
make	O
predictions	O
by	O
computing	O
eq	O
y	O
x	O
y	O
x	O
df	O
z	O
sign	O
of	O
kernel	B
coefficients	O
latent	O
variance	O
averaged	B
predictive	B
probability	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
gp	O
classifier	O
map	B
prediction	O
identical	O
binary	B
decisions	O
where	O
qf	O
y	O
x	O
is	O
gaussian	O
with	O
mean	O
and	O
variance	O
given	O
by	O
equations	O
and	O
respectively	O
notice	O
that	O
because	O
of	O
the	O
non-linear	O
form	O
of	O
the	O
sigmoid	O
the	O
predictive	B
probability	O
from	O
eq	O
is	O
different	O
from	O
the	O
sigmoid	O
of	O
the	O
expectation	O
of	O
f	O
we	O
will	O
call	O
the	O
latter	O
the	O
map	B
prediction	O
to	O
distinguish	O
it	O
from	O
the	O
averaged	B
predictions	O
from	O
eq	O
in	O
fact	O
as	O
shown	O
in	O
bishop	O
sec	O
the	O
predicted	O
test	O
labels	O
given	O
by	O
choosing	O
the	O
class	O
of	O
highest	O
probability	O
obtained	O
by	O
averaged	B
and	O
map	B
predictions	O
are	O
identical	O
for	O
binary	B
classification	B
to	O
see	O
this	O
note	O
that	O
the	O
decision	O
boundary	O
using	O
the	O
the	O
map	B
value	O
eqf	O
y	O
x	O
corresponds	O
to	O
y	O
x	O
or	O
eqf	O
y	O
x	O
the	O
decision	O
boundary	O
of	O
the	O
averaged	B
prediction	O
eq	O
y	O
x	O
also	O
corresponds	O
to	O
eqf	O
y	O
x	O
this	O
follows	O
from	O
the	O
fact	O
that	O
is	O
antisymmetric	O
while	O
qf	O
y	O
x	O
is	O
symmetric	O
thus	O
if	O
we	O
are	O
concerned	O
only	O
about	O
the	O
most	O
probable	O
classification	B
it	O
is	O
not	O
necessary	O
to	O
compute	O
predictions	O
using	O
eq	O
however	O
as	O
soon	O
as	O
we	O
also	O
need	O
a	O
confidence	O
in	O
the	O
prediction	O
if	O
we	O
are	O
concerned	O
about	O
a	O
reject	B
option	I
we	O
need	O
eq	O
y	O
x	O
if	O
is	O
the	O
cumulative	O
gaussian	O
function	B
then	O
eq	O
can	O
be	O
computed	O
analytically	O
as	O
shown	O
in	O
section	O
on	O
the	O
other	O
hand	O
if	O
is	O
the	O
logistic	B
function	B
then	O
we	O
need	O
to	O
resort	O
to	O
sampling	O
methods	O
or	O
analytical	O
approximations	O
to	O
compute	O
this	O
one-dimensional	O
integral	O
one	O
attractive	O
method	O
is	O
to	O
note	O
that	O
the	O
logistic	B
function	B
is	O
the	O
c	O
d	O
f	O
density	O
function	B
corresponding	O
to	O
the	O
p	O
d	O
f	O
density	O
function	B
pz	O
this	O
is	O
known	O
as	O
the	O
logistic	B
or	O
sech-squared	O
distribution	O
see	O
johnson	O
et	O
al	O
ch	O
then	O
by	O
approximating	O
pz	O
as	O
a	O
mixture	O
of	O
gaussians	O
one	O
can	O
approximate	O
by	O
a	O
linear	B
combination	O
of	O
error	O
functions	O
this	O
approximation	O
was	O
used	O
by	O
williams	O
and	O
barber	O
app	O
a	O
and	O
wood	O
and	O
kohn	O
another	O
approximation	O
suggested	O
in	O
mackay	O
is	O
f	O
where	O
vqf	O
y	O
x	O
the	O
effect	O
of	O
the	O
latent	O
predictive	B
variance	O
is	O
as	O
the	O
approximation	O
suggests	O
to	O
soften	O
the	O
prediction	O
that	O
would	O
be	O
obtained	O
using	O
the	O
map	B
prediction	O
f	O
i	O
e	O
to	O
move	O
it	O
towards	O
implementation	O
we	O
give	O
implementations	O
for	O
finding	O
the	O
laplace	B
approximation	I
in	O
algorithm	O
and	O
for	O
making	O
predictions	O
in	O
algorithm	O
care	O
is	O
taken	O
to	O
avoid	O
numerically	O
unstable	O
computations	O
while	O
minimizing	O
the	O
computational	O
effort	O
both	O
can	O
be	O
achieved	O
simultaneously	O
it	O
turns	O
out	O
that	O
several	O
of	O
the	O
desired	O
terms	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
symmetric	O
positive	B
definite	I
matrix	I
b	O
i	O
w	O
computation	O
of	O
which	O
costs	O
only	O
since	O
w	O
is	O
diagonal	O
the	O
b	O
matrix	O
has	O
eigenvalues	O
bounded	O
below	O
by	O
and	O
bounded	O
above	O
by	O
n	O
so	O
for	O
many	O
covariance	B
functions	O
b	O
is	O
guaranteed	O
to	O
be	O
well-conditioned	O
and	O
it	O
is	O
kw	O
multi-class	B
predictions	O
discussed	O
in	O
section	O
the	O
situation	O
is	O
more	O
complicated	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
kw	O
b	O
i	O
w	O
kw	O
input	O
k	O
matrix	O
y	O
targets	O
pyf	O
function	B
initialization	O
newton	O
iteration	O
eval	O
w	O
e	O
g	O
using	O
eq	O
or	O
f	O
repeat	O
w	O
log	O
pyf	O
l	O
choleskyi	O
w	O
b	O
w	O
f	O
log	O
pyf	O
a	O
b	O
w	O
llw	O
f	O
ka	O
kb	O
i	O
log	O
lii	O
objective	O
af	O
log	O
pyf	O
p	O
eq	O
using	O
eq	O
af	O
log	O
pyf	O
until	O
convergence	O
log	O
qyx	O
eq	O
return	O
f	O
f	O
mode	O
log	O
qyx	O
log	O
marg	O
likelihood	B
algorithm	O
mode-finding	O
for	O
binary	B
laplace	O
gpc	O
commonly	O
used	O
convergence	O
criteria	O
depend	O
on	O
the	O
difference	O
in	O
successive	O
values	O
of	O
the	O
objective	O
function	B
from	O
eq	O
the	O
magnitude	O
of	O
the	O
gradient	O
vector	O
from	O
eq	O
andor	O
the	O
magnitude	O
of	O
the	O
difference	O
in	O
successive	O
values	O
of	O
f	O
in	O
a	O
practical	O
implementation	O
one	O
needs	O
to	O
secure	O
against	O
divergence	O
by	O
checking	O
that	O
each	O
iteration	O
leads	O
to	O
an	O
increase	O
in	O
the	O
objective	O
trying	O
a	O
smaller	O
step	O
size	O
if	O
not	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	B
decomposition	I
in	O
line	O
which	O
takes	O
operations	O
the	O
number	O
of	O
newton	O
iterations	O
all	O
other	O
operations	O
are	O
at	O
most	O
quadratic	O
in	O
n	O
thus	O
numerically	O
safe	O
to	O
compute	O
its	O
cholesky	B
decomposition	I
ll	O
b	O
which	O
is	O
useful	O
in	O
computing	O
terms	O
involving	O
b	O
and	O
the	O
mode-finding	O
procedure	O
uses	O
the	O
newton	O
iteration	O
given	O
in	O
eq	O
involving	O
the	O
matrix	O
using	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
we	O
get	O
w	O
k	O
kw	O
where	O
b	O
is	O
given	O
in	O
eq	O
the	O
advantage	O
is	O
that	O
whereas	O
k	O
may	O
have	O
eigenvalues	O
arbitrarily	O
close	O
to	O
zero	O
thus	O
be	O
numerically	O
unstable	O
to	O
invert	O
we	O
can	O
safely	O
work	O
with	O
b	O
in	O
addition	O
algorithm	O
keeps	O
the	O
vector	O
a	O
k	O
in	O
addition	O
to	O
f	O
as	O
this	O
allows	O
evaluation	O
of	O
the	O
part	O
of	O
the	O
objective	O
in	O
eq	O
which	O
depends	O
on	O
f	O
without	O
explicit	O
reference	O
to	O
k	O
to	O
avoid	O
possible	O
numerical	O
problems	O
b	O
k	O
similarly	O
for	O
the	O
computation	O
of	O
the	O
predictive	B
variance	O
vqf	O
from	O
eq	O
we	O
need	O
to	O
evaluate	O
a	O
quadratic	B
form	I
involving	O
the	O
matrix	O
w	O
rewriting	O
this	O
as	O
w	O
w	O
w	O
w	O
w	O
w	O
b	O
achieves	O
numerical	O
stability	O
opposed	O
to	O
inverting	O
w	O
itself	O
which	O
may	O
have	O
arbitrarily	O
small	O
eigenvalues	O
thus	O
the	O
predictive	B
variance	O
from	O
eq	O
can	O
be	O
computed	O
as	O
vqf	O
kx	O
x	O
kx	O
kx	O
kx	O
x	O
vv	O
where	O
v	O
lw	O
kx	O
which	O
was	O
also	O
used	O
by	O
seeger	B
p	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
laplace	B
approximation	I
for	O
the	O
binary	B
gp	O
classifier	O
input	O
f	O
x	O
y	O
targets	O
k	O
function	B
pyf	O
function	B
x	O
test	O
input	O
w	O
log	O
py	O
f	O
l	O
choleskyi	O
w	O
kw	O
f	O
kx	O
log	O
py	O
f	O
vf	O
kx	O
x	O
vv	O
v	O
f	O
vf	O
kx	O
o	O
b	O
i	O
w	O
kw	O
eq	O
eq	O
using	O
eq	O
eq	O
return	O
class	O
probability	O
class	O
algorithm	O
predictions	O
for	O
binary	B
laplace	O
gpc	O
the	O
posterior	O
mode	O
f	O
can	O
be	O
computed	O
using	O
algorithm	O
is	O
input	O
for	O
multiple	O
test	O
inputs	O
lines	O
are	O
applied	O
to	O
each	O
test	O
input	O
computational	O
complexity	O
is	O
operations	O
once	O
plus	O
operations	O
per	O
test	O
case	O
the	O
one-dimensional	O
integral	O
in	O
line	O
can	O
be	O
done	O
analytically	O
for	O
cumulative	O
gaussian	O
likelihood	B
otherwise	O
it	O
is	O
computed	O
using	O
an	O
approximation	O
or	O
numerical	O
quadrature	O
in	O
practice	O
we	O
compute	O
the	O
cholesky	B
decomposition	I
ll	O
b	O
during	O
the	O
newton	O
steps	O
in	O
algorithm	O
which	O
can	O
be	O
re-used	O
to	O
compute	O
the	O
predictive	B
variance	O
by	O
doing	O
backsubstitution	O
with	O
l	O
as	O
discussed	O
above	O
in	O
addition	O
l	O
may	O
again	O
be	O
re-used	O
to	O
compute	O
w	O
for	O
the	O
computation	O
of	O
the	O
marginal	B
likelihood	B
eq	O
as	O
log	O
log	O
lii	O
to	O
save	O
computation	O
one	O
could	O
use	O
an	O
incomplete	O
cholesky	O
factorization	O
in	O
the	O
newton	O
steps	O
as	O
suggested	O
by	O
fine	O
and	O
scheinberg	O
kw	O
sometimes	O
it	O
is	O
suggested	O
that	O
it	O
can	O
be	O
useful	O
to	O
replace	O
k	O
by	O
k	O
where	O
is	O
a	O
small	O
constant	O
to	O
improve	O
the	O
numerical	O
of	O
k	O
however	O
by	O
taking	O
care	O
with	O
the	O
implementation	O
details	O
as	O
above	O
this	O
should	O
not	O
be	O
necessary	O
marginal	B
likelihood	B
incomplete	O
cholesky	O
factorization	O
z	O
it	O
will	O
also	O
be	O
useful	O
for	O
chapter	O
to	O
compute	O
the	O
laplace	B
approximation	I
of	O
the	O
marginal	B
likelihood	B
pyx	O
the	O
regression	O
case	O
with	O
gaussian	O
noise	O
the	O
marginal	B
likelihood	B
can	O
again	O
be	O
calculated	O
analytically	O
see	O
eq	O
we	O
have	O
pyx	O
pyfpfx	O
df	O
using	O
a	O
taylor	O
expansion	O
of	O
locally	O
around	O
f	O
we	O
obtain	O
f	O
faf	O
f	O
and	O
thus	O
an	O
approximation	O
qyx	O
to	O
the	O
marginal	B
likelihood	B
as	O
pyx	O
qyx	O
faf	O
df	O
z	O
df	O
refers	O
to	O
this	O
as	O
adding	O
jitter	B
in	O
the	O
context	O
of	O
markov	B
chain	I
monte	I
carlo	I
based	O
inference	O
in	O
his	O
work	O
the	O
latent	O
variables	O
f	O
are	O
explicitly	O
represented	O
in	O
the	O
markov	O
chain	O
which	O
makes	O
addition	O
of	O
jitter	B
difficult	O
to	O
avoid	O
within	O
the	O
analytical	O
approximations	O
of	O
the	O
distribution	O
of	O
f	O
considered	O
here	O
jitter	B
is	O
unnecessary	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
this	O
gaussian	O
integral	O
can	O
be	O
evaluated	O
analytically	O
to	O
obtain	O
an	O
approximation	O
to	O
the	O
log	O
marginal	B
likelihood	B
log	O
qyx	O
fk	O
f	O
log	O
py	O
f	O
where	O
w	O
w	O
and	O
is	O
a	O
vector	O
of	O
hyperparameters	B
of	O
the	O
covariance	B
function	B
have	O
previously	O
been	O
suppressed	O
from	O
the	O
notation	O
for	O
brevity	O
kw	O
log	O
multi-class	B
laplace	B
approximation	I
f	O
our	O
presentation	O
follows	O
williams	O
and	O
barber	O
we	O
first	O
introduce	O
the	O
vector	O
of	O
latent	O
function	B
values	O
at	O
all	O
n	O
training	O
points	O
and	O
for	O
all	O
c	O
classes	O
n	O
f	O
f	O
n	O
f	O
c	O
f	O
c	O
n	O
f	O
thus	O
f	O
has	O
length	O
cn	O
in	O
the	O
following	O
we	O
will	O
generally	O
refer	O
to	O
quantities	O
pertaining	O
to	O
a	O
particular	O
class	O
with	O
superscript	O
c	O
and	O
a	O
particular	O
case	O
by	O
subscript	O
i	O
usual	O
thus	O
e	O
g	O
the	O
vector	O
of	O
c	O
latents	O
for	O
a	O
particular	O
case	O
is	O
fi	O
however	O
as	O
an	O
exception	O
vectors	O
or	O
matrices	O
formed	O
from	O
the	O
covariance	B
function	B
for	O
class	O
c	O
will	O
have	O
a	O
subscript	O
c	O
the	O
prior	O
over	O
f	O
has	O
the	O
form	O
f	O
n	O
k	O
as	O
we	O
have	O
assumed	O
that	O
the	O
c	O
latent	O
processes	O
are	O
uncorrelated	O
the	O
covariance	B
matrix	I
k	O
is	O
block	O
diagonal	O
in	O
the	O
matrices	O
kc	O
each	O
individual	O
matrix	O
kc	O
expresses	O
the	O
correlations	O
of	O
the	O
latent	O
function	B
values	O
within	O
the	O
class	O
c	O
note	O
that	O
the	O
covariance	B
functions	O
pertaining	O
to	O
the	O
different	O
classes	O
can	O
be	O
different	O
let	O
y	O
be	O
a	O
vector	O
of	O
the	O
same	O
length	O
as	O
f	O
which	O
for	O
each	O
i	O
n	O
has	O
an	O
entry	O
of	O
for	O
the	O
class	O
which	O
is	O
the	O
label	O
for	O
example	O
i	O
and	O
for	O
the	O
other	O
c	O
entries	O
softmax	B
un-normalized	O
posterior	O
let	O
c	O
i	O
denote	O
output	O
of	O
the	O
softmax	B
at	O
training	O
point	O
i	O
i	O
e	O
pyc	O
ifi	O
c	O
i	O
p	O
i	O
expf	O
c	O
i	O
expf	O
then	O
is	O
a	O
vector	O
of	O
the	O
same	O
length	O
as	O
f	O
with	O
entries	O
c	O
analogue	O
of	O
eq	O
is	O
the	O
log	O
of	O
the	O
un-normalized	O
posterior	O
i	O
the	O
multi-class	B
fk	O
nx	O
cx	O
exp	O
f	O
c	O
i	O
log	O
cn	O
log	O
as	O
in	O
the	O
binary	B
case	O
we	O
seek	O
the	O
map	B
value	O
f	O
of	O
pfx	O
y	O
by	O
differentiating	O
eq	O
w	O
r	O
t	O
f	O
we	O
obtain	O
thus	O
at	O
the	O
maximum	O
we	O
have	O
f	O
ky	O
differentiating	O
again	O
and	O
using	O
k	O
y	O
expf	O
j	O
i	O
c	O
i	O
c	O
i	O
i	O
logx	O
j	O
f	O
c	O
i	O
f	O
i	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
multi-class	B
laplace	B
approximation	I
we	O
k	O
w	O
where	O
w	O
diag	O
where	O
is	O
a	O
cn	O
n	O
matrix	O
obtained	O
by	O
stacking	O
vertically	O
the	O
diagonal	O
matrices	O
diag	O
c	O
and	O
c	O
is	O
the	O
subvector	O
of	O
pertaining	O
to	O
class	O
c	O
as	O
in	O
the	O
binary	B
case	O
notice	O
that	O
is	O
positive	B
definite	I
thus	O
is	O
concave	O
and	O
the	O
maximum	O
is	O
unique	O
also	O
exercise	O
predictive	B
distribution	O
for	O
f	O
z	O
as	O
in	O
the	O
binary	B
case	O
we	O
use	O
newton	O
s	O
method	O
to	O
search	O
for	O
the	O
mode	O
of	O
giving	O
f	O
new	O
w	O
f	O
y	O
this	O
update	O
if	O
coded	O
na	O
vely	O
would	O
take	O
oc	O
as	O
matrices	O
of	O
size	O
cn	O
have	O
to	O
be	O
inverted	O
however	O
as	O
described	O
in	O
section	O
we	O
can	O
utilize	O
the	O
structure	O
of	O
w	O
to	O
bring	O
down	O
the	O
computational	O
load	O
to	O
the	O
laplace	B
approximation	I
gives	O
us	O
a	O
gaussian	O
approximation	O
qfx	O
y	O
to	O
the	O
posterior	O
pfx	O
y	O
to	O
make	O
predictions	O
at	O
a	O
test	O
point	O
x	O
we	O
need	O
to	O
compute	O
the	O
posterior	O
distribution	O
qf	O
y	O
x	O
where	O
fx	O
f	O
f	O
c	O
in	O
general	O
we	O
have	O
qf	O
y	O
x	O
pf	O
x	O
fqfx	O
y	O
df	O
as	O
pf	O
x	O
f	O
and	O
qfx	O
y	O
are	O
both	O
gaussian	O
qf	O
y	O
x	O
will	O
also	O
be	O
gaussian	O
and	O
we	O
need	O
only	O
compute	O
its	O
mean	O
and	O
covariance	B
the	O
predictive	B
mean	O
for	O
class	O
c	O
is	O
given	O
by	O
eqf	O
cx	O
y	O
x	O
kcx	O
c	O
f	O
c	O
kcx	O
c	O
where	O
kcx	O
is	O
the	O
vector	O
of	O
covariances	O
between	O
the	O
test	O
point	O
and	O
each	O
of	O
the	O
training	O
points	O
for	O
the	O
cth	O
covariance	B
function	B
and	O
f	O
c	O
is	O
the	O
subvector	O
of	O
f	O
pertaining	O
to	O
class	O
c	O
the	O
last	O
equality	O
comes	O
from	O
using	O
eq	O
at	O
the	O
maximum	O
f	O
note	O
the	O
close	O
correspondence	O
to	O
eq	O
this	O
can	O
be	O
put	O
into	O
a	O
vector	O
form	O
eqf	O
q	O
by	O
defining	O
the	O
cn	O
c	O
matrix	O
q	O
kcx	O
using	O
a	O
similar	O
argument	O
to	O
eq	O
we	O
obtain	O
covqf	O
y	O
x	O
q	O
k	O
w	O
diagkx	O
x	O
q	O
w	O
where	O
is	O
a	O
diagonal	O
c	O
c	O
matrix	O
with	O
cc	O
kcx	O
x	O
k	O
c	O
and	O
kx	O
x	O
is	O
a	O
vector	O
of	O
covariances	O
whose	O
c	O
th	O
element	O
is	O
kcx	O
x	O
c	O
kcx	O
is	O
a	O
sign	O
error	O
in	O
equation	O
of	O
williams	O
and	O
barber	O
but	O
not	O
in	O
their	O
implementation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
input	O
k	O
matrix	O
y	O
targets	O
f	O
repeat	O
initialization	O
newton	O
iteration	O
compute	O
and	O
from	O
f	O
with	O
eq	O
and	O
defn	O
of	O
under	O
eq	O
for	O
c	O
c	O
do	O
c	O
e	O
is	O
block	O
diag	O
d	O
d	O
kd	O
log	O
determinant	O
compute	O
c	O
kcd	O
zc	O
l	O
choleskyin	O
d	O
c	O
lld	O
ec	O
d	O
c	O
i	O
log	O
lii	O
m	O
choleskyp	O
c	O
ec	O
end	O
for	O
b	O
y	O
c	O
ekb	O
a	O
b	O
c	O
ermmrc	O
f	O
ka	O
b	O
w	O
f	O
y	O
from	O
eq	O
af	O
yf	O
i	O
p	O
c	O
expf	O
i	O
i	O
eq	O
using	O
eq	O
and	O
af	O
yf	O
objective	O
c	O
expf	O
i	O
until	O
convergence	O
log	O
qyx	O
eq	O
return	O
f	O
f	O
mode	O
log	O
qyx	O
log	O
marg	O
likelihood	B
algorithm	O
mode-finding	O
for	O
multi-class	B
laplace	O
gpc	O
where	O
d	O
diag	O
r	O
is	O
a	O
matrix	O
of	O
stacked	O
identity	O
matrices	O
and	O
a	O
subscript	O
c	O
on	O
a	O
block	O
diagonal	O
matrix	O
indicates	O
the	O
n	O
n	O
submatrix	O
pertaining	O
to	O
class	O
c	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	B
decomposition	I
in	O
lines	O
and	O
and	O
the	O
forward	O
and	O
backward	O
substitutions	O
in	O
line	O
with	O
total	O
complexity	O
oc	O
the	O
number	O
of	O
newton	O
iterations	O
all	O
other	O
operations	O
are	O
at	O
most	O
when	O
exploiting	O
diagonal	O
and	O
block	O
diagonal	O
structures	O
the	O
memory	O
requirement	O
is	O
for	O
comments	O
on	O
convergence	O
criteria	O
for	O
line	O
and	O
avoiding	O
divergence	O
refer	O
to	O
the	O
caption	O
of	O
algorithm	O
on	O
page	O
c	O
zc	O
we	O
now	O
need	O
to	O
consider	O
the	O
predictive	B
distribution	O
q	O
which	O
is	O
obtained	O
by	O
softmaxing	O
the	O
gaussian	O
qf	O
in	O
the	O
binary	B
case	O
we	O
saw	O
that	O
the	O
predicted	O
classification	B
could	O
be	O
obtained	O
by	O
thresholding	O
the	O
mean	O
value	O
of	O
the	O
gaussian	O
in	O
the	O
multi-class	B
case	O
one	O
does	O
need	O
to	O
take	O
the	O
variability	O
around	O
the	O
mean	O
into	O
account	O
as	O
it	O
can	O
affect	O
the	O
overall	O
classification	B
exercise	O
one	O
simple	O
way	O
will	O
be	O
used	O
in	O
algorithm	O
to	O
estimate	O
the	O
mean	O
prediction	O
eq	O
is	O
to	O
draw	O
samples	O
from	O
the	O
gaussian	O
qf	O
softmax	B
them	O
and	O
then	O
average	O
marginal	B
likelihood	B
the	O
laplace	B
approximation	I
to	O
the	O
marginal	B
likelihood	B
can	O
be	O
obtained	O
in	O
the	O
same	O
way	O
as	O
for	O
the	O
binary	B
case	O
yielding	O
log	O
pyx	O
log	O
qyx	O
fk	O
f	O
y	O
f	O
nx	O
cx	O
log	O
exp	O
f	O
c	O
i	O
log	O
w	O
kw	O
as	O
for	O
the	O
inversion	O
of	O
k	O
w	O
the	O
determinant	O
term	O
can	O
be	O
computed	O
efficiently	O
by	O
exploiting	O
the	O
structure	O
of	O
w	O
see	O
section	O
in	O
this	O
section	O
we	O
have	O
described	O
the	O
laplace	B
approximation	I
for	O
multi-class	B
classification	B
however	O
there	O
has	O
also	O
been	O
some	O
work	O
on	O
ep-type	O
methods	O
for	O
the	O
multi-class	B
case	O
see	O
seeger	B
and	O
jordan	O
c	O
kcd	O
c	O
for	O
c	O
c	O
do	O
l	O
choleskyin	O
d	O
c	O
lld	O
ec	O
d	O
c	O
m	O
choleskyp	O
end	O
for	O
for	O
c	O
c	O
do	O
c	O
ec	O
c	O
ckc	O
b	O
eckc	O
c	O
ecrmmrb	O
for	O
c	O
do	O
end	O
for	O
cc	O
cc	O
kcx	O
x	O
bkc	O
e	O
is	O
block	O
diag	O
d	O
d	O
kd	O
latent	O
test	O
mean	O
from	O
eq	O
latent	O
test	O
covariance	B
from	O
eq	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
multi-class	B
laplace	B
approximation	I
input	O
k	O
matrix	O
f	O
mode	O
x	O
input	O
compute	O
and	O
from	O
f	O
using	O
eq	O
and	O
defn	O
of	O
under	O
eq	O
end	O
for	O
for	O
i	O
s	O
do	O
f	O
n	O
expf	O
c	O
initialize	O
monte	O
carlo	O
loop	O
to	O
estimate	O
predictive	B
class	O
probabilities	O
using	O
s	O
samples	O
sample	O
latent	O
values	O
from	O
joint	B
gaussian	O
posterior	O
expf	O
accumulate	O
probability	O
eq	O
end	O
for	O
normalize	O
mc	O
estimate	O
of	O
prediction	O
vector	O
return	O
eqf	O
x	O
y	O
class	O
probability	O
vector	O
algorithm	O
predictions	O
for	O
multi-class	B
laplace	O
gpc	O
where	O
d	O
diag	O
r	O
is	O
a	O
matrix	O
of	O
stacked	O
identity	O
matrices	O
and	O
a	O
subscript	O
c	O
on	O
a	O
block	O
diagonal	O
matrix	O
indicates	O
the	O
n	O
n	O
submatrix	O
pertaining	O
to	O
class	O
c	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	B
decomposition	I
in	O
lines	O
and	O
with	O
a	O
total	O
complexity	O
oc	O
the	O
memory	O
requirement	O
is	O
for	O
multiple	O
test	O
cases	O
repeat	O
from	O
line	O
for	O
each	O
test	O
case	O
practice	O
for	O
multiple	O
test	O
cases	O
one	O
may	O
reorder	O
the	O
computations	O
in	O
lines	O
to	O
avoid	O
referring	O
to	O
all	O
ec	O
matrices	O
repeatedly	O
implementation	O
the	O
implementation	O
follows	O
closely	O
the	O
implementation	O
for	O
the	O
binary	B
case	O
detailed	O
in	O
section	O
with	O
the	O
slight	O
complications	O
that	O
k	O
is	O
now	O
a	O
block	O
diagonal	O
matrix	O
of	O
size	O
cn	O
cn	O
and	O
the	O
w	O
matrix	O
is	O
no	O
longer	O
diagonal	O
see	O
eq	O
care	O
has	O
to	O
be	O
taken	O
to	O
exploit	O
the	O
structure	O
of	O
these	O
matrices	O
to	O
reduce	O
the	O
computational	O
burden	O
the	O
newton	O
iteration	O
from	O
eq	O
requires	O
the	O
inversion	O
of	O
k	O
w	O
which	O
we	O
first	O
re-write	O
as	O
w	O
k	O
kk	O
w	O
using	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
in	O
the	O
following	O
the	O
inversion	O
of	O
the	O
above	O
matrix	O
k	O
w	O
is	O
our	O
main	O
concern	O
first	O
however	O
we	O
apply	O
the	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
matrix	O
inversion	B
lemma	I
eq	O
to	O
the	O
w	O
w	O
d	O
ri	O
rdr	O
d	O
ro	O
where	O
d	O
diag	O
r	O
d	O
is	O
a	O
cn	O
n	O
matrix	O
of	O
stacked	O
in	O
unit	O
matrices	O
c	O
dc	O
in	O
and	O
o	O
is	O
the	O
zero	O
matrix	O
introducing	O
the	O
above	O
in	O
k	O
w	O
and	O
applying	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
again	O
we	O
have	O
w	O
d	O
ro	O
we	O
use	O
the	O
fact	O
that	O
normalizes	O
over	O
classes	O
rdr	O
e	O
ero	O
rer	O
e	O
erp	O
and	O
rer	O
c	O
ec	O
is	O
a	O
block	O
diagonal	O
matrix	O
c	O
ec	O
the	O
newton	O
iterations	O
can	O
now	O
be	O
computed	O
by	O
inserting	O
eq	O
and	O
in	O
eq	O
as	O
detailed	O
in	O
algorithm	O
the	O
predictions	O
use	O
an	O
equivalent	B
route	O
to	O
compute	O
the	O
gaussian	O
posterior	O
and	O
the	O
final	O
step	O
of	O
deriving	O
predictive	B
class	O
probabilities	O
is	O
done	O
by	O
monte	O
carlo	O
as	O
shown	O
in	O
algorithm	O
where	O
e	O
d	O
d	O
d	O
kd	O
expectation	B
propagation	I
the	O
expectation	B
propagation	I
algorithm	O
is	O
a	O
general	O
approximation	O
tool	O
with	O
a	O
wide	O
range	O
of	O
applications	O
in	O
this	O
section	O
we	O
present	O
only	O
its	O
application	O
to	O
the	O
specific	O
case	O
of	O
a	O
gp	O
model	O
for	O
binary	B
classification	B
we	O
note	O
that	O
opper	O
and	O
winther	O
presented	O
a	O
similar	O
method	O
for	O
binary	B
gpc	O
based	O
on	O
the	O
fixed-point	O
equations	O
of	O
the	O
thouless-anderson-palmer	O
type	O
of	O
mean-field	B
approximation	I
from	O
statistical	O
physics	O
the	O
fixed	O
points	O
for	O
the	O
two	O
methods	O
are	O
the	O
same	O
although	O
the	O
precise	O
details	O
of	O
the	O
two	O
algorithms	O
are	O
different	O
the	O
ep	O
algorithm	O
naturally	O
lends	O
itself	O
to	O
sparse	O
approximations	O
which	O
will	O
not	O
be	O
discussed	O
in	O
detail	O
here	O
but	O
touched	O
upon	O
in	O
section	O
the	O
object	O
of	O
central	O
importance	O
is	O
the	O
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
pfx	O
y	O
in	O
the	O
following	O
notation	O
we	O
suppress	O
the	O
explicit	O
dependence	O
on	O
hyperparameters	B
see	O
section	O
for	O
their	O
treatment	O
the	O
posterior	O
is	O
given	O
by	O
bayes	O
rule	O
as	O
the	O
product	O
of	O
a	O
normalization	O
term	O
the	O
prior	O
and	O
the	O
likelihood	B
pfx	O
y	O
pfx	O
where	O
the	O
prior	O
pfx	O
is	O
gaussian	O
and	O
we	O
have	O
utilized	O
the	O
fact	O
that	O
the	O
likelihood	B
factorizes	O
over	O
the	O
training	O
cases	O
the	O
normalization	O
term	O
is	O
the	O
marginal	B
likelihood	B
pyifi	O
z	O
pyx	O
pfx	O
pyifi	O
df	O
ny	O
ny	O
z	O
z	O
who	O
are	O
disturbed	O
by	O
our	O
sloppy	O
treatment	O
of	O
the	O
inverse	O
of	O
singular	O
matrices	O
are	O
invited	O
to	O
insert	O
the	O
matrix	O
between	O
and	O
in	O
eq	O
and	O
verify	O
that	O
eq	O
coincides	O
with	O
the	O
limit	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
expectation	B
propagation	I
so	O
far	O
everything	O
is	O
exactly	O
as	O
in	O
the	O
regression	O
case	O
discussed	O
in	O
chapter	O
however	O
in	O
the	O
case	O
of	O
classification	B
the	O
likelihood	B
pyifi	O
is	O
not	O
gaussian	O
a	O
property	O
that	O
was	O
used	O
heavily	O
in	O
arriving	O
at	O
analytical	O
solutions	O
for	O
the	O
regression	O
framework	O
in	O
this	O
section	O
we	O
use	O
the	O
probit	B
likelihood	B
page	O
for	O
binary	B
classification	B
and	O
this	O
makes	O
the	O
posterior	O
in	O
eq	O
analytically	O
intractable	O
to	O
overcome	O
this	O
hurdle	O
in	O
the	O
ep	O
framework	O
we	O
approximate	O
the	O
likelihood	B
by	O
a	O
local	O
likelihood	B
approximation	O
in	O
the	O
form	O
of	O
an	O
un-normalized	O
gaussian	O
function	B
in	O
the	O
latent	O
variable	O
fi	O
pyifi	O
pyifi	O
tifi	O
zi	O
i	O
i	O
zin	O
i	O
i	O
site	O
parameters	O
which	O
defines	O
the	O
site	O
parameters	O
zi	O
i	O
and	O
i	O
remember	O
that	O
the	O
notation	O
n	O
is	O
used	O
for	O
a	O
normalized	O
gaussian	B
distribution	I
notice	O
that	O
we	O
are	O
approximating	O
the	O
likelihood	B
i	O
e	O
a	O
probability	O
distribution	O
which	O
normalizes	O
over	O
the	O
targets	O
yi	O
by	O
an	O
un-normalized	O
gaussian	B
distribution	I
over	O
the	O
latent	O
variables	O
fi	O
this	O
is	O
reasonable	O
because	O
we	O
are	O
interested	O
in	O
how	O
the	O
likelihood	B
behaves	O
as	O
a	O
function	B
of	O
the	O
latent	O
fi	O
in	O
the	O
regression	O
setting	O
we	O
utilized	O
the	O
gaussian	O
shape	O
of	O
the	O
likelihood	B
but	O
more	O
to	O
the	O
point	O
the	O
gaussian	B
distribution	I
for	O
the	O
outputs	B
yi	O
also	O
implied	O
a	O
gaussian	O
shape	O
as	O
a	O
function	B
of	O
the	O
latent	O
variable	O
fi	O
in	O
order	O
to	O
compute	O
the	O
posterior	O
we	O
are	O
of	O
course	O
primarily	O
interested	O
in	O
how	O
the	O
likelihood	B
behaves	O
as	O
a	O
function	B
of	O
the	O
property	O
that	O
the	O
likelihood	B
should	O
normalize	O
over	O
yi	O
any	O
value	O
of	O
fi	O
is	O
not	O
simultaneously	O
achievable	O
with	O
the	O
desideratum	O
of	O
gaussian	O
dependence	O
on	O
fi	O
in	O
the	O
ep	O
approximation	O
we	O
abandon	O
exact	O
normalization	O
for	O
tractability	O
the	O
product	O
of	O
the	O
local	O
likelihoods	O
ti	O
is	O
ny	O
i	O
n	O
tifi	O
zi	O
i	O
zi	O
i	O
where	O
is	O
the	O
vector	O
of	O
i	O
and	O
is	O
diagonal	O
with	O
ii	O
the	O
posterior	O
pfx	O
y	O
by	O
qfx	O
y	O
i	O
we	O
approximate	O
ny	O
qfx	O
y	O
zep	O
with	O
and	O
tifi	O
zi	O
i	O
pfx	O
i	O
n	O
where	O
we	O
have	O
used	O
eq	O
to	O
compute	O
the	O
product	O
by	O
definition	O
we	O
know	O
that	O
the	O
distribution	O
must	O
normalize	O
correctly	O
over	O
f	O
notice	O
that	O
we	O
use	O
the	O
tilde-parameters	O
and	O
z	O
for	O
the	O
local	O
likelihood	B
approximations	O
that	O
although	O
each	O
likelihood	B
approximation	O
is	O
local	O
the	O
posterior	O
approximation	O
produced	O
by	O
the	O
ep	O
algorithm	O
is	O
global	O
because	O
the	O
latent	O
variables	O
are	O
coupled	O
through	O
the	O
prior	O
for	O
computing	O
the	O
marginal	B
likelihood	B
normalization	O
becomes	O
crucial	O
see	O
section	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
and	O
plain	O
and	O
for	O
the	O
parameters	O
of	O
the	O
approximate	O
posterior	O
the	O
normalizing	O
term	O
of	O
eq	O
zep	O
qyx	O
is	O
the	O
ep	O
algorithm	O
s	O
approximation	O
to	O
the	O
normalizing	O
term	O
z	O
from	O
eq	O
and	O
eq	O
kl	O
divergence	O
how	O
do	O
we	O
choose	O
the	O
parameters	O
of	O
the	O
local	O
approximating	O
distributions	O
ti	O
one	O
of	O
the	O
most	O
obvious	O
ideas	O
would	O
be	O
to	O
minimize	O
the	O
kullback-leibler	B
divergence	I
section	O
between	O
the	O
posterior	O
and	O
its	O
approximation	O
yqfx	O
direct	O
minimization	O
of	O
this	O
kl	O
divergence	O
for	O
the	O
choose	O
to	O
minimize	O
the	O
reversed	O
kl	O
divergence	O
ypfx	O
with	O
joint	B
distribution	O
on	O
f	O
turns	O
out	O
to	O
be	O
intractable	O
can	O
alternatively	O
respect	O
to	O
the	O
distribution	O
qfx	O
y	O
this	O
has	O
been	O
used	O
to	O
carry	O
out	O
variational	O
inference	O
for	O
gpc	O
see	O
e	O
g	O
seeger	B
instead	O
the	O
key	O
idea	O
in	O
the	O
ep	O
algorithm	O
is	O
to	O
update	O
the	O
individual	O
ti	O
approximations	O
sequentially	O
conceptually	O
this	O
is	O
done	O
by	O
iterating	O
the	O
following	O
four	O
steps	O
we	O
start	O
from	O
some	O
current	O
approximate	O
posterior	O
from	O
which	O
we	O
leave	O
out	O
the	O
current	O
ti	O
giving	O
rise	O
to	O
a	O
marginal	B
cavity	O
distribution	O
secondly	O
we	O
combine	O
the	O
cavity	O
distribution	O
with	O
the	O
exact	O
likelihood	B
pyifi	O
to	O
get	O
the	O
desired	O
marginal	B
thirdly	O
we	O
choose	O
a	O
gaussian	O
approximation	O
to	O
the	O
non-gaussian	B
marginal	B
and	O
in	O
the	O
final	O
step	O
we	O
compute	O
the	O
ti	O
which	O
makes	O
the	O
posterior	O
have	O
the	O
desired	O
marginal	B
from	O
step	O
three	O
these	O
four	O
steps	O
are	O
iterated	O
until	O
convergence	O
in	O
more	O
detail	O
we	O
optimize	O
the	O
ti	O
approximations	O
sequentially	O
using	O
the	O
approximation	O
so	O
far	O
for	O
all	O
the	O
other	O
variables	O
in	O
particular	O
the	O
approximate	O
posterior	O
for	O
fi	O
contains	O
three	O
kinds	O
of	O
terms	O
the	O
prior	O
pfx	O
the	O
local	O
approximate	O
likelihoods	O
tj	O
for	O
all	O
cases	O
j	O
i	O
the	O
exact	O
likelihood	B
for	O
case	O
i	O
pyifi	O
our	O
goal	O
is	O
to	O
combine	O
these	O
sources	O
of	O
information	O
and	O
choose	O
parameters	O
of	O
ti	O
such	O
that	O
the	O
marginal	B
posterior	O
is	O
as	O
accurate	O
as	O
possible	O
we	O
will	O
first	O
combine	O
the	O
prior	O
and	O
the	O
local	O
likelihood	B
approximations	O
into	O
the	O
cavity	O
distribution	O
z	O
pfxy	O
q	O
ifi	O
tjfj	O
zj	O
j	O
j	O
and	O
subsequently	O
combine	O
this	O
with	O
the	O
exact	O
likelihood	B
for	O
case	O
i	O
conceptually	O
one	O
can	O
think	O
of	O
the	O
combination	O
of	O
prior	O
and	O
the	O
n	O
approximate	O
likelihoods	O
in	O
eq	O
in	O
two	O
ways	O
either	O
by	O
explicitly	O
multiplying	O
out	O
the	O
terms	O
or	O
by	O
removing	O
approximate	O
likelihood	B
i	O
from	O
the	O
approximate	O
posterior	O
in	O
eq	O
here	O
we	O
will	O
follow	O
the	O
latter	O
approach	O
the	O
marginal	B
for	O
fi	O
from	O
qfx	O
y	O
is	O
obtained	O
by	O
using	O
eq	O
in	O
eq	O
to	O
give	O
i	O
ii	O
this	O
marginal	B
eq	O
contains	O
one	O
approximate	O
term	O
qfix	O
y	O
n	O
i	O
i	O
where	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
expectation	B
propagation	I
ti	O
too	O
many	O
so	O
we	O
need	O
to	O
divide	O
it	O
by	O
ti	O
to	O
get	O
the	O
cavity	O
distribution	O
cavity	O
distribution	O
q	O
ifi	O
n	O
i	O
i	O
i	O
i	O
where	O
i	O
i	O
i	O
i	O
and	O
i	O
i	O
i	O
note	O
that	O
the	O
cavity	O
distribution	O
and	O
its	O
parameters	O
carry	O
the	O
subscript	O
i	O
indicating	O
that	O
they	O
include	O
all	O
cases	O
except	O
number	O
i	O
the	O
easiest	O
way	O
to	O
verify	O
eq	O
is	O
to	O
multiply	O
the	O
cavity	O
distribution	O
by	O
the	O
local	O
likelihood	B
approximation	O
ti	O
from	O
eq	O
using	O
eq	O
to	O
recover	O
the	O
marginal	B
in	O
eq	O
notice	O
that	O
despite	O
the	O
appearance	O
of	O
eq	O
the	O
cavity	O
mean	O
and	O
variance	O
are	O
course	O
not	O
dependent	O
on	O
i	O
and	O
i	O
see	O
exercise	O
to	O
proceed	O
we	O
need	O
to	O
find	O
the	O
new	O
gaussian	O
marginal	B
which	O
best	O
approximates	O
the	O
product	O
of	O
the	O
cavity	O
distribution	O
and	O
the	O
exact	O
likelihood	B
it	O
is	O
well	O
known	O
that	O
when	O
qx	O
is	O
gaussian	O
the	O
distribution	O
qx	O
which	O
min	O
imizes	O
is	O
the	O
one	O
whose	O
first	O
and	O
second	O
moments	O
match	O
that	O
i	O
q	O
ifipyifi	O
qfi	O
zin	O
i	O
of	O
px	O
see	O
eq	O
as	O
qfi	O
is	O
un-normalized	O
we	O
choose	O
additionally	O
to	O
impose	O
the	O
condition	O
that	O
the	O
zero-th	O
moments	O
constants	O
should	O
match	O
when	O
choosing	O
the	O
parameters	O
of	O
qfi	O
to	O
match	O
the	O
right	O
hand	O
side	O
of	O
eq	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
the	O
derivation	O
of	O
the	O
moments	O
is	O
somewhat	O
lengthy	O
so	O
we	O
have	O
moved	O
the	O
details	O
to	O
section	O
the	O
desired	O
posterior	O
marginal	B
moments	O
are	O
zi	O
i	O
i	O
in	O
i	O
i	O
i	O
n	O
zi	O
yi	O
in	O
i	O
where	O
zi	O
yi	O
i	O
i	O
the	O
final	O
step	O
is	O
to	O
compute	O
the	O
parameters	O
of	O
the	O
approximation	O
ti	O
which	O
achieves	O
a	O
match	O
with	O
the	O
desired	O
moments	O
in	O
particular	O
the	O
product	O
of	O
the	O
cavity	O
distribution	O
and	O
the	O
local	O
approximation	O
must	O
have	O
the	O
desired	O
moments	O
leading	O
to	O
i	O
zi	O
zi	O
p	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
which	O
is	O
easily	O
verified	O
by	O
multiplying	O
the	O
cavity	O
distribution	O
by	O
the	O
local	O
approximation	O
using	O
eq	O
to	O
obtain	O
eq	O
note	O
that	O
the	O
desired	O
marginal	B
posterior	O
variance	O
i	O
given	O
by	O
eq	O
is	O
guaranteed	O
to	O
be	O
smaller	O
than	O
the	O
cavity	O
variance	O
such	O
that	O
i	O
is	O
always	O
this	O
completes	O
the	O
update	O
for	O
a	O
local	O
likelihood	B
approximation	O
ti	O
we	O
then	O
have	O
to	O
update	O
the	O
approximate	O
posterior	O
using	O
eq	O
but	O
since	O
only	O
a	O
cases	O
where	O
the	O
likelihood	B
is	O
log	O
concave	O
one	O
can	O
show	O
that	O
i	O
but	O
for	O
a	O
general	O
likelihood	B
there	O
may	O
be	O
no	O
such	O
guarantee	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
figure	O
approximating	O
a	O
single	O
likelihood	B
term	O
by	O
a	O
gaussian	O
panel	O
dashdotted	O
the	O
exact	O
likelihood	B
corresponding	O
target	O
being	O
yi	O
as	O
a	O
function	B
of	O
the	O
latent	O
fi	O
dotted	O
gaussian	O
cavity	O
distribution	O
n	O
i	O
i	O
solid	O
posterior	O
dashed	O
posterior	O
approximation	O
panel	O
shows	O
an	O
enlargement	O
of	O
panel	O
single	O
site	O
has	O
changed	O
one	O
can	O
do	O
this	O
with	O
a	O
computationally	O
efficient	O
rankone	O
update	O
see	O
section	O
the	O
ep	O
algorithm	O
is	O
used	O
iteratively	O
updating	O
each	O
local	O
approximation	O
in	O
turn	O
it	O
is	O
clear	O
that	O
several	O
passes	O
over	O
the	O
data	O
are	O
required	O
since	O
an	O
update	O
of	O
one	O
local	O
approximation	O
potentially	O
influences	O
all	O
of	O
the	O
approximate	O
marginal	B
posteriors	O
predictions	O
the	O
procedure	O
for	O
making	O
predictions	O
in	O
the	O
ep	O
framework	O
closely	O
resembles	O
the	O
algorithm	O
for	O
the	O
laplace	B
approximation	I
in	O
section	O
ep	O
gives	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
eq	O
the	O
approximate	O
predictive	B
mean	O
for	O
the	O
latent	O
variable	O
f	O
becomes	O
eqf	O
y	O
x	O
k	O
k	O
k	O
k	O
k	O
the	O
approximate	O
latent	O
predictive	B
variance	O
is	O
analogous	O
to	O
the	O
derivation	O
from	O
eq	O
and	O
eq	O
with	O
playing	O
the	O
r	O
ole	O
of	O
w	O
vqf	O
y	O
x	O
kx	O
x	O
k	O
qy	O
y	O
x	O
eq	O
y	O
x	O
the	O
approximate	O
predictive	B
distribution	O
for	O
the	O
binary	B
target	O
becomes	O
y	O
x	O
df	O
where	O
qf	O
y	O
x	O
is	O
the	O
approximate	O
latent	O
predictive	B
gaussian	O
with	O
mean	O
and	O
variance	O
given	O
by	O
eq	O
and	O
eq	O
this	O
integral	O
is	O
readily	O
evaluated	O
using	O
eq	O
giving	O
the	O
predictive	B
probability	O
z	O
qy	O
y	O
x	O
k	O
kx	O
x	O
k	O
p	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
expectation	B
propagation	I
marginal	B
likelihood	B
the	O
ep	O
approximation	O
to	O
the	O
marginal	B
likelihood	B
can	O
be	O
found	O
from	O
the	O
normalization	O
of	O
eq	O
z	O
ny	O
zep	O
qyx	O
pfx	O
tifi	O
zi	O
i	O
i	O
df	O
using	O
eq	O
and	O
eq	O
in	O
an	O
analogous	O
way	O
to	O
the	O
treatment	O
of	O
the	O
regression	O
setting	O
in	O
equations	O
and	O
we	O
arrive	O
at	O
logzep	O
log	O
yi	O
i	O
log	O
nx	O
nx	O
i	O
log	O
i	O
i	O
nx	O
i	O
i	O
i	O
where	O
denotes	O
the	O
hyperparameters	B
of	O
the	O
covariance	B
function	B
this	O
expression	O
has	O
a	O
nice	O
intuitive	O
interpretation	O
the	O
first	O
two	O
terms	O
are	O
the	O
marginal	B
likelihood	B
for	O
a	O
regression	O
model	O
for	O
each	O
component	O
of	O
which	O
has	O
independent	O
gaussian	O
noise	O
of	O
variance	O
ii	O
is	O
diagonal	O
cf	O
eq	O
the	O
remaining	O
three	O
terms	O
come	O
from	O
the	O
normalization	O
constants	O
zi	O
the	O
first	O
of	O
these	O
penalizes	O
the	O
cavity	O
leave-one-out	B
distributions	O
for	O
not	O
agreeing	O
with	O
the	O
classification	B
labels	O
see	O
eq	O
in	O
other	O
words	O
we	O
can	O
see	O
that	O
the	O
marginal	B
likelihood	B
combines	O
two	O
desiderata	O
the	O
means	O
of	O
the	O
local	O
likelihood	B
approximations	O
should	O
be	O
well	O
predicted	O
by	O
a	O
gp	O
and	O
the	O
corresponding	O
latent	O
function	B
when	O
ignoring	O
a	O
particular	O
training	O
example	O
should	O
be	O
able	O
to	O
predict	O
the	O
corresponding	O
classification	B
label	O
well	O
implementation	O
the	O
implementation	O
for	O
the	O
ep	O
algorithm	O
follows	O
the	O
derivation	O
in	O
the	O
previous	O
section	O
closely	O
except	O
that	O
care	O
has	O
to	O
be	O
taken	O
to	O
achieve	O
numerical	O
stability	O
in	O
similar	O
ways	O
to	O
the	O
considerations	O
for	O
laplace	O
s	O
method	O
in	O
section	O
in	O
addition	O
we	O
wish	O
to	O
be	O
able	O
to	O
specifically	O
handle	O
the	O
case	O
were	O
some	O
site	O
variances	O
i	O
may	O
tend	O
to	O
infinity	O
this	O
corresponds	O
to	O
ignoring	O
the	O
corresponding	O
likelihood	B
terms	O
and	O
can	O
form	O
the	O
basis	O
of	O
sparse	O
approximations	O
touched	O
upon	O
in	O
section	O
in	O
this	O
limit	O
everything	O
remains	O
well-defined	O
although	O
this	O
is	O
not	O
obvious	O
e	O
g	O
from	O
looking	O
at	O
eq	O
it	O
turns	O
out	O
to	O
be	O
slightly	O
more	O
convenient	O
to	O
use	O
natural	O
parameters	O
i	O
i	O
and	O
i	O
i	O
for	O
the	O
site	O
and	O
cavity	O
parameters	O
i	O
i	O
rather	O
than	O
importance	O
is	O
i	O
i	O
i	O
i	O
i	O
and	O
i	O
i	O
themselves	O
the	O
symmetric	O
matrix	O
of	O
central	O
i	O
i	O
s	O
diag	O
s	O
which	O
plays	O
a	O
r	O
ole	O
equivalent	B
to	O
eq	O
expressions	O
involving	O
the	O
inverse	O
of	O
b	O
are	O
computed	O
via	O
cholesky	O
factorization	O
which	O
is	O
numerically	O
stable	O
since	O
b	O
i	O
s	O
k	O
s	O
natural	O
parameters	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
input	O
k	O
matrix	O
y	O
targets	O
k	O
initialization	O
and	O
eq	O
repeat	O
o	O
for	O
i	O
to	O
n	O
do	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
and	O
i	O
i	O
o	O
i	O
i	O
i	O
i	O
i	O
compute	O
the	O
marginal	B
moments	O
i	O
and	O
i	O
i	O
ii	O
s	O
i	O
o	O
compute	O
approximate	O
cavity	O
parameters	O
i	O
and	O
i	O
using	O
eq	O
using	O
eq	O
update	O
site	O
parameters	O
i	O
and	O
i	O
using	O
eq	O
update	O
and	O
by	O
eq	O
and	O
eq	O
si	O
is	O
column	O
i	O
of	O
end	O
for	O
l	O
choleskyin	O
s	O
v	O
l	O
s	O
k	O
v	O
and	O
k	O
s	O
k	O
re-compute	O
the	O
approximate	O
posterior	O
parameters	O
and	O
using	O
eq	O
and	O
eq	O
until	O
convergence	O
compute	O
log	O
zep	O
using	O
eq	O
and	O
and	O
the	O
existing	O
l	O
return	O
site	O
param	O
log	O
zep	O
log	O
marg	O
likelihood	B
algorithm	O
expectation	B
propagation	I
for	O
binary	B
classification	B
the	O
targets	O
y	O
are	O
used	O
only	O
in	O
line	O
in	O
lines	O
the	O
parameters	O
of	O
the	O
approximate	O
posterior	O
are	O
re-computed	O
they	O
already	O
exist	O
this	O
is	O
done	O
because	O
of	O
the	O
large	O
number	O
of	O
rank-one	O
updates	O
in	O
line	O
which	O
would	O
eventually	O
cause	O
loss	B
of	O
numerical	O
precision	O
in	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
rank-one	O
updates	O
in	O
line	O
which	O
takes	O
per	O
variable	O
i	O
e	O
for	O
an	O
entire	O
sweep	O
over	O
all	O
variables	O
similarly	O
re-computing	O
in	O
lines	O
is	O
the	O
eigenvalues	O
of	O
b	O
are	O
bounded	O
below	O
by	O
one	O
the	O
parameters	O
of	O
the	O
gaussian	O
approximate	O
posterior	O
from	O
eq	O
are	O
computed	O
as	O
s	O
k	O
kk	O
s	O
k	O
k	O
s	O
k	O
b	O
s	O
after	O
updating	O
the	O
parameters	O
of	O
a	O
site	O
we	O
need	O
to	O
update	O
the	O
approximate	O
posterior	O
eq	O
taking	O
the	O
new	O
site	O
parameters	O
into	O
account	O
for	O
the	O
inverse	O
covariance	B
matrix	I
of	O
the	O
approximate	O
posterior	O
we	O
have	O
from	O
eq	O
k	O
s	O
and	O
thus	O
i	O
where	O
ei	O
is	O
a	O
unit	O
vector	O
in	O
direction	O
i	O
and	O
we	O
have	O
used	O
that	O
s	O
diag	O
using	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
on	O
eq	O
we	O
obtain	O
the	O
new	O
new	O
k	O
sold	O
new	O
i	O
old	O
i	O
new	O
old	O
i	O
old	O
new	O
i	O
old	O
new	O
i	O
i	O
old	O
ii	O
sis	O
i	O
in	O
time	O
where	O
si	O
is	O
the	O
i	O
th	O
column	O
of	O
old	O
the	O
posterior	O
mean	O
is	O
then	O
calculated	O
from	O
eq	O
in	O
the	O
ep	O
algorithm	O
each	O
site	O
is	O
updated	O
in	O
turn	O
and	O
several	O
passes	O
over	O
all	O
sites	O
are	O
required	O
pseudocode	O
for	O
the	O
ep-gpc	O
algorithm	O
is	O
given	O
in	O
algorithm	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
expectation	B
propagation	I
input	O
site	O
param	O
x	O
y	O
targets	O
z	O
s	O
k	O
s	O
k	O
ll	O
s	O
v	O
s	O
l	O
choleskyin	O
s	O
f	O
kx	O
z	O
vf	O
kx	O
x	O
vv	O
vf	O
kx	O
f	O
k	O
function	B
x	O
test	O
input	O
k	O
s	O
eq	O
using	O
eq	O
b	O
in	O
s	O
o	O
o	O
eq	O
using	O
eq	O
eq	O
return	O
class	O
probability	O
class	O
algorithm	O
predictions	O
for	O
expectation	B
propagation	I
the	O
natural	O
site	O
parameters	O
and	O
of	O
the	O
posterior	O
can	O
be	O
computed	O
using	O
algorithm	O
are	O
input	O
for	O
multiple	O
test	O
inputs	O
lines	O
are	O
applied	O
to	O
each	O
test	O
input	O
computational	O
complexity	O
is	O
operations	O
once	O
and	O
plus	O
operations	O
per	O
test	O
case	O
although	O
the	O
cholesky	B
decomposition	I
in	O
line	O
could	O
be	O
avoided	O
by	O
storing	O
it	O
in	O
algorithm	O
note	O
the	O
close	O
similarity	O
to	O
algorithm	O
on	O
page	O
there	O
is	O
no	O
formal	O
guarantee	O
of	O
convergence	O
but	O
several	O
authors	O
have	O
reported	O
that	O
ep	O
for	O
gaussian	B
process	I
models	O
works	O
relatively	O
for	O
the	O
predictive	B
distribution	O
we	O
get	O
the	O
mean	O
from	O
eq	O
which	O
is	O
evaluated	O
using	O
eqf	O
y	O
x	O
k	O
s	O
s	O
k	O
k	O
s	O
and	O
the	O
predictive	B
variance	O
from	O
eq	O
similarly	O
by	O
s	O
b	O
s	O
k	O
vqf	O
y	O
x	O
kx	O
x	O
k	O
s	O
kx	O
x	O
k	O
s	O
k	O
b	O
s	O
pseudocode	O
for	O
making	O
predictions	O
using	O
ep	O
is	O
given	O
in	O
algorithm	O
finally	O
we	O
need	O
to	O
evaluate	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
from	O
eq	O
there	O
are	O
several	O
terms	O
which	O
need	O
careful	O
consideration	O
principally	O
due	O
to	O
the	O
fact	O
the	O
i	O
values	O
may	O
be	O
arbitrarily	O
small	O
cannot	O
safely	O
be	O
inverted	O
we	O
start	O
with	O
the	O
fourth	O
and	O
first	O
terms	O
of	O
eq	O
x	O
log	O
s	O
st	O
log	O
s	O
i	O
i	O
x	O
log	O
log	O
s	O
log	O
lii	O
i	O
i	O
where	O
t	O
is	O
a	O
diagonal	O
matrix	O
of	O
cavity	O
precisions	O
tii	O
i	O
i	O
and	O
l	O
is	O
the	O
cholesky	O
factorization	O
of	O
b	O
in	O
eq	O
we	O
have	O
factored	O
out	O
the	O
matrix	O
s	O
from	O
both	O
determinants	O
and	O
the	O
terms	O
cancel	O
continuing	O
with	O
the	O
part	O
of	O
the	O
has	O
been	O
conjectured	O
not	O
proven	O
by	O
l	O
csat	O
o	O
communication	O
that	O
ep	O
is	O
guaranteed	O
to	O
converge	O
if	O
the	O
likelihood	B
is	O
log	O
concave	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
fifth	O
term	O
from	O
eq	O
which	O
is	O
quadratic	O
in	O
together	O
with	O
the	O
second	O
term	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
k	O
s	O
k	O
s	O
b	O
s	O
where	O
in	O
eq	O
we	O
apply	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
to	O
both	O
parenthesis	O
to	O
be	O
inverted	O
the	O
remainder	O
of	O
the	O
fifth	O
term	O
in	O
eq	O
is	O
evaluated	O
using	O
the	O
identity	O
it	O
s	O
i	O
it	O
s	O
t	O
s	O
i	O
where	O
i	O
is	O
the	O
vector	O
of	O
cavity	O
means	O
i	O
the	O
third	O
term	O
in	O
eq	O
requires	O
in	O
no	O
special	O
treatment	O
and	O
can	O
be	O
evaluated	O
as	O
written	O
experiments	O
in	O
this	O
section	O
we	O
present	O
the	O
results	O
of	O
applying	O
the	O
algorithms	O
for	O
gp	O
classification	B
discussed	O
in	O
the	O
previous	O
sections	O
to	O
several	O
data	O
sets	O
the	O
purpose	O
is	O
firstly	O
to	O
illustrate	O
the	O
behaviour	O
of	O
the	O
methods	O
and	O
secondly	O
to	O
gain	O
some	O
insights	O
into	O
how	O
good	O
the	O
performance	O
is	O
compared	O
to	O
some	O
other	O
commonlyused	O
machine	O
learning	B
methods	O
for	O
classification	B
section	O
illustrates	O
the	O
action	O
of	O
a	O
gp	O
classifier	O
on	O
a	O
toy	O
binary	B
prediction	O
problem	O
with	O
a	O
input	O
space	O
and	O
shows	O
the	O
effect	O
of	O
varying	O
the	O
length-scale	B
in	O
the	O
se	O
covariance	B
function	B
in	O
section	O
we	O
illustrate	O
and	O
compare	O
the	O
behaviour	O
of	O
the	O
two	O
approximate	O
gp	O
methods	O
on	O
a	O
simple	O
onedimensional	O
binary	B
task	O
in	O
section	O
we	O
present	O
results	O
for	O
a	O
binary	B
gp	O
classifier	O
on	O
a	O
handwritten	O
digit	O
classification	B
task	O
and	O
study	O
the	O
effect	O
of	O
varying	O
the	O
kernel	B
parameters	O
in	O
section	O
we	O
carry	O
out	O
a	O
similar	O
study	O
using	O
a	O
multi-class	B
gp	O
classifier	O
to	O
classify	O
digits	O
from	O
all	O
ten	O
classes	O
in	O
section	O
we	O
discuss	O
the	O
methods	O
from	O
both	O
experimental	O
and	O
theoretical	O
viewpoints	O
a	O
toy	O
problem	O
figure	O
illustrates	O
the	O
operation	O
of	O
a	O
gaussian	B
process	I
classifier	O
on	O
a	O
binary	B
problem	O
using	O
the	O
squared	B
exponential	B
kernel	B
with	O
a	O
variable	O
length-scale	B
and	O
the	O
logistic	B
response	B
function	B
the	O
laplace	B
approximation	I
was	O
used	O
to	O
make	O
the	O
plots	O
the	O
data	O
points	O
lie	O
within	O
the	O
square	O
as	O
shown	O
in	O
panel	O
notice	O
in	O
particular	O
the	O
lone	O
white	O
point	O
amongst	O
the	O
black	O
points	O
in	O
the	O
ne	O
corner	O
and	O
the	O
lone	O
black	O
point	O
amongst	O
the	O
white	O
points	O
in	O
the	O
sw	O
corner	O
in	O
panel	O
the	O
length-scale	B
is	O
a	O
relatively	O
short	O
value	O
in	O
this	O
case	O
the	O
latent	O
function	B
is	O
free	O
to	O
vary	O
relatively	O
quickly	O
and	O
so	O
the	O
classifications	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
experiments	O
figure	O
panel	O
shows	O
the	O
location	O
of	O
the	O
data	O
points	O
in	O
the	O
two-dimensional	O
space	O
the	O
two	O
classes	O
are	O
labelled	O
as	O
open	O
circles	O
and	O
closed	O
circles	O
panels	O
show	O
contour	O
plots	O
of	O
the	O
predictive	B
probability	O
eq	O
for	O
signal	O
variance	O
f	O
and	O
length-scales	O
of	O
and	O
respectively	O
the	O
decision	O
boundaries	O
between	O
the	O
two	O
classes	O
are	O
shown	O
by	O
the	O
thicker	O
black	O
lines	O
the	O
maximum	O
value	O
attained	O
is	O
and	O
the	O
minimum	O
is	O
provided	O
by	O
thresholding	O
the	O
predictive	B
probability	O
eq	O
at	O
agrees	O
with	O
the	O
training	O
labels	O
at	O
all	O
data	O
points	O
in	O
contrast	O
in	O
panel	O
the	O
lengthscale	O
is	O
set	B
to	O
now	O
the	O
latent	O
function	B
must	O
vary	O
more	O
smoothly	O
and	O
so	O
the	O
two	O
lone	O
points	O
are	O
misclassified	O
panel	O
was	O
obtained	O
with	O
as	O
would	O
be	O
expected	O
the	O
decision	O
boundaries	O
are	O
more	O
complex	O
for	O
shorter	O
length-scales	O
methods	O
for	O
setting	O
the	O
hyperparameters	B
based	O
on	O
the	O
data	O
are	O
discussed	O
in	O
chapter	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
figure	O
one-dimensional	O
toy	O
classification	B
dataset	O
panel	O
shows	O
the	O
dataset	O
where	O
points	O
from	O
class	O
have	O
been	O
plotted	O
at	O
and	O
class	O
at	O
together	O
with	O
the	O
predictive	B
probability	O
for	O
laplace	O
s	O
method	O
and	O
the	O
ep	O
approximation	O
also	O
shown	O
is	O
the	O
probability	O
py	O
of	O
the	O
data	O
generating	O
process	O
panel	O
shows	O
the	O
corresponding	O
distribution	O
of	O
the	O
latent	O
function	B
f	O
showing	O
curves	O
for	O
the	O
mean	O
and	O
standard	O
deviations	O
corresponding	O
to	O
confidence	O
regions	O
one-dimensional	O
example	O
although	O
laplace	O
s	O
method	O
and	O
the	O
ep	O
approximation	O
often	O
give	O
similar	O
results	O
we	O
here	O
present	O
a	O
simple	O
one-dimensional	O
problem	O
which	O
highlights	O
some	O
of	O
the	O
differences	O
between	O
the	O
methods	O
the	O
data	O
shown	O
in	O
figure	O
consists	O
of	O
data	O
points	O
in	O
three	O
groups	O
generated	O
from	O
a	O
mixture	O
of	O
three	O
gaussians	O
centered	O
on	O
points	O
points	O
and	O
points	O
where	O
the	O
middle	O
component	O
has	O
label	O
and	O
the	O
two	O
other	O
components	O
label	O
all	O
components	O
have	O
standard	O
deviation	O
thus	O
the	O
two	O
left-most	O
components	O
are	O
well	O
separated	O
whereas	O
the	O
two	O
right-most	O
components	O
overlap	O
both	O
approximation	O
methods	O
are	O
shown	O
with	O
the	O
same	O
value	O
of	O
the	O
hyperparameters	B
and	O
f	O
chosen	O
to	O
maximize	O
the	O
approximate	O
marginal	B
likelihood	B
for	O
laplace	O
s	O
method	O
notice	O
in	O
figure	O
that	O
there	O
is	O
a	O
considerable	O
difference	O
in	O
the	O
value	O
of	O
the	O
predictive	B
probability	O
for	O
negative	O
inputs	O
the	O
laplace	B
approximation	I
seems	O
overly	O
cautious	O
given	O
the	O
very	O
clear	O
separation	O
of	O
the	O
data	O
this	O
effect	O
can	O
be	O
explained	O
as	O
a	O
consequence	O
of	O
the	O
intuition	O
that	O
the	O
influence	O
of	O
well-explained	O
data	O
points	O
is	O
effectively	O
reduced	O
see	O
the	O
discussion	O
around	O
eq	O
because	O
the	O
points	O
in	O
the	O
left	O
hand	O
cluster	O
are	O
relatively	O
well-explained	O
by	O
the	O
model	O
they	O
don	O
t	O
contribute	O
as	O
strongly	O
to	O
the	O
posterior	O
and	O
thus	O
the	O
predictive	B
probability	O
never	O
gets	O
very	O
close	O
to	O
notice	O
in	O
figure	O
the	O
confidence	O
region	O
for	O
the	O
latent	O
function	B
for	O
laplace	O
s	O
method	O
actually	O
includes	O
functions	O
that	O
are	O
negative	O
at	O
x	O
which	O
does	O
not	O
seem	O
appropriate	O
for	O
the	O
positive	O
examples	O
centered	O
around	O
x	O
on	O
the	O
right-hand	O
side	O
of	O
figure	O
this	O
effect	O
is	O
not	O
visible	O
because	O
the	O
points	O
around	O
the	O
transition	O
between	O
the	O
classes	O
at	O
x	O
are	O
not	O
so	O
well-explained	O
this	O
is	O
because	O
the	O
points	O
near	O
the	O
boundary	O
are	O
competing	O
against	O
the	O
points	O
from	O
the	O
other	O
class	O
attempting	O
to	O
pull	O
the	O
latent	O
function	B
in	O
opposite	O
directions	O
consequently	O
the	O
datapoints	O
in	O
this	O
region	O
all	O
contribute	O
strongly	O
xpredictive	O
probability	O
xlatent	O
function	B
fxlaplaceep	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
experiments	O
another	O
sign	O
of	O
this	O
effect	O
is	O
that	O
the	O
uncertainty	O
in	O
the	O
latent	O
function	B
which	O
is	O
closely	O
related	O
to	O
the	O
effective	O
local	O
density	O
of	O
the	O
data	O
is	O
very	O
small	O
in	O
the	O
region	O
around	O
x	O
the	O
small	O
uncertainty	O
reveals	O
a	O
high	O
effective	O
density	O
which	O
is	O
caused	O
by	O
all	O
data	O
points	O
in	O
the	O
region	O
contributing	O
with	O
full	O
weight	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
example	O
was	O
artificially	O
constructed	O
specifically	O
to	O
highlight	O
this	O
effect	O
finally	O
figure	O
also	O
shows	O
clearly	O
the	O
effects	O
of	O
uncertainty	O
in	O
the	O
latent	O
function	B
on	O
eq	O
in	O
the	O
region	O
between	O
x	O
to	O
x	O
the	O
latent	O
mean	O
in	O
panel	O
increases	O
slightly	O
but	O
the	O
predictive	B
probability	O
decreases	O
in	O
this	O
region	O
in	O
panel	O
this	O
is	O
caused	O
by	O
the	O
increase	O
in	O
uncertainty	O
for	O
the	O
latent	O
function	B
when	O
the	O
widely	O
varying	O
functions	O
are	O
squashed	O
through	O
the	O
nonlinearity	O
it	O
is	O
possible	O
for	O
both	O
classes	O
to	O
get	O
high	O
probability	O
and	O
the	O
average	O
prediction	O
becomes	O
less	O
extreme	O
binary	B
handwritten	O
digit	O
classification	B
example	O
handwritten	O
digit	O
and	O
character	O
recognition	O
are	O
popular	O
real-world	O
tasks	O
for	O
testing	O
and	O
benchmarking	O
classifiers	O
with	O
obvious	O
application	O
e	O
g	O
in	O
postal	O
services	O
in	O
this	O
section	O
we	O
consider	O
the	O
discrimination	O
of	O
images	O
of	O
the	O
digit	O
from	O
images	O
of	O
the	O
digit	O
as	O
an	O
example	O
of	O
binary	B
classification	B
the	O
specific	O
choice	O
was	O
guided	O
by	O
the	O
experience	O
that	O
this	O
is	O
probably	O
one	O
of	O
the	O
most	O
difficult	O
binary	B
subtasks	O
classification	B
of	O
the	O
digits	O
is	O
described	O
in	O
the	O
following	O
section	O
we	O
use	O
the	O
us	O
postal	O
service	O
database	O
of	O
handwritten	O
digits	O
which	O
consists	O
of	O
segmented	O
greyscale	O
images	O
normalized	O
so	O
that	O
the	O
intensity	O
of	O
the	O
pixels	O
lies	O
in	O
the	O
data	O
was	O
originally	O
split	O
into	O
a	O
training	O
set	B
of	O
cases	O
and	O
a	O
testset	O
of	O
the	O
remaining	O
cases	O
and	O
has	O
often	O
been	O
used	O
in	O
this	O
configuration	O
unfortunately	O
the	O
data	O
in	O
the	O
two	O
partitions	O
was	O
collected	O
in	O
slightly	O
different	O
ways	O
such	O
that	O
the	O
data	O
in	O
the	O
two	O
sets	O
did	O
not	O
stem	O
from	O
the	O
same	O
since	O
the	O
basic	O
underlying	O
assumption	O
for	O
most	O
machine	O
learning	B
algorithms	O
is	O
that	O
the	O
distribution	O
of	O
the	O
training	O
and	O
test	O
data	O
should	O
be	O
identical	O
the	O
original	O
data	O
partitions	O
are	O
not	O
really	O
suitable	O
as	O
a	O
test	O
bed	O
for	O
learning	B
algorithms	O
the	O
interpretation	O
of	O
the	O
results	O
being	O
hampered	O
by	O
the	O
change	O
in	O
distribution	O
secondly	O
the	O
original	O
test	O
set	B
was	O
rather	O
small	O
sometimes	O
making	O
it	O
difficult	O
to	O
differentiate	O
the	O
performance	O
of	O
different	O
algorithms	O
to	O
overcome	O
these	O
two	O
problems	O
we	O
decided	O
to	O
pool	O
the	O
two	O
partitions	O
and	O
randomly	O
split	O
the	O
data	O
into	O
two	O
identically	O
sized	O
partitions	O
of	O
cases	O
each	O
a	O
side-effect	O
is	O
that	O
it	O
is	O
not	O
trivial	O
to	O
compare	O
to	O
results	O
obtained	O
using	O
the	O
original	O
partitions	O
all	O
experiments	O
reported	O
here	O
use	O
the	O
repartitioned	O
data	O
the	O
binary	B
vs	O
data	O
has	O
training	O
cases	O
divided	O
on	O
vs	O
while	O
the	O
test	O
set	B
has	O
cases	O
split	O
usps	B
dataset	I
usps	B
repartitioned	O
we	O
present	O
results	O
of	O
both	O
laplace	O
s	O
method	O
and	O
ep	O
using	O
identical	O
experimental	O
setups	O
the	O
squared	B
exponential	B
covariance	B
function	B
kx	O
squared	B
exponential	B
covariance	B
function	B
is	O
well	O
known	O
e	O
g	O
that	O
the	O
original	O
test	O
partition	O
had	O
more	O
difficult	O
cases	O
than	O
the	O
training	O
set	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
figure	O
binary	B
laplace	B
approximation	I
vs	O
discrimination	O
using	O
the	O
usps	B
data	O
panel	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
log	O
marginal	B
likelihood	B
as	O
a	O
function	B
of	O
log	O
and	O
log	O
f	O
the	O
marginal	B
likelihood	B
has	O
an	O
optimum	O
at	O
log	O
and	O
log	O
f	O
with	O
an	O
optimum	O
value	O
of	O
log	O
pyx	O
panel	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
amount	O
of	O
information	O
excess	O
of	O
a	O
simple	O
base-line	O
model	O
see	O
text	O
about	O
the	O
test	O
cases	O
in	O
bits	B
as	O
a	O
function	B
of	O
the	O
same	O
variables	O
the	O
statistical	O
uncertainty	O
of	O
the	O
finite	O
number	O
of	O
test	O
cases	O
is	O
about	O
bits	B
confidence	O
interval	O
panel	O
shows	O
a	O
histogram	O
of	O
the	O
latent	O
means	O
for	O
the	O
training	O
and	O
test	O
sets	O
respectively	O
at	O
the	O
values	O
of	O
the	O
hyperparameters	B
with	O
optimal	B
marginal	B
likelihood	B
panel	O
panel	O
shows	O
the	O
number	O
of	O
test	O
errors	O
of	O
when	O
predicting	O
using	O
the	O
sign	O
of	O
the	O
latent	O
mean	O
f	O
exp	O
was	O
used	O
so	O
there	O
are	O
two	O
free	O
parameters	O
namely	O
f	O
process	O
standard	O
deviation	O
which	O
controls	O
its	O
vertical	O
scaling	O
and	O
the	O
length-scale	B
controls	O
the	O
input	O
length-scale	B
let	O
log	O
f	O
denote	O
the	O
vector	O
of	O
hyperparameters	B
we	O
first	O
present	O
the	O
results	O
of	O
laplace	O
s	O
method	O
in	O
figure	O
and	O
discuss	O
these	O
at	O
some	O
length	O
we	O
then	O
briefly	O
compare	O
these	O
with	O
the	O
results	O
of	O
the	O
ep	O
method	O
in	O
figure	O
hyperparameters	B
lengthscale	O
logllog	O
magnitude	O
log	O
flog	O
marginal	B
likelihood	B
lengthscale	O
logllog	O
magnitude	O
log	O
finformation	O
about	O
test	O
targets	O
in	O
means	O
ffrequencytraining	O
set	B
latent	O
means	O
means	O
ffrequencytest	O
set	B
latent	O
lengthscale	O
logllog	O
magnitude	O
log	O
of	O
test	O
misclassifications	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
experiments	O
figure	O
the	O
ep	O
algorithm	O
on	O
vs	O
digit	O
discrimination	O
task	O
from	O
the	O
usps	B
data	O
panel	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
log	O
marginal	B
likelihood	B
as	O
a	O
function	B
of	O
the	O
hyperparameters	B
log	O
and	O
log	O
f	O
the	O
marginal	B
likelihood	B
has	O
an	O
optimum	O
at	O
log	O
at	O
the	O
maximum	O
value	O
of	O
log	O
f	O
but	O
the	O
log	O
marginal	B
likelihood	B
is	O
essentially	O
flat	O
as	O
a	O
function	B
of	O
log	O
f	O
in	O
this	O
region	O
so	O
a	O
good	O
point	O
is	O
at	O
log	O
f	O
where	O
the	O
log	O
marginal	B
likelihood	B
has	O
a	O
value	O
of	O
panel	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
amount	O
of	O
information	O
excess	O
of	O
the	O
baseline	O
model	O
about	O
the	O
test	O
cases	O
in	O
bits	B
as	O
a	O
function	B
of	O
the	O
same	O
variables	O
zero	O
bits	B
corresponds	O
to	O
no	O
information	O
and	O
one	O
bit	O
to	O
perfect	O
binary	B
generalization	B
the	O
test	O
cases	O
allows	O
the	O
information	O
to	O
be	O
determined	O
within	O
bits	B
panel	O
shows	O
a	O
histogram	O
of	O
the	O
latent	O
means	O
for	O
the	O
training	O
and	O
test	O
sets	O
respectively	O
at	O
the	O
values	O
of	O
the	O
hyperparameters	B
with	O
optimal	B
marginal	B
likelihood	B
panel	O
a	O
panel	O
shows	O
the	O
number	O
of	O
test	O
errors	O
of	O
when	O
predicting	O
using	O
the	O
sign	O
of	O
the	O
latent	O
mean	O
in	O
figure	O
we	O
show	O
a	O
contour	O
plot	O
of	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
log	O
qyx	O
as	O
a	O
function	B
of	O
log	O
and	O
log	O
f	O
obtained	O
from	O
runs	O
on	O
a	O
grid	O
of	O
evenly-spaced	O
values	O
of	O
log	O
and	O
evenly-spaced	O
values	O
of	O
log	O
f	O
notice	O
that	O
there	O
is	O
a	O
maximum	O
of	O
the	O
marginal	B
likelihood	B
laplace	O
results	O
lengthscale	O
logllog	O
magnitude	O
log	O
flog	O
marginal	B
likelihood	B
lengthscale	O
logllog	O
magnitude	O
log	O
finformation	O
about	O
test	O
targets	O
in	O
means	O
ffrequencytraining	O
set	B
latent	O
means	O
means	O
ffrequencytest	O
set	B
latent	O
lengthscale	O
logllog	O
magnitude	O
log	O
of	O
test	O
misclassifications	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
test	O
log	O
predictive	B
probability	O
base-line	O
method	O
interpretation	O
of	O
information	O
score	O
error	O
rate	O
near	O
log	O
and	O
log	O
f	O
as	O
will	O
be	O
explained	O
in	O
chapter	O
we	O
would	O
expect	O
that	O
hyperparameters	B
that	O
yield	O
a	O
high	O
marginal	B
likelihood	B
would	O
give	O
rise	O
to	O
good	O
predictions	O
notice	O
that	O
an	O
increase	O
of	O
unit	O
on	O
the	O
log	O
scale	O
means	O
that	O
the	O
probability	O
is	O
times	O
larger	O
so	O
the	O
marginal	B
likelihood	B
in	O
figure	O
is	O
fairly	O
well	O
peaked	O
there	O
are	O
at	O
least	O
two	O
ways	O
we	O
can	O
measure	B
the	O
quality	O
of	O
predictions	O
at	O
the	O
test	O
points	O
the	O
first	O
is	O
the	O
test	O
log	O
predictive	B
probability	O
py	O
in	O
figure	O
we	O
plot	O
the	O
average	O
over	O
the	O
test	O
set	B
of	O
the	O
test	O
log	O
predictive	B
probability	O
for	O
the	O
same	O
range	O
of	O
hyperparameters	B
we	O
express	O
this	O
as	O
the	O
amount	O
of	O
information	O
in	O
bits	B
about	O
the	O
targets	O
by	O
using	O
log	O
to	O
the	O
base	O
further	O
we	O
off-set	O
the	O
value	O
by	O
subtracting	O
the	O
amount	O
of	O
information	O
that	O
a	O
simple	O
base-line	O
method	O
would	O
achieve	O
as	O
a	O
base-line	O
model	O
we	O
use	O
the	O
best	O
possible	O
model	O
which	O
does	O
not	O
use	O
the	O
inputs	O
in	O
this	O
case	O
this	O
model	O
would	O
just	O
produce	O
a	O
predictive	B
distribution	O
reflecting	O
the	O
frequency	O
of	O
the	O
two	O
classes	O
in	O
the	O
training	O
set	B
i	O
e	O
bits	B
essentially	O
bit	O
the	O
classes	O
had	O
been	O
perfectly	O
balanced	O
and	O
the	O
training	O
and	O
test	O
partitions	O
also	O
exactly	O
balanced	O
we	O
would	O
arrive	O
at	O
exactly	O
bit	O
thus	O
our	O
scaled	O
information	O
score	O
used	O
in	O
figure	O
would	O
be	O
zero	O
for	O
a	O
method	O
that	O
did	O
random	O
guessing	O
and	O
bit	O
for	O
a	O
method	O
which	O
did	O
perfect	O
classification	B
complete	O
confidence	O
the	O
information	O
score	O
measures	O
how	O
much	O
information	O
the	O
model	O
was	O
able	O
to	O
extract	O
from	O
the	O
inputs	O
about	O
the	O
identity	O
of	O
the	O
output	O
note	O
that	O
this	O
is	O
not	O
the	O
mutual	O
information	O
between	O
the	O
model	O
output	O
and	O
the	O
test	O
targets	O
but	O
rather	O
the	O
kullback-leibler	B
divergence	I
between	O
them	O
figure	O
shows	O
that	O
there	O
is	O
a	O
good	O
qualitative	O
agreement	O
between	O
the	O
marginal	B
likelihood	B
and	O
the	O
test	O
information	O
compare	O
panels	O
and	O
the	O
second	O
perhaps	O
most	O
commonly	O
used	O
method	O
for	O
measuring	O
the	O
quality	O
of	O
the	O
predictions	O
is	O
to	O
compute	O
the	O
number	O
of	O
test	O
errors	O
made	O
when	O
using	O
the	O
predictions	O
this	O
is	O
done	O
by	O
computing	O
eq	O
eq	O
for	O
each	O
test	O
point	O
thresholding	O
at	O
to	O
get	O
hard	O
predictions	O
and	O
counting	O
the	O
number	O
of	O
errors	O
figure	O
shows	O
the	O
number	O
of	O
errors	O
produced	O
for	O
each	O
entry	O
in	O
the	O
grid	O
of	O
values	O
for	O
the	O
hyperparameters	B
the	O
general	O
trend	O
in	O
this	O
table	O
is	O
that	O
the	O
number	O
of	O
errors	O
is	O
lowest	O
in	O
the	O
top	O
left-hand	O
corner	O
and	O
increases	O
as	O
one	O
moves	O
right	O
and	O
downwards	O
the	O
number	O
of	O
errors	O
rises	O
dramatically	O
in	O
the	O
far	O
bottom	O
righthand	O
corner	O
however	O
note	O
in	O
general	O
that	O
the	O
number	O
of	O
errors	O
is	O
quite	O
small	O
are	O
cases	O
in	O
the	O
test	O
set	B
the	O
qualitative	O
differences	O
between	O
the	O
two	O
evaluation	O
criteria	O
depicted	O
in	O
figure	O
panels	O
and	O
may	O
at	O
first	O
sight	O
seem	O
alarming	O
and	O
although	O
panels	O
and	O
show	O
similar	O
trends	O
one	O
may	O
worry	O
about	O
using	O
to	O
select	O
the	O
hyperparameters	B
if	O
one	O
is	O
interested	O
in	O
minimizing	O
the	O
test	O
misclassification	O
rate	O
indeed	O
a	O
full	O
understanding	O
of	O
all	O
aspects	O
of	O
these	O
plots	O
is	O
quite	O
involved	O
but	O
as	O
the	O
following	O
discussion	O
suggests	O
we	O
can	O
explain	O
the	O
major	O
trends	O
first	O
bear	O
in	O
mind	O
that	O
the	O
effect	O
of	O
increasing	O
is	O
to	O
make	O
the	O
kernel	B
function	B
broader	O
so	O
we	O
might	O
expect	O
to	O
observe	O
effects	O
like	O
those	O
in	O
figure	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
experiments	O
where	O
large	O
widths	O
give	O
rise	O
to	O
a	O
lack	O
of	O
flexibility	O
keeping	O
constant	O
the	O
effect	O
of	O
increasing	O
f	O
is	O
to	O
increase	O
the	O
magnitude	O
of	O
the	O
values	O
obtained	O
for	O
f	O
by	O
itself	O
this	O
would	O
lead	O
to	O
harder	O
predictions	O
predictive	B
probabilities	O
closer	O
to	O
or	O
but	O
we	O
have	O
to	O
bear	O
in	O
mind	O
that	O
the	O
variances	O
associated	O
will	O
also	O
increase	O
and	O
this	O
increased	O
uncertainty	O
for	O
the	O
latent	O
variables	O
tends	O
to	O
soften	O
the	O
predictive	B
probabilities	O
i	O
e	O
move	O
them	O
closer	O
to	O
the	O
most	O
marked	O
difference	O
between	O
figure	O
and	O
is	O
the	O
behaviour	O
in	O
the	O
the	O
top	O
left	O
corner	O
where	O
classification	B
error	O
rate	O
remains	O
small	O
but	O
the	O
test	O
information	O
and	O
marginal	B
likelihood	B
are	O
both	O
poor	O
in	O
the	O
left	O
hand	O
side	O
of	O
the	O
plots	O
the	O
length	O
scale	O
is	O
very	O
short	O
this	O
causes	O
most	O
points	O
to	O
be	O
deemed	O
far	O
away	O
from	O
most	O
other	O
points	O
in	O
this	O
regime	O
the	O
prediction	O
is	O
dominated	O
by	O
the	O
class-label	O
of	O
the	O
nearest	O
neighbours	O
and	O
for	O
the	O
task	O
at	O
hand	O
this	O
happens	O
to	O
give	O
a	O
low	O
misclassification	O
rate	O
in	O
this	O
parameter	O
region	O
the	O
test	O
latent	O
variables	O
f	O
are	O
very	O
close	O
to	O
zero	O
corresponding	O
to	O
probabilities	O
very	O
close	O
to	O
consequently	O
the	O
predictive	B
probabilities	O
carry	O
almost	O
no	O
information	O
about	O
the	O
targets	O
in	O
the	O
top	O
left	O
corner	O
the	O
predictive	B
probabilities	O
for	O
all	O
test	O
cases	O
lie	O
in	O
the	O
interval	O
notice	O
that	O
a	O
large	O
amount	O
of	O
information	O
implies	O
a	O
high	O
degree	O
of	O
correct	O
classification	B
but	O
not	O
vice	O
versa	O
at	O
the	O
optimal	B
marginal	B
likelihood	B
values	O
of	O
the	O
hyperparameters	B
there	O
are	O
misclassifications	O
which	O
is	O
slightly	O
higher	O
that	O
the	O
minimum	O
number	O
attained	O
which	O
is	O
errors	O
in	O
exercise	O
readers	O
are	O
encouraged	O
to	O
investigate	O
further	O
the	O
behaviour	O
of	O
f	O
and	O
the	O
predictive	B
probabilities	O
etc	O
as	O
functions	O
of	O
log	O
and	O
log	O
f	O
for	O
themselves	O
in	O
figure	O
we	O
show	O
the	O
results	O
on	O
the	O
same	O
experiment	O
using	O
the	O
ep	O
method	O
the	O
findings	O
are	O
qualitatively	O
similar	O
but	O
there	O
are	O
significant	O
differences	O
in	O
panel	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
has	O
a	O
different	O
shape	O
than	O
for	O
laplace	O
s	O
method	O
and	O
the	O
maximum	O
of	O
the	O
log	O
marginal	B
likelihood	B
is	O
about	O
units	O
on	O
a	O
natural	O
log	O
scale	O
larger	O
the	O
marginal	B
probability	O
is	O
times	O
higher	O
also	O
note	O
that	O
the	O
marginal	B
likelihood	B
has	O
a	O
ridge	B
log	O
that	O
extends	O
into	O
large	O
values	O
of	O
log	O
f	O
for	O
these	O
very	O
large	O
latent	O
amplitudes	O
also	O
panel	O
the	O
probit	B
likelihood	B
function	B
is	O
well	O
approximated	O
by	O
a	O
step	O
function	B
it	O
transitions	O
from	O
low	O
to	O
high	O
values	O
in	O
the	O
domain	O
once	O
we	O
are	O
in	O
this	O
regime	O
it	O
is	O
of	O
course	O
irrelevant	O
exactly	O
how	O
large	O
the	O
magnitude	O
is	O
thus	O
the	O
ridge	B
notice	O
however	O
that	O
this	O
does	O
not	O
imply	O
that	O
the	O
prediction	O
will	O
always	O
be	O
hard	O
since	O
the	O
variance	O
of	O
the	O
latent	O
function	B
also	O
grows	O
figure	O
shows	O
a	O
good	O
qualitative	O
agreement	O
between	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
and	O
the	O
test	O
information	O
compare	O
panels	O
and	O
the	O
best	O
value	O
of	O
the	O
test	O
information	O
is	O
significantly	O
higher	O
for	O
ep	O
than	O
for	O
laplace	O
s	O
method	O
the	O
classification	B
error	O
rates	O
in	O
panel	O
show	O
a	O
fairly	O
similar	O
behaviour	O
to	O
that	O
of	O
laplace	O
s	O
method	O
in	O
figure	O
we	O
show	O
the	O
latent	O
means	O
for	O
training	O
and	O
test	O
cases	O
these	O
show	O
a	O
clear	O
separation	O
on	O
the	O
training	O
set	B
and	O
much	O
larger	O
magnitudes	O
than	O
for	O
laplace	O
s	O
method	O
the	O
absolute	O
values	O
of	O
the	O
entries	O
in	O
f	O
are	O
quite	O
large	O
often	O
well	O
in	O
excess	O
of	O
which	O
may	O
suggest	O
very	O
hard	O
predictions	O
close	O
to	O
zero	O
or	O
one	O
ep	O
results	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
figure	O
map	B
vs	O
averaged	B
predictions	O
for	O
the	O
ep	O
algorithm	O
for	O
the	O
s	O
vs	O
s	O
digit	O
discrimination	O
using	O
the	O
usps	B
data	O
the	O
optimal	B
values	O
of	O
the	O
hyperparameters	B
from	O
figure	O
log	O
and	O
log	O
f	O
are	O
used	O
the	O
map	B
predictions	O
are	O
hard	O
mostly	O
being	O
very	O
close	O
to	O
zero	O
or	O
one	O
on	O
the	O
other	O
hand	O
the	O
averaged	B
predictions	O
eq	O
from	O
eq	O
are	O
a	O
lot	O
less	O
extreme	O
in	O
panel	O
the	O
cases	O
that	O
were	O
misclassified	O
are	O
indicated	O
by	O
crosses	O
classified	O
cases	O
are	O
shown	O
by	O
points	O
note	O
that	O
only	O
of	O
the	O
misclassified	O
points	O
have	O
confident	O
predictions	O
outside	O
notice	O
that	O
all	O
points	O
fall	O
in	O
the	O
triangles	O
below	O
and	O
above	O
the	O
horizontal	O
line	O
confirming	O
that	O
averaging	O
does	O
not	O
change	O
the	O
most	O
probable	O
class	O
and	O
that	O
it	O
always	O
makes	O
the	O
probabilities	O
less	O
extreme	O
closer	O
to	O
panel	O
shows	O
histograms	O
of	O
averaged	B
and	O
map	B
predictions	O
where	O
we	O
have	O
truncated	O
values	O
over	O
since	O
the	O
sigmoid	O
saturates	O
for	O
smaller	O
arguments	O
however	O
when	O
taking	O
the	O
uncertainties	O
in	O
the	O
latent	O
variables	O
into	O
account	O
and	O
computing	O
the	O
predictions	O
using	O
averaging	O
as	O
in	O
eq	O
the	O
predictive	B
probabilities	O
are	O
softened	O
in	O
figure	O
we	O
can	O
verify	O
that	O
the	O
averaged	B
predictive	B
probabilities	O
are	O
much	O
less	O
extreme	O
than	O
the	O
map	B
predictions	O
in	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
the	O
two	O
approximate	O
methods	O
for	O
gp	O
classification	B
we	O
compared	O
to	O
a	O
linear	B
probit	B
model	O
a	O
support	B
vector	I
machine	I
a	O
least-squares	B
classifier	O
and	O
a	O
nearest	O
neighbour	O
approach	O
all	O
of	O
which	O
are	O
commonly	O
used	O
in	O
the	O
machine	O
learning	B
community	O
in	O
figure	O
we	O
show	O
error-reject	O
curves	O
for	O
both	O
misclassification	O
rate	O
and	O
the	O
test	O
information	O
measure	B
the	O
error-reject	B
curve	I
shows	O
how	O
the	O
performance	O
develops	O
as	O
a	O
function	B
of	O
the	O
fraction	O
of	O
test	O
cases	O
that	O
is	O
being	O
rejected	O
to	O
compute	O
these	O
we	O
first	O
modify	O
the	O
methods	O
that	O
do	O
not	O
naturally	O
produce	O
probabilistic	B
predictions	O
to	O
do	O
so	O
as	O
described	O
below	O
based	O
on	O
the	O
predictive	B
probabilities	O
we	O
reject	O
test	O
cases	O
for	O
which	O
the	O
maximum	O
predictive	B
probability	O
is	O
smaller	O
than	O
a	O
threshold	O
varying	O
the	O
threshold	O
produces	O
the	O
error-reject	B
curve	I
the	O
gp	O
classifiers	O
applied	O
in	O
figure	O
used	O
the	O
hyperparameters	B
which	O
optimized	O
the	O
approximate	O
marginal	B
likelihood	B
for	O
each	O
of	O
the	O
two	O
methods	O
for	O
the	O
gp	O
classifiers	O
there	O
were	O
two	O
free	O
parameters	O
f	O
and	O
the	O
linear	B
probit	B
model	O
logistic	B
models	O
are	O
probably	O
more	O
common	O
but	O
we	O
chose	O
the	O
probit	B
here	O
since	O
the	O
other	O
likelihood	B
based	O
methods	O
all	O
used	O
probit	B
can	O
be	O
error-reject	B
curve	I
linear	B
probit	B
model	O
map	B
averagedfrequency	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
experiments	O
figure	O
panel	O
shows	O
the	O
error-reject	B
curve	I
and	O
panel	O
the	O
amount	O
of	O
information	O
about	O
the	O
test	O
cases	O
as	O
a	O
function	B
of	O
the	O
rejection	O
rate	O
the	O
probabilistic	B
one	I
nearest	I
neighbour	I
method	O
has	O
much	O
worse	O
performance	O
than	O
the	O
other	O
methods	O
gaussian	O
processes	O
with	O
ep	O
behaves	O
similarly	O
to	O
svm	O
s	O
although	O
the	O
classification	B
rate	O
for	O
svm	O
for	O
low	O
rejection	O
rates	O
seems	O
to	O
be	O
a	O
little	O
better	O
laplace	O
s	O
method	O
is	O
worse	O
than	O
ep	O
and	O
svm	O
the	O
gp	O
least	O
squares	O
classifier	O
described	O
in	O
section	O
performs	O
the	O
best	O
implemented	O
as	O
gp	O
model	O
using	O
laplace	O
s	O
method	O
which	O
is	O
equivalent	B
to	O
not	O
computationally	O
as	O
efficient	O
as	O
iteratively	B
reweighted	I
least	I
squares	I
the	O
covariance	B
function	B
kx	O
has	O
a	O
single	O
hyperparameter	O
which	O
was	O
set	B
by	O
maximizing	O
the	O
log	O
marginal	B
likelihood	B
this	O
gives	O
log	O
pyx	O
at	O
thus	O
the	O
marginal	B
likelihood	B
for	O
the	O
linear	B
covariance	B
function	B
is	O
about	O
units	O
on	O
a	O
natural	O
log	O
scale	O
lower	O
than	O
the	O
maximum	O
log	O
marginal	B
likelihood	B
for	O
the	O
laplace	B
approximation	I
using	O
the	O
squared	B
exponential	B
covariance	B
function	B
the	O
support	B
vector	I
machine	I
classifier	O
section	O
for	O
further	O
details	O
on	O
the	O
svm	O
used	O
the	O
same	O
se	O
kernel	B
as	O
the	O
gp	O
classifiers	O
for	O
the	O
svm	O
the	O
r	O
ole	O
of	O
is	O
identical	O
and	O
the	O
trade-off	O
parameter	O
c	O
in	O
the	O
svm	O
formulation	O
f	O
we	O
carried	O
out	O
cross	O
validation	O
eq	O
plays	O
a	O
similar	O
r	O
ole	O
to	O
on	O
a	O
grid	O
in	O
parameter	O
space	O
to	O
identify	O
the	O
best	O
combination	O
of	O
parameters	O
w	O
r	O
t	O
the	O
error	O
rate	O
this	O
turned	O
out	O
to	O
be	O
at	O
c	O
our	O
experiments	O
were	O
conducted	O
using	O
the	O
svmtorch	O
software	O
and	O
bengio	O
in	O
order	O
to	O
compute	O
probabilistic	B
predictions	O
we	O
squashed	O
the	O
test-activities	O
through	O
a	O
cumulative	O
gaussian	O
using	O
the	O
methods	O
proposed	O
by	O
platt	O
we	O
made	O
a	O
parameterized	O
linear	B
transformation	O
of	O
the	O
test-activities	O
and	O
fed	O
this	O
through	O
the	O
cumulative	O
the	O
parameters	O
of	O
the	O
linear	B
transformation	O
were	O
chosen	O
to	O
maximize	O
the	O
log	O
predictive	B
probability	O
evaluated	O
on	O
the	O
hold-out	O
sets	O
of	O
the	O
cross	O
validation	O
the	O
probabilistic	B
one	I
nearest	I
neighbour	I
method	O
is	O
a	O
simple	O
natural	O
extension	O
to	O
the	O
classical	O
one	O
nearest	O
neighbour	O
method	O
which	O
provides	O
probabilistic	B
predictions	O
it	O
computes	O
the	O
leave-one-out	B
one	O
nearest	O
neighbour	O
prediction	O
on	O
the	O
training	O
set	B
and	O
records	O
the	O
fraction	O
of	O
cases	O
where	O
the	O
loo	O
predictions	O
were	O
correct	O
on	O
test	O
cases	O
the	O
method	O
then	O
pre	O
used	O
a	O
logistic	B
whereas	O
we	O
use	O
a	O
cumulative	O
gaussian	O
support	B
vector	I
machine	I
probabilistic	B
one	I
nearest	I
neighbour	I
ratemisclassification	O
ratetest	O
information	O
probit	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
dicts	O
the	O
one	O
nearest	O
neighbour	O
class	O
with	O
probability	O
and	O
the	O
other	O
class	O
with	O
probability	O
rejections	O
are	O
based	O
on	O
thresholding	O
on	O
the	O
distance	O
to	O
the	O
nearest	O
neighbour	O
the	O
least-squares	B
classifier	O
is	O
described	O
in	O
section	O
in	O
order	O
to	O
produce	O
probabilistic	B
predictions	O
the	O
method	O
of	O
platt	O
was	O
used	O
described	O
above	O
for	O
the	O
svm	O
using	O
the	O
predictive	B
means	O
only	O
predictive	B
variances	O
were	O
except	O
that	O
instead	O
of	O
the	O
cross	O
validation	O
leave-one-out	B
cross-validation	B
was	O
used	O
and	O
the	O
kernel	B
parameters	O
were	O
also	O
set	B
using	O
loo-cv	O
figure	O
shows	O
that	O
the	O
three	O
best	O
methods	O
are	O
the	O
ep	O
approximation	O
for	O
gpc	O
the	O
svm	O
and	O
the	O
least-squares	B
classifier	O
presenting	O
both	O
the	O
error	O
rates	O
and	O
the	O
test	O
information	O
helps	O
to	O
highlight	O
differences	O
which	O
may	O
not	O
be	O
apparent	O
from	O
a	O
single	O
plot	O
alone	O
for	O
example	O
laplace	O
s	O
method	O
and	O
ep	O
seem	O
very	O
similar	O
on	O
error	O
rates	O
but	O
quite	O
different	O
in	O
test	O
information	O
notice	O
also	O
that	O
the	O
error-reject	B
curve	I
itself	O
reveals	O
interesting	O
differences	O
e	O
g	O
notice	O
that	O
although	O
the	O
method	O
has	O
an	O
error	O
rate	O
comparable	O
to	O
other	O
methods	O
at	O
zero	O
rejections	O
things	O
don	O
t	O
improve	O
very	O
much	O
when	O
rejections	O
are	O
allowed	O
refer	O
to	O
section	O
for	O
more	O
discussion	O
of	O
the	O
results	O
handwritten	O
digit	O
classification	B
example	O
we	O
apply	O
the	O
multi-class	B
laplace	B
approximation	I
developed	O
in	O
section	O
to	O
the	O
handwritten	O
digit	O
classification	B
problem	O
from	O
the	O
usps	B
dataset	I
having	O
n	O
training	O
cases	O
and	O
n	O
cases	O
for	O
testing	O
see	O
page	O
we	O
used	O
a	O
squared	B
exponential	B
covariance	B
function	B
with	O
two	O
hyperparameters	B
a	O
single	O
signal	O
amplitude	O
f	O
common	O
to	O
all	O
latent	O
functions	O
and	O
a	O
single	O
length-scale	B
parameter	O
common	O
to	O
all	O
latent	O
functions	O
and	O
common	O
to	O
all	O
input	O
dimensions	O
the	O
behaviour	O
of	O
the	O
method	O
was	O
investigated	O
on	O
a	O
grid	O
of	O
values	O
for	O
the	O
hyperparameters	B
see	O
figure	O
note	O
that	O
the	O
correspondence	O
between	O
the	O
log	O
marginal	B
likelihood	B
and	O
the	O
test	O
information	O
is	O
not	O
as	O
close	O
as	O
for	O
laplace	O
s	O
method	O
for	O
binary	B
classification	B
in	O
figure	O
on	O
page	O
the	O
maximum	O
value	O
of	O
the	O
log	O
marginal	B
likelihood	B
attained	O
is	O
and	O
for	O
the	O
hyperparameters	B
corresponding	O
to	O
this	O
point	O
the	O
error	O
rate	O
is	O
and	O
the	O
test	O
information	O
bits	B
as	O
with	O
the	O
binary	B
classification	B
problem	O
the	O
test	O
information	O
is	O
standardized	O
by	O
subtracting	O
off	O
the	O
negative	O
entropy	B
of	O
the	O
targets	O
which	O
is	O
bits	B
the	O
classification	B
error	O
rate	O
in	O
figure	O
shows	O
a	O
clear	O
minimum	O
and	O
this	O
is	O
also	O
attained	O
at	O
a	O
shorter	O
length-scale	B
than	O
where	O
the	O
marginal	B
likelihood	B
and	O
test	O
information	O
have	O
their	O
maxima	O
this	O
effect	O
was	O
also	O
seen	O
in	O
the	O
experiments	O
on	O
binary	B
classification	B
to	O
gain	O
some	O
insight	O
into	O
the	O
level	O
of	O
performance	O
we	O
compared	O
these	O
results	O
with	O
those	O
obtained	O
with	O
the	O
probabilistic	B
one	I
nearest	I
neighbour	I
method	O
a	O
multiple	O
logistic	B
regression	I
model	O
and	O
a	O
svm	O
the	O
first	O
uses	O
an	O
course	O
one	O
could	O
also	O
have	O
tried	O
a	O
variant	O
where	O
the	O
full	O
latent	O
predictive	B
distribution	O
was	O
averaged	B
over	O
but	O
we	O
did	O
not	O
do	O
that	O
here	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
experiments	O
figure	O
digit	O
classification	B
using	O
the	O
laplace	B
approximation	I
panel	O
shows	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
reaching	O
a	O
maximum	O
value	O
of	O
log	O
pyx	O
at	O
log	O
and	O
log	O
f	O
in	O
panel	O
information	O
about	O
the	O
test	O
cases	O
is	O
shown	O
the	O
maximum	O
possible	O
amount	O
of	O
information	O
about	O
the	O
test	O
targets	O
corresponding	O
to	O
perfect	O
classification	B
would	O
be	O
bits	B
entropy	B
of	O
the	O
targets	O
at	O
the	O
point	O
of	O
maximum	O
marginal	B
likelihood	B
the	O
test	O
information	O
is	O
bits	B
in	O
panel	O
the	O
test	O
set	B
misclassification	O
rate	O
is	O
shown	O
in	O
percent	O
at	O
the	O
point	O
of	O
maximum	O
marginal	B
likelihood	B
the	O
test	O
error	O
rate	O
is	O
internal	O
leave-one-out	B
assessment	O
on	O
the	O
training	O
set	B
to	O
estimate	O
its	O
probability	O
of	O
being	O
correct	O
for	O
the	O
test	O
set	B
it	O
then	O
predicts	O
the	O
nearest	O
neighbour	O
with	O
probability	O
and	O
all	O
other	O
classes	O
with	O
equal	O
probability	O
we	O
obtained	O
a	O
test	O
information	O
of	O
bits	B
and	O
a	O
test	O
set	B
classification	B
error	O
rate	O
of	O
we	O
also	O
compare	O
to	O
multiple	O
linear	B
logistic	B
regression	I
one	O
way	O
to	O
implement	O
this	O
method	O
is	O
to	O
view	O
it	O
as	O
a	O
gaussian	B
process	I
with	O
a	O
linear	B
covariance	B
lengthscale	O
logllog	O
magnitude	O
log	O
flog	O
marginal	B
likelihood	B
lengthscale	O
logllog	O
magnitude	O
log	O
finformation	O
about	O
the	O
test	O
targets	O
in	O
lengthscale	O
logllog	O
magnitude	O
log	O
ftest	O
set	B
misclassification	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
function	B
although	O
it	O
is	O
equivalent	B
and	O
computationally	O
more	O
efficient	O
to	O
do	O
the	O
laplace	B
approximation	I
over	O
the	O
weights	O
of	O
the	O
linear	B
model	O
in	O
our	O
case	O
there	O
are	O
weights	O
inputs	O
and	O
one	O
bias	O
whereas	O
there	O
are	O
latent	O
function	B
values	O
in	O
the	O
gp	O
the	O
linear	B
covariance	B
function	B
kx	O
has	O
a	O
single	O
hyperparameter	O
for	O
all	O
latent	O
functions	O
optimizing	O
the	O
log	O
marginal	B
likelihood	B
w	O
r	O
t	O
gives	O
log	O
pyx	O
at	O
using	O
this	O
value	O
for	O
the	O
hyperparameter	O
the	O
test	O
information	O
is	O
bits	B
and	O
the	O
test	O
set	B
error	O
rate	O
is	O
finally	O
a	O
support	B
vector	I
machine	I
classifier	O
was	O
trained	O
using	O
the	O
same	O
se	O
kernel	B
as	O
the	O
gaussian	B
process	I
classifiers	O
section	O
for	O
further	O
details	O
on	O
the	O
svm	O
as	O
in	O
the	O
binary	B
svm	O
case	O
there	O
were	O
two	O
free	O
parameters	O
length-scale	B
of	O
the	O
kernel	B
and	O
the	O
trade-off	O
parameter	O
c	O
eq	O
f	O
we	O
carried	O
out	O
cross-validation	B
on	O
a	O
grid	O
which	O
plays	O
a	O
similar	O
r	O
ole	O
to	O
in	O
parameter	O
space	O
to	O
identify	O
the	O
best	O
combination	O
of	O
parameters	O
w	O
r	O
t	O
the	O
error	O
rate	O
this	O
turned	O
out	O
to	O
be	O
at	O
c	O
our	O
experiments	O
were	O
conducted	O
using	O
the	O
svmtorch	O
software	O
and	O
bengio	O
which	O
implements	O
multi-class	B
svm	O
classification	B
using	O
the	O
one-versus-rest	B
method	O
described	O
in	O
section	O
the	O
test	O
set	B
error	O
rate	O
for	O
the	O
svm	O
is	O
we	O
did	O
not	O
attempt	O
to	O
evaluate	O
the	O
test	O
information	O
for	O
the	O
multi-class	B
svm	O
discussion	O
in	O
the	O
previous	O
section	O
we	O
presented	O
several	O
sets	O
of	O
experiments	O
comparing	O
the	O
two	O
approximate	O
methods	O
for	O
inference	O
in	O
gpc	O
models	O
and	O
comparing	O
them	O
to	O
other	O
commonly-used	O
supervised	B
learning	B
methods	O
in	O
this	O
section	O
we	O
discuss	O
the	O
results	O
and	O
attempt	O
to	O
relate	O
them	O
to	O
the	O
properties	O
of	O
the	O
models	O
for	O
the	O
binary	B
examples	O
from	O
figures	O
and	O
we	O
saw	O
that	O
the	O
two	O
approximations	O
showed	O
quite	O
different	O
qualitative	O
behaviour	O
of	O
the	O
approximated	O
log	O
marginal	B
likelihood	B
although	O
the	O
exact	O
marginal	B
likelihood	B
is	O
of	O
course	O
identical	O
the	O
ep	O
approximation	O
gave	O
a	O
higher	O
maximum	O
value	O
of	O
the	O
log	O
marginal	B
likelihood	B
about	O
units	O
on	O
the	O
log	O
scale	O
and	O
the	O
test	O
information	O
was	O
somewhat	O
better	O
than	O
for	O
laplace	O
s	O
method	O
although	O
the	O
test	O
set	B
error	O
rates	O
were	O
comparable	O
however	O
although	O
this	O
experiment	O
seems	O
to	O
favour	O
the	O
ep	O
approximation	O
it	O
is	O
interesting	O
to	O
know	O
how	O
close	O
these	O
approximations	O
are	O
to	O
the	O
exact	O
intractable	O
solutions	O
in	O
figure	O
we	O
show	O
the	O
results	O
of	O
running	O
a	O
sophisticated	O
markov	B
chain	I
monte	I
carlo	I
method	O
called	O
annealed	O
importance	O
sampling	O
carried	O
out	O
by	O
kuss	O
and	O
rasmussen	O
the	O
usps	B
dataset	I
for	O
these	O
experiments	O
was	O
identical	O
to	O
the	O
one	O
used	O
in	O
figures	O
and	O
so	O
the	O
results	O
are	O
directly	O
comparable	O
it	O
is	O
seen	O
that	O
the	O
mcmc	O
results	O
indicate	O
that	O
the	O
ep	O
method	O
achieves	O
a	O
very	O
high	O
level	O
of	O
accuracy	O
i	O
e	O
that	O
the	O
difference	O
between	O
ep	O
and	O
laplace	O
s	O
method	O
is	O
caused	O
almost	O
exclusively	O
by	O
approximation	O
errors	O
in	O
laplace	O
s	O
method	O
the	O
main	O
reason	O
for	O
the	O
inaccuracy	O
of	O
laplace	O
s	O
method	O
is	O
that	O
the	O
high	O
dimensional	O
posterior	O
is	O
skew	O
and	O
that	O
the	O
symmetric	O
approximation	O
centered	O
on	O
the	O
mode	O
is	O
not	O
characterizing	O
the	O
posterior	O
volume	O
very	O
well	O
the	O
posterior	O
monte	O
carlo	O
results	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
discussion	O
figure	O
the	O
log	O
marginal	B
likelihood	B
panel	O
and	O
test	O
information	O
panel	O
for	O
the	O
usps	B
s	O
vs	O
s	O
binary	B
classification	B
task	O
computed	O
using	O
markov	B
chain	I
monte	I
carlo	I
comparing	O
this	O
to	O
the	O
laplace	B
approximation	I
figure	O
and	O
figure	O
shows	O
that	O
the	O
ep	O
approximation	O
is	O
surprisingly	O
accurate	O
the	O
slight	O
wiggliness	O
of	O
the	O
contour	O
lines	O
are	O
caused	O
by	O
finite	O
sample	O
effects	O
in	O
the	O
mcmc	O
runs	O
is	O
a	O
combination	O
of	O
the	O
gaussian	O
prior	O
centered	O
on	O
the	O
origin	O
and	O
the	O
likelihood	B
terms	O
which	O
cut	O
off	O
half-spaces	O
which	O
do	O
not	O
agree	O
with	O
the	O
training	O
set	B
labels	O
therefore	O
the	O
posterior	O
looks	O
like	O
a	O
correlated	B
gaussian	O
restricted	O
to	O
the	O
orthant	O
which	O
agrees	O
with	O
the	O
labels	O
its	O
mode	O
will	O
be	O
located	O
close	O
to	O
the	O
origin	O
in	O
that	O
orthant	O
and	O
it	O
will	O
decrease	O
rapidly	O
in	O
the	O
direction	O
towards	O
the	O
origin	O
due	O
to	O
conflicts	O
from	O
the	O
likelihood	B
terms	O
and	O
decrease	O
only	O
slowly	O
in	O
the	O
opposite	O
direction	O
of	O
the	O
prior	O
seen	O
in	O
this	O
light	O
it	O
is	O
not	O
surprising	O
that	O
the	O
laplace	B
approximation	I
is	O
somewhat	O
inaccurate	O
this	O
explanation	O
is	O
corroborated	O
further	O
by	O
kuss	O
and	O
rasmussen	O
it	O
should	O
be	O
noted	O
that	O
all	O
the	O
methods	O
compared	O
on	O
the	O
binary	B
digits	O
classification	B
task	O
except	O
for	O
the	O
linear	B
probit	B
model	O
are	O
using	O
the	O
squared	B
distance	O
between	O
the	O
digitized	O
digit	O
images	O
measured	O
directly	O
in	O
the	O
image	O
space	O
as	O
the	O
sole	O
input	O
to	O
the	O
algorithm	O
this	O
distance	O
measure	B
is	O
not	O
very	O
well	O
suited	O
for	O
the	O
digit	O
discrimination	O
task	O
for	O
example	O
two	O
similar	O
images	O
that	O
are	O
slight	O
translations	O
of	O
each	O
other	O
may	O
have	O
a	O
huge	O
squared	B
distance	O
although	O
of	O
course	O
identical	O
labels	O
one	O
of	O
the	O
strengths	O
of	O
the	O
gp	O
formalism	O
is	O
that	O
one	O
can	O
use	O
prior	O
distributions	O
over	O
in	O
this	O
case	O
functions	O
and	O
do	O
inference	O
based	O
on	O
these	O
if	O
however	O
the	O
prior	O
over	O
functions	O
depends	O
only	O
on	O
one	O
particular	O
aspect	O
of	O
the	O
data	O
squared	B
distance	O
in	O
image	O
space	O
which	O
is	O
not	O
so	O
well	O
suited	O
for	O
discrimination	O
then	O
the	O
prior	O
used	O
is	O
also	O
not	O
very	O
appropriate	O
it	O
would	O
be	O
more	O
interesting	O
to	O
design	O
covariance	B
functions	O
by	O
hyperparameters	B
which	O
are	O
more	O
appropriate	O
for	O
the	O
digit	O
discrimination	O
task	O
e	O
g	O
reflecting	O
on	O
the	O
known	O
invariances	B
in	O
the	O
images	O
such	O
as	O
the	O
tangent-distance	O
ideas	O
from	O
simard	O
et	O
al	O
see	O
also	O
sch	O
olkopf	O
and	O
smola	O
ch	O
and	O
section	O
the	O
results	O
shown	O
here	O
follow	O
the	O
common	O
approach	O
of	O
using	O
a	O
generic	O
suitablility	O
of	O
the	O
covariance	B
function	B
lengthscale	O
logllog	O
magnitude	O
log	O
flog	O
marginal	B
likelihood	B
lengthscale	O
logllog	O
magnitude	O
log	O
finformation	O
about	O
test	O
targets	O
in	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
covariance	B
function	B
with	O
a	O
minimum	O
of	O
hyperparameters	B
but	O
this	O
doesn	O
t	O
allow	O
us	O
to	O
incorporate	O
much	O
prior	O
information	O
about	O
the	O
problem	O
for	O
an	O
example	O
in	O
the	O
gp	O
framework	O
for	O
doing	O
inference	O
about	O
multiple	O
hyperparameters	B
with	O
more	O
complex	O
covariance	B
functions	O
which	O
provide	O
clearly	O
interpretable	O
information	O
about	O
the	O
data	O
see	O
the	O
carbon	O
dioxide	O
modelling	O
problem	O
discussed	O
on	O
page	O
appendix	O
moment	O
derivations	O
consider	O
the	O
integral	O
of	O
a	O
cumulative	O
gaussian	O
with	O
respect	O
to	O
a	O
gaussian	O
z	O
z	O
or	O
in	O
matrix	O
notation	O
initially	O
for	O
the	O
special	O
case	O
v	O
writing	O
out	O
in	O
full	O
substituting	O
z	O
y	O
x	O
m	O
and	O
w	O
x	O
and	O
interchanging	O
the	O
order	O
of	O
the	O
integrals	B
z	O
x	O
n	O
dy	O
dy	O
dx	O
dw	O
dz	O
w	O
z	O
dw	O
dz	O
dw	O
dz	O
v	O
v	O
x	O
m	O
dx	O
where	O
z	O
z	O
x	O
z	O
z	O
m	O
w	O
z	O
m	O
z	O
z	O
m	O
z	O
w	O
h	O
z	O
m	O
v	O
v	O
z	O
z	O
dz	O
m	O
i	O
e	O
an	O
integral	O
over	O
a	O
joint	B
gaussian	O
the	O
inner	O
integral	O
corresponds	O
to	O
marginalizing	O
over	O
w	O
eq	O
yielding	O
which	O
assumed	O
v	O
if	O
v	O
is	O
negative	O
we	O
can	O
substitute	O
the	O
symmetry	O
z	O
into	O
eq	O
to	O
get	O
exp	O
m	O
m	O
dx	O
where	O
z	O
qx	O
z	O
x	O
m	O
collecting	O
the	O
two	O
cases	O
eq	O
and	O
eq	O
we	O
arrive	O
at	O
z	O
for	O
general	O
v	O
we	O
wish	O
to	O
compute	O
the	O
moments	O
of	O
x	O
m	O
z	O
v	O
v	O
m	O
v	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
exercises	O
where	O
z	O
is	O
given	O
in	O
eq	O
perhaps	O
the	O
easiest	O
way	O
to	O
do	O
this	O
is	O
to	O
differentiate	O
w	O
r	O
t	O
on	O
both	O
sides	O
of	O
eq	O
z	O
x	O
x	O
m	O
x	O
x	O
m	O
v	O
dx	O
v	O
dx	O
z	O
z	O
z	O
n	O
v	O
where	O
we	O
have	O
used	O
n	O
z	O
we	O
recognize	O
the	O
first	O
term	O
in	O
the	O
integral	O
in	O
the	O
top	O
line	O
of	O
eq	O
as	O
z	O
times	O
the	O
first	O
moment	O
of	O
q	O
which	O
we	O
are	O
seeking	O
multiplying	O
through	O
by	O
and	O
rearranging	O
we	O
obtain	O
first	O
moment	O
eqx	O
similarly	O
the	O
second	O
moment	O
can	O
be	O
obtained	O
by	O
differentiating	O
eq	O
twice	O
z	O
h	O
x	O
eqx	O
x	O
m	O
i	O
v	O
dx	O
zn	O
second	O
moment	O
where	O
the	O
first	O
and	O
second	O
terms	O
of	O
the	O
integral	O
in	O
the	O
top	O
line	O
of	O
eq	O
are	O
multiples	O
of	O
the	O
first	O
and	O
second	O
moments	O
the	O
second	O
central	O
moment	O
after	O
reintroducing	O
eq	O
into	O
eq	O
and	O
simplifying	O
is	O
given	O
by	O
eq	O
z	O
n	O
exercises	O
for	O
binary	B
gpc	O
show	O
the	O
equivalence	O
of	O
using	O
a	O
noise-free	O
latent	O
process	O
combined	O
with	O
a	O
probit	B
likelihood	B
and	O
a	O
latent	O
process	O
with	O
gaussian	O
noise	O
combined	O
with	O
a	O
step-function	O
likelihood	B
hint	O
introduce	O
explicitly	O
additional	O
noisy	O
latent	O
variables	O
fi	O
which	O
differ	O
from	O
fi	O
by	O
gaussian	O
noise	O
write	O
down	O
the	O
step	O
function	B
likelihood	B
for	O
a	O
single	O
case	O
as	O
a	O
function	B
of	O
fi	O
integrate	O
out	O
the	O
noisy	O
variable	O
to	O
arrive	O
at	O
the	O
probit	B
likelihood	B
as	O
a	O
function	B
of	O
the	O
noise-free	O
process	O
consider	O
a	O
multinomial	O
random	O
variable	O
y	O
having	O
c	O
states	O
with	O
yc	O
if	O
the	O
variable	O
is	O
in	O
state	O
c	O
and	O
otherwise	O
state	O
c	O
occurs	O
with	O
probability	O
c	O
show	O
that	O
covy	O
ey	O
diag	O
observe	O
that	O
covy	O
being	O
a	O
covariance	B
matrix	I
must	O
necessarily	O
be	O
positive	B
semidefinite	I
using	O
this	O
fact	O
show	O
that	O
the	O
matrix	O
w	O
diag	O
from	O
eq	O
is	O
positive	B
semidefinite	I
by	O
showing	O
that	O
the	O
vector	O
of	O
all	O
ones	O
is	O
an	O
eigenvector	O
of	O
covy	O
with	O
eigenvalue	B
zero	O
verify	O
that	O
the	O
matrix	O
is	O
indeed	O
positive	B
semidefinite	I
and	O
not	O
positive	B
definite	I
section	O
for	O
definitions	O
of	O
positive	B
semidefinite	I
and	O
positive	B
definite	I
matrices	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
classification	B
figure	O
the	O
decision	O
regions	O
for	O
the	O
three-class	O
softmax	B
function	B
in	O
space	O
consider	O
the	O
softmax	B
function	B
pcc	O
expfc	O
where	O
c	O
and	O
are	O
the	O
corresponding	O
activations	O
to	O
more	O
easily	O
visualize	O
the	O
decision	O
boundaries	O
let	O
and	O
thus	O
and	O
similarly	O
for	O
the	O
other	O
classes	O
the	O
decision	O
boundary	O
relating	O
to	O
is	O
the	O
curve	O
the	O
decision	O
regions	O
for	O
the	O
three	O
classes	O
are	O
illustrated	O
in	O
figure	O
let	O
f	O
have	O
a	O
gaussian	B
distribution	I
centered	O
on	O
the	O
origin	O
and	O
let	O
softmaxf	O
we	O
now	O
consider	O
the	O
effect	O
of	O
this	O
distribution	O
on	O
df	O
for	O
a	O
gaussian	O
with	O
given	O
covariance	B
structure	O
this	O
integral	O
is	O
easily	O
approximated	O
by	O
drawing	O
samples	O
from	O
pf	O
show	O
that	O
the	O
classification	B
can	O
be	O
made	O
to	O
fall	O
into	O
any	O
of	O
the	O
three	O
categories	O
depending	O
on	O
the	O
covariance	B
matrix	I
thus	O
by	O
considering	O
displacements	O
of	O
the	O
mean	O
of	O
the	O
gaussian	O
by	O
from	O
the	O
origin	O
into	O
each	O
of	O
the	O
three	O
regions	O
we	O
have	O
shown	O
that	O
overall	O
classification	B
depends	O
not	O
only	O
on	O
the	O
mean	O
of	O
the	O
gaussian	O
but	O
also	O
on	O
its	O
covariance	B
show	O
that	O
this	O
conclusion	O
is	O
still	O
valid	O
when	O
it	O
is	O
recalled	O
that	O
z	O
is	O
derived	O
from	O
f	O
as	O
z	O
t	O
f	O
where	O
t	O
so	O
that	O
covz	O
t	O
covft	O
consider	O
the	O
update	O
equation	O
for	O
f	O
new	O
given	O
by	O
eq	O
when	O
some	O
of	O
the	O
training	O
points	O
are	O
well-explained	O
under	O
f	O
so	O
that	O
ti	O
i	O
and	O
wii	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
exercises	O
for	O
these	O
points	O
break	O
f	O
into	O
two	O
subvectors	O
that	O
corresponds	O
to	O
points	O
that	O
are	O
not	O
well-explained	O
and	O
to	O
those	O
that	O
are	O
re-write	O
w	O
from	O
eq	O
as	O
ki	O
w	O
k	O
and	O
let	O
k	O
be	O
partitioned	O
as	O
and	O
similarly	O
for	O
the	O
other	O
matrices	O
using	O
the	O
partitioned	O
matrix	O
inverse	O
equations	O
section	O
show	O
that	O
log	O
f	O
new	O
f	O
new	O
f	O
new	O
see	O
section	O
for	O
the	O
consequences	O
of	O
this	O
result	O
show	O
that	O
the	O
expressions	O
in	O
eq	O
for	O
the	O
cavity	O
mean	O
i	O
and	O
variance	O
i	O
do	O
not	O
depend	O
on	O
the	O
approximate	O
likelihood	B
terms	O
i	O
and	O
i	O
for	O
the	O
corresponding	O
case	O
despite	O
the	O
appearance	O
of	O
eq	O
consider	O
the	O
usps	B
vs	O
prediction	O
problem	O
discussed	O
in	O
section	O
use	O
the	O
implementation	O
of	O
the	O
laplace	O
binary	B
gpc	O
provided	O
to	O
investigate	O
how	O
f	O
and	O
the	O
predictive	B
probabilities	O
etc	O
vary	O
as	O
functions	O
of	O
log	O
and	O
log	O
f	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
covariance	B
functions	O
we	O
have	O
seen	O
that	O
a	O
covariance	B
function	B
is	O
the	O
crucial	O
ingredient	O
in	O
a	O
gaussian	B
process	I
predictor	O
as	O
it	O
encodes	O
our	O
assumptions	O
about	O
the	O
function	B
which	O
we	O
wish	O
to	O
learn	O
from	O
a	O
slightly	O
different	O
viewpoint	O
it	O
is	O
clear	O
that	O
in	O
supervised	B
learning	B
the	O
notion	O
of	O
similarity	O
between	O
data	O
points	O
is	O
crucial	O
it	O
is	O
a	O
basic	O
assumption	O
that	O
points	O
with	O
inputs	O
x	O
which	O
are	O
close	O
are	O
likely	O
to	O
have	O
similar	O
target	O
values	O
y	O
and	O
thus	O
training	O
points	O
that	O
are	O
near	O
to	O
a	O
test	O
point	O
should	O
be	O
informative	O
about	O
the	O
prediction	O
at	O
that	O
point	O
under	O
the	O
gaussian	B
process	I
view	O
it	O
is	O
the	O
covariance	B
function	B
that	O
defines	O
nearness	O
or	O
similarity	O
an	O
arbitrary	O
function	B
of	O
input	O
pairs	O
x	O
and	O
will	O
not	O
in	O
general	O
be	O
a	O
valid	O
covariance	B
the	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
give	O
examples	O
of	O
some	O
commonly-used	O
covariance	B
functions	O
and	O
to	O
examine	O
their	O
properties	O
section	O
defines	O
a	O
number	O
of	O
basic	O
terms	O
relating	O
to	O
covariance	B
functions	O
section	O
gives	O
examples	O
of	O
stationary	O
dot-product	O
and	O
other	O
non-stationary	O
covariance	B
functions	O
and	O
also	O
gives	O
some	O
ways	O
to	O
make	O
new	O
ones	O
from	O
old	O
section	O
introduces	O
the	O
important	O
topic	O
of	O
eigenfunction	B
analysis	O
of	O
covariance	B
functions	O
and	O
states	O
mercer	O
s	O
theorem	O
which	O
allows	O
us	O
to	O
express	O
the	O
covariance	B
function	B
certain	O
conditions	O
in	O
terms	O
of	O
its	O
eigenfunctions	O
and	O
eigenvalues	O
the	O
covariance	B
functions	O
given	O
in	O
section	O
are	O
valid	O
when	O
the	O
input	O
domain	O
x	O
is	O
a	O
subset	O
of	O
rd	O
in	O
section	O
we	O
describe	O
ways	O
to	O
define	O
covariance	B
functions	O
when	O
the	O
input	O
domain	O
is	O
over	O
structured	O
objects	O
such	O
as	O
strings	O
and	O
trees	O
preliminaries	O
a	O
stationary	O
covariance	B
function	B
is	O
a	O
function	B
of	O
x	O
thus	O
it	O
is	O
invariant	O
to	O
translations	O
in	O
the	O
input	O
for	O
example	O
the	O
squared	B
exponential	B
co	O
be	O
a	O
valid	O
covariance	B
function	B
it	O
must	O
be	O
positive	B
semidefinite	I
see	O
eq	O
stochastic	O
process	O
theory	O
a	O
process	O
which	O
has	O
constant	O
mean	O
and	O
whose	O
covariance	B
function	B
is	O
invariant	O
to	O
translations	O
is	O
called	O
weakly	O
stationary	O
a	O
process	O
is	O
strictly	O
stationary	O
if	O
all	O
of	O
its	O
finite	O
dimensional	O
distributions	O
are	O
invariant	O
to	O
translations	O
sec	O
similarity	O
valid	O
covariance	B
functions	O
stationarity	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
isotropy	B
dot	B
product	I
covariance	B
kernel	B
gram	B
matrix	I
covariance	B
matrix	I
positive	B
semidefinite	I
covariance	B
functions	O
variance	O
function	B
given	O
in	O
equation	O
is	O
stationary	O
if	O
further	O
the	O
covariance	B
function	B
is	O
a	O
function	B
only	O
of	O
then	O
it	O
is	O
called	O
isotropic	O
it	O
is	O
thus	O
invariant	O
to	O
all	O
rigid	O
motions	O
for	O
example	O
the	O
squared	B
exponential	B
covariance	B
function	B
given	O
in	O
equation	O
is	O
isotropic	O
as	O
k	O
is	O
now	O
only	O
a	O
function	B
of	O
r	O
these	O
are	O
also	O
known	O
as	O
radial	O
basis	O
functions	O
if	O
a	O
covariance	B
function	B
depends	O
only	O
on	O
x	O
and	O
through	O
x	O
we	O
call	O
it	O
a	O
dot	B
product	I
covariance	B
function	B
a	O
simple	O
example	O
is	O
the	O
covariance	B
function	B
x	O
which	O
can	O
be	O
obtained	O
from	O
linear	B
regression	I
by	O
putting	O
kx	O
n	O
priors	O
on	O
the	O
coefficients	O
of	O
xd	O
d	O
and	O
a	O
prior	O
of	O
n	O
on	O
the	O
bias	O
constant	O
function	B
see	O
eq	O
another	O
important	O
example	O
x	O
where	O
p	O
is	O
a	O
is	O
the	O
inhomogeneous	B
polynomial	B
kernel	B
kx	O
positive	O
integer	O
dot	B
product	I
covariance	B
functions	O
are	O
invariant	O
to	O
a	O
rotation	O
of	O
the	O
coordinates	O
about	O
the	O
origin	O
but	O
not	O
translations	O
a	O
general	O
name	O
for	O
a	O
function	B
k	O
of	O
two	O
arguments	O
mapping	O
a	O
pair	O
of	O
inputs	O
x	O
x	O
x	O
into	O
r	O
is	O
a	O
kernel	B
this	O
term	O
arises	O
in	O
the	O
theory	O
of	O
integral	O
operators	O
where	O
the	O
operator	B
tk	O
is	O
defined	O
as	O
kx	O
d	O
z	O
x	O
where	O
denotes	O
a	O
measure	B
see	O
section	O
for	O
further	O
explanation	O
of	O
this	O
a	O
real	O
kernel	B
is	O
said	O
to	O
be	O
symmetric	O
if	O
kx	O
x	O
clearly	O
covariance	B
functions	O
must	O
be	O
symmetric	O
from	O
the	O
definition	O
given	O
a	O
set	B
of	O
input	O
points	O
n	O
we	O
can	O
compute	O
the	O
gram	B
matrix	I
k	O
whose	O
entries	O
are	O
kij	O
kxi	O
xj	O
if	O
k	O
is	O
a	O
covariance	B
function	B
we	O
call	O
the	O
matrix	O
k	O
the	O
covariance	B
matrix	I
a	O
real	O
n	O
n	O
matrix	O
k	O
which	O
satisfies	O
qv	O
vkv	O
for	O
all	O
vectors	O
v	O
rn	O
is	O
called	O
positive	B
semidefinite	I
if	O
qv	O
only	O
when	O
v	O
the	O
matrix	O
is	O
positive	B
definite	I
qv	O
is	O
called	O
a	O
quadratic	B
form	I
a	O
symmetric	O
matrix	O
is	O
psd	O
if	O
and	O
only	O
if	O
all	O
of	O
its	O
eigenvalues	O
are	O
non-negative	O
a	O
gram	B
matrix	I
corresponding	O
to	O
a	O
general	O
kernel	B
function	B
need	O
not	O
be	O
psd	O
but	O
the	O
gram	B
matrix	I
corresponding	O
to	O
a	O
covariance	B
function	B
is	O
psd	O
a	O
kernel	B
is	O
said	O
to	O
be	O
positive	B
semidefinite	I
if	O
kx	O
d	O
d	O
z	O
for	O
all	O
f	O
equivalently	O
a	O
kernel	B
function	B
which	O
gives	O
rise	O
to	O
psd	O
gram	B
matrices	O
for	O
any	O
choice	O
of	O
n	O
n	O
and	O
d	O
is	O
positive	B
semidefinite	I
to	O
see	O
this	O
let	O
f	O
be	O
the	O
weighted	O
sum	O
of	O
delta	O
functions	O
at	O
each	O
xi	O
since	O
such	O
functions	O
are	O
limits	O
of	O
functions	O
in	O
eq	O
implies	O
that	O
the	O
gram	B
matrix	I
corresponding	O
to	O
any	O
d	O
is	O
psd	O
for	O
a	O
one-dimensional	O
gaussian	B
process	I
one	O
way	O
to	O
understand	O
the	O
characteristic	O
length-scale	B
of	O
the	O
process	O
this	O
exists	O
is	O
in	O
terms	O
of	O
the	O
number	O
of	O
upcrossings	O
of	O
a	O
level	O
u	O
adler	O
theorem	O
states	O
that	O
the	O
expected	O
speaking	O
readers	O
will	O
usually	O
be	O
able	O
to	O
substitute	O
dx	O
or	O
pxdx	O
for	O
d	O
upcrossing	B
rate	I
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
number	O
of	O
upcrossings	O
enu	O
of	O
the	O
level	O
u	O
on	O
the	O
unit	O
interval	O
by	O
a	O
zero-mean	O
stationary	O
almost	O
surely	O
continuous	O
gaussian	B
process	I
is	O
given	O
by	O
s	O
enu	O
exp	O
if	O
does	O
not	O
exist	O
that	O
the	O
process	O
is	O
not	O
mean	O
square	O
differentiable	O
then	O
if	O
such	O
a	O
process	O
has	O
a	O
zero	O
at	O
then	O
it	O
will	O
almost	O
surely	O
have	O
an	O
infinite	O
number	O
of	O
zeros	O
in	O
the	O
arbitrarily	O
small	O
interval	O
and	O
lindsey	O
p	O
mean	B
square	I
continuity	I
and	O
differentiability	O
we	O
now	O
describe	O
mean	B
square	I
continuity	I
and	O
differentiability	O
of	O
stochastic	O
processes	O
following	O
adler	O
sec	O
let	O
be	O
a	O
sequence	O
of	O
points	O
and	O
x	O
be	O
a	O
fixed	O
point	O
in	O
rd	O
such	O
that	O
x	O
as	O
k	O
then	O
a	O
process	O
fx	O
is	O
continuous	O
in	O
mean	O
square	O
at	O
x	O
if	O
efxk	O
fx	O
as	O
mean	B
square	I
continuity	I
k	O
if	O
this	O
holds	O
for	O
all	O
x	O
a	O
where	O
a	O
is	O
a	O
subset	O
of	O
rd	O
then	O
fx	O
is	O
said	O
to	O
be	O
continuous	O
in	O
mean	O
square	O
over	O
a	O
a	O
random	O
field	O
is	O
continuous	O
in	O
mean	O
square	O
at	O
x	O
if	O
and	O
only	O
if	O
its	O
covariance	B
function	B
kx	O
is	O
continuous	O
at	O
the	O
point	O
x	O
x	O
for	O
stationary	O
covariance	B
functions	O
this	O
reduces	O
to	O
checking	O
continuity	O
at	O
note	O
that	O
ms	O
continuity	O
does	O
not	O
necessarily	O
imply	O
sample	O
function	B
continuity	O
for	O
a	O
discussion	O
of	O
sample	O
function	B
continuity	O
and	O
differentiability	O
see	O
adler	O
ch	O
the	O
mean	O
square	O
derivative	O
of	O
fx	O
in	O
the	O
ith	O
direction	O
is	O
defined	O
as	O
mean	B
square	I
differentiability	I
fx	O
xi	O
l	O
i	O
m	O
h	O
fx	O
hei	O
fx	O
h	O
when	O
the	O
limit	O
exists	O
where	O
l	O
i	O
m	O
denotes	O
the	O
limit	O
in	O
mean	O
square	O
and	O
ei	O
is	O
the	O
unit	O
vector	O
in	O
the	O
ith	O
direction	O
the	O
covariance	B
function	B
of	O
fx	O
xi	O
is	O
given	O
by	O
xi	O
i	O
these	O
definitions	O
can	O
be	O
extended	O
to	O
higher	O
order	O
derivatives	O
for	O
stationary	O
processes	O
if	O
the	O
partial	O
derivative	O
exists	O
and	O
is	O
finite	O
at	O
x	O
then	O
the	O
kth	O
order	O
partial	O
derivative	O
kfx	O
xik	O
exists	O
for	O
all	O
x	O
rd	O
as	O
a	O
mean	O
square	O
limit	O
notice	O
that	O
it	O
is	O
the	O
properties	O
of	O
the	O
kernel	B
k	O
around	O
that	O
determine	O
the	O
smoothness	O
properties	O
differentiability	O
of	O
a	O
stationary	O
process	O
examples	O
of	O
covariance	B
functions	O
in	O
this	O
section	O
we	O
consider	O
covariance	B
functions	O
where	O
the	O
input	O
domain	O
x	O
is	O
a	O
subset	O
of	O
the	O
vector	O
space	O
rd	O
more	O
general	O
input	O
spaces	O
are	O
considered	O
in	O
section	O
we	O
start	O
in	O
section	O
with	O
stationary	O
covariance	B
functions	O
then	O
consider	O
dot-product	O
covariance	B
functions	O
in	O
section	O
and	O
other	O
varieties	O
of	O
non-stationary	O
covariance	B
functions	O
in	O
section	O
we	O
give	O
an	O
overview	O
of	O
some	O
commonly	O
used	O
covariance	B
functions	O
in	O
table	O
and	O
in	O
section	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
we	O
describe	O
general	O
methods	O
for	O
constructing	O
new	O
kernels	O
from	O
old	O
there	O
exist	O
several	O
other	O
good	O
overviews	O
of	O
covariance	B
functions	O
see	O
e	O
g	O
abrahamsen	O
stationary	O
covariance	B
functions	O
in	O
this	O
section	O
section	O
it	O
will	O
be	O
convenient	O
to	O
allow	O
kernels	O
to	O
be	O
a	O
map	B
from	O
x	O
x	O
x	O
into	O
c	O
than	O
r	O
if	O
a	O
zero-mean	O
process	O
f	O
is	O
complexvalued	O
then	O
the	O
covariance	B
function	B
is	O
defined	O
as	O
kx	O
efxf	O
where	O
denotes	O
complex	O
conjugation	O
a	O
stationary	O
covariance	B
function	B
is	O
a	O
function	B
of	O
x	O
sometimes	O
in	O
this	O
case	O
we	O
will	O
write	O
k	O
as	O
a	O
function	B
of	O
a	O
single	O
argument	O
i	O
e	O
k	O
the	O
covariance	B
function	B
of	O
a	O
stationary	O
process	O
can	O
be	O
represented	O
as	O
the	O
fourier	B
transform	I
of	O
a	O
positive	O
finite	O
measure	B
bochner	O
s	O
theorem	O
theorem	O
s	O
theorem	O
a	O
complex-valued	O
function	B
k	O
on	O
rd	O
is	O
the	O
covariance	B
function	B
of	O
a	O
weakly	O
stationary	O
mean	O
square	O
continuous	O
complexvalued	O
random	O
process	O
on	O
rd	O
if	O
and	O
only	O
if	O
it	O
can	O
be	O
represented	O
as	O
where	O
is	O
a	O
positive	O
finite	O
measure	B
k	O
rd	O
is	O
d	O
z	O
spectral	O
density	O
power	O
spectrum	O
the	O
statement	O
of	O
bochner	O
s	O
theorem	O
is	O
quoted	O
from	O
stein	O
p	O
a	O
proof	O
can	O
be	O
found	O
in	O
gihman	O
and	O
skorohod	O
p	O
if	O
has	O
a	O
density	O
ss	O
then	O
s	O
is	O
known	O
as	O
the	O
spectral	O
density	O
or	O
power	O
spectrum	O
corresponding	O
to	O
k	O
the	O
construction	O
given	O
by	O
eq	O
puts	O
non-negative	O
power	O
into	O
each	O
frequency	O
s	O
this	O
is	O
analogous	O
to	O
the	O
requirement	O
that	O
the	O
prior	O
covariance	B
matrix	I
p	O
on	O
the	O
weights	O
in	O
equation	O
be	O
non-negative	O
definite	O
z	O
z	O
in	O
the	O
case	O
that	O
the	O
spectral	O
density	O
ss	O
exists	O
the	O
covariance	B
function	B
and	O
the	O
spectral	O
density	O
are	O
fourier	O
duals	O
of	O
each	O
other	O
as	O
shown	O
in	O
eq	O
this	O
is	O
known	O
as	O
the	O
wiener-khintchine	B
theorem	I
see	O
e	O
g	O
chatfield	O
k	O
notice	O
that	O
the	O
variance	O
of	O
the	O
process	O
is	O
ss	O
ds	O
so	O
the	O
power	O
spectrum	O
ss	O
k	O
is	O
d	O
is	O
ds	O
must	O
be	O
integrable	O
to	O
define	O
a	O
valid	O
gaussian	B
process	I
to	O
gain	O
some	O
intuition	O
for	O
the	O
definition	O
of	O
the	O
power	O
spectrum	O
given	O
in	O
eq	O
it	O
is	O
important	O
to	O
realize	O
that	O
the	O
complex	O
exponentials	O
is	O
x	O
are	O
eigenfunctions	O
of	O
a	O
stationary	O
kernel	B
with	O
respect	O
to	O
lebesgue	O
measure	B
section	O
for	O
further	O
details	O
thus	O
ss	O
is	O
loosely	O
speaking	O
the	O
amount	O
of	O
power	O
allocated	O
on	O
average	O
to	O
the	O
eigenfunction	B
is	O
x	O
with	O
frequency	O
s	O
ss	O
must	O
eventually	O
decay	O
sufficiently	O
fast	O
as	O
so	O
that	O
it	O
is	O
integrable	O
the	O
appendix	O
for	O
details	O
of	O
fourier	O
transforms	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
rate	O
of	O
this	O
decay	O
of	O
the	O
power	O
spectrum	O
gives	O
important	O
information	O
about	O
the	O
smoothness	O
of	O
the	O
associated	O
stochastic	O
process	O
for	O
example	O
it	O
can	O
determine	O
the	O
mean-square	O
differentiability	O
of	O
the	O
process	O
section	O
for	O
further	O
details	O
if	O
the	O
covariance	B
function	B
is	O
isotropic	O
that	O
it	O
is	O
a	O
function	B
of	O
r	O
where	O
r	O
then	O
it	O
can	O
be	O
shown	O
that	O
ss	O
is	O
a	O
function	B
of	O
s	O
only	O
theorem	O
in	O
this	O
case	O
the	O
integrals	B
in	O
eq	O
can	O
be	O
simplified	O
by	O
changing	O
to	O
spherical	O
polar	O
coordinates	O
and	O
integrating	O
out	O
the	O
angular	O
variables	O
e	O
g	O
bracewell	O
ch	O
to	O
obtain	O
z	O
z	O
kr	O
ss	O
ds	O
dr	O
where	O
is	O
a	O
bessel	O
function	B
of	O
order	O
note	O
that	O
the	O
dependence	O
on	O
the	O
dimensionality	O
d	O
in	O
equation	O
means	O
that	O
the	O
same	O
isotropic	O
functional	B
form	O
of	O
the	O
spectral	O
density	O
can	O
give	O
rise	O
to	O
different	O
isotropic	O
covariance	B
functions	O
in	O
different	O
dimensions	O
similarly	O
if	O
we	O
start	O
with	O
a	O
particular	O
isotropic	O
covariance	B
function	B
kr	O
the	O
form	O
of	O
spectral	O
density	O
will	O
in	O
general	O
depend	O
on	O
d	O
e	O
g	O
the	O
mat	O
ern	O
class	O
spectral	O
density	O
given	O
in	O
eq	O
and	O
in	O
fact	O
kr	O
may	O
not	O
be	O
valid	O
for	O
all	O
d	O
a	O
necessary	O
condition	O
for	O
the	O
spectral	O
density	O
to	O
exist	O
is	O
thatr	O
rd	O
dr	O
see	O
stein	O
sec	O
for	O
more	O
details	O
we	O
now	O
give	O
some	O
examples	O
of	O
commonly-used	O
isotropic	O
covariance	B
functions	O
the	O
covariance	B
functions	O
are	O
given	O
in	O
a	O
normalized	O
form	O
where	O
we	O
can	O
multiply	O
k	O
by	O
a	O
constant	O
f	O
to	O
get	O
any	O
desired	O
process	O
variance	O
squared	B
exponential	B
covariance	B
function	B
the	O
squared	B
exponential	B
covariance	B
function	B
has	O
already	O
been	O
introduced	O
in	O
chapter	O
eq	O
and	O
has	O
the	O
form	O
kser	O
exp	O
with	O
parameter	O
defining	O
the	O
characteristic	O
length-scale	B
using	O
eq	O
we	O
see	O
that	O
the	O
mean	O
number	O
of	O
level-zero	O
upcrossings	O
for	O
a	O
se	O
process	O
in	O
is	O
which	O
confirms	O
the	O
r	O
ole	O
of	O
as	O
a	O
length-scale	B
this	O
covariance	B
function	B
is	O
infinitely	O
differentiable	O
which	O
means	O
that	O
the	O
gp	O
with	O
this	O
covariance	B
function	B
has	O
mean	O
square	O
derivatives	O
of	O
all	O
orders	O
and	O
is	O
thus	O
very	O
smooth	O
the	O
spectral	O
density	O
of	O
the	O
se	O
covariance	B
function	B
is	O
ss	O
exp	O
stein	O
argues	O
that	O
such	O
strong	O
smoothness	O
assumptions	O
are	O
unrealistic	O
for	O
modelling	O
many	O
physical	O
processes	O
and	O
recommends	O
the	O
mat	O
ern	O
class	O
below	O
however	O
the	O
squared	B
exponential	B
is	O
probably	O
the	O
most	O
widely-used	O
kernel	B
within	O
the	O
kernel	B
machines	O
field	O
squared	B
exponential	B
characteristic	O
length-scale	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
infinitely	O
divisible	O
the	O
se	O
kernel	B
is	O
infinitely	O
divisible	O
in	O
that	O
is	O
a	O
valid	O
kernel	B
for	O
all	O
t	O
the	O
effect	O
of	O
raising	O
k	O
to	O
the	O
power	O
of	O
t	O
is	O
simply	O
to	O
rescale	O
infinite	O
network	O
construction	O
for	O
se	O
covariance	B
function	B
mat	O
ern	O
class	O
we	O
now	O
digress	O
briefly	O
to	O
show	O
that	O
the	O
squared	B
exponential	B
covariance	B
function	B
can	O
also	O
be	O
obtained	O
by	O
expanding	O
the	O
input	O
x	O
into	O
a	O
feature	B
space	I
defined	O
by	O
gaussian-shaped	O
basis	O
functions	O
centered	O
densely	O
in	O
x-space	O
for	O
simplicity	O
of	O
exposition	O
we	O
consider	O
scalar	O
inputs	O
with	O
basis	O
functions	O
cx	O
where	O
c	O
denotes	O
the	O
centre	O
of	O
the	O
basis	O
function	B
from	O
sections	O
and	O
we	O
recall	O
that	O
with	O
a	O
gaussian	O
prior	O
on	O
the	O
weights	O
w	O
n	O
pi	O
this	O
gives	O
rise	O
to	O
a	O
gp	O
with	O
covariance	B
function	B
kxp	O
xq	O
p	O
cxp	O
cxq	O
now	O
allowing	O
an	O
infinite	O
number	O
of	O
basis	O
functions	O
centered	O
everywhere	O
on	O
an	O
interval	O
scaling	O
down	O
the	O
variance	O
of	O
the	O
prior	O
on	O
the	O
weights	O
with	O
the	O
number	O
of	O
basis	O
functions	O
we	O
obtain	O
the	O
limit	O
nx	O
z	O
cmax	O
cmin	O
nx	O
lim	O
n	O
p	O
n	O
cxp	O
cxq	O
p	O
cxp	O
cxqdc	O
plugging	O
in	O
the	O
gaussian-shaped	O
basis	O
functions	O
eq	O
and	O
letting	O
the	O
integration	O
limits	O
go	O
to	O
infinity	O
we	O
obtain	O
z	O
p	O
kxp	O
xq	O
p	O
which	O
we	O
recognize	O
as	O
a	O
squared	B
exponential	B
covariance	B
function	B
with	O
a	O
times	O
longer	O
length-scale	B
the	O
derivation	O
is	O
adapted	O
from	O
mackay	O
it	O
is	O
straightforward	O
to	O
generalize	O
this	O
construction	O
to	O
multivariate	O
x	O
see	O
also	O
eq	O
for	O
a	O
similar	O
construction	O
where	O
the	O
centres	O
of	O
the	O
basis	O
functions	O
are	O
sampled	O
from	O
a	O
gaussian	B
distribution	I
the	O
constructions	O
are	O
equivalent	B
when	O
the	O
variance	O
of	O
this	O
gaussian	O
tends	O
to	O
infinity	O
the	O
mat	O
ern	O
class	O
of	O
covariance	B
functions	O
the	O
mat	O
ern	O
class	O
of	O
covariance	B
functions	O
is	O
given	O
by	O
kmaternr	O
r	O
k	O
r	O
with	O
positive	O
parameters	O
and	O
where	O
k	O
is	O
a	O
modified	O
bessel	O
function	B
and	O
stegun	O
sec	O
this	O
covariance	B
function	B
has	O
a	O
spectral	O
density	O
ss	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
figure	O
panel	O
covariance	B
functions	O
and	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
mat	O
ern	O
covariance	B
functions	O
eq	O
for	O
different	O
values	O
of	O
with	O
the	O
sample	O
functions	O
on	O
the	O
right	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
equally-spaced	O
points	O
in	O
d	O
dimensions	O
note	O
that	O
the	O
scaling	O
is	O
chosen	O
so	O
that	O
for	O
we	O
obtain	O
the	O
se	O
covariance	B
function	B
e	O
see	O
eq	O
stein	O
named	O
this	O
the	O
mat	O
ern	O
class	O
after	O
the	O
work	O
of	O
mat	O
ern	O
for	O
the	O
mat	O
ern	O
class	O
the	O
process	O
fx	O
is	O
k-times	O
ms	O
differentiable	O
if	O
and	O
only	O
if	O
k	O
the	O
mat	O
ern	O
covariance	B
functions	O
become	O
especially	O
simple	O
when	O
is	O
half-integer	O
p	O
where	O
p	O
is	O
a	O
non-negative	O
integer	O
in	O
this	O
case	O
the	O
covariance	B
function	B
is	O
a	O
product	O
of	O
an	O
exponential	B
and	O
a	O
polynomial	B
of	O
order	O
p	O
the	O
general	O
expression	O
can	O
be	O
derived	O
from	O
and	O
stegun	O
eq	O
giving	O
px	O
r	O
i	O
i	O
i	O
p	O
i	O
r	O
k	O
exp	O
it	O
is	O
possible	O
that	O
the	O
most	O
interesting	O
cases	O
for	O
machine	O
learning	B
are	O
and	O
for	O
which	O
exp	O
exp	O
k	O
k	O
since	O
for	O
the	O
process	O
becomes	O
very	O
rough	O
below	O
and	O
for	O
in	O
the	O
absence	O
of	O
explicit	O
prior	O
knowledge	O
about	O
the	O
existence	O
of	O
higher	O
order	O
derivatives	O
it	O
is	O
probably	O
very	O
hard	O
from	O
finite	O
noisy	O
training	O
examples	O
to	O
distinguish	O
between	O
values	O
of	O
even	O
to	O
distinguish	O
between	O
finite	O
values	O
of	O
and	O
the	O
smooth	O
squared	B
exponential	B
in	O
this	O
case	O
for	O
example	O
a	O
value	O
of	O
was	O
used	O
in	O
et	O
al	O
ornstein-uhlenbeck	B
process	O
and	O
exponential	B
covariance	B
function	B
the	O
special	O
case	O
obtained	O
by	O
setting	O
in	O
the	O
mat	O
ern	O
class	O
gives	O
the	O
exponential	B
covariance	B
function	B
kr	O
exp	O
r	O
the	O
corresponding	O
process	O
exponential	B
distance	O
rcovariance	O
kr	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
figure	O
panel	O
covariance	B
functions	O
and	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
the	O
covariance	B
function	B
eq	O
for	O
different	O
values	O
of	O
with	O
the	O
sample	O
functions	O
are	O
only	O
differentiable	O
when	O
se	O
case	O
the	O
sample	O
functions	O
on	O
the	O
right	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
equally-spaced	O
points	O
is	O
ms	O
continuous	O
but	O
not	O
ms	O
differentiable	O
in	O
d	O
this	O
is	O
the	O
covariance	B
function	B
of	O
the	O
ornstein-uhlenbeck	B
process	O
the	O
ou	O
process	O
and	O
ornstein	O
was	O
introduced	O
as	O
a	O
mathematical	O
model	O
of	O
the	O
velocity	O
of	O
a	O
particle	O
undergoing	O
brownian	O
motion	O
more	O
generally	O
in	O
d	O
setting	O
p	O
for	O
integer	O
p	O
gives	O
rise	O
to	O
a	O
particular	O
form	O
of	O
a	O
continuous-time	O
arp	O
gaussian	B
process	I
for	O
further	O
details	O
see	O
section	O
the	O
form	O
of	O
the	O
mat	O
ern	O
covariance	B
function	B
and	O
samples	O
drawn	O
from	O
it	O
for	O
and	O
are	O
illustrated	O
in	O
figure	O
the	O
covariance	B
function	B
the	O
family	O
of	O
covariance	B
functions	O
which	O
includes	O
both	O
the	O
exponential	B
and	O
squared	B
exponential	B
is	O
given	O
by	O
kr	O
for	O
although	O
this	O
function	B
has	O
a	O
similar	O
number	O
of	O
parameters	O
to	O
the	O
mat	O
ern	O
class	O
it	O
is	O
stein	O
notes	O
in	O
a	O
sense	O
less	O
flexible	O
this	O
is	O
because	O
the	O
corresponding	O
process	O
is	O
not	O
ms	O
differentiable	O
except	O
when	O
it	O
is	O
infinitely	O
ms	O
differentiable	O
the	O
covariance	B
function	B
and	O
random	O
samples	O
from	O
the	O
process	O
are	O
shown	O
in	O
figure	O
a	O
proof	O
of	O
the	O
positive	O
definiteness	O
of	O
this	O
covariance	B
function	B
can	O
be	O
found	O
in	O
schoenberg	O
rational	B
quadratic	I
covariance	B
function	B
the	O
rational	B
quadratic	I
covariance	B
function	B
krqr	O
ornstein-uhlenbeck	B
process	O
rational	B
quadratic	I
distancecovariance	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
figure	O
panel	O
covariance	B
functions	O
and	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
rational	B
quadratic	I
covariance	B
functions	O
eq	O
for	O
different	O
values	O
of	O
with	O
the	O
sample	O
functions	O
on	O
the	O
right	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
equally-spaced	O
points	O
with	O
can	O
be	O
seen	O
as	O
a	O
scale	B
mixture	I
infinite	O
sum	O
of	O
squared	B
exponential	B
covariance	B
functions	O
with	O
different	O
characteristic	O
length-scales	O
of	O
covariance	B
functions	O
are	O
also	O
a	O
valid	O
covariance	B
see	O
section	O
parameterizing	O
now	O
in	O
terms	O
of	O
inverse	O
squared	B
length	O
scales	O
and	O
putting	O
a	O
gamma	B
distribution	I
on	O
p	O
exp	O
we	O
can	O
add	O
up	O
the	O
contributions	O
through	O
the	O
following	O
integral	O
p	O
d	O
d	O
exp	O
z	O
z	O
krqr	O
exp	O
scale	B
mixture	I
where	O
we	O
have	O
set	B
the	O
rational	B
quadratic	I
is	O
also	O
discussed	O
by	O
mat	O
ern	O
p	O
using	O
a	O
slightly	O
different	O
parameterization	O
in	O
our	O
notation	O
the	O
limit	O
of	O
the	O
rq	O
covariance	B
for	O
eq	O
is	O
the	O
se	O
covariance	B
function	B
with	O
characteristic	O
length-scale	B
eq	O
figure	O
illustrates	O
the	O
behaviour	O
for	O
different	O
values	O
of	O
note	O
that	O
the	O
process	O
is	O
infinitely	O
ms	O
differentiable	O
for	O
every	O
in	O
contrast	O
to	O
the	O
mat	O
ern	O
covariance	B
function	B
in	O
figure	O
the	O
previous	O
example	O
is	O
a	O
special	O
case	O
of	O
kernels	O
which	O
can	O
be	O
written	O
as	O
superpositions	O
of	O
se	O
kernels	O
with	O
a	O
distribution	O
p	O
of	O
length-scales	O
kr	O
r	O
exp	O
d	O
this	O
is	O
in	O
fact	O
the	O
most	O
general	O
representation	O
for	O
an	O
isotropic	O
kernel	B
which	O
defines	O
a	O
valid	O
covariance	B
function	B
in	O
any	O
dimension	O
d	O
see	O
sec	O
piecewise	B
polynomial	B
covariance	B
functions	O
with	O
compact	B
support	I
a	O
family	O
of	O
piecewise	B
polynomial	B
functions	O
with	O
compact	B
support	I
provide	O
another	O
interesting	O
class	O
of	O
covariance	B
functions	O
compact	B
support	I
means	O
that	O
note	O
that	O
there	O
are	O
several	O
common	O
ways	O
to	O
parameterize	O
the	O
gamma	B
distribution	I
our	O
choice	O
is	O
convenient	O
here	O
is	O
the	O
shape	O
and	O
is	O
the	O
mean	O
piecewise	B
polynomial	B
covariance	B
functions	O
with	O
compact	B
support	I
distancecovariance	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
figure	O
panel	O
covariance	B
functions	O
and	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
piecewise	B
polynomial	B
covariance	B
functions	O
with	O
compact	B
support	I
from	O
eq	O
with	O
specified	O
parameters	O
positive	O
definiteness	O
restricted	O
dimension	O
the	O
covariance	B
between	O
points	O
become	O
exactly	O
zero	O
when	O
their	O
distance	O
exceeds	O
a	O
certain	O
threshold	O
this	O
means	O
that	O
the	O
covariance	B
matrix	I
will	O
become	O
sparse	O
by	O
construction	O
leading	O
to	O
the	O
possibility	O
of	O
computational	O
the	O
challenge	O
in	O
designing	O
these	O
functions	O
is	O
how	O
to	O
guarantee	O
positive	O
definiteness	O
multiple	O
algorithms	O
for	O
deriving	O
such	O
covariance	B
functions	O
are	O
discussed	O
by	O
wendland	O
ch	O
these	O
functions	O
are	O
usually	O
not	O
positive	B
definite	I
for	O
all	O
input	O
dimensions	O
but	O
their	O
validity	O
is	O
restricted	O
up	O
to	O
some	O
maximum	O
dimension	O
d	O
below	O
we	O
give	O
examples	O
of	O
covariance	B
functions	O
kppdqr	O
which	O
are	O
positive	B
definite	I
in	O
rd	O
rj	O
c	O
q	O
where	O
j	O
b	O
d	O
the	O
properties	O
of	O
three	O
of	O
these	O
covariance	B
functions	O
are	O
illustrated	O
in	O
figure	O
these	O
covariance	B
functions	O
are	O
continuously	O
differentiable	O
and	O
thus	O
the	O
corresponding	O
processes	O
are	O
q-times	O
mean-square	O
differentiable	O
see	O
section	O
it	O
is	O
interesting	O
to	O
ask	O
to	O
what	O
extent	O
one	O
could	O
use	O
the	O
compactly-supported	O
covariance	B
functions	O
described	O
above	O
in	O
place	O
of	O
the	O
other	O
covariance	B
functions	O
mentioned	O
in	O
this	O
section	O
while	O
obtaining	O
inferences	O
that	O
are	O
similar	O
one	O
advantage	O
of	O
the	O
compact	B
support	I
is	O
that	O
it	O
gives	O
rise	O
to	O
sparsity	O
of	O
the	O
gram	B
matrix	I
which	O
could	O
be	O
exploited	O
for	O
example	O
when	O
using	O
iterative	O
solutions	O
to	O
gpr	O
problem	O
see	O
section	O
the	O
product	O
of	O
the	O
inverse	O
covariance	B
matrix	I
with	O
a	O
vector	O
e	O
g	O
for	O
prediction	O
is	O
computed	O
using	O
a	O
conjugate	O
gradient	O
algorithm	O
then	O
products	O
of	O
the	O
covariance	B
matrix	I
with	O
vectors	O
are	O
the	O
basic	O
computational	O
unit	O
and	O
these	O
can	O
obviously	O
be	O
carried	O
out	O
much	O
faster	O
if	O
the	O
matrix	O
is	O
sparse	O
distance	O
rcovariance	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
further	O
properties	O
of	O
stationary	O
covariance	B
functions	O
the	O
covariance	B
functions	O
given	O
above	O
decay	O
monotonically	O
with	O
r	O
and	O
are	O
always	O
positive	O
however	O
this	O
is	O
not	O
a	O
necessary	O
condition	O
for	O
a	O
covariance	B
function	B
for	O
example	O
yaglom	O
shows	O
that	O
kr	O
c	O
r	O
j	O
r	O
is	O
a	O
valid	O
covariance	B
function	B
for	O
and	O
this	O
function	B
has	O
the	O
form	O
of	O
a	O
damped	O
oscillation	O
anisotropic	O
versions	O
of	O
these	O
isotropic	O
covariance	B
functions	O
can	O
be	O
created	O
by	O
setting	O
for	O
some	O
positive	B
semidefinite	I
m	O
if	O
m	O
is	O
diagonal	O
this	O
implements	O
the	O
use	O
of	O
different	O
length-scales	O
on	O
different	O
dimensions	O
for	O
further	O
discussion	O
of	O
automatic	O
relevance	O
determination	B
see	O
section	O
general	O
m	O
s	O
have	O
been	O
considered	O
by	O
mat	O
ern	O
p	O
poggio	O
and	O
girosi	O
and	O
also	O
in	O
vivarelli	O
and	O
williams	O
in	O
the	O
latter	O
work	O
a	O
low-rank	O
m	O
was	O
used	O
to	O
implement	O
a	O
linear	B
dimensionality	O
reduction	O
step	O
from	O
the	O
input	O
space	O
to	O
lower-dimensional	O
feature	B
space	I
more	O
generally	O
one	O
could	O
assume	O
the	O
form	O
where	O
is	O
a	O
d	O
k	O
matrix	O
whose	O
columns	O
define	O
k	O
directions	O
of	O
high	O
relevance	O
and	O
is	O
a	O
diagonal	O
matrix	O
positive	O
entries	O
capturing	O
the	O
axisaligned	O
relevances	O
see	O
also	O
figure	O
on	O
page	O
thus	O
m	O
has	O
a	O
factor	B
analysis	I
form	O
for	O
appropriate	O
choices	O
of	O
k	O
this	O
may	O
represent	O
a	O
good	O
trade-off	O
between	O
flexibility	O
and	O
required	O
number	O
of	O
parameters	O
m	O
kx	O
the	O
kernel	B
ktx	O
stationary	O
kernels	O
can	O
also	O
be	O
defined	O
on	O
a	O
periodic	B
domain	O
and	O
can	O
be	O
readily	O
constructed	O
from	O
stationary	O
kernels	O
on	O
r	O
given	O
a	O
stationary	O
kernel	B
m	O
z	O
kx	O
ml	O
is	O
periodic	B
with	O
period	O
l	O
as	O
shown	O
in	O
section	O
and	O
sch	O
olkopf	O
and	O
smola	O
eq	O
anisotropy	B
factor	B
analysis	I
distance	O
periodization	O
if	O
dot	B
product	I
covariance	B
functions	O
x	O
can	O
as	O
we	O
have	O
already	O
mentioned	O
above	O
the	O
kernel	B
kx	O
we	O
call	O
this	O
the	O
homogeneous	O
be	O
obtained	O
from	O
linear	B
regression	I
linear	B
kernel	B
otherwise	O
it	O
is	O
inhomogeneous	O
of	O
course	O
this	O
can	O
be	O
generalized	B
x	O
by	O
using	O
a	O
general	O
covariance	B
matrix	I
p	O
on	O
the	O
to	O
kx	O
components	O
of	O
x	O
as	O
described	O
in	O
eq	O
it	O
is	O
also	O
the	O
case	O
that	O
kx	O
x	O
is	O
a	O
valid	O
covariance	B
function	B
for	O
positive	O
integer	O
p	O
because	O
of	O
the	O
general	O
result	O
that	O
a	O
positive-integer	O
power	O
of	O
a	O
given	O
covariance	B
function	B
is	O
also	O
a	O
valid	O
covariance	B
function	B
as	O
described	O
in	O
section	O
however	O
it	O
is	O
also	O
interesting	O
to	O
show	O
an	O
explicit	O
feature	B
space	I
construction	O
for	O
the	O
polynomial	B
covariance	B
function	B
we	O
consider	O
the	O
homogeneous	O
polynomial	B
case	O
as	O
the	O
inhomogeneous	O
case	O
can	O
simply	O
be	O
obtained	O
by	O
considering	O
x	O
to	O
be	O
extended	O
the	O
bias	O
term	O
could	O
also	O
be	O
included	O
in	O
the	O
general	O
expression	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
by	O
concatenating	O
a	O
constant	O
we	O
write	O
dx	O
dx	O
dx	O
dp	O
d	O
dp	O
kx	O
dx	O
dx	O
d	O
appears	O
in	O
the	O
monomial	O
under	O
the	O
constraint	O
that	O
pd	O
notice	O
that	O
this	O
sum	O
apparently	O
contains	O
dp	O
terms	O
but	O
in	O
fact	O
it	O
is	O
less	O
than	O
this	O
as	O
the	O
order	O
of	O
the	O
indices	O
in	O
the	O
monomial	O
xdp	O
is	O
unimportant	O
e	O
g	O
for	O
p	O
and	O
are	O
the	O
same	O
monomial	O
we	O
can	O
remove	O
the	O
redundancy	O
by	O
defining	O
a	O
vector	O
m	O
whose	O
entry	O
md	O
specifies	O
the	O
number	O
of	O
times	O
index	O
mi	O
p	O
thus	O
mx	O
the	O
feature	O
corresponding	O
to	O
vector	O
m	O
is	O
proportional	O
to	O
the	O
monomial	O
as	O
usual	O
we	O
define	O
giving	O
the	O
feature	O
map	B
d	O
the	O
degeneracy	O
of	O
mx	O
is	O
xmd	O
p	O
r	O
mx	O
p	O
md	O
xmd	O
d	O
dotfor	O
example	O
for	O
p	O
in	O
d	O
we	O
have	O
product	O
kernels	O
are	O
sometimes	O
used	O
in	O
a	O
normalized	O
form	O
given	O
by	O
eq	O
for	O
regression	O
problems	O
the	O
polynomial	B
kernel	B
is	O
a	O
rather	O
strange	O
choice	O
as	O
the	O
prior	O
variance	O
grows	O
rapidly	O
with	O
for	O
however	O
such	O
kernels	O
have	O
proved	O
effective	O
in	O
high-dimensional	O
classification	B
problems	O
take	O
x	O
to	O
be	O
a	O
vectorized	O
binary	B
image	O
where	O
the	O
input	O
data	O
are	O
binary	B
or	O
greyscale	O
normalized	O
to	O
on	O
each	O
dimension	O
olkopf	O
and	O
smola	O
sec	O
other	O
non-stationary	O
covariance	B
functions	O
above	O
we	O
have	O
seen	O
examples	O
of	O
non-stationary	O
dot	B
product	I
kernels	O
however	O
there	O
are	O
also	O
other	O
interesting	O
kernels	O
which	O
are	O
not	O
of	O
this	O
form	O
in	O
this	O
section	O
we	O
first	O
describe	O
the	O
covariance	B
function	B
belonging	O
to	O
a	O
particular	O
type	O
of	O
neural	B
network	I
this	O
construction	O
is	O
due	O
to	O
neal	O
consider	O
a	O
network	O
which	O
takes	O
an	O
input	O
x	O
has	O
one	O
hidden	O
layer	O
with	O
nh	O
units	O
and	O
then	O
linearly	O
combines	O
the	O
outputs	B
of	O
the	O
hidden	O
units	O
with	O
a	O
bias	O
b	O
to	O
obtain	O
fx	O
the	O
mapping	O
can	O
be	O
written	O
fx	O
b	O
vjhx	O
uj	O
where	O
the	O
vjs	O
are	O
the	O
hidden-to-output	O
weights	O
and	O
hx	O
u	O
is	O
the	O
hidden	O
unit	O
transfer	O
function	B
we	O
shall	O
assume	O
is	O
bounded	O
which	O
depends	O
on	O
the	O
input-to-hidden	O
weights	O
u	O
for	O
example	O
we	O
could	O
choose	O
hx	O
u	O
tanhx	O
u	O
this	O
architecture	O
is	O
important	O
because	O
it	O
has	O
been	O
shown	O
by	O
hornik	O
that	O
networks	O
with	O
one	O
hidden	O
layer	O
are	O
universal	O
approximators	O
as	O
the	O
number	O
of	O
nhx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
hidden	O
units	O
tends	O
to	O
infinity	O
for	O
a	O
wide	O
class	O
of	O
transfer	O
functions	O
excluding	O
polynomials	O
let	O
b	O
and	O
the	O
v	O
s	O
have	O
independent	O
zero-mean	O
distributions	O
v	O
respectively	O
and	O
let	O
the	O
weights	O
uj	O
for	O
each	O
hidden	O
unit	O
of	O
variance	O
be	O
independently	O
and	O
identically	O
distributed	O
denoting	O
all	O
weights	O
by	O
w	O
we	O
obtain	O
neal	O
b	O
and	O
ewfx	O
b	O
euhx	O
uj	O
v	O
j	O
b	O
nh	O
v	O
euhx	O
u	O
where	O
eq	O
follows	O
because	O
all	O
of	O
the	O
hidden	O
units	O
are	O
identically	O
distributed	O
the	O
final	O
term	O
in	O
equation	O
becomes	O
u	O
by	O
letting	O
v	O
scale	O
as	O
the	O
sum	O
in	O
eq	O
is	O
over	O
nh	O
identically	O
and	O
independently	O
distributed	O
random	O
variables	O
as	O
the	O
transfer	O
function	B
is	O
bounded	O
all	O
moments	O
of	O
the	O
distribution	O
will	O
be	O
bounded	O
and	O
hence	O
the	O
central	O
limit	O
theorem	O
can	O
be	O
applied	O
showing	O
that	O
the	O
stochastic	O
process	O
will	O
converge	O
to	O
a	O
gaussian	B
process	I
in	O
the	O
limit	O
as	O
nh	O
r	O
z	O
by	O
evaluating	O
euhx	O
u	O
we	O
can	O
obtain	O
the	O
covariance	B
function	B
of	O
the	O
neural	B
network	I
for	O
example	O
if	O
we	O
choose	O
the	O
error	O
function	B
hz	O
erfz	O
and	O
choose	O
u	O
n	O
then	O
we	O
obtain	O
x	O
dt	O
as	O
the	O
transfer	O
function	B
let	O
hx	O
u	O
e	O
x	O
sin	O
knnx	O
where	O
x	O
xd	O
is	O
an	O
augmented	O
input	O
vector	O
this	O
is	O
a	O
true	O
neural	B
network	I
covariance	B
function	B
the	O
sigmoid	O
kernel	B
kx	O
tanha	O
bx	O
has	O
sometimes	O
been	O
proposed	O
but	O
in	O
fact	O
this	O
kernel	B
is	O
never	O
positive	B
definite	I
and	O
is	O
thus	O
not	O
a	O
valid	O
covariance	B
function	B
see	O
e	O
g	O
sch	O
olkopf	O
and	O
smola	O
p	O
figure	O
shows	O
a	O
plot	O
of	O
the	O
neural	B
network	I
covariance	B
function	B
and	O
samples	O
from	O
the	O
prior	O
we	O
have	O
set	B
diag	O
samples	O
from	O
a	O
gp	O
with	O
this	O
covariance	B
function	B
can	O
be	O
viewed	O
as	O
superpositions	O
of	O
the	O
functions	O
where	O
controls	O
the	O
variance	O
of	O
thus	O
the	O
amount	O
of	O
offset	O
of	O
these	O
functions	O
from	O
the	O
origin	O
and	O
controls	O
u	O
and	O
thus	O
the	O
scaling	O
on	O
the	O
x-axis	O
in	O
figure	O
we	O
observe	O
that	O
the	O
sample	O
functions	O
with	O
larger	O
vary	O
more	O
quickly	O
notice	O
that	O
the	O
samples	O
display	O
the	O
non-stationarity	O
of	O
the	O
covariance	B
function	B
in	O
that	O
for	O
large	O
values	O
of	O
or	O
x	O
they	O
should	O
tend	O
to	O
a	O
constant	O
value	O
consistent	O
with	O
the	O
construction	O
as	O
a	O
superposition	O
of	O
sigmoid	O
functions	O
another	O
interesting	O
construction	O
is	O
to	O
set	B
hx	O
u	O
exp	O
g	O
where	O
g	O
sets	O
the	O
scale	O
of	O
this	O
gaussian	O
basis	O
function	B
with	O
u	O
n	O
ui	O
neural	B
network	I
covariance	B
function	B
modulated	O
squared	B
exponential	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
covariance	B
sample	O
functions	O
figure	O
panel	O
a	O
plot	O
of	O
the	O
covariance	B
function	B
knnx	O
for	O
panel	O
samples	O
drawn	O
from	O
the	O
neural	B
network	I
covariance	B
function	B
with	O
and	O
as	O
shown	O
in	O
the	O
legend	O
the	O
samples	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
equally-spaced	O
points	O
we	O
obtain	O
kgx	O
z	O
xx	O
g	O
exp	O
exp	O
g	O
uu	O
u	O
du	O
exp	O
m	O
s	O
s	O
u	O
and	O
m	O
u	O
e	O
exp	O
u	O
e	O
g	O
u	O
g	O
g	O
m	O
where	O
g	O
this	O
is	O
u	O
scaling	O
in	O
general	O
a	O
non-stationary	O
covariance	B
function	B
but	O
if	O
appropriately	O
we	O
recover	O
the	O
squared	B
exponential	B
kgx	O
exp	O
u	O
kgx	O
comprises	O
a	O
squared	B
exponential	B
covariance	B
function	B
modulated	O
by	O
the	O
gaussian	O
decay	O
envelope	O
function	B
exp	O
m	O
cf	O
the	O
vertical	O
rescaling	O
construction	O
described	O
in	O
section	O
g	O
for	O
a	O
finite	O
value	O
of	O
m	O
exp	O
one	O
way	O
to	O
introduce	O
non-stationarity	O
is	O
to	O
introduce	O
an	O
arbitrary	O
non-linear	O
mapping	O
warping	O
ux	O
of	O
the	O
input	O
x	O
and	O
then	O
use	O
a	O
stationary	O
covariance	B
function	B
in	O
u-space	O
note	O
that	O
x	O
and	O
u	O
need	O
not	O
have	O
the	O
same	O
dimensionality	O
as	O
each	O
other	O
this	O
approach	O
was	O
used	O
by	O
sampson	O
and	O
guttorp	O
to	O
model	O
patterns	O
of	O
solar	O
radiation	O
in	O
southwestern	O
british	O
columbia	O
using	O
gaussian	O
processes	O
another	O
interesting	O
example	O
of	O
this	O
warping	O
construction	O
is	O
given	O
in	O
mackay	O
where	O
the	O
one-dimensional	O
input	O
variable	O
x	O
is	O
mapped	O
to	O
the	O
two-dimensional	O
ux	O
sinx	O
to	O
give	O
rise	O
to	O
a	O
periodic	B
random	O
function	B
of	O
x	O
if	O
we	O
use	O
the	O
squared	B
exponential	B
kernel	B
in	O
u-space	O
then	O
x	O
kx	O
exp	O
as	O
x	O
warping	O
periodic	B
random	O
function	B
xinput	O
x	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
examples	O
of	O
covariance	B
functions	O
figure	O
panel	O
shows	O
the	O
chosen	O
length-scale	B
function	B
panel	O
shows	O
three	O
samples	O
from	O
the	O
gp	O
prior	O
using	O
gibbs	B
covariance	B
function	B
eq	O
this	O
figure	O
is	O
based	O
on	O
fig	O
in	O
gibbs	B
we	O
have	O
described	O
above	O
how	O
to	O
make	O
an	O
anisotropic	O
covariance	B
function	B
by	O
scaling	O
different	O
dimensions	O
differently	O
however	O
we	O
are	O
not	O
free	O
to	O
make	O
these	O
length-scales	O
d	O
be	O
functions	O
of	O
x	O
as	O
this	O
will	O
not	O
in	O
general	O
produce	O
a	O
valid	O
covariance	B
function	B
gibbs	B
derived	O
the	O
covariance	B
function	B
varying	O
length-scale	B
kx	O
dy	O
dx	O
dx	O
dx	O
exp	O
dx	O
where	O
each	O
ix	O
is	O
an	O
arbitrary	O
positive	O
function	B
of	O
x	O
note	O
that	O
kx	O
x	O
for	O
all	O
x	O
this	O
covariance	B
function	B
is	O
obtained	O
by	O
considering	O
a	O
grid	O
of	O
n	O
gaussian	O
basis	O
functions	O
with	O
centres	O
cj	O
and	O
a	O
corresponding	O
length-scale	B
on	O
input	O
dimension	O
d	O
which	O
varies	O
as	O
a	O
positive	O
function	B
dcj	O
taking	O
the	O
limit	O
as	O
n	O
the	O
sum	O
turns	O
into	O
an	O
integral	O
and	O
after	O
some	O
algebra	O
eq	O
is	O
obtained	O
an	O
example	O
of	O
a	O
variable	O
length-scale	B
function	B
and	O
samples	O
from	O
the	O
prior	O
corresponding	O
to	O
eq	O
are	O
shown	O
in	O
figure	O
notice	O
that	O
as	O
the	O
lengthscale	O
gets	O
shorter	O
the	O
sample	O
functions	O
vary	O
more	O
rapidly	O
as	O
one	O
would	O
expect	O
the	O
large	O
length-scale	B
regions	O
on	O
either	O
side	O
of	O
the	O
short	O
length-scale	B
region	O
can	O
be	O
quite	O
strongly	O
correlated	B
if	O
one	O
tries	O
the	O
converse	O
experiment	O
by	O
creating	O
a	O
length-scale	B
function	B
which	O
has	O
a	O
longer	O
length-scale	B
region	O
between	O
two	O
shorter	O
ones	O
then	O
the	O
behaviour	O
may	O
not	O
be	O
quite	O
what	O
is	O
expected	O
on	O
initially	O
transitioning	O
into	O
the	O
long	O
length-scale	B
region	O
the	O
covariance	B
drops	O
off	O
quite	O
sharply	O
due	O
to	O
the	O
prefactor	O
in	O
eq	O
before	O
stabilizing	O
to	O
a	O
slower	O
variation	O
see	O
gibbs	B
sec	O
for	O
further	O
details	O
exercises	O
and	O
invite	O
you	O
to	O
investigate	O
this	O
further	O
paciorek	O
and	O
schervish	O
have	O
generalized	B
gibbs	B
construction	O
to	O
obtain	O
non-stationary	O
versions	O
of	O
arbitrary	O
isotropic	O
covariance	B
functions	O
let	O
ks	O
be	O
a	O
lxinput	O
xoutput	O
fx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
covariance	B
function	B
constant	O
linear	B
polynomial	B
squared	B
exponential	B
mat	O
ern	O
exponential	B
rational	B
quadratic	I
neural	B
network	I
d	O
expression	O
pd	O
exp	O
r	O
sin	O
exp	O
r	O
r	O
k	O
r	O
x	O
x	O
s	O
nd	O
table	O
summary	O
of	O
several	O
commonly-used	O
covariance	B
functions	O
the	O
covariances	O
are	O
written	O
either	O
as	O
a	O
function	B
of	O
x	O
and	O
or	O
as	O
a	O
function	B
of	O
r	O
two	O
columns	O
marked	O
s	O
and	O
nd	O
indicate	O
whether	O
the	O
covariance	B
functions	O
are	O
stationary	O
and	O
nondegenerate	B
respectively	O
degenerate	B
covariance	B
functions	O
have	O
finite	O
rank	O
see	O
section	O
for	O
more	O
discussion	O
of	O
this	O
issue	O
stationary	O
isotropic	O
covariance	B
function	B
that	O
is	O
valid	O
in	O
every	O
euclidean	O
space	O
rd	O
for	O
d	O
let	O
be	O
a	O
d	O
d	O
matrix-valued	O
function	B
which	O
is	O
positive	B
definite	I
for	O
all	O
x	O
and	O
let	O
i	O
set	B
of	O
gibbs	B
ix	O
functions	O
define	O
a	O
diagonal	O
then	O
define	O
the	O
quadratic	B
form	I
qij	O
xj	O
i	O
xj	O
paciorek	O
and	O
schervish	O
show	O
that	O
knsxi	O
xj	O
i	O
j	O
wiener	B
process	I
is	O
a	O
valid	O
non-stationary	O
covariance	B
function	B
in	O
chapter	O
we	O
described	O
the	O
linear	B
regression	I
model	O
in	O
feature	B
space	I
fx	O
o	O
hagan	O
suggested	O
making	O
w	O
a	O
function	B
of	O
x	O
to	O
allow	O
for	O
different	O
values	O
of	O
w	O
to	O
be	O
appropriate	O
in	O
different	O
regions	O
thus	O
he	O
put	O
a	O
gaussian	B
process	I
prior	O
on	O
w	O
of	O
the	O
form	O
covwx	O
for	O
some	O
positive	B
definite	I
matrix	I
giving	O
rise	O
to	O
a	O
prior	O
on	O
fx	O
with	O
covariance	B
kf	O
finally	O
we	O
note	O
that	O
the	O
wiener	B
process	I
with	O
covariance	B
function	B
kx	O
minx	O
is	O
a	O
fundamental	O
non-stationary	O
process	O
see	O
section	O
and	O
texts	O
such	O
as	O
grimmett	O
and	O
stirzaker	O
ch	O
for	O
further	O
details	O
making	O
new	O
kernels	O
from	O
old	O
in	O
the	O
previous	O
sections	O
we	O
have	O
developed	O
many	O
covariance	B
functions	O
some	O
of	O
which	O
are	O
summarized	O
in	O
table	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
combine	O
or	O
modify	O
existing	O
covariance	B
functions	O
to	O
make	O
new	O
ones	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
sum	O
product	O
vertical	O
rescaling	O
convolution	O
direct	O
sum	O
tensor	O
product	O
additive	O
model	O
functional	B
anova	B
examples	O
of	O
covariance	B
functions	O
the	O
sum	O
of	O
two	O
kernels	O
is	O
a	O
kernel	B
proof	O
consider	O
the	O
random	O
process	O
fx	O
where	O
and	O
are	O
independent	O
then	O
kx	O
this	O
construction	O
can	O
be	O
used	O
e	O
g	O
to	O
add	O
together	O
kernels	O
with	O
different	O
characteristic	O
length-scales	O
the	O
product	O
of	O
two	O
kernels	O
is	O
a	O
kernel	B
proof	O
consider	O
the	O
random	O
process	O
fx	O
where	O
and	O
are	O
independent	O
then	O
kx	O
a	O
simple	O
extension	O
of	O
this	O
argument	O
means	O
that	O
kpx	O
is	O
a	O
valid	O
covariance	B
function	B
for	O
p	O
n	O
let	O
ax	O
be	O
a	O
given	O
deterministic	O
function	B
and	O
consider	O
gx	O
axfx	O
where	O
fx	O
is	O
a	O
random	O
process	O
then	O
covgx	O
axkx	O
such	O
a	O
construction	O
can	O
be	O
used	O
to	O
normalize	O
kernels	O
by	O
choosing	O
ax	O
k	O
x	O
kx	O
x	O
x	O
so	O
that	O
kx	O
pkx	O
kx	O
this	O
ensures	O
that	O
kx	O
x	O
for	O
all	O
x	O
we	O
can	O
also	O
obtain	O
a	O
new	O
process	O
by	O
convolution	O
blurring	O
consider	O
an	O
arbitrary	O
fixed	O
kernel	B
hx	O
z	O
and	O
the	O
map	B
gx	O
r	O
hx	O
zfz	O
dz	O
then	O
clearly	O
covgx	O
hx	O
zkz	O
dz	O
if	O
shirani	O
has	O
the	O
form	O
fx	O
c	O
fx	O
c	O
form	O
kx	O
are	O
covariance	B
functions	O
over	O
different	O
spaces	O
and	O
and	O
then	O
the	O
direct	O
sum	O
kx	O
and	O
the	O
tensor	O
product	O
kx	O
are	O
also	O
covariance	B
functions	O
on	O
the	O
product	O
space	O
by	O
virtue	O
of	O
the	O
sum	O
and	O
product	O
constructions	O
the	O
direct	O
sum	O
construction	O
can	O
be	O
further	O
generalized	B
consider	O
a	O
function	B
fx	O
where	O
x	O
is	O
d-dimensional	O
an	O
additive	O
model	O
and	O
tibi	O
e	O
a	O
linear	B
combination	O
of	O
functions	O
of	O
one	O
variable	O
if	O
the	O
individual	O
fi	O
s	O
are	O
taken	O
to	O
be	O
independent	O
stochastic	O
processes	O
then	O
the	O
covariance	B
function	B
of	O
f	O
will	O
have	O
the	O
form	O
of	O
a	O
direct	O
sum	O
if	O
we	O
now	O
admit	O
interactions	O
of	O
two	O
variables	O
so	O
that	O
ijji	O
fijxi	O
xj	O
and	O
the	O
various	O
fi	O
s	O
and	O
fij	O
s	O
are	O
independent	O
stochastic	O
processes	O
then	O
the	O
covariance	B
function	B
will	O
have	O
the	O
j	O
indeed	O
this	O
process	O
can	O
be	O
extended	O
further	O
to	O
provide	O
a	O
functional	B
decomposition	O
ranging	O
from	O
a	O
simple	O
additive	O
model	O
up	O
to	O
full	O
interaction	O
of	O
all	O
d	O
input	O
variables	O
sum	O
can	O
also	O
be	O
truncated	O
at	O
some	O
stage	O
wahba	O
ch	O
and	O
stitson	O
et	O
al	O
suggest	O
using	O
tensor	O
products	O
for	O
kernels	O
with	O
interactions	O
so	O
that	O
in	O
the	O
example	O
above	O
kijxi	O
xj	O
j	O
would	O
have	O
the	O
form	O
kixi	O
j	O
note	O
that	O
if	O
d	O
is	O
large	O
then	O
the	O
large	O
number	O
of	O
pairwise	O
higher-order	O
terms	O
may	O
be	O
problematic	O
plate	O
has	O
investigated	O
using	O
a	O
combination	O
of	O
additive	O
gp	O
models	O
plus	O
a	O
general	O
covariance	B
function	B
that	O
permits	O
full	O
interactions	O
pi	O
kijxi	O
xj	O
i	O
ikjxj	O
i	O
i	O
and	O
are	O
gaussian	O
processes	O
then	O
the	O
product	O
f	O
will	O
not	O
in	O
general	O
be	O
a	O
gaussian	B
process	I
but	O
there	O
exists	O
a	O
gp	O
with	O
this	O
covariance	B
function	B
stands	O
for	O
analysis	O
of	O
variance	O
a	O
statistical	O
technique	O
that	O
analyzes	O
the	O
interac	O
tions	O
between	O
various	O
attributes	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
eigenfunction	B
analysis	O
of	O
kernels	O
we	O
first	O
define	O
eigenvalues	O
and	O
eigenfunctions	O
and	O
discuss	O
mercer	O
s	O
theorem	O
which	O
allows	O
us	O
to	O
express	O
the	O
kernel	B
certain	O
conditions	O
in	O
terms	O
of	O
these	O
quantities	O
section	O
gives	O
the	O
analytical	O
solution	O
of	O
the	O
eigenproblem	O
for	O
the	O
se	O
kernel	B
under	O
a	O
gaussian	O
measure	B
section	O
discusses	O
how	O
to	O
compute	O
approximate	O
eigenfunctions	O
numerically	O
for	O
cases	O
where	O
the	O
exact	O
solution	O
is	O
not	O
known	O
it	O
turns	O
out	O
that	O
gaussian	B
process	I
regression	I
can	O
be	O
viewed	O
as	O
bayesian	O
linear	B
regression	I
with	O
a	O
possibly	O
infinite	O
number	O
of	O
basis	O
functions	O
as	O
discussed	O
in	O
chapter	O
one	O
possible	O
basis	O
set	B
is	O
the	O
eigenfunctions	O
of	O
the	O
covariance	B
function	B
a	O
function	B
that	O
obeys	O
the	O
integral	O
equation	O
kx	O
d	O
z	O
eigenvalue	B
eigenfunction	B
mercer	O
s	O
theorem	O
is	O
called	O
an	O
eigenfunction	B
of	O
kernel	B
k	O
with	O
eigenvalue	B
with	O
respect	O
to	O
the	O
two	O
measures	O
of	O
particular	O
interest	O
to	O
us	O
will	O
be	O
lebesgue	O
measure	B
over	O
a	O
compact	O
subset	O
c	O
of	O
rd	O
or	O
when	O
there	O
is	O
a	O
density	O
px	O
so	O
that	O
d	O
can	O
be	O
written	O
pxdx	O
in	O
general	O
there	O
are	O
an	O
infinite	O
number	O
of	O
eigenfunctions	O
which	O
we	O
label	O
we	O
assume	O
the	O
ordering	O
is	O
chosen	O
such	O
that	O
the	O
eigenfunctions	O
are	O
orthogonal	O
with	O
respect	O
to	O
and	O
can	O
be	O
chosen	O
to	O
be	O
normalized	O
so	O
thatr	O
ix	O
jx	O
d	O
ij	O
where	O
ij	O
is	O
the	O
kronecker	O
delta	O
mercer	O
s	O
theorem	O
e	O
g	O
k	O
onig	O
allows	O
us	O
to	O
express	O
the	O
kernel	B
k	O
in	O
terms	O
of	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
theorem	O
s	O
theorem	O
let	O
be	O
a	O
finite	O
measure	B
space	O
and	O
k	O
l	O
be	O
a	O
kernel	B
such	O
that	O
tk	O
is	O
positive	B
definite	I
eq	O
let	O
i	O
be	O
the	O
normalized	O
eigenfunctions	O
of	O
tk	O
associated	O
with	O
the	O
eigenvalues	O
i	O
then	O
the	O
eigenvalues	O
i	O
are	O
absolutely	O
summable	O
x	O
kx	O
i	O
ix	O
i	O
holds	O
almost	O
everywhere	O
where	O
the	O
series	O
converges	O
absolutely	O
and	O
uniformly	O
almost	O
everywhere	O
this	O
decomposition	O
is	O
just	O
the	O
infinite-dimensional	O
analogue	O
of	O
the	O
diagonalization	O
of	O
a	O
hermitian	O
matrix	O
note	O
that	O
the	O
sum	O
may	O
terminate	O
at	O
some	O
value	O
n	O
n	O
the	O
eigenvalues	O
beyond	O
n	O
are	O
zero	O
or	O
the	O
sum	O
may	O
be	O
infinite	O
we	O
have	O
the	O
following	O
definition	O
et	O
al	O
p	O
further	O
explanation	O
of	O
measure	B
see	O
appendix	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
eigenfunction	B
analysis	O
of	O
kernels	O
definition	O
a	O
degenerate	B
kernel	B
has	O
only	O
a	O
finite	O
number	O
of	O
non-zero	O
values	O
a	O
degenerate	B
kernel	B
is	O
also	O
said	O
to	O
have	O
finite	O
rank	O
if	O
a	O
kernel	B
is	O
not	O
degenerate	B
it	O
is	O
said	O
to	O
be	O
nondegenerate	B
as	O
an	O
example	O
a	O
n-dimensional	O
linear	B
regression	I
model	O
in	O
feature	B
space	I
eq	O
gives	O
rise	O
to	O
a	O
degenerate	B
kernel	B
with	O
at	O
most	O
n	O
non-zero	O
eigenvalues	O
course	O
if	O
the	O
measure	B
only	O
puts	O
weight	O
on	O
a	O
finite	O
number	O
of	O
points	O
n	O
in	O
x-space	O
then	O
the	O
eigendecomposition	O
is	O
simply	O
that	O
of	O
a	O
n	O
n	O
matrix	O
even	O
if	O
the	O
kernel	B
is	O
nondegenerate	B
degenerate	B
nondegenerate	B
kernel	B
the	O
statement	O
of	O
mercer	O
s	O
theorem	O
above	O
referred	O
to	O
a	O
finite	O
measure	B
if	O
we	O
replace	O
this	O
with	O
lebesgue	O
measure	B
and	O
consider	O
a	O
stationary	O
covariance	B
function	B
then	O
directly	O
from	O
bochner	O
s	O
theorem	O
eq	O
we	O
obtain	O
kx	O
is	O
d	O
d	O
z	O
rd	O
is	O
is	O
z	O
rd	O
the	O
complex	O
exponentials	O
is	O
x	O
are	O
the	O
eigenfunctions	O
of	O
a	O
stationary	O
kernel	B
w	O
r	O
t	O
lebesgue	O
measure	B
note	O
the	O
similarity	O
to	O
eq	O
except	O
that	O
the	O
summation	O
has	O
been	O
replaced	O
by	O
an	O
integral	O
the	O
rate	O
of	O
decay	O
of	O
the	O
eigenvalues	O
gives	O
important	O
information	O
about	O
the	O
smoothness	O
of	O
the	O
kernel	B
for	O
example	O
ritter	O
et	O
al	O
showed	O
that	O
in	O
with	O
uniform	O
on	O
processes	O
which	O
are	O
r-times	O
mean-square	O
differentiable	O
have	O
i	O
i	O
asymptotically	O
this	O
makes	O
sense	O
as	O
rougher	O
processes	O
have	O
more	O
power	O
at	O
high	O
frequencies	O
and	O
so	O
their	O
eigenvalue	B
spectrum	O
decays	O
more	O
slowly	O
the	O
same	O
phenomenon	O
can	O
be	O
read	O
off	O
from	O
the	O
power	O
spectrum	O
of	O
the	O
mat	O
ern	O
class	O
as	O
given	O
in	O
eq	O
hawkins	O
gives	O
the	O
exact	O
eigenvalue	B
spectrum	O
for	O
the	O
ou	O
process	O
on	O
widom	O
gives	O
an	O
asymptotic	O
analysis	O
of	O
the	O
eigenvalues	O
of	O
stationary	O
kernels	O
taking	O
into	O
account	O
the	O
effect	O
of	O
the	O
density	O
d	O
pxdx	O
bach	O
and	O
jordan	O
table	O
use	O
these	O
results	O
to	O
show	O
the	O
effect	O
of	O
varying	O
px	O
for	O
the	O
se	O
kernel	B
an	O
exact	O
eigenanalysis	O
of	O
the	O
se	O
kernel	B
under	O
the	O
gaussian	O
density	O
is	O
given	O
in	O
the	O
next	O
section	O
an	O
analytic	O
example	O
for	O
the	O
case	O
that	O
px	O
is	O
a	O
gaussian	O
and	O
for	O
the	O
squared-exponential	O
kernel	B
kx	O
exp	O
there	O
are	O
analytic	O
results	O
for	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
as	O
given	O
by	O
zhu	O
et	O
al	O
sec	O
putting	O
px	O
n	O
we	O
find	O
that	O
the	O
eigenvalues	O
k	O
and	O
eigenfunctions	O
k	O
convenience	O
let	O
k	O
are	O
given	O
by	O
a	O
k	O
kx	O
bk	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
figure	O
the	O
first	O
eigenfunctions	O
of	O
the	O
squared	B
exponential	B
kernel	B
w	O
r	O
t	O
a	O
gaussian	O
density	O
the	O
value	O
of	O
k	O
is	O
equal	O
to	O
the	O
number	O
of	O
zero-crossings	O
of	O
the	O
function	B
the	O
dashed	O
line	O
is	O
proportional	O
to	O
the	O
density	O
px	O
where	O
hkx	O
dk	O
gradshteyn	O
and	O
ryzhik	O
sec	O
a	O
b	O
and	O
dxk	O
exp	O
is	O
the	O
kth	O
order	O
hermite	O
polynomial	B
a	O
a	O
b	O
c	O
b	O
ba	O
hints	O
on	O
the	O
proof	O
of	O
this	O
result	O
are	O
given	O
in	O
exercise	O
a	O
plot	O
of	O
the	O
first	O
three	O
eigenfunctions	O
for	O
a	O
and	O
b	O
is	O
shown	O
in	O
figure	O
the	O
result	O
for	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
is	O
readily	O
generalized	B
to	O
the	O
multivariate	O
case	O
when	O
the	O
kernel	B
and	O
gaussian	O
density	O
are	O
products	O
of	O
the	O
univariate	O
expressions	O
as	O
the	O
eigenfunctions	O
and	O
eigenvalues	O
will	O
simply	O
be	O
products	O
too	O
for	O
the	O
case	O
that	O
a	O
and	O
b	O
are	O
equal	O
on	O
all	O
d	O
dimensions	O
the	O
degeneracy	O
of	O
the	O
eigenvalue	B
which	O
is	O
okd	O
as	O
th	O
eigenvalue	B
has	O
a	O
value	O
given	O
by	O
we	O
see	O
that	O
a	O
d	O
pk	O
d	O
a	O
and	O
this	O
can	O
be	O
used	O
to	O
determine	O
the	O
rate	O
of	O
decay	O
of	O
the	O
spectrum	O
d	O
d	O
numerical	O
approximation	O
of	O
eigenfunctions	O
the	O
standard	O
numerical	O
method	O
for	O
approximating	O
the	O
eigenfunctions	O
and	O
eigenvalues	O
of	O
eq	O
is	O
to	O
use	O
a	O
numerical	O
routine	O
to	O
approximate	O
the	O
integral	O
e	O
g	O
baker	O
ch	O
for	O
example	O
letting	O
d	O
pxdx	O
in	O
eq	O
one	O
could	O
use	O
the	O
approximation	O
c	O
z	O
i	O
kx	O
ix	O
dx	O
n	O
kxl	O
ixl	O
nx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
kernels	O
for	O
non-vectorial	O
inputs	O
where	O
the	O
xl	O
s	O
are	O
sampled	O
from	O
px	O
plugging	O
in	O
xl	O
for	O
l	O
n	O
into	O
eq	O
we	O
obtain	O
the	O
matrix	O
eigenproblem	O
kui	O
mat	O
i	O
ui	O
i	O
nuij	O
where	O
the	O
where	O
k	O
is	O
the	O
n	O
n	O
gram	B
matrix	I
with	O
entries	O
kij	O
kxi	O
xj	O
mat	O
is	O
the	O
ith	O
i	O
ui	O
we	O
have	O
ixj	O
matrix	O
eigenvalue	B
and	O
ui	O
is	O
the	O
corresponding	O
eigenvector	O
so	O
that	O
u	O
n	O
factor	O
arises	O
from	O
the	O
differing	O
normalizations	O
of	O
the	O
eigenvector	O
and	O
eigenfunction	B
thus	O
is	O
an	O
obvious	O
estimator	O
for	O
i	O
for	O
i	O
n	O
for	O
fixed	O
n	O
one	O
would	O
expect	O
that	O
the	O
larger	O
eigenvalues	O
would	O
be	O
better	O
estimated	O
than	O
the	O
smaller	O
ones	O
the	O
theory	O
of	O
the	O
numerical	O
solution	O
of	O
eigenvalue	B
problems	O
shows	O
that	O
for	O
a	O
fixed	O
i	O
will	O
converge	O
to	O
i	O
in	O
the	O
limit	O
that	O
n	O
theorem	O
n	O
mat	O
it	O
is	O
also	O
possible	O
to	O
study	O
the	O
convergence	O
further	O
for	O
example	O
it	O
is	O
quite	O
easy	O
using	O
the	O
properties	O
of	O
principal	O
components	O
analysis	O
in	O
feature	B
space	I
to	O
show	O
that	O
for	O
any	O
l	O
l	O
n	O
en	O
i	O
and	O
en	O
i	O
where	O
en	O
denotes	O
expectation	O
with	O
respect	O
to	O
samples	O
of	O
size	O
n	O
drawn	O
from	O
px	O
for	O
further	O
details	O
see	O
shawe-taylor	O
and	O
williams	O
pl	O
pn	O
pn	O
mat	O
i	O
pl	O
n	O
mat	O
i	O
mat	O
i	O
n	O
i	O
n	O
nystr	O
om	O
method	O
kernel	B
pca	I
the	O
nystr	O
om	O
method	O
for	O
approximating	O
the	O
ith	O
eigenfunction	B
baker	O
and	O
press	O
et	O
al	O
section	O
is	O
given	O
by	O
n	O
mat	O
i	O
where	O
kxn	O
which	O
is	O
obtained	O
from	O
eq	O
by	O
dividing	O
both	O
sides	O
by	O
i	O
equation	O
extends	O
the	O
approximation	O
ixj	O
nuij	O
from	O
the	O
sample	O
points	O
xn	O
to	O
all	O
x	O
there	O
is	O
an	O
interesting	O
relationship	O
between	O
the	O
kernel	B
pca	I
method	O
of	O
sch	O
olkopf	O
et	O
al	O
and	O
the	O
eigenfunction	B
expansion	O
discussed	O
above	O
the	O
eigenfunction	B
expansion	O
has	O
least	O
potentially	O
an	O
infinite	O
number	O
of	O
nonzero	O
eigenvalues	O
in	O
contrast	O
the	O
kernel	B
pca	I
algorithm	O
operates	O
on	O
the	O
n	O
n	O
matrix	O
k	O
and	O
yields	O
n	O
eigenvalues	O
and	O
eigenvectors	O
eq	O
clarifies	O
the	O
relationship	O
between	O
the	O
two	O
however	O
note	O
that	O
eq	O
is	O
identical	O
to	O
scaling	O
factors	O
to	O
sch	O
olkopf	O
et	O
al	O
eq	O
which	O
describes	O
the	O
projection	O
of	O
a	O
new	O
point	O
onto	O
the	O
ith	O
eigenvector	O
in	O
the	O
kernel	B
pca	I
feature	B
space	I
kernels	O
for	O
non-vectorial	O
inputs	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
assumed	O
that	O
the	O
input	O
x	O
is	O
a	O
vector	O
measuring	O
the	O
values	O
of	O
a	O
number	O
of	O
attributes	O
features	O
however	O
for	O
some	O
learning	B
problems	O
the	O
inputs	O
are	O
not	O
vectors	O
but	O
structured	O
objects	O
such	O
as	O
strings	O
trees	O
or	O
general	O
graphs	O
for	O
example	O
we	O
may	O
have	O
a	O
biological	O
problem	O
where	O
we	O
want	O
to	O
classify	O
proteins	O
as	O
strings	O
of	O
amino	O
acid	O
are	O
initially	O
made	O
up	O
of	O
different	O
amino	O
acids	O
of	O
which	O
a	O
few	O
may	O
later	O
be	O
modified	O
bringing	O
the	O
total	O
number	O
up	O
to	O
or	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
or	O
our	O
input	O
may	O
be	O
parse-trees	O
derived	O
from	O
a	O
linguistic	O
analysis	O
or	O
we	O
may	O
wish	O
to	O
represent	O
chemical	O
compounds	O
as	O
labelled	O
graphs	O
with	O
vertices	O
denoting	O
atoms	O
and	O
edges	O
denoting	O
bonds	O
to	O
follow	O
the	O
discriminative	B
approach	I
we	O
need	O
to	O
extract	O
some	O
features	O
from	O
the	O
input	O
objects	O
and	O
build	O
a	O
predictor	O
using	O
these	O
features	O
a	O
classification	B
problem	O
the	O
alternative	O
generative	B
approach	I
would	O
construct	O
class-conditional	O
models	O
over	O
the	O
objects	O
themselves	O
below	O
we	O
describe	O
two	O
approaches	O
to	O
this	O
feature	O
extraction	O
problem	O
and	O
the	O
efficient	O
computation	O
of	O
kernels	O
from	O
them	O
in	O
section	O
we	O
cover	O
string	B
kernels	O
and	O
in	O
section	O
we	O
describe	O
fisher	B
kernels	O
there	O
exist	O
other	O
proposals	O
for	O
constructing	O
kernels	O
for	O
strings	O
for	O
example	O
watkins	O
describes	O
the	O
use	O
of	O
pair	O
hidden	O
markov	O
models	O
that	O
generate	O
output	O
symbols	O
for	O
two	O
strings	O
conditional	B
on	O
the	O
hidden	O
state	O
for	O
this	O
purpose	O
string	B
kernels	O
we	O
start	O
by	O
defining	O
some	O
notation	O
for	O
strings	O
let	O
a	O
be	O
a	O
finite	O
alphabet	O
of	O
characters	O
the	O
concatenation	O
of	O
strings	O
x	O
and	O
y	O
is	O
written	O
xy	O
and	O
denotes	O
the	O
length	O
of	O
string	B
x	O
the	O
string	B
s	O
is	O
a	O
substring	O
of	O
x	O
if	O
we	O
can	O
write	O
x	O
usv	O
for	O
some	O
empty	O
u	O
s	O
and	O
v	O
let	O
sx	O
denote	O
the	O
number	O
of	O
times	O
that	O
substring	O
s	O
appears	O
in	O
string	B
x	O
then	O
we	O
define	O
the	O
kernel	B
between	O
two	O
strings	O
x	O
and	O
as	O
kx	O
x	O
s	O
a	O
ws	O
sx	O
where	O
ws	O
is	O
a	O
non-negative	O
weight	O
for	O
substring	O
s	O
for	O
example	O
we	O
could	O
set	B
ws	O
where	O
so	O
that	O
shorter	O
substrings	O
get	O
more	O
weight	O
than	O
longer	O
ones	O
a	O
number	O
of	O
interesting	O
special	O
cases	O
are	O
contained	O
in	O
the	O
definition	O
setting	O
ws	O
for	O
gives	O
the	O
bag-of-characters	B
kernel	B
this	O
takes	O
the	O
feature	O
vector	O
for	O
a	O
string	B
x	O
to	O
be	O
the	O
number	O
of	O
times	O
that	O
each	O
character	O
in	O
a	O
appears	O
in	O
x	O
in	O
text	O
analysis	O
we	O
may	O
wish	O
to	O
consider	O
the	O
frequencies	O
of	O
word	O
occurrence	O
if	O
we	O
require	O
s	O
to	O
be	O
bordered	O
by	O
whitespace	O
then	O
a	O
bag-of-words	O
representation	O
is	O
obtained	O
although	O
this	O
is	O
a	O
very	O
simple	O
model	O
of	O
text	O
ignores	O
word	O
order	O
it	O
can	O
be	O
surprisingly	O
effective	O
for	O
document	O
classification	B
and	O
retrieval	O
tasks	O
see	O
e	O
g	O
hand	O
et	O
al	O
sec	O
the	O
weights	O
can	O
be	O
set	B
differently	O
for	O
different	O
words	O
e	O
g	O
using	O
the	O
term	O
frequency	O
inverse	O
document	O
frequency	O
weighting	O
scheme	O
developed	O
in	O
the	O
information	O
retrieval	O
area	O
and	O
buckley	O
if	O
we	O
only	O
consider	O
substrings	O
of	O
length	O
k	O
then	O
we	O
obtain	O
the	O
k-spectrum	B
kernel	B
et	O
al	O
bag-of-characters	B
bag-of-words	O
k-spectrum	B
kernel	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
kernels	O
for	O
non-vectorial	O
inputs	O
importantly	O
there	O
are	O
efficient	O
methods	O
using	O
suffix	O
trees	O
that	O
can	O
compute	O
a	O
string	B
kernel	B
kx	O
in	O
time	O
linear	B
in	O
some	O
restrictions	O
on	O
the	O
weights	O
et	O
al	O
vishwanathan	O
and	O
smola	O
work	O
on	O
string	B
kernels	O
was	O
started	O
by	O
watkins	O
and	O
haussler	O
there	O
are	O
many	O
further	O
developments	O
of	O
the	O
methods	O
we	O
have	O
described	O
above	O
for	O
example	O
lodhi	O
et	O
al	O
go	O
beyond	O
substrings	O
to	O
consider	O
subsequences	O
of	O
x	O
which	O
are	O
not	O
necessarily	O
contiguous	O
and	O
leslie	O
et	O
al	O
describe	O
mismatch	O
string	B
kernels	O
which	O
allow	O
substrings	O
s	O
and	O
of	O
x	O
and	O
respectively	O
to	O
match	O
if	O
there	O
are	O
at	O
most	O
m	O
mismatches	O
between	O
them	O
we	O
expect	O
further	O
developments	O
in	O
this	O
area	O
tailoring	O
engineering	O
the	O
string	B
kernels	O
to	O
have	O
properties	O
that	O
make	O
sense	O
in	O
a	O
particular	O
domain	O
the	O
idea	O
of	O
string	B
kernels	O
where	O
we	O
consider	O
matches	O
of	O
substrings	O
can	O
easily	O
be	O
extended	O
to	O
trees	O
e	O
g	O
by	O
looking	O
at	O
matches	O
of	O
subtrees	O
and	O
duffy	O
leslie	O
et	O
al	O
have	O
applied	O
string	B
kernels	O
to	O
the	O
classification	B
of	O
protein	O
domains	O
into	O
superfamilies	O
the	O
results	O
obtained	O
were	O
significantly	O
better	O
than	O
methods	O
based	O
on	O
either	O
searches	O
or	O
a	O
generative	O
hidden	B
markov	I
model	I
classifier	O
similar	O
results	O
were	O
obtained	O
by	O
jaakkola	O
et	O
al	O
using	O
a	O
fisher	B
kernel	B
in	O
the	O
next	O
section	O
saunders	O
et	O
al	O
have	O
also	O
described	O
the	O
use	O
of	O
string	B
kernels	O
on	O
the	O
problem	O
of	O
classifying	O
natural	O
language	O
newswire	O
stories	O
from	O
the	O
database	O
into	O
ten	O
classes	O
fisher	B
kernels	O
score	O
vector	O
as	O
explained	O
above	O
our	O
problem	O
is	O
that	O
the	O
input	O
x	O
is	O
a	O
structured	O
object	O
of	O
arbitrary	O
size	O
e	O
g	O
a	O
string	B
and	O
we	O
wish	O
to	O
extract	O
features	O
from	O
it	O
the	O
fisher	B
kernel	B
by	O
jaakkola	O
et	O
al	O
does	O
this	O
by	O
taking	O
a	O
generative	O
model	O
px	O
where	O
is	O
a	O
vector	O
of	O
parameters	O
and	O
computing	O
the	O
feature	O
vector	O
log	O
px	O
is	O
sometimes	O
called	O
the	O
score	O
vector	O
in	O
string	B
x	O
then	O
a	O
markov	O
model	O
gives	O
px	O
take	O
for	O
example	O
a	O
markov	O
model	O
for	O
strings	O
let	O
xk	O
be	O
the	O
kth	O
symbol	O
a	O
where	O
a	O
here	O
gives	O
the	O
probability	O
that	O
will	O
be	O
the	O
jth	O
symbol	O
in	O
the	O
alphabet	O
a	O
and	O
a	O
is	O
a	O
stochastic	O
matrix	O
with	O
ajk	O
giving	O
the	O
probability	O
that	O
kxi	O
j	O
given	O
such	O
a	O
model	O
it	O
is	O
straightforward	O
to	O
compute	O
the	O
score	O
vector	O
for	O
a	O
given	O
x	O
it	O
is	O
also	O
possible	O
to	O
consider	O
other	O
generative	O
models	O
px	O
for	O
example	O
we	O
might	O
try	O
a	O
kth-order	O
markov	O
model	O
where	O
xi	O
is	O
predicted	O
by	O
the	O
preceding	O
k	O
symbols	O
see	O
leslie	O
et	O
al	O
and	O
saunders	O
et	O
al	O
for	O
an	O
interesting	O
discussion	O
of	O
the	O
similarities	O
of	O
the	O
features	O
used	O
in	O
the	O
k-spectrum	B
kernel	B
and	O
the	O
score	O
vector	O
derived	O
from	O
an	O
order	O
k	O
markov	O
model	O
see	O
also	O
exercise	O
classification	B
of	O
proteins	O
database	O
httpscop	O
mrc-lmb	O
cam	O
ac	O
ukscop	O
iterative	O
basic	O
local	O
alignment	B
search	O
tool	O
see	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
fisher	B
information	I
matrix	I
fisher	B
kernel	B
top	O
kernel	B
another	O
interesting	O
choice	O
is	O
to	O
use	O
a	O
hidden	B
markov	I
model	I
as	O
the	O
generative	O
model	O
as	O
discussed	O
by	O
jaakkola	O
et	O
al	O
see	O
also	O
exercise	O
for	O
a	O
linear	B
kernel	B
derived	O
from	O
an	O
isotropic	O
gaussian	O
model	O
for	O
x	O
rd	O
we	O
define	O
a	O
kernel	B
kx	O
based	O
on	O
the	O
score	O
vectors	O
for	O
x	O
and	O
one	O
simple	O
choice	O
is	O
to	O
set	B
kx	O
where	O
m	O
is	O
a	O
strictly	O
positive	B
definite	I
matrix	I
alternatively	O
we	O
might	O
use	O
the	O
squared	B
exponential	B
kernel	B
kx	O
exp	O
for	O
some	O
the	O
structure	O
of	O
px	O
as	O
varies	O
has	O
been	O
studied	O
extensively	O
in	O
information	O
geometry	O
e	O
g	O
amari	O
it	O
can	O
be	O
shown	O
that	O
the	O
manifold	O
of	O
log	O
px	O
is	O
riemannian	O
with	O
a	O
metric	O
tensor	O
which	O
is	O
the	O
inverse	O
of	O
the	O
fisher	B
information	I
matrix	I
f	O
where	O
f	O
ex	O
setting	O
m	O
f	O
in	O
eq	O
gives	O
the	O
fisher	B
kernel	B
if	O
f	O
is	O
difficult	O
to	O
compute	O
then	O
one	O
might	O
resort	O
to	O
setting	O
m	O
i	O
the	O
advantage	O
of	O
using	O
the	O
fisher	B
information	I
matrix	I
is	O
that	O
it	O
makes	O
arc	O
length	O
on	O
the	O
manifold	O
invariant	O
to	O
reparameterizations	O
of	O
the	O
fisher	B
kernel	B
uses	O
a	O
class-independent	O
model	O
px	O
tsuda	O
et	O
al	O
have	O
developed	O
the	O
tangent	B
of	I
posterior	I
odds	I
kernel	B
based	O
on	O
py	O
log	O
py	O
which	O
makes	O
use	O
of	O
class-conditional	O
distributions	O
for	O
the	O
c	O
and	O
c	O
classes	O
exercises	O
the	O
ou	O
process	O
with	O
covariance	B
function	B
kx	O
exp	O
is	O
the	O
unique	O
stationary	O
first-order	O
markovian	O
gaussian	B
process	I
appendix	O
b	O
for	O
further	O
details	O
consider	O
training	O
inputs	O
xn	O
xn	O
on	O
r	O
with	O
corresponding	O
function	B
values	O
f	O
fxn	O
let	O
xl	O
denote	O
the	O
nearest	O
training	O
input	O
to	O
the	O
left	O
of	O
a	O
test	O
point	O
x	O
and	O
similarly	O
let	O
xu	O
denote	O
the	O
nearest	O
training	O
input	O
to	O
the	O
right	O
of	O
x	O
then	O
the	O
markovian	O
property	O
means	O
that	O
pfx	O
pfx	O
fxu	O
demonstrate	O
this	O
by	O
choosing	O
some	O
x-points	O
on	O
the	O
line	O
and	O
computing	O
the	O
predictive	B
distribution	O
pfx	O
using	O
eq	O
and	O
observing	O
that	O
non-zero	O
contributions	O
only	O
arise	O
from	O
xl	O
and	O
xu	O
note	O
that	O
this	O
only	O
occurs	O
in	O
the	O
noise-free	O
case	O
if	O
one	O
allows	O
the	O
training	O
points	O
to	O
be	O
corrupted	O
by	O
noise	O
and	O
then	O
all	O
points	O
will	O
contribute	O
in	O
general	O
computer	O
exercise	O
write	O
code	O
to	O
draw	O
samples	O
from	O
the	O
neural	B
network	I
covariance	B
function	B
eq	O
in	O
and	O
consider	O
the	O
cases	O
when	O
is	O
either	O
or	O
non-zero	O
explain	O
the	O
form	O
of	O
the	O
plots	O
obtained	O
when	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
exercises	O
consider	O
the	O
random	O
process	O
fx	O
where	O
u	O
n	O
show	O
that	O
this	O
non-linear	O
transform	O
of	O
a	O
process	O
with	O
an	O
inhomogeneous	O
linear	B
covariance	B
function	B
has	O
the	O
same	O
covariance	B
function	B
as	O
the	O
erf	O
neural	B
network	I
however	O
note	O
that	O
this	O
process	O
is	O
not	O
a	O
gaussian	B
process	I
draw	O
samples	O
from	O
the	O
given	O
process	O
and	O
compare	O
them	O
to	O
your	O
results	O
from	O
exercise	O
derive	O
gibbs	B
non-stationary	O
covariance	B
function	B
eq	O
computer	O
exercise	O
write	O
code	O
to	O
draw	O
samples	O
from	O
gibbs	B
non-stationary	O
covariance	B
function	B
eq	O
in	O
and	O
investigate	O
various	O
forms	O
of	O
length-scale	B
function	B
show	O
that	O
the	O
se	O
process	O
is	O
infinitely	O
ms	O
differentiable	O
and	O
that	O
the	O
ou	O
process	O
is	O
not	O
ms	O
differentiable	O
prove	O
that	O
the	O
eigenfunctions	O
of	O
a	O
symmetric	O
kernel	B
are	O
orthogonal	O
w	O
r	O
t	O
the	O
measure	B
let	O
kx	O
and	O
assume	O
px	O
for	O
all	O
x	O
has	O
the	O
same	O
show	O
that	O
the	O
eigenproblem	O
r	O
kx	O
ixdx	O
i	O
eigenvalues	O
asr	O
kx	O
ixdx	O
i	O
and	O
that	O
the	O
eigenfunc	O
tions	O
are	O
related	O
by	O
ix	O
ix	O
also	O
give	O
the	O
matrix	O
version	O
of	O
this	O
problem	O
introduce	O
a	O
diagonal	O
matrix	O
p	O
to	O
take	O
the	O
r	O
ole	O
of	O
px	O
the	O
significance	O
of	O
this	O
connection	O
is	O
that	O
it	O
can	O
be	O
easier	O
to	O
find	O
eigenvalues	O
of	O
symmetric	O
matrices	O
than	O
general	O
matrices	O
apply	O
the	O
construction	O
in	O
the	O
previous	O
exercise	O
to	O
the	O
eigenproblem	O
for	O
the	O
se	O
kernel	B
and	O
gaussian	O
density	O
given	O
in	O
section	O
with	O
px	O
exp	O
exp	O
bx	O
exp	O
using	O
equation	O
in	O
gradshteyn	O
and	O
ryzhik	O
exp	O
thus	O
consider	O
the	O
modified	O
kernel	B
given	O
by	O
kx	O
z	O
x	O
dx	O
y	O
verify	O
that	O
kx	O
exp	O
and	O
and	O
thus	O
confirm	O
equations	O
computer	O
exercise	O
the	O
analytic	O
form	O
of	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
for	O
the	O
se	O
kernel	B
and	O
gaussian	O
density	O
are	O
given	O
in	O
section	O
compare	O
these	O
exact	O
results	O
to	O
those	O
obtained	O
by	O
the	O
nystr	O
om	O
approximation	O
for	O
various	O
values	O
of	O
n	O
and	O
choice	O
of	O
samples	O
let	O
x	O
n	O
consider	O
the	O
fisher	B
kernel	B
derived	O
from	O
this	O
model	O
with	O
respect	O
to	O
variation	O
of	O
regard	O
as	O
a	O
constant	O
show	O
that	O
log	O
px	O
x	O
and	O
that	O
f	O
thus	O
the	O
fisher	B
kernel	B
for	O
this	O
model	O
with	O
is	O
the	O
linear	B
kernel	B
kx	O
x	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
covariance	B
functions	O
consider	O
a	O
k	O
order	O
markov	O
model	O
for	O
strings	O
on	O
a	O
finite	O
alphabet	O
let	O
this	O
model	O
have	O
parameters	O
denoting	O
the	O
probability	O
pxi	O
txi	O
xk	O
sk	O
of	O
course	O
as	O
these	O
are	O
probabilities	O
they	O
enforcing	O
this	O
constraint	O
obey	O
the	O
constraint	O
that	O
p	O
can	O
be	O
achieved	O
automatically	O
by	O
setting	O
p	O
where	O
the	O
parameters	O
are	O
now	O
independent	O
as	O
suggested	O
in	O
et	O
al	O
the	O
current	O
parameter	O
values	O
are	O
denoted	O
let	O
the	O
current	O
values	O
of	O
i	O
e	O
that	O
show	O
that	O
log	O
px	O
log	O
where	O
is	O
be	O
set	B
so	O
that	O
p	O
the	O
number	O
of	O
instances	O
of	O
the	O
substring	O
sk	O
in	O
x	O
thus	O
following	O
leslie	O
et	O
al	O
show	O
that	O
log	O
px	O
where	O
is	O
the	O
number	O
of	O
instances	O
of	O
the	O
substring	O
sk	O
in	O
x	O
as	O
is	O
the	O
expected	O
number	O
of	O
occurrences	O
of	O
the	O
string	B
sk	O
given	O
the	O
count	O
the	O
fisher	B
score	O
captures	O
the	O
degree	O
to	O
which	O
this	O
string	B
is	O
over-	O
or	O
under-represented	O
relative	O
to	O
the	O
model	O
for	O
the	O
k-spectrum	B
kernel	B
the	O
relevant	O
feature	O
is	O
sk	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
in	O
chapters	O
and	O
we	O
have	O
seen	O
how	O
to	O
do	O
regression	O
and	O
classification	B
using	O
a	O
gaussian	B
process	I
with	O
a	O
given	O
fixed	O
covariance	B
function	B
however	O
in	O
many	O
practical	O
applications	O
it	O
may	O
not	O
be	O
easy	O
to	O
specify	O
all	O
aspects	O
of	O
the	O
covariance	B
function	B
with	O
confidence	O
while	O
some	O
properties	O
such	O
as	O
stationarity	B
of	O
the	O
covariance	B
function	B
may	O
be	O
easy	O
to	O
determine	O
from	O
the	O
context	O
we	O
typically	O
have	O
only	O
rather	O
vague	O
information	O
about	O
other	O
properties	O
such	O
as	O
the	O
value	O
of	O
free	O
parameters	O
e	O
g	O
length-scales	O
in	O
chapter	O
several	O
examples	O
of	O
covariance	B
functions	O
were	O
presented	O
many	O
of	O
which	O
have	O
large	O
numbers	O
of	O
parameters	O
in	O
addition	O
the	O
exact	O
form	O
and	O
possible	O
free	O
parameters	O
of	O
the	O
likelihood	B
function	B
may	O
also	O
not	O
be	O
known	O
in	O
advance	O
thus	O
in	O
order	O
to	O
turn	O
gaussian	O
processes	O
into	O
powerful	O
practical	O
tools	O
it	O
is	O
essential	O
to	O
develop	O
methods	O
that	O
address	O
the	O
model	O
selection	O
problem	O
we	O
interpret	O
the	O
model	O
selection	O
problem	O
rather	O
broadly	O
to	O
include	O
all	O
aspects	O
of	O
the	O
model	O
including	O
the	O
discrete	O
choice	O
of	O
the	O
functional	B
form	O
for	O
the	O
covariance	B
function	B
as	O
well	O
as	O
values	O
for	O
any	O
hyperparameters	B
in	O
section	O
we	O
outline	O
the	O
model	O
selection	O
problem	O
in	O
the	O
following	O
sections	O
different	O
methodologies	O
are	O
presented	O
in	O
section	O
bayesian	O
principles	O
are	O
covered	O
and	O
in	O
section	O
cross-validation	B
is	O
discussed	O
in	O
particular	O
the	O
leave-one-out	B
estimator	O
in	O
the	O
remaining	O
two	O
sections	O
the	O
different	O
methodologies	O
are	O
applied	O
specifically	O
to	O
learning	B
in	O
gp	O
models	O
for	O
regression	O
in	O
section	O
and	O
classification	B
in	O
section	O
model	O
selection	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
the	O
model	O
selection	O
problem	O
in	O
order	O
for	O
a	O
model	O
to	O
be	O
a	O
practical	O
tool	O
in	O
an	O
application	O
one	O
needs	O
to	O
make	O
decisions	O
about	O
the	O
details	O
of	O
its	O
specification	O
some	O
properties	O
may	O
be	O
easy	O
to	O
specify	O
while	O
we	O
typically	O
have	O
only	O
vague	O
information	O
available	O
about	O
other	O
aspects	O
we	O
use	O
the	O
term	O
model	O
selection	O
to	O
cover	O
both	O
discrete	O
choices	O
and	O
the	O
setting	O
of	O
continuous	O
parameters	O
of	O
the	O
covariance	B
functions	O
in	O
fact	O
model	O
selection	O
can	O
help	O
both	O
to	O
refine	O
the	O
predictions	O
of	O
the	O
model	O
and	O
give	O
a	O
valuable	O
interpretation	O
to	O
the	O
user	O
about	O
the	O
properties	O
of	O
the	O
data	O
e	O
g	O
that	O
a	O
non-stationary	O
covariance	B
function	B
may	O
be	O
preferred	O
over	O
a	O
stationary	O
one	O
a	O
multitude	O
of	O
possible	O
families	O
of	O
covariance	B
functions	O
exists	O
including	O
squared	B
exponential	B
polynomial	B
neural	B
network	I
etc	O
see	O
section	O
for	O
an	O
overview	O
each	O
of	O
these	O
families	O
typically	O
have	O
a	O
number	O
of	O
free	O
hyperparameters	B
whose	O
values	O
also	O
need	O
to	O
be	O
determined	O
choosing	O
a	O
covariance	B
function	B
for	O
a	O
particular	O
application	O
thus	O
comprises	O
both	O
setting	O
of	O
hyperparameters	B
within	O
a	O
family	O
and	O
comparing	O
across	O
different	O
families	O
both	O
of	O
these	O
problems	O
will	O
be	O
treated	O
by	O
the	O
same	O
methods	O
so	O
there	O
is	O
no	O
need	O
to	O
distinguish	O
between	O
them	O
and	O
we	O
will	O
use	O
the	O
term	O
model	O
selection	O
to	O
cover	O
both	O
meanings	O
we	O
will	O
refer	O
to	O
the	O
selection	O
of	O
a	O
covariance	B
function	B
and	O
its	O
parameters	O
as	O
training	O
of	O
a	O
gaussian	O
in	O
the	O
following	O
paragraphs	O
we	O
give	O
example	O
choices	O
of	O
parameterizations	O
of	O
distance	O
measures	O
for	O
stationary	O
covariance	B
functions	O
enable	O
interpretation	O
hyperparameters	B
training	O
covariance	B
functions	O
such	O
as	O
the	O
squared	B
exponential	B
can	O
be	O
parameterized	O
in	O
terms	O
of	O
hyperparameters	B
for	O
example	O
kxp	O
xq	O
where	O
n	O
is	O
a	O
vector	O
containing	O
all	O
the	O
and	O
denotes	O
the	O
parameters	O
in	O
the	O
symmetric	O
matrix	O
m	O
possible	O
choices	O
for	O
the	O
matrix	O
m	O
include	O
f	O
n	O
pq	O
f	O
xqmxp	O
characteristic	O
length-scale	B
automatic	O
relevance	O
determination	B
diag	O
diag	O
where	O
is	O
a	O
vector	O
of	O
positive	O
values	O
and	O
is	O
a	O
d	O
k	O
matrix	O
k	O
d	O
the	O
properties	O
of	O
functions	O
with	O
these	O
covariance	B
functions	O
depend	O
on	O
the	O
values	O
of	O
the	O
hyperparameters	B
for	O
many	O
covariance	B
functions	O
it	O
is	O
easy	O
to	O
interpret	O
the	O
meaning	O
of	O
the	O
hyperparameters	B
which	O
is	O
of	O
great	O
importance	O
when	O
trying	O
to	O
understand	O
your	O
data	O
for	O
the	O
squared	B
exponential	B
covariance	B
function	B
eq	O
with	O
distance	O
measure	B
from	O
eq	O
the	O
d	O
hyperparameters	B
play	O
the	O
r	O
ole	O
of	O
characteristic	O
length-scales	O
loosely	O
speaking	O
how	O
far	O
do	O
you	O
need	O
to	O
move	O
a	O
particular	O
axis	O
in	O
input	O
space	O
for	O
the	O
function	B
values	O
to	O
become	O
uncorrelated	O
such	O
a	O
covariance	B
function	B
implements	O
automatic	O
relevance	O
determination	B
since	O
the	O
inverse	O
of	O
the	O
length-scale	B
determines	O
how	O
relevant	O
an	O
input	O
is	O
if	O
the	O
length-scale	B
has	O
a	O
very	O
large	O
value	O
the	O
contrasts	O
the	O
use	O
of	O
the	O
word	O
in	O
the	O
svm	O
literature	O
where	O
training	O
usually	O
refers	O
to	O
finding	O
the	O
support	O
vectors	O
for	O
a	O
fixed	O
kernel	B
the	O
noise	O
level	O
parameter	O
n	O
is	O
not	O
considered	O
a	O
hyperparameter	O
however	O
it	O
plays	O
an	O
analogous	O
role	O
and	O
is	O
treated	O
in	O
the	O
same	O
way	O
so	O
we	O
simply	O
consider	O
it	O
a	O
hyperparameter	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
model	O
selection	O
problem	O
figure	O
functions	O
with	O
two	O
dimensional	O
input	O
drawn	O
at	O
random	O
from	O
noise	O
free	O
squared	B
exponential	B
covariance	B
function	B
gaussian	O
processes	O
corresponding	O
to	O
the	O
three	O
different	O
distance	O
measures	O
in	O
eq	O
respectively	O
the	O
parameters	O
were	O
and	O
in	O
panel	O
the	O
two	O
inputs	O
are	O
equally	O
important	O
while	O
in	O
the	O
function	B
varies	O
less	O
rapidly	O
as	O
a	O
function	B
of	O
than	O
in	O
the	O
column	O
gives	O
the	O
direction	O
of	O
most	O
rapid	O
variation	O
covariance	B
will	O
become	O
almost	O
independent	O
of	O
that	O
input	O
effectively	O
removing	O
it	O
from	O
the	O
inference	O
ard	O
has	O
been	O
used	O
successfully	O
for	O
removing	O
irrelevant	O
input	O
by	O
several	O
authors	O
e	O
g	O
williams	O
and	O
rasmussen	O
we	O
call	O
the	O
parameterization	O
of	O
in	O
eq	O
the	O
factor	B
analysis	I
distance	O
due	O
to	O
the	O
analogy	O
with	O
the	O
factor	B
analysis	I
model	O
which	O
seeks	O
to	O
explain	O
the	O
data	O
through	O
a	O
low	O
rank	O
plus	O
diagonal	O
decomposition	O
for	O
high	O
dimensional	O
datasets	O
the	O
k	O
columns	O
of	O
the	O
matrix	O
could	O
identify	O
a	O
few	O
directions	O
in	O
the	O
input	O
space	O
with	O
specially	O
high	O
relevance	O
and	O
their	O
lengths	O
give	O
the	O
inverse	O
characteristic	O
length-scale	B
for	O
those	O
directions	O
in	O
figure	O
we	O
show	O
functions	O
drawn	O
at	O
random	O
from	O
squared	B
exponential	B
covariance	B
function	B
gaussian	O
processes	O
for	O
different	O
choices	O
of	O
m	O
in	O
panel	O
we	O
get	O
an	O
isotropic	O
behaviour	O
in	O
panel	O
the	O
characteristic	O
length-scale	B
is	O
different	O
along	O
the	O
two	O
input	O
axes	O
the	O
function	B
varies	O
rapidly	O
as	O
a	O
function	B
of	O
but	O
less	O
rapidly	O
as	O
a	O
function	B
of	O
in	O
panel	O
the	O
direction	O
of	O
most	O
rapid	O
variation	O
is	O
perpendicular	O
to	O
the	O
direction	O
as	O
this	O
figure	O
illustrates	O
factor	B
analysis	I
distance	O
y	O
y	O
y	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
there	O
is	O
plenty	O
of	O
scope	O
for	O
variation	O
even	O
inside	O
a	O
single	O
family	O
of	O
covariance	B
functions	O
our	O
task	O
is	O
based	O
on	O
a	O
set	B
of	O
training	O
data	O
to	O
make	O
inferences	O
about	O
the	O
form	O
and	O
parameters	O
of	O
the	O
covariance	B
function	B
or	O
equivalently	O
about	O
the	O
relationships	O
in	O
the	O
data	O
it	O
should	O
be	O
clear	O
from	O
the	O
above	O
example	O
that	O
model	O
selection	O
is	O
essentially	O
open	O
ended	O
even	O
for	O
the	O
squared	B
exponential	B
covariance	B
function	B
there	O
is	O
a	O
huge	O
variety	O
of	O
possible	O
distance	O
measures	O
however	O
this	O
should	O
not	O
be	O
a	O
cause	O
for	O
despair	O
rather	O
seen	O
as	O
a	O
possibility	O
to	O
learn	O
it	O
requires	O
however	O
a	O
systematic	O
and	O
practical	O
approach	O
to	O
model	O
selection	O
in	O
a	O
nutshell	O
we	O
need	O
to	O
be	O
able	O
to	O
compare	O
two	O
more	O
methods	O
differing	O
in	O
values	O
of	O
particular	O
parameters	O
or	O
the	O
shape	O
of	O
the	O
covariance	B
function	B
or	O
compare	O
a	O
gaussian	B
process	I
model	O
to	O
any	O
other	O
kind	O
of	O
model	O
although	O
there	O
are	O
endless	O
variations	O
in	O
the	O
suggestions	O
for	O
model	O
selection	O
in	O
the	O
literature	O
three	O
general	O
principles	O
cover	O
most	O
compute	O
the	O
probability	O
of	O
the	O
model	O
given	O
the	O
data	O
estimate	O
the	O
generalization	B
error	I
and	O
bound	O
the	O
generalization	B
error	I
we	O
use	O
the	O
term	O
generalization	B
error	I
to	O
mean	O
the	O
average	O
error	O
on	O
unseen	O
test	O
examples	O
the	O
same	O
distribution	O
as	O
the	O
training	O
cases	O
note	O
that	O
the	O
training	O
error	O
is	O
usually	O
a	O
poor	O
proxy	O
for	O
the	O
generalization	B
error	I
since	O
the	O
model	O
may	O
fit	O
the	O
noise	O
in	O
the	O
training	O
set	B
leading	O
to	O
low	O
training	O
error	O
but	O
poor	O
generalization	B
performance	O
in	O
the	O
next	O
section	O
we	O
describe	O
the	O
bayesian	O
view	O
on	O
model	O
selection	O
which	O
involves	O
the	O
computation	O
of	O
the	O
probability	O
of	O
the	O
model	O
given	O
the	O
data	O
based	O
on	O
the	O
marginal	B
likelihood	B
in	O
section	O
we	O
cover	O
cross-validation	B
which	O
estimates	O
the	O
generalization	B
performance	O
these	O
two	O
paradigms	O
are	O
applied	O
to	O
gaussian	B
process	I
models	O
in	O
the	O
remainder	O
of	O
this	O
chapter	O
the	O
probably	O
approximately	O
correct	O
framework	O
is	O
an	O
example	O
of	O
a	O
bound	O
on	O
the	O
generalization	B
error	I
and	O
is	O
covered	O
in	O
section	O
bayesian	O
model	O
selection	O
in	O
this	O
section	O
we	O
give	O
a	O
short	O
outline	O
description	O
of	O
the	O
main	O
ideas	O
in	O
bayesian	O
model	O
selection	O
the	O
discussion	O
will	O
be	O
general	O
but	O
focusses	O
on	O
issues	O
which	O
will	O
be	O
relevant	O
for	O
the	O
specific	O
treatment	O
of	O
gaussian	B
process	I
models	O
for	O
regression	O
in	O
section	O
and	O
classification	B
in	O
section	O
it	O
is	O
common	O
to	O
use	O
a	O
hierarchical	O
specification	O
of	O
models	O
at	O
the	O
lowest	O
level	O
are	O
the	O
parameters	O
w	O
for	O
example	O
the	O
parameters	O
could	O
be	O
the	O
parameters	O
in	O
a	O
linear	B
model	O
or	O
the	O
weights	O
in	O
a	O
neural	B
network	I
model	O
at	O
the	O
second	O
level	O
are	O
hyperparameters	B
which	O
control	O
the	O
distribution	O
of	O
the	O
parameters	O
at	O
the	O
bottom	O
level	O
for	O
example	O
the	O
weight	O
decay	O
term	O
in	O
a	O
neural	B
network	I
or	O
the	O
ridge	B
term	O
in	O
ridge	B
regression	O
are	O
hyperparameters	B
at	O
the	O
top	O
level	O
we	O
may	O
have	O
a	O
set	B
of	O
possible	O
model	O
structures	O
hi	O
under	O
consideration	O
we	O
will	O
first	O
give	O
a	O
mechanistic	O
description	O
of	O
the	O
computations	O
needed	O
for	O
bayesian	O
inference	O
and	O
continue	O
with	O
a	O
discussion	O
providing	O
the	O
intuition	O
about	O
what	O
is	O
going	O
on	O
inference	O
takes	O
place	O
one	O
level	O
at	O
a	O
time	O
by	O
applying	O
hierarchical	O
models	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bayesian	O
model	O
selection	O
the	O
rules	O
of	O
probability	O
theory	O
see	O
e	O
g	O
mackay	O
for	O
this	O
framework	O
and	O
mackay	O
for	O
the	O
context	O
of	O
neural	O
networks	O
at	O
the	O
bottom	O
level	O
the	O
posterior	O
over	O
the	O
parameters	O
is	O
given	O
by	O
bayes	O
rule	O
pwy	O
x	O
pyx	O
whipw	O
pyx	O
level	O
inference	O
where	O
pyx	O
whi	O
is	O
the	O
likelihood	B
and	O
pw	O
is	O
the	O
parameter	O
prior	O
the	O
prior	O
encodes	O
as	O
a	O
probability	O
distribution	O
our	O
knowledge	O
about	O
the	O
parameters	O
prior	O
to	O
seeing	O
the	O
data	O
if	O
we	O
have	O
only	O
vague	O
prior	O
information	O
about	O
the	O
parameters	O
then	O
the	O
prior	O
distribution	O
is	O
chosen	O
to	O
be	O
broad	O
to	O
reflect	O
this	O
the	O
posterior	O
combines	O
the	O
information	O
from	O
the	O
prior	O
and	O
the	O
data	O
the	O
likelihood	B
the	O
normalizing	O
constant	O
in	O
the	O
denominator	O
of	O
eq	O
pyx	O
is	O
independent	O
of	O
the	O
parameters	O
and	O
called	O
the	O
marginal	B
likelihood	B
evidence	O
and	O
is	O
given	O
by	O
pyx	O
pyx	O
whipw	O
dw	O
z	O
z	O
at	O
the	O
next	O
level	O
we	O
analogously	O
express	O
the	O
posterior	O
over	O
the	O
hyperparameters	B
where	O
the	O
marginal	B
likelihood	B
from	O
the	O
first	O
level	O
plays	O
the	O
r	O
ole	O
of	O
the	O
likelihood	B
p	O
xhi	O
pyx	O
where	O
p	O
is	O
the	O
hyper-prior	O
prior	O
for	O
the	O
hyperparameters	B
the	O
normalizing	O
constant	O
is	O
given	O
by	O
pyxhi	O
pyxhi	O
pyx	O
level	O
inference	O
at	O
the	O
top	O
level	O
we	O
compute	O
the	O
posterior	O
for	O
the	O
model	O
level	O
inference	O
pyx	O
where	O
pyx	O
p	O
phiy	O
x	O
pyxhiphi	O
i	O
pyxhiphi	O
we	O
note	O
that	O
the	O
implementation	O
of	O
bayesian	O
inference	O
calls	O
for	O
the	O
evaluation	O
of	O
several	O
integrals	B
depending	O
on	O
the	O
details	O
of	O
the	O
models	O
these	O
integrals	B
may	O
or	O
may	O
not	O
be	O
analytically	O
tractable	O
and	O
in	O
general	O
one	O
may	O
have	O
to	O
resort	O
to	O
analytical	O
approximations	O
or	O
markov	B
chain	I
monte	I
carlo	I
methods	O
in	O
practice	O
especially	O
the	O
evaluation	O
of	O
the	O
integral	O
in	O
eq	O
may	O
be	O
difficult	O
and	O
as	O
an	O
approximation	O
one	O
may	O
shy	O
away	O
from	O
using	O
the	O
hyperparameter	O
posterior	O
in	O
eq	O
and	O
instead	O
maximize	O
the	O
marginal	B
likelihood	B
in	O
eq	O
w	O
r	O
t	O
the	O
hyperparameters	B
this	O
approximation	O
is	O
known	O
as	O
type	B
ii	I
maximum	I
likelihood	B
of	O
course	O
one	O
should	O
be	O
careful	O
with	O
such	O
an	O
optimization	O
step	O
since	O
it	O
opens	O
up	O
the	O
possibility	O
of	O
overfitting	O
especially	O
if	O
there	O
are	O
many	O
hyperparameters	B
the	O
integral	O
in	O
eq	O
can	O
then	O
be	O
approximated	O
using	O
a	O
local	O
expansion	O
around	O
the	O
maximum	O
laplace	B
approximation	I
this	O
approximation	O
will	O
be	O
good	O
if	O
the	O
posterior	O
for	O
is	O
fairly	O
well	O
peaked	O
which	O
is	O
more	O
often	O
the	O
case	O
for	O
the	O
ml-ii	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
figure	O
the	O
marginal	B
likelihood	B
pyxhi	O
is	O
the	O
probability	O
of	O
the	O
data	O
given	O
the	O
model	O
the	O
number	O
of	O
data	O
points	O
n	O
and	O
the	O
inputs	O
x	O
are	O
fixed	O
and	O
not	O
shown	O
the	O
horizontal	O
axis	O
is	O
an	O
idealized	O
representation	O
of	O
all	O
possible	O
vectors	O
of	O
targets	O
y	O
the	O
marginal	B
likelihood	B
for	O
models	O
of	O
three	O
different	O
complexities	O
are	O
shown	O
note	O
that	O
since	O
the	O
marginal	B
likelihood	B
is	O
a	O
probability	O
distribution	O
it	O
must	O
normalize	O
to	O
unity	O
for	O
a	O
particular	O
dataset	O
indicated	O
by	O
y	O
and	O
a	O
dotted	O
line	O
the	O
marginal	B
likelihood	B
prefers	O
a	O
model	O
of	O
intermediate	O
complexity	O
over	O
too	O
simple	O
or	O
too	O
complex	O
alternatives	O
hyperparameters	B
than	O
for	O
the	O
parameters	O
themselves	O
see	O
mackay	O
for	O
an	O
illuminating	O
discussion	O
the	O
prior	O
over	O
models	O
hi	O
in	O
eq	O
is	O
often	O
taken	O
to	O
be	O
flat	O
so	O
that	O
a	O
priori	O
we	O
do	O
not	O
favour	O
one	O
model	O
over	O
another	O
in	O
this	O
case	O
the	O
probability	O
for	O
the	O
model	O
is	O
proportional	O
to	O
the	O
expression	O
from	O
eq	O
it	O
is	O
primarily	O
the	O
marginal	B
likelihood	B
from	O
eq	O
involving	O
the	O
integral	O
over	O
the	O
parameter	O
space	O
which	O
distinguishes	O
the	O
bayesian	O
scheme	O
of	O
inference	O
from	O
other	O
schemes	O
based	O
on	O
optimization	O
it	O
is	O
a	O
property	O
of	O
the	O
marginal	B
likelihood	B
that	O
it	O
automatically	O
incorporates	O
a	O
trade-off	O
between	O
model	O
fit	O
and	O
model	O
complexity	O
this	O
is	O
the	O
reason	O
why	O
the	O
marginal	B
likelihood	B
is	O
valuable	O
in	O
solving	O
the	O
model	O
selection	O
problem	O
in	O
figure	O
we	O
show	O
a	O
schematic	O
of	O
the	O
behaviour	O
of	O
the	O
marginal	B
likelihood	B
for	O
three	O
different	O
model	O
complexities	O
let	O
the	O
number	O
of	O
data	O
points	O
n	O
and	O
the	O
inputs	O
x	O
be	O
fixed	O
the	O
horizontal	O
axis	O
is	O
an	O
idealized	O
representation	O
of	O
all	O
possible	O
vectors	O
of	O
targets	O
y	O
and	O
the	O
vertical	O
axis	O
plots	O
the	O
marginal	B
likelihood	B
pyxhi	O
a	O
simple	O
model	O
can	O
only	O
account	O
for	O
a	O
limited	O
range	O
of	O
possible	O
sets	O
of	O
target	O
values	O
but	O
since	O
the	O
marginal	B
likelihood	B
is	O
a	O
probability	O
distribution	O
over	O
y	O
it	O
must	O
normalize	O
to	O
unity	O
and	O
therefore	O
the	O
data	O
sets	O
which	O
the	O
model	O
does	O
account	O
for	O
have	O
a	O
large	O
value	O
of	O
the	O
marginal	B
likelihood	B
conversely	O
for	O
a	O
complex	O
model	O
it	O
is	O
capable	O
of	O
accounting	O
for	O
a	O
wider	O
range	O
of	O
data	O
sets	O
and	O
consequently	O
the	O
marginal	B
likelihood	B
doesn	O
t	O
attain	O
such	O
large	O
values	O
as	O
for	O
the	O
simple	O
model	O
for	O
example	O
the	O
simple	O
model	O
could	O
be	O
a	O
linear	B
model	O
and	O
the	O
complex	O
model	O
a	O
large	O
neural	B
network	I
the	O
figure	O
illustrates	O
why	O
the	O
marginal	B
likelihood	B
doesn	O
t	O
simply	O
favour	O
the	O
models	O
that	O
fit	O
the	O
training	O
data	O
the	O
best	O
this	O
effect	O
is	O
called	O
occam	O
s	O
razor	O
after	O
william	O
of	O
occam	O
whose	O
principle	O
plurality	O
should	O
not	O
be	O
assumed	O
without	O
necessity	O
he	O
used	O
to	O
encourage	O
simplicity	O
in	O
explanations	O
see	O
also	O
rasmussen	O
and	O
ghahramani	O
for	O
an	O
investigation	O
into	O
occam	O
s	O
razor	O
in	O
statistical	O
models	O
occam	O
s	O
razor	O
ymarginal	O
likelihood	B
pyxhiall	O
possible	O
data	O
setssimpleintermediatecomplex	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
cross-validation	B
notice	O
that	O
the	O
trade-off	O
between	O
data-fit	O
and	O
model	O
complexity	O
is	O
automatic	O
there	O
is	O
no	O
need	O
to	O
set	B
a	O
parameter	O
externally	O
to	O
fix	O
the	O
trade-off	O
do	O
not	O
confuse	O
the	O
automatic	O
occam	O
s	O
razor	O
principle	O
with	O
the	O
use	O
of	O
priors	O
in	O
the	O
bayesian	O
method	O
even	O
if	O
the	O
priors	O
are	O
flat	O
over	O
complexity	O
the	O
marginal	B
likelihood	B
will	O
still	O
tend	O
to	O
favour	O
the	O
least	O
complex	O
model	O
able	O
to	O
explain	O
the	O
data	O
thus	O
a	O
model	O
complexity	O
which	O
is	O
well	O
suited	O
to	O
the	O
data	O
can	O
be	O
selected	O
using	O
the	O
marginal	B
likelihood	B
in	O
the	O
preceding	O
paragraphs	O
we	O
have	O
thought	O
of	O
the	O
specification	O
of	O
a	O
model	O
as	O
the	O
model	O
structure	O
as	O
well	O
as	O
the	O
parameters	O
of	O
the	O
priors	O
etc	O
if	O
it	O
is	O
unclear	O
how	O
to	O
set	B
some	O
of	O
the	O
parameters	O
of	O
the	O
prior	O
one	O
can	O
treat	O
these	O
as	O
hyperparameters	B
and	O
do	O
model	O
selection	O
to	O
determine	O
how	O
to	O
set	B
them	O
at	O
the	O
same	O
time	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
priors	O
correspond	O
to	O
assumptions	O
about	O
the	O
data	O
if	O
the	O
priors	O
are	O
grossly	O
at	O
odds	O
with	O
the	O
distribution	O
of	O
the	O
data	O
inference	O
will	O
still	O
take	O
place	O
under	O
the	O
assumptions	O
encoded	O
by	O
the	O
prior	O
see	O
the	O
step-function	O
example	O
in	O
section	O
to	O
avoid	O
this	O
situation	O
one	O
should	O
be	O
careful	O
not	O
to	O
employ	O
priors	O
which	O
are	O
too	O
narrow	O
ruling	O
out	O
reasonable	O
explanations	O
of	O
the	O
cross-validation	B
in	O
this	O
section	O
we	O
consider	O
how	O
to	O
use	O
methods	O
of	O
cross-validation	B
for	O
model	O
selection	O
the	O
basic	O
idea	O
is	O
to	O
split	O
the	O
training	O
set	B
into	O
two	O
disjoint	O
sets	O
one	O
which	O
is	O
actually	O
used	O
for	O
training	O
and	O
the	O
other	O
the	O
validation	O
set	B
which	O
is	O
used	O
to	O
monitor	O
performance	O
the	O
performance	O
on	O
the	O
validation	O
set	B
is	O
used	O
as	O
a	O
proxy	O
for	O
the	O
generalization	B
error	I
and	O
model	O
selection	O
is	O
carried	O
out	O
using	O
this	O
measure	B
in	O
practice	O
a	O
drawback	O
of	O
hold-out	O
method	O
is	O
that	O
only	O
a	O
fraction	O
of	O
the	O
full	O
data	O
set	B
can	O
be	O
used	O
for	O
training	O
and	O
that	O
if	O
the	O
validation	O
set	B
it	O
small	O
the	O
performance	O
estimate	O
obtained	O
may	O
have	O
large	O
variance	O
to	O
minimize	O
these	O
problems	O
cv	O
is	O
almost	O
always	O
used	O
in	O
the	O
k-fold	O
cross-validation	B
setting	O
the	O
data	O
is	O
split	O
into	O
k	O
disjoint	O
equally	O
sized	O
subsets	O
validation	O
is	O
done	O
on	O
a	O
single	O
subset	O
and	O
training	O
is	O
done	O
using	O
the	O
union	O
of	O
the	O
remaining	O
k	O
subsets	O
the	O
entire	O
procedure	O
being	O
repeated	O
k	O
times	O
each	O
time	O
with	O
a	O
different	O
subset	O
for	O
validation	O
thus	O
a	O
large	O
fraction	O
of	O
the	O
data	O
can	O
be	O
used	O
for	O
training	O
and	O
all	O
cases	O
appear	O
as	O
validation	O
cases	O
the	O
price	O
is	O
that	O
k	O
models	O
must	O
be	O
trained	O
instead	O
of	O
one	O
typical	O
values	O
for	O
k	O
are	O
in	O
the	O
range	O
to	O
an	O
extreme	O
case	O
of	O
k-fold	O
cross-validation	B
is	O
obtained	O
for	O
k	O
n	O
the	O
number	O
of	O
training	O
cases	O
also	O
known	O
as	O
leave-one-out	B
cross-validation	B
often	O
the	O
computational	O
cost	O
of	O
loo-cv	O
training	O
n	O
models	O
is	O
prohibitive	O
but	O
in	O
certain	O
cases	O
such	O
as	O
gaussian	B
process	I
regression	I
there	O
are	O
computational	O
shortcuts	O
is	O
known	O
as	O
cromwell	O
s	O
dictum	O
after	O
oliver	O
cromwell	O
who	O
on	O
august	O
wrote	O
to	O
the	O
synod	O
of	O
the	O
church	O
of	O
scotland	O
i	O
beseech	O
you	O
in	O
the	O
bowels	O
of	O
christ	B
consider	O
it	O
possible	O
that	O
you	O
are	O
mistaken	O
automatic	O
trade-off	O
cross-validation	B
k-fold	O
cross-validation	B
leave-one-out	B
cross-validation	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
other	O
loss	B
functions	O
model	O
parameters	O
cross-validation	B
can	O
be	O
used	O
with	O
any	O
loss	B
function	B
although	O
the	O
squared	B
error	O
loss	B
is	O
by	O
far	O
the	O
most	O
common	O
for	O
regression	O
there	O
is	O
no	O
reason	O
not	O
to	O
allow	O
other	O
loss	B
functions	O
for	O
probabilistic	B
models	O
such	O
as	O
gaussian	O
processes	O
it	O
is	O
natural	O
to	O
consider	O
also	O
cross-validation	B
using	O
the	O
negative	B
log	I
probability	I
loss	B
craven	O
and	O
wahba	O
describe	O
a	O
variant	O
of	O
cross-validation	B
using	O
squared	B
error	O
known	O
as	O
generalized	B
cross-validation	B
which	O
gives	O
different	O
weightings	O
to	O
different	O
datapoints	O
so	O
as	O
to	O
achieve	O
certain	O
invariance	O
properites	O
see	O
wahba	O
sec	O
for	O
further	O
details	O
model	O
selection	O
for	O
gp	O
regression	O
we	O
apply	O
bayesian	O
inference	O
in	O
section	O
and	O
cross-validation	B
in	O
section	O
to	O
gaussian	B
process	I
regression	I
with	O
gaussian	O
noise	O
we	O
conclude	O
in	O
section	O
with	O
some	O
more	O
detailed	O
examples	O
of	O
how	O
one	O
can	O
use	O
the	O
model	O
selection	O
principles	O
to	O
tailor	O
covariance	B
functions	O
marginal	B
likelihood	B
bayesian	O
principles	O
provide	O
a	O
persuasive	O
and	O
consistent	O
framework	O
for	O
inference	O
unfortunately	O
for	O
most	O
interesting	O
models	O
for	O
machine	O
learning	B
the	O
required	O
computations	O
over	O
parameter	O
space	O
are	O
analytically	O
intractable	O
and	O
good	O
approximations	O
are	O
not	O
easily	O
derived	O
gaussian	B
process	I
regression	I
models	O
with	O
gaussian	O
noise	O
are	O
a	O
rare	O
exception	O
integrals	B
over	O
the	O
parameters	O
are	O
analytically	O
tractable	O
and	O
at	O
the	O
same	O
time	O
the	O
models	O
are	O
very	O
flexible	O
in	O
this	O
section	O
we	O
first	O
apply	O
the	O
general	O
bayesian	O
inference	O
principles	O
from	O
section	O
to	O
the	O
specific	O
gaussian	B
process	I
model	O
in	O
the	O
simplified	O
form	O
where	O
hyperparameters	B
are	O
optimized	O
over	O
we	O
derive	O
the	O
expressions	O
for	O
the	O
marginal	B
likelihood	B
and	O
interpret	O
these	O
since	O
a	O
gaussian	B
process	I
model	O
is	O
a	O
non-parametric	B
model	O
it	O
may	O
not	O
be	O
immediately	O
obvious	O
what	O
the	O
parameters	O
of	O
the	O
model	O
are	O
generally	O
one	O
may	O
regard	O
the	O
noise-free	O
latent	O
function	B
values	O
at	O
the	O
training	O
inputs	O
f	O
as	O
the	O
parameters	O
the	O
more	O
training	O
cases	O
there	O
are	O
the	O
more	O
parameters	O
using	O
the	O
weight-space	O
view	O
developed	O
in	O
section	O
one	O
may	O
equivalently	O
think	O
of	O
the	O
parameters	O
as	O
being	O
the	O
weights	O
of	O
the	O
linear	B
model	O
which	O
uses	O
the	O
basis-functions	O
which	O
can	O
be	O
chosen	O
as	O
the	O
eigenfunctions	O
of	O
the	O
covariance	B
function	B
of	O
course	O
we	O
have	O
seen	O
that	O
this	O
view	O
is	O
inconvenient	O
for	O
nondegenerate	B
covariance	B
functions	O
since	O
these	O
would	O
then	O
have	O
an	O
infinite	O
number	O
of	O
weights	O
we	O
proceed	O
by	O
applying	O
eq	O
and	O
eq	O
for	O
the	O
level	O
of	O
inference	O
which	O
we	O
find	O
that	O
we	O
have	O
already	O
done	O
back	O
in	O
chapter	O
the	O
predictive	B
distribution	O
from	O
eq	O
is	O
given	O
for	O
the	O
weight-space	O
view	O
in	O
eq	O
and	O
eq	O
and	O
equivalently	O
for	O
the	O
function-space	O
view	O
in	O
eq	O
the	O
marginal	B
likelihood	B
evidence	O
from	O
eq	O
was	O
computed	O
in	O
eq	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
regression	O
figure	O
panel	O
shows	O
a	O
decomposition	O
of	O
the	O
log	O
marginal	B
likelihood	B
into	O
its	O
constituents	O
data-fit	O
and	O
complexity	O
penalty	O
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
the	O
training	O
data	O
is	O
drawn	O
from	O
a	O
gaussian	B
process	I
with	O
se	O
covariance	B
function	B
and	O
parameters	O
f	O
n	O
the	O
same	O
as	O
in	O
figure	O
and	O
we	O
are	O
fitting	O
only	O
the	O
length-scale	B
parameter	O
two	O
other	O
parameters	O
have	O
been	O
set	B
in	O
accordance	O
with	O
the	O
generating	O
process	O
panel	O
shows	O
the	O
log	O
marginal	B
likelihood	B
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
for	O
different	O
sizes	O
of	O
training	O
sets	O
also	O
shown	O
are	O
the	O
confidence	O
intervals	O
for	O
the	O
posterior	O
length-scales	O
and	O
we	O
re-state	O
the	O
result	O
here	O
log	O
pyx	O
yk	O
y	O
y	O
log	O
n	O
log	O
where	O
ky	O
kf	O
ni	O
is	O
the	O
covariance	B
matrix	I
for	O
the	O
noisy	O
targets	O
y	O
kf	O
is	O
the	O
covariance	B
matrix	I
for	O
the	O
noise-free	O
latent	O
f	O
and	O
we	O
now	O
explicitly	O
write	O
the	O
marginal	B
likelihood	B
conditioned	O
on	O
the	O
hyperparameters	B
parameters	O
of	O
the	O
covariance	B
function	B
from	O
this	O
perspective	O
it	O
becomes	O
clear	O
why	O
we	O
call	O
eq	O
the	O
log	O
marginal	B
likelihood	B
since	O
it	O
is	O
obtained	O
through	O
marginalization	O
over	O
the	O
latent	O
function	B
otherwise	O
if	O
one	O
thinks	O
entirely	O
in	O
terms	O
of	O
the	O
function-space	O
view	O
the	O
term	O
marginal	B
may	O
appear	O
a	O
bit	O
mysterious	O
and	O
similarly	O
the	O
hyper	O
from	O
the	O
parameters	O
of	O
the	O
covariance	B
the	O
three	O
terms	O
of	O
the	O
marginal	B
likelihood	B
in	O
eq	O
have	O
readily	O
interpretable	O
r	O
oles	O
the	O
only	O
term	O
involving	O
the	O
observed	O
targets	O
is	O
the	O
data-fit	O
yk	O
y	O
log	O
is	O
the	O
complexity	O
penalty	O
depending	O
only	O
on	O
the	O
covariance	B
function	B
and	O
the	O
inputs	O
and	O
n	O
is	O
a	O
normalization	O
constant	O
in	O
figure	O
we	O
illustrate	O
this	O
breakdown	O
of	O
the	O
log	O
marginal	B
likelihood	B
the	O
data-fit	O
decreases	O
monotonically	O
with	O
the	O
length-scale	B
since	O
the	O
model	O
becomes	O
less	O
and	O
less	O
flexible	O
the	O
negative	O
complexity	O
penalty	O
increases	O
with	O
the	O
length-scale	B
because	O
the	O
model	O
gets	O
less	O
complex	O
with	O
growing	O
length-scale	B
the	O
marginal	B
likelihood	B
itself	O
peaks	O
at	O
a	O
value	O
close	O
to	O
for	O
length-scales	O
somewhat	O
longer	O
than	O
the	O
marginal	B
likelihood	B
decreases	O
rapidly	O
the	O
reason	O
that	O
we	O
like	O
to	O
stick	O
to	O
the	O
term	O
marginal	B
likelihood	B
is	O
that	O
it	O
is	O
the	O
likelihood	B
of	O
a	O
non-parametric	B
model	O
i	O
e	O
a	O
model	O
which	O
requires	O
access	O
to	O
all	O
the	O
training	O
data	O
when	O
making	O
predictions	O
this	O
contrasts	O
the	O
situation	O
for	O
a	O
parametric	B
model	O
which	O
absorbs	O
the	O
information	O
from	O
the	O
training	O
data	O
into	O
its	O
parameter	O
this	O
difference	O
makes	O
the	O
two	O
likelihoods	O
behave	O
quite	O
differently	O
as	O
a	O
function	B
of	O
marginal	B
likelihood	B
interpretation	O
probabilitycharacteristic	O
lengthscaleminus	O
complexity	O
penaltydata	O
fitmarginal	O
lengthscalelog	O
marginal	B
conf	O
int	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
figure	O
contour	O
plot	O
showing	O
the	O
log	O
marginal	B
likelihood	B
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
and	O
the	O
noise	O
level	O
for	O
the	O
same	O
data	O
as	O
in	O
figure	O
and	O
figure	O
the	O
signal	O
variance	O
hyperparameter	O
was	O
set	B
to	O
f	O
the	O
optimum	O
is	O
close	O
to	O
the	O
parameters	O
used	O
when	O
generating	O
the	O
data	O
note	O
the	O
two	O
ridges	O
one	O
for	O
small	O
noise	O
and	O
length-scale	B
and	O
another	O
for	O
long	O
length-scale	B
and	O
noise	O
n	O
the	O
contour	O
lines	O
spaced	O
units	O
apart	O
in	O
log	O
probability	O
density	O
log	O
scale	O
due	O
to	O
the	O
poor	O
ability	O
of	O
the	O
model	O
to	O
explain	O
the	O
data	O
compare	O
to	O
figure	O
for	O
smaller	O
length-scales	O
the	O
marginal	B
likelihood	B
decreases	O
somewhat	O
more	O
slowly	O
corresponding	O
to	O
models	O
that	O
do	O
accommodate	O
the	O
data	O
but	O
waste	O
predictive	B
mass	O
at	O
regions	O
far	O
away	O
from	O
the	O
underlying	O
function	B
compare	O
to	O
figure	O
in	O
figure	O
the	O
dependence	O
of	O
the	O
log	O
marginal	B
likelihood	B
on	O
the	O
characteristic	O
length-scale	B
is	O
shown	O
for	O
different	O
numbers	O
of	O
training	O
cases	O
generally	O
the	O
more	O
data	O
the	O
more	O
peaked	O
the	O
marginal	B
likelihood	B
for	O
very	O
small	O
numbers	O
of	O
training	O
data	O
points	O
the	O
slope	O
of	O
the	O
log	O
marginal	B
likelihood	B
is	O
very	O
shallow	O
as	O
when	O
only	O
a	O
little	O
data	O
has	O
been	O
observed	O
both	O
very	O
short	O
and	O
intermediate	O
values	O
of	O
the	O
length-scale	B
are	O
consistent	O
with	O
the	O
data	O
with	O
more	O
data	O
the	O
complexity	O
term	O
gets	O
more	O
severe	O
and	O
discourages	O
too	O
short	O
length-scales	O
marginal	B
likelihood	B
gradient	O
to	O
set	B
the	O
hyperparameters	B
by	O
maximizing	O
the	O
marginal	B
likelihood	B
we	O
seek	O
the	O
partial	O
derivatives	O
of	O
the	O
marginal	B
likelihood	B
w	O
r	O
t	O
the	O
hyperparameters	B
using	O
eq	O
and	O
eq	O
we	O
obtain	O
k	O
k	O
k	O
j	O
k	O
where	O
k	O
log	O
pyx	O
yk	O
k	O
j	O
j	O
j	O
tr	O
the	O
complexity	O
of	O
computing	O
the	O
marginal	B
likelihood	B
in	O
eq	O
is	O
dominated	O
by	O
the	O
need	O
to	O
invert	O
the	O
k	O
matrix	O
log	O
determinant	O
of	O
k	O
is	O
easily	O
computed	O
as	O
a	O
by-product	O
of	O
the	O
inverse	O
standard	O
methods	O
for	O
matrix	O
inversion	O
of	O
positive	B
definite	I
symmetric	O
matrices	O
require	O
time	O
for	O
inversion	O
of	O
an	O
n	O
by	O
n	O
matrix	O
once	O
k	O
is	O
known	O
the	O
computation	O
of	O
the	O
derivatives	O
in	O
eq	O
requires	O
only	O
time	O
per	O
thus	O
the	O
computational	O
that	O
matrix-by-matrix	O
products	O
in	O
eq	O
should	O
not	O
be	O
computed	O
directly	O
in	O
the	O
first	O
term	O
do	O
the	O
vector-by-matrix	O
multiplications	O
first	O
in	O
the	O
trace	O
term	O
compute	O
only	O
the	O
diagonal	O
terms	O
of	O
the	O
product	O
lengthscalenoise	O
standard	O
deviation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
regression	O
head	O
of	O
computing	O
derivatives	O
is	O
small	O
so	O
using	O
a	O
gradient	O
based	O
optimizer	O
is	O
advantageous	O
estimation	O
of	O
by	O
optimzation	O
of	O
the	O
marginal	B
likelihood	B
has	O
a	O
long	O
history	O
in	O
spatial	O
statistics	O
see	O
e	O
g	O
mardia	O
and	O
marshall	O
as	O
n	O
increases	O
one	O
would	O
hope	O
that	O
the	O
data	O
becomes	O
increasingly	O
informative	O
about	O
however	O
it	O
is	O
necessary	O
to	O
contrast	O
what	O
stein	O
sec	O
calls	O
fixed-domain	O
asymptotics	O
one	O
gets	O
increasingly	O
dense	O
observations	O
within	O
some	O
region	O
with	O
increasing-domain	O
asymptotics	O
the	O
size	O
of	O
the	O
observation	O
region	O
grows	O
with	O
n	O
increasing-domain	O
asymptotics	O
are	O
a	O
natural	O
choice	O
in	O
a	O
time-series	O
context	O
but	O
fixed-domain	O
asymptotics	O
seem	O
more	O
natural	O
in	O
spatial	O
machine	O
learning	B
settings	O
for	O
further	O
discussion	O
see	O
stein	O
sec	O
figure	O
shows	O
an	O
example	O
of	O
the	O
log	O
marginal	B
likelihood	B
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
and	O
the	O
noise	O
standard	O
deviation	O
hyperparameters	B
for	O
the	O
squared	B
exponential	B
covariance	B
function	B
see	O
eq	O
the	O
signal	O
variance	O
f	O
was	O
set	B
to	O
the	O
marginal	B
likelihood	B
has	O
a	O
clear	O
maximum	O
around	O
the	O
hyperparameter	O
values	O
which	O
were	O
used	O
in	O
the	O
gaussian	B
process	I
from	O
which	O
the	O
data	O
was	O
generated	O
note	O
that	O
for	O
long	O
length-scales	O
and	O
a	O
noise	O
level	O
of	O
n	O
the	O
marginal	B
likelihood	B
becomes	O
almost	O
independent	O
of	O
the	O
length-scale	B
this	O
is	O
caused	O
by	O
the	O
model	O
explaining	O
everything	O
as	O
noise	O
and	O
no	O
longer	O
needing	O
the	O
signal	O
covariance	B
similarly	O
for	O
small	O
noise	O
and	O
a	O
length-scale	B
of	O
the	O
marginal	B
likelihood	B
becomes	O
almost	O
independent	O
of	O
the	O
noise	O
level	O
this	O
is	O
caused	O
by	O
the	O
ability	O
of	O
the	O
model	O
to	O
exactly	O
interpolate	O
the	O
data	O
at	O
this	O
short	O
length-scale	B
we	O
note	O
that	O
although	O
the	O
model	O
in	O
this	O
hyperparameter	O
region	O
explains	O
all	O
the	O
data-points	O
exactly	O
this	O
model	O
is	O
still	O
disfavoured	O
by	O
the	O
marginal	B
likelihood	B
see	O
figure	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
marginal	B
likelihood	B
does	O
not	O
suffer	O
from	O
multiple	O
local	O
optima	O
practical	O
experience	O
with	O
simple	O
covariance	B
functions	O
seem	O
to	O
indicate	O
that	O
local	O
maxima	O
are	O
not	O
a	O
devastating	O
problem	O
but	O
certainly	O
they	O
do	O
exist	O
in	O
fact	O
every	O
local	O
maximum	O
corresponds	O
to	O
a	O
particular	O
interpretation	O
of	O
the	O
data	O
in	O
figure	O
an	O
example	O
with	O
two	O
local	O
optima	O
is	O
shown	O
together	O
with	O
the	O
corresponding	O
free	O
predictions	O
of	O
the	O
model	O
at	O
each	O
of	O
the	O
two	O
local	O
optima	O
one	O
optimum	O
corresponds	O
to	O
a	O
relatively	O
complicated	O
model	O
with	O
low	O
noise	O
whereas	O
the	O
other	O
corresponds	O
to	O
a	O
much	O
simpler	O
model	O
with	O
more	O
noise	O
with	O
only	O
data	O
points	O
it	O
is	O
not	O
possible	O
for	O
the	O
model	O
to	O
confidently	O
reject	O
either	O
of	O
the	O
two	O
possibilities	O
the	O
numerical	O
value	O
of	O
the	O
marginal	B
likelihood	B
for	O
the	O
more	O
complex	O
model	O
is	O
about	O
higher	O
than	O
for	O
the	O
simple	O
model	O
according	O
to	O
the	O
bayesian	O
formalism	O
one	O
ought	O
to	O
weight	O
predictions	O
from	O
alternative	O
explanations	O
according	O
to	O
their	O
posterior	O
probabilities	O
in	O
practice	O
with	O
data	O
sets	O
of	O
much	O
larger	O
sizes	O
one	O
often	O
finds	O
that	O
one	O
local	O
optimum	O
is	O
orders	O
of	O
magnitude	O
more	O
probable	O
than	O
other	O
local	O
optima	O
so	O
averaging	O
together	O
alternative	O
explanations	O
may	O
not	O
be	O
necessary	O
however	O
care	O
should	O
be	O
taken	O
that	O
one	O
doesn	O
t	O
end	O
up	O
in	O
a	O
bad	O
local	O
optimum	O
above	O
we	O
have	O
described	O
how	O
to	O
adapt	O
the	O
parameters	O
of	O
the	O
covariance	B
function	B
given	O
one	O
dataset	O
however	O
it	O
may	O
happen	O
that	O
we	O
are	O
given	O
several	O
datasets	O
all	O
of	O
which	O
are	O
assumed	O
to	O
share	O
the	O
same	O
hyperparameters	B
this	O
is	O
known	O
as	O
multi-task	B
learning	B
see	O
e	O
g	O
caruana	O
in	O
this	O
case	O
one	O
can	O
multiple	O
local	O
maxima	O
multi-task	B
learning	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
n	O
standard	O
deviation	O
where	O
figure	O
panel	O
shows	O
the	O
marginal	B
likelihood	B
as	O
a	O
function	B
of	O
the	O
hyperparameters	B
and	O
f	O
standard	O
deviation	O
for	O
a	O
data	O
set	B
of	O
observations	O
in	O
panels	O
and	O
there	O
are	O
two	O
local	O
optima	O
indicated	O
with	O
the	O
global	O
optimum	O
has	O
low	O
noise	O
and	O
a	O
short	O
length-scale	B
the	O
local	O
optimum	O
has	O
a	O
high	O
noise	O
and	O
a	O
long	O
length	O
scale	O
in	O
and	O
the	O
inferred	O
underlying	O
functions	O
confidence	O
intervals	O
are	O
shown	O
for	O
each	O
of	O
the	O
two	O
solutions	O
in	O
fact	O
the	O
data	O
points	O
were	O
generated	O
by	O
a	O
gaussian	B
process	I
with	O
f	O
n	O
in	O
eq	O
simply	O
sum	O
the	O
log	O
marginal	B
likelihoods	O
of	O
the	O
individual	O
problems	O
and	O
optimize	O
this	O
sum	O
w	O
r	O
t	O
the	O
hyperparameters	B
and	O
picard	O
cross-validation	B
the	O
predictive	B
log	O
probability	O
when	O
leaving	O
out	O
training	O
case	O
i	O
is	O
log	O
pyix	O
y	O
i	O
log	O
i	O
i	O
log	O
negative	O
log	O
validation	O
density	O
loss	B
where	O
the	O
notation	O
y	O
i	O
means	O
all	O
targets	O
except	O
number	O
i	O
and	O
i	O
and	O
i	O
are	O
computed	O
according	O
to	O
eq	O
and	O
respectively	O
in	O
which	O
the	O
training	O
sets	O
are	O
taken	O
to	O
be	O
i	O
y	O
i	O
accordingly	O
the	O
loo	O
log	O
predictive	B
probability	O
is	O
lloox	O
y	O
log	O
pyix	O
y	O
i	O
nx	O
lengthscalenoise	O
standard	O
deviation	O
xoutput	O
y	O
xoutput	O
y	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
regression	O
pseudo-likelihood	B
see	O
and	O
eddy	O
for	O
a	O
discussion	O
of	O
this	O
and	O
related	O
approaches	O
lloo	O
in	O
eq	O
is	O
sometimes	O
called	O
the	O
log	O
pseudo-likelihood	B
notice	O
that	O
in	O
each	O
of	O
the	O
n	O
loo-cv	O
rotations	O
inference	O
in	O
the	O
gaussian	B
process	I
model	O
fixed	O
hyperparameters	B
essentially	O
consists	O
of	O
computing	O
the	O
inverse	O
covariance	B
matrix	I
to	O
allow	O
predictive	B
mean	O
and	O
variance	O
in	O
eq	O
and	O
to	O
be	O
evaluated	O
there	O
is	O
no	O
parameter-fitting	O
such	O
as	O
there	O
would	O
be	O
in	O
a	O
parametric	B
model	O
the	O
key	O
insight	O
is	O
that	O
when	O
repeatedly	O
applying	O
the	O
prediction	O
eq	O
and	O
the	O
expressions	O
are	O
almost	O
identical	O
we	O
need	O
the	O
inverses	O
of	O
covariance	B
matrices	O
with	O
a	O
single	O
column	O
and	O
row	O
removed	O
in	O
turn	O
this	O
can	O
be	O
computed	O
efficiently	O
from	O
the	O
inverse	O
of	O
the	O
complete	O
covariance	B
matrix	I
using	O
inversion	O
by	O
partitioning	O
see	O
eq	O
a	O
similar	O
insight	O
has	O
also	O
been	O
used	O
for	O
spline	O
models	O
see	O
e	O
g	O
wahba	O
sec	O
the	O
approach	O
was	O
used	O
for	O
hyperparameter	O
selection	O
in	O
gaussian	B
process	I
models	O
in	O
sundararajan	O
and	O
keerthi	O
the	O
expressions	O
for	O
the	O
loo-cv	O
predictive	B
mean	O
and	O
variance	O
are	O
i	O
yi	O
and	O
i	O
where	O
careful	O
inspection	O
reveals	O
that	O
the	O
mean	O
i	O
is	O
in	O
fact	O
independent	O
of	O
yi	O
as	O
indeed	O
it	O
should	O
be	O
the	O
computational	O
expense	O
of	O
computing	O
these	O
quantities	O
is	O
once	O
for	O
computing	O
the	O
inverse	O
of	O
k	O
plus	O
for	O
the	O
entire	O
loocv	O
procedure	O
k	O
is	O
known	O
thus	O
the	O
computational	O
overhead	O
for	O
the	O
loo-cv	O
quantities	O
is	O
negligible	O
plugging	O
these	O
expressions	O
into	O
eq	O
and	O
produces	O
a	O
performance	O
estimator	O
which	O
we	O
can	O
optimize	O
w	O
r	O
t	O
hyperparameters	B
to	O
do	O
model	O
selection	O
in	O
particular	O
we	O
can	O
compute	O
the	O
partial	O
derivatives	O
of	O
lloo	O
w	O
r	O
t	O
the	O
hyperparameters	B
eq	O
and	O
use	O
conjugate	O
gradient	O
optimization	O
to	O
this	O
end	O
we	O
need	O
the	O
partial	O
derivatives	O
of	O
the	O
loo-cv	O
predictive	B
mean	O
and	O
variances	O
from	O
eq	O
w	O
r	O
t	O
the	O
hyperparameters	B
izjk	O
i	O
j	O
where	O
k	O
and	O
zj	O
k	O
k	O
obtained	O
by	O
using	O
the	O
chain-rule	O
and	O
eq	O
to	O
give	O
j	O
ii	O
i	O
j	O
ii	O
the	O
partial	O
derivatives	O
of	O
eq	O
are	O
lloo	O
j	O
nx	O
nx	O
log	O
pyix	O
y	O
i	O
i	O
j	O
i	O
log	O
pyix	O
y	O
i	O
i	O
j	O
i	O
i	O
izj	O
the	O
computational	O
complexity	O
is	O
for	O
computing	O
the	O
inverse	O
of	O
k	O
and	O
per	O
hyperparameter	O
for	O
the	O
derivative	O
eq	O
thus	O
the	O
computational	O
burden	O
of	O
the	O
derivatives	O
is	O
greater	O
for	O
the	O
loo-cv	O
method	O
than	O
for	O
the	O
method	O
based	O
on	O
marginal	B
likelihood	B
eq	O
of	O
the	O
matrix-by-matrix	O
product	O
k	O
k	O
j	O
for	O
each	O
hyperparameter	O
is	O
un	O
avoidable	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
loo-cv	O
with	O
squared	B
error	O
loss	B
in	O
eq	O
we	O
have	O
used	O
the	O
log	O
of	O
the	O
validation	O
density	O
as	O
a	O
crossvalidation	O
measure	B
of	O
fit	O
equivalently	O
the	O
negative	O
log	O
validation	O
density	O
as	O
a	O
loss	B
function	B
one	O
could	O
also	O
envisage	O
using	O
other	O
loss	B
functions	O
such	O
as	O
the	O
commonly	O
used	O
squared	B
error	O
however	O
this	O
loss	B
function	B
is	O
only	O
a	O
function	B
of	O
the	O
predicted	O
mean	O
and	O
ignores	O
the	O
validation	O
set	B
variances	O
further	O
since	O
the	O
mean	O
prediction	O
eq	O
is	O
independent	O
of	O
the	O
scale	O
of	O
the	O
covariances	O
you	O
can	O
multiply	O
the	O
covariance	B
of	O
the	O
signal	O
and	O
noise	O
by	O
an	O
arbitrary	O
positive	O
constant	O
without	O
changing	O
the	O
mean	O
predictions	O
one	O
degree	O
of	O
freedom	O
is	O
left	O
by	O
a	O
loo-cv	O
procedure	O
based	O
on	O
squared	B
error	O
loss	B
any	O
other	O
loss	B
function	B
which	O
depends	O
only	O
on	O
the	O
mean	O
predictions	O
but	O
of	O
course	O
the	O
full	O
predictive	B
distribution	O
does	O
depend	O
on	O
the	O
scale	O
of	O
the	O
covariance	B
function	B
also	O
computation	O
of	O
the	O
derivatives	O
based	O
on	O
the	O
squared	B
error	O
loss	B
has	O
similar	O
computational	O
complexity	O
as	O
the	O
negative	O
log	O
validation	O
density	O
loss	B
in	O
conclusion	O
it	O
seems	O
unattractive	O
to	O
use	O
loo-cv	O
based	O
on	O
squared	B
error	O
loss	B
for	O
hyperparameter	O
selection	O
comparing	O
the	O
pseudo-likelihood	B
for	O
the	O
loo-cv	O
methodology	O
with	O
the	O
marginal	B
likelihood	B
from	O
the	O
previous	O
section	O
it	O
is	O
interesting	O
to	O
ask	O
under	O
which	O
circumstances	O
each	O
method	O
might	O
be	O
preferable	O
their	O
computational	O
demands	O
are	O
roughly	O
identical	O
this	O
issue	O
has	O
not	O
been	O
studied	O
much	O
empirically	O
however	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
marginal	B
likelihood	B
tells	O
us	O
the	O
probability	O
of	O
the	O
observations	O
given	O
the	O
assumptions	O
of	O
the	O
model	O
this	O
contrasts	O
with	O
the	O
frequentist	O
loo-cv	O
value	O
which	O
gives	O
an	O
estimate	O
for	O
the	O
predictive	B
probability	O
whether	O
or	O
not	O
the	O
assumptions	O
of	O
the	O
model	O
may	O
be	O
fulfilled	O
thus	O
wahba	O
sec	O
has	O
argued	O
that	O
cv	O
procedures	O
should	O
be	O
more	O
robust	O
against	O
model	O
mis-specification	O
examples	O
and	O
discussion	O
in	O
the	O
following	O
we	O
give	O
three	O
examples	O
of	O
model	O
selection	O
for	O
regression	O
models	O
we	O
first	O
describe	O
a	O
modelling	O
task	O
which	O
illustrates	O
how	O
special	O
covariance	B
functions	O
can	O
be	O
designed	O
to	O
achieve	O
various	O
useful	O
effects	O
and	O
can	O
be	O
evaluated	O
using	O
the	O
marginal	B
likelihood	B
secondly	O
we	O
make	O
a	O
short	O
reference	O
to	O
the	O
model	O
selection	O
carried	O
out	O
for	O
the	O
robot	B
arm	O
problem	O
discussed	O
in	O
chapter	O
and	O
again	O
in	O
chapter	O
finally	O
we	O
discuss	O
an	O
example	O
where	O
we	O
deliberately	O
choose	O
a	O
covariance	B
function	B
that	O
is	O
not	O
well-suited	O
for	O
the	O
problem	O
this	O
is	O
the	O
so-called	O
mis-specified	O
model	O
scenario	O
mauna	O
loa	O
atmospheric	O
carbon	O
dioxide	O
we	O
will	O
use	O
a	O
modelling	O
problem	O
concerning	O
the	O
concentration	O
of	O
in	O
the	O
atmosphere	O
to	O
illustrate	O
how	O
the	O
marginal	B
likelihood	B
can	O
be	O
used	O
to	O
set	B
multiple	O
hyperparameters	B
in	O
hierarchical	O
gaussian	B
process	I
models	O
a	O
complex	O
covariance	B
function	B
is	O
derived	O
by	O
combining	O
several	O
different	O
kinds	O
of	O
simple	O
covariance	B
functions	O
and	O
the	O
resulting	O
model	O
provides	O
an	O
excellent	O
fit	O
to	O
the	O
data	O
as	O
well	O
the	O
special	O
case	O
where	O
we	O
know	O
either	O
the	O
signal	O
or	O
the	O
noise	O
variance	O
there	O
is	O
no	O
indeterminancy	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
regression	O
figure	O
the	O
observations	O
of	O
monthly	O
averages	O
of	O
the	O
atmospheric	O
concentration	O
of	O
made	O
between	O
and	O
the	O
end	O
of	O
together	O
with	O
predictive	B
confidence	O
region	O
for	O
a	O
gaussian	B
process	I
regression	I
model	O
years	O
into	O
the	O
future	O
rising	O
trend	O
and	O
seasonal	O
variations	O
are	O
clearly	O
visible	O
note	O
also	O
that	O
the	O
confidence	O
interval	O
gets	O
wider	O
the	O
further	O
the	O
predictions	O
are	O
extrapolated	O
as	O
insights	O
into	O
its	O
properties	O
by	O
interpretation	O
of	O
the	O
adapted	O
hyperparameters	B
although	O
the	O
data	O
is	O
one-dimensional	O
and	O
therefore	O
easy	O
to	O
visualize	O
a	O
total	O
of	O
hyperparameters	B
are	O
used	O
which	O
in	O
practice	O
rules	O
out	O
the	O
use	O
of	O
cross-validation	B
for	O
setting	O
parameters	O
except	O
for	O
the	O
gradient-based	O
loo-cv	O
procedure	O
from	O
the	O
previous	O
section	O
the	O
data	O
and	O
whorf	O
consists	O
of	O
monthly	O
average	O
atmospheric	O
concentrations	O
parts	O
per	O
million	O
by	O
volume	O
derived	O
from	O
in	O
situ	O
air	O
samples	O
collected	O
at	O
the	O
mauna	O
loa	O
observatory	O
hawaii	O
between	O
and	O
some	O
missing	O
the	O
data	O
is	O
shown	O
in	O
figure	O
our	O
goal	O
is	O
the	O
model	O
the	O
concentration	O
as	O
a	O
function	B
of	O
time	O
x	O
several	O
features	O
are	O
immediately	O
apparent	O
a	O
long	O
term	O
rising	O
trend	O
a	O
pronounced	O
seasonal	O
variation	O
and	O
some	O
smaller	O
irregularities	O
in	O
the	O
following	O
we	O
suggest	O
contributions	O
to	O
a	O
combined	O
covariance	B
function	B
which	O
takes	O
care	O
of	O
these	O
individual	O
properties	O
this	O
is	O
meant	O
primarily	O
to	O
illustrate	O
the	O
power	O
and	O
flexibility	O
of	O
the	O
gaussian	B
process	I
framework	O
it	O
is	O
possible	O
that	O
other	O
choices	O
would	O
be	O
more	O
appropriate	O
for	O
this	O
data	O
set	B
to	O
model	O
the	O
long	O
term	O
smooth	O
rising	O
trend	O
we	O
use	O
a	O
squared	B
exponential	B
covariance	B
term	O
with	O
two	O
hyperparameters	B
controlling	O
the	O
amplitude	O
and	O
characteristic	O
length-scale	B
smooth	O
trend	O
exp	O
note	O
that	O
we	O
just	O
use	O
a	O
smooth	O
trend	O
actually	O
enforcing	O
the	O
trend	O
a	O
priori	O
to	O
be	O
increasing	O
is	O
probably	O
not	O
so	O
simple	O
and	O
not	O
desirable	O
we	O
can	O
use	O
the	O
periodic	B
covariance	B
function	B
from	O
eq	O
with	O
a	O
period	O
of	O
one	O
year	O
to	O
model	O
the	O
seasonal	O
variation	O
however	O
it	O
is	O
not	O
clear	O
that	O
the	O
seasonal	O
trend	O
is	O
data	O
is	O
available	O
from	O
seasonal	O
component	O
concentration	O
ppm	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
figure	O
panel	O
long	O
term	O
trend	O
dashed	O
left	O
hand	O
scale	O
predicted	O
by	O
the	O
squared	B
exponential	B
contribution	O
superimposed	O
is	O
the	O
medium	O
term	O
trend	O
full	O
line	O
right	O
hand	O
scale	O
predicted	O
by	O
the	O
rational	B
quadratic	I
contribution	O
the	O
vertical	O
dashdotted	O
line	O
indicates	O
the	O
upper	O
limit	O
of	O
the	O
training	O
data	O
panel	O
shows	O
the	O
seasonal	O
variation	O
over	O
the	O
year	O
for	O
three	O
different	O
years	O
the	O
concentration	O
peaks	O
in	O
mid	O
may	O
and	O
has	O
a	O
low	O
in	O
the	O
beginning	O
of	O
october	O
the	O
seasonal	O
variation	O
is	O
smooth	O
but	O
not	O
of	O
exactly	O
sinusoidal	O
shape	O
the	O
peak-to-peak	O
amplitude	O
increases	O
from	O
about	O
ppm	O
in	O
to	O
about	O
ppm	O
in	O
but	O
the	O
shape	O
does	O
not	O
change	O
very	O
much	O
the	O
characteristic	O
decay	O
length	O
of	O
the	O
periodic	B
component	O
is	O
inferred	O
to	O
be	O
years	O
so	O
the	O
seasonal	O
trend	O
changes	O
rather	O
slowly	O
as	O
also	O
suggested	O
by	O
the	O
gradual	O
progression	O
between	O
the	O
three	O
years	O
shown	O
exactly	O
periodic	B
so	O
we	O
modify	O
eq	O
by	O
taking	O
the	O
product	O
with	O
a	O
squared	B
exponential	B
component	O
the	O
product	O
construction	O
from	O
section	O
to	O
allow	O
a	O
decay	O
away	O
from	O
exact	O
periodicity	O
exp	O
where	O
gives	O
the	O
magnitude	O
the	O
decay-time	O
for	O
the	O
periodic	B
component	O
and	O
the	O
smoothness	O
of	O
the	O
periodic	B
component	O
the	O
period	O
has	O
been	O
fixed	O
to	O
one	O
the	O
seasonal	O
component	O
in	O
the	O
data	O
is	O
caused	O
primarily	O
by	O
different	O
rates	O
of	O
uptake	O
for	O
plants	O
depending	O
on	O
the	O
season	O
and	O
it	O
is	O
probably	O
reasonable	O
to	O
assume	O
that	O
this	O
pattern	O
may	O
itself	O
change	O
slowly	O
over	O
time	O
partially	O
due	O
to	O
the	O
elevation	O
of	O
the	O
level	O
itself	O
if	O
this	O
effect	O
turns	O
out	O
not	O
to	O
be	O
relevant	O
then	O
it	O
can	O
be	O
effectively	O
removed	O
at	O
the	O
fitting	O
stage	O
by	O
allowing	O
to	O
become	O
very	O
large	O
to	O
model	O
the	O
medium	O
term	O
irregularities	O
a	O
rational	B
quadratic	I
term	O
medium	O
term	O
irregularities	O
is	O
used	O
eq	O
where	O
is	O
the	O
magnitude	O
is	O
the	O
typical	O
length-scale	B
and	O
is	O
the	O
shape	O
parameter	O
determining	O
diffuseness	O
of	O
the	O
length-scales	O
see	O
the	O
discussion	O
on	O
page	O
one	O
could	O
also	O
have	O
used	O
a	O
squared	B
exponential	B
form	O
for	O
this	O
component	O
but	O
it	O
turns	O
out	O
that	O
the	O
rational	B
quadratic	I
works	O
better	O
higher	O
marginal	B
likelihood	B
probably	O
because	O
it	O
can	O
accommodate	O
several	O
length-scales	O
concentration	O
ppmyear	O
concentration	O
ppmjfmamjjasond	O
concentration	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
regression	O
figure	O
the	O
time	O
course	O
of	O
the	O
seasonal	O
effect	O
plotted	O
in	O
a	O
months	O
vs	O
year	O
plot	O
wrap-around	O
continuity	O
between	O
the	O
edges	O
the	O
labels	O
on	O
the	O
contours	O
are	O
in	O
ppmv	O
of	O
the	O
training	O
period	O
extends	O
up	O
to	O
the	O
dashed	O
line	O
note	O
the	O
slow	O
development	O
the	O
height	O
of	O
the	O
may	O
peak	O
may	O
have	O
started	O
to	O
recede	O
but	O
the	O
low	O
in	O
october	O
may	O
currently	O
be	O
deepening	O
further	O
the	O
seasonal	O
effects	O
from	O
three	O
particular	O
years	O
were	O
also	O
plotted	O
in	O
figure	O
finally	O
we	O
specify	O
a	O
noise	B
model	I
as	O
the	O
sum	O
of	O
a	O
squared	B
exponential	B
con	O
tribution	O
and	O
an	O
independent	O
component	O
xq	O
exp	O
pq	O
where	O
is	O
the	O
magnitude	O
of	O
the	O
correlated	B
noise	O
component	O
is	O
its	O
lengthscale	O
and	O
is	O
the	O
magnitude	O
of	O
the	O
independent	O
noise	O
component	O
noise	O
in	O
the	O
series	O
could	O
be	O
caused	O
by	O
measurement	O
inaccuracies	O
and	O
by	O
local	O
short-term	O
weather	O
phenomena	O
so	O
it	O
is	O
probably	O
reasonable	O
to	O
assume	O
at	O
least	O
a	O
modest	O
amount	O
of	O
correlation	O
in	O
time	O
notice	O
that	O
the	O
correlated	B
noise	O
component	O
the	O
first	O
term	O
of	O
eq	O
has	O
an	O
identical	O
expression	O
to	O
the	O
long	O
term	O
component	O
in	O
eq	O
when	O
optimizing	O
the	O
hyperparameters	B
we	O
will	O
see	O
that	O
one	O
of	O
these	O
components	O
becomes	O
large	O
with	O
a	O
long	O
length-scale	B
long	O
term	O
trend	O
while	O
the	O
other	O
remains	O
small	O
with	O
a	O
short	O
length-scale	B
the	O
fact	O
that	O
we	O
have	O
chosen	O
to	O
call	O
one	O
of	O
these	O
components	O
signal	O
and	O
the	O
other	O
one	O
noise	O
is	O
only	O
a	O
question	O
of	O
interpretation	O
presumably	O
we	O
are	O
less	O
interested	O
in	O
very	O
short-term	O
effect	O
and	O
thus	O
call	O
it	O
noise	O
if	O
on	O
the	O
other	O
hand	O
we	O
were	O
interested	O
in	O
this	O
effect	O
we	O
would	O
call	O
it	O
signal	O
the	O
final	O
covariance	B
function	B
is	O
kx	O
with	O
hyperparameters	B
we	O
first	O
subtract	O
the	O
empirical	O
mean	O
of	O
the	O
data	O
ppm	O
and	O
then	O
fit	O
the	O
hyperparameters	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
using	O
a	O
conjugate	O
gradient	O
optimizer	O
to	O
avoid	O
bad	O
local	O
minima	O
caused	O
by	O
swapping	O
r	O
oles	O
of	O
the	O
rational	B
quadratic	I
and	O
squared	B
exponential	B
terms	O
a	O
few	O
random	O
restarts	O
are	O
tried	O
picking	O
the	O
run	O
with	O
the	O
best	O
marginal	B
likelihood	B
which	O
was	O
log	O
pyx	O
noise	O
terms	O
parameter	O
estimation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
we	O
now	O
examine	O
and	O
interpret	O
the	O
hyperparameters	B
which	O
optimize	O
the	O
marginal	B
likelihood	B
the	O
long	O
term	O
trend	O
has	O
a	O
magnitude	O
of	O
ppm	O
and	O
a	O
length	O
scale	O
of	O
years	O
the	O
mean	O
predictions	O
inside	O
the	O
range	O
of	O
the	O
training	O
data	O
and	O
extending	O
for	O
years	O
into	O
the	O
future	O
are	O
depicted	O
in	O
figure	O
in	O
the	O
same	O
plot	O
right	O
hand	O
axis	O
we	O
also	O
show	O
the	O
medium	O
term	O
effects	O
modelled	O
by	O
the	O
rational	B
quadratic	I
component	O
with	O
magnitude	O
ppm	O
typical	O
length	O
years	O
and	O
shape	O
the	O
very	O
small	O
shape	O
value	O
allows	O
for	O
covariance	B
at	O
many	O
different	O
length-scales	O
which	O
is	O
also	O
evident	O
in	O
figure	O
notice	O
that	O
beyond	O
the	O
edge	O
of	O
the	O
training	O
data	O
the	O
mean	O
of	O
this	O
contribution	O
smoothly	O
decays	O
to	O
zero	O
but	O
of	O
course	O
it	O
still	O
has	O
a	O
contribution	O
to	O
the	O
uncertainty	O
see	O
figure	O
the	O
hyperparameter	O
values	O
for	O
the	O
decaying	O
periodic	B
contribution	O
are	O
magnitude	O
ppm	O
decay-time	O
years	O
and	O
the	O
smoothness	O
of	O
the	O
periodic	B
component	O
is	O
the	O
quite	O
long	O
decay-time	O
shows	O
that	O
the	O
data	O
have	O
a	O
very	O
close	O
to	O
periodic	B
component	O
in	O
the	O
short	O
term	O
in	O
figure	O
we	O
show	O
the	O
mean	O
periodic	B
contribution	O
for	O
three	O
years	O
corresponding	O
to	O
the	O
beginning	O
middle	O
and	O
end	O
of	O
the	O
training	O
data	O
this	O
component	O
is	O
not	O
exactly	O
sinusoidal	O
and	O
it	O
changes	O
its	O
shape	O
slowly	O
over	O
time	O
most	O
notably	O
the	O
amplitude	O
is	O
increasing	O
see	O
figure	O
for	O
the	O
noise	O
components	O
we	O
get	O
the	O
amplitude	O
for	O
the	O
correlated	B
component	O
ppm	O
a	O
length-scale	B
of	O
months	O
and	O
an	O
independent	O
noise	O
magnitude	O
of	O
ppm	O
thus	O
the	O
correlation	O
length	O
for	O
the	O
noise	O
component	O
is	O
indeed	O
inferred	O
to	O
be	O
short	O
and	O
the	O
total	O
magnitude	O
of	O
the	O
noise	O
ppm	O
indicating	O
that	O
the	O
data	O
can	O
be	O
explained	O
very	O
is	O
just	O
well	O
by	O
the	O
model	O
note	O
also	O
in	O
figure	O
that	O
the	O
model	O
makes	O
relatively	O
confident	O
predictions	O
the	O
confidence	O
region	O
being	O
ppm	O
wide	O
at	O
a	O
year	O
prediction	O
horizon	O
in	O
conclusion	O
we	O
have	O
seen	O
an	O
example	O
of	O
how	O
non-trivial	O
structure	O
can	O
be	O
inferred	O
by	O
using	O
composite	O
covariance	B
functions	O
and	O
that	O
the	O
ability	O
to	O
leave	O
hyperparameters	B
to	O
be	O
determined	O
by	O
the	O
data	O
is	O
useful	O
in	O
practice	O
of	O
course	O
a	O
serious	O
treatment	O
of	O
such	O
data	O
would	O
probably	O
require	O
modelling	O
of	O
other	O
effects	O
such	O
as	O
demographic	O
and	O
economic	O
indicators	O
too	O
finally	O
one	O
may	O
want	O
to	O
use	O
a	O
real	O
time-series	O
approach	O
just	O
a	O
regression	O
from	O
time	O
to	O
level	O
as	O
we	O
have	O
done	O
here	O
to	O
accommodate	O
causality	O
etc	O
nevertheless	O
the	O
ability	O
of	O
the	O
gaussian	B
process	I
to	O
avoid	O
simple	O
parametric	B
assumptions	O
and	O
still	O
build	O
in	O
a	O
lot	O
of	O
structure	O
makes	O
it	O
as	O
we	O
have	O
seen	O
a	O
very	O
attractive	O
model	O
in	O
many	O
application	O
domains	O
robot	B
arm	O
inverse	O
dynamics	O
we	O
have	O
discussed	O
the	O
use	O
of	O
gpr	O
for	O
the	O
sarcos	O
robot	B
arm	O
inverse	O
dynamics	O
problem	O
in	O
section	O
this	O
example	O
is	O
also	O
further	O
studied	O
in	O
section	O
where	O
a	O
variety	O
of	O
approximation	O
methods	O
are	O
compared	O
because	O
the	O
size	O
of	O
the	O
training	O
set	B
examples	O
precludes	O
the	O
use	O
of	O
simple	O
gpr	O
due	O
to	O
its	O
storage	O
and	O
time	O
complexity	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
regression	O
figure	O
mis-specification	O
example	O
fit	O
to	O
datapoints	O
drawn	O
from	O
a	O
step	O
function	B
with	O
gaussian	O
noise	O
with	O
standard	O
deviation	O
n	O
the	O
gaussian	B
process	I
models	O
are	O
using	O
a	O
squared	B
exponential	B
covariance	B
function	B
panel	O
shows	O
the	O
mean	O
and	O
confidence	O
interval	O
for	O
the	O
noisy	O
signal	O
in	O
grey	O
when	O
the	O
hyperparameters	B
are	O
chosen	O
to	O
maximize	O
the	O
marginal	B
likelihood	B
panel	O
shows	O
the	O
resulting	O
model	O
when	O
the	O
hyperparameters	B
are	O
chosen	O
using	O
leave-one-out	B
cross-validation	B
note	O
that	O
the	O
marginal	B
likelihood	B
chooses	O
a	O
high	O
noise	O
level	O
and	O
long	O
length-scale	B
whereas	O
loo-cv	O
chooses	O
a	O
smaller	O
noise	O
level	O
and	O
shorter	O
length-scale	B
it	O
is	O
not	O
immediately	O
obvious	O
which	O
fit	O
it	O
worse	O
one	O
of	O
the	O
techniques	O
considered	O
in	O
section	O
is	O
the	O
subset	B
of	I
datapoints	I
method	O
where	O
we	O
simply	O
discard	O
some	O
of	O
the	O
data	O
and	O
only	O
make	O
use	O
of	O
m	O
n	O
training	O
examples	O
given	O
a	O
subset	O
of	O
the	O
training	O
data	O
of	O
size	O
m	O
selected	O
at	O
random	O
we	O
adjusted	O
the	O
hyperparameters	B
by	O
optimizing	O
either	O
the	O
marginal	B
likelihood	B
or	O
lloo	O
as	O
ard	O
was	O
used	O
this	O
involved	O
adjusting	O
d	O
hyperparameters	B
this	O
process	O
was	O
repeated	O
times	O
with	O
different	O
random	O
subsets	O
of	O
the	O
data	O
selected	O
for	O
both	O
m	O
and	O
m	O
the	O
results	O
show	O
that	O
the	O
predictive	B
accuracy	O
obtained	O
from	O
the	O
two	O
optimization	O
methods	O
is	O
very	O
similar	O
on	O
both	O
standardized	B
mean	I
squared	B
error	I
and	O
mean	B
standardized	I
log	I
loss	B
criteria	O
but	O
that	O
the	O
marginal	B
likelihood	B
optimization	O
is	O
much	O
quicker	O
step	O
function	B
example	O
illustrating	O
mis-specification	O
in	O
this	O
section	O
we	O
discuss	O
the	O
mis-specified	O
model	O
scenario	O
where	O
we	O
attempt	O
to	O
learn	O
the	O
hyperparameters	B
for	O
a	O
covariance	B
function	B
which	O
is	O
not	O
very	O
well	O
suited	O
to	O
the	O
data	O
the	O
mis-specification	O
arises	O
because	O
the	O
data	O
comes	O
from	O
a	O
function	B
which	O
has	O
either	O
zero	O
or	O
very	O
low	O
probability	O
under	O
the	O
gp	O
prior	O
one	O
could	O
ask	O
why	O
it	O
is	O
interesting	O
to	O
discuss	O
this	O
scenario	O
since	O
one	O
should	O
surely	O
simply	O
avoid	O
choosing	O
such	O
a	O
model	O
in	O
practice	O
while	O
this	O
is	O
true	O
in	O
theory	O
for	O
practical	O
reasons	O
such	O
as	O
the	O
convenience	O
of	O
using	O
standard	O
forms	O
for	O
the	O
covariance	B
function	B
or	O
because	O
vague	O
prior	O
information	O
one	O
inevitably	O
ends	O
up	O
in	O
a	O
situation	O
which	O
resembles	O
some	O
level	O
of	O
mis-specification	O
as	O
an	O
example	O
we	O
use	O
data	O
from	O
a	O
noisy	O
step	O
function	B
and	O
fit	O
a	O
gp	O
model	O
with	O
a	O
squared	B
exponential	B
covariance	B
function	B
figure	O
there	O
is	O
misspecification	O
because	O
it	O
would	O
be	O
very	O
unlikely	O
that	O
samples	O
drawn	O
from	O
a	O
gp	O
yinput	O
x	O
yinput	O
x	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
figure	O
same	O
data	O
as	O
in	O
figure	O
panel	O
shows	O
the	O
result	O
of	O
using	O
a	O
covariance	B
function	B
which	O
is	O
the	O
sum	O
of	O
two	O
squared-exponential	O
terms	O
although	O
this	O
is	O
still	O
a	O
stationary	O
covariance	B
function	B
it	O
gives	O
rise	O
to	O
a	O
higher	O
marginal	B
likelihood	B
than	O
for	O
the	O
squared-exponential	O
covariance	B
function	B
in	O
figure	O
and	O
probably	O
also	O
a	O
better	O
fit	O
in	O
panel	O
the	O
neural	B
network	I
covariance	B
function	B
eq	O
was	O
used	O
providing	O
a	O
much	O
larger	O
marginal	B
likelihood	B
and	O
a	O
very	O
good	O
fit	O
with	O
the	O
stationary	O
se	O
covariance	B
function	B
would	O
look	O
like	O
a	O
step	O
function	B
for	O
short	O
length-scales	O
samples	O
can	O
vary	O
quite	O
quickly	O
but	O
they	O
would	O
tend	O
to	O
vary	O
rapidly	O
all	O
over	O
not	O
just	O
near	O
the	O
step	O
conversely	O
a	O
stationary	O
se	O
covariance	B
function	B
with	O
a	O
long	O
length-scale	B
could	O
model	O
the	O
flat	O
parts	O
of	O
the	O
step	O
function	B
but	O
not	O
the	O
rapid	O
transition	O
note	O
that	O
gibbs	B
covariance	B
function	B
eq	O
would	O
be	O
one	O
way	O
to	O
achieve	O
the	O
desired	O
effect	O
it	O
is	O
interesting	O
to	O
note	O
the	O
differences	O
between	O
the	O
model	O
optimized	O
with	O
marginal	B
likelihood	B
in	O
figure	O
and	O
one	O
optimized	O
with	O
loo-cv	O
in	O
panel	O
of	O
the	O
same	O
figure	O
see	O
exercise	O
for	O
more	O
on	O
how	O
these	O
two	O
criteria	O
weight	O
the	O
influence	O
of	O
the	O
prior	O
for	O
comparison	O
we	O
show	O
the	O
predictive	B
distribution	O
for	O
two	O
other	O
covariance	B
functions	O
in	O
figure	O
in	O
panel	O
a	O
sum	O
of	O
two	O
squared	B
exponential	B
terms	O
were	O
used	O
in	O
the	O
covariance	B
notice	O
that	O
this	O
covariance	B
function	B
is	O
still	O
stationary	O
but	O
it	O
is	O
more	O
flexible	O
than	O
a	O
single	O
squared	B
exponential	B
since	O
it	O
has	O
two	O
magnitude	O
and	O
two	O
length-scale	B
parameters	O
the	O
predictive	B
distribution	O
looks	O
a	O
little	O
bit	O
better	O
and	O
the	O
value	O
of	O
the	O
log	O
marginal	B
likelihood	B
improves	O
from	O
in	O
figure	O
to	O
in	O
figure	O
we	O
also	O
tried	O
the	O
neural	B
network	I
covariance	B
function	B
from	O
eq	O
which	O
is	O
ideally	O
suited	O
to	O
this	O
case	O
since	O
it	O
allows	O
saturation	O
at	O
different	O
values	O
in	O
the	O
positive	O
and	O
negative	O
directions	O
of	O
x	O
as	O
shown	O
in	O
figure	O
the	O
predictions	O
are	O
also	O
near	O
perfect	O
and	O
the	O
log	O
marginal	B
likelihood	B
is	O
much	O
larger	O
at	O
model	O
selection	O
for	O
gp	O
classification	B
in	O
this	O
section	O
we	O
compute	O
the	O
derivatives	O
of	O
the	O
approximate	O
marginal	B
likelihood	B
for	O
the	O
laplace	O
and	O
ep	O
methods	O
for	O
binary	B
classification	B
which	O
are	O
needed	O
for	O
training	O
we	O
also	O
give	O
the	O
detailed	O
algorithms	O
for	O
these	O
and	O
briefly	O
discuss	O
the	O
possible	O
use	O
of	O
cross-validation	B
and	O
other	O
methods	O
for	O
training	O
binary	B
gp	O
yinput	O
x	O
yinput	O
x	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
classification	B
classifiers	O
derivatives	O
of	O
the	O
marginal	B
likelihood	B
for	O
laplace	O
s	O
approximation	O
recall	O
from	O
section	O
that	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
was	O
given	O
in	O
eq	O
as	O
log	O
qyx	O
fk	O
f	O
log	O
py	O
f	O
log	O
kw	O
and	O
f	O
is	O
the	O
maximum	O
of	O
the	O
posterior	O
eq	O
where	O
b	O
i	O
w	O
found	O
by	O
newton	O
s	O
method	O
in	O
algorithm	O
and	O
w	O
is	O
the	O
diagonal	O
matrix	O
w	O
log	O
py	O
f	O
we	O
can	O
now	O
optimize	O
the	O
approximate	O
marginal	B
likelihood	B
qyx	O
w	O
r	O
t	O
the	O
hyperparameters	B
to	O
this	O
end	O
we	O
seek	O
the	O
partial	O
derivatives	O
of	O
qyx	O
j	O
the	O
covariance	B
matrix	I
k	O
is	O
a	O
function	B
of	O
the	O
hyperparameters	B
but	O
f	O
and	O
therefore	O
w	O
are	O
also	O
implicitly	O
functions	O
of	O
since	O
when	O
changes	O
the	O
optimum	O
of	O
the	O
posterior	O
f	O
also	O
changes	O
thus	O
fi	O
j	O
log	O
qyx	O
log	O
qyx	O
log	O
qyx	O
nx	O
fi	O
j	O
j	O
by	O
the	O
chain	O
rule	O
using	O
eq	O
and	O
eq	O
the	O
explicit	O
term	O
is	O
given	O
by	O
fk	O
k	O
j	O
k	O
f	O
tr	O
k	O
j	O
log	O
qyx	O
j	O
when	O
evaluating	O
the	O
remaining	O
term	O
from	O
eq	O
we	O
utilize	O
the	O
fact	O
that	O
f	O
is	O
the	O
maximum	O
of	O
the	O
posterior	O
so	O
that	O
f	O
at	O
f	O
f	O
where	O
the	O
log	O
posterior	O
is	O
defined	O
in	O
eq	O
thus	O
the	O
implicit	O
derivatives	O
of	O
the	O
two	O
first	O
terms	O
of	O
eq	O
vanish	O
leaving	O
only	O
log	O
qyx	O
fi	O
log	O
fi	O
w	O
tr	O
w	O
w	O
fi	O
log	O
py	O
f	O
f	O
i	O
ii	O
k	O
j	O
in	O
order	O
to	O
evaluate	O
the	O
derivative	O
f	O
j	O
we	O
differentiate	O
the	O
self-consistent	O
eq	O
f	O
k	O
log	O
py	O
f	O
to	O
obtain	O
f	O
log	O
py	O
f	O
j	O
where	O
we	O
have	O
used	O
the	O
chain	O
rule	O
j	O
f	O
j	O
f	O
and	O
the	O
identity	O
log	O
py	O
f	O
f	O
w	O
the	O
desired	O
derivatives	O
are	O
obtained	O
by	O
plugging	O
eq	O
into	O
eq	O
k	O
j	O
log	O
py	O
fk	O
log	O
py	O
f	O
f	O
j	O
f	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
a	O
y	O
input	O
x	O
y	O
targets	O
pyf	O
function	B
compute	O
covariance	B
matrix	I
from	O
x	O
and	O
locate	O
posterior	O
mode	O
using	O
algorithm	O
compute	O
k	O
w	O
log	O
pyf	O
l	O
choleskyi	O
w	O
log	O
z	O
llw	O
c	O
lw	O
r	O
w	O
kw	O
af	O
log	O
pyf	O
p	O
logdiagl	O
diagk	O
log	O
pyf	O
r	O
w	O
k	O
solve	O
ll	O
b	O
i	O
w	O
kw	O
w	O
kw	O
eq	O
eq	O
o	O
o	O
eq	O
eq	O
for	O
j	O
dim	O
do	O
c	O
k	O
j	O
aca	O
b	O
c	O
log	O
pyf	O
b	O
krb	O
j	O
log	O
z	O
s	O
trrc	O
compute	O
derivative	O
matrix	O
from	O
x	O
and	O
eq	O
end	O
for	O
return	O
log	O
z	O
marginal	B
likelihood	B
log	O
z	O
derivatives	O
algorithm	O
compute	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
and	O
its	O
derivatives	O
w	O
r	O
t	O
the	O
hyperparameters	B
for	O
binary	B
laplace	O
gpc	O
for	O
use	O
by	O
an	O
optimization	O
routine	O
such	O
as	O
conjugate	O
gradient	O
optimization	O
in	O
line	O
algorithm	O
on	O
page	O
is	O
called	O
to	O
locate	O
the	O
posterior	O
mode	O
in	O
line	O
only	O
the	O
diagonal	O
elements	O
of	O
the	O
matrix	O
product	O
should	O
be	O
computed	O
in	O
line	O
the	O
notation	O
j	O
means	O
the	O
partial	O
derivative	O
w	O
r	O
t	O
the	O
j	O
th	O
hyperparameter	O
an	O
actual	O
implementation	O
may	O
also	O
return	O
the	O
value	O
of	O
f	O
to	O
be	O
used	O
as	O
an	O
initial	O
guess	O
for	O
the	O
subsequent	O
call	O
an	O
alternative	O
the	O
zero	O
initialization	O
in	O
line	O
of	O
algorithm	O
details	O
of	O
the	O
implementation	O
the	O
implementation	O
of	O
the	O
log	O
marginal	B
likelihood	B
and	O
its	O
partial	O
derivatives	O
w	O
r	O
t	O
the	O
hyperparameters	B
is	O
shown	O
in	O
algorithm	O
it	O
is	O
advantageous	O
to	O
rewrite	O
the	O
equations	O
from	O
the	O
previous	O
section	O
in	O
terms	O
of	O
well-conditioned	O
symmetric	O
positive	B
definite	I
matrices	O
whose	O
solutions	O
can	O
be	O
obtained	O
by	O
cholesky	O
factorization	O
combining	O
numerical	O
stability	O
with	O
computational	O
speed	O
in	O
detail	O
the	O
matrix	O
of	O
central	O
importance	O
turns	O
out	O
to	O
be	O
r	O
k	O
w	O
w	O
kw	O
where	O
the	O
right	O
hand	O
side	O
is	O
suitable	O
for	O
numerical	O
evaluation	O
as	O
in	O
line	O
of	O
algorithm	O
reusing	O
the	O
cholesky	O
factor	O
l	O
from	O
the	O
newton	O
scheme	O
above	O
remember	O
that	O
w	O
is	O
diagonal	O
so	O
eq	O
does	O
not	O
require	O
any	O
real	O
matrix-bymatrix	O
products	O
rewriting	O
eq	O
is	O
straightforward	O
and	O
for	O
eq	O
we	O
apply	O
the	O
matrix	O
inversion	B
lemma	I
to	O
kw	O
to	O
obtain	O
i	O
kr	O
which	O
is	O
used	O
in	O
the	O
implementation	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
factorization	O
in	O
line	O
which	O
takes	O
operations	O
per	O
iteration	O
of	O
the	O
newton	O
scheme	O
in	O
addition	O
the	O
computation	O
of	O
r	O
in	O
line	O
is	O
also	O
all	O
other	O
computations	O
being	O
at	O
most	O
per	O
hyperparameter	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
for	O
gp	O
classification	B
input	O
x	O
y	O
targets	O
log	O
zep	O
compute	O
k	O
k	O
s	O
l	O
choleskyi	O
s	O
ll	O
s	O
b	O
s	O
r	O
bb	O
s	O
ll	O
s	O
for	O
j	O
dim	O
do	O
k	O
compute	O
covariance	B
matrix	I
from	O
x	O
and	O
run	O
the	O
ep	O
algorithm	O
k	O
s	O
b	O
from	O
under	O
eq	O
r	O
bb	O
s	O
b	O
s	O
solve	O
ll	O
b	O
i	O
s	O
trrc	O
end	O
for	O
c	O
k	O
j	O
j	O
log	O
zep	O
compute	O
derivative	O
matrix	O
from	O
x	O
and	O
eq	O
return	O
log	O
zep	O
marginal	B
likelihood	B
log	O
zep	O
derivatives	O
algorithm	O
compute	O
the	O
log	O
marginal	B
likelihood	B
and	O
its	O
derivatives	O
w	O
r	O
t	O
the	O
hyperparameters	B
for	O
ep	O
binary	B
gp	O
classification	B
for	O
use	O
by	O
an	O
optimization	O
routine	O
such	O
as	O
conjugate	O
gradient	O
optimization	O
s	O
is	O
a	O
diagonal	O
precision	O
matrix	O
with	O
entries	O
sii	O
i	O
in	O
line	O
algorithm	O
on	O
page	O
is	O
called	O
to	O
compute	O
parameters	O
of	O
the	O
ep	O
approximation	O
in	O
line	O
only	O
the	O
diagonal	O
of	O
the	O
matrix	O
product	O
should	O
be	O
computed	O
and	O
the	O
notation	O
j	O
means	O
the	O
partial	O
derivative	O
w	O
r	O
t	O
the	O
j	O
th	O
hyperparameter	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
factorization	O
in	O
line	O
and	O
the	O
solution	O
in	O
line	O
both	O
of	O
which	O
are	O
derivatives	O
of	O
the	O
marginal	B
likelihood	B
for	O
ep	O
optimization	O
of	O
the	O
ep	O
approximation	O
to	O
the	O
marginal	B
likelihood	B
w	O
r	O
t	O
the	O
hyperparameters	B
of	O
the	O
covariance	B
function	B
requires	O
evaluation	O
of	O
the	O
partial	O
derivatives	O
from	O
eq	O
luckily	O
it	O
turns	O
out	O
that	O
implicit	O
terms	O
in	O
the	O
derivatives	O
caused	O
by	O
the	O
solution	O
of	O
ep	O
being	O
a	O
function	B
of	O
the	O
hyperparameters	B
is	O
exactly	O
zero	O
we	O
will	O
not	O
present	O
the	O
proof	O
here	O
see	O
seeger	B
consequently	O
we	O
only	O
have	O
to	O
take	O
account	O
of	O
the	O
explicit	O
dependencies	O
log	O
log	O
zep	O
j	O
j	O
s	O
k	O
j	O
s	O
s	O
k	O
k	O
j	O
j	O
s	O
b	O
in	O
algorithm	O
the	O
derivatives	O
from	O
eq	O
are	O
implemented	O
using	O
log	O
zep	O
tr	O
where	O
b	O
s	O
j	O
b	O
s	O
k	O
cross-validation	B
whereas	O
the	O
loo-cv	O
estimates	O
were	O
easily	O
computed	O
for	O
regression	O
through	O
the	O
use	O
of	O
rank-one	O
updates	O
it	O
is	O
not	O
so	O
obvious	O
how	O
to	O
generalize	O
this	O
to	O
classification	B
opper	O
and	O
winther	O
sec	O
use	O
the	O
cavity	O
distributions	O
of	O
their	O
mean-field	O
approach	O
as	O
loo-cv	O
estimates	O
and	O
one	O
could	O
similarly	O
use	O
the	O
cavity	O
distributions	O
from	O
the	O
closely-related	O
ep	O
algorithm	O
discussed	O
in	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
model	O
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
section	O
although	O
technically	O
the	O
cavity	O
distribution	O
for	O
site	O
i	O
could	O
depend	O
on	O
the	O
label	O
yi	O
the	O
algorithm	O
uses	O
all	O
cases	O
when	O
converging	O
to	O
its	O
fixed	O
point	O
this	O
effect	O
is	O
probably	O
very	O
small	O
and	O
indeed	O
opper	O
and	O
winther	O
sec	O
report	O
very	O
high	O
precision	O
for	O
these	O
loo-cv	O
estimates	O
as	O
an	O
alternative	O
k-fold	O
cv	O
could	O
be	O
used	O
explicitly	O
for	O
some	O
moderate	O
value	O
of	O
k	O
other	O
methods	O
for	O
setting	O
hyperparameters	B
alignment	B
above	O
we	O
have	O
considered	O
setting	O
hyperparameters	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
or	O
cross-validation	B
criteria	O
however	O
some	O
other	O
criteria	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
for	O
example	O
cristianini	O
et	O
al	O
define	O
the	O
alignment	B
between	O
a	O
gram	B
matrix	I
k	O
and	O
the	O
corresponding	O
vector	O
of	O
targets	O
y	O
as	O
yky	O
nkkkf	O
ak	O
y	O
trices	O
ki	O
so	O
that	O
k	O
where	O
kkkf	O
denotes	O
the	O
frobenius	B
norm	O
of	O
the	O
matrix	O
k	O
as	O
defined	O
in	O
eq	O
lanckriet	O
et	O
al	O
show	O
that	O
if	O
k	O
is	O
a	O
convex	O
combination	O
of	O
gram	B
mai	O
iki	O
with	O
i	O
for	O
all	O
i	O
then	O
the	O
optimization	O
of	O
the	O
alignment	B
score	O
w	O
r	O
t	O
the	O
i	O
s	O
can	O
be	O
achieved	O
by	O
solving	O
a	O
semidefinite	O
programming	O
problem	O
example	O
for	O
an	O
example	O
of	O
model	O
selection	O
refer	O
to	O
section	O
although	O
the	O
experiments	O
there	O
were	O
done	O
by	O
exhaustively	O
evaluating	O
the	O
marginal	B
likelihood	B
for	O
a	O
whole	O
grid	O
of	O
hyperparameter	O
values	O
the	O
techniques	O
described	O
in	O
this	O
chapter	O
could	O
be	O
used	O
to	O
locate	O
the	O
same	O
solutions	O
more	O
efficiently	O
exercises	O
the	O
optimization	O
of	O
the	O
marginal	B
likelihood	B
w	O
r	O
t	O
the	O
hyperparameters	B
is	O
generally	O
not	O
possible	O
in	O
closed	O
form	O
consider	O
however	O
the	O
situation	O
where	O
one	O
hyperparameter	O
gives	O
the	O
overall	O
scale	O
of	O
the	O
covariance	B
kyx	O
kyx	O
where	O
ky	O
is	O
the	O
covariance	B
function	B
for	O
the	O
noisy	O
targets	O
including	O
noise	O
contributions	O
and	O
kyx	O
may	O
depend	O
on	O
further	O
hyperparameters	B
show	O
that	O
the	O
marginal	B
likelihood	B
can	O
be	O
optimized	O
w	O
r	O
t	O
in	O
closed	O
form	O
p	O
given	O
by	O
p	O
consider	O
the	O
difference	O
between	O
the	O
log	O
marginal	B
likelihood	B
given	O
by	O
i	O
log	O
pyiyj	O
j	O
i	O
and	O
the	O
loo-cv	O
using	O
log	O
probability	O
which	O
is	O
i	O
log	O
pyiyj	O
j	O
i	O
from	O
the	O
viewpoint	O
of	O
the	O
marginal	B
likelihood	B
the	O
loo-cv	O
conditions	O
too	O
much	O
on	O
the	O
data	O
show	O
that	O
the	O
expected	O
loo-cv	O
loss	B
is	O
greater	O
than	O
the	O
expected	O
marginal	B
likelihood	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
in	O
this	O
chapter	O
we	O
discuss	O
a	O
number	O
of	O
concepts	O
and	O
models	O
that	O
are	O
related	O
to	O
gaussian	B
process	I
prediction	O
in	O
section	O
we	O
cover	O
reproducing	O
kernel	B
hilbert	O
spaces	O
which	O
define	O
a	O
hilbert	O
space	O
of	O
sufficiently-smooth	O
functions	O
corresponding	O
to	O
a	O
given	O
positive	B
semidefinite	I
kernel	B
k	O
as	O
we	O
discussed	O
in	O
chapter	O
there	O
are	O
many	O
functions	O
that	O
are	O
consistent	O
with	O
a	O
given	O
dataset	O
d	O
we	O
have	O
seen	O
how	O
the	O
gp	O
approach	O
puts	O
a	O
prior	O
over	O
functions	O
in	O
order	O
to	O
deal	O
with	O
this	O
issue	O
a	O
related	O
viewpoint	O
is	O
provided	O
by	O
regularization	B
theory	O
in	O
section	O
where	O
one	O
seeks	O
a	O
trade-off	O
between	O
data-fit	O
and	O
the	O
rkhs	O
norm	O
of	O
function	B
this	O
is	O
closely	O
related	O
to	O
the	O
map	B
estimator	O
in	O
gp	O
prediction	O
and	O
thus	O
omits	O
uncertainty	O
in	O
predictions	O
and	O
also	O
the	O
marginal	B
likelihood	B
in	O
section	O
we	O
discuss	O
splines	B
a	O
special	O
case	O
of	O
regularization	B
which	O
is	O
obtained	O
when	O
the	O
rkhs	O
is	O
defined	O
in	O
terms	O
of	O
differential	O
operators	O
of	O
a	O
given	O
order	O
there	O
are	O
a	O
number	O
of	O
other	O
families	O
of	O
kernel	B
machines	O
that	O
are	O
related	O
to	O
gaussian	B
process	I
prediction	O
in	O
section	O
we	O
describe	O
support	B
vector	I
machines	O
in	O
section	O
we	O
discuss	O
least-squares	B
classification	B
and	O
in	O
section	O
we	O
cover	O
relevance	O
vector	O
machines	O
reproducing	O
kernel	B
hilbert	O
spaces	O
here	O
we	O
present	O
a	O
brief	O
introduction	O
to	O
reproducing	O
kernel	B
hilbert	O
spaces	O
the	O
theory	O
was	O
developed	O
by	O
aronszajn	O
a	O
more	O
recent	O
treatise	O
is	O
saitoh	O
information	O
can	O
also	O
be	O
found	O
in	O
wahba	O
sch	O
olkopf	O
and	O
smola	O
and	O
wegman	O
the	O
collection	O
of	O
papers	O
edited	O
by	O
weinert	O
provides	O
an	O
overview	O
of	O
the	O
uses	O
of	O
rkhss	O
in	O
statistical	O
signal	O
processing	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
we	O
start	O
with	O
a	O
formal	O
definition	O
of	O
a	O
rkhs	O
and	O
then	O
describe	O
two	O
specific	O
bases	O
for	O
a	O
rkhs	O
firstly	O
through	O
mercer	O
s	O
theorem	O
and	O
the	O
eigenfunctions	O
of	O
k	O
and	O
secondly	O
through	O
the	O
reproducing	O
kernel	B
map	B
definition	O
kernel	B
hilbert	O
space	O
let	O
h	O
be	O
a	O
hilbert	O
space	O
of	O
real	O
functions	O
f	O
defined	O
on	O
an	O
index	B
set	B
x	O
then	O
h	O
is	O
called	O
a	O
reproducing	B
kernel	B
hilbert	I
space	I
endowed	O
with	O
an	O
inner	O
product	O
h	O
ih	O
norm	O
kfkh	O
phf	O
fih	O
if	O
there	O
exists	O
a	O
function	B
k	O
x	O
x	O
r	O
with	O
the	O
following	O
properties	O
for	O
every	O
x	O
kx	O
as	O
a	O
function	B
of	O
belongs	O
to	O
h	O
and	O
k	O
has	O
the	O
reproducing	O
property	O
hf	O
k	O
xih	O
fx	O
see	O
e	O
g	O
sch	O
olkopf	O
and	O
smola	O
and	O
wegman	O
note	O
also	O
that	O
as	O
kx	O
and	O
are	O
in	O
h	O
we	O
have	O
that	O
hkx	O
kx	O
reproducing	O
property	O
the	O
rkhs	O
uniquely	O
determines	O
k	O
and	O
vice	O
versa	O
as	O
stated	O
in	O
the	O
following	O
theorem	O
theorem	O
theorem	O
aronszajn	O
let	O
x	O
be	O
an	O
index	B
set	B
then	O
for	O
every	O
positive	B
definite	I
function	B
k	O
on	O
x	O
x	O
there	O
exists	O
a	O
unique	O
rkhs	O
and	O
vice	O
versa	O
the	O
hilbert	O
space	O
has	O
the	O
dot	B
product	I
hf	O
fxgxdx	O
function	B
is	O
the	O
representer	O
of	O
evaluation	O
i	O
e	O
fx	O
kernels	O
contains	O
many	O
non-smooth	O
functions	O
in	O
is	O
not	O
a	O
rkhs	O
the	O
delta	O
are	O
the	O
analogues	O
of	O
delta	O
functions	O
within	O
the	O
smoother	O
rkhs	O
note	O
that	O
the	O
delta	O
function	B
is	O
not	O
itself	O
in	O
in	O
contrast	O
for	O
a	O
rkhs	O
the	O
kernel	B
k	O
is	O
the	O
representer	O
of	O
evaluation	O
and	O
is	O
itself	O
in	O
the	O
rkhs	O
inner	O
product	O
hf	O
gih	O
the	O
above	O
description	O
is	O
perhaps	O
rather	O
abstract	O
for	O
our	O
purposes	O
the	O
key	O
intuition	O
behind	O
the	O
rkhs	O
formalism	O
is	O
that	O
the	O
squared	B
norm	O
can	O
be	O
thought	O
of	O
as	O
a	O
generalization	B
to	O
functions	O
of	O
the	O
n-dimensional	O
quadratic	B
form	I
fk	O
we	O
have	O
seen	O
in	O
earlier	O
chapters	O
consider	O
a	O
real	O
positive	B
semidefinite	I
kernel	B
kx	O
with	O
an	O
eigenfunction	B
i	O
ix	O
relative	O
to	O
a	O
measure	B
recall	O
from	O
mercer	O
s	O
theorem	O
that	O
the	O
eigenfunctions	O
are	O
orthonormal	O
w	O
r	O
t	O
i	O
e	O
we	O
have	O
expansion	O
kx	O
r	O
ix	O
jx	O
d	O
ij	O
we	O
now	O
consider	O
a	O
hilbert	O
space	O
comprised	O
of	O
linear	B
combinations	O
of	O
the	O
eigenfunctions	O
i	O
e	O
fx	O
functions	O
fx	O
and	O
gx	O
i	O
i	O
we	O
assert	O
that	O
the	O
inner	O
product	O
hf	O
gih	O
in	O
the	O
hilbert	O
space	O
between	O
ix	O
withpn	O
ix	O
is	O
defined	O
as	O
pn	O
thus	O
this	O
hilbert	O
space	O
is	O
equipped	O
with	O
a	O
norm	O
kfkh	O
where	O
hf	O
fih	O
i	O
i	O
note	O
that	O
for	O
kfkh	O
to	O
be	O
finite	O
the	O
sequence	O
of	O
coefficients	O
must	O
decay	O
quickly	O
effectively	O
this	O
imposes	O
a	O
smoothness	O
condition	O
on	O
the	O
space	O
nx	O
figi	O
i	O
hf	O
gih	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
reproducing	O
kernel	B
hilbert	O
spaces	O
we	O
now	O
need	O
to	O
show	O
that	O
this	O
hilbert	O
space	O
is	O
the	O
rkhs	O
corresponding	O
to	O
the	O
kernel	B
k	O
i	O
e	O
that	O
it	O
has	O
the	O
reproducing	O
property	O
this	O
is	O
easily	O
achieved	O
as	O
hf	O
k	O
xih	O
fx	O
similarly	O
nx	O
fi	O
i	O
ix	O
i	O
nx	O
i	O
ix	O
i	O
hkx	O
kx	O
i	O
notice	O
also	O
that	O
kx	O
is	O
in	O
the	O
rkhs	O
as	O
it	O
has	O
normpn	O
linear	B
combinations	O
of	O
the	O
eigenfunctions	O
with	O
the	O
restrictionpn	O
i	O
i	O
kx	O
x	O
we	O
have	O
now	O
demonstrated	O
that	O
the	O
hilbert	O
space	O
comprised	O
of	O
i	O
i	O
fulfils	O
the	O
two	O
conditions	O
given	O
in	O
definition	O
as	O
there	O
is	O
a	O
unique	O
rkhs	O
associated	O
with	O
k	O
this	O
hilbert	O
space	O
must	O
be	O
that	O
rkhs	O
the	O
advantage	O
of	O
the	O
abstract	O
formulation	O
of	O
the	O
rkhs	O
is	O
that	O
the	O
eigenbasis	O
will	O
change	O
as	O
we	O
use	O
different	O
measures	O
in	O
mercer	O
s	O
theorem	O
however	O
the	O
rkhs	O
norm	O
is	O
in	O
fact	O
solely	O
a	O
property	O
of	O
the	O
kernel	B
and	O
is	O
invariant	O
under	O
this	O
change	O
of	O
measure	B
this	O
can	O
be	O
seen	O
from	O
the	O
fact	O
that	O
the	O
proof	O
of	O
the	O
rkhs	O
properties	O
above	O
is	O
not	O
dependent	O
on	O
the	O
measure	B
see	O
also	O
kailath	O
sec	O
ii	O
b	O
a	O
finite-dimensional	O
example	O
of	O
this	O
measure	B
invariance	O
is	O
explored	O
in	O
exercise	O
notice	O
the	O
analogy	O
between	O
the	O
rkhs	O
norm	O
hf	O
fih	O
if	O
we	O
sample	O
the	O
coefficients	O
fi	O
in	O
the	O
eigenexpansion	O
fx	O
i	O
i	O
and	O
the	O
quadratic	B
form	I
fk	O
if	O
we	O
express	O
k	O
and	O
f	O
in	O
terms	O
of	O
the	O
eigenvectors	O
of	O
k	O
we	O
obtain	O
exactly	O
the	O
same	O
form	O
the	O
sum	O
has	O
only	O
n	O
terms	O
if	O
f	O
has	O
length	O
n	O
ix	O
from	O
n	O
i	O
then	O
nx	O
ef	O
i	O
i	O
nx	O
thus	O
if	O
n	O
is	O
infinite	O
the	O
sample	O
functions	O
are	O
not	O
in	O
h	O
probability	O
as	O
the	O
expected	O
value	O
of	O
the	O
rkhs	O
norm	O
is	O
infinite	O
see	O
wahba	O
p	O
and	O
kailath	O
sec	O
ii	O
b	O
for	O
further	O
details	O
however	O
note	O
that	O
although	O
sample	O
functions	O
of	O
this	O
gaussian	B
process	I
are	O
not	O
in	O
h	O
the	O
posterior	O
mean	O
after	O
observing	O
some	O
data	O
will	O
lie	O
in	O
the	O
rkhs	O
due	O
to	O
the	O
smoothing	O
properties	O
of	O
averaging	O
another	O
view	O
of	O
the	O
rkhs	O
can	O
be	O
obtained	O
from	O
the	O
reproducing	O
kernel	B
map	B
construction	O
we	O
consider	O
the	O
space	O
of	O
functions	O
f	O
defined	O
as	O
n	O
fx	O
nx	O
ikx	O
xi	O
n	O
n	O
xi	O
x	O
i	O
ro	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regularizer	O
ridge	B
regression	O
representer	B
theorem	I
now	O
let	O
gx	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
jkx	O
j	O
then	O
we	O
define	O
the	O
inner	O
product	O
hf	O
gih	O
i	O
jkxi	O
j	O
nx	O
nx	O
clearly	O
condition	O
of	O
definition	O
is	O
fulfilled	O
under	O
the	O
reproducing	O
kernel	B
map	B
construction	O
we	O
can	O
also	O
demonstrate	O
the	O
reproducing	O
property	O
as	O
hk	O
x	O
f	O
ikx	O
xi	O
fx	O
regularization	B
the	O
problem	O
of	O
inferring	O
an	O
underlying	O
function	B
fx	O
from	O
a	O
finite	O
possibly	O
noisy	O
dataset	O
without	O
any	O
additional	O
assumptions	O
is	O
clearly	O
ill	O
posed	O
for	O
example	O
in	O
the	O
noise-free	O
case	O
any	O
function	B
that	O
passes	O
through	O
the	O
given	O
data	O
points	O
is	O
acceptable	O
under	O
a	O
bayesian	O
approach	O
our	O
assumptions	O
are	O
characterized	O
by	O
a	O
prior	O
over	O
functions	O
and	O
given	O
some	O
data	O
we	O
obtain	O
a	O
posterior	O
over	O
functions	O
the	O
problem	O
of	O
bringing	O
prior	O
assumptions	O
to	O
bear	O
has	O
also	O
been	O
addressed	O
under	O
the	O
regularization	B
viewpoint	O
where	O
these	O
assumptions	O
are	O
encoded	O
in	O
terms	O
of	O
the	O
smoothness	O
of	O
f	O
we	O
consider	O
the	O
functional	B
jf	O
qy	O
f	O
where	O
y	O
is	O
the	O
vector	O
of	O
targets	O
we	O
are	O
predicting	O
and	O
f	O
fxn	O
is	O
the	O
corresponding	O
vector	O
of	O
function	B
values	O
and	O
is	O
a	O
scaling	O
parameter	O
that	O
trades	O
off	O
the	O
two	O
terms	O
the	O
first	O
term	O
is	O
called	O
the	O
regularizer	O
and	O
represents	O
smoothness	O
assumptions	O
on	O
f	O
as	O
encoded	O
by	O
a	O
suitable	O
rkhs	O
and	O
the	O
second	O
term	O
is	O
a	O
data-fit	O
term	O
assessing	O
the	O
quality	O
of	O
the	O
prediction	O
fxi	O
for	O
the	O
observed	O
datum	O
yi	O
e	O
g	O
the	O
negative	O
log	O
likelihood	B
indeed	O
recalling	O
that	O
pn	O
ridge	B
regression	O
in	O
section	O
can	O
be	O
seen	O
as	O
a	O
particular	O
case	O
i	O
i	O
where	O
fi	O
is	O
the	O
of	O
regularization	B
coefficient	O
of	O
eigenfunction	B
ix	O
we	O
see	O
that	O
we	O
are	O
penalizing	O
the	O
weighted	O
squared	B
coefficients	O
this	O
is	O
taking	O
place	O
in	O
feature	B
space	I
rather	O
than	O
simply	O
in	O
input	O
space	O
as	O
per	O
the	O
standard	O
formulation	O
of	O
ridge	B
regression	O
eq	O
so	O
it	O
corresponds	O
to	O
kernel	B
ridge	B
regression	O
the	O
representer	B
theorem	I
shows	O
that	O
each	O
minimizer	O
f	O
h	O
of	O
jf	O
has	O
the	O
ikx	O
the	O
representer	B
theorem	I
was	O
first	O
stated	O
by	O
kimeldorf	O
and	O
wahba	O
for	O
the	O
case	O
of	O
squared	B
o	O
sullivan	O
et	O
al	O
showed	O
that	O
the	O
representer	B
theorem	I
could	O
be	O
extended	O
to	O
likelihood	B
form	O
fx	O
pn	O
the	O
rkhs	O
contains	O
a	O
null	B
space	I
of	O
unpenalized	O
functions	O
then	O
the	O
given	O
form	O
is	O
correct	O
modulo	O
a	O
term	O
that	O
lies	O
in	O
this	O
null	B
space	I
this	O
is	O
explained	O
further	O
in	O
section	O
proved	O
the	O
representer	B
theorem	I
for	O
the	O
special	O
case	O
of	O
cubic	O
splines	B
and	O
squared	B
error	O
this	O
was	O
result	O
extended	O
to	O
general	O
rkhss	O
in	O
kimeldorf	O
and	O
wahba	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regularization	B
functions	O
arising	O
from	O
generalized	B
linear	B
models	O
the	O
representer	B
theorem	I
can	O
be	O
generalized	B
still	O
further	O
see	O
e	O
g	O
sch	O
olkopf	O
and	O
smola	O
sec	O
if	O
the	O
data-fit	O
term	O
is	O
convex	O
section	O
then	O
there	O
will	O
be	O
a	O
unique	O
minimizer	O
f	O
of	O
jf	O
for	O
gaussian	B
process	I
prediction	O
with	O
likelihoods	O
that	O
involve	O
the	O
values	O
of	O
f	O
at	O
the	O
n	O
training	O
points	O
only	O
that	O
qy	O
f	O
is	O
the	O
negative	O
log	O
likelihood	B
up	O
to	O
some	O
terms	O
not	O
involving	O
f	O
the	O
analogue	O
of	O
the	O
representer	B
theorem	I
is	O
obvious	O
this	O
is	O
because	O
the	O
predictive	B
distribution	O
of	O
fx	O
f	O
at	O
test	O
point	O
x	O
given	O
the	O
data	O
y	O
is	O
pf	O
pf	O
df	O
as	O
derived	O
in	O
eq	O
we	O
thus	O
ef	O
due	O
to	O
the	O
formulae	O
for	O
the	O
conditional	B
distribution	O
of	O
a	O
multivariate	O
gaussian	O
ikx	O
xi	O
where	O
k	O
ef	O
kx	O
have	O
the	O
regularization	B
approach	O
has	O
a	O
long	O
tradition	O
in	O
inverse	O
problems	O
dating	O
back	O
at	O
least	O
as	O
far	O
as	O
tikhonov	O
see	O
also	O
tikhonov	O
and	O
arsenin	O
for	O
the	O
application	O
of	O
this	O
approach	O
in	O
the	O
machine	O
learning	B
literature	O
see	O
e	O
g	O
poggio	O
and	O
girosi	O
in	O
section	O
we	O
consider	O
rkhss	O
defined	O
in	O
terms	O
of	O
differential	O
operators	O
in	O
section	O
we	O
demonstrate	O
how	O
to	O
solve	O
the	O
regularization	B
problem	O
in	O
the	O
specific	O
case	O
of	O
squared	B
error	O
and	O
in	O
section	O
we	O
compare	O
and	O
contrast	O
the	O
regularization	B
approach	O
with	O
the	O
gaussian	B
process	I
viewpoint	O
null	B
space	I
regularization	B
defined	O
by	O
differential	O
operators	O
for	O
x	O
rd	O
define	O
z	O
x	O
mfx	O
xjd	O
d	O
z	O
for	O
example	O
for	O
m	O
and	O
d	O
dx	O
now	O
set	B
kp	O
with	O
non-negative	O
coefficients	O
am	O
notice	O
that	O
kp	O
is	O
translation	O
and	O
rotation	O
invariant	O
in	O
this	O
section	O
we	O
assume	O
that	O
if	O
this	O
is	O
not	O
the	O
case	O
and	O
ak	O
is	O
the	O
first	O
non-zero	O
coefficient	O
then	O
there	O
is	O
a	O
null	B
space	I
of	O
functions	O
that	O
are	O
unpenalized	O
for	O
example	O
if	O
k	O
then	O
constant	O
and	O
linear	B
functions	O
are	O
in	O
the	O
null	B
space	I
this	O
case	O
is	O
dealt	O
with	O
in	O
section	O
kp	O
penalizes	O
f	O
in	O
terms	O
of	O
the	O
variability	O
of	O
its	O
function	B
values	O
and	O
derivatives	O
up	O
to	O
order	O
m	O
how	O
does	O
this	O
correspond	O
to	O
the	O
rkhs	O
formulation	O
of	O
section	O
the	O
key	O
is	O
to	O
recognize	O
that	O
the	O
complex	O
exponentials	O
is	O
x	O
are	O
eigenfunctions	O
of	O
the	O
differential	O
operator	B
if	O
x	O
rd	O
in	O
this	O
case	O
kp	O
sm	O
z	O
mx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
where	O
fs	O
is	O
the	O
fourier	B
transform	I
of	O
fx	O
comparing	O
eq	O
with	O
eq	O
we	O
see	O
that	O
the	O
kernel	B
has	O
the	O
power	O
spectrum	O
ss	O
pm	O
sm	O
pm	O
sm	O
is	O
x	O
z	O
and	O
thus	O
by	O
fourier	O
inversion	O
we	O
obtain	O
the	O
stationary	O
kernel	B
kx	O
ds	O
a	O
slightly	O
different	O
approach	O
to	O
obtaining	O
the	O
kernel	B
is	O
to	O
use	O
calculus	O
of	O
variations	O
to	O
minimize	O
jf	O
with	O
respect	O
to	O
f	O
the	O
euler-lagrange	O
equation	O
leads	O
to	O
nx	O
fx	O
igx	O
xi	O
with	O
mx	O
green	O
s	O
function	B
kernel	B
differential	O
operatorpm	O
where	O
gx	O
is	O
known	O
as	O
a	O
green	O
s	O
function	B
notice	O
that	O
the	O
green	O
s	O
function	B
also	O
depends	O
on	O
the	O
boundary	O
conditions	O
for	O
the	O
case	O
of	O
x	O
rd	O
by	O
fourier	O
transforming	O
eq	O
we	O
recognize	O
that	O
g	O
is	O
in	O
fact	O
the	O
kernel	B
k	O
the	O
and	O
the	O
integral	O
operator	B
k	O
are	O
in	O
fact	O
inverses	O
as	O
shown	O
by	O
eq	O
see	O
poggio	O
and	O
girosi	O
for	O
further	O
details	O
arfken	O
provides	O
an	O
introduction	O
to	O
calculus	O
of	O
variations	O
and	O
green	O
s	O
functions	O
rkhss	O
for	O
regularizers	O
defined	O
by	O
differential	O
operators	O
are	O
sobolev	O
spaces	O
see	O
e	O
g	O
adams	O
for	O
further	O
details	O
on	O
sobolev	O
spaces	O
we	O
now	O
give	O
two	O
specific	O
examples	O
of	O
kernels	O
derived	O
from	O
differential	O
oper	O
ators	O
example	O
set	B
and	O
am	O
for	O
m	O
in	O
d	O
using	O
the	O
fourier	O
pair	O
e	O
we	O
obtain	O
kx	O
e	O
note	O
that	O
this	O
is	O
the	O
covariance	B
function	B
of	O
the	O
ornstein-uhlenbeck	B
process	O
see	O
section	O
example	O
by	O
setting	O
am	O
we	O
obtain	O
and	O
using	O
the	O
power	O
series	O
ey	O
ykk	O
kx	O
is	O
exp	O
sds	O
exp	O
z	O
as	O
shown	O
by	O
yuille	O
and	O
grzywacz	O
this	O
is	O
the	O
squared	B
exponential	B
covariance	B
function	B
that	O
we	O
have	O
seen	O
earlier	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
regularization	B
obtaining	O
the	O
regularized	O
solution	O
the	O
representer	B
theorem	I
tells	O
us	O
the	O
general	O
form	O
of	O
the	O
solution	O
to	O
eq	O
we	O
now	O
consider	O
a	O
specific	O
functional	B
jf	O
n	O
nx	O
which	O
uses	O
a	O
squared	B
error	O
data-fit	O
term	O
to	O
the	O
negative	O
log	O
likelihood	B
of	O
a	O
gaussian	O
noise	B
model	I
with	O
variance	O
n	O
substituting	O
fx	O
pn	O
ikx	O
xi	O
and	O
using	O
hk	O
xi	O
k	O
xjih	O
kxi	O
xj	O
we	O
obtain	O
j	O
k	O
n	O
k	O
n	O
n	O
yk	O
yy	O
n	O
minimizing	O
j	O
by	O
differentiating	O
w	O
r	O
t	O
the	O
vector	O
of	O
coefficients	O
we	O
obtain	O
ni	O
so	O
that	O
the	O
prediction	O
for	O
a	O
test	O
point	O
x	O
is	O
fx	O
kx	O
ni	O
this	O
should	O
look	O
very	O
familiar	O
it	O
is	O
exactly	O
the	O
form	O
of	O
the	O
predictive	B
mean	O
obtained	O
in	O
eq	O
in	O
the	O
next	O
section	O
we	O
compare	O
and	O
contrast	O
the	O
regularization	B
and	O
gp	O
views	O
of	O
the	O
problem	O
the	O
solution	O
fx	O
regularization	B
network	I
in	O
poggio	O
and	O
girosi	O
ikx	O
xi	O
that	O
minimizes	O
eq	O
was	O
called	O
a	O
regularization	B
network	I
the	O
relationship	O
of	O
the	O
regularization	B
view	O
to	O
gaus	O
sian	O
process	O
prediction	O
the	O
regularization	B
method	O
returns	O
f	O
argminf	O
jf	O
for	O
a	O
gaussian	B
process	I
predictor	O
we	O
obtain	O
a	O
posterior	O
distribution	O
over	O
functions	O
can	O
we	O
make	O
a	O
connection	O
between	O
these	O
two	O
views	O
in	O
fact	O
we	O
shall	O
see	O
in	O
this	O
section	O
that	O
f	O
can	O
be	O
viewed	O
as	O
the	O
maximum	O
a	O
posteriori	O
function	B
under	O
the	O
posterior	O
following	O
szeliski	O
and	O
poggio	O
and	O
girosi	O
we	O
consider	O
exp	O
jf	O
kp	O
exp	O
qy	O
f	O
the	O
first	O
term	O
on	O
the	O
rhs	O
is	O
a	O
gaussian	B
process	I
prior	O
on	O
f	O
and	O
the	O
second	O
is	O
proportional	O
to	O
the	O
likelihood	B
as	O
f	O
is	O
the	O
minimizer	O
of	O
jf	O
it	O
is	O
the	O
map	B
function	B
f	O
thus	O
we	O
obtain	O
kp	O
pm	O
to	O
get	O
some	O
intuition	O
for	O
the	O
gaussian	B
process	I
prior	O
imagine	O
fx	O
being	O
represented	O
on	O
a	O
grid	O
in	O
x-space	O
so	O
that	O
f	O
is	O
now	O
an	O
dimensional	O
vector	O
mdmf	O
where	O
dm	O
is	O
an	O
appropriate	O
finite-difference	O
approximation	O
of	O
the	O
differential	O
operator	B
om	O
observe	O
that	O
this	O
prior	O
term	O
is	O
a	O
quadratic	B
form	I
in	O
f	O
amdmfdmf	O
fp	O
m	O
amd	O
to	O
go	O
into	O
more	O
detail	O
concerning	O
the	O
map	B
relationship	O
we	O
consider	O
three	O
cases	O
when	O
qy	O
f	O
is	O
quadratic	O
to	O
a	O
gaussian	O
likelihood	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
when	O
qy	O
f	O
is	O
not	O
quadratic	O
but	O
convex	O
and	O
when	O
qy	O
f	O
is	O
not	O
convex	O
in	O
case	O
we	O
have	O
seen	O
in	O
chapter	O
that	O
the	O
posterior	O
mean	B
function	B
can	O
be	O
obtained	O
exactly	O
and	O
the	O
posterior	O
is	O
gaussian	O
as	O
the	O
mean	O
of	O
a	O
gaussian	O
is	O
also	O
its	O
mode	O
this	O
is	O
the	O
map	B
solution	O
the	O
correspondence	O
between	O
the	O
gp	O
posterior	O
mean	O
and	O
the	O
solution	O
of	O
the	O
regularization	B
problem	O
f	O
was	O
made	O
in	O
kimeldorf	O
and	O
wahba	O
in	O
case	O
we	O
have	O
seen	O
in	O
chapter	O
for	O
classification	B
problems	O
using	O
the	O
logistic	B
probit	B
or	O
softmax	B
response	O
functions	O
that	O
qy	O
f	O
is	O
convex	O
here	O
the	O
map	B
solution	O
can	O
be	O
found	O
by	O
finding	O
f	O
map	B
solution	O
to	O
the	O
n-dimensional	O
problem	O
defined	O
at	O
the	O
training	O
points	O
and	O
then	O
extending	O
it	O
to	O
other	O
x-values	O
through	O
the	O
posterior	O
mean	O
conditioned	O
on	O
f	O
in	O
case	O
there	O
will	O
be	O
more	O
than	O
one	O
local	O
minimum	O
of	O
jf	O
under	O
the	O
regularization	B
approach	O
one	O
could	O
check	O
these	O
minima	O
to	O
find	O
the	O
deepest	O
one	O
however	O
in	O
this	O
case	O
the	O
argument	O
for	O
map	B
is	O
rather	O
weak	O
if	O
there	O
are	O
multiple	O
optima	O
of	O
similar	O
depth	O
and	O
suggests	O
the	O
need	O
for	O
a	O
fully	O
bayesian	O
treatment	O
while	O
the	O
regularization	B
solution	O
gives	O
a	O
part	O
of	O
the	O
gaussian	B
process	I
solu	O
tion	O
there	O
are	O
the	O
following	O
limitations	O
it	O
does	O
not	O
characterize	O
the	O
uncertainty	O
in	O
the	O
predictions	O
nor	O
does	O
it	O
handle	O
well	O
multimodality	O
in	O
the	O
posterior	O
the	O
analysis	O
is	O
focussed	O
at	O
approximating	O
the	O
first	O
level	O
of	O
bayesian	O
inference	O
concerning	O
predictions	O
for	O
f	O
it	O
is	O
not	O
usually	O
extended	O
to	O
the	O
next	O
level	O
e	O
g	O
to	O
the	O
computation	O
of	O
the	O
marginal	B
likelihood	B
the	O
marginal	B
likelihood	B
is	O
very	O
useful	O
for	O
setting	O
any	O
parameters	O
of	O
the	O
covariance	B
function	B
and	O
for	O
model	O
comparison	O
chapter	O
in	O
addition	O
we	O
find	O
the	O
specification	O
of	O
smoothness	O
via	O
the	O
penalties	O
on	O
derivatives	O
to	O
be	O
not	O
very	O
intuitive	O
the	O
regularization	B
viewpoint	O
can	O
be	O
thought	O
of	O
as	O
directly	O
specifying	O
the	O
inverse	O
covariance	B
rather	O
than	O
the	O
covariance	B
as	O
marginalization	O
is	O
achieved	O
for	O
a	O
gaussian	B
distribution	I
directly	O
from	O
the	O
covariance	B
not	O
the	O
inverse	O
covariance	B
it	O
seems	O
more	O
natural	O
to	O
us	O
to	O
specify	O
the	O
covariance	B
function	B
also	O
while	O
non-stationary	O
covariance	B
functions	O
can	O
be	O
obtained	O
from	O
the	O
regularization	B
viewpoint	O
e	O
g	O
by	O
replacing	O
the	O
lebesgue	O
measure	B
in	O
eq	O
with	O
a	O
non-uniform	O
measure	B
calculation	O
of	O
the	O
corresponding	O
covariance	B
function	B
can	O
then	O
be	O
very	O
difficult	O
spline	O
models	O
in	O
section	O
we	O
discussed	O
regularizers	O
which	O
had	O
in	O
eq	O
we	O
now	O
consider	O
the	O
case	O
when	O
in	O
particular	O
we	O
consider	O
the	O
regularizer	O
to	O
be	O
of	O
the	O
form	O
as	O
defined	O
in	O
eq	O
in	O
this	O
case	O
polynomials	O
of	O
degree	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
spline	O
models	O
up	O
to	O
m	O
are	O
in	O
the	O
null	B
space	I
of	O
the	O
regularization	B
operator	B
in	O
that	O
they	O
are	O
not	O
penalized	B
at	O
all	O
in	O
the	O
case	O
that	O
x	O
rd	O
we	O
can	O
again	O
use	O
fourier	O
techniques	O
to	O
obtain	O
the	O
green	O
s	O
function	B
g	O
corresponding	O
to	O
the	O
euler-lagrange	O
equation	O
the	O
result	O
as	O
shown	O
by	O
duchon	O
and	O
meinguet	O
is	O
d	O
log	O
cmdx	O
d	O
gx	O
if	O
d	O
and	O
d	O
even	O
otherwise	O
where	O
cmd	O
is	O
a	O
constant	O
p	O
gives	O
the	O
explicit	O
form	O
note	O
that	O
the	O
constraint	O
d	O
has	O
to	O
be	O
imposed	O
to	O
avoid	O
having	O
a	O
green	O
s	O
function	B
that	O
is	O
singular	O
at	O
the	O
origin	O
explicit	O
calculation	O
of	O
the	O
green	O
s	O
function	B
for	O
other	O
domains	O
x	O
is	O
sometimes	O
possible	O
for	O
example	O
see	O
wahba	O
sec	O
for	O
splines	B
on	O
the	O
sphere	O
because	O
of	O
the	O
null	B
space	I
a	O
minimizer	O
of	O
the	O
regularization	B
functional	B
has	O
the	O
form	O
nx	O
fx	O
kx	O
igx	O
xi	O
jhjx	O
where	O
hkx	O
are	O
polynomials	O
that	O
span	O
the	O
null	B
space	I
the	O
exact	O
values	O
of	O
the	O
coefficients	O
and	O
for	O
a	O
specific	O
problem	O
can	O
be	O
obtained	O
in	O
an	O
analogous	O
manner	O
to	O
the	O
derivation	O
in	O
section	O
in	O
fact	O
the	O
solution	O
is	O
equivalent	B
to	O
that	O
given	O
in	O
eq	O
to	O
gain	O
some	O
more	O
insight	O
into	O
the	O
form	O
of	O
the	O
green	O
s	O
function	B
we	O
consider	O
the	O
equation	O
in	O
fourier	O
space	O
leading	O
to	O
gs	O
s	O
m	O
gs	O
plays	O
a	O
r	O
ole	O
like	O
that	O
of	O
the	O
power	O
spectrum	O
in	O
eq	O
but	O
notice	O
thatr	O
gsds	O
is	O
infinite	O
which	O
would	O
imply	O
that	O
the	O
corresponding	O
process	O
has	O
infinite	O
variance	O
the	O
problem	O
is	O
of	O
course	O
that	O
the	O
null	B
space	I
is	O
unpenalized	O
for	O
example	O
any	O
arbitrary	O
constant	O
function	B
can	O
be	O
added	O
to	O
f	O
without	O
changing	O
the	O
regularizer	O
tions	O
of	O
fx	O
of	O
the	O
form	O
gx	O
because	O
of	O
the	O
null	B
space	I
we	O
have	O
seen	O
that	O
one	O
cannot	O
obtain	O
a	O
simple	O
connection	O
between	O
the	O
spline	O
solution	O
and	O
a	O
corresponding	O
gaussian	B
process	I
problem	O
however	O
by	O
introducing	O
the	O
notion	O
of	O
an	O
intrinsic	B
random	I
function	B
one	O
can	O
define	O
a	O
generalized	B
covariance	B
see	O
cressie	O
sec	O
and	O
stein	O
section	O
for	O
details	O
the	O
basic	O
idea	O
is	O
to	O
consider	O
linear	B
aifx	O
i	O
for	O
which	O
gx	O
is	O
second-order	O
stationary	O
and	O
where	O
hj	O
ka	O
for	O
j	O
k	O
a	O
careful	O
description	O
of	O
the	O
equivalence	O
of	O
spline	O
and	O
irf	O
prediction	O
is	O
given	O
in	O
kent	O
and	O
mardia	O
the	O
power-law	O
form	O
of	O
gs	O
s	O
m	O
means	O
that	O
there	O
is	O
no	O
characteristic	O
length-scale	B
for	O
random	O
functions	O
drawn	O
from	O
this	O
prior	O
thus	O
we	O
obtain	O
the	O
self-similar	O
property	O
characteristic	O
of	O
fractals	O
for	O
further	O
details	O
see	O
szeliski	O
and	O
mandelbrot	O
some	O
authors	O
argue	O
that	O
the	O
lack	O
of	O
a	O
characteristic	O
length-scale	B
is	O
appealing	O
this	O
may	O
sometimes	O
be	O
the	O
case	O
but	O
if	O
we	O
believe	O
there	O
is	O
an	O
appropriate	O
length-scale	B
set	B
of	O
length-scales	O
irf	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
for	O
a	O
given	O
problem	O
but	O
this	O
is	O
unknown	O
in	O
advance	O
we	O
would	O
argue	O
that	O
a	O
hierarchical	O
bayesian	O
formulation	O
of	O
the	O
problem	O
described	O
in	O
chapter	O
would	O
be	O
more	O
appropriate	O
splines	B
were	O
originally	O
introduced	O
for	O
one-dimensional	O
interpolation	O
and	O
smoothing	O
problems	O
and	O
then	O
generalized	B
to	O
the	O
multivariate	O
setting	O
schoenberg	O
considered	O
the	O
problem	O
of	O
finding	O
the	O
function	B
that	O
minimizes	O
spline	O
interpolation	O
natural	O
polynomial	B
spline	O
smoothing	O
spline	O
z	O
b	O
a	O
dx	O
where	O
f	O
denotes	O
the	O
m	O
th	O
derivative	O
of	O
f	O
subject	O
to	O
the	O
interpolation	O
constraints	O
fxi	O
fi	O
xi	O
b	O
for	O
i	O
n	O
and	O
for	O
f	O
in	O
an	O
appropriate	O
sobolev	O
space	O
he	O
showed	O
that	O
the	O
solution	O
is	O
the	O
natural	O
polynomial	B
spline	O
which	O
is	O
a	O
piecewise	B
polynomial	B
of	O
order	O
in	O
each	O
interval	O
i	O
n	O
and	O
of	O
order	O
m	O
in	O
the	O
two	O
outermost	O
intervals	O
the	O
pieces	O
are	O
joined	O
so	O
that	O
the	O
solution	O
has	O
continuous	O
derivatives	O
schoenberg	O
also	O
proved	O
that	O
the	O
solution	O
to	O
the	O
univariate	O
smoothing	O
problem	O
eq	O
is	O
a	O
natural	O
polynomial	B
spline	O
a	O
common	O
choice	O
is	O
m	O
leading	O
to	O
the	O
cubic	O
spline	O
one	O
possible	O
way	O
of	O
writing	O
this	O
solution	O
is	O
nx	O
fx	O
jxj	O
ix	O
where	O
x	O
if	O
x	O
otherwise	O
it	O
turns	O
out	O
that	O
the	O
coefficients	O
and	O
can	O
be	O
computed	O
in	O
time	O
on	O
using	O
an	O
algorithm	O
due	O
to	O
reinsch	O
see	O
green	O
and	O
silverman	O
sec	O
for	O
details	O
splines	B
were	O
first	O
used	O
in	O
regression	O
problems	O
however	O
by	O
using	O
generalized	B
linear	B
modelling	O
and	O
nelder	O
they	O
can	O
be	O
extended	O
to	O
classification	B
problems	O
and	O
other	O
non-gaussian	B
likelihoods	O
as	O
we	O
did	O
for	O
gp	O
classification	B
in	O
section	O
early	O
references	O
in	O
this	O
direction	O
include	O
silverman	O
and	O
o	O
sullivan	O
et	O
al	O
there	O
is	O
a	O
vast	O
literature	O
in	O
relation	O
to	O
splines	B
in	O
both	O
the	O
statistics	O
and	O
numerical	O
analysis	O
literatures	O
for	O
entry	O
points	O
see	O
citations	O
in	O
wahba	O
and	O
green	O
and	O
silverman	O
a	O
gaussian	B
process	I
spline	O
construction	O
in	O
this	O
section	O
we	O
will	O
further	O
clarify	O
the	O
relationship	O
between	O
splines	B
and	O
gaussian	O
processes	O
by	O
giving	O
a	O
gp	O
construction	O
for	O
the	O
solution	O
of	O
the	O
univariate	O
cubic	O
spline	O
smoothing	O
problem	O
whose	O
cost	O
functional	B
is	O
nx	O
yi	O
z	O
dx	O
where	O
the	O
observed	O
data	O
are	O
yii	O
n	O
xn	O
and	O
is	O
a	O
smoothing	O
parameter	O
controlling	O
the	O
trade-off	O
between	O
the	O
first	O
term	O
the	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
spline	O
models	O
data-fit	O
and	O
the	O
second	O
term	O
the	O
regularizer	O
or	O
complexity	O
penalty	O
recall	O
that	O
the	O
solution	O
is	O
a	O
piecewise	B
polynomial	B
as	O
in	O
eq	O
following	O
wahba	O
we	O
consider	O
the	O
random	O
function	B
gx	O
jxj	O
fx	O
where	O
n	O
where	O
kspx	O
and	O
v	O
minx	O
z	O
i	O
and	O
fx	O
is	O
a	O
gaussian	B
process	I
with	O
covariance	B
f	O
kspx	O
u	O
du	O
to	O
complete	O
the	O
analogue	O
of	O
the	O
regularizer	O
in	O
eq	O
we	O
need	O
to	O
remove	O
any	O
penalty	O
on	O
polynomial	B
terms	O
in	O
the	O
null	B
space	I
by	O
making	O
the	O
prior	O
vague	O
notice	O
that	O
the	O
covariance	B
has	O
the	O
form	O
of	O
i	O
e	O
by	O
taking	O
the	O
limit	O
contributions	O
from	O
explicit	O
basis	O
functions	O
hx	O
x	O
and	O
a	O
regular	O
covariance	B
function	B
kspx	O
a	O
problem	O
which	O
we	O
have	O
already	O
studied	O
in	O
section	O
indeed	O
we	O
have	O
computed	O
the	O
limit	O
where	O
the	O
prior	O
becomes	O
vague	O
the	O
result	O
is	O
given	O
in	O
eq	O
plugging	O
into	O
the	O
mean	O
equation	O
from	O
eq	O
we	O
get	O
the	O
predictive	B
mean	O
fx	O
kx	O
y	O
h	O
hx	O
n	O
ij	O
evalwhere	O
ky	O
is	O
the	O
covariance	B
matrix	I
corresponding	O
to	O
uated	O
at	O
the	O
training	O
points	O
h	O
is	O
the	O
matrix	O
that	O
collects	O
the	O
hxi	O
vectors	O
at	O
all	O
training	O
points	O
and	O
y	O
y	O
is	O
given	O
below	O
eq	O
it	O
is	O
not	O
difficult	O
to	O
show	O
that	O
this	O
predictive	B
mean	B
function	B
is	O
a	O
piecewise	B
cubic	O
polynomial	B
since	O
the	O
elements	O
of	O
kx	O
are	O
cubic	O
polynomials	O
showing	O
that	O
the	O
mean	B
function	B
is	O
a	O
first	O
order	O
polynomial	B
in	O
the	O
outer	O
intervals	O
and	O
is	O
left	O
as	O
exercise	O
y	O
h	O
f	O
kspxi	O
xj	O
so	O
far	O
ksp	O
has	O
been	O
produced	O
rather	O
mysteriously	O
from	O
the	O
hat	O
we	O
now	O
provide	O
some	O
explanation	O
shepp	O
defined	O
the	O
l-fold	O
integrated	B
wiener	B
process	I
as	O
l	O
zudu	O
wlx	O
l	O
where	O
zu	O
denotes	O
the	O
gaussian	O
white	O
noise	O
process	O
with	O
covariance	B
note	O
that	O
is	O
the	O
standard	O
wiener	B
process	I
it	O
is	O
easy	O
to	O
show	O
that	O
kspx	O
is	O
the	O
covariance	B
of	O
the	O
once-integrated	O
wiener	B
process	I
by	O
writing	O
and	O
using	O
eq	O
and	O
taking	O
the	O
expectation	O
using	O
the	O
covariance	B
of	O
the	O
white	O
noise	O
process	O
note	O
that	O
wl	O
is	O
the	O
solution	O
to	O
the	O
stochastic	B
differential	I
equation	I
x	O
z	O
see	O
appendix	O
b	O
for	O
further	O
details	O
on	O
sdes	O
thus	O
pieces	O
are	O
joined	O
at	O
the	O
datapoints	O
the	O
points	O
where	O
the	O
minx	O
from	O
the	O
covari	O
z	O
ul	O
ance	O
function	B
is	O
non-differentiable	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
spline	O
covariance	B
squared	B
exponential	B
cov	O
figure	O
panel	O
shows	O
the	O
application	O
of	O
the	O
spline	O
covariance	B
to	O
a	O
simple	O
dataset	O
the	O
full	O
line	O
shows	O
the	O
predictive	B
mean	O
which	O
is	O
a	O
piecewise	B
cubic	O
polynomial	B
and	O
the	O
grey	O
area	O
indicates	O
the	O
confidence	O
area	O
the	O
two	O
thin	O
dashed	O
and	O
dash-dotted	O
lines	O
are	O
samples	O
from	O
the	O
posterior	O
note	O
that	O
the	O
posterior	O
samples	O
are	O
not	O
as	O
smooth	O
as	O
the	O
mean	O
for	O
comparison	O
a	O
gp	O
using	O
the	O
squared	B
exponential	B
covariance	B
function	B
is	O
shown	O
in	O
panel	O
the	O
hyperparameters	B
in	O
both	O
cases	O
were	O
optimized	O
using	O
the	O
marginal	B
likelihood	B
the	O
regularizerr	O
for	O
the	O
cubic	O
spline	O
we	O
set	B
l	O
to	O
obtain	O
the	O
sde	O
z	O
corresponding	O
to	O
we	O
can	O
also	O
give	O
an	O
explicit	O
basis-function	O
construction	O
for	O
the	O
covariance	B
function	B
ksp	O
consider	O
the	O
family	O
of	O
random	O
functions	O
given	O
by	O
fn	O
n	O
ix	O
i	O
n	O
n	O
n	O
where	O
is	O
a	O
vector	O
of	O
parameters	O
with	O
n	O
i	O
note	O
that	O
the	O
sum	O
has	O
the	O
form	O
of	O
evenly	O
spaced	O
ramps	O
whose	O
magnitudes	O
are	O
given	O
by	O
the	O
entries	O
in	O
the	O
vector	O
thus	O
efn	O
n	O
i	O
n	O
i	O
n	O
taking	O
the	O
limit	O
n	O
we	O
obtain	O
eq	O
a	O
derivation	O
which	O
is	O
also	O
found	O
in	O
sec	O
notice	O
that	O
the	O
covariance	B
function	B
ksp	O
given	O
in	O
eq	O
corresponds	O
to	O
a	O
gaussian	B
process	I
which	O
is	O
ms	O
continuous	O
but	O
only	O
once	O
ms	O
differentiable	O
thus	O
samples	O
from	O
the	O
prior	O
will	O
be	O
quite	O
rough	O
although	O
noted	O
in	O
section	O
the	O
posterior	O
mean	O
eq	O
is	O
smoother	O
the	O
constructions	O
above	O
can	O
be	O
generalized	B
to	O
the	O
regularizerr	O
dx	O
by	O
replacing	O
u	O
with	O
um	O
eq	O
and	O
setting	O
hx	O
x	O
xm	O
in	O
eq	O
and	O
similarly	O
in	O
thus	O
we	O
can	O
use	O
a	O
gaussian	B
process	I
formulation	O
as	O
an	O
alternative	O
to	O
the	O
usual	O
spline	O
fitting	O
procedure	O
note	O
that	O
the	O
trade-off	O
parameter	O
from	O
eq	O
xoutput	O
y	O
xoutput	O
y	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
support	B
vector	I
machines	O
figure	O
panel	O
shows	O
a	O
linearly	O
separable	O
binary	B
classification	B
problem	O
and	O
a	O
separating	O
hyperplane	O
panel	O
shows	O
the	O
maximum	O
margin	O
hyperplane	O
n	O
f	O
and	O
f	O
the	O
hyperparameters	B
n	O
can	O
be	O
set	B
is	O
now	O
given	O
as	O
the	O
ratio	O
using	O
the	O
techniques	O
from	O
section	O
by	O
optimizing	O
the	O
marginal	B
likelihood	B
given	O
in	O
eq	O
kohn	O
and	O
ansley	O
give	O
details	O
of	O
an	O
on	O
algorithm	O
on	O
kalman	O
filtering	O
for	O
the	O
computation	O
of	O
the	O
spline	O
and	O
the	O
marginal	B
likelihood	B
in	O
addition	O
to	O
the	O
predictive	B
mean	O
the	O
gp	O
treatment	O
also	O
yields	O
an	O
explicit	O
estimate	O
of	O
the	O
noise	O
level	O
and	O
predictive	B
error	O
bars	O
figure	O
shows	O
a	O
simple	O
example	O
notice	O
that	O
whereas	O
the	O
mean	B
function	B
is	O
a	O
piecewise	B
cubic	O
polynomial	B
samples	O
from	O
the	O
posterior	O
are	O
not	O
smooth	O
in	O
contrast	O
for	O
the	O
squared	B
exponential	B
covariance	B
functions	O
shown	O
in	O
panel	O
both	O
the	O
mean	O
and	O
functions	O
drawn	O
from	O
the	O
posterior	O
are	O
infinitely	O
differentiable	O
support	B
vector	I
machines	O
since	O
the	O
mid	O
s	O
there	O
has	O
been	O
an	O
explosion	O
of	O
interest	O
in	O
kernel	B
machines	O
and	O
in	O
particular	O
the	O
support	B
vector	I
machine	I
the	O
aim	O
of	O
this	O
section	O
is	O
to	O
provide	O
a	O
brief	O
introduction	O
to	O
svms	O
and	O
in	O
particular	O
to	O
compare	O
them	O
to	O
gaussian	B
process	I
predictors	O
we	O
consider	O
svms	O
for	O
classification	B
and	O
regression	O
problems	O
in	O
sections	O
and	O
respectively	O
more	O
comprehensive	O
treatments	O
can	O
be	O
found	O
in	O
vapnik	O
cristianini	O
and	O
shawe-taylor	O
and	O
sch	O
olkopf	O
and	O
smola	O
support	B
vector	I
classification	B
for	O
support	B
vector	I
classifiers	O
the	O
key	O
notion	O
that	O
we	O
need	O
to	O
introduce	O
is	O
that	O
of	O
the	O
maximum	O
margin	O
hyperplane	O
for	O
a	O
linear	B
classifier	I
then	O
by	O
using	O
the	O
kernel	B
trick	I
this	O
can	O
be	O
lifted	O
into	O
feature	B
space	I
we	O
consider	O
first	O
the	O
separable	O
case	O
and	O
then	O
the	O
non-separable	O
case	O
we	O
conclude	O
this	O
section	O
with	O
a	O
comparison	O
between	O
gp	O
classifiers	O
and	O
svms	O
xixjw	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
the	O
separable	O
case	O
figure	O
illustrates	O
the	O
case	O
where	O
the	O
data	O
is	O
linearly	O
separable	O
for	O
a	O
linear	B
classifier	I
with	O
weight	B
vector	I
w	O
and	O
offset	O
let	O
the	O
decision	O
boundary	O
be	O
defined	O
by	O
w	O
x	O
and	O
let	O
w	O
clearly	O
there	O
is	O
a	O
whole	O
version	O
space	O
of	O
weight	O
vectors	O
that	O
give	O
rise	O
to	O
the	O
same	O
classification	B
of	O
the	O
training	O
points	O
the	O
svm	O
algorithm	O
chooses	O
a	O
particular	O
weight	B
vector	I
that	O
gives	O
rise	O
to	O
the	O
maximum	O
margin	O
of	O
separation	O
functional	B
margin	O
geometrical	B
margin	O
let	O
the	O
training	O
set	B
be	O
pairs	O
of	O
the	O
form	O
yi	O
for	O
i	O
n	O
where	O
yi	O
for	O
a	O
given	O
weight	B
vector	I
we	O
can	O
compute	O
the	O
quantity	O
i	O
yiw	O
x	O
which	O
is	O
known	O
as	O
the	O
functional	B
margin	O
notice	O
that	O
i	O
if	O
a	O
training	O
point	O
is	O
correctly	O
classified	O
if	O
the	O
equation	O
fx	O
w	O
x	O
defines	O
a	O
discriminant	O
function	B
that	O
the	O
output	O
is	O
sgnfx	O
then	O
the	O
hyperplane	O
cw	O
x	O
defines	O
the	O
same	O
discriminant	O
function	B
for	O
any	O
c	O
thus	O
we	O
have	O
the	O
freedom	O
to	O
choose	O
the	O
scaling	O
of	O
w	O
so	O
that	O
mini	O
i	O
and	O
in	O
this	O
case	O
w	O
is	O
known	O
as	O
the	O
canonical	B
form	O
of	O
the	O
hyperplane	O
the	O
geometrical	B
margin	O
is	O
defined	O
as	O
i	O
iw	O
for	O
a	O
training	O
point	O
xi	O
that	O
is	O
correctly	O
classified	O
this	O
is	O
simply	O
the	O
distance	O
from	O
xi	O
to	O
the	O
hyperplane	O
to	O
see	O
this	O
let	O
c	O
so	O
that	O
w	O
ww	O
is	O
a	O
unit	O
vector	O
in	O
the	O
direction	O
of	O
w	O
and	O
is	O
the	O
corresponding	O
offset	O
then	O
w	O
x	O
computes	O
the	O
length	O
of	O
the	O
projection	O
of	O
x	O
onto	O
the	O
direction	O
orthogonal	O
to	O
the	O
hyperplane	O
and	O
w	O
x	O
computes	O
the	O
distance	O
to	O
the	O
hyperplane	O
for	O
training	O
points	O
that	O
are	O
misclassified	O
the	O
geometrical	B
margin	O
is	O
the	O
negative	O
distance	O
to	O
the	O
hyperplane	O
the	O
geometrical	B
margin	O
for	O
a	O
dataset	O
d	O
is	O
defined	O
as	O
d	O
mini	O
i	O
thus	O
for	O
a	O
canonical	B
separating	O
hyperplane	O
the	O
margin	O
is	O
we	O
wish	O
to	O
find	O
the	O
maximum	O
margin	O
hyperplane	O
i	O
e	O
the	O
one	O
that	O
maximizes	O
d	O
optimization	O
problem	O
by	O
considering	O
canonical	B
hyperplanes	O
we	O
are	O
thus	O
led	O
to	O
the	O
following	O
op	O
timization	O
problem	O
to	O
determine	O
the	O
maximum	O
margin	O
hyperplane	O
minimize	O
subject	O
to	O
yiw	O
xi	O
over	O
w	O
for	O
all	O
i	O
n	O
it	O
is	O
clear	O
by	O
considering	O
the	O
geometry	O
that	O
for	O
the	O
maximum	O
margin	O
solution	O
there	O
will	O
be	O
at	O
least	O
one	O
data	O
point	O
in	O
each	O
class	O
for	O
which	O
yiw	O
see	O
figure	O
let	O
the	O
hyperplanes	O
that	O
pass	O
through	O
these	O
points	O
be	O
denoted	O
h	O
and	O
h	O
respectively	O
this	O
constrained	O
optimization	O
problem	O
can	O
be	O
set	B
up	O
using	O
lagrange	O
multipliers	O
and	O
solved	O
using	O
numerical	O
methods	O
for	O
quadratic	O
problems	O
the	O
form	O
of	O
the	O
solution	O
is	O
w	O
x	O
quadratic	B
programming	I
problem	O
is	O
an	O
optimization	O
problem	O
where	O
the	O
objective	O
func	O
tion	O
is	O
quadratic	O
and	O
the	O
constraints	O
are	O
linear	B
in	O
the	O
unknowns	O
i	O
iyixi	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
support	B
vector	I
machines	O
support	O
vectors	O
kernel	B
trick	I
soft	B
margin	I
where	O
the	O
i	O
s	O
are	O
non-negative	O
lagrange	O
multipliers	O
notice	O
that	O
the	O
solution	O
is	O
a	O
linear	B
combination	O
of	O
the	O
xi	O
s	O
the	O
key	O
feature	O
of	O
equation	O
is	O
that	O
i	O
is	O
zero	O
for	O
every	O
xi	O
except	O
those	O
which	O
lie	O
on	O
the	O
hyperplanes	O
h	O
or	O
h	O
these	O
points	O
are	O
called	O
the	O
support	O
vectors	O
the	O
fact	O
that	O
not	O
all	O
of	O
the	O
training	O
points	O
contribute	O
to	O
the	O
final	O
solution	O
is	O
referred	O
to	O
as	O
the	O
sparsity	O
of	O
the	O
solution	O
the	O
support	O
vectors	O
lie	O
closest	O
to	O
the	O
decision	O
boundary	O
note	O
that	O
if	O
all	O
of	O
the	O
other	O
training	O
points	O
were	O
removed	O
moved	O
around	O
but	O
not	O
crossing	O
h	O
or	O
h	O
the	O
same	O
maximum	O
margin	O
hyperplane	O
would	O
be	O
found	O
the	O
quadratic	B
programming	I
problem	O
for	O
finding	O
the	O
i	O
s	O
is	O
convex	O
i	O
e	O
there	O
are	O
no	O
local	O
minima	O
notice	O
the	O
similarity	O
of	O
this	O
to	O
the	O
convexity	O
of	O
the	O
optimization	O
problem	O
for	O
gaussian	B
process	I
classifiers	O
as	O
described	O
in	O
section	O
to	O
make	O
predictions	O
for	O
a	O
new	O
input	O
x	O
we	O
compute	O
sgnw	O
x	O
sgn	O
iyixi	O
x	O
nx	O
in	O
the	O
qp	O
problem	O
and	O
in	O
eq	O
the	O
training	O
points	O
and	O
the	O
test	O
point	O
x	O
enter	O
the	O
computations	O
only	O
in	O
terms	O
of	O
inner	O
products	O
thus	O
by	O
using	O
the	O
kernel	B
trick	I
we	O
can	O
replace	O
occurrences	O
of	O
the	O
inner	O
product	O
by	O
the	O
kernel	B
to	O
obtain	O
the	O
equivalent	B
result	O
in	O
feature	B
space	I
the	O
non-separable	O
case	O
for	O
linear	B
classifiers	O
in	O
the	O
original	O
x	O
space	O
there	O
will	O
be	O
some	O
datasets	O
that	O
are	O
not	O
linearly	O
separable	O
one	O
way	O
to	O
generalize	O
the	O
svm	O
problem	O
in	O
this	O
case	O
is	O
to	O
allow	O
violations	O
of	O
the	O
constraint	O
yiw	O
xi	O
but	O
to	O
impose	O
a	O
penalty	O
when	O
this	O
occurs	O
this	O
leads	O
to	O
the	O
soft	B
margin	I
support	B
vector	I
machine	I
problem	O
the	O
minimization	O
of	O
nx	O
c	O
yifi	O
with	O
respect	O
to	O
w	O
and	O
where	O
fi	O
fxi	O
w	O
xi	O
and	O
z	O
if	O
z	O
and	O
otherwise	O
here	O
c	O
is	O
a	O
parameter	O
that	O
specifies	O
the	O
relative	O
importance	O
of	O
the	O
two	O
terms	O
this	O
convex	O
optimization	O
problem	O
can	O
again	O
be	O
solved	O
using	O
qp	O
methods	O
and	O
yields	O
a	O
solution	O
of	O
the	O
form	O
given	O
in	O
eq	O
in	O
this	O
case	O
the	O
support	O
vectors	O
with	O
i	O
are	O
not	O
only	O
those	O
data	O
points	O
which	O
lie	O
on	O
the	O
separating	O
hyperplanes	O
but	O
also	O
those	O
that	O
incur	O
penalties	O
this	O
can	O
occur	O
in	O
two	O
ways	O
the	O
data	O
point	O
falls	O
in	O
between	O
h	O
and	O
h	O
but	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
surface	I
or	O
the	O
data	O
point	O
falls	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
surface	I
in	O
a	O
feature	B
space	I
of	O
dimension	O
n	O
if	O
n	O
n	O
then	O
there	O
will	O
always	O
be	O
separating	O
hyperplane	O
however	O
this	O
hyperplane	O
may	O
not	O
give	O
rise	O
to	O
good	O
generalization	B
performance	O
especially	O
if	O
some	O
of	O
the	O
labels	O
are	O
incorrect	O
and	O
thus	O
the	O
soft	B
margin	I
svm	O
formulation	O
is	O
often	O
used	O
in	O
practice	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
figure	O
a	O
comparison	O
of	O
the	O
hinge	B
error	O
g	O
and	O
g	O
the	O
error	O
function	B
used	O
in	O
svr	O
for	O
both	O
the	O
hard	O
and	O
soft	B
margin	I
svm	O
qp	O
problems	O
a	O
wide	O
variety	O
of	O
algorithms	O
have	O
been	O
developed	O
for	O
their	O
solution	O
see	O
sch	O
olkopf	O
and	O
smola	O
ch	O
for	O
details	O
basic	O
interior	O
point	O
methods	O
involve	O
inversions	O
of	O
n	O
n	O
matrices	O
and	O
thus	O
scale	O
as	O
as	O
with	O
gaussian	B
process	I
prediction	O
however	O
there	O
are	O
other	O
algorithms	O
such	O
as	O
the	O
sequential	O
minimal	O
optimization	O
algorithm	O
due	O
to	O
platt	O
which	O
often	O
have	O
better	O
scaling	O
in	O
practice	O
above	O
we	O
have	O
described	O
svms	O
for	O
the	O
two-class	O
classification	B
problem	O
there	O
are	O
many	O
ways	O
of	O
generalizing	O
svms	O
to	O
the	O
multi-class	B
problem	O
see	O
sch	O
olkopf	O
and	O
smola	O
sec	O
for	O
further	O
details	O
comparing	O
support	B
vector	I
and	O
gaussian	B
process	I
classifiers	O
for	O
the	O
soft	B
margin	I
classifier	O
we	O
obtain	O
a	O
solution	O
of	O
the	O
form	O
w	O
p	O
i	O
iyi	O
and	O
thus	O
i	O
ixi	O
ij	O
i	O
jxi	O
xj	O
kernelizing	O
this	O
we	O
obtain	O
fk	O
k	O
f	O
thus	O
the	O
soft	B
margin	I
objective	O
nx	O
function	B
can	O
be	O
written	O
as	O
fk	O
c	O
yifi	O
fk	O
nx	O
for	O
the	O
binary	B
gp	O
classifier	O
to	O
obtain	O
the	O
map	B
value	O
f	O
of	O
pfy	O
we	O
minimize	O
the	O
quantity	O
log	O
pyifi	O
cf	O
eq	O
final	O
two	O
terms	O
in	O
eq	O
are	O
constant	O
if	O
the	O
kernel	B
is	O
fixed	O
for	O
log-concave	O
likelihoods	O
as	O
those	O
derived	O
from	O
the	O
logistic	B
or	O
probit	B
response	O
functions	O
there	O
is	O
a	O
strong	O
similarity	O
between	O
the	O
two	O
optimization	O
problems	O
in	O
that	O
they	O
are	O
both	O
convex	O
let	O
g	O
e	O
z	O
g	O
the	O
offset	O
has	O
been	O
absorbed	O
into	O
the	O
kernel	B
so	O
it	O
is	O
not	O
an	O
explicit	O
extra	O
param	O
eter	O
exp	O
z	O
log	O
z	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
support	B
vector	I
machines	O
hinge	B
error	I
function	B
log	O
and	O
ghingez	O
z	O
where	O
z	O
yifi	O
we	O
refer	O
to	O
ghinge	O
as	O
the	O
hinge	B
error	I
function	B
due	O
to	O
its	O
shape	O
as	O
shown	O
in	O
figure	O
all	O
three	O
data	O
fit	O
terms	O
are	O
monotonically	O
decreasing	O
functions	O
of	O
z	O
all	O
three	O
functions	O
tend	O
to	O
infinity	O
as	O
z	O
and	O
decay	O
to	O
zero	O
as	O
z	O
the	O
key	O
difference	O
is	O
that	O
the	O
hinge	B
function	B
takes	O
on	O
the	O
value	O
for	O
z	O
while	O
the	O
other	O
two	O
just	O
decay	O
slowly	O
it	O
is	O
this	O
flat	O
part	O
of	O
the	O
hinge	B
function	B
that	O
gives	O
rise	O
to	O
the	O
sparsity	O
of	O
the	O
svm	O
solution	O
thus	O
there	O
is	O
a	O
close	O
correspondence	O
between	O
the	O
map	B
solution	O
of	O
the	O
gp	O
classifier	O
and	O
the	O
svm	O
solution	O
can	O
this	O
correspondence	O
be	O
made	O
closer	O
by	O
considering	O
the	O
hinge	B
function	B
as	O
a	O
negative	O
log	O
likelihood	B
the	O
answer	O
to	O
this	O
is	O
no	O
sollich	O
if	O
cghingez	O
defined	O
a	O
negative	O
log	O
likelihood	B
then	O
exp	O
cghingef	O
exp	O
cghinge	O
f	O
should	O
be	O
a	O
constant	O
independent	O
of	O
f	O
but	O
this	O
is	O
not	O
the	O
case	O
to	O
see	O
this	O
consider	O
the	O
quantity	O
c	O
f	O
exp	O
f	O
cannot	O
be	O
chosen	O
so	O
as	O
to	O
make	O
c	O
independent	O
of	O
the	O
value	O
of	O
f	O
for	O
c	O
by	O
comparison	O
for	O
the	O
logistic	B
and	O
probit	B
likelihoods	O
the	O
analogous	O
expression	O
is	O
equal	O
to	O
sollich	O
suggests	O
choosing	O
exp	O
which	O
ensures	O
that	O
c	O
equality	O
only	O
when	O
f	O
he	O
also	O
gives	O
an	O
ingenious	O
interpretation	O
a	O
don	O
t	O
know	O
class	O
to	O
soak	O
up	O
the	O
unassigned	O
probability	O
mass	O
that	O
does	O
yield	O
the	O
svm	O
solution	O
as	O
the	O
map	B
solution	O
to	O
a	O
certain	O
bayesian	O
problem	O
although	O
we	O
find	O
this	O
construction	O
rather	O
contrived	O
exercise	O
invites	O
you	O
to	O
plot	O
c	O
as	O
a	O
function	B
of	O
f	O
for	O
various	O
values	O
of	O
c	O
one	O
attraction	O
of	O
the	O
gp	O
classifier	O
is	O
that	O
it	O
produces	O
an	O
output	O
with	O
a	O
clear	O
probabilistic	B
interpretation	O
a	O
prediction	O
for	O
py	O
one	O
can	O
try	O
to	O
interpret	O
the	O
function	B
value	O
fx	O
output	O
by	O
the	O
svm	O
probabilistically	O
and	O
platt	O
suggested	O
that	O
probabilistic	B
predictions	O
can	O
be	O
generated	O
from	O
the	O
svm	O
by	O
computing	O
b	O
for	O
some	O
constants	O
a	O
b	O
that	O
are	O
fitted	O
using	O
some	O
unbiased	O
version	O
of	O
the	O
training	O
set	B
using	O
cross-validation	B
one	O
disadvantage	O
of	O
this	O
rather	O
ad	O
hoc	O
procedure	O
is	O
that	O
unlike	O
the	O
gp	O
classifiers	O
it	O
does	O
not	O
take	O
into	O
account	O
the	O
predictive	B
variance	O
of	O
fx	O
eq	O
seeger	B
sec	O
shows	O
that	O
better	O
error-reject	O
curves	O
can	O
be	O
obtained	O
on	O
an	O
experiment	O
using	O
the	O
mnist	O
digit	O
classification	B
problem	O
when	O
the	O
effect	O
of	O
this	O
uncertainty	O
is	O
taken	O
into	O
account	O
support	B
vector	I
regression	I
the	O
svm	O
was	O
originally	O
introduced	O
for	O
the	O
classification	B
problem	O
then	O
extended	O
to	O
deal	O
with	O
the	O
regression	O
case	O
the	O
key	O
concept	O
is	O
that	O
of	O
the	O
error	O
function	B
this	O
is	O
defined	O
as	O
gz	O
if	O
otherwise	O
this	O
function	B
is	O
plotted	O
in	O
figure	O
as	O
in	O
eq	O
we	O
can	O
interpret	O
exp	O
gz	O
as	O
a	O
likelihood	B
model	O
for	O
the	O
regression	O
residuals	O
the	O
squared	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
error	O
function	B
corresponding	O
to	O
a	O
gaussian	O
model	O
however	O
we	O
note	O
that	O
this	O
is	O
quite	O
an	O
unusual	O
choice	O
of	O
model	O
for	O
the	O
distribution	O
of	O
residuals	O
and	O
is	O
basically	O
motivated	O
by	O
the	O
desire	O
to	O
obtain	O
a	O
sparse	O
solution	O
below	O
as	O
in	O
support	B
vector	I
classifier	O
if	O
then	O
the	O
error	O
model	O
is	O
a	O
laplacian	O
distribution	O
which	O
corresponds	O
to	O
least	O
absolute	O
values	O
regression	O
cited	O
in	O
rousseeuw	O
this	O
is	O
a	O
heavier-tailed	O
distribution	O
than	O
the	O
gaussian	O
and	O
provides	O
some	O
protection	O
against	O
outliers	O
girosi	O
showed	O
that	O
the	O
laplacian	O
distribution	O
can	O
be	O
viewed	O
as	O
a	O
continuous	O
mixture	O
of	O
zeromean	O
gaussians	O
with	O
a	O
certain	O
distribution	O
over	O
their	O
variances	O
pontil	O
et	O
al	O
extended	O
this	O
result	O
by	O
allowing	O
the	O
means	O
to	O
uniformly	O
shift	O
in	O
in	O
order	O
to	O
obtain	O
a	O
probabilistic	B
model	O
corresponding	O
to	O
the	O
error	O
function	B
see	O
also	O
section	O
for	O
work	O
on	O
robustification	O
of	O
the	O
gp	O
regression	O
problem	O
for	O
the	O
linear	B
regression	I
case	O
with	O
an	O
error	O
function	B
and	O
a	O
gaussian	O
prior	O
on	O
w	O
the	O
map	B
value	O
of	O
w	O
is	O
obtained	O
by	O
minimizing	O
gyi	O
fi	O
nx	O
w	O
r	O
t	O
w	O
the	O
is	O
fx	O
solution	O
fx	O
ikxi	O
x	O
c	O
ixi	O
x	O
where	O
the	O
coefficients	O
are	O
obtained	O
from	O
a	O
qp	O
problem	O
the	O
problem	O
can	O
also	O
be	O
kernelized	O
to	O
give	O
the	O
as	O
for	O
support	B
vector	I
classification	B
many	O
of	O
the	O
coefficients	O
i	O
are	O
zero	O
the	O
data	O
points	O
which	O
lie	O
inside	O
the	O
tube	O
have	O
i	O
while	O
those	O
on	O
the	O
edge	O
or	O
outside	O
have	O
non-zero	O
i	O
least-squares	B
classification	B
in	O
chapter	O
we	O
have	O
argued	O
that	O
the	O
use	O
of	O
logistic	B
or	O
probit	B
likelihoods	O
provides	O
the	O
natural	O
route	O
to	O
develop	O
a	O
gp	O
classifier	O
and	O
that	O
it	O
is	O
attractive	O
in	O
that	O
the	O
outputs	B
can	O
be	O
interpreted	O
probabilistically	O
however	O
there	O
is	O
an	O
even	O
simpler	O
approach	O
which	O
treats	O
classification	B
as	O
a	O
regression	O
problem	O
our	O
starting	O
point	O
is	O
binary	B
classification	B
using	O
the	O
linear	B
predictor	O
fx	O
wx	O
this	O
is	O
trained	O
using	O
linear	B
regression	I
with	O
a	O
target	O
y	O
for	O
patterns	O
that	O
have	O
label	O
and	O
target	O
y	O
for	O
patterns	O
that	O
have	O
label	O
y	O
y	O
give	O
slightly	O
more	O
flexibility	O
than	O
just	O
using	O
targets	O
of	O
as	O
shown	O
in	O
duda	O
and	O
hart	O
section	O
choosing	O
y	O
y	O
appropriately	O
allows	O
us	O
to	O
obtain	O
the	O
same	O
solution	O
as	O
fisher	B
s	O
linear	B
discriminant	O
using	O
the	O
decision	O
criterion	O
fx	O
also	O
they	O
show	O
that	O
using	O
targets	O
y	O
y	O
with	O
the	O
least-squares	B
error	O
function	B
gives	O
a	O
minimum	O
squared-error	O
approximation	O
to	O
the	O
bayes	O
discriminant	O
function	B
pcx	O
pc	O
as	O
n	O
following	O
rifkin	O
and	O
klautau	O
we	O
call	O
such	O
methods	O
least-squares	B
classification	B
note	O
that	O
under	O
a	O
probabilistic	B
interpretation	O
the	O
squared-error	O
criterion	O
is	O
rather	O
an	O
we	O
have	O
assumed	O
that	O
the	O
constant	O
is	O
included	O
in	O
the	O
input	O
vector	O
x	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
least-squares	B
classification	B
odd	O
choice	O
as	O
it	O
implies	O
a	O
gaussian	O
noise	B
model	I
yet	O
only	O
two	O
values	O
of	O
the	O
target	O
and	O
y	O
are	O
observed	O
it	O
is	O
natural	O
to	O
extend	O
the	O
least-squares	B
classifier	O
using	O
the	O
kernel	B
trick	I
this	O
has	O
been	O
suggested	O
by	O
a	O
number	O
of	O
authors	O
including	O
poggio	O
and	O
girosi	O
and	O
suykens	O
and	O
vanderwalle	O
experimental	O
results	O
reported	O
in	O
rifkin	O
and	O
klautau	O
indicate	O
that	O
performance	O
comparable	O
to	O
svms	O
can	O
be	O
obtained	O
using	O
kernel	B
lsc	O
as	O
they	O
call	O
it	O
the	O
regularized	O
least-squares	B
classifier	O
rlsc	O
consider	O
a	O
single	O
random	O
variable	O
y	O
which	O
takes	O
on	O
the	O
value	O
with	O
probability	O
p	O
and	O
value	O
with	O
probability	O
p	O
then	O
the	O
value	O
of	O
f	O
which	O
minimizes	O
the	O
squared	B
error	O
function	B
e	O
pf	O
pf	O
is	O
f	O
which	O
is	O
a	O
linear	B
rescaling	O
of	O
p	O
to	O
the	O
interval	O
if	O
the	O
targets	O
are	O
and	O
we	O
obtain	O
f	O
p	O
hence	O
we	O
observe	O
that	O
lsc	O
will	O
estimate	O
p	O
correctly	O
in	O
the	O
large	O
data	O
limit	O
if	O
we	O
now	O
consider	O
not	O
just	O
a	O
single	O
random	O
variable	O
but	O
wish	O
to	O
estimate	O
pcx	O
a	O
linear	B
rescaling	O
of	O
it	O
then	O
as	O
long	O
as	O
the	O
approximating	O
function	B
fx	O
is	O
sufficiently	O
flexible	O
we	O
would	O
expect	O
that	O
in	O
the	O
limit	O
n	O
it	O
would	O
converge	O
to	O
pcx	O
more	O
technical	O
detail	O
on	O
this	O
issue	O
see	O
section	O
on	O
consistency	B
hence	O
lsc	O
is	O
quite	O
a	O
sensible	O
procedure	O
for	O
classification	B
although	O
note	O
that	O
there	O
is	O
no	O
guarantee	O
that	O
fx	O
will	O
be	O
constrained	O
to	O
lie	O
in	O
the	O
interval	O
y	O
if	O
we	O
wish	O
to	O
guarantee	O
a	O
probabilistic	B
interpretation	O
we	O
could	O
squash	O
the	O
predictions	O
through	O
a	O
sigmoid	O
as	O
suggested	O
for	O
svms	O
by	O
platt	O
and	O
described	O
on	O
page	O
when	O
generalizing	O
from	O
the	O
binary	B
to	O
multi-class	B
situation	O
there	O
is	O
some	O
freedom	O
as	O
to	O
how	O
to	O
set	B
the	O
problem	O
up	O
sch	O
olkopf	O
and	O
smola	O
sec	O
identify	O
four	O
methods	O
namely	O
one-versus-rest	B
c	O
binary	B
classifiers	O
are	O
trained	O
to	O
classify	O
each	O
class	O
against	O
all	O
the	O
rest	O
all	O
pairs	O
cc	O
binary	B
classifiers	O
are	O
trained	O
error-correcting	O
output	O
coding	O
each	O
class	O
is	O
assigned	O
a	O
binary	B
codeword	O
and	O
binary	B
classifiers	O
are	O
trained	O
on	O
each	O
bit	O
separately	O
and	O
multi-class	B
objective	O
functions	O
the	O
aim	O
is	O
to	O
train	O
c	O
classifiers	O
simultaneously	O
rather	O
than	O
creating	O
a	O
number	O
of	O
binary	B
classification	B
problems	O
one	O
also	O
needs	O
to	O
specify	O
how	O
the	O
outputs	B
of	O
the	O
various	O
classifiers	O
that	O
are	O
trained	O
are	O
combined	O
so	O
as	O
to	O
produce	O
an	O
overall	O
answer	O
for	O
the	O
method	O
one	O
simple	O
criterion	O
is	O
to	O
choose	O
the	O
classifier	O
which	O
produces	O
the	O
most	O
positive	O
output	O
rifkin	O
and	O
klautau	O
performed	O
extensive	O
experiments	O
and	O
came	O
to	O
the	O
conclusion	O
that	O
the	O
one-versus-rest	B
scheme	O
using	O
either	O
svms	O
or	O
rlsc	O
is	O
as	O
accurate	O
as	O
any	O
other	O
method	O
overall	O
and	O
has	O
the	O
merit	O
of	O
being	O
conceptually	O
simple	O
and	O
straightforward	O
to	O
implement	O
probabilistic	B
least-squares	B
classification	B
the	O
lsc	O
algorithm	O
discussed	O
above	O
is	O
attractive	O
from	O
a	O
computational	O
point	O
of	O
view	O
but	O
to	O
guarantee	O
a	O
valid	O
probabilistic	B
interpretation	O
one	O
may	O
need	O
to	O
use	O
a	O
separate	O
post-processing	O
stage	O
to	O
squash	O
the	O
predictions	O
through	O
a	O
sigmoid	O
however	O
it	O
is	O
not	O
so	O
easy	O
to	O
enforce	O
a	O
probabilistic	B
interpretation	O
method	O
is	O
also	O
sometimes	O
called	O
one-versus-all	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
during	O
the	O
training	O
stage	O
one	O
possible	O
solution	O
is	O
to	O
combine	O
the	O
ideas	O
of	O
training	O
using	O
leave-one-out	B
cross-validation	B
covered	O
in	O
section	O
with	O
the	O
use	O
of	O
a	O
sigmoid	O
function	B
as	O
in	O
platt	O
we	O
will	O
call	O
this	O
method	O
the	O
probabilistic	B
least-squares	B
classifier	O
in	O
section	O
we	O
saw	O
how	O
to	O
compute	O
the	O
gaussian	O
leave-one-out	B
predictive	B
probabilities	O
and	O
that	O
training	O
of	O
hyperparameters	B
can	O
be	O
based	O
on	O
the	O
sum	O
of	O
the	O
log	O
loo	O
probabilities	O
using	O
this	O
idea	O
we	O
express	O
the	O
loo	O
probability	O
by	O
squashing	O
a	O
linear	B
function	B
of	O
the	O
gaussian	O
predictive	B
probability	O
through	O
a	O
cumulative	O
gaussian	O
pyix	O
y	O
i	O
fi	O
i	O
yi	O
i	O
i	O
dfi	O
z	O
i	O
where	O
the	O
integral	O
is	O
given	O
in	O
eq	O
and	O
the	O
leave-one-out	B
predictive	B
mean	O
i	O
and	O
variance	O
i	O
are	O
given	O
in	O
eq	O
the	O
objective	O
function	B
is	O
the	O
sum	O
of	O
the	O
log	O
loo	O
probabilities	O
eq	O
which	O
can	O
be	O
used	O
to	O
set	B
the	O
hyperparameters	B
as	O
well	O
as	O
the	O
two	O
additional	O
parameters	O
of	O
the	O
linear	B
transformation	O
and	O
in	O
eq	O
introducing	O
the	O
likelihood	B
in	O
eq	O
into	O
the	O
objective	O
eq	O
and	O
taking	O
derivatives	O
we	O
obtain	O
log	O
pyix	O
y	O
i	O
i	O
yi	O
i	O
n	O
i	O
j	O
i	O
j	O
log	O
pyix	O
y	O
i	O
i	O
i	O
i	O
i	O
j	O
i	O
j	O
where	O
ri	O
i	O
loo	O
parameters	O
i	O
j	O
and	O
linear	B
transformation	O
parameters	O
we	O
have	O
i	O
and	O
the	O
partial	O
derivatives	O
of	O
the	O
gaussian	O
i	O
j	O
are	O
given	O
in	O
eq	O
finally	O
for	O
the	O
lloo	O
j	O
nx	O
nx	O
nx	O
nx	O
lloo	O
lloo	O
n	O
n	O
yi	O
i	O
i	O
i	O
i	O
i	O
these	O
partial	O
derivatives	O
can	O
be	O
used	O
to	O
train	O
the	O
parameters	O
of	O
the	O
gp	O
there	O
are	O
several	O
options	O
on	O
how	O
to	O
do	O
predictions	O
but	O
the	O
most	O
natural	O
would	O
seem	O
to	O
be	O
to	O
compute	O
predictive	B
mean	O
and	O
variance	O
and	O
squash	O
it	O
through	O
the	O
sigmoid	O
parallelling	O
eq	O
applying	O
this	O
model	O
to	O
the	O
usps	B
vs	O
binary	B
classification	B
task	O
discussed	O
in	O
section	O
we	O
get	O
a	O
test	O
set	B
error	O
rate	O
of	O
which	O
compares	O
favourably	O
with	O
the	O
results	O
reported	O
for	O
other	O
methods	O
in	O
figure	O
however	O
the	O
test	O
set	B
information	O
is	O
only	O
which	O
is	O
very	O
poor	O
test	O
information	O
is	O
dominated	O
by	O
a	O
single	O
test	O
case	O
which	O
is	O
predicted	O
confidently	O
to	O
belong	O
to	O
the	O
wrong	O
class	O
visual	O
inspection	O
of	O
the	O
digit	O
reveals	O
that	O
indeed	O
it	O
looks	O
as	O
though	O
the	O
testset	O
label	O
is	O
wrong	O
for	O
this	O
case	O
this	O
observation	O
highlights	O
the	O
danger	O
of	O
not	O
explicitly	O
allowing	O
for	O
data	O
mislabelling	O
in	O
the	O
model	O
for	O
this	O
kind	O
of	O
data	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relevance	O
vector	O
machines	O
relevance	O
vector	O
machines	O
although	O
usually	O
not	O
presented	O
as	O
such	O
the	O
relevance	B
vector	I
machine	I
introduced	O
by	O
tipping	O
is	O
actually	O
a	O
special	O
case	O
of	O
a	O
gaussian	B
process	I
the	O
covariance	B
function	B
has	O
the	O
form	O
nx	O
kx	O
jx	O
j	O
where	O
j	O
are	O
hyperparameters	B
and	O
the	O
n	O
basis	O
functions	O
jx	O
are	O
usually	O
but	O
not	O
necessarily	O
taken	O
to	O
be	O
gaussian-shaped	O
basis	O
functions	O
centered	O
on	O
each	O
of	O
the	O
n	O
training	O
data	O
points	O
jx	O
exp	O
where	O
is	O
a	O
length-scale	B
hyperparameter	O
controlling	O
the	O
width	O
of	O
the	O
basis	O
function	B
notice	O
that	O
this	O
is	O
simply	O
the	O
construction	O
for	O
the	O
covariance	B
function	B
corresponding	O
to	O
an	O
n-dimensional	O
set	B
of	O
basis	O
functions	O
given	O
in	O
section	O
with	O
p	O
diag	O
n	O
the	O
covariance	B
function	B
in	O
eq	O
has	O
two	O
interesting	O
properties	O
firstly	O
it	O
is	O
clear	O
that	O
the	O
feature	B
space	I
corresponding	O
to	O
the	O
covariance	B
function	B
is	O
finite	O
dimensional	O
i	O
e	O
the	O
covariance	B
function	B
is	O
degenerate	B
and	O
secondly	O
the	O
covariance	B
function	B
has	O
the	O
odd	O
property	O
that	O
it	O
depends	O
on	O
the	O
training	O
data	O
this	O
dependency	O
means	O
that	O
the	O
prior	O
over	O
functions	O
depends	O
on	O
the	O
data	O
a	O
property	O
which	O
is	O
at	O
odds	O
with	O
a	O
strict	O
bayesian	O
interpretation	O
although	O
the	O
usual	O
treatment	O
of	O
the	O
model	O
is	O
still	O
possible	O
this	O
dependency	O
of	O
the	O
prior	O
on	O
the	O
data	O
may	O
lead	O
to	O
some	O
surprising	O
effects	O
as	O
discussed	O
below	O
training	O
the	O
rvm	O
is	O
analogous	O
to	O
other	O
gp	O
models	O
optimize	O
the	O
marginal	B
likelihood	B
w	O
r	O
t	O
the	O
hyperparameters	B
this	O
optimization	O
often	O
leads	O
to	O
a	O
significant	O
number	O
of	O
the	O
j	O
hyperparameters	B
tending	O
towards	O
infinity	O
effectively	O
removing	O
or	O
pruning	O
the	O
corresponding	O
basis	O
function	B
from	O
the	O
covariance	B
function	B
in	O
eq	O
the	O
basic	O
idea	O
is	O
that	O
basis	O
functions	O
that	O
are	O
not	O
significantly	O
contributing	O
to	O
explaining	O
the	O
data	O
should	O
be	O
removed	O
resulting	O
in	O
a	O
sparse	O
model	O
the	O
basis	O
functions	O
that	O
survive	O
are	O
called	O
relevance	O
vectors	O
empirically	O
it	O
is	O
often	O
observed	O
that	O
the	O
number	O
of	O
relevance	O
vectors	O
is	O
smaller	O
than	O
the	O
number	O
of	O
support	O
vectors	O
on	O
the	O
same	O
problem	O
the	O
original	O
rvm	O
algorithm	O
was	O
not	O
able	O
to	O
exploit	O
the	O
sparsity	O
very	O
effectively	O
during	O
model	O
fitting	O
as	O
it	O
was	O
initialized	O
with	O
all	O
of	O
the	O
is	O
set	B
to	O
finite	O
values	O
meaning	O
that	O
all	O
of	O
the	O
basis	O
functions	O
contributed	O
to	O
the	O
model	O
however	O
careful	O
analysis	O
of	O
the	O
rvm	O
marginal	B
likelihood	B
by	O
faul	O
and	O
tipping	O
showed	O
that	O
one	O
can	O
carry	O
out	O
optimization	O
w	O
r	O
t	O
a	O
single	O
i	O
analytically	O
this	O
has	O
led	O
to	O
the	O
accelerated	O
training	O
algorithm	O
described	O
in	O
tipping	O
and	O
faul	O
which	O
starts	O
with	O
an	O
empty	O
model	O
all	O
is	O
set	B
to	O
infinity	O
and	O
adds	O
basis	O
functions	O
sequentially	O
as	O
the	O
number	O
of	O
relevance	O
vectors	O
is	O
much	O
less	O
than	O
the	O
number	O
of	O
training	O
cases	O
it	O
will	O
often	O
be	O
much	O
faster	O
to	O
train	O
and	O
make	O
predictions	O
using	O
a	O
rvm	O
than	O
a	O
non-sparse	O
relevance	O
vectors	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
gp	O
also	O
note	O
that	O
the	O
basis	O
functions	O
can	O
include	O
additional	O
hyperparameters	B
e	O
g	O
one	O
could	O
use	O
an	O
automatic	O
relevance	O
determination	B
form	O
of	O
basis	O
function	B
by	O
using	O
different	O
length-scales	O
on	O
different	O
dimensions	O
in	O
eq	O
these	O
additional	O
hyperparameters	B
could	O
also	O
be	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
the	O
use	O
of	O
a	O
degenerate	B
covariance	B
function	B
which	O
depends	O
on	O
the	O
data	O
imagine	O
a	O
test	O
point	O
x	O
which	O
lies	O
far	O
away	O
has	O
some	O
undesirable	O
effects	O
from	O
the	O
relevance	O
vectors	O
at	O
x	O
all	O
basis	O
functions	O
will	O
have	O
values	O
close	O
to	O
zero	O
and	O
since	O
no	O
basis	O
function	B
can	O
give	O
any	O
appreciable	O
signal	O
the	O
predictive	B
distribution	O
will	O
be	O
a	O
gaussian	O
with	O
a	O
mean	O
close	O
to	O
zero	O
and	O
variance	O
close	O
to	O
zero	O
to	O
the	O
inferred	O
noise	O
level	O
this	O
behaviour	O
is	O
undesirable	O
and	O
could	O
lead	O
to	O
dangerously	O
false	O
conclusions	O
if	O
the	O
x	O
is	O
far	O
from	O
the	O
relevance	O
vectors	O
then	O
the	O
model	O
shouldn	O
t	O
be	O
able	O
to	O
draw	O
strong	O
conclusions	O
about	O
the	O
output	O
are	O
extrapolating	O
but	O
the	O
predictive	B
uncertainty	O
becomes	O
very	O
small	O
this	O
is	O
the	O
opposite	O
behaviour	O
of	O
what	O
we	O
would	O
expect	O
from	O
a	O
reasonable	O
model	O
here	O
we	O
have	O
argued	O
that	O
for	O
localized	O
basis	O
functions	O
the	O
rvm	O
has	O
undesirable	O
properties	O
but	O
as	O
argued	O
in	O
rasmussen	O
and	O
qui	O
nonero-candela	O
it	O
is	O
actually	O
the	O
degeneracy	O
of	O
the	O
covariance	B
function	B
which	O
is	O
the	O
core	O
of	O
the	O
problem	O
although	O
the	O
work	O
of	O
rasmussen	O
and	O
qui	O
nonero-candela	O
goes	O
some	O
way	O
towards	O
fixing	O
the	O
problem	O
there	O
is	O
an	O
inherent	O
conflict	O
degeneracy	O
of	O
the	O
covariance	B
function	B
is	O
good	O
for	O
computational	O
reasons	O
but	O
bad	O
for	O
modelling	O
reasons	O
exercises	O
sis	O
vectors	O
f	O
pn	O
pn	O
we	O
motivate	O
the	O
fact	O
that	O
the	O
rkhs	O
norm	O
does	O
not	O
depend	O
on	O
the	O
density	O
px	O
using	O
a	O
finite-dimensional	O
analogue	O
consider	O
the	O
n-dimensional	O
vector	O
f	O
and	O
let	O
the	O
n	O
n	O
matrix	O
be	O
comprised	O
of	O
non-colinear	O
columns	O
n	O
then	O
f	O
can	O
be	O
expressed	O
as	O
a	O
linear	B
combination	O
of	O
these	O
ci	O
i	O
c	O
for	O
some	O
coefficients	O
let	O
the	O
s	O
be	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
k	O
w	O
r	O
t	O
a	O
diagonal	O
matrix	O
p	O
with	O
non-negative	O
entries	O
so	O
that	O
kp	O
where	O
is	O
a	O
diagonal	O
matrix	O
containing	O
the	O
eigenvalues	O
note	O
that	O
in	O
show	O
that	O
i	O
i	O
c	O
fk	O
and	O
thus	O
observe	O
that	O
fk	O
can	O
be	O
expressed	O
as	O
c	O
for	O
any	O
valid	O
p	O
and	O
corresponding	O
hint	O
you	O
may	O
find	O
it	O
useful	O
to	O
set	B
p	O
k	O
p	O
etc	O
plot	O
eq	O
as	O
a	O
function	B
of	O
f	O
for	O
different	O
values	O
of	O
c	O
show	O
that	O
there	O
is	O
no	O
value	O
of	O
c	O
and	O
which	O
makes	O
c	O
equal	O
to	O
for	O
all	O
values	O
of	O
f	O
try	O
setting	O
exp	O
as	O
suggested	O
in	O
sollich	O
and	O
observe	O
what	O
effect	O
this	O
has	O
show	O
that	O
the	O
predictive	B
mean	O
for	O
the	O
spline	O
covariance	B
gp	O
in	O
eq	O
is	O
a	O
linear	B
function	B
of	O
x	O
when	O
x	O
is	O
located	O
either	O
to	O
the	O
left	O
or	O
to	O
the	O
right	O
of	O
all	O
training	O
points	O
hint	O
consider	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
two	O
largest	O
eigenvalues	O
of	O
the	O
training	O
set	B
covariance	B
matrix	I
from	O
eq	O
in	O
the	O
vague	O
limit	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
theoretical	O
perspectives	O
this	O
chapter	O
covers	O
a	O
number	O
of	O
more	O
theoretical	O
issues	O
relating	O
to	O
gaussian	O
processes	O
in	O
section	O
we	O
saw	O
how	O
gpr	O
carries	O
out	O
a	O
linear	B
smoothing	O
of	O
the	O
datapoints	O
using	O
the	O
weight	B
function	B
the	O
form	O
of	O
the	O
weight	B
function	B
can	O
be	O
understood	O
in	O
terms	O
of	O
the	O
equivalent	B
kernel	B
which	O
is	O
discussed	O
in	O
section	O
as	O
one	O
gets	O
more	O
and	O
more	O
data	O
one	O
would	O
hope	O
that	O
the	O
gp	O
predictions	O
would	O
converge	O
to	O
the	O
true	O
underlying	O
predictive	B
distribution	O
this	O
question	O
of	O
consistency	B
is	O
reviewed	O
in	O
section	O
where	O
we	O
also	O
discuss	O
the	O
concepts	O
of	O
equivalence	O
and	O
orthogonality	O
of	O
gps	O
when	O
the	O
generating	O
process	O
for	O
the	O
data	O
is	O
assumed	O
to	O
be	O
a	O
gp	O
it	O
is	O
particularly	O
easy	O
to	O
obtain	O
results	O
for	O
learning	B
curves	O
which	O
describe	O
how	O
the	O
accuracy	O
of	O
the	O
predictor	O
increases	O
as	O
a	O
function	B
of	O
n	O
as	O
described	O
in	O
section	O
an	O
alternative	O
approach	O
to	O
the	O
analysis	O
of	O
generalization	B
error	I
is	O
provided	O
by	O
the	O
pac-bayesian	O
analysis	O
discussed	O
in	O
section	O
here	O
we	O
seek	O
to	O
relate	O
high	O
probability	O
the	O
error	O
observed	O
on	O
the	O
training	O
set	B
to	O
the	O
generalization	B
error	I
of	O
the	O
gp	O
predictor	O
gaussian	O
processes	O
are	O
just	O
one	O
of	O
the	O
many	O
methods	O
that	O
have	O
been	O
developed	O
for	O
supervised	B
learning	B
problems	O
in	O
section	O
we	O
compare	O
and	O
contrast	O
gp	O
predictors	O
with	O
other	O
supervised	B
learning	B
methods	O
the	O
equivalent	B
kernel	B
in	O
this	O
section	O
we	O
consider	O
regression	O
problems	O
we	O
have	O
seen	O
in	O
section	O
that	O
the	O
posterior	O
mean	O
for	O
gp	O
regression	O
can	O
be	O
obtained	O
as	O
the	O
function	B
which	O
minimizes	O
the	O
functional	B
jf	O
n	O
nx	O
where	O
kfkh	O
is	O
the	O
rkhs	O
norm	O
corresponding	O
to	O
kernel	B
k	O
our	O
goal	O
is	O
now	O
to	O
understand	O
the	O
behaviour	O
of	O
this	O
solution	O
as	O
n	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
let	O
y	O
be	O
the	O
probability	O
measure	B
from	O
which	O
the	O
data	O
pairs	O
yi	O
are	O
generated	O
observe	O
that	O
eh	O
nx	O
z	O
n	O
d	O
y	O
let	O
eyx	O
be	O
the	O
regression	O
function	B
corresponding	O
to	O
the	O
probability	O
then	O
writing	O
y	O
f	O
f	O
we	O
obtain	O
measure	B
the	O
variance	O
around	O
is	O
denoted	O
z	O
z	O
d	O
d	O
y	O
d	O
z	O
as	O
the	O
cross	O
term	O
vanishes	O
due	O
to	O
the	O
definition	O
of	O
as	O
the	O
second	O
term	O
on	O
the	O
right	O
hand	O
side	O
of	O
eq	O
is	O
independent	O
of	O
f	O
an	O
idealization	O
of	O
the	O
regression	O
problem	O
consists	O
of	O
minimizing	O
the	O
functional	B
z	O
j	O
n	O
n	O
d	O
the	O
form	O
of	O
the	O
minimizing	O
solution	O
is	O
most	O
easily	O
understood	O
in	O
terms	O
of	O
the	O
eigenfunctions	O
ix	O
of	O
the	O
kernel	B
k	O
w	O
r	O
t	O
to	O
wherer	O
ix	O
jxd	O
form	O
a	O
complete	O
orthonormal	O
basis	O
we	O
write	O
fx	O
i	O
ix	O
where	O
i	O
ixd	O
thus	O
x	O
ij	O
see	O
section	O
assuming	O
that	O
the	O
kernel	B
is	O
nondegenerate	B
so	O
that	O
the	O
s	O
fi	O
ix	O
similarly	O
x	O
j	O
n	O
n	O
i	O
f	O
i	O
i	O
this	O
is	O
readily	O
minimized	O
by	O
differentiation	O
w	O
r	O
t	O
each	O
fi	O
to	O
obtain	O
fi	O
i	O
i	O
nn	O
i	O
the	O
generalized	B
fourier	O
seriesp	O
nn	O
as	O
n	O
so	O
that	O
in	O
this	O
limit	O
we	O
would	O
notice	O
that	O
the	O
term	O
expect	O
that	O
fx	O
will	O
converge	O
to	O
there	O
are	O
two	O
caveats	O
we	O
have	O
assumed	O
that	O
is	O
sufficiently	O
well-behaved	O
so	O
that	O
it	O
can	O
be	O
represented	O
by	O
i	O
ix	O
and	O
we	O
assumed	O
that	O
the	O
kernel	B
is	O
nondegenerate	B
if	O
the	O
kernel	B
is	O
degenerate	B
a	O
polynomial	B
kernel	B
then	O
f	O
should	O
converge	O
to	O
the	O
best	O
approximation	O
to	O
within	O
the	O
span	O
of	O
the	O
s	O
in	O
section	O
we	O
will	O
say	O
more	O
about	O
rates	O
of	O
convergence	O
of	O
f	O
to	O
clearly	O
in	O
general	O
this	O
will	O
depend	O
on	O
the	O
smoothness	O
of	O
the	O
kernel	B
k	O
and	O
the	O
measure	B
y	O
from	O
a	O
bayesian	O
perspective	O
what	O
is	O
happening	O
is	O
that	O
the	O
prior	O
on	O
f	O
is	O
being	O
overwhelmed	O
by	O
the	O
data	O
as	O
n	O
looking	O
at	O
eq	O
we	O
also	O
see	O
n	O
n	O
i	O
then	O
fi	O
is	O
effectively	O
zero	O
this	O
means	O
that	O
we	O
cannot	O
find	O
that	O
if	O
out	O
about	O
the	O
coefficients	O
of	O
eigenfunctions	O
with	O
small	O
eigenvalues	O
until	O
we	O
get	O
sufficient	O
amounts	O
of	O
data	O
ferrari	O
trecate	O
et	O
al	O
demonstrated	O
this	O
by	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
equivalent	B
kernel	B
showing	O
that	O
regression	O
performance	O
of	O
a	O
certain	O
nondegenerate	B
gp	O
could	O
be	O
approximated	O
by	O
taking	O
the	O
first	O
m	O
eigenfunctions	O
where	O
m	O
was	O
chosen	O
so	O
that	O
m	O
using	O
the	O
fact	O
that	O
i	O
r	O
and	O
defining	O
nn	O
of	O
course	O
as	O
more	O
data	O
is	O
obtained	O
then	O
m	O
has	O
to	O
be	O
increased	O
nn	O
we	O
eff	O
obtain	O
x	O
z	O
h	O
x	O
i	O
i	O
ix	O
i	O
eff	O
z	O
fx	O
i	O
i	O
i	O
eff	O
ix	O
d	O
with	O
r	O
hnx	O
notice	O
that	O
in	O
the	O
limit	O
n	O
that	O
the	O
term	O
in	O
square	O
brackets	O
in	O
eq	O
is	O
the	O
equivalent	B
kernel	B
for	O
the	O
smoothing	O
problem	O
we	O
denote	O
it	O
by	O
hnx	O
notice	O
the	O
similarity	O
to	O
the	O
vector-valued	O
weight	B
function	B
hx	O
defined	O
in	O
section	O
the	O
difference	O
is	O
that	O
there	O
the	O
prediction	O
was	O
obtained	O
as	O
a	O
linear	B
combination	O
of	O
a	O
finite	O
number	O
of	O
observations	O
yi	O
with	O
weights	O
given	O
by	O
hix	O
while	O
here	O
we	O
have	O
a	O
noisy	O
function	B
yx	O
instead	O
eff	O
the	O
equivalent	B
kernel	B
tends	O
towards	O
the	O
delta	O
function	B
the	O
form	O
of	O
the	O
equivalent	B
kernel	B
given	O
in	O
eq	O
is	O
not	O
very	O
useful	O
in	O
practice	O
as	O
it	O
requires	O
knowledge	O
of	O
the	O
eigenvaluesfunctions	O
for	O
the	O
combination	O
of	O
k	O
and	O
however	O
in	O
the	O
case	O
of	O
stationary	O
kernels	O
we	O
can	O
use	O
fourier	O
methods	O
to	O
compute	O
the	O
equivalent	B
kernel	B
consider	O
the	O
functional	B
j	O
n	O
dx	O
where	O
has	O
dimensions	O
of	O
the	O
number	O
of	O
observations	O
per	O
unit	O
of	O
x-space	O
etc	O
as	O
appropriate	O
using	O
a	O
derivation	O
similar	O
to	O
eq	O
we	O
obtain	O
hs	O
sf	O
sf	O
n	O
s	O
f	O
n	O
where	O
sf	O
is	O
the	O
power	O
spectrum	O
of	O
the	O
kernel	B
k	O
the	O
term	O
n	O
corresponds	O
to	O
the	O
power	O
spectrum	O
of	O
a	O
white	O
noise	O
process	O
as	O
the	O
delta	O
function	B
covariance	B
function	B
of	O
white	O
noise	O
corresponds	O
to	O
a	O
constant	O
in	O
the	O
fourier	O
domain	O
this	O
analysis	O
is	O
known	O
as	O
wiener	O
filtering	O
see	O
e	O
g	O
papoulis	O
sec	O
equation	O
is	O
the	O
same	O
as	O
eq	O
except	O
that	O
the	O
discrete	O
eigenspectrum	O
has	O
been	O
replaced	O
by	O
a	O
continuous	O
one	O
as	O
can	O
be	O
observed	O
in	O
figure	O
the	O
equivalent	B
kernel	B
essentially	O
gives	O
a	O
weighting	O
to	O
the	O
observations	O
locally	O
around	O
x	O
thus	O
identifying	O
with	O
npx	O
we	O
can	O
obtain	O
an	O
approximation	O
to	O
the	O
equivalent	B
kernel	B
for	O
stationary	O
kernels	O
when	O
the	O
width	O
of	O
the	O
kernel	B
is	O
smaller	O
than	O
the	O
length-scale	B
of	O
variations	O
in	O
px	O
this	O
form	O
of	O
analysis	O
was	O
used	O
by	O
silverman	O
for	O
splines	B
in	O
one	O
dimension	O
some	O
specific	O
examples	O
of	O
equivalent	B
kernels	O
we	O
first	O
consider	O
the	O
ou	O
process	O
in	O
this	O
has	O
kr	O
exp	O
relative	O
to	O
our	O
previous	O
notation	O
and	O
r	O
x	O
and	O
power	O
spectrum	O
equivalent	B
kernel	B
wiener	O
filtering	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
ss	O
let	O
vn	O
n	O
using	O
eq	O
we	O
obtain	O
hs	O
where	O
this	O
again	O
has	O
the	O
form	O
of	O
fourier	B
transform	I
of	O
an	O
vn	O
e	O
in	O
ou	O
covariance	B
and	O
can	O
be	O
inverted	O
to	O
obtain	O
hr	O
particular	O
notice	O
that	O
as	O
n	O
increases	O
thus	O
vn	O
decreases	O
the	O
inverse	O
lengthscale	O
of	O
hr	O
increases	O
asymptotically	O
for	O
large	O
n	O
this	O
shows	O
that	O
the	O
width	O
of	O
equivalent	B
kernel	B
for	O
the	O
ou	O
covariance	B
function	B
will	O
scale	O
as	O
n	O
asymptotically	O
similarly	O
the	O
width	O
will	O
scale	O
as	O
px	O
asymptotically	O
a	O
similar	O
analysis	O
can	O
be	O
carried	O
out	O
for	O
the	O
gaussian	B
process	I
in	O
section	O
which	O
has	O
a	O
power	O
spectrum	O
it	O
is	O
in	O
the	O
mat	O
ern	O
class	O
with	O
in	O
this	O
case	O
we	O
can	O
show	O
the	O
fourier	O
relationships	O
given	O
by	O
papoulis	O
p	O
that	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	B
scales	O
as	O
n	O
asymptotically	O
analyzes	O
end-effects	O
if	O
the	O
domain	O
of	O
interest	O
is	O
a	O
bounded	O
open	O
interval	O
for	O
analysis	O
of	O
the	O
equivalent	B
kernel	B
has	O
also	O
been	O
carried	O
out	O
for	O
spline	O
models	O
silverman	O
gives	O
the	O
explicit	O
form	O
of	O
the	O
equivalent	B
kernel	B
in	O
the	O
case	O
of	O
a	O
one-dimensional	O
cubic	O
spline	O
to	O
the	O
regularizer	O
kp	O
thomas-agnan	O
gives	O
a	O
general	O
expression	O
for	O
the	O
equivalent	B
r	O
kernel	B
for	O
the	O
spline	O
regularizer	O
kp	O
in	O
one	O
dimension	O
and	O
also	O
the	O
regularizer	O
kp	O
in	O
two	O
dimensions	O
the	O
equivalent	B
kernel	B
is	O
sponding	O
to	O
a	O
roughness	O
penalty	O
of	O
r	O
dx	O
the	O
width	O
of	O
the	O
equivalent	B
silverman	O
has	O
also	O
shown	O
that	O
for	O
splines	B
of	O
order	O
m	O
in	O
will	O
scale	O
as	O
n	O
asymptotically	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
this	O
is	O
true	O
for	O
splines	B
in	O
d	O
dimensions	O
too	O
see	O
exercise	O
given	O
in	O
terms	O
of	O
the	O
kelvin	O
function	B
kei	O
et	O
al	O
stein	O
another	O
interesting	O
case	O
to	O
consider	O
is	O
the	O
squared	B
exponential	B
kernel	B
where	O
ss	O
exp	O
thus	O
hses	O
b	O
where	O
b	O
n	O
we	O
are	O
unaware	O
of	O
an	O
exact	O
result	O
in	O
this	O
case	O
but	O
the	O
following	O
approximation	O
due	O
to	O
sollich	O
and	O
williams	O
is	O
simple	O
but	O
effective	O
for	O
large	O
large	O
n	O
b	O
will	O
be	O
small	O
thus	O
for	O
small	O
s	O
we	O
have	O
that	O
hse	O
but	O
for	O
large	O
s	O
it	O
is	O
approximately	O
the	O
change	O
takes	O
place	O
around	O
the	O
point	O
sc	O
where	O
b	O
c	O
as	O
grows	O
quickly	O
with	O
s	O
the	O
transition	O
of	O
hse	O
between	O
and	O
can	O
be	O
expected	O
to	O
be	O
rapid	O
and	O
thus	O
be	O
well-approximated	O
by	O
a	O
step	O
function	B
by	O
using	O
the	O
standard	O
result	O
for	O
the	O
fourier	B
transform	I
of	O
the	O
step	O
function	B
we	O
obtain	O
c	O
i	O
e	O
hsex	O
scx	O
fact	O
that	O
hs	O
has	O
the	O
same	O
form	O
as	O
sf	O
is	O
particular	O
to	O
the	O
ou	O
covariance	B
function	B
and	O
is	O
not	O
generally	O
the	O
case	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
asymptotic	O
analysis	O
for	O
d	O
where	O
sincz	O
sinzz	O
a	O
similar	O
calculation	O
in	O
d	O
using	O
eq	O
gives	O
sc	O
hser	O
r	O
scr	O
notice	O
that	O
sc	O
scales	O
as	O
so	O
that	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	B
will	O
decay	O
very	O
slowly	O
as	O
n	O
increases	O
notice	O
that	O
the	O
plots	O
in	O
figure	O
show	O
the	O
sinc-type	O
shape	O
although	O
the	O
sidelobes	O
are	O
not	O
quite	O
as	O
large	O
as	O
would	O
be	O
predicted	O
by	O
the	O
sinc	O
curve	O
the	O
transition	O
is	O
smoother	O
than	O
a	O
step	O
function	B
in	O
fourier	O
space	O
so	O
there	O
is	O
less	O
ringing	O
asymptotic	O
analysis	O
in	O
this	O
section	O
we	O
consider	O
two	O
asymptotic	O
properties	O
of	O
gaussian	O
processes	O
consistency	B
and	O
equivalenceorthogonality	O
consistency	B
in	O
section	O
we	O
have	O
analyzed	O
the	O
asymptotics	O
of	O
gp	O
regression	O
and	O
have	O
seen	O
how	O
the	O
minimizer	O
of	O
the	O
functional	B
eq	O
converges	O
to	O
the	O
regression	O
function	B
as	O
n	O
we	O
now	O
broaden	O
the	O
focus	O
by	O
considering	O
loss	B
functions	O
other	O
than	O
squared	B
loss	B
and	O
the	O
case	O
where	O
we	O
work	O
directly	O
with	O
eq	O
rather	O
than	O
the	O
smoothed	O
version	O
eq	O
the	O
set	B
up	O
is	O
as	O
follows	O
let	O
l	O
be	O
a	O
pointwise	O
loss	B
function	B
consider	O
a	O
procedure	O
that	O
takes	O
training	O
data	O
d	O
and	O
this	O
loss	B
function	B
and	O
returns	O
a	O
function	B
fdx	O
for	O
a	O
measurable	O
function	B
f	O
the	O
risk	B
loss	B
is	O
defined	O
as	O
z	O
rlf	O
ly	O
fx	O
d	O
y	O
l	O
denote	O
the	O
function	B
that	O
minimizes	O
this	O
risk	B
for	O
squared	B
loss	B
f	O
let	O
f	O
lx	O
eyx	O
for	O
loss	B
with	O
classification	B
problems	O
we	O
choose	O
f	O
lx	O
to	O
be	O
the	O
class	O
c	O
at	O
x	O
such	O
that	O
pccx	O
pcjx	O
for	O
all	O
j	O
c	O
ties	O
arbitrarily	O
definition	O
we	O
will	O
say	O
that	O
a	O
procedure	O
that	O
returns	O
fd	O
is	O
consistent	O
for	O
a	O
given	O
measure	B
y	O
and	O
loss	B
function	B
l	O
if	O
consistency	B
rlfd	O
rlf	O
l	O
as	O
n	O
where	O
convergence	O
is	O
assessed	O
in	O
a	O
suitable	O
manner	O
e	O
g	O
in	O
probability	O
if	O
fdx	O
is	O
consistent	O
for	O
all	O
borel	O
probability	O
measures	O
y	O
then	O
it	O
is	O
said	O
to	O
be	O
versally	O
consistent	O
ing	O
fx	O
a	O
simple	O
example	O
of	O
a	O
consistent	O
procedure	O
is	O
the	O
kernel	B
regression	O
method	O
as	O
described	O
in	O
section	O
one	O
obtains	O
a	O
prediction	O
at	O
test	O
point	O
x	O
by	O
j	O
nadaraya-watson	B
estimator	I
let	O
h	O
be	O
the	O
width	O
of	O
the	O
kernel	B
and	O
d	O
be	O
the	O
dimension	O
of	O
the	O
input	O
wiyi	O
where	O
wi	O
ipn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
space	O
it	O
can	O
be	O
shown	O
that	O
under	O
suitable	O
regularity	O
conditions	O
if	O
h	O
and	O
nhd	O
as	O
n	O
then	O
the	O
procedure	O
is	O
consistent	O
see	O
e	O
g	O
orfi	O
et	O
al	O
theorem	O
for	O
the	O
regression	O
case	O
with	O
squared	B
loss	B
and	O
devroye	O
et	O
al	O
theorem	O
for	O
the	O
classification	B
case	O
using	O
loss	B
an	O
intuitive	O
understanding	O
of	O
this	O
result	O
can	O
be	O
obtained	O
by	O
noting	O
that	O
h	O
means	O
that	O
only	O
datapoints	O
very	O
close	O
to	O
x	O
will	O
contribute	O
to	O
the	O
prediction	O
bias	O
while	O
the	O
condition	O
nhd	O
means	O
that	O
a	O
large	O
number	O
of	O
datapoints	O
will	O
contribute	O
to	O
the	O
prediction	O
noisevariance	O
combination	O
of	O
eigenfunctionsp	O
it	O
will	O
first	O
be	O
useful	O
to	O
consider	O
why	O
we	O
might	O
hope	O
that	O
gpr	O
and	O
gpc	O
should	O
be	O
universally	O
consistent	O
as	O
discussed	O
in	O
section	O
the	O
key	O
property	O
is	O
that	O
a	O
non-degenerate	O
kernel	B
will	O
have	O
an	O
infinite	O
number	O
of	O
eigenfunctions	O
forming	O
an	O
orthonormal	O
set	B
thus	O
from	O
generalized	B
fourier	O
analysis	O
a	O
linear	B
ci	O
ix	O
should	O
be	O
able	O
to	O
represent	O
a	O
sufficiently	O
well-behaved	O
target	O
function	B
f	O
l	O
however	O
we	O
have	O
to	O
estimate	O
the	O
infinite	O
number	O
of	O
coefficients	O
from	O
the	O
noisy	O
observations	O
this	O
makes	O
it	O
clear	O
that	O
we	O
are	O
playing	O
a	O
game	O
involving	O
infinities	O
which	O
needs	O
to	O
be	O
played	O
with	O
care	O
and	O
there	O
are	O
some	O
results	O
and	O
freedman	O
freedman	O
gr	O
unwald	O
and	O
langford	O
which	O
show	O
that	O
in	O
certain	O
circumstances	O
bayesian	O
inference	O
in	O
infinite-dimensional	O
objects	O
can	O
be	O
inconsistent	O
however	O
there	O
are	O
some	O
positive	O
recent	O
results	O
on	O
the	O
consistency	B
of	O
gpr	O
and	O
gpc	O
choudhuri	O
et	O
al	O
show	O
that	O
for	O
the	O
binary	B
classification	B
case	O
under	O
certain	O
assumptions	O
gpc	O
is	O
consistent	O
the	O
assumptions	O
include	O
smoothness	O
on	O
the	O
mean	O
and	O
covariance	B
function	B
of	O
the	O
gp	O
smoothness	O
on	O
eyx	O
and	O
an	O
assumption	O
that	O
the	O
domain	O
is	O
a	O
bounded	O
subset	O
of	O
rd	O
their	O
result	O
holds	O
for	O
the	O
class	O
of	O
response	O
functions	O
which	O
are	O
c	O
d	O
f	O
s	O
of	O
a	O
unimodal	O
symmetric	O
density	O
this	O
includes	O
the	O
probit	B
and	O
logistic	B
functions	O
for	O
gpr	O
choi	O
and	O
schervish	O
show	O
that	O
for	O
a	O
one-dimensional	O
input	O
space	O
of	O
finite	O
length	O
under	O
certain	O
assumptions	O
consistency	B
holds	O
here	O
the	O
assumptions	O
again	O
include	O
smoothness	O
of	O
the	O
mean	O
and	O
covariance	B
function	B
of	O
the	O
gp	O
and	O
smoothness	O
of	O
eyx	O
an	O
additional	O
assumption	O
is	O
that	O
the	O
noise	O
has	O
a	O
normal	O
or	O
laplacian	O
distribution	O
an	O
unknown	O
variance	O
which	O
is	O
inferred	O
there	O
are	O
also	O
some	O
consistency	B
results	O
relating	O
to	O
the	O
functional	B
j	O
nf	O
n	O
n	O
nx	O
m	O
where	O
n	O
as	O
n	O
note	O
that	O
to	O
agree	O
with	O
our	O
previous	O
formulations	O
we	O
would	O
set	B
n	O
but	O
other	O
decay	O
rates	O
on	O
n	O
are	O
often	O
considered	O
in	O
the	O
splines	B
literature	O
cox	O
showed	O
that	O
for	O
regression	O
problems	O
using	O
the	O
regularizer	O
the	O
definitions	O
in	O
eq	O
consistency	B
can	O
be	O
obtained	O
under	O
certain	O
technical	O
conditions	O
cox	O
and	O
o	O
sullivan	O
considered	O
a	O
wide	O
range	O
of	O
problems	O
regression	O
problems	O
with	O
squared	B
loss	B
and	O
classification	B
using	O
logistic	B
loss	B
where	O
the	O
solution	O
is	O
obtained	O
by	O
minimizing	O
the	O
regularized	O
risk	B
using	O
a	O
spline	O
smoothness	O
term	O
l	O
h	O
h	O
is	O
the	O
rkhs	O
corresponding	O
to	O
the	O
spline	O
they	O
showed	O
that	O
if	O
f	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
asymptotic	O
analysis	O
regularizer	O
then	O
as	O
n	O
and	O
n	O
at	O
an	O
appropriate	O
rate	O
one	O
gets	O
convergence	O
of	O
fd	O
to	O
f	O
l	O
more	O
recently	O
zhang	O
theorem	O
has	O
shown	O
that	O
for	O
the	O
classification	B
problem	O
with	O
a	O
number	O
of	O
different	O
loss	B
functions	O
logistic	B
loss	B
hinge	B
loss	B
and	O
quadratic	O
loss	B
and	O
for	O
general	O
rkhss	O
with	O
a	O
nondegenerate	B
kernel	B
that	O
if	O
n	O
nn	O
and	O
y	O
is	O
sufficiently	O
regular	O
then	O
the	O
classification	B
error	O
of	O
fd	O
will	O
converge	O
to	O
the	O
bayes	O
optimal	B
error	O
in	O
probability	O
as	O
n	O
similar	O
results	O
have	O
also	O
been	O
obtained	O
by	O
steinwart	O
with	O
various	O
rates	O
on	O
the	O
decay	O
of	O
n	O
depending	O
on	O
the	O
smoothness	O
of	O
the	O
kernel	B
bartlett	O
et	O
al	O
have	O
characterized	O
the	O
loss	B
functions	O
that	O
lead	O
to	O
universal	O
consistency	B
above	O
we	O
have	O
focussed	O
on	O
regression	O
and	O
classification	B
problems	O
however	O
similar	O
analyses	O
can	O
also	O
be	O
given	O
for	O
other	O
problems	O
such	O
as	O
density	O
estimation	O
and	O
deconvolution	O
see	O
wahba	O
chs	O
for	O
references	O
also	O
we	O
have	O
discussed	O
consistency	B
using	O
a	O
fixed	O
decay	O
rate	O
for	O
n	O
however	O
it	O
is	O
also	O
possible	O
to	O
analyze	O
the	O
asymptotics	O
of	O
methods	O
where	O
n	O
is	O
set	B
in	O
a	O
data-dependent	O
way	O
e	O
g	O
by	O
see	O
wahba	O
sec	O
and	O
references	O
therein	O
for	O
further	O
details	O
consistency	B
is	O
evidently	O
a	O
desirable	O
property	O
of	O
supervised	B
learning	B
procedures	O
however	O
it	O
is	O
an	O
asymptotic	O
property	O
that	O
does	O
not	O
say	O
very	O
much	O
about	O
how	O
a	O
given	O
prediction	O
procedure	O
will	O
perform	O
on	O
a	O
particular	O
problem	O
with	O
a	O
given	O
dataset	O
for	O
instance	O
note	O
that	O
we	O
only	O
required	O
rather	O
general	O
properties	O
of	O
the	O
kernel	B
function	B
non-degeneracy	O
for	O
some	O
of	O
the	O
consistency	B
results	O
however	O
the	O
choice	O
of	O
the	O
kernel	B
can	O
make	O
a	O
huge	O
difference	O
to	O
how	O
a	O
procedure	O
performs	O
in	O
practice	O
some	O
analyses	O
related	O
to	O
this	O
issue	O
are	O
given	O
in	O
section	O
equivalence	O
and	O
orthogonality	O
the	O
presentation	O
in	O
this	O
section	O
is	O
based	O
mainly	O
on	O
stein	O
ch	O
for	O
two	O
probability	O
measures	O
and	O
defined	O
on	O
a	O
measurable	O
space	O
is	O
said	O
to	O
be	O
absolutely	O
continuous	O
w	O
r	O
t	O
if	O
for	O
all	O
a	O
f	O
implies	O
if	O
is	O
absolutely	O
continuous	O
w	O
r	O
t	O
and	O
is	O
absolutely	O
continuous	O
w	O
r	O
t	O
the	O
two	O
measures	O
are	O
said	O
to	O
be	O
equivalent	B
written	O
and	O
are	O
said	O
to	O
be	O
orthogonal	O
written	O
if	O
there	O
exists	O
an	O
a	O
f	O
such	O
that	O
and	O
that	O
in	O
this	O
case	O
we	O
have	O
and	O
where	O
ac	O
is	O
the	O
complement	O
of	O
a	O
the	O
dichotomy	O
theorem	O
for	O
gaussian	O
processes	O
to	O
hajek	O
and	O
independently	O
feldman	O
states	O
that	O
two	O
gaussian	O
processes	O
are	O
either	O
equivalent	B
or	O
orthogonal	O
equivalence	O
and	O
orthogonality	O
for	O
gaussian	O
measures	O
with	O
corresponding	O
probability	O
densities	O
can	O
be	O
characterized	O
in	O
terms	O
of	O
the	O
validation	O
is	O
discussed	O
in	O
section	O
section	O
for	O
background	O
on	O
measurable	O
spaces	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
z	O
symmetrized	O
kullback-leibler	B
divergence	I
klsym	O
between	O
them	O
given	O
by	O
the	O
measures	O
are	O
equivalent	B
if	O
klsym	O
and	O
orthogonal	O
otherwise	O
for	O
two	O
finite-dimensional	O
gaussian	O
distributions	O
n	O
and	O
n	O
we	O
have	O
sec	O
df	O
log	O
klsym	O
trk	O
k	O
k	O
this	O
expression	O
can	O
be	O
simplified	O
considerably	O
by	O
simultaneously	O
diagonalizing	O
and	O
two	O
finite-dimensional	O
gaussian	O
distributions	O
are	O
equivalent	B
if	O
the	O
null	O
spaces	O
of	O
their	O
covariance	B
matrices	O
coincide	O
and	O
are	O
orthogonal	O
otherwise	O
things	O
can	O
get	O
more	O
interesting	O
if	O
we	O
consider	O
infinite-dimensional	O
distributions	O
i	O
e	O
gaussian	O
processes	O
consider	O
some	O
closed	O
subset	O
r	O
rd	O
choose	O
some	O
finite	O
number	O
n	O
of	O
x-points	O
in	O
r	O
and	O
let	O
f	O
fn	O
denote	O
the	O
values	O
corresponding	O
to	O
these	O
inputs	O
we	O
consider	O
the	O
klsym-divergence	O
as	O
above	O
but	O
in	O
the	O
limit	O
n	O
klsym	O
can	O
now	O
diverge	O
if	O
the	O
rates	O
of	O
decay	O
of	O
the	O
eigenvalues	O
of	O
the	O
two	O
processes	O
are	O
not	O
the	O
same	O
for	O
example	O
consider	O
zero-mean	O
periodic	B
processes	O
with	O
period	O
where	O
the	O
eigenvalue	B
i	O
j	O
indicates	O
the	O
amount	O
of	O
power	O
in	O
the	O
sincos	O
terms	O
of	O
frequency	O
j	O
for	O
process	O
i	O
then	O
using	O
eq	O
we	O
have	O
klsym	O
j	O
j	O
j	O
x	O
also	O
p	O
some	O
corresponding	O
results	O
for	O
the	O
equivalence	O
or	O
orthogonality	O
of	O
non-periodic	O
gaussian	O
processes	O
are	O
given	O
in	O
stein	O
pp	O
stein	O
gives	O
an	O
example	O
of	O
two	O
equivalent	B
gaussian	O
processes	O
on	O
r	O
those	O
with	O
covariance	B
functions	O
exp	O
r	O
and	O
exp	O
is	O
easy	O
to	O
check	O
that	O
for	O
large	O
s	O
these	O
have	O
the	O
same	O
power	O
spectrum	O
we	O
now	O
turn	O
to	O
the	O
consequences	O
of	O
equivalence	O
for	O
the	O
model	O
selection	O
problem	O
suppose	O
that	O
we	O
know	O
that	O
either	O
gp	O
or	O
gp	O
is	O
the	O
correct	O
model	O
then	O
if	O
gp	O
gp	O
then	O
it	O
is	O
not	O
possible	O
to	O
determine	O
which	O
model	O
is	O
correct	O
with	O
probability	O
however	O
under	O
a	O
bayesian	O
setting	O
all	O
this	O
means	O
is	O
if	O
we	O
have	O
prior	O
probabilities	O
and	O
on	O
these	O
two	O
hypotheses	O
then	O
after	O
observing	O
some	O
data	O
d	O
the	O
posterior	O
probabilities	O
pgp	O
id	O
i	O
will	O
not	O
be	O
or	O
but	O
could	O
be	O
heavily	O
skewed	O
to	O
one	O
model	O
or	O
the	O
other	O
the	O
other	O
important	O
observation	O
is	O
to	O
consider	O
the	O
predictions	O
made	O
by	O
gp	O
or	O
gp	O
consider	O
the	O
case	O
where	O
gp	O
is	O
the	O
correct	O
model	O
and	O
gp	O
gp	O
then	O
stein	O
sec	O
shows	O
that	O
the	O
predictions	O
of	O
gp	O
are	O
asymptotically	O
optimal	B
in	O
the	O
sense	O
that	O
the	O
expected	O
relative	O
prediction	O
error	O
between	O
gp	O
and	O
gp	O
tends	O
to	O
as	O
n	O
under	O
some	O
technical	O
conditions	O
stein	O
s	O
corollary	O
shows	O
that	O
this	O
conclusion	O
remains	O
true	O
under	O
additive	O
noise	O
if	O
the	O
un-noisy	O
gps	O
are	O
equivalent	B
one	O
caveat	O
about	O
equivalence	O
is	O
although	O
the	O
predictions	O
of	O
gp	O
are	O
asymptotically	O
optimal	B
when	O
gp	O
is	O
the	O
correct	O
model	O
and	O
gp	O
gp	O
one	O
would	O
see	O
differing	O
predictions	O
for	O
finite	O
n	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
average-case	O
learning	B
curves	O
average-case	O
learning	B
curves	O
in	O
section	O
we	O
have	O
discussed	O
the	O
asymptotic	O
properties	O
of	O
gaussian	B
process	I
predictors	O
and	O
related	O
methods	O
in	O
this	O
section	O
we	O
will	O
say	O
more	O
about	O
the	O
speed	O
of	O
convergence	O
under	O
certain	O
specific	O
assumptions	O
our	O
goal	O
will	O
be	O
to	O
obtain	O
a	O
learning	B
curve	I
describing	O
the	O
generalization	B
error	I
as	O
a	O
function	B
of	O
the	O
training	O
set	B
size	O
n	O
this	O
is	O
an	O
average-case	O
analysis	O
averaging	O
over	O
the	O
choice	O
of	O
target	O
functions	O
from	O
a	O
gp	O
and	O
over	O
the	O
x	O
locations	O
of	O
the	O
training	O
points	O
in	O
more	O
detail	O
we	O
first	O
consider	O
a	O
target	O
function	B
f	O
drawn	O
from	O
a	O
gaussian	B
process	I
n	O
locations	O
are	O
chosen	O
to	O
make	O
observations	O
at	O
giving	O
rise	O
to	O
the	O
training	O
set	B
d	O
y	O
the	O
yis	O
are	O
noisy	O
observations	O
of	O
the	O
underlying	O
function	B
f	O
given	O
a	O
loss	B
function	B
l	O
which	O
measures	O
the	O
difference	O
between	O
the	O
prediction	O
for	O
f	O
and	O
f	O
itself	O
we	O
obtain	O
an	O
estimator	O
fd	O
for	O
f	O
below	O
we	O
will	O
use	O
the	O
squared	B
loss	B
so	O
that	O
the	O
posterior	O
mean	O
fdx	O
is	O
the	O
estimator	O
then	O
the	O
generalization	B
error	I
f	O
and	O
d	O
is	O
given	O
by	O
egdf	O
lfx	O
fdx	O
dx	O
generalization	B
error	I
z	O
z	O
as	O
this	O
is	O
an	O
expected	O
loss	B
it	O
is	O
technically	O
a	O
risk	B
but	O
the	O
term	O
generalization	B
error	I
is	O
commonly	O
used	O
egdf	O
depends	O
on	O
both	O
the	O
choice	O
of	O
f	O
and	O
on	O
x	O
that	O
y	O
depends	O
on	O
the	O
choice	O
of	O
f	O
and	O
also	O
on	O
the	O
noise	O
if	O
present	O
the	O
first	O
level	O
of	O
averaging	O
we	O
consider	O
is	O
over	O
functions	O
f	O
drawn	O
from	O
a	O
gp	O
prior	O
to	O
obtain	O
egx	O
egdfpf	O
df	O
it	O
will	O
turn	O
out	O
that	O
for	O
regression	O
problems	O
with	O
gaussian	B
process	I
priors	O
and	O
predictors	O
this	O
average	O
can	O
be	O
readily	O
calculated	O
the	O
second	O
level	O
of	O
averaging	O
assumes	O
that	O
the	O
x-locations	O
of	O
the	O
training	O
set	B
are	O
drawn	O
i	O
i	O
d	O
from	O
px	O
to	O
give	O
z	O
egn	O
pxn	O
dxn	O
a	O
plot	O
of	O
egn	O
against	O
n	O
is	O
known	O
as	O
a	O
learning	B
curve	I
learning	B
curve	I
rather	O
than	O
averaging	O
over	O
x	O
an	O
alternative	O
is	O
to	O
minimize	O
egx	O
w	O
r	O
t	O
x	O
this	O
gives	O
rise	O
to	O
the	O
optimal	B
experimental	I
design	I
problem	O
we	O
will	O
not	O
say	O
more	O
about	O
this	O
problem	O
here	O
but	O
it	O
has	O
been	O
subject	O
to	O
a	O
large	O
amount	O
of	O
investigation	O
an	O
early	O
paper	O
on	O
this	O
subject	O
is	O
by	O
ylvisaker	O
these	O
questions	O
have	O
been	O
addressed	O
both	O
in	O
the	O
statistical	O
literature	O
and	O
in	O
theoretical	O
numerical	O
analysis	O
for	O
the	O
latter	O
area	O
the	O
book	O
by	O
ritter	O
provides	O
a	O
useful	O
overview	O
we	O
now	O
proceed	O
to	O
develop	O
the	O
average-case	O
analysis	O
further	O
for	O
the	O
specific	O
case	O
of	O
gp	O
predictors	O
and	O
gp	O
priors	O
for	O
the	O
regression	O
case	O
using	O
squared	B
loss	B
let	O
f	O
be	O
drawn	O
from	O
a	O
zero-mean	O
gp	O
with	O
covariance	B
function	B
and	O
noise	O
level	O
similarly	O
the	O
predictor	O
assumes	O
a	O
zero-mean	O
process	O
but	O
covariance	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
at	O
a	O
particular	O
test	O
location	O
x	O
averaging	O
over	O
function	B
and	O
noise	O
level	O
f	O
we	O
have	O
efx	O
ef	O
x	O
efx	O
eyyk	O
where	O
kiy	O
kif	O
assumed	O
noise	O
the	O
above	O
expression	O
reduces	O
to	O
x	O
variance	O
of	O
the	O
gp	O
i	O
for	O
i	O
i	O
e	O
the	O
covariance	B
matrix	I
including	O
the	O
if	O
so	O
that	O
the	O
predictor	O
is	O
correctly	O
specified	O
then	O
the	O
predictive	B
z	O
z	O
averaging	O
the	O
error	O
over	O
px	O
we	O
obtain	O
egx	O
efx	O
x	O
dx	O
tr	O
k	O
z	O
tr	O
dx	O
z	O
dx	O
k	O
dx	O
for	O
some	O
choices	O
of	O
px	O
and	O
covariance	B
functions	O
these	O
integrals	B
will	O
be	O
analytically	O
tractable	O
reducing	O
the	O
computation	O
of	O
egx	O
to	O
a	O
n	O
n	O
matrix	O
computation	O
to	O
obtain	O
egn	O
we	O
need	O
to	O
perform	O
a	O
final	O
level	O
of	O
averaging	O
over	O
x	O
in	O
general	O
this	O
is	O
difficult	O
even	O
if	O
egx	O
can	O
be	O
computed	O
exactly	O
but	O
it	O
is	O
sometimes	O
possible	O
e	O
g	O
for	O
the	O
noise-free	O
ou	O
process	O
on	O
the	O
real	O
line	O
see	O
section	O
we	O
use	O
the	O
definition	O
kx	O
the	O
form	O
of	O
egx	O
can	O
be	O
simplified	O
considerably	O
if	O
we	O
express	O
the	O
covariance	B
functions	O
in	O
terms	O
of	O
their	O
eigenfunction	B
expansions	O
in	O
the	O
case	O
that	O
i	O
let	O
be	O
a	O
diagonal	O
matrix	O
of	O
the	O
eigenvalues	O
and	O
be	O
the	O
n	O
n	O
design	O
matrix	O
as	O
defined	O
in	O
section	O
then	O
from	O
eq	O
we	O
obtain	O
i	O
i	O
ix	O
andr	O
kx	O
ixpx	O
dx	O
egx	O
tr	O
tr	O
tr	O
ni	O
n	O
where	O
the	O
second	O
line	O
follows	O
through	O
the	O
use	O
of	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
directly	O
if	O
we	O
use	O
eq	O
as	O
shown	O
in	O
sollich	O
or	O
opper	O
and	O
vivarelli	O
using	O
the	O
fact	O
that	O
ex	O
ni	O
a	O
na	O
ve	O
approximation	O
would	O
replace	O
inside	O
the	O
trace	O
with	O
its	O
expectation	O
in	O
fact	O
opper	O
and	O
vivarelli	O
showed	O
that	O
this	O
gives	O
a	O
lower	O
bound	O
so	O
that	O
egn	O
tr	O
n	O
n	O
i	O
nx	O
i	O
n	O
n	O
i	O
examining	O
the	O
asymptotics	O
of	O
eq	O
we	O
see	O
that	O
for	O
each	O
eigenvalue	B
where	O
i	O
nn	O
onto	O
the	O
bound	O
on	O
the	O
generalization	B
error	I
as	O
we	O
saw	O
nn	O
we	O
add	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
pac-bayesian	O
analysis	O
p	O
in	O
section	O
more	O
eigenfunctions	O
come	O
into	O
play	O
as	O
n	O
increases	O
so	O
the	O
rate	O
of	O
decay	O
of	O
egn	O
is	O
slower	O
than	O
sollich	O
derives	O
a	O
number	O
of	O
more	O
accurate	O
approximations	O
to	O
the	O
learning	B
curve	I
than	O
eq	O
for	O
the	O
noiseless	O
case	O
with	O
there	O
is	O
a	O
simple	O
lower	O
bound	O
egn	O
i	O
due	O
to	O
micchelli	O
and	O
wahba	O
this	O
bound	O
is	O
obtained	O
by	O
demonstrating	O
that	O
the	O
optimal	B
n	O
pieces	O
of	O
information	O
are	O
the	O
projections	O
of	O
the	O
random	O
function	B
f	O
onto	O
the	O
first	O
n	O
eigenfunctions	O
as	O
observations	O
which	O
simply	O
consist	O
of	O
function	B
evaluations	O
will	O
not	O
in	O
general	O
provide	O
such	O
information	O
this	O
is	O
a	O
lower	O
bound	O
plaskota	O
generalized	B
this	O
result	O
to	O
give	O
a	O
bound	O
on	O
the	O
learning	B
curve	I
if	O
the	O
observations	O
are	O
noisy	O
some	O
asymptotic	O
results	O
for	O
the	O
learning	B
curves	O
are	O
known	O
for	O
example	O
in	O
ritter	O
sec	O
covariance	B
functions	O
obeying	O
sacks-ylvisaker	O
of	O
order	O
r	O
in	O
are	O
considered	O
he	O
shows	O
that	O
for	O
an	O
optimal	B
sampling	O
of	O
the	O
input	O
space	O
the	O
generalization	B
error	I
goes	O
as	O
on	O
for	O
the	O
noisy	O
problem	O
similar	O
rates	O
can	O
also	O
be	O
found	O
in	O
sollich	O
for	O
random	O
designs	O
for	O
the	O
noise-free	O
case	O
ritter	O
p	O
gives	O
the	O
rate	O
as	O
on	O
one	O
can	O
examine	O
the	O
learning	B
curve	I
not	O
only	O
asymptotically	O
but	O
also	O
for	O
small	O
n	O
where	O
typically	O
the	O
curve	O
has	O
a	O
roughly	O
linear	B
decrease	O
with	O
n	O
williams	O
and	O
vivarelli	O
explained	O
this	O
behaviour	O
by	O
observing	O
that	O
the	O
introduction	O
of	O
a	O
datapoint	O
reduces	O
the	O
variance	O
locally	O
around	O
a	O
stationary	O
covariance	B
function	B
the	O
addition	O
of	O
another	O
datapoint	O
at	O
will	O
also	O
create	O
a	O
hole	O
there	O
and	O
so	O
on	O
with	O
only	O
a	O
small	O
number	O
of	O
datapoints	O
it	O
is	O
likely	O
that	O
these	O
holes	O
will	O
be	O
far	O
apart	O
so	O
their	O
contributions	O
will	O
add	O
thus	O
explaining	O
the	O
initial	O
linear	B
trend	O
sollich	O
has	O
also	O
investigated	O
the	O
mismatched	O
case	O
where	O
this	O
can	O
give	O
rise	O
to	O
a	O
rich	O
variety	O
of	O
behaviours	O
in	O
the	O
learning	B
curves	O
including	O
plateaux	O
stein	O
chs	O
has	O
also	O
carried	O
out	O
some	O
analysis	O
of	O
the	O
mismatched	O
case	O
although	O
we	O
have	O
focused	O
on	O
gp	O
regression	O
with	O
squared	B
loss	B
we	O
note	O
that	O
malzahn	O
and	O
opper	O
have	O
developed	O
more	O
general	O
techniques	O
that	O
can	O
be	O
used	O
to	O
analyze	O
learning	B
curves	O
for	O
other	O
situations	O
such	O
as	O
gp	O
classification	B
pac-bayesian	O
analysis	O
in	O
section	O
we	O
gave	O
an	O
average-case	O
analysis	O
of	O
generalization	B
taking	O
the	O
average	O
with	O
respect	O
to	O
a	O
gp	O
prior	O
over	O
functions	O
in	O
this	O
section	O
we	O
present	O
a	O
different	O
kind	O
of	O
analysis	O
within	O
the	O
probably	O
approximately	O
correct	O
framework	O
due	O
to	O
valiant	O
seeger	B
has	O
presented	O
a	O
pacbayesian	O
analysis	O
of	O
generalization	B
in	O
gaussian	B
process	I
classifiers	O
and	O
we	O
get	O
to	O
this	O
in	O
a	O
number	O
of	O
stages	O
we	O
first	O
present	O
an	O
introduction	O
to	O
the	O
pac	O
framework	O
then	O
describe	O
the	O
pac-bayesian	O
approach	O
speaking	O
a	O
stochastic	O
process	O
which	O
possesses	O
r	O
ms	O
derivatives	O
but	O
not	O
r	O
is	O
said	O
to	O
satisfy	O
sacks-ylvisaker	O
conditions	O
of	O
order	O
r	O
in	O
this	O
gives	O
rise	O
to	O
a	O
spectrum	O
i	O
i	O
asymptotically	O
the	O
ou	O
process	O
obeys	O
sacks-ylvisaker	O
conditions	O
of	O
order	O
pac	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
and	O
then	O
finally	O
the	O
application	O
to	O
gp	O
classification	B
our	O
presentation	O
is	O
based	O
mainly	O
on	O
seeger	B
the	O
pac	O
framework	O
consider	O
a	O
fixed	O
measure	B
y	O
given	O
a	O
loss	B
function	B
l	O
there	O
exists	O
a	O
function	B
which	O
minimizes	O
the	O
expected	O
risk	B
by	O
running	O
a	O
learning	B
algorithm	O
on	O
a	O
data	O
set	B
d	O
of	O
size	O
n	O
drawn	O
i	O
i	O
d	O
from	O
y	O
we	O
obtain	O
an	O
estimate	O
fd	O
of	O
p	O
which	O
attains	O
an	O
expected	O
risk	B
rlfd	O
we	O
are	O
not	O
able	O
to	O
evaluate	O
rlfd	O
as	O
p	O
we	O
do	O
not	O
know	O
however	O
we	O
do	O
have	O
access	O
to	O
the	O
empirical	O
distribution	O
of	O
i	O
xi	O
yi	O
and	O
can	O
compute	O
the	O
empirical	O
the	O
training	O
set	B
y	O
n	O
i	O
lyi	O
fdxi	O
because	O
the	O
training	O
set	B
had	O
been	O
used	O
to	O
risk	B
rlfd	O
compute	O
fd	O
we	O
would	O
expect	O
rlfd	O
to	O
underestimate	O
and	O
the	O
aim	O
of	O
the	O
pac	O
analysis	O
is	O
to	O
provide	O
a	O
bound	O
on	O
rlfd	O
based	O
on	O
rlfd	O
n	O
a	O
pac	O
bound	O
has	O
the	O
following	O
format	O
pdrlfd	O
rlfd	O
gapfdd	O
where	O
pd	O
denotes	O
the	O
probability	O
distribution	O
of	O
datasets	O
drawn	O
i	O
i	O
d	O
from	O
y	O
and	O
is	O
called	O
the	O
confidence	O
parameter	O
the	O
bound	O
states	O
that	O
averaged	B
over	O
draws	O
of	O
the	O
dataset	O
d	O
from	O
y	O
rlfd	O
does	O
not	O
exceed	O
the	O
sum	O
of	O
rlfd	O
and	O
the	O
gap	O
term	O
with	O
probability	O
of	O
at	O
least	O
the	O
accounts	O
for	O
the	O
probably	O
in	O
pac	O
and	O
the	O
approximately	O
derives	O
from	O
the	O
fact	O
that	O
the	O
gap	O
term	O
is	O
positive	O
for	O
all	O
n	O
it	O
is	O
important	O
to	O
note	O
that	O
pac	O
analyses	O
are	O
distribution-free	O
i	O
e	O
eq	O
must	O
hold	O
for	O
any	O
measure	B
there	O
are	O
two	O
kinds	O
of	O
pac	O
bounds	O
depending	O
on	O
whether	O
gapfdd	O
actually	O
depends	O
on	O
the	O
particular	O
sample	O
d	O
than	O
on	O
simple	O
statistics	O
like	O
n	O
bounds	O
that	O
do	O
depend	O
on	O
d	O
are	O
called	O
data	O
dependent	O
and	O
those	O
that	O
do	O
not	O
are	O
called	O
data	O
independent	O
the	O
pac-bayesian	O
bounds	O
given	O
below	O
are	O
data	O
dependent	O
has	O
mean	O
m	O
an	O
estimate	O
of	O
m	O
is	O
given	O
by	O
the	O
sample	O
mean	O
x	O
p	O
it	O
is	O
important	O
to	O
understand	O
the	O
interpretation	O
of	O
a	O
pac	O
bound	O
and	O
to	O
clarify	O
this	O
we	O
first	O
consider	O
a	O
simpler	O
case	O
of	O
statistical	O
inference	O
we	O
are	O
given	O
a	O
dataset	O
d	O
xn	O
drawn	O
i	O
i	O
d	O
from	O
a	O
distribution	O
that	O
i	O
xin	O
under	O
certain	O
assumptions	O
we	O
can	O
obtain	O
put	O
bounds	O
on	O
the	O
sampling	O
distribution	O
p	O
xm	O
which	O
relates	O
to	O
the	O
choice	O
of	O
dataset	O
d	O
however	O
if	O
we	O
wish	O
to	O
perform	O
probabilistic	B
inference	O
for	O
m	O
we	O
need	O
to	O
combine	O
p	O
xm	O
with	O
a	O
prior	O
distribution	O
pm	O
and	O
use	O
bayes	O
theorem	O
to	O
obtain	O
the	O
the	O
situation	O
is	O
similar	O
somewhat	O
more	O
complex	O
for	O
pac	O
bounds	O
as	O
these	O
concern	O
the	O
sampling	O
distribution	O
of	O
the	O
expected	O
and	O
empirical	O
risks	O
of	O
fd	O
w	O
r	O
t	O
d	O
is	O
also	O
possible	O
to	O
consider	O
pac	O
analyses	O
of	O
other	O
empirical	O
quantities	O
such	O
as	O
the	O
cross-validation	B
error	O
section	O
which	O
do	O
not	O
have	O
this	O
bias	O
introductory	O
treatments	O
of	O
frequentist	O
statistics	O
the	O
logical	O
hiatus	O
of	O
going	O
from	O
the	O
sampling	O
distribution	O
to	O
inference	O
on	O
the	O
parameter	O
of	O
interest	O
is	O
often	O
not	O
well	O
explained	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
pac-bayesian	O
analysis	O
we	O
might	O
wish	O
to	O
make	O
a	O
conditional	B
statement	O
like	O
pdrlfd	O
r	O
gapfdd	O
rlfd	O
r	O
where	O
r	O
is	O
a	O
small	O
value	O
but	O
such	O
a	O
statement	O
cannot	O
be	O
inferred	O
directly	O
from	O
the	O
pac	O
bound	O
this	O
is	O
because	O
the	O
gap	O
might	O
be	O
heavily	O
anti-correlated	O
with	O
rlfd	O
so	O
that	O
the	O
gap	O
is	O
large	O
when	O
the	O
empirical	O
risk	B
is	O
small	O
pac	O
bounds	O
are	O
sometimes	O
used	O
to	O
carry	O
out	O
model	O
selection	O
given	O
a	O
learning	B
machine	O
which	O
depends	O
on	O
a	O
or	O
continuous	O
parameter	O
vector	O
one	O
can	O
seek	O
to	O
minimize	O
the	O
generalization	B
bound	O
as	O
a	O
function	B
of	O
however	O
this	O
procedure	O
may	O
not	O
be	O
well-justified	O
if	O
the	O
generalization	B
bounds	O
are	O
loose	O
let	O
the	O
slack	O
denote	O
the	O
difference	O
between	O
the	O
value	O
of	O
the	O
bound	O
and	O
the	O
generalization	B
error	I
the	O
danger	O
of	O
choosing	O
to	O
minimize	O
the	O
bound	O
is	O
that	O
if	O
the	O
slack	O
depends	O
on	O
then	O
the	O
value	O
of	O
that	O
minimizes	O
the	O
bound	O
may	O
be	O
very	O
different	O
from	O
the	O
value	O
of	O
that	O
minimizes	O
the	O
generalization	B
error	I
see	O
seeger	B
sec	O
for	O
further	O
discussion	O
pac-bayesian	O
analysis	O
we	O
now	O
consider	O
a	O
bayesian	O
set	B
up	O
with	O
a	O
prior	O
distribution	O
pw	O
over	O
the	O
parameters	O
w	O
and	O
a	O
posterior	O
distribution	O
qw	O
speaking	O
the	O
analysis	O
does	O
not	O
require	O
qw	O
to	O
be	O
the	O
posterior	O
distribution	O
just	O
some	O
other	O
distribution	O
but	O
in	O
practice	O
we	O
will	O
consider	O
q	O
to	O
be	O
an	O
posterior	O
distribution	O
we	O
also	O
limit	O
our	O
discussion	O
to	O
binary	B
classification	B
with	O
labels	O
although	O
more	O
general	O
cases	O
can	O
be	O
considered	O
see	O
seeger	B
sec	O
r	O
qf	O
x	O
and	O
the	O
predictive	B
classifier	O
outputs	B
sgnqf	O
the	O
predictive	B
distribution	O
for	O
f	O
at	O
a	O
test	O
point	O
x	O
given	O
qw	O
is	O
qf	O
the	O
gibbs	B
classifier	O
has	O
also	O
been	O
studied	O
in	O
learning	B
theory	O
given	O
a	O
test	O
point	O
x	O
one	O
draws	O
a	O
sample	O
w	O
from	O
qw	O
and	O
predicts	O
the	O
label	O
using	O
sgnfx	O
w	O
the	O
main	O
reason	O
for	O
introducing	O
the	O
gibbs	B
classifier	O
here	O
is	O
that	O
the	O
pacbayesian	O
theorems	O
given	O
below	O
apply	O
to	O
gibbs	B
classifiers	O
for	O
a	O
given	O
parameter	O
vector	O
w	O
giving	O
rise	O
to	O
a	O
classifier	O
cx	O
w	O
the	O
ex	O
pected	O
risk	B
and	O
empirical	O
risk	B
are	O
given	O
by	O
rlw	O
ly	O
cx	O
w	O
d	O
y	O
rlw	O
lyi	O
cxi	O
w	O
as	O
the	O
gibbs	B
classifier	O
draws	O
samples	O
from	O
qw	O
we	O
consider	O
the	O
averaged	B
risks	O
rlq	O
rlwqw	O
dw	O
rlq	O
rlwqw	O
dw	O
nx	O
n	O
z	O
z	O
z	O
theorem	O
s	O
pac-bayesian	O
theorem	O
for	O
any	O
probability	O
measures	O
p	O
and	O
q	O
over	O
w	O
and	O
for	O
any	O
bounded	O
loss	B
function	B
l	O
for	O
which	O
ly	O
cx	O
for	O
any	O
classifier	O
c	O
and	O
input	O
x	O
we	O
have	O
n	O
s	O
pd	O
rlq	O
rlq	O
klqp	O
log	O
log	O
n	O
q	O
o	O
predictive	B
classifier	O
gibbs	B
classifier	O
mcallester	B
s	O
pac-bayesian	O
theorem	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
the	O
proof	O
can	O
be	O
found	O
in	O
mcallester	B
the	O
kullback-leibler	B
divergence	I
klqp	O
is	O
defined	O
in	O
section	O
an	O
example	O
of	O
a	O
loss	B
function	B
which	O
obeys	O
the	O
conditions	O
of	O
the	O
theorem	O
is	O
the	O
loss	B
for	O
the	O
special	O
case	O
of	O
loss	B
seeger	B
gives	O
the	O
following	O
tighter	O
seeger	B
s	O
pacbayesian	O
theorem	O
n	O
bound	O
theorem	O
s	O
pac-bayesian	O
theorem	O
for	O
any	O
distribution	O
over	O
x	O
and	O
for	O
any	O
probability	O
measures	O
p	O
and	O
q	O
over	O
w	O
the	O
following	O
bound	O
holds	O
for	O
i	O
i	O
d	O
samples	O
drawn	O
from	O
the	O
data	O
distribution	O
log	O
n	O
here	O
klber	O
is	O
the	O
kl	O
divergence	O
between	O
two	O
bernoulli	O
distributions	O
in	O
eq	O
thus	O
the	O
theorem	O
bounds	O
high	O
probability	O
the	O
kl	O
divergence	O
between	O
rlq	O
and	O
rlq	O
o	O
pd	O
klber	O
rlqrlq	O
n	O
q	O
the	O
pac-bayesian	O
theorems	O
above	O
refer	O
to	O
a	O
gibbs	B
classifier	O
if	O
we	O
are	O
interested	O
in	O
the	O
predictive	B
classifier	O
sgnqf	O
then	O
seeger	B
shows	O
that	O
if	O
qf	O
is	O
symmetric	O
about	O
its	O
mean	O
then	O
the	O
expected	O
risk	B
of	O
the	O
predictive	B
classifier	O
is	O
less	O
than	O
twice	O
the	O
expected	O
risk	B
of	O
the	O
gibbs	B
classifier	O
however	O
this	O
result	O
is	O
based	O
on	O
a	O
simple	O
bounding	O
argument	O
and	O
in	O
practice	O
one	O
would	O
expect	O
that	O
the	O
predictive	B
classifier	O
will	O
usually	O
give	O
better	O
performance	O
than	O
the	O
gibbs	B
classifier	O
recent	O
work	O
by	O
meir	O
and	O
zhang	O
provides	O
some	O
pac	O
bounds	O
directly	O
for	O
bayesian	O
algorithms	O
the	O
predictive	B
classifier	O
whose	O
predictions	O
are	O
made	O
on	O
the	O
basis	O
of	O
a	O
data-dependent	O
posterior	O
distribution	O
pac-bayesian	O
analysis	O
of	O
gp	O
classification	B
to	O
apply	O
this	O
bound	O
to	O
the	O
gaussian	B
process	I
case	O
we	O
need	O
to	O
compute	O
the	O
kl	O
divergence	O
klqp	O
between	O
the	O
posterior	O
distribution	O
qw	O
and	O
the	O
prior	O
distribution	O
pw	O
although	O
this	O
could	O
be	O
considered	O
w	O
r	O
t	O
the	O
weight	B
vector	I
w	O
in	O
the	O
eigenfunction	B
expansion	O
in	O
fact	O
it	O
turns	O
out	O
to	O
be	O
more	O
convenient	O
to	O
consider	O
the	O
latent	O
function	B
value	O
fx	O
at	O
every	O
possible	O
point	O
in	O
the	O
input	O
space	O
x	O
as	O
the	O
parameter	O
we	O
divide	O
this	O
infinite	O
vector	O
into	O
two	O
parts	O
the	O
values	O
corresponding	O
to	O
the	O
training	O
points	O
xn	O
denoted	O
f	O
and	O
those	O
at	O
the	O
remaining	O
points	O
in	O
x-space	O
test	O
points	O
f	O
the	O
key	O
observation	O
is	O
that	O
all	O
methods	O
we	O
have	O
described	O
for	O
dealing	O
with	O
gp	O
classification	B
problems	O
produce	O
a	O
posterior	O
approximation	O
qfy	O
which	O
is	O
defined	O
at	O
the	O
training	O
points	O
is	O
an	O
approximation	O
for	O
laplace	O
s	O
method	O
and	O
for	O
ep	O
mcmc	O
methods	O
sample	O
from	O
the	O
exact	O
posterior	O
this	O
posterior	O
over	O
f	O
is	O
then	O
extended	O
to	O
the	O
test	O
points	O
by	O
setting	O
qf	O
f	O
qfypf	O
of	O
course	O
for	O
the	O
prior	O
distribution	O
we	O
have	O
a	O
similar	O
decomposition	O
pf	O
f	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
comparison	O
with	O
other	O
supervised	B
learning	B
methods	O
pfpf	O
thus	O
the	O
kl	O
divergence	O
is	O
given	O
by	O
z	O
z	O
klqp	O
qfypf	O
log	O
qfypf	O
qfy	O
log	O
qfy	O
pfpf	O
df	O
df	O
pf	O
df	O
as	O
shown	O
e	O
g	O
in	O
seeger	B
notice	O
that	O
this	O
has	O
reduced	O
a	O
rather	O
scary	O
infinite-dimensional	O
integration	O
to	O
a	O
more	O
manageable	O
n-dimensional	O
integration	O
in	O
the	O
case	O
that	O
qfy	O
is	O
gaussian	O
for	O
the	O
laplace	O
and	O
ep	O
approximations	O
this	O
kl	O
divergence	O
can	O
be	O
computed	O
using	O
eq	O
for	O
the	O
laplace	B
approximation	I
with	O
pf	O
n	O
k	O
and	O
qfy	O
n	O
f	O
a	O
this	O
gives	O
klqp	O
fk	O
f	O
log	O
log	O
seeger	B
has	O
evaluated	O
the	O
quality	O
of	O
the	O
bound	O
produced	O
by	O
the	O
pacbayesian	O
method	O
for	O
a	O
laplace	O
gpc	O
on	O
the	O
task	O
of	O
discriminating	O
handwritten	O
and	O
from	O
the	O
mnist	O
handwritten	O
digits	O
he	O
reserved	O
a	O
test	O
set	B
of	O
examples	O
and	O
used	O
training	O
sets	O
of	O
size	O
and	O
the	O
classifications	O
were	O
replicated	O
ten	O
times	O
using	O
draws	O
of	O
the	O
training	O
sets	O
from	O
a	O
pool	O
of	O
examples	O
we	O
quote	O
example	O
results	O
for	O
n	O
where	O
the	O
training	O
error	O
was	O
the	O
test	O
error	O
was	O
and	O
the	O
pac-bayesian	O
bound	O
on	O
the	O
generalization	B
error	I
for	O
was	O
figures	O
denote	O
a	O
confidence	O
interval	O
the	O
classification	B
results	O
are	O
for	O
the	O
gibbs	B
classifier	O
for	O
the	O
predictive	B
classifier	O
the	O
test	O
error	O
rate	O
was	O
thus	O
the	O
generalization	B
error	I
is	O
around	O
while	O
the	O
pac	O
bound	O
is	O
many	O
pac	O
bounds	O
struggle	O
to	O
predict	O
error	O
rates	O
below	O
so	O
this	O
is	O
an	O
impressive	O
and	O
highly	O
non-trivial	O
result	O
further	O
details	O
and	O
experiments	O
can	O
be	O
found	O
in	O
seeger	B
comparison	O
with	O
other	O
supervised	O
learn	O
ing	O
methods	O
the	O
focus	O
of	O
this	O
book	O
is	O
on	O
gaussian	B
process	I
methods	O
for	O
supervised	B
learning	B
however	O
there	O
are	O
many	O
other	O
techniques	O
available	O
for	O
supervised	B
learning	B
such	O
as	O
linear	B
regression	I
logistic	B
regression	I
decision	O
trees	O
neural	O
networks	O
support	B
vector	I
machines	O
kernel	B
smoothers	O
k-nearest	O
neighbour	O
classifiers	O
etc	O
and	O
we	O
need	O
to	O
consider	O
the	O
relative	O
strengths	O
and	O
weaknesses	O
of	O
these	O
approaches	O
supervised	B
learning	B
is	O
an	O
inductive	B
process	O
given	O
a	O
finite	O
training	O
set	B
we	O
wish	O
to	O
infer	O
a	O
function	B
f	O
that	O
makes	O
predictions	O
for	O
all	O
possible	O
input	O
values	O
the	O
additional	O
assumptions	O
made	O
by	O
the	O
learning	B
algorithm	O
are	O
known	O
as	O
its	O
inductive	B
bias	O
e	O
g	O
mitchell	O
p	O
sometimes	O
these	O
assumptions	O
are	O
explicit	O
but	O
for	O
other	O
algorithms	O
for	O
decision	O
tree	O
induction	O
they	O
can	O
be	O
rather	O
more	O
implicit	O
httpyann	O
lecun	O
comexdbmnist	O
inductive	B
bias	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
however	O
for	O
all	O
their	O
variety	O
supervised	B
learning	B
algorithms	O
are	O
based	O
on	O
the	O
idea	O
that	O
similar	O
input	O
patterns	O
will	O
usually	O
give	O
rise	O
to	O
similar	O
outputs	B
output	O
distributions	O
and	O
it	O
is	O
the	O
precise	O
notion	O
of	O
similarity	O
that	O
differentiates	O
the	O
algorithms	O
for	O
example	O
some	O
algorithms	O
may	O
do	O
feature	O
selection	O
and	O
decide	O
that	O
there	O
are	O
input	O
dimensions	O
that	O
are	O
irrelevant	O
to	O
the	O
predictive	B
task	O
some	O
algorithms	O
may	O
construct	O
new	O
features	O
out	O
of	O
those	O
provided	O
and	O
measure	B
similarity	O
in	O
this	O
derived	O
space	O
as	O
we	O
have	O
seen	O
many	O
regression	O
techniques	O
can	O
be	O
seen	O
as	O
linear	B
smoothers	O
section	O
and	O
these	O
techniques	O
vary	O
in	O
the	O
definition	O
of	O
the	O
weight	B
function	B
that	O
is	O
used	O
one	O
important	O
distinction	O
between	O
different	O
learning	B
algorithms	O
is	O
how	O
they	O
relate	O
to	O
the	O
question	O
of	O
universal	O
consistency	B
section	O
for	O
example	O
a	O
linear	B
regression	I
model	O
will	O
be	O
inconsistent	O
if	O
the	O
function	B
that	O
minimizes	O
the	O
risk	B
cannot	O
be	O
represented	O
by	O
a	O
linear	B
function	B
of	O
the	O
inputs	O
in	O
general	O
a	O
model	O
with	O
a	O
finite-dimensional	O
parameter	O
vector	O
will	O
not	O
be	O
universally	O
consistent	O
examples	O
of	O
such	O
models	O
are	O
linear	B
regression	I
and	O
logistic	B
regression	I
with	O
a	O
finite-dimensional	O
feature	O
vector	O
and	O
neural	O
networks	O
with	O
a	O
fixed	O
number	O
of	O
hidden	O
units	O
in	O
contrast	O
to	O
these	O
parametric	B
models	O
we	O
have	O
non-parametric	B
models	O
as	O
k-nearest	O
neighbour	O
classifiers	O
kernel	B
smoothers	O
and	O
gaussian	O
processes	O
and	O
svms	O
with	O
nondegenerate	B
kernels	O
which	O
do	O
not	O
compress	O
the	O
training	O
data	O
into	O
a	O
finite-dimensional	O
parameter	O
vector	O
an	O
intermediate	O
position	O
is	O
taken	O
by	O
semi-parametric	B
models	O
such	O
as	O
neural	O
networks	O
where	O
the	O
number	O
of	O
hidden	O
units	O
k	O
is	O
allowed	O
to	O
increase	O
as	O
n	O
increases	O
in	O
this	O
case	O
universal	O
consistency	B
results	O
can	O
be	O
obtained	O
et	O
al	O
ch	O
under	O
certain	O
technical	O
conditions	O
and	O
growth	O
rates	O
on	O
k	O
although	O
universal	O
consistency	B
is	O
a	O
good	O
thing	O
it	O
does	O
not	O
necessarily	O
mean	O
that	O
we	O
should	O
only	O
consider	O
procedures	O
that	O
have	O
this	O
property	O
for	O
example	O
if	O
on	O
a	O
specific	O
problem	O
we	O
knew	O
that	O
a	O
linear	B
regression	I
model	O
was	O
consistent	O
for	O
that	O
problem	O
then	O
it	O
would	O
be	O
very	O
natural	O
to	O
use	O
it	O
in	O
the	O
s	O
there	O
was	O
a	O
large	O
surge	O
in	O
interest	O
in	O
artificial	O
neural	O
networks	O
which	O
are	O
feedforward	O
networks	O
consisting	O
of	O
an	O
input	O
layer	O
followed	O
by	O
one	O
or	O
more	O
layers	O
of	O
non-linear	O
transformations	O
of	O
weighted	O
combinations	O
of	O
the	O
activity	O
from	O
previous	O
layers	O
and	O
an	O
output	O
layer	O
one	O
reason	O
for	O
this	O
surge	O
of	O
interest	O
was	O
the	O
use	O
of	O
the	O
backpropagation	O
algorithm	O
for	O
training	O
anns	O
initial	O
excitement	O
centered	O
around	O
that	O
fact	O
that	O
training	O
non-linear	O
networks	O
was	O
possible	O
but	O
later	O
the	O
focus	O
came	O
onto	O
the	O
generalization	B
performance	O
of	O
anns	O
and	O
how	O
to	O
deal	O
with	O
questions	O
such	O
as	O
how	O
many	O
layers	O
of	O
hidden	O
units	O
to	O
use	O
how	O
many	O
units	O
there	O
should	O
be	O
in	O
each	O
layer	O
and	O
what	O
type	O
of	O
non-linearities	O
should	O
be	O
used	O
etc	O
for	O
a	O
particular	O
ann	O
the	O
search	O
for	O
a	O
good	O
set	B
of	O
weights	O
for	O
a	O
given	O
training	O
set	B
is	O
complicated	O
by	O
the	O
fact	O
that	O
there	O
can	O
be	O
local	O
optima	O
in	O
the	O
optimization	O
problem	O
this	O
can	O
cause	O
significant	O
difficulties	O
in	O
practice	O
in	O
contrast	O
for	O
gaussian	B
process	I
regression	I
and	O
classification	B
the	O
posterior	O
for	O
the	O
latent	O
variables	O
is	O
convex	O
neural	O
networks	O
bayesian	O
neural	O
networks	O
one	O
approach	O
to	O
the	O
problems	O
raised	O
above	O
was	O
to	O
put	O
anns	O
in	O
a	O
bayesian	O
framework	O
as	O
developed	O
by	O
mackay	O
and	O
neal	O
this	O
gives	O
rise	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
comparison	O
with	O
other	O
supervised	B
learning	B
methods	O
to	O
posterior	O
distributions	O
over	O
weights	O
for	O
a	O
given	O
architecture	O
and	O
the	O
use	O
of	O
the	O
marginal	B
likelihood	B
section	O
for	O
model	O
comparison	O
and	O
selection	O
in	O
contrast	O
to	O
gaussian	B
process	I
regression	I
the	O
marginal	B
likelihood	B
for	O
a	O
given	O
ann	O
model	O
is	O
not	O
analytically	O
tractable	O
and	O
thus	O
approximation	O
techniques	O
such	O
as	O
the	O
laplace	B
approximation	I
and	O
markov	B
chain	I
monte	I
carlo	I
methods	O
have	O
to	O
be	O
used	O
neal	O
s	O
observation	O
that	O
certain	O
anns	O
with	O
one	O
hidden	O
layer	O
converge	O
to	O
a	O
gaussian	B
process	I
prior	O
over	O
functions	O
section	O
led	O
us	O
to	O
consider	O
gps	O
as	O
alternatives	O
to	O
anns	O
mackay	O
sec	O
raises	O
an	O
interesting	O
question	O
whether	O
in	O
moving	O
from	O
neural	O
networks	O
to	O
gaussian	O
processes	O
we	O
have	O
thrown	O
the	O
baby	O
out	O
with	O
the	O
bathwater	O
this	O
question	O
arises	O
from	O
his	O
statements	O
that	O
neural	O
networks	O
were	O
meant	O
to	O
be	O
intelligent	O
models	O
that	O
discovered	O
features	O
and	O
patterns	O
in	O
data	O
while	O
gaussian	O
processes	O
are	O
simply	O
smoothing	O
devices	O
our	O
answer	O
to	O
this	O
question	O
is	O
that	O
gps	O
give	O
us	O
a	O
computationally	O
attractive	O
method	O
for	O
dealing	O
with	O
the	O
smoothing	O
problem	O
for	O
a	O
given	O
kernel	B
and	O
that	O
issues	O
of	O
feature	O
discovery	O
etc	O
can	O
be	O
addressed	O
through	O
methods	O
to	O
select	O
the	O
kernel	B
function	B
chapter	O
for	O
more	O
details	O
on	O
how	O
to	O
do	O
this	O
note	O
that	O
using	O
a	O
distance	O
function	B
with	O
m	O
having	O
a	O
low-rank	O
form	O
m	O
as	O
in	O
eq	O
features	O
are	O
described	O
by	O
the	O
columns	O
of	O
however	O
some	O
of	O
the	O
non-convexity	O
of	O
the	O
neural	B
network	I
optimization	O
problem	O
now	O
returns	O
as	O
optimizing	O
the	O
marginal	B
likelihood	B
in	O
terms	O
of	O
the	O
parameters	O
of	O
m	O
may	O
well	O
have	O
local	O
optima	O
as	O
we	O
have	O
seen	O
from	O
chapters	O
and	O
linear	B
regression	I
and	O
logistic	B
regression	I
with	O
gaussian	O
priors	O
on	O
the	O
parameters	O
are	O
a	O
natural	O
starting	O
point	O
for	O
the	O
development	O
of	O
gaussian	B
process	I
regression	I
and	O
gaussian	B
process	I
classification	B
however	O
we	O
need	O
to	O
enhance	O
the	O
flexibility	O
of	O
these	O
models	O
and	O
the	O
use	O
of	O
non-degenerate	O
kernels	O
opens	O
up	O
the	O
possibility	O
of	O
universal	O
consistency	B
kernel	B
smoothers	O
and	O
classifiers	O
have	O
been	O
described	O
in	O
sections	O
and	O
at	O
a	O
high	O
level	O
there	O
are	O
similarities	O
between	O
gp	O
prediction	O
and	O
these	O
methods	O
as	O
a	O
kernel	B
is	O
placed	O
on	O
every	O
training	O
example	O
and	O
the	O
prediction	O
is	O
obtained	O
through	O
a	O
weighted	O
sum	O
of	O
the	O
kernel	B
functions	O
but	O
the	O
details	O
of	O
the	O
prediction	O
and	O
the	O
underlying	O
logic	O
differ	O
note	O
that	O
the	O
gp	O
prediction	O
view	O
gives	O
us	O
much	O
more	O
e	O
g	O
error	O
bars	O
on	O
the	O
predictions	O
and	O
the	O
use	O
of	O
the	O
marginal	B
likelihood	B
to	O
set	B
parameters	O
in	O
the	O
kernel	B
section	O
on	O
the	O
other	O
hand	O
the	O
computational	O
problem	O
that	O
needs	O
to	O
be	O
solved	O
to	O
carry	O
out	O
gp	O
prediction	O
is	O
more	O
demanding	O
than	O
that	O
for	O
simple	O
kernel-based	O
methods	O
kernel	B
smoothers	O
and	O
classifiers	O
are	O
non-parametric	B
methods	O
and	O
consistency	B
can	O
often	O
be	O
obtained	O
under	O
conditions	O
where	O
the	O
width	O
h	O
of	O
the	O
kernel	B
tends	O
to	O
zero	O
while	O
nhd	O
the	O
equivalent	B
kernel	B
analysis	O
of	O
gp	O
regression	O
shows	O
that	O
there	O
are	O
quite	O
close	O
connections	O
between	O
the	O
kernel	B
regression	O
method	O
and	O
gpr	O
but	O
note	O
that	O
the	O
equivalent	B
kernel	B
automatically	O
reduces	O
its	O
width	O
as	O
n	O
grows	O
in	O
contrast	O
the	O
decay	O
of	O
h	O
has	O
to	O
be	O
imposed	O
for	O
kernel	B
regression	O
also	O
for	O
some	O
kernel	B
smoothing	O
and	O
classification	B
algorithms	O
the	O
width	O
of	O
the	O
kernel	B
is	O
increased	O
in	O
areas	O
of	O
low	O
observation	O
density	O
for	O
example	O
this	O
would	O
occur	O
in	O
algorithms	O
that	O
consider	O
the	O
k	O
nearest	O
neighbours	O
of	O
a	O
test	O
point	O
again	O
notice	O
from	O
the	O
equivalent	B
kernel	B
analysis	O
that	O
the	O
width	O
linear	B
and	O
logistic	B
regression	I
kernel	B
smoothers	O
and	O
classifiers	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
regularization	B
networks	O
splines	B
svms	O
and	O
rvms	O
of	O
the	O
equivalent	B
kernel	B
is	O
larger	O
in	O
regions	O
of	O
low	O
density	O
although	O
the	O
exact	O
dependence	O
on	O
the	O
density	O
will	O
depend	O
on	O
the	O
kernel	B
used	O
the	O
similarities	O
and	O
differences	O
between	O
gp	O
prediction	O
and	O
regularization	B
networks	O
splines	B
svms	O
and	O
rvms	O
have	O
been	O
discussed	O
in	O
chapter	O
appendix	O
learning	B
curve	I
for	O
the	O
ornstein	O
uhlenbeck	O
process	O
we	O
now	O
consider	O
the	O
calculation	O
of	O
the	O
learning	B
curve	I
for	O
the	O
ou	O
covariance	B
function	B
kr	O
exp	O
on	O
the	O
interval	O
assuming	O
that	O
the	O
training	O
x	O
s	O
are	O
drawn	O
from	O
the	O
uniform	O
distribution	O
our	O
treatment	O
is	O
based	O
on	O
williams	O
and	O
vivarelli	O
we	O
first	O
calculate	O
egx	O
for	O
a	O
fixed	O
design	O
and	O
then	O
integrate	O
over	O
possible	O
designs	O
to	O
obtain	O
egn	O
in	O
the	O
absence	O
of	O
noise	O
the	O
ou	O
process	O
is	O
markovian	O
discussed	O
in	O
appendix	O
b	O
and	O
exercise	O
we	O
consider	O
the	O
interval	O
with	O
points	O
xn	O
xn	O
placed	O
on	O
this	O
interval	O
also	O
let	O
and	O
due	O
to	O
the	O
markovian	O
nature	O
of	O
the	O
process	O
the	O
prediction	O
at	O
a	O
test	O
point	O
x	O
depends	O
only	O
on	O
the	O
function	B
values	O
of	O
the	O
training	O
points	O
immediately	O
to	O
the	O
left	O
and	O
right	O
of	O
x	O
thus	O
in	O
the	O
i-th	O
interval	O
from	O
the	O
bounding	O
points	O
are	O
xi	O
and	O
let	O
this	O
interval	O
have	O
length	O
i	O
using	O
eq	O
we	O
have	O
z	O
z	O
nx	O
xi	O
egx	O
f	O
dx	O
f	O
dx	O
where	O
we	O
have	O
in	O
interval	O
i	O
i	O
n	O
that	O
where	O
k	O
is	O
the	O
gram	B
matrix	I
f	O
is	O
the	O
predictive	B
variance	O
at	O
input	O
x	O
using	O
the	O
markovian	O
property	O
f	O
kxk	O
k	O
i	O
and	O
kx	O
is	O
the	O
corresponding	O
vector	O
of	O
length	O
thus	O
k	O
i	O
k	O
i	O
k	O
i	O
k	O
i	O
k	O
where	O
i	O
i	O
and	O
f	O
i	O
z	O
thus	O
xi	O
ikx	O
x	O
f	O
i	O
xi	O
i	O
i	O
thanks	O
manfred	O
opper	O
for	O
pointing	O
out	O
that	O
the	O
upper	O
bound	O
developed	O
in	O
williams	O
and	O
vivarelli	O
is	O
exact	O
for	O
the	O
noise-free	O
ou	O
process	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
exercises	O
where	O
z	O
k	O
z	O
kzk	O
zdz	O
for	O
kr	O
exp	O
these	O
equations	O
reduce	O
to	O
e	O
e	O
and	O
e	O
thus	O
f	O
i	O
ie	O
i	O
e	O
i	O
this	O
calculation	O
is	O
not	O
correct	O
in	O
the	O
first	O
and	O
last	O
intervals	O
where	O
only	O
f	O
and	O
xn	O
are	O
relevant	O
for	O
the	O
interval	O
we	O
have	O
that	O
and	O
thus	O
z	O
f	O
e	O
xdx	O
z	O
xi	O
z	O
and	O
a	O
similar	O
result	O
holds	O
forr	O
f	O
putting	O
this	O
all	O
together	O
we	O
obtain	O
xn	O
egx	O
n	O
e	O
n	O
n	O
ie	O
i	O
e	O
i	O
choosing	O
a	O
regular	O
grid	O
so	O
that	O
n	O
and	O
i	O
for	O
i	O
n	O
it	O
is	O
straightforward	O
to	O
show	O
exercise	O
that	O
eg	O
scales	O
as	O
on	O
in	O
agreement	O
with	O
the	O
general	O
sacks-ylvisaker	O
result	O
p	O
when	O
it	O
is	O
recalled	O
that	O
the	O
ou	O
process	O
obeys	O
sacks-ylvisaker	O
conditions	O
of	O
order	O
a	O
similar	O
calculation	O
is	O
given	O
in	O
plaskota	O
sec	O
for	O
the	O
wiener	B
process	I
on	O
that	O
this	O
is	O
also	O
markovian	O
but	O
non-stationary	O
we	O
have	O
now	O
worked	O
out	O
the	O
generalization	B
error	I
for	O
a	O
fixed	O
design	O
x	O
however	O
to	O
compute	O
egn	O
we	O
need	O
to	O
average	O
egx	O
over	O
draws	O
of	O
x	O
from	O
the	O
uniform	O
distribution	O
the	O
theory	O
of	O
order	O
statistics	O
david	O
eq	O
tells	O
us	O
that	O
p	O
for	O
all	O
the	O
i	O
i	O
n	O
taking	O
the	O
expectation	O
of	O
egx	O
then	O
turns	O
into	O
the	O
problem	O
of	O
evaluating	O
the	O
one-dimensional	O
integrals	B
r	O
e	O
p	O
and	O
r	O
e	O
e	O
exercise	O
asks	O
you	O
to	O
compute	O
these	O
integrals	B
numerically	O
exercises	O
consider	O
a	O
spline	O
regularizer	O
with	O
sf	O
c	O
we	O
noted	O
in	O
section	O
this	O
is	O
not	O
strictly	O
a	O
power	O
spectrum	O
as	O
the	O
spline	O
is	O
an	O
improper	O
prior	O
but	O
it	O
can	O
be	O
used	O
as	O
a	O
power	O
spectrum	O
in	O
eq	O
for	O
the	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
theoretical	O
perspectives	O
purposes	O
of	O
this	O
analysis	O
the	O
equivalent	B
kernel	B
corresponding	O
to	O
this	O
spline	O
is	O
given	O
by	O
z	O
is	O
x	O
hx	O
n	O
by	O
changing	O
variables	O
in	O
the	O
integration	O
to	O
ds	O
where	O
c	O
show	O
that	O
the	O
width	O
of	O
hx	O
scales	O
as	O
n	O
equation	O
gives	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	B
for	O
a	O
spline	O
regularizer	O
show	O
that	O
is	O
only	O
finite	O
if	O
d	O
transform	O
the	O
integration	O
to	O
polar	O
coordinates	O
this	O
observation	O
was	O
made	O
by	O
p	O
whittle	O
in	O
the	O
discussion	O
of	O
silverman	O
and	O
shows	O
the	O
need	O
for	O
the	O
condition	O
d	O
for	O
spline	O
smoothing	O
computer	O
exercise	O
space	O
n	O
points	O
out	O
evenly	O
along	O
the	O
interval	O
n	O
to	O
be	O
even	O
so	O
that	O
one	O
of	O
the	O
sample	O
points	O
falls	O
at	O
calculate	O
the	O
weight	B
function	B
section	O
corresponding	O
to	O
gaussian	B
process	I
regression	I
with	O
a	O
particular	O
covariance	B
function	B
and	O
noise	O
level	O
and	O
plot	O
this	O
for	O
the	O
point	O
x	O
now	O
compute	O
the	O
equivalent	B
kernel	B
corresponding	O
to	O
the	O
covariance	B
function	B
e	O
g	O
the	O
examples	O
in	O
section	O
plot	O
this	O
on	O
the	O
same	O
axes	O
and	O
compare	O
results	O
hint	O
recall	O
that	O
the	O
equivalent	B
kernel	B
is	O
defined	O
in	O
terms	O
of	O
integration	O
eq	O
so	O
that	O
there	O
will	O
be	O
a	O
scaling	O
factor	O
of	O
hint	O
if	O
you	O
wish	O
to	O
use	O
large	O
n	O
use	O
the	O
ngrid	O
method	O
described	O
in	O
section	O
consider	O
egx	O
as	O
given	O
in	O
eq	O
and	O
choose	O
a	O
regular	O
grid	O
design	O
x	O
so	O
that	O
n	O
and	O
i	O
for	O
i	O
n	O
show	O
that	O
egx	O
scales	O
as	O
on	O
asymptotically	O
hint	O
when	O
expanding	O
exp	O
i	O
be	O
sure	O
to	O
extend	O
the	O
expansion	O
to	O
sufficient	O
order	O
compute	O
numerically	O
the	O
expectation	O
of	O
egx	O
eq	O
over	O
random	O
designs	O
for	O
the	O
ou	O
process	O
example	O
discussed	O
in	O
section	O
make	O
use	O
of	O
the	O
fact	O
eq	O
that	O
p	O
for	O
all	O
the	O
i	O
i	O
n	O
investigate	O
the	O
scaling	O
behaviour	O
of	O
egn	O
w	O
r	O
t	O
n	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
approximation	O
methods	O
for	O
large	O
datasets	O
as	O
we	O
have	O
seen	O
in	O
the	O
preceding	O
chapters	O
a	O
significant	O
problem	O
with	O
gaussian	B
process	I
prediction	O
is	O
that	O
it	O
typically	O
scales	O
as	O
for	O
large	O
problems	O
n	O
both	O
storing	O
the	O
gram	B
matrix	I
and	O
solving	O
the	O
associated	O
linear	B
systems	O
are	O
prohibitive	O
on	O
modern	O
workstations	O
this	O
boundary	O
can	O
be	O
pushed	O
further	O
by	O
using	O
high-performance	O
computers	O
an	O
extensive	O
range	O
of	O
proposals	O
have	O
been	O
suggested	O
to	O
deal	O
with	O
this	O
problem	O
below	O
we	O
divide	O
these	O
into	O
five	O
parts	O
in	O
section	O
we	O
consider	O
reducedrank	O
approximations	O
to	O
the	O
gram	B
matrix	I
in	O
section	O
a	O
general	O
strategy	O
for	O
greedy	O
approximations	O
is	O
described	O
in	O
section	O
we	O
discuss	O
various	O
methods	O
for	O
approximating	O
the	O
gp	O
regression	O
problem	O
for	O
fixed	O
hyperparameters	B
in	O
section	O
we	O
describe	O
various	O
methods	O
for	O
approximating	O
the	O
gp	O
classification	B
problem	O
for	O
fixed	O
hyperparameters	B
and	O
in	O
section	O
we	O
describe	O
methods	O
to	O
approximate	O
the	O
marginal	B
likelihood	B
and	O
its	O
derivatives	O
many	O
not	O
all	O
of	O
these	O
methods	O
use	O
a	O
subset	O
of	O
size	O
m	O
n	O
of	O
the	O
training	O
examples	O
reduced-rank	O
approximations	O
of	O
the	O
gram	B
matrix	I
in	O
the	O
gp	O
regression	O
problem	O
we	O
need	O
to	O
invert	O
the	O
matrix	O
k	O
ni	O
at	O
least	O
to	O
solve	O
a	O
linear	B
system	O
niv	O
y	O
for	O
v	O
if	O
the	O
matrix	O
k	O
has	O
rank	O
q	O
that	O
it	O
can	O
be	O
represented	O
in	O
the	O
form	O
k	O
qq	O
where	O
q	O
is	O
an	O
n	O
q	O
matrix	O
then	O
this	O
matrix	O
inversion	O
can	O
be	O
speeded	O
up	O
using	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
as	O
niq	O
qq	O
notice	O
that	O
the	O
inversion	O
of	O
an	O
n	O
n	O
matrix	O
has	O
now	O
been	O
transformed	O
to	O
the	O
inversion	O
of	O
a	O
q	O
q	O
nin	O
n	O
in	O
n	O
q	O
numerical	O
reasons	O
this	O
is	O
not	O
the	O
best	O
way	O
to	O
solve	O
such	O
a	O
linear	B
system	O
but	O
it	O
does	O
illustrate	O
the	O
savings	O
that	O
can	O
be	O
obtained	O
with	O
reduced-rank	O
representations	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
in	O
the	O
case	O
that	O
the	O
kernel	B
is	O
derived	O
from	O
an	O
explicit	O
feature	O
expansion	O
with	O
n	O
features	O
then	O
the	O
gram	B
matrix	I
will	O
have	O
rank	O
minn	O
n	O
so	O
that	O
exploitation	O
of	O
this	O
structure	O
will	O
be	O
beneficial	O
if	O
n	O
n	O
even	O
if	O
the	O
kernel	B
is	O
non-degenerate	O
it	O
may	O
happen	O
that	O
it	O
has	O
a	O
fast-decaying	O
eigenspectrum	O
e	O
g	O
section	O
so	O
that	O
a	O
reduced-rank	O
approximation	O
will	O
be	O
accurate	O
if	O
k	O
is	O
not	O
of	O
rank	O
n	O
we	O
can	O
still	O
consider	O
reduced-rank	O
approximations	O
to	O
k	O
the	O
optimal	B
reduced-rank	O
approximation	O
of	O
k	O
w	O
r	O
t	O
the	O
frobenius	B
norm	O
eq	O
is	O
uq	O
qu	O
q	O
where	O
q	O
is	O
the	O
diagonal	O
matrix	O
of	O
the	O
leading	O
q	O
eigenvalues	O
of	O
k	O
and	O
uq	O
is	O
the	O
matrix	O
of	O
the	O
corresponding	O
orthonormal	O
eigenvectors	O
and	O
van	O
loan	O
theorem	O
unfortunately	O
this	O
is	O
of	O
limited	O
interest	O
in	O
practice	O
as	O
computing	O
the	O
eigendecomposition	O
is	O
an	O
operation	O
however	O
it	O
does	O
suggest	O
that	O
if	O
we	O
can	O
more	O
cheaply	O
obtain	O
an	O
approximate	O
eigendecomposition	O
then	O
this	O
may	O
give	O
rise	O
to	O
a	O
useful	O
reducedrank	O
approximation	O
to	O
k	O
kmm	O
we	O
now	O
consider	O
selecting	O
a	O
subset	O
i	O
of	O
the	O
n	O
datapoints	O
set	B
i	O
has	O
size	O
m	O
n	O
the	O
remaining	O
n	O
m	O
datapoints	O
form	O
the	O
set	B
r	O
a	O
mnemonic	O
i	O
is	O
for	O
the	O
included	O
datapoints	O
and	O
r	O
is	O
for	O
the	O
remaining	O
points	O
we	O
sometimes	O
call	O
the	O
included	O
set	B
the	O
active	O
set	B
without	O
loss	B
of	O
generality	O
we	O
assume	O
that	O
the	O
datapoints	O
are	O
ordered	O
so	O
that	O
set	B
i	O
comes	O
first	O
thus	O
k	O
can	O
be	O
partitioned	O
as	O
k	O
kn	O
mm	O
kn	O
mn	O
m	O
the	O
top	O
m	O
n	O
block	O
will	O
also	O
be	O
referred	O
to	O
as	O
kmn	O
and	O
its	O
transpose	O
as	O
knm	O
in	O
section	O
we	O
saw	O
how	O
to	O
approximate	O
the	O
eigenfunctions	O
of	O
a	O
kernel	B
using	O
the	O
nystr	O
om	O
method	O
we	O
can	O
now	O
apply	O
the	O
same	O
idea	O
to	O
approximating	O
the	O
eigenvaluesvectors	O
of	O
k	O
we	O
compute	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
kmm	O
and	O
denote	O
them	O
these	O
are	O
extended	O
to	O
all	O
n	O
points	O
using	O
eq	O
to	O
give	O
and	O
i	O
i	O
kmn	O
m	O
i	O
n	O
m	O
r	O
m	O
i	O
un	O
i	O
i	O
m	O
i	O
m	O
knmum	O
i	O
n	O
i	O
has	O
been	O
chosen	O
so	O
that	O
un	O
in	O
general	O
we	O
have	O
where	O
the	O
scaling	O
of	O
un	O
a	O
choice	O
of	O
how	O
many	O
of	O
the	O
approximate	O
eigenvaluesvectors	O
to	O
include	O
in	O
our	O
approximation	O
of	O
k	O
choosing	O
the	O
first	O
p	O
we	O
get	O
k	O
pp	O
un	O
i	O
i	O
i	O
un	O
i	O
i	O
below	O
we	O
will	O
set	B
p	O
m	O
to	O
obtain	O
nystr	O
om	O
approximation	O
k	O
knmk	O
mmkmn	O
using	O
equations	O
and	O
which	O
we	O
call	O
the	O
nystr	O
om	O
approximation	O
to	O
k	O
computation	O
of	O
k	O
takes	O
time	O
as	O
the	O
eigendecomposition	O
of	O
kmm	O
is	O
and	O
the	O
computation	O
of	O
each	O
un	O
is	O
omn	O
fowlkes	O
et	O
al	O
have	O
applied	O
the	O
nystr	O
om	O
method	O
to	O
approximate	O
the	O
top	O
few	O
eigenvectors	O
in	O
a	O
computer	O
vision	O
problem	O
where	O
the	O
matrices	O
in	O
question	O
are	O
larger	O
than	O
in	O
size	O
i	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
reduced-rank	O
approximations	O
of	O
the	O
gram	B
matrix	I
the	O
nystr	O
om	O
approximation	O
has	O
been	O
applied	O
above	O
to	O
approximate	O
the	O
elements	O
of	O
k	O
however	O
using	O
the	O
approximation	O
for	O
the	O
ith	O
eigenfunction	B
where	O
kmx	O
kx	O
xm	O
ix	O
restatement	O
of	O
eq	O
using	O
the	O
current	O
notation	O
and	O
i	O
it	O
is	O
easy	O
to	O
see	O
that	O
in	O
general	O
we	O
obtain	O
an	O
approximation	O
for	O
the	O
kernel	B
kx	O
m	O
i	O
i	O
i	O
pn	O
i	O
ix	O
as	O
kx	O
mx	O
mx	O
i	O
m	O
ix	O
i	O
m	O
m	O
kmxk	O
i	O
kmxum	O
i	O
i	O
clearly	O
eq	O
is	O
obtained	O
by	O
evaluating	O
eq	O
for	O
all	O
pairs	O
of	O
datapoints	O
in	O
the	O
training	O
set	B
by	O
multiplying	O
out	O
eq	O
using	O
kmn	O
m	O
it	O
is	O
easy	O
to	O
show	O
that	O
kmm	O
kmm	O
kmn	O
m	O
kmn	O
m	O
kn	O
mm	O
kn	O
mm	O
but	O
that	O
kn	O
mn	O
m	O
kn	O
mmk	O
kn	O
mn	O
m	O
kn	O
mn	O
m	O
is	O
in	O
fact	O
the	O
schur	O
complement	O
of	O
kmm	O
and	O
van	O
loan	O
p	O
it	O
is	O
easy	O
to	O
see	O
that	O
kn	O
mn	O
m	O
kn	O
mn	O
m	O
is	O
positive	O
semi-definite	O
if	O
a	O
vector	O
f	O
is	O
partitioned	O
as	O
f	O
n	O
m	O
and	O
f	O
has	O
a	O
gaussian	B
distribution	I
with	O
zero	O
mean	O
and	O
covariance	B
k	O
then	O
fn	O
mfm	O
has	O
the	O
schur	O
complement	O
as	O
its	O
covariance	B
matrix	I
see	O
eq	O
mmkmn	O
m	O
the	O
difference	O
m	O
f	O
the	O
nystr	O
om	O
approximation	O
was	O
derived	O
in	O
the	O
above	O
fashion	O
by	O
williams	O
and	O
seeger	B
for	O
application	O
to	O
kernel	B
machines	O
an	O
alternative	O
view	O
which	O
gives	O
rise	O
to	O
the	O
same	O
approximation	O
is	O
due	O
to	O
smola	O
and	O
sch	O
olkopf	O
also	O
sch	O
olkopf	O
and	O
smola	O
sec	O
here	O
the	O
starting	O
point	O
is	O
that	O
we	O
wish	O
to	O
approximate	O
the	O
kernel	B
centered	O
on	O
point	O
xi	O
as	O
a	O
linear	B
combination	O
of	O
kernels	O
from	O
the	O
active	O
set	B
so	O
that	O
cijkxj	O
x	O
kxi	O
x	O
kxi	O
x	O
x	O
j	O
i	O
for	O
some	O
coefficients	O
that	O
are	O
to	O
be	O
determined	O
so	O
as	O
to	O
optimize	O
the	O
approximation	O
a	O
reasonable	O
criterion	O
to	O
minimize	O
is	O
kkxi	O
x	O
kxi	O
nx	O
ec	O
tr	O
k	O
trckmn	O
trckmmc	O
where	O
the	O
coefficients	O
are	O
arranged	O
into	O
a	O
n	O
m	O
matrix	O
c	O
minimizing	O
ec	O
w	O
r	O
t	O
c	O
gives	O
copt	O
knmk	O
mm	O
thus	O
we	O
obtain	O
the	O
approximation	O
k	O
knmk	O
mmkmn	O
in	O
agreement	O
with	O
eq	O
also	O
it	O
can	O
be	O
shown	O
that	O
ecopt	O
trk	O
k	O
smola	O
and	O
sch	O
olkopf	O
suggest	O
a	O
greedy	O
algorithm	O
to	O
choose	O
points	O
to	O
include	O
into	O
the	O
active	O
set	B
so	O
as	O
to	O
minimize	O
the	O
error	O
criterion	O
as	O
it	O
takes	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
omn	O
operations	O
to	O
evaluate	O
the	O
change	O
in	O
e	O
due	O
to	O
including	O
one	O
new	O
datapoint	O
exercise	O
it	O
is	O
infeasible	O
to	O
consider	O
all	O
members	O
of	O
set	B
r	O
for	O
inclusion	O
on	O
each	O
iteration	O
instead	O
smola	O
and	O
sch	O
olkopf	O
suggest	O
finding	O
the	O
best	O
point	O
to	O
include	O
from	O
a	O
randomly	O
chosen	O
subset	O
of	O
set	B
r	O
on	O
each	O
iteration	O
recent	O
work	O
by	O
drineas	O
and	O
mahoney	O
analyzes	O
a	O
similar	O
algorithm	O
to	O
the	O
nystr	O
om	O
approximation	O
except	O
that	O
they	O
use	O
biased	O
sampling	O
with	O
replacement	O
column	O
i	O
of	O
k	O
with	O
probability	O
ii	O
and	O
a	O
pseudoinverse	O
of	O
the	O
inner	O
m	O
m	O
matrix	O
for	O
this	O
algorithm	O
they	O
are	O
able	O
to	O
provide	O
probabilistic	B
bounds	O
on	O
the	O
quality	O
of	O
the	O
approximation	O
earlier	O
work	O
by	O
frieze	O
et	O
al	O
had	O
developed	O
an	O
approximation	O
to	O
the	O
singular	O
value	O
decomposition	O
of	O
a	O
rectangular	O
matrix	O
using	O
a	O
weighted	O
random	O
subsampling	O
of	O
its	O
rows	O
and	O
columns	O
and	O
probabilistic	B
error	O
bounds	O
however	O
this	O
is	O
rather	O
different	O
from	O
the	O
nystr	O
om	O
approximation	O
see	O
drineas	O
and	O
mahoney	O
sec	O
for	O
details	O
fine	O
and	O
scheinberg	O
suggest	O
an	O
alternative	O
low-rank	O
approximation	O
to	O
k	O
using	O
the	O
incomplete	O
cholesky	O
factorization	O
golub	O
and	O
van	O
loan	O
sec	O
the	O
idea	O
here	O
is	O
that	O
when	O
computing	O
the	O
cholesky	B
decomposition	I
of	O
k	O
pivots	O
below	O
a	O
certain	O
threshold	O
are	O
if	O
the	O
number	O
of	O
pivots	O
greater	O
than	O
the	O
threshold	O
is	O
k	O
the	O
incomplete	O
cholesky	O
factorization	O
takes	O
time	O
greedy	O
approximation	O
many	O
of	O
the	O
methods	O
described	O
below	O
use	O
an	O
active	O
set	B
of	O
training	O
points	O
of	O
size	O
m	O
selected	O
from	O
the	O
training	O
set	B
of	O
size	O
n	O
m	O
we	O
assume	O
that	O
it	O
is	O
impossible	O
to	O
search	O
for	O
the	O
optimal	B
subset	O
of	O
size	O
m	O
due	O
to	O
combinatorics	O
the	O
points	O
in	O
the	O
active	O
set	B
could	O
be	O
selected	O
randomly	O
but	O
in	O
general	O
we	O
might	O
expect	O
better	O
performance	O
if	O
the	O
points	O
are	O
selected	O
greedily	O
w	O
r	O
t	O
some	O
criterion	O
in	O
the	O
statistics	O
literature	O
greedy	O
approaches	O
are	O
also	O
known	O
as	O
forward	O
selection	O
strategies	O
a	O
general	O
recipe	O
for	O
greedy	O
approximation	O
is	O
given	O
in	O
algorithm	O
the	O
algorithm	O
starts	O
with	O
the	O
active	O
set	B
i	O
being	O
empty	O
and	O
the	O
set	B
r	O
containing	O
the	O
indices	O
of	O
all	O
training	O
examples	O
on	O
each	O
iteration	O
one	O
index	O
is	O
selected	O
from	O
r	O
and	O
added	O
to	O
i	O
this	O
is	O
achieved	O
by	O
evaluating	O
some	O
criterion	O
and	O
selecting	O
the	O
data	O
point	O
that	O
optimizes	O
this	O
criterion	O
for	O
some	O
algorithms	O
it	O
can	O
be	O
too	O
expensive	O
to	O
evaluate	O
on	O
all	O
points	O
in	O
r	O
so	O
some	O
working	O
set	B
j	O
r	O
can	O
be	O
chosen	O
instead	O
usually	O
at	O
random	O
from	O
r	O
greedy	O
selection	O
methods	O
have	O
been	O
used	O
with	O
the	O
subset	B
of	I
regressors	I
subset	B
of	I
datapoints	I
and	O
the	O
projected	O
process	O
methods	O
described	O
below	O
a	O
technical	O
detail	O
symmetric	O
permutations	O
of	O
the	O
rows	O
and	O
columns	O
are	O
required	O
to	O
stabilize	O
the	O
computations	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
input	O
m	O
desired	O
size	O
of	O
active	O
set	B
initialization	O
i	O
r	O
n	O
for	O
j	O
m	O
do	O
create	O
working	O
set	B
j	O
r	O
compute	O
j	O
for	O
all	O
j	O
j	O
i	O
argmaxj	O
j	O
j	O
update	O
model	O
to	O
include	O
data	O
from	O
example	O
i	O
i	O
i	O
r	O
ri	O
end	O
for	O
return	O
i	O
algorithm	O
general	O
framework	O
for	O
greedy	O
subset	O
selection	O
j	O
is	O
the	O
criterion	O
function	B
evaluated	O
on	O
data	O
point	O
j	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hy	O
perparameters	O
we	O
present	O
six	O
approximation	O
schemes	O
for	O
gpr	O
below	O
namely	O
the	O
subset	B
of	I
regressors	I
the	O
nystr	O
om	O
method	O
the	O
subset	B
of	I
datapoints	I
the	O
projected	B
process	I
approximation	I
the	O
bayesian	B
committee	I
machine	I
and	O
the	O
iterative	O
solution	O
of	O
linear	B
systems	O
section	O
provides	O
a	O
summary	O
of	O
these	O
methods	O
and	O
a	O
comparison	O
of	O
their	O
performance	O
on	O
the	O
sarcos	O
data	O
which	O
was	O
introduced	O
in	O
section	O
subset	B
of	I
regressors	I
pn	O
silverman	O
sec	O
showed	O
that	O
the	O
mean	O
gp	O
predictor	O
can	O
be	O
obtained	O
from	O
a	O
finite-dimensional	O
generalized	B
linear	B
regression	I
model	O
fx	O
ikx	O
xi	O
with	O
a	O
prior	O
n	O
k	O
to	O
see	O
this	O
we	O
use	O
the	O
mean	O
prediction	O
for	O
linear	B
regression	I
model	O
in	O
feature	B
space	I
given	O
by	O
eq	O
i	O
e	O
fx	O
n	O
setting	O
kx	O
k	O
and	O
fx	O
n	O
y	O
with	O
a	O
p	O
p	O
k	O
we	O
obtain	O
n	O
kx	O
kx	O
n	O
kk	O
ni	O
ni	O
in	O
agreement	O
with	O
eq	O
note	O
however	O
that	O
the	O
predictive	B
of	O
this	O
model	O
is	O
different	O
from	O
full	O
gpr	O
a	O
simple	O
approximation	O
to	O
this	O
model	O
is	O
to	O
consider	O
only	O
a	O
subset	O
of	O
regres	O
sors	O
so	O
that	O
mx	O
fsrx	O
ikx	O
xi	O
with	O
m	O
n	O
k	O
mm	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
again	O
using	O
eq	O
we	O
obtain	O
fsrx	O
kmx	O
nkmm	O
vfsrx	O
nkmx	O
nkmm	O
sr	O
marginal	B
likelihood	B
nkmm	O
thus	O
the	O
posterior	O
mean	O
for	O
m	O
is	O
given	O
by	O
m	O
this	O
method	O
has	O
been	O
proposed	O
for	O
example	O
in	O
wahba	O
chapter	O
and	O
in	O
poggio	O
and	O
girosi	O
eq	O
via	O
the	O
regularization	B
framework	O
the	O
name	O
subset	B
of	I
regressors	I
was	O
suggested	O
to	O
us	O
by	O
g	O
wahba	O
the	O
computations	O
for	O
equations	O
and	O
take	O
time	O
to	O
carry	O
out	O
the	O
necessary	O
matrix	O
computations	O
after	O
this	O
the	O
prediction	O
of	O
the	O
mean	O
for	O
a	O
new	O
test	O
point	O
takes	O
time	O
om	O
and	O
the	O
predictive	B
variance	O
takes	O
under	O
the	O
subset	B
of	I
regressors	I
model	O
we	O
have	O
f	O
n	O
k	O
where	O
k	O
is	O
y	O
k	O
log	O
k	O
nin	O
defined	O
as	O
in	O
eq	O
thus	O
the	O
log	O
marginal	B
likelihood	B
under	O
this	O
model	O
is	O
log	O
psryx	O
notice	O
that	O
the	O
covariance	B
function	B
defined	O
by	O
the	O
sr	O
model	O
has	O
the	O
form	O
kx	O
kxk	O
which	O
is	O
exactly	O
the	O
same	O
as	O
that	O
from	O
the	O
nystr	O
om	O
approximation	O
for	O
the	O
covariance	B
function	B
eq	O
in	O
fact	O
if	O
the	O
covariance	B
function	B
kx	O
in	O
the	O
predictive	B
mean	O
and	O
variance	O
equations	O
and	O
is	O
replaced	O
systematically	O
with	O
kx	O
we	O
obtain	O
equations	O
and	O
as	O
shown	O
in	O
appendix	O
nin	O
n	O
if	O
the	O
kernel	B
function	B
decays	O
to	O
zero	O
for	O
for	O
fixed	O
then	O
kx	O
x	O
will	O
be	O
near	O
zero	O
when	O
x	O
is	O
distant	O
from	O
points	O
in	O
the	O
set	B
i	O
this	O
will	O
be	O
the	O
case	O
even	O
when	O
the	O
kernel	B
is	O
stationary	O
so	O
that	O
kx	O
x	O
is	O
independent	O
of	O
x	O
thus	O
we	O
might	O
expect	O
that	O
using	O
the	O
approximate	O
kernel	B
will	O
give	O
poor	O
predictions	O
especially	O
underestimates	O
of	O
the	O
predictive	B
variance	O
when	O
x	O
is	O
far	O
from	O
points	O
in	O
the	O
set	B
i	O
ysr	O
pm	O
an	O
interesting	O
idea	O
suggested	O
by	O
rasmussen	O
and	O
qui	O
nonero-candela	O
to	O
mitigate	O
this	O
problem	O
is	O
to	O
define	O
the	O
sr	O
model	O
with	O
m	O
basis	O
functions	O
where	O
the	O
extra	O
basis	O
function	B
is	O
centered	O
on	O
the	O
test	O
point	O
x	O
so	O
that	O
ikx	O
xi	O
kx	O
x	O
this	O
model	O
can	O
then	O
be	O
used	O
to	O
make	O
predictions	O
and	O
it	O
can	O
be	O
implemented	O
efficiently	O
using	O
the	O
partitioned	O
matrix	O
inverse	O
equations	O
and	O
the	O
effect	O
of	O
the	O
extra	O
basis	O
function	B
centered	O
on	O
x	O
is	O
to	O
maintain	O
predictive	B
variance	O
at	O
the	O
test	O
point	O
so	O
far	O
we	O
have	O
not	O
said	O
how	O
the	O
subset	O
i	O
should	O
be	O
chosen	O
one	O
simple	O
method	O
is	O
to	O
choose	O
it	O
randomly	O
from	O
x	O
another	O
is	O
to	O
run	O
clustering	O
on	O
to	O
obtain	O
centres	O
alternatively	O
a	O
number	O
of	O
greedy	O
forward	O
selection	O
algorithms	O
for	O
i	O
have	O
been	O
proposed	O
luo	O
and	O
wahba	O
choose	O
the	O
next	O
kernel	B
so	O
as	O
to	O
minimize	O
the	O
residual	O
sum	O
of	O
squares	O
knm	O
after	O
optimizing	O
m	O
smola	O
and	O
bartlett	O
take	O
a	O
similar	O
approach	O
but	O
choose	O
as	O
their	O
criterion	O
the	O
quadratic	B
form	I
knm	O
mkmm	O
m	O
y	O
k	O
nin	O
n	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
comparison	O
with	O
rvm	O
where	O
the	O
right	O
hand	O
side	O
follows	O
using	O
eq	O
and	O
the	O
matrix	O
inversion	B
lemma	I
alternatively	O
qui	O
nonero-candela	O
suggests	O
using	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
log	O
psryx	O
eq	O
as	O
the	O
selection	O
criterion	O
in	O
fact	O
the	O
quadratic	O
term	O
from	O
eq	O
is	O
one	O
of	O
the	O
terms	O
comprising	O
log	O
psryx	O
for	O
all	O
these	O
suggestions	O
the	O
complexity	O
of	O
evaluating	O
the	O
criterion	O
on	O
a	O
new	O
example	O
is	O
omn	O
by	O
making	O
use	O
of	O
partitioned	O
matrix	O
equations	O
thus	O
it	O
is	O
likely	O
to	O
be	O
too	O
expensive	O
to	O
consider	O
all	O
points	O
in	O
r	O
on	O
each	O
iteration	O
and	O
we	O
are	O
likely	O
to	O
want	O
to	O
consider	O
a	O
smaller	O
working	O
set	B
as	O
described	O
in	O
algorithm	O
note	O
that	O
the	O
sr	O
model	O
is	O
obtained	O
by	O
selecting	O
some	O
subset	O
of	O
the	O
datapoints	O
of	O
size	O
m	O
in	O
a	O
random	O
or	O
greedy	O
manner	O
the	O
relevance	B
vector	I
machine	I
described	O
in	O
section	O
has	O
a	O
similar	O
flavour	O
in	O
that	O
it	O
automatically	O
selects	O
a	O
greedy	O
fashion	O
which	O
datapoints	O
to	O
use	O
in	O
its	O
expansion	O
however	O
note	O
one	O
important	O
difference	O
which	O
is	O
that	O
the	O
rvm	O
uses	O
a	O
diagonal	O
prior	O
on	O
the	O
s	O
while	O
for	O
the	O
sr	O
method	O
we	O
have	O
m	O
n	O
k	O
mm	O
the	O
nystr	O
om	O
method	O
williams	O
and	O
seeger	B
suggested	O
approximating	O
the	O
gpr	O
equations	O
by	O
replacing	O
the	O
matrix	O
k	O
by	O
k	O
in	O
the	O
mean	O
and	O
variance	O
prediction	O
equations	O
and	O
and	O
called	O
this	O
the	O
nystr	O
om	O
method	O
for	O
approximate	O
gpr	O
notice	O
that	O
in	O
this	O
proposal	O
the	O
covariance	B
function	B
k	O
is	O
not	O
systematically	O
replaced	O
by	O
k	O
it	O
is	O
only	O
occurrences	O
of	O
the	O
matrix	O
k	O
that	O
are	O
replaced	O
as	O
for	O
the	O
sr	O
model	O
the	O
time	O
complexity	O
is	O
to	O
carry	O
out	O
the	O
necessary	O
matrix	O
computations	O
and	O
then	O
on	O
for	O
the	O
predictive	B
mean	O
of	O
a	O
test	O
point	O
and	O
omn	O
for	O
the	O
predictive	B
variance	O
experimental	O
evidence	O
in	O
williams	O
et	O
al	O
suggests	O
that	O
for	O
large	O
m	O
the	O
sr	O
and	O
nystr	O
om	O
methods	O
have	O
similar	O
performance	O
but	O
for	O
small	O
m	O
the	O
nystr	O
om	O
method	O
can	O
be	O
quite	O
poor	O
also	O
the	O
fact	O
that	O
k	O
is	O
not	O
systematically	O
replaced	O
by	O
k	O
means	O
that	O
embarrassments	O
can	O
occur	O
like	O
the	O
approximated	O
predictive	B
variance	O
being	O
negative	O
for	O
these	O
reasons	O
we	O
do	O
not	O
recommend	O
the	O
nystr	O
om	O
method	O
over	O
the	O
sr	O
method	O
however	O
the	O
nystr	O
om	O
method	O
can	O
be	O
effective	O
when	O
the	O
eigenvalue	B
of	O
k	O
is	O
much	O
smaller	O
than	O
n	O
subset	B
of	I
datapoints	I
the	O
subset	B
of	I
regressors	I
method	O
described	O
above	O
approximated	O
the	O
form	O
of	O
the	O
predictive	B
distribution	O
and	O
particularly	O
the	O
predictive	B
mean	O
another	O
simple	O
approximation	O
to	O
the	O
full-sample	O
gp	O
predictor	O
is	O
to	O
keep	O
the	O
gp	O
predictor	O
but	O
only	O
on	O
a	O
smaller	O
subset	O
of	O
size	O
m	O
of	O
the	O
data	O
although	O
this	O
is	O
clearly	O
wasteful	O
of	O
data	O
it	O
can	O
make	O
sense	O
if	O
the	O
predictions	O
obtained	O
with	O
m	O
points	O
are	O
sufficiently	O
accurate	O
for	O
our	O
needs	O
clearly	O
it	O
can	O
make	O
sense	O
to	O
select	O
which	O
points	O
are	O
taken	O
into	O
the	O
active	O
set	B
i	O
and	O
typically	O
this	O
is	O
achieved	O
by	O
greedy	O
algorithms	O
however	O
one	O
has	O
to	O
be	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
wary	O
of	O
the	O
amount	O
of	O
computation	O
that	O
is	O
needed	O
especially	O
if	O
one	O
considers	O
each	O
member	O
of	O
r	O
at	O
each	O
iteration	O
lawrence	O
et	O
al	O
suggest	O
choosing	O
as	O
the	O
next	O
point	O
site	O
for	O
inclusion	O
into	O
the	O
active	O
set	B
the	O
one	O
that	O
maximizes	O
the	O
differential	O
entropy	B
score	O
j	O
hpfj	O
hpnewfj	O
where	O
hpfj	O
is	O
the	O
entropy	B
of	O
the	O
gaussian	O
at	O
site	O
j	O
r	O
is	O
a	O
function	B
of	O
the	O
variance	O
at	O
site	O
j	O
as	O
the	O
posterior	O
is	O
gaussian	O
see	O
eq	O
and	O
hpnewfj	O
is	O
the	O
entropy	B
at	O
this	O
site	O
once	O
the	O
observation	O
at	O
site	O
j	O
has	O
been	O
included	O
let	O
the	O
posterior	O
variance	O
of	O
fj	O
before	O
inclusion	O
be	O
vj	O
as	O
pfjyi	O
yj	O
pfjyin	O
we	O
have	O
j	O
using	O
the	O
fact	O
that	O
the	O
entropy	B
of	O
a	O
gaussian	O
with	O
variance	O
v	O
is	O
we	O
obtain	O
v	O
j	O
ivm	O
j	O
vj	O
j	O
is	O
a	O
monotonic	O
function	B
of	O
vj	O
so	O
that	O
it	O
is	O
maximized	O
by	O
choosing	O
the	O
site	O
with	O
the	O
largest	O
variance	O
lawrence	O
et	O
al	O
call	O
their	O
method	O
the	O
informative	B
vector	I
machine	I
if	O
coded	O
na	O
vely	O
the	O
complexity	O
of	O
computing	O
the	O
variance	O
at	O
all	O
sites	O
in	O
r	O
on	O
a	O
single	O
iteration	O
is	O
as	O
we	O
need	O
to	O
evaluate	O
eq	O
at	O
ni	O
can	O
be	O
done	O
once	O
in	O
each	O
site	O
the	O
matrix	O
inversion	O
of	O
kmm	O
then	O
stored	O
however	O
as	O
we	O
are	O
incrementally	O
growing	O
the	O
matrices	O
kmm	O
and	O
kmn	O
m	O
in	O
fact	O
the	O
cost	O
is	O
omn	O
per	O
inclusion	O
leading	O
to	O
an	O
overall	O
complexity	O
of	O
when	O
using	O
a	O
subset	O
of	O
size	O
m	O
for	O
example	O
once	O
a	O
site	O
has	O
been	O
chosen	O
for	O
inclusion	O
the	O
matrix	O
kmm	O
ni	O
is	O
grown	O
by	O
including	O
an	O
extra	O
row	O
and	O
column	O
the	O
inverse	O
of	O
this	O
expanded	O
matrix	O
can	O
be	O
found	O
using	O
eq	O
although	O
it	O
would	O
be	O
better	O
practice	O
numerically	O
to	O
use	O
a	O
cholesky	B
decomposition	I
approach	O
as	O
described	O
in	O
lawrence	O
et	O
al	O
the	O
scheme	O
evaluates	O
j	O
over	O
all	O
j	O
r	O
at	O
each	O
step	O
to	O
choose	O
the	O
inclusion	O
site	O
this	O
makes	O
sense	O
when	O
m	O
is	O
small	O
but	O
as	O
it	O
gets	O
larger	O
it	O
can	O
make	O
sense	O
to	O
select	O
candidate	O
inclusion	O
sites	O
from	O
a	O
subset	O
of	O
r	O
lawrence	O
et	O
al	O
call	O
this	O
the	O
randomized	O
greedy	O
selection	O
method	O
and	O
give	O
further	O
ideas	O
on	O
how	O
to	O
choose	O
the	O
subset	O
the	O
differential	O
entropy	B
score	O
j	O
is	O
not	O
the	O
only	O
criterion	O
that	O
can	O
be	O
used	O
for	O
site	O
selection	O
for	O
example	O
the	O
information	O
gain	O
criterion	O
klpnewfjpfj	O
can	O
also	O
be	O
used	O
seeger	B
et	O
al	O
the	O
use	O
of	O
greedy	O
selection	O
heuristics	O
here	O
is	O
similar	O
to	O
the	O
problem	O
of	O
active	O
learning	B
see	O
e	O
g	O
mackay	O
projected	B
process	I
approximation	I
the	O
sr	O
method	O
has	O
the	O
unattractive	O
feature	O
that	O
it	O
is	O
based	O
on	O
a	O
degenerate	B
gp	O
the	O
finite-dimensional	O
model	O
given	O
in	O
eq	O
the	O
sd	O
method	O
is	O
a	O
nondegenerate	B
process	O
model	O
but	O
it	O
only	O
makes	O
use	O
of	O
m	O
datapoints	O
the	O
projected	B
process	I
approximation	I
is	O
also	O
a	O
non-degenerate	O
process	O
model	O
but	O
it	O
can	O
make	O
use	O
of	O
all	O
n	O
datapoints	O
we	O
call	O
it	O
a	O
projected	B
process	I
approximation	I
as	O
it	O
represents	O
only	O
m	O
n	O
latent	O
function	B
values	O
but	O
computes	O
a	O
likelihood	B
involving	O
all	O
n	O
datapoints	O
by	O
projecting	O
up	O
the	O
m	O
latent	O
points	O
to	O
n	O
dimensions	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
one	O
problem	O
with	O
the	O
basic	O
gpr	O
algorithm	O
is	O
the	O
fact	O
that	O
the	O
likelihood	B
term	O
requires	O
us	O
to	O
have	O
f-values	O
for	O
the	O
n	O
training	O
points	O
however	O
say	O
we	O
only	O
represent	O
m	O
of	O
these	O
values	O
explicitly	O
and	O
denote	O
these	O
as	O
fm	O
then	O
the	O
remaining	O
f-values	O
in	O
r	O
denoted	O
fn	O
m	O
have	O
a	O
conditional	B
distribution	O
pfn	O
mfm	O
the	O
mean	O
of	O
which	O
is	O
given	O
by	O
efn	O
mfm	O
kn	O
mmk	O
say	O
we	O
replace	O
the	O
true	O
likelihood	B
term	O
for	O
the	O
points	O
in	O
r	O
by	O
n	O
mefn	O
mfm	O
ni	O
including	O
also	O
the	O
likelihood	B
contribution	O
of	O
the	O
points	O
in	O
set	B
i	O
we	O
have	O
qyfm	O
n	O
ni	O
mmfm	O
which	O
can	O
also	O
be	O
written	O
as	O
qyfm	O
n	O
ni	O
the	O
key	O
feature	O
here	O
is	O
that	O
we	O
have	O
absorbed	O
the	O
information	O
in	O
all	O
n	O
points	O
of	O
d	O
into	O
the	O
m	O
points	O
in	O
i	O
the	O
form	O
of	O
qyfm	O
in	O
eq	O
might	O
seem	O
rather	O
arbitrary	O
but	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
if	O
we	O
consider	O
minimizing	O
klqfypfy	O
the	O
kldivergence	O
between	O
the	O
approximating	O
distribution	O
qfy	O
and	O
the	O
true	O
posterior	O
pfy	O
over	O
all	O
q	O
distributions	O
of	O
the	O
form	O
qfy	O
pfrfm	O
where	O
r	O
is	O
positive	O
and	O
depends	O
on	O
fm	O
only	O
this	O
is	O
the	O
form	O
we	O
obtain	O
see	O
seeger	B
lemma	O
and	O
sec	O
for	O
detailed	O
derivations	O
and	O
also	O
csat	O
o	O
sec	O
to	O
make	O
predictions	O
we	O
first	O
have	O
to	O
compute	O
the	O
posterior	O
distribution	O
mmkmn	O
so	O
that	O
effm	O
p	O
then	O
qfmy	O
define	O
the	O
shorthand	O
p	O
k	O
we	O
have	O
qyfm	O
p	O
p	O
n	O
qfmy	O
combining	O
this	O
with	O
the	O
prior	O
pfm	O
exp	O
f	O
mk	O
p	O
p	O
which	O
can	O
be	O
recognized	O
as	O
a	O
gaussian	O
n	O
a	O
with	O
a	O
n	O
n	O
ap	O
y	O
kmm	O
mm	O
p	O
p	O
f	O
mk	O
nk	O
nkmm	O
kmnknm	O
mm	O
n	O
we	O
obtain	O
yp	O
n	O
n	O
k	O
mm	O
nkmm	O
kmnknmk	O
mm	O
thus	O
the	O
predictive	B
mean	O
is	O
given	O
by	O
eqfx	O
kmx	O
kmx	O
which	O
turns	O
out	O
to	O
be	O
just	O
the	O
same	O
as	O
the	O
predictive	B
mean	O
under	O
the	O
sr	O
model	O
as	O
given	O
in	O
eq	O
however	O
the	O
predictive	B
variance	O
is	O
different	O
the	O
argument	O
is	O
the	O
same	O
as	O
in	O
eq	O
and	O
yields	O
mm	O
nkmm	O
kmnknm	O
vqfx	O
kx	O
x	O
kmx	O
kmx	O
mmkmx	O
mmcovfmyk	O
mmkmx	O
kx	O
x	O
kmx	O
nkmx	O
mmkmx	O
nkmm	O
kmnknm	O
is	O
no	O
a	O
priori	O
reason	O
why	O
the	O
m	O
points	O
chosen	O
have	O
to	O
be	O
a	O
subset	O
of	O
the	O
n	O
points	O
in	O
d	O
they	O
could	O
be	O
disjoint	O
from	O
the	O
training	O
set	B
however	O
for	O
our	O
derivations	O
below	O
we	O
will	O
consider	O
them	O
to	O
be	O
a	O
subset	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
notice	O
that	O
predictive	B
variance	O
is	O
the	O
sum	O
of	O
the	O
predictive	B
variance	O
under	O
the	O
sr	O
model	O
term	O
in	O
eq	O
plus	O
kx	O
x	O
kmx	O
mmkmx	O
which	O
is	O
the	O
predictive	B
variance	O
at	O
x	O
given	O
fm	O
thus	O
eq	O
is	O
never	O
smaller	O
than	O
the	O
sr	O
predictive	B
variance	O
and	O
will	O
become	O
close	O
to	O
kx	O
x	O
when	O
x	O
is	O
far	O
away	O
from	O
the	O
points	O
in	O
set	B
i	O
as	O
for	O
the	O
sr	O
model	O
it	O
takes	O
time	O
to	O
carry	O
out	O
the	O
necessary	O
matrix	O
computations	O
after	O
this	O
the	O
prediction	O
of	O
the	O
mean	O
for	O
a	O
new	O
test	O
point	O
takes	O
time	O
om	O
and	O
the	O
predictive	B
variance	O
takes	O
we	O
have	O
qyfm	O
n	O
ni	O
and	O
pfm	O
n	O
kmm	O
by	O
integrating	O
out	O
fm	O
we	O
find	O
that	O
y	O
n	O
k	O
nin	O
thus	O
the	O
marginal	B
likelihood	B
for	O
the	O
projected	B
process	I
approximation	I
is	O
the	O
same	O
as	O
that	O
for	O
the	O
sr	O
model	O
eq	O
again	O
the	O
question	O
of	O
how	O
to	O
choose	O
which	O
points	O
go	O
into	O
the	O
set	B
i	O
arises	O
csat	O
o	O
and	O
opper	O
present	O
a	O
method	O
in	O
which	O
the	O
training	O
examples	O
are	O
presented	O
sequentially	O
an	O
on-line	O
fashion	O
given	O
the	O
current	O
active	O
set	B
i	O
one	O
can	O
compute	O
the	O
novelty	O
of	O
a	O
new	O
input	O
point	O
if	O
this	O
is	O
large	O
then	O
this	O
point	O
is	O
added	O
to	O
i	O
otherwise	O
the	O
point	O
is	O
added	O
to	O
r	O
to	O
be	O
precise	O
the	O
novelty	O
of	O
an	O
input	O
x	O
is	O
computed	O
as	O
kx	O
x	O
kmxk	O
mmkx	O
which	O
can	O
be	O
recognized	O
as	O
the	O
predictive	B
variance	O
at	O
x	O
given	O
non-noisy	O
observations	O
at	O
the	O
points	O
in	O
i	O
if	O
the	O
active	O
set	B
gets	O
larger	O
than	O
some	O
preset	O
maximum	O
size	O
then	O
points	O
can	O
be	O
deleted	O
from	O
i	O
as	O
specified	O
in	O
section	O
of	O
csat	O
o	O
and	O
opper	O
later	O
work	O
by	O
csat	O
o	O
et	O
al	O
replaced	O
the	O
dependence	O
of	O
the	O
algorithm	O
described	O
above	O
on	O
the	O
input	O
sequence	O
by	O
an	O
expectation-propagation	O
type	O
algorithm	O
section	O
as	O
an	O
alternative	O
method	O
for	O
selecting	O
the	O
active	O
set	B
seeger	B
et	O
al	O
suggest	O
using	O
a	O
greedy	O
subset	O
selection	O
method	O
as	O
per	O
algorithm	O
computation	O
of	O
the	O
information	O
gain	O
criterion	O
after	O
incorporating	O
a	O
new	O
site	O
takes	O
omn	O
and	O
is	O
thus	O
too	O
expensive	O
to	O
use	O
as	O
a	O
selection	O
criterion	O
however	O
an	O
approximation	O
to	O
the	O
information	O
gain	O
can	O
be	O
computed	O
cheaply	O
seeger	B
et	O
al	O
eq	O
and	O
seeger	B
sec	O
for	O
further	O
details	O
and	O
this	O
allows	O
the	O
greedy	O
subset	O
algorithm	O
to	O
be	O
run	O
on	O
all	O
points	O
in	O
r	O
on	O
each	O
iteration	O
bayesian	B
committee	I
machine	I
tresp	O
introduced	O
the	O
bayesian	B
committee	I
machine	I
as	O
a	O
way	O
of	O
speeding	O
up	O
gaussian	B
process	I
regression	I
let	O
f	O
be	O
the	O
vector	O
of	O
function	B
values	O
at	O
the	O
test	O
locations	O
under	O
gpr	O
we	O
obtain	O
a	O
predictive	B
gaussian	B
distribution	I
for	O
pf	O
for	O
the	O
bcm	O
we	O
split	O
the	O
dataset	O
into	O
p	O
parts	O
qp	O
where	O
di	O
yi	O
and	O
make	O
the	O
approximation	O
that	O
ypf	O
x	O
qp	O
pyif	O
xi	O
under	O
this	O
approximation	O
we	O
have	O
pf	O
pyif	O
xi	O
c	O
pp	O
qf	O
pf	O
py	O
where	O
c	O
is	O
a	O
normalization	O
constant	O
using	O
the	O
fact	O
that	O
the	O
terms	O
in	O
the	O
numerator	O
and	O
denomination	O
are	O
all	O
gaussian	O
distributions	O
over	O
f	O
it	O
is	O
easy	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
to	O
show	O
exercise	O
that	O
the	O
predictive	B
mean	O
and	O
covariance	B
for	O
f	O
are	O
given	O
by	O
eqf	O
px	O
px	O
where	O
k	O
is	O
the	O
covariance	B
matrix	I
evaluated	O
at	O
the	O
test	O
points	O
here	O
ef	O
and	O
covf	O
are	O
the	O
mean	O
and	O
covariance	B
of	O
the	O
predictions	O
for	O
f	O
given	O
di	O
as	O
given	O
in	O
eqs	O
and	O
note	O
that	O
eq	O
has	O
an	O
interesting	O
form	O
in	O
that	O
the	O
predictions	O
from	O
each	O
part	O
of	O
the	O
dataset	O
are	O
weighted	O
by	O
the	O
inverse	O
predictive	B
covariance	B
we	O
are	O
free	O
to	O
choose	O
how	O
to	O
partition	O
the	O
dataset	O
d	O
this	O
has	O
two	O
aspects	O
the	O
number	O
of	O
partitions	O
and	O
the	O
assignment	O
of	O
data	O
points	O
to	O
the	O
partitions	O
if	O
we	O
wish	O
each	O
partition	O
to	O
have	O
size	O
m	O
then	O
p	O
nm	O
tresp	O
used	O
a	O
random	O
assignment	O
of	O
data	O
points	O
to	O
partitions	O
but	O
schwaighofer	O
and	O
tresp	O
recommend	O
that	O
clustering	O
the	O
data	O
with	O
p-means	O
clustering	O
can	O
lead	O
to	O
improved	O
performance	O
however	O
note	O
that	O
compared	O
to	O
the	O
greedy	O
schemes	O
used	O
above	O
clustering	O
does	O
not	O
make	O
use	O
of	O
the	O
target	O
y	O
values	O
only	O
the	O
inputs	O
x	O
although	O
it	O
is	O
possible	O
to	O
make	O
predictions	O
for	O
any	O
number	O
of	O
test	O
points	O
n	O
this	O
slows	O
the	O
method	O
down	O
as	O
it	O
involves	O
the	O
inversion	O
of	O
n	O
n	O
matrices	O
schwaighofer	O
and	O
tresp	O
recommend	O
making	O
test	O
predictions	O
on	O
blocks	O
of	O
size	O
m	O
so	O
that	O
all	O
matrices	O
are	O
of	O
the	O
same	O
size	O
in	O
this	O
case	O
the	O
computational	O
complexity	O
of	O
bcm	O
is	O
for	O
predicting	O
m	O
test	O
points	O
or	O
omn	O
per	O
test	O
point	O
the	O
bcm	O
approach	O
is	O
transductive	O
rather	O
than	O
inductive	B
in	O
the	O
sense	O
that	O
the	O
method	O
computes	O
a	O
test-set	O
dependent	O
model	O
making	O
use	O
of	O
the	O
test	O
set	B
input	O
locations	O
note	O
also	O
that	O
if	O
we	O
wish	O
to	O
make	O
a	O
prediction	O
at	O
just	O
one	O
test	O
point	O
it	O
would	O
be	O
necessary	O
to	O
hallucinate	O
some	O
extra	O
test	O
points	O
as	O
eq	O
generally	O
becomes	O
a	O
better	O
approximation	O
as	O
the	O
number	O
of	O
test	O
points	O
increases	O
iterative	O
solution	O
of	O
linear	B
systems	O
one	O
straightforward	O
method	O
to	O
speed	O
up	O
gp	O
regression	O
is	O
to	O
note	O
that	O
the	O
linear	B
system	O
niv	O
y	O
can	O
be	O
solved	O
by	O
an	O
iterative	O
method	O
for	O
example	O
conjugate	O
gradients	O
golub	O
and	O
van	O
loan	O
sec	O
for	O
further	O
details	O
on	O
the	O
cg	O
method	O
conjugate	O
gradients	O
gives	O
the	O
exact	O
solution	O
round-off	O
errors	O
if	O
run	O
for	O
n	O
iterations	O
but	O
it	O
will	O
give	O
an	O
approximate	O
solution	O
if	O
terminated	O
earlier	O
say	O
after	O
k	O
iterations	O
with	O
time	O
complexity	O
this	O
method	O
has	O
been	O
suggested	O
by	O
wahba	O
et	O
al	O
the	O
context	O
of	O
numerical	O
weather	O
prediction	O
and	O
by	O
gibbs	B
and	O
mackay	O
the	O
context	O
of	O
general	O
gp	O
regression	O
cg	O
methods	O
have	O
also	O
been	O
used	O
in	O
the	O
context	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
method	O
m	O
sd	O
sr	O
pp	O
bcm	O
smse	O
msll	O
mean	O
runtime	O
table	O
test	O
results	O
on	O
the	O
inverse	O
dynamics	O
problem	O
for	O
a	O
number	O
of	O
different	O
methods	O
ten	O
repetitions	O
were	O
used	O
the	O
mean	O
loss	B
is	O
shown	O
one	O
standard	O
deviation	O
of	O
laplace	O
gpc	O
where	O
linear	B
systems	O
are	O
solved	O
repeatedly	O
to	O
obtain	O
the	O
map	B
solution	O
f	O
sections	O
and	O
for	O
details	O
one	O
way	O
that	O
the	O
cg	O
method	O
can	O
be	O
speeded	O
up	O
is	O
by	O
using	O
an	O
approximate	O
rather	O
than	O
exact	O
matrix-vector	O
multiplication	O
for	O
example	O
recent	O
work	O
by	O
yang	O
et	O
al	O
uses	O
the	O
improved	O
fast	O
gauss	O
transform	O
for	O
this	O
purpose	O
comparison	O
of	O
approximate	O
gpr	O
methods	O
above	O
we	O
have	O
presented	O
six	O
approximation	O
methods	O
for	O
gpr	O
of	O
these	O
we	O
retain	O
only	O
those	O
methods	O
which	O
scale	O
linearly	O
with	O
n	O
so	O
the	O
iterative	O
solution	O
of	O
linear	B
systems	O
must	O
be	O
discounted	O
also	O
we	O
discount	O
the	O
nystr	O
om	O
approximation	O
in	O
preference	O
to	O
the	O
sr	O
method	O
leaving	O
four	O
alternatives	O
subset	B
of	I
regressors	I
subset	O
of	O
data	O
projected	O
process	O
and	O
bayesian	B
committee	I
machine	I
table	O
shows	O
results	O
of	O
the	O
four	O
methods	O
on	O
the	O
robot	B
arm	O
inverse	O
dynamics	O
problem	O
described	O
in	O
section	O
which	O
has	O
d	O
input	O
variables	O
training	O
examples	O
and	O
test	O
examples	O
as	O
in	O
section	O
we	O
used	O
the	O
squared	B
exponential	B
covariance	B
function	B
with	O
a	O
separate	O
length-scale	B
parameter	O
for	O
each	O
of	O
the	O
input	O
dimensions	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
method	O
sd	O
sr	O
pp	O
bcm	O
storage	O
omn	O
omn	O
omn	O
variance	O
initialization	O
mean	O
om	O
om	O
om	O
omn	O
omn	O
table	O
a	O
comparison	O
of	O
the	O
space	O
and	O
time	O
complexity	O
of	O
the	O
four	O
methods	O
using	O
random	O
selection	O
of	O
subsets	O
initialization	O
gives	O
the	O
time	O
needed	O
to	O
carry	O
out	O
preliminary	O
matrix	O
computations	O
before	O
the	O
test	O
point	O
x	O
is	O
known	O
mean	O
variance	O
refers	O
to	O
the	O
time	O
needed	O
to	O
compute	O
the	O
predictive	B
mean	O
at	O
x	O
for	O
the	O
sd	O
method	O
a	O
subset	O
of	O
the	O
training	O
data	O
of	O
size	O
m	O
was	O
selected	O
at	O
random	O
and	O
the	O
hyperparameters	B
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
on	O
this	O
subset	O
as	O
ard	O
was	O
used	O
this	O
involved	O
the	O
optimization	O
of	O
d	O
hyperparameters	B
this	O
process	O
was	O
repeated	O
times	O
giving	O
rise	O
to	O
the	O
mean	O
and	O
standard	O
deviation	O
recorded	O
in	O
table	O
for	O
the	O
sr	O
pp	O
and	O
bcm	O
methods	O
the	O
same	O
subsets	O
of	O
the	O
data	O
and	O
hyperparameter	O
vectors	O
were	O
used	O
as	O
had	O
been	O
obtained	O
from	O
the	O
sd	O
note	O
that	O
the	O
m	O
result	O
is	O
not	O
available	O
for	O
bcm	O
as	O
this	O
gave	O
an	O
out-of-memory	O
error	O
these	O
experiments	O
were	O
conducted	O
on	O
a	O
ghz	O
twin	O
processor	O
machine	O
with	O
gb	O
of	O
ram	O
the	O
code	O
for	O
all	O
four	O
methods	O
was	O
written	O
in	O
a	O
summary	O
of	O
the	O
time	O
complexities	O
for	O
the	O
four	O
methods	O
are	O
given	O
in	O
table	O
thus	O
for	O
a	O
test	O
set	B
of	O
size	O
n	O
and	O
using	O
full	O
and	O
variance	O
predictions	O
we	O
find	O
that	O
the	O
sd	O
method	O
has	O
time	O
complexity	O
for	O
the	O
sr	O
and	O
pp	O
methods	O
it	O
is	O
and	O
for	O
the	O
bcm	O
method	O
it	O
is	O
omnn	O
assuming	O
that	O
n	O
m	O
these	O
reduce	O
to	O
and	O
omnn	O
respectively	O
these	O
complexities	O
are	O
in	O
broad	O
agreement	O
with	O
the	O
timings	O
in	O
table	O
the	O
results	O
from	O
table	O
are	O
plotted	O
in	O
figure	O
as	O
we	O
would	O
expect	O
the	O
general	O
trend	O
is	O
that	O
as	O
m	O
increases	O
the	O
smse	O
and	O
msll	O
scores	O
decrease	O
notice	O
that	O
it	O
is	O
well	O
worth	O
doing	O
runs	O
with	O
small	O
m	O
so	O
as	O
to	O
obtain	O
a	O
learning	B
curve	I
with	O
respect	O
to	O
m	O
this	O
helps	O
in	O
getting	O
a	O
feeling	O
of	O
how	O
useful	O
runs	O
at	O
large	O
m	O
will	O
be	O
both	O
in	O
terms	O
of	O
smse	O
and	O
msll	O
we	O
see	O
surprisingly	O
that	O
sd	O
is	O
inferior	O
to	O
the	O
other	O
methods	O
all	O
of	O
which	O
have	O
similar	O
performance	O
these	O
results	O
were	O
obtained	O
using	O
a	O
random	O
selection	O
of	O
the	O
active	O
set	B
some	O
experiments	O
were	O
also	O
carried	O
out	O
using	O
active	O
selection	O
for	O
the	O
sd	O
method	O
and	O
for	O
the	O
sr	O
method	O
but	O
these	O
did	O
not	O
lead	O
to	O
significant	O
improvements	O
in	O
performance	O
for	O
bcm	O
we	O
also	O
experimented	O
with	O
the	O
use	O
of	O
p-means	O
clustering	O
instead	O
of	O
random	O
assignment	O
to	O
partitions	O
again	O
this	O
did	O
not	O
lead	O
to	O
significant	O
improvements	O
in	O
performance	O
overall	O
on	O
this	O
dataset	O
our	O
con	O
the	O
bcm	O
case	O
it	O
was	O
only	O
the	O
hyperparameters	B
that	O
were	O
re-used	O
the	O
data	O
was	O
parti	O
tioned	O
randomly	O
into	O
blocks	O
of	O
size	O
m	O
thank	O
anton	O
schwaighofer	O
for	O
making	O
his	O
bcm	O
code	O
available	O
to	O
us	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
figure	O
panela	O
plot	O
of	O
smse	O
against	O
m	O
panelb	O
shows	O
the	O
msll	O
for	O
the	O
four	O
methods	O
the	O
error	O
bars	O
denote	O
one	O
standard	O
deviation	O
for	O
clarity	O
in	O
both	O
panels	O
the	O
bcm	O
results	O
are	O
slightly	O
displaced	O
horizontally	O
w	O
r	O
t	O
the	O
sr	O
results	O
clusion	O
is	O
that	O
for	O
fixed	O
m	O
sr	O
or	O
pp	O
are	O
the	O
methods	O
of	O
choice	O
as	O
bcm	O
has	O
longer	O
running	O
times	O
for	O
similar	O
performance	O
however	O
notice	O
that	O
if	O
we	O
compare	O
on	O
runtime	O
then	O
sd	O
for	O
m	O
is	O
competitive	O
with	O
the	O
sr	O
pp	O
and	O
bcm	O
results	O
for	O
m	O
on	O
both	O
time	O
and	O
performance	O
in	O
the	O
above	O
experiments	O
the	O
hyperparameters	B
for	O
all	O
methods	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	B
of	O
the	O
sd	O
model	O
of	O
size	O
m	O
this	O
means	O
that	O
we	O
get	O
a	O
direct	O
comparison	O
of	O
the	O
different	O
methods	O
using	O
the	O
same	O
hyperparameters	B
and	O
subsets	O
however	O
one	O
could	O
alternatively	O
optimize	O
the	O
marginal	B
likelihood	B
for	O
each	O
method	O
section	O
and	O
then	O
compare	O
results	O
notice	O
that	O
the	O
hyperparameters	B
which	O
optimize	O
the	O
approximate	O
marginal	B
likelihood	B
may	O
depend	O
on	O
the	O
method	O
for	O
example	O
figure	O
shows	O
that	O
the	O
maximum	O
in	O
the	O
marginal	B
likelihood	B
occurs	O
at	O
shorter	O
length-scales	O
as	O
the	O
amount	O
of	O
data	O
increases	O
this	O
effect	O
has	O
also	O
been	O
observed	O
by	O
v	O
tresp	O
and	O
a	O
schwaighofer	O
comm	O
when	O
comparing	O
the	O
sd	O
marginal	B
likelihood	B
eq	O
with	O
the	O
full	O
marginal	B
likelihood	B
computed	O
on	O
all	O
n	O
datapoints	O
eq	O
schwaighofer	O
and	O
tresp	O
report	O
some	O
experimental	O
comparisons	O
between	O
the	O
bcm	O
method	O
and	O
some	O
other	O
approximation	O
methods	O
for	O
a	O
number	O
of	O
synthetic	O
regression	O
problems	O
in	O
these	O
experiments	O
they	O
optimized	O
the	O
kernel	B
hyperparameters	B
for	O
each	O
method	O
separately	O
their	O
results	O
are	O
that	O
for	O
fixed	O
m	O
bcm	O
performs	O
as	O
well	O
as	O
or	O
better	O
than	O
the	O
other	O
methods	O
however	O
these	O
results	O
depend	O
on	O
factors	O
such	O
as	O
the	O
noise	O
level	O
in	O
the	O
data	O
generating	O
process	O
they	O
report	O
comm	O
that	O
for	O
relatively	O
large	O
noise	O
levels	O
bcm	O
no	O
longer	O
displays	O
an	O
advantage	O
based	O
on	O
the	O
evidence	O
currently	O
available	O
we	O
are	O
unable	O
to	O
provide	O
firm	O
recommendations	O
for	O
one	O
approximation	O
method	O
over	O
another	O
further	O
research	O
is	O
required	O
to	O
understand	O
the	O
factors	O
that	O
affect	O
performance	O
and	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximations	O
for	O
gpc	O
with	O
fixed	O
hyperparameters	B
approximations	O
for	O
gpc	O
with	O
fixed	O
hy	O
perparameters	O
the	O
approximation	O
methods	O
for	O
gpc	O
are	O
similar	O
to	O
those	O
for	O
gpr	O
but	O
need	O
to	O
deal	O
with	O
the	O
non-gaussian	B
likelihood	B
as	O
well	O
either	O
by	O
using	O
the	O
laplace	B
approximation	I
see	O
section	O
or	O
expectation	B
propagation	I
see	O
section	O
in	O
this	O
section	O
we	O
focus	O
mainly	O
on	O
binary	B
classification	B
tasks	O
although	O
some	O
of	O
the	O
methods	O
can	O
also	O
be	O
extended	O
to	O
the	O
multi-class	B
case	O
pm	O
for	O
the	O
subset	B
of	I
regressors	I
method	O
we	O
again	O
use	O
the	O
model	O
fsrx	O
ikx	O
xi	O
with	O
m	O
n	O
k	O
mm	O
the	O
likelihood	B
is	O
non-gaussian	B
but	O
the	O
optimization	O
problem	O
to	O
find	O
the	O
map	B
value	O
of	O
m	O
is	O
convex	O
and	O
can	O
be	O
obtained	O
using	O
a	O
newton	O
iteration	O
using	O
the	O
map	B
value	O
m	O
and	O
the	O
hessian	O
at	O
this	O
point	O
we	O
obtain	O
a	O
predictive	B
mean	O
and	O
variance	O
for	O
fx	O
which	O
can	O
be	O
fed	O
through	O
the	O
sigmoid	O
function	B
to	O
yield	O
probabilistic	B
predictions	O
as	O
usual	O
the	O
question	O
of	O
how	O
to	O
choose	O
a	O
subset	O
of	O
points	O
arises	O
lin	O
et	O
al	O
select	O
these	O
using	O
a	O
clustering	O
method	O
while	O
zhu	O
and	O
hastie	O
propose	O
a	O
forward	O
selection	O
strategy	O
the	O
subset	B
of	I
datapoints	I
method	O
for	O
gpc	O
was	O
proposed	O
in	O
lawrence	O
et	O
al	O
using	O
an	O
ep-style	O
approximation	O
of	O
the	O
posterior	O
and	O
the	O
differential	O
entropy	B
score	O
section	O
to	O
select	O
new	O
sites	O
for	O
inclusion	O
note	O
that	O
the	O
ep	O
approximation	O
lends	O
itself	O
very	O
naturally	O
to	O
sparsification	O
a	O
sparse	O
model	O
results	O
when	O
some	O
site	O
precisions	O
eq	O
are	O
zero	O
making	O
the	O
corresponding	O
likelihood	B
term	O
vanish	O
a	O
computational	O
gain	O
can	O
thus	O
be	O
achieved	O
by	O
ignoring	O
likelihood	B
terms	O
whose	O
site	O
precisions	O
are	O
very	O
small	O
the	O
projected	B
process	I
approximation	I
can	O
also	O
be	O
used	O
with	O
nongaussian	O
likelihoods	O
csat	O
o	O
and	O
opper	O
present	O
an	O
online	O
method	O
where	O
the	O
examples	O
are	O
processed	O
sequentially	O
while	O
csat	O
o	O
et	O
al	O
give	O
an	O
expectation-propagation	O
type	O
algorithm	O
where	O
multiple	O
sweeps	O
through	O
the	O
training	O
data	O
are	O
permitted	O
the	O
bayesian	B
committee	I
machine	I
has	O
also	O
been	O
generalized	B
to	O
deal	O
with	O
non-gaussian	B
likelihoods	O
in	O
tresp	O
as	O
in	O
the	O
gpr	O
case	O
the	O
dataset	O
is	O
broken	O
up	O
into	O
blocks	O
but	O
now	O
approximate	O
inference	O
is	O
carried	O
out	O
using	O
the	O
laplace	B
approximation	I
in	O
each	O
block	O
to	O
yield	O
an	O
approximate	O
predictive	B
mean	O
eqf	O
and	O
approximate	O
predictive	B
covariance	B
covqf	O
these	O
predictions	O
are	O
then	O
combined	O
as	O
before	O
using	O
equations	O
and	O
approximating	O
the	O
marginal	B
likelihood	B
and	O
its	O
derivatives	O
we	O
consider	O
approximations	O
first	O
for	O
gp	O
regression	O
and	O
then	O
for	O
gp	O
classification	B
for	O
gpr	O
both	O
the	O
sr	O
and	O
pp	O
methods	O
give	O
rise	O
to	O
the	O
same	O
approximate	O
marginal	B
likelihood	B
as	O
given	O
in	O
eq	O
for	O
the	O
sd	O
method	O
a	O
very	O
simple	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
y	O
log	O
mkmm	O
m	O
approximation	O
the	O
datapoints	O
not	O
in	O
the	O
active	O
set	B
is	O
given	O
by	O
log	O
psdymxm	O
where	O
ym	O
is	O
the	O
subvector	O
of	O
y	O
corresponding	O
to	O
the	O
active	O
set	B
eq	O
is	O
simply	O
the	O
log	O
marginal	B
likelihood	B
under	O
the	O
model	O
ym	O
n	O
kmm	O
for	O
the	O
bcm	O
a	O
simple	O
approach	O
would	O
be	O
to	O
sum	O
eq	O
evaluated	O
on	O
each	O
partition	O
of	O
the	O
dataset	O
this	O
ignores	O
interactions	O
between	O
the	O
partitions	O
tresp	O
and	O
schwaighofer	O
comm	O
have	O
suggested	O
a	O
more	O
sophisticated	O
bcm-based	O
method	O
which	O
approximately	O
takes	O
these	O
interactions	O
into	O
account	O
for	O
gpc	O
under	O
the	O
sr	O
approximation	O
one	O
can	O
simply	O
use	O
the	O
laplace	O
or	O
ep	O
approximations	O
on	O
the	O
finite-dimensional	O
model	O
for	O
sd	O
one	O
can	O
again	O
ignore	O
all	O
datapoints	O
not	O
in	O
the	O
active	O
set	B
and	O
compute	O
an	O
approximation	O
to	O
log	O
pymxm	O
using	O
either	O
laplace	O
or	O
ep	O
for	O
the	O
projected	O
process	O
method	O
seeger	B
p	O
suggests	O
the	O
following	O
lower	O
bound	O
z	O
qf	O
pyfpf	O
qf	O
df	O
z	O
z	O
z	O
z	O
nx	O
log	O
pyx	O
log	O
pyfpf	O
df	O
log	O
pyfpf	O
qf	O
qf	O
log	O
df	O
qf	O
log	O
qyf	O
df	O
klqfpf	O
qfi	O
log	O
pyifi	O
dfi	O
klqfmpfm	O
where	O
qf	O
is	O
a	O
shorthand	O
for	O
qfy	O
and	O
eq	O
follows	O
from	O
the	O
equation	O
on	O
the	O
previous	O
line	O
using	O
jensen	O
s	O
inequality	O
the	O
kl	O
divergence	O
term	O
can	O
be	O
readily	O
evaluated	O
using	O
eq	O
and	O
the	O
one-dimensional	O
integrals	B
can	O
be	O
tackled	O
using	O
numerical	O
quadrature	O
we	O
are	O
not	O
aware	O
of	O
work	O
on	O
extending	O
the	O
bcm	O
approximations	O
to	O
the	O
marginal	B
likelihood	B
to	O
gpc	O
given	O
the	O
various	O
approximations	O
to	O
the	O
marginal	B
likelihood	B
mentioned	O
above	O
we	O
may	O
also	O
want	O
to	O
compute	O
derivatives	O
in	O
order	O
to	O
optimize	O
it	O
clearly	O
it	O
will	O
make	O
sense	O
to	O
keep	O
the	O
active	O
set	B
fixed	O
during	O
the	O
optimization	O
although	O
note	O
that	O
this	O
clashes	O
with	O
the	O
fact	O
that	O
methods	O
that	O
select	O
the	O
active	O
set	B
might	O
choose	O
a	O
different	O
set	B
as	O
the	O
covariance	B
function	B
parameters	O
change	O
for	O
the	O
classification	B
case	O
the	O
derivatives	O
can	O
be	O
quite	O
complex	O
due	O
to	O
the	O
fact	O
that	O
site	O
parameters	O
as	O
the	O
map	B
values	O
f	O
see	O
section	O
change	O
as	O
changes	O
have	O
already	O
seen	O
an	O
example	O
of	O
this	O
in	O
section	O
for	O
the	O
non-sparse	O
laplace	B
approximation	I
seeger	B
sec	O
describes	O
some	O
experiments	O
comparing	O
sd	O
and	O
pp	O
methods	O
for	O
the	O
optimization	O
of	O
the	O
marginal	B
likelihood	B
on	O
both	O
regression	O
and	O
classification	B
problems	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
appendix	O
equivalence	O
of	O
sr	O
and	O
gpr	O
using	O
the	O
nystr	O
om	O
approximate	O
kernel	B
appendix	O
equivalence	O
of	O
sr	O
and	O
gpr	O
us-	O
ing	O
the	O
nystr	O
om	O
approximate	O
kernel	B
in	O
section	O
we	O
derived	O
the	O
subset	B
of	I
regressors	I
predictors	O
for	O
the	O
mean	O
and	O
variance	O
as	O
given	O
in	O
equations	O
and	O
the	O
aim	O
of	O
this	O
appendix	O
is	O
to	O
show	O
that	O
these	O
are	O
equivalent	B
to	O
the	O
predictors	O
that	O
are	O
obtained	O
by	O
replacing	O
kx	O
systematically	O
with	O
kx	O
in	O
the	O
gpr	O
prediction	O
equations	O
and	O
first	O
for	O
the	O
mean	O
the	O
gpr	O
predictor	O
is	O
efx	O
kx	O
replacing	O
all	O
occurrences	O
of	O
kx	O
with	O
kx	O
we	O
obtain	O
ni	O
ni	O
e	O
fx	O
kx	O
k	O
kmx	O
kmx	O
n	O
kmx	O
n	O
kmx	O
n	O
kmx	O
mm	O
mm	O
mmkmn	O
mmkmnknmk	O
ni	O
mmkmn	O
knmq	O
y	O
kmnknmq	O
kmny	O
nkmmq	O
kmny	O
nkmm	O
kmnknm	O
which	O
agrees	O
with	O
eq	O
equation	O
where	O
q	O
follows	O
from	O
eq	O
by	O
use	O
of	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
and	O
nkmm	O
kmnknmq	O
for	O
eq	O
follows	O
from	O
eq	O
using	O
im	O
the	O
predictive	B
variance	O
we	O
have	O
v	O
f	O
kx	O
x	O
kx	O
k	O
ni	O
kx	O
kmx	O
kmx	O
kmx	O
mmkmx	O
mmkmnknmk	O
mmkmx	O
kmx	O
mmkmn	O
ni	O
kmx	O
q	O
k	O
mmkmx	O
kmx	O
nkmmk	O
nkmx	O
mmkmx	O
mmkmx	O
mmkmx	O
in	O
agreement	O
with	O
eq	O
the	O
step	O
between	O
eqs	O
and	O
is	O
obtained	O
from	O
eqs	O
and	O
above	O
and	O
eq	O
follows	O
from	O
eq	O
using	O
im	O
nkmm	O
kmnknmq	O
exercises	O
verify	O
that	O
the	O
mean	O
and	O
covariance	B
of	O
the	O
bcm	O
predictions	O
and	O
are	O
correct	O
if	O
you	O
are	O
stuck	O
see	O
tresp	O
for	O
details	O
mm	O
show	O
that	O
ecopt	O
trk	O
k	O
where	O
k	O
knmk	O
mmkmn	O
now	O
consider	O
adding	O
one	O
datapoint	O
into	O
set	B
i	O
so	O
that	O
kmm	O
grows	O
to	O
using	O
eq	O
using	O
eq	O
and	O
the	O
fact	O
that	O
copt	O
knmk	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
approximation	O
methods	O
for	O
large	O
datasets	O
show	O
that	O
the	O
change	O
in	O
e	O
due	O
to	O
adding	O
the	O
extra	O
datapoint	O
can	O
be	O
computed	O
in	O
time	O
omn	O
if	O
you	O
need	O
help	O
see	O
sch	O
olkopf	O
and	O
smola	O
sec	O
for	O
further	O
details	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
chapter	O
further	O
issues	O
and	O
conclusions	O
in	O
the	O
previous	O
chapters	O
of	O
the	O
book	O
we	O
have	O
concentrated	O
on	O
giving	O
a	O
solid	O
grounding	O
in	O
the	O
use	O
of	O
gps	O
for	O
regression	O
and	O
classification	B
problems	O
including	O
model	O
selection	O
issues	O
approximation	O
methods	O
for	O
large	O
datasets	O
and	O
connections	O
to	O
related	O
models	O
in	O
this	O
chapter	O
we	O
provide	O
some	O
short	O
descriptions	O
of	O
other	O
issues	O
relating	O
to	O
gaussian	B
process	I
prediction	O
with	O
pointers	O
to	O
the	O
literature	O
for	O
further	O
reading	O
so	O
far	O
we	O
have	O
mainly	O
discussed	O
the	O
case	O
when	O
the	O
output	O
target	O
y	O
is	O
a	O
single	O
label	O
but	O
in	O
section	O
we	O
describe	O
how	O
to	O
deal	O
with	O
the	O
case	O
that	O
there	O
are	O
multiple	O
output	O
targets	O
similarly	O
for	O
the	O
regression	O
problem	O
we	O
have	O
focussed	O
on	O
i	O
i	O
d	O
gaussian	O
noise	O
in	O
section	O
we	O
relax	O
this	O
condition	O
to	O
allow	O
the	O
noise	O
process	O
to	O
have	O
correlations	O
the	O
classification	B
problem	O
is	O
characterized	O
by	O
a	O
non-gaussian	B
likelihood	B
function	B
however	O
there	O
are	O
other	O
non-gaussian	B
likelihoods	O
of	O
interest	O
as	O
described	O
in	O
section	O
we	O
may	O
not	O
only	O
have	O
observations	O
of	O
function	B
values	O
by	O
also	O
on	O
derivatives	O
of	O
the	O
target	O
function	B
in	O
section	O
we	O
discuss	O
how	O
to	O
make	O
use	O
of	O
this	O
information	O
in	O
the	O
gpr	O
framework	O
also	O
it	O
may	O
happen	O
that	O
there	O
is	O
noise	O
on	O
the	O
observation	O
of	O
the	O
input	O
variable	O
x	O
in	O
section	O
we	O
explain	O
how	O
this	O
can	O
be	O
handled	O
in	O
section	O
we	O
mention	O
how	O
more	O
flexible	O
models	O
can	O
be	O
obtained	O
using	O
mixtures	O
of	O
gaussian	B
process	I
models	O
as	O
well	O
as	O
carrying	O
out	O
prediction	O
for	O
test	O
inputs	O
one	O
might	O
also	O
wish	O
to	O
try	O
to	O
find	O
the	O
global	O
optimum	O
of	O
a	O
function	B
within	O
some	O
compact	O
set	B
approaches	O
based	O
on	O
gaussian	O
processes	O
for	O
this	O
problem	O
are	O
described	O
in	O
section	O
the	O
use	O
of	O
gaussian	O
processes	O
to	O
evaluate	O
integrals	B
is	O
covered	O
in	O
section	O
by	O
using	O
a	O
scale	B
mixture	I
of	O
gaussians	O
construction	O
one	O
can	O
obtain	O
a	O
multivariate	O
student	O
s	O
t	O
distribution	O
this	O
construction	O
can	O
be	O
extended	O
to	O
give	O
a	O
student	O
s	O
t	O
process	O
as	O
explained	O
in	O
section	O
one	O
key	O
aspect	O
of	O
the	O
bayesian	O
framework	O
relates	O
to	O
the	O
incorporation	O
of	O
prior	O
knowledge	O
into	O
the	O
problem	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
further	O
issues	O
and	O
conclusions	O
formulation	O
in	O
some	O
applications	O
we	O
not	O
only	O
have	O
the	O
dataset	O
d	O
but	O
also	O
additional	O
information	O
for	O
example	O
for	O
an	O
optical	O
character	O
recognition	O
problem	O
we	O
know	O
that	O
translating	O
the	O
input	O
pattern	O
by	O
one	O
pixel	O
will	O
not	O
change	O
the	O
label	O
of	O
the	O
pattern	O
approaches	O
for	O
incorporating	O
this	O
knowledge	O
are	O
discussed	O
in	O
section	O
in	O
this	O
book	O
we	O
have	O
concentrated	O
on	O
supervised	B
learning	B
problems	O
however	O
gps	O
can	O
be	O
used	O
as	O
components	O
in	O
unsupervised	O
learning	B
models	O
as	O
described	O
in	O
section	O
finally	O
we	O
close	O
with	O
some	O
conclusions	O
and	O
an	O
outlook	O
to	O
the	O
future	O
in	O
section	O
multiple	B
outputs	B
throughout	O
this	O
book	O
we	O
have	O
concentrated	O
on	O
the	O
problem	O
of	O
predicting	O
a	O
single	O
output	O
variable	O
y	O
from	O
an	O
input	O
x	O
however	O
it	O
can	O
happen	O
that	O
one	O
may	O
wish	O
to	O
predict	O
multiple	O
output	O
variables	O
channels	O
simultaneously	O
for	O
example	O
in	O
the	O
robot	B
inverse	O
dynamics	O
problem	O
described	O
in	O
section	O
there	O
are	O
really	O
seven	O
torques	O
to	O
be	O
predicted	O
a	O
simple	O
approach	O
is	O
to	O
model	O
each	O
output	O
variable	O
as	O
independent	O
from	O
the	O
others	O
and	O
treat	O
them	O
separately	O
however	O
this	O
may	O
lose	O
information	O
and	O
be	O
suboptimal	O
one	O
way	O
in	O
which	O
correlation	O
can	O
occur	O
is	O
through	O
a	O
correlated	B
noise	O
process	O
even	O
if	O
the	O
output	O
channels	O
are	O
a	O
priori	O
independent	O
if	O
the	O
noise	O
process	O
is	O
correlated	B
then	O
this	O
will	O
induce	O
correlations	O
in	O
the	O
posterior	O
processes	O
such	O
a	O
situation	O
is	O
easily	O
handled	O
in	O
the	O
gp	O
framework	O
by	O
considering	O
the	O
joint	B
block-diagonal	O
prior	O
over	O
the	O
function	B
values	O
of	O
each	O
channel	O
another	O
way	O
that	O
correlation	O
of	O
multiple	O
channels	O
can	O
occur	O
is	O
if	O
the	O
prior	O
already	O
has	O
this	O
structure	O
for	O
example	O
in	O
geostatistical	O
situations	O
there	O
may	O
be	O
correlations	O
between	O
the	O
abundances	O
of	O
different	O
ores	O
e	O
g	O
silver	O
and	O
lead	O
this	O
situation	O
requires	O
that	O
the	O
covariance	B
function	B
models	O
not	O
only	O
the	O
correlation	O
structure	O
of	O
each	O
channel	O
but	O
also	O
the	O
cross-correlations	O
between	O
channels	O
some	O
work	O
on	O
this	O
topic	O
can	O
be	O
found	O
in	O
the	O
geostatistics	B
literature	O
under	O
the	O
name	O
of	O
cokriging	B
see	O
e	O
g	O
cressie	O
sec	O
one	O
way	O
to	O
induce	O
correlations	O
between	O
a	O
number	O
of	O
output	O
channels	O
is	O
to	O
obtain	O
them	O
as	O
linear	B
combinations	O
of	O
a	O
number	O
of	O
latent	O
channels	O
as	O
described	O
in	O
teh	O
et	O
al	O
see	O
also	O
micchelli	O
and	O
pontil	O
a	O
related	O
approach	O
is	O
taken	O
by	O
boyle	O
and	O
frean	O
who	O
introduce	O
correlations	O
between	O
two	O
processes	O
by	O
deriving	O
them	O
as	O
different	O
convolutions	O
of	O
the	O
same	O
underlying	O
white	O
noise	O
process	O
noise	O
models	O
with	O
dependencies	O
the	O
noise	O
models	O
used	O
so	O
far	O
have	O
almost	O
exclusively	O
assumed	O
gaussianity	O
and	O
independence	O
non-gaussian	B
likelihoods	O
are	O
mentioned	O
in	O
section	O
below	O
inside	O
the	O
family	O
of	O
gaussian	O
noise	O
models	O
it	O
is	O
not	O
difficult	O
to	O
model	O
dependencies	O
this	O
may	O
be	O
particularly	O
useful	O
in	O
models	O
involving	O
time	O
we	O
simply	O
add	O
terms	O
to	O
the	O
noise	O
covariance	B
function	B
with	O
the	O
desired	O
structure	O
including	O
cokriging	B
coloured	O
noise	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
arma	O
non-gaussian	B
likelihoods	O
hyperparameters	B
in	O
fact	O
we	O
already	O
used	O
this	O
approach	O
for	O
the	O
atmospheric	O
carbon	O
dioxide	O
modelling	O
task	O
in	O
section	O
also	O
murray-smith	O
and	O
girard	O
have	O
used	O
an	O
autoregressive	O
moving-average	O
noise	B
model	I
also	O
eq	O
in	O
a	O
gp	O
regression	O
task	O
non-gaussian	B
likelihoods	O
our	O
main	O
focus	O
has	O
been	O
on	O
regression	O
with	O
gaussian	O
noise	O
and	O
classification	B
using	O
the	O
logistic	B
or	O
probit	B
response	O
functions	O
however	O
gaussian	O
processes	O
can	O
be	O
used	O
as	O
priors	O
with	O
other	O
likelihood	B
functions	O
for	O
example	O
diggle	O
et	O
al	O
were	O
concerned	O
with	O
modelling	O
count	O
data	O
measured	O
geographically	O
using	O
a	O
poisson	O
likelihood	B
with	O
a	O
spatially	O
varying	O
rate	O
they	O
achieved	O
this	O
by	O
placing	O
a	O
gp	O
prior	O
over	O
the	O
log	O
poisson	O
rate	O
goldberg	O
et	O
al	O
stayed	O
with	O
a	O
gaussian	O
noise	B
model	I
but	O
introduced	O
heteroscedasticity	O
i	O
e	O
allowing	O
the	O
noise	O
variance	O
to	O
be	O
a	O
function	B
of	O
x	O
this	O
was	O
achieved	O
by	O
placing	O
a	O
gp	O
prior	O
on	O
the	O
log	O
variance	O
function	B
neal	O
robustified	O
gp	O
regression	O
by	O
using	O
a	O
student	O
s	O
t-distributed	O
noise	B
model	I
rather	O
than	O
gaussian	O
noise	O
chu	O
and	O
ghahramani	O
have	O
described	O
how	O
to	O
use	O
gps	O
for	O
the	O
ordinal	O
regression	O
problem	O
where	O
one	O
is	O
given	O
ranked	O
preference	O
information	O
as	O
the	O
target	O
data	O
derivative	B
observations	I
since	O
differentiation	O
is	O
a	O
linear	B
operator	B
the	O
derivative	O
of	O
a	O
gaussian	B
process	I
is	O
another	O
gaussian	B
process	I
thus	O
we	O
can	O
use	O
gps	O
to	O
make	O
predictions	O
about	O
derivatives	O
and	O
also	O
to	O
make	O
inference	O
based	O
on	O
derivative	O
information	O
in	O
general	O
we	O
can	O
make	O
inference	O
based	O
on	O
the	O
joint	B
gaussian	B
distribution	I
of	O
function	B
values	O
and	O
partial	O
derivatives	O
a	O
covariance	B
function	B
k	O
on	O
function	B
values	O
implies	O
the	O
following	O
covariance	B
between	O
function	B
values	O
and	O
partial	O
derivatives	O
and	O
between	O
partial	O
derivatives	O
kxi	O
xj	O
xdj	O
fj	O
xdj	O
fi	O
xdi	O
fj	O
xej	O
xj	O
xdi	O
xej	O
see	O
e	O
g	O
papoulis	O
ch	O
or	O
adler	O
sec	O
with	O
n	O
datapoints	O
in	O
d	O
dimensions	O
the	O
complete	O
joint	B
distribution	O
of	O
f	O
and	O
its	O
d	O
partial	O
derivatives	O
involves	O
quantities	O
but	O
in	O
a	O
typical	O
application	O
we	O
may	O
only	O
have	O
access	O
to	O
or	O
interest	O
in	O
a	O
subset	O
of	O
these	O
we	O
simply	O
remove	O
the	O
rows	O
and	O
columns	O
from	O
the	O
joint	B
matrix	O
which	O
are	O
not	O
needed	O
observed	O
function	B
values	O
and	O
derivatives	O
may	O
often	O
have	O
different	O
noise	O
levels	O
which	O
are	O
incorporated	O
by	O
adding	O
a	O
diagonal	O
contribution	O
with	O
differing	O
hyperparameters	B
inference	O
and	O
predictions	O
are	O
done	O
as	O
usual	O
this	O
approach	O
was	O
used	O
in	O
the	O
context	O
of	O
learning	B
in	O
dynamical	O
systems	O
by	O
solak	O
et	O
al	O
in	O
figure	O
the	O
posterior	B
process	I
with	O
and	O
without	O
derivative	B
observations	I
are	O
compared	O
noise-free	O
derivatives	O
may	O
be	O
a	O
useful	O
way	O
to	O
enforce	O
known	O
constraints	O
in	O
a	O
modelling	O
problem	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
further	O
issues	O
and	O
conclusions	O
figure	O
in	O
panel	O
we	O
show	O
four	O
data	O
points	O
in	O
a	O
one	O
dimensional	O
noise-free	O
regression	O
problem	O
together	O
with	O
three	O
functions	O
sampled	O
from	O
the	O
posterior	O
and	O
the	O
confidence	O
region	O
in	O
light	O
grey	O
in	O
panel	O
the	O
same	O
observations	O
have	O
been	O
augmented	O
by	O
noise-free	O
derivative	O
information	O
indicated	O
by	O
small	O
tangent	O
segments	O
at	O
the	O
data	O
points	O
the	O
covariance	B
function	B
is	O
the	O
squared	B
exponential	B
with	O
unit	O
process	O
variance	O
and	O
unit	O
length-scale	B
prediction	O
with	O
uncertain	B
inputs	I
it	O
can	O
happen	O
that	O
the	O
input	O
values	O
to	O
a	O
prediction	O
problem	O
can	O
be	O
uncertain	O
for	O
example	O
for	O
a	O
discrete	O
time	O
series	O
one	O
can	O
perform	O
multi-step-ahead	O
predictions	O
by	O
iterating	O
one-step-ahead	O
predictions	O
however	O
if	O
the	O
one-stepahead	O
predictions	O
include	O
uncertainty	O
then	O
it	O
is	O
necessary	O
to	O
propagate	O
this	O
uncertainty	O
forward	O
to	O
get	O
the	O
proper	O
multi-step-ahead	O
predictions	O
one	O
simple	O
approach	O
is	O
to	O
use	O
sampling	O
methods	O
alternatively	O
it	O
may	O
be	O
possible	O
to	O
use	O
analytical	O
approaches	O
girard	O
et	O
al	O
showed	O
that	O
it	O
is	O
possible	O
to	O
compute	O
the	O
mean	O
and	O
variance	O
of	O
the	O
output	O
analytically	O
when	O
using	O
the	O
se	O
covariance	B
function	B
and	O
gaussian	O
input	O
noise	O
more	O
generally	O
the	O
problem	O
of	O
regression	O
with	O
uncertain	B
inputs	I
has	O
been	O
studied	O
in	O
the	O
statistics	O
literature	O
under	O
the	O
name	O
of	O
errors-in-variables	B
regression	I
see	O
dellaportas	O
and	O
stephens	O
for	O
a	O
bayesian	O
treatment	O
of	O
the	O
problem	O
and	O
pointers	O
to	O
the	O
literature	O
mixtures	O
of	O
gaussian	O
processes	O
in	O
chapter	O
we	O
have	O
seen	O
many	O
ideas	O
for	O
making	O
the	O
covariance	B
functions	O
more	O
flexible	O
another	O
route	O
is	O
to	O
use	O
a	O
mixture	O
of	O
different	O
gaussian	B
process	I
models	O
each	O
one	O
used	O
in	O
some	O
local	O
region	O
of	O
input	O
space	O
this	O
kind	O
of	O
model	O
is	O
generally	O
known	O
as	O
a	O
mixture	B
of	I
experts	I
model	O
and	O
is	O
due	O
to	O
jacobs	O
et	O
al	O
in	O
addition	O
to	O
the	O
local	O
expert	O
models	O
the	O
model	O
has	O
a	O
manager	O
that	O
assigns	O
points	O
to	O
the	O
experts	O
rasmussen	O
and	O
ghahramani	O
used	O
gaussian	B
process	I
models	O
as	O
local	O
experts	O
and	O
based	O
their	O
manager	O
on	O
another	O
type	O
of	O
stochastic	O
process	O
the	O
dirichlet	B
process	I
inference	O
in	O
this	O
model	O
required	O
mcmc	O
methods	O
xoutput	O
yx	O
xoutput	O
yx	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
global	O
optimization	O
global	O
optimization	O
often	O
one	O
is	O
faced	O
with	O
the	O
problem	O
of	O
being	O
able	O
to	O
evaluate	O
a	O
continuous	O
function	B
gx	O
and	O
wishing	O
to	O
find	O
the	O
global	O
optimum	O
or	O
minimum	O
of	O
this	O
function	B
within	O
some	O
compact	O
set	B
a	O
rd	O
there	O
is	O
a	O
very	O
large	O
literature	O
on	O
the	O
problem	O
of	O
global	O
optimization	O
see	O
neumaier	O
for	O
a	O
useful	O
overview	O
given	O
a	O
dataset	O
d	O
n	O
one	O
appealing	O
approach	O
is	O
to	O
fit	O
a	O
gp	O
regression	O
model	O
to	O
this	O
data	O
this	O
will	O
give	O
a	O
mean	O
prediction	O
and	O
predictive	B
variance	O
for	O
every	O
x	O
a	O
jones	O
examines	O
a	O
number	O
of	O
criteria	O
that	O
have	O
been	O
suggested	O
for	O
where	O
to	O
make	O
the	O
next	O
function	B
evaluation	O
based	O
on	O
the	O
predictive	B
mean	O
and	O
variance	O
one	O
issue	O
with	O
this	O
approach	O
is	O
that	O
one	O
may	O
need	O
to	O
search	O
to	O
find	O
the	O
optimum	O
of	O
the	O
criterion	O
which	O
may	O
itself	O
be	O
multimodal	O
optimization	O
problem	O
however	O
if	O
evaluations	O
of	O
g	O
are	O
expensive	O
or	O
time-consuming	O
it	O
can	O
make	O
sense	O
to	O
work	O
hard	O
on	O
this	O
new	O
optimization	O
problem	O
for	O
historical	O
references	O
and	O
further	O
work	O
in	O
this	O
area	O
see	O
jones	O
and	O
ritter	O
sec	O
evaluation	O
of	O
integrals	B
another	O
interesting	O
and	O
unusual	O
application	O
of	O
gaussian	O
processes	O
is	O
for	O
the	O
evaluation	O
of	O
the	O
integrals	B
of	O
a	O
deterministic	O
function	B
f	O
one	O
evaluates	O
the	O
function	B
at	O
a	O
number	O
of	O
locations	O
and	O
then	O
one	O
can	O
use	O
a	O
gaussian	B
process	I
as	O
a	O
posterior	O
over	O
functions	O
this	O
posterior	O
over	O
functions	O
induces	O
a	O
posterior	O
over	O
the	O
value	O
of	O
the	O
integral	O
each	O
possible	O
function	B
from	O
the	O
posterior	O
would	O
give	O
rise	O
to	O
a	O
particular	O
value	O
of	O
the	O
integral	O
for	O
some	O
covariance	B
functions	O
the	O
squared	B
exponential	B
one	O
can	O
compute	O
the	O
expectation	O
and	O
variance	O
of	O
the	O
value	O
of	O
the	O
integral	O
analytically	O
it	O
is	O
perhaps	O
unusual	O
to	O
think	O
of	O
the	O
value	O
of	O
the	O
integral	O
as	O
being	O
random	O
it	O
does	O
have	O
one	O
particular	O
deterministic	O
value	O
but	O
it	O
is	O
perfectly	O
in	O
line	O
of	O
bayesian	O
thinking	O
that	O
you	O
treat	O
all	O
kinds	O
of	O
uncertainty	O
using	O
probabilities	O
this	O
idea	O
was	O
proposed	O
under	O
the	O
name	O
of	O
bayes-hermite	O
quadrature	O
by	O
o	O
hagan	O
and	O
later	O
under	O
the	O
name	O
of	O
bayesian	O
monte	O
carlo	O
in	O
rasmussen	O
and	O
ghahramani	O
another	O
approach	O
is	O
related	O
to	O
the	O
ideas	O
of	O
global	O
optimization	O
in	O
the	O
section	O
above	O
one	O
can	O
use	O
a	O
gp	O
model	O
of	O
a	O
function	B
to	O
aid	O
an	O
mcmc	O
sampling	O
procedure	O
which	O
may	O
be	O
advantageous	O
if	O
the	O
function	B
of	O
interest	O
is	O
computationally	O
expensive	O
to	O
evaluate	O
rasmussen	O
combines	O
hybrid	O
monte	O
carlo	O
with	O
a	O
gp	O
model	O
of	O
the	O
log	O
of	O
the	O
integrand	O
and	O
also	O
uses	O
derivatives	O
of	O
the	O
function	B
in	O
section	O
to	O
get	O
an	O
accurate	O
model	O
of	O
the	O
integrand	O
with	O
very	O
few	O
evaluations	O
combining	O
gps	O
with	O
mcmc	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
scale	B
mixture	I
noise	O
entanglement	O
further	O
issues	O
and	O
conclusions	O
student	O
s	O
t	O
process	O
a	O
student	O
s	O
t	O
process	O
can	O
be	O
obtained	O
by	O
applying	O
the	O
scale	B
mixture	I
of	O
gaussians	O
construction	O
of	O
a	O
student	O
s	O
t	O
distribution	O
to	O
a	O
gaussian	B
process	I
hagan	O
o	O
hagan	O
et	O
al	O
we	O
divide	O
the	O
covariances	O
by	O
the	O
scalar	O
and	O
put	O
a	O
gamma	B
distribution	I
on	O
with	O
shape	O
and	O
mean	O
so	O
that	O
exp	O
kx	O
p	O
z	O
where	O
k	O
is	O
any	O
valid	O
covariance	B
function	B
now	O
the	O
joint	B
prior	O
distribution	O
of	O
any	O
finite	O
number	O
n	O
of	O
function	B
values	O
y	O
becomes	O
n	O
py	O
yk	O
y	O
y	O
which	O
is	O
recognized	O
as	O
the	O
zero	O
mean	O
multivariate	O
student	O
s	O
t	O
distribution	O
with	O
degrees	B
of	I
freedom	I
py	O
t	O
we	O
could	O
state	O
a	O
definition	O
analogous	O
to	O
definition	O
on	O
page	O
for	O
the	O
gaussian	B
process	I
and	O
write	O
cf	O
eq	O
the	O
marginal	B
likelihood	B
can	O
be	O
directly	O
evaluated	O
using	O
eq	O
and	O
training	O
can	O
be	O
achieved	O
using	O
the	O
methods	O
discussed	O
in	O
chapter	O
regarding	O
and	O
as	O
hyperparameters	B
the	O
predictive	B
distribution	O
for	O
test	O
cases	O
are	O
also	O
t	O
distributions	O
the	O
derivation	O
of	O
which	O
is	O
left	O
as	O
an	O
exercise	O
below	O
f	O
t	O
notice	O
that	O
the	O
above	O
construction	O
is	O
clear	O
for	O
noise-free	O
processes	O
but	O
that	O
the	O
interpretation	O
becomes	O
more	O
complicated	O
if	O
the	O
covariance	B
function	B
kx	O
contains	O
a	O
noise	O
contribution	O
the	O
noise	O
and	O
signal	O
get	O
entangled	O
by	O
the	O
common	O
factor	O
and	O
the	O
observations	O
can	O
no	O
longer	O
be	O
written	O
as	O
the	O
sum	O
of	O
independent	O
signal	O
and	O
noise	O
contributions	O
allowing	O
for	O
independent	O
noise	O
contributions	O
removes	O
analytic	O
tractability	O
which	O
may	O
reduce	O
the	O
usefulness	O
of	O
the	O
t	O
process	O
exercise	O
using	O
the	O
scale	B
mixture	I
representation	O
from	O
eq	O
derive	O
the	O
posterior	O
predictive	B
distribution	O
for	O
a	O
student	O
s	O
t	O
process	O
exercise	O
consider	O
the	O
generating	O
process	O
implied	O
by	O
eq	O
and	O
write	O
a	O
program	O
to	O
draw	O
functions	O
at	O
random	O
characterize	O
the	O
difference	O
between	O
the	O
student	O
s	O
t	O
process	O
and	O
the	O
corresponding	O
gaussian	B
process	I
in	O
the	O
limit	O
and	O
explain	O
why	O
the	O
t	O
process	O
is	O
perhaps	O
not	O
as	O
exciting	O
as	O
one	O
might	O
have	O
hoped	O
invariances	B
it	O
can	O
happen	O
that	O
the	O
input	O
is	O
apparently	O
in	O
vector	O
form	O
but	O
in	O
fact	O
it	O
has	O
additional	O
structure	O
a	O
good	O
example	O
is	O
a	O
pixelated	O
image	O
where	O
the	O
array	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
invariances	B
of	O
pixels	O
can	O
be	O
arranged	O
into	O
a	O
vector	O
in	O
raster-scan	O
order	O
imagine	O
that	O
the	O
image	O
is	O
of	O
a	O
handwritten	O
digit	O
then	O
we	O
know	O
that	O
if	O
the	O
image	O
is	O
translated	O
by	O
one	O
pixel	O
it	O
will	O
remain	O
the	O
same	O
digit	O
thus	O
we	O
have	O
knowledge	O
of	O
certain	O
invariances	B
of	O
the	O
input	O
pattern	O
in	O
this	O
section	O
we	O
describe	O
a	O
number	O
of	O
ways	O
in	O
which	O
such	O
invariances	B
can	O
be	O
exploited	O
our	O
discussion	O
is	O
based	O
on	O
sch	O
olkopf	O
and	O
smola	O
ch	O
prior	O
knowledge	O
about	O
the	O
problem	O
tells	O
us	O
that	O
certain	O
transformations	O
of	O
the	O
input	O
would	O
leave	O
the	O
class	O
label	O
invariant	O
these	O
include	O
simple	O
geometric	O
transformations	O
such	O
as	O
translations	O
rescalings	O
and	O
rather	O
less	O
obvious	O
ones	O
such	O
as	O
line	O
thickness	O
given	O
enough	O
data	O
it	O
should	O
be	O
possible	O
to	O
learn	O
the	O
correct	O
input-output	O
mapping	O
but	O
it	O
would	O
make	O
sense	O
to	O
try	O
to	O
make	O
use	O
of	O
these	O
known	O
invariances	B
to	O
reduce	O
the	O
amount	O
of	O
training	O
data	O
needed	O
there	O
are	O
at	O
least	O
three	O
ways	O
in	O
which	O
this	O
prior	O
knowledge	O
has	O
been	O
used	O
as	O
described	O
below	O
the	O
first	O
approach	O
is	O
to	O
generate	O
synthetic	O
training	O
examples	O
by	O
applying	O
valid	O
transformations	O
to	O
the	O
examples	O
we	O
already	O
have	O
this	O
is	O
simple	O
but	O
it	O
does	O
have	O
the	O
disadvantage	O
of	O
creating	O
a	O
larger	O
training	O
set	B
as	O
kernel-machine	O
training	O
algorithms	O
typically	O
scale	O
super-linearly	O
with	O
n	O
this	O
can	O
be	O
problematic	O
a	O
second	O
approach	O
is	O
to	O
make	O
the	O
predictor	O
invariant	O
to	O
small	O
transformations	O
of	O
each	O
training	O
case	O
this	O
method	O
was	O
first	O
developed	O
by	O
simard	O
et	O
al	O
for	O
neural	O
networks	O
under	O
the	O
name	O
of	O
tangent	O
prop	O
for	O
a	O
single	O
training	O
image	O
we	O
consider	O
the	O
manifold	O
of	O
images	O
that	O
are	O
generated	O
as	O
various	O
transformations	O
are	O
applied	O
to	O
it	O
this	O
manifold	O
will	O
have	O
a	O
complex	O
structure	O
but	O
locally	O
we	O
can	O
approximate	O
it	O
by	O
a	O
tangent	O
space	O
the	O
idea	O
in	O
tangent	O
prop	O
is	O
that	O
the	O
output	O
should	O
be	O
invariant	O
to	O
perturbations	O
of	O
the	O
training	O
example	O
in	O
this	O
tangent	O
space	O
for	O
neural	O
networks	O
it	O
is	O
quite	O
straightforward	O
to	O
modify	O
the	O
training	O
objective	O
function	B
to	O
penalize	O
deviations	O
from	O
this	O
invariance	O
see	O
simard	O
et	O
al	O
for	O
details	O
section	O
in	O
sch	O
olkopf	O
and	O
smola	O
describes	O
some	O
ways	O
in	O
which	O
these	O
ideas	O
can	O
be	O
extended	O
to	O
kernel	B
machines	O
the	O
third	O
approach	O
to	O
dealing	O
with	O
invariances	B
is	O
to	O
develop	O
a	O
representation	O
of	O
the	O
input	O
which	O
is	O
invariant	O
to	O
some	O
or	O
all	O
of	O
the	O
transformations	O
for	O
example	O
binary	B
images	O
of	O
handwritten	O
digits	O
are	O
sometimes	O
skeletonized	O
to	O
remove	O
the	O
effect	O
of	O
line	O
thickness	O
if	O
an	O
invariant	O
representation	O
can	O
be	O
achieved	O
for	O
all	O
transformations	O
it	O
is	O
the	O
most	O
desirable	O
but	O
it	O
can	O
be	O
difficult	O
or	O
perhaps	O
impossible	O
to	O
achieve	O
for	O
example	O
if	O
a	O
given	O
training	O
pattern	O
can	O
belong	O
to	O
more	O
than	O
one	O
class	O
an	O
ambiguous	O
handwritten	O
digit	O
then	O
it	O
is	O
clearly	O
not	O
possible	O
to	O
find	O
a	O
new	O
representation	O
which	O
is	O
invariant	O
to	O
transformations	O
yet	O
leaves	O
the	O
classes	O
distinguishable	O
digit	O
recognition	O
problem	O
is	O
only	O
invariant	O
to	O
small	O
rotations	O
we	O
must	O
avoid	O
turning	O
a	O
into	O
a	O
changing	O
the	O
thickness	O
of	O
the	O
pen	O
we	O
write	O
with	O
within	O
reasonable	O
bounds	O
does	O
not	O
change	O
the	O
digit	O
we	O
write	O
synthetic	O
training	O
examples	O
tangent	O
prop	O
invariant	O
representation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
further	O
issues	O
and	O
conclusions	O
latent	O
variable	O
models	O
gtm	O
gplvm	O
our	O
main	O
focus	O
in	O
this	O
book	O
has	O
been	O
on	O
supervised	B
learning	B
however	O
gps	O
have	O
also	O
been	O
used	O
as	O
components	O
for	O
models	O
carrying	O
out	O
non-linear	O
dimensionality	O
reduction	O
a	O
form	O
of	O
unsupervised	O
learning	B
the	O
key	O
idea	O
is	O
that	O
data	O
which	O
is	O
apparently	O
high-dimensional	O
a	O
pixelated	O
image	O
may	O
really	O
lie	O
on	O
a	O
low-dimensional	O
non-linear	O
manifold	O
which	O
we	O
wish	O
to	O
model	O
let	O
z	O
rl	O
be	O
a	O
latent	O
hidden	O
variable	O
and	O
let	O
x	O
rd	O
be	O
a	O
visible	O
variable	O
we	O
suppose	O
that	O
our	O
visible	O
data	O
is	O
generated	O
by	O
picking	O
a	O
point	O
in	O
z-space	O
and	O
mapping	O
this	O
point	O
into	O
the	O
data	O
space	O
through	O
a	O
non	O
linear	B
mapping	O
and	O
then	O
optionally	O
adding	O
noise	O
thus	O
px	O
pxzpzdz	O
if	O
the	O
mapping	O
from	O
z	O
to	O
x	O
is	O
linear	B
and	O
z	O
has	O
a	O
gaussian	B
distribution	I
then	O
this	O
is	O
the	O
factor	B
analysis	I
model	O
and	O
the	O
mean	O
and	O
covariance	B
of	O
the	O
gaussian	O
in	O
x-space	O
can	O
easily	O
be	O
determined	O
however	O
if	O
the	O
mapping	O
is	O
non-linear	O
then	O
the	O
integral	O
cannot	O
be	O
computed	O
exactly	O
in	O
the	O
generative	B
topographic	I
mapping	I
model	O
et	O
al	O
the	O
integral	O
was	O
approximated	O
using	O
a	O
grid	O
of	O
points	O
in	O
z-space	O
in	O
the	O
original	O
gtm	O
paper	O
the	O
non-linear	O
mapping	O
was	O
taken	O
to	O
be	O
a	O
linear	B
combination	O
of	O
non-linear	O
basis	O
functions	O
but	O
in	O
bishop	O
et	O
al	O
this	O
was	O
replaced	O
by	O
a	O
gaussian	B
process	I
mapping	O
between	O
the	O
latent	O
and	O
visible	O
spaces	O
more	O
recently	O
lawrence	O
has	O
introduced	O
a	O
rather	O
different	O
model	O
known	O
as	O
the	O
gaussian	B
process	I
latent	B
variable	I
model	I
instead	O
of	O
having	O
a	O
prior	O
thus	O
a	O
posterior	O
distribution	O
over	O
the	O
latent	O
space	O
we	O
consider	O
that	O
each	O
data	O
point	O
xi	O
is	O
derived	O
from	O
a	O
corresponding	O
latent	O
point	O
zi	O
through	O
a	O
non-linear	O
mapping	O
added	O
noise	O
if	O
a	O
gaussian	B
process	I
is	O
used	O
for	O
this	O
non-linear	O
mapping	O
then	O
one	O
can	O
easily	O
write	O
down	O
the	O
joint	B
distribution	O
pxz	O
of	O
the	O
visible	O
variables	O
conditional	B
on	O
the	O
latent	O
variables	O
optimization	O
routines	O
can	O
then	O
be	O
used	O
to	O
find	O
the	O
locations	O
of	O
the	O
latent	O
points	O
that	O
optimize	O
pxz	O
this	O
has	O
some	O
similarities	O
to	O
the	O
work	O
on	O
regularized	O
principal	O
manifolds	O
olkopf	O
and	O
smola	O
ch	O
except	O
that	O
in	O
the	O
gplvm	O
one	O
integrates	O
out	O
the	O
latent-to-visible	O
mapping	O
rather	O
than	O
optimizing	O
it	O
conclusions	O
and	O
future	O
directions	O
in	O
this	O
section	O
we	O
briefly	O
wrap	O
up	O
some	O
of	O
the	O
threads	O
we	O
have	O
developed	O
throughout	O
the	O
book	O
and	O
discuss	O
possible	O
future	O
directions	O
of	O
work	O
on	O
gaussian	O
processes	O
in	O
chapter	O
we	O
saw	O
how	O
gaussian	B
process	I
regression	I
is	O
a	O
natural	O
extension	O
of	O
bayesian	O
linear	B
regression	I
to	O
a	O
more	O
flexible	O
class	O
of	O
models	O
for	O
gaussian	O
noise	O
the	O
model	O
can	O
be	O
treated	O
analytically	O
and	O
is	O
simple	O
enough	O
that	O
the	O
gp	O
model	O
could	O
be	O
often	O
considered	O
as	O
a	O
replacement	O
for	O
the	O
traditional	O
linear	B
analogue	O
we	O
have	O
also	O
seen	O
that	O
historically	O
there	O
have	O
been	O
numerous	O
ideas	O
along	O
the	O
lines	O
of	O
gaussian	B
process	I
models	O
although	O
they	O
have	O
only	O
gained	O
a	O
sporadic	O
following	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
conclusions	O
and	O
future	O
directions	O
one	O
may	O
indeed	O
speculate	O
why	O
are	O
gps	O
not	O
currently	O
used	O
more	O
widely	O
in	O
applications	O
we	O
see	O
three	O
major	O
reasons	O
firstly	O
that	O
the	O
application	O
of	O
gaussian	O
processes	O
requires	O
the	O
handling	O
of	O
large	O
matrices	O
while	O
these	O
kinds	O
of	O
computations	O
were	O
tedious	O
years	O
ago	O
and	O
impossible	O
further	O
in	O
the	O
past	O
even	O
na	O
ve	O
implementations	O
suffice	O
for	O
moderate	O
sized	O
problems	O
on	O
an	O
anno	O
pc	O
another	O
possibility	O
is	O
that	O
most	O
of	O
the	O
historical	O
work	O
on	O
gps	O
was	O
done	O
using	O
fixed	O
covariance	B
functions	O
with	O
very	O
little	O
guide	O
as	O
to	O
how	O
to	O
choose	O
such	O
functions	O
the	O
choice	O
was	O
to	O
some	O
degree	O
arbitrary	O
and	O
the	O
idea	O
that	O
one	O
should	O
be	O
able	O
to	O
infer	O
the	O
structure	O
or	O
parameters	O
of	O
the	O
covariance	B
function	B
as	O
we	O
discuss	O
in	O
chapter	O
is	O
not	O
so	O
well	O
known	O
this	O
is	O
probably	O
a	O
very	O
important	O
step	O
in	O
turning	O
gps	O
into	O
an	O
interesting	O
method	O
for	O
practitioners	O
the	O
viewpoint	O
of	O
placing	O
gaussian	B
process	I
priors	O
over	O
functions	O
is	O
a	O
bayesian	O
one	O
although	O
the	O
adoption	O
of	O
bayesian	O
methods	O
in	O
the	O
machine	O
learning	B
community	O
is	O
quite	O
widespread	O
these	O
ideas	O
have	O
not	O
always	O
been	O
appreciated	O
more	O
widely	O
in	O
the	O
statistics	O
community	O
although	O
modern	O
computers	O
allow	O
simple	O
implementations	O
for	O
up	O
to	O
a	O
few	O
thousand	O
training	O
cases	O
the	O
computational	O
constraints	O
are	O
still	O
a	O
significant	O
limitation	O
for	O
applications	O
where	O
the	O
datasets	O
are	O
significantly	O
larger	O
than	O
this	O
in	O
chapter	O
we	O
have	O
given	O
an	O
overview	O
of	O
some	O
of	O
the	O
recent	O
work	O
on	O
approximations	O
for	O
large	O
datasets	O
although	O
there	O
are	O
many	O
methods	O
and	O
a	O
lot	O
of	O
work	O
is	O
currently	O
being	O
undertaken	O
both	O
the	O
theoretical	O
and	O
practical	O
aspects	O
of	O
these	O
approximations	O
need	O
to	O
be	O
understood	O
better	O
in	O
order	O
to	O
be	O
a	O
useful	O
tool	O
to	O
the	O
practitioner	O
the	O
computations	O
required	O
for	O
the	O
gaussian	B
process	I
classification	B
models	O
developed	O
in	O
chapter	O
are	O
a	O
lot	O
more	O
involved	O
than	O
for	O
regression	O
although	O
the	O
theoretical	O
foundations	O
of	O
gaussian	B
process	I
classification	B
are	O
well	O
developed	O
it	O
is	O
not	O
yet	O
clear	O
under	O
which	O
circumstances	O
one	O
would	O
expect	O
the	O
extra	O
work	O
and	O
approximations	O
associated	O
with	O
treating	O
a	O
full	O
probabilistic	B
latent	B
variable	I
model	I
to	O
pay	O
off	O
the	O
answer	O
may	O
depend	O
heavily	O
on	O
the	O
ability	O
to	O
learn	O
meaningful	O
covariance	B
functions	O
the	O
incorporation	O
of	O
prior	O
knowledge	O
through	O
the	O
choice	O
and	O
parameterization	O
of	O
the	O
covariance	B
function	B
is	O
another	O
prime	O
target	O
for	O
future	O
work	O
on	O
gps	O
in	O
chapter	O
we	O
have	O
presented	O
many	O
families	O
of	O
covariance	B
functions	O
with	O
widely	O
differing	O
properties	O
and	O
in	O
chapter	O
we	O
presented	O
principled	O
methods	O
for	O
choosing	O
between	O
and	O
adapting	O
covariance	B
functions	O
particularly	O
in	O
the	O
machine	O
learning	B
community	O
there	O
has	O
been	O
a	O
tendency	O
to	O
view	O
gaussian	O
processes	O
as	O
a	O
black	O
box	O
what	O
exactly	O
goes	O
on	O
in	O
the	O
box	O
is	O
less	O
important	O
as	O
long	O
as	O
it	O
gives	O
good	O
predictions	O
to	O
our	O
mind	O
we	O
could	O
perhaps	O
learn	O
something	O
from	O
the	O
statisticians	O
here	O
and	O
ask	O
how	O
and	O
why	O
the	O
models	O
work	O
in	O
fact	O
the	O
hierarchical	O
formulation	O
of	O
the	O
covariance	B
functions	O
with	O
hyperparameters	B
the	O
testing	O
of	O
different	O
hypotheses	O
and	O
the	O
adaptation	O
of	O
hyperparameters	B
gives	O
an	O
excellent	O
opportunity	O
to	O
understand	O
more	O
about	O
the	O
data	O
we	O
have	O
attempted	O
to	O
illustrate	O
this	O
line	O
of	O
thinking	O
with	O
the	O
carbon	O
dioxide	O
prediction	O
example	O
developed	O
at	O
some	O
length	O
in	O
section	O
although	O
this	O
problem	O
is	O
comparatively	O
simple	O
and	O
very	O
easy	O
to	O
get	O
an	O
intuitive	O
understanding	O
of	O
the	O
principles	O
of	O
trying	O
out	O
different	O
components	O
in	O
the	O
covariance	B
structure	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
further	O
issues	O
and	O
conclusions	O
and	O
adapting	O
their	O
parameters	O
could	O
be	O
used	O
universally	O
indeed	O
the	O
use	O
of	O
the	O
isotropic	O
squared	B
exponential	B
covariance	B
function	B
in	O
the	O
digit	O
classification	B
examples	O
in	O
chapter	O
is	O
not	O
really	O
a	O
choice	O
which	O
one	O
would	O
expect	O
to	O
provide	O
very	O
much	O
insight	O
to	O
the	O
classification	B
problem	O
although	O
some	O
of	O
the	O
results	O
presented	O
are	O
as	O
good	O
as	O
other	O
current	O
methods	O
in	O
the	O
literature	O
one	O
could	O
indeed	O
argue	O
that	O
the	O
use	O
of	O
the	O
squared	B
exponential	B
covariance	B
function	B
for	O
this	O
task	O
makes	O
little	O
sense	O
and	O
the	O
low	O
error	O
rate	O
is	O
possibly	O
due	O
to	O
the	O
inherently	O
low	O
difficulty	O
of	O
the	O
task	O
there	O
is	O
a	O
need	O
to	O
develop	O
more	O
sensible	O
covariance	B
functions	O
which	O
allow	O
for	O
the	O
incorporation	O
of	O
prior	O
knowledge	O
and	O
help	O
us	O
to	O
gain	O
real	O
insight	O
into	O
the	O
data	O
going	O
beyond	O
a	O
simple	O
vectorial	O
representation	O
of	O
the	O
input	O
data	O
to	O
take	O
into	O
account	O
structure	O
in	O
the	O
input	O
domain	O
is	O
also	O
a	O
theme	O
which	O
we	O
see	O
as	O
very	O
important	O
examples	O
of	O
this	O
include	O
the	O
invariances	B
described	O
in	O
section	O
arising	O
from	O
the	O
structure	O
of	O
images	O
and	O
the	O
kernels	O
described	O
in	O
section	O
which	O
encode	O
structured	O
objects	O
such	O
as	O
strings	O
and	O
trees	O
as	O
this	O
brief	O
discussion	O
shows	O
we	O
see	O
the	O
current	O
level	O
of	O
development	O
of	O
gaussian	B
process	I
models	O
more	O
as	O
a	O
rich	O
principled	O
framework	O
for	O
supervised	B
learning	B
than	O
a	O
fully-developed	O
set	B
of	O
tools	O
for	O
applications	O
we	O
find	O
the	O
gaussian	B
process	I
framework	O
very	O
appealing	O
and	O
are	O
confident	O
that	O
the	O
near	O
future	O
will	O
show	O
many	O
important	O
developments	O
both	O
in	O
theory	O
methodology	O
and	O
practice	O
we	O
look	O
forward	O
very	O
much	O
to	O
following	O
these	O
developments	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
appendix	O
a	O
mathematical	O
background	O
joint	B
marginal	B
and	O
conditional	B
probability	O
let	O
the	O
n	O
or	O
continuous	O
random	O
variables	O
yn	O
have	O
a	O
joint	B
probability	O
yn	O
or	O
py	O
for	O
technically	O
one	O
ought	O
to	O
distinguish	O
between	O
probabilities	O
discrete	O
variables	O
and	O
probability	O
densities	O
for	O
continuous	O
variables	O
throughout	O
the	O
book	O
we	O
commonly	O
use	O
the	O
term	O
probability	O
to	O
refer	O
to	O
both	O
let	O
us	O
partition	O
the	O
variables	O
in	O
y	O
into	O
two	O
groups	O
ya	O
and	O
yb	O
where	O
a	O
and	O
b	O
are	O
two	O
disjoint	O
sets	O
whose	O
union	O
is	O
the	O
set	B
n	O
so	O
that	O
py	O
pya	O
yb	O
each	O
group	O
may	O
contain	O
one	O
or	O
more	O
variables	O
the	O
marginal	B
probability	O
of	O
ya	O
is	O
given	O
by	O
z	O
pya	O
pya	O
yb	O
dyb	O
the	O
integral	O
is	O
replaced	O
by	O
a	O
sum	O
if	O
the	O
variables	O
are	O
discrete	O
valued	O
notice	O
that	O
if	O
the	O
set	B
a	O
contains	O
more	O
than	O
one	O
variable	O
then	O
the	O
marginal	B
probability	O
is	O
itself	O
a	O
joint	B
probability	O
whether	O
it	O
is	O
referred	O
to	O
as	O
one	O
or	O
the	O
other	O
depends	O
on	O
the	O
context	O
if	O
the	O
joint	B
distribution	O
is	O
equal	O
to	O
the	O
product	O
of	O
the	O
marginals	O
then	O
the	O
variables	O
are	O
said	O
to	O
be	O
independent	O
otherwise	O
they	O
are	O
dependent	O
the	O
conditional	B
probability	O
function	B
is	O
defined	O
as	O
pyayb	O
pya	O
yb	O
pyb	O
defined	O
for	O
pyb	O
as	O
it	O
is	O
not	O
meaningful	O
to	O
condition	O
on	O
an	O
impossible	O
event	O
if	O
ya	O
and	O
yb	O
are	O
independent	O
then	O
the	O
marginal	B
pya	O
and	O
the	O
conditional	B
pyayb	O
are	O
equal	O
can	O
deal	O
with	O
more	O
general	O
cases	O
where	O
the	O
density	O
function	B
does	O
not	O
exist	O
by	O
using	O
the	O
distribution	O
function	B
joint	B
probability	O
marginal	B
probability	O
independence	O
conditional	B
probability	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bayes	O
rule	O
gaussian	O
definition	O
conditioning	O
and	O
marginalizing	O
products	O
mathematical	O
background	O
theorem	O
using	O
the	O
definitions	O
of	O
both	O
pyayb	O
and	O
pybya	O
we	O
obtain	O
bayes	O
pyayb	O
pyapybya	O
pyb	O
since	O
conditional	B
distributions	O
are	O
themselves	O
probabilities	O
one	O
can	O
use	O
all	O
of	O
the	O
above	O
also	O
when	O
further	O
conditioning	O
on	O
other	O
variables	O
for	O
example	O
in	O
supervised	B
learning	B
one	O
often	O
conditions	O
on	O
the	O
inputs	O
throughout	O
which	O
would	O
lead	O
e	O
g	O
to	O
a	O
version	O
of	O
bayes	O
rule	O
with	O
additional	O
conditioning	O
on	O
x	O
in	O
all	O
four	O
probabilities	O
in	O
eq	O
see	O
eq	O
for	O
an	O
example	O
of	O
this	O
gaussian	O
identities	O
the	O
multivariate	O
gaussian	O
normal	O
distribution	O
has	O
a	O
joint	B
probability	O
density	O
given	O
by	O
pxm	O
m	O
where	O
m	O
is	O
the	O
mean	O
vector	O
length	O
d	O
and	O
is	O
the	O
positive	B
definite	I
covariance	B
matrix	I
size	O
d	O
d	O
as	O
a	O
shorthand	O
we	O
write	O
x	O
n	O
let	O
x	O
and	O
y	O
be	O
jointly	O
gaussian	O
random	O
vectors	O
x	O
y	O
a	O
x	O
y	O
n	O
c	O
c	O
b	O
n	O
x	O
y	O
a	O
c	O
c	O
b	O
then	O
the	O
marginal	B
distribution	O
of	O
x	O
and	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
y	O
are	O
x	O
n	O
x	O
a	O
and	O
xy	O
x	O
cb	O
y	O
a	O
cb	O
or	O
xy	O
x	O
a	O
cy	O
y	O
a	O
see	O
e	O
g	O
von	O
mises	O
sec	O
and	O
eqs	O
the	O
product	O
of	O
two	O
gaussians	O
gives	O
another	O
gaussian	O
n	O
an	O
b	O
z	O
c	O
where	O
c	O
ca	O
b	O
and	O
c	O
b	O
notice	O
that	O
the	O
resulting	O
gaussian	O
has	O
a	O
precision	O
variance	O
equal	O
to	O
the	O
sum	O
of	O
the	O
precisions	O
and	O
a	O
mean	O
equal	O
to	O
the	O
convex	O
sum	O
of	O
the	O
means	O
weighted	O
by	O
the	O
precisions	O
the	O
normalizing	O
constant	O
looks	O
itself	O
like	O
a	O
gaussian	O
a	O
or	O
b	O
z	O
b	O
ba	O
b	O
to	O
prove	O
eq	O
simply	O
write	O
out	O
the	O
expressions	O
by	O
introducing	O
eq	O
and	O
eq	O
into	O
eq	O
and	O
expand	O
the	O
terms	O
inside	O
the	O
exp	O
to	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
matrix	O
identities	O
generating	O
multivariate	O
gaussian	O
samples	O
verify	O
equality	O
hint	O
it	O
may	O
be	O
helpful	O
to	O
expand	O
c	O
using	O
the	O
matrix	O
inversion	B
lemma	I
eq	O
c	O
a	O
aab	O
b	O
bab	O
to	O
generate	O
samples	O
x	O
n	O
k	O
with	O
arbitrary	O
mean	O
m	O
and	O
covariance	B
matrix	I
k	O
using	O
a	O
scalar	O
gaussian	O
generator	O
is	O
readily	O
available	O
in	O
many	O
programming	O
environments	O
we	O
proceed	O
as	O
follows	O
first	O
compute	O
the	O
cholesky	B
decomposition	I
known	O
as	O
the	O
matrix	O
square	O
root	O
l	O
of	O
the	O
positive	B
definite	I
symmetric	O
covariance	B
matrix	I
k	O
ll	O
where	O
l	O
is	O
a	O
lower	O
triangular	O
matrix	O
see	O
section	O
then	O
generate	O
u	O
n	O
i	O
by	O
multiple	O
separate	O
calls	O
to	O
the	O
scalar	O
gaussian	O
generator	O
compute	O
x	O
m	O
lu	O
which	O
has	O
the	O
desired	O
distribution	O
with	O
mean	O
m	O
and	O
covariance	B
leuul	O
ll	O
k	O
the	O
independence	O
of	O
the	O
elements	O
of	O
u	O
in	O
practice	O
it	O
may	O
be	O
necessary	O
to	O
add	O
a	O
small	O
multiple	O
of	O
the	O
identity	O
matrix	O
to	O
the	O
covariance	B
matrix	I
for	O
numerical	O
reasons	O
this	O
is	O
because	O
the	O
eigenvalues	O
of	O
the	O
matrix	O
k	O
can	O
decay	O
very	O
rapidly	O
section	O
for	O
a	O
closely	O
related	O
analytical	O
result	O
and	O
without	O
this	O
stabilization	O
the	O
cholesky	B
decomposition	I
fails	O
the	O
effect	O
on	O
the	O
generated	O
samples	O
is	O
to	O
add	O
additional	O
independent	O
noise	O
of	O
variance	O
from	O
the	O
context	O
can	O
usually	O
be	O
chosen	O
to	O
have	O
inconsequential	O
effects	O
on	O
the	O
samples	O
while	O
ensuring	O
numerical	O
stability	O
matrix	O
identities	O
the	O
matrix	O
inversion	B
lemma	I
also	O
known	O
as	O
the	O
woodbury	O
sherman	O
morri-	O
matrix	O
inversion	B
lemma	I
son	O
formula	O
e	O
g	O
press	O
et	O
al	O
p	O
states	O
that	O
u	O
w	O
v	O
z	O
z	O
v	O
assuming	O
the	O
relevant	O
inverses	O
all	O
exist	O
here	O
z	O
is	O
n	O
n	O
w	O
is	O
m	O
m	O
and	O
u	O
and	O
v	O
are	O
both	O
of	O
size	O
n	O
m	O
consequently	O
if	O
z	O
is	O
known	O
and	O
a	O
low	O
rank	O
m	O
n	O
perturbation	O
is	O
made	O
to	O
z	O
as	O
in	O
left	O
hand	O
side	O
of	O
eq	O
considerable	O
speedup	O
can	O
be	O
achieved	O
a	O
similar	O
equation	O
exists	O
for	O
determinants	O
u	O
w	O
v	O
v	O
let	O
the	O
invertible	O
n	O
n	O
matrix	O
a	O
and	O
its	O
inverse	O
a	O
be	O
partitioned	O
into	O
p	O
q	O
r	O
s	O
p	O
q	O
r	O
s	O
a	O
a	O
where	O
p	O
and	O
p	O
are	O
matrices	O
and	O
s	O
and	O
s	O
are	O
matrices	O
with	O
n	O
the	O
submatrices	O
of	O
a	O
are	O
given	O
in	O
press	O
et	O
al	O
p	O
as	O
p	O
p	O
p	O
rp	O
q	O
p	O
r	O
m	O
rp	O
s	O
m	O
where	O
m	O
rp	O
determinants	O
inversion	O
of	O
a	O
partitioned	O
matrix	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
mathematical	O
background	O
or	O
equivalently	O
p	O
n	O
q	O
n	O
qs	O
r	O
s	O
s	O
s	O
s	O
qs	O
where	O
n	O
qs	O
matrix	O
derivatives	O
derivatives	O
of	O
the	O
elements	O
of	O
an	O
inverse	O
matrix	O
k	O
k	O
k	O
k	O
where	O
k	O
positive	B
definite	I
symmetric	O
matrix	O
we	O
have	O
is	O
a	O
matrix	O
of	O
elementwise	O
derivatives	O
for	O
the	O
log	O
determinant	O
of	O
a	O
log	O
k	O
derivative	O
of	O
inverse	O
derivative	O
of	O
log	O
determinant	O
matrix	O
norms	O
the	O
frobenius	B
norm	O
kakf	O
of	O
a	O
matrix	O
a	O
is	O
defined	O
as	O
f	O
traa	O
and	O
van	O
loan	O
p	O
cholesky	B
decomposition	I
the	O
cholesky	B
decomposition	I
of	O
a	O
symmetric	O
positive	B
definite	I
matrix	I
a	O
decomposes	O
a	O
into	O
a	O
product	O
of	O
a	O
lower	O
triangular	O
matrix	O
l	O
and	O
its	O
transpose	O
ll	O
a	O
where	O
l	O
is	O
called	O
the	O
cholesky	O
factor	O
the	O
cholesky	B
decomposition	I
is	O
useful	O
for	O
solving	O
linear	B
systems	O
with	O
symmetric	O
positive	B
definite	I
coefficient	O
matrix	O
a	O
to	O
solve	O
ax	O
b	O
for	O
x	O
first	O
solve	O
the	O
triangular	O
system	O
ly	O
b	O
by	O
forward	O
substitution	O
and	O
then	O
the	O
triangular	O
system	O
lx	O
y	O
by	O
back	O
substitution	O
using	O
the	O
backslash	O
operator	B
we	O
write	O
the	O
solution	O
as	O
x	O
llb	O
where	O
the	O
notation	O
ab	O
is	O
the	O
vector	O
x	O
which	O
solves	O
ax	O
b	O
both	O
the	O
forward	O
and	O
backward	O
substitution	O
steps	O
require	O
operations	O
when	O
a	O
is	O
of	O
size	O
n	O
n	O
the	O
computation	O
of	O
the	O
cholesky	O
factor	O
l	O
is	O
considered	O
numerically	O
extremely	O
stable	O
and	O
takes	O
time	O
so	O
it	O
is	O
the	O
method	O
of	O
choice	O
when	O
it	O
can	O
be	O
applied	O
solving	O
linear	B
systems	O
computational	O
cost	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
entropy	B
and	O
kullback-leibler	B
divergence	I
ny	O
nx	O
note	O
also	O
that	O
the	O
determinant	O
of	O
a	O
positive	B
definite	I
symmetric	O
matrix	O
can	O
be	O
calculated	O
efficiently	O
by	O
ii	O
or	O
log	O
log	O
lii	O
where	O
l	O
is	O
the	O
cholesky	O
factor	O
from	O
a	O
entropy	B
and	O
kullback-leibler	B
divergence	I
the	O
entropy	B
hpx	O
of	O
a	O
distribution	O
px	O
is	O
a	O
non-negative	O
measure	B
of	O
the	O
amount	O
of	O
uncertainty	O
in	O
the	O
distribution	O
and	O
is	O
defined	O
as	O
hpx	O
px	O
log	O
px	O
dx	O
z	O
the	O
integral	O
is	O
substituted	O
by	O
a	O
sum	O
for	O
discrete	O
variables	O
entropy	B
is	O
measured	O
in	O
bits	B
if	O
the	O
log	O
is	O
to	O
the	O
base	O
and	O
in	O
nats	B
in	O
the	O
case	O
of	O
the	O
natural	O
log	O
the	O
entropy	B
of	O
a	O
gaussian	O
in	O
d	O
dimensions	O
measured	O
in	O
nats	B
is	O
hn	O
the	O
kullback-leibler	B
divergence	I
relative	O
entropy	B
klpq	O
be	O
e	O
tween	O
two	O
distributions	O
px	O
and	O
qx	O
is	O
defined	O
as	O
px	O
log	O
px	O
klpq	O
qx	O
dx	O
log	O
d	O
z	O
it	O
is	O
easy	O
to	O
show	O
that	O
klpq	O
with	O
equality	O
if	O
p	O
q	O
everywhere	O
for	O
the	O
case	O
of	O
two	O
bernoulli	O
random	O
variables	O
p	O
and	O
q	O
this	O
reduces	O
to	O
klberpq	O
p	O
log	O
p	O
q	O
p	O
log	O
p	O
q	O
where	O
we	O
use	O
p	O
and	O
q	O
both	O
as	O
the	O
name	O
and	O
the	O
parameter	O
of	O
the	O
bernoulli	O
distributions	O
for	O
two	O
gaussian	O
distributions	O
n	O
and	O
n	O
we	O
have	O
sec	O
consider	O
a	O
general	O
distribution	O
px	O
on	O
rd	O
and	O
a	O
gaussian	B
distribution	I
qx	O
n	O
then	O
klpq	O
dx	O
z	O
log	O
px	O
log	O
px	O
dx	O
log	O
tr	O
z	O
log	O
d	O
determinant	O
entropy	B
divergence	O
of	O
bernoulli	O
random	O
variables	O
divergence	O
of	O
gaussians	O
minimizing	O
klpq	O
divergence	O
leads	O
to	O
moment	O
matching	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
mathematical	O
background	O
equation	O
can	O
be	O
minimized	O
w	O
r	O
t	O
and	O
by	O
differentiating	O
w	O
r	O
t	O
these	O
parameters	O
and	O
setting	O
the	O
resulting	O
expressions	O
to	O
zero	O
the	O
optimal	B
q	O
is	O
the	O
one	O
that	O
matches	O
the	O
first	O
and	O
second	O
moments	O
of	O
p	O
the	O
kl	O
divergence	O
can	O
be	O
viewed	O
as	O
the	O
extra	O
number	O
of	O
nats	B
needed	O
on	O
average	O
to	O
code	O
data	O
generated	O
from	O
a	O
source	O
px	O
under	O
the	O
distribution	O
qx	O
as	O
opposed	O
to	O
px	O
limits	O
the	O
limit	O
of	O
a	O
rational	B
quadratic	I
is	O
a	O
squared	B
exponential	B
lim	O
measure	B
and	O
integration	O
here	O
we	O
sketch	O
some	O
definitions	O
concerning	O
measure	B
and	O
integration	O
fuller	O
treatments	O
can	O
be	O
found	O
e	O
g	O
in	O
doob	O
and	O
bartle	O
let	O
be	O
the	O
set	B
of	O
all	O
possible	O
outcomes	O
of	O
an	O
experiment	O
for	O
example	O
for	O
a	O
d-dimensional	O
real-valued	O
variable	O
rd	O
let	O
f	O
be	O
a	O
of	O
subsets	O
of	O
which	O
contains	O
all	O
the	O
events	O
in	O
whose	O
occurrences	O
we	O
may	O
be	O
then	O
is	O
a	O
countably	O
additive	O
measure	B
if	O
it	O
is	O
real	O
and	O
non-negative	O
and	O
for	O
all	O
mutually	O
disjoint	O
sets	O
f	O
we	O
have	O
ai	O
x	O
finite	O
measure	B
probability	O
measure	B
lebesgue	O
measure	B
if	O
then	O
is	O
called	O
a	O
finite	O
measure	B
and	O
if	O
it	O
is	O
called	O
a	O
probability	O
measure	B
the	O
lebesgue	O
measure	B
defines	O
a	O
uniform	O
measure	B
over	O
subsets	O
of	O
euclidean	O
space	O
here	O
an	O
appropriate	O
is	O
the	O
borel	O
bd	O
where	O
b	O
is	O
the	O
generated	O
by	O
the	O
open	O
subsets	O
of	O
r	O
for	O
example	O
on	O
the	O
line	O
r	O
the	O
lebesgue	O
measure	B
of	O
the	O
interval	O
b	O
is	O
b	O
a	O
we	O
now	O
restrict	O
to	O
be	O
rd	O
and	O
wish	O
to	O
give	O
meaning	O
to	O
integration	O
of	O
a	O
function	B
f	O
rd	O
r	O
with	O
respect	O
to	O
a	O
measure	B
z	O
fx	O
d	O
we	O
assume	O
that	O
f	O
is	O
measurable	O
i	O
e	O
that	O
for	O
any	O
borel-measurable	O
set	B
a	O
r	O
f	O
bd	O
there	O
are	O
two	O
cases	O
that	O
will	O
interest	O
us	O
when	O
is	O
the	O
lebesgue	O
measure	B
and	O
when	O
is	O
a	O
probability	O
measure	B
for	O
the	O
first	O
case	O
expression	O
reduces	O
to	O
the	O
usual	O
integral	O
notationr	O
fxdx	O
restriction	O
to	O
a	O
of	O
subsets	O
is	O
important	O
technically	O
to	O
avoid	O
paradoxes	O
such	O
as	O
the	O
banach-tarski	O
paradox	O
informally	O
we	O
can	O
think	O
of	O
the	O
as	O
restricting	O
consideration	O
to	O
reasonable	O
subsets	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
fourier	O
transforms	O
for	O
a	O
probability	O
measure	B
on	O
x	O
the	O
non-negative	O
function	B
px	O
is	O
called	O
the	O
density	O
of	O
the	O
measure	B
if	O
for	O
all	O
a	O
bd	O
we	O
have	O
px	O
dx	O
if	O
such	O
a	O
density	O
exists	O
it	O
is	O
uniquely	O
determined	O
almost	O
everywhere	O
i	O
e	O
except	O
for	O
sets	O
with	O
measure	B
zero	O
not	O
all	O
probability	O
measures	O
have	O
densities	O
only	O
distributions	O
that	O
assign	O
zero	O
probability	O
to	O
individual	O
points	O
in	O
x-space	O
can	O
have	O
if	O
px	O
exists	O
then	O
we	O
have	O
z	O
fx	O
d	O
fxpx	O
dx	O
z	O
a	O
z	O
if	O
does	O
not	O
have	O
a	O
density	O
expression	O
still	O
has	O
meaning	O
by	O
the	O
standard	O
construction	O
of	O
the	O
lebesgue	O
integral	O
for	O
rd	O
the	O
probability	O
measure	B
can	O
be	O
related	O
to	O
the	O
distribution	O
function	B
f	O
rd	O
which	O
is	O
defined	O
as	O
f	O
xd	O
zd	O
the	O
distribution	O
function	B
is	O
more	O
general	O
than	O
the	O
density	O
as	O
it	O
is	O
always	O
defined	O
for	O
a	O
given	O
probability	O
measure	B
a	O
simple	O
example	O
of	O
a	O
random	O
variable	O
which	O
has	O
a	O
distribution	O
function	B
but	O
no	O
density	O
is	O
obtained	O
by	O
the	O
following	O
construction	O
a	O
coin	O
is	O
tossed	O
and	O
with	O
probability	O
p	O
it	O
comes	O
up	O
heads	O
if	O
it	O
comes	O
up	O
heads	O
x	O
is	O
chosen	O
from	O
uniform	O
distribution	O
on	O
otherwise	O
probability	O
p	O
x	O
is	O
set	B
to	O
this	O
distribution	O
has	O
a	O
point	O
mass	O
atom	O
at	O
x	O
point	O
mass	O
example	O
lp	O
spaces	O
let	O
be	O
a	O
measure	B
on	O
an	O
input	O
set	B
x	O
for	O
some	O
function	B
f	O
x	O
r	O
and	O
p	O
we	O
define	O
kfklpx	O
d	O
if	O
the	O
integral	O
exists	O
for	O
p	O
we	O
define	O
kfkl	O
ess	O
sup	O
x	O
x	O
where	O
ess	O
sup	O
denotes	O
the	O
essential	O
supremum	O
i	O
e	O
the	O
smallest	O
number	O
that	O
upper	O
bounds	O
almost	O
everywhere	O
the	O
function	B
space	O
lpx	O
is	O
defined	O
for	O
any	O
p	O
in	O
p	O
as	O
the	O
space	O
of	O
functions	O
for	O
which	O
kfklpx	O
fourier	O
transforms	O
for	O
sufficiently	O
well-behaved	O
functions	O
on	O
rd	O
we	O
have	O
fx	O
is	O
x	O
ds	O
fs	O
fxe	O
is	O
x	O
dx	O
z	O
z	O
measure	B
has	O
a	O
density	O
if	O
and	O
only	O
if	O
it	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
lebesgue	O
measure	B
on	O
rd	O
i	O
e	O
every	O
set	B
that	O
has	O
lebesgue	O
measure	B
zero	O
also	O
has	O
zero	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
mathematical	O
background	O
where	O
fs	O
is	O
called	O
the	O
fourier	B
transform	I
of	O
fx	O
see	O
e	O
g	O
bracewell	O
we	O
refer	O
to	O
the	O
equation	O
on	O
the	O
left	O
as	O
the	O
synthesis	O
equation	O
and	O
the	O
equation	O
on	O
the	O
right	O
as	O
the	O
analysis	O
equation	O
there	O
are	O
other	O
conventions	O
for	O
fourier	O
transforms	O
particularly	O
those	O
involving	O
s	O
however	O
this	O
tends	O
to	O
destroy	O
symmetry	O
between	O
the	O
analysis	O
and	O
synthesis	O
equations	O
so	O
we	O
use	O
the	O
definitions	O
given	O
above	O
here	O
we	O
have	O
defined	O
fourier	O
transforms	O
for	O
fx	O
being	O
a	O
function	B
on	O
rd	O
for	O
related	O
transforms	O
for	O
periodic	B
functions	O
functions	O
defined	O
on	O
the	O
integer	O
lattice	O
and	O
on	O
the	O
regular	O
n-polygon	O
see	O
section	O
convexity	O
convex	O
sets	O
convex	O
function	B
below	O
we	O
state	O
some	O
definitions	O
and	O
properties	O
of	O
convex	O
sets	O
and	O
functions	O
taken	O
from	O
boyd	O
and	O
vandenberghe	O
a	O
set	B
c	O
is	O
convex	O
if	O
the	O
line	O
segment	O
between	O
any	O
two	O
points	O
in	O
c	O
lies	O
in	O
c	O
i	O
e	O
if	O
for	O
any	O
c	O
and	O
for	O
any	O
with	O
we	O
have	O
a	O
function	B
f	O
x	O
r	O
is	O
convex	O
if	O
its	O
domain	O
x	O
is	O
a	O
convex	O
set	B
and	O
if	O
for	O
all	O
x	O
and	O
with	O
we	O
have	O
c	O
f	O
where	O
x	O
is	O
a	O
improper	O
subset	O
of	O
rd	O
f	O
is	O
concave	O
if	O
f	O
is	O
convex	O
a	O
function	B
f	O
is	O
convex	O
if	O
and	O
only	O
if	O
its	O
domain	O
x	O
is	O
a	O
convex	O
set	B
and	O
its	O
hessian	O
is	O
positive	B
semidefinite	I
for	O
all	O
x	O
x	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
appendix	O
b	O
gaussian	O
markov	O
processes	O
a	O
discrete-time	O
autoregressive	B
process	I
of	O
order	O
p	O
can	O
be	O
written	O
as	O
ar	O
process	O
px	O
particularly	O
when	O
the	O
index	B
set	B
for	O
a	O
stochastic	O
process	O
is	O
one-dimensional	O
such	O
as	O
the	O
real	O
line	O
or	O
its	O
discretization	O
onto	O
the	O
integer	O
lattice	O
it	O
is	O
very	O
interesting	O
to	O
investigate	O
the	O
properties	O
of	O
gaussian	O
markov	O
processes	O
in	O
this	O
appendix	O
we	O
use	O
xt	O
to	O
define	O
a	O
stochastic	O
process	O
with	O
continuous	O
time	O
parameter	O
t	O
in	O
the	O
discrete	O
time	O
case	O
the	O
process	O
is	O
denoted	O
x	O
etc	O
we	O
assume	O
that	O
the	O
process	O
has	O
zero	O
mean	O
and	O
is	O
unless	O
otherwise	O
stated	O
stationary	O
xt	O
akxt	O
k	O
where	O
zt	O
n	O
and	O
all	O
zt	O
s	O
are	O
i	O
i	O
d	O
notice	O
the	O
order-p	O
markov	O
property	O
that	O
given	O
the	O
history	O
xt	O
xt	O
xt	O
depends	O
only	O
on	O
the	O
previous	O
p	O
x	O
s	O
this	O
relationship	O
can	O
be	O
conveniently	O
expressed	O
as	O
a	O
graphical	O
model	O
part	O
of	O
an	O
process	O
is	O
illustrated	O
in	O
figure	O
the	O
name	O
autoregressive	O
stems	O
from	O
the	O
fact	O
that	O
xt	O
is	O
predicted	O
from	O
the	O
p	O
previous	O
x	O
s	O
through	O
a	O
regression	O
equation	O
if	O
one	O
stores	O
the	O
current	O
x	O
and	O
the	O
p	O
previous	O
values	O
as	O
a	O
state	O
vector	O
then	O
the	O
arp	O
scalar	O
process	O
can	O
be	O
written	O
equivalently	O
as	O
a	O
vector	O
process	O
figure	O
graphical	O
model	O
illustrating	O
an	O
process	O
moving	O
from	O
the	O
discrete	O
time	O
to	O
the	O
continuous	O
time	O
setting	O
the	O
question	O
arises	O
as	O
to	O
how	O
generalize	O
the	O
markov	O
notion	O
used	O
in	O
the	O
discrete-time	O
ar	O
process	O
to	O
define	O
a	O
continuoous-time	O
ar	O
process	O
it	O
turns	O
out	O
that	O
the	O
correct	O
generalization	B
uses	O
the	O
idea	O
of	O
having	O
not	O
only	O
the	O
function	B
value	O
but	O
also	O
p	O
of	O
its	O
derivatives	O
at	O
time	O
t	O
giving	O
rise	O
to	O
the	O
stochastic	B
differential	I
equation	I
sde	O
stochastic	B
differential	I
equation	I
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
markov	O
processes	O
apx	O
ap	O
where	O
x	O
denotes	O
the	O
ith	O
derivative	O
of	O
xt	O
and	O
zt	O
is	O
a	O
white	O
gaussian	O
noise	O
process	O
with	O
covariance	B
this	O
white	O
noise	O
process	O
can	O
be	O
considered	O
the	O
derivative	O
of	O
the	O
wiener	B
process	I
to	O
avoid	O
redundancy	O
in	O
the	O
coefficients	O
we	O
assume	O
that	O
ap	O
a	O
considerable	O
amount	O
of	O
mathematical	O
machinery	O
is	O
required	O
to	O
make	O
rigorous	O
the	O
meaning	O
of	O
such	O
equations	O
see	O
e	O
g	O
ksendal	O
as	O
for	O
the	O
discrete-time	O
case	O
one	O
can	O
write	O
eq	O
as	O
a	O
first-order	O
vector	O
sde	O
by	O
defining	O
the	O
state	O
to	O
be	O
xt	O
and	O
its	O
first	O
p	O
derivatives	O
we	O
begin	O
this	O
chapter	O
with	O
a	O
summary	O
of	O
some	O
fourier	O
analysis	O
results	O
in	O
section	O
fourier	O
analysis	O
is	O
important	O
to	O
linear	B
time	O
invariant	O
systems	O
such	O
as	O
equations	O
and	O
because	O
ist	O
is	O
an	O
eigenfunction	B
of	O
the	O
corresponding	O
difference	O
differential	O
operator	B
we	O
then	O
move	O
on	O
in	O
section	O
to	O
discuss	O
continuous-time	O
gaussian	O
markov	O
processes	O
on	O
the	O
real	O
line	O
and	O
their	O
relationship	O
to	O
the	O
same	O
sde	O
on	O
the	O
circle	O
in	O
section	O
we	O
describe	O
discrete-time	O
gaussian	O
markov	O
processes	O
on	O
the	O
integer	O
lattice	O
and	O
their	O
relationship	O
to	O
the	O
same	O
difference	O
equation	O
on	O
the	O
circle	O
in	O
section	O
we	O
explain	O
the	O
relationship	O
between	O
discrete-time	O
gmps	O
and	O
the	O
discrete	O
sampling	O
of	O
continuous-time	O
gmps	O
finally	O
in	O
section	O
we	O
discuss	O
generalizations	O
of	O
the	O
markov	O
concept	O
in	O
higher	O
dimensions	O
much	O
of	O
this	O
material	O
is	O
quite	O
standard	O
although	O
the	O
relevant	O
results	O
are	O
often	O
scattered	O
through	O
different	O
sources	O
and	O
our	O
aim	O
is	O
to	O
provide	O
a	O
unified	O
treatment	O
the	O
relationship	O
between	O
the	O
second-order	O
properties	O
of	O
the	O
sdes	O
on	O
the	O
real	O
line	O
and	O
the	O
circle	O
and	O
difference	O
equations	O
on	O
the	O
integer	O
lattice	O
and	O
the	O
regular	O
polygon	O
is	O
to	O
our	O
knowledge	O
novel	O
fourier	O
analysis	O
we	O
follow	O
the	O
treatment	O
given	O
by	O
kammler	O
we	O
consider	O
fourier	O
analysis	O
of	O
functions	O
on	O
the	O
real	O
line	O
r	O
of	O
periodic	B
functions	O
of	O
period	O
l	O
on	O
the	O
circle	O
tl	O
of	O
functions	O
defined	O
on	O
the	O
integer	O
lattice	O
z	O
and	O
of	O
functions	O
on	O
pn	O
the	O
regular	O
n-polygon	O
which	O
is	O
a	O
discretization	O
of	O
tl	O
for	O
sufficiently	O
well-behaved	O
functions	O
on	O
r	O
we	O
have	O
fx	O
isx	O
ds	O
fs	O
fxe	O
isx	O
dx	O
we	O
refer	O
to	O
the	O
equation	O
on	O
the	O
left	O
as	O
the	O
synthesis	O
equation	O
and	O
the	O
equation	O
on	O
the	O
right	O
as	O
the	O
analysis	O
equation	O
for	O
functions	O
on	O
tl	O
we	O
obtain	O
the	O
fourier	O
series	O
representations	O
fxe	O
ikxl	O
dx	O
ikxl	O
fk	O
fx	O
x	O
ak	O
coefficients	O
in	O
equations	O
and	O
are	O
not	O
intended	O
to	O
have	O
a	O
close	O
relationship	O
an	O
approximate	O
relationship	O
might	O
be	O
established	O
through	O
the	O
use	O
of	O
finite-difference	O
approximations	O
to	O
derivatives	O
z	O
k	O
z	O
z	O
l	O
l	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
fourier	O
analysis	O
z	O
l	O
n	O
where	O
fk	O
denotes	O
the	O
coefficient	O
of	O
ikxl	O
in	O
the	O
expansion	O
we	O
use	O
square	O
to	O
denote	O
that	O
the	O
argument	O
is	O
discrete	O
so	O
that	O
xt	O
and	O
xt	O
are	O
brackets	O
equivalent	B
notations	O
similarly	O
for	O
z	O
we	O
obtain	O
fn	O
isnl	O
ds	O
fs	O
x	O
n	O
l	O
fne	O
isnl	O
note	O
that	O
fs	O
is	O
periodic	B
with	O
period	O
l	O
and	O
so	O
is	O
defined	O
only	O
for	O
s	O
l	O
to	O
avoid	O
aliasing	O
often	O
this	O
transform	O
is	O
defined	O
for	O
the	O
special	O
case	O
l	O
but	O
the	O
general	O
case	O
emphasizes	O
the	O
duality	O
between	O
equations	O
and	O
finally	O
for	O
functions	O
on	O
pn	O
we	O
have	O
the	O
discrete	O
fourier	B
transform	I
fn	O
iknn	O
fk	O
n	O
fne	O
iknn	O
n	O
note	O
that	O
there	O
are	O
other	O
conventions	O
for	O
fourier	O
transforms	O
particularly	O
those	O
involving	O
s	O
however	O
this	O
tends	O
to	O
destroy	O
symmetry	O
between	O
the	O
analysis	O
and	O
synthesis	O
equations	O
so	O
we	O
use	O
the	O
definitions	O
given	O
above	O
in	O
the	O
case	O
of	O
stochastic	O
processes	O
the	O
most	O
important	O
fourier	O
relationship	O
is	O
between	O
the	O
covariance	B
function	B
and	O
the	O
power	O
spectrum	O
this	O
is	O
known	O
as	O
the	O
wiener-khintchine	B
theorem	I
see	O
e	O
g	O
chatfield	O
sampling	O
and	O
periodization	O
we	O
can	O
obtain	O
relationships	O
between	O
functions	O
and	O
their	O
transforms	O
on	O
r	O
tl	O
z	O
pn	O
through	O
the	O
notions	O
of	O
sampling	O
and	O
periodization	O
definition	O
h-sampling	O
given	O
a	O
function	B
f	O
on	O
r	O
and	O
a	O
spacing	O
parameter	O
h	O
we	O
construct	O
a	O
corresponding	O
discrete	O
function	B
on	O
z	O
using	O
similarly	O
we	O
can	O
discretize	O
a	O
function	B
defined	O
on	O
tl	O
onto	O
pn	O
but	O
in	O
this	O
case	O
we	O
must	O
take	O
h	O
ln	O
so	O
that	O
n	O
steps	O
of	O
size	O
h	O
will	O
equal	O
the	O
period	O
l	O
fnh	O
n	O
z	O
definition	O
periodization	O
by	O
summation	O
let	O
fx	O
be	O
a	O
function	B
on	O
r	O
that	O
rapidly	O
approaches	O
as	O
x	O
we	O
can	O
sum	O
translates	O
of	O
the	O
function	B
to	O
produce	O
the	O
l-periodic	O
function	B
gx	O
fx	O
ml	O
for	O
l	O
analogously	O
when	O
is	O
defined	O
on	O
z	O
and	O
rapidly	O
approaches	O
as	O
n	O
we	O
can	O
construct	O
a	O
function	B
on	O
pn	O
by	O
n-summation	O
by	O
setting	O
mn	O
x	O
m	O
x	O
m	O
h-sampling	O
periodization	O
by	O
summation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
markov	O
processes	O
let	O
be	O
obtained	O
by	O
h-sampling	O
from	O
fx	O
with	O
corresponding	O
fourier	O
transforms	O
and	O
fs	O
then	O
we	O
have	O
z	O
fnh	O
isnh	O
ds	O
isnl	O
ds	O
by	O
breaking	O
up	O
the	O
domain	O
of	O
integration	O
in	O
eq	O
we	O
obtain	O
z	O
l	O
x	O
x	O
z	O
l	O
z	O
z	O
l	O
ml	O
x	O
x	O
m	O
m	O
x	O
m	O
m	O
m	O
isnh	O
ds	O
using	O
the	O
change	O
of	O
variable	O
s	O
ml	O
now	O
set	B
hl	O
and	O
use	O
inm	O
for	O
n	O
m	O
integers	O
to	O
obtain	O
fs	O
ml	O
isnl	O
ds	O
which	O
implies	O
that	O
fs	O
ml	O
p	O
with	O
l	O
alternatively	O
setting	O
l	O
one	O
obtains	O
similarly	O
if	O
f	O
is	O
defined	O
on	O
tl	O
and	O
f	O
nl	O
m	O
f	O
sm	O
h	O
n	O
is	O
obtained	O
by	O
sampling	O
then	O
h	O
fn	O
mn	O
thus	O
we	O
see	O
that	O
sampling	O
in	O
x-space	O
causes	O
periodization	O
in	O
fourier	O
space	O
periodic	B
function	B
gx	O
now	O
consider	O
the	O
periodization	O
of	O
a	O
function	B
fx	O
with	O
x	O
r	O
to	O
give	O
the	O
lm	O
fx	O
ml	O
let	O
gk	O
be	O
the	O
fourier	O
coefficients	O
of	O
gx	O
we	O
obtain	O
z	O
l	O
z	O
gk	O
l	O
l	O
gxe	O
ikxl	O
dx	O
l	O
fxe	O
ikxl	O
dx	O
z	O
l	O
x	O
k	O
m	O
f	O
l	O
l	O
fx	O
mle	O
ikxl	O
dx	O
assuming	O
that	O
fx	O
is	O
sufficiently	O
well-behaved	O
that	O
the	O
summation	O
and	O
integration	O
operations	O
can	O
be	O
exchanged	O
a	O
similar	O
relationship	O
can	O
be	O
obtained	O
for	O
the	O
periodization	O
of	O
a	O
function	B
defined	O
on	O
z	O
thus	O
we	O
see	O
that	O
periodization	O
in	O
x-space	O
gives	O
rise	O
to	O
sampling	O
in	O
fourier	O
space	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
continuous-time	O
gaussian	O
markov	O
processes	O
continuous-time	O
gaussian	O
markov	O
processes	O
we	O
first	O
consider	O
continuous-time	O
gaussian	O
markov	O
processes	O
on	O
the	O
real	O
line	O
and	O
then	O
relate	O
the	O
covariance	B
function	B
obtained	O
to	O
that	O
for	O
the	O
stationary	O
solution	O
of	O
the	O
sde	O
on	O
the	O
circle	O
our	O
treatment	O
of	O
continuous-time	O
gmps	O
on	O
r	O
follows	O
papoulis	O
ch	O
continuous-time	O
gmps	O
on	O
r	O
we	O
wish	O
to	O
find	O
the	O
power	O
spectrum	O
and	O
covariance	B
function	B
for	O
the	O
stationary	O
process	O
corresponding	O
to	O
the	O
sde	O
given	O
by	O
eq	O
recall	O
that	O
the	O
covariance	B
function	B
of	O
a	O
stationary	O
process	O
kt	O
and	O
the	O
power	O
spectrum	O
ss	O
form	O
a	O
fourier	B
transform	I
pair	O
the	O
fourier	B
transform	I
of	O
the	O
stochastic	O
process	O
xt	O
is	O
a	O
stochastic	O
process	O
z	O
xs	O
given	O
by	O
z	O
xs	O
xte	O
ist	O
dt	O
xt	O
ist	O
ds	O
where	O
the	O
integrals	B
are	O
interpreted	O
as	O
a	O
mean-square	O
limit	O
let	O
denote	O
complex	O
conjugation	O
and	O
h	O
denote	O
expectation	O
with	O
respect	O
to	O
the	O
stochastic	O
process	O
then	O
for	O
a	O
stationary	O
gaussian	B
process	I
we	O
have	O
h	O
x	O
hxtx	O
dt	O
z	O
z	O
z	O
d	O
k	O
the	O
delta	O
function	B
r	O
e	O
istdt	O
this	O
shows	O
that	O
and	O
are	O
using	O
the	O
change	O
of	O
variables	O
t	O
and	O
the	O
integral	O
representation	O
of	O
uncorrelated	O
for	O
i	O
e	O
that	O
the	O
fourier	O
basis	O
are	O
eigenfunctions	O
of	O
the	O
differential	O
operator	B
also	O
from	O
eq	O
we	O
obtain	O
x	O
isk	O
ist	O
ds	O
now	O
if	O
we	O
fourier	B
transform	I
eq	O
we	O
obtain	O
isk	O
xs	O
zs	O
z	O
px	O
where	O
zs	O
denotes	O
the	O
fourier	B
transform	I
of	O
the	O
white	O
noise	O
taking	O
the	O
product	O
of	O
equation	O
with	O
its	O
complex	O
conjugate	O
and	O
taking	O
expectations	O
we	O
obtain	O
px	O
px	O
ak	O
h	O
x	O
z	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
process	O
arp	O
process	O
let	O
az	O
pp	O
spectrum	O
of	O
white	O
noise	O
is	O
we	O
obtain	O
akzk	O
then	O
using	O
eq	O
and	O
the	O
fact	O
that	O
the	O
power	O
gaussian	O
markov	O
processes	O
srs	O
note	O
that	O
the	O
denominator	O
is	O
a	O
polynomial	B
of	O
order	O
p	O
in	O
the	O
relationship	O
of	O
stationary	O
solutions	O
of	O
pth-order	O
sdes	O
to	O
rational	O
spectral	O
densities	O
can	O
be	O
traced	O
back	O
at	O
least	O
as	O
far	O
as	O
doob	O
above	O
we	O
have	O
assumed	O
that	O
the	O
process	O
is	O
stationary	O
however	O
this	O
depends	O
on	O
the	O
coefficients	O
ap	O
to	O
analyze	O
this	O
issue	O
we	O
assume	O
a	O
solution	O
of	O
the	O
form	O
xt	O
e	O
t	O
when	O
the	O
driving	O
term	O
this	O
leads	O
to	O
the	O
condition	O
ak	O
k	O
must	O
lie	O
in	O
the	O
left	O
for	O
stationarity	B
that	O
the	O
roots	O
of	O
the	O
polynomialpp	O
half	O
plane	O
o	O
p	O
example	O
process	O
in	O
this	O
case	O
we	O
have	O
the	O
sde	O
where	O
for	O
stationarity	B
this	O
gives	O
rise	O
to	O
the	O
power	O
spectrum	O
ss	O
is	O
is	O
taking	O
the	O
fourier	B
transform	I
we	O
obtain	O
kt	O
e	O
to	O
the	O
power	O
spectrum	O
ss	O
this	O
process	O
is	O
known	O
as	O
the	O
ornstein-uhlenbeck	B
process	O
and	O
ornstein	O
and	O
was	O
introduced	O
as	O
a	O
mathematical	O
model	O
of	O
the	O
velocity	O
of	O
a	O
particle	O
undergoing	O
brownian	O
motion	O
it	O
can	O
be	O
shown	O
that	O
the	O
ou	O
process	O
is	O
the	O
unique	O
stationary	O
first-order	O
gaussian	B
markov	I
process	I
example	O
arp	O
process	O
in	O
general	O
the	O
covariance	B
transform	O
corresponding	O
ak	O
isk	O
can	O
be	O
quite	O
complicated	O
for	O
example	O
papoulis	O
p	O
gives	O
three	O
forms	O
of	O
is	O
the	O
covariance	B
function	B
for	O
the	O
process	O
depending	O
on	O
whether	O
greater	O
than	O
equal	O
to	O
or	O
less	O
than	O
however	O
if	O
the	O
coefficients	O
ap	O
are	O
chosen	O
in	O
a	O
particular	O
way	O
then	O
one	O
can	O
obtain	O
iskpp	O
ss	O
ance	O
function	B
is	O
of	O
the	O
formpp	O
for	O
some	O
it	O
can	O
be	O
shown	O
p	O
that	O
the	O
corresponding	O
ktke	O
for	O
some	O
coefficients	O
p	O
e	O
for	O
the	O
ou	O
process	O
for	O
e	O
these	O
are	O
special	O
cases	O
of	O
the	O
mat	O
ern	O
for	O
p	O
we	O
have	O
already	O
seen	O
that	O
kt	O
p	O
we	O
obtain	O
kt	O
class	O
of	O
covariance	B
functions	O
described	O
in	O
section	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
wiener	B
process	I
continuous-time	O
gaussian	O
markov	O
processes	O
example	O
wiener	B
process	I
although	O
our	O
derivations	O
have	O
focussed	O
on	O
stationary	O
gaussian	O
markov	O
processes	O
there	O
are	O
also	O
several	O
important	O
non-stationary	O
processes	O
one	O
of	O
the	O
most	O
important	O
is	O
the	O
wiener	B
process	I
that	O
satisfies	O
the	O
sde	O
zt	O
for	O
t	O
with	O
the	O
initial	O
condition	O
this	O
process	O
has	O
covariance	B
function	B
kt	O
s	O
mint	O
s	O
an	O
interesting	O
variant	O
of	O
the	O
wiener	B
process	I
known	O
as	O
the	O
brownian	B
bridge	I
tied-down	B
wiener	B
process	I
is	O
obtained	O
by	O
conditioning	O
on	O
the	O
wiener	B
process	I
passing	O
through	O
this	O
has	O
covariance	B
kt	O
s	O
mint	O
s	O
st	O
for	O
s	O
t	O
see	O
e	O
g	O
grimmett	O
and	O
stirzaker	O
for	O
further	O
information	O
on	O
these	O
processes	O
markov	O
processes	O
derived	O
from	O
sdes	O
of	O
order	O
p	O
are	O
p	O
times	O
ms	O
differentiable	O
this	O
is	O
easy	O
to	O
see	O
heuristically	O
from	O
eq	O
given	O
that	O
a	O
process	O
gets	O
rougher	O
the	O
more	O
times	O
it	O
is	O
differentiated	O
eq	O
tells	O
us	O
that	O
x	O
is	O
like	O
the	O
white	O
noise	O
process	O
i	O
e	O
not	O
ms	O
continuous	O
so	O
for	O
example	O
the	O
ou	O
process	O
also	O
the	O
wiener	B
process	I
are	O
ms	O
continuous	O
but	O
not	O
ms	O
differentiable	O
the	O
solution	O
of	O
the	O
corresponding	O
sde	O
on	O
the	O
cir	O
cle	O
the	O
analogous	O
analysis	O
to	O
that	O
on	O
the	O
real	O
line	O
is	O
carried	O
out	O
on	O
tl	O
using	O
xt	O
intl	O
xn	O
l	O
xte	O
intldt	O
x	O
n	O
z	O
l	O
as	O
xt	O
is	O
assumed	O
stationary	O
we	O
obtain	O
an	O
analogous	O
result	O
to	O
eq	O
i	O
e	O
that	O
the	O
fourier	O
coefficients	O
are	O
independent	O
sn	O
px	O
h	O
xm	O
x	O
if	O
m	O
n	O
otherwise	O
p	O
similarly	O
the	O
covariance	B
function	B
on	O
the	O
cirle	O
is	O
given	O
by	O
kt	O
s	O
hxtx	O
x	O
n	O
int	O
sl	O
let	O
l	O
then	O
plugging	O
in	O
the	O
expression	O
n	O
lk	O
xnein	O
lt	O
into	O
the	O
sde	O
eq	O
and	O
equating	O
terms	O
in	O
we	O
obtain	O
akin	O
lk	O
xn	O
zn	O
as	O
in	O
the	O
real-line	O
case	O
we	O
form	O
the	O
product	O
of	O
equation	O
with	O
its	O
complex	O
conjugate	O
and	O
take	O
expectations	O
to	O
give	O
note	O
that	O
stn	O
is	O
equal	O
to	O
n	O
stn	O
i	O
e	O
that	O
it	O
is	O
a	O
sampling	O
of	O
sr	O
at	O
intervals	O
where	O
srs	O
is	O
the	O
power	O
spectrum	O
of	O
the	O
continuous	O
process	O
on	O
the	O
real	O
line	O
given	O
in	O
equation	O
let	O
kth	O
denote	O
the	O
covariance	B
function	B
on	O
the	O
l	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
markov	O
processes	O
circle	O
and	O
krh	O
denote	O
the	O
covariance	B
function	B
on	O
the	O
real	O
line	O
for	O
the	O
sde	O
then	O
using	O
eq	O
we	O
find	O
that	O
x	O
m	O
ktt	O
krt	O
ml	O
order	O
sde	O
example	O
sde	O
on	O
r	O
for	O
the	O
ou	O
process	O
we	O
have	O
krt	O
by	O
summing	O
the	O
series	O
geometric	O
progressions	O
we	O
obtain	O
ktt	O
e	O
e	O
e	O
l	O
sinh	O
e	O
for	O
l	O
t	O
l	O
eq	O
is	O
also	O
given	O
to	O
scaling	O
factors	O
in	O
grenander	O
et	O
al	O
eq	O
where	O
it	O
is	O
obtained	O
by	O
a	O
limiting	O
argument	O
from	O
the	O
discrete-time	O
gmp	O
on	O
pn	O
see	O
section	O
discrete-time	O
gaussian	O
markov	O
processes	O
we	O
first	O
consider	O
discrete-time	O
gaussian	O
markov	O
processes	O
on	O
z	O
and	O
then	O
relate	O
the	O
covariance	B
function	B
obtained	O
to	O
that	O
of	O
the	O
stationary	O
solution	O
of	O
the	O
difference	O
equation	O
on	O
pn	O
chatfield	O
and	O
diggle	O
provide	O
good	O
coverage	O
of	O
discrete-time	O
arma	O
models	O
on	O
z	O
discrete-time	O
gmps	O
on	O
z	O
assuming	O
that	O
the	O
process	O
is	O
stationary	O
the	O
covariance	B
function	B
ki	O
denotes	O
hxtxtii	O
t	O
z	O
that	O
because	O
of	O
stationarity	B
ki	O
k	O
i	O
we	O
first	O
use	O
a	O
fourier	O
approach	O
to	O
derive	O
the	O
power	O
spectrum	O
and	O
hence	O
the	O
covariance	B
function	B
of	O
the	O
arp	O
process	O
defining	O
we	O
can	O
rewrite	O
akxt	O
k	O
the	O
fourier	O
pair	O
for	O
xt	O
is	O
x	O
t	O
istl	O
ds	O
xs	O
l	O
xte	O
istl	O
akxt	O
k	O
we	O
obtain	O
px	O
ake	O
i	O
xs	O
zs	O
eq	O
aspp	O
z	O
l	O
xt	O
plugging	O
this	O
intopp	O
where	O
l	O
as	O
above	O
taking	O
the	O
product	O
of	O
eq	O
with	O
its	O
complex	O
conjugate	O
and	O
taking	O
expectations	O
we	O
obtain	O
szs	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
discrete-time	O
gaussian	O
markov	O
processes	O
above	O
we	O
have	O
assumed	O
that	O
the	O
process	O
is	O
stationary	O
however	O
this	O
depends	O
on	O
the	O
coefficients	O
ap	O
to	O
analyze	O
this	O
issue	O
we	O
assume	O
a	O
solution	O
of	O
the	O
form	O
xt	O
zt	O
when	O
the	O
driving	O
term	O
this	O
leads	O
to	O
the	O
condition	O
akzp	O
k	O
must	O
lie	O
inside	O
for	O
stationarity	B
that	O
the	O
roots	O
of	O
the	O
polynomial	B
pp	O
the	O
unit	O
circle	O
see	O
hannan	O
theorem	O
p	O
for	O
further	O
details	O
as	O
well	O
as	O
deriving	O
the	O
covariance	B
function	B
from	O
the	O
fourier	B
transform	I
of	O
the	O
power	O
spectrum	O
it	O
can	O
also	O
be	O
obtained	O
by	O
solving	O
a	O
set	B
of	O
linear	B
equations	O
our	O
first	O
observation	O
is	O
that	O
xs	O
is	O
independent	O
of	O
zt	O
for	O
s	O
t	O
multiplying	O
equation	O
through	O
by	O
zt	O
and	O
taking	O
expectations	O
we	O
obtain	O
hxtzti	O
and	O
hxt	O
izti	O
for	O
i	O
by	O
multiplying	O
equation	O
through	O
by	O
xt	O
j	O
for	O
j	O
and	O
taking	O
expectations	O
we	O
obtain	O
the	O
yule-walker	B
equations	I
yule-walker	B
equations	I
process	O
kj	O
aiki	O
aikj	O
i	O
j	O
the	O
first	O
p	O
of	O
these	O
equations	O
form	O
a	O
linear	B
system	O
that	O
can	O
be	O
used	O
to	O
solve	O
for	O
kp	O
in	O
terms	O
of	O
and	O
ap	O
and	O
eq	O
can	O
be	O
used	O
to	O
obtain	O
kj	O
for	O
j	O
p	O
recursively	O
example	O
process	O
the	O
simplest	O
example	O
of	O
an	O
ar	O
process	O
is	O
the	O
process	O
defined	O
as	O
xt	O
this	O
gives	O
rise	O
to	O
the	O
yule-walker	B
equations	I
and	O
x	O
where	O
the	O
linear	B
system	O
for	O
can	O
easily	O
be	O
solved	O
to	O
give	O
kj	O
a	O
is	O
the	O
variance	O
of	O
the	O
process	O
notice	O
that	O
for	O
the	O
process	O
to	O
x	O
be	O
stationary	O
we	O
require	O
the	O
corresponding	O
power	O
spectrum	O
obtained	O
from	O
equation	O
is	O
ss	O
cos	O
ls	O
similarly	O
to	O
the	O
continuous	O
case	O
the	O
covariance	B
function	B
for	O
the	O
discrete-time	O
process	O
has	O
three	O
different	O
forms	O
depending	O
on	O
these	O
are	O
described	O
in	O
diggle	O
example	O
the	O
solution	O
of	O
the	O
corresponding	O
difference	O
equa	O
tion	O
on	O
pn	O
we	O
now	O
consider	O
variables	O
x	O
xn	O
arranged	O
around	O
the	O
circle	O
with	O
n	O
p	O
by	O
appropriately	O
modifying	O
eq	O
we	O
obtain	O
xt	O
akxmodt	O
kn	O
px	O
px	O
px	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
markov	O
processes	O
pn	O
the	O
zt	O
s	O
are	O
i	O
i	O
d	O
and	O
n	O
thus	O
z	O
zn	O
has	O
density	O
pz	O
exp	O
t	O
equation	O
shows	O
that	O
x	O
and	O
z	O
are	O
related	O
by	O
a	O
linear	B
transformation	O
and	O
thus	O
z	O
px	O
exp	O
n	O
xt	O
px	O
akxmodt	O
kn	O
this	O
is	O
an	O
n-dimensional	O
multivariate	O
gaussian	O
for	O
an	O
arp	O
process	O
the	O
inverse	O
covariance	B
matrix	I
has	O
a	O
circulant	O
structure	O
consisting	O
of	O
a	O
diagonal	O
band	O
entries	O
wide	O
and	O
appropriate	O
circulant	O
entries	O
in	O
the	O
corners	O
thus	O
pxtx	O
xt	O
pxtxmodt	O
xmodt	O
pn	O
xmodtpn	O
which	O
geman	O
and	O
geman	O
call	O
the	O
two-sided	O
markov	O
property	O
notice	O
that	O
it	O
is	O
the	O
zeros	O
in	O
the	O
inverse	O
covariance	B
matrix	I
that	O
indicate	O
the	O
conditional	B
independence	O
structure	O
see	O
also	O
section	O
the	O
properties	O
of	O
eq	O
have	O
been	O
studied	O
by	O
a	O
number	O
of	O
authors	O
e	O
g	O
whittle	O
the	O
name	O
of	O
circulant	O
processes	O
kashyap	O
and	O
chellappa	O
the	O
name	O
of	O
circular	O
autoregressive	O
models	O
and	O
grenander	O
et	O
al	O
cyclic	O
markov	O
process	O
as	O
above	O
we	O
define	O
the	O
fourier	B
transform	I
pair	O
xn	O
inmn	O
xm	O
n	O
xne	O
inmn	O
n	O
n	O
px	O
by	O
similar	O
arguments	O
to	O
those	O
above	O
we	O
obtain	O
ak	O
imn	O
zm	O
where	O
and	O
thus	O
spm	O
imn	O
as	O
in	O
the	O
continuous-time	O
case	O
we	O
see	O
that	O
spm	O
is	O
obtained	O
by	O
sampling	O
the	O
power	O
spectrum	O
of	O
the	O
corresponding	O
process	O
on	O
the	O
line	O
so	O
that	O
spm	O
ml	O
n	O
thus	O
using	O
eq	O
we	O
have	O
x	O
kpn	O
m	O
kzn	O
mn	O
process	O
example	O
process	O
for	O
this	O
process	O
xt	O
the	O
diagonal	O
entries	O
in	O
the	O
inverse	O
covariance	B
are	O
and	O
the	O
non-zero	O
offdiagonal	O
entries	O
are	O
by	O
summing	O
the	O
covariance	B
function	B
kzn	O
we	O
obtain	O
x	O
a	O
kpn	O
x	O
an	O
a	O
n	O
n	O
n	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
the	O
relationship	O
between	O
discrete-time	O
and	O
sampled	O
continuous-time	O
gmps	O
we	O
now	O
illustrate	O
this	O
result	O
for	O
n	O
in	O
this	O
case	O
the	O
covariance	B
matrix	I
has	O
diagonal	O
entries	O
of	O
the	O
inverse	O
covariance	B
matrix	I
has	O
the	O
structure	O
described	O
above	O
multiplying	O
these	O
two	O
matrices	O
together	O
we	O
do	O
indeed	O
obtain	O
the	O
identity	O
matrix	O
and	O
off-diagonal	O
entries	O
of	O
x	O
x	O
the	O
relationship	O
between	O
discrete-time	O
and	O
sampled	O
continuous-time	O
gmps	O
we	O
now	O
consider	O
the	O
relationship	O
between	O
continuous-time	O
and	O
discrete-time	O
gmps	O
in	O
particular	O
we	O
ask	O
the	O
question	O
is	O
a	O
regular	O
sampling	O
of	O
a	O
continuoustime	O
arp	O
process	O
a	O
discrete-time	O
arp	O
process	O
it	O
turns	O
out	O
that	O
the	O
answer	O
will	O
in	O
general	O
be	O
negative	O
first	O
we	O
define	O
a	O
generalization	B
of	O
ar	O
processes	O
known	O
as	O
autoregressive	O
moving-average	O
processes	O
arma	O
processes	O
the	O
arp	O
process	O
defined	O
above	O
is	O
a	O
special	O
case	O
of	O
the	O
more	O
general	O
armap	O
q	O
process	O
which	O
is	O
defined	O
as	O
px	O
qx	O
xt	O
aixt	O
i	O
bjzt	O
j	O
observe	O
that	O
the	O
arp	O
process	O
is	O
in	O
fact	O
also	O
an	O
armap	O
process	O
a	O
spectral	O
analysis	O
of	O
equation	O
similar	O
to	O
that	O
performed	O
in	O
section	O
gives	O
where	O
bz	O
density	O
of	O
the	O
form	O
ss	O
ss	O
bjzj	O
in	O
continuous	O
time	O
a	O
process	O
with	O
a	O
rational	O
spectral	O
we	O
require	O
q	O
p	O
as	O
ssds	O
is	O
known	O
as	O
a	O
armap	O
q	O
process	O
for	O
this	O
to	O
define	O
a	O
valid	O
covariance	B
function	B
discrete-time	O
observation	O
of	O
a	O
continuous-time	O
process	O
let	O
xt	O
be	O
a	O
continuous-time	O
process	O
having	O
covariance	B
function	B
kt	O
and	O
power	O
spectrum	O
ss	O
let	O
xh	O
be	O
the	O
discrete-time	O
process	O
obtained	O
by	O
sampling	O
xt	O
at	O
interval	O
h	O
so	O
that	O
xhn	O
xnh	O
for	O
n	O
z	O
clearly	O
the	O
covariance	B
function	B
of	O
this	O
process	O
is	O
given	O
by	O
khn	O
knh	O
by	O
eq	O
this	O
means	O
that	O
x	O
m	O
shs	O
ss	O
m	O
h	O
where	O
shs	O
is	O
defined	O
using	O
l	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
gaussian	O
markov	O
processes	O
theorem	O
let	O
x	O
be	O
a	O
continuous-time	O
stationary	O
gaussian	B
process	I
and	O
xh	O
be	O
the	O
discretization	O
of	O
this	O
process	O
if	O
x	O
is	O
an	O
arma	O
process	O
then	O
xh	O
is	O
also	O
an	O
arma	O
process	O
however	O
if	O
x	O
is	O
an	O
ar	O
process	O
then	O
xh	O
is	O
not	O
necessarily	O
an	O
ar	O
process	O
the	O
proof	O
is	O
given	O
in	O
ihara	O
theorem	O
it	O
is	O
easy	O
to	O
see	O
using	O
the	O
covariance	B
functions	O
given	O
in	O
sections	O
and	O
that	O
the	O
discretization	O
of	O
a	O
continuous-time	O
process	O
is	O
indeed	O
a	O
discrete-time	O
process	O
however	O
ihara	O
shows	O
that	O
in	O
general	O
the	O
discretization	O
of	O
a	O
continuous-time	O
process	O
is	O
not	O
a	O
discrete-time	O
process	O
markov	O
processes	O
in	O
higher	O
dimensions	O
we	O
have	O
concentrated	O
above	O
on	O
the	O
case	O
where	O
t	O
is	O
one-dimensional	O
in	O
higher	O
dimensions	O
it	O
is	O
interesting	O
to	O
ask	O
how	O
the	O
markov	O
property	O
might	O
be	O
generalized	B
let	O
s	O
be	O
an	O
infinitely	O
differentiable	O
closed	O
surface	O
separating	O
rd	O
into	O
a	O
bounded	O
part	O
s	O
and	O
an	O
unbounded	O
part	O
s	O
loosely	O
a	O
random	O
field	O
xt	O
is	O
said	O
to	O
be	O
quasi-markovian	O
if	O
xt	O
for	O
t	O
s	O
and	O
xu	O
for	O
u	O
s	O
are	O
independent	O
given	O
xs	O
for	O
s	O
s	O
wong	O
showed	O
that	O
the	O
only	O
isotropic	O
quasi-markov	O
gaussian	O
field	O
with	O
a	O
continuous	O
covariance	B
function	B
is	O
the	O
degenerate	B
case	O
xt	O
where	O
is	O
a	O
gaussian	O
variate	O
however	O
if	O
instead	O
of	O
conditioning	O
on	O
the	O
values	O
that	O
the	O
field	O
takes	O
on	O
in	O
s	O
one	O
conditions	O
on	O
a	O
somewhat	O
larger	O
set	B
then	O
gaussian	O
random	O
fields	O
with	O
non-trivial	O
markovtype	O
structure	O
can	O
be	O
obtained	O
for	O
example	O
random	O
fields	O
with	O
an	O
inverse	O
kj	O
pseudo-markovian	O
of	O
order	O
p	O
for	O
example	O
the	O
d-dimensional	O
tensor-product	O
e	O
iti	O
is	O
pseudo-markovian	O
of	O
order	O
d	O
for	O
further	O
discussion	O
of	O
markov	O
properties	O
of	O
random	O
fields	O
see	O
the	O
appendix	O
in	O
adler	O
power	O
spectrum	O
of	O
the	O
formp	O
and	O
cs	O
sp	O
of	O
the	O
ou	O
process	O
kt	O
qd	O
d	O
with	O
for	O
some	O
c	O
are	O
said	O
to	O
be	O
k	O
skd	O
skd	O
d	O
if	O
instead	O
of	O
rd	O
we	O
wish	O
to	O
define	O
a	O
markov	B
random	I
field	I
on	O
a	O
graphical	O
structure	O
example	O
the	O
lattice	O
zd	O
things	O
become	O
more	O
straightforward	O
we	O
follow	O
the	O
presentation	O
in	O
jordan	O
let	O
g	O
e	O
be	O
a	O
graph	O
where	O
x	O
is	O
a	O
set	B
of	O
nodes	O
that	O
are	O
in	O
one-to-one	O
correspondence	O
with	O
a	O
set	B
of	O
random	O
variables	O
and	O
e	O
be	O
the	O
set	B
of	O
undirected	O
edges	O
of	O
the	O
graph	O
let	O
c	O
be	O
the	O
set	B
of	O
all	O
maximal	O
cliques	O
of	O
g	O
a	O
potential	O
function	B
cxc	O
is	O
a	O
function	B
on	O
the	O
possible	O
realizations	O
xc	O
of	O
the	O
maximal	O
clique	O
xc	O
potential	O
functions	O
are	O
assumed	O
to	O
be	O
positive	O
real-valued	O
functions	O
the	O
probability	O
distribution	O
px	O
corresponding	O
to	O
the	O
markov	B
random	I
field	I
is	O
given	O
by	O
y	O
function	B
obtained	O
by	O
summingintegratingq	O
px	O
c	O
c	O
z	O
cxc	O
where	O
z	O
is	O
a	O
normalization	O
factor	O
in	O
statistical	O
physics	O
as	O
the	O
partition	O
c	O
c	O
cxc	O
over	O
all	O
possible	O
as	O
a	O
precise	O
formulation	O
of	O
this	O
definition	O
involving	O
see	O
adler	O
p	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
markov	O
processes	O
in	O
higher	O
dimensions	O
signments	O
of	O
values	O
to	O
the	O
nodes	O
x	O
under	O
this	O
definition	O
it	O
is	O
easy	O
to	O
show	O
that	O
a	O
local	O
markov	O
property	O
holds	O
i	O
e	O
that	O
for	O
any	O
variable	O
x	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
all	O
other	O
variables	O
in	O
x	O
depends	O
only	O
on	O
those	O
variables	O
that	O
are	O
neighbours	O
of	O
x	O
a	O
useful	O
reference	O
on	O
markov	O
random	O
fields	O
is	O
winkler	O
a	O
simple	O
example	O
of	O
a	O
gaussian	O
markov	B
random	I
field	I
has	O
the	O
form	O
x	O
x	O
px	O
exp	O
i	O
i	O
ijj	O
n	O
where	O
ni	O
denotes	O
the	O
set	B
of	O
neighbours	O
of	O
node	O
xi	O
and	O
on	O
one	O
might	O
choose	O
a	O
four-connected	O
neighbourhood	O
i	O
e	O
those	O
nodes	O
to	O
the	O
north	O
south	O
east	O
and	O
west	O
of	O
a	O
given	O
node	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
appendix	O
c	O
datasets	O
and	O
code	O
the	O
datasets	O
used	O
for	O
experiments	O
in	O
this	O
book	O
and	O
implementations	O
of	O
the	O
algorithms	O
presented	O
are	O
available	O
for	O
download	O
at	O
the	O
website	O
of	O
the	O
book	O
httpwww	O
gaussianprocess	O
orggpml	O
the	O
programs	O
are	O
short	O
stand-alone	O
implementations	O
and	O
not	O
part	O
of	O
a	O
larger	O
package	O
they	O
are	O
meant	O
to	O
be	O
simple	O
to	O
understand	O
and	O
modify	O
for	O
a	O
desired	O
purpose	O
some	O
of	O
the	O
programs	O
allow	O
specification	O
of	O
covariance	B
functions	O
from	O
a	O
selection	O
provided	O
or	O
to	O
link	O
in	O
user	O
defined	O
covariance	B
code	O
for	O
some	O
of	O
the	O
plots	O
code	O
is	O
provided	O
which	O
produces	O
a	O
similar	O
plot	O
as	O
this	O
may	O
be	O
a	O
convenient	O
way	O
of	O
conveying	O
the	O
details	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
abrahamsen	O
p	O
a	O
review	O
of	O
gaussian	O
random	O
fields	O
and	O
correlation	O
functions	O
techhttppublications	O
nr	O
no	O
p	O
nical	O
report	O
norwegian	O
computing	O
center	O
oslo	O
norway	O
rapport	O
pdf	O
abramowitz	O
m	O
and	O
stegun	O
i	O
a	O
handbook	O
of	O
mathematical	O
functions	O
dover	O
new	O
york	O
pp	O
adams	O
r	O
sobolev	O
spaces	O
academic	O
press	O
new	O
york	O
p	O
adler	O
r	O
j	O
the	O
geometry	O
of	O
random	O
fields	O
wiley	O
chichester	O
pp	O
amari	O
s	O
differential-geometrical	O
methods	O
in	O
statistics	O
springer-verlag	O
berlin	O
p	O
ansley	O
c	O
f	O
and	O
kohn	O
r	O
estimation	O
filtering	O
and	O
smoothing	O
in	O
state	O
space	O
models	O
with	O
p	O
incompletely	O
specified	O
initial	O
conditions	O
annals	O
of	O
statistics	O
arat	O
o	O
m	O
linear	B
stochastic	O
systems	O
with	O
constant	O
coefficients	O
springer-verlag	O
berlin	O
lecture	O
p	O
notes	O
in	O
control	O
and	O
information	O
sciences	O
arfken	O
g	O
mathematical	O
methods	O
for	O
physicists	O
academic	O
press	O
san	O
diego	O
pp	O
xv	O
aronszajn	O
n	O
theory	O
of	O
reproducing	O
kernels	O
trans	O
amer	O
math	O
soc	O
pp	O
bach	O
f	O
r	O
and	O
jordan	O
m	O
i	O
kernel	B
independent	O
component	O
analysis	O
journal	O
of	O
machine	O
p	O
learning	B
research	O
baker	O
c	O
t	O
h	O
the	O
numerical	O
treatment	O
of	O
integral	O
equations	O
clarendon	O
press	O
oxford	O
pp	O
barber	O
d	O
and	O
saad	O
d	O
does	O
extra	O
knowledge	O
necessarily	O
improve	O
generalisation	O
neural	O
p	O
computation	O
bartle	O
r	O
g	O
the	O
elements	O
of	O
integration	O
and	O
lebesgue	O
measure	B
wiley	O
new	O
york	O
p	O
bartlett	O
p	O
l	O
jordan	O
m	O
i	O
and	O
mcauliffe	O
j	O
d	O
convexity	O
classification	B
and	O
risk	B
bounds	O
technical	O
report	O
department	O
of	O
statistics	O
university	O
of	O
california	O
berkeley	O
available	O
from	O
accepted	O
for	O
publication	O
in	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
p	O
berger	O
j	O
o	O
statistical	O
decision	O
theory	O
and	O
bayesian	O
analysis	O
springer	O
new	O
york	O
second	O
pp	O
edition	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
bishop	O
c	O
m	O
neural	O
networks	O
for	O
pattern	O
recognition	O
clarendon	O
press	O
oxford	O
p	O
bishop	O
c	O
m	O
svensen	O
m	O
and	O
williams	O
c	O
k	O
i	O
developments	O
of	O
the	O
generative	O
topographic	O
p	O
mapping	O
neurocomputing	O
bishop	O
c	O
m	O
svensen	O
m	O
and	O
williams	O
c	O
k	O
i	O
gtm	O
the	O
generative	O
topographic	O
p	O
mapping	O
neural	O
computation	O
blake	O
i	O
f	O
and	O
lindsey	O
w	O
c	O
level-crossing	O
problems	O
for	O
random	O
processes	O
ieee	O
trans	O
p	O
information	O
theory	O
blight	O
b	O
j	O
n	O
and	O
ott	O
l	O
a	O
bayesian	O
approach	O
to	O
model	O
inadequacy	O
for	O
polynomial	B
regression	O
p	O
biometrika	O
boyd	O
s	O
and	O
vandenberghe	O
l	O
convex	O
optimization	O
cambridge	O
university	O
press	O
cambridge	O
p	O
uk	O
boyle	O
p	O
and	O
frean	O
m	O
dependent	O
gaussian	O
processes	O
in	O
saul	O
l	O
k	O
weiss	O
y	O
and	O
bottou	O
l	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
bracewell	O
r	O
n	O
the	O
fourier	B
transform	I
and	O
its	O
applications	O
mcgraw-hill	O
singapore	O
interpp	O
national	O
edition	O
caruana	O
r	O
multitask	O
learning	B
machine	O
learning	B
p	O
chatfield	O
c	O
the	O
analysis	O
of	O
time	O
series	O
an	O
introduction	O
chapman	O
and	O
hall	O
london	O
pp	O
edition	O
choi	O
t	O
and	O
schervish	O
m	O
j	O
posterior	O
consistency	B
in	O
nonparametric	O
regression	O
problems	O
under	O
gaussian	B
process	I
priors	O
technical	O
report	O
department	O
of	O
statistics	O
cmu	O
p	O
choudhuri	O
n	O
ghosal	O
s	O
and	O
roy	O
a	O
nonparametric	O
binary	B
regression	O
using	O
a	O
gaussian	O
p	O
process	O
prior	O
unpublished	O
sghosalpapers	O
html	O
chu	O
w	O
and	O
ghahramani	O
z	O
gaussian	O
processes	O
for	O
ordinal	O
regression	O
journal	O
of	O
machine	O
p	O
learning	B
research	O
collins	O
m	O
and	O
duffy	O
n	O
convolution	O
kernels	O
for	O
natural	O
language	O
in	O
diettrich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
collobert	O
r	O
and	O
bengio	O
s	O
gression	O
problems	O
bengioprojectssvmtorch	O
html	O
journal	O
of	O
machine	O
learning	B
research	O
svmtorch	O
support	B
vector	I
machines	O
for	O
large-scale	O
rehttpwww	O
idiap	O
ch	O
pp	O
cornford	O
d	O
nabney	O
i	O
t	O
and	O
williams	O
c	O
k	O
i	O
modelling	O
frontal	O
discontinuities	O
in	O
wind	O
p	O
fields	O
journal	O
of	O
nonparameteric	O
statsitics	O
cox	O
d	O
d	O
multivariate	O
smoothing	O
spline	O
functions	O
siam	O
journal	O
on	O
numerical	O
analysis	O
p	O
cox	O
d	O
d	O
and	O
o	O
sullivan	O
f	O
asymptotic	O
analysis	O
of	O
penalized	B
likelihood	B
and	O
related	O
estip	O
mators	O
annals	O
of	O
statistics	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
craven	O
p	O
and	O
wahba	O
g	O
smoothing	O
noisy	O
data	O
with	O
spline	O
functions	O
numer	O
math	O
p	O
cressie	O
n	O
a	O
c	O
statistics	O
for	O
spatial	O
data	O
wiley	O
new	O
york	O
pp	O
cristianini	O
n	O
and	O
shawe-taylor	O
j	O
an	O
introduction	O
to	O
support	B
vector	I
machines	O
cambridge	O
p	O
university	O
press	O
cristianini	O
n	O
shawe-taylor	O
j	O
elisseeff	O
a	O
and	O
kandola	O
j	O
on	O
kernel-target	O
alignment	B
in	O
diettrich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
csat	O
o	O
l	O
gaussian	O
processes	O
iterative	O
sparse	O
approximations	O
phd	O
thesis	O
aston	O
university	O
p	O
uk	O
csat	O
o	O
l	O
and	O
opper	O
m	O
sparse	O
on-line	O
gaussian	O
processes	O
neural	O
computation	O
pp	O
csat	O
o	O
l	O
opper	O
m	O
and	O
winther	O
o	O
tap	O
gibbs	B
free	O
energy	O
belief	O
propagation	O
and	O
sparsity	O
in	O
diettrich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
pp	O
daley	O
r	O
atmospheric	O
data	O
analysis	O
cambridge	O
university	O
press	O
cambridge	O
uk	O
p	O
david	O
h	O
a	O
order	O
statistics	O
wiley	O
new	O
york	O
davis	O
p	O
j	O
circulant	O
matrices	O
wiley	O
new	O
york	O
dawid	O
a	O
p	O
properties	O
of	O
diagnostic	O
data	O
distributions	O
biometrics	O
pp	O
p	O
p	O
dellaportas	O
p	O
and	O
stephens	O
d	O
a	O
bayesian	O
analysis	O
of	O
errors-in-variables	B
regression	I
models	O
p	O
biometrics	O
devroye	O
l	O
gy	O
orfi	O
l	O
and	O
lugosi	O
g	O
a	O
probabilistic	B
theory	O
of	O
pattern	O
recognition	O
springer	O
pp	O
new	O
york	O
diaconis	O
p	O
and	O
freedman	O
d	O
on	O
the	O
consistency	B
of	O
bayes	O
estimates	O
annals	O
of	O
statistics	O
p	O
diggle	O
p	O
j	O
time	O
series	O
a	O
biostatistical	O
introduction	O
clarendon	O
press	O
oxford	O
pp	O
diggle	O
p	O
j	O
tawn	O
j	O
a	O
and	O
moyeed	O
r	O
a	O
model-based	O
geostatistics	B
discussion	O
p	O
applied	O
statistics	O
doob	O
j	O
l	O
the	O
elementary	O
gaussian	O
processes	O
annals	O
of	O
mathematical	O
statistics	O
p	O
doob	O
j	O
l	O
measure	B
theory	O
springer-verlag	O
new	O
york	O
p	O
drineas	O
p	O
and	O
mahoney	O
m	O
w	O
on	O
the	O
nystr	O
om	O
method	O
for	O
approximating	O
a	O
gram	B
matrix	I
for	O
improved	O
kernel-based	O
learning	B
technical	O
report	O
yale	O
university	O
p	O
httpcs-www	O
cs	O
yale	O
eduhomesmmahoney	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
duchon	O
j	O
splines	B
minimizing	O
rotation-invariant	O
semi-norms	O
in	O
sobolev	O
spaces	O
in	O
schempp	O
w	O
and	O
zeller	O
k	O
editors	O
constructive	O
theory	O
of	O
functions	O
of	O
several	O
variables	O
pages	O
springer-verlag	O
p	O
duda	O
r	O
o	O
and	O
hart	O
p	O
e	O
pattern	O
classification	B
and	O
scene	O
analysis	O
john	O
wiley	O
new	O
york	O
p	O
edgeworth	O
f	O
y	O
on	O
observations	O
relating	O
to	O
several	O
quantities	O
hermathena	O
p	O
faul	O
a	O
c	O
and	O
tipping	O
m	O
e	O
analysis	O
of	O
sparse	O
bayesian	O
learning	B
in	O
dietterich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
cambridge	O
massachussetts	O
mit	O
press	O
p	O
feldman	O
j	O
equivalence	O
and	O
perpendicularity	O
of	O
gaussian	O
processes	O
pacific	O
j	O
math	O
p	O
erratum	O
in	O
pacific	O
j	O
math	O
ferrari	O
trecate	O
g	O
williams	O
c	O
k	O
i	O
and	O
opper	O
m	O
finite-dimensional	O
approximation	O
of	O
gaussian	O
processes	O
in	O
kearns	O
m	O
s	O
solla	O
s	O
a	O
and	O
cohn	O
d	O
a	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
fine	O
s	O
and	O
scheinberg	O
k	O
efficient	O
svm	O
training	O
using	O
low-rank	O
kernel	B
representations	O
pp	O
journal	O
of	O
machine	O
learning	B
research	O
fowlkes	O
c	O
belongie	O
s	O
and	O
malik	O
j	O
efficient	O
spatiotemporal	O
grouping	O
using	O
the	O
nystr	O
om	O
method	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	O
cvpr	O
p	O
freedman	O
d	O
on	O
the	O
bernstein-von	O
mises	O
theorem	O
with	O
infinite-dimensional	O
parameters	O
p	O
annals	O
of	O
statistics	O
frieze	O
a	O
kannan	O
r	O
and	O
vempala	O
s	O
fast	O
monte-carlo	O
algorithms	O
for	O
finding	O
low-rank	O
approximations	O
in	O
conference	O
on	O
the	O
foundations	O
of	O
computer	O
science	O
pages	O
p	O
geisser	O
s	O
and	O
eddy	O
w	O
f	O
a	O
predictive	B
approach	O
to	O
model	O
selection	O
journal	O
of	O
the	O
americal	O
p	O
statistical	O
association	O
geman	O
s	O
and	O
geman	O
d	O
stochastic	O
relaxation	O
gibbs	B
distributions	O
and	O
the	O
bayesian	O
restorap	O
tion	O
of	O
images	O
ieee	O
trans	O
pattern	O
analysis	O
and	O
machine	O
intellligence	O
gibbs	B
m	O
n	O
bayesian	O
gaussian	O
processes	O
for	O
regression	O
and	O
classification	B
phd	O
thesis	O
p	O
department	O
of	O
physics	O
university	O
of	O
cambridge	O
gibbs	B
m	O
n	O
and	O
mackay	O
d	O
j	O
c	O
efficient	O
implementation	O
of	O
gaussian	O
processes	O
unpublished	O
manuscript	O
cavendish	O
laboratory	O
cambridge	O
uk	O
httpwww	O
inference	O
phy	O
cam	O
ac	O
uk	O
mackaybayesgp	O
html	O
p	O
gibbs	B
m	O
n	O
and	O
mackay	O
d	O
j	O
c	O
variational	O
gaussian	B
process	I
classifiers	O
ieee	O
transactions	O
p	O
on	O
neural	O
networks	O
gihman	O
i	O
i	O
and	O
skorohod	O
a	O
v	O
the	O
theory	O
of	O
stochastic	O
processes	O
volume	O
springer	O
p	O
verlag	O
berlin	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
girard	O
a	O
rasmussen	O
c	O
e	O
qui	O
nonero-candela	O
j	O
and	O
murray-smith	O
r	O
gaussian	B
process	I
priors	O
with	O
uncertain	B
inputs	I
application	O
to	O
multiple-step	O
ahead	O
time	O
series	O
forecasting	O
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
girosi	O
f	O
models	O
of	O
noise	O
and	O
robust	O
estimates	O
technical	O
report	O
ai	O
memo	O
mit	O
ai	O
p	O
laboratory	O
girosi	O
f	O
jones	O
m	O
and	O
poggio	O
t	O
regularization	B
theory	O
and	O
neural	O
networks	O
architectures	O
p	O
neural	O
computation	O
goldberg	O
p	O
w	O
williams	O
c	O
k	O
i	O
and	O
bishop	O
c	O
m	O
regression	O
with	O
input-dependent	O
noise	O
a	O
gaussian	B
process	I
treatment	O
in	O
jordan	O
m	O
i	O
kearns	O
m	O
j	O
and	O
solla	O
s	O
a	O
editors	O
advances	O
p	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
cambridge	O
ma	O
golub	O
g	O
h	O
and	O
van	O
loan	O
c	O
f	O
matrix	O
computations	O
johns	O
hopkins	O
university	O
press	O
pp	O
baltimore	O
second	O
edition	O
gradshteyn	O
i	O
s	O
and	O
ryzhik	O
i	O
m	O
tables	O
of	O
integrals	B
series	O
and	O
products	O
academic	O
press	O
pp	O
corrected	O
and	O
enlarged	O
edition	O
prepared	O
by	O
a	O
jeffrey	O
green	O
p	O
j	O
and	O
silverman	O
b	O
w	O
nonparametric	O
regression	O
and	O
generalized	B
linear	B
models	O
p	O
chapman	O
and	O
hall	O
london	O
grenander	O
u	O
chow	O
y	O
and	O
keenan	O
d	O
m	O
hands	O
a	O
pattern	O
theoretic	O
study	O
of	O
biological	O
pp	O
shapes	O
springer-verlag	O
new	O
york	O
grimmett	O
g	O
r	O
and	O
stirzaker	O
d	O
r	O
probability	O
and	O
random	O
processes	O
oxford	O
university	O
pp	O
press	O
oxford	O
england	O
second	O
edition	O
gr	O
unwald	O
p	O
d	O
and	O
langford	O
j	O
suboptimal	O
behaviour	O
of	O
bayes	O
and	O
mdl	O
in	O
classification	B
under	O
misspecification	O
in	O
proc	O
seventeenth	O
annual	O
conference	O
on	O
computational	O
learning	B
theory	O
p	O
gy	O
orfi	O
l	O
kohler	O
m	O
krzy	O
zak	O
a	O
and	O
walk	O
h	O
a	O
distribution-free	O
theory	O
of	O
nonparametric	O
p	O
regression	O
springer	O
new	O
york	O
hajek	O
j	O
on	O
a	O
property	O
of	O
normal	O
distributions	O
of	O
any	O
stochastic	O
process	O
russian	O
czechoslovak	O
math	O
j	O
translated	O
in	O
selected	O
trans	O
math	O
statist	O
probab	O
also	O
available	O
in	O
collected	O
works	O
of	O
jaroslav	O
hajek	O
eds	O
m	O
hu	O
skov	O
a	O
r	O
beran	O
v	O
dupa	O
c	O
wiley	O
p	O
hand	O
d	O
j	O
mannila	O
h	O
and	O
smyth	O
p	O
principles	O
of	O
data	O
mining	O
mit	O
press	O
hannan	O
e	O
j	O
multiple	O
time	O
series	O
wiley	O
new	O
york	O
p	O
p	O
hansen	O
l	O
k	O
liisberg	O
c	O
and	O
salamon	O
p	O
the	O
error-reject	O
tradeoff	O
open	O
sys	O
information	O
p	O
dyn	O
hastie	O
t	O
j	O
and	O
tibshirani	O
r	O
j	O
generalized	B
additive	O
models	O
chapman	O
and	O
hall	O
pp	O
haussler	O
d	O
convolution	O
kernels	O
on	O
discrete	O
structures	O
technical	O
report	O
p	O
dept	O
of	O
computer	O
science	O
university	O
of	O
california	O
at	O
santa	O
cruz	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
hawkins	O
d	O
l	O
some	O
practical	O
problems	O
in	O
implementing	O
a	O
certain	O
sieve	O
estimator	O
of	O
the	O
gaussian	O
mean	B
function	B
communications	O
in	O
statistics	O
simulation	O
and	O
computation	O
p	O
hoerl	O
a	O
e	O
and	O
kennard	O
r	O
w	O
ridge	B
regression	O
biased	O
estimation	O
for	O
nonorthogonal	O
p	O
problems	O
technometrics	O
hornik	O
k	O
some	O
new	O
results	O
on	O
neural	B
network	I
approximation	O
neural	O
networks	O
p	O
ihara	O
s	O
information	O
theory	O
for	O
continuous	O
systems	O
world	O
scientific	O
singapore	O
p	O
jaakkola	O
t	O
s	O
diekhans	O
m	O
and	O
haussler	O
d	O
a	O
discriminative	O
framework	O
for	O
detecting	O
pp	O
remote	O
protein	O
homologies	O
journal	O
of	O
computational	O
biology	O
jaakkola	O
t	O
s	O
and	O
haussler	O
d	O
probabilistic	B
kernel	B
regression	O
models	O
in	O
heckerman	O
d	O
and	O
whittaker	O
j	O
editors	O
workshop	O
on	O
artificial	O
intelligence	O
and	O
statistics	O
morgan	O
kaufmann	O
p	O
jacobs	O
r	O
a	O
jordan	O
m	O
i	O
nowlan	O
s	O
j	O
and	O
hinton	O
g	O
e	O
adaptive	O
mixtures	O
of	O
local	O
p	O
experts	O
neural	O
computation	O
johnson	O
n	O
l	O
kotz	O
s	O
and	O
balakrishnan	O
n	O
continuous	O
univariate	O
distributions	O
volume	O
p	O
john	O
wiley	O
and	O
sons	O
new	O
york	O
second	O
edition	O
jones	O
d	O
r	O
a	O
taxonomy	O
of	O
global	O
optimization	O
methods	O
based	O
on	O
response	O
surfaces	O
j	O
p	O
global	O
optimization	O
jordan	O
m	O
i	O
an	O
introduction	O
to	O
probabilistic	B
graphical	O
models	O
draft	O
book	O
journel	O
a	O
g	O
and	O
huijbregts	O
c	O
j	O
mining	O
geostatistics	B
academic	O
press	O
p	O
p	O
kailath	O
t	O
rkhs	O
approach	O
to	O
detection	O
and	O
estimation	O
problems	O
part	O
i	O
deterministic	O
p	O
signals	O
in	O
gaussian	O
noise	O
ieee	O
trans	O
information	O
theory	O
kammler	O
d	O
w	O
a	O
first	O
course	O
in	O
fourier	O
analysis	O
prentice-hall	O
upper	O
saddle	O
river	O
nj	O
p	O
kashyap	O
r	O
l	O
and	O
chellappa	O
r	O
stochastic	O
models	O
for	O
closed	O
boundary	O
analysis	O
represenp	O
tation	O
and	O
reconstruction	O
ieee	O
trans	O
on	O
information	O
theory	O
keeling	O
c	O
d	O
and	O
whorf	O
t	O
p	O
atmospheric	O
records	O
from	O
sites	O
in	O
the	O
sio	O
air	O
sampling	O
network	O
in	O
trends	O
a	O
compendium	O
of	O
data	O
on	O
global	O
change	O
carbon	O
dioxide	O
information	O
analysis	O
p	O
center	O
oak	O
ridge	B
national	O
laboratory	O
oak	O
ridge	B
tenn	O
u	O
s	O
a	O
kent	O
j	O
t	O
and	O
mardia	O
k	O
v	O
the	O
link	O
between	O
kriging	B
and	O
thin-plate	O
splines	B
in	O
kelly	O
p	O
f	O
p	O
editor	O
probability	O
statsitics	O
and	O
optimization	O
pages	O
wiley	O
kimeldorf	O
g	O
and	O
wahba	O
g	O
a	O
correspondence	O
between	O
bayesian	O
estimation	O
of	O
stochastic	O
p	O
processes	O
and	O
smoothing	O
by	O
splines	B
annals	O
of	O
mathematical	O
statistics	O
kimeldorf	O
g	O
and	O
wahba	O
g	O
some	O
results	O
on	O
tchebycheffian	O
spline	O
functions	O
j	O
mathematical	O
p	O
analysis	O
and	O
applications	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
kohn	O
r	O
and	O
ansley	O
c	O
f	O
a	O
new	O
algorithm	O
for	O
spline	O
smoothing	O
based	O
on	O
smoothing	O
a	O
p	O
stochastic	O
process	O
siam	O
j	O
sci	O
stat	O
comput	O
kolmogorov	O
a	O
n	O
interpolation	O
und	O
extrapolation	O
von	O
station	O
aren	O
zuf	O
aligen	O
folgen	O
izv	O
akad	O
p	O
nauk	O
sssr	O
k	O
onig	O
h	O
eigenvalue	B
distribution	O
of	O
compact	O
operators	O
birkh	O
auser	O
kullback	O
s	O
information	O
theory	O
and	O
statistics	O
dover	O
new	O
york	O
p	O
pp	O
kuss	O
m	O
and	O
rasmussen	O
c	O
e	O
assessing	O
approximations	O
for	O
gaussian	B
process	I
classification	B
in	O
weiss	O
y	O
sch	O
olkopf	O
b	O
and	O
platt	O
j	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
pp	O
lanckriet	O
g	O
r	O
g	O
cristianini	O
n	O
bartlett	O
p	O
l	O
el	O
ghaoui	O
l	O
and	O
jordan	O
m	O
i	O
learning	B
the	O
kernel	B
matrix	O
with	O
semidefinite	O
programming	O
journal	O
of	O
machine	O
learning	B
research	O
p	O
lauritzen	O
s	O
l	O
time	O
series	O
analysis	O
in	O
a	O
discussion	O
of	O
contributions	O
made	O
by	O
p	O
t	O
n	O
thiele	O
international	O
statistical	O
review	O
lawrence	O
n	O
gaussian	B
process	I
latent	O
variable	O
models	O
for	O
visualization	O
of	O
high	O
dimensional	O
data	O
in	O
thrun	O
s	O
saul	O
l	O
and	O
sch	O
olkopf	O
b	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
p	O
systems	O
pages	O
mit	O
press	O
lawrence	O
n	O
seeger	B
m	O
and	O
herbrich	O
r	O
fast	O
sparse	O
gaussian	B
process	I
methods	O
the	O
informative	B
vector	I
machine	I
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
pp	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
leslie	O
c	O
eskin	O
e	O
weston	O
j	O
and	O
stafford	O
noble	O
w	O
mismatch	O
string	B
kernels	O
for	O
svm	O
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
pp	O
protein	O
classification	B
information	O
processing	O
systems	O
mit	O
press	O
lin	O
x	O
wahba	O
g	O
xiang	O
d	O
gao	O
f	O
klein	O
r	O
and	O
klein	O
b	O
smoothing	O
spline	O
anova	B
models	O
for	O
large	O
data	O
sets	O
with	O
bernoulli	O
observations	O
and	O
the	O
randomized	O
gacv	O
annals	O
of	O
statistics	O
p	O
lindley	O
d	O
v	O
making	O
decisions	O
john	O
wiley	O
and	O
sons	O
london	O
uk	O
second	O
edition	O
p	O
lodhi	O
h	O
shawe-taylor	O
j	O
cristianini	O
n	O
and	O
watkins	O
c	O
j	O
c	O
h	O
text	O
classification	B
using	O
string	B
kernels	O
in	O
leen	O
t	O
k	O
diettrich	O
t	O
g	O
and	O
tresp	O
v	O
editors	O
advances	O
in	O
neural	O
information	O
p	O
processing	O
systems	O
mit	O
press	O
luo	O
z	O
and	O
wahba	O
g	O
hybrid	O
adaptive	O
splines	B
j	O
amer	O
statist	O
assoc	O
p	O
mackay	O
d	O
j	O
c	O
a	O
practical	O
bayesian	O
framework	O
for	O
backpropagation	O
networks	O
neural	O
pp	O
computation	O
mackay	O
d	O
j	O
c	O
bayesian	O
interpolation	O
neural	O
computation	O
pp	O
xiii	O
xvi	O
mackay	O
d	O
j	O
c	O
information-based	O
objective	O
functions	O
for	O
active	O
data	O
selection	O
neural	O
p	O
computation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
mackay	O
d	O
j	O
c	O
the	O
evidence	O
framework	O
applied	O
to	O
classification	B
networks	O
neural	O
comp	O
putation	O
mackay	O
d	O
j	O
c	O
introduction	O
to	O
gaussian	O
processes	O
in	O
bishop	O
c	O
m	O
editor	O
neural	O
networks	O
pp	O
and	O
machine	O
learning	B
springer-verlag	O
mackay	O
d	O
j	O
c	O
comparison	O
of	O
approximate	O
methods	O
for	O
handling	O
hyperparameters	B
neural	O
p	O
computation	O
mackay	O
d	O
j	O
c	O
information	O
theory	O
inference	O
and	O
learning	B
algorithms	O
cambridge	O
univerpp	O
xiv	O
sity	O
press	O
cambridge	O
uk	O
malzahn	O
d	O
and	O
opper	O
m	O
a	O
variational	O
approach	O
to	O
learning	B
curves	O
in	O
diettrich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
mandelbrot	O
b	O
b	O
the	O
fractal	B
geometry	O
of	O
nature	O
w	O
h	O
freeman	O
san	O
francisco	O
p	O
mardia	O
k	O
v	O
and	O
marshall	O
r	O
j	O
maximum	O
likelihood	B
estimation	O
for	O
models	O
of	O
residual	O
p	O
covariance	B
in	O
spatial	O
regression	O
biometrika	O
mat	O
ern	O
b	O
spatial	O
variation	O
meddelanden	O
fr	O
an	O
statens	O
skogsforskningsinstitut	O
pp	O
alm	O
anna	O
f	O
orlaget	O
stockholm	O
second	O
edition	O
springer-verlag	O
berlin	O
matheron	O
g	O
the	O
intrinsic	O
random	O
functions	O
and	O
their	O
applications	O
advances	O
in	O
applied	O
p	O
probability	O
maxwell	O
j	O
c	O
letter	O
to	O
lewis	O
campbell	O
reproduced	O
in	O
l	O
campbell	O
and	O
w	O
garrett	O
the	O
life	O
p	O
v	O
of	O
james	O
clerk	O
maxwell	O
macmillan	O
mcallester	B
d	O
pac-bayesian	O
stochastic	O
model	O
selection	O
machine	O
learning	B
p	O
mccullagh	O
p	O
and	O
nelder	O
j	O
generalized	B
linear	B
models	O
chapman	O
and	O
hall	O
pp	O
meinguet	O
j	O
multivariate	O
interpolation	O
at	O
arbitrary	O
points	O
made	O
simple	O
journal	O
of	O
applied	O
p	O
mathematics	O
and	O
physics	O
meir	O
r	O
and	O
zhang	O
t	O
generalization	B
error	I
bounds	O
for	O
bayesian	O
mixture	O
algorithms	O
journal	O
p	O
of	O
machine	O
learning	B
research	O
micchelli	O
c	O
a	O
and	O
pontil	O
m	O
kernels	O
for	O
multi-task	B
learning	B
in	O
saul	O
l	O
k	O
weiss	O
y	O
and	O
bottou	O
l	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
p	O
press	O
micchelli	O
c	O
a	O
and	O
wahba	O
g	O
design	O
problems	O
for	O
optimal	B
surface	O
interpolation	O
in	O
ziegler	O
p	O
z	O
editor	O
approximation	O
theory	O
and	O
applications	O
pages	O
academic	O
press	O
minka	O
t	O
p	O
a	O
family	O
of	O
algorithms	O
for	O
approximate	O
bayesian	O
inference	O
phd	O
thesis	O
maspp	O
sachusetts	O
institute	O
of	O
technology	O
minka	O
t	O
p	O
a	O
comparison	O
of	O
numerical	O
optimizers	O
for	O
logistic	B
regression	I
httpresearch	O
microsoft	O
com	O
minkapaperslogreg	O
p	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
minka	O
t	O
p	O
and	O
picard	O
r	O
w	O
learning	B
how	O
to	O
learn	O
is	O
learning	B
with	O
point	O
sets	O
httpresearch	O
microsoft	O
com	O
minkapaperspoint-sets	O
html	O
mitchell	O
t	O
m	O
machine	O
learning	B
mcgraw-hill	O
new	O
york	O
p	O
pp	O
murray-smith	O
r	O
and	O
girard	O
a	O
gaussian	B
process	I
priors	O
with	O
arma	O
noise	O
models	O
in	O
irish	O
signals	O
and	O
systems	O
conference	O
pages	O
maynooth	O
httpwww	O
dcs	O
gla	O
ac	O
uk	O
rod	O
p	O
neal	O
r	O
m	O
bayesian	O
learning	B
for	O
neural	O
networks	O
springer	O
new	O
york	O
lecture	O
notes	O
in	O
pp	O
xiii	O
statistics	O
neal	O
r	O
m	O
monte	O
carlo	O
implementation	O
of	O
gaussian	B
process	I
models	O
for	O
bayesian	O
regression	O
and	O
classification	B
technical	O
report	O
department	O
of	O
statistics	O
university	O
of	O
toronto	O
httpwww	O
cs	O
toronto	O
edu	O
radford	O
p	O
neal	O
r	O
m	O
regression	O
and	O
classification	B
using	O
gaussian	B
process	I
priors	O
in	O
bernardo	O
j	O
m	O
berger	O
j	O
o	O
dawid	O
a	O
p	O
and	O
smith	O
a	O
f	O
m	O
editors	O
bayesian	O
statistics	O
pages	O
oxford	O
pp	O
university	O
press	O
discussion	O
neal	O
r	O
m	O
annealed	O
importance	O
sampling	O
statistics	O
and	O
computing	O
p	O
neumaier	O
a	O
introduction	O
to	O
global	O
optimization	O
httpwww	O
mat	O
univie	O
ac	O
at	O
neum	O
p	O
gloptintro	O
html	O
o	O
hagan	O
a	O
curve	O
fitting	O
and	O
optimal	B
design	O
for	O
prediction	O
journal	O
of	O
the	O
royal	O
statistical	O
pp	O
society	O
b	O
discussion	O
o	O
hagan	O
a	O
bayes-hermite	O
quadrature	O
journal	O
of	O
statistical	O
planning	O
and	O
inference	O
pp	O
o	O
hagan	O
a	O
kennedy	O
m	O
c	O
and	O
oakley	O
j	O
e	O
uncertainty	O
analysis	O
and	O
other	O
inference	O
tools	O
for	O
complex	O
computer	O
codes	O
in	O
bernardo	O
j	O
m	O
berger	O
j	O
o	O
dawid	O
a	O
p	O
and	O
smith	O
a	O
f	O
m	O
editors	O
bayesian	O
statistics	O
pages	O
oxford	O
university	O
press	O
discussion	O
p	O
ksendal	O
b	O
stochastic	O
differential	O
equations	O
springer-verlag	O
berlin	O
p	O
opper	O
m	O
and	O
vivarelli	O
f	O
general	O
bounds	O
on	O
bayes	O
errors	O
for	O
regression	O
with	O
gaussian	O
processes	O
in	O
kearns	O
m	O
s	O
solla	O
s	O
a	O
and	O
cohn	O
d	O
a	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
opper	O
m	O
and	O
winther	O
o	O
gaussian	O
processes	O
for	O
classification	B
mean-field	O
algorithms	O
pp	O
neural	O
computation	O
o	O
sullivan	O
f	O
yandell	O
b	O
s	O
and	O
raynor	O
w	O
j	O
automatic	O
smoothing	O
of	O
regression	O
functions	O
in	O
generalized	B
linear	B
models	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
pp	O
paciorek	O
c	O
and	O
schervish	O
m	O
j	O
nonstationary	O
covariance	B
functions	O
for	O
gaussian	B
process	I
regression	I
in	O
thrun	O
s	O
saul	O
l	O
and	O
sch	O
olkopf	O
b	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
pp	O
systems	O
mit	O
press	O
papoulis	O
a	O
probability	O
random	O
variables	O
and	O
stochastic	O
processes	O
mcgraw-hill	O
new	O
york	O
pp	O
third	O
edition	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
plaskota	O
l	O
noisy	O
information	O
and	O
computational	O
complexity	O
cambridge	O
university	O
press	O
pp	O
cambridge	O
plate	O
t	O
a	O
accuarcy	O
versus	O
interpretability	O
in	O
flexible	O
modeling	O
implementing	O
a	O
tradeoff	O
p	O
using	O
gaussian	B
process	I
models	O
behaviourmetrika	O
platt	O
j	O
c	O
fast	O
training	O
of	O
support	B
vector	I
machines	O
using	O
sequential	O
minimal	O
optimization	O
in	O
sch	O
olkopf	O
b	O
burges	O
c	O
j	O
c	O
and	O
smola	O
a	O
j	O
editors	O
advances	O
in	O
kernel	B
methods	O
pages	O
p	O
mit	O
press	O
platt	O
j	O
c	O
probabilities	O
for	O
sv	O
machines	O
in	O
smola	O
a	O
bartlett	O
p	O
sch	O
olkopf	O
b	O
and	O
schuurmans	O
d	O
editors	O
advances	O
in	O
large	O
margin	O
classifiers	O
pages	O
mit	O
press	O
pp	O
poggio	O
t	O
and	O
girosi	O
f	O
networks	O
for	O
approximation	O
and	O
learning	B
proceedings	O
of	O
ieee	O
pp	O
poggio	O
t	O
voorhees	O
h	O
and	O
yuille	O
a	O
a	O
regularized	O
solution	O
to	O
edge	O
detection	O
technical	O
p	O
report	O
ai	O
memo	O
mit	O
ai	O
laboratory	O
pontil	O
m	O
mukherjee	O
s	O
and	O
girosi	O
f	O
on	O
the	O
noise	B
model	I
of	O
support	B
vector	I
machine	I
p	O
regression	O
technical	O
report	O
ai	O
memo	O
mit	O
ai	O
laboratory	O
press	O
w	O
h	O
teukolsky	O
s	O
a	O
vetterling	O
w	O
t	O
and	O
flannery	O
b	O
p	O
numerical	O
recipes	O
in	O
c	O
pp	O
cambridge	O
university	O
press	O
second	O
edition	O
qui	O
nonero-candela	O
j	O
learning	B
with	O
uncertainty	O
gaussian	O
processes	O
and	O
relevance	O
vector	O
machines	O
phd	O
thesis	O
informatics	O
and	O
mathematical	O
modelling	O
technical	O
univeristy	O
of	O
denmark	O
p	O
rasmussen	O
c	O
e	O
evaluation	O
of	O
gaussian	O
processes	O
and	O
other	O
methods	O
for	O
non-linear	O
regression	O
phd	O
thesis	O
dept	O
of	O
computer	O
science	O
university	O
of	O
toronto	O
httpwww	O
kyb	O
mpg	O
de	O
p	O
rasmussen	O
c	O
e	O
gaussian	O
processes	O
to	O
speed	O
up	O
hybrid	O
monte	O
carlo	O
for	O
expensive	O
bayesian	O
integrals	B
in	O
bernardo	O
j	O
m	O
bayarri	O
m	O
j	O
berger	O
j	O
o	O
dawid	O
a	O
p	O
heckerman	O
d	O
smith	O
a	O
f	O
m	O
and	O
west	O
m	O
editors	O
bayesian	O
statistics	O
pages	O
oxford	O
university	O
press	O
p	O
rasmussen	O
c	O
e	O
and	O
ghahramani	O
z	O
occam	O
s	O
razor	O
in	O
leen	O
t	O
dietterich	O
t	O
g	O
and	O
tresp	O
v	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
rasmussen	O
c	O
e	O
and	O
ghahramani	O
z	O
in	O
diettrich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
infinite	O
mixtures	O
of	O
gaussian	B
process	I
experts	O
rasmussen	O
c	O
e	O
and	O
ghahramani	O
z	O
bayesian	O
monte	O
carlo	O
in	O
suzanna	O
becker	O
s	O
t	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
rasmussen	O
c	O
e	O
and	O
qui	O
nonero-candela	O
j	O
healing	O
the	O
relevance	B
vector	I
machine	I
through	O
pp	O
augmentation	O
in	O
proc	O
international	O
conference	O
on	O
machine	O
learning	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
rifkin	O
r	O
and	O
klautau	O
a	O
in	O
defense	O
of	O
one-vs-all	O
classification	B
journal	O
of	O
machine	O
learning	B
pp	O
research	O
ripley	O
b	O
spatial	O
statistics	O
wiley	O
new	O
york	O
p	O
ripley	O
b	O
pattern	O
recognition	O
and	O
neural	O
networks	O
cambridge	O
university	O
press	O
cambridge	O
p	O
uk	O
ritter	O
k	O
average-case	O
analysis	O
of	O
numerical	O
problems	O
springer	O
verlag	O
pp	O
ritter	O
k	O
wasilkowski	O
g	O
w	O
and	O
wo	O
zniakowski	O
h	O
multivariate	O
integration	O
and	O
approximation	O
of	O
random	O
fields	O
satisfying	O
sacks-ylvisaker	O
conditions	O
annals	O
of	O
applied	O
probability	O
p	O
rousseeuw	O
p	O
j	O
least	O
median	O
of	O
squares	O
regression	O
journal	O
of	O
the	O
american	O
statistical	O
p	O
association	O
sacks	O
j	O
welch	O
w	O
j	O
mitchell	O
t	O
j	O
and	O
wynn	O
h	O
p	O
design	O
and	O
analysis	O
of	O
computer	O
pp	O
experiments	O
statistical	O
science	O
saitoh	O
s	O
theory	O
of	O
reproducing	O
kernels	O
and	O
its	O
applications	O
longman	O
harlow	O
england	O
p	O
salton	O
g	O
and	O
buckley	O
c	O
term-weighting	O
approaches	O
in	O
automatic	O
text	O
retrieval	O
information	O
p	O
processing	O
and	O
management	O
sampson	O
p	O
d	O
and	O
guttorp	O
p	O
nonparametric	O
estimation	O
of	O
nonstationary	O
covariance	B
strucp	O
ture	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
santner	O
t	O
j	O
williams	O
b	O
j	O
and	O
notz	O
w	O
the	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
p	O
springer	O
new	O
york	O
saunders	O
c	O
gammerman	O
a	O
and	O
vovk	O
v	O
ridge	B
regression	O
learning	B
algorithm	O
in	O
dual	O
variables	O
in	O
shavlik	O
j	O
editor	O
proceedings	O
of	O
the	O
fifteenth	O
international	O
conference	O
on	O
machine	O
learning	B
morgan	O
kaufmann	O
p	O
saunders	O
c	O
shawe-taylor	O
j	O
and	O
vinokourov	O
a	O
string	B
kernels	O
fisher	B
kernels	O
and	O
finite	O
state	O
automata	O
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
schoenberg	O
i	O
j	O
metric	O
spaces	O
and	O
positive	B
definite	I
functions	O
trans	O
american	O
mathematical	O
p	O
society	O
schoenberg	O
i	O
j	O
spline	O
functions	O
and	O
the	O
problem	O
of	O
graduation	O
proc	O
nat	O
acad	O
sci	O
usa	O
pp	O
sch	O
olkopf	O
b	O
and	O
smola	O
a	O
j	O
learning	B
with	O
kernels	O
mit	O
press	O
pp	O
xvi	O
sch	O
olkopf	O
b	O
smola	O
a	O
j	O
and	O
m	O
uller	O
k	O
-r	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	B
p	O
eigenvalue	B
problem	O
neural	O
computation	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
schwaighofer	O
a	O
and	O
tresp	O
v	O
transductive	O
and	O
inductive	B
methods	O
for	O
approximate	O
gaussian	B
process	I
regression	I
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
pp	O
scott	O
d	O
w	O
multivariate	O
density	O
estimation	O
wiley	O
new	O
york	O
p	O
seeger	B
m	O
bayesian	O
model	O
selection	O
for	O
support	B
vector	I
machines	O
gaussian	O
processes	O
and	O
other	O
kernel	B
classifiers	O
in	O
solla	O
s	O
a	O
leen	O
t	O
k	O
and	O
m	O
uller	O
k	O
-r	O
editors	O
advances	O
in	O
neural	O
pp	O
information	O
processing	O
systems	O
mit	O
press	O
cambridge	O
ma	O
seeger	B
m	O
pac-bayesian	O
generalisation	O
error	O
bounds	O
for	O
gaussian	B
process	I
classification	B
pp	O
journal	O
of	O
machine	O
learning	B
research	O
seeger	B
m	O
bayesian	O
gaussian	B
process	I
models	O
pac-bayesian	O
generalisation	O
error	O
bounds	O
and	O
sparse	O
approximations	O
phd	O
thesis	O
school	O
of	O
informatics	O
university	O
of	O
edinburgh	O
httpwww	O
cs	O
berkeley	O
edu	O
mseeger	O
pp	O
seeger	B
m	O
expectation	B
propagation	I
for	O
exponential	B
families	O
httpwww	O
cs	O
berkeley	O
edu	O
p	O
mseegerpapersepexpfam	O
ps	O
gz	O
seeger	B
m	O
and	O
jordan	O
m	O
i	O
sparse	O
gaussian	B
process	I
classification	B
with	O
multiple	O
classes	O
p	O
technical	O
report	O
tr	O
department	O
of	O
statistics	O
university	O
of	O
california	O
at	O
berkeley	O
seeger	B
m	O
williams	O
c	O
k	O
i	O
and	O
lawrence	O
n	O
fast	O
forward	O
selection	O
to	O
speed	O
up	O
sparse	O
gaussian	B
process	I
regression	I
in	O
bishop	O
c	O
and	O
frey	O
b	O
j	O
editors	O
proceedings	O
of	O
the	O
ninth	O
international	O
workshop	O
on	O
artificial	O
intelligence	O
and	O
statistics	O
society	O
for	O
artificial	O
intelligence	O
and	O
statistics	O
pp	O
shawe-taylor	O
j	O
and	O
williams	O
c	O
k	O
i	O
the	O
stability	O
of	O
kernel	B
principal	O
components	O
analysis	O
and	O
its	O
relation	O
to	O
the	O
process	O
eigenspectrum	O
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
shepp	O
l	O
a	O
radon-nikodym	O
derivatives	O
of	O
gaussian	O
measures	O
annals	O
of	O
mathematical	O
statisp	O
tics	O
silverman	O
b	O
w	O
density	O
ratios	O
empirical	O
likelihood	B
and	O
cot	O
death	O
applied	O
statistics	O
p	O
silverman	O
b	O
w	O
spline	O
smoothing	O
the	O
equivalent	B
variable	O
kernel	B
method	O
annals	O
of	O
pp	O
statistics	O
silverman	O
b	O
w	O
some	O
aspects	O
of	O
the	O
spline	O
smoothing	O
approach	O
to	O
non-parametric	B
regression	O
pp	O
curve	O
fitting	O
discussion	O
j	O
roy	O
stat	O
soc	O
b	O
simard	O
p	O
victorri	O
b	O
le	O
cun	O
y	O
and	O
denker	O
j	O
tangent	O
prop	O
a	O
formalism	O
for	O
specifying	O
selected	O
invariances	B
in	O
an	O
adaptive	O
network	O
in	O
moody	O
j	O
e	O
hanson	O
s	O
j	O
and	O
lippmann	O
r	O
p	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
morgan	O
kaufmann	O
pp	O
smola	O
a	O
j	O
and	O
bartlett	O
p	O
l	O
sparse	O
greedy	O
gaussian	B
process	I
regression	I
in	O
leen	O
t	O
k	O
diettrich	O
t	O
g	O
and	O
tresp	O
v	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
p	O
mit	O
press	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
smola	O
a	O
j	O
and	O
sch	O
olkopf	O
b	O
sparse	O
greedy	O
matrix	O
approximation	O
for	O
machine	O
learning	B
in	O
proceedings	O
of	O
the	O
seventeenth	O
international	O
conference	O
on	O
machine	O
learning	B
morgan	O
kaufmann	O
pp	O
solak	O
e	O
murray-smith	O
r	O
leithead	O
w	O
e	O
leith	O
d	O
and	O
rasmussen	O
c	O
e	O
derivative	B
observations	I
in	O
gaussian	B
process	I
models	O
of	O
dynamic	O
systems	O
in	O
becker	O
s	O
s	O
t	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
sollich	O
p	O
learning	B
curves	O
for	O
gaussian	O
processes	O
in	O
kearns	O
m	O
s	O
solla	O
s	O
a	O
and	O
cohn	O
pp	O
d	O
a	O
editors	O
neural	O
information	O
processing	O
systems	O
vol	O
mit	O
press	O
sollich	O
p	O
bayesian	O
methods	O
for	O
support	B
vector	I
machines	O
evidence	O
and	O
predictive	B
class	O
pp	O
probabilities	O
machine	O
learning	B
sollich	O
p	O
and	O
williams	O
c	O
k	O
i	O
using	O
the	O
equivalent	B
kernel	B
to	O
understand	O
gaussian	B
process	I
in	O
saul	O
l	O
k	O
weiss	O
y	O
and	O
bottou	O
l	O
editors	O
advances	O
in	O
neural	O
information	O
p	O
regression	O
processing	O
systems	O
mit	O
press	O
stein	O
m	O
l	O
a	O
kernel	B
approximation	O
to	O
the	O
kriging	B
predictor	O
of	O
a	O
spatial	O
process	O
ann	O
inst	O
p	O
statist	O
math	O
stein	O
m	O
l	O
interpolation	O
of	O
spatial	O
data	O
springer-verlag	O
new	O
york	O
pp	O
steinwart	O
i	O
consistency	B
of	O
support	B
vector	I
machines	O
and	O
other	O
regularized	O
kernel	B
classifiers	O
p	O
ieee	O
trans	O
on	O
information	O
theory	O
stitson	O
m	O
o	O
gammerman	O
a	O
vapnik	O
v	O
n	O
vovk	O
v	O
watkins	O
c	O
j	O
c	O
h	O
and	O
weston	O
j	O
support	B
vector	I
regression	I
with	O
anova	B
decomposition	O
kernels	O
in	O
sch	O
olkopf	O
b	O
burges	O
c	O
j	O
c	O
and	O
smola	O
a	O
j	O
editors	O
advances	O
in	O
kernel	B
methods	O
mit	O
press	O
p	O
sundararajan	O
s	O
and	O
keerthi	O
s	O
s	O
predictive	B
approaches	O
for	O
choosing	O
hyperparameters	B
in	O
p	O
gaussian	O
processes	O
neural	O
computation	O
suykens	O
j	O
a	O
k	O
and	O
vanderwalle	O
j	O
least	O
squares	O
support	B
vector	I
machines	O
neural	O
processing	O
p	O
letters	O
szeliski	O
r	O
regularization	B
uses	O
fractal	B
priors	O
in	O
proceedings	O
of	O
the	O
national	O
conference	O
pp	O
on	O
artificial	O
intelligence	O
teh	O
y	O
w	O
seeger	B
m	O
and	O
jordan	O
m	O
i	O
semiparametric	O
latent	O
factor	O
models	O
in	O
cowell	O
r	O
g	O
and	O
ghahramani	O
z	O
editors	O
proceedings	O
of	O
tenth	O
international	O
workshop	O
on	O
artificial	O
inte	O
lligence	O
and	O
statistics	O
pages	O
society	O
for	O
artificial	O
intelligence	O
and	O
statistics	O
p	O
thomas-agnan	O
c	O
computing	O
a	O
family	O
of	O
reproducing	O
kernels	O
for	O
statistical	O
applications	O
p	O
numerical	O
algorithms	O
thompson	O
p	O
d	O
optimum	O
smoothing	O
of	O
two-dimensional	O
fields	O
tellus	O
p	O
tikhonov	O
a	O
n	O
solution	O
of	O
incorrectly	O
formulated	O
problems	O
and	O
the	O
regularization	B
method	O
p	O
soviet	O
math	O
dokl	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
tikhonov	O
a	O
n	O
and	O
arsenin	O
v	O
y	O
solutions	O
of	O
ill-posed	O
problems	O
w	O
h	O
winston	O
washington	O
p	O
d	O
c	O
tipping	O
m	O
e	O
sparse	O
bayesian	O
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
journal	O
of	O
machine	O
p	O
learning	B
research	O
tipping	O
m	O
e	O
and	O
faul	O
a	O
c	O
fast	O
marginal	B
likelihood	B
maximisation	O
for	O
sparse	O
bayesian	O
models	O
in	O
bishop	O
c	O
m	O
and	O
frey	O
b	O
j	O
editors	O
proceedings	O
of	O
ninth	O
international	O
workshop	O
on	O
p	O
artificial	O
intelligence	O
and	O
statistics	O
society	O
for	O
artificial	O
intelligence	O
and	O
statistics	O
tresp	O
v	O
a	O
bayesian	B
committee	I
machine	I
neural	O
computation	O
pp	O
tsuda	O
k	O
kawanabe	O
m	O
r	O
atsch	O
g	O
sonnenburg	O
s	O
and	O
m	O
uller	O
k	O
-r	O
a	O
new	O
discriminative	O
p	O
kernel	B
from	O
probabilistic	B
models	O
neural	O
computation	O
uhlenbeck	O
g	O
e	O
and	O
ornstein	O
l	O
s	O
on	O
the	O
theory	O
of	O
brownian	O
motion	O
phys	O
rev	O
pp	O
valiant	O
l	O
g	O
a	O
theory	O
of	O
the	O
learnable	O
communications	O
of	O
the	O
acm	O
p	O
vapnik	O
v	O
n	O
the	O
nature	O
of	O
statistical	O
learning	B
theory	O
springer	O
verlag	O
new	O
york	O
pp	O
vapnik	O
v	O
n	O
statistical	O
learning	B
theory	O
john	O
wiley	O
and	O
sons	O
p	O
vijayakumar	O
s	O
d	O
souza	O
a	O
and	O
schaal	O
s	O
incremental	O
online	O
learning	B
in	O
high	O
dimensions	O
pp	O
accepted	O
for	O
publication	O
in	O
neural	O
computation	O
vijayakumar	O
s	O
d	O
souza	O
a	O
shibata	O
t	O
conradt	O
j	O
and	O
schaal	O
s	O
statistical	O
learning	B
for	O
p	O
humanoid	O
robots	O
autonomous	O
robot	B
vijayakumar	O
s	O
and	O
schaal	O
s	O
lwpr	O
an	O
on	O
algorithm	O
for	O
incremental	O
real	O
time	O
learning	B
in	O
high	O
dimensional	O
space	O
in	O
proc	O
of	O
the	O
seventeenth	O
international	O
conference	O
on	O
machine	O
learning	B
pages	O
p	O
vishwanathan	O
s	O
v	O
n	O
and	O
smola	O
a	O
j	O
fast	O
kernels	O
for	O
string	B
and	O
tree	O
matching	O
in	O
becker	O
s	O
thrun	O
s	O
and	O
obermayer	O
k	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
vivarelli	O
f	O
and	O
williams	O
c	O
k	O
i	O
discovering	O
hidden	O
features	O
with	O
gaussian	O
processes	O
regression	O
in	O
kearns	O
m	O
s	O
solla	O
s	O
a	O
and	O
cohn	O
d	O
a	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
von	O
mises	O
r	O
mathematical	O
theory	O
of	O
probability	O
and	O
statistics	O
academic	O
press	O
p	O
wahba	O
g	O
improper	O
priors	O
spline	O
smoothing	O
and	O
the	O
problem	O
of	O
guarding	O
against	O
model	O
p	O
errors	O
in	O
regression	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
wahba	O
g	O
a	O
comparison	O
of	O
gcv	O
and	O
gml	O
for	O
choosing	O
the	O
smoothing	O
parameter	O
in	O
the	O
p	O
generalized	B
spline	O
smoothing	O
problem	O
annals	O
of	O
statistics	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
wahba	O
g	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
philadelphia	O
pa	O
cbms-nsf	O
regional	O
conference	O
series	O
in	O
applied	O
mathematics	O
pp	O
spline	O
models	O
for	O
observational	O
data	O
wahba	O
g	O
johnson	O
d	O
r	O
gao	O
f	O
and	O
gong	O
j	O
adaptive	O
tuning	O
of	O
numerical	O
weather	O
prediction	O
models	O
randomized	O
gcv	O
in	O
three-and	O
four-dimensional	O
data	O
assimilation	O
monthly	O
weather	O
review	O
p	O
watkins	O
c	O
j	O
c	O
h	O
dynamic	O
alignment	B
kernels	O
technical	O
report	O
dept	O
of	O
p	O
computer	O
science	O
royal	O
holloway	O
university	O
of	O
london	O
watkins	O
c	O
j	O
c	O
h	O
dynamic	O
alignment	B
kernels	O
in	O
smola	O
a	O
j	O
bartlett	O
p	O
l	O
and	O
sch	O
olkopf	O
b	O
editors	O
advances	O
in	O
large	O
margin	O
classifiers	O
pages	O
mit	O
press	O
cambridge	O
ma	O
p	O
wegman	O
e	O
j	O
reproducing	O
kernel	B
hilbert	O
spaces	O
in	O
kotz	O
s	O
and	O
johnson	O
n	O
l	O
editors	O
pp	O
encyclopedia	O
of	O
statistical	O
sciences	O
volume	O
pages	O
wiley	O
new	O
york	O
weinert	O
h	O
l	O
editor	O
reproducing	O
kernel	B
hilbert	O
spaces	O
hutchinson	O
ross	O
stroudsburg	O
p	O
pennsylvania	O
wendland	O
h	O
scattered	O
data	O
approximation	O
cambridge	O
monographs	O
on	O
applied	O
and	O
compup	O
tational	O
mathematics	O
cambridge	O
university	O
press	O
whittle	O
p	O
prediction	O
and	O
regulation	O
by	O
linear	B
least-square	O
methods	O
english	O
universities	O
pp	O
press	O
widom	O
h	O
asymptotic	O
behavior	O
of	O
the	O
eigenvalues	O
of	O
certain	O
integral	O
equations	O
trans	O
of	O
the	O
p	O
american	O
mathematical	O
society	O
widom	O
h	O
asymptotic	O
behavior	O
of	O
the	O
eigenvalues	O
of	O
certain	O
integral	O
equations	O
ii	O
archive	O
p	O
for	O
rational	O
mechanics	O
and	O
analysis	O
wiener	O
n	O
extrapolation	O
interpolation	O
and	O
smoothing	O
of	O
stationary	O
time	O
series	O
mit	O
press	O
p	O
cambridge	O
mass	O
williams	O
c	O
k	O
i	O
computation	O
with	O
infinite	O
neural	O
networks	O
neural	O
computation	O
p	O
williams	O
c	O
k	O
i	O
and	O
barber	O
d	O
bayesian	O
classification	B
with	O
gaussian	O
processes	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
ieee	O
pp	O
williams	O
c	O
k	O
i	O
and	O
rasmussen	O
c	O
e	O
gaussian	O
processes	O
for	O
regression	O
in	O
touretzky	O
d	O
s	O
mozer	O
m	O
c	O
and	O
hasselmo	O
m	O
e	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
pp	O
williams	O
c	O
k	O
i	O
rasmussen	O
c	O
e	O
schwaighofer	O
a	O
and	O
tresp	O
v	O
observations	O
on	O
the	O
nystr	O
om	O
method	O
for	O
gaussian	B
process	I
prediction	O
technical	O
report	O
university	O
of	O
edinburgh	O
httpwww	O
dai	O
ed	O
ac	O
ukhomesckiwonline	O
pubs	O
html	O
p	O
williams	O
c	O
k	O
i	O
and	O
seeger	B
m	O
using	O
the	O
nystr	O
om	O
method	O
to	O
speed	O
up	O
kernel	B
machines	O
in	O
leen	O
t	O
k	O
diettrich	O
t	O
g	O
and	O
tresp	O
v	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
pp	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
bibliography	O
williams	O
c	O
k	O
i	O
and	O
vivarelli	O
f	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
learning	B
curve	I
for	O
gaussian	O
pp	O
proccesses	O
machine	O
learning	B
winkler	O
g	O
image	O
analysis	O
random	O
fields	O
and	O
dynamic	O
monte	O
carlo	O
methods	O
springer	O
p	O
berlin	O
wong	O
e	O
stochastic	O
processes	O
in	O
information	O
and	O
dynamical	O
systems	O
mcgraw-hill	O
new	O
york	O
p	O
wood	O
s	O
and	O
kohn	O
r	O
a	O
bayesian	O
approach	O
to	O
robust	O
binary	B
nonparametric	O
regression	O
j	O
p	O
american	O
statistical	O
association	O
yaglom	O
a	O
m	O
correlation	O
theory	O
of	O
stationary	O
and	O
related	O
random	O
functions	O
volume	O
i	O
basic	O
p	O
results	O
springer	O
verlag	O
yang	O
c	O
duraiswami	O
r	O
and	O
david	O
l	O
efficient	O
kernel	B
machines	O
using	O
the	O
improved	O
fast	O
gauss	O
transform	O
in	O
saul	O
l	O
k	O
weiss	O
y	O
and	O
bottou	O
l	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
mit	O
press	O
p	O
ylvisaker	O
d	O
designs	O
on	O
random	O
fields	O
in	O
srivastava	O
j	O
n	O
editor	O
a	O
survey	O
of	O
statistical	O
p	O
design	O
and	O
linear	B
models	O
pages	O
north-holland	O
yuille	O
a	O
and	O
grzywacz	O
n	O
m	O
a	O
mathematical	O
analysis	O
of	O
motion	O
coherence	O
theory	O
interp	O
national	O
journal	O
of	O
computer	O
vision	O
zhang	O
t	O
statistical	O
behaviour	O
and	O
consistency	B
of	O
classification	B
methods	O
based	O
on	O
convex	O
p	O
risk	B
minimization	O
discussion	O
annals	O
of	O
statistics	O
zhu	O
h	O
williams	O
c	O
k	O
i	O
rohwer	O
r	O
j	O
and	O
morciniec	O
m	O
gaussian	O
regression	O
and	O
optimal	B
finite	O
dimensional	O
linear	B
models	O
in	O
bishop	O
c	O
m	O
editor	O
neural	O
networks	O
and	O
machine	O
learning	B
springer-verlag	O
berlin	O
p	O
zhu	O
j	O
and	O
hastie	O
t	O
j	O
kernel	B
logistic	B
regression	I
and	O
the	O
import	O
vector	O
machine	O
in	O
diettrich	O
t	O
g	O
becker	O
s	O
and	O
ghahramani	O
z	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
mit	O
press	O
p	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
author	O
index	O
abrahamsen	O
p	O
abramowitz	O
m	O
adams	O
r	O
adler	O
r	O
j	O
amari	O
s	O
ansley	O
c	O
f	O
ansley	O
c	O
f	O
see	O
kohn	O
r	O
arat	O
o	O
m	O
arfken	O
g	O
xv	O
aronszajn	O
n	O
arsenin	O
v	O
y	O
see	O
tikhonov	O
a	O
n	O
bach	O
f	O
r	O
baker	O
c	O
t	O
h	O
balakrishnan	O
n	O
see	O
johnson	O
n	O
l	O
barber	O
d	O
barber	O
d	O
see	O
williams	O
c	O
k	O
i	O
bartle	O
r	O
g	O
bartlett	O
p	O
l	O
bartlett	O
p	O
l	O
see	O
lanckriet	O
g	O
r	O
g	O
bartlett	O
p	O
l	O
see	O
smola	O
a	O
j	O
belongie	O
s	O
see	O
fowlkes	O
c	O
bengio	O
s	O
see	O
collobert	O
r	O
berger	O
j	O
o	O
bishop	O
c	O
m	O
bishop	O
c	O
m	O
see	O
goldberg	O
p	O
w	O
blake	O
i	O
f	O
blight	O
b	O
j	O
n	O
boyd	O
s	O
boyle	O
p	O
bracewell	O
r	O
n	O
buckley	O
c	O
see	O
salton	O
g	O
caruana	O
r	O
chatfield	O
c	O
chellappa	O
r	O
see	O
kashyap	O
r	O
l	O
choi	O
t	O
choudhuri	O
n	O
chow	O
y	O
see	O
grenander	O
u	O
chu	O
w	O
collins	O
m	O
collobert	O
r	O
conradt	O
j	O
see	O
vijayakumar	O
s	O
cornford	O
d	O
cox	O
d	O
d	O
craven	O
p	O
cressie	O
n	O
a	O
c	O
cristianini	O
n	O
cristianini	O
n	O
see	O
lanckriet	O
g	O
r	O
g	O
cristianini	O
n	O
see	O
lodhi	O
h	O
csat	O
o	O
l	O
daley	O
r	O
david	O
h	O
a	O
david	O
l	O
see	O
yang	O
c	O
davis	O
p	O
j	O
dawid	O
a	O
p	O
dellaportas	O
p	O
denker	O
j	O
see	O
simard	O
p	O
devroye	O
l	O
diaconis	O
p	O
diekhans	O
m	O
see	O
jaakkola	O
t	O
s	O
diggle	O
p	O
j	O
doob	O
j	O
l	O
drineas	O
p	O
d	O
souza	O
a	O
see	O
vijayakumar	O
s	O
duchon	O
j	O
duda	O
r	O
o	O
duffy	O
n	O
see	O
collins	O
m	O
duraiswami	O
r	O
see	O
yang	O
c	O
eddy	O
w	O
f	O
see	O
geisser	O
s	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
author	O
index	O
edgeworth	O
f	O
y	O
el	O
ghaoui	O
l	O
see	O
lanckriet	O
g	O
r	O
g	O
elisseeff	O
a	O
see	O
cristianini	O
n	O
eskin	O
e	O
see	O
leslie	O
c	O
faul	O
a	O
c	O
faul	O
a	O
c	O
see	O
tipping	O
m	O
e	O
feldman	O
j	O
ferrari	O
trecate	O
g	O
fine	O
s	O
flannery	O
b	O
p	O
see	O
press	O
w	O
h	O
fowlkes	O
c	O
frean	O
m	O
see	O
boyle	O
p	O
freedman	O
d	O
freedman	O
d	O
see	O
diaconis	O
p	O
frieze	O
a	O
gammerman	O
a	O
see	O
saunders	O
c	O
gammerman	O
a	O
see	O
stitson	O
m	O
o	O
gao	O
f	O
see	O
lin	O
x	O
gao	O
f	O
see	O
wahba	O
g	O
geisser	O
s	O
geman	O
d	O
see	O
geman	O
s	O
geman	O
s	O
ghahramani	O
z	O
see	O
chu	O
w	O
ghahramani	O
z	O
see	O
rasmussen	O
c	O
e	O
ghosal	O
s	O
see	O
choudhuri	O
n	O
gibbs	B
m	O
n	O
gihman	O
i	O
i	O
girard	O
a	O
girard	O
a	O
see	O
murray-smith	O
r	O
girosi	O
f	O
girosi	O
f	O
see	O
poggio	O
t	O
girosi	O
f	O
see	O
pontil	O
m	O
goldberg	O
p	O
w	O
golub	O
g	O
h	O
gong	O
j	O
see	O
wahba	O
g	O
gradshteyn	O
i	O
s	O
green	O
p	O
j	O
grenander	O
u	O
grimmett	O
g	O
r	O
gr	O
unwald	O
p	O
d	O
grzywacz	O
n	O
m	O
see	O
yuille	O
a	O
guttorp	O
p	O
see	O
sampson	O
p	O
d	O
gy	O
orfi	O
l	O
gy	O
orfi	O
l	O
see	O
devroye	O
l	O
hajek	O
j	O
hand	O
d	O
j	O
hannan	O
e	O
j	O
hansen	O
l	O
k	O
hart	O
p	O
e	O
see	O
duda	O
r	O
o	O
hastie	O
t	O
j	O
hastie	O
t	O
j	O
see	O
zhu	O
j	O
haussler	O
d	O
haussler	O
d	O
see	O
jaakkola	O
t	O
s	O
hawkins	O
d	O
l	O
herbrich	O
r	O
see	O
lawrence	O
n	O
hinton	O
g	O
e	O
see	O
jacobs	O
r	O
a	O
hoerl	O
a	O
e	O
hornik	O
k	O
huijbregts	O
c	O
j	O
see	O
journel	O
a	O
g	O
ihara	O
s	O
jaakkola	O
t	O
s	O
jacobs	O
r	O
a	O
johnson	O
d	O
r	O
see	O
wahba	O
g	O
johnson	O
n	O
l	O
jones	O
d	O
r	O
jones	O
m	O
see	O
girosi	O
f	O
jordan	O
m	O
i	O
jordan	O
m	O
i	O
see	O
bach	O
f	O
r	O
jordan	O
m	O
i	O
see	O
bartlett	O
p	O
l	O
jordan	O
m	O
i	O
see	O
jacobs	O
r	O
a	O
jordan	O
m	O
i	O
see	O
lanckriet	O
g	O
r	O
g	O
jordan	O
m	O
i	O
see	O
seeger	B
m	O
jordan	O
m	O
i	O
see	O
teh	O
y	O
w	O
journel	O
a	O
g	O
kailath	O
t	O
kammler	O
d	O
w	O
kandola	O
j	O
see	O
cristianini	O
n	O
kannan	O
r	O
see	O
frieze	O
a	O
kashyap	O
r	O
l	O
kawanabe	O
m	O
see	O
tsuda	O
k	O
keeling	O
c	O
d	O
keenan	O
d	O
m	O
see	O
grenander	O
u	O
keerthi	O
s	O
s	O
see	O
sundararajan	O
s	O
kennard	O
r	O
w	O
see	O
hoerl	O
a	O
e	O
kennedy	O
m	O
c	O
see	O
o	O
hagan	O
a	O
kent	O
j	O
t	O
kimeldorf	O
g	O
klautau	O
a	O
see	O
rifkin	O
r	O
klein	O
b	O
see	O
lin	O
x	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
author	O
index	O
klein	O
r	O
see	O
lin	O
x	O
kohler	O
m	O
see	O
gy	O
orfi	O
l	O
kohn	O
r	O
kohn	O
r	O
see	O
ansley	O
c	O
f	O
kohn	O
r	O
see	O
wood	O
s	O
kolmogorov	O
a	O
n	O
k	O
onig	O
h	O
kotz	O
s	O
see	O
johnson	O
n	O
l	O
krzy	O
zak	O
a	O
see	O
gy	O
orfi	O
l	O
kullback	O
s	O
kuss	O
m	O
lanckriet	O
g	O
r	O
g	O
langford	O
j	O
see	O
gr	O
unwald	O
p	O
d	O
lauritzen	O
s	O
l	O
lawrence	O
n	O
lawrence	O
n	O
see	O
seeger	B
m	O
le	O
cun	O
y	O
see	O
simard	O
p	O
leith	O
d	O
see	O
solak	O
e	O
leithead	O
w	O
e	O
see	O
solak	O
e	O
leslie	O
c	O
liisberg	O
c	O
see	O
hansen	O
l	O
k	O
lin	O
x	O
lindley	O
d	O
v	O
lindsey	O
w	O
c	O
see	O
blake	O
i	O
f	O
lodhi	O
h	O
lugosi	O
g	O
see	O
devroye	O
l	O
luo	O
z	O
mackay	O
d	O
j	O
c	O
xiii	O
xiv	O
xvi	O
mackay	O
d	O
j	O
c	O
see	O
gibbs	B
m	O
n	O
mahoney	O
m	O
w	O
see	O
drineas	O
p	O
malik	O
j	O
see	O
fowlkes	O
c	O
malzahn	O
d	O
mandelbrot	O
b	O
b	O
mannila	O
h	O
see	O
hand	O
d	O
j	O
mardia	O
k	O
v	O
mardia	O
k	O
v	O
see	O
kent	O
j	O
t	O
marshall	O
r	O
j	O
see	O
mardia	O
k	O
v	O
mat	O
ern	O
b	O
matheron	O
g	O
maxwell	O
j	O
c	O
v	O
mcallester	B
d	O
mcauliffe	O
j	O
d	O
see	O
bartlett	O
p	O
l	O
mccullagh	O
p	O
meinguet	O
j	O
meir	O
r	O
micchelli	O
c	O
a	O
minka	O
t	O
p	O
mitchell	O
t	O
j	O
see	O
sacks	O
j	O
mitchell	O
t	O
m	O
morciniec	O
m	O
see	O
zhu	O
h	O
moyeed	O
r	O
a	O
see	O
diggle	O
p	O
j	O
mukherjee	O
s	O
see	O
pontil	O
m	O
m	O
uller	O
k	O
-r	O
see	O
sch	O
olkopf	O
b	O
m	O
uller	O
k	O
-r	O
see	O
tsuda	O
k	O
murray-smith	O
r	O
murray-smith	O
r	O
see	O
girard	O
a	O
murray-smith	O
r	O
see	O
solak	O
e	O
nabney	O
i	O
t	O
see	O
cornford	O
d	O
neal	O
r	O
m	O
xiii	O
nelder	O
j	O
see	O
mccullagh	O
p	O
neumaier	O
a	O
notz	O
w	O
see	O
santner	O
t	O
j	O
nowlan	O
s	O
j	O
see	O
jacobs	O
r	O
a	O
oakley	O
j	O
e	O
see	O
o	O
hagan	O
a	O
o	O
hagan	O
a	O
ksendal	O
b	O
opper	O
m	O
opper	O
m	O
see	O
csat	O
o	O
l	O
opper	O
m	O
see	O
ferrari	O
trecate	O
g	O
opper	O
m	O
see	O
malzahn	O
d	O
ornstein	O
l	O
s	O
see	O
uhlenbeck	O
g	O
e	O
o	O
sullivan	O
f	O
see	O
cox	O
d	O
d	O
o	O
sullivan	O
f	O
ott	O
l	O
see	O
blight	O
b	O
j	O
n	O
paciorek	O
c	O
papoulis	O
a	O
picard	O
r	O
w	O
see	O
minka	O
t	O
p	O
plaskota	O
l	O
plate	O
t	O
a	O
platt	O
j	O
c	O
poggio	O
t	O
poggio	O
t	O
see	O
girosi	O
f	O
pontil	O
m	O
pontil	O
m	O
see	O
micchelli	O
c	O
a	O
press	O
w	O
h	O
qui	O
nonero-candela	O
j	O
qui	O
nonero-candela	O
j	O
see	O
girard	O
a	O
qui	O
nonero-candela	O
j	O
see	O
rasmussen	O
c	O
e	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
author	O
index	O
rasmussen	O
c	O
e	O
rasmussen	O
c	O
e	O
see	O
girard	O
a	O
rasmussen	O
c	O
e	O
see	O
kuss	O
m	O
rasmussen	O
c	O
e	O
see	O
solak	O
e	O
rasmussen	O
c	O
e	O
see	O
williams	O
c	O
k	O
i	O
r	O
atsch	O
g	O
see	O
tsuda	O
k	O
raynor	O
w	O
j	O
see	O
o	O
sullivan	O
f	O
rifkin	O
r	O
ripley	O
b	O
ritter	O
k	O
rohwer	O
r	O
j	O
see	O
zhu	O
h	O
rousseeuw	O
p	O
j	O
roy	O
a	O
see	O
choudhuri	O
n	O
ryzhik	O
i	O
m	O
see	O
gradshteyn	O
i	O
s	O
saad	O
d	O
see	O
barber	O
d	O
sacks	O
j	O
saitoh	O
s	O
salamon	O
p	O
see	O
hansen	O
l	O
k	O
salton	O
g	O
sampson	O
p	O
d	O
santner	O
t	O
j	O
saunders	O
c	O
schaal	O
s	O
see	O
vijayakumar	O
s	O
scheinberg	O
k	O
see	O
fine	O
s	O
schervish	O
m	O
j	O
see	O
choi	O
t	O
schervish	O
m	O
j	O
see	O
paciorek	O
c	O
schoenberg	O
i	O
j	O
sch	O
olkopf	O
b	O
xvi	O
sch	O
olkopf	O
b	O
see	O
smola	O
a	O
j	O
schwaighofer	O
a	O
schwaighofer	O
a	O
see	O
williams	O
c	O
k	O
i	O
scott	O
d	O
w	O
seeger	B
m	O
seeger	B
m	O
see	O
lawrence	O
n	O
seeger	B
m	O
see	O
teh	O
y	O
w	O
seeger	B
m	O
see	O
williams	O
c	O
k	O
i	O
shawe-taylor	O
j	O
shawe-taylor	O
j	O
see	O
cristianini	O
n	O
shawe-taylor	O
j	O
see	O
lodhi	O
h	O
shawe-taylor	O
j	O
see	O
saunders	O
c	O
shepp	O
l	O
a	O
shibata	O
t	O
see	O
vijayakumar	O
s	O
silverman	O
b	O
w	O
silverman	O
b	O
w	O
see	O
green	O
p	O
j	O
simard	O
p	O
skorohod	O
a	O
v	O
see	O
gihman	O
i	O
i	O
smola	O
a	O
j	O
smola	O
a	O
j	O
see	O
sch	O
olkopf	O
b	O
xvi	O
smola	O
a	O
j	O
see	O
vishwanathan	O
s	O
v	O
n	O
smyth	O
p	O
see	O
hand	O
d	O
j	O
solak	O
e	O
sollich	O
p	O
sonnenburg	O
s	O
see	O
tsuda	O
k	O
stafford	O
noble	O
w	O
see	O
leslie	O
c	O
stegun	O
i	O
a	O
see	O
abramowitz	O
m	O
stein	O
m	O
l	O
steinwart	O
i	O
stephens	O
d	O
a	O
see	O
dellaportas	O
p	O
stirzaker	O
d	O
r	O
see	O
grimmett	O
g	O
r	O
stitson	O
m	O
o	O
sundararajan	O
s	O
suykens	O
j	O
a	O
k	O
svensen	O
m	O
see	O
bishop	O
c	O
m	O
szeliski	O
r	O
tawn	O
j	O
a	O
see	O
diggle	O
p	O
j	O
teh	O
y	O
w	O
teukolsky	O
s	O
a	O
see	O
press	O
w	O
h	O
thomas-agnan	O
c	O
thompson	O
p	O
d	O
tibshirani	O
r	O
j	O
see	O
hastie	O
t	O
j	O
tikhonov	O
a	O
n	O
tipping	O
m	O
e	O
tipping	O
m	O
e	O
see	O
faul	O
a	O
c	O
tresp	O
v	O
tresp	O
v	O
see	O
schwaighofer	O
a	O
tresp	O
v	O
see	O
williams	O
c	O
k	O
i	O
tsuda	O
k	O
uhlenbeck	O
g	O
e	O
valiant	O
l	O
g	O
van	O
loan	O
c	O
f	O
see	O
golub	O
g	O
h	O
vandenberghe	O
l	O
see	O
boyd	O
s	O
vanderwalle	O
j	O
see	O
suykens	O
j	O
a	O
k	O
vapnik	O
v	O
n	O
vapnik	O
v	O
n	O
see	O
stitson	O
m	O
o	O
vempala	O
s	O
see	O
frieze	O
a	O
vetterling	O
w	O
t	O
see	O
press	O
w	O
h	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
author	O
index	O
victorri	O
b	O
see	O
simard	O
p	O
vijayakumar	O
s	O
vinokourov	O
a	O
see	O
saunders	O
c	O
vishwanathan	O
s	O
v	O
n	O
vivarelli	O
f	O
vivarelli	O
f	O
see	O
opper	O
m	O
vivarelli	O
f	O
see	O
williams	O
c	O
k	O
i	O
von	O
mises	O
r	O
voorhees	O
h	O
see	O
poggio	O
t	O
vovk	O
v	O
see	O
saunders	O
c	O
vovk	O
v	O
see	O
stitson	O
m	O
o	O
wahba	O
g	O
wahba	O
g	O
see	O
craven	O
p	O
wahba	O
g	O
see	O
kimeldorf	O
g	O
wahba	O
g	O
see	O
lin	O
x	O
wahba	O
g	O
see	O
luo	O
z	O
wahba	O
g	O
see	O
micchelli	O
c	O
a	O
walk	O
h	O
see	O
gy	O
orfi	O
l	O
wasilkowski	O
g	O
w	O
see	O
ritter	O
k	O
watkins	O
c	O
j	O
c	O
h	O
watkins	O
c	O
j	O
c	O
h	O
see	O
lodhi	O
h	O
watkins	O
c	O
j	O
c	O
h	O
see	O
stitson	O
m	O
o	O
wegman	O
e	O
j	O
welch	O
w	O
j	O
see	O
sacks	O
j	O
wendland	O
h	O
weston	O
j	O
see	O
leslie	O
c	O
weston	O
j	O
see	O
stitson	O
m	O
o	O
whittle	O
p	O
whorf	O
t	O
p	O
see	O
keeling	O
c	O
d	O
widom	O
h	O
wiener	O
n	O
williams	O
b	O
j	O
see	O
santner	O
t	O
j	O
williams	O
c	O
k	O
i	O
williams	O
c	O
k	O
i	O
see	O
bishop	O
c	O
m	O
williams	O
c	O
k	O
i	O
see	O
cornford	O
d	O
williams	O
c	O
k	O
i	O
see	O
ferrari	O
trecate	O
g	O
williams	O
c	O
k	O
i	O
see	O
goldberg	O
p	O
w	O
williams	O
c	O
k	O
i	O
see	O
seeger	B
m	O
williams	O
c	O
k	O
i	O
see	O
shawe-taylor	O
j	O
williams	O
c	O
k	O
i	O
see	O
sollich	O
p	O
williams	O
c	O
k	O
i	O
see	O
vivarelli	O
f	O
williams	O
c	O
k	O
i	O
see	O
zhu	O
h	O
winkler	O
g	O
winther	O
o	O
see	O
csat	O
o	O
l	O
winther	O
o	O
see	O
opper	O
m	O
wong	O
e	O
wood	O
s	O
wo	O
zniakowski	O
h	O
see	O
ritter	O
k	O
wynn	O
h	O
p	O
see	O
sacks	O
j	O
xiang	O
d	O
see	O
lin	O
x	O
yaglom	O
a	O
m	O
yandell	O
b	O
s	O
see	O
o	O
sullivan	O
f	O
yang	O
c	O
ylvisaker	O
d	O
yuille	O
a	O
yuille	O
a	O
see	O
poggio	O
t	O
zhang	O
t	O
zhang	O
t	O
see	O
meir	O
r	O
zhu	O
h	O
zhu	O
j	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
subject	O
index	O
arma	O
process	O
see	O
autoregressive	O
moving-average	O
covariance	B
cokriging	B
consistency	B
convex	O
function	B
set	B
predictive	B
covariance	B
function	B
see	O
also	O
kernel	B
anova	B
compact	B
support	I
dot	B
product	I
exponential	B
gaussian	O
see	O
covariance	B
function	B
squared	B
ex	O
alignment	B
anisotropy	B
ar	O
process	O
see	O
autoregressive	B
process	I
ard	O
see	O
automatic	O
relevance	O
determination	B
process	O
automatic	O
relevance	O
determination	B
autoregressive	B
moving-average	I
process	I
noise	B
model	I
autoregressive	B
process	I
bayes	B
classifier	I
bayes	O
theorem	O
bayesian	B
committee	I
machine	I
bcm	O
see	O
bayesian	B
committee	I
machine	I
bias	O
inductive	B
binary	B
classification	B
bits	B
blitzkrieging	O
see	O
fast	O
gaussian	O
processes	O
bochner	O
s	O
theorem	O
brownian	B
bridge	I
brownian	O
motion	O
see	O
wiener	B
process	I
canonical	B
hyperplane	I
cholesky	B
decomposition	I
christ	B
bowels	O
of	O
classification	B
binary	B
least-squares	B
probabilistic	B
multi-class	B
probabilistic	B
classifier	O
gibbs	B
predictive	B
ponential	O
inhomogeneous	B
polynomial	B
mat	O
ern	O
neural	B
network	I
ornstein-uhlenbeck	B
covariance	B
ou	O
see	O
uhlenbeck	O
periodic	B
polynomial	B
piecewise	B
function	B
ornstein	O
radial	O
basis	O
function	B
see	O
covariance	B
function	B
squared	B
exponential	B
rational	B
quadratic	I
rbf	O
see	O
covariance	B
function	B
squared	B
expo	O
se	O
see	O
covariance	B
function	B
squared	B
exponen	O
nential	O
tial	O
squared	B
exponential	B
covariance	B
matrix	I
cromwell	O
s	O
dictum	O
cross-validation	B
generalized	B
leave-one-out	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
subject	O
index	O
dataset	O
robot	B
inverse	O
dynamics	O
usps	B
decision	B
region	I
decision	B
surface	I
degenerate	B
see	O
kernel	B
degenerate	B
degrees	B
of	I
freedom	I
derivative	B
observations	I
dirichlet	B
process	I
discriminative	B
approach	I
eigenfunction	B
eigenvalue	B
entropy	B
ep	O
see	O
expectation	B
propagation	I
error	O
function	B
equivalent	B
kernel	B
error	O
generalization	B
error	I
function	B
hinge	B
error-reject	B
curve	I
errors-in-variables	B
regression	I
evidence	O
see	O
marginal	B
likelihood	B
expectation	B
propagation	I
experimental	O
design	O
optimal	B
factor	B
analysis	I
feature	B
space	I
fisher	B
information	I
matrix	I
fisher	B
kernel	B
fourier	B
transform	I
fractal	B
gamma	B
distribution	I
gaussian	B
distribution	I
gaussian	B
markov	I
process	I
gaussian	B
process	I
gaussian	B
process	I
classification	B
gaussian	B
process	I
latent	B
variable	I
model	I
gaussian	B
process	I
regression	I
generalization	B
error	I
generative	B
approach	I
generative	B
topographic	I
mapping	I
geostatistics	B
gmp	O
see	O
gaussian	B
markov	I
process	I
gp	O
see	O
gaussian	B
process	I
gpc	O
see	O
gaussian	B
process	I
classification	B
gplvm	O
see	O
gaussian	B
process	I
latent	B
variable	I
model	I
gpr	O
see	O
gaussian	B
process	I
regression	I
gram	B
matrix	I
green	O
s	O
function	B
gtm	O
see	O
generative	B
topographic	I
mapping	I
hidden	B
markov	I
model	I
hinge	B
error	I
function	B
hyperparameters	B
hyperplane	O
canonical	B
index	B
set	B
informative	B
vector	I
machine	I
integrals	B
evaluation	O
of	O
intrinsic	B
random	I
function	B
invariances	B
irls	O
see	O
iteratively	B
reweighted	I
least	I
squares	I
isotropy	B
iteratively	B
reweighted	I
least	I
squares	I
ivm	O
see	O
informative	B
vector	I
machine	I
jitter	B
kernel	B
see	O
also	O
covariance	B
function	B
bag-of-characters	B
degenerate	B
equivalent	B
fisher	B
k-spectrum	B
nondegenerate	B
positive	B
definite	I
string	B
tangent	B
of	I
posterior	I
odds	I
kernel	B
classifier	I
kernel	B
pca	I
kernel	B
ridge	B
regression	O
see	O
regression	O
ridge	B
kernel	B
kernel	B
smoother	I
kernel	B
trick	I
kriging	B
kullback-leibler	B
divergence	I
laplace	B
approximation	I
latent	B
variable	I
model	I
learning	B
curve	I
learning	B
supervised	O
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
subject	O
index	O
least-squares	B
classification	B
leave-one-out	B
leave-one-out	B
cross-validation	B
length-scale	B
characteristic	O
likelihood	B
mean	B
square	I
continuity	I
mean	B
square	I
differentiability	I
mean	B
standardized	I
log	I
loss	B
mean-field	B
approximation	I
measure	B
mercer	O
s	O
theorem	O
mixture	B
of	I
experts	I
ml-ii	O
see	O
type	B
ii	I
maximum	I
likelihood	B
model	O
non-parametric	B
parametric	B
semi-parametric	B
moore-aronszajn	B
theorem	I
ms	O
continuity	O
see	O
mean	B
square	I
continuity	I
ms	O
differentiability	O
see	O
mean	O
square	O
differentiabil	O
ity	O
msll	O
see	O
mean	B
standardized	I
log	I
loss	B
multi-class	B
classification	B
multi-task	B
learning	B
multiple	B
outputs	B
nadaraya-watson	B
estimator	I
nats	B
neural	B
network	I
newton	O
s	O
method	O
noise	B
model	I
correlated	B
heteroscedastic	B
norm	O
frobenius	B
null	B
space	I
nystr	O
om	O
approximation	O
nystr	O
om	O
method	O
occam	O
s	O
razor	O
one-versus-rest	B
operator	B
integral	O
optimal	B
experimental	I
design	I
outputs	B
multiple	O
see	O
probabilistic	B
one	I
nearest	I
neighbour	I
pac-bayesian	O
theorem	O
mcallester	B
seeger	B
penalized	B
maximum	I
likelihood	B
estimate	I
plsc	O
see	O
probabilistic	B
least-squares	B
classification	B
positive	B
definite	I
matrix	I
positive	B
semidefinite	I
matrix	I
logistic	B
multiple-logistic	B
non-gaussian	B
probit	B
linear	B
classifier	I
linear	B
regression	I
link	B
function	B
log	B
odds	I
ratio	I
logistic	B
function	B
logistic	B
regression	I
loo	O
see	O
leave-one-out	B
loss	B
negative	B
log	I
probability	I
squared	B
zero-one	B
loss	B
function	B
loss	B
matrix	I
lsc	O
see	O
least-squares	B
classification	B
map	B
see	O
maximum	O
a	O
posteriori	O
margin	O
functional	B
geometrical	B
marginal	B
likelihood	B
marginalization	B
property	I
markov	B
chain	I
monte	I
carlo	I
markov	B
random	I
field	I
matrix	O
covariance	B
fisher	B
information	I
gram	B
inversion	B
lemma	I
loss	B
partitioned	B
inversion	I
of	I
positive	B
definite	I
positive	B
semidefinite	I
maximum	O
a	O
posteriori	O
see	O
penalized	B
maximum	O
like	O
lihood	O
maximum	O
likelihood	B
penalized	B
mcmc	O
see	O
markov	B
chain	I
monte	I
carlo	I
mean	B
function	B
c	O
e	O
rasmussen	O
c	O
k	O
i	O
williams	O
gaussian	O
processes	O
for	O
machine	O
learning	B
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
www	O
gaussianprocess	O
orggpml	O
isbn	O
subject	O
index	O
stationarity	B
stochastic	B
differential	I
equation	I
student	O
s	O
t	O
process	O
subset	B
of	I
datapoints	I
subset	B
of	I
regressors	I
supervised	B
learning	B
support	B
vector	I
support	B
vector	I
machine	I
soft	B
margin	I
support	B
vector	I
regression	I
svm	O
see	O
support	B
vector	I
machine	I
svr	O
see	O
support	B
vector	I
regression	I
tangent	B
of	I
posterior	I
odds	I
kernel	B
top	O
kernel	B
see	O
tangent	B
of	I
posterior	I
odds	I
kernel	B
transduction	B
type	B
ii	I
maximum	I
likelihood	B
uncertain	B
inputs	I
upcrossing	B
rate	I
usps	B
dataset	I
weight	B
function	B
weight	B
vector	I
wiener	B
process	I
integrated	B
tied-down	B
wiener-khintchine	B
theorem	I
yule-walker	B
equations	I
posterior	B
process	I
pp	O
see	O
projected	B
process	I
approximation	I
prediction	O
classification	B
averaged	B
map	B
probabilistic	B
classification	B
probabilistic	B
least-squares	B
classification	B
probabilistic	B
one	I
nearest	I
neighbour	I
probability	O
conditional	B
joint	B
marginal	B
probit	B
regression	I
projected	B
process	I
approximation	I
pseudo-likelihood	B
quadratic	B
form	I
quadratic	B
programming	I
regression	O
errors-in-variables	B
gaussian	B
process	I
linear	B
polynomial	B
ridge	B
kernel	B
regularization	B
regularization	B
network	I
reject	B
option	I
relative	O
entropy	B
see	O
kullback	O
leibler	O
divergence	O
relevance	B
vector	I
machine	I
representer	B
theorem	I
reproducing	B
kernel	B
hilbert	I
space	I
response	B
function	B
ridge	B
regression	O
see	O
regression	O
ridge	B
risk	B
rkhs	O
see	O
reproducing	B
kernel	B
hilbert	I
space	I
rvm	O
see	O
relevance	B
vector	I
machine	I
scale	B
mixture	I
sd	O
see	O
subset	B
of	I
datapoints	I
sde	O
see	O
stochastic	B
differential	I
equation	I
smse	O
see	O
standardized	B
mean	I
squared	B
error	I
softmax	B
splines	B
sr	O
see	O
subset	B
of	I
regressors	I
standardized	B
mean	I
squared	B
error	I
