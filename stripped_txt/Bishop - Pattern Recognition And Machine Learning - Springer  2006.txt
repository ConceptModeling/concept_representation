information science and statistics series editors m. jordan j. kleinberg b. scho lkopf information science and statistics akaike and kitagawa the practice of time series analysis. bishop pattern recognition and machine learning. cowell dawid lauritzen and spiegelhalter probabilistic networks and expert systems. doucet de freitas and gordon sequential monte carlo methods in practice. fine feedforward neural network methodology. hawkins and olwell cumulative sum charts and charting for quality improvement. jensen bayesian networks and decision graphs. marchette computer intrusion detection and network monitoring a statistical viewpoint. rubinstein and kroese the cross-entropy method a unified approach to combinatorial optimization monte carlo simulation and machine learning. studen probabilistic conditional independence structures. vapnik the nature of statistical learning theory second edition. wallace statistical and inductive inference by minimum massage length. christopher m. bishop pattern recognition and machine learning christopher m. bishop f.r.eng. assistant director microsoft research ltd cambridge u.k. cmbishopmicrosoft.com httpresearch.microsoft.com cmbishop series editors michael jordan department of computer science and department of statistics university of california berkeley berkeley ca usa professor jon kleinberg department of computer science cornell university ithaca ny usa bernhard scho lkopf max planck institute for biological cybernetics spemannstrasse tu bingen germany library of congress control number printed on acid-free paper. springer sciencebusiness media llc all rights reserved. this work may not be translated or copied in whole or in part without the written permission of the publisher sciencebusiness media llc spring street new york ny usa except for brief excerpts in connection with reviews or scholarly analysis. use in connection with any form of information storage and retrieval electronic adaptation computer software or by similar or dissimilar methodology now known or hereafter developed is forbidden. the use in this publication of trade names trademarks service marks and similar terms even if they are not identified as such is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights. printed in singapore. springer.com this book is dedicated to my family jenna mark and hugh total eclipse of the sun antalya turkey march preface pattern recognition has its origins in engineering whereas machine learning grew out of computer science. however these activities can be viewed as two facets of the same field and together they have undergone substantial development over the past ten years. in particular bayesian methods have grown from a specialist niche to become mainstream while graphical models have emerged as a general framework for describing and applying probabilistic models. also the practical applicability of bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational bayes and expectation propagation. similarly new models based on kernels have had significant impact on both algorithms and applications. this new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. it is aimed at advanced undergraduates or first year phd students as well as researchers and practitioners and assumes no previous knowledge of pattern recognition or machine learning concepts. knowledge of multivariate calculus and basic linear algebra is required and some familiarity with probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. because this book has broad scope it is impossible to provide a complete list of references and in particular no attempt has been made to provide accurate historical attribution of ideas. instead the aim has been to give references that offer greater detail than is possible here and that hopefully provide entry points into what in some cases is a very extensive literature. for this reason the references are often to more recent textbooks and review articles rather than to original sources. the book is supported by a great deal of additional material including lecture slides as well as the complete set of figures used in the book and the reader is encouraged to visit the book web site for the latest information httpresearch.microsoft.com cmbishopprml vii viii preface exercises the exercises that appear at the end of every chapter form an important component of the book. each exercise has been carefully chosen to reinforce concepts explained in the text or to develop and generalize them in significant ways and each is graded according to difficulty ranging from which denotes a simple exercise taking a few minutes to complete through to which denotes a significantly more complex exercise. it has been difficult to know to what extent these solutions should be made widely available. those engaged in self study will find worked solutions very beneficial whereas many course tutors request that solutions be available only via the publisher so that the exercises may be used in class. in order to try to meet these conflicting requirements those exercises that help amplify key points in the text or that fill in important details have solutions that are available as a pdf file from the book web site. such exercises are denoted by www solutions for the remaining exercises are available to course tutors by contacting the publisher details are given on the book web site. readers are strongly encouraged to work through the exercises unaided and to turn to the solutions only as required. although this book focuses on concepts and principles in a taught course the students should ideally have the opportunity to experiment with some of the key algorithms using appropriate data sets. a companion volume and nabney will deal with practical aspects of pattern recognition and machine learning and will be accompanied by matlab software implementing most of the algorithms discussed in this book. acknowledgements first of all i would like to express my sincere thanks to markus svens en who has provided immense help with preparation of figures and with the typesetting of the book in latex. his assistance has been invaluable. i am very grateful to microsoft research for providing a highly stimulating research environment and for giving me the freedom to write this book views and opinions expressed in this book however are my own and are therefore not necessarily the same as those of microsoft or its affiliates. springer has provided excellent support throughout the final stages of preparation of this book and i would like to thank my commissioning editor john kimmel for his support and professionalism as well as joseph piliero for his help in designing the cover and the text format and maryann brickner for her numerous contributions during the production phase. the inspiration for the cover design came from a discussion with antonio criminisi. i also wish to thank oxford university press for permission to reproduce excerpts from an earlier textbook neural networks for pattern recognition the images of the mark perceptron and of frank rosenblatt are reproduced with the permission of arvin calspan advanced technology center. i would also like to thank asela gunawardana for plotting the spectrogram in figure and bernhard sch olkopf for permission to use his kernel pca code to plot figure preface ix many people have helped by proofreading draft material and providing comments and suggestions including shivani agarwal c edric archambeau arik azran andrew blake hakan cevikalp michael fourman brendan frey zoubin ghahramani thore graepel katherine heller ralf herbrich geoffrey hinton adam johansen matthew johnson michael jordan eva kalyvianaki anitha kannan julia lasserre david liu tom minka ian nabney tonatiuh pena yuan qi sam roweis balaji sanjiya toby sharp ana costa e silva david spiegelhalter jay stokes tara symeonides martin szummer marshall tappen ilkay ulusoy chris williams john winn and andrew zisserman. finally i would like to thank my wife jenna who has been hugely supportive throughout the several years it has taken to write this book. chris bishop cambridge february mathematical notation i have tried to keep the mathematical content of the book to the minimum necessary to achieve a proper understanding of the field. however this minimum level is nonzero and it should be emphasized that a good grasp of calculus linear algebra and probability theory is essential for a clear understanding of modern pattern recognition and machine learning techniques. nevertheless the emphasis in this book is on conveying the underlying concepts rather than on mathematical rigour. i have tried to use a consistent notation throughout the book although at times this means departing from some of the conventions used in the corresponding research literature. vectors are denoted by lower case bold roman letters such as x and all vectors are assumed to be column vectors. a superscript t denotes the transpose of a matrix or vector so that xt will be a row vector. uppercase bold roman letters such as m denote matrices. the notation wm denotes a row vector with m elements while the corresponding column vector is written as w wm the notation b is used to denote the closed interval from a to b that is the interval including the values a and b themselves while b denotes the corresponding open interval that is the interval excluding a and b. similarly b denotes an interval that includes a but excludes b. for the most part however there will be little need to dwell on such refinements as whether the end points of an interval are included or not. the m m identity matrix known as the unit matrix is denoted im which will be abbreviated to i where there is no ambiguity about it dimensionality. it has elements iij that equal if i j and if i j. functional is discussed in appendix d. a functional is denoted fy where yx is some function. the concept of a the notation gx ofx denotes that is bounded as x for instance if gx then gx the expectation of a function fx y with respect to a random variable x is denoted by exfx y. in situations where there is no ambiguity as to which variable is being averaged over this will be simplified by omitting the suffix for instance xi xii mathematical notation ex. if the distribution of x is conditioned on another variable z then the corresponding conditional expectation will be written exfxz. similarly the variance is denoted varfx and for vector variables the covariance is written covx y. we shall also use covx as a shorthand notation for covx x. the concepts of expectations and covariances are introduced in section if we have n values xn of a d-dimensional vector x xdt we can combine the observations into a data matrix x in which the nth row of x corresponds to the row vector xt n. thus the n i element of x corresponds to the ith element of the nth observation xn. for the case of one-dimensional variables we shall denote such a matrix by x which is a column vector whose nth element is xn. note that x has dimensionality n uses a different typeface to distinguish it from x has dimensionality d. contents preface mathematical notation contents introduction vii xi xiii xiii example polynomial curve fitting probability theory probability densities expectations and covariances bayesian probabilities the gaussian distribution curve fitting re-visited bayesian curve fitting model selection decision theory the curse of dimensionality minimizing the misclassification rate minimizing the expected loss the reject option inference and decision loss functions for regression information theory relative entropy and mutual information exercises xiv contents probability distributions binary variables the beta distribution multinomial variables sequential estimation the dirichlet distribution the gaussian distribution conditional gaussian distributions marginal gaussian distributions bayes theorem for gaussian variables maximum likelihood for the gaussian bayesian inference for the gaussian student s t-distribution periodic variables mixtures of gaussians the exponential family maximum likelihood and sufficient statistics conjugate priors noninformative priors nonparametric methods kernel density estimators nearest-neighbour methods exercises linear models for regression linear basis function models maximum likelihood and least squares geometry of least squares regularized least squares multiple outputs the bias-variance decomposition sequential learning equivalent kernel bayesian linear regression parameter distribution predictive distribution bayesian model comparison the evidence approximation evaluation of the evidence function maximizing the evidence function effective number of parameters limitations of fixed basis functions exercises fisher s linear discriminant fisher s discriminant for multiple classes linear models for classification discriminant functions two classes multiple classes least squares for classification relation to least squares the perceptron algorithm probabilistic generative models continuous inputs maximum likelihood solution discrete features exponential family probabilistic discriminative models fixed basis functions logistic regression multiclass logistic regression canonical link functions the laplace approximation model comparison and bic iterative reweighted least squares probit regression bayesian logistic regression laplace approximation predictive distribution exercises neural networks feed-forward network functions weight-space symmetries network training parameter optimization local quadratic approximation use of gradient information gradient descent optimization error backpropagation evaluation of error-function derivatives a simple example efficiency of backpropagation the jacobian matrix the hessian matrix diagonal approximation outer product approximation inverse hessian contents xv xvi contents finite differences exact evaluation of the hessian fast multiplication by the hessian regularization in neural networks invariances consistent gaussian priors early stopping tangent propagation training with transformed data convolutional networks soft weight sharing mixture density networks bayesian neural networks posterior parameter distribution hyperparameter optimization bayesian neural networks for classification exercises kernel methods dual representations constructing kernels radial basis function networks nadaraya-watson model gaussian processes linear regression revisited gaussian processes for regression learning the hyperparameters automatic relevance determination gaussian processes for classification laplace approximation connection to neural networks exercises sparse kernel machines maximum margin classifiers overlapping class distributions relation to logistic regression multiclass svms svms for regression computational learning theory relevance vector machines rvm for regression analysis of sparsity rvm for classification exercises contents xvii graphical models bayesian networks example polynomial regression generative models discrete variables linear-gaussian models conditional independence three example graphs d-separation markov random fields inference on a chain conditional independence properties factorization properties illustration image de-noising relation to directed graphs inference in graphical models trees the sum-product algorithm the max-sum algorithm exact inference in general graphs loopy belief propagation learning the graph structure factor graphs exercises mixture models and em k-means clustering image segmentation and compression mixtures of gaussians maximum likelihood em for gaussian mixtures an alternative view of em gaussian mixtures revisited relation to k-means mixtures of bernoulli distributions em for bayesian linear regression the em algorithm in general exercises approximate inference variational inference factorized distributions properties of factorized approximations example the univariate gaussian model comparison illustration variational mixture of gaussians xviii contents variational distribution variational lower bound predictive density determining the number of components induced factorizations variational linear regression variational distribution predictive distribution lower bound exponential family distributions variational message passing local variational methods variational logistic regression variational posterior distribution optimizing the variational parameters inference of hyperparameters expectation propagation example the clutter problem expectation propagation on graphs exercises sampling methods basic sampling algorithms standard distributions rejection sampling adaptive rejection sampling importance sampling sampling-importance-resampling sampling and the em algorithm markov chain monte carlo markov chains the metropolis-hastings algorithm gibbs sampling slice sampling the hybrid monte carlo algorithm dynamical systems hybrid monte carlo estimating the partition function exercises continuous latent variables principal component analysis maximum variance formulation minimum-error formulation applications of pca pca for high-dimensional data contents xix probabilistic pca maximum likelihood pca em algorithm for pca bayesian pca factor analysis kernel pca nonlinear latent variable models independent component analysis autoassociative neural networks modelling nonlinear manifolds exercises sequential data markov models hidden markov models maximum likelihood for the hmm the forward-backward algorithm the sum-product algorithm for the hmm scaling factors the viterbi algorithm extensions of the hidden markov model linear dynamical systems inference in lds learning in lds extensions of lds particle filters exercises combining models bayesian model averaging committees boosting minimizing exponential error error functions for boosting tree-based models conditional mixture models mixtures of linear regression models mixtures of logistic models mixtures of experts exercises appendix a data sets appendix b probability distributions appendix c properties of matrices xx contents appendix d calculus of variations appendix e lagrange multipliers references index introduction the problem of searching for patterns in data is a fundamental one and has a long and successful history. for instance the extensive astronomical observations of tycho brahe in the century allowed johannes kepler to discover the empirical laws of planetary motion which in turn provided a springboard for the development of classical mechanics. similarly the discovery of regularities in atomic spectra played a key role in the development and verification of quantum physics in the early twentieth century. the field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. consider the example of recognizing handwritten digits illustrated in figure each digit corresponds to a pixel image and so can be represented by a vector x comprising real numbers. the goal is to build a machine that will take such a vector x as input and that will produce the identity of the digit as the output. this is a nontrivial problem due to the wide variability of handwriting. it could be introduction figure examples of hand-written dig its taken from us zip codes. tackled using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on and invariably gives poor results. far better results can be obtained by adopting a machine learning approach in which a large set of n digits xn called a training set is used to tune the parameters of an adaptive model. the categories of the digits in the training set are known in advance typically by inspecting them individually and hand-labelling them. we can express the category of a digit using target vector t which represents the identity of the corresponding digit. suitable techniques for representing categories in terms of vectors will be discussed later. note that there is one such target vector t for each digit image x. the result of running the machine learning algorithm can be expressed as a function yx which takes a new digit image x as input and that generates an output vector y encoded in the same way as the target vectors. the precise form of the function yx is determined during the training phase also known as the learning phase on the basis of the training data. once the model is trained it can then determine the identity of new digit images which are said to comprise a test set. the ability to categorize correctly new examples that differ from those used for training is known as generalization. in practical applications the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors and so generalization is a central goal in pattern recognition. for most practical applications the original input variables are typically preprocessed to transform them into some new space of variables where it is hoped the pattern recognition problem will be easier to solve. for instance in the digit recognition problem the images of the digits are typically translated and scaled so that each digit is contained within a box of a fixed size. this greatly reduces the variability within each digit class because the location and scale of all the digits are now the same which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes. this pre-processing stage is sometimes also called feature extraction. note that new test data must be pre-processed using the same steps as the training data. pre-processing might also be performed in order to speed up computation. for example if the goal is real-time face detection in a high-resolution video stream the computer must handle huge numbers of pixels per second and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible. instead the aim is to find useful features that are fast to compute and yet that introduction also preserve useful discriminatory information enabling faces to be distinguished from non-faces. these features are then used as the inputs to the pattern recognition algorithm. for instance the average value of the image intensity over a rectangular subregion can be evaluated extremely efficiently and jones and a set of such features can prove very effective in fast face detection. because the number of such features is smaller than the number of pixels this kind of pre-processing represents a form of dimensionality reduction. care must be taken during pre-processing because often information is discarded and if this information is important to the solution of the problem then the overall accuracy of the system can suffer. applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. cases such as the digit recognition example in which the aim is to assign each input vector to one of a finite number of discrete categories are called classification problems. if the desired output consists of one or more continuous variables then the task is called regression. an example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants the temperature and the pressure. in other pattern recognition problems the training data consists of a set of input vectors x without any corresponding target values. the goal in such unsupervised learning problems may be to discover groups of similar examples within the data where it is called clustering or to determine the distribution of data within the input space known as density estimation or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization. finally the technique of reinforcement learning and barto is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward. here the learning algorithm is not given examples of optimal outputs in contrast to supervised learning but must instead discover them by a process of trial and error. typically there is a sequence of states and actions in which the learning algorithm is interacting with its environment. in many cases the current action not only affects the immediate reward but also has an impact on the reward at all subsequent time steps. for example by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon to a high standard here the network must learn to take a board position as input along with the result of a dice throw and produce a strong move as the output. this is done by having the network play against a copy of itself for perhaps a million games. a major challenge is that a game of backgammon can involve dozens of moves and yet it is only at the end of the game that the reward in the form of victory is achieved. the reward must then be attributed appropriately to all of the moves that led to it even though some moves will have been good ones and others less so. this is an example of a credit assignment problem. a general feature of reinforcement learning is the trade-off between exploration in which the system tries out new kinds of actions to see how effective they are and exploitation in which the system makes use of actions that are known to yield a high reward. too strong a focus on either exploration or exploitation will yield poor results. reinforcement learning continues to be an active area of machine learning research. however a introduction figure plot of a training data set of n points shown as blue circles each comprising an observation of the input variable x along with the corresponding target variable t. the green curve shows the function x used to generate the data. our goal is to predict the value of t for some new value of x without knowledge of the green curve. t x detailed treatment lies beyond the scope of this book. although each of these tasks needs its own tools and techniques many of the key ideas that underpin them are common to all such problems. one of the main goals of this chapter is to introduce in a relatively informal way several of the most important of these concepts and to illustrate them using simple examples. later in the book we shall see these same ideas re-emerge in the context of more sophisticated models that are applicable to real-world pattern recognition applications. this chapter also provides a self-contained introduction to three important tools that will be used throughout the book namely probability theory decision theory and information theory. although these might sound like daunting topics they are in fact straightforward and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications. example polynomial curve fitting we begin by introducing a simple regression problem which we shall use as a running example throughout this chapter to motivate a number of key concepts. suppose we observe a real-valued input variable x and we wish to use this observation to predict the value of a real-valued target variable t. for the present purposes it is instructive to consider an artificial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model. the data for this example is generated from the function x with random noise included in the target values as described in detail in appendix a. now suppose that we are given a training set comprising n observations of x written x xn together with corresponding observations of the values of t denoted t tnt. figure shows a plot of a training set comprising n data points. the input data set x in figure was generated by choosing values of xn for n n spaced uniformly in range and the target data set t was obtained by first computing the corresponding values of the function example polynomial curve fitting x and then adding a small level of random noise having a gaussian distribution gaussian distribution is discussed in section to each such point in order to obtain the corresponding value tn. by generating data in this way we are capturing a property of many real data sets namely that they possess an underlying regularity which we wish to learn but that individual observations are corrupted by random noise. this noise might arise from intrinsically stochastic random processes such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved. of the target variable for some new of the input variable. as we shall see set. furthermore the observed data are corrupted with noise and so for a there is uncertainty as to the appropriate value probability theory discussed later this involves implicitly trying to discover the underlying function x. this is intrinsically a difficult problem as we have to generalize from a finite data our goal is to exploit this training set in order to make predictions of the value in section provides a framework for expressing such uncertainty in a precise and quantitative manner and decision theory discussed in section allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria. for the moment however we shall proceed rather informally and consider a simple approach based on curve fitting. in particular we shall fit the data using a polynomial function of the form yx w wm xm wjxj where m is the order of the polynomial and xj denotes x raised to the power of j. the polynomial coefficients wm are collectively denoted by the vector w. note that although the polynomial function yx w is a nonlinear function of x it is a linear function of the coefficients w. functions such as the polynomial which are linear in the unknown parameters have important properties and are called linear models and will be discussed extensively in chapters and the values of the coefficients will be determined by fitting the polynomial to the training data. this can be done by minimizing an error function that measures the misfit between the function yx w for any given value of w and the training set data points. one simple choice of error function which is widely used is given by the sum of the squares of the errors between the predictions yxn w for each data point xn and the corresponding target values tn so that we minimize ew w where the factor of is included for later convenience. we shall discuss the motivation for this choice of error function later in this chapter. for the moment we simply note that it is a nonnegative quantity that would be zero if and only if the introduction figure the error function corresponds to half of the sum of the squares of the displacements by the vertical green bars of each data point from the function yx w. t tn yxn w xn x exercise function yx w were to pass exactly through each training data point. the geometrical interpretation of the sum-of-squares error function is illustrated in figure we can solve the curve fitting problem by choosing the value of w for which ew is as small as possible. because the error function is a quadratic function of the coefficients w its derivatives with respect to the coefficients will be linear in the elements of w and so the minimization of the error function has a unique solution denoted by which can be found in closed form. the resulting polynomial is given by the function yx there remains the problem of choosing the order m of the polynomial and as we shall see this will turn out to be an example of an important concept called model comparison or model selection. in figure we show four examples of the results of fitting polynomials having orders m and to the data set shown in figure we notice that the constant and first order polynomials give rather poor fits to the data and consequently rather poor representations of the function x. the third order polynomial seems to give the best fit to the function x of the examples shown in figure when we go to a much higher order polynomial we obtain an excellent fit to the training data. in fact the polynomial passes exactly through each data point and however the fitted curve oscillates wildly and gives a very poor representation of the function x. this latter behaviour is known as over-fitting. as we have noted earlier the goal is to achieve good generalization by making accurate predictions for new data. we can obtain some quantitative insight into the dependence of the generalization performance on m by considering a separate test set comprising data points generated using exactly the same procedure used to generate the training set points but with new choices for the random noise values included in the target values. for each choice of m we can then evaluate the residual value of given by for the training data and we can also evaluate for the test data set. it is sometimes more convenient to use the root-mean-square t t example polynomial curve fitting m x m m t x m t x x figure plots of polynomials having various orders m shown as red curves fitted to the data set shown in figure error defined by erms in which the division by n allows us to compare different sizes of data sets on an equal footing and the square root ensures that erms is measured on the same scale in the same units as the target variable t. graphs of the training and test set rms errors are shown for various values of m in figure the test set error is a measure of how well we are doing in predicting the values of t for new data observations of x. we note from figure that small values of m give relatively large values of the test set error and this can be attributed to the fact that the corresponding polynomials are rather inflexible and are incapable of capturing the oscillations in the function x. values of m in the range m give small values for the test set error and these also give reasonable representations of the generating function x as can be seen for the case of m from figure introduction figure graphs of the root-mean-square error defined by evaluated on the training set and on an independent test set for various values of m. training test s m r e m for m the training set error goes to zero as we might expect because this polynomial contains degrees of freedom corresponding to the coefficients and so can be tuned exactly to the data points in the training set. however the test set error has become very large and as we saw in figure the corresponding function yx exhibits wild oscillations. this may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases. the m polynomial is therefore capable of generating results at least as good as the m polynomial. furthermore we might suppose that the best predictor of new data would be the function x from which the data was generated we shall see later that this is indeed the case. we know that a power series expansion of the function x contains terms of all orders so we might expect that results should improve monotonically as we increase m. we can gain some insight into the problem by examining the values of the coefficients obtained from polynomials of various order as shown in table we see that as m increases the magnitude of the coefficients typically gets larger. in particular for the m polynomial the coefficients have become finely tuned to the data by developing large positive and negative values so that the correspond table table of the coefficients for polynomials of various order. observe how the typical magnitude of the coefficients increases dramatically as the order of the polynomial increases. m m m m example polynomial curve fitting t n t n x x figure plots of the solutions obtained by minimizing the sum-of-squares error function using the m polynomial for n data points plot and n data points plot. we see that increasing the size of the data set reduces the over-fitting problem. ing polynomial function matches each of the data points exactly but between data points near the ends of the range the function exhibits the large oscillations observed in figure intuitively what is happening is that the more flexible polynomials with larger values of m are becoming increasingly tuned to the random noise on the target values. it is also interesting to examine the behaviour of a given model as the size of the data set is varied as shown in figure we see that for a given model complexity the over-fitting problem become less severe as the size of the data set increases. another way to say this is that the larger the data set the more complex other words more flexible the model that we can afford to fit to the data. one rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple or of the number of adaptive parameters in the model. however as we shall see in chapter the number of parameters is not necessarily the most appropriate measure of model complexity. also there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set. it would seem more reasonable to choose the complexity of the model according to the complexity of the problem being solved. we shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood in section and that the over-fitting problem can be understood as a general property of maximum likelihood. by adopting a bayesian approach the over-fitting problem can be avoided. we shall see that there is no difficulty from a bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points. indeed in a bayesian model the effective number of parameters adapts automatically to the size of the data set. for the moment however it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where we section introduction t ln t ln x x figure plots of m polynomials fitted to the data set shown in figure using the regularized error function for two values of the regularization parameter corresponding to ln and ln the case of no regularizer i.e. corresponding to ln is shown at the bottom right of figure may wish to use relatively complex and flexible models. one technique that is often used to control the over-fitting phenomenon in such cases is that of regularization which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values. the simplest such penalty term takes the form of a sum of squares of all of the coefficients leading to a modified error function of the form w where wtw m and the coefficient governs the relative importance of the regularization term compared with the sum-of-squares error term. note that often the coefficient is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable et al. or it may be included but with its own regularization coefficient shall discuss this topic in more detail in section again the error function in can be minimized exactly in closed form. techniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefficients. the particular case of a quadratic regularizer is called ridge regression and kennard in the context of neural networks this approach is known as weight decay. figure shows the results of fitting the polynomial of order m to the same data set as before but now using the regularized error function given by we see that for a value of ln the over-fitting has been suppressed and we now obtain a much closer representation of the underlying function x. if however we use too large a value for then we again obtain a poor fit as shown in figure for ln the corresponding coefficients from the fitted polynomials are given in table showing that regularization has the desired effect of reducing exercise example polynomial curve fitting table table of the coefficients for m polynomials with various values for the regularization parameter note that ln corresponds to a model with no regularization i.e. to the graph at the bottom right in figure we see that as the value of increases the typical magnitude of the coefficients gets smaller. ln ln ln the magnitude of the coefficients. the impact of the regularization term on the generalization error can be seen by plotting the value of the rms error for both training and test sets against ln as shown in figure we see that in effect now controls the effective complexity of the model and hence determines the degree of over-fitting. the issue of model complexity is an important one and will be discussed at length in section here we simply note that if we were trying to solve a practical application using this approach of minimizing an error function we would have to find a way to determine a suitable value for the model complexity. the results above suggest a simple way of achieving this namely by taking the available data and partitioning it into a training set used to determine the coefficients w and a separate validation set also called a hold-out set used to optimize the model complexity m or in many cases however this will prove to be too wasteful of valuable training data and we have to seek more sophisticated approaches. so far our discussion of polynomial curve fitting has appealed largely to intuition. we now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory. as well as providing the foundation for nearly all of the subsequent developments in this book it will also section figure graph of the root-mean-square error versus ln for the m polynomial. training test s m r e ln introduction give us some important insights into the concepts we have introduced in the context of polynomial curve fitting and will allow us to extend these to more complex situations. probability theory a key concept in the field of pattern recognition is that of uncertainty. it arises both through noise on measurements as well as through the finite size of data sets. probability theory provides a consistent framework for the quantification and manipulation of uncertainty and forms one of the central foundations for pattern recognition. when combined with decision theory discussed in section it allows us to make optimal predictions given all the information available to us even though that information may be incomplete or ambiguous. we will introduce the basic concepts of probability theory by considering a simple example. imagine we have two boxes one red and one blue and in the red box we have apples and oranges and in the blue box we have apples and orange. this is illustrated in figure now suppose we randomly pick one of the boxes and from that box we randomly select an item of fruit and having observed which sort of fruit it is we replace it in the box from which it came. we could imagine repeating this process many times. let us suppose that in so doing we pick the red box of the time and we pick the blue box of the time and that when we remove an item of fruit from a box we are equally likely to select any of the pieces of fruit in the box. in this example the identity of the box that will be chosen is a random variable which we shall denote by b. this random variable can take one of two possible values namely r to the red box or b to the blue box. similarly the identity of the fruit is also a random variable and will be denoted by f it can take either of the values a apple or o orange. to begin with we shall define the probability of an event to be the fraction of times that event occurs out of the total number of trials in the limit that the total number of trials goes to infinity. thus the probability of selecting the red box is figure we use a simple example of two coloured boxes each containing fruit shown in green and oranges shown in orange to introduce the basic ideas of probability. probability theory figure we can derive the sum and product rules of probability by considering two random variables x which takes the values where i m and y which takes the values where j l. in this illustration we have m and l if we consider a total number n of instances of these variables then we denote the number of instances where x xi and y yj by nij which is the number of points in the corresponding cell of the array. the number of points in column i corresponding to x xi is denoted by ci and the number of points in row j corresponding to y yj is denoted by rj. yj ci nij xi rj and the probability of selecting the blue box is we write these probabilities as pb r and pb b note that by definition probabilities must lie in the interval also if the events are mutually exclusive and if they include all possible outcomes instance in this example the box must be either red or blue then we see that the probabilities for those events must sum to one. we can now ask questions such as what is the overall probability that the selection procedure will pick an apple? or given that we have chosen an orange what is the probability that the box we chose was the blue one? we can answer questions such as these and indeed much more complex questions associated with problems in pattern recognition once we have equipped ourselves with the two elementary rules of probability known as the sum rule and the product rule. having obtained these rules we shall then return to our boxes of fruit example. in order to derive the rules of probability consider the slightly more general example shown in figure involving two random variables x and y could for instance be the box and fruit variables considered above. we shall suppose that x can take any of the values xi where i m and y can take the values yj where j l. consider a total of n trials in which we sample both of the variables x and y and let the number of such trials in which x xi and y yj be nij. also let the number of trials in which x takes the value xi of the value that y takes be denoted by ci and similarly let the number of trials in which y takes the value yj be denoted by rj. the probability that x will take the value xi and y will take the value yj is written px xi y yj and is called the joint probability of x xi and y yj. it is given by the number of points falling in the cell ij as a fraction of the total number of points and hence px xi y yj nij n here we are implicitly considering the limit n similarly the probability that x takes the value xi irrespective of the value of y is written as px xi and is given by the fraction of the total number of points that fall in column i so that px xi ci n because the number of instances in column i in figure is just the sum of the number of instances in each cell of that column we have ci j nij and therefore introduction from and we have px xi px xi y yj which is the sum rule of probability. note that px xi is sometimes called the marginal probability because it is obtained by marginalizing or summing out the other variables this case y if we consider only those instances for which x xi then the fraction of such instances for which y yj is written py yjx xi and is called the conditional probability of y yj given x xi. it is obtained by finding the fraction of those points in column i that fall in cell ij and hence is given by py yjx xi nij ci from and we can then derive the following relationship px xi y yj nij n nij ci ci n py yjx xipx xi which is the product rule of probability. so far we have been quite careful to make a distinction between a random variable such as the box b in the fruit example and the values that the random variable can take for example r if the box were the red one. thus the probability that b takes the value r is denoted pb r. although this helps to avoid ambiguity it leads to a rather cumbersome notation and in many cases there will be no need for such pedantry. instead we may simply write pb to denote a distribution over the random variable b or pr to denote the distribution evaluated for the particular value r provided that the interpretation is clear from the context. with this more compact notation we can write the two fundamental rules of probability theory in the following form. the rules of probability sum rule px y px y product rule px y py here px y is a joint probability and is verbalized as the probability of x and y similarly the quantity py is a conditional probability and is verbalized as the probability of y given x whereas the quantity px is a marginal probability probability theory and is simply the probability of x these two simple rules form the basis for all of the probabilistic machinery that we use throughout this book. from the product rule together with the symmetry property px y py x we immediately obtain the following relationship between conditional probabilities py pxy px which is called bayes theorem and which plays a central role in pattern recognition and machine learning. using the sum rule the denominator in bayes theorem can be expressed in terms of the quantities appearing in the numerator px pxy y we can view the denominator in bayes theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of over all values of y equals one. in figure we show a simple example involving a joint distribution over two variables to illustrate the concept of marginal and conditional distributions. here a finite sample of n data points has been drawn from the joint distribution and is shown in the top left. in the top right is a histogram of the fractions of data points having each of the two values of y from the definition of probability these fractions would equal the corresponding probabilities py in the limit n we can view the histogram as a simple way to model a probability distribution given only a finite number of points drawn from that distribution. modelling distributions from data lies at the heart of statistical pattern recognition and will be explored in great detail in this book. the remaining two plots in figure show the corresponding histogram estimates of px and pxy let us now return to our example involving boxes of fruit. for the moment we shall once again be explicit about distinguishing between the random variables and their instantiations. we have seen that the probabilities of selecting either the red or the blue boxes are given by pb r pb b respectively. note that these satisfy pb r pb b now suppose that we pick a box at random and it turns out to be the blue box. then the probability of selecting an apple is just the fraction of apples in the blue box which is and so pf ab b in fact we can write out all four conditional probabilities for the type of fruit given the selected box pf ab r pf ob r pf ab b pf ob b introduction px y py y y x px pxy x x figure an illustration of a distribution over two variables x which takes possible values and y which takes two possible values. the top left figure shows a sample of points drawn from a joint probability distribution over these variables. the remaining figures show histogram estimates of the marginal distributions px and py as well as the conditional distribution pxy corresponding to the bottom row in the top left figure. again note that these probabilities are normalized so that pf ab r pf ob r and similarly pf ab b pf ob b we can now use the sum and product rules of probability to evaluate the overall probability of choosing an apple pf a pf ab rpb r pf ab bpb b from which it follows using the sum rule that pf o probability theory suppose instead we are told that a piece of fruit has been selected and it is an orange and we would like to know which box it came from. this requires that we evaluate the probability distribution over boxes conditioned on the identity of the fruit whereas the probabilities in give the probability distribution over the fruit conditioned on the identity of the box. we can solve the problem of reversing the conditional probability by using bayes theorem to give from the sum rule it then follows that pb bf o pb rf o pf ob rpb r pf o we can provide an important interpretation of bayes theorem as follows. if we had been asked which box had been chosen before being told the identity of the selected item of fruit then the most complete information we have available is provided by the probability pb. we call this the prior probability because it is the probability available before we observe the identity of the fruit. once we are told that the fruit is an orange we can then use bayes theorem to compute the probability pbf which we shall call the posterior probability because it is the probability obtained after we have observed f note that in this example the prior probability of selecting the red box was so that we were more likely to select the blue box than the red one. however once we have observed that the piece of selected fruit is an orange we find that the posterior probability of the red box is now so that it is now more likely that the box we selected was in fact the red one. this result accords with our intuition as the proportion of oranges is much higher in the red box than it is in the blue box and so the observation that the fruit was an orange provides significant evidence favouring the red box. in fact the evidence is sufficiently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one. finally we note that if the joint distribution of two variables factorizes into the product of the marginals so that px y pxpy then x and y are said to be independent. from the product rule we see that py py and so the conditional distribution of y given x is indeed independent of the value of x. for instance in our boxes of fruit example if each box contained the same fraction of apples and oranges then pfb p so that the probability of selecting say an apple is independent of which box is chosen. probability densities as well as considering probabilities defined over discrete sets of events we also wish to consider probabilities with respect to continuous variables. we shall limit ourselves to a relatively informal discussion. if the probability of a real-valued variable x falling in the interval x x is given by px x for x then px is called the probability density over x. this is illustrated in figure the probability that x will lie in an interval b is then given by b px b px dx. a introduction figure the concept of probability for discrete variables can be extended to that of a probability density px over a continuous variable x and is such that the probability of x lying in the interval x x is given by px x for x the probability density can be expressed as the derivative of a cumulative distribution function p px p x x because probabilities are nonnegative and because the value of x must lie somewhere on the real axis the probability density px must satisfy the two conditions px px dx under a nonlinear change of variable a probability density transforms differently from a simple function due to the jacobian factor. for instance if we consider a change of variables x gy then a function fx fgy. now consider a probability density pxx that corresponds to a density pyy with respect to the new variable y where the suffices denote the fact that pxx and pyy are different densities. observations falling in the range x x will for small values of x be transformed into the range y y where pxx x pyy y and hence dx pyy pxx dy pxgyg exercise one consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable. the probability that x lies in the interval z is given by the cumulative distribution function defined by z p px dx which satisfies p px as shown in figure if we have several continuous variables xd denoted collectively by the vector x then we can define a joint probability density px xd such probability theory that the probability of x falling in an infinitesimal volume x containing the point x is given by px x. this multivariate probability density must satisfy px px dx in which the integral is taken over the whole of x space. we can also consider joint probability distributions over a combination of discrete and continuous variables. note that if x is a discrete variable then px is sometimes called a probability mass function because it can be regarded as a set of probability masses concentrated at the allowed values of x. the sum and product rules of probability as well as bayes theorem apply equally to the case of probability densities or to combinations of discrete and continuous variables. for instance if x and y are two real variables then the sum and product rules take the form px px y dy px y pyxpx. a formal justification of the sum and product rules for continuous variables requires a branch of mathematics called measure theory and lies outside the scope of this book. its validity can be seen informally however by dividing each real variable into intervals of width and considering the discrete probability distribution over these intervals. taking the limit then turns sums into integrals and gives the desired result. expectations and covariances one of the most important operations involving probabilities is that of finding weighted averages of functions. the average value of some function fx under a probability distribution px is called the expectation of fx and will be denoted by ef. for a discrete distribution it is given by ef pxfx x so that the average is weighted by the relative probabilities of the different values of x. in the case of continuous variables expectations are expressed in terms of an integration with respect to the corresponding probability density ef pxfx dx. in either case if we are given a finite number n of points drawn from the probability distribution or probability density then the expectation can be approximated as a introduction finite sum over these points ef n fxn. we shall make extensive use of this result when we discuss sampling methods in chapter the approximation in becomes exact in the limit n sometimes we will be considering expectations of functions of several variables in which case we can use a subscript to indicate which variable is being averaged over so that for instance denotes the average of the function fx y with respect to the distribution of x. note that exfx y will be a function of y. exfx y we can also consider a conditional expectation with respect to a conditional distribution so that exfy pxyfx x with an analogous definition for continuous variables. the variance of fx is defined by varf e exercise exercise and provides a measure of how much variability there is in fx around its mean value efx. expanding out the square we see that the variance can also be written in terms of the expectations of fx and varf in particular we can consider the variance of the variable x itself which is given by varx for two random variables x and y the covariance is defined by covx y exy exy ey exyxy exey which expresses the extent to which x and y vary together. if x and y are independent then their covariance vanishes. in the case of two vectors of random variables x and y the covariance is a matrix exyt covx y exy exyxyt exeyt. if we consider the covariance of the components of a vector x with each other then we use a slightly simpler notation covx covx x. probability theory bayesian probabilities so far in this chapter we have viewed probabilities in terms of the frequencies of random repeatable events. we shall refer to this as the classical or frequentist interpretation of probability. now we turn to the more general bayesian view in which probabilities provide a quantification of uncertainty. consider an uncertain event for example whether the moon was once in its own orbit around the sun or whether the arctic ice cap will have disappeared by the end of the century. these are not events that can be repeated numerous times in order to define a notion of probability as we did earlier in the context of boxes of fruit. nevertheless we will generally have some idea for example of how quickly we think the polar ice is melting. if we now obtain fresh evidence for instance from a new earth observation satellite gathering novel forms of diagnostic information we may revise our opinion on the rate of ice loss. our assessment of such matters will affect the actions we take for instance the extent to which we endeavour to reduce the emission of greenhouse gasses. in such circumstances we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence as well as subsequently to be able to take optimal actions or decisions as a consequence. this can all be achieved through the elegant and very general bayesian interpretation of probability. the use of probability to represent uncertainty however is not an ad-hoc choice but is inevitable if we are to respect common sense while making rational coherent inferences. for instance cox showed that if numerical values are used to represent degrees of belief then a simple set of axioms encoding common sense properties of such beliefs leads uniquely to a set of rules for manipulating degrees of belief that are equivalent to the sum and product rules of probability. this provided the first rigorous proof that probability theory could be regarded as an extension of boolean logic to situations involving uncertainty numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy good savage definetti lindley in each case the resulting numerical quantities behave precisely according to the rules of probability. it is therefore natural to refer to these quantities as probabilities. in the field of pattern recognition too it is helpful to have a more general no thomas bayes thomas bayes was born in tunbridge wells and was a clergyman as well as an amateur scientist and a mathematician. he studied logic and theology at edinburgh university and was elected fellow of the royal society in during the century issues regarding probability arose in connection with gambling and with the new concept of insurance. one particularly important problem concerned so-called inverse probability. a solution was proposed by thomas bayes in his paper essay towards solving a problem in the doctrine of chances which was published in some three years after his death in the philosophical transactions of the royal society. in fact bayes only formulated his theory for the case of a uniform prior and it was pierre-simon laplace who independently rediscovered the theory in general form and who demonstrated its broad applicability. introduction tion of probability. consider the example of polynomial curve fitting discussed in section it seems reasonable to apply the frequentist notion of probability to the random values of the observed variables tn. however we would like to address and quantify the uncertainty that surrounds the appropriate choice for the model parameters w. we shall see that from a bayesian perspective we can use the machinery of probability theory to describe the uncertainty in model parameters such as w or indeed in the choice of model itself. bayes theorem now acquires a new significance. recall that in the boxes of fruit example the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one. in that example bayes theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data. as we shall see in detail later we can adopt a similar approach when making inferences about quantities such as the parameters w in the polynomial curve fitting example. we capture our assumptions about w before observing the data in the form of a prior probability distribution pw. the effect of the observed data d tn is expressed through the conditional probability pdw and we shall see later in section how this can be represented explicitly. bayes theorem which takes the form pwd pdwpw pd then allows us to evaluate the uncertainty in w after we have observed d in the form of the posterior probability pwd. the quantity pdw on the right-hand side of bayes theorem is evaluated for the observed data set d and can be viewed as a function of the parameter vector w in which case it is called the likelihood function. it expresses how probable the observed data set is for different settings of the parameter vector w. note that the likelihood is not a probability distribution over w and its integral with respect to w does not equal one. given this definition of likelihood we can state bayes theorem in words posterior likelihood prior where all of these quantities are viewed as functions of w. the denominator in is the normalization constant which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one. indeed integrating both sides of with respect to w we can express the denominator in bayes theorem in terms of the prior distribution and the likelihood function pd pdwpw dw. in both the bayesian and frequentist paradigms the likelihood function pdw plays a central role. however the manner in which it is used is fundamentally different in the two approaches. in a frequentist setting w is considered to be a fixed parameter whose value is determined by some form of estimator and error bars probability theory on this estimate are obtained by considering the distribution of possible data sets d. by contrast from the bayesian viewpoint there is only a single data set d the one that is actually observed and the uncertainty in the parameters is expressed through a probability distribution over w. a widely used frequentist estimator is maximum likelihood in which w is set to the value that maximizes the likelihood function pdw. this corresponds to choosing the value of w for which the probability of the observed data set is maximized. in the machine learning literature the negative log of the likelihood function is called an error function. because the negative logarithm is a monotonically decreasing function maximizing the likelihood is equivalent to minimizing the error. one approach to determining frequentist error bars is the bootstrap hastie et al. in which multiple data sets are created as follows. suppose our original data set consists of n data points x xn. we can create a new data set xb by drawing n points at random from x with replacement so that some points in x may be replicated in xb whereas other points in x may be absent from xb. this process can be repeated l times to generate l data sets each of size n and each obtained by sampling from the original data set x. the statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets. one advantage of the bayesian viewpoint is that the inclusion of prior knowledge arises naturally. suppose for instance that a fair-looking coin is tossed three times and lands heads each time. a classical maximum likelihood estimate of the probability of landing heads would give implying that all future tosses will land heads! by contrast a bayesian approach with any reasonable prior will lead to a much less extreme conclusion. there has been much controversy and debate associated with the relative merits of the frequentist and bayesian paradigms which have not been helped by the fact that there is no unique frequentist or even bayesian viewpoint. for instance one common criticism of the bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a reflection of any prior beliefs. even the subjective nature of the conclusions through their dependence on the choice of prior is seen by some as a source of difficulty. reducing the dependence on the prior is one motivation for so-called noninformative priors. however these lead to difficulties when comparing different models and indeed bayesian methods based on poor choices of prior can give poor results with high confidence. frequentist evaluation methods offer some protection from such problems and techniques such as cross-validation remain useful in areas such as model comparison. this book places a strong emphasis on the bayesian viewpoint reflecting the huge growth in the practical importance of bayesian methods in the past few years while also discussing useful frequentist concepts as required. although the bayesian framework has its origins in the century the practical application of bayesian methods was for a long time severely limited by the difficulties in carrying through the full bayesian procedure particularly the need to marginalize or integrate over the whole of parameter space which as we shall section section section introduction see is required in order to make predictions or to compare different models. the development of sampling methods such as markov chain monte carlo in chapter along with dramatic improvements in the speed and memory capacity of computers opened the door to the practical use of bayesian techniques in an impressive range of problem domains. monte carlo methods are very flexible and can be applied to a wide range of models. however they are computationally intensive and have mainly been used for small-scale problems. more recently highly efficient deterministic approximation schemes such as variational bayes and expectation propagation in chapter have been developed. these offer a complementary alternative to sampling methods and have allowed bayesian techniques to be used in large-scale applications et al. the gaussian distribution we shall devote the whole of chapter to a study of various probability distributions and their key properties. it is convenient however to introduce here one of the most important probability distributions for continuous variables called the normal or gaussian distribution. we shall make extensive use of this distribution in the remainder of this chapter and indeed throughout much of the book. for the case of a single real-valued variable x the gaussian distribution is de fined by x exp which is governed by two parameters called the mean and called the variance. the square root of the variance given by is called the standard deviation and the reciprocal of the variance written as is called the precision. we shall see the motivation for these terms shortly. figure shows a plot of the gaussian distribution. from the form of we see that the gaussian distribution satisfies n exercise also it is straightforward to show that the gaussian is normalized so that pierre-simon laplace it is said that laplace was seriously lacking in modesty and at one point declared himself to be the best mathematician in france at the time a claim that was arguably true. as well as being prolific in mathematics he also made numerous contributions to astronomy including the nebular hypothesis by which the earth is thought to have formed from the condensation and cooling of a large rotating disk of gas and dust. in he published the first edition of th eorie analytique des probabilit es in which laplace states that probability theory is nothing but common sense reduced to calculation this work included a discussion of the inverse probability calculation termed bayes theorem by poincar e which he used to solve problems in life expectancy jurisprudence planetary masses triangulation and error estimation. figure plot of the univariate gaussian showing the mean and the standard deviation n probability theory dx x x thus satisfies the two requirements for a valid probability density. we can readily find expectations of functions of x under the gaussian distribu tion. in particular the average value of x is given by x x exercise exercise ex x dx because the parameter represents the average value of x under the distribution it is referred to as the mean. similarly for the second order moment dx from and it follows that the variance of x is given by varx and hence is referred to as the variance parameter. the maximum of a distribution is known as its mode. for a gaussian the mode coincides with the mean. we are also interested in the gaussian distribution defined over a d-dimensional vector x of continuous variables which is given by n exp where the d-dimensional vector is called the mean the d d matrix is called the covariance and denotes the determinant of we shall make use of the multivariate gaussian distribution briefly in this chapter although its properties will be studied in detail in section introduction figure illustration of the likelihood function for a gaussian distribution shown by the red curve. here the black points denote a data set of values and the likelihood function given by corresponds to the product of the blue values. maximizing the likelihood involves adjusting the mean and variance of the gaussian so as to maximize this product. px n xn x now suppose that we have a data set of observations x xn representing n observations of the scalar variable x. note that we are using the typeface x to distinguish this from a single observation of the vector-valued variable xdt which we denote by x. we shall suppose that the observations are drawn independently from a gaussian distribution whose mean and variance are unknown and we would like to determine these parameters from the data set. data points that are drawn independently from the same distribution are said to be independent and identically distributed which is often abbreviated to i.i.d. we have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. because our data set x is i.i.d. we can therefore write the probability of the data set given and in the form px xn section when viewed as a function of and this is the likelihood function for the gaussian and is interpreted diagrammatically in figure one common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function. this might seem like a strange criterion because from our foregoing discussion of probability theory it would seem more natural to maximize the probability of the parameters given the data not the probability of the data given the parameters. in fact these two criteria are related as we shall discuss in the context of curve fitting. for the moment however we shall determine values for the unknown parameters and in the gaussian by maximizing the likelihood function in practice it is more convenient to maximize the log of the likelihood function. because the logarithm is a monotonically increasing function of its argument maximization of the log of a function is equivalent to maximization of the function itself. taking the log not only simplifies the subsequent mathematical analysis but it also helps numerically because the product of a large number of small probabilities can easily underflow the numerical precision of the computer and this is resolved by computing instead the sum of the log probabilities. from and the log likelihood exercise section exercise function can be written in the form x ln p probability theory n ln n maximizing with respect to we obtain the maximum likelihood solution given by ml which is the sample mean i.e. the mean of the observed values similarly maximizing with respect to we obtain the maximum likelihood solution for the variance in the form xn ml n n which is the sample variance measured with respect to the sample mean ml. note that we are performing a joint maximization of with respect to and but in the case of the gaussian distribution the solution for decouples from that for so that we can first evaluate and then subsequently use this result to evaluate later in this chapter and also in subsequent chapters we shall highlight the significant limitations of the maximum likelihood approach. here we give an indication of the problem in the context of our solutions for the maximum likelihood parameter settings for the univariate gaussian distribution. in particular we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution. this is an example of a phenomenon called bias and is related to the problem of over-fitting encountered in the context of polynomial curve fitting. we first note that the maximum likelihood solutions ml and ml are functions of the data set values xn consider the expectations of these quantities with respect to the data set values which themselves come from a gaussian distribution with parameters and it is straightforward to show that e ml e ml n n so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor the intuition behind this result is given by figure from it follows that the following estimate for the variance parameter is unbiased n n ml n introduction figure illustration of how bias arises in using maximum likelihood to determine the variance of a gaussian. the green curve shows the true gaussian distribution from which data is generated and the three red curves show the gaussian distributions obtained by fitting to three data sets each consisting of two data points shown in blue using the maximum likelihood results and averaged across the three data sets the mean is correct but the variance is systematically under-estimated because it is measured relative to the sample mean and not relative to the true mean. in section we shall see how this result arises automatically when we adopt a bayesian approach. note that the bias of the maximum likelihood solution becomes less significant as the number n of data points increases and in the limit n the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data. in practice for anything other than small n this bias will not prove to be a serious problem. however throughout this book we shall be interested in more complex models with many parameters for which the bias problems associated with maximum likelihood will be much more severe. in fact as we shall see the issue of bias in maximum likelihood lies at the root of the over-fitting problem that we encountered earlier in the context of polynomial curve fitting. curve fitting re-visited we have seen how the problem of polynomial curve fitting can be expressed in terms of error minimization. here we return to the curve fitting example and view it from a probabilistic perspective thereby gaining some insights into error functions and regularization as well as taking us towards a full bayesian treatment. section the goal in the curve fitting problem is to be able to make predictions for the target variable t given some new value of the input variable x on the basis of a set of training data comprising n input values x xn and their corresponding target values t tn we can express our uncertainty over the value of the target variable using a probability distribution. for this purpose we shall assume that given the value of x the corresponding value of t has a gaussian distribution with a mean equal to the value yx w of the polynomial curve given by thus we have ptx w tyx w where for consistency with the notation in later chapters we have defined a precision parameter corresponding to the inverse variance of the distribution. this is illustrated schematically in figure figure schematic illustration of a gaussian conditional distribution for t given x given by in which the mean is given by the polynomial function yx w and the precision is given by the parameter which is related to the variance by t w probability theory yx w w x we now use the training data t to determine the values of the unknown parameters w and by maximum likelihood. if the data are assumed to be drawn independently from the distribution then the likelihood function is given by ptx w tnyxn w as we did in the case of the simple gaussian distribution earlier it is convenient to maximize the logarithm of the likelihood function. substituting for the form of the gaussian distribution given by we obtain the log likelihood function in the form ln ptx w w n ln n consider first the determination of the maximum likelihood solution for the polynomial coefficients which will be denoted by wml. these are determined by maximizing with respect to w. for this purpose we can omit the last two terms on the right-hand side of because they do not depend on w. also we note that scaling the log likelihood by a positive constant coefficient does not alter the location of the maximum with respect to w and so we can replace the coefficient with finally instead of maximizing the log likelihood we can equivalently minimize the negative log likelihood. we therefore see that maximizing likelihood is equivalent so far as determining w is concerned to minimizing the sum-of-squares error function defined by thus the sum-of-squares error function has arisen as a consequence of maximizing likelihood under the assumption of a gaussian noise distribution. we can also use maximum likelihood to determine the precision parameter of the gaussian conditional distribution. maximizing with respect to gives wml ml n introduction section w wtw. again we can first determine the parameter vector wml governing the mean and subsequently use this to find the precision ml as was the case for the simple gaussian distribution. having determined the parameters w and we can now make predictions for new values of x. because we now have a probabilistic model these are expressed in terms of the predictive distribution that gives the probability distribution over t rather than simply a point estimate and is obtained by substituting the maximum likelihood parameters into to give tyx wml ml ptx wml ml now let us take a step towards a more bayesian approach and introduce a prior distribution over the polynomial coefficients w. for simplicity let us consider a gaussian distribution of the form exp wtw pw n where is the precision of the distribution and m is the total number of elements in the vector w for an m th order polynomial. variables such as which control the distribution of model parameters are called hyperparameters. using bayes theorem the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function pwx t ptx w we can now determine w by finding the most probable value of w given the data in other words by maximizing the posterior distribution. this technique is called maximum posterior or simply map. taking the negative logarithm of and combining with and we find that the maximum of the posterior is given by the minimum of thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function encountered earlier in the form with a regularization parameter given by bayesian curve fitting although we have included a prior distribution pw we are so far still making a point estimate of w and so this does not yet amount to a bayesian treatment. in a fully bayesian approach we should consistently apply the sum and product rules of probability which requires as we shall see shortly that we integrate over all values of w. such marginalizations lie at the heart of bayesian methods for pattern recognition. probability theory in the curve fitting problem we are given the training data x and t along with a new test point x and our goal is to predict the value of t. we therefore wish to evaluate the predictive distribution ptx x t. here we shall assume that the parameters and are fixed and known in advance later chapters we shall discuss how such parameters can be inferred from data in a bayesian setting. a bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability which allow the predictive distribution to be written in the form ptx x t ptx wpwx t dw. here ptx w is given by and we have omitted the dependence on and to simplify the notation. here pwx t is the posterior distribution over parameters and can be found by normalizing the right-hand side of we shall see in section that for problems such as the curve-fitting example this posterior distribution is a gaussian and can be evaluated analytically. similarly the integration in can also be performed analytically with the result that the predictive distribution is given by a gaussian of the form ptx x t tmx where the mean and variance are given by here the matrix s is given by mx s i where i is the unit matrix and we have defined the vector with elements ix xi for i m. we see that the variance as well as the mean of the predictive distribution in is dependent on x. the first term in represents the uncertainty in the predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution through ml. however the second term arises from the uncertainty in the parameters w and is a consequence of the bayesian treatment. the predictive distribution for the synthetic sinusoidal regression problem is illustrated in figure introduction figure the predictive distribution resulting from a bayesian treatment of polynomial curve fitting using an m polynomial with the fixed parameters and to the known noise variance in which the red curve denotes the mean of the predictive distribution and the red region corresponds to standard deviation around the mean. t x model selection in our example of polynomial curve fitting using least squares we saw that there was an optimal order of polynomial that gave the best generalization. the order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity. with regularized least squares the regularization coefficient also controls the effective complexity of the model whereas for more complex models such as mixture distributions or neural networks there may be multiple parameters governing complexity. in a practical application we need to determine the values of such parameters and the principal objective in doing so is usually to achieve the best predictive performance on new data. furthermore as well as finding the appropriate values for complexity parameters within a given model we may wish to consider a range of different types of model in order to find the best one for our particular application. we have already seen that in the maximum likelihood approach the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. if data is plentiful then one approach is simply to use some of the available data to train a range of models or a given model with a range of values for its complexity parameters and then to compare them on independent data sometimes called a validation set and select the one having the best predictive performance. if the model design is iterated many times using a limited size data set then some over-fitting to the validation data can occur and so it may be necessary to keep aside a third test set on which the performance of the selected model is finally evaluated. in many applications however the supply of data for training and testing will be limited and in order to build good models we wish to use as much of the available data as possible for training. however if the validation set is small it will give a relatively noisy estimate of predictive performance. one solution to this dilemma is to use cross-validation which is illustrated in figure this allows a proportion of the available data to be used for training while making use of all of the the curse of dimensionality figure the technique of s-fold cross-validation illustrated here for the case of s involves taking the available data and partitioning it into s groups the simplest case these are of equal size. then s of the groups are used to train a set of models that are then evaluated on the remaining group. this procedure is then repeated for all s possible choices for the held-out group indicated here by the red blocks and the performance scores from the s runs are then averaged. run run run run data to assess performance. when data is particularly scarce it may be appropriate to consider the case s n where n is the total number of data points which gives the leave-one-out technique. one major drawback of cross-validation is that the number of training runs that must be performed is increased by a factor of s and this can prove problematic for models in which the training is itself computationally expensive. a further problem with techniques such as cross-validation that use separate data to assess performance is that we might have multiple complexity parameters for a single model instance there might be several regularization parameters. exploring combinations of settings for such parameters could in the worst case require a number of training runs that is exponential in the number of parameters. clearly we need a better approach. ideally this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run. we therefore need to find a measure of performance which depends only on the training data and which does not suffer from bias due to over-fitting. historically various information criteria have been proposed that attempt to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over-fitting of more complex models. for example the akaike information criterion or aic chooses the model for which the quantity is largest. here pdwml is the best-fit log likelihood and m is the number of adjustable parameters in the model. a variant of this quantity called the bayesian information criterion or bic will be discussed in section such criteria do not take account of the uncertainty in the model parameters however and in practice they tend to favour overly simple models. we therefore turn in section to a fully bayesian approach where we shall see how complexity penalties arise in a natural and principled way. ln pdwml m the curse of dimensionality in the polynomial curve fitting example we had just one input variable x. for practical applications of pattern recognition however we will have to deal with spaces introduction figure scatter plot of the oil flow data for input variables and in which red denotes the homogenous class green denotes the annular class and blue denotes the laminar class. our goal is to classify the new test point denoted by of high dimensionality comprising many input variables. as we now discuss this poses some serious challenges and is an important factor influencing the design of pattern recognition techniques. in order to illustrate the problem we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil water and gas and james these three materials can be present in one of three different geometrical configurations known as homogenous annular and laminar and the fractions of the three materials can also vary. each data point comprises a input vector consisting of measurements taken with gamma ray densitometers that measure the attenuation of gamma rays passing along narrow beams through the pipe. this data set is described in detail in appendix a. figure shows points from this data set on a plot showing two of the measurements and remaining ten input values are ignored for the purposes of this illustration. each data point is labelled according to which of the three geometrical classes it belongs to and our goal is to use this data as a training set in order to be able to classify a new observation such as the one denoted by the cross in figure we observe that the cross is surrounded by numerous red points and so we might suppose that it belongs to the red class. however there are also plenty of green points nearby so we might think that it could instead belong to the green class. it seems unlikely that it belongs to the blue class. the intuition here is that the identity of the cross should be determined more strongly by nearby points from the training set and less strongly by more distant points. in fact this intuition turns out to be reasonable and will be discussed more fully in later chapters. how can we turn this intuition into a learning algorithm? one very simple approach would be to divide the input space into regular cells as indicated in figure when we are given a test point and we wish to predict its class we first decide which cell it belongs to and we then find all of the training data points that the curse of dimensionality figure illustration of a simple approach to the solution of a classification problem in which the input space is divided into cells and any new test point is assigned to the class that has a majority number of representatives in the same cell as the test point. as we shall see shortly this simplistic approach has some severe shortcomings. fall in the same cell. the identity of the test point is predicted as being the same as the class having the largest number of training points in the same cell as the test point ties being broken at random. there are numerous problems with this naive approach but one of the most severe becomes apparent when we consider its extension to problems having larger numbers of input variables corresponding to input spaces of higher dimensionality. the origin of the problem is illustrated in figure which shows that if we divide a region of a space into regular cells then the number of such cells grows exponentially with the dimensionality of the space. the problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty. clearly we have no hope of applying such a technique in a space of more than a few variables and so we need to find a more sophisticated approach. we can gain further insight into the problems of high-dimensional spaces by returning to the example of polynomial curve fitting and considering how we would section of figure illustration the curse of dimensionality showing how the number of regions of a regular grid grows exponentially with the dimensionality d of the space. for clarity only a subset of the cubical regions are shown for d d d d introduction extend this approach to deal with input spaces having several variables. if we have d input variables then a general polynomial with coefficients up to order would take the form yx w wixi wijxixj wijkxixjxk. exercise as d increases so the number of independent coefficients all of the coefficients are independent due to interchange symmetries amongst the x variables grows proportionally to in practice to capture complex dependencies in the data we may need to use a higher-order polynomial. for a polynomial of order m the growth in the number of coefficients is like dm although this is now a power law growth rather than an exponential growth it still points to the method becoming rapidly unwieldy and of limited practical utility. our geometrical intuitions formed through a life spent in a space of three dimensions can fail badly when we consider spaces of higher dimensionality. as a simple example consider a sphere of radius r in a space of d dimensions and ask what is the fraction of the volume of the sphere that lies between radius r and r we can evaluate this fraction by noting that the volume of a sphere of radius r in d dimensions must scale as rd and so we write vdr kdrd exercise where the constant kd depends only on d. thus the required fraction is given by exercise which is plotted as a function of for various values of d in figure we see that for large d this fraction tends to even for small values of thus in spaces of high dimensionality most of the volume of a sphere is concentrated in a thin shell near the surface! as a further example of direct relevance to pattern recognition consider the behaviour of a gaussian distribution in a high-dimensional space. if we transform from cartesian to polar coordinates and then integrate out the directional variables we obtain an expression for the density pr as a function of radius r from the origin. thus pr r is the probability mass inside a thin shell of thickness r located at radius r. this distribution is plotted for various values of d in figure and we see that for large d the probability mass of the gaussian is concentrated in a thin shell. the severe difficulty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality in this book we shall make extensive use of illustrative examples involving input spaces of one or two dimensions because this makes it particularly easy to illustrate the techniques graphically. the reader should be warned however that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions. figure plot of the fraction of the volume of a sphere lying in the range r to r for various values of the dimensionality d. the curse of dimensionality n o i t c a r f e m u o v l d d d d although the curse of dimensionality certainly raises important issues for pattern recognition applications it does not prevent us from finding effective techniques applicable to high-dimensional spaces. the reasons for this are twofold. first real data will often be confined to a region of the space having lower effective dimensionality and in particular the directions over which important variations in the target variables occur may be so confined. second real data will typically exhibit some smoothness properties least locally so that for the most part small changes in the input variables will produce small changes in the target variables and so we can exploit local interpolation-like techniques to allow us to make predictions of the target variables for new values of the input variables. successful pattern recognition techniques exploit one or both of these properties. consider for example an application in manufacturing in which images are captured of identical planar objects on a conveyor belt in which the goal is to determine their orientation. each image is a point figure plot of the probability density with to radius r of a gausrespect sian distribution for various values of in a high-dimensional space most of the probability mass of a gaussian is located within a thin shell at a specific radius. the dimensionality d. r p d d d r introduction in a high-dimensional space whose dimensionality is determined by the number of pixels. because the objects can occur at different positions within the image and in different orientations there are three degrees of freedom of variability between images and a set of images will live on a three dimensional manifold embedded within the high-dimensional space. due to the complex relationships between the object position or orientation and the pixel intensities this manifold will be highly nonlinear. if the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position then there is only one degree of freedom of variability within the manifold that is significant. decision theory we have seen in section how probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty. here we turn to a discussion of decision theory that when combined with probability theory allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition. suppose we have an input vector x together with a corresponding vector t of target variables and our goal is to predict t given a new value for x. for regression problems t will comprise continuous variables whereas for classification problems t will represent class labels. the joint probability distribution px t provides a complete summary of the uncertainty associated with these variables. determination of px t from a set of training data is an example of inference and is typically a very difficult problem whose solution forms the subject of much of this book. in a practical application however we must often make a specific prediction for the value of t or more generally take a specific action based on our understanding of the values t is likely to take and this aspect is the subject of decision theory. consider for example a medical diagnosis problem in which we have taken an x-ray image of a patient and we wish to determine whether the patient has cancer or not. in this case the input vector x is the set of pixel intensities in the image and output variable t will represent the presence of cancer which we denote by the class or the absence of cancer which we denote by the class we might for instance choose t to be a binary variable such that t corresponds to class and t corresponds to class we shall see later that this choice of label values is particularly convenient for probabilistic models. the general inference problem then involves determining the joint distribution pxck or equivalently px t which gives us the most complete probabilistic description of the situation. although this can be a very useful and informative quantity in the end we must decide either to give treatment to the patient or not and we would like this choice to be optimal in some appropriate sense and hart this is the decision step and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities. we shall see that the decision stage is generally very simple even trivial once we have solved the inference problem. here we give an introduction to the key ideas of decision theory as required for decision theory the rest of the book. further background as well as more detailed accounts can be found in berger and bather before giving a more detailed analysis let us first consider informally how we might expect probabilities to play a role in making decisions. when we obtain the x-ray image x for a new patient our goal is to decide which of the two classes to assign to the image. we are interested in the probabilities of the two classes given the image which are given by pckx. using bayes theorem these probabilities can be expressed in the form pckx pxckpck px note that any of the quantities appearing in bayes theorem can be obtained from the joint distribution pxck by either marginalizing or conditioning with respect to the appropriate variables. we can now interpret pck as the prior probability for the class ck and pckx as the corresponding posterior probability. thus represents the probability that a person has cancer before we take the x-ray measurement. similarly is the corresponding probability revised using bayes theorem in light of the information contained in the x-ray. if our aim is to minimize the chance of assigning x to the wrong class then intuitively we would choose the class having the higher posterior probability. we now show that this intuition is correct and we also discuss more general criteria for making decisions. minimizing the misclassification rate suppose that our goal is simply to make as few misclassifications as possible. we need a rule that assigns each value of x to one of the available classes. such a rule will divide the input space into regions rk called decision regions one for each class such that all points in rk are assigned to class ck. the boundaries between decision regions are called decision boundaries or decision surfaces. note that each decision region need not be contiguous but could comprise some number of disjoint regions. we shall encounter examples of decision boundaries and decision regions in later chapters. in order to find the optimal decision rule consider first of all the case of two classes as in the cancer problem for instance. a mistake occurs when an input vector belonging to class is assigned to class or vice versa. the probability of this occurring is given by pmistake px px dx dx. we are free to choose the decision rule that assigns each point x to one of the two classes. clearly to minimize pmistake we should arrange that each x is assigned to whichever class has the smaller value of the integrand in thus if for a given value of x then we should assign that x to class from the product rule of probability we have pxck pckxpx. because the factor px is common to both terms we can restate this result as saying that the minimum introduction x figure schematic illustration of the joint probabilities pxck for each of two classes plotted against x together with the decision boundary x bx. values of x bx are classified as class and hence belong to decision region whereas points x bx are classified as and belong to errors arise from the blue green and red regions so that for x bx the errors are due to points from class being misclassified as by the sum of the red and green regions and conversely for points in the region x bx the errors are due to points from class being misclassified as by the blue region. as we vary the location bx of the decision boundary the combined areas of the blue and green regions remains constant whereas the size of the red region varies. the optimal choice for bx is where the curves for and cross corresponding to bx because in this case the red region disappears. this is equivalent to the minimum misclassification rate decision rule which assigns each value of x to the class having the higher posterior probability pckx. probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability pckx is largest. this result is illustrated for two classes and a single input variable x in figure for the more general case of k classes it is slightly easier to maximize the probability of being correct which is given by rk px rkck pcorrect pxck dx which is maximized when the regions rk are chosen such that each x is assigned to the class for which pxck is largest. again using the product rule pxck pckxpx and noting that the factor of px is common to all terms we see that each x should be assigned to the class having the largest posterior probability pckx. figure an example of a loss matrix with elements lkj for the cancer treatment problem. the rows correspond to the true class whereas the columns correspond to the assignment of class made by our decision criterion. cancer normal decision theory cancer normal minimizing the expected loss for many applications our objective will be more complex than simply minimizing the number of misclassifications. let us consider again the medical diagnosis problem. we note that if a patient who does not have cancer is incorrectly diagnosed as having cancer the consequences may be some patient distress plus the need for further investigations. conversely if a patient with cancer is diagnosed as healthy the result may be premature death due to lack of treatment. thus the consequences of these two types of mistake can be dramatically different. it would clearly be better to make fewer mistakes of the second kind even if this was at the expense of making more mistakes of the first kind. we can formalize such issues through the introduction of a loss function also called a cost function which is a single overall measure of loss incurred in taking any of the available decisions or actions. our goal is then to minimize the total loss incurred. note that some authors consider instead a utility function whose value they aim to maximize. these are equivalent concepts if we take the utility to be simply the negative of the loss and throughout this text we shall use the loss function convention. suppose that for a new value of x the true class is ck and that we assign x to class cj j may or may not be equal to k. in so doing we incur some level of loss that we denote by lkj which we can view as the k j element of a loss matrix. for instance in our cancer example we might have a loss matrix of the form shown in figure this particular loss matrix says that there is no loss incurred if the correct decision is made there is a loss of if a healthy patient is diagnosed as having cancer whereas there is a loss of if a patient having cancer is diagnosed as healthy. the optimal solution is the one which minimizes the loss function. however the loss function depends on the true class which is unknown. for a given input vector x our uncertainty in the true class is expressed through the joint probability distribution pxck and so we seek instead to minimize the average loss where the average is computed with respect to this distribution which is given by el k j rj lkjpxck dx. each x can be assigned independently to one of the decision regions rj. our goal is to choose the regions rj in order to minimize the expected loss which k lkjpxck. as before we can use implies that for each x we should minimize the product rule pxck pckxpx to eliminate the common factor of px. thus the decision rule that minimizes the expected loss is the one that assigns each introduction figure illustration of the reject option. inputs x such that the larger of the two posterior probabilities is less than or equal to some threshold will be rejected. reject region x new x to the class j for which the lkjpckx k is a minimum. this is clearly trivial to do once we know the posterior class probabilities pckx. the reject option we have seen that classification errors arise from the regions of input space where the largest of the posterior probabilities pckx is significantly less than unity or equivalently where the joint distributions pxck have comparable values. these are the regions where we are relatively uncertain about class membership. in some applications it will be appropriate to avoid making decisions on the difficult cases in anticipation of a lower error rate on those examples for which a classification decision is made. this is known as the reject option. for example in our hypothetical medical illustration it may be appropriate to use an automatic system to classify those x-ray images for which there is little doubt as to the correct class while leaving a human expert to classify the more ambiguous cases. we can achieve this by introducing a threshold and rejecting those inputs x for which the largest of the posterior probabilities pckx is less than or equal to this is illustrated for the case of two classes and a single continuous input variable x in figure note that setting will ensure that all examples are rejected whereas if there are k classes then setting will ensure that no examples are rejected. thus the fraction of examples that get rejected is controlled by the value of we can easily extend the reject criterion to minimize the expected loss when a loss matrix is given taking account of the loss incurred when a reject decision is made. inference and decision we have broken the classification problem down into two separate stages the inference stage in which we use training data to learn a model for pckx and the exercise decision theory subsequent decision stage in which we use these posterior probabilities to make optimal class assignments. an alternative possibility would be to solve both problems together and simply learn a function that maps inputs x directly into decisions. such a function is called a discriminant function. in fact we can identify three distinct approaches to solving decision problems all of which have been used in practical applications. these are given in decreasing order of complexity by first solve the inference problem of determining the class-conditional densities pxck for each class ck individually. also separately infer the prior class probabilities pck. then use bayes theorem in the form pckx pxckpck to find the posterior class probabilities pckx. as usual the denominator in bayes theorem can be found in terms of the quantities appearing in the numerator because px pxckpck. px k equivalently we can model the joint distribution pxck directly and then normalize to obtain the posterior probabilities. having found the posterior probabilities we use decision theory to determine class membership for each new input x. approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models because by sampling from them it is possible to generate synthetic data points in the input space. first solve the inference problem of determining the posterior class probabilities pckx and then subsequently use decision theory to assign each new x to one of the classes. approaches that model the posterior probabilities directly are called discriminative models. find a function fx called a discriminant function which maps each input x directly onto a class label. for instance in the case of two-class problems f might be binary valued and such that f represents class and f represents class in this case probabilities play no role. let us consider the relative merits of these three alternatives. approach is the most demanding because it involves finding the joint distribution over both x and ck. for many applications x will have high dimensionality and consequently we may need a large training set in order to be able to determine the class-conditional densities to reasonable accuracy. note that the class priors pck can often be estimated simply from the fractions of the training set data points in each of the classes. one advantage of approach however is that it also allows the marginal density of data px to be determined from this can be useful for detecting new data points that have low probability under the model and for which the predictions may introduction s e i t i s n e d s s a c l x x figure example of the class-conditional densities for two classes having a single input variable x plot together with the corresponding posterior probabilities plot. note that the left-hand mode of the class-conditional density shown in blue on the left plot has no effect on the posterior probabilities. the vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassification rate. be of low accuracy which is known as outlier detection or novelty detection tarassenko however if we only wish to make classification decisions then it can be wasteful of computational resources and excessively demanding of data to find the joint distribution pxck when in fact we only really need the posterior probabilities pckx which can be obtained directly through approach indeed the classconditional densities may contain a lot of structure that has little effect on the posterior probabilities as illustrated in figure there has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning and in finding ways to combine them lasserre et al. an even simpler approach is in which we use the training data to find a discriminant function fx that maps each x directly onto a class label thereby combining the inference and decision stages into a single learning problem. in the example of figure this would correspond to finding the value of x shown by the vertical green line because this is the decision boundary giving the minimum probability of misclassification. with option however we no longer have access to the posterior probabilities pckx. there are many powerful reasons for wanting to compute the posterior probabilities even if we subsequently use them to make decisions. these include minimizing risk. consider a problem in which the elements of the loss matrix are subjected to revision from time to time as might occur in a financial decision theory application. if we know the posterior probabilities we can trivially revise the minimum risk decision criterion by modifying appropriately. if we have only a discriminant function then any change to the loss matrix would require that we return to the training data and solve the classification problem afresh. reject option. posterior probabilities allow us to determine a rejection criterion that will minimize the misclassification rate or more generally the expected loss for a given fraction of rejected data points. compensating for class priors. consider our medical x-ray problem again and suppose that we have collected a large number of x-ray images from the general population for use as training data in order to build an automated screening system. because cancer is rare amongst the general population we might find that say only in every examples corresponds to the presence of cancer. if we used such a data set to train an adaptive model we could run into severe difficulties due to the small proportion of the cancer class. for instance a classifier that assigned every point to the normal class would already achieve accuracy and it would be difficult to avoid this trivial solution. also even a large data set will contain very few examples of x-ray images corresponding to cancer and so the learning algorithm will not be exposed to a broad range of examples of such images and hence is not likely to generalize well. a balanced data set in which we have selected equal numbers of examples from each of the classes would allow us to find a more accurate model. however we then have to compensate for the effects of our modifications to the training data. suppose we have used such a modified data set and found models for the posterior probabilities. from bayes theorem we see that the posterior probabilities are proportional to the prior probabilities which we can interpret as the fractions of points in each class. we can therefore simply take the posterior probabilities obtained from our artificially balanced data set and first divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model. finally we need to normalize to ensure that the new posterior probabilities sum to one. note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities. combining models. for complex applications we may wish to break the problem into a number of smaller subproblems each of which can be tackled by a separate module. for example in our hypothetical medical diagnosis problem we may have information available from say blood tests as well as x-ray images. rather than combine all of this heterogeneous information into one huge input space it may be more effective to build one system to interpret the xray images and a different one to interpret the blood data. as long as each of the two models gives posterior probabilities for the classes we can combine the outputs systematically using the rules of probability. one simple way to do this is to assume that for each class separately the distributions of inputs for the x-ray images denoted by xi and the blood data denoted by xb are introduction independent so that pxi xbck pxickpxbck. section section section appendix d this is an example of conditional independence property because the independence holds when the distribution is conditioned on the class ck. the posterior probability given both the x-ray and blood data is then given by pckxi xb pxi xbckpck pxickpxbckpck pckxipckxb pck thus we need the class prior probabilities pck which we can easily estimate from the fractions of data points in each class and then we need to normalize the resulting posterior probabilities so they sum to one. the particular conditional independence assumption is an example of the naive bayes model. note that the joint marginal distribution pxi xb will typically not factorize under this model. we shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption loss functions for regression so far we have discussed decision theory in the context of classification problems. we now turn to the case of regression problems such as the curve fitting example discussed earlier. the decision stage consists of choosing a specific estimate yx of the value of t for each input x. suppose that in doing so we incur a loss lt yx. the average or expected loss is then given by el lt yxpx t dx dt. a common choice of loss function in regression problems is the squared loss given by lt yx in this case the expected loss can be written el t dx dt. our goal is to choose yx so as to minimize el. if we assume a completely flexible function yx we can do this formally using the calculus of variations to give tpx t dt solving for yx and using the sum and product rules of probability we obtain el yx yx tpx t dt px tptx dt ettx figure the regression function yx which minimizes the expected squared loss is given by the mean of the conditional distribution ptx. t decision theory yx x which is the conditional average of t conditioned on x and is known as the regression function. this result is illustrated in figure it can readily be extended to multiple target variables represented by the vector t in which case the optimal solution is the conditional average yx ettx. exercise we can also derive this result in a slightly different way which will also shed light on the nature of the regression problem. armed with the knowledge that the optimal solution is the conditional expectation we can expand the square term as follows etx etx etxetx t where to keep the notation uncluttered we use etx to denote ettx. substituting into the loss function and performing the integral over t we see that the cross-term vanishes and we obtain an expression for the loss function in the form dx. px dx el the function yx we seek to determine enters only in the first term which will be minimized when yx is equal to etx in which case this term will vanish. this is simply the result that we derived previously and that shows that the optimal least squares predictor is given by the conditional mean. the second term is the variance of the distribution of t averaged over x. it represents the intrinsic variability of the target data and can be regarded as noise. because it is independent of yx it represents the irreducible minimum value of the loss function. as with the classification problem we can either determine the appropriate probabilities and then use these to make optimal decisions or we can build models that make decisions directly. indeed we can identify three distinct approaches to solving regression problems given in order of decreasing complexity by first solve the inference problem of determining the joint density px t. then normalize to find the conditional density ptx and finally marginalize to find the conditional mean given by introduction first solve the inference problem of determining the conditional density ptx and then subsequently marginalize to find the conditional mean given by find a regression function yx directly from the training data. the relative merits of these three approaches follow the same lines as for classification problems above. the squared loss is not the only possible choice of loss function for regression. indeed there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches. an important example concerns situations in which the conditional distribution ptx is multimodal as often arises in the solution of inverse problems. here we consider briefly one simple generalization of the squared loss called the minkowski loss whose expectation is given by tqpx t dx dt elq which reduces to the expected squared loss for q the function tq is plotted against y t for various values of q in figure the minimum of elq is given by the conditional mean for q the conditional median for q and the conditional mode for q section exercise information theory in this chapter we have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the subsequent discussion in this book. we close this chapter by introducing some additional concepts from the field of information theory which will also prove useful in our development of pattern recognition and machine learning techniques. again we shall focus only on the key concepts and we refer the reader elsewhere for more detailed discussions and omura cover and thomas mackay we begin by considering a discrete random variable x and we ask how much information is received when we observe a specific value for this variable. the amount of information can be viewed as the degree of surprise on learning the value of x. if we are told that a highly improbable event has just occurred we will have received more information than if we were told that some very likely event has just occurred and if we knew that the event was certain to happen we would receive no information. our measure of information content will therefore depend on the probability distribution px and we therefore look for a quantity hx that is a monotonic function of the probability px and that expresses the information content. the form of h can be found by noting that if we have two events x and y that are unrelated then the information gain from observing both of them should be the sum of the information gained from each of them separately so that hx y hx hy. two unrelated events will be statistically independent and so px y pxpy. from these two relationships it is easily shown that hx must be given by the logarithm of px and so we have exercise q t y q t y q y t q y t q t y q t y information theory q y t q y t figure plots of the quantity lq tq for various values of q. hx px where the negative sign ensures that information is positive or zero. note that low probability events x correspond to high information content. the choice of basis for the logarithm is arbitrary and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of in this case as we shall see shortly the units of hx are bits binary digits now suppose that a sender wishes to transmit the value of a random variable to a receiver. the average amount of information that they transmit in the process is obtained by taking the expectation of with respect to the distribution px and is given by hx px px. x this important quantity is called the entropy of the random variable x. note that limp p ln p and so we shall take px ln px whenever we encounter a value for x such that px so far we have given a rather heuristic motivation for the definition of informa introduction tion and the corresponding entropy we now show that these definitions indeed possess useful properties. consider a random variable x having possible states each of which is equally likely. in order to communicate the value of x to a receiver we would need to transmit a message of length bits. notice that the entropy of this variable is given by hx bits. now consider an example and thomas of a variable having possible states b c d e f g h for which the respective probabilities are given by hx the entropy in this case is given by bits. we see that the nonuniform distribution has a smaller entropy than the uniform one and we shall gain some insight into this shortly when we discuss the interpretation of entropy in terms of disorder. for the moment let us consider how we would transmit the identity of the variable s state to a receiver. we could do this as before using a number. however we can take advantage of the nonuniform distribution by using shorter codes for the more probable events at the expense of longer codes for the less probable events in the hope of getting a shorter average code length. this can be done by representing the states b c d e f g h using for instance the following set of code strings the average length of the code that has to be transmitted is then average code length bits which again is the same as the entropy of the random variable. note that shorter code strings cannot be used because it must be possible to disambiguate a concatenation of such strings into its component parts. for instance decodes uniquely into the state sequence c a d. this relation between entropy and shortest coding length is a general one. the noiseless coding theorem states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable. from now on we shall switch to the use of natural logarithms in defining entropy as this will provide a more convenient link with ideas elsewhere in this book. in this case the entropy is measured in units of nats instead of bits which differ simply by a factor of ln we have introduced the concept of entropy in terms of the average amount of information needed to specify the state of a random variable. in fact the concept of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretation as a measure of disorder through developments in statistical mechanics. we can understand this alternative view of entropy by considering a set of n identical objects that are to be divided amongst a set of bins such that there are ni objects in the ith bin. consider information theory the number of different ways of allocating the objects to the bins. there are n ways to choose the first object ways to choose the second object and so on leading to a total of n! ways to allocate all n objects to the bins where n! factorial n denotes the product n however we don t wish to distinguish between rearrangements of objects within each bin. in the ith bin there are ni! ways of reordering the objects and so the total number of ways of allocating the n objects to the bins is given by w i ni! which is called the multiplicity. the entropy is then defined as the logarithm of the multiplicity scaled by an appropriate constant h n ln w n ln n! n ln ni!. we now consider the limit n in which the fractions nin are held fixed and apply stirling s approximation i which gives h lim n i ln n! n ln n n ni n ln ni n i pi ln pi i ni n. here pi limn is the probability where we have used of an object being assigned to the ith bin. in physics terminology the specific arrangements of objects in the bins is called a microstate and the overall distribution of occupation numbers expressed through the ratios nin is called a macrostate. the multiplicity w is also known as the weight of the macrostate. we can interpret the bins as the states xi of a discrete random variable x where px xi pi. the entropy of the random variable x is then hp pxi ln pxi. i appendix e distributions pxi that are sharply peaked around a few values will have a relatively low entropy whereas those that are spread more evenly across many values will have higher entropy as illustrated in figure because pi the entropy is nonnegative and it will equal its minimum value of when one of the pi and all other the maximum entropy configuration can be found by maximizing h using a lagrange multiplier to enforce the normalization constraint on the probabilities. thus we maximize pxi ln pxi i i pxi introduction s e i t i l i b a b o r p h s e i t i l i b a b o r p h figure histograms of two probability distributions over bins illustrating the higher value of the entropy h for the broader distribution. the largest entropy would arise from a uniform distribution that would give h exercise from which we find that all of the pxi are equal and are given by pxi where m is the total number of states xi. the corresponding value of the entropy is then h ln m. this result can also be derived from jensen s inequality be discussed shortly. to verify that the stationary point is indeed a maximum we can evaluate the second derivative of the entropy which gives pxi pxj iij pi where iij are the elements of the identity matrix. we can extend the definition of entropy to include distributions px over continuous variables x as follows. first divide x into bins of width then assuming px is continuous the mean value theorem tells us that for each such bin there must exist a value xi such that px dx pxi i we can now quantize the continuous variable x by assigning any value x to the value xi whenever x falls in the ith bin. the probability of observing the value xi is then pxi this gives a discrete distribution for which the entropy takes the form h pxi ln pxi ln pxi ln i i i pxi which follows from we now omit where we have used the second term ln on the right-hand side of and then consider the limit information theory the first term on the right-hand side of will approach the integral of px ln px in this limit so that i lim pxi ln pxi px ln px dx where the quantity on the right-hand side is called the differential entropy. we see that the discrete and continuous forms of the entropy differ by a quantity ln which diverges in the limit this reflects the fact that to specify a continuous variable very precisely requires a large number of bits. for a density defined over multiple continuous variables denoted collectively by the vector x the differential entropy is given by hx px ln px dx. in the case of discrete distributions we saw that the maximum entropy configuration corresponded to an equal distribution of probabilities across the possible states of the variable. let us now consider the maximum entropy configuration for a continuous variable. in order for this maximum to be well defined it will be necessary to constrain the first and second moments of px as well as preserving the normalization constraint. we therefore maximize the differential entropy with the ludwig boltzmann ludwig eduard boltzmann was an austrian physicist who created the field of statistical mechanics. prior to boltzmann the concept of entropy was already known from classical thermodynamics where it quantifies the fact that when we take energy from a system not all of that energy is typically available to do useful work. boltzmann showed that the thermodynamic entropy s a macroscopic quantity could be related to the statistical properties at the microscopic level. this is expressed through the famous equation s k ln w in which w represents the number of possible microstates in a macrostate and k units of joules per kelvin is known as boltzmann s constant. boltzmann s ideas were disputed by many scientists of they day. one difficulty they saw arose from the second law of thermo dynamics which states that the entropy of a closed system tends to increase with time. by contrast at the microscopic level the classical newtonian equations of physics are reversible and so they found it difficult to see how the latter could explain the former. they didn t fully appreciate boltzmann s arguments which were statistical in nature and which concluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase. boltzmann even had a longrunning dispute with the editor of the leading german physics journal who refused to let him refer to atoms and molecules as anything other than convenient theoretical constructs. the continued attacks on his work lead to bouts of depression and eventually he committed suicide. shortly after boltzmann s death new experiments by perrin on colloidal suspensions verified his theories and confirmed the value of the boltzmann constant. the equation s k ln w is carved on boltzmann s tombstone. introduction three constraints px dx xpx dx dx appendix e appendix d exercise exercise the constrained maximization can be performed using lagrange multipliers so that we maximize the following functional with respect to px px dx px ln px dx xpx dx dx using the calculus of variations we set the derivative of this functional to zero giving px exp exp the lagrange multipliers can be found by back substitution of this result into the three constraint equations leading finally to the result px and so the distribution that maximizes the differential entropy is the gaussian. note that we did not constrain the distribution to be nonnegative when we maximized the entropy. however because the resulting distribution is indeed nonnegative we see with hindsight that such a constraint is not necessary. if we evaluate the differential entropy of the gaussian we obtain hx thus we see again that the entropy increases as the distribution becomes broader i.e. as increases. this result also shows that the differential entropy unlike the discrete entropy can be negative because hx in for e. suppose we have a joint distribution px y from which we draw pairs of values of x and y. if a value of x is already known then the additional information needed to specify the corresponding value of y is given by ln pyx. thus the average additional information needed to specify y can be written as hyx py x ln pyx dy dx information theory exercise which is called the conditional entropy of y given x. it is easily seen using the product rule that the conditional entropy satisfies the relation hx y hyx hx where hx y is the differential entropy of px y and hx is the differential entropy of the marginal distribution px. thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additional information required to specify y given x. relative entropy and mutual information so far in this section we have introduced a number of concepts from information theory including the key notion of entropy. we now start to relate these ideas to pattern recognition. consider some unknown distribution px and suppose that we have modelled this using an approximating distribution qx. if we use qx to construct a coding scheme for the purpose of transmitting values of x to a receiver then the average additional amount of information nats required to specify the value of x we choose an efficient coding scheme as a result of using qx instead of the true distribution px is given by px ln qx dx px ln px dx px ln qx px dx. this is known as the relative entropy or kullback-leibler divergence or kl divergence and leibler between the distributions px and qx. note that it is not a symmetrical quantity that is to say we now show that the kullback-leibler divergence satisfies with equality if and only if px qx. to do this we first introduce the concept of convex functions. a function fx is said to be convex if it has the property that every chord lies on or above the function as shown in figure any value of x in the interval from x a to x b can be written in the form a where the corresponding point on the chord is given by fa claude shannon after graduating from michigan and mit shannon joined the att bell telephone laboratories in his paper a mathematical theory of communication published in the bell system technical journal in laid the foundations for modern information the ory. this paper introduced the word bit and his concept that information could be sent as a stream of and paved the way for the communications revolution. it is said that von neumann recommended to shannon that he use the term entropy not only because of its similarity to the quantity used in physics but also because nobody knows what entropy really is so in any discussion you will always have an advantage introduction figure a convex function f is one for which every chord in blue lies on or above the function in red. fx chord a x x b x and the corresponding value of the function is f a convexity then implies f a fa exercise exercise this is equivalent to the requirement that the second derivative of the function be everywhere positive. examples of convex functions are x ln x x and a function is called strictly convex if the equality is satisfied only for and if a function has the opposite property namely that every chord lies on or below the function it is called concave with a corresponding definition for strictly concave. if a function fx is convex then fx will be concave. using the technique of proof by induction we can show from that a convex function fx satisfies f ixi i i for any set of points the result is where i and known as jensen s inequality. if we interpret the i as the probability distribution over a discrete variable x taking the values then can be written ifxi where e denotes the expectation. for continuous variables jensen s inequality takes the form f efx f xpx dx fxpx dx. we can apply jensen s inequality in the form to the kullback-leibler divergence to give px ln qx px dx ln qx dx information theory where we have used the fact that ln x is a convex function together with the norqx dx in fact ln x is a strictly convex function malization condition so the equality will hold if and only if qx px for all x. thus we can interpret the kullback-leibler divergence as a measure of the dissimilarity of the two distributions px and qx. we see that there is an intimate relationship between data compression and density estimation the problem of modelling an unknown probability distribution because the most efficient compression is achieved when we know the true distribution. if we use a distribution that is different from the true one then we must necessarily have a less efficient coding and on average the additional information that must be transmitted is least equal to the kullback-leibler divergence between the two distributions. suppose that data is being generated from an unknown distribution px that we wish to model. we can try to approximate this distribution using some parametric distribution qx governed by a set of adjustable parameters for example a multivariate gaussian. one way to determine is to minimize the kullback-leibler divergence between px and qx with respect to we cannot do this directly because we don t know px. suppose however that we have observed a finite set of training points xn for n n drawn from px. then the expectation with respect to px can be approximated by a finite sum over these points using so that ln qxn ln pxn the second term on the right-hand side of is independent of and the first term is the negative log likelihood function for under the distribution qx evaluated using the training set. thus we see that minimizing this kullback-leibler divergence is equivalent to maximizing the likelihood function. now consider the joint distribution between two sets of variables x and y given by px y. if the sets of variables are independent then their joint distribution will factorize into the product of their marginals px y pxpy. if the variables are not independent we can gain some idea of whether they are close to being independent by considering the kullback-leibler divergence between the joint distribution and the product of the marginals given by ix y klpx px y ln pxpy px y dx dy which is called the mutual information between the variables x and y. from the properties of the kullback-leibler divergence we see that ix y with equality if and only if x and y are independent. using the sum and product rules of probability we see that the mutual information is related to the conditional entropy through ix y hx hxy hy hyx. exercise introduction thus we can view the mutual information as the reduction in the uncertainty about x by virtue of being told the value of y vice versa. from a bayesian perspective we can view px as the prior distribution for x and pxy as the posterior distribution after we have observed new data y. the mutual information therefore represents the reduction in uncertainty about x as a consequence of the new observation y. exercises www consider the sum-of-squares error function given by in which the function yx w is given by the polynomial show that the coefficients w that minimize this error function are given by the solution to the following set of linear equations where aij aijwj ti ti here a suffix i or j denotes the index of a component whereas denotes x raised to the power of i. write down the set of coupled linear equations analogous to satisfied by the coefficients wi which minimize the regularized sum-of-squares error function given by suppose that we have three coloured boxes r b and g box r contains apples oranges and limes box b contains apple orange and limes and box g contains apples oranges and limes. if a box is chosen at random with probabilities pr pb pg and a piece of fruit is removed from the box equal probability of selecting any of the items in the box then what is the probability of selecting an apple? if we observe that the selected fruit is in fact an orange what is the probability that it came from the green box? www consider a probability density pxx defined over a continuous variable x and suppose that we make a nonlinear change of variable using x gy so that the density transforms according to by differentiating show that the of the maximum of the density in y is not in general related to the of the maximum of the density over x by the simple functional relation as a consequence of the jacobian factor. this shows that the maximum of a probability density contrast to a simple function is dependent on the choice of variable. verify that in the case of a linear transformation the location of the maximum transforms in the same way as the variable itself. using the definition show that varfx satisfies which we can evaluate by first writing its square in the form dx i exp i exp dx dy. show that if two variables x and y are independent then their covariance is zero. exercises www in this exercise we prove the normalization condition for the univariate gaussian. to do this consider the integral now make the transformation from cartesian coordinates y to polar coordinates and then substitute u show that by performing the integrals over and u and then taking the square root of both sides we obtain finally use this result to show that the gaussian distribution n is normalized. i www by using a change of variables verify that the univariate gaussian distribution given by satisfies next by differentiating both sides of the normalization condition x dx with respect to verify that the gaussian satisfies finally show that holds. www show that the mode the maximum of the gaussian distribution is given by similarly show that the mode of the multivariate gaussian is given by www suppose that the two variables x and z are statistically independent. show that the mean and variance of their sum satisfies ex z ex ez varx z varx varz. by setting the derivatives of the log likelihood function with respect to and equal to zero verify the results and introduction www using the results and show that exnxm inm where xn and xm denote data points sampled from a gaussian distribution with mean and variance and inm satisfies inm if n m and inm otherwise. hence prove the results and suppose that the variance of a gaussian is estimated using the result but with the maximum likelihood estimate ml replaced with the true value of the mean. show that this estimator has the property that its expectation is given by the true variance show that an arbitrary square matrix with elements wij can be written in the form wij ws ij are symmetric and anti-symmetric matrices respectively satisfying ws ji for all i and j. now consider the second order term in a higher order polynomial in d dimensions given by ij wa ij and wa ij wa ij where ws ij ws ji and wa show that wijxixj. wijxixj ws ijxixj so that the contribution from the anti-symmetric matrix vanishes. we therefore see that without loss of generality the matrix of coefficients wij can be chosen to be symmetric and so not all of the elements of this matrix can be chosen independently. show that the number of independent parameters in the matrix ws ij is given by dd www in this exercise and the next we explore how the number of independent parameters in a polynomial grows with the order m of the polynomial and with the dimensionality d of the input space. we start by writing down the m th order term for a polynomial in d dimensions in the form im im xim the coefficients im comprise dm elements but the number of independent parameters is significantly fewer due to the many interchange symmetries of the factor xim begin by showing that the redundancy in the coefficients can be removed by rewriting this m th order term in the form im im im xim note that the precise relationship between coefficients and w coefficients need exercises not be made explicit. use this result to show that the number of independent parameters nd m which appear at order m satisfies the following recursion relation next use proof by induction to show that the following result holds nd m ni m m m m! which can be done by first proving the result for d and arbitrary m by making use of the result then assuming it is correct for dimension d and verifying that it is correct for dimension d finally use the two previous results together with proof by induction to show m m! nd m to do this first show that the result is true for m and any value of d by comparison with the result of exercise then make use of together with to show that if the result holds at order m then it will also hold at order m in exercise we proved the result for the number of independent parameters in the m th order term of a d-dimensional polynomial. we now find an expression for the total number nd m of independent parameters in all of the terms up to and including the order. first show that nd m satisfies nd m nd m where nd m is the number of independent parameters in the term of order m. now make use of the result together with proof by induction to show that nd m m! d! m! this can be done by first proving that the result holds for m and arbitrary d then assuming that it holds at order m and hence showing that it holds at order m finally make use of stirling s approximation in the form n! nne n for large n to show that for d m the quantity nd m grows like dm and for m d it grows like m d. consider a cubic polynomial in d dimensions and evaluate numerically the total number of independent parameters for d and d which correspond to typical small-scale and medium-scale machine learning applications. introduction www the gamma function is defined by ux u du. using integration by parts prove the relation x show also that and hence that x! when x is an integer. www we can use the result to derive an expression for the surface area sd and the volume vd of a sphere of unit radius in d dimensions. to do this consider the following result which is obtained by transforming from cartesian to polar coordinates e i dxi sd e rd dr. using the definition of the gamma function together with evaluate both sides of this equation and hence show that sd next by integrating with respect to radius from to show that the volume of the unit sphere in d dimensions is given by finally use the results and reduce to the usual expressions for d and d vd sd d to show that and consider a sphere of radius a in d-dimensions together with the concentric hypercube of side so that the sphere touches the hypercube at the centres of each of its sides. by using the results of exercise show that the ratio of the volume of the sphere to the volume of the cube is given by volume of sphere volume of cube now make use of stirling s formula in the form which is valid for x to show that as d the ratio goes to zero. show also that the ratio of the distance from the centre of the hypercube to one of d which the corners divided by the perpendicular distance to one of the sides is therefore goes to as d from these results we see that in a space of high dimensionality most of the volume of a cube is concentrated in the large number of corners which themselves become very long spikes exercises www in this exercise we explore the behaviour of the gaussian distribution in high-dimensional spaces. consider a gaussian distribution in d dimensions given by px exp exp where sd is the surface area of a unit sphere in d dimensions. show that the function d by considering we wish to find the density with respect to radius in polar coordinates in which the direction variables have been integrated out. to do this show that the integral of the probability density over a thin shell of radius r and thickness where is given by pr where pr sdrd pr has a single stationary point located for large d where show that for large d exp which shows is a maximum of the radial probability density and also that pr decays exponentially away from its maximum with length scale we have already seen that for large d and so we see that most of the probability density px is larger at the origin than at the by a factor of mass is concentrated in a thin shell at large radius. finally show that the probability we therefore see that most of the probability mass in a high-dimensional gaussian distribution is located at a different radius from the region of high probability density. this property of distributions in spaces of high dimensionality will have important consequences when we consider bayesian inference of model parameters in later chapters. consider two nonnegative numbers a and b and show that if a b then a use this result to show that if the decision regions of a two-class classification problem are chosen to minimize the probability of misclassification this probability will satisfy pmistake dx. www given a loss matrix with elements lkj the expected risk is minimized if for each x we choose the class that minimizes verify that when the loss matrix is given by lkj ikj where ikj are the elements of the identity matrix this reduces to the criterion of choosing the class having the largest posterior probability. what is the interpretation of this form of loss matrix? derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes. introduction www consider a classification problem in which the loss incurred when an input vector from class ck is classified as belonging to class cj is given by the loss matrix lkj and for which the loss incurred in selecting the reject option is find the decision criterion that will give the minimum expected loss. verify that this reduces to the reject criterion discussed in section when the loss matrix is given by lkj ikj. what is the relationship between and the rejection threshold www consider the generalization of the squared loss function for a single target variable t to the case of multiple target variables described by the vector t given by elt yx t dx dt. using the calculus of variations show that the function yx for which this expected loss is minimized is given by yx ettx. show that this result reduces to for the case of a single target variable t. by expansion of the square in derive a result analogous to and hence show that the function yx that minimizes the expected squared loss for the case of a vector t of target variables is again given by the conditional expectation of t. www consider the expected loss for regression problems under the lq loss function given by write down the condition that yx must satisfy in order to minimize elq. show that for q this solution represents the conditional median i.e. the function yx such that the probability mass for t yx is the same as for t yx. also show that the minimum expected lq loss for q is given by the conditional mode i.e. by the function yx equal to the value of t that maximizes ptx for each x. in section we introduced the idea of entropy hx as the information gained on observing the value of a random variable x having distribution px. we saw that for independent variables x and y for which px y pxpy the entropy functions are additive so that hx y hx hy. in this exercise we derive the relation between h and p in the form of a function hp. first show that and hence by induction that hpn nhp where n is a positive integer. hence show that hpnm where m is also a positive integer. this implies that hpx xhp where x is a positive rational number and hence by continuity when it is a positive real number. finally show that this implies hp must take the form hp ln p. www consider an m-state discrete random variable x and use jensen s inequality in the form to show that the entropy of its distribution px satisfies hx ln m. evaluate the kullback-leibler divergence between two gaussians px n and qx n table the joint distribution px y for two binary variables x and y used in exercise exercises y x www consider two variables x and y having joint distribution px y. show that the differential entropy of this pair of variables satisfies hx y hx hy with equality if and only if x and y are statistically independent. consider a vector x of continuous variables with distribution px and corresponding entropy hx. suppose that we make a nonsingular linear transformation of x to obtain a new variable y ax. show that the corresponding entropy is given by hy hx lna where denotes the determinant of a. suppose that the conditional entropy hyx between two discrete random variables x and y is zero. show that for all values of x such that px the variable y must be a function of x in other words for each x there is only one value of y such that pyx www use the calculus of variations to show that the stationary point of the functional is given by then use the constraints and to eliminate the lagrange multipliers and hence show that the maximum entropy solution is given by the gaussian www use the results and to show that the entropy of the univariate gaussian is given by a strictly convex function is defined as one for which every chord lies above the function. show that this is equivalent to the condition that the second derivative of the function be positive. using the definition together with the product rule of probability prove the result www using proof by induction show that the inequality for convex functions implies the result consider two binary variables x and y having the joint distribution given in table evaluate the following quantities hx hy hyx hxy hx y ix y. draw a diagram to show the relationship between these various quantities. introduction by applying jensen s inequality with fx ln x show that the arith metic mean of a set of real numbers is never less than their geometrical mean. www using the sum and product rules of probability show that the mutual information ix y satisfies the relation probability distributions in chapter we emphasized the central role played by probability theory in the solution of pattern recognition problems. we turn now to an exploration of some particular examples of probability distributions and their properties. as well as being of great interest in their own right these distributions can form building blocks for more complex models and will be used extensively throughout the book. the distributions introduced in this chapter will also serve another important purpose namely to provide us with the opportunity to discuss some key statistical concepts such as bayesian inference in the context of simple models before we encounter them in more complex situations in later chapters. one role for the distributions discussed in this chapter is to model the probability distribution px of a random variable x given a finite set xn of observations. this problem is known as density estimation. for the purposes of this chapter we shall assume that the data points are independent and identically distributed. it should be emphasized that the problem of density estimation is fun probability distributions damentally ill-posed because there are infinitely many probability distributions that could have given rise to the observed finite data set. indeed any distribution px that is nonzero at each of the data points xn is a potential candidate. the issue of choosing an appropriate distribution relates to the problem of model selection that has already been encountered in the context of polynomial curve fitting in chapter and that is a central issue in pattern recognition. we begin by considering the binomial and multinomial distributions for discrete random variables and the gaussian distribution for continuous random variables. these are specific examples of parametric distributions so-called because they are governed by a small number of adaptive parameters such as the mean and variance in the case of a gaussian for example. to apply such models to the problem of density estimation we need a procedure for determining suitable values for the parameters given an observed data set. in a frequentist treatment we choose specific values for the parameters by optimizing some criterion such as the likelihood function. by contrast in a bayesian treatment we introduce prior distributions over the parameters and then use bayes theorem to compute the corresponding posterior distribution given the observed data. we shall see that an important role is played by conjugate priors that lead to posterior distributions having the same functional form as the prior and that therefore lead to a greatly simplified bayesian analysis. for example the conjugate prior for the parameters of the multinomial distribution is called the dirichlet distribution while the conjugate prior for the mean of a gaussian is another gaussian. all of these distributions are examples of the exponential family of distributions which possess a number of important properties and which will be discussed in some detail. one limitation of the parametric approach is that it assumes a specific functional form for the distribution which may turn out to be inappropriate for a particular application. an alternative approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set. such models still contain parameters but these control the model complexity rather than the form of the distribution. we end this chapter by considering three nonparametric methods based respectively on histograms nearest-neighbours and kernels. binary variables we begin by considering a single binary random variable x for example x might describe the outcome of flipping a coin with x representing heads and x representing tails we can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails. the probability of x will be denoted by the parameter so that px binary variables where from which it follows that px the probability distribution over x can therefore be written in the form bernx x exercise which is known as the bernoulli distribution. it is easily verified that this distribution is normalized and that it has mean and variance given by ex varx now suppose we have a data set d xn of observed values of x. we can construct the likelihood function which is a function of on the assumption that the observations are drawn independently from px so that pd pxn xn. in a frequentist setting we can estimate a value for by maximizing the likelihood function or equivalently by maximizing the logarithm of the likelihood. in the case of the bernoulli distribution the log likelihood function is given by ln pd ln pxn ln xn section at this point it is worth noting that the log likelihood function depends on the n observations xn only through their sum n xn. this sum provides an example of a sufficient statistic for the data under this distribution and we shall study the important role of sufficient statistics in some detail. if we set the derivative of ln pd with respect to equal to zero we obtain the maximum likelihood estimator ml n xn jacob bernoulli jacob bernoulli also known as jacques or james bernoulli was a swiss mathematician and was the first of many in the bernoulli family to pursue a career in science and mathematics. although compelled to study philosophy and theology against his will by his parents he travelled extensively after graduating in order to meet with many of the leading scientists of his time including boyle and hooke in england. when he returned to switzerland he taught mechanics and became professor of mathematics at basel in unfortunately rivalry between jacob and his younger brother johann turned an initially productive collaboration into a bitter and public dispute. jacob s most significant contributions to mathematics appeared in the artofconjecture published in eight years after his death which deals with topics in probability theory including what has become known as the bernoulli distribution. probability distributions figure histogram plot of the binomial distribution as a function of m for n and m which is also known as the sample mean. if we denote the number of observations of x within this data set by m then we can write in the form ml m n so that the probability of landing heads is given in this maximum likelihood framework by the fraction of observations of heads in the data set. now suppose we flip a coin say times and happen to observe heads. then n m and ml in this case the maximum likelihood result would predict that all future observations should give heads. common sense tells us that this is unreasonable and in fact this is an extreme example of the over-fitting associated with maximum likelihood. we shall see shortly how to arrive at more sensible conclusions through the introduction of a prior distribution over we can also work out the distribution of the number m of observations of x given that the data set has size n. this is called the binomial distribution and from we see that it is proportional to m. in order to obtain the normalization coefficient we note that out of n coin flips we have to add up all of the possible ways of obtaining m heads so that the binomial distribution can be written binmn m where n m n m n! m!m! exercise is the number of ways of choosing m objects out of a total of n identical objects. figure shows a plot of the binomial distribution for n and the mean and variance of the binomial distribution can be found by using the result of exercise which shows that for independent events the mean of the sum is the sum of the means and the variance of the sum is the sum of the variances. because m xn and for each observation the mean and variance are binary variables given by and respectively we have em varm mbinmn n binmn n exercise these results can also be proved directly using calculus. the beta distribution we have seen in that the maximum likelihood setting for the parameter in the bernoulli distribution and hence in the binomial distribution is given by the fraction of the observations in the data set having x as we have already noted this can give severely over-fitted results for small data sets. in order to develop a bayesian treatment for this problem we need to introduce a prior distribution p over the parameter here we consider a form of prior distribution that has a simple interpretation as well as some useful analytical properties. to motivate this prior we note that the likelihood function takes the form of the product of factors of the form x. if we choose a prior to be proportional to powers of and then the posterior distribution which is proportional to the product of the prior and the likelihood function will have the same functional form as the prior. this property is called conjugacy and we will see several examples of it later in this chapter. we therefore choose a prior called the beta distribution given by exercise beta b b a where is the gamma function defined by and the coefficient in ensures that the beta distribution is normalized so that beta b d exercise the mean and variance of the beta distribution are given by e var a a b b ab the parameters a and b are often called hyperparameters because they control the distribution of the parameter figure shows plots of the beta distribution for various values of the hyperparameters. the posterior distribution of is now obtained by multiplying the beta prior by the binomial likelihood function and normalizing. keeping only the factors that depend on we see that this posterior distribution has the form p l a b ma probability distributions a b a b a b a b figure plots of the beta distribution beta b given by as a function of for various values of the hyperparameters a and b. where l n m and therefore corresponds to the number of tails in the coin example. we see that has the same functional dependence on as the prior distribution reflecting the conjugacy properties of the prior with respect to the likelihood function. indeed it is simply another beta distribution and its normalization coefficient can therefore be obtained by comparison with to give p l a b a l b a b ma we see that the effect of observing a data set of m observations of x and l observations of x has been to increase the value of a by m and the value of b by l in going from the prior distribution to the posterior distribution. this allows us to provide a simple interpretation of the hyperparameters a and b in the prior as an effective number of observations of x and x respectively. note that a and b need not be integers. furthermore the posterior distribution can act as the prior if we subsequently observe additional data. to see this we can imagine taking observations one at a time and after each observation updating the current posterior prior likelihood function binary variables posterior figure illustration of one step of sequential bayesian inference. the prior is given by a beta distribution with parameters a b and the likelihood function given by with n m corresponds to a single observation of x so that the posterior is given by a beta distribution with parameters a b distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new revised posterior distribution. at each stage the posterior is a beta distribution with some total number of and actual observed values for x and x given by the parameters a and b. incorporation of an additional observation of x simply corresponds to incrementing the value of a by whereas for an observation of x we increment b by figure illustrates one step in this process. section we see that this sequential approach to learning arises naturally when we adopt a bayesian viewpoint. it is independent of the choice of prior and of the likelihood function and depends only on the assumption of i.i.d. data. sequential methods make use of observations one at a time or in small batches and then discard them before the next observations are used. they can be used for example in real-time learning scenarios where a steady stream of data is arriving and predictions must be made before all of the data is seen. because they do not require the whole data set to be stored or loaded into memory sequential methods are also useful for large data sets. maximum likelihood methods can also be cast into a sequential framework. if our goal is to predict as best we can the outcome of the next trial then we must evaluate the predictive distribution of x given the observed data set d. from the sum and product rules of probability this takes the form px px d p d e using the result for the posterior distribution p together with the result for the mean of the beta distribution we obtain m a px m a l b which has a simple interpretation as the total fraction of observations real observations and fictitious prior observations that correspond to x note that in the limit of an infinitely large data set m l the result reduces to the maximum likelihood result as we shall see it is a very general property that the bayesian and maximum likelihood results will agree in the limit of an infinitely probability distributions exercise exercise large data set. for a finite data set the posterior mean for always lies between the prior mean and the maximum likelihood estimate for corresponding to the relative frequencies of events given by from figure we see that as the number of observations increases so the posterior distribution becomes more sharply peaked. this can also be seen from the result for the variance of the beta distribution in which we see that the variance goes to zero for a or b in fact we might wonder whether it is a general property of bayesian learning that as we observe more and more data the uncertainty represented by the posterior distribution will steadily decrease. to address this we can take a frequentist view of bayesian learning and show that on average such a property does indeed hold. consider a general bayesian inference problem for a parameter for which we have observed a data set d described by the joint distribution p the following result where e ede pd dd p d e ed p d says that the posterior mean of averaged over the distribution generating the data is equal to the prior mean of similarly we can show that var ed vard the term on the left-hand side of is the prior variance of on the righthand side the first term is the average posterior variance of and the second term measures the variance in the posterior mean of because this variance is a positive quantity this result shows that on average the posterior variance of is smaller than the prior variance. the reduction in variance is greater if the variance in the posterior mean is greater. note however that this result only holds on average and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance. multinomial variables binary variables can be used to describe quantities that can take one of two possible values. often however we encounter discrete variables that can take on one of k possible mutually exclusive states. although there are various alternative ways to express such variables we shall see shortly that a particularly convenient representation is the scheme in which the variable is represented by a k-dimensional vector x in which one of the elements xk equals and all remaining elements equal multinomial variables x so for instance if we have a variable that can take k states and a particular observation of the variable happens to correspond to the state where then x will be represented by note that such vectors satisfy by the parameter k then the distribution of x is given x xk if we denote the probability of xk px xk k where kt and the parameters k are constrained to satisfy k k k because they represent probabilities. the distribution can be and regarded as a generalization of the bernoulli distribution to more than two outcomes. it is easily seen that the distribution is normalized px k and that ex now consider a data set d of n independent observations xn the x px m corresponding likelihood function takes the form pd xnk k p k n xnk mk k we see that the likelihood function depends on the n data points only through the k quantities mk xnk n which represent the number of observations of xk these are called the sufficient statistics for this distribution. in order to find the maximum likelihood solution for we need to maximize ln pd with respect to k taking account of the constraint that the k must sum to one. this can be achieved using a lagrange multiplier and maximizing mk ln k k setting the derivative of with respect to k to zero we obtain k mk section appendix e probability distributions we can solve for the lagrange multiplier by substituting into the constraint k k to give n. thus we obtain the maximum likelihood solution in the form k mk ml n which is the fraction of the n observations for which xk we can consider the joint distribution of the quantities mk conditioned on the parameters and on the total number n of observations. from this takes the form mk n n mk mk k which is known as the multinomial distribution. the normalization coefficient is the number of ways of partitioning n objects into k groups of size mk and is given by n mk n! mk! note that the variables mk are subject to the constraint mk n. the dirichlet distribution we now introduce a family of prior distributions for the parameters k of the multinomial distribution by inspection of the form of the multinomial distribution we see that the conjugate prior is given by k k p where k and k k here k are the parameters of the distribution and denotes kt. note that because of the summation constraint the distribution over the space of the k is confined to a simplex of dimensionality k as illustrated for k in figure exercise the normalized form for this distribution is by dir k k k which is called the dirichlet distribution. here is the gamma function defined by while k. multinomial variables figure the dirichlet distribution over three variables is confined to a simplex bounded linear manifold of the form shown as a consequence of the constraints k and k k p plots of the dirichlet distribution over the simplex for various settings of the parameters k are shown in figure posterior distribution for the parameters k in the form multiplying the prior by the likelihood function we obtain the p pd kmk k we see that the posterior distribution again takes the form of a dirichlet distribution confirming that the dirichlet is indeed a conjugate prior for the multinomial. this allows us to determine the normalization coefficient by comparison with so that p dir m n k mk kmk k where we have denoted m mkt. as for the case of the binomial distribution with its beta prior we can interpret the parameters k of the dirichlet prior as an effective number of observations of xk note that two-state quantities can either be represented as binary variables and lejeune dirichlet johann peter gustav lejeune dirichlet was a modest and reserved mathematician who made contributions in number theory mechanics and astronomy and who gave the first rigorous analysis of fourier series. his family originated from richelet in belgium and the name lejeune dirichlet comes from le jeune de richelet young person from richelet. dirichlet s first paper which was published in brought him instant fame. it concerned fermat s last theorem which claims that there are no positive integer solutions to xn yn zn for n dirichlet gave a partial proof for the case n which was sent to legendre for review and who in turn completed the proof. later dirichlet gave a complete proof for n although a full proof of fermat s last theorem for arbitrary n had to wait until the work of andrew wiles in the closing years of the century. probability distributions figure plots of the dirichlet distribution over three variables where the two horizontal axes are coordinates in the plane of the simplex and the vertical axis corresponds to the value of the density. here k on the left plot k in the centre plot and k in the right plot. modelled using the binomial distribution or as variables and modelled using the multinomial distribution with k the gaussian distribution the gaussian also known as the normal distribution is a widely used model for the distribution of continuous variables. in the case of a single variable x the gaussian distribution can be written in the form n exp n where is the mean and is the variance. for a d-dimensional vector x the multivariate gaussian distribution takes the form where is a d-dimensional mean vector is a d d covariance matrix and denotes the determinant of exp section exercise the gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. for example we have already seen that for a single real variable the distribution that maximizes the entropy is the gaussian. this property applies also to the multivariate gaussian. another situation in which the gaussian distribution arises is when we consider the sum of multiple random variables. the central limit theorem to laplace tells us that subject to certain mild conditions the sum of a set of random variables which is of course itself a random variable has a distribution that becomes increasingly gaussian as the number of terms in the sum increases we can the gaussian distribution n n n figure histogram plots of the mean of n uniformly distributed numbers for various values of n. we observe that as n increases the distribution tends towards a gaussian. illustrate this by considering n variables xn each of which has a uniform distribution over the interval and then considering the distribution of the mean xn for large n this distribution tends to a gaussian as illustrated in practice the convergence to a gaussian as n increases can be in figure very rapid. one consequence of this result is that the binomial distribution which is a distribution over m defined by the sum of n observations of the random binary variable x will tend to a gaussian as n figure for the case of n the gaussian distribution has many important analytical properties and we shall consider several of these in detail. as a result this section will be rather more technically involved than some of the earlier sections and will require familiarity with various matrix identities. however we strongly encourage the reader to become proficient in manipulating gaussian distributions using the techniques presented here as this will prove invaluable in understanding the more complex models presented in later chapters. we begin by considering the geometrical form of the gaussian distribution. the appendix c carl friedrich gauss it is said that when gauss went to elementary school at age his teacher b uttner trying to keep the class occupied asked the pupils to sum the integers from to to the teacher s amazement gauss arrived at the answer in a matter of moments by noting that the sum can be represented as pairs etc. each of which added to giving the answer it is now believed that the problem which was actually set was of the same form but somewhat harder in that the sequence had a larger starting value and a larger increment. gauss was a german math ematician and scientist with a reputation for being a hard-working perfectionist. one of his many contributions was to show that least squares can be derived under the assumption of normally distributed errors. he also created an early formulation of non-euclidean geometry self-consistent geometrical theory that violates the axioms of euclid but was reluctant to discuss it openly for fear that his reputation might suffer if it were seen that he believed in such a geometry. at one point gauss was asked to conduct a geodetic survey of the state of hanover which led to his formulation of the normal distribution now also known as the gaussian. after his death a study of his diaries revealed that he had discovered several important mathematical results years or even decades before they were published by others. probability distributions functional dependence of the gaussian on x is through the quadratic form which appears in the exponent. the quantity is called the mahalanobis distance from to x and reduces to the euclidean distance when is the identity matrix. the gaussian distribution will be constant on surfaces in x-space for which this quadratic form is constant. first of all we note that the matrix can be taken to be symmetric without loss of generality because any antisymmetric component would disappear from the exponent. now consider the eigenvector equation for the covariance matrix ui iui exercise exercise where i d. because is a real symmetric matrix its eigenvalues will be real and its eigenvectors can be chosen to form an orthonormal set so that where iij is the i j element of the identity matrix and satisfies ut i uj iij iij if i j otherwise. exercise the covariance matrix can be expressed as an expansion in terms of its eigenvectors in the form iuiut i i uiut i and similarly the inverse covariance matrix can be expressed as substituting into the quadratic form becomes i i where we have defined we can interpret as a new coordinate system defined by the orthonormal vectors ui that are shifted and rotated with respect to the original xi coordinates. forming the vector y ydt we have i yi ut y ux the gaussian distribution figure the red curve shows the elliptical surface of constant probability density for a gaussian in a two-dimensional space x on which the density is exp of its value at x the major axes of the ellipse are defined by the eigenvectors ui of the covariance matrix with corresponding eigenvalues i. appendix c where u is a matrix whose rows are given by ut i from it follows that u is an orthogonal matrix i.e. it satisfies uut i and hence also utu i where i is the identity matrix. the quadratic form and hence the gaussian density will be constant on surfaces if all of the eigenvalues i are positive then these for which is constant. surfaces represent ellipsoids with their centres at and their axes oriented along ui and with scaling factors in the directions of the axes given by as illustrated in figure i for the gaussian distribution to be well defined it is necessary for all of the eigenvalues i of the covariance matrix to be strictly positive otherwise the distribution cannot be properly normalized. a matrix whose eigenvalues are strictly positive is said to be positive definite. in chapter we will encounter gaussian distributions for which one or more of the eigenvalues are zero in which case the distribution is singular and is confined to a subspace of lower dimensionality. if all of the eigenvalues are nonnegative then the covariance matrix is said to be positive semidefinite. now consider the form of the gaussian distribution in the new coordinate system defined by the yi. in going from the x to the y coordinate system we have a jacobian matrix j with elements given by jij xi yj uji where uji are the elements of the matrix ut. using the orthonormality property of the matrix u we see that the square of the determinant of the jacobian matrix is and hence also the determinant of the covariance matrix can be written probability distributions as the product of its eigenvalues and hence j thus in the yj coordinate system the gaussian distribution takes the form py pxj exp j j ex which is the product of d independent univariate gaussian distributions. the eigenvectors therefore define a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions. the integral of the distribution in the y coordinate system is then py dy exp j j dyj where we have used the result for the normalization of the univariate gaussian. this confirms that the multivariate gaussian is indeed normalized. we now look at the moments of the gaussian distribution and thereby provide an interpretation of the parameters and the expectation of x under the gaussian distribution is given by exp x dx exp where we have changed variables using z x we now note that the exponent is an even function of the components of z and because the integrals over these are taken over the range the term in z in the factor will vanish by symmetry. thus dz zt ex and so we refer to as the mean of the gaussian distribution. we now consider second order moments of the gaussian. in the univariate case we considered the second order moment given by for the multivariate gaussian there are second order moments given by exixj which we can group together to form the matrix exxt. this matrix can be written as exxt exp exp zt xxt dx dz the gaussian distribution where again we have changed variables using z x note that the cross-terms involving zt and tz will again vanish by symmetry. the term t is constant and can be taken outside the integral which itself is unity because the gaussian distribution is normalized. consider the term involving zzt. again we can make use of the eigenvector expansion of the covariance matrix given by together with the completeness of the set of eigenvectors to write z yjuj exp zt zzt dz uiut j exp k k yiyj dy where yj ut j z which gives uiut i i where we have made use of the eigenvector equation together with the fact that the integral on the right-hand side of the middle line vanishes by symmetry unless i j and in the final line we have made use of the results and together with thus we have exxt t for single random variables we subtracted the mean before taking second moments in order to define a variance. similarly in the multivariate case it is again convenient to subtract off the mean giving rise to the covariance of a random vector x defined by covx e for the specific case of a gaussian distribution we can make use of ex together with the result to give exx ext covx because the parameter matrix governs the covariance of x under the gaussian distribution it is called the covariance matrix. although the gaussian distribution is widely used as a density model it suffers from some significant limitations. consider the number of free parameters in the distribution. a general symmetric covariance matrix will have dd independent parameters and there are another d independent parameters in giving dd parameters in total. for large d the total number of parameters exercise probability distributions figure contours of constant probability density for a gaussian distribution in two dimensions in which the covariance matrix is of general form diagonal in which the elliptical contours are aligned with the coordinate axes and proportional to the identity matrix in which the contours are concentric circles. therefore grows quadratically with d and the computational task of manipulating and inverting large matrices can become prohibitive. one way to address this problem is to use restricted forms of the covariance matrix. if we consider covariance matrices that are diagonal so that diag i we then have a total of independent parameters in the density model. the corresponding contours of constant density are given by axis-aligned ellipsoids. we could further restrict the covariance matrix to be proportional to the identity matrix known as an isotropic covariance giving d independent parameters in the model and spherical surfaces of constant density. the three possibilities of general diagonal and isotropic covariance matrices are illustrated in figure unfortunately whereas such approaches limit the number of degrees of freedom in the distribution and make inversion of the covariance matrix a much faster operation they also greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data. a further limitation of the gaussian distribution is that it is intrinsically unimodal has a single maximum and so is unable to provide a good approximation to multimodal distributions. thus the gaussian distribution can be both too flexible in the sense of having too many parameters while also being too limited in the range of distributions that it can adequately represent. we will see later that the introduction of latent variables also called hidden variables or unobserved variables allows both of these problems to be addressed. in particular a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of gaussians as discussed in section similarly the introduction of continuous latent variables as described in chapter leads to models in which the number of free parameters can be controlled independently of the dimensionality d of the data space while still allowing the model to capture the dominant correlations in the data set. indeed these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of practical applications. for instance the gaussian version of the markov random field which is widely used as a probabilistic model of images is a gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reflecting the spatial organization of the pixels. similarly the linear dynamical system used to model time series data for applications such as tracking is also a joint gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution. a powerful framework for expressing the form and properties of section section the gaussian distribution such complex distributions is that of probabilistic graphical models which will form the subject of chapter conditional gaussian distributions an important property of the multivariate gaussian distribution is that if two sets of variables are jointly gaussian then the conditional distribution of one set conditioned on the other is again gaussian. similarly the marginal distribution of either set is also gaussian. consider first the case of conditional distributions. suppose x is a d-dimensional vector with gaussian distribution n and that we partition x into two disjoint subsets xa and xb. without loss of generality we can take xa to form the first m components of x with xb comprising the remaining d m components so that we also define corresponding partitions of the mean vector given by and of the covariance matrix given by note that the symmetry t of the covariance matrix implies that aa and bb are symmetric while ba t ab. in many situations it will be convenient to work with the inverse of the covari ance matrix which is known as the precision matrix. in fact we shall see that some properties of gaussian distributions are most naturally expressed in terms of the covariance whereas others take a simpler form when viewed in terms of the precision. we therefore also introduce the partitioned form of the precision matrix aa ab ba bb exercise corresponding to the partitioning of the vector x. because the inverse of a symmetric matrix is also symmetric we see that aa and bb are symmetric while t ab ba. it should be stressed at this point that for instance aa is not simply given by the inverse of aa. in fact we shall shortly examine the relation between the inverse of a partitioned matrix and the inverses of its partitions. let us begin by finding an expression for the conditional distribution pxaxb. from the product rule of probability we see that this conditional distribution can be x xa xb a b aa ab ba bb probability distributions evaluated from the joint distribution px pxa xb simply by fixing xb to the observed value and normalizing the resulting expression to obtain a valid probability distribution over xa. instead of performing this normalization explicitly we can obtain the solution more efficiently by considering the quadratic form in the exponent of the gaussian distribution given by and then reinstating the normalization coefficient at the end of the calculation. if we make use of the partitioning and we obtain at abxb b at aaxa a bt bbxb b. bt baxa a we see that as a function of xa this is again a quadratic form and hence the corresponding conditional distribution pxaxb will be gaussian. because this distribution is completely characterized by its mean and its covariance our goal will be to identify expressions for the mean and covariance of pxaxb by inspection of this is an example of a rather common operation associated with gaussian distributions sometimes called completing the square in which we are given a quadratic form defining the exponent terms in a gaussian distribution and we need to determine the corresponding mean and covariance. such problems can be solved straightforwardly by noting that the exponent in a general gaussian distribution n can be written xt xt const and the coefficient of the linear term in x to where const denotes terms which are independent of x and we have made use of the symmetry of thus if we take our general quadratic form and express it in the form given by the right-hand side of then we can immediately equate the matrix of coefficients entering the second order term in x to the inverse covariance from which we can matrix obtain now let us apply this procedure to the conditional gaussian distribution pxaxb for which the quadratic form in the exponent is given by we will denote the mean and covariance of this distribution by ab and ab respectively. consider the functional dependence of on xa in which xb is regarded as a constant. if we pick out all terms that are second order in xa we have xt a aaxa from which we can immediately conclude that the covariance precision of pxaxb is given by ab aa the gaussian distribution now consider all of the terms in that are linear in xa a aa a abxb b xt where we have used t the coefficient of xa in this expression must equal ba ab. from our discussion of the general form ab ab and hence ab ab aa a abxb b a aa abxb b where we have made use of the results and are expressed in terms of the partitioned precision matrix of the original joint distribution pxa xb. we can also express these results in terms of the corresponding partitioned covariance matrix. to do this we make use of the following identity for the inverse of a partitioned matrix mbd m d d d a b c d exercise where we have defined the quantity m is known as the schur complement of the matrix on the left-hand side of with respect to the submatrix d. using the definition m bd aa ab ba bb aa ab ba bb and making use of we have aa aa ab bb ba ab aa ab bb ba ab bb from these we obtain the following expressions for the mean and covariance of the conditional distribution pxaxb bb b ab a ab ab aa ab bb ba. comparing and we see that the conditional distribution pxaxb takes a simpler form when expressed in terms of the partitioned precision matrix than when it is expressed in terms of the partitioned covariance matrix. note that the mean of the conditional distribution pxaxb given by is a linear function of xb and that the covariance given by is independent of xa. this represents an example of a linear-gaussian model. section probability distributions marginal gaussian distributions we have seen that if a joint distribution pxa xb is gaussian then the conditional distribution pxaxb will again be gaussian. now we turn to a discussion of the marginal distribution given by pxa pxa xb dxb which as we shall see is also gaussian. once again our strategy for evaluating this distribution efficiently will be to focus on the quadratic form in the exponent of the joint distribution and thereby to identify the mean and covariance of the marginal distribution pxa. the quadratic form for the joint distribution can be expressed using the partitioned precision matrix in the form because our goal is to integrate out xb this is most easily achieved by first considering the terms involving xb and then completing the square in order to facilitate integration. picking out just those terms that involve xb we have bb mt bbxb b m bb m b bbxbxt xt bb m mt where we have defined m bb b baxa a. we see that the dependence on xb has been cast into the standard quadratic form of a gaussian distribution corresponding to the first term on the right-hand side of plus a term that does not depend on xb that does depend on xa. thus when we take the exponential of this quadratic form we see that the integration over xb required by will take the form bb mt bbxb bb m dxb. exp this integration is easily performed by noting that it is the integral over an unnormalized gaussian and so the result will be the reciprocal of the normalization coefficient. we know from the form of the normalized gaussian given by that this coefficient is independent of the mean and depends only on the determinant of the covariance matrix. thus by completing the square with respect to xb we can integrate out xb and the only term remaining from the contributions on the left-hand side of that depends on xa is the last term on the right-hand side of in which m is given by combining this term with the remaining terms from the gaussian distribution that depend on xa we obtain bb b baxa at bb bb b baxa a a aa a ab b const xt a aaxa xt a aa ab xt bb baxa a aa ab bb ba a const where const denotes quantities independent of xa. again by comparison with we see that the covariance of the marginal distribution of pxa is given by a aa ab bb ba similarly the mean is given by a aa ab bb ba a a where we have used the covariance in is expressed in terms of the partitioned precision matrix given by we can rewrite this in terms of the corresponding partitioning of the covariance matrix given by as we did for the conditional distribution. these partitioned matrices are related by aa ab ba bb aa ab ba bb aa. making use of we then have aa ab bb ba thus we obtain the intuitively satisfying result that the marginal distribution pxa has mean and covariance given by exa a covxa aa. we see that for a marginal distribution the mean and covariance are most simply expressed in terms of the partitioned covariance matrix in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expressions. our results for the marginal and conditional distributions of a partitioned gaus sian are summarized below. partitioned gaussians given a joint gaussian distribution n with and xa xb a b x probability distributions xb xb pxaxb pxa xb pxa xa xa figure the plot on the left shows the contours of a gaussian distribution pxa xb over two variables and the plot on the right shows the marginal distribution pxa curve and the conditional distribution pxaxb for xb curve. aa ab ba bb aa ab ba bb conditional distribution pxaxb n ab aa ab a aa abxb b. marginal distribution pxa n a aa. we illustrate the idea of conditional and marginal distributions associated with a multivariate gaussian using an example involving two variables in figure bayes theorem for gaussian variables in sections and we considered a gaussian px in which we partitioned the vector x into two subvectors x xb and then found expressions for the conditional distribution pxaxb and the marginal distribution pxa. we noted that the mean of the conditional distribution pxaxb was a linear function of xb. here we shall suppose that we are given a gaussian marginal distribution px and a gaussian conditional distribution pyx in which pyx has a mean that is a linear function of x and a covariance which is independent of x. this is an example of the gaussian distribution a linear gaussian model and ghahramani which we shall study in greater generality in section we wish to find the marginal distribution py and the conditional distribution pxy. this is a problem that will arise frequently in subsequent chapters and it will prove convenient to derive the general results here. we shall take the marginal and conditional distributions to be px pyx x yax b l where a and b are parameters governing the means and and l are precision matrices. if x has dimensionality m and y has dimensionality d then the matrix a has size d m. first we find an expression for the joint distribution over x and y. to do this we define z x y and then consider the log of the joint distribution ln pz ln px ln pyx ax btly ax b const where const denotes terms independent of x and y. as before we see that this is a quadratic function of the components of z and hence pz is gaussian distribution. to find the precision of this gaussian we consider the second order terms in which can be written as xt atlax la x y ytly ytlax atla atl l xtatly ztrz and so the gaussian distribution over z has precision covariance matrix given by r atla atl la l x y exercise the covariance matrix is found by taking the inverse of the precision which can be done using the matrix inversion formula to give covz r l a a probability distributions similarly we can find the mean of the gaussian distribution over z by identify ing the linear terms in which are given by xt xtatlb ytlb atlb lb x y atlb lb using our earlier result obtained by completing the square over the quadratic form of a multivariate gaussian we find that the mean of z is given by ez r exercise making use of we then obtain ez a b section section next we find an expression for the marginal distribution py in which we have marginalized over x. recall that the marginal distribution over a subset of the components of a gaussian random vector takes a particularly simple form when expressed in terms of the partitioned covariance matrix. specifically its mean and covariance are given by and respectively. making use of and we see that the mean and covariance of the marginal distribution py are given by ey a b covy l a a special case of this result is when a i in which case it reduces to the convolution of two gaussians for which we see that the mean of the convolution is the sum of the mean of the two gaussians and the covariance of the convolution is the sum of their covariances. finally we seek an expression for the conditional pxy. recall that the results for the conditional distribution are most easily expressed in terms of the partitioned precision matrix using and applying these results to and we see that the conditional distribution pxy has mean and covariance given by exy atla covxy atla atly b the evaluation of this conditional can be seen as an example of bayes theorem. we can interpret the distribution px as a prior distribution over x. if the variable y is observed then the conditional distribution pxy represents the corresponding posterior distribution over x. having found the marginal and conditional distributions we effectively expressed the joint distribution pz pxpyx in the form pxypy. these results are summarized below. the gaussian distribution marginal and conditional gaussians given a marginal gaussian distribution for x and a conditional gaussian distribution for y given x in the form px n pyx n b l the marginal distribution of y and the conditional distribution of x given y are given by py n b l a pxy n b where atla maximum likelihood for the gaussian given a data set x xn in which the observations are assumed to be drawn independently from a multivariate gaussian distribution we can estimate the parameters of the distribution by maximum likelihood. the log likelihood function is given by ln px n d n ln by simple rearrangement we see that the likelihood function depends on the data set only through the two quantities xn xnxt n. appendix c these are known as the sufficient statistics for the gaussian distribution. using the derivative of the log likelihood with respect to is given by ln px and setting this derivative to zero we obtain the solution for the maximum likelihood estimate of the mean given by ml n xn probability distributions exercise which is the mean of the observed set of data points. the maximization of with respect to is rather more involved. the simplest approach is to ignore the symmetry constraint and show that the resulting solution is symmetric as required. alternative derivations of this result which impose the symmetry and positive definiteness constraints explicitly can be found in magnus and neudecker the result is as expected and takes the form ml n mlxn mlt which involves ml because this is the result of a joint maximization with respect to and note that the solution for ml does not depend on ml and so we can first evaluate ml and then use this to evaluate ml. if we evaluate the expectations of the maximum likelihood solutions under the exercise true distribution we obtain the following results e ml e ml n this bias by defining a different given by we see that the expectation of the maximum likelihood estimate for the mean is equal to the true mean. however the maximum likelihood estimate for the covariance has an expectation that is less than the true value and hence it is biased. we can correct n clearly from and the expectation is equal to mlxn mlt. n sequential estimation our discussion of the maximum likelihood solution for the parameters of a gaussian distribution provides a convenient opportunity to give a more general discussion of the topic of sequential estimation for maximum likelihood. sequential methods allow data points to be processed one at a time and then discarded and are important for on-line applications and also where large data sets are involved so that batch processing of all data points at once is infeasible. consider the result for the maximum likelihood estimator of the mean ml when it is based on n observations. if we ml which we will denote by the gaussian distribution figure a schematic illustration of two correlated random variables z and together with the regression function f given by the conditional expectation ez the robbinsmonro algorithm provides a general sequential procedure for finding the root of such functions. z f dissect out the contribution from the final data point xn we obtain ml n xn n xn n n xn n n ml xn ml n n this result has a nice interpretation as follows. after observing n data points we have estimated by we now observe data point xn and we obtain our revised estimate ml by moving the old estimate a small amount proportional to in the direction of the error signal note that as n increases so the contribution from successive data points gets smaller. ml ml ml the result will clearly give the same answer as the batch result because the two formulae are equivalent. however we will not always be able to derive a sequential algorithm by this route and so we seek a more general formulation of sequential learning which leads us to the robbins-monro algorithm. consider a pair of random variables and z governed by a joint distribution pz the conditional expectation of z given defines a deterministic function f that is given by f ez zpz dz and is illustrated schematically in figure functions defined in this way are called regression functions. our goal is to find the root at which f if we had a large data set of observations of z and then we could model the regression function directly and then obtain an estimate of its root. suppose however that we observe values of z one at a time and we wish to find a corresponding sequential estimation scheme for the following general procedure for solving such problems was given by probability distributions robbins and monro we shall assume that the conditional variance of z is finite so that and we shall also without loss of generality consider the case where f for and f for as is the case in figure the robbins-monro procedure then defines a sequence of successive estimates of the root given by e an where z is an observed value of z when takes the value the coefficients represent a sequence of positive numbers that satisfy the conditions n an lim an n n n it can then be shown and monro fukunaga that the sequence of estimates given by does indeed converge to the root with probability one. note that the first condition ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value. the second condition is required to ensure that the algorithm does not converge short of the root and the third condition is needed to ensure that the accumulated noise has finite variance and hence does not spoil convergence. now let us consider how a general maximum likelihood problem can be solved sequentially using the robbins-monro algorithm. by definition the maximum likelihood solution ml is a stationary point of the log likelihood function and hence satisfies exchanging the derivative and the summation and taking the limit n we have ml n ln pxn lim n n ln pxn ex ln px and so we see that finding the maximum likelihood solution corresponds to finding the root of a regression function. we can therefore apply the robbins-monro procedure which now takes the form an ln pxn the gaussian distribution z figure in the case of a gaussian distribution with corresponding to the mean the regression function illustrated in figure takes the form of a straight line as shown in red. in this case the random variable z corresponds to the derivative of the log likelihood function and is given by ml and its expectation that defines the regression function is a straight line given by ml the root of the regression function corresponds to the maximum likelihood estimator ml. ml pz as a specific example we consider once again the sequential estimation of the mean of a gaussian distribution in which case the parameter is the estimate ml of the mean of the gaussian and the random variable z is given by ml. z ln px ml ml thus the distribution of z is gaussian with mean ml as illustrated in figure substituting into we obtain the univariate form of provided we choose the coefficients an to have the form an note that although we have focussed on the case of a single variable the same technique together with the same restrictions on the coefficients an apply equally to the multivariate case bayesian inference for the gaussian the maximum likelihood framework gave point estimates for the parameters and now we develop a bayesian treatment by introducing prior distributions over these parameters. let us begin with a simple example in which we consider a single gaussian random variable x. we shall suppose that the variance is known and we consider the task of inferring the mean given a set of n observations x xn. the likelihood function that is the probability of the observed data given viewed as a function of is given by px pxn again we emphasize that the likelihood function px is not a probability distribution over and is not normalized. exp we see that the likelihood function takes the form of the exponential of a quadratic form in thus if we choose a prior p given by a gaussian it will be a probability distributions conjugate distribution for this likelihood function because the corresponding posterior will be a product of two exponentials of quadratic functions of and hence will also be gaussian. we therefore take our prior distribution to be p and the posterior distribution is given by p px exercise simple manipulation involving completing the square in the exponent shows that the posterior distribution is given by p n n where n n n n n n ml in which ml is the maximum likelihood solution for given by the sample mean ml n xn. it is worth spending a moment studying the form of the posterior mean and variance. first of all we note that the mean of the posterior distribution given by is a compromise between the prior mean and the maximum likelihood solution ml. if the number of observed data points n then reduces to the prior mean as expected. for n the posterior mean is given by the maximum likelihood solution. similarly consider the result for the variance of the posterior distribution. we see that this is most naturally expressed in terms of the inverse variance which is called the precision. furthermore the precisions are additive so that the precision of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points. as we increase the number of observed data points the precision steadily increases corresponding to a posterior distribution with steadily decreasing variance. with no observed data points we have the prior variance whereas if the number of data points n the variance n goes to zero and the posterior distribution becomes infinitely peaked around the maximum likelihood solution. we therefore see that the maximum likelihood result of a point estimate for given by is recovered precisely from the bayesian formalism in the limit of an infinite number in which the of observations. note also that for finite n if we take the limit prior has infinite variance then the posterior mean reduces to the maximum n likelihood result while from the posterior variance is given by the gaussian distribution figure illustration of bayesian inference for the mean of a gaussian distribution in which the variance is assumed to be known. the curves show the prior distribution over curve labelled n which in this case is itself gaussian along with the posterior distribution given by for increasing numbers n of data points. the data points are generated from a gaussian of mean and variance and the prior is chosen to have mean in both the prior and the likelihood function the variance is set to the true value. n n n n exercise section we illustrate our analysis of bayesian inference for the mean of a gaussian distribution in figure the generalization of this result to the case of a ddimensional gaussian random variable x with known covariance and unknown mean is straightforward. we have already seen how the maximum likelihood expression for the mean of a gaussian can be re-cast as a sequential update formula in which the mean after observing n data points was expressed in terms of the mean after observing n data points together with the contribution from data point xn in fact the bayesian paradigm leads very naturally to a sequential view of the inference problem. to see this in the context of the inference of the mean of a gaussian we write the posterior distribution with the contribution from the final data point xn separated out so that n p p pxn pxn the term in square brackets is to a normalization coefficient just the posterior distribution after observing n data points. we see that this can be viewed as a prior distribution which is combined using bayes theorem with the likelihood function associated with data point xn to arrive at the posterior distribution after observing n data points. this sequential view of bayesian inference is very general and applies to any problem in which the observed data are assumed to be independent and identically distributed. so far we have assumed that the variance of the gaussian distribution over the data is known and our goal is to infer the mean. now let us suppose that the mean is known and we wish to infer the variance. again our calculations will be greatly simplified if we choose a conjugate form for the prior distribution. it turns out to be most convenient to work with the precision the likelihood function for takes the form px exp n probability distributions a b a b a b figure plot of the gamma distribution gam b defined by for various values of the parameters a and b. the corresponding conjugate prior should therefore be proportional to the product of a power of and the exponential of a linear function of this corresponds to the gamma distribution which is defined by gam b ba a exp b exercise exercise here is the gamma function that is defined by and that ensures that is correctly normalized. the gamma distribution has a finite integral if a and the distribution itself is finite if a it is plotted for various values of a and b in figure the mean and variance of the gamma distribution are given by e a b var a consider a prior distribution gam if we multiply by the likelihood function then we obtain a posterior distribution p exp which we recognize as a gamma distribution of the form gam bn where an n bn n ml where ml is the maximum likelihood estimator of the variance. note that in there is no need to keep track of the normalization constants in the prior and the likelihood function because if required the correct coefficient can be found at the end using the normalized form for the gamma distribution. the gaussian distribution from we see that the effect of observing n data points is to increase the value of the coefficient a by thus we can interpret the parameter in the prior in terms of effective prior observations. similarly from we see that the n data points contribute n ml is the variance and so we can interpret the parameter in the prior as arising from the effective prior observations having variance recall that we made an analogous interpretation for the dirichlet prior. these distributions are examples of the exponential family and we shall see that the interpretation of a conjugate prior in terms of effective fictitious data points is a general one for the exponential family of distributions. to the parameter b where section instead of working with the precision we can consider the variance itself. the conjugate prior in this case is called the inverse gamma distribution although we shall not discuss this further because we will find it more convenient to work with the precision. now suppose that both the mean and the precision are unknown. to find a conjugate prior we consider the dependence of the likelihood function on and exp px exp exp xn n we now wish to identify a prior distribution p that has the same functional dependence on and as the likelihood function and that should therefore take the form p exp c expc d exp exp where c d and are constants. since we can always write p p we can find p and p by inspection. in particular we see that p is a gaussian whose precision is a linear function of and that p is a gamma distribution so that the normalized prior takes the form d p n b where we have defined new constants given by c a b d the distribution is called the normal-gamma or gaussian-gamma distribution and is plotted in figure note that this is not simply the product of an independent gaussian prior over and a gamma prior over because the precision of is a linear function of even if we chose a prior in which and were independent the posterior distribution would exhibit a coupling between the precision of and the value of probability distributions figure contour plot of the normal-gamma distribution for parameter values a and b exercise in the case of the multivariate gaussian distribution x for a ddimensional variable x the conjugate prior distribution for the mean assuming the precision is known is again a gaussian. for known mean and unknown precision matrix the conjugate prior is the wishart distribution given by w b d exp where is called the number of degrees of freedom of the distribution w is a d d scale matrix and tr denotes the trace. the normalization constant b is given by trw i bw dd again it is also possible to define a conjugate prior over the covariance matrix itself rather than over the precision matrix which leads to the inverse wishart distribution although we shall not discuss this further. if both the mean and the precision are unknown then following a similar line of reasoning to the univariate case the conjugate prior is given by p w n which is known as the normal-wishart or gaussian-wishart distribution. student s t-distribution we have seen that the conjugate prior for the precision of a gaussian is given by a gamma distribution. if we have a univariate gaussian n together with a gamma prior gam b and we integrate out the precision we obtain the marginal distribution of x in the form section exercise figure plot of student s t-distribution for and for various values of the limit corresponds to a gaussian distribution with mean and precision the gaussian distribution ba px a b n bae b a b d exp b d a where we have made the change of variable z by convention we define new parameters given by and ab in terms of which the distribution px a b takes the form stx which is known as student s t-distribution. the parameter is sometimes called the precision of the t-distribution even though it is not in general equal to the inverse of the variance. the parameter is called the degrees of freedom and its effect is illustrated in figure for the particular case of the t-distribution reduces to the cauchy distribution while in the limit the t-distribution stx becomes a gaussian n with mean and precision from we see that student s t-distribution is obtained by adding up an infinite number of gaussian distributions having the same mean but different precisions. this can be interpreted as an infinite mixture of gaussians mixtures will be discussed in detail in section the result is a distribution that in general has longer tails than a gaussian as was seen in figure this gives the tdistribution an important property called robustness which means that it is much less sensitive than the gaussian to the presence of a few data points which are outliers. the robustness of the t-distribution is illustrated in figure which compares the maximum likelihood solutions for a gaussian and a t-distribution. note that the maximum likelihood solution for the t-distribution can be found using the expectationmaximization algorithm. here we see that the effect of a small number of exercise exercise probability distributions figure illustration of the robustness of student s t-distribution compared to a gaussian. histogram distribution of data points drawn from a gaussian distribution together with the maximum likelihood fit obtained from a t-distribution curve and a gaussian curve largely hidden by the red curve. because the t-distribution contains the gaussian as a special case it gives almost the same solution as the gaussian. the same data set but with three additional outlying data points showing how the gaussian curve is strongly distorted by the outliers whereas the t-distribution curve is relatively unaffected. outliers is much less significant for the t-distribution than for the gaussian. outliers can arise in practical applications either because the process that generates the data corresponds to a distribution having a heavy tail or simply through mislabelled data. robustness is also an important property for regression problems. unsurprisingly the least squares approach to regression does not exhibit robustness because it corresponds to maximum likelihood under a gaussian distribution. by basing a regression model on a heavy-tailed distribution such as a t-distribution we obtain a more robust model. if we go back to and substitute the alternative parameters ab and ba we see that the t-distribution can be written in the form stx x gam d we can then generalize this to a multivariate gaussian n to obtain the corresponding multivariate student s t-distribution in the form stx n d exercise using the same technique as for the univariate case we can evaluate this integral to give the gaussian distribution stx where d is the dimensionality of x and is the squared mahalanobis distance defined by exercise this is the multivariate form of student s t-distribution and satisfies the following properties ex covx modex if if with corresponding results for the univariate case. periodic variables although gaussian distributions are of great practical significance both in their own right and as building blocks for more complex probabilistic models there are situations in which they are inappropriate as density models for continuous variables. one important case which arises in practical applications is that of periodic variables. an example of a periodic variable would be the wind direction at a particular geographical location. we might for instance measure values of wind direction on a number of days and wish to summarize this using a parametric distribution. another example is calendar time where we may be interested in modelling quantities that are believed to be periodic over hours or over an annual cycle. such quantities can conveniently be represented using an angular coordinate and we might be tempted to treat periodic variables by choosing some direction as the origin and then applying a conventional distribution such as the gaussian. such an approach however would give results that were strongly dependent on the arbitrary choice of origin. suppose for instance that we have two observations at and we model them using a standard univariate gaussian distribution. if we choose the origin at then the sample mean of this data set will be with standard deviation whereas if we choose the origin at then the mean will be we clearly need to develop a special approach for the treatment of periodic variables. let us consider the problem of evaluating the mean of a set of observations d n of a periodic variable. from now on we shall assume that is measured in radians. we have already seen that the simple average n will be strongly coordinate dependent. to find an invariant measure of the mean we note that the observations can be viewed as points on the unit circle and can therefore be described instead by two-dimensional unit vectors xn where for n n as illustrated in figure we can average the vectors and the standard deviation will be probability distributions figure illustration of the representation of values n of a periodic variable as twodimensional vectors xn living on the unit circle. also shown is the average x of those vectors. x r instead to give xn x n and then find the corresponding angle of this average. clearly this definition will ensure that the location of the mean is independent of the origin of the angular coordinate. note that x will typically lie inside the unit circle. the cartesian coordinates of the observations are given by xn n sin n and we can write the cartesian coordinates of the sample mean in the form x cos r sin substituting into and equating the and components then gives r cos n cos n r sin n sin n. taking the ratio and using the identity tan sin cos we can solve for to give tan n sin n n cos n shortly we shall see how this result arises naturally as the maximum likelihood estimator for an appropriately defined distribution over a periodic variable. we now consider a periodic generalization of the gaussian called the von mises distribution. here we shall limit our attention to univariate distributions although periodic distributions can also be found over hyperspheres of arbitrary dimension. for an extensive discussion of periodic distributions see mardia and jupp by convention we will consider distributions p that have period any probability density p defined over must not only be nonnegative and integrate the gaussian distribution figure the von mises distribution can be derived by considering a two-dimensional gaussian of the form whose density contours are shown in blue and conditioning on the unit circle shown in red. px r to one but it must also be periodic. thus p must satisfy the three conditions p p d p p from it follows that p p for any integer m. we can easily obtain a gaussian-like distribution that satisfies these three properties as follows. consider a gaussian distribution over two variables x having mean and a covariance matrix where i is the identity matrix so that exp the contours of constant px are circles as illustrated in figure now suppose we consider the value of this distribution along a circle of fixed radius. then by construction this distribution will be periodic although it will not be normalized. we can determine the form of this distribution by transforming from cartesian coordinates to polar coordinates so that r sin we also map the mean into polar coordinates by writing r cos cos sin next we substitute these transformations into the two-dimensional gaussian distribution and then condition on the unit circle r noting that we are interested only in the dependence on focussing on the exponent in the gaussian distribution we have cos cos sin sin cos cos sin sin cos const probability distributions m m m m figure the von mises distribution plotted for two different parameter values shown as a cartesian plot on the left and as the corresponding polar plot on the right. exercise where const denotes terms independent of and we have made use of the following trigonometrical identities a a if we now define m we obtain our final expression for the distribution of p along the unit circle r in the form cos a cos b sin a sin b cosa b. p m expm cos which is called the von mises distribution or the circular normal. here the parameter corresponds to the mean of the distribution while m which is known as the concentration parameter is analogous to the inverse variance for the gaussian. the normalization coefficient in is expressed in terms of which is the zeroth-order bessel function of the first kind and stegun and is defined by expm cos d exercise for large m the distribution becomes approximately gaussian. the von mises distribution is plotted in figure and the function is plotted in figure now consider the maximum likelihood estimators for the parameters and m for the von mises distribution. the log likelihood function is given by cos n ln pd m n n ln m the gaussian distribution am m m figure plot of the bessel function defined by together with the function am defined by setting the derivative with respect to equal to zero gives sin n to solve for we make use of the trigonometric identity sina b cos b sin a cos a sin b n sin n n cos n exercise from which we obtain tan ml which we recognize as the result obtained earlier for the mean of the observations viewed in a two-dimensional cartesian space. similarly maximizing with respect to m and making use of i and stegun we have am n cos n ml where we have substituted for the maximum likelihood solution for ml that we are performing a joint optimization over and m and we have defined am the function am is plotted in figure making use of the trigonometric identity we can write in the form amml cos ml sin n sin ml cos n n n probability distributions on the left figure plots of the old faithful data in which the blue curves show contours of constant probability density. is a single gaussian distribution which has been fitted to the data using maximum likelihood. note that this distribution fails to capture the two clumps in the data and indeed places much of its probability mass in the central region between the clumps where the data are relatively sparse. on the right the distribution is given by a linear combination of two gaussians which has been fitted to the data by maximum likelihood using techniques discussed chapter and which gives a better representation of the data. the right-hand side of is easily evaluated and the function am can be inverted numerically. for completeness we mention briefly some alternative techniques for the construction of periodic distributions. the simplest approach is to use a histogram of observations in which the angular coordinate is divided into fixed bins. this has the virtue of simplicity and flexibility but also suffers from significant limitations as we shall see when we discuss histogram methods in more detail in section another approach starts like the von mises distribution from a gaussian distribution over a euclidean space but now marginalizes onto the unit circle rather than conditioning and jupp however this leads to more complex forms of distribution and will not be discussed further. finally any valid distribution over the real axis as a gaussian can be turned into a periodic distribution by mapping successive intervals of width onto the periodic variable which corresponds to wrapping the real axis around unit circle. again the resulting distribution is more complex to handle than the von mises distribution. one limitation of the von mises distribution is that it is unimodal. by forming mixtures of von mises distributions we obtain a flexible framework for modelling periodic variables that can handle multimodality. for an example of a machine learning application that makes use of von mises distributions see lawrence et al. and for extensions to modelling conditional densities for regression problems see bishop and nabney mixtures of gaussians while the gaussian distribution has some important analytical properties it suffers from significant limitations when it comes to modelling real data sets. consider the example shown in figure this is known as the old faithful data set and comprises measurements of the eruption of the old faithful geyser at yellowstone national park in the usa. each measurement comprises the duration of appendix a the gaussian distribution figure example of a gaussian mixture distribution in one dimension showing three gaussians scaled by a coefficient in blue and their sum in red. px the eruption in minutes axis and the time in minutes to the next eruption axis. we see that the data set forms two dominant clumps and that a simple gaussian distribution is unable to capture this structure whereas a linear superposition of two gaussians gives a better characterization of the data set. x such superpositions formed by taking linear combinations of more basic distributions such as gaussians can be formulated as probabilistic models known as mixture distributions and basford mclachlan and peel in figure we see that a linear combination of gaussians can give rise to very complex densities. by using a sufficient number of gaussians and by adjusting their means and covariances as well as the coefficients in the linear combination almost any continuous density can be approximated to arbitrary accuracy. we therefore consider a superposition of k gaussian densities of the form px kn k k which is called a mixture of gaussians. each gaussian density n k k is called a component of the mixture and has its own mean k and covariance k. contour and surface plots for a gaussian mixture having components are shown in figure in this section we shall consider gaussian components to illustrate the framework of mixture models. more generally mixture models can comprise linear combinations of other distributions. for instance in section we shall consider mixtures of bernoulli distributions as an example of a mixture model for discrete variables. section the parameters k in are called mixing coefficients. if we integrate both sides of with respect to x and note that both px and the individual gaussian components are normalized we obtain k also the requirement that px together with n k k implies k for all k. combining this with the condition we obtain k probability distributions figure illustration of a mixture of gaussians in a two-dimensional space. contours of constant density for each of the mixture components in which the components are denoted red blue and green and the values of the mixing coefficients are shown below each component. contours of the marginal probability density px of the mixture distribution. a surface plot of the distribution px. we therefore see that the mixing coefficients satisfy the requirements to be probabilities. from the sum and product rules the marginal density is given by px pkpxk which is equivalent to in which we can view k pk as the prior probability of picking the kth component and the density n k k pxk as the probability of x conditioned on k. as we shall see in later chapters an important role is played by the posterior probabilities pkx which are also known as responsibilities. from bayes theorem these are given by kx pkx pkpxk l plpxl kn k k l ln l l we shall discuss the probabilistic interpretation of the mixture distribution in greater detail in chapter the form of the gaussian mixture distribution is governed by the parameters and where we have used the notation k k and k. one way to set the values of these parameters is to use maximum likelihood. from the log of the likelihood function is given by kn k k ln px ln the exponential family where x xn. we immediately see that the situation is now much more complex than with a single gaussian due to the presence of the summation over k inside the logarithm. as a result the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. one approach to maximizing the likelihood function is to use iterative numerical optimization techniques nocedal and wright bishop and nabney alternatively we can employ a powerful framework called expectation maximization which will be discussed at length in chapter the exponential family the probability distributions that we have studied so far in this chapter the exception of the gaussian mixture are specific examples of a broad class of distributions called the exponential family and hart bernardo and smith members of the exponential family have many important properties in common and it is illuminating to discuss these properties in some generality. the exponential family of distributions over x given parameters is defined to be the set of distributions of the form px hxg exp tux where x may be scalar or vector and may be discrete or continuous. here are called the natural parameters of the distribution and ux is some function of x. the function g can be interpreted as the coefficient that ensures that the distribution is normalized and therefore satisfies g hx exp tux dx where the integration is replaced by summation if x is a discrete variable. we begin by taking some examples of the distributions introduced earlier in the chapter and showing that they are indeed members of the exponential family. consider first the bernoulli distribution px bernx x. expressing the right-hand side as the exponential of the logarithm we have px expx ln x exp ln ln x comparison with allows us to identify probability distributions which we can solve for to give where exp is called the logistic sigmoid function. thus we can write the bernoulli distribution using the standard representation in the form px exp x where we have used which is easily proved from comparison with shows that next consider the multinomial distribution that for a single observation x takes the form px ux x hx g xk k exp xk ln k where x xn again we can write this in the standard representation so that where k ln k and we have defined m again comparing with we have px exp tx ux x hx g note that the parameters k are not independent because the parameters k are subject to the constraint k so that given any m of the parameters k the value of the remaining parameter is fixed. in some circumstances it will be convenient to remove this constraint by expressing the distribution in terms of only m parameters. this can be achieved by using the relationship to eliminate m by expressing it in terms of the remaining k where k m thereby leaving m parameters. note that these remaining parameters are still subject to the constraints k k m exp exp exp xk ln k m m we now identify ln ln m m k k k xk ln k xk ln ln xk k j m j j k k exp k j exp j m the exponential family making use of the constraint the multinomial distribution in this representation then becomes which we can solve for k by first summing both sides over k and then rearranging and back-substituting to give this is called the softmax function or the normalized exponential. in this representation the multinomial distribution therefore takes the form px exp k exp tx. this is the standard form of the exponential family with parameter vector m in which ux x hx g m exp k finally let us consider the gaussian distribution. for the univariate gaussian we have px exp exp x probability distributions exercise exercise which after some simple rearrangement can be cast in the standard exponential family form with x ux hx g exp maximum likelihood and sufficient statistics let us now consider the problem of estimating the parameter vector in the general exponential family distribution using the technique of maximum likelihood. taking the gradient of both sides of with respect to we have g hx exp tux dx g hx exp tux ux dx rearranging and making use again of then gives g g g hx exp tux ux dx eux where we have used we therefore obtain the result ln g eux. note that the covariance of ux can be expressed in terms of the second derivatives of g and similarly for higher order moments. thus provided we can normalize a distribution from the exponential family we can always find its moments by simple differentiation. xn for which the likelihood function is given by now consider a set of independent identically distributed data denoted by x hxn g exp t uxn px setting the gradient of ln px with respect to to zero we get the following condition to be satisfied by the maximum likelihood estimator ml ln g ml n uxn which can in principle be solved to obtain ml. we see that the solution for the n uxn which maximum likelihood estimator depends on the data only through is therefore called the sufficient statistic of the distribution we do not need to store the entire data set itself but only the value of the sufficient statistic. for the bernoulli distribution for example the function ux is given just by x and so we need only keep the sum of the data points whereas for the gaussian ux and so we should keep both the sum of and the sum of n. if we consider the limit n then the right-hand side of becomes eux and so by comparing with we see that in this limit ml will equal the true value in fact this sufficiency property holds also for bayesian inference although we shall defer discussion of this until chapter when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these important concepts. conjugate priors we have already encountered the concept of a conjugate prior several times for example in the context of the bernoulli distribution which the conjugate prior is the beta distribution or the gaussian the conjugate prior for the mean is a gaussian and the conjugate prior for the precision is the wishart distribution. in general for a given probability distribution px we can seek a prior p that is conjugate to the likelihood function so that the posterior distribution has the same functional form as the prior. for any member of the exponential family there exists a conjugate prior that can be written in the form p f exp t where f is a normalization coefficient and g is the same function as appears in to see that this is indeed conjugate let us multiply the prior by the likelihood function to obtain the posterior distribution up to a normalization coefficient in the form the exponential family p g exp t uxn this again takes the same functional form as the prior confirming conjugacy. furthermore we see that the parameter can be interpreted as a effective number of pseudo-observations in the prior each of which has a value for the sufficient statistic ux given by noninformative priors in some applications of probabilistic inference we may have prior knowledge that can be conveniently expressed through the prior distribution. for example if the prior assigns zero probability to some value of variable then the posterior distribution will necessarily also assign zero probability to that value irrespective of probability distributions any subsequent observations of data. in many cases however we may have little idea of what form the distribution should take. we may then seek a form of prior distribution called a noninformative prior which is intended to have as little influence on the posterior distribution as possible box and tao bernardo and smith this is sometimes referred to as letting the data speak for themselves if we have a distribution px governed by a parameter we might be tempted to propose a prior distribution p const as a suitable prior. if is a discrete variable with k states this simply amounts to setting the prior probability of each state to in the case of continuous parameters however there are two potential difficulties with this approach. the first is that if the domain of is unbounded this prior distribution cannot be correctly normalized because the integral over diverges. such priors are called improper. in practice improper priors can often be used provided the corresponding posterior distribution is proper i.e. that it can be correctly normalized. for instance if we put a uniform prior distribution over the mean of a gaussian then the posterior distribution for the mean once we have observed at least one data point will be proper. a second difficulty arises from the transformation behaviour of a probability density under a nonlinear change of variables given by if a function h is constant and we change variables to h will also be constant. however if we choose the density p to be constant then the density of will be given from by d d p p p and so the density over will not be constant. this issue does not arise when we use maximum likelihood because the likelihood function px is a simple function of and so we are free to use any convenient parameterization. if however we are to choose a prior distribution that is constant we must take care to use an appropriate representation for the parameters. here we consider two simple examples of noninformative priors first of all if a density takes the form px fx then the parameter is known as a location parameter. this family of densities exhibits translation invariance because if we shift x by a constant to x c where we have c. thus the density takes the same form in the then new variable as in the original one and so the density is independent of the choice of origin. we would like to choose a prior distribution that reflects this translation invariance property and so we choose a prior that assigns equal probability mass to the exponential family an interval a b as to the shifted interval a c b c. this implies b b c b p d a a c p d a p c d and because this must hold for all choices of a and b we have p c p which implies that p is constant. an example of a location parameter would be the mean of a gaussian distribution. as we have seen the conjugate prior distribution for in this case is a gaussian p and we obtain a indeed from and noninformative prior by taking the limit we see that this gives a posterior distribution over in which the contributions from the prior vanish. n as a second example consider a density of the form px f x exercise where note that this will be a normalized density provided fx is correctly normalized. the parameter is known as a scale parameter and the density exhibits scale invariance because if we scale x by a constant to cx then where we have c this transformation corresponds to a change of f scale for example from meters to kilometers if x is a length and we would like to choose a prior distribution that reflects this scale invariance. if we consider an interval a b and a scaled interval ac bc then the prior should assign equal probability mass to these two intervals. thus we have b bc p d p d a ac c c d b p a and because this must hold for choices of a and b we have c c p p and hence p note that again this is an improper prior because the integral of the distribution over is divergent. it is sometimes also convenient to think of the prior distribution for a scale parameter in terms of the density of the log of the parameter. using the transformation rule for densities we see that pln const. thus for this prior there is the same probability mass in the range as in the range and in probability distributions an example of a scale parameter would be the standard deviation of a gaussian distribution after we have taken account of the location parameter because x as discussed earlier it is often more convenient to work in terms n exp of the precision rather than itself. using the transformation rule for densities we see that a distribution p corresponds to a distribution over of the form p we have seen that the conjugate prior for was the gamma distribution gam given by the noninformative prior is obtained as the special case again if we examine the results and for the posterior distribution of we see that for the posterior depends only on terms arising from the data and not from the prior. section nonparametric methods throughout this chapter we have focussed on the use of probability distributions having specific functional forms governed by a small number of parameters whose values are to be determined from a data set. this is called the parametric approach to density modelling. an important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data which can result in poor predictive performance. for instance if the process that generates the data is multimodal then this aspect of the distribution can never be captured by a gaussian which is necessarily unimodal. in this final section we consider some nonparametric approaches to density estimation that make few assumptions about the form of the distribution. here we shall focus mainly on simple frequentist methods. the reader should be aware however that nonparametric bayesian methods are attracting increasing interest et al. neal m uller and quintana teh et al. let us start with a discussion of histogram methods for density estimation which we have already encountered in the context of marginal and conditional distributions in figure and in the context of the central limit theorem in figure here we explore the properties of histogram density models in more detail focussing on the case of a single continuous variable x. standard histograms simply partition x into distinct bins of width i and then count the number ni of observations of x falling in bin i. in order to turn this count into a normalized probability density we simply divide by the total number n of observations and by the width i of the bins to obtain probability values for each bin given by pi ni n i px dx this gives a model for the density for which it is easily seen that px that is constant over the width of each bin and often the bins are chosen to have the same width i figure an illustration of the histogram approach to density estimation in which a data set of data points is generated from the distribution shown by the green curve. histogram density estimates based on with a common bin width are shown for various values of nonparametric methods in figure we show an example of histogram density estimation. here the data is drawn from the distribution corresponding to the green curve which is formed from a mixture of two gaussians. also shown are three examples of histogram density estimates corresponding to three different choices for the bin width we see that when is very small figure the resulting density model is very spiky with a lot of structure that is not present in the underlying distribution that generated the data set. conversely if is too large figure then the result is a model that is too smooth and that consequently fails to capture the bimodal property of the green curve. the best results are obtained for some intermediate value of figure. in principle a histogram density model is also dependent on the choice of edge location for the bins though this is typically much less significant than the value of note that the histogram method has the property the methods to be discussed shortly that once the histogram has been computed the data set itself can be discarded which can be advantageous if the data set is large. also the histogram approach is easily applied if the data points are arriving sequentially. in practice the histogram technique can be useful for obtaining a quick visualization of data in one or two dimensions but is unsuited to most density estimation applications. one obvious problem is that the estimated density has discontinuities that are due to the bin edges rather than any property of the underlying distribution that generated the data. another major limitation of the histogram approach is its scaling with dimensionality. if we divide each variable in a d-dimensional space into m bins then the total number of bins will be m d. this exponential scaling with d is an example of the curse of dimensionality. in a space of high dimensionality the quantity of data needed to provide meaningful estimates of local probability density would be prohibitive. the histogram approach to density estimation does however teach us two important lessons. first to estimate the probability density at a particular location we should consider the data points that lie within some local neighbourhood of that point. note that the concept of locality requires that we assume some form of distance measure and here we have been assuming euclidean distance. for histograms section probability distributions this neighbourhood property was defined by the bins and there is a natural smoothing parameter describing the spatial extent of the local region in this case the bin width. second the value of the smoothing parameter should be neither too large nor too small in order to obtain good results. this is reminiscent of the choice of model complexity in polynomial curve fitting discussed in chapter where the degree m of the polynomial or alternatively the value of the regularization parameter was optimal for some intermediate value neither too large nor too small. armed with these insights we turn now to a discussion of two widely used nonparametric techniques for density estimation kernel estimators and nearest neighbours which have better scaling with dimensionality than the simple histogram model. kernel density estimators let us suppose that observations are being drawn from some unknown probability density px in some d-dimensional space which we shall take to be euclidean and we wish to estimate the value of px. from our earlier discussion of locality let us consider some small region r containing x. the probability mass associated with this region is given by p r px dx. section now suppose that we have collected a data set comprising n observations drawn from px. because each data point has a probability p of falling within r the total number k of points that lie inside r will be distributed according to the binomial distribution binkn p n! k!n k! p p k. using we see that the mean fraction of points falling inside the region is ekn p and similarly using we see that the variance around this mean is varkn p p for large n this distribution will be sharply peaked around the mean and so if however we also assume that the region r is sufficiently small that the probability density px is roughly constant over the region then we have k n p. where v is the volume of r. combining and we obtain our density estimate in the form p pxv px k n v note that the validity of depends on two contradictory assumptions namely that the region r be sufficiently small that the density is approximately constant over the region and yet sufficiently large relation to the value of that density that the number k of points falling inside the region is sufficient for the binomial distribution to be sharply peaked. nonparametric methods we can exploit the result in two different ways. either we can fix k and determine the value of v from the data which gives rise to the k-nearest-neighbour technique discussed shortly or we can fix v and determine k from the data giving rise to the kernel approach. it can be shown that both the k-nearest-neighbour density estimator and the kernel density estimator converge to the true probability density in the limit n provided v shrinks suitably with n and k grows with n and hart we begin by discussing the kernel method in detail and to start with we take the region r to be a small hypercube centred on the point x at which we wish to determine the probability density. in order to count the number k of points falling within this region it is convenient to define the following function ku otherwise i d which represents a unit cube centred on the origin. the function ku is an example of a kernel function and in this context is also called a parzen window. from the quantity kx xnh will be one if the data point xn lies inside a cube of side h centred on x and zero otherwise. the total number of data points lying inside this cube will therefore be x xn k k substituting this expression into then gives the following result for the estimated density at x h px n hd k x xn h where we have used v hd for the volume of a hypercube of side h in d dimensions. using the symmetry of the function ku we can now re-interpret this equation not as a single cube centred on x but as the sum over n cubes centred on the n data points xn. as it stands the kernel density estimator will suffer from one of the same problems that the histogram method suffered from namely the presence of artificial discontinuities in this case at the boundaries of the cubes. we can obtain a smoother density model if we choose a smoother kernel function and a common choice is the gaussian which gives rise to the following kernel density model px n exp where h represents the standard deviation of the gaussian components. thus our density model is obtained by placing a gaussian over each data point and then adding up the contributions over the whole data set and then dividing by n so that the density is correctly normalized. in figure we apply the model to the data probability distributions figure illustration of the kernel density model applied to the same data set used to demonstrate the histogram approach in figure we see that h acts as a smoothing parameter and that if it is set too small panel the result is a very noisy density model whereas if it is set too large panel then the bimodal nature of the underlying distribution from which the data is generated by the green curve is washed out. the best density model is obtained for some intermediate value of h panel. h h h set used earlier to demonstrate the histogram technique. we see that as expected the parameter h plays the role of a smoothing parameter and there is a trade-off between sensitivity to noise at small h and over-smoothing at large h. again the optimization of h is a problem in model complexity analogous to the choice of bin width in histogram density estimation or the degree of the polynomial used in curve fitting. we can choose any other kernel function ku in subject to the condi tions ku ku du which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one. the class of density model given by is called a kernel density estimator or parzen estimator. it has a great merit that there is no computation involved in the training phase because this simply requires storage of the training set. however this is also one of its great weaknesses because the computational cost of evaluating the density grows linearly with the size of the data set. nearest-neighbour methods one of the difficulties with the kernel approach to density estimation is that the parameter h governing the kernel width is fixed for all kernels. in regions of high data density a large value of h may lead to over-smoothing and a washing out of structure that might otherwise be extracted from the data. however reducing h may lead to noisy estimates elsewhere in data space where the density is smaller. thus the optimal choice for h may be dependent on location within the data space. this issue is addressed by nearest-neighbour methods for density estimation. we therefore return to our general result for local density estimation and instead of fixing v and determining the value of k from the data we consider a fixed value of k and use the data to find an appropriate value for v to do this we consider a small sphere centred on the point x at which we wish to estimate the nonparametric methods figure illustration of k-nearest-neighbour density estimation using the same data set as in figures and we see that the parameter k governs the degree of smoothing so that a small value of k leads to a very noisy density model panel whereas a large value panel smoothes out the bimodal nature of the true distribution by the green curve from which the data set was generated. k k k exercise density px and we allow the radius of the sphere to grow until it contains precisely k data points. the estimate of the density px is then given by with v set to the volume of the resulting sphere. this technique is known as k nearest neighbours and is illustrated in figure for various choices of the parameter k using the same data set as used in figure and figure we see that the value of k now governs the degree of smoothing and that again there is an optimum choice for k that is neither too large nor too small. note that the model produced by k nearest neighbours is not a true density model because the integral over all space diverges. we close this chapter by showing how the k-nearest-neighbour technique for density estimation can be extended to the problem of classification. to do this we apply the k-nearest-neighbour density estimation technique to each class separately and then make use of bayes theorem. let us suppose that we have a data set comprising nk points in class ck with n points in total so that k nk n. if we wish to classify a new point x we draw a sphere centred on x containing precisely k points irrespective of their class. suppose this sphere has volume v and contains kk points from class ck. then provides an estimate of the density associated with each class pxck kk nkv similarly the unconditional density is given by px k n v while the class priors are given by pck nk n we can now combine and using bayes theorem to obtain the posterior probability of class membership pckx pxckpck px kk k probability distributions figure in the k-nearestneighbour classifier a new point shown by the black diamond is classified according to the majority class membership of the k closest training data points in this case k in the nearest-neighbour approach to classification the resulting decision boundary is composed of hyperplanes that form perpendicular bisectors of pairs of points from different classes. if we wish to minimize the probability of misclassification this is done by assigning the test point x to the class having the largest posterior probability corresponding to the largest value of kkk. thus to classify a new point we identify the k nearest points from the training data set and then assign the new point to the class having the largest number of representatives amongst this set. ties can be broken at random. the particular case of k is called the nearest-neighbour rule because a test point is simply assigned to the same class as the nearest point from the training set. these concepts are illustrated in figure in figure we show the results of applying the k-nearest-neighbour algorithm to the oil flow data introduced in chapter for various values of k. as expected we see that k controls the degree of smoothing so that small k produces many small regions of each class whereas large k leads to fewer larger regions. k k k figure plot of data points from the oil data set showing values of plotted against where the red green and blue points correspond to the laminar annular and homogeneous classes respectively. also shown are the classifications of the input space given by the k-nearest-neighbour algorithm for various values of k. exercises an interesting property of the nearest-neighbour classifier is that in the limit n the error rate is never more than twice the minimum achievable error rate of an optimal classifier i.e. one that uses the true class distributions and hart as discussed so far both the k-nearest-neighbour method and the kernel density estimator require the entire training data set to be stored leading to expensive computation if the data set is large. this effect can be offset at the expense of some additional one-off computation by constructing tree-based search structures to allow near neighbours to be found efficiently without doing an exhaustive search of the data set. nevertheless these nonparametric methods are still severely limited. on the other hand we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent. we therefore need to find density models that are very flexible and yet for which the complexity of the models can be controlled independently of the size of the training set and we shall see in subsequent chapters how to achieve this. exercises erties www verify that the bernoulli distribution satisfies the following prop px ex varx show that the entropy hx of a bernoulli distributed random binary variable x is given by hx ln the form of the bernoulli distribution given by is not symmetric between the two values of x. in some situations it will be more convenient to use an equivalent formulation for which x in which case the distribution can be written where show that the distribution is normalized and evaluate its mean variance and entropy. px www in this exercise we prove that the binomial distribution is normalized. first use the definition of the number of combinations of m identical objects chosen from a total of n to show that n m n m n m probability distributions use this result to prove by induction the following result n m xn xm which is known as the binomial theorem and which is valid for all real values of x. finally show that the binomial distribution is normalized so that m n m which can be done by first pulling out a factor out of the summation and then making use of the binomial theorem. show that the mean of the binomial distribution is given by to do this differentiate both sides of the normalization condition with respect to and then rearrange to obtain an expression for the mean of n. similarly by differentiating twice with respect to and making use of the result for the mean of the binomial distribution prove the result for the variance of the binomial. www in this exercise we prove that the beta distribution given by is correctly normalized so that holds. this is equivalent to showing that a d b from the definition of the gamma function we have exp xxa dx exp yyb dy. use this expression to prove as follows. first bring the integral over y inside the integrand of the integral over x next make the change of variable t y x where x is fixed then interchange the order of the x and t integrations and finally make the change of variable x t where t is fixed. make use of the result to show that the mean variance and mode of the beta distribution are given respectively by e var mode a a b ab b a a b exercises consider a binomial random variable x given by with prior distribution for given by the beta distribution and suppose we have observed m occurrences of x and l occurrences of x show that the posterior mean value of x lies between the prior mean and the maximum likelihood estimate for to do this show that the posterior mean can be written as times the prior mean plus times the maximum likelihood estimate where this illustrates the concept of the posterior distribution being a compromise between the prior distribution and the maximum likelihood solution. consider two variables x and y with joint distribution px y. prove the follow ing two results ex ey varx ey vary here exxy denotes the expectation of x under the conditional distribution pxy with a similar notation for the conditional variance. www in this exercise we prove the normalization of the dirichlet distribution using induction. we have already shown in exercise that the beta distribution which is a special case of the dirichlet for m is normalized. we now assume that the dirichlet distribution is normalized for m variables and prove that it is normalized for m variables. to do this consider the dirichlet k by distribution over m variables and take account of the constraint eliminating m so that the dirichlet is written pm m cm j and our goal is to find an expression for cm to do this integrate over m taking care over the limits of integration and then make a change of variable so that this integral has limits and by assuming the correct result for cm and making use of derive the expression for cm using the property x of the gamma function derive the following results for the mean variance and covariance of the dirichlet distribution given by e j j var j j j cov j l j l j l where is defined by m m k k m probability distributions www by expressing the expectation of ln j under the dirichlet distribution as a derivative with respect to j show that eln j j where is given by and d da ln is the digamma function. the uniform distribution for a continuous variable x is defined by uxa b b a a x b. verify that this distribution is normalized and find expressions for its mean and variance. evaluate the kullback-leibler divergence between two gaussians px n and qx n l. www this exercise demonstrates that the multivariate distribution with maximum entropy for a given covariance is a gaussian. the entropy of a distribution px is given by hx px ln px dx. we wish to maximize hx over all distributions px subject to the constraints that px be normalized and that it have a specific mean and covariance so that px dx pxx dx pxx dx by performing a variational maximization of and using lagrange multipliers to enforce the constraints and show that the maximum likelihood distribution is given by the gaussian show that the entropy of the multivariate gaussian n is given by hx ln d where d is the dimensionality of x. exercises www consider two random variables and having gaussian distributions with means and precisions respectively. derive an expression for the differential entropy of the variable x to do this first find the distribution of x by using the relation px and completing the square in the exponent. then observe that this represents the convolution of two gaussian distributions which itself will be gaussian and finally make use of the result for the entropy of the univariate gaussian. www consider the multivariate gaussian distribution given by by as the sum of a symwriting the precision matrix covariance matrix metric and an anti-symmetric matrix show that the anti-symmetric term does not appear in the exponent of the gaussian and hence that the precision matrix may be taken to be symmetric without loss of generality. because the inverse of a symmetric matrix is also symmetric exercise it follows that the covariance matrix may also be chosen to be symmetric without loss of generality. consider a real symmetric matrix whose eigenvalue equation is given by by taking the complex conjugate of this equation and subtracting the original equation and then forming the inner product with eigenvector ui show that the eigenvalues i are real. similarly use the symmetry property of to show that two eigenvectors ui and uj will be orthogonal provided j i. finally show that without loss of generality the set of eigenvectors can be chosen to be orthonormal so that they satisfy even if some of the eigenvalues are zero. show that a real symmetric matrix having the eigenvector equation can be expressed as an expansion in the eigenvectors with coefficients given by the has a eigenvalues of the form similarly show that the inverse matrix representation of the form www a positive definite matrix can be defined as one for which the quadratic form is positive for any real value of the vector a. show that a necessary and sufficient condition for to be positive definite is that all of the eigenvalues i of defined by are positive. at a show that a real symmetric matrix of size d d has dd independent parameters. www show that the inverse of a symmetric matrix is itself symmetric. by diagonalizing the coordinate system using the eigenvector expansion show that the volume contained within the hyperellipsoid corresponding to a constant probability distributions mahalanobis distance is given by vd d where vd is the volume of the unit sphere in d dimensions and the mahalanobis distance is defined by www prove the identity by multiplying both sides by the matrix a b c d and making use of the definition in sections and we considered the conditional and marginal distributions for a multivariate gaussian. more generally we can consider a partitioning of the components of x into three groups xa xb and xc with a corresponding partitioning of the mean vector and of the covariance matrix in the form a b c aa ab ac ba bb bc ca cb cc by making use of the results of section find an expression for the conditional distribution pxaxb in which xc has been marginalized out. a very useful result from linear algebra is the woodbury matrix inversion formula given by bcd a a da by multiplying both sides by bcd prove the correctness of this result. let x and z be two independent random vectors so that px z pxpz. show that the mean of their sum y x z is given by the sum of the means of each of the variable separately. similarly show that the covariance matrix of y is given by the sum of the covariance matrices of x and z. confirm that this result agrees with that of exercise www consider a joint distribution over the variable x y z whose mean and covariance are given by and respectively. by making use of the results and show that the marginal distribution px is given similarly by making use of the results and show that the conditional distribution pyx is given by exercises using the partitioned matrix inversion formula show that the inverse of the precision matrix is given by the covariance matrix by starting from and making use of the result verify the result consider two multidimensional random vectors x and z having gaussian distributions px n x x and pz n z z respectively together with their sum y xz. use the results and to find an expression for the marginal distribution py by considering the linear-gaussian model comprising the product of the marginal distribution px and the conditional distribution pyx. www this exercise and the next provide practice at manipulating the quadratic forms that arise in linear-gaussian models as well as giving an independent check of results derived in the main text. consider a joint distribution px y defined by the marginal and conditional distributions given by and by examining the quadratic form in the exponent of the joint distribution and using the technique of completing the square discussed in section find expressions for the mean and covariance of the marginal distribution py in which the variable x has been integrated out. to do this make use of the woodbury matrix inversion formula verify that these results agree with and obtained using the results of chapter consider the same joint distribution as in exercise but now use the technique of completing the square to find expressions for the mean and covariance of the conditional distribution pxy. again verify that these agree with the corresponding expressions and www to find the maximum likelihood solution for the covariance matrix of a multivariate gaussian we need to maximize the log likelihood function with respect to noting that the covariance matrix must be symmetric and positive definite. here we proceed by ignoring these constraints and doing a straightforward maximization. using the results and from appendix c show that the covariance matrix that maximizes the log likelihood function is given by the sample covariance we note that the final result is necessarily symmetric and positive definite the sample covariance is nonsingular. use the result to prove now using the results and show that exnxm t inm where xn denotes a data point sampled from a gaussian distribution with mean and covariance and inm denotes the m element of the identity matrix. hence prove the result www using an analogous procedure to that used to obtain derive an expression for the sequential estimation of the variance of a univariate gaussian probability distributions distribution by starting with the maximum likelihood expression ml n verify that substituting the expression for a gaussian distribution into the robbinsmonro sequential estimation formula gives a result of the same form and hence obtain an expression for the corresponding coefficients an using an analogous procedure to that used to obtain derive an expression for the sequential estimation of the covariance of a multivariate gaussian distribution by starting with the maximum likelihood expression verify that substituting the expression for a gaussian distribution into the robbins-monro sequential estimation formula gives a result of the same form and hence obtain an expression for the corresponding coefficients an use the technique of completing the square for the quadratic form in the expo nent to derive the results and starting from the results and for the posterior distribution of the mean of a gaussian random variable dissect out the contributions from the first n data points and hence obtain expressions for the sequential update of n and n now derive the same results starting from the posterior distribution p xn n n n and multiplying by the likelihood function pxn n and then completing the square and normalizing to obtain the posterior distribution after n observations. www consider a d-dimensional gaussian random variable x with distribution n in which the covariance is known and for which we wish to infer the mean from a set of observations x xn. given a prior distribution p n find the corresponding posterior distribution p use the definition of the gamma function to show that the gamma dis tribution is normalized. evaluate the mean variance and mode of the gamma distribution the following distribution px q q exp px q dx is a generalization of the univariate gaussian distribution. show that this distribution is normalized so that and that it reduces to the gaussian when q consider a regression model in which the target variable is given by t yx w and is a random noise exercises variable drawn from the distribution show that the log likelihood function over w and for an observed data set of input vectors x xn and corresponding target variables t tnt is given by ln ptx w w tnq n q const where const denotes terms independent of both w and note that as a function of w this is the lq error function considered in section consider a univariate gaussian distribution n having conjugate gaussian-gamma prior given by and a data set x xn of i.i.d. observations. show that the posterior distribution is also a gaussian-gamma distribution of the same functional form as the prior and write down expressions for the parameters of this posterior distribution. verify that the wishart distribution defined by is indeed a conjugate prior for the precision matrix of a multivariate gaussian. www verify that evaluating the integral in leads to the result www show that in the limit the t-distribution becomes a gaussian. hint ignore the normalization coefficient and simply look at the dependence on x. by following analogous steps to those used to derive the univariate student s t-distribution verify the result for the multivariate form of the student s t-distribution by marginalizing over the variable in using the definition show by exchanging integration variables that the multivariate t-distribution is correctly normalized. by using the definition of the multivariate student s t-distribution as a convolution of a gaussian with a gamma distribution verify the properties and for the multivariate t-distribution defined by show that in the limit the multivariate student s t-distribution reduces to a gaussian with mean and precision www the various trigonometric identities used in the discussion of periodic variables in this chapter can be proven easily from the relation expia cos a i sin a in which i is the square root of minus one. by considering the identity expia exp ia prove the result similarly using the identity cosa b expia b probability distributions where denotes the real part prove finally by using sina b expia b where denotes the imaginary part prove the result for large m the von mises distribution becomes sharply peaked around the mode by defining and making the taylor expansion of the cosine function given by show that as m the von mises distribution tends to a gaussian. cos o using the trigonometric identity show that solution of for is given by by computing first and second derivatives of the von mises distribution and using for m show that the maximum of the distribution occurs when and that the minimum occurs when by making use of the result together with and the trigonometric identity show that the maximum likelihood solution mml for the concentration of the von mises distribution satisfies amml r where r is the radius of the mean of the observations viewed as unit vectors in the two-dimensional euclidean plane as illustrated in figure www express the beta distribution the gamma distribution and the von mises distribution as members of the exponential family and thereby identify their natural parameters. verify that the multivariate gaussian distribution can be cast in exponential family form and derive expressions for ux hx and g analogous to the result showed that the negative gradient of ln g for the exponential family is given by the expectation of ux. by taking the second derivatives of show that ln g euxuxt euxeuxt covux. by changing variables using y x show that the density will be correctly normalized provided fx is correctly normalized. www consider a histogram-like density model in which the space x is divided into fixed regions for which the density px takes the constant value hi over the ith region and that the volume of region i is denoted i. suppose we have a set of n observations of x such that ni of these observations fall in region i. using a lagrange multiplier to enforce the normalization constraint on the density derive an expression for the maximum likelihood estimator for the show that the k-nearest-neighbour density model defines an improper distribu tion whose integral over all space is divergent. linear models for regression the focus so far in this book has been on unsupervised learning including topics such as density estimation and data clustering. we turn now to a discussion of supervised learning starting with regression. the goal of regression is to predict the value of one or more continuous target variables t given the value of a d-dimensional vector x of input variables. we have already encountered an example of a regression problem when we considered polynomial curve fitting in chapter the polynomial is a specific example of a broad class of functions called linear regression models which share the property of being linear functions of the adjustable parameters and which will form the focus of this chapter. the simplest form of linear regression models are also linear functions of the input variables. however we can obtain a much more useful class of functions by taking linear combinations of a fixed set of nonlinear functions of the input variables known as basis functions. such models are linear functions of the parameters which gives them simple analytical properties and yet can be nonlinear with respect to the input variables. linear models for regression given a training data set comprising n observations where n n together with corresponding target values the goal is to predict the value of t for a new value of x. in the simplest approach this can be done by directly constructing an appropriate function yx whose values for new inputs x constitute the predictions for the corresponding values of t. more generally from a probabilistic perspective we aim to model the predictive distribution ptx because this expresses our uncertainty about the value of t for each value of x. from this conditional distribution we can make predictions of t for any new value of x in such a way as to minimize the expected value of a suitably chosen loss function. as discussed in section a common choice of loss function for real-valued variables is the squared loss for which the optimal solution is given by the conditional expectation of t. although linear models have significant limitations as practical techniques for pattern recognition particularly for problems involving input spaces of high dimensionality they have nice analytical properties and form the foundation for more sophisticated models to be discussed in later chapters. linear basis function models the simplest linear model for regression is one that involves a linear combination of the input variables yx w wdxd where x xdt. this is often simply known as linear regression. the key property of this model is that it is a linear function of the parameters wd. it is also however a linear function of the input variables xi and this imposes significant limitations on the model. we therefore extend the class of models by considering linear combinations of fixed nonlinear functions of the input variables of the form m yx w wj jx where jx are known as basis functions. by denoting the maximum value of the index j by m the total number of parameters in this model will be m. the parameter allows for any fixed offset in the data and is sometimes called a bias parameter to be confused with bias in a statistical sense. it is often convenient to define an additional dummy basis function so that m yx w wj jx wt where w wm and m in many practical applications of pattern recognition we will apply some form of fixed pre-processing linear basis function models or feature extraction to the original data variables. if the original variables comprise the vector x then the features can be expressed in terms of the basis functions jx. by using nonlinear basis functions we allow the function yx w to be a nonlinear function of the input vector x. functions of the form are called linear models however because this function is linear in w. it is this linearity in the parameters that will greatly simplify the analysis of this class of models. however it also leads to some significant limitations as we discuss in section the example of polynomial regression considered in chapter is a particular example of this model in which there is a single input variable x and the basis functions take the form of powers of x so that jx xj. one limitation of polynomial basis functions is that they are global functions of the input variable so that changes in one region of input space affect all other regions. this can be resolved by dividing the input space up into regions and fit a different polynomial in each region leading to spline functions et al. there are many other possible choices for the basis functions for example jx exp where the j govern the locations of the basis functions in input space and the parameter s governs their spatial scale. these are usually referred to as gaussian basis functions although it should be noted that they are not required to have a probabilistic interpretation and in particular the normalization coefficient is unimportant because these basis functions will be multiplied by adaptive parameters wj. another possibility is the sigmoidal basis function of the form x j s jx where is the logistic sigmoid function defined by exp a equivalently we can use the tanh function because this is related to the logistic sigmoid by tanha and so a general linear combination of logistic sigmoid functions is equivalent to a general linear combination of tanh functions. these various choices of basis function are illustrated in figure yet another possible choice of basis function is the fourier basis which leads to an expansion in sinusoidal functions. each basis function represents a specific frequency and has infinite spatial extent. by contrast basis functions that are localized to finite regions of input space necessarily comprise a spectrum of different spatial frequencies. in many signal processing applications it is of interest to consider basis functions that are localized in both space and frequency leading to a class of functions known as wavelets. these are also defined to be mutually orthogonal to simplify their application. wavelets are most applicable when the input values live linear models for regression figure examples of basis functions showing polynomials on the left gaussians of the form in the centre and sigmoidal of the form on the right. on a regular lattice such as the successive time points in a temporal sequence or the pixels in an image. useful texts on wavelets include ogden mallat and vidakovic most of the discussion in this chapter however is independent of the particular choice of basis function set and so for most of our discussion we shall not specify the particular form of the basis functions except for the purposes of numerical illustration. indeed much of our discussion will be equally applicable to the situation in which the vector of basis functions is simply the identity x. furthermore in order to keep the notation simple we shall focus on the case of a single target variable t. however in section we consider briefly the modifications needed to deal with multiple target variables. maximum likelihood and least squares in chapter we fitted polynomial functions to data sets by minimizing a sumof-squares error function. we also showed that this error function could be motivated as the maximum likelihood solution under an assumed gaussian noise model. let us return to this discussion and consider the least squares approach and its relation to maximum likelihood in more detail. as before we assume that the target variable t is given by a deterministic func tion yx w with additive gaussian noise so that t yx w where is a zero mean gaussian random variable with precision variance thus we can write ptx w n w section recall that if we assume a squared loss function then the optimal prediction for a new value of x will be given by the conditional mean of the target variable. in the case of a gaussian conditional distribution of the form the conditional mean will be simply etx tptx dt yx w. note that the gaussian noise assumption implies that the conditional distribution of t given x is unimodal which may be inappropriate for some applications. an extension to mixtures of conditional gaussian distributions which permit multimodal conditional distributions will be discussed in section now consider a data set of inputs x xn with corresponding target values tn we group the target variables into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target which would be denoted t. making the assumption that these data points are drawn independently from the distribution we obtain the following expression for the likelihood function which is a function of the adjustable parameters w and in the form ptx w n where we have used note that in supervised learning problems such as regression classification we are not seeking to model the distribution of the input variables. thus x will always appear in the set of conditioning variables and so from now on we will drop the explicit x from expressions such as ptx w in order to keep the notation uncluttered. taking the logarithm of the likelihood function and making use of the standard form for the univariate gaussian we have ln ptw lnn n ln n edw where the sum-of-squares error function is defined by edw wt having written down the likelihood function we can use maximum likelihood to determine w and consider first the maximization with respect to w. as observed already in section we see that maximization of the likelihood function under a conditional gaussian noise distribution for a linear model is equivalent to minimizing a sum-of-squares error function given by edw. the gradient of the log likelihood function takes the form ln ptw tn wt linear basis function models linear models for regression setting this gradient to zero gives solving for w we obtain tt tn wt the quantity wml which are known as the normal equations for the least squares problem. here is an n m matrix called the design matrix whose elements are given by nj jxn so that t m m m t t is known as the moore-penrose pseudo-inverse of the matrix and mitra golub and van loan it can be regarded as a generalization of the notion of matrix inverse to nonsquare matrices. indeed if is square and invertible then using the property b we see that at this point we can gain some insight into the role of the bias parameter if we make the bias parameter explicit then the error function becomes m t m wj j edw wj setting the derivative with respect to equal to zero and solving for we obtain where we have defined t n tn j n jxn. thus the bias compensates for the difference between the averages the training set of the target values and the weighted sum of the averages of the basis function values. we can also maximize the log likelihood function with respect to the noise precision parameter giving ml n wt ml linear basis function models figure geometrical interpretation of the least-squares solution in an n-dimensional space whose axes are the values of tn the least-squares regression function is obtained by finding the orthogonal projection of the data vector t onto the subspace spanned by the basis functions jx in which each basis function is viewed as a vector j of length n with elements jxn. s t y and so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function. geometry of least squares at this point it is instructive to consider the geometrical interpretation of the least-squares solution. to do this we consider an n-dimensional space whose axes are given by the tn so that t tnt is a vector in this space. each basis function jxn evaluated at the n data points can also be represented as a vector in the same space denoted by j as illustrated in figure note that j corresponds to the jth column of whereas corresponds to the nth row of if the number m of basis functions is smaller than the number n of data points then the m vectors jxn will span a linear subspace s of dimensionality m. we define y to be an n-dimensional vector whose nth element is given by yxn w where n n. because y is an arbitrary linear combination of the vectors j it can live anywhere in the m-dimensional subspace. the sum-of-squares error is then equal to a factor of to the squared euclidean distance between y and t. thus the least-squares solution for w corresponds to that choice of y that lies in subspace s and that is closest to t. intuitively from figure we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace s. this is indeed the case as can easily be verified by noting that the solution for y is given by wml and then confirming that this takes the form of an orthogonal projection. in practice a direct solution of the normal equations can lead to numerical difficulties when t is close to singular. in particular when two or more of the basis vectors j are co-linear or nearly so the resulting parameter values can have large magnitudes. such near degeneracies will not be uncommon when dealing with real data sets. the resulting numerical difficulties can be addressed using the technique of singular value decomposition or svd et al. bishop and nabney note that the addition of a regularization term ensures that the matrix is nonsingular even in the presence of degeneracies. sequential learning batch techniques such as the maximum likelihood solution which involve processing the entire training set in one go can be computationally costly for large data sets. as we have discussed in chapter if the data set is sufficiently large it may be worthwhile to use sequential algorithms also known as on-line algorithms exercise linear models for regression in which the data points are considered one at a time and the model parameters updated after each such presentation. sequential learning is also appropriate for realtime applications in which the data observations are arriving in a continuous stream and predictions must be made before all of the data points are seen. we can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent also known as sequential gradient descent as follows. if the error function comprises a sum over data points e n en then after presentation of pattern n the stochastic gradient descent algorithm updates the parameter vector w using where denotes the iteration number and is a learning rate parameter. we shall discuss the choice of value for shortly. the value of w is initialized to some starting vector for the case of the sum-of-squares error function this gives w w w n n where n this is known as least-mean-squares or the lms algorithm. the value of needs to be chosen with care to ensure that the algorithm converges and nabney w w en regularized least squares in section we introduced the idea of adding a regularization term to an error function in order to control over-fitting so that the total error function to be minimized takes the form edw ew where is the regularization coefficient that controls the relative importance of the data-dependent error edw and the regularization term ew one of the simplest forms of regularizer is given by the sum-of-squares of the weight vector elements if we also consider the sum-of-squares error function given by ew wtw. ew wt then the total error function becomes wt wtw. this particular choice of regularizer is known in the machine learning literature as weight decay because in sequential learning algorithms it encourages weight values to decay towards zero unless supported by the data. in statistics it provides an example of a parameter shrinkage method because it shrinks parameter values towards linear basis function models q q q q figure contours of the regularization term in for various values of the parameter q. zero. it has the advantage that the error function remains a quadratic function of w and so its exact minimizer can be found in closed form. specifically setting the gradient of with respect to w to zero and solving for w as before we obtain w i t this represents a simple extension of the least-squares solution a more general regularizer is sometimes used for which the regularized error tt. exercise appendix e takes the form wt where q corresponds to the quadratic regularizer figure shows contours of the regularization function for different values of q. the case of q is know as the lasso in the statistics literature it has the property that if is sufficiently large some of the coefficients wj are driven to zero leading to a sparse model in which the corresponding basis functions play no role. to see this we first note that minimizing is equivalent to minimizing the unregularized sum-of-squares error subject to the constraint for an appropriate value of the parameter where the two approaches can be related using lagrange multipliers. the origin of the sparsity can be seen from figure which shows that the minimum of the error function subject to the constraint as is increased so an increasing number of parameters are driven to zero. regularization allows complex models to be trained on data sets of limited size without severe over-fitting essentially by limiting the effective model complexity. however the problem of determining the optimal model complexity is then shifted from one of finding the appropriate number of basis functions to one of determining a suitable value of the regularization coefficient we shall return to the issue of model complexity later in this chapter. linear models for regression figure plot of the contours of the unregularized error function along with the constraint region for the quadratic regularizer q on the left and the lasso regularizer q on the right in which the optimum value for the parameter vector w is denoted by the lasso gives a sparse solution in which for the remainder of this chapter we shall focus on the quadratic regularizer both for its practical importance and its analytical tractability. multiple outputs so far we have considered the case of a single target variable t. in some applications we may wish to predict k target variables which we denote collectively by the target vector t. this could be done by introducing a different set of basis functions for each component of t leading to multiple independent regression problems. however a more interesting and more common approach is to use the same set of basis functions to model all of the components of the target vector so that yx w wt where y is a k-dimensional column vector w is an m k matrix of parameters and is an m-dimensional column vector with elements jx with as before. suppose we take the conditional distribution of the target vector to be an isotropic gaussian of the form ptx w n if we have a set of observations tn we can combine these into a matrix t of size n k such that the nth row is given by tt n. similarly we can combine the input vectors xn into a matrix x. the log likelihood function is then given by ln ptx w lnn n k ln tn wt as before we can maximize this function with respect to w giving if we examine this result for each target variable tk we have wml wk t the bias-variance decomposition t tt. ttk tk exercise where tk is an n-dimensional column vector with components tnk for n n. thus the solution to the regression problem decouples between the different target variables and we need only compute a single pseudo-inverse matrix which is shared by all of the vectors wk. the extension to general gaussian noise distributions having arbitrary covariance matrices is straightforward. again this leads to a decoupling into k independent regression problems. this result is unsurprising because the parameters w define only the mean of the gaussian noise distribution and we know from section that the maximum likelihood solution for the mean of a multivariate gaussian is independent of the covariance. from now on we shall therefore consider a single target variable t for simplicity. the bias-variance decomposition so far in our discussion of linear models for regression we have assumed that the form and number of basis functions are both fixed. as we have seen in chapter the use of maximum likelihood or equivalently least squares can lead to severe over-fitting if complex models are trained using data sets of limited size. however limiting the number of basis functions in order to avoid over-fitting has the side effect of limiting the flexibility of the model to capture interesting and important trends in the data. although the introduction of regularization terms can control over-fitting for models with many parameters this raises the question of how to determine a suitable value for the regularization coefficient seeking the solution that minimizes the regularized error function with respect to both the weight vector w and the regularization coefficient is clearly not the right approach since this leads to the unregularized solution with as we have seen in earlier chapters the phenomenon of over-fitting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a bayesian setting. in this chapter we shall consider the bayesian view of model complexity in some depth. before doing so however it is instructive to consider a frequentist viewpoint of the model complexity issue known as the biasvariance trade-off. although we shall introduce this concept in the context of linear basis function models where it is easy to illustrate the ideas using simple examples the discussion has more general applicability. in section when we discussed decision theory for regression problems we considered various loss functions each of which leads to a corresponding optimal prediction once we are given the conditional distribution ptx. a popular choice is linear models for regression the squared loss function for which the optimal prediction is given by the conditional expectation which we denote by hx and which is given by hx etx tptx dt. at this point it is worth distinguishing between the squared loss function arising from decision theory and the sum-of-squares error function that arose in the maximum likelihood estimation of model parameters. we might use more sophisticated techniques than least squares for example regularization or a fully bayesian approach to determine the conditional distribution ptx. these can all be combined with the squared loss function for the purpose of making predictions. we showed in section that the expected squared loss can be written in the form el px dx t dx dt. recall that the second term which is independent of yx arises from the intrinsic noise on the data and represents the minimum achievable value of the expected loss. the first term depends on our choice for the function yx and we will seek a solution for yx which makes this term a minimum. because it is nonnegative the smallest that we can hope to make this term is zero. if we had an unlimited supply of data unlimited computational resources we could in principle find the regression function hx to any desired degree of accuracy and this would represent the optimal choice for yx. however in practice we have a data set d containing only a finite number n of data points and consequently we do not know the regression function hx exactly. if we model the hx using a parametric function yx w governed by a parameter vector w then from a bayesian perspective the uncertainty in our model is expressed through a posterior distribution over w. a frequentist treatment however involves making a point estimate of w based on the data set d and tries instead to interpret the uncertainty of this estimate through the following thought experiment. suppose we had a large number of data sets each of size n and each drawn independently from the distribution pt x. for any given data set d we can run our learning algorithm and obtain a prediction function yxd. different data sets from the ensemble will give different functions and consequently different values of the squared loss. the performance of a particular learning algorithm is then assessed by taking the average over this ensemble of data sets. d takes the form because this quantity will be dependent on the particular data set d we take its average over the ensemble of data sets. if we add and subtract the quantity edyxd consider the integrand of the first term in which for a particular data set the bias-variance decomposition inside the braces and then expand we obtain edyxd edyxd edyxdedyxd hx. we now take the expectation of this expression with respect to d and note that the final term will vanish giving ed ed variance we see that the expected squared difference between yxd and the regression function hx can be expressed as the sum of two terms. the first term called the squared bias represents the extent to which the average prediction over all data sets differs from the desired regression function. the second term called the variance measures the extent to which the solutions for individual data sets vary around their average and hence this measures the extent to which the function yxd is sensitive to the particular choice of data set. we shall provide some intuition to support these definitions shortly when we consider a simple example. so far we have considered a single input value x. if we substitute this expansion back into we obtain the following decomposition of the expected squared loss expected loss variance noise where variance noise dx ed t dx dt px dx and the bias and variance terms now refer to integrated quantities. our goal is to minimize the expected loss which we have decomposed into the sum of a bias a variance and a constant noise term. as we shall see there is a trade-off between bias and variance with very flexible models having low bias and high variance and relatively rigid models having high bias and low variance. the model with the optimal predictive capability is the one that leads to the best balance between bias and variance. this is illustrated by considering the sinusoidal data set from chapter here we generate data sets each containing n data points independently from the sinusoidal curve hx x. the data sets are indexed by l l where l and for each data set dl we appendix a linear models for regression t t t ln t x x ln t x x ln t x x figure illustration of the dependence of bias and variance on model complexity governed by a regularization parameter using the sinusoidal data set from chapter there are l data sets each having n data points and there are gaussian basis functions in the model so that the total number of parameters is m including the bias parameter. the left column shows the result of fitting the model to the data sets for various values of ln clarity only of the fits are shown. the right column shows the corresponding average of the fits along with the sinusoidal function from which the data sets were generated the bias-variance decomposition figure plot of squared bias and variance together with their sum corresponding to the results shown in figure also shown is the average test set error for a test data set size of points. the minimum value of variance occurs around ln which is close to the value that gives the minimum error on the test data. variance variance test error ln fit a model with gaussian basis functions by minimizing the regularized error function to give a prediction function ylx as shown in figure the top row corresponds to a large value of the regularization coefficient that gives low variance the red curves in the left plot look similar but high bias the two curves in the right plot are very different. conversely on the bottom row for which is small there is large variance by the high variability between the red curves in the left plot but low bias by the good fit between the average model fit and the original sinusoidal function. note that the result of averaging many solutions for the complex model with m is a very good fit to the regression function which suggests that averaging may be a beneficial procedure. indeed a weighted averaging of multiple solutions lies at the heart of a bayesian approach although the averaging is with respect to the posterior distribution of parameters not with respect to multiple data sets. we can also examine the bias-variance trade-off quantitatively for this example. the average prediction is estimated from yx l ylx and the integrated squared bias and integrated variance are then given by l variance n n ylxn yxn where the integral over x weighted by the distribution px is approximated by a finite sum over data points drawn from that distribution. these quantities along with their sum are plotted as a function of ln in figure we see that small values of allow the model to become finely tuned to the noise on each individual linear models for regression data set leading to large variance. conversely a large value of pulls the weight parameters towards zero leading to large bias. although the bias-variance decomposition may provide some interesting insights into the model complexity issue from a frequentist perspective it is of limited practical value because the bias-variance decomposition is based on averages with respect to ensembles of data sets whereas in practice we have only the single observed data set. if we had a large number of independent training sets of a given size we would be better off combining them into a single large training set which of course would reduce the level of over-fitting for a given model complexity. given these limitations we turn in the next section to a bayesian treatment of linear basis function models which not only provides powerful insights into the issues of over-fitting but which also leads to practical techniques for addressing the question model complexity. bayesian linear regression in our discussion of maximum likelihood for setting the parameters of a linear regression model we have seen that the effective model complexity governed by the number of basis functions needs to be controlled according to the size of the data set. adding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coefficient although the choice of the number and form of the basis functions is of course still important in determining the overall behaviour of the model. this leaves the issue of deciding the appropriate model complexity for the particular problem which cannot be decided simply by maximizing the likelihood function because this always leads to excessively complex models and over-fitting. independent hold-out data can be used to determine model complexity as discussed in section but this can be both computationally expensive and wasteful of valuable data. we therefore turn to a bayesian treatment of linear regression which will avoid the over-fitting problem of maximum likelihood and which will also lead to automatic methods of determining model complexity using the training data alone. again for simplicity we will focus on the case of a single target variable t. extension to multiple target variables is straightforward and follows the discussion of section parameter distribution we begin our discussion of the bayesian treatment of linear regression by introducing a prior probability distribution over the model parameters w. for the moment we shall treat the noise precision parameter as a known constant. first note that the likelihood function ptw defined by is the exponential of a quadratic function of w. the corresponding conjugate prior is therefore given by a gaussian distribution of the form pw n having mean and covariance exercise exercise bayesian linear regression next we compute the posterior distribution which is proportional to the product of the likelihood function and the prior. due to the choice of a conjugate gaussian prior distribution the posterior will also be gaussian. we can evaluate this distribution by the usual procedure of completing the square in the exponential and then finding the normalization coefficient using the standard result for a normalized gaussian. however we have already done the necessary work in deriving the general result which allows us to write down the posterior distribution directly in the form pwt n sn mn sn t n s s s tt where note that because the posterior distribution is gaussian its mode coincides with its mean. thus the maximum posterior weight vector is simply given by wmap mn with the mean mn if we consider an infinitely broad prior of the posterior distribution reduces to the maximum likelihood value wml given by similarly if n then the posterior distribution reverts to the prior. furthermore if data points arrive sequentially then the posterior distribution at any stage acts as the prior distribution for the subsequent data point such that the new posterior distribution is again given by for the remainder of this chapter we shall consider a particular form of gaussian prior in order to simplify the treatment. specifically we consider a zero-mean isotropic gaussian governed by a single precision parameter so that pw n and the corresponding posterior distribution over w is then given by with mn sn tt n i t s the log of the posterior distribution is given by the sum of the log likelihood and the log of the prior and as a function of w takes the form ln pwt wt wtw const. maximization of this posterior distribution with respect to w is therefore equivalent to the minimization of the sum-of-squares error function with the addition of a quadratic regularization term corresponding to with we can illustrate bayesian learning in a linear basis function model as well as the sequential update of a posterior distribution using a simple example involving straight-line fitting. consider a single input variable x a single target variable t and linear models for regression a linear model of the form yx w because this has just two adaptive parameters we can plot the prior and posterior distributions directly in parameter space. we generate synthetic data from the function fx a with parameter values and by first choosing values of xn from the uniform distribution ux then evaluating fxn a and finally adding gaussian noise with standard deviation of to obtain the target values tn. our goal is to recover the values of and from such data and we will explore the dependence on the size of the data set. we assume here that the noise variance is known and hence we set the precision parameter to its true value similarly we fix the parameter to we shall shortly discuss strategies for determining and from the training data. figure shows the results of bayesian learning in this model as the size of the data set is increased and demonstrates the sequential nature of bayesian learning in which the current posterior distribution forms the prior when a new data point is observed. it is worth taking time to study this figure in detail as it illustrates several important aspects of bayesian inference. the first row of this figure corresponds to the situation before any data points are observed and shows a plot of the prior distribution in w space together with six samples of the function yx w in which the values of w are drawn from the prior. in the second row we see the situation after observing a single data point. the location t of the data point is shown by a blue circle in the right-hand column. in the left-hand column is a plot of the likelihood function ptx w for this data point as a function of w. note that the likelihood function provides a soft constraint that the line must pass close to the data point where close is determined by the noise precision for comparison the true parameter values and used to generate the data set are shown by a white cross in the plots in the left column of figure when we multiply this likelihood function by the prior from the top row and normalize we obtain the posterior distribution shown in the middle plot on the second row. samples of the regression function yx w obtained by drawing samples of w from this posterior distribution are shown in the right-hand plot. note that these sample lines all pass close to the data point. the third row of this figure shows the effect of observing a second data point again shown by a blue circle in the plot in the right-hand column. the corresponding likelihood function for this second data point alone is shown in the left plot. when we multiply this likelihood function by the posterior distribution from the second row we obtain the posterior distribution shown in the middle plot of the third row. note that this is exactly the same posterior distribution as would be obtained by combining the original prior with the likelihood function for the two data points. this posterior has now been influenced by two data points and because two points are sufficient to define a line this already gives a relatively compact posterior distribution. samples from this posterior distribution give rise to the functions shown in red in the third column and we see that these functions pass close to both of the data points. the fourth row shows the effect of observing a total of data points. the left-hand plot shows the likelihood function for the data point alone and the middle plot shows the resulting posterior distribution that has now absorbed information from all observations. note how the posterior is much sharper than in the third row. in the limit of an infinite number of data points the bayesian linear regression figure illustration of sequential bayesian learning for a simple linear model of the form yx w a detailed description of this figure is given in the text. linear models for regression posterior distribution would become a delta function centred on the true parameter values shown by the white cross. other forms of prior over the parameters can be considered. for instance we can generalize the gaussian prior to give pw q exp in which q corresponds to the gaussian distribution and only in this case is the prior conjugate to the likelihood function finding the maximum of the posterior distribution over w corresponds to minimization of the regularized error function in the case of the gaussian prior the mode of the posterior distribution was equal to the mean although this will no longer hold if q predictive distribution in practice we are not usually interested in the value of w itself but rather in making predictions of t for new values of x. this requires that we evaluate the predictive distribution defined by ptt ptw dw exercise exercise in which t is the vector of target values from the training set and we have omitted the corresponding input vectors from the right-hand side of the conditioning statements to simplify the notation. the conditional distribution ptx w of the target variable is given by and the posterior weight distribution is given by we see that involves the convolution of two gaussian distributions and so making use of the result from section we see that the predictive distribution takes the form where the variance ptx t n n of the predictive distribution is given by n n n the first term in represents the noise on the data whereas the second term reflects the uncertainty associated with the parameters w. because the noise process and the distribution of w are independent gaussians their variances are additive. note that as additional data points are observed the posterior distribution becomes n narrower. as a consequence it can be shown et al. that n in the limit n the second term in goes to zero and the variance of the predictive distribution arises solely from the additive noise governed by the parameter as an illustration of the predictive distribution for bayesian linear regression models let us return to the synthetic sinusoidal data set of section in figure t t bayesian linear regression t x x t x x figure examples of the predictive distribution for a model consisting of gaussian basis functions of the form using the synthetic sinusoidal data set of section see the text for a detailed discussion. we fit a model comprising a linear combination of gaussian basis functions to data sets of various sizes and then look at the corresponding posterior distributions. here the green curves correspond to the function x from which the data points were generated the addition of gaussian noise. data sets of size n n n and n are shown in the four plots by the blue circles. for each plot the red curve shows the mean of the corresponding gaussian predictive distribution and the red shaded region spans one standard deviation either side of the mean. note that the predictive uncertainty depends on x and is smallest in the neighbourhood of the data points. also note that the level of uncertainty decreases as more data points are observed. the plots in figure only show the point-wise predictive variance as a function of x. in order to gain insight into the covariance between the predictions at different values of x we can draw samples from the posterior distribution over w and then plot the corresponding functions yx w as shown in figure linear models for regression t t t x x t x x figure plots of the function yx w using samples from the posterior distributions over w corresponding to the plots in figure if we used localized basis functions such as gaussians then in regions away from the basis function centres the contribution from the second term in the predic thus tive variance will go to zero leaving only the noise contribution the model becomes very confident in its predictions when extrapolating outside the region occupied by the basis functions which is generally an undesirable behaviour. this problem can be avoided by adopting an alternative bayesian approach to regression known as a gaussian process. note that if both w and are treated as unknown then we can introduce a conjugate prior distribution pw that from the discussion in section will be given by a gaussian-gamma distribution et al. in this case the predictive distribution is a student s t-distribution. section exercise exercise bayesian linear regression figure the equivalent kernel kx for the gaussian basis functions in figure shown as a plot of x versus together with three slices through this matrix corresponding to three different values of x. the data set used to generate this kernel comprised values of x equally spaced over the interval equivalent kernel the posterior mean solution for the linear basis function model has an interesting interpretation that will set the stage for kernel methods including gaussian processes. if we substitute into the expression we see that the predictive mean can be written in the form chapter yx mn mt n tt where sn is defined by thus the mean of the predictive distribution at a point x is given by a linear combination of the training set target variables tn so that we can write yx mn kx xntn where the function kx is known as the smoother matrix or the equivalent kernel. regression functions such as this which make predictions by taking linear combinations of the training set target values are known as linear smoothers. note that the equivalent kernel depends on the input values xn from the data set because these appear in the definition of sn the equivalent kernel is illustrated for the case of gaussian basis functions in have been plotted as a function of figure in which the kernel functions kx x for three different values of x. we see that they are localized around x and so the x mean of the predictive distribution at x given by yx mn is obtained by forming a weighted combination of the target values in which data points close to x are given higher weight than points further removed from x. intuitively it seems reasonable that we should weight local evidence more strongly than distant evidence. note that this localization property holds not only for the localized gaussian basis functions but also for the nonlocal polynomial and sigmoidal basis functions as illustrated in figure linear models for regression figure examples of equivalent kernels kx for x plotted as a function of corresponding to the polynomial basis functions and to the sigmoidal basis functions shown in figure note that these are localized functions of even though the corresponding basis functions are nonlocal. further insight into the role of the equivalent kernel can be obtained by consid ering the covariance between yx and which is given by covyx cov wt where we have made use of and from the form of the equivalent kernel we see that the predictive mean at nearby points will be highly correlated whereas for more distant pairs of points the correlation will be smaller. the predictive distribution shown in figure allows us to visualize the pointwise uncertainty in the predictions governed by however by drawing samples from the posterior distribution over w and plotting the corresponding model functions yx w as in figure we are visualizing the joint uncertainty in the posterior distribution between the y values at two more x values as governed by the equivalent kernel. the formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows. instead of introducing a set of basis functions which implicitly determines an equivalent kernel we can instead define a localized kernel directly and use this to make predictions for new input vectors x given the observed training set. this leads to a practical framework for regression classification called gaussian processes which will be discussed in detail in section we have seen that the effective kernel defines the weights by which the training set target values are combined in order to make a prediction at a new value of x and it can be shown that these weights sum to one in other words kx xn exercise by noting that the summation is equivalent to considering the predictive for all values of x. this intuitively pleasing result can easily be proven informally for a set of target data in which tn for all n. provided the basis functions are linearly independent that there are more data points than basis functions and that one of the basis functions is constant to the bias parameter then it is clear that we can fit the training data exactly and hence that the predictive mean will be from which we obtain note that the kernel function can bayesian model comparison chapter be negative as well as positive so although it satisfies a summation constraint the corresponding predictions are not necessarily convex combinations of the training set target variables. finally we note that the equivalent kernel satisfies an important property shared by kernel functions in general namely that it can be expressed in the form an inner product with respect to a vector of nonlinear functions so that kx z where n bayesian model comparison in chapter we highlighted the problem of over-fitting as well as the use of crossvalidation as a technique for setting the values of regularization parameters or for choosing between alternative models. here we consider the problem of model selection from a bayesian perspective. in this section our discussion will be very general and then in section we shall see how these ideas can be applied to the determination of regularization parameters in linear regression. as we shall see the over-fitting associated with maximum likelihood can be avoided by marginalizing or integrating over the model parameters instead of making point estimates of their values. models can then be compared directly on the training data without the need for a validation set. this allows all available data to be used for training and avoids the multiple training runs for each model associated with cross-validation. it also allows multiple complexity parameters to be determined simultaneously as part of the training process. for example in chapter we shall introduce the relevance vector machine which is a bayesian model having one complexity parameter for every training data point. section the bayesian view of model comparison simply involves the use of probabilities to represent uncertainty in the choice of model along with a consistent application of the sum and product rules of probability. suppose we wish to compare a set of l models where i l. here a model refers to a probability distribution over the observed data d. in the case of the polynomial curve-fitting problem the distribution is defined over the set of target values t while the set of input values x is assumed to be known. other types of model define a joint distributions over x and t. we shall suppose that the data is generated from one of these models but we are uncertain which one. our uncertainty is expressed through a prior probability distribution pmi. given a training set d we then wish to evaluate the posterior distribution the prior allows us to express a preference for different models. let us simply assume that all models are given equal prior probability. the interesting term is the model evidence pdmi which expresses the preference shown by the data for pmid pmipdmi. linear models for regression different models and we shall examine this term in more detail shortly. the model evidence is sometimes also called the marginal likelihood because it can be viewed as a likelihood function over the space of models in which the parameters have been marginalized out. the ratio of model evidences pdmipdmj for two models is known as a bayes factor and raftery once we know the posterior distribution over models the predictive distribution is given from the sum and product rules by ptxd ptxmidpmid. this is an example of a mixture distribution in which the overall predictive distribution is obtained by averaging the predictive distributions ptxmid of individual models weighted by the posterior probabilities pmid of those models. for instance if we have two models that are a-posteriori equally likely and one predicts a narrow distribution around t a while the other predicts a narrow distribution around t b the overall predictive distribution will be a bimodal distribution with modes at t a and t b not a single model at t a simple approximation to model averaging is to use the single most probable model alone to make predictions. this is known as model selection. for a model governed by a set of parameters w the model evidence is given from the sum and product rules of probability by pdmi pdwmipwmi dw. chapter from a sampling perspective the marginal likelihood can be viewed as the probability of generating the data set d from a model whose parameters are sampled at random from the prior. it is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in bayes theorem when evaluating the posterior distribution over parameters because pwdmi pdwmipwmi pdmi we can obtain some insight into the model evidence by making a simple approximation to the integral over parameters. consider first the case of a model having a single parameter w. the posterior distribution over parameters is proportional to pdwpw where we omit the dependence on the model mi to keep the notation uncluttered. if we assume that the posterior distribution is sharply peaked around the most probable value wmap with width wposterior then we can approximate the integral by the value of the integrand at its maximum times the width of the peak. if we further assume that the prior is flat with width wprior so that pw wprior then we have pd pdwpw dw pdwmap wposterior wprior bayesian model comparison figure we can obtain a rough approximation to the model evidence if we assume that the posterior distribution over parameters is sharply peaked around its mode wmap. wposterior and so taking logs we obtain ln pd ln pdwmap ln wmap wprior w wposterior wprior this approximation is illustrated in figure the first term represents the fit to the data given by the most probable parameter values and for a flat prior this would correspond to the log likelihood. the second term penalizes the model according to its complexity. because wposterior wprior this term is negative and it increases in magnitude as the ratio wposterior wprior gets smaller. thus if parameters are finely tuned to the data in the posterior distribution then the penalty term is large. for a model having a set of m parameters we can make a similar approximation for each parameter in turn. assuming that all parameters have the same ratio of wposterior wprior we obtain ln pd ln pdwmap m ln wposterior wprior section thus in this very simple approximation the size of the complexity penalty increases linearly with the number m of adaptive parameters in the model. as we increase the complexity of the model the first term will typically decrease because a more complex model is better able to fit the data whereas the second term will increase due to the dependence on m. the optimal model complexity as determined by the maximum evidence will be given by a trade-off between these two competing terms. we shall later develop a more refined version of this approximation based on a gaussian approximation to the posterior distribution. we can gain further insight into bayesian model comparison and understand how the marginal likelihood can favour models of intermediate complexity by considering figure here the horizontal axis is a one-dimensional representation of the space of possible data sets so that each point on this axis corresponds to a specific data set. we now consider three models and of successively increasing complexity. imagine running these models generatively to produce example data sets and then looking at the distribution of data sets that result. any given linear models for regression pd figure schematic illustration of the distribution of data sets for three models of different comin which is the plexity simplest and is the most complex. note that the distributions are normalized. in for the particthis example ular observed data set the model with intermediate complexity has the largest evidence. d model can generate a variety of different data sets since the parameters are governed by a prior probability distribution and for any choice of the parameters there may be random noise on the target variables. to generate a particular data set from a specific model we first choose the values of the parameters from their prior distribution pw and then for these parameter values we sample the data from pdw. a simple model example based on a first order polynomial has little variability and so will generate data sets that are fairly similar to each other. its distribution pd is therefore confined to a relatively small region of the horizontal axis. by contrast a complex model as a ninth order polynomial can generate a great variety of different data sets and so its distribution pd is spread over a large region of the space of data sets. because the distributions pdmi are normalized we see that the particular data set can have the highest value of the evidence for the model of intermediate complexity. essentially the simpler model cannot fit the data well whereas the more complex model spreads its predictive probability over too broad a range of data sets and so assigns relatively small probability to any one of them. implicit in the bayesian model comparison framework is the assumption that the true distribution from which the data are generated is contained within the set of models under consideration. provided this is so we can show that bayesian model comparison will on average favour the correct model. to see this consider two models and in which the truth corresponds to for a given finite data set it is possible for the bayes factor to be larger for the incorrect model. however if bayes factor in the form we average the bayes factor over the distribution of data sets we obtain the expected ln dd section where the average has been taken with respect to the true distribution of the data. this quantity is an example of the kullback-leibler divergence and satisfies the property of always being positive unless the two distributions are equal in which case it is zero. thus on average the bayes factor will always favour the correct model. we have seen that the bayesian framework avoids the problem of over-fitting and allows models to be compared on the basis of the training data alone. however the evidence approximation a bayesian approach like any approach to pattern recognition needs to make assumptions about the form of the model and if these are invalid then the results can be misleading. in particular we see from figure that the model evidence can be sensitive to many aspects of the prior such as the behaviour in the tails. indeed the evidence is not defined if the prior is improper as can be seen by noting that an improper prior has an arbitrary scaling factor other words the normalization coefficient is not defined because the distribution cannot be normalized. if we consider a proper prior and then take a suitable limit in order to obtain an improper prior example a gaussian prior in which we take the limit of infinite variance then the evidence will go to zero as can be seen from and figure it may however be possible to consider the evidence ratio between two models first and then take a limit to obtain a meaningful answer. in a practical application therefore it will be wise to keep aside an independent test set of data on which to evaluate the overall performance of the final system. the evidence approximation in a fully bayesian treatment of the linear basis function model we would introduce prior distributions over the hyperparameters and and make predictions by marginalizing with respect to these hyperparameters as well as with respect to the parameters w. however although we can integrate analytically over either w or over the hyperparameters the complete marginalization over all of these variables is analytically intractable. here we discuss an approximation in which we set the hyperparameters to specific values determined by maximizing the marginal likelihood function obtained by first integrating over the parameters w. this framework is known in the statistics literature as empirical bayes and smith gelman et al. or type maximum likelihood or generalized maximum likelihood and in the machine learning literature is also called the evidence approximation mackay if we introduce hyperpriors over and the predictive distribution is obtained by marginalizing over w and so that ptt ptw dw d d where ptw is given by and pwt is given by with mn and sn defined by and respectively. here we have omitted the dependence on the input variable x to keep the notation uncluttered. if the posterior distribution p is sharply peaked around then the predictive distribution is obtained simply by marginalizing over w in which and are fixed to the so that ptt dw. linear models for regression from bayes theorem the posterior distribution for and is given by if the prior is relatively flat then in the evidence framework the values of and are obtained by maximizing the marginal likelihood function pt we shall p pt proceed by evaluating the marginal likelihood for the linear basis function model and then finding its maxima. this will allow us to determine values for these hyperparameters from the training data alone without recourse to cross-validation. recall that the ratio is analogous to a regularization parameter. as an aside it is worth noting that if we define conjugate prior distributions over and then the marginalization over these hyperparameters in can be performed analytically to give a student s t-distribution over w section although the resulting integral over w is no longer analytically tractable it might be thought that approximating this integral for example using the laplace approximation discussed which is based on a local gaussian approximation centred on the mode of the posterior distribution might provide a practical alternative to the evidence framework and weigend however the integrand as a function of w typically has a strongly skewed mode so that the laplace approximation fails to capture the bulk of the probability mass leading to poorer results than those obtained by maximizing the evidence returning to the evidence framework we note that there are two approaches that we can take to the maximization of the log evidence. we can evaluate the evidence function analytically and then set its derivative equal to zero to obtain re-estimation equations for and which we shall do in section alternatively we use a technique called the expectation maximization algorithm which will be discussed in section where we shall also show that these two approaches converge to the same solution. evaluation of the evidence function the marginal likelihood function pt is obtained by integrating over the weight parameters w so that pt ptw dw. exercise exercise one way to evaluate this integral is to make use once again of the result for the conditional distribution in a linear-gaussian model. here we shall evaluate the integral instead by completing the square in the exponent and making use of the standard form for the normalization coefficient of a gaussian. from and we can write the evidence function in the form pt exp ew dw exercise exercise the evidence approximation where m is the dimensionality of w and we have defined ew edw ew wtw. we recognize as being equal up to a constant of proportionality to the regularized sum-of-squares error function we now complete the square over w giving ew emn mn mn where we have introduced a i t together with emn mt n mn note that a corresponds to the matrix of second derivatives of the error function a ew and is known as the hessian matrix. here we have also defined mn given by mn a tt. n and hence is equivalent to the previous using we see that a s definition and therefore represents the mean of the posterior distribution. the integral over w can now be evaluated simply by appealing to the standard result for the normalization coefficient of a multivariate gaussian giving exp ew dw exp emn exp emn exp mntaw mn dw using we can then write the log of the marginal likelihood in the form ln pt m ln n ln emn lna n which is the required expression for the evidence function. returning to the polynomial regression problem we can plot the model evidence against the order of the polynomial as shown in figure here we have assumed a prior of the form with the parameter fixed at the form of this plot is very instructive. referring back to figure we see that the m polynomial has very poor fit to the data and consequently gives a relatively low value linear models for regression figure plot of the model evidence versus the order m for the polynomial regression model showing that the evidence favours the model with m m for the evidence. going to the m polynomial greatly improves the data fit and hence the evidence is significantly higher. however in going to m the data fit is improved only very marginally due to the fact that the underlying sinusoidal function from which the data is generated is an odd function and so has no even terms in a polynomial expansion. indeed figure shows that the residual data error is reduced only slightly in going from m to m because this richer model suffers a greater complexity penalty the evidence actually falls in going from m to m when we go to m we obtain a significant further improvement in data fit as seen in figure and so the evidence is increased again giving the highest overall evidence for any of the polynomials. further increases in the value of m produce only small improvements in the fit to the data but suffer increasing complexity penalty leading overall to a decrease in the evidence values. looking again at figure we see that the generalization error is roughly constant between m and m and it would be difficult to choose between these models on the basis of this plot alone. the evidence values however show a clear preference for m since this is the simplest model which gives a good explanation for the observed data. maximizing the evidence function let us first consider the maximization of pt with respect to this can be done by first defining the following eigenvector equation from it then follows that a has eigenvalues i. now consider the derivative of the term involving lna in with respect to we have t ui iui. d d lna d d ln i d d i i ln i i i thus the stationary points of with respect to satisfy m n mn mt i i multiplying through by and rearranging we obtain the evidence approximation i i mt n mn m i i i since there are m terms in the sum over i the quantity can be written exercise the interpretation of the quantity will be discussed shortly. from we see that the value of that maximizes the marginal likelihood satisfies n mn mt note that this is an implicit solution for not only because depends on but also because the mode mn of the posterior distribution itself depends on the choice of we therefore adopt an iterative procedure in which we make an initial choice for and use this to find mn which is given by and also to evaluate which is given by these values are then used to re-estimate using and the process repeated until convergence. note that because the matrix t is fixed we can compute its eigenvalues once at the start and then simply multiply these by to obtain the i. it should be emphasized that the value of has been determined purely by looking at the training data. in contrast to maximum likelihood methods no independent data set is required in order to optimize the model complexity. we can similarly maximize the log marginal likelihood with respect to to do this we note that the eigenvalues i defined by are proportional to and hence d id i giving d d lna d d ln i the stationary point of the marginal likelihood therefore satisfies n exercise and rearranging we obtain tn mt n n tn mt n i i i i again this is an implicit solution for and can be solved by choosing an initial value for and then using this to calculate mn and and then re-estimate using repeating until convergence. if both and are to be determined from the data then their values can be re-estimated together after each update of linear models for regression figure contours of the likelihood function and the prior in which the axes in parameter space have been rotated to align with the eigenvectors ui of the hessian. for the mode of the posterior is given by the maximum likelihood solution wml whereas for nonzero the mode is at wmap mn in the direction the eigenvalue defined by is small compared with and so the quantity is close to zero and the corresponding map value of is also close to zero. by contrast in the direction the eigenvalue is large compared with and so the quantity is close to unity and the map value of is close to its maximum likelihood value. wml wmap effective number of parameters the result has an elegant interpretation which provides insight into the bayesian solution for to see this consider the contours of the likelihood function and the prior as illustrated in figure here we have implicitly transformed to a rotated set of axes in parameter space aligned with the eigenvectors ui defined in contours of the likelihood function are then axis-aligned ellipses. the eigenvalues i measure the curvature of the likelihood function and so in figure the eigenvalue is small compared with a smaller curvature corresponds to a greater elongation of the contours of the likelihood function. because t is a positive definite matrix it will have positive eigenvalues and so the ratio i i will lie between and consequently the quantity defined by will lie in the range m. for directions in which i the corresponding parameter wi will be close to its maximum likelihood value and the ratio i i will be close to such parameters are called well determined because their values are tightly constrained by the data. conversely for directions in which i the corresponding parameters wi will be close to zero as will the ratios i i these are directions in which the likelihood function is relatively insensitive to the parameter value and so the parameter has been set to a small value by the prior. the quantity defined by therefore measures the effective total number of well determined parameters. we can obtain some insight into the result for re-estimating by comparing it with the corresponding maximum likelihood result given by both of these formulae express the variance inverse precision as an average of the squared differences between the targets and the model predictions. however they differ in that the number of data points n in the denominator of the maximum likelihood result is replaced by n in the bayesian result. we recall from that the maximum likelihood estimate of the variance for a gaussian distribution over a the evidence approximation single variable x is given by ml n and that this estimate is biased because the maximum likelihood solution ml for the mean has fitted some of the noise on the data. in effect this has used up one degree of freedom in the model. the corresponding unbiased estimate is given by and takes the form map n we shall see in section that this result can be obtained from a bayesian treatment in which we marginalize over the unknown mean. the factor of n in the denominator of the bayesian result takes account of the fact that one degree of freedom has been used in fitting the mean and removes the bias of maximum likelihood. now consider the corresponding results for the linear regression model. the mean of the target distribution is now given by the function wt which contains m parameters. however not all of these parameters are tuned to the data. the effective number of parameters that are determined by the data is with the remaining m parameters set to small values by the prior. this is reflected in the bayesian result for the variance that has a factor n in the denominator thereby correcting for the bias of the maximum likelihood result. we can illustrate the evidence framework for setting hyperparameters using the sinusoidal synthetic data set from section together with the gaussian basis function model comprising basis functions so that the total number of parameters in the model is given by m including the bias. here for simplicity of illustration we have set to its true value of and then used the evidence framework to determine as shown in figure we can also see how the parameter controls the magnitude of the parameters by plotting the individual parameters versus the effective number of parameters as shown in figure if we consider the limit n m in which the number of data points is large in relation to the number of parameters then from all of the parameters will be well determined by the data because t involves an implicit sum over data points and so the eigenvalues i increase with the size of the data set. in this case m and the re-estimation equations for and become m n where ew and ed are defined by and respectively. these results can be used as an easy-to-compute approximation to the full evidence re-estimation linear models for regression ln ln figure the left plot shows curve and ew curve versus ln for the sinusoidal synthetic data set. it is the intersection of these two curves that defines the optimum value for given by the evidence procedure. the right plot shows the corresponding graph of log evidence ln pt versus ln curve showing that the peak coincides with the crossing point of the curves in the left plot. also shown is the test set error curve showing that the evidence maximum occurs close to the point of best generalization. formulae because they do not require evaluation of the eigenvalue spectrum of the hessian. figure plot of the parameters wi from the gaussian basis function model versus the effective number of parameters in which the hyperparameter is varied in the range causing to vary in the range m. wi limitations of fixed basis functions throughout this chapter we have focussed on models comprising a linear combination of fixed nonlinear basis functions. we have seen that the assumption of linearity in the parameters led to a range of useful properties including closed-form solutions to the least-squares problem as well as a tractable bayesian treatment. furthermore for a suitable choice of basis functions we can model arbitrary nonlinearities in the exercises mapping from input variables to targets. in the next chapter we shall study an analogous class of models for classification. it might appear therefore that such linear models constitute a general purpose framework for solving problems in pattern recognition. unfortunately there are some significant shortcomings with linear models which will cause us to turn in later chapters to more complex models such as support vector machines and neural networks. the difficulty stems from the assumption that the basis functions jx are fixed before the training data set is observed and is a manifestation of the curse of dimensionality discussed in section as a consequence the number of basis functions needs to grow rapidly often exponentially with the dimensionality d of the input space. fortunately there are two properties of real data sets that we can exploit to help alleviate this problem. first of all the data vectors typically lie close to a nonlinear manifold whose intrinsic dimensionality is smaller than that of the input space as a result of strong correlations between the input variables. we will see an example of this when we consider images of handwritten digits in chapter if we are using localized basis functions we can arrange that they are scattered in input space only in regions containing data. this approach is used in radial basis function networks and also in support vector and relevance vector machines. neural network models which use adaptive basis functions having sigmoidal nonlinearities can adapt the parameters so that the regions of input space over which the basis functions vary corresponds to the data manifold. the second property is that target variables may have significant dependence on only a small number of possible directions within the data manifold. neural networks can exploit this property by choosing the directions in input space to which the basis functions respond. exercises www show that the tanh function and the logistic sigmoid function are related by tanha hence show that a general linear combination of logistic sigmoid functions of the form x j s x j s is equivalent to a linear combination of tanh functions of the form yx w wj yx u uj tanh and find expressions to relate the new parameters um to the original parameters wm. linear models for regression show that the matrix t t takes any vector v and projects it onto the space spanned by the columns of use this result to show that the least-squares solution corresponds to an orthogonal projection of the vector t onto the manifold s as shown in figure consider a data set in which each data point tn is associated with a weighting factor rn so that the sum-of-squares error function becomes rn edw tn wt find an expression for the solution that minimizes this error function. give two alternative interpretations of the weighted sum-of-squares error function in terms of data dependent noise variance and replicated data points. www consider a linear model of the form yx w wixi together with a sum-of-squares error function of the form edw w now suppose that gaussian noise with zero mean and variance is added independently to each of the input variables xi. by making use of ei and eij ij show that minimizing ed averaged over the noise distribution is equivalent to minimizing the sum-of-squares error for noise-free input variables with the addition of a weight-decay regularization term in which the bias parameter is omitted from the regularizer. www using the technique of lagrange multipliers discussed in appendix e show that minimization of the regularized error function is equivalent to minimizing the unregularized sum-of-squares error subject to the constraint discuss the relationship between the parameters and www consider a linear basis function regression model for a multivariate target variable t having a gaussian distribution of the form ptw n w where yx w wt exercises together with a training data set comprising input basis vectors and corresponding target vectors tn with n n. show that the maximum likelihood solution wml for the parameter matrix w has the property that each column is given by an expression of the form which was the solution for an isotropic noise distribution. note that this is independent of the covariance matrix show that the maximum likelihood solution for is given by n tn wt ml tn wt ml by using the technique of completing the square verify the result for the posterior distribution of the parameters w in the linear basis function model in which mn and sn are defined by and respectively. www consider the linear basis function model in section and suppose that we have already observed n data points so that the posterior distribution over w is given by this posterior can be regarded as the prior for the next observation. by considering an additional data point tn and by completing the square in the exponential show that the resulting posterior distribution is again given by but with sn replaced by sn and mn replaced by mn repeat the previous exercise but instead of completing the square by hand make use of the general result for linear-gaussian models given by www by making use of the result to evaluate the integral in verify that the predictive distribution for the bayesian linear regression model is given by in which the input-dependent variance is given by we have seen that as the size of a data set increases the uncertainty associated with the posterior distribution over model parameters decreases. make use of the matrix identity c m vvt m vtm vtm to show that the uncertainty given by satisfies n associated with the linear regression function n n we saw in section that the conjugate prior for a gaussian distribution with unknown mean and unknown precision variance is a normal-gamma distribution. this property also holds for the case of the conditional gaussian distribution ptx w of the linear regression model. if we consider the likelihood function then the conjugate prior for w and is given by pw n linear models for regression show that the corresponding posterior distribution takes the same functional form so that pw n bn and find expressions for the posterior parameters mn sn an and bn show that the predictive distribution ptx t for the model discussed in ex ercise is given by a student s t-distribution of the form ptx t stt and obtain expressions for and in this exercise we explore in more detail the properties of the equivalent kernel defined by where sn is defined by suppose that the basis functions jx are linearly independent and that the number n of data points is greater than the number m of basis functions. furthermore let one of the basis functions be constant say by taking suitable linear combinations of these basis functions we can construct a new basis set jx spanning the same space but that are orthonormal so that jxn kxn ijk where ijk is defined to be if j k and otherwise and we take show that for the equivalent kernel can be written as kx where m use this result to show that the kernel satisfies the summation constraint kx xn www consider a linear basis function model for regression in which the parameters and are set using the evidence framework. show that the function emn defined by satisfies the relation n. derive the result for the log evidence function pt of the linear regression model by making use of to evaluate the integral directly. show that the evidence function for the bayesian linear regression model can be written in the form in which ew is defined by www by completing the square over w show that the error function in bayesian linear regression can be written in the form show that the integration over w in the bayesian linear regression model gives the result hence show that the log marginal likelihood is given by www starting from verify all of the steps needed to show that maximization of the log marginal likelihood function with respect to leads to the re-estimation equation exercises an alternative way to derive the result for the optimal value of in the evidence framework is to make use of the identity lna tr d d a d d a prove this identity by considering the eigenvalue expansion of a real symmetric matrix a and making use of the standard results for the determinant and trace of a expressed in terms of its eigenvalues c. then make use of to derive starting from starting from verify all of the steps needed to show that maximization of the log marginal likelihood function with respect to leads to the re-estimation equation www show that the marginal probability of the data in other words the model evidence for the model described in exercise is given by pt ban n by first marginalizing with respect to w and then with respect to repeat the previous exercise but now use bayes theorem in the form pt ptw pw and then substitute for the prior and posterior distributions and the likelihood function in order to derive the result linear models for classification in the previous chapter we explored a class of regression models having particularly simple analytical and computational properties. we now discuss an analogous class of models for solving classification problems. the goal in classification is to take an input vector x and to assign it to one of k discrete classes ck where k k. in the most common scenario the classes are taken to be disjoint so that each input is assigned to one and only one class. the input space is thereby divided into decision regions whose boundaries are called decision boundaries or decision surfaces. in this chapter we consider linear models for classification by which we mean that the decision surfaces are linear functions of the input vector x and hence are defined by hyperplanes within the d-dimensional input space. data sets whose classes can be separated exactly by linear decision surfaces are said to be linearly separable. for regression problems the target variable t was simply the vector of real numbers whose values we wish to predict. in the case of classification there are various linear models for classification ways of using target values to represent class labels. for probabilistic models the most convenient in the case of two-class problems is the binary representation in which there is a single target variable t such that t represents class and t represents class we can interpret the value of t as the probability that the class is with the values of probability taking only the extreme values of and for k classes it is convenient to use a coding scheme in which t is a vector of length k such that if the class is cj then all elements tk of t are zero except element tj which takes the value for instance if we have k classes then a pattern from class would be given the target vector t again we can interpret the value of tk as the probability that the class is ck. for nonprobabilistic models alternative choices of target variable representation will sometimes prove convenient. in chapter we identified three distinct approaches to the classification problem. the simplest involves constructing a discriminant function that directly assigns each vector x to a specific class. a more powerful approach however models the conditional probability distribution pckx in an inference stage and then subsequently uses this distribution to make optimal decisions. by separating inference and decision we gain numerous benefits as discussed in section there are two different approaches to determining the conditional probabilities pckx. one technique is to model them directly for example by representing them as parametric models and then optimizing the parameters using a training set. alternatively we can adopt a generative approach in which we model the class-conditional densities given by pxck together with the prior probabilities pck for the classes and then we compute the required posterior probabilities using bayes theorem pckx pxckpck px we shall discuss examples of all three approaches in this chapter. in the linear regression models considered in chapter the model prediction yx w was given by a linear function of the parameters w. in the simplest case the model is also linear in the input variables and therefore takes the form yx wtx so that y is a real number. for classification problems however we wish to predict discrete class labels or more generally posterior probabilities that lie in the range to achieve this we consider a generalization of this model in which we transform the linear function of w using a nonlinear function f so that yx f wtx in the machine learning literature f is known as an activation function whereas its inverse is called a link function in the statistics literature. the decision surfaces correspond to yx constant so that wtx constant and hence the decision surfaces are linear functions of x even if the function f is nonlinear. for this reason the class of models described by are called generalized linear models discriminant functions and nelder note however that in contrast to the models used for regression they are no longer linear in the parameters due to the presence of the nonlinear function f this will lead to more complex analytical and computational properties than for linear regression models. nevertheless these models are still relatively simple compared to the more general nonlinear models that will be studied in subsequent chapters. the algorithms discussed in this chapter will be equally applicable if we first make a fixed nonlinear transformation of the input variables using a vector of basis functions as we did for regression models in chapter we begin by considering classification directly in the original input space x while in section we shall find it convenient to switch to a notation involving basis functions for consistency with later chapters. discriminant functions a discriminant is a function that takes an input vector x and assigns it to one of k classes denoted ck. in this chapter we shall restrict attention to linear discriminants namely those for which the decision surfaces are hyperplanes. to simplify the discussion we consider first the case of two classes and then investigate the extension to k classes. two classes the simplest representation of a linear discriminant function is obtained by tak ing a linear function of the input vector so that yx wtx where w is called a weight vector and is a bias to be confused with bias in the statistical sense. the negative of the bias is sometimes called a threshold. an input vector x is assigned to class if yx and to class otherwise. the corresponding decision boundary is therefore defined by the relation yx which corresponds to a hyperplane within the d-dimensional input space. consider two points xa and xb both of which lie on the decision surface. because yxa yxb we have wtxa xb and hence the vector w is orthogonal to every vector lying within the decision surface and so w determines the orientation of the decision surface. similarly if x is a point on the decision surface then yx and so the normal distance from the origin to the decision surface is given by wtx we therefore see that the bias parameter determines the location of the decision surface. these properties are illustrated for the case of d in figure furthermore we note that the value of yx gives a signed measure of the perpendicular distance r of the point x from the decision surface. to see this consider linear models for classification figure illustration of the geometry of a linear discriminant function in two dimensions. the decision surface shown in red is perpendicular to w and its displacement from the origin is controlled by the bias parameter also the signed orthogonal distance of a general point x from the decision surface is given by y y y w x x an arbitrary point x and let x be its orthogonal projection onto the decision surface so that x x r w multiplying both sides of this result by wt and adding and making use of yx wtx and yx wtx we have r yx this result is illustrated in figure as with the linear regression models in chapter it is sometimes convenient to use a more compact notation in which we introduce an additional dummy input value and then w x so that yx in this case the decision surfaces are d-dimensional hyperplanes passing through the origin of the d expanded input space. multiple classes now consider the extension of linear discriminants to k classes. we might be tempted be to build a k-class discriminant by combining a number of two-class discriminant functions. however this leads to some serious difficulties and hart as we now show. consider the use of k classifiers each of which solves a two-class problem of separating points in a particular class ck from points not in that class. this is known as a one-versus-the-rest classifier. the left-hand example in figure shows an discriminant functions not not figure attempting to construct a k class discriminant from a set of two class discriminants leads to ambiguous regions shown in green. on the left is an example involving the use of two discriminants designed to distinguish points in class ck from points not in class ck. on the right is an example involving three discriminant functions each of which is used to separate a pair of classes ck and cj. example involving three classes where this approach leads to regions of input space that are ambiguously classified. an alternative is to introduce kk binary discriminant functions one for every possible pair of classes. this is known as a one-versus-one classifier. each point is then classified according to a majority vote amongst the discriminant functions. however this too runs into the problem of ambiguous regions as illustrated in the right-hand diagram of figure we can avoid these difficulties by considering a single k-class discriminant comprising k linear functions of the form ykx wt and then assigning a point x to class ck if ykx yjx for all j k. the decision boundary between class ck and class cj is therefore given by ykx yjx and hence corresponds to a hyperplane defined by k x wjtx this has the same form as the decision boundary for the two-class case discussed in section and so analogous geometrical properties apply. the decision regions of such a discriminant are always singly connected and convex. to see this consider two points xa and xb both of which lie inside decision region rk as illustrated in figure any that lies on the line connecting xa and xb can be expressed in the form xa linear models for classification figure illustration of the decision regions for a multiclass linear discriminant with the decision if two points xa boundaries shown in red. and xb both lie inside the same decision region rk then any point bx that lies on the line connecting these two points must also lie in rk and hence the decision region must be singly connected and convex. ri xa rj rk x xb where from the linearity of the discriminant functions it follows that ykxa ykxb yjxb for all j k and hence and also lies because both xa and xb lie inside rk it follows that ykxa yjxa and inside rk. thus rk is singly connected and convex. note that for two classes we can either employ the formalism discussed here based on two discriminant functions and or else use the simpler but equivalent formulation described in section based on a single discriminant function yx. we now explore three approaches to learning the parameters of linear discriminant functions based on least squares fisher s linear discriminant and the perceptron algorithm. least squares for classification in chapter we considered models that were linear functions of the parameters and we saw that the minimization of a sum-of-squares error function led to a simple closed-form solution for the parameter values. it is therefore tempting to see if we can apply the same formalism to classification problems. consider a general classification problem with k classes with a binary coding scheme for the target vector t. one justification for using least squares in such a context is that it approximates the conditional expectation etx of the target values given the input vector. for the binary coding scheme this conditional expectation is given by the vector of posterior class probabilities. unfortunately however these probabilities are typically approximated rather poorly indeed the approximations can have values outside the range due to the limited flexibility of a linear model as we shall see shortly. each class ck is described by its own linear model so that ykx wt k x where k k. we can conveniently group these together using vector notation so that yx can then be written as discriminant functions a dummy input this representation was discussed in detail in section a error function as we did for regression in chapter consider a training data set tn where n n and define a matrix t whose nth row is the vector tt n n. the sum-of-squares error function where is a matrix whose kth column comprises the d vector wt k is the corresponding augmented input vector xtt with new input x is then assigned to the class for which the output yk is largest. we now determine the parameter matrix by minimizing a sum-of-squares together with a matrix whose nth row t setting the derivative with respect tow to zero and rearranging we then obtain the solution forw in the formw t is the pseudo-inverse of the matrix as discussed in section we where yx tt then obtain the discriminant function in the form edw tr exercise section an interesting property of least-squares solutions with multiple target variables is that if every target vector in the training set satisfies some linear constraint attn b for some constants a and b then the model prediction for any value of x will satisfy the same constraint so that atyx b thus if we use a coding scheme for k classes then the predictions made by the model will have the property that the elements of yx will sum to for any value of x. however this summation constraint alone is not sufficient to allow the model outputs to be interpreted as probabilities because they are not constrained to lie within the interval the least-squares approach gives an exact closed-form solution for the discriminant function parameters. however even as a discriminant function we use it to make decisions directly and dispense with any probabilistic interpretation it suffers from some severe problems. we have already seen that least-squares solutions lack robustness to outliers and this applies equally to the classification application as illustrated in figure here we see that the additional data points in the righthand figure produce a significant change in the location of the decision boundary even though these point would be correctly classified by the original decision boundary in the left-hand figure. the sum-of-squares error function penalizes predictions that are too correct in that they lie a long way on the correct side of the decision linear models for classification figure the left plot shows data from two classes denoted by red crosses and blue circles together with the decision boundary found by least squares curve and also by the logistic regression model curve which is discussed later in section the right-hand plot shows the corresponding results obtained when extra data points are added at the bottom left of the diagram showing that least squares is highly sensitive to outliers unlike logistic regression. boundary. in section we shall consider several alternative error functions for classification and we shall see that they do not suffer from this difficulty. however problems with least squares can be more severe than simply lack of robustness as illustrated in figure this shows a synthetic data set drawn from three classes in a two-dimensional input space having the property that linear decision boundaries can give excellent separation between the classes. indeed the technique of logistic regression described later in this chapter gives a satisfactory solution as seen in the right-hand plot. however the least-squares solution gives poor results with only a small region of the input space assigned to the green class. the failure of least squares should not surprise us when we recall that it corresponds to maximum likelihood under the assumption of a gaussian conditional distribution whereas binary target vectors clearly have a distribution that is far from gaussian. by adopting more appropriate probabilistic models we shall obtain classification techniques with much better properties than least squares. for the moment however we continue to explore alternative nonprobabilistic methods for setting the parameters in the linear classification models. fisher s linear discriminant one way to view a linear classification model is in terms of dimensionality reduction. consider first the case of two classes and suppose we take the d discriminant functions figure example of a synthetic data set comprising three classes with training data points denoted in red green and blue lines denote the decision boundaries and the background colours denote the respective classes of the decision regions. on the left is the result of using a least-squares discriminant. we see that the region of input space assigned to the green class is too small and so most of the points from this class are misclassified. on the right is the result of using logistic regressions as described in section showing correct classification of the training data. dimensional input vector x and project it down to one dimension using y wtx. if we place a threshold on y and classify y as class and otherwise class then we obtain our standard linear classifier discussed in the previous section. in general the projection onto one dimension leads to a considerable loss of information and classes that are well separated in the original d-dimensional space may become strongly overlapping in one dimension. however by adjusting the components of the weight vector w we can select a projection that maximizes the class separation. to begin with consider a two-class problem in which there are points of class and points of class so that the mean vectors of the two classes are given by xn xn. n n the simplest measure of the separation of the classes when projected onto w is the separation of the projected class means. this suggests that we might choose w so as to maximize where mk wtmk linear models for classification figure the left plot shows samples from two classes in red and blue along with the histograms resulting from projection onto the line joining the class means. note that there is considerable class overlap in the projected space. the right plot shows the corresponding projection based on the fisher linear discriminant showing the greatly improved class separation. appendix e exercise i is the mean of the projected data from class ck. however this expression can be made arbitrarily large simply by increasing the magnitude of w. to solve this i using problem we could constrain w to have unit length so that a lagrange multiplier to perform the constrained maximization we then find that w there is still a problem with this approach however as illustrated in figure this shows two classes that are well separated in the original twodimensional space but that have considerable overlap when projected onto the line joining their means. this difficulty arises from the strongly nondiagonal covariances of the class distributions. the idea proposed by fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class thereby minimizing the class overlap. the projection formula transforms the set of labelled data points in x into a labelled set in the one-dimensional space y. the within-class variance of the transformed data from class ck is therefore given by k n ck where yn wtxn. we can define the total within-class variance for the whole data set to be simply the fisher criterion is defined to be the ratio of the between-class variance to the within-class variance and is given by jw exercise we can make the dependence on w explicit by using and to rewrite the fisher criterion in the form discriminant functions jw wtsbw wtsww where sb is the between-class covariance matrix and is given by sb and sw is the total within-class covariance matrix given by sw n n differentiating with respect to w we find that jw is maximized when from we see that sbw is always in the direction of furthermore we do not care about the magnitude of w only its direction and so we can drop the scalar factors and multiplying both sides of by s w we then obtain note that if the within-class covariance is isotropic so that sw is proportional to the unit matrix we find that w is proportional to the difference of the class means as discussed above. w s w the result is known as fisher s linear discriminant although strictly it is not a discriminant but rather a specific choice of direction for projection of the data down to one dimension. however the projected data can subsequently be used to construct a discriminant by choosing a threshold so that we classify a new point as belonging to if yx and classify it as belonging to otherwise. for example we can model the class-conditional densities pyck using gaussian distributions and then use the techniques of section to find the parameters of the gaussian distributions by maximum likelihood. having found gaussian approximations to the projected classes the formalism of section then gives an expression for the optimal threshold. some justification for the gaussian assumption comes from the central limit theorem by noting that y wtx is the sum of a set of random variables. relation to least squares the least-squares approach to the determination of a linear discriminant was based on the goal of making the model predictions as close as possible to a set of target values. by contrast the fisher criterion was derived by requiring maximum class separation in the output space. it is interesting to see the relationship between these two approaches. in particular we shall show that for the two-class problem the fisher criterion can be obtained as a special case of least squares. so far we have considered coding for the target values. if however we adopt a slightly different target coding scheme then the least-squares solution for linear models for classification the weights becomes equivalent to the fisher solution and hart in particular we shall take the targets for class to be where is the number of patterns in class and n is the total number of patterns. this target value approximates the reciprocal of the prior probability for class for class we shall take the targets to be where is the number of patterns in class setting the derivatives of e with respect to and w to zero we obtain respectively the sum-of-squares error function can be written wtxn tn e wtxn tn wtxn tn xn from and making use of our choice of target coding scheme for the tn we obtain an expression for the bias in the form where we have used wtm tn n n and where m is the mean of the total data set and is given by m n xn n exercise after some straightforward algebra and again making use of the choice of tn the second equation becomes sw n sb w where sw is defined by sb is defined by and we have substituted for the bias using using we note that sbw is always in the direction of thus we can write w s w where we have ignored irrelevant scale factors. thus the weight vector coincides with that found from the fisher criterion. in addition we have also found an expression for the bias value given by this tells us that a new vector x should be classified as belonging to class if yx wtx m and class otherwise. discriminant functions fisher s discriminant for multiple classes we now consider the generalization of the fisher discriminant to k classes and we shall assume that the dimensionality d of the input space is greater than the k x where number k of classes. next we introduce d k d these feature values can conveniently be grouped together to form a vector y. similarly the weight vectors can be considered to be the columns of a matrix w so that linear features yk wt note that again we are not including any bias parameters in the definition of y. the generalization of the within-class covariance matrix to the case of k classes follows from to give y wtx. sw sk xn n ck nk n ck mkxn mkt where sk mk and nk is the number of patterns in class ck. in order to find a generalization of the between-class covariance matrix we follow duda and hart and consider first the total covariance matrix where m is the mean of the total data set st mxn mt m n xn n nkmk and n k nk is the total number of data points. the total covariance matrix can be decomposed into the sum of the within-class covariance matrix given by and plus an additional matrix sb which we identify as a measure of the between-class covariance st sw sb where sb nkmk mmk mt. n ck n ck linear models for classification these covariance matrices have been defined in the original x-space. we can now define similar matrices in the projected d y-space sw kyn kt and where sb k nk nk k k yn nk k. n jw tr s w sb again we wish to construct a scalar that is large when the between-class covariance is large and when the within-class covariance is small. there are now many possible choices of criterion one example is given by this criterion can then be rewritten as an explicit function of the projection matrix w in the form jw tr maximization of such criteria is straightforward though somewhat involved and is discussed at length in fukunaga the weight values are determined by those eigenvectors of s w sb that correspond to the d largest eigenvalues. there is one important result that is common to all such criteria which is worth emphasizing. we first note from that sb is composed of the sum of k matrices each of which is an outer product of two vectors and therefore of rank in addition only of these matrices are independent as a result of the constraint thus sb has rank at most equal to and so there are at most nonzero eigenvalues. this shows that the projection onto the subspace spanned by the eigenvectors of sb does not alter the value of jw and so we are therefore unable to find more than linear features by this means the perceptron algorithm another example of a linear discriminant model is the perceptron of rosenblatt which occupies an important place in the history of pattern recognition algorithms. it corresponds to a two-class model in which the input vector x is first transformed using a fixed nonlinear transformation to give a feature vector and this is then used to construct a generalized linear model of the form yx f wt discriminant functions where the nonlinear activation function f is given by a step function of the form fa a a the vector will typically include a bias component in earlier discussions of two-class classification problems we have focussed on a target coding scheme in which t which is appropriate in the context of probabilistic models. for the perceptron however it is more convenient to use target values t for class and t for class which matches the choice of activation function. the algorithm used to determine the parameters w of the perceptron can most easily be motivated by error function minimization. a natural choice of error function would be the total number of misclassified patterns. however this does not lead to a simple learning algorithm because the error is a piecewise constant function of w with discontinuities wherever a change in w causes the decision boundary to move across one of the data points. methods based on changing w using the gradient of the error function cannot then be applied because the gradient is zero almost everywhere. we therefore consider an alternative error function known as the perceptron criterion. to derive this we note that we are seeking a weight vector w such that patterns xn in class will have wt whereas patterns xn in class have wt using the t target coding scheme it follows that we would like all patterns to satisfy wt the perceptron criterion associates zero error with any pattern that is correctly classified whereas for a misclassified pattern xn it tries to minimize the quantity wt the perceptron criterion is therefore given by epw wt ntn n m frank rosenblatt rosenblatt s perceptron played an important role in the history of machine learning. initially rosenblatt simulated the perceptron on an ibm computer at cornell in but by the early he had built special-purpose hardware that provided a direct parallel implementation of perceptron learning. many of his ideas were encapsulated in principles of neurodynamics perceptrons and the theory of brain mechanisms published in rosenblatt s work was criticized by marvin minksy whose objections were published in the book perceptrons co-authored with seymour papert. this book was widely misinterpreted at the time as showing that neural networks were fatally flawed and could only learn solutions for linearly separable problems. in fact it only proved such limitations in the case of single-layer networks such as the perceptron and merely conjectured that they applied to more general network models. unfortunately however this book contributed to the substantial decline in research funding for neural computing a situation that was not reversed until the today there are many hundreds if not thousands of applications of neural networks in widespread use with examples in areas such as handwriting recognition and information retrieval being used routinely by millions of people. linear models for classification section where m denotes the set of all misclassified patterns. the contribution to the error associated with a particular misclassified pattern is a linear function of w in regions of w space where the pattern is misclassified and zero in regions where it is correctly classified. the total error function is therefore piecewise linear. we now apply the stochastic gradient descent algorithm to this error function. the change in the weight vector w is then given by w w epw w ntn where is the learning rate parameter and is an integer that indexes the steps of the algorithm. because the perceptron function yx w is unchanged if we multiply w by a constant we can set the learning rate parameter equal to without of generality. note that as the weight vector evolves during training the set of patterns that are misclassified will change. the perceptron learning algorithm has a simple interpretation as follows. we cycle through the training patterns in turn and for each pattern xn we evaluate the perceptron function if the pattern is correctly classified then the weight vector remains unchanged whereas if it is incorrectly classified then for class we add the vector onto the current estimate of weight vector w while for class we subtract the vector from w. the perceptron learning algorithm is illustrated in figure if we consider the effect of a single update in the perceptron learning algorithm we see that the contribution to the error from a misclassified pattern will be reduced because from we have w ntn w ntn ntnt ntn w ntn where we have set and made use of of course this does not imply that the contribution to the error function from the other misclassified patterns will have been reduced. furthermore the change in weight vector may have caused some previously correctly classified patterns to become misclassified. thus the perceptron learning rule is not guaranteed to reduce the total error function at each stage. however the perceptron convergence theorem states that if there exists an exact solution other words if the training data set is linearly separable then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps. proofs of this theorem can be found for example in rosenblatt block nilsson minsky and papert hertz et al. and bishop note however that the number of steps required to achieve convergence could still be substantial and in practice until convergence is achieved we will not be able to distinguish between a nonseparable problem and one that is simply slow to converge. even when the data set is linearly separable there may be many solutions and which one is found will depend on the initialization of the parameters and on the order of presentation of the data points. furthermore for data sets that are not linearly separable the perceptron learning algorithm will never converge. discriminant functions figure illustration of the convergence of the perceptron learning algorithm showing data points from two classes and blue in a two-dimensional feature space the top left plot shows the initial parameter vector w shown as a black arrow together with the corresponding decision boundary line in which the arrow points towards the decision region which classified as belonging to the red class. the data point circled in green is misclassified and so its feature vector is added to the current weight vector giving the new decision boundary shown in the top right plot. the bottom left plot shows the next misclassified point to be considered indicated by the green circle and its feature vector is again added to the weight vector giving the decision boundary shown in the bottom right plot for which all data points are correctly classified. linear models for classification figure illustration of the mark perceptron hardware. the photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene in this case a printed character was illuminated by powerful lights and an image focussed onto a array of cadmium sulphide photocells giving a primitive pixel image. the perceptron also had a patch board shown in the middle photograph which allowed different configurations of input features to be tried. often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring in contrast to a modern digital computer. the photograph on the right shows one of the racks of adaptive weights. each weight was implemented using a rotary variable resistor also called a potentiometer driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm. aside from difficulties with the learning algorithm the perceptron does not provide probabilistic outputs nor does it generalize readily to k classes. the most important limitation however arises from the fact that common with all of the models discussed in this chapter and the previous one it is based on linear combinations of fixed basis functions. more detailed discussions of the limitations of perceptrons can be found in minsky and papert and bishop analogue hardware implementations of the perceptron were built by rosenblatt based on motor-driven variable resistors to implement the adaptive parameters wj. these are illustrated in figure the inputs were obtained from a simple camera system based on an array of photo-sensors while the basis functions could be chosen in a variety of ways for example based on simple fixed functions of randomly chosen subsets of pixels from the input image. typical applications involved learning to discriminate simple shapes or characters. at the same time that the perceptron was being developed a closely related system called the adaline which is short for adaptive linear element was being explored by widrow and co-workers. the functional form of the model was the same as for the perceptron but a different approach to training was adopted and hoff widrow and lehr probabilistic generative models we turn next to a probabilistic view of classification and show how models with linear decision boundaries arise from simple assumptions about the distribution of the data. in section we discussed the distinction between the discriminative and the generative approaches to classification. here we shall adopt a generative probabilistic generative models figure plot of the logistic sigmoid function defined by shown in red together with the scaled probit function a for shown in dashed blue where is defined by the scaling factor is chosen so that the derivatives of the two curves are equal for a approach in which we model the class-conditional densities pxck as well as the class priors pck and then use these to compute posterior probabilities pckx through bayes theorem. can be written as consider first of all the case of two classes. the posterior probability for class exp a a ln and is the logistic sigmoid function defined by where we have defined exp a which is plotted in figure the term sigmoid means s-shaped. this type of function is sometimes also called a squashing function because it maps the whole real axis into a finite interval. the logistic sigmoid has been encountered already in earlier chapters and plays an important role in many classification algorithms. it satisfies the following symmetry property as is easily verified. the inverse of the logistic sigmoid is given by a a ln and is known as the logit function. it represents the log of the ratio of probabilities ln for the two classes also known as the log odds. linear models for classification note that in we have simply rewritten the posterior probabilities in an equivalent form and so the appearance of the logistic sigmoid may seem rather vacuous. however it will have significance provided ax takes a simple functional form. we shall shortly consider situations in which ax is a linear function of x in which case the posterior probability is governed by a generalized linear model. for the case of k classes we have pckx pxckpck j pxcjpcj expak j expaj which is known as the normalized exponential and can be regarded as a multiclass generalization of the logistic sigmoid. here the quantities ak are defined by ak ln pxckpck. the normalized exponential is also known as the softmax function as it represents a smoothed version of the max function because if ak aj for all j k then pckx and pcjx we now investigate the consequences of choosing specific forms for the classconditional densities looking first at continuous input variables x and then discussing briefly the case of discrete inputs. continuous inputs let us assume that the class-conditional densities are gaussian and then explore the resulting form for the posterior probabilities. to start with we shall assume that all classes share the same covariance matrix. thus the density for class ck is given by k pxck exp kt consider first the case of two classes. from and we have where we have defined w t ln t we see that the quadratic terms in x from the exponents of the gaussian densities have cancelled to the assumption of common covariance matrices leading to a linear function of x in the argument of the logistic sigmoid. this result is illustrated for the case of a two-dimensional input space x in figure the resulting probabilistic generative models figure the left-hand plot shows the class-conditional densities for two classes denoted red and blue. on the right is the corresponding posterior probability which is given by a logistic sigmoid of a linear function of x. the surface in the right-hand plot is coloured using a proportion of red ink given by and a proportion of blue ink given by decision boundaries correspond to surfaces along which the posterior probabilities pckx are constant and so will be given by linear functions of x and therefore the decision boundaries are linear in input space. the prior probabilities pck enter only through the bias parameter so that changes in the priors have the effect of making parallel shifts of the decision boundary and more generally of the parallel contours of constant posterior probability. for the general case of k classes we have from and akx wt k x where we have defined wk k t k k ln pck. we see that the akx are again linear functions of x as a consequence of the cancellation of the quadratic terms due to the shared covariances. the resulting decision boundaries corresponding to the minimum misclassification rate will occur when two of the posterior probabilities two largest are equal and so will be defined by linear functions of x and so again we have a generalized linear model. if we relax the assumption of a shared covariance matrix and allow each classconditional density pxck to have its own covariance matrix k then the earlier cancellations will no longer occur and we will obtain quadratic functions of x giving rise to a quadratic discriminant. the linear and quadratic decision boundaries are illustrated in figure linear models for classification figure the left-hand plot shows the class-conditional densities for three classes each having a gaussian distribution coloured red green and blue in which the red and green classes have the same covariance matrix. the right-hand plot shows the corresponding posterior probabilities in which the rgb colour vector represents the posterior probabilities for the respective three classes. the decision boundaries are also shown. notice that the boundary between the red and green classes which have the same covariance matrix is linear whereas those between the other pairs of classes are quadratic. maximum likelihood solution once we have specified a parametric functional form for the class-conditional densities pxck we can then determine the values of the parameters together with the prior class probabilities pck using maximum likelihood. this requires a data set comprising observations of x along with their corresponding class labels. consider first the case of two classes each having a gaussian class-conditional density with a shared covariance matrix and suppose we have a data set tn where n n. here tn denotes class and tn denotes class we denote the prior class probability so that for a data point xn from class we have tn and hence n similarly for class we have tn and hence thus the likelihood function is given by pt n tn where t tnt. as usual it is convenient to maximize the log of the likelihood function. consider first the maximization with respect to the terms in exercise probabilistic generative models the log likelihood function that depend on are ln tn n setting the derivative with respect to equal to zero and rearranging we obtain tn n where denotes the total number of data points in class and denotes the total number of data points in class thus the maximum likelihood estimate for is simply the fraction of points in class as expected. this result is easily generalized to the multiclass case where again the maximum likelihood estimate of the prior probability associated with class ck is given by the fraction of the training set points assigned to that class. now consider the maximization with respect to again we can pick out of the log likelihood function those terms that depend on giving tnxn const. tn lnn setting the derivative with respect to to zero and rearranging we obtain which is simply the mean of all the input vectors xn assigned to class by a similar argument the corresponding result for is given by tnxn tnxn which again is the mean of all the input vectors xn assigned to class finally consider the maximum likelihood solution for the shared covariance matrix picking out the terms in the log likelihood function that depend on we have tn ln n tn ln ln n tr tnxn tnxn linear models for classification where we have defined s n n n n using the standard result for the maximum likelihood solution for a gaussian distribution we see that s which represents a weighted average of the covariance matrices associated with each of the two classes separately. this result is easily extended to the k class problem to obtain the corresponding maximum likelihood solutions for the parameters in which each class-conditional density is gaussian with a shared covariance matrix. note that the approach of fitting gaussian distributions to the classes is not robust to outliers because the maximum likelihood estimation of a gaussian is not robust. discrete features let us now consider the case of discrete feature values xi. for simplicity we begin by looking at binary feature values xi and discuss the extension to more general discrete features shortly. if there are d inputs then a general distribution would correspond to a table of numbers for each class containing independent variables to the summation constraint. because this grows exponentially with the number of features we might seek a more restricted representation. here we will make the naive bayes assumption in which the feature values are treated as independent conditioned on the class ck. thus we have class-conditional distributions of the form pxck xi xi exercise section section which contain d independent parameters for each class. substituting into then gives akx ln ki xi ki ln pck exercise which again are linear functions of the input values xi. for the case of k classes we can alternatively consider the logistic sigmoid formulation given by analogous results are obtained for discrete variables each of which can take m states. exponential family as we have seen for both gaussian distributed and discrete inputs the posterior class probabilities are given by generalized linear models with logistic sigmoid probabilistic discriminative models classes or softmax classes activation functions. these are particular cases of a more general result obtained by assuming that the class-conditional densities pxck are members of the exponential family of distributions. using the form for members of the exponential family we see that the distribution of x can be written in the form px k hxg k exp t k ux we now restrict attention to the subclass of such distributions for which ux x. then we make use of to introduce a scaling parameter s so that we obtain the restricted set of exponential family class-conditional densities of the form s px k s s h s x g k exp t k x note that we are allowing each class to have its own parameter vector k but we are assuming that the classes share the same scale parameter s. for the two-class problem we substitute this expression for the class-conditional densities into and we see that the posterior class probability is again given by a logistic sigmoid acting on a linear function ax which is given by ax ln g ln g ln ln similarly for the k-class problem we substitute the class-conditional density expression into to give akx t k x ln g k ln pck and so again is a linear function of x. probabilistic discriminative models for the two-class classification problem we have seen that the posterior probability of class can be written as a logistic sigmoid acting on a linear function of x for a wide choice of class-conditional distributions pxck. similarly for the multiclass case the posterior probability of class ck is given by a softmax transformation of a linear function of x. for specific choices of the class-conditional densities pxck we have used maximum likelihood to determine the parameters of the densities as well as the class priors pck and then used bayes theorem to find the posterior class probabilities. however an alternative approach is to use the functional form of the generalized linear model explicitly and to determine its parameters directly by using maximum likelihood. we shall see that there is an efficient algorithm finding such solutions known as iterative reweighted least squares or irls. the indirect approach to finding the parameters of a generalized linear model by fitting class-conditional densities and class priors separately and then applying linear models for classification figure illustration of the role of nonlinear basis functions in linear classification models. the left plot shows the original input space together with data points from two classes labelled red and blue. two gaussian basis functions and are defined in this space with centres shown by the green crosses and with contours shown by the green circles. the right-hand plot shows the corresponding feature space together with the linear decision boundary obtained given by a logistic regression model of the form discussed in section this corresponds to a nonlinear decision boundary in the original input space shown by the black curve in the left-hand plot. bayes theorem represents an example of generative modelling because we could take such a model and generate synthetic data by drawing values of x from the marginal distribution px. in the direct approach we are maximizing a likelihood function defined through the conditional distribution pckx which represents a form of discriminative training. one advantage of the discriminative approach is that there will typically be fewer adaptive parameters to be determined as we shall see shortly. it may also lead to improved predictive performance particularly when the class-conditional density assumptions give a poor approximation to the true distributions. fixed basis functions so far in this chapter we have considered classification models that work directly with the original input vector x. however all of the algorithms are equally applicable if we first make a fixed nonlinear transformation of the inputs using a vector of basis functions the resulting decision boundaries will be linear in the feature space and these correspond to nonlinear decision boundaries in the original x space as illustrated in figure classes that are linearly separable in the feature space need not be linearly separable in the original observation space x. note that as in our discussion of linear models for regression one of the probabilistic discriminative models basis functions is typically set to a constant say so that the corresponding parameter plays the role of a bias. for the remainder of this chapter we shall include a fixed basis function transformation as this will highlight some useful similarities to the regression models discussed in chapter for many problems of practical interest there is significant overlap between the class-conditional densities pxck. this corresponds to posterior probabilities pckx which for at least some values of x are not or in such cases the optimal solution is obtained by modelling the posterior probabilities accurately and then applying standard decision theory as discussed in chapter note that nonlinear transformations cannot remove such class overlap. indeed they can increase the level of overlap or create overlap where none existed in the original observation space. however suitable choices of nonlinearity can make the process of modelling the posterior probabilities easier. such fixed basis function models have important limitations and these will be resolved in later chapters by allowing the basis functions themselves to adapt to the data. notwithstanding these limitations models with fixed nonlinear basis functions play an important role in applications and a discussion of such models will introduce many of the key concepts needed for an understanding of their more complex counterparts. logistic regression we begin our treatment of generalized linear models by considering the problem of two-class classification. in our discussion of generative approaches in section we saw that under rather general assumptions the posterior probability of class can be written as a logistic sigmoid acting on a linear function of the feature vector so that with here is the logistic sigmoid function defined by in the terminology of statistics this model is known as logistic regression although it should be emphasized that this is a model for classification rather than regression. y wt for an m-dimensional feature space this model has m adjustable parameters. by contrast if we had fitted gaussian class conditional densities using maximum likelihood we would have used parameters for the means and mm parameters for the covariance matrix. together with the class prior this gives a total of mm parameters which grows quadratically with m in contrast to the linear dependence on m of the number of parameters in logistic regression. for large values of m there is a clear advantage in working with the logistic regression model directly. we now use maximum likelihood to determine the parameters of the logistic regression model. to do this we shall make use of the derivative of the logistic sigmoid function which can conveniently be expressed in terms of the sigmoid function itself d da section exercise linear models for classification for a data set n tn where tn and n with n n the likelihood function can be written ptw n tn ytn where t tn and yn n. as usual we can define an error function by taking the negative logarithm of the likelihood which gives the crossentropy error function in the form ew ln ptw exercise section exercise ln yn tn yn where yn and an wt n. taking the gradient of the error function with respect to w we obtain ew tn n where we have made use of we see that the factor involving the derivative of the logistic sigmoid has cancelled leading to a simplified form for the gradient of the log likelihood. in particular the contribution to the gradient from data point n is given by the error yn tn between the target value and the prediction of the model times the basis function vector n. furthermore comparison with shows that this takes precisely the same form as the gradient of the sum-of-squares error function for the linear regression model. if desired we could make use of the result to give a sequential algorithm in which patterns are presented one at a time in which each of the weight vectors is updated using in which en is the nth term in it is worth noting that maximum likelihood can exhibit severe over-fitting for data sets that are linearly separable. this arises because the maximum likelihood solution occurs when the hyperplane corresponding to equivalent to wt separates the two classes and the magnitude of w goes to infinity. in this case the logistic sigmoid function becomes infinitely steep in feature space corresponding to a heaviside step function so that every training point from each class k is assigned a posterior probability pckx furthermore there is typically a continuum of such solutions because any separating hyperplane will give rise to the same posterior probabilities at the training data points as will be seen later in figure maximum likelihood provides no way to favour one such solution over another and which solution is found in practice will depend on the choice of optimization algorithm and on the parameter initialization. note that the problem will arise even if the number of data points is large compared with the number of parameters in the model so long as the training data set is linearly separable. the singularity can be avoided by inclusion of a prior and finding a map solution for w or equivalently by adding a regularization term to the error function. probabilistic discriminative models iterative reweighted least squares in the case of the linear regression models discussed in chapter the maximum likelihood solution on the assumption of a gaussian noise model leads to a closed-form solution. this was a consequence of the quadratic dependence of the log likelihood function on the parameter vector w. for logistic regression there is no longer a closed-form solution due to the nonlinearity of the logistic sigmoid function. however the departure from a quadratic form is not substantial. to be precise the error function is concave as we shall see shortly and hence has a unique minimum. furthermore the error function can be minimized by an efficient iterative technique based on the newton-raphson iterative optimization scheme which uses a local quadratic approximation to the log likelihood function. the newton-raphson update for minimizing a function ew takes the form bishop and nabney where h is the hessian matrix whose elements comprise the second derivatives of ew with respect to the components of w. let us first of all apply the newton-raphson method to the linear regression model with the sum-of-squares error function the gradient and hessian of this error function are given by wnew wold h ew. ew h ew n tn n t w tt n t n t section where is the n m design matrix whose nth row is given by t raphson update then takes the form t wold tt wnew wold t n. the newton t tt which we recognize as the standard least-squares solution. note that the error function in this case is quadratic and hence the newton-raphson formula gives the exact solution in one step. now let us apply the newton-raphson update to the cross-entropy error function for the logistic regression model. from we see that the gradient and hessian of this error function are given by ew tn n ty t h ew yn n t n tr linear models for classification rnn yn. where we have made use of also we have introduced the n n diagonal matrix r with elements we see that the hessian is no longer constant but depends on w through the weighting matrix r corresponding to the fact that the error function is no longer quadratic. using the property yn which follows from the form of the logistic sigmoid function we see that uthu for an arbitrary vector u and so the hessian matrix h is positive definite. it follows that the error function is a concave function of w and hence has a unique minimum. exercise the newton-raphson update formula for the logistic regression model then be comes wnew wold tr ty t tr wold ty t tr tr trz where z is an n-dimensional vector with elements z wold r t. we see that the update formula takes the form of a set of normal equations for a weighted least-squares problem. because the weighing matrix r is not constant but depends on the parameter vector w we must apply the normal equations iteratively each time using the new weight vector w to compute a revised weighing matrix r. for this reason the algorithm is known as iterative reweighted least squares or irls as in the weighted least-squares problem the elements of the diagonal weighting matrix r can be interpreted as variances because the mean and variance of t in the logistic regression model are given by et y vart y where we have used the property t for t in fact we can interpret irls as the solution to a linearized problem in the space of the variable a wt the quantity zn which corresponds to the nth element of z can then be given a simple interpretation as an effective target value in this space obtained by making a local linear approximation to the logistic sigmoid function around the current operating point wold anw anwold dan dyn nwold tn yn wold yn zn. t probabilistic discriminative models section multiclass logistic regression in our discussion of generative models for multiclass classification we have seen that for a large class of distributions the posterior probabilities are given by a softmax transformation of linear functions of the feature variables so that pck yk expak j expaj where the activations ak are given by ak wt k there we used maximum likelihood to determine separately the class-conditional densities and the class priors and then found the corresponding posterior probabilities using bayes theorem thereby implicitly determining the parameters here we consider the use of maximum likelihood to determine the parameters of this model directly. to do this we will require the derivatives of yk with respect to all of the activations aj. these are given by exercise ykikj yj yk aj where ikj are the elements of the identity matrix. next we write down the likelihood function. this is most easily done using the coding scheme in which the target vector tn for a feature vector n belonging to class ck is a binary vector with all elements zero except for element k which equals one. the likelihood function is then given by wk pck ntnk ytnk nk where ynk yk n and t is an n k matrix of target variables with elements tnk. taking the negative logarithm then gives wk ln wk tnk ln ynk which is known as the cross-entropy error function for the multiclass classification problem. we now take the gradient of the error function with respect to one of the parameter vectors wj. making use of the result for the derivatives of the softmax function we obtain wj wk tnj n exercise linear models for classification k tnk once again we see the same form arising where we have made use of for the gradient as was found for the sum-of-squares error function with the linear model and the cross-entropy error for the logistic regression model namely the product of the error tnj times the basis function n. again we could use this to formulate a sequential algorithm in which patterns are presented one at a time in which each of the weight vectors is updated using we have seen that the derivative of the log likelihood function for a linear regression model with respect to the parameter vector w for a data point n took the form of the error yn tn times the feature vector n. similarly for the combination of logistic sigmoid activation function and cross-entropy error function and for the softmax activation function with the multiclass cross-entropy error function we again obtain this same simple form. this is an example of a more general result as we shall see in section to find a batch algorithm we again appeal to the newton-raphson update to obtain the corresponding irls algorithm for the multiclass problem. this requires evaluation of the hessian matrix that comprises blocks of size m m in which block j k is given by wj wk wk ynkikj ynj n t n. exercise as with the two-class problem the hessian matrix for the multiclass logistic regression model is positive definite and so the error function again has a unique minimum. practical details of irls for the multiclass case can be found in bishop and nabney probit regression we have seen that for a broad range of class-conditional distributions described by the exponential family the resulting posterior class probabilities are given by a logistic softmax transformation acting on a linear function of the feature variables. however not all choices of class-conditional density give rise to such a simple form for the posterior probabilities instance if the class-conditional densities are modelled using gaussian mixtures. this suggests that it might be worth exploring other types of discriminative probabilistic model. for the purposes of this chapter however we shall return to the two-class case and again remain within the framework of generalized linear models so that pt fa where a wt and f is the activation function. one way to motivate an alternative choice for the link function is to consider a noisy threshold model as follows. for each input n we evaluate an wt n and then we set the target value according to tn if an tn otherwise. probabilistic discriminative models figure schematic example of a probability density p shown by the blue curve given in this example by a mixture of two gaussians along with its cumulative distribution function f shown by the red curve. note that the value of the blue curve at any point such as that indicated by the vertical green line corresponds to the slope of the red curve at the same point. conversely the value of the red curve at this point corresponds to the area under the blue curve indicated by the shaded green region. in the stochastic threshold model the class label takes the value t if the value of a wt exceeds a threshold otherwise it takes the value t this is equivalent to an activation function given by the cumulative distribution function f if the value of is drawn from a probability density p then the corresponding activation function will be given by the cumulative distribution function a fa p d as illustrated in figure as a specific example suppose that the density p is given by a zero mean unit variance gaussian. the corresponding cumulative distribution function is given by a n d which is known as the probit function. it has a sigmoidal shape and is compared with the logistic sigmoid function in figure note that the use of a more general gaussian distribution does not change the model because this is equivalent to a re-scaling of the linear coefficients w. many numerical packages provide for the evaluation of a closely related function defined by erfa exp d a exercise and known as the erf function or error function to be confused with the error function of a machine learning model. it is related to the probit function by erfa the generalized linear model based on a probit activation function is known as probit regression. we can determine the parameters of this model using maximum likelihood by a straightforward extension of the ideas discussed earlier. in practice the results found using probit regression tend to be similar to those of logistic regression. we shall linear models for classification however find another use for the probit model when we discuss bayesian treatments of logistic regression in section one issue that can occur in practical applications is that of outliers which can arise for instance through errors in measuring the input vector x or through mislabelling of the target value t. because such points can lie a long way to the wrong side of the ideal decision boundary they can seriously distort the classifier. note that the logistic and probit regression models behave differently in this respect because the tails of the logistic sigmoid decay asymptotically like exp x for x whereas for the probit activation function they decay like exp and so the probit model can be significantly more sensitive to outliers. however both the logistic and the probit models assume the data is correctly labelled. the effect of mislabelling is easily incorporated into a probabilistic model by introducing a probability that the target value t has been flipped to the wrong value and winther leading to a target value distribution for data point x of the form ptx where is the activation function with input vector x. here may be set in advance or it may be treated as a hyperparameter whose value is inferred from the data. canonical link functions for the linear regression model with a gaussian noise distribution the error function corresponding to the negative log likelihood is given by if we take the derivative with respect to the parameter vector w of the contribution to the error function from a data point n this takes the form of the error yn tn times the feature vector n where yn wt n. similarly for the combination of the logistic sigmoid activation function and the cross-entropy error function and for the softmax activation function with the multiclass cross-entropy error function we again obtain this same simple form. we now show that this is a general result of assuming a conditional distribution for the target variable from the exponential family along with a corresponding choice for the activation function known as the canonical link function. we again make use of the restricted form of exponential family distributions. note that here we are applying the assumption of exponential family distribution to the target variable t in contrast to section where we applied it to the input vector x. we therefore consider conditional distributions of the target variable of the form pt s s h t s t s g exp using the same line of argument as led to the derivation of the result we see that the conditional mean of t which we denote by y is given by y et s d d ln g the laplace approximation thus y and must related and we denote this relation through following nelder and wedderburn we define a generalized linear model to be one for which y is a nonlinear function of a linear combination of the input feature variables so that y fwt where f is known as the activation function in the machine learning literature and is known as the link function in statistics. f now consider the log likelihood function for this model which as a function of is given by ln pt s ln ptn s ln g n ntn s const where we are assuming that all observations share a common scale parameter corresponds to the noise variance for a gaussian distribution for instance and so s is independent of n. the derivative of the log likelihood with respect to the model parameters w is then given by w ln pt s d d n ln g n tn s d n dyn dyn dan an yn n s where an wt n and we have used yn fan together with the result for et we now see that there is a considerable simplification if we choose a particular form for the link function f given by which gives f y and hence f we have a and hence f function reduces to f in this case the gradient of the error also because a f ln ew s tn n. for the gaussian s whereas for the logistic model s the laplace approximation in section we shall discuss the bayesian treatment of logistic regression. as we shall see this is more complex than the bayesian treatment of linear regression models discussed in sections and in particular we cannot integrate exactly linear models for classification chapter chapter over the parameter vector w since the posterior distribution is no longer gaussian. it is therefore necessary to introduce some form of approximation. later in the book we shall consider a range of techniques based on analytical approximations and numerical sampling. here we introduce a simple but widely used framework called the laplace approximation that aims to find a gaussian approximation to a probability density defined over a set of continuous variables. consider first the case of a single continuous variable z and suppose the distribution pz is defined by pz z fz fz dz is the normalization coefficient. we shall suppose that the where z value of z is unknown. in the laplace method the goal is to find a gaussian approximation qz which is centred on a mode of the distribution pz. the first step is to or equivalently find a mode of pz in other words a point such that p a gaussian distribution has the property that its logarithm is a quadratic function of the variables. we therefore consider a taylor expansion of ln fz centred on the mode so that dfz dz where ln fz ln az a ln fz note that the first-order term in the taylor expansion does not appear since is a local maximum of the distribution. taking the exponential we obtain fz exp a we can then obtain a normalized distribution qz by making use of the standard result for the normalization of a gaussian so that qz a exp a the laplace approximation is illustrated in figure note that the gaussian approximation will only be well defined if its precision a in other words the stationary point must be a local maximum so that the second derivative of fz at the point is negative. the laplace approximation figure illustration of the laplace approximation applied to the distribution pz exp where is the logistic sigmoid function defined by e z the left plot shows the normalized distribution pz in yellow together with the laplace approximation centred on the mode of pz in red. the right plot shows the negative logarithms of the corresponding curves. we can extend the laplace method to approximate a distribution pz fzz defined over an m-dimensional space z. at a stationary point the gradient fz will vanish. expanding around this stationary point we have ln fz ln where the m m hessian matrix a is defined by a ln and is the gradient operator. taking the exponential of both sides we obtain fz exp qz exp the distribution qz is proportional to fz and the appropriate normalization coefficient can be found by inspection using the standard result for a normalized multivariate gaussian giving where denotes the determinant of a. this gaussian distribution will be well defined provided its precision matrix given by a is positive definite which implies that the stationary point must be a local maximum not a minimum or a saddle point. n a in order to apply the laplace approximation we first need to find the mode and then evaluate the hessian matrix at that mode. in practice a mode will typically be found by running some form of numerical optimization algorithm linear models for classification and nabney many of the distributions encountered in practice will be multimodal and so there will be different laplace approximations according to which mode is being considered. note that the normalization constant z of the true distribution does not need to be known in order to apply the laplace method. as a result of the central limit theorem the posterior distribution for a model is expected to become increasingly better approximated by a gaussian as the number of observed data points is increased and so we would expect the laplace approximation to be most useful in situations where the number of data points is relatively large. one major weakness of the laplace approximation is that since it is based on a gaussian distribution it is only directly applicable to real variables. in other cases it may be possible to apply the laplace approximation to a transformation of the variable. for instance if then we can consider a laplace approximation of ln the most serious limitation of the laplace framework however is that it is based purely on the aspects of the true distribution at a specific value of the variable and so can fail to capture important global properties. in chapter we shall consider alternative approaches which adopt a more global perspective. model comparison and bic as well as approximating the distribution pz we can also obtain an approxi mation to the normalization constant z. using the approximation we have z fz dz exp dz where we have noted that the integrand is gaussian and made use of the standard result for a normalized gaussian distribution. we can use the result to obtain an approximation to the model evidence which as discussed in section plays a central role in bayesian model comparison. consider a data set d and a set of models having parameters i. for each model we define a likelihood function pd imi. if we introduce a prior p imi over the parameters then we are interested in computing the model evidence pdmi for the various models. from now on we omit the conditioning on mi to keep the notation uncluttered. from bayes theorem the model evidence is given by pd pd d identifying f pd and z pd and applying the result we obtain ln pd ln pd map ln p map m lna occam factor exercise exercise section bayesian logistic regression where map is the value of at the mode of the posterior distribution and a is the hessian matrix of second derivatives of the negative log posterior a ln pd mapp map ln p mapd. the first term on the right hand side of represents the log likelihood evaluated using the optimized parameters while the remaining three terms comprise the occam factor which penalizes model complexity. if we assume that the gaussian prior distribution over parameters is broad and that the hessian has full rank then we can approximate very roughly using ln pd ln pd map m ln n where n is the number of data points m is the number of parameters in and we have omitted additive constants. this is known as the bayesian information criterion or the schwarz criterion note that compared to aic given by this penalizes model complexity more heavily. complexity measures such as aic and bic have the virtue of being easy to evaluate but can also give misleading results. in particular the assumption that the hessian matrix has full rank is often not valid since many of the parameters are not well-determined we can use the result to obtain a more accurate estimate of the model evidence starting from the laplace approximation as we illustrate in the context of neural networks in section bayesian logistic regression we now turn to a bayesian treatment of logistic regression. exact bayesian inference for logistic regression is intractable. in particular evaluation of the posterior distribution would require normalization of the product of a prior distribution and a likelihood function that itself comprises a product of logistic sigmoid functions one for every data point. evaluation of the predictive distribution is similarly intractable. here we consider the application of the laplace approximation to the problem of bayesian logistic regression and lauritzen mackay laplace approximation recall from section that the laplace approximation is obtained by finding the mode of the posterior distribution and then fitting a gaussian centred at that mode. this requires evaluation of the second derivatives of the log posterior which is equivalent to finding the hessian matrix. because we seek a gaussian representation for the posterior distribution it is natural to begin with a gaussian prior which we write in the general form pw n linear models for classification where and are fixed hyperparameters. the posterior distribution over w is given by where t tnt. taking the log of both sides and substituting for the prior distribution using and for the likelihood function using we obtain pwt pwptw ln pwt ln yn tn yn const where yn n. to obtain a gaussian approximation to the posterior distribution we first maximize the posterior distribution to give the map posterior solution wmap which defines the mean of the gaussian. the covariance is then given by the inverse of the matrix of second derivatives of the negative log likelihood which takes the form sn ln pwt s yn n t n. the gaussian approximation to the posterior distribution therefore takes the form qw n sn having obtained a gaussian approximation to the posterior distribution there remains the task of marginalizing with respect to this distribution in order to make predictions. predictive distribution the predictive distribution for class given a new feature vector is obtained by marginalizing with respect to the posterior distribution pwt which is itself approximated by a gaussian distribution qw so that t wpwt dw with the corresponding probability for class given by t t. to evaluate the predictive distribution we first note that the function depends on w only through its projection onto denoting a wt we have dw wt da where is the dirac delta function. from this we obtain dw da where pa bayesian logistic regression wt dw. we can evaluate pa by noting that the delta function imposes a linear constraint on w and so forms a marginal distribution from the joint distribution qw by integrating out all directions orthogonal to because qw is gaussian we know from section that the marginal distribution will also be gaussian. we can evaluate the mean and covariance of this distribution by taking moments and interchanging the order of integration over a and w so that a ea paa da qwwt dw wt map where we have used the result for the variational posterior distribution qw. similarly a vara da pa qw n dw tsn note that the distribution of a takes the same form as the predictive distribution for the linear regression model with the noise variance set to zero. thus our variational approximation to the predictive distribution becomes a da a da. this result can also be derived directly by making use of the results for the marginal of a gaussian distribution given in section the integral over a represents the convolution of a gaussian with a logistic sigmoid and cannot be evaluated analytically. we can however obtain a good approximation and lauritzen mackay barber and bishop by making use of the close similarity between the logistic sigmoid function defined by and the probit function defined by in order to obtain the best approximation to the logistic function we need to re-scale the horizontal axis so that we approximate by a. we can find a suitable value of by requiring that the two functions have the same slope at the origin which gives the similarity of the logistic sigmoid and the probit function for this choice of is illustrated in figure the advantage of using a probit function is that its convolution with a gaussian can be expressed analytically in terms of another probit function. specifically we can show that an da exercise exercise exercise linear models for classification we now apply the approximation a to the probit functions appearing on both sides of this equation leading to the following approximation for the convolution of a logistic sigmoid with a gaussian da exercises where we have defined applying this result to we obtain the approximate predictive distribution in the form t a a where a and fined by a are defined by and respectively and a is denote that the decision boundary corresponding to t is given by a which is the same as the decision boundary obtained by using the map value for w. thus if the decision criterion is based on minimizing misclassification rate with equal prior probabilities then the marginalization over w has no effect. however for more complex decision criteria it will play an important role. marginalization of the logistic sigmoid model under a gaussian approximation to the posterior distribution will be illustrated in the context of variational inference in figure given a set of data points we can define the convex hull to be the set of all points x given by x n n consider a second set of points together with separable if there exists a and a scalar such for all where n and xn for all yn. show that if their convex hulls intersect the two their corresponding convex hull. by definition the two sets of points will be linearly nxn n sets of points cannot be linearly separable and conversely that if they are linearly separable their convex hulls do not intersect. www consider the minimization of a sum-of-squares error function and suppose that all of the target vectors in the training set satisfy a linear constraint attn b where tn corresponds to the nth row of the matrix t in show that as a consequence of this constraint the elements of the model prediction yx given by the least-squares solution also satisfy this constraint so that atyx b exercises to do so assume that one of the basis functions so that the corresponding parameter plays the role of a bias. extend the result of exercise to show that if multiple linear constraints are satisfied simultaneously by the target vectors then the same constraints will also be satisfied by the least-squares prediction of a linear model. www show that maximization of the class separation criterion given by with respect to w using a lagrange multiplier to enforce the constraint wtw leads to the result that w by making use of and show that the fisher criterion can be written in the form using the definitions of the between-class and within-class covariance matrices given by and respectively together with and and the choice of target values described in section show that the expression that minimizes the sum-of-squares error function can be written in the form www show that the logistic sigmoid function satisfies the property a and that its inverse is given by y. using and derive the result for the posterior class probability in the two-class generative model with gaussian densities and verify the results and for the parameters w and www consider a generative classification model for k classes defined by prior class probabilities pck k and general class-conditional densities p where is the input feature vector. suppose we are given a training data set n tn where n n and tn is a binary target vector of length k that uses the coding scheme so that it has components tnj ijk if pattern n is from class ck. assuming that the data points are drawn independently from this model show that the maximum-likelihood solution for the prior probabilities is given by where nk is the number of data points assigned to class ck. k nk n consider the classification model of exercise and now suppose that the class-conditional densities are given by gaussian distributions with a shared covariance matrix so that p n k show that the maximum likelihood solution for the mean of the gaussian distribution for class ck is given by tnk n k nk linear models for classification which represents the mean of those feature vectors assigned to class ck. similarly show that the maximum likelihood solution for the shared covariance matrix is given by nk n sk where sk nk tnk n k n kt. thus is given by a weighted average of the covariances of the data associated with each class in which the weighting coefficients are given by the prior probabilities of the classes. consider a classification problem with k classes for which the feature vector has m components each of which can take l discrete states. let the values of the components be represented by a binary coding scheme. further suppose that conditioned on the class ck the m components of are independent so that the class-conditional density factorizes with respect to the feature vector components. show that the quantities ak given by which appear in the argument to the softmax function describing the posterior class probabilities are linear functions of the components of note that this represents an example of the naive bayes model which is discussed in section www verify the relation for the derivative of the logistic sigmoid func tion defined by www by making use of the result for the derivative of the logistic sigmoid show that the derivative of the error function for the logistic regression model is given by show that for a linearly separable data set the maximum likelihood solution for the logistic regression model is obtained by finding a vector w whose decision boundary wt separates the classes and then taking the magnitude of w to infinity. show that the hessian matrix h for the logistic regression model given by is positive definite. here r is a diagonal matrix with elements yn and yn is the output of the logistic regression model for input vector xn. hence show that the error function is a concave function of w and that it has a unique minimum. consider a binary classification problem in which each observation xn is known to belong to one of two classes corresponding to t and t and suppose that the procedure for collecting training data is imperfect so that training points are sometimes mislabelled. for every data point xn instead of having a value t for the class label we have instead a value n representing the probability that tn given a probabilistic model pt write down the log likelihood function appropriate to such a data set. exercises www show that the derivatives of the softmax activation function where the ak are defined by are given by using the result for the derivatives of the softmax activation function show that the gradients of the cross-entropy error are given by www write down expressions for the gradient of the log likelihood as well as the corresponding hessian matrix for the probit regression model defined in section these are the quantities that would be required to train such a model using irls. show that the hessian matrix for the multiclass logistic regression problem defined by is positive semidefinite. note that the full hessian matrix for this problem is of size m k m k where m is the number of parameters and k is the number of classes. to prove the positive semidefinite property consider the product uthu where u is an arbitrary vector of length m k and then apply jensen s inequality. show that the probit function and the erf function are related by using the result derive the expression for the log model evi dence under the laplace approximation. www in this exercise we derive the bic result starting from the laplace approximation to the model evidence given by show that if the prior over parameters is gaussian of the form p n the log model evidence under the laplace approximation takes the form ln pd ln pd map lnh const where h is the matrix of second derivatives of the log likelihood ln pd evaluated at map. now assume that the prior is broad so that v is small and the second term on the right-hand side above can be neglected. furthermore consider the case of independent identically distributed data so that h is the sum of terms one for each data point. show that the log model evidence can then be written approximately in the form of the bic expression map m map mtv use the results from section to derive the result for the marginalization of the logistic regression model with respect to a gaussian posterior distribution over the parameters w. suppose we wish to approximate the logistic sigmoid defined by by a scaled probit function a where is defined by show that if is chosen so that the derivatives of the two functions are equal at a then linear models for classification in this exercise we prove the relation for the convolution of a probit function with a gaussian distribution. to do this show that the derivative of the lefthand side with respect to is equal to the derivative of the right-hand side and then integrate both sides with respect to and then show that the constant of integration vanishes. note that before differentiating the left-hand side it is convenient first to introduce a change of variable given by a z so that the integral over a is replaced by an integral over z. when we differentiate the left-hand side of the relation we will then obtain a gaussian integral over z that can be evaluated analytically. neural networks in chapters and we considered models for regression and classification that comprised linear combinations of fixed basis functions. we saw that such models have useful analytical and computational properties but that their practical applicability was limited by the curse of dimensionality. in order to apply such models to largescale problems it is necessary to adapt the basis functions to the data. support vector machines discussed in chapter address this by first defining basis functions that are centred on the training data points and then selecting a subset of these during training. one advantage of svms is that although the training involves nonlinear optimization the objective function is convex and so the solution of the optimization problem is relatively straightforward. the number of basis functions in the resulting models is generally much smaller than the number of training points although it is often still relatively large and typically increases with the size of the training set. the relevance vector machine discussed in section also chooses a subset from a fixed set of basis functions and typically results in much neural networks sparser models. unlike the svm it also produces probabilistic outputs although this is at the expense of a nonconvex optimization during training. an alternative approach is to fix the number of basis functions in advance but allow them to be adaptive in other words to use parametric forms for the basis functions in which the parameter values are adapted during training. the most successful model of this type in the context of pattern recognition is the feed-forward neural network also known as the multilayer perceptron discussed in this chapter. in fact multilayer perceptron is really a misnomer because the model comprises multiple layers of logistic regression models continuous nonlinearities rather than multiple perceptrons discontinuous nonlinearities. for many applications the resulting model can be significantly more compact and hence faster to evaluate than a support vector machine having the same generalization performance. the price to be paid for this compactness as with the relevance vector machine is that the likelihood function which forms the basis for network training is no longer a convex function of the model parameters. in practice however it is often worth investing substantial computational resources during the training phase in order to obtain a compact model that is fast at processing new data. the term neural network has its origins in attempts to find mathematical representations of information processing in biological systems and pitts widrow and hoff rosenblatt rumelhart et al. indeed it has been used very broadly to cover a wide range of different models many of which have been the subject of exaggerated claims regarding their biological plausibility. from the perspective of practical applications of pattern recognition however biological realism would impose entirely unnecessary constraints. our focus in this chapter is therefore on neural networks as efficient models for statistical pattern recognition. in particular we shall restrict our attention to the specific class of neural networks that have proven to be of greatest practical value namely the multilayer perceptron. we begin by considering the functional form of the network model including the specific parameterization of the basis functions and we then discuss the problem of determining the network parameters within a maximum likelihood framework which involves the solution of a nonlinear optimization problem. this requires the evaluation of derivatives of the log likelihood function with respect to the network parameters and we shall see how these can be obtained efficiently using the technique of error backpropagation. we shall also show how the backpropagation framework can be extended to allow other derivatives to be evaluated such as the jacobian and hessian matrices. next we discuss various approaches to regularization of neural network training and the relationships between them. we also consider some extensions to the neural network model and in particular we describe a general framework for modelling conditional probability distributions known as mixture density networks. finally we discuss the use of bayesian treatments of neural networks. additional background on neural network models can be found in bishop feed-forward network functions feed-forward network functions the linear models for regression and classification discussed in chapters and respectively are based on linear combinations of fixed nonlinear basis functions jx and take the form wj jx yx w f where f is a nonlinear activation function in the case of classification and is the identity in the case of regression. our goal is to extend this model by making the basis functions jx depend on parameters and then to allow these parameters to be adjusted along with the coefficients during training. there are of course many ways to construct parametric nonlinear basis functions. neural networks use basis functions that follow the same form as so that each basis function is itself a nonlinear function of a linear combination of the inputs where the coefficients in the linear combination are adaptive parameters. this leads to the basic neural network model which can be described a series of functional transformations. first we construct m linear combinations of the input variables xd in the form aj ji xi w w where j m and the superscript indicates that the corresponding eters are in the first layer of the network. we shall refer to the parameters w ji as as biases following the nomenclature of chapter weights and the parameters w the quantities aj are known as activations. each of them is then transformed using a differentiable nonlinear activation function h to give zj haj. these quantities correspond to the outputs of the basis functions in that in the context of neural networks are called hidden units. the nonlinear functions h are generally chosen to be sigmoidal functions such as the logistic sigmoid or the tanh function. following these values are again linearly combined to give output unit activations ak w kj zj w where k k and k is the total number of outputs. this transformation responds to the second layer of the network and again the w are bias parameters. finally the output unit activations are transformed using an appropriate activation function to give a set of network outputs yk. the choice of activation function is determined by the nature of the data and the assumed distribution of target variables exercise neural networks figure network diagram for the twolayer neural network corresponding to the input hidden and output variables are represented by nodes and the weight parameters are represented by links between the nodes in which the bias parameters are denoted by links coming from additional input and hidden variables and arrows denote the direction of information flow through the network during forward propagation. xd inputs hidden units zm m d w km w yk outputs w and follows the same considerations as for linear models discussed in chapters and thus for standard regression problems the activation function is the identity so that yk ak. similarly for multiple binary classification problems each output unit activation is transformed using a logistic sigmoid function so that where yk exp a finally for multiclass problems a softmax activation function of the form is used. the choice of output unit activation function is discussed in detail in section we can combine these various stages to give the overall network function that for sigmoidal output unit activation functions takes the form ykx w w kj h ji xi w w w where the set of all weight and bias parameters have been grouped together into a vector w. thus the neural network model is simply a nonlinear function from a set of input variables to a set of output variables controlled by a vector w of adjustable parameters. this function can be represented in the form of a network diagram as shown in figure the process of evaluating can then be interpreted as a forward propagation of information through the network. it should be emphasized that these diagrams do not represent probabilistic graphical models of the kind to be considered in chapter because the internal nodes represent deterministic variables rather than stochastic ones. for this reason we have adopted a slightly different graphical feed-forward network functions notation for the two kinds of model. we shall see later how to give a probabilistic interpretation to a neural network. as discussed in section the bias parameters in can be absorbed into the set of weight parameters by defining an additional input variable whose value is clamped at so that takes the form aj ji xi. w we can similarly absorb the second-layer biases into the second-layer weights so that the overall network function becomes ykx w w kj h ji xi w as can be seen from figure the neural network model comprises two stages of processing each of which resembles the perceptron model of section and for this reason the neural network is also known as the multilayer perceptron or mlp. a key difference compared to the perceptron however is that the neural network uses continuous sigmoidal nonlinearities in the hidden units whereas the perceptron uses step-function nonlinearities. this means that the neural network function is differentiable with respect to the network parameters and this property will play a central role in network training. if the activation functions of all the hidden units in a network are taken to be linear then for any such network we can always find an equivalent network without hidden units. this follows from the fact that the composition of successive linear transformations is itself a linear transformation. however if the number of hidden units is smaller than either the number of input or output units then the transformations that the network can generate are not the most general possible linear transformations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units. in section we show that networks of linear units give rise to principal component analysis. in general however there is little interest in multilayer networks of linear units. the network architecture shown in figure is the most commonly used one in practice. however it is easily generalized for instance by considering additional layers of processing each consisting of a weighted linear combination of the form followed by an element-wise transformation using a nonlinear activation function. note that there is some confusion in the literature regarding the terminology for counting the number of layers in such networks. thus the network in figure may be described as a network counts the number of layers of units and treats the inputs as units or sometimes as a single-hidden-layer network counts the number of layers of hidden units. we recommend a terminology in which figure is called a two-layer network because it is the number of layers of adaptive weights that is important for determining the network properties. another generalization of the network architecture is to include skip-layer connections each of which is associated with a corresponding adaptive parameter. for neural networks figure example of a neural network having a general feed-forward topology. note that each hidden and output unit has an associated bias parameter for clarity. inputs outputs instance in a two-layer network these would go directly from inputs to outputs. in principle a network with sigmoidal hidden units can always mimic skip layer connections bounded input values by using a sufficiently small first-layer weight that over its operating range the hidden unit is effectively linear and then compensating with a large weight value from the hidden unit to the output. in practice however it may be advantageous to include skip-layer connections explicitly. furthermore the network can be sparse with not all possible connections within a layer being present. we shall see an example of a sparse network architecture when we consider convolutional neural networks in section because there is a direct correspondence between a network diagram and its mathematical function we can develop more general network mappings by considering more complex network diagrams. however these must be restricted to a feed-forward architecture in other words to one having no closed directed cycles to ensure that the outputs are deterministic functions of the inputs. this is illustrated with a simple example in figure each or output unit in such a network computes a function given by zk h wkjzj j where the sum runs over all units that send connections to unit k a bias parameter is included in the summation. for a given set of values applied to the inputs of the network successive application of allows the activations of all units in the network to be evaluated including those of the output units. the approximation properties of feed-forward networks have been widely studied cybenko hornik et al. stinchecombe and white cotter ito hornik kreinovich ripley and found to be very general. neural networks are therefore said to be universal approximators. for example a two-layer network with linear outputs can uniformly approximate any continuous function on a compact input domain to arbitrary accuracy provided the network has a sufficiently large number of hidden units. this result holds for a wide range of hidden unit activation functions but excluding polynomials. although such theorems are reassuring the key problem is how to find suitable parameter values given a set of training data and in later sections of this chapter we figure illustration of the capability of a multilayer perceptron to approximate four different functions comprising f f sinx f and f hx where hx is the heaviside step function. in each case n data points shown as blue dots have been sampled uniformly in x over the interval and the corresponding values of f evaluated. these data points are then used to train a twolayer network having hidden units with tanh activation functions and linear output units. the resulting network functions are shown by the red curves and the outputs of the three hidden units are shown by the three dashed curves. feed-forward network functions will show that there exist effective solutions to this problem based on both maximum likelihood and bayesian approaches. the capability of a two-layer network to model a broad range of functions is illustrated in figure this figure also shows how individual hidden units work collaboratively to approximate the final function. the role of hidden units in a simple classification problem is illustrated in figure using the synthetic classification data set described in appendix a. weight-space symmetries one property of feed-forward networks which will play a role when we consider bayesian model comparison is that multiple distinct choices for the weight vector w can all give rise to the same mapping function from inputs to outputs et al. consider a two-layer network of the form shown in figure with m hidden units having tanh activation functions and full connectivity in both layers. if we change the sign of all of the weights and the bias feeding into a particular hidden unit then for a given input pattern the sign of the activation of the hidden unit will be reversed because tanh is an odd function so that tanh a tanha. this transformation can be exactly compensated by changing the sign of all of the weights leading out of that hidden unit. thus by changing the signs of a particular group of weights a bias the input output mapping function represented by the network is unchanged and so we have found two different weight vectors that give rise to the same mapping function. for m hidden units there will be m such sign-flip neural networks figure example of the solution of a simple twoclass classification problem involving synthetic data using a neural network having two inputs two hidden units with tanh activation functions and a single output having a logistic sigmoid activation function. the dashed blue lines show the z contours for each of the hidden units and the red line shows the y decision surface for the network. for comparison the green line denotes the optimal decision boundary computed from the distributions used to generate the data. symmetries and thus any given weight vector will be one of a set equivalent weight vectors similarly imagine that we interchange the values of all of the weights the bias leading both into and out of a particular hidden unit with the corresponding values of the weights bias associated with a different hidden unit. again this clearly leaves the network input output mapping function unchanged but it corresponds to a different choice of weight vector. for m hidden units any given weight vector will belong to a set of m! equivalent weight vectors associated with this interchange symmetry corresponding to the m! different orderings of the hidden units. the network will therefore have an overall weight-space symmetry factor of for networks with more than two layers of weights the total level of symmetry will be given by the product of such factors one for each layer of hidden units. it turns out that these factors account for all of the symmetries in weight space for possible accidental symmetries due to specific choices for the weight values. furthermore the existence of these symmetries is not a particular property of the tanh function but applies to a wide range of activation functions urkov a and kainen in many cases these symmetries in weight space are of little practical consequence although in section we shall encounter a situation in which we need to take them into account. network training so far we have viewed neural networks as a general class of parametric nonlinear functions from a vector x of input variables to a vector y of output variables. a simple approach to the problem of determining the network parameters is to make an analogy with the discussion of polynomial curve fitting in section and therefore to minimize a sum-of-squares error function. given a training set comprising a set of input vectors where n n together with a corresponding set of network training target vectors we minimize the error function ew w however we can provide a much more general view of network training by first giving a probabilistic interpretation to the network outputs. we have already seen many advantages of using probabilistic predictions in section here it will also provide us with a clearer motivation both for the choice of output unit nonlinearity and the choice of error function. we start by discussing regression problems and for the moment we consider a single target variable t that can take any real value. following the discussions in section and we assume that t has a gaussian distribution with an xdependent mean which is given by the output of the neural network so that ptx w tyx w where is the precision variance of the gaussian noise. of course this is a somewhat restrictive assumption and in section we shall see how to extend this approach to allow for more general conditional distributions. for the conditional distribution given by it is sufficient to take the output unit activation function to be the identity because such a network can approximate any continuous function from x to y. given a data set of n independent identically distributed observations x xn along with corresponding target values t tn we can construct the corresponding likelihood function ptnxn w ptx w taking the negative logarithm we obtain the error function w n ln n which can be used to learn the parameters w and in section we shall discuss the bayesian treatment of neural networks while here we consider a maximum likelihood approach. note that in the neural networks literature it is usual to consider the minimization of an error function rather than the maximization of the likelihood and so here we shall follow this convention. consider first the determination of w. maximizing the likelihood function is equivalent to minimizing the sum-of-squares error function given by ew w neural networks where we have discarded additive and multiplicative constants. the value of w found by minimizing ew will be denoted wml because it corresponds to the maximum likelihood solution. in practice the nonlinearity of the network function yxn w causes the error ew to be nonconvex and so in practice local maxima of the likelihood may be found corresponding to local minima of the error function as discussed in section having found wml the value of can be found by minimizing the negative log likelihood to give ml n wml note that this can be evaluated once the iterative optimization required to find wml is completed. if we have multiple target variables and we assume that they are independent conditional on x and w with shared noise precision then the conditional distribution of the target values is given by tyx w exercise following the same argument as for a single target variable we see that the maximum likelihood weights are determined by minimizing the sum-of-squares error function the noise precision is then given by ptx w ml n k wml exercise where k is the number of target variables. the assumption of independence can be dropped at the expense of a slightly more complex optimization problem. recall from section that there is a natural pairing of the error function by the negative log likelihood and the output unit activation function. in the regression case we can view the network as having an output activation function that is the identity so that yk ak. the corresponding sum-of-squares error function has the property yk tk e ak which we shall make use of when discussing error backpropagation in section now consider the case of binary classification in which we have a single target variable t such that t denotes class and t denotes class following the discussion of canonical link functions in section we consider a network having a single output whose activation function is a logistic sigmoid y so that yx w we can interpret yx w as the conditional probability with given by yx w. the conditional distribution of targets given inputs is then a bernoulli distribution of the form exp a ptx w yx wt yx t network training if we consider a training set of independent observations then the error function which is given by the negative log likelihood is then a cross-entropy error function of the form ln yn tn yn ew exercise where yn denotes yxn w. note that there is no analogue of the noise precision because the target values are assumed to be correctly labelled. however the model is easily extended to allow for labelling errors. simard et al. found that using the cross-entropy error function instead of the sum-of-squares for a classification problem leads to faster training as well as improved generalization. if we have k separate binary classifications to perform then we can use a network having k outputs each of which has a logistic sigmoid activation function. associated with each output is a binary class label tk where k k. if we assume that the class labels are independent given the input vector then the conditional distribution of the targets is ptx w ykx wtk ykx tk exercise exercise ew taking the negative logarithm of the corresponding likelihood function then gives the following error function ln ynk tnk ynk where ynk denotes ykxn w. again the derivative of the error function with respect to the activation for a particular output unit takes the form just as in the regression case. it is interesting to contrast the neural network solution to this problem with the corresponding approach based on a linear classification model of the kind discussed in chapter suppose that we are using a standard two-layer network of the kind shown in figure we see that the weight parameters in the first layer of the network are shared between the various outputs whereas in the linear model each classification problem is solved independently. the first layer of the network can be viewed as performing a nonlinear feature extraction and the sharing of features between the different outputs can save on computation and can also lead to improved generalization. finally we consider the standard multiclass classification problem in which each input is assigned to one of k mutually exclusive classes. the binary target variables tk have a coding scheme indicating the class and the network outputs are interpreted as ykx w ptk leading to the following error function ew tkn ln ykxn w. neural networks figure geometrical view of the error function ew as a surface sitting over weight space. point wa is a local minimum and wb is the global minimum. at any point wc the local gradient of the error surface is given by the vector e. ew wa wb wc e following the discussion of section we see that the output unit activation function which corresponds to the canonical link is given by the softmax function j expakx w expajx w ykx w which satisfies yk and k yk note that the ykx w are unchanged if a constant is added to all of the akx w causing the error function to be constant for some directions in weight space. this degeneracy is removed if an appropriate regularization term is added to the error function. exercise a particular output unit takes the familiar form once again the derivative of the error function with respect to the activation for in summary there is a natural choice of both output unit activation function and matching error function according to the type of problem being solved. for regression we use linear outputs and a sum-of-squares error for independent binary classifications we use logistic sigmoid outputs and a cross-entropy error function and for multiclass classification we use softmax outputs with the corresponding multiclass cross-entropy error function. for classification problems involving two classes we can use a single logistic sigmoid output or alternatively we can use a network with two outputs having a softmax output activation function. parameter optimization we turn next to the task of finding a weight vector w which minimizes the chosen function ew. at this point it is useful to have a geometrical picture of the error function which we can view as a surface sitting over weight space as shown in figure first note that if we make a small step in weight space from w to w w then the change in the error function is e wt ew where the vector ew points in the direction of greatest rate of increase of the error function. because the error ew is a smooth continuous function of w its smallest value will occur at a section point in weight space such that the gradient of the error function vanishes so that network training ew as otherwise we could make a small step in the direction of ew and thereby further reduce the error. points at which the gradient vanishes are called stationary points and may be further classified into minima maxima and saddle points. our goal is to find a vector w such that ew takes its smallest value. however the error function typically has a highly nonlinear dependence on the weights and bias parameters and so there will be many points in weight space at which the gradient vanishes is numerically very small. indeed from the discussion in section we see that for any point w that is a local minimum there will be other points in weight space that are equivalent minima. for instance in a two-layer network of the kind shown in figure with m hidden units each point in weight space is a member of a family of equivalent points. furthermore there will typically be multiple inequivalent stationary points and in particular multiple inequivalent minima. a minimum that corresponds to the smallest value of the error function for any weight vector is said to be a global minimum. any other minima corresponding to higher values of the error function are said to be local minima. for a successful application of neural networks it may not be necessary to find the global minimum in general it will not be known whether the global minimum has been found but it may be necessary to compare several local minima in order to find a sufficiently good solution. because there is clearly no hope of finding an analytical solution to the equation ew we resort to iterative numerical procedures. the optimization of continuous nonlinear functions is a widely studied problem and there exists an extensive literature on how to solve it efficiently. most techniques involve choosing some initial value for the weight vector and then moving through weight space in a succession of steps of the form w w w where labels the iteration step. different algorithms involve different choices for the weight vector update w many algorithms make use of gradient information and therefore require that after each update the value of ew is evaluated at the new weight vector w in order to understand the importance of gradient information it is useful to consider a local approximation to the error function based on a taylor expansion. local quadratic approximation insight into the optimization problem and into the various techniques for solving it can be obtained by considering a local quadratic approximation to the error function. consider the taylor expansion of ew around some in weight space ew neural networks where cubic and higher terms have been omitted. here b is defined to be the gradient of e evaluated b ewbw and the hessian matrix h e has elements e wi wj e b hw wbw from the corresponding local approximation to the gradient is given by for points w that are sufficiently close to these expressions will give reasonable approximations for the error and its gradient. consider the particular case of a local quadratic approximation around a point that is a minimum of the error function. in this case there is no linear term because e at and becomes ew where the hessian h is evaluated at in order to interpret this geometrically consider the eigenvalue equation for the hessian matrix hui iui where the eigenvectors ui form a complete orthonormal set c so that we now expand as a linear combination of the eigenvectors in the form ut i uj ij. w iui. i this can be regarded as a transformation of the coordinate system in which the origin is translated to the point and the axes are rotated to align with the eigenvectors the orthogonal matrix whose columns are the ui and is discussed in more detail in appendix c. substituting into and using and allows the error function to be written in the form ew i i i a matrix h is said to be positive definite if and only if vthv for all v. the error figure in the neighbourhood of a minimum function can be approximated by a quadratic. contours of constant error are then ellipses whose axes are aligned with the eigenvectors ui of the hessian matrix with lengths that are inversely proportional to the square roots of the corresponding eigenvectors i. network training because the eigenvectors form a complete set an arbitrary vector v can be written in the form from and we then have v ciui. i i vthv i i exercise exercise and so h will be positive definite if and only if all of its eigenvalues are positive. in the new coordinate system whose basis vectors are given by the eigenvectors the contours of constant e are ellipses centred on the origin as illustrated in figure for a one-dimensional weight space a stationary point will be a minimum if exercise the corresponding result in d-dimensions is that the hessian matrix evaluated at should be positive definite. use of gradient information as we shall see in section it is possible to evaluate the gradient of an error function efficiently by means of the backpropagation procedure. the use of this gradient information can lead to significant improvements in the speed with which the minima of the error function can be located. we can see why this is so as follows. in the quadratic approximation to the error function given in the error surface is specified by the quantities b and h which contain a total of w independent elements the matrix h is symmetric where w is the dimensionality of w the total number of adaptive parameters in the network. the location of the minimum of this quadratic approximation therefore depends on ow parameters and we should not expect to be able to locate the minimum until we have gathered ow independent pieces of information. if we do not make use of gradient information we would expect to have to perform ow function exercise neural networks evaluations each of which would require ow steps. thus the computational effort needed to find the minimum using such an approach would be ow now compare this with an algorithm that makes use of the gradient information. because each evaluation of e brings w items of information we might hope to find the minimum of the function in ow gradient evaluations. as we shall see by using error backpropagation each such evaluation takes only ow steps and so the minimum can now be found in ow steps. for this reason the use of gradient information forms the basis of practical algorithms for training neural networks. w w ew gradient descent optimization the simplest approach to using gradient information is to choose the weight update in to comprise a small step in the direction of the negative gradient so that where the parameter is known as the learning rate. after each such update the gradient is re-evaluated for the new weight vector and the process repeated. note that the error function is defined with respect to a training set and so each step requires that the entire training set be processed in order to evaluate e. techniques that use the whole data set at once are called batch methods. at each step the weight vector is moved in the direction of the greatest rate of decrease of the error function and so this approach is known as gradient descent or steepest descent. although such an approach might intuitively seem reasonable in fact it turns out to be a poor algorithm for reasons discussed in bishop and nabney for batch optimization there are more efficient methods such as conjugate gradients and quasi-newton methods which are much more robust and much faster than simple gradient descent et al. fletcher nocedal and wright unlike gradient descent these algorithms have the property that the error function always decreases at each iteration unless the weight vector has arrived at a local or global minimum. in order to find a sufficiently good minimum it may be necessary to run a gradient-based algorithm multiple times each time using a different randomly chosen starting point and comparing the resulting performance on an independent validation set. there is however an on-line version of gradient descent that has proved useful in practice for training neural networks on large data sets cun et al. error functions based on maximum likelihood for a set of independent observations comprise a sum of terms one for each data point ew enw. on-line gradient descent also known as sequential gradient descent or stochastic gradient descent makes an update to the weight vector based on one data point at a time so that w w enw error backpropagation this update is repeated by cycling through the data either in sequence or by selecting points at random with replacement. there are of course intermediate scenarios in which the updates are based on batches of data points. one advantage of on-line methods compared to batch methods is that the former handle redundancy in the data much more efficiently. to see this consider an extreme example in which we take a data set and double its size by duplicating every data point. note that this simply multiplies the error function by a factor of and so is equivalent to using the original error function. batch methods will require double the computational effort to evaluate the batch error function gradient whereas online methods will be unaffected. another property of on-line gradient descent is the possibility of escaping from local minima since a stationary point with respect to the error function for the whole data set will generally not be a stationary point for each data point individually. nonlinear optimization algorithms and their practical application to neural net work training are discussed in detail in bishop and nabney error backpropagation our goal in this section is to find an efficient technique for evaluating the gradient of an error function ew for a feed-forward neural network. we shall see that this can be achieved using a local message passing scheme in which information is sent alternately forwards and backwards through the network and is known as error backpropagation or sometimes simply as backprop. it should be noted that the term backpropagation is used in the neural computing literature to mean a variety of different things. for instance the multilayer perceptron architecture is sometimes called a backpropagation network. the term backpropagation is also used to describe the training of a multilayer perceptron using gradient descent applied to a sum-of-squares error function. in order to clarify the terminology it is useful to consider the nature of the training process more carefully. most training algorithms involve an iterative procedure for minimization of an error function with adjustments to the weights being made in a sequence of steps. at each such step we can distinguish between two distinct stages. in the first stage the derivatives of the error function with respect to the weights must be evaluated. as we shall see the important contribution of the backpropagation technique is in providing a computationally efficient method for evaluating such derivatives. because it is at this stage that errors are propagated backwards through the network we shall use the term backpropagation specifically to describe the evaluation of derivatives. in the second stage the derivatives are then used to compute the adjustments to be made to the weights. the simplest such technique and the one originally considered by rumelhart et al. involves gradient descent. it is important to recognize that the two stages are distinct. thus the first stage namely the propagation of errors backwards through the network in order to evaluate derivatives can be applied to many other kinds of network and not just the multilayer perceptron. it can also be applied to error functions other that just the simple sum-of-squares and to the eval neural networks uation of other derivatives such as the jacobian and hessian matrices as we shall see later in this chapter. similarly the second stage of weight adjustment using the calculated derivatives can be tackled using a variety of optimization schemes many of which are substantially more powerful than simple gradient descent. evaluation of error-function derivatives we now derive the backpropagation algorithm for a general network having arbitrary feed-forward topology arbitrary differentiable nonlinear activation functions and a broad class of error function. the resulting formulae will then be illustrated using a simple layered network structure having a single layer of sigmoidal hidden units together with a sum-of-squares error. many error functions of practical interest for instance those defined by maximum likelihood for a set of i.i.d. data comprise a sum of terms one for each data point in the training set so that i ew here we shall consider the problem of evaluating enw for one such term in the error function. this may be used directly for sequential optimization or the results can be accumulated over the training set in the case of batch methods. enw. consider first a simple linear model in which the outputs yk are linear combina tions of the input variables xi so that yk wkixi together with an error function that for a particular input pattern n takes the form en k tnjxni en wji where ynk ykxn w. the gradient of this error function with respect to a weight wji is given by which can be interpreted as a local computation involving the product of an error signal ynj tnj associated with the output end of the link wji and the variable xni associated with the input end of the link. in section we saw how a similar formula arises with the logistic sigmoid activation function together with the cross entropy error function and similarly for the softmax activation function together with its matching cross-entropy error function. we shall now see how this simple result extends to the more complex setting of multilayer feed-forward networks. in a general feed-forward network each unit computes a weighted sum of its inputs of the form aj wjizi i error backpropagation where zi is the activation of a unit or input that sends a connection to unit j and wji is the weight associated with that connection. in section we saw that biases can be included in this sum by introducing an extra unit or input with activation fixed at we therefore do not need to deal with biases explicitly. the sum in is transformed by a nonlinear activation function h to give the activation zj of unit j in the form zj haj. note that one or more of the variables zi in the sum in could be an input and similarly the unit j in could be an output. for each pattern in the training set we shall suppose that we have supplied the corresponding input vector to the network and calculated the activations of all of the hidden and output units in the network by successive application of and this process is often called forward propagation because it can be regarded as a forward flow of information through the network. now consider the evaluation of the derivative of en with respect to a weight wji. the outputs of the various units will depend on the particular input pattern n. however in order to keep the notation uncluttered we shall omit the subscript n from the network variables. first we note that en depends on the weight wji only via the summed input aj to unit j. we can therefore apply the chain rule for partial derivatives to give we now introduce a useful notation j en aj en wji en aj aj wji where the s are often referred to as errors for reasons we shall see shortly. using we can write aj wji zi. substituting and into we then obtain en wji jzi. equation tells us that the required derivative is obtained simply by multiplying the value of for the unit at the output end of the weight by the value of z for the unit at the input end of the weight z in the case of a bias. note that this takes the same form as for the simple linear model considered at the start of this section. thus in order to evaluate the derivatives we need only to calculate the value of j for each hidden and output unit in the network and then apply as we have seen already for the output units we have k yk tk neural networks figure illustration of the calculation of j for hidden unit j by backpropagation of the s from those units k to which unit j sends connections. the blue arrow denotes the direction of information flow during forward propagation and the red arrows indicate the backward propagation of error information. zi wji j zj wkj k provided we are using the canonical link as the output-unit activation function. to evaluate the s for hidden units we again make use of the chain rule for partial derivatives j en aj en ak ak aj k where the sum runs over all units k to which unit j sends connections. the arrangement of units and weights is illustrated in figure note that the units labelled k could include other hidden units andor output units. in writing down we are making use of the fact that variations in aj give rise to variations in the error function only through variations in the variables ak. if we now substitute the definition of given by into and make use of and we obtain the following backpropagation formula j h wkj k k which tells us that the value of for a particular hidden unit can be obtained by propagating the s backwards from units higher up in the network as illustrated in figure note that the summation in is taken over the first index on wkj to backward propagation of information through the network whereas in the forward propagation equation it is taken over the second index. because we already know the values of the s for the output units it follows that by recursively applying we can evaluate the s for all of the hidden units in a feed-forward network regardless of its topology. the backpropagation procedure can therefore be summarized as follows. error backpropagation apply an input vector xn to the network and forward propagate through the network using and to find the activations of all the hidden and output units. evaluate the k for all the output units using backpropagate the s using to obtain j for each hidden unit in the network. use to evaluate the required derivatives. error backpropagation for batch methods the derivative of the total error e can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns e wji en wji n in the above derivation we have implicitly assumed that each hidden or output unit in the network has the same activation function h the derivation is easily generalized however to allow different units to have individual activation functions simply by keeping track of which form of h goes with which unit. a simple example the above derivation of the backpropagation procedure allowed for general forms for the error function the activation functions and the network topology. in order to illustrate the application of this algorithm we shall consider a particular example. this is chosen both for its simplicity and for its practical importance because many applications of neural networks reported in the literature make use of this type of network. specifically we shall consider a two-layer network of the form illustrated in figure together with a sum-of-squares error in which the output units have linear activation functions so that yk ak while the hidden units have logistic sigmoid activation functions given by where a useful feature of this function is that its derivative can be expressed in a par ticularly simple form we also consider a standard sum-of-squares error function so that for pattern n the error is given by h where yk is the activation of output unit k and tk is the corresponding target for a particular input pattern xn. for each pattern in the training set in turn we first perform a forward propagation using aj ha tanha tanha ea e a ea e a en ji xi w zj tanhaj yk w kj zj. neural networks next we compute the s for each output unit using k yk tk. then we backpropagate these to obtain s for the hidden units using j j wkj k. finally the derivatives with respect to the first-layer and second-layer weights are given by en ji w jxi en kj w kzj. efficiency of backpropagation one of the most important aspects of backpropagation is its computational efficiency. to understand this let us examine how the number of computer operations required to evaluate the derivatives of the error function scales with the total number w of weights and biases in the network. a single evaluation of the error function a given input pattern would require ow operations for sufficiently large w this follows from the fact that except for a network with very sparse connections the number of weights is typically much greater than the number of units and so the bulk of the computational effort in forward propagation is concerned with evaluating the sums in with the evaluation of the activation functions representing a small overhead. each term in the sum in requires one multiplication and one addition leading to an overall computational cost that is ow an alternative approach to backpropagation for computing the derivatives of the error function is to use finite differences. this can be done by perturbing each weight in turn and approximating the derivatives by the expression enwji enwji en wji where in a software simulation the accuracy of the approximation to the derivatives can be improved by making smaller until numerical roundoff problems arise. the accuracy of the finite differences method can be improved significantly by using symmetrical central differences of the form o enwji enwji en wji exercise in this case the o corrections cancel as can be verified by taylor expansion on the right-hand side of and so the residual corrections are the number of computational steps is however roughly doubled compared with the main problem with numerical differentiation is that the highly desirable ow scaling has been lost. each forward propagation requires ow steps and error backpropagation figure illustration of a modular pattern recognition system in which the jacobian matrix can be used to backpropagate error signals from the outputs through to earlier modules in the system. u x v z w y there are w weights in the network each of which must be perturbed individually so that the overall scaling is ow however numerical differentiation plays an important role in practice because a comparison of the derivatives calculated by backpropagation with those obtained using central differences provides a powerful check on the correctness of any software implementation of the backpropagation algorithm. when training networks in practice derivatives should be evaluated using backpropagation because this gives the greatest accuracy and numerical efficiency. however the results should be compared with numerical differentiation using for some test cases in order to check the correctness of the implementation. the jacobian matrix we have seen how the derivatives of an error function with respect to the weights can be obtained by the propagation of errors backwards through the network. the technique of backpropagation can also be applied to the calculation of other derivatives. here we consider the evaluation of the jacobian matrix whose elements are given by the derivatives of the network outputs with respect to the inputs jki yk xi where each such derivative is evaluated with all other inputs held fixed. jacobian matrices play a useful role in systems built from a number of distinct modules as illustrated in figure each module can comprise a fixed or adaptive function which can be linear or nonlinear so long as it is differentiable. suppose we wish to minimize an error function e with respect to the parameter w in figure the derivative of the error function is given by kj e w e yk yk zj zj w in which the jacobian matrix for the red module in figure appears in the middle term. because the jacobian matrix provides a measure of the local sensitivity of the outputs to changes in each of the input variables it also allows any known errors xi neural networks associated with the inputs to be propagated through the trained network in order to estimate their contribution yk to the errors at the outputs through the relation yk yk xi xi which is valid provided the xi are small. in general the network mapping represented by a trained neural network will be nonlinear and so the elements of the jacobian matrix will not be constants but will depend on the particular input vector used. thus is valid only for small perturbations of the inputs and the jacobian itself must be re-evaluated for each new input vector. i the jacobian matrix can be evaluated using a backpropagation procedure that is similar to the one derived earlier for evaluating the derivatives of an error function with respect to the weights. we start by writing the element jki in the form j j jki yk xi yk aj aj xi wji yk aj where we have made use of the sum in runs over all units j to which the input unit i sends connections example over all units in the first hidden layer in the layered topology considered earlier. we now write down a recursive backpropagation formula to determine the derivatives yk aj yk aj al aj yk al l l h wlj yk al where the sum runs over all units l to which unit j sends connections to the first index of wlj. again we have made use of and this backpropagation starts at the output units for which the required derivatives can be found directly from the functional form of the output-unit activation function. for instance if we have individual sigmoidal activation functions at each output unit then yk aj whereas for softmax outputs we have kj kjyk ykyj. yk aj we can summarize the procedure for evaluating the jacobian matrix as follows. apply the input vector corresponding to the point in input space at which the jacobian matrix is to be found and forward propagate in the usual way to obtain the the hessian matrix activations of all of the hidden and output units in the network. next for each row k of the jacobian matrix corresponding to the output unit k backpropagate using the recursive relation starting with or for all of the hidden units in the network. finally use to do the backpropagation to the inputs. the jacobian can also be evaluated using an alternative forward propagation formalism which can be derived in an analogous way to the backpropagation approach given here. again the implementation of such algorithms can be checked by using numeri exercise cal differentiation in the form ykxi ykxi yk xi which involves forward propagations for a network having d inputs. the hessian matrix we have shown how the technique of backpropagation can be used to obtain the first derivatives of an error function with respect to the weights in the network. backpropagation can also be used to evaluate the second derivatives of the error given by wji wlk note that it is sometimes convenient to consider all of the weight and bias parameters as elements wi of a single vector denoted w in which case the second derivatives form the elements hij of the hessian matrix h where i j w and w is the total number of weights and biases. the hessian plays an important role in many aspects of neural computing including the following several nonlinear optimization algorithms used for training neural networks are based on considerations of the second-order properties of the error surface which are controlled by the hessian matrix and nabney the hessian forms the basis of a fast procedure for re-training a feed-forward network following a small change in the training data the inverse of the hessian has been used to identify the least significant weights in a network as part of network pruning algorithms cun et al. the hessian plays a central role in the laplace approximation for a bayesian neural network section its inverse is used to determine the predictive distribution for a trained network its eigenvalues determine the values of hyperparameters and its determinant is used to evaluate the model evidence. various approximation schemes have been used to evaluate the hessian matrix for a neural network. however the hessian can also be calculated exactly using an extension of the backpropagation technique. neural networks an important consideration for many applications of the hessian is the efficiency with which it can be evaluated. if there are w parameters and biases in the network then the hessian matrix has dimensions w w and so the computational effort needed to evaluate the hessian will scale like ow for each pattern in the data set. as we shall see there are efficient methods for evaluating the hessian whose scaling is indeed ow diagonal approximation some of the applications for the hessian matrix discussed above require the inverse of the hessian rather than the hessian itself. for this reason there has been some interest in using a diagonal approximation to the hessian in other words one that simply replaces the off-diagonal elements with zeros because its inverse is trivial to evaluate. again we shall consider an error function that consists of a sum of terms one for each pattern in the data set so that e n en. the hessian can then be obtained by considering one pattern at a time and then summing the results over all patterns. from the diagonal elements of the hessian for pattern n can be written ji j i using and the second derivatives on the right-hand side of can be found recursively using the chain rule of differential calculus to give a backpropagation equation of the form j h ak h wkj en ak if we now neglect off-diagonal elements in the second-derivative terms we obtain and le cun le cun et al. k k k k j h kj k h wkj en ak note that the number of computational steps required to evaluate this approximation is ow where w is the total number of weight and bias parameters in the network compared with ow for the full hessian. ricotti et al. also used the diagonal approximation to the hessian but they retained all terms in the evaluation of j and so obtained exact expressions for the diagonal terms. note that this no longer has ow scaling. the major problem with diagonal approximations however is that in practice the hessian is typically found to be strongly nondiagonal and so these approximations which are driven mainly be computational convenience must be treated with care. the hessian matrix outer product approximation when neural networks are applied to regression problems it is common to use a sum-of-squares error function of the form e exercise an analogous result can be obtained for multiclass networks having softmax outputunit activation functions. ynbnbt n. exercise exercise exercise where we have considered the case of a single output in order to keep the notation simple extension to several outputs is straightforward. we can then write the hessian matrix in the form h e yn yn tn yn. if the network has been trained on the data set and its outputs yn happen to be very close to the target values tn then the second term in will be small and can be neglected. more generally however it may be appropriate to neglect this term by the following argument. recall from section that the optimal function that minimizes a sum-of-squares loss is the conditional average of the target data. the quantity tn is then a random variable with zero mean. if we assume that its value is uncorrelated with the value of the second derivative term on the right-hand side of then the whole term will average to zero in the summation over n. by neglecting the second term in we arrive at the levenberg marquardt approximation or outer product approximation the hessian matrix is built up from a sum of outer products of vectors given by bnbt n where bn yn an because the activation function for the output units is simply the identity. evaluation of the outer product approximation for the hessian is straightforward as it only involves first derivatives of the error function which can be evaluated efficiently in ow steps using standard backpropagation. the elements of the matrix can then be found in ow steps by simple multiplication. it is important to emphasize that this approximation is only likely to be valid for a network that has been trained appropriately and that for a general network mapping the second derivative terms on the right-hand side of will typically not be negligible. in the case of the cross-entropy error function for a network with logistic sigmoid output-unit activation functions the corresponding approximation is given by h h neural networks inverse hessian we can use the outer-product approximation to develop a computationally efficient procedure for approximating the inverse of the hessian and stork first we write the outer-product approximation in matrix notation as bnbt n hn where bn wan is the contribution to the gradient of the output unit activation arising from data point n. we now derive a sequential procedure for building up the hessian by including data points one at a time. suppose we have already obtained the inverse hessian using the first l data points. by separating off the contribution from data point l we obtain in order to evaluate the inverse of the hessian we now consider the matrix identity m vvt hl m vtm vtm where i is the unit matrix which is simply a special case of the woodbury identity if we now identify hl with m and with v we obtain h h l l h l l bt exercise in this way data points are sequentially absorbed until n and the whole data set has been processed. this result therefore represents a procedure for evaluating the inverse of the hessian using a single pass through the data set. the initial matrix is chosen to be i where is a small quantity so that the algorithm actually finds the inverse of h i. the results are not particularly sensitive to the precise value of extension of this algorithm to networks having more than one output is straightforward. we note here that the hessian matrix can sometimes be calculated indirectly as part of the network training algorithm. in particular quasi-newton nonlinear optimization algorithms gradually build up an approximation to the inverse of the hessian during training. such algorithms are discussed in detail in bishop and nabney finite differences as in the case of the first derivatives of the error function we can find the second derivatives by using finite differences with accuracy limited by numerical precision. if we perturb each possible pair of weights in turn we obtain wji wlk wlk ewji wlk ewji wlk ewji wlk the hessian matrix again by using a symmetrical central differences formulation we ensure that the residual errors are rather than o. because there are w elements in the hessian matrix and because the evaluation of each element requires four forward propagations each needing ow operations pattern we see that this approach will require ow operations to evaluate the complete hessian. it therefore has poor scaling properties although in practice it is very useful as a check on the software implementation of backpropagation methods. a more efficient version of numerical differentiation can be found by applying central differences to the first derivatives of the error function which are themselves calculated using backpropagation. this gives wji wlk e wji e wji because there are now only w weights to be perturbed and because the gradients can be evaluated in ow steps we see that this method gives the hessian in ow operations. exact evaluation of the hessian so far we have considered various approximation schemes for evaluating the hessian matrix or its inverse. the hessian can also be evaluated exactly for a network of arbitrary feed-forward topology using extension of the technique of backpropagation used to evaluate first derivatives which shares many of its desirable features including computational efficiency bishop it can be applied to any differentiable error function that can be expressed as a function of the network outputs and to networks having arbitrary differentiable activation functions. the number of computational steps needed to evaluate the hessian scales like ow similar algorithms have also been considered by buntine and weigend here we consider the specific case of a network having two layers of weights for which the required equations are easily derived. we shall use indices i and i to to denote inputs indices j and j denote outputs. we first define to denoted hidden units and indices k and k k en ak ak where en is the contribution to the error from data point n. the hessian matrix for this network can then be considered in three separate blocks as follows. both weights in the second layer exercise kj w w neural networks both weights in the first layer w ji w one weight in each layer ji w w xih zj w k k k w kj w exercise here is the j j element of the identity matrix. if one or both of the weights is a bias term then the corresponding expressions are obtained simply by setting the appropriate activations to inclusion of skip-layer connections is straightforward. fast multiplication by the hessian for many applications of the hessian the quantity of interest is not the hessian matrix h itself but the product of h with some vector v. we have seen that the evaluation of the hessian takes ow operations and it also requires storage that is ow the vector vth that we wish to calculate however has only w elements so instead of computing the hessian as an intermediate step we can instead try to find an efficient approach to evaluating vth directly in a way that requires only ow operations. to do this we first note that vth vt e where denotes the gradient operator in weight space. we can then write down the standard forward-propagation and backpropagation equations for the evaluation of e and apply to these equations to give a set of forward-propagation and backpropagation equations for the evaluation of vth ller pearlmutter this corresponds to acting on the original forward-propagation and backpropagation equations with a differential operator vt pearlmutter used the notation r to denote the operator vt and we shall follow this convention. the analysis is straightforward and makes use of the usual rules of differential calculus together with the result rw v. the technique is best illustrated with a simple example and again we choose a two-layer network of the form shown in figure with linear output units and a sum-of-squares error function. as before we consider the contribution to the error function from one pattern in the data set. the required vector is then obtained as the hessian matrix usual by summing over the contributions from each of the patterns separately. for the two-layer network the forward-propagation equations are given by i j aj wjixi zj haj yk wkjzj. we now act on these equations using the r operator to obtain a set of forward propagation equations in the form raj rzj h ryk wkjrzj vkjzj vjixi i j j where vji is the element of the vector v that corresponds to the weight wji. quantities of the form rzj raj and ryk are to be regarded as new variables whose values are found using the above equations. because we are considering a sum-of-squares error function we have the fol lowing standard backpropagation expressions k yk tk j h wkj k. again we act on these equations with the r operator to obtain a set of backpropagation equations in the form k r k ryk r j h wkj k k vkj k h h k wkjr k. k finally we have the usual equations for the first derivatives of the error e wkj e wji kzj jxi neural networks and acting on these with the r operator we obtain expressions for the elements of the vector vth r kzj krzj xir j. r r e wkj e wji the implementation of this algorithm involves the introduction of additional variables raj rzj and r j for the hidden units and r k and ryk for the output units. for each input pattern the values of these quantities can be found using the above results and the elements of vth are then given by and an elegant aspect of this technique is that the equations for evaluating vth mirror closely those for standard forward and backward propagation and so the extension of existing software to compute this product is typically straightforward. if desired the technique can be used to evaluate the full hessian matrix by choosing the vector v to be given successively by a series of unit vectors of the form each of which picks out one column of the hessian. this leads to a formalism that is analytically equivalent to the backpropagation procedure of bishop as described in section though with some loss of efficiency due to redundant calculations. regularization in neural networks the number of input and outputs units in a neural network is generally determined by the dimensionality of the data set whereas the number m of hidden units is a free parameter that can be adjusted to give the best predictive performance. note that m controls the number of parameters and biases in the network and so we might expect that in a maximum likelihood setting there will be an optimum value of m that gives the best generalization performance corresponding to the optimum balance between under-fitting and over-fitting. figure shows an example of the effect of different values of m for the sinusoidal regression problem. the generalization error however is not a simple function of m due to the presence of local minima in the error function as illustrated in figure here we see the effect of choosing multiple random initializations for the weight vector for a range of values of m. the overall best validation set performance in this case occurred for a particular solution having m in practice one approach to choosing m is in fact to plot a graph of the kind shown in figure and then to choose the specific solution having the smallest validation set error. there are however other ways to control the complexity of a neural network model in order to avoid over-fitting. from our discussion of polynomial curve fitting in chapter we see that an alternative approach is to choose a relatively large value for m and then to control complexity by the addition of a regularization term to the error function. the simplest regularizer is the quadratic giving a regularized error regularization in neural networks m m m figure examples of two-layer networks trained on data points drawn from the sinusoidal data set. the graphs show the result of fitting networks having m and hidden units respectively by minimizing a sum-of-squares error function using a scaled conjugate-gradient algorithm. of the form ew wtw. this regularizer is also known as weight decay and has been discussed at length in chapter the effective model complexity is then determined by the choice of the regularization coefficient as we have seen previously this regularizer can be interpreted as the negative logarithm of a zero-mean gaussian prior distribution over the weight vector w. consistent gaussian priors one of the limitations of simple weight decay in the form is that is inconsistent with certain scaling properties of network mappings. to illustrate this consider a multilayer perceptron network having two layers of weights and linear output units which performs a mapping from a set of input variables to a set of output variables the activations of the hidden units in the first hidden layer figure plot of the sum-of-squares test-set error for the polynomial data set versus the number of hidden units in the network with random starts for each network size showing the effect of local minima. for each new start the weight vector was initialized by sampling from an isotropic gaussian distribution having a mean of zero and a variance of neural networks take the form zj h wjixi while the activations of the output units are given by i yk wkjzj suppose we perform a linear transformation of the input data of the form j xi axi b. exercise then we can arrange for the mapping performed by the network to be unchanged by making a corresponding linear transformation of the weights and biases from the inputs to the units in the hidden layer of the form wji. a a wji wji b yk cyk d wkj cwkj d. i similarly a linear transformation of the output variables of the network of the form can be achieved by making a transformation of the second-layer weights and biases using if we train one network using the original data and one network using data for which the input andor target variables are transformed by one of the above linear transformations then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. any regularizer should be consistent with this property otherwise it arbitrarily favours one solution over another equivalent one. clearly simple weight decay that treats all weights and biases on an equal footing does not satisfy this property. we therefore look for a regularizer which is invariant under the linear transformations and these require that the regularizer should be invariant to re-scaling of the weights and to shifts of the biases. such a regularizer is given by where denotes the set of weights in the first layer denotes the set of weights in the second layer and biases are excluded from the summations. this regularizer w w pw exp k k j j wk regularization in neural networks will remain unchanged under the weight transformations provided the regularization parameters are re-scaled using and c the regularizer corresponds to a prior of the form pw exp w w note that priors of this form are improper cannot be normalized because the bias parameters are unconstrained. the use of improper priors can lead to difficulties in selecting regularization coefficients and in model comparison within the bayesian framework because the corresponding evidence is zero. it is therefore common to include separate priors for the biases then break shift invariance having their own hyperparameters. we can illustrate the effect of the resulting four hyperparameters by drawing samples from the prior and plotting the corresponding network functions as shown in figure any number of groups wk so that more generally we can consider priors in which the weights are divided into where k as a special case of this prior if we choose the groups to correspond to the sets of weights associated with each of the input units and we optimize the marginal likelihood with respect to the corresponding parameters k we obtain automatic relevance determination as discussed in section early stopping an alternative to regularization as a way of controlling the effective complexity of a network is the procedure of early stopping. the training of nonlinear network models corresponds to an iterative reduction of the error function defined with respect to a set of training data. for many of the optimization algorithms used for network training such as conjugate gradients the error is a nonincreasing function of the iteration index. however the error measured with respect to independent data generally called a validation set often shows a decrease at first followed by an increase as the network starts to over-fit. training can therefore be stopped at the point of smallest error with respect to the validation data set as indicated in figure in order to obtain a network having good generalization performance. the behaviour of the network in this case is sometimes explained qualitatively in terms of the effective number of degrees of freedom in the network in which this number starts out small and then to grows during the training process corresponding to a steady increase in the effective complexity of the model. halting training before neural networks w b w b w b w b w b w b w b w b figure illustration of the effect of the hyperparameters governing the prior distribution over weights and biases in a two-layer network having a single input a single linear output and hidden units having tanh activation functions. the priors are governed by four hyperparameters b which represent the precisions of the gaussian distributions of the first-layer biases first-layer weights second-layer biases and second-layer weights respectively. we see that the parameter w governs the vertical scale of functions the different vertical axis ranges on the top two diagrams w governs the horizontal scale of variations in the function values and b whose effect is not illustrated here governs the range of vertical offsets of the functions. governs the horizontal range over which variations occur. the parameter b and w w b a minimum of the training error has been reached then represents a way of limiting the effective network complexity. in the case of a quadratic error function we can verify this insight and show that early stopping should exhibit similar behaviour to regularization using a simple weight-decay term. this can be understood from figure in which the axes in weight space have been rotated to be parallel to the eigenvectors of the hessian matrix. if in the absence of weight decay the weight vector starts at the origin and proceeds during training along a path that follows the local negative gradient vector then the weight vector will move initially parallel to the axis through a point corresponding roughly to and then move towards the minimum of the error funceigenvalues of the hessian. stopping at a point is therefore similar to weight tion wml. this follows from the shape of the error surface and the widely differing exercise decay. the relationship between early stopping and weight decay can be made quantitative thereby showing that the quantity is the iteration index and is the learning rate parameter plays the role of the reciprocal of the regularization regularization in neural networks figure an illustration of the behaviour of training set error and validation set error during a typical training session as a function of the iteration step for the sinusoidal data set. the goal of achieving the best generalization performance suggests that training should be stopped at the point shown by the vertical dashed lines corresponding to the minimum of the validation set error. parameter the effective number of parameters in the network therefore grows during the course of training. invariances in many applications of pattern recognition it is known that predictions should be unchanged or invariant under one or more transformations of the input variables. for example in the classification of objects in two-dimensional images such as handwritten digits a particular object should be assigned the same classification irrespective of its position within the image invariance or of its size invariance. such transformations produce significant changes in the raw data expressed in terms of the intensities at each of the pixels in the image and yet should give rise to the same output from the classification system. similarly in speech recognition small levels of nonlinear warping along the time axis which preserve temporal ordering should not change the interpretation of the signal. if sufficiently large numbers of training patterns are available then an adaptive model such as a neural network can learn the invariance at least approximately. this involves including within the training set a sufficiently large number of examples of the effects of the various transformations. thus for translation invariance in an image the training set should include examples of objects at many different positions. this approach may be impractical however if the number of training examples is limited or if there are several invariants the number of combinations of transformations grows exponentially with the number of such transformations. we therefore seek alternative approaches for encouraging an adaptive model to exhibit the required invariances. these can broadly be divided into four categories the training set is augmented using replicas of the training patterns transformed according to the desired invariances. for instance in our digit recognition example we could make multiple copies of each example in which the neural networks figure a schematic illustration of why early stopping can give similar results to weight decay in the case of a quadratic error function. the ellipse shows a contour of constant error and wml denotes the minimum of the error function. if the weight vector starts at the origin and moves according to the local negative gradient direction then it will follow the path shown by the curve. by stopping training early a weight vector ew is found that is qualitatively similar to that obtained with a simple weight-decay regularizer and training to the minimum of the regularized error as can be seen by comparing with figure wml digit is shifted to a different position in each image. a regularization term is added to the error function that penalizes changes in the model output when the input is transformed. this leads to the technique of tangent propagation discussed in section invariance is built into the pre-processing by extracting features that are invariant under the required transformations. any subsequent regression or classification system that uses such features as inputs will necessarily also respect these invariances. the final option is to build the invariance properties into the structure of a neural network into the definition of a kernel function in the case of techniques such as the relevance vector machine. one way to achieve this is through the use of local receptive fields and shared weights as discussed in the context of convolutional neural networks in section approach is often relatively easy to implement and can be used to encourage complex invariances such as those illustrated in figure for sequential training algorithms this can be done by transforming each input pattern before it is presented to the model so that if the patterns are being recycled a different transformation from an appropriate distribution is added each time. for batch methods a similar effect can be achieved by replicating each data point a number of times and transforming each copy independently. the use of such augmented data can lead to significant improvements in generalization et al. although it can also be computationally costly. approach leaves the data set unchanged but modifies the error function through the addition of a regularizer. in section we shall show that this approach is closely related to approach regularization in neural networks figure illustration of the synthetic warping of a handwritten digit. the original image is shown on the left. on the right the top row shows three examples of warped digits with the corresponding displacement fields shown on the bottom row. these displacement fields are generated by sampling random displacements x y at each pixel and then smoothing by convolution with gaussians of width and respectively. one advantage of approach is that it can correctly extrapolate well beyond the range of transformations included in the training set. however it can be difficult to find hand-crafted features with the required invariances that do not also discard information that can be useful for discrimination. tangent propagation we can use regularization to encourage models to be invariant to transformations of the input through the technique of tangent propagation et al. consider the effect of a transformation on a particular input vector xn. provided the transformation is continuous as translation or rotation but not mirror reflection for instance then the transformed pattern will sweep out a manifold m within the d-dimensional input space. this is illustrated in figure for the case of d for simplicity. suppose the transformation is governed by a single parameter might be rotation angle for instance. then the subspace m swept out by xn figure illustration of a two-dimensional input space showing the effect of a continuous transformation on a particular input vector xn. a onedimensional transformation parameterized by the continuous variable applied to xn causes it to sweep out a one-dimensional manifold m. locally the effect of the transformation can be approximated by the tangent vector n. n xn m neural networks will be one-dimensional and will be parameterized by let the vector that results from acting on xn by this transformation be denoted by sxn which is defined so that sx x. then the tangent to the curve m is given by the directional derivative s and the tangent vector at the point xn is given by n sxn under a transformation of the input vector the network output vector will in general change. the derivative of output k with respect to is given by yk yk xi xi jki i where jki is the i element of the jacobian matrix j as discussed in section the result can be used to modify the standard error function so as to encourage local invariance in the neighbourhood of the data points by the addition to the original error function e of a regularization function to give a total error function of the form e jnki ni n k where is a regularization coefficient and n k ynk exercise the regularization function will be zero when the network mapping function is invariant under the transformation in the neighbourhood of each pattern vector and the value of the parameter determines the balance between fitting the training data and learning the invariance property. in a practical implementation the tangent vector n can be approximated using finite differences by subtracting the original vector xn from the corresponding vector after transformation using a small value of and then dividing by this is illustrated in figure the regularization function depends on the network weights through the jacobian j. a backpropagation formalism for computing the derivatives of the regularizer with respect to the network weights is easily obtained by extension of the techniques introduced in section if the transformation is governed by l parameters l for the case of translations combined with in-plane rotations in a two-dimensional image then the manifold m will have dimensionality l and the corresponding regularizer is given by the sum of terms of the form one for each transformation. if several transformations are considered at the same time and the network mapping is made invariant to each separately then it will be invariant to combinations of the transformations et al. regularization in neural networks figure illustration showing the original image x of a handwritten digit the tangent vector corresponding to an infinitesimal clockwise rotation the result of adding a small contribution from the tangent vector to the original image giving x with degrees and the true image rotated for comparison. a related technique called tangent distance can be used to build invariance properties into distance-based methods such as nearest-neighbour classifiers et al. training with transformed data we have seen that one way to encourage invariance of a model to a set of transformations is to expand the training set using transformed versions of the original input patterns. here we show that this approach is closely related to the technique of tangent propagation leen as in section we shall consider a transformation governed by a single parameter and described by the function sx with sx x. we shall also consider a sum-of-squares error function. the error function for untransformed inputs can be written the infinite data set limit in the form dx dt e as discussed in section here we have considered a network having a single output in order to keep the notation uncluttered. if we now consider an infinite number of copies of each data point each of which is perturbed by the transformation neural networks in which the parameter is drawn from a distribution p then the error function defined over this expanded data set can be written as dx dt d we now assume that the distribution p has zero mean with small variance so that we are only considering small transformations of the original input vectors. we can then expand the transformation function as a taylor series in powers of to give sx sx sx sx o denotes the second derivative of sx with respect to evaluated at x o where this allows us to expand the model function to give ysx yx t yx yx t yx o substituting into the mean error function and expanding we then have dx dt e t t yxptxpx dx dt e t yx ptxpx dx dt o yx t yx because the distribution of transformations has zero mean we have e also we shall denote e by omitting terms of o the average error function then becomes where e is the original sum-of-squares error and the regularization term takes the form e yx t yx t yx px dx in which we have performed the integration over t. regularization in neural networks we can further simplify this regularization term as follows. in section we saw that the function that minimizes the sum-of-squares error is given by the conditional average etx of the target values t. from we see that the regularized error will equal the unregularized sum-of-squares plus terms which are o and so the network function that minimizes the total error will have the form yx etx o thus to leading order in the first term in the regularizer vanishes and we are left with px dx t yx exercise which is equivalent to the tangent propagation regularizer if we consider the special case in which the transformation of the inputs simply consists of the addition of random noise so that x x then the regularizer takes the form px dx which is known as tikhonov regularization and arsenin bishop derivatives of this regularizer with respect to the network weights can be found using an extended backpropagation algorithm we see that for small noise amplitudes tikhonov regularization is related to the addition of random noise to the inputs which has been shown to improve generalization in appropriate circumstances and dow convolutional networks another approach to creating models that are invariant to certain transformation of the inputs is to build the invariance properties into the structure of a neural network. this is the basis for the convolutional neural network cun et al. lecun et al. which has been widely applied to image data. consider the specific task of recognizing handwritten digits. each input image comprises a set of pixel intensity values and the desired output is a posterior probability distribution over the ten digit classes. we know that the identity of the digit is invariant under translations and scaling as well as rotations. furthermore the network must also exhibit invariance to more subtle transformations such as elastic deformations of the kind illustrated in figure one simple approach would be to treat the image as the input to a fully connected network such as the kind shown in figure given a sufficiently large training set such a network could in principle yield a good solution to this problem and would learn the appropriate invariances by example. however this approach ignores a key property of images which is that nearby pixels are more strongly correlated than more distant pixels. many of the modern approaches to computer vision exploit this property by extracting local features that depend only on small subregions of the image. information from such features can then be merged in later stages of processing in order to detect higher-order features neural networks input image convolutional layer sub-sampling layer figure diagram illustrating part of a convolutional neural network showing a layer of convolutional units followed by a layer of subsampling units. several successive pairs of such layers may be used. and ultimately to yield information about the image as whole. also local features that are useful in one region of the image are likely to be useful in other regions of the image for instance if the object of interest is translated. these notions are incorporated into convolutional neural networks through three mechanisms local receptive fields weight sharing and subsampling. the structure of a convolutional network is illustrated in figure in the convolutional layer the units are organized into planes each of which is called a feature map. units in a feature map each take inputs only from a small subregion of the image and all of the units in a feature map are constrained to share the same weight values. for instance a feature map might consist of units arranged in a grid with each unit taking inputs from a pixel patch of the image. the whole feature map therefore has adjustable weight parameters plus one adjustable bias parameter. input values from a patch are linearly combined using the weights and the bias and the result transformed by a sigmoidal nonlinearity using if we think of the units as feature detectors then all of the units in a feature map detect the same pattern but at different locations in the input image. due to the weight sharing the evaluation of the activations of these units is equivalent to a convolution of the image pixel intensities with a kernel comprising the weight parameters. if the input image is shifted the activations of the feature map will be shifted by the same amount but will otherwise be unchanged. this provides the basis for the invariance of regularization in neural networks the network outputs to translations and distortions of the input image. because we will typically need to detect multiple features in order to build an effective model there will generally be multiple feature maps in the convolutional layer each having its own set of weight and bias parameters. the outputs of the convolutional units form the inputs to the subsampling layer of the network. for each feature map in the convolutional layer there is a plane of units in the subsampling layer and each unit takes inputs from a small receptive field in the corresponding feature map of the convolutional layer. these units perform subsampling. for instance each subsampling unit might take inputs from a unit region in the corresponding feature map and would compute the average of those inputs multiplied by an adaptive weight with the addition of an adaptive bias parameter and then transformed using a sigmoidal nonlinear activation function. the receptive fields are chosen to be contiguous and nonoverlapping so that there are half the number of rows and columns in the subsampling layer compared with the convolutional layer. in this way the response of a unit in the subsampling layer will be relatively insensitive to small shifts of the image in the corresponding regions of the input space. in a practical architecture there may be several pairs of convolutional and subsampling layers. at each stage there is a larger degree of invariance to input transformations compared to the previous layer. there may be several feature maps in a given convolutional layer for each plane of units in the previous subsampling layer so that the gradual reduction in spatial resolution is then compensated by an increasing number of features. the final layer of the network would typically be a fully connected fully adaptive layer with a softmax output nonlinearity in the case of multiclass classification. the whole network can be trained by error minimization using backpropagation to evaluate the gradient of the error function. this involves a slight modification of the usual backpropagation algorithm to ensure that the shared-weight constraints are satisfied. due to the use of local receptive fields the number of weights in the network is smaller than if the network were fully connected. furthermore the number of independent parameters to be learned from the data is much smaller still due to the substantial numbers of constraints on the weights. soft weight sharing one way to reduce the effective complexity of a network with a large number of weights is to constrain weights within certain groups to be equal. this is the technique of weight sharing that was discussed in section as a way of building translation invariance into networks used for image interpretation. it is only applicable however to particular problems in which the form of the constraints can be specified in advance. here we consider a form of soft weight sharing and hinton in which the hard constraint of equal weights is replaced by a form of regularization in which groups of weights are encouraged to have similar values. furthermore the division of weights into groups the mean weight value for each group and the spread of values within the groups are all determined as part of the learning process. exercise neural networks section exercise recall that the simple weight decay regularizer given in can be viewed as the negative log of a gaussian prior distribution over the weights. we can encourage the weight values to form several groups rather than just one group by considering instead a probability distribution that is a mixture of gaussians. the centres and variances of the gaussian components as well as the mixing coefficients will be considered as adjustable parameters to be determined as part of the learning process. thus we have a probability density of the form pw pwi where i jn j j pwi and j are the mixing coefficients. taking the negative logarithm then leads to a regularization function of the form ln jn j j i the total error function is then given by ew where is the regularization coefficient. this error is minimized both with respect to the weights wi and with respect to the parameters j j j of the mixture model. if the weights were constant then the parameters of the mixture model could be determined by using the em algorithm discussed in chapter however the distribution of weights is itself evolving during the learning process and so to avoid numerical instability a joint optimization is performed simultaneously over the weights and the mixture-model parameters. this can be done using a standard optimization algorithm such as conjugate gradients or quasi-newton methods. in order to minimize the total error function it is necessary to be able to evaluate its derivatives with respect to the various adjustable parameters. to do this it is convenient to regard the j as prior probabilities and to introduce the corresponding posterior probabilities which following are given by bayes theorem in the form jw jn j j k kn k k j j e wi jwi j wi the derivatives of the total error function with respect to the weights are then given by regularization in neural networks the effect of the regularization term is therefore to pull each weight towards the centre of the jth gaussian with a force proportional to the posterior probability of that gaussian for the given weight. this is precisely the kind of effect that we are seeking. derivatives of the error with respect to the centres of the gaussians are also j i jwi i wj j exercise easily computed to give which has a simple intuitive interpretation because it pushes j towards an average of the weight values weighted by the posterior probabilities that the respective weight parameters were generated by component j. similarly the derivatives with respect to the variances are given by exercise j i jwi j j which drives j towards the weighted average of the squared deviations of the weights around the corresponding centre j where the weighting coefficients are again given by the posterior probability that each weight is generated by component j. note that in a practical implementation new variables j defined by j exp j are introduced and the minimization is performed with respect to the j. this ensures that the parameters j remain positive. it also has the effect of discouraging pathological solutions in which one or more of the j goes to zero corresponding to a gaussian component collapsing onto one of the weight parameter values. such solutions are discussed in more detail in the context of gaussian mixture models in section for the derivatives with respect to the mixing coefficients j we need to take account of the constraints j i j which follow from the interpretation of the j as prior probabilities. this can be done by expressing the mixing coefficients in terms of a set of auxiliary variables j using the softmax function given by j exp j exp k exercise the derivatives of the regularized error function with respect to the j then take the form neural networks figure the left figure shows a two-link robot arm in which the cartesian coordinates of the end effector are determined uniquely by the two joint angles and and the lengths and of the arms. this is know as the forward kinematics of the arm. in practice we have to find the joint angles that will give rise to a desired end effector position and as shown in the right figure this inversekinematicshas two solutions corresponding to elbow up and elbow down j elbow up j jwi i elbow down we see that j is therefore driven towards the average posterior probability for component j. mixture density networks exercise the goal of supervised learning is to model a conditional distribution ptx which for many simple regression problems is chosen to be gaussian. however practical machine learning problems can often have significantly non-gaussian distributions. these can arise for example with inverse problems in which the distribution can be multimodal in which case the gaussian assumption can lead to very poor predictions. as a simple example of an inverse problem consider the kinematics of a robot arm as illustrated in figure the forward problem involves finding the end effector position given the joint angles and has a unique solution. however in practice we wish to move the end effector of the robot to a specific position and to do this we must set appropriate joint angles. we therefore need to solve the inverse problem which has two solutions as seen in figure forward problems often corresponds to causality in a physical system and generally have a unique solution. for instance a specific pattern of symptoms in the human body may be caused by the presence of a particular disease. in pattern recognition however we typically have to solve an inverse problem such as trying to predict the presence of a disease given a set of symptoms. if the forward problem involves a many-to-one mapping then the inverse problem will have multiple solutions. for instance several different diseases may result in the same symptoms. in the robotics example the kinematics is defined by geometrical equations and the multimodality is readily apparent. however in many machine learning problems the presence of multimodality particularly in problems involving spaces of high dimensionality can be less obvious. for tutorial purposes however we shall consider a simple toy problem for which we can easily visualize the multimodality. data for this problem is generated by sampling a variable x uniformly over the interval to give a set of values and the corresponding target values tn are obtained figure on the left is the data set for a simple forward problem in which the red curve shows the result of fitting a two-layer neural network by minimizing the sum-of-squares error function. the corresponding inverse problem shown on the right is obtained by exchanging the roles of x and t. here the same network trained again by minimizing the sum-of-squares error function gives a very poor fit to the data due to the multimodality of the data set. mixture density networks by computing the function xn xn and then adding uniform noise over the interval the inverse problem is then obtained by keeping the same data points but exchanging the roles of x and t. figure shows the data sets for the forward and inverse problems along with the results of fitting two-layer neural networks having hidden units and a single linear output unit by minimizing a sumof-squares error function. least squares corresponds to maximum likelihood under a gaussian assumption. we see that this leads to a very poor model for the highly non-gaussian inverse problem. we therefore seek a general framework for modelling conditional probability distributions. this can be achieved by using a mixture model for ptx in which both the mixing coefficients as well as the component densities are flexible functions of the input vector x giving rise to the mixture density network. for any given value of x the mixture model provides a general formalism for modelling an arbitrary conditional density function ptx. provided we consider a sufficiently flexible network we then have a framework for approximating arbitrary conditional distributions. here we shall develop the model explicitly for gaussian components so that ptx t kx kx this is an example of a heteroscedastic model since the noise variance on the data is a function of the input vector x. instead of gaussians we can use other distributions for the components such as bernoulli distributions if the target variables are binary rather than continuous. we have also specialized to the case of isotropic covariances for the components although the mixture density network can readily be extended to allow for general covariance matrices by representing the covariances using a cholesky factorization even with isotropic components the conditional distribution ptx does not assume factorization with respect to the components of t contrast to the standard sum-of-squares regression model as a consequence of the mixture distribution. we now take the various parameters of the mixture model namely the mixing kx to be governed by coefficients kx the means kx and the variances neural networks xd ptx m figure the mixturedensitynetwork can represent general conditional probability densities ptx by considering a parametric mixture model for the distribution of t whose parameters are determined by the outputs of a neural network that takes x as its input vector. t the outputs of a conventional neural network that takes x as its input. the structure of this mixture density network is illustrated in figure the mixture density network is closely related to the mixture of experts discussed in section the principle difference is that in the mixture density network the same function is used to predict the parameters of all of the component densities as well as the mixing coefficients and so the nonlinear hidden units are shared amongst the input-dependent functions. the neural network in figure can for example be a two-layer network having sigmoidal tanh hidden units. if there are l components in the mixture model and if t has k components then the network will have l output unit activations denoted by a k that determine the mixing coefficients kx k outputs k that determine the kernel widths kx and l k outputs denoted denoted by a kj that determine the components kjx of the kernel centres kx. the total by a number of network outputs is given by as compared with the usual k outputs for a network which simply predicts the conditional means of the target variables. the mixing coefficients must satisfy the constraints kx kx which can be achieved using a set of softmax outputs expa k expa l kx similarly the variances must satisfy of the exponentials of the corresponding network activations using kx and so can be represented in terms kx expa k. finally because the means kx have real components they can be represented mixture density networks directly by the network output activations kjx a kj. the adaptive parameters of the mixture density network comprise the vector w of weights and biases in the neural network that can be set by maximum likelihood or equivalently by minimizing an error function defined to be the negative logarithm of the likelihood. for independent data this error function takes the form ew ln kxn tn kxn w kxn w where we have made the dependencies on w explicit. in order to minimize the error function we need to calculate the derivatives of the error ew with respect to the components of w. these can be evaluated by using the standard backpropagation procedure provided we obtain suitable expressions for the derivatives of the error with respect to the output-unit activations. these represent error signals for each pattern and for each output unit and can be backpropagated to the hidden units and the error function derivatives evaluated in the usual way. because the error function is composed of a sum of terms one for each training data point we can consider the derivatives for a particular pattern n and then find the derivatives of e by summing over all patterns. because we are dealing with mixture distributions it is convenient to view the mixing coefficients kx as x-dependent prior probabilities and to introduce the corresponding posterior probabilities given by ktx knnk lnnl where nnk denotes n kxn kxn. the derivatives with respect to the network output activations governing the mix exercise ing coefficients are given by en a k k k. similarly the derivatives with respect to the output activations controlling the component means are given by exercise exercise en a kl k kl tl k k k k en a k finally the derivatives with respect to the output activations controlling the component variances are given by neural networks figure plot of the mixing coefficients kx as a function of x for the three kernel functions in a mixture density network trained on the data shown in figure the model has three gaussian components and uses a two-layer multilayer perceptron with five tanh sigmoidal units in the hidden layer and nine outputs to the means and variances of the gaussian components and the mixing coefficients. at both small and large values of x where the conditional probability density of the target data is unimodal only one of the kernels has a high value for its prior probability while at intermediate values of x where the conditional density is trimodal the three mixing coefficients have comparable values. plots of the means kx using the same colour coding as for the mixing coefficients. plot of the contours of the corresponding conditional probability density of the target data for the same mixture density network. the approximate conditional mode shown by the red points of the conditional density. plot of we illustrate the use of a mixture density network by returning to the toy example of an inverse problem shown in figure plots of the mixing coefficients kx the means kx and the conditional density contours corresponding to ptx are shown in figure the outputs of the neural network and hence the parameters in the mixture model are necessarily continuous single-valued functions of the input variables. however we see from figure that the model is able to produce a conditional density that is unimodal for some values of x and trimodal for other values by modulating the amplitudes of the mixing components kx. once a mixture density network has been trained it can predict the conditional density function of the target data for any given value of the input vector. this conditional density represents a complete description of the generator of the data so far as the problem of predicting the value of the output vector is concerned. from this density function we can calculate more specific quantities that may be of interest in different applications. one of the simplest of these is the mean corresponding to the conditional average of the target data and is given by e tptx dt kx kx bayesian neural networks where we have used because a standard network trained by least squares is approximating the conditional mean we see that a mixture density network can reproduce the conventional least-squares result as a special case. of course as we have already noted for a multimodal distribution the conditional mean is of limited value. we can similarly evaluate the variance of the density function about the condi kx kx kx lx lx exercise tional average to give e where we have used and this is more general than the corresponding least-squares result because the variance is a function of x. we have seen that for multimodal distributions the conditional mean can give a poor representation of the data. for instance in controlling the simple robot arm shown in figure we need to pick one of the two possible joint angle settings in order to achieve the desired end-effector location whereas the average of the two solutions is not itself a solution. in such cases the conditional mode may be of more value. because the conditional mode for the mixture density network does not have a simple analytical solution this would require numerical iteration. a simple alternative is to take the mean of the most probable component the one with the largest mixing coefficient at each value of x. this is shown for the toy data set in figure bayesian neural networks so far our discussion of neural networks has focussed on the use of maximum likelihood to determine the network parameters and biases. regularized maximum likelihood can be interpreted as a map posterior approach in which the regularizer can be viewed as the logarithm of a prior parameter distribution. however in a bayesian treatment we need to marginalize over the distribution of parameters in order to make predictions. in section we developed a bayesian solution for a simple linear regression model under the assumption of gaussian noise. we saw that the posterior distribution which is gaussian could be evaluated exactly and that the predictive distribution could also be found in closed form. in the case of a multilayered network the highly nonlinear dependence of the network function on the parameter values means that an exact bayesian treatment can no longer be found. in fact the log of the posterior distribution will be nonconvex corresponding to the multiple local minima in the error function. the technique of variational inference to be discussed in chapter has been applied to bayesian neural networks using a factorized gaussian approximation neural networks to the posterior distribution and van camp and also using a fullcovariance gaussian and bishop barber and bishop the most complete treatment however has been based on the laplace approximation mackay and forms the basis for the discussion given here. we will approximate the posterior distribution by a gaussian centred at a mode of the true posterior. furthermore we shall assume that the covariance of this gaussian is small so that the network function is approximately linear with respect to the parameters over the region of parameter space for which the posterior probability is significantly nonzero. with these two approximations we will obtain models that are analogous to the linear regression and classification models discussed in earlier chapters and so we can exploit the results obtained there. we can then make use of the evidence framework to provide point estimates for the hyperparameters and to compare alternative models example networks having different numbers of hidden units. to start with we shall discuss the regression case and then later consider the modifications needed for solving classification tasks. posterior parameter distribution consider the problem of predicting a single continuous target variable t from a vector x of inputs extension to multiple targets is straightforward. we shall suppose that the conditional distribution ptx is gaussian with an x-dependent mean given by the output of a neural network model yx w and with precision variance ptx w n w similarly we shall choose a prior distribution over the weights w that is gaussian of the form for an i.i.d. data set of n observations xn with a corresponding set of target values d tn the likelihood function is given by pw n pdw n w and so the resulting posterior distribution is then pwd pw which as a consequence of the nonlinear dependence of yx w on w will be nongaussian. we can find a gaussian approximation to the posterior distribution by using the laplace approximation. to do this we must first find a maximum of the posterior and this must be done using iterative numerical optimization. as usual it is convenient to maximize the logarithm of the posterior which can be written in the bayesian neural networks form ln pwd wtw w const which corresponds to a regularized sum-of-squares error function. assuming for the moment that and are fixed we can find a maximum of the posterior which we denote wmap by standard nonlinear optimization algorithms such as conjugate gradients using error backpropagation to evaluate the required derivatives. having found a mode wmap we can then build a local gaussian approximation by evaluating the matrix of second derivatives of the negative log posterior distribution. from this is given by a ln pwd i h where h is the hessian matrix comprising the second derivatives of the sum-ofsquares error function with respect to the components of w. algorithms for computing and approximating the hessian were discussed in section the corresponding gaussian approximation to the posterior is then given from by qwd n a similarly the predictive distribution is obtained by marginalizing with respect to this posterior distribution ptxd ptx wqwd dw. however even with the gaussian approximation to the posterior this integration is still analytically intractable due to the nonlinearity of the network function yx w as a function of w. to make progress we now assume that the posterior distribution has small variance compared with the characteristic scales of w over which yx w is varying. this allows us to make a taylor series expansion of the network function around wmap and retain only the linear terms yx w yx wmap gtw wmap where we have defined g wyx wwwmap with this approximation we now have a linear-gaussian model with a gaussian distribution for pw and a gaussian for ptw whose mean is a linear function of w of the form tyx wmap gtw wmap ptx w exercise we can therefore make use of the general result for the marginal pt to give ptxd tyx wmap neural networks where the input-dependent variance is given by gta we see that the predictive distribution ptxd is a gaussian whose mean is given by the network function yx wmap with the parameter set to their map value. the variance has two terms the first of which arises from the intrinsic noise on the target variable whereas the second is an x-dependent term that expresses the uncertainty in the interpolant due to the uncertainty in the model parameters w. this should be compared with the corresponding predictive distribution for the linear regression model given by and hyperparameter optimization so far we have assumed that the hyperparameters and are fixed and known. we can make use of the evidence framework discussed in section together with the gaussian approximation to the posterior obtained using the laplace approximation to obtain a practical procedure for choosing the values of such hyperparameters. the marginal likelihood or evidence for the hyperparameters is obtained by integrating over the network weights pd pdw dw. exercise this is easily evaluated by making use of the laplace approximation result taking logarithms then gives ln pd ewmap lna w ln n ln n where w is the total number of parameters in w and the regularized error function is defined by ewmap wmap wt mapwmap. we see that this takes the same form as the corresponding result for the linear regression model. in the evidence framework we make point estimates for and by maximizing ln pd consider first the maximization with respect to which can be done by analogy with the linear regression case discussed in section we first define the eigenvalue equation where h is the hessian matrix comprising the second derivatives of the sum-ofsquares error function evaluated at w wmap. by analogy with we obtain hui iui wt mapwmap section section bayesian neural networks where represents the effective number of parameters and is defined by i i note that this result was exact for the linear regression case. for the nonlinear neural network however it ignores the fact that changes in will cause changes in the hessian h which in turn will change the eigenvalues. we have therefore implicitly ignored terms involving the derivatives of i with respect to similarly from we see that maximizing the evidence with respect to gives the re-estimation formula n wmap as with the linear model we need to alternate between re-estimation of the hyperparameters and and updating of the posterior distribution. the situation with a neural network model is more complex however due to the multimodality of the posterior distribution. as a consequence the solution for wmap found by maximizing the log posterior will depend on the initialization of w. solutions that differ only as a consequence of the interchange and sign reversal symmetries in the hidden units are identical so far as predictions are concerned and it is irrelevant which of the equivalent solutions is found. however there may be inequivalent solutions as well and these will generally yield different values for the optimized hyperparameters. in order to compare different models for example neural networks having different numbers of hidden units we need to evaluate the model evidence pd. this can be approximated by taking and substituting the values of and obtained from the iterative optimization of these hyperparameters. a more careful evaluation is obtained by marginalizing over and again by making a gaussian approximation bishop in either case it is necessary to evaluate the determinant of the hessian matrix. this can be problematic in practice because the determinant unlike the trace is sensitive to the small eigenvalues that are often difficult to determine accurately. the laplace approximation is based on a local quadratic expansion around a mode of the posterior distribution over weights. we have seen in section that any given mode in a two-layer network is a member of a set of equivalent modes that differ by interchange and sign-change symmetries where m is the number of hidden units. when comparing networks having different numbers of hidden units this can be taken into account by multiplying the evidence by a factor of bayesian neural networks for classification so far we have used the laplace approximation to develop a bayesian treatment of neural network regression models. we now discuss the modifications to neural networks exercise exercise this framework that arise when it is applied to classification. here we shall consider a network having a single logistic sigmoid output corresponding to a two-class classification problem. the extension to networks with multiclass softmax outputs is straightforward. we shall build extensively on the analogous results for linear classification models discussed in section and so we encourage the reader to familiarize themselves with that material before studying this section. the log likelihood function for this model is given by ln pdw ln yn tn yn n where tn are the target values and yn yxn w. note that there is no hyperparameter because the data points are assumed to be correctly labelled. as before the prior is taken to be an isotropic gaussian of the form the first stage in applying the laplace framework to this model is to initialize the hyperparameter and then to determine the parameter vector w by maximizing the log posterior distribution. this is equivalent to minimizing the regularized error function ew ln pdw wtw and can be achieved using error backpropagation combined with standard optimization algorithms as discussed in section having found a solution wmap for the weight vector the next step is to evaluate the hessian matrix h comprising the second derivatives of the negative log likelihood function. this can be done for instance using the exact method of section or using the outer product approximation given by the second derivatives of the negative log posterior can again be written in the form and the gaussian approximation to the posterior is then given by to optimize the hyperparameter we again maximize the marginal likelihood which is easily shown to take the form ln pd ewmap where the regularized error function is defined by ewmap lna w ln const ln yn tn yn wt mapwmap in which yn yxn wmap. maximizing this evidence function with respect to again leads to the re-estimation equation given by the use of the evidence procedure to determine is illustrated in figure for the synthetic two-dimensional data discussed in appendix a. finally we need the predictive distribution which is defined by again this integration is intractable due to the nonlinearity of the network function. the bayesian neural networks figure illustration of the evidence framework applied to a synthetic two-class data set. the green curve shows the optimal decision boundary the black curve shows the result of fitting a two-layer network with hidden units by maximum likelihood and the red curve shows the result of including a regularizer in which is optimized using the evidence procedure starting from the initial value note that the evidence procedure greatly reduces the over-fitting of the network. simplest approximation is to assume that the posterior distribution is very narrow and hence make the approximation ptxd ptx wmap. we can improve on this however by taking account of the variance of the posterior distribution. in this case a linear approximation for the network outputs as was used in the case of regression would be inappropriate due to the logistic sigmoid outputunit activation function that constrains the output to lie in the range instead we make a linear approximation for the output unit activation in the form ax w amapx btw wmap where amapx ax wmap and the vector b ax wmap can be found by backpropagation. because we now have a gaussian approximation for the posterior distribution over w and a model for a that is a linear function of w we can now appeal to the results of section the distribution of output unit activation values induced by the distribution over network weights is given by paxd a amapx btxw wmap qwd dw where qwd is the gaussian approximation to the posterior distribution given by from section we see that this distribution is gaussian with mean amap ax wmap and variance ax btxa finally to obtain the predictive distribution we must marginalize over a using pt da. neural networks figure an illustration of the laplace approximation for a bayesian neural network having hidden units with tanh activation functions and a single logistic-sigmoid output unit. the weight parameters were found using scaled conjugate gradients and the hyperparameter was optimized using the evidence framework. on the left is the result of using the simple approximation based on a point estimate wmap of the parameters in which the green curve shows the y decision boundary and the other contours correspond to output probabilities of y and on the right is the corresponding result obtained using note that the effect of marginalization is to spread out the contours and to make the predictions less confident so that at each input point x the posterior probabilities are shifted towards while the y contour itself is unaffected. the convolution of a gaussian with a logistic sigmoid is intractable. we therefore apply the approximation to giving pt abtwmap where is defined by recall that both a and b are functions of x. figure shows an example of this framework applied to the synthetic classi fication data set described in appendix a. exercises consider a two-layer network function of the form in which the hiddenunit nonlinear activation functions g are given by logistic sigmoid functions of the form exp a show that there exists an equivalent network which computes exactly the same function but with hidden unit activation functions given by tanha where the tanh function is defined by hint first find the relation between and tanha and then show that the parameters of the two networks differ by linear transformations. www show that maximizing the likelihood function under the conditional distribution for a multioutput neural network is equivalent to minimizing the sum-of-squares error function exercises consider a regression problem involving multiple target variables in which it is assumed that the distribution of the targets conditioned on the input vector x is a gaussian of the form ptx w n w where yx w is the output of a neural network with input vector x and weight vector w and is the covariance of the assumed gaussian noise on the targets. given a set of independent observations of x and t write down the error function that must be minimized in order to find the maximum likelihood solution for w if we assume that is fixed and known. now assume that is also to be determined from the data and write down an expression for the maximum likelihood solution for note that the optimizations of w and are now coupled in contrast to the case of independent target variables discussed in section consider a binary classification problem in which the target values are t with a network output yx w that represents pt and suppose that there is a probability that the class label on a training data point has been incorrectly set. assuming independent and identically distributed data write down the error function corresponding to the negative log likelihood. verify that the error function is obtained when note that this error function makes the model robust to incorrectly labelled data in contrast to the usual error function. www show that maximizing likelihood for a multiclass neural network model in which the network outputs have the interpretation ykx w ptk is equivalent to the minimization of the cross-entropy error function www show the derivative of the error function with respect to the activation ak for an output unit having a logistic sigmoid activation function satisfies show the derivative of the error function with respect to the activation ak for output units having a softmax activation function satisfies we saw in that the derivative of the logistic sigmoid activation function can be expressed in terms of the function value itself. derive the corresponding result for the tanh activation function defined by www the error function for binary classification problems was derived for a network having a logistic-sigmoid output activation function so that yx w and data having target values t derive the corresponding error function if we consider a network having an output yx w and target values t for class and t for class what would be the appropriate choice of output unit activation function? www consider a hessian matrix h with eigenvector equation by setting the vector v in equal to each of the eigenvectors ui in turn show that h is positive definite if and only if all of its eigenvalues are positive. neural networks www consider a quadratic error function defined by in which the hessian matrix h has an eigenvalue equation given by show that the contours of constant error are ellipses whose axes are aligned with the eigenvectors ui with lengths that are inversely proportional to the square root of the corresponding eigenvalues i. www by considering the local taylor expansion of an error function about a stationary point show that the necessary and sufficient condition for the stationary point to be a local minimum of the error function is that the hessian matrix h defined by be positive definite. show that as a consequence of the symmetry of the hessian matrix h the number of independent elements in the quadratic error function is given by w by making a taylor expansion verify that the terms that are o cancel on the right-hand side of in section we derived a procedure for evaluating the jacobian matrix of a neural network using a backpropagation procedure. derive an alternative formalism for finding the jacobian based on forward propagation equations. the outer product approximation to the hessian matrix for a neural network using a sum-of-squares error function is given by extend this result to the case of multiple outputs. consider a squared loss function of the form w e px t dx dt where yx w is a parametric function such as a neural network. the result shows that the function yx w that minimizes this error is given by the conditional expectation of t given x. use this result to show that the second derivative of e with respect to two elements wr and ws of the vector w is given by wr ws y wr y ws px dx. note that for a finite sample from px we obtain consider a two-layer network of the form shown in figure with the addition of extra parameters corresponding to skip-layer connections that go directly from the inputs to the outputs. by extending the discussion of section write down the equations for the derivatives of the error function with respect to these additional parameters. www derive the expression for the outer product approximation to the hessian matrix for a network having a single output with a logistic sigmoid output-unit activation function and a cross-entropy error function corresponding to the result for the sum-of-squares error function. exercises derive an expression for the outer product approximation to the hessian matrix for a network having k outputs with a softmax output-unit activation function and a cross-entropy error function corresponding to the result for the sum-ofsquares error function. extend the expression for the outer product approximation of the hessian matrix to the case of k output units. hence derive a recursive expression analogous to for incrementing the number n of patterns and a similar expression for incrementing the number k of outputs. use these results together with the identity to find sequential update expressions analogous to for finding the inverse of the hessian by incrementally including both extra patterns and extra outputs. derive the results and for the elements of the hessian matrix of a two-layer feed-forward network by application of the chain rule of calculus. extend the results of section for the exact hessian of a two-layer network to include skip-layer connections that go directly from inputs to outputs. verify that the network function defined by and is invariant under the transformation applied to the inputs provided the weights and biases are simultaneously transformed using and similarly show that the network outputs can be transformed according by applying the transformation and to the second-layer weights and biases. www consider a quadratic error function of the form e where represents the minimum and the hessian matrix h is positive definite and constant. suppose the initial weight vector is chosen to be at the origin and is updated using simple gradient descent w w e where denotes the step number and is the learning rate is assumed to be small. show that after steps the components of the weight vector parallel to the eigenvectors of h can be written j j w j where wj wtuj and uj and j are the eigenvectors and eigenvalues respectively of h so that show that as this gives w as expected provided j now suppose that training is halted after a finite number of steps. show that the huj juj. neural networks components of the weight vector parallel to the eigenvectors of the hessian satisfy j j when j j when j w j compare this result with the discussion in section of regularization with simple weight decay and hence show that is analogous to the regularization parameter the above results also show that the effective number of parameters in the network as defined by grows as the training progresses. consider a multilayer perceptron with arbitrary feed-forward topology which is to be trained by minimizing the tangent propagation error function in which the regularizing function is given by show that the regularization term can be written as a sum over patterns of terms of the form where g is a differential operator defined by n k g xi i i by acting on the forward propagation equations zj haj with the operator g show that n can be evaluated by forward propagation using the following equations wjizi aj i j h j j wji i. i where we have defined the new variables j gzj j gaj. now show that the derivatives of n with respect to a weight wrs in the network can be written in the form k n wrs where we have defined k krzs kr s kr yk ar kr g kr. write down the backpropagation equations for kr and hence derive a set of backpropagation equations for the evaluation of the kr. exercises www consider the framework for training with transformed data in the special case in which the transformation consists simply of the addition of random noise x x where has a gaussian distribution with zero mean and unit covariance. by following an argument analogous to that of section show that the resulting regularizer reduces to the tikhonov form www consider a neural network such as the convolutional network discussed in section in which multiple weights are constrained to have the same value. discuss how the standard backpropagation algorithm must be modified in order to ensure that such constraints are satisfied when evaluating the derivatives of an error function with respect to the adjustable parameters in the network. www verify the result verify the result verify the result show that the derivatives of the mixing coefficients k defined by with respect to the auxiliary parameters j are given by jk j j k. k j hence by making use of the constraint k k derive the result write down a pair of equations that express the cartesian coordinates for the robot arm shown in figure in terms of the joint angles and and the lengths and of the links. assume the origin of the coordinate system is given by the attachment point of the lower arm. these equations define the forward kinematics of the robot arm. www derive the result for the derivative of the error function with respect to the network output activations controlling the mixing coefficients in the mixture density network. derive the result for the derivative of the error function with respect to the network output activations controlling the component means in the mixture density network. derive the result for the derivative of the error function with respect to the network output activations controlling the component variances in the mixture density network. verify the results and for the conditional mean and variance of the mixture density network model. using the general result derive the predictive distribution for the laplace approximation to the bayesian neural network model. neural networks www make use of the laplace approximation result to show that the evidence function for the hyperparameters and in the bayesian neural network model can be approximated by www outline the modifications needed to the framework for bayesian neural networks discussed in section to handle multiclass problems using networks having softmax output-unit activation functions. by following analogous steps to those given in section for regression networks derive the result for the marginal likelihood in the case of a network having a cross-entropy error function and logistic-sigmoid output-unit activation function. kernel methods in chapters and we considered linear parametric models for regression and classification in which the form of the mapping yx w from input x to output y is governed by a vector w of adaptive parameters. during the learning phase a set of training data is used either to obtain a point estimate of the parameter vector or to determine a posterior distribution over this vector. the training data is then discarded and predictions for new inputs are based purely on the learned parameter vector w. this approach is also used in nonlinear parametric models such as neural networks. however there is a class of pattern recognition techniques in which the training data points or a subset of them are kept and used also during the prediction phase. for instance the parzen probability density model comprised a linear combination of kernel functions each one centred on one of the training data points. similarly in section we introduced a simple technique for classification called nearest neighbours which involved assigning to each new test vector the same label as the chapter section kernel methods closest example from the training set. these are examples of memory-based methods that involve storing the entire training set in order to make predictions for future data points. they typically require a metric to be defined that measures the similarity of any two vectors in input space and are generally fast to train but slow at making predictions for test data points. many linear parametric models can be re-cast into an equivalent dual representation in which the predictions are also based on linear combinations of a kernel function evaluated at the training data points. as we shall see for models which are based on a fixed nonlinear feature space mapping the kernel function is given by the relation kx from this definition we see that the kernel is a symmetric function of its arguments so that kx x. the kernel concept was introduced into the field of pattern recognition by aizerman et al. in the context of the method of potential functions so-called because of an analogy with electrostatics. although neglected for many years it was re-introduced into machine learning in the context of largemargin classifiers by boser et al. giving rise to the technique of support vector machines. since then there has been considerable interest in this topic both in terms of theory and applications. one of the most significant developments has been the extension of kernels to handle symbolic objects thereby greatly expanding the range of problems that can be addressed. the simplest example of a kernel function is obtained by considering the identity mapping for the feature space in so that x in which case kx we shall refer to this as the linear kernel. the concept of a kernel formulated as an inner product in a feature space allows us to build interesting extensions of many well-known algorithms by making use of the kernel trick also known as kernel substitution. the general idea is that if we have an algorithm formulated in such a way that the input vector x enters only in the form of scalar products then we can replace that scalar product with some other choice of kernel. for instance the technique of kernel substitution can be applied to principal component analysis in order to develop a nonlinear variant of pca olkopf et al. other examples of kernel substitution include nearest-neighbour classifiers and the kernel fisher discriminant et al. roth and steinhage baudat and anouar there are numerous forms of kernel functions in common use and we shall encounter several examples in this chapter. many have the property of being a function only of the difference between the arguments so that kx kx which are known as stationary kernels because they are invariant to translations in input space. a further specialization involves homogeneous kernels also known as radial basis functions which depend only on the magnitude of the distance euclidean between the arguments so that kx for recent textbooks on kernel methods see sch olkopf and smola her brich and shawe-taylor and cristianini chapter section section dual representations dual representations many linear models for regression and classification can be reformulated in terms of a dual representation in which the kernel function arises naturally. this concept will play an important role when we consider support vector machines in the next chapter. here we consider a linear regression model whose parameters are determined by minimizing a regularized sum-of-squares error function given by jw wt tn wtw where if we set the gradient of jw with respect to w equal to zero we see that the solution for w takes the form of a linear combination of the vectors with coefficients that are functions of w of the form w wt tn an ta where is the design matrix whose nth row is given by here the vector a an and we have defined an wt tn instead of working with the parameter vector w we can now reformulate the leastsquares algorithm in terms of the parameter vector a giving rise to a dual representation. if we substitute w ta into jw we obtain ja at t ta at tt ttt at ta where t tn we now define the gram matrix k t which is an n n symmetric matrix with elements knm kxn xm where we have introduced the kernel function kx defined by in terms of the gram matrix the sum-of-squares error function can be written as ja atkka atkt ttt atka. setting the gradient of ja with respect to a to zero we obtain the following solution a in t. kernel methods if we substitute this back into the linear regression model we obtain the following prediction for a new input x yx wt at kxt in t where we have defined the vector kx with elements knx kxn x. thus we see that the dual formulation allows the solution to the least-squares problem to be expressed entirely in terms of the kernel function kx this is known as a dual formulation because by noting that the solution for a can be expressed as a linear combination of the elements of we recover the original formulation in terms of the parameter vector w. note that the prediction at x is given by a linear combination of the target values from the training set. in fact we have already obtained this result using a slightly different notation in section in the dual formulation we determine the parameter vector a by inverting an n n matrix whereas in the original parameter space formulation we had to invert an m m matrix in order to determine w. because n is typically much larger than m the dual formulation does not seem to be particularly useful. however the advantage of the dual formulation as we shall see is that it is expressed entirely in terms of the kernel function kx we can therefore work directly in terms of kernels and avoid the explicit introduction of the feature vector which allows us implicitly to use feature spaces of high even infinite dimensionality. the existence of a dual representation based on the gram matrix is a property of many linear models including the perceptron. in section we will develop a duality between probabilistic linear models for regression and the technique of gaussian processes. duality will also play an important role when we discuss support vector machines in chapter exercise exercise constructing kernels in order to exploit kernel substitution we need to be able to construct valid kernel functions. one approach is to choose a feature space mapping and then use this to find the corresponding kernel as is illustrated in figure here the kernel function is defined for a one-dimensional input space by kx x ix ix where ix are the basis functions. an alternative approach is to construct kernel functions directly. in this case we must ensure that the function we choose is a valid kernel in other words that it corresponds to a scalar product in some infinite dimensional feature space. as a simple example consider a kernel function given by kx z xtz constructing kernels figure illustration of the construction of kernel functions starting from a corresponding set of basis functions. in each column the lower plot shows the kernel function kx defined by plotted as a function of x for while the upper plot shows the corresponding basis functions given by polynomials column gaussians column and logistic sigmoids column. if we take the particular case of a two-dimensional input space x we can expand out the terms and thereby identify the corresponding nonlinear feature mapping kx z xtz and we see that the feature mapping takes the form therefore comprises all possible second order terms with a specific weighting between them. more generally however we need a simple way to test whether a function constitutes a valid kernel without having to construct the function explicitly. a necessary and sufficient condition for a function kx to be a valid kernel and cristianini is that the gram matrix k whose elements are given by kxn xm should be positive semidefinite for all possible choices of the set note that a positive semidefinite matrix is not the same thing as a matrix whose elements are nonnegative. one powerful technique for constructing new kernels is to build them out of simpler kernels as building blocks. this can be done using the following properties appendix c kernel methods techniques for constructing new kernels. given valid kernels and the following new kernels will also be valid kx kx kx q kx exp kx kx kx kx kx kaxa kx kaxa where c is a constant f is any function q is a polynomial with nonnegm is a valid kernel in ative coefficients is a function from x to r m a is a symmetric positive semidefinite matrix xa and xb are variables r necessarily disjoint with x xb and ka and kb are valid kernel functions over their respective spaces. a kbxb b akbxb b equipped with these properties we can now embark on the construction of more complex kernels appropriate to specific applications. we require that the kernel kx be symmetric and positive semidefinite and that it expresses the appropriate form of similarity between x and according to the intended application. here we consider a few common examples of kernel functions. for a more extensive discussion of kernel engineering see shawe-taylor and cristianini we saw that the simple polynomial kernel kx contains only if we consider the slightly generalized kernel kx with c then the corresponding feature mapping contains con terms of degree two. c stant and linear terms as well as terms of order two. similarly kx contains all monomials of order m. for instance if x and are two images then the kernel represents a particular weighted sum of all possible products of m pixels in the first image with m pixels in the second image. this can similarly be generalized to include all terms up to degree m by considering kx with c using the results and for combining kernels we see that these will all be valid kernel functions. c another commonly used kernel takes the form kx exp and is often called a gaussian kernel. note however that in this context it is not interpreted as a probability density and hence the normalization coefficient is constructing kernels omitted. we can see that this is a valid kernel by expanding the square to give kx exp xtx exp exp exercise exercise and then making use of and together with the validity of the linear kernel kx note that the feature vector that corresponds to the gaussian kernel has infinite dimensionality. kernel substitution in to replace obtain the gaussian kernel is not restricted to the use of euclidean distance. if we use with a nonlinear kernel we kx exp x an important contribution to arise from the kernel viewpoint has been the extension to inputs that are symbolic rather than simply vectors of real numbers. kernel functions can be defined over objects as diverse as graphs sets strings and text documents. consider for instance a fixed set and define a nonvectorial space consisting of all possible subsets of this set. if and are two such subsets then one simple choice of kernel would be where denotes the intersection of sets and and denotes the number of subsets in a. this is a valid kernel function because it can be shown to correspond to an inner product in a feature space. one powerful approach to the construction of kernels starts from a probabilistic generative model which allows us to apply generative models in a discriminative setting. generative models can deal naturally with missing data and in the case of hidden markov models can handle sequences of varying length. by contrast discriminative models generally give better performance on discriminative tasks than generative models. it is therefore of some interest to combine these two approaches et al. one way to combine them is to use a generative model to define a kernel and then use this kernel in a discriminative approach. given a generative model px we can define a kernel by kx this is clearly a valid kernel function because we can interpret it as an inner product in the one-dimensional feature space defined by the mapping px. it says that two inputs x and are similar if they both have high probabilities. we can use and to extend this class of kernels by considering sums over products of different probability distributions with positive weighting coefficients pi of the form kx i kernel methods section section exercise this is equivalent up to an overall multiplicative constant to a mixture distribution in which the components factorize with the index i playing the role of a latent variable. two inputs x and will give a large value for the kernel function and hence appear similar if they have significant probability under a range of different components. taking the limit of an infinite sum we can also consider kernels of the form kx dz where z is a continuous latent variable. now suppose that our data consists of ordered sequences of length l so that an observation is given by x xl. a popular generative model for sequences is the hidden markov model which expresses the distribution px as a marginalization over a corresponding sequence of hidden states z zl. we can use this approach to define a kernel function measuring the similarity of two sequences x and by extending the mixture representation to give kx z so that both observed sequences are generated by the same hidden sequence z. this model can easily be extended to allow sequences of differing length to be compared. an alternative technique for using generative models to define kernel functions is known as the fisher kernel and haussler consider a parametric generative model px where denotes the vector of parameters. the goal is to find a kernel that measures the similarity of two input vectors x and induced by the generative model. jaakkola and haussler consider the gradient with respect to which defines a vector in a feature space having the same dimensionality as in particular they consider the fisher score g x ln px from which the fisher kernel is defined by kx g xtf here f is the fisher information matrix given by f ex g xg xt where the expectation is with respect to x under the distribution px this can be motivated from the perspective of information geometry which considers the differential geometry of the space of model parameters. here we simply note that the presence of the fisher information matrix causes this kernel to be invariant under a nonlinear re-parameterization of the density model in practice it is often infeasible to evaluate the fisher information matrix. one approach is simply to replace the expectation in the definition of the fisher information with the sample average giving g xng xnt. f n radial basis function networks section this is the covariance matrix of the fisher scores and so the fisher kernel corresponds to a whitening of these scores. more simply we can just omit the fisher information matrix altogether and use the noninvariant kernel kx g xtg an application of fisher kernels to document retrieval is given by hofmann a final example of a kernel function is the sigmoidal kernel given by kx tanh b whose gram matrix in general is not positive semidefinite. this form of kernel has however been used in practice possibly because it gives kernel expansions such as the support vector machine a superficial resemblance to neural network models. as we shall see in the limit of an infinite number of basis functions a bayesian neural network with an appropriate prior reduces to a gaussian process thereby providing a deeper link between neural networks and kernel methods. section radial basis function networks in chapter we discussed regression models based on linear combinations of fixed basis functions although we did not discuss in detail what form those basis functions might take. one choice that has been widely used is that of radial basis functions which have the property that each basis function depends only on the radial distance euclidean from a centre j so that jx historically radial basis functions were introduced for the purpose of exact function interpolation given a set of input vectors xn along with corresponding target values tn the goal is to find a smooth function fx that fits every target value exactly so that fxn tn for n n. this is achieved by expressing fx as a linear combination of radial basis functions one centred on every data point fx the values of the coefficients are found by least squares and because there are the same number of coefficients as there are constraints the result is a function that fits every target value exactly. in pattern recognition applications however the target values are generally noisy and exact interpolation is undesirable because this corresponds to an over-fitted solution. expansions in radial basis functions also arise from regularization theory and girosi bishop for a sum-of-squares error function with a regularizer defined in terms of a differential operator the optimal solution is given by an expansion in the green s functions of the operator are analogous to the eigenvectors of a discrete matrix again with one basis function centred on each data kernel methods point. if the differential operator is isotropic then the green s functions depend only on the radial distance from the corresponding data point. due to the presence of the regularizer the solution no longer interpolates the training data exactly. another motivation for radial basis functions comes from a consideration of the interpolation problem when the input than the target variables are noisy if the noise on the input variable x is described bishop by a variable having a distribution then the sum-of-squares error function becomes d e appendix d exercise using the calculus of variations we can optimize with respect to the function fx to give yxn tnhx xn where the basis functions are given by hx xn xn xn we see that there is one basis function centred on every data point. this is known as the nadaraya-watson model and will be derived again from a different perspective in section if the noise distribution is isotropic so that it is a function only of then the basis functions will be radial. n hx xn for any value of x. the effect of such normalization is shown in figure normalization is sometimes used in practice as it avoids having regions of input space where all of the basis functions take small values which would necessarily lead to predictions in such regions that are either small or controlled purely by the bias parameter. note that the basis functions are normalized so that another situation in which expansions in normalized radial basis functions arise is in the application of kernel density estimation to the problem of regression as we shall discuss in section because there is one basis function associated with every data point the corresponding model can be computationally costly to evaluate when making predictions for new data points. models have therefore been proposed and lowe moody and darken poggio and girosi which retain the expansion in radial basis functions but where the number m of basis functions is smaller than the number n of data points. typically the number of basis functions and the locations i of their centres are determined based on the input data alone. the basis functions are then kept fixed and the coefficients are determined by least squares by solving the usual set of linear equations as discussed in section radial basis function networks figure plot of a set of gaussian basis functions on the left together with the corresponding normalized basis functions on the right. one of the simplest ways of choosing basis function centres is to use a randomly chosen subset of the data points. a more systematic approach is called orthogonal least squares et al. this is a sequential selection process in which at each step the next data point to be chosen as a basis function centre corresponds to the one that gives the greatest reduction in the sum-of-squares error. values for the expansion coefficients are determined as part of the algorithm. clustering algorithms such as k-means have also been used which give a set of basis function centres that no longer coincide with training data points. nadaraya-watson model in section we saw that the prediction of a linear regression model for a new input x takes the form of a linear combination of the training set target values with coefficients given by the equivalent kernel where the equivalent kernel satisfies the summation constraint we can motivate the kernel regression model from a different perspective starting with kernel density estimation. suppose we have a training set tn and we use a parzen density estimator to model the joint distribution px t so that section section px t n fx xn t tn where fx t is the component density function and there is one such component centred on each data point. we now find an expression for the regression function yx corresponding to the conditional average of the target variable conditioned on kernel methods the input variable which is given by yx etx tptx dt tpx t dt px t dt m n gx xntn gx xm kx xntn yx kx xn gx xn gx xm m n n m tfx xn t tn dt fx xm t tm dt we now assume for simplicity that the component density functions have zero mean so that fx tt dt for all values of x. using a simple change of variable we then obtain where n m n and the kernel function kx xn is given by and we have defined gx fx t dt. the result is known as the nadaraya-watson model or kernel regression watson for a localized kernel function it has the property of giving more weight to the data points xn that are close to x. note that the kernel satisfies the summation constraint kx xn gaussian processes figure illustration of the nadaraya-watson kernel regression model using isotropic gaussian kernels for the sinusoidal data set. the original sine function is shown by the green curve the data points are shown in blue and each is the centre of an isotropic gaussian kernel. the resulting regression function given by the conditional mean is shown by the red line along with the twostandard-deviation region for the conditional distribution ptx shown by the red shading. the blue ellipse around each data point shows one standard deviation contour for the corresponding kernel. these appear noncircular due to the different scales on the horizontal and vertical axes. in fact this model defines not only a conditional expectation but also a full conditional distribution given by ptx pt x pt x dt n fx xn t tn fx xm t tm dt exercise m from which other expectations can be evaluated. as an illustration we consider the case of a single input variable x in which fx t is given by a zero-mean isotropic gaussian over the variable z t with variance the corresponding conditional distribution is given by a gaussian mixture and is shown together with the conditional mean for the sinusoidal synthetic data set in figure an obvious extension of this model is to allow for more flexible forms of gaussian components for instance having different variance parameters for the input and target variables. more generally we could model the joint distribution pt x using a gaussian mixture model trained using techniques discussed in chapter and jordan and then find the corresponding conditional distribution ptx. in this latter case we no longer have a representation in terms of kernel functions evaluated at the training set data points. however the number of components in the mixture model can be smaller than the number of training set points resulting in a model that is faster to evaluate for test data points. we have thereby accepted an increased computational cost during the training phase in order to have a model that is faster at making predictions. gaussian processes in section we introduced kernels by applying the concept of duality to a nonprobabilistic model for regression. here we extend the role of kernels to probabilis kernel methods tic discriminative models leading to the framework of gaussian processes. we shall thereby see how kernels arise naturally in a bayesian setting. in chapter we considered linear regression models of the form yx w wt in which w is a vector of parameters and is a vector of fixed nonlinear basis functions that depend on the input vector x. we showed that a prior distribution over w induced a corresponding prior distribution over functions yx w. given a training data set we then evaluated the posterior distribution over w and thereby obtained the corresponding posterior distribution over regression functions which in turn the addition of noise implies a predictive distribution ptx for new input vectors x. in the gaussian process viewpoint we dispense with the parametric model and instead define a prior probability distribution over functions directly. at first sight it might seem difficult to work with a distribution over the uncountably infinite space of functions. however as we shall see for a finite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points and so in practice we can work in a finite space. models equivalent to gaussian processes have been widely studied in many different fields. for instance in the geostatistics literature gaussian process regression is known as kriging similarly arma moving average models kalman filters and radial basis function networks can all be viewed as forms of gaussian process models. reviews of gaussian processes from a machine learning perspective can be found in mackay williams and mackay and a comparison of gaussian process models with alternative approaches is given in rasmussen see also rasmussen and williams for a recent textbook on gaussian processes. linear regression revisited in order to motivate the gaussian process viewpoint let us return to the linear regression example and re-derive the predictive distribution by working in terms of distributions over functions yx w. this will provide a specific example of a gaussian process. consider a model defined in terms of a linear combination of m fixed basis functions given by the elements of the vector so that yx wt where x is the input vector and w is the m-dimensional weight vector. now consider a prior distribution over w given by an isotropic gaussian of the form pw n governed by the hyperparameter which represents the precision variance of the distribution. for any given value of w the definition defines a particular function of x. the probability distribution over w defined by therefore induces a probability distribution over functions yx. in practice we wish to evaluate this function at specific values of x for example at the training data points gaussian processes xn we are therefore interested in the joint distribution of the function values yxn which we denote by the vector y with elements yn yxn for n n. from this vector is given by y w where is the design matrix with elements nk kxn. we can find the probability distribution of y as follows. first of all we note that y is a linear combination of gaussian distributed variables given by the elements of w and hence is itself gaussian. we therefore need only to find its mean and covariance which are given from by ey ew exercise covy e yyt e wwt t t k where k is the gram matrix with elements knm kxn xm and kx is the kernel function. this model provides us with a particular example of a gaussian process. in general a gaussian process is defined as a probability distribution over functions yx such that the set of values of yx evaluated at an arbitrary set of points xn jointly have a gaussian distribution. in cases where the input vector x is two dimensional this may also be known as a gaussian random field. more generally a stochastic process yx is specified by giving the joint probability distribution for any finite set of values yxn in a consistent manner. a key point about gaussian stochastic processes is that the joint distribution over n variables yn is specified completely by the second-order statistics namely the mean and the covariance. in most applications we will not have any prior knowledge about the mean of yx and so by symmetry we take it to be zero. this is equivalent to choosing the mean of the prior over weight values pw to be zero in the basis function viewpoint. the specification of the gaussian process is then completed by giving the covariance of yx evaluated at any two values of x which is given by the kernel function e kxn xm. for the specific case of a gaussian process defined by the linear regression model with a weight prior the kernel function is given by we can also define the kernel function directly rather than indirectly through a choice of basis function. figure shows samples of functions drawn from gaussian processes for two different choices of kernel function. the first of these is a gaussian kernel of the form and the second is the exponential kernel given by which corresponds to the ornstein-uhlenbeck process originally introduced by uhlenbeck and ornstein to describe brownian motion. exp x kx x kernel methods figure samples from gaussian processes for a gaussian kernel and an exponential kernel gaussian processes for regression in order to apply gaussian process models to the problem of regression we need to take account of the noise on the observed target values which are given by tn yn where yn yxn and is a random noise variable whose value is chosen independently for each observation n. here we shall consider noise processes that have a gaussian distribution so that ptnyn n where is a hyperparameter representing the precision of the noise. because the noise is independent for each data point the joint distribution of the target values t tnt conditioned on the values of y ynt is given by an isotropic gaussian of the form pty n where in denotes the n n unit matrix. from the definition of a gaussian process the marginal distribution py is given by a gaussian whose mean is zero and whose covariance is defined by a gram matrix k so that py n k. the kernel function that determines k is typically chosen to express the property that for points xn and xm that are similar the corresponding values yxn and yxm will be more strongly correlated than for dissimilar points. here the notion of similarity will depend on the application. in order to find the marginal distribution pt conditioned on the input values xn we need to integrate over y. this can be done by making use of the results from section for the linear-gaussian model. using we see that the marginal distribution of t is given by pt ptypy dy n c gaussian processes where the covariance matrix c has elements cxn xm kxn xm nm. this result reflects the fact that the two gaussian sources of randomness namely that associated with yx and that associated with are independent and so their covariances simply add. one widely used kernel function for gaussian process regression is given by the exponential of a quadratic form with the addition of constant and linear terms to give kxn xm exp nxm. note that the term involving corresponds to a parametric model that is a linear function of the input variables. samples from this prior are plotted for various values of the parameters in figure and figure shows a set of points sampled from the joint distribution along with the corresponding values defined by so far we have used the gaussian process viewpoint to build a model of the joint distribution over sets of data points. our goal in regression however is to make predictions of the target variables for new inputs given a set of training data. let us suppose that tn tnt corresponding to input values xn comprise the observed training set and our goal is to predict the target variable tn for a new input vector xn this requires that we evaluate the predictive distribution ptn note that this distribution is conditioned also on the variables xn and xn however to keep the notation simple we will not show these conditioning variables explicitly. to find the conditional distribution ptn we begin by writing down the joint distribution ptn where tn denotes the vector tn tn we then apply the results from section to obtain the required conditional distribution as illustrated in figure from the joint distribution over tn will be given by ptn n cn where cn is an covariance matrix with elements given by because this joint distribution is gaussian we can apply the results from section to find the conditional gaussian distribution. to do this we partition the covariance matrix as follows cn where cn is the n n covariance matrix with elements given by for n m n the vector k has elements kxn xn for n n and the scalar cn k kt c kernel methods figure samples from a gaussian process prior defined by the covariance function the title above each plot denotes using the results and we see that the conc kxn xn ditional distribution ptn is a gaussian distribution with mean and covariance given by mxn ktc n t c ktc n k. these are the key results that define gaussian process regression. because the vector k is a function of the test point input value xn we see that the predictive distribution is a gaussian whose mean and variance both depend on xn an example of gaussian process regression is shown in figure the only restriction on the kernel function is that the covariance matrix given by must be positive definite. if i is an eigenvalue of k then the corresponding it is therefore sufficient that the kernel matrix eigenvalue of c will be i kxn xm be positive semidefinite for any pair of points xn and xm so that i because any eigenvalue i that is zero will still give rise to a positive eigenvalue for c because this is the same restriction on the kernel function discussed earlier and so we can again exploit all of the techniques in section to construct figure illustration of the sampling of data points from a gaussian process. the blue curve shows a sample function from the gaussian process prior over functions and the red points show the values of yn obtained by evaluating the function at a set of input values the corresponding values of shown in green are obtained by adding independent gaussian noise to each of the t suitable kernels. gaussian processes x note that the mean of the predictive distribution can be written as a func tion of xn in the form mxn ankxn xn exercise n t. thus if the kernel function kxn xm where an is the nth component of c depends only on the distance then we obtain an expansion in radial basis functions. the results and define the predictive distribution for gaussian process regression with an arbitrary kernel function kxn xm. in the particular case in which the kernel function kx is defined in terms of a finite set of basis functions we can derive the results obtained previously in section for linear regression starting from the gaussian process viewpoint. for such models we can therefore obtain the predictive distribution either by taking a parameter space viewpoint and using the linear regression result or by taking a function space viewpoint and using the gaussian process result. the central computational operation in using gaussian processes will involve the inversion of a matrix of size n n for which standard methods require on computations. by contrast in the basis function model we have to invert a matrix sn of size m m which has om computational complexity. note that for both viewpoints the matrix inversion must be performed once for the given training set. for each new test point both methods require a vector-matrix multiply which has cost on in the gaussian process case and om for the linear basis function model. if the number m of basis functions is smaller than the number n of data points it will be computationally more efficient to work in the basis function kernel methods figure illustration of the mechanism of gaussian process regression for the case of one training point and one test point in which the red ellipses show contours of the joint distribution here is the training data point and conditioning on the value of corresponding to the vertical blue line we obtain shown as a function of by the green curve. framework. however an advantage of a gaussian processes viewpoint is that we can consider covariance functions that can only be expressed in terms of an infinite number of basis functions. for large training data sets however the direct application of gaussian process methods can become infeasible and so a range of approximation schemes have been developed that have better scaling with training set size than the exact approach tresp smola and bartlett williams and seeger csat o and opper seeger et al. practical issues in the application of gaussian processes are discussed in bishop and nabney we have introduced gaussian process regression for the case of a single target variable. the extension of this formalism to multiple target variables known as co-kriging is straightforward. various other extensions of gaus exercise figure illustration of gaussian process regression applied to the sinusoidal data set in figure in which the three right-most data points have been omitted. the green curve shows the sinusoidal function from which the data points shown in blue are obtained by sampling and addition of gaussian noise. the red line shows the mean of the gaussian process predictive distribution and the shaded region corresponds to plus and minus two standard deviations. notice how the uncertainty increases in the region to the right of the data points. gaussian processes sian process regression have also been considered for purposes such as modelling the distribution over low-dimensional manifolds for unsupervised learning et al. and the solution of stochastic differential equations learning the hyperparameters the predictions of a gaussian process model will depend in part on the choice of covariance function. in practice rather than fixing the covariance function we may prefer to use a parametric family of functions and then infer the parameter values from the data. these parameters govern such things as the length scale of the correlations and the precision of the noise and correspond to the hyperparameters in a standard parametric model. techniques for learning the hyperparameters are based on the evaluation of the likelihood function pt where denotes the hyperparameters of the gaussian process model. the simplest approach is to make a point estimate of by maximizing the log likelihood function. because represents a set of hyperparameters for the regression problem this can be viewed as analogous to the type maximum likelihood procedure for linear regression models. maximization of the log likelihood can be done using efficient gradient-based optimization algorithms such as conjugate gradients nocedal and wright bishop and nabney the log likelihood function for a gaussian process regression model is easily evaluated using the standard form for a multivariate gaussian distribution giving ln pt lncn ttc n t n section for nonlinear optimization we also need the gradient of the log likelihood function with respect to the parameter vector we shall assume that evaluation of the derivatives of cn is straightforward as would be the case for the covariance functions considered in this chapter. making use of the result for the derivative of n together with the result for the derivative of lncn we obtain c n c cn i ttc n cn i n t. c ln pt i because ln pt will in general be a nonconvex function it can have multiple maxima. tr it is straightforward to introduce a prior over and to maximize the log posterior using gradient-based methods. in a fully bayesian treatment we need to evaluate marginals over weighted by the product of the prior p and the likelihood function pt in general however exact marginalization will be intractable and we must resort to approximations. the gaussian process regression model gives a predictive distribution whose mean and variance are functions of the input vector x. however we have assumed that the contribution to the predictive variance arising from the additive noise governed by the parameter is a constant. for some problems known as heteroscedastic the noise variance itself will also depend on x. to model this we can extend the kernel methods for gaussian processes figure samples from the ard prior in which the kernel function is given by the left plot corresponds to and the right plot corresponds to gaussian process framework by introducing a second gaussian process to represent the dependence of on the input x et al. because is a variance and hence nonnegative we use the gaussian process to model ln automatic relevance determination in the previous section we saw how maximum likelihood could be used to determine a value for the correlation length-scale parameter in a gaussian process. this technique can usefully be extended by incorporating a separate parameter for each input variable and williams the result as we shall see is that the optimization of these parameters by maximum likelihood allows the relative importance of different inputs to be inferred from the data. this represents an example in the gaussian process context of automatic relevance determination or ard which was originally formulated in the framework of neural networks neal the mechanism by which appropriate inputs are preferred is discussed in section consider a gaussian process with a two-dimensional input space x having a kernel function of the form kx exp ixi x samples from the resulting prior over functions yx are shown for two different settings of the precision parameters i in figure we see that as a particular parameter i becomes small the function becomes relatively insensitive to the corresponding input variable xi. by adapting these parameters to a data set using maximum likelihood it becomes possible to detect input variables that have little effect on the predictive distribution because the corresponding values of i will be small. this can be useful in practice because it allows such inputs to be discarded. ard is illustrated using a simple synthetic data set having three inputs and in figure the target variable t is generated by sampling values of from a gaussian evaluating the function and then adding gaussian processes figure illustration of automatic relevance determination in a gaussian process for a synthetic problem having three inputs and for which the curves show the corresponding values of the hyperparameters and as a function of the number of iterations when optimizing the marginal likelihood. details are given in the text. note the logarithmic scale on the vertical axis. gaussian noise. values of are given by copying the corresponding values of and adding noise and values of are sampled from an independent gaussian distribution. thus is a good predictor of t is a more noisy predictor of t and has only chance correlations with t. the marginal likelihood for a gaussian process with ard parameters is optimized using the scaled conjugate gradients algorithm. we see from figure that converges to a relatively large value converges to a much smaller value and becomes very small indicating that is irrelevant for predicting t. the ard framework is easily incorporated into the exponential-quadratic kernel to give the following form of kernel function which has been found useful for applications of gaussian processes to a range of regression problems kxn xm exp ixni xnixmi where d is the dimensionality of the input space. gaussian processes for classification in a probabilistic approach to classification our goal is to model the posterior probabilities of the target variable for a new input vector given a set of training data. these probabilities must lie in the interval whereas a gaussian process model makes predictions that lie on the entire real axis. however we can easily adapt gaussian processes to classification problems by transforming the output of the gaussian process using an appropriate nonlinear activation function. consider first the two-class problem with a target variable t if we define a gaussian process over a function ax and then transform the function using a logistic sigmoid y given by then we will obtain a non-gaussian stochastic process over functions yx where y this is illustrated for the case of a one-dimensional input space in figure in which the probability distri kernel methods figure the left plot shows a sample from a gaussian process prior over functions ax and the right plot shows the result of transforming this sample using a logistic sigmoid function. bution over the target variable t is then given by the bernoulli distribution pta t. as usual we denote the training set inputs by xn with corresponding observed target variables t tnt. we also consider a single test point xn with target value tn our goal is to determine the predictive distribution ptn where we have left the conditioning on the input variables implicit. to do this we introduce a gaussian process prior over the vector an which has components axn this in turn defines a non-gaussian process over tn and by conditioning on the training data tn we obtain the required predictive distribution. the gaussian process prior for an takes the form pan n cn unlike the regression case the covariance matrix no longer includes a noise term because we assume that all of the training data points are correctly labelled. however for numerical reasons it is convenient to introduce a noise-like term governed by a parameter that ensures that the covariance matrix is positive definite. thus the covariance matrix cn has elements given by cxn xm kxn xm nm where kxn xm is any positive semidefinite kernel function of the kind considered in section and the value of is typically fixed in advance. we shall assume that the kernel function kx is governed by a vector of parameters and we shall later discuss how may be learned from the training data. for two-class problems it is sufficient to predict ptn because the value of ptn is then given by ptn the required gaussian processes predictive distribution is given by ptn ptn dan where ptn this integral is analytically intractable and so may be approximated using sampling methods alternatively we can consider techniques based on an analytical approximation. in section we derived the approximate formula for the convolution of a logistic sigmoid with a gaussian distribution. we can use this result to evaluate the integral in provided we have a gaussian approximation to the posterior distribution pan the usual justification for a gaussian approximation to a posterior distribution is that the true posterior will tend to a gaussian as the number of data points increases as a consequence of the central limit theorem. in the case of gaussian processes the number of variables grows with the number of data points and so this argument does not apply directly. however if we consider increasing the number of data points falling in a fixed region of x space then the corresponding uncertainty in the function ax will decrease again leading asymptotically to a gaussian and barber three different approaches to obtaining a gaussian approximation have been considered. one technique is based on variational inference and mackay and makes use of the local variational bound on the logistic sigmoid. this allows the product of sigmoid functions to be approximated by a product of gaussians thereby allowing the marginalization over an to be performed analytically. the approach also yields a lower bound on the likelihood function ptn the variational framework for gaussian process classification can also be extended to multiclass problems by using a gaussian approximation to the softmax function a second approach uses expectation propagation and winther minka seeger because the true posterior distribution is unimodal as we shall see shortly the expectation propagation approach can give good results. laplace approximation the third approach to gaussian process classification is based on the laplace approximation which we now consider in detail. in order to evaluate the predictive distribution we seek a gaussian approximation to the posterior distribution over an which using bayes theorem is given by pan antn dan pan pan anptnan an dan pan dan ptn ptn pan dan section section section section kernel methods where we have used ptnan an ptnan. the conditional distribution pan is obtained by invoking the results and for gaussian process regression to give pan n n an c ktc n k. we can therefore evaluate the integral in by finding a laplace approximation for the posterior distribution pantn and then using the standard result for the convolution of two gaussian distributions. the prior pan is given by a zero-mean gaussian process with covariance ma trix cn and the data term independence of the data points is given by ptnan tn eantn an. we then obtain the laplace approximation by taylor expanding the logarithm of pantn which up to an additive normalization constant is given by the quantity ln pan ln ptnan at n c n an n lncn tt n an ean const. first we need to find the mode of the posterior distribution and this requires that we evaluate the gradient of which is given by tn n c n an where n is a vector with elements we cannot simply find the mode by setting this gradient to zero because n depends nonlinearly on an and so we resort to an iterative scheme based on the newton-raphson method which gives rise to an iterative reweighted least squares algorithm. this requires the second derivatives of which we also require for the laplace approximation anyway and which are given by wn c n where wn is a diagonal matrix with elements and we have used the result for the derivative of the logistic sigmoid function. note that these diagonal elements lie in the range and hence wn is a positive definite matrix. because cn hence its inverse is positive definite by construction and because the sum of two positive definite matrices is also positive definite we see that the hessian matrix a is positive definite and so the posterior distribution pantn is log convex and therefore has a single mode that is the global section exercise gaussian processes maximum. the posterior distribution is not gaussian however because the hessian is a function of an using the newton-raphson formula the iterative update equation for an exercise is given by n cn wn cn n wn an anew these equations are iterated until they converge to the mode which we denote by n at the mode the gradient will vanish and hence n will satisfy n cn n. once we have found the mode n of the posterior we can evaluate the hessian matrix given by h wn c n where the elements of wn are evaluated using proximation to the posterior distribution pantn given by n this defines our gaussian ap qan n n h exercise we can now combine this with and hence evaluate the integral because this corresponds to a linear-gaussian model we can use the general result to give ean kttn n varan c ktw n cn now that we have a gaussian distribution for pan we can approximate the integral using the result as with the bayesian logistic regression model of section if we are only interested in the decision boundary corresponding to ptn then we need only consider the mean and we can ignore the effect of the variance. we also need to determine the parameters of the covariance function. one approach is to maximize the likelihood function given by ptn for which we need expressions for the log likelihood and its gradient. if desired suitable regularization terms can also be added leading to a penalized maximum likelihood solution. the likelihood function is defined by ptn ptnan dan this integral is analytically intractable so again we make use of the laplace approximation. using the result we obtain the following approximation for the log of the likelihood function ln ptn n lnwn c n n kernel methods n ln n ln where n we also need to evaluate the gradient of ln ptn with respect to the parameter vector note that changes in will cause changes in n leading to additional terms in the gradient. thus when we differentiate with respect to we obtain two sets of terms the first arising from the dependence of the covariance matrix cn on and the rest arising from dependence of n on the terms arising from the explicit dependence on can be found by using together with the results and and are given by ln ptn j n c n cn j n n c tr cn wn cn j n and so to compute the terms arising from the dependence of n on we note that the laplace approximation has been constructed such that has zero gradient at an n gives no contribution to the gradient as a result of its dependence on n this leaves the following contribution to the derivative with respect to a component j of n lnwn c n n j cn wn nn n n j n where definition of wn we can evaluate the derivative of entiating the relation with respect to j to give n and again we have used the result together with the n with respect to j by differ n j cn j n cn wn n j rearranging then gives n j wn cn cn j n combining and we can evaluate the gradient of the log likelihood function which can be used with standard nonlinear optimization algorithms in order to determine a value for we can illustrate the application of the laplace approximation for gaussian processes using the synthetic two-class data set shown in figure extension of the laplace approximation to gaussian processes involving k classes using the softmax activation function is straightforward and barber appendix a gaussian processes figure illustration of the use of a gaussian process for classification showing the data on the left together with the optimal decision boundary from the true distribution in green and the decision boundary from the gaussian process classifier in black. on the right is the predicted posterior probability for the blue and red classes together with the gaussian process decision boundary. connection to neural networks we have seen that the range of functions which can be represented by a neural network is governed by the number m of hidden units and that for sufficiently large m a two-layer network can approximate any given function with arbitrary accuracy. in the framework of maximum likelihood the number of hidden units needs to be limited a level dependent on the size of the training set in order to avoid over-fitting. however from a bayesian perspective it makes little sense to limit the number of parameters in the network according to the size of the training set. in a bayesian neural network the prior distribution over the parameter vector w in conjunction with the network function fx w produces a prior distribution over functions from yx where y is the vector of network outputs. neal has shown that for a broad class of prior distributions over w the distribution of functions generated by a neural network will tend to a gaussian process in the limit m it should be noted however that in this limit the output variables of the neural network become independent. one of the great merits of neural networks is that the outputs share the hidden units and so they can borrow statistical strength from each other that is the weights associated with each hidden unit are influenced by all of the output variables not just by one of them. this property is therefore lost in the gaussian process limit. we have seen that a gaussian process is determined by its covariance function. williams has given explicit forms for the covariance in the case of two specific choices for the hidden unit activation function and gaussian. these kernel functions kx are nonstationary i.e. they cannot be expressed as a function of the difference x as a consequence of the gaussian weight prior being centred on zero which breaks translation invariance in weight space. kernel methods exercises by working directly with the covariance function we have implicitly marginalized over the distribution of weights. if the weight prior is governed by hyperparameters then their values will determine the length scales of the distribution over functions as can be understood by studying the examples in figure for the case of a finite number of hidden units. note that we cannot marginalize out the hyperparameters analytically and must instead resort to techniques of the kind discussed in section www consider the dual formulation of the least squares linear regression problem given in section show that the solution for the components an of the vector a can be expressed as a linear combination of the elements of the vector denoting these coefficients by the vector w show that the dual of the dual formulation is given by the original representation in terms of the parameter vector w. in this exercise we develop a dual formulation of the perceptron learning algorithm. using the perceptron learning rule show that the learned weight vector w can be written as a linear combination of the vectors tn where tn denote the coefficients of this linear combination by n and derive a formulation of the perceptron learning algorithm and the predictive function for the perceptron in terms of the n. show that the feature vector enters only in the form of the kernel function kx the nearest-neighbour classifier assigns a new input vector x to the same class as that of the nearest input vector xn from the training set where in the simplest case the distance is defined by the euclidean metric by expressing this rule in terms of scalar products and then making use of kernel substitution formulate the nearest-neighbour classifier for a general nonlinear kernel. in appendix c we give an example of a matrix that has positive elements but that has a negative eigenvalue and hence that is not positive definite. find an example of the converse property namely a matrix with positive eigenvalues yet that has at least one negative element. www verify the results and for constructing valid kernels. verify the results and for constructing valid kernels. www verify the results and for constructing valid kernels. verify the results and for constructing valid kernels. verify the results and for constructing valid kernels. show that an excellent choice of kernel for learning a function fx is given by kx by showing that a linear learning machine based on this kernel will always find a solution proportional to fx. by making use of the expansion and then expanding the middle factor as a power series show that the gaussian kernel can be expressed as the inner product of an infinite-dimensional feature vector. exercises www consider the space of all possible subsets a of a given fixed set d. show that the kernel function corresponds to an inner product in a feature space of dimensionality defined by the mapping where a is a subset of d and the element u indexed by the subset u is given by u if u a otherwise. here u a denotes that u is either a subset of a or is equal to a. show that the fisher kernel defined by remains invariant if we make a nonlinear transformation of the parameter vector where the function is invertible and differentiable. www write down the form of the fisher kernel defined by for the case of a distribution px n s that is gaussian with mean and fixed covariance s. by considering the determinant of a gram matrix show that a positive definite kernel function kx x satisfies the cauchy-schwartz inequality consider a parametric model governed by the parameter vector w together with a data set of input values xn and a nonlinear feature mapping suppose that the dependence of the error function on w takes the form jw fwt wt gwtw where g is a monotonically increasing function. by writing w in the form w n w show that the value of w that minimizes jw takes the form of a linear combination of the basis functions for n n. www consider the sum-of-squares error function for data having noisy inputs where is the distribution of the noise. use the calculus of variations to minimize this error function with respect to the function yx and hence show that the optimal solution is given by an expansion of the form in which the basis functions are given by kernel methods consider a nadaraya-watson model with one input variable x and one target variable t having gaussian components with isotropic covariances so that the covariance matrix is given by where i is the unit matrix. write down expressions for the conditional density ptx and for the conditional mean etx and variance vartx in terms of the kernel function kx xn. another viewpoint on kernel regression comes from a consideration of regression problems in which the input variables as well as the target variables are corrupted with additive noise. suppose each target value tn is generated as usual by taking a function yzn evaluated at a point zn and adding gaussian noise. the value of zn is not directly observed however but only a noise corrupted version xn zn n where the random variable is governed by some distribution g consider a set of observations tn where n n together with a corresponding sum-of-squares error function defined by averaging over the distribution of input noise to give e n g n d n. by minimizing e with respect to the function yz using the calculus of variations d show that optimal solution for yx is given by a nadaraya-watson kernel regression solution of the form with a kernel of the form www verify the results and www consider a gaussian process regression model in which the kernel function is defined in terms of a fixed set of nonlinear basis functions. show that the predictive distribution is identical to the result obtained in section for the bayesian linear regression model. to do this note that both models have gaussian predictive distributions and so it is only necessary to show that the conditional mean and variance are the same. for the mean make use of the matrix identity and for the variance make use of the matrix identity consider a regression problem with n training set input vectors xn and l test set input vectors xn xn and suppose we define a gaussian process prior over functions tx. derive an expression for the joint predictive distribution for txn txn given the values of txn show the marginal of this distribution for one of the test observations tj where n j n l is given by the usual gaussian process regression result and www consider a gaussian process regression model in which the target variable t has dimensionality d. write down the conditional distribution of tn for a test input vector xn given a training set of input vectors xn and corresponding target observations tn show that a diagonal matrix w whose elements satisfy wii is positive definite. show that the sum of two positive definite matrices is itself positive definite. exercises www using the newton-raphson formula derive the iterative update n of the posterior distribution in the gaussian formula for finding the mode process classification model. using the result derive the expressions and for the mean and variance of the posterior distribution pan in the gaussian process classification model. derive the result for the log likelihood function in the laplace approximation framework for gaussian process classification. similarly derive the results and for the terms in the gradient of the log likelihood. sparse kernel machines in the previous chapter we explored a variety of learning algorithms based on nonlinear kernels. one of the significant limitations of many such algorithms is that the kernel function kxn xm must be evaluated for all possible pairs xn and xm of training points which can be computationally infeasible during training and can lead to excessive computation times when making predictions for new data points. in this chapter we shall look at kernel-based algorithms that have sparse solutions so that predictions for new inputs depend only on the kernel function evaluated at a subset of the training data points. we begin by looking in some detail at the support vector machine which became popular in some years ago for solving problems in classification regression and novelty detection. an important property of support vector machines is that the determination of the model parameters corresponds to a convex optimization problem and so any local solution is also a global optimum. because the discussion of support vector machines makes extensive use of lagrange multipliers the reader is sparse kernel machines encouraged to review the key concepts covered in appendix e. additional information on support vector machines can be found in vapnik burges cristianini and shawe-taylor m uller et al. sch olkopf and smola and herbrich the svm is a decision machine and so does not provide posterior probabilities. we have already discussed some of the benefits of determining probabilities in section an alternative sparse kernel technique known as the relevance vector machine is based on a bayesian formulation and provides posterior probabilistic outputs as well as having typically much sparser solutions than the svm. section maximum margin classifiers we begin our discussion of support vector machines by returning to the two-class classification problem using linear models of the form yx wt b where denotes a fixed feature-space transformation and we have made the bias parameter b explicit. note that we shall shortly introduce a dual representation expressed in terms of kernel functions which avoids having to work explicitly in feature space. the training data set comprises n input vectors xn with corresponding target values tn where tn and new data points x are classified according to the sign of yx. we shall assume for the moment that the training data set is linearly separable in feature space so that by definition there exists at least one choice of the parameters w and b such that a function of the form satisfies yxn for points having tn and yxn for points having tn so that tnyxn for all training data points. there may of course exist many such solutions that separate the classes exactly. in section we described the perceptron algorithm that is guaranteed to find a solution in a finite number of steps. the solution that it finds however will be dependent on the initial values chosen for w and b as well as on the order in which the data points are presented. if there are multiple solutions all of which classify the training data set exactly then we should try to find the one that will give the smallest generalization error. the support vector machine approaches this problem through the concept of the margin which is defined to be the smallest distance between the decision boundary and any of the samples as illustrated in figure in support vector machines the decision boundary is chosen to be the one for which the margin is maximized. the maximum margin solution can be motivated using computational learning theory also known as statistical learning theory. however a simple insight into the origins of maximum margin has been given by tong and koller who consider a framework for classification based on a hybrid of generative and discriminative approaches. they first model the distribution over input vectors x for each class using a parzen density estimator with gaussian kernels section y y y maximum margin classifiers y y y margin figure the margin is defined as the perpendicular distance between the decision boundary and the closest of the data points as shown on the left figure. maximizing the margin leads to a particular choice of decision boundary as shown on the right. the location of this boundary is determined by a subset of the data points known as support vectors which are indicated by the circles. having a common parameter together with the class priors this defines an optimal misclassification-rate decision boundary. however instead of using this optimal boundary they determine the best hyperplane by minimizing the probability of error relative to the learned density model. in the limit the optimal hyperplane is shown to be the one having maximum margin. the intuition behind this result is that as is reduced the hyperplane is increasingly dominated by nearby data points relative to more distant ones. in the limit the hyperplane becomes independent of data points that are not support vectors. we shall see in figure that marginalization with respect to the prior distribution of the parameters in a bayesian approach for a simple linearly separable data set leads to a decision boundary that lies in the middle of the region separating the data points. the large margin solution has similar behaviour. recall from figure that the perpendicular distance of a point x from a hyperplane defined by yx where yx takes the form is given by furthermore we are only interested in solutions for which all data points are correctly classified so that tnyxn for all n. thus the distance of a point xn to the decision surface is given by tnyxn tnwt b the margin is given by the perpendicular distance to the closest point xn from the data set and we wish to optimize the parameters w and b in order to maximize this distance. thus the maximum margin solution is found by solving arg max where we have taken the factor outside the optimization over n because w wt b tn wb min n sparse kernel machines does not depend on n. direct solution of this optimization problem would be very complex and so we shall convert it into an equivalent problem that is much easier to solve. to do this we note that if we make the rescaling w w and b b then the distance from any point xn to the decision surface given by is unchanged. we can use this freedom to set tn wt b tn for the point that is closest to the surface. in this case all data points will satisfy the constraints wt b n n. this is known as the canonical representation of the decision hyperplane. in the case of data points for which the equality holds the constraints are said to be active whereas for the remainder they are said to be inactive. by definition there will always be at least one active constraint because there will always be a closest point and once the margin has been maximized there will be at least two active constraints. the optimization problem then simply requires that we maximize which is equivalent to minimizing and so we have to solve the optimization problem arg min wb subject to the constraints given by the factor of in is included for later convenience. this is an example of a quadratic programming problem in which we are trying to minimize a quadratic function subject to a set of linear inequality constraints. it appears that the bias parameter b has disappeared from the optimization. however it is determined implicitly via the constraints because these require that changes to be compensated by changes to b. we shall see how this works shortly. in order to solve this constrained optimization problem we introduce lagrange multipliers an with one multiplier an for each of the constraints in giving the lagrangian function lw b a tnwt b an appendix e where a an note the minus sign in front of the lagrange multiplier term because we are minimizing with respect to w and b and maximizing with respect to a. setting the derivatives of lw b a with respect to w and b equal to zero we obtain the following two conditions antn antn. w maximum margin classifiers eliminating w and b from lw b a using these conditions then gives the dual representation of the maximum margin problem in which we maximize an with respect to a subject to the constraints anamtntmkxn xm an n n antn here the kernel function is defined by kx again this takes the form of a quadratic programming problem in which we optimize a quadratic function of a subject to a set of inequality constraints. we shall discuss techniques for solving such quadratic programming problems in section the solution to a quadratic programming problem in m variables in general has computational complexity that is om in going to the dual formulation we have turned the original optimization problem which involved minimizing over m variables into the dual problem which has n variables. for a fixed set of basis functions whose number m is smaller than the number n of data points the move to the dual problem appears disadvantageous. however it allows the model to be reformulated using kernels and so the maximum margin classifier can be applied efficiently to feature spaces whose dimensionality exceeds the number of data points including infinite feature spaces. the kernel formulation also makes clear the role of the constraint that the kernel function kx be positive definite because this ensures that the lagrangian is bounded below giving rise to a well defined optimization problem. in order to classify new data points using the trained model we evaluate the sign of yx defined by this can be expressed in terms of the parameters and the kernel function by substituting for w using to give yx antnkx xn b. joseph-louis lagrange although widely considered to be a french mathematician lagrange was born in turin in italy. by the age of nineteen he had already made important contributions mathematics and had been appointed as professor at the royal artillery school in turin. for many years euler worked hard to persuade lagrange to move to berlin which he eventually did in where he succeeded euler as director of mathematics at the berlin academy. later he moved to paris narrowly escaping with his life during the french revolution thanks to the personal intervention of lavoisier french chemist who discovered oxygen who himself was later executed at the guillotine. lagrange made key contributions to the calculus of variations and the foundations of dynamics. sparse kernel machines in appendix e we show that a constrained optimization of this form satisfies the karush-kuhn-tucker conditions which in this case require that the following three properties hold an tnyxn an thus for every data point either an or tnyxn any data point for which an will not appear in the sum in and hence plays no role in making predictions for new data points. the remaining data points are called support vectors and because they satisfy tnyxn they correspond to points that lie on the maximum margin hyperplanes in feature space as illustrated in figure this property is central to the practical applicability of support vector machines. once the model is trained a significant proportion of the data points can be discarded and only the support vectors retained. having solved the quadratic programming problem and found a value for a we can then determine the value of the threshold parameter b by noting that any support vector xn satisfies tnyxn using this gives amtmkxn xm b tn m s where s denotes the set of indices of the support vectors. although we can solve this equation for b using an arbitrarily chosen support vector xn a numerically more n stable solution is obtained by first multiplying through by tn making use of and then averaging these equations over all support vectors and solving for b to give n s m s b ns tn amtmkxn xm where ns is the total number of support vectors. for later comparison with alternative models we can express the maximummargin classifier in terms of the minimization of an error function with a simple quadratic regularizer in the form e where e is a function that is zero if z and otherwise and ensures that the constraints are satisfied. note that as long as the regularization parameter satisfies its precise value plays no role. figure shows an example of the classification resulting from training a support vector machine on a simple synthetic data set using a gaussian kernel of the maximum margin classifiers figure example of synthetic data from two classes in two dimensions showing contours of constant yx obtained from a support vector machine having a gaussian kernel function. also shown are the decision boundary the margin boundaries and the support vectors. form although the data set is not linearly separable in the two-dimensional data space x it is linearly separable in the nonlinear feature space defined implicitly by the nonlinear kernel function. thus the training data points are perfectly separated in the original data space. this example also provides a geometrical insight into the origin of sparsity in the svm. the maximum margin hyperplane is defined by the location of the support vectors. other data points can be moved around freely long as they remain outside the margin region without changing the decision boundary and so the solution will be independent of such data points. overlapping class distributions so far we have assumed that the training data points are linearly separable in the feature space the resulting support vector machine will give exact separation of the training data in the original input space x although the corresponding decision boundary will be nonlinear. in practice however the class-conditional distributions may overlap in which case exact separation of the training data can lead to poor generalization. we therefore need a way to modify the support vector machine so as to allow some of the training points to be misclassified. from we see that in the case of separable classes we implicitly used an error function that gave infinite error if a data point was misclassified and zero error if it was classified correctly and then optimized the model parameters to maximize the margin. we now modify this approach so that data points are allowed to be on the wrong side of the margin boundary but with a penalty that increases with the distance from that boundary. for the subsequent optimization problem it is convenient to make this penalty a linear function of this distance. to do this we introduce slack variables n where n n with one slack variable for each training data point cortes and vapnik these are defined by n for data points that are on or inside the correct margin boundary and n yxn for other points. thus a data point that is on the decision boundary yxn will have n and points sparse kernel machines figure illustration of the slack variables n data points with circles around them are support vectors. y y y tnyxn n with n will be misclassified. the exact classification constraints are then replaced with n n in which the slack variables are constrained to satisfy n data points for which n are correctly classified and are either on the margin or on the correct side of the margin. points for which n lie inside the margin but on the correct side of the decision boundary and those data points for which n lie on the wrong side of the decision boundary and are misclassified as illustrated in figure this is sometimes described as relaxing the hard margin constraint to give a soft margin and allows some of the training set data points to be misclassified. note that while slack variables allow for overlapping class distributions this framework is still sensitive to outliers because the penalty for misclassification increases linearly with our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary. we therefore minimize c n where the parameter c controls the trade-off between the slack variable penalty and the margin. because any point that is misclassified has n it follows that n n is an upper bound on the number of misclassified points. the parameter c is therefore analogous to inverse of a regularization coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. in the limit c we will recover the earlier support vector machine for separable data. n the corresponding lagrangian is given by we now wish to minimize subject to the constraints together with n an n n n lw b a c appendix e where and n are lagrange multipliers. the corresponding set of kkt conditions are given by maximum margin classifiers an tnyxn n an n n n n n where n n. we now optimize out w b and n making use of the definition of yx an c antn for n n where are known as box constraints. this again represents a quadratic programming problem. if we substitute into we see that predictions for new data points are again made by using we can now interpret the resulting solution. as before a subset of the data points may have an in which case they do not contribute to the predictive to give l w l b antn w antn an c n. using these results to eliminate w b and n from the lagrangian we obtain the dual lagrangian in the form l n an anamtntmkxn xm which is identical to the separable case except that the constraints are somewhat different. to see what these constraints are we note that an is required because these are lagrange multipliers. furthermore together with n implies an c. we therefore have to minimize with respect to the dual variables subject to sparse kernel machines model the remaining data points constitute the support vectors. these have an and hence from must satisfy tnyxn n. if an c then implies that n which from requires n and hence such points lie on the margin. points with an c can lie inside the margin and can either be correctly classified if n or misclassified if n to determine the parameter b in we note that those support vectors for which an c have n so that tnyxn and hence will satisfy tn amtmkxn xm b again a numerically stable solution is obtained by averaging to give m s m s b nm n m tn amtmkxn xm where m denotes the set of indices of data points having an c. an alternative equivalent formulation of the support vector machine known as the has been proposed by sch olkopf et al. this involves maximizing anamtntmkxn xm subject to the constraints an antn an this approach has the advantage that the parameter which replaces c can be interpreted as both an upper bound on the fraction of margin errors for which n and hence which lie on the wrong side of the margin boundary and which may or may not be misclassified and a lower bound on the fraction of support vectors. an example of the applied to a synthetic data set is shown in figure here gaussian kernels of the form exp have been used with although predictions for new inputs are made using only the support vectors the training phase the determination of the parameters a and b makes use of the whole data set and so it is important to have efficient algorithms for solving maximum margin classifiers figure illustration of the applied to a nonseparable data set in two dimensions. the support vectors are indicated by circles. the quadratic programming problem. we first note that the objective given by or is quadratic and so any local optimum will also be a global optimum provided the constraints define a convex region they do as a consequence of being linear. direct solution of the quadratic programming problem using traditional techniques is often infeasible due to the demanding computation and memory requirements and so more practical approaches need to be found. the technique of chunking exploits the fact that the value of the lagrangian is unchanged if we remove the rows and columns of the kernel matrix corresponding to lagrange multipliers that have value zero. this allows the full quadratic programming problem to be broken down into a series of smaller ones whose goal is eventually to identify all of the nonzero lagrange multipliers and discard the others. chunking can be implemented using protected conjugate gradients although chunking reduces the size of the matrix in the quadratic function from the number of data points squared to approximately the number of nonzero lagrange multipliers squared even this may be too big to fit in memory for large-scale applications. decomposition methods et al. also solve a series of smaller quadratic programming problems but are designed so that each of these is of a fixed size and so the technique can be applied to arbitrarily large data sets. however it still involves numerical solution of quadratic programming subproblems and these can be problematic and expensive. one of the most popular approaches to training support vector machines is called sequential minimal optimization or smo it takes the concept of chunking to the extreme limit and considers just two lagrange multipliers at a time. in this case the subproblem can be solved analytically thereby avoiding numerical quadratic programming altogether. heuristics are given for choosing the pair of lagrange multipliers to be considered at each step. in practice smo is found to have a scaling with the number of data points that is somewhere between linear and quadratic depending on the particular application. we have seen that kernel functions correspond to inner products in feature spaces that can have high or even infinite dimensionality. by working directly in terms of the kernel function without introducing the feature space explicitly it might therefore seem that support vector machines somehow manage to avoid the curse of di sparse kernel machines section section mensionality. this is not the case however because there are constraints amongst the feature values that restrict the effective dimensionality of feature space. to see this consider a simple second-order polynomial kernel that we can expand in terms of its components kx z xtz this kernel function therefore represents an inner product in a feature space having six dimensions in which the mapping from input space to feature space is described by the vector function however the coefficients weighting these different features are constrained to have specific forms. thus any set of points in the original two-dimensional space x would be constrained to lie exactly on a two-dimensional nonlinear manifold embedded in the six-dimensional feature space. we have already highlighted the fact that the support vector machine does not provide probabilistic outputs but instead makes classification decisions for new input vectors. veropoulos et al. discuss modifications to the svm to allow the trade-off between false positive and false negative errors to be controlled. however if we wish to use the svm as a module in a larger probabilistic system then probabilistic predictions of the class label t for new inputs x are required. to address this issue platt has proposed fitting a logistic sigmoid to the outputs of a previously trained support vector machine. specifically the required conditional probability is assumed to be of the form pt b where yx is defined by values for the parameters a and b are found by minimizing the cross-entropy error function defined by a training set consisting of pairs of values yxn and tn. the data used to fit the sigmoid needs to be independent of that used to train the original svm in order to avoid severe over-fitting. this twostage approach is equivalent to assuming that the output yx of the support vector machine represents the log-odds of x belonging to class t because the svm training procedure is not specifically intended to encourage this the svm can give a poor approximation to the posterior probabilities relation to logistic regression as with the separable case we can re-cast the svm for nonseparable distributions in terms of the minimization of a regularized error function. this will also allow us to highlight similarities and differences compared to the logistic regression model. we have seen that for data points that are on the correct side of the margin boundary and which therefore satisfy yntn we have n and for the maximum margin classifiers ez figure plot of the hinge error function used in support vector machines shown in blue along with the error function for logistic regression rescaled by a factor of so that it passes through the point shown in red. also shown are the misclassification error in black and the squared error in green. z remaining points we have n yntn. thus the objective function can be written to an overall multiplicative constant in the form esvyntn where and esv is the hinge error function defined by esvyntn yntn where denotes the positive part. the hinge error function so-called because of its shape is plotted in figure it can be viewed as an approximation to the misclassification error i.e. the error function that ideally we would like to minimize which is also shown in figure when we considered the logistic regression model in section we found it convenient to work with target variable t for comparison with the support vector machine we first reformulate maximum likelihood logistic regression using the target variable t to do this we note that pt where yx is given by and is the logistic sigmoid function defined by it follows that pt y where we have used the properties of the logistic sigmoid function and so we can write pty exercise from this we can construct an error function by taking the negative logarithm of the likelihood function that with a quadratic regularizer takes the form elryntn where elryt ln exp yt sparse kernel machines for comparison with other error functions we can divide by so that the error function passes through the point this rescaled error function is also plotted in figure and we see that it has a similar form to the support vector error function. the key difference is that the flat region in esvyt leads to sparse solutions. both the logistic error and the hinge loss can be viewed as continuous approximations to the misclassification error. another continuous error function that has sometimes been used to solve classification problems is the squared error which is again plotted in figure it has the property however of placing increasing emphasis on data points that are correctly classified but that are a long way from the decision boundary on the correct side. such points will be strongly weighted at the expense of misclassified points and so if the objective is to minimize the misclassification rate then a monotonically decreasing error function would be a better choice. multiclass svms the support vector machine is fundamentally a two-class classifier. in practice however we often have to tackle problems involving k classes. various methods have therefore been proposed for combining multiple two-class svms in order to build a multiclass classifier. one commonly used approach is to construct k separate svms in which the kth model ykx is trained using the data from class ck as the positive examples and the data from the remaining k classes as the negative examples. this is known as the one-versus-the-rest approach. however in figure we saw that using the decisions of the individual classifiers can lead to inconsistent results in which an input is assigned to multiple classes simultaneously. this problem is sometimes addressed by making predictions for new inputs x using yx max k ykx. unfortunately this heuristic approach suffers from the problem that the different classifiers were trained on different tasks and there is no guarantee that the realvalued quantities ykx for different classifiers will have appropriate scales. another problem with the one-versus-the-rest approach is that the training sets are imbalanced. for instance if we have ten classes each with equal numbers of training data points then the individual classifiers are trained on data sets comprising negative examples and only positive examples and the symmetry of the original problem is lost. a variant of the one-versus-the-rest scheme was proposed by lee et al. who modify the target values so that the positive class has target and the negative class has target weston and watkins define a single objective function for training all k svms simultaneously based on maximizing the margin from each to remaining classes. however this can result in much slower training because instead of solving k separate optimization problems each over n data points with an overall cost of okn a single optimization problem of size must be solved giving an overall cost of ok maximum margin classifiers another approach is to train kk different svms on all possible pairs of classes and then to classify test points according to which class has the highest number of votes an approach that is sometimes called one-versus-one. again we saw in figure that this can lead to ambiguities in the resulting classification. also for large k this approach requires significantly more training time than the one-versus-the-rest approach. similarly to evaluate test points significantly more computation is required. the latter problem can be alleviated by organizing the pairwise classifiers into a directed acyclic graph to be confused with a probabilistic graphical model leading to the dagsvm et al. for k classes the dagsvm has a total of kk classifiers and to classify a new test point only k pairwise classifiers need to be evaluated with the particular classifiers used depending on which path through the graph is traversed. a different approach to multiclass classification based on error-correcting output codes was developed by dietterich and bakiri and applied to support vector machines by allwein et al. this can be viewed as a generalization of the voting scheme of the one-versus-one approach in which more general partitions of the classes are used to train the individual classifiers. the k classes themselves are represented as particular sets of responses from the two-class classifiers chosen and together with a suitable decoding scheme this gives robustness to errors and to ambiguity in the outputs of the individual classifiers. although the application of svms to multiclass classification problems remains an open issue in practice the one-versus-the-rest approach is the most widely used in spite of its ad-hoc formulation and its practical limitations. there are also single-class support vector machines which solve an unsupervised learning problem related to probability density estimation. instead of modelling the density of data however these methods aim to find a smooth boundary enclosing a region of high density. the boundary is chosen to represent a quantile of the density that is the probability that a data point drawn from the distribution will land inside that region is given by a fixed number between and that is specified in advance. this is a more restricted problem than estimating the full density but may be sufficient in specific applications. two approaches to this problem using support vector machines have been proposed. the algorithm of sch olkopf et al. tries to find a hyperplane that separates all but a fixed fraction of the training data from the origin while at the same time maximizing the distance of the hyperplane from the origin while tax and duin look for the smallest sphere in feature space that contains all but a fraction of the data points. for kernels kx that are functions only of x the two algorithms are equivalent. svms for regression we now extend support vector machines to regression problems while at the same time preserving the property of sparseness. in simple linear regression we section sparse kernel machines figure plot of an error function red in which the error increases linearly with distance beyond the insensitive region. also shown for comparison is the quadratic error function green. ez z minimize a regularized error function given by to obtain sparse solutions the quadratic error function is replaced by an error function which gives zero error if the absolute difference between the prediction yx and the target t is less than where a simple example of an error function having a linear cost associated with errors outside the insensitive region is given by eyx t t otherwise if t c and is illustrated in figure we therefore minimize a regularized error function given by eyxn tn where yx is given by by convention the regularization parameter denoted c appears in front of the error term. as before we can re-express the optimization problem by introducing slack variables. for each data point xn we now need two slack variables n and corresponds to a point for which tn yxn as illustrated in figure n where n corresponds to a point for which tn yxn n the condition for a target point to lie inside the is that yn tn yn where yn yxn. introducing the slack variables allows points to lie outside the tube provided the slack variables are nonzero and the corresponding conditions are tn yxn n tn yxn n. maximum margin classifiers figure illustration of svm regression showing the regression curve together with the tube also shown are examples of the slack variables and b points above the have and b points below the have and b and points inside the have b yx y y y x the error function for support vector regression can then be written as c n n and this can be achieved by introducing lagrange multipliers an which must be minimized subject to the constraints n n as well as n n and optimizing the lagrangian n n n n yn tn. n n an n yn tn grangian with respect to w b n n to zero giving we now substitute for yx using and then set the derivatives of the la n l c w n c. an n c l w l b l n l n exercise using these results to eliminate the corresponding variables from the lagrangian we see that the dual problem involves maximizing sparse kernel machines xm with respect to and where we have introduced the kernel kx we note that an are both required because these are lagrange multipliers. also n and n together with and require an c c and so again we have the box constraints again this is a constrained maximization and to find the constraints together with the condition substituting into we see that predictions for new inputs can be made using yx xn b which is again expressed in terms of the kernel function. the corresponding karush-kuhn-tucker conditions which state that at the solution the product of the dual variables and the constraints must vanish are given by and such points must lie either on or below the lower boundary of the from these we can obtain several useful results. first of all we note that a coefficient an can only be nonzero if n yn tn which implies that the data point either lies on the upper boundary of the n or lies above the upper boundary n similarly a nonzero value implies n yn tn furthermore the two constraints n yn tn and n yn tn n are nonnegative while is strictly positive and so for every data point xn either an both must be zero. in other words those for which either an these are points that are incompatible as is easily seen by adding them together and noting that n and the support vectors are those data points that contribute to predictions given by lie on the boundary of the or outside the tube. all points within the tube have an c c an n yn tn n yn tn n an n an we again have a sparse solution and the only terms that have to be maximum margin classifiers evaluated in the predictive model are those that involve the support vectors. the parameter b can be found by considering a data point for which an c which from must have n and from must therefore satisfy yn tn using and solving for b we obtain b tn wt tn xm for which c. in practice it is better to average over all such estimates of where we have used we can obtain an analogous result by considering a point b. as with the classification case there is an alternative formulation of the svm for regression in which the parameter governing complexity has a more intuitive interpretation olkopf et al. in particular instead of fixing the width of the insensitive region we fix instead a parameter that bounds the fraction of points lying outside the tube. this involves maximizing xm subject to the constraints an cn cn c. it can be shown that there are at most n data points falling outside the insensitive tube while at least n data points are support vectors and so lie either on the tube or outside it. the use of a support vector machine to solve a regression problem is illustrated using the sinusoidal data set in figure here the parameters and c have been chosen by hand. in practice their values would typically be determined by crossvalidation. appendix a sparse kernel machines figure illustration of the for regression applied to the sinusoidal synthetic data set using gaussian kernels. the predicted regression curve is shown by the red line and the tube corresponds to the shaded region. also the data points are shown in green and those with support vectors are indicated by blue circles. t x computational learning theory historically support vector machines have largely been motivated and analysed using a theoretical framework known as computational learning theory also sometimes called statistical learning theory and biggs kearns and vazirani vapnik vapnik this has its origins with valiant who formulated the probably approximately correct or pac learning framework. the goal of the pac framework is to understand how large a data set needs to be in order to give good generalization. it also gives bounds for the computational cost of learning although we do not consider these here. suppose that a data set d of size n is drawn from some joint distribution px t where x is the input variable and t represents the class label and that we restrict attention to noise free situations in which the class labels are determined by some deterministic function t gx. in pac learning we say that a function fxd drawn from a space f of such functions on the basis of the training set d has good generalization if its expected error rate is below some pre-specified threshold so that where i is the indicator function and the expectation is with respect to the distribution px t. the quantity on the left-hand side is a random variable because it depends on the training set d and the pac framework requires that holds with probability greater than for a data set d drawn randomly from px t. here is another pre-specified parameter and the terminology probably approximately correct comes from the requirement that with high probability than the error rate be small than for a given choice of model space f and for given parameters and pac learning aims to provide bounds on the minimum size n of data set needed to meet this criterion. a key quantity in pac learning is the vapnik-chervonenkis dimension or vc dimension which provides a measure of the complexity of a space of functions and which allows the pac framework to be extended to spaces containing an infinite number of functions. ext t the bounds derived within the pac framework are often described as worst relevance vector machines case because they apply to any choice for the distribution px t so long as both the training and the test examples are drawn from the same distribution and for any choice for the function fx so long as it belongs to f. in real-world applications of machine learning we deal with distributions that have significant regularity for example in which large regions of input space carry the same class label. as a consequence of the lack of any assumptions about the form of the distribution the pac bounds are very conservative in other words they strongly over-estimate the size of data sets required to achieve a given generalization performance. for this reason pac bounds have found few if any practical applications. one attempt to improve the tightness of the pac bounds is the pac-bayesian framework which considers a distribution over the space f of functions somewhat analogous to the prior in a bayesian treatment. this still considers any possible choice for px t and so although the bounds are tighter they are still very conservative. relevance vector machines support vector machines have been used in a variety of classification and regression applications. nevertheless they suffer from a number of limitations several of which have been highlighted already in this chapter. in particular the outputs of an svm represent decisions rather than posterior probabilities. also the svm was originally formulated for two classes and the extension to k classes is problematic. there is a complexity parameter c or well as a parameter in the case of regression that must be found using a hold-out method such as cross-validation. finally predictions are expressed as linear combinations of kernel functions that are centred on training data points and that are required to be positive definite. the relevance vector machine or rvm is a bayesian sparse kernel technique for regression and classification that shares many of the characteristics of the svm whilst avoiding its principal limitations. additionally it typically leads to much sparser models resulting in correspondingly faster performance on test data whilst maintaining comparable generalization error. in contrast to the svm we shall find it more convenient to introduce the regres sion form of the rvm first and then consider the extension to classification tasks. rvm for regression the relevance vector machine for regression is a linear model of the form studied in chapter but with a modified prior that results in sparse solutions. the model defines a conditional distribution for a real-valued target variable t given an input vector x which takes the form ptx w n sparse kernel machines where by a linear model of the form is the noise precision noise variance and the mean is given yx wi ix wt with fixed nonlinear basis functions ix which will typically include a constant term so that the corresponding weight parameter represents a bias the relevance vector machine is a specific instance of this model which is intended to mirror the structure of the support vector machine. in particular the basis functions are given by kernels with one kernel associated with each of the data points from the training set. the general expression then takes the svm-like form yx wnkx xn b where b is a bias parameter. the number of parameters in this case is m n and yx has the same form as the predictive model for the svm except that the coefficients an are here denoted wn. it should be emphasized that the subsequent analysis is valid for arbitrary choices of basis function and for generality we shall work with the form in contrast to the svm there is no restriction to positivedefinite kernels nor are the basis functions tied in either number or location to the training data points. suppose we are given a set of n observations of the input vector x which we n with n n. the denote collectively by a data matrix x whose nth row is xt corresponding target values are given by t tnt. thus the likelihood function is given by ptx w ptnxn w next we introduce a prior distribution over the parameter vector w and as in chapter we shall consider a zero-mean gaussian prior. however the key difference in the rvm is that we introduce a separate hyperparameter i for each of the weight parameters wi instead of a single shared hyperparameter. thus the weight prior takes the form pw n i where i represents the precision of the corresponding parameter wi and denotes m we shall see that when we maximize the evidence with respect to these hyperparameters a significant proportion of them go to infinity and the corresponding weight parameters have posterior distributions that are concentrated at zero. the basis functions associated with these parameters therefore play no role relevance vector machines in the predictions made by the model and so are effectively pruned out resulting in a sparse model. using the result for linear regression models we see that the posterior distribution for the weights is again gaussian and takes the form pwt x n where the mean and covariance are given by m tt where is the n m design matrix with elements ni ixn and a diag i. note that in the specific case of the model we have k where k is the symmetric kernel matrix with elements kxn xm. a t the values of and are determined using maximum likelihood also known as the evidence approximation in which we maximize the marginal likelihood function obtained by integrating out the weight parameters ptx w dw. ptx because this represents the convolution of two gaussians it is readily evaluated to give the log marginal likelihood in the form ln ptx lnn c n lnc ttc where t tnt and we have defined the n n matrix c given by c a t. section exercise our goal is now to maximize with respect to the hyperparameters and this requires only a small modification to the results obtained in section for the evidence approximation in the linear regression model. again we can identify two approaches. in the first we simply set the required derivatives of the marginal likelihood to zero and obtain the following re-estimation equations exercise new i new i i i i n section where mi is the ith component of the posterior mean m defined by the quantity i measures how well the corresponding parameter wi is determined by the data and is defined by sparse kernel machines i i ii in which ii is the ith diagonal component of the posterior covariance given by learning therefore proceeds by choosing initial values for and evaluating the mean and covariance of the posterior using and respectively and then alternately re-estimating the hyperparameters using and and re-estimating the posterior mean and covariance using and until a suitable convergence criterion is satisfied. exercise section exercise section the second approach is to use the em algorithm and is discussed in section these two approaches to finding the values of the hyperparameters that maximize the evidence are formally equivalent. numerically however it is found that the direct optimization approach corresponding to and gives somewhat faster convergence as a result of the optimization we find that a proportion of the hyperparameters i are driven to large principle infinite values and so the weight parameters wi corresponding to these hyperparameters have posterior distributions with mean and variance both zero. thus those parameters and the corresponding basis functions ix are removed from the model and play no role in making predictions for new inputs. in the case of models of the form the inputs xn corresponding to the remaining nonzero weights are called relevance vectors because they are identified through the mechanism of automatic relevance determination and are analogous to the support vectors of an svm. it is worth emphasizing however that this mechanism for achieving sparsity in probabilistic models through automatic relevance determination is quite general and can be applied to any model expressed as an adaptive linear combination of basis functions. having found values and for the hyperparameters that maximize the marginal likelihood we can evaluate the predictive distribution over t for a new input x. using and this is given by ptx x t ptx w t dw tmt thus the predictive mean is given by with w set equal to the posterior mean m and the variance of the predictive distribution is given by where is given by in which and are set to their optimized values and this is just the familiar result obtained in the context of linear regression. recall that for localized basis functions the predictive variance for linear regression models becomes small in regions of input space where there are no basis functions. in the case of an rvm with the basis functions centred on data points the model will therefore become increasingly certain of its predictions when extrapolating outside the domain of the data and qui nonero-candela which of course is undesirable. the predictive distribution in gaussian process regression does not relevance vector machines figure illustration of rvm regression using the same data set and the same gaussian kernel functions as used in figure for the regression model. the mean of the predictive distribution for the rvm is shown by the red line and the one standarddeviation predictive distribution is shown by the shaded region. also the data points are shown in green and the relevance vectors are indicated by blue circles. note that there are only relevance vectors compared to support vectors for the in figure t x suffer from this problem. however the computational cost of making predictions with a gaussian processes is typically much higher than with an rvm. figure shows an example of the rvm applied to the sinusoidal regression data set. here the noise precision parameter is also determined through evidence maximization. we see that the number of relevance vectors in the rvm is significantly smaller than the number of support vectors used by the svm. for a wide range of regression and classification tasks the rvm is found to give models that are typically an order of magnitude more compact than the corresponding support vector machine resulting in a significant improvement in the speed of processing on test data. remarkably this greater sparsity is achieved with little or no reduction in generalization error compared with the corresponding svm. the principal disadvantage of the rvm compared to the svm is that training involves optimizing a nonconvex function and training times can be longer than for a comparable svm. for a model with m basis functions the rvm requires inversion of a matrix of size m m which in general requires om computation. in the specific case of the svm-like model we have m n as we have noted there are techniques for training svms whose cost is roughly quadratic in n. of course in the case of the rvm we always have the option of starting with a smaller number of basis functions than n more significantly in the relevance vector machine the parameters governing complexity and noise variance are determined automatically from a single training run whereas in the support vector machine the parameters c and are generally found using cross-validation which involves multiple training runs. furthermore in the next section we shall derive an alternative procedure for training the relevance vector machine that improves training speed significantly. analysis of sparsity we have noted earlier that the mechanism of automatic relevance determination causes a subset of parameters to be driven to zero. we now examine in more detail sparse kernel machines c t c t figure illustration of the mechanism for sparsity in a bayesian linear regression model showing a training set vector of target values given by t indicated by the cross for a model with one basis vector which is poorly aligned with the target data vector t. on the left we see a model having only isotropic noise so that c corresponding to with set to its most probable value. on the right we see the same model but with a finite value of in each case the red ellipse corresponds to unit mahalanobis distance with taking the same value for both plots while the dashed green circle shows the contrition arising from the noise term we see that any finite value of reduces the probability of the observed data and so for the most probable solution the basis vector is removed. the mechanism of sparsity in the context of the relevance vector machine. in the process we will arrive at a significantly faster procedure for optimizing the hyperparameters compared to the direct techniques given above. before proceeding with a mathematical analysis we first give some informal insight into the origin of sparsity in bayesian linear models. consider a data set comprising n observations and together with a model having a single basis function with hyperparameter along with isotropic noise having precision from the marginal likelihood is given by pt n c in which the covariance matrix takes the form c i t where denotes the n-dimensional vector and similarly t notice that this is just a zero-mean gaussian process model over t with covariance c. given a particular observation for t our goal is to find and by maximizing the marginal likelihood. we see from figure that if there is a poor alignment between the direction of and that of the training data vector t then the corresponding hyperparameter will be driven to and the basis vector will be pruned from the model. this arises because any finite value for will always assign a lower probability to the data thereby decreasing the value of the density at t provided that is set to its optimal value. we see that any finite value for would cause the distribution to be elongated in a direction away from the data thereby increasing the probability mass in regions away from the observed data and hence reducing the value of the density at the target data vector itself. for the more general case of m relevance vector machines basis vectors m a similar intuition holds namely that if a particular basis vector is poorly aligned with the data vector t then it is likely to be pruned from the model. we now investigate the mechanism for sparsity from a more mathematical perspective for a general case involving m basis functions. to motivate this analysis we first note that in the result for re-estimating the parameter i the terms on the right-hand side are themselves also functions of i. these results therefore represent implicit solutions and iteration would be required even to determine a single i with all other j for j i fixed. this suggests a different approach to solving the optimization problem for the rvm in which we make explicit all of the dependence of the marginal likelihood on a particular i and then determine its stationary points explicitly and tipping tipping and faul to do this we first pull out the contribution from i in the matrix c defined by to give c j j t j c i i i t i i i t i where i denotes the ith column of in other words the n-dimensional vector with elements ixn in contrast to n which denotes the nth row of the matrix c i represents the matrix c with the contribution from basis function i removed. using the matrix identities and the determinant and inverse of c can then be written i c c c i i i t i c i i t i i c i i i t i c using these results we can then write the log marginal likelihood function in the form where l i is simply the log marginal likelihood with basis function i omitted and the quantity i is defined by l l i i i ln i ln i si i i si and contains all of the dependence on i. here we have introduced the two quantities si t qi t i c i c i i i t. here si is called the sparsity and qi is known as the quality of i and as we shall see a large value of si relative to the value of qi means that the basis function i exercise sparse kernel machines of the figure plots log likelihood i versus marginal ln i showing on the left the single maximum at a finite i for i and si that i si and on the right the maximum at i for i and si that i si. is more likely to be pruned from the model. the sparsity measures the extent to which basis function i overlaps with the other basis vectors in the model and the quality represents a measure of the alignment of the basis vector n with the error between the training set values t tnt and the vector y i of predictions that would result from the model with the vector i excluded and faul the stationary points of the marginal likelihood with respect to i occur when the derivative d i i i i i si is equal to zero. there are two possible forms for the solution. recalling that i we see that if i si we can solve for i to obtain i si then i provides a solution. conversely if d i i i i si exercise these two solutions are illustrated in figure we see that the relative size of the quality and sparsity terms determines whether a particular basis vector will be pruned from the model or not. a more complete analysis and tipping based on the second derivatives of the marginal likelihood confirms these solutions are indeed the unique maxima of i. note that this approach has yielded a closed-form solution for i for given values of the other hyperparameters. as well as providing insight into the origin of sparsity in the rvm this analysis also leads to a practical algorithm for optimizing the hyperparameters that has significant speed advantages. this uses a fixed set of candidate basis vectors and then cycles through them in turn to decide whether each vector should be included in the model or not. the resulting sequential sparse bayesian learning algorithm is described below. sequential sparse bayesian learning algorithm if solving a regression problem initialize initialize using one basis function with hyperparameter set using with the remaining hyperparameters j for j i initialized to infinity so that only is included in the model. relevance vector machines evaluate and m along with qi and si for all basis functions. select a candidate basis function i. if i si and i so that the basis vector i is already included in i si and i then add i to the model and evaluate hyperpai si and i then remove basis function i from the model the model then update i using if if rameter i using and set i if solving a regression problem update if converged terminate otherwise go to note that if from the model and no action is required. i si and i then the basis function i is already excluded in practice it is convenient to evaluate the quantities qi t si t i c i c i. the quality and sparseness variables can then be expressed in the form qi iqi i si isi i si note that when i we have qi qi and si si. using we can write si qi t si t i t t i i t i tt i t i where and involve only those basis vectors that correspond to finite hyperparameters i. at each stage the required computations therefore scale like om where m is the number of active basis vectors in the model and is typically much smaller than the number n of training patterns. rvm for classification we can extend the relevance vector machine framework to classification problems by applying the ard prior over weights to a probabilistic linear classification model of the kind studied in chapter to start with we consider two-class problems with a binary target variable t the model now takes the form of a linear combination of basis functions transformed by a logistic sigmoid function yx w wt exercise sparse kernel machines section exercise where is the logistic sigmoid function defined by if we introduce a gaussian prior over the weight vector w then we obtain the model that has been considered already in chapter the difference here is that in the rvm this model uses the ard prior in which there is a separate precision hyperparameter associated with each weight parameter. in contrast to the regression model we can no longer integrate analytically over the parameter vector w. here we follow tipping and use the laplace approximation which was applied to the closely related problem of bayesian logistic regression in section we begin by initializing the hyperparameter vector for this given value of we then build a gaussian approximation to the posterior distribution and thereby obtain an approximation to the marginal likelihood. maximization of this approximate marginal likelihood then leads to a re-estimated value for and the process is repeated until convergence. let us consider the laplace approximation for this model in more detail. for a fixed value of the mode of the posterior distribution over w is obtained by maximizing ln pwt lnptwpw ln pt ln yn tn yn wtaw const where a diag i. this can be done using iterative reweighted least squares as discussed in section for this we need the gradient vector and hessian matrix of the log posterior distribution which from are given by ln pwt ln pwt tt y aw tb a where b is an n n diagonal matrix with elements bn yn the vector y ynt and is the design matrix with elements ni ixn. here we have used the property for the derivative of the logistic sigmoid function. at convergence of the irls algorithm the negative hessian represents the inverse covariance matrix for the gaussian approximation to the posterior distribution. the mode of the resulting approximation to the posterior distribution corresponding to the mean of the gaussian approximation is obtained setting to zero giving the mean and covariance of the laplace approximation in the form a tt y tb a we can now use this laplace approximation to evaluate the marginal likelihood. using the general result for an integral evaluated using the laplace approxi relevance vector machines mation we have ptwpw dw pt if we substitute for and and then set the derivative of the marginal likelihood with respect to i equal to zero we obtain exercise i i ii defining i i ii and rearranging then gives i i new i which is identical to the re-estimation formula obtained for the regression rvm. if we define we can write the approximate log marginal likelihood in the form b y n lnc ln pt where c b a t. appendix a section this takes the same form as in the regression case and so we can apply the same analysis of sparsity and obtain the same fast learning algorithm in which we fully optimize a single hyperparameter i at each step. figure shows the relevance vector machine applied to a synthetic classification data set. we see that the relevance vectors tend not to lie in the region of the decision boundary in contrast to the support vector machine. this is consistent with our earlier discussion of sparsity in the rvm because a basis function ix centred on a data point near the boundary will have a vector i that is poorly aligned with the training data vector t. one of the potential advantages of the relevance vector machine compared with the svm is that it makes probabilistic predictions. for example this allows the rvm to be used to help construct an emission density in a nonlinear extension of the linear dynamical system for tracking faces in video sequences et al. so far we have considered the rvm for binary classification problems. for k classes we again make use of the probabilistic approach in section in which there are k linear models of the form ak wt k x sparse kernel machines figure example of the relevance vector machine applied to a synthetic data set in which the left-hand plot shows the decision boundary and the data points with the relevance vectors indicated by circles. comparison with the results shown in figure for the corresponding support vector machine shows that the rvm gives a much sparser model. the right-hand plot shows the posterior probability given by the rvm output in which the proportion of red ink indicates the probability of that point belonging to the red class. which are combined using a softmax function to give outputs expaj ykx the log likelihood function is then given by j ln wk ytnk nk where the target values tnk have a coding for each data point n and t is a matrix with elements tnk. again the laplace approximation can be used to optimize the hyperparameters in which the model and its hessian are found using irls. this gives a more principled approach to multiclass classification than the pairwise method used in the support vector machine and also provides probabilistic predictions for new data points. the principal disadvantage is that the hessian matrix has size m k m k where m is the number of active basis functions which gives an additional factor of k in the computational cost of training compared with the two-class rvm. the principal disadvantage of the relevance vector machine is the relatively long training times compared with the svm. this is offset however by the avoidance of cross-validation runs to set the model complexity parameters. furthermore because it yields sparser models the computation time on test points which is usually the more important consideration in practice is typically much less. exercises exercises www suppose we have a data set of input vectors with corresponding target values tn and suppose that we model the density of input vectors within each class separately using a parzen kernel density estimator section with a kernel kx write down the minimum misclassification-rate decision rule assuming the two classes have equal prior probability. show also that if the kernel is chosen to be kx then the classification rule reduces to simply assigning a new input vector to the class having the closest mean. finally show that if the kernel takes the form kx that the classification is based on the closest mean in the feature space show that if the on the right-hand side of the constraint is replaced by some arbitrary constant the solution for the maximum margin hyperplane is unchanged. show that irrespective of the dimensionality of the data space a data set consisting of just two data points one from each class is sufficient to determine the location of the maximum-margin hyperplane. www show that the value of the margin for the maximum-margin hyper plane is given by where are given by maximizing subject to the constraints and an show that the values of and in the previous exercise also satisfy is defined by similarly show that consider the logistic regression model with a target variable t if we define pt where yx is given by show that the negative log likelihood with the addition of a quadratic regularization term takes the form consider the lagrangian for the regression support vector machine. by setting the derivatives of the lagrangian with respect to w b n n to zero and then back substituting to eliminate the corresponding variables show that the dual lagrangian is given by sparse kernel machines www for the regression support vector machine considered in section show that all training data points for which n will have an c and similarly all points for n will c. verify the results and for the mean and covariance of the posterior distribution over weights in the regression rvm. www derive the result for the marginal likelihood function in the regression rvm by performing the gaussian integral over w in using the technique of completing the square in the exponential. repeat the above exercise but this time make use of the general result www show that direct maximization of the log marginal likelihood for the regression relevance vector machine leads to the re-estimation equations and where i is defined by in the evidence framework for rvm regression we obtained the re-estimation formulae and by maximizing the marginal likelihood given by extend this approach by inclusion of hyperpriors given by gamma distributions of the form and obtain the corresponding re-estimation formulae for and by maximizing the corresponding posterior probability pt with respect to and derive the result for the predictive distribution in the relevance vector machine for regression. show that the predictive variance is given by www using the results and show that the marginal likelihood can be written in the form where n is defined by and the sparsity and quality factors are defined by and respectively. by taking the second derivative of the log marginal likelihood for the regression rvm with respect to the hyperparameter i show that the stationary point given by is a maximum of the marginal likelihood. using and together with the matrix identity show that the quantities sn and qn defined by and can be written in the form and www show that the gradient vector and hessian matrix of the log posterior distribution for the classification relevance vector machine are given by and verify that maximization of the approximate log marginal likelihood function for the classification relevance vector machine leads to the result for re-estimation of the hyperparameters. graphical models probabilities play a central role in modern pattern recognition. we have seen in chapter that probability theory can be expressed in terms of two simple equations corresponding to the sum rule and the product rule. all of the probabilistic inference and learning manipulations discussed in this book no matter how complex amount to repeated application of these two equations. we could therefore proceed to formulate and solve complicated probabilistic models purely by algebraic manipulation. however we shall find it highly advantageous to augment the analysis using diagrammatic representations of probability distributions called probabilistic graphical models. these offer several useful properties they provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models. insights into the properties of the model including conditional independence properties can be obtained by inspection of the graph. graphical models complex computations required to perform inference and learning in sophisticated models can be expressed in terms of graphical manipulations in which underlying mathematical expressions are carried along implicitly. a graph comprises nodes called vertices connected by links known as edges or arcs. in a probabilistic graphical model each node represents a random variable group of random variables and the links express probabilistic relationships between these variables. the graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables. we shall begin by discussing bayesian networks also known as directed graphical models in which the links of the graphs have a particular directionality indicated by arrows. the other major class of graphical models are markov random fields also known as undirected graphical models in which the links do not carry arrows and have no directional significance. directed graphs are useful for expressing causal relationships between random variables whereas undirected graphs are better suited to expressing soft constraints between random variables. for the purposes of solving inference problems it is often convenient to convert both directed and undirected graphs into a different representation called a factor graph. in this chapter we shall focus on the key aspects of graphical models as needed for applications in pattern recognition and machine learning. more general treatments of graphical models can be found in the books by whittaker lauritzen jensen castillo et al. jordan cowell et al. and jordan bayesian networks in order to motivate the use of directed graphs to describe probability distributions consider first an arbitrary joint distribution pa b c over three variables a b and c. note that at this stage we do not need to specify anything further about these variables such as whether they are discrete or continuous. indeed one of the powerful aspects of graphical models is that a specific graph can make probabilistic statements for a broad class of distributions. by application of the product rule of probability we can write the joint distribution in the form pa b c pca bpa b. a second application of the product rule this time to the second term on the righthand side of gives pa b c pca bpbapa. note that this decomposition holds for any choice of the joint distribution. we now represent the right-hand side of in terms of a simple graphical model as follows. first we introduce a node for each of the random variables a b and c and associate each node with the corresponding conditional distribution on the right-hand side of figure a directed graphical model representing the joint probability distribution over three variables a b and c corresponding to the decomposition on the right-hand side of a b bayesian networks c then for each conditional distribution we add directed links to the graph from the nodes corresponding to the variables on which the distribution is conditioned. thus for the factor pca b there will be links from nodes a and b to node c whereas for the factor pa there will be no incoming links. the result is the graph shown in figure if there is a link going from a node a to a node b then we say that node a is the parent of node b and we say that node b is the child of node a. note that we shall not make any formal distinction between a node and the variable to which it corresponds but will simply use the same symbol to refer to both. an interesting point to note about is that the left-hand side is symmetrical with respect to the three variables a b and c whereas the right-hand side is not. indeed in making the decomposition in we have implicitly chosen a particular ordering namely a b c and had we chosen a different ordering we would have obtained a different decomposition and hence a different graphical representation. we shall return to this point later. for the moment let us extend the example of figure by considering the joint distribution over k variables given by xk. by repeated application of the product rule of probability this joint distribution can be written as a product of conditional distributions one for each of the variables xk xk for a given choice of k we can again represent this as a directed graph having k nodes one for each conditional distribution on the right-hand side of with each node having incoming links from all lower numbered nodes. we say that this graph is fully connected because there is a link between every pair of nodes. so far we have worked with completely general joint distributions so that the decompositions and their representations as fully connected graphs will be applicable to any choice of distribution. as we shall see shortly it is the absence of links in the graph that conveys interesting information about the properties of the class of distributions that the graph represents. consider the graph shown in figure this is not a fully connected graph because for instance there is no link from to or from to we shall now go from this graph to the corresponding representation of the joint probability distribution written in terms of the product of a set of conditional distributions one for each node in the graph. each such conditional distribution will be conditioned only on the parents of the corresponding node in the graph. for instance will be conditioned on and the joint distribution of all variables graphical models figure example of a directed acyclic graph describing the joint distribution over variables the corresponding decomposition of the joint distribution is given by is therefore given by the reader should take a moment to study carefully the correspondence between and figure px pxkpak we can now state in general terms the relationship between a given directed graph and the corresponding distribution over the variables. the joint distribution defined by a graph is given by the product over all of the nodes of the graph of a conditional distribution for each node conditioned on the variables corresponding to the parents of that node in the graph. thus for a graph with k nodes the joint distribution is given by where pak denotes the set of parents of xk and x xk. this key equation expresses the factorization properties of the joint distribution for a directed graphical model. although we have considered each node to correspond to a single variable we can equally well associate sets of variables and vector-valued variables with the nodes of a graph. it is easy to show that the representation on the righthand side of is always correctly normalized provided the individual conditional distributions are normalized. the directed graphs that we are considering are subject to an important restriction namely that there must be no directed cycles in other words there are no closed paths within the graph such that we can move from node to node along links following the direction of the arrows and end up back at the starting node. such graphs are also called directed acyclic graphs or dags. this is equivalent to the statement that there exists an ordering of the nodes such that there are no links that go from any node to any lower numbered node. example polynomial regression as an illustration of the use of directed graphs to describe probability distributions we consider the bayesian polynomial regression model introduced in sec exercise exercise bayesian networks figure directed graphical model representing the joint distribution corresponding to the bayesian polynomial regression model introduced in section w tn tion the random variables in this model are the vector of polynomial coefficients w and the observed data t tnt. in addition this model contains the input data x xn the noise variance and the hyperparameter representing the precision of the gaussian prior over w all of which are parameters of the model rather than random variables. focussing just on the random variables for the moment we see that the joint distribution is given by the product of the prior pw and n conditional distributions ptnw for n n so that pt w pw ptnw. this joint distribution can be represented by a graphical model shown in figure when we start to deal with more complex models later in the book we shall find it inconvenient to have to write out multiple nodes of the form tn explicitly as in figure we therefore introduce a graphical notation that allows such multiple nodes to be expressed more compactly in which we draw a single representative node tn and then surround this with a box called a plate labelled with n indicating that there are n nodes of this kind. re-writing the graph of figure in this way we obtain the graph shown in figure we shall sometimes find it helpful to make the parameters of a model as well as its stochastic variables explicit. in this case becomes pt wx pw ptnw xn correspondingly we can make x and explicit in the graphical representation. to do this we shall adopt the convention that random variables will be denoted by open circles and deterministic parameters will be denoted by smaller solid circles. if we take the graph of figure and include the deterministic parameters we obtain the graph shown in figure when we apply a graphical model to a problem in machine learning or pattern recognition we will typically set some of the random variables to specific observed figure an alternative more compact representation of the graph shown in figure in which we have introduced a plate box labelled n that represents n nodes of which only a single example tn is shown explicitly. tn n w graphical models figure this shows the same model as in figure but with the deterministic parameters shown explicitly by the smaller solid nodes. xn tn n w values for example the variables from the training set in the case of polynomial curve fitting. in a graphical model we will denote such observed variables by shading the corresponding nodes. thus the graph corresponding to figure in which the variables are observed is shown in figure note that the value of w is not observed and so w is an example of a latent variable also known as a hidden variable. such variables play a crucial role in many probabilistic models and will form the focus of chapters and having observed the values we can if desired evaluate the posterior distribution of the polynomial coefficients w as discussed in section for the moment we note that this involves a straightforward application of bayes theorem pwt pw ptnw where again we have omitted the deterministic parameters in order to keep the notation uncluttered. in general model parameters such as w are of little direct interest in themselves because our ultimate goal is to make predictions for new input values. suppose we are given a new input and we wish to find the corresponding probability distribution conditioned on the observed data. the graphical model that describes this problem is shown in figure and the corresponding joint distribution of all of the random variables in this model conditioned on the deterministic parameters is then given by t x ptnxn w pw w figure as in figure but with the nodes shaded to indicate that the corresponding random variables have been set to their observed set values. xn tn n w bayesian networks figure the polynomial regression model corresponding to figure showing also a new input value bx together with the corresponding model prediction bt. xn w tn n the required predictive distribution is then obtained from the sum rule of t x probability by integrating out the model parameters w so that x t t x dw where we are implicitly setting the random variables in t to the specific values observed in the data set. the details of this calculation were discussed in chapter generative models there are many situations in which we wish to draw samples from a given probability distribution. although we shall devote the whole of chapter to a detailed discussion of sampling methods it is instructive to outline here one technique called ancestral sampling which is particularly relevant to graphical models. consider a joint distribution xk over k variables that factorizes according to corresponding to a directed acyclic graph. we shall suppose that the variables have been ordered such that there are no links from any node to any lower numbered node in other words each node has a higher number than any of its parents. our goal is to draw a from the joint distribution. distribution which we we then work through each of the nodes in or to do this we start with the lowest-numbered node and draw a sample from the der so that for node n we draw a sample from the conditional distribution pxnpan in which the parent variables have been set to their sampled values. note that at each stage these parent values will always be available because they correspond to lowernumbered nodes that have already been sampled. techniques for sampling from specific distributions will be discussed in detail in chapter once we have sampled from the final variable xk we will have achieved our objective of obtaining a sample from the joint distribution. to obtain a sample from some marginal distribution corresponding to a subset of the variables we simply take the sampled values for the required nodes and ignore the sampled values for the remaining nodes. for example to draw a sample from the distribution we simply sample from the full joint distribution and then retain the and discard the remaining values graphical models figure a graphical model representing the process by which images of objects are created in which the identity of an object discrete variable and the position and orientation of that object variables have independent prior probabilities. the image vector of pixel intensities has a probability distribution that is dependent on the identity of the object as well as on its position and orientation. object position orientation image for practical applications of probabilistic models it will typically be the highernumbered variables corresponding to terminal nodes of the graph that represent the observations with lower-numbered nodes corresponding to latent variables. the primary role of the latent variables is to allow a complicated distribution over the observed variables to be represented in terms of a model constructed from simpler exponential family conditional distributions. we can interpret such models as expressing the processes by which the observed data arose. for instance consider an object recognition task in which each observed data point corresponds to an image a vector of pixel intensities of one of the objects. in this case the latent variables might have an interpretation as the position and orientation of the object. given a particular observed image our goal is to find the posterior distribution over objects in which we integrate over all possible positions and orientations. we can represent this problem using a graphical model of the form show in figure the graphical model captures the causal process by which the observed data was generated. for this reason such models are often called generative models. by contrast the polynomial regression model described by figure is not generative because there is no probability distribution associated with the input variable x and so it is not possible to generate synthetic data points from this model. we could make it generative by introducing a suitable prior distribution px at the expense of a more complex model. the hidden variables in a probabilistic model need not however have any explicit physical interpretation but may be introduced simply to allow a more complex joint distribution to be constructed from simpler components. in either case the technique of ancestral sampling applied to a generative model mimics the creation of the observed data and would therefore give rise to fantasy data whose probability distribution the model were a perfect representation of reality would be the same as that of the observed data. in practice producing synthetic observations from a generative model can prove informative in understanding the form of the probability distribution represented by that model. discrete variables we have discussed the importance of probability distributions that are members of the exponential family and we have seen that this family includes many wellknown distributions as particular cases. although such distributions are relatively simple they form useful building blocks for constructing more complex probability section bayesian networks figure this fully-connected graph describes a general distribution over two k-state discrete variables having a total of k parameters. by dropping the link between the nodes the number of parameters is reduced to distributions and the framework of graphical models is very useful in expressing the way in which these building blocks are linked together. such models have particularly nice properties if we choose the relationship between each parent-child pair in a directed graph to be conjugate and we shall explore several examples of this shortly. two cases are particularly worthy of note namely when the parent and child node each correspond to discrete variables and when they each correspond to gaussian variables because in these two cases the relationship can be extended hierarchically to construct arbitrarily complex directed acyclic graphs. we begin by examining the discrete case. the probability distribution px for a single discrete variable x having k possible states the representation is given by px xk k and is governed by the parameters kt. due to the constraint k k only k values for k need to be specified in order to define the now suppose that we have two discrete variables and each of which has k states and we wish to model their joint distribution. we denote the probability of observing both and by the parameter kl where denotes the kth component of and similarly for the joint distribution can be written distribution. kl k l kl this distribecause the parameters kl are subject to the constraint bution is governed by k parameters. it is easily seen that the total number of parameters that must be specified for an arbitrary joint distribution over m variables is km and therefore grows exponentially with the number m of variables. using the product rule we can factor the joint distribution in the form which corresponds to a two-node graph with a link going from the node to the node as shown in figure the marginal distribution is governed by k parameters as before similarly the conditional distribution requires the specification of k parameters for each of the k possible values of the total number of parameters that must be specified in the joint distribution is therefore kk k as before. now suppose that the variables and were independent corresponding to the graphical model shown in figure each variable is then described by graphical models figure this chain of m discrete nodes each having k states requires the specification of k parameters which grows linearly with the length m of the chain. in contrast a fully connected graph of m nodes would have km parameters which grows exponentially with m. xm a separate multinomial distribution and the total number of parameters would be for a distribution over m independent discrete variables each having k states the total number of parameters would be mk which therefore grows linearly with the number of variables. from a graphical perspective we have reduced the number of parameters by dropping links in the graph at the expense of having a restricted class of distributions. more generally if we have m discrete variables xm we can model the joint distribution using a directed graph with one variable corresponding to each node. the conditional distribution at each node is given by a set of nonnegative parameters subject to the usual normalization constraint. if the graph is fully connected then we have a completely general distribution having km parameters whereas if there are no links in the graph the joint distribution factorizes into the product of the marginals and the total number of parameters is mk graphs having intermediate levels of connectivity allow for more general distributions than the fully factorized one while requiring fewer parameters than the general joint distribution. as an illustration consider the chain of nodes shown in figure the marginal distribution requires k parameters whereas each of the m conditional distributions pxixi for i m requires kk parameters. this gives a total parameter count of k which is quadratic in k and which grows linearly than exponentially with the length m of the chain. an alternative way to reduce the number of independent parameters in a model is by sharing parameters known as tying of parameters. for instance in the chain example of figure we can arrange that all of the conditional distributions pxixi for i m are governed by the same set of kk parameters. together with the k parameters governing the distribution of this gives a total of k parameters that must be specified in order to define the joint distribution. we can turn a graph over discrete variables into a bayesian model by introducing dirichlet priors for the parameters. from a graphical point of view each node then acquires an additional parent representing the dirichlet distribution over the parameters associated with the corresponding discrete node. this is illustrated for the chain model in figure the corresponding model in which we tie the parameters governing the conditional distributions pxixi for i m is shown in figure another way of controlling the exponential growth in the number of parameters in models of discrete variables is to use parameterized models for the conditional distributions instead of complete tables of conditional probability values. to illustrate this idea consider the graph in figure in which all of the nodes represent binary variables. each of the parent variables xi is governed by a single parame bayesian networks figure an extension of the model of figure to include dirichlet priors over the parameters governing the discrete distributions. figure as in figure but with a single set of parameters shared amongst all of the conditional distributions pxixi m xm xm ter i representing the probability pxi giving m parameters in total for the parent nodes. the conditional distribution xm however would require parameters representing the probability py for each of the possible settings of the parent variables. thus in general the number of parameters required to specify this conditional distribution will grow exponentially with m. we can obtain a more parsimonious form for the conditional distribution by using a logistic sigmoid function acting on a linear combination of the parent variables giving section py xm wixi where a is the logistic sigmoid x xm is an vector of parent states augmented with an additional variable whose value is clamped to and w wm is a vector of m parameters. this is a more restricted form of conditional distribution than the general case but is now governed by a number of parameters that grows linearly with m. in this sense it is analogous to the choice of a restrictive form of covariance matrix example a diagonal matrix in a multivariate gaussian distribution. the motivation for the logistic sigmoid representation was discussed in section figure a graph comprising m parents xm and a single child y used to illustrate the idea of parameterized conditional distributions for discrete variables. xm y graphical models linear-gaussian models in the previous section we saw how to construct joint probability distributions over a set of discrete variables by expressing the variables as nodes in a directed acyclic graph. here we show how a multivariate gaussian can be expressed as a directed graph corresponding to a linear-gaussian model over the component variables. this allows us to impose interesting structure on the distribution with the general gaussian and the diagonal covariance gaussian representing opposite extremes. several widely used techniques are examples of linear-gaussian models such as probabilistic principal component analysis factor analysis and linear dynamical systems and ghahramani we shall make extensive use of the results of this section in later chapters when we consider some of these techniques in detail. consider an arbitrary directed acyclic graph over d variables in which node i represents a single continuous random variable xi having a gaussian distribution. the mean of this distribution is taken to be a linear combination of the states of its parent nodes pai of node i xi j pai pxipai n wijxj bi vi where wij and bi are parameters governing the mean and vi is the variance of the conditional distribution for xi. the log of the joint distribution is then the log of the product of these conditionals over all nodes in the graph and hence takes the form ln px ln pxipai xi wijxj bi j pai const where x xdt and const denotes terms independent of x. we see that this is a quadratic function of the components of x and hence the joint distribution px is a multivariate gaussian. we can determine the mean and covariance of the joint distribution recursively as follows. each variable xi has on the states of its parents a gaussian distribution of the form and so xi wijxj bi vii where is a zero mean unit variance gaussian random variable satisfying ei and eij iij where iij is the i j element of the identity matrix. taking the expectation of we have exi wij exj bi. j pai j pai figure a directed graph over three gaussian variables with one missing link. bayesian networks thus we can find the components of ex exdt by starting at the lowest numbered node and working recursively through the graph we again assume that the nodes are numbered such that each node has a higher number than its parents. similarly we can use and to obtain the i j element of the covariance matrix for px in the form of a recursion relation covxi xj e exixj exj exi k paj e wjkcovxi xk iijvj k paj wjkxk exk vjj and so the covariance can similarly be evaluated recursively starting from the lowest numbered node. let us consider two extreme cases. first of all suppose that there are no links in the graph which therefore comprises d isolated nodes. in this case there are no parameters wij and so there are just d parameters bi and d parameters vi. from the recursion relations and we see that the mean of px is given by bdt and the covariance matrix is diagonal of the form vd. the joint distribution has a total of parameters and represents a set of d independent univariate gaussian distributions. now consider a fully connected graph in which each node has all lower numbered nodes as parents. the matrix wij then has i entries on the ith row and hence is a lower triangular matrix no entries on the leading diagonal. then the total number of parameters wij is obtained by taking the number of elements in a d d matrix subtracting d to account for the absence of elements on the leading diagonal and then dividing by because the matrix has elements only below the diagonal giving a total of dd the total number of independent parameters and in the covariance matrix is therefore dd corresponding to a general symmetric covariance matrix. graphs having some intermediate level of complexity correspond to joint gaussian distributions with partially constrained covariance matrices. consider for example the graph shown in figure which has a link missing between variables and using the recursion relations and we see that the mean and covariance of the joint distribution are given by section exercise graphical models we can readily extend the linear-gaussian graphical model to the case in which the nodes of the graph represent multivariate gaussian variables. in this case we can write the conditional distribution for node i in the form pxipai n wijxj bi i xi j pai section where now wij is a matrix is nonsquare if xi and xj have different dimensionalities. again it is easy to verify that the joint distribution over all variables is gaussian. note that we have already encountered a specific example of the linear-gaussian relationship when we saw that the conjugate prior for the mean of a gaussian variable x is itself a gaussian distribution over the joint distribution over x and is therefore gaussian. this corresponds to a simple two-node graph in which the node representing is the parent of the node representing x. the mean of the distribution over is a parameter controlling a prior and so it can be viewed as a hyperparameter. because the value of this hyperparameter may itself be unknown we can again treat it from a bayesian perspective by introducing a prior over the hyperparameter sometimes called a hyperprior which is again given by a gaussian distribution. this type of construction can be extended in principle to any level and is an illustration of a hierarchical bayesian model of which we shall encounter further examples in later chapters. conditional independence an important concept for probability distributions over multiple variables is that of conditional independence consider three variables a b and c and suppose that the conditional distribution of a given b and c is such that it does not depend on the value of b so that pab c pac. we say that a is conditionally independent of b given c. this can be expressed in a slightly different way if we consider the joint distribution of a and b conditioned on c which we can write in the form pa bc pab cpbc pacpbc. where we have used the product rule of probability together with thus we see that conditioned on c the joint distribution of a and b factorizes into the product of the marginal distribution of a and the marginal distribution of b both conditioned on c. this says that the variables a and b are statistically independent given c. note that our definition of conditional independence will require that conditional independence figure the first of three examples of graphs over three variables a b and c used to discuss conditional independence properties of directed graphical models. c a b or equivalently must hold for every possible value of c and not just for some values. we shall sometimes use a shorthand notation for conditional independence in which a b c denotes that a is conditionally independent of b given c and is equivalent to conditional independence properties play an important role in using probabilistic models for pattern recognition by simplifying both the structure of a model and the computations needed to perform inference and learning under that model. we shall see examples of this shortly. if we are given an expression for the joint distribution over a set of variables in terms of a product of conditional distributions the mathematical representation underlying a directed graph then we could in principle test whether any potential conditional independence property holds by repeated application of the sum and product rules of probability. in practice such an approach would be very time consuming. an important and elegant feature of graphical models is that conditional independence properties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. the general framework for achieving this is called d-separation where the d stands for directed here we shall motivate the concept of d-separation and give a general statement of the d-separation criterion. a formal proof can be found in lauritzen three example graphs we begin our discussion of the conditional independence properties of directed graphs by considering three simple examples each involving graphs having just three nodes. together these will motivate and illustrate the key concepts of d-separation. the first of the three examples is shown in figure and the joint distribution corresponding to this graph is easily written down using the general result to give if none of the variables are observed then we can investigate whether a and b are independent by marginalizing both sides of with respect to c to give pa b c pacpbcpc. pa b pacpbcpc. c in general this does not factorize into the product papb and so a b graphical models figure as in figure but where we have conditioned on the value of variable c. c a b where denotes the empty set and the symbol means that the conditional independence property does not hold in general. of course it may hold for a particular distribution by virtue of the specific numerical values associated with the various conditional probabilities but it does not follow in general from the structure of the graph. now suppose we condition on the variable c as represented by the graph of figure from we can easily write down the conditional distribution of a and b given c in the form pa bc pa b c pc pacpbc and so we obtain the conditional independence property a b c. we can provide a simple graphical interpretation of this result by considering the path from node a to node b via c. the node c is said to be tail-to-tail with respect to this path because the node is connected to the tails of the two arrows and the presence of such a path connecting nodes a and b causes these nodes to be dependent. however when we condition on node c as in figure the conditioned node blocks the path from a to b and causes a and b to become independent. we can similarly consider the graph shown in figure the joint distribution corresponding to this graph is again obtained from our general formula to give pa b c papcapbc. first of all suppose that none of the variables are observed. again we can test to see if a and b are independent by marginalizing over c to give pa b pa pcapbc papba. c figure the second of our three examples of graphs used to motivate the conditional independence framework for directed graphical models. a c b figure as in figure but now conditioning on node c. a c conditional independence which in general does not factorize into papb and so a b b as before. now suppose we condition on node c as shown in figure using bayes theorem together with we obtain pa bc pa b c pc papcapbc pc pacpbc and so again we obtain the conditional independence property a b c. as before we can interpret these results graphically. the node c is said to be head-to-tail with respect to the path from node a to node b. such a path connects nodes a and b and renders them dependent. if we now observe c as in figure then this observation blocks the path from a to b and so we obtain the conditional independence property a b c. finally we consider the third of our examples shown by the graph in figure as we shall see this has a more subtle behaviour than the two previous graphs. the joint distribution can again be written down using our general result to give pa b c papbpca b. consider first the case where none of the variables are observed. marginalizing both sides of over c we obtain pa b papb figure the last of our three examples of graphs used to explore conditional independence properties in graphical models. this graph has rather different properties from the two previous examples. a b c graphical models figure as in figure but conditioning on the value of node c. in this graph the act of conditioning induces a dependence between a and b. a b c and so a and b are independent with no variables observed in contrast to the two previous examples. we can write this result as a b now suppose we condition on c as indicated in figure the conditional distribution of a and b is then given by pa bc pa b c pc papbpca b pc exercise which in general does not factorize into the product papb and so a b c. thus our third example has the opposite behaviour from the first two. graphically we say that node c is head-to-head with respect to the path from a to b because it connects to the heads of the two arrows. when node c is unobserved it blocks the path and the variables a and b are independent. however conditioning on c unblocks the path and renders a and b dependent. there is one more subtlety associated with this third example that we need to consider. first we introduce some more terminology. we say that node y is a descendant of node x if there is a path from x to y in which each step of the path follows the directions of the arrows. then it can be shown that a head-to-head path will become unblocked if either the node or any of its descendants is observed. in summary a tail-to-tail node or a head-to-tail node leaves a path unblocked unless it is observed in which case it blocks the path. by contrast a head-to-head node blocks a path if it is unobserved but once the node andor at least one of its descendants is observed the path becomes unblocked. it is worth spending a moment to understand further the unusual behaviour of the graph of figure consider a particular instance of such a graph corresponding to a problem with three binary random variables relating to the fuel system on a car as shown in figure the variables are called b representing the state of a battery that is either charged or flat f representing the state of the fuel tank that is either full of fuel or empty and g which is the state of an electric fuel gauge and which indicates either full or empty b f b f b f conditional independence g g g figure an example of a graph used to illustrate the phenomenon of explaining away the three nodes represent the state of the battery the state of the fuel tank and the reading on the electric fuel gauge see the text for details. the battery is either charged or flat and independently the fuel tank is either full or empty with prior probabilities pb pf given the state of the fuel tank and the battery the fuel gauge reads full with probabilities given by pg f pg f pg f pg f so this is a rather unreliable fuel gauge! all remaining probabilities are determined by the requirement that probabilities sum to one and so we have a complete specification of the probabilistic model. before we observe any data the prior probability of the fuel tank being empty is pf now suppose that we observe the fuel gauge and discover that it reads empty i.e. g corresponding to the middle graph in figure we can use bayes theorem to evaluate the posterior probability of the fuel tank being empty. first we evaluate the denominator for bayes theorem given by pg f pg b f b and similarly we evaluate pg pg f and using these results we have pf pg pg graphical models and so pf pf thus observing that the gauge reads empty makes it more likely that the tank is indeed empty as we would intuitively expect. next suppose that we also check the state of the battery and find that it is flat i.e. b we have now observed the states of both the fuel gauge and the battery as shown by the right-hand graph in figure the posterior probability that the fuel tank is empty given the observations of both the fuel gauge and the battery state is then given by pf b pg f f pg f where the prior probability pb has cancelled between numerator and denominator. thus the probability that the tank is empty has decreased to as a result of the observation of the state of the battery. this accords with our intuition that finding out that the battery is flat explains away the observation that the fuel gauge reads empty. we see that the state of the fuel tank and that of the battery have indeed become dependent on each other as a result of observing the reading on the fuel gauge. in fact this would also be the case if instead of observing the fuel gauge directly we observed the state of some descendant of g. note that the probability pf b is greater than the prior probability pf because the observation that the fuel gauge reads zero still provides some evidence in favour of an empty fuel tank. d-separation we now give a general statement of the d-separation property for directed graphs. consider a general directed graph in which a b and c are arbitrary nonintersecting sets of nodes union may be smaller than the complete set of nodes in the graph. we wish to ascertain whether a particular conditional independence statement a b c is implied by a given directed acyclic graph. to do so we consider all possible paths from any node in a to any node in b. any such path is said to be blocked if it includes a node such that either the arrows on the path meet either head-to-tail or tail-to-tail at the node and the node is in the set c or the arrows meet head-to-head at the node and neither the node nor any of its descendants is in the set c. if all paths are blocked then a is said to be d-separated from b by c and the joint distribution over all of the variables in the graph will satisfy a b c. the concept of d-separation is illustrated in figure in graph the path from a to b is not blocked by node f because it is a tail-to-tail node for this path and is not observed nor is it blocked by node e because although the latter is a head-to-head node it has a descendant c because is in the conditioning set. thus the conditional independence statement a b c does not follow from this graph. in graph the path from a to b is blocked by node f because this is a tail-to-tail node that is observed and so the conditional independence property a b f will figure illustration of the concept of d-separation. see the text for details. a f a f conditional independence b e c b e c be satisfied by any distribution that factorizes according to this graph. note that this path is also blocked by node e because e is a head-to-head node and neither it nor its descendant are in the conditioning set. for the purposes of d-separation parameters such as and in figure indicated by small filled circles behave in the same was as observed nodes. however there are no marginal distributions associated with such nodes. consequently parameter nodes never themselves have parents and so all paths through these nodes will always be tail-to-tail and hence blocked. consequently they play no role in d-separation. section another example of conditional independence and d-separation is provided by the concept of i.i.d. identically distributed data introduced in section consider the problem of finding the posterior distribution for the mean of a univariate gaussian distribution. this can be represented by the directed graph shown in figure in which the joint distribution is defined by a prior p together with a set of conditional distributions pxn for n n. in practice we observe d xn and our goal is to infer suppose for a moment that we condition on and consider the joint distribution of the observations. using d-separation we note that there is a unique path from any xi to any other and that this path is tail-to-tail with respect to the observed node every such path is blocked and so the observations d xn are independent given so that figure directed graph corresponding to the problem of inferring the mean of a univariate gaussian distribution from observations xn the same graph drawn using the plate notation. pd pxn xn xn n n graphical models figure a graphical representation of the naive bayes conditioned on the model class label z the components of the observed vector x xdt are assumed to be independent. for classification. z xd however if we integrate over the observations are in general no longer independent pd pd d pxn. section independence property here is a latent variable because its value is not observed. another example of a model representing i.i.d. data is the graph in figure corresponding to bayesian polynomial regression. here the stochastic nodes corre spond to w we see that the node for w is tail-to-tail with respect to the path to any one of the nodes tn and so we have the following conditional tn w. is independent of the training data tn. we can therefore first use the predictions for new input thus conditioned on the polynomial coefficients w the predictive distribution for training data to determine the posterior distribution over the coefficients w and then we can discard the training data and use the posterior distribution for w to make a related graphical structure arises in an approach to classification called the naive bayes model in which we use conditional independence assumptions to simplify the model structure. suppose our observed variable consists of a d-dimensional vector x xdt and we wish to assign observed values of x to one of k classes. using the encoding scheme we can represent these classes by a kdimensional binary vector z. we can then define a generative model by introducing a multinomial prior pz over the class labels where the kth component k of is the prior probability of class ck together with a conditional distribution pxz for the observed vector x. the key assumption of the naive bayes model is that conditioned on the class z the distributions of the input variables xd are independent. the graphical representation of this model is shown in figure we see that observation of z blocks the path between xi and xj for j i such paths are tail-to-tail at the node z and so xi and xj are conditionally independent given z. if however we marginalize out z that z is unobserved the tail-to-tail path from xi to xj is no longer blocked. this tells us that in general the marginal density px will not factorize with respect to the components of x. we encountered a simple application of the naive bayes model in the context of fusing data from different sources for medical diagnosis in section if we are given a labelled training set comprising inputs xn together with their class labels then we can fit the naive bayes model to the training data conditional independence using maximum likelihood assuming that the data are drawn independently from the model. the solution is obtained by fitting the model for each class separately using the correspondingly labelled data. as an example suppose that the probability density within each class is chosen to be gaussian. in this case the naive bayes assumption then implies that the covariance matrix for each gaussian is diagonal and the contours of constant density within each class will be axis-aligned ellipsoids. the marginal density however is given by a superposition of diagonal gaussians weighting coefficients given by the class priors and so will no longer factorize with respect to its components. the naive bayes assumption is helpful when the dimensionality d of the input space is high making density estimation in the full d-dimensional space more challenging. it is also useful if the input vector contains both discrete and continuous variables since each can be represented separately using appropriate models bernoulli distributions for binary observations or gaussians for real-valued variables. the conditional independence assumption of this model is clearly a strong one that may lead to rather poor representations of the class-conditional densities. nevertheless even if this assumption is not precisely satisfied the model may still give good classification performance in practice because the decision boundaries can be insensitive to some of the details in the class-conditional densities as illustrated in figure we have seen that a particular directed graph represents a specific decomposition of a joint probability distribution into a product of conditional probabilities. the graph also expresses a set of conditional independence statements obtained through the d-separation criterion and the d-separation theorem is really an expression of the equivalence of these two properties. in order to make this clear it is helpful to think of a directed graph as a filter. suppose we consider a particular joint probability distribution px over the variables x corresponding to the nodes of the graph. the filter will allow this distribution to pass through if and only if it can be expressed in terms of the factorization implied by the graph. if we present to the filter the set of all possible distributions px over the set of variables x then the subset of distributions that are passed by the filter will be denoted df for directed factorization. this is illustrated in figure alternatively we can use the graph as a different kind of filter by first listing all of the conditional independence properties obtained by applying the d-separation criterion to the graph and then allowing a distribution to pass only if it satisfies all of these properties. if we present all possible distributions px to this second kind of filter then the d-separation theorem tells us that the set of distributions that will be allowed through is precisely the set df. it should be emphasized that the conditional independence properties obtained from d-separation apply to any probabilistic model described by that particular directed graph. this will be true for instance whether the variables are discrete or continuous or a combination of these. again we see that a particular graph is describing a whole family of probability distributions. at one extreme we have a fully connected graph that exhibits no conditional independence properties at all and which can represent any possible joint probability distribution over the given variables. the set df will contain all possible distribu graphical models px df figure we can view a graphical model this case a directed graph as a filter in which a probability distribution px is allowed through the filter if and only if it satisfies the directed factorization property the set of all possible probability distributions px that pass through the filter is denoted df. we can alternatively use the graph to filter distributions according to whether they respect all of the conditional independencies implied by the d-separation properties of the graph. the d-separation theorem says that it is the same set of distributions df that will be allowed through this second kind of filter. tions px. at the other extreme we have the fully disconnected graph i.e. one having no links at all. this corresponds to joint distributions which factorize into the product of the marginal distributions over the variables comprising the nodes of the graph. note that for any given graph the set of distributions df will include any distributions that have additional independence properties beyond those described by the graph. for instance a fully factorized distribution will always be passed through the filter implied by any graph over the corresponding set of variables. we end our discussion of conditional independence properties by exploring the concept of a markov blanket or markov boundary. consider a joint distribution xd represented by a directed graph having d nodes and consider the conditional distribution of a particular node with variables xi conditioned on all of the remaining variables using the factorization property we can express this conditional distribution in the form xd k xd dxi pxkpak pxkpak dxi k in which the integral is replaced by a summation in the case of discrete variables. we now observe that any factor pxkpak that does not have any functional dependence on xi can be taken outside the integral over xi and will therefore cancel between numerator and denominator. the only factors that remain will be the conditional distribution pxipai for node xi itself together with the conditional distributions for any nodes xk such that node xi is in the conditioning set of pxkpak in other words for which xi is a parent of xk. the conditional pxipai will depend on the parents of node xi whereas the conditionals pxkpak will depend on the children markov random fields figure the markov blanket of a node xi comprises the set of parents children and co-parents of the node. it has the property that the conditional distribution of xi conditioned on all the remaining variables in the graph is dependent only on the variables in the markov blanket. xi of xi as well as on the co-parents in other words variables corresponding to parents of node xk other than node xi. the set of nodes comprising the parents the children and the co-parents is called the markov blanket and is illustrated in figure we can think of the markov blanket of a node xi as being the minimal set of nodes that isolates xi from the rest of the graph. note that it is not sufficient to include only the parents and children of node xi because the phenomenon of explaining away means that observations of the child nodes will not block paths to the co-parents. we must therefore observe the co-parent nodes also. markov random fields we have seen that directed graphical models specify a factorization of the joint distribution over a set of variables into a product of local conditional distributions. they also define a set of conditional independence properties that must be satisfied by any distribution that factorizes according to the graph. we turn now to the second major class of graphical models that are described by undirected graphs and that again specify both a factorization and a set of conditional independence relations. a markov random field also known as a markov network or an undirected graphical model and snell has a set of nodes each of which corresponds to a variable or group of variables as well as a set of links each of which connects a pair of nodes. the links are undirected that is they do not carry arrows. in the case of undirected graphs it is convenient to begin with a discussion of conditional independence properties. conditional independence properties in the case of directed graphs we saw that it was possible to test whether a particular conditional independence property holds by applying a graphical test called d-separation. this involved testing whether or not the paths connecting two sets of nodes were blocked the definition of blocked however was somewhat subtle due to the presence of paths having head-to-head nodes. we might ask whether it is possible to define an alternative graphical semantics for probability distributions such that conditional independence is determined by simple graph separation. this is indeed the case and corresponds to undirected graphical models. by removing the section graphical models figure an example of an undirected graph in which every path from any node in set a to any node in set b passes through at least one node in set c. consequently the conditional independence property a b c holds for any probability distribution described by this graph. c b a directionality from the links of the graph the asymmetry between parent and child nodes is removed and so the subtleties associated with head-to-head nodes no longer arise. suppose that in an undirected graph we identify three sets of nodes denoted a b and c and that we consider the conditional independence property a b c. to test whether this property is satisfied by a probability distribution defined by a graph we consider all possible paths that connect nodes in set a to nodes in set b. if all such paths pass through one or more nodes in set c then all such paths are blocked and so the conditional independence property holds. however if there is at least one such path that is not blocked then the property does not necessarily hold or more precisely there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation. this is illustrated with an example in figure note that this is exactly the same as the d-separation criterion except that there is no explaining away phenomenon. testing for conditional independence in undirected graphs is therefore simpler than in directed graphs. an alternative way to view the conditional independence test is to imagine removing all nodes in set c from the graph together with any links that connect to those nodes. we then ask if there exists a path that connects any node in a to any node in b. if there are no such paths then the conditional independence property must hold. the markov blanket for an undirected graph takes a particularly simple form because a node will be conditionally independent of all other nodes conditioned only on the neighbouring nodes as illustrated in figure factorization properties we now seek a factorization rule for undirected graphs that will correspond to the above conditional independence test. again this will involve expressing the joint distribution px as a product of functions defined over sets of variables that are local to the graph. we therefore need to decide what is the appropriate notion of locality in this case. markov random fields figure for an undirected graph the markov blanket of a node xi consists of the set of neighbouring nodes. it has the property that the conditional distribution of xi conditioned on all the remaining variables in the graph is dependent only on the variables in the markov blanket. if we consider two nodes xi and xj that are not connected by a link then these variables must be conditionally independent given all other nodes in the graph. this follows from the fact that there is no direct path between the two nodes and all other paths pass through nodes that are observed and hence those paths are blocked. this conditional independence property can be expressed as pxi xjxij pxixijpxjxij where xij denotes the set x of all variables with xi and xj removed. the factorization of the joint distribution must therefore be such that xi and xj do not appear in the same factor in order for the conditional independence property to hold for all possible distributions belonging to the graph. this leads us to consider a graphical concept called a clique which is defined as a subset of the nodes in a graph such that there exists a link between all pairs of nodes in the subset. in other words the set of nodes in a clique is fully connected. furthermore a maximal clique is a clique such that it is not possible to include any other nodes from the graph in the set without it ceasing to be a clique. these concepts are illustrated by the undirected graph over four variables shown in figure this graph has five cliques of two nodes given by and as well as two maximal cliques given by and the set is not a clique because of the missing link from to we can therefore define the factors in the decomposition of the joint distribution to be functions of the variables in the cliques. in fact we can consider functions of the maximal cliques without loss of generality because other cliques must be subsets of maximal cliques. thus if is a maximal clique and we define an arbitrary function over this clique then including another factor defined over a subset of these variables would be redundant. let us denote a clique by c and the set of variables in that clique by xc. then figure a four-node undirected graph showing a clique in green and a maximal clique in blue. graphical models the joint distribution is written as a product of potential functions cxc over the maximal cliques of the graph px cxc. c z here the quantity z sometimes called the partition function is a normalization constant and is given by z cxc x c which ensures that the distribution px given by is correctly normalized. by considering only potential functions which satisfy cxc we ensure that px in we have assumed that x comprises discrete variables but the framework is equally applicable to continuous variables or a combination of the two in which the summation is replaced by the appropriate combination of summation and integration. note that we do not restrict the choice of potential functions to those that have a specific probabilistic interpretation as marginal or conditional distributions. this is in contrast to directed graphs in which each factor represents the conditional distribution of the corresponding variable conditioned on the state of its parents. however in special cases for instance where the undirected graph is constructed by starting with a directed graph the potential functions may indeed have such an interpretation as we shall see shortly. one consequence of the generality of the potential functions cxc is that their product will in general not be correctly normalized. we therefore have to introduce an explicit normalization factor given by recall that for directed graphs the joint distribution was automatically normalized as a consequence of the normalization of each of the conditional distributions in the factorization. the presence of this normalization constant is one of the major limitations of undirected graphs. if we have a model with m discrete nodes each having k states then the evaluation of the normalization term involves summing over km states and so the worst case is exponential in the size of the model. the partition function is needed for parameter learning because it will be a function of any parameters that govern the potential functions cxc. however for evaluation of local conditional distributions the partition function is not needed because a conditional is the ratio of two marginals and the partition function cancels between numerator and denominator when evaluating this ratio. similarly for evaluating local marginal probabilities we can work with the unnormalized joint distribution and then normalize the marginals explicitly at the end. provided the marginals only involves a small number of variables the evaluation of their normalization coefficient will be feasible. so far we have discussed the notion of conditional independence based on simple graph separation and we have proposed a factorization of the joint distribution that is intended to correspond to this conditional independence structure. however we have not made any formal connection between conditional independence and factorization for undirected graphs. to do so we need to restrict attention to potential functions cxc that are strictly positive never zero or negative for any markov random fields choice of xc. given this restriction we can make a precise relationship between factorization and conditional independence. to do this we again return to the concept of a graphical model as a filter corresponding to figure consider the set of all possible distributions defined over a fixed set of variables corresponding to the nodes of a particular undirected graph. we can define ui to be the set of such distributions that are consistent with the set of conditional independence statements that can be read from the graph using graph separation. similarly we can define uf to be the set of such distributions that can be expressed as a factorization of the form with respect to the maximal cliques of the graph. the hammersley-clifford theorem states that the sets ui and uf are identical. because we are restricted to potential functions which are strictly positive it is convenient to express them as exponentials so that cxc exp exc where exc is called an energy function and the exponential representation is called the boltzmann distribution. the joint distribution is defined as the product of potentials and so the total energy is obtained by adding the energies of each of the maximal cliques. in contrast to the factors in the joint distribution for a directed graph the potentials in an undirected graph do not have a specific probabilistic interpretation. although this gives greater flexibility in choosing the potential functions because there is no normalization constraint it does raise the question of how to motivate a choice of potential function for a particular application. this can be done by viewing the potential function as expressing which configurations of the local variables are preferred to others. global configurations that have a relatively high probability are those that find a good balance in satisfying the conflicting influences of the clique potentials. we turn now to a specific example to illustrate the use of undirected graphs. illustration image de-noising we can illustrate the application of undirected graphs using an example of noise removal from a binary image geman and geman besag although a very simple example this is typical of more sophisticated applications. let the observed noisy image be described by an array of binary pixel values yi where the index i d runs over all pixels. we shall suppose that the image is obtained by taking an unknown noise-free image described by binary pixel values xi and randomly flipping the sign of pixels with some small probability. an example binary image together with a noise corrupted image obtained by flipping the sign of the pixels with probability is shown in figure given the noisy image our goal is to recover the original noise-free image. because the noise level is small we know that there will be a strong correlation between xi and yi. we also know that neighbouring pixels xi and xj in an image are strongly correlated. this prior knowledge can be captured using the markov graphical models figure illustration of image de-noising using a markov random field. the top row shows the original binary image on the left and the corrupted image after randomly changing of the pixels on the right. the bottom row shows the restored images obtained using iterated conditional models on the left and using the graph-cut algorithm on the right. icm produces an image where of the pixels agree with the original image whereas the corresponding number for graph-cut is random field model whose undirected graph is shown in figure this graph has two types of cliques each of which contains two variables. the cliques of the form yi have an associated energy function that expresses the correlation between these variables. we choose a very simple energy function for these cliques of the form xiyi where is a positive constant. this has the desired effect of giving a lower energy encouraging a higher probability when xi and yi have the same sign and a higher energy when they have the opposite sign. the remaining cliques comprise pairs of variables xj where i and j are indices of neighbouring pixels. again we want the energy to be lower when the pixels have the same sign than when they have the opposite sign and so we choose an energy given by xixj where is a positive constant. because a potential function is an arbitrary nonnegative function over a maximal clique we can multiply it by any nonnegative functions of subsets of the clique or markov random fields figure an undirected graphical model representing a markov random field for image de-noising in which xi is a binary variable denoting the state of pixel i in the unknown noise-free image and yi denotes the corresponding value of pixel i in the observed noisy image. yi xi equivalently we can add the corresponding energies. in this example this allows us to add an extra term hxi for each pixel i in the noise-free image. such a term has the effect of biasing the model towards pixel values that have one particular sign in preference to the other. the complete energy function for the model then takes the form i ex y h xi xixj which defines a joint distribution over x and y given by exp ex y. px y z xiyi i we now fix the elements of y to the observed values given by the pixels of the noisy image which implicitly defines a conditional distribution pxy over noisefree images. this is an example of the ising model which has been widely studied in statistical physics. for the purposes of image restoration we wish to find an image x having a high probability the maximum probability. to do this we shall use a simple iterative technique called iterated conditional modes or icm and f oglein which is simply an application of coordinate-wise gradient ascent. the idea is first to initialize the variables which we do by simply setting xi yi for all i. then we take one node xj at a time and we evaluate the total energy for the two possible states xj and xj keeping all other node variables fixed and set xj to whichever state has the lower energy. this will either leave the probability unchanged if xj is unchanged or will increase it. because only one variable is changed this is a simple local computation that can be performed efficiently. we then repeat the update for another site and so on until some suitable stopping criterion is satisfied. the nodes may be updated in a systematic way for instance by repeatedly raster scanning through the image or by choosing nodes at random. if we have a sequence of updates in which every site is visited at least once and in which no changes to the variables are made then by definition the algorithm exercise graphical models figure example of a directed graph. the equivalent undirected graph. xn xn xn xn will have converged to a local maximum of the probability. this need not however correspond to the global maximum. for the purposes of this simple illustration we have fixed the parameters to be and h note that leaving h simply means that the prior probabilities of the two states of xi are equal. starting with the observed noisy image as the initial configuration we run icm until convergence leading to the de-noised image shown in the lower left panel of figure note that if we set which effectively removes the links between neighbouring pixels then the global most probable solution is given by xi yi for all i corresponding to the observed noisy image. later we shall discuss a more effective algorithm for finding high probability solutions called the max-product algorithm which typically leads to better solutions although this is still not guaranteed to find the global maximum of the posterior distribution. however for certain classes of model including the one given by there exist efficient algorithms based on graph cuts that are guaranteed to find the global maximum et al. boykov et al. kolmogorov and zabih the lower right panel of figure shows the result of applying a graph-cut algorithm to the de-noising problem. exercise section relation to directed graphs we have introduced two graphical frameworks for representing probability distributions corresponding to directed and undirected graphs and it is instructive to discuss the relation between these. consider first the problem of taking a model that is specified using a directed graph and trying to convert it to an undirected graph. in some cases this is straightforward as in the simple example in figure here the joint distribution for the directed graph is given as a product of conditionals in the form px pxnxn now let us convert this to an undirected graph representation as shown in figure in the undirected graph the maximal cliques are simply the pairs of neighbouring nodes and so from we wish to write the joint distribution in the form px z n xn. figure example of a simple directed graph and the corresponding moral graph markov random fields this is easily done by identifying n xn pxnxn where we have absorbed the marginal for the first node into the first potential function. note that in this case the partition function z let us consider how to generalize this construction so that we can convert any distribution specified by a factorization over a directed graph into one specified by a factorization over an undirected graph. this can be achieved if the clique potentials of the undirected graph are given by the conditional distributions of the directed graph. in order for this to be valid we must ensure that the set of variables that appears in each of the conditional distributions is a member of at least one clique of the undirected graph. for nodes on the directed graph having just one parent this is achieved simply by replacing the directed link with an undirected link. however for nodes in the directed graph having more than one parent this is not sufficient. these are nodes that have head-to-head paths encountered in our discussion of conditional independence. consider a simple directed graph over nodes shown in figure the joint distribution for the directed graph takes the form px we see that the factor involves the four variables and and so these must all belong to a single clique if this conditional distribution is to be absorbed into a clique potential. to ensure this we add extra links between all pairs of parents of the node anachronistically this process of marrying the parents has become known as moralization and the resulting undirected graph after dropping the arrows is called the moral graph. it is important to observe that the moral graph in this example is fully connected and so exhibits no conditional independence properties in contrast to the original directed graph. thus in general to convert a directed graph into an undirected graph we first add additional undirected links between all pairs of parents for each node in the graph and graphical models section section then drop the arrows on the original links to give the moral graph. then we initialize all of the clique potentials of the moral graph to we then take each conditional distribution factor in the original directed graph and multiply it into one of the clique potentials. there will always exist at least one maximal clique that contains all of the variables in the factor as a result of the moralization step. note that in all cases the partition function is given by z the process of converting a directed graph into an undirected graph plays an important role in exact inference techniques such as the junction tree algorithm. converting from an undirected to a directed representation is much less common and in general presents problems due to the normalization constraints. we saw that in going from a directed to an undirected representation we had to discard some conditional independence properties from the graph. of course we could always trivially convert any distribution over a directed graph into one over an undirected graph by simply using a fully connected undirected graph. this would however discard all conditional independence properties and so would be vacuous. the process of moralization adds the fewest extra links and so retains the maximum number of independence properties. we have seen that the procedure for determining the conditional independence properties is different between directed and undirected graphs. it turns out that the two types of graph can express different conditional independence properties and it is worth exploring this issue in more detail. to do so we return to the view of a specific or undirected graph as a filter so that the set of all possible distributions over the given variables could be reduced to a subset that respects the conditional independencies implied by the graph. a graph is said to be a d map dependency map of a distribution if every conditional independence statement satisfied by the distribution is reflected in the graph. thus a completely disconnected graph links will be a trivial d map for any distribution. alternatively we can consider a specific distribution and ask which graphs have the appropriate conditional independence properties. if every conditional independence statement implied by a graph is satisfied by a specific distribution then the graph is said to be an i map independence map of that distribution. clearly a fully connected graph will be a trivial i map for any distribution. if it is the case that every conditional independence property of the distribution is reflected in the graph and vice versa then the graph is said to be a perfect map for figure venn diagram illustrating the set of all distributions p over a given set of variables together with the set of distributions d that can be represented as a perfect map using a directed graph and the set u that can be represented as a perfect map using an undirected graph. d u p inference in graphical models figure a directed graph whose conditional independence properties cannot be expressed using an undirected graph over the same three variables. a b c that distribution. a perfect map is therefore both an i map and a d map. consider the set of distributions such that for each distribution there exists a directed graph that is a perfect map. this set is distinct from the set of distributions such that for each distribution there exists an undirected graph that is a perfect map. in addition there are distributions for which neither directed nor undirected graphs offer a perfect map. this is illustrated as a venn diagram in figure figure shows an example of a directed graph that is a perfect map for a distribution satisfying the conditional independence properties a b and a b c. there is no corresponding undirected graph over the same three variables that is a perfect map. conversely consider the undirected graph over four variables shown in figure this graph exhibits the properties a b c d a b and a b c d. there is no directed graph over four variables that implies the same set of conditional independence properties. the graphical framework can be extended in a consistent way to graphs that include both directed and undirected links. these are called chain graphs and wermuth frydenberg and contain the directed and undirected graphs considered so far as special cases. although such graphs can represent a broader class of distributions than either directed or undirected alone there remain distributions for which even a chain graph cannot provide a perfect map. chain graphs are not discussed further in this book. figure an undirected graph whose conditional independence properties cannot be expressed in terms of a directed graph over the same variables. a b c d inference in graphical models we turn now to the problem of inference in graphical models in which some of the nodes in a graph are clamped to observed values and we wish to compute the posterior distributions of one or more subsets of other nodes. as we shall see we can exploit the graphical structure both to find efficient algorithms for inference and graphical models figure a graphical representation of bayes see the text for details. theorem. x y x y x y to make the structure of those algorithms transparent. specifically we shall see that many algorithms can be expressed in terms of the propagation of local messages around the graph. in this section we shall focus primarily on techniques for exact inference and in chapter we shall consider a number of approximate inference algorithms. to start with let us consider the graphical interpretation of bayes theorem. suppose we decompose the joint distribution px y over two variables x and y into a product of factors in the form px y pxpyx. this can be represented by the directed graph shown in figure now suppose we observe the value of y as indicated by the shaded node in figure we can view the marginal distribution px as a prior over the latent variable x and our goal is to infer the corresponding posterior distribution over x. using the sum and product rules of probability we can evaluate py pyx which can then be used in bayes theorem to calculate pxy pyxpx thus the joint distribution is now expressed in terms of py and pxy. from a graphical perspective the joint distribution px y is now represented by the graph shown in figure in which the direction of the arrow is reversed. this is the simplest example of an inference problem for a graphical model. py inference on a chain now consider a more complex problem involving the chain of nodes of the form shown in figure this example will lay the foundation for a discussion of exact inference in more general graphs later in this section. specifically we shall consider the undirected graph in figure we have already seen that the directed chain can be transformed into an equivalent undirected chain. because the directed graph does not have any nodes with more than one parent this does not require the addition of any extra links and the directed and undirected versions of this graph express exactly the same set of conditional independence statements. inference in graphical models the joint distribution for this graph takes the form px z n xn. we shall consider the specific case in which the n nodes represent discrete variables each having k states in which case each potential function n xn comprises an k k table and so the joint distribution has parameters. let us consider the inference problem of finding the marginal distribution pxn for a specific node xn that is part way along the chain. note that for the moment there are no observed nodes. by definition the required marginal is obtained by summing the joint distribution over all variables except xn so that pxn px. xn xn in a naive implementation we would first evaluate the joint distribution and then perform the summations explicitly. the joint distribution can be represented as a set of numbers one for each possible value for x. because there are n variables each with k states there are k n values for x and so evaluation and storage of the joint distribution as well as marginalization to obtain pxn all involve storage and computation that scale exponentially with the length n of the chain. we can however obtain a much more efficient algorithm by exploiting the conditional independence properties of the graphical model. if we substitute the factorized expression for the joint distribution into then we can rearrange the order of the summations and the multiplications to allow the required marginal to be evaluated much more efficiently. consider for instance the summation over xn the potential n xn is the only one that depends on xn and so we can perform the summation n xn xn first to give a function of xn we can then use this to perform the summation over xn which will involve only this new function together with the potential n xn because this is the only other place that xn appears. similarly the summation over involves only the potential and so can be performed separately to give a function of and so on. because each summation effectively removes a variable from the distribution this can be viewed as the removal of a node from the graph. if we group the potentials and summations together in this way we can express graphical models the desired marginal in the form n xn pxn xn z xn n xn the reader is encouraged to study this re-ordering carefully as the underlying idea forms the basis for the later discussion of the general sum-product algorithm. here the key concept that we are exploiting is that multiplication is distributive over addition so that ab ac ab c in which the left-hand side involves three arithmetic operations whereas the righthand side reduces this to two operations. let us work out the computational cost of evaluating the required marginal using this re-ordered expression. we have to perform n summations each of which is over k states and each of which involves a function of two variables. for instance the summation over involves only the function which is a table of k k numbers. we have to sum this table over for each value of and so this has ok cost. the resulting vector of k numbers is multiplied by the matrix of numbers and so is again ok because there are n summations and multiplications of this kind the total cost of evaluating the marginal pxn is on k this is linear in the length of the chain in contrast to the exponential cost of a naive approach. we have therefore been able to exploit the many conditional independence properties of this simple graph in order to obtain an efficient calculation. if the graph had been fully connected there would have been no conditional independence properties and we would have been forced to work directly with the full joint distribution. we now give a powerful interpretation of this calculation in terms of the passing of local messages around on the graph. from we see that the expression for the marginal pxn decomposes into the product of two factors times the normalization constant pxn z we shall interpret as a message passed forwards along the chain from node xn to node xn. similarly can be viewed as a message passed backwards inference in graphical models xn xn xn figure the marginal distribution pxn for a node xn along the chain is obtained by multiplying the two messages and and then normalizing. these messages can themselves be evaluated recursively by passing messages from both ends of the chain towards node xn. xn along the chain to node xn from node note that each of the messages comprises a set of k values one for each choice of xn and so the product of two messages should be interpreted as the point-wise multiplication of the elements of the two messages to give another set of k values. the message can be evaluated recursively because n xn xn n xn we therefore first evaluate xn and then apply repeatedly until we reach the desired node. note carefully the structure of the message passing equation. the outgoing message in is obtained by multiplying the incoming message by the local potential involving the node variable and the outgoing variable and then summing over the node variable. similarly the message can be evaluated recursively by starting with node xn and using xn xn this recursive message passing is illustrated in figure the normalization constant z is easily evaluated by summing the right-hand side of over all states of xn an operation that requires only ok computation. graphs of the form shown in figure are called markov chains and the corresponding message passing equations represent an example of the chapmankolmogorov equations for markov processes graphical models now suppose we wish to evaluate the marginals pxn for every node n n in the chain. simply applying the above procedure separately for each node will have computational cost that is on however such an approach would be very wasteful of computation. for instance to find we need to propagate a message from node xn back to node similarly to evaluate we need to propagate a messages from node xn back to node this will involve much duplicated computation because most of the messages will be identical in the two cases. suppose instead we first launch a message starting from node xn and propagate corresponding messages all the way back to node and suppose we similarly launch a message starting from node and propagate the corresponding messages all the way forward to node xn provided we store all of the intermediate messages along the way then any node can evaluate its marginal simply by applying the computational cost is only twice that for finding the marginal of a single node rather than n times as much. observe that a message has passed once in each direction across each link in the graph. note also that the normalization constant z need be evaluated only once using any convenient node. if some of the nodes in the graph are observed then the corresponding variables are simply clamped to their observed values and there is no summation. to see this note that the effect of clamping a variable xn to an observed value can additional function which takes the value when xn and the value contain xn. summations over xn then contain only one term in which xn otherwise. one such function can then be absorbed into each of the potentials that be expressed by multiplying the joint distribution by or more copies of an now suppose we wish to calculate the joint distribution pxn xn for two neighbouring nodes on the chain. this is similar to the evaluation of the marginal for a single node except that there are now two variables that are not summed out. a few moments thought will show that the required joint distribution can be written in the form pxn xn z n xn thus we can obtain the joint distributions over all of the sets of variables in each of the potentials directly once we have completed the message passing required to obtain the marginals. this is a useful result because in practice we may wish to use parametric forms for the clique potentials or equivalently for the conditional distributions if we started from a directed graph. in order to learn the parameters of these potentials in situations where not all of the variables are observed we can employ the em algorithm and it turns out that the local joint distributions of the cliques conditioned on any observed data is precisely what is needed in the e step. we shall consider some examples of this in detail in chapter trees we have seen that exact inference on a graph comprising a chain of nodes can be performed efficiently in time that is linear in the number of nodes using an algorithm exercise chapter inference in graphical models figure examples treestructured graphs showing an undirected tree a directed tree and a directed polytree. of that can be interpreted in terms of messages passed along the chain. more generally inference can be performed efficiently using local message passing on a broader class of graphs called trees. in particular we shall shortly generalize the message passing formalism derived above for chains to give the sum-product algorithm which provides an efficient framework for exact inference in tree-structured graphs. in the case of an undirected graph a tree is defined as a graph in which there is one and only one path between any pair of nodes. such graphs therefore do not have loops. in the case of directed graphs a tree is defined such that there is a single node called the root which has no parents and all other nodes have one parent. if we convert a directed tree into an undirected graph we see that the moralization step will not add any links as all nodes have at most one parent and as a consequence the corresponding moralized graph will be an undirected tree. examples of undirected and directed trees are shown in figure and note that a distribution represented as a directed tree can easily be converted into one represented by an undirected tree and vice versa. if there are nodes in a directed graph that have more than one parent but there is still only one path the direction of the arrows between any two nodes then the graph is a called a polytree as illustrated in figure such a graph will have more than one node with the property of having no parents and furthermore the corresponding moralized undirected graph will have loops. factor graphs the sum-product algorithm that we derive in the next section is applicable to undirected and directed trees and to polytrees. it can be cast in a particularly simple and general form if we first introduce a new graphical construction called a factor graph kschischnang et al. exercise both directed and undirected graphs allow a global function of several variables to be expressed as a product of factors over subsets of those variables. factor graphs make this decomposition explicit by introducing additional nodes for the factors themselves in addition to the nodes representing the variables. they also allow us to be more explicit about the details of the factorization as we shall see. let us write the joint distribution over a set of variables in the form of a product of factors px fsxs where xs denotes a subset of the variables. for convenience we shall denote the s graphical models figure example of a factor graph which corresponds to the factorization fa fb fc fd individual variables by xi however as in earlier discussions these can comprise groups of variables as vectors or matrices. each factor fs is a function of a corresponding set of variables xs. directed graphs whose factorization is defined by represent special cases of in which the factors fsxs are local conditional distributions. similarly undirected graphs given by are a special case in which the factors are potential functions over the maximal cliques normalizing coefficient can be viewed as a factor defined over the empty set of variables. in a factor graph there is a node as usual by a circle for every variable in the distribution as was the case for directed and undirected graphs. there are also additional nodes by small squares for each factor fsxs in the joint distribution. finally there are undirected links connecting each factor node to all of the variables nodes on which that factor depends. consider for example a distribution that is expressed in terms of the factorization px this can be expressed by the factor graph shown in figure note that there are two factors and that are defined over the same set of variables. in an undirected graph the product of two such factors would simply be lumped together into the same clique potential. similarly and could be combined into a single potential over and the factor graph however keeps such factors explicit and so is able to convey more detailed information about the underlying factorization. f fb fa figure an undirected graph with a single clique potential a factor graph with factor f representing the same distribution as the undirected graph. a different factor graph representing the same distribution whose factors satisfy inference in graphical models f fc fa fb figure a directed graph with the factorization a factor graph representing the same distribution as the directed graph whose factor satisfies f a different factor graph representing the same distribution with factors and factor graphs are said to be bipartite because they consist of two distinct kinds of nodes and all links go between nodes of opposite type. in general factor graphs can therefore always be drawn as two rows of nodes nodes at the top and factor nodes at the bottom with links between the rows as shown in the example in figure in some situations however other ways of laying out the graph may be more intuitive for example when the factor graph is derived from a directed or undirected graph as we shall see. if we are given a distribution that is expressed in terms of an undirected graph then we can readily convert it to a factor graph. to do this we create variable nodes corresponding to the nodes in the original undirected graph and then create additional factor nodes corresponding to the maximal cliques xs. the factors fsxs are then set equal to the clique potentials. note that there may be several different factor graphs that correspond to the same undirected graph. these concepts are illustrated in figure similarly to convert a directed graph to a factor graph we simply create variable nodes in the factor graph corresponding to the nodes of the directed graph and then create factor nodes corresponding to the conditional distributions and then finally add the appropriate links. again there can be multiple factor graphs all of which correspond to the same directed graph. the conversion of a directed graph to a factor graph is illustrated in figure we have already noted the importance of tree-structured graphs for performing efficient inference. if we take a directed or undirected tree and convert it into a factor graph then the result will again be a tree other words the factor graph will have no loops and there will be one and only one path connecting any two nodes. in the case of a directed polytree conversion to an undirected graph results in loops due to the moralization step whereas conversion to a factor graph again results in a tree as illustrated in figure in fact local cycles in a directed graph due to links connecting parents of a node can be removed on conversion to a factor graph by defining the appropriate factor function as shown in figure we have seen that multiple different factor graphs can represent the same directed or undirected graph. this allows factor graphs to be more specific about the graphical models figure a directed polytree. the result of converting the polytree into an undirected graph showing the creation of loops. the result of converting the polytree into a factor graph which retains the tree structure. precise form of the factorization. figure shows an example of a fully connected undirected graph along with two different factor graphs. in the joint distribution is given by a general form px whereas in it is given by the more specific factorization px it should be emphasized that the factorization in does not correspond to any conditional independence properties. the sum-product algorithm we shall now make use of the factor graph framework to derive a powerful class of efficient exact inference algorithms that are applicable to tree-structured graphs. here we shall focus on the problem of evaluating local marginals over nodes or subsets of nodes which will lead us to the sum-product algorithm. later we shall modify the technique to allow the most probable state to be found giving rise to the max-sum algorithm. also we shall suppose that all of the variables in the model are discrete and so marginalization corresponds to performing sums. the framework however is equally applicable to linear-gaussian models in which case marginalization involves integration and we shall consider an example of this in detail when we discuss linear dynamical systems. section figure a fragment of a directed graph having a local cycle. conversion to a fragment of a factor graph having a tree structure in which f fa inference in graphical models fb fc figure a fully connected undirected graph. and two factor graphs each of which corresponds to the undirected graph in there is an algorithm for exact inference on directed graphs without loops known as belief propagation lauritzen and spiegelhalter and is equivalent to a special case of the sum-product algorithm. here we shall consider only the sum-product algorithm because it is simpler to derive and to apply as well as being more general. we shall assume that the original graph is an undirected tree or a directed tree or polytree so that the corresponding factor graph has a tree structure. we first convert the original graph into a factor graph so that we can deal with both directed and undirected models using the same framework. our goal is to exploit the structure of the graph to achieve two things to obtain an efficient exact inference algorithm for finding marginals in situations where several marginals are required to allow computations to be shared efficiently. we begin by considering the problem of finding the marginal px for particular variable node x. for the moment we shall suppose that all of the variables are hidden. later we shall see how to modify the algorithm to incorporate evidence corresponding to observed variables. by definition the marginal is obtained by summing the joint distribution over all variables except x so that px px xx where x x denotes the set of variables in x with variable x omitted. the idea is to substitute for px using the factor graph expression and then interchange summations and products in order to obtain an efficient algorithm. consider the fragment of graph shown in figure in which we see that the tree structure of the graph allows us to partition the factors in the joint distribution into groups with one group associated with each of the factor nodes that is a neighbour of the variable node x. we see that the joint distribution can be written as a product of the form px fsx xs s nex nex denotes the set of factor nodes that are neighbours of x and xs denotes the set of all variables in the subtree connected to the variable node x via the factor node graphical models figure a fragment of a factor graph illustrating the evaluation of the marginal px. s x x s f fs xx fs x fs and fsx xs represents the product of all the factors in the group associated with factor fs. substituting into and interchanging the sums and products we ob tain px s nex s nex fsx xs xs fs xx. fs xx fsx xs here we have introduced a set of functions fs xx defined by xs which can be viewed as messages from the factor nodes fs to the variable node x. we see that the required marginal px is given by the product of all the incoming messages arriving at node x. in order to evaluate these messages we again turn to figure and note that each factor fsx xs is described by a factor and so can itself be factorized. in particular we can write fsx xs fsx xm gm xsm where for convenience we have denoted the variables associated with factor fx in addition to x by xm this factorization is illustrated in figure note that the set of variables xm is the set of variables on which the factor fs depends and so it can also be denoted xs using the notation of substituting into we obtain fs xx fsx xm fsx xm gmxm xsm xxm xm fsxm xm xm m nefsx m nefsx inference in graphical models figure illustration of the factorization of the subgraph as sociated with factor node fs. xm xm fsxm fs fs xx x xm gmxm xsm where nefs denotes the set of variable nodes that are neighbours of the factor node fs and nefs x denotes the same set but with node x removed. here we have defined the following messages from variable nodes to factor nodes xm fsxm gmxm xsm. xsm we have therefore introduced two distinct kinds of message those that go from factor nodes to variable nodes denoted f xx and those that go from variable nodes to factor nodes denoted x f in each case we see that messages passed along a link are always a function of the variable associated with the variable node that link connects to. the result says that to evaluate the message sent by a factor node to a variable node along the link connecting them take the product of the incoming messages along all other links coming into the factor node multiply by the factor associated with that node and then marginalize over all of the variables associated with the incoming messages. this is illustrated in figure it is important to note that a factor node can send a message to a variable node once it has received incoming messages from all other neighbouring variable nodes. finally we derive an expression for evaluating the messages from variable nodes to factor nodes again by making use of the factorization. from figure we see that term gmxm xsm associated with node xm is given by a product of terms flxm xml each associated with one of the factor nodes fl that is linked to node xm node fs so that gmxm xsm flxm xml l nexmfs where the product is taken over all neighbours of node xm except for node fs. note that each of the factors flxm xml represents a subtree of the original graph of precisely the same kind as introduced in substituting into we graphical models figure illustration of the evaluation of the message sent by a variable node to an adjacent factor node. fl fl xm fs then obtain xm fsxm l nexmfs l nexmfs flxm xml flxm xml xml fl xmxm where we have used the definition of the messages passed from factor nodes to variable nodes. thus to evaluate the message sent by a variable node to an adjacent factor node along the connecting link we simply take the product of the incoming messages along all of the other links. note that any variable node that has only two neighbours performs no computation but simply passes messages through unchanged. also we note that a variable node can send a message to a factor node once it has received incoming messages from all other neighbouring factor nodes. recall that our goal is to calculate the marginal for variable node x and that this marginal is given by the product of incoming messages along all of the links arriving at that node. each of these messages can be computed recursively in terms of other messages. in order to start this recursion we can view the node x as the root of the tree and begin at the leaf nodes. from the definition we see that if a leaf node is a variable node then the message that it sends along its one and only link is given by as illustrated in figure similarly if the leaf node is a factor node we see from that the message sent should take the form x f f xx fx figure the sum-product algorithm begins with messages sent by the leaf nodes which depend on whether the leaf node is a variable node or a factor node. x f f xx fx x f f x inference in graphical models as illustrated in figure at this point it is worth pausing to summarize the particular version of the sumproduct algorithm obtained so far for evaluating the marginal px. we start by viewing the variable node x as the root of the factor graph and initiating messages at the leaves of the graph using and the message passing steps and are then applied recursively until messages have been propagated along every link and the root node has received messages from all of its neighbours. each node can send a message towards the root once it has received messages from all of its other neighbours. once the root node has received messages from all of its neighbours the required marginal can be evaluated using we shall illustrate this process shortly. to see that each node will always receive enough messages to be able to send out a message we can use a simple inductive argument as follows. clearly for a graph comprising a variable root node connected directly to several factor leaf nodes the algorithm trivially involves sending messages of the form directly from the leaves to the root. now imagine building up a general graph by adding nodes one at a time and suppose that for some particular graph we have a valid algorithm. when one more or factor node is added it can be connected only by a single link because the overall graph must remain a tree and so the new node will be a leaf node. it therefore sends a message to the node to which it is linked which in turn will therefore receive all the messages it requires in order to send its own message towards the root and so again we have a valid algorithm thereby completing the proof. now suppose we wish to find the marginals for every variable node in the graph. this could be done by simply running the above algorithm afresh for each such node. however this would be very wasteful as many of the required computations would be repeated. we can obtain a much more efficient procedure by overlaying these multiple message passing algorithms to obtain the general sum-product algorithm as follows. arbitrarily pick any or factor node and designate it as the root. propagate messages from the leaves to the root as before. at this point the root node will have received messages from all of its neighbours. it can therefore send out messages to all of its neighbours. these in turn will then have received messages from all of their neighbours and so can send out messages along the links going away from the root and so on. in this way messages are passed outwards from the root all the way to the leaves. by now a message will have passed in both directions across every link in the graph and every node will have received a message from all of its neighbours. again a simple inductive argument can be used to verify the validity of this message passing protocol. because every variable node will have received messages from all of its neighbours we can readily calculate the marginal distribution for every variable in the graph. the number of messages that have to be computed is given by twice the number of links in the graph and so involves only twice the computation involved in finding a single marginal. by comparison if we had run the sum-product algorithm separately for each node the amount of computation would grow quadratically with the size of the graph. note that this algorithm is in fact independent of which node was designated as the root exercise graphical models figure the sum-product algorithm can be viewed purely in terms of messages sent out by factor nodes to other factor nodes. in this example the outgoing message shown by the blue arrow is obtained by taking the product of all the incoming messages shown by green arrows multiplying by the factor fs and marginalizing over the variables and fs exercise and indeed the notion of one node having a special status was introduced only as a convenient way to explain the message passing protocol. next suppose we wish to find the marginal distributions pxs associated with the sets of variables belonging to each of the factors. by a similar argument to that used above it is easy to see that the marginal associated with a factor is given by the product of messages arriving at the factor node and the local factor at that node pxs fsxs xi fsxi i nefs in complete analogy with the marginals at the variable nodes. if the factors are parameterized functions and we wish to learn the values of the parameters using the em algorithm then these marginals are precisely the quantities we will need to calculate in the e step as we shall see in detail when we discuss the hidden markov model in chapter the message sent by a variable node to a factor node as we have seen is simply the product of the incoming messages on other links. we can if we wish view the sum-product algorithm in a slightly different form by eliminating messages from variable nodes to factor nodes and simply considering messages that are sent out by factor nodes. this is most easily seen by considering the example in figure so far we have rather neglected the issue of normalization. if the factor graph was derived from a directed graph then the joint distribution is already correctly normalized and so the marginals obtained by the sum-product algorithm will similarly be normalized correctly. however if we started from an undirected graph then in general there will be an unknown normalization coefficient as with the simple chain example of figure this is easily handled by working with an unnormal ized of the joint distribution where px we first run the sum-product algorithm to find the corresponding unnormalized the directly. coefficient is then easily obtained by normalizing any one of these marginals and this is computationally efficient because the normalization is done over a single variable rather than over the entire set of variables as would be required to normalize at this point it may be helpful to consider a simple example to illustrate the operation of the sum-product algorithm. figure shows a simple factor figure a simple factor graph used to illustrate the sum-product algorithm. inference in graphical models fa fb fc graph whose unnormalized joint distribution is given by in order to apply the sum-product algorithm to this graph let us designate node as the root in which case there are two leaf nodes and starting with the leaf nodes we then have the following sequence of six messages the direction of flow of these messages is illustrated in figure once this message propagation is complete we can then propagate messages from the root node out to the leaf nodes and these are given by fa fc fa fc fb. fb fb fb fc fa fa fb fc graphical models figure flow of messages for the sum-product algorithm applied to the example graph in figure from the leaf nodes and towards the root node from the root node towards the leaf nodes. one message has now passed in each direction across each link and we can now evaluate the marginals. as a simple check let us verify that the marginal is given by the correct expression. using and substituting for the messages using the above results we have fa fb fc as required. so far we have assumed that all of the variables in the graph are hidden. in most practical applications a subset of the variables will be observed and we wish to calculate posterior distributions conditioned on these observations. observed nodes are easily handled within the sum-product algorithm as follows. suppose we partition x into hidden variables h and observed variables v and that the observed value of v is then we simply multiply the joint distribution px by i where if v and otherwise. this product corresponds to ph v and hence is an unnormalized version of phv by runphiv up to a normalization coefficient whose value can be found efficiently ning the sum-product algorithm we can efficiently calculate the posterior marginals using a local computation. any summations over variables in v then collapse into a single term. we have assumed throughout this section that we are dealing with discrete variables. however there is nothing specific to discrete variables either in the graphical framework or in the probabilistic construction of the sum-product algorithm. for inference in graphical models table example of a joint distribution over two binary variables for which the maximum of the joint distribution occurs for different variable values compared to the maxima of the two marginals. y y x x section continuous variables the summations are simply replaced by integrations. we shall give an example of the sum-product algorithm applied to a graph of linear-gaussian variables when we consider linear dynamical systems. the max-sum algorithm the sum-product algorithm allows us to take a joint distribution px expressed as a factor graph and efficiently find marginals over the component variables. two other common tasks are to find a setting of the variables that has the largest probability and to find the value of that probability. these can be addressed through a closely related algorithm called max-sum which can be viewed as an application of dynamic programming in the context of graphical models et al. a simple approach to finding latent variable values having high probability would be to run the sum-product algorithm to obtain the marginals pxi for every variable and then for each marginal in turn to find the value i that maximizes that marginal. however this would give the set of values that are individually the most probable. in practice we typically wish to find the set of values that jointly have the largest probability in other words the vector xmax that maximizes the joint distribution so that xmax arg max px for which the corresponding value of the joint probability will be given by pxmax max x px. x in general xmax is not the same as the set of i values as we can easily show using a simple example. consider the joint distribution px y over two binary variables x y given in table the joint distribution is maximized by setting x and y corresponding the value however the marginal for px obtained by summing over both values of y is given by px and px and similarly the marginal for y is given by py and py and so the marginals are maximized by x and y which corresponds to a value of for the joint distribution. in fact it is not difficult to construct examples for which the set of individually most probable values has probability zero under the joint distribution. we therefore seek an efficient algorithm for finding the value of x that maximizes the joint distribution px and that will allow us to obtain the value of the joint distribution at its maximum. to address the second of these problems we shall simply write out the max operator in terms of its components max x px max max xm px exercise graphical models where m is the total number of variables and then substitute for px using its expansion in terms of a product of factors. in deriving the sum-product algorithm we made use of the distributive law for multiplication. here we make use of the analogous law for the max operator which holds if a will always be the case for the factors in a graphical model. this allows us to exchange products with maximizations. maxab ac a maxb c consider first the simple example of a chain of nodes described by the evaluation of the probability maximum can be written as max x px max z max xn z max n xn max n xn xn as with the calculation of marginals we see that exchanging the max and product operators results in a much more efficient computation and one that is easily interpreted in terms of messages passed from node xn backwards along the chain to node we can readily generalize this result to arbitrary tree-structured factor graphs by substituting the expression for the factor graph expansion into and again exchanging maximizations with products. the structure of this calculation is identical to that of the sum-product algorithm and so we can simply translate those results into the present context. in particular suppose that we designate a particular variable node as the root of the graph. then we start a set of messages propagating inwards from the leaves of the tree towards the root with each node sending its message towards the root once it has received all incoming messages from its other neighbours. the final maximization is performed over the product of all messages arriving at the root node and gives the maximum value for px. this could be called the max-product algorithm and is identical to the sum-product algorithm except that summations are replaced by maximizations. note that at this stage messages have been sent from leaves to the root but not in the other direction. in practice products of many small probabilities can lead to numerical underflow problems and so it is convenient to work with the logarithm of the joint distribution. the logarithm is a monotonic function so that if a b then ln a ln b and hence the max operator and the logarithm function can be interchanged so that ln max x px max x ln px. the distributive property is preserved because maxa b a c a maxb c. thus taking the logarithm simply has the effect of replacing the products in the max-product algorithm with sums and so we obtain the max-sum algorithm. from inference in graphical models the results and derived earlier for the sum-product algorithm we can readily write down the max-sum algorithm in terms of message passing simply by replacing sum with max and replacing products with sums of logarithms to give ln fx xm xm f m nefsx x f fl xx. f xx max l nexf the initial messages sent by the leaf nodes are obtained by analogy with and and are given by x f f xx ln fx pmax max x fs xx s nex s nex while at the root node the maximum probability can then be computed by analogy with using so far we have seen how to find the maximum of the joint distribution by propagating messages from the leaves to an arbitrarily chosen root node. the result will be the same irrespective of which node is chosen as the root. now we turn to the second problem of finding the configuration of the variables for which the joint distribution attains this maximum value. so far we have sent messages from the leaves to the root. the process of evaluating will also give the value xmax for the most probable value of the root node variable defined by xmax arg max x fs xx at this point we might be tempted simply to continue with the message passing algorithm and send messages from the root back out to the leaves using and then apply to all of the remaining variable nodes. however because we are now maximizing rather than summing it is possible that there may be multiple configurations of x all of which give rise to the maximum value for px. in such cases this strategy can fail because it is possible for the individual variable values obtained by maximizing the product of messages at each node to belong to different maximizing configurations giving an overall configuration that no longer corresponds to a maximum. the problem can be resolved by adopting a rather different kind of message passing from the root node to the leaves. to see how this works let us return once again to the simple chain example of n variables xn each having k states graphical models figure a lattice or trellis diagram showing explicitly the k possible states per row of the diagram for each of the variables xn in the in this illustration k the archain model. row shows the direction of message passing in the max-product algorithm. for every state k of each variable xn to column n of the diagram the function defines a unique state at the previous variable indicated by the black lines. the two paths through the lattice correspond to configurations that give the global maximum of the joint probability distribution and either of these can be found by tracing back along the black lines in the opposite direction to the arrow. k k k n n n n corresponding to the graph shown in figure suppose we take node xn to be the root node. then in the first phase we propagate messages from the leaf node to the root node using xn fn xnxn fn xnxn max xn ln fn xn xn f n which follow from applying and to this particular graph. the initial message sent from the leaf node is simply the most probable value for xn is then given by xmax n arg max xn fn xn now we need to determine the states of the previous variables that correspond to the same maximizing configuration. this can be done by keeping track of which values of the variables gave rise to the maximum state of each variable in other words by storing quantities given by arg max xn ln fn xn xn f n to understand better what is happening it is helpful to represent the chain of variables in terms of a lattice or trellis diagram as shown in figure note that this is not a probabilistic graphical model because the nodes represent individual states of variables while each variable corresponds to a column of such states in the diagram. for each state of a given variable there is a unique state of the previous variable that maximizes the probability are broken either systematically or at random corresponding to the function given by and this is indicated inference in graphical models by the lines connecting the nodes. once we know the most probable value of the final node xn we can then simply follow the link back to find the most probable state of node xn and so on back to the initial node this corresponds to propagating a message back down the chain using n xmax n and is known as back-tracking. note that there could be several values of xn all of which give the maximum value in provided we chose one of these values when we do the back-tracking we are assured of a globally consistent maximizing configuration. in figure we have indicated two paths each of which we shall suppose corresponds to a global maximum of the joint probability distribution. if k and k each represent possible values of xmax n then starting from either state and tracing back along the black lines which corresponds to iterating we obtain a valid global maximum configuration. note that if we had run a forward pass of max-sum message passing followed by a backward pass and then applied at each node separately we could end up selecting some states from one path and some from the other path giving an overall configuration that is not a global maximizer. we see that it is necessary instead to keep track of the maximizing states during the forward pass using the functions and then use back-tracking to find a consistent solution. the extension to a general tree-structured factor graph should now be clear. if a message is sent from a factor node f to a variable node x a maximization is performed over all other variable nodes xm that are neighbours of that factor node using when we perform this maximization we keep a record of which values of the variables xm gave rise to the maximum. then in the back-tracking step having found xmax we can then use these stored values to assign consistent maximizing states xmax m the max-sum algorithm with back-tracking gives an exact maximizing configuration for the variables provided the factor graph is a tree. an important application of this technique is for finding the most probable sequence of hidden states in a hidden markov model in which case it is known as the viterbi algorithm. xmax as with the sum-product algorithm the inclusion of evidence in the form of observed variables is straightforward. the observed variables are clamped to their observed values and the maximization is performed over the remaining hidden variables. this can be shown formally by including identity functions for the observed variables into the factor functions as we did for the sum-product algorithm. it is interesting to compare max-sum with the iterated conditional modes algorithm described on page each step in icm is computationally simpler because the messages that are passed from one node to the next comprise a single value consisting of the new state of the node for which the conditional distribution is maximized. the max-sum algorithm is more complex because the messages are functions of node variables x and hence comprise a set of k values for each possible state of x. unlike max-sum however icm is not guaranteed to find a global maximum even for tree-structured graphs. section graphical models exact inference in general graphs the sum-product and max-sum algorithms provide efficient and exact solutions to inference problems in tree-structured graphs. for many practical applications however we have to deal with graphs having loops. the message passing framework can be generalized to arbitrary graph topologies giving an exact inference procedure known as the junction tree algorithm and spiegelhalter jordan here we give a brief outline of the key steps involved. this is not intended to convey a detailed understanding of the algorithm but rather to give a flavour of the various stages involved. if the starting point is a directed graph it is first converted to an undirected graph by moralization whereas if starting from an undirected graph this step is not required. next the graph is triangulated which involves finding chord-less cycles containing four or more nodes and adding extra links to eliminate such chord-less cycles. for instance in the graph in figure the cycle a c b d a is chord-less a link could be added between a and b or alternatively between c and d. note that the joint distribution for the resulting triangulated graph is still defined by a product of the same potential functions but these are now considered to be functions over expanded sets of variables. next the triangulated graph is used to construct a new tree-structured undirected graph called a join tree whose nodes correspond to the maximal cliques of the triangulated graph and whose links connect pairs of cliques that have variables in common. the selection of which pairs of cliques to connect in this way is important and is done so as to give a maximal spanning tree defined as follows. of all possible trees that link up the cliques the one that is chosen is one for which the weight of the tree is largest where the weight for a link is the number of nodes shared by the two cliques it connects and the weight for the tree is the sum of the weights for the links. if the tree is condensed so that any clique that is a subset of another clique is absorbed into the larger clique this gives a junction tree. as a consequence of the triangulation step the resulting tree satisfies the running intersection property which means that if a variable is contained in two cliques then it must also be contained in every clique on the path that connects them. this ensures that inference about variables will be consistent across the graph. finally a two-stage message passing algorithm essentially equivalent to the sum-product algorithm can now be applied to this junction tree in order to find marginals and conditionals. although the junction tree algorithm sounds complicated at its heart is the simple idea that we have used already of exploiting the factorization properties of the distribution to allow sums and products to be interchanged so that partial summations can be performed thereby avoiding having to work directly with the joint distribution. the role of the junction tree is to provide a precise and efficient way to organize these computations. it is worth emphasizing that this is achieved using purely graphical operations! the junction tree is exact for arbitrary graphs and is efficient in the sense that for a given graph there does not in general exist a computationally cheaper approach. unfortunately the algorithm must work with the joint distributions within each node of which corresponds to a clique of the triangulated graph and so the computational cost of the algorithm is determined by the number of variables in the largest inference in graphical models clique and will grow exponentially with this number in the case of discrete variables. an important concept is the treewidth of a graph which is defined in terms of the number of variables in the largest clique. in fact it is defined to be as one less than the size of the largest clique to ensure that a tree has a treewidth of because there in general there can be multiple different junction trees that can be constructed from a given starting graph the treewidth is defined by the junction tree for which the largest clique has the fewest variables. if the treewidth of the original graph is high the junction tree algorithm becomes impractical. loopy belief propagation for many problems of practical interest it will not be feasible to use exact inference and so we need to exploit effective approximation methods. an important class of such approximations that can broadly be called variational methods will be discussed in detail in chapter complementing these deterministic approaches is a wide range of sampling methods also called monte carlo methods that are based on stochastic numerical sampling from distributions and that will be discussed at length in chapter here we consider one simple approach to approximate inference in graphs with loops which builds directly on the previous discussion of exact inference in trees. the idea is simply to apply the sum-product algorithm even though there is no guarantee that it will yield good results. this approach is known as loopy belief propagation and mackay and is possible because the message passing rules and for the sum-product algorithm are purely local. however because the graph now has cycles information can flow many times around the graph. for some models the algorithm will converge whereas for others it will not. in order to apply this approach we need to define a message passing schedule. let us assume that one message is passed at a time on any given link and in any given direction. each message sent from a node replaces any previous message sent in the same direction across the same link and will itself be a function only of the most recent messages received by that node at previous steps of the algorithm. we have seen that a message can only be sent across a link from a node when all other messages have been received by that node across its other links. because there are loops in the graph this raises the problem of how to initiate the message passing algorithm. to resolve this we suppose that an initial message given by the unit function has been passed across every link in each direction. every node is then in a position to send a message. there are now many possible ways to organize the message passing schedule. for example the flooding schedule simultaneously passes a message across every link in both directions at each time step whereas schedules that pass one message at a time are called serial schedules. following kschischnang et al. we will say that a or factor node a has a message pending on its link to a node b if node a has received any message on any of its other links since the last time it send a message to b. thus when a node receives a message on one of its links this creates pending messages on all of its other links. only pending messages need to be transmitted because graphical models exercise other messages would simply duplicate the previous message on the same link. for graphs that have a tree structure any schedule that sends only pending messages will eventually terminate once a message has passed in each direction across every link. at this point there are no pending messages and the product of the received messages at every variable give the exact marginal. in graphs having loops however the algorithm may never terminate because there might always be pending messages although in practice it is generally found to converge within a reasonable time for most applications. once the algorithm has converged or once it has been stopped if convergence is not observed the local marginals can be computed using the product of the most recently received incoming messages to each variable node or factor node on every link. in some applications the loopy belief propagation algorithm can give poor results whereas in other applications it has proven to be very effective. in particular state-of-the-art algorithms for decoding certain kinds of error-correcting codes are equivalent to loopy belief propagation berrou et al. mceliece et al. mackay and neal frey learning the graph structure in our discussion of inference in graphical models we have assumed that the structure of the graph is known and fixed. however there is also interest in going beyond the inference problem and learning the graph structure itself from data and koller this requires that we define a space of possible structures as well as a measure that can be used to score each structure. from a bayesian viewpoint we would ideally like to compute a posterior distribution over graph structures and to make predictions by averaging with respect to this distribution. if we have a prior pm over graphs indexed by m then the posterior distribution is given by pmd pmpdm where d is the observed data set. the model evidence pdm then provides the score for each model. however evaluation of the evidence involves marginalization over the latent variables and presents a challenging computational problem for many models. exploring the space of structures can also be problematic. because the number of different graph structures grows exponentially with the number of nodes it is often necessary to resort to heuristics to find good candidates. exercises www by marginalizing out the variables in order show that the representation for the joint distribution of a directed graph is correctly normalized provided each of the conditional distributions is normalized. www show that the property of there being no directed cycles in a directed graph follows from the statement that there exists an ordered numbering of the nodes such that for each node there are no links going to a lower-numbered node. table the joint distribution over three binary variables. exercises a b c pa b c consider three binary variables a b c having the joint distribution given in table show by direct evaluation that this distribution has the property that a and b are marginally dependent so that pa b papb but that they become independent when conditioned on c so that pa bc pacpbc for both c and c evaluate the distributions pa pbc and pca corresponding to the joint distribution given in table hence show by direct evaluation that pa b c papcapbc. draw the corresponding directed graph. www draw a directed probabilistic graphical model corresponding to the relevance vector machine described by and for the model shown in figure we have seen that the number of parameters required to specify the conditional distribution xm where xi could be reduced from to m by making use of the logistic sigmoid representation an alternative representation is given by py xm ixi where the parameters i represent the probabilities pxi and is an additional parameters satisfying the conditional distribution is known as the noisy-or. show that this can be interpreted as a soft form of the logical or function the function that gives y whenever at least one of the xi discuss the interpretation of using the recursion relations and show that the mean and covariance of the joint distribution for the graph shown in figure are given by and respectively. www show that a b c d implies a b d. www using the d-separation criterion show that the conditional distribution for a node x in a directed graph conditioned on all of the nodes in the markov blanket is independent of the remaining variables in the graph. graphical models figure example of a graphical model used to explore the conditional independence properties of the head-to-head path a c b when a descendant of c namely the node d is observed. a b c d consider the directed graph shown in figure in which none of the variables is observed. show that a b suppose we now observe the variable d. show that in general a b d. consider the example of the car fuel system shown in figure and suppose that instead of observing the state of the fuel gauge g directly the gauge is seen by the driver d who reports to us the reading on the gauge. this report is either that the gauge shows full d or that it shows empty d our driver is a bit unreliable as expressed through the following probabilities pd pd suppose that the driver tells us that the fuel gauge shows empty in other words that we observe d evaluate the probability that the tank is empty given only this observation. similarly evaluate the corresponding probability given also the observation that the battery is flat and note that this second probability is lower. discuss the intuition behind this result and relate the result to figure www show that there are distinct undirected graphs over a set of m distinct random variables. draw the possibilities for the case of m consider the use of iterated conditional modes to minimize the energy function given by write down an expression for the difference in the values of the energy associated with the two states of a particular variable xj with all other variables held fixed and show that it depends only on quantities that are local to xj in the graph. consider a particular case of the energy function given by in which the coefficients h show that the most probable configuration of the latent variables is given by xi yi for all i. www show that the joint distribution pxn xn for two neighbouring nodes in the graph shown in figure is given by an expression of the form exercises consider the inference problem of evaluating pxnxn for the graph shown in figure for all nodes n n show that the message passing algorithm discussed in section can be used to solve this efficiently and discuss which messages are modified and in what way. consider a graph of the form shown in figure having n nodes in which nodes and are observed. use d-separation to show that show that if the message passing algorithm of section is applied to the evaluation of the result will be independent of the value of www show that a distribution represented by a directed tree can trivially be written as an equivalent distribution over the corresponding undirected tree. also show that a distribution expressed as an undirected tree can by suitable normalization of the clique potentials be written as a directed tree. calculate the number of distinct directed trees that can be constructed from a given undirected tree. apply the sum-product algorithm derived in section to the chain-ofnodes model discussed in section and show that the results and are recovered as a special case. www consider the message passing protocol for the sum-product algorithm on a tree-structured factor graph in which messages are first propagated from the leaves to an arbitrarily chosen root node and then from the root node out to the leaves. use proof by induction to show that the messages can be passed in such an order that at every step each node that must send a message has received all of the incoming messages necessary to construct its outgoing messages. www show that the marginal distributions pxs over the sets of variables xs associated with each of the factors fxxs in a factor graph can be found by first running the sum-product message passing algorithm and then evaluating the required marginals using consider a tree-structured factor graph in which a given subset of the variable nodes form a connected subgraph any variable node of the subset is connected to at least one of the other variable nodes via a single factor node. show how the sum-product algorithm can be used to compute the marginal distribution over that subset. www in section we showed that the marginal distribution pxi for a variable node xi in a factor graph is given by the product of the messages arriving at this node from neighbouring factor nodes in the form show that the marginal pxi can also be written as the product of the incoming message along any one of the links with the outgoing message along the same link. show that the marginal distribution for the variables xs in a factor fsxs in a tree-structured factor graph after running the sum-product message passing algorithm can be written as the product of the message arriving at the factor node along all its links times the local factor fxs in the form graphical models in we verified that the sum-product algorithm run on the graph in figure with node designated as the root node gives the correct marginal for show that the correct marginals are obtained also for and similarly show that the use of the result after running the sum-product algorithm on this graph gives the correct joint distribution for consider a tree-structured factor graph over discrete variables and suppose we wish to evaluate the joint distribution pxa xb associated with two variables xa and xb that do not belong to a common factor. define a procedure for using the sumproduct algorithm to evaluate this joint distribution in which one of the variables is successively clamped to each of its allowed values. consider two discrete variables x and y each having three possible states for example x y construct a joint distribution px y over these variables having the property that the that maximizes the marginal px along with the that maximizes the marginal py together have probability zero under the joint distribution so that www the concept of a pending message in the sum-product algorithm for a factor graph was defined in section show that if the graph has one or more cycles there will always be at least one pending message irrespective of how long the algorithm runs. www show that if the sum-product algorithm is run on a factor graph with a tree structure loops then after a finite number of messages have been sent there will be no pending messages. mixture models and em if we define a joint distribution over observed and latent variables the corresponding distribution of the observed variables alone is obtained by marginalization. this allows relatively complex marginal distributions over observed variables to be expressed in terms of more tractable joint distributions over the expanded space of observed and latent variables. the introduction of latent variables thereby allows complicated distributions to be formed from simpler components. in this chapter we shall see that mixture distributions such as the gaussian mixture discussed in section can be interpreted in terms of discrete latent variables. continuous latent variables will form the subject of chapter as well as providing a framework for building more complex probability distributions mixture models can also be used to cluster data. we therefore begin our discussion of mixture distributions by considering the problem of finding clusters in a set of data points which we approach first using a nonprobabilistic technique called the k-means algorithm then we introduce the latent variable section mixture models and em section section section view of mixture distributions in which the discrete latent variables can be interpreted as defining assignments of data points to specific components of the mixture. a general technique for finding maximum likelihood estimators in latent variable models is the expectation-maximization algorithm. we first of all use the gaussian mixture distribution to motivate the em algorithm in a fairly informal way and then we give a more careful treatment based on the latent variable viewpoint. we shall see that the k-means algorithm corresponds to a particular nonprobabilistic limit of em applied to mixtures of gaussians. finally we discuss em in some generality. gaussian mixture models are widely used in data mining pattern recognition machine learning and statistical analysis. in many applications their parameters are determined by maximum likelihood typically using the em algorithm. however as we shall see there are some significant limitations to the maximum likelihood approach and in chapter we shall show that an elegant bayesian treatment can be given using the framework of variational inference. this requires little additional computation compared with em and it resolves the principal difficulties of maximum likelihood while also allowing the number of components in the mixture to be inferred automatically from the data. k-means clustering we begin by considering the problem of identifying groups or clusters of data points in a multidimensional space. suppose we have a data set xn consisting of n observations of a random d-dimensional euclidean variable x. our goal is to partition the data set into some number k of clusters where we shall suppose for the moment that the value of k is given. intuitively we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. we can formalize this notion by first introducing a set of d-dimensional vectors k where k k in which k is a prototype associated with the kth cluster. as we shall see shortly we can think of the k as representing the centres of the clusters. our goal is then to find an assignment of data points to clusters as well as a set of vectors k such that the sum of the squares of the distances of each data point to its closest vector k is a minimum. it is convenient at this point to define some notation to describe the assignment of data points to clusters. for each data point xn we introduce a corresponding set of binary indicator variables rnk where k k describing which of the k clusters the data point xn is assigned to so that if data point xn is assigned to cluster k then rnk and rnj for j k. this is known as the coding scheme. we can then define an objective function sometimes called a distortion measure given by j which represents the sum of the squares of the distances of each data point to its k-means clustering assigned vector k. our goal is to find values for the and the k so as to minimize j. we can do this through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the rnk and the k. first we choose some initial values for the k. then in the first phase we minimize j with respect to the rnk keeping the k fixed. in the second phase we minimize j with respect to the k keeping rnk fixed. this two-stage optimization is then repeated until convergence. we shall see that these two stages of updating rnk and updating k correspond respectively to the e and m steps of the em algorithm and to emphasize this we shall use the terms e step and m step in the context of the k-means algorithm. section consider first the determination of the rnk. because j in is a linear function of rnk this optimization can be performed easily to give a closed form solution. the terms involving different n are independent and so we can optimize for each n separately by choosing rnk to be for whichever value of k gives the minimum value of in other words we simply assign the nth data point to the closest cluster centre. more formally this can be expressed as if k arg minj otherwise. rnk now consider the optimization of the k with the rnk held fixed. the objective function j is a quadratic function of k and it can be minimized by setting its derivative with respect to k to zero giving k rnkxn k n n rnk which we can easily solve for k to give the denominator in this expression is equal to the number of points assigned to cluster k and so this result has a simple interpretation namely set k equal to the mean of all of the data points xn assigned to cluster k. for this reason the procedure is known as the k-means algorithm. the two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments until some maximum number of iterations is exceeded. because each phase reduces the value of the objective function j convergence of the algorithm is assured. however it may converge to a local rather than global minimum of j. the convergence properties of the k-means algorithm were studied by macqueen the k-means algorithm is illustrated using the old faithful data set in figure for the purposes of this example we have made a linear re-scaling of the data known as standardizing such that each of the variables has zero mean and unit standard deviation. for this example we have chosen k and so in this exercise appendix a mixture models and em figure illustration of the k-means algorithm using the re-scaled old faithful data set. green points denote the data set in a two-dimensional euclidean space. the initial choices for centres and are shown by the red and blue crosses respectively. in the initial e step each data point is assigned either to the red cluster or to the blue cluster according to which cluster centre is nearer. this is equivalent to classifying the points according to which side of the perpendicular bisector of the two cluster centres shown by the magenta line they lie on. in the subsequent m step each cluster centre is re-computed to be the mean of the points assigned to the corresponding cluster. show successive e and m steps through to final convergence of the algorithm. k-means clustering figure plot of the cost function j given by after each e step points and m step points of the kmeans algorithm for the example shown in figure the algorithm has converged after the third m step and the final em cycle produces no changes in either the assignments or the prototype vectors. j section section exercise case the assignment of each data point to the nearest cluster centre is equivalent to a classification of the data points according to which side they lie of the perpendicular bisector of the two cluster centres. a plot of the cost function j given by for the old faithful example is shown in figure note that we have deliberately chosen poor initial values for the cluster centres so that the algorithm takes several steps before convergence. in practice a better initialization procedure would be to choose the cluster centres k to be equal to a random subset of k data points. it is also worth noting that the k-means algorithm itself is often used to initialize the parameters in a gaussian mixture model before applying the em algorithm. a direct implementation of the k-means algorithm as discussed here can be relatively slow because in each e step it is necessary to compute the euclidean distance between every prototype vector and every data point. various schemes have been proposed for speeding up the k-means algorithm some of which are based on precomputing a data structure such as a tree such that nearby points are in the same subtree and paliwal moore other approaches make use of the triangle inequality for distances thereby avoiding unnecessary distance calculations elkan so far we have considered a batch version of k-means in which the whole data set is used together to update the prototype vectors. we can also derive an on-line stochastic algorithm by applying the robbins-monro procedure to the problem of finding the roots of the regression function given by the derivatives of j in with respect to k. this leads to a sequential update in which for each data point xn in turn we update the nearest prototype k using new k old k nxn old k where n is the learning rate parameter which is typically made to decrease monotonically as more data points are considered. the k-means algorithm is based on the use of squared euclidean distance as the measure of dissimilarity between a data point and a prototype vector. not only does this limit the type of data variables that can be considered would be inappropriate for cases where some or all of the variables represent categorical labels for instance mixture models and em section but it can also make the determination of the cluster means nonrobust to outliers. we can generalize the k-means algorithm by introducing a more general dissimilarity measure vx between two vectors x and and then minimizing the following distortion measure rnkvxn k which gives the k-medoids algorithm. the e step again involves for given cluster prototypes k assigning each data point to the cluster for which the dissimilarity to the corresponding prototype is smallest. the computational cost of this is okn as is the case for the standard k-means algorithm. for a general choice of dissimilarity measure the m step is potentially more complex than for k-means and so it is common to restrict each cluster prototype to be equal to one of the data vectors assigned to that cluster as this allows the algorithm to be implemented for any choice of dissimilarity measure v so long as it can be readily evaluated. thus the m step involves for each cluster k a discrete search over the nk points assigned to that cluster which requires on k evaluations of v one notable feature of the k-means algorithm is that at each iteration every data point is assigned uniquely to one and only one of the clusters. whereas some data points will be much closer to a particular centre k than to any other centre there may be other data points that lie roughly midway between cluster centres. in the latter case it is not clear that the hard assignment to the nearest cluster is the most appropriate. we shall see in the next section that by adopting a probabilistic approach we obtain soft assignments of data points to clusters in a way that reflects the level of uncertainty over the most appropriate assignment. this probabilistic formulation brings with it numerous benefits. image segmentation and compression as an illustration of the application of the k-means algorithm we consider the related problems of image segmentation and image compression. the goal of segmentation is to partition an image into regions each of which has a reasonably homogeneous visual appearance or which corresponds to objects or parts of objects and ponce each pixel in an image is a point in a space comprising the intensities of the red blue and green channels and our segmentation algorithm simply treats each pixel in the image as a separate data point. note that strictly this space is not euclidean because the channel intensities are bounded by the interval nevertheless we can apply the k-means algorithm without difficulty. we illustrate the result of running k-means to convergence for any particular value of k by re-drawing the image replacing each pixel vector with the g b intensity triplet given by the centre k to which that pixel has been assigned. results for various values of k are shown in figure we see that for a given value of k the algorithm is representing the image using a palette of only k colours. it should be emphasized that this use of k-means is not a particularly sophisticated approach to image segmentation not least because it takes no account of the spatial proximity of different pixels. the image segmentation problem is in general extremely difficult k k k original image k-means clustering figure two examples of the application of the k-means clustering algorithm to image segmentation showing the initial images together with their k-means segmentations obtained using various values of k. this also illustrates of the use of vector quantization for data compression in which smaller values of k give higher compression at the expense of poorer image quality. and remains the subject of active research and is introduced here simply to illustrate the behaviour of the k-means algorithm. we can also use the result of a clustering algorithm to perform data compression. it is important to distinguish between lossless data compression in which the goal is to be able to reconstruct the original data exactly from the compressed representation and lossy data compression in which we accept some errors in the reconstruction in return for higher levels of compression than can be achieved in the lossless case. we can apply the k-means algorithm to the problem of lossy data compression as follows. for each of the n data points we store only the identity k of the cluster to which it is assigned. we also store the values of the k cluster centres k which typically requires significantly less data provided we choose k n. each data point is then approximated by its nearest centre k. new data points can similarly be compressed by first finding the nearest k and then storing the label k instead of the original data vector. this framework is often called vector quantization and the vectors k are called code-book vectors. mixture models and em the image segmentation problem discussed above also provides an illustration of the use of clustering for data compression. suppose the original image has n pixels comprising g b values each of which is stored with bits of precision. then to transmit the whole image directly would cost bits. now suppose we first run k-means on the image data and then instead of transmitting the original pixel intensity vectors we transmit the identity of the nearest vector k. because there are k such vectors this requires k bits per pixel. we must also transmit the k code book vectors k which requires bits and so the total number of bits required to transmit the image is n k up to the nearest integer. the original image shown in figure has pixels and so requires bits to transmit directly. by comparison the compressed images require bits bits and bits respectively to transmit. these represent compression ratios compared to the original image of and respectively. we see that there is a trade-off between degree of compression and image quality. note that our aim in this example is to illustrate the k-means algorithm. if we had been aiming to produce a good image compressor then it would be more fruitful to consider small blocks of adjacent pixels for instance and thereby exploit the correlations that exist in natural images between nearby pixels. mixtures of gaussians in section we motivated the gaussian mixture model as a simple linear superposition of gaussian components aimed at providing a richer class of density models than the single gaussian. we now turn to a formulation of gaussian mixtures in terms of discrete latent variables. this will provide us with a deeper insight into this important distribution and will also serve to motivate the expectation-maximization algorithm. recall from that the gaussian mixture distribution can be written as a linear superposition of gaussians in the form px kn k k. let us introduce a k-dimensional binary random variable z having a representation in which a particular element zk is equal to and all other elements are equal to the values of zk therefore satisfy zk and k zk and we see that there are k possible states for the vector z according to which element is nonzero. we shall define the joint distribution px z in terms of a marginal distribution pz and a conditional distribution pxz corresponding to the graphical model in figure the marginal distribution over z is specified in terms of the mixing coefficients k such that pzk k mixtures of gaussians figure graphical representation of a mixture model in which the joint distribution is expressed in the form px z pzpxz. z x where the parameters k must satisfy together with k k in order to be valid probabilities. because z uses a representation we can also write this distribution in the form pz zk k similarly the conditional distribution of x given a particular value for z is a gaussian pxzk n k k which can also be written in the form pxz n k kzk exercise the joint distribution is given by pzpxz and the marginal distribution of x is then obtained by summing the joint distribution over all possible states of z to give px pzpxz kn k k z where we have made use of and thus the marginal distribution of x is a gaussian mixture of the form if we have several observations xn then because we have represented the marginal distribution in the form px z px z it follows that for every observed data point xn there is a corresponding latent variable zn. we have therefore found an equivalent formulation of the gaussian mixture involving an explicit latent variable. it might seem that we have not gained much by doing so. however we are now able to work with the joint distribution px z mixture models and em instead of the marginal distribution px and this will lead to significant simplifications most notably through the introduction of the expectation-maximization algorithm. another quantity that will play an important role is the conditional probability of z given x. we shall use to denote pzk whose value can be found using bayes theorem pzk pzk pzj kn k k jn j j section we shall view k as the prior probability of zk and the quantity as the corresponding posterior probability once we have observed x. as we shall see later can also be viewed as the responsibility that component k takes for explaining the observation x. we can use the technique of ancestral sampling to generate random samples distributed according to the gaussian mixture model. to do this we first generate a value for z which we from the marginal distribution pz and then generate a value for x from the conditional distribution techniques for sampling from standard distributions are discussed in chapter we can depict samples from the joint distribution px z by plotting points at the corresponding values of x and then colouring them according to the value of z in other words according to which gaussian component was responsible for generating them as shown in figure similarly samples from the marginal distribution px are obtained by taking the samples from the joint distribution and ignoring the values of z. these are illustrated in figure by plotting the x values without any coloured labels. we can also use this synthetic data set to illustrate the responsibilities by evaluating for every data point the posterior probability for each component in the mixture distribution from which this data set was generated. in particular we can represent the value of the responsibilities associated with data point xn by plotting the corresponding point using proportions of red blue and green ink given by for k respectively as shown in figure so for instance a data point for which will be coloured red whereas one for which will be coloured with equal proportions of blue and green ink and so will appear cyan. this should be compared with figure in which the data points were labelled using the true identity of the component from which they were generated. maximum likelihood suppose we have a data set of observations xn and we wish to model this data using a mixture of gaussians. we can represent this data set as an n d mixtures of gaussians figure example of points drawn from the mixture of gaussians shown in figure samples from the joint distribution pzpxz in which the three states of z corresponding to the three components of the mixture are depicted in red green and blue and the corresponding samples from the marginal distribution px which is obtained by simply ignoring the values of z and just plotting the x values. the data set in is said to be complete whereas that in is incomplete. the same samples in which the colours represent the value of the responsibilities associated with data point xn obtained by plotting the corresponding point using proportions of red blue and green ink given by for k respectively matrix x in which the nth row is given by xt n. similarly the corresponding latent variables will be denoted by an n k matrix z with rows zt n. if we assume that the data points are drawn independently from the distribution then we can express the gaussian mixture model for this i.i.d. data set using the graphical representation shown in figure from the log of the likelihood function is given by ln px ln kn k k before discussing how to maximize this function it is worth emphasizing that there is a significant problem associated with the maximum likelihood framework applied to gaussian mixture models due to the presence of singularities. for simplicity consider a gaussian mixture whose components have covariance matrices given by k ki where i is the unit matrix although the conclusions will hold for general covariance matrices. suppose that one of the components of the mixture model let us say the jth component has its mean j exactly equal to one of the data figure graphical representation of a gaussian mixture model for a set of n i.i.d. data points with corresponding latent points where n n. zn xn n mixture models and em figure illustration of how singularities in the likelihood function arise with mixtures of gaussians. this should be compared with the case of a single gaussian shown in figure for which no singularities arise. px points so that j xn for some value of n. this data point will then contribute a term in the likelihood function of the form x j n j i if we consider the limit j then we see that this term goes to infinity and so the log likelihood function will also go to infinity. thus the maximization of the log likelihood function is not a well posed problem because such singularities will always be present and will occur whenever one of the gaussian components collapses onto a specific data point. recall that this problem did not arise in the case of a single gaussian distribution. to understand the difference note that if a single gaussian collapses onto a data point it will contribute multiplicative factors to the likelihood function arising from the other data points and these factors will go to zero exponentially fast giving an overall likelihood that goes to zero rather than infinity. however once we have least two components in the mixture one of the components can have a finite variance and therefore assign finite probability to all of the data points while the other component can shrink onto one specific data point and thereby contribute an ever increasing additive value to the log likelihood. this is illustrated in figure these singularities provide another example of the severe over-fitting that can occur in a maximum likelihood approach. we shall see that this difficulty does not occur if we adopt a bayesian approach. for the moment however we simply note that in applying maximum likelihood to gaussian mixture models we must take steps to avoid finding such pathological solutions and instead seek local maxima of the likelihood function that are well behaved. we can hope to avoid the singularities by using suitable heuristics for instance by detecting when a gaussian component is collapsing and resetting its mean to a randomly chosen value while also resetting its covariance to some large value and then continuing with the optimization. a further issue in finding maximum likelihood solutions arises from the fact that for any given maximum likelihood solution a k-component mixture will have a total of k! equivalent solutions corresponding to the k! ways of assigning k sets of parameters to k components. in other words for any given point in the space of parameter values there will be a further k! additional points all of which give rise to exactly the same distribution. this problem is known as section mixtures of gaussians identifiability and berger and is an important issue when we wish to interpret the parameter values discovered by a model. identifiability will also arise when we discuss models having continuous latent variables in chapter however for the purposes of finding a good density model it is irrelevant because any of the equivalent solutions is as good as any other. maximizing the log likelihood function for a gaussian mixture model turns out to be a more complex problem than for the case of a single gaussian. the difficulty arises from the presence of the summation over k that appears inside the logarithm in so that the logarithm function no longer acts directly on the gaussian. if we set the derivatives of the log likelihood to zero we will no longer obtain a closed form solution as we shall see shortly. one approach is to apply gradient-based optimization techniques nocedal and wright bishop and nabney although gradient-based techniques are feasible and indeed will play an important role when we discuss mixture density networks in chapter we now consider an alternative approach known as the em algorithm which has broad applicability and which will lay the foundations for a discussion of variational inference techniques in chapter em for gaussian mixtures an elegant and powerful method for finding maximum likelihood solutions for models with latent variables is called the expectation-maximization algorithm or em algorithm et al. mclachlan and krishnan later we shall give a general treatment of em and we shall also show how em can be generalized to obtain the variational inference framework. initially we shall motivate the em algorithm by giving a relatively informal treatment in the context of the gaussian mixture model. we emphasize however that em has broad applicability and indeed it will be encountered in the context of a variety of different models in this book. let us begin by writing down the conditions that must be satisfied at a maximum of the likelihood function. setting the derivatives of ln px in with respect to the means k of the gaussian components to zero we obtain kxn k kn k k j jn j j section where we have made use of the form for the gaussian distribution. note that the posterior probabilities or responsibilities given by appear naturally on the right-hand side. multiplying by we assume to be nonsingular and rearranging we obtain k k nk where we have defined nk mixture models and em section appendix e we can interpret nk as the effective number of points assigned to cluster k. note carefully the form of this solution. we see that the mean k for the kth gaussian component is obtained by taking a weighted mean of all of the points in the data set in which the weighting factor for data point xn is given by the posterior probability that component k was responsible for generating xn. if we set the derivative of ln px with respect to k to zero and follow a similar line of reasoning making use of the result for the maximum likelihood solution for the covariance matrix of a single gaussian we obtain kxn kt k nk which has the same form as the corresponding result for a single gaussian fitted to the data set but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points associated with the corresponding component. finally we maximize ln px with respect to the mixing coefficients k. here we must take account of the constraint which requires the mixing coefficients to sum to one. this can be achieved using a lagrange multiplier and maximizing the following quantity ln px k which gives n k k j jn j j where again we see the appearance of the responsibilities. if we now multiply both sides by k and sum over k making use of the constraint we find n. using this to eliminate and rearranging we obtain k nk n so that the mixing coefficient for the kth component is given by the average responsibility which that component takes for explaining the data points. it is worth emphasizing that the results and do not constitute a closed-form solution for the parameters of the mixture model because the responsibilities depend on those parameters in a complex way through however these results do suggest a simple iterative scheme for finding a solution to the maximum likelihood problem which as we shall see turns out to be an instance of the em algorithm for the particular case of the gaussian mixture model. we first choose some initial values for the means covariances and mixing coefficients. then we alternate between the following two updates that we shall call the e step mixtures of gaussians l l l l figure illustration of the em algorithm using the old faithful set as used for the illustration of the k-means algorithm in figure see the text for details. section and the m step for reasons that will become apparent shortly. in the expectation step or e step we use the current values for the parameters to evaluate the posterior probabilities or responsibilities given by we then use these probabilities in the maximization step or m step to re-estimate the means covariances and mixing coefficients using the results and note that in so doing we first evaluate the new means using and then use these new values to find the covariances using in keeping with the corresponding result for a single gaussian distribution. we shall show that each update to the parameters resulting from an e step followed by an m step is guaranteed to increase the log likelihood function. in practice the algorithm is deemed to have converged when the change in the log likelihood function or alternatively in the parameters falls below some threshold. we illustrate the em algorithm for a mixture of two gaussians applied to the rescaled old faithful data set in figure here a mixture of two gaussians is used with centres initialized using the same values as for the k-means algorithm in figure and with precision matrices initialized to be proportional to the unit matrix. plot shows the data points in green together with the initial configuration of the mixture model in which the one standard-deviation contours for the two mixture models and em gaussian components are shown as blue and red circles. plot shows the result of the initial e step in which each data point is depicted using a proportion of blue ink equal to the posterior probability of having been generated from the blue component and a corresponding proportion of red ink given by the posterior probability of having been generated by the red component. thus points that have a significant probability for belonging to either cluster appear purple. the situation after the first m step is shown in plot in which the mean of the blue gaussian has moved to the mean of the data set weighted by the probabilities of each data point belonging to the blue cluster in other words it has moved to the centre of mass of the blue ink. similarly the covariance of the blue gaussian is set equal to the covariance of the blue ink. analogous results hold for the red component. plots and show the results after and complete cycles of em respectively. in plot the algorithm is close to convergence. note that the em algorithm takes many more iterations to reach convergence compared with the k-means algorithm and that each cycle requires significantly more computation. it is therefore common to run the k-means algorithm in order to find a suitable initialization for a gaussian mixture model that is subsequently adapted using em. the covariance matrices can conveniently be initialized to the sample covariances of the clusters found by the k-means algorithm and the mixing coefficients can be set to the fractions of data points assigned to the respective clusters. as with gradient-based approaches for maximizing the log likelihood techniques must be employed to avoid singularities of the likelihood function in which a gaussian component collapses onto a particular data point. it should be emphasized that there will generally be multiple local maxima of the log likelihood function and that em is not guaranteed to find the largest of these maxima. because the em algorithm for gaussian mixtures plays such an important role we summarize it below. em for gaussian mixtures given a gaussian mixture model the goal is to maximize the likelihood function with respect to the parameters the means and covariances of the components and the mixing coefficients. initialize the means k covariances k and mixing coefficients k and evaluate the initial value of the log likelihood. e step. evaluate the responsibilities using the current parameter values kn k k jn j j an alternative view of em m step. re-estimate the parameters using the current responsibilities new k nk new k new k nk nk n new k new k where evaluate the log likelihood ln px nk ln kn k k and check for convergence of either the parameters or the log likelihood. if the convergence criterion is not satisfied return to step an alternative view of em in this section we present a complementary view of the em algorithm that recognizes the key role played by latent variables. we discuss this approach first of all in an abstract setting and then for illustration we consider once again the case of gaussian mixtures. the goal of the em algorithm is to find maximum likelihood solutions for models having latent variables. we denote the set of all observed data by x in which the nth row represents xt n and similarly we denote the set of all latent variables by z with a corresponding row zt n. the set of all model parameters is denoted by and so the log likelihood function is given by ln px ln px z z note that our discussion will apply equally well to continuous latent variables simply by replacing the sum over z with an integral. a key observation is that the summation over the latent variables appears inside the logarithm. even if the joint distribution px z belongs to the exponential mixture models and em family the marginal distribution px typically does not as a result of this summation. the presence of the sum prevents the logarithm from acting directly on the joint distribution resulting in complicated expressions for the maximum likelihood solution. now suppose that for each observation in x we were told the corresponding value of the latent variable z. we shall call z the complete data set and we shall refer to the actual observed data x as incomplete as illustrated in figure the likelihood function for the complete data set simply takes the form ln px z and we shall suppose that maximization of this complete-data log likelihood function is straightforward. in practice however we are not given the complete data set z but only the incomplete data x. our state of knowledge of the values of the latent variables in z is given only by the posterior distribution pzx because we cannot use the complete-data log likelihood we consider instead its expected value under the posterior distribution of the latent variable which corresponds we shall see to the e step of the em algorithm. in the subsequent m step we maximize this expectation. if the current estimate for the parameters is denoted old then a pair of successive e and m steps gives rise to a revised estimate new. the algorithm is initialized by choosing some starting value for the parameters the use of the expectation may seem somewhat arbitrary. however we shall see the motivation for this choice when we give a deeper treatment of em in section in the e step we use the current parameter values old to find the posterior distribution of the latent variables given by pzx old. we then use this posterior distribution to find the expectation of the complete-data log likelihood evaluated for some general parameter value this expectation denoted q old is given by q old pzx old ln px z z in the m step we determine the revised parameter estimate new by maximizing this function new arg max note that in the definition of q old the logarithm acts directly on the joint distribution px z and so the corresponding m-step maximization will by supposition be tractable. q old. section the general em algorithm is summarized below. it has the property as we shall show later that each cycle of em will increase the incomplete-data log likelihood it is already at a local maximum. the general em algorithm given a joint distribution px z over observed variables x and latent variables z governed by parameters the goal is to maximize the likelihood function px with respect to choose an initial setting for the parameters old. an alternative view of em e step evaluate pzx old. m step evaluate new given by q old new arg max where q old pzx old ln px z exercise z check for convergence of either the log likelihood or the parameter values. if the convergence criterion is not satisfied then let old new and return to step the em algorithm can also be used to find map posterior solutions for models in which a prior p is defined over the parameters. in this case the e step remains the same as in the maximum likelihood case whereas in the m step the quantity to be maximized is given by q old ln p suitable choices for the prior will remove the singularities of the kind illustrated in figure here we have considered the use of the em algorithm to maximize a likelihood function when there are discrete latent variables. however it can also be applied when the unobserved variables correspond to missing values in the data set. the distribution of the observed values is obtained by taking the joint distribution of all the variables and then marginalizing over the missing ones. em can then be used to maximize the corresponding likelihood function. we shall show an example of the application of this technique in the context of principal component analysis in figure this will be a valid procedure if the data values are missing at random meaning that the mechanism causing values to be missing does not depend on the unobserved values. in many situations this will not be the case for instance if a sensor fails to return a value whenever the quantity it is measuring exceeds some threshold. gaussian mixtures revisited we now consider the application of this latent variable view of em to the specific case of a gaussian mixture model. recall that our goal is to maximize the log likelihood function which is computed using the observed data set x and we saw that this was more difficult than for the case of a single gaussian distribution due to the presence of the summation over k that occurs inside the logarithm. suppose then that in addition to the observed data set x we were also given the values of the corresponding discrete variables z. recall that figure shows a complete data set one that includes labels showing which component generated each data point while figure shows the corresponding incomplete data set. the graphical model for the complete data is shown in figure mixture models and em figure this shows the same graph as in figure except that we now suppose that the discrete variables zn are observed as well as the data variables xn. zn xn n now consider the problem of maximizing the likelihood for the complete data set z. from and this likelihood function takes the form px z k n k kznk znk where znk denotes the kth component of zn. taking the logarithm we obtain ln px z znk k lnn k k comparison with the log likelihood function for the incomplete data shows that the summation over k and the logarithm have been interchanged. the logarithm now acts directly on the gaussian distribution which itself is a member of the exponential family. not surprisingly this leads to a much simpler solution to the maximum likelihood problem as we now show. consider first the maximization with respect to the means and covariances. because zn is a k-dimensional vector with all elements equal to except for a single element having the value the complete-data log likelihood function is simply a sum of k independent contributions one for each mixture component. thus the maximization with respect to a mean or a covariance is exactly as for a single gaussian except that it involves only the subset of data points that are assigned to that component. for the maximization with respect to the mixing coefficients we note that these are coupled for different values of k by virtue of the summation constraint again this can be enforced using a lagrange multiplier as before and leads to the result k n znk so that the mixing coefficients are equal to the fractions of data points assigned to the corresponding components. thus we see that the complete-data log likelihood function can be maximized trivially in closed form. in practice however we do not have values for the latent variables so as discussed earlier we consider the expectation with respect to the posterior distribution of the latent variables of the complete-data log likelihood. exercise section exercise an alternative view of em using and together with bayes theorem we see that this posterior distribution takes the form pzx kn k kznk and hence factorizes over n so that under the posterior distribution the are independent. this is easily verified by inspection of the directed graph in figure and making use of the d-separation criterion. the expected value of the indicator variable znk under this posterior distribution is then given by znk kn k kznk jn j j kn k k jn j j eznk znk znj which is just the responsibility of component k for data point xn. the expected value of the complete-data log likelihood function is therefore given by ezln px z k lnn k k we can now proceed as follows. first we choose some initial values for the parameters old old and old and use these to evaluate the responsibilities e step. we then keep the responsibilities fixed and maximize with respect to k k and k m step. this leads to closed form solutions for new new and new given by and as before. this is precisely the em algorithm for gaussian mixtures as derived earlier. we shall gain more insight into the role of the expected complete-data log likelihood function when we give a proof of convergence of the em algorithm in section relation to k-means comparison of the k-means algorithm with the em algorithm for gaussian mixtures shows that there is a close similarity. whereas the k-means algorithm performs a hard assignment of data points to clusters in which each data point is associated uniquely with one cluster the em algorithm makes a soft assignment based on the posterior probabilities. in fact we can derive the k-means algorithm as a particular limit of em for gaussian mixtures as follows. consider a gaussian mixture model in which the covariance matrices of the mixture components are given by where is a variance parameter that is shared mixture models and em by all of the components and i is the identity matrix so that px k k exp we now consider the em algorithm for a mixture of k gaussians of this form in which we treat as a fixed constant instead of a parameter to be re-estimated. from the posterior probabilities or responsibilities for a particular data point xn are given by k exp j j exp if we consider the limit we see that in the denominator the term for which is smallest will go to zero most slowly and hence the responsibilities for the data point xn all go to zero except for term j for which the responsibility will go to unity. note that this holds independently of the values of the k so long as none of the k is zero. thus in this limit we obtain a hard assignment of data points to clusters just as in the k-means algorithm so that rnk where rnk is defined by each data point is thereby assigned to the cluster having the closest mean. the em re-estimation equation for the k given by then reduces to the k-means result note that the re-estimation formula for the mixing coefficients simply re-sets the value of k to be equal to the fraction of data points assigned to cluster k although these parameters no longer play an active role in the algorithm. finally in the limit the expected complete-data log likelihood given by exercise becomes ezln px z const. thus we see that in this limit maximizing the expected complete-data log likelihood is equivalent to minimizing the distortion measure j for the k-means algorithm given by note that the k-means algorithm does not estimate the covariances of the clusters but only the cluster means. a hard-assignment version of the gaussian mixture model with general covariance matrices known as the elliptical k-means algorithm has been considered by sung and poggio mixtures of bernoulli distributions so far in this chapter we have focussed on distributions over continuous variables described by mixtures of gaussians. as a further example of mixture modelling and to illustrate the em algorithm in a different context we now discuss mixtures of discrete binary variables described by bernoulli distributions. this model is also known as latent class analysis and henry mclachlan and peel as well as being of practical importance in its own right our discussion of bernoulli mixtures will also lay the foundation for a consideration of hidden markov models over discrete variables. section an alternative view of em consider a set of d binary variables xi where i d each of which is governed by a bernoulli distribution with parameter i so that px i xi xi where x xdt and dt. we see that the individual variables xi are independent given the mean and covariance of this distribution are easily seen to be ex covx diag i. now let us consider a finite mixture of these distributions given by px kpx k where k k and xi. xi exercise the mean and covariance of this mixture distribution are given by px k k k ex exext covx k k t k k where k diag ki. because the covariance matrix covx is no longer diagonal the mixture distribution can capture correlations between the variables unlike a single bernoulli distribution. if we are given a data set x xn then the log likelihood function for this model is given by ln px kpxn k ln again we see the appearance of the summation inside the logarithm so that the maximum likelihood solution no longer has closed form. we now derive the em algorithm for maximizing the likelihood function for the mixture of bernoulli distributions. to do this we first introduce an explicit latent mixture models and em exercise variable z associated with each instance of x. as in the case of the gaussian mixture z zkt is a binary k-dimensional variable having a single component equal to with all other components equal to we can then write the conditional distribution of x given the latent variable as pxz px kzk zk k while the prior distribution for the latent variables is the same as for the mixture of gaussians model so that pz if we form the product of pxz and pz and then marginalize over z then we recover in order to derive the em algorithm we first write down the complete-data log likelihood function which is given by ln px z znk ln k ln ki xni ki where x and z next we take the expectation of the complete-data log likelihood with respect to the posterior distribution of the latent variables to give ezln px z ln k ln ki xni ki where eznk is the posterior probability or responsibility of component k given data point xn. in the e step these responsibilities are evaluated using bayes theorem which takes the form eznk znk znk kpxn kznk jpxn j kpxn k jpxn j znj an alternative view of em if we consider the sum over n in we see that the responsibilities enter only through two terms which can be written as nk nk xk where nk is the effective number of data points associated with component k. in the m step we maximize the expected complete-data log likelihood with respect to the parameters k and if we set the derivative of with respect to k equal to zero and rearrange the terms we obtain exercise exercise exercise section k xk. we see that this sets the mean of component k equal to a weighted mean of the data with weighting coefficients given by the responsibilities that component k takes for data points. for the maximization with respect to k we need to introduce a k k following analogous lagrange multiplier to enforce the constraint steps to those used for the mixture of gaussians we then obtain k nk n which represents the intuitively reasonable result that the mixing coefficient for component k is given by the effective fraction of points in the data set explained by that component. note that in contrast to the mixture of gaussians there are no singularities in which the likelihood function goes to infinity. this can be seen by noting that the likelihood function is bounded above because pxn k there exist singularities at which the likelihood function goes to zero but these will not be found by em provided it is not initialized to a pathological starting point because the em algorithm always increases the value of the likelihood function until a local maximum is found. we illustrate the bernoulli mixture model in figure by using it to model handwritten digits. here the digit images have been turned into binary vectors by setting all elements whose values exceed to and setting the remaining elements to we now fit a data set of n such digits comprising the digits and with a mixture of k bernoulli distributions by running iterations of the em algorithm. the mixing coefficients were initialized to k and the parameters kj were set to random values chosen uniformly in j kj the range and then normalized to satisfy the constraint that we see that a mixture of bernoulli distributions is able to find the three clusters in the data set corresponding to the different digits. the conjugate prior for the parameters of a bernoulli distribution is given by the beta distribution and we have seen that a beta prior is equivalent to introducing mixture models and em figure illustration of the bernoulli mixture model in which the top row shows examples from the digits data set after converting the pixel values from grey scale to binary using a threshold of on the bottom row the first three images show the parameters ki for each of the three components in the mixture model. as a comparison we also fit the same data set using a single multivariate bernoulli distribution again using maximum likelihood. this amounts to simply averaging the counts in each pixel and is shown by the right-most image on the bottom row. section exercise exercise additional effective observations of x. we can similarly introduce priors into the bernoulli mixture model and use em to maximize the posterior probability distributions. it is straightforward to extend the analysis of bernoulli mixtures to the case of multinomial binary variables having m states by making use of the discrete distribution again we can introduce dirichlet priors over the model parameters if desired. em for bayesian linear regression as a third example of the application of em we return to the evidence approximation for bayesian linear regression. in section we obtained the reestimation equations for the hyperparameters and by evaluation of the evidence and then setting the derivatives of the resulting expression to zero. we now turn to an alternative approach for finding and based on the em algorithm. recall that our goal is to maximize the evidence function pt given by with respect to and because the parameter vector w is marginalized out we can regard it as a latent variable and hence we can optimize this marginal likelihood function using em. in the e step we compute the posterior distribution of w given the current setting of the parameters and and then use this to find the expected complete-data log likelihood. in the m step we maximize this quantity with respect to and we have already derived the posterior distribution of w because this is given by the complete-data log likelihood function is then given by ln pt w ln ptw ln pw an alternative view of em where the likelihood ptw and the prior pw are given by and respectively and yx w is given by taking the expectation with respect to the posterior distribution of w then gives e pt w m n ln wt e wtw ln e exercise setting the derivatives with respect to to zero we obtain the m step re-estimation equation m e m n mn trsn mt exercise an analogous result holds for note that this re-estimation equation takes a slightly different form from the corresponding result derived by direct evaluation of the evidence function. however they each involve computation and inversion eigen decomposition of an m m matrix and hence will have comparable computational cost per iteration. these two approaches to determining should of course converge to the same result they find the same local maximum of the evidence function. this can be verified by first noting that the quantity is defined by m m trsn i at a stationary point of the evidence function the re-estimation equation will be self-consistently satisfied and hence we can substitute for to give n mn m trsn mt and solving for we obtain which is precisely the em re-estimation equation. as a final example we consider a closely related model namely the relevance vector machine for regression discussed in section there we used direct maximization of the marginal likelihood to derive re-estimation equations for the hyperparameters and here we consider an alternative approach in which we view the weight vector w as a latent variable and apply the em algorithm. the e step involves finding the posterior distribution over the weights and this is given by in the m step we maximize the expected complete-data log likelihood which is defined by ew ptx w exercise where the expectation is taken with respect to the posterior distribution computed using the old parameter values. to compute the new parameter values we maximize with respect to and to give mixture models and em new i new i ii n i i exercise these re-estimation equations are formally equivalent to those obtained by direct maxmization. the em algorithm in general the expectation maximization algorithm or em algorithm is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables et al. mclachlan and krishnan here we give a very general treatment of the em algorithm and in the process provide a proof that the em algorithm derived heuristically in sections and for gaussian mixtures does indeed maximize the likelihood function and tusnady hathaway neal and hinton our discussion will also form the basis for the derivation of the variational inference framework. section consider a probabilistic model in which we collectively denote all of the observed variables by x and all of the hidden variables by z. the joint distribution px z is governed by a set of parameters denoted our goal is to maximize the likelihood function that is given by px px z z here we are assuming z is discrete although the discussion is identical if z comprises continuous variables or a combination of discrete and continuous variables with summation replaced by integration as appropriate. we shall suppose that direct optimization of px is difficult but that optimization of the complete-data likelihood function px z is significantly easier. next we introduce a distribution qz defined over the latent variables and we observe that for any choice of qz the following decomposition holds ln px lq where we have defined lq qz ln qz ln z z px z qz pzx qz note that lq is a functional appendix d for a discussion of functionals of the distribution qz and a function of the parameters it is worth studying the em algorithm in general figure illustration of the decomposition given by which holds for any choice of distribution qz. because the kullback-leibler divergence satisfies we see that the quantity lq is a lower bound on the log likelihood function ln px klqp lq ln px exercise section carefully the forms of the expressions and and in particular noting that they differ in sign and also that lq contains the joint distribution of x and z while contains the conditional distribution of z given x. to verify the decomposition we first make use of the product rule of probability to give ln px z ln pzx ln px which we then substitute into the expression for lq this gives rise to two terms one of which cancels while the other gives the required log likelihood ln px after noting that qz is a normalized distribution that sums to from we see that is the kullback-leibler divergence between qz and the posterior distribution pzx recall that the kullback-leibler divergence satisfies with equality if and only if qz pzx it therefore follows from that lq ln px in other words that lq is a lower bound on ln px the decomposition is illustrated in figure the em algorithm is a two-stage iterative optimization technique for finding maximum likelihood solutions. we can use the decomposition to define the em algorithm and to demonstrate that it does indeed maximize the log likelihood. suppose that the current value of the parameter vector is old. in the e step the lower bound lq old is maximized with respect to qz while holding old fixed. the solution to this maximization problem is easily seen by noting that the value of ln px old does not depend on qz and so the largest value of lq old will occur when the kullback-leibler divergence vanishes in other words when qz is equal to the posterior distribution pzx old. in this case the lower bound will equal the log likelihood as illustrated in figure in the subsequent m step the distribution qz is held fixed and the lower bound lq is maximized with respect to to give some new value new. this will cause the lower bound l to increase it is already at a maximum which will necessarily cause the corresponding log likelihood function to increase. because the distribution q is determined using the old parameter values rather than the new values and is held fixed during the m step it will not equal the new posterior distribution pzx new and hence there will be a nonzero kl divergence. the increase in the log likelihood function is therefore greater than the increase in the lower bound as mixture models and em klqp figure illustration of the e step of the em algorithm. the q distribution is set equal to the posterior distribution for the current parameter values old causing the lower bound to move up to the same value as the log likelihood function with the kl divergence vanishing. lq old ln px old shown in figure if we substitute qz pzx old into we see that after the e step the lower bound takes the form lq pzx old ln px z pzx old ln pzx old z q old const z where the constant is simply the negative entropy of the q distribution and is therefore independent of thus in the m step the quantity that is being maximized is the expectation of the complete-data log likelihood as we saw earlier in the case of mixtures of gaussians. note that the variable over which we are optimizing appears only inside the logarithm. if the joint distribution pz x comprises a member of the exponential family or a product of such members then we see that the logarithm will cancel the exponential and lead to an m step that will be typically much simpler than the maximization of the corresponding incomplete-data log likelihood function px the operation of the em algorithm can also be viewed in the space of parameters as illustrated schematically in figure here the red curve depicts the klqp figure illustration of the m step of the em the distribution qz algorithm. is held fixed and the lower bound lq is maximized with respect to the parameter vector to give a revised value new. because the kl divergence is nonnegative this causes the log likelihood ln px to increase by at least as much as the lower bound does. lq new ln px new figure the em algorithm involves alternately computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values. see the text for a full discussion. the em algorithm in general ln px l old new exercise complete data log likelihood function whose value we wish to maximize. we start with some initial parameter value old and in the first e step we evaluate the posterior distribution over latent variables which gives rise to a lower bound l whose value equals the log likelihood at as shown by the blue curve. note that the bound makes a tangential contact with the log likelihood at so that both curves have the same gradient. this bound is a convex function having a unique maximum mixture components from the exponential family. in the m step the bound is maximized giving the value which gives a larger value of log likelihood than the subsequent e step then constructs a bound that is tangential at as shown by the green curve. for the particular case of an independent identically distributed data set x will comprise n data points while z will comprise n corresponding latent variables where n n. from the independence assumption we have n pxn zn and by marginalizing over the we have px px z n pxn. using the sum and product rules we see that the posterior probability that is evaluated in the e step takes the form pzx px z px z z z pxn zn pxn zn pznxn and so the posterior distribution also factorizes with respect to n. in the case of the gaussian mixture model this simply says that the responsibility that each of the mixture components takes for a particular data point xn depends only on the value of xn and on the parameters of the mixture components not on the values of the other data points. we have seen that both the e and the m steps of the em algorithm are increasing the value of a well-defined bound on the log likelihood function and that the mixture models and em complete em cycle will change the model parameters in such a way as to cause the log likelihood to increase it is already at a maximum in which case the parameters remain unchanged. we can also use the em algorithm to maximize the posterior distribution p for models in which we have introduced a prior p over the parameters. to see this we note that as a function of we have p p xpx and so ln p ln p x ln px. making use of the decomposition we have ln p lq ln p ln px lq ln p ln px. where ln px is a constant. we can again optimize the right-hand side alternately with respect to q and the optimization with respect to q gives rise to the same estep equations as for the standard em algorithm because q only appears in lq the m-step equations are modified through the introduction of the prior term ln p which typically requires only a small modification to the standard maximum likelihood m-step equations. the em algorithm breaks down the potentially difficult problem of maximizing the likelihood function into two stages the e step and the m step each of which will often prove simpler to implement. nevertheless for complex models it may be the case that either the e step or the m step or indeed both remain intractable. this leads to two possible extensions of the em algorithm as follows. the generalized em or gem algorithm addresses the problem of an intractable m step. instead of aiming to maximize lq with respect to it seeks instead to change the parameters in such a way as to increase its value. again because lq is a lower bound on the log likelihood function each complete em cycle of the gem algorithm is guaranteed to increase the value of the log likelihood the parameters already correspond to a local maximum. one way to exploit the gem approach would be to use one of the nonlinear optimization strategies such as the conjugate gradients algorithm during the m step. another form of gem algorithm known as the expectation conditional maximization or ecm algorithm involves making several constrained optimizations within each m step and rubin for instance the parameters might be partitioned into groups and the m step is broken down into multiple steps each of which involves optimizing one of the subset with the remainder held fixed. we can similarly generalize the e step of the em algorithm by performing a partial rather than complete optimization of lq with respect to qz and hinton as we have seen for any given value of there is a unique maximum of lq with respect to qz that corresponds to the posterior distribution q pzx and that for this choice of qz the bound lq is equal to the log likelihood function ln px it follows that any algorithm that converges to the global maximum of lq will find a value of that is also a global maximum of the log likelihood ln px provided px z is a continuous function of exercises then by continuity any local maximum of lq will also be a local maximum of ln px consider the case of n independent data points xn with corresponding latent variables zn the joint distribution px z factorizes over the data points and this structure can be exploited in an incremental form of em in which at each em cycle only one data point is processed at a time. in the e step instead of recomputing the responsibilities for all of the data points we just re-evaluate the responsibilities for one data point. it might appear that the subsequent m step would require computation involving the responsibilities for all of the data points. however if the mixture components are members of the exponential family then the responsibilities enter only through simple sufficient statistics and these can be updated efficiently. consider for instance the case of a gaussian mixture and suppose we perform an update for data point m in which the corresponding old and new values of the responsibilities are denoted oldzmk and newzmk. in the m step the required sufficient statistics can be updated incrementally. for instance for the means the sufficient statistics are defined by and from which we obtain new k old k newzmk oldzmk n new k xm old k together with n new k n old k newzmk oldzmk. the corresponding results for the covariances and the mixing coefficients are analogous. thus both the e step and the m step take fixed time that is independent of the total number of data points. because the parameters are revised after each data point rather than waiting until after the whole data set is processed this incremental version can converge faster than the batch version. each e or m step in this incremental algorithm is increasing the value of lq and as we have shown above if the algorithm converges to a local global maximum of lq this will correspond to a local global maximum of the log likelihood function ln px www consider the k-means algorithm discussed in section show that as a consequence of there being a finite number of possible assignments for the set of discrete indicator variables rnk and that for each such assignment there is a unique optimum for the k the k-means algorithm must converge after a finite number of iterations. apply the robbins-monro sequential estimation procedure described in section to the problem of finding the roots of the regression function given by the derivatives of j in with respect to k. show that this leads to a stochastic k-means algorithm in which for each data point xn the nearest prototype k is updated using exercise exercises mixture models and em www consider a gaussian mixture model in which the marginal distribution pz for the latent variable is given by and the conditional distribution pxz for the observed variable is given by show that the marginal distribution px obtained by summing pzpxz over all possible values of z is a gaussian mixture of the form suppose we wish to use the em algorithm to maximize the posterior distribution over parameters p for a model containing latent variables where x is the observed data set. show that the e step remains the same as in the maximum likelihood case whereas in the m step the quantity to be maximized is given by q old ln p where q old is defined by consider the directed graph for a gaussian mixture model shown in figure by making use of the d-separation criterion discussed in section show that the posterior distribution of the latent variables factorizes with respect to the different data points so that pzx pznxn consider a special case of a gaussian mixture model in which the covariance matrices k of the components are all constrained to have a common value derive the em equations for maximizing the likelihood function under such a model. www verify that maximization of the complete-data log likelihood for a gaussian mixture model leads to the result that the means and covariances of each component are fitted independently to the corresponding group of data points and the mixing coefficients are given by the fractions of points in each group. www show that if we maximize with respect to k while keeping the responsibilities fixed we obtain the closed form solution given by show that if we maximize with respect to k and k while keeping the responsibilities fixed we obtain the closed form solutions given by and consider a density model given by a mixture distribution px kpxk and suppose that we partition the vector x into two parts so that x xb. show that the conditional density pxbxa is itself a mixture distribution and find expressions for the mixing coefficients and for the component densities. exercises in section we obtained a relationship between k means and em for gaussian mixtures by considering a mixture model in which all components have covariance show that in the limit maximizing the expected completedata log likelihood for this model given by is equivalent to minimizing the distortion measure j for the k-means algorithm given by www consider a mixture distribution of the form px kpxk where the elements of x could be discrete or continuous or a combination of these. denote the mean and covariance of pxk by k and k respectively. show that the mean and covariance of the mixture distribution are given by and using the re-estimation equations for the em algorithm show that a mixture of bernoulli distributions with its parameters set to values corresponding to a maximum of the likelihood function has the property that ex n xn x. nents have the same mean k for k k then the em algorithm will hence show that if the parameters of this model are initialized such that all compo converge after one iteration for any choice of the initial mixing coefficients and that this solution has the property k x. note that this represents a degenerate case of the mixture model in which all of the components are identical and in practice we try to avoid such solutions by using an appropriate initialization. consider the joint distribution of latent and observed variables for the bernoulli distribution obtained by forming the product of pxz given by and pz given by show that if we marginalize this joint distribution with respect to z then we obtain www show that if we maximize the expected complete-data log likelihood function for a mixture of bernoulli distributions with respect to k we obtain the m step equation show that if we maximize the expected complete-data log likelihood function for a mixture of bernoulli distributions with respect to the mixing coefficients k using a lagrange multiplier to enforce the summation constraint we obtain the m step equation www show that as a consequence of the constraint pxn k for the discrete variable xn the incomplete-data log likelihood function for a mixture of bernoulli distributions is bounded above and hence that there are no singularities for which the likelihood goes to infinity. mixture models and em consider a bernoulli mixture model as discussed in section together with a prior distribution p kak bk over each of the parameter vectors k given by the beta distribution and a dirichlet prior p given by derive the em algorithm for maximizing the posterior probability p consider a d-dimensional variable x each of whose components i is itself a multinomial variable of degree m so that x is a binary vector with components xij where i d and j m subject to the constraint that j xij for all i. suppose that the distribution of these variables is described by a mixture of the discrete multinomial distributions considered in section so that px where kpx k xij kij. px k the parameters kij represent the probabilities pxij k and must satisfy kij together with the constraint j kij for all values of k and i. given an observed data set where n n derive the e and m step equations of the em algorithm for optimizing the mixing coefficients k and the component parameters kij of this distribution by maximum likelihood. www show that maximization of the expected complete-data log likelihood function for the bayesian linear regression model leads to the m step reestimation result for using the evidence framework of section derive the m-step re-estimation equations for the parameter in the bayesian linear regression model analogous to the result for by maximization of the expected complete-data log likelihood defined by derive the m step equations and for re-estimating the hyperparameters of the relevance vector machine for regression. www in section we used direct maximization of the marginal likelihood to derive the re-estimation equations and for finding values of the hyperparameters and for the regression rvm. similarly in section we used the em algorithm to maximize the same marginal likelihood giving the re-estimation equations and show that these two sets of re-estimation equations are formally equivalent. verify the relation in which lq and are defined by and respectively. exercises www show that the lower bound lq given by with qz pzx has the same gradient with respect to as the log likelihood function ln px at the point www consider the incremental form of the em algorithm for a mixture of gaussians in which the responsibilities are recomputed only for a specific data point xm. starting from the m-step formulae and derive the results and for updating the component means. derive m-step formulae for updating the covariance matrices and mixing coefficients in a gaussian mixture model when the responsibilities are updated incrementally analogous to the result for updating the means. approximate inference a central task in the application of probabilistic models is the evaluation of the posterior distribution pzx of the latent variables z given the observed data variables x and the evaluation of expectations computed with respect to this distribution. the model might also contain some deterministic parameters which we will leave implicit for the moment or it may be a fully bayesian model in which any unknown parameters are given prior distributions and are absorbed into the set of latent variables denoted by the vector z. for instance in the em algorithm we need to evaluate the expectation of the complete-data log likelihood with respect to the posterior distribution of the latent variables. for many models of practical interest it will be infeasible to evaluate the posterior distribution or indeed to compute expectations with respect to this distribution. this could be because the dimensionality of the latent space is too high to work with directly or because the posterior distribution has a highly complex form for which expectations are not analytically tractable. in the case of continuous variables the required integrations may not have closed-form approximate inference analytical solutions while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration. for discrete variables the marginalizations involve summing over all possible configurations of the hidden variables and though this is always possible in principle we often find in practice that there may be exponentially many hidden states so that exact calculation is prohibitively expensive. in such situations we need to resort to approximation schemes and these fall broadly into two classes according to whether they rely on stochastic or deterministic approximations. stochastic techniques such as markov chain monte carlo described in chapter have enabled the widespread use of bayesian methods across many domains. they generally have the property that given infinite computational resource they can generate exact results and the approximation arises from the use of a finite amount of processor time. in practice sampling methods can be computationally demanding often limiting their use to small-scale problems. also it can be difficult to know whether a sampling scheme is generating independent samples from the required distribution. in this chapter we introduce a range of deterministic approximation schemes some of which scale well to large applications. these are based on analytical approximations to the posterior distribution for example by assuming that it factorizes in a particular way or that it has a specific parametric form such as a gaussian. as such they can never generate exact results and so their strengths and weaknesses are complementary to those of sampling methods. in section we discussed the laplace approximation which is based on a local gaussian approximation to a mode a maximum of the distribution. here we turn to a family of approximation techniques called variational inference or variational bayes which use more global criteria and which have been widely applied. we conclude with a brief introduction to an alternative variational framework known as expectation propagation. variational inference variational methods have their origins in the century with the work of euler lagrange and others on the calculus of variations. standard calculus is concerned with finding derivatives of functions. we can think of a function as a mapping that takes the value of a variable as the input and returns the value of the function as the output. the derivative of the function then describes how the output value varies as we make infinitesimal changes to the input value. similarly we can define a functional as a mapping that takes a function as the input and that returns the value of the functional as the output. an example would be the entropy hp which takes a probability distribution px as the input and returns the quantity hp px ln px dx variational inference as the output. we can the introduce the concept of a functional derivative which expresses how the value of the functional changes in response to infinitesimal changes to the input function et al. the rules for the calculus of variations mirror those of standard calculus and are discussed in appendix d. many problems can be expressed in terms of an optimization problem in which the quantity being optimized is a functional. the solution is obtained by exploring all possible input functions to find the one that maximizes or minimizes the functional. variational methods have broad applicability and include such areas as finite element methods and maximum entropy although there is nothing intrinsically approximate about variational methods they do naturally lend themselves to finding approximate solutions. this is done by restricting the range of functions over which the optimization is performed for instance by considering only quadratic functions or by considering functions composed of a linear combination of fixed basis functions in which only the coefficients of the linear combination can vary. in the case of applications to probabilistic inference the restriction may for example take the form of factorization assumptions et al. jaakkola now let us consider in more detail how the concept of variational optimization can be applied to the inference problem. suppose we have a fully bayesian model in which all parameters are given prior distributions. the model may also have latent variables as well as parameters and we shall denote the set of all latent variables and parameters by z. similarly we denote the set of all observed variables by x. for example we might have a set of n independent identically distributed data for which x xn and z zn. our probabilistic model specifies the joint distribution px z and our goal is to find an approximation for the posterior distribution pzx as well as for the model evidence px. as in our discussion of em we can decompose the log marginal probability using ln px lq where we have defined lq dz dz. qz ln px z qz pzx qz qz ln this differs from our discussion of em only in that the parameter vector no longer appears because the parameters are now stochastic variables and are absorbed into z. since in this chapter we will mainly be interested in continuous variables we have used integrations rather than summations in formulating this decomposition. however the analysis goes through unchanged if some or all of the variables are discrete simply by replacing the integrations with summations as required. as before we can maximize the lower bound lq by optimization with respect to the distribution qz which is equivalent to minimizing the kl divergence. if we allow any possible choice for qz then the maximum of the lower bound occurs when the kl divergence vanishes which occurs when qz equals the posterior distribution pzx. approximate inference figure illustration of the variational approximation for the example considered earlier in figure the left-hand plot shows the original distribution along with the laplace and variational approximations and the right-hand plot shows the negative logarithms of the corresponding curves. however we shall suppose the model is such that working with the true posterior distribution is intractable. we therefore consider instead a restricted family of distributions qz and then seek the member of this family for which the kl divergence is minimized. our goal is to restrict the family sufficiently that they comprise only tractable distributions while at the same time allowing the family to be sufficiently rich and flexible that it can provide a good approximation to the true posterior distribution. it is important to emphasize that the restriction is imposed purely to achieve tractability and that subject to this requirement we should use as rich a family of approximating distributions as possible. in particular there is no over-fitting associated with highly flexible distributions. using more flexible approximations simply allows us to approach the true posterior distribution more closely. one way to restrict the family of approximating distributions is to use a parametric distribution qz governed by a set of parameters the lower bound lq then becomes a function of and we can exploit standard nonlinear optimization techniques to determine the optimal values for the parameters. an example of this approach in which the variational distribution is a gaussian and we have optimized with respect to its mean and variance is shown in figure factorized distributions here we consider an alternative way in which to restrict the family of distributions qz. suppose we partition the elements of z into disjoint groups that we denote by zi where i m. we then assume that the q distribution factorizes with respect to these groups so that qz qizi. variational inference it should be emphasized that we are making no further assumptions about the distribution. in particular we place no restriction on the functional forms of the individual factors qizi. this factorized form of variational inference corresponds to an approximation framework developed in physics called mean field theory amongst all distributions qz having the form we now seek that distribution for which the lower bound lq is largest. we therefore wish to make a free form optimization of lq with respect to all of the distributions qizi which we do by optimizing with respect to each of the factors in turn. to achieve this we first substitute into and then dissect out the dependence on one of the factors qjzj. denoting qjzj by simply qj to keep the notation uncluttered we then obtain lq qj zj dzj where we have defined a new zj by the relation zj px z const. here the notation denotes an expectation with respect to the q distributions over all variables zi for i j so that ln px z qj ln qj dzj const qj ln qj dzj const ln px z qi dzi dzj ln qi dz i qi i qj px z ln px z qi dzi. now suppose we keep the fixed and maximize lq in with respect to all possible forms for the distribution qjzj. this is easily done by recognizing that is a negative kullback-leibler divergence between qjzj and zj. thus maximizing is equivalent to minimizing the kullback-leibler leonhard euler euler was a swiss mathematician and physicist who worked in st. petersburg and berlin and who is widely considered to be one of the greatest mathematicians of all time. he is certainly the most prolific and his collected works fill volumes. amongst his many contributions he formulated the modern theory of the function he developed with lagrange the calculus of variations and he discovered the formula ei which relates four of the most important numbers in mathematics. during the last years of his life he was almost totally blind and yet he produced nearly half of his results during this period. approximate inference divergence and the minimum occurs when qjzj zj. thus we obtain a general expression for the optimal solution j given by ln j px z const. it is worth taking a few moments to study the form of this solution as it provides the basis for applications of variational methods. it says that the log of the optimal solution for factor qj is obtained simply by considering the log of the joint distribution over all hidden and visible variables and then taking the expectation with respect to all of the other factors for i j. the additive constant in is set by normalizing the distribution thus if we take the exponential of both sides and normalize we have exp px z j j exp px z dzj in practice we shall find it more convenient to work with the form and then reinstate the normalization constant required by inspection. this will become clear from subsequent examples. the set of equations given by for j m represent a set of consistency conditions for the maximum of the lower bound subject to the factorization constraint. however they do not represent an explicit solution because the expression on the right-hand side of for the optimum j depends on expectations computed with respect to the other factors qizi for i j. we will therefore seek a consistent solution by first initializing all of the factors qizi appropriately and then cycling through the factors and replacing each in turn with a revised estimate given by the right-hand side of evaluated using the current estimates for all of the other factors. convergence is guaranteed because bound is convex with respect to each of the factors qizi and vandenberghe properties of factorized approximations our approach to variational inference is based on a factorized approximation to the true posterior distribution. let us consider for a moment the problem of approximating a general distribution by a factorized distribution. to begin with we discuss the problem of approximating a gaussian distribution using a factorized gaussian which will provide useful insight into the types of inaccuracy introduced in using factorized approximations. consider a gaussian distribution pz n over two correlated variables z in which the mean and precision have elements and due to the symmetry of the precision matrix. now suppose we wish to approximate this distribution using a factorized gaussian of the form qz we first apply the general result to find an expression for the variational inference optimal factor in doing so it is useful to note that on the right-hand side we only need to retain those terms that have some functional dependence on because all other terms can be absorbed into the normalization constant. thus we have ln pz const const. const next we observe that the right-hand side of this expression is a quadratic function of and so we can identify as a gaussian distribution. it is worth emphasizing that we did not assume that qzi is gaussian but rather we derived this result by variational optimization of the kl divergence over all possible distributions qzi. note also that we do not need to consider the additive constant in explicitly because it represents the normalization constant that can be found at the end by inspection if required. using the technique of completing the square we can identify the mean and precision of this gaussian giving section n where is also gaussian and can be written as by symmetry n in which note that these solutions are coupled so that depends on expectations computed with respect to and vice versa. in general we address this by treating the variational solutions as re-estimation equations and cycling through the variables in turn updating them until some convergence criterion is satisfied. we shall see an example of this shortly. here however we note that the problem is sufficiently simple that a closed form solution can be found. in particular because and we see that the two equations are satisfied if we take and and it is easily shown that this is the only solution provided the distribution is nonsingular. this result is illustrated in figure we see that the mean is correctly captured but that the variance of qz is controlled by the direction of smallest variance of pz and that the variance along the orthogonal direction is significantly under-estimated. it is a general result that a factorized variational approximation tends to give approximations to the posterior distribution that are too compact. by way of comparison suppose instead that we had been minimizing the reverse kullback-leibler divergence as we shall see this form of kl divergence exercise approximate inference figure comparison of the the two alternative forms for kullback-leibler divergence. the green contours corresponding to and standard deviations for a correlated gaussian distribution pz over two variables and and the red contours represent the corresponding levels for an qz approximating over the same variables given by the product of two independent univariate gaussian distributions whose parameters are obtained by minimization of the kullbackleibler divergence and the reverse kullback-leibler divergence distribution is used in an alternative approximate inference framework called expectation propagation. we therefore consider the general problem of minimizing when qz is a factorized approximation of the form the kl divergence can then be written in the form pz ln qizi dz const section exercise where the constant term is simply the entropy of pz and so does not depend on qz. we can now optimize with respect to each of the factors qjzj which is easily done using a lagrange multiplier to give j pz dzi pzj. in this case we find that the optimal solution for qjzj is just given by the corresponding marginal distribution of pz. note that this is a closed-form solution and so does not require iteration. to apply this result to the illustrative example of a gaussian distribution pz over a vector z we can use which gives the result shown in figure we see that once again the mean of the approximation is correct but that it places significant probability mass in regions of variable space that have very low probability. the difference between these two results can be understood by noting that there is a large positive contribution to the kullback-leibler divergence qz ln dz pz qz variational inference figure another comparison of the two alternative forms for the kullback-leibler divergence. the blue contours show a bimodal distribution pz given by a mixture of two gaussians and the red contours correspond to the single gaussian distribution qz that best approximates pz in the sense of minimizing the kullbackleibler divergence as in but now the red contours correspond to a gaussian distribution qz found by numerical minimization of the kullback-leibler divergence as in but showing a different local minimum of the kullback-leibler divergence. from regions of z space in which pz is near zero unless qz is also close to zero. thus minimizing this form of kl divergence leads to distributions qz that avoid regions in which pz is small. conversely the kullback-leibler divergence is minimized by distributions qz that are nonzero in regions where pz is nonzero. we can gain further insight into the different behaviour of the two kl divergences if we consider approximating a multimodal distribution by a unimodal one as illustrated in figure in practical applications the true posterior distribution will often be multimodal with most of the posterior mass concentrated in some number of relatively small regions of parameter space. these multiple modes may arise through nonidentifiability in the latent space or through complex nonlinear dependence on the parameters. both types of multimodality were encountered in chapter in the context of gaussian mixtures where they manifested themselves as multiple maxima in the likelihood function and a variational treatment based on the minimization of will tend to find one of these modes. by contrast if we were to minimize the resulting approximations would average across all of the modes and in the context of the mixture model would lead to poor predictive distributions the average of two good parameter values is typically itself not a good parameter value. it is possible to make use of to define a useful inference procedure but this requires a rather different approach to the one discussed here and will be considered in detail when we discuss expectation propagation. the two forms of kullback-leibler divergence are members of the alpha family section approximate inference of divergences and silvey amari minka defined by d dx where is a continuous parameter. the kullback-leibler divergence corresponds to the limit whereas corresponds to the limit for all values of we have d with equality if and only if px qx. suppose px is a fixed distribution and we minimize d with respect to some set of distributions qx. then for the divergence is zero forcing so that any values of x for which px will have qx and typically qx will under-estimate the support of px and will tend to seek the mode with the largest mass. conversely for the divergence is zero-avoiding so that values of x for which px will have qx and typically qx will stretch to cover all of px and will over-estimate the support of px. when we obtain a symmetric divergence that is linearly related to the hellinger distance given by dx. the square root of the hellinger distance is a valid distance metric. example the univariate gaussian we now illustrate the factorized variational approximation using a gaussian distribution over a single variable x our goal is to infer the posterior distribution for the mean and precision given a data set d xn of observed values of x which are assumed to be drawn independently from the gaussian. the likelihood function is given by pd p exp we now introduce conjugate prior distributions for and given by where gam is the gamma distribution defined by together these distributions constitute a gaussian-gamma conjugate prior distribution. p gam for this simple problem the posterior distribution can be found exactly and again takes the form of a gaussian-gamma distribution. however for tutorial purposes we will consider a factorized variational approximation to the posterior distribution given by q q exercise section exercise variational inference note that the true posterior distribution does not factorize in this way. the optimum factors q and q can be obtained from the general result as follows. for q we have ln e pd ln p const completing the square over we see that q is a gaussian e const. n n with exercise mean and precision given by n n x n n ne note that for n this gives the maximum likelihood result in which n x and the precision is infinite. similarly the optimal solution for the factor q is given by e pd ln p ln p const ln ln n ln e const and hence q is a gamma distribution gam bn with parameters an n e bn exercise section again this exhibits the expected behaviour when n it should be emphasized that we did not assume these specific functional forms for the optimal distributions q and q they arose naturally from the structure of the likelihood function and the corresponding conjugate priors. thus we have expressions for the optimal distributions q and q each of which depends on moments evaluated with respect to the other distribution. one approach to finding a solution is therefore to make an initial guess for say the moment e and use this to re-compute the distribution q given this revised distribution we can then extract the required moments e and e and use these to recompute the distribution q and so on. since the space of hidden variables for this example is only two dimensional we can illustrate the variational approximation to the posterior distribution by plotting contours of both the true posterior and the factorized approximation as illustrated in figure approximate inference figure illustration of variational inference for the mean and precision of a univariate gaussian distribution. contours of the true posterior distribution p are shown in green. contours of the initial factorized approximation q are shown in blue. after re-estimating the factor q after re-estimating the factor q contours of the optimal factorized approximation to which the iterative scheme converges are shown in red. appendix b in general we will need to use an iterative approach such as this in order to solve for the optimal factorized posterior distribution. for the very simple example we are considering here however we can find an explicit solution by solving the simultaneous equations for the optimal factors q and q before doing this we can simplify these expressions by considering broad noninformative priors in which although these parameter settings correspond to improper priors we see that the posterior distribution is still well defined. using the standard result e an for the mean of a gamma distribution together with and we have e e n e then using and we obtain the first and second order moments of variational inference q in the form e x e n e exercise we can now substitute these moments into and then solve for e to give e n n section we recognize the right-hand side as the familiar unbiased estimator for the variance of a univariate gaussian distribution and so we see that the use of a bayesian approach has avoided the bias of the maximum likelihood solution. model comparison as well as performing inference over the hidden variables z we may also wish to compare a set of candidate models labelled by the index m and having prior probabilities pm. our goal is then to approximate the posterior probabilities pmx where x is the observed data. this is a slightly more complex situation than that considered so far because different models may have different structure and indeed different dimensionality for the hidden variables z. we cannot therefore simply consider a factorized approximation qzqm but must instead recognize that the posterior over z must be conditioned on m and so we must consider qz m qzmqm. we can readily verify the following decomposition based on this variational distribution pz mx qzmqm ln px lm qzmqm ln where the lm is a lower bound on ln px and is given by lm qzmqm ln pz x m qzmqm m z m z here we are assuming discrete z but the same analysis applies to continuous latent variables provided the summations are replaced with integrations. we can maximize lm with respect to the distribution qm using a lagrange multiplier with the result qm pm explm. however if we maximize lm with respect to the qzm we find that the solutions for different m are coupled as we expect because they are conditioned on m. we proceed instead by first optimizing each of the qzm individually by optimization exercise exercise approximate inference of and then subsequently determining the qm using after normalization the resulting values for qm can be used for model selection or model averaging in the usual way. illustration variational mixture of gaussians we now return to our discussion of the gaussian mixture model and apply the variational inference machinery developed in the previous section. this will provide a good illustration of the application of variational methods and will also demonstrate how a bayesian treatment elegantly resolves many of the difficulties associated with the maximum likelihood approach the reader is encouraged to work through this example in detail as it provides many insights into the practical application of variational methods. many bayesian models corresponding to much more sophisticated distributions can be solved by straightforward extensions and generalizations of this analysis. our starting point is the likelihood function for the gaussian mixture model illustrated by the graphical model in figure for each observation xn we have a corresponding latent variable zn comprising a binary vector with elements znk for k k. as before we denote the observed data set by x xn and similarly we denote the latent variables by z zn. from we can write down the conditional distribution of z given the mixing coefficients in the form pz znk k similarly from we can write down the conditional distribution of the observed data vectors given the latent variables and the component parameters pxz xn k k where k and k. note that we are working in terms of precision matrices rather than covariance matrices as this somewhat simplifies the mathematics. next we introduce priors over the parameters and the analysis is considerably simplified if we use conjugate prior distributions. we therefore choose a dirichlet distribution over the mixing coefficients p dir c k where by symmetry we have chosen the same parameter for each of the components and c is the normalization constant for the dirichlet distribution defined section illustration variational mixture of gaussians figure directed acyclic graph representing the bayesian mixture of gaussians model in which the box denotes a set of n i.i.d. observations. here denotes k and denotes k. zn xn n section by as we have seen the parameter can be interpreted as the effective prior number of observations associated with each component of the mixture. if the value of is small then the posterior distribution will be influenced primarily by the data rather than by the prior. similarly we introduce an independent gaussian-wishart prior governing the mean and precision of each gaussian component given by p p w k section because this represents the conjugate prior distribution when both the mean and precision are unknown. typically we would choose by symmetry. the resulting model can be represented as a directed graph as shown in figure note that there is a link from to since the variance of the distribution over in is a function of this example provides a nice illustration of the distinction between latent variables and parameters. variables such as zn that appear inside the plate are regarded as latent variables because the number of such variables grows with the size of the data set. by contrast variables such as that are outside the plate are fixed in number independently of the size of the data set and so are regarded as parameters. from the perspective of graphical models however there is really no fundamental difference between them. variational distribution in order to formulate a variational treatment of this model we next write down the joint distribution of all of the random variables which is given by px z pxz in which the various factors are defined above. the reader should take a moment to verify that this decomposition does indeed correspond to the probabilistic graphical model shown in figure note that only the variables x xn are observed. approximate inference we now consider a variational distribution which factorizes between the latent variables and the parameters so that qz qzq it is remarkable that this is the only assumption that we need to make in order to obtain a tractable practical solution to our bayesian mixture model. in particular the functional form of the factors qz and q will be determined automatically by optimization of the variational distribution. note that we are omitting the subscripts on the q distributions much as we do with the p distributions in and are relying on the arguments to distinguish the different distributions. the corresponding sequential update equations for these factors can be easily derived by making use of the general result let us consider the derivation of the update equation for the factor qz. the log of the optimized factor is given by ln e px z const. we now make use of the decomposition note that we are only interested in the functional dependence of the right-hand side on the variable z. thus any terms that do not depend on z can be absorbed into the additive normalization constant giving ln e pz e pxz const. substituting for the two conditional distributions on the right-hand side and again absorbing any terms that are independent of z into the additive constant we have ln znk ln nk const where we have defined ln nk eln k where d is the dimensionality of the data variable x. taking the exponential of both sides of we obtain e k k e k d kt kxn k znk nk rznk nk exercise requiring that this distribution be normalized and noting that for each value of n the quantities znk are binary and sum to over all values of k we obtain where illustration variational mixture of gaussians rnk nk nj we see that the optimal solution for the factor qz takes the same functional form as the prior pz note that because nk is given by the exponential of a real quantity the quantities rnk will be nonnegative and will sum to one as required. for the discrete distribution we have the standard result eznk rnk from which we see that the quantities rnk are playing the role of responsibilities. note that the optimal solution for depends on moments evaluated with respect to the distributions of other variables and so again the variational update equations are coupled and must be solved iteratively. at this point we shall find it convenient to define three statistics of the observed data set evaluated with respect to the responsibilities given by nk nk nk xk sk rnkxn rnkxn xkxn xkt. rnk note that these are analogous to quantities evaluated in the maximum likelihood em algorithm for the gaussian mixture model. now let us consider the factor q in the variational posterior distribu tion. again using the general result we have ln ln p ln p k k ez pz eznk xn k k const. we observe that the right-hand side of this expression decomposes into a sum of terms involving only together with terms only involving and which implies that the variational posterior q factorizes to give q furthermore the terms involving and themselves comprise a sum over k of terms involving k and k leading to the further factorization q q q k k. approximate inference identifying the terms on the right-hand side of that depend on we have ln ln k rnk ln k const where we have used taking the exponential of both sides we recognize as a dirichlet distribution dir exercise exercise where has components k given by k nk. finally the variational posterior distribution k k does not factorize into the product of the marginals but we can always use the product rule to write it in the form k k k k. the two factors can be found by inspecting and reading off those terms that involve k and k. the result as expected is a gaussian-wishart distribution and is given by kmk k k k k w kwk k where we have defined k nk mk k nkxk w w k k nk. nksk nk these update equations are analogous to the m-step equations of the em algorithm for the maximum likelihood solution of the mixture of gaussians. we see that the computations that must be performed in order to update the variational posterior distribution over the model parameters involve evaluation of the same sums over the data set as arose in the maximum likelihood treatment. in order to perform this variational m step we need the expectations eznk rnk representing the responsibilities. these are obtained by normalizing the nk that are given by we see that this expression involves expectations with respect to the variational distributions of the parameters and these are easily evaluated to give kt kxn k e k k d k e k k e k k k kxn mktwkxn mk k i d ln lnwk appendix b illustration variational mixture of gaussians where we have introduced definitions k k and is the digamma function defined by if we substitute and into and make use of the standard properties of the wishart and dirichlet distributions. we obtain the following result for the responsibilities k k. the results and follow from rnk k k exp d k k mktwkxn mk notice the similarity to the corresponding result for the responsibilities in maximum likelihood em which from can be written in the form kt kxn k rnk k exp where we have used the precision in place of the covariance to highlight the similarity to thus the optimization of the variational posterior distribution involves cycling between two stages analogous to the e and m steps of the maximum likelihood em algorithm. in the variational equivalent of the e step we use the current distributions over the model parameters to evaluate the moments in and and hence evaluate eznk rnk. then in the subsequent variational equivalent of the m step we keep these responsibilities fixed and use them to re-compute the variational distribution over the parameters using and in each case we see that the variational posterior distribution has the same functional form as the corresponding factor in the joint distribution this is a general result and is a consequence of the choice of conjugate distributions. figure shows the results of applying this approach to the rescaled old faithful data set for a gaussian mixture model having k components. we see that after convergence there are only two components for which the expected values of the mixing coefficients are numerically distinguishable from their prior values. this effect can be understood qualitatively in terms of the automatic trade-off in a bayesian model between fitting the data and the complexity of the model in which the complexity penalty arises from components whose parameters are pushed away from their prior values. components that take essentially no responsibility for explaining the data points have rnk and hence nk from we see that k and from we see that the other parameters revert to their prior values. in principle such components are fitted slightly to the data points but for broad priors this effect is too small to be seen numerically. for the variational gaussian mixture model the expected values of the mixing coefficients in the posterior distribution are given by section section exercise consider a component for which nk and k if the prior is broad so that then e k and the component plays no role in the model whereas if e k k nk k n approximate inference figure variational bayesian mixture of k gaussians applied to the old faithful data set in which the ellipses denote the one standard-deviation density contours for each of the components and the density of red ink inside each ellipse corresponds to the mean value of the mixing coefficient for each component. the number in the top left of each diagram shows the number of iterations of variational inference. components whose expected mixing coefficient are numerically indistinguishable from zero are not plotted. the prior tightly constrains the mixing coefficients so that then e k in figure the prior over the mixing coefficients is a dirichlet of the form recall from figure that for the prior favours solutions in which some of the mixing coefficients are zero. figure was obtained using and resulted in two components having nonzero mixing coefficients. if instead we choose we obtain three components with nonzero mixing coefficients and for all six components have nonzero mixing coefficients. as we have seen there is a close similarity between the variational solution for the bayesian mixture of gaussians and the em algorithm for maximum likelihood. in fact if we consider the limit n then the bayesian treatment converges to the maximum likelihood em algorithm. for anything other than very small data sets the dominant computational cost of the variational algorithm for gaussian mixtures arises from the evaluation of the responsibilities together with the evaluation and inversion of the weighted data covariance matrices. these computations mirror precisely those that arise in the maximum likelihood em algorithm and so there is little computational overhead in using this bayesian approach as compared to the traditional maximum likelihood one. there are however some substantial advantages. first of all the singularities that arise in maximum likelihood when a gaussian component collapses onto a specific data point are absent in the bayesian treatment. illustration variational mixture of gaussians indeed these singularities are removed if we simply introduce a prior and then use a map estimate instead of maximum likelihood. furthermore there is no over-fitting if we choose a large number k of components in the mixture as we saw in figure finally the variational treatment opens up the possibility of determining the optimal number of components in the mixture without resorting to techniques such as cross validation. section variational lower bound we can also straightforwardly evaluate the lower bound for this model. in practice it is useful to be able to monitor the bound during the re-estimation in order to test for convergence. it can also provide a valuable check on both the mathematical expressions for the solutions and their software implementation because at each step of the iterative re-estimation procedure the value of this bound should not decrease. we can take this a stage further to provide a deeper test of the correctness of both the mathematical derivation of the update equations and of their software implementation by using finite differences to check that each update does indeed give a maximum of the bound en and bishop for the variational mixture of gaussians the lower bound is given by l px z qz ln qz eln px z eln qz eln pxz eln pz eln p eln p d d d z eln qz eln q eln q where to keep the notation uncluttered we have omitted the superscript on the q distributions along with the subscripts on the expectation operators because each expectation is taken with respect to all of the random variables in its argument. the various terms in the bound are easily evaluated to give the following results exercise k d nk kxk mktwkxk mk d k ktrskwk rnk k eln pxz eln pz eln p ln c k approximate inference eln p d ln k d k k ln ktrw wk kmk d k k k ln c k d rnk ln rnk ln k eln qz eln q eln q h k d where d is the dimensionality of x hq k is the entropy of the wishart distribution given by and the coefficients c and bw are defined by and respectively. note that the terms involving expectations of the logs of the q distributions simply represent the negative entropies of those distributions. some simplifications and combination of terms can be performed when these expressions are summed to give the lower bound. however we have kept the expressions separate for ease of understanding. finally it is worth noting that the lower bound provides an alternative approach for deriving the variational re-estimation equations obtained in section to do this we use the fact that since the model has conjugate priors the functional form of the factors in the variational posterior distribution is known namely discrete for z dirichlet for and gaussian-wishart for k k. by taking general parametric forms for these distributions we can derive the form of the lower bound as a function of the parameters of the distributions. maximizing the bound with respect to these parameters then gives the required re-estimation equations. exercise predictive density in applications of the bayesian mixture of gaussians model we will often be interested in the predictive density for a new of the observed variable. associated with this observation will be a corresponding latent and the pre d d d dictive density is then given by bz illustration variational mixture of gaussians where p is the true posterior distribution of the parameters. using and we can first perform the summation to give k p d d d k because the remaining integrations are intractable we approximate the predictive density by replacing the true posterior distribution p with its variational approximation q to give k k q k k d d k d k exercise where we have made use of the factorization and in each term we have implicitly integrated out all variables j j for j k the remaining integrations can now be evaluated analytically giving a mixture of student s t-distributions lk k d in which the kth component has mean mk and the precision is given by lk k d k k wk exercise in which k is given by when the size n of the data set is large the predictive distribution reduces to a mixture of gaussians. section exercise determining the number of components we have seen that the variational lower bound can be used to determine a posterior distribution over the number k of components in the mixture model. there is however one subtlety that needs to be addressed. for any given setting of the parameters in a gaussian mixture model for specific degenerate settings there will exist other parameter settings for which the density over the observed variables will be identical. these parameter values differ only through a re-labelling of the components. for instance consider a mixture of two gaussians and a single observed variable x in which the parameters have the values a b c d e f. then the parameter values b a d c f e in which the two components have been exchanged will by symmetry give rise to the same value of px. if we have a mixture model comprising k components then each parameter setting will be a member of a family of k! equivalent settings. in the context of maximum likelihood this redundancy is irrelevant because the parameter optimization algorithm example em will depending on the initialization of the parameters find one specific solution and the other equivalent solutions play no role. in a bayesian setting however we marginalize over all possible approximate inference figure plot of the variational lower bound l versus the number k of components in the gaussian mixture model for the old faithful data showing a distinct peak at k components. for each value of k the model is trained from different random starts and the results shown as symbols plotted with small random horizontal perturbations so that they can be distinguished. note that some solutions find suboptimal local maxima but that this happens infrequently. pdk k parameter values. we have seen in figure that if the true posterior distribution is multimodal variational inference based on the minimization of will tend to approximate the distribution in the neighbourhood of one of the modes and ignore the others. again because equivalent modes have equivalent predictive densities this is of no concern provided we are considering a model having a specific number k of components. if however we wish to compare different values of k then we need to take account of this multimodality. a simple approximate solution is to add a term ln k! onto the lower bound when used for model comparison and averaging. figure shows a plot of the lower bound including the multimodality factor versus the number k of components for the old faithful data set. it is worth emphasizing once again that maximum likelihood would lead to values of the likelihood function that increase monotonically with k the singular solutions have been avoided and discounting the effects of local maxima and so cannot be used to determine an appropriate model complexity. by contrast bayesian inference automatically makes the trade-off between model complexity and fitting the data. this approach to the determination of k requires that a range of models having different k values be trained and compared. an alternative approach to determining a suitable value for k is to treat the mixing coefficients as parameters and make point estimates of their values by maximizing the lower bound and bishop with respect to instead of maintaining a probability distribution over them as in the fully bayesian approach. this leads to the re-estimation equation k n rnk and this maximization is interleaved with the variational updates for the q distribution over the remaining parameters. components that provide insufficient contribution exercise section exercise section illustration variational mixture of gaussians to explaining the data will have their mixing coefficients driven to zero during the optimization and so they are effectively removed from the model through automatic relevance determination. this allows us to make a single training run in which we start with a relatively large initial value of k and allow surplus components to be pruned out of the model. the origins of the sparsity when optimizing with respect to hyperparameters is discussed in detail in the context of the relevance vector machine. induced factorizations in deriving these variational update equations for the gaussian mixture model we assumed a particular factorization of the variational posterior distribution given by however the optimal solutions for the various factors exhibit additional factorizations. in particular the solution for is given by the product of an independent distribution k k over each of the components k of the mixture whereas the variational posterior distribution over the latent variables given by factorizes into an independent distribution for each observation n that it does not further factorize with respect to k because for each value of n the znk are constrained to sum to one over k. these additional factorizations are a consequence of the interaction between the assumed factorization and the conditional independence properties of the true distribution as characterized by the directed graph in figure we shall refer to these additional factorizations as induced factorizations because they arise from an interaction between the factorization assumed in the variational posterior distribution and the conditional independence properties of the true joint distribution. in a numerical implementation of the variational approach it is important to take account of such additional factorizations. for instance it would be very inefficient to maintain a full precision matrix for the gaussian distribution over a set of variables if the optimal form for that distribution always had a diagonal precision matrix to a factorization with respect to the individual variables described by that gaussian. such induced factorizations can easily be detected using a simple graphical test based on d-separation as follows. we partition the latent variables into three disjoint groups a b c and then let us suppose that we are assuming a factorization between c and the remaining latent variables so that qa b c qa bqc. using the general result together with the product rule for probabilities we see that the optimal solution for qa b is given by ln b ecln px a b c const ecln pa bx c const. we now ask whether this resulting solution will factorize between a and b in other words whether b this will happen if and only if ln pa bx c ln pax c ln pbx c that is if the conditional independence relation a b x c approximate inference is satisfied. we can test to see if this relation does hold for any choice of a and b by making use of the d-separation criterion. to illustrate this consider again the bayesian mixture of gaussians represented by the directed graph in figure in which we are assuming a variational factorization given by we can see immediately that the variational posterior distribution over the parameters must factorize between and the remaining parameters and because all paths connecting to either or must pass through one of the nodes zn all of which are in the conditioning set for our conditional independence test and all of which are head-to-tail with respect to such paths. variational linear regression exercise as a second illustration of variational inference we return to the bayesian linear regression model of section in the evidence framework we approximated the integration over and by making point estimates obtained by maximizing the log marginal likelihood. a fully bayesian approach would integrate over the hyperparameters as well as over the parameters. although exact integration is intractable we can use variational methods to find a tractable approximation. in order to simplify the discussion we shall suppose that the noise precision parameter is known and is fixed to its true value although the framework is easily extended to include the distribution over for the linear regression model the variational treatment will turn out to be equivalent to the evidence framework. nevertheless it provides a good exercise in the use of variational methods and will also lay the foundation for variational treatment of bayesian logistic regression in section recall that the likelihood function for w and the prior over w are given by n n ptw pw n where n we now introduce a prior distribution over from our discussion in section we know that the conjugate prior for the precision of a gaussian is given by a gamma distribution and so we choose where gam is defined by thus the joint distribution of all the variables is given by p gam pt w ptwpw this can be represented as a directed graphical model as shown in figure variational distribution our first goal is to find an approximation to the posterior distribution pw to do this we employ the variational framework of section with a variational variational linear regression figure probabilistic graphical model representing the joint disregression the bayesian linear for tribution model. w n tn n posterior distribution given by the factorized expression qw qwq we can find re-estimation equations for the factors in this distribution by making use of the general result recall that for each factor we take the log of the joint distribution over all variables and then average with respect to those variables not in that factor. consider first the distribution over keeping only terms that have a functional dependence on we have ln ln p ew pw const ln m ln ewtw const. we recognize this as the log of a gamma distribution and so identifying the coefficients of and ln we obtain gam bn where an m ewtw. bn similarly we can find the variational re-estimation equation for the posterior distribution over w. again using the general result and keeping only those terms that have a functional dependence on w we have ln ln ptw e pw const n e const wt e t w wt tt const. because this is a quadratic form the distribution is gaussian and so we can complete the square in the usual way to identify the mean and covariance giving n sn approximate inference where mn sn tt sn e t note the close similarity to the posterior distribution obtained when was treated as a fixed parameter. the difference is that here is replaced by its expectation e under the variational distribution. indeed we have chosen to use the same notation for the covariance matrix sn in both cases. using the standard results and we can obtain the required moments as follows e an ewwt mn mt n sn the evaluation of the variational posterior distribution begins by initializing the parameters of one of the distributions qw or q and then alternately re-estimates these factors in turn until a suitable convergence criterion is satisfied specified in terms of the lower bound to be discussed shortly. it is instructive to relate the variational solution to that found using the evidence framework in section to do this consider the case corresponding to the limit of an infinitely broad prior over the mean of the variational posterior distribution q is then given by e an bn m n mn trsn mt comparison with shows that in the case of this particularly simple model the variational approach gives precisely the same expression as that obtained by maximizing the evidence function using em except that the point estimate for is replaced by its expected value. because the distribution qw depends on q only through the expectation e we see that the two approaches will give identical results for the case of an infinitely broad prior. predictive distribution the predictive distribution over t given a new input x is easily evaluated for this model using the gaussian variational posterior for the parameters ptx t ptx wpwt dw ptx wqw dw n n n sn dw variational linear regression where we have evaluated the integral by making use of the result for the linear-gaussian model. here the input-dependent variance is given by note that this takes the same form as the result obtained with fixed except that now the expected value e appears in the definition of sn lower bound another quantity of importance is the lower bound l defined by lq eln pw t eln qw e qww eln q ewln ptw ew pw e p exercise evaluation of the various terms is straightforward making use of results obtained in previous chapters and gives eln ptww n ln tr eln pw m an ttt mt n tt t mt m n sn ln bn eln p ln ln bn mt n mn trsn ln an bn lnsn m eln qww eln q ln ln bn an figure shows a plot of the lower bound lq versus the degree of a polynomial model for a synthetic data set generated from a degree three polynomial. here the prior parameters have been set to corresponding to the noninformative prior p which is uniform over ln as discussed in section as we saw in section the quantity l represents lower bound on the log marginal likelihood ptm for the model. if we assign equal prior probabilities pm to the different values of m then we can interpret l as an approximation to the posterior model probability pmt. thus the variational framework assigns the highest probability to the model with m this should be contrasted with the maximum likelihood result which assigns ever smaller residual error to models of increasing complexity until the residual error is driven to zero causing maximum likelihood to favour severely over-fitted models. approximate inference figure plot of the lower bound l versus the order m of the polynomial for a polynomial model in which a set of data points is generated from a polynomial with m sampled over the interval with additive gaussian noise of variance the value of the bound gives the log probability of the model and we see that the value of the bound peaks at m corresponding to the true model from which the data set was generated. exponential family distributions in chapter we discussed the important role played by the exponential family of distributions and their conjugate priors. for many of the models discussed in this book the complete-data likelihood is drawn from the exponential family. however in general this will not be the case for the marginal likelihood function for the observed data. for example in a mixture of gaussians the joint distribution of observations xn and corresponding hidden variables zn is a member of the exponential family whereas the marginal distribution of xn is a mixture of gaussians and hence is not. up to now we have grouped the variables in the model into observed variables and hidden variables. we now make a further distinction between latent variables denoted z and parameters denoted where parameters are intensive in number independent of the size of the data set whereas latent variables are extensive in number with the size of the data set. for example in a gaussian mixture model the indicator variables zkn specify which component k is responsible for generating data point xn represent the latent variables whereas the means k precisions k and mixing proportions k represent the parameters. consider the case of independent identically distributed data. we denote the data values by x where n n with corresponding latent variables z now suppose that the joint distribution of observed and latent variables is a member of the exponential family parameterized by natural parameters so that px z hxn zng exp tuxn zn we shall also use a conjugate prior for which can be written as p f exp o t recall that the conjugate prior distribution can be interpreted as a prior number of observations all having the value for the u vector. now consider a variational exponential family distributions distribution that factorizes between the latent variables and the parameters so that qz qzq using the general result we can solve for the two factors as follows ln e px z const ln hxn zn e tuxn zn const. thus we see that this decomposes into a sum of independent terms one for each value of n and hence the solution for will factorize over n so that n this is an example of an induced factorization. taking the exponential section of both sides we have hxn zng exp e tuxn zn where the normalization coefficient has been re-instated by comparison with the standard form for the exponential family. similarly for the variational distribution over the parameters we have ln ln p ezln px z const ln g t ln g t eznuxn zn const. again taking the exponential of both sides and re-instating the normalization coefficient by inspection we have f n n n exp t n where we have defined n n n eznuxn zn. note that the solutions for and are coupled and so we solve them iteratively in a two-stage procedure. in the variational e step we evaluate the expected sufficient statistics euxn zn using the current posterior distribution qzn over the latent variables and use this to compute a revised posterior distribution q over the parameters. then in the subsequent variational m step we use this revised parameter posterior distribution to find the expected natural parameters e t which gives rise to a revised variational distribution over the latent variables. variational message passing we have illustrated the application of variational methods by considering a specific model the bayesian mixture of gaussians in some detail. this model can be approximate inference described by the directed graph shown in figure here we consider more generally the use of variational methods for models described by directed graphs and derive a number of widely applicable results. the joint distribution corresponding to a directed graph can be written using the decomposition px pxipai where xi denotes the variables associated with node i and pai denotes the parent set corresponding to node i. note that xi may be a latent variable or it may belong to the set of observed variables. now consider a variational approximation in which the distribution qx is assumed to factorize with respect to the xi so that qx qixi. i i ln pxipai note that for observed nodes there is no factor qxi in the variational distribution. we now substitute into our general result to give ln j const. i any terms on the right-hand side that do not depend on xj can be absorbed into in fact the only terms that do depend on xj are the conthe additive constant. ditional distribution for xj given by pxjpaj together with any other conditional distributions that have xj in the conditioning set. by definition these conditional distributions correspond to the children of node j and they therefore also depend on the co-parents of the child nodes i.e. the other parents of the child nodes besides node xj itself. we see that the set of all nodes on which depends corresponds to the markov blanket of node xj as illustrated in figure thus the update of the factors in the variational posterior distribution represents a local calculation on the graph. this makes possible the construction of general purpose software for variational inference in which the form of the model does not need to be specified in advance et al. if we now specialize to the case of a model in which all of the conditional distributions have a conjugate-exponential structure then the variational update procedure can be cast in terms of a local message passing algorithm and bishop in particular the distribution associated with a particular node can be updated once that node has received messages from all of its parents and all of its children. this in turn requires that the children have already received messages from their coparents. the evaluation of the lower bound can also be simplified because many of the required quantities are already evaluated as part of the message passing scheme. this distributed message passing formulation has good scaling properties and is well suited to large networks. local variational methods local variational methods the variational framework discussed in sections and can be considered a global method in the sense that it directly seeks an approximation to the full posterior distribution over all random variables. an alternative local approach involves finding bounds on functions over individual variables or groups of variables within a model. for instance we might seek a bound on a conditional distribution pyx which is itself just one factor in a much larger probabilistic model specified by a directed graph. the purpose of introducing the bound of course is to simplify the resulting distribution. this local approximation can be applied to multiple variables in turn until a tractable approximation is obtained and in section we shall give a practical example of this approach in the context of logistic regression. here we focus on developing the bounds themselves. we have already seen in our discussion of the kullback-leibler divergence that the convexity of the logarithm function played a key role in developing the lower bound in the global variational approach. we have defined a convex function as one for which every chord lies above the function. convexity also plays a central role in the local variational framework. note that our discussion will apply equally to concave functions with min and max interchanged and with lower bounds replaced by upper bounds. let us begin by considering a simple example namely the function fx exp x which is a convex function of x and which is shown in the left-hand plot of figure our goal is to approximate fx by a simpler function in particular a linear function of x. from figure we see that this linear function will be a lower bound on fx if it corresponds to a tangent. we can obtain the tangent line yx at a specific value of x say x by making a first order taylor expansion section so that yx fx with equality when x for our example function fx yx f f figure in the left-hand figure the red curve shows the function exp x and the blue line shows the tangent at x defined by with this line has slope exp note that any other tangent line for example the ones shown in green will have a smaller value of y at x the right-hand figure shows the corresponding plot of the function g where g is given by versus for in which the maximum corresponds to exp x g approximate inference y fx y g fx x x x x g figure in the left-hand plot the red curve shows a convex function f and the blue line represents the linear function x which is a lower bound on f because f x for all x. for the given value of slope the contact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy by the green dashed lines given by f x. this defines the dual function g which corresponds to the of the intercept of the tangent line having slope exp x we therefore obtain the tangent line in the form yx exp exp which is a linear function parameterized by for consistency with subsequent discussion let us define exp so that yx x ln different values of correspond to different tangent lines and because all such lines are lower bounds on the function we have fx yx thus we can write the function in the form fx max x ln we have succeeded in approximating the convex function fx by a simpler linear function yx the price we have paid is that we have introduced a variational parameter and to obtain the tightest bound we must optimize with respect to we can formulate this approach more generally using the framework of convex duality jordan et al. consider the illustration of a convex function fx shown in the left-hand plot in figure in this example the function x is a lower bound on fx but it is not the best lower bound that can be achieved by a linear function having slope because the tightest bound is given by the tangent line. let us write the equation of the tangent line having slope as x g where the intercept g clearly depends on the slope of the tangent. to determine the intercept we note that the line must be moved vertically by an amount equal to the smallest vertical distance between the line and the function as shown in figure thus g min x max x x fx x local variational methods now instead of fixing and varying x we can consider a particular x and then adjust until the tangent plane is tangent at that particular x. because the y value of the tangent line at a particular x is maximized when that value coincides with its contact point we have x g fx max we see that the functions fx and g play a dual role and are related through and let us apply these duality relations to our simple example fx exp x. from we see that the maximizing value of x is given by ln and back-substituting we obtain the conjugate function g in the form g ln as obtained previously. the function g is shown for in the right-hand plot in figure as a check we can substitute into which gives the maximizing value of exp x and back-substituting then recovers the original function fx exp x. for concave functions we can follow a similar argument to obtain upper bounds in which max is replaced with min so that fx min g min x x g x fx if the function of interest is not convex concave then we cannot directly apply the method above to obtain a bound. however we can first seek invertible transformations either of the function or of its argument which change it into a convex form. we then calculate the conjugate function and then transform back to the original variables. an important example which arises frequently in pattern recognition is the logistic sigmoid function defined by e x as it stands this function is neither convex nor concave. however if we take the logarithm we obtain a function which is concave as is easily verified by finding the second derivative. from the corresponding conjugate function then takes the form g min x x fx ln which we recognize as the binary entropy function for a variable whose probability of having the value is using we then obtain an upper bound on the log sigmoid ln x g exercise appendix b approximate inference figure the left-hand plot shows the logistic sigmoid function defined by in red together with two examples of the exponential upper bound shown in blue. the right-hand plot shows the logistic sigmoid again in red together with the gaussian lower bound shown in blue. here the parameter and the bound is exact at x and x denoted by the dashed green lines. and taking the exponential we obtain an upper bound on the logistic sigmoid itself of the form exp x g exercise which is plotted for two values of on the left-hand plot in figure we can also obtain a lower bound on the sigmoid having the functional form of a gaussian. to do this we follow jaakkola and jordan and make transformations both of the input variable and of the function itself. first we take the log of the logistic function and then decompose it so that x ln ln e e we now note that the function fx e is a convex function of the variable as can again be verified by finding the second derivative. this leads to a lower bound on fx which is a linear function of whose conjugate function is given by e e g max f the stationarity condition leads to dx d dx x fx tanh if we denote this value of x corresponding to the contact point of the tangent line for this particular value of by then we have tanh local variational methods instead of thinking of as the variational parameter we can let play this role as this leads to simpler expressions for the conjugate function which is then given by g f lne e hence the bound on fx can be written as fx g lne e the bound on the sigmoid then becomes exp section section where is defined by this bound is illustrated in the right-hand plot of figure we see that the bound has the form of the exponential of a quadratic function of x which will prove useful when we seek gaussian representations of posterior distributions defined through logistic sigmoid functions. the logistic sigmoid arises frequently in probabilistic models over binary variables because it is the function that transforms a log odds ratio into a posterior probability. the corresponding transformation for a multiclass distribution is given by the softmax function. unfortunately the lower bound derived here for the logistic sigmoid does not directly extend to the softmax. gibbs proposes a method for constructing a gaussian distribution that is conjectured to be a bound no rigorous proof is given which may be used to apply local variational methods to multiclass problems. we shall see an example of the use of local variational bounds in sections for the moment however it is instructive to consider in general terms how these bounds can be used. suppose we wish to evaluate an integral of the form i da where is the logistic sigmoid and pa is a gaussian probability density. such integrals arise in bayesian models when for instance we wish to evaluate the predictive distribution in which case pa represents a posterior parameter distribution. because the integral is intractable we employ the variational bound which we write in the form fa where is a variational parameter. the integral now becomes the product of two exponential-quadratic functions and so can be integrated analytically to give a bound on i fa da f i we now have the freedom to choose the variational parameter which we do by finding the value that maximizes the function f the resulting value f represents the tightest bound within this family of bounds and can be used as an approximation to i. this optimized bound however will in general not be exact. approximate inference although the bound fa on the logistic sigmoid can be optimized exactly the required choice for depends on the value of a so that the bound is exact for one value of a only. because the quantity f is obtained by integrating over all values of a the value of represents a compromise weighted by the distribution pa. variational logistic regression we now illustrate the use of local variational methods by returning to the bayesian logistic regression model studied in section there we focussed on the use of the laplace approximation while here we consider a variational treatment based on the approach of jaakkola and jordan like the laplace method this also leads to a gaussian approximation to the posterior distribution. however the greater flexibility of the variational approximation leads to improved accuracy compared to the laplace method. furthermore the laplace method the variational approach is optimizing a well defined objective function given by a rigourous bound on the model evidence. logistic regression has also been treated by dybowski and roberts from a bayesian perspective using monte carlo sampling techniques. variational posterior distribution here we shall make use of a variational approximation based on the local bounds introduced in section this allows the likelihood function for logistic regression which is governed by the logistic sigmoid to be approximated by the exponential of a quadratic form. it is therefore again convenient to choose a conjugate gaussian prior of the form for the moment we shall treat the hyperparameters and as fixed constants. in section we shall demonstrate how the variational formalism can be extended to the case where there are unknown hyperparameters whose values are to be inferred from the data. in the variational framework we seek to maximize a lower bound on the marginal likelihood. for the bayesian logistic regression model the marginal likelihood takes the form ptnw pw dw. we first note that the conditional distribution for t can be written as pt t ptwpw dw ptw t e a e a a e a eat a e eat where a wt in order to obtain a lower bound on pt we make use of the variational lower bound on the logistic sigmoid function given by which variational logistic regression we reproduce here for convenience exp where we can therefore write ptw eat a eat exp note that because this bound is applied to each of the terms in the likelihood function separately there is a variational parameter n corresponding to each training set observation n tn. using a wt and multiplying by the prior distribution we obtain the following bound on the joint distribution of t and w pt w ptwpw hw where denotes the set n of variational parameters and nwt n wt ntn n hw n exp evaluation of the exact posterior distribution would require normalization of the lefthand side of this inequality. because this is intractable we work instead with the right-hand side. note that the function on the right-hand side cannot be interpreted as a probability density because it is not normalized. once it is normalized to give a variational posterior distribution qw however it no longer represents a bound. because the logarithm function is monotonically increasing the inequality a b implies ln a ln b. this gives a lower bound on the log of the joint distribution of t and w of the form lnptwpw ln pw ln n wt ntn n nwt n substituting for the prior pw the right-hand side of this inequality becomes as a function of w wt ntn nwt n t nw const. approximate inference where this is a quadratic function of w and so we can obtain the corresponding variational approximation to the posterior distribution by identifying the linear and quadratic terms in w giving a gaussian variational posterior of the form qw n sn mn sn s n s s n n n t n. as with the laplace framework we have again obtained a gaussian approximation to the posterior distribution. however the additional flexibility provided by the variational parameters n leads to improved accuracy in the approximation and jordan here we have considered a batch learning context in which all of the training data is available at once. however bayesian methods are intrinsically well suited to sequential learning in which the data points are processed one at a time and then discarded. the formulation of this variational approach for the sequential case is straightforward. note that the bound given by applies only to the two-class problem and so this approach does not directly generalize to classification problems with k classes. an alternative bound for the multiclass case has been explored by gibbs exercise optimizing the variational parameters we now have a normalized gaussian approximation to the posterior distribution which we shall use shortly to evaluate the predictive distribution for new data points. first however we need to determine the variational parameters n by maximizing the lower bound on the marginal likelihood. to do this we substitute the inequality back into the marginal likeli hood to give ln pt ln ptwpw dw ln hw dw l as with the optimization of the hyperparameter in the linear regression model of section there are two approaches to determining the n. in the first approach we recognize that the function l is defined by an integration over w and so we can view w as a latent variable and invoke the em algorithm. in the second approach we integrate over w analytically and then perform a direct maximization over let us begin by considering the em approach. n which we denote collectively by old. the em algorithm starts by choosing some initial values for the parameters in the e step of the em algorithm variational logistic regression we then use these parameter values to find the posterior distribution over w which is given by in the m step we then maximize the expected complete-data log likelihood which is given by q old e hw where the expectation is taken with respect to the posterior distribution qw evaluated using old. noting that pw does not depend on and substituting for hw we obtain q old ln n n t n ewwt n n const where const denotes terms that are independent of we now set the derivative with respect to n equal to zero. a few lines of algebra making use of the definitions of and then gives n t n ewwt n n. is a monotonic function of for and that we can we now note that restrict attention to nonnegative values of without loss of generality due to the and hence we obtain the symmetry of the bound around thus following re-estimation equations new n t n ewwt n t n sn mn mt n n where we have used let us summarize the em algorithm for finding the variational posterior distribution. we first initialize the variational parameters old. in the e step we evaluate the posterior distribution over w given by in which the mean and covariance are defined by and in the m step we then use this variational posterior to compute a new value for given by the e and m steps are repeated until a suitable convergence criterion is satisfied which in practice typically requires only a few iterations. an alternative approach to obtaining re-estimation equations for is to note that in the integral over w in the definition of the lower bound l the integrand has a gaussian-like form and so the integral can be evaluated analytically. having evaluated the integral we can then differentiate with respect to n. it turns out that this gives rise to exactly the same re-estimation equations as does the em approach given by as we have emphasized already in the application of variational methods it is useful to be able to evaluate the lower bound l given by the integration over w can be performed analytically by noting that pw is gaussian and hw is the exponential of a quadratic function of w. thus by completing the square and making use of the standard result for the normalization coefficient of a gaussian distribution we can obtain a closed form solution which takes the form exercise exercise exercise approximate inference figure illustration of the bayesian approach to logistic regression for a simple linearly separable data set. the plot on the left shows the predictive distribution obtained using variational inference. we see that the decision boundary lies roughly mid way between the clusters of data points and that the contours of the predictive distribution splay out away from the data reflecting the greater uncertainty in the classification of such regions. the plot on the right shows the decision boundaries corresponding to five samples of the parameter vector w drawn from the posterior distribution pwt. l ln mt ln n n s mt s n mn n n n this variational framework can also be applied to situations in which the data is arriving sequentially and jordan in this case we maintain a gaussian posterior distribution over w which is initialized using the prior pw. as each data point arrives the posterior is updated by making use of the bound and then normalized to give an updated posterior distribution. the predictive distribution is obtained by marginalizing over the posterior distribution and takes the same form as for the laplace approximation discussed in section figure shows the variational predictive distributions for a synthetic data set. this example provides interesting insights into the concept of large margin which was discussed in section and which has qualitatively similar behaviour to the bayesian solution. inference of hyperparameters so far we have treated the hyperparameter in the prior distribution as a known constant. we now extend the bayesian logistic regression model to allow the value of this parameter to be inferred from the data set. this can be achieved by combining the global and local variational approximations into a single framework so as to maintain a lower bound on the marginal likelihood at each stage. such a combined approach was adopted by bishop and svens en in the context of a bayesian treatment of the hierarchical mixture of experts model. variational logistic regression specifically we consider once again a simple isotropic gaussian prior distribu tion of the form pw n our analysis is readily extended to more general gaussian priors for instance if we wish to associate a different hyperparameter with different subsets of the parameters wj. as usual we consider a conjugate hyperprior over given by a gamma distribution p gam governed by the constants and the marginal likelihood for this model now takes the form pt pw t dw d where the joint distribution is given by pw t ptwpw we are now faced with an analytically intractable integration over w and which we shall tackle by using both the local and global variational approaches in the same model to begin with we introduce a variational distribution qw and then apply the decomposition which in this instance takes the form where the lower bound lq and the kullback-leibler divergence are defined by ln pt lq qw ln pw t qw pw qw dw d qw ln at this point the lower bound lq is still intractable due to the form of the likelihood factor ptw. we therefore apply the local variational bound to each of the logistic sigmoid factors as before. this allows us to use the inequality and place a lower bound on lq which will therefore also be a lower bound on the log marginal likelihood dw d lq ln pt lq qw ln hw qw dw d next we assume that the variational distribution factorizes between parameters and hyperparameters so that qw qwq approximate inference with this factorization we can appeal to the general result to find expressions for the optimal factors. consider first the distribution qw. discarding terms that are independent of w we have ln qw e const ln hw e pw const. we now substitute for ln hw using and for ln pw using giving ln qw e wtw n nwt n t nw const. we see that this is a quadratic function of w and so the solution for qw will be gaussian. completing the square in the usual way we obtain where we have defined n n qw n n n n n e n n t n. similarly the optimal solution for the factor q is obtained from ln q ew pw ln p const. substituting for ln pw using and for ln p using we obtain ln q m ln e wtw ln const. we recognize this as the log of a gamma distribution and so we obtain q gam bn where an m ew bn wtw expectation propagation maximizing the lower omitting terms that are independent of and we also need to optimize the variational parameters n and this is also done by integrating over we have qw ln hw dw const. note that this has precisely the same form as and so we can again appeal to our earlier result which can be obtained by direct optimization of the marginal likelihood function leading to re-estimation equations of the form new n t n n n t n n. appendix b we have obtained re-estimation equations for the three quantities qw q and and so after making suitable initializations we can cycle through these quantities updating each in turn. the required moments are given by e e an bn wtw n t n n expectation propagation we conclude this chapter by discussing an alternative form of deterministic approximate inference known as expectation propagation or ep minka as with the variational bayes methods discussed so far this too is based on the minimization of a kullback-leibler divergence but now of the reverse form which gives the approximation rather different properties. consider for a moment the problem of minimizing with respect to qz when pz is a fixed distribution and qz is a member of the exponential family and so from can be written in the form qz hzg exp tuz as a function of the kullback-leibler divergence then becomes ln g t epzuz const where the constant terms are independent of the natural parameters we can minimize within this family of distributions by setting the gradient with respect to to zero giving however we have already seen in that the negative gradient of ln g is given by the expectation of uz under the distribution qz. equating these two results we obtain ln g epzuz. eqzuz epzuz. approximate inference we see that the optimum solution simply corresponds to matching the expected sufficient statistics. so for instance if qz is a gaussian n then we minimize the kullback-leibler divergence by setting the mean of qz equal to the mean of the distribution pz and the covariance equal to the covariance of pz. this is sometimes called moment matching. an example of this was seen in figure now let us exploit this result to obtain a practical algorithm for approximate inference. for many probabilistic models the joint distribution of data d and hidden variables parameters comprises a product of factors in the form pd fi i this would arise for example in a model for independent identically distributed data in which there is one factor fn pxn for each data point xn along with a factor p corresponding to the prior. more generally it would also apply to any model defined by a directed probabilistic graph in which each factor is a conditional distribution corresponding to one of the nodes or an undirected graph in which each factor is a clique potential. we are interested in evaluating the posterior distribution p for the purpose of making predictions as well as the model evidence pd for the purpose of model comparison. from the posterior is given by p fi i pd i and the model evidence is given by pd fi d here we are considering continuous variables but the following discussion applies equally to discrete variables with integrals replaced by summations. we shall suppose that the marginalization over along with the marginalizations with respect to the posterior distribution required to make predictions are intractable so that some form of approximation is required. expectation propagation is based on an approximation to the posterior distribu tion which is also given by a product of factors q in which each in the approximation corresponds to one of the factors obtain a practical algorithm we need to constrain the in some way fi in the true posterior and the factor is the normalizing constant needed to ensure that the left-hand side of integrates to unity. in order to i and in particular we shall assume that they come from the exponential family. the product of the factors will therefore also be from the exponential family and so can z expectation propagation be described by a finite set of sufficient statistics. for example if each of ideally we would like to determine by minimizing the kullback-leibler is a gaussian then the overall approximation q will also be gaussian. divergence between the true posterior and the approximation given by i z i kl kl pd fi note that this is the reverse form of kl divergence compared with that used in variational inference. in general this minimization will be intractable because the kl divergence involves averaging with respect to the true distribution. as a rough approximation we could instead minimize the kl divergences between the corresponding pairs fi of factors. this represents a much simpler problem to solve and has the advantage that the algorithm is noniterative. however because each factor is individually approximated the product of the factors could well give a poor approximation. this is similar in spirit to the update of factors in the variational bayes framework expectation propagation makes a much better approximation by optimizing each factor in turn in the context of all of the remaining factors. it starts by initializing the and then cycles through the factors refining them one at a time. considered earlier. suppose we wish to refine we first remove this conceptually we will now determine a revised form of the by ensuring that the product factor from the product to give qnew fj is as close as possible to in which we keep fixed all of the for i j. this ensures that the to the clutter problem to achieve this we first remove the from the approximation is most accurate in the regions of high posterior probability as defined by the remaining factors. we shall see an example of this effect when we apply ep current approximation to the posterior by defining the unnormalized distribution from the product of factors i j although note that we could instead find q in practice division is usually easier. this is now combined with the factor fj to give a distribution q q fj zj section approximate inference figure illustration of the expectation propagation approximation using a gaussian distribution for the example considered earlier in figures and the left-hand plot shows the original distribution along with the laplace global variational and ep approximations and the right-hand plot shows the corresponding negative logarithms of the distributions. note that the ep distribution is broader than that variational inference as a consequence of the different form of kl divergence. where zj is the normalization constant given by we now determine a revised by minimizing the kullback-leibler diver zj fj d gence kl fj zj qnew this is easily solved because the approximating distribution qnew is from the exponential family and so we can appeal to the result which tells us that the parameters of qnew are obtained by matching its expected sufficient statistics to the corresponding moments of we shall assume that this is a tractable operation. for example if we choose q to be a gaussian distribution n then and is is set equal to the mean of the distribution fj set to its covariance. more generally it is straightforward to obtain the required expectations for any member of the exponential family provided it can be normalized because the expected statistics can be related to the derivatives of the normalization coefficient as given by the ep approximation is illustrated in figure from we see that the revised factor can be found by taking qnew and dividing out the remaining factors so that k qnew qj where we have used the coefficient k is determined by multiplying both expectation propagation sides of by q and integrating to give k d where we have used the fact that qnew is normalized. the value of k can therefore be found by matching zeroth-order moments d d fj combining this with we then see that k zj and so can be found by evaluating the integral in in practice several passes are made through the set of factors revising each factor in turn. the posterior distribution p is then approximated using and the model evidence pd can be approximated by using with the factors fi replaced by their expectation propagation we are given a joint distribution over observed data d and stochastic variables in the form of a product of factors pd fi and we wish to approximate the posterior distribution p by a distribution of the form i z q we also wish to approximate the model evidence pd. initialize all of the approximating initialize the posterior approximation by setting q i i until convergence choose a to refine. from the posterior by division q q approximate inference evaluate the new posterior by setting the sufficient statistics including evaluation of the of qnew equal to those of q normalization constant zj d q evaluate and store the new factor qnew qj evaluate the approximation to the model evidence zj pd d i a special case of ep known as assumed density filtering or moment matching lauritzen boyen and koller opper and winther is obtained by initializing all of the approximating factors except the first to unity and then making one pass through the factors updating each of them once. assumed density filtering can be appropriate for on-line learning in which data points are arriving in a sequence and we need to learn from each data point and then discard it before considering the next point. however in a batch setting we have the opportunity to re-use the data points many times in order to achieve improved accuracy and it is this idea that is exploited in expectation propagation. furthermore if we apply adf to batch data the results will have an undesirable dependence on the order in which the data points are considered which again ep can overcome. one disadvantage of expectation propagation is that there is no guarantee that the iterations will converge. however for approximations q in the exponential family if the iterations do converge the resulting solution will be a stationary point of a particular energy function although each iteration of ep does not necessarily decrease the value of this energy function. this is in contrast to variational bayes which iteratively maximizes a lower bound on the log marginal likelihood in which each iteration is guaranteed not to decrease the bound. it is possible to optimize the ep cost function directly in which case it is guaranteed to converge although the resulting algorithms can be slower and more complex to implement. another difference between variational bayes and ep arises from the form of kl divergence that is minimized by the two algorithms because the former minimizes whereas the latter minimizes as we saw in figure for distributions p which are multimodal minimizing can lead to poor approximations. in particular if ep is applied to mixtures the results are not sensible because the approximation tries to capture all of the modes of the posterior distribution. conversely in logistic-type models ep often out-performs both local variational methods and the laplace approximation and rasmussen expectation propagation figure illustration of the clutter problem for a data space dimensionality of d training data points denoted by the crosses are drawn from a mixture of two gaussians with components shown in red and green. the goal is to infer the mean of the green gaussian from the observed data. x example the clutter problem following minka we illustrate the ep algorithm using a simple example in which the goal is to infer the mean of a multivariate gaussian distribution over a variable x given a set of observations drawn from that distribution. to make the problem more interesting the observations are embedded in background clutter which itself is also gaussian distributed as illustrated in figure the distribution of observed values x is therefore a mixture of gaussians which we take to be of the form px wn i wn ai where w is the proportion of background clutter and is assumed to be known. the prior over is taken to be gaussian p n bi and minka chooses the parameter values a b and w the joint distribution of n observations d xn and is given by pd p pxn and so the posterior distribution comprises a mixture of gaussians. thus the computational cost of solving this problem exactly would grow exponentially with the size of the data set and so an exact solution is intractable for moderately large n. to apply ep to the clutter problem we first identify the factors p and fn pxn next we select an approximating distribution from the exponential family and for this example it is convenient to choose a spherical gaussian q n vi. approximate inference snn vni the factor approximations will therefore take the form of exponential-quadratic functions of the form where n n and we equal to the prior p note that the use of convenient shorthand notation. the for n n can n does not imply that the right-hand side is a well-defined gaussian density fact as we shall see the variance parameter vn can be negative but is simply a be initialized to unity corresponding to sn vn and mn where d is the dimensionality of x and hence of the initial q defined by is therefore equal to the prior. we then iteratively refine the factors by taking one factor fn at a time and applying and note that we do not need to revise the term because an ep update will leave this term unchanged. here we state the results and leave the reader to fill in the details. first we remove the current from q by division using which has mean and inverse variance given by n mn mn m v v v n exercise exercise to give q next we evaluate the normalization constant zn using to give zn wn wn ai. exercise similarly we compute the mean and variance of qnew by finding the mean and variance of q to give v m mn n vn n vn v v mn n dvn where the quantity we use to compute the refined whose parameters are given by has a simple interpretation as the probability of the point xn not being clutter. then n w zn n ai n v mn mn v sn zn vni mn this refinement process is repeated until a suitable termination criterion is satisfied for instance that the maximum change in parameter values resulting from a complete expectation propagation figure examples of the approximation of specific factors for a one-dimensional version of the clutter problem showing fn in blue efn in red and qn in green. notice that the current form for qn controls the range of over which efn will be a good approximation to fn pass through all factors is less than some threshold. finally we use to evaluate the approximation to the model evidence given by pd vn mt nmn vn where b v examples factor approximations for the clutter problem with a one-dimensional parameter space are shown in figure note that the factor approximations can have infinite or even negative values for the variance parameter vn. this simply corresponds to approximations that curve upwards instead of downwards and are not necessarily problematic provided the overall approximate posterior q has positive variance. figure compares the performance of ep with variational bayes field theory and the laplace approximation on the clutter problem. expectation propagation on graphs so far in our general discussion of ep we have allowed the factors fi in the distribution p to be functions of all of the components of and similarly for the approximating in the approximating distribution q we now consider situations in which the factors depend only on subsets of the variables. such restrictions can be conveniently expressed using the framework of probabilistic graphical models as discussed in chapter here we use a factor graph representation because this encompasses both directed and undirected graphs. approximate inference posterior mean r o r r e laplace vb ep flops evidence vb r o r r e laplace flops ep figure comparison of expectation propagation variational inference and the laplace approximation on the clutter problem. the left-hand plot shows the error in the predicted posterior mean versus the number of floating point operations and the right-hand plot shows the corresponding results for the model evidence. we shall focus on the case in which the approximating distribution is fully factorized and we shall show that in this case expectation propagation reduces to loopy belief propagation to start with we show this in the context of a simple example and then we shall explore the general case. first of all recall from that if we minimize the kullback-leibler divergence with respect to a factorized distribution q then the optimal solution for each factor is simply the corresponding marginal of p. section now consider the factor graph shown on the left in figure which was introduced earlier in the context of the sum-product algorithm. the joint distribution is given by we seek an approximation qx that has the same factorization so that px qx note that normalization constants have been omitted and these can be re-instated at the end by local normalization as is generally done in belief propagation. now suppose we restrict attention to approximations in which the factors themselves factorize with respect to the individual variables so that qx which corresponds to the factor graph shown on the right in figure because the individual factors are factorized the overall distribution qx is itself fully factorized. now we apply the ep algorithm using the fully factorized approximation. suppose that we have initialized all of the factors and that we choose to refine factor expectation propagation fa fb fc figure on the left is a simple factor graph from figure and reproduced here for convenience. on the right is the corresponding factorized approximation. we first remove this factor from the approximating distribution to give q and we then multiply this by the exact factor to give the result as noted above is that qnewz comprises the product of factors one for each variable xi in which each factor is given by the corresponding marginal of q we now find qnewx by minimizing the kullback-leibler divergence these four marginals are given by and qnewx is obtained by multiplying these marginals together. we see that the only factors in qx that change when we are those that involve the variables in fb namely and to obtain the refined we simply divide qnewx by q which gives approximate inference section fb sent by factor node fb to variable node and is given by simi these are precisely the messages obtained using belief propagation in which messages from variable nodes to factor nodes have been folded into the messages from factor nodes to variable nodes. in particular corresponds to the message larly if we substitute into we obtain in corresponds to fa corresponds to fc giving the message which corresponds to fb factors at a time for instance if we refine is unchanged by definition while the refined version is again given by if this result differs slightly from standard belief propagation in that messages are passed in both directions at the same time. we can easily modify the ep procedure to give the standard form of the sum-product algorithm by updating just one of the we are refining only one term at a time then we can choose the order in which the refinements are done as we wish. in particular for a tree-structured graph we can follow a two-pass update scheme corresponding to the standard belief propagation schedule which will result in exact inference of the variable and factor marginals. the initialization of the approximation factors in this case is unimportant. now let us consider a general factor graph corresponding to the distribution p fi i where i represents the subset of variables associated with factor fi. we approximate this using a fully factorized distribution of the form i k q i k where k corresponds to an individual variable node. suppose that we wish to refine the particular l keeping all other terms fixed. we first remove the term j from q to give and then multiply by the exact factor fj j. to determine the refined l k k q we need only consider the functional dependence on l and so we simply find the corresponding marginal of up to a multiplicative constant this involves taking the marginal of fj j multiplied that are functions of any of the variables in j. terms that by any terms from q correspond to other factors i for i j will cancel between numerator and q j. denominator when we subsequently divide by q l fj j j we therefore obtain k m. exercises we recognize this as the sum-product rule in the form in which messages from variable nodes to factor nodes have been eliminated as illustrated by the example shown in figure the m corresponds to the message fj m m which factor node j sends to variable node m and the product over k in is over all factors that depend on the variables m that have variables than variable l in common with factor fj j. in other words to compute the outgoing message from a factor node we take the product of all the incoming messages from other factor nodes multiply by the local factor and then marginalize. thus the sum-product algorithm arises as a special case of expectation propagation if we use an approximating distribution that is fully factorized. this suggests that more flexible approximating distributions corresponding to partially disconnected graphs could be used to achieve higher accuracy. another generalization is to group factors fi i together into sets and to refine all the factors in a set together at each iteration. both of these approaches can lead to improvements in accuracy in general the problem of choosing the best combination of grouping and disconnection is an open research issue. we have seen that variational message passing and expectation propagation optimize two different forms of the kullback-leibler divergence. minka has shown that a broad range of message passing algorithms can be derived from a common framework involving minimization of members of the alpha family of divergences given by these include variational message passing loopy belief propagation and expectation propagation as well as a range of other algorithms which we do not have space to discuss here such as tree-reweighted message passing et al. fractional belief propagation and heskes and power ep exercises www verify that the log marginal distribution of the observed data ln px can be decomposed into two terms in the form where lq is given by and is given by use the properties and to solve the simultaneous equations and and hence show that provided the original distribution pz is nonsingular the unique solution for the means of the factors in the approximation distribution is given by and www consider a factorized variational distribution qz of the form by using the technique of lagrange multipliers verify that minimization of the kullback-leibler divergence with respect to one of the factors qizi keeping all other factors fixed leads to the solution suppose that px is some fixed distribution and that we wish to approximate it using a gaussian distribution qx n by writing down the form of the kl divergence for a gaussian qx and then differentiating show that approximate inference minimization of with respect to and leads to the result that is given by the expectation of x under px and that is given by the covariance. www consider a model in which the set of all hidden stochastic variables denoted collectively by z comprises some latent variables z together with some model parameters suppose we use a variational distribution that factorizes between latent variables and parameters so that qz qzzq in which the distribution q is approximated by a point estimate of the form q where is a vector of free parameters. show that variational optimization of this factorized distribution is equivalent to an em algorithm in which the e step optimizes qzz and the m step maximizes the expected complete-data log posterior distribution of with respect to the alpha family of divergences is defined by show that the kullbackleibler divergence corresponds to this can be done by writing p exp ln p ln p and then taking similarly show that corresponds to consider the problem of inferring the mean and precision of a univariate gaussian using a factorized variational approximation as considered in section show that the factor q is a gaussian of the form n n n with mean and precision given by and respectively. similarly show that the factor q is a gamma distribution of the form gam bn with parameters given by and consider the variational posterior distribution for the precision of a univariate gaussian whose parameters are given by and by using the standard results for the mean and variance of the gamma distribution given by and show that if we let n this variational posterior distribution has a mean given by the inverse of the maximum likelihood estimator for the variance of the data and a variance that goes to zero. by making use of the standard result e an for the mean of a gamma distribution together with and derive the result for the reciprocal of the expected precision in the factorized variational treatment of a univariate gaussian. www derive the decomposition given by that is used to find approxi mate posterior distributions over models using variational inference. www by using a lagrange multiplier to enforce the normalization constraint on the distribution qm show that the maximum of the lower bound is given by starting from the joint distribution and applying the general result show that the optimal variational distribution over the latent variables for the bayesian mixture of gaussians is given by by verifying the steps given in the text. exercises www starting from derive the result for the optimum variational posterior distribution over k and k in the bayesian mixture of gaussians and hence verify the expressions for the parameters of this distribution given by using the distribution verify the result using the result show that the expected value of the mixing coefficients in the variational mixture of gaussians is given by www verify the results and for the first two terms in the lower bound for the variational gaussian mixture model given by verify the results for the remaining terms in the lower bound for the variational gaussian mixture model given by in this exercise we shall derive the variational re-estimation equations for the gaussian mixture model by direct differentiation of the lower bound. to do this we assume that the variational distribution has the factorization defined by and with factors given by and substitute these into and hence obtain the lower bound as a function of the parameters of the variational distribution. then by maximizing the bound with respect to these parameters derive the re-estimation equations for the factors in the variational distribution and show that these are the same as those obtained in section derive the result for the predictive distribution in the variational treat ment of the bayesian mixture of gaussians model. www this exercise explores the variational bayes solution for the mixture of gaussians model when the size n of the data set is large and shows that it reduces we would expect to the maximum likelihood solution based on em derived in chapter note that results from appendix b may be used to help answer this exercise. first show that the posterior distribution k of the precisions becomes sharply peaked around the maximum likelihood solution. do the same for the posterior distribution of the means k k. next consider the posterior distribution for the mixing coefficients and show that this too becomes sharply peaked around the maximum likelihood solution. similarly show that the responsibilities become equal to the corresponding maximum likelihood values for large n by making use of the following asymptotic result for the digamma function for large x ln x o finally by making use of show that for large n the predictive distribution becomes a mixture of gaussians. show that the number of equivalent parameter settings due to interchange sym metries in a mixture model with k components is k!. approximate inference we have seen that each mode of the posterior distribution in a gaussian mixture model is a member of a family of k! equivalent modes. suppose that the result of running the variational inference algorithm is an approximate posterior distribution q that is localized in the neighbourhood of one of the modes. we can then approximate the full posterior distribution as a mixture of k! such q distributions once centred on each mode and having equal mixing coefficients. show that if we assume negligible overlap between the components of the q mixture the resulting lower bound differs from that for a single component q distribution through the addition of an extra term ln k!. www consider a variational gaussian mixture model in which there is no prior distribution over mixing coefficients k. instead the mixing coefficients are treated as parameters whose values are to be found by maximizing the variational lower bound on the log marginal likelihood. show that maximizing this lower bound with respect to the mixing coefficients using a lagrange multiplier to enforce the constraint that the mixing coefficients sum to one leads to the re-estimation result note that there is no need to consider all of the terms in the lower bound but only the dependence of the bound on the k. www we have seen in section that the singularities arising in the maximum likelihood treatment of gaussian mixture models do not arise in a bayesian treatment. discuss whether such singularities would arise if the bayesian model were solved using maximum posterior estimation. the variational treatment of the bayesian mixture of gaussians discussed in section made use of a factorized approximation to the posterior distribution. as we saw in figure the factorized assumption causes the variance of the posterior distribution to be under-estimated for certain directions in parameter space. discuss qualitatively the effect this will have on the variational approximation to the model evidence and how this effect will vary with the number of components in the mixture. hence explain whether the variational gaussian mixture will tend to under-estimate or over-estimate the optimal number of components. extend the variational treatment of bayesian linear regression to include a gamma hyperprior gam over and solve variationally by assuming a factorized variational distribution of the form qwq derive the variational update equations for the three factors in the variational distribution and also obtain an expression for the lower bound and for the predictive distribution. by making use of the formulae given in appendix b show that the variational lower bound for the linear basis function regression model defined by can be written in the form with the various terms defined by rewrite the model for the bayesian mixture of gaussians introduced in section as a conjugate model from the exponential family as discussed in section hence use the general results and to derive the specific results and exercises www show that the function fx lnx is concave for x by computing its second derivative. determine the form of the dual function g defined by and verify that minimization of x g with respect to according to indeed recovers the function lnx. by evaluating the second derivative show that the log logistic function fx e x is concave. derive the variational upper bound directly by making a second order taylor expansion of the log logistic function around a point x by finding the second derivative with respect to x show that the function fx e is a concave function of x. now consider the second derivatives with respect to the variable and hence show that it is a convex function of plot graphs of fx against x and against derive the lower bound on the logistic sigmoid function directly by making a first order taylor series expansion of the function fx in the variable centred on the value www consider the variational treatment of logistic regression with sequential learning in which data points are arriving one at a time and each must be processed and discarded before the next data point arrives. show that a gaussian approximation to the posterior distribution can be maintained through the use of the lower bound in which the distribution is initialized using the prior and as each data point is absorbed its corresponding variational parameter n is optimized. by differentiating the quantity q old defined by with respect to the variational parameter n show that the update equation for n for the bayesian logistic regression model is given by in this exercise we derive re-estimation equations for the variational parameters in the bayesian logistic regression model of section by direct maximization of the lower bound given by to do this set the derivative of l with respect to n equal to zero making use of the result for the derivative of the log of a determinant together with the expressions and which define the mean and covariance of the variational posterior distribution qw. derive the result for the lower bound l in the variational logistic regression model. this is most easily done by substituting the expressions for the gaussian prior qw n together with the lower bound hw on the likelihood function into the integral which defines l next gather together the terms which depend on w in the exponential and complete the square to give a gaussian integral which can then be evaluated by invoking the standard result for the normalization coefficient of a multivariate gaussian. finally take the logarithm to obtain consider the adf approximation scheme discussed in section and show that inclusion of the factor fj leads to an update of the model evidence of the form pjd pj approximate inference where zj is the normalization constant defined by by applying this result recursively and initializing with derive the result zj. j pd www consider the expectation propagation algorithm from section and suppose that one of the factors in the definition has the same exponential family functional form as the approximating distribution q show that if the is initialized to be then an ep update to leaves unchanged. this situation typically arises when one of the factors is the prior p and so we see that the prior factor can be incorporated once exactly and does not need to be refined. in this exercise and the next we shall verify the results for the expectation propagation algorithm applied to the clutter problem. begin by using the division formula to derive the expressions and by completing the square inside the exponential to identify the mean and variance. also show that the normalization constant zn defined by is given for the clutter problem by this can be done by making use of the general result show that the mean and variance of qnew for ep applied to the clutter problem are given by and to do this first prove the following results for the expectations of and t under qnew e mn v mn ln zn e t vn ln zn and then make use of the result for zn. next prove the results by using and completing the square in the exponential. finally use to derive the result sampling methods for most probabilistic models of practical interest exact inference is intractable and so we have to resort to some form of approximation. in chapter we discussed inference algorithms based on deterministic approximations which include methods such as variational bayes and expectation propagation. here we consider approximate inference methods based on numerical sampling also known as monte carlo techniques. although for some applications the posterior distribution over unobserved variables will be of direct interest in itself for most situations the posterior distribution is required primarily for the purpose of evaluating expectations for example in order to make predictions. the fundamental problem that we therefore wish to address in this chapter involves finding the expectation of some function fz with respect to a probability distribution pz. here the components of z might comprise discrete or continuous variables or some combination of the two. thus in the case of continuous sampling methods figure schematic illustration of a function f whose expectation is to be evaluated with respect to a distribution pz. pz fz z variables we wish to evaluate the expectation ef fzpz dz where the integral is replaced by summation in the case of discrete variables. this is illustrated schematically for a single continuous variable in figure we shall suppose that such expectations are too complex to be evaluated exactly using analytical techniques. the general idea behind sampling methods is to obtain a set of samples zl l l drawn independently from the distribution pz. this allows the expectation to be approximated by a finite sum as long as the samples zl are drawn from the distribution pz then ef and so the has the correct mean. the variance of the estimator is given fzl. l exercise by l e is the variance of the function fz under the distribution pz. it is worth emphasizing that the accuracy of the estimator therefore does not depend on the dimensionality of z and that in principle high accuracy may be achievable with a relatively small number of samples zl. in practice ten or twenty independent samples may suffice to estimate an expectation to sufficient accuracy. the problem however is that the samples might not be independent and so the effective sample size might be much smaller than the apparent sample size. also referring back to figure we note that if fz is small in regions where pz is large and vice versa then the expectation may be dominated by regions of small probability implying that relatively large sample sizes will be required to achieve sufficient accuracy. for many models the joint distribution pz is conveniently specified in terms of a graphical model. in the case of a directed graph with no observed variables it is sampling methods straightforward to sample from the joint distribution that it is possible to sample from the conditional distributions at each node using the following ancestral sampling approach discussed briefly in section the joint distribution is specified by pz pzipai where zi are the set of variables associated with node i and pai denotes the set of variables associated with the parents of node i. to obtain a sample from the joint distribution we make one pass through the set of variables in the order zm sampling from the conditional distributions pzipai. this is always possible because at each step all of the parent values will have been instantiated. after one pass through the graph we will have obtained a sample from the joint distribution. now consider the case of a directed graph in which some of the nodes are instantiated with observed values. we can in principle extend the above procedure at least in the case of nodes representing discrete variables to give the following logic sampling approach which can be seen as a special case of importance sampling discussed in section at each step when a sampled value is obtained for a variable zi whose value is observed the sampled value is compared to the observed value and if they agree then the sample value is retained and the algorithm proceeds to the next variable in turn. however if the sampled value and the observed value disagree then the whole sample so far is discarded and the algorithm starts again with the first node in the graph. this algorithm samples correctly from the posterior distribution because it corresponds simply to drawing samples from the joint distribution of hidden variables and data variables and then discarding those samples that disagree with the observed data the slight saving of not continuing with the sampling from the joint distribution as soon as one contradictory value is observed. however the overall probability of accepting a sample from the posterior decreases rapidly as the number of observed variables increases and as the number of states that those variables can take increases and so this approach is rarely used in practice. in the case of probability distributions defined by an undirected graph there is no one-pass sampling strategy that will sample even from the prior distribution with no observed variables. instead computationally more expensive techniques must be employed such as gibbs sampling which is discussed in section as well as sampling from conditional distributions we may also require samples from a marginal distribution. if we already have a strategy for sampling from a joint distribution pu v then it is straightforward to obtain samples from the marginal distribution pu simply by ignoring the values for v in each sample. there are numerous texts dealing with monte carlo methods. those of particular interest from the statistical inference perspective include chen et al. gamerman gilks et al. liu neal and robert and casella also there are review articles by besag et al. brooks diaconis and saloff-coste jerrum and sinclair neal tierney and andrieu et al. that provide additional information on sampling sampling methods methods for statistical inference. diagnostic tests for convergence of markov chain monte carlo algorithms are summarized in robert and casella and some practical guidance on the use of sampling methods in the context of machine learning is given in bishop and nabney basic sampling algorithms in this section we consider some simple strategies for generating random samples from a given distribution. because the samples will be generated by a computer algorithm they will in fact be pseudo-random numbers that is they will be deterministically calculated but must nevertheless pass appropriate tests for randomness. generating such numbers raises several subtleties et al. that lie outside the scope of this book. here we shall assume that an algorithm has been provided that generates pseudo-random numbers distributed uniformly over and indeed most software environments have such a facility built in. standard distributions we first consider how to generate random numbers from simple nonuniform distributions assuming that we already have available a source of uniformly distributed random numbers. suppose that z is uniformly distributed over the interval and that we transform the values of z using some function f so that y fz. the distribution of y will be governed by py pz dy dz exercise z hy where in this case pz our goal is to choose the function fz such that the resulting values of y have some specific desired distribution py. integrating we obtain y and so we have to which is the indefinite integral of py. thus y h transform the uniformly distributed random numbers using a function which is the inverse of the indefinite integral of the desired distribution. this is illustrated in figure consider for example the exponential distribution py exp y where y in this case the lower limit of the integral in is and so hy exp y. thus if we transform our uniformly distributed variable z using y z then y will have an exponential distribution. basic sampling algorithms figure geometrical interpretation of the transformation method for generating nonuniformly distributed random numbers. hy is the indefinite integral of the desired distribution py. if a uniformly distributed random variable z is transformed using y h then y will be distributed according to py. hy py y another example of a distribution to which the transformation method can be applied is given by the cauchy distribution py exercise in this case the inverse of the indefinite integral can be expressed in terms of the tan function. the generalization to multiple variables is straightforward and involves the ja cobian of the change of variables so that ym zm zm ym as a final example of the transformation method we consider the box-muller method for generating samples from a gaussian distribution. first suppose we generate pairs of uniformly distributed random numbers which we can do by transforming a variable distributed uniformly over using z this leads to a uniform next we discard each pair unless it satisfies distribution of points inside the unit circle with as illustrated in figure then for each pair we evaluate the quantities figure the box-muller method for generating gaussian distributed random numbers starts by generating samples from a uniform distribution inside the unit circle. sampling methods ln ln exercise exercise where then the joint distribution of and is given by exp exp and so and are independent and each has a gaussian distribution with zero mean and unit variance. if y has a gaussian distribution with zero mean and unit variance then y will have a gaussian distribution with mean and variance to generate vectorvalued variables having a multivariate gaussian distribution with mean and covariance we can make use of the cholesky decomposition which takes the form llt et al. then if z is a vector valued random variable whose components are independent and gaussian distributed with zero mean and unit variance then y lz will have mean and covariance obviously the transformation technique depends for its success on the ability to calculate and then invert the indefinite integral of the required distribution. such operations will only be feasible for a limited number of simple distributions and so we must turn to alternative approaches in search of a more general strategy. here we consider two techniques called rejection sampling and importance sampling. although mainly limited to univariate distributions and thus not directly applicable to complex problems in many dimensions they do form important components in more general strategies. rejection sampling the rejection sampling framework allows us to sample from relatively complex distributions subject to certain constraints. we begin by considering univariate distributions and discuss the extension to multiple dimensions subsequently. suppose we wish to sample from a distribution pz that is not one of the simple standard distributions considered so far and that sampling directly from pz is difficult. furthermore suppose as is often the case that we are easily able to evaluate pz for any given value of z up to some normalizing constant z so that zp can readily be evaluated but zp is unknown. pz in order to apply rejection sampling we need some simpler distribution qz sometimes called a proposal distribution from which we can readily draw samples. figure in the rejection sampling method samples are drawn from a simple distribution qz and rejected if they fall in the grey area between the unnormalized distribution epz and the scaled distribution kqz. the resulting samples are distributed according to pz which is the normalized version of epz. basic sampling algorithms kqz pz exercise z we next introduce a constant k whose value is chosen such that kqz for all values of z. the function kqz is called the comparison function and is illustrated for a univariate distribution in figure each step of the rejection sampler involves generating two random numbers. first we generate a number from the distribution qz. next we generate a number from the uniform distribution over this pair of random numbers has uniform distribution under the curve of the function kqz. finally if then the sample is rejected otherwise ure the remaining pairs then have uniform distribution under the curve ples are then accepted with and so the probability that a and hence the corresponding z values are distributed according to pz as desired. the original values of z are generated from the distribution qz and these sam is retained. thus the pair is rejected if it lies in the grey shaded region in fig paccept sample will be accepted is given by qz dz dz. the area under the unnormalized to the area under the curve kqz. limitation that kqz must be nowhere less we therefore see that the constant k should be as small as possible subject to the thus the fraction of points that are rejected by this method depends on the ratio of k as an illustration of the use of rejection sampling consider the task of sampling from the gamma distribution gamza b baza exp bz which for a has a bell-shaped form as shown in figure a suitable proposal distribution is therefore the cauchy because this too is bell-shaped and because we can use the transformation method discussed earlier to sample from it. we need to generalize the cauchy slightly to ensure that it nowhere has a smaller value than the gamma distribution. this can be achieved by transforming a uniform random variable y using z b tan y c which gives random numbers distributed according to. exercise sampling methods figure plot showing the gamma distribution given by as the green curve with a scaled cauchy proposal distribution shown by the red curve. samples from the gamma distribution can be obtained by sampling from the cauchy and then applying the rejection sampling criterion. pz z qz the minimum reject rate is obtained by setting c a and choosing the constant k to be as small as possible while still satisfying the requirement kqz the resulting comparison function is also illustrated in figure k adaptive rejection sampling in many instances where we might wish to apply rejection sampling it proves difficult to determine a suitable analytic form for the envelope distribution qz. an alternative approach is to construct the envelope function on the fly based on measured values of the distribution pz and wild construction of an envelope function is particularly straightforward for cases in which pz is log concave in other words when ln pz has derivatives that are nonincreasing functions of z. the construction of a suitable envelope function is illustrated graphically in figure the function ln pz and its gradient are evaluated at some initial set of grid points and the intersections of the resulting tangent lines are used to construct the envelope function. next a sample value is drawn from the envelope distribution. this is straightforward because the log of the envelope distribution is a succession exercise figure in the case of distributions that are log concave an envelope function for use in rejection sampling can be constructed using the tangent lines computed at a set of grid points. if a sample point is rejected it is added to the set of grid points and used to refine the envelope distribution. ln pz z basic sampling algorithms figure illustrative example of rejection sampling involving sampling from a gaussian distribution pz shown by the green curve by using rejection sampling from a proposal distribution qz that is also gaussian and whose scaled version kqz is shown by the red curve. pz z of linear functions and hence the envelope distribution itself comprises a piecewise exponential distribution of the form qz ki i exp iz zi zi z zi. once a sample has been drawn the usual rejection criterion can be applied. if the sample is accepted then it will be a draw from the desired distribution. if however the sample is rejected then it is incorporated into the set of grid points a new tangent line is computed and the envelope function is thereby refined. as the number of grid points increases so the envelope function becomes a better approximation of the desired distribution pz and the probability of rejection decreases. a variant of the algorithm exists that avoids the evaluation of derivatives the adaptive rejection sampling framework can also be extended to distributions that are not log concave simply by following each rejection sampling step with a metropolis-hastings step be discussed in section giving rise to adaptive rejection metropolis sampling et al. clearly for rejection sampling to be of practical value we require that the comparison function be close to the required distribution so that the rate of rejection is kept to a minimum. now let us examine what happens when we try to use rejection sampling in spaces of high dimensionality. consider for the sake of illustration a somewhat artificial problem in which we wish to sample from a zero-mean multivariate gaussian distribution with covariance pi where i is the unit matrix by rejection sampling from a proposal distribution that is itself a zero-mean gaussian distribution having covariance p in order that there exists a k such that kqz pz. in d-dimensions the optimum value of k is given by k q pd as illustrated for d in figure the acceptance rate will be the ratio of volumes under pz and kqz which because both distributions are normalized is just thus the acceptance rate diminishes exponentially with dimensionality. even if q exceeds p by just one percent for d the acceptance ratio will be approximately in this illustrative example the comparison function is close to the required distribution. for more practical examples where the desired distribution may be multimodal and sharply peaked it will be extremely difficult to find a good proposal distribution and comparison function. qi. obviously we must have q sampling methods figure importance sampling addresses the problem of evaluating the expectation of a function f with respect to a distribution pz from which it is difficult to draw samples diinstead samples are drawn rectly. from a simpler distribution qz and the corresponding terms in the summation are weighted by the ratios pzlqzl. pz qz fz z furthermore the exponential decrease of acceptance rate with dimensionality is a generic feature of rejection sampling. although rejection can be a useful technique in one or two dimensions it is unsuited to problems of high dimensionality. it can however play a role as a subroutine in more sophisticated algorithms for sampling in high dimensional spaces. importance sampling one of the principal reasons for wishing to sample from complicated probability distributions is to be able to evaluate expectations of the form the technique of importance sampling provides a framework for approximating expectations directly but does not itself provide a mechanism for drawing samples from distribution pz. the finite sum approximation to the expectation given by depends on being able to draw samples from the distribution pz. suppose however that it is impractical to sample directly from pz but that we can evaluate pz easily for any given value of z. one simplistic strategy for evaluating expectations would be to discretize z-space into a uniform grid and to evaluate the integrand as a sum of the form ef pzlfzl. an obvious problem with this approach is that the number of terms in the summation grows exponentially with the dimensionality of z. furthermore as we have already noted the kinds of probability distributions of interest will often have much of their mass confined to relatively small regions of z space and so uniform sampling will be very inefficient because in high-dimensional problems only a very small proportion of the samples will make a significant contribution to the sum. we would really like to choose the sample points to fall in regions where pz is large or ideally where the product pzfz is large. as in the case of rejection sampling importance sampling is based on the use of a proposal distribution qz from which it is easy to draw samples as illustrated in figure we can then express the expectation in the form of a finite sum over basic sampling algorithms samples drawn from qz ef l fzpz dz fz pz qz qz dz pzl qzl fzl. the quantities rl pzlqzl are known as importance weights and they correct the bias introduced by sampling from the wrong distribution. note that unlike rejection sampling all of the samples generated are retained. normalization constant so that pz can be evaluated easily distribution qz which has the same property. we then have it will often be the case that the distribution pz can only be evaluated up to a whereas zp is unknown. similarly we may wish to use an importance sampling we can use the same sample set to evaluate the ratio l ef fz fzpz dz zq zp zq zp qz dz qz dz dz ef wlfzl zq l zpzq with the result zp zq and hence where we have defined wl as with rejection sampling the success of the importance sampling approach depends crucially on how well the sampling distribution qz matches the desired sampling methods distribution pz. if as is often the case pzfz is strongly varying and has a significant proportion of its mass concentrated over relatively small regions of z space then the set of importance weights may be dominated by a few weights having large values with the remaining weights being relatively insignificant. thus the effective sample size can be much smaller than the apparent sample size l. the problem is even more severe if none of the samples falls in the regions where pzfz is large. in that case the apparent variances of rl and rlfzl may be small even though the estimate of the expectation may be severely wrong. hence a major drawback of the importance sampling method is the potential to produce results that are arbitrarily in error and with no diagnostic indication. this also highlights a key requirement for the sampling distribution qz namely that it should not be small or zero in regions where pz may be significant. for distributions defined in terms of a graphical model we can apply the importance sampling technique in various ways. for discrete variables a simple approach is called uniform sampling. the joint distribution for a directed graph is defined by each sample from the joint distribution is obtained by first setting those variables zi that are in the evidence set equal to their observed values. each of the remaining variables is then sampled independently from a uniform distribution over the space of possible instantiations. to determine the corresponding weight associ ated with a sample zl we note that the sampling is uniform over the possible choices for z and where x denotes the subset of variables that are observed and the equality follows from the fact that every sample z that is generated is necessarily consistent with the evidence. thus the weights rl are simply proportional to pz. note that the variables can be sampled in any order. this approach can yield poor results if the posterior distribution is far from uniform as is often the case in practice. an improvement on this approach is called likelihood weighted sampling and chang shachter and peot and is based on ancestral sampling of the variables. for each variable in turn if that variable is in the evidence set then it is just set to its instantiated value. if it is not in the evidence set then it is sampled from the conditional distribution pzipai in which the conditioning variables are set to their currently sampled values. the weighting associated with the resulting sample z is then given by pzipai pzipai. e rz pzipai pzipai zi e zi e this method can be further extended using self-importance sampling and peot in which the importance sampling distribution is continually updated to reflect the current estimated posterior distribution. sampling-importance-resampling the rejection sampling method discussed in section depends in part for its success on the determination of a suitable value for the constant k. for many pairs of distributions pz and qz it will be impractical to determine a suitable basic sampling algorithms value for k in that any value that is sufficiently large to guarantee a bound on the desired distribution will lead to impractically small acceptance rates. as in the case of rejection sampling the sampling-importance-resampling approach also makes use of a sampling distribution qz but avoids having to determine the constant k. there are two stages to the scheme. in the first stage l samples zl are drawn from qz. then in the second stage weights wl are constructed using finally a second set of l samples is drawn from the discrete distribution zl with probabilities given by the weights wl. the resulting l samples are only approximately distributed according to pz but the distribution becomes correct in the limit l to see this consider the univariate case and note that the cumulative distribution of the resampled values is given by l izl wl pz a where i. is the indicator function equals if its argument is true and otherwise. taking the limit l and assuming suitable regularity of the distributions we can replace the sums by integrals weighted according to the original sampling distribution qz iz qz dz qz dz iz dz dz pz a iz apz dz which is the cumulative distribution function of pz. again we see that the normalization of pz is not required. for a finite value of l and a given initial sample set the resampled values will only approximately be drawn from the desired distribution. as with rejection sampling the approximation improves as the sampling distribution qz gets closer to the desired distribution pz. when qz pz the initial samples zl have the desired distribution and the weights wn so that the resampled values also have the desired distribution. if moments with respect to the distribution pz are required then they can be sampling methods evaluated directly using the original samples together with the weights because efz fzpz dz dz dz wlfzl. sampling and the em algorithm in addition to providing a mechanism for direct implementation of the bayesian framework monte carlo methods can also play a role in the frequentist paradigm for example to find maximum likelihood solutions. in particular sampling methods can be used to approximate the e step of the em algorithm for models in which the e step cannot be performed analytically. consider a model with hidden variables z visible variables x and parameters the function that is optimized with respect to in the m step is the expected complete-data log likelihood given by q old pzx old ln pz x dz. we can use sampling methods to approximate this integral by a finite sum over samples which are drawn from the current estimate for the posterior distribution pzx old so that q old l ln pzl x the q function is then optimized in the usual way in the m step. this procedure is called the monte carlo em algorithm. it is straightforward to extend this to the problem of finding the mode of the posterior distribution over map estimate when a prior distribution p has been defined simply by adding ln p to the function q old before performing the m step. a particular instance of the monte carlo em algorithm called stochastic em arises if we consider a finite mixture model and draw just one sample at each e step. here the latent variable z characterizes which of the k components of the mixture is responsible for generating each data point. in the e step a sample of z is taken from the posterior distribution pzx old where x is the data set. this effectively makes a hard assignment of each data point to one of the components in the mixture. in the m step this sampled approximation to the posterior distribution is used to update the model parameters in the usual way. markov chain monte carlo now suppose we move from a maximum likelihood approach to a full bayesian treatment in which we wish to sample from the posterior distribution over the parameter vector in principle we would like to draw samples from the joint posterior p zx but we shall suppose that this is computationally difficult. suppose further that it is relatively straightforward to sample from the complete-data parameter posterior p x. this inspires the data augmentation algorithm which alternates between two steps known as the i-step step analogous to an e step and the p-step step analogous to an m step. ip algorithm i-step. we wish to sample from pzx but we cannot do this directly. we therefore note the relation pzx pz xp d and hence for l l we first draw a sample from the current estimate for p and then use this to draw a sample zl from pz x. p-step. given the relation p p xpzx dz we use the samples obtained from the i-step to compute a revised estimate of the posterior distribution over given by p l p x. by assumption it will be feasible to sample from this approximation in the i-step. note that we are making a artificial distinction between parameters and hidden variables z. from now on we blur this distinction and focus simply on the problem of drawing samples from a given posterior distribution. markov chain monte carlo in the previous section we discussed the rejection sampling and importance sampling strategies for evaluating expectations of functions and we saw that they suffer from severe limitations particularly in spaces of high dimensionality. we therefore turn in this section to a very general and powerful framework called markov chain monte carlo which allows sampling from a large class of distributions sampling methods and which scales well with the dimensionality of the sample space. markov chain monte carlo methods have their origins in physics and ulam and it was only towards the end of the that they started to have a significant impact in the field of statistics. as with rejection and importance sampling we again sample from a proposal distribution. this time however we maintain a record of the current state z and the proposal distribution qzz depends on this current state and so the sequence of samples forms a markov chain. again if we write pz we will assume can readily be evaluated for any given value of z although section the value of zp may be unknown. the proposal distribution itself is chosen to be sufficiently simple that it is straightforward to draw samples from it directly. at each cycle of the algorithm we generate a candidate sample from the proposal distribution and then accept the sample according to an appropriate criterion. in the basic metropolis algorithm et al. we assume that the proposal distribution is symmetric that is qzazb qzbza for all values of za and zb. the candidate sample is then accepted with probability z min this can be achieved by choosing a random number u with uniform distribution over the unit interval and then accepting the sample if z u. note that if the step from z to causes an increase in the value of pz then the candidate point is certain to be kept. if the candidate sample is accepted then z otherwise the candidate point is discarded z is set to z and another candidate sample is drawn from the distribution qzz this is in contrast to rejection sampling where rejected samples are simply discarded. in the metropolis algorithm when a candidate point is rejected the previous sample is included instead in the final list of samples leading to multiple copies of samples. of course in a practical implementation only a single copy of each retained sample would be kept along with an integer weighting factor recording how many times that state appears. as we shall see as long as qzazb is positive for any values of za and zb is a sufficient but not necessary condition the distribution of z tends to pz as it should be emphasized however that the sequence is not a set of independent samples from pz because successive samples are highly correlated. if we wish to obtain independent samples then we can discard most of the sequence and just retain every m th sample. for m sufficiently large the retained samples will for all practical purposes be independent. figure shows a simple illustrative example of sampling from a two-dimensional gaussian distribution using the metropolis algorithm in which the proposal distribution is an isotropic gaussian. further insight into the nature of markov chain monte carlo algorithms can be gleaned by looking at the properties of a specific example namely a simple random markov chain monte carlo figure a simple illustration using metropolis algorithm to sample from a gaussian distribution whose one standard-deviation contour is shown by the ellipse. the proposal distribution is an isotropic gaussian distribution whose standard deviation is steps that are accepted are shown as green lines and rejected steps are shown in red. a total of candidate samples are generated of which are rejected. exercise walk. consider a state space z consisting of the integers with probabilities pz z pz z pz z where z denotes the state at step if the initial state is then by symmetry the expected state at time will also be zero ez and similarly it is easily seen that ez thus after steps the random walk has only travelled a distance that on average is proportional to the square root of this square root dependence is typical of random walk behaviour and shows that random walks are very inefficient in exploring the state space. as we shall see a central goal in designing markov chain monte carlo methods is to avoid random walk behaviour. markov chains before discussing markov chain monte carlo methods in more detail it is useful to study some general properties of markov chains in more detail. in particular we ask under what circumstances will a markov chain converge to the desired distribution. a first-order markov chain is defined to be a series of random variables zm such that the following conditional independence property holds for m m zm this of course can be represented as a directed graph in the form of a chain an example of which is shown in figure we can then specify the markov chain by giving the probability distribution for the initial variable together with the sampling methods conditional probabilities for subsequent variables in the form of transition probabilities tmzm a markov chain is called homogeneous if the transition probabilities are the same for all m. the marginal probability for a particular variable can be expressed in terms of the marginal probability for the previous variable in the chain in the form zm a distribution is said to be invariant or stationary with respect to a markov chain if each step in the chain leaves that distribution invariant. thus for a homogeneous markov chain with transition probabilities t z the distribution is invariant if t note that a given markov chain may have more than one invariant distribution. for instance if the transition probabilities are given by the identity transformation then any distribution will be invariant. a sufficient not necessary condition for ensuring that the required distribution pz is invariant is to choose the transition probabilities to satisfy the property of detailed balance defined by z for the particular distribution it is easily seen that a transition probability that satisfies detailed balance with respect to a particular distribution will leave that distribution invariant because z a markov chain that respects detailed balance is said to be reversible. our goal is to use markov chains to sample from a given distribution. we can achieve this if we set up a markov chain such that the desired distribution is invariant. however we must also require that for m the distribution pzm converges to the required invariant distribution irrespective of the choice of initial distribution this property is called ergodicity and the invariant distribution is then called the equilibrium distribution. clearly an ergodic markov chain can have only one equilibrium distribution. it can be shown that a homogeneous markov chain will be ergodic subject only to weak restrictions on the invariant distribution and the transition probabilities in practice we often construct the transition probabilities from a set of base transitions bk. this can be achieved through a mixture distribution of the form t z z markov chain monte carlo for some set of mixing coefficients k satisfying k and k k alternatively the base transitions may be combined through successive application so that t bk zk z. z zn if a distribution is invariant with respect to each of the base transitions then obviously it will also be invariant with respect to either of the t z given by or for the case of the mixture if each of the base transitions satisfies detailed balance then the mixture transition t will also satisfy detailed balance. this does not hold for the transition probability constructed using although by symmetrizing the order of application of the base transitions in the form bk bk detailed balance can be restored. a common example of the use of composite transition probabilities is where each base transition changes only a subset of the variables. the metropolis-hastings algorithm earlier we introduced the basic metropolis algorithm without actually demonstrating that it samples from the required distribution. before giving a proof we first discuss a generalization known as the metropolis-hastings algorithm to the case where the proposal distribution is no longer a symmetric function of its arguments. in particular at step of the algorithm in which the current state is z we draw a sample from the distribution qkzz and then accept it with probability z where z min here k labels the members of the set of possible transitions being considered. again the evaluation of the acceptance criterion does not require knowledge of the normal izing constant zp in the probability distribution pz for a symmetric proposal distribution the metropolis-hastings criterion reduces to the standard metropolis criterion given by we can show that pz is an invariant distribution of the markov chain defined by the metropolis-hastings algorithm by showing that detailed balance defined by is satisfied. using we have z min min as required. the specific choice of proposal distribution can have a marked effect on the performance of the algorithm. for continuous state spaces a common choice is a gaussian centred on the current state leading to an important trade-off in determinif the variance is small then the ing the variance parameter of this distribution. sampling methods figure schematic illustration of the use of an isotropic gaussian proposal distribution circle to sample from a correlated multivariate gaussian distribution ellipse having very different standard deviations in different directions using the metropolis-hastings algorithm. in order to keep the rejection rate low the scale of the proposal distribution should be on the order of the smallest standard deviation min which leads to random walk behaviour in which the number of steps separating states that are approximately independent is of order max where max is the largest standard deviation. min max proportion of accepted transitions will be high but progress through the state space takes the form of a slow random walk leading to long correlation times. however if the variance parameter is large then the rejection rate will be high because in the kind of complex problems we are considering many of the proposed steps will be to states for which the probability pz is low. consider a multivariate distribution pz having strong correlations between the components of z as illustrated in figure the scale of the proposal distribution should be as large as possible without incurring high rejection rates. this suggests that should be of the same order as the smallest length scale min. the system then explores the distribution along the more extended direction by means of a random walk and so the number of steps to arrive at a state that is more or less independent of the original state is of order max in fact in two dimensions the increase in rejection rate as increases is offset by the larger steps sizes of those transitions that are accepted and more generally for a multivariate gaussian the number of steps required to obtain independent samples scales like max where is the second-smallest standard deviation these details aside it remains the case that if the length scales over which the distributions vary are very different in different directions then the metropolis hastings algorithm can have very slow convergence. gibbs sampling gibbs sampling and geman is a simple and widely applicable markov chain monte carlo algorithm and can be seen as a special case of the metropolishastings algorithm. consider the distribution pz zm from which we wish to sample and suppose that we have chosen some initial state for the markov chain. each step of the gibbs sampling procedure involves replacing the value of one of the variables by a value drawn from the distribution of that variable conditioned on the values of the remaining variables. thus we replace zi by a value drawn from the distribution pzizi where zi denotes the ith component of z and zi denotes zm but with zi omitted. this procedure is repeated either by cycling through the variables gibbs sampling in some particular order or by choosing the variable to be updated at each step at random from some distribution. and at step of the algorithm we have selected values z replace z bution for example suppose we have a distribution over three variables we first obtained by sampling from the conditional by a new value z z and z z next we replace z distribution by a value z obtained by sampling from the conditional z so that the new value for is used straight away in subsequent sampling steps. then we update with a sample z drawn from z and so on cycling through the three variables in turn. gibbs sampling initialize i m for t sample z sample z z z z z m m sample z sample z j pzjz z j z z m m pzmz z z m josiah willard gibbs gibbs spent almost his entire life living in a house built by his father in new haven connecticut. in gibbs was granted the first phd in engineering in the united states and in he was appointed to the first chair of mathematical physics in the united states at yale a post for which he received no salary because at the time he had no publications. he developed the field of vector analysis and made contributions to crystallography and planetary orbits. his most famous work entitled ontheequilibriumofheterogeneous substances laid the foundations for the science of physical chemistry. sampling methods to show that this procedure samples from the required distribution we first of all note that the distribution pz is an invariant of each of the gibbs sampling steps individually and hence of the whole markov chain. this follows from the fact that when we sample from pzizi the marginal distribution pzi is clearly invariant because the value of zi is unchanged. also each step by definition samples from the correct conditional distribution pzizi. because these conditional and marginal distributions together specify the joint distribution we see that the joint distribution is itself invariant. the second requirement to be satisfied in order that the gibbs sampling procedure samples from the correct distribution is that it be ergodic. a sufficient condition for ergodicity is that none of the conditional distributions be anywhere zero. if this is the case then any point in z space can be reached from any other point in a finite number of steps involving one update of each of the component variables. if this requirement is not satisfied so that some of the conditional distributions have zeros then ergodicity if it applies must be proven explicitly. the distribution of initial states must also be specified in order to complete the algorithm although samples drawn after many iterations will effectively become independent of this distribution. of course successive samples from the markov chain will be highly correlated and so to obtain samples that are nearly independent it will be necessary to subsample the sequence. we can obtain the gibbs sampling procedure as a particular instance of the metropolis-hastings algorithm as follows. consider a metropolis-hastings sampling step involving the variable zk in which the remaining variables zk remain fixed and for which the transition probability from z to is given by kzk. we note that zk because these components are unchanged by the sampling step. also pz pzkzkpzk. thus the factor that determines the acceptance probability in the metropolis-hastings is given by z kzk where we have used zk. thus the metropolis-hastings steps are always accepted. as with the metropolis algorithm we can gain some insight into the behaviour of gibbs sampling by investigating its application to a gaussian distribution. consider a correlated gaussian in two variables as illustrated in figure having conditional distributions of width l and marginal distributions of width l. the typical step size is governed by the conditional distributions and will be of order l. because the state evolves according to a random walk the number of steps needed to obtain independent samples from the distribution will be of order of course if the gaussian distribution were uncorrelated then the gibbs sampling procedure would be optimally efficient. for this simple problem we could rotate the coordinate system in order to decorrelate the variables. however in practical applications it will generally be infeasible to find such transformations. one approach to reducing random walk behaviour in gibbs sampling is called over-relaxation in its original form this applies to problems for which gibbs sampling l figure illustration of gibbs sampling by alternate updates of two variables whose distribution is a correlated gaussian. the step size is governed by the standard deviation of the conditional distribution curve and is ol leading to slow progress in the direction of elongation of the joint distribution ellipse. the number of steps needed to obtain an independent sample from the distribution is l the conditional distributions are gaussian which represents a more general class of distributions than the multivariate gaussian because for example the non-gaussian distribution pz y exp has gaussian conditional distributions. at each step of the gibbs sampling algorithm the conditional distribution for a particular component zi has some mean i and some variance i in the over-relaxation framework the value of zi is replaced with i i i i z i then so too does z where is a gaussian random variable with zero mean and unit variance and is a parameter such that for the method is equivalent to standard gibbs sampling and for the step is biased to the opposite side of the mean. this step leaves the desired distribution invariant because if zi has mean i and variance i. the effect of over-relaxation is to encourage directed motion through state space when the variables are highly correlated. the framework of ordered over-relaxation generalizes this approach to nongaussian distributions. the practical applicability of gibbs sampling depends on the ease with which samples can be drawn from the conditional distributions pzkzk. in the case of probability distributions specified using graphical models the conditional distributions for individual nodes depend only on the variables in the corresponding markov blankets as illustrated in figure for directed graphs a wide choice of conditional distributions for the individual nodes conditioned on their parents will lead to conditional distributions for gibbs sampling that are log concave. the adaptive rejection sampling methods discussed in section therefore provide a framework for monte carlo sampling from directed graphs with broad applicability. if the graph is constructed using distributions from the exponential family and if the parent-child relationships preserve conjugacy then the full conditional distributions arising in gibbs sampling will have the same functional form as the orig sampling methods figure the gibbs sampling method requires samples to be drawn from the conditional distribution of a variable conditioned on the remaining variables. for graphical models this conditional distribution is a function only of the states of the nodes in the markov blanket. for an undirected graph this comprises the set of neighbours as shown on the left while for a directed graph the markov blanket comprises the parents the children and the co-parents as shown on the right. inal conditional distributions on the parents defining each node and so standard sampling techniques can be employed. in general the full conditional distributions will be of a complex form that does not permit the use of standard sampling algorithms. however if these conditionals are log concave then sampling can be done efficiently using adaptive rejection sampling the corresponding variable is a scalar. if at each stage of the gibbs sampling algorithm instead of drawing a sample from the corresponding conditional distribution we make a point estimate of the variable given by the maximum of the conditional distribution then we obtain the iterated conditional modes algorithm discussed in section thus icm can be seen as a greedy approximation to gibbs sampling. because the basic gibbs sampling technique considers one variable at a time there are strong dependencies between successive samples. at the opposite extreme if we could draw samples directly from the joint distribution operation that we are supposing is intractable then successive samples would be independent. we can hope to improve on the simple gibbs sampler by adopting an intermediate strategy in which we sample successively from groups of variables rather than individual variables. this is achieved in the blocking gibbs sampling algorithm by choosing blocks of variables not necessarily disjoint and then sampling jointly from the variables in each block in turn conditioned on the remaining variables et al. slice sampling we have seen that one of the difficulties with the metropolis algorithm is the sensitivity to step size. if this is too small the result is slow decorrelation due to random walk behaviour whereas if it is too large the result is inefficiency due to a high rejection rate. the technique of slice sampling provides an adaptive step size that is automatically adjusted to match the characteristics of the distribution. again it requires that we are able to evaluate the unnormalized consider first the univariate case. slice sampling involves augmenting z with an additional variable u and then drawing samples from the joint u space. we shall see another example of this approach when we discuss hybrid monte carlo in section the goal is to sample uniformly from the area under the distribution pz u z slice sampling pz zmin u zmax z z z for a given value z a value of u is chosen uniformly in figure illustration of slice sampling. the region u epz which then defines a slice through the distribution shown by the solid horizontal because it is infeasible to sample directly from a slice a new sample of z is drawn from a region lines. zmin z zmax which contains the previous value z given by where zp u dz. the marginal distribution over z is given by epz u du if u otherwise pz du zp zp values. this can be achieved by alternately sampling z and u. given the value of z and so we can sample from pz by sampling u and then ignoring the u we and then sample u uniformly in the range u which is distribution defined by u. this is illustrated in figure u invariant which can be achieved by ensuring that detailed balance is in practice it can be difficult to sample directly from a slice through the distribution and so instead we define a sampling scheme that leaves the uniform distribution straightforward. then we fix u and sample z uniformly from the slice through the satisfied. suppose the current value of z is denoted z and that we have obtained a corresponding sample u. the next value of z is obtained by considering a region zmin z zmax that contains z it is in the choice of this region that the adaptation to the characteristic length scales of the distribution takes place. we want the region to encompass as much of the slice as possible so as to allow large moves in z space while having as little as possible of this region lying outside the slice because this makes the sampling less efficient. one approach to the choice of region involves starting with a region containing z having some width w and then testing each of the end points to see if they lie within the slice. if either end point does not then the region is extended in that direction by increments of value w until the end point lies outside the region. a is then chosen uniformly from this region and if it lies within the candidate value z slice then it forms z if it lies outside the slice then the region is shrunk such forms an end point and such that the region still contains z then another that z sampling methods candidate point is drawn uniformly from this reduced region and so on until a value of z is found that lies within the slice. slice sampling can be applied to multivariate distributions by repeatedly sampling each variable in turn in the manner of gibbs sampling. this requires that we are able to compute for each component zi a function that is proportional to pzizi. the hybrid monte carlo algorithm as we have already noted one of the major limitations of the metropolis algorithm is that it can exhibit random walk behaviour whereby the distance traversed through the state space grows only as the square root of the number of steps. the problem cannot be resolved simply by taking bigger steps as this leads to a high rejection rate. in this section we introduce a more sophisticated class of transitions based on an analogy with physical systems and that has the property of being able to make large changes to the system state while keeping the rejection probability small. it is applicable to distributions over continuous variables for which we can readily evaluate the gradient of the log probability with respect to the state variables. we will discuss the dynamical systems framework in section and then in section we explain how this may be combined with the metropolis algorithm to yield the powerful hybrid monte carlo algorithm. a background in physics is not required as this section is self-contained and the key results are all derived from first principles. dynamical systems the dynamical approach to stochastic sampling has its origins in algorithms for simulating the behaviour of physical systems evolving under hamiltonian dynamics. in a markov chain monte carlo simulation the goal is to sample from a given probability distribution pz. the framework of hamiltonian dynamics is exploited by casting the probabilistic simulation in the form of a hamiltonian system. in order to remain in keeping with the literature in this area we make use of the relevant dynamical systems terminology where appropriate which will be defined as we go along. the dynamics that we consider corresponds to the evolution of the state variable z under continuous time which we denote by classical dynamics is described by newton s second law of motion in which the acceleration of an object is proportional to the applied force corresponding to a second-order differential equation over time. we can decompose a second-order equation into two coupled firstorder equations by introducing intermediate momentum variables r corresponding to the rate of change of the state variables z having components ri dzi d where the zi can be regarded as position variables in this dynamics perspective. thus the hybrid monte carlo algorithm for each position variable there is a corresponding momentum variable and the joint space of position and momentum variables is called phase space. without loss of generality we can write the probability distribution pz in the form pz exp ez zp where ez is interpreted as the potential energy of the system when in state z. the system acceleration is the rate of change of momentum and is given by the applied force which itself is the negative gradient of the potential energy dri d ez zi it is convenient to reformulate this dynamical system using the hamiltonian framework. to do this we first define the kinetic energy by kr i i the total energy of the system is then the sum of its potential and kinetic energies hz r ez kr exercise where h is the hamiltonian function. using and we can now express the dynamics of the system in terms of the hamiltonian equations given by dzi d dri d h ri h zi william hamilton william rowan hamilton was an irish mathematician and physicist and child prodigy who was appointed professor of astronomy at trinity college dublin in before he had even graduated. one of hamilton s most important contributions was a new formulation of dynamics which played a significant role in the later development of quantum mechanics. his other great achievement was the development of quaternions which generalize the concept of complex numbers by introducing three distinct square roots of minus one which satisfy ijk it is said that these equations occurred to him while walking along the royal canal in dublin with his wife on october and he promptly carved the equations into the side of broome bridge. although there is no longer any evidence of the carving there is now a stone plaque on the bridge commemorating the discovery and displaying the quaternion equations. sampling methods during the evolution of this dynamical system the value of the hamiltonian h is constant as is easily seen by differentiation i i dh d h zi dzi d h zi h ri h ri h ri dri d h zi a second important property of hamiltonian dynamical systems known as liouville s theorem is that they preserve volume in phase space. in other words if we consider a region within the space of variables r then as this region evolves under the equations of hamiltonian dynamics its shape may change but its volume will not. this can be seen by noting that the flow field of change of location in phase space is given by dz d and that the divergence of this field vanishes v dr d i i zi zi dzi d ri dri d h ri ri h zi div v now consider the joint distribution over phase space whose total energy is the hamiltonian i.e. the distribution given by pz r exp hz r. zh using the two results of conservation of volume and conservation of h it follows that the hamiltonian dynamics will leave pz r invariant. this can be seen by considering a small region of phase space over which h is approximately constant. if we follow the evolution of the hamiltonian equations for a finite time then the volume of this region will remain unchanged as will the value of h in this region and hence the probability density which is a function only of h will also be unchanged. although h is invariant the values of z and r will vary and so by integrating the hamiltonian dynamics over a finite time duration it becomes possible to make large changes to z in a systematic way that avoids random walk behaviour. evolution under the hamiltonian dynamics will not however sample ergodically from pz r because the value of h is constant. in order to arrive at an ergodic sampling scheme we can introduce additional moves in phase space that change the value of h while also leaving the distribution pz r invariant. the simplest way to achieve this is to replace the value of r with one drawn from its distribution conditioned on z. this can be regarded as a gibbs sampling step and hence from the hybrid monte carlo algorithm exercise section we see that this also leaves the desired distribution invariant. noting that z and r are independent in the distribution pz r we see that the conditional distribution prz is a gaussian from which it is straightforward to sample. in a practical application of this approach we have to address the problem of performing a numerical integration of the hamiltonian equations. this will necessarily introduce numerical errors and so we should devise a scheme that minimizes the impact of such errors. in fact it turns out that integration schemes can be devised for which liouville s theorem still holds exactly. this property will be important in the hybrid monte carlo algorithm which is discussed in section one scheme for achieving this is called the leapfrog discretization and involves alternately updat ing discrete-time to the position and momentum variables using e zi e zi we see that this takes the form of a half-step update of the momentum variables with step size followed by a full-step update of the position variables with step size followed by a second half-step update of the momentum variables. if several leapfrog steps are applied in succession it can be seen that half-step updates to the momentum variables can be combined into full-step updates with step size the successive updates to position and momentum variables then leapfrog over each other. in order to advance the dynamics by a time interval we need to take steps. the error involved in the discretized approximation to the continuous time dynamics will go to zero assuming a smooth function ez in the limit however for a nonzero as used in practice some residual error will remain. we shall see in section how the effects of such errors can be eliminated in the hybrid monte carlo algorithm. in summary then the hamiltonian dynamical approach involves alternating between a series of leapfrog updates and a resampling of the momentum variables from their marginal distribution. note that the hamiltonian dynamics method unlike the basic metropolis algorithm is able to make use of information about the gradient of the log probability distribution as well as about the distribution itself. an analogous situation is familiar from the domain of function optimization. in most cases where gradient information is available it is highly advantageous to make use of it. informally this follows from the fact that in a space of dimension d the additional computational cost of evaluating a gradient compared with evaluating the function itself will typically be a fixed factor independent of d whereas the d-dimensional gradient vector conveys d pieces of information compared with the one piece of information given by the function itself. sampling methods hybrid monte carlo as we discussed in the previous section for a nonzero step size the discretization of the leapfrog algorithm will introduce errors into the integration of the hamiltonian dynamical equations. hybrid monte carlo et al. neal combines hamiltonian dynamics with the metropolis algorithm and thereby removes any bias associated with the discretization. specifically the algorithm uses a markov chain consisting of alternate stochastic updates of the momentum variable r and hamiltonian dynamical updates using the leapfrog algorithm. after each application of the leapfrog algorithm the resulting candidate state is accepted or rejected according to the metropolis criterion based on the value of the hamiltonian h. thus if r is the initial state and is the state after the leapfrog integration then this candidate state is accepted with probability min exphz r if the leapfrog integration were to simulate the hamiltonian dynamics perfectly then every such candidate step would automatically be accepted because the value of h would be unchanged. due to numerical errors the value of h may sometimes decrease and we would like the metropolis criterion to remove any bias due to this effect and ensure that the resulting samples are indeed drawn from the required distribution. in order for this to be the case we need to ensure that the update equations corresponding to the leapfrog integration satisfy detailed balance this is easily achieved by modifying the leapfrog scheme as follows. before the start of each leapfrog integration sequence we choose at random with equal probability whether to integrate forwards in time step size or backwards in time step size we first note that the leapfrog integration scheme and is time-reversible so that integration for l steps using step size will exactly undo the effect of integration for l steps using step size next we show that the leapfrog integration preserves phase-space volume exactly. this follows from the fact that each step in the leapfrog scheme updates either a zi variable or an ri variable by an amount that is a function only of the other variable. as shown in figure this has the effect of shearing a region of phase space while not altering its volume. finally we use these results to show that detailed balance holds. consider a small region r of phase space that under a sequence of l leapfrog iterations of step size maps to a region using conservation of volume under the leapfrog iteration we see that if r has volume v then so too will if we choose an initial point from the distribution and then update it using l leapfrog interactions the probability of the transition going from r to is given by zh exp hr v exp hr where the factor of arises from the probability of choosing to integrate with a positive step size rather than a negative one. similarly the probability of starting in the hybrid monte carlo algorithm ri i r zi i z figure each step of the leapfrog algorithm modifies either a position variable zi or a momentum variable ri. because the change to one variable is a function only of the other any region in phase space will be sheared without change of volume. region and integrating backwards in time to end up in region r is given by zh exp hr exp v exercise it is easily seen that the two probabilities and are equal and hence detailed balance holds. note that this proof ignores any overlap between the regions r and but is easily generalized to allow for such overlap. it is not difficult to construct examples for which the leapfrog algorithm returns to its starting position after a finite number of iterations. in such cases the random replacement of the momentum values before each leapfrog integration will not be sufficient to ensure ergodicity because the position variables will never be updated. such phenomena are easily avoided by choosing the magnitude of the step size at random from some small interval before each leapfrog integration. we can gain some insight into the behaviour of the hybrid monte carlo algorithm by considering its application to a multivariate gaussian. for convenience consider a gaussian distribution pz with independent components for which the hamiltonian is given by i i hz r i i i our conclusions will be equally valid for a gaussian distribution having correlated components because the hybrid monte carlo algorithm exhibits rotational isotropy. during the leapfrog integration each pair of phase-space variables zi ri evolves independently. however the acceptance or rejection of the candidate point is based on the value of h which depends on the values of all of the variables. thus a significant integration error in any one of the variables could lead to a high probability of rejection. in order that the discrete leapfrog integration be a reasonably sampling methods good approximation to the true continuous-time dynamics it is necessary for the leapfrog integration scale to be smaller than the shortest length-scale over which the potential is varying significantly. this is governed by the smallest value of i which we denote by min. recall that the goal of the leapfrog integration in hybrid monte carlo is to move a substantial distance through phase space to a new state that is relatively independent of the initial state and still achieve a high probability of acceptance. in order to achieve this the leapfrog integration must be continued for a number of iterations of order max min. by contrast consider the behaviour of a simple metropolis algorithm with an isotropic gaussian proposal distribution of variance considered earlier. in order to avoid high rejection rates the value of s must be of order min. the exploration of state space then proceeds by a random walk and takes of order max steps to arrive at a roughly independent state. estimating the partition function as we have seen most of the sampling algorithms considered in this chapter require only the functional form of the probability distribution up to a multiplicative constant. thus if we write pez exp ez ze then the value of the normalization constant ze also known as the partition function is not needed in order to draw samples from pz. however knowledge of the value of ze can be useful for bayesian model comparison since it represents the model evidence the probability of the observed data given the model and so it is of interest to consider how its value might be obtained. we assume that direct evaluation by summing or integrating the function exp ez over the state space of z is intractable. for model comparison it is actually the ratio of the partition functions for two models that is required. multiplication of this ratio by the ratio of prior probabilities gives the ratio of posterior probabilities which can then be used for model selection or model averaging. one way to estimate a ratio of partition functions is to use importance sampling ze zg from a distribution with energy function gz z exp ez z exp gz z exp ez gz exp gz z exp gz egzexp e g exp ezl gzl l estimating the partition function where are samples drawn from the distribution defined by pgz. if the distribution pg is one for which the partition function can be evaluated analytically for example a gaussian then the absolute value of ze can be obtained. this approach will only yield accurate results if the importance sampling distribution pg is closely matched to the distribution pe so that the ratio pepg does not have wide variations. in practice suitable analytically specified importance sampling distributions cannot readily be found for the kinds of complex models considered in this book. an alternative approach is therefore to use the samples obtained from a markov chain to define the importance-sampling distribution. if the transition probability for the markov chain is given by t and the sample set is given by zl then the sampling distribution can be written as exp gz zg which can be used directly in t z methods for estimating the ratio of two partition functions require for their success that the two corresponding distributions be reasonably closely matched. this is especially problematic if we wish to find the absolute value of the partition function for a complex distribution because it is only for relatively simple distributions that the partition function can be evaluated directly and so attempting to estimate the ratio of partition functions directly is unlikely to be successful. this problem can be tackled using a technique known as chaining barber and bishop which involves introducing a succession of intermediate distributions pm that interpolate between a simple distribution for which we can evaluate the normalization coefficient and the desired complex distribution pm we then have zm zm zm in which the intermediate ratios can be determined using monte carlo methods as discussed above. one way to construct such a sequence of intermediate systems is to use an energy function containing a continuous parameter that interpolates between the two distributions e em if the intermediate ratios in are to be found using monte carlo it may be more efficient to use a single markov chain run than to restart the markov chain for each ratio. in this case the markov chain is run initially for the system and then after some suitable number of steps moves on to the next distribution in the sequence. note however that the system must remain close to the equilibrium distribution at each stage. sampling methods exercises www show that the finite sample defined by has mean equal to ef and variance given by suppose that z is a random variable with uniform distribution over and where hy is given by show that y that we transform z using y h has the distribution py. given a random variable z that is uniformly distributed over find a trans formation y fz such that y has a cauchy distribution given by suppose that and are uniformly distributed over the unit circle as shown in figure and that we make the change of variables given by and show that will be distributed according to www let z be a d-dimensional random variable having a gaussian distribution with zero mean and unit covariance matrix and suppose that the positive definite symmetric matrix has the cholesky decomposition llt where l is a lowertriangular matrix one with zeros above the leading diagonal. show that the variable y lz has a gaussian distribution with mean and covariance this provides a technique for generating samples from a general multivariate gaussian using samples from a univariate gaussian having zero mean and unit variance. www in this exercise we show more carefully that rejection sampling does indeed draw samples from the desired distribution pz. suppose the proposal distribution is qz and show that the probability of a sample value z being accepted is given is any unnormalized distribution that is proportional to pz and the constant k is set to the smallest value that ensures kqz for all values of z. note that the probability of drawing a value z is given by the probability of drawing that value from qz times the probability of accepting that value given that it has been drawn. make use of this along with the sum and product rules of probability to write down the normalized form for the distribution over z and show that it equals pz. suppose that z has a uniform distribution over the interval show that the variable y b tan z c has a cauchy distribution given by determine expressions for the coefficients ki in the envelope distribution for adaptive rejection sampling using the requirements of continuity and normalization. by making use of the technique discussed in section for sampling from a single exponential distribution devise an algorithm for sampling from the piecewise exponential distribution defined by show that the simple random walk over the integers defined by and has the property that ez ez and hence by induction that ez figure a probability distribution over two variables and that is uniform over the shaded regions and that is zero everywhere else. exercises www show that the gibbs sampling algorithm discussed in section satisfies detailed balance as defined by consider the distribution shown in figure discuss whether the standard gibbs sampling procedure for this distribution is ergodic and therefore whether it would sample correctly from this distribution consider the simple graph shown in figure in which the observed node x is given by a gaussian distribution n with mean and precision suppose that the marginal distributions over the mean and precision are given by n and gam b where gam denotes a gamma distribution. write down expressions for the conditional distributions p and p that would be required in order to apply gibbs sampling to the posterior distribution p verify that the over-relaxation update in which zi has mean i and i with variance i and where has zero mean and unit variance gives a value z mean i and variance i www using and show that the hamiltonian equation is equivalent to similarly using show that is equivalent to by making use of and show that the conditional dis tribution prz is a gaussian. figure a graph involving an observed gaussian variable x with prior distributions over its mean and precision x sampling methods www verify that the two probabilities and are equal and hence that detailed balance holds for the hybrid monte carlo algorithm. appendix a in chapter we discussed probabilistic models having discrete latent variables such as the mixture of gaussians. we now explore models in which some or all of the latent variables are continuous. an important motivation for such models is that many data sets have the property that the data points all lie close to a manifold of much lower dimensionality than that of the original data space. to see why this might arise consider an artificial data set constructed by taking one of the off-line digits represented by a x pixel grey-level image and embedding it in a larger image of size x by padding with pixels having the value zero to white pixels in which the location and orientation of the digit is varied at random as illustrated in figure each of the resulting images is represented by a point in the x ooo-dimensional data space. however across a data set of such images there are only three degrees offreedom of variability corresponding to the vertical and horizontal translations and the rotations. the data points will therefore live on a subspace of the data space whose intrinsic dimensionality is three. note continuous latent variables figure a synthetic data sel obtained by taking one of the off-line digit images and creating ple copies in each of which the digit has undergone a random displacement and rotation within some larger image field. the resulting images each have pixels. that the manifold will be nonlinear because. for instance. if we translate the digit past a particular pixel that pixel value will go from zero one and back to zero again. which is clearly a nonlinear function of the digit position. in this example. lranslation and rotation parameters are latent variables because we observe only the image vectors and are not told which values of the translation or rotation variables were used to create them. for real digit image data there will be a funher degree of freedom arising from scaling. moreover there will be multiple addilional degrees of freedom associaled wilh more complex deformations due to the variability in an individuals wriling well as lhe differences in writing slyles between individuals. evenheless. the number of such degrees of freedom will be small compared to the dimensionality of ihe data set. another example is provided by the oil flow data set. in which a given geometrical configuration of the gas woller and oil phases there are only two degrees of freedom of variability corresponding to the fraction of oil in the pipe and the tion of water fraction of gas ihen being determined. ahhough the data space comprises measuremenls a data set of points will lie close to a iwo-dimensional manifold embedded within this space. in this case the manifold comprises scveral distinct segments corresponding to different flow regimes. each such segment being a continuous two-dimensional manifold. if our goal is data compression. or density modelling then there can be benefits in exploiling this manifold struclure. the data points will not be confined precisely to a smooth dimensional manifold and we can interpret the departures of data points from the manifold as noise. this leads naturally to a generative view of such models in which we first select a poinl within the manifold according to some latent variable distribution and then generate an observed data point by noise drawn from some conditional distribution of the data varillbles given the latent varillbles. in praclice. thc simplest continuous latent variable model assumes gaussian distributions for both thc latent and observed variables and makes use of a lineargaussian dependence of the observed variables on ihe slate of the latent variables. this leads to a probabilislic fonnulation of the well-known technique of principal component analysis as well as a related model called factor analysis. in this chapter w will begin wilh a slandard nonprobabilistic treatment of pea. and thcn we show how pea arises naturally as the maximum likelihood solution appendix a section section principal analjsis pifcipal compooont a seeks dimensionality. ktwil as plno pal subspace denoted ijy the magenta line. such itlet the grthogonet data points doisl onto tpns the varia... of projated points dois. an pca is based on m..mizing the squares of projection errors. indcated by the bfi.e lines. scrio sdi a panlcula fonn of linear-gauian latem model. this probabilistic mulation bring many adimlags suh as tll use if em for parameter eslimalion rrinciplej ctensioos oliturc of pea model and basian formulatons that allow tbe number of rrincipal comoncnts to be detennined aulomatically from data. finally. dislus briefly gencraliation of the latent yariable concept that gl beood tbe linear-gaussian assumption including non gauin i.tcnt abies lea to tbe fr.me....ork of indrlmj.m compon.nl anal-.. as a models haing a nonlinear rclationship bet latent and ooseej principal component analysis principal compooem analy or rca.s a technique tha! is ued for appli. cations such as dimensionality lossy data comprcion feature etracti. and data vualizatioll its also kno.... as tile karoan.n i.. tran f. lbcrc an t....o commonly used definitions of pea that giye rise to the algorithm. pea can be defined as the unhoglnal projtttion of the data a lo....er dimensionallincar space. kno....n as the prilcip.al p.aa. soch that the of the projttted data i maimiej equialemlyt can be defined as tbe linear projection that minimi. the average projttlion cost. defined as t mean squa.-ed distance the data and tbeir pojtttioo the ljocs of onhogonal projection i illustraled in figute we conider each of these definitions in tum. mllximllm variance lormulation conider a dala set obserlations where s and x i a euclidean variable dimenionality d. our goal is to project if data onto a haing dimenionality m d hile the of the projttted data. for the we assume that tbe of m is g en. latcr in this continuous latent variables chapter we shall consider techniques to determine an appropriate value of iv! from the data. to begin with consider the projection onto a one-dimensional space we can define the direction of this space using a d-dimensional vector ul which for convenience without loss of generality we shall choose to be a unit vector so that uf ul that we are only interested in the direction defined by ul not in the magnitude of ul itself. each data point x n is then projected onto a scalar value uf x n the mean of the projected data is ufx where x is the sample set mean given by and the variance of the projected data is given by where s is the data covariance matrix defined by s xxn xt n nlj nl appendix e we now maximize the projected variance ufsul with respect to ul. clearly this has to be a constrained maximization to prevent ilulll the appropriate constraint comes from the normalization condition uf ul to enforce this constraint we introduce a lagrange multiplier that we shall denote by ai and then make an unconstrained maximization of by setting the derivative with respect to ul equal to zero we see that this quantity will have a stationary point when which says that ul must be an eigenvector of s. if we left-multiply by uf and make use of uf ul we see that the variance is given by and so the variance will be a maximum when we set ul equal to the eigenvector having the largest eigenvalue ai. this eigenvector is known as the first principal component. we can define additional principal components in an incremental fashion by choosing each new direction to be that which maximizes the projected variance exercise section appendix c principal component analysis amongst all possible directions orthogonal to those already considered. if we sider the general case of an m projection space the optimal linear jection for which the variance of the projected data is maximized is now defined by the m eigenvectors u u m of the data covariance matrix s corresponding to the m largest eigenvalues this is easily shown using proof by induction. to summarize principal component analysis involves evaluating the mean x and the covariance matrix s of the data set and then finding the m eigenvectors of s corresponding to the m largest eigenvalues. algorithms for finding eigenvectors and eigenvalues as well as additional theorems related to eigenvector decomposition can be found in golub and van loan note that the computational cost of computing the full eigenvector decomposition for a matrix of size d x dis if we plan to project our data onto the first m principal components then we only need to find the first m eigenvalues and eigenvectors. this can be done with more efficient techniques such as the power method and van loan that scale like omd or alternatively we can make use of the em algorithm. minimum-error formulation we now discuss an alternative formulation of pea based on projection error minimization. to do this we introduce a complete orthonormal set of d-dimensional basis vectors where i d that satisfy because this basis is complete each data point can be represented exactly by a linear combination of the basis vectors d x n laniui il where the coefficients ani will be different for different data points. this simply corresponds to a rotation of the coordinate system to a new system defined by the and the original d components xnd are replaced by an equivalent set taking the inner product with uj and making use of the thonormality property we obtain anj xuj and so without loss of generality we can write d x n l ui il our goal however is to approximate this data point using a representation volving a restricted number m d of variables corresponding to a projection onto a lower-dimensional subspace. the m linear subspace can be sented without loss of generality by the first m of the basis vectors and so we approximate each data point x n by m xn l d zniui l biui il iml continuous latent variables where the depend on the particular data point whereas the are constants that are the same for all data points. we are free to choose the the and the so as to minimize the distortion introduced by the reduction in ity. as our distortion measure we shall use the squared distance between the original data point x n and its approximation xn averaged over the data set so that our goal is to minimize n j l ilxn xn nl consider first of all the minimization with respect to the quantities stituting for xn setting the derivative with respect to znj to zero and making use of the orthonormality conditions we obtain where j similarly setting the derivative of j with respect to bi to zero and again making use of the orthonormality relations gives where j m if we substitute for zni and bi and make use of the general expansion we obtain b j x uj x n x n l x n xtud ui d iml from which we see that the displacement vector from x n to xn lies in the space orthogonal to the principal subspace because it is a linear combination of for i m d as illustrated in figure this is to be expected because the projected points xn must lie within the principal subspace but we can move them freely within that subspace and so the minimum error is given by the orthogonal projection. we therefore obtain an expression for the distortion measure j as a function purely of the in the form j n l l l u i sui. t d x n ui x ui nl iml iml there remains the task of minimizing j with respect to the which must be a constrained minimization otherwise we will obtain the vacuous result ui o. the constraints arise from the orthonormality conditions and as we shall see the solution will be expressed in terms of the eigenvector expansion of the covariance matrix. before considering a formal solution let us try to obtain some intuition about the result by considering the case of a two-dimensional data space d and a dimensional principal subspace m we have to choose a direction so as to principal component analysis minimize j subject to the normalization constraint ui using a lagrange multiplier to enforce the constraint we consider the minimization of setting the derivative with respect to to zero we obtain so that is an eigenvector of s with eigenvalue thus any eigenvector will define a tionary point of the distortion measure. to find the value of j at the minimum we back-substitute the solution for into the distortion measure to give j we therefore obtain the minimum value of j by choosing to be the eigenvector sponding to the smaller of the two eigenvalues. thus we should choose the principal subspace to be aligned with the eigenvector having the larger eigenvalue. this result accords with our intuition that in order to minimize the average squared projection distance we should choose the principal component subspace to pass through the mean of the data points and to be aligned with the directions of maximum variance. for the case when the eigenvalues are equal any choice of principal direction will give rise to the same value of j. the general solution to the minimization of j for arbitrary d and arbitrary m d is obtained by choosing the to be eigenvectors of the covariance matrix given by where i and as usual the eigenvectors are chosen to be mal. the corresponding value of the distortion measure is then given by sui aiui d j l ai iml which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal to the principal subspace. we therefore obtain the minimum value of j by selecting these eigenvectors to be those having the d m smallest eigenvalues and hence the eigenvectors defining the principal subspace are those corresponding to the m largest eigenvalues. although we have considered m d the pca analysis still holds if m d in which case there is no dimensionality reduction but simply a rotation of the coordinate axes to align with principal components. finally it is worth noting that there exists a closely related linear dimensionality reduction technique called canonical correlation analysis or cca bach and jordan whereas pca works with a single random variable cca considers two more variables and tries to find a corresponding pair of linear subspaces that have high cross-correlation so that each component within one of the subspaces is correlated with a single component from the other subspace. its solution can be expressed in terms of a generalized eigenvector problem. applications of pea we can illustrate the use of pca for data compression by considering the line digits data set. because each eigenvector of the covariance matrix is a vector exercise appendix a coltinuolis latfit figure the mean x aklog with iit lou pca egerrveclrll ul. cligits data set. tgetller with correspondi. lor the the d-limensional space. we can represent tho eigenwctos as imago of tho same silo as data poi_ first ihe along wich tlo sponding are in figure a plo! ofllo complete spectm uf oigo alue. sone! into decreasing order. is shown in figure the ditortion measure j assqciated wilh chooing a particular value of m is gi.en by tho sum of the eig.nlues from m i up to and is pto!ted for different of in figure if and into we can write the ica appro imation to a data x i the fonn lxu i m xlxu-xtuu m to fiiiure piol at ejoinv.loo lor the off digits data set sum at the which s.um-ol sqes distortlon j i by projectixl the data onto a pincipal componenl slllspaee dimensionalitv m. anal fiiiur. an irom lie digils data with its pea reonstnxlions oblair...! by jincipal various val as increason reonstuctiofi more aourate and woukl portee! when k d x ai a seeron where we hae made moe of the relation x l u which follow. from the completene of the i thi. represent. a contpreioo the data ilttaue for each data poim we ha.. repla d the v dimensiooal x wilh an having componem x. the of m. the greater the degree of comp.-eion. example. of pea of data points for the digits data set are shown in figure anolher application of priocipal compcmenl analyi. i to data pre-processing. in thi case lhe goal is no! dimensionality but rather the tmnformmion of a data sel in ork to standalli.e eenain of ils pmpenies. this can be inportanl in allowing pallem algorithm. be applied successfully the data typically. il is done wilen the original are meaured in dif. ferent unil or significantly difterent for instance in the old faithful data sel. the time betv.-een eruption. i. typicany an order of greater than lhe durali of.n erupt. when we applied the algorill thi data set first made a separ.te linear re-sealing of the individual socb thm each had zero mean and unit llus is known as slllnlardiv the dota. and the coanance matrix for lhe dala has components where is the of this i known as the matri. of the original dota and ha the propeny thai if to rompooent x and x of the data are correl.ted. then ai i. nd if they a.-e uocorrelated. then ai o. using pea we can make a itofe subst.mial nonnalizatoo of the data to gic it zero mean and unit co ariance. so that different become latel to do this. we first the equation in the form su ul continuous latent variables b o tj cpo o figure illustration of the effects of linear pre-processing applied to the old faithful data set. the plot on the left shows the original data. the centre plot shows the result of standardizing the individual variables to zero mean and unit variance. also shown are the principal axes of this normalized data set plotted over the range the plot on the right shows the result of whitening of the data to give it zero mean and unit covariance. where l is a d x d diagonal matrix with elements ai and u is a d x d onal matrix with columns given by ui. then we define for each data point x n a transformed value given by where x is the sample mean defined by clearly the set has zero mean and its covariance is given by the identity matrix because n l l xxn l l i. nl appendix a appendix a this operation is known as whitening or sphereing the data and is illustrated for the old faithful data set in figure it is interesting to compare pca with the fisher linear discriminant which was discussed in section both methods can be viewed as techniques for linear dimensionality reduction. however pca is unsupervised and depends only on the values x n whereas fisher linear discriminant also uses class-label information. this difference is highlighted by the example in figure another common application of principal component analysis is to data ization. here each data point is projected onto a two-dimensional principal subspace so that a data point x n is plotted at cartesian coordinates given by xj. and xj. where ul and are the eigenvectors corresponding to the largest and second largest eigenvalues. an example of such a plot for the oil flow data set is shown in figure iincipal clm anals fig a comparison proipal mnt analysis fishas linaar discriminant ality rduclion. here too data in two dimansions belonging to two classes in red and blue. is to be pfoicled onto a s.ingle di mension. pca cxlsas the direc tion maximum variae. try tha co. leads to strong class overlap. whereas fislef iimar discfornillant takes too class labels and leads to a projection onto the gean cum! giving much tetler class separation fig visualilatlon oilllow liet obtained try projoecting the onto the lirst two prin. cipal the blue and points corre-spond to and flow oonligurations pea for high-dimensional data in some application. of plitlcipal component analysis. the number of data points is smaller than t!c dimensionality of troe data foi example. might want to apply pea to a data of a few hundred images each of rorrespooos to a in a of poientially million dimensiolls tn thfle enlour for each of the pi.ls in troe image noie that in a d-limenional space a set of jy points. n d. defines a linear subspae dimensinality is at n and so there is linle point in applying pea for of m thai greater than n indeed if pelf pea we will find that at least d n i of the eigen.lues art lero. eorrespnnding tq eigenvectors aloog direclioos the data has varianee. funhemore. typical algolithm for finding the eigeneet of a d x d matrix hae a computatiooal eosl thm scales like o dj. aoo so for appliealions such as the image eample. a direc application of pea will be computatiooally infe-sibje. w. can resohe this problem as foil firl. let us define x to be the dj i continuous latent variables dimensional centred data matrix whose nth row is given by n xt. the ance matrix can then be written as s n- and the corresponding eigenvector equation becomes t xui aiui n now pre-multiply both sides by x to give nxx aixui t if we now define vi xui we obtain t vi aivi n which is an eigenvector equation for the n x n matrix n- we see that this has the same n eigenvalues as the original covariance matrix itself has an additional d n eigenvalues of value zero. thus we can solve the eigenvector problem in spaces of lower dimensionality with computational cost instead of od in order to determine the eigenvectors we multiply both sides of by x t to give t nx x vi aix vi t t from which we see that is an eigenvector of s with eigenvalue ai. note however that these eigenvectors need not be normalized. to determine the ate normalization we re-scale ui ex x tvi by a constant such that ilui ii which assuming vi has been normalized to unit length gives ui x vi t in summary to apply this approach we first evaluate xxt and then find its vectors and eigenvalues and then compute the eigenvectors in the original data space using probabilistic pea the formulation of pca discussed in the previous section was based on a linear projection of the data onto a subspace of lower dimensionality than the original data space. we now show that pca can also be expressed as the maximum likelihood solution of a probabilistic latent variable model. this reformulation of pca known as probabilistic pea brings several advantages compared with conventional pca probabilistic pca represents a constrained form of the gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. section probabilistic pea we can derive an em algorithm for pca that is computationally efficient in situations where only a few leading eigenvectors are required and that avoids having to evaluate the data covariance matrix as an intermediate step. the combination of a probabilistic model and em allows us to deal with ing values in the data set. mixtures of probabilistic pca models can be formulated in a principled way and trained using the em algorithm. section probabilistic pca forms the basis for a bayesian treatment of pca in which the dimensionality of the principal subspace can be found automatically from the data. the existence of a likelihood function allows direct comparison with other probabilistic density models. by contrast conventional pca will assign a low reconstruction cost to data points that are close to the principal subspace even if they lie arbitrarily far from the training data. probabilistic pca can be used to model class-conditional densities and hence be applied to classification problems. the probabilistic pca model can be run generatively to provide samples from the distribution. this formulation of pca as a probabilistic model was proposed independently by tipping and bishop and by roweis as we shall see later it is closely related to factor analysis probabilistic pca is a simple example of the linear-gaussian framework in which all of the marginal and conditional distributions are gaussian. we can late probabilistic pca by first introducing an explicit latent variable z corresponding to the principal-component subspace. next we define a gaussian prior distribution pz over the latent variable together with a gaussian conditional distribution pxl z for the observed variable x conditioned on the value of the latent variable. cally the prior distribution over z is given by a zero-mean unit-covariance gaussian pz nzio i. similarly the conditional distribution of the observed variable x conditioned on the value of the latent variable z is again gaussian of the form pxlz nxlwz j-l a in which the mean of x is a general linear function of z governed by the d x m matrix wand the d-dimensional vector j-l. note that this factorizes with respect to the elements of x in other words this is an example of the naive bayes model. as we shall see shortly the columns of w span a linear subspace within the data space that corresponds to the principal subspace. the other parameter in this model is the scalar a governing the variance of the conditional distribution. note that there is no section section continuous lat!nt flgu.. in oilte iifative viw potabi!st pea modeifof two-dimensiooal space and a space an oberved point x is generated by first drawing a value i fof vafiatlle prior distt p and itlen drawing a val fof x lrom an isofopk gaussian distrt by the red ciries having mean wi and the lfer ellips. show l!le density the pix. loss of gerajity in assuming a zero mean. unit coariance gauian for the latent distributin iiz because a more gcneral diributin would gie rise to an equivalent probabiliic nodel. we can view the probabilistic pea model from a geoeratie in a sampled of the obyed is obiained by first chooing a for the latent aod then the ooej cooditioned on this lao tent specifically the v-dimenional ooed x is defined by a lin ea tranformati of the dimeninal latcnt z plu additi-e gaussian that w!ere z is an m-dinsional gaussian lalent variable. and is a v dimensinal gau..ian-distributed noi.. witb co-ariance this generative process is illustrated in figure noie that this frame.-orl is based on a mapping from latent data space. in contrast the nll ica dis.cusd alxe mapping from data space to the latent space. he oolained using ha ycs lhwnm. suf!llosc we wish detenine the ofllo parameters i and using maximum likelihuol to write lhe likeliltood function we need an for tlo marginal distributioo p of tlo this is exprt__sed. fmn the sum aod poduct rules in the form ee-ise llaus this corresponds to a linear gauin thi marginal ditribulion is again gaussian. atld is given by nxllfc probabilistic pea where the d x d covariance matrix c is defined by c wwt this result can also be derived more directly by noting that the predictive distribution will be gaussian and then evaluating its mean and covariance using this gives iex covx iewz jl e jl ie ewz et ie ieeet wwt where we have used the fact that z and e are independent random variables and hence are uncorrelated. intuitively we can think of the distribution px as being defined by taking an isotropic gaussian can and moving it across the principal subspace spraying and weighted by the prior distribution. gaussian ink with density determined by accumulated ink density gives rise to a shaped distribution ing the marginal density px. the predictive distribution px is governed by the parameters jl w and however there is redundancy in this parameterization corresponding to rotations of the latent space coordinates. to see this consider a matrix w wr where r is an orthogonal matrix. using the orthogonality property rrt i we see that the quantity wwt that appears in the covariance matrix c takes the form and hence is independent of r. thus there is a whole family of matrices w all of which give rise to the same predictive distribution. this invariance can be understood in terms of rotations within the latent space. we shall return to a discussion of the number of independent parameters in this model later. when we evaluate the predictive distribution we require c- which involves the inversion of a d x d matrix. the computation required to do this can be reduced by making use of the matrix inversion identity to give c- t where the m x m matrix m is defined by m wtw because we invert m rather than inverting c directly the cost of evaluating c- is reduced from to as well as the predictive distribution px we will also require the posterior distributionpzlx which can again be written down directly using the result for linear-gaussian models to give note that the posterior mean depends on x whereas the posterior covariance is dependent of x. exercise continuous latent variables figure the probabilistic pea model for a data set of n vations of x can be expressed as a directed graph in which each observation x n is associated with a value zn of the latent variable. n maximum likelihood pea we next consider the determination of the model parameters using maximum likelihood. given a data set x of observed data points the probabilistic pea model can be expressed as a directed graph as shown in figure the corresponding log likelihood function is given from by inpxijl wo n l ln pxn nl n n nd ln ie l..xn jl c- jl. t nl setting the derivative of the log likelihood with respect to jl equal to zero gives the expected result jl x where x is the data mean defined by back-substituting we can then write the log likelihood function in the form inpxiw jl in ie tr n where s is the data covariance matrix defined by because the log likelihood is a quadratic function of jl this solution represents the unique maximum as can be confirmed by computing second derivatives. maximization with respect to w and is more complex but nonetheless has an exact closed-form solution. it was shown by tipping and bishop that all of the stationary points of the log likelihood function can be written as where u m is a d x m matrix whose columns are given by any subset size m of the eigenvectors of the data covariance matrix s the m x m diagonal matrix l m has elements given by the corresponding eigenvalues and r is an arbitrary m x m orthogonal matrix. furthermore tipping and bishop showed that the maximum of the lihood function is obtained when the m eigenvectors are chosen to be those whose eigenvalues are the m largest other solutions being saddle points. a similar sult was conjectured independently by roweis although no proof was given. probabilistic pea again we shall assume that the eigenvectors have been arranged in order of ing values of the corresponding eigenvalues so that the m principal eigenvectors are ul um. in this case the columns of w define the principal subspace of dard pca. the corresponding maximum likelihood solution for is then given by d-m l ai d iml so that is the average variance associated with the discarded dimensions. because r is orthogonal it can be interpreted as a rotation matrix in the m x m latent space. if we substitute the solution for w into the expression for c and make use of the orthogonality property rrt i we see that c is independent of r. this simply says that the predictive density is unchanged by rotations in the latent space as discussed earlier. for the particular case of r i we see that the columns of w are the principal component eigenvectors scaled by the variance parameters ai the interpretation of these scaling factors is clear once we recognize that for a convolution of independent gaussian distributions this case the latent space distribution and the noise model the variances are additive. thus the variance ai in the direction of an eigenvector ui is composed of the sum of a contribution ai from the projection of the unit-variance latent space distribution into data space through the corresponding column of w plus an isotropic contribution of variance which is added in all directions by the noise model. it is worth taking a moment to study the form of the covariance matrix given by consider the variance of the predictive distribution along some direction specified by the unit vector v where vtv which is given by vtcv. first suppose that v is orthogonal to the principal subspace in other words it is given by some linear combination of the discarded eigenvectors. then v tv and hence v tcv thus the model predicts a noise variance orthogonal to the principal subspace which from is just the average of the discarded eigenvalues. now suppose that v ui where ui is one of the retained eigenvectors defining the cipal subspace. then vtcv ai. in other words this model correctly captures the variance of the data along the principal axes and approximates the variance in all remaining directions with a single average value one way to construct the maximum likelihood density model would simply be to find the eigenvectors and eigenvalues of the data covariance matrix and then to evaluate wand using the results given above. in this case we would choose r i for convenience. however if the maximum likelihood solution is found by numerical optimization of the likelihood function for instance using an algorithm such as conjugate gradients nocedal and wright bishop and nabney or through the em algorithm then the resulting value of r is sentially arbitrary. this implies that the columns of w need not be orthogonal. if an orthogonal basis is required the matrix w can be post-processed appropriately and van loan alternatively the em algorithm can be modified in such a way as to yield orthonormal principal directions sorted in descending order of the corresponding eigenvalues directly and oh section continuous latent variables the rotational invariance in latent space represents a form of statistical tifiability analogous to that encountered for mixture models in the case of discrete latent variables. here there is a continuum of parameters all of which lead to the same predictive density in contrast to the discrete nonidentifiability associated with component re-labelling in the mixture setting. if we consider the case of m d so that there is no reduction of ality then u m u and l m l. making use of the orthogonality properties uut i and rrt i we see that the covariance c of the marginal distribution for x becomes and so we obtain the standard maximum likelihood solution for an unconstrained gaussian distribution in which the covariance matrix is given by the sample ance. conventional pca is generally formulated as a projection of points from the dimensional data space onto an m linear subspace. probabilistic pca however is most naturally expressed as a mapping from the latent space into the data space via for applications such as visualization and data compression we can reverse this mapping using bayes theorem. any point x in data space can then be summarized by its posterior mean and covariance in latent space. from the mean is given by where m is given by this projects to a point in data space given by wlezlx j-l. section note that this takes the same form as the equations for regularized linear regression and is a consequence of maximizing the likelihood function for a linear gaussian model. similarly the posterior covariance is given from by and is independent of x. if we take the limit then the posterior mean reduces to exercise exercise section which represents an orthogonal projection of the data point onto the latent space and so we recover the standard pca model. the posterior covariance in this limit is the latent projection zero however and the density becomes singular. for shifted towards the origin relative to the orthogonal projection. finally we note that an important role for the probabilistic pca model is in defining a multivariate gaussian distribution in which the number of degrees of dom in other words the number of independent parameters can be controlled whilst still allowing the model to capture the dominant correlations in the data. recall that a general gaussian distribution has dd independent parameters in its covariance matrix another d parameters in its mean. thus the number of parameters scales quadratically with d and can become excessive in spaces of high probabilistic pea dimensionality. if we restrict the covariance matrix to be diagonal then it has only d independent parameters and so the number of parameters now grows linearly with dimensionality. however it now treats the variables as if they were independent and hence can no longer express any correlations between them. probabilistic pea vides an elegant compromise in which the m most significant correlations can be captured while still ensuring that the total number of parameters grows only linearly with d. we can see this by evaluating the number of degrees of freedom in the ppca model as follows. the covariance matrix c depends on the parameters w giving a total parameter count of dm however which has size d x m and a we have seen that there is some redundancy in this parameterization associated with rotations of the coordinate system in the latent space. the orthogonal matrix r that expresses these rotations has size m x m. in the first column of this matrix there are m independent parameters because the column vector must be normalized to unit length. in the second column there are m independent parameters because the column must be normalized and also must be orthogonal to the previous column and so on. summing this arithmetic series we see that r has a total of mm independent parameters. thus the number of degrees of freedom in the covariance matrix c is given by dm mm exercise section section the number of independent parameters in this model therefore only grows linearly with d for fixed m. if we take m d then we recover the standard result for a full covariance gaussian. in this case the variance along d linearly dependent directions is controlled by the columns of w and the variance along the remaining direction is given by a if m the model is equivalent to the isotropic covariance case. em algorithm for pea as we have seen the probabilistic pca model can be expressed in terms of a marginalization over a continuous latent space z in which for each data point x n there is a corresponding latent variable zn. we can therefore make use of the em algorithm to find maximum likelihood estimates of the model parameters. this may seem rather pointless because we have already obtained an exact closed-form lution for the maximum likelihood parameter values. however in spaces of high dimensionality there may be computational advantages in using an iterative em procedure rather than working directly with the sample covariance matrix. this em procedure can also be extended to the factor analysis model for which there is no closed-form solution. finally it allows missing data to be handled in a principled way. we can derive the em algorithm for probabilistic pca by following the general framework for em. thus we write down the complete-data log likelihood and take its expectation with respect to the posterior distribution of the latent distribution evaluated using parameter values. maximization of this expected data log likelihood then yields the parameter values. because the data points continuous latent variables are assumed independent the complete-data log likelihood function takes the form inp zijl w l lnpzn n nl where the nth row of the matrix z is given by zn. we already know that the exact maximum likelihood solution for jl is given by the sample mean x defined by and it is convenient to substitute for jl at this stage. making use of the expressions and for the latent and conditional distributions respectively and ing the expectation with respect to the posterior distribution over the latent variables we obtain note that this depends on the posterior distribution only through the sufficient tics of the gaussian. thus in the e step we use the old parameter values to evaluate x leznleznt which follow directly from the posterior distribution together with the dard result leznz covzn jeznjeznt. here m is defined by in the m step we maximize with respect to wand keeping the posterior statistics fixed. maximization with respect to is straightforward. for the mization with respect to w we make use of and obtain the m-step equations exercise w new nd l new nl x n the em algorithm for probabilistic pca proceeds by initializing the parameters and then alternately computing the sufficient statistics of the latent space posterior distribution using and in the e step and revising the parameter values using and in the m step. one of the benefits of the em algorithm for pca is computational efficiency for large-scale applications unlike conventional pca based on an probabilistic pea eigenvector decomposition of the sample covariance matrix the em approach is iterative and so might appear to be less attractive. however each cycle of the em algorithm can be computationally much more efficient than conventional pca in spaces of high dimensionality. to see this we note that the eigendecomposition of the covariance matrix requires od computation. often we are interested only in the first m eigenvectors and their corresponding eigenvalues in which case we can use algorithms that are however the evaluation of the covariance matrix itself takes computations where n is the number of data points. algorithms such as the snapshot method which assume that the eigenvectors are linear combinations of the data vectors avoid direct evaluation of the covariance matrix but are and hence unsuited to large data sets. the em algorithm described here also does not construct the covariance matrix explicitly. instead the most computationally demanding steps are those involving sums over the data set that are d m. for large d and m d this can be a significant saving compared to and can offset the iterative nature of the em algorithm. note that this em algorithm can be implemented in an on-line form in which each d-dimensional data point is read in and processed and then discarded before the next data point is considered. to see this note that the quantities evaluated in the e step m-dimensional vector and an m x m matrix can be computed for each data point separately and in the m step we need to accumulate sums over data points which we can do incrementally. this approach can be advantageous if both nand d are large. because we now have a fully probabilistic model for pca we can deal with missing data provided that it is missing at random by marginalizing over the tribution of the unobserved variables. again these missing values can be treated using the em algorithm. we give an example of the use of this approach for data visualization in figure another elegant feature ofthe em approach is that we can take the limit a corresponding to standard pca and still obtain a valid em-like algorithm from we see that the only quantity we need to compute in the estep is jezn. furthermore the m step is simplifie because m wtw. to emphasize the simplicity of the algorithm let us define x to be a matrix of size n x d whose nth row is given by the vector x n x and similarly define to be a matrix of size d x m whose nth row is given by the vector jezn. the estep of the em algorithm for pca then becomes o wold-lwdx and the m step takes the form w new xtotoot-l. again these can be implemented in an on-line form. these equations have a simple interpretation as follows. from our earlier discussion we see that the e step involves an orthogonal projection of the data points onto the current estimate for the principal subspace. correspondingly the m step represents a re-estimation of the principal contlnljoljs ihtiit vi riarles fig.. probabilistic pca visoozsbon a portion data setlo ihe einls the left.....nd plot oiiows ihe ioleoo mean oilhfi poims on lhe principal subspace. the hind plot is obtained by firsl ranlomly omitting variable and lhen usrlg em mndie i mi...... values. note i!iai data nos allea. one missing mea.uement but lhoallhe plot i. to lhe ona obtained witl miss.... vallll ewrrise subspace to minimize squared reoonslructioo error in the projetion are c.n. we ean ghe a physical analogy for this em algorithm. which is easily visualized for d and m cooider a collectioo nf data point twi dimension aod let tile u-dimensiunal principal subspace be represented by a rod. now atlach each data point to the nxi via a ooing hooie law energy i square of lile spring. length. in e we keep the nxi hed and allow the attachment point tn up and nxi a to minimize elly this cau. each attachment point position itself at the orthogonal pmjeclion of the csponding data point onto the nxi. in the m we keep the attachment poiol filed and then release tile nxi and allow it to me tile minimum energy posilion. e and m are then repeated until a cvergence cri.eri is a. is illuslrated in figure bayesian pea sj far in oilr diioo of pea. we have ihal tile for dlnenionalit of tile principal is gien in praclice. nlmt cooose a suilable according the application. for we geny choose whereas for oiher application the approrrialc choice for ma be less dea. one appmaoh i. pi the eigenalue for lhe data set. analog the example in figure for the off_line digits dala sci and look to see if lite eigei nmurally form two groups comprising a set of separated by a gap from a of relativel large indicating a natural cholcc fr ai in practice. such a gap i oflen seen o o o o flgu.. syntelic illustrating too em algorithm pca defined by and a data set x with the data points shown in l tlltm tim pmdpal as scaled by ite squafll the eigejllllluel. initial configurat too principalsulslat defined by w shown in md toolhfir with the fkijeclions the points z inlo too space giitoo by zwt shown in cyan alter m step too laten! sib l pas been update! wiih z rell nxed. me tte success.... e slep ite z havu been ihogoooal rrojecliqn with w hk! fixed. aft.... tile seoll m s!flp. after l!ie mcl e stl suion i.j beau th pmxlhi lilic pea modd has a well defined likelillood fflction we employ to delermine the of dinsiooa!ity by tit larget log likelihood a data set such an opprooch. hov. can become computationally rolly. if we cqnsid probabilistic mixlure of pea modds and bishop. in we seek the appropriate dimenionalily for toch componenl in mixm gi-en thai w. ha-e a probabilislic formulalion of pea il s ms natural s k u buyeian approach model seleclion. to do thi. nee! marginalize the model paramele und wilh to appropriate prior distribution. this can be done by uing a framework to the allulylic.lly intractable murginaliuoioo marginal likelihood v.lues. given by ttle bour.d cun lhen be cmpund for a r.nge of different ar.d itie giving iht largest marginal likelihood we consider. simpler approach introducoo by b.ased on the rddmu continuous latent variables figure probabilistic graphical model for bayesian pea in which the distribution over the parameter matrix w is governed by a vector a of hyperparameters. w n proximation which is appropriate when the number of data points is relatively large and the corresponding posterior distribution is tightly peaked it involves a specific choice of prior over w that allows surplus dimensions in the principal subspace to be pruned out of the model. this corresponds to an example of automatic relevance determination or ard discussed in section specifically we define an independent gaussian prior over each column of w which represent the vectors defining the principal subspace. each such gaussian has an independent variance governed by a precision hyperparameter oi so that where wi is the i th column of w. the resulting model can be represented using the directed graph shown in figure the values for oi will be found iteratively by maximizing the hood function in which w has been integrated out. as a result of this optimization some of the oi may be driven to infinity with the corresponding parameters tor wi being driven to zero posterior distribution becomes a delta function at the origin giving a sparse solution. the effective dimensionality of the principal subspace is then determined by the number of finite oi values and the ing vectors wi can be thought of as for modelling the data distribution. in this way the bayesian approach is automatically making the trade-off between improving the fit to the data by using a larger number of vectors wi with their responding eigenvalues ai each tuned to the data and reducing the complexity of the model by suppressing some of the wi vectors. the origins of this sparsity were discussed earlier in the context of relevance vector machines. the values of oi are re-estimated during training by maximizing the log marginal likelihood given by pxla j-l jpxiw j-l dw where the log ofpxiw j-l is given by note that for simplicity we also treat j-l and as parameters to be estimated rather than defining priors over these parameters. section probabilistic pea section section because this integration is intractable we make use of the laplace tion. if we assume that the posterior distribution is sharply peaked as will occur for sufficiently large data sets then the re-estimation equations obtained by maximizing the marginal likelihood with respect to ai take the simple form which follows from noting that the dimensionality of wi is d. these estimations are interleaved with the em algorithm updates for determining wand a the e-step equations are again given by and similarly the step equation for a is again given by the only change is to the m-step equation for w which is modified to give where a diagai the value of i- is given by the sample mean as before. if we choose m d then if all ai values are finite the model represents a full-covariance gaussian while if all the ai go to infinity the model is equivalent to an isotropic gaussian and so the model can encompass all pennissible values for the effective dimensionality of the principal subspace. it is also possible to consider smaller values of m which will save on computational cost but which will limit the maximum dimensionality of the subspace. a comparison of the results of this algorithm with standard probabilistic pca is shown in figure bayesian pca provides an opportunity to illustrate the gibbs sampling rithm discussed in section figure shows an example of the samples from the hyperparameters in ai for a data set in d dimensions in which the mensionality of the latent space is m but in which the data set is generated from a probabilistic pca model having one direction of high variance with the remaining directions comprising low variance noise. this result shows clearly the presence of three distinct modes in the posterior distribution. at each step of the iteration one of the hyperparameters has a small value and the remaining two have large values so that two of the three latent variables are suppressed. during the course of the gibbs sampling the solution makes sharp transitions between the three modes. the model described here involves a prior only over the matrix w. a fully bayesian treatment of pca including priors over a and n and solved ing variational methods is described in bishop for a discussion of ous bayesian approaches to detennining the appropriate dimensionality for a pca model see minka factor analysis factor analysis is a linear-gaussian latent variable model that is closely related to probabilistic pca. its definition differs from that of probabilistic pca only in that the conditional distribution of the observed variable x given the latent variable z is continuous latent variables figure diagrams of the matrix w in which each element the matrix is depicted as a square lor positive and black lor negative values whose area is proportional to the magnitude of that element. the synthetic data sel comprises data points in d dimensions sampled from a gaussian distribution having standard deviation in directions and standard deviation in the remaining directions for a data set in d dimensions having at directions with larger variance than the remaining directions. the left-hand plol shows the result irom maximum likelihood probabilistic pca and the left hand plot shows the corresponding resuft from bayesian pea. we see how the bayesian model is able to discover the appropriate dimensionality by suppressing the surplus degrees of freedom. taken to have a diagonal rather than an isotropic covariance so that pxlz nxlwz where ill is a d x d diagonal matrix. note that the factor analysis model in common with probabilistic pca. assumes that the observed variables xl are dent. given the latent variable z. in essence. the factor analysis model is explaining the observed covariance structure of the data by representing the independent ance associated with each coordinate in the matrix and capturing the covariance between variables in the matrix w. in the factor analysis literature. the columns of w. which capture the correlations between observed variables. are calledfaclor loadings. and the diagonal elements of which represent the independent noise variances for each of the variables are called llniqllenesses. the origins of factor analysis are as old as those of pca. and discussions of factor analysis can be found in the books by everitt bartholomew and basilevsky links between factor analysis and pca were investigated by lilwley and anderson who showed that at stationary points of the likelihood function. for a faclor analysis model with the columns of w are scaled eigenvectors of the sample covariance matrix. and is the average of the discarded eigenvalues. later. tipping and bishop showed that the maximum of the log likelihood function occurs when the eigenvectors comprising ware chosen to be the principal eigenvectors. making use of we see that the marginal distribution for the observed ica gillbs baylslan pca shming plots oj ino versus number br three showing trtions tbe th moots posterior distribution. betw..... values. eurre sna i gi-n by n xlj c whe now cwwti. as with probabilistic pc a thi momi is im-ri.rrl to in latent histoocally factor anals has been lhe of wroe a!tempt h-e bttn to place an intcptlioo on the indvidual faclon coofdinates in z_space. which pmen problematic due to lr.e of factof analysis associmed with mation in this from oor perspeoh-e howe-er. we shall factor analysis as a form of lalent densily model. in which the form of tlc lalent i of interest but no! the particular choicc of coordinates used to descritc il. if we wish to remove the degeneracy asociated with latent roiations. mut conider non-gaussian latent ditribution. giirrg rise independent component models. we can detennie the parameters i. in the fac!of an.lyi model by muimum likelihood. solution for i i agin given by the how eyc. probabilitic lca.lllcre i no longer a closed-form maximum likelihood solution for mu.ltherdorc be found because faclor anali. is a latent variable model thi can be don. using an em algorilhm and thayer. is to the one used pmllbili.tie pea. specihcally. lhe e-lep eqnjtioo are g-en by ezoj gwt-xn xl ezzj g ezoezt where he defid noie tht thi i e.preed in a for thai in-ohes inycrsin of mal rices silo i x rathelhan d x d for tbe d x d diagooal matrix oj in-erse i. continuous latent variables exercise to compute in od steps which is convenient because often m d. similarly the m-step equations take the form w new diags-w.w xl where the operator sets all of the nondiagonal elements of a matrix to zero. a bayesian treatment of the factor analysis model can be obtained by a straightforward application of the techniques discussed in this book. another difference between probabilistic pca and factor analysis concerns their different behaviour under transformations of the data set. for pca and tic pca if we rotate the coordinate system in data space then we obtain exactly the same fit to the data but with the w matrix transformed by the corresponding rotation matrix. however for factor analysis the analogous property is that if we make a component-wise re-scaling of the data vectors then this is absorbed into a corresponding re-scaling of the elements of exercise kernel pea in chapter we saw how the technique of kernel substitution allows us to take an algorithm expressed in terms of scalar products of the form x t x and generalize that algorithm by replacing the scalar products with a nonlinear kernel. here we apply this technique of kernel substitution to principal component analysis thereby obtaining a nonlinear generalization called kernel pea et al. consider a data set of observations where n n in a space of dimensionality d. in order to keep the notation uncluttered we shall assume that we have already subtracted the sample mean from each of the vectors x n so that ln x n o. the first step is to express conventional pca in such a form that the data vectors n appear only in the form of the scalar products x x m recall that the principal components are defined by the eigenvectors ui of the covariance matrix where i here the d x d sample covariance matrix s is defined by sui aiui and the eigenvectors are normalized such that ut ui now consider a nonlinear transformation into an m feature space so that each data point x n is thereby projected onto a point we can kmci lco. sctiematic kernel pea. a hi in lhe oflglnal space l_ plot pfoleled by tranllklfmalion fa.tur. space plot. by ib pca in the we oblaoilha pmeiilai tnt ie in blue... by lha plolklio. onio lhe iirsl poiridl poofcllu. in oillll hole iiuiiin gmefm la nol pox poi v tha gr-. in imiun apam indicma iha ptrform pea ill fnlllk lopice. liiils model ill onpnll cbuo as in fllift princlpai lei los!oulllt illt diu. lobo halnro mean fu ji l. o. we dwl itl pol co-.ullcc mmfu l by l c and opanion i lined by cv av m. our goal is soh lhis eigenlilue problem wilhoul hainlllo work in f.liture from definilion of c. lhe equal ions lell u thai y l v a tilt v is lin by ii_ romblllaon of illt d j..... jo he iillhc l v continuous latent variables substituting this expansion back into the eigenvector equation we obtain n n l nl n l n aim ai l ml nl ain the key step is now to express this in terms of the kernel function kxn x m which we do by multiplying both sides by to give n m n lkxixn l nl ml aimkxnxm ai lainkxixn n nl this can be written in matrix notation as where ai is an n-dimensional column vector with elements ani for n we can find solutions for ai by solving the following eigenvalue problem exercise in which we have removed a factor of k from both sides of note that the solutions of and differ only by eigenvectors of k having zero eigenvalues that do not affect the principal components projection. the normalization condition for the coefficients ai is obtained by requiring that the eigenvectors in feature space be normalized. using and we have vvi l l n n nl ml ainaim ak ainaai having solved the eigenvector problem the resulting principal component jections can then also be cast in terms of the kernel function so that using the projection of a point x onto eigenvector i is given by yix l ain l ainkx x n n n nl nl and so again is expressed in terms of the kernel function. in the original d-dimensional x space there are d orthogonal eigenvectors and hence we can find at most d linear principal components. the dimensionality m of the feature space however can be much larger than d infinite and thus we can find a number of nonlinear principal components that can exceed d. note however that the number of nonzero eigenvalues cannot exceed the number n of data points because if m n the covariance matrix in feature space has rank at most equal to n. this is reflected in the fact that kernel pca involves the eigenvector expansion of the n x n matrix k. kernel pca so far we have assumed that the projected data set given by has zero mean which in general will not be the case. we cannot simply compute and then subtract off the mean since we wish to avoid working directly in feature space and so again we formulate the algorithm purely in-!erms of the kernel function. the projected data points after centralizing denoted are given by and the corresponding elements of the gram matrix are given by k nm n n l zl n n n n l ll zl jl zl n kxn x m n l kxz x m zl n lkxnxz llkxjxl n n jl n zl this can be expressed in matrix notation as exercise where in denotes the n x n matrix in which every element takes the value ln. thus we can evaluate k using only the kernel function and then use k to determine the eigenvalues and eigenvectors. note that the standard pca algorithm is recovered as a special case if we use a linear kernel kx x xtx. figure shows an example of kernel pca applied to a synthetic data set et al. here a kernel of the form kx x exp-llx is applied to a synthetic data set. the lines correspond to contours along which the projection onto the corresponding principal component defined by n l nl ainkx x n is constant. continuous latent valuables figure ellmple kernel pca with a gaussian kernel awiioo a synthetic sat in two showing firsl flight eigenfunclions along wh l!eir the oootours am lines along which onlo t coffaspmding principal costam nola haw ihe firsl two th dusters. ill spiii oilhe eluste into hamos. and t loliowing ihree again spi!he duste into halves along directions orthogonal tho premous splils one obvioo dls.ajmotaeof iemel is thaf if invohes finding lhe tors of the n x n malri k raw. ihan lhe d x d malri s of cor..emionallinear and!io in for large data approlmation are often usd finally. ooie that i linear ica we often retain some redocel num ber l dof eigenvectors and then approlmale data vttlr xn b its projection i lhe l-dimensional principal subspace defined by i-l i kernellca. this will in not be floslble to see thl ooie ihat the ping maps the d-dimensional x space it d-dimensioo.l manijqiii in lhe m-dimemioo.l femure space tlie x i koown a lhe f.imagr of lhe cponding poil however fhe of poinl in feature the linear rca in that will typically lie on fhe nonlinear dimensional manifold and will nul ha.. a cpondlng p.lmoein dolo spa.c technlque ho.-e lherefore bttn proposed for finding approximale pre-image iblr nat.. nonlinear latent variable models nonlinear latent variable models in this chapter we have focussed on the simplest class of models having continuous latent variables namely those based on linear-gaussian distributions. as well as having great practical importance these models are relatively easy to analyse and to fit to data and can also be used as components in more complex models. here we consider briefly some generalizations of this framework to models that are either nonlinear or non-gaussian or both. in fact the issues of nonlinearity and non-gaussianity are related because a general probability density can be obtained from a simple fixed reference density such as a gaussian by making a nonlinear change of variables. this idea forms the basis of several practical latent variable models as we shall see shortly. exercise independent component analysis we begin by considering models in which the observed variables are related linearly to the latent variables but for which the latent distribution is non-gaussian. an important class of such models known as independent component analysis or lea arises when we consider a distribution over the latent variables that factorizes so that m pz iipzj. jl to understand the role of such models consider a situation in which two people are talking at the same time and we record their voices using two microphones. if we ignore effects such as time delay and echoes then the signals received by the microphones at any point in time will be given by linear combinations of the amplitudes of the two voices. the coefficients of this linear combination will be constant and if we can infer their values from sample data then we can invert the mixing process it is nonsingular and thereby obtain two clean signals each of which contains the voice of just one person. this is an example of a problem called blind source separation in which refers to the fact that we are given only the mixed data and neither the original sources nor the mixing coefficients are observed this type of problem is sometimes addressed using the following approach in which we ignore the temporal nature of the signals and treat the successive samples as i.i.d. we consider a generative model in which there are two latent variables corresponding to the unobserved speech signal amplitudes and there are two observed variables given by the signal values at the microphones. the latent variables have a joint distribution that factorizes as above and the observed variables are given by a linear combination of the latent variables. there is no need to include a noise distribution because the number of latent variables equals the number of served variables and therefore the marginal distribution of the observed variables will not in general be singular so the observed variables are simply deterministic linear combinations of the latent variables. given a data set of observations the continuous latent variables likelihood function for this model is a function of the coefficients in the linear bination. the log likelihood can be maximized using gradient-based optimization giving rise to a particular version of independent component analysis. the success of this approach requires that the latent variables have non-gaussian distributions. to see this recall that in probabilistic pca in factor analysis the latent-space distribution is given by a zero-mean isotropic gaussian. the model therefore cannot distinguish between two different choices for the latent variables where these differ simply by a rotation in latent space. this can be verified directly by noting that the marginal density and hence the likelihood function is unchanged if we make the transformation w wr where r is an orthogonal matrix satisfying rrt i because the matrix c given by is itself invariant. extending the model to allow more general gaussian latent distributions does not change this conclusion because as we have seen such a model is equivalent to the zero-mean isotropic gaussian latent variable model. another way to see why a gaussian latent variable distribution in a linear model is insufficient to find independent components is to note that the principal nents represent a rotation of the coordinate system in data space such as to ize the covariance matrix so that the data distribution in the new coordinates is then uncorrelated. although zero correlation is a necessary condition for independence it is not however sufficient. in practice a common choice for the latent-variable distribution is given by exercise pz j which has heavy tails compared to a gaussian reflecting the observation that many real-world distributions also exhibit this property. the original ica model and sejnowski was based on the tion of an objective function defined by information maximization. one advantage of a probabilistic latent variable formulation is that it helps to motivate and late generalizations of basic ica. for instance independent factor analysis considers a model in which the number of latent and observed variables can differ the observed variables are noisy and the individual latent variables have ible distributions modelled by mixtures of gaussians. the log likelihood for this model is maximized using em and the reconstruction of the latent variables is proximated using a variational approach. many other types of model have been considered and there is now a huge literature on ica and its applications and herault comon et at. amari et at. pearlmutter and parra hyvarinen and oja hinton et at. miskin and mackay hojen-sorensen et at. choudrey and roberts chan et at. stone autoassociative neural networks in chapter we considered neural networks in the context of supervised ing where the role of the network is to predict the output variables given values nonlinear latent variable models figure an autoassociative multilayer perceptron having two layers of weights. such a network is trained to map input vectors onto themselves by tion ot a sum-ot-squares error. even with linear units in the hidden layer such a network is equivalent to linear principal component ysis. links representing bias parameters have been omitted for clarity. inputs outputs for the input variables. however neural networks have also been applied to supervised learning where they have been used for dimensionality reduction. this is achieved by using a network having the same number of outputs as inputs and optimizing the weights so as to minimize some measure of the reconstruction error between inputs and outputs with respect to a set of training data. consider first a multilayer perceptron of the form shown in figure ing d inputs d output units and m hidden units with m d. the targets used to train the network are simply the input vectors themselves so that the network is attempting to map each input vector onto itself. such a network is said to form an autoassociative mapping. since the number of hidden units is smaller than the number of inputs a perfect reconstruction of all input vectors is not in general sible. we therefore determine the network parameters w by minimizing an error function which captures the degree of mismatch between the input vectors and their reconstructions. in particular we shall choose a sum-of-squares error of the form n ew l ilyxn w xn nl if the hidden units have linear activations functions then it can be shown that the error function has a unique global minimum and that at this minimum the network performs a projection onto the m subspace which is spanned by the first m principal components of the data and kamp baldi and hornik thus the vectors of weights which lead into the hidden units in figure form a basis set which spans the principal subspace. note however that these tors need not be orthogonal or normalized. this result is unsurprising since both principal component analysis and the neural network are using linear dimensionality reduction and are minimizing the same sum-of-squares error function. it might be thought that the limitations of a linear dimensionality reduction could be overcome by using nonlinear activation functions for the hidden units in the network in figure however even with nonlinear hidden units the imum error solution is again given by the projection onto the principal component subspace and kamp there is therefore no advantage in using layer neural networks to perform dimensionality reduction. standard techniques for principal component analysis on singular value decomposition are teed to give the correct solution in finite time and they also generate an ordered set of eigenvalues with corresponding orthonormal eigenvectors. continuous latent variables figure addition of extra hidden ers of noolinear units gives an auloassocialive network which can perform a noolinear siooality reduction. f f inputs x outputs x the situation is different however. if additional hidden layers are pcrmillcd in the network. consider the four-layer autoassociativc network shown in figure again the output units are linear and the m units in the second hidden layer can also be linear. however the first and third hidden layers have sigmoidal nonlinear tion functions. the network is again trained by minimization of the error function we can view this network as two successive functional mappings f and f as indicated in figure the first mapping f projects the original dimensional data onto an ai-dimensional subspace s defined by the activations of the units in the second hidden layer. because of the presence of the first hidden layer of nonlinear units. this mapping is very general. and in particular is not restricted to being linear. similarly. the second half of the network defines an arbitrary functional mapping from the m space back into the original d-dimensional input space. this has a simple geometrical interpretation. as indicated for the case d and m in figure such a network effectively perfonns a nonlinear principal component analysis. x figure geometrical interpretation of the mappings performed by the network in figure g for the case of inputs and ai units in the middle hidden layer. the function f maps from an m-dimensional space s into a d-dimensiooal space and therefore defines the way in which the space s is embedded within the original x-space. since the mapping f can be rillinear the embedding s can be nonplanar as indicated in the figure. the mapping f. then defines a projectiorl of points in the original d-dimensional space into the m subspace s. nonlinear latent variable models it has the advantage of not being limited to linear transformations although it tains standard principal component analysis as a special case. however training the network now involves a nonlinear optimization problem since the error function is no longer a quadratic function of the network parameters. ally intensive nonlinear optimization techniques must be used and there is the risk of finding a suboptimal local minimum of the error function. also the dimensionality of the subspace must be specified before training the network. modelling nonlinear manifolds as we have already noted many natural sources of data correspond to dimensional possibly noisy nonlinear manifolds embedded within the higher mensional observed data space. capturing this property explicitly can lead to proved density modelling compared with more general methods. here we consider briefly a range of techniques that attempt to do this. one way to model the nonlinear structure is through a combination of linear models so that we make a piece-wise linear approximation to the manifold. this can be obtained for instance by using a clustering technique such as k based on euclidean distance to partition the data set into local groups with standard pca plied to each group. a better approach is to use the reconstruction error for cluster assignment and leen hinton et al. as then a common cost function is being optimized in each stage. however these approaches still suffer from limitations due to the absence of an overall density model. by using abilistic pca it is straightforward to define a fully probabilistic model simply by considering a mixture distribution in which the components are probabilistic pca models and bishop such a model has both discrete latent ables corresponding to the discrete mixture as well as continuous latent variables and the likelihood function can be maximized using the em algorithm. a fully bayesian treatment based on variational inference and winn allows the number of components in the mixture as well as the effective dimensionalities of the individual models to be inferred from the data. there are many variants of this model in which parameters such as the w matrix or the noise variances are tied across components in the mixture or in which the isotropic noise distributions are replaced by diagonal ones giving rise to a mixture of factor analysers and hinton ghahramani and beal the mixture of probabilistic pca models can also be extended hierarchically to produce an interactive data tion algorithm and tipping an alternative to considering a mixture of linear models is to consider a single nonlinear model. recall that conventional pca finds a linear subspace that passes close to the data in a least-squares sense. this concept can be extended to dimensional nonlinear surfaces in the form of principal curves and stuetzle we can describe a curve in a d-dimensional data space using a vector-valued function f which is a vector each of whose elements is a function of the scalar there are many possible ways to parameterize the curve of which a natural choice is the arc length along the curve. for any given point xin data space we can find the point on the curve that is closest in euclidean distance. we denote this point by continuous latent variables gfx because it depends on the particular curve f. for a continuous data density px a principal curve is defined as one for which every point on the curve is the mean of all those points in data space that project to it so that je f. for a given continuous density there can be many principal curves. in practice we are interested in finite data sets and we also wish to restrict attention to smooth curves. hastie and stuetzle propose a two-stage iterative procedure for ing such principal curves somewhat reminiscent of the em algorithm for pca. the curve is initialized using the first principal component and then the algorithm nates between a data projection step and curve re-estimation step. in the projection step each data point is assigned to a value of corresponding to the closest point on the curve. then in the re-estimation step each point on the curve is given by a weighted average of those points that project to nearby points on the curve with points closest on the curve given the greatest weight. in the case where the subspace is constrained to be linear the procedure converges to the first principal component and is equivalent to the power method for finding the largest eigenvector of the variance matrix. principal curves can be generalized to multidimensional manifolds called principal surfaces although these have found limited use due to the difficulty of data smoothing in higher dimensions even for two-dimensional manifolds. pca is often used to project a data set onto a lower-dimensional space for ample two dimensional for the purposes of visualization. another linear technique with a similar aim is multidimensional scaling or mds and cox it finds a low-dimensional projection of the data such as to preserve as closely as possible the pairwise distances between data points and involves finding the eigenvectors of the distance matrix. in the case where the distances are euclidean it gives equivalent results to pca. the mds concept can be extended to a wide variety of data types specified in terms of a similarity matrix giving nonmetric mds. two other nonprobabilistic methods for dimensionality reduction and data sualization are worthy of mention. locally linear embedding or lle and saul first computes the set of coefficients that best reconstructs each data point from its neighbours. these coefficients are arranged to be invariant to tions translations and scalings of that data point and its neighbours and hence they characterize the local geometrical properties of the neighbourhood. lle then maps the high-dimensional data points down to a lower dimensional space while if the local neighbourhood for a particular ing these neighbourhood coefficients. data point can be considered linear then the transformation can be achieved using a combination of translation rotation and scaling such as to preserve the angles formed between the data points and their neighbours. because the weights are variant to these transformations we expect the same weight values to reconstruct the data points in the low-dimensional space as in the high-dimensional data space. in spite of the nonlinearity the optimization for lle does not exhibit local minima. in isometric feature mapping or isomap et ai. the goal is to project the data to a lower-dimensional space using mds but where the ilarities are defined in terms of the geodesic distances measured along the mani nonlinear latent variable models fold. for instance if two points lie on a circle then the geodesic is the arc-length distance measured around the circumference of the circle not the straight line tance measured along the chord connecting them. the algorithm first defines the neighbourhood for each data point either by finding the k nearest neighbours or by finding all points within a sphere of radius e. a graph is then constructed by ing all neighbouring points and labelling them with their euclidean distance. the geodesic distance between any pair of points is then approximated by the sum of the arc lengths along the shortest path connecting them itself is found using standard algorithms. finally metric mds is applied to the geodesic distance matrix to find the low-dimensional projection. our focus in this chapter has been on models for which the observed ables are continuous. we can also consider models having continuous latent ables together with discrete observed variables giving rise to latent trait models in this case the marginalization over the continuous latent variables even for a linear relationship between latent and observed variables not be performed analytically and so more sophisticated techniques are required. tipping uses variational inference in a model with a two-dimensional latent space allowing a binary data set to be visualized analogously to the use of pca to visualize continuous data. note that this model is the dual of the bayesian logistic regression problem discussed in section in the case of logistic regression we have n observations of the feature vector which are parameterized by a single parameter vector w whereas in the latent space visualization model there is a single latent space variable x to and n copies of the latent variable w n a generalization of probabilistic latent variable models to general exponential family distributions is described in collins et al. we have already noted that an arbitrary distribution can be formed by taking a gaussian random variable and transforming it through a suitable nonlinearity. this is exploited in a general latent variable model called a density network mackay and gibbs in which the nonlinear function is governed by a multilayered neural network. if the network has enough hidden units it can imate a given nonlinear function to any desired accuracy. the downside of having such a flexible model is that the marginalization over the latent variables required in order to obtain the likelihood function is no longer analytically tractable. instead the likelihood is approximated using monte carlo techniques by drawing samples from the gaussian prior. the marginalization over the latent variables then becomes a simple sum with one term for each sample. however because a large number of sample points may be required in order to give an accurate representation of the marginal this procedure can be computationally costly. if we consider more restricted forms for the nonlinear function and make an propriate choice of the latent variable distribution then we can construct a latent able model that is both nonlinear and efficient to train. the generative topographic mapping or gtm et ai. bishop et ai. bishop et ai. uses a latent distribution that is defined by a finite regular grid of delta functions over the two-dimensional latent space. marginalization over the latent space then simply involves summing over the contributions from each of the grid locations. chapter chapter jj continuous latent va ot trle oillkyw wllisualiz.ed using pea on the left and gtm on itle ngrt fof tile gtm flliu.e model. each poinils plollfld at tile mean ot its posmk dislribution in sace tile mlhe gtm sepamlion betwoon the groups of data points to be ckl.arfy chllf j setioo the noliotar mapping is gien by a linear regression model thai allow for general iiollinearily while being a linear fuoction of tile adapli-e parameler noie thai tilt usual limitation of linear regression models arising from the en of dimeniooalily does arise in the of lhe gti sie the generall ha to diltn sions irrespecti-e of the dimensionality of the data space a coo!nce of illese cooices is that the likelihood funclion can be epressed analytically in dosed form and can be optimilc.! efficiently oing the em algorithm_ the resolting gtm model his a lwo-dimensional nonlinear manifold tile dala sel. and by ealualing the posterior distriljlion latent space for the data poi they can he projecttj back to the lalent for purposes figure sls a comparison of the oil wilh lincar pea and wilh lhe iiolhnear gti tilt gtm can be seen as a probabilistic of an earlier nlodl callm the orgnidng or som kobonen. which also represents a iwo-dimensiooal iiollincar manifoid as a regular array of discle points. the som i somewhat reminnt of the k trlcan algorithm in that data points are a.igr.ed to nearby prololjc thai are lhen subscjuenlly updale!. initially. lhe proioijls are distribuled at random and during the training process they organize so as to aplroimalea smoolh manifold. unlike k howee.. the som is tioi optimizing any well.ddine! cost function al.. making difficult to s. the parameters of the model and assess con-ergence. there i also no guarantee that the will take place this is depennl the choice of appropriate paranlttcr f any particular data sel. by oofitrast gtm optimize the log likelihood functioo and the resolting model define a probabilily denity in dma in fael il correponds to a conmincd miture of gaussian in which the component. a conlnlon nd the mean are contrained to lie on a tw-o-diitlcniooal this proba section exercises appendix e exercises bilistic foundation also makes it very straightforward to define generalizations of gtm et al. such as a bayesian treatment dealing with missing values a principled extension to discrete variables the use of gaussian processes to define the manifold or a hierarchical gtm model and nabney because the manifold in gtm is defined as a continuous surface not just at the prototype vectors as in the som it is possible to compute the magnification factors corresponding to the local expansions and compressions of the manifold needed to fit the data set et al. as well as the directional curvatures of the manifold et al. these can be visualized along with the projected data and provide additional insight into the model. lib in this exercise we use proof by induction to show that the linear projection onto an m subspace that maximizes the variance of the jected data is defined by the m eigenvectors of the data covariance matrix s given by corresponding to the m largest eigenvalues. in section this result was proven for the case of m now suppose the result holds for some general value of m and show that it consequently holds for dimensionality m to do this first set the derivative of the variance of the projected data with respect to a vector defining the new direction in data space equal to zero. this should be done subject to the constraints that um be orthogonal to the existing vectors um and also that it be normalized to unit length. use lagrange multipliers to enforce these constraints. then make use of the orthonormality properties of the vectors um to show that the new vector is an eigenvector of s. finally show that the variance is maximized if the eigenvector is chosen to be the one corresponding to eigenvector where the eigenvalues have been ordered in decreasing value. show that the minimum value of the pca distortion measure j given by with respect to the ui subject to the orthonormality constraints is obtained when the ui are eigenvectors of the data covariance matrix s. to do this introduce a matrix h of lagrange multipliers one for each constraint so that the modified distortion measure in matrix notation reads tr utsu tr hi utu where uis a mtrix of dimensiod x m whose columns are gi..en b ui. now minimize j with respect to u and show that the sution satisfies su uh. clearly one possible solution is that the columns of u are eigenvectors of s in which case h is a diagonal matrix containing the corresponding eigenvalues. to obtain the general solution show that h can be assumed to be a symmetr maix and by using its eigenvect r expansion show that the general solution to su gives the same value for j as the specific solution in which the columns of u are continuous latent variables the eigenvectors of s. because these solutions are all equivalent it is convenient to choose the eigenvector solution. verify that the eigenvectors defined by are normalized to unit length assuming that the eigenvectors vi have unit length. imm suppose we replace the zero-mean unit-covariance latent space bution in the probabilistic pca model by a general gaussian distribution of the formnzlm by redefining the parameters of the model show that this leads to an identical model for the marginal distribution px over the observed variables for any valid choice of m and let x be a d-dimensional random variable having a gaussian distribution given by nxijl and consider the m-dimensional random variable given by y ax b where a is an m x d matrix. show that y also has a gaussian distribution and find expressions for its mean and covariance. discuss the form of this gaussian distribution for m d for m d and for m d. imm draw a directed probabilistic graph for the probabilistic pca model described in section in which the components of the observed variable x are shown explicitly as separate nodes. hence verify that the probabilistic pca model has the same independence structure as the naive bayes model discussed in tion by making use of the results and for the mean and covariance of a general distribution derive the result for the marginal distribution px in the probabilistic pca model. by making use of the result show that the posterior distribution pzlx for the probabilistic pca model is given by verify that maximizing the log likelihood for the probabilistic pca model with respect to the parameter jl gives the result jlml x where x is the mean of the data vectors. by evaluating the second derivatives of the log likelihood function for the probabilistic pca model with respect to the parameter jl show that the stationary point jlml x represents the unique maximum. imm show that in the limit the posterior mean for the probabilistic pca model becomes an orthogonal projection onto the principal subspace as in conventional pca. for show that the posterior mean in the probabilistic pca model is shifted towards the origin relative to the orthogonal projection. show that the optimal reconstruction of a data point under probabilistic pca according to the least squares projection cost of conventional pca is given by exercises the number of independent parameters in the covariance matrix for the bilistic pca model with an m latent space and a d-dimensional data space is given by verify that in the case of m d the number of independent parameters is the same as in a general covariance gaussian whereas for m it is the same as for a gaussian with an isotropic covariance. iiii!i derive the m-step equations and for the probabilistic pca model by maximization of the expected complete-data log likelihood function given by in figure we showed an application of probabilistic pca to a data set in which some of the data values were missing at random. derive the em algorithm for maximizing the likelihood function for the probabilistic pca model in this ation. note that the as well as the missing data values that are components of the vectors n are now latent variables. show that in the special case in which all of the data values are observed this reduces to the em algorithm for probabilistic pca derived in section iiii!i let w be a d x m matrix whose columns define a linear subspace of dimensionality m embedded within a data space of dimensionality d and let be a d-dimensional vector. given a data set n where n n we can approximate the data points using a linear mapping from a set of m vectors so that xn is approximated by w zn the associated squares reconstruction cost is given by n j l ilxn wzn nl first show that minimizing j with respect to to an analogous expression with x n and zn replaced by zero-mean variables x n x and zn z respectively where x and z denote sample means. then show that minimizing j with respect to zn where w is kept fixed gives rise to the pca estep and that minimizing j with respect to w where is kept fixed gives rise to the pca m step derive an expression for the number of independent parameters in the factor analysis model described in section iiii!i show that the factor analysis model described in section is invariant under rotations of the latent space coordinates. by considering second derivatives show that the only stationary point of the log likelihood function for the factor analysis model discussed in section with respect to the parameter is given by the sample mean defined by furthermore show that this stationary point is a maximum. derive the formulae and for the e step of the em algorithm for factor analysis. note that from the result of exercise the parameter can be replaced by the sample mean x. continuous latent variables write down an expression for the expected complete-data log likelihood tion for the factor analysis model and hence derive the corresponding m step tions and iii!i draw a directed probabilistic graphical model representing a discrete mixture of probabilistic pca models in which each pca model has its own values of w jl and now draw a modified graph in which these parameter values are shared between the components of the mixture. we saw in section that students t-distribution can be viewed as an infinite mixture of gaussians in which we marginalize with respect to a ous latent variable. by exploiting this representation formulate an em algorithm for maximizing the log likelihood function for a multivariate students t-distribution given an observed set of data points and derive the forms of the e and m step tions. iii!i consider a linear-gaussian latent-variable model having a latent space distribution pz nxio i and a conditional distribution for the observed able pxlz nxlwz il where is an arbitrary symmetric definite noise covariance matrix. now suppose that we make a nonsingular linear transformation of the data variables x ax where a is a d x d matrix. if jlml w ml and represent the maximum likelihood solution corresponding to the original untransformed data show that ajlml awml and a will resent the corresponding maximum likelihood solution for the transformed data set. finally show that the form of the model is preserved in two cases a is a diagonal matrix and is a diagonal matrix. this corresponds to the case of factor analysis. the transformed remains diagonal and hence factor analysis is covariant under component-wise re-scaling of the data variables a is orthogonal and is this corresponds to probabilistic pca. portional to the unit matrix so that transformed matrix remains proportional to the unit matrix and hence bilistic pca is covariant under a rotation of the axes of data space as is the case for conventional pca. show that any vector ai that satisfies will also satisfy also show that for any solution of having eigenvalue a we can add any multiple of an eigenvector of k having zero eigenvalue and obtain a solution to that also has eigenvalue a. finally show that such modifications do not affect the principal-component projection given by show that the conventional linear pca algorithm is recovered as a special case of kernel pca if we choose the linear kernel function given by kx x x t x. iii!i use the transformation property of a probability density under a change of variable to show that any density py can be obtained from a fixed density qx that is everywhere nonzero by making a nonlinear change of variable y fx in which fx is a monotonic function so that jx write down the differential equation satisfied by f and draw a diagram illustrating the transformation of the density. exercises suppose that two variables zl and are independent so thatpzl show that the covariance matrix between these variables is diagonal. this shows that independence is a sufficient condition for two variables to be correlated. now consider two variables yl and in which yl and yg. write down the conditional distribution and observe that this is dependent on yb showing that the two variables are not independent. now show that the covariance matrix between these two variables is again diagonal. to do this use the relation pyl pyi to show that the off-diagonal terms are zero. this counter-example shows that zero correlation is not a sufficient condition for independence. sequential data so far in this book we have focussed primarily on sets of data points that were assumed to be independent and identically distributed this assumption allowed us to express the likelihood function as the product over all data points of the probability distribution evaluated at each data point. for many applications however the i.i.d. assumption will be a poor one. here we consider a particularly important class of such data sets namely those that describe sequential data. these often arise through measurement of time series for example the rainfall measurements on successive days at a particular location or the daily values of a currency exchange rate or the acoustic features at successive time frames used for speech recognition. an example involving speech data is shown in figure sequential data can also arise in contexts other than time series for example the sequence of nucleotide base pairs along a strand of dna or the sequence of characters in an english sentence. for convenience we shall sometimes refer to past and future observations in a sequence. however the models explored in this chapter are equally applicable to all sequential data figure example of a spectrogram of the spoken words bayes theorem showing a plot of the intensity of the spectral coefficients versus time index. forms of sequential data not just temporal sequences. it is useful to distinguish between stationary and nonstationary sequential distributions. in the stationary case the data evolves in time but the distribution from which it is generated remains the same. for the more complex nonstationary situation the generative distribution itself is evolving with time. here we shall focus on the stationary case. for many applications such as financial forecasting we wish to be able to predict the next value in a time series given observations of the previous values. intuitively we expect that recent observations are likely to be more informative than more historical observations in predicting future values. the example in figure shows that successive observations of the speech spectrum are indeed highly correlated. furthermore it would be impractical to consider a general dependence of future observations on all previous observations because the complexity of such a model would grow without limit as the number of observations increases. this leads us to consider markov models in which we assume that future predictions are inde markov models figure the simplest approach to modelling a sequence of observations is to treat them as independent corresponding to a graph without links. pendent of all but the most recent observations. although such models are tractable they are also severely limited. we can obtain a more general framework while still retaining tractability by the introduction of latent variables leading to state space models. as in chapters and we shall see that complex models can thereby be constructed from simpler components particular from distributions belonging to the exponential family and can be readily characterized using the framework of probabilistic graphical models. here we focus on the two most important examples of state space models namely the hidden markov model in which the latent variables are discrete and linear dynamical systems in which the latent variables are gaussian. both models are described by directed graphs having a tree structure loops for which inference can be performed efficiently using the sum-product algorithm. markov models the easiest way to treat sequential data would be simply to ignore the sequential aspects and treat the observations as i.i.d. corresponding to the graph in figure such an approach however would fail to exploit the sequential patterns in the data such as correlations between observations that are close in the sequence. suppose for instance that we observe a binary variable denoting whether on a particular day it rained or not. given a time series of recent observations of this variable we wish to predict whether it will rain on the next day. if we treat the data as i.i.d. then the only information we can glean from the data is the relative frequency of rainy days. however we know in practice that the weather often exhibits trends that may last for several days. observing whether or not it rains today is therefore of significant help in predicting if it will rain tomorrow. to express such effects in a probabilistic model we need to relax the i.i.d. assumption and one of the simplest ways to do this is to consider a markov model. first of all we note that without loss of generality we can use the product rule to express the joint distribution for a sequence of observations in the form xn xn if we now assume that each of the conditional distributions on the right-hand side is independent of all previous observations except the most recent we obtain the first-order markov chain which is depicted as a graphical model in figure the sequential data figure a first-order markov chain of observations in which the distribution pxnxn of a particular observation xn is conditioned on the value of the previous observation xn joint distribution for a sequence of n observations under this model is given by xn pxnxn section exercise from the d-separation property we see that the conditional distribution for observation xn given all of the observations up to time n is given by xn pxnxn which is easily verified by direct evaluation starting from and using the product rule of probability. thus if we use such a model to predict the next observation in a sequence the distribution of predictions will depend only on the value of the immediately preceding observation and will be independent of all earlier observations. in most applications of such models the conditional distributions pxnxn that define the model will be constrained to be equal corresponding to the assumption of a stationary time series. the model is then known as a homogeneous markov chain. for instance if the conditional distributions depend on adjustable parameters values might be inferred from a set of training data then all of the conditional distributions in the chain will share the same values of those parameters. although this is more general than the independence model it is still very restrictive. for many sequential observations we anticipate that the trends in the data over several successive observations will provide important information in predicting the next value. one way to allow earlier observations to have an influence is to move to higher-order markov chains. if we allow the predictions to depend also on the previous-but-one value we obtain a second-order markov chain represented by the graph in figure the joint distribution is now given by xn pxnxn xn again using d-separation or by direct evaluation we see that the conditional distribution of xn given xn and xn is independent of all observations xn figure a second-order markov chain in which the conditional distribution of a particular observation xn depends on the values of the two previous observations xn and xn figure we can represent sequential data using a markov chain of latent variables with each observation conditioned on the state of the corresponding latent variable. this important graphical structure forms the foundation both for the hidden markov model and for linear dynamical systems. markov models zn zn xn xn each observation is now influenced by two previous observations. we can similarly consider extensions to an m th order markov chain in which the conditional distribution for a particular variable depends on the previous m variables. however we have paid a price for this increased flexibility because the number of parameters in the model is now much larger. suppose the observations are discrete variables having k states. then the conditional distribution pxnxn in a first-order markov chain will be specified by a set of k parameters for each of the k states of xn giving a total of kk parameters. now suppose we extend the model to an m th order markov chain so that the joint distribution is built up from conditionals pxnxn m xn if the variables are discrete and if the conditional distributions are represented by general conditional probability tables then the number of parameters in such a model will have km parameters. because this grows exponentially with m it will often render this approach impractical for larger values of m. for continuous variables we can use linear-gaussian conditional distributions in which each node has a gaussian distribution whose mean is a linear function of its parents. this is known as an autoregressive or ar model et al. thiesson et al. an alternative approach is to use a parametric model for pxnxn m xn such as a neural network. this technique is sometimes called a tapped delay line because it corresponds to storing the previous m values of the observed variable in order to predict the next value. the number of parameters can then be much smaller than in a completely general model example it may grow linearly with m although this is achieved at the expense of a restricted family of conditional distributions. suppose we wish to build a model for sequences that is not limited by the markov assumption to any order and yet that can be specified using a limited number of free parameters. we can achieve this by introducing additional latent variables to permit a rich class of models to be constructed out of simple components as we did with mixture distributions in chapter and with continuous latent variable models in chapter for each observation xn we introduce a corresponding latent variable zn may be of different type or dimensionality to the observed variable. we now assume that it is the latent variables that form a markov chain giving rise to the graphical structure known as a state space model which is shown in figure it satisfies the key conditional independence property that zn and are independent given zn so that zn zn. sequential data the joint distribution for this model is given by xn zn pznzn pxnzn. using the d-separation criterion we see that there is always a path connecting any two observed variables xn and xm via the latent variables and that this path is never blocked. thus the predictive distribution xn for observation given all previous observations does not exhibit any conditional independence properties and so our predictions for depends on all previous observations. the observed variables however do not satisfy the markov property at any order. we shall discuss how to evaluate the predictive distribution in later sections of this chapter. there are two important models for sequential data that are described by this graph. if the latent variables are discrete then we obtain the hidden markov model or hmm et al. note that the observed variables in an hmm may be discrete or continuous and a variety of different conditional distributions can be used to model them. if both the latent and the observed variables are gaussian a linear-gaussian dependence of the conditional distributions on their parents then we obtain the linear dynamical system. section section hidden markov models the hidden markov model can be viewed as a specific instance of the state space model of figure in which the latent variables are discrete. however if we examine a single time slice of the model we see that it corresponds to a mixture distribution with component densities given by pxz. it can therefore also be interpreted as an extension of a mixture model in which the choice of mixture component for each observation is not selected independently but depends on the choice of component for the previous observation. the hmm is widely used in speech recognition rabiner and juang natural language modelling and sch utze on-line handwriting recognition et al. and for the analysis of biological sequences such as proteins and dna et al. durbin et al. baldi and brunak as in the case of a standard mixture model the latent variables are the discrete multinomial variables zn describing which component of the mixture is responsible for generating the corresponding observation xn. again it is convenient to use a coding scheme as used for mixture models in chapter we now allow the probability distribution of zn to depend on the state of the previous latent variable zn through a conditional distribution pznzn because the latent variables are k-dimensional binary variables this conditional distribution corresponds to a table of numbers that we denote by a the elements of which are known as transition probabilities. they are given by ajk pznk and because they are probabilities they satisfy ajk with k ajk so that the matrix a hidden markov models figure transition diagram showing a model whose latent variables have three possible states corresponding to the three boxes. the black lines denote the elements of the transition matrix ajk. k k k has kk independent parameters. we can then write the conditional distribution explicitly in the form pznzn zn znk jk a the initial latent node is special in that it does not have a parent node and so it has a marginal distribution represented by a vector of probabilities with elements k so that k where k k the transition matrix is sometimes illustrated diagrammatically by drawing the states as nodes in a state transition diagram as shown in figure for the case of k note that this does not represent a probabilistic graphical model because the nodes are not separate variables but rather states of a single variable and so we have shown the states as boxes rather than circles. section it is sometimes useful to take a state transition diagram of the kind shown in figure and unfold it over time. this gives an alternative representation of the transitions between latent states known as a lattice or trellis diagram and which is shown for the case of the hidden markov model in figure the specification of the probabilistic model is completed by defining the conditional distributions of the observed variables pxnzn where is a set of parameters governing the distribution. these are known as emission probabilities and might for example be given by gaussians of the form if the elements of x are continuous variables or by conditional probability tables if x is discrete. because xn is observed the distribution pxnzn consists for a given value of of a vector of k numbers corresponding to the k possible states of the binary vector zn. sequential data figure if we unfold the state transition diagram of figure over time we obtain a lattice or trellis representation of the latent states. each column of this diagram corresponds to one of the latent variables zn. k k k n n n n we can represent the emission probabilities in the form pxnzn pxn kznk we shall focuss attention on homogeneous models for which all of the conditional distributions governing the latent variables share the same parameters a and similarly all of the emission distributions share the same parameters extension to more general cases is straightforward. note that a mixture model for an i.i.d. data set corresponds to the special case in which the parameters ajk are the same for all values of j so that the conditional distribution pznzn is independent of zn this corresponds to deleting the horizontal links in the graphical model shown in figure the joint probability distribution over both latent and observed variables is then given by px z pznzn a pxmzm exercise where x xn z zn and a denotes the set of parameters governing the model. most of our discussion of the hidden markov model will be independent of the particular choice of the emission probabilities. indeed the model is tractable for a wide range of emission distributions including discrete tables gaussians and mixtures of gaussians. it is also possible to exploit discriminative models such as neural networks. these can be used to model the emission density pxz directly or to provide a representation for pzx that can be converted into the required emission density pxz using bayes theorem et al. we can gain a better understanding of the hidden markov model by considering it from a generative point of view. recall that to generate samples from a mixture of k k k hidden markov models figure illustration of sampling from a hidden markov model having a latent variable z and a gaussian emission model pxz where x is contours of constant probability density for the emission distributions corresponding to each of the three states of the latent variable. a sample of points drawn from the hidden markov model colour coded according to the component that generated them and with lines connecting the successive observations. here the transition matrix was fixed so that in any state there is a probability of making a transition to each of the other states and consequently a probability of remaining in the same state. gaussians we first chose one of the components at random with probability given by the mixing coefficients k and then generate a sample vector x from the corresponding gaussian component. this process is repeated n times to generate a data set of n independent samples. in the case of the hidden markov model this procedure is modified as follows. we first choose the initial latent variable with probabilities governed by the parameters k and then sample the corresponding observation now we choose the state of the variable according to the transition probabilities using the already instantiated value of thus suppose that the sample for corresponds to state j. then we choose the state k of with probabilities ajk for k k. once we know we can draw a sample for and also sample the next latent variable and so on. this is an example of ancestral sampling for a directed graphical model. if for instance we have a model in which the diagonal transition elements akk are much larger than the off-diagonal elements then a typical data sequence will have long runs of points generated from a single component with infrequent transitions from one component to another. the generation of samples from a hidden markov model is illustrated in figure there are many variants of the standard hmm model obtained for instance by imposing constraints on the form of the transition matrix a here we mention one of particular practical importance called the left-to-right hmm which is obtained by setting the elements ajk of a to zero if k j as illustrated in the section sequential data figure example of the state transition diagram for a left-to-right hidden markov model. note that once a state has been vacated it cannot later be re-entered. k k k state transition diagram for a hmm in figure typically for such models the initial state probabilities for are modified so that and for j in other words every sequence is constrained to start in state j the transition matrix may be further constrained to ensure that large changes in the state index do not occur so that ajk if k j this type of model is illustrated using a lattice diagram in figure many applications of hidden markov models for example speech recognition or on-line character recognition make use of left-to-right architectures. as an illustration of the left-to-right hidden markov model we consider an example involving handwritten digits. this uses on-line data meaning that each digit is represented by the trajectory of the pen as a function of time in the form of a sequence of pen coordinates in contrast to the off-line digits data discussed in appendix a which comprises static two-dimensional pixellated images of the ink. examples of the online digits are shown in figure here we train a hidden markov model on a subset of data comprising examples of the digit there are k states each of which can generate a line segment of fixed length having one of possible angles and so the emission distribution is simply a table of probabilities associated with the allowed angle values for each state index value. transition probabilities are all set to zero except for those that keep the state index k the same or that increment it by and the model parameters are optimized using iterations of em. we can gain some insight into the resulting model by running it generatively as shown in figure figure lattice diagram for a leftto-right hmm in which the state index k is allowed to increase by at most at each transition. k k k n n n n hidden markov models figure top row examples of on-line handwritten digits. bottom row synthetic digits sampled generatively from a left-to-right hidden markov model that has been trained on a data set of handwritten digits. one of the most powerful properties of hidden markov models is their ability to exhibit some degree of invariance to local warping and stretching of the time axis. to understand this consider the way in which the digit is written in the on-line handwritten digits example. a typical digit comprises two distinct sections joined at a cusp. the first part of the digit which starts at the top left has a sweeping arc down to the cusp or loop at the bottom left followed by a second moreor-less straight sweep ending at the bottom right. natural variations in writing style will cause the relative sizes of the two sections to vary and hence the location of the cusp or loop within the temporal sequence will vary. from a generative perspective such variations can be accommodated by the hidden markov model through changes in the number of transitions to the same state versus the number of transitions to the successive state. note however that if a digit is written in the reverse order that is starting at the bottom right and ending at the top left then even though the pen tip coordinates may be identical to an example from the training set the probability of the observations under the model will be extremely small. in the speech recognition context warping of the time axis is associated with natural variations in the speed of speech and again the hidden markov model can accommodate such a distortion and not penalize it too heavily. maximum likelihood for the hmm if we have observed a data set x xn we can determine the parameters of an hmm using maximum likelihood. the likelihood function is obtained from the joint distribution by marginalizing over the latent variables px px z z because the joint distribution px z does not factorize over n contrast to the mixture distribution considered in chapter we cannot simply treat each of the summations over zn independently. nor can we perform the summations explicitly because there are n variables to be summed over each of which has k states resulting in a total of k n terms. thus the number of terms in the summation grows sequential data exponentially with the length of the chain. in fact the summation in corresponds to summing over exponentially many paths through the lattice diagram in figure we have already encountered a similar difficulty when we considered the inference problem for the simple chain of variables in figure there we were able to make use of the conditional independence properties of the graph to re-order the summations in order to obtain an algorithm whose cost scales linearly instead of exponentially with the length of the chain. we shall apply a similar technique to the hidden markov model. a further difficulty with the expression for the likelihood function is that because it corresponds to a generalization of a mixture distribution it represents a summation over the emission models for different settings of the latent variables. direct maximization of the likelihood function will therefore lead to complex expressions with no closed-form solutions as was the case for simple mixture models that a mixture model for i.i.d. data is a special case of the hmm. section we therefore turn to the expectation maximization algorithm to find an efficient framework for maximizing the likelihood function in hidden markov models. the em algorithm starts with some initial selection for the model parameters which we denote by old. in the e step we take these parameter values and find the posterior distribution of the latent variables pzx old. we then use this posterior distribution to evaluate the expectation of the logarithm of the complete-data likelihood function as a function of the parameters to give the function q old defined by q old pzx old ln px z z at this point it is convenient to introduce some notation. we shall use to denote the marginal posterior distribution of a latent variable zn and zn to denote the joint posterior distribution of two successive latent variables so that pznx old zn pzn znx old. for each value of n we can store using a set of k nonnegative numbers that sum to unity and similarly we can store zn using a k k matrix of nonnegative numbers that again sum to unity. we shall also use to denote the conditional probability of znk with a similar use of notation for znk and for other probabilistic variables introduced later. because the expectation of a binary random variable is just the probability that it takes the value we have eznk znk ezn z if we substitute the joint distribution px z given by into z exercise exercise hidden markov models and make use of the definitions of and we obtain q old ln k znk ln ajk ln pxn k. the goal of the e step will be to evaluate the quantities and zn efficiently and we shall discuss this in detail shortly. in the m step we maximize q old with respect to the parameters a in which we treat and zn as constant. maximization with respect to and a is easily achieved using appropriate lagrange multipliers with the results k ajk znk znl the em algorithm must be initialized by choosing starting values for and a which should of course respect the summation constraints associated with their probabilistic interpretation. note that any elements of or a that are set to zero initially will remain zero in subsequent em updates. a typical initialization procedure would involve selecting random starting values for these parameters subject to the summation and non-negativity constraints. note that no particular modification to the em results are required for the case of left-to-right models beyond choosing initial values for the elements ajk in which the appropriate elements are set to zero because these will remain zero throughout. to maximize q old with respect to k we notice that only the final term in depends on k and furthermore this term has exactly the same form as the data-dependent term in the corresponding function for a standard mixture distribution for i.i.d. data as can be seen by comparison with for the case of a gaussian mixture. here the quantities are playing the role of the responsibilities. if the parameters k are independent for the different components then this term decouples into a sum of terms one for each value of k each of which can be maximized independently. we are then simply maximizing the weighted log likelihood function for the emission density px k with weights here we shall suppose that this maximization can be done efficiently. for instance in the case of sequential data gaussian emission densities we have px k n k k and maximization of the function q old then gives k kxn kt k exercise section for the case of discrete multinomial observed variables the conditional distribution of the observations takes the form pxz xizk ik and the corresponding m-step equations are given by ik an analogous result holds for bernoulli observed variables. the em algorithm requires initial values for the parameters of the emission distribution. one way to set these is first to treat the data initially as i.i.d. and fit the emission density by maximum likelihood and then use the resulting values to initialize the parameters for em. the forward-backward algorithm next we seek an efficient procedure for evaluating the quantities and znk corresponding to the e step of the em algorithm. the graph for the hidden markov model shown in figure is a tree and so we know that the posterior distribution of the latent variables can be obtained efficiently using a twostage message passing algorithm. in the particular context of the hidden markov model this is known as the forward-backward algorithm or the baum-welch algorithm there are in fact several variants of the basic algorithm all of which lead to the exact marginals according to the precise form of hidden markov models the messages that are propagated along the chain we shall focus on the most widely used of these known as the alpha-beta algorithm. as well as being of great practical importance in its own right the forwardbackward algorithm provides us with a nice illustration of many of the concepts introduced in earlier chapters. we shall therefore begin in this section with a conventional derivation of the forward-backward equations making use of the sum and product rules of probability and exploiting conditional independence properties which we shall obtain from the corresponding graphical model using d-separation. then in section we shall see how the forward-backward algorithm can be obtained very simply as a specific example of the sum-product algorithm introduced in section it is worth emphasizing that evaluation of the posterior distributions of the latent variables is independent of the form of the emission density pxz or indeed of whether the observed variables are continuous or discrete. all we require is the values of the quantities pxnzn for each value of zn for every n. also in this section and the next we shall omit the explicit dependence on the model parameters old because these fixed throughout. we therefore begin by writing down the following conditional independence properties pxzn xnzn pxn zn pxn pzn x pzn xnzn xn zn xn xn zn xn xnzn pxzn zn xn xnzn where x xn. these relations are most easily proved using d-separation. for instance in the first of these results we note that every path from any one of the nodes xn to the node xn passes through the node zn which is observed. because all such paths are head-to-tail it follows that the conditional independence property must hold. the reader should take a few moments to verify each of these properties in turn as an exercise in the application of d-separation. these relations can also be proved directly though with significantly greater effort from the joint distribution for the hidden markov model using the sum and product rules of probability. let us begin by evaluating recall that for a discrete multinomial random variable the expected value of one of its components is just the probability of that component having the value thus we are interested in finding the posterior distribution xn of zn given the observed data set xn this exercise sequential data represents a vector of length k whose entries correspond to the expected values of znk. using bayes theorem we have pznx pxznpzn px note that the denominator px is implicitly conditioned on the parameters old of the hmm and hence represents the likelihood function. using the conditional independence property together with the product rule of probability we obtain xn xnzn px px where we have defined xn zn xnzn. the quantity represents the joint probability of observing all of the given data up to time n and the value of zn whereas represents the conditional probability of all future data from time n up to n given the value of zn. again and each represent set of k numbers one for each of the possible settings of the coded binary vector zn. we shall use the notation to denote the value of when znk with an analogous interpretation of we now derive recursion relations that allow and to be evaluated efficiently. again we shall make use of conditional independence properties in particular and together with the sum and product rules allowing us to express in terms of as follows xn zn xnznpzn xn xn zn pxnzn xn zn zn zn zn zn zn pxnzn pxnzn pxnzn xn znzn xn xn zn making use of the definition for we then obtain pxnzn zn hidden markov models figure illustration of the forward recursion for evaluation of the variables. in this fragment of the lattice we see that the quantity is obtained by taking the elements of at step n and summing them up with weights given by corresponding to the values of pznzn and then multiplying by the data contribution k k k n n it is worth taking a moment to study this recursion relation in some detail. note that there are k terms in the summation and the right-hand side has to be evaluated for each of the k values of zn so each step of the recursion has computational cost that scaled like ok the forward recursion equation for is illustrated using a lattice diagram in figure in order to start this recursion we need an initial condition that is given by xnzn which tells us that for k k takes the value k. starting at the first node of the chain we can then work along the chain and evaluate for every latent node. because each step of the recursion involves multiplying by a k k matrix the overall cost of evaluating these quantities for the whole chain is of ok we can similarly find a recursion relation for the quantities by making use of the conditional independence properties and giving xnzn xn sequential data figure illustration of the backward recursion for evaluation of the variables. in this fragment of the lattice we see that the quantity is obtained by taking the components of at step n and summing them up with weights given by the products of corresponding to the values of and the corresponding values of the emission density k k k n n making use of the definition for we then obtain note that in this case we have a backward message passing algorithm that evaluates in terms of at each step we absorb the effect of observation through the emission probability multiply by the transition matrix and then marginalize out this is illustrated in figure again we need a starting condition for the recursion namely a value for this can be obtained by setting n n in and replacing with its definition to give pznx px zn px which we see will be correct provided we take for all settings of zn in the m step equations the quantity px will cancel out as can be seen for instance in the m-step equation for k given by which takes the form k however the quantity px represents the likelihood function whose value we typically wish to monitor during the em optimization and so it is useful to be able to evaluate it. if we sum both sides of over zn and use the fact that the left-hand side is a normalized distribution we obtain px zn hidden markov models thus we can evaluate the likelihood function by computing this sum for any convenient choice of n. for instance if we only want to evaluate the likelihood function then we can do this by running the recursion from the start to the end of the chain and then use this result for n n making use of the fact that is a vector of in this case no recursion is required and we simply have px zn let us take a moment to interpret this result for px. recall that to compute the likelihood we should take the joint distribution px z and sum over all possible values of z. each such value represents a particular choice of hidden state for every time step in other words every term in the summation is a path through the lattice diagram and recall that there are exponentially many such paths. by expressing the likelihood function in the form we have reduced the computational cost from being exponential in the length of the chain to being linear by swapping the order of the summation and multiplications so that at each time step n we sum the contributions from all paths passing through each of the states znk to give the intermediate quantities next we consider the evaluation of the quantities zn which correspond to the values of the conditional probabilities pzn znx for each of the k k settings for zn. using the definition of zn and applying bayes theorem we have zn pzn znx pxzn znpzn zn xn xnznpznzn px px px where we have made use of the conditional independence property together with the definitions of and given by and thus we can calculate the zn directly by using the results of the and recursions. let us summarize the steps required to train a hidden markov model using the em algorithm. we first make an initial selection of the parameters old where a the a and parameters are often initialized either uniformly or randomly from a uniform distribution their non-negativity and summation constraints. initialization of the parameters will depend on the form of the distribution. for instance in the case of gaussians the parameters k might be initialized by applying the k-means algorithm to the data and k might be initialized to the covariance matrix of the corresponding k means cluster. then we run both the forward recursion and the backward recursion and use the results to evaluate and zn. at this stage we can also evaluate the likelihood function. sequential data this completes the e step and we use the results to find a revised set of parameters new using the m-step equations from section we then continue to alternate between e and m steps until some convergence criterion is satisfied for instance when the change in the likelihood function is below some threshold. note that in these recursion relations the observations enter through conditional distributions of the form pxnzn. the recursions are therefore independent of the type or dimensionality of the observed variables or the form of this conditional distribution so long as its value can be computed for each of the k possible states of zn. since the observed variables are fixed the quantities pxnzn can be pre-computed as functions of zn at the start of the em algorithm and remain fixed throughout. exercise we have seen in earlier chapters that the maximum likelihood approach is most effective when the number of data points is large in relation to the number of parameters. here we note that a hidden markov model can be trained effectively using maximum likelihood provided the training sequence is sufficiently long. alternatively we can make use of multiple shorter sequences which requires a straightforward modification of the hidden markov model em algorithm. in the case of left-to-right models this is particularly important because in a given observation sequence a given state transition corresponding to a nondiagonal element of a will seen at most once. another quantity of interest is the predictive distribution in which the observed data is x xn and we wish to predict xn which would be important for real-time applications such as financial forecasting. again we make use of the sum and product rules together with the conditional independence properties and giving pxn px pxn zn pxn pxn pxn pzn znx pzn zn zn zn pxn pzn pzn x px pxn pzn zn which can be evaluated by first running a forward recursion and then computing the final summations over zn and zn the result of the first summation over zn can be stored and used once the value of xn is observed in order to run the recursion forward to the next step in order to predict the subsequent value xn figure a fragment of the factor graph representation for the hidden markov model. hidden markov models zn n zn gn gn xn xn note that in the influence of all data from to xn is summarized in the k values of thus the predictive distribution can be carried forward indefinitely using a fixed amount of storage as may be required for real-time applications. here we have discussed the estimation of the parameters of an hmm using maximum likelihood. this framework is easily extended to regularized maximum likelihood by introducing priors over the model parameters a and whose values are then estimated by maximizing their posterior probability. this can again be done using the em algorithm in which the e step is the same as discussed above and the m step involves adding the log of the prior distribution p to the function q old before maximization and represents a straightforward application of the techniques developed at various points in this book. furthermore we can use variational methods to give a fully bayesian treatment of the hmm in which we marginalize over the parameter distributions as with maximum likelihood this leads to a two-pass forward-backward recursion to compute posterior probabilities. the sum-product algorithm for the hmm the directed graph that represents the hidden markov model shown in figure is a tree and so we can solve the problem of finding local marginals for the hidden variables using the sum-product algorithm. not surprisingly this turns out to be equivalent to the forward-backward algorithm considered in the previous section and so the sum-product algorithm therefore provides us with a simple way to derive the alpha-beta recursion formulae. we begin by transforming the directed graph of figure into a factor graph of which a representative fragment is shown in figure this form of the factor graph shows all variables both latent and observed explicitly. however for the purpose of solving the inference problem we shall always be conditioning on the variables xn and so we can simplify the factor graph by absorbing the emission probabilities into the transition probability factors. this leads to the simplified factor graph representation in figure in which the factors are given by fnzn zn pznzn section section sequential data figure a simplified form of factor graph to describe the hidden markov model. h fn zn zn to derive the alpha-beta algorithm we denote the final hidden variable zn as the root node and first pass messages from the leaf node h to the root. from the general results and for message propagation we see that the messages which are propagated in the hidden markov model take the form zn fnzn fn zn fn znzn fnzn zn zn fnzn zn these equations represent the propagation of messages forward along the chain and are equivalent to the alpha recursions derived in the previous section as we shall now show. note that because the variable nodes zn have only two neighbours they perform no computation. sion for the f z messages of the form we can eliminate zn fnzn from using to give a recur fn znzn fnzn zn fn zn if we now recall the definition and if we define zn fn znzn then we obtain the alpha recursion given by we also need to verify that the quantities are themselves equivalent to those defined previously. this is easily done by using the initial condition and noting that is given by which is identical to because the initial is the same and because they are iteratively computed using the same equation all subsequent quantities must be the same. next we consider the messages that are propagated from the root node back to the leaf node. these take the form fnzn where as before we have eliminated the messages of the type z f since the variable nodes perform no computation. using the definition to substitute for and defining znzn hidden markov models we obtain the beta recursion given by again we can verify that the beta variables themselves are equivalent by noting that implies that the initial message send by the root variable node is zn fn which is identical to the initialization of given in section the sum-product algorithm also specifies how to evaluate the marginals once all the messages have been evaluated. in particular the result shows that the local marginal at the node zn is given by the product of the incoming messages. because we have conditioned on the variables x xn we are computing the joint distribution pzn x fn znzn znzn dividing both sides by px we then obtain pzn x px px exercise in agreement with the result can similarly be derived from scaling factors there is an important issue that must be addressed before we can make use of the forward backward algorithm in practice. from the recursion relation we note that at each step the new value is obtained from the previous value by multiplying by quantities pznzn and pxnzn. because these probabilities are often significantly less than unity as we work our way forward along the chain the values of can go to zero exponentially quickly. for moderate lengths of chain or so the calculation of the will soon exceed the dynamic range of the computer even if double precision floating point is used. in the case of i.i.d. data we implicitly circumvented this problem with the evaluation of likelihood functions by taking logarithms. unfortunately this will not help here because we are forming sums of products of small numbers are in fact implicitly summing over all possible paths through the lattice diagram of figure we therefore work with re-scaled versions of and whose values remain of order unity. as we shall see the corresponding scaling factors cancel out when we use these re-scaled quantities in the em algorithm. in we defined xn zn representing the joint distribution of all the observations up to xn and the latent variable zn. now we define a normalized version of given by xn xn which we expect to be well behaved numerically because it is a probability distribution over k variables for any value of n. in order to relate the scaled and original alpha variables we introduce scaling factors defined by conditional distributions over the observed variables cn xn sequential data from the product rule we then have cm xn and so cm xn pxnzn we can then turn the recursion equation for into one given by note that at each stage of the forward message passing phase used to that normalizes the right-hand side of to we can similarly define re-scaled using are simply the ratio of two conditional probabilities which will again remain within machine precision because from the quan we have to evaluate and store cn which is easily done because it is the coefficient cm zn xnzn xn the recursion result for then gives the following recursion for the re-scaled variables in applying this recursion relation we make use of the scaling factors cn that were previously computed in the phase. from we see that the likelihood function can be found using px cn. exercise similarly using and together with we see that the required marginals are given by zn section hidden markov models finally we note that there is an alternative formulation of the forward-backward algorithm in which the backward pass is defined by a recursion based the quantities instead of this recursion requires that the forward pass be completed first so that all the quantities are available for the backward pass whereas the forward and backward passes of the algorithm can be done independently. although these two algorithms have comparable computational cost the version is the most commonly encountered one in the case of hidden markov models whereas for linear dynamical systems a recursion analogous to the form is more usual. the viterbi algorithm in many applications of hidden markov models the latent variables have some meaningful interpretation and so it is often of interest to find the most probable sequence of hidden states for a given observation sequence. for instance in speech recognition we might wish to find the most probable phoneme sequence for a given series of acoustic observations. because the graph for the hidden markov model is a directed tree this problem can be solved exactly using the max-sum algorithm. we recall from our discussion in section that the problem of finding the most probable sequence of latent states is not the same as that of finding the set of states that are individually the most probable. the latter problem can be solved by first running the forward-backward algorithm to find the latent variable marginals and then maximizing each of these individually et al. however the set of such states will not in general correspond to the most probable sequence of states. in fact this set of states might even represent a sequence having zero probability if it so happens that two successive states which in isolation are individually the most probable are such that the transition matrix element connecting them is zero. in practice we are usually interested in finding the most probable sequence of states and this can be solved efficiently using the max-sum algorithm which in the context of hidden markov models is known as the viterbi algorithm note that the max-sum algorithm works with log probabilities and so there is no need to use re-scaled variables as was done with the forward-backward algorithm. figure shows a fragment of the hidden markov model expanded as lattice diagram. as we have already noted the number of possible paths through the lattice grows exponentially with the length of the chain. the viterbi algorithm searches this space of paths efficiently to find the most probable path with a computational cost that grows only linearly with the length of the chain. as with the sum-product algorithm we first represent the hidden markov model as a factor graph as shown in figure again we treat the variable node zn as the root and pass messages to the root starting with the leaf nodes. using the results and we see that the messages passed in the max-sum algorithm are given by zn fn znzn max zn ln zn sequential data figure a fragment of the hmm lattice showing two possible paths. the viterbi algorithm efficiently determines the most probable path from amongst the exponentially many possibilities. for any given path the corresponding probability is given by the product of the elements of the transition matrix ajk corresponding to the probabilities for each segment of the path along with the emission densities pxnk associated with each node on the path. k k k n n n n if we eliminate zn between these two equations and make use of we obtain a recursion for the f z messages of the form ln max zn where we have introduced the notation fn znzn. from and these messages are initialized using ln ln where we have used note that to keep the notation uncluttered we omit the dependence on the model parameters that are held fixed when finding the most probable sequence. exercise the viterbi algorithm can also be derived directly from the definition of the joint distribution by taking the logarithm and then exchanging maximizations and summations. it is easily seen that the quantities have the probabilistic interpretation max xn zn. once we have completed the final maximization over zn we will obtain the value of the joint distribution px z corresponding to the most probable path. we also wish to find the sequence of latent variable values that corresponds to this path. to do this we simply make use of the back-tracking procedure discussed in section specifically we note that the maximization over zn must be performed for each of the k possible values of suppose we keep a record of the values of zn that correspond to the maxima for each value of the k values of let us denote this function by where k k. once we have passed messages to the end of the chain and found the most probable state of zn we can then use this function to backtrack along the chain by applying it recursively kmax n hidden markov models intuitively we can understand the viterbi algorithm as follows. naively we could consider explicitly all of the exponentially many paths through the lattice evaluate the probability for each and then select the path having the highest probability. however we notice that we can make a dramatic saving in computational cost as follows. suppose that for each path we evaluate its probability by summing up products of transition and emission probabilities as we work our way forward along each path through the lattice. consider a particular time step n and a particular state k at that time step. there will be many possible paths converging on the corresponding node in the lattice diagram. however we need only retain that particular path that so far has the highest probability. because there are k states at time step n we need to keep track of k such paths. at time step n there will be k possible paths to consider comprising k possible paths leading out of each of the k current states but again we need only retain k of these corresponding to the best path for each state at time when we reach the final time step n we will discover which state corresponds to the overall most probable path. because there is a unique path coming into that state we can trace the path back to step n to see what state it occupied at that time and so on back through the lattice to the state n extensions of the hidden markov model the basic hidden markov model along with the standard training algorithm based on maximum likelihood has been extended in numerous ways to meet the requirements of particular applications. here we discuss a few of the more important examples. we see from the digits example in figure that hidden markov models can be quite poor generative models for the data because many of the synthetic digits look quite unrepresentative of the training data. if the goal is sequence classification there can be significant benefit in determining the parameters of hidden markov models using discriminative rather than maximum likelihood techniques. suppose we have a training set of r observation sequences xr where r r each of which is labelled according to its class m where m m. for each class we have a separate hidden markov model with its own parameters m and we treat the problem of determining the parameter values as a standard classification problem in which we optimize the cross-entropy ln pmrxr. using bayes theorem this can be expressed in terms of the sequence probabilities associated with the hidden markov models ln pxr rpmr pxr lplr where pm is the prior probability of class m. optimization of this cost function is more complex than for maximum likelihood and in particular sequential data figure section of an autoregressive hidden markov model in which the distribution of the observation xn depends on a subset of the previous observations as well as on the hidden state zn. in this example the distribution of xn depends on the two previous observations xn and xn zn zn xn xn requires that every training sequence be evaluated under each of the models in order to compute the denominator in hidden markov models coupled with discriminative training methods are widely used in speech recognition a significant weakness of the hidden markov model is the way in which it represents the distribution of times for which the system remains in a given state. to see the problem note that the probability that a sequence sampled from a given hidden markov model will spend precisely t steps in state k and then make a transition to a different state is given by pt akk exp t ln akk and so is an exponentially decaying function of t for many applications this will be a very unrealistic model of state duration. the problem can be resolved by modelling state duration directly in which the diagonal coefficients akk are all set to zero and each state k is explicitly associated with a probability distribution ptk of possible duration times. from a generative point of view when a state k is entered a value t representing the number of time steps that the system will remain in state k is then drawn from ptk. the model then emits t values of the observed variable xt which are generally assumed to be independent so that the corresponding pxtk. this approach requires some straightforward sion density is simply modifications to the em optimization procedure another limitation of the standard hmm is that it is poor at capturing longrange correlations between the observed variables between variables that are separated by many time steps because these must be mediated via the first-order markov chain of hidden states. longer-range effects could in principle be included by adding extra links to the graphical model of figure one way to address this is to generalize the hmm to give the autoregressive hidden markov model et al. an example of which is shown in figure for discrete observations this corresponds to expanded tables of conditional probabilities for the emission distributions. in the case of a gaussian emission density we can use the lineargaussian framework in which the conditional distribution for xn given the values of the previous observations and the value of zn is a gaussian whose mean is a linear combination of the values of the conditioning variables. clearly the number of additional links in the graph must be limited to avoid an excessive the number of free parameters. in the example shown in figure each observation depends on figure example of an input-output hidden markov model. in this case both the emission probabilities and the transition probabilities depend on the values of a sequence of observations un hidden markov models un un zn zn xn xn the two preceding observed variables as well as on the hidden state. although this graph looks messy we can again appeal to d-separation to see that in fact it still has a simple probabilistic structure. in particular if we imagine conditioning on zn we see that as with the standard hmm the values of zn and are independent corresponding to the conditional independence property this is easily verified by noting that every path from node zn to node passes through at least one observed node that is head-to-tail with respect to that path. as a consequence we can again use a forward-backward recursion in the e step of the em algorithm to determine the posterior distributions of the latent variables in a computational time that is linear in the length of the chain. similarly the m step involves only a minor modification of the standard m-step equations. in the case of gaussian emission densities this involves estimating the parameters using the standard linear regression equations discussed in chapter we have seen that the autoregressive hmm appears as a natural extension of the standard hmm when viewed as a graphical model. in fact the probabilistic graphical modelling viewpoint motivates a plethora of different graphical structures based on the hmm. another example is the input-output hidden markov model and frasconi in which we have a sequence of observed variables un in addition to the output variables xn whose values influence either the distribution of latent variables or output variables or both. an example is shown in figure this extends the hmm framework to the domain of supervised learning for sequential data. it is again easy to show through the use of the d-separation criterion that the markov property for the chain of latent variables still holds. to verify this simply note that there is only one path from node zn to node and this is head-to-tail with respect to the observed node zn. this conditional independence property again allows the formulation of a computationally efficient learning algorithm. in particular we can determine the parameters of the model by maximizing the likelihood function l pxu where u is a matrix whose rows are given by ut n. as a consequence of the conditional independence property this likelihood function can be maximized efficiently using an em algorithm in which the e step involves forward and backward recursions. another variant of the hmm worthy of mention is the factorial hidden markov model and jordan in which there are multiple independent exercise sequential data figure a factorial hidden markov model comprising two markov chains of latent variables. for continuous observed variables x one possible choice of emission model is a linear-gaussian density in which the mean of the gaussian is a linear combination of the states of the corresponding latent variables. n n n n xn xn markov chains of latent variables and the distribution of the observed variable at a given time step is conditional on the states of all of the corresponding latent variables at that same time step. figure shows the corresponding graphical model. the motivation for considering factorial hmm can be seen by noting that in order to represent say bits of information at a given time step a standard hmm would need k latent states whereas a factorial hmm could make use of binary latent chains. the primary disadvantage of factorial hmms however lies in the additional complexity of training them. the m step for the factorial hmm model is straightforward. however observation of the x variables introduces dependencies between the latent chains leading to difficulties with the e step. this can be seen by noting that in figure the variables n are connected by a path which is head-to-head at node xn and hence they are not d-separated. the exact e step for this model does not correspond to running forward and backward recursions along the m markov chains independently. this is confirmed by noting that the key conditional independence property is not satisfied for the individual markov chains in the factorial hmm model as is shown using d-separation in figure now suppose that there are m chains of hidden nodes and for simplicity suppose that all latent variables have the same number k of states. then one approach would be to note that there are km combinations of latent variables at a given time step n and figure example of a path highlighted in green which is head-to-head at the observed nodes xn and and head-to-tail at the unobserved nodes n and thus the path is not blocked and so the conditional independence property does not hold for the individual latent chains of the factorial hmm model. as a consequence there is no efficient exact e step for this model. n n n n n xn xn section linear dynamical systems and so we can transform the model into an equivalent standard hmm having a single chain of latent variables each of which has km latent states. we can then run the standard forward-backward recursions in the e step. this has computational complexity on k that is exponential in the number m of latent chains and so will be intractable for anything other than small values of m. one solution would be to use sampling methods in chapter as an elegant deterministic alternative ghahramani and jordan exploited variational inference techniques to obtain a tractable algorithm for approximate inference. this can be done using a simple variational posterior distribution that is fully factorized with respect to the latent variables or alternatively by using a more powerful approach in which the variational distribution is described by independent markov chains corresponding to the chains of latent variables in the original model. in the latter case the variational inference algorithms involves running independent forward and backward recursions along each chain which is computationally efficient and yet is also able to capture correlations between variables within the same chain. clearly there are many possible probabilistic structures that can be constructed according to the needs of particular applications. graphical models provide a general technique for motivating describing and analysing such structures and variational methods provide a powerful framework for performing inference in those models for which exact solution is intractable. linear dynamical systems in order to motivate the concept of linear dynamical systems let us consider the following simple problem which often arises in practical settings. suppose we wish to measure the value of an unknown quantity z using a noisy sensor that returns a observation x representing the value of z plus zero-mean gaussian noise. given a single measurement our best guess for z is to assume that z x. however we can improve our estimate for z by taking lots of measurements and averaging them because the random noise terms will tend to cancel each other. now let s make the situation more complicated by assuming that we wish to measure a quantity z that is changing over time. we can take regular measurements of x so that at some point in time we have obtained xn and we wish to find the corresponding values xn if we simply average the measurements the error due to random noise will be reduced but unfortunately we will just obtain a single averaged estimate in which we have averaged over the changing value of z thereby introducing a new source of error. intuitively we could imagine doing a bit better as follows. to estimate the value of zn we take only the most recent few measurements say xn l xn and just average these. if z is changing slowly and the random noise level in the sensor is high it would make sense to choose a relatively long window of observations to average. conversely if the signal is changing quickly and the noise levels are small we might be better just to use xn directly as our estimate of zn perhaps we could do even better if we take a weighted average in which more recent measurements sequential data make a greater contribution than less recent ones. although this sort of intuitive argument seems plausible it does not tell us how to form a weighted average and any sort of hand-crafted weighing is hardly likely to be optimal. fortunately we can address problems such as this much more systematically by defining a probabilistic model that captures the time evolution and measurement processes and then applying the inference and learning methods developed in earlier chapters. here we shall focus on a widely used model known as a linear dynamical system. as we have seen the hmm corresponds to the state space model shown in figure in which the latent variables are discrete but with arbitrary emission probability distributions. this graph of course describes a much broader class of probability distributions all of which factorize according to we now consider extensions to other distributions for the latent variables. in particular we consider continuous latent variables in which the summations of the sum-product algorithm become integrals. the general form of the inference algorithms will however be the same as for the hidden markov model. it is interesting to note that historically hidden markov models and linear dynamical systems were developed independently. once they are both expressed as graphical models however the deep relationship between them immediately becomes apparent. one key requirement is that we retain an efficient algorithm for inference which is linear in the length of the chain. this requires that for instance when we take xn and multiply by the transition probability pznzn and the emission probability pxnzn and then marginalize over zn we obtain a distribution over a representing the posterior probability of zn given observations zn that is of the same functional form as that over that is to say the distribution must not become more complex at each stage but must only change in its parameter values. not surprisingly the only distributions that have this property of being closed under multiplication are those belonging to the exponential family. here we consider the most important example from a practical perspective which is the gaussian. in particular we consider a linear-gaussian state space model so that the latent variables as well as the observed variables are multivariate gaussian distributions whose means are linear functions of the states of their parents in the graph. we have seen that a directed graph of linear-gaussian units is equivalent to a joint gaussian distribution over all of the variables. furthermore marginals such are also gaussian so that the functional form of the meseach of which has a mean that is linear in zn. then even is gaussian the will be a mixture of k gaussians will be a mixture of k sages is preserved and we will obtain an efficient inference algorithm. by contrast suppose that the emission densities pxnzn comprise a mixture of k gaussians gaussians and so on and exact inference will not be of practical value. we have seen that the hidden markov model can be viewed as an extension of the mixture models of chapter to allow for sequential correlations in the data. in a similar way we can view the linear dynamical system as a generalization of the continuous latent variable models of chapter such as probabilistic pca and factor analysis. each pair of nodes xn represents a linear-gaussian latent variable linear dynamical systems model for that particular observation. however the latent variables are no longer treated as independent but now form a markov chain. because the model is represented by a tree-structured directed graph inference problems can be solved efficiently using the sum-product algorithm. the forward recursions analogous to the messages of the hidden markov model are known as the kalman filter equations zarchan and musoff and the backward recursions analogous to the messages are known as the kalman smoother equations or the rauch-tung-striebel equations et al. the kalman filter is widely used in many real-time tracking applications. because the linear dynamical system is a linear-gaussian model the joint distribution over all variables as well as all marginals and conditionals will be gaussian. it follows that the sequence of individually most probable latent variable values is the same as the most probable latent sequence. there is thus no need to consider the analogue of the viterbi algorithm for the linear dynamical system. because the model has linear-gaussian conditional distributions we can write the transition and emission distributions in the general form pznzn n pxnzn n the initial latent variable also has a gaussian distribution which we write as n exercise exercise note that in order to simplify the notation we have omitted additive constant terms from the means of the gaussians. in fact it is straightforward to include them if desired. traditionally these distributions are more commonly expressed in an equivalent form in terms of noisy linear equations given by zn azn wn xn czn vn u where the noise terms have the distributions w n v n u n the parameters of the model denoted by c can be determined using maximum likelihood through the em algorithm. in the e step we need to solve the inference problem of determining the local posterior marginals for the latent variables which can be solved efficiently using the sum-product algorithm as we discuss in the next section. sequential data inference in lds we now turn to the problem of finding the marginal distributions for the latent variables conditional on the observation sequence. for given parameter settings we also wish to make predictions of the next latent state zn and of the next observation xn conditioned on the observed data xn for use in real-time applications. these inference problems can be solved efficiently using the sum-product algorithm which in the context of the linear dynamical system gives rise to the kalman filter and kalman smoother equations. it is worth emphasizing that because the linear dynamical system is a lineargaussian model the joint distribution over all latent and observed variables is simply a gaussian and so in principle we could solve inference problems by using the standard results derived in previous chapters for the marginals and conditionals of a multivariate gaussian. the role of the sum-product algorithm is to provide a more efficient way to perform such computations. linear dynamical systems have the identical factorization given by to hidden markov models and are again described by the factor graphs in figures and inference algorithms therefore take precisely the same form except that summations over latent variables are replaced by integrations. we begin by considering the forward equations in which we treat zn as the root node and propagate messages from the leaf node to the root. from the initial message will be gaussian and because each of the factors is gaussian all subsequent messages will also be gaussian. by convention we shall propagate messages that are normalized marginal distributions corresponding to xn which we denote by this is precisely analogous to the propagation of scaled given by n n vn. in the discrete case of the hidden markov model and so the recursion equation now takes the form pxnzn dzn substituting for the conditionals pznzn and pxnzn using and respectively and making use of we see that becomes cnn n vn n n n vn dzn here we are supposing that n and vn are known and by evaluating the integral in we wish to determine values for n and vn. the integral is easily evaluated by making use of the result from which it follows that n n vn dzn n n pn linear dynamical systems where we have defined pn avn we can now combine this result with the first factor on the right-hand side of by making use of and to give n a n knxn ca n vn kncpn cn n n cpn here we have made use of the matrix inverse identities and and also defined the kalman gain matrix kn pn cpn thus given the values of n and vn together with the new observation xn we can evaluate the gaussian marginal for zn having mean n and covariance vn as well as the normalization coefficient cn. the initial conditions for these recursion equations are obtained from because is given by and is given by we can again make use of to calculate and to calculate and giving c n where similarly the likelihood function for the linear dynamical system is given by in which the factors cn are found using the kalman filtering equations. we can interpret the steps involved in going from the posterior marginal over zn to the posterior marginal over zn as follows. in we can view the quantity a n as the prediction of the mean over zn obtained by simply taking the mean over zn and projecting it forward one step using the transition probability matrix a. this predicted mean would give a predicted observation for xn given by cazn obtained by applying the emission probability matrix c to the predicted hidden state mean. we can view the update equation for the mean of the hidden variable distribution as taking the predicted mean a n and then adding a correction that is proportional to the error xn cazn between the predicted observation and the actual observation. the coefficient of this correction is given by the kalman gain matrix. thus we can view the kalman filter as a process of making successive predictions and then correcting these predictions in the light of the new observations. this is illustrated graphically in figure sequential data zn zn zn figure the linear dynamical system can be viewed as a sequence of steps in which increasing uncertainty in the state variable due to diffusion is compensated by the arrival of new data. in the left-hand plot the blue curve shows the distribution pzn xn which incorporates all the data up to step n the diffusion arising from the nonzero variance of the transition probability pznzn gives the distribution xn shown in red in the centre plot. note that this is broader and shifted relative to the blue curve is shown dashed in the centre plot for comparison. the next data observation xn contributes through the emission density pxnzn which is shown as a function of zn in green on the right-hand plot. note that this is not a density with respect to zn and so is not normalized to one. inclusion of this new data point leads to a revised distribution xn for the state density shown in blue. we see that observation of the data has shifted and narrowed the distribution compared to xn is shown in dashed in the right-hand plot for comparison. exercise exercise if we consider a situation in which the measurement noise is small compared to the rate at which the latent variable is evolving then we find that the posterior distribution for zn depends only on the current measurement xn in accordance with the intuition from our simple example at the start of the section. similarly if the latent variable is evolving slowly relative to the observation noise level we find that the posterior mean for zn is obtained by averaging all of the measurements obtained up to that time. one of the most important applications of the kalman filter is to tracking and this is illustrated using a simple example of an object moving in two dimensions in figure so far we have solved the inference problem of finding the posterior marginal for a node zn given observations from up to xn. next we turn to the problem of finding the marginal for a node zn given all observations to xn for temporal data this corresponds to the inclusion of future as well as past observations. although this cannot be used for real-time prediction it plays a key role in learning the parameters of the model. by analogy with the hidden markov model this problem can be solved by propagating messages from node xn back to node and combining this information with that obtained during the forward message passing stage used to compute of rather than in terms because must also be in the lds literature it is usual to formulate this backward recursion in terms gaussian we write it in the form n to derive the required recursion we start from the backward recursion for linear dynamical systems figure an illustration of a linear dynamical system being used to track a moving object. the blue points indicate the true positions of the object in a two-dimensional space at successive time steps the green points denote noisy measurements of the positions and the red crosses indicate the means of the inferred posterior distributions of the positions obtained by running the kalman filtering equations. the covarithe inferred positions ances of are indicated by the red ellipses which correspond to contours having one standard deviation. which for continuous latent variables can be written in the form we now multiply both sides of and substitute for n n jn vn jn and using and then we make use of and together with and after some manipulation we obtain a n pn jt n where we have defined exercise exercise and we have made use of avn pnjt n. note that these recursions require that the forward pass be completed first so that the quantities n and vn will be available for the backward pass. jn vnat zn can be obtained from in the form for the em algorithm we also require the pairwise posterior marginals which n n vn substituting using and rearranging we see that zn is a gaussian with mean given with components and and a covariance between zn and zn given by covzn zn jn sequential data learning in lds so far we have considered the inference problem for linear dynamical systems assuming that the model parameters c are known. next we consider the determination of these parameters using maximum likelihood and hinton because the model has latent variables this can be addressed using the em algorithm which was discussed in general terms in chapter we can derive the em algorithm for the linear dynamical system as follows. let us denote the estimated parameter values at some particular cycle of the algorithm by old. for these parameter values we can run the inference algorithm to determine the posterior distribution of the latent variables pzx old or more precisely those in particular we shall local posterior marginals that are required in the m step. require the following expectations e e n t t jn n znzt n znzt n e n where we have used now we consider the complete-data log likelihood function which is obtained by taking the logarithm of and is therefore given by ln px z ln ln pznzn a ln pxnzn c in which we have made the dependence on the parameters explicit. we now take the expectation of the complete-data log likelihood with respect to the posterior distribution pzx old which defines the function q old ez old px z in the m step this function is maximized with respect to the components of consider first the parameters and if we substitute for in using and then take the expectation with respect to z we obtain const q old ez old where all terms not dependent on or have been absorbed into the additive constant. maximization with respect to and is easily performed by making use of the maximum likelihood solution for a gaussian distribution discussed in section giving exercise linear dynamical systems similarly to optimize a and we substitute for pznzn a in new vnew using giving q old n ez old ln azn azn const in which the constant comprises terms that are independent of a and maximizing with respect to these parameters then gives anew e znzt n zn n new n znzt n e e anew e znzt n anew anew e zn n e zn n note that anew must be evaluated first and the result can then be used to determine new. pxnzn c in using giving finally in order to determine the new values of c and we substitute for q old n maximizing with respect to c and then gives cznt czn const. ln ez old n xne cnew xne zt n new n cnew xnxt e xt n e znzt n zt n cnew cnew e znzt n cnew exercise exercise sequential data we have approached parameter learning in the linear dynamical system using maximum likelihood. inclusion of priors to give a map estimate is straightforward and a fully bayesian treatment can be found by applying the analytical approximation techniques discussed in chapter though a detailed treatment is precluded here due to lack of space. extensions of lds as with the hidden markov model there is considerable interest in extending the basic linear dynamical system in order to increase its capabilities. although the assumption of a linear-gaussian model leads to efficient algorithms for inference and learning it also implies that the marginal distribution of the observed variables is simply a gaussian which represents a significant limitation. one simple extension of the linear dynamical system is to use a gaussian mixture as the initial distribution for if this mixture has k components then the forward recursion equations will lead to a mixture of k gaussians over each hidden variable zn and so the model is again tractable. for many applications the gaussian emission density is a poor approximation. if instead we try to use a mixture of k gaussians as the emission density then the will also be a mixture of k gaussians. however from the will comprise a mixture of k gaussians and so on being given by a mixture of k n gaussians. thus the number of components grows exponentially with the length of the chain and so this model is impractical. chapter more generally introducing transition or emission models that depart from the linear-gaussian other exponential family model leads to an intractable inference problem. we can make deterministic approximations such as assumed density filtering or expectation propagation or we can make use of sampling methods as discussed in section one widely used approach is to make a gaussian approximation by linearizing around the mean of the predicted distribution which gives rise to the extended kalman filter and musoff as with hidden markov models we can develop interesting extensions of the basic linear dynamical system by expanding its graphical representation. for example the switching state space model and hinton can be viewed as a combination of the hidden markov model with a set of linear dynamical systems. the model has multiple markov chains of continuous linear-gaussian latent variables each of which is analogous to the latent chain of the linear dynamical system discussed earlier together with a markov chain of discrete variables of the form used in a hidden markov model. the output at each time step is determined by stochastically choosing one of the continuous latent chains using the state of the discrete latent variable as a switch and then emitting an observation from the corresponding conditional output distribution. exact inference in this model is intractable but variational methods lead to an efficient inference scheme involving forward-backward recursions along each of the continuous and discrete markov chains independently. note that if we consider multiple chains of discrete latent variables and use one as the switch to select from the remainder we obtain an analogous model having only discrete latent variables known as the switching hidden markov model. efzn fznpznxn dzn fznpznxn xn dzn fznpxnznpznxn dzn pxnznpznxn dzn linear dynamical systems chapter particle filters for dynamical systems which do not have a linear-gaussian for example if they use a non-gaussian emission density we can turn to sampling methods in order to find a tractable inference algorithm. in particular we can apply the samplingimportance-resampling formalism of section to obtain a sequential monte carlo algorithm known as the particle filter. consider the class of distributions represented by the graphical model in figure and suppose we are given the observed values xn xn and we wish to draw l samples from the posterior distribution pznxn. using bayes theorem we have wl n fzl n where n is a set of samples drawn from pznxn and we have made use of the conditional independence property pxnzn xn pxnzn which follows from the graph in figure the sampling weights n are defined by pxnzl n pxnzm n wl n where the same samples are used in the numerator as in the denominator. thus the posterior distribution pznxn is represented by the set of samples n together with the corresponding weights n and n note that these weights satisfy w n because we wish to find a sequential sampling scheme we shall suppose that a set of samples and weights have been obtained at time step n and that we have subsequently observed the value of and we wish to find the weights and samples at time step n we first sample from the distribution this is l w sequential data straightforward since again using bayes theorem xnpznxn dzn dzn xn dzn dzn pxnznpznxn dzn n wl n where we have made use of the conditional independence properties l xn pxnzn xn pxnzn which follow from the application of the d-separation criterion to the graph in figure the distribution given by is a mixture distribution and samples can be drawn by choosing a component l with probability given by the mixing coefficients wl and then drawing a sample from the corresponding component. in summary we can view each step of the particle filter algorithm as comprising two stages. at time step n we have a sample representation of the posterior distribution pznxn expressed as samples n this can be viewed as a mixture representation of the form to obtain the corresponding representation for the next time step we first draw l samples from the mixture distribution and then for each sample we use the new this is vation to evaluate the corresponding weights w illustrated for the case of a single variable z in figure n with corresponding weights the particle filtering or sequential monte carlo approach has appeared in the literature under various names including the bootstrap filter et al. survival of the fittest et al. and the condensation algorithm and blake exercises www use the technique of d-separation discussed in section to verify that the markov model shown in figure having n nodes in total satisfies the conditional independence properties for n n. similarly show that a model described by the graph in figure in which there are n nodes in total pznxn exercises z figure schematic illustration of the operation of the particle filter for a one-dimensional latent space. at time step n the posterior pznxn is represented as a mixture distribution shown schematically as circles whose sizes are proportional to the weights wl n a set of l samples is then drawn from this distribution and the new weights wl evaluated using satisfies the conditional independence properties xn pxnxn xn for n n. consider the joint probability distribution corresponding to the directed graph of figure using the sum and product rules of probability verify that this joint distribution satisfies the conditional independence property for n n. similarly show that the second-order markov model described by the joint distribution satisfies the conditional independence property xn pxnxn xn for n n. by using d-separation show that the distribution xn of the observed data for the state space model represented by the directed graph in figure does not satisfy any conditional independence properties and hence does not exhibit the markov property at any finite order. www consider a hidden markov model in which the emission densities are represented by a parametric model pxz w such as a linear regression model or a neural network in which w is a vector of adaptive parameters. describe how the parameters w can be learned from data using maximum likelihood. sequential data verify the m-step equations and for the initial state probabilities and transition probability parameters of the hidden markov model by maximization of the expected complete-data log likelihood function using appropriate lagrange multipliers to enforce the summation constraints on the components of and a. show that if any elements of the parameters or a for a hidden markov model are initially set to zero then those elements will remain zero in all subsequent updates of the em algorithm. consider a hidden markov model with gaussian emission densities. show that maximization of the function q old with respect to the mean and covariance parameters of the gaussians gives rise to the m-step equations and www for a hidden markov model having discrete observations governed by a multinomial distribution show that the conditional distribution of the observations given the hidden variables is given by and the corresponding m step equations are given by write down the analogous equations for the conditional distribution and the m step equations for the case of a hidden markov with multiple binary output variables each of which is governed by a bernoulli conditional distribution. hint refer to sections and for a discussion of the corresponding maximum likelihood solutions for i.i.d. data if required. www use the d-separation criterion to verify that the conditional independence properties are satisfied by the joint distribution for the hidden markov model defined by by applying the sum and product rules of probability verify that the conditional independence properties are satisfied by the joint distribution for the hidden markov model defined by starting from the expression for the marginal distribution over the variables of a factor in a factor graph together with the results for the messages in the sum-product algorithm obtained in section derive the result for the joint posterior distribution over two successive latent variables in a hidden markov model. suppose we wish to train a hidden markov model by maximum likelihood using data that comprises r independent sequences of observations which we denote by xr where r r. show that in the e step of the em algorithm we simply evaluate posterior probabilities for the latent variables by running the and recursions independently for each of the sequences. also show that in the m step the initial probability and transition probability parameters are re-estimated using modified forms of and given by k ajk n z nk n z nl exercises where for notational convenience we have assumed that the sequences are of the same length generalization to sequences of different lengths is straightforward. similarly show that the m-step equation for re-estimation of the means of gaussian emission models is given by k nk n nk note that the m-step equations for other emission model parameters and distributions take an analogous form. www use the definition of the messages passed from a factor node to a variable node in a factor graph together with the expression for the joint distribution in a hidden markov model to show that the definition of the alpha message is the same as the definition use the definition of the messages passed from a factor node to a variable node in a factor graph together with the expression for the joint distribution in a hidden markov model to show that the definition of the beta message is the same as the definition use the expressions and for the marginals in a hidden markov model to derive the corresponding results and expressed in terms of re-scaled variables. in this exercise we derive the forward message passing equation for the viterbi algorithm directly from the expression for the joint distribution. this involves maximizing over all of the hidden variables zn by taking the logarithm and then exchanging maximizations and summations derive the recursion sequential data where the quantities are defined by show that the initial condition for this recursion is given by www show that the directed graph for the input-output hidden markov model given in figure can be expressed as a tree-structured factor graph of the form shown in figure and write down expressions for the initial factor and for the general factor fnzn zn where n n. using the result of exercise derive the recursion equations including the initial conditions for the forward-backward algorithm for the input-output hidden markov model shown in figure www the kalman filter and smoother equations allow the posterior distributions over individual latent variables conditioned on all of the observed variables to be found efficiently for linear dynamical systems. show that the sequence of latent variable values obtained by maximizing each of these posterior distributions individually is the same as the most probable sequence of latent values. to do this simply note that the joint distribution of all latent and observed variables in a linear dynamical system is gaussian and hence all conditionals and marginals will also be gaussian and then make use of the result www use the result to prove use the results and together with the matrix identities and to derive the results and where the kalman gain matrix kn is defined by www using together with the definitions and and the result derive using together with the definitions and and the result derive and www consider a generalization of and in which we include constant terms a and c in the gaussian means so that pznzn n a pxnzn n c show that this extension can be re-case in the framework discussed in this chapter by defining a state vector z with an additional component fixed at unity and then augmenting the matrices a and c using extra columns corresponding to the parameters a and c. in this exercise we show that when the kalman filter equations are applied to independent observations they reduce to the results given in section for the maximum likelihood solution for a single gaussian distribution. consider the problem of finding the mean of a single gaussian random variable x in which we are given a set of independent observations xn. to model this we can use exercises a linear dynamical system governed by and with latent variables zn in which c becomes the identity matrix and where the transition probability a because the observations are independent. let the parameters and of the initial state be denoted by and respectively and suppose that becomes write down the corresponding kalman filter equations starting from the general results and together with and show that these are equivalent to the results and obtained directly by considering independent data. consider a special case of the linear dynamical system of section that is equivalent to probabilistic pca so that the transition matrix a the covariance i and the noise covariance by making use of the matrix inversion identity show that if the emission density matrix c is denoted w then the posterior distribution over the hidden states defined by and reduces to the result for probabilistic pca. www consider a linear dynamical system of the form discussed in section in which the amplitude of the observation noise goes to zero so that show that the posterior distribution for zn has mean xn and zero variance. this accords with our intuition that if there is no noise we should just use the current observation xn to estimate the state variable zn and ignore all previous observations. consider a special case of the linear dynamical system of section in which the state variable zn is constrained to be equal to the previous state variable which corresponds to a i and for simplicity assume also that so that the initial conditions for z are unimportant and the predictions are determined purely by the data. use proof by induction to show that the posterior mean for state zn is determined by the average of xn. this corresponds to the intuitive result that if the state variable is constant our best estimate is obtained by averaging the observations. starting from the backwards recursion equation derive the rts smoothing equations and for the gaussian linear dynamical system. starting from the result for the pairwise posterior marginal in a state space model derive the specific form for the case of the gaussian linear dynamical system. starting from the result and by substituting using verify the result for the covariance between zn and zn www verify the results and for the m-step equations for and in the linear dynamical system. verify the results and for the m-step equations for a and in the linear dynamical system. sequential data verify the results and for the m-step equations for c and in the linear dynamical system. combining models in earlier chapters we have explored a range of different models for solving classification and regression problems. it is often found that improved performance can be obtained by combining multiple models together in some way instead of just using a single model in isolation. for instance we might train l different models and then make predictions using the average of the predictions made by each model. such combinations of models are sometimes called committees. in section we discuss ways to apply the committee concept in practice and we also give some insight into why it can sometimes be an effective procedure. one important variant of the committee method known as boosting involves training multiple models in sequence in which the error function used to train a particular model depends on the performance of the previous models. this can produce substantial improvements in performance compared to the use of a single model and is discussed in section instead of averaging the predictions of a set of models an alternative form of combining models model combination is to select one of the models to make the prediction in which the choice of model is a function of the input variables. thus different models become responsible for making predictions in different regions of input space. one widely used framework of this kind is known as a decision tree in which the selection process can be described as a sequence of binary selections corresponding to the traversal of a tree structure and is discussed in section in this case the individual models are generally chosen to be very simple and the overall flexibility of the model arises from the input-dependent selection process. decision trees can be applied to both classification and regression problems. one limitation of decision trees is that the division of input space is based on hard splits in which only one model is responsible for making predictions for any given value of the input variables. the decision process can be softened by moving to a probabilistic framework for combining models as discussed in section for example if we have a set of k models for a conditional distribution ptx k where x is the input variable t is the target variable and k k indexes the model then we can form a probabilistic mixture of the form ptx kxptx k in which kx pkx represent the input-dependent mixing coefficients. such models can be viewed as mixture distributions in which the component densities as well as the mixing coefficients are conditioned on the input variables and are known as mixtures of experts. they are closely related to the mixture density network model discussed in section bayesian model averaging it is important to distinguish between model combination methods and bayesian model averaging as the two are often confused. to understand the difference consider the example of density estimation using a mixture of gaussians in which several gaussian components are combined probabilistically. the model contains a binary latent variable z that indicates which component of the mixture is responsible for generating the corresponding data point. thus the model is specified in terms of a joint distribution and the corresponding density over the observed variable x is obtained by marginalizing over the latent variable px z px px z. z section committees in the case of our gaussian mixture example this leads to a distribution of the form px kn k k with the usual interpretation of the symbols. this is an example of model combi nation. for independent identically distributed data we can use to write the marginal probability of a data set x xn in the form px pxn pxn zn zn thus we see that each observed data point xn has a corresponding latent variable zn. now suppose we have several different models indexed by h h with prior probabilities ph. for instance one model might be a mixture of gaussians and another model might be a mixture of cauchy distributions. the marginal distribution over the data set is given by px pxhph. this is an example of bayesian model averaging. the interpretation of this summation over h is that just one model is responsible for generating the whole data set and the probability distribution over h simply reflects our uncertainty as to which model that is. as the size of the data set increases this uncertainty reduces and the posterior probabilities phx become increasingly focussed on just one of the models. this highlights the key difference between bayesian model averaging and model combination because in bayesian model averaging the whole data set is generated by a single model. by contrast when we combine multiple models as in we see that different data points within the data set can potentially be generated from different values of the latent variable z and hence by different components. although we have considered the marginal probability px the same considerations apply for the predictive density pxx or for conditional distributions such as ptx x t. exercise committees section the simplest way to construct a committee is to average the predictions of a set of individual models. such a procedure can be motivated from a frequentist perspective by considering the trade-off between bias and variance which decomposes the error due to a model into the bias component that arises from differences between the model and the true function to be predicted and the variance component that represents the sensitivity of the model to the individual data points. recall from figure combining models that when we trained multiple polynomials using the sinusoidal data and then averaged the resulting functions the contribution arising from the variance term tended to cancel leading to improved predictions. when we averaged a set of low-bias models to higher order polynomials we obtained accurate predictions for the underlying sinusoidal function from which the data were generated. in practice of course we have only a single data set and so we have to find a way to introduce variability between the different models within the committee. one approach is to use bootstrap data sets discussed in section consider a regression problem in which we are trying to predict the value of a single continuous variable and suppose we generate m bootstrap data sets and then use each to train a separate copy ymx of a predictive model where m m. the committee prediction is given by ymx. ycomx m this procedure is known as bootstrap aggregation or bagging suppose the true regression function that we are trying to predict is given by hx so that the output of each of the models can be written as the true value plus an error in the form ymx hx the average sum-of-squares error then takes the form ex where ex denotes a frequentist expectation with respect to the distribution of the input vector x. the average error made by the models acting individually is therefore ex eav ex m m m ecom ex ex ymx hx similarly the expected error from the committee is given by if we assume that the errors have zero mean and are uncorrelated so that ex ex m l exercise then we obtain boosting ecom m eav. this apparently dramatic result suggests that the average error of a model can be reduced by a factor of m simply by averaging m versions of the model. unfortunately it depends on the key assumption that the errors due to the individual models are uncorrelated. in practice the errors are typically highly correlated and the reduction in overall error is generally small. it can however be shown that the expected committee error will not exceed the expected error of the constituent models so that ecom eav. in order to achieve more significant improvements we turn to a more sophisticated technique for building committees known as boosting. exercise boosting boosting is a powerful technique for combining multiple base classifiers to produce a form of committee whose performance can be significantly better than that of any of the base classifiers. here we describe the most widely used form of boosting algorithm called adaboost short for adaptive boosting developed by freund and schapire boosting can give good results even if the base classifiers have a performance that is only slightly better than random and hence sometimes the base classifiers are known as weak learners. originally designed for solving classification problems boosting can also be extended to regression the principal difference between boosting and the committee methods such as bagging discussed above is that the base classifiers are trained in sequence and each base classifier is trained using a weighted form of the data set in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers. in particular points that are misclassified by one of the base classifiers are given greater weight when used to train the next classifier in the sequence. once all the classifiers have been trained their predictions are then combined through a weighted majority voting scheme as illustrated schematically in figure consider a two-class classification problem in which the training data comprises input vectors xn along with corresponding binary target variables tn where tn each data point is given an associated weighting parameter wn which is initially set for all data points. we shall suppose that we have a procedure available for training a base classifier using weighted data to give a function yx at each stage of the algorithm adaboost trains a new classifier using a data set in which the weighting coefficients are adjusted according to the performance of the previously trained classifier so as to give greater weight to the misclassified data points. finally when the desired number of base classifiers have been trained they are combined to form a committee using coefficients that give different weight to different base classifiers. the precise form of the adaboost algorithm is given below. combining models figure schematic illustration of the boosting framework. each base classifier ymx is trained on a weighted form of the training set arrows in which the weights wm depend on the performance of the previous base classifier ym arrows. once all base classifiers have been trained they are combined to give the final classifier ym arrows. n n n n ym ym sign mymx m adaboost initialize the data weighting coefficients by setting w n for n n. for m m fit a classifier ymx to the training data by minimizing the weighted error function jm n iymxn tn wm where iymxn tn is the indicator function and equals when ymxn tn and otherwise. evaluate the quantities n iymxn tn wm wm n and then use these to evaluate m ln update the data weighting coefficients n wm n exp miymxn tn make predictions using the final model which is given by boosting ym sign mymx we see that the first base classifier is trained using weighting n that are all equal which therefore corresponds to the usual procedure cients w for training a single classifier. from we see that in subsequent iterations the weighting coefficients w are increased for data points that are misclassified n and decreased for data points that are correctly classified. successive classifiers are therefore forced to place greater emphasis on points that have been misclassified by previous classifiers and data points that continue to be misclassified by successive classifiers receive ever greater weight. the quantities represent weighted measures of the error rates of each of the base classifiers on the data set. we therefore see that the weighting coefficients m defined by give greater weight to the more accurate classifiers when computing the overall output given by the adaboost algorithm is illustrated in figure using a subset of data points taken from the toy classification data set shown in figure here each base learners consists of a threshold on one of the input variables. this simple classifier corresponds to a form of decision tree known as a decision stumps i.e. a decision tree with a single node. thus each base learner classifies an input according to whether one of the input features exceeds some threshold and therefore simply partitions the space into two regions separated by a linear decision surface that is parallel to one of the axes. section minimizing exponential error boosting was originally motivated using statistical learning theory leading to upper bounds on the generalization error. however these bounds turn out to be too loose to have practical value and the actual performance of boosting is much better than the bounds alone would suggest. friedman et al. gave a different and very simple interpretation of boosting in terms of the sequential minimization of an exponential error function. consider the exponential error function defined by exp tnfmxn e where fmx is a classifier defined in terms of a linear combination of base classifiers ylx of the form fmx and tn are the training set target values. our goal is to minimize e with respect to both the weighting coefficients l and the parameters of the base classifiers ylx. lylx combining models m m m m m m figure illustration of boosting in which the base learners consist of simple thresholds applied to one or other of the axes. each figure shows the number m of base learners trained so far along with the decision boundary of the most recent base learner black line and the combined decision boundary of the ensemble green line. each data point is depicted by a circle whose radius indicates the weight assigned to that data point when training the most recently added base learner. thus for instance we see that points that are misclassified by the m base learner are given greater weight when training the m base learner. instead of doing a global error function minimization however we shall suppose that the base classifiers ym are fixed as are their coefficients m and so we are minimizing only with respect to m and ymx. separating off the contribution from base classifier ymx we can then write the error function in the form e exp tnfm tn mymxn wm n exp tn mymxn n exp tnfm can be viewed as constants where the coefficients w if we denote by tm the set of because we are optimizing only m and ymx. data points that are correctly classified by ymx and if we denote the remaining misclassified points by mm then we can in turn rewrite the error function in the boosting n mm wm n form e e wm n e n tm e n iymxn tn e wm wm n exercise exercise when we minimize this with respect to ymx we see that the second term is constant and so this is equivalent to minimizing because the overall multiplicative factor in front of the summation does not affect the location of the minimum. similarly minimizing with respect to m we obtain in which is defined by from we see that having found m and ymx the weights on the data tn mymxn points are updated using n wm n exp making use of the fact that tnymxn tn we see that the weights w n are updated at the next iteration using exp exp miymxn tn wm n n because the term exp is independent of n we see that it weights all data points by the same factor and so can be discarded. thus we obtain finally once all the base classifiers are trained new data points are classified by evaluating the sign of the combined function defined according to because the factor of does not affect the sign it can be omitted giving error functions for boosting the exponential error function that is minimized by the adaboost algorithm differs from those considered in previous chapters. to gain some insight into the nature of the exponential error function we first consider the expected error given by ext tyx exp tyxptxpx dx. t if we perform a variational minimization with respect to all possible functions yx we obtain yx ln pt pt combining models figure plot of the exponential and rescaled cross-entropy error functions along with the hinge error used in support vector machines and the misclassification for large error note that negative values of z tyx the cross-entropy gives a linearly increasing penalty whereas the exponential loss gives an exponentially increasing penalty. ez z which is half the log-odds. thus the adaboost algorithm is seeking the best approximation to the log odds ratio within the space of functions represented by the linear combination of base classifiers subject to the constrained minimization resulting from the sequential optimization strategy. this result motivates the use of the sign function in to arrive at the final classification decision. we have already seen that the minimizer yx of the cross-entropy error for two-class classification is given by the posterior class probability. in the case of a target variable t we have seen that the error function is given by exp yt. this is compared with the exponential error function in figure where we have divided the cross-entropy error by a constant factor so that it passes through the point for ease of comparison. we see that both can be seen as continuous approximations to the ideal misclassification error function. an advantage of the exponential error is that its sequential minimization leads to the simple adaboost scheme. one drawback however is that it penalizes large negative values of tyx much more strongly than cross-entropy. in particular we see that for large negative values of ty the cross-entropy grows linearly with whereas the exponential error function grows exponentially with thus the exponential error function will be much less robust to outliers or misclassified data points. another important difference between cross-entropy and the exponential error function is that the latter cannot be interpreted as the log likelihood function of any well-defined probabilistic model. furthermore the exponential error does not generalize to classification problems having k classes again in contrast to the cross-entropy for a probabilistic model which is easily generalized to give the interpretation of boosting as the sequential optimization of an additive model under an exponential error et al. opens the door to a wide range of boosting-like algorithms including multiclass extensions by altering the choice of error function. it also motivates the extension to regression problems if we consider a sum-of-squares error function for regression then sequential minimization of an additive model of the form simply involves fitting each new base classifier to the residual errors tn fm from the previous model. as we have noted however the sum-of-squares error is not robust to outliers and this section exercise section exercise figure comparison of the squared error with the absolute error showing how the latter places much less emphasis on large errors and hence is more robust to outliers and mislabelled data points. tree-based models ez z can be addressed by basing the boosting algorithm on the absolute deviation t instead. these two error functions are compared in figure tree-based models there are various simple but widely used models that work by partitioning the input space into cuboid regions whose edges are aligned with the axes and then assigning a simple model example a constant to each region. they can be viewed as a model combination method in which only one model is responsible for making predictions at any given point in input space. the process of selecting a specific model given a new input x can be described by a sequential decision making process corresponding to the traversal of a binary tree that splits into two branches at each node. here we focus on a particular tree-based framework called classification and regression trees or cart et al. although there are many other variants going by such names as and quinlan figure shows an illustration of a recursive binary partitioning of the input space along with the corresponding tree structure. in this example the first step figure illustration of a two-dimensional input space that has been partitioned into five regions using axis-aligned boundaries. b a e c d combining models figure binary tree corresponding to the parinput space shown in fig titioning of ure a b c d e divides the whole of the input space into two regions according to whether or where is a parameter of the model. this creates two subregions each of which can then be subdivided independently. for instance the region is further subdivided according to whether or giving rise to the regions denoted a and b. the recursive subdivision can be described by the traversal of the binary tree shown in figure for any new input x we determine which region it falls into by starting at the top of the tree at the root node and following a path down to a specific leaf node according to the decision criteria at each node. note that such decision trees are not probabilistic graphical models. within each region there is a separate model to predict the target variable. for instance in regression we might simply predict a constant over each region or in classification we might assign each region to a specific class. a key property of treebased models which makes them popular in fields such as medical diagnosis for example is that they are readily interpretable by humans because they correspond to a sequence of binary decisions applied to the individual input variables. for instance to predict a patient s disease we might first ask is their temperature greater than some threshold? if the answer is yes then we might next ask is their blood pressure less than some threshold? each leaf of the tree is then associated with a specific diagnosis. in order to learn such a model from a training set we have to determine the structure of the tree including which input variable is chosen at each node to form the split criterion as well as the value of the threshold parameter i for the split. we also have to determine the values of the predictive variable within each region. consider first a regression problem in which the goal is to predict a single target variable t from a d-dimensional vector x xdt of input variables. the training data consists of input vectors xn along with the corresponding continuous labels tn. if the partitioning of the input space is given and we minimize the sum-of-squares error function then the optimal value of the predictive variable within any given region is just given by the average of the values of tn for those data points that fall in that region. now consider how to determine the structure of the decision tree. even for a fixed number of nodes in the tree the problem of determining the optimal structure choice of input variable for each split as well as the corresponding thresh exercise tree-based models olds to minimize the sum-of-squares error is usually computationally infeasible due to the combinatorially large number of possible solutions. instead a greedy optimization is generally done by starting with a single root node corresponding to the whole input space and then growing the tree by adding nodes one at a time. at each step there will be some number of candidate regions in input space that can be split corresponding to the addition of a pair of leaf nodes to the existing tree. for each of these there is a choice of which of the d input variables to split as well as the value of the threshold. the joint optimization of the choice of region to split and the choice of input variable and threshold can be done efficiently by exhaustive search noting that for a given choice of split variable and threshold the optimal choice of predictive variable is given by the local average of the data as noted earlier. this is repeated for all possible choices of variable to be split and the one that gives the smallest residual sum-of-squares error is retained. given a greedy strategy for growing the tree there remains the issue of when to stop adding nodes. a simple approach would be to stop when the reduction in residual error falls below some threshold. however it is found empirically that often none of the available splits produces a significant reduction in error and yet after several more splits a substantial error reduction is found. for this reason it is common practice to grow a large tree using a stopping criterion based on the number of data points associated with the leaf nodes and then prune back the resulting tree. the pruning is based on a criterion that balances residual error against a measure of model complexity. if we denote the starting tree for pruning by then we define t to be a subtree of if it can be obtained by pruning nodes from other words by collapsing internal nodes by combining the corresponding regions. suppose the leaf nodes are indexed by with leaf node representing a region r of input space having n data points and denoting the total number of leaf nodes. the optimal prediction for region r is then given by y tn and the corresponding contribution to the residual sum-of-squares is then q y xn r n xn r the pruning criterion is then given by ct q the regularization parameter determines the trade-off between the overall residual sum-of-squares error and the complexity of the model as measured by the number of leaf nodes and its value is chosen by cross-validation. for classification problems the process of growing and pruning the tree is similar except that the sum-of-squares error is replaced by a more appropriate measure combining models of performance. if we define p k to be the proportion of data points in region r assigned to class k where k k then two commonly used choices are the cross-entropy q p k ln p k and the gini index q p k p k exercise these both vanish for p k and p k and have a maximum at p k they encourage the formation of regions in which a high proportion of the data points are assigned to one class. the cross entropy and the gini index are better measures than the misclassification rate for growing the tree because they are more sensitive to the node probabilities. also unlike misclassification rate they are differentiable and hence better suited to gradient based optimization methods. for subsequent pruning of the tree the misclassification rate is generally used. the human interpretability of a tree model such as cart is often seen as its major strength. however in practice it is found that the particular tree structure that is learned is very sensitive to the details of the data set so that a small change to the training data can result in a very different set of splits et al. there are other problems with tree-based methods of the kind considered in this section. one is that the splits are aligned with the axes of the feature space which may be very suboptimal. for instance to separate two classes whose optimal decision boundary runs at degrees to the axes would need a large number of axis-parallel splits of the input space as compared to a single non-axis-aligned split. furthermore the splits in a decision tree are hard so that each region of input space is associated with one and only one leaf node model. the last issue is particularly problematic in regression where we are typically aiming to model smooth functions and yet the tree model produces piecewise-constant predictions with discontinuities at the split boundaries. conditional mixture models we have seen that standard decision trees are restricted by hard axis-aligned splits of the input space. these constraints can be relaxed at the expense of interpretability by allowing soft probabilistic splits that can be functions of all of the input variables not just one of them at a time. if we also give the leaf models a probabilistic interpretation we arrive at a fully probabilistic tree-based model called the hierarchical mixture of experts which we consider in section an alternative way to motivate the hierarchical mixture of experts model is to start with a standard probabilistic mixtures of unconditional density models such as gaussians and replace the component densities with conditional distributions. here we consider mixtures of linear regression models and mixtures of chapter conditional mixture models logistic regression models in the simplest case the mixing coefficients are independent of the input variables. if we make a further generalization to allow the mixing coefficients also to depend on the inputs then we obtain a mixture of experts model. finally if we allow each component in the mixture model to be itself a mixture of experts model then we obtain a hierarchical mixture of experts. mixtures of linear regression models one of the many advantages of giving a probabilistic interpretation to the linear regression model is that it can then be used as a component in more complex probabilistic models. this can be done for instance by viewing the conditional distribution representing the linear regression model as a node in a directed probabilistic graph. here we consider a simple example corresponding to a mixture of linear regression models which represents a straightforward extension of the gaussian mixture model discussed in section to the case of conditional gaussian distributions. we therefore consider k linear regression models each governed by its own weight parameter wk. in many applications it will be appropriate to use a common noise variance governed by a precision parameter for all k components and this is the case we consider here. we will once again restrict attention to a single target variable t though the extension to multiple outputs is straightforward. if we denote the mixing coefficients by k then the mixture distribution can be written pt kn k where denotes the set of all adaptive parameters in the model namely w k and the log likelihood function for this model given a data set of observations n tn then takes the form ln pt ln kn k n where t tnt denotes the vector of target variables. in order to maximize this likelihood function we can once again appeal to the em algorithm which will turn out to be a simple extension of the em algorithm for unconditional gaussian mixtures of section we can therefore build on our experience with the unconditional mixture and introduce a set z of binary latent variables where znk in which for each data point n all of the elements k k are zero except for a single value of indicating which component of the mixture was responsible for generating that data point. the joint distribution over latent and observed variables can be represented by the graphical model shown in figure the complete-data log likelihood function then takes the form ln pt z znk ln kn k n exercise exercise combining models figure probabilistic directed graph representing a mixture of linear regression models defined by w zn n tn n the em algorithm begins by first choosing an initial value old for the model parameters. in the e step these parameter values are then used to evaluate the posterior probabilities or responsibilities of each component k for every data point n given by nk eznk pk n old kn j jn k n j n exercise the responsibilities are then used to determine the expectation with respect to the posterior distribution pzt old of the complete-data log likelihood which takes the form q old ez pt z in the m step we maximize the function q old with respect to keeping the nk fixed. for the optimization with respect to the mixing coefficients k we need k k which can be done with the aid of a to take account of the constraint lagrange multiplier leading to an m-step re-estimation equation for k in the form ln k lnn k n nk k n nk. note that this has exactly the same form as the corresponding result for a simple mixture of unconditional gaussians given by next consider the maximization with respect to the parameter vector wk of the kth linear regression model. substituting for the gaussian distribution we see that the function q old as a function of the parameter vector wk takes the form nk q old tn wt k n const where the constant term includes the contributions from other weight vectors wj for j k. note that the quantity we are maximizing is similar to the of the standard sum-of-squares error for a single linear regression model but with the inclusion of the responsibilities nk. this represents a weighted least squares conditional mixture models problem in which the term corresponding to the nth data point carries a weighting coefficient given by nk which could be interpreted as an effective precision for each data point. we see that each component linear regression model in the mixture governed by its own parameter vector wk is fitted separately to the whole data set in the m step but with each data point n weighted by the responsibility nk that model k takes for that data point. setting the derivative of with respect to wk equal to zero gives nk tn wt k n n which we can write in matrix notation as where rk diag nk is a diagonal matrix of size n n. solving for wk we obtain trkt wk this represents a set of modified normal equations corresponding to the weighted least squares problem of the same form as found in the context of logistic regression. note that after each e step the matrix rk will change and so we will have to solve the normal equations afresh in the subsequent m step. finally we maximize q old with respect to keeping only terms that trkt. wk trk depend on the function q old can be written q old ln tn wt k n nk setting the derivative with respect to equal to zero and rearranging we obtain the m-step equation for in the form n nk tn wt k n in figure we illustrate this em algorithm using the simple example of fitting a mixture of two straight lines to a data set having one input variable x and one target variable t. the predictive density is plotted in figure using the converged parameter values obtained from the em algorithm corresponding to the right-hand plot in figure also shown in this figure is the result of fitting a single linear regression model which gives a unimodal predictive density. we see that the mixture model gives a much better representation of the data distribution and this is reflected in the higher likelihood value. however the mixture model also assigns significant probability mass to regions where there is no data because its predictive distribution is bimodal for all values of x. this problem can be resolved by extending the model to allow the mixture coefficients themselves to be functions of x leading to models such as the mixture density networks discussed in section and hierarchical mixture of experts discussed in section combining models figure example of a synthetic data set shown by the green points having one input variable x and one target variable t together with a mixture of two linear regression models whose mean functions yx wk where k are shown by the blue and red lines. the upper three plots show the initial configuration the result of running iterations of em and the result after iterations of em here was initialized to the reciprocal of the true variance of the set of target values. the lower three plots show the corresponding responsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the posterior probability of the blue line for that data point similarly for the red segment. mixtures of logistic models because the logistic regression model defines a conditional distribution for the target variable given the input vector it is straightforward to use it as the component distribution in a mixture model thereby giving rise to a richer family of conditional distributions compared to a single logistic regression model. this example involves a straightforward combination of ideas encountered in earlier sections of the book and will help consolidate these for the reader. the conditional distribution of the target variable for a probabilistic mixture of k logistic regression models is given by pt kyt k t where is the feature vector yk denotes the adjustable parameters namely k and is the output of component k and now suppose we are given a data set n tn. the corresponding likelihood k wt conditional mixture models figure the left plot shows the predictive conditional density corresponding to the converged solution in figure this gives a log likelihood value of a vertical slice through one of these plots at a particular value of x represents the corresponding conditional distribution ptx which we see is bimodal. the plot on the right shows the predictive density for a single linear regression model fitted to the same data set using maximum likelihood. this model has a smaller log likelihood of function is then given by pt kytn nk tn where ynk k n and t tnt. we can maximize this likelihood function iteratively by making use of the em algorithm. this involves introducing latent variables znk that correspond to a coded binary indicator variable for each data point n. the complete-data likelihood function is then given by pt z kytn nk tn where z is the matrix of latent variables with elements znk. we initialize the em algorithm by choosing an initial value old for the model parameters. in the e step we then use these parameter values to evaluate the posterior probabilities of the components k for each data point n which are given by nk eznk pk n old kytn j jytn nk tn nj tn these responsibilities are then used to find the expected complete-data log likelihood as a function of given by q old ez pt z nk k tn ln ynk tn ln ynk combining models n the m step involves maximization of this function with respect to keeping old and hence nk fixed. maximization with respect to k can be done in the usual way k k giving with a lagrange multiplier to enforce the summation constraint the familiar result k nk. to determine the we note that the q old function comprises a sum over terms indexed by k each of which depends only on one of the vectors wk so that the different vectors are decoupled in the m step of the em algorithm. in other words the different components interact only via the responsibilities which are fixed during the m step. note that the m step does not have a closed-form solution and must be solved iteratively using for instance the iterative reweighted least squares algorithm. the gradient and the hessian for the vector wk are given by kq nktn ynk n hk k kq ynk n t n section where k denotes the gradient with respect to wk. for fixed nk these are independent of for j k and so we can solve for each wk separately using the irls algorithm. thus the m-step equations for component k correspond simply to fitting a single logistic regression model to a weighted data set in which data point n carries a weight nk. figure shows an example of the mixture of logistic regression models applied to a simple classification problem. the extension of this model to a mixture of softmax models for more than two classes is straightforward. section exercise mixtures of experts in section we considered a mixture of linear regression models and in section we discussed the analogous mixture of linear classifiers. although these simple mixtures extend the flexibility of linear models to include more complex multimodal predictive distributions they are still very limited. we can further increase the capability of such models by allowing the mixing coefficients themselves to be functions of the input variable so that ptx kxpktx. this is known as a mixture of experts model et al. in which the mixing coefficients kx are known as gating functions and the individual component densities pktx are called experts. the notion behind the terminology is that different components can model the distribution in different regions of input space conditional mixture models figure illustration of a mixture of logistic regression models. the left plot shows data points drawn from two classes denoted red and blue in which the background colour varies from pure red to pure blue denotes the true probability of the class label. the centre plot shows the result of fitting a single logistic regression model using maximum likelihood in which the background colour denotes the corresponding probability of the class label. because the colour is a near-uniform purple we see that the model assigns a probability of around to each of the classes over most of input space. the right plot shows the result of fitting a mixture of two logistic regression models which now gives much higher probability to the correct labels for many of the points in the blue class. are experts at making predictions in their own regions and the gating functions determine which components are dominant in which region. the gating functions kx must satisfy the usual constraints for mixing coefficients namely kx and k kx they can therefore be represented for example by linear softmax models of the form and if the experts are also linear or classification models then the whole model can be fitted efficiently using the em algorithm with iterative reweighted least squares being employed in the m step and jacobs such a model still has significant limitations due to the use of linear models for the gating and expert functions. a much more flexible model is obtained by using a multilevel gating function to give the hierarchical mixture of experts or hme model and jacobs to understand the structure of this model imagine a mixture distribution in which each component in the mixture is itself a mixture distribution. for simple unconditional mixtures this hierarchical mixture is trivially equivalent to a single flat mixture distribution. however when the mixing coefficients are input dependent this hierarchical model becomes nontrivial. the hme model can also be viewed as a probabilistic version of decision trees discussed in section and can again be trained efficiently by maximum likelihood using an em algorithm with irls in the m step. a bayesian treatment of the hme has been given by bishop and svens en based on variational inference. we shall not discuss the hme in detail here. however it is worth pointing out the close connection with the mixture density network discussed in section the principal advantage of the mixtures of experts model is that it can be optimized by em in which the m step for each mixture component and gating model involves a convex optimization the overall optimization is nonconvex. by contrast the advantage of the mixture density network approach is that the component exercise section combining models exercises densities and the mixing coefficients share the hidden units of the neural network. furthermore in the mixture density network the splits of the input space are further relaxed compared to the hierarchical mixture of experts in that they are not only soft and not constrained to be axis aligned but they can also be nonlinear. www consider a set models of the form ptx zh h h in which x is the input vector t is the target vector h indexes the different models zh is a latent variable for model h and h is the set of parameters for model h. suppose the models have prior probabilities ph and that we are given a training set x xn and t tn. write down the formulae needed to evaluate the predictive distribution ptx x t in which the latent variables and the model index are marginalized out. use these formulae to highlight the difference between bayesian averaging of different models and the use of latent variables within a single model. the expected sum-of-squares error eav for a simple committee model can be defined by and the expected error of the committee itself is given by assuming that the individual errors satisfy and derive the result www by making use of jensen s inequality for the special case of the convex function fx show that the average expected sum-of-squares error eav of the members of a simple committee model given by and the expected error ecom of the committee itself given by satisfy ecom eav. by making use of jensen s in equality show that the result derived in the previous exercise hods for any error function ey not just sum-ofsquares provided it is a convex function of y. www consider a committee in which we allow unequal weighting of the constituent models so that ycomx mymx. in order to ensure that the predictions ycomx remain within sensible limits suppose that we require that they be bounded at each value of x by the minimum and maximum values given by any of the members of the committee so that yminx ycomx ymaxx. show that a necessary and sufficient condition for this constraint is that the coefficients m satisfy m m exercises www by differentiating the error function with respect to m show that the parameters m in the adaboost algorithm are updated using in which is defined by by making a variational minimization of the expected exponential error function given by with respect to all possible functions yx show that the minimizing function is given by show that the exponential error function which is minimized by the adaboost algorithm does not correspond to the log likelihood of any well-behaved probabilistic model. this can be done by showing that the corresponding conditional distribution ptx cannot be correctly normalized. www show that the sequential minimization of the sum-of-squares error function for an additive model of the form in the style of boosting simply involves fitting each new base classifier to the residual errors tn fm from the previous model. verify that if we minimize the sum-of-squares error between a set of training values and a single predictive value t then the optimal solution for t is given by the mean of the consider a data set comprising data points from class and data points from class suppose that a tree model a splits these into at the first leaf node and at the second leaf node where m denotes that n points are assigned to and m points are assigned to similarly suppose that a second tree model b splits them into and evaluate the misclassification rates for the two trees and hence show that they are equal. similarly evaluate the cross-entropy and gini index for the two trees and show that they are both lower for tree b than for tree a. extend the results of section for a mixture of linear regression models to the case of multiple target values described by a vector t. to do this make use of the results of section www verify that the complete-data log likelihood function for the mixture of linear regression models is given by use the technique of lagrange multipliers e to show that the m-step re-estimation equation for the mixing coefficients in the mixture of linear regression models trained by maximum likelihood em is given by www we have already noted that if we use a squared loss function in a regression problem the corresponding optimal prediction of the target variable for a new input vector is given by the conditional mean of the predictive distribution. show that the conditional mean for the mixture of linear regression models discussed in section is given by a linear combination of the means of each component distribution. note that if the conditional distribution of the target data is multimodal the conditional mean can give poor predictions. combining models extend the logistic regression mixture model of section to a mixture of softmax classifiers representing c classes. write down the em algorithm for determining the parameters of this model through maximum likelihood. www consider a mixture model for a conditional distribution ptx of the form ptx k ktx in which each mixture component ktx is itself a mixture model. show that this two-level hierarchical mixture is equivalent to a conventional single-level mixture model. now suppose that the mixing coefficients in both levels of such a hierarchical model are arbitrary functions of x. again show that this hierarchical model is again equivalent to a single-level model with x-dependent mixing coefficients. finally consider the case in which the mixing coefficients at both levels of the hierarchical mixture are constrained to be linear classification or softmax models. show that the hierarchical mixture cannot in general be represented by a single-level mixture having linear classification models for the mixing coefficients. hint to do this it is sufficient to construct a single counter-example so consider a mixture of two components in which one of those components is itself a mixture of two components with mixing coefficients given by linear-logistic models. show that this cannot be represented by a single-level mixture of components having mixing coefficients determined by a linear-softmax model. appendix a. data sets in this appendix we give a brief introduction to the data sets used to illustrate some of the algorithms described in this book. detailed information on file formats for these data sets as well as the data files themselves can be obtained from the book web site httpresearch.microsoft.com cmbishopprml handwritten digits the digits data used in this book is taken from the mnist data set et al. which itself was constructed by modifying a subset of the much larger data set produced by nist national institute of standards and technology. it comprises a training set of examples and a test set of examples. some of the data was collected from census bureau employees and the rest was collected from high-school children and care was taken to ensure that the test examples were written by different individuals to the training examples. the original nist data had binary or white pixels. to create mnist these images were size normalized to fit in a pixel box while preserving their aspect ratio. as a consequence of the anti-aliasing used to change the resolution of the images the resulting mnist digits are grey scale. these images were then centred in a box. examples of the mnist digits are shown in figure error rates for classifying the digits range from for a simple linear classifier through for a carefully designed support vector machine to for a convolutional neural network et al. a. data sets figure one hundred examples of the mnist digits chosen at random from the training set. oil flow this is a synthetic data set that arose out of a project aimed at measuring noninvasively the proportions of oil water and gas in north sea oil transfer pipelines and james it is based on the principle of dual-energy gamma densitometry. the ideas is that if a narrow beam of gamma rays is passed through the pipe the attenuation in the intensity of the beam provides information about the density of material along its path. thus for instance the beam will be attenuated more strongly by oil than by gas. a single attenuation measurement alone is not sufficient because there are two degrees of freedom corresponding to the fraction of oil and the fraction of water fraction of gas is redundant because the three fractions must add to one. to address this two gamma beams of different energies other words different frequencies or wavelengths are passed through the pipe along the same path and the attenuation of each is measured. because the absorbtion properties of different materials vary differently as a function of energy measurement of the attenuations at the two energies provides two independent pieces of information. given the known absorbtion properties of oil water and gas at the two energies it is then a simple matter to calculate the average fractions of oil and water hence of gas measured along the path of the gamma beams. there is a further complication however associated with the motion of the materials along the pipe. if the flow velocity is small then the oil floats on top of the water with the gas sitting above the oil. this is known as a laminar or stratified figure the three geometrical configurations of the oil water and gas phases used to generate the oilflow data set. for each configuration the proportions of the three phases can vary. a. data sets stratified annular oil water gas mix homogeneous flow configuration and is illustrated in figure as the flow velocity is increased more complex geometrical configurations of the oil water and gas can arise. for the purposes of this data set two specific idealizations are considered. in the annular configuration the oil water and gas form concentric cylinders with the water around the outside and the gas in the centre whereas in the homogeneous configuration the oil water and gas are assumed to be intimately mixed as might occur at high flow velocities under turbulent conditions. these configurations are also illustrated in figure we have seen that a single dual-energy beam gives the oil and water fractions measured along the path length whereas we are interested in the volume fractions of oil and water. this can be addressed by using multiple dual-energy gamma densitometers whose beams pass through different regions of the pipe. for this particular data set there are six such beams and their spatial arrangement is shown in figure a single observation is therefore represented by a vector comprising the fractions of oil and water measured along the paths of each of the beams. we are however interested in obtaining the overall volume fractions of the three phases in the pipe. this is much like the classical problem of tomographic reconstruction used in medical imaging for example in which a two-dimensional dis figure cross section of the pipe showing the arrangement of the six beam lines each of which comprises a single dualenergy gamma densitometer. note that the vertical beams are asymmetrically arranged relative to the central axis by the dotted line. a. data sets tribution is to be reconstructed from an number of one-dimensional averages. here there are far fewer line measurements than in a typical tomography application. on the other hand the range of geometrical configurations is much more limited and so the configuration as well as the phase fractions can be predicted with reasonable accuracy from the densitometer data. for safety reasons the intensity of the gamma beams is kept relatively weak and so to obtain an accurate measurement of the attenuation the measured beam intensity is integrated over a specific time interval. for a finite integration time there are random fluctuations in the measured intensity due to the fact that the gamma beams comprise discrete packets of energy called photons. in practice the integration time is chosen as a compromise between reducing the noise level requires a long integration time and detecting temporal variations in the flow requires a short integration time. the oil flow data set is generated using realistic known values for the absorption properties of oil water and gas at the two gamma energies used and with a specific choice of integration time seconds chosen as characteristic of a typical practical setup. each point in the data set is generated independently using the following steps choose one of the three phase configurations at random with equal probability. choose three random numbers and from the uniform distribution over and define foil fwater this treats the three phases on an equal footing and ensures that the volume fractions add to one. for each of the six beam lines calculate the effective path lengths through oil and water for the given phase configuration. perturb the path lengths using the poisson distribution based on the known beam intensities and integration time to allow for the effect of photon statistics. each point in the data set comprises the path length measurements together with the fractions of oil and water and a binary label describing the phase configuration. the data set is divided into training validation and test sets each of which comprises independent data points. details of the data format are available from the book web site. in bishop and james statistical machine learning techniques were used to predict the volume fractions and also the geometrical configuration of the phases shown in figure from the vector of measurements. the observation vectors can also be used to test data visualization algorithms. this data set has a rich and interesting structure as follows. for any given configuration there are two degrees of freedom corresponding to the fractions of a. data sets oil and water and so for infinite integration time the data will locally live on a twodimensional manifold. for a finite integration time the individual data points will be perturbed away from the manifold by the photon noise. in the homogeneous phase configuration the path lengths in oil and water are linearly related to the fractions of oil and water and so the data points lie close to a linear manifold. for the annular configuration the relationship between phase fraction and path length is nonlinear and so the manifold will be nonlinear. in the case of the laminar configuration the situation is even more complex because small variations in the phase fractions can cause one of the horizontal phase boundaries to move across one of the horizontal beam lines leading to a discontinuous jump in the observation space. in this way the two-dimensional nonlinear manifold for the laminar configuration is broken into six distinct segments. note also that some of the manifolds for different phase configurations meet at specific points for example if the pipe is filled entirely with oil it corresponds to specific instances of the laminar annular and homogeneous configurations. old faithful old faithful shown in figure is a hydrothermal geyser in yellowstone national park in the state of wyoming u.s.a. and is a popular tourist attraction. its name stems from the supposed regularity of its eruptions. the data set comprises observations each of which represents a single eruption and contains two variables corresponding to the duration in minutes of the eruption and the time until the next eruption also in minutes. figure shows a plot of the time to the next eruption versus the duration of the eruptions. it can be seen that the time to the next eruption varies considerably although knowledge of the duration of the current eruption allows it to be predicted more accurately. note that there exist several other data sets relating to the eruptions of old faithful. figure the old faithful geyser national t. gourley in park. www.brucegourley.com. yellowstone a. data sets figure plot of the time to the next eruption in minutes axis versus the duration of the eruption in minutes axis for the old faithful data set. synthetic data throughout the book we use two simple synthetic data sets to illustrate many of the algorithms. the first of these is a regression problem based on the sinusoidal function shown in figure the input values are generated uniformly in range and the corresponding target values are obtained by first computing the corresponding values of the function x and then adding random noise with a gaussian distribution having standard deviation various forms of this data set having different numbers of data points are used in the book. the second data set is a classification problem having two classes with equal prior probabilities and is shown in figure the blue class is generated from a single gaussian while the red class comes from a mixture of two gaussians. because we know the class priors and the class-conditional densities it is straightforward to evaluate and plot the true posterior probabilities as well as the minimum misclassification-rate decision boundary as shown in figure a. data sets t t x x figure the left-hand plot shows the synthetic regression data set along with the underlying sinusoidal function from which the data points were generated. the right-hand plot shows the true conditional distribution ptx from which the labels are generated in which the green curve denotes the mean and the shaded region spans one standard deviation on each side of the mean. figure the left plot shows the synthetic classification data set with data from the two classes shown in red and blue. on the right is a plot of the true posterior probabilities shown on a colour scale going from pure red denoting probability of the red class is to pure blue denoting probability of the red class is because these probabilities are known the optimal decision boundary for minimizing the misclassification rate corresponds to the contour along which the posterior probabilities for each class equal can be evaluated and is shown by the green curve. this decision boundary is also plotted on the left-hand figure. appendix b. probability distributions in this appendix we summarize the main properties of some of the most widely used probability distributions and for each distribution we list some key statistics such as the expectation ex the variance covariance the mode and the entropy hx. all of these distributions are members of the exponential family and are widely used as building blocks for more sophisticated probabilistic models. bernoulli this is the distribution for a single binary variable x representing for example the result of flipping a coin. it is governed by a single continuous parameter that represents the probability of x bernx x ex varx hx ln modex if otherwise the bernoulli is a special case of the binomial distribution for the case of a single observation. its conjugate prior for is the beta distribution. b. probability distributions beta this is a distribution over a continuous variable which is often used to represent the probability for some binary event. it is governed by two parameters a and b that are constrained by a and b to ensure that the distribution can be normalized. beta b b a e var mode a a b ab b a a b the beta is the conjugate prior for the bernoulli distribution for which a and b can be interpreted as the effective prior number of observations of x and x respectively. its density is finite if a and b otherwise there is a singularity at andor for a b it reduces to a uniform distribution. the beta distribution is a special case of the k-state dirichlet distribution for k binomial the binomial distribution gives the probability of observing m occurrences of x in a set of n samples from a bernoulli distribution where the probability of observing x is m binmn n m em n varm n modem where denotes the largest integer that is less than or equal to and the quantity n m n! m!n m! denotes the number of ways of choosing m objects out of a total of n identical objects. here m! pronounced factorial m denotes the product m the particular case of the binomial distribution for n is known as the bernoulli distribution and for large n the binomial distribution is approximately gaussian. the conjugate prior for is the beta distribution. b. probability distributions dirichlet the dirichlet is a multivariate distribution over k random variables k where k k subject to the constraints k k denoting kt and kt we have dir c k k e k var k k k eln k k h k cov j k j k mode k k k ln c c k. k d da ln where and here is known as the digamma function and stegun the parameters k are subject to the constraint k in order to ensure that the distribution can be normalized. the dirichlet forms the conjugate prior for the multinomial distribution and represents a generalization of the beta distribution. in this case the parameters k can be interpreted as effective numbers of observations of the corresponding values of the k-dimensional binary observation vector x. as with the beta distribution the dirichlet has finite density everywhere provided k for all k. b. probability distributions gamma the gamma is a probability distribution over a positive random variable governed by parameters a and b that are subject to the constraints a and b to ensure that the distribution can be normalized. gam b ba a b e a b var a mode a eln ln b h ln ln b a for b where is the digamma function defined by the gamma distribution is the conjugate prior for the precision variance of a univariate gaussian. for a the density is everywhere finite and the special case of a is known as the exponential distribution. gaussian the gaussian is the most widely used distribution for continuous variables. it is also known as the normal distribution. in the case of a single variable x it is governed by two parameters the mean and the variance n exp ex varx modex hx ln the inverse of the variance is called the precision and the square root of the variance is called the standard deviation. the conjugate prior for is the gaussian and the conjugate prior for is the gamma distribution. if both and are unknown their joint conjugate prior is the gaussian-gamma distribution. for a d-dimensional vector x the gaussian is governed by a d-dimensional mean vector and a d d covariance matrix that must be symmetric and b. probability distributions positive-definite. n exp ex covx modex hx ln d is the precision matrix which is also the inverse of the covariance matrix symmetric and positive definite. averages of random variables tend to a gaussian by the central limit theorem and the sum of two gaussian variables is again gaussian. the gaussian is the distribution that maximizes the entropy for a given variance covariance. any linear transformation of a gaussian random variable is again gaussian. the marginal distribution of a multivariate gaussian with respect to a subset of the variables is itself gaussian and similarly the conditional distribution is also gaussian. the conjugate prior for is the gaussian the conjugate prior for is the wishart and the conjugate prior for is the gaussian-wishart. if we have a marginal gaussian distribution for x and a conditional gaussian distribution for y given x in the form px n pyx n b l then the marginal distribution of y and the conditional distribution of x given y are given by py n b l a pxy n b where if we have a joint gaussian distribution n with and we atla define the following partitions x xa xb aa ab ba bb a b aa ab ba bb then the conditional distribution pxaxb is given by aa pxaxb n ab ab a aa abxb b b. probability distributions and the marginal distribution pxa is given by pxa n a aa. gaussian-gamma this is the conjugate prior distribution for a univariate gaussian n in which the mean and the precision are both unknown and is also called the normal-gamma distribution. it comprises the product of a gaussian distribution for whose precision is proportional to and a gamma distribution over p a b o gam b. gaussian-wishart this is the conjugate prior distribution for a multivariate gaussian n in which both the mean and the precision are unknown and is also called the normal-wishart distribution. it comprises the product of a gaussian distribution for whose precision is proportional to and a wishart distribution over p w w for the particular case of a scalar x this is equivalent to the gaussian-gamma distribution. multinomial if we generalize the bernoulli distribution to an k-dimensional binary variable x with components xk such that k xk then we obtain the following discrete distribution px xk k exk k varxk k covxjxk ijk k hx k ln k b. probability distributions where ijk is the j k element of the identity matrix. because pxk k the parameters must satisfy k and k k the multinomial distribution is a multivariate generalization of the binomial and gives the distribution over counts mk for a k-state discrete variable to be in state k given a total number of observations n. mk n n mm emk n k varmk n k covmjmk n j k where kt and the quantity n mk n! mk! mk k gives the number of ways of taking n identical objects and assigning mk of them to bin k for k k. the value of k gives the probability of the random variable taking state k and so these parameters are subject to the constraints k k k the conjugate prior distribution for the parameters k is the and dirichlet. normal the normal distribution is simply another name for the gaussian. in this book we use the term gaussian throughout although we retain the conventional use of the symbol n to denote this distribution. for consistency we shall refer to the normalgamma distribution as the gaussian-gamma distribution and similarly the normalwishart is called the gaussian-wishart. student s t this distribution was published by william gosset in but his employer guiness breweries required him to publish under a pseudonym so he chose student in the univariate form student s t-distribution is obtained by placing a conjugate gamma prior over the precision of a univariate gaussian distribution and then integrating out the precision variable. it can therefore be viewed as an infinite mixture b. probability distributions of gaussians having the same mean but different variances. stx ex modex varx for for here is called the number of degrees of freedom of the distribution. the particular case of is called the cauchy distribution. for a d-dimensional variable x student s t-distribution corresponds to marginalizing the precision matrix of a multivariate gaussian with respect to a conjugate wishart prior and takes the form stx ex for covx modex for where is the squared mahalanobis distance defined by in the limit the t-distribution reduces to a gaussian with mean and precision student s t-distribution provides a generalization of the gaussian whose maximum likelihood parameter values are robust to outliers. uniform this is a simple distribution for a continuous variable x defined over a finite interval x b where b a. uxa b ex b a a varx hx lnb a. if x has distribution then a ax will have distribution uxa b. b. probability distributions von mises the von mises distribution also known as the circular normal or the circular gaussian is a univariate gaussian-like periodic distribution for a variable p m expm cos where is the zeroth-order bessel function of the first kind. the distribution has period so that p p for all care must be taken in interpreting this distribution because simple expectations will be dependent on the choice of origin for the variable the parameter is analogous to the mean of a univariate gaussian and the parameter m known as the concentration parameter is analogous to the precision variance. for large m the von mises distribution is approximately a gaussian centred on wishart the wishart distribution is the conjugate prior for the precision matrix of a multivariate gaussian. w bw d exp where bw dd trw i e w e i d ln lnw h ln bw d where w is a d d symmetric positive definite matrix and is the digamma function defined by the parameter is called the number of degrees of freedom of the distribution and is restricted to d to ensure that the gamma function in the normalization factor is well-defined. in one dimension the wishart reduces to the gamma distribution gam b given by with parameters a and b e d appendix c. properties of matrices in this appendix we gather together some useful properties and identities involving matrices and determinants. this is not intended to be an introductory tutorial and it is assumed that the reader is already familiar with basic linear algebra. for some results we indicate how to prove them whereas in more complex cases we leave the interested reader to refer to standard textbooks on the subject. in all cases we assume that inverses exist and that matrix dimensions are such that the formulae are correctly defined. a comprehensive discussion of linear algebra can be found in golub and van loan and an extensive collection of matrix properties is given by l utkepohl matrix derivatives are discussed in magnus and neudecker basic matrix identities a matrix a has elements aij where i indexes the rows and j indexes the columns. we use in to denote the n n identity matrix called the unit matrix and where there is no ambiguity over dimensionality we simply use i. the transpose matrix at has elements aji. from the definition of transpose we have which can be verified by writing out the indices. the inverse of a denoted a satisfies btat aa a i. because abb i we have also we have b at a c. properties of matrices which is easily proven by taking the transpose of and applying a useful identity involving matrix inverses is the following btr pbtbpbt r which is easily verified by right multiplying both sides by r. suppose that p has dimensionality n n while r has dimensionality m m so that b is m n. then if m n it will be much cheaper to evaluate the right-hand side of than the left-hand side. a special case that sometimes arises is ab ai ba another useful identity involving inverses is the following bd a a ca which is known as the woodbury identity and which can be verified by multiplying both sides by bd this is useful for instance when a is large and diagonal and hence easy to invert while b has many rows but few columns conversely for c so that the right-hand side is much cheaper to evaluate than the left-hand side. a set of vectors an is said to be linearly independent if the relation n nan holds only if all n this implies that none of the vectors can be expressed as a linear combination of the remainder. the rank of a matrix is the maximum number of linearly independent rows equivalently the maximum number of linearly independent columns. traces and determinants trace and determinant apply to square matrices. the trace tra of a matrix a is defined as the sum of the elements on the leading diagonal. by writing out the indices we see that trab trba. by applying this formula multiple times to the product of three matrices we see that trabc trcab trbca which is known as the cyclic property of the trace operator and which clearly extends to the product of any number of matrices. the determinant of an n n matrix a is defined by an in in which the sum is taken over all products consisting of precisely one element from each row and one element from each column with a coefficient or according c. properties of matrices to whether the permutation in is even or odd respectively. note that thus for a matrix the determinant takes the form the determinant of a product of two matrices is given by as can be shown from also the determinant of an inverse matrix is given by atb abt atb abt a useful special case is where a and b are n-dimensional column vectors. matrix derivatives which can be shown by taking the determinant of and applying if a and b are matrices of size n m then sometimes we need to consider derivatives of vectors and matrices with respect to scalars. the derivative of a vector a with respect to a scalar x is itself a vector whose components are given by a x ai x with an analogous definition for the derivative of a matrix. derivatives with respect to vectors and matrices can also be defined for instance i i x a x ai and similarly the following is easily proven by writing out the components a b ai bj ij x xta x atx a. c. properties of matrices similarly x a x b a b x the derivative of the inverse of a matrix can be expressed as a x a a x a as can be shown by differentiating the equation a i using and then right multiplying by a also lna tr x a a x which we shall prove later. if we choose x to be one of the elements of a we have aij tr bji as can be seen by writing out the matrices using index notation. we can write this result more compactly in the form with this notation we have the following properties tr bt. a a tr atb b a tra i trabat ab bt a a which can again be proven by writing out the matrix indices. we also have lna a which follows from and eigenvector equation for a square matrix a of size m m the eigenvector equation is defined by aui iui c. properties of matrices for i m where ui is an eigenvector and i is the corresponding eigenvalue. this can be viewed as a set of m simultaneous homogeneous linear equations and the condition for a solution is that ii which is known as the characteristic equation. because this is a polynomial of order m in i it must have m solutions these need not all be distinct. the rank of a is equal to the number of nonzero eigenvalues. of particular interest are symmetric matrices which arise as covariance matrices kernel matrices and hessians. symmetric matrices have the property that aij aji or equivalently at a. the inverse of a symmetric matrix is also symmetric as can be seen by taking the transpose of a i and using aa i together with the symmetry of i. in general the eigenvalues of a matrix are complex numbers but for symmetric matrices the eigenvalues i are real. this can be seen by first left multiplying by i where denotes the complex conjugate to give i aui i i ui. next we take the complex conjugate of and left multiply by ut i to give i ut i i ut i i where we have used a because we consider only real matrices a. taking the transpose of the second of these equations and using at a we see that the left-hand sides of the two equations are equal and hence that i i and so i must be real. the eigenvectors ui of a real symmetric matrix can be chosen to be orthonormal orthogonal and of unit length so that ut i uj iij where iij are the elements of the identity matrix i. to show this we first left multiply by ut j to give and hence by exchange of indices we have ut j aui iut j ui ut i auj jut i uj. we now take the transpose of the second equation and make use of the symmetry property at a and then subtract the two equations to give i j ut hence for i j we have ut i uj and hence ui and uj are orthogonal. if the two eigenvalues are equal then any linear combination ui uj is also an eigenvector with the same eigenvalue so we can select one linear combination arbitrarily i uj c. properties of matrices and then choose the second to be orthogonal to the first can be shown that the degenerate eigenvectors are never linearly dependent. hence the eigenvectors can be chosen to be orthogonal and by normalizing can be set to unit length. because there are m eigenvalues the corresponding m orthogonal eigenvectors form a complete set and so any m-dimensional vector can be expressed as a linear combination of the eigenvectors. we can take the eigenvectors ui to be the columns of an m m matrix u which from orthonormality satisfies utu i. such a matrix is said to be orthogonal. interestingly the rows of this matrix are also orthogonal so that uut i. to show this note that implies utuu u ut and so uu uut i. using it also follows that the eigenvector equation can be expressed in terms of u in the form where is an m m diagonal matrix whose diagonal elements are given by the eigenvalues i. au u if we consider a column vector x that is transformed by an orthogonal matrix u to give a new vector then the length of the vector is preserved because and similarly the angle between any two such vectors is preserved because ux xtutux xtx xtutuy xty. thus multiplication by u can be interpreted as a rigid rotation of the coordinate system. from it follows that utau and because is a diagonal matrix we say that the matrix a is diagonalized by the matrix u. if we left multiply by u and right multiply by ut we obtain taking the inverse of this equation and using together with u ut we have a u ut a u a a iuiut i i uiut i i. c. properties of matrices these last two equations can also be written in the form if we take the determinant of and use we obtain similarly taking the trace of and using the cyclic property of the trace operator together with utu i we have tra i. we leave it as an exercise for the reader to verify by making use of the results and a matrix a is said to be positive definite denoted by a if wtaw for all values of the vector w. equivalently a positive definite matrix has i for all of its eigenvalues can be seen by setting w to each of the eigenvectors in turn and by noting that an arbitrary vector can be expanded as a linear combination of the eigenvectors. note that positive definite is not the same as all the elements being positive. for example the matrix has eigenvalues and a matrix is said to be positive semidefinite if wtaw holds for all values of w which is denoted a and is equivalent to i appendix d. calculus of variations we can think of a function yx as being an operator that for any input value x returns an output value y. in the same way we can define a functional f to be an operator that takes a function yx and returns an output value f an example of a functional is the length of a curve drawn in a two-dimensional plane in which the path of the curve is defined in terms of a function. in the context of machine learning a widely used functional is the entropy hx for a continuous variable x because for any choice of probability density function px it returns a scalar value representing the entropy of x under that density. thus the entropy of px could equally well have been written as hp. a common problem in conventional calculus is to find a value of x that maximizes minimizes a function yx. similarly in the calculus of variations we seek a function yx that maximizes minimizes a functional f that is of all possible functions yx we wish to find the particular function for which the functional f is a maximum minimum. the calculus of variations can be used for instance to show that the shortest path between two points is a straight line or that the maximum entropy distribution is a gaussian. if we weren t familiar with the rules of ordinary calculus we could evaluate a conventional derivative dy dx by making a small change to the variable x and then expanding in powers of so that yx yx and finally taking the limit similarly for a function of several variables xd the corresponding partial derivatives are defined by dy dx xd xd y xi the analogous definition of a functional derivative arises when we consider how much a functional f changes when we make a small change to the function d. calculus of variations figure a functional derivative can be defined by considering how the value of a functional f changes when the function yx is changed to yx where is an arbitrary function of x. yx yx x yx where is an arbitrary function of x as illustrated in figure we denote the functional derivative of ef with respect to fx by f fx and define it by the following relation f f f yx dx this can be seen as a natural extension of in which f now depends on a continuous set of variables namely the values of y at all points x. requiring that the functional be stationary with respect to small variations in the function yx gives e yx dx because this must hold for an arbitrary choice of it follows that the functional derivative must vanish. to see this imagine choosing a perturbation that is zero everywhere except in the neighbourhood of a in which case the functional derivative must be zero at x however because this must be true for every choice the functional derivative must vanish for all values of x. consider a functional that is defined by an integral over a function gy y that depends on both yx and its derivative y dence on x f g y x as well as having a direct x dx where the value of yx is assumed to be fixed at the boundary of the region of integration might be at infinity. if we now consider variations in the function yx we obtain f f g y g dx we now have to cast this in the form to do so we integrate the second term by parts and make use of the fact that must vanish at the boundary of the integral yx is fixed at the boundary. this gives d dx f f dx g g y d. calculus of variations from which we can read off the functional derivative by comparison with requiring that the functional derivative vanishes then gives g y d dx g which are known as the euler-lagrange equations. for example if g then the euler-lagrange equations take the form yx this second order differential equation can be solved for yx by making use of the boundary conditions on yx. often we consider functionals defined by integrals whose integrands take the form gy x and that do not depend on the derivatives of yx. in this case stationarity simply requires that g yx for all values of x. if we are optimizing a functional with respect to a probability distribution then we need to maintain the normalization constraint on the probabilities. this is often most conveniently done using a lagrange multiplier which then allows an unconstrained optimization to be performed. the extension of the above results to a multidimensional variable x is straightforward. for a more comprehensive discussion of the calculus of variations see sagan appendix e appendix e. lagrange multipliers lagrange multipliers also sometimes called undetermined multipliers are used to find the stationary points of a function of several variables subject to one or more constraints. consider the problem of finding the maximum of a function subject to a constraint relating and which we write in the form one approach would be to solve the constraint equation and thus express as a function of in the form this can then be substituted into to give a function of alone of the form the maximum with respect to could then be found by differentiation in the usual way to give the stationary value with the corresponding value of given by one problem with this approach is that it may be difficult to find an analytic solution of the constraint equation that allows to be expressed as an explicit function of also this approach treats and differently and so spoils the natural symmetry between these variables. a more elegant and often simpler approach is based on the introduction of a parameter called a lagrange multiplier. we shall motivate this technique from a geometrical perspective. consider a d-dimensional variable x with components xd. the constraint equation gx then represents a surface in x-space as indicated in figure we first note that at any point on the constraint surface the gradient gx of the constraint function will be orthogonal to the surface. to see this consider a point x that lies on the constraint surface and consider a nearby point x that also lies on the surface. if we make a taylor expansion around x we have gx gx gx. because both x and x lie on the constraint surface we have gx gx and hence gx in the limit we have gx and because is e. lagrange multipliers figure a geometrical picture of the technique of lagrange multipliers in which we seek to maximize a function f subject to the constraint gx if x is d dimensional the constraint gx corresponds to a subspace of dimensionality d indicated by the red curve. the problem can be solved by optimizing the lagrangian function lx f gx. fx xa gx gx then parallel to the constraint surface gx we see that the vector g is normal to the surface. next we seek a point on the constraint surface such that fx is maximized. such a point must have the property that the vector fx is also orthogonal to the constraint surface as illustrated in figure because otherwise we could increase the value of fx by moving a short distance along the constraint surface. thus f and g are parallel anti-parallel vectors and so there must exist a parameter such that where is known as a lagrange multiplier. note that can have either sign. at this point it is convenient to introduce the lagrangian function defined by f g lx fx gx. the constrained stationarity condition is obtained by setting xl furthermore the condition l leads to the constraint equation gx thus to find the maximum of a function fx subject to the constraint gx we define the lagrangian function given by and we then find the stationary point of lx with respect to both x and for a d-dimensional vector x this gives d equations that determine both the stationary point and the value of if we are only interested in then we can eliminate from the stationarity equations without needing to find its value the term undetermined multiplier as a simple example suppose we wish to find the stationary point of the function subject to the constraint as illustrated in figure the corresponding lagrangian function is given by the conditions for this lagrangian to be stationary with respect to and give the following coupled equations lx e. lagrange multipliers figure a simple example of the use of lagrange multipliers in which the aim is to maximize f subject to the constraint where the circles show contours of the function f and the diagonal line shows the constraint surface solution of these equations then gives the stationary point as the corresponding value for the lagrange multiplier is and so far we have considered the problem of maximizing a function subject to an equality constraint of the form gx we now consider the problem of maximizing fx subject to an inequality constraint of the form gx as illustrated in figure there are now two kinds of solution possible according to whether the constrained stationary point lies in the region where gx in which case the constraint is inactive or whether it lies on the boundary gx in which case the constraint is said to be active. in the former case the function gx plays no role and so the stationary condition is simply fx this again corresponds to a stationary point of the lagrange function but this time with the latter case where the solution lies on the boundary is analogous to the equality constraint discussed previously and corresponds to a stationary point of the lagrange function with now however the sign of the lagrange multiplier is crucial because the function fx will only be at a maximum if its gradient is oriented away from the region gx as illustrated in figure we therefore have fx gx for some value of for either of these two cases the product gx thus the solution to the figure illustration of f subject gx the problem of maximizing to the inequality constraint fx xa gx xb gx gx e. lagrange multipliers problem of maximizing fx subject to gx is obtained by optimizing the lagrange function with respect to x and subject to the conditions gx gx these are known as the karush-kuhn-tucker conditions kuhn and tucker note that if we wish to minimize than maximize the function fx subject to an inequality constraint gx then we minimize the lagrangian function lx fx gx with respect to x again subject to finally it is straightforward to extend the technique of lagrange multipliers to the case of multiple equality and inequality constraints. suppose we wish to maximize fx subject to gjx for j j and hkx for k k. we then introduce lagrange multipliers j and k and then optimize the lagrangian function given by lx j k fx jgjx khkx appendix d subject to k and khkx for k k. extensions to constrained functional derivatives are similarly straightforward. for a more detailed discussion of the technique of lagrange multipliers see nocedal and wright references references abramowitz m. and i. a. stegun handbook of mathematical functions. dover. adler s. l. over-relaxation method for the monte carlo evaluation of the partition function for multiquadratic actions. physical review d ahn j. h. and j. h. oh a constrained em algorithm for principal component analysis. neural computation aizerman m. a. e. m. braverman and l. i. rozonoer the probability problem of pattern recognition learning and the method of potential functions. automation and remote control akaike h. a new look at statistical model identification. ieee transactions on automatic control ali s. m. and s. d. silvey a general class of coefficients of divergence of one distribution from another. journal of the royal statistical society b allwein e. l. r. e. schapire and y. singer reducing multiclass to binary a unifying approach for margin classifiers. journal of machine learning research amari s. differential-geometrical methods in statistics. springer. amari s. a. cichocki and h. h. yang a new learning algorithm for blind signal separation. in d. s. touretzky m. c. mozer and m. e. hasselmo advances in neural information processing systems volume pp. mit press. amari s. i. natural gradient works ciently in learning. neural computation anderson j. a. and e. rosenfeld neurocomputing foundations of research. mit press. anderson t. w. asymptotic theory for principal component analysis. annals of mathematical statistics andrieu c. n. de freitas a. doucet and m. i. jordan an introduction to mcmc for machine learning. machine learning anthony m. and n. biggs an introduction to computational learning theory. cambridge university press. attias h. independent factor analysis. neu ral computation attias h. inferring parameters and structure of latent variable models by variational bayes. in k. b. laskey and h. prade references uncertainty in artificial intelligence proceedings of the fifth conference pp. morgan kaufmann. bach f. r. and m. i. jordan kernel independent component analysis. journal of machine learning research bakir g. h. j. weston and b. sch olkopf learning to find pre-images. in s. thrun l. k. saul and b. sch olkopf advances in neural information processing systems volume pp. mit press. baldi p. and s. brunak bioinformatics the machine learning approach ed.. mit press. baldi p. and k. hornik neural networks and principal component analysis learning from examples without local minima. neural networks barber d. and c. m. bishop bayesian model comparison by monte carlo chaining. in m. mozer m. jordan and t. petsche advances in neural information processing systems volume pp. mit press. barber d. and c. m. bishop ensemble learning for multi-layer networks. in m. i. jordan k. j. kearns and s. a. solla advances in neural information processing systems volume pp. barber d. and c. m. bishop ensemble learning in bayesian neural networks. in c. m. bishop generalization in neural networks and machine learning pp. springer. bartholomew d. j. latent variable models and factor analysis. charles griffin. basilevsky a. statistical factor analysis and related methods theory and applications. wiley. baum l. e. an inequality and associated maximization technique in statistical estimation of probabilistic functions of markov processes. inequalities becker s. and y. le cun improving the convergence of back-propagation learning with second order methods. in d. touretzky g. e. hinton and t. j. sejnowski proceedings of the connectionist models summer school pp. morgan kaufmann. bell a. j. and t. j. sejnowski an information maximization approach to blind separation and blind deconvolution. neural computation bellman r. adaptive control processes a guided tour. princeton university press. bengio y. and p. frasconi an input output hmm architecture. in g. tesauro d. s. touretzky and t. k. leen advances in neural information processing systems volume pp. mit press. bennett k. p. robust linear programming discrimination of two linearly separable sets. optimization methods and software berger j. o. statistical decision theory and bayesian analysis ed.. springer. bernardo j. m. and a. f. m. smith bayesian theory. wiley. berrou c. a. glavieux and p. thitimajshima near shannon limit error-correcting coding and decoding turbo-codes in proceedings icc pp. besag j. on spatio-temporal models and markov fields. in transactions of the prague conference on information theory statistical decision functions and random processes pp. academia. bather j. decision theory an introduction to dynamic programming and sequential decisions. wiley. besag j. on the statistical analysis of dirty pictures. journal of the royal statistical society baudat g. and f. anouar generalized discriminant analysis using a kernel approach. neural computation besag j. p. j. green d. hidgon and k. megersen bayesian computation and stochastic systems. statistical science references bishop c. m. a fast procedure for retraining the multilayer perceptron. international journal of neural systems bishop c. m. exact calculation of the hessian matrix for the multilayer perceptron. neural computation bishop c. m. curvature-driven smoothing a learning algorithm for feedforward networks. ieee transactions on neural networks bishop c. m. novelty detection and neural network validation. iee proceedings vision image and signal processing special issue on applications of neural networks. bishop c. m. neural networks for pattern recognition. oxford university press. bishop c. m. training with noise is equivalent to tikhonov regularization. neural computation bishop c. m. bayesian pca. in m. s. kearns s. a. solla and d. a. cohn advances in neural information processing systems volume pp. mit press. variational principal bishop c. m. components. in proceedings ninth international conference on artificial neural networks icann volume pp. iee. bishop c. m. and g. d. james analysis of multiphase flows using dual-energy gamma densitometry and neural networks. nuclear instruments and methods in physics research bishop c. m. and i. t. nabney modelling conditional probability distributions for periodic variables. neural computation bishop c. m. and i. t. nabney pattern recognition and machine learning a matlab companion. springer. in preparation. bishop c. m. d. spiegelhalter and j. winn vibes a variational inference engine for bayesian networks. in s. becker s. thrun and k. obermeyer advances in neural information processing systems volume pp. mit press. bishop c. m. and m. svens en bayesian hierarchical mixtures of experts. in u. kjaerulff and c. meek proceedings nineteenth conference on uncertainty in artificial intelligence pp. morgan kaufmann. bishop c. m. m. svens en and g. e. hinton distinguishing text from graphics in online handwritten ink. in f. kimura and h. fujisawa proceedings ninth international workshop on frontiers in handwriting recognition tokyo japan pp. bishop c. m. m. svens en and c. k. i. williams em optimization of latent variable density models. in d. s. touretzky m. c. mozer and m. e. hasselmo advances in neural information processing systems volume pp. mit press. bishop c. m. m. svens en and c. k. i. williams gtm a principled alternative to the self-organizing map. in m. c. mozer m. i. jordan and t. petche advances in neural information processing systems volume pp. mit press. bishop c. m. m. svens en and c. k. i. williams magnification factors for the gtm algorithm. in proceedings iee fifth international conference on artificial neural networks cambridge u.k. pp. institute of electrical engineers. bishop c. m. m. svens en and c. k. i. williams developments of the generative topographic mapping. neurocomputing bishop c. m. m. svens en and c. k. i. williams gtm the generative topographic mapping. neural computation bishop c. m. and m. e. tipping a hierarchical latent variable model for data visualization. ieee transactions on pattern analysis and machine intelligence references bishop c. m. and j. winn non-linear bayesian image modelling. in proceedings sixth european conference on computer vision dublin volume pp. springer. blei d. m. m. i. jordan and a. y. ng hierarchical bayesian models for applications in information retrieval. in j. m. b. et al. bayesian statistics pp. oxford university press. block h. d. the perceptron a model for brain functioning. reviews of modern physics reprinted in anderson and rosenfeld blum j. a. multidimensional stochastic approximation methods. annals of mathematical statistics bodlaender h. a tourist guide through treewidth. acta cybernetica boser b. e. i. m. guyon and v. n. vapnik a training algorithm for optimal margin classifiers. in d. haussler proceedings fifth annual workshop on computational learning theory pp. acm. bourlard h. and y. kamp auto-association by multilayer perceptrons and singular value decomposition. biological cybernetics box g. e. p. g. m. jenkins and g. c. reinsel time series analysis. prentice hall. box g. e. p. and g. c. tao bayesian infer ence in statistical analysis. wiley. boyd s. and l. vandenberghe convex opti mization. cambridge university press. boyen x. and d. koller tractable inference for complex stochastic processes. in g. f. cooper and s. moral proceedings annual conference on uncertainty in artificial intelligence pp. morgan kaufmann. breiman l. bagging predictors. machine learning breiman l. j. h. friedman r. a. olshen and p. j. stone classification and regression trees. wadsworth. brooks s. p. markov chain monte carlo method and its application. the statistician broomhead d. s. and d. lowe multivariable functional interpolation and adaptive networks. complex systems buntine w. and a. weigend bayesian back propagation. complex systems buntine w. l. and a. s. weigend computing second derivatives in feed-forward networks a review. ieee transactions on neural networks burges c. j. c. a tutorial on support vector machines for pattern recognition. knowledge discovery and data mining cardoso j.-f. blind signal separation statistical principles. proceedings of the ieee casella g. and r. l. berger statistical in ference ed.. duxbury. castillo e. j. m. guti errez and a. s. hadi expert systems and probabilistic network models. springer. chan k. t. lee and t. j. sejnowski variational bayesian learning of ica with missing data. neural computation chen a. m. h. lu and r. hecht-nielsen on the geometry of feedforward neural network error surfaces. neural computation chen m. h. q. m. shao and j. g. ibrahim monte carlo methods for bayesian computation. springer. boykov y. o. veksler and r. zabih fast approximate energy minimization via graph cuts. ieee transactions on pattern analysis and machine intelligence chen s. c. f. n. cowan and p. m. grant orthogonal least squares learning algorithm for radial basis function networks. ieee transactions on neural networks choudrey r. a. and s. j. roberts variational mixture of bayesian independent component analyzers. neural computation clifford p. markov random fields in statistics. in g. r. grimmett and d. j. a. welsh disorder in physical systems. a volume in honour of john m. hammersley pp. oxford university press. collins m. s. dasgupta and r. e. schapire a generalization of principal component analysis to the exponential family. in t. g. dietterich s. becker and z. ghahramani advances in neural information processing systems volume pp. mit press. comon p. c. jutten and j. herault blind source separation problems statement. signal processing corduneanu a. and c. m. bishop variational bayesian model selection for mixture distributions. in t. richardson and t. jaakkola proceedings eighth international conference on artificial intelligence and statistics pp. morgan kaufmann. cormen t. h. c. e. leiserson r. l. rivest and c. stein introduction to algorithms ed.. mit press. cortes c. and v. n. vapnik support vector networks. machine learning cotter n. e. the stone-weierstrass theorem and its application to neural networks. ieee transactions on neural networks cover t. and p. hart nearest neighbor pattern classification. ieee transactions on information theory cover t. m. and j. a. thomas elements of information theory. wiley. cowell r. g. a. p. dawid s. l. lauritzen and d. j. spiegelhalter probabilistic networks and expert systems. springer. cox r. t. probability frequency and reasonable expectation. american journal of physics references cox t. f. and m. a. a. cox multidimensional scaling ed.. chapman and hall. cressie n. statistics for spatial data. wiley. cristianini n. and j. shawe-taylor support vector machines and other kernel-based learning methods. cambridge university press. csat o l. and m. opper sparse on-line gaussian processes. neural computation csiszar i. and g. tusnady information geometry and alternating minimization procedures. statistics and decisions cybenko g. approximation by superpositions of a sigmoidal function. mathematics of control signals and systems dawid a. p. conditional independence in statistical theory discussion. journal of the royal statistical society series b dawid a. p. conditional independence for statistical operations. annals of statistics definetti b. theory of probability. wiley and sons. dempster a. p. n. m. laird and d. b. rubin maximum likelihood from incomplete data via the em algorithm. journal of the royal statistical society b denison d. g. t. c. c. holmes b. k. mallick and a. f. m. smith bayesian methods for nonlinear classification and regression. wiley. diaconis p. and l. saloff-coste what do we know about the metropolis algorithm? journal of computer and system sciences dietterich t. g. and g. bakiri solving multiclass learning problems via error-correcting output codes. journal of artificial intelligence research duane s. a. d. kennedy b. j. pendleton and d. roweth hybrid monte carlo. physics letters b duda r. o. and p. e. hart pattern classifi cation and scene analysis. wiley. references duda r. o. p. e. hart and d. g. stork pat fletcher r. practical methods of optimiza tern classification ed.. wiley. tion ed.. wiley. durbin r. s. eddy a. krogh and g. mitchison biological sequence analysis. cambridge university press. dybowski r. and s. roberts an anthology of probabilistic models for medical informatics. in d. husmeier r. dybowski and s. roberts probabilistic modeling in bioinformatics and medical informatics pp. springer. efron b. bootstrap methods another look at the jackknife. annals of statistics elkan c. using the triangle inequality to accelerate k-means. in proceedings of the twelfth international conference on machine learning pp. aaai. elliott r. j. l. aggoun and j. b. moore hidden markov models estimation and control. springer. ephraim y. d. malah and b. h. juang on the application of hidden markov models for enhancing noisy speech. ieee transactions on acoustics speech and signal processing erwin e. k. obermayer and k. schulten self-organizing maps ordering convergence properties and energy functions. biological cybernetics everitt b. s. an introduction to latent vari able models. chapman and hall. faul a. c. and m. e. tipping analysis of sparse bayesian learning. in t. g. dietterich s. becker and z. ghahramani advances in neural information processing systems volume pp. mit press. feller w. an introduction to probability theory and its applications ed. volume wiley. feynman r. p. r. b. leighton and m. sands the feynman lectures of physics volume two. addison-wesley. chapter forsyth d. a. and j. ponce computer vi sion a modern approach. prentice hall. freund y. and r. e. schapire experiments with a new boosting algorithm. in l. saitta thirteenth international conference on machine learning pp. morgan kaufmann. frey b. j. graphical models for machine learning and digital communication. mit press. frey b. j. and d. j. c. mackay a revolution belief propagation in graphs with cycles. in m. i. jordan m. j. kearns and s. a. solla advances in neural information processing systems volume mit press. friedman j. h. greedy function approximation a gradient boosting machine. annals of statistics friedman j. h. t. hastie and r. tibshirani additive logistic regression a statistical view of boosting. annals of statistics friedman n. and d. koller being bayesian about network structure a bayesian approach to structure discovery in bayesian networks. machine learning frydenberg m. the chain graph markov property. scandinavian journal of statistics fukunaga k. introduction to statistical pattern recognition ed.. academic press. funahashi k. on the approximate realization of continuous mappings by neural networks. neural networks fung r. and k. c. chang weighting and integrating evidence for stochastic simulation in bayesian networks. in p. p. bonissone m. henrion l. n. kanal and j. f. lemmer uncertainty in artificial intelligence volume pp. elsevier. gallager r. g. low-density parity-check codes. mit press. gamerman d. markov chain monte carlo stochastic simulation for bayesian inference. chapman and hall. gelman a. j. b. carlin h. s. stern and d. b. rubin bayesian data analysis ed.. chapman and hall. geman s. and d. geman stochastic relaxation gibbs distributions and the bayesian restoration of images. ieee transactions on pattern analysis and machine intelligence ghahramani z. and m. j. beal variational inference for bayesian mixtures of factor analyzers. in s. a. solla t. k. leen and k. r. m uller advances in neural information processing systems volume pp. mit press. ghahramani z. and g. e. hinton the em algorithm for mixtures of factor analyzers. technical report university of toronto. ghahramani z. and g. e. hinton parameter estimation for linear dynamical systems. technical report university of toronto. ghahramani z. and g. e. hinton variational learning for switching state-space models. neural computation ghahramani z. and m. i. jordan supervised learning from incomplete data via an em appproach. in j. d. cowan g. t. tesauro and j. alspector advances in neural information processing systems volume pp. morgan kaufmann. ghahramani z. and m. i. jordan factorial hidden markov models. machine learning gibbs m. n. bayesian gaussian processes for regression and classification. phd thesis university of cambridge. gibbs m. n. and d. j. c. mackay variational gaussian process classifiers. ieee transactions on neural networks references gilks w. r. derivative-free adaptive rejection sampling for gibbs in j. bernardo j. berger a. p. dawid and a. f. m. smith bayesian statistics volume oxford university press. sampling. gilks w. r. n. g. best and k. k. c. tan adaptive rejection metropolis sampling. applied statistics gilks w. r. s. richardson and d. j. spiegelhalter markov chain monte carlo in practice. chapman and hall. gilks w. r. and p. wild adaptive rejection sampling for gibbs sampling. applied statistics gill p. e. w. murray and m. h. wright practical optimization. academic press. goldberg p. w. c. k. i. williams and c. m. bishop regression with input-dependent noise a gaussian process treatment. in advances in neural information processing systems volume pp. mit press. golub g. h. and c. f. van loan matrix computations ed.. john hopkins university press. good i. probability and the weighing of ev idence. hafners. gordon n. j. d. j. salmond and a. f. m. smith approach to nonlinearnoniee novel gaussian bayesian proceedings-f estimation. state graepel t. solving noisy linear operator equations by gaussian processes application to ordinary and partial differential equations. in proceedings of the twentieth international conference on machine learning pp. greig d. b. porteous and a. seheult exact maximum a-posteriori estimation for binary images. journal of the royal statistical society series b gull s. f. developments in maximum entropy data analysis. in j. skilling maximum entropy and bayesian methods pp. kluwer. references hassibi b. and d. g. stork second order derivatives for network pruning optimal brain surgeon. in s. j. hanson j. d. cowan and c. l. giles advances in neural information processing systems volume pp. morgan kaufmann. hastie t. and w. stuetzle principal curves. the american statistical associa journal of tion hastie t. r. tibshirani and j. friedman the elements of statistical learning. springer. hastings w. k. monte carlo sampling methods using markov chains and their applications. biometrika hathaway r. j. another interpretation of the em algorithm for mixture distributions. statistics and probability letters haussler d. convolution kernels on discrete structures. technical report university of california santa cruz computer science department. henrion m. propagation of uncertainty by logic sampling in bayes networks. in j. f. lemmer and l. n. kanal uncertainty in artificial intelligence volume pp. north holland. herbrich r. learning kernel classifiers. mit press. hertz j. a. krogh and r. g. palmer introduction to the theory of neural computation. addison wesley. hinton g. e. p. dayan and m. revow modelling the manifolds of images of handwritten digits. ieee transactions on neural networks hinton g. e. and d. van camp keeping neural networks simple by minimizing the description length of the weights. in proceedings of the sixth annual conference on computational learning theory pp. acm. hinton g. e. m. welling y. w. teh and s. osindero a new view of ica. in proceedings of the international conference on independent component analysis and blind signal separation volume hodgson m. e. reducing computational requirements of the minimum-distance classifier. remote sensing of environments hoerl a. e. and r. kennard ridge regression biased estimation for nonorthogonal problems. technometrics hofmann t. learning the similarity of documents an information-geometric approach to document retrieval and classification. in s. a. solla t. k. leen and k. r. m uller advances in neural information processing systems volume pp. mit press. hojen-sorensen p. a. o. winther and l. k. hansen mean field approaches to independent component analysis. neural computation hornik k. approximation capabilities of multilayer feedforward networks. neural networks hornik k. m. stinchcombe and h. white multilayer feedforward networks are universal approximators. neural networks hotelling h. analysis of a complex of statistical variables into principal components. journal of educational psychology hotelling h. relations between two sets of variables. biometrika hyv arinen a. and e. oja a fast fixed-point algorithm for independent component analysis. neural computation isard m. and a. blake condensation conditional density propagation for visual tracking. international journal of computer vision ito y. representation of functions by superpositions of a step or sigmoid function and their applications to neural network theory. neural networks references jaakkola t. and m. i. jordan bayesian parameter estimation via variational methods. statistics and computing jaakkola t. s. tutorial on variational approximation methods. in m. opper and d. saad advances in mean field methods pp. mit press. jaakkola t. s. and d. haussler exploiting generative models in discriminative classifiers. in m. s. kearns s. a. solla and d. a. cohn advances in neural information processing systems volume mit press. jacobs r. a. m. i. jordan s. j. nowlan and g. e. hinton adaptive mixtures of local experts. neural computation jaynes e. t. probability theory the logic of science. cambridge university press. jebara t. machine learning discrimina tive and generative. kluwer. jeffries h. an invariant form for the prior probability in estimation problems. pro. roy. soc. aa jelinek f. statistical methods for speech recognition. mit press. jensen c. a. kong and u. kjaerulff blocking gibbs sampling in very large probabilistic expert systems. international journal of human computer studies. special issue on real-world applications of uncertain reasoning. jordan m. i. an introduction to probabilis tic graphical models. in preparation. jordan m. i. z. ghahramani t. s. jaakkola and l. k. saul an introduction to variational methods for graphical models. in m. i. jordan learning in graphical models pp. mit press. jordan m. i. and r. a. jacobs hierarchical mixtures of experts and the em algorithm. neural computation jutten c. and j. herault blind separation of sources an adaptive algorithm based on neuromimetic architecture. signal processing kalman r. e. a new approach to linear filtering and prediction problems. transactions of the american society for mechanical engineering series d journal of basic engineering kambhatla n. and t. k. leen dimension reduction by local principal component analysis. neural computation kanazawa k. d. koller and s. russel stochastic simulation algorithms for dynamic probabilistic networks. in uncertainty in artificial intelligence volume morgan kaufmann. kapadia s. discriminative training of hidden markov models. phd thesis university of cambridge u.k. kapur j. maximum entropy methods in sci jensen f. v. an introduction to bayesian ence and engineering. wiley. networks. ucl press. jerrum m. and a. sinclair the markov chain monte carlo method an approach to approximate counting and integration. in d. s. hochbaum approximation algorithms for np-hard problems. pws publishing. jolliffe i. t. principal component analysis ed.. springer. jordan m. i. learning in graphical models. mit press. karush w. minima of functions of several variables with inequalities as side constraints. master s thesis department of mathematics university of chicago. kass r. e. and a. e. raftery bayes factors. journal of the american statistical association kearns m. j. and u. v. vazirani an introduction to computational learning theory. mit press. references kindermann r. and j. l. snell markov random fields and their applications. american mathematical society. kittler j. and j. f oglein contextual classification of multispectral pixel data. image and vision computing kohonen t. self-organized formation of topologically correct feature maps. biological cybernetics kohonen t. self-organizing maps. springer. kolmogorov v. and r. zabih what energy functions can be minimized via graph cuts? ieee transactions on pattern analysis and machine intelligence kreinovich v. y. arbitrary nonlinearity is sufficient to represent all functions by neural networks a theorem. neural networks krogh a. m. brown i. s. mian k. sj olander and d. haussler hidden markov models in computational biology applications to protein modelling. journal of molecular biology kschischnang f. r. b. j. frey and h. a. loeliger factor graphs and the sum-product algorithm. ieee transactions on information theory kuhn h. w. and a. w. tucker nonlinear programming. in proceedings of the berkeley symposium on mathematical statistics and probabilities pp. university of california press. kullback s. and r. a. leibler on information and sufficiency. annals of mathematical statistics k urkov a v. and p. c. kainen functionally equivalent feed-forward neural networks. neural computation in advances in neural information processing systems number mit press. in press. lasserre j. c. m. bishop and t. minka principled hybrids of generative and discriminative models. in proceedings ieee conference on computer vision and pattern recognition new york. lauritzen s. and n. wermuth graphical models for association between variables some of which are qualitative some quantitative. annals of statistics lauritzen s. l. propagation of probabilities means and variances in mixed graphical association models. journal of the american statistical association lauritzen s. l. graphical models. oxford university press. lauritzen s. l. and d. j. spiegelhalter local computations with probabailities on graphical structures and their application to expert systems. journal of the royal statistical society lawley d. n. a modified method of estimation in factor analysis and some large sample results. in uppsala symposium on psychological factor analysis number in nordisk psykologi monograph series pp. uppsala almqvist and wiksell. lawrence n. d. a. i. t. rowstron c. m. bishop and m. j. taylor optimising synchronisation times for mobile devices. in t. g. dietterich s. becker and z. ghahramani advances in neural information processing systems volume pp. mit press. lazarsfeld p. f. and n. w. henry latent structure analysis. houghton mifflin. le cun y. b. boser j. s. denker d. henderson r. e. howard w. hubbard and l. d. jackel backpropagation applied to handwritten zip code recognition. neural computation kuss m. and c. rasmussen assessing approximations for gaussian process classification. le cun y. j. s. denker and s. a. solla optimal brain damage. in d. s. touretzky references advances in neural information processing systems volume pp. morgan kaufmann. lecun y. l. bottou y. bengio and p. haffner gradient-based learning applied to document recognition. proceedings of the ieee lee y. y. lin and g. wahba multicategory support vector machines. technical report department of statistics university of madison wisconsin. leen t. k. from data distributions to regularization in invariant learning. neural computation lindley d. v. scoring rules and the inevitability of probability. international statistical review liu j. s. monte carlo strategies in scientific computing. springer. lloyd s. p. least squares quantization in pcm. ieee transactions on information theory l utkepohl h. handbook of matrices. wiley. mackay d. j. c. bayesian interpolation. neural computation mackay d. j. c. the evidence framework applied to classification networks. neural computation mackay d. j. c. a practical bayesian framework for back-propagation networks. neural computation mackay d. j. c. bayesian methods for backprop networks. in e. domany j. l. van hemmen and k. schulten models of neural networks iii chapter pp. springer. mackay d. j. c. bayesian neural networks and density networks. nuclear instruments and methods in physics research a mackay d. j. c. ensemble learning for hidden markov models. unpublished manuscript department of physics university of cambridge. mackay d. j. c. introduction to gaussian processes. in c. m. bishop neural networks and machine learning pp. springer. mackay d. j. c. comparison of approximate methods for handling hyperparameters. neural computation mackay d. j. c. information theory inference and learning algorithms. cambridge university press. mackay d. j. c. and m. n. gibbs density networks. in j. w. kay and d. m. titterington statistics and neural networks advances at the interface chapter pp. oxford university press. mackay d. j. c. and r. m. neal good errorcorrecting codes based on very sparse matrices. ieee transactions on information theory macqueen j. some methods for classification and analysis of multivariate observations. in l. m. lecam and j. neyman proceedings of the fifth berkeley symposium on mathematical statistics and probability volume i pp. university of california press. magnus j. r. and h. neudecker matrix differential calculus with applications in statistics and econometrics. wiley. mallat s. a wavelet tour of signal process ing ed.. academic press. manning c. d. and h. sch utze foundations of statistical natural language processing. mit press. mardia k. v. and p. e. jupp directional statistics. wiley. maybeck p. s. stochastic models estima tion and control. academic press. mcallester d. a. pac-bayesian stochastic model selection. machine learning references mccullagh p. and j. a. nelder generalized linear models ed.. chapman and hall. mcculloch w. s. and w. pitts a logical calculus of the ideas immanent in nervous activity. bulletin of mathematical biophysics reprinted in anderson and rosenfeld mceliece r. j. d. j. c. mackay and j. f. cheng turbo decoding as an instance of pearl s belief ppropagation algorithm. ieee journal on selected areas in communications mclachlan g. j. and k. e. basford mixture models inference and applications to clustering. marcel dekker. mclachlan g. j. and t. krishnan the em algorithm and its extensions. wiley. mclachlan g. j. and d. peel finite mixture models. wiley. meng x. l. and d. b. rubin maximum likelihood estimation via the ecm algorithm a general framework. biometrika metropolis n. a. w. rosenbluth m. n. rosenbluth a. h. teller and e. teller equation of state calculations by fast computing machines. journal of chemical physics metropolis n. and s. ulam the monte carlo method. journal of the american statistical association mika s. g. r atsch j. weston and b. sch olkopf fisher discriminant analysis with kernels. in y. h. hu j. larsen e. wilson and s. douglas neural networks for signal processing ix pp. ieee. minka t. expectation propagation for approximate bayesian inference. in j. breese and d. koller proceedings of the seventeenth conference on uncertainty in artificial intelligence pp. morgan kaufmann. minka t. a family of approximate algorithms for bayesian inference. ph. d. thesis mit. minka t. power ep. technical report microsoft research cambridge. minka t. divergence measures and message passing. technical report microsoft research cambridge. minka t. p. automatic choice of dimensionality for pca. in t. k. leen t. g. dietterich and v. tresp advances in neural information processing systems volume pp. mit press. minsky m. l. and s. a. papert perceptrons. mit press. expanded edition miskin j. w. and d. j. c. mackay ensemble learning for blind source separation. in s. j. roberts and r. m. everson independent component analysis principles and practice. cambridge university press. m ller m. efficient training of feedforward neural networks. ph. d. thesis aarhus university denmark. moody j. and c. j. darken fast learning in networks of locally-tuned processing units. neural computation moore a. w. the anchors hierarch using the triangle inequality to survive high dimensional data. in proceedings of the twelfth conference on uncertainty in artificial intelligence pp. m uller k. r. s. mika g. r atsch k. tsuda and b. sch olkopf an introduction to kernelbased learning algorithms. ieee transactions on neural networks m uller p. and f. a. quintana nonparametric bayesian data analysis. statistical science nabney i. t. netlab algorithms for pattern recognition. springer. nadaraya e. a. on estimating regression. theory of probability and its applications references nag r. k. wong and f. fallside script recognition using hidden markov models. in pp. ieee. neal r. m. probabilistic inference using markov chain monte carlo methods. technical report department of computer science university of toronto canada. neal r. m. bayesian learning for neural networks. springer. lecture notes in statistics neal r. m. monte carlo implementation of gaussian process models for bayesian regression and classification. technical report department of computer statistics university of toronto. neal r. m. suppressing random walks in markov chain monte carlo using ordered overrelaxation. in m. i. jordan learning in graphical models pp. mit press. neal r. m. markov chain sampling for dirichlet process mixture models. journal of computational and graphical statistics neal r. m. slice sampling. annals of statis tics neal r. m. and g. e. hinton a new view of the em algorithm that justifies incremental and other variants. in m. i. jordan learning in graphical models pp. mit press. nelder j. a. and r. w. m. wedderburn generalized linear models. journal of the royal statistical society a nilsson n. j. learning machines. mcgrawhill. reprinted as the mathematical foundations of learning machines morgan kaufmann nocedal j. and s. j. wright numerical op timization. springer. nowlan s. j. and g. e. hinton simplifying neural networks by soft weight sharing. neural computation ogden r. t. essential wavelets for statistical applications and data analysis. birkh auser. opper m. and o. winther a bayesian approach to on-line learning. in d. saad online learning in neural networks pp. cambridge university press. opper m. and o. winther gaussian processes and svm mean field theory and leave-one-out. in a. j. smola p. l. bartlett b. sch olkopf and d. shuurmans advances in large margin classifiers pp. mit press. opper m. and o. winther gaussian processes for classification. neural computation osuna e. r. freund and f. girosi support vector machines training and applications. a.i. memo mit. papoulis a. probability random variables and stochastic processes ed.. mcgrawhill. parisi g. statistical field theory. addison wesley. pearl j. probabilistic reasoning in intelli gent systems. morgan kaufmann. pearlmutter b. a. fast exact multiplication by the hessian. neural computation pearlmutter b. a. and l. c. parra maximum likelihood source separation a context-sensitive generalization of ica. in m. c. mozer m. i. jordan and t. petsche advances in neural information processing systems volume pp. mit press. pearson k. on lines and planes of closest fit to systems of points in space. the london edinburgh and dublin philosophical magazine and journal of science sixth series platt j. c. fast training of support vector machines using sequential minimal optimization. in b. sch olkopf c. j. c. burges and a. j. smola advances in kernel methods support vector learning pp. mit press. references platt j. c. probabilities for sv machines. in a. j. smola p. l. bartlett b. sch olkopf and d. shuurmans advances in large margin classifiers pp. mit press. platt j. c. n. cristianini and j. shawe-taylor large margin dags for multiclass classification. in s. a. solla t. k. leen and k. r. m uller advances in neural information processing systems volume pp. mit press. poggio t. and f. girosi networks for approximation and learning. proceedings of the ieee powell m. j. d. radial basis functions for multivariable interpolation a review. in j. c. mason and m. g. cox algorithms for approximation pp. oxford university press. press w. h. s. a. teukolsky w. t. vetterling and b. p. flannery numerical recipes in c the art of scientific computing ed.. cambridge university press. qazaz c. s. c. k. i. williams and c. m. bishop an upper bound on the bayesian error bars for generalized linear regression. in s. w. ellacott j. c. mason and i. j. anderson mathematics of neural networks models algorithms and applications pp. kluwer. quinlan j. r. induction of decision trees. machine learning quinlan j. r. programs for machine learning. morgan kaufmann. rabiner l. and b. h. juang fundamentals of speech recognition. prentice hall. rabiner l. r. a tutorial on hidden markov models and selected applications in speech the ieee recognition. proceedings of ramasubramanian v. and k. k. paliwal a generalized optimization of the k-d tree for fast nearest-neighbour search. in proceedings fourth ieee region international conference pp. ramsey f. truth and probability. in r. braithwaite the foundations of mathematics and other logical essays. humanities press. rao c. r. and s. k. mitra generalized in verse of matrices and its applications. wiley. rasmussen c. e. evaluation of gaussian processes and other methods for non-linear regression. ph. d. thesis university of toronto. rasmussen c. e. and j. qui nonero-candela healing the relevance vector machine by augmentation. in l. d. raedt and s. wrobel proceedings of the international conference on machine learning pp. rasmussen c. e. and c. k. i. williams gaussian processes for machine learning. mit press. rauch h. e. f. tung and c. t. striebel maximum likelihood estimates of linear dynamical systems. aiaa journal ricotti l. p. s. ragazzini and g. martinelli learning of word stress in a sub-optimal second order backpropagation neural network. in proceedings of the ieee international conference on neural networks volume pp. ieee. ripley b. d. pattern recognition and neu ral networks. cambridge university press. robbins h. and s. monro a stochastic approximation method. annals of mathematical statistics robert c. p. and g. casella monte carlo statistical methods. springer. rockafellar r. convex analysis. princeton university press. rosenblatt f. principles of neurodynamics perceptrons and the theory of brain mechanisms. spartan. roth v. and v. steinhage nonlinear discriminant analysis using kernel functions. in s. a. references solla t. k. leen and k. r. m uller advances in neural information processing systems volume mit press. roweis s. em algorithms for pca and spca. in m. i. jordan m. j. kearns and s. a. solla advances in neural information processing systems volume pp. mit press. roweis s. and z. ghahramani a unifying review of linear gaussian models. neural computation roweis s. and l. saul december. nonlinear dimensionality reduction by locally linear embedding. science rubin d. b. iteratively reweighted least squares. in encyclopedia of statistical sciences volume pp. wiley. rubin d. b. and d. t. thayer em algorithms for ml factor analysis. psychometrika rumelhart d. e. g. e. hinton and r. j. williams learning internal representations by error propagation. in d. e. rumelhart j. l. mcclelland and the pdp research group parallel distributed processing explorations in the microstructure of cognition volume foundations pp. mit press. reprinted in anderson and rosenfeld rumelhart d. e. j. l. mcclelland and the pdp research group parallel distributed processing explorations in the microstructure of cognition volume foundations. mit press. sagan h. introduction to the calculus of variations. dover. savage l. j. the subjective basis of statistical practice. technical report department of statistics university of michigan ann arbor. sch olkopf b. j. platt j. shawe-taylor a. smola and r. c. williamson estimating the support of a high-dimensional distribution. neural computation sch olkopf b. a. smola and k.-r. m uller nonlinear component analysis as a kernel eigenvalue problem. neural computation sch olkopf b. a. smola r. c. williamson and p. l. bartlett new support vector algorithms. neural computation sch olkopf b. and a. j. smola learning with kernels. mit press. schwarz g. estimating the dimension of a model. annals of statistics schwarz h. r. finite element methods. aca demic press. seeger m. bayesian gaussian process models pac-bayesian generalization error bounds and sparse approximations. ph. d. thesis university of edinburg. seeger m. c. k. i. williams and n. lawrence fast forward selection to speed up sparse gaussian processes. in c. m. bishop and b. frey proceedings ninth international workshop on artificial intelligence and statistics key west florida. shachter r. d. and m. peot simulation approaches to general probabilistic inference on belief networks. in p. p. bonissone m. henrion l. n. kanal and j. f. lemmer uncertainty in artificial intelligence volume elsevier. shannon c. e. a mathematical theory of communication. the bell system technical journal and shawe-taylor j. and n. cristianini kernel methods for pattern analysis. cambridge university press. sietsma j. and r. j. f. dow creating artificial neural networks that generalize. neural networks simard p. y. le cun and j. denker efficient pattern recognition using a new transformation distance. in s. j. hanson j. d. cowan and references c. l. giles advances in neural information processing systems volume pp. morgan kaufmann. simard p. b. victorri y. le cun and j. denker tangent prop a formalism for specifying selected invariances in an adaptive network. in j. e. moody s. j. hanson and r. p. lippmann advances in neural information processing systems volume pp. morgan kaufmann. simard p. y. d. steinkraus and j. platt best practice for convolutional neural networks applied to visual document analysis. in proceedings international conference on document analysis and recognition pp. ieee computer society. sirovich l. turbulence and the dynamics of coherent structures. quarterly applied mathematics smola a. j. and p. bartlett sparse greedy gaussian process regression. in t. k. leen t. g. dietterich and v. tresp advances in neural information processing systems volume pp. mit press. spiegelhalter d. and s. lauritzen sequential updating of conditional probabilities on directed graphical structures. networks stinchecombe m. and h. white universal approximation using feed-forward networks with non-sigmoid hidden layer activation functions. in international joint conference on neural networks volume pp. ieee. stone j. v. independent component analy sis a tutorial introduction. mit press. sung k. k. and t. poggio example-based learning for view-based human face detection. a.i. memo mit. sutton r. s. and a. g. barto reinforcement learning an introduction. mit press. tarassenko l. novelty detection for the identification of masses in mamograms. in proceedings fourth iee international conference on artificial neural networks volume pp. iee. tax d. and r. duin data domain description by support vectors. in m. verleysen proceedings european symposium on artificial neural networks esann pp. d. facto press. teh y. w. m. i. jordan m. j. beal and d. m. blei hierarchical dirichlet processes. journal of the americal statistical association. to appear. tenenbaum j. b. v. de silva and j. c. langford december. a global framework for nonlinear dimensionality reduction. science tesauro g. td-gammon a self-teaching backgammon program achieves master-level play. neural computation thiesson b. d. m. chickering d. heckerman and c. meek arma time-series modelling with graphical models. in m. chickering and j. halpern proceedings of the twentieth conference on uncertainty in artificial intelligence banff canada pp. auai press. tibshirani r. regression shrinkage and selection via the lasso. journal of the royal statistical society b tierney l. markov chains for exploring posterior distributions. annals of statistics tikhonov a. n. and v. y. arsenin solutions of ill-posed problems. v. h. winston. tino p. and i. t. nabney hierarchical gtm constructing localized non-linear projection manifolds in a principled way. ieee transactions on pattern analysis and machine intelligence svens en m. and c. m. bishop robust bayesian mixture modelling. neurocomputing tino p. i. t. nabney and y. sun using directional curvatures to visualize folding patterns of the gtm projection manifolds. in references g. dorffner h. bischof and k. hornik artificial neural networks icann pp. springer. vapnik v. n. estimation of dependences based on empirical data. springer. vapnik v. n. the nature of statistical learn tipping m. e. probabilistic visualisation of high-dimensional binary data. in m. s. kearns s. a. solla and d. a. cohn advances in neural information processing systems volume pp. mit press. tipping m. e. sparse bayesian learning and the relevance vector machine. journal of machine learning research tipping m. e. and c. m. bishop probabilistic principal component analysis. technical report neural computing research group aston university. tipping m. e. and c. m. bishop mixtures of probabilistic principal component analyzers. neural computation tipping m. e. and c. m. bishop probabilistic principal component analysis. journal of the royal statistical society series b tipping m. e. and a. faul fast marginal likelihood maximization for sparse bayesian models. in c. m. bishop and b. frey proceedings ninth international workshop on artificial intelligence and statistics key west florida. tong s. and d. koller restricted bayes optimal classifiers. in proceedings national conference on artificial intelligence pp. aaai. tresp v. scaling kernel-based systems to large data sets. data mining and knowledge discovery uhlenbeck g. e. and l. s. ornstein on the theory of brownian motion. phys. rev. ing theory. springer. vapnik v. n. statistical learning theory. wi ley. veropoulos k. c. campbell and n. cristianini controlling the sensitivity of support vector machines. in proceedings of the international joint conference on artificial intelligence workshop pp. vidakovic b. statistical modelling by wavelets. wiley. viola p. and m. jones robust real-time face detection. international journal of computer vision viterbi a. j. error bounds for convolutional codes and an asymptotically optimum decoding algorithm. ieee transactions on information theory viterbi a. j. and j. k. omura principles of digital communication and coding. mcgrawhill. wahba g. a comparison of gcv and gml for choosing the smoothing parameter in the generalized spline smoothing problem. numerical mathematics wainwright m. j. t. s. jaakkola and a. s. willsky a new class of upper bounds on the log partition function. ieee transactions on information theory walker a. m. on the asymptotic behaviour of posterior distributions. journal of the royal statistical society b walker s. g. p. damien p. w. laud and a. f. m. smith bayesian nonparametric inference for random distributions and related functions discussion. journal of the royal statistical society b valiant l. g. a theory of the learnable. communications of the association for computing machinery watson g. s. smooth regression analysis. sankhy a the indian journal of statistics. series a references webb a. r. functional approximation by feed-forward networks a least-squares approach to generalisation. ieee transactions on neural networks williams o. a. blake and r. cipolla sparse bayesian learning for efficient visual tracking. ieee transactions on pattern analysis and machine intelligence williams p. m. using neural networks to model conditional multivariate densities. neural computation winn j. and c. m. bishop variational message passing. journal of machine learning research zarchan p. and h. musoff fundamentals of kalman filtering a practical approach ed.. aiaa. weisstein e. w. crc concise encyclopedia of mathematics. chapman and hall and crc. weston j. and c. watkins multi-class support vector machines. in m. verlysen proceedings esann brussels. d-facto publications. whittaker j. graphical models in applied multivariate statistics. wiley. widrow b. and m. e. hoff adaptive switching circuits. in ire wescon convention record volume pp. reprinted in anderson and rosenfeld widrow b. and m. a. lehr years of adaptive neural networks perceptron madeline and backpropagation. proceedings of the ieee wiegerinck w. and t. heskes fractional belief propagation. in s. becker s. thrun and k. obermayer advances in neural information processing systems volume pp. mit press. williams c. k. i. computation with infinite neural networks. neural computation williams c. k. i. prediction with gaussian processes from linear regression to linear prediction and beyond. in m. i. jordan learning in graphical models pp. mit press. williams c. k. i. and d. barber bayesian classification with gaussian processes. ieee transactions on pattern analysis and machine intelligence williams c. k. i. and m. seeger using the nystrom method to speed up kernel machines. in t. k. leen t. g. dietterich and v. tresp advances in neural information processing systems volume pp. mit press. index index page numbers in bold indicate the primary source of information for the corresponding topic. coding scheme acceptance criterion activation function active constraint adaboost adaline adaptive rejection sampling adf see assumed density filtering aic see akaike information criterion akaike information criterion family of divergences recursion ancestral sampling annular flow ar model see autoregressive model arc ard see automatic relevance determination arma see autoregressive moving average assumed density filtering autoassociative networks automatic relevance determination autoregressive hidden markov model autoregressive model autoregressive moving average back-tracking backgammon backpropagation bagging basis function batch training baum-welch algorithm bayes theorem bayes thomas bayesian analysis vii hierarchical model averaging bayesian information criterion bayesian model comparison bayesian network bayesian probability belief propagation bernoulli distribution mixture model bernoulli jacob beta distribution beta recursion between-class covariance bias bias parameter bias-variance trade-off bic see bayesian information criterion binary entropy binomial distribution index biological sequence bipartite graph bits blind source separation blocked path boltzmann distribution boltzmann ludwig eduard boolean logic boosting bootstrap bootstrap filter box constraints box-muller method calculus of variations canonical correlation analysis canonical link function cart see classification and regression trees cauchy distribution causality cca see canonical correlation analysis central differences central limit theorem chain graph chaining chapman-kolmogorov equations child node cholesky decomposition chunking circular normal see von mises distribution classical probability classification classification and regression trees clique clustering clutter problem co-parents code-book vectors combining models committee complete data set completing the square computational learning theory concave function concentration parameter condensation algorithm conditional entropy conditional expectation conditional independence conditional mixture model see mixture model conditional probability conjugate prior convex duality convex function convolutional neural network correlation matrix cost function covariance between-class within-class covariance matrix diagonal isotropic partitioned positive definite cox s axioms credit assignment cross-entropy error function cross-validation cumulative distribution function curse of dimensionality curve fitting d map see dependency map d-separation dag see directed acyclic graph dagsvm data augmentation data compression decision boundary decision region decision surface see decision boundary decision theory decision tree decomposition methods degrees of freedom degrees-of-freedom parameter density estimation index density network dependency map descendant node design matrix differential entropy digamma function directed acyclic graph directed cycle directed factorization dirichlet distribution dirichlet lejeune discriminant function discriminative model distortion measure distributive law of multiplication dna document retrieval dual representation dual-energy gamma densitometry dynamic programming dynamical system e step see expectation step early stopping ecm see expectation conditional maximization edge effective number of observations effective number of parameters elliptical k-means em see expectation maximization emission probability empirical bayes see evidence approximation energy function entropy conditional differential relative ep see expectation propagation error function equality constraint equivalent kernel erf function error backpropagation see backpropagation error function error-correcting output codes euler leonhard euler-lagrange equations evidence approximation evidence function expectation expectation conditional maximization expectation maximization gaussian mixture generalized sampling methods expectation propagation expectation step explaining away exploitation exploration exponential distribution exponential family extensive variables face detection face tracking factor analysis mixture model factor graph factor loading factorial hidden markov model factorized distribution feature extraction feature map feature space fisher information matrix fisher kernel fisher s linear discriminant flooding schedule forward kinematics forward problem forward propagation forward-backward algorithm fractional belief propagation frequentist probability fuel system function interpolation functional derivative index gamma densitometry gamma distribution gamma function gating function gauss carl friedrich gaussian conditional marginal maximum likelihood mixture sequential estimation sufficient statistics wrapped gaussian kernel gaussian process gaussian random field gaussian-gamma distribution gaussian-wishart distribution gem see expectation maximization generalized generalization generalized linear model generalized maximum likelihood see evidence ap proximation generative model generative topographic mapping directional curvature magnification factor geodesic distance gibbs sampling blocking gibbs josiah willard gini index global minimum gradient descent gram matrix graph-cut algorithm graphical model bipartite directed factorization fully connected inference tree treewidth triangulated undirected green s function gtm see generative topographic mapping hamilton william rowan hamiltonian dynamics hamiltonian function hammersley-clifford theorem handwriting recognition handwritten digit head-to-head path head-to-tail path heaviside step function hellinger distance hessian matrix diagonal approximation exact evaluation fast multiplication finite differences inverse outer product approximation heteroscedastic hidden markov model autoregressive factorial forward-backward algorithm input-output left-to-right maximum likelihood scaling factor sum-product algorithm switching variational inference hidden unit hidden variable hierarchical bayesian model hierarchical mixture of experts hinge error function hinton diagram histogram density estimation hme see hierarchical mixture of experts hold-out set homogeneous flow homogeneous kernel homogeneous markov chain hooke s law hybrid monte carlo hyperparameter hyperprior i map see independence map i.i.d. see independent identically distributed ica see independent component analysis icm see iterated conditional modes identifiability image de-noising importance sampling importance weights improper prior imputation step imputation-posterior algorithm inactive constraint incomplete data set independence map independent component analysis independent factor analysis independent identically distributed independent variables independent identically distributed induced factorization inequality constraint inference information criterion information geometry information theory input-output hidden markov model intensive variables intrinsic dimensionality invariance inverse gamma distribution inverse kinematics inverse problem inverse wishart distribution ip algorithm see imputation-posterior algorithm irls see iterative reweighted least squares ising model isomap isometric feature map iterated conditional modes index iterative reweighted least squares jacobian matrix jensen s inequality join tree junction tree algorithm k nearest neighbours k-means clustering algorithm k-medoids algorithm kalman filter extended kalman gain matrix kalman smoother karhunen-loeve transform karush-kuhn-tucker conditions kernel density estimator kernel function fisher gaussian homogeneous nonvectorial inputs stationary kernel pca kernel regression kernel substitution kernel trick kinetic energy kkt see karush-kuhn-tucker conditions kl divergence see kullback-leibler divergence kriging see gaussian process kullback-leibler divergence lagrange multiplier lagrange joseph-louis lagrangian laminar flow laplace approximation laplace pierre-simon large margin see margin lasso latent class analysis latent trait model latent variable index lattice diagram lds see linear dynamical system leapfrog discretization learning learning rate parameter least-mean-squares algorithm leave-one-out likelihood function likelihood weighted sampling linear discriminant fisher linear dynamical system inference linear independence linear regression em mixture model variational linear smoother linear-gaussian model linearly separable link link function liouville s theorem lle see locally linear embedding lms algorithm see least-mean-squares algorithm local minimum local receptive field locally linear embedding location parameter log odds logic sampling logistic regression bayesian mixture model multiclass logistic sigmoid logit function loopy belief propagation loss function loss matrix lossless data compression lossy data compression lower bound m step see maximization step machine learning vii macrostate mahalanobis distance manifold map see maximum posterior margin error soft marginal likelihood marginal probability markov blanket markov boundary see markov blanket markov chain first order homogeneous second order markov chain monte carlo markov model homogeneous markov network see markov random field markov random field max-sum algorithm maximal clique maximal spanning tree maximization step maximum likelihood gaussian mixture singularities type see evidence approximation maximum margin see margin maximum posterior mcmc see markov chain monte carlo mdn see mixture density network mds see multidimensional scaling mean mean field theory mean value theorem measure theory memory-based methods message passing pending message schedule variational metropolis algorithm metropolis-hastings algorithm index microstate minimum risk minkowski loss missing at random missing data mixing coefficient mixture component mixture density network mixture distribution see mixture model mixture model conditional linear regression logistic regression symmetries mixture of experts mixture of gaussians mlp see multilayer perceptron mnist data model comparison model evidence model selection moment matching momentum variable monte carlo em algorithm monte carlo sampling moore-penrose pseudo-inverse see pseudo-inverse moralization mrf see markov random field multidimensional scaling multilayer perceptron multimodality multinomial distribution multiplicity mutual information nadaraya-watson see kernel regression naive bayes model nats natural language modelling natural parameters nearest-neighbour methods neural network convolutional regularization relation to gaussian process newton-raphson node noiseless coding theorem nonidentifiability noninformative prior nonparametric methods normal distribution see gaussian normal equations normal-gamma distribution normal-wishart distribution normalized exponential see softmax function novelty detection object recognition observed variable occam factor oil flow data old faithful data on-line learning see sequential learning one-versus-one classifier one-versus-the-rest classifier ordered over-relaxation ornstein-uhlenbeck process orthogonal least squares outlier outliers over-fitting over-relaxation pac learning see probably approximately correct pac-bayesian framework parameter shrinkage parent node particle filter partition function parzen estimator see kernel density estimator parzen window pattern recognition vii pca see principal component analysis pending message perceptron convergence theorem hardware perceptron criterion perfect map index periodic variable phase space photon noise plate polynomial curve fitting polytree position variable positive definite covariance positive definite matrix positive semidefinite covariance positive semidefinite matrix posterior probability posterior step potential energy potential function power ep power method precision matrix precision parameter predictive distribution preprocessing principal component analysis bayesian em algorithm gibbs sampling mixture distribution physical analogy principal curve principal subspace principal surface prior conjugate consistent improper noninformative probabilistic graphical model see graphical model probabilistic pca probability bayesian classical density frequentist mass function prior product rule sum rule theory probably approximately correct probit function probit regression product rule of probability proposal distribution protected conjugate gradients protein sequence pseudo-inverse pseudo-random numbers quadratic discriminant quality parameter radial basis function rauch-tung-striebel equations regression regression function regularization tikhonov regularized least squares reinforcement learning reject option rejection sampling relative entropy relevance vector relevance vector machine responsibility ridge regression rms error see root-mean-square error robbins-monro algorithm robot arm robustness root node root-mean-square error rosenblatt frank rotation invariance rts equations see rauch-tung-striebel equations running intersection property rvm see relevance vector machine sample mean sample variance sampling-importance-resampling scale invariance scale parameter scaling factor schwarz criterion see bayesian information crite rion self-organizing map sequential data sequential estimation sequential gradient descent sequential learning sequential minimal optimization serial message passing schedule shannon claude shared parameters shrinkage shur complement sigmoid see logistic sigmoid simplex single-class support vector machine singular value decomposition sinusoidal data sir see sampling-importance-resampling skip-layer connection slack variable slice sampling smo see sequential minimal optimization smoother matrix smoothing parameter soft margin soft weight sharing softmax function som see self-organizing map sparsity sparsity parameter spectrogram speech recognition sphereing spline functions standard deviation standardizing state space model switching stationary kernel statistical bias see bias statistical independence see independent variables index statistical learning theory see computational learn ing theory steepest descent stirling s approximation stochastic stochastic em stochastic gradient descent stochastic process stratified flow student s t-distribution subsampling sufficient statistics sum rule of probability sum-of-squares error sum-product algorithm for hidden markov model supervised learning support vector support vector machine for regression multiclass survival of the fittest svd see singular value decomposition svm see support vector machine switching hidden markov model switching state space model synthetic data sets tail-to-tail path tangent distance tangent propagation tapped delay line target vector test set threshold parameter tied parameters tikhonov regularization time warping tomography training training set transition probability translation invariance tree-reweighted message passing treewidth index trellis diagram see lattice diagram triangulated graph type maximum likelihood see evidence approxi mation undetermined multiplier see lagrange multiplier undirected graph see markov random field uniform distribution uniform sampling uniquenesses unobserved variable see latent variable unsupervised learning utility function validation set vapnik-chervonenkis dimension variance variational inference for gaussian mixture for hidden markov model local vc dimension see vapnik-chervonenkis dimen sion vector quantization vertex see node visualization viterbi algorithm von mises distribution wavelets weak learner weight decay weight parameter weight sharing soft weight vector weight-space symmetry weighted least squares well-determined parameters whitening wishart distribution within-class covariance woodbury identity wrapped distribution yellowstone national park