stochastic mechanics applications of random media mathematics signal processing stochastic modelling and image synthesis and applied probability mathematical economics stochastic optimization stochastic control edited by i. karatzas m. yor advisory board p. bremaud e. carlen r. dobrushin w. fleming d. geman g. grimmett g. papanicolaou j. scheinkman springer new york berlin heidelberg barcelona budapest hong kong london milan paris santa clara singapore tokyo applications of mathematics flemingrishel deterministic and stochastic optimal control marchuk methods of numerical mathematics second ed. balakrishnan applied functional analysis second ed. borovkov stochastic processes in queueing theory liptserlshiryayev statistics of random processes i general theory liptserlshiryayev statistics of random processes ii applications vorobev game theory lectures for economists and systems scientists shiryayev optimal stopping rules ibragimovrozanov gaussian random processes wonham linear multivariable control a geometric approach third ed. hida brownian motion hestenes conjugate direction methods in optimization kallianpur stochastic filtering theory krylov controlled diffusion processes prabhu stochastic storage processes queues insurance risk and dams ibragimovhasminskii statistical estimation asymptotic theory cesari optimization theory and applications elliott stochastic calculus and applications marchukshaidourov difference methods and their extrapolations hijab stabilization of control systems protter stochastic integration and differential equations benvenistemetivierpriouret adaptive algorithms and stochastic approximations kloedeniplaten numerical solution of stochastic differential equations kushnerdupuis numerical methods for stochastic control problems in continuous time flemingsoner controlled markov processes and viscosity solutions baccellilbremaud elements of queueing theory winkler image analysis random fields and dynamic monte carlo methods an introduction to mathematical aspects kalpazidou cycle representations of markov processes elliott! aggounimoore hidden markov models estimation and control hernandez-lermailasserre discrete-time markov control processes basic optimality criteria devroyegyorfllugosi a probabilistic theory of pattern recognition maitraisudderth discrete gambling and stochastic games luc devroye laszlo gyorfi gabor lugosi a probabilistic theory of pattern recognition with figures springer gyorfi gabor lugosi department of mathematics and computer science technical university of budapest budapest hungary luc devroye school of computer science mcgill university montreal quebec canada managing editors i. karatzas department of statistics columbia university new york ny usa m. yor cnrs laboratoire de probabilites universite pierre et marie curie place jussieu tour paris cedex os france mathematics subject classification library of congress cataloging-in-publication data devroye luc. a probabilistic theory of pattern recognitionluc devroye laszlo gyorfi gabor lugosi. p. cm. includes bibliographical references and index. isbn pattern perception. probabilities. gyorfi laszlo. ii. lugosi gabor. iii. title. printed on acid-free paper. springer-verlag new york inc. all rights reserved. this work may not be translated or copied in whole or in part without the written permission of the publisher new york inc. fifth avenue new york ny usa except for brief excerpts in connection with reviews or scholarly analysis. use in connection with any form of information storage and retrieval electronic adaptation computer software or by similar or dissimilar methodology now known or after developed is forbidden. the use of general descriptive names trade names trademarks etc. in this publication even if the former are not especially identified is not to be taken as a sign that such names as understood by the trade marks and merchandise marks act may accordingly be used freely by anyone. production managed by francine mcneill manufacturing supervised by jeffrey taub. photocomposed copy prepared using springers svsing.sty macro. printed and bound by braun-brumfield inc. ann arbor mi. printed in the united states of america. second printing. isbn springer-verlag new york berlin heidelberg spin preface life is just a long random walk. things are created because the circumstances happen to be right. more often than not creations such as this book are dental. nonparametric estimation came to life in the fifties and sixties and started developing at a frenzied pace in the late sixties engulfing pattern recognition in its growth. in the mid-sixties two young men tom cover and peter hart showed the world that the nearest neighbor rule in all its simplicity was guaranteed to err at most twice as often as the best possible discrimination method. toms results had a profound influence on terry wagner who became a professor at the university of texas at austin and brought probabilistic rigor to the young field of metric estimation. around vapnik and chervonenkis started publishing a revolutionary series of papers with deep implications in pattern recognition but their work was not well known at the time. however tom and terry had noticed the potential of the work and terry asked luc devroye to read that work in ration for his ph.d. dissertation at the university of texas. the year was luc ended up in texas quite by accident thanks to a tip by his friend and fellow belgian willy wouters who matched him up with terry. by the time lucs dissertation was published in pattern recognition had taken off in earnest. on the ical side important properties were still being discovered. in stone stunned the nonparametric community by showing that there are iionparametric rules that are convergent for all distributions of the data. this is called distribution-free or universal consistency and it is what makes nonparametric methods so attractive. yet very few researchers were concerned with universal consistency-one notable exception was laci gyorfi who at that time worked in budapest amid an energetic group of nonparametric specialists that included sandor csibi fritz and pal revesz. vi preface so linked by a common vision luc and laci decided to join forces in the early eighties. in they wrote six chapters of a book on nonparametric regression function estimation but these were never published. in fact the notes are still in drawers in their offices today. they felt that the subject had not matured yet. a book on nonparametric density estimation saw the light in unfortunately as true baby-boomers neither luc nor laci had the time after to write a text on nonpararnetric pattern recognition. enter gabor lugosi who obtained his doctoral degree under lacis supervision in gabor had prepared a set of rough course notes on the subject around and proposed to coordinate the project-this book-in with renewed energy we set out to write the book that we should have written at least ten years ago. discussions and work sessions were held in budapest montreal leuven and louvain-la-neuve. in leuven our gracious hosts were ed van der meulen and jan beirlant and in la-neuve we were gastronomically and spiritually supported by leopold simar and irene gijbels. we thank all of them. new results accumulated and we had to resist the temptation to publish these in journals. finally in may the manuscript had bloated to such extent that it had to be sent to the publisher for otherwise it would have become an encyclopedia. some important unanswered questions were quickly turned into masochistic exercises or wild conjectures. we will explain subject selection classroom use chapter dependence and personal viewpoints in the introduction. we do apologize of course for all remaining errors. we were touched influenced guided and taught by many people. terry ners rigor and taste for beautiful nonparametric problems have infected us for life. we thank our past and present coauthors on nonpararnetric papers alain berlinet michel broniatowski ricardo cao paul deheuvels andras farago adam krzyzak tamas linder andrew nobel mirek pawlak igor vajda harro walk and ken zeger. tamas linder read most of the book and provided able feedback. his help is especially appreciated. several chapters were critically read by students in budapest. we thank all of them especially andras antos miklos csuros balazs kegl istvan pali and marti pinter. finally here is an phabetically ordered list of friends who directly or indirectly contributed to our knowledge and love of nonparametrics andrew and roger barron denis bosq prabhir burman tom cover antonio cuevas pierre devijver ricardo fraiman ned glick wenceslao gonzalez-manteiga peter hall eiichi isogai ed mack arthur nadas georg pflug george roussas winfried stute tamas szabados godfried toussaint sid yakowitz and yannis yatracos. gabor diligently typed the entire manuscript and coordinated all contributions. he became quite a texpert in the process. several figures were made by idraw and xi ig by gabor and luc. most of the drawings were directly programmed in postscript by luc and an undergraduate student at mcgill university hisham petry to whom we are grateful. for gabor this book comes at the beginning of his career. unfortunately the other two authors are not so lucky. as both luc and laci felt that they would probably not write another book on nonparametric pattern recognition-the random walk must go on-they decided to put their general preface vii view of the subject area on paper while trying to separate the important from the irrelevant. surely this has contributed to the length of the text. so far our random excursions have been happy ones. coincidentally luc is married to bea the most understanding woman in the world and happens to have two great daughters natasha and birgit who do not stray off their random courses. similarly laci has an equally wonderful wife kati and two children with steady compasses kati and janos. during the preparations of this book gabor met a wonderful girl arrate. they have recently decided to tie their lives together. on the less amorous and glamorous side we gratefully acknowledge the research support of nserc canada fcar quebec otka hungary and the exchange program between the hungarian academy of sciences and the royal belgian academy of sciences. early versions of this text were tried out in some classes at the technical university of budapest katholieke universiteit leuven universitat stuttgart and universite montpellier ii. we would like to thank those students for their help in making this a better book. montreal quebec canada budapest hungary budapest hungary luc devroye laci gyorfi gabor lugosi contents preface introduction the bayes error another simple example the bayes problem a simple example other formulas for the bayes risk problems and exercises plug-in decisions bayes error versus dimension inequalities and alternate distance measures measuring discriminatory information the kolmogorov variational distance the nearest neighbor error the bhattacharyya affinity entropy jeffreysdivergence f-errors the mahalanobis distance i-divergences problems and exercises v x contents linear discrimination univariate discrimination and stoller splits linear discriminants the fisher linear discriminant the normal distribution empirical risk minimization minimizing other criteria problems and exercises nearest neighbor rules introduction notation and simple asymptotics proof of stones lemma the asymptotic probability of error the asymptotic error probability of weighted nearest neighbor rules k-nearest neighbor rules even k inequalities for the probability of error behavior when l is small nearest neighbor rules when l admissibility of the nearest neighbor rule the i-nearest neighbor rule problems and exercises consistency universal consistency classification and regression estimation partitioning rules the histogram rule stones theorem the k-nearest neighbor rule classification is easier than regression function estimation smart rules problems and exercises slow rates of convergence finite training sequence slow rates problems and exercises error estimation error counting hoeffdings inequality error estimation without testing data selecting classifiers contents xi estimating the bayes error problems and exercises the regular histogram rule the method of bounded differences strong universal consistency problems and exercises kernel rules consistency proof of the consistency theorem potential function rules problems and exercises consistency of the k-nearest neighbor rule strong consistency breaking distance ties recursive methods scale-invariant rules weighted nearest neighbor rules rotation-invariant rules relabeling rules problems and exercises vapnik-chervonenkis theory empirical error minimization fingering the glivenko-cantelli theorem uniform deviations of relative frequencies from probabilities classifier selection sample complexity the zero-error case extensions problems and exercises combinatorial aspects of vapnik-chervonenkis theory shatter coefficients and vc dimension shatter coefficients of some classes linear and generalized linear discrimination rules convex sets and monotone layers problems and exercises lower bounds for empirical classifier selection minimax lower bounds the case lc classes with infinite vc dimension xii contents the case lc sample complexity problems and exercises the maximum likelihood principle maximum likelihood the formats the maximum likelihood method regression format consistency examples classical maximum likelihood distribution format problems and exercises parametric classification example exponential families standard plug-in rules minimum distance estimates empirical error minimization problems and exercises generalized linear discrimination fourier series classification generalized linear classification problems and exercises complexity regularization structural risk minimization poor approximation properties of vc classes simple empirical covering problems and exercises condensed and edited nearest neighbor rules condensed nearest neighbor rules edited nearest neighbor rules sieves and prototypes problems and exercises tree classifiers invariance trees with the x-property balanced search trees binary search trees the chronological k-d tree the deep k-d tree quadtrees best possible perpendicular splits splitting criteria based on impurity functions a consistent splitting criterion bsp trees primitive selection constructing consistent tree classifiers a greedy classifier problems and exercises data-dependent partitioning introduction a vapnik-chervonenkis inequality for partitions consistency statistically equivalent blocks partitioning rules based on clustering data-based scaling classification trees problems and exercises splitting the data the holdout estimate consistency and asymptotic optimality nearest neighbor rules with automatic scaling classification based on clustering statistically equivalent blocks binary tree classifiers problems and exercises the resubstitution estimate the resubstitution estimate histogram rules data-based histograms and rule selection problems and exercises deleted estimates of the error probability a general lower bound a general upper bound for deleted estimates nearest neighbor rules kernel rules histogram rules problems and exercises automatic kernel rules consistency data splitting kernel complexity multiparameter kernel rules contents xiii xiv contents kernels of infinite complexity on minimizing the apparent error rate minimizing the deleted estimate sieve methods squared error minimization problems and exercises automatic nearest neighbor rules consistency data splitting data splitting for weighted nn rules reference data and data splitting variable metric nn rules selection of k based on the deleted estimate problems and exercises independent components hypercubes and discrete spaces multinomial discrimination quantization boolean classifiers series methods for the hypercube maximum likelihood kernel methods problems and exercises epsilon entropy and totally bounded sets definitions examples totally bounded classes skeleton estimates rate of convergence problems and exercises uniform laws of large numbers minimizing the empirical squared error uniform deviations of averages from expectations empirical squared error minimization proof of theorem covering numbers and shatter coefficients generalized linear classification problems and exercises neural networks multilayer perceptrons arrangements approximation by neural networks vc dimension error minimization the adaline and padaline polynomial networks kolmogorov-lorentz networks and additive models projection pursuit radial basis function networks problems and exercises other error estimates smoothing the error count posterior probability estimates rotation estimate bootstrap problems and exercises feature extraction dimensionality reduction transformations with small distortion admissible and sufficient transformations problems and exercises appendix probability inequalities a.l basics of measure theory the lebesgue integral denseness results convergence of random variables conditional expectation the binomial distribution the hypergeometric distribution the multinomial distribution a.ll the exponential and gamma distributions the multivariate normal distribution notation references author index subject index contents xv introduction pattern recognition or discrimination is about guessing or predicting the unknown nature of an observation a discrete quantity such as black or white one or zero sick or healthy real or fake. an observation is a collection of numerical measurements such as an image is a sequence of bits one per pixel a vector of weather data an electrocardiogram or a signature on a check suitably digitized. more formally an observation is a d-dimensional vector x. the unknown nature of the observation is called a class. it is denoted by y and takes values in a finite set m. in pattern recognition one creates a function gx nd m which represents ones guess of y given x. the mapping g is called a classifier. our classifier errs on x if gx i y. how one creates a rule g depends upon the problem at hand. experts can be called for medical diagnoses or earthquake predictions-they try to mold g to their own knowledge and experience often by trial and error. theoretically each expert operates with a built-in classifier g but describing this g explicitly in mathematical form is not a sinecure. the sheer magnitude and richness of the space of x may defeat even the best expert-it is simply impossible to specify g for all possible xs one is likely to see in the future. we have to be prepared to live with imperfect classifiers. in fact how should we measure the quality of a classifier? we cant just dismiss a classifier just because it misclassifies a particular x. for one thing if the observation does not fully describe the underlying process is if y is not a deterministic function of x it is possible that the same x may give rise to two different ys on different occasions. for example if we just measure water content of a persons body and we find that the person is dehydrated then the cause class may range from a low water intake in hot weather to severe diarrhea. thus we introduce a probabilistic setting and let y be an n d x m-valued introduction random pair. the distribution of y describes the frequency of encountering particular pairs in practice. an error occurs if gx i y and the probability of error for a classifier g is lg pgx i y there is a best possible classifier g which is defined by g arg min pgx i y grd-i note that g depends upon the distribution of y. if this distribution is known g may be computed. the problem of finding g is bayes j problem and the sifier g is called the bayes classifier the bayes rule. the minimal probability of error is called the bayes error and is denoted by l lg. mostly the distribution of y is unknown so that g is unknown too. we do not consult an expert to try to reconstruct g but have access to a good database of pairs yi i n observed in the past. this database may be the result of experimental observation for meteorological data fingerprint data ecg data or handwritten characters. it could also be obtained through an expert or a teacher who filled in the yis after having seen the xso to find a classifier g with a small probability of error is hopeless unless there is some assurance that the yisjointly are somehow representative of the unknown distribution. we shall assume in this book that yn the data is a sequence of independent identically distributed random pairs with the same distribution as that of y. this is a very strong assumption indeed. however some retical results are emerging that show that classifiers based on slightly dependent data pairs and on i.i.d. data pairs behave roughly the same. also simple models are easier to understand are more amenable to interpretation. a classifier is constructed on the basis of xl x n yn and is denoted by gn y is guessed by gnx xl yi xn yn. the process of constructing gn is called learning supervised learning or learning with a teacher. the performance of gn is measured by the conditional probability of error this is a random variable because it depends upon the data. so ln averages over the distribution of y but the data is held fixed. averaging over the data as well would be unnatural because in a given application one has to live with the data at hand. it would be marginally useful to know the number eln as this number would indicate the quality of an average data sequence not your data sequence. this text is thus about l n the conditional probability of error. an individual mapping gn nd x x mn m is still called a classifier. a sequence n i is called a rule. thus classifiers are functions and rules are sequences of functions. a novice might ask simple questions like this how does one construct a good classifier? how good can a classifier be? is classifier a better than classifier b? can we estimate how good a classifier is? what is the best classifier? this book partially introduction answers such simple questions. a good deal of energy is spent on the mathematical formulations of the novices questions. for us a rule-not a classifier-is good if it is consistent that is if lim eln l n-oo or equivalently if ln l in probability as n we assume that the reader has a good grasp of the basic elements of probability including notions such as convergence in probability strong laws of large numbers for averages and conditional probability. a selection of results and definitions that may be useful for this text is given in the appendix. a consistent rule guarantees us that taking more samples essentially suffices to roughly reconstruct the unknown distribution of y because ln can be pushed as close as desired to l in other words infinite amounts of information can be gleaned from finite samples. without this guarantee we would not be motivated to take more samples. we should be careful and not impose conditions on y for the consistency of a rule because such conditions may not be verifiable. if a rule is consistent for all distributions of y itis said to be universally consistent. interestingly until it was not known if a universally consistent rule existed. all consistency results came with restrictions on y. in stone showed that one could just take any k-nearest neighbor rule with k kn and kin the k-nearest neighbor classifier gnx takes a majority vote over the ys in the subset of k pairs yi from yi yn that have the smallest values for ii xi x ii for which xi is closest to x. since stones proof of the universal consistency of the k-nearest neighbor rule several other rules have been shown to be universally consistent as well. this book stresses universality and hopefully gives a reasonable account of the developments in this direction. probabilists may wonder why we did not use convergence with probability one in our definition of consistency. indeed strong consistency-convergence of ln to l with probability one-implies convergence for almost every sample as it grows. fortunately for most well-behaved rules consistency and strong consistency are equivalent. for example for the k-nearest neighbor rule k and kin together imply ln l with probability one. the equivalence will be dealt with but it will not be a major focus of attention. most if not all equivalence results are based upon some powerful concentration inequalities such as mcdiarmids. for example we will be able to show that for the k-nearest neighbor rule there exists a number c such that for all e there exists n depending upon the distribution of y such that pln l e n ne this illustrates yet another focus of the book-inequalities. whenever possible we make a case or conclude a proof via explicit inequalities. various parameters can be substituted in these inequalities to allow the user to draw conclusions regarding sample size or to permit identification of the most important parameters. the material in the book is often technical and dry. so to stay focused on the main issues we keep the problem simple introduction a. we only deal with binary classification the class y takes values in i and a classifier gn is a mapping rd x x ln i. b. we only consider i.i.d. data sequences. we also disallow active learning a set-up in which the user can select the xis deterministically. c. we do not consider infinite spaces. for example x cannot be a random function such as a cardiogram. x must be a rd-valued random vector. the reader should be aware that many results given here may be painlessly extended to certain metric spaces of infinite dimension. let us return to our novices questions. we know that there are good rules but just how good can a classifier be? obviously ln l in all cases. it is thus important to know l or to estimate it for if l is large any classifier including yours will perform poorly. but even if l were zero ln could still be large. thus it would be nice to have explicit inequalities for probabilities such as pln l however such inequalities must necessarily depend upon the distribution of y. that is for any rule lim inf n-oo all distributions of with sup pln l e universal rate of convergence guarantees do not exist. rate of convergence studies must involve certain subclasses of distributions of y. for this reason with few exceptions we will steer clear of the rate of convergence quicksand. even if there are no universal performance guarantees we might still be able to satisfy our novices curiosity if we could satisfactorily estimate ln for the rule at hand by a function ln of the data. such functions are called error estimates. for example for the k-nearest neighbor classifier we could use the deleted estimate where gnixi classifies xi by the k-nearest neighbor method based upon the data yd yn with yi deleted. if this is done we have a free inequality i piln ln i e nf rogers-wagner inequality provided that distance ties are broken in an propriate manner. in other words without knowing the distribution of y we can state with a certain confidence that ln is contained in f f. thus for many classifiers it is indeed possible to estimate ln from the data at hand. however it is impossible to estimate l universally well for any n and any timate of l based upon the data sequence there always exists a distribution of y for which the estimate is arbitrarily poor. can we compare rules and again the answer is negative there exists introduction no classifier superclassifier as for any rule there exists a bution of y and another rule such that for all n elg elgn. if there had been a universally best classifier this book would have been essary we would all have to use it all the time. this nonexistence implies that the debate between practicing pattern recognizers will never end and that simulations on particular examples should never be used to compare classifiers. as an ple consider the i-nearest neighbor rule a simple but not universally consistent rule. yet among all k-nearest neighbor classifiers the i-nearest neighbor classifier is admissible-there are distributions for which its expected probability of error is better than for any k-nearest neighbor classifier with k so it can never be totally dismissed. thus we must study all simple rules and we will reserve many pages for the nearest neighbor rule and its derivatives. we will for example prove the cover-hart inequality and hart which states that for all distributions of y lim sup eln n-oo where ln is the probability of error with the i-nearest neighbor rule. as l is usually small otherwise you would not want to do discrimination is small too and the i-nearest neighbor rule will do just fine. the nonexistence of a best classifier may disappoint our novice. however we may change the setting somewhat and limit the classifiers to a certain class c such as all k-nearest neighbor classifiers with all possible values for k. is it possible to select the best classifier from this class? phrased in this manner we cannot possibly do better than def l mf pgnx y gn ec typically l l interestingly there is a general paradigm for picking classifiers from c and to obtain universal performance guarantees. it uses empirical risk imization a method studied in great detail in the work of vapnik and chervonenkis for example if we select gn from c by minimizing then the corresponding probability of error ln satisfies the following inequality for all e pln les v here v is an integer depending upon the massiveness of c only. v is called the vc dimension of c and may be infinite for large classes c. for sufficiently restricted classes c v is finite and the explicit universal bound given above can be used to obtain performance guarantees for the selected g n to l not l the bound above is only valid if c is independent of the data pairs yi yn. fixed classes such as all classifiers that decide on a halfspace and on its complement are fine. we may also sample m more pairs addition to the n pairs introduction already present and use the n pairs as above to select the best k for use in the k-nearest neighbor classifier based on the m pairs. as we will see the selected rule is universally consistent if both m and n diverge and n flog m and we have automatically solved the problem of picking k. recall that stones universal consistency theorem only told us to pick k om and to let k but it does not tell us whether k m is preferable over k m empirical risk minimization produces a random data-dependent k that is not even guaranteed to tend to infinity or to be oem yet the selected rule is universally consistent. we offer virtually no help with algorithms as in standard texts with two notable exceptions. ease of computation storage and interpretation has spurred the velopment of certain rules. for example tree classifiers construct a tree for storing the data and partition r by certain cuts that are typically perpendicular to a ordinate axis. we say that a coordinate axis is such classifiers have obvious computational advantages and are amenable to interpretation-the components of the vector x that are cut at the early stages of the tree are most crucial in reaching a decision. expert systems automated medical diagnosis and a host of other nition rules use tree classification. for example in automated medical diagnosis one may first check a patients pulse if this is zero the patient is dead. if it is below the patient is weak. the first component is cut twice. in each case we may then consider another component and continue the breakdown into more and more specific cases. several interesting new universally consistent tree classifiers are described in chapter the second group of classifiers whose development was partially based upon easy implementations is the class of neural network classifiers descendants of rosenblatts perceptron these classifiers have unknown rameters that must be trained or selected by the data in the way we let the data pick k in the k-nearest neighbor classifier. most research papers on neural networks deal with the training aspect but we will not. when we say the parameters by empirical risk minimization we will leave the important algorithmic complexity questions unanswered. perceptrons divide the space by one hyperplane and attach decisions and to the two halfspaces. such simple classifiers are not consistent except for a few distributions. this is the case for example when x takes ues on hypercube and the components of x are independent. neural networks with one hidden layer are universally consistent if the parameters are well-chosen. we will see that there is also some gain in considering two hidden layers but that it is not really necessary to go beyond two. complexity of the training algorithm-the phase in which a classifier gn is selected from c-is of course important. sometimes one would like to obtain classifiers that are invariant under certain transformations. for example the nearest neighbor classifier is not invariant under nonlinear transformations of the coordinate axes. this is a drawback as components are often measurements in an arbitrary scale. switching to a logarithmic scale or stretching a scale out by using fahrenheit instead of celsius should not affect good discrimination rules. there exist variants of the k-nearest neighbor rule that have the given invariance. in character recognition sometimes all components of a vector x that represents introduction a character are true measurements involving only vector differences between lected points such as the leftmost and rightmost points the geometric center the weighted center of all black pixels the topmost and bottommost points. in this case the scale has essential information and invariance with respect to changes of scale would be detrimental. here however some invariance with respect to orthonormal rotations is healthy. we follow the standard notation from textbooks on probability. thus random variables are uppercase characters such as x y and z. probability measures are denoted by greek letters such as jl and numbers and vectors are denoted by lowercase letters such as a b c x and y. sets are also denoted by roman capitals but there are obvious mnemonics s denotes a sphere b denotes a borel set and so forth. if we need many kinds of sets we will typically use the beginning of the alphabet b c. most functions are denoted by j g and calligraphic letters such as a c andf are used to denote classes of functions or sets. a short list of frequently used symbols is found at the end of the book. at the end of this chapter you will find a directed acyclic graph that describes the dependence between chapters. clearly prospective teachers will have to select small subsets of chapters. all chapters without exception are unashamedly retical. we did not scar the pages with backbreaking simulations or quick engineering solutions. the methods gleaned from this text must be supplemented with a healthy dose of engineering savvy. ideally students should have a panion text filled with beautiful applications such as automated virus recognition telephone eavesdropping language recognition voice recognition in security tems fingerprint recognition or handwritten character recognition. to run a real pattern recognition project from scratch several classical texts on statistical tern recognition could and should be consulted as our work is limited to general probability-theoretical aspects of pattern recognition. we have over exercises to help the scholars. these include skill honing exercises brainteasers cute zles open problems and serious mathematical challenges. there is no solution manual. this book is only a start. use it as a toy-read some proofs enjoy some inequalities learn new tricks and study the art of camouflaging one problem to look like another. learn for the sake of learning. introduction introduction inequalities and alternate distance measures the bayes error linear discrimination nearest neighbor rules consistency slow rates of convergence error estimation the regular histogram rule kernel rules consistency of the k-nearest neighbor rule vapnik-chervonenkis theoi combinatorial aspects of v apnik theory lower bounds for empirical classifier selection the maximum likelihood principle parametric classification generalized linear discrimination complexity regularization condensed and edited nearest neighbor rules tree classifiers data-dependent partitioning splitting the data the resubstitution estimate deleted estimates of the error probability automatic kernel rules automatic nearest neighbor rules hypercubes and discrete spaces epsilon entropy and totally bounded sets uniform laws of large numbers neural networks other error estimates feature extraction figure the bayes error the bayes problem in this section we define the mathematical model and introduce the notation we will use for the entire book. let y be a pair of random variables taking their respective values from rd and i. the random pair y may be described in a variety of ways for example it is defined by the pair where il is the probability measure for x and is the regression of y on x. more precisely for a borel-measurable set a s r d ila px e a and for any x e r d ply llx x eyix x. thus is the conditional probability that y is given x x. to see that this suffices to describe the distribution of y observe that for any c s rd x i we have c n x u n x dt co x u x and px y e c px e co y o px e y i lco lci the bayes error as this is valid for any borel-measurable set c the distribution of y is mined by ry. the function ry is sometimes called the a posteriori probability. any function g n d defines a classifier or a decision function. the error probability of g is lg pgx y. of particular interest is the bayes decision function g if ryx otherwise. this decision function minimizes the error probability. theorem for any decision function g nd pgx y pgx y that is g is the optimal decision. proof. given x x the conditional error probability of any decision g may be expressed as pgx yix x py gxix x gx x gx xd iix x igxopy oix x fgxo ryx where fa denotes the indicator of the set a. thus for every x e n d pgx yix x pgx yix x ryx fgexl ryx igxo by the definition of g. the statement now follows by integrating both sides with respect to il-dx. decide class figure the bayes decision in the example on the left is if x a and otherwise. a simple example remark. g is called the bayes decision and l pgx y is referred to as the bayes probability of error bayes error or bayes risk. the proof given above reveals that lg e igxol ryx and in particular l e ryx we observe that the a posteriori probability ryx py lx x eyx x minimizes the squared error when y is to be predicted by fx for some function f rd r e e to see why the above inequality is true observe that for each x e r d e x e ryx x ryx ryx x ryx eryx yix x ryx e x the conditional median i.e. the function minimizing the absolute error e i f y i is even more closely related to the bayes rule problem a simple example let us consider the prediction of a students performance in a course when given a number of important factors. first let y denote a pass and let y stand for failure. the sole observation x is the number of hours of study per week. this in itself is not a foolproof predictor of a students performance because for that we would need more information about the students quickness of mind health and social habits. the regression function ryx py x is probably monotonically increasing in x. if it were known to be ryx x x c say our problem would be solved because the bayes decision is gx if ryx x c otherwise. the bayes error the corresponding bayes error is l lg e x ex while we could deduce the bayes decision from alone the same cannot be said for the bayes error l requires knowledge of the distribution of x. if x e with probability one in an army school where all students are forced to study e hours per week then l if we have a population that is nicely spread out say x is uniform on then the situation improves mince x e x l dx far away from x e discrimination is really simple. in general discrimination is much easier than estimation because of this phenomenon. another simple example let us work out a second simple example in which y or y according to whether a student fails or passes a course. x represents one or more observations regarding the student. the components of x in our example will be denoted by t b and e respectively where t is the average number of hours the students watches tv b is the average number of beers downed each day and e is an intangible quantity measuring extra negative factors such as laziness and learning difficulties. in our cooked-up example we have yl otherwise. thus if t b and e are known y is known as well. the bayes classifier decides if t b e and otherwise. the corresponding bayes probability of error is zero. unfortunately e is intangible and not available to the observer. we only have access to t and b. given t and b when should we guess that y i? to answer this question one must know the joint distribution of b e or equivalently the joint distribution of b y. so let us assume that t b and e are i.i.d. exponential random variables they have density e-u on the bayes rule compares py lit b with py b and makes a decision consistent with the maximum of these two values. a simple calculation shows that py lit b pt b e b pre t bit b max the crossover between two decisions occurs when this value equals thus the bayes classifier is as follows gt b if t o otherwise. another simple example of course this classifier is not perfect. the probability of error is pgt b y pt e b t b e e p e i xe-x dx xe-x dx o the density of t b is ue-u on log e jxoo ue-udu xe- x e if we have only access to t then the bayes classifier is allowed to use t only. first we find pyiit max the crossover at occurs at t c so that the bayes classifier is given by def gt if t otherwise. the probability of error is pgt y pt c t b e pt c t b e e itcd p e-xl dx e-x dx e the bayes error has increased slightly but not by much. finally if we do not have access to any of the three variables t b and e the best we can do is see which the bayes error class is most likely. to this end we compute pry o pt b e if we set g all the time we make an error with probability in practice bayes classifiers are unknown simply because the distribution of y is unknown. consider a classifier based upon b. rosenblatts tron chapter looks for the best linear classifier based upon the data. that is the decision is of the form i get b if at bb c otherwise for some data-based choices for a band c. if we have lots of data at our disposal then it is possible to pick out a linear classifier that is nearly optimal. as we have seen above the bayes classifier happens to be linear. that is a sheer coincidence of course. if the bayes classifier had not been linear-for example if we had y even the best perceptron would be suboptimal regardless of how many data pairs one would have. if we use the neighbor rule the asymptotic probability of error is not more than times the bayes error which in our example is about the example above also shows the need to look at individual components and to evaluate how many and which components would be most useful for discrimination. this subject is covered in the chapter on feature extraction other formulas for the bayes risk the following forms of the bayes error are often convenient l inf pgx y grcol e e ii figure the bayes decision when conditional densities exist. in the figure on the left the decision is on b and elsewhere. pi class i class class plug-in decisions in special cases we may obtain other helpful forms. for example if x has a density f then l f f minl pfox plixdx where p py i and fix is the density of x given that y i. p and p are called the class probabilities and fa fl are the class-conditional densities. if fa and ii are nonoverlapping that is f fofl then obviously l assume moreover that p then l f min flxdx f ii f iflx foxldx. fox dx here g denotes the positive part of a function g. thus the bayes error is directly related to the l distance between the class densities. figure the shaded area is the l distance between the class-conditional densities. plug-in decisions the best guess of y from the observation x is the bayes decision if if otherwise. g otherwise the function is typically unknown. assume that we have access to nonnegative functions ix respectively. in this case it seems natural to use the plug-in decision function ix that approximate and gx if ix otherwise the bayes error to approximate the bayes decision. the next well-known theorem e.g. van ryzin wolverton and wagner glick csibi gyorfi devroye and wagner devroye and devroye and gyorfi states that if is close to the real a posteriori probability in l then the error probability of decision g is near the optimal decision g. theorem for the error probability of the plug-in decision g defined above we have pgx y l lrd and pgx y l s r lrd proof. if for some x e nd gx gx then clearly the difference between the conditional error probabilities of g and g is zero pgx yix x pgx yix x o. otherwise if gx gx then as seen in the proof of theorem the difference may be written as pgx yix x pgx yix x igxl liigx gx. thus pgx y l jrd since gx gx implies jrf when the classifier gx can be put in the form s g otherwise where are some approximations of and respectively the situation differs from that discussed in theorem if does not necessarily equal to one. however an inequality analogous to that of theorem remains true bayes error versus dimension theorem the error probability of the decision defined above is bounded from above by pgx y l ljx jrd iloxll-ldx jrd iljx ill the proof is left to the reader remark. assume that the class-conditional densities fo ii exist and are mated by the densities ir assume furthermore that the class probabilities p pry and p py o are approximated by pi and po respectively. then for the error probability of the plug-in decision function gx if pijix pojox otherwise pgx y l pfoex pojoexldx jrd jrd ipfiex piirexldx. see problem bayes error versus dimension the components of x that matter in the bayes classifier are those that explicitly appear in ljx. in fact then all discrimination problems are one-dimensional as we could equally well replace x by ljex or by any strictly monotone function of such as lj ex ljx. unfortunately lj is unknown in general. in the example in section we had in one case ljt b max t- b and in another case ljet max the former format suggests that we could base all decisions on t b. this means that if we had no access to t and b individually but to t b jointly we would be able to achieve the same results! since lj is unknown all of this is really irrelevant. in general the bayes risk increases if we replace x by tex for any formation t problem as this destroys information. on the other hand there exist transformations as ljx that leave the bayes error untouched. for more on the relationship between the bayes error and the dimension refer to chapter the bayes error problems and exercises problem let t x x be an arbitrary measurable function. if l and lx denote the bayes error probabilities for y and y respectively then prove that shows that transformations of x destroy information because the bayes risk creases. problem let x be independent of y. prove that lexxf l. problem show that l minp p where p p are the class probabilities. show that equality holds if x and y are independent. exhibit a distribution where x is not independent of y but l rninp p. problem neyman-pearson lemma. consider again the decision problem but with a decision g we now assign two error probabilities log pgx o and pgx oiy i. assume that the class-conditional densities fo fl exist. for c define the decision if cflx otherwise. prove that for any decision g if log logc then llgc in other words if l is required to be kept under a certain level then the decision minimizing l has the form of gc for some c. note that g is like that. problem decisions with rejection. sometimes in decision problems one is allowed to say dont know if this does not happen frequently. these decisions are called decisions with a reject option e.g. forney chow formally a decision gx can have three values and there are two performance measures the probability of rejection pgx and the error probability pgx i ylgx i for a c define the decision gcx if ryx c ifryx c otherwise. show that for any decision g if pgx pgcx then pgx i y!gx i pgcx i ylgcx i thus to keep the probability of rejection under a certain level decisions of the form of gc are optimal gyorfi and vajda problems and exercises problem consider the prediction of a students failure based upon variables t and b where y and e is an inaccessible variable section let t b and e be independent. merely by changing the distribution of e show that the bayes error for classification based upon b can be made as close as desired to let t and b be independent and exponentially distributed. find ajoint distribution of b e such that the bayes classifier is not a linear classifier. let t and b be independent and exponentially distributed. find a joint distribution of b e such that the bayes classifier is given by gt b if otherwise. find the bayes classifier and bayes error for classification based on b y as above if b e is uniformly distributed on problem assume that t b and e are independent uniform random variables with interpretations as in section let y denote whether a student passes a course. assume that y if and only if t b e find the bayes decision if no variable is available if only t is available and if only t and b are available. determine in all three cases the bayes error. determine the best linear classifier based upon t and b only. problem let nd be arbitrary measurable functions and define the corresponding decisions by gx itx and gx itix prove that lgff pgx i gffx and e problem prove theorem problem assume that the class-conditional densities fo and fi exist and are proximated by the densities fo and a respectively. assume furthermore that the class probabilities p pry i and p pry o are approximated by pi and po. prove that for the error probability of the plug-in decision function if pi a po ex otherwise gx we have pgx i y l llpfl pia exldx pfox poloxldx. nd problem using the notation of problem show that if for a sequence of fmn and pmn nd the bayes error then for the corresponding sequence of plug-in decisions limn---oo pgnx yj l and wagner hint according to problem it suffices to show that if we are given a deterministic sequence of density functions f h then implies n---oo lim f lim f ifnx fx dx fxldx o. function f is called a density function if it is nonnegative and f fxdx to see this observe that where ai a is a partition of n d into unit cubes and f denotes the positive part of a function f. the key observation is that convergence to zero of each term of the infinite sum implies convergence of the whole integral by the dominated convergence theorem since f fx dx f fnxdx handle the right-hand side by the cauchy-schwarz inequality. problem define the ll error of a function f n d n by jf elfx yi. show that a function minimizing j is the bayes rule g that is j inf f j j thus j l define a decision by gx if fxs otherwise prove that its error probability lg pgx y satisfies the inequality lg l jf j. inequalities and alternate distance measures measuring discriminatory information in our two-class discrimination problem the best rule has probability of error l e this quantity measures how difficult the discrimination problem is. it also serves as a gauge of the quality of the distribution of y for pattern recognition. put differently if and are certain many-to-one mappings l may be used to compare discrimination based on y with that based on y. when projects n d to n d by taking the first d coordinates and takes the last coordinates the corresponding bayes errors will help us decide which projection is better. in this sense l is the fundamental quantity in feature extraction. other quantities have been suggested over the years that measure the natory power hidden in the distribution of y. these may be helpful in some settings. for example in theoretical studies or in certain proofs the relationship tween l and the distribution of y may become clearer via certain inequalities that link l with other functionals of the distribution. we all understand moments and variances but how do these simple functionals relate to l perhaps we may even learn a thing or two about what it is that makes l small. in feature selection some explicit inequalities involving l may provide just the kind of numerical formation that will allow one to make certain judgements on what kind of feature is preferable in practice. in short we will obtain more information about l with a variety of uses in pattern recognition. inequalities and alternate distance measures in the next few sections we avoid putting any conditions on the distribution of y. the kolmogorov variational distance inspired by the total variation distance between distributions the kolmogorov variational distance bko iix pry dixi e captures the distance between the two classes. we will not need anything special to deal with bko as l e g ii i i okq the nearest neighbor error the asymptotic error of the nearest neighbor rule is lnn ryx chapter clearly as also using the notation a ryx we have l lnn a a the second association inequality of theorem which are well-known inequalities of cover and hart lnn provides us with quite a bit of information about l the measure lnn has been rediscovered under other guises devijver and kittler and vajda call it the quadratic entropy and mathai and rathie refer to it as the harmonic mean coefficient. the bhattacharyya affinity figure relationship between the bayes error and the asymptotic est neighbor error. every point in the unshaded region is possible. lnn o the bhattacharyya affinity the bhattacharyya measure of affinity is log p where p e will be referred to as the matushita error. it does not occur naturally as the limit of any standard discrimination rule however problem p was suggested as a distance measure for pattern recognition by matushita it also occurs under other guises in mathematical statistics-see for example the hellinger distance literature cam beran clearly p if and only if e i with probability one that is if l o. furthermore p takes its maximal value if and only if with probability one. the relationship between p and l is not linear though. we will show that for all distributions lnn is more useful than p if it is to be used as an approximation of l theorem for all distributions we have t nn l p. proof. first of all e jensens inequality inequalities and alternate distance measures l l the cover-hart inequality second as finally by the cover-hart inequality for all e we see that p lnn l. l putting all these things together establishes the chain of inequalities. figure the inequalities linking p to l are illustrated. note that the gion is larger than that cut out in the lnn plane in figure p the inequality lnn p is due to ito the inequality lnn is due to horibe the inequality l p can be found in kailath the left-hand side of the last inequality was shown by hudimoto all these inequalities are tight problem the appeal of quantities like lnn and p is that they involve polynomials of whereas l is nonpolynomial. for certain discrimination problems in which x has a distribution that is known up to certain parameters one may be able to compute lnn and p explicitly as a function of these parameters. via inequalities this may then be used to obtain performance guarantees for parametric discrimination rules of the plug-in type chapter for completeness we mention a generalization of bhattacharyyas measure of affinity first suggested by chernoff where ex e is fixed. for ex be p. the asymmetry introduced by taking ex has no practical interpretation however. entropy the entropy of a discrete probability distribution is defined by entropy lpi log pi where by definition log the key quantity in information theory cover and thomas it has countless applications in many branches of computer science mathematical statistics and physics. the entropys main properties may be summarized as follows. a. with equality if and only if pi for some i. proof log pi for all i with equality if and only if pi for some i. thus entropy is minimal for a degenerate distribution i.e. a distribution with the least amount of b. pk log k with equality if and only if pi pk k. in other words the entropy is maximal when the distribution is mally smeared out. proof by the inequality logx x x o. c. for a bernoulli distribution p the binary entropy p p log p p p is concave in p. assume that x is a discrete random variable that must be guessed by asking questions of the type x e a? for some sets a. let n be the minimum expected number of questions required to determine x with certainty. it is well known that log cover and thomas thus not only measures how spread out the mass of x is but also provides us with concrete computational bounds for certain algorithms. in the simple example above is in fact proportional to the expected computational time of the best algorithm. we are not interested in information theory per se but rather in its usefulness in pattern recognition. for our discussion if we fix x x then y is bernoulli hence the conditional entropy of y given x x is rx rx. it measures the amount of uncertainty or chaos in y given x x. as we know it takes values between rjx e i and log rx and is inequalities and alternate distance measures concave in we define the expected conditional entropy by e e log for brevity we will refer to e as the entropy. as pointed out above e if and only if e l with probability one. thus e and l are related to each other. theorem a. e l l equality fano see cover and thomas p. b. e l nn l c. e log nn log l figure inequalities and of theorem are trated here. o o log proof. part a. define a then e a ea is concave by jensens inequality l partb. e log a a logl a a jensens inequality a jensens inequality jeffreys divergence l nn l part c. by the concavity of ha a as a function of a and taylor series expansion a log therefore by part a log l the cover-hart inequality log remark. the nearly monotone relationship between and l will see lots of uses. we warn the reader that near the origin l may decrease linearly in but it may also decrease much faster than such wide variation was not observed in the relationship between l and lnn it is linear or l and p it is between linear and quadratic. jeffreys divergence jeffreys divergence is a symmetric form of the kullback-leibler divergence okl e rjx log rjx rjx it will be denoted by j e log rjx rjx to understand j note that the function log is symmetric about convex and has minimum at rj as rj rj t the function becomes unbounded. therefore j if prjx e i for this reason its inequalities and alternate distance measures use in discrimination is necessarily limited. for generalizations of j see renyi burbea and rao taneja and burbea it is thus impossible to bound j from above by a function of lnn andor l however lower bounds are easy to obtain. as x x is convex in x and log log we note that by jensens inequality j log ii log l the first bound cannot be universally bettered is achieved when is constant over the space. also for fixed l any value of j above the lower bound is possible for some distribution of y. from the definition of j we see that j if and only if with probability one or l figure this figure illustrates the above lower bound on jeffreys divergence in terms of the bayes ror. j?. logl-cl l related bounds were obtained by toussaint jl j v log jl the last bound is strictly better than our l bound given above. see problem f-errors the error measures discussed so far are all related to expected values of concave functions of py lix. in general if f is a concave function on f-errors we define the f corresponding to y by dfx y e examples of f are the bayes error l fx minx x the asymptotic nearest neighbor error lnn fx x the matushita error p fx x the expected conditional entropy f log x x x the negated jeffreys divergence fx log lx hashlamoun varshney and samarasooriya point out that if fx minxl x for each x e then the corresponding f-error is an upper bound on the bayes error. the closer f is to minx x the tighter the upper bound is. for example fx sinjtx x yields an upper bound tighter than l nn all these errors share the property that the error increases if x is transformed by an arbitrary function. theorem let t nd nk be an arbitrary measurable function. thenfor any distribution of y proof. define nk by py x and observe that e thus e e e ltx jensens inequality dfx y. remark. we also see from the proof that the f remains unchanged if the transformation t is invertible. theorem states that f are a bit like bayes errors-when information is lost replacing x by tx f increase. inequalities and alternate distance measures the mahalanobis distance two conditional distributions with about the same covariance matrices and means that are far away from each other are probably so well separated that l is small. an interesting measure of the visual distance between two random variables xo and xl is the so-called mahalanobis distance given by whereml mo exo are the means e mdxl mil and e moxo mol are the covariance matrices p p l is the transpose of a vector and p p is a mixture parameter. if i where i is the identity matrix then m_l is a scaled version of the distance between the means. if i i then iiml moll j pjf varies between ilml and ilml as p changes from too. assume that we have a discrimination problem in which given y x is distributed as xl given y x is distributed as xo and p py p are the class probabilities. then interestingly is related to the bayes error in a general sense. if the mahalanobis distance between the class-conditional distributions is large then l is small. theorem and kittler p. for all distributions of yfor which e we have ll nn p remark. for a distribution with mean m and covariance matrix the mahalanobis distance from a point x e n d to m is in one dimension this is simply interpreted as distance from the mean as measured in units of standard deviation. the use of mahalanobis distance in discrimination is based upon the intuitive notion that we should classify according to the class for which we are within the least units of standard deviations. at least for distributions that look like nice globular clouds such a recommendation may make sense. f-divergences proof. first assume that d that is x is real valued. let u and c be real numbers and consider the quantity e c we will show that if u and c are chosen to minimize this number then it satisfies o e c lnn which proves the theorem for d to see this note that the expression is minimized for c ex e c then e c i varx covx i covx z ex exz ez-which is in turn minimized for u covx straightforward calculation shows that indeed holds. to extend the inequality to multidimensional problems apply it to the one-dimensional decision problem y where z xtb-lml mo. then the theorem follows by noting that by theorem where lnnx y denotes the nearest-neighbor error corresponding to y. in case x i and xo are both normal with the same covariance matrices we have theorem see problem when xl and xo are multivariate normal random variables with bl bo b then if the class-conditional densities and may be written as functions of md and molbolx mo respectively remains relatively tightly linked with l and krzanowski but such tributions are the exception rather than the rule. in general when is small it is impossible to deduce whether l is small or not problem f-divergences we have defined error measures as the expected value of a concave function of this makes it easier to relate these measures to the bayes error l and other error probabilities. in this section we briefly make the connection to the more classical statistical theory of distances between probability measures. a general concept of these distance measures called i-divergences was introduced by csiszar inequalities and alternate distance measures the corresponding theory is summarized in vajda f defined earlier may be calculated if one knows the class probabilities p p and the conditional distributions flo fl of x given o and i that is flia px e aiy i i for fixed class probabilities an f is small if the two conditional distributions are away from each other. a metric quantifying this distance may be defined as follows. let f r u oo be a convex function with fl the f between two probability measures fl and v on rd is defined by difl v sup l vajf j aa j j vaj where the supremum is taken over all finite measurable partitions a of rd. if a is a measure dominating fl and v-that is both fl and v are absolutely continuous with respect to a-and p dflda and q dvda are the corresponding densities then the f may be put in the form difl v f qxf adx. qx clearly this quantity is independent of the choice of a. for example we may take a fl v. if fl and v are absolutely continuous with respect to the lebesgue measure then a may be chosen to be the lebesgue measure. by jensens inequality difl v and difl fl an important example of f is the total variation or variational distance obtained by choosing f ix yielding vfl v sup l ifla j vajl aa j j for this divergence the equivalence of the two definitions is stated by scheffes theorem problem theorem vfl v sp ifla vai f ipx qxiadx where the supremum is taken over all borel subsets of rd. another important example is the hellinger distance given by fx h v sup l j fla j j aa j j v pxqxadx i-divergences the quantity v f is often called the hellinger integral. we mention two useful inequalities in this respect. for the sake of simplicity we state their discrete form. integral forms are analogous see problem lemma for positive sequences ai and bi both summing to one proof. by the cauchy-schwarz inequality l..t yuiui_ re iaibi this together with the inequality and symmetry implies cb ai bi lminai bi. lemma and gyorfi p. let al ak bt bk be nonnegative numbers such that li ai li bi then proof. the cauchy-schwarz inequality inequalities and alternate distance measures k il l yai sl-tjah which proves the lemma. information divergence is obtained by taking f x log x i v is also called the kullback-leibler number. our last example is the defined by fx va sup aa j j vaj f adx qx next we highlight the connection between f and f let ilo and ill denote the conditional distributions of x given o and i. assume that the class probabilities are equal p if f is a concave function then the f dfx y may be written as where fx f lx and d f is the corresponding f it is easy to see that f is convex whenever f is concave. a special case of this correspondence is l v il if p also it is easy to verify that where p is the matushita error. for further connections we refer the reader to the exercises. problems and exercises problems and exercises showthatforeveryi is i there exists a distribution of y with lnn ii andl therefore the cover-hart inequalities are not universally improvable. problem tightness of the bounds. theorem cannot be improved. show that for all a e there exists a distribution of y such that lnn l a. show that for all a e there exists a distribution of y such that lnn a l show that for all a e there exists a distribution of y such that lpa. show that for all a e there exists a distribution of y such that l nn-a l- problem show that e l problem for any a find a sequence of distributions of yn having expected conditional entropies en and bayes errors l such that l as n and en decreases to zero at the same rate as a problem concavity of error measures. let y denote the mixture random variable taking the value with probability p and the value with probability let x be a fixed rd-valuedrandom variable and define llx x x where are bernoulli random variables. clearly prjl p which of the error measures l p lnn e are concave in p for fixed joint distribution of x can every discrimination problem y be decomposed this way for some p where e i for all x? if not will the condition e i for all x do? problem show that for every e there exists a distribution of y with l l and e hl l thus fanos inequality is tight. problem toussaints inequalities mimic a proof in the text to show that j log j i problem show that l e-oc where dc is chernoffs measure of affinity with parameter a e problem prove that l p where p pry i and maxx problem show that j p where p pry i problem let fl and fo be two multivariate normal densities with means rno rn and common covariance matrix l ifpy i p and ii fo are the conditional densities of inequalities and alternate distance measures x given y and y respectively then show that p where p is the matushita error and is the mahalanobis distance. problem for every e and e with find tions ilo and ill for x given y y such that the distance tl yet l therefore the distance is not universally related to the bayes risk. problem show that the mahalanobis distance is invariant under linear invertible transformations of x. problem lissack and fu have suggested the measures for ex this is twice the distance show the following ifo ex then olf l oa. if ex then ja l olf problem hashlamoun varshney and samarasooriya suggest using the error with the function f x sill trx e to obtain tight upper bounds on l show that f minx x so that the sponding f is indeed an upper bound on the bayes risk. problem prove that l p vilo ild. problem prove that l philolll. hint mina b.jqb. problem assume that the components of x xed are conditionally independent y and identically distributed that is pxu e aiy j vja for i d and j use the previous exercise to show that problem show that fili hint x-i problem show the following analog of theorem let t n d n k be a surable function and il v probability measures on nd. define the measures ilt and vt on n k by ilta ilt-a and vta vet-lea show that for any convex function j djil v djilt vt. problem prove the following connections between the hellinger integral and the total variation and hint proceed analogously to lemmas and v hil v problems and exercises problem pinskers inequality. show that v kullback and kemperman hint first prove the inequality if and v are concentrated on the same two atoms. then define a px q and the measures v on the set i by va and apply the previous result. conclude by pointing out that scheffes theorem states v v and that v v. and vo linear discrimination in this chapter we split the space by a hyperplane and assign a different class to each halfspace. such rules offer tremendous advantages-they are easy to interpret as each decision is based upon the sign of ll aixi ao where x xed and the ais are weights. the weight vector determines the relative importance of the components. the decision is also easily implemented-in a standard software solution the time of a decision is proportional to d-and the prospect that a small chip can be built to make a virtually instantaneous decision is particularly exciting. rosenblatt realized the tremendous potential of such linear rules and called themperceptrons. changing one or more weights as new data arrive allows us to quickly and easily adapt the weights to new situations. training or ing patterned after the human brain thus became a reality. this chapter merely looks at some theoretical properties of perceptrons. we begin with the simple dimensional situation and deal with the choice of weights in nd further on. unless one is terribly lucky linear discrimination rules cannot provide error probabilities close to the bayes risk but that should not diminish the value of this chapter. linear discrimination is at the heart of nearly every successful pattern recognition method including tree classifiers and generalized linear fiers and neural networks we also encounter for the first time rules in which the parameters are dependent upon the data. linear discrimination input oarl weights figure rosenblatts perceptron. the decision is based upon a linear combination of the components of the input vector. univariate discrimination and stoller splits as an introductory example let x be univariate. the crudest and simplest possible rule is the linear discrimination rule gx y y otherwise if x x where x is a split point and y e is aclass. ingeneralx and y are measurable functions of the data dn. within this class of simple rules there is of course a best possible rule that can be determined if we know the distribution. assume for example that y is described in the standard manner let py p. given y x has a distribution function flx px xly and given y x has a distribution function fox px xly o where fo and fl are the class-conditional distribution functions. then a theoretically optimal rule is determined by the split point x and class y given by y arg min pgx y minimum is always reached if we allow the values x and x we call the corresponding minimal probability of error l and note that l inf fl fox ivl fl p a split defined by y will be called a theoretical stoller split univariate discrimination and stoller splits lemma l with equality if and only if l proof. take y then the probability of erroris p py oj. take y then the probability of error is p. clearly l l minp p. this proves the first part of the lemma. for the second part if l then p and for every x pflx fox and fix p the first inequality implies p fi p p p fox p therefore l while the second implies p fi pfox p thus for all x means that for every x pfix fi fox and therefore l d lemma l sup ipfix x pfox p in particular if p then l sup ifix foxl. x proof. set px p fi p then by definition l inf min p p px x sup i px p i mina b b x la d the last property relates the quality of theoretical stoller splits to the mogorov-smirnov distance supx i fi fox i between the class-conditional distribution functions. as a fun exercise consider two classes with means mo exiy oj ml exiy l and variances varxiy o and af var x i y i. then the following inequality holds. theorem remark. when p chernoff proved l linear discrimination moreover becker pointed out that this is the best possible bound problem proof. assume without loss of generality that mo mi. clearly l is smaller than the probability of error for the rule that decides when x mo and otherwise where ml mo o. decide class decide class figure the split providing the bound of theorem the probability of error of the latter rule is ppx ml i ppx mo o p the chebyshev-cantelli inequality see appendix theorem p p lli a o and molaoao we have yet another example of the principle that well-separated classes yield small values for l and thus l separation is now measured in terms of the largeness of mo i with respect to another inequality in the same spirit is given in problem the limitations of theoretical stoller splits are best shown in a simple example. consider a uniform random variable x and define y if x e ifiex-e if for some small e o. as y is a function of x we have l o. if we are forced to make a trivial x decision then the best we can do is to set gx univariate discrimination and stoller splits the probability of error is p e x e consider next a theoretical stoller split. one sees quickly that the best split occurs at x or x and thus that l in other words even the best theoretical split is superfluous. note also that in the above example mo ml so that the inequality of theorem says l i-it degenerates. we now consider what to do when a split must be data-based. stoller suggests taking y such that the empirical error is minimal. he finds y such that argrrun l... ixsxydy ixxydl-y n il and y are now random variables but in spite of our convention we keep the lowercase notation for now. we will call this stollers rule. the split is referred to as an empirical stoller split. denote the set x x u x y by cx y. then y argmin vncx y where vn is the empirical measure for the data dn yi yn that is for every measurable set a e r x i vna ixiyiea. denoting the measure of y in r x i by v it is clear that evnc vc px x y y px x y y. let ln pgnx yidn be the error probability of the splitting rule gn with the data-dependent choice y given above conditioned on the data. then ln vcx y vcx y vncx y vncx y sup y vncx y vncx y y minimizes vcx y over all y sup ivcx y vncx vcx y sup ivcx y vncx y l. from the next theorem we see that the supremum above is small even for ately large n and therefore stollers rule performs closely to the best split less of the distribution of y. theorem for stollers rule and e and pln l e eln l n linear discrimination proof. by the inequality given just above the theorem p ivcx y vncx y p vncx i p vncx h by a double application of massarts tightened version of the kiefer-wolfowitz inequality see problem we do not prove this inequality here but we will thoroughly discuss several such inequalities in chapter in a greater generality. the second inequality follows from the first via problem the probability of error of stollers rule is uniformly close to l over all possible distributions. this is just a preview of things to come as we may be able to obtain good performance guarantees within a limited class of rules. linear discriminants rosenblatts perceptron see nilsson for a good sion is based upon a dichotomy of rd into two parts by a hyperplane. the linear discrimination rule with weights ao ai ad is given by figure a linear discriminant in that rectly classifies all but four data points if laixi il otherwise gx l d where x o xed. linear discriminants its probability of error is for now denoted by la ao where a ad. again we set l inf aerdaoer la ao for the best possible probability of error within this class. let the class-conditional distribution functions of a i xl xed be denoted by foa and fia depending upon whether y or y for la ao we may use the bounds of lemma and apply them to foa and fia. thus l spsp ipfiax pfoax p which for p reduces to a l sup sup i foaxi. x therefore l if and only if p and for all a fia foa. then apply the following simple lemma. lemma and wold xl and x random variables taking values in r d are identically distributed if and only if at xl and at have the same distribution for all vectors a e rd. proof. two random variables have identical distributions if and only if they have the same characteristic function-see for example lukacs and laha now the characteristic function of x i xid is e e eiaixi assumption the characteristic function of x thus we have proved the following theorem l with equality if and only if l thus as in the one-dimensional case whenever l a meaningful cut by a hyperplane is possible. there are also examples in which no cut improves over a rule in which gx y for some y and all x yet l and l to generalize theorem we offer the following result. a related inequality is shown in problem the idea of using chebyshevs inequality to obtain such bounds is due to yau and lin also devijver and kittler linear discrimination theorem let xo and x i be random variables distributed as x given y and y respectively. set mo exo ml exd. define also the covariance matrices e mlx i mil and e moxo mol. then ll inf aerd proof. for any a end we may apply theorem to at xo and at xl. theorem follows by noting that e xo atexo at mo eatxi and that var xo e moxo mot a atoa varatxi we may obtain explicit inequalities by different choices of a. a ml mo yields a convenient formula. we see from the next section that a mo with p is also a meaningful choice also problem the fisher linear discriminant data-based values for a may be found by various criteria. one of the first methods was suggested by fisher let ml and mo be the sample means for the two classes ml xdli yi picture projecting xl xn to a line in the direction of a. note that this is perpendicular to the hyperplane given by a t x ao o. the projected values are a t xl a t x n. these are all equal to o for those xi on the hyperplane at x through the origin and grow in absolute value as we flee that hyperplane. let and be the sample scatters for classes and respectively that is tx a t--- i-a ml asia t and similarly for ag where iyil sl l mdxi mdt is the scatter matrix for class the fisher linear discriminant is that linear function a t x for which the criterion the normal distribution ja t ml a mo t t-a ml mo atsl soa is maximum. this corresponds to finding a direction a that best separates at rn from at rno relative to the sample scatter. luckily to find that a we need not resort to numerical iteration-the solution is given by fishers suggestion is to replace yn by xl x n yn and to perform one-dimensional discrimination. usually the rule uses a simple split gao if at x ao i otherwise for some constant ao. unfortunately fisher discriminants can be arbitrarily bad there are distributions such that even though the two classes are linearly separable l the fisher linear discriminant has an error probability close to problem the normal distribution there are a few situations in which by sheer accident the bayes rule is a ear discriminant. while this is not a major issue it is interesting to identify the most important case i.e. that of the multivariate normal distribution. the general multivariate normal density is written as where m is the mean x and m are d-component column vectors l is the d x d covariance matrix l-l is the inverse of l and detl is its determinant. nm l. clearly if x has density j then m ex and l we write j ex mx ml. the multivariate normal density is completely specified by d e formal rameters and l. a sample from the density is clustered in an elliptical cloud. the loci of points of constant density are ellipsoids described by for some constant r o. the number r is the mahalanobis distance from x to m and is in fact useful even when the underlying distribution is not normal. it takes into account the directional stretch of the space determined by l. linear discrimination figure points at equal mahalanobis distance from m. given a two-class problem in which x has a density p p!i and and are both multivariate normal with parameters mi li i the bayes rule is easily described by gx if p!lx p!ox otherwise. take logarithms and note that gx if and only if ml motlolx mo p logdetlo in practice one might wish to estimate m mo l lo and p from the data and use these estimates in the formula for g. interestingly as mi f li mi is the squared mahalanobis distance from x to mi in class i rl the bayes rule is simply g otherwise. ifrl pplogdetlodetld in particular when p lo ll l we have x g f r ro otherwise just classify according to the class whose mean is at the nearest mahalanobis distance from x. when lo l l the bayes rule becomes linear if a t x ao otherwise g where a mol-l and ao p mt l-lmo mfl-lml. thus linear discrimination rules occur as special cases of bayes rules for variate normal distributions. our intuition that a should be in the direction m mo to best separate the classes is almost right. note nevertheless that a is not perpendicular in general to the hyperplane of loci at equal distance from mo and mi. when l is replaced by empirical risk minimization the standard data-based estimate we obtain in fact the fisher linear discriminant. furthermore when i the decision boundary is usually not linear and fishers linear discriminant must therefore be suboptimal. in early. statistical work on discrimination the normal historical remarks. distribution plays a central role for a simple introduction we refer to duda and hart mclachlan has more details and raudys relates the error dimensionality and sample size for normal and nearly normal models. see also raudys and pikelis empirical risk minimization in this section we present an algorithm that yields a classifier whose error ity is very close to the minimal error probability l achievable by linear classifiers provided that x has a density. the algorithm selects a classifier by minimizing the empirical error over finitely classifiers. for a rule if at x ao otherwise the probability of error is l p i y. l may be estimated by the empirical risk that is the number of errors made by the classifier is counted and normalized. assume that x has a density and consider d arbitrary data points xii x xid among xn and let at x ao be a hyperplane containing these points. because of the density assumption the d points are in general position with probability one and this hyperplane is unique. this hyperplane determines two classifiers if at x ao otherwise and i if at x ao otherwise whose empirical errors ln l and l n may be calculated. to each d-tuple xii xid of data points we may assign two classifiers in this manner yielding altogether classifiers. denote these classifiers by l let be a linear classifier that minimizes ln i over all i d linear discrimination we denote the best possible error probability by l inf lfjj over the class of all linear rules and define fjj arg min l fjj as the best linear rule. if there are several classifiers with lfjj l then we choose fjj among these in an arbitrary fixed manner. next we show that the classifier corresponding to is really very good. figure if the data points are in general position thenfor each linear rule there exists a linear split defined by a hyperplane crossing d points such that the difference between the empirical errors is at most din first note that there is no linear classifier fjj whose empirical error is smaller than din. this follows from the fact that since the data points are in general position the density assumption then for each linear classifier we may find one whose defining hyperplane contains exactly d data points such that the two decisions agree on all data points except possibly for these d see figure thus we may view minimization of the empirical error over the finite set as approximate minimization over the infinite set of linear classifiers. in chapters and we will develop the full theory for rules that are found by empirical risk minimization. theorem just gives you a taste of things to come. other-more involved but also more general-proofs go back to vapnik and chervonenkis theorem assume that x has a density. isfound by empirical error mization as described above thenjor all possible distributions ojx y ifn d and e we have moreover ifn d then e l log n n remark. with some care theorem and theorem below can be extended so that the density assumption may be dropped. one needs to ensure that the selected empirical risk minimization linear rule has empirical error close to that of the best possible linear rule. with the classifier suggested above this property may fail to hold if the data points are not necessarily of general position. the ideas presented here are generalized in chapter theorem d proof. we begin with the following simple inequality lj l lj ln ln l ln ln din for any max i ln j ln l d n therefore by the union-of-events bound we have p l l e lp l i-ln j- ln n e il to bound the second term on the right-hand side observe that nln is nomially distributed with parameters nand l by an inequality due to chernoff and okamoto for the tail of the binomial distribution p ln l e we prove this inequality later theorem next we bound one term of the sum on the right-hand side. note that by symmetry all terms are equal. assume that the classifier is determined by the d-tuple of the first d data points xl. xd. we write p l l n l e l ln d i xl xd and bound the conditional probability inside. let y yj be independent of the data and be distributed as the data yd yd. define y! y yi l i if i d ifi d. then p l l l n l i xl xd p d t i lxiyd xl xd n idl linear discrimination p l l p l l l ijixjyi xl xd n n il ei d n i i binomlaln l l x i xd e dj l l depends upon xl xd only and y y are independent of xl xd e ii theorem use the fact that e the inequality for the expected value follows from the probability inequality by the following simple argument by the cauchy-schwarz inequality e denoting z for any u ez eziz upz u upz u pz u u e-nu u if u by the probability inequality and since g n d choosing u to minimize the obtained expression yields the desired inequality first verify that the minimum occurs for ne u n where e check that if n d then u n then note that the bound ee-nu u equals n nee n n log n observe for now that the bound on p l e decreases rapidly with n. to have an impact it must become less than for smallo. this happens roughly speaking when n e log for some constant e. doubling d the dimension causes this minimal sample size to roughly double as well. empiiical risk minimization an important special case is when the distribution is linearly separable that is l o. in such cases the empirical risk minimization above performs even better as the size of the error improves to log n in from j d log n in. clearly the data points are linearly separable as well that is with probability one and therefore din with probability one. theorem assume that x has a density and that the best linear classifier has zero probability of error then for the empirical risk minimization algorithm of theorem for all n d and e p j and e l dlogn n-d proof. by the union bound pl p li li d by symmetry this sum equals ni li n xi xd where as in theorem l is determined by the data points xl x d ever p no xi xd p ixdl ydl l l ei xl xd all of the most d errors committed by l occur for yi yd since the probability that no yi pair i d n falls in the set y y is less than e if the probability of the set is larger than e. the x e-x proof of the probability inequality may be completed by noting that linear discrimination for the expected error probability note that for any u pl tdt el u rx u d the probability inequality and g nd u pl tdt e-n-dt dt n we choose u to minimize the obtained bound which yields the desired inequality. o minimizing other criteria empirical risk minimization uses extensive computations because ln is not a unimodal function in general problems and also gradient timization is difficult because the gradients are zero almost everywhere. in fact given n labeled points in n d finding the best linear dichotomy is np hard johnson and preparata to aid in the optimization some have suggested minimizing a modified empirical error such as or n a xi ao t n il where is a positive convex function. of particular importance here is the mean square error criterion e.g. widrow and hoff one can easily verify that ln has a gradient respect to ao that may aid in locating a local minimum. let denote the linear discrimination rule minimizing over all a and ao. a description of the solution is given in problem even in a one-dimensional situation the mean square error criterion muddles the issue and does not give any performance guarantees minimizing other criteria theorem if supxy denotes the supremum with respect to all distributions on r x i then sup l where is a linear discriminant obtained by minimizing over all al and ao. remark. this theorem establishes the existence of distributions of y for which e and l e simultaneously for arbitrarily small e therefore minimizing the mean square error criterion is not recommended unless one has additional information regarding the distribution of y. proof. let e and consider a triatomic distribution of y px y i px y i px y o e. figure a distribution for which squared error minimization fails. probability probability probability for e the best linear rule decides class on and elsewhere for a probability of error l e the mean square error criterion asks that we minimize l e with respect to ao v and a u. setting the derivatives with respect to u and v equal to zero yields v u and v u e for v if we let e and let t then v thus for e small enough and large enough considering the decision at only e because at x ux v v thus l for e small enough and large enough. linear discrimination others have suggested minimizing n l t xi ao il where au is a sigmoid that is an increasing function from to such as see for example wassel and sklansky do tu and installe e- u fritz and gyorfi and sklansky and wassel clearly au iuco provides the empirical en-or probability. however the point here is to use smooth sigmoids so that gradient algorithms may be used to find the optimum. this may be viewed as a compromise between the mean squared en-or criteria and empirical en-or minimization. here too anomalies can occur and the en-or space is not well behaved displaying many local minima krogh and palmer see however problems and problems and exercises problem with the notation of theorem show that the error probability l of a one-dimensional theoretical stoller split satisfies p l p p and vajda is this bound better than that of theorem hint for any threshold rule gcx ix.c and u write lgj pix c pix c i plux c e c i by chebyshevs inequality. choose u and c to minimize the upper bound. problem let p if l is the error probability of the one-dimensional theoretical stoller split show that l show that the bound is achieved for some distribution when the class-conditional tions of x is given y and y are concentrated on two points each one of which is shared by both classes becker problem let x be a univariate random variable. the distribution functions for x given y and y are and fo respectively. assume that the moment generating functions for x exist that is e olt e o ten where are finite for all t. in the spirit of theorem derive an upper bound for l in function of apply your bound to the case that and fo are both normal with possibly different means and variances. problems and exercises problem signals in additne gaussian noise. let so sl e rd be fixed and let n be a multivariate gaussian random variable with zero mean and covariance matrix let pry o pry i and define x so si if y if y construct the bayes decision and calculate l prove that if is the identity matrix and so and si have constant components then l exponentially rapidly as d problem in the last step of the proof of theorem we used the wolfowitz-massart inequality this result states that if zi zn are i.i.d. random variables on the real line with distribution function fz p zi z and empirical distribution function fnz iziozj then use this inequality to conclude that p ivcx vncx i hint map y on the real line by a one-to-one function x i r such that z y if and only if y o. use the dvoretzky-kiefer-wolfowitz-massart inequality for z. problem let l be the probability of error for the best sphere rule that is for the rule that associates a class with the inside of a sphere sxn and the other class with the outside. here the center x and radius r are both variable. show that l if and only if l and that l problem with the notation of theorem show that the probability of error l of the best linear discriminant satisfies p l where fl jrnl rno is the mahalanobis distance with and vajda interestingly the upper bound is just twice the bound of theorem for the asymptotic nearest neighbor error. thus a large mahalanobis distance does not only imply that the bayes error is small but also small error probabilities may be achieved by simple linear classifiers. hint apply the inequality of problem for the univariate random variable x at x a rno. problem if rni and a? are the mean and variance of at x given that y i i where a is a column vector of weights then show that the criterion linear discrimination is minimized for a mo where mi and are the mean vector and covariance matrix of x given y i. also show that is minimized for a po-jmj mo where p p are the class probabilities. this exercise shows that if discrimination is attempted in one dimension we might consider projections a t x where a maximizes the weighted distance between the projected means. problem in the fisher linear discriminant rule with free parameter ao show that for any e there exists a distribution for y x e n such that infao elgao e. moreover if ao is chosen to minimize the squared error with l and eiixii then elgao e. problem find a distribution of y with x e n such that with probability at least one half ln is not unimodal with respect to the weight vector ao. problem the following observation may help in developing a fast algorithm to find the best linear classifier in certain cases. assume that the bayes rule is a linear split cutting through the origin that is l la for some coefficient vector a e n d where la denotes the error probability of the classifier if aixi otherwise and a ad show that la is unimodal as a function of a end and la is monotone increasing along rays pointing from a that is for any e and a e n d la laa and gyorfi hint use the expression la j sign aixu idx to show that la lcaa fa for some set a end. problem let a aj and a arg min e ajx iydgaxid a and gax ialxaoo show that for every e there exists a distribution of y on n x i such that leii l e where la is the error probability for gao hint argue as in the proof of theorem a distribution with four atoms suffices. problem repeat the previous exercise for a argmine ajx aol. a problems and exercises problem let denote the linear discrimination rule that minimizes the mean square error e i at x over all a and ao. as this criterion is quadratic in ao it is unimodal. one usually approximates by by minimizing li i at xi over all a and ao. show that the minimal column vector ao is given by lx where x is a i-dimensional column vector. problem the perceptron criterion is j l xiao latxi find a distribution for which l l yet liminfn---ooeln where is the linear discrimination rule obtained by using the a and ao that minimize j. problem let be a monotone nondecreasing function on n satisfying limu---_oo and limu---oo ou for h define ohu ohu. consider the linear nation rule with a and ao chosen to minimize t t il xi ao for every fixed h and e exhibit a distribution with l e and liminfeln e. n-oo on the other hand show that if h depends on the sample size n such that h as n then for all distributions eln l. problem given y i let x be normal with mean mi and covariance matrix li i consider discrimination based upon the minimization of the criterion with respect to a w and c a d x d matrix d x vector and constant respectively where ou e-u is the standard sigmoid function. show that this is minimized for the same a w and c that minimize the probability of error and conclude that in this particular case the squared error criterion may be used to obtain a bayes-optimal classifier and hush nearest neighbor rules introduction simple rules survive. the k-nearest neighbor rule since its conception in and and hodges has thus attracted many followers and continues to be studied by many researchers. formally we define the k rule by gnx if wni iyill wni iyiol otherwise where wni k if xi is among the k nearest neighbors of x and wni elsewhere. xi is said to be the k-th nearest neighbor of x if the distance iix xi ii is the k-th smallest among ilx xiii ilx xn ii. in case of a distance tie the candidate with the smaller index is said to be closer to x. the decision is based upon a majority vote. it is convenient to let k be odd to avoid voting ties. several issues are worth considering universal consistency. establish convergence to the bayes rule if k and k n as n this is dealt with in chapter finite k performance. what happens if we hold k fixed and let n tend to infinity? the choice of the weight vector l wnn are equal weights for the k nearest neighbors better than unequal weights in some sense? nearest neighbor rules the choice of a distance metric. achieve invariance with respect to a certain family of transformations. the reduction of the data size. can we obtain good performance when the data set is edited andor reduced in size to lessen the storage load? figure at every point the decision is the label of the closest data point. the set of points whose nearest neighbor is xi is called the voronoi cell of xi. the partition induced by the voronoi cells is a varona! partition. a voronoi partition of random points is shown here. in the first couple of sections we will be concerned with convergence issues for k nearest neighbor rules when k does not change with n. in particular we will see that for all distributions the expected error probability el n tends to a limit lknn that is in general close to but larger than l the methodology for obtaining this result is interesting in its own right. the expression for lknn is then studied and several key inequalities such as lnn and hart and lknn l k are proved and applied. the other issues mentioned above are dealt with in the remaining sections. for surveys of various aspects of the nearest neighbor or related methods see dasarathy devijver or devroye and wagner remark. computational concerns. storing the n data pairs in an array and searching for the k nearest neighbors may take time proportional to nkd if done in a naive manner-the accounts for the cost of one distance computation. this complexity may be reduced in terms of one or more of the three factors involved. typically with k and d fixed on lid worst-case time and bentley and n expected time bentley and finkel may be achieved. multidimensional search trees that partition the space and guide the search are invaluable-for this approach see fukunaga and n arendra friedman bentley and finkel niemann and goppert kim and park and broder we refer to a survey in dasarathy for more references. other approaches are described by yunck friedman baskett and shustek vidal sethi and linder and lugosi generally with preprocessing one may considerably reduce the overall complexity in terms of nand d. notation and simple asymptotics notation and simple asymptotics we fix x e rdandreorderthedatax yd ynaccordingtoincreasing values of ii xi x ii. the reordered data sequence is denoted by if no confusion is possible. xkx is the k-th nearest neighbor of x. remark. we note here that rather arbitrarily we defined neighbors in terms of y ii. surprisingly the asymptotic properties derived in the euclidean distance ilx this chapter remain valid to a wide variety of metrics-the asymptotic probability of error is independent of the distance measure problem denote the probability measure for x by fj. and let sxe be the closed ball centered at x of radius e o. the collection of all x with fj. sx for all e is called the support of x or fj.. this set plays a key role because of the following property. lemma if x e supportfj. and limn-oo kin then iixkx x ii with probability one. if x is independent of the data and has probability measure then iixkx xii with probability one whenever kin o. proof. take e o. by definition x e supportfj. implies that fj.sxe o. observe that iixkx x ii e if and only if n lixes n il k n i xe by the strong law of large numbers the left-hand side converges to fj.sxe with probability one while by assumption the right-hand side tends to zero. therefore iixkx xii with probability one. the second statement follows from the previous argument as well. first note that by lemma in the appendix px e supportfj. therefore for every e p xii e e xii eix e supportfj. which converges to zero by the dominated convergence theorem proving gence in probability. if k does not change with n then ii xkx x ii is monotone decreasing for n k therefore it converges with probability one as well. if k kn is allowed to grow with n such that kin then using the notation xknnx xklx we see by a similar argument that the sequence of monotone decreasing random variables sup iixkmmx xii iixknnx xii mn nearest neighbor rules converges to zero in probability and therefore with probability one as well. this completes the proof. because is measurable thus well-behaved in a general sense and iixkx xii is small the values should be close to for all i small enough. we now introduce a proof method that exploits this fact and will make subsequent analyses very simple-it suffices to look at data samples in a new way via embedding. the basic idea is to define an auxiliary rule gl in which the yixs are replaced by k i.i.d. bernoulli random variables with parameter the yixs behave in such a way. it is easy to show that the error probabilities of the two rules are close and analyzing the behavior of the auxiliary rule is much more convenient. to make things more precise we assume that we are given i.i.d. data pairs ud un all distributed as u where x is as before has probability measure f.l on the borel sets of r d and u is uniformly distributed on and independent of x. if we set y i luitjxi then y i y n are i.i.d. and distributed as the prototype pair y. so why bother with the uis? in embedding arguments we will use the same uis to construct a second data sequence that is heavily correlated with the original data sequence and is more convenient to analyze. for example for fixed x e r d we may define yx luiryxl we now have an i.i.d. sequence with i-th vector given by xi yi yx ui. ordering the data sequence according to increasing values of iixi x ii yields a new sequence with the i-th vector denoted by xix yix yix uix. if no confusion is possible the argument x will be dropped. a rule is called k-local if for n k gn is of the form x if ljrx ylx ykx otherwise gil for some function for the k-nn rule we have for example in other words gn takes a majority vote over the k nearest neighbors of x and breaks ties in favor of class to study gn turns out to be almost equivalent to studying the approximate rule g n ifljrx ylx ykx i gil otherwise. the latter rule is of no practical value because it requires the knowledge of interestingly however it is easier to study as ylx ykx are i.i.d. whereas ylx yk are not. note in particular the following lemma for all x n k notation and simple asymptotics p ylx yk yclx yck le k il and pgnx gx le k il proof. both statements follow directly from the observation that yclx ykx yclx yckx yk yck c c u uixs u u hxs ucixs k k il il and using the union bound and the fact that the uixs are uniform we need the following result in which x is distributed as xl but independent of the data sequence lemma for any integrable function f any n and any k n k le kydelfxi il depends upon the dimension only. where yd the proof of this lemma is beautiful but a bit technical-it is given in a separate section. here is how it is applied and why for fixed k we may think of fxkx as fx for all practical purposes. lemma for any integrable function f k k il fxixi as n whenever kin o. proof. given e find a uniformly continuous function g vanishing off a boundedsetasuchthatelgx- fxi e theorem in the appendix. nearest neighbor rules thenforeache othereisao such that ilx-zll o implies e. thus fxix i k k il e k l e gxix i k il k k il fxix i l e yde e iigiioop xkx ii o lemma where depends on e only yde lemma proof of stones lemma in this section we prove lemma for e e a cone cx e is the lection of all y e nd for which anglex y e. equivalently in vector notation x t yiixillyi cose. the set z cx e is the translation of cx e by z. figure a cone of angle e. cx.e ifyz e iizllthenily-zll ilzllaswewillnowshow. indeed ii y liz y ililz ii cosn ii z ilzll figure proof of stones lemma figure the key geometrical erty of cones of angle n the following covering lemma is needed in what follows lemma lete e then there exists a set end such that yd nd u cxi e. il furthermore it is always possible to take ld yd since for e n we have yd proof. we assume without loss of generality that iixi ii for all i. each xi is the center of a sphere si of radius r since si has the property that iixll lnsi iixll lncxie. let us only look at xi such that iixi j r for all j i i. in that case u cxi e covers nd if and only if u si covers iix ii i. then the spheres s of radius centered at the xis are disjoint and u s s figure nearest neighbor rules figure bounding yd. thus if vd volumesol or ydvdd vdld yd l. sme r the last inequality follows from the fact that sin d v figure covering the space by cones. with the preliminary results out of the way we cover nd by yd cones x j yd and mark in each cone the xi that is nearest to the asymptotic probability of error x if such an xi exists. if xi belongs to x cxj and is not marked then x cannot be the nearest neighbor of xi in xi-i x xil x n similarly we might mark all k nearest neighbors of x in each cone there are less than k points in a cone mark all of them. by a similar argument if xi e x c j j is not marked then x cannot be among the k nearest bors of xi in xi x xi x n. order of this set of points is important if distance ties occur with positive probability and they are broken by comparing indices. therefore if is a nonnegative function k le il e ixi is among the k nearest neighbors of xin e t e fx t ik i ix is among the knearest neighbors of xi in exchanging x and xi kyde!x as we can mark at most k nodes in each cone and the number of cones is at most yd-see lemma this concludes the proof of stones lemma. the asymptotic probability of error we return to k-iocal rules in particular to k-nearest neighbor rules. let d i yi vi yn vn be the i.i.d. data augmented by the uniform random variables vi vn as described earlier. for a decision gn based on dn we have the probability of error ln pgnx i yid p ylx ykx i where is the function whose sign determines gn see define the random variables yclx yckx as we did earlier and set l p yclx yckx nearest neighbor rules by lemmas and eiln lzl p ylx ykx f yclx yck l e k il because limn---ooelz eln we need only study the rule gz gn otherwise o zk are i.i.d. bernoulli unless we are concerned with the closeness of ln to eln as well. we now illustrate this important time-saving device on the i-nearest neighbor rule. clearly zi and therefore el pzj f y e we have without further work theorem for the nearest neighbor rulejor any distribution ojx y lim el n e l nn n---oo under various continuity conditions has a density j and both j and are almost everywhere continuous this result is due to cover and hart in the present generality it essentially appears in stone see also devroye elsewhere we show that l lnn l hence the previous result says that the nearest neighbor rule is asymptotically at most twice as bad as the bayes rule-especially for small l this property should be useful. we formally define the quantity when k is odd lknn we have the following result theorem let k be odd andfixed. thenjor the k-nn rule lim el n l knn n---oo the asymptotic error probability of weighted nearest neighbor rules proof. we note that it suffices to show that limnoo el lknn the ously introduced notation. but for every n el p zk y o p zk y i p zk zo o p zk zo i zo zk are i.i.d. bernoulli random variables which leads directly to the sought result. several representations of lknn will be useful for later analysis. for example we have lknn e ryxp binomialk i x e ryxp binomialk ryx i x e e ryx binomialk ryx i x it should be stressed that the limit result in theorem is distribution-free. the limit lknn depends upon only. the continuity or lack of smoothness of is immaterial-it only matters for the speed with which el n approaches the limit l knn the asymptotic error probability of weighted nearest neighbor rules following royall a weighted nearest neighbor rule with weights wi wk makes a decision according to gnx if o otherwise. l...-lcixi w l l...-lycixo w l in case of a voting tie this rule is not symnletric. we may modify it so that gn dt if we have a voting tie. the should be considered as an indecision. by nearest neighbor rules previous arguments the asymptotic probability of error is a function of wi wk given by lwl wk where ap p wiy t wl y p wi y t wl- y p where now y y are i.i.d. bernoulli equivalently with zi e ap pp wizi o pp wizi o assume that p wi zi o for now. then if p ap p wizi o. and an antisymmetric expression is valid when p note next the following. ifweletnz be the number of vectors z zk e lk with l izil i and l wizi then nz nk-z e. thus k l pk-z l pi l nz pl-l pi zo i k p i i i i i i. p ik even note that i i i i does not depend on the vector of weights and represents pbinomialkl p pbinomialk p finally since p i i l nz pl-z pi l pi o. the asymptotic error probability of weighted nearest neighbor rules this term is zero if and only if nz for alii in other words it vanishes if and only if no numerical minority of wis can sum to a majority in the case where alone a numerical minority outweighs the others. but such cases are equivalent to ordinaryk-nearest neighbor rules if k is odd. when k is even and we add a tiny weight to one wi as in e ek ek k k k for small e then no numerical minority can win either and we have an optimal rule i we have thus shown the following theorem and jain let lwi wk be the asymptotic probability of error of the weighted k-nn rule with weights wi wk. let the k-nn rule be defined by k ii k ii k if k is odd and by k ii k ii k for e i k when k is even. denoting the asymptotic probability of error by lknn for the latter rule we have if then equality occurs ifand only if every numerical minority of the wis carries less than half of the total weight. the result states that standard k-nearest neighbor rules are to be preferred in an asymptotic sense. this does not mean that for a particular sample size one should steer clear of nonuniform weights. in fact if k is allowed to vary with n then nonuniform weights are advantageous consider the space wofall weight vectors wk with wi ll wi is it totally ordered with respect to lwi wk or not? to answer this tion we must return to ap once again. the weight vector only influences the term i i given there. consider for example the weight vectors and numerical minorities are made up of one two or three components. for both weight vectors ni however in the former case and in the latter. thus the i term is uniformly smaller over all p in the latter case and we see that for all distributions the second weight vector is better. when the nts are not strictly nested such a universal comparison becomes impossible as in the example of problem hence w is only partially ordered. unwittingly we have also shown the following theorem theorem for all distributions l l l lnn nearest neighbor rules proof. it suffices once again to look at ap. consider the weight vector wi normalization as for the i-nn rule. the term i i is zero as no nl nk however the l-nn rule with vector wi has a nonzero term i i because no nk- yet nk ekl hence l l remark. we have strict inequality whenever rj. when l we have lnn lsnn as well. k-nearest neighbor rules even k until now we assumed throughout that k was odd so that voting ties were avoided. the tie-breaking procedure we follow forthe neighbor rule is as follows gnx i yclx if ycix k if yix k if ycix k. formally this is equivalent to a weighted neighbor rule with weight vector it is easy to check from theorem that this is the asymptotically best weight vector. even values do not decrease the probability of error. in particular we have the following theorem for all distributions and all integers k l proof. recall that lknn may be written in the form lknn where lim p yix x n---oo is the pointwise asymptotic error probability of the k-nn rule gk it is convenient to consider zl i.i.d. l-valuedrandom variables withpzi i p and to base the decision upon the sign of zi. from the general formula for weighted nearest neighbor rules the pointwise asymptotic error probability of the rule is n---oo n lim p yix x pptziopptziozio pp pp zi inequalities for the probability of error pp pp o lim p yjx x n----oo n therefore inequalities for the probability of error we return to the case when k is odd. recall that where rninp p lip rninp p since l e we may exploit this representation to obtain a variety of inequalities on lknn l we begin with one that is very easy to prove but perhaps not the strongest. theorem for all odd k and all distributions lknn l proof. by the above representation sup pp is binomial p sup kp p k sup p the okamoto-hoeffding inequality-theorem sup ue- o nearest neighbor rules theorem and gyorfi for all distributions and all odd k proof. we note that for p with b binomial p e kpl p jvarb p inequality inequality p hence lknn l e ryx inequality nn k remark. for large k b is approximately normal p and thus eib kpl as the first absolute moment of a normal random variable is problem working this through yields an approximate bound of j lnn k. the bound is proportional to this can be improved to l if instead of bounding it from above by markovs inequality we directly approximate p kp k p as shown below. theorem for all distributions and k odd where y supro r n is normal and refers to k constants are given in the proof. inequalities for the probability of error the constant y in the proof cannot be improved. a slightly weaker bound was obtained by devijver lknn lnn e l k nk l lnn k see lemma see also devijver and kittler lemma for p and with k odd p p k! dx a proof. consider k i.i.d. uniform random variables on the number of values in p is binomial p. the number exceeds if and only ifthe order statistic of the at most p. the latter is beta distributed explaining the first equality note that we have written a discrete sum as an integral-in some cases such tricks payoff handsome rewards. to show the inequality replace x by ar and use the inequality us e-u to obtain a bound as shown with finally a p kl kl is binomial k k kl k-l y l nearest neighbor rules proof of theorem from earlier remarks lknn l e e cninryx ryx minryx ryx k sup b p l is binomial p. we merely bound the factor in brackets. clearly by lemma lknn l l sup a i-jk-l p take a as the solution of y which is possible if k n setting v p we have i-jk-l sup p max sup a-jk-l sv v j ovsa-jk-l v j v e- sup max max y y for all u y collect all bounds and note that a behavior when l is small in this section we look more closely at lknn when l is small. recalling that lknn with minp minp p for odd k it is easily seen that lknn function because for some behavior when l is small mlllp p p we also have lknn e for some other function ljfk. out forms of l knn include l exjl- k-jl l ee as pa pa is a function of p p for integer a this may be further reduced to simplified forms such as lnn lsnn e the behavior of ak near zero is very informative. as p we have p asp pl p while for the bayes error l eaoo where a oo rninp p aj pas p o. assume that p at all x. then as p lnn aj and aj l moreover lnn l aj l l aj assume that l p then whereas l for all practical purposes the rule is virtually perfect. for this reason the rule is highly recommended. little is gained by considering the rule when p is small as lsnn l let ak be the smallest number such that akp ak minp p for all p tangents in figure then nearest neighbor rules this is precisely at the basis of the inequalities of theorems through where it was shown that ak i figure akp as a function ofp and thus the classes are separated. this does not imply that the support of x given for every fixed k the k-nearest neighbor rule is consistent. cover has a beautiful nearest neighbor rules when l from theorem we retain that if l then lknn for all k. in fact then example to illustrate this remarkable fact. l implies that e i for all x y is different from the support of x given y take for example a random rational number from generate i j independently and at random from the geometric distribution on and set x min j maxi j. every rational number on has positive probability. given y x is as above and given y x is uniform on let py i py o the support of x is identical in both cases. as if x is irrational if x is rational i we see that l and that the nearest neighbor rule is consistent. if someone shows us a number x drawn from the same distribution as the data then we may decide the rationality of x merely by looking at the rationality of the nearest neighbor of x. although we did not show this the same is true if we are given any x e lim px is rational ylx is not rational n---oo lim px is not rational ylx is rational n---oo problem the i-nearest neighbor rule admissibility of the nearest neighbor rule the consistency theorems of chapter show us that we should take k k n in the k-nn rule. the decreasing nature ofl knn corroborates this. yet there exist distributions for which for all n the i-nn rule is better than the k-nn rule for any k this observation due to cover and hart rests on the following class of examples. let so and sl be two spheres of radius centered at a and b where iia bll given y x is uniform on sl while given y x is uniform on so whereas py i py o we note that given n observations with the i-nn rule eln py yn i py yn o for the k-nn rule k being odd we have el n p y t p t p s n when k jo hence the k-nn rule is worse than the i-nn rule for every n when the distribution is given above. we refer to the exercises regarding some interesting admissibility questions for k-nn rules. the i-nearest neighbor rule in hellman proposed the i-nearest neighbor rule which is tical to the k-nearest neighbor rule but refuses to make a decision unless at least i k observations are from the same class. formally we set gnx if ll yix ski if ll yix i i otherwise decision. define the pseudoprobability of error by ln pgnx yidj that is the probability that we reach a decision and correctly classify x. clearly ln s pgnx yidn our standard probability of error. the latter inequality nearest neighbor rules is only superficially interesting as the probability of not reaching a decision is not taken into account in ln. we may extend theorem to show the following theorem for the i-nearest neighbor rule the pseudoprobability of error ln satisfies n-oo lim el n k pbinomialk iix def lkl the above result is distribution-free. note that the k-nearest neighbor rule for odd k corresponds to the limit lkl by itself is not interesting but it was shown by devijver that lkl holds information regarding the bayes error l. theorem for all distributions and with k odd also lknn l k l l knn this theorem which we refer to problem shows that l is tightly sandwiched between lknn the asymptotic probability of error of the k-nearest neighbor rule and the rule which requires that the difference of votes between the two classes among the k nearest neighbors be at least two. if ln is close to its limit and if we can estimate ln the chapters on error estimation then we may be able to use devijvers inequalities to obtain estimates of the bayes error l for additional results see loizou and maybank as a corollary of devijvers inequalities we note that l knn we have l lknn lkl l x pbinomialk ilx and therefore i x pbinomialk l ix problems and exercises e x g minryx ryxjl minryx ryxk-l llk lk-l kk l uk-l reaches its maximum on at u lj k k llk!.zk-l kj lkl by stirlings formula. with i we thus obtain lknn l k v vk improving on theorem various other inequalities may be derived in this ner as well. problems and exercises problem let ii ii be an arbitrary norm on n d and define the k-nearest neighbor rule in terms of the distance px z iix z ii. show that theorems and remain valid. hint only stones lemma needs adjusting. the role of cones cx jt used in the proof are now played by sets with the following property x and z belong to the same set if and only if problem does there exist a distribution for which supn i e ln for the nearest neighbor rule? problem show that and that lsnn problem show that if c is a compact subset of n d and c is the support set for the probability measure fl iixl xii sup xecnc with probability one where xl is the nearest neighbor of x among xi xn nearest neighbor rules problem let fl be the probability measure of x given y and let v be the probability measure of x given y assume that x is real-valued and that py o py i find a pair fl such that supportfl support v l conclude that l does not tell us a lot about the support sets of fl and v. problem consider the i-nearest neighbor rule for distributions y with p constant and y independent of x. this exercise explores the behavior of as p to. for fixed integer as p to show that use a convenient representation of to conclude that as p t p kl k k p op kl problem das gupta and lin proposed the following rule for data with x e r. assume x is nonatomic. first reorder xi x n x according to increasing values and denote the ordered set by x xci x xil xn.theyisare permuted so that yj is the label of xj. take votes among yil until for the first time there is agreement yijl at which time we decide that class that is gn yi j yi j this rule is invariant under monotone transformations of the x-axis. if l denotes the asymptotic expected probability of error show that for all atomic x l e i show that l is the same as for the rule in which x e rd and we consider etc. rules in turn stopping at the first rule for which there is no voting tie. assume for simplicity that x has a density a good distance-tie breaking rule this may be dropped. show that l lnn l nn and thus that l l show that l l nn hence the rule performs somewhere in between the i-nn and rules. problem let y be independent of x and p constant. consider a weighted i-nearest neighbor rule with weights are where k m for m we obtain the rule. let lk m be the asymptotic probability of error. using results from problem show that lk m p opk-ml k-m kml as p t conclude that within this class of rules for small p the goodness of a rule is measured by k m. let be small and set p o. show that if x is binomial and z is binomial then for fixed i as problems and exercises pix l pz l l and pix i pz l i conclude that for fixed k m as lk m k m kpz k m i p k m p k m i take weight vector w with k fixed and m l j and compare it with weight vector w with components and m as p and p t assume that k is very large but fixed. in particular show that w is better as p and w is better as p t for the last example note that for fixed c as k........oo by the central limit theorem. conclude that there exist different weight vectors w wi for which there exists a pair of distributions of y such that their asymptotic error probabilities are differently ordered. thus w is not totally ordered with respect to the probability of error. problem patrick and fisher find the k-th nearest neighbor in each of the two classes and classify according to which is nearest. show that their rule is equivalent to a i-nearest neighbor rule. problem rabiner et al generalize the rule of problem so as to classify according to the average distance to the k-th nearest neighbor within each class. assume that x has a density. for fixed k find the asymptotic probability of error. problem if n is normal then eini prove this. problem show that if then llllnn problem show that l l for all distributions hint find the smallest constanta such that l a using the representation of in terms of the binomial tail problem show that if x has a density j then for all u lim p xii ux e-fxvu n--oo d with probability one where v is dx is the volume ofthe unit ball in n d nearest neighbor rules problem consider a rule that takes a majority vote over all yi for which iixi v n j d where v is dx is the volume of the unit ball and c is fixed. in case of a tie decide gnx o. if x has a density j show thatliminfn--ooeln e hint use the obvious inequality el n py fj--nsxcjvn oj. if y is independent of x and p then e l e too p as p t show this. conclude that sup lim infn--oo el n l and thus that distribution-free bounds of the form limn-- oo e ln c l obtained for k-nearest neighbor estimates do not exist for these simple rules problem take an example with and show that the bound lknn l ijke cannot be essentially bettered for large values of k that is there exists a sequence of distributions by k for which as k where n is a normal random variable. problem if b is binomial p then sup pb i i n v p i n. problem show that for k k k problem show that there exists a sequence of distributions of y by k in which y is independent of x and p p depending on k only such that lim inf n--oo lknn l l v k y where y is the constant of theorem hint verify the proof of orem but bound things from below. sluds inequality lemma in the appendix may be of use here. problem consider a weighted nearest neighbor rule with weights p for p show that the expected probability of error tends for all distributions to a limit lp. hint truncate at k fixed but large and argue that the tail has asymptotically negligible weight. problems and exercises problem continued. with lp as in the previous exercise show that lp lnn whenever p problem continued. prove or disprove as p increases from to lp decreases monotonically from lnn to l question is difficult. problem show that in the weighted nn rule with weights p p the asymptotic probability of error is lnn if p and is if p problem is there any k other than one for which the k-nn rule is admissible that is for which there exists a distribution of y such that el n for the k-nn rule is smaller than el n for any k-nn rule with k k for all n? hint this is difficult. note that if this is to hold for all n then it must hold for the limits. from this deduce that with probability one e i for any such distribution. problem for every fixed n and odd k with n loook find a distribution of y such that eln for the k-nn rule is smaller than eln for any k-nn rule with k k k odd. thus for a given n no k can be a priori discarded from consideration. problem let x be uniform on x and pry o pry i show that for the nearest neighbor rule eln and hart peterson problem for the nearest neighbor rule if x has a density then ieln elndl nl problem let x have a density f c on and assume that f and f exist and are uniformly bounded. show that for the nearest neighbor rule e ln lnn for d-dimensional problems this result was generalized by psaltis snapp and venkatesh problem show that lknn is the best possible bound of the form lknn a valid simultaneously for all k problem show that lknn for all k problem let x e consider the nearest neighbor rule based upon vectors with components x x show that this is asymptotically not better than if we had used show by example that x x may yield a worse asymptotic error probability than problem uniform order statistics. let un be order statistics of n i.i.d. uniform random variables. show the following uk is beta n k that is uk has density f x n! x k- xn-k x k! nearest neighbor rules a e rk arn rkrn a for any a l!a a eu n k for a where l!a is a function of a only problem dudanis rule. dudani proposes a weighted k-nn rule where yix receives weight iixkx xii iixix xii i k. why is this roughly speaking equivalent to attaching weight i kld to the i-th nearest neighbor if x has a density? hint if is the probability measure of x then are distributed like uk where ul un are the order statistics of n i.i.d. uniform random variables. replace by a good local approximation and use results from the previous exercise. problem show devijvers theorem in two parts first establish the inequality l for the tennis rule and then establish the monotonicity. problem show theorem for the i nearest neighbor rule. problem let r be the asymptotic error probability of the neighbor rule. prove that r l nn problem for the nearest neighbor rule show that for all distributions lim pgnx y i n-oo lim pgnx y o n-oo and kittler thus errors of both kinds are equally likely. problem let py i py o and let x be a random rational if y defined in section such that every rational number has positive probability and let x be uniform if y show that for every x e not rational pylx i as n while for every x e rational pylx o as n problem let xl xn be i.i.d. and have a common density. show that for fixed k np is among the k nearest neighbors of xl and in xn show that the same result remains valid whenever k varies with n such that kl in problems and exercises problem imperfect training. let y z yi zd quence of i.i.d. triples in rd x i x i with pry llx x and pz llx x let zlx be z j if x j is the nearest neighbor of x among x i x n show that lim pzlx y e n-oo problem improve the bound in lemma to yd then yd problem show that if cxyd is a collection of cones ing r d problem recalling that lknn e where akp minp p pip minp p show that for every fixed p p minp p as k is the mono tonicity that is harder to show. how would you then prove that limk---oo lknn l problem show that the asymptotic error probability of the rule that decides gnx is identical to that of the rule in which gnx problem show that for all distributions lsnn e where vrsu u problem show that for all distributions and that problem let xl be the nearest neighbor of x among xl x n construct an example for which e ii xl x ii for all x e rd. we have to steer clear of convergence in the mean in lemma let x be the nearest neighbor of x i among x x n construct a distribution such that eiixl xiii for all n. problem consider the weighted nearest neighbor rule with weights wk. define a new weight vector wk-l vi vz where vi wk. thus the weight vectors are partially ordered by the operation assume that all weights are nonnegative. let the asymptotic expected probability of errors be land l respectively. true or false for all distributions of y l l. problem gabriel neighbors. given xl. xn e r d we say that xi and xj are gabriel neighbors if the ball centered at x j of radius iixi xj contains no x k k i j and sokal matula and sokal clearly if xj is the nearest neighbor of xi then xi and xj are gabriel neighbors. show that if x has a density and x i xn are i.i.d. and drawn from the distribution of x then the expected number of gabriel neighbors of xl tends to as n nearest neighbor rules figure the gabriel graph of points on the plane is shown gabriel neighbors are connected by an edge. note that all circles dling these edges have no data point in their interior. problem gabriel neighbor rule. define the gabriel neighbor rule simply as the rule that takes a majority vote over all yi for the gabriel neighbors of x among xl x n ties are broken by flipping a coin. let ln be the conditional probability of error for the gabriel rule. using the result of the previous exercise show that if l is the bayes error then for determine the best possible value of c. hint use theorem and try obtaining for d a lower bound for p x where n x is the number of gabriel neighbors of x among xl xn lim el n if l n---oo limsupel n lnn if l d n--oo lim sup eln cl for some c if d n--oo consistency universal consistency if we are given a sequence dn yi yn of training data the best we can expect from a classification function is to achieve the bayes error probability l generally we cannot hope to obtain a function that exactly achieves the bayes error probability but it is possible to construct a sequence of classification functions that is a classification rule such that the error probability gets arbitrarily close to l with large probability is for dn. this idea is formulated in the definitions of consistency definition and strong consistency. a classification rule is sistent asymptotically bayes-risk efficient for a certain distribution of y if and strongly consistent if lim ln l with probability n-oo remark. consistency is defined as the convergence of the expected value of ln to l since ln is a random variable bounded between l and this convergence consistency every e is equivalent to the convergence of ln to l in probability which means that for lim p l e n---oo obviously since almost sure convergence always implies convergence in bility strong consistency implies consistency. d a consistent rule guarantees that by increasing the amount of data the probability that the error probability is within a very small distance of the optimal achievable gets arbitrarily close to one. intuitively the rule can eventually learn the optimal decision from a large amount of training data with high probability. strong sistency means that by using more data the error probability gets arbitrarily close to the optimum for every training sequence except for a set of sequences that has zero probability altogether. a decision rule can be consistent for a certain class of distributions of y but may not be consistent for others. it is clearly desirable to have a rule that is consistent for a large class of distributions. since in many situations we do not have any prior information about the distribution it is essential to have a rule that gives good performance for all distributions. this very strong requirement of universal goodness is formulated as follows definition consistency. a sequence of decision rules is called universally consistent if it is consistent for any distribution of the pair y. in this chapter we show that such universally consistent classification rules exist. at first this may seem very surprising for some distributions are very and seem hard to learn. for example let x be uniformly distributed on with probability and let x be atomic on the rationals with probability for example if the rationals are enumerated rl then px rd let y if x is rational and y if x is irrational. obviously l if a classification rule gn is consistent then the probability of incorrectly guessing the rationality of x tends to zero. note here that we cannot whether x is rational or not but we should base our decision solely on the data dn given to us. one consistent rule is the following gnx dn yk if x k is the closest point to x among xl x n the fact that the rationals are dense in makes the statement even more surprising. see problem classification and regression estimation in this section we show how consistency of classification rules can be deduced from consistent regression estimation. in many cases the a posteriori probability rjx is estimated from the training data dn by some function rjnx rjnx dn. classification and regression estimation in this case the error probability lgn pgnx yidn of the plug-in rule if rjnx gnx otherwise is a random variable. then a simple corollary of theorem is as follows corollary the error probability of the classifier gn defined above fies the inequality the next corollary follows from the cauchy-schwarz inequality. corollary if if rjnx h ot erwise gnx then its error probability satisfies clearly rjx py iix x eyix x is just the regression function of y on x. therefore the most interesting consequence of theorem is that the mere existence of a regression function estimate rjn for which in probability or with probability one implies that the plug-in decision rule gn is consistent or strongly consistent respectively. clearly from theorem one can arrive at a conclusion analogous to corollary when the probabilities rjox py x and rjlx py iix x are estimated from data separately by some rjon and rjln respectively. usually a key part of proving consistency of classification rules is writing the rules in one of the plug-in forms and showing of the approximating functions to the a posteriori probabilities. here we have some freedom as for any positive function in we have for example g n if rjlnx rjonx otherwise if llnx ronx tnx tnx otherwise. consistency partitioning rules many important classification rules partition nd into disjoint cells ai a and classify in each cell according to the majority vote among the labels of the xis falling in the same cell. more precisely gn otherwise if iylixeax iyoixeax where ax denotes the cell containing x. the decision is zero if the number of ones does not exceed the number of zeros in the cell where x falls and vice versa. the partitions we consider in this section may change with n and they may also depend on the points xi x n but we assume that the labels do not playa role in constructing the partition. the next theorem is a general consistency result for such partitioning rules. it requires two properties of the partition first cells should be small enough so that local changes of the distribution can be detected. on the other hand cells should be large enough to contain a large number of points so that averaging among the labels is effective. diama denotes the diameter of a set a that is diama sup iix yii. xyea let nx nlnax l ixeax n denote the number of xis falling in the same cell as x. the conditions of the theorem below require that a random cell-selected according to the distribution of x a small diameter and contains many points with large probability. theorem consider a partitioning classification rule as defined above. then el n l if diamax in probability n in probability. proof. define py llx x. from corollary we recall that we need only show where lnx exeax yi introduce ijx e ax. by the triangle inequality elrynx elrynx elijx by conditioning on the random variable nx it is easy to see that nxrynx is distributed as bnx ijx a binomial random variable with parameters nx and ijx. thus the histogram rule ijxiix ixl eax ixileax i e i bnx ijx nx inxo i inx o x ix ixileax ijxl ijx nx i inx o x ixeax ixileax e inxo by the cauchy-schwarz inequality. taking expectations we see that e oi pnx o nx ipnx k pnx o for any k and this can be made small first by choosing k large enough and then by using condition for e find a uniformly continuous i-valued function on a bounded set c and vanishing off c so that e e. next we employ the triangle inequality e e e i i i i i i where ijex e ax. clearly i i i e by choice of since is uniformly continuous we can find a such that i i e pdiamax therefore i i for n large enough by condition finally i taken together these steps prove the theorem. i i i e. the histogram rule in this section we describe the cubic histogram rule and show its universal sistency by checking the conditions of theorem the rule partitions rd into consistency cubes of the same size and makes a decision according to the majority vote among the ys such that the corresponding xi falls in the same cube as x. formally let p n a beapartitionofrd into cubes ofsizeh n that is into sets of the type n where the ks are integers. for every x e rd let an ani if x e ani. the histogram rule is defined by gn x if iyilixieanx iyioixieanx otherwise. figure a cubic histogram rule the decision is in the shaded area. oe e o consistency of the histogram rule was established under some additional ditions by glick universal consistency follows from the results of gordon and olshen a direct proof of strong universal consistency is given in chapter the next theorem establishes universal consistency of certain cubic histogram rules. theorem if hn rule is universally consistent. and nh as n then the cubic histogram proof. we check the two simple conditions of theorem clearly the diameter of each cell is therefore condition follows trivially. to show condition we need to prove that for any m pnx m o. let s be an arbitrary ball centered at the origin. then the number of cells intersecting s is not more than ci hd for some positive constants ci then pnx m l px e a nj nx m px esc ja nj ja nj fj.a nj ja nj fj.a nj stones theorem jalsi fla nj chebyshevs inequality ja nj nsi fla nj cl hd c because nh d since s is arbitrary the proof of the theorem is complete. stones theorem a general theorem by stone allows us to deduce universal consistency of several classification rules. consider a rule based on an estimate of the a posteriori probability of the form n l iyil wnix lyi wnix n where the weights wnix wnix xl xn are nonnegative and sum to one il il il the classification rule is defined as gn x if iyil wnix iyio wnix otherwise if yi wnix otherwise. consistency is a weighted average estimator of it is intuitively clear that pairs yi such that xi is close to x should provide more information about than those far from x. thus the weights are typically much larger in the neighborhood of x so is roughly a relative frequency of the xis that have label among points in the neighborhood of x. thus might be viewed as a local average estimator and gn a local majority vote. examples of such rules include the histogram kernel and nearest neighbor rules. these rules will be studied in depth later. theorem satisfy the following three conditions assume that for any distribution of x the weights there is a constant c such thatfor every nonnegative measurable function f satisfying efx e wnixfx s cefx. for all a lim e i mx wnix n---oo then gn is universally consistent. remark. condition requires that the overall weight of xs outside of any ball of a fixed radius centered at x must go to zero. in other words only points in a shrinking neighborhood of x should be taken into account in the averaging. condition requires that no single xi has too large a contribution to the estimate. hence the number of points encountered in the averaging must tend to infinity. condition is technical. proof. by corollary it suffices to show that for every distribution of y introduce the notation lim e n---oo n rnx l il stones theorem then by the simple inequality we have e rynx e ilnx rynx tfnx e rynx therefore it is enough to show that both terms on the right-hand side tend to zero. since the wnis are nonnegative and sum to one by jensens inequality the first term is e wnixx r e t wnixx if the function ry is continuous with bounded support then it is uniformly continuous as well for every e there is an a such that for ilxi x ii a iryxd e. recall here that ilxli denotes the norm ofa vector x e rd. thus since iryxd ryx e t wnixx ryxi e wnxjiix-xlia e wnixe e by since the set of continuous functions with bounded support is dense in for every e we can choose ry such that e ryx e. by this choice using the inequality b c? follows from the cauchy-schwarz inequality e tfnx e t wnixryx wnix ryxi ryx wnixx ryxi consistency where we used therefore lim sup e c. n-oo to handle the second term of the right side of observe that by independence. therefore e e wnxryxi yir n l le yi j yj n il jl n il le e w.ex e wnix wnjx e wnx by and the theorem is proved. the k-nearest neighbor rule in chapter we discussed asymptotic properties of the k-nearest neighbor rule when k remains fixed as the sample size n grows. in such cases the expected probability of error converges to a number between l and in this section we show that if k is allowed to grow with n such that kin the rule is weakly universally consistent. the proof is a very simple application of stones theorem. this result appearing in stones paper was the first universal consistency result for any rule. strong consistency and many other different aspects of the k-nn rule are studied in chapters and recall the definition of the k-nearest neighbor rule first the data are ordered according to increasing euclidean distances of the x j s to x classification is easier than regression function estimation that is xix is the i-th nearest neighbor of x among the points xl x n distance ties are broken by comparing indices that is in case of ii xi x ii j x ii xi is considered to be to x if i j. the k-nn classification rule is defined as gn if ll. iyixl ll iyixo otherwise. in other words gn is a majority vote among the labels of the k nearest neighbors of x. theorem ifk and kn thenforall distributions eln- l. proof. we proceed by checking the conditions of stones weak convergence rem the weight wnix in theorem equals k iff xi is among the k nearest neighbors of x and equals otherwise. condition is obvious since k for condition observe that holds whenever p xii e where xkx denotes the k-th nearest neighbor of x among xl x n but we know from lemma that this is true for all e whenever kn finally we consider condition we have to show that for any nonnegative measurable function f with efx e is among the k nearest neighbors of xfxd e for some constant c. but we have shown in lemma that this inequality always holds with c yd. thus condition is verified. classification is easier than regression function estimation once again assume that our decision is based on some estimate of the a posteriori probability function that is otherwise. gn consistency the bounds of theorems and corollary point out that if is a consistent estimate of then the resulting rule is also consistent. for example writing ln pgnx i yidn we have that is l estimation of the regression function leads to consistent classification and in fact this is the main tool used in the proof of theorem while the said bounds are useful for proving consistency they are almost useless when it comes to studying rates of convergence. as theorem below shows for consistent rules rates of convergence of pgn i y to l are always orders of magnitude better than rates of convergence of je to zero. o x figure the difference between the error probabilities grows roughly in proportion to the shaded area. elsewhere does not need to be close pattern recognition is thus easier than regression function estimation. this will be a recurring theme-to achieve acceptable results in pattern recognition we can do more with smaller sample sizes than in regression function estimation. this is really just a consequence of the fact that less is required in pattern recognition. it also corroborates our belief that pattern recognition is dramatically different from regression function estimation and that it deserves separate treatment in the statistical community. theorem let be a weakly consistent regression estimate that is define lim e n-oo o. gnx otherwise. classification is easier than regression function estimation then el lim n-oo n flx? that is eln l converges to zero faster than the l of the regression estimate. proof. we start with the equality of theorem eln l gx fix e o. we may bound the last factor by e i gxd e flnxiigx gx e e ignx gx flnxiigx flx? x jplflx e flex jpgnx gx iflx e the cauchy-schwarz inequality. since gnx gx and iflx e imply that irjnx consistency of the regression estimate implies that for any fixed e e lim pgnx gx iflx e o. n-oo on the other hand plflx e flex as e which completes the proof. the actual value of the ratio eln l pn flx? cannot be universally bounded. in fact pn may tend to zero arbitrarily slowly problem on the other hand pn may tend to zero extremely quickly. in problems and and in the theorem below upper bounds on pn are given that consistency may be used in deducing rate-of-convergence results. theorem in particular states that eln l tends to zero as fast as the square of the error of the regression estimate i.e. e whenever l o. just how slowly pn tends to zero depends upon two things basically the rate of convergence of to and the behavior of e as a function of e when e the behavior of at those xs where theorem assume that l and consider the decision gn otherwise. then pgnx y e proof. by theorem pgnxj y gx y by the assumption l hpgnx y the cauchy-schwarz inequality. dividing both sides by y yields the result. the results above show that the bounds of theorems and corollary may be arbitrarily loose and the error probability converges to l faster than the l of the regression estimate converges to zero. in some cases consistency may even occur without convergence of to zero. consider for example a strictly separable distribution that is a distribution such that there exist two sets a bend with inf xeayeb y ii for some and having the property that px e aiy i e biy in such cases there is a version of that has on a and on b. we say version because is not defined on sets of measure zero. for such strictly separable distributions l o. let ij be e on band e on a. then with gx if ijx otherwise if x e b if x e a classification is easier than regression function estimation we have pgx y l o. yet is arbitrarily close to one. in a more realistic example we consider the kernel rule chapter gn ifrynx otherwise in which where k is the standard normal density in nd ku i_ e- iiu assume that a and b consist of one point each at distance from each other-that is the distribution of x is concentrated on two points. if py o py i we see that lim l kx xi n n ko ko with probability one at x e a u b by the law of large numbers. also if x e a if x e b with probability one. thus lim rynx if x e a if x e b hence as ryx ion a and ryx on b n--oo ko with probability one. lim n--oo ko yet l and pgnx y o. in fact if dn denotes the training data lim pgnx yidn l with probability one n--oo consistency and iljnx dn ko with probability one. i this shows very strongly that for any for many practical classification rules we do not need convergence of ljn to lj at all! as all the consistency proofs in chapters through rely on the convergence of ljn to lj we will create unnecessary conditions for some distributions although it will always be possible to find distributions of y for which the conditions are needed-in the latter sense the conditions of these universal consistency results are not improvable. smart rules a rule is a sequence of mappings gn rd x x to lf to i. most rules are expected to perform better when n increases. so we say that a rule is smart if for all distributions of y elgn is nonincreasing where some dumb rules are smart such as the rule that for each n takes a majority over all yi ignoring the xis. this follows from the fact that p t y or y is monotone in n. this is a property of the binomial distribution problem a histogram rule with a fixed partition is smart the neighbor rule is not smart. to see this let y be and with probabilities p and p respectively where z is uniform on verify that for n eln p while for n eizi p p which is larger than p whenever p e this shows that in all these cases it is better to have n than n similarly the standard kernel rule-discussed in chapter fixed h is not smart problems the error probabilities of the above examples of smart rules do not change dramatically with n. however change is necessary to guarantee bayes risk tency. at the places of change-for example when hn jumps to a new value in the histogram rule-the monotonicity may be lost. this leads to the conjecture that no universally consistent rule can be smart. problems and exercises problems and exercises problem let the i.i.d. random variables xl xn be distributed on rd according to the density f. estimate f by fn a function of x and xl x n and assume that f i fn f idx in probability with probability one. then show that there exists a consistent strongly consistent classification rule whenever the conditional densities fa and fl exist. problem histogram density estimation. let xl xn be i.i.d. random variables in rd with density f. let p n be a partition of rd into cubes of size hn and define the histogram density estimate by where an is the set in p n that contains x. prove that the estimate is universally consistent in ll if hn and nh as n that is for any f the ll error of the estimate f ifnx fxldx converges to zero in probability or equivalently e ifnx fxldx o. hint the following suggestions may be helpful fl e ifn efnl f iefn fl. e ifn e itl efn i lj first show f iefn arbitrary densities. f-inanjl. fl for uniformly continuous f and then extend it to problem let x be uniformly distributed on with probability and let x be atomic on the rationals with probability if the rationals are enumerated rl then px rd let y if x is rational and y if x is irrational. give a direct proof of consistency of the i-nearest neighbor rule. hint given y the conditional distribution of x is discrete. thus for every e there is an integer k such that given y x equals one of k rationals with probability at least i-e. now if n is large enough every point in this set captures data points with label i with large probability. also for large n the space between these points is filled with data points labeled with zeros. problem prove the consistency of the cubic histogram rule by checking the conditions of stones theorem. hint to check first bound wnix by since n jl ixieanx l ixjeanx lin. e efx it suffices to show that there is a constant c such that for any nonnegative function f with efx consistency in doing so you may need to use lemma to prove that condition holds write and use lemma problem let be a sequence of positive numbers converging to zero. give an ple of an a posteriori probability function and a sequence of functions approximating such that pgnx y l f in anje where gnx if otherwise. thus the rate of convergence in theorem may be arbitrarily slow. hint define hex where hex is a very slowly increasing nonnegative function. problem let and assume that for all x. consider the decision if otherwise. prove that pgnx y l this shows that the rate of convergence implied by the inequality of theorem may be preserved for very general classes of distributions. problem assume that l and consider the decision show that for all p if otherwise. hint proceed as in the proof of theorem but use holders inequality. problem theorem cannot be generalized to the ll error. in particular show by example that it is not always true that el lim n---oo e n when e as for some regression function estimate thus the inequality eln l cannot be universally improved. problems and exercises problem let rd and define gx irlx assume that the random that as n prove that lgn lg for all distributions of variable x satisfies that let be a sequence of functions such y satisfying the condition on x abovewhere gnx illnx problem a lying teacher. sometimes the training labels yj yn are not able but can only be observed through a noisy binary channel. still we want to decide on y. consider the following model. assume that the yi s in the training data are replaced by the i.i.d. binary-valued random variables zi whose distribution is given by pzi llyi o p pzi i q pzi llyi xi x pzi oiyi xi x consider the decision gx where pzi ixi x. show that pgx y l if otherwise q i q use problem to conclude that if the binary channel is symmetric p q and p then l i-consistent estimation leads to a consistent rule in spite of the fact that the labels yi were not available in the training sequence problem develop a discrimination rule which has the property lim eln p e n-oo for all distributions such that x has a density. note clearly since p l this rule is not universally consistent but it will aid you in the matushita error! problem if zn is binomial p and z is bernoulli independent of zn then show that pzn z o pzn z is nonincreasing in n. problem let gn be the histogram rule based on a fixed partition p. show that gn is smart. problem show that the kernel rule with gaussian kernel and h d is not smart rules are discussed in chapter hint consider n and n only. problem show that the kernel rule on r with kx i-lljx and h t such that nh is not smart. problem conjecture no universally consistent rule is smart. rd x x l r is called symmetric if gn dn problem a rule gn gn d for every x and every training sequence dn where d is an arbitrary permutation of the pairs yi in dn. any nonsymmetric rule gn may be symmetrized by taking a jority vote at every x e rd over all gnx d obtained by then! permutations of dn. it may intuitively be expected that symmetrized rules perform better. prove that this is false that is exhibit a distribution and a nonsymmetric classifier gn such that its expected probability of error is smaller than that of the symmetrized version of gn hint take slow rates of convergence in this chapter we consider the general pattern recognition problem given the observation x and the training data dn yi yn of dent identically distributed random variable pairs we estimate the label y by the decision the error probability is obviously the average error probability eln py gnx is completely determined by the distribution of the pair y and the classifier gn we have seen in chapter that there exist classification rules such as the cubic histogram rule with properly chosen cube sizes such that limn--- oo eln l for all possible distributions. the next question is whether there are classification rules with eln tending to the bayes risk at a specified rate for all distributions. disappointingly such rules do not exist. finite training sequence the first negative result shows that for any classification rule and for any fixed n there exists a distribution such that the difference between the error probability of the rule and l is larger than to explain this note that for fixed n we can find a sufficiently complex distribution for which the sample size n is hopelessly small. slow rates of convergence theorem let e be an arbitrarily small number. with bayes risk l such that for any integer n and classification rule gn there exists a distribution of y eln e. proof. first we construct a family of distributions of y. then we show that the error probability of any classifier is large for at least one member of the family. for every member of the family x is uniformly distributed on the set k of positive integers px i k pi otherwise if i e k where k is a large integer specified later. now the family of distributions of y is parameterized by a number b e that is every b determines a distribution as follows. let b e have binary expansion b and define y bx as the label y is a function of x there exists a perfect decision and thus l we show that for any decision rule gn there is a b such that if y bx then gn has very poor performance. denote the average error probability corresponding to the distribution determined by b by rnb eln. the proof of the existence of a bad distribution is based on the so-called abilistic method. here the key trick is the randomization of b. define a random variable b which is uniformly distributed in and independent of x and xl x n then we may compute the expected value of the random variable rnb. since for any decision rule gn sup rnb ernb beoi a lower bound for ernb proves the existence of abe whose sponding error probability exceeds the lower bound. since b is uniformly distributed in its binary extension b is a sequence of independent binary random variables with pbi o pbi i but ernb p dn bx p xl bxl xn bxj bx e xl bx x n bxj bx x xl xn xl x x x xn since if x xi for all i n then given x xl x n y bx is conditionally independent of gnx dn and y takes values and with probability slow rates but clearly px i xl x i x x i xnl x px i xiixn ikt. in summary sup rnb kt. heol the lower bound tends to as k d theorem states that even though we have rules that are universally consistent that is they asymptotically provide the optimal performance for any distribution their finite sample performance is always extremely bad for some distributions. this means that no classifier guarantees that with a sample size of n we get within of the bayes error probability for all distributions. however as the bad distribution depends upon n theorem does not allow us to conclude that there is one distribution for which the error probability is more than l for all n. indeed that would contradict the very existence of universally consistent rules. slow rates the next question is whether a certain universal rate of convergence to l is achievable for some classifier. for example theorem does not exclude the existence of a classifier such that for every n eln l c n for all distributions for some constant c depending upon the actual distribution. the next negative result is that this cannot be the case. theorem below states that the error probability eln of any classifier is larger than l clog log log n for every n for some distribution even if c depends on the distribution. can be seen by considering that by theorem there exists a distribution of y such that eln l log log n for every n. moreover there is no sequence of numbers an converging to zero such that there is a classification rule with error probability below l plus an for all distributions. thus in practice no classifier assures us that its error probability is close to l unless the actual distribution is known to be a member of a restricted class of distributions. now it is easily seen that in the proof of both theorems we could take x to have uniform distribution on or any other density problem therefore putting restrictions on the distribution of x alone does not suffice to obtain rate-of-convergence results. for such results one needs conditions on the a posteriori probability as well. however if only training data give information about the joint distribution then theorems with extra conditions on the distribution have little practical value as it is impossible to detect whether for example the a posteriori probability is twice differentiable or not. now the situation may look hopeless but this is not so. simply put the bayes error is too difficult a target to shoot at. slow rates of convergence weaker versions of theorem appeared earlier in the literature. first cover showed that for any sequence of classification rules for sequences converging to zero at arbitrarily slow algebraic rates as for arbitrarily small there exists a distribution such that eln l an infinitely often. devroye strengthened covers result allowing sequences tending to zero arbitrarily slowly. the next result asserts that eln l an for every n. theorem let be a sequence of positive numbers converging to zero with a i for every sequence of classification rules there exists a distribution of y with l such that for all n. this result shows that universally good classification rules do not exist. rate of convergence studies for particular rules must necessarily be accompanied by conditions on y. that these conditions too are necessarily restrictive follows from examples suggested in problem under certain regularity conditions it is possible to obtain upper bounds for the rates of convergence for the probability of error of certain rules to l then it is natural to ask what the fastest achievable rate is for the given class of distributions. a theory for regression function estimation was worked out by stone related results for classification were obtained by marron in the proof of theorem we will need the following simple lemma lemma for any monotone decreasing sequence of positive numbers converging to zero with al a probability distribution may be found such that pi and for all n l pi max proof. it suffices to look for pis such that l pi max these conditions are easily satisfied. for positive integers u v define the tion h v u ll ii. first we find a sequence n of integers with the following properties hnkl nk is monotonically increasing nr for all k note that may only be satisfied if anl al to this end define constants cl by slow rates so that the ck s are decreasing in k and for n e nkl we define pn we claim that these numbers have the required properties. indeed is decreasing and finally if n e nkl then p n i inl j j c. jkl jkl clearly on the one hand by the monotonicity of hnkl nk ck on the other hand this concludes the proof. proof of theorem we introduce some notation. let b o.b l be a real number on with the shown binary expansion and let b be a random variable uniformly distributed on with expansion b o.bi let us restrict ourselves to a random variable x with px i pi i where pi and lnl pi max for every n. that such pis exist follows from lemma set y bx as y is a function of x we see that l each b e however describes a different distribution. with b replaced by b we have a random distribution. introduce the short notation bxj bxj and define g ni gni probabilityoferrorpgnx yib xl xn for the random distribution then we note that we may write lnb l pjgni bd il slow rates of convergence if lnb is the probability of error for a distribution parametrized by b then lnb b n we consider only the conditional expectation for now. we have e lnb i xl x n p xl lplnb xl x lp xl x xn l e ln i i x i x x n we bound the conditional probabilities inside the sum p p l i pjgltijb i noting that g ni xl xn are all functions of we have piibil i p l p pi ibil p pibi the pis are decreasing by stochastic dominance now everything boils down to bounding these probabilities from above. we ceed by chernoffs bounding technique. the idea is the following for any random variable x and s by markovs inequality slow rates by cleverly choosing s one can often obtain very sharp bounds. for more discussion and examples of chernoffs method refer to chapter in our case e-x x x for x x e- x sb exp b lnl pj exp bpnl taking s l and the fact that b exp b pnll... pnl b thus we conclude that supinfe-- lncb b n nl so that there exists a b for which elnb an for all n. slow rates of convergence problems and exercises problem extend theorem for distributions with l show that if an is a sequence of positive numbers as in theorem then for any classification rule there is a distribution such that eln l an for every n for which l an problem prove theorems and under one of the following additional tions which make the case that one will need very restrictive conditions indeed to study rates of convergence. x has a uniform density on x has a uniform density on and is infinitely many times continuously is unimodal in x e n that is decreases as a increases for any differentiable on x e is l-valued x is n and the set i is a compact convex set containing the origin. problem there is no super-classifier. show that for every sequence of tion rules there is a universally consistent sequence of rules such that for some distribution of y pgnx y pgx y for all n. problem the next two exercises are intended to demonstrate that the weaponry of pattern recognition can often be successfully used for attacking other statistical problems. for example a consequence of theorem is that estimating infinite discrete distributions is hard. consider the problem of estimating a distribution on the positive integers from a sample xl xn of i.i.d. random variables with i pi i show that for any decreasing sequence of positive numbers converging to zero with al and any estimate there exists a distribution such that e ipi pinl an hint consider a classification problem with l pry o and x concentrated on assume that the class-conditional probabilities pia px ily o and pil px ily i are estimated from two i.i.d. samples xiol xo and xill xl distributed according to and respectively. use theorem to show that for the classification rule obtained from these estimates in a natural way therefore the lower bound of theorem can be applied. problem a similar slow-rate result appears in density estimation. consider the lem of estimating a density on n from an i.i.d. sample x i xn having density show that for any decreasing sequence of positive numbers converging to zero with a i and any density estimate in there exists a distribution such that problems and exercises e ifx fnxldx an this result was proved by birge using a different-and in our view much more d comp lcate hint put pi fnxdx and apply problem and pin ji ril f r error estimation error counting estimating the error probability ln pgn y i dn of a classification function gn is of essential importance. the designer always wants to know what performance can be expected from a classifier. as the designer does not know the distribution of the data-otherwise there would not be any need to design a classifier-it is important to find error estimation methods that work well without any condition on the distribution of y. this motivates us to search for distribution-free performance bounds for error estimation methods. suppose that we want to estimate the error probability of a classifier gn designed from the training sequence dn yl yn assume first that a testing sequence tm ynl ynm is available which is a sequence of i.i.d. pairs that are independent of y and dn and that are distributed as y. an obvious way to estimate ln is to count the number of errors that gn commits on tm. the error-counting estimator lnm is defined by the relative frequency the estimator is clearly unbiased in the sense that error estimation and the conditional distribution of mlnm given the training data dn is binomial with parameters m and ln. this makes analysis easy for properties of the binomial distribution are well known. one main tool in the analysis is hoeffdings inequality which we will use many many times throughout this book. hoeffdings inequality the following inequality was proved for binomial random variables by chernoff and okamoto the general format is due to hoeffding theorem let xl xn be independent bounded random variables such that xi falls in the interval bd with probability one. denote their sum by sn xi. thenfor any e we have psn esn e and the proof uses a simple auxiliary inequality lemma let x be a random variable with ex a x b. then for s proof. note that by convexity of the exponential function esx esb esa x-a b-a b-x b-a for a x b. exploiting ex and introducing the notation p a we get b a esa esb b-a p pesb-a e-psb-a b-a def e where u sb a and pu p pe u calculation it is easy to see that the derivative of is but by straightforward p p p p e u therefore moreover p pe-u p e thus by taylor series expansion with remainder for some e e u hoeffdings inequality proof of theorem the proof is based on chernoffs bounding method by markovs inequality for any nonnegative random variable x and any e ex pxes-. e therefore if s is an arbitrary positive number then for any random variable x in chernoffs method we find an s that minimizes the upper bound or makes the upper bound small. in our case we have e-se n e e-se n il n n independence lemma il the second inequality is proved analogouslo. the two inequalities in theorem may be combined to get now we can apply this inequality to get a distribution-free performance bound for the counting error estimate corollary for every e error estimation the variance of the estimate can easily be computed using the fact that tioned on the data dn mlnm is binomially distributed these are just the types of inequalities we want for these are valid for any bution and data size and the bounds do not even depend on gn consider a special case in which all the xs take values on c and have zero mean. then hoeffdings inequality states that p n e s e-nr this bound while useful for e larger than c ignores variance information. when varxi it is indeed possible to outperform hoeffdings inequality. in particular we have theorem and bernstein let xl xn be independent real-valued random variables with zero mean and assume that xi c with probability one. let lvarxd. n n il then for any e and p t xi e exp n il the proofs are left as exercises we note that bernsteins inequality kicks in when e is larger than about max cl it is typically better than hoeffdings inequality when c. error estimation without testing data a serious problem concerning the practical applicability of the estimate introduced above is that it requires a large independent testing sequence. in practice ever an additional sample is rarely available. one usually wants to incorporate all available yi pairs in the decision function. in such cases to estimate l n we have to rely on the training data only_ there are well-known methods that we selecting classifiers will discuss later that are based on cross-validation leave-one-out and brailovsky stone and holdout resubstitution rotation smoothing and bootstrapping which may be employed to construct an empirical risk from the training sequence thus obviating the need for a testing sequence. kanal cover and wagner toussaint glick hand jain dubes and chen and mclachlan for surveys discussion and empirical comparison. analysis of these methods in general is clearly a much harder problem as can depend on dn in a rather complicated way. if we construct some estimator ln from d n then it would be desirable to obtain distribution-free bounds on or on e lnl q for some q conditional probabilities and expectations given dn are ingless since everything is a funseion of dn. here however we have to be much more careful as we do not want ln to be optimistically biased because the same data are used both for training and testing. distribution-free bounds for the above quantities would be extremely helpful as we usually do not know the distribution of y. while for some rules such estimates exist-we will exhibit several avenues in chapters and i-it is disappointing that a single error estimation method cannot possibly work for all discrimination rules. it is therefore important to point out that we have to consider ln pairs-for every rule one or more error estimates must be found if possible and vice versa for every error estimate its limitations have to be stated. secondly rules for which no good error estimates are known should be avoided. luckily most popular rules do not fall into this category. on the other hand proven distribution-free performance guarantees are rarely available-see chapters and for examples. selecting classifiers probably the most important application of error estimation is in the selection of a classification function from a class c of functions. if a class c of classifiers is given then it is tempting to pick the one that minimizes an estimate of the error probability over the class. a good method should pick a classifier with an error probability that is close to the minimal error probability in the class. here we require much more than distribution-free performance bounds of the error estimator for each of the classifiers in the class. problem demonstrates that it is not sufficient to be able to estimate the error probability of all classifiers in the class. intuitively if we can estimate the error probability for the classifiers in c uniformly well then the classification function that minimizes the estimated error probability is likely to have an error probability that is close to the best in the class. to certify this error estimation intuition consider the following situation let c be a class of classifiers that is a class of mappings of the form n d i. assume that the error count n ln n j ipx y j jl is used to estimate the error probability l p i y of each classifier e c. denote by the classifier that minimizes the estimated error probability over the class ln ln for all e c. then for the error probability of the selected rule we have lemma and chervonenkis see also devroye l inf l sup iln l iln l sup iln l proof. l l n ln inf l l ln sup iln l sup iln l the second inequality is trivially true. we see that upper bounds for suppec iln l provide us with upper bounds for two things simultaneously an upper bound for the suboptimality of within c that is a bound for l infpec l an upper bound for the error iln l committed when l n is used to estimate the probability of error l of the selected rule. in other words by bounding suppec iln l we kill two flies at once. it is particularly useful to know that even though ln is usually optimistically biased it is within given bounds of the unknown probability of error with and that no other test sample is needed to estimate this probability of error. whenever selecting classifiers our bounds indicate that we are close to the optimum in c we must at the same time have a good estimate of the probability of error and vice versa. as a simple but interesting application of lemma we consider the case when the class c contains finitely many classifiers. theorem assume that the cardinality oj c is bounded by n. then we have for all e proof. p iln l e lpiln l e where we used hoeffding s inequality and the fact that the random variable nln is binomially distributed with parameters nand l remark. distribution-free properties. theorem shows that the problem studied here is purely combinatorial. the actual distribution of the data does not playa role at all in the upper bounds. remark. without testing data. very often a class of rules c of the form n nx dn is given and the same data dn are used to select a rule by minimizing some estimates ln n of the error probabilities l n p nx yidn. a similar analysis can be carried out in this case. in particular if denotes the selected rule then we have similar to lemma theorem and iln l sup iln n l nl. ec if c is finite then again similar to theorem we have for example error estimation estimating the bayes error it is also important to have a good estimate of the optimal error probability l first of all if l is large we would know beforehand that any rule is going to perform poorly. then perhaps the information might be used to return to the feature selection stage. also a comparison of estimates of ln and l gives us an idea how much room is left for improvement. typically l is estimated by an estimate of the error probability of some consistent classification rule fukunaga and kessel chen and fu fukunaga and hummels and garnett and yau clearly if the estimate we use is consistent in the sense that in ln with probability one as n and the rule is strongly consistent then in l with probability one. in other words we have a consistent estimate of the bayes error probability. there are two problems with this approach. the first problem is that if our purpose is comparing l with l n then using the same estimate for both of them does not any information. the other problem is that even though for many classifiers ln ln can be guaranteed to converge to zero rapidly regardless what the distribution of y is chters and in view of the results of chapter the rate of convergence of ln to l using such a method may be arbitrarily slow. thus we cannot expect good performance for all distributions from such a method. the question is whether it is possible to come up with a method of estimating l such that the difference in l converges to zero rapidly for all distributions. unfortunately there is no method that guarantees a certain finite sample performance for all distributions. this disappointing fact is reflected in the following negative result theorem for every n for any estimate in of the bayes error probability l and for every e there exists a distribution of x y such that e li e. proof. for a fixed n we construct a family f of distributions and show that for at least one member of the family e i in l i e. the family contains distributions where m is a large integer specified later. in all cases xl xn are drawn independently by a uniform distribution from the set m. let bo bi bn be i.i.d. bernoulli random variables independent of the xis with pbi o pbi i for the first member of the family f let yi bi for i n. thus for this distribution l the bayes error for the other members of the family is zero. these distributions are determined by m binary parameters ai e i as follows ri py i ai. in other words yi a xi for every i n. clearly l for these tions. note also that all distributions with x distributed uniformly on m problems and exercises and l are members of the family. just as in the proofs of theorems and we randomize over the family f of distributions. however the way of ization is different here. the trick is to use bo b i bn in randomly picking a distribution. that these random variables are just the labels yi yn in the training sequence for the first distribution in the family. we choose a tion randomly as follows if bo then we choose the first member of f one with l if bo then the labels of the training sequence are given by if xi xl xi x xi xi l if j i is the smallest index such that xi x j note that in case of bo for any fixed realization hi hn e i of b i bn the bayes risk is zero. therefore the distribution is in the family f. now let a be the event that all the xis are different. observe that under a ln is a function of xl x n b i bn only but not bo. therefore supeiln li eiln li bo b i bn random e i a i ln l i e ii. ib! ii. e lin e now if we pick m large enough pa can be as close to as desired. hence sup e i ln l i where the supremum is taken over all distributions of y. problems and exercises let b be a binomial random variable with parameters nand p. show that and pb e ee-np-elogejnp np pb e ee-np-elogejnp np hint proceed by chernoffs bounding method. error estimation problem prove the inequalities of bennett and bernstein given in theorem to help you we will guide you through different stages show that for any s and any random variable x with ex x where show that fflu for u by chernoffs bounding method show that feu log e csu ecs lu lu show that feu fo ufo cs using the bound of find the optimal value of s and derive bennetts inequality. cs u. problem use bernsteins inequality to show that if b is a binomial p random variable then for e and problem letx xn be independent binary-valued random variables withpxi i pxi o pi. set p ll pi and sn ll xi prove that and valiant see also hagerup and rub compare the results with bernsteins inequality for this case. hint put s e and s e in the chernoff bounding argument. prove and exploit the elementary inequalities and e e-lo. problem let b be a binomial p random variable. show that for p a show that for a p the same upper bounds hold for pb an see also hagerup and rub hint use chernoffs method with parameter s and set s a problem let b be a binomial p random variable. show that if p problems and exercises and if p hint use chernoffs method and the inequality pb np e- x x log xlog p p x p for p x and for x p problem let b be a binomial p random variable. prove that and p-jb fp eyn p-jb fp hint use chernoffs method and the inequalities x log x log yp x e and x log x log yp x e p x p x p p x p problem give a class c of decision functions of the form n d i the training data do not play any role in the decision such that for every e supp l e for every distribution where ln is the error-counting estimate of the error probability l p y of decision and at the same time if fn is the class of mappings minimizing the error count ln over the class c then there exists one distribution such that p sup l for all n. inf l problem let c be a class of classifiers that is a class of mappings of the form nx dn nx. assume that an independent testing sequence tm is given and that the error count m lnm n l i m j! is used to estimate the error probability l n p nx yidn of each classifier n e c. denote by the classifier that minimizes the estimated error probability over the class. prove that for the error probability error of the selected rule we have also if c is of finite cardinality with lei n then problem show that if a rule gn is consistent then we can always find an estimate of the error such that elin ln iq for all q hint split the data sequence dn and use the second half to estimate the error probability of problem open-ended problem. is there a rule for which no error estimate works for all distributions? more specifically is there a sequence of classification rules gn such that for all n large enough infsupein lni c in xy for some constant c where the infimum is taken over all possible error estimates? are such rules necessarily inconsistent? problem consider the problem of estimating the asymptotic probability of error of the nearest neighbor rule lnn x x show that for every n for any estimate ln of l nn and for every e there exists a distribution of ex y such that the regular histogram rule in this chapter we study the cubic histogram rule. recall that this rule partitions rd into cubes of the same size and gives the decision according to the number of zeros and ones among the yis such that the corresponding xi falls in the same cube as x. pn a denotes a partition of rd into cubes of size h n that is into sets of the type lhn where the kis are integers and the histogram rule is defined by where for every x e rd anx ani if x e ani. that is the decision is zero if the number of ones does not exceed the number of zeros in the cell in which x falls. weak universal consistency of this rule was shown in chapter under the conditions hn and nh as n the purpose of this chapter is to introduce some techniques by proving strong universal consistency of this rule. these techniques will prove very useful in handling other problems as well. first we introduce the method of bounded differences. the method of bounded differences in this section we present a generalization of hoeffdings inequality due to mcdiarmid the result will equip us with a powerful tool to handle plicated functions of independent random variables. this inequality follows by results of hoeffding and azuma who observed that theorem the regular histogram rule can be generalized to bounded martingale difference sequences. the inequality has found many applications in combinatorics as well as in nonparametric statistics mcdiarmid and devroye for surveys. let us first recall the notion of martingales. consider a probability space f p. definition a sequence of random variables z is called a martingale if e zd zi with probability one for each i let xl x be an arbitrary sequence of random variables. zl is called a martingale with respect to the sequence xl x iffor every i zi is afunction of xl xi and eziiixi xd zi with probability one. obviously if zl is a martingale with respect to xl then zl is a martingale since e zd e xd zi zd eziizl the most important examples of martingales are sums of independent mean random variables. let be independent random variables with zero mean. then the random variables i si l uj jl i form a martingale problem martingales share many properties of sums of independent variables. our purpose here is to extend hoeffdings inequality to martingales. the role of the independent random variables is played here by a so-called martingale difference sequence. definition a sequence of random variables vi is a martingale ference sequence if e vi vi with probability one for every i a sequence of random variables vi is called a martingale difference sequence with respect to a sequence of random variables xl x iffor every i vi is a function of xl xi and e ix xd with probability one. the method of bounded differences again it is easily seen that if vi is a martingale difference sequence with respect to a sequence xl x of random variables then it is a martingale ference sequence. also any martingale zl leads naturally to a martingale difference sequence by defining for i o. the key result in the method of bounded differences is the following inequality that relaxes the independence assumption in theorem allowing martingale difference sequences theorem azuma let xl x be a sequence of random variables and assume that vi is a martingale difference quence with respect to x i x assume furthermore that there exist random variables z and nonnegative constants such that for every i zi is afunction of xl xi-i and then for any e and n and the proof is a rather straightforward extension of that of hoeffding s inequality. first we need an analog of lemma lemma assume that the random variables v and z satisfy with probability one that e v i z and for some function f and constant c fez v fez c. then for every s the proof of the lemma is left as an exercise proof of theorem as in the proof of hoeffding s inequality we proceed by chernoffs bounding method. set sk then for any s ps n_e e-see ssn the rygular histogram rule e-se ct e e l-il ci previous argument s c. lzl z n the second inequality is proved analogously. now we are ready to state the main inequality of this section. it is a large deviation-type inequality for functions of independent random variables such that the function is relatively robust to individual changes in the values of the random variables. the condition of the function requires that by changing the value of its i variable the value of the function cannot change by more than a constant ci. theorem let xl xn be independent random riables taking values in a set a and assume that f an r satisfies sup ifxl xn fxi xi-i x ci i n xl xea then for all e xn efx xn e e- and p x n fx x n e e cf proof. define v fx xn-efxi xn. introduce vi ev and for k so that v ll vk. clearly vi vn form a martingale difference sequence with respect to xl x n define the random variables and vk hkx i x k f hkx x k- i xfkdx where the integration is with respect to fk the probability measure of x k. introduce the random variables wk sp hkx i xk-i u f hkx xk-j xfkdx and the method of bounded differences clearly zk vk wk with probability one. since for every k zk is a function of xl xk-l we can apply theorem directly to v vk if we can show that wk zk ck. but this follows from wk zk sup sup xk-iu hkx i xk-l v u by the condition of the theorem. clearly if the xis are bounded then the choice ixl xn xi yields hoeffdings inequality. many times the inequality can be used to handle very complicated functions of independent random variables with great elegance. for examples in nonparametric statistics see problems similar methods to those used in the proof of theorem may be used to bound the variance xn. other inequalities for the variance of general functions of independent random variables were derived by efron and stein and steele theorem assume that the conditions of theorem hold. then proof. using the notations of the proof of theorem we have to show that varv z cf n il observe that varv e where in the last step we used the martingale property in the following way for i j we have with probability one. the regular histogram rule thus the theorem follows if we can show that introducing wi and zi as in the proof of theorem we see that with bility one zi s vi s zi ci. since zi is a function of xl xi therefore conditioned on xl xi-i is a zero mean random variable taking values in the interval zi cd. but an arbitrary random variable u taking values in an interval b has variance not exceeding so that cf evi ixi xi-d s which concludes the proof. strong universal consistency the purpose of this section is to prove strong universal consistency of the histogram rule. this is the first such result that we mention. later we will prove the same property for other rules too. the theorem stated here for cubic partitions is essentially due to devroye and gyorfi for more general sequences of partitions see problem an alternative proof of the theorem based on the vapnik-chervonenkis inequality will be given later-see the remark following theorem theorem assume that the sequence of partitions satisfies the following two conditions as n and nh for any distribution of y andfor every e there is an integer no such that for n no for the error probability ln of the histogram rule pln l e s thus the cubic histogram rule is strongly universally consistent. proof. define y. lcl c ry x n nflanx clearly the decision based on strong universal consistency is just the histogram rule. therefore by theorem it suffices to prove that for n large enough p iryx ryxijldx decompose the difference as the convergence of the first term on the right-hand side implies weak consistency of the histogram rule. the technique we use to bound this term is similar to that which we already saw in the proof of theorem for completeness we give the details here. however new ideas have to appear in our handling of the second term. we begin with the first term. since the set of continuous functions with bounded support is dense in l it is possible to find a continuous function of bounded support rx such that f rxif.ldx note that rx is uniformly continuous. introduce the function rx e n f.lanx then we can further decompose the first term on the right-hand side of as irx rx i irx we proceed term by term first term the integral of by the definition of rx. respect to f.l is smaller than second term using fubinis theorem we have e i f.ldx f.l j rxif.ldx f irx l ahiaj o j aj the regular histogram rule as rex is uniformly continuous if hn is small enough then ry! for every x yea for any cell a e pn then the double integral in the above expression can be bounded from above as follows l. l irx ryllldxlldy j note that we used the condition hn here. summing over the cells we get f irx rzxllldx third term we have f l ie j y ixea j i ajepn a ili li f irx fourth term our aim is to show that for n large enough e f to this end let s be an arbitrary large ball centered at the origin. denote by mn the number of cells of the partition pn that intersect s. clearly mn is proportional to h as hn o. using the notation vna iyilxea it is clear that vna fa now we can write e f e l e l ievnanj j strong universal consistency e l ievnanj ja sc denotes the complement of s l je ievnanj ja the cauchy-schwarz inequality mn l j mn ja n mn l mn j.a n n tlan jensens inequality if n and the radius of s are large enough since mnn converges to zero by the condition nh and tlsc can be made arbitrarily small by choice of s. we have proved for the first term on the right-hand side of that for n large enough e f finally we handle the second term on the right-hand side of by obtaining an exponential bound for f e f using theorem fix the training data yl yn e rd x and replace yi by yi changing the value of to then differs from zero only on anxi and anx and thus f f f lxltldx the regular histogram rule so by theorem for sufficiently large n p iryx ryxijldx p iryx ryxijldx e f iryx e-n ryxijldx remark. strong universal consistency follows from the exponential bound on the probability pln l e via the borel-cantelli lemma. the inequality in theorem may seem universal in nature. however it is distribution-dependent in a surreptitious way because its range of validity n no depends heavily on e h n and the distribution. we know that distribution-free upper bounds could not exist anyway in view of theorem problems and exercises problem let vi be independent random variables with zero mean. show that the random variables si vj i form a martingale. problem prove lemma problem let xl x n be real valued i.i.d. random variables with distribution tion f and corresponding empirical distribution function fn denote the kolmogorov-smirnov statistic by vn sup ifnx fxl xer use theorem to show that compare this result with theorem of them implies the other. also consider a class a of subsets of nd. let zl zn be i.i.d. random variables in n d with common distribution pzl e a va and consider the random variable wn sup ivna vai aea where vnca denotes the standard empirical measure of a. prove that compare this result with theorem and note that this result is true even if sea n for all n. problems and exercises problem the lazy histogram rule. let pn be a sequence of tions satisfying the conditions of the convergence theorem define the lazy histogram rule as follows gnx yj x e ani where xj is the minimum-index point among xl xn for which xj e ani. in other words we ignore all but one point in each set of the partition. if ln is the conditional probability of error for the lazy histogram rule then show that for any distribution of y lim sup eln n-oo problem assume that pn p a k is a fixed partition into k sets. consider the lazy histogram rule defined in problem based on p. show that for all distributions of y limn- oo eln exists and satisfies lim eln pipaj n-oo k l.... ii where fl is the probability measure for x and pi fa show that the limit of the probability of error el for the ordinary histogram rule is lim el minpi i pi n-oo k l.... ii and show that lim eln lim el. n-oo n-oo problem histogram density estimation. let xl xn be i.i.d. random variables in n d with density f. let p be a partition of n d and define the histogram density estimate by n fnx naax ixeax where ax is the set in p that contains x and a is the lebesgue measure. prove for the l of the estimate that p iflx fxldx e f ifnx fxdxi e conclude that weak of the estimate implies strong sistency see also problem problem general partitions. extend the consistency result of theorem for quences of general not necessarily cubic partitions. actually cells of the partitions need not even be hyperrectangles. assume that the sequence of partitions satisfies the following two conditions. for every ball s centered at the origin and lim. max ilx yii n-oo xyean lim ani n s o. n-oo n prove that the corresponding histogram classification rule is strongly universally consistent. the regular histogram rule problem show that for cubic histograms the conditions of problem on the partition are equivalent to the conditions hn and nh respectively. problem linear scaling. partition n d into congruent rectangles of the form where kl are integers and hi denote the size of the edges of the if hi for every i d and nh hd as n hint this is a corollary of problem problem nonlinear scaling. let fi fd n n be invertible strictly tone increasing functions. consider the partition of n d whose cells are rectangles of the form rectangles. prove that the corresponding histogram rule is strongly universally consistent fi-ikl l x x fkd lhd problem prove that the histogram rule corresponding to this partition is strongly universally consistent under the conditions of problem hint use problem problem necessary and sufficient conditions for the bias. a sequence of titions is called if for every measurable set a for every e and for all sufficiently large n there is a set an e apn denotes the a-algebra generated by cells of the partition p such that e. prove that the bias term f converges to zero for all distributions of y if and only if the sequence of partitions is for every probability measure on n d conclude that the first condition of problem implies that is for every probability measure problem necessary and sufficient conditions for the variation. assume that for every probability measure fj- on n d every measurable set a every c and every e there is an ne c a such that for all n ne c a l n a e. nasc n prove that the variation term f converges to zero for all distributions of ex y if and only if the sequence of partitions satisfies the condition above jaoude problem the e-effective cardinality mp ilv a e of a partition with respect to the probability measure restricted to a set a is the minimum number of sets in p such that the union of the remaining sets intersected with a has less than e. prove that the sequence of partitions satisfies the condition of problem if and only if for every e mpn a e n gyorfi and van der meulen problem in n partition the plane by taking three fixed points not on a line x y and z. at each of these points partition n by considering k equal sectors of angle k each. sets in the histogram partition are obtained as intersections of cones. is the induced histogram rule strongly universally consistent? if yes state the conditions on k and if no provide a counterexample. problems and exercises problem partition n into shells of size h each. the i-th shell contains all points at lh ih from the origin. let h and nh as n consider distance d e the histogram rule. as n to what does eln converge? kernel rules histogram rules have the somewhat undesirable property that the rule is less curate at borders of cells of the partition than in the middle of cells. looked at intuitively this is because points near the border of a cell should have less weight in a decision regarding the cells center. to remedy this problem one might troduce the moving window rule which is smoother than the histogram rule. this classifier simply takes the data points within a certain distance of the point to be classified and decides according to majority vote. working formally let h be a positive number. then the moving window rule is defined as gnx if iyioxiesxh iyi esxh otherwise where sxh denotes the closed ball of radius h centered at x. it is possible to make the decision even smoother by giving more weight to closer points than to more distant ones. let k rd r be a kernel junction which is usually nonnegative and monotone decreasing along rays starting from the origin. the kernel classification rule is given by gnx i l..il otherwise. k i l..il k kernel rules figure the moving dow rule in the decision is in the shaded area. g o class class the number h is called the smoothing factor or bandwidth. it provides some form of distance weighting. figure kernel rule on the real line. the figure shows xi! hfor n ku u epanechnikov kernel and three smoothing factors h. one definitely undersmooths and one oversmooths. we took p and the class-cconditional ties are fox x and flex on clearly the kernel rule is a generalization of the moving window rule since taking the special kernel k ixesod yields the moving window rule. this kernel is sometimes called the naive kernel. other popular kernels include the sian kernel kx e- the cauchy kernel kx iixll dl and the epanechnikov kernel kx distance. where ii ii denotes euclidean consistency gaussian kernel cauchy kernel epanechnikov kernel uniform kernel o figure various kernels on r. kernel-based rules are derived from the kernel estimate in density estimation originally studied by parzen rosenblatt akaike and coullos problems and and in regression estimation troduced by nadaraya and watson for particular choices of k rules of this sort have been proposed by fix and hodges sebestyen van ryzin and meisel statistical analysis of these rules andor the corresponding regression function estimate can be found in nadaraya rejto and revesz devroye and wagner greblicki krzyzak and pawlak and devroye and krzyzak usage of cauchy kernels in discrimination is investigated by arkadjew and braverman hand and coomans and broeckaert consistency in this section we demonstrate strong universal consistency of kernel-based rules under general conditions on hand k. let h be a smoothing factor depending only on n and let k be a kernel function. if the conditional densities fo exist then weak and strong consistency follow from problems and tively via problem we state the universal consistency theorem for a large class of kernel functions namely for all regular kernels. definition the kernel k is called regular if it is nonnegative and there is a ball sor of radius r centered at the origin and constant b such that kx bisor and f supyexsor kydx we provide three informative exercises on regular kernels in all cases regular kernels are bounded and integrable. the last condition holds whenever k is integrable and uniformly continuous. introduce the short notation khx kkv. the next theorem states strong universal sistency of kernel rules. the theorem is essentially due to devroye and krzyzak kernel rules under the assumption that x has a density it was proven by devroye and gyorfi and zhao theorem and krzyzak assume that k is a regular kernel.lf h and nhd as n thenfor any distribution of y andfor every e there is an integer no such that for n no for the error probability ln of the kernel rule pln l e where the constant p depends on the kernel k and the dimension only. thus the kernel rule is strongly universally consistent. clearly naive kernels are regular and moving window rules are thus strongly universally consistent. for the sake of readability we give the proof for this special case only and leave the extension to regular kernels to the reader-see problems and before we embark on the proof in the next section we should warn the reader that theorem is of no help whatsoever regarding the choice of k or h. one possible solution is to derive explicit upper bounds for the probability of error as a function of descriptors of the distribution of y and of k nand h. minimizing such bounds with respect to k and h will lead to some expedient choices. typically such bounds would be based upon the inequality eln l e p!ox pno!noxldx f ip!lx pnjinlxldx chapter where fo are the class densities fno are their kernel mates problem p and p are the class probabilities and pno pnl are their relative-frequency estimates. bounds for the expected in sity estimation may be found in devroye for d and holmstrom and klemehi for d under regularity conditions on the distribution the choice h cn-d for some constant is asymptotically optimal in density timation. however c depends upon unknown distributional parameters. rather than following this roundabout process we ask the reader to be patient and to wait until chapter where we study automatic kernel rules i.e. rules in which h and sometimes k as well is picked by the data without intervention from the statistician. it is still too early to say meaningful things about the choice of a kernel. the kernel density estimate fnx nd c xi based upon an i.i.d. sample xl drawn from an unknown density f is clearly a density in its own right if k and j k also there are certain consistency popular choices of k that are based upon various optimality criteria. in pattern recognition the story is much more confused as there is no compelling a priori reason to pick a function k that is nonnegative or integrable. let us make a few points with the trivial case n ting h the kernel rule is given by ifyi gi otherwise. if k then gnx if yi or if yi and kx xd o. as we would obviously like gnx if and only if it seems necessary to insist on k everywhere. however this restriction makes the kernel estimate nonlocal in nature. for n and d consider next a negative-valued kernel such as the hermite kernel figure hermite kernel. it is easy to verify that k if and only if i x i also j k o. nevertheless we note that it yields a simple rule gl otherwise. if yi ix xii or if yi i ix xii if we have a biatomic distribution for x with equally likely atoms at and and and y if x and y if x then l and the probability of error for this kernel rule i is as well. note also that for all n gn gi if we keep the same k. consider now any positive kernel in the same example. if xl xn are all zero then the decision is gnx for all x. hence ln ipxl xn o. our negative zero-integral kernel is strictly better for all n than any positive kernel! such kernels should not be discarded without further thought. in density estimation negative-valued kernels are used to reduce the bias under some smoothness conditions. here as shown above there is an additional reason-negative weights given to points far away from the xis may actually be beneficial. staying with the same example if k everywhere then eli y i y o which maybe o happens when e i everywhere. for this particular example we would have obtained the same result kernel rules even if k everywhere. with k we simply ignore the xis and take a majority vote among the yis k it would be a minority vote! if iyio iyil otherwise. gn let nn be the number of yis equal to zero. as nn is binomial p with p py i we see that eln pp pp minp p simply by invoking the law of large numbers. thus eln minp p. as in the case with n the limit is when p even though l when e i everywhere. it is interesting to note the following though eli p minp p minp p p lim elno n-oo the expected error with one observation is at most twice as bad as the expected error with an infinite sequence. we have seen various versions of this inequality at work in many instances such as the nearest neighbor rule. let us apply the inequality for el to each part in a fixed partition p of rd. on each of the k sets ai ak ofp we apply a simple majority vote among the yis as in the histogram rule. if we define the lazy histogram rule as the one in which in each set ai we assign the class according to the yj for which x j e ai and j is the lowest such index first point to fall in a. it is clear problems and that lim ellazyn lim eln n-oo n-oo t l ryxldx where ln is the probability of error for the ordinary histogram rule. again the vast majority of observations is barely needed to reach a good decision. just for fun let us return to a majority vote rule now applied to the first three observations only. with p p i we see that p pfp p by just writing down binomial probabilities. observe that p p minp pl lim eln lim n-oo n-oo p proof of the consistency theorem if limn--oo eln is small to start with e.g. limn--oo eln then x in such cases it just does not pay to take more than three observations. kernels with fixed smoothing factors have no local sensitivity and except in some circumstances have probabilities of error that do not converge to l the versal consistency theorem makes a strong case for decreasing smoothing there is no hope in general of approaching l unless decisions are asymptotically local. the consistency theorem describes kernel rules with h these rules become more and more local in nature as n the necessity oflocal rules is not ent from the previous biatomic example. however it is clear that if we consider a distribution in which given y x is uniform on and given y x is uniform on that is with the two classes intimately interwoven a kernel rule with k of compact support and h will have ln i where ni is the number of xs at the i-th atom. hence eln goes to zero nentially fast. if in the above example we assign x by a geometric distribution on when y and by a geometric distribution on when y then to obtain eln it is necessary that h problem remark. it is worthwhile to investigate what happens for negative-valued kernels k when h nhd and k has compact support. every decision becomes an average over many local decisions. if fj has a density f then at almost all points x f may be approximated very nicely by f fasxo for small where s xo is the closed ball of radius about x. this implies roughly speaking that the number of weighted votes from class observations in a neighborhood of x is about fxnhd f k while for class the weight is about f k. the correct decision is nearly always made for nh d large enough provided that j k o. see problem on why kernels with j k should be avoided. proof of the consistency theorem in the proof we can proceed as for the histogram. the crucial difference is captured in the following covering lemmas. let denote the minimum number of balls of radius that cover the ball so if k is the naive kernel then p in theorem lemma lemma.f kx ixesoil thenforany y end h and probability measure fj f khx y f khx zfjdz fjdx kernel rules proof. cover the ball syh by fjd balls of radius denote their centers by xl then x e implies c sxh and thus we may write jlsxh f ixesyh jldx t f jldx t f ixesxi jldx jl xh il il lemma let h r and let s c rd be a ball of radius r. then for every probability measure jl where cd depends upon the dimension d only. proof. cover s with balls of radius h centered at center points of a regular grid of dimension x x denote these centers by xl where m is the number of balls that cover s. clearly m volumesorh volumegrid cell vdr hd is the volume of the unit ball in r d d c where the constant c depends upon the dimension only. every x gets covered at most kl times where kl depends upon d only. then we have proof of the consistency theorem m f i l il j ttdx the same argument as in lemma m l j il m m l il the cauchy-schwarz inequality jklm he d where cd depends upon the dimension only. proof of theorem define yjkhx x j nekhx x since the decision rule can be written as gnx if yjkhx x j yjkhx x j nekhx x nekhx x otherwise by theorem what we have to prove is that for n large enough p iryx rynxiildx we use a decomposition as in the proof of strong consistency of the histogram rule to handle the first term on the right-hand side fix e and let r rd r be a continuous function of bounded support satisfying f rxlttdx e. kernel rules obviously we can choose the function r such that s rx s for all x e rd. then we have the following simple upper bound e x i rex ekhx x i e x i ekhx x i next we bound the integral of each term on the right-hand side of the inequality above. first term by the definition of r f rxltldx fl. second term since rx is continuous and zero outside of a bounded set it is also uniformly continuous that is there exists a such that ilx y implies irx ft. also rex is bounded. thus rex tldx ekhx x e x i f i f irx f rykhx ytldy i jldx s f khx y ekhx x ekhx x ry!tldytldx s clearly we have in the last step we used the fact that supxy fsxii eh?ltldy s and by the uniform continuity of rex supzesxo irx ft. thus the first term at the end of the chain of inequalities above is bounded by ft. the second term converges to zero since h for all n large enough which in tum implies fs ez tldy o. is obvious for the naive kernel. for regular kernels convergence to zero follows from problem the convergence of the integral respect to tldx follows from the dominated proof of the consistency theorem convergence theorem. in summary we have shown that e x i lim sup n--oo rx ekhx x i i pdx e. third term e x ekhx x i elnx pdx i i iry lyi! khx zpdzpdypdx pdx khx y lykhx ypdy i ekhx x i i i i i f k iry i plry lyipdy pe fubinis theorem ryylldy where in the last two steps we used the covering lemma lemma for the naive kernel and problem for general kernels and the definition of rex. p is the constant appearing in the covering lemma fourth term we show that e ierynx for the naive kernel we have ryn i idx o. e je e x j ely khx xl n eykhx x eykhx nekhx kernel rules where we used the cauchy-schwarz inequality and properties of the naive kernel. extension to regular kernels is straightforward. next we use the inequality above to show that the integral converges to zero. divide the integral over nd into two terms namely an integral over a large ball s centered at the origin of radius r and an integral over sc. for the integral outside of the ball we have e fldx with probability one as n which can be shown in the same way we proved jrd jrd the first second and third terms of clearly the radius r of the ball s can be chosen such that to bound the integral over s we employ lemma f. e fldx fldx s j flsxh the inequality obtained above h cd by assumption nhd therefore if n is sufficiently large then for the first term on the right-hand side of we have e iryx if we take e! ryn ii-dx ep it remains to show that the second term on the right-hand side of is small with large probability. to do this we use mcdiarmids inequality for i iryx rynxll-dx e iryx rynxii-dx fix the training data at yi yn and replace the i-th pair yi by changing the value of to clearly by the covering lemma i i i sup i y fldx yerd nekhx x n potential function rules so by theorem p ix i p ix rynxidx e iryx the proof is now completed. potential function rules kernel classification rules may be formulated in terms of the so-called potential function rules. these rules were originally introduced and studied by bashkirov braverman and muchnik aizerman braverman and rozonoer braverman and braverman and pyatniskii the original idea was the following put a unit of positive electrical charge at every data point xi where yi and a unit of negative charge at data points xi where yi o. the resulting potential field defines an intuitively appealing rule the decision at a point x is one if the potential at that point is positive and zero if it is negative. this idea leads to a rule that can be generalized to obtain rules of the form gnx if i otherwise where n fnx l rnidnknix xi il where the kns describe the potential field around xi and the rns are their weights. rules that can be put into this form are often called potential function rules. here we give a brief survey of these rules. kernel rules. clearly kernel rules studied in the previous section are potential function rules with knix y k t x y here k is a fixed kernel function and hi is a sequence of positive numbers. histogram rules. similarly histogram rules chapters and can be put in this form by choosing kernel rules and rnidn recall that an denotes the cell of the partition in which x falls. knjx y iyeanx polynomial discriminant functions. specht suggested applying a nomial expansion to the kernel k cy. this led to the choice knix y l v-rxljfjy k jl and rnidn where v-rk are fixed real-valued functions on nd. when these functions are polynomials the corresponding classifier gn is called a polynomial discriminant function. the potential function rule obtained this way is a generalized linear rule chapter with k fnx l anjljljx jl where the coefficients an depend on the data dn only through n anj il lv-rjxi. this choice of the coefficients does not necessarily lead to a consistent rule unless the functions v-rk are allowed to change with n or k is allowed to vary with n. nevertheless the rule has some computational advantages over kernel rules. in many practical situations there is enough time to preprocess the data dn but once the observation x becomes known the decision has to be made very quickly. clearly the coefficients an ann can be computed by knowing the training data dn only and if the values v-rkx are easily computable then fnx can be computed much more quickly than in a kernel-based decision where all n terms of the sum have to be computed in real time if no preprocessing is done. however using preprocessing of the data may also help with kernel rules cially when d for a survey of computational speed-up with kernel methods see devroye and machell recursive kernel rules. consider the choice knixyk x y observe that the only difference between this and the ordinary kernel rule is that in the expression of kni the smoothing parameter hn is replaced with hi. with this change we can compute the rule recursively by observing that fnlx fnx lk x xnl hnl problems and exercises the computational advantage of this rule is that if one collects additional data then the rule does not have to be entirely recomputed. it can be adjusted using the formula above. consistency properties of this rule were studied by devroye and wagner krzyzak and pawlak krzyzak and greblicki and pawlak several similar recursive kernel rules have been studied in the literature. wolverton and wagner greblicki and krzyzak and pawlak studied the situation when y knix y k t the corresponding rule can be computed recursively by fnlx fnx k hnl xnl hnl motivated by stochastic approximation methods chapter revesz suggested and studied the rule obtained from fnlx fnx n fnx-d-k hnl xnl hnl a similar rule was studied by gyorfi fnlx fnx n fnxk xnl hnl problem let k be a nonnegative kernel with compact support on show problems and exercises that for some distribution h is necessary for consistency of the kernel rule. to this end consider the following example. given y x has a geometric distribution on and given y x has a geometric distribution on then show that to obtain eln l it is necessary that h nd with density f. let k be a kernel function integrating to one and let hn be a problem kernel density estimation. let xl xn be i.i.d. random variables in smoothing factor. the kernel density estimate is defined by fnx nh ft k t parzen prove that the estimate is weakly universally consistent in ll if h n and nh as n hint proceed as in problem kernel rules problem strong consistency of kernel density estimation. let xl xn be i.i.d. random variables in nd with density f. let k be a nonnegative function integrating to one kernel and h a smoothing factor. as in the previous exercise the kernel density estimate is defined by fnx k-h i n i! prove for the ll-error of the estimate that p l.hlx fxldx e f ifnx fxdxi e s conclude that weak ll-consistency of the estimate implies strong sistency problem this is a way to show that weak and strong ll-consistencies of the kernel density estimate are equivalent also for d since if k is nonnegative then e f ifnx fxldx cannot converge to zero faster than for any density devroye and gyorfi therefore the inequality above implies that for any density lim f ifnx n-co e f ifnx fxldx fxldx with probability one this property is called the relative stability of the l i error. it means that the asymptotic behavior of the l i is the same as that of its expected value. hint use mcdiarmids inequality. problem if f k show that under the assumption that fj. has a density f and that h nhd the kernel rule has lim eln e i l n-co thus the rule makes the wrong decisions and such kernels should be avoided. hint you may use the fact that for the kernel density estimate with kernel l satisfying f l f ifnx fxldx with probability one if h and nhd problems and problem consider a devilish kernel that attaches counterproductive weight to the origin kx if iixll s if ilxll s if ilxll assume that l o. show that ln with assume that h yet nhd probability one. concession if you find that you cant handle the universality try first proving the statement for strictly separable distributions. problem show that for the distribution depicted in figure the kernel rule with kernel ku is consistent whenever h the smoothing factor remains fixed and h s problem the limit for fixed h. consider a kernel rule with fixed h and fixed kernel k. find a simple argument that proves problems and exercises lim eln looi n-oo where loo is the probability of error for the decision goo defined by if ekx if ekx l i find a distribution such that for the window kernel loo yet l is there such a distribution for any kernel? hint try proving a convergence result at each x by invoking the law of large numbers and then replace x by x. problem show that the conditions h n and nhd of theorem are not necessary for consistency that is exhibit a distribution such that the kernel rule is consistent with hn and exhibit another distribution for which the kernel rule is consistent with hn rv lnljd. problem prove that the conditions hn and nhd of theorem are necessary for universal consistency that is show that if one of these conditions are violated then there is a distribution for which the kernel rule is not consistent problem this exercise provides an argument in favor of monotonicity of the kernel k. in find a nonatornic distribution for y and a positive kernel with f k k vanishing off so.o for some such that for all h and all n the kernel rule b in the universal consistency theorem cannot be abolished altogether. has eln while l o. this result says that the condition kx bisool for some problem with k as in the previous problem and taking h show that lim eln l n-oo under the following conditions k has compact support vanishing off soo k and k bisol for some e o. we say that we have agreement on sxo when for all z e sxo either or we ask that pagreement on sxo problem the previous exercise shows that at points where there is agreement we make asymptotically the correct decision with kernels with fixed smoothing factor. let d be the set and let the of d be defined by do ii y x ii for some xed. let fl be the probability measure for x. take k h as in the previous exercise. noting that x f do means that we have agreement on sxo show that for all distributions of y lim sup eln l fldo. n-oo kernel rules figure of a set d. problem continuation. clearly as when fj.d o. convince yourself that fj.d for most problems. if you knew how fast tended to zero then the previous exercise would enable you to pick h as a function of n such that h and such that the upper bound for eln obtained by analogy from the previous exercise is approximately minimal. if in n d d is the surface of the unit ball x has a bounded density f and is lipschitz determine a bound for by considering the proof ofthe universal consistency theorem show how to choose h such that problem extension of the covering lemma to regular kernels. let k be a regular kernel and let fj. be an arbitrary probability measure. prove that there exists a finite constant p pk only depending upon k such that for any y and h and krzyzak hint prove this by checking the following first take a bounded overlap cover of nd with translates of where r is the constant appearing in the definition of a regular kernel. this cover has an infinite number of member balls but every x gets covered at most k times where k depends upon d only. the centers of the balls are called xi i the integral condition on k implies that l sup kxs il for another finite constant show that k j dx sup kydxs khx y l sup khx yixeyhxi il xeyhxi and from conclude problems and exercises l. supzehxiso khz il bfly hxi f fly hxi khz fl bfly hxi il sup b il khz b where depends on k and d only. problem let k be a regular kernel and let fl be an arbitrary probability measure. prove that for any r h sp f khx yillx-yllo f khx zfld fl x hint substitute khz in the proof of problem by khzillzil and notice that f khx yillx-yllo khx zfldz f sup y fldx l. il sup khzilizll as h o. problem use problems and to extend the proof of theorem for arbitrary regular kernels. problem show that the constant in lemma is never more than problem show that if l is a bounded function that is monotonically decreasing on with the property that f u ludu and if k n d is a function with kx llix ii then k is regular. d problem find a kernel k that is monotonically decreasing along rays krx kx for all x e n d and all r such that k is not regular. exercise is intended to convince you that it is very difficult to find well-behaved kernels that are not regular. problem let kx lllxll for some bounded function l o. show that k is regular if l is decreasing on and f kxdx conclude that the gaussian and cauchy kernels are regular. problem regularity of the kernel is not necessary for universal consistency. gate universal consistency with a nonintegrable kernel-that is for which f k as kx ixl. greblicki krzyzak and pawlak proved consistency of the kernel rule with smoothing factor hn satisfying hn and nh if the nel k satisfies the following conditions kx cilixliolj for some c and for some cl ii kx where h is a nonincreasing function on with ud hu as u kernel rules problem consider the kernel rule with kernel kx lllx ilr r such kernels are useless for atomic distributions unless we take limits and define gn as usual when x tj. s the collection of points z with xi x z for some pair j. for xes we take a majority vote over the yis for which xi x. discuss the weak universal consistency of this rule which has the curious property that gn is invariant to the smoothing factor h-so we might as well set h without loss of generality. note also that for r d is kxdx and for r d isc kxdx where soi is the unit ball of nd centered at the origin. in particular if r d by considering x uniform on so i if y and x uniform on the surface of so i if y show that even though l the probability of error of the rule may tend to a nonzero limit for certain values of p i. hence the rule is not universally consistent. for r d prove or disprove the weak universal consistency noting that the rules decisions are by-and-iarge based on the few nearest neighbors. prove the rule is weakly consistent for all r d whenever x has a density. o! problem assume that the class densities coincide that is ii for every x e n and assume p p show that the expected probability of error of the kernel rule with k is smaller than that with any unimodal regular kernel for every n and h small enough. exhibit a distribution such that the kernel rule with a symmetric kernel such that k i is monotone increasing has smaller expected error probability than that with any unimodal regular kernel. problem scaling. assume that the kernel k can be written into the following product form of one-dimensional kernels kx kxj xed n kixi d i-i assume also that k is regular. one can use different smoothing factors along the different coordinate axes to define a kernel rule by n if l yi k i-i d xj h in otherwise where xj denotes the j component of xi prove that gn is strongly universally consistent if hin for all i d and hdn problem let k be a function and l a symmetric positive definite d x d matrix. for x e n d define kx k find conditions on k such that the kernel rule with kernel k is universally consistent. problem prove that the recursive kernel rule defined by is strongly universally consistent if k is a regular kernel hn and nh as n and pawlak problem show that the recursive kernel rule of is strongly universally sistent whenever k is a regular kernel. limn hxl hn and ll h and weaker assumptions on the kernel. they assume that kx c!lllxlllj for some c and pawlak note greblicki and pawlak showed convergence under significantly tion on with ud hu as u they also showed that under the additional that for some ci ci hlixll kx where h is a nonincreasing assumption f k the following conditions on hn are necessary and sufficient for universal consistency problems and exercises problem open-ended problem. let py i given y let x be formly distributed on given y let x be atomic on the rationals with the following distribution let x v w where v and ware independent identically distributed and pv i i consider the kernel rule with the window kernel. what is the behavior of the smoothing factor h that minimizes the expected probability of error eln? consistency of the k-nearest neighbor rule in chapter we discuss results about the asymptotic behavior of k-nearest neighbor classification rules where the value of k-the number of neighbors taken into account at the decision-is kept at a fixed number as the size of the training data n increases. this choice leads to asymptotic error probabilities smaller than but no universal consistency. in chapter we showed that if we let k grow to infinity as n such that k n then the resulting rule is weakly consistent. the main purpose of this chapter is to demonstrate strong consistency and to discuss various versions of the rule. we are not concerned here with the data-based choice of k-that subject deserves a chapter of its own we are also not tackling the problem of the selection of a suitable-even data-based-metric. at the end of this chapter and in the exercises we draw the attention to i-nearest neighbor relabeling rules which combine the computational comfort of the i-nearest neighbor rule with the asymptotic performance of variable-k nearest neighbor rules. consistency of k-nearest neighbor classification and corresponding regression and density estimation has been studied by many researchers. see fix and hodges cover stone beck gyorfi and gyorfi devroye collomb bickel and breiman mack stute devroye and gyorfi bhattacharya and mack zhao and devroye gyorfi krzyzak and lugosi recall the definition of the k-nearest neighbor rule first reorder the data ylx y!x according to increasing euclidean distances of the x j s to x. in other words x co is the i nearest neighbor of x among the points xl x n if distance ties occur consstency of the k-nearest neighbor rule a tie-breaking strategy must be defined. if fjv is absolutely continuous with respect to the lebesgue measure that is it has a density then no ties occur with probability one so formally we break ties by comparing indices. however for general fjv the problem of distance ties turns out to be important and its solution is messy. the issue of tie breaking becomes important when one is concerned with convergence of ln with probability one. for weak universal consistency it suffices to break ties by comparing indices. the k-nn classification rule is defined as gnx if iyuxl iycixo otherwise. in other words gn is a majority vote among the labels of the k nearest neighbors ofx. strong consistency in this section we prove theorem we assume the existence of a density for fjv so that we can avoid messy technicalities necessary to handle distance ties. we discuss this issue briefly in the next section. the following result implies strong consistency whenever x has an absolutely continuous distribution. the result was proved by devroye and gyorfi and zhao the proof presented here basically appears in devroye gyorfi krzyzak and lugosi where strong universal consistency is proved under an appropriate tie-breaking strategy discussion later. some of the main ideas appeared in the proof of the strong universal consistency of the regular histogram rule theorem and gyorfi zhao assume that fjv has a density. if k and k n then for every e there is an no such that forn no pln l e. where yd is the minimal number of cones centered at the origin of angle jr that cover rd. the definition of a cone see chapter thus the k-nn rule is strongly consistent. remark. at first glance the upper bound in the theorem does not seem to depend on k. it is no that depends on the sequence of ks. what we really prove is the following for every e there exists a e such that for any there is an no such that if n no k and kin the exponential inequality holds. for the proof we need a generalization of lemma the role of this covering lemma is analogous to that of lemma in the proof of consistency of kernel rules. strong consistency lemma and gyorfi let bax tlsxl!x-xll a then for all x e rd proof. for x e rd let cx s c rd be a cone of angle jr centered at x. the cone consists of all y with the property that either y x or angley x s jr where s is a fixed direction. if y y e cx s and iix yll iix yli then y y y ii this follows from a simple geometric argument in the vector space spanned by x y and y the proof of lemma now let c i cyd be a collection of cones centered at x with different central direction covering rd. then yd tlbax l tlci n bax. il let x e ci n ba then by the property of the cones mentioned above we have fj..ci n sx l ii n bax a where we use the fact that x e bax. since x is arbitrary tlci n bax a which completes the proof of the lemma. an immediate consequence of the lemma is that the number of points among x i xn such that x is one of their k nearest neighbors is not more than a constant times k. corollary n xi in kyd. il proof. apply lemma with a k n and let tl be the empirical measure tln of xl xn that is for each borel set a r d tlna ixi ea proof of theorem since the decision rule gn may be rewritten as gn otherwise where is the corresponding regression function estimate k k lyix il consistency of the k-nearest neighbor rule the statement follows from theorem if we show that for sufficiently large n p ix s define pn as the solution of the equation note that the absolute continuity of j-l implies that the solution always exists. is the only point in the proof where we use this assumption. also define the basis of the proof is the following decomposition irjnx irjx rjxl for the first term on the right-hand side observe that denoting rn ii xk xii where fi is defined as with y replaced by the constant random variable y and is the corresponding regression function. thus rx first we show that the expected values of the integrals of both terms on the hand side converge to zero. then we use mcdiarmids inequality to prove that both terms are very close to their expected values with large probability. for the expected value of the first term on the right side of using the cauchy-schwarz inequality we have e f e f iryx f je strong consistency n varlxes k xpn x n-isxpnx-idx i s i i n n which converges to zero. for the expected value of the second term on the right-hand side of note that in the proof of theorems and we already showed that lim ei o. n-oo therefore e i s e i e i o. assume now that n is so large that e i ix e i then by we have p iryx u ix ryxlfldx e i ix ixlfldx e i ryxlfldx ixlfldx next we get an exponential bound for the first probability on the right-hand side of by mcdiarmids inequality fix an arbitrary realization of the data dn yi yn and replace yj by yi changing the value of to then consistency of the k-nearest neighbor rule but is bounded by k and can differ from zero only if iix xi ii pnx or iix xi ii pnx. observe that iix xiii pnx if and only if f.lsxlix-xlj kin. but the measure of such xs is bounded by ydkln by lemma therefore and by theorem finally we need a bound for the second term on the right-hand side of this probability may be bounded by mcdiarmids inequality exactly the same way as for the first term obtaining and the proof is completed. remark. the conditions k and kin are optimal in the sense that they are also necessary for consistency for some distributions with a density. however for some distributions they are not necessary for consistency and in fact keeping k for all n may be a better choice. this latter property dealt with in problem shows that the i-nearest neighbor rule is admissible. breaking distance ties theorem provides strong consistency under the assumption that x has a density. this assumption was needed to avoid problems caused by equal distances. turning to the general case we see that if f.l does not have a density then distance ties can occur with nonzero probability so we have to deal with the problem of breaking them. to see that the density assumption cannot be relaxed to the condition that f.l is merely nonatomic without facing frequent distance ties consider the following distribution on n d x n d with d d f.l x ad x td where td denotes the uniform distribution on the surface of the unit sphere of n d and ad denotes the unit point mass at the origin of nd. observe that if x has distribution td x ad and x has distribution ad x td then ilx xii ji. breaking distance ties hence if xl x x are independent with distribution j-l thenpiix i next we list some methods of breaking distance ties. ne-breaking by indices if xi and xj are equidistant from x then xi is declared closer if i j. this method has some undesirable properties. for example if x is monoatomic with then x is the nearest neighbor of all x j j but x j is only the j i-st nearest neighbor of x the influence of x in such a situation is too large making the estimate very unstable and thus undesirable. in fact in this monoatomic case if l e p l e e for some c problem thus we cannot expect a free version of theorem with this tie-breaking method. stones tie-breaking stone introduced a version of the nearest neighbor rule where the labels of the points having the same distance from x as the k-th nearest neighbor are averaged. if we denote the distance of the k-th nearest neighbor to x by rn then stones rule is the following o l ii lix xi ii iyio il ilx xi ii r x xl n x k gnx k x i illx-xdlrx otherwise. this is not a k-nearest neighbor rule in a strict sense since this estimate in general uses more than k neighbors. stone proved weak universal consistency of this rule. adding a random component to circumvent the aforementioned ties we may artificially increase the dimension of the feature vector by one. define the the d i-dimensional random vectors where the randomizing variables v vi vn are real-valued i.i.d. dom variables independent of x y and dn and their common distribution has a density. clearly because of the independence of v the bayes error corresponding to the pair y is the same as that of y. the algorithm performs the k-nearest neighbor rule on the modified data set d x yd yn. it finds the k nearest neighbors of x and uses a majority vote among these labels to guess y. since v has a density and is independent of x distance consistency of the k-nearest neighbor rule ties occur with zero probability. strong universal consistency of this rule can be seen by observing that the proof of theorem used the existence of the density in the definition of pnx only. with our randomization pnx is well-defined and the same proof yields strong universal consistency. estingly this rule is consistent whenever u has a density and is independent of y. if for example the magnitude of u is much larger than that of ii x ii then the rule defined this way will significantly differ from the nearest neighbor rule though it still preserves universal consistency. one should expect however a dramatic decrease in the performance. of course if u is very small then the rule remains intuitively appealing tie-breaking by randomization there is another perhaps more natural way of breaking ties via randomization. we assume that u is a random vector independent of the data where u is independent of x and uniformly distributed on we also artificially enlarge the data by introducing un where the us are i.i.d. uniform as well. thus each ui is distributed as u. let u yclx u u ycnx u be a reordering of the data according to increasing values of ilx xi ii. in case of distance ties we declare ui closer to u than j uj provided that lui ul juj uj. define the k-nn classification rule as gnx if ll. iycicxul ll iyicxuo otherwise and denote the error probability of gn by devroye gyorfi krzyzak and lugosi proved that ln l with probability one for all distributions if k and k n the basic argument in is the same as that of theorem except that the covering lemma has to be appropriately modified. it should be stressed again that if fl has a density or just has an absolutely tinuous component then tie-breaking is needed with zero probability and becomes therefore irrelevant. recursive methods to find the nearest neighbor of a point x among xl x n we may preprocess the data in log n time such that each query may be answered in n scale-invariant rules worst-case time-see for example preparata and shamos other recent developments in computational geometry have made the nearest neighbor rules computationally feasible even when n is formidable. without preprocessing ever one must resort to slow methods. if we need to find a decision at x and want to process the data file once when doing so a simple rule was proposed by vroye and wise it is a fully recursive rule that may be updated as more observations become available. split the data sequence dn into disjoint blocks of length ii in where ii in are positive integers satisfying ii n. in each block find the nearest neighbor of x and denote the nearest neighbor of x from the i-th block by let ytx be the corresponding label. ties are broken by comparing indices. the classification rule is defined as a majority vote among the nearest neighbors from each block note that we have only defined the rule gn for n satisfying ll ii n for some n. a possible extension for all ns is given by gnx gmx where m is the largest integer not exceeding n that can be written as ll ii for some n. the rule is weakly universally consistent if lim in n---oo and wise see problem scale-invariant rules a scale-invariant rule is a rule that is invariant under rescalings of the components. it is motivated by the lack of a universal yardstick when components of a tor represent physically different quantities such as temperature blood pressure alcohol and the number of lost teeth. more formally let xi xed be the d components of a vector x. if d are strictly monotone mappings n n and if we define dxd then gn is scale-invariant if gnx dn dl where yn in other words if all the xis and x are transformed in the same manner the decision does not change. some rules based on statistically equivalent blocks in chapters and are scale-invariant while the k-nearest neighbor rule clearly is not. here we describe a scale-invariant modification of the k-nearest neighbor rule suggested by olshen and devroye consistency of the k-nearest neighbor rule the scale-invariant k-nearest neighbor rule is based upon empirical distances that are defined in terms of the order statistics along the d coordinate axes. first order the points x xl xn according to increasing values of their first ponents xi xii xl breaking ties via randomization. denote the rank of xii by r? and the rank of xl by rl. repeating the same procedure for the other coordinates we obtain the ranks ri d l n. define the empirical distance between x and xi by px xi max irj ijd i a k-nn rule can be defined based on these distances by a majority vote among the yis with the corresponding xis whose empirical distance from x are among the k smallest. since these distances are integer-valued ties frequently occur. these ties should be broken by randomization. devroye proved that this rule randomized tie-breaking is weakly universally consistent when k and kj n problem for another consistent scale-invariant nearest neighbor rule we refer to problem ii ji figure scale-invariant distances of points from a fixed point are shown here. weighted nearest neighbor rules in the k-nn rule each of the k nearest neighbors of a point x plays an equally tant role in the decision. however intuitively speaking nearer neighbors should rotation-invariant rules provide more information than more distant ones. royall first suggested using rules in which the labels yi are given unequal voting powers in the decision according to the distances of the xis from x the i nearest neighbor receives weight wni where usually wnl w whn and wni the rule is defined as wnjyixl wni iyixol gn otherwise. we get the ordinary k-nearest neighbor rule back by the choice wni otherwise. if i k k the following conditions for consistency were established by stone and lim max wni n--hx lin wni lim n-oo for some k with kin problem weighted versions of the recursive and scale-invariant methods described above can also be defined similarly. kin rotation-invariant rules assume that an affine transformation t is applied to x and xl xn any number of combinations of rotations translations and linear rescalings and that for any such linear transformation t gnx dn gntx d where d txr yr then we call gn rotation-invariant. rotation-invariance is indeed a very strong property. in r d in the context of k-nn estimates it suffices to be able to define a rotation-invariant distance measure. these are necessarily data-dependent. an example of this goes as follows. any collection of d points in general position defines a polyhedron in a hyperplane of rd. for points xij we denote this polyhedron by pul id. then we define the distance pxi x l isegmentxi.xintersectspil il ihil idl near points have few intersections. using p. in a k-nn rule with k and k n we expect weak universal consistency under an appropriate scheme of tie-breaking. the answer to this is left as an open problem for the scholars. consistency of the k-nearest neighbor rule figure rotation-invariant dis tancesfrom x. relabeling rules the i-nn rule appeals to the masses who crave simplicity and attracts the grammers who want to write short understandable code. nearest neighbors may be found efficiently if the data are preprocessed chapter for references. can we make the i-nn rule universally consistent as well? in this section we introduce a tool called relabeling which works as follows. assume that we have a tion rule dn x e n d n i where dn is the data yl yn. this rule will be called the ancestral rule. define the labels these are the decisions for the xis themselves obtained by mere resubstitution. in the relabeling method we apply the i-nn rule to the new data zl zn. if all goes well when the ancestral rule gn is universally consistent so should the relabeling rule. we will show this by example starting from a consistent k-nn rule as ancestral rule k kin unfortunately relabeling rules do not always inherit consistency from their ancestral rules so that a more general theorem is more difficult to obtain unless one adds in a lot of regularity conditions-this does not seem to be the right time for that sort of effort. to see that universal consistency of the ancestral rule does not imply consistency of the relabeling rule consider the following rule hn if x xi and x x j all j i otherwise where gn is a weakly universally consistent rule. it is easy to show problem that hn is universally consistent as well. changing a rule on a set of measure zero indeed does not affect ln. also if x xi is at an atom of the distribution of x we only change gn to yi if xi is the sole occurrence of that atom in the data. this has asymptotically no impact on ln. however if hn is used as an ancestral rule and x is nonatomic then hnxi dn yi for all i and therefore the relabeling rule is a i-nn rule based on the data yl yn. if l for the distribution of y then the relabeling rule has probability of error converging to one! for most nonpathological ancestral rules relabeling does indeed preserve versal consistency. we offer a prototype proof for the k-nn rule. relabeling rules theorem let gn be the k-nn rule in which tie-breaking is done by ization as in assume that k and k n that gn is weakly universally consistent. then the relabeling rule based upon gn is weakly sally consistent as well. proof. we verify the conditions of stones weak convergence theorem rem to keep things simple we assume that the distribution of x has a density so that distance ties happen with probability zero. in our case the weight wnix of theorem equals k iff xi is among the k nearest neighbors of xlx where xlx is the nearest neighbor of x. it is zero otherwise. condition is trivially satisfied since k for condition we note that if xix denotes the i-th nearest neighbor of x among xl x n then just note that and that the latter sphere contains k data points. but we already know from the proof of weak consistency of the k-nn rule that if k n then for all e p xii e finally we consider condition here we have arguing partially as in stone e ixisamongtheknnsofxcjxiin the roles of xi and x n n ek llixj is the nn of xi in xn.x-xd il jl x ix is among the k nns of xj in however by lemma n l il ixj is thenn of xi in yd consistency of the k-nearest neighbor rule also n l ix is among thek nns of xj in kyd jl therefore by a double application of lemma e t irx jong thok nn of xoxjj!x yj efx and condition is verified. problems and exercises problem show that the conditions k and k n are necessary for universal bounded lim inf n-hx eln l exhibit a second distribution such that if k n e consistency of the k-nearest neighbor rule. that is exhibit a distribution such that if k remains for all n and some e then lim infn- hx eln l problem let x be monoatomic with show that for e for some c where ln is the error probability of the k-nn rule with tie-breaking by indices. problem prove that the recursive nearest neighbor rule is universally consistent vided that limn-oo in and wise hint check the conditions of theorem problem prove that the nearest neighbor rule defined by any l p-distance measure p is universally consistent under the usual conditions on k. the l p between x y end is defined by yiip rip foro p ooandbysupi yi i for p where x xed hint check the conditions of theorem problem let o-x z xi zd px xi izi zl be a generalized distance between z and zi where x xl xn are as in the description of the scale-invariantk-nn rule px xi is the empirical distance defined there and z zi e are real numbers added to break ties at random. the sequence zl zn is i.i.d. uniform and is independent of the data dn. with the k-nn rule based on the artificial distances show that the rule is universally consistent by verifying the conditions of theorem when k and kin in particular show first that if z is uniform and independent of x y dn and zi zn and if wnix z is the weight of zi in this k-nn rule it is k iff zi is among the k nearest neighbors of z according to then e wnix zfxd efx for all nonnegative measurable f with efx problems and exercises if kin then for all a o. hint check the conditions of theorem problem the layered nearest neighbor rule partitions the space at x into rants. in each quadrant the outer-layer points are marked that is those xi for which the hyperrectangle defined by x and xi contains no other data point. then it takes a majority vote over the yi for the marked points. observe that this rule is scale-invariant. show that whenever x has nonatornic marginals avoid ties eln l in probability. i figure the layered nearest neighbor rule takes a majority vote over the marked points. xl i empty i hint it suffices to show that the number of marked points increases unboundedly in ability and that its proportion to unmarked points tends to zero in probability. problem prove weak universal consistency of the weighted nearest neighbor rule if the weights satisfy lim max wni n---oo lsisn and lim n---oo ksisn wni for some k with kin hint check the conditions of theorem problem if wnn is a probability vector then limn---oo lino wni for all if and only if there exists a sequence of integers k kn such that k on k and lik wni show this. conclude that the conditions of problem are equivalent to lim max wni n---oo lsisn lim wni for all o. n---oo ino problem verify the conditions of problems and for weight vectors of the form wni cn i i ci where a is a constant and cn is a normalizing constant. in particular check that they do not hold for a but that they do hold for a problem consider as weight vector. show that the conditions of problem hold if pn and npn iii i n w pn pny x pn-n consistency of the k-nearest neighbor rule problem let wni pz ipz n i n where z is a poisson random variable. show that there is no choice of an such that i n is a consistent weight sequence following the conditions of problem problem let wni pbinomialn pn i pnn-i. derive conditions on pn for this choice of weight sequence to be consistent in the sense of problem problem k-nn density estimation. we recall from problem that if the ditional densities exist then l density estimation leads to a consistent classification rule. consider now the k-nearest neighbor density estimate introduced by lofts gaarden and quesenberry c let xi x n be independent identically tributed random variables in r d by with common density the k-nn estimate of is defined fncx a sxlix- xckxlll k where xklcx is the k-th nearest neighbor of x among xl x n show that for every n f ifx fnxldx so that the density estimate is never consistent in l on the other hand according to problem the corresponding classification rule is consistent so read on. problem assume that the conditional densities fo and exist. then we can use a rule suggested by patrick and fischer let no iyio and n no be the number of zeros and ones in the training data. denote by the k-th nearest neighbor of x among the xs with yi define xiklcx similarly. if aca denotes the volume of a set a c rd then the rule is defined as if noa nda otherwise. this estimate is based on the k-nearest neighbor density estimate introduced by loftsgaarden and quesenberry their estimate of fo is then the rule gn can be re-written as if ponlon plnlrn otherwise where pon no nand pln nd n are the obvious estimates of the class probabilities. show that gn is weakly consistent if k and kin whenever the conditional densities exist. problem consider a weakly universally consistent rule gn and define the rule if x xi and x x j all j i otherwise. show that h n too is weakly universally consistent. note it is the atomic cor partially atomic distributions of x that make this exercise interesting. hint the next exercise may help. problems and exercises problem let x have an atomic distribution which puts probability pi at atom i. let xl xn be an u.d. sample drawn from this distribution. then show the following. pn en e s where e and n is the number of atoms number of different values in the data sequence. en in o. n i n almost surely. lix rli for all j sn pi almost surely. problem royalls rule. royall proposes the regression function estimate lnhnj lnx l yix n ii n n where leu is a smooth kernel-like function on with f ludu and h n is a smoothing factor. suggestions included leu d define that this function becomes negative. if otherwise. assume that hn nhn derive sufficient conditions on that guarantee the weak universal consistency of royalls rule. in particular insure that choice is weakly universally consistent. hint try adding an appropriate smoothness condition to l. problem let k be a kernel and let rnx denote the distance between x and its k-th nearest neighbor x among x i x n the discrimination rule that corresponds to a kernel-type nearest neighbor regression function estimate of mack is if otherwise. i ik rnx idea of replacing the smoothing factor in the kernel estimate by a local rank-based value such as rnx is due to breiman meisel and purcell for the kernel k iso.! this rule coincides with the k-nn rule. for regular kernels chapter show that the rule remains weakly universally consistent whenever k and kin by verifying stones conditions of theorem problem let be a weakly universally consistent sequence of classifiers. split the data sequence dn into two parts dm yi ym and tn- m xmi yn use gn-m and the second part tn m to relabel the first part i.e. define y gn-mxi tn- m for i m. prove that the i-nn rule based on the data y y is weakly universally consistent whenever m and n m hint use problem problem consider the k-nn rule with a fixed k as the ancestral rule and apply the rule using the relabeled data. investigate the convergence of eln. is the limit lknn or something else? vapnik-chervonenkis theory empirical error minimization in this chapter we select a decision rule from a class of rules with the help of training data. working formally let c be a class of functions nd one wishes to select a function from c with small error probability. assume that the training data dn n yn are given to pick one of the functions from c to be used as a classifier. perhaps the most natural way of selecting a function is to minimize the empirical error probability n ln l ilxi!yd n il over the class c. denote the empirically optimal rule by argmin ln thus is the classifier that according to the data d n best among the classifiers in c. this idea of minimizing the empirical risk in the construction of a rule was developed to great extent by vapnik and chervonenkis intuitively the selected classifier should be good in the sense that its true error probability l p yidn is expected to be close to the optimal error probability within the class. their difference is the quantity that primarily interests us in this chapter l inf l vapnik-chervonenkis theory the latter difference may be bounded in a distribution-free manner and a rate of convergence results that only depends on the structure of c. while this is very exciting we must add that l may be far away from the bayes error l note that l l inf l l l ec ec the size of c is a compromise when c is large inf ec l may be close to l but the former error the estimation error is probably large as well. if c is too small there is no hope to make the approximation error inf ec l l small. for example if c is the class of all decision functions then we can always find a classifier in c with zero empirical error but it may have arbitrary values outside of the points xl x n for example an empirically optimal classifier is if x xi i otherwise. this is clearly not what we are looking for. this phenomenon is called overjitting as the overly large class c overfits the data. we will give precise conditions on c that allow us to avoid this anomaly. the choice of c such that inf ec l is close to l has been the subject of various chapters on consistency-just assume that c is allowed to grow with n in some manner. here we take the point of view that c is fixed and that we have to live with the functions in c. the best we may then hope for is to minimize l inf ec l a typical situation is shown in figure picked rule best rule in class inf stimation error be controlled approximation error controllable larger than estimation error figure various errors in empirical classifier selection. empirical error minimization consider first a finite collection c and assume that one of the classifiers in c has zero error probability that is min ec l o. then clearly ln with probability one. we then have the following performance bound theorem and chervonenkis assume ici and min ec l o. then for every nand e and proof. clearly el log ici. n pl e p illax ecl l e i e l e e i ecl e l pln since the probability that no yi pair falls in the set y y is less than en if the probability of the set is larger than e. the probability inequality of the theorem follows from the simple inequality x e-x to bound the expected error probability note that for any u el i f pl tdt u pl tdt u ici. oo e- nt dt u-e ici n since u was arbitrary we may choose it to minimize the obtained upper bound. the optimal choice is u log icin which yields the desired inequality. theorem shows that empirical selection works very well if the sample size n is much larger than the logarithm of the size of the family c. unfortunately the vapnik-chervonenkis theory assumption on the distribution of y that is that min ec l is very restrictive. in the sequel we drop this assumption and deal with the free problem. one of our main tools is taken from lemma this leads to the study of uniform deviations of relative frequencies from their probabilities by the following simple observation let v be a probability measure of y on nd x i and let vn be the empirical measure based upon dn. that is for any fixed measurable set a c nd x i va px y e a and vna ixiyieaj. then l vx y i y is just the v-measure of the set of pairs y e n d x i where i y. formally l is the v-measure of the set i x u o x similarly ln vnx y i y. thus sup iln l sup ivna vai ec aea where a is the collection of all sets i x u o x e c. for a fixed set a for any probability measure v by the law of large numbers vna va almost surely as n moreover by hoeffdings inequality plvna e however it is a much harder problem to obtain such results for supaea ivna vai. if the class of sets a analogously in the pattern recognition context c is of finite cardinality then the union bound trivially gives however if a contains infinitely many sets in many of the interesting cases then the problem becomes nontrivial spawning a vast literature. the most erful weapons to attack these problems are distribution-free large deviation-type inequalities first proved by vapnik and chervonenkis in their pigneering work. however in some situations we can handle the problem in a much simpler way. we have already seen such an example in section fingering fingering recall that in section we studied a specific rule that selects a linear classifier by minimizing the empirical error. the performance bounds provided by theorems and show that the selected rule performs very closely to the best possible linear rule. these bounds apply only to the specific algorithm used to find the pirical minima-we have not showed that any classifier minimizing the empirical error performs well. this matter will be dealt with in later sections. in this section we extend theorems and to classes other than linear classifiers. let c be the class of classifiers assigning to those x contained in a closed perrectangle and to all other points. then a classifier minimizing the empirical error ln over all e c may be obtained by the following algorithm to each x of points from xl x n assign the smallest angle containing these points. if we assume that x has a density then the points x xn are in general position with probability one. this way we obtain at most sets. let i be the classifier corresponding to the i-th such gle that is the one assigning to those x contained in the hyperrectangle and to other points. clearly for each e c there exists a i i such that for all x j except possibly for those on the boundary of the hyperrectangle. since the points are in general position there are at most such exceptional points. therefore if we select a classifier among g to minimize the empirical error then it approximately minimizes the empirical error over the whole class c as well. a quick scan through the proof of theorem reveals that by similar arguments we may obtain the performance bound for n and e the idea may be generalized. it always works if for some k k-tuples of points determine classifiers from c such that no matter where the other data points fall the minimal empirical error over these sets coincides with the overall minimum. then we may fix these sets-put our finger on them-and look for the empirical minima over this finite collection. the next theorems whose proofs are left as an exercise show that if c has this property then works extremely well whenever n k. theorem assume that the class c of classifiers has the following property c such that for all for some integer k there exists a function xl xn e rd and all e c there exists a k-tuple ii ik e of different indices such that wxi xhx for all j n with j ii k vapnik-chervonenkis theory with probability one. let be found by fingering that is by empirical error mization over the collection of n! k! classifiers of the form ii ik e n different. then for n k and n e p inf l e k eepec moreover if n k then e inf l epec log n n the smallest k for which c has the property described in the theorem may be called the fingering dimension of c. in most interesting cases it is independent of n. problem offers a few such classes. we will see later in this chapter that the fingering dimension is closely related the so-called vc dimension of c also problem again we get much smaller errors if infepec l o. the next inequality generalizes theorem theorem assume that c has the property described in theorem with fingering dimension k. assume in addition that infepec l o. then for all n and e and e l k logn n-k remark. even though the results in the next few sections based on the chervonenkis theory supersede those of this section requiring less from the class c and being able to bound the error of any classifier minimizing the empirical risk we must remark that the exponents in the above probability inequalities are the best possible and bounds of the same type for the general case can only be obtained with significantly more effort. d the glivenko-cantelli theorem in the next two sections we prove the vapnik-chervonenkis inequality a powerful generalization of the classical glivenko-cantelli theorem. it provides upper bounds on random variables of the type sup ivna vai. aea the glivenko-cantelli theorem as we noted in section such bounds yield performance bounds for any classifier selected by minimizing the empirical error. to make the material more digestible we first present the main ideas in a simple one-dimensional setting and then prove the general theorem in the next section. we drop the pattern recognition setting momentarily and return to probability theory. the following theorem is sometimes referred to as the fundamental theorem of mathematical statistics stating uniform almost sure convergence of the empirical distribution function to the true one theorem theorem. let zl zn be i.i.d. valued random variables with distribution function fz pzl z. denote the standard empirical distribution function by then p ifz e zer and in particular by the borel-cantelli lemma lim sup ifz with probability one. n-foo zer proof. the proof presented here is not the simplest possible but it contains the main ideas leading to a powerful generalization. introduce the notation va pzl e a and vnca izjea for all measurable sets a c r. let a denote the class of sets of form z for z e r. with these notations sup ifz sup ivna vai. zer aea we prove the theorem in several steps following symmetrization ideas of dudley and pollard we assume that ne since otherwise the bound is trivial. in the first step we introduce a symmetrization. step first symmetrization by a ghost sample. define the random variables zi z e r such that zl zn zi are all independent and tically distributed. denote by v the empirical measure corresponding to the new sample then for ne we have p ivna e ivna va i aea aea vapnik-chervonenkis theory to see this let a e a be a set for which ivn v e if such a set exists and let a be a fixed set in a otherwise. then p ivna e aea p va e p va e iva va e ilvna-vaiep iv va j zl zn the conditional probability inside may be bounded by chebyshevs inequality as follows p ivna va zl zn ej f va va ne whenever ne in summary p ivna e aea plvna va e p ivna e aea step second symmetrization by random signs. let be i.i.d. sign variables independent of zl zn and z z with pi pi i clearly because zl zi zn z are all independent and identically distributed the distribution of is the same as the distribution of ituazi iazi sup it iazi aea il thus by step p ivna e aea ituazi aea n il iazi the glivenko-cantelli theorem simply applying the union bound we can remove the auxiliary random variables zi z step conditioning. to bound the probability we condition on z zn. fix z e n d and note that as z ranges over n the number of different vectors izn is at most n thus conditional on zl zn the supremum in the probability above is just a maximum taken over at most n random variables. thus applying the union bound gives with the supremum now outside the probability it suffices to find an exponential bound on the conditional probability step hoeffdinos inequality. with zl zn fixed ajazi is the sum of n independent zero mean random variables bounded between and therefore theorem applies in a straightforward manner thus p i i e i sup zl zn aea n il le vapnik-chervonenkis theory taking the expected value on both sides we have in summary p ivna e d aea uniform deviations of relative frequencies from probabilities in this section we prove the vapnik-chervonenkis inequality a mighty ization of theorem in the proof we need only a slight adjustment of the proof above. in the general setting let the independent identically distributed random variables zl zn take their values from rd. again we use the tation va pzi e a and vna ll izjea for all measurable sets a c rd. the vapnik-chervonenkis theory begins with the concepts of shatter coefficient and vapnik-chervonenkis vc dimension definition let a be a collection of measurable sets. for e let n azl zn be the number of different sets in zn n a a e a. the n-th shatter coefficient of a is that is the shatter coefficient is the maximal number of different subsets of n points that can be picked out by the class of sets a. the shatter coefficients measure the richness of the class a. clearly sea n as there are subsets of a set with n elements. if n azl zn for some zn then we say that a shatters zn. if sea n then any set of n points has a subset such that there is no set in a that contains exactly that subset of the n points. clearly if sea k for some integer k then sea n for all n k. the first time when this happens is important definition let a be a collection of sets with iai the largest integer k for which sea k is denoted by va and it is called the chervonenkis dimension vc dimension of the class a. if sea n for all n then by definition va uniform deviations of relative frequencies from probabilities for example if a contains allhalfiines of form x x e r thensa and va this is easily seen by observing that for any two different points zl there is no set of the form x that contains but not zl. a class of sets a for which va is called a vapnik-chervonenkis class. in a sense v a may be considered as the complexity or size of a. several properties of the shatter coefficients and the ve dimension will be shown in chapter the main purpose of this section is to prove the following important result by vapnik and chervonenkis theorem and chervonenkis for any probability measure v and class of sets a and for any nand e p ivna e aea proof. the proof parallels that of theorem we may again assume that in the first two steps we prove that this may be done exactly the same way as in theorem we do not repeat the argument. the only difference appears in step step conditioning. to bound the probability again we condition on z zn. fix z e r d and observe that as a ranges over a the number of different vectors iazn is just the number of different subsets of zn produced by intersecting it with sets in a which by definition cannot exceed sea n. therefore with zl zn fixed the supremum in the above probability is a maximum of at most n a zn random variables. this number by definition is bounded from above by sea n. by the union bound we get i therefore as before it suffices to bound the conditional probability vapnik-chervonenkis theory this may be done by hoeffdings inequality exactly as in step of the proof of theorem finally we obtain p ivna e s aea the bound of theorem is useful when the shatter coefficients do not increase too quickly with n. for example if a contains all borel sets of nd then we can shatter any collection of n different points at will and obtain sea n this would be useless of course. the smaller a the smaller the shatter coefficient is. to apply the vc bound it suffices to compute shatter coefficients for certain families of sets. examples may be found in cover vapnik and chervonenkis devroye and wagner feinholz devroye massart dudley simon and stengle and yukich this list of references is far from exhaustive. more information about shatter coefficients is given in chapter remark. measurability. the supremum in theorem is not always surable. measurability must be verified for every family a. for all our examples the quantities are indeed measurable. for more on the measurability question see dudley massart and gaenssler gine andzinn and yukich provide further work on suprema of the type shown in theorem remark. optimal exponent. for the sake of readability we followed the line of pollards proof instead of the original by vapnik and chervonenkis. in particular the exponent in theorem is worse than the established in the original paper. the best known exponents together with some other related results are mentioned in section the basic ideas of the original proof by vapnik and chervonenkis appear in the proof of theorem below. remark. necessary and sufficient conditions. the theorem that it can be strengthened to it is clear from the proof of p ivna e s azl zn e aea where z zn are i.i.d. random variables with probability measure v. although this upper bound is tighter than that in the stated inequality it is usually more difficult to handle since the coefficient in front of the exponential term depends on the distribution of zl while sea n is purely combinatorial in nature. however this form is important in a different setting we say that the uniform law of large numbers holds if sup ivna aea in probability. classifier selection it follows from this form of theorem that the uniform law of large numbers holds if e zn o. n vapnik and chervonenkis showed that this condition is also necessary for the uniform law of large numbers. another characterization of the uniform law of large numbers is given by talagrand who showed that the uniform law of large numbers holds if and only if there does not exist a set a c rd with va such that with probability one the set zn n a is shattered by a.d classifier selection the following theorem relates the results of the previous sections to empirical classifier selection that is when the empirical error probability ln is minimized over a class of classifiers c. we emphasize that unlike in section here we allow any classifier with the property that has minimal empirical error l n in c. first introduce the shatter coefficients and vc dimension of c definition let c be a class of decision functions of the form cj rd i. define a as the collection of all sets cjx i x u cjx o x cj e c. define the n-th shatter coefficient sc n of the class of classifiers cas furthermore define the vc dimension vc of c as sc n sea n. for the performance of the empirically selected decision cj we have the lowing theorem letc be a class of decision functions oftheformcj rd i. then using the notation lncj i and lcj pcjx y we have and therefore p inf lcj e ec where cj denotes the classifier minimizing ln over the class c. vapnik-chervonenkis theory proof. the statements are immediate consequences of theorem and lemma the next corollary an easy application of theorem problem makes things a little more transparent. corollary in the notation of theorem e n inf l epec n if s n increases polynomially with n then the average error probability of the selected classifier is within jlog n n of the error of the best rule in the class. we point out that this result is completely distribution-free. furthermore note the nonasymptotic nature of these inequalities they hold for every n. from here on the problem is purely combinatorial-one has to estimate the shatter coefficients. many properties are given in chapter in particular if vc then sc n n vc that is if the class c has finite vc dimension then sc n increases at a polynomial rate and e inf l epec vc logn remark. theorem provides a bound for the behavior of the empirically mal classifier. in practice finding an empirically optimal classifier is often tationally very expensive. in such cases the designer is often forced to put up with algorithms yielding suboptimal classifiers. assume for example that we have an algorithm which selects a classifier gn such that its empirical error is not too far from the optimum with large probability where and are sequences of positive numbers converging to zero. then it is easy to see problem that p inf l e iln l e en epec epec and theorem may be used to obtain bounds for the error probability of gn. interestingly the empirical error probability of the empirically optimal classifier is always close to its expected value as may be seen from the following example corollary let c be an arbitrary class of classification rules is tions of the form n d i. let e c be the rule that minimizes the number of errors committed on the training sequence dn among the classifiers in c. in other words sample complexity where ln n lil i thenfor every nand e n the corollary follows immediately from theorem by observing that ing the value of one yi pair in the training sequence results in a change of at most lin in the value of ln the corollary is true even if the vc dimension of c is infinite the result shows that ln is always very close to its expected value with large probability even if e is far from inf ec l also problem sample complexity in his theory of learning valiant rephrases the empirical classifier selection problem as follows. for e define an learning algorithm as a method that selects a classifier gn from c using the data dn such that for the selected rule sup p inf l e ec whenever n ne here ne is the sample complexity of the algorithm defined as the smallest integer with the above property. since the supremum is taken over all possible distributions of y the integer ne is the number of data pairs that guarantees e accuracy with confidence for any distribution. note that we use the notation gn and not i in the definition above as the definition does not force us to take the empirical risk minimizer i we may use theorem to get an upper bound on the sample complexity of the selection algorithm based on empirical error minimization the classifier corollary the sample complexity of the method based on empirical error minimization is bounded from above by ne max log e e e the corollary is a direct consequence of theorem the details are left as an exercise the constants may be improved by using refined versions of theorem e.g. theorem the sample size that guarantees the prescribed accuracy and confidence is proportional to the maximum of vc log vc and log vapnik-chervonenkis theory here we have our first practical interpretation of the vc dimension. doubling the vc dimension requires that we basically double the sample size to obtain the same accuracy and confidence. doubling the accuracy however forces us to quadruple the sample size. on the other hand the confidence level has little influence on the sample size as it is hidden behind a logarithmic term thanks to the exponential nature of the vapnik-chervonenkis inequality. the vapnik-chervonenkis bound and the sample complexity ne also allow us to compare different classes in a unified manner. for example if we pick by minimizing the empirical error over all hyperrectangles of n will we need a sample size that exceeds that of the rule that minimizes the empirical error over all linear halfspaces of with the sample complexity in hand it is just a matter of comparing vc dimensions. as a function of e the above bound grows as itis possible interestingly to get rid of the term at the expense of increasing the linearity in the vc dimension problem the zero-error case theorem is completely general as it applies to any class of classifiers and all distributions. in some cases however when we have some additional information about the distribution it is possible to obtain even better bounds. for example in the theory of concept learning one commonly assumes that l and that the bayes decision is contained in c e.g. valiant blumer ehrenfeucht haussler and warmuth natarajan the following theorem provides significant improvement. its various forms have been proved by devroye and wagner vapnik and blumer ehrenfeucht haussler and warmuth for a sharper result see problem theorem let c be a class of decision functions mapping n d to i and let i be a function in c that minimizes the empirical error based on the training sample dn. suppose that infjjec l i.e. the bayes decision is contained in c and l then to contrast this with theorem observe that the exponent in the upper bound for the empirically optimal rule is proportional to instead of to see the significance of this difference note that theorem implies that the error bility of the selected classifier is within n n of the optimal rule in the class equals zero in this case see problem as opposed to o ylognn from theorem we show in chapter that this is not a technical coincidence but since both bounds are essentially tight it is a mathematical witness to the fact that it is remarkably easier to select a good classifier when l the proof is the zero-error case based on the random permutation argument developed in the original proof of the vapnik inequality proof. for ne the inequality is clearly true. so we assume that ne first observe that since infqec l oln o with probability one. it is easily seen that l is sup il ln qlnq now we return to the notation of the previous sections that is zi denotes the pair yi v denotes its measure and vn is the empirical measure based on zl zn. also a consists of all sets of the form a y y for e c. with these notations sup qlnq il ln sup ivna vai. avnao step symmetrization by a ghost sample. the first step ofthe proof is similar to that of theorem introduce the auxiliary sample z z such that the random variables z zn zi z are i.i.d. and let v be the empirical measure for zi z. then for ne p vall e a vall the proof of this inequality parallels that of the corresponding one in the proof of theorem however an important difference is that the condition there is more restrictive than the condition ne here. the details of the proof are left to the reader problem step symmetrization by permuting. note that the distribution of i a.vna-o is the same as the distribution of n sup ivna n sup iazi l iaz sup t i t df a liiiazi-o n a where is an arbitrary permutation of the random variables zl zn zi z. the possible permutations are denoted by therefore p sup avllao ivna e i j h vapnik-chervonenkis theory i e-l jl sup i i is a j a step conditioning next we fix z zn z z and bound the value of the random variable above. let a c a be a collection of sets such that any two sets in a pick different subsets of zn z z and its cardinality is n azi zn z z that is all possible subsets are represented exactly once. then it suffices to take the supremum over a instead of a a ii iatijzd- i sup j iu iatijz aea aea step counting. clearly the expression behind the first summation sign is just the number of permutations of the points z z zn z with the property divided by the total number of permutations. observe that if l a iazd is the total number of points in a among zi z zn z for a fixed set a then the number of permutations such that is zero if l ne if l ne then the fraction of the number of permutations with the above property and the number of all permutations can not exceed to see this note that for the above product of indicators to be all the points falling in a have to be in the second half of the permuted sample. now clearly the zero-error case l summarizing we have p e! n sup a ii iaitjzml aea e zn z sea and the theorem is proved. again we can bound the sample complexity n restricted to the class of distributions with inf ec l o. just as theorem implies corollary theorem yields corollary the sample complexity ne that guarantees inf l e p sup l ec for n ne is bounded by ne max e e e a quick comparison with corollary shows that the factors in the inators there are now replaced by e. for the same accuracy much smaller samples suffice if we know that inf ec l o. interestingly the sample complexity is still roughly linear in the vc dimension. remark. we also note that under the condition of theorem p e zn z z vapnik-chervonenkis theory where the class a of sets is defined in the proof of theorem remark. as theorem shows the difference between the error probability of an empirically optimal classifier and that of the optimal in the class is much smaller if the latter quantity is known to be zero than if no restriction is imposed on the distribution. to bridge the gap between the two bounds one may put the restriction inicpec lt l on the distribution where l e is a fixed number. devroye and wagner and vapnik obtained such bounds. for example it follows from a result by vapnik that as expected the bound becomes smaller as l decreases. we face the same nomenon in chapter where lower bounds are obtained for the probability above. o extensions we mentioned earlier that the constant in the exponent in theorem can be improved at the expense of a more complicated argument. the best possible ponent appears in the following result whose proof is left as an exercise theorem p ivna e csa aea where the constant c does not exceed e even though the coefficient in front is larger than in theorem it becomes very quickly absorbed by the exponential term. we will see in chapter that for va sea n n va so sea this difference is negligible compared to the difference between the exponential terms even for moderately large values of both theorem and theorem imply that e ivna vai o ylognn aea extensions problem however it is possible to get rid of the logarithmic term to obtain fn. for example for the kolmogorov-srnirnov statistic we have the following result by dvoretzky kiefer and wolfowitz sharpened by massart theorem kiefer and wolfowitz massart using the notation of theorem we have for every nand e p ifz e zer for the general case we also have alexanders bound theorem for ne p sup ivna e aea v ne r e- ne the theorem implies the following corollary e sup ivna aea fn n the bound in theorem is theoretically interesting since it implies problem that for fixed v a the expected value of the supremum decreases as a fn instead of jlog n n. however a quick comparison reveals that alexanders bound is larger than that of theorem unless n an astronomically large value. recently talagrand obtained a very strong result. he proved that there exists a universal constant c such that for more information about these inequalities see also vapnik gaenssler gaenssler and stute and massart it is only natural to ask whether the uniform law of large numbers sup ivna aea holds if we allow a to be the class of all measurable subsets of nd. in this case the supremum above is called the total variation between the measures vn and v. the convergence clearly can not hold if vn is the standard empirical measure vapnik-chevonenkis theory but is there another empirical measure such that the convergence holds? the somewhat amusing answer is no. as devroye and gyorfi proved for any empirical measure vn-that is a function depending on zl zn assigning a nonnegative number to any measurable set-there exists a distribution of z such that for all n sup ivna aea almost surely. thus in this generality the problem is hopeless. for meaningful results either a or v must be restricted. for example if we assume that v is absolutely continuous with density f and that vn is absolutely continuous too density fn then by scheffes theorem rd sup ivna aea i fnx fx problem but as we see from problems and there exist density estimators as histogram and kernel estimates such that the l converges to zero almost surely for all possible densities. therefore the total variation between the empirical measures derived from these density estimates and the true measure converges to zero almost surely for all distributions with a density. for other large classes of distributions that can be estimated consistently in total variation we refer to barron gyorfi and van der meulen problems and exercises for all t and some c then problem prove that if a nonnegative random variable z satisfies pz t furthermore ez jloe. hint use the identity tdt and set iou luoo. bound the first integral by u and the second by the exponential inequality. find the value of u that minimizes the obtained upper bound. problem generalize the arguments of theorems and to prove theorems and problem determine the fingering dimension of classes of classifiers c ixeaa e a if the class ais the class of all closed intervals in n the class of all sets obtained as the union of m closed intervals in n the class of balls in n d centered at the origin the class of all balls in n d problems and exercises the class of sets of the form xd x x xd in n d and the class of all convex polygons in problem let c be a class of classifiers with fingering dimension k of n. show that vc k log k. problem prove that theorem implies corollary hint find ne such that vc whenever n n to see this first show that n vc is satisfied for n log which follows from the fact that log x x if x but in this case vc the upper bound does not exceed if n log problem let c be a class of classifiers n d and let be a fier minimizing the empirical error probability measured on dn. assume that we have an algorithm which selects a classifier gn such that where and are sequences of positive numbers converging to zero. show that p inf l e on iln l e en. ec ec find conditions on and so that elgn inf l ec ogn n that is elgn converges to the optimum at the same order as the error probability of problem prove that p sup avnao ivna e sup avnao ivna holds if ne this inequality is needed to complete the proof of theorem hint proceed as in the proof of theorem introduce a with and justify the validity of the steps of the following chain of inequalities p lfo ivna e ejp va i zi zn p e p e where bn e is a binomial random variable with parameters nand e. finish the proof by showing that the probability on the right-hand side is greater than or equal to if ne the slightly more restrictive condition ne this follows from chebyshevs inequality. vapq.ik-chervonenkis theory problem prove that theorem implies that if inf.ec then i n og we note here that haussler littlestone and warmuth demonstrated the existence of a classifier with when inf.ec o. hint use the identity fooo px tdt for nonnegative random variables x and employ the fact that ex sc n nvc theorem problem prove the following version of theorem let be a function that minimizes the empirical error over a class in c. assume that inf ec o. then p e anthony and biggs hint modify the proof of theorem by introducing a ghost sample z with size m be specified after optimization. only the first symmetrization step needs adjusting show that for any a e p sup va es i_mete p sup vna ae aiia-o e avla-o where vi is the empirical measure based on the ghost sample bernsteins theorem the rest of the proof is similar to that of theorem choose m n and a nn m. problem prove that alexanders bound implies that if a is a vapnik-chervonenkis class with vc dimension v va then e sup ivna aea v loge v r. v n hint justify the following steps if is a negative decreasing concave function then eii hint bound li by using its taylor series expansion. etdt f oo ii let b be fixed. then for u hint use the previous step. let x be a positive random variable for which px u au h e- u where a b c are positive constants. then if b e ex jblogb a hint use ex fooo px u and bound the probability either by one or by the bound of the previous step. problem use alexanders inequality to obtain the following sample size bound for empirical error minimization problems and exercises ne s max zlog e e for what values of e does this bound beat corollary problem let zi zn be i.i.d. random variables in rd with measure v and standard empirical measure vn let a be an arbitrary class of subsets of rd. show that as n if and only if sup ivna aea in probability sup ivna with probability one. aea hint use mcdiarmids inequality. problem prove scheffes theorem let p and v be absolute continuous ability measures on nd with densities i and g respectively. prove that sup f ix gxldx aea where a is the class of all borel-measurable sets. hint show that the supremum is achieved for the set ix gx. problem learning based on empirical covering. this problem demonstrates an alternative method of picking a classifier which works as well as empirical error tion. the method based on empirical covering of the class of classifiers was introduced by buescher and kumar the idea of covering the class goes back to vapnik see also benedek and itai kulkarni and dudley kulkarni richardson and zeitouni letc be a class ofclassifierscp rd i. thedatasetdn is split into two parts dm xi yi ym and tt xmi ymi yn where n m i. we use the first part dm to cover c as follows. define the random variable n as the number of different values the binary vector bmcp i cpxm takes as cp is varied over c. clearly n s sc m. take n classifiers from c such that all n possible values of the binary vector b m are represented exactly once. denote these classifiers by n. among these functions pick one that minimizes the empirical error on the second part of the data set tt denote the selected classifier by show that for every n m and e the difference between the error probability lpl p yidn and the minimal error probability in the class satisfies vapnik-chervonenkis theory and kumar for example by taking m we get e ljn f clogn inf lj c n where c is a universal constant. the fact that the number of samples m used for covering c is very small compared to n may make the algorithm computationally more attractive than the method of empirical error minimization. hint use the decomposition p n inf lj e bound the first term on the right-hand side by using lemma and hoeffdings inequality to bound the second term of the decomposition observe that inf l i i-i inf lj sup ilj l sup pjxi sup va aeavmao where a i ix i e c and vma lxea. bound the latter quantity by applying theorem to do this you will need to bound the shatter coefficients sea in chapter we introduce simple tools for this. for example it is easy to deduce from parts and of theorem that sea problem prove that for all e e p va e csa aea where c hint proceed as indicated by the following steps introduce an i.i.d. ghost sample zi z of size m n where zi is distributed as z denote the corresponding empirical measure by v. as in the proof of the first step of theorem prove that for a e e p ivna ae aea p ivna va e. e m aea introduce permutations it r itnm of the n m random variables as in step of the proof of theorem show that problems and exercises ii ll iatijczi- lrl jactijczici-ae show that for each a e a by using hoeffdings inequality for sampling without replacement from valued elements theorem choose a ine. combinatorial aspects of vapnik theory shatter coefficients and vc dimension in this section we list a few interesting properties of shatter coefficients sea n and of the vc dimension v a of a class of sets a. we begin with a property that makes things easier. in chapter we noted the importance of classes of the form a x u a c x a e a sets a are of the form i and the sets in a are sets of pairs y for which y. recall that if c is a class of classifiers rd i then by definition sc n sea n and vc va. the first result states that sc n sea n so it suffices to investigate properties of a a class of subsets of rd. theorem for every n we have sea n sea n and therefore va va. proof. let n be a positive integer. we show that for any n pairs from rd x i if n sets from a pick n different subsets of the n pairs then there are n corresponding sets in a that pick n different subsets of n points in r d and vice versa. fix n pairs note that since ordering does not matter we may arrange any n pairs in this manner. assume that for a certain set a e a the corresponding set a a x u ac x e a picks out the pairs that is the set of these pairs is the intersection of a and the n pairs. again we can assume without loss of generality that the pairs are ordered in this way. this means that a picks from the set combinatorial aspects of vapnik-chervonenkis theory the subset xmll and the two subsets uniquely determine each other. this proves sea n sea n. to prove the other direction notice that if a picks a subset of k points xl xb then the corresponding set a e a picks the pairs with the same indices from o. equality of the vc dimensions follows from the equality of the shatter coefficients. the following theorem attributed to vapnik and chervonenkis and sauer describes the relationship between the vc dimension and shatter cients of a class of sets. this is the most important tool for obtaining useful upper bounds on the shatter coefficients in terms of the vc dimension. theorem if a is a class of sets with vc dimension v a then for every n sea n t io proof. recall the definition of the shatter coefficients sea n max naxl xn where naxi ixi e all clearly it suffices to prove that for every xl x n but since n axi xn is just the shatter coefficient of the class of finite sets we need only to prove the theorem for finite sets. we assume without loss of generality that a is a class of subsets of xn with vc dimension va. note that in this case sea n iai. we prove the theorem by induction with respect to n and va. the statement is obviously true for n for any class with v a it is also true for any n if va since sea n for all n in this case. thus we assume va assume that the statement is true for all k n for all classes of subsets of xd with vc dimension not exceeding va and for n for all classes with vc dimension smaller than va. define the following two classes of subsets of xn a a e a and shatter coefficients and vc dimension note that both a and a contain subsets of xn-d. a contains all sets a that are members of a such that au is also in a but xn tj. a. then iai iai iai. to see this write a xn e a a e aru xn tj. a a e a u thus iai n ia xn e a a e ai ia xn tj. a a e ai ia xn e a a e ai ia xn tj. a a e ai iai iai iai-iai. since iai iai and a is a class of subsets of the induction hypothesis implies that iai sea n io next we show that va.s va which will imply by the induction hypothesis. to see this consider a set s c xn-d that is shattered by a. then s u is shattered by a. to prove this we have to show that any set s c sand s u is the intersection of s u and a set from a. since s is shattered by a if s c s then there exists a set a e a such that s s n a. but since by definition xn tj. a we must have s u n a and sf u u n since by the definition of a both a and au are in a we see that s u is indeed shattered by a. but any set that is shattered by a must have cardinality not exceeding va therefore lsi va l. but s was an arbitrary set shattered by a which means va va thus we have shown that va-l sea n iai iai iai l n l n va io io straightforward application of the identity cl shows that comhinatorial aspects of vapnik-chervonenkis theory theorem has some very surprising implications. for example it follows immediately from the binomial theorem that sea n va. this means that a shatter coefficient falls in one of two categories either sea n for all n or sea n lva which happens if the vc dimension of a is finite. we cannot have sea n for example. if va the upper bound in theorem decreases exponentially quickly with n. other sharper bounds are given below. theorem for all n v sea n l va en va io l va theorem follows from theorem below. we leave the details as an exercise problem theorem for all n and va n h sea n en n where hx logx x logo xfor x e and ho ho is the binary entropy function. theorem is a consequence of theorem and the inequality below. a different probabilistic proof is sketched in problem also problem lemma for k n proof. introduce. kin by the binomial theorem e t io l the desired inequality. shatter coefficients of some classes remark. the binary entropy function hx plays a central role in information theory e.g. csiszar and korner cover and thomas its main properties are the following hx is symmetric around where it takes its maximum. it is continuous concave strictly monotone increasing in decreasing in and equals zero for x and x next we present some simple results about shatter coefficients of classes that are obtained by combinations of classes of sets. theorem if a al u a then sea n sai n n. given a class a define ac c for a n a a e a a e a sea n sea nsa n. for a a a e a a e a sea n sea nsa n. for a x a a e a a e a sea n sea nsa n. a e a. then sac n sea n. proof. and are trivial. to prove fix n points xl x n and assume that a picks n sea n subsets c i cn. then a picks from c at most sea icil subsets. therefore sets of the form ana pick at most n l sea i ci i sea n n il subsets. here we used the obvious monotonicity property sea n sea n m. to prove observe that a u a a e a a e a n a c c a e a a e a the statement now follows from and shatter coefficients of some classes here we calculate shatter coefficients of some simple but important examples of classes of subsets of nd. we begin with a simple observation. theorem if a contains finitely many sets then va sea n iai for every n. lai and proof. the first inequality follows from the fact that at least sets are necessary to shatter n points. the second inequality is trivial. combinatorial aspects of vapnik theory in the next example it is interesting to observe that the bound of theorem is tight. theorem if a is the class of all half lines a x x e r then va and sa n n if a is the class of all intervals in r then va and sea n nn proof. is easy. to see that va in observe that if we fix three different points in r then there is no interval that does not contain the middle point but does contain the other two. the shatter coefficient can be calculated by counting that there are at most n k sets in n x n a e a such that ia n k for k n and one set such that ia n xn i this gives altogether o l..-n k kl nn now we can generalize the result above for classes of intervals and rectangles in rd theorem if a xd x x xd then va d. if a is the class of all rectangles in r d then va proof. we prove the first part is left as an exercise we have to show that there are points that can be shattered by a but for any set of points there is a subset of it that can not be picked by sets in a. to see the first part just consider the following points figure shatter coefficients of some classes figure rectangles shatter points in gj on the other hand for any given set of points we can choose a subset of at most points with the property that it contains a point with largest first coordinate a point with smallest first coordinate a point with largest second coordinate and so forth. clearly there is no set in a that contains these points but does not contain the others d max y figure no points can be shattered by rectangles in minx o z max x miny theorem dudley let be afinite-dimensional vector space of real functions on nd. the class of sets a gx o g e g has vc dimension va r where r dimensiong. proof. it suffices to show that no set of size m r can be shattered by sets of the form gx o. fix m arbitrary points xl x m and define the linear mapping l nm as combinatorial aspects of vapnik theory then the image of q lq is a linear subspace of nm of dimension not ceeding the dimension of q that is m then there exists a nonzero vector y ym e n m that is orthogonal to lq that is for every g e q we can assume that at least one of the yi is negative. rearrange this equality so that terms with nonnegative yi stay on the left-hand side l yigxi l iyio iyio now suppose that there exists age q such that the set gx o picks exactly the xis on the left-hand side. then all terms on the left-hand side are nonnegative while the terms on the right-hand side must be negative which is a contradiction so xl xm cannot be shattered and the proof is completed. remark. theorem implies that the shatter coefficients of the class of sets in theorem are bounded as follows sea n t io in many cases it is possible to get sharper estimates. let be the linear space of functions spanned by some fixed functions r n d n and define orx cover showed that if for some xl e nd every r-element subset of is linearly independent then the n-th shatter coefficient of the class of sets a gx o g e q actually equals sea n l n io problem by using the last identity in the proof of theorem it is easily seen that the difference between the bound obtained from theorem and the true value is using covers result for the shatter coefficients we can actually improve theorem in that the vc dimension of the class a equals r. to see this note that shatter coefficients of some classes while sea r t io l io l r therefore no r points are shattered. it is interesting that theorem above combined with theorem and theorem gives the bound san s nr when r covers result however improves it to sea n s perhaps the most important class of sets is the class of halfspaces in rd that is sets containing points falling on one side of a hyperplane. the shatter coefficients of this class can be obtained from the results above corollary let a be the class of half spaces that is subsets ofrd of the form ax b where a e rd be rtakeallpossiblevalues. then va and proof. this is an immediate consequence of the remark above if we take to be the linear space spanned by the functions xl xed and where xl xed denote the d components of the vector x. it is equally simple now to obtain an upper bound on the vc dimension of the class of all closed balls in rd cover devroye or dudley for more information. corollary let a be the class of all closed balls in r d that is subsets of rd of the form where ai ad b e r take all possible values. then va s d proof. if we write d l il ixi ail d b l il d d lxiai lal- b il combinatorial aspects of vapnik-chervonenkis theory then it is clear that theorem yields the result by setting to be the linear space spanned by d l ixui z xl dlx xed and dzx il it follows from theorems and that the class of all polytopes with a bounded number of faces has finite vc dimension. the next negative example demonstrates that this boundedness is necessary. theorem if a is the class of all convex polygons in r z then va figure any subset of n points on the unit circle can be picked by a convex polygon. proof. let xl e lie on the unit circle. then it is easy to see that for any subset of these points there is a polygon that picks that subset. linear and generalized linear discrimination rules recall from chapter that a linear classification rule classifies x into one of the two classes according to whether d ao laixi il is positive or negative where xci denote the components of x e rd. the coefficients ai are determined by the training sequence. these decisions chotomize the space r d by virtue of a halfspace and assign class to one halfspace and class to the other. points on the border are treated as belonging to class o. consider a classifier that adjusts the coefficients by minimizing the number of linear and generalized linear discrimination rules errors committed on dn. in the terminology of chapter c is the collection of all linear classifiers. o figure an empirically mal linear classifier. o x xs o decide class o decide class xii glick pointed out that for the error probability l l of this classifier l l inf ec l almost surely. however from theorems and corollary we can now provide more details theorem for all nand e the error probability l of the empirically optimal linear classifier satisfies p n inf l e ec comparing the above inequality with theorem note that there we selected by a specific algorithm while this result holds for any linear classifier whose empirical error is minimal. generalized linear classification rules duda and hart are defined by where d is a positive integer the functions d are fixed and the ficients ao are functions of the data dn. these include for example all quadratic discrimination rules in n d when we choose all functions that are either components of x or squares of components of x or products of two components of x. that is the functions oi are of the form either xj or xj xk. in all d dd the argument used for linear discriminants remain valid and we obtain theorem let c be the class of generalized linear discriminants the coefficients vary the basis functions oi are fixed. for the error probability l l of the empirically optimal classifier for all d and e we have combinatorial aspects of vapnik theory also for n the second inequality is obtained by using the bound of theorem for the shatter coefficients. note nevertheless that unless d therefore c is allowed to increase with n there is no hope of obtaining universal consistency. the question of universal consistency will be addressed in chapters and convex sets and monotone layers classes of infinite vc dimension are not hopeless by any means. in this section we offer examples that will show how they may be useful in pattern recognition. the classes of interest to us for now are c all convex sets of n c all monotone layers of n i.e. all sets of the form for some nonincreasing function in discrimination this corresponds to making decisions of the form ixec c e c or c e c and similarly for c. decisions of these forms are important in many situations. for example if is monotone decreasing in both components of x e then the bayes rule is of the form gx hxel for some l e c. we have pointed out elsewhere and problem that vc v.c ex. to see this note that any set of n points on the unit circle is shattered by c while any set of n points on the antidiagonal is shattered by c. nevertheless shattering becomes unlikely if x has a density. our starting point here is the bound obtained while proving theorem where nax xnis the number of sets in n xn a e a. the following theorem is essential theorem if x has a density f on n then when a is either c or this theorem a proof of which is a must for the reader implies the following convex sets and monotone layers corollary let x have a density f on let be picked by minimizing the empirical error over all classifiers of the form if x e a if x tf a where a or a c is in c then l inf l for c or cc in c with probability one similarly for proof. this follows from the inequality of lemma theorem and the borel-cantelli lemma. d remark. theorem and the corollary may be extended to n d but this eralization holds nothing new and will only result in tedious notations. d proof of theorem we show the theorem for and indicate the proof for c. take two sequences of integers m and r where m and r m so that m yet as n consider the set and partition each side into m equal intervals thus obtaining an m x m grid of square cells cl cm denote co let no nl be the number of points among xl xn that belong to these cells. the vector n l n is clearly multinomially distributed. let be a nonincreasing function n n defining a set in by l let be the collection of all cell sets cut by that is all cells with a nonempty intersection with both land l c. the collection c is shaded in figure figure a monotone layer and bordering cells. combinatorial aspects of vapnik-chervonenkis theory we bound n dx xn from above conservatively as follows the number of different collections c cannot exceed because each cell in c may be obtained from its predecessor cell by either moving right on the same row or moving down one cell in the same column. for a particular collection denoting pi px e c we have pi po pi po n ci ci applying lemma exp co p po exp c en sets a witufa f l f a u ci s t. ci because i m and r and by the absolute continuity of x. as this estimate is uniform over all collections c we see that the expected value of is not more than eon eon. the argument for the collection c of convex sets is analogous. remark. the theorem implies that if a is the class of all convex sets then with probability one whenever fl has a density. this supaea iflna is a special case of a result of ranga rao assume now that the density f of x is bounded and of bounded support. then in the proof above we may take r fixed so that contains the support of i. then the estimate of theorem is e ai n x denotes the bound on i problems and exercises ii flloo we take m for a constant a. this implies by corollary that e inf rjlc for coree in c l this latter inequality was proved by steele to see that it cannot be extended to arbitrary densities observe that the data points falling on the convex hull upper layer of the points x i x n can always be shattered by convex sets monotone layers respectively. thus nax i xn is at least where mn is the number of points among xl x n falling on the convex hull upper layer of x i x n thus but it follows from results of carnal and devroye that for each a there exists a density such that emn hmsup n-oo na the important point of this interlude is that with infinite vc dimension we may under some circumstances get expected error rates that are but larger than however the bounds are sometimes rather loose. the reason is the looseness of the vapnik inequality when the collections a become very big. to get such results for classes with infinite vc dimension it is necessary to impose some conditions on the distribution. we will prove this in chapter problems and exercises problem show that the inequality of theorem is tight that is exhibit a class a of sets such that for each n sea n g. problem show that for all n and that combinatorial aspects of vapnik theory if va hint there are several ways to prove the statements. one can proceed rectly by using the recurrence l lfo el lfo-l e a simpler way to prove g va is by using theorem the third inequality is an immediate consequence of the first two. problem give an alternative proof of lemma by completing the following probabilistic argument. observe that for k n k where bn is a binomial random variable with parameters nand then chernoffs bounding technique the proof of theorem may be used to bound this probability for all s pbn k e-ske sell exp s take the derivative of the exponent with respect to s to minimize the upper bound. substitute the obtained value into the bound to get the desired inequality. problem let b be a binomial random variable with parameters nand p. prove that for k np pb k exp n p. hint use chernoffs bounding technique as in the previous problem. problem prove part of theorem problem prove that for the class of sets defined in the remark following theorem sea n l n io l hint proceed by induction with respect to nand r. in particular show that the recurrence scar n scan n sar- n holds where ar denotes the class of sets defined as gx o where g runs through a vector space spanned by the first r of the sequence of functions problem let a and b be two families of subsets of nd assume that for some r sea n nrsb n for all n show that if vb then va r log r hint by theorem sea n n vsr-l. clearly va is not larger than any k for which kvsr-l problem determine the vc dimension of the class of subsets of the real line such that each set in the class can be written as a union of k intervals. problems and exercises problem determine the vc dimension of the collection of all polygons with k vertices in the plane. problem what is the vc dimension of the collection of all ellipsoids ofnd problem determine the vc dimension of the collection of all subsets of kd where k and d are fixed. how does the answer change if we restrict the subsets to those of cardinality l kd? problem let a consist of all simplices ofnd that is all sets of the form where xl are fixed points of nd. determine the vc dimension of a. problem let axi be the set of all x en that are of the form where xl xk are fixed numbers and are fixed functions on the integers. let a xl xk en. determine the vc dimension of a. problem in some sense vc dimension measures the of a class of sets. ever it has little to do with cardinalities as this exercise demonstrates. exhibit a class of subsets of the integers with uncountably many sets yet vc dimension property was pointed out to us by andras farago. note on the other hand the class of all subsets of integers cannot be written as a countable union of classes with finite vc dimension rem hint find a class of subsets of the reals with the desired properties and make a proper correspondence between sets of integers and sets in the class. problem show that if a class of sets a is linearly ordered by inclusion that is for any pair of sets a b e a either a c b or b c a and iai then va conversely assume that va and for every set b with i b i b e n b a e a. prove that then a is linearly ordered by inclusion problem we say that four sets a b c d form a diamond if a c b c c a c dec but b ct d and d ct c. let a be a class of sets. show that va if and only if for some set r the class n r a e a includes a diamond problem let a be a class of sets and define its density by da inf sup sea n oo nl nr verify the following properties das va for each positive integer k there exists a class a of sets such that va k yet da and combinatorial aspects of vapnik-chervonenkis theory da if and only if va problem continued. let a and ai be classes of sets and define b a u a. show that db max da vbs va va and for every pair of positive integers k m there exist classes a and ai such that va va and vb problem a set a c n d is called a monotone layer if x e a implies that yea for all y x each component of x is not larger than the corresponding component of y. show that the class of all monotone layers has infinite vc dimension. problem letx lsuchthaty ixea where a is a convex set. let dn xl yd yn be an i.i.d. training sequence and consider the classifier if x is in the convex hull of the xis with y otherwise. find a distribution for which gil is not consistent and find conditions for consistency. hint recall theorem and its proof. lower bounds for empirical classifier selection in chapter a classifier was selected by minimizing the empirical error over a class of classifiers c. with the help of the vapnik theory we have been able to obtain distribution-free performance guarantees for the selected rule. for example it was shown that the difference between the expected error bility of the selected rule and the best error probability in the class behaves at least as well as o lognn where vc is the vapnik-chervonenkis dimension of c and n is the size of the training data dn. upper bound is obtained from theorem corollary may be used to replace the log n term with log vc. two questions arise immediately are these upper bounds least up to the order of magnitude tight? is there a much better way of selecting a classifier than mizing the empirical error? this chapter attempts to answer these questions. as it turns out the answer is essentially affirmative for the first question and negative for the second. these questions were also asked in the learning theory setup where it is usually assumed that the error probability of the best classifier in the class is zero blumer ehrenfeucht haussler and warmuth haussler littlestone and warmuth and ehrenfeucht haussler kearns and valiant in this case as the bound of theorem implies the error of the rule selected by minimizing the empirical error is within log n n of that of the best in the class equals zero by assumption. we will see that essentially there is no way to beat this upper bound either. loer bounds for empirical classifier selection minimax lower bounds let us formulate exactly what we are interested in. let c be a class of decision functions nd i. the training sequence dn yi yn is used to select the classifier gnx gnx dn from c where the selection is based on the data dn. we emphasize here that gn can be an arbitrary function of the data we do not restrict our attention to empirical error minimization where gn is a classifier in c that minimizes the number errors committed on the data dn. as before we measure the performance of the selected classifier by the ference between the error probability lgn pgnx i yidn of the selected classifier and that of the best in the class. to save space further on denote this optimum by ijec in particular we seek lower bounds for lc df inf p i y. and supplgn lc e supelgn lc where the supremum is taken over all possible distributions of the pair y. a lower bound for one of these quantities means that no matter what our method of picking a rule from c is we may face a distribution such that our method performs worse than the bound. this view may be criticized as too pessimistic. however it is clearly a perfectly meaningful question to pursue as typically we have no other information available than the training data so we have to be prepared for the worst situation. actually we investigate a stronger problem in that the supremum is taken over all distributions with lc kept at a fixed value between zero and we will see that the bounds depend on n vc and lc jointly. as it turns out the situations for lc and lc are quite different. because of its relative simplicity we first treat the case lc o. all the proofs are based on a technique called probabilistic method. the basic idea here is that the existence of a distribution is proved by considering a large class of distributions and bounding the average behavior over the class. lower bounds on the probabilities plgn lc e may be translated into lower bounds on the sample complexity ne we obtain lower bounds for the size ofthe training sequence such that for any classifier plgn lc e cannot be smaller than for all distributions if n is smaller than this bound. the case lc in this section we obtain lower bounds under the assumption that the best classifier in the class has zero error probability. in view of theorem we see that the the case lc situation here is different from when lc there exist methods of picking a classifier from c minimization of the empirical error such that the error probability decreases to zero at a rate of log n in. we obtain minimax lower bounds close to the upper bounds obtained for empirical error minimization. for example theorem shows that if lc then the expected error probability cannot decrease faster than a sequence proportional to vc i n for some distributions. theorem and chervonenkis haussler littlestone and warmuth let c be a class of discrimination functions with vc mension v. let x be the set of all random variables y for which lc then for every discrimination rule gn based upon xl yi x n yn and n v sup eln v n proof. the idea is to construct a family f of v-i distributions within the butions with lc as follows first find points x i xv that are shattered by c. each distribution in f is concentrated on the set of these points. a member in f is described by v bits bi bv for convenience this is represented as a bit vector b. assume v n. for a particular bit vector we let x xi v with probability lin each while x xv with probability ln. then set y fbx where fb is defined as follows j hx if x xv. if x xi i v note that since y is a function of x we must have l also lc as the set xv is shattered by c i.e. there is age c with gxi fbxi for i v. clearly sup el n lc sup el n lc supeln lc b el n lc b is replaced by b uniformly distributed over v-i el n pgnx xl yi x n yn fbx the last probability may be viewed as the error probability of the decision function gn n d x x ln in predicting the value of the random variable fbx based on the observation zn xl yi x n yn. naturally this probability is bounded from below by the bayes probability of error l fbx inf pgnzn fbx gil lm.yer bounds for empirical classifier selection corresponding to the decision problem fbx by the results of chapter where pfbx lizn. observe that z n or otherwise. if x xl x xn x xv thus we see that sup el n lc l fbx p xl x x n x xv v-i pix pix il v-i v-i n lie. this concludes the proof. minimax lower bounds on the probability pln e can also be obtained. these bounds have evolved through several papers see ehrenfeucht haussler kearns and valiant and blumer ehrenfeucht haussler and warmuth the tightest bounds we are aware of thus far are given by the next theorem. theorem and lugosi let c be a class of discrimination functions with vc dimension v let x be the set of all random variables yfor which lc o. assume e assume n v-i. then for every discrimination rule gn based upon xl yi x n yn sup pln e e-jrrv v-i if on the other hand n and n then sup pln e proof. we randomize as in the proof of theorem the difference now is that we pick xi with probability peach. thuspx pv we inherit the notation from the proof of theorem for a fixed b denote the error probability by the case lc we now randomize and replace b by b. clearly sup pln e sup plnb e b e eib plnb e. as in the proof of theorem observe that lnb cannot be smaller than the bayes risk corresponding to the decision problem ibex where thus lnb e i xn. as in the proof of theorem we see that e i xn f x i. x f xn x f xvix i xn v-i p l ii for fixed xl xn we denote by the collection s j s v-i x j this is the collection of empty cells xi. we summarize we consider two choices for p choice a. take p lin and assume s v-i e note that for n eill pn also since sill s v-i we have var iii s by the chebyshev-cantelli inequality plll plll plll plll eill s lowr bounds for empirical classifier selection var iii this proves the second inequality for supplil e. choice b. assume that e by the pigeonhole principle iii p if the number of points xi i n that are not equal to xv does not exceed v-i p. therefore we have a further lower bound plll pbinomialn v-i define v rv take p by assumption n then the lower bound is pbinomialn v v v v ek by stirlings formula v v v v lv n v x x v n this concludes the proof. classes with infinite vc dimension the results presented in the previous section may also be applied to classes with infinite vc dimension. for example it is not hard to derive the following result from theorem theorem assume that vc for every n and classification rule gn there is a distribution with lc such that the case lc elgn j. for the proof see problem thus when vc distribution-free nontrivial performance guarantees for lg n lc do not exist. this generalizes theorem where a similar result is shown if c is the class of all measurable discrimination functions. we have also seen in theorem that if c is the class of all measurable classifiers then no universal rate of convergence exists. however we will see in chapter that for some classes with infinite vc dimension it is possible to find a classification rule such that lgn l cjlog nn for any distribution such that the bayes classifier is in c. the constant c however necessarily depends on the distribution as is apparent from theorem infinite vc dimension means that the class c shatters finite sets of any size. on the other hand if c shatters infinitely many points then no universal rate of convergence exists. this may be seen by an appropriate modification of theorem as follows. see problem for the proof. theorem let be a sequence of positive numbers converging to zero with al let c be a class of classifiers with the property that there exists a set a c nd of infinite cardinality such that for any subset b of a there exists e c such that if x e band if x e a-b. then for every sequence of classification rules there exists a distribution of y with le such that for all n. note that the basic difference between this result and all others in this chapter is that in theorem the distribution does not vary with n. this theorem shows that selecting a classifier from a class shattering infinitely many points is essentially as hard as selecting one from the class of all classifiers. the case lc in the more general case when the best decision in the class c has positive error probability the upper bounds derived in chapter for the expected error bility of the classifier obtained by minimizing the empirical risk are much larger than when le o. in this section we show that these upper bounds are necessarily large and they may be tight for some distributions. moreover there is no other classifier that performs significantly better than empirical error minimization. theorem below gives a lower bound for supexylc fixed elgn le. as a function of nand vc the bound decreases basically as in the upper bound tained from theorem interestingly the lower bound becomes smaller as le lower bounds for empirical classifier selection decreases as should be expected. the bound is largest when lc is close to the constants in the bound may be tightened at the expense of more complicated expressions. the theorem is essentially due to devroye and lugosi though the proof given here is different. similar bounds-without making the dependence on lc explicit-have been proved by vapnik and chervonenkis and simon theorem let c be a class of discrimination functions with vc dimension let x be the set of all random variables y for which for fixed v l e l inf pgx i y gee then for every discrimination rule gn based upon xl x n yn sup el n l e- ifn il proof. again we consider the finite family f from the previous section. the notation band b is also as above. x now puts mass p at xi i v and mass lp which will be satisfied. next introduce the constant c e we no longer have y as a function of x. instead we have a uniform random variable u independent of x and define lp at xv. this imposes the condition y if u c x xi i v otherwise. thus when x xi i v y is with probability cor c. a simple argument shows that the best rule for b is the one which sets f if x i v bi otherwise. b also observe that noting that we may write l c c for i v for fixed b by the equality in theorem v-i ln l l fbxdl il it is sometimes convenient to make the dependence of gn upon b explicit by sidering gnxi as a function of xi xl x n u un i.i.d. sequence of uniform random variables and bi the proof given here is based on the case lc the ideas used in the proofs of theorems and we replace b by a formly distributed random b over i v-i. after this randomization denote zn xl yi x n yn. thus sup eln l supeln l b v-i ii random b eln l l j ibex ibex ibex where as before l ibex denotes the bayes probability of error of dicting the value of ibex based on observing zn all we have to do is to find a suitable lower bound for where pibx lizn. observe that next we compute pbi llyij yi yik yk for yi yk e i. denoting the numbers of zeros and ones by ko i s k y j o i and k i s k yj we see that pbi llyij yi yik yd therefore if x xij x ik xi i v then min min kj lower bounds for empirical classifier selection in summary denoting a we have l ibex e e i px xde il jensens inequality. lpa next we bound e ljxjxi clearly if bk q denotes a binomial random variable with parameters k and q e gpkl- pn-ke c klj however by straightforward calculation we see that c kl je c jkl therefore applying jensens inequality once again we get t pt-ke c kl ko k summarizing what we have obtained so far we have supeln l b ibex p p cv the inequality x ex cv a rough asymptotic analysis shows that the best asymptotic choice for c is given by c the case lc then the constraint l p c leaves us with a quadratic equation in c. instead of solving this equation it is more convenient to take c j with this choiceforc using l straightforward calculation provides sup el n l jv ll the condition p v-i implies that we need to ask that n this concludes the proof of theorem d next we obtain a probabilistic bound. its proof below is based upon hellinger distances and its methodology is essentially due to assouad for ments and applications we refer to birge and devroye theorem and lugosi let c be a class of discrimination functions with vc dimension v let x be the set of all random variables y for which for fixed l e l inf pgx y gee then for every discrimination rule gn based upon xl x n yn and any e l sup pln proof. the method of randomization here is similar to that in the proof of theorem using the same notation as there it is clear that sup pln l e ei i fbxie exlxvxoi first observe that if then lower bounds for empirical classifier selection where be denotes the binary vector h bv that is the complement of b. therefore for e pcv the last expression in the lower bound above is bounded from below by lecams inequality lemma l l j pbx ypbcx y b it is easy to see that for x xv pbxypbcxy and for x xi i v pbx ypbcx y p c thus we have the equality summarizing since l pv c we have sup pln l e c exp where we used the inequality x e-xl-x again. we may choose c as it is easy to verify that condition holds. also pv from the condition l e we deduce that c the exponent in the expression above may be bounded as sample complexity substituting c e thus as desired. sup pln l e exp l sample complexity we may rephrase the probability bounds above in terms of the sample complexity of algorithms for selecting a classifier from a class. recall that for given e the sample complexity of a selection rule gn is the smallest integer ne such that sup plgn lc e for all n ne the supremum is taken over a class of distributions of y. here we are interested in distributions such that lc is fixed. we start with the case lc by checking the implications of theorem for the sample complexity n first blumer ehrenfeucht haussler and warmuth showed that for any algorithm neocgioggvc where c is a universal constant. in ehrenfeucht haussler kearns and valiant the lower bound was partially improved to vc and it may be combined with the previous bound. when e theorem provides the following bounds corollary let c be a class of discrimination functions with vc dimension v let x be the set of all random variables yfor which lc assume lower bounds for empirical classifier selection e and denote v rev thenfor every discrimination rule gn based upon xl f l x n fn when e and then n. log g finallyjor and e v-i ne proof. the second bound follows trivially from the second inequality of theorem by the first inequality there sup pln e v-i v v n e ls we assume log g v yv the function nv e- sile varies unimodally in n and achieves a peak at n v ise. for n below this threshold by monotonicity we apply the bound at n vise. it is easy to verify that the value of the bound at v is always at least if on the other hand n v the lower bound achieves its minimal value at ise and the value there is this proves the first bound. corollary shows that for any classifier at least vc max se log training samples are necessary to achieve e accuracy with confidence for all distributions. apart from a log factor the order of magnitude of this expression is the same as that of the upper bound for empirical error minimization obtained in corollary that the upper and lower bounds are very close has two important messages. on the one hand it gives a very good estimate for the number of training samples needed to achieve a certain performance. on the other hand it shows that there is essentially no better method than minimizing the empirical error probability. in the case lc we may derive lower bounds for ne from theorems and problems and exercises corollary let be a class of discrimination functions with vc dimension v let x be the set of all random variables y for which for fixed l e l infpgxly gee then for every discrimination rule gn based upon xl x n yn lv le- lo also and in particular for e l xmin l ne log proof. the first bound may be obtained easily from the expectation-bound of theorem problem setting the bound of theorem equal to provides the second bound on n these bounds may of course be combined. they show that n is bounded from below by terms like of vc and as i is typically much smaller than e. by comparing these bounds with the upper bounds of corollary we see that the only difference between the orders of magnitude is a log ie so all remarks made for the case le remain valid. interestingly all bounds depend on the class c only through its vc dimension. this fact suggests that when studying distribution-free properties of lgn le the vc dimension is the most important characteristic of the class. also all bounds are linear in the vc dimension which links it conveniently to sample size. remark. it is easy to see from the proofs that all results remain valid if we allow randomization in the rules gn. problems and exercises problem show that theorem implies that for every discrimination rule gn based upon d lv le- io ne x mill hint assume that pln l e then clearly eln l e problem prove theorem first apply the proof method of theorem to show that for every n andg there is a distribution with lc such thatelgn use a monotonicity argument to finish the proof. problem prove theorem by modifying the proof of theorem the maximum likelihood principle in this chapter we explore the various uses of the maximum likelihood principle in discrimination. in general the principle is only applicable if we have some a priori knowledge of the problem at hand. we offer definitions consistency results and examples that highlight the advantages and shortcomings. maximum likelihood the formats sometimes advance information takes a very specific form y x is normal a often it is rather vague believe that x has a density or is thought to be a monotone function of x e r. if we have information in set format the maximum likelihood principle is less appropriate. here we know that the bayes rule gx is of the form gx ixea where a e a and a is a class of sets of rd. we refer to the chapters on empirical risk minimization chapter and also chapter for this situation. if we know that the true belongs to a class of functions that map n d to then we say that we are given information in regressionformat. with each e we associate a set a and a discrimination rule gx ixea. the class of these rules is denoted by c. assume that we somehow could estimate by then it makes sense to use the associated rule gnx the maximum likelihood method suggests a way of picking the from that in some sense is most likely given the data. it is fully automatic-the user does not have to pick any parameters-but it does require a serious implementation effort in many cases. in a sense the regression format is more powerful than the the maximum likelihood principle set format as there is more information in knowing a function than in knowing the indicator function still no structure is assumed on the part of x and none is needed to obtain consistency results. a third format even more detailed is that in which we know that the distribution of y belongs to a class v of distributions on nd x i. for a given tion we know so we may once again deduce a rule g by setting gx this distributionformat is even more powerful as the positions x i xn alone in some cases may determine the unknown parameters in the model. this tion fits in squarely with classical parameter estimation in mathematical statistics. once again we may apply the maximum likelihood principle to select a tion from v. unfortunately as we move to more restrictive and stronger formats the number of conditions under which the maximum likelihood principle is tent increases as well. we will only superficially deal with the distribution format chapter for more detail. the maximum likelihood method regression format given xl x n the probability of observing yi yn yn is n l- yi il if is unknown but belongs to a family f of functions we may wish to select that from f it exists for which that likelihood product is maximal. more formally we select so that the logarithm is maximal. if the family f is too rich this will overfit and consequently the selected function has a probability of error that does not tend to l for convenience we assume here that there exists an element of f maximizing we do not assume here that the class f is very small. classes in which each in f is known up to one or a few parameters are loosely called parametric. sometimes f is defined via a generic description such as f is the class of all nd that are lipschitz with constant c. such classes are called nonparametric. in certain cases the boundary between parametric and nonparametric is unclear. we will be occupied with the consistency question does with probability one for all distribution of y? p i y is the maximum likelihood method regression format the probability of error of the natural rule that corresponds to rj. if additionally f is rich enough or our prior information is good enough we may have inf ef l l but that is not our concern here as f is given to us. we will not be concerned with the computational problems related to the mization of lnrj over f. gradient methods or variations of them are sometimes used-refer to mclachlan for a bibliography. it suffices to say that in simple cases an explicit form for rjn may be available. an example follows. our first lemma illustrates that the maximum likelihood method should only be used when the true unknown rj indeed belongs to f. recall that the same was not true for empirical risk minimization over vc classes chapter lemma consider the class f with two functions rja rjb let rjn be the function picked by the maximum likelihood method. there exists a distribution for y with rj tj. f such that with probability one as n lrjn max lrj min lrj. thus maximum likelihood picks the wrong classifier. proof. define the distribution of y on x by px y o p y o p y and p y then one may quickly verify that that l but this is irrelevant here. within f rjb is the better for our distribution. by the strong law of large numbers we have with probability one similarly for rjb. if one works out the values it is seen that with probability one rjn rja for all n large enough. hence lrjn lrj with probability one. remark. besides the clear theoretical hazard of not capturing rj in f the maximum likelihood method runs into a practical problem with for example take rjb and assume rj for all n large enough both classes are represented in the data sample with probability one. this implies that lnrja lnrjb the maximum likelihood estimate rjn is ill-defined while any reasonable rule should quickly be able to pick rja over rjb. the lemma shows that when rj tj. f the maximum likelihood method is not even capable of selecting one of two choices. the situation changes dramatically when rj e f. for finite classes f nothing can go wrong. noting that whenever rj e f we have l lrj we may now expect that lrjn l in probability or with probability one. the maximum likelihood principle theorem lflfi k and e f then the maximum likelihood method is consistent that is l in probability. proof. for a fixed distribution of y we rank the members of f by increasing values of put let io be the largest index for which l let be the maximum likelihood choice from f. for any a we have l p rp.x a a. iio define the entropy of y by log chapter recall that log we also need the negative divergences di e log log i which are easily seen to be nonpositive for all i jensens inequality. more di if and only if with probability one. observe that for i io we cannot have this. let e maxiio di io k this set is empty but then the theorem is trivially true. it is advantageous to take a e observe that e di c e f if i lo. thus plryn i l p l p n by the law of large numbers we see that both terms converge to zero. note in particular that it is true even if for some i io di d for infinite classes many things can go wrong. assume that f is the class of all with fa and a is a convex set containing the origin. pick x uniformly on the perimeter of the unit circle. then the maximum likelihood estimate matches the data as we may always find a closed polygon pn with consistency vertices at the xis with yi for rn lpn we have lnrn maximal value yet lrn py i p and l the class f is plainly too rich. for distributions of x on the positive integers maximum likelihood does not behave as poorly even though it must pick among infinitely many possibilities. assume f is the class of all r but we know that x puts all its mass on the positive integers. then maximum likelihood tries to maximize n il r over r e f where nli ll ixjiyjl and noi ll ixjiyjo. the maximization is to be done over all from fortunately thus if noi we set rni while if noi nli we this is turned into a maximization for each individual i we are not so lucky. pick rni as the value u that maximizes nli log u noi u. setting the derivative with respect to u equal to zero shows that nli rn l nli noi in other words rn is the familiar histogram estimate with bin width less than one half. it is known to be universally consistent theorem thus maximum likelihood may work for large f if we restrict the distribution of y a bit. consistency finally we are ready for the main consistency result for rn when f may have infinitely many elements. the conditions of the theorem involve the bracketing metric entropy of f defined as follows for every distribution of x and e let fxe be a set of functions such that for each r e f there exist functions r r e fxe such that for all x end and rx rx erx rx e. that is every r e f is between two members of fxe whose distance is not larger than e. let n e denote the cardinality of the smallest such fxe if nx e log nx e is called the bracketing e of f corresponding to x. the maximum likelihood principle theorem let f be a class of regression functions n d assume that for every distribution of x and e nx e then the maximum likelihood choice rjn satisfies lim l n--oo in probability for all distributions of y with rj e f. thus consistency is guaranteed if f has a finite bracketing e for every x and e. we provide examples in the next section. for the proof first we need a simple corollary of lemma corollary letrj rj nd and let ybeanndxo random variable pair with py llx x rjx. define l eminrjx lrj p y then e rjx l proof. the first inequality follows by lemma and the second by theorem now we are ready to prove the theorem. some of the ideas used here appear in wong and shen proof of theorem look again at the proof for the case i fi define e as there. let fe be the collection of those rj e f with lrj l e recalling that lrj p y. for every a for reasons that will be obvious later we take a thus consistency noting that e the law of large numbers implies that the first term on the right-hand side converges to zero for every e. problem for more information. next we bound the second term. for a fixed distribution let be the smallest set of functions such that for each ry e f there exist ry ry e with ryx ryx ryx x end and eryx ryx where will be specified later. by assumption nx we have ry l-y e ryxi ryxi y p sup ryefe il nn ryxjy n sup p n ryu ryefe il ry ry e f x ry ry ry and eryx ry n i y ryl i l-y i the union bound nx p x sup ryxn ryefe the maximum likelihood principle where in the last step we used markovs inequality and independence. we now find a good bound for e ryxryx jl ryx ryx for each e fee as follows e ryxryx jl ry ryx e j ryx ryx ryx ry we used rab fa jb e the cauchy-schwarz inequality e corollary summarizing we obtain ryefe p sup nx r by taking we see that the probability converges to zero exponentially rapidly which concludes the proof. examples in this section we show by example how to apply the previous consistency results. in all cases we assume that e f and we are concerned with the weak convergence examples of lcrj to l for all such distributions of y. the classes are as follows a b oo cab c e a b oo ai bi i d fl f ryx cxl x c o is monotone decreasing on i ry ryx ryx e e r ilx m e r d ry ryx eaoo? x t eaoa x e r x a e rd these are all rather simple yet they will illustrate various points. of these classes is nonparametric yet it behaves than the one-parameter class for example. we emphasize again that we are interested in consistency for all distributions of x. for f ryn will agree with the samples that is rynx i yi for all i and therefore ryn e fl is any function of the form with xl-i a xl xr b xrl where xn are the order statistics for xi xo xnl xl is the smallest data point with yl and xr is the largest data point with yr as l we claim that elryn pxl-l x xl x xrl the rule is simply excellent and has universal performance guarantees. remark. note that in this case maximum likelihood minimizes the empirical risk over the class of classifiers c a b. as c has vc dimension vc and inf ec l theorem implies that for all e plryn e and that elryn problem with the analysis given here we have gotten rid of the log n factor. the maximum likelihood principle the class f is much more interesting. here you will observe a dramatic ference with empirical risk minimization as the parameter c plays a key role that will aid a lot in the selection. note that the likelihood product is zero if ni or if yi for some i with xi tj b and it is cno otherwise where no is the number of yi pairs with a xi b yi and ni is the number of yi pairs with a xi b yi for fixed no n i this is maximal when c---noni resubstitution yields that the likelihood product is zero if ni or if yi for some i with xi tj b and equals exp ni no nl no log no no ni if otherwise. thus we should pick ciab such thatc ninoni and b maximizes the divergence nilog see problem ni noni no no log noni for by a similar argument as for let bd x x bd be the smallest rectangle of r d that encloses all xis for which yi then we know that xadbd agrees with the data. furthermore l and problem the logarithm of the likelihood product for is l logcxi cxi n iyil il setting the derivative with respect to c equal to zero in the hope of obtaining an equation that must be satisfied by the maximum we see that c must satisfy nitt xi il cxi xi unless xl xn where nl iyii this equation has a unique solution for c. the rule corresponding to is of the form gx icxi ixlc equivalently g if nl xxi otherwise. examples this surprising example shows that we do not even have to know the parameter c in order to describe the maximum likelihood rule. in quite a few cases this shortcut makes such rules very appealing indeed. furthermore as the condition of theorem is fulfilled e implies that the rule is consistent as well. for it is convenient to order the xis from small to large and to identify k consecutive groups each group consisting of any number of xis with yi and one xi with yi thus k iyii then a moments thought shows that ln must be piecewise constant taking values al ak on the consecutive groups and the value zero past the k-th group can only consist of xis with yi the likelihood product thus is of the form aknk where n i n k are the cardinalities of the groups minus one the number of yj o-elements in the i group. finally we have ak argmax it ajn j k lal jl to check consistency we see that for every x and e nx e problem thus the condition of theorem is satisfied and lln l in probability whenever e in the maximum likelihood method will attempt to place m at the center of the highest concentration in n d of xis with yi while staying away from xis with yi certainly there are computational problems but it takes little thought to verify that the conditions of theorem are satisfied. explicit description of the rule is not necessary for some theoretical analysis! class is a simple one-parameter class that does not satisfy the condition of theorem in fact maximum likelihood fails here for the following reason let x be uniform on then the likelihood product is it x it iyio this product reaches a degenerate global maximum as regardless of the true value of that gave rise to the data. see problem class is used in the popular logistic discrimination problem reviewed and studied by anderson see also mclachlan chapter it is larly important to observe that with this model the maximum likelihood principle where log thus subsumes linear discriminants. it does also force a bit more structure on the problem making rj approach zero or one as we move away from the separating hyperplane. day and kerridge point out that model is appropriate if the class-conditional densities take the form cfx exp mte-x m where c is a normalizing constant f is a density m is a vector and b is a positive definite matrix f and must be the same for the two densities but c and m may be different. unfortunately obtaining the best values for cio and ci by maximum likelihood takes a serious computational effort. had we tried to estimate f m and in the last example we would have done more than what is needed as both f and b drop out of the picture. in this respect the regression format is both parsimonious and lightweight. classical maximum likelihood distribution format in a more classical approach we assume that given y x has a density ii and given y x has a density fo where both fo and ii belong to a given family of densities. a similar setup may be used for atomic distributions but this will not add anything new here and is rather routine. the likelihood product for the data yd yn is n n pfoxil-yi il where p p i is assumed to be unknown as well. the maximum likelihood choices for p fo fl are given by f ft arg max lnp fo ii peo fo ii ef where having determined p fo ft that the solution is not necessarily unique the maximum likelihood rule is i if pfox p ftx otherwise. gn generally speaking the distribution format is more sensitive than the regression format. it may work better under the correct circumstances. however we give problems and exercises up our universality as the nature of the distribution of x must be known. for example if x is distributed on a lower dimensional nonlinear manifold of n d the distribution format is particularly inconvenient. consistency results and examples are provided in a few exercises. problems and exercises problem show that if ifi k and e f then l with probability one where is obtained by the maximum likelihood method. also prove that convergence with probability one holds in theorem problem prove that if are regression functions then the divergence e log log is nonpositive and that d if and only if with probability one. in this sense d measures the distance between and problem let l p y and e log where py llx x and nd is arbitrary. prove that also problem hint first prove that for p q e plog- plog q p p taylors series with remainder term for h problem show that for each there exists an eo such that for all e e eo hint proceed by chernoffs bounding technique. problem consider e f and let be a maximum likelihood estimate over f. let p py i. show that c l pmin derive an upper bound for e l the maximum likelihood principle where in case of multiple choices for you take the smallest in the equivalence class. this is an important problem as f picks a histogram cell in a data-based manner. f may be generalized to the automatic selection of the best k-cell histogram let f be the collection of that are constant on the k intervals determined by breakpoints al ak-l problem show that for the class s nl when yj e and yjn is the maximum likelihood estimate. problem show that for the class holds for all x and e that is the bracketing e-entropy of is bounded by hint cover the class by a class of monotone decreasing piecewise constant functions whose values are multiples of e and whose breakpoints are at the ke of the distribution of x e l. problem discuss the maximum likelihood method for the class if at x otherwise ee e a e rd e r. what do the discrimination rules look like? if yj e f is the rule consistent? can you guarantee a certain rate of convergence for elyjn? if yj f can you prove that does not converge in probability to lyj for some distribution of y with p x x? how would you obtain the values of e e a for the maximum likelihood choice problem let xl be i.i.d. uniform random variables. let yi yn be arbitrary numbers. show that with probability one lim sup n x n cos e-oo iyil iyio while for any t with probability one sup n sinexi x n cos osest iyi iyio l. parametric classification what do you do if you believe someone tells you that the conditional tions of x given y and y are members of a given family of distributions described by finitely many real-valued parameters? of course it does not make sense to say that there are say six parameters. by interleaving the bits of binary expansions we can always make one parameter out of six and by splitting binary expressions we may make a countable number of parameters out of one parameter writing the bits down in triangular fashion as shown below. bs bs bi ho thus we must proceed with care. the number of parameters of a family really is measured more by the sheer size or vastness of the family than by mere sentation of numbers. if the family is relatively small we will call it parametric but we will not give you a formal definition of for now we let the set of all possible values of the parameter e be a subset of a finite-dimensional euclidean space. formally let pe e e be a class of probability distributions on the borel sets of rd. typically the family pe is parametrized in a smooth way. that is two distributions corresponding to two parameter vectors close to each other are in some sense close to each other parametric classification as well. assume that the class-conditional densities fo and fi exist and that both belong to the class of densities fe e e e. discrete examples may be handled similarly. take for example all gaussian butions on n d in which where mend is the vector of means and l is the covariance matrix. recall that x t denotes the transposition ofthe column vector x and detl is the determinant of l. this class is conveniently parametrized bye l that is a vector of d dd real numbers. knowing that the class-conditional distributions are in pe makes discrimination so much easier-rates of convergence to l are excellent. take fe as the class of uniform densities on hyperrectangles of n d this has natural parameters the coordinates of the lower left and upper right vertices. figure the class-conditional densities are form on hyperrectangles. class given yd yn a child could not do things wrong-for class estimate the upper right vertex by and similarly for the upper right vertex of the class density. lower left vertices are estimated by considering minima. if ad a are the two unknown hyperrectangles and p py the bayes rule is simply if x e al ad o if x e ad al p if x e al n ad aao aai p gx o if x e al nao p aad aao p parametric classification in reality replace ao ai and p by the sample estimates ao al above and p n fyilj. this way of doing things works very well and we will pick up the example a bit further on. however it is a bit ad hoc. there are indeed a few main principles that may be used in the design of classifiers under the additional information given here. in no particular order here are a few methodologies as the bayes classifiers belong to the class c fp!el p e e e it suffices to consider classifiers in c. for example if fe is the normal family then c coincides with indicators of functions in the set x t ax bt x c a is a d x d matrix b e r d c e r that is the family of quadratic decisions. in the hyperrectangular example above every is of the form f al where al and are hyperrectangles of rd. finding the best classifier ofthe form fa where a e a is something we can do in a variety of ways one such way empirical risk minimization is dealt with in chapter for example. plug-in rules estimate by ih and p pry i by pfrom the data and form the rule gn otherwise. i if pfejx the rule here is within the class c described in the previous paragraph. we are hopeful that the performance with gn is close to the performance with the bayes rule g when is close to for this strategy to work it is absolutely essential that lp probability of error when p are the parameters be continuous in robustness is a key ingredient. if the coj.?tinuity can be captured in an inequality then we may get performance guarantees for e ln l in terms of the distance between and methods of estimating the parameters include maximum likelihood. this methodology is dealt with in chapter this method is rather sensitive to incorrect hypotheses if we were wrong about our assumption that the class-conditional distributions were in p e another strategy minimum distance estimation picks that member from p e that is closest in some sense to the raw empirical measure that puts mass n at each if the n data points. see section this approach does not care about continuity of lp as it judges members of pe by closeness in some space under a metric that is directly related to the probability of error. robustness will drop out naturally. a general approach should not have to know whether e can be described by a nite number of parameters. for example it should equally well handle descriptions parametric classification as pe is the class of all distributions of y in which py llx x is monotonically increasing in all the components of x. universal paradigms such as maximum likelihood minimum distance estimation and empirical error imization are all applicable here. this particular pe is dealt with in chapter just to show you that the description of the class does invalidate the underlying principles. example exponential families a class pe is exponential if every class-conditional density fe can be written in the form where ok rd r a jtk e r are fixed tions and c is a normalizing constant. examples of exponential families include the gaussian gamma beta rayleigh and maxwell densities problem the bayes-rule can be rewritten as gx this is equivalent to p!ex if log otherwise. gx jtie log otherwise. the bayes-rule is a so-called generalized linear rule with ok as basis functions. such rules are easily dealt with by empirical risk minimization and related methods such as complexity regularization another important point is that g does not involve the function for all we know may be some esoteric ill-behaved function that would make estimating fex all but impossible if were unknown. even if pe is the huge family in which is left undetermined but it is known to be identical for the two conditional densities ace is just a normalization factor we would still only have to look at the same small class of generalized linear discrimination rules! so densities do not matter-ratios of densities do. pattern recognition should be and is easier than density estimation. those who first estimate by it and then construct rules based on the sign of fij- fj do themselves a disservice. standard plug-in rules standard plug-in rules in standard plug-in methods we construct estimates bt and pfrom the data and use them to form a classifier gn otherwise. it is generally not true that if p p and bt in probability then lgn l in probability where lgn is the probability of error with gn consider the following simple example example. letfe be the class of all uniform densities on if and on if let p then a reasonable estimate of would be fh maxiyil ixi i clearly fh in probability. however as fh with probability one we note that gn for x and thus even though l the supports of fed and fel are not overlapping lgn py i p the problem with this is that there is no continuity with respect to in basic consistency based upon continuity considerations is indeed easy to lish. as ratios of densities matter it helps to introduce pfel p fed p fel py llx x where or as the case may be. we recall that if gnx where ois an estimate of then lgn l dn where dn is the data sequence. thus e l by the lebesgue dominated convergence theorem we have without further ado theorem if is continuous in in the l sense where m is the measure of x and in probability then elgn l for the standard plug-in rule. in some cases we can do better and derive rates of convergence by examining the local behavior of for example if e-ex x e r then and elgn l parametric classification yielding an explicit bound. in general is multivariate consisting at least of the triple but the above example shows the way to happy analysis. for the simple example given here e lgn l if e e? o. in fact this seems to suggest that for this family e should be found to minimize e this is false. one is always best off minimizing the probability of error. other criteria may be relevant via continuity but should be considered with care. how certain parameters are estimated for given families of distributions is what mathematical statistics is all about. the maximum likelihood principle looms large is estimated for densities ie by argmax n lexj e iyio for example. if you work out this product you will often discover a simple form for the estimate of the data. in discrimination only matters not the class-conditional densities. maximum likelihood in function of the was studied in chapter we saw there that this is often consistent but that maximum likelihood behaves poorly when the true distribution is not in pe. we will work out two simple examples as an example of the maximum likelihood method in discrimination we assume that fe x a l x e ixo a oj is the class of all gamma densities with the likelihood product given yl yn is if are the unknown parameters n n peoxil-yi ii this is the probability of observing the data sequence if the ies were in fact discrete probabilities. this product is simply where the first thing we notice is that this expression depends only on certain functions of the data notably l yi xi ll yi l yi log xi ll yi log xi and l yi these are called the sufficient statistics for the problem at hand. we may in fact throwaway the data and just store the sufficient statistics. the likelihood product has to be maximized. even in this rather simple univariate example this is a nontrivial task. luckily we immediately note that p occurs in the factor pn p where n l yi this is maximal at p n in a well-known result. for fixed ao ai we can also get but for standard plug-in rules variable ao ai the optimization is difficult. in d-dimensional cases one has nearly always to resort to specialized algorithms. as a last example we return to fe densities on rectangles of rd. here the likelihood product once again has pn p as a factor leading to p n in. the other factor bl bo are the lower left upper right vertices of the rectangles for is zero if for some i yi xi tf- rectangleal bd or yi xi tf- rectangleao bo otherwise it is where ilbl al ii til aij denotes the volume of the rectangle h and similarly for iibo ii. this is maximal if iibl ii and iibo ii are minimal. thus the maximum likelihood estimates are min xk k d i ai jyji jk max xk jyfi k d i where ai ai bi bi and xi xi rates of convergence may be obtained via some of the of chapter such as elgn l pfeo elgn l elgn l i ryx i the rate with which e approaches in e with some metric may be very different from that with which lgn approaches l as shown in theorem the inequality is always loose yet it is this inequality that is often used to derive rates of convergence by authors and researchers. let us take a simple example to illustrate this point. assume fe is the family of nonnal densities on the real line. if ao al are the unknown means and standard deviations of the class-conditional densities and p py is also unknown then we may estimate p by p ll yj in and mi and ai by respectively when denominators are positive. if a denominator is set mi from chebyshevs inequality we can verify that for fixed p e i parametric classification elp ph elmi mil elai if we compute e we will discover that elgn l o however if we compute e ignxtgxd we will find that elgn l thus while the parameters converge at a rate o i dictated by the central limit theorem and while converges to in l with the same rate the error rate in discrimination is much smaller. see problems to for some practice in this respect. bibliographic remarks. mclachlan has a comprehensive treatment on parametric classification. duda and hart have many good introductory examples and a nice discussion on sufficient statistics a topic we do not deal with in this text. for maximum likelihood estimation see hjort minimum distance estimates here we describe a general parameter estimation principle that appears to be more suitable for plug-in classification rules than the maximum likelihood method. the estimated parameter is obtained by the projection of the empirical measure on the parametric family. the principle of minimum distance estimation may be described as follows. let e be a parametric family of distributions and assume that pe is the unknown distribution of the i.i.d. observations zl zn. denote by vn the empirical measure let d be a metric on the set of all probability distributions on nd the minimum distance estimate of is defined as argmin dvn pe eee if it exists and is unique. if it is not unique select one candidate for which the minimum is attained. consider for example the kolmogorov-smirnov distance dksp q sup ifz gz! zend minimum distance estimates pen vn figure the member oips closest to the empirical measure vn is chosen by the minimum distance estimate. where f and g are the distribution functions of the measures p and q on n d respectively. it is easy to see that d ks is a metric. note that sup ifz sup lpa qai zerd aea where a is the class of sets of the form x x zed for z zed e nd. for the kolmogorov-smirnov distance between the estimated and the true distributions by the triangle inequality we have dkspen pe dkspell vn dksvn pe pe where in the second inequality we used the definition of en. now notice that the upper bound is just twice the kolmogorov-smirnov distance between an empirical distribution and the true distribution. by a straightforward application of theorem for every nand e since the n-th shatter coefficient sea n of the class of sets cannot exceed dd. this can easily be seen by an argument similar to that in the proof of theorem from the inequalities above we see that the smirnov distance between the estimated and the true distributions is always o jlog n n the only condition we require is that en be well defined. parametric classification of course rather than the kolmogorov-smirnov distance it is the error ability of the plug-in classification rule in which we are primarily interested. in order to make the connection we adapt the notions of minimum distance tion and kolmogorov-smirnov distance to better suit the classification problem we are after. every parametric family of distributions defines a class of sets in nd as the collection a of sets of the form i where the classifiers are the possible plug-in rules defined by the parametric family. the idea here is to perform minimum distance estimation with the generalized kolmogorov-smirnov distance with respect to a class of sets closely related to a. assume that both class-conditional distributions pea pel belong to a parametric family pe and the class-conditional densities feo fe exist. then the bayes rule may be written as ifaex! g where aex pfelx pfeax and p py i. we use the short notation the function ae may be thought of as the radon-nikodym derivative of the signed measure qe ppeo in other words to each borel set a c n d qe assigns the real number qea ppeoa. given the data dn xl yd yn we define the empirical counterpart of qe by otherwise the minimum distance classification rule we propose projects the empirical signed measure on the set of measures qe. the metric we use is also specifically fitted to the given pattern recognition problem define the class of sets a e nd aex o p e e e a is just the class of sets a c nd such that ixea is the bayes rule for some also introduce n be a b e a given two signed measures q q we define their generalized smirnov distance by dbq q sup iqa qai aeb that is instead of the class of half-infinite intervals as in the definition of the ordinary kolmogorov-smirnov distance here we take the supremum over a class tailored to our discrimination problem. now we are ready to define our minimum distance estimate e argmin dbqe e minimum distance estimates where the minimum is taken over all triples e eo with p e e e. the corresponding classification rule is if ae x ggx otherwise. i the next theorem shows that if the parametric assumption is valid then ge performs extremely well. the theorem shows that if a has finite vc dimension v a then out any additional conditions imposed on the parametric class the corresponding error probability lge is not large lge l log nn. theorem assume that both conditional densities feo fe l are in the metric class fe. then for the classification rule defined above we have for every nand e that p l e where n is the n-th shatter coefficient of furthermore e l n. remark. recalling from chapter that n n and that sea n lva where va is the vc dimension of a we obtain the bound p l e the proof of the theorem is based upon two key observations. the first lemma provides a bound on lge l in terms of the generalized kolmogorov-smirnov distance between the estimated and the true parameters. the second lemma is a straightforward extension of the vapnik-chervonenkis inequality to signed sures. lemma proof. denote ae aex o and ae aex a that is ggx iagx o gx iaex o and ae ae e a. at the first crucial step we use the equality of theorem lge l f igexfgex laexldx r c aexdx j aenae j aenae aexdx parametric classification r aexdx r aexdx j aena r aexdx r aexdx janae janae j aena qeae n a qeae n a qea n ae qea n ae qe since both ae n a and a n ae are members of be. the next lemma is an extension of the vapnik inequality. the proof is left to the reader lemma for every nand e p e the rest of the proof of theorem shows that the generalized smimov distance with respect to b between the estimated and true distributions is small with large probability. this can be done as we proved for the smimov distance at the beginning of the section. proof of theorem by lemma lge l qe the definition of e. the triangle inequality the theorem now follows by lemma finally we examine robustness of the minimum distance rule against modeling errors that is what happens if the distributions are not in pe. a good rule should still work reasonably well if the distributions are in some sense close to the modeled parametric class pe. observe that if for some e eo the bayes rule can be written as if ae g otherwise then lemma remains valid even when the class-conditional distributions are not in p e. denote the true class-conditional distributions by p pt let p pry and introduce q p pt pp. thus lge l lemma qe qe qe vn qe the triangle inequality the definition of qe by the triangle inequality. empirical error minimization lemma now applies to the first term on the right-hand side. thus we conclude that if the bayes rule is in a then for all e inf qe ds q qe p l ejs the constant in the exponent may be improved significantly by more careful ysis. in other words if the bayes rule is in a and the true distribution is close to the parametric family in the generalized kolmogorov-smimov distance specified above then the minimum distance rule still performs close to the bayes error. unfortunately we cannot say the same if a does not contain the bayes rule. pirical error minimization discussed in the next section is however very robust in all situations. empirical error minimization in this section we explore the connection between parametric classification and rule selection by minimizing the empirical error studied in chapter consider the class c of classifiers of the form if pfex otherwise where p e and eo el e the parametric assumption means that the bayes rule is contained in c. then it is a very natural approach to minimize the empirical error probability measured on the training data dn over classifiers in the class c. denote the empirically selected rule the one minimizing ln by for most typical parametric classes the vc dimension vc is finite. therefore as a straightforward consequence of theorem we have corollary if both conditional distributions are contained in the ric family pes then for the error probability l p yidn of the empirically optimal rule we have for every nand e p l e the result above means that n n rate of convergence to the bayes rule is guaranteed for the empirically optimal rule whenever the vc dimension vc is finite. this is the case for example for exponential families. if pes is an exponential family with densities of the form jex caejjxexp parametric classification then by results from chapter sc n observe that in this approach nothing but properties of the class c are used to derive the a jlog n in rate of convergence. remark. robustness. the method of empirical minimization is clearly extremely robust against errors in the parametric model. obviously theorem if the true conditional distributions are not contained in the class p e then l can be replaced in corollary by inf p y. if the model is fairly good then this number should be very close to the bayes risk l d remark. lower bounds. the results of chapter imply that for some butions the error probability of the selected rule is about a i vfn away from the bayes risk. in the parametric case however since the class of distributions is restricted by the parametric model this is not necessarily true. in some cases a much faster rate of convergence is possible than the a jlog n in rate guaranteed by corollary see for example problem where an example is given in which the error rate is d problems and exercises problem show that if both conditional distributions of x given y and y are gaussian then the bayes decision is quadratic that is it can be written as gx atjix ao otherwise where the functions ljix are either of the form xi i-th component of the vector x or x and ao e r. problem let be the normal density on the real line with mean m and standard deviation and we draw an i.i.d. sample x i xn from and set and lxi m n show that e mil in and e cti in by using chebyshevs inequality. show that this rate is in fact tight. prove also that the result remains true if n is replaced by n a binomial p random variable independent of xi x n where p e that is m becomes n xi if n and otherwise and similarly for ct. problem assume that p and that both class-conditional densities and are gaussian on r with unit variance but different means. we use the maximum likelihood estimates mo iii of the conditional means mo exiy o and m exiy i to obtain the plug-in classifier ge. show that e n. then go on to show thate l oon. problems and exercises problem show that the following classes of densities on r constitute exponential families gaussian family gamma family beta family rayleigh family rea x tj-i x ixeo.i. a ae-tj e problem this exercise shows that one-parameter classes may be incredibly rich. let c be the class of rules of the form gex if x e ae if x ae where x e r e r is a parameter and a is a union of intervals on the real line. equivalently ge iae. let a u i where b i are bits obtained as follows first list all sequences of length l then those of length et cetera so that i is a concatenation of et cetera. show that for all n there exists a set x n c r that can be shattered by a set from e e en. conclude that the vc dimension of c is infinite. if we use c for empirical error minimization and l what can you say about el n the probability of error of the selected rule? problem continuation. let x be uniform on i with probability i set y bi if x e i so that l o. derive the class of bayes rules. work out the details of the generalized kolmogorov-smirnov distance minimizer based on the class of provide the best upper bound on el n you can get with any method. regardless of discrimination how would you estimate e? derive the asymptotic behavior of e ln for the plug-in rule based on your estimate of e. problem if fe uniform densities on rectangles of rd and if we use the maximum likelihood estimates of p derived in the text show that e ln l parametric classification problem continued. assume that the true class-conditional densities are fa and fa ii with the same maximum likelihood estimates given above find fa for which el n yet l o. problem continued. show that the rate above can in general not be proved. problem show that by noting that the supremum in the definition of d ks may be replaced by a maximum over nd carefully selected points of nd. hint use the idea of fingering from chapters and problem prove lemma by following the line of the proof of the chervonenkis inequality hint in the second symmetrization step observe that has the same distribution as that of sup t ai ixi ea i aeb n il problem minimum distance density estimation. letfe e e be a parametric class of densities on rd and assume that the i.i.d. sample xl xn is drawn from the density fe e define the class of sets and define the minimum distance estimate of e by arg min d ape fin eee where pe is the distribution belonging to the density fe fin is the empirical distribution defined by xl x n and dais the generalized kolmogorov-smimov distance defined by aea prove that if a has finite vc dimension then dap q sup ipa qai. e ifg-x fexldx for more information on minimum distance density estimation see yatracos and devroye hint follow the steps listed below f ifg- fe i pe theorem. dapg- pe fin as in the text. finish the proof by applying alexanders improvement of the kis inequality generalized linear discrimination the classifiers we study here have their roots in the fourier series estimate or other series estimates of an unknown density potential function methods chapter and generalized linear classifiers. all these estimators can be put into the following form classify x as belonging to class if k l anjojx jl where the js are fixed functions forming a base for the series estimate anj is a fixed function of the training data and k controls the amount of smoothing. when the os are the usual trigonometric basis then this leads to the fourier series classifier studied by greblicki and pawlak when the os form an orthonormal system based upon hermite polynomials we obtain the classifiers studied by greblicki and greblicki and pawlak when j is the collection of all products of components of x as etcetera we obtain the polynomial method of specht we start with classification based on fourier series expansion which has its origins in fourier series density estimation which in tum was developed by cencov schwartz kronmal and tarter tarter and kronmal and specht its use in classification was considered by greblicki specht greblicki and pawlak and others. generalized linear discrimination fourier series classification let f be the density of x and assume that f e that is f f fxdx and f and recall that a denotes the lebesgue measure on nd. let be a complete orthonormal set of bounded functions in l with sup jx b an orthonormal set of functions in l is such that f ljjoi iuj for all i j. it is complete if f alji for all i for some function a e l implies that a almost everywhere respect to a. if f e l then the class-conditional densities fo and fl exist and are in then the function ax pfox lfx is in l as well. recall that the bayes decision is given by g if ax otherwise. let the bounded functions ljl form a complete orthonormal basis and let the fourier representation of a be given by a lajljj. jl the above convergence is understood in l that is the infinite sum means that the coefficients are given by we approximate ax by a truncation of its fourier representation to finitely many terms and use the data dn to estimate the coefficients appearing in this sum. formally consider the classification rule where the estimates anj of a j are very easy to compute before discussing the properties of these rules we list a few examples of complete orthonormal systems on the real line. some of these systems contain functions fourier series classification on a bounded interval. these of course can only be used if the distribution of x is concentrated on an interval. the completeness of these systems may typically be checked via the stone-weierstrass theorem a general way of producing complete orthonormal systems on n d is taking products of dimensional functions of the d components as sketched in problem for more information on orthogonal series we refer to sansone and zygmund the standard trigonometric basis on the interval jr is formed by the functions v cosix r vjr r i vjr sinix this is a complete orthonormal system in l n. the legendre polynomials form a complete orthonormal basis over the terval i x x i i f!ii d i ol the set of hermite functions is complete and orthonormal over the whole real line functions of the laguerre basis are defined on by rei oix rz d i x x e z! dx iex e where a is a parameter of the system. a complete orthonormal system over the whole real line may be obtained by the haar basis contains functions on that take three different values. it is orthonormal and complete. functions with finitely many values have computational advantages in pattern recognition. write the integer i as i j where k i j j then oix if x e if x e otherwise. generalized linear discrimination functions on of the rademacher basis take only two values and it is an orthonormal system but it is not complete lixj i the basis may be completed as follows write i such that il ik. this form is unique. define where the j are the rademacher functions. the resulting basis is orthonormal and complete and is called the walsh basis. there is no system of basis functions that is better than another for all tions. in selecting the basis of a fourier series rule the designer must use a mixture of intuition error estimation and computational concerns. we have the following consistency theorem theorem let be a complete orthonormal basis on rd such that for some b b for each i and x. assume that the class-conditional densities fo and fl exist and are in if kn and as n kn n then the fourier series classification rule defined in is consistent lim elgn l n---oo if then kn and as n kn logn n lim lgn l n---oo with probability one that is the rule is strongly consistent. proof. first observe that the an are unbiased estimates of the a j eanj e e e lix e j and that their variance can be bounded from above as follows fourier series classification n fxdx ct n j n ct n where we used the boundedness of the ljs. by parsevals identity therefore exploiting orthonormality of the j we have j t a dx j a j dx thus the expected l is bounded as follows k lvarctnj l jl jknl ctj generalized linear discrimination since ll a the second term tends to zero if kn if at the same time kn n then the expected converges to zero that is the estimate is consistent in l now convergence of the error probability follows from problem to prove strong consistency convergence with probability one fix e and assume that n is so large that l a jknl then kn l p jl the union bound where we used hoeffdings inequality because k n log n n the upper bound is eventually smaller than which is summable. the borel-cantelli lemma yields strong consistency. strong consistency of the classifier then follows from problem remark. it is clear from the inequality that the rate of convergence is determined by the choice of kn if kn is small then the first term which corresponds to the estimation error is small but the approximation error expressed by the second term is larger. for large kn the generalized linear classification situation is reversed. the optimal choice of kn depends on how fast the second term goes to zero as kn grows. this depends on other properties of f such as the size of its tail and its smoothness. remark. consistency in theorem is not universal since we needed to sume the existence of square integrable conditional densities. this however is not a restrictive assumption in some practical situations. for example if the tions are corrupted by additive gaussian noise then the conditions of the theorem hold however if does not have a density the method may be inconsistent problem fourier series classifiers have two rather unattractive features in general they are not invariant under translations of the coordinate space. most series classifiers are not local in nature-points at arbitrary distances from x affect the decision at x. in kernel and nearest neighbor rules the locality is easily controlled. generalized linear classification when x is purely atomic or singular continuous theorem is not applicable. a theme of this book is that pattern recognition can be developed in a free manner since after all the distribution of y is not known. besides even if we had an i.i.d. sample yn at our disposal we do not know of any test for verifying whether x has a square integrable density. so we proceed a bit differently to develop universally consistent rules. to generalize fourier series classifiers let be bounded functions on nd. these functions do not necessarily form an orthonormal basis of consider the classifier where the coefficients an are some functions of the data dn. this may be viewed as a generalization of fourier series rules where the coefficients were unbiased estimates ofthe fourier coefficients of ax here we will sider some other choices of the coefficients. observe that gn is a generalized linear classifier as defined in chapter an intuitively appealing way to determine the coefficients is to pick them to minimize the empirical error committed on dn as in empirical risk minimization. the critical parameter here is kn the number of basis functions used in the linear combination. if we keep k n fixed then as we saw in chapter the error probability of the selected rule converges quickly to that of the best classifier of the above form. however for some distributions this infimum may be far from the bayes risk so it is useful to let kn grow as n becomes generalized linear discrimination larger. however choosing kn too large may result in overfitting the data. using the terminology introduced in chapter let ckn be the class of classifiers of the form where the a j range through n. choose the coefficients to minimize the empirical error let gn be the corresponding classifier. we recall from chapter that the is not more than kn therefore by theorem for every vc dimension of ckll n where h is the binary entropy function. the right-hand side is if kn on. however to obtain consistency we need to know how close infecknj l is to l this obviously depends on the choice of the s as well as on the distribution. if kn is not allowed to grow with n and is bounded by a number k then it follows from theorem that for every b universal consistency eludes us as for some distribution infeck l l inf l l inf al p.dx jl p.dx. the limit supremum of the right-hand side can be arbitrarily close to zero for all distributions if kn as n and the set of functions is dense in ll on balls of the form ilx ii b for all p.. this means that given any probability measure p. and function f with j ifldp. for every e there exists an integer k and coefficients ai ak such that thus we have obtained the following consistency result problems and exercises theorem let be a sequence of functions such that the set of all finite linear combinations of the j is dense in l on balls of the form ilx ii s b for any probability measure jl. then then gn is strongly universally consistent when kn and as n kn n remark. to see that the statement of theorem is not vacuous note that ness in l on balls follows from denseness with respect to the supremum norm on balls. denseness in the loo sense may be checked by the stone-weierstrass orem for example the class of all polynomial classifiers satisfies the conditions of the theorem. this class can be obtained by choosing the tions as the simple polynomials xl x cd xci similarly the functions oj can be chosen as bases of a trigonometric series. remark. the histogram rule. it is clear that theorem can be modified in the following way let jb j k k be functions such that for every fell f-l concentrated on a bounded ball and e there is a ko such that for all k ko there is a function of the form ll a j jk whose distance from f in l is smaller than e. let be the class of generalized linear classifiers based on the functions olkn okkn if k n and kn n then the classifier gn that minimizes the empirical error over ckn is strongly universally consistent. this modification has an interesting implication. consider for example functions olkn ok that are indicators of sets of a partition of nd. then it is easy to see that the classifier that minimizes the empirical error is just the histogram classifier based on the same partition. the denseness assumption requires that the partition becomes infinitesimally fine as n in fact we have obtained an alternative proof for the strong universal consistency of the regular histogram rule the details are left as an exercise problem let yrl be a sequence of real-valued functions on a bounded problems and exercises terval b such that f yrixyrxdx ii and the set of finite linear combinations ai yri is dense in the class of continuous functions on b with respect to the supremum norm. define the functions bd n by show that these functions form a complete orthonormal set of functions on bd. problem let z be an arbitrary random variable on n and v be a real random variable independent of z that has a density h e show that the density f of the generalized linear discrimination random variable x z v exists and f hint use jensens inequality to prove that f f problem derive the strong universal consistency of the regular histogram rule from theorem as indicated in the remark following it. problem let be the standard trigonometric basis and consider the classification rule defined in show that the rule is not consistent for the following distribution given y l let x be and given y let x be uniformly distributed on jt. assume furthermore that pry problem the haar basis is not bounded. determine whether or not the laguerre hermite and legendre bases are bounded. complexity regularization this chapter offers key theoretical results that confirm the existence of certain rules. although the proofs are constructive-we do tell you how you may design such rules-the computational requirements are often prohibitive. many of these rules are thus not likely to filter down to the software packages and pattern recognition implementations. an attempt at reducing the computational ity somewhat is described in the section entitled empirical covering. nevertheless we feel that much more serious work on discovering practical rithms for empirical risk minimization is sorely needed. in chapter the empirical error probability was minimized over a class e of decision rules n d i. we saw that if the vc dimension vc of the class is finite then the error probability of the selected rule is within constant times log n n of that of the best rule in e. unfortunately classes with finite vc dimension are nearly always too small and thus the error probability of the best rule in e is typically far from the bayes risk l for some distribution theorem in chapter we investigated the special classes of generalized linear rules. theorem for example shows that if we increase the size of the class in a controlled fashion as the sample size n increases the error probability of the selected rule approaches l for any distribution. thus vc increases with n! theorem may be generalized straightforwardly for other types of classifiers. consider a sequence of classes el containing classifiers of the form n d i. the training data dn yd yn are used to select a classifier by minimizing the empirical error probability ln over the class ekn where the integer kn depends on n in a specified way. the following generalization is based on the proof of theorem problem complexity regularization theorem assume thatel is a sequence of classes of decision rules such that for any distribution of y lim inf l l i--oo and that the vc dimensions vcc are all finite. if kn and vckn log n as n n then the classifier that minimizes the empirical error over the class e ckn is strongly universally consistent. theorem is the missing link-we are now ready to apply the rich theory of vapnik classes in the construction of universally consistent rules. the theorem does not help us however with the choice of the classes eco or the choice of the sequence examples for sequences of classes satisfying the condition of theorem include generalized linear decisions with properly chosen basis functions or neural networks a word of warning here. the universally consistent rules obtained via theorem may come at a tremendous computational price. as we will see further on we will often have exponential complexities in n instead of polynomial time that would be obtained if we just minimized the empirical risk over a fixed vc class. the computational complexity of these rules are often incomparably larger than that of some simple universally consistent rules as k-nearest neighbor kernel or partitioning rules. structural risk minimization let ecl be a sequence of classes of classifiers from which we wish to select a sequence of classifiers with the help of the training data dn. previously we picked l from the kn class eckn where the integer kn is some prespecified tion of the sample size n only. the integer kn basically determines the complexity of the class from which the decision rule is selected. theorem shows that under mild conditions on the sequence of classes it is possible to find sequences such that the rule is universally consistent. typically kn should grow with n in order to assure convergence of the approximation error infjjecknj l l but it cannot grow too rapidly for otherwise the estimation error l inf j l might fail to converge to zero. ideally to get best performance the two types of error should be about the same order of magnitude. clearly a pre specified choice of the complexity kn cannot balance the two sides of the trade-off for all tions. therefore it is important to find methods such that the classifier is selected from a class whose index is automatically determined by the data dn. this section deals with such methods. structural risk minimization the most obvious method would be based on selecting a candidate decision rule n from every class cj example by minimizing the empirical error over cj and then minimizing the empirical error over these rules. however typically the vc dimension of c ucj equals infinity which in view of results in chapter implies that this approach will not work. in fact in order to guarantee it is necessary that inf inf l l vc theorems and a possible solution for the problem is a method proposed by vapnik and vonenkis and vapnik called structural risk minimization. first we select a classifier from every class cj which minimizes the empirical error over the class. then we know from theorem that for every j with very large probability vcuogn now pick a classifier that minimizes the upper bound over j to make things more precise for every nand j we introduce the complexity penalty rj n n vcj logen. let nl be classifiers minimizing the empirical error ln over the classes cl respectively. for e cj define the complexity-penalized error estimate ln ln rj n. finally select the classifier i minimizing the complexity penalized error estimate in n over j the nexttheorem states thatthis method avoids overfitting the data. the only condition is that each class in the sequence has finite vc dimension. theorem let cl be a sequence of classes of classifiers such that for any distribution of y lim inf l l assume also that the vc dimensions vc! are finite and satisfy l e-vcj regularization then the classification rule cp based on structural risk minimization as defined above is strongly universally consistent. remark. note that the condition on the vc dimensions is satisfied if we insist that vcji vcj for all j a natural assumption. see also problem n instead of minimizing the empirical error ln over the set of candidates c the method of structural risk minimization minimizes in the sum of the empirical error and a term vcj logen which increases as the vc dimension of the class cj containing cp increases. since classes with larger vc dimension can be considered as more complex than those with smaller vc dimension the term added to the empirical error may be considered as a penalty for complexity. the idea of minimizing the sum of the empirical error and a term penalizing the complexity has been investigated in various statistical problems by for example rissanen akaike barron barron and cover and lugosi and zeger barron minimizes the penalized empirical risk over a suitably chosen countably infinite list of candidates. this approach is close in spirit to the skeleton estimates discussed in chapter he makes the connection between structural risk minimization and the minimum description length principle and obtains results similar to those discussed in this section. the theorems presented here were essentially developed in lugosi and zeger proof of theorem we show that both terms on the right-hand side of the following decomposition converge to zero with probability one for the first term we have p z i.pj e p lncp e p nj in ll.j e l p nj ln e rj n l vcj theorem jl jl structural risk minimization l vcj jl l e f..e jl using the defining expression for rj n where f.. ll e- vcj by assumption. thus it follows from the borel-cantelli lemma that l in nj j?. with probability one as n now we can tum to investigating the second term inffl inc nj l by our assumptions for every e there exists an integer k such that inf lc l e. fix such a k. thus it suffices to prove that lim sup inf inc nj n-oo j?.l inf lc with probability one. clearly for any fixed k if n is large enough then rck n e vck logc en thus for such large n p inc nj j?.i inf lc e inf lc e p nk p n c n k r c k n inf l c e p nk inf lc p sup iln vck e- by theorem therefore since vck the proof is completed. theorem shows that the method of structural risk minimization is sally consistent under very mild conditions on the sequence of classes el complexity regularization this property however is shared with the minimizer of the empirical error over the where kn is a properly chosen function of the sample size n class ckn the real strength then of structural risk minimization is seen from the next result. theorem let cl be a sequence of classes of classifiers such that the vc dimensions vcl v are all finite. assume further that the bayes rule g e c ucj jl that is a bayes rule is contained in one of the classes. let k be the smallest integer such that g e ck. thenfor every nand e satisfying the error probability of the classification rule based on structural risk minimization satisfies proof. theorem follows by examining the proof of theorem the only difference is that by assumption there is an integer k such that inf rjjeck l l therefore for this k l l in nj in nj inf l rjjeck and the two terms on the right-hand side may be bounded as in the proof of theorem theorem implies that if g e c there is a universal constant co and a finite k such that vck logn n that is the rate of convergence is always of the order of jlog n n and the constant factor vck depends on the distribution. the number vck may be viewed as the inherent complexity of the bayes rule for the distribution. the intuition is that the simplest rules are contained in cl and more and more complex rules are added to the class as the index of the class increases. the size of the error is about the same as if we had known k beforehand and minimized the empirical error over ck. in view of the results of chapter it is clear that the classifier described in theorem does not share this property since if l then the error probability of the structural risk minimization rule selected from ckn is larger than a constant times jvckn log n n for some distribution-even if g e ck for some fixed k. just as in designs based upon minimum description length automatic model selection and other complexity regularization methods akaike barron and barron and cover structural risk minimization automatically finds where to look for the optimal classifier. the constants appearing in theorem may be improved by using refined versions of the vapnik inequality. the condition g e c in theorem is not very severe as c can be a large class with infinite vc dimension. the only requirement is that it should be written as a countable union of classes of finite vc dimension. note however that the class of all decision rules can not be decomposed as such theorem we also emphasize that in order to achieve the jlog n n rate of convergence we do not have to assume that the distribution is a member of a known finite-dimensional parametric family chapter the condition is imposed solely on the form of the bayes classifier g. by inspecting the proof of theorem we see that for every k el l co vck log n inf l l n eck in fact theorem is the consequence of this inequality. the first term on the right-hand side which may be called estimation error usually increases with growing k while the second the approximation error usually decreases with it. importantly the above inequality is true for every k so that vck logn inf l l eck n thus structural risk minimization finds a nearly optimal balance between the two terms. see also problem remark. it is worthwhile mentioning that under the conditions of theorem an even faster rate of convergence is achievable at the expense of nifying the constant factor. more precisely it is possible to define the complexity penalties rj n such that the resulting classifier satisfies ela. l where the constant cl depends on the distribution. these penalties may be defined by exploiting alexanders inequality and the inequality above can be proved by using the bound in problem see problem remark. we see from the proof of theorem that p in e complexity regularization this means that in does not underestimate the true error probability of by much. this is a very attractive feature of in as an error estimate as the designer can be confident about the performance of the rule the initial excitement over a consistent rule with a guaranteed n n rate of convergence to the bayes error is tempered by a few sobering facts the user needs to choose the sequence cm and has to know vcw however problems and the method of simple empirical covering below. it is difficult to find the structural risk minimizer efficiently. after all the minimization is done over an infinite sequence of infinite sets. the second concern above deserves more attention. consider the following simple example let d and let cj be the class of classifiers for which ua i j il where each ai is an interval ocr. then from theorem vcw and we may take rj n n in structural risk minimization we find those j empty intervals that minimize ln rj n and call the corresponding classifier nj for j we have rj n loge en. as rn n rl n and rj n is monotone in j it is easily seen that to pick the best j as well we need only consider j n. still this is a formidable exercise. for fixed j the best j intervals may be found by considering all possible insertions of interval boundaries among the n xso this brute force method takes computation time bounded from below by if we let j run up to n then we have as a lower bound just the last term alone g grows as a ul r and is prohibitively large for n fortunately in this particular case there is an algorithm which finds a classifier minimizing ln rj n over c ul cj in computational time polynomial in n see problem another example when the structural risk minimizer is easy to find is described in problem however such fast algorithms are not always available and exponential running time prohibits the use of structural risk minimization even for relatively small values of n. t jl simple empirical covering poor approximation properties of vc classes we pause here for a moment to summarize some interesting by-products that readily follow from theorem and the slow convergence results of chapter theorem states that for a large class of distributions an n n rate of convergence to the bayes error l is achievable. on the other hand theorem asserts that no universal rates of convergence to l exist. therefore the class of distributions for which theorem is valid cannot be that large after all. the combination of these facts results in the following three theorems which say that vc classes-classes of subsets of n d with finite vc dimension-have necessarily very poor approximation properties. the proofs are left to the reader as easy exercises problems to for direct proofs see for example benedek and itai theorem letc be any class of classifiers of the fo rm nd i with vc dimension vc then for every e there exists a distribution such that mf l l ec e. theorem let be a sequence of classifiers such that the vc dimensions vcl are all finite. then for any sequence of positive numbers converging to zero arbitrarily slowly there exists a distribution such that for every k large enough inf l l ak. eck theorem the class c of all measurable decision rules of form rd i cannot be written as c u cen jl where the vc dimension of each class cj is finite. in other words the class b of all borel subsets ofnd cannot be written as a union of countably many vc classes. in fact the same is true for the class of all subsets of the set of positive integers. simple empirical covering as theorem shows the method of structural risk minimization provides tomatic protection against the danger of overfitting the data by penalizing plex candidate classifiers. one of the disadvantages of the method is that the penalty terms rj n require knowledge of the vc dimensions of the classes cj or upper bounds of these dimensions. next we discuss a method proposed by complexity regularization buescher and kumar which is applicable even if the vc dimensions of the classes are completely unknown. the method called simple empirical cov ering is closely related to the method based on empirical covering studied in problem in simple empirical covering we first split the data sequence dn into two parts. the first part is dm yd ym and the second part is yn the positive integers m and i n m will be specified later. the first part dm is used to cover c cen as follows. for every e c define the binary m-vector bm by there are n different values of bm usually as ve n that is all possible values of bm occur as is varied over c but of course n depends on the values of x i x m. we call a classifier simpler than another classifier if the smallest index i such that e ci is smaller than or equal to the index j such that e cj. for every binary m-vector b e lm that can be written as b bm for some e c we pick a candidate classifier with k e n such that bmpmk b and it is the simplest such classifier that is there is no e c such that simultaneously bmpmk b and is simpler than m.k. this yields n candidates among these we select one that minimizes the empirical error measured on the independent testing sequence denote the selected classifier by the next theorem asserts that the method works under circumstances similar to structural risk minimization. theorem and kumar let cl be a nested sequence of classes of classifiers such that for any distribution of y lim joo eej inf l l assume also that the vc dimensions vel are all finite. if m i log n and min then the classification rule based on simple empirical covering as defined above is strongly universally consistent. proof. we decompose the difference between the error probability of the selected rule and the bayes risk as follows the first term can be handled by lemma and hoeffdings inequality simple empirical covering p inf l mk e l-sk-sn p sup il mk e dm lz e l-sk-sn e e because m on the latter expression converges to zero exponentially rapidly. thus it remains to show that inf l mk l l-sk-sn with probability one. by our assumptions for every e there is an integer k such that inf lep l e. eck then there exists a classifier epee e ek with lepe l e. therefore we are done if we prove that lim sup inf l mk lepe with probability one. n-hx! l-sk-sn clearly there is a classifier mj among the candidates ml mn such that since by definition mj is simpler than epe and the classes el are nested it follows that mj e ek. therefore sup ilep lep i where the last supremum is taken over all pairs of classifiers such that their sponding binary vectors bm and bm are equal. but from problem gomplexity regularization which is summable if m log n remark. as in theorem we may assume again that there is an integer k such that infeok l l then from the proof of the theorem above we see that the error probability of the classifier obtained by the method of simple empirical covering satisfies p l e e-n-me unfortunately for any choice of m this bound is much larger than the analogous bound obtained for structural risk minimization. in particular it does not yield an ocjlognn rate of convergence. thus it appears that the price paid for the advantages of simple empirical covering-no knowledge of the vc dimensions is required and the implementation may require significantly less computational time in general-is a slower rate of convergence. see problem problems and exercises problem prove theorem problem define the complexity penalties rj n so that under the conditions of theorem the classification rule based upon structural risk minimization satisfies where the constant cl depends on the distribution. hint use alexanders bound and the inequality of problem problem let cl be a sequence of classes of decision rules with finite vc veuj on the vc dimensions are known. dimensions. assume that only upper bounds a j define the complexity penalties by rj n logen. n show that if li e-cxj then theorems and carryover to the classifier based on structural risk minimization defined by these penalties. this points out that knowledge of relatively rough estimates of the vc dimensions suffice. problem let cci be a sequence of classes of classifiers such that the vc dimensions vel are all finite. assume furthermore that the bayes rule is tained in one of the classes and that l o. let be the rule obtained by structural risk minimization using the positive penalties rj n satisfying problems and exercises for each n rj n is strictly monotone increasing in j. for each j limn-oo rj n show that el olognn and zeger for related work see benedek and itai problem let cj be the class of classifiers n d to i satisfying i u ai j il where the ais are bounded intervals in n. the purpose of this exercise is to point out that there is a fast algorithm to find the structural risk minimizer over c ul cen that is which minimizes in ln rj n over c where the penalties rj n are defined as in theorems and the property below was pointed out to us by miklos csuros and szabo. let al aj be the j intervals defining the classifier minimizing ln over cj. show that the optimal intervals for cjl arl a jll satisfy the following property either j of the intervals coincide with a l a j or of them are among a r a j and the remaining two intervals are j subsets of the j interval. use property to define an algorithm that finds in running time polynomial in the sample size n. problem assume that the distribution of x is concentrated on the unit cube that is px e ld let p be a partition of ld into cubes of size j that is p contains cubic cells. let cj be the class of all histogram classifiers ld to i based on p. in other words p contains all classifiers which assign the same label to points falling in the same cell of p. what is the vc dimension vej of cen? point out that the classifier minimizing ln over cj is just the regular histogram rule based on p. chapter thus we have another example in which the empirically optimal classifier is computationally inexpensive. the structural risk minimizer based on c ul cj is also easy to find. assume that the a posteriori probability is uniformly lipschitz that is for any x y e ld cllx where c is some constant. find upper bounds for the rate of convergence of el to l problem prove theorem hint use theorem problem prove theorem hint use theorem problem prove theorem hint use theorems and problem assume that the bayes rule g is contained in c ul cj. let be the classifier obtained by simple empirical covering. determine the value of the design parameter m that minimizes the bounds obtained in the proof of theorem obtain a tight upper bound for el l compare your results with theorem condensed and edited nearest neighbor rules condensed nearest neighbor rules condensing is the process by which we eliminate data points yet keep the same behavior. for example in the nearest neighbor rule by condensing we might mean the reduction of yn to y yl such that for all x e n d the l-nn rule is identical based on the two samples. this will be called pure condensing. this operation has no effect on l n and therefore is recommended whenever space is at a premium. the space savings should be substantial whenever the classes are separated. unfortunately pure condensing is computationally expensive and offers no hope of improving upon the performance of the ordinary l-nn rule. o o class under i-nn rule figure pure condensing eliminating the marked points does not change the decision class under rule o gondensed and edited nearest neighbor rules hart has the first simple algorithm for condensing. he picks a subset that correctly classifies the remaining data by the i-nn rule. finding a minimal such subset is computationally difficult but heuristics may do the job. harts heuristic is also discussed in devijver and kittler for probabilistic analysis we take a more abstract setting. let y be a sequence that depends in an arbitrary fashion on the data dn and let gn be the i-nearest neighbor rule with y yi where for simplicity m is fixed beforehand. the data might for example be obtained by finding the subset of the data of size m for which the error with the i-nn rule committed on the remaining n m data is minimal will be called harts rule. regardless if in n ignxy and ln pgnx yidnl then we have the following theorem and wagner for all e and all butions piln lnl es d ne remark. the estimate in is called the resubstitution estimate of the error ability. it is thoroughly studied in chapter where several results of the mentioned kind are stated. proof. observe that where bi is the voronoi cell of x in the voronoi partition corresponding to xi xl that is bi is the set of points of nd closer to x than to any other x appropriate distance-tie breaking. similarly we use the simple upper bound iln inl sup ivna vai aeam where v denotes the measure of y vn is the corresponding empirical measure and am is the family of all subsets ofnd x i of the form ul bi x where b l bm are voronoi cells corresponding to xl x m xi end yi i. we use the vapnik-chervonenkis inequality to bound the above supremum. by theorem condensed nearest neighbor rules where a is the class of sets x but each set in a is an intersection of at most m hyperplanes. therefore by theorems and sea n sup d n.e d where n denotes the number of points in r d x the result now follows from theorem remark. with harts rule at least m data points are correctly classified by the rule we handle distance ties satisfactorily. therefore ln min. the following is a special case let m n be fixed and let dm be an arbitrary random subsetofm pairs from yn usedingn let the remaining n m points be denoted by tn- m we write lndm for the probability of error with the based upon dm and we define in harts rule ln would be zero for example. then we have theorem for all e where ln is the probability of error with gn that dm depends in an arbitrary fashion upon dn and l n.m is ln.mdm tn- m with the data set dm. proof. list the m-element subsets im of n and define d as the sequence of m pairs from dn indexed by i ii i in i a ccor mg y enote n-m d d en di th in ti d n condensed and edited nearest neighbor rules hoeffdings inequality because given d mlnmdi tm is binomial m lnd by checking the proof we also see that if dm is selected to minimize the error estimate lnm t-m then the error probability ln of the obtained rule satisfies p inf e p sup ilndnj lnmdm e d theorem in the proof of theorem thus for the particular rule that mimics harts rule the exception that m is fixed if m is not too large-it must be much smaller than n log n-ln is likely to be close to the best possible we can hope for with a rule based upon a subsample of size m. with some work problem we see that by theorem where lm is the probability of error with the rule based upon a sample of m data pairs. hence if m on log n m lim sup el n lnn for the i-nn rule based on m data pairs dm selected to minimize the error estimate lnmdm tn-m. however this is very pessimistic indeed. it reassures us that with only a small fraction of the original data we obtain at least as good a performance as with the full data set-so this method of condensing is worthwhile. this is not very surprising. interestingly however the following much stronger result is true. condensed nearest neighbor rules theorem if m on log n and m and if lns the probability of error for the condensed nearest neighbor rule in which lnmdm tn- m is minimized over all data sets dm then lim eln l. n--oo proof. by it suffices to establish that as m such that m on where lndm is the probability of error of the i-nn rule with dm. as this is one of the fundamental properties of nearest neighbor rules not previously found in texts we offer a thorough analysis and proof of this result in the remainder of this section culminating in theorem the distribution-free result of theorem sets the stage for many consistency proofs for rules that use condensing editing or proto typing as defined in the next two sections. it states that inherently partitions of the space by i-nn rules are rich. historical remark. other condensed nearest neighbor rules are presented by gates ullmann ritter woodruff lowry and isenhour tomek swonger gowda and krishna and fukunaga and mantock define z let yi zi i n be i.i.d. tuples pendent of y z where x may have a distribution different from x but the support set of the distribution j- of x is identical to that of j- the distribution of x. furthermore pyi x and zi is the bayes decision at x. lemma let ln p zlx zixi zi x zn be the probability of error for the j-nn rule based on zi i n that is zlx zi if x is the nearest neighbor of x among xi x. then lim el n o. n--oo proof. denote by xlx the nearest neighbor of x among xi x. notice that the proof of lemma may be extended in a straightforward way to show that iixlx xii with probability one. since this is the only property of the nearest neighbor of x that we used in deriving the asymptotic formula for the ordinary i-nn error limn--oo eln equals lnn corresponding to the pair z. but we have p x x thus the bayes probability of error l condensed and edited nearest neighbor rules for the pattern recognition problem with z is zero. hence for this distribution the i-nn rule is consistent as lnn l o. lemma let zix be as in the previous lemma. let ln p i yix x yn be the probability of error for the discrimination problem for y z. then lim e ln l where l is the bayes error corresponding to y. proof. p i y p i z i z l by lemma theorem let dm be a subset of size m drawn from dn. if m and min as n then lim pinflndm le n-oo for all e where ln denotes the conditional probability of error of the nearest neighbor rule with dnu and the infimum ranges over all subsets. proof. let d be the subset of dn consisting of those pairs yi for which yi irjxi zi. if jdj m let d be the first m pairs of d and if jdi m let d yi ym. then if n jdi m then we know that the pairs in d are i.i.d. and drawn from the distribution of z where x has the same support set as x see problem for properties of x. in particular s pn m m lnd l sieves and prototypes pnmplnd le pbinomialn p m n el l p py z l and by markovs inequality e by the law of large numbers we use mn and by lemma we use m edited nearest neighbor rules edited nearest neighbor rules are i-nn rules that are based upon carefully selected subsets y yi this situation is partially dealt with in the previous section as the frontier between condensed and edited nearest neighbor rules is defined. the idea of editing based upon the k-nn rule was first suggested by wilson and later studied by wagner and penrod and wagner wilson suggests the following scheme compute yi zi where zi is the k-nn decision at xi based on the full data set with yi deleted. then eliminate all data pairs for which yi zi. the remaining data pairs are used with the i-nn rule the k-nn rule. another rule based upon data splitting is dealt with by devijver and kittler a survey is given by dasarathy devijver and devijver and kittler repeated editing was investigated by tomek devijver and kittler propose a modification of wilsons leave-one-out method of editing based upon data splitting. sieves and prototypes let gn be a rule that uses the i-nn classification based upon prototype data pairs y ythatdependinsomefashionontheoriginaldata.ifthepairs form a subset of the data pairs thus m n we have edited or condensed nearest neighbor rules. however the yf pairs may be strategically picked outside the original data set. for example in relabeling section m n x xi and y gxi where gxi is the k-nn decision at xi. under some conditions the relabeling rule is consistent theorem the true objective of proto typing is to extract information from the data by insisting that m be much smaller than n. condensed and edited nearest neighbor rules figure a rule based upon prototypes. in this example all the data points are correctly classified based upon the prototype rule chang describes a rule in which we iterate the following step until a given stopping rule is satisfied merge the two closest nearest neighbors of the same class and replace both pairs by a new average prototype pair. kohonen recognizes the advantages of such prototyping in general as a device for partitioning rd-he calls this learning vector quantization. this theme was picked up again by geva and sitte who pick x x as a random subset of x i xn and allow y to be different from yi diverging a bit from geva and sitte we might minimize the empirical error with the prototyped rule over all y y where the empirical error is that committed on the remaining data. we show that this simple strategy leads to a bayes-risk consistent rule whenever m and min o. note in particular that we may take x xm and that we yi y m as these are not used. we may in fact use additional data with missing yi-values for this purpose-the unclassified data are thus efficiently used to partition the space. let be the empirical risk on the remaining data where gn is the rule based upon yd y. let g be the i-nn rule with the choice of y y that minimizes lngn. let lg denote its probability of error. theorem lg l in probability for all distributions whenever m and min o. proof. there are different possible functions gn thus p sp ilngn i xl xn supp ei xl xm gn by hoeffdings inequality. also p l p ln e p lgn e p l e sieves and prototypes minimizes lgn sp ilngn xl xn we used lngn lng p l e g is the rule based on zl zm with zi iyx as in lemma m by lemma if m and m n if we let xi xn have arbitrary values-not only among those taken by xl xn-then we get a much larger more flexible class of classifiers. for ample every linear discriminant is nothing but a prototype i-nn rule with m take and place x and x in the right places. in this sense prototype rules generalize a vast class of rules. the most promising strategy of choosing prototypes is to minimize the empirical error committed on the ing sequence dn. finding this optimum may be computationally very expensive. nevertheless the theoretical properties provided in the next result may provide useful guidance. theorem let c be the class of nearest neighbor rules based on prototype pairs yd ym m where the yi range through nd x i. given the training data dn i yd yn let gn be the nearest neighbor rule from c minimizing the error estimate then for each e the rule is consistent if m such that m log m on. for d and d condensed and edited nearest neighbor rules the probability bound may be improved significantly. for d andford in both cases the rule is consistent if m and m log m on. proof. it follows from theorem that where sc n is the n-th shatter coefficient of the class of sets i e c. all we need is to find suitable upper bounds for s n. each classifier is a partitioning rule based on the m voronoi cells defined by xl x m therefore sc n is not more than times the number of different ways n points in nd can be partitioned by voronoi partitions defined by m points. in each partition there are at most mm cell boundaries that are subsets of d i-dimensional hyperplanes. thus the sought number is not greater than the number of different ways mm hyperplanes can partition n points. by results of chapter this is at most proving the first inequality. the other two inequalities follow by sharper bounds on the number of cell boundaries. for d this is clearly at most m. to prove the third inequality for each voronoi partition construct a graph whose vertices are xl x m and two vertices are connected with an edge if and only if their corresponding voronoi cells are connected. it is easy to see that this graph is planar. but the number of edges of a planar graph with m vertices cannot exceed nishizeki and chiba which proves the inequality. the consistency results follow from the stated inequalities and from the fact that inf ec l tends to l as m the proof of theorem again. problems and exercises lim infn-cxj enn whenever l true or false if l then en on. problem let n n be the size of the data set after pure condensing. show that hint consider the real line and note that all points whose right and left neighbors are of the same class are eliminated. problem let yd be an i.i.d. sequence of pairs of random variables in n d x l with pry llx x ryx. let z be the first pair yi in problems and exercises the sequence such that yi iix show that the distribution of x is absolutely continuous with respect to the common distribution of the xis with density nikodym derivative d d l where l is the bayes error corresponding to yl. let y be a l-valued random variable with py x if l denotes the bayes error corresponding to y then show that l l problem consider the following edited nn rule. the pair yz is eliminated from the training sequence if the k-nn rule on the remaining n pairs incorrectly classifies xi the i-nn rule is used with the edited data. show that this rule is consistent whenever x has a density if k and k n o. related papers wilson wagner penrod and wagner and devijver and kittler tree classifiers classification trees partition nd into regions often hyperrectangles parallel to the axes. among these the most important are the binary classification trees since they have just two children per node and are thus easiest to manipulate and update. we recall the simple terminology of books on data structures. the top of a binary tree is called the root. each node has either no child that case it is called a terminal node or leaf a left child a right child or a left child and a right child. each node is the root of a tree itself. the trees rooted at the children of a node are called the left and right subtrees of that node. the depth of a node is the length of the path from the node to the root. the height of a tree is the maximal depth of any node. trees with more than two children per node can be reduced to binary trees by root figure a binary tree. right child leaf tree classifiers a simple device-just associate a left child with each node by selecting the oldest child in the list of children. call the right child of a node its next sibling figures and the new binary tree is called the oldest-childlnext-sibling binary tree e.g. cormen leiserson and rivest for a general introduction. we only mention this particular mapping because it enables us to only consider binary trees for simplicity. a a b f g h h figure ordered tree the dren are ordered from oldest to youngest. figure the corresponding nary tree. in a classification tree each node represents a set in the space nd. also each node has exactly two or zero children. if a node u represents the set a and its children u u represent a and a then we require that a a u a and a n a the root represents n d and the leaves taken together form a partition of nd. assume that we know x e a. then the question x e a? should be answered in a computationally simple manner so as to conserve time. therefore if x xed we may just limit ourselves to questions of the following forms is xu ex? this leads to ordinary binary classification trees with partitions into hyperrectangles. is alxl adxd ex? this leads to bsp trees space partition trees. each decision is more time consuming but the space is more flexibly cut up into convex polyhedral cells. is ilx z ex? z is a point of n d to be picked for each node. this induces a partition into pieces of spheres. such trees are called sphere trees. is ljjx o? here ljj is a nonlinear function different for each node. every classifier can be thought of as being described in this format-decide class one if ljjx o. however this misses the point as tree classifiers should really be built up from fundamental atomic operations and queries such as those listed in we will not consider such trees any further. tree classifiers i i-- figure partition induced by an ordinary binary tree. figure corresponding tree. figure partition induced by a bsp tree. figure partition induced by a sphere tree. we associate a class in some manner with each leaf in a classification tree. the tree structure is usually data dependent as well and indeed it is in the construction itself where methods differ. if a leaf represents region a then we say that the classifier gn is natural if if l yi l yi x e a ixiea ixiea otherwise. that is in every leaf region we take a majority vote over all yis with xi in the same region. ties are broken as usual in favor of class o. in this set-up natural tree classifiers are but special cases of data-dependent partitioning rules. the latter are further described in detail in chapter tree classifiers figure a natural classifier based on an ordinary binary tree. the sion is in regions where points with label form a majority. these areas are shaded. regular histograms can also be thought of as natural binary tree classifiers-the construction and relationship is obvious. however as n histograms change size and usually histogram partitions are not nested as n grows. trees offer the exciting perspective of fully dynamic classification-as data are added we may update the tree slightly say by splitting a leaf or so to obtain an updated classifier. the most compelling reason for using binary tree classifiers is to explain plicated data and to have a classifier that is easy to analyze and understand. in fact expert system design is based nearly exclusively upon decisions obtained by going down a binary classification tree. some argue that binary classification trees are preferable over bsp trees for this simple reason. as argued in breiman man olshen and stone trees allow mixing component variables that are heterogeneous-some components may be of a nonnumerical nature others may represent integers and still others may be real numbers. invariance nearly all rules in this chapter and in chapters and show some sort of invariance with respect to certain transformations of the input. this is often a major asset in pattern recognition methods. we say a rule gn is invariant under transformation t if for all values of the arguments. in this sense we may require translation invariance rotation invariance linear translation invariance and monotone transformation invariance maps each coordinate separately by a strictly increasing but sibly nonlinear function. monotone transformation invariance frees us from worries about the kind of measuring unit. for example it would not matter whether earthquakes were sured on a logarithmic scale or a linear scale. rotation invariance matters of course in situations in which input data have no natural coordinate axis system. in many cases data are of the ordinal form-colors and names spring to mind-and trees with the x-property ordinal values may be translated into numeric values by creating bit vectors. here distance loses its physical meaning and any rule that uses ordinal data perhaps mixed in with numerical data should be monotone transformation invariant. tree methods that are based upon perpendicular splits are usually not ways monotone transformation invariant and translation invariant. tree methods based upon linear hyperplane splits are sometimes linear transformation invariant. the partitions of space cause some problems if the data points can line up along hyperplanes. this is just a matter of housekeeping of course but the fact that some projections of x to a line have atoms or some components of x have atoms will make the proofs heavier to digest. for this reason only we assume throughout this chapter that x has a density f. as typically no conditions are put on f in our consistency theorems it will be relatively easy to generalize them to all distributions. the density assumption affords us the luxury of being able to say that with probability one no d points fall in a hyperplane no d points fall iii a hyperplane perpendicular to one axis no d points fall in a hyperplane perpendicular to two axes etcetera. if a rule is monotone transformation invariant we can without harm transform all the data as follows for the purpose of analysis only. let ii fd be the marginal densities of x problem with corresponding distribution functions f i fd. then replace in the data each xi by txi where each component of txi is now uniformly distributed on of course as we do not know t beforehand this device could only be used in the analysis. the transformation t will be called the uniform marginal transformation. observe that the original density is now transformed into another density. trees with the x it is possible to prove the convergence of many tree classifiers all at once. what is needed clearly is a partition into small regions yet most majority votes should be over sufficiently large sample. in many of the cases considered the form of the tree is determined by the xs only that is the labels yi do not playa role in constructing the partition but they are used in voting. this is of course rather simplistic but as a start it is very convenient. we will say that the classification tree has the x for lack of a better mnemonic. let the leaf regions be an n possibly random. define n j as the number of xis falling in a j. as the leaf regions form a partition we have li n j n. by diama j we mean the diameter of the cell a j that is the maximal distance between two points of a j. finally decisions are taken by majority vote so for x e a j s j s n tree classifiers the rule is i gnx if l yi l yi x e a j ixeaj ixeaj otherwise. ax denotes the set ofthe partition an into which x falls and nx is the number of data points falling in this set. recall that the general consistency result given in theorem is applicable in such cases. consider a natural classification tree as defined above and assume the x theorem states that then el n l if diamax in probability nx in probability. a more general but also more complicated consistency theorem is proved in ter let us start with the simplest possible example. we verify the conditions of theorem for the k-spacing rule in one dimension. this rule partitions the real line by using the k-th so on order statistics see also parthasarathy and bhattacharya o figure a classifier. formally let k n be a positive integer and let xl xn be the order statistics of the data points. recall that xl xn are obtained by permuting xl xn in such a way that xl xn. note that this ordering is unique with probability one as x has a density. we partition r into n intervals ai an where n r n i k l such that for j n a j satisfies xku-li.. xkj e a j and the rightmost cell an satisfies we have not specified the endpoints of each cell of the partition. for simplicity let the borders between a j and a ji be put halfway between the rightmost data point in a j and leftmost data point in a ji j n the classification rule gn is defined in the usual way theorem let gn be the k-spacing classifier given above. assume that the distribution of x has a density f on r. then the classification rule gn is consistent that is limn--oo el n l if k and kin as n tends to infinity. trees with the x remark. le discuss various generalizations of this rule in chapter proof. we check the conditions of theorem as the partition has the x condition is obvious from k to establish condition fix e o. note that by the invariance of the rule under monotone transformations we may assume without loss of generality that f is the uniform density on among the intervals ai an there are at most lie disjoint intervals of length greater than e in thus pdiamax e max fla j e ljn e mx flna j m.ax ifla j h e hp ifla flnai ljn ljn flna ji where the supremum is taken over all intervals in r. the first term within the parentheses converges to zero by the second condition of the theorem while the second term goes to zero by an obvious extension of the classical glivenko-cantelli theorem this completes the proof. we will encounter several trees in which the partition is determined by a small fraction of the data such as binary k trees and quadtrees. in these cases condition of theorem may be verified with the help of the following lemma lemma let pi pk be a probability vector. let n l nk be mially distributed random variables with parameters n and pi pk then if the random variable x is independent oj n l nb andpx i pi we have for any m pnx m n this probability goes to if kin uniformly over all probability vectors with k components! proof. let zi be binomial with parameters n and pi. then pnx m n l pipzi ezi m npi n c e p.p z l_ i tree classifiers p- var zd inm i n chebyshevs inequality l n inpi npi o n the previous lemma implies that for any binary tree classifier constructed on the basis of xl x k with k regions nx in probability whenever kln k kin it suffices to note that we may take m arbitrarily large but fixed in lemma this remark saves us the trouble of having to verify just how large or small the probability mass of the region is. in fact it also implies that we should not worry so much about regions with few data points. what matters more than anything else is the number of regions. stopping rules based upon cardinalities of regions can effectively be dropped in many cases! balanced search trees balanced multidimensional search trees are computationally attractive. binary trees with n leaves have n height for example when at each node the size of every subtree is at least ex times the size of the other subtree rooted at the parent for some constant ex o. it is thus important to verify the consistency of balanced search trees used in classification. we again consider binary classification trees with the x and majority votes over the leaf regions. take for example a tree in which we split every node perfectly that is if there are n points we find the median according to one coordinate and create two subtrees of sizes ln and rn the median itself stays behind and is not sent down to the subtrees. repeat this for k levels of nodes at each level cutting along the next coordinate axe in a rotational manner. this leads to leaf regions each having at least n k points and at most points. such a tree will be called a median tree. figure median tree withfour leaf regions in setting up such a tree is very easy and hence such trees may appeal to certain grammers. in hypothesis testing median trees were studied by anderson theorem natural classifiers based upon median trees with k levels leaf regions are consistent ln l whenever x has a density if balanced search trees n and k-oo. the conditions of k are fulfilled if k n n k we may prove the theorem by checking the conditions of theorem condition follows trivially by the fact that each leaf region contains at least n k points and the condition thus we need only verify the first condition of theorem to make the proof more transparent we first analyze a closely related hypothetical tree the theoretical median tree. also we restrict the analysis to d the multidimensional extension is straightforward. the theoretical median tree rotates through the coordinates and cuts each hyperrectangle precisely so that the two new hyperrectangles have equal jl-measure. figure theoretical median tree with three levels of cuts. i observe that the rule is invariant under monotone transformations of the coordinate axes. recall that in such cases there is no harm in assuming that the marginal distributions are all uniform on we let vd denote the horizontal and vertical sizes of the rectangles after k levels of cuts. of course we begin with hi vi when k o. we now show that for the theoretical median tree diamax in probability as k note that diamax hx vex where hx and vex are the horizontal and vertical sizes of the rectangle ax. we show that if k is even ehx vex from which the claim follows. after the k-th round of splits since rectangles have equal probability measure we have apply another round of splits all vertical. then each term vj spawns so to speak two new rectangles with horizontal and vertical sizes vi and tree classifiers vi with hi h h that contribute the next round yields horizontal splits with total contribution now figure v. h v. h v. h v. i i i i i i i i vi thus over two iterations of splits we see that e h v is halved and the claim follows by simple induction. we show now what happens in the real median tree when cuts are based upon a random sample. we deviate of course from the theoretical median tree but consistency is preserved. the reason seen intuitively is that if the number of points in a cell is large then the sample median will be close to the theoretical median so that the shrinking-diameter property is preserved. the methodology followed here shows how one may approach the analysis in general by separating the theoretical model from the sample-based model. proof of theorem as we noted before all we have to show is that diamax in probability. again we assume without loss of generality that the marginals of x are uniform and that d again we show that ehx vex o. figure a rectangle after two rounds asplits. h i p! i v! i h! i p i hi tvt v i i p tvy i h p v i i h i if a rectangle of probability mass pi and sizes hi vi is split into four rectangles as in figure with probability masses p p p pt then the contribution pichi vi to ehx vex becomes after two levels of cuts. this does not exceed if and balanced search trees v e pi i max pi pi pi ii iii i max pi pi v e pi pi maxp plli z z pz pz that is when all three cuts are within of the true median. we call such cuts good. if all cuts are good we thus note that in two levels of cuts ehx vex is reduced by also all pis decrease at a controlled rate. let g be the event that all cuts in a median tree with k levels are good. then at level k all ps are at most thus lpihi vi il since ll vj if k is even. hence after k levels of cuts the last term tends to zero if e is small enough. we bound p g e by times the probability that one cut is bad. let us cut a cell with n points and probability content p in a given direction. a quick check of the median tree shows that given the position and size of the cell the n points inside the cell are distributed in an i.i.d. manner according to the restriction of fl to the cell. after the cut we have and rn points in the new cells and probability contents ln pi and p. it is clear that we may assume without loss of generality that p thus if all points are projected down in the direction of the cut and f and f n denote the distribution function and empirical distribution function of the obtained one-dimensional data then p is not goodl n p p n i ii tree classifiers phpfx-fnx-lin theorem n k. hence for n large enough binary search trees the simplest trees to analyze are those whose structure depends in a straightforward way on the data. to make this point we begin with the binary search tree and its multivariate extension the k-d tree cormen leiserson and rivest for the binary search tree for multivariate binary trees we refer to samet a full-fledged k-d tree is defined as follows we promote the first data point xl to the root and partition xn into two sets those whose first coordinate exceeds that of xl and the remaining points. within each set points are ordered by original index. the former set is used to build the right subtree of xl and the latter to construct the left subtree of xl for each subtree the same construction is applied recursively with only one variant at depth l in the tree the mod d l-st coordinate is used to split the data. in this manner we rotate through the coordinate axes periodically. attach to each leaf two new nodes and to each node with one child a ond child. call these new nodes external nodes. each of the n external nodes correspond to a region of r d and collectively the external nodes define a tion ofrd we define exactly what happens on the boundaries between regions. binary search trees figure a k-d tree random points on the plane and the induced partition. put differently we may look at the external nodes as the leaves of a new tree with nodes and declare this new tree to be our new binary classification tree. as there are n leaf regions and n data points the natural binary tree classifier it induces is degenerate-indeed all external regions contain very few points. clearly we must have a mechanism for trimming the tree to insure better populated leaves. let us look at just three naive strategies. for convenience we assume that the data points determining the tree are not counted when taking a majority vote over the cells. as the number of these points is typically much smaller than n this restriction does not make a significant difference. fix k n and construct a k tree with k internal nodes and k external nodes based on the first k data points xl xk. classify by majority vote over all k regions as in natural classification tees the data pairs ykd yn into account. call this the chronological k-d tree. fix k and truncate the k-d tree to k levels. all nodes at level k are declared leaves and classification is again by majority vote over the leaf regions. call this the deep k-d tree. fix k and trim the tree so that each node represents at least k points in the original construction. consider the maximal such tree. the number of regions here is random with between and nj k regions. call this the populated k-d tree. tree classifiers let the leaf regions be an n possibly random and denote the leaf nodes by ui un. the strict descendants of ui in the full k-d tree have indices that we will collect in an index set ii. define i ii i ni. as the leaf regions form a partition we have n lnin-n il because the leaf nodes themselves are not counted in ii. voting is by majority vote so the rule is the chronological k-d tree here we have n k also l are distributed as uniform spacings. that is if u i uk are i.i.d. uniform random variables ing k spacings ul ul uk uk-i uk by their order statistics ul uk then these spacings are jointly distributed as this can be shown by induction. when is added first picks a spacing with probability equal to the size of the spacing. then it cuts that spacing in a uniform manner. as the same is true when the chronological k-d tree grows by one leaf the property follows by induction on k. theorem we have e ln l for all distributions of x with a density for the chronological k-d tree classifier whenever k and kin o. proof. we verify the conditions of theorem as the number of regions is k and the partition is determined by the first k data points condition immediately follows from lemma and the remark following it. condition of theorem requires significantly more work. we verify dition for d leaving the straightforward extension to r d d to the reader. throughout we assume without loss of generality that the marginal butions are uniform we may do so by the invariance properties discussed earlier. fix a point x e rd. we insert points xl xk into an initially empty k tree and let r rk be the rectangles containing x just after these points were inserted. note that rl rk assume for simplicity that the integer k is a perfect cube and set l k define the distances from x to the sides of ri by hi h vi v figure m k the chronological k-d tree figure a rectangle ri containing x with its distances to the sides. hi v. i x h vi we construct a sequence of events that forces the diameter of rk to be small with high probability. let e be a small positive number to be specified later. denote the four squares with opposite vertices x x e e by then define the following events el n n xd il h h e v maxh h e maxv v h h least three of vm v hm hl are e v hm hs e v hk hd e if or hold then diamr k assume that we find a set bend such that px e b and for all x e b and sufficiently small e u u as k then by the lebesgue dominated convergence theorem diamax in probability and condition of theorem would follow. in the remaining part of the proof we define such a set b and show that u u this will require some work. we define the set b in terms of the density f of x. x e b if and only if fe i fzdz for all e small enough f fzdz r ar fx for all e small enough inf rectangles r containing x of diameter fx o. that px e b follows from a property of the support a corollary of the lessen-marcinkiewicz-zygmund theorem this implies for almost all x and the fact that for j.i-almost all x f o. it is easy to verify the following pe ped n e n e tree classifiers n e n n e n e n n n e n we show that each term tends to at x e b. term by the union bound pef ti ci ti cd il il exp min lsia by part of the definition of b. term by a simple geometric argument. hence pei n en o. term to show that n e n n e we assume without loss of generality that maxvz v e while maxhz hi a e. let be a subset of i i m consisting of those xis that fall in re we introduce three notions in this sequence first zi is the absolute value of the difference of the of x and x. let wi be the absolute value of the difference of the xl-coordinates of x and x. we re-index the sequence x wi and zi so that i runs from to n where n ixertl. m izl to avoid trivialities assume that n will be shown to happen with ability tending to one. call x a record if zi minzl zi. call x a good point if wi e. an x causes minhm hl e if that x is a good point and a record and if it defines a vertical cut. the alternating nature of the cuts makes our analysis a bit heavier than needed. we show here what happens when all directions are picked independently of each other leaving the rotating-cuts case to the reader thus if we set si i is a record x defines a vertical cut we have the chronological k-d tree re-index again and let x x all be records. note that given x xl is distributed according to f restricted to the rectangle r of height minv ziabove x height min z i below x width hz to the left of x width h! to the right of x. call these four quantities v v h h respectively. then pw. eix efx v v because the marginal distribution of an independent x is uniform and thus p e rir v v while by property of b e r wi eir recall the re-indexing. let m be the number of records m is the length of our sequence x. then e! n n n e n oj iwsix o but as cuts have independently picked directions and since eixj we see that n n n n o e c c efx m fno we rewrite m fix is a record and recall that the indicator variables in this sum are independent and are of mean i i hence for c e ll iii x e-x e ii i logn l the latter formula remains valid even if n o. thus with c pie! n n n ej e efxr tree classifiers l flrz. we know from the introduction n is binomial with parameters of this section that flrt is distributed as the minimum of l i.i.d. uniform random variables. thus for eflrz and pflrz l hence e pflrz i e i ll-c l l-c p binomialm l i l the first term is small by choice of the second one is the third one is bounded from above by chebyshevs inequality by i term this term is handled exactly as term note however that i and m now become m and k respectively. the convergence to requires now m i m which is still the case. this concludes the proof of theorem the deep k-d tree theorem the deep k-d tree classifier is consistent el n l for all distributions such that x has a density whenever lim k and n----oo k hmsup-- n----oo log n proof. in problem you are asked to show that k implies diamax in probability. theorem may be invoked here. we now show that the assumption lim supn----oo kl log n implies that nx in probability. let d be the depth from the root of x when x is inserted into a k-d tree having n elements. clearly nx d k so it suffices to show that d k in probability. we know that d log n in probability e.g. devroye and problem this concludes the proof of the theorem. quadtrees quadtrees quadtrees or hyperquadtrees are unquestionably the most prominent trees in puter graphics. easy to manipulate and compact to store they have found their way into mainstream computer science. discovered by finkel and bentley and surveyed by samet they take several forms. we are given dimensional data x i x n the tree is constructed as the k tree. in particular xl becomes the root of the tree. it partitions x xn into empty sets according to membership in one of the quadrants centered at xl figure r--- i t figure quadtree and the induced partition of the points on the right are shown in the position in space. the root is specially marked. the partitioning process is repeated at the child nodes until a certain stopping rule is satisfied. in analogy with the k-d tree we may define the chronological quadtree k splits are allowed defined by the first k points x i x k and the deep quadtree levels of splits are allowed. other more balanced versions may also be introduced. classification is by majority vote over all yi k i n in the chronological quadtree-that fall in the same region as x. ties are broken in favor of class o. we will refer to this as the deep quadtree classifier. theorem whenever x has a density the chronological quadtree classifier is consistent ln l provided that k and kin o. proof. assume without loss of generality that all marginal distributions are form as the x-property holds theorem applies. by lemma since we have external regions pnx m n k for all m provided that kin o. hence we need only verify the condition tree classifiers diamax in probability. this is a bit easier than in the proof of theorem for the k-d tree and is thus left to the reader remark. full-fledged random quadtrees with n nodes have expected height o n whenever x has a density e.g. devroye and laforest with k nodes every region is thus reached in only o k steps on the average. thermore quadtrees enjoy the same monotone transformation invariance that we observed for k-d trees and median trees. best possible perpendicular splits for computational reasons classification trees are most often produced by mining the splits recursively. at a given stage of the tree-growing algorithm some criterion is used to determine which node of the tree should be split next and where the split should be made. as these criteria typically use all the data the resulting trees no longer have the x in this section we examine perhaps the most natural criterion. in the following sections we introduce some alternative splitting criteria. a binary classification tree can be obtained by associating with each node a splitting function obtained in a top-down fashion from the data. for example at the root we may select the function sxi ex where i the component cut ex e n the threshold and s e i a larization are all dependent upon the data. the root then splits the data dn yd yn into two sets d d dn idiidi n such that d y e dn oj d y e dn o. a decision is made whether to split a node or not and the procedure is applied recursively to the subtrees. natural majority vote decisions are taken at the leaf level. all such trees will be called perpendicular splitting trees. in chapter we introduced univariate stoller splits that is splits that minimize the empirical error. this could be at the basis of a perpendicular splitting tree. one realizes immediately that the number of possibilities for stopping is endless. to name two we could stop after k splitting nodes have been defined or we could make a tree with k full levels of splits that all leaves are at distance k from the root. we first show that for d any such strategy is virtually doomed to fail. to make this case we will argue on the basis of distribution functions only. for convenience we consider a two-dimensional problem. given a rectangle r now best possible perpendicular splits assigned to one class y e i we see that the current probability of error in r before splitting is p e r y y. let r range over all rectangles of the form r n-oo a x r r n x r r n x ad or r n x and let r r r. then after asplit based upon r the probability of error over the rectangle r is px e r y i px e r y o as we assign class to r and class to r. the decrease in probability of error if we minimize over all r. computelj.r for all leaf rectangles and then proceed to split that rectangle leaf for which ij.r is maximal. the data-based rule based upon this would proceed similarly if pa is replaced everywhere by the empirical estimate ixiyieaj where a is of the form r x r x or r x y as the case may be. let us denote by l o l l the sequence of the overall probabilities of error for the theoretically optimal sequence of cuts described above. here we start with r and y for example. for fixed e we now construct a simple example in which l and lk as k l-e thus applying the best split incrementally even if we use the true probability of error as our criterion for splitting is not advisable. the example is very simple x has uniform distribution on with bility e and on with probability e. also y is a deterministic function of x so that l figure repeated stoller splits are not consistent in this two-dimensional example. cuts will always be made in the leftmost square. probability e o the way y depends on x is shown in figure y if x e u u u u x tree classifiers where al ak and so forth. we verify easily that py i also the error probability before any cut is made is lo the best split has r x r so that al is cut off. therefore li we continue and split off and so forth leading to the tree of figure figure the tree obtained by repeated stoller splits. yes o verify that and in general that lk e as claimed. splitting criteria based on impurity functions in breiman friedman olshen and stone presented their cart program for constructing classification trees with perpendicular splits. one of the key ideas in their approach is the notion that trees should be constructed from the bottom up by combining small subtrees. the starting point is a tree with n leaf regions c defined by a partition of the space based on the n data points. such a tree is much too large and is pruned by some methods that will not be explored here. when splitting criteria based on impurity functions constructing a starting tree a certain splitting criterion is applied recursively. the criterion determines which rectangle should be split and where the cut should be made. to keep the classifier invariant under monotone transformation of the coordinate axes the criterion should only depend on the coordinatewise ranks of the points and their labels. typically the criterion is a function of the numbers of points labeled by and in the rectangles after the cut is made. one such class of let a e n and let i be a given coordinate i d. let r be a hyperrectangle to be cut. define the following quantities for a split at a perpendicular to the i coordinate criteria is described here. xtr a j yj xj e r xji a are the sets of pairs falling to the left and to the right of the cut respectively. are the numbers of such pairs. finally the numbers of points with label and label in these sets are denoted respectively by niocr a ixir a n j yj yj oi ixicr a n j yj yj nilcr a nocra nlra i xtranxj following breiman friedman olshen and stone we define an impurity junction for a possible split a by ii a ljj nio nil ni ljj nio nil nio nil nl i i i i ni i nio nil nio nil where we dropped the argument a throughout. here ljj is a nonnegative tion with the following properties ljj g d ljjp p for any p e ljjo p increases in p on and decreases in p on a rectangle r is split at a along the i-th coordinate if iir a is minimal. ii penalizes splits in which the subregions have about equal proportions from both classes. examples of such functions ljj include the entropy junction p p log p p p et al. tree classifiers the gini function ljfp p p leading to the gini index of diversity f i et al. the probability ofmisclassification tp p minp p. in this case the splitting criterion leads to the empirical stoller splits studied in the previous section. we have two kinds of splits theforced split force a split along the i-th coordinate but minimize fica r over all a and rectangles r. the free split choose the most advantageous coordinate for splitting that is minimize fica r over all a i and r. unfortunately regardless of which kind of policy we choose there are distributions for which no splitting based on an impurity function leads to a consistent classifier. to see this note that the two-dimensional example of the previous section applies to all impurity functions. assume that we had infinite sample size. then fica r would approach aljfp p btq q where p is the probability content of r of the rectangles obtained after the cut is made b is that of r other rectangle p py e r and q py llx e r. if x is uniformly distributed on the checkerboard shown in figure regardless where we try to cut p q and every cut seems to be undesirable. figure no cut decreases the value of the impurity function. this simple example may be made more interesting by mixing it with a tion with much less weight in which xl- and x splits are alternatingly encouraged all the time. therefore impurity functions should be avoided in their raw form for splitting. this may explain partially why in cart the original tree is undesirable and must be pruned from the bottom up. see problems to for more information. in the next section and in the last section of this chapter we propose other ways of growing trees with much more desirable properties. the derivation shown above does not indicate that the empirical version will not work properly but simple versions of it will certainly not. see problem remark. malicious splitting. the impurity functions suggested above all avoid leaving the proportions of zeros and ones intact through splitting. they push wards more homogeneous regions. assume now that we do the opposite. through such splits we can in fact create hyperplane classification trees that are globally poor that is that are such that every trimmed version of the tree is also a poor classifier. such splitting methods must of course use the yi our example shows splitting criteria based on impurity functions that any general consistency theorem for hyperplane classification trees must come with certain restrictions on the splitting process-the x property is good times it is necessary to force cells to shrink sometimes the position of the split is restricted by empirical error minimization or some other criterion. the ham-sandwich theorem edelsbrunner states that given classo points and class-l points in nd d there exists a hyperplane cut that leaves n class-o points and m class-l points in each halfspace. so assume that x has a density and that p py i in a sample of size n let y be the majority class are broken in favor of class figure ham-sandwich cut each halfspace contains exactly half the points from each class o o regardless of the sample make-up if y we may construct a hyperplane-tree classifier in which during the construction every node represents a region in which the majority vote would be o. this property has nothing to do with the distribution of y and therefore for any trimmed version of the tree classifier and pln p py o if p obviously as we may take l these classifiers are hopeless. bibliographic remarks. empirical stoller splits without forced rotation were ommended by payne and meisel and rounds but their failure to be universally good was noted by gordon and olshen the last two thors recommended a splitting scheme that combined several ideas but roughly speaking they perform empirical stoller splits with forced rotation through the ordinate axes and olshen other splitting criteria include the aid criterion of morgan and sonquist which is a predecessor of the gini index of diversity used in cart friedman olshen and stone see also gelfand and delp guo and gelfand gelfand ravishankar and delp and ciampi michel-briand and milhaud also observed the failure of multivariate classification trees based on the aid criterion. the shannon entropy or modifications of it are recommended by talmon sethi and sarvarayudu wang and suen goodman and smyth and chou permutation statistics are used in li and dubes still without forced rotations through the coordinate axes. quinlan has a more involved splitting criterion. a general discussion on tree splitting may be tree classifiers found in the paper by sethi a class of impurity functions is studied in burshtein della pietra kanevsky and nadas among the pioneers of tree splitting perpendicular cuts are sebestyen and henrichon and fu for related work we refer to stoffel sethi and chatterjee argentiero chin and beaudet you andfu anderson andfu qing-yun and fu hartmann varshney mehrotra and gerberich and casey and nagy references on nonperpendicular splitting methods are given below in the section on esp trees. a consistent splitting criterion there is no reason for pessimism after the previous sections. rest assured there are several consistent splitting strategies that are fully automatic and depend only upon the populations of the regions. in this section we provide a solution for the simple case when x is univariate and nonatomic. it is possible to generalize the method for d if we force cuts to alternate directions. we omit here the detailed analysis for multidimensional cases for two reasons. first it is significantly heavier than for d secondly in the last section of this chapter we introduce a fully automatic way of building up consistent trees that is without forcing the directions of the splits. to a partition ai an of r we assign the quantity n q l noai where n noa l ixjeayjoj jl n ni l ixjeayjl jl are the respective numbers of points labeled with and falling in the region a. the tree-growing algorithm starts with the trivial partition and at each step it makes a cut that yields the minimal value of q. it proceeds recursively until the improvement in the value of q falls below a threshold remark. notice that this criterion always splits a cell that has many points from both classes the proof of the theorem below. thus it avoids the anomalies of impurity-function criteria described in the previous section. on the other hand it does not necessarily split large cells if they are almost homogeneous. for a comparison recall that the gini criterion minimizes the quantity noai o noai ni thus favoring cutting cells with very few points. we realize that the criterion q introduced here is just one of many with similarly good properties and albeit probably imperfect it is certainly one of the simplest. bsp trees theorem let x have a nonatomic distribution on the real line and consider the tree classifier obtained by the algorithm described above. if the threshold satisfies n and n then the classification rule is strongly consistent. proof. there are two key properties of the algorithm that we exploit property if minnoa ni for a cell a then a gets cut by the algorithm. to see this observe that the argument a from the notation if no n and we cut a so that the number of o-labeled points in the two child regions differ by at most one then the contribution of these two new regions to q is r n n r where n and n are the numbers of points in the two child regions. thus the decrease of q if a is split is at least if minno nd then d.n and a cut is made. property no leaf node has less than n points. assume that after a region is cut in one of the child regions the total number of points is n n k. then the improvement in q caused by the split is bounded by nonl n n s nonl knl k s s nk. therefore if k in then the improvement is smaller than thus no cut is made that leaves behind a child region with fewer than i n points. it follows from property that if a leaf region has more than points then since the class in minority has less than points in it it may be cut into intervals containing between and points without altering the decision since the majority vote within each region remains the same. summarizing we see that the tree classifier is equivalent to a classifier that partitions the real line into intervals each containing at least and at most data points. thus in this partition each interval has a number of points growing to infinity as on. we emphasize that the number of points in a region of the studied tree classifier may be large but such regions are almost homogeneous and therefore the classifier is equivalent to another classifier which has on points in each region. consistency of such partitioning classifiers is proved in the next chapter-see theorem d bsp trees binary space partition trees bsp trees partition rd by hyperplanes. trees of this nature have evolved in the computer graphics literature via the work of fuchs tree classifiers kedem and naylor and fuchs abram and grant also samet kaplan and sung and shirley in discrimination they are at the same time generalizations of linear discriminants of histograms and of binary tree classifiers. bsp trees were recommended for use in discrimination by henrichon and fu mizoguchi kizawa and shimura and friedman further studies include sklansky and michelotti argentiero chin and beaudet qing-yun and fu breiman friedman olshen and stone loh and vanichsetakul and park and sklansky there are numerous ways of constructing bsp trees. most methods of course use the y to determine good splits. nevertheless we should mention first simple splits with the x if only to better understand the bsp trees. figure a raw bsp tree and its induced partition in the plane. every region is split by a line determined by the two data points with smallest index in the region. we call the raw bsp tree the tree obtained by letting xl xd determine the first hyperplane. the d data points remain associated with the root and the others x k are sent down to the subtrees where the process is repeated as far as possible. the remaining points xkl xn are used in a majority vote in the external regions. note that the number of regions is not more than kid. thus by lemma if kin we have nx in probability. combining this with problem we have our first result theorem the natural binary tree classifier based upon a raw bsp tree with k and kin is consistent whenever x has a density. hyperplanes may also be selected by optimization of a criterion. typically this would involve separating the classes in some way. all that was said for perpen dicular splitting remains valid here. it is worthwhile recalling therefore that there primitive selection are distributions for which the best empirical stoller split does not improve the probabilit of error. take for example the uniform distribution in the unit ball of nd in which y if iixii o if iixii figure no single split improves on the error ability for this distribution. here no linear split would be helpful as the l s would always hold a strong majority. minimizing other impurity functions such as the gini criterion may be helpful however bibliographic remarks. hyperplane splits may be generalized to include quadratic splits and fu for example mui and fu suggest taking d d and forming quadratic classifiers as in normal discrimination chapter based upon vectors in r d. the cuts are thus perpendicular to d d axes but quadratic in the subspace r d tance or clustering for determining splits. as a novelty within each leaf region the decision is not by majority vote but rather by a slightly more cated rule such as the k-nn rule or linear discrimination. here no optimization is required along the way. loh and vanichsetakul allow linear splits but use f ratios to select desirable hyperplanes. lin and fu employ the bhattacharyya primitive selection there are two reasons for optimizing a tree configuration. first of all it just does not make sense to ignore the class labels when constructing a tree classifier so the yis must be used to help in the selection of a best tree. secondly some trees may not be consistent provably consistent yet when optimized over a family of trees consistency drops out. we take the following example let be a class of binary tree classifiers with the x with the space partitioned into k regions determined by xl xk only. examples include the chronological k-d tree and some kinds of bsp trees. we estimate the error for g e by realizing the danger of using the same data that were used to obtain majority votes to estimate the error. an optimistic bias will be introduced. more on such error estimates see chapter let g be the classifier one of the classifiers in for which ln is minimum. assume that oo-for example could tree classifiers be all k! chronological k-d trees obtained by permuting xl xk. we call g then the permutation-optimized chronological k-d classifier. when k l jiognj one can verify that k! one for any e so that the computational burden-at least on paper-is not out of sight. we assume that has a consistent classifier sequence that is as n k usually grows unbounded and elngk l for a sequence gk e example. among the chronological k-d trees modulo permutations the first one the one corresponding to the identity permutation was shown to be consistent in theorem if x has a density k and kin o. example. let be the class of bsp trees in which we take as possible hyperplanes for splitting the root the hyperplane through x i x d the d hyperplanes through x i x d l that are parallel to one axis the hyperplanes through x i that are parallel to two axes the cl d hyperplanes through xl that are parallel to d axes. thus conservatively estimated i k because there are at most possible choices at each node and there are k! permutations of x i x k. granted the number of external regions is very variable but it remains bounded by k in any case. as contains the chronological k-d tree it has a consistent sequence of classifiers when k and nlk log k theorem let gl lng. then has a consistent sequence of classifiers if the number of regions in the partitions for all g e are at most k andifk and nl log then elng l where lng is the conditional probability of error of g. in the examples cited above we must take k n i k furthermore log i o kl ok log k in both cases. thus gl is consistent whenever x has a density k and k onl log n. this is a simple way of constructing a basically universally consistent bsp tree. proof. let g arg ln lng l lng lng lng lng lng l. clearly def i i i. obviously i in the mean by our assumption and for e primitive selection next we bound the probabilities on the right-hand side. let pw pil denote px e region i y o and px e region i y respectively with regions determined by g i k let n nw l ixjefegion iyjo jkl and nil l ixjeregion iyji n jkl then and thus lng l pilinionid l pwinionid kl ilng l pil l pig io_ i i n n k i il i il n n k introduce the notation z for the random variable on the right-hand side of the above inequality. by the cauchy-schwarz inequality kl l il e nokr ix i xc given xl xb nil is binomial k pil np j npok rnk another use of the cauchy-schwarz inequality n-k tree classifiers thus ezixl x k as kin o. note that pz xd p z ezixi i xl xk e ezixi xk which happens when n exp k mcdiarmids inequality as changing the value of an x j j k changes z by at most k see theorem exp thus taking expected values we see that if k this tends to for all e if kin and nllog constructing consistent tree classifiers thus far we have taken you through a forest of beautiful trees and we have shown you a few tricks of the trade. when you read write a research paper on tree classifiers and try to directly apply a consistency theorem you will get frustrated however-most real-life tree classifiers use the data in intricate ways to suit a certain application. it really helps to have a few truly general results that have universal impact. in this section we will point you to three different places in the book where you may find useful results in this respect. first of all there is a consistency theorem-theorem applies to rules that partition the space and decide by majority vote. the partition is arbitrary and may thus be generated by using some or all of xl x n yn if a rule satisfies the two conditions of theorem it must be universally consistent. to put it differently even the worst rule within the boundaries of the theorems conditions must perform well asymptotically. second we will briefly discuss the design of tree classifiers obtained by mizing the empirical error estimate over possibly infinite classes of classifiers. such classifiers however hard to find by an algorithm have asymptotic properties that are related to the vc dimension of the class of rules. consistency follows most without work if one can calculate or bound the vc dimension appropriately. constructing consistent tree classifiers while chapters and deal in more detail with the vc dimension it is necessary to give a few examples here. third we point the reader to chapter on data splitting where the previous approach is applied to the minimization. of the holdout estimate obtained by trees based upon part of the sample and using another part to select the best tree in the bunch. here too the vc dimension plays a crucial role. theorem allows space partitions that depend quite arbitrarily on all the data and extends earlier universally applicable results of gordon and olshen and breiman friedman olshen and stone a particularly useful format is given in theorem if the partition is by recursive hyperplane splits as in bsp trees and the number of splits is at most m n if mn log n i n and if diamax n sb with probability one for all sb sb is the ball of radius b centered at the origin then the classification rule is strongly consistent. the last condition forces a randomly picked region in the partition to be small. however mn log n i n guarantees that no devilish partition can be inconsistent. the latter condition is certainly satisfied if each region contains at least kn points where knl log n next we take a look at full-fledged minimization of ln the empirical error over certain classes of tree classifiers. here we are not concerned with the unacceptable computational effort. for example let qk be the class of all binary tree classifiers based upon a tree consisting of k internal nodes each representing a hyperplane cut in the bsp tree and all possible labelings of the k leaf regions. pick such a classifier for which is minimal. observe that the chosen tree is always natural that is it takes majority votes over the leaf regions. thus the minimization is equivalent to the minimization of the resubstitution error estimate in chapter over the corresponding class of natural tree classifiers. we say that a sequence of classes is rich if we can find a sequence gk e qk such that lgk l for hyperplanes this is the case if k as n oo-just make the hyperplane cuts form a regular histogram grid and recall theorem let sqb n be the shatter coefficient of qk a definition see chapter definition for example for the hyperplane family sqk n nkdl. then by corollary we have elng l for the selected classifier g when qk is rich here and log sqk n on that is ko logn n tree classifiers observe that no conditions are placed on the distribution of y here! tency follows from basic notions-one combinatorial to keep us from overfitting and one approximation-theoretical richness. the above result remains valid under the same conditions on k in the following classes all trees based upon k internal nodes each representing a perpendicular split. sh n ldk. all trees based upon k internal nodes each representing a quadtree split. sh n d ll. a greedy classifier in this section we define simply a binary tree classifier that is grown via tion of a simple criterion. it has the remarkable property that it does not require a forced rotation through the coordinate axes or special safeguards against small or large regions or the like. it remains entirely parameter-free is picked by the user is monotone transformation invariant and fully automatic. we show that in nd it is always consistent. it serves as a prototype for teaching about such rules and should not be considered as more than that. for fully practical methods we believe one will have to tinker with the approach. the space is partitioned into rectangles as shown below cj figure a tree based on partitioning the plane into rectangles. the right subtree of each internal node belongs to the inside of a rectangle and the left subtree belongs to the complement of the same rectangle denotes the complement of i. rectangles are not allowed to overlap. a hyperrectangle defines a split in a natural way. the theory presented here applies for many other types of cuts. these will be discussed after the main sistency theorem is stated. a greedy classifier a partition is denoted by p and a decision on a set a e p is by majority vote. we write gp for such a rule gpx if l yi yi x e a ixea otherwise. ixea given a partition p a legal rectangle a is one for which a n b or a s b for all sets b e p. if we refine p by adding a legal rectangle t somewhere then we obtain the partition t. the decision gy agrees with gp except on the set b e p that contains t. we introduce the convenient notation px e a y j e i an estimate of the quality of gp is where n l ixergpxj!y minvonr vinr n ii here we use two different arguments for ln and p but the distinction should be clear. we may similarly define lnt. given a partition p the greedy classifier selects that legal rectangle t for which lnt is minimal any appropriate policy for breaking ties. let r be the set of p containing t. then the greedy classifier picks that t for which is minimal. starting with the trivial partition po d we repeat the previous step k times leading thus to k regions. the sequence of partitions is denoted by po pi pk. we put no safeguards in place-the rectangles are not forced to shrink. and in fact it is easy to construct examples in which most rectangles do not shrink. the main result of the section and indeed of this chapter is that the obtained classifier is consistent theorem for the greedy classifier with k and k j n log n assuming that x has nonatomic marginals we have ln l with probability one. tree classifiers remark. we note that with techniques presented in the next chapter it is possible to improve the second condition on k to k vi n log n problem o before proving the theorem we mention that the same argument may be used to establish consistency of greedily grown trees with many other types of cuts. we have seen in section that repeated stoller splits do not result in good classifiers. the reason is that optimization is over a collection of sets that is not guaranteed to improve matters-witness the examples provided in previous sections. a good cutting method is one that includes somehow many not too many small sets. for example let us split at the root making d hyperplane cuts at once that is by finding the d cuts that together produce the largest decrease in the empirical error probability. then repeat this step recursively in each region k times. the procedure is consistent under the same conditions on k as in theorem whenever x has a density problem the d hyperplane cuts may be considered as an elementary cut which is repeated in a greedy manner. in figures to we show a few elementary cuts that may be repeated greedily for a consistent classifier. the straightforward proofs of consistency are left to the reader in problem proof of theorem we restrict ourselves to n but the proof remains similar in n d the notation ln was introduced above where the argument is allowed to be a partition or a set in a partition. we similarly define lr min px e r y iy yeo.i lp minvor vir l lr. rep figure an elementary cut here is composed of d hyperplane cuts. they are jointly optimized. figure rectangular cuts determine an elementary cut. all cuts have arbitrary directions there are no forced directions. a greedy classifier figure simplex cuts. a cut is determined by a polyhedron with d vertices. the simplices are not allowed to overlap just as for rectangular cuts in the greedy classifier. three consecutive simplex cuts in are shown here. mal. two consecutive rectangular grid figure at every iteration we fine the partition by selecting that angle r in the partition and that x x x rectangular grid cut of r for which the empirical error is cuts are shown. for example let denote a partition of r into a rectangular l x i grid. figure an i x i grid i here. it is clear that for all e there exists an i i and an i x i grid such that if q is another finer partition into rectangles each set of q is a rectangle and intersects at most one rectangle of then necessarily lq l we will call q a refinement of the next lemma is a key property of partitioning classifiers. in our eyes it is the main technical property of this entire chapter. we say that the partition t is an extension of p by a set q-where q rep-if t contains all cells of p other than r plus q and r q. tree classifiers lemma let oz be a finite partition with l l e. let p be a finite partition oird and let q be a refinement oiboth p and oz. then there exists a set q e q q is contained in one set oip only and an extension oip by q to tq such that if lp l e proof. first fix rep and let q q n be the sets of q contained in r. define il lr minp q first we show that there exists an integer i such that lr lq lr q lr lq or equivalently where il i minp q minpi qi minp pi q qi. to see this assume without loss of generality that p q. if pi qi for all i then minp q i minpi qi p i pi n n ll il so we are done. assume therefore that pi qi for i e a where a is a set of indices with i a i for such i and thus i ili ipi qj p i pi iqi p lminpi qj. n iea lea i iea il but then ail a iai iea i max u lsisn u i p ll minpi qi iai p ll minpi qi n and the claim follows. to prove the lemma notice that since lq l e it suffices to show that a greedy classifier maxlp ltq qeq lp lq q i i or equivalently that maxlr q lq lr q q qeq lp lq q i i where rq is the unique cell of p containing q. however maxlr q lq lr q q qeq max lq lr q repqr lr lqcr lq iri max rep the inequality shown above where i r i denotes the number of sets of q contained in r lrep lqr lq lrep iri lrep lr lqeq lq iqi lp lq iqi and the lemma is proved. let us return to the proof of theorem the previous lemma applied to our situation pi shows that we may extend pi by a rectangle q e q q will e is be a collection of rectangles refining both qz and pi such that ltq smaller by a guaranteed amount than lpi e. is the partition obtained by extending pi. to describe q we do the following that q must consist entirely of rectangles for otherwise the lemma is useless take the rectangles of pi and extend all four sides the order of birth of the rectangles until they hit a side of another rectangle or an extended border of a previous rectangle if they hit anything at all. figure illustrates the partition into rectangles. note that this partition consists of line segments and at most rectangles can be seen by noting that we can write in each non original rectangle of pi the number of an original neighboring rectangle each number appearing at most times. tree classifiers figure extensions of rectangles to get a rectangular grid. the rectangular grid partition thus obtained is intersected with to yield q. to apply lemma we only need a bound on i qi. to the rectangular partition just created we add each of the lines of the grid one by one. start with the horizontal lines. each time one is added it creates at most new rectangles. then each vertical line adds at most new rectangles for a total of not more than l hence i qi apply the lemma and observe that there is a rectangle q e q such that the extension of pi by q to p would yield at this point in the proof the reader can safely forget q. it has done what it was supposed to do. of course we cannot hope to find p because we do not know the distribution. let us denote the actual new partition by and let ril be the selected rectangle by the empirical minimization. observe that lpid lp lnpid lnp lp lp as pi and n have most sets in comm many terms cancel in the last ble sum. we are left with just l and ln terms for sets of the form r ril r q q with r e pi. thus lp sup ilnr lri r where the supremum is taken over all rectangles of the above described form. these are sets of the form obtainable in pi every such set can be written as the difference of a infinite rectangle and at most i nonoverlapping contained rectangles. as i k in our analysis we call zk the class of all sets a greedy classifier zk where zo zl zk that may be described in this form zo zl are rectangles each zi zo zl zk are mutually disjoint. rectangles may be infinite or half infinite. hence for i k lf sup ilnz lzi. zezk fof a fixed z e zk l nz lzi minvonz minvoz ivonz thus l j vn sup p def ivonz define the event zezk g where will be picked later. on g we see that for all i k e lp e e l we now introduce a convenient lemma. lemma let an bn be sequences of positive numbers with bn t bo and let be fixed. if an bn then proof. we have an! aoe-ejob j ao bj i i i i i i. clearly i ao exp lo b j a trivial bound for i i i i i is conclude that on g e e- llsl tree classifiers thus plpk l p ge o when ok and e finally introduce the notation lnr lnp px e r y i gpx l lnr. rep as lpk takes the partition into account but not the majority vote and lnpk does we need a small additional argument lnpk lnpk lpk lpk l ivi vlnri lpk repk putting things together p l p e p l p e p o under the given conditions on and k. observe that if for a set z def unz ivoz ivlz vlnzi then vn sup unz sup rectangles z disjoint sets of k rectangles zi k lunzi il sup rectangles z wn sup unz. rectangles z we may now use the vapnik-chervonenkis inequality and to conclude that for all e p sup unz e rectangles z problems and exercises armed with this we have pwn e pvn o e both terms tend to with n if n and n take e i to satisfy our earlier side condition and note that we need l by log n we in fact have strong convergence to for lnpk the borel-cantelli lemma. the crucial element in the proof of theorem is the fact that the number of sets in the partition q grows at most linearly in i the iteration number. had it grown quadratically say it would not have been good enough-we would not have had guaranteed improvements of large enough sizes to push the error probability towards l in the multidimensional version and in extension to other types of cuts this is virtually the only thing that must be verified. for hyperplane cuts an additional inequality of the vc type is needed extending that for classes of hyperplanes. our proof is entirely combinatorial and geometric and comes with explicit bounds. the only reference to the bayes error we need is the quantity ie which is the smallest value l for which inf alllxl grids lyl l e. it depends heavily on the distribution of course. calli the grid complexity of the distribution for lack of a better term. for example if x is discrete and takes values on the hypercube ld then ie if x takes values on amd then ie ld. if x has an arbitrary distribution on the real line and is monotone then l e however if is unimodal l e in one dimension ie is sensitive to the number of places where the bayes decision changes. in two dimensions however l measures the complexity of the distribution especially near regions with when x is uniform on and y if and only if the components of x sum to less than one then i i as e for example. the grid complexity is eminently suited to study rates of convergence as it is explicitly featured in the inequalities of our proof. there is no room in this book for following this thread however. problems and exercises problem let x be a random vector with density f on nd. show that each component has a density as well. problem show that both condition and condition of theorem are necessary for consistency of trees with the x tree classifiers problem for a theoretical median tree in n d with uniform marginals and k levels of splitting show that ehx vex when k is a multiple of d. d problem a-balanced trees. consider the following generalization of the median tree at a node in the tree that represents n points if a split occurs both subtrees must be of size at least an for some fixed a o. repeat the splits for k levels resulting in leaf regions. the tree has the x and the classifier is natural. however the points at which cuts are made are picked among the data points in an arbitrary fashion. the splits rotate through the coordinate axes. generalize the consistency theorem for median trees. problem consider the following tree classifier. first we find the median according to one coordinate and create two subtrees of sizes len and rn repeat this at each level cutting the next coordinate axis in a rotational manner. do not cut a node any further if either all data points in the corresponding region have identical yi values or the region contains less than k points. prove that the obtained natural classifier is consistent whenever x has a density if k and log n k o. problem let m be a probability measure with a density f on n d and define c x cd m xci e x x xed en all e o. show that m c hint proceed as in the proof of lemma a.l in the appendix. problem let ul u cn be uniform order statistics defining spacings sl snl with si ui uu-l if unl and uo o. show that sl snl are identically distributed psl x xy x e esil ln problem in the proof of theorem we assumed in the second part that horizontal and vertical cuts were meted out independently. return to the proof and see how you can modify it to take care of the forced alternating cuts. problem let xl x xn be i.i.d. with common density f on the real line. call xi a record if xi minx i xj. let k fx is a record show that r l rn are independent and that p i i. conclude that the expected number of records is logn. problem the deep k-d tree. assume that x has a density f and that all marginal densities are uniform. show that k implies that diamax in probability. do this by arguing that diamax in probability for the chronological k-d tree with the same parameter k. in a random k tree with n elements show that the depth d of the last inserted node satisfies d i log n i in probability. argue first that you may restrict yourself to d then write d as a sum of indicators of records. conclude by computing ed and vard or at least bounding these quantities in an appropriate manner. improve the second condition of theorem to log nijlog n that it is possible that lim supn-hxl k i log n hint show that i d log n i jlog n in probability by referring to the previous part of this exercise. problems and exercises problem consider a full-fledged k-d tree with n external node regions. let ln be the probability of error if classification is based upon this tree and if external node regions are assigned the labels of their immediate parents figure figure the external nodes are signed the classes of their parents. leaf region show that whenever x has a density e ln lnn just as for the i-nearest neighbor rule. problem continuation. show that ln lnn in probability. problem meisel and propose a binary tree classifier with perpendicular cuts in which all leaf regions are homogeneous that is all yis for the xis in the same region are identical. figure example of a tree partition into homogeneous regions if l give a stopping rule for constructing a consistent rule whenever x has a density. if l show that there exists a consistent homogeneous partition classifier with on expected number of cells. if l show a complicated way of constructing a homogeneous tition that yields a tree classifier with e ln l whenever x has a density. hint first make a consistent nonhomogeneous binary tree classifier and refine it to make it homogeneous. if l then show that the expected number of homogeneous regions is at least en for some e such rules are therefore not practical unless l overfitting will occur. problem let x be uniform on and let y be independent of x with py i p. find a tree classifier based upon simple interval splitting for which each region has one data point and hmmf p-o lim infn-oo eln p tree classifiers we know that for the nearest neighbor rule hm p-o lim supn-cxl eln p so that the given interval splitting method is worse than the nearest neighbor method. provides a counterexample to a conjecture of breiman friedman olshen and stone hint sort the points and adjust the sizes of the intervals given to the class and class points in favor of the by doing so the asymptotic probability of error can be made as close as desired to p. problem continuation. let x be uniform on and let y be independent of x with p i p. find a data-based tree classifier based upon perpendicular cuts for which each region has one data point diamax in probability theorems and and hmmf p-o lim infn- cxl eln p conclude that in n d we can construct tree classifiers with one point per cell that are much worse than the nearest neighbor rule. hint the next two problems may help you with the construction and analysis. problem continuation. draw ani.i.d. sample xl xn from the butionon and let yi yn bei.i.d. and independent of the xis withpy i p. construct a binary tree partition with perpendicular cuts for yi i such that every leaf region has one and only one point and diamax in probability. how would you proceed avoiding putting xis on borders of regions? prove diamax in probability. add the yi pairs with yi to the leaf regions so that every region has one observation and zero or more class-o observations. give the observation the largest area containing no class-o points as shown in figure show that this can always be done by adding perpendicular cuts and keeping at least one observation per region. figure cutting a rectangle by giving a large area to the single point. partition all rectangles with more than one point further to finally obtain a point-per-leaf-region partition. if there are n points in a region of the tree before the class-l and class-o points are separated n class-o points and one point then show that the expected proportion of the regions area given to class given n times n tends to explicit lower bound will be helpful. hint use the next problem. write the probability of error for the rule in terms of areas of rectangles and use part to get an asymptotic lower bound. problems and exercises now let p tend to zero and get an asymptotic expression for your lower bound in terms of p. compare this with p the asymptotic error probability of the i-nearest neighbor rule. problem continuation. draw a sample xl xn of n i.i.d. observations formly distributed on id. the rectangles defined by the origin and x i xn as posite vertices are denoted by r i rn respectively. the probability content of ri is clearly flri nl xij study mn maxflri i n ri does not contain r j for any j i the probability content of the largest empty rectangle with a vertex at the origin. for d mil is just the minimum of the xs and thus nmn e where e is the exponential distribution. also emn ln for d mn is larger. show that nmn in probability and try obtaining the first term in the rate of increase. problem show that diamax in probability for the chronological quadtree whenever k and x has a density. hint mimic the proof for the chronological quadtree. problem show that the deep quadtree is consistent if x has a density and k levels of splits are applied where k and kl log n o. leaf problem consider a full-fledged quadtree with n nodes thus regions. assign to each region the y-iabel of its parent in the quadtree. with this simple classifier show that whenever x has a density eln l nn in particular lim supn-. do e ln and the classifier is consistent when l o. problem in r partition the space as follows xl define nine regions by vertical and horizontal lines through them. x x k are sent down to the appropriate subtrees in the tree and within each subtree with at least two points the process is repeated recursively. a decision at x is by a majority vote yki yn among those xis in the same rectangle of the partition as x show that if k kin the rule is consistent whenever x has a density. problem on the two-dimensional counterexample shown in the text for multivariate stoller splits prove that if splits are performed based upon a sample drawn from the bution and if we stop after k splits with k depending on n in such a way that k i log n then l n the conditional probability of error satisfies lim infhdo elj hint bound the probability of ever splitting anywhere by noting that the maximal ence between the empirical distribution functions for the first coordinate of x given y and y is i in when restricted to problem let x have the uniform distribution on and let y so that l o. construct a binary classification tree by selecting at each iteration the split that minimizes the impurity function i where is the gini criterion. consider just the first three splits made in this manner. let ln be the probability of error with the given rule a majority vote over the leaf regions. show that ln in probability. analyze the algorithm when the gini criterion is replaced by the probability-of-error criterion. problem let x be uniform on and let y be independent of x with ply i draw a sample of size n from this distribution. investigate where the first cut tree classifiers might take place based upon minimizing the impurity function with p gini criterion. this is established you will have discovered the nature of the classification tree roughly speaking. problem complete the consistency proof of theorem for the raw bsp tree for n by showing that diamax in probability. problem balanced bsp trees. we generalize median trees to allow splitting the space along hyperplanes. figure a balanced bsp tree each hyperplane cut splits a region into two cells of the same cardinality. assume that x has a density and that the tree possesses the x keep splitting until there are leaf regions as with median trees. call the trees balanced bsp trees. show that there are ways of splitting in n that lead to nonconsistent rules gardless how k varies with n. if every splitting hyperplane is forced to contain d data points nd and these data points stay with the splitting node are not sent down to subtrees then show that once again there exists a splitting method that leads to nonconsistent rules regardless of how k varies with n. problem letx have a uniform distribution on the unit ball of rd. let y so that l o. assume that we split the space by a hyperplane by minimizing an impurity function based upon the gini criterion. if is very large where approximately would the cut take place a rotation of course? problem there exist singular continuous distributions that admit uniform marginals in n d. show for example that if x is uniformly distributed on the surface of the unit sphere of then its three components are all uniformly distributed on problem verify that theorem remains valid in rd. problem prove that theorem remains valid if rectangular cuts are replaced by any of the elementary cuts shown on figures to and such cuts are performed recursively k times always by maximizing the decrease of the empirical error. problem show that theorem remains valid if k ex and k j n log n. hint in the proof of the theorem the bound on and w is loose. you may get more efficient bounds by applying theorem from the next chapter. problem study the behavior of the grid complexity as e t for the following cases x is uniform on the perimeter of the unit circle of r with probability and x is uniform on otherwise. let y if and only if x is on the perimeter of that circle that l x x is uniform on f and y if and only if xl data-dependent partitioning introduction in chapter we investigated properties of the regular histogram rule. histogram classifiers partition the observation space nd and classify the input vector x cording to a majority vote among the ys whose corresponding xs fall in the same cell of the partition as x. partitions discussed in chapter could depend on the sample size n but were not allowed to depend on the data dn itself. we dealt mostly with grid partitions but will now allow other partitions as well. just consider clustered training observations xl x n near the clusters center finer partitions are called for. similarly when the components have different physical dimensions the scale of one coordinate axis is not related at all to the other scales and some data-adaptive stretching is necessary. sometimes the data are trated on or around a hyperplane. in all these cases although consistent the regular histogram method behaves rather poorly especially if the dimension of the space is large. therefore it is useful to allow data-dependent partitions while keeping a majority voting scheme within each cell. the simplest data-dependent partitioning methods are based on statistically equivalent blocks in which each cell contains the same number of points. in dimensional problems statistically equivalent blocks reduce to k-spacing estimates where the k-th etc. order statistics determine the partition of the real line. sometimes it makes sense to cluster the data points into groups such that points in a group are close to each other and define the partition so that each group is in a different cell. many other data-dependent partitioning schemes have been introduced in the lit data-dependent partitioning erature. in most of these algorithms the cells of the partition correspond to the leaves of a binary tree which makes computation of the corresponding classification rule fast and convenient. tree classifiers were dealt with in chapter analysis of versal consistency for these algorithms and corresponding density estimates was begun by abou-jaoude and gordon and olshen in a general framework and was extended for example by breiman friedman olshen and stone chen and zhao zhao krishnaiah and chen lugosi and nobel and nobel this chapter is more general than the chapter on tree classifiers as every partition induced by a tree classifier is a valid partition of space but not vice versa. the example below shows a rectangular partition of the plane that cannot be obtained by consecutive perpendicular cuts in a binary classification tree. figure a rectangular partition of that cannot be achieved by a tree of consecutive cuts. in this chapter we first establish general sufficient conditions for the consistency of data-dependent histogram classifiers. because of the complicated dependence of the partition on the data methods useful for handling regular histograms have to be significantly refined. the main tool is a large deviation inequality for families of partitions that is related to the vapnik inequality for families of sets. the reader is asked for forgiveness-we want to present a very generally applicable theorem and have to sacrifice by increasing the density of the text. however as you will discover the rewards will be sweet. a vapnik inequality for partitions this is a technical section. we will use its results in the next section to establish a general consistency theorem for histogram rules with data-dependent partitions. the main goal of this section is to extend the basic vapnik-chervonenkis inequality to families of partitions from families of sets. the line of thought followed here essentially appears in zhao krishnaiah and chen for angular partitions and more generally in lugosi and nobel a substantial simplification in the proof was pointed out to us by andrew barron. by a partition of rd we mean a countable collection p of subsets of rd such that ujl a j rd and ai n a j if i j. each set a j is called a cell of the partition p. a vapnik-chervonenkis inequality for partitions let m be a positive number. for each partition p we define pm as the tion ofp to the ball sm that sm c rd denotes the closed ball of radius m centered at the origin. in other words pm is a partition of sm whose cells are obtained by intersecting the cells oip with sm. we assume throughout that pis such that ipm i for each m we denote by bpm the collection of all sets obtained by unions of cells of pm. just as we dealt with classes of sets in chapter here we introduce lies of partitions. let f be a infinite collection of partitions of rd. fm p e f denotes the family of partitions of sm obtained by stricting members of f to sm. for each m. we will measure the complexity of a family of partitions fm by the shatter coefficients of the class of sets obtained as unions of cells of partitions taken from the family pm. formally we define the combinatorial quantity fl.n as follows introduce the class a of subsets ofrd by am e bpm for some pm e pm and define fl.npm sam n the shatter coefficient of a a is thus the class of all sets that can be obtained as unions of cells of some partition of s m in the collection pm. for example if all members of fm partition sm into two sets then fl.npm is just the shatter coefficient of all sets in these partitions and sm included in the collection of sets. let jl be a probability measure on rd and let xl x be i.i.d. random vectors in rd with distribution jl. for n let jln denote the empirical distribution of xl x n which places mass lin at each of xl x n to establish the consistency of data-driven histogram methods we require information about the large deviations of random variables of the form sup l pmefm aepm where f is an appropriate family of partitions. remark. just as in chapter the supremum above is not guaranteed to be measurable. in order to insure measurability it is necessary to impose regularity conditions on uncountable collections of partitions. it suffices to mention that in all our applications the measurability can be verified by checking conditions given e.g. in dudley or pollard the following theorem is a consequence of the vapnik inequality theorem and nobel let xl xn be i.i.d. random vectors in rd with measure jl and empirical measure jln. let f be a collection partitions ard. thenor each m and e p sup l ijlna jlai e e-ne pmepm aepm data-dependent partitioning proof. for a fixed partition p define a as the set a u a. then l iflna aepm fla fla flsm flnsm sup aebpm iflna flsm flnsm recall that the class of sets bpm contains sets obtained by unions of cells of pm. therefore sup pm efw aebpmusm sup iflna flsm flnsm observe that the first term on the right-hand side of the inequality is a uniform deviation of the empirical measure fln from fl over a specific class of sets. the class contains all sets that can be written as unions of cells of a partition pm in the class of partitions this class of sets is just a defined above. the theorem now follows from the vapnik-chervonenkis inequality the definition of and hoeffdings inequality we will use a special application of theorem summarized in the following corollary corollary let yl be a sequence ofi.i.d. random pairs in rd x i and let be a sequence offamilies of partitions of rd. if form lim log n-ioo n then sup l ltyjxiea-eyixeaio pmefm aepm n il with probability one as n tends to infinity. a vapnik-chervonenkis inequality for partitions proof. let v be the measure of y on rd x i and let vn be the empirical measure corresponding to the sequence yd yn. using fm we define a family gm of partitions ofrd x i by gm x pme fm u x pm e fm where p x a x x x we apply theorem for these families of partitions. def sup l i tyjxi ea sup l ivna x va x sup l ivna vai pcmefjm aepcm pcmefjm aepcm n il pcmeym aepcm it is easy to see that therefore the stated convergence follows from theorem and the borel-cantelli lemma.d lemma assume that the family fm is such that the number of cells of the partitions in the family are uniformly bounded that is there is a constant n such that ipmi n for each pm e fm. then where is maximal number of different ways n points can be partitioned by members of fm. example. flexible grids. as a first simple example let us take in fn all partitions into d-dimensional grids flexible grids as they may be visualized as wire fences with unequally spaced vertical and horizontal wires in which cells are made up as cartesian products of d intervals and each coordinate axis contributes one of mn intervals to these products. clearly if p is a member partition of fn then ipi m. nevertheless there are uncountably many ps as there are uncountably many intervals of the real line. this is why the finite quantity comes in so handy. we will verify later that for each m s d so that the condition of corollary is fulfilled when m d lim o. n-oo n data-dependent partitioning figure afiexible grid partition. consistency in this section we establish a general consistency theorem for a large class of data-based partitioning rules. using the training data dn we produce a partition pn jrnx r yr x n yn according to a prescribed rule jrn. we then use the partition p n in conjunction with dn to produce a classification rule based on a majority vote within the cells of the partition. that is the training set is used twice and it is this feature of data-dependent histogram methods that distinguishes them from regular histogram methods. formally an n-sample partitioning rule for nd is a function jrn that associates every n-tuple of pairs yi yn e n d x i with a measurable partition of nd. associated with every partitioning rule jrn there is a fixed random family of partitions yi xn yn yi yn e nd x i. is the family of partitions produced by the partitioning rule jrn for all possible realizations of the training sequence dn. when a partitioning rule jrn is applied to the sequence dn yr yn it produces a random partition pn jrncdn e in what follows we suppress the dependence of pn on dn for notational simplicity. for every x e nd let anx be the unique cell of pn that contains the point x. now let be a fixed sequence of partitioning rules. the classification rule gn gnc dn is defined by taking a majority vote among the classes appearing in a given cell of pn that is we emphasize here that the partition pn can depend on the vectors xi and the labels yi first we establish the strong consistency of the rules for a wide class of partitioning rules. as always diama denotes the diameter of a set a that is the maximum euclidean distance between any two points of a diama sup yi. xyea consistency theorem and nobel let be afixed sequence of partitioning rules and for each n let fn be the collection of partitions associated with the n-sample partitioning rule if for each m hm n--oo log n and for all balls sb and all y lim jj.- diamanx n sm y n--oo with probability one then the classification rule corresponding to satisfies with probability one. in other words the rule is strongly consistent. remark. in some applications we need a weaker condition to replace in the theorem. the following condition will do for every y and e jj.-x diamailx n t y with probability one. lim n-oo tcrdjltl-o inf the verification of this is left to the reader the proof given below requires quite some effort. the utility of the theorem is not immediately apparent. the length of the proof is indicative of the generality of the conditions in the theorem. given a data-dependent partitioning rule we must verify two things condition merely relates to the richness of the class of partitions of rd that may possibly occur such as flexible grids. condition tells us that the rule should eventually make local decisions. from examples in earlier chapters it should be obvious that is not necessary. finite partitions of rd necessarily have component sets with infinite diameter hence we need a condition that states that such sets have small jj.--measure. condition requires that a randomly chosen cell have a small diameter. thus it may be viewed as the version of condition of theorem however the weaker version of condition stated in the above remark is more subtle. by considering examples in which jj.- has bounded support more than just balls s m are needed as the sets of the partition near the boundary of the support may all have infinite diameter as well. hence we introduced an infimum with respect to sets t over all t with jj.-t o. it suffices to mention that is satisfied for all distributions in some of the examples that follow. theorem then allows us to conclude that such rules are strongly universally consistent. the theorem has done most of the digestion of data-dependent partitioning such proofs so we are left with virtually no work at all. consistency results win follow like dominos falling. proof of theorem observe that the partitioning classifier gn can be rewritten in the form if ll iyilixeanx ll iyoixiea!lx flanx otherwise. flanx introduce the notation x yi l....-ll i nmanx for any e there is an m e such that px tf. sm e. thus by an application of theorem we see that lgn l pgnx y x e smidn pgx y x e sm e jsm jsm e where ydixeanx n nmanx by symmetry since e is arbitrary it suffices to show that for each m with probability one. js m fix e and let r rd r be a continuous function with bounded support such that ism rximdx e. such r exists by theorem now define the auxiliary functions and note that both ryn and fn are piecewise constant on the cells of the random partition pn we may decompose the error as follows i consistency the integral of the first term on the right side is smaller than e by the definition of r. for the third term we have r ifnx l ie y ixea i dn e rxixea i dn i a epm r rxipdx e. consider the fourth term on the right-hand side of clearly it follows from the first condition of the theorem and corollary of theorem that r with probability one. finally we consider the second term on the right-hand side of using fubinis theorem we have data-dependent partitioning fix e as r is uniformly continuous there exists a number y osuch that if diama y then irx for every x yea. in addition there is a constant m such that i rx i m for every x e rd. fix n now and consider the integrals fla ja ja ryimdxmdy appearing in the sum above. we always have the upper bound majaja ryimdxmdy assume now that a e pm has diama y. then we can write summing over the cells a e pim with ma these bounds give irx jsm rnximdx l diamanx y letting n tend to infinity gives lim sup n--oo rnximdx with probability one by the second condition of the theorem. summarizing lim sup with probability one. j sm since e and are arbitrary the theorem is proved. statistically equivalent blocks in this section we apply theorem both to classifiers based on uniform spacings in one dimension and to their extension to multidimensional problems. we refer to these as rules based on statistically equivalent blocks. the order statistics of the components of the training data are used to construct a partition into rectangles. all statistically equivalent blocks such classifiers are invariant with respect to all strictly monotone transformations of the coordinate axes. the simplest such rule is the k-spacing rule studied in chapter theorem generalizations are possible in several ways. theorem allows us to handle partitions depending on the whole data sequence-and not only on the xis. the next simple result is sometimes useful. theorem consider a data-dependent partitioning classifier on the real line that partitions r into intervals in such a way that each interval contains at least an and at most bn points. assume that x has a nonatomic distribution. then the classifier is strongly consistent whenever an and bnln as n proof. we check the conditions of theorem let contain all partitions of n into m r n i an l intervals. since for each m all partitions in fm have at most m cells we can bound according to the lemma by the lemma d.nfm does not exceed times the number of different ways n points can be partitioned into m intervals. a little thought confirms that this number is and therefore leth denote the binary entropy function hx logx x loge x for x e note that h is symmetric about and that h is increasing for x by the inequality of theorem log g shtls. therefore it is easy to see that mnmh nm nlan as when x the condition an implies that which establishes condition to establish condition of theorem we proceed similarly to the proof of theorem fix y e o. there exists an interval m such that jl m my e and consequently jlx diamanx y e flx diamanx y n md where anx denotes the cell of the partition p n containing x. among the intervals of pn there can be at most i y disjoint intervals of length greater than y in data-dependent partitioning mj. thus we may bound the second term on the right-hand side above by flx diamanx y n m aepn max fla flna max ifla sup ifla y y y flnai n aea flnai where a is the set of all intervals in r. the first term in the parenthesis converges to zero by the second condition of the theorem while the second term goes to zero with probability one by an obvious extension of the classical glivenko-cantelli theorem summarizing we have shown that for any y e lim sup fl diam an y e with probability one. n---cxj this completes the proof. the d-dimensional generalizations of k-spacing rules include rules based upon statistically equivalent blocks that is partitions with sets that contain k points each. it is obvious that one can proceed in many ways see for example anderson patrick patrick and fisher quesenberry and gessaman and gessaman and gessaman as a first example consider the following algorithm the k-th smallest xlt coordinate among the training data defines the first cut. the rectangle with n k points is cut according to the isolating another k points. this can be repeated on a rotational basis for all coordinate axes. unfortunately the classifier obtained this way is not consistent. to see this observe that if k is much smaller than n-a clearly necessary requirement for consistency-then almost all cells produced by the cuts are long and thin. we sketched a distribution in figure for which the error probability of this classifier fails to converge to l the details are left to the reader this example highlights the importance of condition of theorem that is that the diameters of the cells should shrink in some sense as n statistically equivalent blocks figure a nonconsistent k-block rithm k in the picture. in the shaded area and in the white area. o o o rules have been developed in which the rectangular partition depends not only upon the xis in the training sequence but also upon the yis e.g. henrichon and fu meisel and michalopoulos and friedman for ample friedman cuts the axes at the places where the absolute differences between the marginal empirical distribution functions are largest to insure minimal ical error after the cut. his procedure is based upon stoller splits chapters and rules depending on the coordinatewise ranks of data points are interesting cause they are invariant under monotone transformations of the coordinate axes. this is particularly important in practice when the components are not physically comparable. is an adjective often used to point out a property that is universally valid. for such methods in statistics see the survey of das gupta statistically equivalent sets in partitions are called free because the measure ma of a set in the partition does not depend upon the distribution of x. we already noted a similar distribution-free behavior for k trees and median trees there is no reason to stay with rectangular-shaped sets and benning beakley and tuteur but doing so greatly simplifies the interpretation of a classifier. in this book to avoid confusion we reserve the term for consistency results or other theoretical properties that hold for all distributions of y. it is possible to define consistent partitions that have statistically equivalent sets. to fix the ideas we take gessamans rule as our prototype rule for further study for hypothesis testing this partition was already noted by anderson for each n let m injknldl project the vectors xl xn onto the first coordinate axis and then partition the data into m sets using hyperplanes figure gessamans partition with m data-dependent partitioning o o o o l------ po perpendicular to that axis in such a way that each set contains an equal number of points possibly the rightmost set where fewer points may fall if n is not a multiple of m. we obtain m cylindrical sets. in the same fashion cut each of these cylindrical sets along the second axis into m boxes such that each box contains the same number of data points. continuing in the same way along the remaining coordinate axes we obtain m d rectangular cells each of which the exception ofthose on the boundary contains about kn points. the classification rule gn uses a majority vote among those yi for which xi lies within a given cell. consistency of this classification rule can be established by an argument similar to that used for the kn rule above. one needs to check that the conditions of theorem are satisfied. the only minor difference appears in the computation of lln which in this case is bounded from above by the following theorem summarizes the result theorem assume that the marginal distributions of x in nd are nonatomic. then the partitioning classification rule based on gessamans rule is strongly consistent if kn and kn n as n tends to infinity. to consider distributions with possibly atomic marginals the partitioning rithm must be modified since every atom has more than kn points falling on it for large n. with a proper modification a strongly universally consistent rule can be obtained. we leave the details to the reader remark. consistency of gessamans classification scheme can also be derived from the results of gordon and olshen under the additional condition kn fo results of breiman friedman olshen and stone can be partitioning rules based on clustering used to improve this condition to kn log n theorem guarantees sistency under the weakest possible condition kn partitioning rules based on clustering clustering is one of the most widely used methods in statistical data analysis. typical clustering schemes divide the data into a finite number of disjoint groups by minimizing some empirical error measure such as the average squared distance from cluster centers hartigan in this section we outline the application of our results to classification rules based on k-means clustering of unlabeled observations. as a first step we divide xl xn into kn disjoint groups having cluster centers ai akn e rd. the vectors ai akn are chosen to minimize the empirical squared euclidean distance error over all the nearest-neighbor clustering rules having kn representatives bi e rd where ii ii denotes the usual euclidean norm. note that the choice of bkn cluster centers depends only on the vectors xi not on their labels. for the behavior of enai akj see problem the vectors ai akll give rise to a voronoi partition p n akn in a natural way for each j e kn let ties are broken by assigning points on the boundaries to the vector that has the smallest index. the classification rule gn is defined in the usual way is a majority vote among those yjs such that x j falls in an if the measure fl of x has a bounded support theorem shows that the classification rule is strongly consistent if kn grows with n at an appropriate rate. note that this rule is just another of the prototype nearest neighbor rules that we discussed in chapter data-dependent partitioning figure example of partition_ ing based on clustering with kn the criterion we minimize is the sum of the squares of the distances of the xs to the aso theorem and nobel assume that there is a bounded set a c rd such that pix e a let be a sequence of integers for which kn and as n k logn n let gn dn be the histogram classification rule based on the voronoi partition of kn cluster centers minimizing the empirical squared euclidean distance error. then with probability one as n tends to infinity. if d or then the second condition on kn can be relaxed to kn logn n proof. again we check the conditions of theorem let fn consist of au voronoi partitions of kn points in rd. as each partition consists of kn cells we may use lemma to bound clearly boundaries between cells are subsets of hyperplanes. since there are at most knckn boundaries between the kn voronoi cells each cell of a partition in fn is a polytope with at most knckn k faces. by theorem n fixed points in r d d can be split by hyperplanes in at most different ways. it follows that for each partitioning rules based on clustering m fj.nfm and consequently fj.nfn n kn n lk log n n by the second condition on the sequence thus condition of theorem is satisfied. it remains to establish condition of theorem this time we need the weaker condition mentioned in the remark after the theorem that is that for every y and e inf p diam an n t y with probability one. clearly we are done if we can prove that there is a sequence of subsets tn of nd depending on the data dn such that fltn with probability one and flx diamanx n tn y o. to this end let a i akn denote the optimal cluster centers corresponding to d n and define kn u n a j jl implies where sxr is the ball of radius r around the vector x. clearly x e that ilx ax ii y where ax denotes the closest cluster center to x among al akn but since it follows that and fl diamanx n tn y o. it remains to show that fltn with probability one as n using markovs inequality we may write e iix dn using a large-deviation inequality for the empirical squared error of neighbor clustering schemes it can be shown problem that if x has bounded support then e iix ajfl dn ijkn min erd e iix ljkn data-dependent partitioning with probability one if log n n as n moreover it is easy to see that ber e iix bj as kn it follows that fltn as desired. if d then the cells of the voronoi partition are intervals on the real line and therefore nkn similarly if d then the number of hyperplanes defining the voronoi partition increases linearly with kn to see this observe that if we connect centers of neighboring clusters by edges then we obtain a planar graph. from eulers theorem e.g. edelsbrunner the number of edges in a planar graph is bounded by n where n is the number of vertices. thus in order to satisfy condition it suffices that kn log n n in both cases. remark. in the theorem above we assume that the cluster centers a are empirically optimal in the sense that they are chosen to minimize the empirical squared euclidean distance error practically speaking it is hard to determine minimum. to get around this difficulty several fast algorithms have been proposed that approximate the optimum hartigan for a survey. perhaps the most popular algorithm is the so-called k-means clustering method also known in the theory of quantization as the max algorithm max linde buzo and gray the iterative method works as follows step s tep x x t k k llutla centers a ak e a e cl h d uster t e ata pomts ak into k sets such that the m-th set ci contains the x js that are closer to a than to any other center. ties are broken in favor of smaller indices. d an set l d h t e centers a n aroun h as t e averages f h t e ak step determme t e new centers a data points within the clusters h ail m l j j.xjecn icii step increase i by one and repeat steps and until there are no changes in the cluster centers. it is easy to see that each step of the algorithm decreases the empirical squared euclidean distance error. on the other hand the empirical squared error can take finitely many different values during the execution of the algorithm. therefore data-based scaling the algorithm halts in finite time. by inspecting the proof of theorem it is not hard to see that consistency can also be proved for partitions given by the suboptimal cluster centers obtained by the k-means method provided that it is initialized appropriately problem data-based scaling we now choose the grid size h in a cubic histogram rule in a data-dependent manner and denote its value by hn. theorem implies the following general result theorem let gn be the cubic histogram classifier based on a partition into cubes of size hn. if lim hn and lim nh with probability one then the partitioning rule is strongly universally consistent. to prove the theorem we need the following auxiliary result lemma let z be a sequence of nonnegative random variables. if limn--fcxj zn with probability one then there exists a sequence an of positive numbers such that limn--fcxj izn?.an with probability one. proof. define vn supm?.n zm. then clearly vn can find a subsequence nl of positive integers such that for each k with probability one. we then the borel-cantelli lemma implies that lim ivllk k with probability one. k cxj the fact that vn zn and imply the statement. proof of theorem let and be sequences of positive numbers with an bn then data-dependent partitioning it follows from lemma that there exist sequences of positive numbers and satisfying an bn bn and na as n such that lim ihilanbzl with probability one. n-oo therefore we may assume that for each n phn e bn since bn condition of theorem holds trivially as all diameters of all cells are inferior to bnji. it remains to check condition clearly for each m each partition in contains less than cells where the constant c depends on m and d only. on the other hand it is easy to see that n points can not be partitioned more than different ways by cubic-grid partitions with cube size h an for some other constant therefore for each m n a n and condition is satisfied. in many applications different components of the feature vector x correspond to different physical measurements. for example in a medical application the first coordinate could represent blood pressure the second cholesterol level and the third the weight of the patient. in such cases there is no reason to use cubic histograms because the resolution of the partition hn along the coordinate axes pends on the apparent scaling of the measurements which is rather arbitrary. then one can use scale-independent partitions such as methods based on order statistics described earlier. alternatively one might use rectangular partitions instead of cubic ones and let the data decide the scaling along the different coordinate axes. again theorem can be used to establish conditions of universal consistency of the classification rule corresponding to data-based rectangular partitions theorem consider a data-dependent histogram rule when the cells of the partition are all rectangles of the form x x lhnd where k kd run through the set of integers and the edge sizes of the rectangles hnd are determinedfrom the data n hni for each i d and nhnl hnd with probability one then the data-dependent rectangular partitioning rule is strongly universally sistent. to prove this just check the conditions of theorem we may pick for example hnd to minimize the resubstitution estimate n l ignxi il subject of course to certain conditions so that hni with probability one yet n hni with probability one. see problem problems and exercises classification trees consider a partition of the space obtained by a binary classification tree in which each node dichotomizes its set by ahyperplane chapter for more on cation trees. the construction of the tree is stopped according to some unspecified rule and classification is by majority vote over the convex polytopes of the partition. the following corollary of theorem generalizes the consis tency results in the book by breiman friedman olshen and stone theorem and nobel let gn be a binary tree classifier based upon at most mn hyperplane splits where mn onlogn. if in addition condition of theorem is satisfied then gn is strongly consistent. in particular the rule is strongly consistent if condition otheorem holds and every cell of the partition contains at least kn points where kn log n proof. to check condition of theorem recall theorem which implies that n points in a d-dimensional euclidean space can be dichotomized by hyperplanes in at most different ways. from this we see that the number of different ways n points of rd can be partitioned by the rule gil can be bounded by as there are not more than mn cells in the partition. thus by the assumption that mn log n n we have ognjn n mn mnd n n so that condition of theorem is satisfied. for the second part of the statement observe that there are no more than n k n cells in any partition and that the tree-structured nature of the partitions assures that gn is based on at most n kn hyperplane splits. this completes the proof. d problems and exercises problem let p be a partition of n d. prove that l iilna aep ilai sup iilnb ilbi besp where the class of sets bp contains all sets obtained by unions of cells of p. this is scheffes theorem for partitions. see also problem data-dependent partitioning problem show that condition of theorem may be replaced by the following for every y and e lim tcrd inf diama n t y with probability one. problem let x be uniformly distributed on the unit square let if xl and otherwise figure consider the algorithm when first the xl-coordinate is cut at the k-th smallest xl-value among the training data. next the rectangle with n k points is cut according to the isolating another k points. this is repeated on a rotational basis for the two coordinate axes. show that the error probability of the obtained partitioning classifier does not tend to l can you determine the asymptotic error probability? problem modify gessamans rule based on statistically equivalent blocks so that the rule is strongly universally consistent. problem cut each axis independently into intervals containing exactly k of the jected data points. the i axis has intervals a li form a histogram rule that takes a majority vote over the product sets a il x x aidd. figure a partition based upon the method obtained above with k cii this rule does not guarantee a minimal number of points in every cell. nevertheless if k d on k show that this decision rule is consistent i.e. that e la l in probability. problem show that each step of the k-means clustering algorithm decreases the empirical squared error. conclude that theorem is also true if the clusters are given by the k-means algorithm. hint observe that the only property of the clusters used in the proof of theorem is that e iix dn with probability one. this can be proven for clusters given by the k-means algorithm if it is appropriately initialized. to this end use the techniques of problem problem prove theorem problem consider the hni in theorem the interval sizes for cubic histograms. let the hni be found by minimizing n l ignxljyil i problems and exercises subject to the condition that each marginal interval contain at least k data points is at least k data points have that one coordinate in the interval. under which condition on k is the rule consistent? problem take a histogram rule with data-dependent sizes hnl hnd as in rem defined as follows ni max ljn hni i d where and wni are and percentiles mm ljn j y n ls were d x j xi r xd. j j h xu j of xii xi. assume for convenience that x has nonatomic marginals. show that leads sometimes to an inconsistent rule even if d show that always yields a scale-invariant consistent rule. splitting the data the holdout estimate universal consistency gives us a partial satisfaction-without knowing the lying distribution taking more samples is guaranteed to push us close to the bayes rule in the long run. unfortunately we will never know just how close we are to the bayes rule unless we are given more information about the unknown distribution chapter a more modest goal is to do as well as possible within a given class of rules. to fix the ideas consider all nearest neighbor rules based upon metrics of the form d il where ai for all i and x xd here the ais are variable scale tors. let be a particular nearest neighbor rule for a given choice of ad and let gn be a data-based rule chosen from this class. the best we can hope for now is something like lgn infpn lpn in probability for all distributions where lgn pgnx yidn is the conditional ity of error for gn. this sort of optimality-within-a-class is definitely achievable. however proving such optimality is generally not easy as gn depends on the data. in this chapter we present one possible methodology for selecting provably good rules from restricted classes. this is achieved by splitting the data into a training splitting the data sequence and a testing sequence. this idea was explored and analyzed in depth in devroye and is now formalized. the data sequence dn is split into a training sequence dm yd ym and a testing sequence tt ymr yml where l m n. the sequence dm defines a class of classifiers en whose members are denoted by m m dm. the testing sequence is used to select a classifier from cm that minimizes the error count this error estimate is called the holdout estimate as the testing sequence is out of the design of m. thus the selected classifier gn e cm satisfies lmlgn lml m for all m e cm the subscript n in gn may be a little confusing since gn is in cm a class of classifiers depending on the first m pairs dm only. however gil depends on the entire data dn as the rest of the data is used for testing the classifiers in cm we are interested in the difference between the error probability and that of the best classifier in cm inf ecn l m. note that l m p m y i dm denotes the error probability conditioned on dm. the conditional bility is small when most testing sequences tt pick a rule gn whose error probability is within e of the best classifier in cm we have already addressed similar questions in chapters and there we have seen that lgn inf l m sup ilml m l mi. if cm contains finitely many rules then the bound of theorem may be useful if we take m then theorem shows problem that on the average we are within icm i i n of the best possible error rate whatever it is. if cm is of infinite cardinality then we can use the vapnik-chervonenkis theory to get similar inequalities. for example from theorem we get consistency and asymptotic optimality and consequently p inf l m ei dm s where scm i is the l-th shatter coefficient corresponding to the class of classifiers em theorem for the definition. since cm depends on the training data dm the shatter coefficients scm l may depend on d m too. however usually itis possible to find upper bounds on the random variable scm i that depend on m and i only but not on the actual values of the random variables xl x m y m both upper bounds above are distribution-free and the problem now is purely combinatorial count icm i is usually trivial or compute scm l. remark. with much more effort it is possible to obtain performance bounds of the holdout estimate in the form of bounds for for some special rules where m and n are carefully defined. for example devroye and wagner give upper bounds when both m and n are k-nearest neighbor classifiers with the same k working with different sample size. d remark. minimizing the holdout estimate is not the only possibility. other error estimates that do not split the data may be used in classifier selection as well. such estimates are discussed in chapters and however these estimates are usually tailored to work well for specific discrimination rules. the most general and robust method is certainly the data splitting described here. d consistency and asymptotic optimality typically cm becomes richer as m grows and it is natural to ask whether the empirically best classifier in cm is consistent. theorem assume that from each cm we can pick one m such that the sequence of m is consistent for a certain class of distributions. then the matic rule gn defined above is consistent for the same class of distributions elgn l as n if l l o. proof. decompose the difference between the actual error probability and the bayes error as splitting the data the convergence of the first term on the right-hand side is a direct corollary of the second term converges to zero by assumption. theorem shows that a consistent rule is picked if the sequence of cms contains a consistent rule even if we do not know which functions from em lead to consistency. if we are just worried about consistency theorem reassures us that nothing is lost as long as we take l much larger than logescm l. often this reduces to a very weak condition on the size l of the testing set. let us now introduce the notion of asymptotic optimality. a sequence of rules gn is said to be asymptotically optimal for a given distribution of y when elgn l hm e mecm l m l l. our definition is not entirely fair because gn uses n observations whereas the family of rules in the denominator is restricted to using m observations. if gn is not taken from the same em then it is possible to have a ratio which is smaller than one. but if gn e cm then the ratio always is at least one. that is why the definition makes sense in our setup. when our selected rule is asymptotically optimal we have achieved something very strong we have in effect picked a rule better a sequence of rules which has a probability of error converging at the optimal rate attainable within the sequence of ems. and we do not even have to know what the optimal rate of convergence is. this is especially important in nonparametric rules where some researchers choose smoothing factors based on theoretical results about the optimal attainable rate of convergence for certain classes of distributions. we are constantly faced with the problem of choosing between parametric and nonparametric discriminators. parametric discriminators are based upon an lying model in which a finite number of unknown parameters is estimated from the data. a case in point is the multivariate normal distribution which leads to linear or quadratic discriminators. if the model is wrong parametric methods can perform very poorly when the model is right their performance is difficult to beat. the method based on splitting the data chooses among the best discriminator depending upon which happens to be best for the given data. we can throw in em a variety of rules including nearest neighbor rules a few linear discriminators a couple of tree classifiers and perhaps a kernel rule. the probability bounds above can be used when the complexity of em by its shatter coefficients does not get out of hand. the notion of asymptotic optimality can be too strong in many cases. the reason for this is that in some rare lucky situations e mecm l m l may be very small. in these cases it is impossible to achieve asymptotic optimality. we can fix this problem by introducing the notion of em where em is a positive sequence decreasing to with m. a rule is said to be em-optimal when elgn l hm n-oo e mecm l m l l. nearest neighbor rules with automatic scaling for all distributions of y for which lim e l m l in what follows we apply the idea of data splitting to scaled nearest neighbor rules and to rules closely related to the data-dependent partitioning classifiers studied in chapter many more examples are presented in chapters and nearest neighbor rules with automatic scaling let us work out the simple example introduced above. the rule is the nearest neighbor rule for the metric ilxll where x xed the class em is the class of all ad-nn rules for dm yi ym. the testing sequence tz is used to choose ad so as to minimize the holdout error estimate. in order to have it suffices that log e l l this puts a lower bound on l. to get this lower bound one must compute s clearly seem l is bounded by the number of ways of classifying xml xmz using rules picked from em that is the total number of different values for we now show that regardless of dm and xml xmz for n we have this sort of result is typical-the bound does not depend upon dm or tz. when plugged back into the condition of convergence it yields the simple condition logm o. l in fact we may thus take slightly larger than log m. it would plainly be silly.to take as we would thereby in fact throwaway most of the data. splitting the data set am mxmd mxmz m e cm. to count the number of values in am note that all squared distances can be written as x j laipkij d kl where pkij is a nonnegative number only depending upon xi and x j. note that each squared distance is linear in aj. now consider the space of a. observe that in this space mxmd is constant within each cell of the partition determined by the hyperplanes laipkjml laipkilml s i if sm. d kl d kl to see this note that within each set in the partition keeps the same nearest neighbor among xl x m it is known that k hyperplanes in rd create partitions of cardinality not exceeding k d problem now overlay the i partitions obtained for mxmd mxnhz respectively. this yields at most sets as the overlays are determined by i g hyperplanes. but clearly on each of these sets mxmz is constant. therefore scm iaml d classification based on clustering recall the classification rule based on clustering that was introduced in chapter the data points xl xn are grouped into k clusters where k is a predetermined integer and a majority vote decides within the k clusters. if k is chosen such that k and k log n n then the rule is consistent. for a given finite n however these conditions give little guidance. also the choice of k could dramatically affect the performance of the rule as there may be a mismatch between k and some unknown natural number of clusters. for example one may construct distributions in which the optimal number of clusters does not increase with n. let us split the data and let the testing sequence decide the value of k. in the framework of this chapter em contains the classifiers based on the first m pairs dm of the data with all possible values of k. clearly en is a finite family with icml m. in this case by problem we have i the consistency result theorem implies that statistically equivalent blocks for all distributions ifthe xis have bounded support. thus we see that our strategy leads to a universally consistent rule whenever mj o. this is a very mild condition since we can take l equal to a small fraction of n without sacrificing consistency. if l is small compared to n then m is close to n so inf mecm is likely to be close to inf ii ecli thus we do not lose much by sacrificing a part of the data for testing purposes but the gain can be tremendous as we are guaranteed to be within m j of the optimum in the class em. that we cannot take i and hope to obtain consistency should be obvious. it should also be noted that for i m we are roughly within of the best possible probability of error within the family. also the empirical selection rule is jfogm j i-optimal. statistically equivalent blocks recall from chapter that classifiers based on statistically equivalent blocks typically partition the feature space rd into rectangles such that each rectangle contains k points where k is a certain positive integer the parameter of the rule. this may be done in several different ways. one of many such rules-the rule introduced by gessaman is consistent if k and k j n again we can let the data pick the value of k by minimizing the holdout estimate. just as in the previous section lem i m and every remark mentioned there about consistency and asymptotic optimality remains true for this case as well. we can enlarge the family em by allowing partitions without restrictions on cardinalities of cells. this leads very quickly to oversized families of rules and we have to impose reasonable restrictions. consider cuts into at most k rectangles where k is a number picked beforehand. recall that for a fixed partition the class assigned to every rectangle is decided upon by a majority vote among the training points. on the real line choosing a partition into at most k sets is equivalent to choosing k cut positions from m i n spacings between all test and training points. hence seml n for consistency k has to grow as n grows. it is easily seen that splitting the data if k and m to achieve consistency of the selected rule however we also need log scm l l llogn now consider d-dimensional partitions defined by at most k consecutive orthogonal cuts that is the first cut divides rd into two halfspaces along a hyper plane perpendicular to one of the coordinate axes. the second cut splits one of the halfspaces into two parts along another orthogonal hyperplane and so forth. this procedure yields a partition of the space into k rectangles. we see that for the first cut there are at most dn possible combinations to choose from. this yields the loose upper bound this bound is also valid for all grids defined by at most k cuts. the main difference here is that every cut defines two halfspaces of r d and not two rectangles of a cells so that we usually end up with rectangles in the partition. assume that cm contains all histograms with partitions into at most k infinite rectangles. then considering that a rectangle in rd requires choosing spacings between all test and training points two per coordinate axis see feinholz for more work on such partitions. binary tree classifiers we can analyze binary tree classifiers from the same viewpoint. recall that such classifiers are represented by binary trees where each internal node corresponds to a split of a cell by a hyperplane and the terminal nodes represent the cells of the partition. assume that there are k cells therefore k splits leading to the partition. if every split is perpendicular to one of the axes then the situation is the same as in the previous section for smaller families of rules whose cuts depend upon the training sequence only the bound is pessimistic. others have proposed generalizing orthogonal cuts by using general hyperplane cuts. recall that there are at most ways of dichotomizing n points in nd by hyperplanes theorem thus if we allow up to k internal nodes hyperplane cuts problems and exercises sem l the number of internal nodes has to be restricted in order to obtain consistency from this bound refer to chapter for more details. problems and exercises problem prove that n hyperplanes partition n d into at most contiguous regions when d n cover hint proceed by induction. problem assume that gn is selected so as to minimize the holdout error estimate ltm m over cm the class of rules based upon the first m data points. assume furthermore that we vary lover n and that we pick the best i m n by minimizing the holdout error estimate again. show that if scm i ony for some y then the obtained rule is strongly universally consistent. problem let cm be the class of ad-nn rules based upon dm. show that if min then inf l m inf l n in probability for all distributions of y for which x has a density. conclude that if ljlogn m n on then lgn inf l n in probability. problem finding the best split. this exercise is concerned with the automatic selection of m and i n m. if gn is the selected rule minimizing the holdout estimate then mf l m l i since in most interesting cases scm i is bounded from above as a polynomial ofm and i the estimation error typically decreases as i increases. on the other hand the approximation error infpmecm l m l typically decreases as m increases as the class cm gets richer. some kind of balance between the two terms is required to get optimum performance. we may use the empirical estimates lml m again to decide which value of m we wish to choose however as m gets large-and therefore i small-the class cm will tend to overfit the data tz providing strongly optimistically biased estimates for infpmecm l m. to prevent overfitting we may apply the method of complexity regularization by we may define the penalty term by rem i logn i splitting the data and minimize the penalized error estimate lml lml rem l over all e ullcm. denote the selected rule by prove that for every n and all distributions of y mm m! l inf l m l y n hint proceed similarly to the proof of theorem the resubstitution estimate estimating the error probability is of primordial importance for classifier selection. the method explored in the previous chapter attempts to solve this problem by using a testing sequence to obtain a reliable holdout estimate. the independence of testing and training sequences leads to a rather straightforward analysis. for a good performance the testing sequence has to be sufficiently large we often get away with testing sequences as small as about log n. when data are expensive this constitutes a waste. assume that we do not split the data and use the same sequence for testing and training. often dangerous this strategy nevertheless works if the class of rules from which we select is sufficiently restricted. the error estimate in this case is appropriately called the resubstitution estimate and it will be denoted by lr. this chapter explores its virtues and pitfalls. a third error estimate the deleted estimate is discussed in the next chapter. estimates based upon other paradigms are treated briefly in chapter the resubstitution estimate the resubstitution estimate lr counts the number of errors committed on the training sequence by the classification rule. expressed formally sometimes lr is called the apparent error rate. it is usually strongly optimisti the resubstitution estimate cally biased. since the classifier gn is tuned by dn it is intuitively clear that gn may behave better on dn than on independent data. the best way to demonstrate this biasedness is to consider the i-nearest neigh_ bor rule. if x has a density then the nearest neighbor of xi among xl xn is xi itself with probability one. therefore lr regardless of the value of ln lgn. in this case the resubstitution estimate is useless. for k-nearest neigh_ bor rules with large k lr is close to ln. this was demonstrated by devroye and wagner who obtained upper bounds on the performance of the resubsti_ tution estimate for the k-nearest neighbor rule without posing any assumption on the distribution. also if the classifier whose error probability is to be estimated is a member of a class of classifiers with finite vapnik-chervonenkis dimension the definitions in chapter then we can get good performance bounds for the resubstitution estimate. to see this consider any generalized linear classification rule that is any rule that can be put into the following form nx g if ao.n otherwise where the are fixed functions and the coefficients ain depend on the data dn in an arbitrary but measurable way. we have the following estimate for the performance of the resubstitution estimate l. theorem and wagner for all nand e the substitution estimate l of the error probability ln of a generalized linear rule satisfies proof. define the set an c rd x i as the set of all pairs y e rd x on which gn errs an y gnx y. observe that and or denoting the measure of y by and the corresponding empirical measure by ln and histogram rules the set an depends on the data dn so that for example e vnan e van. fortunately the powerful vapnik-chervonenkis theory comes to the rescue via the inequality iln lrissup ivnc vci cec where c is the family of all sets of the form y y where rd is a generalized linear classifier based on the functions d. by theorems and we have p ivnc e cec similar inequalities can be obtained for other classifiers. for example for titioning rules with fixed partitions we have the following theorem let gn be a classifier whose value is constant over cells of a fixed partition ofrd into k cells. then p lnl e the proof is left as an exercise from theorems and we get bounds for the expected difference between the resubstitution estimate and the actual error probability ln. for example theorem implies problem that e lnl v in some special cases the expected behavior of the resubstitution estimate can be analyzed in more detail. for example mclachlan proved that if the conditional distributions of x given y and y are both normal with the same covariance matrices and the rule is linear and based on the estimated parameters then the bias of the estimate is of the order n mclachlan also showed for this case that for large n the expected value of the substitution estimate is smaller than that of l n that is the estimate is optimistically biased as expected. histogram rules in this section we explore the properties of the resubstitution estimate for togram rules. let p az be a fixed partition of rd and let gn be the the resubstitution estimate corresponding histogram classifier chapters and introduce the notation the analysis is simplified if we rewrite the estimate lr in the following form problem it is also interesting to observe that lr can be put in the following form lr f where ljl i and i ljl we can compare this form with the following expression of ln ln f ln problem we begin the analysis of performance of the estimate by showing that its mean squared error is not larger than a constant times the number. of cells in the partition over n. theorem for any distribution of y andfor all n the estimate lr of the error probability of any histogram rule satisfies also the estimate is optimistically biased that is if in addition the histogram rule is based on a partition p ofrd into at most k cells then proof. the first inequality is an immediate consequence of theorem and introduce the auxiliary quantity r lminvoaj vi where voa py x e a and vi py x e a. we use the decomposition histogram rules first we bound the second term on the right-hand side of observe that r is just the bayes error corresponding to the pair of random variables y where the function t transforms x according to t i if x e ai. since the histogram classification rule gnx can be written as a function of tx its error probability ln cannot be smaller than r. furthermore by the first identity of theorem we have ln r l isignvlnai-vonai signvlai-voai if the partition has at most k cells then by the cauchy-schwarz inequality e e voa vona i k lvar we bound the first term on the right-hand side of as we have seen earlier varl lin so it suffices to bound ir elri. by jensens inequality i min i evl.nai i r. the resubstitution estimate so we bound r elr from above. by the inequality imina b mince la el ib dl we have r el le ivla i vinadl l i the cauchy-schwarz inequality voai voai viai vi voain viai- via i.tila i where we used the elementary inequality fa b. therefore elr to complete the proof of the third inequality observe that if there are at most k cells then k k. l n jensens inequality therefore finally eln elr n r e o. we see that if the partition contains a small number of cells then the tution estimate performs very nicely. however if the partition has a large number of cells then the resubstitution estimate of ln can be very misleading as the next result indicates data-based histograms and rule selection theorem and wagner for every n there exists a titioning rule and a distribution such that proof. let ai be cells of the partition such that flaj i assume further that for every x e n d that is y with probability one. then clearly lr o. on the other hand if a cell does not contain any of the data points xl x n then gnx in that cell. but since the number of points is only half of the number of cells at least half of the cells are empty. therefore ln and ilr ln i this concludes the proof. o data-based histograms and rule selection theorem demonstrates the usefulness of lr for histogram rules with a fixed partition provided that the number of cells in the partition is not too large. if we want to use lr to select a good classifier the estimate should work uniformly well over the class from which we select a rule. in this section we explore such data-based histogram rules. let f be a class of partitions of nd. we will assume that each member of f titions n d into at most k cells. for each partition p e f define the corresponding histogram rule by if iylxeax iyoxeax otherwise where ax is the cell of p that contains x. denote the error probability of gpx by the corresponding error estimate is denoted by l?p l minvona vlna. aep by analogy with theorems and we can derive the following result which gives a useful bound for the largest difference between the estimate and the error probability within the class of histogram classifiers defined by f. the combinatorial coefficient defined in chapter appears as a coefficient in the upper bound. the computation of for several different classes of partitions is illustrated in chapter the resubstitution estimate theorem assume that each member of f partitions nd into at most k cells. for every nand e psup ilrp e pef proof. we can proceed as in the proof of theorem the shatter coefficient sc n corresponding to the class of histogram classifiers defined by partitions in f can clearly be bounded from above by the number of different ways in which n points can be partitioned by members of f times as there are at most different ways to assign labels to cells of a partition of at most k cells. the theorem has two interesting implications. the error estimate lr can also be used to estimate the perfolmance of histogram rules based on data-dependent partitions chapter the argument ofthe proof of theorem is not valid for these rules. however theorem provides performance guarantees for these rules in the following corollaries corollary let gnx be a histogram classifier based on a random partition pn into at most k cells which is determined by the data dn. assume that for any possible realization ot the training data dn the partition p n is a member of a class of partitions f. if ln is the error probability of gn then p i l l i e tj n n n and e ln ln n proof. the first inequality follows from theorem by the obvious inequality p ln i e p sup ilrp e pef the second inequality follows from the first one via problem perhaps the most important application of theorem is in classifier selection. let cn be a class of data-dependent histogram rules. we may use the error estimate lr to select a classifier that minimizes the estimated error probability. denote the selected histogram rule by that is lr lr n for all n e cn. here lr n denotes the estimated error probability of the classification rule in. the question is how well the selection method works in other words how close the error probability of the selected classifier l is to the error probability of the best rule in the class infcpnecn l n. it turns out that if the possible partitions are not too complex then the method works very well problems and exercises corollary assume that jor any realization oj the data dn the possible partitions that define the histogram classifiers in en belong to a class ojpartitions f whose members partition nd into at most k cells. then p l z l n e proof. by lemma l inf l n sup ilr n l sup ilrp lpi. therefore the statement follows from theorem problems and exercises problem prove theorem hint proceed as in the proof of theorem problem show that for histogram rules the resubstitution estimate may be written as problem consider the resubstitution estimate lr of the error probability ln of a histogram rule based on a fixed sequence of partitions pn show that if the regression function estimate ljjeax is consistent that is it satisfies e as n then lim e lnll n-oo andalsoelr l. problem let y yl be a sequence that depends in an arbitrary ion on the data din and let gn be the nearest neighbor rule with y y where m is fixed. let lr denote the resubstitution estimate of ln lgn. show that for all e and all distributions p li e problem find a rule for y e r x i such that for all nonatomic distributions with l we have eln yete eln. lr maybe pessimistically biased even for a consistent rule. problem for histogram rules on fixed partitions that do not change with n and are independent of the data show that e is monotonically nonincreasing. problem assume that x has a density and investigate the resubstitution estimate of the rule. what is the limit of e as n oo? deleted estimates of the error probability the deleted estimate called cross-validation leave-one-out or u-method attempts to avoid the bias present in the resubstitution estimate. proposed and developed by lunts and brailovsky lachenbruch cover and stone the method deletes the first pair yl from the training data and makes a decision gn-l using the remaining n pairs. it tests for an an error on yl and repeats this procedure for all n pairs of the training data dn. the estimate ld is the average the number of errors. we formally denote the training set with yj deleted by then we define clearly the deleted estimate is almost unbiased in the sense that thus ld should be viewed as an estimator of ln- l rather than of ln. in most of the interesting cases ln converges with probability one so that the difference between ln- l and ln becomes negligible for large n. the designer has the luxury of being able to pick the most convenient gn-l. in some cases the choice is very natural in other cases it is not. for example if gn is the i-nearest neighbor rule then letting gn-l be the i-nearest neighbor rule based on n data pairs seems to be an obvious choice. we will see later deleted estimates of the error probability that this indeed yields an extremely good estimator. but what should gn-l be if gn is for example a k-nn rule? should it be the k-nearest neighbor classifier the i-nearest neighbor classifier or maybe something else? well the choice is k typically nontrivial and needs careful attention if the designer wants a distribution_ free performance guarantee for the resulting estimate. because of the variety of choices for gn-l we should not speak of the deleted estimate but rather of a deleted estimate. in this chapter we analyze the performance of deleted estimates for a few type classifiers such as the kernel nearest neighbor and histogram rules. in most cases studied here deleted estimates have good distribution-free properties. a general lower bound nonparametric rules. an error estimate ln is merely a function x lr we begin by exploring general limitations of error estimates for some important which is applied to the data dn i yi yn theorem let gn be one of the following classifiers the kernel rule gnx if i i l.dl otherwise k i l....il k with a nonnegative kernel k of compact support and smoothing factor h the histogram rule iyilixieax iyioixieax gil otherwise based on a fixed partition p a containing at least n cells. the lazy histogram rule where xj is the minimum-index point among xl xnforwhich xj e ai wherep a isajixedpartition containing atleastn cells. denote the probability of error for gil by ln pgnx dn ydn. thenor with l such that every n andor every error estimatorln there exists a distribution ox y the theorem says that for any estimate ln there exists a distribution with the property thate ln i for the rules gn given in the theorem a general lower bound no error estimate can possibly give better distribution-free performance guarantees. error estimates are necessarily going to perform at least this poorly for some distributions. proof of theorem let ln be an arbitrary fixed error estimate. the proof relies on randomization ideas similar to those of the proofs of theorems and we construct a family of distributions forcx y.forb e be its binary expansion. in all cases the distribution of x is uniform on n points x n for the histogram and lazy histogram rules choose xl xn such that they fall into different cells. for the kernel rule choose xl xn so that they are isolated from each other that is k cxixj for all i j e n i j. to simplify the notation we will refer to these points by their indices that is we will write x i instead of x xi. for a fixed b define y we may create infinitely many samples cone for each b e drawn from the distribution of cx y as follows. let x i xn be i.i.d. and uniformly distributed on n. all the samples share the same xl x n but differ in their ys. for given xi define yi b xi write zn cx xn and ni ixji observe that dn is a function of zn and b. it is clear that for all classifiers covered by our assumptions for a fixed b where s i n ni o is the set of empty bins. we randomize the distribution as follows. let b be a uniform random variable independent of cx y. then clearly b i b are independent uniform binary random variables. note that supeilncdn-lnl eilncdn-lnl b cwhere b is replaced by b e ilncdn ln ii zn in what follows we bound the conditional expectation within the brackets. to make the dependence upon b explicit we write lnczn b lncdn and lncb ln. thus e ilnzn b zn b ilnczn b zn b bi for i with ni and b bi for i with ni zn lnzn b lnczn b deleted estimates of the error probability the expression for ln given above lsi bisi is a binomial random variable fist khintchines inequality see lemma a.s. in summary we have we have only to bound the right-hand side. we apply lemma aa to the random variable jist clearly eisi no lnn and e lnioj l lnionioj il ij eisi nn e so that in nn vn---- i the proof is now complete. a general upper bound for deleted estimates a general upper bound for deleted estimates the following inequality is a general tool for obtaining distribution-free upper bounds for the difference between the deleted estimate and the true error probability ln theorem and wagner devroye and wagner assume that gn is a symmetric classifier that is gnx dn gnx d where d is obtained by permuting the pairs of dn arbitrarily. then proof. first we express the three terms on the right-hand side of the first term can be bounded by using symmetry of gn by e e jg y r e t ign_lxidni yd n il the second term is written as e ln e t ign-lxidili yd n il deleted estimates of the error probability n le dn y gn-lxi dnj yiidn n il pgnx dn y gn-ixl dnl yd. for the third term we introduce the pair yl independent of x y and d n having the same distribution as y. then el epgnxdn yidnf e dn yidnpgnxi dn yiidn e dn y gnxi dn yiidn where we used independence of yl. we introduce the notation d akij yn yi yj d yd bki yd and we formally replace y and yl by ya and so that we may work with the indices a and with this notation we have shown thus far the following e l n n n note that also pa a pa a symmetry i ii. pba symmetry nearest neighbor rules pbatl btl.a btla btla iiiiv. using the fact that for events ipci c j pci pcj we bound i def ii v iii v iv pbtl the upper bounds for ii and iii are identical by symmetry. also and iv for a grand total of v. this concludes the proof. d nearest neighbor rules theorem can be used to obtain distribution-free upper bounds for specific rules. here is the most important example. theorem and wagner let gn be the k-nearest neighbor rule with randomized tie-breaking. if ld is the deleted estimate with gn-l chosen as the k-nn rule the same k and with the same randomizing random variables then e ln n proof. because of the randomized tie-breaking the k-nn rule is symmetric and theorem is applicable. we only have to show that clearly gnx dn gn-l dn- l can happen only if xn is among the k nearest neighbors of x. but the probability of this event is just kin since by symmetry all points are equally likely to be among the k nearest neighbors. d deleted estimates ofthe error probability remark. if gn is the k-nn rule such that distance ties are broken by compaling indices then gn is not symmetric and theorem is no longer applicable e.g. x has a density. another non symmetric classifier is the lazy histogram rule o remark. applying theorem to the i-nn rule the cauchy-schwarz inequality implies e i ld ln i n for all distributions. remark. clearly the inequality of theorem holds for any rule that is some function of the k nearest points. for the k-nearest neighbor rule with a more careful analysis devroye and wagner improved theorem to e l n problem n n nhii probability inequalities for ild ln i can also be obtained with further work. by chebyshevs inequality we immediately get so that the above bounds on the expected squared error can be used. sharper distribution-free inequalities were obtained by devroye and wagner for several nonparametric rules. here we present a result that follows immediately from what we have already seen theorem consider the k-nearest neighbor rule with randomized ing. if ld is the deleted estimate with gn-l chosen as the k-nn rule with the same tie-breaking then proof. the result follows immediately from mcdiarmids inequality by the lowing argument from lemma given n points in nd a particular point can be among the k nearest neighbors of at most kyd points. to see this just set fjv equal to the empirical measure of the n points in lemma therefore changing the value of one pair from the training data can change the value of the estimate by at most now since eld eln-l theorem yields the result. exponential upper bounds for the probability pild ln i e are typically much harder to obtain. we mention one result without proof. kernel rules theorem and wagner for the k-nearest neighbor rule pild lnl e one of the drawbacks of the deleted estimate is that it requires much more computation than the resubstitution estimate. if conditional on y and y x is gaussian and the classification rule is the appropriate parametric rule then the estimate can be computed quickly. see lachenbruch and mickey fukunaga and kessel and mclachlan for further references. another and probably more serious disadvantage of the deleted estimate is its large variance. this fact can be illustrated by the following example from devroye and wagner let n be even and let the distribution of y be such that y is independent of x with py o py i consider the k-nearest neighbor rule with k n then obviously ln clearly if the number of zeros and ones among the labels yn are equal then ld thus for e pil lnl e p iyll by stirlings formula we have piln lnl e v e therefore for this simple rule and certain distributions the probability above can not decrease to zero faster than note that in the example above eld so the lower bound holds for pild eldi e as well. also in this example we have e el lyo e il in chapter we describe other estimates with much smaller variances. kernel rules theorem may also be used to obtain tight distribution-free upper bounds for the performance of the deleted estimate of the error probability of kernel rules. we have the following bound theorem assume that k is a regular kernel of bounded support that is it is a function satisfying kx kx b kx iixlls p ilxll r deleted estimates of the error probability for some positive finite constants p band r. let the kernel rule be defined by iyiok xi iyilk xi gn otherwise and define similarly. then there exist constants depending upon d only and depending upon k only such that for all n one may take rlpd b i remark. since is a scale-invariant factor the theorem applies to the rule with ku replaced by khu tk for any smoothing factor. as itis is minimal and equal to if we let k be the uniform kernel on the unit ball p b the assumptions of the theorem require that gn-l is defined with the same kernel and smoothing factor as gn remark. the theorem applies to virtually any kernel of compact support that is of interest to the practitioners. note however that the gaussian kernel is not covered by the result. the theorem generalizes an earlier result of devroye and wagner in which a more restricted class of kernels was considered. they showed that if k is the uniform kernel then cd e ln ln jfi see problem we need the following auxiliary inequality which we quote without proof lemma let zl zn be real-valuedi.i.d. random variables. for e where c is a universal constant. corollary let zl zn be real-valued i.i.d. random variables. fore a i e ce z p n i jfi a z where c is a universal constant. proof of theorem we apply theorem by finding an upper bound for histogram rules for the kernel rule with kernel k in whichh is absorbed p dn gn-l dn- l p lkx xill kx xn kx xn o define b b. we have p lkx x i kx xn kx xn o p ikx b xn e sx.r sxr is the ball of radius r centered at x e ixnesxr lkx xdl fjix corollary since b e ixnesxr fjjnflsxp cb j that ku fj for liull p so that ikx xii fj px i sxp cb f is fj.jn fldy fldx sxr j flsxp ccd b fj.jn where we used lemma the constant cd depends upon the dimension only. o histogram rules in this section we discuss properties of the deleted estimate of the error probability of histogram rules. let p a be a partition of r d and let gn be the corresponding histogram classifier chapters and to get a performance bound for the deleted estimate we can simply apply theorem deleted estimates of the error probability theorem for the histogram rule gn corresponding to any partition p and for all n e l f-l n n l j lr i i i n n i i and in particular d e ln ln e n proof. the first inequality follows from theorem if we can find an upper bound for pgnx gn-l we introduce the notation clearly gn-l can differ from gnx only if both xn and x fall in the same cell of the partition and if the number of zeros in the cell is either equal or less by one than the number of ones. therefore by independence we have pgnx gn-lx l p x e ai xn e ad if-ln-iajji p von-lai xeaixneai f-laj. the terms in the sum above may be bounded as follows p i xeaxnea o p independence e vo-i if.n- j i xi xn j_a o f. a l e i o lemma f-lar e iln_la o jensens inequality e-nla problems and exercises where in the last step we use lemma this concludes the proof of the first inequality. the second one follows trivially by noting that xe-x ie for all x. o remark. it is easy to see that the inequalities of theorem are tight up to a constant factor in the sense that for any partition p there exists a distribution such that d e ln ln e problem remark. the second inequality in theorem points out an important difference between the behavior of the resubstitution and the deleted estimates for histogram rules. as mentioned above for some distributions the variance of l can be of the order i.jli. this should be contrasted with the much smaller variance of the resubstitution estimate. the small variance of lr comes often with a larger bias. other types of error estimates with small variance are discussed in chapter remark. theorem shows that for any partition sup e on the other hand if k o where k is the number of cells in the partition then for the resubstitution estimate we have a better guaranteed distribution-free performance sup e at first sight the resubstitution estimate seems preferable to the deleted estimate. however if the partition has a large number of cells lr may be off the mark see theorem problems and exercises problem show the following variant of theorem for all symmetric classifiers e dn gn-l dn- pgnx dn gnx d pgn-lx dn- gn-lx d_j deleted estimates of the error probability where d and dz- i are just dn and dn- l with yj replaced by an independent copy yo. problem let gn be the relabeling nn rule with the k-nn classifier as ancestral rule as defined in chapter provide an upper bound for the squared error e of the deleted estimate. problem let gil be the rule obtained by choosing the best k ko in the k-nn rule is a constant by minimizing the standard deleted estimate l with respect to k. how would you estimate the probability of error for this rule? give the best possible distribution-free performance guarantees you can find. problem consider the kernel rule with the window kernel k so i. show that eld-l n n n inn hint follow the line of the proof of theorem problem show that for any partition p there exists a distribution such that for the deleted estimate of the error probability of the corresponding histogram rule e ln ln e l hint proceed as in the proof of the similar inequality for k-nearest neighbor rules. problem consider the k-spacings method chapter we estimate the proba bility of error by a modified deleted estimate ln as follows where gni is a histogram rule based upon the same k-spacings partition used for gn-that is the partition determined by k-spacings of the data points x i xn-but in which a majority vote is based upon the yjs in the same cell of the partition with yi deleted. show that eln n hint condition on the xis and verify that the inequality of theorem remains valid. problem consider a rule in which we rank the real-valued observations xi xn from small to large to obtain xc! xn assume that xi has a density. derive an inequality for the error ild lili for some deleted estimate ld your choice when the rule is defined by a majority vote over the data-dependent partition defined by points respectively. problems and exercises problem prove that for the k-nearest neighbor rule e n. n-jiii and wagner hint obtain a refined upper bound for using techniques not unlike those of the proof of theorem problem open-ended problem. investigate if theorem can be extended to nels with unbounded support such as the gaussian kernel. automatic kernel rules we saw in chapter that for a large class of kernels if the smoothing parameter h converges to zero such that nh d goes to infinity as n then the kernel cation rule is universally consistent. for a particular n asymptotic results provide little guidance in the selection of h. on the other hand selecting the wrong value of h may lead to catastrophic error rates-in fact the crux of every nonparametric estimation problem is the choice of an appropriate smoothing factor. it tells us how far we generalize each data point xi in the space. purely atomic distributions require little smoothing will generally be fine while distributions with densities require a lot of smoothing. as there are no simple tests for verifying whether the data are drawn from an absolutely continuous distribution-let alone a distribution with a lipschitz density-it is important to let the data dn mine h. a data-dependent smoothing factor is merely a mathematical function hn x lf for brevity we will simply write hn to denote the random variable hndn. this chapter develops results regarding such functions hn. this chapter is not a luxury but a necessity. anybody developing software for pattern recognition must necessarily let the data do the talking-in fact good universally applicable programs can have only data-dependent parameters. consider the family of kernel decision rules gn and let the smoothing factor h play the role of parameter. the best parameter is the one that minimizes ln. unfortunately it is unknown as is l opt the corresponding minimal probability of error. the first goal of any data-dependent smoothing factor hn should be to approach the performance of hopt we are careful here to avoid saying that hn should be close to hopt as closeness of smoothing factors does not necessarily imply closeness of error probabilities and vice versa. guarantees one might want automatic kernel rules in this respect are eln l opt an for some suitable sequence an or better still eln l l for another sequence.bn o. but before one even attempts to develop such dependent smoothing factors ones first concern should be with consistency is it true that with the given hn ln l in probability or with probability one? this question is dealt with in the next section. in subsequent sections we give various examples of data-dependent smoothing factors. consistency we start with consistency results that generalize theorem the first result assumes that the value of the smoothing parameter is picked from a discrete set. theorem assume that the random variable hn takes its values from the set of real numbers of the form where k is a nonnegative integer and o. let k be a regular kernel function. definition define the kernel classification rule corresponding to the random smoothing parameter hn by gnx k lil i k ifn otherwise. i lil if and hn and nhi with probability one as n then lgn l with probability one that is gn is strongly universally consistent. proof. the theorem is a straightforward extension of theorem clearly lgn l with probability one if and only if for every e ilgn-le with probability one. now for any ilgn-le we have to show that the random variables on the right-hand side converge to zero with probability one. the convergence of the second and third terms follows from since it states that for any e there exist.b and no such that for the the conditions on hn. the convergence of the first term follows from theorem error probability lnk of the kernel rule with smoothing parameter h pl nk l e consistency for some constant c depending on the dimension only provided that n no h fj and nh d fj now clearly p l elhn fj nh fj p sup kl jjnl jj lnk l e cn sup plnk l e kl by the union bound where cn is the number of possible values of hn in the given range. as we note that cn log log fj logl on eon n by the condition on the sequence combining this with theorem for n no we get p l e hn fj n h fj e which is summable in n. the borel-cantelli lemma implies that with probability one and the theorem is proved. for weak consistency it suffices to require convergence of and nh in probability theorem assume that the random variable hn takes its values from the set of real numbers of the form where k is a nonnegative integer and on o. let k be a regular kernel. if and and nhl in probability as n then the kernel classification rule corresponding to the random smoothing eter hn is universally consistent that is lgn l in probability. we are now prepared to prove a result similar to theorem without restricting the possible values of the random smoothing parameter hn. for technical reasons automatic kernel rules we need to assume some additional regularity conditions on the kernel function k must be decreasing along rays starting from the origin. but it should not decrease too rapidly. rapidly decreasing functions such as the gaussian kernel or functions of bounded support such as the window kernel are excluded. theorem let k be a regular kernel that is monotone decreasing along rays that is for any x e nd and a kax kx. assume in addition that there exists a constant c such that for every sufficiently small and x e rd kl let be a sequence of random variables satisfying hn and nh with probability one as n then the error probability lgn of the kernel classification rule with kernel k and smoothing parameter hn converges to l with probability one that is the rule is strongly universally consistent. remark. the technical condition on k is needed to ensure that small changes in h do not cause dramatic changes in lgn. we expect some smooth behavior of lgn as a function of h. the conditions are rather restrictive as the kernels must have infinite support and decrease slower than at a polynomial rate. an example satisfying the conditions is kx if iixll otherwise where r problem the conditions on hn are by no means necessary. we have already seen that consistency occurs for atomic distributions if k and hn or for distributions with l when hn takes any value. however theorem provides us with a simple collection of sufficient conditions. proof of theorem first we discretize hn. define a sequence on satisfying the condition in theorem and introduce the random variables h n and hn as follows h n where kn is the smallest integer such that hn and let h n onh n thus h n hn h n. note that bothh.n and h n satisfy the conditions of theorem as usual the consistency proof is based on theorem here however we need a somewhat tricky choice of the denominator of the functions that approximate introduce l..il j k k i h clearly the value of the classification rule gnx equals one if and only is greater than the function defined similarly with the iyill replaced with iyioi then by theorem it suffices to show that f consistency with probability one. we use the following decomposition f f f the second term on the right-hand side converges to zero with probability one which can be seen by repeating the argument of the proof of theorem using the observation that in the proof of theorem we proved consistency via an exponential probability inequality for f ll the first term may be bounded as the following simple chain of inequalities cates h n n and the condition on k if n is large enough f k j k fldz h since h n satisfies the conditions of theorem the integral on the right-hand side converges to one with probability one just as we argued for the second term on the right-hand side of but converges to zero. therefore the first term on the right-hand side of tends to zero with probability one. remark. a quick inspection of the proof above shows that if and are deterministic sequences with the property that an bn bn and na automatic kernel rules then for the kernel estimate with kernel as in theorem we have sup lnh l with probability one allshsbll for all distributions. one would never use the worst smoothing factor over the range bn but this corollary points out just how powerful theorem is. data splitting our first example of a data-dependent hn is based upon the minimization of a suitable error estimate. you should have read chapter on data splitting if you want to understand the remainder of this section. the data sequence dn yn is divided into two parts. the first part dm yl m ynj is used for training while the remaining l n m pairs constitute the testing sequence tz ymz. the training sequence dm is used to design a class of classifiers em which in our case is the class of kernel rules based on dm with all possible values of h for fixed kernel k. note that the value of the kernel rule gmx with smoothing parameter h is zero if and only if x. h jmex lyi i so. m il classifiers in em are denoted by a classifier is selected from em that minimizes the holdout estimate of the error probability lml m l irpmxmi!ymd l il the particular rule selected in this manner is called gil the question is how far the error probability lgn of the obtained rule is from that of the optimal rule in cm. finite collections. it is computationally attractive to restrict the possible values of h to a finite set of real numbers. for example em could consist of all kernel rules with h e for some positive integer km the advantage of this choice of cm is that the best h in this class is within a factor of two of the best h among all possible real smoothing factors unless the best h is smaller than or larger than clearly lem i and as pointed out in chapter for the selected rule gn hoeffdings inequality and the union bound imply that p inf l m ej dm rpmecm if km eol then the upper bound decreases exponentially in i and in fact data splitting e lgn inf lcpm mecm i by theorem em contains a subsequence of consistent rules if m and km as n to make sure that gn is strongly universally consistent as well we only need that limn--oo i and km eol theorem under these conditions the rule is i-optimal chapter the discussion above does little to help us with the selection of m i and km safe but possibly suboptimal choices might be i n m n km n. note that the argument above is valid for any regular kernel k. infinite collections. if we do not want to exclude any value of the smoothing parameter and pick h from then em is of infinite cardinality. here we need something stronger like the vapnik theory. for example from chapter we have where sem i is the l-th shatter coefficient corresponding to the class of classifiers em. we now obtain upper bounds for seem i for different choices of k. define the function recall that for the kernel rule based on d m if fmx dm otherwise. gm we introduce the kernel complexity km km sup of sign changes of jnx yi ym as h varies from to infinity. suppose we have a kernel with kernel complexity km. then as h varies from to infinity the binary i-vector changes at most ikm times. it can thus take at most ikm different values. fore automatic kernel rules we postpone the issue of computing kernel complexities until the next section. it suffices to note that if gn is obtained by minimizing the holdout error estimate lml m by varying h then i various probability bounds may also be derived from the results of chapter for example we have theorem assume that gn minimizes the holdout estimate lml m over all kernel rules withfixed kernel k of kernel complexity km and overall smoothing factors h o. then gn is strongly universally consistent if limn-oo m lim log km n-oo hm n-oo log n k is a regular kernel. i for weak universal consistency may be replaced by limn-oo i proof. note that cm contains a strongly universally consistent take h m- for example and apply theorem noting that h yet mhd thus lim n-oo mecm inf l m l with probability one. it suffices to apply theorem and to note that the bound in is summable in n when lilog n and log km ol. for weak universal consistency a simple application of suffices to note that we only need instead of lilogn approximation errors decrease with m. for example if class densities exist we may combine the inequality of problem with bounds from devroye and gyorfi and holmstrom and klemehi to conclude thate mecm l l is of the order of with ex e under suitable conditions on k kernel complexity and the densities. by the estimation error is jlogikm i requiring instead large values for i. clearly some sort of balance is called for. ignoring the logarithmic term for now we see that i should be roughly m if we are to balance errors of both kinds. unfortunately all of this is ad hoc and based upon unverifiable distributional conditions. ideally one should let the data select i and m. see problem and problem on optimal data splitting. for some distributions the estimation error is just too large to obtain totic optimality as defined in chapter for example the best bound on the estimation error is jlog n n attained when km i i n. if the tion of x is atomic with finitely many atoms then the expected approximation error is jiii. hence the error introduced by the selection process smothers the approximation error when m is linear in n. similar conclusions may even be with if x e and if x e for the kernel rule with drawn when x has a density consider the uniform distribution on u h k the expected approximation error tends to l exponentially quickly in m and this is always far better than the estimation error which at best is jlog n n kernel complexity we now tum to the kernel complexity km. the following lemmas are useful in our computations. if l log n we note that for strong consistency it suffices that km for some finite y gust verify the proof again. this as it turns out is satisfied for nearly all practical kernels. lemma let b bm be fixed numbers and let ai e r i m befixed with the restriction that am i then the function fx aixh has at most m nonzero positive roots. proof. note first that f cannot be identically zero on any interval of nonzero length. let z denote the number of nonzero positive roots of a function g. we have z z aixci ci bi bi all i thus cl automatic kernel rules a continuously differentiable g we have zg zgl z note that the b are increasing b and a as z bm we derive our claim by simple induction on m. for am lemma let a i am be fixed real numbers and let bi bm be different nonnegative reals. then if ex the function fx l ai e- bix x? m is either identically zero or takes the value at most m times. proof. define y e-xct if ex y ranges from to by lemma gy ai ybi takes the value at at most m positive y-values unless it is identically zero everywhere. this concludes the proof of the lemma. a star-shaped kernel is one of the form kx lxea where a is a star-shaped set of unit lebesgue measure that is x f a implies cx f a for all c it is clear that km m by a simple thresholding argument. on the real line the kernel kx l-oo for ai oscillates infinitely often and has km for all values of m we must therefore disallow such kernels. for the same reason kernels such as k xx y on the real line are not good problem if k ll ai hi for some finite k some numbers ai and some star-shaped sets ai then km km consider next kernels of the form where a is star-shaped and r is a constant sebestyen we se that h m m lyi i hr iyi xill- r kernel complexity which changes sign at most as often as f m hr. from our earlier remarks it is easy to see that km m as km is the same as for the kernel k i a. if a is replaced by nd then the kernel is not integrable but clearly km o. assume next that we have kx if ilx ii otherwise where r o. for r d these kernels are integrable and thus regular. note that x hr k h illxiish illxllh iixll r as h increases f m which is of the form i illx-xilish ilx xl llr i illx-xillh transfers an xi from one sum to the other at most m times. on an interval on which no such transfer occurs fm varies as a r and has at most one sign change. therefore km cannot be more than m for each h-interval plus m for each transfer so that km l. for more practice with such computations we refer to the exercises. we now continue with a few important classes of kernels. consider next exponential kernels such as for some a where ii ii is any norm on nd. these kernels include the popular gaussian kernels. as the decision rule based on dm is of the form a simple application of lemma shows that km m. the entire class of kernels behaves nicely. among compact support kernels kernels of the form kx ai iixll b iljxjjsli for real numbers ai and hi are important. a particularly popular kernel in d-dimensional density estimation is deheuvels kernel if the kernel was kx ai ilx ilhi without the indicator function then lemma would immediately yield the estimate km k uniformly over all m. such kernels would be particularly interesting. with the indicator function automatic kernel rules multiplied in we have km km simply because fmx at each h is based upon a subset of the xjs j m with the subset growing monotonically with h. for each subset the function fm is a polynomial in iix ii with powers bi bk and changes sign at most k times. therefore polynomial kernels of compact support also have small complexities. observe that the in the bound km km refers to the number of terms in the polynomial and not the maximal power. a large class of kernels of finite complexity may be obtained by applying the rich theory of total positivity. see karlin for a thorough treatment. a valued function l of two real variables is said to be totally positive on a x b c if for all n and all sn si e a tl tn ti e b the determinant of the matrix with elements lsi tj is nonnegative. a key property of such functions is the following result which we cite without proof theorem let l be a totally positive function on a x b e r and let a b r be a bounded function. define the function ls tatadt on a where a is a finite measure on b. then changes sign at most as many times as as does. number of sign changes of a function is defined as the supremum of sign changes of sequences of the form where n is arbitrary and sn. corollary assume that the kernel k is such that the function ls t kst is totally positive for s and t e rd. then the kernel complexity of k satisfies km m proof. we apply theorem we are interested in the number of sign changes of the function m ikxi xs il on s e plays the role of h. but may be written as ls tatadt where ls t kst the measure a puts mass ion each t xi i and act is defined at these points as a yi if t xi x. other values of act are irrelevant for the integral above. clearly act can be defined such that it changes sign at most m times. then theorem implies that changes sign at most m times as desired. this corollary equips us with a whole army of kernels with small complexity. for examples refer to the monograph of karlin multiparameter kernel rules multiparameter kernel rules assume that in the kernel rules considered in em we perform an optimization with respect to more than one parameter. collect these parameters in e and write the discrimination function as fmx t kex xi il examples product kernels. take kextik h d jl where e hd is a vector of smoothing factors-one per sion-and k is a fixed one-dimensional kernel. kernels of variable form. define kex e-uxlllh where a is a shape parameter and h is the standard smoothing parameter. here e h is two-dimensional. define i kex rrtx iirxii where x t is the transpose of x and r is an orthogonal transformation matrix all of whose free components taken together are collected in e. kernels of this kind may be used to adjust automatically to a certain variance-covariance structure in the data. we will not spend a lot of time on these cases. clearly one route is to properly generalize the definition of kernel complexity. in some cases it is more convenient to directly find upper bounds for seem l. in the product kernel case with dimensional kernel we claim for example that seem i the corresponding rule takes a majority vote over centered rectangles with sides equal to to see why the inequality is true consider the dimensional quadrant of points obtained by taking the absolute values of the vectors xj xi m j m i n i m where the absolute value of a vector is a vector whose components are the absolute values of the components ofthe vector. to compute seem l it suffices to count how many different subsets can be obtained from these lm points by considering all possible rectangles with one vertex at the origin and the diagonally opposite vertex in the quadrant. this is the strong universal consistency of the latter family em is insured when m and i! log n automatic kernel rules kernels of infinite complexity in this section we demonstrate that not every kernel function supports smoothing factor selection based on data splitting. these kernels have infinite kernel plexity or even worse infinite vc dimension. some of the examples may appear rather artificial but some kernels will surprise us by misbehaving. we begin with a kernel k having the property that the class of sets has vc dimension va for any fixed value of xl using the notation of the previous sections vel hence with one sample point all hope is lost to use the vapnik-chervonenkis inequality in any meaningful way. unfortunately the kernel k takes alternately positive and negative values. in the second part of this section a kernel is constructed that is unimodal and symmetric and has vem for m when xl takes certain values. finally in the last part we construct a positive kernel with the property that for any m and any nondegenerate non atomic distribution limmoo p oo we return to a. our function is picked as follows kx axgi s x il i where gi e i for all i ax for all x ax as x t x and ax as x x monotonicity is not essential and may be dropped but the resulting class of kernels will be even less esting. we enumerate all binary strings in lexicographical order replace all os by and map the bit sequence to gl hence the bit sequence becomes call this sequence s. for every n we can find a set i x n that can be shattered by sets from a. take xl a subset of this may be characterized by a string sn from denoting membership and denoting absence. we find the first occurrence of sn in s and let the starting point be gk. take h observe that k x i k xn x i n which agrees in sign with sn as desired. hence the vc dimension is infinite. the next kernel is symmetric unimodal and piecewise quadratic. the intervals into which is divided are denoted by ao ai from left to right kernels of infinite complexity where i i o. on each ai k is of the form ax bx c. observe that kf! takes the sign of a. also any finite symmetric difference of order has the sign of a as k k x x x e ai. we take four points and construct the class a. t k x x o h o where xl yi are fixed for now. on ai we let the coefficient have the same sign as gi known from the previous example. all three quadratic coefficients are picked so that k and k is unimodal. for each n we show that the set can be shattered by intersecting with sets from a subset is again identified by a ln-valued string sn and its first match in sis gk gk n we take h note that for i n sign k k sign k l sign by the finite-difference property of quadratics gki as desired. hence any subset of can be picked out. the previous example works whenever it takes just a little thought to see that if yr are i.i.d. and drawn from the distribution of y then p ivai oo pxi yl yz i and this is positive if given y x has atoms at and for some and given y x has an atom at o. however with some work we may even remove these restrictions problem we also draw the readers attention to an example in which k is metric unimodal and convex on yet ves problem next we turn to our general m-point example. let the class of rules be given by g mh if i otherwise i lk h automatic kernel rules h where the data yi ym are fixed for now. we exhibit a nonnegative kernel such that if the xis are different and not all the yis are the same vcm this situation occurs with probability tending to one whenever x is nonatomic and py i e it is stressed here that the same kernel k is used regardless of the data. the kernel is of the form kx koxiax where kox and a c r will be specially picked. order x i xm into x cm and find the first pair xi xui with opposite values for yi. without loss of generality assume that xci and is the y-value that corresponds to xci let the smallest value of j i j i be denoted by the contribution of all such xjs for x e is not more than h mkol h. the contribution of either xci or xil is at least h. for fixed and m all they are given we first find h such that h h implies ko h h for h h the rule for x e is equivalent to a rule based on if k ezi k e-i gmh otherwise. we define the set a by au u kl kl where the sets akl are specified later. c and b c r c b e r x c e b. here k represents the length of a bit string and i cycles through all bit strings of length k. for a particular such bit string bi bk represented by l we define akl as follows. first of all akl so that all sets akl are nonoverlapping. akl consists of the sets u j akl is completed by symmetry. we now exhibit for each integer k a set of that size that can be shattered. these sets will be positioned in at coordinate values p where p is a suitable large integer such that assume that we wish to extract the set indexed by the bit vector bk means that must be extracted. to do so we are only allowed to vary h. first we find a pair i where k is at least k and on minimizing the apparent error rate s h is needed too. also is such that it matches bi h in its first k s k bits. take h and p k and observe that gnh k if k otherwise c- c- if e akl if akl unimportant otherwise if bi if bi akl e akl i s k. thus we pick the desired set. this construction may be repeated for all values of of course. we have shown the following theorem if x is nonatomic py i e and if vcm denotes the vc dimension of the class of kernel rules based on m i.i.d. data drawn from the distribution of y and with the kernel specified above then lim pvcm oo m-oo on minimizing the apparent error rate in this section we look more closely at kernel rules that are picked by minimizing the resubstitution estimate lr over kernel rules with smoothing factor h o. we make two remarks in this respect the procedure is generally inconsistent if x is nonatomic. the method is consistent if x is purely atomic. to see take k lsod we note that lr if h miniij iixi xj ii as gnxi yi for such h. in fact if hn is the minimizing h it may take any value on if x is independent of y py i and x has a density then lim lim inf p ii xi x j ii d. c--oo n--oo lj.y-lyj-o n thus nhd in probability and therefore in this case elgj ties are broken by favoring the class. hence the inconsistency. automatic kernel rules consider next x purely atomic. fix yd yn the data and sider the class en of kernel rules in which h is a free parameter. if a typical rule is gnh let an be the class of sets gnhx i h o. if gn is the rule that minimizes lrgnh we have sup gizec sup sup aean denotes the collection of sets gnhx od aea sup beb where b is the collection of all borel sets. however the latter quantity tends to zero with probability one-as denoting the set of the atoms of x by t we have sup beb l l!tnx d d! xet l xea flxdi c flac a is an arbitrary finite subset of t l xea the first term is small by choice of a and the last two terms are small by applying hoeffdings inequality to each of the terms. for yet a different proof the reader is referred to problems and note next that but if ko and kx as iixll along any ray then we have infgnhecn lgnh lgl where gz is the fundamental rule discussed in chapter which h theorem shows that l with probability one and therefore lgn with probability one proving theorem take any kernel k with ko and kx as i!xll along any ray and let gn be selected by minimizing lr over all h o. then lgn l almost surely whenever the distribution of x is purely atomic. minimizing the deleted estimate remark. the above theorem is all the more surprising since we may take just about any kernel such as kx e- kx siniix ii or kx also if x puts its mass on a dense subset of n d the data will look and feel like data from an absolutely continuous distribution very roughly speaking yet there is a dramatic difference with rules that minimize lr when the xs are indeed drawn from a distribution with a density! minimizing the deleted estimate we have seen in chapter that the deleted estimate of the error probability of a kernel rule is generally very reliable. this suggests using the estimate as a basis of selecting a smoothing factor for the kernel rule. let ldgnh be the deleted estimate of lgnh obtained by using in gn-lh the same kernel k and smoothing factor h. define the set of h s for which l n gnh heooo f ld in gnh by a. set hn inf h e a. two fundamental questions regarding hn must be asked if we use hn in the kernel estimate is the rule universally consistent? note in this respect that theorem cannot be used because it is not true that hn in probability in all cases. if hn is used as smoothing factor how does elgn l compare to e lgnlj l to our knowledge both questions have been unanswered so far. we believe that this way of selecting h is very effective. below generalizing a result by tutz we show that the kernel rule obtained by minimizing the deleted estimate is consistent when the distribution of x is atomic with finitely many atoms. remark. if i everywhere so that yi i for all i and if k has support equal to the unit ball in n d and k on this ball then ldgnh is minimal and zero if where x f n denotes the nearest neighbor of x j among the data points xl xj-l xjl x n but this shows that hn mx iixj xfnii. l-s-sn automatic kernel rules we leave it as an exercise to show that for any nonatomic distribution of x n hl with probability one. however it is also true that for some distributions hn with probability one nevertheless for the rule is consistent! if everywhere then yi for all i. under the same condition on k as above with tie breaking in favor of class in the entire book it is clear that hn o. interestingly here too the rule is consistent despite the strange value of hn. o we state the following theorem for window kernels though it can be extended to include kernels taking finitely many different values problem theorem let gn be the kernel rule with kernel k iafor some bounded set a c nd containing the origin and with smoothing factor chosen by minimizing the deleted estimate as described above. then e lgn l whenever x has a discrete distribution with finitely many atoms. proof. let ui un be independent uniform random variables dent of the data d n and consider the augmented sample where x vi. for each h introduce the rule ghx! if lil k lil o otherwise liuui where xl u e nd x observe that minimizing the resubstitution estimate lrgh over these rules yields the same h as minimizing ldgnh over the original kernel rules. furthermore e lgn e lg if g is obtained by minimizing lrglh it follows from the universal consistency of kernel rules that lim inf lg h l with probability one. n--oo h thus it suffices to investigate lg subsets al c nd x by infh lgh for a given d define the a ghxi i h e arguing as in the previous section we see that minimizing the deleted estimate where v is the measure of x on nd x and vn is the empirical measure determined by d. observe that each a can be written as ah e rd t k e xi o u end x un t k e xi ko liuu where and clearly as it suffices to prove that suph in probability as n then sup ivnb h sup ivnb sup ivcb h h sup ivncb sup ivb mahl. h the first term on the right-hand side tends to zero in probability which may be seen by problem and the independence of the ui s of dn. to bound the second term observe that for each h ivb fl it k e xi o!- ko t px xj k xjxi n where x i x n are the atoms of x. thus the proof is finished if we show that for each xj automatic kernel rules now we exploit the special form k fa of the kernel. observe that ii i h l..ll a su.p fin i n l fivivk isi where kl vk l ixixk and xl is an ordering of the atoms of x such that x j and xk h e a implies xk-l h e a for k n. by properties of the binomial distribution as n this completes the proof. historical remark. in an early paper by habbema hermans and van den broek the deleted estimate is used to select an appropriate subspace for the kernel rule. the kernel rules in tum have smoothing factors that are selected by maximum likelihood. sieve methods sieve methods pick a best estimate or rule from a limited class of rules. for example our sieve ck might consist of rules of the form where the ai are real numbers the xi are points from n d and the his are positive numbers. there are formally free scalar parameters. if we pick a rule that minimizes the empirical error on yi yn we are governed by the theorems of chapters and and we will need to find the vc dimension of ck for this conditions on k will be needed. we will return to this question in chapter on neural networks as they are closely related to the sieves described here. squared eltor minimization squared error minimization many researchers have considered the problem of selecting h in order to minimize the error squared error of the kernel estimate y. k n l..ll k n l..ll h h of the regression function p x x for example hardie and marron proposed and studied a cross-validation method for choosing the optimal h for the kernel regression estimate. they obtain asymptotic optimality for the integrated squared error. although their method gives us a choice for h if we consider py llx x as the regression function it is not clear that the h thus obtained is optimal for the probability of error. in fact as the following theorem illustrates for some distributions the smoothing parameter that minimizes the error yields a rather poor error probability compared to that corresponding to the optimal h. theorem let d l. consider the kernel classification rule with the window kernel k and smoothing parameter h o. denote its error probability by lnh. let h be the smoothing parameter that minimizes the mean integrated squared error then for some distributions elnh hm n-oo infh elnh and the convergence is exponentially fast. we leave the details of the proof to the reader only a rough sketch is given here. consider x uniform on u and define if x e x otherwise. i x the optimal value of h the value minimizing the error probability is one. it is constant independent of n. this shows that we should not a priori exclude any values of h as is commonly done in studies on regression and density estimation. the minimal error probability can be bounded from above hoeffdings inequality by e- for some constant cl on the other hand straightforward calculations show that the smoothing factor h that minimizes the mean integrated automatic kernel rules squared error goes to zero as n as i the corresponding error probability is larger than for some constant the order-of-magnitude difference between the exponents explains the exponential speed of convergence to infinity. problems and exercises problem prove theorem problem show that for any r the kernel kx if ilxll s otherwise satisfies the conditions of theorem problem assume that k is a regular kernel with kernel complexity km s my for some constant y let gn be selected so as to minimize the holdout error estimate over cm the class of kernel rules based upon the first m data points with smoothing factor h assume furthermore that we vary lover n and that we pick the best l m n by minimizing the holdout error estimate again. show that the obtained rule is strongly universally consistent. problem prove that the kernel complexity km of the de la vallee-poussin kernel kx x e r is infinite when m problem show that km for m when kx x e r. problem compute an upper bound for the kernel complexity km for the following kernels where x xed e rd a. b. c. d. kx d ti il i-x iuxiil d kx ex kx ti kx ti il d il problem can you construct a kernel on r with the property that its complexity satisfies s km for all m? prove your claim. problem show that for kernel classes cm with kernel rules having a fixed training sequence dm but variable h we have vcm s km problems and exercises problem calculate upper bounds for sem when em is the class of kernel rules based on fixed training data but with variable parameter e in the following cases definitions see section kg is a product kernel of r d where the unidimensional kernel k has kernel complexity km. kg llixlaha where e a h a are two parameters. kgx lxea where a is any ellipsoid ofrd centered at the origin. problem prove or disprove if dm is fixed and em is the class of all kernel rules based on dm with k la a being any convex set ofrd containing the origin is it possible that vcm or is vcm for all possible configurations of dm? problem let as t k x i o h o with xl x xs yi ys and let k be piecewise cubic. extending the quadratic example in the text show that the vc dimension of is infinite for some k in this class that is symmetric unimodal positive and convex on problem draw yd the distribution of y on rd xo i where x has a density and e at all x. find a symmetric unimodal k such that as x t k x x i h has vc dimension satisfying p v oo can you find such a kernel k with a bounded support? problem let x have a density f on rd and let xl xn bei.i.d. drawn from f. show that lim liminfp iixi xjllds n apply this result in the following situation define hn minijylyjo iixi xi ii where yd yn are i.i.d. rd x random variables distributed as y with x absolutely continuous y independent of x and py i e show that lim lim inf p c n conclude that nhd in probability. if you have a kernel rule with kernel on r d and if the smoothing factor hn is random but satisfies nh in probability then lim el n pry whenever x has a density. show this. problem consider the variable kernel rule based upon the variable kernel density estimate of breiman meisel and purcell and studied by krzyzak gnx otherwise. if khx xi liyo khx xi automatic kernel rules here k is a positive-valued kernel ku for u and hi is the distance between xi and the k-th nearest neighbor of xi among xj j ii j n. investigate the consistency of this rule when k n and k and x has a density. problem continuation. fix k and let k be the normal density in nd. if xl has a density what can you say about the asymptotic probability of error of the variable kernel rule? is the inequality of cover and hart still valid? repeat the exercise for the uniform kernel on the unit ball of nd. problem if x xl are discrete i.i.d. random variables and n denotes the ber of different values taken by x i x n then en hm n and n n with probability one. hint for the weak convergence assume without loss of generality that the probabilities are monotone. for the strong convergence use mcdiarmids inequality. problem let b be the class of all borel subsets of nd. using the previous exercise show that for any discrete distribution sup ijlna jlai with probability one. aeb hint recall the necessary and sufficient condition e n a xn n from chapter problem prove theorem allowing kernel functions taking finitely many ferent values. problem let x i xn be an li.d. sample drawn from the distribution of x. let xfn denote the nearest neighbor of xj among xl x j x j x n define bn max iix xfnii j show that for all nonatomic distributions of x nb with probability one. is it true that for every x with a density there exists a constant c such that with probability one nb clog n for all n large enough? exhibit distributions on the real line for which bn with probability one. hint look at the difference between the first and second order statistics. problem prove theorem hint use the example given in the text. get an upper bound for the error probability corresponding to h by hoeffdings inequality. the mean integrated squared error can be computed for every h in a straightforward way by observing that split the integral between and in three parts to h h to h and h to setting the derivative of the obtained expression with respect to h equal to zero leads to a third-order problems and exercises to get a lower bound for the corresponding error equation in h whose roots are on- i probability use the crude bound iii i-h elnh i yix x now estimate the tail of a binomial distribution from below and use stirlings formula to show that modulo polynomial factors the error probability is larger than automatic nearest neighbor rules the error probability of the k-nearest neighbor rule converges to the bayes risk for all distributions when k and kin as n the convergence result is extended here to include data-dependent choices of k. we also look at the data-based selection of a metric and of weights in weighted nearest neighbor rules. to keep the notation consistent with that of earlier chapters random values of k are denoted by kn. in most instances kn is merely a function of dn the data sequence yd yn. the reader should not confuse kn with the kernel k in other chapters. consistency we start with a general theorem assessing strong consistency of the k-nearest neighbor rule with data-dependent choices of k. for the sake of simplicity we assume the existence of the density of x. the general case can be taken care of by introducing an appropriate tie-breaking method as in chapter theorem let k k be integer valued random variables and let gn be the kn-nearest neighbor rule. if x has a density and kn and knn with probability one as n then lgn l with probability one. automatic nearest neighbor rules proof. lgj l with probability one if and only if for every e oilgn-ue with probability one. clearly for any we are done if both random variables on the right-hand side converge to zero with probability one. the convergence of the second term for all follows trivially from the conditions of the theorem. the convergence of the first term follows from the remark following theorem which states that for any e there exist and no such that for the error probability lnk of the k-nearest neighbor rule pl nk l e for some constant c depending on the dimension only provided that n no k and kin now clearly plgn l e ii kn knln p sup lnk l e n sup plnk l e by the union bound. combining this with theorem we get plgn l e ii kn knln n no. the borel-cantelli lemma implies that with probability one and the theorem is proved. sometimes we only know that kn and knln in probability. in such cases weak consistency is guaranteed. the proof is left as an exercise theorem let ki be integer valued random variables and let gn be the kn neighbor rule. if x has a density and kn and knln in probability as n then limn-- oo elgn l that is gn is weakly consistent. data splitting consistency by itself may be obtained by choosing k l fn j but few-if users will want to blindly use such recipes. instead a healthy dose of back from the data is preferable. if we proceed as in chapter we may split data splitting for weighted nn rules the data sequence dn yi yn into a training sequence dm yd ym and a testing sequence tz yml yn where m i n. the training sequence dm is used to design a class of classifiers em. the testing sequence is used to select a classifier from em that minimizes the holdout estimate of the error probability if em contains all k-nearest neighbor rules with i k m then m. therefore we have where gn is the selected k-nearest neighbor rule. by combining theorems and we immediately deduce that gn is universally consistent if lim m n---oo i n---oo log m it is strongly universally consistent if limn---oo log n also. note too that e inf l problem so that it is indeed important to pick i much larger than log m. data splitting for weighted nn rules royall introduced the weighted nn rule in which the i-th nearest neighbor receives weight wi where wi wk and the wis sum to one. we assume that wkl wn if there are n data points. besides the natural appeal of attaching more weight to nearer neighbors there is also a practical product if the wis are all of the form zi where zi is a prime integer then no two subsums of wis are equal and therefore voting ties are avoided altogether. consider now data splitting in which em consists of all weighted k-nn rules as described above-clearly k m now. as lem i we compute the shatter coefficients seem i. we claim that if i k if i k. this result is true even if we do not insist that wi wk o. proof of each xj in the testing sequence is classified based upon the sign of ll aij wi where aij e i depends upon the class of the i nearest automatic nearest neighbor rules neighbor of xj among xl xm does not depend upon the wis. consider the i-vector of signs of aijwi m j s n. in the computation of seem we consider the aijs as fixed numbers and vary the wis subject to the condition laid out above. here is the crucial step in the argument the collection of all vectors wk for which xj is assigned to class is a linear halfspace of rk. therefore seem l is bounded from above by the number of cells in the partition of rk defined by llinear halfspaces. this is bounded by g problem if k s l. let gn be the rule in lem i that minimizes the empirical error committed on the test sequence yn. then by if n m k we have implies m we have universal consistency when if k k logl l o. the estimation error is of the order of jk log the nology of chapter the rule is j k log l i-optimal. this error must be weighed against the unknown approximation error. let us present a quick heuristic ment. on the real line there is compelling evidence to suggest that when x has a smooth density k is nearly optimal. with this choice if both and m grow linearly in n the estimation error is of the order of jlog n n this is painfully large-to reduce this error by a factor of two sample sizes must rise by a factor of about the reason for this disappointing result is that em is just too rich for the values of k that interest us. automatic selection may lead to rules that overfit the data. if we restrict em by making m very small the following rough argument may be used to glean information about the size of m and i n m. we will take k m n. for smooth regression function the estimation error may be anywhere between m and m on the real line. as the estimation error is of the order of jmlognn equating the errors leads to the rough recipe that m and m respectively. both errors are then about and respectively. this is better than with the previous example with m linear in n. unfortunately it is difficult to test whether the conditions on that guarantee certain errors are satisfied. the above procedure is thus doomed to remain heuristic. reference data and data splitting split the data into dm and tz as is done in the previous section. let em contain alli-nn rules that are based upon the data yi yk where k s m is to be picked xk c xm and yk e ik. note that because the ys are free parameters yi yk is not necessarily a subset of i yi ym-this allows us to flip certain y-values at some data points. trivially lem i hence variable metric nn rules where i n m and gn e em minimizes the empirical error on the test sequence tz. the best rule in em is universally consistent when k theorem therefore gn is universally consistent when the above bound converges to zero. sufficient conditions are lim i klogm lim o. n-oo i as the estimation error is j k log m i it is important to make i large while keeping k small. the selected sequence yi yk may be called reference data as it captures the information in the larger data set. if k is sufficiently small the computation of gn is extremely fast. the idea of selecting reference data or throwing out useless or data points has been proposed and studied by many researchers under names such as condensed nn rules edited nn rules and selective nn rules. see hart gates wilson wagner ullmann ritter et al. tomek and devijver and kittler see also section variable metric nn rules the data may also be used to select a suitable metric for use with the k-nn rule. the metric adapts itself for certain scale information gleaned from the data. for example we may compute the distance between xl and by the formula aat lxi where is a column vector l denotes its transpose a is a d x d transformation matrix and l aa t is a positive definite matrix. the elements of a or l may be determined from the data according to some heuristic formulas. we refer to fukunaga and hostetler short and fukunaga fukunaga and flick and myles and hand for more information. for example the object of principal component analysis is to find a tion matrix a such that the components of the vector a t x have unit variance and are uncorrelated. these methods are typically based on estimating the eigenvalues of the covariance matrix of x. for such situations we prove the following general consistency result automatic nearest neighbor rules theorem let the random metric pn be of the form pnx y iia yli where the matrix an is a function of xl x n assume that distance ties occur with zero probability and there are two sequences of nonnegative random variables and such thatfor any n and x y end and if p mn o o. n-oo mn lim kn and n-oo kn hm n-oo n then the kn neighbor rule based on the metric pn is consistent. proof. we verify the three conditions of theorem in this case wnjx kn if xi is one of the kn nearest neighbors of x to pn and zero otherwise. condition holds trivially. just as in the proof of consistency of the ordinary knnearest neighbor rule for condition we need the property that the number of data points that can be among the kn nearest neighbors of a particular point is at most kn yd where the constant yd depends on the dimension only. this is a deterministic property and it can be proven exactly the same way as for the standard nearest neighbor rule. the only condition of theorem whose justification needs extra work is condition we need to show that for any a lim e o. n-oo il denote the k-th nearest neighbor of x according to pn by xk and the k-th nearest neighbor of x according to the euclidean metric by xk. then e wnixiiix-xdlaj mn iix yll pnx y selection of k based on the deleted estimate p pnx xkn p ipnxxmna kn mnllx y ii pnx y piix-xdl but we know from the consistency proof of the ordinary neighbor rule in chapter that for each a lim p xkn ii a o. it follows from the condition on mn mn that the probability above converges to zero as well. the conditions of the theorem hold if for example the elements of the matrix an converge to the elements of an invertible matrix a in probability. in that case we may take mn as the smallest and as the largest eigenvalues of a an. then mn converges to the ratio of the smallest and largest eigenvalues of at a a positive number. if we pick the elements of a by minimizing the empirical error of a test sequence tz over cn where cm contains all k-nn rules based upon a training sequence dm the elements of a are the free parameters the value of scm l is too large to be useful-see problem furthermore such minimization is not computationally feasible. selection of k based on the deleted estimate if you wish to use all the available data in the training sequence without splitting then empirical selection based on minimization of other estimates of the error probability may be your solution. unfortunately performance guarantees for the selected rule are rarely available. if the class of rules is finite as when k is selected from n there are useful inequalities. we will show you how this works. automatic nearest neighbor rules let en be the class of all k-nearest neighbor rules based on a fixed training sequence dn but with k variable. clearly i en i n. assume that the deleted estimate is used to pick a classifier gn from en we can derive performance bounds for gn from theorem since the result gives poor bounds for large values of k the range of ks has to be restricted-see the discussion following theorem let ko denote the value of the largest k allowed that is en now contains all k-nearest neighbor rules with k ranging from to ko. theorem let gn be the classijierminimizing the deleted estimate of the error probability over en the class of k-nearest neighbor rules with k ko. then where c is a constant depending on the dimension only. if ko ko log nj n then gn is strongly universally consistent. and proof. from theorem we recall the inequality now follows from theorem via the union bound. universal consistency follows from the previous inequality and the fact that the ko-nn rule is strongly universally consistent theorem problems and exercises problem let k i k be integer valued random variables and let gn be the nearest neighbor rule. show that if x has a density and kn and kill n in probability as n then elgn l. problem let c be the class of all l-nn rules based upon pairs yi yk where k is a fixed parameter varying with n and the yis are variable pairs from r d x l. let gn be the rule that minimizes the empirical error over c or equivalently let gn be the rule that minimizes the resubstitution estimate lr over c. compute a suitable upper bound for sc n. compute a good upper bound for vc as a function of k and d. if k with n show that the sequence of classes c contains a strongly um. versally consistent subsequence of rules may assume for convenience that x has a density to avoid distance ties. under what condition on k can you guarantee strong universal consistency of gn! give an upper bound for problems and exercises problem let cm contain all k-nn rules based upon data pairs yl ym. the metric used in computing the neighbors is derived from the norm d ii l l x xu x o xed d il jl where forms a positive definite matrix the elements of are the free parameters in cm. compute upper and lower bounds for seem i as a function of m i k and d. problem let gn be the rule obtained by minimizing ld over all k-nn rules with k n. prove or disprove gn is strongly universally consistent. note that in view of theorem it suffices to consider en log n k n for all e o. hypercubes and discrete spaces in many situations the pair y is purely binary taking values in id x i. examples include boolean settings component of x represents or representations of continuous variables through quantization variables are always represented by bit strings in computers and ordinal data component of x is i if and only if a certain item is present. the components of x are denoted by x xd. in this chapter we review pattern recognition briefly in this setup. without any particular structure in the distribution of y or the function py iix x x e id the pattern recognition problem might as well be cast in the space of the first positive integers y e x i. this is dealt with in the first section. however things become more interesting under certain structural assumptions such as the assumption that the components of x be independent. this is dealt with in the third section. general discrimination rules on hypercubes are treated in the rest of the chapter. multinomial discrimination at first sight discrimination on a finite set multinomial mination-may seem utterly trivial. let us call the following rule the fundamental rule as it captures what most of us would do in the absence of any additional information. hypercubes and discrete spaces the fundamental rule coincides with the standard kernel and histogram rules if the smoothing factor or bin width are taken small enough. if pi px i and yj is as usual then it takes just a second to see that yjxpx and eln l yjxpxcl pxn x where ln lg. assume yjx at all x. then l and eln l pxl pxn. x if px ii k for all x we have eln if k this simple calculation shows that we cannot say anything useful about fundamental rules unless k at the very least. on the positive side the following universal bound is useful. ii kn theorem for the fundamental rule we have ln l with probability one as n and in fact for all distributions eln l k and eln l proof. the first statement follows trivially from the strong universal consistency of histogram rules theorem it is the universal inequality that is of interest here. if bn p denotes a binomial random variable with parameters nand p then we have k eln lpx yjx pbnx yjx xl nx is a binomial px random variable k x from this if sex multinomial discrimination sex nxsxl g sex k xl xl sex the chebyshev-cantelli inequality-theorem l px sex l t l t pal l nx l t pxe o l t pxn t px e l t p the function u n jensens inequality nx k xv xl for u lemma and the fact that the worst distribution has px k for all x l k ljp xl l fk e-u for u en v and by the cauchy-schwarz inequality. this concludes the proof as e for several key properties of the fundamental rule the reader is referred to glick and to problem other references include krzanowski stein and goldstein and dillon note also the following extension of theorem which shows that the fundamental rule can handle all discrete distributions. hypercubes and discrete spaces theorem if x is purely atomic possibly infinitely many atoms then ln l with probability one for the fundamental rule. proof. number the atoms define xi minx k and replace yt by yi where x minxi k. apply the fundamental rule to the new problem and note that by theorem if k is fixed ln l with probability one for the new rule new distribution. however the difference in lns and in l between the truncated and nontruncated versions cannot be more than p k which may be made as small as desired by choice of k. quantization consider a fixed partition of space nd into k sets ad and let gn be the standard partitioning rule based upon majority votes in the ais are broken by favoring the response as elsewhere in the book. we consider two rules the rule gn considered above the data are yi yn with y e nd x to i. the probability of error is denoted by l n and the bayes probability of error is l with py llx end. the fundamental rule operating on the quantized data yn with y e k x to i x j if x e a j the bayes probability of error is l with py xi xl xl e k. the probability of error is denoted by clearly g is nothing but the fundamental rule for the quantized data. as gnx where xl j if x e a j we see that however the bayes error probabilities are different in the two situations. we claim however the following theorem for the standard partitioning rule eln l where e furthermore l l l o. quantization proof. clearly l l problem also l eminnx nx eminnx nx furthermore eln s l s l s by theorem as an immediate corollary we show how to use the last bound to get useful distribution-free performance bounds. theorem let f be a class of distributions of yfor which x e ld with probability one and for some constants c ci i cllx zw x z e rd. then ifwe consider all cubic histogram rules gn chapter for a definition we have inf cubic histogram rule gn elgn l a fn b n where a and b are constants depending upon c ci and d only the prooffor explicit expressions. theorem establishes the existence of rules that perform uniformly at rate o a over f. results like this have an impact on the number of data points required to guarantee a given performance for any y e f. proof. consider a cubic grid with cells of volume hd covering ld does not exceed h we apply theorem to obtain as the number of cells elgn s l s where sup c sup sup liz xlla c x ai xzeai the right-hand side is approximately maximal when h in def c n resubstitution yields the result. hypercubes and discrete spaces independent components let x xcd have components that are conditionally independent given and also given oj. introduce the notation pxci pi qi pxi o p py with x xed e ld we see that px x ppx xly ppx xly o and px xly it piti pil-xi d il px xly o it qiti qil-xu d simple consideration shows that the bayes rule is given by il if p nl pixi pil-xi gx p nl qil-xi o otherwise. taking logarithms it is easy to see that this is equivalent to the following rule g otherwise l....il x ci where log log pi ft p qi log pi qi qi pi l d. in other words the bayes classifier is linear! this beautiful fact was noted by minsky winder and chow having identified the bayes rule as a linear discrimination rule we may apply the full force of the vapnik-chervonenkis theory. let gn be the rule that minimizes the empirical error over the class of all linear discrimination rules. as the class independent components of linear halfspaces of rd has vc dimension d corollary we recall from theorem that for gn plgn l e elgn l corollary plgn l e ne and where elgn ls yfi c c these bounds are useful they tend to zero with n if d onlogn. in contrast without the independence assumption we have pointed out that no trivial guarantee can be given about elgn unless n. the independence assumption has led us out of the high-dimensional quagmire. one may wish to attempt to estimate the pis and qis by pi and qi and to use these in the plug-in rule gn that decides if d p n putu pil-xi fi n qixi qul-xi d il il where p is the standard sample-based estimate of p. the maximum-likelihood estimate of pi is given by while q-i ll ixil yo n ljl l j with equal to problem note that the plug-in rule too is linear with nx g ao l...il ai x l otherwise where ao p log log p il no! no! nll nooi nloi nooi ai log nll nooi nol nloi l d hypercubes and discrete spaces and n nooi l ixjoyjo jl n n loi l ixjlyjo jl n noli l ixjoyjl jl n nlli l ixjilyjl jl d p l nll il for all this we refer to warner toronto veasey and stephenson also mclachlan use the inequality elgn l where is as given in the text and is as but with p pi qi replaced by p pi qi to establish consistency theorem for the plug-in rule lgn l with probability one and elgn l whenever the components are independent. we refer to the problem section for an evaluation of an upper bound for e l problem linear discrimination on the hypercube has of course limited value. the world is full of examples that are not linearly separable. for example on if y xo that y implements the boolean or or function the problem is not linearly separable if all four possible values of x have positive probability. however the exclusive or function may be dealt with very nicely if one considers quadratic discriminants dealt with in a later section on series methods. boolean classifiers by a boolean classification problem we mean a pattern recognition problem on the hypercube for which l this setup relates to the fact that if we consider y i as a circuit failure and y as an operable circuit then y is a deterministic function of the xis which may be considered as gates or switches. in that case y may be written as a boolean function of x xcd. we may limit boolean classifiers in various ways by partially specifying this function. for example lowing natarajan we first consider all monomials that is all functions g ld i of the form boolean classifiers for some k d and some indices il ik d that algebraic tiplication corresponds to a boolean in such situations one might attempt to minimize the empirical error. as we know that g is also a monomial it is clear that the minimal empirical error is zero. one such minimizing monomial is given by where d an mm ljon xu i d ll lk thus gn picks those components for which every data point has a clearly the empirical error is zero. the number of possible functions is therefore by theorem and elgn n here again we have avoided the curse of dimensionality. for good performance it suffices that n be a bit larger than d regardless of the distribution of the data! assume that we limit the complexity of a boolean classifier g by requiring that g must be written as an expression having at most k operations or with the xis as inputs. to avoid problems with precedence rules we assume that any number of parentheses is allowed in the expression. one may visualize each expression as an expression tree that is a tree in which internal nodes represent operations and leaves represent operands the number of such binary trees with k internal nodes thus k leaves is given by the catalan number k k e.g. kemp furthermore we may associate any of the xis with the leaves preceded by and or with each binary internal node thus obtaining a total of not more than k possible boolean functions of this kind. as k this bound is not more than for k large enough. again for k large enough hypercubes and discrete spaces and elgn k n note that k is much more important than d in determining the sample size. for historic reasons we mention that if g is any boolean expression consisting of at most k or operations then the number of such functions was shown by pippenger not to exceed k y pearl used pippengers estimate to obtain performance bounds such as the ones given above. series methods for the hypercube it is interesting to note that we may write any function rt on the hypercube as a linear combination of rademacher-walsh polynomials io i id i d i d i we verify easily that so that the form an orthogonal system. therefore we may write fl-xrtx l ai io where series methods for the hypercube and px x. also l biljix io with bi lrtxl ryx jlx e liyoi sample-based estimates of ai and bi are the bayes rule is given by replacing ai bi formally by b yields the plug-in rule. observe that this is just a discrete version of the fourier series rules discussed in chapter this rule requires the estimation of differences ai bi therefore we might as well have used the fundamental rule. when our hand is forced by the dimension we may wish to consider only rules in the class c given by gx i if b lr otherwise i i where k d is a positive integer and ab bb ai b are arbitrary constants. we have seen that the vc dimension of this class is not more than d k within this class estimation errors of the order of dk log n n are thus possible if we minimize the empirical error this in effect forces us to take k logd n. for larger k pattern recognition is all but impossible. as an interesting side note observe that for a given parameter k each member of c is a k-th degree polynomial in x. remark. performance of the plug-in rule. define the plug-in rule by gnx i o otherwise. if t. lr i i hypercubes and discrete spaces as n by the law oflarge numbers gn approaches goo goox ai o otherwise lio b i x where ai bi are as defined above. interestingly goo may be much worse than the best rule in c! consider the simple example for d k ii xl xl table of ii ii xl xi table of ilx px x p ii s-lop p a simple calculation shows that for any choice p e either goo or goo o. we have goo if p and goo otherwise. however in obvious notation lg lg the best constant rule is g when p and g otherwise. for p the plug-in rule does not even pick the best constant rule let alone the best rule in c with k which it was intended to pick. this example highlights the danger of parametric rules or plug-in rules when applied to incorrect or incomplete models. remark. historical notes. the rademacher-walsh expansion occurs frequently in switching theory and was given in duda and hart the feid expansion is similar in nature. ito presents error bounds for discrimination based upon a k-term truncation of the series. ott and kronmal provide further statistical properties. the rules described here with k defining the number of interactions are also obtained if we model p x i y i and p x i y by functions of the form and exp bi oi the latter model is called the log-linear model mclachlan section maximum likelihood the maximum likelihood method chapter should not be used for picking the best rule from a class c that is not guaranteed to include the bayes rule. perhaps a simple example will suffice to make the point. consider the following hypercube setting with d maximum likelihood px ii xl i xl ii xl p r q s in this example l o. apply maximum likelihood to a class f with two members where bx lis if xl if xl if xl if xl then maximum likelihood wont even pick the best member from f! to verify this with gax gbx we see that lga p q lg b r s. however if is given by and if we write nij for the number of data pairs k yk having x k i yk j then a if o if nol nlo if n n and otherwise. equivalently if and only if nlo o. apply the strong law of large numbers to note that nolin r nloln s nooln p and nlln q with probability one as n thus lim p n-oo if r s otherwise. take r s e very small p q e. then for the maximum likelihood rule however when f contains the bayes rule maximum likelihood is consistent theorem hypercubes and discrete spaces kernel methods sometimes d is so large with respect to n that the atoms in the hypercube are sparsely populated. some amount of smoothing may help under some stances. consider a kernel k and define the kernel rule gnx if xill h i o otherwise n l....-i-l where ilx z ii is just the hamming distance the number of disagreements between components of x and z. with k e the rule above reduces to a rule given in aitchison and aitken in their paper different h s are considered for the two classes but we wont consider that distinction here. observe that at h we obtain the fundamental rule. as h we obtain a majority rule over the entire sample. the weight given to an observation xi decreases exponentially in ilx xi for consistency we merely need h condition nh d of theorem is no longer needed. and in fact we even have consistency with h as this yields the fundamental rule. the data-based choice of h has been the object of several papers including hall and hall and wand in the latter paper a mean squared error criterion is minimized. we only mention the work of tutz who picks h so as to minimize the deleted estimate ld. theorem let hn be the smoothing factor in the aitken rule that minimizes l. then the rule is weakly consistent for all butions of y on the hypercube. proof. see theorem problems and exercises problem the fundamental rule. let g be the fundamental rule on a finite set k and define lg. let g be the bayes rule error probability l and let inf minrx rx let lr be the resubstitution error estimate apparent error rate. show the following e eln apparent error rate is always optimistic hills e l apparent error rate is in fact very optimistic glick eln l a similar inequality related to hellinger distances see glick this is an exponential but distribution-dependent error rate. problem discrete lipschitz classes. consider the class of regression functions r e with i rx rz i cpx zt where x z e ld p. denotes the hamming distance and c and a are constants that a is not bounded from above. the purpose is to design a discrimination rule for which uniformly over all distributions of y on x i with such pry x we have problems and exercises el l a d in n where the function a d is as small as possible. note for e the class contains all regression functions on the hypercube and thus a d how small should e be to make polynomial in d? problem with a cubic histogram partition of into kd cells volume kd each we have for the lipschitz a classf of theorem sup e cxyef k this grows as d a hint consult conway and sloane can you define a partition into k d cells for which supcx yef is smaller? problem consider the following randomized histogram rule xl x k partition into polyhedra based on the nearest neighbor rule. within each cell we employ a majority rule based upon x kl x n if x is uniform on and is lipschitz a in theorem then can you derive an upper bound for el n l as a function of k n e a and d? how does your bound compare with the cubic histogram rule that uses the same number of cells? problem letf be the class of all lipschitz a functions e nd let y e denote the fact that y has regression function py x in f. then for any cubic histogram rule show that sup e cxyef ln l l thus the compactness condition on the space is essential for the distribution-free error bound given in theorem problem independent model. show that in the independent model the maximum likelihood estimate pi of pi is given by rl ixjiliyjl rl iyjl problem independent model. for the plug-in rule in the independent model is it true that eln l in uniformly over all pairs y on id x i? if so find a constant e depending upon d only such that el n l e if not provide a counterexample. problem consider a hypercube problem in which x xcd and each xci e ternary generalization. assume that the xus are independent but not hypercubes and discrete spaces necessarily identically distributed. show that there exists a quadratic bayes rule i.e. gx is on the set d ao l aixi l aijxi xj d i ijl where ao and are some weights and steinbuch problem let a be the class of all sets on the hypercube of the form xci! xik where xed e ld ik d. a is the class of all sets carved out by the monomials. show that the vc dimension of cis d. hint the set a is shattered by a. no set of size d can be shattered by a by the pigeonhole principle. problem show that the catalan number n i n e.g. kemp problem provide an argument to show that the number of boolean functions with at most k operations or and d operands of the form xu or i xi xci e i is not more than k k is times less than the bound given in the text. problem provide upper and lower bounds on the vc dimension of the class of sets a on the hypercube ld that can be described by a boolean expression with the xis or xis as operands and with at most k operations or problem linear discrimination on the hypercube. let gn be the rule that mizes the empirical error ln over all linear rules when the data are drawn from any distribution on ld x i. let ln be its probability of error and let l be the minimum error probability over all linear rules. show that for e deduce that el n l j ffn d ffn compare this result with the general vapnik-chervonenkis bound for linear rules and deduce when the bound given above is better. hint count the number of possible linear rules. problem on the hypercube to ld show that the kernel rule of aitchison and aitken is strongly consistent when limn-oo h problem pick h in the kernel estimate by minimizing the resubstitution estimate lr and call it hy. for ld we call it hd. assume that the kernel function is of the form kii iii h with k ku as u t let ln be the error estimate for the kernel rule with one of these two choices. is it possible to find a constant c depending upon d and k only such that problems and exercises e inf lnhs h.o yn if so give a proof. if not provide a counterexample. note if the answer is positive a minor corollary of this result is tutzs theorem. however an explicit constant c may aid in determining appropriate sample sizes. it may also be minimized with respect to k. problem construct a partition of the hypercube ld in the lowing manner based upon a binary classification tree with perpendicular splits every node at level i splits the subset according to xi or xi so that there are at most d levels of nodes. practice the most important component should be xl. for example all possible partitions of obtainable with cuts are i assign to each internal node region a class. define the horton-strahler number of a tree as follows if a tree has one node then if the root of the tree has left and right subtrees with horton-strahler numbers and then set let c be the class of classifiers g described above with horton-strahler number let s e ilx ii n where ii ii denotes hamming distance from the all-zero vector. show that s is shattered by the class of sets g e c. show that lsi ito e conclude that the vc dimension of c is at least has shown that the vc dimension of c is exactly this but that proof is more involved. assuming l obtain an upper bound for el n as afunctionof andd where ln is the probability of eltor for the rule picked by minimizing the empirical eltor over c. interpret as the height of the largest complete binary tree that can be embedded in the classification tree. epsilon entropy and totally bounded sets definitions this chapter deals with discrimination rules that are picked from a certain class of classifiers by minimizing the empirical probability of error over a finite set of carefully selected rules. we begin with a class f of regression functions a posteriori probability functions tj nd from which tjn will be picked by the data. the massiveness of f can be measured in many ways-the route followed here is suggested in the work of kolmogorov and tikhomirov we will depart from their work only in details. we suggest comparing the results here with those from chapters and let fe tjn be a finite collection of functions nd such that where stie is the ball of all functions nd with ii tjiioo sup ix e. x in other words for each tj e f there exists an tju e fe with supx itjx tjix i e. the fewer tjis needed to cover f the smaller f is in a certain sense. fe is called an e r of f. the minimal value of i fe lover all e is called the e-covering number following kolmogorov and tikhomirov epsilon entropy and totally bounded sets is called the e of f. we will also call it the metric entropy. a collection f is totally bounded if ne for all e o. it is with such classes that we are concerned in this chapter. the next section gives a few examples. in the following section we define the skeleton estimate based upon picking the empirically best member from fe figure an e of the unit square. examples totally bounded classes the simple scalar parametric class f x e r o is not totally bounded this is due simply to the presence of an unrestricted scale factor. it would still fail to be totally bounded if we restricted to or however if we force e and change the class f to have functions if ixl otherwise then the class is totally bounded. while it is usually difficult to compute n exactly it is often simple to obtain matching upper and lower bounds. here is a simple argument. take i i l i j j and define each of these defines a function collect these in fe note that with e f with parameter if e is the nearest value among i l j j u then ie e. but then sup el e. ixisl hence fe is an e-cover of f of cardinality we conclude that f is totally bounded and that problem for a d-dimensional generalization. ne examples totally bounded classes for a lower bound we use once again an idea from kolmogorov and tikhomirov let oe c f be a subset with the property that for every i j supx e. the set oe is thus e-separated. the maximal cardinality e set is called the e number e number me. it is easy to see that me with this in hand we see that oe may be constructed as follows for our example begin with then define by e etcetera until it is clear that this wayan e set oe may be constructed with ioe i l e e j thus ne the e-entropy of f grows as as e t consider next a larger class not of a parametric nature let f be a class of functions on ll satisfying the lipschitz condition i clx xii and taking values on kolmogorov and tikhomirov showed that if e min then n e llc e i e problem observe that the metric entropy is exponentially larger than for the parametric class considered above. this has a major impact on sample sizes needed for similar performances the next sections. another class of functions of interest is that containing functions that are s-times differentiable and for which the s-th derivative satisfies a holder condition of order ex clx xll a x xl e where c is a constant. in that case me and are both as e t where an means that an obn and bn oan. this result also due to kolmogorov and tikhomirov establishes a continuum of rates of increase of the e-entropy. in n d with functions n ld if the holder condition holds for all derivatives of order s then ne epsilon entropy and totally bounded sets here we have an interesting interpretation of dimension doubling the dimension roughly offsets the effect of doubling the number of derivatives the degree of smoothness of the working with lipschitz functions on rl is roughly equivalent to working with functions on r for which all order derivatives are lipschitz! as there are such derivatives we note immediately how much we must pay for certain performances in high dimensions. let be the class of all entire analytic functions whose periodic continuation satisfies for some constants c a is a complex variable is its imaginary part. for this class we know that as eo and tikhomirov the class appears to be as small as our parametric class. see also vitushkin skeleton estimates the members of form a representative skeleton of f. we assume that fe c f condition was not imposed in the definition of an e-cover. for each e f we define its discrimination rule as g if otherwise. thus we will take the liberty of referring to as a rule. for each such we define the probability of error as usual pgx y. the empirical probability of error of is denoted by we define the skeleton estimate by argminln refe one of the best rules in is denoted by e f. skeleton estimates the first objective as in standard empirical risk minimization is to ensure that is close to lrj. if the true a posteriori probability function rj is in f that py x n then it is clear that l lrj. it will be seen from the theorem below that under this condition the skeleton estimate has nice consistency and rate-of-convergence properties. the result is distribution-free in the sense that no structure on the distribution of x is assumed. problems and show that convergence of lrjn to lrj for all rjs-that is not only for those in f-holds if x has a density. in any case because the skeleton estimate is selected from a finite deterministic set may be constructed before data are collected! probability bounding is trivial for all e we have p inf lrj! refe ife i sup p tjefe lemma le- inequality. theorem let f be a totally bounded class offunctions rj nd there is a sequence o and a sequence of skeletons fen c f such that if rjn is the skeleton estimate drawn from fen then lrjn l with probability one whenever the true regression function py x is in f. it suffices to take fe as an e of f that ife i need not equal the e number ne and to define en as the smallest positive number for which finally with en picked in this manner e l vs n tf. proof. we note that inftjefe lrj s l because if rj e stje then elrjx s sup irx s e x and thus by theorem l l s then for any l plrjn inf le- above epsilon entropy and totally bounded sets which is summable in n as en this shows the first part of the theorem. for the second part we have l inf r efen min i dt vsen jsen jsen t. e- e t for t the proof is completed. observe that the estimate is picked from a deterministic class. this of course requires quite a bit of preparation and knowledge on behalf of the user. knowledge of the e-entropy at least an upper bound is absolutely essential. furthermore one must be able to construct fe this is certainly not computationally simple. skeleton estimates should therefore be mainly of theoretical importance. they may be used for example to establish the existence of estimates with a guaranteed error bound as given in theorem a similar idea in nonparametric density estimation was proposed and worked out in devroye remark. in the first step of the proof we used the inequality sup i i e. x it is clear from this that what we really need is not an e of f with respect to the supremum norm but rather with respect to the l norm. in other words the skeleton estimate works equally well if the skeleton is an e of f with respect to the ll norm that is it is a list of finitely many candidates with the property that for each e f there exists an lji in the list such that i e. it follows from the inequality above that the smallest such covering has fewer elements than that of any e of f with respect to the supremum norm. therefore estimates based on such skeletons perform better. in fact the difference may be essential. as an example consider the class f of all regression functions on that are monotone increasing. for e this class does not have a finite e with respect to the supremum norm. however for any m it is possible to find an e-cover of f with respect to with not more rate of convergence than elements unfortunately an e-cover with respect to llll depends on il the distribution of x. since il is not known in advance we cannot construct this better skeleton. however in some cases we may have some a priori information about j.l.forexample if we know that j.l is a member of a known class of distributions then we may be able to construct a skeleton that is an e for all measures in the class. in the above example if we know that il has a density bounded by a known number then there is a finite skeleton with this property problem we note here that the basic idea behind the empirical covering method of buescher and kumar described in problem is to find a good skeleton based on a fraction of the data. investigating this question further we notice that even covering in l i is more than what we really need. from the proof of theorem we see that all we need is a skeleton fe such that infrylefe l e for all e f. staying with the example of the class of monotonically increasing we see that we may take in fe the functions ix?.q where qi is the i-th e-quantile of j.l that is qi is the smallest number z such that p z i e. this collection of functions forms a skeleton in the required sense with about elements instead of the obtained by covering in l i a significant improvement. problem illustrates another application of this idea. for more work on this we refer to vapnik benedek and itai kulkarni dudley kulkarni richardson and zeitouni and buescher and kumar rate of convergence in this section we take a closer look at the distribution-free upper bound e iln l viscn tf for typical parametric classes as the one discussed in a section we have if we take ife i close enough to ne then ell is the solution of n e or en glog n and we achieve a guaranteed rate of n the same is true for the example of the class of analytic functions discussed earlier. the situation is different for massive classes such as the lipschitz functions on ld. recalling that as e we note that ell gn- for this class we have epsilon entropy and totally bounded sets here once again we encounter the phenomenon called the of ality. in order to achieve the performance e l s e we need a sample of size n exponentially large in d. note that the class of classifiers defined by this class of functions has infinite vc dimension. the skeleton estimates thus provide a vehicle for dealing with very large classes. finally if we take all on forwhichs derivatives exist and is lipschitz with a given constant c similar considerations show that the rate of convergence is which ranges from s to s as the class becomes smaller we can guarantee better rates of convergence. of course this requires more a priori knowledge about the true we also note that if log in theorem then the bound is vsen vs logll. v v the error grows only sub-logarithmically in the size of the skeleton set. it grows as the square root of the e roughly speaking ignoring the dependence of en upon n we may say that for the same performance guarantees doubling the e implies that we should double the sample size keep log n constant. when referring to e-entropy it is important to keep this sample size interpretation in mind. problems and exercises problem show that the class of functions on the real line with parameter is not totally bounded. problem compute a good upper bound for n as a function of d and e for the class f of all functions on n d given by if ilxll otherwise where e is a parameter. repeat this question if are in and hint both answers are polynomial in lie as e problem show that me for any totally bounded set f and tikhomirov problem find a class f of functions rd such that for every e it has a finite e-cover the vc dimension of a e f is infinite. problems and exercises problem show that if f ljl is lipschitz with constant c then for e small enough ne s ale where a is a constant depending upon and c. is not as precise as the statement in the text obtained by kolmogorov and tikhomirov but it will give you excellent practice. problem obtain an estimate for the cardinality of the smallest e-cover with respect to the l norm for the class of ls on that are increasing. in particular show that for any f.l it is possible to find an e-cover with elements. can you do something similar for ljs on that are increasing in each coordinate? problem consider the class of ls on that are increasing. show that for every e there is a finite list ll ljn such that for all lj in the class whenever x has a density bounded by b estimate the smallest such n. problem assume that x has a bounded density on and that lj is monotonically increasing in both coordinates. is a reasonable assumption in many applications. then the set gx o is a monotone layer. consider the following classification rule take a k x k grid in and minimize the empirical error over all classifiers such that o is a monotone layer and it is a union of cells in the k x k grid. what is the optimal choice of k? obtain an upper bound for lgn l compare your result with that obtained for empirical error minimization over the class of all monotone layers in section hint count the number of different classifiers in the class. use hoeffdings inequality and the union-of-events bound for the estimation error. bound the approximation error using the bounded density assumption. problem apply problem to extend the consistency result in theorem as follows. let f be a totally bounded class of functions l r such that j..x ljlx for each ljl e f is the lebesgue measure on r d. show that there is a sequence o and a sequence of skeletons fen such that if ln is the skeleton estimate drawn from fell then lljj ll with probability one whenever x has a density. in particular lljn l with probability one if the bayes rule takes the for some l e f. note the true regression function l is not required form gx to be in f. uniform laws of large numbers minimizing the empirical squared error in chapter the data dn were used to select a function from a class f of candidate regression functions nd the corresponding classification rule gn is selecting was done in two steps a skeleton-an of f was formed and the empirical error count was minimized over the skeleton. this method is computationally cumbersome. it is tempting to use some other empirical quantity to select a classifier. perhaps the most popular among these measures is the empirical squared error assume now that the function is selected by minimizing the empirical squared error over f that is as always we are interested in the error probability of the resulting classifier. if the true regression function p y x x is not in the class f then it is easy to see that empirical squared error minimization may fail miserably problem however if e f then for every e f uniform laws of large numbers we have lr! l e lx corollary e e ief where the two equalities follow from the fact that lx eyix. thus we have lln l e inf e ief by an argument as in the proof of lemma thus the method is consistent if the supremum above converges to zero. if we define zi yd and izd then we see that we need only to bound where f is a class of bounded functions. in the next four sections we develop upper bounds for such uniform deviations of averages from their expectations. then we apply these techniques to establish consistency of generalized linear classifiers obtained by minimization of the empirical squared error. uniform deviations of averages from expectations let f be a class of real-valued functions defined on n d and let z zn be i.i.d. nd-valued random variables. we assume that for each i e f ix m for all x e nd and some m by hoeffdings inequality for any i e f. however it is much less trivial to obtain information about the probabilities p i t izj e ief n il uniform deviations of averages from expectations vapnik and chervonenkis were the first to obtain bounds for the probability above. for example the following simple observation makes theorems and easy to apply in the new situation lemma sup l fzi efz m i n fef n il l iuz t pfz t i sup n fefto n il proof. exploiting the identity fooo px t ex for nonnegative random variables we have fef n il sup i t fzi sup i t iuz t pfz t dtl fef sup i t iuz t pfz tll m n il fefto n il for example from theorem and lemma we get corollary define the collection of sets f ft f e f t e m where for every f e f and t e m the set a ft e nd is defined as a ft fez t. then example. consider the empirical squared error minimization problem sketched in the previous section. let f be the class of monotone increasing functions r and let be the function selected by minimizing the empirical squared error. by if pry llx x is also monotone increasing then uniform laws of large numbers if t contains all subsets of r x i of the form a y t ry e f t e then it is easy to see that its n-th shatter coefficient satisfies set n thus corollary can be applied and the empirical squared error minimization is consistent. in many cases corollary does not provide the best possible bound. to state a similar but sometimes more useful result we introduce ii-covering numbers. the notion is very similar to that of covering numbers discussed in chapter the main difference is that here the balls are defined in terms of an ii rather than the supremum norm. definition let a be a bounded subset of rd. for every e the covering number denoted by ne a is defined as the cardinality of the smallest finite set in rd such that for every z e a there is a point t e rd in the finite till e. ll denotes the ii-norm of the set such that vector x xed in rd. in other words ne a is the smallest number of ii-balls of radius ed whose union contains a. logne a is often called the metric entropy of a. we will mainly be interested in covering numbers of special sets. let zl zn be n fixed points in r d and define the following set the h number of fzl is ne fzl if z? zn is a sequence ofi.i.d. random variables thenne fcz! is a random variable whose expected value plays a central role in our problem theorem for any nand e the proof of the theorem is given in section remark. theorem is a generalization of the basic vapnik equality. to see this define loo-covering numbers based on the maximum norm and chervonenkis nooe a is the cardinality of the smallest finite set in rd such that for every z e a there is a point t e rd in the set such that maxiid izi e. if the functions in f are indicators of sets from a class a of sbsets of r d then it is easy to see that for every e e empirical squared error minimization where n azl zn is the combinatorial quantity that was used in definition of shatter coefficients. since theorem remains true with loo-covering numbers therefore it is a ization of theorem to see this notice that if f contains indicators of sets of the class a then sup i t fzi sup i t iziea pzi e ai aea n il ief n il for inequalities sharper and more general than theorem we refer to vapnik pollard haussler and anthony and shawe-taylor empirical squared error minimization we return to the minimization of the empirical squared error. let f be a class of functions r! nd containing the true a posteriori function the empirical squared error is minimized over e f to obtain the estimate the next result shows that empirical squared error minimization is consistent under general conditions. serve that these are the same conditions that we assumed in theorem to prove consistency of skeleton estimates. theorem assume that f is a totally bounded class of functions. the definition see chapter if e f then the classification rule obtained by minimizing the empirical squared error over f is strongly consistent that is lim l with probability one. n--oo proof. recall that by we apply theorem to show that for every e the probability on the hand side converges to zero exponentially as n to this end we need to find a suitable upper bound on ene where j is the class of functions uniform laws of large numbers ix y y? froeu nd x i to where e f and zi yi. observe that for any i i e sup x this inequality implies that for every f enf where is the covering number of f defined in chapter by the assumption of total boundedness for every e since does not depend on n the theorem is proved. remark. the nonasymptotic exponential nature of the inequality in theorem makes it possible to obtain upper bounds for the rate of convergence of to l in terms of the covering numbers of the class f. however since we started our analysis by the loose inequality l the resulting rates are likely to be suboptimal theorem also the inequality of theorem may be loose in this case. in a somewhat different setup barron developed a proof method based on bernsteins inequality that is useful for obtaining tighter upper bounds for l in certain cases. proof of theorem the main tricks in the proof resemble those of theorem we can show that for nf p j t fey n il izj eizij f j taiizij fey n il where an are i.i.d. l-valued random variables independent of the zis with pai i pai the only minor difference with theorem appears when chebyshevs inequality is applied. we use the fact that by boundedness varizi for every i e f. now take a minimal of fz that is m fz! functions gl gm such that for every i e f there is a g e gm with n iizi f proof of theorem for any function f we have and thus i i taigczih tyfczi gczi n i l n l oigzi l ifzi gzil n il n il s fef n il as igzi s p i t zl zn s p sup i t t s p i toigjzji zl zn. fef n il n il g n il ifczj zl zn now that we have been able to convert the into a we can use the union bound we need only find a uniform bound for the probability following the this however is easy since after conditioning on zl zn oigzi is the sum of independent bounded random variables whose expected value is zero. therefore hoeffdings inequality gives i f oigjzi zl zn s e i p in summary uniform laws of large numbers fzn the theorem is proved. properties of ne will be studied in the next section. covering numbers and shatter coefficients in this section we study covering numbers and relate them to shatter coefficients of certain classes of sets. as in chapter we introduce ii-packing numbers. let fbe a class of functions on n d taking their values in mj. let be an arbitrary probability measure on nd. let gl gm be a finite collection of functions from f with the property that for any two of them the largest m for which such a collection exists is called the packing number of f to and is denoted by me f. if places probability lin on each of zl zn then by definition me f me fzi and it is easy to see that fzid ne fz me an important feature of a class of functions f is the vc dimension v.r of f t t f f e f this is clarified by the following theorem which is a slight refinement of a result by pollard which is based on dudleys work. it connects the packing number of f with the shatter coefficients of f. see also haussler for a somewhat different argument. theorem let f be a class of m-valued functions on nd. for every e and probability measure j.l fl me f s k where k fm e proof. let be an arbitrary e-packing of f of size m me f. the proof is in the spirit of the probabilistic method of combinatorics e.g. spencer to prove the inequality we create k random points on nd x m in the following way where k is a positive integer specified later. we ate k independent random variables sl sk on nd with common distribution covering numbers and shatter coefficients jk and independently of this we generate another k independent random ables tl tb uniformly distributed on m. this yields k random pairs tl tk. for any two functions gi and gj in an e-packing the bility that the sets gi t t and g j t t gjx pick the same points from our random set of k points is bounded as follows p g i and g j pick the same points k ri p tr e gil. g j e p e gil. g j i s d gjsjlil emi e-ke m where we used the definition of the functions gl gm observe that the expected number of pairs gj of these functions such that the corresponding sets g i t gixandg j t gjx pick the same points is bounded by e gj g i and g j pick the same points i g and g j pick the same point since for k randomly chosen points the average number of pairs that pick the same points is bounded by ge-ke m there exist k points in rd x m such that the number of pairs gj that pick the same points is actually bounded by ce-ke m for each such pair we can add one more point in rd x m such that the point is contained in gil. g j. thus we have obtained a set of no more than k points such that the sets g i g m pick different subsets of it. since k was arbitrary we can choose it to minimize this expression. this yields l log g m j points so the shatter coefficient of f corresponding to this number must be greater than m which proves the statement. the meaning of theorem is best seen from the following simple corollary corollary let f be a class m-valuedunctions on rd. for every e and probability measure jk me f uniform laws of large numbers proof. recall that theorem implies the inequality follows from theorem by straightforward calculation. the details are left as an exercise d recently haussler was able to get rid of the factor in the above upper bound. he proved that if e kin for an integer k then me f ed v.f e e the quantity v.f is sometimes called the pseudo dimension of f problem it follows immediately from theorem that if f is a linear space of functions of dimension r then its pseudo dimension is at most r a few more properties are worth mentioning theorem and dudley let g r d r be an arbitrary junction and consider the class ojjunctions j j e f. then vg vf. proof. if the points tl tk e rd x r are shattered by f then the points tl gsl tk gsk are shattered by g. this proves the proof of the other inequality is similar. d theorem and pollard dudley let g m r be a fixed nondecreasing junction and define the class j j e f. then vg vp. proof. assume that n vg and let the functions fr e f be such that the binary vector takes all values if j for all i n define the numbers ctd igjsn and covering numbers and shatter coefficients by the monotonicity of g ui li. then the binary vector takes the same value as for every j therefore the pairs sn sl are shattered by f which proves the theorem. next we present a few results about covering numbers of classes of functions whose members are sums or products of functions from other classes. similar results can be found in nobel nolan and pollard and pollard theorem let fl fk be classes of real functions on rd. for n arbitrary fixed points z zn in r d define the sets fl in rn by j k. also introduce fzn f e f for the class offunctions thenfor every e and ne it ne k k jl proof. let sl sk c rn be minimal e k-coverings of fl fkzj spectively. this implies that for any e fj there is a vector s j sjn e sj such that for every j k. moreover isj i ne k fjzj. we show that ssl esjji uniform laws of large numbers is an e-covering of this follows immediately from the triangle inequality since for any fl there is si sk such that i l....- hzl sl l....- sk n i n ci i ci i k-. e k theorem letf and be classes of real functions on r d bounded by ml and m respectively. is e.g. s ml for every x e rd and f e forarbitraryjixedpoints zn in rd dejine the sets and gz in rh as in theorem introduce for the class of functions f e g e g. then for every e and z proof. let s c md n be an of that is for any f e there is a vector s sen e s such that n i i l....n ii e fzt s ci i it is easy to see that s can be chosen such that lsi similarly let t c m be an of with iti gzd such that for any g e there is at ten e t with we show that the set u s e s t e t is an e-covering of let f e and g e be arbitrary and s e sand t e t the corresponding vectors such that then generalized linear classification generalized linear classification in this section we use the uniform laws of large numbers discussed in this chapter to prove that squared error minimization over an appropriately chosen class of generalized linear classifiers yields a universally consistent rule. consider the class ckn of generalized linear classifiers whose members are functions nd of the form where are fixed basis functions and the coefficients ai are arbitrary real numbers. the training sequence dn is used to determine the cients ai in chapter we studied the behavior of the classifier whose coefficients are picked to minimize the empirical error probability instead of minimizing the empirical error probability ln several authors gested minimizing the empirical squared error e.g. duda and hart vapnik this is rather dangerous. for example for k and d it is easy to find a distribution such that the error ability of the linear classifier that minimizes the empirical squared error converges to e while the error probability of the best linear classifier is e where e is an arbitrarily small positive number clearly similar examples can uniform laws of large numbers be found for any fixed k. this demonstrates powerfully the danger of minimizing squared error instead of error count. minimizing the latter yields a classifier whose average error probability is always within jlog n n of the optimum in the class for fixed k. we note here that in some special cases minimization of the two types of error are equivalent problem interestingly though if kn as n increases we can obtain universal consistency by minimizing the empirical squared error. theorem let vrl be a sequence of bounded functions with i such that the set of all finite linear combinations of the vr j q e r is dense in l on all balls of the form ilx ii m for any probability measure let the coefficients at at minimize the empirical squared error under the constraint ll la j i bit bn define the generalized linear sifier gn by if kn and bn satisfy then e lgn l for all distributions of y that is the rule gn is universally consistent. if we assume additionally that b log n on then gn is strongly universally consistent. proof. let be arbitrary. then there exists a constant m such that piixii m o. thus lgn l pgnx i y iixii midn pgx i y iixii m. it suffices to show that pgnx i y iixii midn pgx i y iixii m in the required sense for every m o. introduce the notation fnx ll ajvrjx. by corollary we see that pgnx i y iixii midn pgx i y iixii m generalized linear classification we prove that the right-hand side converges to zero in probability. observe that since llx x for any function hex ehx x x chapter therefore denoting the class of functions over which we minimize by we have jiixiism fj.dx e i dn e i dn inf e fe! inf e fefn ijlx ii sm e the last two terms may be combined to yield inf fefn jiixiism fj.dx which converges to zero by the denseness assumption. to prove that the first term converges to zero in probability observe that we may assume without loss of generality that p ii x ii m o. as in the proof of lemma it is easy to show that e i dn inf e fe! e dn inf e fe! n e i sup i thxi yi ehx yi hej n il uniform laws of large numbers where the class of functions j is defined by j y f e fn observe that since and i i we have therefore theorem asserts that p dn inf e e fefn p i thxi yi ehx hej n il where yn. next for fixed we estimate the ing number n j for arbitrary fl e consider the functions hix y then for any probability measure v on nd x i and h y f ih y ylvdx y f iflx f hxlbn lvdx y f i fi hex vdx y where fj. is the marginal measure for v on nd. thus for any yd yn and e ne jzl n fx therefore it suffices to estimate the covering number corresponding to fn. since kn fn is a subset of a linear space of functions we have corollary n e ll n xl b e n b e n og e problems and exercises summarizing we have p dn inf e e which goes to zero if knb logbnn the proof of the theorem is completed. it is easy to see that if we assume additionally that b log n n then strong universal consistency follows by applying the borel-cantelli lemma to the last probability. remark. minimization of the squared error is attractive because there are efficient algorithms to find the minimizing coefficients while minimizing the number of errors committed on the training sequence is computationally more difficult. if the dimension k of the generalized linear classifier is fixed then stochastic mation asymptotically provides the minimizing coefficients. for more information about this we refer to robbins and monro kiefer and wolfowitz dvoretzky fabian tsypkin nevelson and khasminskii kushner ruppert and ljung pflug and walk for example gyorfi proved that if vi form a stationary and ergodic sequence in which each pair is distributed as the bounded random variable pair v e nk x n and the vector of coefficients a ak minimizes and ao e nk is arbitrary then the sequence of coefficient vectors defined by an u n v. u satisfies lim ran ra a.s. n-oo problems and exercises problem find a class containing two functions n and a bution of y such that l but as n the probability converges to one where is selected from by minimizing the empirical squared error. uniform laws of large numbers problem prove corollary problem let fbe a class of functions on n d taking their values in m. haussler defines the pseudo dimension of f as the largest integer n for which there exist n points in n d zj zn and a vector v vn e nn such that the binary n-vector takes possible values as f ranges through f. prove that the pseudo dimension of f equals the quantity v f defined in the text. problem consistency of clustering. let x xi xn be i.i.d. random variables in nd and assume that there is a number m such that px e md l. take the empirically optimal clustering of xl x n that is find the points ai ak that minimize the empirical squared error the error of the clustering is defined by the mean squared error prove that if aj ak denote the empirically optimal cluster centers then and that for every e p s p u bj erd ie ii k i b b e k conclude that the error of the empirically optimal clustering converges to that of the truly optimal one as n linder lugosi and zeger hint for the first part proceed as in the proof of lemma for the second part use the technique shown in corollary to compute the vc dimension exploit corollary problem let v!k be indicator functions of cells of a k-way partition of nd. consider generalized linear classifiers based on these functions. show that the classifier obtained by minimizing the number of errors made on the training sequence is the same as for the classifier obtained by minimizing the empirical squared error. point out that this is just the histogram classifier based on the partition defined by the v!is neural networks multilayer perceptrons the linear discriminant or perceptron chapter makes a decision if otherwlse based upon a linear combination of the inputs co l cixi co ct x d il where the cis are weights x xdl and c cdt. this is called a neural network without hidden layers figure in a neural network with one hidden layer one takes co l ci cr k il where the cs are as before and each is of the form given in hi ll aijxj for some constants hi and aij. the function cr is called a sigmoid. we define sigmoids to be nondecreasing functions with crx as x and crx as x t examples include the threshold sigmoid crx if x if x neural networks the standard or logistic sigmoid o-x e- x the arctan sigmoid o-x arctanx tc the gaussian sigmoid o-x e- u j x oot figure a neural network with one hidden layer. the hidden neurons are those within the frame. ax f dv i-e-x ax ax arctanx x figure the threshold standard arctan and gaussian moids. multilayer perceptrons for early discussion of multilayer perceptrons see rosenblatt barron nilsson and minsky and papert surveys may be found in barron and barron ripley hertz krogh and palmer and weiss and kulikowski in the perceptron with one hidden layer we say that there are k hidden the output of the i-th hidden neuron is ui cj oix. thus may be rewritten as co l ciui k il which is similar in form to we may continue this process and create layer feed-forward neural networks. for example a two-hidden-iayer perceptron uses co l cizi i il where and u cj a xi j k and the dijs bs and ajs are constants. the first hidden layer has k hidden neurons while the second hidden layer has i hidden neurons. il figure afeed-forward neural network with two hidden layers. neural networks the step from perceptron to a one-hidden-iayer neural network is nontrivial. we know that linear discriminants cannot possibly lead to universally consistent rules. fortunately one-hidden-iayer neural networks yield universally consistent discriminants provided that we allow k the number of hidden neurons to grow unboundedly with n. the interest in neural networks is undoubtedly due to the possibility of implementing them directly via processors and circuits. as the ware is fixed beforehand one does not have the luxury to let k become a function of n and thus the claimed universal consistency is a moot point. we will deal with both fixed architectures and variable-sized neural networks. because of the universal consistency of one-hidden-iayer neural networks there is little ical gain in considering neural networks with more than one hidden layer. there may however be an information-theoretic gain as the number of hidden neurons needed to achieve the same performance may be substantially reduced. in fact we will make a case for two hidden layers and show that after two hidden layers little is gained for classification. for theoretical analysis the neural networks are rooted in a classical theorem by kolmogorov and lorentz which states that every continuous function f on ld can be written as where the g ij s and the fi s are continuous functions whose form depends on f. we will see that neural networks approximate any measurable function with arbitrary precision despite the fact that the form of the sigmoids is fixed beforehand. as an example consider d the function is rewritten as which is in the desired form. however it is much less obvious how one would rewrite more general continuous functions. in fact in neural networks we imate the gijs and fis by functions of the form aba t x and allow the number of tunable coefficients to be high enough such that any continuous function may be represented-though no longer rewritten exactly in the form of kolmogorov and lorentz. we discuss other examples of approximations based upon such resentations in a later section. arrangements input figure the general kolmogorov-lorentz representation of a continuous function. arrangements a finite set a of hyperplanes in n d partitions the space into connected convex polyhedral pieces of various dimensions. such a partition p pea is called an arrangement. an arrangement is called simple if any d hyperplanes of a have a unique point in common and if d hyperplanes have no point in common. figure an arrangement of five lines in the plane. figure an arrangement sifier. a simple arrangement creates polyhedral cells. interestingly the number of these cells is independent of the actual configuration of the hyperplanes. in particular neural networks the number of cells is exactly if d k and where iai k. for a proof of this see problem or lemma of edelsbrunner for general arrangements this is merely an upper bound. we may of course use arrangements for designing classifiers. we let ga be the natural classifier obtained by taking majority votes over all yis for which xi is in the same cell of the arrangement p pea as x. all classifiers discussed in this section possess the property that they are invariant under linear transformations and universally consistent some cases we assume that x has a density but that is only done to avoid messy technicalities. if we fix k and find that a with iai k for which the empirical error is minimal we obtain-perhaps at great computational expense-the empirical risk optimized classifier. there is a general theorem for such classifiers-see for example corollary conditions of which are as follows it must be possible to select a given sequence of as for which lnga conditional probability of error with ga tends to l in probability. but if k we may align the hyperplanes with the axes and create a cubic histogram for which by theorem we have consistency if the grid expands to and the cell sizes in the grid shrink to o. thus as k this condition holds trivially. the collection is not too rich in the sense that n where n denotes the shatter coefficient of that is the maximal number of ways yi yn can be split by sets of the form u a x u u a x aepa a epa if iai we know that n d chapter for iai k a trivial upper bound is d the consistency condition is fulfilled if k onj log n. we have theorem the empirical-risk-optimized arrangement classifier based upon arrangements with iai k has el n l for all distributions if k and k onj log n. anangements arrangements can also be made from the data at hand in a simpler way. fix k points xl x k in general position and look at all possible e hyperplanes you can form with these points. these form your collection a which defines your arrangement. no optimization of any kind is performed. we take the ural classifier obtained by a majority vote within the cells of the partition over ykd yn. figure arrangement determined by k data points on the plane. here we cannot apply the powerful consistency theorem mentioned above. also the arrangement is no longer simple. nevertheless the partition of space depends on the xis only and thus theorem with lemma is useful. the rule thus obtained is consistent if diamax in probability and the number of cells is on where ax is the cell to which x belongs in the arrangement. as the number of cells is certainly not more than d l io l where k e we see that the number of cells divided by n tends to zero if this puts a severe restriction on the growth of k. however it is easy to prove the following lemma if k then diamax in probability whenever x has a density. proof. as noted in chapter problem the set of all x for which for all e we have jlx e qi for all quadrants ql having one vertex at and sides of length one has jl-measure one. for such x if at least one of the xs k falls in each of the quadrants x eqi then diamax figure neural networks figure the diameter of the cell taining x is less than if there is a data point in each of the four quadrants of size e around x. therefore for arbitrary e pdiamax min mx eqdk o. thus by the lebesgue dominated convergence theorem pdiamax o. theorem the arrangement classifier defined above is consistent whenever x has a density and the theorem points out that empirical error minimization over a finite set of arrangements can also be consistent. such a set may be formed as the collection of arrangements consisting of hyperplanes through d points of xl x k. as nothing new is added here to the discussion we refer the reader to problem so how do we deal with arrangements in a computer? clearly to reach a cell we find for each hyperplane a e a the side to which x belongs. if fx at x ao then fx in one halfplane fx on the hyperplane and fx in the other halfplane. if a ad the vector o ihkx o describes the cell to which x belongs where hi is a linear function that is positive if x is on one side of the hyperplane ai negative if x is on the other side of ai and if x e ai. a decision is thus reached in time okd. more importantly the whole process is easily parallelizable and can be pictured as a battery of perceptrons. it is easy to see that the classifier depicted in figure is identical to the arrangement classifier. in neural network terminology the first hidden layer of neurons corresponds to just k perceptrons has k d weights or parameters if you wish. the first layer outputs a k-vector of bits that pinpoints the precise location of x in the cells of the arrangement. the second layer only assigns a class to each cell of the arrangement by firing up one neuron. it has neurons class assignments but of course in natural classifiers these neurons do not require training or learning-the majority vote takes care of that. arrangements i-oorljl x o or oor figure arrangement classifier realized by a layer neural network. each of the cells in the second hidden layer peiforms an operation the output of node is ifits three inputs are and respectively. otherwise its output is o. thus one and only one of the outputs is if a more classical second layer is needed-without boolean operations-let b bk be the k-vector of bits seen at the output of the first layer. assign a perceptron in the second layer to each region of the arrangement and define the output z e to be c j b j k where c j e are weights. for each region of the arrangement we have a description in terms of c ck. the argument of the sigmoid function is if j c j for all j and is negative otherwise. hence z if and only if c. assume we now take a decision based upon the sign of lwzzz wo z where the wzs are weights and the zz are the outputs of the second hidden layer. assume that we wish to assign class to s regions in the arrangement and class to t other regions. for a class region l set wz and for a class region set wz define wo s t. then if zj zi i j l wzzz wo w j wo l wi if wj if wj z ij neural networks second hidden layer oor l figure the second hidden layer of a two-hidden-layer neural network with threshold sigmoids in the first layer. for each k-vector of bits b bk at the output of the first layer we may find a decision gb e i. now return to the hidden-layer network of figure and assign the values gb to the neurons in the second hidden layer to obtain an equivalent network. thus every arrangement classifier corresponds to a neural network with two hidden layers and threshold units. the correspondence is also reciprocal. assume someone shows a two-hidden-iayer neural network with the first hidden layer as above-thus outputs consist of a vector of k bits-and with the second hidden layer consisting once again a battery of perceptrons figure whatever happens in the second hidden layer the decision is just a function of the uration of k input bits. the output of the first hidden layer is constant over each region of the arrangement defined by the hyperplanes given by the input weights of the units of the first layer. thus the neural network classifier with threshold units in the first hidden layer is equivalent to an arrangement classifier with the same number of hyperplanes as units in the first hidden layer. the equivalence with tree classifiers is described in problem of course equivalence is only valid up to a certain point. if the number of neurons in the second layer is small then neural networks are more restricted. this could be an advantage in training. however the majority vote in an arrangement classifier avoids training of the second layers neurons altogether and offers at the same time an easier interpretation of the classifier. conditions on consistency of general two-hidden-layer neural networks will be given in section approximation by neural networks approximation by neural networks consider first the class ck of classifiers that contains all neural network classifiers with the threshold sigmoid and k hidden nodes in two hidden layers. the training data dn are used to select a classifier from ck. for good performance of the selected rule it is necessary that the best rule in ck has probability of error close to l that is that inf l l eck is small. we call this quantity the approximation error. naturally for fixed k the approximation error is positive for most distributions. however for large k it is expected to be small. the question is whether the last statement is true for all distributions of y. we showed in the section on arrangements that the class of two-hid den-layer neural network classifiers with m nodes in the first layer and nodes in the second layer contains all arrangement classifiers with m hyperplanes. therefore for k m the class of all arrangement classifiers with m hyperplanes is a subclass of ck from this we easily obtain the following approximation result theorem ijck is the class of all neural network classifiers with the old sigmoid and k neurons in two hidden layers then inf l l lim k-oo eck for all distributions of y. it is more surprising that the same property holds if ck is the class of hidden-layer neural networks with k hidden neurons and an arbitrary sigmoid. more precisely ck is the class of classifiers if otherwise where is as in by theorem we have l l where yjx py iix x. thus inf eck l l as k if for some sequence with k e ck for kx h!ikx for universal consistency we need only assure that the family of can approximate any in the lim sense. in other words the approximation error infcjeck l l converges to zero if the class of functions is dense in li for every m. another sufficient condition for this-but of course much too severe-is that the class neural networks of functions becomes dense in the loo norm in the space of continuous functions ca bd on bd where bd denotes the hyperrectangle of rd defined by its opposite vertices a and b for any a and b. lemma assume that a sequence of classes of functions becomes dense in the loo norm in the space of continuous functions ca bd bd is the hyperrectangle ofrd defined by a b. in other words assume that for every a b e r d and every bounded function g lim inf k-oo jefk xeabd sup ifx then for any distribution of y lim k-oo rjjeck inf l l where ck is the class of classifiers for e proof. for fixed e find a b such that fla bd where fl is the probability measure of x. choose a continuous function off bd such that elryx find k and f e such that e sup xeabjd ix i e for iux we have by theorem l e ryxiixeabdd ryxi sup ifx xeabd e.o e ryx i this text is basically about all such good families such as families that are tainable by summing kernel functions and histogram families. the first results for approximation with neural networks with one hidden layer appeared in when cybenko hornik stinchcombe and white and funahashi proved independently that feedforward neural networks with one hidden layer are dense with respect to the supremum norm on bounded sets in the set of continuous functions. in other words by taking k large enough every continuous function approximation by neural networks on nd can be approximated arbitrarily closely uniformly over any bounded set by functions realized by neural networks with one hidden layer. for a survey of various denseness results we refer to barron and hornik the proof given here uses ideas of chen chen and liu it uses the denseness of the class of trigonometric polynomials in the loo sense for co ld a special case of the stone-weierstrass theorem see theorem in the appendix that is by functions of the form l at x bt x i where ai bi are integer-valued vectors of rd. theorem for every continuous function f bd nand for every e there exists a neural network with one hidden layer and function tx as in such that sup ifx e. xeabd proof. we prove the theorem for the threshold sigmoid ax if x if x o. the extension to general non decreasing sigmoids is left as an exercise lem fix e o. we take the fourier series approximation of fx. by the stone-weierstrass theorem there exists a large positive ger m nonzero real coefficients al. am fh and integers mi for i m j d such that sup it i cos x sin x xeabd il a a where mi i m. it is clear that every continuous tion on the real line that is zero outside some bounded interval can be arbitrarily closely approximated uniformly on the interval by one-dimensional neural works that is by functions of the form k l ciaai x hi co il just observe that the indicator function of an interval c may be written as ax b a c. this implies that bounded functions such as sin and cos can be approximated arbitrarily closely by neural networks. in particular there exist neural networks uicx vix with i m mappings from nd to n such that sup xeabd juix cos x j e_ a i i a neural networks and sup xeabjd sin x! e_. i a therefore applying the triangle inequality we get since the uis and vis are neural networks their linear combination m l il is a neural network too and in fact sup ifx xeabd sup. xeabd ifx t i cos x sin x i sup it cos x sin x xeabd il il a a a a the convergence may be arbitrarily slow for some f. by restricting the class of functions it is possible to obtain upper bounds for the rate of convergence. for an example see barron the following corollary of theorem is obtained via lemma corollary let ck contain all neural network classifiers defined by works of one hidden layer with k hidden nodes and an arbitrary sigmoid then for any distribution of y lim k-oo cpeck inf l l o. the above convergence also holds if the range of the parameters aij bi ci is restricted to an interval where limk-oobk remark. it is also true that the class of one-hidden-iayer neural networks with k hidden neurons becomes dense in l for every probability measure j-l on nd as k problem then theorem may be used directly to prove corollary vc dimension in practice the network architecture k in our case is given to the designer who can only adjust the parameters aij bi and ci depending on the data dn. in this respect the above results are only of theoretical interest. it is more interesting to find out how far the error probability of the chosen rule is from inf ceck l we discuss this problem in the next few sections. vc dimension assume now that the data dn yd yn are used to tune the parameters of the network. to choose a classifier from ck we focus on the ference between the probability of error of the selected rule and that of the best classifier in ck. recall from chapters and that the vc dimension vck of the class ck determines the performance of some learning algorithms. theorem tells us that no method of picking a classifier from ck can guarantee better than q n performance uniformly for all distributions. thus for meaningful distribution-free performance guarantees the sample size n has to be significantly larger than the vc dimension. on the other hand by corollary there exists a way of choosing the parameters of the network-namely by minimization of the empirical error probability-such that the obtained classifier satisfies e inf lp ceck vck log n for all distributions. on the other hand if vck then for any n and any rule some bad distributions exist that induce very large error probabilities theorem we start with a universal lower bound on the vc dimension of networks with one hidden layer. theorem let a be an arbitrary sigmoid and consider the class ck of neural net classifiers with k nodes in one hidden layer. then proof. we prove the statement for the threshold sigmoid and leave the extension as an exercise we need to show that there is a set of n points in rd that can be shattered by sets of the form ljfx where ljf is a one-layer neural network of k hidden nodes. clearly it suffices to prove this for even k. in fact we prove more if k is even any set of n kd points in general position can be shattered are in general position if no d points fall on i-dimensional hyperplane. let xn be a set of n kd such the same d points. for each subset of this set we construct a neural network ljf with k hidden nodes such that if and only if xi is a member of this subset. we may neural networks assume without loss of generality that the cardinality of the subset to be picked out is at most since otherwise we can use vrx where vr picks out the complement of the subset. partition the subset into at most n k groups each containing at most d points. for each such group there exists a hyperplane a t x b that contains these points but no other point from xn. moreover there exists a small positive number h such that at xi b e h if and only if xi is among this group of at most d points. therefore the simple network aat x b h a-a t x b h is larger than on xi for exactly these xis. denote the vectors a and parameters b h obtained for the groups by ai bt and hi. let h min j h j. it is easy to see that vrx l b j jl is larger than for exactly the desired xis. this network has k hidden nodes. theorem implies that there is no hope for good performance guarantees unless the sample size is much larger than kd. recall chapter where we showed that n vck is necessary for a guaranteed small eltor probability regardless of the method of tuning the parameters. bartlett improved theorem in several ways. for example he proved that vck d min d d bartlett also obtained similar lower bounds for not fully connected networks-see problem for two-hidden-layer networks. next we show that for the threshold sigmoid the bound of theorem is tight up to a logarithmic factor that is the vc dimension is at most of the order of kdlogk. theorem and haussler let be the threshold sigmoid and let ck be the class of neural net classifiers with k nodes in the hidden layer. then the shatter coefficients satisfy ne kl dl which implies that for all k d vck vc dimension proof. fix n points xl e nd. we bound the number of different values of the vector as ranges through ck. a node j in the hidden layer realizes a dichotomy of the n points by a hyperplane split. by theorems and this can be done at mostefj different ways. the different splittings obtained at the k nodes determine the k-dimensional input of the output node. different choices of the parameters co ci ck of the output node determine different k-dimensionallinear splits of the n input vectors. this cannot be done in more than lj different ways for a fixed setting of the au and bi parameters. this altogether yields at most different dichotomies of the n points x i x n as desired. the bound on the vc dimension follows from the fact that vck n if sck n for threshold sigmoids the gap between the lower and upper bounds above is logarithmic in kd. notice that the vc dimension is about the number of weights tunable parameters w kd of the network. surprisingly maass proved that for networks with at least two hidden layers the upper bound has the right order of magnitude that is the vc dimension is q w log w. a simple application of theorems and provides the next consistency result that was pointed out in farago and lugosi theorem let be the threshold sigmoid. let gn be a classifier from ck that minimizes the empirical error over e ck. if k such that k log n n as n then gn is strongly universally consistent that is with probability one for all distributions of y. lim lgn l n-oo proof. by the usual decomposition into approximation and estimation errors lgn l inf l l l. fjjeck fjjeck the second term on the right-hand side tends to zero by corollary for the estimation error by theorems and p inf l e fjjeck neural networks which is summable if k onj log n. the theorem assures us that if a is the threshold sigmoid then a sequence of properly sized networks may be trained to asymptotically achieve the optimum probability of error regardless of what the distribution of y is. for example k fn will do the job. however this is clearly not the optimal choice in the majority of cases. since theorem provides suitable upper bounds on the vc dimension of each class ck one may use complexity regularization as described in chapter to find a near-optimum size network. unfortunately the situation is much less clear for more general continuous sigmoids. the vc dimension then depends on the specific sigmoid. it is not hard to see that the vc dimension of ck with an arbitrary nondecreasing sigmoid is always larger than or equal to that with the threshold sigmoid typically the vc dimension of a class of such networks is significantly larger than that for the threshold sigmoid. in fact it can even be infinite! macintyre and sontag demonstrated the existence of continuous infinitely many times differentiable monotone increasing sigmoids such that the vc dimension of ck is infinite if k their sigmoids have little squiggles creating the large variability. it is even more surprising that infinite vc dimension may occur for even smoother sigmoids whose second derivative is negative for x and positive for x o. in chapter problem we basically proved the following result. the details are left to the reader theorem there exists a sigmoid a that is monotone increasing continuous concave on and convex on such that vck for each k we recall once again that infinite vc dimension implies that there is no hope of obtaining nontrivial distribution-free upper bounds on no matter how the training sequence dn is used to select the parameters of the neural network. however as we will see later it is still possible to obtain universal consistency. finiteness of the vc dimension has been proved for many types of sigmoids. maass and goldberg and jerrum obtain upper bounds for piecewise polynomial sigmoids. the results of goldberg and jerrum apply for general classes parametrized by real numbers e.g. for classes of neural networks with the sigmoid ax if x if x o. macintyre and sontag prove vck for a large class of sigmoids which includes the standard arctan and gaussian sigmoids. while finiteness is useful the vc dimension lack of an explicit tight upper bound on vck prevents us from getting meaningful upper bounds on the performance of gn and also from applying the structural risk minimization of chapter for the standard sigmoid and for networks with k hidden nodes and w tunable weights karpinski and macintyre recently reported the upper bound v kwkw wl see also shawe-taylor unfortunately the consistency result of theorem is only of theoretical interest as there is no efficient algorithm to find a classifier that minimizes the empirical error probability. relatively little effort has been made to solve this important problem. farago and lugosi exhibit an algorithm that finds the empirically optimal network. however their method takes time exponential in kd which is intractable even for the smallest toy problems. much more effort has been invested in the tuning of networks by minimizing the empirical squared error or the empirical l error. these problems are also computationally demanding but numerous suboptimal hill-climbing algorithms have been used with some success. most famous among these is the back propagation algorithm of rumelhart hinton and williams nearly all known algorithms that run in reasonable time may get stuck at local optima which results in classifiers whose probability of error is hard to predict. in the next section we study the error probability of neural net classifiers obtained by minimizing empirical l p errors. we end this section with a very simple kind of one-layer network. the committee machine e.g. nilsson and schmidt is a special case of a hidden-layer neural network of the form with co cl ck and the threshold sigmoid. o or figure the committee machine has fixed weights at the output of the hidden layer. committee machines thus use a majority vote over the outcomes of the hidden neurons. it is interesting that the lower bound of theorem remains valid when ck is the class of all committee machines with k neurons in the hidden neural layer. it is less obvious however that the class of committee machines is large enough for the asymptotic property lim k-hx inf lrp l for all distributions of y problem figure a partition of the plane determined by a committee machine with hidden neurons. the total vote is shown in each region. the region in which we decide is shaded. ll error minimization in the previous section we obtained consistency for the standard threshold sigmoid networks by empirical risk minimization. we could not apply the same ogy for general sigmoids simply because the vc dimension for general sigmoidal networks is not bounded. it is bounded for certain classes of sigmoids and for those empirical risk minimization yields universally consistent classifiers. even if the vc dimension is infinite we may get consistency but this must then be proved by other methods such as methods based upon metric entropy and covering bers chapters and as well as the survey by haussler one could also train the classifier by minimizing another empirical criterion which is exactly what we will do in this section. we will be rewarded with a general consistency theorem for all sigmoids. for p the empirical l p error of a neural network lf is defined by the most interesting cases are p and p for p this is just the empirical squared error while p yields the empirical absolute error. often it makes sense to attempt to choose the parameters of the network lf such that jp lf is ll error minimization minimized. in situations where one is not only interested in the number of errors but also how robust the decision is such error measures may be meaningful. in other words these error measures penalize even good decisions if is close to the threshold value o. minimizing jpis like finding a good regression function mate. our concern is primarily with the error probability. in chapter we already highlighted the dangers of squared error minimization and l p errors in general. here we will concentrate on the consistency properties. we minimize the empirical error over a class of functions which should not be too large to avoid overfitting. however the class should be large enough to contain a good approximation of the target function. thus we let the class of candidate functions grow with the sample size n as in grenanders of sieves its consistency and rates of convergence have been widely studied primarily for least squares regression function estimation and nonparametric maximum likelihood density estimation-see geman and hwang gallant and wong and shen remark. regression function estimation. in the regression function tion setup white proved consistency of neural network estimates based on squared error minimization. barron used a complexity-regularized modification of these error measures to obtain the fastest possible rate of vergence for nonparametric neural network estimates. haussler provides a general framework for empirical error minimization and provides useful tools for handling neural networks. various consistency properties of nonparametric ral network estimates have been proved by white mielniczuk and tyrcha and lugosi and zeger we only consider the p case as the generalization to other values of p is straightforward. define the ll error of a function rd r by jljf eiljfx yi. we pointed out in problem that one of the functions minimizing j is the bayes rule g whose error is denoted by then clearly j l we have also seen that if we define a decision by j inf jljf jg. gx if otherwise then its error probability lg pgx y satisfies the inequality lg l jljf j. our approach is to select a neural network from a suitably chosen class of networks by minimizing the empirical error neural networks denoting this function by on according to the inequality above the classifier gnx if onx h ot erwlse is consistent if the lienor converges to j in probability. convergence with probability one provides strong consistency. for universal convergence the class over which the minimization is performed has to be defined carefully. the following theorem shows that this may be achieved by neural networks with k nodes in which the range of the output weights co ci ck is restricted. theorem and zeger let be an arbitrary sigmoid. define the class of neural networks by and let on be afunction that minimizes the empirical ll error over e if kn and satisfy lim k n n---oo lim and n---oo lull n---oo logkn n then the classification rule gn if onx otherwise is universally consistent. remark. strong universal consistency may also be shown by imposing slightly more restrictive conditions on k n and problem proof. by the argument preceding the theorem it suffices to prove that jtln j in probability. write jljin j inf jlji jo j lj error minimization to handle the approximation error-the second term on the right-hand side-let t e be a function such that eltx eltx for each t e the existence of such a function may be seen by noting that eltx is a continuous function of the parameters ai hi ci of the neural network t. clearly inf jt j jt j eltx yi elgx yi eltx gxi which converges to zero as n by problem we start the analysis of the estimation error by noting that as in lemma we have jtn inf jt sup ijt sup eltx yi l l itxd yil i in n il define the class mn of functions on nd x i by mil y itcigarx yl ai e rd hi e r fjn. then the previous bound becomes sup emx y mem l i in lmxi yi n il such quantities may be handled by the uniform law of large numbers of theorem which applies to classes of uniformly bounded functions. indeed for each m emn lcd neural networks if n is so large that for such ns theorem states that p sup iemx y yii e memn where ne mndn denotes the h number of the random set mndn defined in chapter all we need now is to estimate these covering numbers. observe that for e mn with y yl and y yl for any probability measure v on n d x i f y ylvdx y f where fl is the marginal of von nd. therefore it follows thatne mndn ne where xn. it means that an upper bound on the covering number of the class of neural networks is also an upper bound on the quantity that interests us. this bounding may be done by applying lemmas from chapters and define the following three collections of functions qi x b a end ben t x b a end ben x b a end ben c e by theorem the vc dimension of the class of sets qt t t e qd is vgt d this implies by lemma that vg d so by corollary for any xn ne e e where e nn z gxn g e now using similar notations theorem allows us to estimate covering numbers of ne n if fin finally we can apply lemma to obtain the adaline and padaline thus substituting this bound into the probability inequality above we get for n large enough which tends to zero if concluding the proof. n there are yet other ways to obtain consistency for general sigmoidal networks. we may restrict the network by discretizing the values of the coefficients in some way-thus creating a sieve with a number of members that is easy to and applying complexity regularization this is the method followed by barron the adaline and padaline widrow and widrow and hoff introduced the adaline and specht studied polynomial discriminant functions such as the padaline. looked at formally the discriminant function used in the decision gx o is of a polynomial nature with consisting of sums of monomials like a x x where ii id are integers and a is a coefficient. the order of a monomial is i id usually all terms up to and including those of order r are included. widrows adaline has r the total number of monomials of order r or less does not exceed the motivation for developing these discriminants is that only up to ld coefficients need to be trained and stored. in applications in which data continuously arrive the coefficients may be updated on-line and the data can be discarded. this property is of course shared with standard ral networks. in most cases order r polynomial discriminants are not translation invariant. minimizing a given criterion on-line is a phenomenal task so specht noted that training is not necessary if the as are chosen so as to give decisions that are close to those of the kernel method with normal kernels. for example if ku e- the kernel method picks a smoothing factor h neural networks that h and nh d see chapter and uses xiii n il h the same decision is obtained if we use now approximate this by using taylors series expansion and truncating to the order r terms. for example the coefficient of d in the expansion of would be if ll i j i these sums are easy to update on-line and decisions are based on the sign of the order r truncation r of the classifier is called the padaline. specht notes that overfitting in does not occur due to the fact that overfitting does not occur for the kemel method based on his method interpolates between the latter method generalized linear discrimination and generalizations of the perceptron. for fixed r the pad aline defined above is not universally consistent the same reason linear discriminants are not universally consistent but if r is allowed to grow with n the decision based on the sign of becomes universally consistent recall however that padaline was not designed with a variable r in mind. polynomial networks besides adaline and padaline there are several ways of constructing mial many-layered networks in which basic units are of the form for inputs xci xk to that level. pioneers in this respect are gabor ivakhnenko who invented the gmdh method-the group method of data handling-and barron see also ivakhnenko konovalenko tulupchuk and tymchenko and ivakhnenko petrache and krasytskyy these networks can be visualized in the following way polynomial networks figure simple polynomial network each gi represents a simple polynomialfunction of its input. in barrons work barron and barron the gi are sometimes elements of the form gix y ai bix ciy dixy. if the gi are barrons quadratic elements then the network shown in figure represents a particular polynomial of order in which the largest degree of any xci is at most the number of unknown coefficients is in the example shown in the figure while in a full-fledged polynomial network it would be much larger. for training these networks many strategies have been proposed by barron and his associates. ivakhnenko for example trains one layer at a time and lets only the best neurons in each layer survive for use as input in the next layer. it is easy to see that polynomial networks even with only two inputs per node and with degree in each cell restricted to two but with an unrestricted number of layers can implement any polynomial in d variables. as the polynomials are dense in the loo sense on ca bd for all a bend we note by lemma that such networks include a sequence of classifiers approaching the bayes error for any distribution. consider several classes of polynomials where ok are fixed monomials but the ais are free coefficients. e ok are monomials of order r. order of a monomial d is il id. e ok are monomials of order r but k is not fixed. as k does not generally become dense in ca bd in the loo sense unless we add okl in a special way. however becomes dense as r by theorem and becomes dense as both k and r contains a subclass of for a smaller r depending upon k obtained by including in lii ok all monomials in increasing order. the vc dimension of the class of classifiers based on is not more than k theorem the vc dimension of classifiers based upon does not exceed neural networks those of which in tum is nothing but the number of possible monomials of order r. a simple counting argument shows that this is bounded by see also anthony and holden these simple bounds may be used to study the consistency of polynomial works. let us take a fixed structure network in which all nodes are fixed-they have at most k inputs with k fixed and represent polynomials of order r with r fixed. for example with r each cell with input z i zk computes li ai oi zk where the ais are coefficients and the oi are fixed als of order r or less and all such monomials are included. assume that the layout is fixed and is such that it can realize all polynomials of order s on the input xl xd. one way of doing this is to realize all polynomials of order r by taking all possible input combinations and to repeat at the second level with cells of neurons and so forth for a total of s r layers of cells. this tion is obviously redundant but it will do for now. then note that the vc dimension is not more than as noted above. if we choose the best coefficients in the cells by empirical risk minimization then the method is consistent theorem in the fixed-structure polynomial network described above if ln is the probability of error of the empirical risk minimizer then e ln l if s and s on lid. proof. apply lemma and theorem assume a fixed-structure network as above such that all polynomials of order s are realized plus some other ones while the number of layers of cells is not more than i. then the vc dimension is not more than i ld because the maximal order is not more than ir. hence we have consistency under the same conditions as above that is s and i on lid. similar considerations can now be used in a variety of situations. kolmogorov-lorentz networks and additive models answering one of hilberts famous questions kolmogorov and lorentz also sprecher and hecht-nielsen obtained the following interesting representation of any continuous function on theorem lorentz let f be continuous on then f can be rewritten asfollows let be an arbitrary constant and choose e rational. then f l gzk kl kolmogorov-lorentz networks and additive models where g r r is a continuous function upon f and e and each zk is rewritten as d zk lak ek k. jl here a is real and is monotonic and increasing in its argument. also both a and are universal of f and is lipschitz clx y for some c the kolmogorov-lorentz theorem states that f may be represented by a very simple network that we will call the kolmogorov-lorentz network. what is ing is that the first layer is fixed and known beforehand. only the mapping g depends on f. this representation immediately opens up new revenues of we need not mix the input variables. simple additive functions of the input variables suffice to represent all continuous functions. input figure the kolmogorov-lorentz network of theorem to explain what is happening here we look at the interleaving of bits to make one-dimensional numbers out of d-dimensional vectors. for the sake of simplicity let f r. let be the binary expansions of x and y and consider a representation for the function fx y. the bit-interleaved number z e has binary expansion and may thus be written as z where neural networks and thus we can also retrieve x and y from z by noting that x y and therefore jx y def gz one-dimensional function of z g the function cp is strictly monotone increasing. unfortunately cp and are not continuous. kolmogorovs theorem for this special case is as follows theorem there exist jive monotone junctions cpi cps rsatisfying ixi withthejollowing property jor every j e eo continuous junctions on there exists a continuous junction g such thatjor all e the difference with pure bit-interleaving is that now the cpi are continuous and g is continuous whenever j is. also just as in our simle example kolmogorov gives an explicit construction for cpl cps. kolmogorovs theorem may be used to show the denseness of certain classes of functions that may be described by networks. there is one pitfall however any such result must involve at least one neuron or cell that has a general function in it and we are back at square one because a general function even on only one input may be arbitrarily complicated and wild. additive models include for example models such as d ex l oixi il where the ois are unspecified univariate functions and silverman hastie and tibshirani these are not powerful enough to mate all functions. a generalized additive model is kolmogorov-lorentz networks and additive models and tibshirani where is now a given or unspecified function. from kolmogorovs theorem we know that the model with lfik lf unspecified functions includes all continuous functions on compact sets and is thus ideally suited for constructing networks. in fact we may take ctk k and take alllfiks as specified in kolmogorovs theorem. this leaves only lf as the unknown. now consider the following any continuous univariate function f can be proximated on bounded sets to within e by simple combinations of threshold sigmoids of the form k l ci il where ai ci k are variable. this leads to a two-hidden-layer neural network sentation related to that of kurkova where only the last layer has unknown coefficients for a total of theorem consider a network classifier of the form described above in which is the threshold sigmoid and the ais and ci are found by empirical error minimization. then el n l for all distributions of y if k and klognjn o. proof. we will only outline the proof. first observe that we may approximate all functions on ca bd by selecting k large enough. by lemma and orem it suffices to show that the vc dimension of our class of classifiers is on j log n. considering cik lfikxci as new input elements called yb s k s we note that the vc dimension is not more than that of the classifiers based on k l l ci kl il which in turn is not more than that of the classifiers given by l dz il where are parameters and is an input sequence. by theorem the vc dimension is not more than this concludes the proof. for more results along these lines we refer to kurkova neural networks projection pursuit in projection pursuit and tukey friedman and stuetzle friedman stuetzle and schroeder huber hall flick jones priest and herman one considers functions of the form k ox l ojbj aj x jl where b j e r a j e rd are constants and ok are fixed functions. this is related to but not a special case of one-hidden-iayer neural networks. based upon the kolmogorov-lorentz representation theorem we may also consider d ox l ojxj jl for fixed functions oj and silverman hastie and tibshirani in and the ojs may be approximated in tum by spline functions or other nonparametric constructs. this approach is covered in the ature on generalized additive models hastie and tibshirani the class of functions eat x a e r d satisfies the conditions of the weierstrass theorem and is therefore dense in the loo norm on ca bd for any a b e rd e.g. diaconis and shahshahani as a corollary we note that the same denseness result applies to the family k l oiat x il where k is arbitrary and are general functions. the latter result is at the basis of projection pursuit methods for approximating functions where one tries to find vectors ai and functions oi that approximate a given function very well. remark. in some cases approximations by functions as in may be exact. for example and theorem and shahshahani let m be a positive teger. there are distinct vectors a j e rd such that any homogeneous polynomial f of order m can be written as projection pursuit c!-l fx l cjaj xm jl for some real numbers c j. every polynomial of order mover n d is a homogeneous polynomial of order m over n d by replacing the constant by a component x raised to an appropriate power. thus any polynomial of order mover nd may be decomposed exactly by f t c j j x b j m jl for some real numbers b j c j and vectors a j e nd. polynomials may thus be represented exactly in the form cfjiaf x with k as the polynomials are dense in ca bd we have yet another proof that cfjiaf x is dense in ca bd. see logan and shepp or logan for other proofs. the previous discussion suggests at least two families from which to select a discriminant function. as usual we let gx o for a discriminant function here could be picked from or where m is sufficiently large. if we draw by minimizing the empirical error at a tremendous computational cost then convergence may result if m is not too large. we need to know the vc dimension of the classes of classifiers corresponding ton and f. note that fm coincides with all polynomials of order m and each such polynomial is the sum of at most ld monomials. if we invoke lemma and theorem then we get theorem empirical risk minimization to determine j b j c j in fm leads to a universally consistent classifier provided that m and m on lid log n. projection pursuit is very powerful and not at all confined to our limited sion above. in particular there are many other ways of constructing good consistent classifiers that do not require extensive computations such as empirical error imization. neural networks radial basis function networks we may perform discrimination based upon networks with functions of the form decision gx o where k is an integer ai ak hi hk are constants xl xk e rd and k is a kernel function as ku or ku io in this form covers several well-known methodologies the kernel rule take k n ai hi h xi xi. with this choice for a large class of kernels we are guaranteed convergence if h and nh d this approach is attractive as no difficult optimization problem needs to be solved. the potential function method. in bashkirov braverman and muchnik the parameters are k n hi h xi xi. the weights ai are picked to minimize the empirical error on the data and h is held fixed. the original kernel suggested there is ku io linear discrimination. for k ku e- hi h al the set o is a linear halfspace. this of course is not versally consistent. observe that the separating hyperplane is the collection of all points x at equal distance from xl and by varying xl and all hyperplanes may be obtained. radial basis function neural networks powell head and lowe moody and darken poggio and girosi xu krzyzak and oja xu krzyzak and yuille and krzyzak linder and lugosi an even more general function is usually employed here k l ci k xd t aix xi co il where the as are tunable d x d matrices. sieve methods. grenander and geman and hwang advocate the use of maximum likelihood methods to find suitable values for the tunable parameters in k k fixed beforehand subject to certain compactness constraints on these parameters to control the abundance of choices one may have. if we were to use empirical error minimization we would find if k n that all data points can be correctly classified the his small enough setai xi xi k n causing overfitting. hence k must be smaller than n if parameters are picked in this manner. practical ways of choosing the parameters are discussed by kraaijveld and duin radial basis function networks and chou and chen in both and the xis may be thought of as representative prototypes the ais as weights and the his as the radii of influence. as a rule k is much smaller than n as xl xk summarizes the information present at the data. to design a consistent rbf neural network classifier we may proceed as in we may also take k but k on. just let xd xk ak and choose ai or hi to minimize a given error criterion based upon x k i x n such as a ln l igx n n ikl where gx o and is as in this is nothing but data splitting convergence conditions are described in theorem a more ambitious person might try empirical risk minimization to find the best xl xb ai ab ai ak x d matrices as in based upon n igx!yd n ii if k the class of rules contains a consistent subsequence and therefore it suffices only to show that the vc dimension is on log n. this is a difficult task and some kernels yield infinite vc dimension even if d i and k is very small chapter however there is a simple argument if k ir for a simple set r. let a x a ay y e r a end a a d x d matrix if r is a sphere then a is the class of all ellipsoids. the number of ways of shattering a set xn by intersecting with members from a is not more than ndd theorem problem the number of ways of shattering a set x by intersecting with sets of the form is not more than the product of all ways of shattering by intersections with ri with and so forth that is the logarithm of the shatter coefficient is on if k on log n. thus by corollary we have theorem linder and lugosi lfwe take k k on log n in the rbf classifier in which k ir r being the unit ball of neural networks r d and in which all the parameters are chosen by empirical risk minimization then el n l for all distributions of y. furthermore ijqk is the class of all rbf classifiers with k prototypes el n inf lg geqk kdd log n the theorem remains valid modified constants in the error estimate when r is a hyperrectangle or polytope with a bounded number of faces. however for more general k the vc dimension is more difficult to evaluate. for general kernels consistent rbf classifiers can be obtained by empirical l i or error minimization however no efficient practical algorithms are known to compute the minima. finally as suggested by chou and chen and kraaijveld and duin it is a good idea to place xl by k-means clustering or another clustering method and to build an rbf classifier with those values or by optimization started at the given cluster centers. problems and exercises problem let k i be integers with d k n and i assume x has a density. let a be a collection of hyperplanes drawn from the e possible hyperplanes through d points of x k and let ga be the corresponding natural classifier based upon the arrangement pea. take i such collections a at random and with replacement and pick the best a by minimizing lnga where show that the selected classifier is consistent if i ld on n k this is applicable with k l n j that is half the sample is used to define a and the other half is used to pick a classifier empilically. problem we are given a tree classifier with k internal linear splits and k i leaf regions bsp tree. show how to combine the neurons in a two-hidden-iayer perceptron with k and k i hidden neurons in the two hidden layers so as to obtain a decision that is identical to the tree-based classifier sethi for more on the equivalence of decision trees and neural networks see meisel koutsougeras and papachristou or golea and marchand hint mimic the argument for arrangements in text. problem extend the proof of theorem so that it includes any nondecreasing sigmoid with limx_oo o-x and limxoo o-x hint if t is large o-tx imates the threshold sigmoid. problem this exercise states denseness in l i for any probability measure fj.. show that for every probability measure fj. on r d every measurable function f rd r problems and exercises with j and every e there exists a neural network with one hidden layer and function as in such that f i!x e hint proceed as in theorem considering the following approximate! in l by a continuous function gx that is zero outside some bounded set b c since gx is bounded its maximum maxxerd is finite. now choose a to be a positive number large enough so that both b c ad andu ad is large. extend the restriction of gx to ad periodically by tiling over all of nd. the obtained function ix is a good approximation of gx in l j take the fourier series approximation of ix and use the stone-weierstrass theorem as in theorem observe that every continuous function on the real line that is zero outside some bounded interval can be arbitrarily closely approximated uniformly over the whole real line by one-dimensional neural networks. thus bounded functions such as the sine and cosine functions can be approximated arbitrarily closely by neural networks in l for any probability measure j on n. apply the triangle inequality to finish the proof. problem generalize the previous exercise for denseness in lpu. more precisely let p show that for every probability measure on nd every measurable function! nd nwithjl!xipudx ooandeverye othereexistsaneural network with one hidden layer hx such that ix hex! lip e problem committee machines. let ck be the class of all committee machines. prove that for all distributions of y lim inf l l k---oo hint for a one-hidden-layer neural network with coefficients ci in approximate the cis by discretization to a grid of values and note that ci may thus be approximated in a committee machine by a sufficient number of identical copies of this only forces the number of neurons to be a bit larger. problem prove theorem for arbitrary sigmoids. hint approximate the old sigmoid by a x for a sufficiently large t. problem let a be a nondecreasing sigmoid with ax if x and ax if x denote by ck the class of corresponding neural network classifiers with k hidden layers. show that vck vck where ck is the class corresponding to the threshold sigmoid. neural networks problem this exercise generalizes theorem for not fully connected neural works with one hidden layer. consider the class ck of one-hidden-iayer neural networks with the threshold sigmoid such that each of the k nodes in the hidden layer are connected to d dk inputs where di d. more precisely ck contains all classifiers based on functions of the form co l where l d k i where for each i mid is a vector of distinct positive integers not exceeding d. show that if the as cs and mis are the tunable parameters i problem let be a sigmoid that takes m different values. find upper bounds on the vc dimension of the class ck. problem consider a one-hidden-iayer neural network if y yn are fixed and all xis are different show that with n hidden neurons we are always able to tune the weights such that yi for all i. remains true if yen instead of y e i. the property above describes a situation of overfitting that occurs when the neural network becomes too also that the vc dimension which is at least d times the number of hidden neurons must remain smaller than n for any meaningful training. problem the bernstein perceptron. consider the following perceptron for dimensional data if xk-i otherwise. let us call this the bernstein perceptron since it involves bernstein polynomials. if n data points are collected how would you choose k a function of n and how would you adjust the weights ais to make sure that the bernstein perceptron is consistent for all distributions of y with pix e in i? can you make the bernstein perceptron consistent for all distributions of y on n x i? figure the bernstein ceptron. problem use the ideas of section and problem to prove theorem problem consider spechts padaline with r rn t let h hll and nhd show that for any distribution of y elj l problems and exercises problem bounded first layers. consider a feed-forward neural network with any number of layers with only one restriction that is the first layer has at most k d outputs zk where x xd is the input and is an arbitrary function n n just a sigmoid. the integer k remains fixed. let a denote the k x matrix of weights aji bj if l is the bayes error for a recognition problem based upon y with z zk and then show that for some distribution of y inf a l l where l is the bayes probability of error for y. if k d however show that for any strictly monotonically increasing sigmoid inf a l l use to conclude that any neural network based upon a first layer with k d outputs is not consistent for some distribution regardless of how many layers it has however that the inputs of each layer are restricted to be the outputs of the previous layer. figure the first layer is stricted to have k outputs. it has ked tunable parameters. zj problem find conditions on kll and that guarantee strong universal consistency in theorem problem barron networks. call a barron network a network of any number of layers in figure with inputs per cell and cells that perform the operation a yy on inputs x yen with trainable weights a y if is the output neural networks of a network with d inputs and k cells in any way compute an upper bound for the vc dimension of the classifier gx o as a function of d and k. note that the structure of the network the positions of the cells and the connections is variable. problem continued. restrict the balton network to llayers and k cells per layer kl cells total and repeat the previous exercise. problem continued. find conditions on k and l in the previous exercises that would guarantee the universal consistency of the barron network if we train to minimize the empirical eltor. problem consider the family of functions of the form i l l wij x i x k d i i k j! where all the wi and wil j are tunable parameters. show that for every i e co ld and e there exist k llarge enough so that for some g e supxeoljd ix e. problem continued. obtain an upper bound on the vc dimension of the above two-hidden-iayer network. hint the vc dimension is usually about equal to the number of degrees of freedom is dlk here. problem continued. if gn is the rule obtained by empirical error minimization over then show that lgn l in probability if k l and kl onj log n. problem how many different monomials of order r in xl xed are there? how does this grow with r when d is held fixed? problem show that and in the bit-interleaving example in the section on kolmogorov-lorentz representations are not continuous. which are the points of nuity? problem let pick c j by empirical risk minimization for the classifier gx o e showthateln l for all distributions of ywhenm ooandm on d jlogn. problem write as and identify the coefficients show that there is an entire subspace of solutions and shahshahani problem show that ex and cannot be written in the form x for any finite k where x and ai e i k and shahshahani thus the projection pursuit representation of functions can only at best approximate all continuous functions on bounded sets. problem letf be the class of classifiers of the form g where al ii for arbitrary functions and coefficients e r. show that for some distribution of y on x infge.f lg l so that there is no hope of meaningful distribution-free classification based on additive functions only. problems and exercises problem continued. repeat the previous exercise for functions of the form ai and distributions of y on x l. thus additive functions of pairs do not suffice either. problem let k r r be a nonnegative bounded kernel with f k show that for any e any measurable function i rd r and probability measure fj- such that f iixifj-dx there exists a function of the form ci k bit aix bi co such that f iix e poggio and girosi park and sandberg darken donahue gurvits and sontag krzyzak linder and lugosi for such denseness results. hint relate the problem to a similar result for kernel estimates. problem let ck be the class of classifiers defined by the functions k l ci k bil aix bi co il find upper bounds on its vc dimension when k is an indicator of an interval containing the origin. problem consider the class of radial basis function networks where k is nonnegative unimodal bounded and continuous. let vr n be a function that minimizes i n- i yi lover e f n and define gn as the corresponding classifier. prove that if kn fjn and knfj on as n then gn is universally consistent linder and lugosi hint proceed as in the proof of theorem use problem to handle the approximation error. bounding the covering numbers needs a little additional work. other error estimates in this chapter we discuss some alternative error estimates that have been troduced to improve on the performance of the standard estimates-holdout substitution and deleted-we have encountered so far. the first group of these estimates-smoothed and posterior probability estimates-are used for their small variance. however we will give examples that show that classifier selection based on the minimization of these estimates may fail even in the simplest situations. among other alternatives we deal briefly with the rich class of bootstrap estimates. smoothing the error count the resubstitution deleted and holdout estimates of the error probability chapters and are all based on counting the number of errors committed by the classifier to be tested. this is the reason for the relatively large variance inherently present in these estimates. this intuition is based on the following. most classification rules can be written into the form gn dn otherwise where dn is either an estimate of the a posteriori probability as in the case of histogram kernel or nearest neighbor rules or something else as for generalized linear or neural network classifiers. in any case if dn is close to then we feel that the decision is less robust compared to when the value of dn is far from in other words intuitively inverting the value of the other error estimates decision gn at a point x where dn is close to makes less difference in the eltor probability than if dn is large. the eltor estimators based on counting the number of eltors do not take the value of into account they eltofs with the same amount no matter what the value of is. for example in the case of the resubstitution estimate each eltor contributes with n to the overall count. now if dn is close to a small perturbation of xi can flip the decision gnxi and therefore change the value of the estimate by n although the eltor probability of the rule gn probably does not change by this much. this phenomenon is what causes the relatively large variance of eltor counting estimators. glick proposed a modification of the counting eltor estimates. the eral form of his estimate is where r is a monotone increasing function satisfying r u r u. possible choices of the smoothing function ru are ru u or ru io cu where the parameter c may be adjusted to improve the behavior of the estimate also glick knoke or tutz both of these estimates give less penalty to eltofs close to the decision boundary that is to eltors where is close to note that taking ru coltesponds to the resubstitution estimate. we will see in theorem below that if r is smooth then ls indeed has a very small variance in many situations. just like the resubstitution estimate the estimate ls may be strongly mistically biased. just consider the i-nearest neighbor rule when ls with probability one whenever x has a density. to combat this defect one may define the deleted version of the smoothed estimate where dni is the training sequence with the i-th pair yi deleted. the first thing we notice is that this estimate is still biased even asymptotically. to illustrate this point consider ru u. in this case e e dn-l d n e dn- dn- if the estimate dn-l was perfect that is equal to for every x then the expected value above would be nx which is the asymptotic error probability lnn of the i-nn rule. in fact smoothing the error count ie lnni ie d n- dn- dn- this means that when the estimate of the a posteriori probability of is consistent in then lsd converges to l nn and not to l biasedness of an error estimate is not necessarily a bad property. in most plications all we care about is how the classification rule selected by minimizing the error estimate works. unfortunately in this respect smoothed estimates form poorly even compared to other strongly biased error estimates such as the empirical squared error problem the next example illustrates our point. theorem let the distribution of x be concentrated on two values such that px a px b and let and assume that the smoothed error estimate is minimized over e f to select a classifier from f where the class f contains two functions the true a posteriori probability function and ij where ija then the probability that ij is selected converges to one as n proof. straightforward calculation shows that the statement follows from the law of large numbers. remark. the theorem shows that even if the true a posteriori probability function is contained in a finite class of candidates the smoothed estimate with ru u is unable to select a good discrimination rule. the result may be extended to general smooth rs. as theorems and show empirical squared error minimization or maximum likelihood never fail in this situation. finally we demonstrate that if r is smooth then the variance of ls is indeed small. our analysis is based on the work of lugosi and pawlak the bounds other error estimates for the variance of ls remain valid for lsd. consider classification rules of the form ifrjnx dn gn otherwise where dn is an estimate of the a posteriori probability examples include the histogram rule where chapters and the k-nearest neighbor rule where and or the kernel rule where in the sequel we concentrate on the performance of the smoothed estimate of the error probability of these nonparametric rules. the next theorem shows that for these rules the variance of the smoothed error estimate is n no matter what the distribution is. this is a significant improvement over the variance of the deleted estimate which as pointed out in chapter can be larger than theorem assume that the smoothing function ru satisfies ru for u e and is uniformly lipschitz continuous that is iru clu vi for all u v and for some constant c. then the smoothed estimate ls of the histogram k-nearest neighbor and moving window rules kernel k iso satisfies and varls n c where c is a constant depending on the rule only. in the case of the histogram rule the value of c is c for the k-nearest neighbor rule c and for the moving window rule c here c is the constant in the lipschitz condition yd is the minimal number of cones centered at the origin of angle n that coverrd and is the minimal number of balls of radius that cover the unit ball in rd. smoothing the error count remark. notice that the inequalities of theorem are valid for all n e and h for the histogram and moving window rules and k for the nearest neighbor rules. interestingly the constant c does not change with the dimension in the histogram case but grows exponentially with d for the k-nearest neighbor and moving window rules. proof. the probability inequalities follow from appropriate applications of diarmids inequality. the upper bound on the variance follows similarly from theorem we consider each of the three rules in tum. the histogram rule. let yd yn be a fixed training sequence. if we can show that by replacing the value of a pair yi in the training sequence by some y the value of the estimate ls can change by at most then the inequality follows by applying theorem with ci sin. the i term of the sum in ls can change by one causing in change in the average. obviously all the other terms in the sum that can change are the ones corresponding to the xjs that are in the same set of the partition as either xi or x. denoting the number of points in the same set with xi and xi by k and k respectively it is easy to see that the estimate of the a posteriori probabilities in these points can change by at most k and k respectively. it means that the overall change in the value of the sum can not exceed f in in. the k-nearest neighbor rule. to avoid difficulties caused by breaking distance ties assume that x has a density. then recall that the application of lemma for the empirical distribution implies that no x j can be one of the k nearest neighbors of more than kyd points from dn. thus changing the value of one pair in the training sequence can change at most terms in the expression of ld one of them by at most and all the others by at most c i k. theorem yields the result. the moving window rule. again we only have to check the condition of theorem with ci fix a training sequence yl yn and replace the pair yi by y then the i term in the sum of the expression of ls can change by at most one. clearly the j-th term for which xj fj sxi. h and xj fj sx.h keeps its value. it is easy to see that all the other terms can change by at most c max i k j i kj where k j and kj are the numbers of points xk k i j from the training sequence that fall in sxi.h and sx.h respectively. thus the overall change in the sum does not exceed l k. l.. k xjesxi.h xjesx.h it suffices to show by symmetry that lxes i j xk e sxih n sxjhl. then clearly xih llkj s let nj ixb k lk-sln. xjesxih xjesxj.h other error estimates to bound the right-hand side from above cover sxjh by fjd balls sl of radius denote the number of points falling in them by lm fjd lm ixk k i i xk e sxjh n sml then and the theorem is proved. posterior probability estimates the error estimates discussed in this section improve on the biasedness of smoothed estimates while preserving their small variance. still these estimates are of tionable utility in classifier selection. considering the formula for the error probability of a classification rule gnx it is plausible to introduce the estimate lp n n l dn n il dn that is the expected value is estimated by a sample average and instead of the a posteriori probability yjx its estimate dn is plugged into the formula of ln. the estimate l? is usually called the posterior probability error estimate. in the case of nonparametric rules such as histogram kernel and k-nn rules it is natural to use the corresponding nonparametric estimates of the a riori probabilities for plugging in the expression of the error probability. this and similar estimates of ln have been introduced and studied by fukunaga and kessel rora and wilcox fitzmaurice and rand ganesalingam and mclachlan kittler and devijver matloff and pruitt moore whitsitt and landgrebe pawlak schwemer and dunn and lugosi and pawlak it is interesting to notice the similarity between the estimates ls and lp although they were developed from different scenarios. to reduce the bias we can use the leave-one-out deleted version of the estimate lpd n n l dnj n il dni the deleted version lp d has a much better bias than lsd. we have the bound posterior probability estimates ielpd ie dn- dn-d this means that if the estimate of the a posteriori probability of is sistent in l then elp d converges to l this is the case for all distributions for the histogram and moving window rules if h and nh d and the k-nearest neighbor rule if k and k n as it is seen in chapters and for specific cases it is possible to obtain sharper bounds on the bias of lp d. for the histogram rule lugosi and pawlak carried out such analysis. they showed for example that the estimate lpd is optimistically biased problem posterior probability estimates of ln share the good stability properties of smoothed estimates finally let us select a function nd a corresponding rule gx ih a class f based on the minimization of the posterior probability error estimate observe that when e i for all x that is rule selection based on this estimate just does not make sense. the reason is that lp ignores the yis of the data sequence! fukunaga and kessel argued that efficient posterior probability tors can be obtained if additional unclassified observations are available. very often in practice in addition to the training sequence dn further feature vectors xnz are given without their labels ynz where the xni are i.i.d. independent from x and dn and they have the same distribution as that of x. this situation is typical in medical applications when large sets of medical records are available but it is usually very expensive to get their correct diagnosis. these unclassified samples can be efficiently used for testing the performance of a classifier designed from dn by using the estimate z i l dn il dn again using l for rule selection is meaningless. other error estimates rotation estimate this method suggested by toussaint and donaldson is a combination of the holdout and deleted estimates. it is sometimes called the n let s n be a positive integer much smaller than n and assume for the sake of simplicity that q n s is an integer. the rotation method forms the holdout estimate by holding the first s pairs of the training data out then the second s pairs and so forth. the estimate is defined by averaging the q numbers obtained this way. to formalize denote by dj the training data with the j-th s-block held out q dj yd ysj-l ysl yn the estimate is defined by q l l l i s n s q s isj-ll s yields the deleted estimate. if s then the estimate is usually more biased than the deleted estimate as but usually exhibits smaller variance. elj eln- s bootstrap bootstrap methods for estimating the error became popular lowing the revolutionary work of efron all bootstrap estimates introduce artificial randomization. the bootstrap sample d yb in ycb in in is a sequence of random variable pairs drawn randomly with replacement from the set yd yn. in other words conditionally on the training sample dn yi yn the pairs y? are drawn independently from vn the empirical distribution based on dn in n d x i. one of the standard bootstrap estimates aims to compensate the mistic bias bgn elgn l?gn of the resubstitution estimate lr. to estimate bgn a bootstrap sample of size m n may be used often bootstrap sampling is repeated several times to average out effects of the additional randomization. in our case bootstrap yields the estimator lbgn lrgn bngn. another instance of a bootstrap sample of size n the so-called eo estimator uses resubstitution on the training pairs not appearing in the bootstrap sample. the estimator is defined by here too averages may be taken after generating the bootstrap sample several times. many other versions of bootstrap estimates have been reported such as the estimate bootstrap and bootstrap hand jain dubes and chen and mclachlan for surveys and ditional references. clearly none of these estimates provides a universal remedy but for several specific classification rules bootstrap estimates have been mentally found to outperform the deleted and resubstitution estimates. however one point has to be made clear we always lose information with the additional randomization. we summarize this in the following simple general result theorem let x i xn be drawn from an unknown distribution jl and let afl be afunctional to be estimated. let r be a convex risk function as ru or ru lui. let xib xi be a bootstrap sample drawn from xl x n then the theorem states that no matter how large m is the class of estimators that are functions of the original sample is always at least as good as the class of all estimators that are based upon bootstrap samples. in our case ajl plays the role of the expected error probability eln pgnx y. if we take ru then it follows from the theorem that there is no estimator ln based on the bootstrap sample d whose squared error e is smaller than that of some nonbootstrap estimate. in the proof of the theorem we construct such a non-bootstrap estimator. it is clear however that in general the latter estimator is too complex to have any practical value. the randomization of bootstrap methods may provide a useful tool to overcome the computational difficulties in finding good estimators. other error estimates proof. let be any mapping taking m arguments. then e x ajl lxi xn m n i imci xi ajl jensens inequality and the convexity of r r i r xn ajl n xij ajl where now after taking expectations with respect to xl x n we see that for every we start out with there is a that is at least as good. i xm ym if m then the bootstrap has an additional problem related to the coupon collector problem. let n be the number of different pairs in the bootstrap sample yb en m en lor some constant c t en n n e-c with probability one. to see this note that lcn thf ne-c h en n n so e n n e furthermore if one of the m drawings is varied n changes by at most one. hence by mcdiarmids inequality for e from which we conclude that n n e-c with probability one. as n e-c of the original data pairs do not appear in the bootstrap sample a considerable loss of information takes place that will be reflected in the performance. this phenomenon is well-known and motivated several modifications of the simplest bootstrap estimate. for more information see the surveys by hand jain dubes and chen and mclachlan problems and exercises problems and exercises problem show that the posterior probability estimate lp of the histogram k-nearest neighbor and moving window rules satisfies and varl n where c is a constant depending on the rule. in the case of the histogram rule the value of c is c for the k-nearest neighbor rule c and for the moving window rule c also show that the deleted version lp d of the estimate satisfies the same inequalities and pawlak hint proceed as in the proof of theorem problem show that the deleted posterior probability estimate of the error probability of a histogram rule is always optimistically biased that is for all n and all distributions e el n- l problem show that for any classification rule and any estimate tjnx dn of the a posteriori probabilities for all distributions of y for alli n and e p il e i e i dn and var i. further show that for alii e e problem empirical squared error. consider the deleted empirical squared error show that een e dn-d lnn where lnn is the asymptotic error probability on the i-nearest neighbor rule. show that if tjn-j is the histogram kernel or k-nn estimate then varen cn for some constant depending on the dimension only. we see that en is an asymptotically optimistically biased estimate of lgn when tjn-j is an l estimate of tj. still this estimate is useful in classifier selection theorem feature extraction dimensionality reduction so far we have not addressed the question of how the components of the feature vector x are obtained. in general these components are based on d measurements of the object to be classified. how many measurements should be made? what should these measurements be? we study these questions in this chapter. general recipes are hard to give as the answers depend on the specific problem. however there are some rules of thumb that should be followed. one such rule is that noisy measurements that is components that are independent of y should be avoided. also adding a component that is a function of other components is useless. a essary and sufficient condition for measurements providing additional information is given in problem our goal of course is to make the error probability lgn as small as possible. this depends on many things such as the joint distribution of the selected nents and the label y the sample size and the classification rule gn to make things a bit simpler we first investigate the bayes errors corresponding to the selected components. this approach makes sense since the bayes error is the theoretical limit of the performance of any classifier. as problem indicates collecting more measurements cannot increase the bayes error. on the other hand having too many components is not desirable. just recall the curse of dimensionality that we often faced to get good error rates the number of training samples should be exponentially large in the number of components. also computational and storage limitations may prohibit us from working with many components. we may formulate the feature selection problem as follows let xl xed feature extraction be random variables representing d measurements. for a set a of indices let x a denote the i a i-dimensional random vector whose components are the xis with i e a the order of increasing indices. define l inf grjai--o i pgxa y as the bayes error corresponding to the pair y. figure a possible rangement of las for d l o obviously l l whenever b c a and l minpy oj py i. the problem is to find an efficient way of selecting an index set a with iai k whose corresponding bayes error is the smallest. here k d is a fixed integer. exhaustive search through the possibilities is often sirable because of the imposed computational burden. many attempts have been made to find fast algorithms to obtain the best subset of features. see fu min and li kanal ben-bassat and devijver and kittler for surveys. it is easy to see that the best k individual features-that is components corresponding to the k smallest values of l i not necessarily constitute the best k-dimensional vector just consider a case in which xl xk. cover and van campenhout showed that any ordering of the subsets of d consistent with the obvious requirement l l if b a is possible. more precisely they proved the following surprising result theorem and van campenhout let ai a be an ordering of the subsets of satisfying the consistency property i j if ai c a j al and d. then there exists a distribution of the random variables y xed y such that the theorem shows that every feature selection algorithm that finds the best element subset has to search exhaustively through all k-element subsets for some distributions. any other method is doomed to failure for some distribution. many suboptimal heuristic algorithms have been introduced trying to avoid the tational demand of exhaustive search e.g. sebestyen meisel dimensionality reduction chang vilmansen and devijver and kittler narendra and fukunaga introduced an efficient branch-and-bound method that finds the optimal set of features. their method avoids searching through all subsets in many cases by making use of the monotonicity of the bayes error with respect to the partial ordering of the subsets. the key of our proof ofthe theorem is the following simple lemma lemma let ai be as in theorem let i assume that the distribution of y on nd x i is such that the distribution of x is concentrated on a finite set l l and l j for each i then there exists another finite distribution such that l j remains unchanged for each i and la j foreachi. proof. we denote the original distribution of x by and the a posteriori probability function by fj. we may assume without loss of generality that every atom of the distribution of x is in md for some m since l d the value of fjx is either zero or one at each atom. we construct the new distribution by duplicating each atom in a special way. we describe the new distribution by defining a measure on nd and an a posteriori function fj nd i. define the vector vai e nd such that its m-th component equals m if m ai and zero if m e ai. the new measure has twice as many atoms as for each atom x e nd of the new measure has two atoms namely xl x and x va i the new distribution is specified by q q fj fjx and fj fjx where q e is specified later. it remains for us to verify that this distribution satisfies the requirements of the lemma for some q. first observe that the values l j remain unchanged for all i. this follows from the fact that there is at least one component in a j along which the new set of atoms is strictly separated from the old one leaving the corresponding contribution to the bayes error unchanged. on the other hand as we vary q from zero to the new value of l grows continuously from the old value of l to therefore since by assumption max ji l j there exists a value of q such that the new l satisfies maxji l j l as desired. proof of theorem we construct the desired distribution in steps applying lemma in each step. the procedure for d is illustrated in figure feature extraction cd ag cd cd cd cd cd i i i i cd figure construction of a prespecijied ordering. in this three-dimensional example the first four steps of the procedure are shown when the desired ordering is black circles represent atoms with and white circles are those with o. we start with a monoatomic distribution concentrated at the origin with then clearly l o. by lemma we construct a distribution such that by applying the lemma again we can construct a distribution with l l l after i steps we have a distribution satisfying the last i inequalities of the desired ordering. the construction is finished after steps. remark. the original example of cover and van campenhout uses the multidimensional gaussian distribution. van campenhout developed the idea further by showing that not only all possible orderings but all possible values of the l can be achieved by some distributions. the distribution constructed in the above proof is discrete. it has atoms. one may suspect that feature extraction is much easier if given y the ponents xd are conditionally independent. however three and dimensional examples given by elashoff elashoff and goldman toussaint and cover show that even the individually best two independent components are not the best pair of components. we do not know if theorem dimensionality reduction generalizes to the case when the components are conditionally independent. in the next example the pair of components consisting of the two worst single features is the best pair and vice versa. theorem there exist binary-valued random variables xl x y e i such that xl x and are conditionally independent y and l l l but l l l proof. let py i then the joint distribution of xl y is ified by the conditional probabilities pxi o and pxi lly i straightforward calculation shows that the values pxi lly o iiy o iiy o pxi iiy i iiy iiy i satisfy the stated inequalities. as our ultimate goal is to minimize the error probability finding the feature set minimizing the bayes error is not the best we can do. for example we know that we will use the neighbor rule then it makes more sense to select the set of features that minimizes the asymptotic error probability of the neighbor rule. recall from chapter that e the situation here is even messier than for the bayes error. as the next example shows it is not even true that a c b implies where a b are two subsets of components and denotes the asymptotic error probability of the neighbor rule for y. in other words adding components may increase this can never happen to the bayes error-and in fact to any f the anomaly is due to the fact that the function x is convex near zero and one. example. let the joint distribution of x be uniform on the joint distribution of y is defined by the a posteriori probabilities given by if x e x if x e x if x e x if x e x x feature extraction straightforward calculations show that while a smaller value! figure anexamplewhen an additionalfeature increases the error probability of the rule. lnn of course the real measure of the goodness of the selected feature set is the error probability lgn of the classifier designed by using training data dn. if the classification rule gn is not known at the stage of feature selection then the best one can do is to estimate the bayes errors l for each set a of features and select a feature set by minimizing the estimate. unfortunately as theorem shows no method of estimating the bayes errors can guarantee good performance. if we know what classifier will be used after feature selection then the best strategy is to select a set of measurements based on comparing estimates of the error probabilities. we do not pursue this question further. for special cases we do not need to mount a big search for the best features. here is a simple example given y i let x xed have d independent components where given y i xj is normal ji a. it is easy to verify that if p i then lpn where n is a standard normal random variable and l jl cyj is the square of the mahalanobis distance also duda and hart pp. for this case the quality of the j-th feature is measured by we may as well rank these values and given that we need only d d features we are best off taking the d features with the highest quality index. transformations with small distortion it is possible to come up with analytic solutions in other special cases as well. for example raudys and raudys and pikelis investigated the pendence of e lgn on the dimension of the feature space in the case of certain linear classifiers and normal distributions. they point out that for a fixed n by increasing the number of features the expected error probability elgn first decreases and then after reaching an optimum grows again. transformations with small distortion one may view the problem of feature extraction in general as the problem of ing a transformation a function t nd nd so that the bayes error l corresponding to the pair y is close to the bayes error l corresponding to the pair y. one typical example of such transformations is fine quantization discretization of x when t maps the observed values into a set of finitely or countably infinitely many values. reduction of dimensionality of the tions can be put in this framework as well. in the following result we show that small distortion of the observation cannot cause large increase in the bayes error probability. theorem and gyorfi assume that for a sequence of transformations tn n in probability where ii ii denotes the euclidean norm in nd. then if l is the bayes error for y and ln is the bayes error for y l l. t proof. for arbitrarily small e we can choose a uniformly continuous function o ijx such that ijxi e. for any transformation t consider now the decision problem of y where the random variable y satisfies py x x ijx for every x e nd. denote the corresponding bayes error by i and the bayes error corresponding to the pair y by i obviously tn t tn tn to bound the first and third terms on the right-hand side observe that for any transformation t py tx e tx e tx feature extraction therefore the bayes error corresponding to the pair y equals l e e thus il ie tx e tx e e e e e ryxi jensens inequality e so the first and third terms of are less than e. for the second term define the decision function gn if ryx otherwise which has error probability ign pgntnx y. then ign ii and we have by theorem all we have to show now is that the limit supremum of the above quantity does not exceed e. let o e be the inverse modulus of continuity of the a posteriori probability ry that is oe sup yll for every e we have oe by the uniform continuity of ry. now we have irytnx ryxi clearly the first term on the right-hand side converges to zero by assumption while the second term does not exceed e by the definition of oe. remark. it is clear from the proof of theorem that everything remains true if the observation x takes its values from a separable metric space with metric p and the condition of the theorem is modified to ptx x in probability. this generalization has significance in curve recognition when x is a stochastic process. then theorem asserts that one does not lose much information by using usual discretization methods such as for example karhunen-loeve series expansion problems to admissible and sufficient transformations admissible and sufficient transformations sometimes the cost of guessing zero while the true value of y is one is different from the cost of guessing one while y o. these situations may be handled as follows. define the costs crn i rn l here cy gx is the cost of deciding on gx when the true label is y. the risk of a decision function g is defined as the expected value of the cost note that if rg ecy gx l if rn otherwise c rn then the risk is just the probability of error. introduce the notation qmx rn co rn rn then we have the following extension of theorem theorem define gx if ql qox otherwise. then for all decision functions g we have rg rg rg is called the bayes risk. the proof is left as an exercise problem which transformations preserve all the necessary information in the sense that the bayes error probability corresponding to the pair y equals that of y? clearly every invertible mapping t has this property. however the practically interesting transformations are the ones that provide some compression of the data. the most efficient of such transformations is the bayes decision g itself g is specifically designed to minimize the error probability. if the goal is to minimize the bayes risk with respect to some other cost function than the error probability then g generally fails to preserve the bayes risk. it is natural to ask what transformations preserve the bayes risk for all possible cost functions. this question has a practical significance when collecting data and construction of the decision are separated in space or in time. in such cases the data should be transmitted via a communication channel should be stored. in both cases there is a need for an efficient data compression rule. in this problem formulation when getting the data one may not know the final cost function. therefore a desirable data compression does not increase the bayes risk for any cost function c- here t is a measurable function mapping from rd to rd for some positive integer d. feature extraction definition let r t denote the bayes riskfor the costfunction c and formed observation tx. a transformation t is called admissible iffor any cost function c rt r where r denotes the bayes risk for the original observation. obviously each invertible transformation t is admissible. a nontrivial example of an admissible transformation is tx since according to theorem the bayes decision for any cost function can be constructed by the a posteriori probability and by the cost function. ingly this is basically the only such transformation in the following sense theorem a transformation t is admissible if and only if there is a mapping g such that gtx with probability one. proof. the converse is easy since for such g r rtx rgtx r. assume now that t is admissible but such function g does not exist. then there is a set a c nd such that fla tx is constant on a while all values then there are real numbers c and e and sets b dca such that of are different on a that is if x yea then x y implies flb fld and c e if x e b c-e if xed. now choose a cost function with the following values coo cii coi and then cio c c qox qix and the bayes decision on bud is given by if c otherwise. g admissible and sufficient transformations now let get be an arbitrary decision. without loss of generality we can assume that gtx if x e a. then the difference between the risk of gtx and the bayes risk is jrd r qgxx mdx i qlx mdx i mdx i cx jldx o. d we can give another characterization of admissible transformations by virtue of a well-known concept of mathematical statistics definition tx is called a sufficient statistic if the random variables x tx and y form a markov chain in this order. that is for any set a py e aitx x pry e aitx. theorem a transformation t is admissible if and only ift is a sufficient statistic. proof. assume that t is admissible. then according to theorem there is a mapping g such that then and gtx with probability one. py iitx x py llx gtx py iitx epy litx xitx e itx gtx thus py iitx x py litx therefore tx is sufficient. on the other hand if tx is sufficient then py llx py iitx x py iitx feature extraction so for the choice gtx py iitx we have the desired function g and therefore t is admissible. theorem states that we may replace x by any sufficient statistic tx without altering the bayes error. the problem with this in practice is that we do not know the sufficient statistics because we do not know the distribution. if the distribution of y is known to some extent then theorem may be useful. example. assume that it is known that e-cllxll for some unknown c o. then ii x ii is a sufficient statistic. thus for discrimination we may replace the d-dimensional vector x by the i-dimensional random variable iixii without loss of discrimination power. example. if for some function then is a sufficient statistic. for discrimination there is no need to deal with xl it suffices to extract the features x x and x example. if given y i x is normal with unknown mean mi and diagonal covariance matrix a i unknown a then is a function of iix ii x mo only for unknown mo mi. here we have no obvious sufficient statistic. however if mo and m are both known then a quick inspection shows that x t mo is a i-dimensional sufficient statistic. again it suffices to look for the simplest possible argument for if mo m but the covariance matrices are ag i and al i given that y or y then iixii is a sufficient statistic. in summary the results of this section are useful for picking out features when some theoretical information is available regarding the distribution of y. problems and exercises consider the pair y e rdxo lofrandomvariablesandletxcdl e r be an additional component. define the augmented random vector x x cdl. denote the bayes errors corresponding to the pairs y and y by l and l respectively. clearly l l prove that equality holds if and only if p itcx where the a posteriori probability functions r rd i and r r i are defined as rx py llx x and rx py llx x. hint consult with the proof of theorem problem letpy i ii xcd have d independent components where xj is normal ji a. prove that l p n where n is a standard normal random variable and is the square of the mahalanobis distance ll jl m a and hart pp. problems and exercises problem sampling of a stochastic process. let xt t e be a tic process a collection of real-valued random variables indexed by t and let y be a binary random variable. for integer n define xw x n i n. find sufficient conditions on the function met and on the covariance function kt s ext extxs exs such that lim l l n--oo where lx is the bayes error corresponding to xf and lx is the infimum of the error probabilities of decision functions that map measurable functions into i. hint introduce the stochastic process xnt as the linear interpolation of xl xn. find conditions under which lim e n--oo and use theorem and the remark following it. problem expansion of a stochastic process. let xt met and kt s be as in the previous problem. let be a complete orthonormal system of functions on define find conditions under which lim l xn l n--oo problem extend theorem such that the transformations tnx dj are allowed to depend on the training data. this extension has significance because feature extraction algorithms use the training data. problem for discrete x prove that tx is sufficient iff y tx x form a markov chain this order. problem prove theorem problem recall the definition of f from chapter let f be a strictly cave function. show that dfx y dftx y if and only if the transformation t is admissible. conclude that lnnx y lnntx y construct a t and a distribution of y such that l y l y but t is not admissible. problem find sufficient statistics of minimal dimension for the following nation problems it is known that for two given sets a and b with a n b if y we have x e a and if y then x e b or vice versa. given y x is a vector of independent gamma random variables with common unknown shape parameter a and common unknown scale parameter b the marginal density of each component of x is of the form xa-je-xjb a x the parameters a and b depend upon y. problem let x have support on the surface of a ball of n d centered at the origin of unknown radius. find a sufficient statistic for discrimination of dimension smaller than d. feature extraction problem assume that the distribution of x is such that x and xl with probability one. find a simple sufficient statistic. appendix in this appendix we summarize some basic definitions and results from the theory of probability. most proofs are omitted as they may be found in standard textbooks on probability such as ash shiryayev chow and teicher durrett grimmett and stirzaker and zygmund we also give a list of useful inequalities that are used in the text. a.i basics of measure theory definition a.l. let s be a set and let f be a family of subsets of s. f is called a a if e f a e f ai e f implies ul ai e f. implies a c e f a a is closed under complement and union of countably infinitely many sets. conditions and imply that s e f. moreover and imply that a a-algebra is closed under countably infinite intersections. definition let s be a set and let f be a a of subsets of s. then is called a measurable space. the elements of f are called measurable sets. definition if s nd is the smallest a-algebra containing all angles is called the borel a-algebra. the elements are called borel sets. appendix definition aa. let f be a measurable space and let f function. f is called measurable if for all b e b s n be a f-lb fs e b e f that is the inverse image of any borel set b is in f. obviously if a is a measurable set then the indicator variable ia is a able function. moreover finite linear combinations of indicators of measurable sets simple functions are also measurable functions. it can be shown that the set of measurable functions is closed under addition subtraction multiplication and division. moreover the supremum and infimum of a sequence of measurable functions as well as its pointwise limit supremum and limit infimum are able. definition let f be a measurable space and let v f be a function. v is a measure on f if v is that is ai e f and ai n aj i j imply that vulai vai. in other words a measure is a nonnegative set function. definition v is a finite measure if vs v is a measure if there are countably many measurable sets ai a such that ul ai sand vai i definition the triple f v is a measure space ifs f is a measurable space and v is a measure on f. definition the lebesgue measure a on nd is a measure on the borel ofnd such that the a measure of each rectangle equals to its volume. the lebesgue integral definition let f v be a measure space and f xjai a simple function such that the measurable sets ai an are disjoint. then the integral of f with respect to v is defined by f fdv is fsvds txivai. if f is a nonnegative-valued measurable function then introduce a sequence of simple functions as follows us ln fs kin k if if fs the lebesgue integral then the fns are simple functions and fns fs in a monotone decreasing fashion. therefore the sequence of integrals f fndv is monotone decreasing with a limit. the integral f is then defined by f fdv fsvds lim f fn dv j s n---oo if f is an arbitrary measurable function then decompose it as a difference of its positive and negative parts fs fest fs- fest fst where f and f- are both nonnegative functions. define the integral of f by if at least one term on the right-hand side is finite. then we say that the integral exists. if the integral is finite then f is integrable. definition a.io. iff jdv exists and a is a measurablefunjinthen fa fdv is defined by c i fdv f fjadv. definition a.ii. we say that fn f v if v i fs o. theorem a.i. theorem. if fns fs in a monotone ing way for some nonnegative integrable function f then f lim fn dv lim f fn dv iz---oo theorem dominated convergence theorem. assume that gsfors e s n where f gdv fn f then f lim h lim f dv theorem lemma. let fl be measurable functions. if there exists a measurable function g with f gdv such thatjor every n hi gs then lim inf f hi d v f lim inf d v n---oo iz---oo appendix if there is a a measurable function g with j gdv such that fns gs for every n then lim sup f fn dv f lim sup fn dv n---oo n---oo definition let vi and be measures on a measurable space f. we say that vi is absolutely continuous with respect to if and only if implies vi e f. we denote this relation by vi theorem theorem. let vi and be measures on the measurable space f such that vi and is a-finite. then there exists a measurable function f such that for all a e f via f f is unique if vi is finite then f has a finite integral. definition f is called the density or radon-nikodym derivative of vi with respect to we use the notation f definition let vi and be measures on a measurable space f. if and then vi is singular there exists a set a e f such that via c with respect to vice versa. theorem a.s. decomposition theorem. if fj. is a a-finite measure on a measurable space f then there exist two unique measures vi such that fjv vi where vi fj. and is singular with respect to fj.. definition a.is. let f v be a measure space and let f be a measurable function. then f induces a measure fj. on the borel a as follows fj.b vf-ib b e b. theorem let v be a measure on the borel a b ofr and let f and g be measurable functions. then for all b e b r gxfj.dx r ib if-icb gfsvds where fjv is induced by f. definition let vi and be measures on the measurable spaces fd and f respectively. let f be a measurable space such that s sl x and fi x e f whenever fi e fi and e f v is called the product measure of vi and on f iffor fi e and e x vi the product of more than two measures can be defined similarly. denseness results theorem a. theorem. let h be a measurable function on the product space f. then is hu vvdu v is hu vldu is hu v v assuming that one of the three integrals is finite. denseness results lemma a.t. and hart let fl be a probability measure on nd and define its support set by a supportfl for all r flsxr o then fla proof. by the definition of a a c flsxrj for some rx o. let q denote the set of vectors in nd with rational components any countable dense set. then for each x e ac there is a yx e q with ilx yxlls this implies c sxrx therefore flsyxrx and c c u a xeac the right-hand side is a union of countably many sets of zero measure and therefore fla definition let f v be a measure space. for a fixed number p l p denotes the set of all measurable functions satisfying f i f i p d v theorem a.s. for every probability measure v on n d the set of tions with bounded support is dense in l p in other words for every e and f e l p there is a continuous function with bounded support gel p such that the following theorem is a rich source of denseness results appendix theorem theorem. let f be afamily of real-valued continuous functions on a closed bounded subset b of rd. assume that f is an algebra that is for any ii h e f and a b e r we have ali bh e f and fl h e f. assume furthermore that if x y then there is an f e f such that f f and that for each x e b there exists an f e f with f o. then for every e and continuous function g b r there exists an f e f such that sup igx xeb e. the following two theorems concern differentiation of integrals. good general references are whee den and zygmund and de guzman theorem a.io. lebesgue density theorem. let f be a density on rd. let be a sequence of closed cubes centered at x and contracting to x. then hm k--oo fqkx ifx fyldy aqkx at almost all x where a denotes the lebesgue measure. note that this implies at almost all x. corollary a.i. let a be a collection of subsets of so with the property that for all a e a aa casol for some fixed c o. then for almost all x if x r a x rea sup r-o aea ax r a i fxra fydy fi x the lebesgue density theorem also holds if is replaced by a sequence of contracting balls centered at x or indeed by any sequence of sets that satisfy x brks s qkx s x arks where s is the unit ball of rd rk and b a are fixed constants. this follows from the lebesgue density theorem. it does not hold in general when is a sequence of hyperrectangles containing x and contracting to x. for that an additional restriction is needed theorem jessen-marcinkiewicz-zygmund theorem. let f be a density on rd with f f logd-l fdx let be a sequence of hyperrectangles containing x andfor which diamqkx o. then at almost all x. probability corollary iff is a density and is as in theorema.ll then fydy fx lim inf k-oo aqkx qkx i at almost all x. to see this take g minf m for large fixed m. as f g logd-ll g by the lessen-marcinkiewicz-zygmund theorem i gydy gx lim inf k-oo aqkx qkx at almost all x. conclude by letting m along the integers. probability definition a measure space f p is called a probability space if pq q is the sample space or sure event the measurable sets are called events and the measurable functions are called random variables. if xl xn are random variables then x xn is a vector-valued random variable. definition let x be a random variable then x induces the measure fl on the borel ofr by flb p xw e bll px e b b e b. the probability measure fl is called the distribution of the random variable x. definition let x be a random variable. the expectation of x is the integral of x with respect to the distribution fl of x ex l xfldx if it exists. definition let x be a random variable. the variance of x is varx e ifex isjinite and ifex is notjinite or does not exist. definition let xl xn be random variables. they induce the measure on the borel o-algebra ofrn with the property appendix pn is called the joint distribution of the random variables x i x n. let fji be the distribution of xi n. the random variables xl xn are independent if their joint distribution fj is the product measure of fjn. the events ai an e are independent if the random variables iaj ian are independent. fubinis theorem implies the following theorem if the random variables xl xn are independent and have finite expectations then a.s inequalities theorem inequality. if the random variables x and y have finite second moments then theorem inequality. let p q e such that let x and y be random variables such that and then theorem a.is. inequality. let x be a nonnegative-valued random variable. then for each t px t s ex t theorem inequality. let x be a random variable. then for each t pix exi t s varx t theorem inequality. let t then px ex t varx varx t proof. we may assume without loss of generality that ex then for all t t et x s et xixst. inequalities thus for t from the cauchy-schwarz inequality t et xfeilxst et t t t that is and the claim follows. t px t varx theorem a.ls. inequality. if f is a real-valued convex function on afinite or infinite interval ofr and x is a random variable withfinite expectation taking its values in this interval then fex efx. theorem inequalities. let x be a real-valued random variable and let fx and gx be monotone nondecreasing real-valuedfunctions. then efxgx efxegx provided that all expectations exist and are finite. if f is monotone increasing and g is monotone decreasing then efxgx efxegx. proof. we prove the first inequality. the second follows by symmetry. let x have distribution fl. then we write efxgx efxegx f fxgx fldx f fey fldy f gx fldx f fygx f f hex ygx where hex y fx fey. by fubinis theorem the last integral equals r hex ygx ir ly hex ygx ly hex ygx appendix since hx x for all x. here idy. the second integral on the right-hand side is just thus we have efxgx efxegx hex ygx ygx hey xgy hex ygx gy xy xy y y since hey x y and by the fact that hex y and gx gy if x y. convergence of random variables definition let n be a sequence of random variables. we say that lim xn x n-oo in probability iffor each e we say that lim pixn xi e o. n-oo lim xn x with probability one almost surely n-oo if xn x p that is p lim xnw xw for a fixed number p we say that if lim xn x n-oo in l p lim e xi p o. conditional expectation theorem convergence in l p implies convergence in probability. theorem limnoo xn x with probability one if and only if lim sup ixm xi noo nsm in probability. thus convergence with probability one implies convergence in probability. theorem lemma. let an n be a sequence of events. introduce the notation i.o. lim sup an nl un am. ncx stands for often. if then lpan nl pan i.o. o. by theorems and we have theorem if for each e cx lpixn xi e nl then limncx xn x with probability one. conditional expectation if y is a random variable with finite expectation and a is an event with positive probability then the conditional expectation of y given a is defined by eyia ey ia pa the conditional probability of an event b given a is pb ia elb ia pa pa n b appendix definition let y be a random variable with finite expectation and x be a d vector-valued random variable. let f x be the a generated by x fx b e sn. the conditional expectation e y i x of y given x is a random variable with the property that for all a e fx i y dp i eyixdp. the existence and uniqueness probability one of e y i x is a consequence of the radon-nikodym theorem if we apply it to the measures such that and definition let c be an event and x be a d-dimensional vector-valued random variable. then the conditional probability of c given x is pcix x theorem let y be a random variable with finite expectation. let c be an event and let x and z be vector-valued random variables. then there is a measurable function g on nd such that eyix gx with probability one. ey eeyix pc epcix. eyix eeyix zix pcix epcix yix. ify is a function of x theneyix y. ify fx zforameasurablefunction f and x and z are independent ify x and z are independent then eyix z eyix. then eyix gx where gx efx z. the binomial distribution an integer-valued random variable x is said to be binomially distributed with parameters nand p if px k k p p n-k k n. k if ai an are independent events with pad p then x is binomial p. iai is called a bernoulli random variable with parameter p. the binomial distribution lemma let the random variable bn p be binomially distributed with rameters nand p. then and ii proof. follows from the following simple calculation k t pk p l n p ko n pn-kl pt-k k p ko k p for we have e pbnp o s e l bn p s by lemma let b be a binomial random variable with parameters nand p. then for every p andfor p p!b lj proof. the lemma follows from stirlings formula n! e e.g. feller appendix lemma and gyorfi p. for any random variable x with finite fourth moment proof. fix a o. the function lx is minimal on whenx thus x a x replace x by ixi and take expectations the lower bound considered as a function of a is maximized if we take a resubstitution yields the given inequality. lemma a.s. let b be a binomial random variable. then proof. this bound is a special case of khintchines inequality szarek haagerup and also devroye and gyorfi p. rather than proving the given inequality we will show how to apply the previous lemma to get further work the inequality indeed e and e thus n i in v e b i lemma let b be a binomial p random variable with p thenfor p k np pb k p k np where n is normal the multinomial distribution the hypergeometric distribution let n b and n be positive integers with n n and n b. a random variable x taking values on the integers b is hypergeometric with parameters n b and n if k b. x models the number of blue balls in a sample of n balls drawn without replacement from an urn containing b blue and n b red balls. theorem let the set a consist of n numbers aj an. let zi zn denote a random sample taken without replacement from a where n n. denote then for any e we have specifically if x is hypergeometrically distributed with parameters n b and n then for more inequalities of this type see hoeffding and serfling the multinomial distribution a vector nk of integer-valued random variables is multinomially tributed with parameters pi pk if if ll i j k otherwise. i j lemma the moment-generating function of a multinomial pi pk vector is appendix a.ii the exponential and gamma distributions a nonnegative random variable has exponential distribution with parameter if it has a density fx ax x o. a nonnegative-valued random variable has the gamma distribution with parameters a b if it has density the sum of n i.i.d. exponential random variables has gamma distribution with parameters nand the multivariate normal distribution a d-dimensional random variable x xed has the multivariate mal distribution if it has a density where mend is a positive definite symmetric d x d matrix with entries and dete denotes the determinant of then ex m and for all i j d is called the covariance matrix of x. notation indicator of an event a. indicator function of a set b. i a i b ixeb iai cardinality of a finite set a. a c complement of a set a. alb symmetric difference of sets a b. jog composition of functions j g. log natural logarithm e. lx j integer part of the real number x. i x l upper integer part of the real number x. x z xi components of the d-dimensional column vector x. ilx ii jlfl l of x e nd. x e rd observation vector-valued random variable. y e i dn yl yn training data sequence of i.i.d. pairs that are independent of y and have the same distribution as that of y if x and z have the same distribution. label binary random variable. py iix x py x a posteriori probabilities. p py i p py o class probabilities. rd x x l r l classification g rd i bayes decision function. rd i gn functions. the short notation gnx gnx dn is also used. l pgx y bayes risk the error probability of the bayes decision. notation ln lgn pgjx dn yidn error probability of a classification in i yi empirical error probability of a classifier function gn ixiea empirical measure corresponding to xl x n. a lebesgue measure on nd. i density of x radon-nikodym derivative of with respect to a it px e a probability measure of x. exists. conditional densities of x given y and y respectively they exist. p partition of nd. xklx xk k-th nearest neighbor of x among xl x n k nd n kernel function. h hn smoothing factor for a kernel rule. khx hkxl h scaled kernel function. tm ynl ynm testing data sequence of i.i.d. pairs that are independent of y and d n and have the same distribution as that of y. a class of sets. c cn classes of classification functions. sea n n-th shatter coefficient of the class of sets a. va vapnik-chervonenkis dimension of the class of sets a. sc n n-th shatter coefficient of the class of classifiers c. vc vapnik-chervonenkis dimension of the class of classifiers c. sxr e n d ii y x ii r closed euclidean ball in nd centered at x e n d with radius r o. references abou-jaoude s. conditions necessaires et suffisantes de convergence en abilite de lhistogramme pour une densite. annales de l institut henri poincare abou-jaoude s. la convergence li et loo de lestimateur de la partition aleatoire pour une densite. annales de linstitut henri poincare abou-jaoude s. sur une condition necessaire et suffisante de li-convergence presque complete de l estimateur de la partition fixe pour une densite. comptes rendus de l academie des sciences de paris aitchison j. and aitken c. multivariate binary discrimination by the kernel method. aizerman m. braverman e. and rozonoer l. the method of potential functions for the problem of restoring the characteristic of a function converter from randomly observed points. automation and remote control aizerman m. braverman e. and rozonoer l. the probability problem of pattern recognition learning and the method of potential functions. automation and remote aizerman m. braverman e. and rozonoer l. theoretical foundations of the tential function method in pattern recognition learning. automation and remote control aizerman m. braverman e. and rozonoer l. extrapolative problems in matic control and the method of potential functions. american mathematical society translations akaike h. an approximation to the density function. annals of the institute of statistical mathematics akaike h. a new look at the statistical model identification. ieee transactions on automatic control references alexander k. probability inequalities for empirical processes and a law of the iterated logarithm. annals of probability anderson a. and fu k. design and development of a linear binary tree classifier for leukocytes. technical report purdue university lafayette in. anderson j. logistic discrimination. in handbook of statistics krishnaiah p. and kanal l. editors volume pages north-holland amsterdam. anderson m. and benning r. a distribution-free discrimination procedure based on clustering. ieee transactions on information theory anderson t. an introduction to multivariate statistical analysis. john wiley new york. anderson t. some nonparametric multivariate procedures based on statistically equivalent blocks. in multivariate analysis krishnaiah p. editor pages academic press new york. angluin d. and valiant l. fast probabilistic algorithms for hamiltonian circuits and matchings. journal of computing system science anthony m. and holden s. on the power of polynomial discriminators and radial basis function networks. in proceedings of the sixth annual acm conference on putational learning theory pages association for computing machinery new york. anthony m. and shawe-taylor j. a result ofvapnik with applications. technical report university of london surrey. argentiero p. chin r. and beaudet p. an automated approach to the design of cision tree classifiers. ieee transactions on pattern analysis and machine intelligence arkadjew a. and braverman e. zeichenerkennung und maschinelles lernen. old enburg verlag miinchen wien. ash r. real analysis and probability. academic press new york. assouad p. densite et dimension. annales de linstitut fourier assouad p. deux remarques sur i comptes rendus de lacademie des sciences de paris azuma k. weighted sums of certain dependent random variables. tohoku matical journal bahadur r. a representation of the joint distribution of responses to n dichotomous items. in studies in item analysis and prediction solomon h. editor pages stanford university press stanford ca. bailey t. and jain a. a note on distance-weighted k-nearest neighbor rules. ieee transactions on systems man and cybernetics barron a. logically smooth density estimation. technical report tr ment of statistics stanford university stanford ca. barron a. statistical properties of artificial neural networks. in proceedings of the conference on decision and control pages tampa fl. barron a. complexity regularization with application to artificial neural networks. in nonparametric functional estimation and related topics roussas g. editor pages nato asi series kluwer academic publishers dordrecht. barron a. universal approximation bounds for superpositions of a sigmoidal tion. ieee transactions on information theory barron a. approximation and estimation bounds for artificial neural networks. machine learning references barron a. and barron r. statistical learning networks a unifying view. in ceedings of the symposium on the interface computing science and statistics wegman e. gantz d. and miller j. editors pages ams alexandria va. barron a. and cover t. minimum complexity density estimation. ieee tions on information theory barron a. gyorfi l. and van der meulen e. distribution estimation consistent in total variation and in two types of information divergence. ieee transactions on information theory barron r. learning networks improve computer-aided prediction and control. puter design bartlett p. lower bounds on the vapnik-chervonenkis dimension of multi-layer threshold networks. in proceedings of the sixth annual acm conference on tional learning theory pages association for computing machinery new york. bashkirov braverman e. and muchnik i. potential function algorithms for pattern recognition learning machines. automation and remote control baum e. on the capabilities of multilayer perceptrons. journal of complexity baum e. and haussler d. what size net gives valid generalization? neural putation beakley g. and tuteur f. distribution-free pattern verification using statistically equivalent blocks. ieee transactions on computers beck j. the exponential rate of convergence of error for kn-nn nonparametric regression and decision. problems of control and information theory becker p. recognition of patterns. polyteknisk forlag copenhagen. ben-bassat m. use of distance measures information measures and error bounds in feature evaluation. in handbook of statistics krishnaiah p. and kanal l. editors volume pages north-holland amsterdam. benedek g. and itai a. learnability by fixed distributions. in computational learning theory proceedings of the workshop pages morgan kaufman san mateo ca. benedek g. and itai a. nonuniform learnability. journal of computer and systems sciences bennett g. probability inequalities for the sum of independent random variables. journal of the american statistical association beran r. minimum hellinger distance estimates for parametric models. annals of statistics beran r. comments on new theoretical and algorithmical basis for estimation identification and control by p. kovanic. automatica bernstein s. the theory of probabilities. gastehizdat publishing house moscow. bhattacharya p. and mack y. weak convergence of k-nn density and regression estimators with varying k and applications. annals of statistics bhattacharyya a. on a measure of divergence between two multinomial tions. sankhya series a bickel p. and breiman l. sums of functions of nearest neighbor distances moment bounds limit theorems and a goodness of fit test. annals of probability birge l. approximation dans les espaces metriques et theorie de lestimation. zeitschrijt for wahrscheinlichkeitstheorie und verwandte gebiete references birge l. on estimating a density using hellinger distance and some other strange facts. probability theory and related fields blumer a. ehrenfeucht a. haussler d. and warmuth m. learnability and the vapnik-chervonenkis dimension. journal of the acm braverman e. the method of potential functions. automation and remote control braverman e. and pyatniskii e. estimation of the rate of convergence of algorithms based on the potential function method. automation and remote control breiman l. friedman j. olshen r. and stone c. classification and regression trees. wadsworth international belmont ca. breiman l. meisel w. and purcell e. variable kernel estimates of multivariate densities. technometrics brent r. fast training algorithms for multilayer neural nets. ieee transactions on neural networks broder a. strategies for efficient incremental nearest neighbor search. pattern recognition broornhead d. and lowe d. multivariable functional interpolation and adaptive networks. complex systems buescher k. and kumar p. learning by canonical smooth estimation part i simultaneous estimation. ieee transactions on automatic control buescher k. and kumar p. learning by canonical smooth estimation part ii learning and choice of model complexity. ieee transactions on automatic control burbea j. the convexity with respect to gaussian distributions of divergences of order ex. utilitas mathematica burbea j. and rao c. on the convexity of some divergence measures based on entropy functions. ieee transactions on information theory burshtein d. della pietra v. kanevsky d. and nadas a. minimum impurity partitions. annals of statistics cacoullos t. estimation of a multivariate density. annals of the institute of tical mathematics carnal h. die konvexe hiille von n rotationssymmetrisch verteilten punkten. zeitschrijt for wahrscheinlichkeitstheorie und verwandte gebiete casey r. and nagy g. decision tree design using a probabilistic model. ieee transactions on information theory cencov n. evaluation of an unknown distribution density from observations. soviet math. doklady chang c. finding prototypes for nearest neighbor classifiers. ieee transactions on computers chang c. dynamic programming as applied to feature selection in pattern nition systems. ieee transactions on systems man and cybernetics chen t. chen h. and liu r. a constructive proof and an extension of cybenkos approximation theorem. in proceedings of the symposium of the interface puting science and statistics pages american statistical association dria va. chen x. and zhao l. almost sure l i-norm convergence for data-based histogram density estimates. journal of multivariate analysis references chen z. and fu k. nonparametric bayes risk estimation for pattern classification. in proceedings of the ieee conference on systems man and cybernetics. boston ma. chernoff h. a measure of asymptotic efficiency of tests of a hypothesis based on the sum of observations. annals of mathematical statistics chernoff h. a bound on the classification error for discriminating between lations with specified means and variances. in studi di probabilita statistica e ricerca operativa in onare di giuseppe pompilj pages oderisi gubbio. chou p. optimal partitioning for classification and regression trees. ieee actions on pattern analysis and machine intelligence chou w. and chen y. a new fast algorithm for effective training of neural fiers. pattern recognition chow c. statistical independence and threshold functions. ieee transactions on computers chow c. on optimum recognition error and rejection tradeoff. ieee transactions on information theory chow y. and teicher h. probability theory independence interchangeability martingales. springer-verlag new york. ciampi a. generalized regression trees. computational statistics and data ysis collomb g. estimation de la regression par la methode des k points les plus proches proprietes de convergence ponctuelle. comptes rendus de l academie des sciences de paris collomb g. estimation de la regression par la methode des k points les plus proches avec noyau. lecture notes in mathematics springer-verlag berlin. collomb g. estimation non parametrique de la regression revue bibliographique. international statistical review conway j. and sloane n. sphere-packings lattices and groups. springer-verlag berlin. coomans d. and broeckaert i. potential pattern recognition in chemical and medical decision making. research studies press letchworth hertfordshire england. cormen t. leiserson c. and rivest r. introduction to algorithms. mit press boston ma. cover t. geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. ieee transactions on electronic computers cover t. estimation by the nearest neighbor rule. ieee transactions on tion theory cover t. rates of convergence for nearest neighbor procedures. in proceedings of the hawaii international conference on systems sciences pages honolulu. cover t. learning in pattern recognition. in methodologies of pattern recognition watanabe s. editor pages academic press new york. cover t. the best two independent measurements are not the two best. ieee transactions on systems man and cybernetics cover t. and hart p. nearest neighbor pattern classification. ieee transactions on information theory cover t. and thomas j. elements of information theory. john wiley new york. cover t. and van campenhout j. on the possible orderings in the measurement selection problem. ieee transactions on systems man and cybernetics references cover t. and wagner t. topics in statistical pattern recognition. communication and cybernetics cramer h. and wold h. some theorems on distribution functions. journal of the london mathematical society csibi s. simple and compound processes in iterative machine learning. technical report cism summer course udine italy. csibi s. using indicators as a base for estimating optimal decision functions. in colloquia mathematica societatis janos botyai topics in information theory pages keszthely hungary. csiszar i. information-type measures of difference of probability distributions and indirect observations. studia scientiarium mathematicarum hungarica csiszar i. generalized entropy and quantization problems. in transactions of the sixth prague conference on information theory statistical decision functions dom processes pages academia prague. csiszar i. and korner j. information theory coding theoremsfor discrete oryless systems. academic press new york. cybenko g. approximations by superpositions of sigmoidal functions. math. trol signals systems darken c. donahue m. gurvits l. and sontag e. rate of approximation results motivated by robust neural network learning. in proceedings of the sixth acm shop on computational learning theory pages association for computing machinery new york. das gupta s. nonparametric classification rules. sankhya series a das gupta s. and lin h. nearest neighbor rules of statistical classification based on ranks. sankhya series a dasarathy b. nearest neighbor pattern classification techniques. ieee computer society press los alamitos ca. day n. and kerridge d. a general maximum likelihood discriminant. biometrics de guzman m. differentiation of integrals in rn. lecture notes in mathematics springer-verlag berlin. deheuvels p. estimation nonparametrique de la densite par histogrammes alises. publications de linstitut de statistique de l universite de paris devijver p. a note on ties in voting with the k-nn rule. pattern recognition devijver p. new error bounds with the nearest neighbor rule. ieee transactions on information theory devijver p. an overview of asymptotic properties of nearest neighbor rules. in pattern recognition in practice gelsema e. and kanal l. editors pages elsevier science publishers amsterdam. devijver p. and kittler j. on the edited nearest neighbor rule. in proceedings of the fifth international conference on pattern recognition pages pattern recognition society los alamitos ca. devijver p. and kittler j. pattern recognition a statistical approach. hall englewood cliffs nj. devroye l. a universal k-nearest neighbor procedure in discrimination. in ceedings of the ieee computer society conference on pattern recognition and image processing pages ieee computer society long beach ca. references devroye l. on the almost everywhere convergence of nonparametric regression function estimates. annals of statistics devroye l. b. on the asymptotic probability of error in nonparametric tion. annals of statistics devroye l. c. on the inequality of cover and hart in nearest neighbor discrimination. ieee transactions on pattern analysis and machine intelligence devroye l. bounds for the uniform deviation of empirical measures. journal of multivariate analysis devroye l. necessary and sufficient conditions for the almost everywhere vergence of nearest neighbor regression function estimates. zeitschrijt for lichkeitstheorie und verwandte gebiete devroye l. the equivalence of weak strong and complete convergence in for kernel density estimates. annals of statistics devroye l. a course in density estimation. birkhauser boston ma. devroye l. applications of the theory of records in the study of random trees. acta informatica devroye l. automatic pattern recognition a study of the probability of error. ieee transactions on pattern analysis and machine intelligence devroye l. the expected size of some graphs in computational geometry. puters and mathematics with applications devroye l. the kernel estimate is relatively stable. probability theory and related fields devroye l. a. exponential inequalities in nonparametric estimation. in n ric functional estimation and related topics roussas g. editor pages nato asi series kluwer academic publishers dordrecht. devroye l. on the oscillation of the expected number of points on a random convex hull. statistics and probability letters devroye l. and gyorfi l. distribution-free exponential bound on the l error of partitioning estimates of a regression function. in proceedings of the fourth pannonian symposium on mathematical statistics konecny e j. and wertz w. editors pages akademiai budapest hungary. devroye l. and gyorfi l. nonparametric density estimation the view. john wiley new york. devroye l. and gyorfi l. no empirical probability measure can converge in the total variation sense for all distributions. annals of statistics devroye l. gyorfi l. krzyzak a. and lugosi g. on the strong universal tency of nearest neighbor regression function estimates. annals of statistics devroye l. and krzyzak a. an equivalence theorem for convergence of the kernel regression estimate. journal of statistical planning and inference devroye l. and laforest l. an analysis of random d-dimensional quadtrees. siam journal on computing devroye l. and lugosi g. lower bounds in pattern recognition and learning. pattern recognition devroye l. and machell e data structures in kernel density estimation. ieee transactions on pattern analysis and machine intelligence devroye l. and wagner t. a distribution-free performance bound in error mation. ieee transactions information theory references devroye l. and wagner t. nonparametric discrimination and density estimation. technical report electronics research center university of texas. devroye l. and wagner t. distribution-free inequalities for the deleted and out error estimates. ieee transactions on information theory devroye l. and wagner t. distribution-free performance bounds for potential function rules. ieee transactions on information theory devroye l. and wagner t. distribution-free performance bounds with the stitution error estimate. ieee transactions on information theory devroye l. and wagner t. distribution-free consistency results in nonparametric discrimination and regression function estimation. annals of statistics devroye l. and wagner t. on the ll convergence ofkemel estimators of reg sion functions with applications in discrimination. zeitschriji for theorie und verwandte gebiete devroye l. and wagner t. nearest neighbor methods in discrimination. in book of statistics krishnaiah p. and kanal l. editors volume pages north holland amsterdam. devroye l. and wise g. consistency of a recursive nearest neighbor regression function estimate. journal of multivariate analysis diaconis p. and shahshahani m. on nonlinear functions of linear combinations. siam journal on scientific and statistical computing do-tu h. and installe m. on adaptive solution of piecewise linear tion problem-application to modeling and identification. in milwaukee symposium on automatic computation and control pages milwaukee wi. duda r. and hart p. pattern classification and scene analysis. john wiley new york. dudani s. the distance-weighted k-nearest-neighbor rule. ieee transactions on systems man and cybernetics dudley r. central limit theorems for empirical measures. annals of probability dudleyr. balls in rk do not cut all subsets of points. advances in mathematics dudley r. empirical processes. in ecole de probabilite de st. flour lecture notes in mathematics springer-verlag new york. dudley r. universal donsker classes and metric entropy. annals of probability dudley r. kulkarni s. richardson t. and zeitouni o. a metric entropy bound is not sufficient for learnability. ieee transactions on information theory durrett r. probability theory and examples. wadsworth and brookscole pacific grove ca. dvoretzky a. on stochastic approximation. in proceedings of the first berkeley symposium on mathematical statistics and probability neyman j. editor pages university of california press berkeley los angeles. dvoretzky a. kiefer j. and wolfowitz j. asymptotic minimax character of a sample distribution function and of the classical multinomial estimator. annals of ematical statistics edelsbrunner h. algorithmsfor computational geometry. springer-verlag berlin. efron b. bootstrap methods another look at the jackknife. annals of statistics references efron b. estimating the error rate of a prediction rule improvement on cross validation. journal of the american statistical association efron b. and stein c. the jackknife estimate of variance. annals of statistics ehrenfeucht a. haussler d. kearns m. and valiant l. a general lower bound on the number of examples needed for learning. information and computation elashoff j. elashoff r. and goldmann g. on the choice of variables in cation problems with dichotomous variables. biometrika fabian v. stochastic approximation. in optimizing methods in statistics rustagi j. editor pages academic press new york london. fano r. class notes for transmission of information. course mit bridge ma. farago a. linder t. and lugosi g. fast nearest neighbor search in dissimilarity spaces. ieee transactions on pattern analysis and machine intelligence farago a. and lugosi g. strong universal consistency of neural network classifiers. ieee transactions on information theory farago t. and gyorfi l. on the continuity of the error distortion function for multiple hypothesis decisions. ieee transactions on information theory feinholz l. estimation of the performance of partitioning algorithms in pattern classification. masters thesis department of mathematics mcgill university treal. feller w. an introduction to probability theory and its applications you. john wiley new york. finkel r. and bentley j. quad trees a data structure for retrieval on composite keys. acta informatica fisher r. the case of multiple measurements in taxonomic problems. annals of eugenics part ii fitzmaurice g. and hand d. a comparison of two average conditional error rate estimators. pattern recognition letters fix e. and hodges j. discriminatory analysis. nonparametric discrimination consistency properties. technical report project number usaf school of aviation medicine randolph field tx. fix e. and hodges j. discriminatory analysis small sample performance. nical report usaf school of aviation medicine randolph field tx. fix e. and hodges j. a. discriminatory analysis nonparametric discrimination sistency properties. in nearest neighbor pattern classification techniques dasarathy b. editor pages ieee computer society press los alamitos ca. fix e. and hodges j. discriminatory analysis small sample performance. in nearest neighbor pattern classification techniques dasarathy b. editor pages ieee computer society press los alamitos ca. flick t. jones l. priest r. and herman c. pattern classification using protection pursuit. pattern recognition forney g. exponential error bounds for erasure list and decision feedback schemes. ieee transactions on information theory friedman j. a recursive partitioning decision rule for nonparametric classification. ieee transactions on computers references friedman j. baskett f. and shustek l. an algorithm for finding nearest neighbor. ieee transactions on computers friedman j. bentley j. and finkel r. an algorithm for finding best matches in logarithmic expected time. acm transactions on mathematical software friedman j. and silverman b. flexible parsimonious smoothing and additive eling. technometrics friedman j. and stuetzle w. projection pursuit regression. journal of the american statistical association friedman j. stuetzle w. and schroeder a. projection pursuit density estimation. journal of the american statistical association friedman and tukey a projection pursuit algorithm for exploratory data analysis. ieee transactions on computers fritz and gyorfi l. on the minimization of classification error probability in statistical pattern recognition. problems of control and information theory fu k. min p. and li t. feature selection in pattern recognition. ieee transactions on systems science and cybernetics fuchs h. abram g. and grant e. near real-time shaded display of rigid objects. computer graphics fuchs h. kedem z. and naylor b. on visible surface generation by a priori tree structures. in proceedings siggraph pages published as computer graphics volume fukunaga k. and flick t. an optimal global nearest neighbor metric. ieee actions on pattern analysis and machine intelligence fukunaga k. and hostetler l. optimization of k-nearest neighbor density estimates. ieee transactions on information theory fukunaga k. and hummels d. bayes error estimation using parzen and k-nn procedures. ieee transactions on pattern analysis and machine intelligence fukunaga k. and kessel d. estimation of classification error. ieee transactions on computers fukunaga k. and kessel d. nonparametric bayes error estimation using sified samples. ieee transactions information theory fukunaga k. and mantock j. nonparametric data reduction. ieee transactions on pattern analysis and machine intelligence fukunaga k. and narendra p. a branch and bound algorithm for computing nearest neighbors. ieee transactions on computers funahashi k. on the approximate realization of continuous mappings by neural networks. neural networks gabor d. a universal nonlinear filter predictor and simulator which optimizes itself. proceedings of the institute of electrical engineers gabriel k. and sokal r. a new statistical approach to geographic variation analysis. systematic zoology gaenssler p. empirical processes. lecture notes-monograph series institute of mathematical statistics hayward ca. gaenssler p. and stute w. empirical processes a survey of results for independent identically distributed random variables. annals of probability gallant a. nonlinear statistical models. john wiley new york. references ganesalingam s. and mclachlan g. error rate estimation on the basis of posterior probabilities. pattern recognition garnett j. and yau s. nonparametric estimation of the bayes error offeature tors using ordered nearest neighbour sets. ieee transactions on computers gates g. the reduced nearest neighbor rule. ieee transactions on information theory gelfand s. and delp e. on tree structured classifiers. in artificial neural networks and statistical pattern recognition old and new connections sethi i. and jain a. editors pages elsevier science publishers amsterdam. gelfand s. ravishankar c. and delp e. an iterative growing and pruning gorithm for classification tree design. in proceedings of the ieee international conference on systems man and cybernetics pages ieee press piscataway nj. gelfand s. ravishankar c. and delp e. an iterative growing and pruning rithm for classification tree design. ieee transactions on pattern analysis and machine intelligence geman s. and hwang c. nonparametric maximum likelihood estimation by the method of sieves. annals of statistics gessaman m. a consistent nonparametric multivariate density estimator based on statistically equivalent blocks. annals of mathematical statistics gessaman m. and gessaman p. a comparison of some multivariate discrimination procedures. journal of the american statistical association geva s. and sitte j. adaptive nearest neighbor pattern classification. ieee actions on neural networks gine e. and zinn j. some limit theorems for empirical processes. annals of ability glick n. sample-based multinomial classification. biometrics glick n. sample-based classification procedures related to empiric distributions. ieee transactions on information theory glick n. additive estimators for probabilities of correct classification. pattern recognition goldberg p. and jerrum m. bounding the vapnik-chervonenkis dimension of concept classes parametrized by real numbers. in proceedings of the sixth acm shop on computational learning theory pages association for computing machinery new york. goldstein m. a two-group classification procedure for multivariate dichotomous responses. multivariate behavorial research goldstein m. and dillon w. discrete discriminant analysis. john wiley new york. golea m. and marchand m a growth algorithm for neural network decision trees. europhysics letters goodman r. and smyth p. decision tree design from a communication theory viewpoint. ieee transactions on information theory gordon l. and olshen r. almost surely consistent nonparametric regression from recursive partitioning schemes. ournal of multivariate analysis gordon l. and olshen r. asymptotically efficient solutions to the classification problem. annals of statistics references gordon l. and olshen r. consistent nonparametric regression from recursive partitioning schemes. journal of multivariate analysis gowda k. and krishna g. the condensed nearest neighbor rule using the concept of mutual nearest neighborhood. ieee transactions on information theory greblicki w. asymptotically optimal probabilistic algorithms for pattern tion and identification. technical report monografie prace naukowe instytutu cybernetyki technicznej politechniki wroclawsjiej no. wroclaw poland. greblicki w. asymptotically optimal pattern recognition procedures with density estimates. ieee transactions on information theory greblicki w. pattern recognition procedures with nonparametric density estimates. ieee transactions on systems man and cybernetics greblicki w. asymptotic efficiency of classifying procedures using the hermite series estimate of multivariate probability densities. ieee transactions on information theory greblicki w. krzyzak a. and pawlak m. distribution-free pointwise consistency of kernel regression estimate. annals of statistics greblicki w. and pawlak m. classification using the fourier series estimate of multivariate density functions. ieee transactions on systems man and cybernetics greblicki w. and pawlak m. a classification procedure using the multiple fourier series. information sciences greblicki w. and pawlak m. almost sure convergence of classification procedures using hermite series density estimates. pattern recognition letters greblicki w. and pawlak m. pointwise consistency of the hermite series density estimate. statistics and probability letters greblicki w. and pawlak m. necessary and sufficient conditions for bayes risk consistency of a recursive kernel classification rule. ieee transactions on information theory grenander u. abstract inference. john wiley new york. grimmett g. and stirzaker d. probability and random processes. oxford sity press oxford. guo h. and gelfand s. classification trees with neural network feature extraction. ieee transactions on neural networks gyorfi l. an upper bound of error probabilities for multihypothesis testing and its application in adaptive pattern recognition. problems of control and information theory gyorfi l. on the rate of convergence of nearest neighbor rules. ieee transactions on information theory gyorfi l. recent results on nonparametric regression estimate and multiple sification. problems of control and information theory gyorfi l. adaptive linear procedures under general conditions. ieee transactions on information theory gyorfi l. and gyorfi z. on the nonparametric estimate of a posteriori probabilities of simple statistical hypotheses. in colloquia mathematica societatis janos bolyai topics in information theory pages keszthely hungary. gyorfi l. and gyorfi z. an upper bound on the asymptotic error probability of the k-nearest neighbor rule for multiple classes. ieee transactions on information theory references gyorfi l. gyorfi z. and vajda i. bayesian decision with rejection. problems of control and information theory gyorfi l. and vajda i. upper bound on the error probability of detection in gaussian noise. problems of control and information theory haagerup u. les meilleures constantes de linegalite de khintchine. comptes rendus de academie des sciences de paris a habbema j. hermans j. and van den broek k. a stepwise discriminant analysis program using density estimation. in compstat bruckmann g. editor pages physica verlag wien. hagerup t. and riib c. a guided tour of chernoff bounds. information processing letters hall p. on nonparametric multivariate binary discrimination. biometrika hall p. on projection pursuit regression. annals of statistics hall p. and wand m. on nonparametric discrimination using density differences. biometrika hand d. discrimination and classification. john wiley chichester u.k. hand d. recent advances in error rate estimation. pattern recognition letters hardie w. and marron j. optimal bandwidth selection in nonparametric regression function estimation. annals of statistics hart p. the condensed nearest neighbor rule. ieee transactions on information theory hartigan j. clustering algorithms. john wiley new york. hartmann c. varshney p. mehrotra k. and gerberich c. application of formation theory to the construction of efficient decision trees. ieee transactions on information theory hashlamoun w. varshney p. and samarasooriya v. a tight upper bound on the bayesian probability of error. ieee transactions on pattern analysis and machine intelligence hastie t. and tibshirani r. generalized additive models. chapman and hall london u.k. haussler d. sphere packing numbers for subsets of the boolean n-cube with bounded v apnik dimension. technical report computer research oratory university of california santa cruz. haussler d. decision theoretic generalizations of the pac model for neural net and other learning applications. information and computation haussler d. littlestone n. and warmuth m. predicting functions from randomly drawn points. in proceedings of the ieee symposium on the foundations of computer science pages ieee computer society press los alamitos ca. hecht-nielsen r. kolmogorovs mapping network existence theorem. in ieee first international conference on neural networks volume pages ieee piscataway nj. hellman m. the nearest neighbor classification rule with a reject option. ieee transactions on systems man and cybernetics henrichon e. and fu k. a nonparametric partitioning procedure for pattern sification.ieee transactions on computers references hertz j. krogh a. and palmer r. introduction to the theory of neural tation. addison-wesley redwood city ca. hills m. allocation rules and their error rates. journal of the royal statistical society hjort n. contribution to the discussion of a paper by p. diaconis and freeman. annals of statistics hjort n. notes on the theory of statistical symbol recognition. technical report norwegian computing centre oslo. hoeffding w. probability inequalities for sums of bounded random variables. nal of the american statistical association holmstrom l. and klemeui j. asymptotic bounds for the expected error of a multivariate kernel density estimator. journal of multivariate analysis hora s. and wilcox j. estimation of error rates in several-population discriminant analysis. journal of marketing research horibe y. on zero error probability of binary decisions. ieee transactions on information theory home b. and hush d. on the optimality of the sigmoid perceptron. in international joint conference on neural networks volume pages lawrence erlbaum associates hillsdale nj. hornik k. approximation capabilities of multilayer feedforward networks. neural networks hornik k. some new results on neural network approximation. neural networks hornik k. stinchcombe m. and white h. multi-layer feedforward networks are universal approximators. neural networks huber p. projection pursuit. annals of statistics hudimoto h. a note on the probability of the correct classification when the distributions are not specified. annals of the institute of statistical mathematics ito t. note on a class of statistical recognition functions. ieee transactions on computers ito t. approximate error bounds in pattern recognition. in machine intelligence meltzer b. and mitchie d. editors pages edinburgh university press burgh. ivakhnenko a. the group method of data handling-a rival of the method of stochastic approximation. soviet automatic control ivakhnenko a. polynomial theory of complex systems. ieee transactions on systems man and cybernetics ivakhnenko a. konovalenko v. tulupchuk y. and tymchenko i. the group method of data handling in pattern recognition and decision problems. soviet automatic control ivakhnenko a. petrache g. and krasytskyy m. a gmdh algorithm with random selection of pairs. soviet automatic control jain a. dubes r. and chen c. bootstrap techniques for error estimation. ieee transactions on pattern analysis and machine intelligence jeffreys h. theory of probability. clarendon press oxford. johnson d. and preparata f. the densest hemisphere problem. theoretical puter science references kurkova v. kolmogorovs theorem and multilayer neural networks. neural works kailath t. the divergence and bhattacharyya distance measures in signal detection. ieee transactions on communication technology kanal l. patterns in pattern recognition. ieee transactions on information theory kaplan m. the uses of spatial coherence in ray tracing. siggraph course notes karlin a. total positivity volume stanford university press stanford ca. karp r. probabilistic analysis of algorithms. class notes university of california berkeley. karpinski m. and macintyre a. quadratic bounds for vc dimension of sigmoidal neural networks. submitted to the acm symposium on theory of computing. kazmierczak h. and steinbuch k. adaptive systems in pattern recognition. ieee transactions on electronic computers kemp r. fundamentals of the average case analysis of particular algorithms. b.g. teubner stuttgart. kemperman j. on the optimum rate of transmitting information. in probability and information theory pages springer lecture notes in mathematics verlag berlin. kiefer j. and wolfowitz j. stochastic estimation of the maximum of a regression function. annals of mathematical statistics kim b. and park s. a fast k-nearest neighbor finding algorithm based on the ordered partition. ieee transactions on pattern analysis and machine intelligence kittler j. and devijver p. an efficient estimator of pattern recognition system error probability. pattern recognition knoke j. the robust estimation of classification error rates. computers and ematics with applications kohonen t. self-organization and associative memory. springer-verlag berlin. kohonen t. statistical pattern recognition revisited. in advanced neural ers eckmiller r. editor pages north-holland amsterdam. kolmogorov a. on the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition. doklady akademii nauk ussr kolmogorov a. and tikhomirov v. e-entropy and e-capacity of sets in function spaces. translations of the american mathematical society koutsougeras c. and papachristou c. training of a neural network for pattern classification based on an entropy measure. in proceedings of ieee international ference on neural networks volume pages ieee san diego section san diego ca. kraaijveld m. and duin r. generalization capabilities of minimal kernel-based methods. in international joint conference on neural networks volume pages piscataway nj. kronmal r. and tarter m. the estimation of probability densities and cumulatives by fourier series methods. journal of the american statistical association krzanowski w. a comparison between two distance-based discriminant principles. journal of classification references krzyzak a. classification procedures using multivariate variable kernel density estimate. pattern recognition letters krzyzak a. the rates of convergence of kernel regression estimates and tion rules. ieee transactions on information theory krzyzak a. on exponential bounds on the bayes risk of the kernel classification rule. ieee transactions on information theory krzyzak a. linder t. and lugosi g. nonparametric estimation and classification using radial basis function nets and empirical risk minimization. ieee transactions on neural networks. to appear. krzyzak a. and pawlak m. universal consistency results for wolverton-wagner regression function estimate with application in discrimination. problems of control and information theory krzyzak a. and pawlak m. almost everywhere convergence of recursive kernel regression function estimates. ieee transactions on information theory krzyzak a. and pawlak m. distribution-free consistency of a nonparametric kernel regression estimate and classification. ieee transactions on information theory kulkarni s. problems of computational and information complexity in machine vision and learning. phd thesis department of electrical engineering and computer science mit cambridge ma. kullback s. a lower bound for discrimination information in terms of variation. ieee transactions on information theory kullback s. and leibler a. on information and sufficiency. annals of mathematical statistics kushner h. approximation and weak convergence methods for random processes with applications to stochastic systems theory. mit press cambridge ma. lachenbruch p. an almost unbiased method of obtaining confidence intervals for the probability of misclassification in discriminant analysis. biometrics lachenbruch p. and mickey m. estimation of error rates in discriminant analysis. technometrics lecam l. on the assumptions used to prove asymptotic normality of maximum likelihood estimates. annals of mathematical statistics lecam l. convergence of estimates under dimensionality restrictions. annals of statistics li x. and dubes r. tree classifier design with a permutation statistic. pattern recognition lin y. and fu k. automatic classification of cervical cells using a binary tree classifier. pattern recognition linde y. buzo a. and gray r. an algorithm for vector quantizer design. ieee transactions on communications linder t. lugosi g. and zeger k. rates of convergence in the source coding rem empirical quantizer design and universal lossy source coding. ieee transactions on information theory lissack t. and fu k. error estimation in pattern recognition via za distance between posterior density functions. ieee transactions on information theory ljung l. pflug g. and walk h. stochastic approximation and optimization of random systems. birkhauser basel boston berlin. references lloyd s. least squares quantization in pcm. ieee transactions on information theory loftsgaarden d. and quesenberry c. a nonparametric estimate of a multivariate density function. annals of mathematical statistics. logan b. the uncertainty principle in reconstructing functions from projections. duke mathematical journal logan b. and shepp l. optimal reconstruction of a function from its projections. duke mathematical journal loh w. and vanichsetakul n. tree-structured classification via generalized criminant analysis. journal of the american statistical association loizou g. and maybank s. the nearest neighbor and the bayes error rates. ieee transactions on pattern analysis and machine intelligence lorentz g. the thirteenth problem of hilbert. in proceedings of symposia in pure mathematics volume pages providence ri. lugosi g. learning with an unreliable teacher. pattern recognition lugosi g. and nobel a. consistency of data-driven histogram methods for density estimation and classification. annals of statistics lugosi g. and pawlak m. on the posterior-probability estimate of the error rate of nonparametric classification rules. ieee transactions on information theory lugosi g. and zeger k. nonparametric estimation via empirical risk minimization. ieee transactions on information theory lugosi g. and zeger k. concept learning using complexity regularization. ieee transactions on information theory lukacs e. and laha r. applications of characteristic functions in probability theory. griffin london. lunts a. and brailovsky v. evaluation of attributes obtained in statistical decision rules. engineering cybernetics maass w. bounds for the computational power and learning complexity of log neural nets. in proceedings of the annual acm symposium on the theory of computing pages association of computing machinery new york. maass w. neural nets with superlinear vc-dimension. neural computation macintyre a. and sontag e. finiteness results for sigmoidal networks. in proceedings of the annual acm symposium on the theory of computing pages association of computing machinery new york. mack y. local properties of k neighbor regression estimates. siam journal on algebraic and discrete methods mahalanobis p. on the generalized distance in statistics. proceedings of the national institute of sciences of india mahalanobis p. a method offractile graphical analysis. sankhya series a marron j. optimal rates of convergence to bayes risk in nonparametric nation. annals of statistics massart p. vitesse de convergence dans le theoreme de la limite centrale pour le processus empirique. phd thesis universite paris-sud orsay france. massart p. the tight constant in the dvoretzky-kiefer-wolfowitz inequality. annals of probability references mathai a. and rathie p. basic concepts in information theory and statistics. wiley eastern ltd. new delhi. matloff n. and pruitt r. the asymptotic distribution of an estimator of the bayes error rate. pattern recognition letters matula d. and sokal r. properties of gabriel graphs relevant to geographic tion research and the clustering of points in the plane. geographical analysis matushita k. decision rule based on distance for the classification problem. annals of the institute of statistical mathematics matushita k. discrimination and the affinity of distributions. in discriminant ysis and applications cacoullos t. editor pages academic press new york. max j. quantizing for minimum distortion. ieee transactions on information theory mcdiarmid c. on the method of bounded differences. in surveys in combinatorics pages cambridge university press cambridge. mclachlan g. the bias of the apparent error rate in discriminant analysis. biometrika mclachlan g. discriminant analysis and statistical pattern recognition. john wiley new york. meisel w. potential functions in mathematical pattern recognition. ieee tions on computers meisel w. computer oriented approaches to pattern recognition. academic press new york. meisel w. parsimony in neural networks. in international conference on neural networks pages lawrence erlbaum associates hillsdale nj. meisel w. and michalopoulos d. a partitioning algorithm with application in pattern classification and the optimization of decision tree. ieee transactions on puters michel-briand c. and milhaud x. asymptotic behavior of the aid method. nical report universite montpellier montpellier. mielniczuk j. and tyrcha j. consistency of multilayer perceptron regression mators. neural networks to appear. minsky m. steps towards artificial intelligence. in proceedings of the ire ume pages minsky m. and papert s. perceptrons an introduction to computational geometry. mit press cambridge ma. mitchell a. and krzanowski w. the mahalanobis distance and elliptic tions. biometrika mizoguchi r. kizawa m. and shimura m. piecewise linear discriminant tions in pattern recognition. systems-computers-controls moody j. and darken j. fast learning in networks of locally-tuned processing units. neural computation moore d. whitsitt s. and landgrebe d. variance comparisons for unbiased estimators of probability of correct classification. ieee transactions on information theory morgan j. and sonquist j. problems in the analysis of survey data and a proposal. journal of the american statistical association references mui and fu k. automated classification of nucleated blood cells using a binary tree classifier. ieee transactions on pattern analysis and machine intelligence myles and hand d. the multi-class metric problem in nearest neighbour crimination rules. pattern recognition nadaraya e. on estimating regression. theory of probability and its applications nadaraya e. remarks on nonparametric estimates for density functions and sion curves. theory of probability and its applications narendra p. and fukunaga k. a branch and bound algorithm for feature subset selection. ieee transactions on computers natarajan b. machine learning a theoretical approach. morgan kaufmann san mateo ca. nevelson m. and khasminskii r. stochastic approximation and recursive timation. translations of mathematical monographs vol. american mathematical society providence ri. niemann h. and goppert r. an efficient branch-and-bound nearest neighbour classifier. pattern recognition letters nilsson n. learning machines foundations of trainable pattern classifying tems. mcgraw-hill new york. nishizeki t. and chiba n. planar graphs theory and algorithms. north holland amsterdam. nobel a. histogram regression estimates using data-dependent partitions. cal report beckman institute university of illinois urbana-champaign. nobel a. on uniform laws of averages. phd thesis department of statistics stanford university stanford ca. nolan d. and pollard d. u-processes rates of convergence. annals of statistics okamoto m. some inequalities relating to the partial sum of binomial probabilities. annals of the institute of statistical mathematics olshen r. comments on a paper by c.l. stone. annals of statistics ott and kronmal r. some classification procedures for multivariate binary data using orthogonal functions. journal of the american statistical association papadimitriou c. and bentley a worst-case analysis of nearest neighbor ing by projection. in automata languages and programming pages lecture notes in computer science springer-verlag berlin. park and sandberg i. universal approximation using radial-basis-function works. neural computation park and sandberg i. approximation and radial-basis-function networks. neural computation park y. and sklansky automated design of linear tree classifiers. pattern nition parthasarathy k. and bhattacharya p. some limit theorems in regression theory. sankhya series a parzen e. on the estimation of a probability density function and the mode. annals of mathematical statistics references patrick e. distribution-free minimum conditional risk learning systems. technical report purdue university lafayette in. patrick e. and fischer f. a generalized k-nearest neighbor rule. information and patrick e. and fisher f. introduction to the performance of distribution-free conditional risk learning systems. technical report purdue university lafayette in. pawlak m. on the asymptotic properties of smoothed estimators of the classification error rate. pattern recognition payne h. and meisel w. an algorithm for constructing optimal binary decision trees. ieee transactions on computers pearl j. capacity and error estimates for boolean classifiers with limited complexity. ieee transactions on pattern analysis and machine intelligence penrod c. and wagner t. another look at the edited nearest neighbor rule. ieee transactions on systems man and cybernetics peterson d. some convergence properties of a nearest neighbor decision rule. ieee transactions on information theory petrov v. sums of independent random variables. springer-verlag berlin. pippenger n. information theory and the complexity of boolean functions. ematical systems theory poggio t. and girosi f. a theory of networks for approximation and learning. proceedings of the ieee pollard d. strong consistency of k-means clustering. annals of statistics pollard d. quantization and the method of k-means. ieee transactions on mation theory pollard d. convergence of stochastic processes. springer-verlag new york. pollard d. rates of uniform almost sure convergence for empirical processes dexed by unbounded classes of functions. manuscript. pollard d. empirical processes theory and applications. nsf-cbms regional conference series in probability and statistics institute of mathematical statistics hayward ca. powell m. radial basis functions for multivariable interpolation a review. rithms for approximation. clarendon press oxford. prep arata f. and shamos m. computational geometry-an introduction. verlag new york. psaltis d. snapp r. and venkatesh s. on the finite sample performance of the nearest neighbor classifier. ieee transactions on information theory qing-yun s. and fu k. a method for the design of binary tree classifiers. pattern recognition quesenberry c. and gessaman m. nonparametric discrimination using tolerance regions. annals of mathematical statistics quinlan j. programs for machine learning. morgan kaufmann san mateo. rabiner l. levinson s. rosenberg a. and wilson j. nition of isolated words using clustering techniques. ieee transactions on acoustics speech and signal processing references rao r. relations between weak and uniform convergence of measures with cations. annals of mathematical statistics raudys s. on the amount of a priori information in designing the classification algorithm. technical cybernetics raudys s. on dimensionality learning sample size and complexity of classification algorithms. in proceedings of the international conference on pattern recognition pages ieee computer society long beach ca. raudys s. and pikelis v. on dimensionality sample size classification error and complexity of classification algorithm in pattern recognition. ieee transactions on pattern analysis and machine intelligence raudys s. and pikelis v. collective selection of the best version of a pattern recognition system. pattern recognition letters rejto l. and revesz p. density estimation and pattern classification. problems of control and information theory renyi a. on measures of entropy and information. in proceedings of the fourth berkeley symposium pages university of california press berkeley. revesz p. robbins-monroe procedures in a hilbert space and its application in the theory of learning processes. studia scientiarium mathematicarum hungarica ripley b. statistical aspects of neural networks. in networks and chaos--statistical and probabilistic aspects barndorff-nielsen jensen j. and kendall w. editors pages chapman and hall london u.k. ripley b. neural networks and related methods for classification. journal of the royal statistical society rissanen l. a universal prior for integers and estimation by minimum description length. annals of statistics ritter g. woodruff h. lowry s. and isenhour t. an algorithm for a selective nearest neighbor decision rule. ieee transactions on information theory robbins h. and monro s. a stochastic approximation method. annals of matical statistics rogers w. and wagner t. a finite sample distribution-free performance bound for local discrimination rules. annals of statistics rosenblatt f. principles ofneurodynamics perceptrons and the theory of brain mechanisms. spartan books washington dc. rosenblatt m. remarks on some nonparametric estimates of a density function. annals of mathematical statistics rounds e. a combined nonparametric approach to feature selection and binary decision tree design. pattern recognition royall r. a class of n onparametric estimators of a smooth regression function. phd thesis stanford university stanford ca. rumelhart d. hinton g. and williams r. learning internal representations by ror propagation. in parallel distributed processing vol. i rumelhart d. j and the pdp research group editors. mit press cambridge ma. reprinted in l.a. anderson and e. rosenfeld neurocomputing--foundations of research mit press cambridge ma. pp. ruppert d. stochastic approximation. in handbook of sequential analysis ghosh b. and sen p. editors pages marcel dekker new york. references samet h. the quadtree and related hierarchical data structures. computing surveys samet h. applications of spatial data structures. addison-wesley reading ma. samet h. the design and analysis of spatial data structures. addison-wesley reading ma. sansone g. orthogonal functions. interscience new york. sauer n. on the density of families of sets. journal of combinatorial theory series h. a useful convergence theorem for probability distributions. annals of mathematical statistics schiaffli l. gesammelte mathematische abhandlungen. birkhauser-verlag basel. schmidt w. neural pattern classifying systems. phd thesis technical university delft the netherlands. schoenberg i. on frequency functions ii variation-diminishing integral operators of the convolution type. acta scientiarium mathematicarum szeged schwartz s. estimation of probability density by an orthogonal series. annals of mathematical statistics schwemer g. and dunn o. posterior probability estimators in classification ulations. communications in statistics--simulation sebestyen g. decision-making processes in pattern recognition. macmillan new york. serfiing r. probability inequalities for the sum in sampling without replacement. annals of statistics sethi i. a fast algorithm for recognizing nearest neighbors. ieee transactions on systems man and cybernetics sethi i. entropy nets from decision trees to neural nets. proceedings of the ieee sethi i. decision tree performance enhancement using an artificial neural network interpretation. in artificial neural networks and statistical pattern recognition old and new connections sethi i. and jain a. editors pages elsevier science publishers amsterdam. sethi i. and chatterjee b. efficient decision tree design for discrete variable pattern recognition problems. pattern recognition sethi i. and sarvarayudu g. hierarchical classifier design using mutual tion. ieee transactions on pattern analysis and machine intelligence shannon c. a mathematical theory of communication. bell systems technical shawe-taylor j. sample sizes for sigmoidal neural networks. technical report department of computer science royal holloway university of london egham land. shawe-taylor j. anthony m. and biggs n.l. bounding sample size with the vapnik-chervonenkis dimension. discrete applied mathematics shiryayev a. probability. springer-verlag new york. short r. and fukunaga k. the optimal distance measure for nearest neighbor classification. ieee transactions on information theory references simon h. the vapnik-chervonenkis dimension of decision trees with bounded rank. information processing letters simon h. general lower bounds on the number of examples needed for learning probabilistic concepts. in proceedings of the sixth annual acm conference on tational learning theory pages association for computing machinery new york. sklansky j. and michelotti locally trained piecewise linear classifiers. ieee actions on pattern analysis and machine intelligence sklansky j. and wassel g. pattern classifiers and trainable machines. verlag new york. slud e. distribution inequalities for the binomial law. annals of probability specht d. generation of polynomial discriminant functions for pattern tion. ieee transactions on electronic computers specht d. series estimation of a probability density function. technometrics specht d. probabilistic neural networks and the polynomial adaline as tary techniques for classification. ieee transactions on neural networks spencer j. ten lectures on the probabilistic method. siam philadelphia pa. sprecher d. on the structure of continuous functions of several variables. actions of the american mathematical society steele j. combinatorial entropy and uniform limit laws. phd thesis stanford university stanford ca. steele j. an efron-stein inequality for nonsymmetric statistics. annals of statistics stengle g. and yukich j. some new vapnik-chervonenkis classes. annals of statistics stoffel j. a classifier design technique for discrete variable pattern recognition problems. ieee transactions on computers stoller d. univariate two-population distribution-free discrimination. journal of the american statistical association stone c. consistent nonparametric regression. annals of statistics stone c. optimal global rates of convergence for nonparametric regression. annals of statistics stone c. additive regression and other nonparametric models. annals of statistics stone m. cross-validatory choice and assessment of statistical predictions. journal of the royal statistical society stute w. asymptotic normality of nearest neighbor regression function estimates. annals of statistics sung k. and shirley p. ray tracing with the bsp tree. in graphics gems iii kirk d. editor pages academic press boston ma. swonger c. sample set condensation for a condensed nearest neighbor decision rule for pattern recognition. in frontiers of pattern recognition watanabe s. editor pages academic press new york. szarek s. on the best constants in the khintchine inequality. studia mathematica references szego g. orthogonal polynomials volume american mathematical society providence ri. talagrand m. the glivenko-cantelli problem. annals of probability talagrand m. sharper bounds for gaussian and empirical processes. annals of probability talmon j. a multiclass nonparametric partitioning algorithm. in pattern nition in practice ii gelsema e. and kanal l. editors. elsevier science publishers amsterdam. taneja on characterization of j-divergence and its generalizations. journal of combinatorics information and system sciences statistical aspects of divergence measures. journal of statistical planning and inference tarter m. and kronmal r. on multivariate density estimates based on orthogonal expansions. annals of mathematical statistics tomek a generalization of the k-nn rule. ieee transactions on systems man and cybernetics tomek two modifications of cnn. ieee transactions on systems man and cybernetics toussaint g. note on optimal selection of independent binary-valued features for pattern recognition. ieee transactions on information theory toussaint g. bibliography on estimation ofmisclassification. ieee transactions on information theory toussaint g. on the divergence between two distributions and the probability of misclassification of several decision rules. in proceedings of the second international joint conference on pattern recognition pages copenhagen. toussaint g. and donaldson r. algorithms for recognizing contour-traced printed characters. ieee transactions on computers tsypkin y. adaptation and learning in automatic systems. academic press new york. tutz g. smoothed additive estimators for non-error rates in multiple discriminant analysis. pattern recognition tutz g. an alternative choice of smoothing for kernel-based density estimates in discrete discriminant analysis. biometrika tutz g. smoothing for discrete kernels in discrimination. biometrics journal tutz g. on cross-validation for discrete kernel estimates in discrimination. munications in statistics-theory and methods ullmann j. automatic selection of reference data for use in a nearest-neighbor method of pattern classification. ieee transactions on information theory vajda i. the estimation of minimal error probability for testing finite or countable number of hypotheses. problemy peredaci informacii vajda theory of statistical inference and information. kluwer academic lishers dordrecht. valiant l. a theory of the learnable. communications of the acm van campenhout j. the arbitrary relation between probability of error and surement subset. journal of the american statistical association references van ryzin j. bayes risk consistency of classification procedures using density estimation. sankhya series a vapnik v. estimation of dependencies based on empirical data. springer-verlag new york. vapnik v. and chervonenkis a. on the uniform convergence of relative frequencies of events to their probabilities. theory of probability and its applications vapnik v. and chervonenkis a. ordered risk minimization. i. automation and remote control vapnik v. and chervonenkis a. ordered risk minimization. ii. automation and remote control vapnik v. and chervonenkis a. theory of pattern recognition. nauka moscow. russian german translation theorie der zeichenerkennung akademie verlag berlin vapnik v. and chervonenkis a. necessary and sufficient conditions for the uniform convergence of means to their expectations. theory of probability and its applications vidal e. an algorithm for finding nearest neighbors in constant average time. pattern recognition letters vilmansen t. feature evaluation with measures of probabilistic dependence. ieee transactions on computers vitushkin a. the absolute e-entropy of metric spaces. translations of the american mathematical society wagner t. convergence of the nearest neighbor rule. ieee transactions on mation theory wagner t. convergence of the edited nearest neighbor. ieee transactions on information theory wang q. and suen c. analysis and design of decision tree based on entropy reduction and its application to large character set recognition. ieee transactions on pattern analysis and machine intelligence warner h. toronto a. veasey l. and stephenson r. a mathematical approach to medical diagnosis. journal of jhe american medical association wassel g. and sklansky j. training a one-dimensional classifier to minimize the probability of error. ieee transactions on systems man and cybernetics watson g. smooth regression analysis. sankhya series a weiss s. and kulikowski c. computer systems that learn. morgan kaufmann san mateo ca. wenocur r. and dudley r. some special vapnik-chervonenkis classes. discrete mathematics wheeden r. and zygmund a. measure and integral. marcel dekker new york. white h. connectionist nonparametric regression multilayer feedforward works can learn arbitrary mappings. neural networks white h. nonparametric estimation of conditional quantiles using neural networks. in proceedings of the symposium of the interface computing science and tics pages american statistical association alexandria va. widrow b. adaptive sampled-data systems-a statistical theory of adaptation. in ire wescon convention record volume part pages references widrow b. and hoff m. adaptive switching circuits. in ire wescon convention record volume part pages reprinted in j anderson and e. rosenfeld neurocomputing foundations of research mit press cambridge ma. wilson d. asymptotic properties of nearest neighbor rules using edited data. ieee transactions on systems man and cybernetics winder r. threshold logic in artificial intelligence. in artificial intelligence pages ieee special publication wolverton c. and wagner t. asymptotically optimal discriminant functions for pattern classification. ieee transactions on systems science and cybernetics wolverton c. and wagner t. recursive estimates of probability densities. ieee transactions on systems science and cybernetics wong w. and shen x. probability inequalities for likelihood ratios and convergence rates of sieve mles. technical report department of statistics university of chicago chicago il. xu l. krzyzak a. and oja e. rival penalized competitive learning for clustering analysis rbf net and curve detection. ieee transactions on neural networks xu l. krzyzak a. and yuille a. on radial basis function nets and kernel sion approximation ability convergence rate and receptive field size. neural networks. to appear. yatracos y. rates of convergence of minimum distance estimators and mogorovs entropy. annals of statistics yau s. and lin t. on the upper bound of the probability of error of a linear pattern classifier for probabilistic pattern classes. proceedings of the ieee you k. and fu k. an approach to the design of a linear binary tree classifier. in proceedings of the symposium of machine processing of remotely sensed data pages purdue university lafayette in. yukich j. laws of large numbers for classes of functions. journal of multivariate analysis yunck t. a technique to identify nearest neighbors. ieee transactions on systems man and cybernetics zhao l. exponential bounds of mean error for the nearest neighbor estimates of regression functions. journal of multivariate analysis zhao l. exponential bounds of mean error for the kernel estimates of regression functions. journal of multivariate analysis zhao l. krishnaiah p. and chen x. almost sure lr-norm convergence for based histogram estimates. theory of probability and its applications zygmund a. trigonometric series i. university press cambridge. author index abou-jaoude s. abram g.d. aitchison aitken cg.g. aizerman m.a. akaike h. alexander k. anderson a.c. anderson j.a. anderson m.w. anderson t.w. angluin d. anthony m. antos a. vi argentiero p. arkadjew a.g. ash rb. assouad p. azuma k. bahadur rr bailey t. barron a.r vi barron rl. vi bartlett p. bashkirov baskett f. baum e.b. beakley g.w. beaudet p. beck j. becker p. beiriant j. vi ben-bassat m. benedek g.m. bennett benning rd. bentley j.l. beran rh. beriinet a. vi bernstein s.n. bhattacharya p.k. bhattacharyya a. bickel p.j. birge l. blumer a. bosq d. vi brailovsky v.l. braverman e.m. breiman l. brent rp. broder a.j. author index broeckaert i. broniatowski m. vi broomhead d.s. buescher k.l. burbea j. burman p. vi burshtein d. buzo a cacoullos t cao r vi carnal h. casey rg. cencov n.n. chang e.l. chang c.y chatterjee b. chen c. chen h. chen t chen x.r chen ye. chen z. chernoff h. chervonenkis ay v chin r chou p.a chou w.s. chow c.k. chow ys. ciampi a. collomb g. conway j.h. coomans d. cormen th. cover tm. v vi cramer h. csibi s. v csiszar csuros m. vi cuevas a vi cybenko g. darken e. dasarathy b.v das gupta s. day n.e. de guzman m. deheuvels p vi della pietra vd. delp e.j. devijver p.a vi devroyel. diaconis p dillon w.r. donahue m. donaldson rw. do-tu h. dubes rc. duda ro. dudani s.a. dudley r.m. duin rp.w. dunn oj. durrett r. dvoretzky a. edelsbrunner h. efron b. ehrenfeucht a elashoff j.d. elashoff rm. fabian v fano rm. farago a. vi farago t feinholz l. feller w. finkel ra. fisher f.p. fisher ra fitzmaurice g.m. fix e. flick t.e. forney g.d. fraiman r vi friedman j.h. fritz j. v fu ks. fuchs h. fukunagak. funahashi k gabor d. gabriel kr gaenssler p. gallant ar ganesalingam s. garnett j.m. gates g.w. gelfand s.b. geman s. gerberich cl. gessaman m.p. gessaman p.h. geva s. gijbels i. vi gine e. girosi e glick n. vi goldberg p. goldmann g.e. goldstein m. golea m. gonzalez-manteiga w. vi goodman rm. goppert r gordon l. gowda kc. grant e.d. gray rm. greblicki w. grenander u. grimmett g.r guo h. gurvits l. author index gyorfi l. gyorfi z. haagerup u. habbema j.d.e hagerup t. hall p. vi hand d.j. hardie w. halt p.e. v hartigan j.a. hartmann crp. hashlamoun w.a hastie t. haussler d. hecht-nielsen r hellman m.e. henrichon e.g. herman c. hermans j. hertz j. hills m. hinton g.e. hjort n. hodges j.l. hoeffding w. hoff m.e. holden s.b. holmstrom l. hora s.c. horibe y home b. hornik k hostetler l.d. huber p.j. hudimoto h. hummels d.m. hush d. hwang cr installe m. isenhour t.l. isogai e. vi itai a author index ito t. ivakhnenko a.g. jain a.k. jeffreys h. jerrum m. johnson d.s. jones l.k. kailath t. kanal l.n. kanevsky d. kaplan m.r karlin a.s. karp rm. karpinski m. kazmierczak h. kearns m. kedem z.m. kegl b. vi kemp r kemperman j.h.b. kerridge n.e. kessel d.l. khasminskii rz. kiefer j. kim b.s. kittler j. kizawa m. klemehi j. knoke j.d. kohonen t. kolmogorov a.n. konovalenko v.v. komer j. koutsougeras c. kraaijveld m.a. krasitskyy m.s. krishna g. krishnaiah p.r kronmal ra. krzanowski w.j. krzyzak a. vi kulikowski c.a. kulkarni s.r kullback s. kumar p.r kurkova v. kushner h.j. lachenbruch p.a. laforest l. laha rg. landgrebe d.a. lecam l. leibler a. leiserson c.e. levinson s.e. li t.l. li x. lin h.e. lin t.t. lin y.k. linde y. linder t. vi lissack t. littlestone n. liu r ljung l. lloyd s.p. loftsgaarden d.o. logan b.e loh w.y. loizou g. lorentz g.g. lowe d. lowry s.l. lugosi g. lukacs e. lunts a.l. maass w. machell e macintyre a. mack y.p. vi mahalanobis p.c. mantock j.m. marchand m. marron j.s. massart p mathai a.m. matloff n. matula d.w. matushita k max j. maybank s.j. mcdiarmid c. mclachlan g.j. mehrotra kg. meisel w. michalopoulos w.s. michel-briand c. michelotti mickey p.a. mielniczuk j. milhaud x. min p.j. minsky m.l. mitchell a.f.s. mizoguchi r monro s. moody j. moore d.s. morgan j.n. muchnik i.e. mui j.k myles j.p. nadaraya e.a. nadas a. vi nagy g. narendra p.m. natarajan b.k naylor b. m.b. niemann h. nilsson n.j. nobel a.b. vi nolan d. oja e. okamoto m. olshen ra. ott l author index paii i. vi palmer rg. papachristou c.a. papadimitriou c.h. papert s. park j. park s.b. park y. parthasarathy kr pmzen e. patrick e.a. pawlak m. vi payne h.j. pearl j. penrod c.s. peterson d.w. petrache g. petrov v.v. petry h. vi pflug g. vi pikelis v. pinsker m. pinter m. vi pippinger n. poggio t. pollard d. powell m.j.d. prep arata p.p. priest rg. pruitt r psaltis d. purcell e. pyatniskii e.s. qing-yun s. quesenberry c.p. quinlan j.r rabiner l.r rao c.r rao rr rathie p.n. raudys s. ravishankar c.s. rejto l. renyi a. revesz p v author index richardson t. ripley b.d. rissanen j. ritter g.l. rivest rl. robbins h. rogers w.h. rosenberg a.e. rosenblatt e rosenblatt m. rounds e.m. roussas g. vi royall rm. rozonoer l.i. rub c. rumelhart d.e. ruppert d. samarasooriya vn.s. samet h. sandberg i.w. sansone g. sarvarayudu g.p.r sauer n. scheffe h. schhiffli l. schmidt w.e schoenberg lj. schroeder a. schwartz s.c. schwemer g.t. sebestyen g. serfling r.i. sethi i.k. shahshahani m. shamos m.i. shannon c.e. shawe-taylor j. shen x. shepp l. shimura m. shirley p. shiryayev a.n. short rd. shustek l.j. silverman b.w. simar l. vi simon h.d. sitte j. sklansky sloane n.j.a. slud e.v smyth p. snapp rr sokal rr sonquist j.a. sontag e. specht d.e spencer j. sprecher d.a. steele j.m. stein c. steinbuch k. stengle g. stephenson r. stinchcombe m. stirzaker d.r stoffel j.c. stoller d.s. stone c.i. v stone m. stuetzle w. stute w. vi suen c.y. sung k. swonger c.w. szabados t. vi r szarek s.i. szego g. talagrand m. talmon taneja i.j. tarter m.e. thomas j.a. tibshirani r.i. tikhomirov vm. tomek i. toronto a.e toussaint g.t. vi tsypkin y.z. tukey j. tulupchuk y.m. author index tuteur f.b. tutz g.e. tymchenko lk tyrcha j. ullmann j.r vajda igor vi vajda istvan valiant l.g. van campenhout j.m. van den broek k van der meulen e.c. vi vanichsetakul n. van ryzin j. vapnik v.n. v varshney p.k veasey l.g. venkatesh s.s. vidal e. vilmansen t.e. vitushkin ag. wagner tj. v vi walk h. vi wand m.p. wang q.r warmuth m.k warner h.r wassel g.n. watson g.s. weiss s.m. wenocur rs. white h. whitsitt s.j. widrow b. wilcox j.b. williams rj. wilson d.l. wilson j.g. winder ro. wise g.l. wold h. wolfowitz j. wolverton c.t. wong w.h. woodruff h.b. wouters w. v xu l. yakowitz s. vi yatracos yg. vi yau s.s. you kc. yuille al. yukich j.e. yunck t.p. zeitouni zhaol.c. zeger k vi zinn j. zygmund a subject index absolute error accuracy adaline additive model admissible transformation agreement aid criterion alexanders inequality ancestral rule a posteriori probability apparent error rate see error estimation resubstitution approximation error arrangement arrangement classifier association inequality asymptotic optimality back propagation bahadur-lazarsfeld expansion bandwidth see smoothing factor barron network basis function complete orthonormal haar hermite laguerre legendre rademacher rademacher-walsh trigonometric walsh bayes classifier rule bayes decision see bayes classifier bayes error subject index estimation of bayes problem bayes risk see bayes error bennetts inequality beppo-levy theorem bernstein perceptron bernstein polynomial bernsteins inequality beta distribution bhattacharyya affinity bias binary space partition tree balanced raw binomial distribution binomial theorem boolean classification borel-cantelli lemma bracketing metric entropy branch-and-bound method esp tree see binary space partition tree cart catalan number cauchy-schwarz inequality central limit theorem channel characteristic function chebyshev-cantelli inequality chebyshevsinequality chernoffs affinity chernoffs bounding method class-conditional density class-conditional distribution classifier selection lower bounds for class probability clustering committee machine complexity penalty complexity regularization concept learning see learning confidence consistency definition of of fourier series rule of generalized linear rules of kernel rules of maximum likelihood of nearest neighbor rules of neural network classifiers of partitioning rules subject index of skeleton estimates of squared error minimization strong see strong consistency strong universal see strong universal tie breaking consistency distribution universal see universal consistency consistent rule see consistency convex hull cover-hart inequality covering of a class of classifiers of a class of functions empirical simple empirical covering lemma covering number cross-validation curse of dimensionality decision with rejection denseness in l in l in lp with respect to the supremum norm density estimation density of a class of sets diamond dimension reduction of distance data-based empirical euclidean generalized lp rotation-invariant beta binomial class-conditional exponential geometric hypergeometric maxwell multinomial normal rayleigh strictly separable distribution function class-conditional empirical divergence see kullback-leibler divergence dominated convergence theorem dvoretzky-kiefer-wolfowitz-massart inequality elementary cut embedding empirical classifier selection see classifier selection empirical covering see covering empirical error empirically optimal classifier empirical measure subject index empirical risk see empirical error empirical risk minimization empirical squared euclidean distance error bias of rotation smoothed u see leave-one-out estimate estimation error euclidean norm eulers theorem exponential distribution exponential family fanos inequality fatous lemma f-divergence feature extraction entropy e number see covering number e see metric entropy em-optimality e number see packing number error estimation estimator bootstrap complexity penalized cross validated see leave-one-out estimate deleted see leave-one-out estimate double bootstrap eo estimator error counting holdout variance of posterior probability randomized bootstrap resubstitution f fingering fingering dimension fisher linear discriminant flexible grid fourier series classifier fourier series density estimation fubinis theorem fundamental rule fundamental theorem of mathematical statistics see glivenko-cantelli theorem gabriel graph gabriel neighbor gamma distribution gaussian distribution see normal distribution gaussian noise geometric distribution gessamans rule ghost sample giniindex glivenko-cantelli theorem gradient optimization grenanders method of sieves grid complexity group method of data handling hamming distance ham-sandwich theorem harts rule hellinger distance hellinger integral histogram density estimation histogram rule cubic data-dependent lazy randomized hoeffdings inequality holders inequality hypergeometric distribution imperfect training impurity function inequality subject index cover-hart dvoretzky-kiefer-wolfowitz-massart fanos hoeffdings holders jensens khinchines large deviation lecams markovs mcdiarmids pinskers rogers-wagner vapnik-chervonenkis information divergence see leibler divergence jeffreys divergence jensens inequality j essen-marcinkiewitz-z ygmund theorem alexanders association bennetts bernsteins cauchy-schwarz chebyshev-cantelli chebyshevs karhunen-loeve expansion k-d tree chronological deep permutation-optimized chronological well-populated kernel subject index cauchy de la vallee-poussin devilish epanechniko exponential gaussian hermite multiparameter naive negative valued polynomial product regular star-shaped uniform see naive kernel window see naive kernel kernel complexity kernel density estimation kernel rule automatic variable khinchines inequality k-local rule k-means clustering kolmogorov-lorentz network kolmogorov-lorentz representation kolmogorov-smirnov distance generalized kolmogorov-smirnov statistic kolmogorov variational distance k-spacing rule kullback-leibler divergence l consistency l convergence l distance l error consistency large deviation inequality learning algorithm supervised with a teacher learning vector quantization lebesgues density theorem lecams inequality likelihood product linear classifier generalized linear discriminant see linear classifier linear independence linear ordering by inclusion lloyd-max algorithm local average estimator logistic discrimination log-linear model l p error lying teacher see imperfect training mahalanobis distance majority vote markovs inequality martingale martingale difference sequence matushita error maximum likelihood distribution format regression format set format maxwell distribution mcdiarmids inequality median tree theoretical method of bounded differences metric see distance metric entropy minimax lower bound minimum description length principle minimum distance estimation moment generating function monomial monotone layer moving window rule multinomial discrimination multinomial distribution multivariate normal distribution see normal distribution subject index nearest neighbor clustering nearest neighbor density estimation nearest neighbor error nearest neighbor rule admissibility of the rule asymptotic error probability of the rule see nearest neighbor error asymptotic error probability of the k-nn rule automatic based on reference data see prototype nn rule condensed data-based see automatic nearest neighbor rule edited k-nn l-nn layered prototype recursive relabeling selective variable metric weighted neural network natural classifier nearest neighbor classifier multilayer with one hidden layer subject index with two hidden layers neyman-pearson lemma normal distribution order statistics outer layer see monotone layer overfitting packing number parameter estimation parametric classification parsevals identity partition complexity of a family of cubic data-dependent e cardinality of family of partitioning rule see histogram rule perceptron perceptron criterion pigeonhole principle pinskers inequality planar graph plug-in decision polynomial discriminant function polynomial network potential function rule principal component analysis probabilistic method projection pursuit pseudo dimension pseudo probability of error quadratic discrimination rule quadtree chronological deep quantization radial basis function radon-nikodym derivative radon-nikodym theorem rate of convergence rayleigh distribution record recursive kernel rule regression function estimation relative stability rogers-wagner inequality rotation invariance see transformation invariance royalls rule sample complexity sample scatter sampling without replacement subject index scale invariance see transformation invariance standard empirical measure see empirical scing measure scatter matrix scheffes theorem search tree balanced shatter coefficient shattering sieve method sigmoid arctan gaussian logistic see standard sigmoid piecewise polynomial standard threshold signed measure skeleton estimate smart rule smoothing factor data-dependent spacing splitting criterion splitting function splitting the data squared error statistically equivalent blocks stirlings formula stochastic approximation stochastic process expansion of sampling of stoller split empirical theoretical stollers rule stones lemma stones theorem stone-weierstrass theorem strictly separable distribution strong consistency definition of of fourier series rule of kernel rules of nearest neighbor rules of neural network classifiers of partitioning rules strong universal consistency of complexity regularization definition of of generalized linear rules of kernel rules of nearest neighbor rules of neural network classifiers of partitioning rules structural risk minimization see complexity regularization sufficient statistics superclassifier subject index support symmetric rule symmetrization tennis rule testing sequence total boundedness total positivity total variation transformation invariance tree a-balanced binary bsp see binary space partition tree classifier depth of expression greedy height of horton-strahler number of hyperplane see binary space partition tree k-d see k-d tree median see median tree ordered ordinary perpendicular splitting pruning of see trimming of a tree sphere trimming of unbiased estimator uniform deviations see uniform laws of large numbers uniform marginal transformation universal consistency of complexity regularization definition of of generalized linear rules ofkemel rules of nearest neighbor rules of neural network classifiers of partitioning rules vapnik-chervonenkis dimension see vc dimension vapnik-chervonenkis inequality variation vc class vc dimension voronoi cell voronoi partition weighted average estimator uniform laws of large numbers x-property pattern recognition presents one of the most significant challenges for scientists and engineers and many different approaches have been proposed. the aim of this book is to provide a self-contained account of probabilistic analysis of these approaches. the book includes a discussion of distance measures nonparametric methods based on kernels or nearest neighbors vapnik-chervonenkis ry epsilon entropy parametric classification error estimation tree classifiers and neural networks. wherever possible distribution-free properties and inequalities are derived. a substantial portion of the results or the analysis is new. over problems and exercises complement the material. isbn isb!